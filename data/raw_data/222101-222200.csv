question_id,title,body,tags
4553476,Statistical distance between two coin-flipping distributions,"I have two distributions that I suspect are close in statistical (variational) distance i.e. $\sum_{x} \| D_1(x) - D_2(x) \|$ is small. The first distribution is $N$ coin-flips of iid coins with probability $p$ of coming up heads. For the second distribution, I first pick $F$ locations uniformly randomly out of the $N$ and set them to heads. For the remaining locations, I flip iid coins with bias $p' = \frac{pN - F}{N- F}$ . (The rationale for the adjusted probability is so that the expected number of heads match up in both situations.) How close are these distributions in statistical distance? Is there an easy bound one can place? Here I'm thinking of $p = N^{-1/10}$ and $F \le N^{1/100}$ . My progress Let's note that both distributions are permutation symmetric so the statistical distance between the two distributions is equivalent to the statistical distance between random variables $X$ and $Y$ where $X \sim \textrm{Binomial}(N, p)$ and $Y \sim F + \textrm{Binomial}(N-F, p')$ . The intuition as to why these random variables should have similar distribution is that as $N \rightarrow \infty$ , the distributions approach normal distributions which are both centered at $pN$ but $X$ concentrates faster than $Y$ .","['probability-distributions', 'combinatorics', 'binomial-coefficients']"
4553495,"Closed subspace of $L^{p}( [a,b])$ consisting of continuous functions","I'm wondering if a closed subspace of $L^p [a,b](1\leq p<\infty)$ consisting of continuous functions must be finite-dimensional? I already know it's true for $L^2([a,b]): $ this question","['measure-theory', 'functional-analysis', 'analysis']"
4553550,Counting integers $n \leq x$ such that $rad(n)=r$,"Let $r$ be the largest square-free integer that divides $n$ . Then $$r = \operatorname{rad}(n)=\prod_{\substack{p\mid n\\p\text{ prime}}}p$$ $r$ is called the ""radical"" of $n$ , or the square-free kernel. Have also seen the term ""core"". For a given square-free $r$ and integer $x$ , my question is how many integers $n\leq x$ exist such that $\operatorname{rad}(n) = r$ . For example with $r=15$ , and $x=100$ , then $15,45,75$ are the only $3$ such integers. I haven't found any formula or algorithm in the literature that seems useful here, for generic $r$ and $x$ .","['number-theory', 'combinatorics', 'discrete-mathematics', 'algorithms', 'prime-numbers']"
4553699,Edge density of graph,"I've started reading Graph Regularity Lemma but the author starts with the notion of edge density. Definition (Edge density) Let $X$ and $Y$ be sets of vertices in a graph $G$ . Let $e_G(X,Y)$ be
the number of edges  between $X$ and $Y$ ; that is, $$e_G(X,Y):=|\{(x,y)\in X\times Y: xy\in E(G)\}|.$$ Define the edge
density between $X$ and $Y$ in $G$ by $$d_G(X,Y):=\frac{e_G(X,Y)}{|X||Y|}.$$ We allow $X$ and $Y$ to overlap in the definition above. But I do not think that $e_G(X,Y)$ defined above counts the number of edges between $X$ and $Y$ .
Indeed, if we take a look at the following graph: the number of edges here is actually 7. But the formula give 8 because it double counts the edge 34. I am wondering is that normal that the formula double counts some edges?","['graph-theory', 'combinatorics', 'discrete-mathematics']"
4553718,How many time do we need to shuffle two decks red and blue to get back to initial colors?,"Question Let's say: I have two decks of $n$ cards each (a blue and a red decks), I am not interested in card faces, just in colors of card backs; Each time I shuffle perfectly ( $n$ cards in each hand, 1 card from right, 1 card from left, and so on); Then I split the $2n$ cards deck into two $n$ decks and I reshuffle again. How many time do I have to shuffle the deck in order to get the two single colored decks again? For $n=5$ , it takes 6 shuffles: Left hand                                  Right hand
['B', 'B', 'B', 'B', 'B',    'R', 'R', 'R', 'R', 'R']
['B', 'R', 'B', 'R', 'B',    'R', 'B', 'R', 'B', 'R']
['B', 'R', 'R', 'B', 'B',    'R', 'R', 'B', 'B', 'R']
['B', 'R', 'R', 'R', 'R',    'B', 'B', 'B', 'B', 'R']
['B', 'B', 'R', 'B', 'R',    'B', 'R', 'B', 'R', 'R']
['B', 'B', 'B', 'R', 'R',    'B', 'B', 'R', 'R', 'R']
['B', 'B', 'B', 'B', 'B',    'R', 'R', 'R', 'R', 'R'] Context Not related, but fun fact: this question arose with my daughter when we did shuffled two bicycles decks when we learned to play to Eleusis last week. To answer it, we tried experimentally for small decks of sizes ( $n<10$ ) and it looks not so simple as we tough. On the same time we had the feeling that for some specific values the number of shuffle simply is the number of cards minus 1 (4, 9, 12, 21, 24, 36, 40, 49, 52). Doing some representations by hand I also have the feeling that solving this problem is somehow related to combinatorics, permutations and binary strings. But I lack some insights to formalize it thus I resorted to numerical modeling. Numerical results Then I modeled the process with the following Python program: class Shuffler:
    
    @staticmethod
    def new(n):
        return np.array([""B""]*n + [""R""]*n)
    
    @staticmethod
    def shuffle(x):
        return x.reshape((2, -1)).T.reshape((2, -1)).flatten()
    
    @staticmethod
    def identity(n):
        return np.arange(2*n) + 1
    
    @staticmethod
    def permutation(n):
        return Shuffler.shuffle(Shuffler.identity(2*n))
    
    @staticmethod
    def generate(x=None, n=1):
        
        if x is None:
            x = Shuffler.new(n)
        else:
            n = x.size // 2
        assert x.size == 2*n
            
        x0 = x.copy()
        yield x0
        
        while True:
            x = Shuffler.shuffle(x)
            yield x
            
            if np.all(x == x0):
                break
    
    @staticmethod
    def length(x):
        n = 0
        for x in Shuffler.generate(x):
            n += 1
        return n - 1
    
    @staticmethod
    def screen(n):
        t = np.arange(n) + 1
        c = np.full(t.shape, 0)
        for i, m in enumerate(t):
            x = Shuffler.new(m)
            c[i] = Shuffler.length(x)
        return t, c The result for the first thousand deck sizes looks like: First 100 values are: 1,   2,   4,   3,   6,  10,  12,   4,   8,  18,   6,  11,  20,
    18,  28,   5,  10,  12,  36,  12,  20,  14,  12,  23,  21,   8,
    52,  20,  18,  58,  60,   6,  12,  66,  22,  35,   9,  20,  30,
    39,  54,  82,   8,  28,  11,  12,  10,  36,  48,  30, 100,  51,
    12, 106,  36,  36,  28,  44,  12,  24, 110,  20, 100,   7,  14,
   130,  18,  36,  68, 138,  46,  60,  28,  42, 148,  15,  24,  20,
    52,  52,  33, 162,  20,  83, 156,  18, 172,  60,  58, 178, 180,
    60,  36,  40,  18,  95,  96,  12, 196,  99, ... The first 73 of this series exactly match the series A002326 in OEIS: Multiplicative order of 2 mod 2n+1. t, c = Shuffler.screen(100)
# From https://oeis.org/A002326/list
check = np.array([
    1, 2, 4, 3, 6, 10, 12, 4, 8, 18, 6, 11, 20, 18, 28, 5, 10,
    12, 36, 12, 20, 14, 12, 23, 21, 8, 52, 20, 18, 58, 60, 6,
    12, 66, 22, 35, 9, 20, 30, 39, 54, 82, 8, 28, 11, 12, 10, 36,
    48, 30, 100, 51, 12, 106, 36, 36, 28, 44, 12, 24, 110, 20, 100,
    7, 14, 130, 18, 36, 68, 138, 46, 60, 28
])
np.all(c[0:check.size] == check)  # True It's tenting to think from it that there is always a finite number of shuffles that reset the two decks: The process seems not get trapped in a cycle (infinite number of shuffles); The number of shuffles seems to be bounded by some dominating function (looking like a line of slope 2); When testing for coincidence, resetting colors seems to always happen when the card orders also reset, maybe resetting the colors is always achieved by resetting the initial setup. So here I am, kindly asking the community a bit of help to get more grasp on this question: Can we express the number of shuffles in term of deck size? Or is it something more like the Collatz Conjecture .","['permutations', 'conjectures', 'combinatorics', 'card-games', 'recreational-mathematics']"
4553727,Convergence in $L^p(\lambda)$ of mean of backward shifted integrable functions,"Let $(\Omega, \mathcal{A}, \mu)$ be a $\sigma$ -finite measure space. I want to prove the following (Exercise 7.1.2 in Klenke's Probability book): Let $p \in (1, \infty), f \in \mathcal{L}^p(\lambda)$ , where $\lambda$ is the Lebesgue measure on $\mathbb{R}$ . Let $T: \mathbb{R} \to \mathbb{R}, x \to x + 1$ . Show that, \begin{align}
\frac{1}{n}\sum_{k = 0}^{n - 1}f\circ T^k \to 0 \hspace{0.2cm} \text{as} \hspace{0.2cm} n \to \infty \hspace{0.2cm} \text{in} \hspace{0.2cm} L^p(\lambda).
\end{align} I am lost on how to prove this. I think I am at least able to prove that the sequence is a Cauchy sequence and thus converges, but even there I am not sure. Here is my attempt: Let $S_n = \frac{1}{n}\sum_{k = 0}^{n - 1}f\circ T^k$ . Then \begin{align}
\|S_n - S_{n - 1}\|_p &= \|\frac{1}{n} f \circ T^{n-1} - \frac{1}{n(n-1)}\sum_{k = 0}^{n -2}f \circ T^k \|_p \\
& \leq \frac{1}{n} \|f \circ T^{n - 1}\|_p + \frac{1}{n(n - 1)}\sum_{k = 0}^{n -2} \|f \circ T^k \|_p\\
&= \frac{2M}{n} \to 0,
\end{align} where $M = \|f\circ T^k\|_p$ . Here I am obviously assuming $\|f\circ T^i\|_p = \|f\circ T^j\|_p$ for all $i, j \in \{0, \ldots, n- 1\}$ , which I think is true, but can only provide intuitive (as opposed to formal) justification (Since $f \in \mathcal{L}^p (\lambda)$ , then the integral of the function should not change by shifting it around horizontally. I guess the idea would be to invoke the translation invariance of the Lebesgue measure?). I have two questions then: (1) Is my proof that the sequence converges in $L^p(\lambda)$ correct? Is my above assumption correct? If yes, what would be the formal justification? (2) How would one prove that the limit is $0$ ? Thanks much in advance!","['lebesgue-integral', 'measure-theory', 'lebesgue-measure', 'convergence-divergence']"
4553751,Probability of the intersection of the complement of two events,"Suppose that $A$ and $B$ are events for which $P(A)=x, P(B)=y$ , and $P(A\cap B)=z.$ Express the following probability in terms of $x, y, \text{and} \,z$ . $P(\bar A\cup \bar B$ ). Answer: $1-z$ . This is the Exercise 1.16 from (Paul Meyer, ""Introductory Probability and Statistical Applications"", 2ed) My attempt: $P(\bar A\cup \bar B) = P(\bar A) + P(\bar B) - P(\bar A\cap \bar B)$ $\qquad=1-x +1-y-[(1-x)(1-y)]$ $\qquad=2-x-y-[1-y-x+xy]$ $\qquad=1-xy$ What was my mistake here? I don't get why $z$ appeared in the answer since we are taking the interjection between the complement events of $A$ and $B$ .","['statistics', 'probability']"
4553769,Efficient recomputation of row-space projection matrix given a new row vector,The projection matrix that projects onto the row space of a matrix $A$ is $$P_A = A^* (A A^*)^{-1} A$$ Let $B$ be $A$ with a new row vector $v$ appended. Is there an efficient way to compute $P_B$ from $P_A$ without starting from scratch? Something like the matrix determinant lemma or Sherman–Morrison formula ?,"['matrices', 'computational-mathematics', 'linear-algebra', 'algorithms', 'projection-matrices']"
4553778,Are most finite topologies $T_{0}$? (or are most preorders partial orders?),"I've wondered for a while whether or not most finite topologies are $T_{0}$ , in the sense that as n goes to infinity, does the proportion of finite topologies of size n that are $T_{0}$ go to 1 or to 0? or does it limit to something inbetween? or not at all? I'm thinking about the number of finite topologies up to homeomorphism, but if there are similar results about, for example, random finite topologies I would also be interested to hear them. I had at first assumed it would be obvious from the first terms of the sequences in the OEIS, but they seem to grow at similar rates. https://oeis.org/A000112 https://oeis.org/A001930 (the number of $T_{0}$ topologies of size n is the same as the number of partial orders, due to the isomorphism of categories between Alexandrov topologies and preorders)","['order-theory', 'general-topology', 'combinatorics']"
4553837,Is there an algebraic solution to $\log_{\sqrt2}{\left(x\right)} = (\sqrt2)^x$?,I’m trying to solve $$\log_{\sqrt2}{\left(x\right)} = (\sqrt2)^x$$ My next step is $$\ln{x}= (\sqrt2)^x\ \cdot\ \ln\sqrt2$$ EDIT: I’m only up to high school math.,"['logarithms', 'inverse-function', 'lambert-w', 'closed-form', 'algebra-precalculus']"
4553904,"Probability of $k \in \{1, 2, \dots, n\}$ people sharing the same birthday in a room of $n$ people","In a room of $n$ people, what is the probability that at least $k \in \{1, 2, \dots, n\}$ people all share a common birthday? For example, what is the probability that at least 5 people all share the same birthday in a room of a 1000 people. I have found a formula on Wikipedia for solving this problem when $k = 2$ but I would like to be able to solve for $n > 365$ and $k > 2$ . I know that $p(k \text{ people share the same birthday}) = 1$ for $k = \lceil\frac{n}{365}\rceil$ but I don't know how to solve beyond this point. I assume there are $365^n$ different ways the birthdays of $n$ people can be distributed within a year hence I am struggling to reach an answer using a combinatorial approach. I have researched this problem online but most answers stop at the probability of one or more shared birthday in a group of 60 people max.","['combinatorics', 'probability']"
4553995,Isn't a differential curve with corners contradictory?,"While reading ""Riemannian Geometry"" by M. Do Carmo, I've ran into a confusing remark following the definition of a curve.
The author presents the following definition (2.8, chapter 1) for a curve: A differential mapping $c:I \rightarrow  M$ of an interval $I \subset  \mathbb{R}$ into a differentiable manifold M is called a (parametrized) curve Following the definition, Do Carmo remarks: Observe that a parametrized curve can admit self-intersections as well as corners. My Question is - Isn't the geometric interpretation of a ""differentiable"" is ""smooth""? How come a differentiable curve admits a corner? I'd be glad to see an example for a differentiable curve with a corner. Many Thanks!","['curves', 'smooth-manifolds', 'differential-geometry']"
4554022,Why does the delta depend on the epsilon in the definition of the limit of a function?,"For example, why can't it be a definition of the limit? $$\lim_{x \to a}f(x)=l\Leftrightarrow( \forall \delta> 0,\exists\varepsilon > 0:|f(x)-l|<\varepsilon \Rightarrow|x-a|<\delta ) $$","['limits', 'calculus', 'analysis']"
4554028,Which even-graded integral cohomology classes arise as Chern classes?,"Given a CW complex $X$ , is there some way to determine which even-graded elements of its integral cohomology ring $H^\star(X, \mathbb Z)$ arise as Chern classes of a complex vector bundle $E \to X$ ? I am particularly interested in the case where $X$ is an infinite-dimensional CW complex whose skeleta are all complex manifolds (e.g., direct limits of flag varieties or complex tori) and the restriction of $E$ to each skeleton is a holomorphic vector bundle.","['complex-geometry', 'vector-bundles', 'algebraic-geometry', 'algebraic-topology']"
4554046,On the Bernoulli differential equation $ y'(x) = \dfrac{y}{x + \sqrt{xy}} $,"$$ y'(x) = \frac{y}{x + \sqrt{xy}} $$ I have attempted to solve it. However, I am not able to arrive at the exact expression of the final answer. Maybe I am making a mistake in a step? Can you please help? The answer should be $$ \sqrt{x} = \sqrt{y}(\log\sqrt{y} + \mbox{Constant}) $$ PS: This is my first question on this site, I will be more than willing to provide my own attempts at this, if it is required. Thanks. Here are my workings, $$ \frac{1}{\sqrt{x}}x'(y)-\frac{\sqrt{x}}{y}=\frac{1}{\sqrt{y}} $$ $$ Taking, \sqrt{x}=z $$ Differentiating  wrt  to x and dividing by dy $$ ,\frac{1}{2\sqrt{x}}x'(y)=z'(y) $$ Substituting, $$ z'(y)-\frac{z}{2y}=\frac{1}{2\sqrt{y}} $$ Now taking integrating factor $$ e^{\int_{}{}-\frac{1}{2y} dy}\ $$ $$ =e^{-\frac{1}{2}\log y } $$ Multiplying both sides by the integrating factor, $$ z'(y)e^{-\frac{1}{2}\log y }-\frac{z}{2y}e^{-\frac{1}{2}\log y }=\frac{1}{2\sqrt{y}}e^{-\frac{1}{2}\log y } $$ $$ (ze^{-\frac{1}{2}\log y })'(y)=\frac{1}{2\sqrt{y}}e^{-\frac{1}{2}\log y } $$ $$ ze^{-\frac{1}{2}\log y }=\int_{}{}\frac{1}{2y} dy\ $$ $$ze^{-\frac{1}{2}\log y }=\frac{1}{2}log(y)+Constant $$ $$ \frac{\sqrt{x}}{\sqrt{y}}=\frac{1}{2}log(y)+Constant $$ There you go, did I mess up some logarithmic property? or is it something with the integration I am missing? The answer is close, but not it.",['ordinary-differential-equations']
4554101,finding the volume form under a change of metric,"Let $(M,g_{ij})$ be a smooth Riemannian manifold.
We introduce a new metric $\tilde{g}_{ij}: = (e^{\phi} g)_{ij}$ where $\phi:M \to \mathbb{R}$ is a smooth function. Now we want to calculate the volume form w.r.t. the new metric in terms of the old volume form. All of this in local coordinates.
I found two ways to do this: Let $(U,x_1,...,x_n)$ be the local coordinates on M, such that the frame $(\partial x_1,..., \partial x_n)$ is orthonormal w.r.t. $g_{ij}$ . the ""old"" volume form is then just $vol = dx_1 \wedge dx_2 \wedge ... \wedge dx_n$ . an orthonormal frame $(f_1,...,f_n)$ w.r.t. $\tilde{g}_{ij}$ is then given by $f_i := e^{-\frac{\phi}{2}} \partial{x_i}$ . So that we get the new volume form: $vol_{new} = e^{-\frac{\phi}{2}} d{x_1} \wedge ... \wedge e^{-\frac{\phi}{2}}d{x_n} = e^{-n\frac{\phi}{2}} vol$ $vol_{new} = \sqrt{det(\tilde{g}_{ij}})dx_1 \wedge ... \wedge dx_n = e^{n \frac{\phi}{2}}\sqrt{(det(g_{ij})}dx_1 \wedge...\wedge dx_n = e^{n \frac{\phi}{2}} vol$ . Now why don't I get the same result ?","['volume', 'change-of-basis', 'riemannian-geometry', 'differential-geometry']"
4554104,How to define all the points inside of some polygon as the domain of the function?,"Let's say that we have some polygon defined by the following vertices: $$
A(1, 1) \\
B(2, 1) \\
C(5, 3) \\
D(2, 2) \\
E(3, 1) \\
$$ This polygon contains an infinite number of points denoted by their coordinates $p_x$ and $p_y$ . Let's say that I want to have some simple function, such as the one that would sum the coordinates for every point: $$
f(p_x, p_y) = p_x + p_y
$$ I don't understand how can I denote the domain of this function, since I want it to only take as input the values of $p_x$ and $p_y$ which are located within the polygon described above. $$
p_x, p_y \in ?
$$","['functions', 'geometry']"
4554132,Am I right? Building matrix from polynomial and derivative,"I'm just really unsure whether I'm correct about building a system of equations since there are derivatives. What is known: $$f(x) = a_1x^3 + a_2x^2 + a_3x + a_4$$ $$f(−1) = 5, f(1) = 3, f'(−1) =−7, f'(1) = 13$$ So we have: $f(-1) = a_1(-1)^3 + a_2(-1)^2 + a_3(-1) + a_4 = -a_1 + a_2 -a_3 + a_4 = 5$ $f(1) = a_1\times1^3 + a_2\times1^2 + a_3\times1 + a_4 = a_1 + a_2 + a_3 + a_4 = 3$ The derivative form of this function is the following (I guess): $f'(x) = 3a_1x^2 + 2a_2x + a_3 $ As a result, $f'(-1) = 3a_1(-1)^2 + 2a_2(-1) + a_3 = 3a_1 -2a_2 + a_3 =-7$ $f'(1) = 3a*1^2 + 2b*1 + c = 3a_1 + 2a_2 + a_3 = 13$ So we have system of equations: $-x_1 + x_2 - x_3 + x_4 = 5$ $x_1 + x_2 + x_3 + x_4 = 3$ $3x_1 - 2x_2 +x_3 + 0 \times x_4 = -7$ $3x_1 + 2x_2 +x_3 + 0 \times x_4 = 13$ And from this system I can build matrix. Is everything correct?","['matrices', 'derivatives', 'linear-algebra', 'polynomials']"
4554143,Question on Hartshorne Theorem II.4.9,"In the proof of the theorem, Hartshorne claims that $X=\mathbb{P}_\mathbb{Z}^n=\operatorname{Proj}\mathbb{Z}[x_0,\cdots,x_n]$ is proper over $\operatorname{Spec} \mathbb{Z}$ , and he uses the valuative criterion to prove his claim: For a given valuation ring $R$ , let $K=\operatorname{Frac}(R)$ . Given morphisms $U=\operatorname{Spec} K\to X$ , $T=\operatorname{Spec} R\to \operatorname{Spec}\mathbb{Z}$ as shown: Then there exists a unique morphism $T\to X$ making the whole diagram commutative. I understand how he gives the morphism, but I cannot figure out why it is unique.  In the proof, he says ""The uniqueness of this morphism follows from the construction and the way the $V_i=D_+(x_i)$ patch together."" The followings are my attempt: To give a morphism $T\to X$ is equivalent to give a point $y_0\in X$ corresponding to the image of maximal ideal of $R$ and a local homomorphism between $\mathcal{O}_{X,y_o}\to R$ .  And the whole commuting diagram requires us to have a commutative diagram: where vertical homomorphisms are natural and horizon local homomorphisms are given by $T\to X$ and $U\to X$ . But I don't know whether this requirement can imply the local homomorphism $\mathcal{O}_{X,y_o}\to R$ is unique. Thanks for your answer!","['algebraic-geometry', 'schemes']"
4554276,How can I prove that this is a martingale?,"We define $S_0=0$ and $S_n=X_1+...+X_n$ where $(X_n)$ is an i.i.d. sequence such that $\Bbb{P}(X_1=-1)=\Bbb{P}(X_1=1)=1/2$ . We define $\mathfrak{F}_n=\sigma(S_0,...,S_n)$ . I want to show that $(P(S_n,n))_{n\geq 0}$ is an $\mathfrak{F}_n$ -martingale ( $P$ is a polynomial in 2 variables) if for all $l,m\in \Bbb{Z}$ $$P(l+1,m+1)-2P(l,m)+P(l-1,m+1)=0$$ I know that I need to show that $\Bbb{E}(P(S_{n+1},n+1)|\mathfrak{F}_n)=P(S_n,n)$ somehow I don't see how to work with the polynomial case, how can I split this up to get somehow a sum of conditional expectations? I know that this has to do with the equality from above. Could someone maybe give me a hint?","['conditional-expectation', 'probability-theory', 'probability']"
4554292,How to prove that $\int_{0}^{1} (f'(x))^2 dx \geq \frac{2}{e^2}$,"Here is the question I am trying to solve: Suppose $f$ is a real, continuously differentiable function on $[0,1], f(0) = f(1) = 0,$ and $$\int_{0}^{1} f(x)e^x dx = 1.$$ Prove that $(a) \int_{0}^{1} f'(x) e^x dx = -1,$ and $(b) \int_{0}^{1} (f'(x))^2 dx \geq \frac{2}{e^2}.$ My question is: I know how to prove $(a)$ where I used integration by parts but I do not know how to solve $(b),$ should I use the mean value theorem or intermediate value theorem? Can anyone help me in solving letter $(b)$ please?","['integration', 'analysis', 'real-analysis', 'calculus', 'mean-value-theorem']"
4554307,"find all $f:\mathbb{R}\to\mathbb{R}$ so that $|f(x+y)-f(x-y) - y|\leq y^2$ for all $x,y\in\mathbb{R}$","Find all $f:\mathbb{R}\to\mathbb{R}$ so that $|f(x+y)-f(x-y) - y|\leq y^2$ for all $x,y\in\mathbb{R}$ . A standard approach when solving functional equations or functional inequalities is to guess which function might work, and then show that the guessed function is unique. One can show uniqueness by showing that if $g$ is the guessed function and $f$ is any function satisfying the given conditions, then $f-g$ is identically zero. The function $f(x)=x/2$ clearly works. It's definitely the only linear function with zero constant term that works. Note that we may assume $f(0) = 0$ because if we let $h(x)=f(x)+C$ for an arbitrary constant $C$ , then $h$ satisfies the same inequality as $f$ . In general, any function satisfying the given inequality is the sum of a constant and a function $f$ satisfying the inequality and $f(0) = 0$ .  If $f(x)=cx$ for all x then the inequality reads $|c(x+y) - c(x-y) - y|\leq y^2\,\forall x,y\Rightarrow \,\forall y, |y(2c-1)|\leq y^2.$ But as $y\to 0,|\dfrac{y^2}{y}|\to 0$ so eventually $y^2$ will be less than $|y(2c-1)|$ for sufficiently small y, and hence $2c-1$ must be equal to $0$ . Thus we've shown $c=1/2$ if $f(x)=cx$ for all $x$ . Let $g(x)=f(x)-x/2.$ It could be useful to show $g(x)=0$ for all x. We clearly have $|g(x+y)-g(x-y)| = |f(x+y)-(x+y)/2 - f(x-y) +(x-y)/2| = |f(x+y)-f(x-y) - y|\leq y^2$ for all $x,y\in\mathbb{R}$ . Plugging in $x=y$ gives that $|g(2y)-g(0)| \leq y^2$ for all $y$ and by our assumption, $g(0)$ can be assumed to be zero, so $|g(2y)|\leq y^2$ for all $y$ . But I'm not sure how to show that $g$ must be identically zero, assuming $f(0) = 0$ .","['contest-math', 'functional-equations', 'algebra-precalculus', 'inequality']"
4554339,Condition number of random matrix gets worse as dimension grows,"I observed experimentally that, when generating (pseudo) random matrices, their condition number increases with the dimension. Why? When using the norm induced by the Euclidean norm for vector spaces, the condition number is the absolute value of the ratio between the greatest and the smallest eigenvalue. It seems to me that, since increasing the dimension implies getting more eigenvalues, it is easier that this ratio increases.","['matrices', 'random-matrices', 'condition-number', 'linear-algebra']"
4554345,"show that all the roots of a given equation are in $[-1,1]$","Suppose $c\in \mathbb{R}$ is such that one of the roots of the equation $x^3 - \dfrac{3}4 x+c=0$ belongs to $[-1,1]$ . Show that all the roots belong to $[-1,1]$ . The equation is equivalent to $p(x)=0$ where $p(x) = 4x^3 - 3x+4c.$ Observe that $4\cos^3\theta - 3\cos \theta = \cos(3\theta).$ Let the root $r$ of the equation in $[-1,1]$ be equal to $\cos \theta$ for some $\theta \in [0,\pi]$ . From the above observation, $c = -\dfrac{\cos \theta_2}4$ for some $\theta_2 \in [0,\pi]$ . The equation has two more roots, and if one of them is complex, then because the polynomial $p(x)$ has real coefficients, both of the others must be complex. We know that if $r_2,r_3$ are the other roots, then $r r_2 r_3 = -c, r + r_2 + r_3 = 0, r r_2 + r r_3 + r_2r_3 = -3/4.$ I'm not sure how to derive a contradiction from here.","['algebra-precalculus', 'polynomials', 'trigonometry']"
4554348,find a closed form formula for $\sum_{k=1}^n \frac{1}{x_{2k}^2 - x_{2k-1}^2}$,"Let $\{x\} = x-\lfloor x\rfloor$ be the fractional part of $x$ . Order the (real) solutions to $\sqrt{\lfloor x\rfloor \lfloor x^3\rfloor} + \sqrt{\{x\}\{x^3\}} = x^2$ with $x\ge 1$ from smallest to largest by $x_1,x_2,\cdots$ . Provide a closed form formula for $\sum_{k=1}^n \frac{1}{x_{2k}^2 - x_{2k-1}^2}.$ First, I'm not even sure why there are infinitely many solutions to $\sqrt{\lfloor x\rfloor \lfloor x^3\rfloor} + \sqrt{\{x\}\{x^3\}} = x^2$ . One inequality that might be useful is the Cauchy-Schwarz inequality. There's also the AM-GM inequality. We have $ac + bd \leq \sqrt{a^2+b^2}\sqrt{c^2+d^2}$ for all real numbers $a,b,c,d$ where $ac,bd\ge 0$ with equality iff $(a,b),(c,d)$ are proportional vectors in $\mathbb{R}^2$ . Observe that $\lfloor x\rfloor$ always has the same sign as $x$ , so $\lfloor x\rfloor \lfloor x^3\rfloor$ has the same sign as $x^4 \ge 0$ . It might be useful to substitute $\{x\} = x-\lfloor x\rfloor$ into the original equation and simplify the result somehow. Also, it could be possible to write the given sum as a telescoping sum.","['summation', 'inequality', 'calculus', 'cauchy-schwarz-inequality', 'algebra-precalculus']"
4554349,Piecewise continuous transformation of a normally distributed random variable,"Say $U$ is a continuous random variable such that: $$U \sim N(0, \sigma^2)$$ Another variable $X$ is defined such that $X = g(U)$ where: $$g(\omega) =
 \begin{cases} 
      \omega - a & \omega\geq a \\
      0 & -a\leq \omega\leq a \\
      \omega + a & \omega\leq a
   \end{cases} $$ How can I find the PDF of $X$ ? I've tried to start by saying: $$F_X(x) = Pr(X \leq x) = Pr(g(U) \leq x)$$ ... with the aim of ultimately differentiating the CDF to get the PDF. However, I don't really know how to proceed from this point onwards, as it is my first time working with a piecewise transformation. How do I solve this problem?","['probability-distributions', 'probability-theory', 'probability', 'normal-distribution']"
4554402,Solve the equation $(x+1)^4=2(1+x^4)$,"Solve the equation $$(x+1)^4=2(1+x^4)$$ The most intuitive approach for me was to use the formula $$(a+1)^4=a^4+4a^3+6a^2+4a+1,$$ so our equation is $$x^4+4x^3+6x^2+4x+1-2-2x^4=0\\-x^4+4x^3+6x^2+4x-1=0$$ $\pm1$ aren't solutions, so this equation does not have whole roots. Another thing I tried is to factor $$1+x^4\ne(x+1)(x^3-x^2+x-1)=x^4-1,$$ but then I remembered it is for odd $n$ . I don't know what else to try.",['algebra-precalculus']
4554442,An example when $(U^\perp)^\perp = U$ but $V\ne U\oplus U^\perp$?,"Let's talk of inner product spaces. I can show that if $V = U\oplus U^\perp$ , then $(U^\perp)^\perp = U$ holds. Then I wonder about the converse. I was able to dig up a post on SE where an example was given, showing that $U$ being closed is not enough for $V = U\oplus U^\perp$ . I know that if $V$ is Hilbert, then $U$ being closed implies $(U^\perp)^\perp = U$ . But the example's $V$ is not, rendering my search still open since there $U^\perp = \{0\}$ and $U\subsetneq V$ . But I am not able to find one, or disprove this. Hence I seek help! I'd appreciate if what you give is accessible with my current background. My background: I am taking a course on linear algebra, and the professor deals only with finite dimensional spaces. But whenever a chance comes up, I try to generalise the results and see what fails in infinite dimensions. So far, I have seen some counterexamples via $\ell^2$ .","['inner-products', 'orthogonality', 'direct-sum', 'linear-algebra', 'functional-analysis']"
4554487,"Spivak, Ch. 22, Problem 7b: Can we assume a sequence has a limit to show what the limit is, or must we first prove that it converges?","The following is a problem from Chapter 22 ""Infinite Sequences"" from Spivak's Calculus In Problem 2-16 we saw that any rational approximation $k/l$ to $\sqrt{2}$ can be replaced by a better approximation $\frac{k+2l}{k+l}$ . In particular, starting with $k=l=1$ , we obtain $$1,\frac{3}{2},\frac{7}{5},\frac{17}{12},...$$ (a) Prove that this sequence is given recursively by $$a_1=1$$ $$a_{n+1}=1+\frac{1}{1+a_n}$$ (b) Prove that $\lim\limits_{n\to\infty} a_n=\sqrt{2}$ . This gives the
so-called continued function expansion $$\sqrt{2}=1+\frac{1}{2+\frac{1}{2+...}}$$ The solution manual proof of $(b)$ is interesting but a bit long and complicated. The following is an attempt at the proof. It assumes that $\lim\limits_{n\to\infty} a_{n+1}=l$ and uses the fact that $\lim\limits_{n\to\infty} a_{n+1}=\lim\limits_{n\to\infty} a_{n}$ . $$a_{n+1}=1+\frac{1}{1+a_n}=\frac{2+a_n}{1+a_n}$$ $$\lim\limits_{n\to\infty} a_{n+1}=l=\frac{2+l}{1+l}$$ $$l+l^2=2+l$$ $$l=\sqrt{2}$$ This proof assumes that $\lim\limits_{n\to\infty} a_{n+1}$ exists. This seems fishy. Is it the case that we must prove that ${a_n}$ converges before we can use the above proof?","['calculus', 'solution-verification', 'sequences-and-series']"
4554514,"Is set $A=[0,1]^2\cap\mathbb{Q}^2$ Lebesgue measurable?","I'm confused by the notion of measurable sets in measure theory. If we take a set $A=[0,1]^2\cap\mathbb{Q}^2$ , would it be Lebesgue measurable? On one hand: if it is measurable, then outer measure $\lambda^*(A)=\lambda(A)$ . And minimal rectange which covers set $A$ is $[0,1]^2$ , it means $\lambda^*(A)=1$ . So $\lambda(A)=1$ . But on the other hand: $A$ is just a countable union of measurable sets with measure $0$ , so $\lambda(A)=0$",['measure-theory']
4554524,Why do we have that $u\cdot v$ converges weakly to a standard Gaussian random variables as $n\to \infty$?,"Following this question: Can we get the concentration inequality of the inner product of two unit vectors distributed on the sphere? . Let $u$ and $v$ be two random vectors on $R^n$ that are independent and uniformly distributed on the unit sphere. That means we can represent it as Gaussian random vectors $g\sim N(0, I_n)$ , $$u=\frac{g}{\|g\|_2}.
$$ Why do we have that $u\cdot v$ (may be with some orders?) converges weakly to a standard Gaussian random variables as $n\to \infty$ ? That means $u\cdot v=O_p(1)$ .","['statistics', 'probability', 'real-analysis']"
4554537,"How to prove that if a series converges, then the sequence of it's terms multiplies by the inverses of their positions converges","How to prove that for a descending sequence $(a_n)_{n \geq 1}$ , if $\lim_{n \to \infty} \sum_1^n a_n$ converges to something finite, then $\lim_{n \to \infty} n a_n = 0$ A similar problem was asked here but it did not include that the sequence, in the case of the problem the sequence of the differences, is descending. I attempted using the definition. If I take an $\epsilon$ , I can try to prove that after a point $n a_n$ will always be smaller than $\epsilon$ , because after a point there has to be an $n$ for which $n a_n < \epsilon$ , because otherwise the series wouldn't converge as $\sum \frac{\epsilon}{n}$ doesn't converge, but unfortunately just because $a_n$ is deceasing, that doesn't mean $n a_n$ is also decreasing so I was stuck, because basically the product can go lower than $\epsilon$ and then go back over","['limits', 'convergence-divergence', 'sequences-and-series']"
4554540,Find the smallest possible value for $x^2-3x+2y^2+4y+2$,"Find the smallest possible value for $x^2-3x+2y^2+4y+2$ . I know this: $x^2-3x+2y^2+4y+2=(x-\frac{3}{2})^2+2(y+1)^2-\frac{9}{4}$ so the smallest value is $-\frac{9}{4}$ . Now we also know this: $x^2-3x+2y^2+4y+2=(x-1)^2-1-x+2(y+1)^2=(*)(x-1)^2+2(y+1)^2-(1+x)$ My question is: why we cannot say that the smallest value possible is $-(1+x)$ when $(x-1)^2=0$ (i.e. when $x=1$ ) and $(y+1)^2=0$ , and because $x=1$ so the smallest value is $-2$ ?","['calculus', 'algebra-precalculus', 'analysis', 'real-analysis']"
4554545,Properties and Applications of Probability Distributions,"I am an MBA Student taking courses in Statistics. Our prof gave us a data manipulation assignment in which we have a column that contains a single weather measurement over a period of 100 days. For example, the weather can be either Sunny, Rainy, Cloudy, Snowy or Windy. Based on this data, we have to calculate the probability of ""two consecutive days of any weather combination"". For example, what is the probability of a Sunny day followed by another Sunny day - or what is the probability of a Cloudy day followed by a Windy day? As such, I think there are 25 possible combinations - I made a 5 x 5 contingency table in which each entry consists of a possible weather combination. Using the data, I calculated the probability of each possible weather combination and populated the entries within the table. As a logical check, I ensured that the sum of probabilities in any given row always add up to 1. Although this was all the assignment was asking us for, I thought of the following idea: What if these actually aren't the ""true"" probabilities? By this I mean, what if this weather over these 100 days was unusual - is it possible to place a ""range"" on these probabilities ? For example, the probability of two consecutive Sunny days is 0.13, but in reality this probability could be anywhere between 0.11 and 0.15. Or, given that today is Snowy, there is a 0.3 ± 0.05 probability that it will be Sunny tomorrow This way, given the current weather, I could compare the probabilities and ranges for all possible weather combinations and find out the most likely weather for tomorrow. I spent the day thinking about this question, and it sounds that maybe a ""confidence interval"" might be the answer I am looking for. Now, the question becomes how to calculate the confidence interval for these 25 probabilities. I kept thinking about this question and thought that this question is like rolling a 5 sided dice. We learned that flipping a two sided coin is a Binomial Probability Distribution whereas anything with more than two sides is a Multinomial Probability Distribution ( https://en.wikipedia.org/wiki/Multinomial_distribution ). I know that the general formula of a 95% confidence interval (based on the standard deviation of the Binomial Distribution ) for a proportion can be given by : Estimated Proportion ± 1.96 sqrt[ (Estimated Proportion * (1 - Estimated Proportion)) / sqrt(n) ]. In the case of the Multinomial Probability Distribution - could I say that the 95% confidence interval: Estimated Proportion ± sqrt[n * Estimated Proportion * (1-Estimated Proportion)]? In the above formula, ""n"" would be the total number of points (i.e. 100) and I would repeat this formula for each of the 25 probabilities. This formula is based on the Standard Deviation of the Multinomial Distribution. Is my understanding of this correct?","['statistics', 'probability']"
4554562,How can I solve this second order ODE with Dirac delta,"I need to solve the following ODE (a steady state Fokker-Planck/Kolmogorov Forward equation) \begin{equation}
   \dfrac{\partial^2}{\partial x^2}\left[ax^2f(x)\right] - \dfrac{\partial}{\partial x}\left[bxf(x)\right] - c f(x) + c\delta(x-\hat{x}) = 0
\end{equation} where $x\in(0,\infty)$ and where $f(x)\rightarrow 0$ as $x \rightarrow \infty$ . The constants $a,b,c$ are all positive and $\hat{x}$ is positive and far away from zero (far enough such that the solution at $x=0$ should be $f(0)=0$ ). The solution ignoring the Dirac impulse $\delta(x-\hat{x})$ (the solutions where $x \neq \hat{x}$ ) and imposing that $\lim_{x \rightarrow \infty}f(x) = \lim_{x \rightarrow 0}f(x) = 0$ is given by $$
f(x) =
\begin{cases}
c_1 x^{\lambda_1} \quad \text{if}  \ x<\hat{x}\\
c_2 x^{\lambda_2} \quad \text{if}  \ x>\hat{x}.
\end{cases}
$$ This is found by guessing the solution has the shape $Ax^\lambda$ and plugging such guess to the ODE shown above. Doing this we can verify that the solution satisfies the ODE and gives us the expression for $\lambda$ . $$ \lambda = \dfrac{b-3a \pm \sqrt{(b-a)^2+4ac}}{2a} $$ Since I have a Dirac impulse at $x=\hat{x}$ , I should be solving for two ODEs, one below $x=\hat{x}$ and another above $x=\hat{x}$ . I then have to glue together both solutions at $x=\hat{x}$ . This tells us that $$c_1 = c_2 \hat{x}^{\lambda_2-\lambda_1}.$$ Finally, as we have a Dirac delta at $x=\hat{x}$ , we get yet another boundary condition by integrating the ODE around $\hat{x}$ . $$ \lim_{\epsilon \rightarrow 0}\int_{\hat{x}-\epsilon}^{\hat{x}+\epsilon} \left(\dfrac{\partial^2}{\partial x^2}\left[ax^2f(x)\right] - \dfrac{\partial}{\partial x}\left[bxf(x)\right] - c f(x) + c\delta(x-\hat{x}) \right) \text{d}x = 0$$ When we do this, the second and third terms of the ODE drop out (by continuity these two terms must be equal to zero when we integrate them around $\hat{x}$ ). Thus we end up with the next condition. $\dfrac{c}{a} + c_2\lambda_2\hat{x}^{\lambda_2+1} = c_1\lambda_1\hat{x}^{\lambda_1+1}$ Putting everything together we get the coefficients $c_1,c_2$ $$ c_1 = \dfrac{c}{a}\dfrac{\hat{x}^{-(1+\lambda_1)}}{(\lambda_1-\lambda_2)}, \ c_2 = \dfrac{c}{a}\dfrac{\hat{x}^{-(1+\lambda_2)}}{(\lambda_1-\lambda_2)},$$ and we use the $\lambda$ that is a negative root for $x > \hat{x}$ , (and viceversa for $x<\hat{x}$ ). The steps I followed to tackle this question are based on two previous questions I asked here. One case did not have diffusion but had convection (just a first order ODE - link here ), whereas the other case had diffusion but no convection (second order ODE without the first derivative term - link here ). This case now has diffusion and convection. The problem I have is that 1) The solution does not match comparisons with numerical results which I know are correct. 2) The solution does not integrate to 1, that is $\int f(x)\text{d}x \neq 1$ . I tried to impose this as a boundary condition but once again my results do not make much sense. Where did I make a mistake, or how would you solve this problem? A bit more on the intuition behind the problem. The ODE in question is a steady state Fokker-Planck (or Kolmogorov Forward) Equation. Mass is injected at $x=\hat{x}$ and dissipates both to the left and right of $x=\hat{x}$ because of diffusion. Diffusion is controled by $a$ . The result should display a density with mass skewed to the right of $\hat{x}$ since $b>0$ . $b$ controls convection. Then, mass anywhere in $x\in(0,\infty)$ is taken out at a rate $c$ and reinjected back to $x=\hat{x}$ .","['dirac-delta', 'ordinary-differential-equations', 'probability-distributions', 'stochastic-processes', 'partial-differential-equations']"
4554568,What is the fractal dimension of the image given by a combinatorial sequence about permutation cycles?,"OEIS sequence A186759 is a triangle read by rows: $T(n,k)$ is the number of permutations of $\{1,2,\dots,n\}$ having $k$ nonincreasing cycles or fixed points, where a cycle $(b_1\ b_2\ \cdots\ b_m)$ is said to be increasing if, when written with its smallest element in the first position, it satisfies $b_1 < b_2 < b_3 < \cdots < b_m$ . According to the OEIS, this sequence is given by the exponential generating function $$
  G(t,z)=\frac{\exp[(1-t)(e^z-1-z)]}{(1-z)^t}.
$$ The sequence begins $$
1 \\
0\hspace{0.5cm}1 \\
1\hspace{0.5cm}0\hspace{0.5cm}1 \\
1\hspace{0.5cm}4\hspace{0.5cm}0\hspace{0.5cm}1\\
4\hspace{0.5cm}9\hspace{0.5cm}10\hspace{0.5cm}0\hspace{0.5cm}1\\
11\hspace{0.4cm}53\hspace{0.4cm}35\hspace{0.4cm}20\hspace{0.4cm}0\hspace{0.5cm}1\\
\cdots
$$ I learned about this sequence when my Twitter bot @oeisTriangles tweeted out a picture of its ""parity triangle"" earlier this month. This Twitter bot works by drawing a light color when the value at a position is even and a dark color when it is odd. The resulting image, shown below, appears to be a fractal. (In this image, I drew the first 1024 rows.) What is the fractal dimension of this parity triangle in the limit? Or, if this is too hard to determine, what are some upper/lower bounds?","['permutation-cycles', 'oeis', 'combinatorics', 'recreational-mathematics', 'fractals']"
4554605,How to apply Fubini's theorem here?,"Given $f:\mathbb{R}^2\times \mathbb{R}^2 \to \mathbb{R}$ , my classmate and I are discussing how to apply Fubini's theorem in the next integral with $x=(x_1,x_2), y=(y_1,y_2)$ $$\int_{x\in C}\int_{y\in C-x}{f(x,y)}dydx,$$ where , $$C=\{x=(x_1,x_2)\in \mathbb{R}^2:2<x_1^2+x_2^2<3\}$$ and $$ C-x=\{y-x:y\in C\}.$$ Our first attempt was $$\int_{x\in C}\int_{y\in C-x}{f(x,y)}dydx=\int_{y\in C}\int_{x\in C+y}{f(x,y)}dxdy,$$ we discarded this option by means of the simpler 1D example: $$\int_{x=-1}^{x=1}\int_{y= -1-x}^{y=1-x}f(x,y)dydx\neq\int_{y=-1}^{y=1}\int_{x= -1-y}^{x=1-y}{f(x,y)}dxdy.$$ Does anyone have any hint that can help us?","['integration', 'multivariable-calculus', 'lebesgue-integral', 'real-analysis']"
4554623,limiting probability inequality,"Let $\{a_i(x)\}_{i=1}^n$ and $\{b_i(x)\}_{i=1}^n$ , with $x\in R^m$ be i.i.d random vectors, such that $$
\lim_{x\to \infty}a_i\to N(0,1), \textit{and} \lim_{x\to \infty}b_i\to N(0,1), \quad i=1, \ldots, n.\quad (1)
$$ Let $\Phi(\alpha)$ denote a standard normal distribution function, where $\alpha$ denotes the significance level. I am trying to find bound of $$
\lim_{n,x\to \infty}P(max\{\{a_i(x)\}_{i=1}^n, \{b_i(x)\}_{i=1}^n\}> \frac{1}{\Phi})
$$ So far, I arrived to $$
P(max\{\{a_i(x)\}_{i=1}^n, \{b_i(x)\}_{i=1}^n\leq \frac{1}{\Phi}\}=\prod_{i=1}^nP(a_i\leq\frac{1}{\Phi})P(b_i\leq\frac{1}{\Phi}).
$$ But now I am stuck, as condition (1) says that each $a_i$ and $b_i$ converges to $N(0,1)$ and I have $1/\Phi$ in the inequality...","['inequality', 'probability-distributions', 'probability-theory', 'probability']"
4554641,Why is $E[X^{2}] \ge E[X]^{2}$?,"I was trying to prove that $E[X^{2}] \ge E[X]^{2} \quad \mathbf{(0)}$ . Where $E[X]$ is the expected value of a random variable, $X$ , i.e., $E[X] = \sum_{x}x p_{X}(x)$ Now, I'm not asking how to prove $\mathbf{(0)}$ , I know the method using variance etc. I'm asking is why is $\mathbf{(3)}$ true? (In other words, how to prove this continuing from $\mathbf{(3)}$ . $$\begin{align*}
E[X]^{2} \qquad & \longleftrightarrow \qquad ( \ \sum_{x}x \ p_{X}(x) \ )^{2} \\
& \longleftrightarrow \qquad ( \ x_{1} \ p_{X}(x_{1}) + x_{2} \ p_{X}(x_{2}) + \dots + x_{n} \ p_{X}(x_{n}) \ )^{2} \\
& \longleftrightarrow \qquad \ x_{1}^{2} \ p_{X}(x_{1})^{2} + x_{2}^{2} \ p_{X}(x_{2})^{2} + \dots + x_{n}^{2} \ p_{X}(x_{n})^{2} + \dots + 2( \ x_{i}x_{j} \ p_{X}(x_{i}) \ p_{X}(x_{j}) ) + \dots \quad \mathbf{(1)} \\ 
\textrm{And likewise, } \\\\
E[X^{2}] \qquad & \longleftrightarrow \qquad \sum_{x}x^{2} \ p_{X}(x) \\
& \longleftrightarrow \qquad x_{1}^{2} \ p_{X}(x_{1}) + x_{2}^{2} \ p_{X}(x_{2})+ \dots + x_{n}^{2} \ p_{X}(x_{n}) \quad \mathbf{(2)}\\
\textrm{Now, why is:  } \\
\end{align*} \\
x_{1}^{2} \ p_{X}(x_{1}) + x_{2}^{2} \ p_{X}(x_{2})+ \dots + x_{n}^{2} \ p_{X}(x_{n}) \ge \ x_{1}^{2} \ p_{X}(x_{1})^{2} + x_{2}^{2} \ p_{X}(x_{2})^{2} + \dots + x_{n}^{2} \ p_{X}(x_{n})^{2} + \dots + 2( \ x_{i}x_{j} \ p_{X}(x_{i}) \ p_{X}(x_{j}) ) + \dots \qquad \mathbf{(3)}\\
$$ $ \ \ \quad$ i.e., why is, $\mathbf{(1)} \ge \mathbf{(2)}$","['statistics', 'summation', 'proof-explanation', 'expected-value', 'probability']"
4554789,For what $m$ does $\iint_{\mathbb{R^2}}\frac{1}{(1+x^2+y^2)^m}dxdy$ converge or diverge?,"For what $m$ does $\iint_{\mathbb{R^2}}\frac{1}{(1+x^2+y^2)^m}dxdy$ converge or diverge? Attempt: We write the double integral as $$\iint_{\mathbb{R^2}}\frac{1}{(1+x^2+y^2)^m}dxdy=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} \frac{1}{(1+x^2+y^2)^m}dxdy$$ We note that out integrand is symmetric, thus $$\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} \frac{1}{(1+x^2+y^2)^m}dxdy=4\int_{0}^{\infty}\int_{0}^{\infty} \frac{1}{(1+x^2+y^2)^m}dxdy$$ We now establish polar coordinates $x=r\cos{\theta}$ , $y=r\sin{\theta}$ . Then (this is where I'm slighly unsure), we have integration bounds $0\leq\theta\leq2\pi$ , $0\leq r \leq \infty$ . Recalling that the jacobian of this change of variables is simply $r$ , we are now considering the integral $$4\int_{0}^{\infty}\int_{0}^{2\pi} \frac{r}{(1+r^2)^m}d\theta dr=8\pi\int_{0}^{\infty} \frac{r}{(1+r^2)^m} dr$$ This is trivially integrated using $u=1+r^2$ , which yields $$8\pi\int_{0}^{\infty} \frac{r}{(1+r^2)^m} dr=8\pi \frac{(1+r^2)^{1-m}}{1-m} \Big|_0^\infty$$ Then it should stand to reason that the integral diverges for $m\leq1 $ and converges for $m>1$ . There was no answer provided in the exercise, so I used some python code to check, and it appears to converge for $m=0$ , but otherwise seems to agree with my result (however code can be unshaky at times, so I turned to you guys). Input would be much appriciated!","['multivariable-calculus', 'calculus', 'convergence-divergence']"
4554793,Hyper-extensions of Hom space,"We fix an ultrafilter $\mathcal{F}$ of $\mathbb{N}$ which contains the cofinite filter.
Let $A,B$ be sets and ${}^{*}A,{}^{*}B$ their hyper-extensions. Then is $$
{\rm Hom}({}^{*}A,{}^{*}B)
$$ equal to the hyper extension of ${\rm Hom}(A,B)$ ?
Let $f=[f_{m}]_{m\in \mathbb{N}}$ be an element of ${}^{*}{\rm Hom}(A,B)$ . Then for any $a=[a_{m}]_{m\in \mathbb{N}}$ , we define $f(a)$ by $[f_{m}(a_{m})]_{m\in \mathbb{N}}\in {}^{*}B$ . Perhaps this is well-defined, so we can consider a natural map $$
{}^{*}{\rm Hom}(A,B)\longrightarrow {\rm Hom}({}^{*}A,{}^{*}B).
$$ My question is whether or not this natural map is bijective. If not, is this injective or surjective or neither?","['surreal-numbers', 'model-theory', 'nonstandard-models', 'elementary-set-theory', 'set-theory']"
4554798,Do we have $\int_M \mathcal L_X\omega=0$ for $m$-form $\omega$?,"Let $M$ be an oriented $m$ -manifold without boundary, $X\in \mathfrak X(M)$ be a vector field, and $\omega \in \Omega^m_c(M)$ be a compactly supported top-degree differential form. Question : Do we always have $$\int_M \mathcal L_X\omega=0,$$ where $\mathcal L_X$ is the Lie derivative? Motivation : The integration operation has a diffeomorphism invariance, i.e., if $F:M\to M$ is an orientation-preserving diffeomorphism, then $$\int_M \omega = \int_M F^*\omega.$$ Now, suppose that the diffeomorphism $F$ is infinitesimal, i.e., $F$ maps each $p\in M$ to a point that is infinitesimally separated from $p$ . Such infinitesimal diffeomorphism can be heuristically represented by a vector field $X\in \mathfrak X(M)$ , which is an 'arrow' from $p$ to $F(p)$ . Furthermore, the infinitesimal version of the pullback $F^*\omega$ is the Lie derivative $\mathcal L_X\omega$ . Hence, I suspect that the infinitesimal statement of the diffeomorphism invariance of integral should be $$\int_M \mathcal L_X\omega=0.$$","['lie-derivative', 'differential-geometry']"
4554816,Definite integral with a strange power,"I've been struggling for a while now on evaluating this disgusting integral: $$(\ln2)\int_0^{(\ln2)^{1/\ln2}}2^{\ln x}\cdot x^\left(\frac{x^{\ln2}+1}{\ln x}-1\right)dx$$ My maths teacher gave our class this question a while ago, and he said that we should be able to do it (I am in high-school, and we have only been taught a fairly basic level of integration).
So today I spent many hours applying every integration technique I know to this monster, but I got absolutely nowhere. It got to a point where I couldn't think of another variable to use as a substitution because I had already made so many. I eventually decided to plug this into an integral calculator and received a surprisingly nice result of $e$ , however there was no further information and so I was not able to view any of the steps in how they got there. I am so stuck on this problem :( Does anyone know how they got there? What are the steps in finding its indefinite form?","['integration', 'exponentiation', 'calculus', 'definite-integrals']"
4554819,Difficulty in proving $i_{\alpha} (\vartheta \wedge \zeta) = i_{\alpha} \vartheta \wedge \zeta + (-1)^k \vartheta \wedge i_{\alpha} \zeta.$,"Let $M$ be a smooth manifold. Let $\mathcal \vartheta \in \mathfrak X^k (M)$ be a $k$ -vector field and $\alpha \in \Omega^1 (M)$ be a $1$ -form on $M.$ Then we define $i_{\alpha} \vartheta \in \mathfrak X^{k-1} (M)$ (the interior product of $\vartheta$ by $\alpha$ ) as follows $:$ $$i_{\alpha} \vartheta\ (\alpha_1, \cdots, \alpha_{k-1}) : = \vartheta\ (\alpha, \alpha_1, \cdots, \alpha_{k-1})$$ where $\alpha_1, \cdots, \alpha_{k - 1} \in \Omega^1 (M).$ With this definition in mind we have to show that given $\vartheta \in \mathfrak X^k (M)$ and $\zeta \in \mathfrak X^s (M)$ the following equality holds $:$ $$i_{\alpha} (\vartheta \wedge  \zeta) = i_{\alpha} \vartheta \wedge \zeta + (-1)^k \vartheta \wedge i_{\alpha} \zeta.$$ Which I could not able to prove. Any help would be greatly appreciated. Thanks for your time.","['differential-geometry', 'vector-fields', 'smooth-manifolds', 'differential-forms', 'exterior-algebra']"
4554840,"Creating problems of form ""$a_1l_1^n+\ldots + a_k l_k^n$ is always divisible by $2023$""","Say you're writing home assignments for future freshman-year students of next year. Teaching about induction, question such as ""prove that $2^{n+1} + 5^n$ is divisible by $3$ for every natural $n$ "" appear frequently. Are there any interesting ways of creating such problems, easily proven by induction, but with the given expression being divisible by $2023$ ? Say we wish to concentrate on problems of form "" $f(n) = a_1l_1^n+\ldots + a_k l_k^n$ is always divisible by $m=2023$ , for each $n \in \Bbb{N}$ "" for natural numbers $l_1 < \ldots < l_k$ and some integers $a_1 ,\ldots , a_k$ . Let's assume we were able to choose them in a way s.t. $f(0) = 0$ (as an induction base). The inductive step, assuming $f(n)$ is divisible by 2023, is of form $$
\begin{align}
f(n+1)   &=  a_1 l_1 l_1^n + a_2 l_2 l_2^n + \ldots + a_k l_k l_k^n \\
         &=  l_1 f(n) + a_2 (l_2-l_1) l_2^n + \ldots + a_k (l_k-l_1) l_k^n \\
         &\equiv a_2 (l_2-l_1) l_2^n + \ldots + a_k (l_k-l_1) l_k^n  ~\operatorname{mod} (2023).
\end{align}
$$ This reduces the problem in ""one step"", into a problem with $2(n-1)$ coefficients to choose. As $2023=17\cdot 17\cdot 7 = 119 \cdot 7$ , by taking $k=2, a_2 = 119$ and $l_2 - l_1 =17$ , we can make the expression above divisible by $2023$ . This gives, for example, the sequence $f(n) = 119\cdot {22}^{n+1}-2618\cdot 5^n$ . Are there more elegant ways of creating such sequences? Or, for example, sequences of form $a\cdot p^n + b \cdot n + c$ that are divisible by $2023$ ?","['number-theory', 'elementary-number-theory']"
4554849,How do I prove that the distance between two points is longer than one point in the normal and the other point in a plane?,"So, imagine $P$ is a plane with the normal $n$ , and $A = (a1, a2, a3)$ is a point above the plane $P$ that does not lie on the plane. $B$ is a point collinear on the line $L$ : $r = (a_1, a_2, a_3) + \lambda n$ . $C$ is a point on $P$ other than $B$ . How do I prove $\|A-C \| > \|A-B \|$ for all points $C \ne B$ ? I tried drawing a diagram with points $A$ and $B$ in a line $L$ parallel to the normal, since the shortest distance from $A$ to $P$ is $L$ perpendicular. I drew a line from $B$ to $C$ . Since the $L$ is parallel to the normal $n$ , $L$ is perpendicular to the plane $P$ . Hence, this would be a right-angled triangle $ABC$ . Now, a hypothenuse is longer than the other two sides. What's the best way to approach this question?","['geometry', 'plane-geometry']"
4554864,Another completion of $\overline{\Bbb{Q}}$,"$\overline{\Bbb{Q}}$ is the field of algebraic numbers, let $E$ be the set of embeddings $\overline{\Bbb{Q}}\to \Bbb{C}$ and consider the following norm on $\overline{\Bbb{Q}}$ $$\|\alpha\|=\sup_{\sigma\in E} |\sigma(\alpha)|$$ making it a topological field. The completion for that norm is a ring containing $\overline{\Bbb{Q}}\otimes_\Bbb{Q}\Bbb{R}$ (including some zero divisors such as $(1\otimes \sqrt2)^2=(\sqrt2\otimes 1)^2$ )
as well as things such as $$\sum_{n\ge 1} 2^{-n} \sqrt{n}$$ Is there a simple description of this completion?","['field-theory', 'complete-spaces', 'abstract-algebra']"
4554871,How to solve this functional integral?,"Here is a little integral I made, but I think my solving steps could be flawed: $$\int{f'(x)\cdot f(x)^{\left(\frac{f(x)}{\ln{f(x)}}\right)}dx}$$ for $f(x)>1$ . My solution: using $a^{\frac{1}{\ln(a)}}=e$ , $f(x)^{\left(\frac{f(x)}{\ln{f(x)}}\right)}=e^{f(x)}$ , since $f(x)>1$ (This is where I think I'm wrong) Therefore our integral becomes: $$\int{f'(x)\cdot e^{f(x)}dx}$$ $=e^{f(x)}+C$ Leaving us with a fairly nice answer. However, as mentioned before I feel as if there is something wrong with my first step. Im thinking that you aren't able to use that property with entire functions instead of just constants, right? Is it actually correct? If not, why?","['integration', 'indefinite-integrals', 'calculus', 'functions']"
4554901,Glivenko-Cantelli for $k$-points,"Let $\mu$ be a probability measure on $\mathbb{R}$ . Let $f$ be a measurable function on $\mathbb{R}$ . Let $(X_i)_{i \in \mathbb{N}}$ be a sequence of random iid variables, all of law $\mu$ . Then Glivenko-Cantelli's theorem allows us to derive the convergence in law $\frac{1}{n} \sum^n_{i=1} \delta_{f(X_i)} \stackrel{n \to \infty}{\rightarrow} f_* \mu$ . Do we have a $2$ -point Glivenko-Cantelli's theorem? That is, let $f$ be a measurable function on $\mathbb{R}\times \mathbb{R}$ . Do we have the convergence in law $\frac{1}{n(n-1)} \sum_{\substack{i,j \in \{1,\cdots,n\}\\ i\neq j}} \delta_{f(X_i,X_j)} \stackrel{n \to \infty}{\rightarrow} f_* (\mu \otimes \mu)$ ? And a $k$ -point Glivenko-Cantelli's theorem? I don't know if it is possible to prove $2$ -point Glivenko-Cantelli from the usual one, since the family $(X_i,X_j)_{\substack{i,j \in \mathbb{N}\\ i\neq j}}$ is not independent. But still, most of the pairs of members of the family are independent pairs...","['statistics', 'probability-limit-theorems', 'probability-theory']"
4554903,Reference request for certain PDE appearing in a probability application,"In working on a stochastic processes application, I derived a PDE for the joint transform of some process. It takes the form $$\frac{\partial \zeta(t,z,s)}{\partial t}-(\beta s+1)\frac{\partial\zeta(t,z,s)}{\partial s}=\frac{z}{s}\left(\zeta(t,z,0)-\zeta(t,z,s)\right),$$ where we have boundary conditions $\zeta(0,z,s)=e^{-s}$ , $\zeta(t,0,s)=0$ and $\zeta(t,1,0)=1$ . Here $\beta>0$ is just some scalar. If convenient, I'm fine with setting it to unity. I don't have much experience with PDE, and I'm a bit stuck in this problem because I either want to solve this PDE or I'd like to calculate expressions like $\frac{\partial \zeta(t,z,s)}{\partial z}$ evaluated in $(t,1,0)$ for arbitrary $t>0$ . In this context, it may be helpful to use the expression $\zeta(t,z,s)=\mathbb Ez^{N(t)}e^{-s\Lambda(t)}$ from my probability application. Rewriting the PDE and using this expression gives us ODEs for moments of $N(t),\Lambda(t)$ , but only expressed in higher moments, making a recursive solution inpossible. Therefore I'm looking for something different. I was hoping this PDE belongs to some well-known class, or for some analytical method to solve this PDE; I'm looking for some reference. Any help is much appreciated.","['stochastic-processes', 'reference-request', 'probability', 'partial-differential-equations']"
4554982,Question about whether a series converges almost surely,"We are given that $\{X_n\}_{n\in\mathbb N}$ are iid $\text{Bernoulli}(1/2)$ . Then let $Y_n=X_n/n^{\theta},\ \theta>0$ . The question is to say whether $S_n=\sum_{i=1}^nY_i$ converges almost surely or not. Towards a solution, I note that if $\theta>1$ , then since the sum is dominated by $\sum_{i=1}^n 1/i^\theta$ which has a finite limit, $S_n$ also converges, and hence in particular, converges almost surely. But for the $0<\theta\le 1$ case, I am stuck. No bound I am considering is being strong enough to point me towards a contradiction (or towards a affirmative answer, which I have a hunch is not the case, but I might be wrong). Any help is appreciated!","['bernoulli-distribution', 'almost-everywhere', 'sequences-and-series', 'probability-theory', 'probability']"
4554985,On a problem of sum of $n$ squares (Generalized IMO 1988 P6),"Suppose $a_1, a_2, \cdots, a_n$ are positive integers, prove that if $a_1^2+a_2^2+\cdots+a_n^2$ is divisible by $a_1a_2\cdots a_n+1$ , then there exists nonnegative integers $x_1, x_2, \cdots, x_{n-1}$ such that $$\frac{a_1^2+a_2^2+\cdots+a_n^2}{a_1a_2\cdots a_n+1}=x_1^2+x_2^2+\cdots+x_{n-1}^2.$$ Here is my approach. $n=1$ is trivial, and $n=2$ is already well-known $(1988/6)$ . By Lagrange's four square theorem, $n\geqslant 5$ are also obvious. What's left is $n=3$ and $4$ . For $n=3$ , by the sum of two squares theorem, if I use the standard method of Vieta jumping, setting the quotient of $a_1^2+a_2^2+\cdots+a_n^2$ and $a_1a_2\cdots a_n+1$ as $k$ , then we now have to prove there must not exist a prime $p$ s.t. $p^\alpha \,|\,k$ and $4\,|\,p-1$ . I think achieving this result is much harder than the $n=2$ case because I may need to investigate the prime decomposition of $k$ , which Vieta jumping might not be that helpful. For $n=4$ , by the sum of three squares theorem, we have to prove that $k$ can be expressed as the form $k=4^\alpha(8s+7)$ . Both of these cases are difficult to me and I haven't come up with a good way to approach this problem. Partial Solution (Very Little! Partial Solution for $n=3$ by Vieta Jumping) Let $(a,b,c)$ be the minimal solution with $a\geqslant b\geqslant c\geqslant 1$ . View it as a quadratic equation of $a$ , then by Vieta's formula, we know that there exists $a'$ s.t. $a+a'=kbc$ , $aa'=b^2+c^2-k$ , so $$a'=\frac{b^2+c^2-k}{a}\leqslant 2a-\frac k a=2a-\frac{a+a'}{abc},$$ which is $$\left(1+\frac 1 {abc}\right)a'\leqslant 2a-\frac 1{bc}.$$ Therefore, if $a'>0$ $$a\leqslant a'\leqslant \frac{2abc-1}{bc}\,\frac{abc}{abc+1}=2a-\frac{3a}{abc+1}\leqslant 2a-\frac 3 {bc+1}.$$ We get $3a\leqslant a(abc+1)$ , so $abc\geqslant 2$ , which implies $a\geqslant 2$ .
Let $f(x)=x^2-kbcx+\left(b^2+c^2-k\right)$ , then if $a=b$ , $f(b)=0$ . By $f(c)\geqslant 0$ , we have $$b^2+2c^2-kbc^2-k-\left(2b^2+c^2-kb^2c-k\right)\geqslant 0 \implies kbc\geqslant b+c.$$ Hope I kinda want to prove a strict inequality $a>b>c$ . If I proved all the solutions can be generated by a trivial solution $(x_1, x_2, \cdots, x_n)=(0, \square , \square, \cdots, \square)$ , then we're done. Generalizations Will the generalizations help? If we change the $1$ to $C$ , i.e. $a_1a_2\cdots a_n+C\,|\,a_1^2+a_2^2+\cdots+a_n^2$ , can we still make a similar conclusion? $$\frac{a_1^2+a_2^2+\cdots+a_n^2}{a_1a_2\cdots a_n+C}=\;???$$ This also reminds me of the Hurwitz equations , which states that if $C\in \mathbb N$ , and $x_1, x_2, \cdots, x_n$ where $n>1$ , then equation $$x_1^2+x_2^2+\cdots +x_m^2=Cx_1x_2\cdots x_n$$ has at least one solution if and only if $C\leqslant n$ . Here, $C$ is not necessarily a square or a sum of a few squares (e.g. when $n=3$ , $C=1,3$ , $C=3$ is the Markov equation ). Edit (October 21, 2022) I know how to solve the $n=3$ case now (strengthening to $k=\alpha^2+\beta^2$ where $\alpha, \beta \in \mathbb N$ ), but not the $n=4$ case. The proof doesn't incorporates the sum of two squares theorem. Proof. Suppose $(a_0, b_0, c_0)$ is the minimal solution of the equation $$\frac{a^2+b^2+c^2}{abc+1}=k$$ where $k$ is an positive integer and $a_0\geqslant b_0\geqslant c_0\geqslant 1$ . By Vieta's formulas we know that there exists $a'\in\mathbb Z$ s.t. $a_0+a'=kb_0c_0$ and $a_0a'=b_0^2+c_0^2-k$ . Case 1: $a'<0$ . Then $a'b_0c_0+1\leqslant 0 \implies k(a'b_0c_0+1)\leqslant0 <a'^2+b_0^2+c_0^2$ , which is a contradiction. Case 2: $a'=0$ . $k=b_0^2+c_0^2$ . Correct. Case 3: $a'>0$ . By the minimality of $a_0$ , we must have $a'\geqslant a_0\geqslant b_0\geqslant c_0\geqslant1$ , so $$a'b_0\leqslant a_0a'=b_0^2+c_0^2-k\leqslant 2b_0^2-k<2b_0^2 \implies a'<2b_0\,,$$ $\implies 4b_0=2b_0+2b_0>a_0+a'=kb_0c_0$ $\implies 4>kc_0$ . If $k$ cannot be represented as a sum of two squares $k$ must be $3$ , and $c_0=1$ . Then, $b^2\leqslant a_0a'=b_0^2+c_0^2-k=b_0^2-2$ , which is also a contradiction. I don't know if a geometric perspective, or some advanced number theory techniques would help.","['number-theory', 'quadratics', 'elementary-number-theory']"
4555014,Blow-up of a Pencil of Cubic Curves (Miranda's basic theory of elliptic surfaces),"In Rick Miranda's ""The basic theory of elliptic surfaces"" the Example (I.5.1) see page 7 on a pencil of plane curves contains an argument Inot understand yet: Let $C_1$ be a smooth cubic curve in $\mathbb{P^2}$ and let $C_2$ be any other cubic. By intersection theory and Bezout's theorem the intersection number $C_1 \cdot C_2$ is $9$ . We form a pencil $P \subset \mathbb{P^2} $ generated by $C_1$ and $C_2$ ; in english that is a $\mathbb{P^1}$ -family of curves (or more generally divisors) $[ \lambda C_1 + \mu C_2 ]$ , which has $9$ base points $x_1,..., x_9$ . This gives only a rational map to $\mathbb{P^1}$ . After blowing them up the fundamental locus of this rational map is resolved and we obtain a honest morphism $\pi: X \to \mathbb{P^1}$ where $X= Blowup(P)_{x_1,..., x_9}$ is the blowup of the pencil $P$ in these $ 9 $ points. Then it is claimed that the canonical class of $X$ is $-C_1$ and in particular that this implies that $K_X^2= (-C_1)^2 =0$ . How to check that the canonical class of $X$ equals $-C_1$ and why does it have as consequence self-intersection number zero? The divisor $-C_1$ is definietely not vertical and therefore I not see why it's interesection with itself should vanish.","['algebraic-geometry', 'intersection-theory', 'blowup', 'elliptic-curves']"
4555055,Can One Discuss Induction without Sets?,"The standard presentation of mathematical induction involves subsets having a certain property. Here is a typical formulation from Gallian's Contemporary Abstract Algebra , Ninth edition: It seems to me, then, that set theory is more fundamental than induction in this formulation. One works with sets as axiomatized by ZFC or NBG or something else, and in particular with the set of integers and its subsets. On the other hand, construction of the natural numbers seems not to require sets at all. For example, many sources including Wikipedia specify the Peano axioms without referencing sets. It therefore seems feasible to specify the procedure of mathematical induction in a way which does not reference sets at all. Can you provide a source which does this precisely?","['foundations', 'integers', 'natural-numbers', 'elementary-set-theory', 'induction']"
4555104,Solve $x^2y' + xy + 1 = 0$ first order differential equation. Where is my mistake?,"Solve $x^2y' + xy + 1 = 0$ first order differential equation. The answer from the textbook is $xy = C - \ln |x|$ . Here is my long (but wrong) solution and I can't see the mistake. First I rewrite it as $y' + \dfrac{y}{x} = -\dfrac{1}{x^2}$ , this is a first order linear differential equation. Let's consider $y' = -\dfrac{y}{x}$ . From here $\dfrac{dy}{y} = -\dfrac{dx}{x}$ and $\ln y = - \ln x + C$ and $y = Ce^{\frac{1}{x}}$ . Now for the general solution assume $y = C(x)e^{\frac{1}{x}}$ . Then $C'(x)e^{\frac{1}{x}} - \dfrac{C(x)e^{\frac{1}{x}}}{x^2} + \dfrac{C(x)e{\frac{1}{x}}}{x} = -\dfrac{1}{x^2}$ and $C'(x) - \dfrac{C(x)}{x^2} + \dfrac{C(x)}{x} = -\dfrac{1}{x^2e^{\frac{1}{x}}}$ . Let $u \equiv C(x)$ . We have $u' - u(\frac{1}{x} - \frac{1}{x^2}) = -\dfrac{1}{x^2e^{\frac{1}{x}}}$ This is another linear differential equation. Let's consider $\dfrac{du}{u} = dx(\dfrac{1}{x} - \dfrac{1}{x^2})$ . Integrating we have $\ln u = \ln x + \dfrac{1}{x} + C_1$ and $u = C_1xe^{\frac{1}{x}}$ . Now suppose $u = C_1(x)xe^{\frac{1}{x}}$ . Then $C_1'(x)xe^{\frac{1}{x}} + C_1(x)e^{\frac{1}{x}} - \dfrac{C_1(x)e^{\frac{1}{x}}}{x} - C_1(x)e^{\frac{1}{x}}(1 - \dfrac{1}{x}) = -\dfrac{1}{x^2e^{\frac{1}{x}}}$ and $C_1'(x)x + C_1(x) - \dfrac{C_1(x)}{x} - C_1(x)(1 - \dfrac{1}{x}) = -\dfrac{1}{x^2e^{\frac{2}{x}}}$ and finally $C_1'(x) = -\dfrac{1}{x^2e^{\frac{2}{x}}}$ . Also before passing to integration suppose $x = \dfrac{1}{\ln z}$ . Then $dx = \dfrac{dz}{z \ln^2 z}$ . So $$C_1(x) = - \int {\dfrac{dx}{x^2e^{\frac{2}{x}}}} = - \int{zdz} = -z^2/2 + C = -e^{\frac{2}{x}} + C$$ Then finally $y = C(x)e^{\frac{1}{x}} \equiv ue^{\frac{1}{x}} = C_1(x)xe^{\frac{2}{x}} =-e^{\frac{4}{x}}x + Cxe^{\frac{2}{x}}$ .","['integration', 'solution-verification', 'ordinary-differential-equations']"
4555115,Defining $A+B$ for self-adjoint operators $A$ and $B$?,"Consider the following observations: If $A$ and $B$ are bounded operators on a Banach space over $\mathbb{C}$ , then the Lie product formula implies that $$ e^{it(A+B)} = \lim_{n\to\infty} (e^{itA/n}e^{itB/n})^n. \tag{1} $$ This then allows to extract $A+B$ from the limit in the right-hand side: $$ A + B = \frac{1}{i}\left( \frac{\mathrm{d}}{\mathrm{d}t}\biggr|_{t=0} \right) \lim_{n\to\infty} (e^{itA/n}e^{itB/n})^n. \tag{2} $$ Now, let $A$ and $B$ be self-adjoint operators defined on dense subsets $\mathrm{Dom}(A)$ and $\mathrm{Dom}(B)$ of a Hilbert space $\mathcal{H}$ , respectively. Then a version of Trotter product formula tells that, if $\mathrm{Dom}(A) \cap \mathrm{Dom}(B)$ is dense and $A+B$ is essentially self-adjoint on $\mathrm{Dom}(A) \cap \mathrm{Dom}(B)$ , then the equality $\text{(1)}$ holds in the strong operator topology. These led me to the following question: Question. Is it possible that $A+B$ , as an operator defined on $\mathrm{Dom}(A) \cap \mathrm{Dom}(B)$ , is not essentially self-adjoint, but $A+B$ can still be defined as a self-adjoint operator via the formula $\text{(2)}$ ? If the answer is negative, does it mean that the RHS of $\text{(1)}$ diverges (or does not define a strongly continuous one-parameter unitary group) whenever $A+B$ is not essentially self-adjoint? The formula $\text{(1)}$ is less affected from the ""densely-definedness"" issue of self-adjoint operators. So I am curious whether this small advantage actually makes any differences and possibly opens up more rooms for defining $A+B$ or not. Unfortunately, I have only tangential knowledge on functional analysis and operator theory, which means that I don't have any good intuition regarding this question. I am fairly certain that this kind of question has been already examined by many people, so the fact that I can't seem to find any reference is likely because there are some serious obstructions in this approach (or I am too clumsy at googling 😔). Some Attempts. I tried this question with $\mathcal{H} = L^2[0, 1]$ , $A = -\frac{\mathrm{d}^2}{\mathrm{d}x^2}$ , and $B = -\frac{\mathrm{d}^2}{\mathrm{d}x^2}$ with the respective domains $$\mathrm{Dom}(A) = \{f \in H^2[0, 1] : f(0) = f(1) = 0\}$$ and $$\mathrm{Dom}(B) = \{f \in H^2[0, 1] : f'(0) = f'(1) = 0\}.$$ Then $A + B$ defined on the intersection of their respective domains is not essentially self-adjoint, since both $2A$ and $2B$ are self-adjoint extensions of $A+B$ . I tried simulating the RHS of $\text{(1)}$ numerically, and the results seem suggesting that the limit is convergent. However, I am not sure whether this is an artifact of the numerical scheme I adopted or not.","['operator-theory', 'functional-analysis', 'self-adjoint-operators']"
4555302,Error in an exponential/logarithmic limit calculation,"Below is my calculation, but the result is incorrect. It should be $21$ ( WolframAlpha ), but I get $2$ . My textbook explains a different way to get the correct answer, but I couldn't figure out why my calculation leads to an incorrect result. Which part did I get wrong? $$\lim_{x \to 0}\frac{2}{x} \ln \frac{e^x(e^{20x}-1)}{20(e^x-1)}$$ $$=\lim_{x \to 0}\frac{2}{x} \ln (e^x \times \frac{e^{20x}-1}{20x} \times \frac{x}{e^x-1})$$ $$=\lim_{x \to 0}\frac{2}{x} \ln (e^x \times 1 \times 1)$$ $$=\lim_{x \to 0}\frac{2}{x} \ln {e^x}=\lim_{x \to 0}\frac{2}{x} \times x=2$$","['logarithms', 'analysis', 'real-analysis', 'limits', 'exponential-function']"
4555352,Loewner order and rank-$1$ matrices,"The Loewner order is defined over the set of Hermitian matrices as $A \leq B$ if and only if $B - A$ is positive semidefinite. If $B$ is a rank- $1$ , positive semidefinite matrix, what are the matrices $A$ such that $0 \leq A \leq B$ ? Are there other solutions than $A = \lambda B$ for $0 \leq \lambda \leq 1$ ?","['positive-semidefinite', 'rank-1-matrices', 'matrices', 'linear-algebra', 'symmetric-matrices']"
4555384,"If $g∘f$ is a homemorphism, then $g$ one-one (or $f$ onto) implies that $f$ and $g$ are homeomorphisms.","I need a little explanation, please. Seymour Lipschutz - General Topology, Chapter 7, page 110: Let $X,Y,Z$ be topological spaces and let $f:X\longrightarrow Y$ and $g:Y\longrightarrow Z$ be continuous. Show that if $g\circ f:X\longrightarrow Z$ is a homemorphism, then $g$ one-one (or $f$ onto) implies that $f$ and $g$ are homeomorphisms. To show that two functions $f$ and $g$ are homeomorphisms we need to show that there exists an one-to-one correspondence, and $f,f^{-1},g,g^{-1}$ are continuous. As $f$ and $g$ are already continuous by hypothesis, and $g$ one-one (or $f$ onto), we only have to show that $g$ onto (or $f$ one-one), $f^{-1}$ , and $g^{-1}$ are continuous. Am I right?",['general-topology']
4555398,"""$\varepsilon$-regular pair"" vs. ""$\varepsilon$-homogeneous"" in Regularity Lemma","Definition ( $\varepsilon$ -regular pair) Let $G$ be a graph and $U,W\subseteq V(G)$ . We call $(U,W)$ an $\varepsilon$ - regular pair in $G$ if for all $A\subseteq U$ and $B\subseteq W$ with $|A|\geq \varepsilon |U|$ and $|B|\geq \varepsilon
 |W|$ , one has $$|d(A,B)-d(U,W)|\leq \varepsilon.$$ Definition (An alternate definition of regular pairs) Let $G$ be a graph and $X,Y\subseteq V(G)$ . Say that $(X,Y)$ is $\varepsilon$ - homogeneous if for all $A\subseteq X$ and $B\subseteq Y$ , one has $$|e(A,B)-|A||B|d(X,Y)|\leq \varepsilon|X||Y| \Leftrightarrow $$ $$|d(A,B)-d(X,Y)|\leq \varepsilon \frac{|X||Y|}{|A||B|}.$$ Show that if $(X,Y)$ is $\varepsilon$ -regular, then it is $\varepsilon$ -homogeneous. Show that if $(X,Y)$ is $\varepsilon^3$ -homogenous, then it is $\varepsilon$ -regular. Part 2 follows easily from definitions but part 1 does not seem so easy. I have some thoughts but they did not work out. We need to show that $(X,Y)$ is $\varepsilon$ -homogeneous, provided that $(X,Y)$ is $\varepsilon$ -regular. Let $A\subseteq X$ and $B\subseteq Y$ . I tried to consider the following cases:
i) If $|A|\geq \varepsilon |X|$ and $|B|\geq \varepsilon |Y|$ , then the result follows by $\varepsilon$ -regularity. But we can also have the following cases: ii) $|A|\geq \varepsilon |X|,\ |B|< \varepsilon |Y|$ ; iii) $|A|< \varepsilon |X|,\ |B|\geq \varepsilon |Y|$ ; iv) ii) $|A|< \varepsilon |X|,\ |B|< \varepsilon |Y|$ ; I belive that the remaning 3 cases should be identical to each other. I had some ideas for ii) but they did not work out: if $|B|<\varepsilon |Y|$ , then $|Y\setminus B|\geq (1-\varepsilon)|Y|$ and I tried to play with that but ... I'd be happy to see any ideas which lead to the solution! Thank you!","['graph-theory', 'combinatorics', 'discrete-mathematics']"
4555404,Why is the combination formula (without replacement) not just $\frac{n(n - 1)(n - 2) \cdots (n - r + 1)}{r!}$?,"Why is the combination formula (without replacement) not just $\frac{n(n - 1)(n - 2) \cdots (n - r + 1)}{r!}$ ? I tried many examples. Both the title formula and the standard combination formula gave the same answers. At first, I thought it was because of something about $\binom{n}{r}$ and $\binom{n}{n - r}$ , but those examples didn't seem to matter. Ex: from $\binom{5}{2}$ , from $\binom{5}{3}$ gave the same answers from both formulas and  from $\binom{20}{17}$ , from $\binom{20}{3}$ gave the same answers from both formulas)","['combinations', 'combinatorics']"
4555416,Understand conditional expectation of indicator functions.,"I am new to conditional probability/expectation and am trying to get my head around the following equation. $$\int \operatorname{var}\left(\mathbb{E}\left(1_{\{Y \leq t\}} \mid X\right)\right) d \mu(t),$$ where $\mu(t)$ is the distribution of Y, Y=aX+bZ, and X and Z are standard normal I.i.d. . I had a difficult time thinking about $\mathbb{E}\left(1_{\{Y \leq t\}} \mid X\right)$ . Would it be equal to $1_{\{ax+bE(Z) \leq t\}}$ or $P(aX+bE(Z)\leq t)$ ? How should I think of it? Any help is greatly appreciated.","['conditional-probability', 'conditional-expectation', 'probability-theory']"
4555457,"Spivak, Calculus, Ch. 22: How do we compute the limit $\lim\limits_{n\to\infty} \frac{(n+1)^{\frac{n+1}{n}}}{n}$?","My question is simply how do we compute the limit $$\lim\limits_{n\to\infty} \frac{(n+1)^{\frac{n+1}{n}}}{n}$$ I know the limit is $1$ , both from the context below and because I checked in Maple. Here is the context in which this limit arose. In Chapter 22 of Spivak's Calculus , Problem 13 asks us to show first that if $f$ is increasing on $[1,\infty)$ then $$f(1)+...+f(n-1)<\int_1^n f(x)dx<f(2)+...+f(n)$$ When we apply this result to the function $f(x)=\log{(x)}$ we easily obtain the relationship $$\frac{n^n}{e^{n-1}}<n!<\frac{(n+1)^{n+1}}{e^n}\tag{1}$$ Spivak concludes for us that given this result we can say that $$\lim\limits_{n\to\infty} \frac{\sqrt[n]{n!}}{n}=\frac{1}{e}\tag{2}$$ I am interested in going from $(1)$ to $(2)$ . Starting at $(1)$ , if we take the n-th root and divide by $n$ we have $$0<\frac{1}{e^{\frac{n-1}{n}}}<\frac{\sqrt[n]{n!}}{n}<\frac{(n+1)^{\frac{n+1}{n}}}{n}\cdot \frac{1}{e}$$ Now, $$\lim\limits_{n\to\infty} \frac{1}{e^{\frac{n-1}{n}}}=\frac{1}{e}$$ I would like to compute the limit $$\lim\limits_{n\to\infty} \frac{(n+1)^{\frac{n+1}{n}}}{n}\cdot \frac{1}{e}$$ Which necessitates computing the limit that gave rise to the current question $$\lim\limits_{n\to\infty} \frac{(n+1)^{\frac{n+1}{n}}}{n}$$","['limits', 'calculus']"
4555525,Why Quine–McCluskey algorithm is considered as an analogous to Buchberger's algorithm?,"According to the wikipedia pages of https://en.wikipedia.org/wiki/Quine%E2%80%93McCluskey_algorithm and https://en.wikipedia.org/wiki/Buchberger%27s_algorithm These two algorithms are analogous to each other but Wikipedia does not provide further explanation or any comparison criterias. My question is why Quine–McCluskey algorithm is considered as an analogous to Buchberger's algorithm ? If this is true then there is an interesting consequences Notice that there is a step in the Quine–McCluskey algorithm that required us to solve and NP-hard problem called "" The set covering "" problem.  I am also wondering that what would be the equivalence NP-hard problem that the Buchberger's algorithm required us to solve under the assumption that Quine–McCluskey algorithm and Buchberger's algorithm is analogous ? Thank you for your enthusiasm","['boolean-algebra', 'logic', 'algebraic-geometry', 'algorithms', 'soft-question']"
4555588,Measurable argmax correspondence on probability spaces,"I'm trying to wrap my head around the ""Measurable maximum theorem"", Thm 14.91 in ""A hitchhiker's guide to infinite dimensional analysis"" by Aliprantis & Border.
I wonder if I can use it in a case when the underlying measurable space is a probability space. (I will explain in which sense below). The statement of the theorem goes like this: Let $X$ be a polish space and $(S,\Sigma)$ a measurable space. Let $\varphi: S \twoheadrightarrow X$ (My clarification: $\varphi$ is set valued/a correspondance) be a weakly measurable correspondance with nonempty compact values, and suppose that $f: S \times X \to \mathbb{R}$ is a Caratheodory function (measurable in $s$ and continuous in $x$ ). Define the value function $m$ by: $$m(s) = \max_{x \in \varphi(s)} f(s,x),$$ and the correspondence $$ \mu(s) = \{ x \in \varphi(s): f(s,x) = m(s) \}.$$ Then: The value function $m$ is measurable. The argmax correspondance $\mu$ is measurable, has nonempty compact values and admits a measurable selector. Clarification: By weakly measurable correspondence is meant that $\{s \in S: \varphi(s) \cap K \neq \emptyset \}$ belongs to $\Sigma$ for each compact set $K$ . My question is the following; suppose that I have a probability space $\left( \Omega, \mathcal{A}, \mathbb{P} \right)$ and a Caratheodory function $f: \Omega \times \mathbb{R}^d \to \mathbb{R}$ . I would like to check that the argmax correspondence $$\text{argmax}_{x \in \mathbb{R}} = f(\omega,x)$$ is a measurable correspondence and admits a measurable selector.
Is it possible to use the theorem above to do this? If yes, how do I define $\varphi$ and check that it's weakly measurable in the above sense?","['set-valued-analysis', 'measurable-functions', 'probability']"
4555596,Is the set of triangles closed?,"Let $\mathcal{P}$ be the set of simple polygons in $\mathbb{R}^2$ with strictly positive area. Define the distance between any two such polygons, say $A$ and $B$ , to be $d(A,B) = \mu(A \Delta B)$ where $\mu$ is the Lebesgue measure and $A \Delta B = (A - B) \cup(B-A)$ is the symmetric difference of $A$ and $B$ . The distance function $d$ makes $\mathcal{P}$ a metric space. Let $\mathcal{T}$ denote the set of all triangles in $\mathcal{P}$ . Is $\mathcal{T}$ a closed subset of $\mathcal{P}$ ? My attempt When need to show if $T_n \in \mathcal{T}$ is such that there is a $P_0$ in $\mathcal{P}$ such that $d(T_n,P_0) \to 0$ then $P_0 \in \mathcal{T}$ . Since $|\mu(T_n) - \mu(P_0)| \leq d(T_n,P_0)$ this also means $\lim_{n\to\infty} \mu(T_n) = \mu(P_0) > 0$ . Let $x_n,y_n,z_n$ denote the vertices of $T_n$ in some order, then if we can show the $T_n$ 's are uniformly bounded in $\mathbb{R}^2$ then, passing to a subsequence if necessary, we can assume wlog that $x_n,y_n,z_n$ converges to (say) $x_0,y_0,z_0$ . Let $T_0$ be the (possibly degenerate) triangle described by $x_0,y_0,z_0$ , then it is sufficient to show $d(T_n,T_0) \to 0$ and $T_0$ is not degenerate. We have, $d(T_n,T_0) = \mu(T_n) + \mu(T_0) - 2\mu(T_n \cap T_0)$ .
Clearly $\mu(T_n) \to \mu(T_0)$ . This also shows $\mu(T_0) > 0$ and so $T_0$ is not degenerate. And we also have, $\mu(T_n \cap T_0) = \int_{x \in \mathbb{R}^2}I_{T_n}(x)I_T(x)dx$ (where $I_A()$ is the indicator function of the set $A$ ). We can show $I_{T_n}(x) \to 1$ whenever $x$ is an interior point of $T_0$ and $I_{T_n}(x) \to 0$ for $x \not\in T_0$ , this is sufficient to show, using DCT, $\mu(T_n \cap T_0) \to \mu(T_0)$ and we are done. So we need to show $T_n$ 's are uniformly bounded in $\mathbb{R}^2$ .","['measure-theory', 'lebesgue-measure', 'geometry', 'metric-spaces', 'general-topology']"
4555620,The form of a $\sigma$-algebra generated by a countable collection of pairwise disjoint subsets,"I'm trying to prove a statement in this thread , i.e., Theorem: Let $\mathcal A = (A_i)_{i\in I}$ be a countable collection of pairwise disjoint subsets of $X$ such that $\emptyset \in \mathcal A$ . Then $$
\sigma (\mathcal A) = \left \{ \cup_{i \in J} A_i,  X \setminus (\cup_{i \in J} A_i)  \,\middle\vert\, J \subset I  \right\}.
$$ Could you have a check on my attempt? Proof: First, we need a simple lemma. Lemma: Let $\mathcal A = (A_i)_{i\in I}$ be a countable partition of $X$ such that $\emptyset \in \mathcal A$ . Then $$
\sigma (\mathcal A) = \mathcal B:= \left \{ \cup_{i \in J} A_i  \,\middle\vert\, J \subset I  \right\}.
$$ Proof: It's clear that any $\sigma$ -algebra that contains $\mathcal A$ will contain $\mathcal B$ . It remains to prove that $\mathcal B$ is itself a $\sigma$ -algebra. Clearly, $\mathcal B$ is closed under countable union. Let's prove that it is closed under complement. Let $A := \cup_{i \in J} A_i$ with $J \subset I$ . Then $A^c = \bigcup_{i \in J'} A_i \in \mathcal B$ with $J' =I \setminus J$ . Now we come back to the original theorem. Let $B:= X \setminus (\cup_{i\in I} A_i)$ . Then $$
\sigma (\mathcal A) = \sigma (\mathcal A \cup \{B\}).
$$ Clearly, $\mathcal A \cup \{B\}$ is a countable partition of $X$ . By our Lemma , $$
\sigma (\mathcal A) =  \left \{ \cup_{i \in J} A_i, B \cup (\cup_{i \in J} A_i)  \,\middle\vert\, J \subset I  \right\}.
$$ Notice that $$
B \cup (\cup_{i \in J} A_i)   = X \setminus (\cup_{i \in J'} A_i),
$$ with $J' = I \setminus J$ . This completes the proof.","['elementary-set-theory', 'measure-theory']"
4555651,Approximative formula for normal distribution being above threshold,"Suppose that $X \sim \mathcal{N}(r + \frac{1}{N}, s) $ and $Y \sim \mathcal{N}(r, s)$ for some $r, s \approx 1$ and $N \approx 10^6$ . What are good approximate formulas for the quantity $$\frac{ \mathbb{E}(X 1_{X > n})}{\mathbb{E}(Y1_{Y > n}) + \frac{1}{N}}$$ as a function of $r$ in the limit where $r \approx n$ ? The problem arises as one potential answer here: Assessing the efficiency of a single vote in a multiparty presidential election","['applications', 'voting-theory', 'approximation', 'probability']"
4555670,Linear maps that have the same matrix regardless of the bases chosen for domain and codomain,"Question: Other than the zero map, what linear map has the same matrix $A_{E,F}$ with respect to all $E$ and $F$ ? For linear map $T:\mathbb{R}^n \rightarrow  \mathbb{R}^n$ , given a basis $E$ for domain and basis $F$ for codomain, I can find a unique corresponding matrix $A_{E,F}$ , where $A_{E,F}$ generally depends on $E$ and $F$ . Note that for any $E$ and $F$ , the matrix corresponding to the zero map is always the zero matrix, since the map sends all vectors in $E$ to $\mathbf{0}$ , and $\mathbf{0}$ can only be represented by $(0,...,0)$ with respect to any basis $F$ .","['matrices', 'change-of-basis', 'linear-algebra', 'linear-transformations', 'similar-matrices']"
4555671,Range of a function with a restricted domain.,"Consider a function, $$
f(x)=2x-4\sin x,
$$ having a given domain of $[0,2\pi]$ . Is it incorrect to say that its range is, $$
\{f\in [-1.37,13.93]\}.
$$ Please also comment on the notation. I am trying to improve my precision in writing mathematical notations.",['functions']
4555682,Discontinuous function of two variables,"Let \begin{equation}
     f: \mathbb{R}^{2} \rightarrow \mathbb{R}
\end{equation} be a function of two real variables given by \begin{equation}
 f(x,y) = \begin{cases}
       \frac{x}{y} & \text{for}\quad y\neq 0  \\
       0 & \text{for}\quad y=0 \ \\
       \end{cases}
\end{equation} Does $f(0,0)=0$ ? My understating is that $f(0,0) =0$ since $f(x,0)=0$ for all $x \in \mathbb{R}$ . Is this correct?","['analysis', 'continuity', 'multivariable-calculus', 'functions', 'limits']"
4555697,How to solve this ODE problem,"We have the following ODE: \begin{equation}
    \begin{cases}
    \frac{dy}{dx}=y(1-y)\\
    y(0)=1/2
    \end{cases}
\end{equation} We can use the following equality: $$\frac{1}{y(1-y)}=1/2\bigg(\frac{1}{y}+\frac{1}{1-y}\bigg)$$ Then we do the following: $$\frac{dx}{dy}=\frac{1}{2y}+\frac{1}{2-2y}$$ $$dx=1/2\frac{1}{y}dy+\frac{1}{2-2y}dy$$ $$1/2\ln  y-1/2\ln (1-y)=x+C$$ $$C+x=\ln\frac{\sqrt{y}}{\sqrt{1-y}}$$ $$Ce^x=\frac{\sqrt{y}}{\sqrt{1-y}}$$ $$Ce^{2x}=\frac{y}{1-y}$$ But I am struggling to separate this. And then I find it even harder to answer the following: ""Note that y=1 is composed of critical points, determine with the use of the ODE if these CPs are stable"". This is difficult to know, because the system has no eigenmatrix, and without an eigenmatrix, the are no eigenvalues, so I cannot tell if the ""system"" is stable. Besides, it is not a ""system"" either. Any ideas? Thanks!",['ordinary-differential-equations']
4555756,Prove $\sum^{n+1}_{j=1}\left|\cos\left(j\cdot x\right)\right|\geqslant \frac n4$,"How do you prove that $\displaystyle\sum^{n+1}_{j=1}\left|\cos\left(j\cdot x\right)\right|\geqslant \dfrac{n}{4}$ , where $x\in\mathbb{R}$ ? I tried mathematical induction, but it doesn't work. I also have some ideas on complex number solution, such as setting $z=\cos\left(x\right)+i\sin\left(x\right)$ and working on $Re\left(z\right)$ , but neither does that work.","['trigonometry', 'summation', 'complex-numbers', 'inequality']"
4555768,Is it true that if two disjoint connected open sets in $\mathbb{R}^n$ have the same boundary then the boundary must be connected?,Let us have two disjoint connected open sets $C_1$ and $C_2$ in $\mathbb{R}^n$ such that $\partial C_1 = \partial C_2$ . My claim is that $\partial C_1$ must be connected. I cannot rigorously prove it but my idea is as follows. Assume that $\partial C_1$ is not connected and let $x$ and $y$ be two points from two different connected components of $\partial C_1$ . Since open connectedness implies path connectedness in $\mathbb{R}^n$ there are paths connecting $x$ and $y$ in both $C_1$ and $C_2$ . So we can form a closed curve passing through the points $x$ and $y$ and also through $C_1$ and $C_2$ . I am pretty sure that this curve cannot be shrinked to a point. But I don't know how I can write it mathematically. I appreciate it if someone can help. Thanks.,"['geometric-topology', 'general-topology', 'real-analysis']"
4555818,What are some recommended Rating Systems for a Single-Elimination Bracket Tournament?,"I have gathered data on a set of certain contests. The contests are in the form of single-elimination. That is, there are $2^n$ competitors in $n$ rounds. In each game, two players face each other, and the loser gets eliminated, until only one player remains who is the winner. Now, the data I have is of multiple such contests, taking place annually. One caveat is that once a player wins, they don't play in these contests anymore. I want to ask for recommended rating systems for such a contest. I considered Elo Rating System, but as some winners play less matches than those who have never won, and Elo depends more on matches won, it wasn't as effective. I didn't understand Glicko, but I feel this won't work, as well. I currently just use the victory percentage, which I calculate as $S=100 \times \frac {1.0\times v + 0.5 \times d} {m}$ where $v$ is victories, $d$ is draws and $m$ is the number of matches. One thing is that the players get a score in each match (which generally ranges from 100 to 10000, but isn't limited by anything), which I think can be used in some way.",['statistics']
4555848,Using Wronskian to solve nonhomegeneous ODE,"I have the given ODE: $$y''+2y'+2y=e^{-x}\sin x$$ This has the homogeneous solution $y_h=C_1\cos(i-1)x+C_2\sin(-i-1)x$ . The particular solution, in the form $y_p=uy_1+vy_2$ , we seek the Ansatz: $y_p=uy_1+vy_2=e^{-x}(\sin x+\cos x)$ .  So $y_1=e^{-x}\sin x$ and $y_2=e^{-x}\cos x$ Then we aim to solve for $u$ and $v$ by  use of the variation of parameters formula: $$u'y_1+v'y_2=0$$ $$u'y_1'+v'y_2'=f(x)$$ where $f(x)=e^{-x}\sin x$ . So here I should  use the Wronskian to facilitate the process. The Wronskian is naturally dependent on $y_1$ and $y_2$ and are $y_1=e^{-x}\sin x$ and $y_2=e^{-x}\cos x$ . So the Wronskian would be \begin{equation}
\text{Det}\begin{vmatrix}
e^{-x}\sin x & e^{-x}\cos x\\
e^{-x}\cos x-e^{-x}\sin x & -e^{-x}\cos x-e^{-x}\sin x
\end{vmatrix}
\end{equation} My calculation gives: $Det=-e^{-2x}\cos2x$ So how is this useful to solve the ODE, when I could just use the formula for variation of parameters? Thanks","['wronskian', 'ordinary-differential-equations']"
4555856,"Finding $x-\frac{1}{x}$, given $x^3 - \frac{1}{x^3} = 108+76\sqrt{2}$","If $x^3 - \dfrac{1}{x^3} = 108+76\sqrt{2}$ , find the value of $x-\dfrac{1}{x}$ . Here's what I've tried so far. $$\begin{align}
\left(x-\dfrac{1}{x}\right)^3&=x^3-\dfrac{1}{x^3}-3\left(x-\dfrac{1}{x}\right) \\
\rightarrow \quad \left(x-\dfrac{1}{x}\right)^3&=108+76\sqrt{2}-3\left(x-\dfrac{1}{x}\right) \\u:=x-\dfrac{1}{x} \quad\rightarrow \quad u^3+3u-108-76\sqrt{2}&=0
\end{align}$$ Got stuck here since I didn't know how to solve this cubic equation. I also tried factorizing $x^3-\dfrac{1}{x^3}$ . $$\begin{align}x^3-\dfrac{1}{x^3}&=\left(x-\dfrac{1}{x}\right)\left(x^2+\dfrac{1}{x^2}+1\right) \\
&= \left(x-\dfrac{1}{x}\right)\left(x^2+\dfrac{1}{x^2}+2-1^2\right) \\
&= \left(x-\dfrac{1}{x}\right)\left(\left(x+\dfrac{1}{x}\right)^2-1^2\right) \\
&= \left(x-\dfrac{1}{x}\right)\left(x+\dfrac{1}{x}+1\right)\left(x+\dfrac{1}{x}-1\right)
\end{align}$$ Again, I didn't know what I could do with this.",['algebra-precalculus']
4555948,Given any ODE is there a way to know if it has any explicit solution?,"Given any ODE of the form $$F(x,y,y',...,y^n)=0$$ is there a way to know if there are any explicit solutions? For example for the differential equation $$(1+y^2)(e^{2x}-e^yy')-(1+y)y'=0$$ the solution I can come up with is $$\dfrac{e^{2x}}{2}+C=\arctan(y)+\dfrac{ln|1+y^2|}{2}+e^y+K$$ And I cannot find any explicit form of $y$ for that equation. (Don't know if there are) So I am trying to generalize the case to any ODE, is there a way I can know if an equation has an explicit solution before even doing it?","['ordinary-differential-equations', 'real-analysis']"
4555953,What are the applications of a more general matrix exponential and its derivative?,"The standard, decoupled matrix exponential $\exp(At)$ for a real matrix $A$ and a continuous variable $t$ occurs in the solution to systems of linear ordinary differential equations. But, this wikipedia article has a slightly more generalized structure https://en.wikipedia.org/wiki/Derivative_of_the_exponential_map Instead of $\exp(At),$ we have $\exp(A(t))$ , where $A(t)$ is a general function of the variable $t$ in matrix form. What are the applications of this slightly more generalized matrix form, and its derivative?","['matrices', 'operator-theory', 'derivatives']"
4555976,Geometric proof that angle does not exceed $90^\circ$,"Problem: In rectangle $ABCD$ , $BC=2AB$ . Point $E$ is arbitrary point on $BC$ . Prove that $\angle AED \le 90^\circ$ . I can show it using trig. Let $CE=x$ then $$\cos \angle AED =\frac{AB^2+(2AB-x)^2+AB^2+x^2-4AB^2}{2 \cdot AE \cdot DE}=\frac{2AB^2-4AB\cdot x+2x^2}{2 \cdot AE \cdot DE}=\frac{(AB-x)^2}{AE \cdot DE} \ge0$$ However, I struggle to provide a geometric proof. Any ideas?","['euclidean-geometry', 'geometry']"
4555984,Must every $\mathbb Z$-shaped point of an arithmetic scheme be contained in an affine open subscheme?,"Assume $X$ is a scheme over the base-ring $\mathbb Z$ and $t:\text{Spec}\,\mathbb Z \to X$ is an integer shaped point of $X$ . Is there necessarily an affine open subscheme $U$ of $X$ through which $t$ factors? Context: Richard Garner claims on page 5 first paragraph of An embedding theorem  for tangent categories that all schemes are microlinear (in the sense of synthetic differential geometry). They claim that I can reduce the proof to the affine case by using that open embeddings of schemes are formally etale. Formally etale means that if I have some $t: \text{Spec } \mathbb Z = \{0\} \to X$ , lying in some open affine subspace $U$ , then any infinitesimal thickening of $\{0\}$ such as e.g. $D_2(2) = \text{Spec } \mathbb Z[x,y]/(x,y)^3$ lies also in $U$ . But to use this I have to know that $t$ lies in some open affine in the first place. I am equally happy if you can tell me why any scheme is infinitesimally linear (the weaker version of microlinearity).","['arithmetic-geometry', 'algebraic-geometry', 'synthetic-differential-geometry', 'schemes']"
4556014,Solve differential equation $(x^2 + y^2 + x)dx + ydy = 0$ by making a variable substitution or by turning it into a total differential equation,"Solve differential equation $(x^2 + y^2 + x)dx + ydy = 0$ by making a variable substitution or by turning it into a total differential equation. I tried various substitutions. For example this equation is equivalent to $(x^2+y^2)dx + d(xy) = 0$ so I tried substituting with $(x,y) \to (x, u=xy)$ so the equation becomes $(x^2 + \dfrac{u^2}{x^2})dx + d(u) = 0$ . If I rewrite this as $u' + \dfrac{u^2}{x^2} = -x^2$ then it is a Ricatti differential equation, though I can't find any solution for this equation. For example, it seemed logical trying for $u = x^\alpha$ , but $\alpha x^{\alpha-1} + x^{2\alpha-2}=-x^2$ doesn't say much to me. Any suggestions?",['ordinary-differential-equations']
4556060,Find the probability that the smallest/largest number is 5.,"Ten persons in a room are wearing badges marked $1$ through $10$ . Three persons are chosen at random, and asked to leave the room simultaneously. Their badge number is noted. (a) What is the probability that the smallest badge number is $5$ ? (b) what is the probability that the largest badge number is $5$ ? Answer: (a) $\frac{1}{12}$ , (b) $\frac{1}{20}$ . My attempt: (a) We have five numbers greater than $5$ from a total of ten, so the probability wanted is: $P=\frac{5\times4\times3}{10\times9\times8}=\frac{1}{12}$ (b) Using the same logic we have: $P=\frac{4\times3\times2}{10\times9\times8}=\frac{1}{30}$ I know it's a combination problem, I've calculated this way cause I find more intuitive, I just don't get why it worked for one item but not the other.
What was my mistake?","['statistics', 'combinatorics', 'probability']"
4556077,Upper bound in information theory puzzle,"Consider the following puzzle: I have just flipped $n$ fair coins. Before I start revealing them to you, you can ask me one yes/no question. Then, you can make $n$ Head/Tail guesses. What's your question and strategy for maximizing the number of correct guesses? The baseline (no question asked a priori) is to guess only H (or T). This will give you $\dfrac{n}{2}$ correct guesses (in expectation). A first question would be: ""Is the first coin H?"" . If yes, guess only H, otherwise guess only T. So you now get $1 + \dfrac{n-1}{2} = \dfrac{n}{2} + \dfrac12$ correct guesses. Another question: ""Are there more H than T?"" -- and just answer H or T depending on the answer. This strategy yields $n\cdot \Pr(H \geq T) = n\cdot\sum_{k=n/2}^{n} \binom{n}{k} \cdot \dfrac{1}{2^n}$ correct guesses (from Binomial PMF), which is the best so far. My questions are the following: How much more can we maximize the number of correct guesses? Is there a (tractable) way to quantify this (an upper bound)? One yes/no question means one bit of entropy. Can we derive some sort of relationship between this quantity and the max correct guesses? For example, if one asks two questions (2 bits of entropy), how much better off are we? Clearly, if we're allowed $n$ bits of entropy, then we can guess the entire sequence.","['puzzle', 'probability', 'information-theory']"
4556093,"Prove that $\min_\limits{x\in\{-1,1\}^n} \lVert Ax\rVert_\infty≤ C\sqrt{n\log(n)}$",I’m struggling a lot since few hours on this exercice and I really have no idea how to make the log appears. The problem is the following: Let $A$ be a matrix of size $n\times n$ whose entries $a_{ij}$ are such that $\left|a_{ij}\right|\leqslant1$ . Prove that there exists $C>0$ a constant that doesn’t depend on $n$ such that: $\min_\limits{x\in\{-1;1\}^n}$ $\lVert Ax\rVert_{\infty}\leqslant C\sqrt{n\log(n)}$ I did a lot of exercises dealing with inequalities of matrix norm but I really have no idea for this one. I learnt about Jensen inequality but it led me nowhere. Thanks in advance for any kind of help.,"['matrices', 'inequality', 'linear-algebra']"
4556096,"Find all $f\in C^\infty(\mathbb R)$ such that : $\forall x\in \mathbb R, f(ax)+f(bx)+f(cx)=0$ with $a,b,c>0$ and distinct","Let $a,b,c\in\mathbb R^*_+$ with $a\neq b$ , $a\neq c$ and $b\neq c$ , Find all $f\in C^\infty(\mathbb R)$ such that : $\forall x\in \mathbb R, f(ax)+f(bx)+f(cx)=0$ with $a,b,c>0$ and distinct. We can make $a<b<c=1$ . Hint given : We need to show that $\forall A>0, \forall p\in\mathbb N,\exists C>0,\forall x\in[-A,A], |f(x)|\le C|x|^p$ $(\star)$ I noticed that : $a^nf^{(n)}(ax)+b^nf^{(n)}(bx)+c^nf^{(n)}(cx)=0$ so $\forall n\in\mathbb N, f^{(n)}(0)=0.$ Also : $f\in C^\infty(\mathbb R)$ so $\forall A>0,\forall n\in \mathbb N,\forall x\in [-A,A]$ : $$|f(x)|= \left|\sum_{k=0}^{n}\dfrac{x^kf^{(k)}(0)}{k!} + R_n\right| = |R_n| \le \sup_{y\in[-A,A]}|f^{(n+1)}(y)|\dfrac{|x|^{n+1}}{(n+1)!}=C|x|^{n+1}$$ The case : $\forall A>0, \forall x\in [-A,A], \exists C>0, |f(x)| \le C$ because $f\in C^0([-A,A],\mathbb R)$ . So we did end up to the property $(\star)$ . After that I don't know what to do. Do you have some ideas ?","['integration', 'continuity', 'functions']"
4556120,Does there exist a tangent field of normal vectors along a curve that covers all points near the curve?,"Let $\gamma: [0,1] \to \mathbb R^2$ be a parameterized curve, where $\gamma(t) = (x(t),y(t))$ is continuously differentiable with $(x' (t), y'(t)) \ne (0,0)$ for all $t \in [0,1]$ . Let $N(t) = (-y'(t),x'(t))$ be the normal vector to the tangent vector $\gamma' (t)$ . Is it true that for all $t \in (0,1)$ , there exists an open ball $B_r (t)$ with radius $r > 0$ such that for all $(x,y) \in B_r (t)$ , there exists $s \in (0,1)$ and $\alpha \in \mathbb R$ with $\gamma (s) + \alpha N(s) = (x,y)$ ? If we assume that $\gamma'$ also is continuously differentiable, we can use the Inverse Function Theorem: Define $f: \mathbb R^2 \to \mathbb R^2$ by $f( s, \alpha ) = \gamma (s) + \alpha N(s)$ . Then the Jacobian matrix of $f$ is: $\left[ \begin{array} 
 ,x'(s) - \alpha y''(s) & y'(s)+ \alpha x''(s) \\
-y'(s) & x'(s) \end{array} \right]$ The determinant of the Jacobian evaluated at $t$ is: $x'(t)^2 + y'(t)^2 \ne 0$ which shows that the matrix is invertible. It follows from the Inverse Function Theorem that $f$ is invertible in an open ball with center $t$ , which proves the claim. However, when $\gamma'$ is not differentiable, I am unsure how to create the proof. My attempt would be to fix $t_0,t_1 \in (0,1)$ with $t_0 < t < t_1$ and $K > 0$ such that the curve with image: $\{ f( t_0, \alpha) : \alpha \in [-K,K] \} \cup \{ f( t_1, \alpha) : \alpha \in [-K,K] \} \cup \{ f( s, K ) : s \in [t_0,t_1] \} \cup \{ f( s, -K ) : s \in [t_0,t_1] \}$ has positive distance $d$ from $\gamma(t)$ . Then we take the limit $t_0,t_1 \to t$ and $K \to 0$ , so the curve contracts continuously to $\gamma(t)$ . Then we can use tools from algebraic topology to show that for all $(x,y) \in B_d (t)$ , the contraction must pass through the point $(x,y)$ . Is such an approach viable?","['curves', 'differential-geometry']"
4556162,Prove expected value of first hitting time is finite for irreducible finite state Markov Chain,"In Levin and Peres' Markov Chains and Mixing Times, lemma 1.13 states:
For any states $x$ and $y$ of an irreducible chain, $E_x(\tau^+_y)<\infty$ where $\tau^+_x := \min\{t \geq 1 : X_t = x\}$ .
The proof in the text is: The definition of irreducibility implies that there exist an integer $r > 0$ and a real $\epsilon > 0$ with the following property: for any states $z, w \in \mathcal{X}$ , there exists a $j \leq r$ with $P^j(z, w) >\epsilon$ . Thus for any value of $X_t$ , the probability of hitting state $y$ at a time between $t$ and $t + r$ is at least $\epsilon$ . Hence for $k > 0$ we have (1.17): \begin{equation}
P_x\{\tau^+_y > kr\} ≤ (1 − \epsilon)P_x\left\{\tau^+_y > (k-1)r\right\}
\end{equation} Repeated application of (1.17) yields: $$P_x\{\tau^+_y > kr\} \leq (1 − \epsilon)^k$$ Lastly: $$E_x(\tau^+_y) =\sum_{t\geq0}P_x\{\tau^+_y  > t\} ≤\sum_{k\geq0}rP_x\{\tau^+_y> kr\}≤ r(1 − \epsilon)^k <\infty.$$ How do we prove (1.17)?. It seems induction is the way to go. (1.17) holds when $k=1$ but showing that (1.17) holds for $k+1$ if it holds for $k$ is more complicated. Also in the last sentence of the proof, why is the first inequality true?","['markov-chains', 'stochastic-processes', 'markov-process', 'probability-theory', 'probability']"
4556169,Solve the system $ \left\lbrace \begin{array}{ccc} \sqrt{1-x} + \sqrt{1-y} &=& \frac{y-x}{2} \\ x+y &=& 2xy \end{array}\right. $,"I have to solve the following system: $$
\left\lbrace \begin{array}{ccc}
\sqrt{1-x} + \sqrt{1-y} &=& \frac{y-x}{2} \\
x+y  &=& 2xy
\end{array}\right.
$$ I've showed that it is equivalent to $$
\left\lbrace \begin{array}{ccc}
	\sqrt{1-x} - \sqrt{1-y} &=& 2 \\
	x+y  &=& 2xy
\end{array}\right. \text{ or } x=y= 1
$$ And i can't conclude nothing frome this system: $$
\left\lbrace \begin{array}{ccc}
	\sqrt{1-x} - \sqrt{1-y} &=& 2 \\
	x+y  &=& 2xy
\end{array}\right.
$$","['algebra-precalculus', 'systems-of-equations']"
4556184,find the smallest n such that $a_n > 2019$,"Source: Question 11 of this problem set . I'd like to point out that I'm not cheating on any contest by posting this problem (e.g. the OTIS). It was released in the past to help students prepare for the Putnam competition. Let $a_1 = 1, a_{n+1} = a_n + \lfloor \sqrt{a_n}\rfloor$ for $n\ge 1$ . Find, with proof, the smallest $n$ such that $a_n > 2019$ ? My first instinct is to try to guess a formula for $a_n$ that can hopefully be easily proven via induction. Alternatively one may be able to prove some useful properties about the sequence using induction. We have $\lfloor \sqrt{a_n}\rfloor = k\Leftrightarrow k^2 \leq a_n \leq k^2 + 2k.$ Computing the first few terms of the sequence gives $1,2,3,4,6,8,10,13,16,20,24,28,33,38.$ But there doesn't seem to be any noticeable pattern that can lead to a general formula. However, observing the perfect squares in the sequence, we see that the only two seen so far are $4$ and $16$ . It might not just be a coincidence that these are both powers of 4 (1). Then it might be useful to consider the number of times $\lfloor \sqrt{a_n}\rfloor$ occurs. The sequence $\lfloor \sqrt{a_n}\rfloor$ is as follows: $1,1,1,2,2,2,3,3,4,4,4,5,5,6$ . Again, it may not be a coincidence that the values that occur $3$ times in this small sample are powers of 2 while all other values occur exactly twice (2). Assume that hypotheses (1) and (2) hold for all $n\leq k, k\ge 14$ . We want to show they still hold for $k+1$ . Let $4^q \leq k+1 < 4^{q+1}.$ We have for $n\ge 2$ that $a_n = 1 + \sum_{i=1}^{n-1} \lfloor \sqrt{a_n}\rfloor.$ I'm not sure how to prove the inductive step from here.","['contest-math', 'ceiling-and-floor-functions', 'elementary-number-theory', 'sequences-and-series', 'induction']"
4556193,Closed form of a power series involving double factorial: $\sum_{n=0}^{\infty} \frac{x^n}{(2n-1)!!}$,"I meet a series of the form $$\sum_{n=0}^{\infty} \frac{x^n}{(2n-1)!!}$$ where $(-1)!! = 1$ . I guess it is a Taylor expansion of a function but I don't know what it is. Could anyone here help me? Remark: The problem comes from calculating a renewal process. Assume $N(t)$ is a renewal process with interarrival time $X_i$ where $X_i$ i.i.d. follow $\chi^2_1$ . Then the arrival time of the $k$ th event is $S_k \sim \chi^2_k$ . Then the renewal function is $$m(t) = \mathbb{E}N(t) =\sum_{k=1}^\infty Pr(S_k \leq t)$$ which is $$\sum_{k=1}^\infty \int_0^t \frac{x^{k/2-1}e^{-x/2}}{2^{k/2}\Gamma(k/2)}dx.$$ We can exchange the summation and the integral and divide the summation into two parts according to $k$ is even or odd. The part for $k$ is even is easy. But for $k$ is odd, I think we need to deal the series in the beginning of the problem.","['power-series', 'calculus', 'error-function', 'sequences-and-series']"
4556211,Find the closed formula for the following recusive sequence.,"Find the closed formula for the recursive sequence defined by $a_0 = 4$ , $a_1 = 12$ , $a_n = 6a_{n-1}-a_{n-2}$ for $n>1$ . This question is stumping me. I don't know any methods besides guess and check. Current progress: The first five terms are $4, 12, 68, 396, 2308$ . Each term is divisible by four, yielding $1, 3, 17, 99, 577$ Each of these is off of a perfect square by one or negative one $0-1, 4-3, 16-17, 100-99, 576-577$ = $-1,1,-1,1,-1$ Other than that I'm lost. Any help would be appreciated.","['recursion', 'discrete-mathematics', 'sequences-and-series']"
4556217,Determining circle radius/angle from an arc chord.,"I have to admit upfront that while I did fine at high school Geometry, it probably remains one of the subjects where I thought, 'okay, when am I ever going to use this ?' And sort of blanked it out of my mind for direct reference. Notably, this is not the only subject that has 'come back to bite me', or in undergrad studying first in Philosophy, I took a course on logic, where we learned about 'truth tables'-- And lo-and-behold, some 15 years later I find in FPGA's and system state logic, what do you have, but 'truth tables !'. In any case, unfortunately my 'geometry' is maybe a little too far back and too fuzzy. But I am trying to work on a little side project at the time, and rather than 'theoretical' this actually applies to a 'real world' type example: So say I have this series of arcs/chords, and I am trying to determine the radius of the circle they are composed of. I know there are formula's out there such as this . But that requires you to know 'h' or how far the center of the circle is. Since these drawings/sketches come off a piece of machinery, let's just say it is 'not reasonably possible' for me to figure out the actual origin of the circle. Yet, it seems there must be some other way I can determine the figures that I want, no ? But... I'm a little confused as to how to go about it, or what formulae to use. Sorry for being 'naïve' or just 'forgetting', but any assistance is greatly appreciated. Best,","['angle', 'circles', 'geometry']"
4556357,How many odd integers are there between $0$ and $100$ that have distinct digits?,"I'm interested in counting the odd integers between $0$ and $100$ with distinct digits using a combinatorial approach. My attempt is as follows. There are $10$ possible choices for the first digit, that is the tens digit. Then, the second digit can be chosen in $5$ ways if the tens digit is even, and in $4$ ways if the tens digit is odd. But then I'm stuck since the two events seem to be dependent and I don't see a straightforward way to apply the multiplication rule.",['combinatorics']
4556447,When is the projection $\mathrm{P}:x\longmapsto \mathrm{J}(x)^\top(\mathrm{J}(x)\mathrm{J}(x)^\top)^{-1}\mathrm{J}(x)$ Lipschitz continuous?,"$f:\mathbb{R}^n\to\mathbb{R}^m$ is a smooth function with $n > m$ . $\mathrm{J}(x)$ is the Jacobian matrix of $f$ and it is full row-rank at every $x\in\mathbb{R}^n$ . For any $x\in\mathbb{R}^n$ define the projection matrix as the $n\times n$ symmetric idempotent matrix $$
\mathrm{P}(x) = \mathrm{J}(x)^\top(\mathrm{J}(x)\mathrm{J}(x)^\top)^{-1}\mathrm{J}(x).
$$ Can I say that $\mathrm{P}: x\longmapsto \mathrm{P}(x)$ is Lipschitz continuous? If not, what conditions do I need to say this?","['real-analysis', 'multivariable-calculus', 'calculus', 'linear-algebra', 'differential-geometry']"
4556451,How can I maximize an absorbing state outcome in a Markov Chain?,"I am working on Markov Chain for a computer science project. The problem is based on the gunshot in ""The Good The Bad and the Ugly"". Three shooters have a certain chance to hit their target and a certain chance to choose one of the other two shooters. So the Good has a chance g to hit his target, The Bad has a chance b and The Ugly a chance u . I want to show that each shooter should target the best gunslinger when they shoot to maximize their likelihood to survive. For this I introduce gu , the probability for G to choose U, bu the probability for B to choose U and ub the probability for U to choose B. I've built my Markov chain, I implemented my transition matrix, identified the fundamental matrix F and the Absorbtion matrix A. So I just have to look at the first column of A to know that to go from the initial state (GBU everyone is alive and G starts shooting ) to the absorbing states G,B,U I have a probability of respectively A11, A21 and A31. So my question is, how can I find a way to maximize those last three probabilities. I've considered setting g , b and u and find optimal values of gu , bu and ub but I don't see how I could do that. Should I consider A as a function of gu , bu and ub and find the critical points such that ∇A=0 ? But considering that A is a very long expression it seems very unpractical. https://i.sstatic.net/TXR6f.jpg (The Markov chain and the transition matrix). Thanks in advance for your help. Small update: the question accepted answered in a mathematical point of view to the question, however I am still looking for an approach using Markov chain and the absorbing matrix! So feel free to comment!","['matrices', 'markov-process', 'markov-chains', 'probability']"
4556508,Classify infinities,"In mathematics, "" $\infty - \infty$ "" is an indetermination because $\infty$ can be reached by lots of different ways, and we can define some of them as ""bigger"" than the others. Counter-intuitively, we also have that $\infty = \infty + \infty = \infty^2 = e^\infty = \infty^\infty$ . What a mess ! Is there a way to ""classify"" the different infinities, in order to apply classical algebra to infinities ? Is there some kind of ""infinity-algebra"" ? For instance, can we define: $$\infty_0 := \lim_{x\rightarrow +\infty} x$$ We could consider that "" $x\rightarrow +\infty$ "" $ \Leftrightarrow  $ "" $x\rightarrow \infty_0$ "", and thus: $$\lim_{x\rightarrow \infty_0} x^2 = \infty_0^2$$ We would also have: $$\lim_{x\rightarrow 0} \frac{1}{x} = \lim_{x\rightarrow \infty_0} x = \infty_0 =\ ""\frac{1}{0}""$$ and so on.","['limits', 'infinity']"
4556536,What is the area bound by two shifted sine waves in polar coordinates?,"I need to calculate the area bound by the curve $(\sin(\theta), \sin(\theta -\phi)), \theta \in [0, 2\pi]$ and have the area in terms of $\phi$ . So far I have been saying $x=\sin(\theta)$ and $y = \sin(\theta -\phi)$ then $r^2 = \sin^2(\theta)+\sin^2(\theta - \phi)$ . I then do a polar double integral of them form $$A=\int_0 ^{2\pi}\int_0^{\sqrt{\sin^2(\theta)+\sin^2(\theta - \phi)}} r dr d\theta$$ $$A=\frac12 \int^{2\pi} _0\left[r^2\right]^{\sqrt{\sin^2(\theta)+\sin^2(\theta - \phi)}}_0 d\theta = \frac12 \int^{2\pi}_0\sin^2(\theta)+\sin^2(\theta - \phi) d\theta$$ $$A = \frac12 \int _0 ^{2\pi} \left(\frac{1 - \cos(2\theta)}2 +\frac{1 - \cos(2(\theta-\phi))}2\right) d\theta$$ $$A=\frac12\left[\frac{2\theta-\sin(2\theta)}4 + \frac{2\theta-\sin(2(\theta-\phi))}4\right]_0 ^{2\pi}$$ $$A=\pi$$ After a few attempts I keep getting the answer to be $\pi$ , when obviously the area should be a function of $\phi$ . Am I formulating the problem incorrectly or do I keep making a mistake during the integration? For reference, this is the area I am trying to determine, as a function of $\phi$ https://www.desmos.com/calculator/qeq6toiqki . Thanks in advance!","['integration', 'calculus', 'polar-coordinates']"
4556539,Holomorphic function has infinite many derivatives (proof without using Cauchy theorem),"Let $f$ be an holomorphic function. It is possible to prove that it has infinitely many derivatives but without using Cauchy theorem? It is not an homework from my math class, it is just a curiosity.
Hope someone could help or give a reference. Thank you in advance.","['complex-analysis', 'calculus', 'derivatives']"
4556592,Check if the applications defined below are linear transformations:,"Check if the applications defined below are linear transformations: a) $T: \mathbb R^2 \to \mathbb R^2$ , $T(x_1, x_2) = (x_1 – 1, x_2).$ b) $T: \mathbb R^2 \to \mathbb R^2$ , $T(x_1, x_2) = (x_2, x_1).$ c) $T: M_{2,2} \to \mathbb R, T (\left [ \begin{matrix}
    a & b \\
    c & d \\
   \end{matrix} \right ]) = a – 2b + c – 2d$ . My attempt: a) Let $u = (u_1, u_2)$ e $v = (v_1, v_2)$ $T(u+v) = T(u_1+v_1, u_2+v_2) = (u_1 – 1 + v_1 – 1, u_2 + v_2) = (u_1 + v_1 – 2, u_2 + v_2) \ne T(u) + T(v)$ $T(cu) = T(cu_1, cu_2) = (c(u_1 - 1), cu_2) = (cu_1 - c, u_2) \ne cT(u)$ . No. b)Let $u = (u_1, u_2)$ e $v = (v_1, v_2)$ $T(u+v) = T(u_1+v_1, u_2+v_2) =  (u_2 + v_2 , u_1 + v_1) = T(v) + T(u).$ $T(cu) = T(cu_1, cu_2) = (cu_2, cu_1) = c(u_2, u_1) \ne cT(u).$ No. c)Let $u = (a, b, c, d)$ e $v = (e, f, g, h)$ $T(u+v) = T([a, b, c, d] + [e, f, g, h]) =  (a+b) – 2(b+f) + (c+g) – 2(d+h) = (a -2b +c -2d, e -2f + c -2h) = T(u) + T(v)$ $T(ku) = T(k[a, b, c, d]) = ka – k2b + kc – k2d = k(a – 2b + c – 2d) = kT(u).$ Yes. Are my checks correct? Thanks.","['solution-verification', 'linear-algebra', 'linear-transformations']"
