question_id,title,body,tags
4644266,Arithmetic properties of cardinality of unions,"Given that the cardinality of a countable set $A$ equal $\mathfrak a$ , and $A$ be partitioned into singleton with elements within. Can we define the cardinality of disjoint double union with index $i\in I$ and $j\in J$ , $I, J\subset \Bbb N$ such that: $$A=\bigcup_{i=1}^{|I|}\bigcup_{j=1}^{|J|}\{a_{ij}\}$$ $$\left|\bigcup_{i=1}^{|I|}\bigcup_{j=1}^{|J|}\{a_{ij}\} \right|=\sum_{i=1}^{|I|}\sum_{j=1}^{|J|}|\{a_{ij}\}|=\sum_{i=1}^{|I|}\sum_{j=1}^{|J|}1 $$ $$=|I||J|=\mathfrak a\text{ is true?}$$ And if so, can it serve as an alternate definition of same cardinality between sets as existing a bijection? EDIT: The answer provided by @Hamdiken was satisfactory. Just to add the context of my motivation: I thought of a prove that $|\{2k:k\in\Bbb N\}|=|\{2k-1:k\in\Bbb N\}|=|\Bbb N|$ like: $$|\{2k:k\in\Bbb N\}|=\left|\lim_{n\to\infty}\bigcup_{k=1}^n\{2k\} \right| $$ $$=\lim_{n\to\infty}\left|\bigcup_{k=1}^n\{2k\} \right|=\lim_{n\to\infty}\sum_{k=1}^n|\{2k\}| $$ $$\lim_{n\to\infty}\sum_{k=1}^n 1=\lim_{n\to\infty}n=|\Bbb N|  $$ The same can be done for $\{2k-1:k\in\Bbb N\}$ . So given that for that case works, then I tried to prove with the same method that $|\Bbb Z|=|\Bbb N|$ , so i tried to define $\Bbb Z=\{k:k\in \Bbb N \}\cup\{-k:k\in \Bbb N\}\cup\{0\}$ , so: $$|\Bbb Z|=\lim_{n\to\infty}\left|\{0\}\cup\left(\bigcup_{i=1}^2\bigcup_{k=1}^n\{(-1)^ik\}\right) \right| $$ $$=\lim_{n\to\infty} 1+\left|\bigcup_{i=1}^2\bigcup_{k=1}^n\{(-1)^ik\} \right|=1+\lim_{n\to\infty}\sum_{i=1}^2\sum_{k=1}^n|\{(-1)^ik\}| $$ $$=1+\lim_{n\to\infty}\sum_{i=1}^2\sum_{k=1}^n 1=1+\lim_{n\to\infty}2n $$ $$1+2|\Bbb N| $$ and by the info in this page , $1+2|\Bbb N|=|\Bbb N| $ , and then this method would also work for $|\Bbb Q|=|\Bbb N|$ . Sadly, doesn't for for all cardinality so I don't think it serve as a proof.","['elementary-set-theory', 'cardinals']"
4644319,"Given a function $F$, how to evaluate $\max_{t\in (0,1)}\left\vert \frac{\partial^2 F}{\partial x_i \partial x_j}(ty)\right\vert$?","As an ""extra problem"" for my calculus 2 test class we got the following. Since it is an extra problem, I expected it to be more difficult than the other exercises, but actually but not that much. This is the problem: Let $F:\mathbb{R}^3\setminus\{0\}\to\mathbb{R}$ such that $$F:(x_1, x_2, x_3)\mapsto -\frac{1}{|x|^2} +\frac{1}{|x|}\in\mathbb{R}.$$ For a fixed $y\in\mathbb{R}^3\setminus\{0\}$ , find $$\max_{t\in (0,1)}\left\vert \frac{\partial^2 F}{\partial x_i \partial x_j}(ty)\right\vert.$$ To start I computed $\nabla F(x)$ which is $$\nabla F(x) = \left(\frac{2}{|x|^4}-\frac{1}{|x|^3}\right) x,$$ where $x$ denotes the vector $x=(x_1, x_2, x_3)$ . I went ahead by computing the second partial derivatives. I had $$\frac{\partial^2 F}{\partial x_i^2} = \frac{2}{|x|^4}-\frac{1}{|x|^3} +\frac{3x_i^2}{|x|^5}-\frac{8x_i^2}{|x|^6}, \quad i\in\{1, 2, 3\}$$ and $$\frac{\partial^2 F}{\partial x_i\partial x_j} = \frac{3x_i x_j}{|x|^5}-\frac{8x_i x_j}{|x|^6}, \quad i\in\{1, 2, 3\}, i\neq j.$$ I got stuck at this point since I have no idea how to establish the maximum. I’ve been thinking about it unsuccessfully for a couple of days, but I am really interested in the solution. Could someone please help in finding that maximum? Any hint will be very helpful. Thank you in advance.","['real-analysis', 'multivariable-calculus', 'calculus', 'functional-analysis', 'derivatives']"
4644350,Consistent or inconsistent estimator,"If $\hat{\theta}_n$ is an estimator for the parameter $\theta$ , then the two sufficient conditions to ensure consistency of $\hat{\theta}_n$ are: Bias( $\hat{\theta}_n)\to 0$ and Var $(\hat{\theta}_n)\to 0$ , then we will have $\lim_{n\to\infty}Pr(|\hat{\theta}_n-\theta|>\varepsilon)=0, \forall\varepsilon>0$ . Now suppose $X_1,\ldots X_n$ be iid samples drawn from the Bernoulli distribution with parameter $p$ . Let $\hat{p}=\bar{X}$ be the estimator for $p$ , where $\bar{X}$ is the sample mean. It is clear that $Bias(\hat p)=E(\hat{p})-p=0$ and $Var(\hat{p})=\frac{Var(X_1)}{n}\to 0$ , as $n\to\infty$ . We should expect that $\hat{p}$ is a consistent estimator. However, from the definition of consistency, if I choose $\varepsilon=0.25, p=0.5$ and $X_i=1$ for all i. Then $\hat{p}= 1$ and $$Pr(|\hat{p}-p|>\varepsilon) = Pr(1-0.5>0.25) = 1.$$ This looks like a contradiction to the sufficient conditions above. Can anyone let me know about the mistakes in my argument? Thank you!","['statistics', 'probability-limit-theorems', 'parameter-estimation', 'probability']"
4644357,Monte Carlo Integration by sampling according to general measure,"Is the following statement true: Let $f: C \rightarrow \mathbb{R}$ be a bounded function, where $C \subset \mathbb{R}^m$ compact. Let $\mu$ be a probability measure over $C$ . Furthermore, let $(X_n)_{n \in \mathbb{N}}$ be a sequence of i.i.d. random variables with $X_n \sim \mu$ . Then $\lim_{N \rightarrow \infty} \frac{1}{N} \sum_{i=1}^N f(X_n) = \mathbb{E}_{\mu}[f]= \int_C f d \mu$ almost surely. Proof:
Define $Y_n := f(X_n)$ for $n \in \mathbb{N}$ . As $f$ is bounded, $Y_n$ is a bounded random variable. By Popoviciu's inequality on variances , $Var(Y_n) = \sigma^2 < \infty$ . Therefore, the law of large numbers applies and the statement follows. I am hesitant because there are hardly any restrictions on $\mu$ . The compactness of $C$ rules out things like a Cauchy distribution. I am thankful for any pointers and sources on this.","['measure-theory', 'probability-theory']"
4644390,Simple proof involving functional equations,"I'm struggling with a proof involving a simple functional equation.  The problem asks to show that if $f$ satisfies ( for all $x,y$ real): $$f(xy+x+y) = f(xy)+f(x)+f(y),$$ then $f$ must also satisfy $$f(x+y) = f(x)+f(y).$$ I tried contrapositive.  Suppose there is one pair $(x,y)$ that satisfies the functional equation, but for which the second equation is not satisfied.  Then, it follows that $$f(xy+x+y) \ne f(xy)+f(x+y).$$ That is, if $(x,y)$ does not hold for the second equation, then neither does $(xy,x+y)$ .  This gives us a collection of points that, if the functional equation is true, also do not hold for second equation.  My goal was to show that the second equation would fail at all points and thus give a contradiction (because both equations are clearly satisfied if $x = 0$ , $y = 0$ , for example), but not sure if this is the right approach. I'm new to study of functional equations, so bear with me.","['functions', 'functional-equations', 'abstract-algebra', 'functional-analysis']"
4644488,Trying to do integration using residue theorem,"prove using residue theorem $$\int_{0}^{2\pi}\frac{\cos2\theta}{5+4\cos\theta}d\theta={\pi}/6$$ I tried by using $$z=e^{i\theta}$$ now $$\cos\theta=\frac{e^{i\theta}+e^{-i\theta}}{2}=\frac{z+z^{-1}}{2}$$ $$dz=izd\theta$$ $$\int_{0}^{2\pi}\frac{\cos2\theta}{5+4\cos\theta}d\theta=\int_{0}^{2\pi}\frac{2\cos^2\theta-1}{5+4\cos\theta}d\theta=\int_{|z|=1}\frac{(z^4+1)dz}{2iz^2(2z+1)(z+2)}$$ Now poles occur at $$z=0, -1/2, -2$$ rejecting $$z=-2,$$ as $$ |z|=2>1$$ Now $$Res_{z=-1/2}=\frac{(-1/2)^4+1)}{2i(-1/2)^2(-1/2+2)}=17/12i$$ Pole at z=0 is  of order 2 $$Res_{z=0}=\lim_{z \to 0} \frac{d}{dz}(\frac{(z^4+1)}{2i(2z+1)(z+2)})=-5/8i$$ $$\int_{0}^{2\pi}\frac{\cos2\theta}{5+4\cos\theta}d\theta={2\pi}i(Res_{z=-1/2}+Res_{z=0})=19{\pi}/12$$ But the answer is $${\pi}/6$$ I tried many times, but i get the same answer","['complex-analysis', 'residue-calculus', 'definite-integrals']"
4644501,In what sense is the sphere the limit of convex polyhedra?,"It seems intuitively clear that the sphere can be approximated (both in surface area, but also in a more geometric sense) by certain classes of polyhedra. Do there exist any good formalizations of this notion? Is there a simple sequence of polyhedra which ""converge"" to the sphere? Does anyone know of a good reference on this subject?","['spheres', 'polyhedra', 'combinatorial-geometry', 'geometry']"
4644509,Computation of Statistical Curvature for a Distribution [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question $\newcommand{\bracket}[1]{\left ( #1 \right )}$ $\newcommand{\curly}[1]{\left \{ #1 \right \} }$ $\newcommand{\squared}[1]{\left [ #1 \right ]}$ $\newcommand\tab[1][1cm]{\hspace*{#1}} $ This post is purely about checking my computation .
I am getting a result I am not supposed to get , which means there must be a computation mistake but I really need help finding it . To answer this post, all you need to do is to follow through my calculation and find out the first mistake you see. (Update: the LaTeX newcommand issue and the font issue has been completely resolved.) Fix positive integer $d$ and real number $\theta$ . Define function $f: \mathbb{R}^d \to \mathbb{R}$ by setting, for every $x = (x_1, \cdots, x_d)$ : \begin{equation*}
f(x) = C \exp \curly{- \sum_{i=1}^d (x_i-\theta)^4}
\end{equation*} Here $C$ is a constant such that $f$ becomes a probability density function with respect to the Lebesgue measure. The integral calculator shows that: \begin{equation*}
\int_{\mathbb{R}} \exp(-x^4)\, dx = \frac{1}{2} \Gamma \bracket{\frac{1}{4}}
\end{equation*} \begin{equation*}
\int_{\mathbb{R}} x^3\exp(-x^4)\,dx = 0
\end{equation*} \begin{equation*}
\int_{\mathbb{R}} x^2\exp(-x^4)\,dx = \frac{1}{2} \Gamma \bracket{\frac{3}{4}}
\end{equation*} \begin{equation*}
\int_{\mathbb{R}} x^4\exp(-x^4)\, dx = \frac{1}{8} \Gamma \bracket{\frac{1}{4}}
\end{equation*} \begin{equation*}
\int_{\mathbb{R}} x^6\exp(-x^4)\, dx = \frac{3}{8} \Gamma \bracket{\frac{3}{4}}
\end{equation*} Therefore, $C = \bracket{\frac{1}{2} \Gamma \bracket{\frac{1}{4}}}^{-d}$ . Next, define: \begin{equation*}
l_{\theta}(x) = \ln (C) - \sum_{i=1}^d (x_i-\theta)^4 
\end{equation*} \begin{equation*}
\dot{l_{\theta}}(x) = 4 \sum_{i=1}^d (x_i-\theta)^3
\end{equation*} \begin{equation*}
\ddot{l_{\theta}}(x) = - 12 \sum_{i=1}^d (x_i - \theta)^2
\end{equation*} And we define the matrix: \begin{equation*}
    M_{\theta} =
    \begin{bmatrix}
        \mathbb{E}_{\theta} \squared{\dot{l_{\theta}}^2} & \mathbb{E}_{\theta} \squared{\dot{l_{\theta}}\ddot{l_{\theta}}} \\
        \mathbb{E}_{\theta} \squared{\dot{l_{\theta}}\ddot{l_{\theta}}} & \mathbb{E}_{\theta} \squared{\ddot{l_{\theta}}^2} - \iota_{\theta}^2 
    \end{bmatrix}
\end{equation*} Here $\iota_{\theta} = \mathbb{E}_{\theta} \squared{\dot{l_{\theta}}^2}$ .
Finally, the statistical curvature $\gamma_{\theta}$ is defined as: \begin{equation*}
    \gamma_{\theta} = \sqrt{\frac{\det{M_{\theta}}}{\iota_{\theta}^3}} =
    \sqrt{\frac{\mathbb{E}_{\theta}\squared{\ddot{l_{\theta}}^2}}{\iota_{\theta}^2} 
    - \frac{\mathbb{E}_{\theta}^2\squared{\dot{l_{\theta}} \ddot{l_{\theta}}}}{\iota_{\theta}^3}- 1}
\end{equation*} Our goal is to compute the curvature for this $f$ .
We will be repeatedly using the formula: \begin{equation*}
\bracket{\sum_{i=1}^da_i}^2 = \sum_{i=1}^da_i^2 + 2\sum_{i=1}^d \sum_{j=1}^{i-1} a_ia_j
\end{equation*} We have: \begin{equation*}
    \begin{split}
    & \iota_{\theta} = \mathbb{E}_{\theta} \squared{\dot{l_{\theta}}^2} 
    = \int_{\mathbb{R}^d} \bracket{4 \sum_{i=1}^d (x_i-\theta)^3}^2 C \exp{- \sum_{i=1}^d (x_i-\theta)^4} dx
    \\ & = 16 C \curly{\sum_{i=1}^d \int_{\mathbb{R}^d} x_i^6 \exp{- \sum_{i=1}^d x_i^4}  dx +
    2 \sum_{i=1}^d \sum_{j=1}^{i-1}\int_{\mathbb{R}^d} x_i^3x_j^3 \exp{- \sum_{i=1}^d x_i^4}  dx }
    \\ & = 16 C d \bracket{\frac{1}{2} \Gamma \bracket{\frac{1}{4}}}^{d-1} \frac{3}{8} \Gamma \bracket{\frac{3}{4}}
    = 12d \bracket{\Gamma \bracket{\frac{1}{4}}}^{-1}\Gamma \bracket{\frac{3}{4}}
    \end{split}
\end{equation*} Clearly, $\mathbb{E}_{\theta} \squared{\dot{l_{\theta}} \ddot{l_{\theta}} } = 0$ . Finally: \begin{equation*}
    \begin{split}
        & \mathbb{E}_{\theta} \squared{\ddot{l_{\theta}}^2} =
        \mathbb{E}_{\theta} \squared{ \curly{- 12 \sum_{i=1}^d (x_i - \theta)^2 }^2 }
        \\ & = 144C \curly{\sum_{i=1}^d \int_{\mathbb{R}^d} x_i^4 \exp{-\sum_{i=1}^d x_i^4} dx 
        + 2\sum_{i=1}^d \sum_{j=1}^{i-1} \int_{\mathbb{R}^d} x_i^2x_j^2 \exp{-\sum_{i=1}^d x_i^4} dx}
        \\ & = 144C \curly{d \bracket{\frac{1}{2} \Gamma\bracket{\frac{1}{4}}}^{d-1} \frac{1}{8} \Gamma \bracket{\frac{1}{4}} +
        d(d-1) \bracket{\frac{1}{2} \Gamma\bracket{\frac{1}{4}}}^{d-2} \bracket{\frac{1}{2} \Gamma \bracket{\frac{3}{4}}}^2}
        \\ & = 36d + 144d(d-1) \bracket{\Gamma\bracket{\frac{1}{4}}}^{-2}\bracket{\Gamma\bracket{\frac{3}{4}}}^{2}
    \end{split}
\end{equation*} We see that the last entry is: \begin{equation*}
    \mathbb{E}_{\theta} \squared{\ddot{l_{\theta}}^2} - \iota_{\theta}^2  
    = 36d - 144d \bracket{\Gamma\bracket{\frac{1}{4}}}^{-2}\bracket{\Gamma\bracket{\frac{3}{4}}}^{2}
\end{equation*} Now we have a problem. The covariance matrix $M_{\theta}$ is positive-semidefinite.
The statistical curvature should be a real number.
However, my computation shows that this is not the case for $f$ . \begin{equation*}
    M_{\theta} =
    \begin{bmatrix}
        12d \bracket{\Gamma \bracket{\frac{1}{4}}}^{-1}\Gamma \bracket{\frac{3}{4}} & 0 \\
        0 &  36d - 144d \bracket{\Gamma\bracket{\frac{1}{4}}}^{-2}\bracket{\Gamma\bracket{\frac{3}{4}}}^{2}
    \end{bmatrix}
\end{equation*} \begin{equation*}
    \gamma_{\theta} = \frac{\sqrt{\bracket{\Gamma \bracket{\frac{1}{4}}}^2 - 4 \bracket{\Gamma \bracket{\frac{3}{4}}}^2}}
    {2 \Gamma \bracket{\frac{3}{4}}} 
\end{equation*} What went wrong ?","['statistical-inference', 'statistics', 'solution-verification', 'probability-theory']"
4644511,Minimizing a function in $\mathbb R^3.$,"I am currently solving the following task: Exercise. Minimize in $\mathbb R^3$ the function $f(x_1,x_2,x_3) = x_1^2 + x_2^2 + x_3^2 - x_1x_2 + x_2x_3 - x_1x_3.$ My attempt. I was able to show that $(0,0,0)$ is a local  strict minimizer (consequently, $f(0,0,0) = 0$ is a local strict minimum) using the usual arguments, i.e., I found the critical points of $f$ (which is only one, $(0,0,0)$ ); then, I found the hessian of $f$ in the point $(0,0,0)$ and proved it was positive defined. Therefore, from the theorem below I am able to guarantee that $(0,0,0)$ is a local strict minimizer. Theorem. Assume that $f$ is twice differentiable in $\bar x \in X.$ Then, if $\nabla f(\bar x) = 0$ and $H(\bar x) = \nabla ^2 f(\bar x)$ is positive defined, then $\bar x$ is a local strict minimizer. My question. After this, one question comes up to my mind: is $(0,0,0)$ a global minimizer? Looking to answer this further question, I gave some values to some of the coordinates ( $x_1, x_2 $ or $x_3$ ) individually and plotted the resultant graphs (which are $3D$ ). Every one I plotted (and I plotted a nice quantity...) leads me to the same conclusion: $(0,0,0)$ is, in fact, a global minimizer. After this, I tried to prove analitically that $f(x_1,x_2,x_3) \geqslant 0,$ for every $(x_1,x_2,x_3) \in \mathbb R^3$ (to show that, in fact $(0,0,0)$ is a global minimizer). But I am having a real hard time doing this. Any help towards proving this is apreciatted in advance.","['real-analysis', 'multivariable-calculus', 'solution-verification', 'optimization', 'hessian-matrix']"
4644513,"How to procedurally generate deterministic, easily differentiable terrain?","For my A-Level computer science project I'm making an arcade game in which I need to generate a heightmap for 3D terrain.
Originally I tried using a sum of sine curves (with randomly initialised x and y scale factors) as I need it to be easily differentiable (players are modelled as spheres rolling along the terrain so I need to know the gradient to calculate the portion of weight acting down the slope) but this doesn't provide a high enough periodicity or level of detail.
So without a very high number of sine curves to sum (which is very computationally expensive) this doesn't seem to really work.
Using a high degree polynomial is also difficult as the terrain should maintain roughly the same average height as the game progresses.
I need to be able to initialise an equation for the curve at the start of the game and then base all calculations on that since terrain generation should be deterministic (unloading then reloading the same chunk should generate same terrain without having to store it in memory) so I don't think I can just use Perlin Noise unfortunately.
Any ideas for alternate methods?","['fourier-analysis', 'continuity', 'calculus', 'polynomials', 'trigonometry']"
4644573,Proving an integral identity involving binomial coefficients,"I'm having a lot of trouble proving or disproving the following statement: $$\int_0^1(1-x^{\alpha})^n\ dx=\frac{n!\alpha^n}{\prod_{k=1}^n(\alpha k+1)}$$ $\alpha\in\mathbb{R}^{+*}$ and $n\in\mathbb{N}^*$ . I've tried proving it via the binomial coefficients and an induction, but this has lead to nothing: $$\int_0^1(1-x^{\alpha})^n\ dx=\int_0^1 \sum_{k=0}^n\begin{pmatrix}
n\\
k
\end{pmatrix}(-x^\alpha)^k\ dx=\sum_{k=0}^n\begin{pmatrix}
n\\
k
\end{pmatrix}(-1)^k\int_0^1x^{\alpha k}\ dx$$ $$=\sum_{k=0}^n\begin{pmatrix}
n\\
k
\end{pmatrix}(-1)^k(\alpha k+1)^{-1}$$ The induction blocks beccause of the fact that: $$\begin{pmatrix}
n\\
k
\end{pmatrix}=\frac{n!}{k!(n-k)!}$$ Making it impossible to factor out $(n-k+1)$ from the denominator in the hereditary step of the proof because this term isn't constant.
Attacking the problem via series has also proven unfruitful on my end; the series can't be evaluated at $x=1$ or $x=0$ , and evaluating it at other points leads to a disgusting series. I'm just hoping someone can give me some indicators on what to do! This is a purely recereational question and I thought it would be interesting to tie this conjecture to integrals of this form as I believe there was a similar method for deriving the Wallis product for $\pi$ via integrals like this. For the reference to this integral and the wallis product for $\pi$ , see the ""Historical motivation"" paragraph of this article: https://mindyourdecisions.com/blog/2016/10/12/the-wallis-product-formula-for-pi-and-its-proof/","['integration', 'calculus']"
4644584,Is $SL_2(\mathbb{Z}[i])$ finitely generated?,"This may be a naïve question, but I have been thinking about the structure of the the matrix group $SL_2(\mathbb{Z}[i]) \subset SL_2(\mathbb{C})$ . One thing that has been on my mind is whether or not $SL_2(\mathbb{Z}[i])$ is finitely generated. We know that $SL_2(\mathbb{Z}) \subset SL_2(\mathbb{R})$ is finitely generated, namely by $$S = \begin{pmatrix} 0 & -1 \\
 1 & 0 \end{pmatrix}; \quad T =\begin{pmatrix} 1 & 1 \\
 0 & 1 \end{pmatrix}$$ Since, $S$ and $T$ are in $SL_2(\mathbb{Z}[i])$ is it possible to come up with a generating set?
My thought would be potentially having $S, T$ and maybe $i I_2$ . If anyone has any insight, please share.","['group-theory', 'gaussian-integers']"
4644692,Two measures over different spaces with the same integral,"Suppose that $\mathcal{A}$ , $\mathcal{B}$ are two compact Hausdorff spaces, $\mu_1$ is a measure in the Borel $\sigma$ -algebra $\sigma(\mathcal{A})$ and $\mu_2$ is a measure in the Borel $\sigma$ -algebra $\sigma(\mathcal{B})$ . Let $p:\mathcal{A}\rightarrow\mathcal{B}$ continuous and surjective function. I need to prove that if $$\int_{\mathcal{B}}f d\mu_2=\int_{\mathcal{A}}(f\circ p) d\mu_1$$ for every continuous function $f:\mathcal{B}\rightarrow\mathbb{C}$ , then $\mu_2=\mu_1\circ p^{-1}$ , where $p^{-1}$ is the inverse image, not the inverse function (for example $p^{-1}(U)$ is the inverse image of $U\subseteq\mathcal{B}$ ).
I have search for similar problems, but none has the same conditions than this one. Moreover, I have tried to prove this using the monotone convergence theorem, approximating the functions at both sides by simple functions. My problem is that the integrals are over different spaces. How can I prove this? Any suggestions would be welcome.","['measure-theory', 'borel-measures']"
4644722,"If $\frac{P(E_n)}{P(F_n)} \to 1$, does $\frac{P(A \cap F_n)}{P(A \cap E_n)} \to 1$?","Let $(\Omega, \mathcal F, P)$ be a probability space. Let $E_1\supset E_2\supset ...$ and $F_1 \supset F_2 \supset...$ be two decreasing sequences of events in $\mathcal F$ with the following properties. $E_n \subset F_n$ . $\bigcap_n E_n  = \bigcap_n F_n \neq \emptyset$ . $P(E_n) > 0$ . Let $A \in \mathcal F$ . Let's assume in addition that $P(A \cap E_n)>0$ . Question. If $P(E_n)/P(F_n) \to 1$ , does $P(A \cap F_n)/P(A \cap E_n) \to 1$ as well? The answer is obviously yes if $P(A \cap \bigcap_nE_n)>0$ , so we can assume this isn't the case. Heuristically, it seems to me that the answer should be yes: $P(F_n)$ and $P(E_n)$ are decreasing to the same limit at the same rate and $P(A \cap E_n)$ and $P(A \cap F_n)$ are decreasing to the same limit. I can't see why intersecting with a single set $A$ would change the rates of convergence, so it seems like the result should hold. I think I'm just missing an easy algebraic trick or something like that, however, because I haven't been able to prove this.","['probability', 'real-analysis']"
4644740,Is equidistant points an open problem?,"This post asks whether for any $n$ -dimensional (presumably real) normed vector space, you can find $n+1$ equidistant points. They receive two answers saying that it is possible, but neither give much detail. Wikipedia states that this is an open problem, though its source is from 1971. Neither of these seem particularly convincing as to whether the problem is open or not. They also seem to contradict each other. So I would like to know whether it is an open problem whether for any $n$ -dimensional normed vector space $(\mathbb{R}^n,\|\cdot\|)$ there is always a set of $n+1$ equidistant points.","['normed-spaces', 'linear-algebra', 'open-problem']"
4644745,"Number of maximal antichains in the set $\{1,2,3,4,5,6,...,120\}$ where the order is by divisibility relation.","Find the number of maximal antichains in the set $\{1,2,3,4,5,6,7,...,120\}$ where the order is divisibility relation. For example, $\{6,7,15\}$ is an antichain but not a maximal antichain, and $\{1\}$ is a maximal antichain, and $\{p : p \text{ is a prime less than } 120 \}$ is a maximal antichain. Here is my work so far: whatever we end up doing with primes, we must add one to our final result because $\{1\}$ is a special antichain. Since there are precisely $30$ primes less than $120$ , I was at first thinking about the number of ways of putting $30$ primes into $k$ indistinct boxes, so that for example if $2, 5, 7$ were in the same box, then we would have their product $70$ in the resulting set. We can form maximal antichains this way; however, this is not the only way to form maximal antichains, since $18,20$ could be part of some maximal antichains. I was then thinking about how to distribute the primes $2,3,5,7$ properly because $2^6 < 120 < 2^7, 3^4 < 120 < 3^5, 5^2 < 120 < 5^3, 7^2 < 120 < 7^3,$ and all the rest primes $p$ satisfy $p < 120 < p^2$ . There are $17$ primes from $2$ to $59$ , including the two ends; there are $26$ primes from $11$ to $113$ , including both ends. This question comes from a chapter dealing with Sperner's theorem. I am not sure if we could relate this question explicitly to the theorem though. How can I proceed?","['prime-factorization', 'coprime', 'combinatorics', 'prime-numbers']"
4644751,Do the lifts of commuting circle homeomorphisms commute?,"I am going over proofs of the following result concerning the rotation number of orientation-preserving homeomorphisms of the circle: If $f, g \in \text{Homeo}^+(S^1)$ commute with respect to composition (i.e. $f\circ g = g\circ f$ ), then $\rho(f\circ g) = \rho(f) + \rho(g)$ where $\rho(f) \in S^1$ is the rotation number. The Problem: From what I've seen on this site (e.g., here ), the proof involves lifting $f$ and $g$ to maps $F$ and $G$ on $\mathbb{R}$ , and then utilizing $F\circ G = G\circ F$ . But I'm having trouble seeing why the lifts must commute. My work so far: Certainly it is clear from $f\circ g = g\circ f$ that the lifts satisfy $\pi\circ(F\circ G) = \pi\circ(G\circ F)$ , where $\pi : \mathbb{R} \to S^1$ is the canonical covering map. Therefore the map $F\circ G - G\circ F$ maps $\mathbb{R}$ into $\mathbb{Z}$ . So by continuity of the lifts, there is a constant $k \in \mathbb{Z}$ such that $F\circ G = G \circ F + k$ . Thus the problem reduces to showing $k = 0$ . I can at least prove that $k \in \{-1,0,1\}$ by the following method: let $F$ and $G$ be the lifts such that $F(0),G(0) \in [0,1)$ . Then $F(G(0)) \in [F(0), F(0)+1) \subseteq [0,2)$ and $G(F(0)) \in [G(0), G(0)+1) \subseteq [0,2)$ . Immediately it follows that $|k| = |F(G(0)) - G(F(0))| < 2$ . However, I can't seem to proceed from here. It should be noted that it doesn't matter which lifts of $f$ and $g$ we pick, because if $F_1, F_2$ lift $f$ and $G_1, G_2$ lift $g$ , then there exist constants $p, q \in \mathbb{Z}$ such that $F_1 = F_2 + p$ and $G_1 = G_2 + q$ . It follows that $$
F_1 \circ G_1 = F_1 \circ (G_2 + q) = (F_1 \circ G_2) + q = [(F_2 + p)\circ G_2] + q = F_2 \circ G_2 + p + q
$$ and $$
G_1 \circ F_1 = G_1 \circ (F_2 + p) = (G_1 \circ F_2) + p = [(G_2 + q)\circ F_2] + p = G_2 \circ F_2 + p + q
$$ hence $F_1\circ G_1 = G_1 \circ F_1$ if and only if $F_2 \circ G_2 = G_2 \circ F_2$ . It should also be noted that it is absolutely necessary (at least for the linked proof) that $k = 0$ , because otherwise $F\circ G = G\circ F + k$ implies that $(F\circ G)^n = F^n\circ G^n$ plus some quadratic function of $n$ (I believe it's $k$ times the $n$ th triangular number, but I've not worked the details). The quadratic does not get eliminated once we take the rotation number limit in this case.","['general-topology', 'function-and-relation-composition', 'dynamical-systems']"
4644766,Integral that gives one or minus one,"Consider following integral $$\int_{-\infty }^{\infty } \frac{\text{sech}\left(\frac{\pi  (-a+x+1)}{c}\right)}{c} \, dx$$ with $c$ real and $a$ is a complex number of the form $e^{i\phi}$ for real $\phi$ . Looks like this integral is either 1 or -1 which is what I got from Mathematica. How do I determine the range of $a$ (or $c e^{i\phi}$ ) for which this integral is positive 1 and range for which it is $-1$ ? I suspect this is related to \begin{equation}
\frac{2 \left(\tan ^{-1}(x)+\cot ^{-1}(x)\right)}{\pi }=\pm 1
\end{equation} for complex x. But I am not quite sure.","['integration', 'improper-integrals', 'calculus', 'definite-integrals']"
4644767,A Series Representation for the Natural Logarithm Not Comprised of Exponential Terms,"Using the Riemann Integral and the fact that the indefinite integral of $\frac{1}{x}$ is $\ln|x| + C$ I get $$\int_a^b \frac{1}{x}dx = \lim_{n \to \infty} \sum_{i=1} ^{n} \frac{1}{a+\Delta xi}\Delta x=(\ln|b|+C)-(\ln|a|+C)=\ln|b|-\ln|a|$$ where $\Delta x =\frac{b-a}{n}$ Taking $\Delta x$ out of the numerator and denominator $$\lim_{n \to \infty} \sum_{i=1} ^{n} \frac{1}{\frac{a}{\Delta x} + i}\frac{\Delta x}{\Delta x}=\ln|b|-\ln|a|$$ $$\lim_{n \to \infty} \sum_{i=1} ^{n} \frac{1}{\frac{a}{\Delta x} + i}=\ln|b|-\ln|a|$$ Replacing $\Delta x$ with $\frac{b-a}{n}$ $$\lim_{n \to \infty} \sum_{i=1} ^{n} \frac{1}{\frac{a}{\frac{b-a}{n}} + i}=\ln|b|-\ln|a|$$ $$\lim_{n \to \infty} \sum_{i=1} ^{n} \frac{1}{\frac{an}{b-a} + i}=\ln|b|-\ln|a|$$ Substituting $a=1$ $$\lim_{n \to \infty} \sum_{i=1} ^{n} \frac{1}{\frac{n}{b-1} + i}=\ln|b|-\ln|1|=\ln|b|$$ Finally, replacing $b=x$ $$\ln|x| = \lim_{n \to \infty} \sum_{i=1} ^{n} \frac{1}{i + \frac{n}{x-1}}$$ As far as I'm aware there is no known series of $\ln|x|$ where none of the terms have exponents. Could this particular series be of any use in the approximation of natural logarithm values, or any other application of the natural logarithm?","['logarithms', 'functions', 'sequences-and-series', 'numerical-methods', 'exponential-function']"
4644790,Equation for genus 2 curves,"I found the following claim (in Milne, p.2 ): ... every curve of genus 2 has an equation of the form $$Y^2Z^4=f_0X^6+f_1X^5Z+\cdots +f_6Z^6.$$ I tried to check that such curves have genus $g=2$ . First by the Jacobian criterion, it has one singular point $\mathcal{O}=(0:1:0)$ . Then by the genus-degree formula, $$g=(6-1)(6-2)/2-r(r-1)/2$$ where $r$ is the ""ordinary singularity multiplicity"" of $\mathcal{O}$ (I don't understand what this means). But there is no integer $r$ such that $g=2$ , this is weird. Please show me the correct discussion.","['algebraic-curves', 'algebraic-geometry']"
4644794,Show that the set $\lbrace F(x) = \displaystyle\int_0^x f(t)dt | f \in M\rbrace$ is sequentially compact.,"Let $M$ be a bounded subset of $C([0,1])$ . Show that the set $\left\lbrace F(x) = \displaystyle\int_0^x f(t)dt | f \in M \right\rbrace$ is sequentially compact (meaning that any sequence in the set has a convergent subsequence). My idea is that: Let $\lbrace F_n \rbrace$ be the sequence in the set and fix $x \in [0,1]$ , then $\lbrace F_n(x) \rbrace$ is bounded, so by Bolzano-Weierstrass, there exists a subsequent $\lbrace F_{n_k}(x)\rbrace$ converges. So I guess if I repeat the same procedure, I may have a convergent subsequence? But for different $x$ , I may have different subsequence, so I don't know what to handle this. Can somebody please help me with this?","['functional-analysis', 'analysis', 'compactness']"
4644809,Foci of ellipse,"My question is given a ellipse of the equation : $$\frac{x^2}{a^2}+\frac{y^2}{b^2} =1$$ where $a>b$ then how we can find the coordinates of the foci. I want to find those coordinates without the presuming that the foci exists because most proofs I found online assume the properties of foci to be true and then take some extreme case to find foci's coordinates. So to rephrase my question: Given a closed curve of the equation $$\frac{x^2}{a^2}+\frac{y^2}{b^2} =1$$ prove that there exists two points inside the curve such that if we take any point on the boundary of the curve and join it to those two points then the sum of those lengths will give a fixed constant based on $a$ and $b$ (Assume $a>b$ ). Here's my attempt which gave me nothing: Let's take a point $P$ on the curve as $\left(x,\  b\sqrt{1-\frac{x^2}{a^2}}\right)$ Let those two points be $(-f,0)$ and $(f,0)$ then the sum of lengths from point $P$ becomes $$S = \sqrt{(f+x)^2+b^2\left(1-\frac{x^2}{a^2}\right)}+\sqrt{(f-x)^2+b^2\left(1-\frac{x^2}{a^2}\right)}$$ Differentiating this wrt to $f$ and equating to $0$ to find the stationary case $$\dfrac{f+x}{\sqrt{\left(f+x\right)^2+b^2\left(1-\frac{x^2}{a^2}\right)}}+\dfrac{f-x}{\sqrt{\left(f-x\right)^2+b^2\left(1-\frac{x^2}{a^2}\right)}}=0$$ Squaring and simplifying $$4bfx\left(1-\frac{x^2}{a^2}\right)=0 \implies f =0$$ which is obviously wrong... Note that in my attempt I too assumed two points, that the foci will be symmetrical and on the major axis, if we can even take these assumptions out that would be amazing. It's just that with these assumptions I was able to at least start somewhere. P.S. $\textbf{thanks to the comments by @Blue}$ differentiating $S$ wrt $x$ and equating to zero $$\dfrac{2\left(x+f\right)-\frac{2b^2x}{a^2}}{2\sqrt{\left(x+f\right)^2+b^2\cdot\left(1-\frac{x^2}{a^2}\right)}}+\dfrac{-\frac{2b^2x}{a^2}-2\left(f-x\right)}{2\sqrt{b^2\cdot\left(1-\frac{x^2}{a^2}\right)+\left(f-x\right)^2}}=0$$ Squaring and simplifying $$ a^4b^2fx(f^2-(a^2-b^2))=0$$ So, $$f= \sqrt{a^2-b^2}$$","['algebra-precalculus', 'calculus', 'geometry', 'calculus-of-variations']"
4644816,Quantifier question proof,"The following question I have been doing is below and I'm having trouble with part ii). My working out is below but I think something is wrong. Would anyone be able to help me out with what is wrong with part ii? By the way I'm new here so my mathematical writing may not be typed out the best it can. Thank you! Consider the statement $$\exists K\in \{1,2,3,4,...\}\quad\forall x < \frac{1}{K}\quad\exists k\geq K,\quad\frac{kx}{1+kx^2}>\frac{1}{2x}$$ i) What is the negation of this statement? ii) Which is true, the statement or its negation? Prove it."" My answer for part i) is $$\forall K\in \{1,2,3,4,...\}\quad\exists x < \frac{1}{K}\quad\forall k\geq K,\quad\frac{kx}{1+kx^2}\leq\frac{1}{2x}.$$ My answer for part ii) is Let $K\in \{1,2,3,4,...\}$ and further let $x=\frac{1}{\sqrt{k}} < \frac{1}{K}$ . Now suppose that we have that $k\geq K$ . Then we get the following: $$\frac{kx}{1+kx^2}\leq\frac{1}{2x}$$ when $x=\frac{1}{\sqrt{k}}$ .
Thus, the negation of the statement is true.","['logic', 'analysis']"
4644828,Show that $x_n \to x$ as $n \to \infty.$,Let $\{x_n\}_{n \geq 1}$ be a sequence of real numbers such that $\sin (t x_n) \to \sin (t x)$ for all $t \in \mathbb R.$ Then show that $x_n \to x.$ My aim is to take the power series of cosine and then differentiate it term by term which respects convergence. But for that we need to first show that $\{x_n\}_{n \geq 1}$ is bounded. But I am having hard time showing this. Could anyone please help me? Thanks a bunch!,"['convergence-divergence', 'sequences-and-series', 'real-analysis']"
4644833,Does convexity on a set implies hessian PSD on the boundary?,"Let $C \subset \mathbb{R}^N$ be a closed convex set with nonempty interior.
Let $f : \mathbb{R}^N \to \mathbb{R}$ be twice differentiable on $C$ . We also assume that it is convex on $C$ : $$ (\forall x,y \in C) (\forall \alpha \in [0,1]) \quad f((1- \alpha)x + \alpha y) \leq (1- \alpha)f(x) + \alpha f(y).$$ My main question is : can we say that the Hessian of $f$ is positive semidefinite on $C$ ? Some comments: The basic result I know holds when $C$ is an open set ; so applying such result on the interior of $C$ , we immediately get that for all $x \in {\rm{int}}~C$ , $\nabla^2 f(x) \succeq 0$ . So my question is about what happens at the boundary. What is also clear is that if I assume further that $f$ is of class $C^2$ on $C$ (and not only twice differentiable), we could pass to the limit from the interior to the boundary of $C$ to conclude that yes, $\nabla^2 f(x) \succeq 0$ for all $x \in C$ . I have been trying to find a counterexample, but every example of ""twice differentiable but not $C^2$ "" function I find is highly nonconvex, so I start to wonder if convexity and twice differentiable implies $C^2$ ? After all, it is known that: convex functions are always continuous in the interior of their domain the subdifferential of a convex function has a closed graph ; so differentiable convex functions have a continuous gradient So my secondary question is : is the hessian of a twice differentiable convex function always continous?","['continuity', 'convex-analysis', 'derivatives', 'hessian-matrix']"
4644929,Proving $\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}f(x)\sin{x}dx \geq \pi - 2$ given a twice differentiable function and an initial inequality condition,"Let $f : [-\frac{\pi}{2}, \frac{\pi}{2}] \rightarrow \mathbb{R}$ be a twice differentiable function such that $(f^{''}(x) - f(x))\tan{x} + 2f^{'}(x) \geq 1, \forall x \in (-\frac{\pi}{2}, \frac{\pi}{2})$ . Prove that $\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}f(x)\sin{x}dx \geq \pi - 2$ . The given inequality does not seem very generous, so what I tried to do, seeing that we need to obtain $\sin{x}$ and we have a $\tan{x}$ inside the inequality, was to process it by rearranging terms and eventually obtaining $f(x)\sin{x}$ on one side. Therefore: $(f^{''}(x) - f(x))\tan{x} \geq 1 - 2f^{'}(x) \Leftrightarrow (f^{''}(x) - f(x))\frac{\sin{x}}{\cos{x}} \geq 1 - 2f^{'}(x)$ At this stage, since we only really care about $I = (-\frac{\pi}{2}, \frac{\pi}{2}) \subset \mathbb{R}$ , we know that $\cos{x} > 0, \forall x \in I$ , but $\sin{x}$ takes both negative, positive and a null value, therefore we cannot multiply by it. We can, however, multiply by $cos{x}$ , unaffecting our inequality. We obtain: $(f^{''}(x) - f(x))\sin{x} \geq (1 - 2f^{'}(x))\cos{x} \Leftrightarrow f^{''}(x)\sin{x} - f(x)\sin{x} \geq \cos{x} - 2f^{'}(x)\cos{x}$ By rearranging and isolating the $f(x)\sin{x}$ terms, we finally obtain that: $f(x)\sin{x} \leq 2f^{'}(x)\cos{x} + f^{''}(x)\sin{x} - \cos{x}, \forall x \in I$ The problem with this inequality is that we have a $\leq$ , but we need a $\geq$ . Knowing this is true for all $x \in I$ , it must also remain true for every $-x \in I$ , therefore we map $x \rightarrow -x$ in our inequality, to obtain: $-f(-x)\sin{x} \leq 2f^{'}(-x)\cos{x} - f^{''}(-x)\sin{x} - \cos{x}$ Denote $J = \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}f(x)\sin{x}dx$ . It is known that $\int_{a}^{b}f(x)dx = \int_{a}^{b}f(a+b-x)dx$ . Therefore, substituting $u = -x$ in J, we obtain that $J = \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}f(-x)\sin{(-x)}dx = -\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}f(-x)\sin{x}dx$ , since sine is an odd function. However, this is where I am lost; I do not exactly know how to turn the sign from $\leq$ to $\geq$ , even with my substitution idea. Another thing I noticed is that the $f^{'}(x)\cos{x} + f^{''}(x)\sin{x}$ term can be written as $(f^{'}(x)\sin{x})^{'}$ , but the constant term $2$ is particularly annoying. Any help, hints, ideas, or solutions would be of tremendous use to me. Thank you very much for your time! :)","['integration', 'analysis', 'real-analysis', 'calculus', 'derivatives']"
4644963,Proving a trigonometric finite sum $\sum_{k=1}^N(-1)^k(\cos \frac{k\pi}{N})^{N-m}(\sin\frac{k\pi}{N})^m=(-1)^{m/2}\frac{N}{2^{N-1}}$,"How to prove this following formula? $\sum_{k=1}^N(-1)^k(\cos \frac{k\pi}{N})^{N-m}(\sin\frac{k\pi}{N})^m=(-1)^{m/2}\frac{N}{2^{N-1}}$ for m is even, and $0$ for m is odd. If we know $\sum_{k=1}^N(-1)^k(\cos\frac{k\pi}{N})^m=N/2^{N-1}$ for $m=N$ , and $=-1/2(1-(-1)^{N+m})$ for $m<N$ . I have some ideas for this problem, but all fail. I tried Abel transformation or recursion. Maybe others ways can solve, like residual theory, but I'm not familiar with it. So I ask for help about this question. From the given series, Abel transformation is easy to take. That is, one series is $(-1)^k*(\cos\frac{k\pi}{N})^{N-m}$ and the other is $(\sin\frac{k\pi}{N})^m$ . But it doesn't work, because, I don't know the truncated sum for the first series, which is a necessary condition for Abel transformation calculation. (Update on 20230224) I want to add some notes on the origin of these formulas. I encounter this identity when I want to calculate the witness for GHZ state as shown in doi:10.1103/PhysRevA.76.030305 and the following picture: I hope this note would be helpful for future readers.","['trigonometric-series', 'sequences-and-series']"
4645048,"Completeness of $\Lambda^\alpha$ in $D([0,\infty))$","I was going through this paper in which the weak $L_\alpha$ norm $$\Vert X\Vert_{\alpha,w}^\alpha = \sup_{\lambda>0} \lambda^\alpha \mathbb P\bigg[\sup_{0\leq t\leq T} |X_t|>\lambda\bigg]$$ was defined. The space $$\Lambda^\alpha :=\bigg\lbrace X \text{ cadlag and adapted } : \Vert X\Vert_{\alpha,w}<\infty\bigg\rbrace $$ was introduced. Now it was claimed that $\Lambda^\alpha$ is a complete linear metric space for all $0<\alpha<2$ . It's easy to show that this space is indeed linear, but i have some trouble verifying that it is complete. Here is my attempt to far Let $(X^{(n)})_{n\in\mathbb N}$ be a Cauchy sequence (w.r.t. $\Vert\cdot\Vert_{\alpha,w}$ ) in $\Lambda^\alpha$ , meaning for all $\epsilon>0$ and sufficiently large $M$ we have $$\epsilon> \Vert X^{(n)}-X^{(m)}\Vert_{\alpha,w}^\alpha = \sup_{\lambda>0}\lambda^\alpha \mathbb P\bigg[\sup_{0\leq t\leq T} |X_t^{(n)}-X_t^{(m)}|>\lambda\bigg]\geq \sup_{\lambda>0} \lambda^\alpha \mathbb P[|X_t^{(n)}-X_t^{(m)}|>\lambda]$$ for $n,m>M$ and each $t\in[0,T]$ . This implies that the finite-dimensional projections of $X^{(n)}$ converge weakly. Now i would have to verify some tightness criterion of the sequence $X^{(n)}$ to show that it converges weakly in $D([0,T])$ and show that its limit $X$ satisfies $\Vert X\Vert_{\alpha,w}<\infty$ . For the tightness criterion, one might use the characterization using the modulus of continuity $w'$ , meaning one has to show that $$\lim_{\delta\to 0}\limsup_{n\to\infty} \mathbb P[w'(X^{(n)},0,T,\delta)>\epsilon]=0.$$ Now by the triangle inequality we get $$\mathbb P[w'(X^{(n)},0,T,\delta)>\epsilon]\leq \mathbb P[w'(X^{(n)}-X^{(m)},0,T,\delta)>\epsilon]+\mathbb P[w'(X^{(m)},0,T,\delta)>\epsilon] \hspace{1cm} (*)$$ which i think is sufficient to show that $\lim_{\delta\to 0}\limsup_{n\to\infty} (*)=0$ . Is this proof correct? Is there an easier way to show the completeness?","['measure-theory', 'probability', 'stochastic-processes', 'skorohod-space', 'stochastic-calculus']"
4645077,Is there a name for the relationship between functions $f(x)$ and $g(x)$ satisfying $T(f(x))=g(T(x))$ for some $T(x)$?,Is there an established name for the relationship between functions $f(x)$ and $g(x)$ satisfying $T(f(x))=g(T(x))$ for some $T(x)$ ? It is similar to an isomorphism and it seems that it might be useful since it follows by induction that $T(f^n(x))=g^n(T(x))$ . One thing that this formula can be used for is a kind of fun derivation of the formula for the sum of a geometric series.  If f(x)=ax+1 then $1+a+a^2+...+a^n = f^n(1)$ . Let g(x)=ax. Then if T(x)= x +1/(a-1) we get $T(f(x))=g(T(x))$ and $f^n(1)+1/(a-1)=a^n(1+1/(a-1))$ . Solving for $f^n(1)$ gives the formula.,['functions']
4645195,"Solving master equation for binding kinetics $\frac{dp_m(t)}{dt}=a\,(N-m+1)(M-m+1)p_{m-1}(t)+b\,(m+1)p_{m+1}(t)-[b\,m+a(N-m)(M-m)]p_m(t)$","Background : This problem appears when you try solving the stochastic binding kinetics . There are a total of $M$ and $N$ molecules of the types $R$ and $L$ . These molecules can be either in a bound ( $RL$ ) or an unbound state. Each pair of unbound $R$ and $L$ molecules can bind with the rate $a$ , and each bound molecule $RL$ can unbind with the rate $b$ . Starting with $m_0$ initially bound pairs, I would like to find $p_m(t)$ , the probability of having $m$ molecules in the bound state $RL$ at time $t$ . Problem : The following set of coupled differential equations (also known as Master equations) describe the dynamics of the probabilities $p_m(t)$ as functions of time $$
\frac{dp_m(t)}{dt}=a\,(N-m+1)(M-m+1)p_{m-1}(t)+b\,(m+1)p_{m+1}(t)-[b\,m+a(N-m)(M-m)]p_m(t),
$$ where $M$ and $N$ are integers with $0<M\leq N$ and $a>0$ and $b>0$ are real numbers, for integer $m$ s between $0\leq m\leq M$ , with the initial conditions $p_m(0) = \delta_{m,m_0}$ , $0\leq m_0\leq M$ . Assume $p_{-1}(t)=p_{M+1}(t)=0$ . I would like to solve these equations. What I tried so far :
We can write the set of ODEs as a Matrix equation of the form $$
\frac{d}{dt}\left(\begin{array}{c}
p_0\\
\vdots\\
p_M
\end{array}\right)=\underbrace{\left(\begin{array}{cccc}
-aNM&b &0&0&\dots\\
aNM&-b-a(N-1)(M-1) &2b&0&\dots\\
&\vdots
\end{array}\right)}_{\mathbf{A}}\left(\begin{array}{c}
p_0\\
\vdots\\
p_M
\end{array}\right)
$$ with the formal solution $\mathbf{p}(t)=e^{\mathbf{A}t}\mathbf{p}(0)$ . But in order to exponential the matrix $t\mathbf{A}$ , I need to diagonalize $\mathbf A$ , but I don't know how to do that. It is a tridiagonal matrix, each column adds up to one, and it has a simple form. I'm sure someone on SE knows how to diagonalize this. Alternatively, I tried solving the problem using a generating function defined as $$
f(z,t) = \sum_{m=0}^M p_m(t) z^m.
$$ You can differentiate $f$ with respect to $t$ and find the following PDE satisfied by $f$ : $$
\partial_t f = a(z-1)\left[MNf-\left((M+N-1)z+\frac{b}{a}\right)\partial_zf+z^2\partial^2_zf\right],
$$ with the initial condition $f(z,0)=z^{m_0}$ .
But I do not know how to solve this equation either. It is a second-order linear PDE with a solution that is a polynomial in $z$ , so it should be solvable. Steady State Results : Binding kinetics has an equilibrium solution that is given by Boltzmann distribution with the partition function $$\mathcal{Z} = \sum_{m=0}^M \left(\frac ba\right)^m {M \choose m}{N \choose m}={}_2F_1(-M,-N,1,b/a).$$ This gives $$p_m(\infty) = \frac{\left(\frac ba\right)^m {M \choose m}{N \choose m}}{{}_2F_1(-M,-N,1,b/a)}.$$ What I need : Ideally looking for a closed-form solution for either $p_m(t)$ or $f(z,t)$ . If there is no closed-form solution, at least an explicit expression for $p_m(t)$ that could involve special functions or maybe even a sum or series, but something that I can explicitly calculate. I'm not looking for a numerical solution.","['markov-chains', 'stochastic-processes', 'linear-algebra', 'partial-differential-equations', 'probability']"
4645216,Compute the integral $\int_{-\infty}^\infty \frac{\sin(x) (1+\cos(x))}{x(2+\cos(x))} dx$,"Background I recently found out about Lobachevsky's integral formula, so I tried to create a problem on my own for which I'd be able to apply this formula. The problem is presented below. Problem Compute the integral $\int_{-\infty}^\infty \frac{\sin(x) (1+\cos(x))}{x(2+\cos(x))} dx$ Attempt If we define $f(x) := \frac{1+\cos(x)}{2+\cos(x)}$ , which most definitely is $\pi$ - periodic. The integral is, using our notation above, on the form $$I = \int_{-\infty}^\infty \frac{\sin(x)}{x}f(x) dx.$$ The integrand is even, so we might as well compute $$ I = 2 \int_{0}^\infty \frac{\sin(x)}{x}f(x) dx.$$ We will now have to make use of a theorem. Lobachevsky's integral formula states that if $f(x)$ is a continous $\pi$ - periodic function then we have that $$ \int_0^\infty \frac{\sin(x)}{x}f(x) dx= \int_0^{\pi/2} f(x) dx.$$ Substituing our $f(x)$ yields us $$ \int_0^{\pi/2} \frac{1+\cos(x)}{2+\cos(x)} dx = \pi/2 - \int_0^{\pi/2}\frac{1}{2+\cos(x)}dx $$ where $$I_2 = \int_0^{\pi/2}\frac{1}{2+\cos(x)}dx = \int_0^{\pi/2}\frac{\sec^2(x/2)}{3+\tan^2(x/2)}dx.$$ Letting $ u = \tan(x/2)/\sqrt{3}$ , for which $du = \sec^2(x/2)/(2\sqrt{3})dx$ , therefore gives us: $$ I_2 = \int_0^{1/\sqrt{3}}\frac{2\sqrt{3}}{3u^2+3} = \frac{\pi}{3\sqrt{3}}.$$ Finally we can compute $I$ to $$I = 2\left(\frac{\pi}{2} - \frac{\pi}{3\sqrt{3}}\right) = \frac{\pi(3\sqrt{3}-2)}{3\sqrt{3}}.$$ I've tried calculating this integral in Desmos where it gives me $0$ when I calculate the integrand on the interval $(-\infty, \infty)$ , and something negative for $(0,\infty)$ . This contradicts my answer. I also tried typing it into Wolfram, without success. Can anyone confirm the validity of my result?","['integration', 'improper-integrals', 'real-analysis']"
4645256,Show that $G$ is a subgroup of $\operatorname{GL}_3(\mathbb R)$ and $G \cong \mathbb R^2 \rtimes (\mathbb R^\times \times \mathbb R^\times)$,"This is exercise $3-5$ from Milne's group theory book. $\DeclareMathOperator{\GL}{GL}$ $\newcommand{\R}{\mathbb R}$ Problem. Let $G$ be all matrices in $\GL_3 (\R)$ of the form $$\begin{pmatrix} a & 0 & b\\ 0 & a & c \\ 0 & 0 & d \\ \end{pmatrix},$$ where $ad\neq 0$ . Check that $G$ is a subgroup of $\GL_3 (\R)$ and that it is a semidirect product of $\R^2$ (additive group) and $\R^\times \times \R^\times$ . Is it a direct product of these two groups? Attempt. First, to check $G$ is a subgroup of $\GL_3 (\R)$ we can take a matrix $A \in G$ and find that its inverse is given by $$A^{-1} = \begin{pmatrix} a^{-1} & 0 & e\\ 0 & a^{-1} & f \\ 0 & 0 & d^{-1}\end{pmatrix},$$ where $e=-a^{-1}bd^{-1}$ and $f= -a^{-1}cd^{-1}$ . Then straightforward matrix multiplication shows if $A,B \in G$ then $AB^{-1}\in G$ , so by the subgroup test $G$ is a subgroup. Now to show $G  \cong \R^2 \rtimes (\R^\times \times \R^\times)$ it suffices to find $N \cong \R^2$ and $Q \cong \R^\times \times \R^\times$ such that \begin{align*}
    N &\vartriangleleft G;\\
    NQ &= G;\\
    N \cap Q &= \{1\}.\\
\end{align*} Let $N$ be all matrices of the form $$\begin{pmatrix} a & 0 & 0 \\ 0 & a & 0 \\ 0 & 0 & d \end{pmatrix}$$ such that $ad \neq 0$ , and let $Q$ be all matrices of the form $$\begin{pmatrix} 1 & 0 & b \\ 0 & 1 & c \\ 0 & 0 & 1\end{pmatrix},$$ such that $b,c \in \R^\times$ . Then we can identify $N$ with $\R^2$ and $Q$ with $\R^\times \times \R^\times$ in the obvious way, and we see that the three conditions for semidirect product are also satisfied. Therefore $G \cong \R^2 \rtimes (\R^\times \times \R^\times)$ as desired. Note that $G$ cannot be a direct product of these two subgroups because, e.g., $Q$ is not normal in $G$ . Is this correct?","['matrices', 'abstract-algebra', 'linear-algebra', 'solution-verification', 'group-theory']"
4645299,Differentiating between points and their coordinates,"In Croatian mathematical textbooks it is customary to use the notation $A(x,y)$ to mean that $(x,y)$ are the coordinates of point $A$ . The coordinates of the point are clearly differentiated from the point itself. However, I have never encountered this notation in foreign (English) literature. Rather, I’ve seen this written as $A=(x,y)$ . This confuses me because if interpreted literally the notation says that point $A$ is the ordered pair of coordinates $(x,y)$ with no differentiation made between the two whatsoever. Is this notation shorthand or are points and ordered pairs of real numbers indistinguishable in this paradigm? I get that it makes no difference in practice since every n-dimensional Euclidean space is isomorphic to $\mathbb{R}^n$ but I’m still curious as to the precise meaning of this notation. If the notation should be taken literally, i.e. points are tuples, what notation should be used to express coordinates for points in other Euclidean spaces? Any insight is appreciated :)","['notation', 'coordinate-systems', 'analytic-geometry', 'geometry']"
4645301,Derive Laplacian in polar coordinates using covariant derivative,"In cartesian coordinates since the metric components $g^{ab}$ are constant, we know that $\partial_c = \nabla_c$ where the RHS is the covariant derivative. So we can write the laplacian in cartesian coordinates as $\nabla^2 \phi = g^{ab} \nabla_a \nabla_b \phi$ . I am told that this is invariant under change of coordinates. How would I calculate this quantity in polar coordinates? The best I've got is $\nabla^2 \phi = \frac{1}{r^2}\nabla_{\theta} \nabla_{\theta} \phi + \nabla_{r} \nabla_{r} \phi$ . I know what the laplacian should look like under polar, but I cannot see how to simplify further from here. I can find the christoffel symbols but how do I apply them to a scalar field?","['coordinate-systems', 'derivatives', 'differential-geometry']"
4645314,Topological conjugacy of dynamical systems knowing all there periodic orbits,"Let's assume we have a discrete dynamical system with $M \subset \mathbb{R}^n$ and $$x_t=f(x_{t-1}), \text{ where } f: M \rightarrow M \text{ continuous} $$ Let's further assume we know all its cyclic points, i.e. we know all $$x^*=f^k(x^*) \quad \forall k$$ . Is this information enough to know the topological conjugacy class of the dynamical system? If this information is not enough, would it be enough if we further know that the map only has finitely many cyclic points?","['fixed-point-theorems', 'functions', 'general-topology', 'nonlinear-dynamics', 'dynamical-systems']"
4645326,"Finding $(x, y)$ as a function of $n$ radians","For a joystick thing in a game, I need to find where a line with an angle of $n$ radians intersects with a square of width and height of $2$ around the joystick with a radius of 1. Right now I have a simple sin cos function that only gets the coordinates on the circle and not the square. $$
f(n) = [\sin(n), \cos(n)]
$$ What function is used to find a vector of $x$ and $y$ as a function of $n$ radians that is on the square?","['trigonometry', 'functions']"
4645380,Smallest constant $C$ to bound $\mathbb{E}[\mathrm{tr}((\overline{X}_n + I)^{-1})] \leq C~\mathrm{tr}((\mathbb{E}[X] + I)^{-1})$?,"Let $X_1, \dots, X_n$ be random real-valued symmetric rank-one matrices, $$
X_i = x_i \otimes x_i, 
$$ where $x_i$ are such that the standard Euclidean norm satisfies $\|x_i\|^2 \leq a$ almost surely.
Assume that $x_i$ are independently and identically distributed and let $M = \mathbb{E}[X_i] = \mathbb{E}[x_i \otimes x_i]$ , denote their common mean. Define their average $\overline{X}_{n} = n^{-1} S_n$ where $S_n = \sum_{i=1}^n X_i$ . Let $f(T) := \mathrm{trace}((T + I)^{-1})$ . Question: What is the smallest constant $C = C_d(a, n) \geq 1$ such that we have $$
\mathbb{E}[f(\overline{X}_n)] \leq C~f(M)?
$$ It should be emphasized that the constant $C$ is universal: it is valid for any law of $x_i$ , supported on the Euclidean ball of (squared) radius $a$ . It should be dependent only on $a, n$ and the dimension $d$ . Comments: Necessarily $C \geq 1$ . Note that by Jensen's inequality, we have the following inequality, $
\mathbb{E}[f(\overline{X}_n)] \geq f(M), 
$ since $f$ is a convex function on the symmetric positive definite matrices. In the case $d = 1$ , I was able to solve this problem and calculate the extremal distribution. See this post .","['positive-semidefinite', 'trace', 'expected-value', 'inequality', 'probability']"
4645402,Characterization of Connected Components for Compact Topological Spaces,"I'm trying to prove the following statement from James Dugundji's Topology (XI - section 2 - problem 6). Let $X$ be compact (Hausdorff). Define an equivalence relation $R$ as follows: $xRy$ if for every continuous $f\colon X\to E^1$ such that $f(x)=0$ , $f(y)=1$ , there is a $u\in X$ with $f(u)=\frac{1}{2}$ . Show that the equivalence classes are the components of $X$ . (I'll denote the equivalence class of $x$ as $[x]$ and it's component as $C(x)$ ). I want to show that $[x]=C(x)$ for each $x\in X$ . Showing $C(x)\subseteq [x]$ is easy: let $y\in C(x)$ and let $f\colon X\to E^1$ be continuous with $f(x)=0$ and $f(y)=1$ . Since $C(x)$ is connected, $f\big(C(x)\big)$ is connected and thus $[0,1]=[f(x),f(y)]\subseteq f(C(x))$ . It is then clear that $C(x)\subseteq [x]$ . In order to prove $[x]\subseteq C(x)$ , I want prove that $[x]$ is connected. I have no idea how to prove this. We haven't used compactness yet so clearly this is the hard part of the problem. I'm looking for some hints on how to proceed What are some hints in order to prove $[x]$ is connected? Update: I'm now looking for hints, partial solutions, complete solutions, or anything that might allow me to eventually read a proof (either written by me or someone else) of the property. Definitions. A space $X$ is Connected if it can't be decomposed into two disjoint open sets (i.e. no continuous $f\colon X\to \{0,1\}$ is surjective). A connected component for a point $x$ (written $C(x)$ ) is defined as the union of all connected sets containing $x$ . $E^1$ denotes the first Euclidean space: $(\mathbb{R},\mathscr{T})$ where $\mathscr{T}$ is the usual topology. $[x]=\{y\in X\mid \forall f\in C(X), [(f(x)=0\land f(y)=1)\implies (\exists u\in X,\; f(u)=1/2)]\}$ .","['connectedness', 'general-topology', 'compactness']"
4645410,"If infinitely many equidistant points on a circle are connected in every possible way, will there be a line that crosses the center of the circle?","I've been thinking about the fact that if an odd number of points are placed on a circle equidistantly (same arc length between points) and these points are connected in every possible way, there will not be any line that cross the center of the circle, no matter how many points there are if this number of points is odd. Only if there are an even number of points there can be lines connecting points that cross the center of the circle. But what about when there are infinite points? One can think that if we divide a circle into infinitely equidistant points and connect these points in every possible way, all the internal points of the circle will be crossed by some line, but this is not true given that only an even number of points are able of generate a line that crosses the center of the circle. My question is: is ""infinity"" even or odd? If someone were to divide the circle forever and connect the points in every possible way, in the end would the center be crossed or not? EDIT:
In cartesian coordinates, points on a unitary circle are given by: $$P =\left(cos\left(\theta\right),sin\left(\theta\right)\right) $$ Since the circle has been divided into N pieces, the angle between each point is: $$\theta = \frac{2\pi}{N}$$ If you say some point A has angle $\phi = \frac{n2\pi}{N}$ where n can be any number from zero to N-1, and point B has angle $\gamma = \frac{\left(n+k\right)2\pi}{N}$ where k can be any number from one to N-1, you end up with the condition: $$k = \left(2p+\frac{1}{2}\right)N$$ For the line between the points A and B to be equal to the diameter of the circle. if k and N are integers, then it means N needs to be even. That's why this question popped into my head.
Here p is an integer that can vary from zero to infinity.","['geometry', 'infinity']"
4645419,PMA Rudin Theorem 7.26: Change of variable,"After $(51)$ , Rudin changes the variable $P_n(x) = \int_{-1}^{1} f(x+t)Q_n(t)dt$ $\to$ $P_n(x) = \int_{-x}^{1-x} f(x+t)Q_n(t)dt$ How does he change it? Update: Now I know It's not the change of variable And I know that $f(x) = 0$ except $x \in [0,1]$ , but I want to give a rigorous process to convince me $$\int_{-1}^{1} f(x+t)Q_n(t)dt = \int_{-x}^{1-x} f(x+t)Q_n(t)dt$$ Update: Equivalent to prove $$\int_{-1}^{-x} f(x+t)Q_n(t)dt + \int_{1-x}^{1} f(x+t)Q_n(t)dt = 0 $$ Proof : $x+t \in [x-1,0]$ in the first integral, $x+t \in [1,x+1]$ in the second, because $x \in [0,1]$ , so both of these $x+t$ outside of $[0,1]$ , so both of integrals are $0$ .","['integration', 'definite-integrals', 'analysis', 'real-analysis', 'calculus']"
4645421,Formula for $\int_0^{\infty} \frac{\ln \left(x^4+a^4\right)}{b^2+x^2} d x =\frac{\pi}{ b} \ln \left(a^2+b^2+a b \sqrt{2}\right) $,"In my post , I had proved that $$
\int_0^{\infty} \frac{\ln \left(x^2+a^2\right)}{b^2+x^2} d x=\frac{ \pi}{b} \ln (a+b) \tag*{(*)} 
$$ To go further, I guess that $$\int_0^{\infty} \frac{\ln \left(x^4+a^4\right)}{b^2+x^2} d x =\frac{\pi}{ |b|} \ln \left(a^2+b^2+|a|| b| \sqrt{2}\right) $$ Proof: For $a,b>0$ , Using $\ln \left(a^2+b^2\right)=2 Re(\ln (a+b i))$ , we can reduce the power $4$ to $2$ . $$
\begin{aligned}
\int_0^{\infty} \frac{\ln \left(x^4+a^4\right)}{b^2+x^2} d x
& = 2\int_{0}^{\infty} \frac {Re\left[\ln \left(x^2+a ^2i\right)\right]}{b^2+x^2} d x \\
& =2  Re\left(\int_0^{\infty} \frac{\ln \left(x^2+\left[\left(\frac{1+i}{\sqrt{2}}\right) a\right]^2\right)}{b^2+x^2} d x\right)
\end{aligned}
$$ Using (*), we have $$
\begin{aligned}\int_0^{\infty} \frac{\ln \left(x^4+a^4\right)}{b^2+x^2} d x&=2 Re\left[\frac{\pi}{b} \ln \left(\frac{1+i}{\sqrt{2}} a+b\right)\right] \\&=\frac{2 \pi}{b} R e\left[\ln \left(\frac{a}{\sqrt{2}}+b+\frac{a}{\sqrt{2}}i\right)\right] \\&= \boxed{\frac{\pi}{b} \ln \left(a^2+b^2+a b \sqrt{2}\right)}\end{aligned}
$$ In general, for any $a, b \in \mathbb{R} \backslash\{0\}$ , replacing $a$ and $b$ by $|a|$ and $|b|$ yields $$\boxed{\int_0^{\infty} \frac{\ln \left(x^4+a^4\right)}{b^2+x^2} d x =\frac{\pi}{ |b|} \ln \left(a^2+b^2+|a|| b| \sqrt{2}\right) }$$ For example, $$
\int_0^{\infty} \frac{\ln \left(x^4+16\right)}{9+x^2} d x= \frac{\pi}{3} \ln (13+6 \sqrt{2})
$$ Comments and alternative methods are highly appreciated.","['integration', 'improper-integrals', 'calculus', 'definite-integrals']"
4645428,Is there a method to factor this equation?,This equation appears in a question book but the method for factoring (if there is one) is not familiar to me. $$x^3 + x^2 -4x -4 $$ The solution manual lists the following as the factorised form of the above equation: $$x^2(x + 1) - 4(x + 1)$$ What is the method for achieving this? Thanks,"['algebra-precalculus', 'factoring']"
4645436,"Can we show that for every $\delta>0$, there exist constants $\alpha>0, \beta>0$ so that the following inequality holds with high probability?","Consider two $n-$ dimensional random vectors $u$ and $v$ uniformly distributed on the sphere. Define $X_n :=u\cdot v$ . Note that as $n\to \infty$ , $\sqrt{n}X_n \to N(0,1)$ as $n\to \infty$ . Fix $\epsilon>0$ (very small). Can we show that for every $\delta>0$ , there exist constants $\alpha>0, \beta>0$ so that $$
\lim_{n\to \infty}P\left(\frac{X_n^{-2}-1}{\epsilon^{-2}-1}<\alpha n^\beta\right)\ge 1-\delta.
$$ Or can we revise this upper bound for $\frac{X_n^{-2}-1}{\epsilon^{-2}-1}$ depending on $n$ . Since we know that the order $X_n=O_p(n^{-1/2})$ , then the order of $\frac{X_n^{-2}-1}{\epsilon^{-2}-1}$ is about $O_p(n)$ . But I am stuck on how get the strict upper bound. (Maybe this question would be helpful: Can we find $C>1$ so that $ P(|X|\le \frac{\epsilon}{C})\ge 1-\delta $? ) Let $Y\sim N(0,1)$ (hence we can write $X_n=n^{-1/2}Y$ . Note that $$\begin{align*}
\lim_{n\to \infty}P\left(\frac{X_n^{-2}-1}{\epsilon^{-2}-1}<\alpha n^\beta\right)&=P\left(n\frac{Y^{-2}-1}{\epsilon^{-2}-1}<\alpha n^\beta\right)\\&=P\left(nY^{-2}<\alpha(\epsilon^{-2}-1)n^{\beta}+1\right)\\&=P\left(Y^2>\frac{n}{\alpha(\epsilon^{-2}-1)n^{\beta}+1}\right)\end{align*}
$$ I am not sure if we can apply the concentration result of the Gaussian variable to find proper $\alpha, \beta>0$ so that this probability larger than $1-\delta$ .","['inequality', 'statistics', 'probability', 'real-analysis']"
4645445,Alternates to the Neyman-Pearson Lemma?,"I was reading about the Neyman-Pearson Lemma ( https://en.wikipedia.org/wiki/Neyman%E2%80%93Pearson_lemma ). Let $X_1, X_2, \ldots, X_n$ be a random sample from a distribution with unknown parameter $\theta$ . Consider two hypotheses: Null hypothesis $H_0$ : $\theta = \theta_0$ Alternative hypothesis $H_1$ : $\theta = \theta_1$ ( $\theta_1 \neq \theta_0$ ) Let $f(x|\theta)$ denote the probability density function (or probability mass function) of $X_1, X_2, \ldots, X_n$ . The Neyman-Pearson lemma states that the likelihood ratio test (LRT) is the most powerful test at level $\alpha$ , where the LRT rejects $H_0$ if and only if: $$\frac{L(\theta_1 | \mathbf{x})}{L(\theta_0 | \mathbf{x})} > k,$$ where $\mathbf{x} = (x_1, x_2, \ldots, x_n)$ is the observed sample, $L(\theta|\mathbf{x}) = \prod_{i=1}^n f(x_i|\theta)$ is the likelihood function, and $k$ is a constant chosen such that the test has level $\alpha$ . My Question: This might sound like a rhetorical question - but what other possible tests  could exist? The Neyman-Pearson Lemma is said to be very important as it shows us what is the most powerful statistical test - but it seems to me that the test indicated by the Neyman-Pearson is practically the only test that exists. What other tests can be considered as competitors? Thanks!","['statistics', 'probability']"
4645456,Can you make a Set Theory definition of tuples that has limited depth?,"I was thinking about the set theory definitions of ordered pairs, such as $(a,b)=\{\{a\},\{a,b\}\}$ . And I think people usually add elements to make longer ordered tuples using the definition $(a,b,c)=((a,b),c)$ ; $(a,b,c,d)=((a,b,c),d)$ ; etc. But by these definitions, as you make longer and longer ordered tuples, you get deeper and deeper sets representing them, like $(a,b)$ has depth $2$ , $(a,b,c)$ has depth $4$ , $(a,b,c,d)$ has depth $6$ , etc. Is it possible to make definitions for ordered pairs and tuples so that all tuples' depths are less than a constant $M$ ? I do not want to use any non-set elements. However, in that case, there's a limited number of possible sets with less than depth $M$ , while there's an unlimited number of possible tuples. Therefore, we will ignore the depth within a tuple's own elements and pretend that they have $0$ depth. Like if $a$ is a set that has depth $M+1$ , that does not mean $(a,b)$ 's set form will break the limit, because we will pretend $a$ has $0$ depth. However, if $a$ is used in $(b,c)$ 's set form, then in that case $a$ will have $M+1$ depth and it will break the limit.","['elementary-set-theory', 'definition']"
4645463,Reasoning behind intuitive understanding of specific equivalence class $a \sim b \iff f(a) = f(b)$,"I was attempting this question the other day: Let $f:A \rightarrow B$ be a surjective map of sets. Prove the relation $$a \sim b \iff f(a) = f(b)$$ is an equivalence relation whose equivalence classes are the fibres of $f$ . In principle this is easy to solve. Reflexivity, Symmetry and Transitivity are all straightforward since $f(a) = f(a)$ $f(a) = f(b) \iff f(b) = f(a)$ $f(a) = f(b) \wedge f(b) = f(c) \rightarrow f(a) = f(c)$ Any fibre of $f$ is $\{a\in A|f(a) = b\}$ and since the right inverse $h$ exists, $b$ can be written as $f(c)$ . $f(a) = f(c) \iff a\sim b$ . Simple enough? Except this all sounded a little too straightforward and I couldn't really understand in particular how the equivalence classes are the same as the fibres of $f$ . So I thought up this lil visual representation. All the elements of $B$ are occupied i.e. the codomain is the range. Sometimes multiple elements of $A$ point towards a single element of $B$ . That is allowed by our required surjectivity. Now every time $f(x) = f(y)$ for two elements $x,y \in A$ , $x \sim y$ . $x$ and $y$ share an equivalence class by this definition. I feel this captures the intrinsic symmetry of equivalence classes well. $x$ and $y$ are ""equivalent"" in that there is a certain indistinguishability about them. They are identical in the one way we care about, that applying $f$ to either yields the same result. Within that little coloured bubble inside $A$ , they are all the same. I think this best shows how the equivalence classes are the fibres of $f$ . P.S. you can also see how all the equivalence classes form a partition of $A$ . Am I correct in everything above? In particular, I still do not understand why $f$ must be surjective because while it was an integral part of the proof, I could always add a few extra harmless elements to $B$ , invalidating the surjective property of $f$ , and the equivalence classes would remain the fibres of $f$ .","['elementary-set-theory', 'equivalence-relations']"
4645476,"Prove that $X := \{ f: [0,1] \to [0,1] : f \text{ is continuous and } f(1) = 0 \}$ with the given distance is neither connected nor separable","Prove that $X := \{ f: [0,1] \to [0,1] : f \text{ is continuous and } f(1) = 0 \}$ with $d(f,g) := \inf \{r \geq 0 : f(t) = g(t) \forall r ≤ t ≤ 1 \}$ is neither connected nor separable. Here is my work so far: let $y \in [0,1]$ and consider the function $f_y \in X$ that is $y$ on $[0,99/100]$ and is linear and continuous on $[99/100 , 1]$ . By construction, $f_y \in X$ for all $y \in [0,1]$ . If we assume that $X$ is separable, then there exists a countable dense subset $\\{g_n\\}$ , so that for any given $y \in [0,1]$ and $\epsilon > 0$ there exists some $g_n$ such that $d(f_y,g_n) < \epsilon$ , so that $f_y(x) = g_n(x)$ for all $x \in [\epsilon,1]$ . By looking at the values $g_n(98/100)$ , we see that $\\{ g_n(98/100) \\} = [0,1]$ , a contradiction. Does this work look okay? I was also am lost on how to prove that $X$ is disconnected. I cannot think of a disconnection of $X$ .","['connectedness', 'separable-spaces', 'metric-spaces', 'real-analysis', 'functional-analysis']"
4645492,"Can someone prove that the sequence $1,\tan(1),\tan(\tan(1)),\ldots$ is infinite? If so, can someone prove if it diverges or converges?","Can someone prove that the sequence $1, \tan(1),\tan(\tan(1)),\ldots$ is infinite? If so, can someone prove if it diverges or converges? Intuitively, this sequence is infinite as it's hard to hit exactly at $n\pi+\frac{1}{2}\pi$ , but how can someone prove that? Also, is there a solution of the sequence $A, \tan(A), \tan(\tan(A)),\ldots$ for any given value $A$ ? The context of this question is on a Zhihu question where someone asked if there is any sequence that one do not know if it converges or diverges and someone answers with this sequence. I believe that this sequence might finally converge to one of the stationary points where $\tan(x)=x$ , but I have no idea  which one it will end up for any given value $A$ .","['trigonometry', 'convergence-divergence', 'sequences-and-series']"
4645543,"Product of limit, sin, infinity, error?","Hello I would like to know if there is a mistake :
I have to show that for any $t\geqslant0$ fixed $$\lim_{n\to \infty}\sin\sqrt{t+4\pi n^{2}}=0$$ That's what I said, Since $\sin(\cdot)$ is continuous and $$\sqrt{t+4\pi n^{2}}=2n\pi\sqrt{1+\frac{t}{4\pi n^{2}}}$$ for $n\geqslant1$ . $$\lim_{n\to \infty}\sin\sqrt{t+4\pi n^{2}}=\lim_{n\to \infty}\sin\left(2n\pi\sqrt{1+\frac{t}{4\pi n^{2}}}\right)$$ $$=\sin\left(\lim_{n\to\infty}2n\pi\sqrt{1+\frac{t}{4\pi n^{2}}}\right)=\\=\sin\left(\lim_{n\to\infty}2n\pi\cdot\lim_{n\to\infty}\sqrt{1+\frac{t}{4\pi n^{2}}}\right)=$$ $$=\sin\left(\lim_{n\to\infty}2n\pi\right)=\lim_{n\to\infty}\sin 2n\pi=\lim_{n\to\infty}0=0$$","['limits', 'sequences-and-series', 'infinity', 'real-analysis']"
4645563,"Excercise from Isaacs' book ""Character theory of Finite Groups""","I am trying to prove a statement from Isaacs' book ""Character theory of Finite Groups"" (exercise $8.5$ ). Let $\chi \in \operatorname{Irr}(G)$ and suppose $|G| = mn$ , with $(m, n) = 1$ . Assume that $\chi(x) = 0$ for all $1 \neq x \in G$ such that $x^{n} = 1$ . Suppose $y \in G$ and $y^{m} \neq 1$ . Show that $\chi(y) = 0$ . I tried to solve this problem like this: Since $y^m\neq 1$ , we can say that there is at least one prime divisor of $n$ that is a divisor of $|y|$ . $[\chi, \chi] = \frac{1}{|G|}\sum\limits_{x \in G}\chi(x)\overline{\chi(x)} = \frac{1}{|G|}\sum |\chi(x)|^{2} = 1$ In the second equality, the sum is taken over those $x\in G : |x|\not\mid \; n$ . Let 's try to use the following theorem: Let $\chi \in \operatorname{Irr}(G).$ Let $\exists \; p \in \mathbb{P}$ such that $p$ doesn't divide $\frac{|G|}{\chi(1)}$ . Then $\chi(g) = 0$ for all $g \in G$ such that $p \; | \; |g|$ . If $n \; |\; |y|$ , then our statement will be fulfilled. If $\exists \; p \in \mathbb{P}$ such that $p$ doesn't divide $\frac{|G|}{\chi(1)}$ and $p \; | \; |y|$ , then our statement will be fulfilled. If $\chi(1) = |G|$ , then our statement will be fulfilled Unfortunately, I'm stuck in this place, and I don't know what to do next to solve this problem. I will be grateful for any help!","['group-theory', 'finite-groups', 'characters']"
4645582,Prove the existence of a point $c$.,"Problem Let $f$ be a continuous function, $f:[0,1]\to\mathbb{R}$ with $\int_{0}^1 (2x-1)f(x) dx = 0$ . Prove that there exists a point c between $(0, 1)$ such that $\int_{0}^c (x-c)(f(x)-f(c)) dx = 0$ . This problem was given at a regional competition in Romania for $12$ th grade students. Attempt From Rolle's Theorem we have that there exists q such that $f(q) = (2q - 1)f(q) = 0$ , where we have $f(q)=0$ . My other attempt was: fix $p$ to $0.5$ and separate according to the Mean Value Theorem for Integrals $\int_{0}^1 (2x-1)f(x) dx = 0$ into $-f(c1) * \int_{0}^p (1-2x) dx$ $+ $ $f(c2) * \int_{p}^1 (2x-1) dx = 0$ where we get that $f(c1)=f(c2)$","['integration', 'rolles-theorem', 'definite-integrals', 'real-analysis']"
4645608,Why would the |f(x)| be non differentiable for x belonging to Real Numbers?,"I'm just started calculus and come across a statement in 1 of my books ( VG Advanced Problems in Mathematics ) that goes like: If $y = f(x)$ is differentiable for x belonging to the set of Real numbers, then $y=|f(x)|$ is not differentiable for all x belonging to the set of Real numbers. So what I inferred initially is that modulus function are continuous but non differentiable at the point where $f(x)=0$ (where $f(x)=|x|$ ofc) as the left hand derivative and the right hand derivatives are different. But then I know if $f(x)$ = $|x^3|$ at $x=0$ the left and the right hand derivative would be defined and equal, thus being defined for all x belonging to Real numbers. And moreover $x^3$ is inside the modulus ( which ummm... is not defined at $f(x)=0$ as modulus functions work that way ? ). In short how can the statement written in my book be correct if $f(x)=|x^3|$ ?","['graphing-functions', 'continuity', 'calculus', 'derivatives', 'piecewise-continuity']"
4645609,"Devising an analytical approach to solving two infinite (improper) integrals of type $\int_0^\infty f(u) J_\mu (au) J_\nu(bu) \,\mathrm{d}u$","While I was elaborating on a fluid mechanical problem, I came across the two following functions defined in terms of non-trivial infinite (improper) integrals over the wavenumber $u$ in the interval $(0,\infty)$ $$
\psi_\alpha (a,b) = \int_0^\infty \frac{u^\frac{3}{2}}{\left( u^2+\alpha^2 \right)^{\frac{1}{2}}} \, J_0 (au) J_{\frac{3}{2}}(bu) \,\mathrm{d}u
$$ and $$
\phi_\alpha (a,b) = \int_0^\infty \left( \frac{u^2+\alpha^2}{u} \right)^{\frac{1}{2}} J_1 (au) J_{\frac{5}{2}}(bu) \,\mathrm{d}u
$$ wherein $\alpha > 0$ is a small parameter, and $a>0$ and $b>0$ are the variables.
Those two improper integrals arose while solving dual integral equations using the well-established analytical approaches devised by $\,$ S n e d d o n $\,$ and $\,$ C o p s o n. It can easily be demonstrated that those two integrals are convergent.
In particular, for $\alpha = 0$ , the corresponding expression can readily be determined analytically.
They are found to depend on whether $a<b$ or $a>b$ . In the general case of interest, however, I am unable to figure out how to proceed. I have tried to use the series or integral representations of the Bessel functions and try to make an analytical progress but unfortunately this did not seem to work. Any help is highly appreciated. Thank you very much! N O T E : Using the change of variables $v=u/\alpha$ , $A=\alpha a$ , and $B=\alpha b$ , the integrands can me made independent of the parameter $\alpha$ . Poisson’s and Related Integrals ( 10.9.3 in DLMF ) $$
J_\nu(z) = \frac{2 \left( \frac{z}{2} \right)^\nu}{\pi^\frac{1}{2} \Gamma \left( \nu + \frac{1}{2} \right)} 
\int_0^1 \left( 1-t^2 \right)^{\nu-\frac{1}{2}} \cos (zt) \, \mathrm{d}t \, .
$$","['integration', 'improper-integrals', 'real-analysis', 'calculus', 'indefinite-integrals']"
4645622,"If $\lim \frac{f(x)}{x^2} = 5$, find $\lim f(x)$ and $\lim \frac{f(x)}{x}$ as $x \to 0$.","I don't know the answer to this problem, but I have an attempt.  It's given that the quotient $f(x)/x^2$ has $5$ as a limit.  I cannot apply the quotient rule for limits because I don't know if $\lim f(x)$ exists.  What I do know is that $\lim x^2 = 0$ , which clearly is a number. An attempt .  So I can multiply both sides of the hypothesis by $\lim x^2 = 0$ getting $$\begin{align}
  \lim \frac{f(x)}{x^2} &= 5 \\
  \lim \frac{f(x)}{x^2}\lim x^2 &= 5\lim x^2 \\
  \lim \frac{f(x)}{x^2}x^2 &= 5\lim x^2 \\
  \lim f(x) &= 5\lim x^2 = 0
\end{align}$$ If this is right, then we got the first request.  For the second request, instead of multiplying by $\lim x^2$ , we can multiply both sides by $\lim x$ , which should give us $\lim f(x)/x = 0$ . Reference .  This is problem 56 in section 2.3, Stewart's Calculus, 6th edition.  It's an even number, so I think there's no solution anywhere --- solution's manual nor at the back of the book.","['limits', 'calculus', 'solution-verification', 'limits-without-lhopital']"
4645637,Set vs Multi-set in specific examples,"I'm not a set theorist, but I feel like I've been taught all my life that I can write sets with repeating elements but those repeats are sort of degenerate and not counted as separate. So $\{a, b, a\} = \{a, b\}$ . This always fit nicely with the fact that I could only check if two sets are equal by asking if they are subsets of one another. But a (computer science with math background) colleague of mine got into a long debate with me on whether I'm ever allowed to do this without referring to some multiset . For a moment I said, OK, maybe I've just not been careful. But now I'm seeing this issue pop up everywhere. Below is a recent example from notes I was writing for my students. Can anyone help me with a reference to the fact that $\{a, b, a\} = \{a, b\}$ is definitely OK or not OK?? Example:
For a group $G$ and an element $a \in G$ , define $$\langle a \rangle := \{ a^n \in G \vert n \in \mathbb{Z} \}$$ Of course if the group is finite then for some $n \ne m$ we have $a^n = a^m$ and I feel like this is used a lot to describe this fairly reasonable set. In some sense it has repeating elements but if you take the approach above it's not an issue at all. What am I missing?","['elementary-set-theory', 'multisets', 'set-theory', 'reference-request']"
4645664,The number of partial derangements of a $52$-card deck (ignoring suits),"I was reading this paper by Ekhad, Koutschan, and Zeilberger titled ""There are EXACTLY 1493804444499093354916284290188948031229880469556
Ways to Derange a Standard Deck of Cards (ignoring suits) [and many other such useful facts]"" , where the authors calculated the number of derangements of a deck of $52$ cards, where cards of the same rank are considered identical. My question is about generalizing this result to partial derangements. Question: For each $k\in \{0,1,\dots,52\}$ , what is the number of permutations of a deck of $52$ cards which have exactly $k$ fixed points, where cards of the same rank are considered identical? Call this number $D_k$ . Would using the Laguerre polynomials that the paper uses suffice to compute $D_k$ ?  I believe that $$D_0 = \int_{0}^{\infty} e^{-x} (L_4(x))^{13} dx = 1493804444499093354916284290188948031229880469556$$ from the paper. For 1 fixed point, we could have $$D_1 = -13\int_{0}^{\infty} e^{-x} (L_4(x))^{12} L_3(x) dx$$ and for more fixed points, we could have something like $$D_k = (-1)^{52-k} \sum_{p\in \text{partitions}(k,4)} \frac{13!}{\left( 13-|p| \right)!} \int_{0}^{\infty} e^{-x} \left( \prod_{p_i\in p}L_{(4-p_i)}(x) \right) L_4(x)^{13-|p|}  dx$$ where $\text{partitions}(n, k)$ is the set of partitions of $n$ with parts $\leq k$ and $|p|$ is the number of parts in a particular partition $p$ . Any suggestions are welcome.","['derangements', 'combinatorics']"
4645713,Is the solution to this 1st order matrix ODE uniquely determined?,"I have a matrix function $e(t)$ (i.e. for each $t$ , $e(t)$ is a matrix) and the ODE $$
e'(t)=e(t)^{-T}g(t)
$$ where $-T$ denotes the inverse transpose, and $g(t)$ is some fixed matrix function. My question is if given the initial condition $e(0)$ , whether $e(t)$ is determined uniquely? The answer is probably yes -- usually one could go so far as to integrate each side using the initial condition and then iterate the expression to obtain an 'explicit' expression for $e(t)$ (the path ordered exponential for example), but I am abit put-off by the appearance of the inverse $e^{-1}$ , hence my slight apprehension.","['matrices', 'matrix-equations', 'ordinary-differential-equations']"
4645732,What's the explanation for the distribution of this kind of dice roll?,"Consider the following type of dice roll for the attack damage in some tabletop RPG: Roll three six-sided dice. Pick two of them that have a sum greater six. The remaining die is the attack damage. If no combination of two dice has a sum greater than six then the attack damage is zero. Naturally players want to maximize damage and thus pick the lowest possible sum of two dice that is still greater than six. I tried to simulate that roll to see the probability distribution of the attack damage and was surprised that there is a probability of ~20% to roll the maximum damage of six. Here is the full distribution after 1M rolls: dmg p 0 0.195335 1 0.153332 2 0.097448 3 0.125001 4 0.101249 5 0.115615 6 0.21202 I makes sense to me that the attack damage is biased towards higher values, because players will choose the higher one when having the choice, but I kind of expected the distribution to increase from 1 to 6 as a result of that instead of fluctuating so much. So what's happening here? Inspired by some of the comments I plotted the distribution with this roll/selection for different kind of dice: The target value is always the most probable value when rolling two dice and taking the sum (so 7 for d6, 11 for d10 and 21 for d20).","['statistics', 'probability-distributions', 'dice', 'probability']"
4645795,Radicals ideals equal in field extension,"If $f,g \in F[x]$ ( $F$ is a field) are two polynomials, then we can obviously have $\text{rad} (f)=\text{rad} (g),$ for instance, $f=x$ and $g=x^2.$ However if the polynomials are irreducible, then this isn't possible. However, if we consider some field extension $K/F,$ is $\text{rad} (f)=\text{rad} (g)$ possible in $K[x]?$ I am taking $f,g$ to be irreducible in $F[x].$ If $f,g$ are irreducible in a characteristic $0$ field, then $f,g$ won't have repeated factors in $K,$ and so $\text{rad} (f)=\text{rad} (g)$ won't be possible. So I believe my question is more about other fields, where we can have repeated factors, for instance, $x^2+t \in \mathbb{F}_2(t)[x]$ factors as $(x+\sqrt{t})^2$ in $\mathbb{F}_2(\sqrt{t})[x]$ which has repeated factors. Perhaps a more general version is to ask if for two prime ideals $\mathfrak{p} \ne \mathfrak{q}$ in a ring $R,$ can we have $\text{rad} \; \mathfrak{p} = \text{rad} \; \mathfrak{q}$ in some ring extension $S \supseteq R?$","['maximal-and-prime-ideals', 'field-theory', 'algebraic-geometry', 'ring-theory', 'abstract-algebra']"
4645808,Convergence to $\ln(n)$,"Is there any hint for $\,\displaystyle\lim_{n \to \infty} \int_{0}^{\pi/2}\!\!\!\frac{n\cos x}{\sqrt{1+n^2x^2}}\,\mathrm dx\;$ ? I've tried with $n \approx 9000000000000000000$ and find that it seems to converge to $\ln(n)$ but no clue to prove that. E.g. $n=9000000000000000000$ , the integral is $44.2316884183270$ (while $\ln(n) = 43.6437562512290$ ). $n=90000000000000000$ , the integral is $39.6265182323389$ (while $\ln(n) = 39.0385860652409$ )","['integration', 'limits', 'definite-integrals']"
4645868,For which values of $\alpha$ is the solution stable?,If we have the following differential equation $X'=\begin{bmatrix} 1 & 1 \\ -1 & \alpha  \end{bmatrix}X$ for which values of $\alpha$ is the zero solution stable? My attempt : For stability we need the real part of all eigenvalues to be negative and geometric and algebraic multiplicity to be the same.I tried computing the eigenvalues and see the different cases but it is an incredible amount of work and I got stuck. Would there be any simpler and nicer way to do this(with knowledge of a first course in ODE)? Thanks,"['matrices', 'linear-algebra', 'ordinary-differential-equations', 'eigenvalues-eigenvectors']"
4645919,How to show that the ratio of two linear second-order ODEs that solve $y'' + P(x)y' + Q(x)y = 0$ with a common zero at a point $x$ is constant.,"So far I've figured out $$y_1''(x_0) + p(x_0)y_1'(x_0) = y_2''(x_0) + p(x_0)y_2'(x_0) = 0,$$ and I've thought about integrating or doing something with this to help show that $y_2/y_1$ is constant, but don't think I can since we only know the equation holds at the point $x_0$ . You can get $$\frac{y_1}{y_2} = \frac{y_1''+P(x)y_1}{y_2'' + P(x)y_2}$$ just from plugging in the values, but I don't see how we can use these equations to show it's a constant. If anyone could help me out I would appreciate it - thank you! PS the problem is from Simmons' ""Differential equations with Application and Historical Notes"" (Section 14 exercise 10 - page 113) Edit: I've thought about this a little more, and I think it might have something to do with the fact that a second ODE is uniquely determined by its equation, $y(x_0)$ , and $y'(x_0)$ . Since the equation and $y(x_0)$ are already picked the difference in the two solutions correlates between the difference in $y'(x_0)$ , but I'm still not quite sure how this can help prove the ratio is constant.",['ordinary-differential-equations']
4645920,Computing derivative of pullback of time-dependent metric,"$\qquad$ Following Topping's book on Ricci flow, $X(t)$ be a time-dependent collection of vector fields with associated collection of diffeomorphisms $\psi_t$ defined on a compact, closed manifold $M$ . Given $g=g(t)$ a family of metrics on $M$ , why is it that if we define $\hat{g}(t)=\sigma(t)\psi_t^*(g(t))$ ( $\sigma$ a smooth, real valued function) that $$\frac{\partial \hat{g}}{\partial t}=\sigma'(t)\psi_t^*(g(t))+\sigma(t)\psi_t^*\left(\frac{\partial g}{\partial t}\right)+\sigma(t)\psi_t^*(\mathcal{L}_Xg)?$$ One definition of the Lie derivative I know is $\mathcal{L}_Xg=\left(\frac{\partial}{\partial t}\big|_{t=0}\psi_t^*\right)g$ , so directly using the product rule and this definition I get $$\frac{\partial \hat{g}}{\partial t}=\sigma'(t)\psi_t^*(g(t))+\sigma(t)\psi_t^*\left(\frac{\partial g}{\partial t}\right)+\sigma(t)\mathcal{L}_Xg.$$ Where is my mistake, or how do I see the first equation holds? I've looked at How to show $\partial_t \hat g = \sigma'(t)\psi_t^* (g) + \sigma(t) \psi_t^*(\partial_t g) + \sigma(t) \psi_t^*(L_Xg)$? but there's not a clear answer.","['tensors', 'ricci-flow', 'lie-derivative', 'differential-geometry']"
4645936,Derivative of numerator greater than denominator implies quotient is increasing.,"Let $u, v$ be two non-negative functions satisfying $u'>v'>0$ . Then I would like to show $$\left(\frac{u}{v}\right)'>0.$$ I'm not sure if this is actually even true, although it is intuitive and has worked for every example that I've tried. Here is my work so far: \begin{align}
\left(\frac{u}{v}\right)'=\frac{u'v-uv'}{v^2},
\end{align} so it remains to show that $u'v-uv'>0$ . We can naively substitute the inequality $u'v-uv'>v'(v-u)$ , but I believe $v-u>0$ is not required. If someone can fill in what I'm missing then I would appreciate it.","['power-series', 'fractions', 'calculus', 'derivatives']"
4645963,Pre-Hilbert space and its completion,"I'm currently managing to understand how to find a completion of Pre-Hilbert space $\mathcal H_0$ in Stein's real analysis. The textbook says the completion $\mathcal H$ has three properties, and the proof of the second one (ii) $(f,g)_0 = (f,g)$ whenever $f,g \in \mathcal H_0$ is written on the red line as follows. This is where I'm stuck on. The following proof is what I've tried at most. $|(f_n-f, g_n-g)| \le ||f_n-f||_H ||g_n-g||_H$ , and since $||f_n-f||_H \to 0$ and $||g_n-g||_H \to 0$ as $n \to \infty$ , $|(f_n-f, g_n-g)| \to 0$ . Thus $lim_{n \to \infty}(f_n,g_n) = (f,g)$ . This is what I've interpreted the definition of $(f,g)$ written on the red line. If this is correct, what should I do to induce $(f,g)_0$ ? The above proof doesn't include no inner product of $\mathcal H_0$ , $(,)_0$ , so I have a difficulty associating my trial with $(,)_0$ . Any help would be appreciated. Thank you.","['hilbert-spaces', 'measure-theory', 'real-analysis']"
4645988,Generalizations of the Gauss circle problem,"The Wikipedia article on the Gauss circle problem states that, if there are $N(r)$ lattice points of radius less than $r$ from the origin, then we know that $|E(r)|=|N(r)-\pi r^2|=O(r^{131/208})$ . Do we know whether this bound holds if we put the center of the circle at an arbitrary point in the plane rather than at a lattice point?","['number-theory', 'gaussian-integers']"
4646011,Bijection but not diffeomorphism,"Let $f:\mathbb{R}\to \mathbb{R}$ be the function defined as $$f(x) =
\begin{cases}
x+x^2\sin \frac{1}{x}, & \text{if }x\neq 0 \\
0, & \text{if }x=0.
\end{cases}$$ I want to prove the following: there are neighborhoods $U$ of $0$ and $V$ of $0$ such that $f:U\to V$ is a bijection. there is no neighborhood of $0$ where the function $f$ is diffeomorphism. My approach: I think that it should be not so difficult but I am missing something.
First of all, one can show that $$f'(x) =
\begin{cases}
1+2x\sin \frac{1}{x}-\cos \frac{1}{x}, & \text{if }x\neq 0 \\
1, & \text{if }x=0.
\end{cases}$$ Also $f'(\frac{1}{2\pi n})=0$ , i.e, one can find arbitrary small zero of $f'$ . I am wondering can anyone answer my questions?
Thank you!","['derivatives', 'real-analysis']"
4646060,Is this a new discovery in the diagonals of Pascal's triangle?,"To preface, I am a senior in high school and my knowledge of combinatorics and its notation is quite limited. I came across Pascal's triangle while drilling the binomial expansions into my head, and I quickly became fascinated with its various properties. When I looked at the diagonals, I wanted to challenge myself by coming up with formulae for the diagonals, and I began with the first two diagonals. Once again, I am not familiar with the standard notation, so please look past it for now. However, I would greatly appreciate advice on what I should change, and where I can learn the standard notation. I am making this post off of a ""paper"" I wrote to collect my thoughts, screenshots of said paper will be used. For the diagonals, I will use $D_n$ , where each value of $n$ represents each diagonal. (ex: $D_2 = [1, 2, 3, 4, 5, 6\cdots]$ For the numbers within the sets of $D_n$ , I will use $d_n$ . $D_n = [d_1, d_2, d_3\cdots]$ So for the first two diagonals $D_1: d_n=1$ $D_2: d_n=x$ As I approached the third diagonal, my mind immediately jumped to recursion, and the formula for $D_n$ would be $D_3: d_n=  d_{n-1} + x$ ; $d_{n<1} = 0$ (this applies for all subsequent formulae) The formula for $D_4$ took me a little longer, but I knew the set for $D_4$ was larger than $D_3$ , so its formula should contain the formula from $D_3$ and another positive term. After some messing around, I arrived at making the next term as follows. $D_4: d_n= d_{n-1} + x + (d_{n-1} - d_{n-2})$ The next day, I continued my brainstorming and made the next 2 formulae. $D_5: d_n= d_{n-1} + x + 2(d_{n-1} - d_{n-2}) - (d_{n-2} - d_{n-3})$ $D_6: d_n= d_{n-1} + x + 3(d_{n-1} - d_{n-2}) - 3(d_{n-2} - d_{n-3})+(d_{n-3} - d_{n-4})$ I hit a brick wall for a little while, and I stared at the formulae I had for hours until I noticed something. I looked at the coefficients of the terms, and saw a pattern forming; the coefficients follow Pascal's triangle. This can be visualized best in the image below, which includes the formulae from $D_3$ through $D_8$ . Apologies for my handwriting The coefficients of the formulae appear to follow the values of Pascal's triangle indefinitely. It is worth noting for sets $D_n$ where $d_2$ is an odd number, the coefficients are always central binomial rows and the last term is always negative. And for sets $D_n$ where $d_2$ is an even number, the coefficients are always central trinomial rows and the last term is always positive. With this in mind, I followed the pattern to make the formula for $D_{11}$ , and then used it to solve $d_{10}$ of $D_{11}$ . again, handwriting So this is where I am at right now, nothing much else besides that except my final questions and some basic proof at the very bottom. Have I found something new? Not the formula's use itself, because I know there are other much more efficient methods, but the pattern that I am seeing in the coefficients. Do other methods for getting a number in a diagonal of Pascal's triangle show the rows of Pascal's triangle like this method is? Regardless of if this is new or not, can I get some help with the notation? I know there is some way to make and notate a formula for formulae. $D_3$ $D_4$ $D_5$ $D_6$ $D_7$ $D_8$","['notation', 'binomial-coefficients', 'combinatorics']"
4646083,Derivative of $y = \sin^3(\frac\pi 3(\cos(\frac\pi{3\sqrt2}(-4x^3 + 5x^2 + 1)^{3/2})))$ at $x=1$,"Today  I came across a problem: If $y = \sin^3\left(\frac\pi 3\left(\cos\left(\frac\pi{3\sqrt2}\left(-4x^3 + 5x^2 + 1\right)^{3/2}\right)\right)\right)$ , then at $x=1$ which of the following option is correct? $2y'+\sqrt{3} \pi^2 y = 0$ $2y' +3 \pi^2 y = 0$ $\sqrt2 y' - 3\pi^2 y = 0 $ $y' + 3\pi^2 y = 0$ This question is from JEE Main 2023 examination. My attempt: Firstly I used the identity $\sin^3(x) = \frac34 \sin(x) - \frac14 \sin(3x)$ and then by applying a brute force chain rule, I got that $f'(1) = \frac3{16} \pi^2$ and $f(1) = -\frac18$ . So, second option i.e. $2y' + 3\pi^2 y = 0$ is correct. But I don't think that this question is designed to be solved in this manner. Is there something which I'm missing?","['calculus', 'derivatives']"
4646084,Unraveling the various definitions of $k$-space or compactly generated space,"There are multiple (incompatible) definitions of ""compactly generated"" or "" $k$ -space"" in the literature.  For a sample, see the various references mentioned in this question .  See also the discussion here and here for example. One starts with a topological space $(X,T)$ , where $T$ is the topology.  Sometimes some separation condition (like weak-Hausdorff, etc) is added to the definition, sometimes not, but we'll ignore that part here.  What all the definitions have in common is that one takes the final topology on $X$ with respect to some family of continuous maps into $X$ .  One can either take all the inclusion maps from certain subspaces, or all the continuous maps from arbitrary spaces of a certain kind.  And on the other hand, one can take the domains of these maps to be compact, or compact Hausdorff.  That gives a total of four combinations.  I am trying to unravel things and understand the relationships between these four possible definitions, spelled out explicitly below. In each case, one starts with an appropriate family $\mathcal F$ of continuous functions into $X$ and one forms the final topology on $X$ with respect to $\mathcal F$ .  That topology is called the k-ification of $(X,T)$ , denoted $T_{\mathcal F}$ .  The space $X$ is then called a $k$ -space or compactly generated if its topology coincides with its k-ification.  So here are the four possibilities with separate names for the purpose of discussion: (1) CGSub: $\mathcal F=$ the family of all inclusions from compact subspaces of $X$ .  The corresponding definition of compactly generated space is the one in wikipedia . (2) CG: $\mathcal F=$ the family of all continuous maps from arbitrary compact spaces. (3) HCGSub: $\mathcal F=$ the family of all inclusions from compact Hausdorff subspaces of $X$ .  The wikipedia article calls the corresponding spaces ""Hausdorff-compactly generated"". (4) HCG: $\mathcal F=$ the family of all continuous maps from arbitrary compact Hausdorff spaces.  This is the definition in nlab and is the one more commonly used in algebraic topology. Note that for CG and HCG, the families of maps are not sets but proper classes.  The final topology is still perfectly well-defined, as explained here . To understand the relationships between the various k-ifications, the following is useful. Lemma 1: Let $X$ be a set, $\mathcal F$ a family of functions from topological spaces to $X$ , and $\mathcal G$ a subfamily of $\mathcal F$ .  Then the respective final topologies satisfy $T_{\mathcal F}\subseteq T_{\mathcal G}$ . This implies these relationships between the k-ifications: $T_{CG}\subseteq T_{CGSub}\subseteq T_{HCGSub}$ $T_{CG}\subseteq T_{HCG}\subseteq T_{HCGSub}$ Now combine this with Lemma 2: Suppose $(X,T)$ is a topological space and $\mathcal F$ and $\mathcal G$ are two families of continuous functions from topological spaces to $X$ .  If each $f:Y\to X$ in $\mathcal F$ factors through an element $g:Z\to X$ in $\mathcal G$ (via some continuous function $Y\to Z$ ), then $T_{\mathcal G}\subseteq T_{\mathcal F}$ . We can apply this to CG and CGSub.  Each continuous function in CG, namely $f:C\to X$ from a compact space $C$ , has compact image and factors through the inclusion $f(C)\to X$ in CGSub.  So $T_{CGSub}\subseteq T_{CG}$ . Combining this with the previous inclusions gives this relationship between a topology and its k-ifications: $$T\subseteq T_{CGSub}=T_{CG}\subseteq T_{HCG}\subseteq T_{HCGSub}.$$ And for the corresponding definitions of spaces, $X$ is CGSub if $T=T_{CGSub}$ , etc.  So the relationships between the various definitions of space properties are: $$HCGSub\implies HCG\implies CG \iff CGSub.$$ In particular, ""nlab compactly generated"" (HCG) implies ""wikipedia compactly generated"" (CGSub).  And for the wikipedia definition, it does not matter if it is defined via subspaces (CGSub) or via maps from arbitrary spaces (CG). The implication HCG $\implies$ CGSub cannot be reversed.  For example, as explained in this question , the one-point compactification of a space which is not CGSub (like the Arens-Fort space) is CGSub but not HCG (because HCG is preserved by open subspaces, but that's not the case in the example). Also note: if $X$ is Hausdorff, all notions above coincide. Apart from validating the above, I would be interested if anyone could point to a reference that ""nlab definition"" implies ""wikipedia definition"". Specific question: Can the implication HCGSub $\implies$ HCG be reversed?  In general a continuous map from a compact Hausdorff space does not seem to factor through an inclusion of a compact Hausdorff subspace of the codomain.  So my guess would be no.  But what would be an example of a HCG space that is not HCGSub?  A compact example would be even better. Minor question: HCGSub is called ""Hausdorff-compactly generated"" in wikipedia (note the hyphen!).  That name seems very dubious.  Never heard of it anywhere.  Maybe someone can comment on that.  Maybe for categorical reasons HCGSub is not a very useful notion compared with HCG, for uses in algebraic topology in particular?",['general-topology']
4646086,Definition of ground state?,"In quantum mechanics, a ground state is an eigenstate of the hamiltonian with the minimal eigenvalue and its existence is guaranteed by appropriate theorems. At least that's how it's defined in undergraduate courses. In the formalism of operator algebras, the definition of ground state is equally clear but so different that I cannot relate it to the classic definition. Given a suitable operator algebra $U$ and a continuous group of automorphisms $\tau$ defineing a dynamical system on $U$ , and calling $\tau$ the infinitesimal generator of the system (for a quantum system this would be the Hamiltonian) a ground state $\omega$ is one that for every operator $A$ of the algebra with adjoint $A*$ satisfies $$- i \omega(A^* \delta(A)) \geq 0 \ \ \ \ \ (1) $$ Eq. (1) follows from the KMS condition, I will paste a screenshot from one source here: I wonder if anybody has an intuition on how to bridge this definition to the classic one.","['c-star-algebras', 'operator-theory', 'functional-analysis', 'quantum-mechanics', 'quantum-information']"
4646124,Proof of Running Maximum of Brownian motion has continuous distribution without using the density or the fact that it is absolutely continuous,"I wanted to show that the running maximum say $\max_{t\in [0,1]}W_{t}$ has continuous distribution without taking help from the fact that it is absolutely continuous and has the distribution of $|W_{1}|$ . To clarify, I want to just use the definition of Brownian motion and to prove this. (I am always talking about the standard brownian motion here). Let us restrict to the set (of probability $1$ ) where $W_{t}$ is continuous. Then I want to show that $P(\{\max_{t\in[0,1]}W_{t}=x\})=0$ for each $x\in\Bbb{R}$ . So let us consider an enumeration of rationals $\{r_{n}\}_{n\in\Bbb{N}}$ in $[0,1]$ and define $Q_{n}=\{r_{1},...,r_{n}\}$ . We have that $\max_{t\in[0,1]}W_{t}=\sup_{r\in\Bbb{Q}\cap[0,1]}W_{r}$ . Also $\sup_{r\in\Bbb{Q}\cap[0,1]}W_{r}=x\implies W_{r}\leq x\,,\forall r\in\Bbb{Q}\cap[0,1]$ and for each $m\in\Bbb{N}$ , there exists $r\in\Bbb{Q}\cap[0,1]$ such that $W_{r}\in (x,x-\frac{1}{m}]$ . So I want to write $\displaystyle P(\sup_{r\in[0,1]\cap\Bbb{Q}} W_{r}=x)\leq P\big(\bigcap_{m\in\Bbb{N}}\bigcup_{n\in\Bbb{N}}\{\exists r\in Q_{n}\,: W_{r}\in (x-\frac{1}{m},x]\}\big)=\lim_{m\to\infty}P(\bigcup_{n\in\Bbb{N}}\{\exists r\in Q_{n}: W_{r}\in(x-\frac{1}{m},x])$ $$\leq \lim_{m\to\infty}P\bigg(\bigcup_{n\in\Bbb{N}}\big\{\exists r\in Q_{n}:W_{r}\in (x-\frac{1}{m2^{n}},x]\big\}\bigg)\leq \lim_{m\to\infty}\lim_{n\to\infty}\sum_{k=1}^{n}P(W_{r_{k}}\in(x-\frac{1}{m2^{n}},x])$$ $$\leq \lim_{m\to\infty}\lim_{n\to\infty}\sum_{k=1}^{n}\frac{n}{m2^{n}}=0$$ However I am having trouble making this precise and I obviously am making a mistake in the step of considering $\lim_{m\to\infty}\lim_{n\to\infty}\sum_{k=1}^{n}P(W_{r_{k}}\in(x-\frac{1}{m2^{n}},x])$ . Can anyone tell me how do I make it correct?. Also is there an easier way to do this? (I mean without using the fact that it has a density). Another way I tried to think of is that if I can show that $\max_{t\in[0,1]} W_{t}=\max_{r\in\Bbb{Q}\cap[0,1]}W_{r}$ (note that I mean $\max$ and not $\sup$ ) , then I can do it easily as for any finitely many points $r_{1},...,r_{n}$ , we have $P(\max_{r\in Q_{n}}W_{r}=x)\leq \sum_{k=1}^{n} P(W_{r_{k}}=x)=0$ for all $n$ . So I can proceed with the countable union and conclude what I want. But this is not necessarily true for continuous functions. I mean for example , $\sin(2x)$ achieves it's maximum at $x=\frac{\pi}{4}\in[0,1]$ and not at a rational point. EDIT: I also tried the following but I am unsure of it. Consider the sets $A_{n}=\bigcap_{m\in\Bbb{N}}\bigg\{\exists r\in Q_{n}:W_{r}\in(x-\frac{1}{m},x]\bigg\}$ . Then $A_{n}$ 's increase to $\bigcap_{m\in\Bbb{N}}\bigg\{\exists r\in \Bbb{Q}\cap[0,1]:W_{r}\in(x-\frac{1}{m},x]\bigg\}$ . But $\displaystyle P(A_{n})=P\bigg(\bigcap_{m\in\Bbb{N}}\bigg\{\bigcup_{r\in Q_{n}}W_{r}\in (x-\frac{1}{m},x]\bigg\}\bigg)=\lim_{m\to\infty}P\bigg(\bigcup_{r\in Q_{n}}\bigg\{W_{r}\in(x-\frac{1}{m},x]\bigg\}\bigg)$ (We could switch the intersection over $m$ to limit because for each fixed $n$ , the sets $\{\exists r\in Q_{n}:W_{r}\in(x-\frac{1}{m},x]\}$ is a decreasing sequence) $$\leq \lim_{m\to\infty} \sum_{k=1}^{n}\frac{1}{m}\frac{1}{\sqrt{r_{k}}}=0$$ (By using a simple bound for standard gaussian that $P(X\in(a,b))\leq (b-a)$ for $X\sim N(0,1))$ Also to note that the maximum is always $\geq 0$ a.s. . So we can ignore $r=0$ and consider $r\in\Bbb{Q}\cap(0,1]$ as $W_{0}=0$ a.s. So $P(A_{n})=0\,,\forall n\in\Bbb{N}$ and hence $P(\bigcup_{n\in\Bbb{N}}A_{n})=0$ which is what is required.","['probability-distributions', 'solution-verification', 'brownian-motion', 'probability-theory', 'probability']"
4646176,"How do I prove that the BVP $y'' = y\sin\left(x\right), y\left(0\right) = a, y\left(1\right) = b$ has a unique solution?","I want to prove that the BVP $$ y'' = y\sin\left(x\right), y\left(0\right) = a, y\left(1\right) = b $$ has a unique solution. I know how to prove uniqueness:
Suppose there are two different solutions $y_1,y_2$ . We look at $z= y_1 - y_2$ . $z\left(0\right) = z\left(1\right) = 0$ . So by Rolle's theorem $z$ has a positive maximum or a negative minimum. W.L.O.G $z$ has a positive maximum at point $x_0$ .
Then on one side $z''\left(x_0\right) < 0$ .
On the other side $z''\left(x_0\right) = z\sin\left(x_0\right) > 0$ . Now I need to prove existence of a solution. I think that I need to start by looking at the IVP $$ {y_q}'' = {y_q}\sin\left(x\right), {y_q}\left(0\right) = a, {y_q}'\left(0\right) = q $$ I know that it has a solution, so I need to show that for a big q, ${y_q} > b$ and for a small q, $y_q < b$ but I don't know how to prove that.","['boundary-value-problem', 'ordinary-differential-equations']"
4646179,How to prove that $-\frac{\pi}{4}\int_{-\infty}^\infty \frac{\psi\left(\tfrac12+ip\right)+\psi\left(\tfrac12-ip\right)+2\gamma}{\cosh^2\pi p}dp=1$?,"The question is motivated by the study of spectral functions of a certain operator arising in a physics problem. The integral in the title is the first from a sequence of integrals $$ I_n=-\frac{\pi}{4}\int_{-\infty}^\infty \frac{\psi\left(\tfrac12+ip\right)+\psi\left(\tfrac12-ip\right)+2\gamma}{\cosh^2\pi p}p^{2n}dp\tag{$\spadesuit$}$$ that seem to be given by rational numbers: $$I_0\stackrel{?}{=}1 ,\qquad I_1\stackrel{?}{=}\frac{1}{36}, \qquad I_2\stackrel{?}{=}-\frac{19}{3600}, \qquad \ldots $$ Simpler integrals $J_n=\displaystyle\int_{-\infty}^\infty \frac{p^{2n}dp}{\cosh^2\pi p}$ can be repackaged into a generating function $$G(x)=\frac{\pi}{2}\displaystyle \int_{-\infty}^\infty \frac{e^{2\pi p x}dp}{\cosh^2\pi p}=\frac{\pi x}{\sin\pi x},\qquad -1<\Re x<1$$ that can be easily computed by residues. Perhaps a similar trick combined with the recurrence relation $\psi(z+1)=\psi(z)+\frac1z$ or/and the series representation $\psi(z)+\gamma=\sum\limits_{n=0}^\infty\left(\frac{1}{n+1}-\frac{1}{n+z}\right)$ can be applied to ( $\spadesuit$ ) but I was not successful with this approach so far, which is why I decided to challenge the non-artificial intelligence.","['integration', 'definite-integrals', 'special-functions', 'complex-analysis', 'calculus']"
4646184,Is my proof for $X\times Y = Y\times X\Leftrightarrow X=Y$ correct?,"I used an indirect proof for $""\Rightarrow""$ : Let $X\neq Y$ . That means that there exists $x\in X$ and $y\in Y$ with $(x,y)\neq(y,x)$ . Because of $$
(x,y)=(x',y')\Leftrightarrow (x=x')\wedge (y=y')
$$ we have $X\times Y\neq Y\times X$ . For the other direction $""\Leftarrow""$ I used a direct proof. Let $X=Y$ . That means $$
X=Y\Leftrightarrow(X\subset Y)\wedge (Y\subset X) \Leftrightarrow (\forall x\in X\Rightarrow x\in Y) \wedge(\forall y\in Y\Rightarrow y\in X).  
$$ Therefore we have $$
X\times Y \subset X\times X\subset Y\times Y\subset Y\times X
$$ and $$
Y\times X\subset Y\times Y \subset X\times X \subset X \times Y
$$ which means $$
(X\times Y\subset Y\times X)\wedge (Y\times X \subset X\times Y) \Leftrightarrow X=Y.
$$ The thing is I am not sure whether this is proof is complete or if I did something wrong (or skipped a step somewhere). Could someone help me out?",['elementary-set-theory']
4646193,$\int_a^b \frac{(x-a)^p(b-x)^{1-p}}{\text{P}(x)} dx$ using contour integration and residue theorem,"So a while ago I came across these types of integrals that has the form of $$
I = \int_a^b (x-a)^p(b-x)^{1-p}dx
$$ Where $a$ , $b$ and $p$ are real numbers, $a<b$ and $0<p<1$ . And those types integral has a closed form, namely $$
I = \frac{\pi}{2}p(1-p)(b-a)^2\csc(p\pi)
$$ But then I also came across integrals like these along the way $$
\int_0^3 \frac{x^{3/4}(3-x)^{1/4}}{5-x}dx
$$ So I asked myself, is there a more generalized form of the $I$ integral, namely $$
J = \int_a^b \frac{(x-a)^p(b-x)^{1-p}}{\text{P}(x)} dx
$$ Where $\text{P}(x)$ is any polynomial. To begin with, I decided to make a more concise (?) definition of the function $$
\text{P}(x) = \prod_{k=1}^n (x-c_k)^{q_k}
$$ Where $c_k$ is the $k$ -th root of $\text{P}(x)$ and $q_k$ is the $k$ -th root's multiplicity. $c_1 \not = c_2 \not = c_3 \not = \ldots \not = c_n$ , $c_k \not \in (a, b)$ , and if $a$ or $b$ is a root of $\text{P}(x)$ then they can't have a multiplicity of 2 or higher First off, I'll analyze this function $$
f(z) = \frac{(z-a)^p (b-z)^{1-p}}{\text{P}(z)} \\
= \frac{|z-a|^p |b-z|^{1-p}}{\text{P}(z)} e^{i(p\arg(z-a) + (1-p)\arg(b-z))}
$$ The poles of this function are $z = c_k$ with order $q_k$ for $k = 1,2,3,\ldots, n$ To begin, I integrated $f(z)$ over the dogbone contour Which means I'll have $$
\oint_{\text{Dogbone}} f(z) dz = \left(\int_{\gamma_1} + \int_{\gamma_2} + \int_{\mu_1} + \int_{\mu_2}\right) f(z) dz
$$ If we call the radius of the $\mu$ paths $\varepsilon$ , and have it approach 0, then both integrals over $\mu_1$ and $\mu_2$ will go to 0 also. The integral over $\gamma_1$ will be $J$ and the integral over $\gamma_2$ will be $-e^{(1-p)2\pi} J$ . Which means $$
\oint_{\text{Dogbone}} f(z) dz = \left(1 - e^{(1-p)2\pi}\right) J
$$ Now, to continue, we can inflate the contour into this I'll call the big circular contour $\Gamma$ and the small ones surrounding the poles at $c_k$ $\omega_k$ . Which means we'll have $$
\left(1 - e^{(1-p)2\pi}\right) J = \left(\oint_\Gamma + \sum_{k=1}^n \oint_{\omega_k}\right) f(z) dz
$$ For the $\omega$ integrals, we can simply use residue theorem for higher order poles $$
\oint_{\omega_k} f(z) dz = - \frac{2\pi i}{(q_k - 1)!} \lim_{z \rightarrow c_k} \partial_z^{q_k - 1} (z-c_k)^{q_k} f(z)
$$ Plugging it back into the equation $$
\left(1 - e^{(1-p)2\pi}\right) J = \oint_\Gamma f(z) dz - \sum_{k=1}^n \frac{2 \pi i}{(q_k - 1)!} \lim_{z \rightarrow c_k} \partial_z^{q_k - 1} (z-c_k)^{q_k} f(z)
$$ Now, my main problem is the final integral. I know that it'll be the residue of $f(z)$ at infinity, I know there's a formula for it, but I have no idea of how to evaluate such residues. My guess is that it'll some how evaluate to 0, but still, I have no idea how that'll be the answer. 26/2 Edit: Fixed some of the mistakes in calculation + Thanks to @Diger for pointing out the mistake in the inflated contour","['integration', 'residue-calculus', 'complex-analysis', 'contour-integration', 'branch-cuts']"
4646267,"What does ""$f$ is a function of $x$"" mean?","Rigorously speaking, a function $f: X \to Y$ is a mapping that maps each element from set $X$ to an element in set $Y$ . (Or more rigorously it can be defined using cartesian product).
For $x \in X$ , people often say $f$ is a function of $x$ . What does that mean？Isn't $x$ just an element in $X$ and functions (in my understanding) are meant to be dealing with the entire set $X$ .","['elementary-functions', 'definition', 'functions', 'terminology']"
4646278,Request an explain for a limit [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question $$\lim_{x\to 0+}\exp\left(\frac{\ln \left( \frac{\ln(1+\tan 4x)}{4x} \right)}{\tan x}\right)=\lim_{x\to 0+}\exp\left(\frac{ \frac{\ln(1+\tan 4x)}{4x} -1}{x}\right)$$ Can anybody explain above equation? I can't understand right hand side of the equation. Where is $\tan x$ and how pop up $-1$ ? Thank you. Sorry, infact I solved a question linked my question using Taylor expansion and I'm looking for a solution that not using Taylor and I saw the solution I have mentioned but I can't get it.",['limits']
4646311,Positivity of a matrix built on Pascal's triangle,"For an integer $n$ , let $T_n$ be the $(n+1) \times (n+1)$ Toeplitz matrix built on the $2n$ th row of Pascal's triangle, i.e., its $(i,j)$ entry equals $\binom{2n}{n+i-j}$ . For example, $$ T_2 =\begin{pmatrix} 6 & 4 & 1 \\ 4 & 6 & 4 \\ 1 & 4 & 6 \end{pmatrix}. $$ Numerical experiments indicate that $T_n$ is always positive definite. How to prove this claim?","['matrices', 'toeplitz-matrices', 'binomial-coefficients', 'positive-definite']"
4646362,About an integral of the MIT Integration Bee Finals (2023),"I would like to solve the first problem of the MIT Integration Bee Finals, which is the following integral : $$\int_0^{\frac{\pi}{2}} \frac{\sqrt[3]{\tan(x)}}{(\cos(x) + \sin(x))^2}dx$$ I tried substitution $u=\tan(x)$ , King Property, but nothing leads me to the solution which is apparently $\frac{2\sqrt{3}}{9} \pi$ . If anybody knows how to solve it I would be grateful.","['integration', 'trigonometry']"
4646365,Is there an algorithmic description of a bijection $\mathbb{N} \leftrightarrow \mathbb{N}^2$ with a traversal described below,"Is there some pattern/recurrence for every column and row in distributing numbers $a_1,a_2,a_3,…$ as the graphic shows? The way shown of distributing $a_1,a_2,a_3,…$ continues down and to the right for ever. For the first row ( $1,2,9,10,25,26,…$ ), it seems to be $n^2$ for odd, and $(n-1)^2+1$ for even $n$ th element of the row For the first column ( $1,4,5,16,17,…$ ), it seems to be $n^2$ for even, and $(n-1)^2+1$ for odd $n$ th element of the column.","['combinatorics', 'analysis']"
4646425,Do these two definitions of ring of regular functions coincide?,"I'm following Gathmann's notes (from 2021) and I got to the section on regular functions (on affine varieties). He gives the following definition, which I believe is pretty standard: Definition 1. Let $X$ be an affine variety (not necessarily irreducible), and let $U$ be an open subset of $X$ . A regular function on $U$ is a map $\varphi\colon U \to K$ with the following property: for every $a\in U$ there are polynomial functions $f,g\in k[X]$ with $f(x)\neq 0$ and $\varphi(x) = \frac{g(x)}{f(x)}$ for all $x$ in an open subset $U_a$ with $a\in U_a\subseteq U$ . The set of all regular function on $U$ will be denoted $\mathcal{O}_X(U)$ . However, I don't find this definition very intuitive, specially when compared to other texts. For instance, an older version of the notes (from 2002), or Mukai's book, give an alternative one, which I think is more natural: Definition 2. Let $X$ be an irreducible affine variety. As its coordinate ring is an integral domain, it has a well defined field of fractions, called the set of rational functions on $X$ , which we denote as $k(X) := \mathrm{Quot}(k[X])$ . Given a point $P\in X$ , the set of rational functions on $X$ defined at $P$ is $$\mathcal{O}_{X,P} := \left\{\frac{f}{g}\in k(X)\colon g(P) \neq 0 \right\}.$$ Given an open set $U\subseteq X$ , the set of rational functions of $X$ defined on $U$ is the set $$\mathcal{O}_X(U) := \bigcap_{P\in U} \mathcal{O}_{X,P}$$ . I'm aware these sets are usually called the local ring of $X$ at $P$ , and the ring of regular functions of $X$ on $U$ , respectively. My first question is the following: Q1. Do these two definitions actually coincide? This question is prompted because the texts tend to emphasize the fact that the local representations of a regular function may vary from point to point, ie. there is not a global representation of a regular function on all of $U$ , necessarily. The thing is I'm failing to catch the local aspect of the second definition: Q2. How is the second definition local? In particular, I'm struggling to understand the meaning of this example: Example. Let $X:= \mathbb{V}(x_1x_4-x_2x_3)\subseteq \mathbb{A}^4_k$ , the open subset $U:= X - \mathbb{V}(x_2,x_4)\subseteq X$ . The function $$
(x_1,x_2,x_3,x_4) \longmapsto \begin{cases}
 \frac{x_1}{x_2} &\text{if $x_2\neq 0$},\\
\frac{x_3}{x_4} &\text{if $x_4\neq 0$}
\end{cases}
$$ is easily seen as regular on $U$ in the sense of the first definition, and has no representation as a quotient that works in all of $U$ , which is fine by the definition. The thing is, I can't wrap my head around using the second definition to study this example. I don't think the function is regular according to that definition. I don't even think is in $k(X)$ . Q3. What is going on? Is the function actually regular in the sense of the second definition? How is it an element of $k(X)$ ? Thanks in advance.","['algebraic-curves', 'algebraic-geometry', 'commutative-algebra']"
4646441,Solving a gradient equation involving Jacobian of orthonormal vectors,"Revised to more specific question after realizing some steps Let $N(x)$ be a $K\times D$ matrix function of $x\in \mathbb{R}^D$ with orthonormal rows, so that $N(x)N(x)^T=I_{K\times K}$ . Let $\vec{n}_k$ be the $k$ -th row of $N$ , written as a column vector, and let $D[\vec{n}_k]$ be the Jacobian matrix of $\vec{n}_k$ . Consider the matrix equation $$P(x) \nabla \log p(x) = N(x)^T \vec{q}(x)+\sum_{k=1}^K D[\vec{n}_k(x)]\vec{n}_k(x),$$ where the vector $\vec{q}(x)$ has $r$ -th component $$q^r(x) = \operatorname{Tr}(N(x) D[n_r(x)] N^T(x)),$$ for $r=1,2,\dotsc, K$ and where $P(x)=I_{D\times D} - N(x)^T N(x)$ is the orthogonal projection of $\mathbb{R}^D$ onto the tangent space $T_x M$ at $x$ of some manifold $M$ , and the unknown $p(x)$ is a nice enough function of $x\in \mathbb{R}^D$ . Question: Can we solve this equation explicitly for the unknown function $p(x)$ ? Some simplifications and easy cases: When $K=1$ , and $\operatorname{Tr}(n(x)^T D[n(x)] n(x)) n=0$ we have $P(x)\nabla \log p(x) = D[n]n$ , I know how to proceed, see this answer . This more general case is giving me some trouble however. In general, notice that applying $P(x)$ to both sides of the original equation and using the fact that $P(x)^2=P(x)$ , that $D[\vec{n}_k(x)]\vec{n}_k(x)$ already lies on $T_x M$ , and that (suppresing notation on $x$ ) \begin{align*}
PN^T q &= (I-N^TN)N^T q\\
& = N^Tq -N^T N N^Tq\\
& = N^Tq - N^T I_{p\times p} q\\
& = N^Tq-N^Tq=\vec{0}
\end{align*} where we have used the orthogonality of $N$ , i.e. $NN^T=I_{K\times K}$ , we obtain the equation $$P \nabla \log p = \sum_{k=1}^K D[\vec{n}_k]\vec{n}_k.$$ It follows that $$\nabla \log p = \sum_{k=1}^K \left(D[\vec{n}_k] +c_kI_{D\times D} \right)\vec{n}_k$$ for some scalar functions $c_k$ (since $\{n_1,\dotsc, n_K\}$ is an orthonormal basis for the kernel of $P$ . It remains to show the RHS is a gradient, analogous to the case $K=1$ . I think it is possible to generalize the argument for $K=1$ but the exact details are escaping me at the moment, so I would appreciate any tips or ideas. Update 2/26/2023: When $N$ comes from orthonormalizing a Jacobian matrix $D[f]$ with full rank on $f^{-1}(\{0\})$ of some smooth function, and we also assume that the rows of $D[f]$ are already orthogonal then we can directly use the previous method linked above with the appropriate changes. We obtain in this case that $\sum_{k=1}^K D[\vec{n}_k]\vec{n}_k = \frac12 \nabla \left[\sum_{k=1}^K H_k\right] - \sum_{k=1}^K \mu_k \vec{n}_k$ and then we get that $$\nabla \log p(x) = \nabla \left[\sum_{k=1}^K \frac12 H_k\right]$$ $$ = \nabla \left[\log \prod_{k=1}^K \|\nabla f_k\|\right],$$ hence $p(x) = \prod_{k=1}^K \|\nabla f_k(x)\|$ . Here $H_k = 2\log \|\nabla f_k\|$ and $$\mu_k = \frac{1}{\|\nabla f_k\|} n_k^T (\nabla^2 f_k)n_k,$$ and $n_k = \nabla f_k / \|\nabla f_k\|$ . I have omitted a bit of the details but if anyone wants to see them I can add them in later when I have more time. In general, my conjecture is the solution is $p(x) = \sqrt{\det J_f(x) J_f(x)^T}$ , where $J_f := D[f]$ .","['linear-algebra', 'ordinary-differential-equations', 'differential-geometry']"
4646443,Find the value of $x_1$ [duplicate],"This question already has answers here : Formula for n-th composition of Mobius function. (1 answer) how do you prove $x_1 = x_2 = x_3 =...=x_n$, and find the value of $x_1.$ (1 answer) STEP paper 1 1999 question - is it possible to justify this solution? (3 answers) Closed last year . The positive numbers $x_1,x_2,\cdots,x_n$ $n\ge3$ satisfy $$x_1=1+\frac{1}{x_2},x_2=1+\frac{1}{x_3},\cdots,x_{n-1}=1+\frac{1}{x_n}$$ And also $$x_n=1+\frac{1}{x_1}$$ Find the value of $x_1.$ My first thought is that this question has an unknown number of variables $:$ $x_1,\cdots, x_n.$ That makes it seem rather complicated. I might, if necessary, try to understand the result by choosing an easy value for $n$ (maybe $n = 3$ ). If I manage to prove some of the results in this special case, I will certainly go back to the general case $:$ doing the special case might help me tackle the general case. Also, I don't think that I can apply any inequality here. I can see that each $x_i>1.$ But now I'm stuck. No idea is striking to my mind. Any help is greatly appreciated.","['elementary-number-theory', 'recurrence-relations', 'sequences-and-series']"
4646532,size of tensor product,"In a recent intro to homological algebra, someone drew an analogy between tensoring and the geometric operation of intersecting varieties. (I didn't understand this as I don't know algebraic geometry.) But it left me with the following intuitive question: Why does taking the tensor product of $\mathbb{Z}$ -modules sometimes make things bigger and sometimes smaller? Examples: The tensor of free modules of finite rank is free of bigger rank. The tensor of the infinitely generated non-free module $\mathbb{Q}$ with itself is again itself. $\mathbb{Z}/2\otimes \mathbb{Z}/3 = 0$ . In general, the tensor of modules has annihilator something like the ideal sum of annihilators. Also contrastingly, the intersection of submodules $2\mathbb{Z}$ and $3\mathbb{Z}$ within $\mathbb{Z}$ is their ideal product, which is $6\mathbb{Z}$ , set-theoretically smaller. But isomorphic to both inputs as a module. Is there a ""geometric"" way of thinking that covers all of this?","['homological-algebra', 'algebraic-geometry', 'commutative-algebra']"
4646553,"If $-k$ is an eigenvalue of a $k-$regular graph $G$, then $G$ is bipartite","I'm trying to show that, for $G$ a $k$ -regular graph with adjacency matrix $A,$ if $-k$ is an eigenvalue of $A$ , then $G$ must be bipartite. What I know: Since $G$ is $k-$ regular, $k$ is an eigenvalue of $G.$ Furthermore, if $G$ is not bipartite, it has at least one cycle of odd length $m$ . Then we should have that $\text{trace}(A^{m}) > 0.$ For $\{\lambda_{i}\}_{1}^{n}$ the eigenvalues of $A, \text{trace}(A^{m}) = \sum_{i}^{n}{\lambda_{i}^{m}} = (-k)^{m} + \lambda_{2}^{m} + ... + \lambda_{n-1}^{m} + (k)^{m} = \lambda_{2}^{m} + ... + \lambda_{n-1}^{m}.$ But I'm not sure how to now show that the rest of these eigenvalues will also sum to $0$ . Any direction?","['matrices', 'graph-theory', 'linear-algebra', 'discrete-mathematics']"
4646611,How find the closed -form $\{a_{n}\}$ if such Strange condtion,"let $\{a_{n}\}$ such $a_{1}=0,a_{n}<0,\forall n\ge 2$ ,such $$(a_{n+1}+1)^2(2a_{n+1}+1)=2a^4_{n+1}(2a_{n}+1)(a_{n}+1)$$ find the closed form $a_{n}$ it easy to get $$a_{2}=1-\sqrt{2},~~~~a_{3}=3+2\sqrt{2}-\sqrt{2(10+7\sqrt{2})}$$ also by condtion we get $$\dfrac{a_{n+1}+1}{a_{n}+1}\cdot\dfrac{2a_{n+1}+1}{2a_{n}+1}=\dfrac{2a^4_{n+1}}{a_{n+1}+1}$$",['sequences-and-series']
4646643,$f:\mathbb{R}\rightarrow\mathbb{R}$ and $f(x+1)+f(x+2)=f(x-1)+f(x)$ and $f(-x)=-f(x)$. Then find value of $f(2)$.,"$f:\mathbb{R}\rightarrow\mathbb{R}$ and $f(x+1)+f(x+2)=f(x-1)+f(x)$ and $f(-x)=-f(x)$ . Then find value of $f(2)$ . My Attempt $f(0)=0$ since $f(-x)=-f(x)$ By putting $x=0$ in the functional equation I am able to obtain $f(2)=-2f(1)$ Then putting $x=2,3,4...$ I am getting $f(n)=(-1)^{n+1}nf(1)$ But then what can we say about $f(1)$ . Also, I observe that $f(x)=\sin (\pi x)$ satisfies the given functional equation. Can there be more of such functions.","['functional-equations', 'algebra-precalculus', 'functions']"
4646701,Density of normal matrices in $M_n(\mathbb{C})$,"A matrix $A\in M_n(\mathbb{C})$ is said to be normal if $A^*A=AA^*$ , where $A^*$ is the Hermitian conjugate. Consider $M_n(\mathbb{C})$ with its norm topology. Question: Is the space of normal matrices dense in $M_n(\mathbb{C})$ ? Thoughts: I know that the space of all diagonalisable matrices is dense in $M_n(\mathbb{C})$ , and that a matrix is normal if and only if it is unitarily diagonalizable. So the question amounts to asking whether the unitarily diagonalizable matrices still form a dense subset.","['general-topology', 'linear-algebra', 'functional-analysis']"
4646713,Approximation scheme for potentials in Schrodinger Equation- A Request,"Setup: The Time Independent Schrodinger Equation(Eigenvalue problem): $(-\frac{\hbar^2}{2m}\Delta +V)\psi = E\psi$ When dealing with computing bound states/bound state energies of say an electron in complicated potentials, one trick I have seen informally is that we can replace the given complicated potential with something that has 'similar behaviour' but has known eigenstates/eigenvalues to 'estimate' the eigenstates/eigenvalues. What I wish to know is, under what conditions is something like this scheme faithful/what conditions need to be imposed to achieve this kind of approximation? Roughly speaking,
If $V$ is the given potential, and $V_{\epsilon}$ is another potential whose eigenvalues and eigenstates are known, such that $\|V_\epsilon-V\|<\epsilon$ (in some norm) then if $E_{\epsilon}, E$ denote the ground state energies in $V_{\epsilon}, V$ respectively, then what is an estimate on $|E_{\epsilon}-E|$ ? Is it of the order $\epsilon$ ? I do not know what this class of problems is called so I could not look for it by just googling for example. Any references on it are requested.","['analysis', 'reference-request', 'partial-differential-equations', 'physics', 'mathematical-physics']"
4646715,Proof that $\frac{Ax}{\|Ax\|}$ has fixed points.,"Let $A\in \operatorname{Mat}_{2\times 2}(\Bbb{R})$ with eigenvalues $\lambda\in (1,\infty)$ and $\mu\in (0,1)$ . Define $$T:S^1\rightarrow S^1;~~x\mapsto \frac{Ax}{\|Ax\|}$$ I need to show that $T$ has 4 fixed points. My idea was the following. Since $\mu, \lambda$ are eigenvalues there exists, $x,y\neq 0$ such that $Ax=\mu x$ and $Ay=\lambda y$ .Then I claim that $x,y$ are fixed points: $$Tx=\frac{Ax}{\|Ax\|}=\frac{\mu x}{\|\mu x\|}=\operatorname{sign}(\mu)\frac{x}{\|x\|}=\frac{x}{\|x\|}=x$$ similarly one can show that $Ty=y$ . But then I don't see where the other two fixed points should come from?","['matrices', 'dynamical-systems', 'fixed-points', 'eigenvalues-eigenvectors']"
4646752,Bumping Series and its Formula,"I was thinking of random series, popping up in mind, when I thought of one possible series in my head. It is as follows: The basic idea is, take a line of natural numbers $\mathbb{N}$ which goes till infinity, and add them. One visible thing here is that the most maximum highest number $\mathbb{N}_{max}$ would be $\mathbb{N}_{i}$ . In basic words, if we go till number 5, $\mathbb{N}_5$ then the height it reaches by summation is 5. Further, carrying on, we can get: The basic implication here is that we bump the numbers by specific $\mathbb{N}$ . At start, we take the beginning number, in our case it is 1, we move once up and then once down. Then we do it twice, thrice and so on. So 1 3 2 according to my diagram is one bump. At the ending $\mathbb{N}$ which is 2 here, we will jump it by 2 and make it low by 2. So it gets 2 5 12 7 4 . Here, assume $\mathbb{N}_i$ as the number of incrementation, before it was 1, now it is 2. We get different sets, with different terms, but total number of terms we get through this would be $2 \mathbb{N}_i + 1$ . Now, it will start from 4, go on making 3 jumps before landing by three terms. By this, we get series highlighted by circles in that triangular array as: 1, 3, 2, 5, 12, 7, 4, 9, 20, 44, 24, 13, 7, 15, 32, 68, 144, 76, 40, 21, 11, 23, 48, 100, 208, 432, 224, 116, 60, 31, 16... The series seem to be divergent, my specific question here is how to represent this series in Mathematical terms. Can there be a formula to guess the next number or any probability test?","['discrete-mathematics', 'probability', 'sequences-and-series']"
4646753,Dedekind MacNeille completion. Definition of supremum as $\left(\bigcup M\right)^{ul}$. Counterexample for definition as $\bigcup M$?,"I didn't found a mathematical text for Dedekind MacNeille completion
, so I ""defined"" supremum in the completion the following way: $$\sup M := \left(\bigcup M\right)^{ul},$$ where M is a family of some subsets of $S_{DM}$ , the completion of $(S,<)$ . (okay, supremum is always defined, yes, here as term $(\iota x \in S_{DM}: M\leqslant x)$ , but then sometimes it is interpreted as proper partial function on the support of the first order model.) And it works perfectly, indeed a supremum. But when I tried $\sup M := (\bigcup M)$ as definition, I got stuck in the proof.
So I suspect, that there is a counterexample for such definition. Could you please either provide such counterexample, or actually prove that the second definition is also good?","['elementary-set-theory', 'order-theory', 'lattice-orders']"
4646765,Waiting times between record observations,"Let $\{X_i\}_{i=1}^\infty$ be a sequence of i.i.d. random variables with a continuous CDF. Let $V_1:=\min \{n\in\mathbb N\, \vert \,X_n>X_1\}$ . Let $V_{r+1}:=\min \{ n\in \mathbb N \,|\, n>V_r \mbox{ and } X_n>X_{V_r} \}$ for all $r\ge 1$ . Define $\Delta_1 :=V_1$ and $\Delta_{r+1}:=V_{r+1} -V_r$ for all $r\geq 1$ . Now I want to show that $$\frac{\log \Delta_r}{r}\to 1 \,\,\mbox{ almost surely.}$$ This is supposed to be easy because it is in the warmups of a sequence of exercises, but I could not think of a easy way to solve it. I can prove the convergence above by proving that $\Delta_r \to e^r$ almost surely. However, the proof is quite long and will use many external results. Actually the first proof of that $\Delta_r \to e^r$ almost surely was published in the 60's in a journal. The desired inequality above shall be much weaker and easier to be proved but I just couldn't think of an easy way... Thanks for any comment/hint/answer.","['almost-everywhere', 'probability-limit-theorems', 'probability-theory', 'probability']"
4646773,Universal substitution or Feynman trick to solve this integral,"I started with an integral $ \int_{0}^{2\pi} \sqrt{2[\sin^2(t) + 16\cos^2(t) - 4\sin(t)\cos(t)]} \,dt $ And I simplified it to $ \int_{0}^{2\pi} \sqrt{17 + 15\cos(2t) - 4\sin(2t)} \, dt$ My question: I know this can be simplified with some sort of substitution that cancels the $\sin$ and $\cos$ with a $u$ -sub, but I do not know how. I saw it online, with no explanation (see the first answer: find length of curve of intersection ). I think this has an exact elementary solution, if you use $\tan\left(\frac x2\right)$ substitution and possibly Feynman's trick if necessary.","['integration', 'multivariable-calculus', 'definite-integrals', 'parametric']"
4646859,"Computing $\lim\limits_{n\to\infty}n\bigl(a_n-\int_0^1f^2(x)\,dx\bigr)$ given a function $f\in\mathcal{C}^1([0, 1])$","Let $f : [0, 1] \to \mathbb{R}$ be a continuous function and $(a_n)_{n \geq 1}$ a sequence defined by $$a_n = \sum_{k=1}^{n} \biggl( f\left(\frac{k-1}{n}\right)\int_{\frac{k-1}{n}}^{\frac{k}{n}}f(t) \, dt\biggr), \qquad \forall n \in \mathbb{N}^{*}.$$ a) Prove that $\lim\limits_{n \to \infty}a_n = \int_0^1f^2(x)dx$ b) Furthermore, if $f \in \mathcal{C}^1([0, 1])$ (differentiable with its derivative continuous), compute: $$\lim_{n\to\infty} n\left( a_n - \int_0^1 f^2(x) \, dx \right)$$ Since $f$ is a continuous function, then there exists an antiderivative of $f$ such that $F : [0, 1] \to \mathbb{R}$ , $F'(x) = f(x)$ . Therefore, $$
\int_{\frac{k-1}{n}}^{\frac{k}{n}}f(t)dt = F\left(\frac{k}{n}\right) - F\left(\frac{k-1}{n}\right).
$$ From this, we deduce that: $$
a_n = \sum_{k =1}^n f\left(\frac{k-1}{n}\right)\left[F\left(\frac{k}{n}\right) - F\left(\frac{k-1}{n}\right)\right]
$$ Using Lagrange's Mean Value Theorem, one can easily see that there exists $c_{k, n} \in \bigl( \frac{k-1}{n}, \frac{k}{n} \bigr)$ , so that: $$\frac{F(\frac{k}{n}) - F(\frac{k-1}{n})}{\frac{k}{n} - \frac{k-1}{n}} = F'(c_{k, n}) = f(c_{k, n})$$ Using this, we obtain that: $$
a_n = \frac{1}{n}\sum_{k=1}^n f\left(\frac{k-1}{n}\right)f(c_{k, n})
$$ Note that $c_{k, n} \in \bigl( \frac{k-1}{n}, \frac{k}{n} \bigr)$ , and $\left| f(c_{k, n}) - f\bigl(\frac{k-1}{n}\bigr) \right| \leq \epsilon, \forall \epsilon > 0$ and $n$ sufficiently large since $f$ is a continuous function. Then, by choosing $\Delta = (x_0, x_1, \dots, x_n) \in \mathcal{D}([0, 1])$ , with $x_k = \frac{k}{n}$ , we realize that $\|\Delta\| = \frac{1}{n}$ , and then by choosing $\xi_k \in [x_{k - 1}, x_k]$ , $a_n$ is precisely the Riemann sum, thus because $f$ is Riemann integrable, and by Lebesgue's criterion, $f\cdot f = f^2$ is Riemann integrable: $$
\lim_{n \to \infty}a_n = \int_0^1 f^2(x) \, dx
$$ Thus concluding a). Now, my problem is that I am stuck at b). The given limit obviously screams for the Stolz–Cesàro lemma to be applied, however when I try to apply the Lagrange Mean Value theorem again to reduce the differences of antiderivatives, I cannot seem to get an idea of how to reduce terms like $\frac{k}{n}$ and $\frac{k}{n + 1}$ , where the denominator is different. My idea would be to write: $$
L = \lim_{n \to \infty} \frac{a_n - \int_0^1f^2(x) \, dx}{\frac{1}{n}}
$$ And instead computing: $$L = \lim_{n \to \infty} \frac{a_{n+1} - a_n}{\frac{1}{n + 1} - \frac{1}{n}}$$ Any ideas, hints or solutions would be of very much help. Thank you very much! :)","['integration', 'real-analysis', 'continuity', 'limits', 'derivatives']"
4646865,"Minimizing $f(t,s) = a \cos(t) + b \cos(s) + c \cos(t - s) $","I am trying to minimize $$ f(t, s) = a \cos(t) + b \cos(s) + c \cos(t - s) $$ Where $a, b, c \gt 0 $ . My effort: Find the partial derivatives of $f$ with respect to $t$ and $s$ and equate them to zero: $$f_t = - a \sin(t) - c \sin(t - s) = 0 $$ $$f_s = - b \sin(s) + c \sin(t - s) = 0 $$ It follows from these two equations that $$ - a \sin(t) = b \sin(s) $$ Now, $$\cos^2(t) = 1 - \sin^2(t) = 1 - \left( \dfrac{b}{a} \sin(s) \right)^2 $$ We now have: $$ c \sin(t - s) = - a \sin(t) $$ Expanding the left hand side, $$ c \bigg( \sin(t) \cos(s) - \cos(t) \sin(s) \bigg) = - a \sin(t) $$ Eliminate $\sin(t) $ and $\cos(t) $ , and substitute for them in terms of $\sin(s)$ : $$ c \bigg( - \dfrac{b}{a} \sin(s) \cos(s) - \sin(s) \left( \pm \sqrt{ 1 - \left( \dfrac{b}{a} \sin(s) \right)^2} \right) \bigg) = b \sin(s) $$ This implies that either $ \sin(s) = 0 $ , in which case $\sin(t) = 0 $ as well, and the value of the function in this case will be $ a + b + c $ , which is obviously the maximum, or it is non-zero, and that $$ c \bigg( - \dfrac{b}{a} \cos(s) \mp \sqrt{ 1 - \left( \dfrac{b}{a} \sin(s) \right)^2 } \bigg) = b $$ So that $$    \dfrac{b}{a} \cos(s) \pm \sqrt{ 1 - \left( \dfrac{b}{a} \sin(s) \right)^2 } = -\dfrac{b}{c} $$ Re-arranging and squaring $$ 1 - \left( \dfrac{b}{a} \sin(s) \right)^2 = \left(\dfrac{b}{c}\right)^2 + \left( \dfrac{b}{a} \cos(s) \right)^2 + 2 \left( \dfrac{b^2}{ac} \cos(s) \right) $$ And this simplifies to $$ 1 - \left( \dfrac{b}{a} \right)^2 = \left( \dfrac{b}{c} \right)^2 +2 \left( \dfrac{b^2}{ac} \cos(s) \right) $$ Multiplying through by $ a^2 c^2 $ $$ a^2 c^2 - b^2 c^2 = a^2 b^2 + 2  a b^2 c \cos(s) $$ From which we have $$ \cos(s) =  \dfrac{ a^2 c^2 - b^2 c^2 - a^2 b^2 }{ 2 a b^2 c } $$ And this can be written as $$ \cos(s) = \dfrac{1}{2 b} \left( \dfrac{a c}{b} - \dfrac{bc}{a} - \dfrac{a b}{c} \right)$$ From symmetry of the function $f(t,s) $ this implies that $$ \cos(t) = \dfrac{1}{2a} \left( \dfrac{b c}{a} - \dfrac{a c}{b} - \dfrac{ a b }{c} \right)$$ It follows that $$ \sin^2(s)= \dfrac{1}{4 b^2} \left( - \dfrac{a^2 c^2}{b^2} - \dfrac{b^2 c^2}{a^2} - \dfrac{a^2 b^2}{c^2} + 2 a^2 + 2 b^2 + 2 c^2 \right) $$ And $$ \sin^2(t) = \dfrac{1}{4 a^2} \left( - \dfrac{a^2 c^2}{b^2} - \dfrac{b^2 c^2}{a^2} - \dfrac{a^2 b^2}{c^2} + 2 a^2 + 2 b^2 + 2 c^2 \right) $$ We now have $$ \cos(t - s) = \cos(t) \cos(s) + \sin(t) \sin(s) $$ $$ \cos(t) \cos(s) = \dfrac{1}{4 a b} \left( \left( \dfrac{a b}{c} \right)^2 - \left( \dfrac{a c}{b} \right)^2 - \left( \dfrac{b c}{a} \right)^2 + 2 c^2 \right) $$ $$ \sin(t) \sin(s) = -\dfrac{1}{4 a b} \left( 2 a^2 + 2 b^2 + 2 c^2 - \left( \dfrac{a c }{b} \right)^2 - \left( \dfrac{ b c }{a } \right)^2 - \left( \dfrac{a b }{c} \right)^2 \right)$$ Adding the two expressions $$ \cos(t - s) = \dfrac{1}{2 a b} \left( - a^2 - b^2 + \left(\dfrac{a b }{c}\right)^2 \right) $$ Now $$f(t, s) = a \cos(t) + b \cos(s) + c \cos(t - s) $$ Substituting the above expressions for $\cos(t), \cos(s)$ and $\cos(t - s) $ gives us $$ f^* = \dfrac{1}{2} \left( \dfrac{b c}{a} - \dfrac{a c}{b} - \dfrac{ a b }{c} \right) + \dfrac{1}{2} \left( \dfrac{a c}{b} - \dfrac{bc}{a} - \dfrac{a b}{c} \right) + \dfrac{1}{2} \left( - \dfrac{ a c }{b} - \dfrac{b c }{a} + \dfrac{a b }{c} \right) $$ And this reduces to $$ f^* = - \dfrac{1}{2} \left( \dfrac{ab}{c} + \dfrac{ac}{b} + \dfrac{bc}{a} \right) $$ So in conclusion, the maximum is $ a + b + c $ and the minimum is given by the expression for $f^*$ . My question is:  Are my findings correct ? Any helpful comments, suggestions, and answers are highly appreciated. EDIT: After a comment by Andrei, I found that a necessary condition for the formula for $f^*$ to work is that $$ c \ge \dfrac{a b}{a + b} , \ a \ge \dfrac{b c}{b+c} , \ b \ge \dfrac{a c}{a + c} $$","['multivariable-calculus', 'calculus', 'trigonometry']"
4646907,"If $G$ is a finite group, show that there is some $g\in G$ so that $|G|\cdot |Sg \cap S| \leq |Sg|\cdot |S|$","Let $G$ be a finite group, and $S\subseteq G$ any non-empty subset. For any $g\in G$ , write $Sg:=\{s\cdot g\mid s\in S\}$ . I would like to show that there exists some $g\in G$ so that $|G|\cdot |Sg \cap S| \leq |Sg|\cdot |S|$ . I've tried a couple of things, like induction; it is clear when $|S|=1$ , so maybe if $T=S\cup\{t\}$ then we may write $Tg\cap T=(Sg\cap S)\cup H$ where $H$ is either empty, contains $t$ , contains $tg$ or contains both. But then I get kind of stuck. We have to change this $g$ so that the inequality works for $T$ , but I don't really see how we can change it without destroying the nice property we had for $S$ . Another thing I tried was simply out of contradiction. So assume $|G|\cdot |Sg\cap S|>|Sg|\cdot |S|$ holds for all $g\in G$ . Then maybe it would be ""too much"". For example, maybe this gives us some surjective map that should never be surjective. But I can't really think of anything here.","['group-theory', 'abstract-algebra', 'finite-groups']"
4646962,Potential and Field,"So given a field: $$\vec E(r)=\frac{\alpha(\vec p \cdot \vec e_r)\vec e_r + \beta \vec p}{r^3}$$ where $α, β$ are constants, $\vec e_r$ is the unit vector in the direction $\vec r$ , and $\vec p$ is a constant vector. I'm supposed to find out the relationship between $\alpha$ and $\beta$ such that $\vec E$ is a physical field. My first instinct is, since the field s given in direction of $r$ , I'm going to use spherical coordinates to make my life easier and then calculate $\nabla \times E = 0$ . I know that in spherical coordinates $$\nabla \times E = \frac{1}{r\sin \theta} \Big\{ \frac{\partial}{\partial \theta}(\sin \theta V_{\theta}) - \frac{\partial V_{\theta}}{\partial \theta}\Big\} + \cdots$$ For obvious reasons, I'm not typing everything out. My first question is, when we look at the field $$\vec E(r)=\frac{\alpha(\vec p \cdot \vec e_r)\vec e_r + \beta \vec p}{r^3}$$ isn't this the same as $$\vec E(r)=\frac{\alpha\vec p \cdot \vec e_r + \beta \vec p}{r^3}$$ because a unit vector dotted with itself is $1$ ? My second question: the direction of the $P$ vector isn't given except in the first part, how do I relate it to $\theta$ head and $\phi$ head? Third question (sorry for my shaky calculus foundation): taking the example of the first term of the curl of the field (which I typed out in LaTeX), since nothing in the $E$ field has a $\theta$ or $\phi$ term, the first term is $0$ ? I guess my insecurity about this assumption is because we have a constant vector pointing in some direction which unless is completely parallel to $r$ head we can relate its direction to $r$ head with appropriate $\theta$ and $\phi$ terms? Thanks for any help!","['multivariable-calculus', 'calculus', 'curl', 'vector-analysis']"
4646982,Evaluating $\int x \sqrt{x^2 - x}\ dx$,Problem is to integrate $ x \sqrt{x^2 - x}$ . My attempt: I made it ready for a substitution $u = x^2 - x$ $$\begin{aligned} \int x \sqrt{x^2  -x}\ dx &= \int (2x-1)\sqrt{x^2 - x} \ dx - \int(x - 1)\sqrt{x^2 - x}\ dx\\& = \int \sqrt u\ du  - \int(x - 1)\sqrt{x^2 - x}\ dx\\& = \frac23(x^2 -x)^{3/2} + C_1 - \int(x-1) \sqrt{x^2 -x}\ dx\end{aligned}$$ I don't know how to continue from here. Alternatively I tried this: $$\begin{aligned} \int x \sqrt{x^2 - x}\ dx &= \int x^2 \sqrt{1- \frac{1}{x}}dx\\ & \overset{1- \frac1x = t^2}{=} \int  \frac{2t^2}{(1-t^2)^4}\ dt\\& \overset{t =\sin(\theta)}{=} \int \frac{2\sin^2(\theta) \cos(\theta)\ d\theta}{\cos^4(\theta)}\\& = \int 2 \tan(\theta)\tan(\theta) \sec(\theta) \ d\theta\\ & = \int 2\sqrt{\sec^2(\theta) - 1}\tan(\theta) \sec(\theta) \ d\theta\\&  \overset{\sec(\theta) = u}{=} \int 2 \sqrt{u^2 - 1}\ du\\& = u \sqrt{u^2 - 1} - \ln|u + \sqrt{u^2- 1}| + C\\& = \sqrt{x^2 - x} - \ln|x + \sqrt{x^2 - x}| + C\end{aligned}$$ This method is very tedious. Is there any easy way to do the original integral?,"['integration', 'indefinite-integrals', 'calculus']"
