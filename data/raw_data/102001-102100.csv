question_id,title,body,tags
1419594,Prove that $ \lim_\limits{x \to \infty} x(\sqrt{x^2+1}-x) $ = $ \frac{1}{2}$?,"How can you prove that $$ \lim_\limits{x \to \infty}  x(\sqrt{x^2+1}-x) =  \frac{1}{2} \text{ ?}$$ I can not find a way to calculate this. This is one idea :
$$ \lim_{x \to \infty}  x(\sqrt{x^2+1}-x) \approx \lim_\limits{x \to \infty}  x(\sqrt{x^2}-x) = 0 $$ but that is wrong.","['limits-without-lhopital', 'radicals', 'limits']"
1419602,Which of the following condition ensure that the function $f:R^n\to R$ is continuous?,"I encountered an interesting problem in my Economics class about continuity. Which of the following conditions on the function $f:\mathbb R^n\to \mathbb R$ ensures that the function $f$ is continuous? For all $y$ the sets $\{x:x\in \mathbb R^n, f(x) < y\}$ and $\{x:x\in \mathbb R^n, f(x) > y\}$ are open; For all $y$ the sets $\{x:x\in \mathbb R^n, f(x) \le y\}$ and $\{x:x\in \mathbb R^n, f(x) \ge y\}$ are open; For all $y$ the sets $\{x:x\in \mathbb R^n, f(x) < y\}$ and $\{x:x\in \mathbb R^n, f(x)> y\}$ are closed; For all $y$ the sets $\{x:x\in \mathbb R^n, f(x) \le y\}$ and $\{x:x\in \mathbb R^n, f(x)\ge y\}$ are closed; I have proved using contradiction that 3, 4 implies continuity. The proof follows as below: Suppose we assume 3, and suppose to the contrary that $f$ is not continuous. Then there exists sequence $x_n\to x_0$ such that $f(x_n) \not\to f(x_0)$. So given $\epsilon>0$, there exists some $N\in \mathbb{N}$ such that for all $n > N$, $|f(x_n)-f(x_0)|>\epsilon$. The last inequality is equivalent to $f(x_n)<f(x_0)-\epsilon$ or $f(x_n)>f(x_0)+\epsilon$. Now from the assumption 3, the sets $\{x:x\in R^n, f(x) < f(x_0)\}$ and $\{x:x\in R^n, f(x)> f(x_0)\}$ are closed. Hence, it contains all its limit points. Without loss of generality, let us consider just left case. Since $f(x_n)<f(x_0)-\epsilon<f(x_0)$ for all $n>N$, the limit point of $x_n$ which is $x_0\in \{x:x\in R^n, f(x) < f(x_0)\}$. This implies $f(x_0)<f(x_0)-\epsilon$, a contradiction. But I was not quite sure how to prove/disprove 1, 2 implies continuity. Number 1 seems close to topological definition of continuity, but it is weak (I think). Any helps, hints, or counter examples will be appreciated.","['continuity', 'real-analysis', 'general-topology']"
1419638,"Coin toss problem, get exactly 2 heads in 5 tosses","Suppose we toss a fair coin until we get exactly 2 heads. What is
  the probability that exactly 5 tosses are required? My try: 
We have to make sure that the first 4 tosses does not have 2 heads and the last toss must be a head. That is, the first 4 tosses need to contain 1 head and 3 tails. The probability of this event is $\frac{4}{2^4}=1/4$. Then the probability of 5th toss is head is $1/2$. Hence, in the end the answer is $\frac{1}{4}\cdot\frac{1}{2}=\frac{1}{8}$. Am I correct?",['probability']
1419645,Vector fields and tangent vector fields?,"I am wondering if there are times when people would call a tangent vector field simply by a vector field? Are not these two concepts different? 
For example,
a vector field assigns (say) to each point of $\mathbb{R}^{n}$ exactly one point of $\mathbb{R}^{n}$; whereas a tangent vector field assigns (say) to each point $x$ of $\mathbb{R}^{n}$ exactly one point of the tangent space $\{ x\} \times \mathbb{R}^{n}$. 
Clearly, the vector field has range in $\mathbb{R}^{n}$, but the tangent vector field has range in the tangent bundle $\mathbb{R}^{n} \times \mathbb{R}^{n}$ of $\mathbb{R}^{n}$.","['differential-geometry', 'vectors', 'functions']"
1419648,Domain values of inverse funtion,If I'm plotting $$y=3e^{{x\over3}+1}$$ from $x=0$ to $x=1$ and on the same axes I want to plot its inverse $$y=3\ln\left({x\over3}\right)-3$$ but only for the domain values of $x$ given by the range of $f$ Would the inverse domain range be $x=8.15$ to $x=11.38$?,['functions']
1419652,"Prove that in any triangle $ABC$, $\cos^2A+\cos^2B+\cos^2C\geq\frac{3}{4}$","I have two similar looking questions. (1) Prove that in triangle $ABC$ , $$\cos^2A+\cos^2B+\cos^2C\geq\frac{3}{4}.$$ (2) If $\Delta ABC$ is an acute angled, then prove that $$\cos^2A+\cos^2B+\cos^2C<\frac{3}{2}$$ If I apply Jensen's inequality, then $\cos^2x$ is a concave function, because its second derivative is $-2\cos 2x$ and with it being concave function $$\cos^2A+\cos^2B+\cos^2C\leq\frac{3}{4}$$ which is not there in the question.How will we prove both of these questions. I have some intuition that in the second question, as $ABC$ is an acute angled triangle,this has something to do. Please guide me in the right direction. Thank you.","['convex-analysis', 'calculus', 'trigonometry']"
1419656,Closure in a topological product: is AC needed?,"I'm working on a proof of $\prod_{\alpha\in\Lambda}\overline{A_\alpha}=\overline{\prod_{\alpha\in\Lambda}A_\alpha}$ in the product topology. This has been asked before, i.e. Closure in a product of topological spaces , The closure of a product is the product of closures? but they aren't explicit on the parts which are giving me trouble. If $(C_\alpha)_{\alpha\in\Lambda}$ is a collection of closed sets, the product $\prod_{\alpha\in\Lambda}C_\alpha$ can be written as $\bigcap_{\alpha\in\Lambda}\pi_\alpha^{-1}(C_\alpha)$, and continuity of $\pi_\alpha$ implies $\pi_\alpha^{-1}(C_\alpha)$ is closed, so $\prod_{\alpha\in\Lambda}C_\alpha$ is also closed. Setting $C_\alpha=\overline{A_\alpha}$ this proves $\prod_{\alpha\in\Lambda}\overline{A_\alpha}\supseteq\overline{\prod_{\alpha\in\Lambda}A_\alpha}$. For the reverse inclusion, suppose $x=(x_\alpha)_{\alpha\in\Lambda}\in\prod_{\alpha\in\Lambda}\overline{A_\alpha}$, and let $U=\prod_{\alpha\in\Lambda}U_\alpha$ be a basic open set containing $x$, with $I\subseteq \Lambda$ finite such that $U_\alpha=X_\alpha$ for $\alpha\notin I$. Since $x_\alpha\in \overline{A_\alpha}\cap U_\alpha$, there is a $y_\alpha\in A_\alpha\cap U_\alpha$, so an application of the axiom of choice would give a $y=(y_\alpha)_{\alpha\in\Lambda}\in$ $\prod_{\alpha\in\Lambda}A_\alpha\cap U$ as desired. Is AC necessary here? So far I haven't even been able to show that $\prod_{\alpha\in\Lambda}\overline{A_\alpha}\ne\emptyset$ implies $\prod_{\alpha\in\Lambda}A_\alpha\ne\emptyset$, and surely the theorem can't be proven without establishing this.","['infinite-product', 'general-topology', 'axiom-of-choice']"
1419718,Wanted: A purely algebraic proof of the Frobenius theorem on distributions,"Is there a purely algebraic proof of the Frobenius theorem? Here's a rough sketch of what i'm looking for: Let $Der(R)$ denote the $R$-module of ($R$-valued) derivations of the algebra $R$ endowed with the lie bracket given by the commutator. Definiton: A distribution $D$ is a submodule of $Der(R)$. ""Frobenius"" Theorem - Under certain restriction on the base algebra $R$ 
  (and on the algebra $S$ that will be introduced) the following holds: A distribution $D \subset Der(R)$ is closed under the lie bracket of $Der(R)$ iff
  for every maximal ideal $m \subset R$ there exists an epimorphism $f: R \to S$,  such that after localizing $R$ by $m$ and $S$ by $f(m)$ we have: $v \in D_m \iff \exists u \in Der(S)_{f(m)}$ satisfying $f_m \circ v = u \circ f_m$. I'm sure there is a ""nicer"" algebraic formulation of this problem but that's the best i could do with my current knowledge - any improvement suggestions would be very welcome. Is there such a general theorem? Does it even make sense? Denoting the exterior algebra of $Der(R)$ by $\mathcal{A}^*$. Am i right that the following equivalence is purely algebraic and no geometric input is neaded? (i did prove it, i think... need to be sure): A distribution $D \subset Der(R)$ is closed under the lie bracket $\iff$ $I(D) = \bigcup_k \{\omega \in \mathcal{A}^k : \omega(m_1,...,m_k)=0  \text{ for every tuple of elements } \{m_i\}_{i \le k} \subset D \} \subset \mathcal{A}^*$ is a differential ideal . ($d I(D) \subset I(D)$).","['algebraic-geometry', 'differential-geometry', 'differential-forms', 'commutative-algebra']"
1419754,How to complete this proof? Union of a countably infinite set and a finite set is countably infinite,"Theorem. Let $X$ be a countably infinite set and $Y$ be a finite set. Then $X\cup Y$ is countably infinite. Proof. Since $X$ is a countably infinite set, then there exists a bijection function $f\colon \mathbb{N}\longrightarrow X$ and since $Y$ is a finite set, then $X\cup Y$ is (why) is infinite. Therefore there exists (why) a bijection function $g\colon X\longrightarrow X\cup Y$ so $g\circ f\colon \mathbb{N}\longrightarrow X\cup Y$ is a 1-1 and surjective function. This means that $X\cup Y$ is countably infinite. Could you please tell me the reason of the two WHYs?",['elementary-set-theory']
1419816,is it true every left inverse of a matrix is also right inverse of it?,"I am wondering that, consider there are $m$ linear equations with $n$ unknowns. We can represent it as $AX=B$. Let $L$ is the left inverse of $A$ therefore $LA=I$. Again from $AX=B$, we get $LAX=LB$ implies $X=LB$. Till this I have no problem but from $X=LB$, multiplying it by $A$ we get $AX=ALB$ implies $B=ALB$. So does it imply also $AL=I$ ?","['inverse', 'linear-algebra', 'matrices']"
1419857,Proving that $\tan^n\angle A + \tan^n\angle B + \tan^n\angle C \ge 3 + \frac{3n}{2}$,"Given a acute $\triangle ABC$ . Prove that $$\tan^n\angle A + \tan^n\angle B + \tan^n\angle C \ge  3 + \dfrac{3n}{2}$$ I have tried by using a inductive proof. In case $n=0$ , the equality holds. In case $n=1$ , we can prove that $\tan\angle A + \tan\angle B + \tan\angle C \ge  3\sqrt3>\frac92$ . Use AM-GM inequality, we obtain: $$\tan A+\tan B+\tan C\ge3\sqrt[3]{\tan A\tan B\tan C}=3\sqrt[3]{\tan A+\tan B+\tan C}$$ Hence, we have Q.E.D However, I think that by using this way, I can't prove my problem. Any Hint ? Or any solution which is simplier?","['inequality', 'trigonometry']"
1419868,How can I show that $\sum \limits_{n=2}^\infty\frac{1}{n\ln n}$ is divergent without using the integral test? [duplicate],This question already has answers here : Infinite series $\sum _{n=2}^{\infty } \frac{1}{n \log (n)}$ (6 answers) Closed 3 years ago . How can I show that $\sum \limits_{n=2}^\infty\frac{1}{n\ln n}$ is divergent without using the integral test? I tried using the comparison test but I could not come up with an inequality that helps me show the divergence of a series. I also tried using the limit comparison test but I was not successful. Please do not give me solutions; just a hint so that I can figure it out myself.,"['sequences-and-series', 'calculus']"
1419906,"$\frac{d^{100}}{dx^{100}}\left[\frac{f(x)}{g(x)}\right]=\frac{p(x)}{q(x)}$,then find the degrees of the polynomials $p(x)$ and $q(x)$","Let $g(x)=x^3-x$,and $f(x)$ be a polynomial of degree $\leq100$.If $f(x)$ and $g(x)$ have no common factor and $\frac{d^{100}}{dx^{100}}\left[\frac{f(x)}{g(x)}\right]=\frac{p(x)}{q(x)}$,then find the degrees of the polynomials $p(x)$ and $q(x)$. I tried this problem.SInce the degree of $f(x)$ is atmost 100,and degree of $g(x)$ is 3,so the atmost degree of $\frac{f(x)}{g(x)}$ is 97 and after differentiation we get degree of $\frac{p(x)}{q(x)}$ as $-3$ but i cannot exactly pinpoint the degree of $p(x)$ and $q(x)$. Answer in my book says degree of $p(x)$ is $6\times 2^{99}-101$ and the degree of $q(x)$ is $6\times 2^{99}$.How can i get this answer.Please help me.","['polynomials', 'derivatives']"
1419937,Prove that the order of an element in $S_n$ equals the least common multiple of the lengths of the cycles in its cycle decomposition.,"Prove that the order of an element in $S_n$ equals the least common multiple of the lengths of the cycles in its cycle decomposition. Proof: Let $\sigma \in S_n$ . Then $\sigma = (a_1a_2...a_{m_1})(a_{m_1+1}a_{m_1+2}...a_{m_2})....(a_{m_{k-1}+1}a_{m_{k-1}+2}...a_{m_{k}})$ represents the cycle decomposition of $\sigma$ .
Suppose $|\sigma| = n$ is the order of an element in $S_n$ .
So $\sigma^n = 1$ . Can someone please help  me?  I don't know how if I am doing this fine. And I am stuck. Thank you for any help.","['abstract-algebra', 'group-theory', 'symmetric-groups', 'permutations']"
1419938,Example of topological spaces where sequential continuity does not imply continuity,"Please give an example of a function $f : X \to Y  $ where $X,Y$ are topological space , such that there exist $x \in X$ such that for every sequence $\{x_n\}$ in $X$ converging to $x$ , $\{f(x_n)\}$ converges to $f(x)$ but $f$ is not continuous at $x$ ; also please give such an example that $f$ is not continuous any where in the domain but for every $x \in X$ and sequence $\{x_n\}$ in $X$ converging to $x$ , $\{f(x_n)\}$ converges to $f(x)$.","['continuity', 'examples-counterexamples', 'general-topology']"
1419990,What is the graph that corresponds to $Q'_8$ generalized quadrangle ? Could you please explain this in plain english?,"In this paper a table about large graphs with given degree and diameter graphs is shown: I would like to know what the adjacency list of the graph denoted by: in the table above is. Could you please explain how I can get that adjacency list of $Q'_8$? The paper says that : and . So $Q_8$ is the incidence graph of the regular generalised quadrangle and $Q'_8$ is the quotient of this. The problem is that I only understand the words ""the"", ""so"", ""and"", ""is"", ""of"" and ""this"" from the previous sentence. What is the incidence graph of a regular generalized quadrangle ? Is this incidence graph bipartite ? If yes, why ? What is the quotient of that ? How can I get the actual graph based on the above sentence ? Is Mathematica a good tool for this task?","['graph-theory', 'geometry']"
1419995,"Who is the ""father of number theory""?","I noticed that some sources state Fermat as the father of modern number theory while others say Gauss. I am trying to start a paper on the history of number theory for a presentation, but I cannot figure out the fundamental difference between the roles of Fermat and Gauss in founding modern number theory so to speak. What exactly is the difference between them; can they be likened to Newton and Cauchy for example? (Newton founding calculus, Cauchy putting it on a rigorous foundation once and for all.) Thanks!","['math-history', 'number-theory', 'soft-question']"
1420014,Real Linear vs. Complex Linear,"I recently started a new math course and got hung up on a particular problem from the book ""Linear Algebra Done Wrong"". Specifically, problem 1.3.6 (c). I am an engineer, and I believe I simply lack terminology/definition to solve the problem. Again, it is part (c) of the problem: The set $\mathbb C$ of complex numbers can be canonically identied with the space $\mathbb R^2$
by treating each ($z = x + iy$) of $\mathbb C$ as a column $(x , y)^T$ of $\mathbb R^2$. Define $T(x+iy) = 2x-y+i(x-3y)$. Show that this transformation is not
a linear transformation in the complex vectors space $\mathbb C$, but if we treat $\mathbb C$ as the real vector space $\mathbb R^2$ then it is a linear transformation there (i.e. that $T$ is a real linear but not a complex linear transformation). Find the matrix of the real liner transformation $T$. It appears to me that: $T(au+bv) = aT(u)+bT(v)$ I can't see why it is not linear on $\mathbb C$ but it is on $\mathbb R^2$. However, I recognize that I don't understand how this changes the outcome. Any ideas? Thanks","['linear-algebra', 'transformation', 'linear-transformations']"
1420047,Equivalence of integral kernels and basis transformations,"I am not sure if I have seen integral transforms in the right way, but given a transform like Fourier transform - it's actually a basis transformation right ? $$ F(y) = \int K(x,y) f(x) \text{d}x $$
where $K(x,y) = \text{e}^{-ixy}$ for the case Fourier transform. The functions $F(y)$ and $f(x)$ can be seen as $\left<y|F\right>$ and $\left<x|f\right>$ respectively. In such a case the above integral equation can be rewritten as - $$ \left< y|F \right> = \left<y|\mathbb{\hat I}|F\right> = \sum_x \left<y |x\right> \left<x |f\right> $$ So is $\left<y |x\right>$ one way of looking at the integral kernel for all general cases ? If not, I wish to understand how one can precisely look at integral kernels. EDIT 1: I also wish to know that can transforms like Laplace, Mellin etc. also be treated like that as Transformation matrix, also in which case it might not be unitary matrix in all cases, but rather just a map from one inner product space to another.","['integral-transforms', 'inner-products', 'functional-analysis', 'integration']"
1420075,Rolling a circle around a two dimensional curve,"This is a sort of funny idea I had the other day, and although I expect to get a very technical answer I am fine with any intuitive explanation. Consider being given a function in the plane, for example $y = x^2$, or maybe even implicitly defined functions. If we were to take a circle and roll it along either side of every part of the curve defined by the equation, what is the largest circle that can do this without having to jump any gaps of the curve? So for instance, $y = x^2$  has points at $(-1, 1)$ and $(1, 1)$, a distance of $2$ units, so obviously a circle of diameter $2$ rolling along from the right would not get down to the groove of the origin, it would skip over to the other side above somewhere and have to roll up again. I assume this has something to do with the second derivative but I'm not sure quite how to apply it. I understand the problem is very general, so a solution to just the case $y = x^2$ would suffice for me. I am not very good at geometry admittedly, but I am open to any interesting suggestions of that form. If there are any topological explanations for this kind of thing, such as a definition I am unfamiliar with, I would like to know about that as well.","['geometry', 'differential-geometry']"
1420092,Prove the size of the result of a cartesian product is equal to the product of the size of the two sets.,"We want to show: $$|S_1 \times S_2| = |S_1| \cdot |S_2|$$ I am not sure how to go about showing this in general terms. I believe that we will need to use the definition of the cartesian product during this, so that is where I am at right now. $$S_1 \times S_2 = \{(a,b): a \in S_1 \; \text{and} \; b \in S_2\}$$ These are finite sets.",['elementary-set-theory']
1420109,Does every compact simply-connected subset of $\mathbb{R}^n$ have an efficient $r$-covering path for all $r>0$?,"Let $A$ denote a subset of $\mathbb{R}^n$. Definition 0. Given a positive real number $r$, an $r$-covering path of $A$ is a non-negative real number $T$ together with a differentiable function $c:[0,T] \rightarrow \mathbb{R}^n$ satisfying: for all $a \in A$, there exists $t \in [0,T]$ such that $\|a - c(t)\| \leq r.$ Remark. I'm not especially attached to the details of the above definition. For example, you may wish to replace ""differentiable"" with ""rectifiable."" Or, you may wish to include an assumption that $c'(t) \neq 0$ for all $t \in [0,T],$ in order to remove the possibility of ""kinks"" from the path. So basically, use whatever definition makes the question easier to answer in the affirmative. Definition 1. An $r$-covering path of $A$ is efficient iff there is no $r$-covering path of strictly shorter length. I reckon the following question is pretty interesting, especially if the answer is ""yes."" Question. Is it true that for all $r>0$, every compact simply-connected subset of $\mathbb{R}^n$ has an efficient $r$-covering path? The conditions ""compact and simply-connected"" were basically plucked out of thin air to in order to eliminate weird and silly counterexamples. Anyway, feel free to modify them as you see fit. The same goes for ""differentiable"": feel free to replace this with ""smooth"" or even ""analytic"" if it makes for a cleaner answer.","['connectedness', 'real-analysis', 'general-topology', 'compactness']"
1420150,The meaning of the Imaginary value of the Residue while Evaluating a Real Improper Integral,"When evaluating the improper integral $$\int_{0}^{\infty}\frac{x^{3}\sin\left(2x\right)}{\left(x^{2}+1\right)^{2}}\,dx$$ (which is an even function, so half of the $(-\infty,\infty)$ integral), I used the function $$\oint_{\gamma}\frac{iz^{3}e^{-2iz}}{\left(z^{2}+1\right)^{2}}\,dz,$$ over the upper half circle with the radius $r$ and then returning through the reals using the residue method (and made sure that when $r\to\infty$, the half circle's integral is $0$). I found that:
$$\operatorname{Res}\left(\frac{iz^{3}e^{-2iz}}{\left(z^{2}+1\right)^{2}},i\right)=\left.\frac{d}{dz}\left(\frac{iz^{3}e^{-2iz}}{\left(z+i\right)^{2}}\right)\right|_{z=i}=ie^{2}$$
which means:
$$\int_{0}^{\infty}\frac{x^{3}\sin\left(2x\right)}{\left(x^{2}+1\right)^{2}}dx=\frac{1}{2}\int_{-\infty}^{\infty}\frac{x^{3}\sin\left(2x\right)}{\left(x^{2}+1\right)^{2}}dx=\frac{1}{2}\Re(ie^{2})=0.$$
While I evaluated the integral, I wanted to know: what does $\Im\left(\operatorname{Res}\left(f,i\right)\right)=e^2$ mean. Clearly not $\int_{-\infty}^{\infty}\frac{x^{3}\cos\left(2x\right)}{\left(x^{2}+1\right)^{2}}dx$, since it's an odd function which means the integral is zero. Have I made a mistake anywhere? If not, what does $e^2$ mean?","['contour-integration', 'complex-analysis', 'improper-integrals', 'residue-calculus']"
1420151,Binomial expansion derivative limit definition,Can someone help me with this? I am supposed to use a binomial expansion to calculate $\sqrt x$ directly from the limit definition of a derivative.,"['calculus', 'limits', 'derivatives']"
1420154,A question about angles in the Euclidean plane,"It has long been known that an arbitrary angle (in the Euclidean plane) cannot be trisected using only ruler and compass, but that this can be done using a mechanical linkage. Given any positive integer $n$ greater than 1, does there always exist a mechanical linkage (as defined by Kempe) that can divide an arbitrary angle (in the plane) into $n$ equal parts?",['geometry']
1420164,Proving $(1-x)\cdot (1-x^2)\cdots(1-x^{n-1})=n$ if $x^n=1$ and $x\neq 1$ [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 8 years ago . Improve this question If we have a equation $x^n=1$, then how can we prove $$(1-x)\cdot (1-x^2)\cdots (1-x^{n-1})=n $$ when $x$ is not $1$? I know that $x= e^{(2\pi + 2k\pi)/n}$ and we can get different value of $x$ when $ k=0,1....,n-1$ but expanding our product will be a heavy task, is there any simpler way around?","['complex-analysis', 'complex-numbers']"
1420222,Different results in integrating both sides of $\sin{2x}=2\cos x\sin x$,I feel like there is something I am missing here. When integrating both sides of the trigonometric identity $\sin{2x}=2\cos x\sin x$ I get different results. The left side of course results in $-\frac{1}{2}\cos{2x}+C$. The right side I solve with u-substitution: $u=\cos x$ $du=-\sin x dx$ $-2\int udu=-u^2+C=-\cos^2 x+C$ While writing this question I noticed another identity $\cos^2 x=\frac{1}{2}+\frac{1}{2}\cos 2x$. So apparently the $\frac{1}{2}$ falls out because of the $+C$ resulting from indefinite integration? This is still a little confusing to me.,"['trigonometry', 'integration']"
1420226,Algebraic and combinatorial proof of an identity,"For any two integers $2 \le k \le n-2$, there is the identity
$$\dbinom{n}{2} = \dbinom{k}{2} + k(n-k) + \dbinom{n-k}{2}.$$ a) Give an algebraic proof of this identity, writing the binomial coefficients in terms of factorials and simplifying. b) Give a combinatorial proof (and interpretation) of this identity. For part a, I turned the combinations into factorials and tried to get the RHS equal to the LFS, which is $\frac{n!}{2!(n-2)!}$. However, I got stuck on the third step with all the $k$'s and $(n-k)!$'s. For part b, I got as far as saying you choose 2 kids out of n kids to receive candy on the LHS. I see the $k$ and $n-k$ in a way accounts for each other on the right but I cannot explain this in words. I am fairly new to mathematical proofs and any help is appreciated. Thanks!","['combinations', 'statistics', 'combinatorial-proofs', 'combinatorics']"
1420232,Confusing probability problems based on product rule and combinations,"I am going thru probability exercise. Faced first problem: Book Q1. Ten tickets are numbered 1,2,3,...,10. Six tickets are selected at random one at a time with replacement . What is the probability the largest number appearing on the selected tickets is 7? My logic: if one of six tickets should be 7, the $\color{red}{\text{remaining 5}}$ can be any of 1 to 7, so it should be $7^5$. But turns out that the given solution is $\frac{7^6-6^6}{10^6}$. My Q1. Though I understood the logic behind $\frac{7^6-6^6}{10^6}$, I was wondering what is exact logical mistake with $7^5$? I guessed that $7^5$ completely ignores what should be 6th ticket, it only puts restriction on 5 tickets. Is it like that? Then I came across similar but more involved problem, with significant difference from above one that it performs action without replacement: Book Q2. Three numbers are chosen at random without replacement from (1,2,3,...,10). What is the probability that the minimum number is 3 or the maximum number is 7? My logic: Noticing that this is without replacement, I guessed the solution should be 
$$
=
\begin{pmatrix} 
\text{selections with}\\ 
\text{minimum}\\ 
\text{number is 3} 
\end{pmatrix}
+
\begin{pmatrix} 
\text{selections with}\\ 
\text{maximum}\\ 
\text{number is 7} 
\end{pmatrix} 
-
\begin{pmatrix} 
\text{selections with}\\ 
\text{maximum}\\ 
\text{number is 7}\\ 
\text{and minimum}\\ 
\text{number is 3} 
\end{pmatrix} 
$$ $$=
\frac{
\overbrace{(\binom{8}{3}-\binom{7}{3})}^{\text{#selections with min 3}}
+
\overbrace{(\binom{7}{3}-\binom{6}{3})}^{\text{#selections with max 7}}
-
\overbrace{3\times {^3P_3}}^{\text{#selections with max 7 and min 3}}
}
{\binom{10}{3}
}
$$
But the book solutions says: P(minimum 3) or P(maximum 7) P(minimum 3) $=\frac{\binom{7}{2}}{\binom{10}{3}}=\frac{21}{120}$ P(max 7) $=\frac{\binom{6}{2}}{\binom{10}{2}}=\frac{15}{120}$ Thus the solution is $\frac{11}{40}$ My Q2. How even by books logic the solution $\frac{11}{40}$ is achieved. I am unable to understand it as I find the explanation insufficient. My Q3. If book Q2 answer is correct then why for book Q1 solution is not $7^5$ which is what I initially guessed (because the only difference being with / without replacement, the logic of getting $\color{red}{\text{remaining m}}$ stuffs out n should remain same)? My Q4. If we make first question without replacement, will the solution be $\frac{\binom{7}{6}-\binom{6}{6}}{\binom{10}{6}}$? My Q5. What will be the solution if we make book Q2 with replacement? My Q6. Where my logic for solution to Book's Q2 is wrong?","['probability', 'combinatorics']"
1420236,Runge-Kutta methods with strictly positive Butcher tableau,"An explicit $s$-staged Runge-Kutta method for an autonomous ODE $\dot y = L(y)$ can be written as
$$
k_i = L\left(y_n + \tau\sum_{j=1}^{i-1} a_{ij} k_j \right)\\
y_{n+1} = y_n + \tau\sum_{i=1}^s b_i k_i.
$$
It seems that there exist some methods for which all the coefficients $a_{ij}, b_{i}$ are positive for $j < i$, e.g.: $s = 1$ explicit Euler method
$$
\begin{array}{c|c}
&\\
\hline
&1
\end{array}
$$ For $s = 2$ explicit trapezoid method
$$
\begin{array}{c|cc}
&&\\
1&1\\
\hline
&1/2 &1/2
\end{array}
$$ For $s = 3$ (could not find the name, but I've checked the order conditions)
$$
\begin{array}{c|ccc}
\\
1&1\\
1/2&1/4&1/4\\
\hline
&1/6&1/6&2/3
\end{array}.
$$ These methods are important for building TVD Runge-Kutta methods. It seems that there's no method with four stages of the fourth order with positive Butcher tableau, but I wonder if there are methods of fourth order and $s > 4$ satisfying the positiveness condition.","['numerical-methods', 'ordinary-differential-equations']"
1420256,Complex Chebyshev Polynomials,"Chebyshev Polynomials can be used to compute a very nearly minimax polynomial approximation of an analytic function on $[-1,1]$.  Is there a complex analog that can compute a nearly minimax polynomial for a holomorphic function on the unit disc?","['chebyshev-polynomials', 'numerical-methods', 'complex-analysis']"
1420267,Having an independent event with animals,"In a building for 24 apartments. It is known that there is only one dog in 8 apartments and a single cat in 6 apartments. How many apartments must have cat and dog for events ""have dog"" and "" have cat"" to be independent? I know that Events $C$ and $D$ are independent if
$$P(C\cap D)=P(C)P(D)\ .$$ probability of cats 8/24 = 1/3
and probability of dogs 6/24 = 1/4
probability of cats or dog =(1/4+1/3) = 7/12? I have to find a number that do the probability be 1/1. Am I riht??","['probability', 'statistics']"
1420281,Fibonacci identity: $f_{n+1}f_{n-1} = f_n^2 + (-1)^n$,"How do I see that $$f_{n+1}f_{n-1} = f_n^2 + (-1)^n$$ for $n \ge 2$ , where $f_1 = 1$ , $f_2 = 1$ , and $f_{n+2} = f_{n+1} + f_n$ for $n \in \mathbb{N}$ ?","['recurrence-relations', 'induction', 'algebra-precalculus', 'fibonacci-numbers']"
1420284,What is a surjective function?,"I am a 9th grader self-studying about set theory and functions. I understood most basic concepts, but I didn't understand what is a surjective function. I have understood what is an injective function, and if I know what is a surjective function, I think I could understand what is a bijective function (this is my main goal). In formal terms a function $f$ from $A$ to $B$ is said to be surjective if for all $y$ in $B$, there exists $x$ in $A$ such that $f(x)=y$. I don't understand this clearly because i'm still new to these notations. Can you explain this in intuitive way? And for example can you give me a surjective function that is not injective, and inversely, and neither one of the two?","['elementary-set-theory', 'functions']"
1420306,"""Hard"" exercises on Linear Algebra and Analytic Geometry","I started lecturing this subject called ""Linear Algebra and Analytic Geometry"" and in the second day of class I was approached by an undergrad student, asking for referenced that would contain ""hard"" exercises. I understand that the ones on the book I'm mostly following might seem pretty basic, since they aim for more ""average"" students (by the way, this is in Brazil). I checked on this student's profile and noticed that she has actually received some medals on Brazil's Olympiads of Mathematics for teenagers. I do not want to lose this student to get bored with her first experience with University-level Mathematics (her graduation subject is Computer Science), and since she said she'd have no problem with an english or spanish textbook, I would like to ask my fellow Stackers for textbooks that contain above-average exercises on Linear Algebra and Analytic Geometry. ""Theoretical"" exercises, as in ""show this identity"" or ""proof this result"" are more into what I'm expecting. Thanks in advance.","['linear-algebra', 'analytic-geometry']"
1420316,Are $B=PAP^{-1}$ and $B=P^{-1}AP$ equivalent?,"Im looking at the solution to one of my questions.
Basically, we started off with a matrix $A$ (in the elementary basis) which we want to convert into a diagonal matrix $B$ of another basis. Question 1: The teacher uses $B=PAP^{-1}$ and $B=P^{-1}AP$ interchangeably, can I really do that if $P$ is the basis of $B$? I would think they would give different values for $B$ wouldn't they? Question 2: Since Matrix $A$ is in the elementary basis, isn't $BP=A$? Why can't we find $B$ by $AP^{-1}$ and why do we have $BP=P^{-1}A$?","['transformation', 'matrices']"
1420375,"Real Analysis, Cauchy but not null.","I came across this question in a book on p-adic numbers and thought it looked interesting. However, I am having trouble getting started with it. Any hints/suggestions is much welcomed Let $(a_n)$ be Cauchy but not null. Show there exists a $c>0$ and $N>0$ such that $||a_n||>c$ when $n>N$.","['analysis', 'sequences-and-series']"
1420386,"Why countable unions, intersections etc.?","I was just wondering why one always insists on countability when it comes to the definition of a $\sigma$-algebra in measure theory. I mean, measure theory works as it does, but is there a deeper reason why e.g. it is reasonable to demand that the countable union of measurable sets is measurable again? Why not an uncountable union? Thanks in advance!","['intuition', 'definition', 'measure-theory']"
1420433,"Is there more than one way to divide the ""L""-shaped tromino into four congruent, connected pieces?","Recently my sister-in-law, who is training to become a high school mathematics teacher, asked me the following question: Consider the following polygon constructed by adjoining three squares of equal area. (Aka, a ""tromino"".)  Determine a method of subdividing this polygon into four congruent polygons. After a some minutes, I came up with what I think is the most obvious answer (my apologies for my scribbled drawings). (SPOILER!) In retrospect, this is an intuitive answer because it consists of constructing the subdivisions by rescaling the original polygon and applying a Euclidean isometry to the result.  To my great dismay, I can't think of another answer. Question. How many answers are there to the question posed by my sister-in-law?  If the answer above is unique, is it easily provable, and what is the proof? Addendum. I'd also be interested if there were a tiling of the tromino consisting of four congruent, connected parts that are not necessarily polygons.",['geometry']
1420439,Linear equation and linear differential equations,"I remember noting from an algebra class that $x$ and $y$ of a linear equation neither divide or multiply with each other which is somewhat clear from the forms of linear equations: General form of linear equation: $Ax + By + C = 0$ Slope intercept form: $y = mx + b$ Is this also true for linear differential equations ? The definition goes like this: ""A differential equation is said to be linear if the dependent variable and its differential coeficients (derivates) occur only in the first degree and not multiplied together."" ${dy \over dx} = {Py + Q}$ Where P, Q are functions of $x$ only. What exactly does this mean? Does the algebraic linear equation has something to do with linear differential equation?","['linear-algebra', 'ordinary-differential-equations']"
1420440,"A Vitali set is non-measurable, direct proof, without using countable additivity","I am teaching a measure theory class, where we are in the process of constructing Lebesgue measure on $\mathbb{R}$ via the usual Caratheodory outer measure construction. As motivation, we began by constructing a Vitali set $V \subset [0,1)$ which has the property that $\bigcup_{q \in \mathbb{Q} \cap [0,1)} V \oplus q = [0,1)$, where $\oplus$ is the usual ""addition mod 1"", and the sets $V \oplus q$ are pairwise disjoint.  This led us to conclude that Lebesgue measure cannot measure every set; i.e. there is no measure defined on all sets which is countably additive, translation invariant, and has $m([0,1)) = 1$. We have also constructed Lebesgue outer measure $m^*$ in the usual way, by defining $m^*(A) = \inf\left\{\sum_{i} b_i - a_i : A \subseteq \bigcup_i [a_i, b_i]\right\}$, and proved that it is countably subadditive, translation invariant, and that $m^*([a,b]) = b-a$. Now the typical next step is to define a set $E$ to be measurable if for every $A \subset \mathbb{R}$ we have $m^*(A) = m^*(A \cap E) + m^*(A \cap E^c)$.  We will of course show that when $m^*$ is restricted to the measurable sets, it is countably additive.  It would then follow, indirectly, that the Vitali set $V$ cannot have been measurable. But since this is a lot of work, I would like to start by proving directly that $V$ is not measurable; i.e. find a set $A$ such that $m^*(A) < m^*(A \cap V) + m^*(A \cap V^c)$.  This should help to motivate the definition of ""measurable"". I presume $A = [0,1)$ should work, so that we should try to prove $1 < m^*(V) + m^*([0,1) \setminus V)$.  Of course it is clear from countable subadditivity that we must have $m^*(V) > 0$ and $m^*([0,1) \setminus V) > 0$.  But I don't immediately see how to prove the sum exceeds 1. So in short: Is there a simple proof that $m^*(V) + m^*([0,1) \setminus V) > 1$, using only the basic properties of outer measure $m^*$, and in particular not using the countable additivity of Lebesgue measure? I also saw Outer Measure of the complement of a Vitali Set in [0,1] equal to 1 .  It's hard to follow without the textbook in question, but it seems to use the fact that open sets are measurable, and that $m^*(A) = \inf\{m^*(U) : A \subset U, U \text{ open}\}$.  Again, I would like to avoid that if possible.","['lebesgue-measure', 'real-analysis', 'measure-theory']"
1420445,"How many $3$ integer subsets have no consecutive integers, where integers are less than $20$?","I have to determine how many integers between $1$ and $20$ are possible if no two consecutive integers are in a set. I've thought it has something to do with a combination of an element $(a,a+2,a+4)$ and all of these possibilities but I know I'm missing something. I don't know how I would determine all of the possibilities, especially say $a=19$, would $a+2$ loop back around to $1$? I may be completely off in this approach though, so any help would be great.","['discrete-mathematics', 'combinatorics']"
1420472,Connecting a vector space to its dual - why?,"Can someone explain to me - intuitively - why embedding a vector space into its dual should naturally fix its geometry? I mean, I can run the usual statements through my mind - ""The injection into the dual gives an non-degenerate bilinear form (inner product), which allows us to define length, angle, etc, while conversely the bilinear form has an embedding into the dual as a by-product"", but I feel that there is some sort of understanding that is fluttering just over my head as I ponder these arguments. Why is this the right way, or the natural way, to go about putting a geometry on the vector space? Once we have fixed such a geometry, what do we gain from interpreting a vector as a functional and vice versa? It just seems very strange that once a correspondence with the dual has been fixed, then so should whatever geometrical properties that the space may have, or why the natural geometry of the space should arise from this connection.","['vector-spaces', 'linear-algebra', 'intuition']"
1421491,distribution of one random over the sum of random variables,"Suppose that $X_1,\ldots,X_n$ are independent random variables with $X_i\sim Gamma(\alpha_i,\beta)$. Define $U_i=\frac{X_i}{X_1+\cdots+X_n}$ for $i=1,2,\ldots,n$. Show that $U_i\sim Beta(\alpha_i,\sum_{j\neq i}\alpha_j)$. This is a question of past comprehensive exam. It also gave a hint: Think of $U_i$ as $X_i/(X_i+W)$, where $W=\sum_{j\neq i}X_j$ is independent of $X_i$. Can someone give me more hint about it?","['probability', 'statistics', 'statistical-inference']"
1421497,How do we address a function whose values are again functions?,"One may call a function whose values are functions simply a function-valued function. But is there a canonical name for such an object? A $k$-form on some open $A \subset \mathbb{R}^{n}$ is an example of such an object, for it assigns to every $x \in A$ exactly one alternating $k$-tensor on the tangent space $T_{x}(\mathbb{R}^{n})$, which is a function $\prod_{1}^{k}T_{x}(\mathbb{R}^{n}) \to \mathbb{R}$.","['differential-geometry', 'terminology', 'functions']"
1421526,How can there be two different answers from two identical equations for the same value of $x$? $(x-1)/(x^2-1)$ and $1/(x+1)$.,"Given the following equation: $$f(x) = \frac{x-1}{x^2-1}$$ find the limit of $f(x)$ when $x \to 1$. I know how to solve it, by simplifying the above equation to $f(x)=1/(x+1)$, giving the answer $1/2$. But my question is: $(x-1)/(x^2-1)$ is identical to $1/x+1$. How come $(x-1)/(x^2-1)$ gives $0/0$ when $x=1$, but $1/(x+1)$ gives $1/2$ when $x=1$?","['limits', 'algebra-precalculus']"
1421547,Will the Lebesgue integral of a real valued function always be a Riemann sum?,"If we have a real valued integral that is Lebesgue integrable but not Riemann integrable, can the value of the Lebesgue integral be given by a Riemann sum by choosing appropriate points in the partitioning process? Example: The indicator function for the irrationals: 
$$f(x)=\begin{cases} 1 &\mbox{if } x\not\in \mathbb{Q}\\
                     0 &\mbox{if } x \in \mathbb{Q} \end{cases}$$ has Lebesgue integral $1$ on $[0,1]$. This is the same value we would get if we formed the Riemann sum using irrational endpoints.","['real-analysis', 'measure-theory']"
1421551,"Is the ""product topology"" a topology?","The question is Let $(\Omega_1,\tau_1)$ and $(\Omega_2,\tau_2)$ be two topological spaces, then is $\left( {{{{\Omega }}_1} \times {{{\Omega }}_2},\tau} \right)$ where $\tau=\{A\times B:A\in \tau_1,B\in \tau_2\}$ also a topological space? I was trying to prove the claim but was not successful. $\emptyset  \times \emptyset  = \emptyset  \in {\tau _1} \times {\tau _2}$; $Ω_1×Ω_2∈τ_1×τ_2$. Given finitely many $A_1,A_2,…,A_n\in\tau$, then each $A_k=U_k\times V_k$ where $U_k∈τ_1,V_k∈τ_2$ for each $k=1,2,…,n$. Then $⋂_{k=1}^nA_k=⋂_{k=1}^n(U_k×V_k)=(⋂_{k=1}^nU_k )×(⋂_{k=1}^nV_k) ∈τ_1×τ_2$. To see the last equality, check $(x,y)∈⋂_{k=1}^nU_k×V_k⟺(x,y)∈U_k×V_k⟺x∈U_k,y∈V_k$ for every $k ⟺x∈⋂_{k=1}^nU_k ,y∈⋂_{k=1}^nV_k ⟺(x,y)∈(⋂_{k=1}^nU_k ×⋂_{k=1}^nV_k )$. However, I got a problem for union. Given a family of sets $\{A_k \}_{k∈J}$, then each $A_k=U_k×V_k$ where $U_k∈τ_1,V_k∈τ_2$ for each $k∈J$. Then $⋃A_k =⋃(U_k×V_k)$. I get stuck here since we only have $⋃(U_k×V_k)\subseteq(⋃U_k)\times (⋃V_k)$ rather than equality. I now doubt the claim might be false. Can anyone help with 3) or give a counterexample? Thank you!",['general-topology']
1421557,Extending the domains of densely defined bounded integral transforms on $L^2(\Bbb R)$,"This is a question I've contemplated for quite some time since it's pretty closely related to Fourier theory (particularly choosing the ""right"" space to define the Fourier transform on). However I've never been able to come up with anything resembling an answer for this. Nor have I seen it be addressed anywhere. Let $X$ be dense in $L^2(\Bbb R)$ and $T:X\subseteq L^2(\Bbb R)\to L^2(\Bbb R)$ be the integral operator given by $$ Tf(x) = \int_{-\infty}^{\infty} k(y,x) f(x)\,dx$$ where the integral is the Lebesgue integral. Assume that $T$ is bounded. If $g\in L^2(\Bbb R)\setminus X$ but we have that $$ \int_{-\infty}^{\infty} |k(y,x)| |g(x)|\,dx < \infty$$ for each $y$ (i.e. $Tg$ is well-posed). Is it necessarily the case that $Tg$ is in $L^2(\Bbb R)$? Note here that $T$ is meant as an integral transform, not the extension of $T$ (since that would be true trivially). My first approach was to consider some limit of elements in $X$ which approach $g$ in $L^2$, but I couldn't really piece any more of an argument together since it wasn't obvious to me how to proceed. A partial attempt: Since $X$ is dense in $L^2(\Bbb R)$, there is a a sequence $(f_m)\subseteq X$ such that $f_m\to g$ in $L^2(\Bbb R)$. Since $L^p$ convergence implies pointwise almost everywhere convergence of a subsequence $(f_{m_k})$ to $g$. Moreover, $f_{m_k}\to g$ in $L^2(\Bbb R)$. Since $f_{m_k}(x) \to g(x)$ for almost every $x$, we have that $k(y,x) f_{m_k}(x) \to k(y,x)g(x)$ almost everywhere. Assuming that it can be shown that $$\int_{-\infty}^{\infty} k(y,x) f_{m_k}(x)\,dx\to \int_{-\infty}^{\infty} k(y,x)g(x)\,dx,\tag{1}$$ then since $(f_{m_k})$ is Cauchy and $T$ is bounded, $(Tf_{m_k})$ is Cauchy and thus converges to an element of $L^2(\Bbb R)$. This then says that $$\int_{-\infty}^{\infty} k(y,x) g(x)\,dx = \lim_k \int_{-\infty}^{\infty} k(y,x) f_{m_k}(x)\,dx = \lim_k Tf_{m_k}(y).$$ Since $\lim_k Tf_{m_k}\in L^2(\Bbb R)$, we'd have that $Tg\in L^2(\Bbb R)$ since they agree almost everywhere. This is predicated on $(1)$ being true. $(1)$ can be shown if dominated convergence can be applied, though it isn't clear that the $k(y,\cdot)f_{m_k}$ can be bounded uniformly by an integrable function for a fixed but arbitrary $y$. If $g$ could be approximated within by elements in $X$, then this would work, but that is a very strong condition and will not be the case in general.","['integral-transforms', 'functional-analysis']"
1421572,Dihedral angle Finding,"I meet a wall while some problem solving. The wall is following question. There is a triangle ABC, The vertice A touch bottom plane and the distance from B, C to the bottom : BE = b, CD = c .
When ED= l_1 , DA=l_2 , AE = l_3 , What can I find the dihedral angle between plane ABC and plane AED ? I want to break this wall. 
Please some help to me !","['solid-geometry', 'geometry']"
1421575,Going from composites to individual functions,"$f(g(k(x)))=\sqrt{1+4x^2}$ and $g(k(f(x)))=1+4x$ What is a systematic way to solve for $f$,$g$,and $k$? I never learned anything like this in algebra.","['functional-analysis', 'algebra-precalculus']"
1421595,What's the derivative of a map defined on manifolds?,"I'm going through Warner's book on differentiable manifolds. On page 8 he defines what it means for a map $f: U \subset M \to \mathbb R$ to be differentiable: $f$ is differentiable iff $f \circ \psi$ is differentiable for all charts $\psi$ on $M$. He does not proceed to give a definition of the derivative of $f$. I tried to do a web search but did not find a definition. Is the derivative of $f$ just defined to be the derivative of $f \circ \psi$? What's the definition of the derivative of a map defined on manifolds? Edit At the bottom of page 105 in this book (in the proof of the regular level set theorem) the author calculates the Jacobian of a map $F: M \to N$. This Jacobian contains entries of the form ${\partial F \over \partial x_i}$. So, it seems to me that the derivative, at least partial derivatives exist (although a comment below by Mariano Suarez-Alvarez suggests otherwise)","['differential-topology', 'differential-geometry', 'definition']"
1421598,Primes $p$ for which $2p \pm 1$ are also primes,"Out of curiosity and trouble sleeping, I decided to look at the distribution of primes $p$ for which $2p \pm 1$ are also primes. I looked at the first 25,910,000 primes and counted the number of primes $p$ for which either, neither, both, or only one of the two numbers $2p \pm 1$ is also a prime. Here's the last entry in my table: n: 25,910,000 p: 491,073,763 (the 25,910,000-th prime is $p =$ 491,073,763) 2p-1: false 2p+1: false ($p$ is such that neither $2p-1$ nor $2p+1$ is a prime) # only 2p-1: 1,746,284 (there are 1,746,284 primes $p$ less than or equal to 491,073,763 such that $2p-1$ is also a prime but $2p+1$ is not a prime) # only 2p+1: 1,747,286 (there are 1,747,286 primes $p$ less than or equal to 491,073,763 such that $2p+1$ is also a prime but $2p-1$ is not a prime) # neither: 22,416,428 (there are 22,416,428 primes $p$ less than or equal to 491,073,763 such that neither $2p-1$ nor $2p+1$ is a prime) # either: 3,493,572 (there are 3,493,572 primes $p$ less than or equal to 491,073,763 such that at least one of $2p-1$ and $2p+1$ is a prime) # both: 2 (there are 2 primes $p$ less than or equal to 491,073,763 such that both $2p-1$ and $2p+1$ are primes) The table I generated actually has rows only for every 10,000 primes (and isn't as verbose as the above) but the counting is, of course, done for every prime $p$ such that $2p+1$ is less than or equal to the last prime in the list I have, of the first 50 million primes . I have three questions: Is it known whether 2 and 3 are the only primes $p$ such that both $2p \pm 1$ are primes? If so, is there an elementary reason why they are (or are not) the only such primes? Answered in a comment below Are there any results regarding the fluctuation in the numbers of only-$(2p-1)$ and only-$(2p+1)$? I noticed that for sometimes quite large ranges one number dominates the other, only to be dominated later on for another, often also large, range. Are there results concerning the distribution of primes that differ by  $2m$, for a given value of $m$? It didn't occur to me to tabulate those until just now so I don't have any data to look at at the moment. I know that this question is related to Sophie Germain primes and there's even a related question here already but my questions above are somewhat more specific. Disclaimer : Although I'm reasonably well versed in many areas of math (I was a theoretical physicist once, in a life now in the past), Number Theory is not one of those areas, unfortunately, so please provide answers at the appropriate level.","['prime-numbers', 'number-theory']"
1421612,About $R/I$ Where $I$ is a Prime Ideal,"A well known result in Commutative Algebra says: for a commutative ring $R$ with $1$, $R/I$ is an Integral Domain if and only if $I$ is a Prime Ideal of $R$. Can this result be generalised for non commutative rings?","['ring-theory', 'integral-domain', 'ideals', 'abstract-algebra', 'maximal-and-prime-ideals']"
1421615,Basic composite function,"If $f(x)={1\over x}$ and $g(x)=\sin(x)$ Just checking if I'm understanding this correctly. Are the formulas below correct? $$f(g(x))={1\over \sin(x)}\\
g(f(x))=\sin\left({1\over x}\right)$$ And their domains for both would be: $(x\ne0)$?",['functions']
1421626,"Halmos, Naive Set Theory, recursion theorem proof: why must he do it that way?","Summary: I understand the proof Halmos gives in his Naive Set Theory for the recursion theorem, but I don't understand why he has to do it that way. I give an alternative proof which may be flawed. If so, I want to understand why it's flawed. The context is the following discussion made by Halmos in Section 12 (The Peano Axioms). Induction is often used not only to prove things but also to define things. Suppose, to be specific, that $f$ is a function from a set $X$ into the same set $X$, and suppose that $a$ is an element of $X$. It seems natural to try to define an infinite sequence $\{u(n)\}$ of elements of $X$ (that is, a function $u$ from $\omega$ to $X$) in some such way as this: write $u(0) = a,\ u(1) = f(u(0)),\ u(2) = f(u(1))$, and so on. If the would-be definer were pressed to explain the ""and so on,"" he might lean on induction. What it all means, he might say, is that we define $u(0) = a$, and then, inductively, we define $u(n^+)$ as $f(u(n))$ for every $n$. This may sound plausible, but as justification for an existential assertion, it is insufficient. The principle of mathematical induction does indeed prove, easily, that there can be at most one function satisfying all the stated conditions, but it does not establish the existence of such a function. OK, fair enough. This argument is circular in that it uses the function $u$ in order to define $u$. Halmos then proves a theorem which does establish the existence of such a function: Recursion theorem. If $a$ is an element of a set $X$, and if $f$ is a function from $X$ into $X$, then there exists a function $u$ from $\omega$ into $X$ such that $u(0) = a$ and such that $u(n^+) = f(u(n))$ for all $a \in \omega$. He proves this by considering the class $\mathcal{C}$ of all subsets $A$ of $\omega \times X$ such that $(0,a) \in A$ and for which $(n^+, f(x)) \in A$ whenever $(n,x) \in A$. He notes that the class is not vacuous, because $\omega \times X$ itself satisfies the conditions. He then forms the intersection of all elements of $\mathcal{C}$ and proves that it is a function with the desired properties. I have no problem understanding the proof, but I am trying to understand why it's necessary to do it the way he did. He uses what one might call a ""top-down"" approach, starting with all of $\omega \times X$, and reducing it to a subset which is the desired function. Would not the following ""bottom-up"" approach work? To me it just seems like a more pedantic version of the flawed induction argument, so I'm guessing that this is also wrong. But I don't see what the problem is. Possibly flawed proof: For each $n \in \omega$, define $U_n = \{(n, f^n(a))\} \in \omega \times X$, where $f^n$ denotes the $n$-fold composition of $f$. My convention (and that of Halmos) is that $0 \in \omega$, and by $f^0(a)$ I mean simply $a$. Define $U = \bigcup_{n=0}^{\infty}U_n$. Clearly $U \subseteq \omega \times X$. To prove that $U$ is a function from $\omega$ to $X$, we need to verify that for each $n \in \omega$, there is exactly one element of $U$ with $n$ in the first slot. But this is immediately clear from the construction of $U$. It remains to verify that $U$ satisfies the indicated recursion. Since $U_0 = \{(0, a)\} \subseteq U$, we have $U(0) = a$. Moreover, given any $n \in \omega$, we have $U_n = \{(n, f^n(a)\} \subseteq U$, so $U(n) = f^n(a)$, and $U_{n^+} = U_{n+1} = \{(n+1, f^{n+1}(a)\} \subseteq U$, so $U(n+1) = f^{n+1}(a)$. But the latter is simply $f(f^n(a)) = f(U(n))$, and so $U$ satisfies the recursion. The following question is related but not the same as mine. I understand why the method proposed in that question is flawed due to circular reasoning. Definition by Recursion: why is the existence part not (almost) obvious?",['elementary-set-theory']
1421644,Solving Functional Equations by Limits ...,"Let $f(x)$ be a continuous function and satisfying the equation : $f(2x) - f(x) = x$. Given $f(0)=1$ ; Find $f(3)=?$ My teacher solves this as : $$f(x) - f(x/2) = x/2$$
$$f(x/2) - f(x/4) = x/4$$
. . . $$f(x/2^{n-1})  - f(x/2^{n}) = x/ 2^{n-1}$$ ...........…......
Add them up : And let $n\rightarrow \infty$ $$f(x)  - f(x/2^{n}) =(x/2)/(1-(1/2))$$ Thus $f(x)  - f(0) = x$. Thus $f(x) = x+1$. Thus $f(3)= 4$. However my problem is that I didn't find it intuitive ; I didn't understand how to get such an idea. So is there an alternate way to go about this problem ?","['limits', 'algebra-precalculus', 'functional-equations']"
1421646,"$K, N < G$ and $|G:N|$ and $|K|$ coprime, then $K<N$. Group action argument?","A common exercise in finite group theory is Suppose $G$ is a finite group with $K < G$, $N \lhd G$.
  Suppose that $|K|$ and $|G : N|$ are coprime. Then $K < N$. The normal way to attack this is to note that $|KN:N|$ is coprime to $|K|$, but it is also equal to $\frac{|K|}{|K \cap N|}$, so it divides $K$, so it is $1$. I'm interested to know if this can be turned into a group action argument (e.g. let $K$ act on the cosets of $N$ and do some counting), or if this fact is inherently not group action-y. In general I'm wondering if most non-trivial arguments in finite group theory can be converted to group action arguments without adding much complexity.","['abstract-algebra', 'group-theory', 'finite-groups']"
1421650,Any neat way to calculate this Vandermonde-like determinant?,"Let $x_i,i\in\{1,\cdots,n\}$ be real numbers, and $s_k=x_1^k+\cdots+x_n^k$, I'm asked to calculate
$$
    |S|:=
    \begin{vmatrix}
      s_0 & s_1 & s_2 & \cdots & s_{n-1}\\
      s_1 & s_2 & s_3 & \cdots & s_n\\
      s_2 & s_3 & s_4 & \cdots & s_{n+1}\\
      \vdots & \vdots & \vdots & \ddots & \vdots \\
      s_{n-1} & s_{n} & s_{n+1} & \cdots & s_{2n-2}
    \end{vmatrix}
$$
and to prove that $|S|\ge 0$ for all possible real $x_i$. I found that
$$
|S|=\det[(v_1+\cdots v_n), (x_1v_1+\cdots+x_nv_n),\cdots,(x_1^{n-1}v_1+\cdots+x_n^{n-1}v_n)],\quad\text{where}\, v_j=\begin{bmatrix}
  1 \\ x_j \\ \vdots\\ x_j^{n-1}
\end{bmatrix}
$$
Due to multilinearity of the $\det$ function, I sense it might have something to do with Vandermonde determinant. In fact, it must have the form
$$|S|=(\det[v_1,\cdots, v_n])\cdot \text{something}$$
But that ""something"" involves many cyclic sums and is therefore a horrible mess.. Anyway, is there a neat way to calculate this tricky determinant? Thanks!","['hankel-matrices', 'determinant', 'linear-algebra', 'matrices']"
1421656,a problem involving trigonometry functions,"Let us suppose I am standing on a side of a harbour and I am given that the angle to some point on the other shore is $88$ degrees. I decide to walk $50$ meters along the shore and from this new location I measure my angle to the same point as before. This time the angle from my new position to this point is $84$ degrees in roughly the same direction. What is the distance from my original position to that point ? Try: Here is an sketch of the triangle: Where angle $A$ is $88$, and angle $B$ is $84$. My goal is to find $x$. I am trying to use $\cos 88 = y/x$ and $\tan 84 = DC / (y-50) $. But now I have three variables. am I on the right track ?",['trigonometry']
1421663,"Borel $\sigma$-algebra, definition","This following statement is from a book: Let $C$ denote all open intervals. Since every open set in
  $\mathbb{R}$ is the countable union of open intervals, we have
  $\sigma(C)=$ the Borel $\sigma$-algebra of $\mathbb{R}$. I need help understanding this. First, when it is said that $C$ denotes all open intervals, does it literally mean all imaginable open intervals $(a,b)$ in $\mathbb{R}$? If so, isn't the number of those intervals infinite and uncountable? Second, can you give me a literal example of what is meant by the statement that every open set in $\mathbb{R}$ is the countable union of open intervals? I assume that an ""open set"" differs from an ""open interval"" in $\mathbb{R}$ by possibly having missing points or intervals in the middle? Finally, how does one exactly go from every open set being the countable union of open intervals to $C$ generating the Borel sigma-algebra? This is not explained in the book at all.",['probability-theory']
1421679,"Rigorous definition of ""oriented line"" in an Euclidean affine space","Let $\mathcal{A}^n$ be an affine space of dimension $n$. For example, let's take $n=3$. A line $\mathcal{s}$ of $\mathcal{A}^3$ is an affine subspace of dimension $1$, that is: $\mathcal{s}=\{P \in \mathcal{A}_3 \text{ such that } \overrightarrow{AP} \in \langle u \rangle \}$. Now, what is not clear to me is: if we consider an Euclidean affine space $\mathcal{E}^3$, what is the
  ( rigorous ) definition of an oriented line ?","['euclidean-geometry', 'geometry', 'linear-algebra', 'affine-geometry']"
1421688,Nice applications of estimation theory and hypothesis testing,"As a mathematics professor in an engineeer school, I want to write some lab work for students in Statistics. This work should last four hours and will be made in a language such as Matlab or Python. Therefore, I am looking for nice applications of Statistics, in particular estimation theory, or hypothesis testing (potentially regression). I thought of: restoration of a blurred image with noise (based on the method of maximum likelihood) problems in signal detection. Are you aware of other applications?","['statistical-inference', 'hypothesis-testing', 'applications', 'statistics', 'parameter-estimation']"
1421692,Reference for gradient expression of a function on matrices,"I'm looking for a reference (I suppose the statement is correct) for the following formula: $$
\langle\nabla f(\rho)^\dagger,V\rangle=\left.\frac d{dt} f(\rho+tV)\right|_{t=0}
$$ for any direction $V \in \mathbb{C}^{n \times n}$ . For a differentiable function $f : \mathbb{C}^{n \times n} \to \mathbb{C}$ and a matrix $\rho = (\rho_{ij})_{ij} \in \mathbb{C}^{n \times n}$ we define the gradient of $f$ in $\rho$ as $$
\nabla f(\rho)=\Big(\frac{d}{d\rho_{ij}} f(\rho)\Big)_{ij}.
$$ I used the trace inner product $$
\langle A,B\rangle = \text{tr} \left(A^\dagger B\right).
$$ Please tell me, if I'm wrong or give me a book, that I can cite in my thesis.","['analysis', 'trace', 'matrix-calculus', 'derivatives']"
1421711,"Some ""facts"" on oriented angles in the Euclidean affine space of dimension 2","Let $\mathcal{E}^2$ be an Euclidean affine space of dimension $2$ oriented by $R=(O,B=\{u_1,u_2\}$. Let 
$$\mathcal{r}=\{P \in \mathcal{E}_2 \text{ such that } \overrightarrow{AP} \in \langle u \rangle \}$$ and 
$$\mathcal{s}=\{P \in \mathcal{E}_2 \text{ such that } \overrightarrow{BP} \in \langle v \rangle \}$$
be two lines of $\mathcal{E}_2$. Let $\alpha$ be the angle between r and s and $\beta$ be the angle between $s$ and $r$. Then I’ve been given the following facts (without any proof): $\alpha$ depends on the orientation of $\mathcal{E}_2$ and on the reciprocal orientation of $r$ and $s$ and on the order in which you pick the lines. Furthermore, if you change the orientation of $r$, then, $\alpha’ = \alpha + \pi$ $\alpha + \beta = 2 \pi$ $ \cos \alpha = \cos \beta$ and depends only on the orientation of $\mathcal{E}_2$ and on the reciprocal orientation of the lines $|\cos \alpha |= |\cos \beta|$ depends only on the orientation of $\mathcal{E}_2$. $ \cos \alpha = \cos \beta = \frac{u \cdot v}{\lVert{ u }\rVert \lVert{ v }\rVert  }$, where $u \cdot v$ is the scalar product of $u$ and $v$. My question is: The statements are clear, but surely the proofs don't seem obvious to me; how does one prove them?","['euclidean-geometry', 'geometry', 'linear-algebra', 'affine-geometry']"
1421727,Trigonometry - Double angle,"I have tried to solve this problem, but everything that I try does not work. Please help me solve this equation: $$\cos {6}x + 2 = 5\sin {3}x$$ Thanks :)",['trigonometry']
1421750,Binomial coefficients identity [duplicate],"This question already has answers here : Identity for convolution of central binomial coefficients: $\sum\limits_{k=0}^n \binom{2k}{k}\binom{2(n-k)}{n-k}=2^{2n}$ (4 answers) Closed 8 years ago . Prove algebraically or otherwise: $$\sum \limits_{r=0}^n {2r \choose r} {2n-2r \choose n-r} = 4^n $$ where ${n \choose r}$ denotes the usual binomial coefficient. I think there is a combinatorial proof using path counting arguments, but I haven't been able to find it.","['binomial-coefficients', 'combinatorics']"
1421785,How many routes are there from $A$ to $B$ that cross every node exactly once?,"Imagine an $n \times n$ grid, we start on one corner of the grid in square $A$, and need to reach the opposite corner to square $B$. The rules are, you can only move to an adjacent square, you can't move diagonally and you must pass every square exactly once. How many possible routes are there in an $n \times n$ grid? It is always $0$ if $n$ is even, and I know that the answer is $2$ for $n = 3$. I have a couple of ways of approaching this problem but no general formula.","['graph-theory', 'hamiltonian-path', 'combinatorics']"
1421790,"Verification for a block-determinant evaluation, and some further thoughts","First, I want some verification for the validity of my approach for this det evaluation question: If $A,B\in M_n(K)$, $K$ is a number field (in the sense that $\Bbb Q$ is the smallest possible one), and $AB=BA$, prove this determinat equality
  $$
\begin{vmatrix}
A & -B \\ B & A
\end{vmatrix}
=|A^2+B^2|
$$ My approach: Case 1: when $A=(a_{ij})$ is non-singular Consider
$$
\begin{bmatrix}
  I & O \\ -B & A
\end{bmatrix}
\begin{bmatrix}
  A & -B \\ B & A
\end{bmatrix}=
\begin{bmatrix}
  A & -B \\ O & A^2+B^2
\end{bmatrix}
$$
therefore
$$
\det \begin{bmatrix}
  I & O \\ -B & A
\end{bmatrix}
\det \begin{bmatrix}
  A & -B \\ B & A
\end{bmatrix}=
\det \begin{bmatrix}
  A & -B \\ O & A^2+B^2
\end{bmatrix}
$$
by Laplace expansion, it is clear that
$$
\begin{vmatrix}
A & -B \\ B & A
\end{vmatrix}=\frac1{|A|}\cdot|A|\cdot |A^2+B^2|=|A^2+B^2|
$$ Case 2: when $A$ is singular Consider $\require{cancel} A(t)\xcancel{=(a_{ij}+t)}=A+It$, since $A$ is singular, we have $A(0)=0$. That's to say, $0$ is a root of $A(t)$, which we regard now as a (non-zero) polynomial w.r.t. the parameter $t$. Since any non-zero polynomial has only finitely many roots in $\Bbb C$, it is clear that there exists a positive $\delta$ such that $t\in (-\delta,0)\cup(0,\delta)$ implies $A(t)\ne 0$, otherwise there would be an infinite sequence consisting of $A(t)$'s roots that converges to $0$. So if we pick any $t\in (-\delta,0)\cup(0,\delta)$ and replace $A$ by $A(t)$ in case 1, we'll obtain
$$
\begin{vmatrix}
A(t) & -B \\ B & A(t)
\end{vmatrix}=|A^2(t)+B^2|
$$
However, this is also a real polynomial function and therefore continuous at $t=0$, hence
$$
\begin{vmatrix}
A & -B \\ B & A
\end{vmatrix}=
\lim_{t\to 0}\begin{vmatrix}
A(t) & -B \\ B & A(t)
\end{vmatrix}=
|A^2(0)+B^2|=|A^2+B^2|
$$ Second, if my previous deduction is sound , will it be also proper to extend similar convert-singular-to-non-singular tricks elsewhere (except when the underlying field $K$ is not a number field in the usual sense, say, $K$ is a finite field)? For instance, there is an exercise marked as ""difficult"" in my textbook For $n\times n$ matrices $A,B$, prove
  $$(AB)^*=B^*A^*$$
  $(A^*)$ denotes the adjoint matrix (transpose of the cofactor matrix) of $A$. I know that it is marked as ""difficult"" because of the possibility that $A$ or $B$ can be singular, for if they are both non-singular, the proof will be extremely easy. However, if I apply my previous tricks here -- like making some $A(u),B(v)$ stuff, and noticing that every entry of $(AB)^*$ is continuously dependent on all the entries of $A(u),B(v)$, which are respectively continuously dependent on $u$ and $v$ due to the way I construct $A(u),B(v)$ -- will it not be extremely easy to extend the ""non-singular"" case to the ""singular"" case and thus complete the whole proof? Further yet, if we have successfully proven a matrix/det equality for a bunch of non-singular matrices like $A,B,C\cdots$, and this equality only involves things (like $\det$, but not $\text{rank}$ of course) that are continuously dependent on each entry of these matrices, will it be natural to extend the result to all cases, whether the involved matrices are singular or not? Any rectification/inspiration/clarification on my thoughts is welcomed. Best regards! EIDT @user1551 has pointed out that in my approach if $A(t)\equiv 0$ there'd be a fallacy, and has suggested that it can be fixed if I replace my $A(t)$ by $A+It$, which will in no case be a 0 polynomial. EDIT Or, is there any such matrix equations (both sides only include matrix addition and multiplication but not there inverses)  where invertibility makes all the difference?","['determinant', 'linear-algebra', 'proof-verification', 'matrices']"
1421799,There exists no injective function from the power set of A to A,"It is not so hard to see that there doesn't exist a surjective function from a set $A$ to $\mathcal{P}(A)$, the power set of $A$. Namely, let us suppose there does exist such a function $f:A\rightarrow\mathcal{P}(A)$. Then, consider the set $X=\{x\in A\mid x\not\in f(x)\}$. Since $f$ is surjective, there exists an $a\in A$ such that $f(a)=X$. This however instantly results in a contradiction. Now, one would expect a proof of the statement There doesn't exist an injective function from the power set of a set to the set itself to be provable in a similar way. However, so far I have not been able to alter above proof to prove this statement, how would one do this?",['elementary-set-theory']
1421809,Solve the equation $\sqrt{x+5}=5-x^2$,"Solve the equation $\sqrt{x+5}=5-x^2$. I have tried to make the substitution $x=\sqrt{5}\tan^2 \theta$ and wanted to make use of the identity $\tan^2\theta+1=\sec^2\theta$ but it didn't work out. I also tried to make the substitution $y=x+5$ but it lead to nowhere. Since this was a contest problem, I believe there is a short, elegant and elementary solution, please helps.","['contest-math', 'algebra-precalculus']"
1421821,Can a finite set support a $\sigma$-algebra,"Assume the set $\Omega$ is finite (finite number of elements). $A$ is a collection of subsets of $\Omega$. It is clear that $A$ can be an algebra, but is $A$ then also automatically a $\sigma$-algebra? If not, can it actually ever be a $\sigma$-algebra due to the finite nature of $\Omega$?",['probability-theory']
1421825,Estimating the expectation of a derivative,"Assume $Y$ is a continuously differential function of $X$. Given i.i.d. data $(x_i,y_i)_{i=1}^n$, I would like to estimate $E\left[\left.\frac{\partial Y}{\partial X}\right|_{X=X_0}\right]$. What got me thinking about this problem was estimation of the coefficients in a linear regression using $E\left[\frac{\partial Y}{\partial X}\right]$ (I know it is not the best way to estimate the coefficients, and may even be a bad way to do so). From this question Derivative of a random variable w.r.t. a deterministic variable , I know that $\frac{\partial Y}{\partial X}$ makes sense but I'm trying to understand how to estimate $\frac{\partial Y}{\partial X}$ when there is randomness (without randomness estimation can be done by finite differences, for example). I don't have any good ideas how to estimate $E\left[\left.\frac{\partial Y}{\partial X}\right|_{X=X_0}\right]$ but if I was forced to give a way, I would give weights to points around $X_0$ based on how close they are to $X_0$ and then sample two points at a time based on the weights and first difference the two points and do this many times and take the sample average. References / summary of techniques / specifics are greatly appreciated.","['probability', 'expectation']"
1421839,Find $z$ when $z^4=-i$?,"Consider $z^4=-i$, find $z$. I'd recall the fact that $z^n=r^n(\cos(n\theta)+(i\sin(n\theta))$ $\implies z^4=|z^4|(\cos(4\theta)+(i\sin(4\theta))$ $|z^4|=\sqrt{(-1)^2}=1$ $\implies z^4=(\cos(4\theta)+(i\sin(4\theta))$ $\cos(4\theta)=Re(z^4)=0 \iff \arccos(0)=4\theta =\frac{\pi}{2} \iff \theta=\frac{\pi}{8}$ Since $z^n=r^n\cdot e^{in\theta}$, $z^4$ can now be rewritten as $z^4=e^{i\cdot4\cdot\frac{\pi}{8}} \iff z=e^{i\frac{\pi}{8}}$ However, my answer file says this is wrong. Can anyone give me a hint on how to find $z$?","['complex-analysis', 'complex-numbers']"
1421855,Solving $3x^{4}-7x^{3}+2x^{2}=950$ over the rationals,"I am asked to find only rational solutions. Factoring by $x^{2}$, I get:
$$x^{2}(3x^{2}-7x+2)=950$$
By applying the quadratic formula, I have:
$$x^{2}(3x-1)(x-2)=950$$
I don't know how to proceed from there. Thank you for your help.","['rational-numbers', 'algebra-precalculus']"
1421866,smooth manifolds and preimage of momentum mappings,"Let $G$ be a Lie group acting on itself as $\phi(h)(g)= L_h(g)$ as a left translation. Then we can consider the cotangent lift of this action, namely $\Phi: G \times T^*G \rightarrow T^*G$ 
as $\Phi(h)(g,p) = (hg,(dL_{h^{-1}}(hg))^*p).$ It can now be shown that such a map induces a canonical Hamilton function with moment map on the cotangent bundle $H_{\xi}(g,p) =  J(q,p)(\xi):=(dR_g)^*(e)(p)(\xi)$ for some $\xi \in \mathfrak{g}.$ This is now my motivation for the question: If we consider $J^{-1}(x)$ for $x \in \mathfrak{g}^*$ then this set is given by $$J^{-1}(x) = \{ (g , (dR_{g^{-1}})^*(g)(x));g \in G \}.$$ My question is: Why is it a manifold? (I admit that it looks very much like an application of the regular value theorem, but I don't see why it applies) If there is anything unclear about my question, please let me know.","['lie-groups', 'differential-topology', 'differential-geometry', 'lie-algebras', 'symplectic-geometry']"
1421873,A few intro questions about limits,"I'm a first year university student and it is my first time posting on the forum so if I have posted incorrectly please let me know and I'll keep it in mind for next time! Although I've already finished my first calculus class, the course moved quite quickly and so I've been revising to clear up a few points I think could use some strengthening. While going over some $\delta-\varepsilon$ limit exercises, I realized there are few things about the $\delta-\varepsilon$ definition that either I don't understand, or would just like some confirmation about. Firstly, why is the definition not an equivalence statement? For any $\varepsilon$ value considered wouldn't there be a corresponding $\delta$ range for $x$? Also why is statement $0 < |x-a|< \delta$ , with $<$ rather than $\le$? With the $0<|x-a|$, is this simply to increase the strength of the statement by not requiring that the definition hold for the limit at $x=a$? But why then have $|x-a|< \delta$? Is there some issue with allowing $|x-a| \le \delta$? I realized this was a point I really didn't understand while reviewing the definition we were given for limits as $x \to \infty$. Here we were told that this has the limit $l$ when there exists an $N$ and $\varepsilon$ such that: $x \ge N \implies |f(x)-l|< \varepsilon$. So here the statement allows for a $\le$, while the finite statement doesn't, which seemed a little curious.","['limits', 'epsilon-delta']"
1421919,"Bijection between $\Bigl\{1, 2, \dots, \frac{N(N+1)}{2}\Bigr\}$ and $\{ (i, j) \in \mathbb{N} : i \le j \le N\}$","Let $N$ be some positive integer and $A$ be the following set $\{ (i, j) \in \mathbb{N}^2 : 1 \le i \le j \le N\} = \{ (1, 1), (1, 2), \ldots, (1, N), (2, 2), (2, 3), \ldots, (2, N), \ldots, (N, N) \}$ I know that $A$ has $\frac{N(N+1)}{2}$ elements. Also, let $B$ be the set $\Bigl\{1, 2, 3, \ldots, \frac{N(N+1)}{2} \Bigr\}$. The function $f(i, j) = j + i \cdot N - \frac{i(i+1)}{2} $ is an injective function from $A$ to $B$ but I'm trying to find a injection from $B$ to $A$ and I am not having success. I need this function to optimize a for loop  in a program that only needs to care about the upper triangular part of a matrix (if you pay attention, you will see that $A$ has the indexes of a $N \times N$ upper triangular matrix...). Does someone have any idea? Thank you very much!","['elementary-set-theory', 'combinatorics', 'functions']"
1421920,How to calculate the limit that seems very complex..,"Someone gives me a limit about trigonometric function and combinatorial numbers. $I=\displaystyle \lim_{n\to\infty}\left(\frac{\sin\frac{1}{n^2}+\binom{n}{1}\sin\frac{2}{n^2}+\binom{n}{2}\sin\frac{3}{n^2}\cdots\binom{n}{n}\sin\frac{n+1}{n^2}}{\cos\frac{1}{n^2}+\binom{n}{1}\cos\frac{2}{n^2}+\binom{n}{2}\cos\frac{3}{n^2}\cdots\binom{n}{n}\cos\frac{n+1}{n^2}}+1\right)^n$ when $n$ is big enough, $\displaystyle 0\leqslant\frac{n+1}{n^2}\leqslant \frac\pi 2$ I tried $\displaystyle \frac{x}{1+x}\leqslant\sin x\leqslant x$ Use $\sin x\leqslant x$, I got $I\leqslant e$, But use another inequality I got nothing. I don't know the answer is $e$ or not. Who can help me. Thanks.","['calculus', 'limits', 'binomial-coefficients']"
1421934,Set $E\subset \mathbb{R}^n$ of positive Lebesgue measure such that the Lebesgue measure of its boundary is zero,"Let $E\subset \mathbb{R}^n$ have positive Lebesgue measure. What are easily interpretable sufficient conditions on $E$ to guarantee that the difference between the closure $\bar{E}$ and the interior $\operatorname{Int}(E)$ has zero Lebesgue measure? In particular, I am interested in the following situation $-$ would the above mentioned condition be satisfied if $E$ is compact and convex?","['lebesgue-measure', 'measure-theory']"
1421947,Derivative of integral in interval,"Let $$F(x)=\int_{2}^{x^3}\frac{dt}{\ln t}$$ and $x$ is in $(2,3)$. Find $F'(x)$. Can somebody give me idea how to do this? Thank you","['calculus', 'derivatives']"
1421974,Can there be a Diaconis-Shahshahani Upper Bound Lemma for Compact Groups?,"Let $G$ be a finite group and $\nu\in M_p(G)\subset \mathbb{C} G$ a probability measure on $G$ and let $\pi$ be the uniform distribution on $G$ . Denote by $d_\rho$ the dimension of a non-trivial irreducible unitary representation $\rho:G\rightarrow \operatorname{GL}(V_\rho)$ and define a map $\mathbb{C}G\rightarrow \operatorname{GL}(V_\rho)$ by: $$\nu\mapsto \widehat{\nu}(\rho)=\sum_{t\in G}\nu(\delta_t)\rho(t).$$ Denote by $\operatorname{I}^*(G)$ a family of non-trivial, pairwise inequivalent irreducible unitary representations. Denote by $\|\cdot\|_{\text{TV}}$ the total variation norm on $\mathbb{C} G$ : $$\|\mu\|_{\text{TV}}=\frac12 \|\mu\|_1=\frac12 \sum_{t\in G}|\mu(\delta_t)|.$$ The Upper Bound Lemma of Diaconis & Shahshahani states that: $$\|\nu-\pi\|_{\text{TV}}^2\leq \frac14 \sum_{\rho\in\operatorname{I}^*(G)}d_\rho \operatorname{Tr}\left[\widehat{\nu}(\rho)^*\widehat{\nu}(\rho)\right].$$ This can be used to analyse the rate of convergence for random walks on finite groups. For example see my own MSc thesis . Where $h$ is the Haar measure, can a similar formula for $\|\nu-h\|$ hold for compact groups? If yes, are you aware of a reference? If no, what are the barriers? Similar here means a formula that uses a $\sum_{\operatorname{I}^*(G)}$ . I expect that the problem is with the norm $\|\cdot\|_{\text{TV}}$ . Note that one of the benefits of using the total variation norm is that lower bounds are also available via: $$\|\mu\|_{\text{TV}}\geq \frac12 |\mu(\phi)|,$$ for a test function $\phi\in F(G)$ such that $\|\phi\|_{\infty}\leq 1$ . In the 2-norm: $$\|\mu\|_2=\sqrt{\sum_{t\in G}|\mu(\delta_t)|^2},$$ the upper bound lemma is actually an equation: $$\|\nu-\pi\|_2=\sqrt{\sum_{\rho\in\operatorname{I}^*(G)}d_\rho \operatorname{Tr}\left[\widehat{\nu}(\rho)^*\widehat{\nu}(\rho)\right]}.$$ A Partial Answer: There is a paper of Rosenthal (a student of Diaconis) where he states that The previously mentioned finite-group methods appear to be applicable to compact groups. The Diaconis-Shahshahani Upper Bound Lemma is one of these previously mentioned methods. Revised Question: Therefore the question is revised but is more difficult and also much softer in its scope: What are major barriers to applying the Diaconis-Shahshahani Upper
Bound Lemma to a random walk on a compact group?","['representation-theory', 'markov-chains', 'group-theory', 'reference-request', 'topological-groups']"
1422006,Determinant map is homomorphism and surjective.,"I just came from a course of abstract algebra, and my teacher told us that the determinant map $\det : GL(n, \mathbb{R}) \to \mathbb{R}^\times$ is a surjective homomorphism. Here, $GL(n, \mathbb{R}) = $ the set of $(n \times n)$ matrices $M$ such that $\det(M) \neq 0$ Why is $\det$ surjective?",['abstract-algebra']
1422012,How is Buchberger algorithm a generalization of the Euclid GCD algorithm?,"It is said in many places (for example, on the Wikipedia article for Buchberger's algorithm )
that Buchberger's algorithm to find Groebner basis is a generalization of Euclid's GCD algorithm. This is not obvious to me. Just think about two polynomials 
$f(x)=a_n x^n + \cdots a_0$, and $g(x)=b_m x^m + \cdots b_0$, with
$n>m$. Start by finding the subtraction polynomial $S(f,g)$, here
I do not see a clue of Euclid's algoritm and what does it have to do
with Euclid's GCD algorithm. Please do not respond that both Euclid and Buchberger produce the same result. I already know that. I am questioning about how one algorithm (Buchberger's) reduce to the other (Euclid's) for univariate polynomials. Any clue? Thanks.","['abstract-algebra', 'polynomials', 'groebner-basis']"
1422045,Matrix multiplication of columns times rows instead of rows times columns,"In ordinary matrix multiplication $AB$ where we multiply each column $b_{i}$ by $A$, each resulting column of $AB$ can be viewed as a linear combination of $A$. If however if we decided to multiply each column of $A$ by each row of $B$, we  get an entire matrix for each column-row multiply. My question is: Does each matrix resulting from an outer product have any known meaning aside from being a part of the sum(a summand?) of the final $AB$? Edit: Say we have $AB$ $$ \left( \begin{array}{ccc}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9 \end{array} \right) \left( \begin{array}{ccc}
10 & 11 & 12 \\
13 & 14 & 15 \\
15 & 16 & 17 \end{array} \right) $$ Normally we would multiply each column of $B$ by A and get a linear combination of A , e.g.
$$10\left( \begin{array}{c}
1 \\ 4\\ 7 \end{array} \right)+ 13\left( \begin{array}{c}
2 \\ 5\\ 8 \end{array} \right)+ 15\left( \begin{array}{c}
3 \\ 6\\ 9 \end{array} \right)$$ which is one column of $AB$. If however we multiply each column of $A$ by each row of $B$, e.g.
$$\left( \begin{array}{c}
1 \\ 4\\ 7 \end{array} \right)\left( \begin{array}{ccc}
10 & 11 & 12 \end{array} \right)$$ we get a matrix. Each of the 3 matrices $a_{i}b_{i}^{T}$ summed together gives us $AB$. I was wondering if each individual matrix that sums to $AB$ has any sort of special meaning. This second way of performing multiplication also seems to be called column-row expansion. ( http://www.math.nyu.edu/~neylon/linalgfall04/project1/dj/crexpansion.htm ). I actually read about it in I believe section 2.4 of Strang's Introduction to Linear Algebra book. He mentions that not everybody is aware that matrix multiplication can be performed in this way.","['linear-algebra', 'matrices']"
1422050,Showing that a set union is the smallest set,Show that $A\cup B$ is the smallest set containing both $A$ and $B$ in the sense that it is contained in every such set. I am not sure how to show that $A\cup B$ is the smallest set. It seems very trivial to me. I realize that if $A\cup B \subseteq$ of every set containing $A$ and $B$ then it is indeed the smallest. How can that be done? I start my attempt at the proof by: Let $S$ be a set containing $A$ and $B$. Then I don't now where to go,['elementary-set-theory']
1422061,"Based space, commuting in diagram up to homotopy, dual Barratt-Puppe sequence.","For a based map $f : X \to Y$, define the ""homotopy fiber"" $Ff$ to be$$Fd = X \times_f PY = \{(x, \chi) : f(x) = \chi(1)\} \subset X \times PY.$$Equivalently, $Ff$ is the pullback displayed in the diagram where $\pi(x, \chi) = x$. As a pullback of a fibration, $\pi$ is a fibration. If $\rho: Nf \to f$ is defined by $\rho(x, \chi) = \chi(0)$, then $f = \rho \circ \nu$, where $\nu(x) = (x, c_{f(x)})$, and $Ff$ is the fiber $\rho^{-1}(*)$. Thus the homotopy fiber $Ff$ is constructed by first replacing $f$ by the fibration $\rho$ and then taking the actual fiber. Let $\iota: \Omega Y \to Ff$ be the inclusion specified by $\iota(\chi) = (*, \chi)$. The sequence$$\dots \to \Omega^2 X \overset{\Omega^2 f}{\longrightarrow} \Omega^2Y \overset{-\Omega\iota}{\longrightarrow} \Omega Ff \overset{-\Omega\pi}{\longrightarrow} \Omega X \overset{-\Omega f}{\longrightarrow} \Omega Y \overset{\iota}{\to} Ff \overset{\pi}{\to} X \overset{f}{\to} Y$$is called the fiber sequence generated by the map $f$; here$$(-\Omega f)(\zeta)(t) = (f \circ \zeta)(1 - t) \text{ for }\zeta \in \Omega X.$$These ""long exact sequences of based spaces"" also give rise to long exact sequences of pointed sets, covariantly. Theorem. For any based space $Z$, the induced sequence$$\dots \to [Z, \Omega F f] \to [Z,\Omega X] \to [Z, \Omega Y] \to [Z, Ff] \to [Z, X] \to [Z, Y]$$is an exact sequence of pointed sets, or of groups to the left of $[Z, \Omega Y]$, or of Abelian groups to the left of $[Z, \Omega^2Y]$. Exactness is clear at the first stage. To see this, consider the diagram Here $h: c_* \simeq f \circ g$, and we view $h$ as a map $Z \to PY$. Thus we check exactness by using any given homotopy to lift $g$ to the fiber. I claim that, up to homotopy equivalence, each consecutive pair of maps in my fiber sequence is the composite of a map and the projection from its fiber onto its source. This will imply the source. I observe that, that for any map $f$, interchange of coordinates gives a homeomorphism$$\Omega F f \cong F(\Omega f)$$such that the following diagram commutes: Here $\tau$ is obtained by interchanging the loop coordinates and is homotopic to − \text{id}. We have $\iota(f)$, $\pi(f)$, etc., to indicate the maps to which the generic constructions $\iota$ and $\pi$ are applied. Using this inductively, we see that we need only verify our claim for the two pairs of maps $(\iota(f), \pi(f))$ and $(-\Omega f, \iota(f))$. Anyways, one key step in finishing the proof: I need that the right triangle commutes and the left triangle commutes up to homotopy in the following diagram. My question is, what is the easiest way to see that the two triangles commute up to homotopy? Thanks in advance.","['abstract-algebra', 'general-topology', 'algebraic-topology', 'homotopy-theory', 'category-theory']"
1422068,"""Every regular function on an affine variety is polynomial"" - generalisation to the case of a reducible variety","$K$ is an algebraically-closed field, and for an affine variety $X$, $A(X) $ denotes the ring of polynomial functions on $X$. What I would like to prove is the following: ""Let $X \subset \Bbb{A}^n(K)   $ be Zariski-closed, and $ f: X \rightarrow K   $ be regular; that is, for each $x \in X$ there's a neighbourhood $U_x $ containing $x$ and $g,h \in A(X)$, with $h$ nonzero on $U_x$ and $f = g/h$ on $U_x$. Then, $f \in A(X) $."" I've seen and understood a proof in the case where $X$ is irreducible, and I've also seen this thread - Does a regular function on an affine variety lie in the coordinate ring?(Lemma 2.1, Joe Harris) - where I agree with them that the proof in Harris fails at the penultimate line; but I didn't understand what was said in the answer (or indeed most of what was said in the question). What I'd like is if someone could give me a proof, or a link to a proof, of the general theorem; or could explain the answer in that other thread. I've looked around the web for such a proof, but just can't find one. I'm halfway through a first course in algebraic geometry, so don't know about schemes, sheaves etc. but am fine with localisation and other undergrad (basic level grad?) commutative algebra. Thanks so much in advance!",['algebraic-geometry']
1422070,"Every $f\in\omega^\omega$ is bounded by the ""increasing enumeration"" of the intersection of a countable dense set and a dense open set in $\mathbb{R}$","I am studying the theorem 2.2.6 of ""On the structure of the real line"" of book  Bartosznky-Judah. In the proof of theorem 2.2.6 the  part $(4) \to (5)$ $(4)$ for every family of dense open subsets of $\mathbb{R}$, $\{D_\alpha: \alpha < \kappa \}$ and a countable dense $X\subseteq \mathbb{R}$, there exists a countable dense subset $Y \subseteq X$ such that $|Y\setminus D_\alpha|<\aleph_o$ for $\alpha<\kappa$. $(5)$ $\mathfrak{b}>\kappa$ Fix a countable dense subset $X=\{q_n:n \in \omega \}$ of $\mathbb{R}$.
For an subset $Y\subseteq X$ let $f_Y \in \omega^\omega$ be the increasing function such that $Y=\{q_{f_{Y}(n)}:n \in \omega\}$. I do not know why: For every function $f\in \omega$ prove that there exists an open dense $D\subseteq \mathbb{R}$ such that $f(n) \leq f_Y(n)$ for all but finitely many $n$, where $Y=D \cap X$. Any contribution. I would be very grateful.","['set-theory', 'real-analysis', 'general-topology']"
1422075,How many different arrangements are there problem,"How many different arrangements are there of all the nine letters A, A, A, B, B, B, C, C, C in a row if no two of the same letters are adjacent? First I tried to find how many ways to arrange so at least two similar letters are adjacent (the complementary) then subtract from total ways without restriction. I tried to do this via extended addition rule (i.e. with the three circle venn diagram) and now I'm confused how to calculate each case. My attempt so far: 
let a be set of 'two A's adjacent to each other'
let b be set of 'two B's adjacent to each other'
let c be set of 'two C's adjacent to each other' I need to find |complement of a U b U c| (Let Z be universal set, no restriction)
= |Z| - |a| - |b| - |c| + |ab| + |bc| + |ca| - |abc| I know |a| = |b| = |c| and |ab| = |bc| = |ca| 
therefore |complement of a U b U c| = |Z| - 3|a| + 3|ab| -|abc|. |Z| = 9!/3!3!3!, but I'm not sure how to compute |a| or |ab| or |abc|",['combinatorics']
1422077,Expected value and variance of ratio of two sums of two sets of random variables,"Let $X_1,X_2,\ldots,X_n$ be iid $\operatorname{Gamma}(\alpha,\beta)$ random variables. Suppose that, conditionally on $X_1,X_2,\ldots,X_n$, the random variables $Y_1,Y_2,\ldots,Y_n$ are independent and $Y_{i}\mid X_{i}\sim \operatorname{Gamma}(\alpha,\beta X_i)$. Show that $$E\left(\frac{\bar{Y}}{\bar{X}}\right)=\alpha\beta$$ and $$\operatorname{Var}\left(\frac{\bar{Y}}{\bar{X}}\right)=\alpha\beta^2E\left(\frac{\sum X_i^2}{(\sum X_i)^2}\right).$$ It is a question from a past comprehensive exam. The hint says ""use the iterated expectation and variance formulas"". But I do not see anywhere can use this hint.","['probability', 'statistics', 'statistical-inference']"
1422101,"Calculating in closed form $\int_0^{\pi/2} \arctan\left(\sin ^3(x)\right) \, dx \ ?$","It's not hard to see that for powers like $1,2$, we have a nice closed form. What can be said about the cubic version, that is $$\int_0^{\pi/2} \arctan\left(\sin ^3(x)\right) \, dx \ ?$$ What are your ideas on it? Differentiation under the integral sign? Other ways? Mathematica 9 says that $$\int_0^{\pi/2} \arctan\left(\sin ^3(x)\right) \, dx=\frac{2}{3} \, _5F_4\left(\frac{1}{2},\frac{2}{3},1,1,\frac{4}{3};\frac{5}{6},\frac{7}{6},\frac{3}{2},\frac{3}{2};-1\right).$$","['calculus', 'real-analysis', 'definite-integrals', 'integration', 'polylogarithm']"
1422117,$1/|x|^\alpha$ is integrable on the unit ball in $\mathbb R^n$ iff $\alpha < n$ [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question I've always remember the statement that $1/|x|^\alpha$ is integrable on the unit ball in $\mathbb R^n$ if and only if $\alpha < n$ . But I don't know how to prove it. Can anyone show me how to prove it or link a proof of it. Any help is appreciated.","['multivariable-calculus', 'lebesgue-integral', 'integration']"
1422121,What is number of group homomorphisms from $D_{12}$ to $D_{18}$?,"I am willing to find out the number of group homomorphisms from $D_{12}$ to $D_{18}$ where $D_m:=\langle r_m, f_m: r_m^m=f_m^2=(r_mf_m)^2=e_m \rangle$ is the standrard dihedral group of order $2m$. Basically I am motivated from this paper . I have studied and understood the first two cases. But for the third case, I am not getting any clue. SO this is why I have started numerical example. Let $\rho:D_{12}\rightarrow D_{18}$ be a group homomoprhism. So we can say that $\#$Hom$(D_{12}, D_{18})=$ trivial + non-trivial group homomoprhisms. Here non-trivial group homomorphisms = number of possible choice for $\rho(f_m)\times$ number of possible choices for $\rho(r_m)$. here according to rule it is supposed to be $4+4\times 18+18\times (12,18)$. BUt I want to understand the answer step by step. So I started from the begining. Here $\rho(r_{12})$ has the possibilities $e_{18}, r_{18}^\alpha$ with $1\leq \alpha\leq 17$. BUt I dont understand why in the paper it is said another possibilities $r_{18}^\beta f_{18}$. How to count the homomorphisms ? Please enlighten me. thanks in advance My Attempt: 
Hom$(D_{12}, D_{18})=Hom(D_{12}/[D_{12}, D_{12}], D_{18})=Hom(\mathbb Z_2, D_{18})$. And we know that $\#Hom(\mathbb Z_n, G)=\#\{x\in G: x^n = e_G\}$. SO if we use this theorem, we then can conclude that required number of nothing but $\#\{x\in D_{18}: x^2=e\}$. Is it correct ? Not sure if this will give me correct answer according to the formula provided in the paper stated above","['abstract-algebra', 'group-theory']"
1422142,Why $ K \cap H $ is a maximal subgroup of $H $?,"Suppose $ G $ is a finite group and $ H \unlhd G $. Suppose that $ H \cap M $ is either $ H $ or a maximal subgroup of $ H $ for any maximal subgroup $ M $ of $ G $. Let $ N $ be a minimal normal subgroup of $ G $.  Suppose $ M/N $ is a maximal subgroup of $ G/N $. Then $ M/N \cap HN/N = ( M \cap H )N/N $ equal $ HN/N $ or a maximal subgroup of $ HN/N $. Suppose $ K/N $ is a maximal subgroup of $ HN/N $, then $ K/N = K/N \cap HN/N = (K \cap H)N/N $. Why $ K \cap H $ is a maximal subgroup of $H $?","['abstract-algebra', 'group-theory', 'maximal-subgroup', 'finite-groups']"
1422175,How do i calculate the probability of the relay in the circuits?,"I am trying to solve my following probability question but I can't see how to make any progress. Any help will be highly appreciated Question: The probability of the closing of the $i$-th relay in the circuits shown is given by $p_i$ for $i = 1,2,3,4,5$. If all relays function independently, what is the probability that a current flows between $A$ and $B$ for the respective circuits?","['probability', 'probability-distributions']"
1422177,Understanding part of the proof of Spectral Theorem for symmetric matrices,"I'm reading a textbook where the Spectral Theorem for symmetric matrices is proven. I understand almost everything about the proof except for one thing. The theorem is stated as follows: Theorem: Let $A \in \mathbb{R}^{n \times n}$. Then $A$ is orthogonally diagonalizable if and only if $A$ is symmetric. The first implication is easy. The converse is proven by induction by the author. Here is part of the proof: We want to prove that for any symmetric matrix $A$, there is an
  orthogonal matrix $P$ and a diagonal matrix $D$ such that $P^T AP =
D$. We prove this by induction. Any $1 \times 1$ symmetric matrix is
  already diagonal, so we can take $P = I$ and the basic step is proven. Now assume the theorem holds for $(n -1) \times (n-1)$ symmetric
  matrices, with $n \geq 2$. Then we now prove it also holds for $n$. So
  let $A$ be an $ n \times n$ symmetric matrix. We know that $A$ has
  only real eigenvalues (he concludes this on the basis of a preceding
  theorem). Let $\lambda_1$ be any eigenvalue of $A$, and let $v_1$ be
  the corresponding eigenvector which satisfies $||v_1 || = 1 $. Then we
  can extend the set $\left\{v_1 \right\}$ to a basis $\left\{ v_1, x_1,
x_2, \ldots, x_n \right\}$ of $\mathbb{R}^n$. We can then use the
  Gram-Schmidt process to transform into an orthonormal basis $B =
\left\{v_1, v_2, \ldots, v_n \right\}$ of $\mathbb{R}^n$. Let $P$ be the matrix whose columns are the vectors in $B$, with the
  first column being $v_1$. Then $P$ is orthogonal because its column
  vectors are all orthonormal. Now $P^T A P = P^{-1} AP$ represents the
  linear transformation $T: x \mapsto Ax $ in the basis $B$. But we know
  that the first column of $P^T AP$ will be the coordinate vector of
  $T(v_1)$ with respect to the basis $B$. Now, $T(v_1) = Av_1 =
\lambda_1 v_1$, so this coordinate vector is \begin{align*}
\begin{pmatrix} \lambda_1 \\ 0 \\ \vdots \\ 0 \end{pmatrix}.
\end{align*} It follows that... He then shows $P^T A P$ is diagonal by making use of induction hypothesis on a smaller block matrix. But here is what I don't understand. He says $P^T A P$ represents the linear transformation $T: x \mapsto Ax$. What does he mean here? Does he mean the linear transformation $L_A : \mathbb{R}^n \rightarrow \mathbb{R}^n$ ? This doesn't seem right to me, since the matrixrepresentation of $L_A$ is just $A$. Also, what he says after that doesn't really make sense to me, i.e. that the first column $P^T A P$ is the coordinate vector $T(v_1)$ with respect to $B$. Maybe someone can clarify this, or provide an example?","['orthogonality', 'linear-algebra', 'diagonalization']"
1422179,"Does this conjecture about prime numbers exist? If $n$ is a prime, then there is exist at least one prime between $n^2$ and $n^2+n$.","I made an observation on prime numbers, want to check if any conjecture already exist or not? I am a computer programmer by profession and I am interested in number theory. As like many others I am intrigued by prime numbers. Based on my observation, I found following to be true If $n$ is a prime, then there is exist at least one prime between $n^2$
  and $n^2+n$ I am not sure if this conjecture already exist? I tried searching in the internet but did not find any exact conjecture. I would like to know, first of all is my above statement is correct? if not, can any provide me with a counter example where it fails. If this statement is correct, does this conjecture already proposed by someone?","['conjectures', 'prime-numbers', 'number-theory']"
1422183,Is a locally countable family of open subsets of an separable space countable?,"Definition A topological space $(X, \mathfrak{T} )$ is said to be an separable space if $X$ contains a contable subset $D \subseteq X$ such that $D$ is dense in $X$ Definition Let $(X, \mathfrak{T} )$ be a topological space and $\mathfrak{F}$ be a family of open subsets of $X$. The family $\mathfrak{F}$ is said to be locally countable if it satisfies the following property: for every $x \in X$, there exists an open subset $x \in U(x) \subseteq X$ such that there are only countably many members in $\mathfrak{T}$ which intersect $U(x)$. Question Let $(X,\mathfrak{T})$ be an separable space and $\mathfrak{F}$ be a locally countable family of open subsets of $X$. Is $\mathfrak{F}$ a countable family? I want to prove that $\mathfrak{F}$ is a countable family. My Approach For convenience, we may assume that $\emptyset \notin \mathfrak{F}$. Because $(X,\mathfrak{T})$ is an analytic space, $X$ contains a countable subsets $D = \lbrace x_n \ | \ n \in \mathbb{N} \rbrace$ in $X$ such that $D \subseteq X \subseteq \overline{D}$. Therefore, for every $U \in \mathfrak{F}$, there is a smallest $n \in \mathbb{N}$ such that $x_n \in U$. Because $\mathfrak{F}$ is a locally countable family, there are only countable many members $V \in \mathfrak{F}$ containing $x_n$. In other words, we may separate $\mathfrak{F}$ to be a disjoint union of the form $\mathfrak{F} = \bigsqcup_{n \in \mathbb{N}} \mathfrak{F}_n$, where $\mathfrak{F}_n$ is the family of all $U \in \mathfrak{F}$ so that $n$ is the smallest positive integer such that $x_n \in U$. By the preceding arguments, because each $\mathfrak{F}_n$ is a countable family, there is an injective map $\iota_n : \mathfrak{F}_n \rightarrow \mathbb{N}$. We define $\iota : \mathfrak{F} \rightarrow \mathbb{N} \times \mathbb{N}$ to be the map $U \mapsto (n,\iota_n(U))$ whenever $U \in \mathfrak{F}_n$. Obviously, the map $\iota$ is injective. Because $\mathbb{N} \times \mathbb{N}$ is countable, it shows that $\mathfrak{F}$ is also countable. Is my proof correct? Thank you!",['general-topology']
1422185,My proof ɛ-δ Definition of a limit only disproves the limit... What have I done wrong?,"Whilst attempting to prove that
$$\lim_{x\to8} \sqrt[3]{(3x+3)} = 3 $$ 
I came up with the following as my proof:
$$\lim_{x\to8} \sqrt[3]{(3x+3)} = 3 ⇔ (∀ε>0)(∃δ>0)(∀x)|x-8|<δ⇒|\sqrt[3]{(3x+3)}-\sqrt[3]{(27)}|≤ε⇒$$
$$|\sqrt[3]{(3x-24)}|≤ε⇒|\sqrt[3]{(3(x-8))}|≤ε∴|\sqrt[3]{(3δ)}|≤ε□$$
But that's when I realized that even though 
$$If δ=9, \;then \;ε≥3∵(f(x_{0}-δ)≤y_{0}-ε)$$
that
$$If δ=9, \;then \;ε≥3※∵¬(f(x_{0}+δ)≤y_{0}+ε)$$
and thus my proof falls apart. Where did I make a mistake? (or maybe I am misunderstanding, and my proof is correct?) Thank you in advance! Note: I am not 100% certain that I used all of these mathematical symbols correctly... EDIT: Thanks to the comments that the wonderful people of math.stackexchange.com have provided below, I have concluded that
$$|\sqrt[3]{a}-\sqrt[3]{b}| ≠ |\sqrt[3]{(a-b)}|$$
Even so, the expression could be true, but they are not always equal. Therefore, I must re-evaluate this problem, and I will update this question if I find another (correct) solution. If anyone knows how to find the correct proof, please let me know what it is! Thank you! SECOND EDIT: I have refined my proof to this:
$$\lim_{x\to8} \sqrt[3]{(3x+3)} = 3 ⇔ (∀ε>0)(∃δ>0)(∀x)|x-8|<δ⇒|\sqrt[3]{(3x+3)}-\sqrt[3]{(27)}|≤ε⇒$$
$$|\sqrt[3]{(3x-24)}-\sqrt[3]{27}+\sqrt[3]{27}|≤ε⇒|\sqrt[3]{(3x-24)}|≤ε⇒$$
$$|\sqrt[3]{(3(x-8))}|≤ε∴|\sqrt[3]{(3δ)}|≤ε□$$
Yet I come out to the same answer. Unless 
$$\sqrt[3]{(3x+3)}-\sqrt[3]{27} ≠ \sqrt[3]{3x+3}-\sqrt[3]{27}+\sqrt[3]{27}$$
I have no idea what's still wrong about it! Again, thank you for helping! THIRD EDIT: I figured it out! The attempt above has the same problem as the first, I just made the mistake in a different way. Here's the real answer, which actually works.
$$\lim_{x\to8} \sqrt[3]{(3x+3)} = 3 ⇔ (∀ε>0)(∃δ>0)(∀x)|x-8|<δ⇒|\sqrt[3]{(3x+3)}-3|≤ε⇒$$
$$|\sqrt[3]{3x-24+27}-3|≤ε⇒|\sqrt[3]{3(x-8)+27}-3|≤ε∴\sqrt[3]{3δ+27}-3≤ε□$$
And that's it! It works! Thank you!","['calculus', 'limits', 'epsilon-delta']"
