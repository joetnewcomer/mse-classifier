question_id,title,body,tags
3916843,fourth moments of truncated unit-variance variables are summable,"In an article I found the following: If $X$ is a r.v. with zero mean and finite variance, then $$ \sum_N \frac 1 {N^2} \mathbb E\left[ |X|^4 \mathbf 1_{|X|<\sqrt N} \right]<+\infty  $$ and I am struggling to understand how to prove it.
I tried to do the classical estimation, that is $$
\mathbb E\left[ |X|^4 \mathbf 1_{|X|<\sqrt N} \right] \le N \mathbb E\left[ |X|^2 \mathbf 1_{|X|<\sqrt N} \right] \le N
$$ but it is not enough. I guess I can get $o(N)$ , but that is still not enough. I also tried to come up with some counterexample, but for example a continuous distribution with a density with tail $O(x^{-k})$ needs $k>3$ to have finite variance, that coincides with the condition to get summability. And  if $X$ has a distribution with compact support, then all the moments are bounded by a same constant, so the summability follows.","['expected-value', 'distribution-tails', 'probability', 'random-variables']"
3916923,Can all linear operators be represented as matrices? (In infinite dimension.),"Old title : Components of unbounded endomorphism wrt Schauder basis? Changed for discoverarbility. In finite-dimensional linear algebra, given an endomorphism $T$ and a basis $\boldsymbol{e}_i$ , we can easily find a matrix representation of $T$ : $$
 T(v)
 = T(\sum_i v^i \, \boldsymbol{e}_i)
 \overset{*}{=} \sum_i T(v^i \, \boldsymbol{e}_i)
 = \sum_i v^i T(\boldsymbol{e}_i)
 =: \sum_{i,j} v^i \, T_{ij} \, \boldsymbol{e}_j
$$ Let us now assume that $X$ is a separable Banach space with a Schauder basis $\{\boldsymbol{e_i}\}_{i \in \mathbb{N}}$ and $T: X \to X$ is an unbounded linear operator. If we try to replicate the previous equation, the sum will become infinite, and since for unbounded operators $T(\lim_n v_n) = \lim_n T(v_n)$ doesn't hold, the equality $(\overset{*}{=})$ above doesn't hold. This would seem to indicate that unbounded operators don't have a coordinate representation in a Schauder basis, at least not in the same sense as bounded operators do. Or is there a simple generalization that I just don't see? If we now consider $X$ to be a separable Hilbert space and $\boldsymbol{e}_i$ an orthonormal basis, we know that it's isomorphic to $\ell_2$ with basis: $$
  \begin{pmatrix}1\\0\\0\\\vdots\end{pmatrix},
  \begin{pmatrix}0\\1\\0\\\vdots\end{pmatrix},
  \begin{pmatrix}0\\0\\1\\\vdots\end{pmatrix},
  \;\dots
$$ This means that either the endomorphisms on the space of „infinite column vectors“ aren't always „infinite matrices“, or that my previous conclusion about the „non-representability“ of unbounded operators was wrong. Which of these is true? In summary, these are my questions: Is it possible to represent an unbounded operator using its coordinates wrt. a Schauder basis of a separable Banach/Hilbert space? If yes, how? If no, are there other practical ways to represent it? (Eg. using Hamel basis? Or as an integral kernel on $L^p$ ?) Are all operators on $\ell_2$ „infinite matrices“?","['banach-spaces', 'schauder-basis', 'hilbert-spaces', 'linear-algebra', 'functional-analysis']"
3916963,Showing that every automorphism of $S_3$ is a conjugation without using the orders of the elements,"I'd like to demonstrate that every automorphism of $S_3$ is a conjugation but without looking at the order of the elements. My attempt is the following : I've already proved that there is an injective homomorphism $f : Aut(S_3) \hookrightarrow S_3$ and also that the conjugation map $Ad$ is an automorphism. Now let's consider $g : S_3 \longrightarrow Aut(S_3)$ such as $g(\sigma) = Ad(\sigma)$ . It is easy to prove that $g$ is an injective homomorphism. Let $\psi$ be in $Aut(S_3)$ and $\sigma$ be in $S_3$ . Thus we have : $$g \circ f : Aut(S3) \hookrightarrow S_3 \hookrightarrow Aut(S_3)$$ $$ (g \circ f)(\psi) = g(\sigma) = Ad(\sigma)$$ Because of the two injections, we can deduce that $S_3 \cong Aut(S_3)$ and thus $g \circ f$ is an automorphism. I think it also proves that there is as much elements in $Aut(S_3)$ than conjugations by the elements of $S_3$ . Therefore, as the set of conjugations by the elements of $S_3$ is a subset of $Aut(S_3)$ we can deduce that every automorphism of $S_3$ is a conjugation. I'm not quite sure about what I said. Could you please help me ? Thanks a lot.","['permutations', 'group-homomorphism', 'automorphism-group', 'abstract-algebra', 'group-theory']"
3917021,Finding the limit of $a_n = \frac{n+1}{2^{n+1}}\left(\frac{2}{1}+\frac{2^2}{2}+...+\frac{2^{n}}{n}\right)$,We have to prove the convergence and find the limit of $a_n = \frac{n+1}{2^{n+1}}\left(\frac{2}{1}+\frac{2^2}{2}+...+\frac{2^{n}}{n}\right)$ . I tried to find $a_{n+1}$ in terms of $a_{n}$ and got $a_{n+1}=\frac{n+2}{2(n+1)}\left(a_n+1\right)$ and could not proceed further.Any help is appreciated. Edit : Following K.defaoite's answer limit appears to be 1 . But we still need to prove convergence.,"['limits', 'sequences-and-series']"
3917061,How to find a path tangent to a distribution,"Let $D$ be the smooth distribution on $\mathbb{R}^3$ such that $$
D_{(a,b,c)}=\{\,(x,y,z)\in\mathbb{R}^3\,:\,z-bx=0\,\}.
$$ How to show that for any $p,\,q\in\mathbb{R}^3$ , there exists a path $\alpha$ from $p$ to $q$ tangent to $D$ ?",['differential-geometry']
3917080,$θ_2$ is better than $θ_1$ to estimate $μ$?,"We offer two estimators for the average concentration $μ$ of lead in the atmosphere of a region of Quebec where factories manufacturing dyes are located. The first estimator $θ_1$ has a bias equal to $0.2$ and a variance of $0.02$ . The second estimator $θ_2$ is unbiased and has a variance equal to $0.06$ . Which one is the best estimator? I think $θ_2$ is better than $θ_1$ to estimate $μ$ , but I am not sure. EDIT A PhD student in statistics explained to me that if $MSE(\theta_1) = MSE(\theta_2)$ , then we cannot conclude. In other words, $\theta_2$ is not preferred over $\theta_1$ or inversely. I am not sure about that.","['statistics', 'parameter-estimation', 'estimation']"
3917172,Any reference for induces exact sequence $0 \to Ker(f) \to Ker(gf) \to Ker(g) \xrightarrow{\delta} Coker(f) \to Coker(gf) \to Coker(g) \to 0.$,"Let $\mathscr{C}$ be an abelian category and $f:A \to B$ and $g:B \to C$ morphisms in $\mathscr{C}$ , then we have the following exact sequence $$0 \to \operatorname{Ker}(f) \to \operatorname{Ker}(gf) \to \operatorname{Ker}(g) \xrightarrow{\delta} \operatorname{Coker}(f) \to \operatorname{Coker}(gf) \to \operatorname{Coker}(g) \to 0.$$ I need to use this result which I know its true since I almost got it, but instead of looking help ending the proof. I'm wondering if someone knows a book or text where I can reference this result? This one looks pretty much like the Snake Lemma but I cannot find them in Literature. Or if someone help me see this as an application of a the Snake Lemma it would also be helpful. Thanks","['homological-algebra', 'category-theory', 'reference-request', 'exact-sequence', 'group-theory']"
3917198,Evaluating limits using a series,"I'm trying to use a Taylor series centered at $0$ to evaluate this limit: $$\lim_{x\to \infty}4x^3(e^\frac{-2}{x^3}-1)$$ I rewrote the function as its Maclaurin series: $$4x^3(e^\frac{-2}{x^3}-1)=\sum_{k=1}^\infty\frac{4x^{3-\frac{2k}{x^3}}}{{k!}}$$ In expanded form: $$\sum_{k=1}^\infty\frac{4x^{3-\frac{2k}{x^3}}}{{k!}}=4x^{3-\frac{2}{x^3}}+\frac{4x^{3-\frac{4}{x^3}}}{2!}+\frac{4x^{3-\frac{6}{x^3}}}{3!}+...$$ As $x$ goes to $\infty$ , $\frac{2}{x^3}$ goes to $0$ .  Thus, the limit of the first term is simply the limit of $4x^3$ , which is $\infty$ .  Based on this fact alone, I would assume, the limit of the entire series is $\infty$ , but apparently the answer is $-8$ .  What did I do wrong?","['limits', 'calculus', 'taylor-expansion', 'sequences-and-series']"
3917211,Proof of a compound proposition being a tautology,"The question states the following compound proposition is a tautology, which I have to prove. $$(p \rightarrow q) \wedge (q \rightarrow r) \rightarrow (p \rightarrow r)$$ My attempt is as follows: $$\equiv (\neg p \vee q) \wedge (\neg q \vee r) \rightarrow (\neg p \vee r)$$ $$\equiv \neg((\neg p \vee q) \wedge (\neg q \vee r)) \vee (\neg p \vee r)$$ $$\equiv \neg[((\neg p \vee q) \wedge \neg q) \vee ((\neg p \vee q) \wedge r)] \vee (\neg p \vee r)\text{ [Distribution of conjunction over disjunction]}$$ $$\equiv \neg[((\neg p \wedge \neg q) \vee (q \wedge \neg q) \vee ((\neg p \vee q) \wedge r)] \vee (\neg p \vee r)\text{ [Distribution of conjunction over disjunction]}$$ $$\equiv \neg[((\neg p \wedge \neg q) \vee F )\vee ((\neg p \vee q) \wedge r)] \vee (\neg p \vee r)\text{ [Idempotent law]}$$ $$\equiv \neg[(\neg p \wedge \neg q) \vee ((\neg p \vee q) \wedge r)] \vee (\neg p \vee r)\text{ [Idempotent law]}$$ $$\equiv [(p \vee q) \wedge ((p \wedge \neg q) \vee \neg r)] \vee (\neg p \vee r)\text{ [De Morgan's law]}$$ $$\equiv (p \vee q) \wedge ((p \wedge \neg q) \vee \neg r) \vee (\neg p \vee r)$$ $$\equiv (p \vee q) \wedge (\neg p \vee r) \vee ((p \wedge \neg q) \vee \neg r)\text{ [Commutative law]}$$ $$\equiv [((p \vee q) \wedge \neg p) \vee ((p \vee q) \wedge r)] \vee ((p \wedge \neg q) \vee \neg r)\text{ [Conjunction over disjunction]}$$ $$\equiv [((p \wedge \neg p) \vee (q \wedge \neg p)) \vee ((p \vee q) \wedge r)] \vee ((p \wedge \neg q) \vee \neg r)\text{ [Conjunction over disjunction]}$$ $$\equiv [(F \vee (q \wedge \neg p)) \vee ((p \vee q) \wedge r)] \vee ((p \wedge \neg q) \vee \neg r)\text{ [Negation law]}$$ This goes on for quite a while, but I don't get the desired $T$ result. I'm guessing I've done something horribly wrong in the above calculation. I'll be so grateful for your help here. Anything regarding this text, from calculation error to using some incorrect terms. Thank yoou.","['propositional-calculus', 'logic', 'discrete-mathematics']"
3917316,Help solving the limit of the sequence: $\left( \frac{2n^2 - 1}{2n^2 + 1} \right) ^ { \left( \frac{2n^3 - n}{n + 3} \right) }$,"Given the following sequence, $$\left( \frac{2n^2 - 1}{2n^2 + 1} \right) ^ { \left( \frac{2n^3 - n}{n + 3} \right) }$$ I am asked to determine to which $l \in \mathbb{R}$ the sequence converges. This is what I have tried so far: $$
\lim_{n} \left( \frac{2n^2 - 1}{2n^2 + 1} \right) ^ { \left( \frac{2n^3 - n}{n + 3} \right) } \implies
\left( \frac{2n^2 + 1}{2n^2 - 1} \right) ^ { - \left( \frac{2n^3 - n}{n + 3} \right) }  \implies 
\left( \frac{2n^2}{2n^2 - 1} + \frac{1}{2n^2 - 1} \right) ^ { - \left( \frac{2n^3 - n}{n + 3} \right) } \implies 
\left( 1 + \frac{1}{2n^2 - 1} \right) ^ { - \left( \frac{2n^3 - n}{n + 3} \right) } \implies ? $$ My objective was to algebraically modify the sequence such that I could be able to reduce it to the form: $$\lim _{x\to +\infty }\left(1+{\frac {1}{x}}\right)^{x}=e$$ but seem to be stuck at the ? point. Any suggestion? Thank you.","['limits', 'convergence-divergence', 'sequences-and-series']"
3917514,$\lVert A^{-1} \rVert \ge \frac{1}{\lVert A-B \rVert}$ for norm $\lVert•\rVert$ and nonsingular (matrix) $A$ and singular $B$?,"I have a question. For any norm $\lVert•\rVert$ and any nonsingular matrix $A$ and singular matrix $B$ , $$\lVert A^{-1} \rVert \ge \frac{1}{\lVert A-B \rVert}$$ How can we show that?","['matrices', 'numerical-methods', 'normed-spaces', 'linear-algebra']"
3917532,"Is there any significance to this ""doubly stochastic matrix"" with both a discrete and continuous index?","This is just idle curiosity. Consider the function $(\lambda, n) \mapsto e^{-\lambda} \frac{\lambda^n}{n!}$ , where $\lambda \in \mathbb{R}_{\ge 0}$ is a nonnegative real parameter and $n \in \mathbb{Z}_{\ge 0}$ is a nonnegative integer parameter. This function has the very funny property of being a ""doubly stochastic matrix"" in the following sense: we have both $$\int_0^{\infty} e^{-\lambda} \frac{\lambda^n}{n!} \, d \lambda = 1$$ (the integrand being, for fixed $n$ , the probability density function of a sum of $n + 1$ exponential random variables $\text{Exp}(1)$ , or an Erlang random variable $\text{Erlang}(n+1, 1)$ ) and $$\sum_{n \ge 0} e^{-\lambda} \frac{\lambda^n}{n!} = 1$$ (the summand being, for fixed $\lambda$ , the probability density function of a Poisson random variable $\text{Pois}(\lambda)$ ). Question: What significance, if any, does this observation have? What this means concretely is that $e^{-\lambda} \frac{\lambda^n}{n!}$ can be used as a ""kernel"" that converts between probability distributions on $\mathbb{Z}_{\ge 0}$ and probability distributions on $\mathbb{R}_{\ge 0}$ , in either direction. The two descriptions of this function above also have the funny implication that for large $n$ as a function of $\lambda$ we have a Gaussian approximation, and the same for large $\lambda$ as a function of $n$ , as a result of applying the central limit theorem first to a sum of exponential random variables and then to a sum of Poisson random variables.","['stochastic-matrices', 'probability-distributions', 'probability']"
3917544,"Is my proof that the Sharkovsky Ordering is a total ordering, correct?","The Sharkovsky ordering is an ordering of the natural numbers $\mathbb{N}$ , where $3$ $\prec$ $5 $ $\prec$ $7 $ $\prec$ $9$ $\prec$ ... $2*3$ $\prec$ $2*5$ $\prec$ $2*7$ $\prec$ $2*9$ $\prec$ ... ... $2^n*3$ $\prec$ $2^n*5$ $\prec$ $2^n*7$ $\prec$ $2^n*9$ $\prec$ ... ... ... $2^n$ $\prec$ ... $\prec$ $2^3$ $\prec$ $2^2$ $\prec$ $2$ $\prec$ $1$ The ordering starts with all odd numbers, except for one, in increasing order, followed by two times the odds, $2^2$ times the odds, $2^3$ times the odds and so one. Finally the powers of two are listed last in decreasing order. I know what to prove that this order is a total ordering. Thus, I need to show Antisymmetry: If $a \leq b$ and $b \leq a$ then $a=b$ Transitivity: If $a \leq b$ and $b \leq c$ then $a \leq c$ Connexity: $a\leq b$ or $b \leq a$ I came up with the following mapping from the Sharkovsky ordering to the natural numbers: \begin{equation}\label{eq1}
  P =
    \begin{cases}
      \mathbb{N}_0 \times \mathbb{N}_0 & \rightarrow \mathbb{N}\\
      (r,p) & \rightarrow 2^r \cdot (2p+1)
    \end{cases}       
\end{equation} As this function is bijective, each natural number appears exactly once in the Sharkovsky ordering. See answer below for attempt at proof.","['well-orders', 'real-analysis', 'natural-numbers', 'elementary-set-theory', 'dynamical-systems']"
3917562,Norm of a linear form in a function space,"I would appreciate some help with the following problem. Let $u, u_n:C([0,1])\to\mathbb{R}$ the linear forms defined by $$u(f)=\int_0^1f(x)dx\quad\&\quad u_n(f)=\frac{1}{n}\sum_{k=1}^nf\left(\frac{k}{n}\right).$$ I'm trying to prove that $$||u_n-u||=\sup_{||f||_\infty=1}|u_n(f)-u(f)|=2.$$ I already have that $||u_n-u||\leq2$ and I've tried to show that the $\sup$ is attained (I'm not sure that it is true) by using trigonometric functions but I haven't been able to get anything. Could anyone give me a hint? Thanks in advance.","['definite-integrals', 'normed-spaces', 'functions', 'functional-analysis', 'riemann-sum']"
3917577,Conditional probability and independent sigma algebras,"Exercise: Given a random variable $X$ , sigma algebras $\mathcal{G},\mathcal{H}$ such that $\mathcal{H}$ is independent from both $\mathcal{G}$ and $X$ prove that $\operatorname{E}[X\mid\sigma(\mathcal{G},\mathcal{H})]=\operatorname{E}[X\mid\mathcal{G}]$ . I've proven that $\sigma(\mathcal{G},\mathcal{H})=\sigma(S)$ where $S=\{ H\cap G \vert H\in\mathcal{H},G\in\mathcal{G}\}$ . Obviously $\operatorname{E}[X\mid\mathcal{G}]$ is $\sigma(\mathcal{G},\mathcal{H})$ -mesurable.  I've also proven that $\forall A\in S ,\ \operatorname{E}[X\mid\mathcal{G}]$ satisfies the conditional expectation property, i.e. $\int_AX=\int_A\operatorname{E}[X\mid\mathcal{G}]$ I would like to know if i can conclude using a criteria for coincidence of finite measures: given $A\in\sigma(\mathcal{G},\mathcal{H})$ the maps that send $A \rightarrow \int_AX$ and $A \rightarrow \int_A\operatorname{E}[X\mid\mathcal{G}]$ are well defined finite measures on $\sigma(\mathcal{G},\mathcal{H})$ and both coincide on $S$ which is a set of generators for $\sigma(\mathcal{G},\mathcal{H})$ and it is also stable under finite intersection, thus such measures coincide on $\sigma(\mathcal{G},\mathcal{H})$ which is equivalent with what i want to show. Thank you","['measure-theory', 'independence', 'conditional-expectation', 'probability', 'random-variables']"
3917631,Can an isomorphism of subgroups of a finite group always be extended to an automorphism on the group?,"Consider the problem: Let $G$ be a finite group. Let $H, K$ be arbitrary subgroups of $G$ . Let $f: H\to K$ be an isomorphism on groups. Can we always construct $F\in\text{Aut}\,(G)$ such that $F_{|H}=f$ ? I am quite sure the answer is no. To this end, I am seeking the following: A concrete example of when this fails. A characterization of groups where this condition fails/does not fail from a representation-theoretic perspective (if it exists). I have searched around a bit and not come up with much else other than this thread on MO and this thread on MSE from ten years ago, so links to (recent?) research papers are very much appreciated! Edit: I am now aware of a paper by Pettet and another one by Schupp both of which assert that an automorphism $\phi: G\to G$ is extendable to an automorphism $\psi: E\to E$ where there is a natural inclusion $\iota: G\to E$ if and only if $\phi$ is inner. I suppose what I am seeking now is an equivalent formulation and proof of this problem (if one exists) from a representation-theoretic perspective. I would also be interested in approaches making use of homological algebra.","['group-isomorphism', 'representation-theory', 'reference-request', 'abstract-algebra', 'group-theory']"
3917729,Product of derivations,"Let $A$ be an algebra over a field $K$ . If $D_1$ and $D_2$ are derivations of $A$ , show that $D_1 \circ D_2$ is not necessarily a derivation (it is if $D_1$ or $D_2 = 0$ ), but $D_1 \circ D_2−D_2 \circ D_1$ is always a derivation of $A$ . It's a problem from Tu, Loring W's An introduction to manifolds . Obviously, the composition of linear maps is linear, but how can I check the Leibniz rule? Meanwhile, why it defines to be $D_1 \circ D_2−D_2 \circ D_1$ ? Are there any motivations or backgrounds?","['manifolds', 'abstract-algebra', 'derivatives', 'differential-geometry']"
3917735,How many functions are possible?,"I would appreciable any insight you give to solve this type of question: How many $f: \{1,2,3\}\to \{1,2,3\}$ satisfy $f(f(x))=f(f(f(x)))$ for all $x$ ? Normally questions of this type I used try by guessing, until I can't see any more possibilities. But I don't like this way of solving, do you know a theorem or some tool to solve this type of question with certainty?",['functions']
3917770,Derivative of trace function,"$\DeclareMathOperator{\tr}{tr}$ Let $A,B$ be self-adjoint matrices and $f$ be a real differentiable function on $\mathbb{R}$ with derivative $f'$ . Then why is it true that $$
\left.\ \frac{d}{dt}\right|_0 \tr f(A+tB)=\tr (f'(A)B)
$$ This is used in the Klein's inequality . However, I'm not sure why exactly this is true in general. It's pretty clear why it's true for polnomials since we can use the commutation relation of the trace function, but it's harder to justify in general. I also checked the linked reference (E. Carlen, Trace Inequalities and Quantum Entropy: An Introductory Course, Contemp. Math. 529 (2010) 73–140) with no luck, as the author didn't give much explanation. EDIT : After some further thought, let me provide an incomplete proof of what I got so far. Hopefully someone with better knowledge can finish the proof. For simplicity, let $\lambda_i(A)$ denote the eigenvalues of $A$ in descending order, i.e., $\lambda_1(A) \ge \cdots \ge \lambda_d (A)$ . Then $$
\tr \left( \frac{f(A+tB)-f(A)}{t}\right) = \sum_i \frac{1}{t}[f(\lambda_i(A+tB)-f(\lambda_i(A))]
$$ Notice that by Weyl's inequality (stability of eigenvalues), we see that $|\lambda_i(A+tB)-\lambda_i(A)|\le t||B||$ . Hence, using an $\epsilon,\delta$ arguement, we can replace the above with $$
\sum_i \frac{1}{t}(\lambda_i(A+tB)-\lambda_i(A)) f'(\lambda_i(A))
$$ Now first assume that $A$ has a simple spectrum, then $A+tB$ is also simple for sufficiently small $t$ . Then by Hadarmard's variation formula, we see that $$
\frac{1}{t}(\lambda_i(A+tB)-\lambda_i(A)) \to \langle i|B| i\rangle
$$ where $|i\rangle$ is the corresponding eigenvector (unique up to phase since we are assuming that $A$ is simple) to $\lambda_i(A)$ . Plugging all this back in, we see that the formula at least holds when $A$ is simple. EDIT 2 . I think I now have a way of dealing with degenerate eigenvalues. I will provide a sketch and fill in the details later (if someone else doesn't point out an error). Let $\lambda_1 (A)=\cdots =\lambda_r(A)$ be the degenerate eigenvalues. Then for sufficiently small $t$ , the eigenvalues $\lambda_i (A+tB),i=1,...,r$ will not touch the other eigenvalues (Weyl's inequality again). Let us use the Riesz projector $$
P_A =\frac{1}{2\pi i} \oint_\Gamma \frac{dz}{A-z}
$$ where $\Gamma$ is some ""smooth"" contour around the $\lambda_1 (A)=\cdots =\lambda_r(A)$ and its interior does not contain any other eigenvalues. By Weyl's inequality, we can assume that $\lambda_i(A+tB),i=1,...,r$ are still in the interior of $\Gamma$ for sufficiently small $t$ . Notice that $$
\frac{d}{dt} \Big|_0 \tr {((A+tB)P_{A+tB})} = \tr(BP_A)
$$ where I got some inspiration from @Ruy's comment and used the fact that \begin{align}
\frac{d}{dt}\Big|_0 \tr{(A(P_{A+tB}-P_A))}&=\tr A\oint_\Gamma \frac{dz}{(z-A)^2}B \\
&= \sum_{i=1}^r \oint_\Gamma \lambda_i(A)\frac{1}{(z-\lambda_i(A))^2} dz \langle i|B|i\rangle \\
&=0
\end{align} Hence, if we combine this with the previous part, we see that the equality holds. My proof is a little convoluted, so I would still hope to see a more straightforward approach","['functional-analysis', 'real-analysis']"
3917785,How to simplify function to $\log (1-\theta)+\frac{\theta}{2-\theta}$?,"\begin{align}
f(\theta)& = \log \frac{1}{1-\theta}-\frac{\theta}{2-\theta}+\frac{\theta^{2}}{(2-\theta)^{2}} \\
& = \log (1-\theta)+\frac{\theta}{2-\theta}
\end{align} How are the two equations above equal to each other, i.e. reduce the first into the second? How can one term be outside of the logarithm on the second line, whereas all 3 terms fall inside the logarithm on the first line? Actually, I'm not even sure if all the terms in the first line are supposed to all fall inside the logarithm, no brackets were given, so it could be either or Source of first equation: Formula 22 in Mercier 2005 Source of second equation: Formula 4.4 in Kumar 2011 If they don't equal each other like how the second author claims, which author made a mistake? Edit I found yet another equality presented by the same second author. This time, Formula 3.41 of Kumar 2014 : $$
f(\theta) = \log (1-\theta)+\frac{2 \theta(1-\theta)}{(2-\theta)^{2}}
$$","['copula', 'functions', 'entropy', 'logarithms']"
3917840,Intuition for multivariable functions,"Normal equations are easy enough to perceive as geometrical shapes or volumes when defined explicitly in the form $y=f(x)$ . But how do I understand equations like $g(x+y)=e^yg(x)+e^xg(y)$ ? I can make some sense of it algebraically, with it being a definition of a function and $x$ and $y$ being two variables. But how is it intuitively represented?","['intuition', 'functions', 'graphing-functions']"
3917870,Why this is an empirical relation?,"As a high school student I am not sure why this following formula which is used as a relation between mean , median and mode found to hold for unimodal distribution , is called empirical : $$ mean-mode=3(mean-median)$$ is there really no mathematical proof? My text book says that this is completely based on symmetry. Can some one help me to clarify this? Is there any existing proof in higher classes? Thanks in advance. EDIT :Later I found this in cross validated but that's not quite accessible to me I will be happy if I get a simpler version or at least an intuitive understanding.","['descriptive-statistics', 'statistics']"
3917912,Lower bound for sum of reciprocals of positive real numbers,"I am reading an article where the author seems to use a known relationship between the sum of a finite sequence of real positive numbers $a_1 +a_2 +... +a_n = m$ and the sum of their reciprocals. In particular, I suspect that \begin{equation}
\sum_{i=1}^n \frac{1}{a_i} \geq \frac{n^2}{m} 
\end{equation} with equality when $a_i = \frac{m}{n} \forall i$ . Are there any references or known theorems where this inequality is proven? This interesting answer provides a different lower bound. However, I am doing some experimental evaluations where the bound is working perfectly (varying $n$ and using $10^7$ uniformly distributed random numbers).","['inequality', 'upper-lower-bounds', 'summation', 'sequences-and-series']"
3917939,"Prove that, if $\frac{\tau_i}{\tau_j}\in\Bbb Q\space j\ne i,\space i,j\in\{1,\ldots,n\},$ the function $f(x)=\sum_{k=1}^nf_k(x)$","Let $f_1,f_2,\ldots,f_n:\Bbb R\to\Bbb R$ be (non-constant) periodic functions with periods $\tau_1,\tau_2,\ldots,\tau_n\in\Bbb R^+$ respectively. Prove that, if $\dfrac{\tau_i}{\tau_j}\in\Bbb Q\space j\ne i,\space i,j\in\{1,\ldots,n\},$ the function $f(x)=\sum\limits_{k=1}^nf_k(x)$ is periodic. My attempt: I gave it a try by induction. Let's look at the base case $n=2$ and let's take $g(x)=(f_1+f_2)(x),$ where $f_1(x)=f_1(x+\tau_1),\space\forall x\in\Bbb R$ and $f_2(x)=f_2(x+\tau_2)\forall x\in\Bbb R$ . If $\dfrac{\tau_1}{\tau_2}\in\Bbb Q,$ then we either have $$\tau_1\in\Bbb Q^+\space\land\space\tau_2\in\Bbb Q^+$$ or $$\tau_1=q_1r\in\Bbb R\setminus\Bbb Q\space\land\space\tau_2=q_2r\in\Bbb R\setminus\Bbb Q,\quad q_1,q_2\in\Bbb Q^+,\space r\in\Bbb R^+\setminus\Bbb Q$$ In the first case, let $\tau_1=\dfrac{a}b,\space\tau_2=\dfrac{c}d$ and, WLOG, let's assume $\gcd(a,b)=1,\space\gcd(c,d)=1,\space a,b,c,d\in\Bbb N$ (I don't include $0$ ). Then: $$\begin{aligned}f_1(x)&=f_1\left(x+\frac{a}b\right)=f_1(x+a),\forall x\in\Bbb R\\f_2(x)&=f_2\left(x+\frac{c}d\right)=f_2(x+c),\forall x\in\Bbb R\end{aligned},$$ but then, we can find $m,n\in\Bbb N$ s. t. $ma=nc=\gcd(a,c)=k,$ so $f_1(x)=f_1(x+k),\space\forall x\in\Bbb R$ and $f_2(x)=f_2(x+k),\space\forall x\in\Bbb R$ and therefore, $g(x)=(f_1+f_2)(x)=(f_1+f_2)(x+k)=g(x+k)\space\forall x\in\Bbb R$ . In the other case, analogously, let $q_1=\dfrac{a}b,q_2=\dfrac{c}d,\space\gcd(a,b)=1,\gcd(c,d)=1,a,b,c,d\in\Bbb N$ and let $r\in\Bbb R^+$ . Then: $$\begin{aligned}f_1(x)&=f_1\left(x+\dfrac{a}br\right)=f_1(x+ar),\forall x\in\Bbb R\\f_2(x)&=f_2\left(x+\dfrac{c}dr\right)=f_2(x+cr),\forall x\in\Bbb R\end{aligned}$$ and we can again find $\gcd(a,c)=ma=nc=k$ s. t. $g(x)=(f_1+f_2)(x)=(f_1+f_2)(x+kr)=g(x+kr)\forall x\in\Bbb R$ . Then, if we assume the statement holds for any $2\le m\le n$ , i. e., that $g(x)=\sum\limits_{k=1}^m f_k(x)$ , where the period of $f_k$ is $\tau_g=\dfrac{a_k}{b_k}\in\Bbb Q^+$ or $\tau_g=\dfrac{a_k}{b_k}r\in\Bbb R^+$ and therefore $g(x)=g(x+\gcd(a_1,a_2,\ldots,a_m))$ or $g(x)=g(x+\gcd(a_1,a_2,\ldots,a_m)r),2\le m\le n$ , then: if $$\tau_{n+1}=\dfrac{a_{n+1}}{b_{n+1}}\in\Bbb Q\space\land\space\tau_g\in\Bbb Q$$ or $$\tau_{n+1}=\dfrac{a_{n+1}}{b_{n+1}}\in\Bbb R^+\setminus\Bbb Q\space\land\space\tau_g\in\Bbb R^+\setminus\Bbb Q$$ $f(x)=g(x)+f_n(x)$ will be periodic with a period $\tau=\gcd(a_1,a_2,\ldots,a_n,a_{n+1})$ or $\tau=\gcd(a_1,a_2,\ldots,a_n,a_{n+1})r$ respectively. May I ask for verification of my work so far and advice on what I have to correct/improve? Thank you in advance!","['periodic-functions', 'functions', 'solution-verification', 'real-analysis']"
3917973,Differentiation in fields with norm or metric,"We start learning about derivatives in the real numbers setting.
And we know that the reals are the only complete ordered field possible.
But what happens if we relax the ""order"" property? The definition f'(x) = lim (f(x+h) - f(x))/h, h -> 0, still holds,
if we introduce a metric (or norm), so that the limit makes sense.
So what is the general theory for this setting, i.e.,
calculus in metric, or normed, fields? I have seen various articles even for topological fields,
but is there a good textbook survey of the various approaches and applications?
And is this topic an active area of current research?","['derivatives', 'reference-request']"
3918001,A lattice of circles and a line which does not cut any of them.,"Each lattice point is a center of a circle, all with radius $d$ . Let line $y={2\over 5}x+n$ doesn't cut or touch any circle, for some $n$ . Find the supremum for $d$ . I was trying to maximize $$d={|2x-5y+5n|\over \sqrt{29}}$$ where $x,y$ runs over all integers and we can assume that $n$ is in $(0,1)$ , but I don't know how to do it. Any help?","['analytic-geometry', 'geometry']"
3918106,1-parameter group of diffeomorphisms with vector field without compact support,"Let M be smooth manifold $(0,1)\subset \mathbb{R}$ , let vector field on it be the stanadard $\frac{d}{dt}$ prove this vector field does not generate the 1-parameter group of diffeomorphisms. The solution for the  equation: $\frac{dx}{dt} = 1$ , is given as $F(t,\bar{x}) = t + \bar{x}$ why it can't be the 1-parameter group of diffeomorphism?(that is diffeomorphic + flow axiom)","['smooth-manifolds', 'differential-geometry']"
3918121,Proving identity in infinite series,"I am trying to prove the equation \begin{align*}
\sum_{n=0}^{\infty} a^n \sum_n b_1^{c_1} \cdot b_2^{c_2} \cdots b_d^{c_d} = \prod_{i=1}^d \frac{1}{1 - a b_i},
\end{align*} where we have that $n = c_1 + c_2 + ... c_d$ . This is what I have tried so far: \begin{align*}
\sum_{n=0}^{\infty} a^{c_1 + c_2 + \ldots + c_d} \sum_n b_1^{c_1} \cdot b_2^{c_2} \cdots b_d^{c_d}
&= \sum_{n=0}^{\infty} a^{c_1} \cdot a^{c_2} \cdot \cdot \cdot a^{c_d} \sum_n b_1^{c_1} \cdot b_2^{c_2} \cdot \cdot \cdot b_d^{c_d} \\
&= \sum_{c_1}^{\infty} a^{c_1} \cdots \sum_{c_d}^{\infty} a^{c_d} \cdot \sum_{c_1}^{} b_1^{c_1} \cdots \sum_{c_d} b_d^{c_d} \\
&= \sum_{c_1}^{\infty} (a b_1)^{c_1} \cdots \sum_{c_d}^{\infty} (ab_d)^{c_d}.
\end{align*} My doubts in here are whether it is even possible to split up the sum in this manner and then simply recombining it in the last line, since the product of two sums is not necessarily the sum of the product. From here I would use \begin{align*}
\sum_{k}^{\infty} x^k = \frac{1}{1-x},
\end{align*} such that it would become \begin{align}
\frac{1}{1-a b_1} \times \cdots \times\frac{1}{1 - a b_d} = \prod_{i = 1}^{d} \frac{1}{1-a b_i}.
\end{align} I would like to ask where my errors are and if there is a better way to prove this identity. Thank you for your time.","['summation', 'sequences-and-series']"
3918173,Emedding of $\mathbb{ RP}^3$,"Is there a simple formula for an embedding (homeomorphic onto its image) of $\mathbb{RP}^3$ in some Euclidean space? I have seen a simple formula for $\mathbb{RP}^2$ in $\mathbb R^4$ , but I can't find much of anything on $\mathbb{RP}^3$ . I am not asking for an immersion, but a true embedding.","['submanifold', 'projective-space', 'differential-geometry']"
3918202,"$A_1,...,A_n,...$ measurable sets such that: ... Then $ \lambda (\bigcup _{k=1}^{\infty} A_k) \geq 1 $.","$A_1,...,A_n,...$ measurable sets such that: i) $\lambda (A_n) \geq 1/2 $ forall $k$ ii) $\lambda (A_s \cap A_k ) \leq 1/4 $ forall $k \neq s$ Then $ \lambda (\bigcup _{k=1}^{\infty} A_k) \geq 1 $ . Attemp: We consider the metric Boolean algebra $^\mathcal{A } / _\sim$ defined from : $d(A,B) = \mu (A \triangle B)$ . We know that $^\mathcal{A } / _\sim$ is complete and when $\mathcal{A}= \mathbb{R} $ then $^\mathcal{A } / _\sim$ is also seperable. Also we have that $d(A_n,A_m) \geq \frac{1}{2}$ for all $n \neq m$ .","['measure-theory', 'measurable-functions']"
3918285,Limit of $a_{n+1} = a_{n}(1+1/n^2)$,"I read the following exercise : Let $(a_n)$ be a sequence such as : $a_1 = 2$ , $a_{n+1} = a_n.(1+1/n^2)$ Prove that $\lim_{n \rightarrow \infty} a_n$ exists. Hint : $\ln : \: (0, \infty) \rightarrow \mathbb{R} \:$ is a continuous, increasing function such as $\ln(1+x) \leq x$ and $\ln(ab) = \ln(a)+\ln(b)$ . It is quite clear that $a_n$ is increasing but then I don't really know how to proceed. Could you please help me ? Thanks.","['limits', 'sequences-and-series', 'real-analysis']"
3918294,Is this continuous function injective?,"The following question was part of my complex analysis quiz and I was unable to mark 1 option . I am not able to deduce that is it correct on not. Let  X= { $(a,b)\in \mathbb{R}^2 : a^2+b^2 =1$ } and f : X-> $\mathbb{R}
 $ be a continuous function. Then is f injective or not? I have given ample time to this option but I am unable to find any contradiction or a function satisfying it. So, with no option left I am posting it here. Please help!","['continuity', 'general-topology', 'real-analysis']"
3918344,Homomorphisms of a Finite Group & Perfect Squares,"I'm working on a textbook problem: Come up with a finite group $G$ and homomorphism: $f: G \rightarrow G$ such that $3$ % of the elements of in $G$ satisfy $f(x) = x^2$ (where $x$ is an element in $G$ ). So far, I've tried a lot of different ideas. When I made the binary operator for $G$ additive, I had a tough time since the homomorphism must follow a lot of conditions such as $f(2) = 2f(1)$ , not leaving a lot of space for me to try and come up with a way to get $f(x) = x^2.$ I tried making the group a power group and multiplicative. In both scenarios, I didn't really get anything too useful – every idea that I tried ultimately failed. Does anyone know how to do this?","['group-homomorphism', 'group-theory', 'finite-groups']"
3918387,Easy way to see that $(x^2 + 5x + 4)(x^2 + 5x + 6) - 48 = (x^2 + 5x + 12)(x^2 + 5x - 2)$?,"As the title suggests, is there an easy way to see that $$(x^2 + 5x + 4)(x^2 + 5x + 6) - 48 = (x^2 + 5x + 12)(x^2 + 5x - 2)$$ that doesn't require expanding in full? Is there a trick?","['contest-math', 'algebra-precalculus', 'quadratics', 'factoring']"
3918405,Calculating the limit of the following series,"I want to prove that $$\lim_{x\to1^-}(1-x)\sum_{n=1}^{\infty}(-1)^{n-1}\frac{nx^n}{1-x^{2n}} = \frac14$$ So far I tried to manipulate the series for instance using $$\sum_{n=1}^{\infty}(-1)^{n-1}\frac{nx^n}{1-x^{2n}} =  -\sum_{n=1}^{\infty}(-1)^{n}nx^n\sum_{m=0}^{\infty}\left(x^{2n}\right)^m$$ since $x < 1$ . Interchanging the two sums (not sure if allowed) I obtained, assuming I did not make mistakes, the sum $$\sum_{m=0}^{\infty}\frac{x^{2m+1}}{(1+x^{2m+1})^2}$$ I am unable to continue from this point. Perhaps my work isn't actually useful at all. Can you help me?","['limits', 'sequences-and-series', 'real-analysis']"
3918416,USAMO problem hint.,"Prove that for every positive integer n there exists an n-digit number divisible by 5 $^n$ all of whose digits are odd. USAMO 2003. This is the first time i have seen a problem like this, so i am not sure what to do, induction, construction, checking small cases, contradiction are some of the things i have tried. I know I can easily find a solution anywhere but i don't want to look at a solution so please give HINTS . I HAVE POSTED A SOLUTION USAMO problem solution HERE, PLEASE DO CHECK IT OUT. Please don't give the full solution, any hints would be appreciated.","['divisibility', 'number-theory', 'elementary-number-theory', 'exponential-function', 'prime-numbers']"
3918423,On some doubts on tangent space of immersed submanifold,"In Lee's book ""Introduction to Smooth manifolds"", he following lemma can be found.
Lemma 8.26 Let $M$ be a smooth manifold, let $S\subseteq M$ be an embedded submanifold, and let $Y$ be a smooth vector field on $M$ . Then $Y$ is tangent to $S$ iff $Yf$ vanishes on $S$ for all $f\in C^\infty(M)$ such that $f|_S=0.$ My question is what if $S$ just an immersed submanifold? Is the same conclusion true? It should be because earlier Lee proves that any immersed submanifold is locally embedded submanifold. But why does Lee state the theorem only for embedded submanifod?","['submanifold', 'smooth-manifolds', 'differential-geometry']"
3918428,Local analysis two-variable functions with absolute value,"I have a problem with the local analysis of two-variable functions which have absolute value. For example, how can I study nature of point $(0,0)$ in $f:\mathbb{R}^2 \to \mathbb{R}, f(x,y)=sin(xy+|xy|)$ ? Or in the function $f:\mathbb{R}^2 \to \mathbb{R}, f(x,y)=e^{sin(|xy|)}$ ? I think that in both cases $(0,0)$ is a local minimum, but I don't know how to prove it. Typically, I use the Hessian matrix to do the analysis, however in this case I don't think it will help me...","['multivariable-calculus', 'hessian-matrix', 'analysis']"
3918460,Serge Lang's projection,"This question is a follow-up to Identity up to isomorphism treated as identity in proof . I thought that with all the kind help given there, now I would be able to work out the sketch of a proof given by Lang for the corollary dual to the one in the above thread, and to eliminate his assumptions of identity based on an identity up to isomorphism there, too. But I can't. Here is the problem: In ""Fundamentals of Differential Geometry"", 1999, pp.18-19, Serge Lang gives the following definition: And then this corollary to the inverse mapping theorem: First of all, some clarifications: Morphism means $ C^p$ map, local isomorphism means local $ C^p$ diffeomorphism, toplinear isomorphism can be considered to be a linear isomorphism here. Furthermore, I understand to be $ V_1 \subseteq E_1 $ and $ V_2 \subseteq E_2 $ , and the local inverse h, which Lang refers to, to be $ \varphi^{-1} $ , and not the inverse of the derivative, as Lang's wording implies. Again, what I don't see is how $ \varphi^{-1} $ satisfies the requirement of the corollary. In order to eliminate the identification $ E_2=F $ in the proof, let instead be $ \varphi: E_1 \times E_2 \rightarrow E_1 \times F $ . Then introduce the $ C^p $ diffeomorphism $ g: E_1 \times E_2 \rightarrow E_1 \times F: \quad (x_1,x_2) \mapsto (id_1, D_2f(a_1,a_2))[x_1,x_2] $ and replace $ h:=\varphi^{-1} $ by the $ C^p $ diffeomorphism $ h \circ g: E_1 \times E_2 \rightarrow E_1 \times E_2 $ . But with this, how does the resulting map $ f \circ h \circ g: E_1 \times E_2 \rightarrow F $ factor into an ordinary projection $ V_1 \times V_2 \rightarrow V_2 $ and a linear isomorphism $ V_2 \rightarrow W(0) \subseteq F $ with an open neighborhood W? Can we state the local map $ \varphi^{-1} $ explicitly? Is it $ \varphi^{-1}(x_1,y) = (x_1, pr_2 \circ f^{-1}(y)) $ for $ y \in F $ ? Clearly $ \varphi^{-1}(\varphi(x_1,x_2))= \varphi^{-1}(x_1,f(x_1,x_2)) = (x_1,x_2) $ . But the other way around does not resolve properly: $ \varphi(\varphi^{-1}(x_1,y))= \varphi(x_1, pr_2 \circ f^{-1}(y)) =(x_1,f(x_1,pr_2 \circ f^{-1}(y)) $ . And by the way, can we take f to be locally invertible, too? Evaluating the composition $ f \circ h \circ g $ seems to lead nowhere $ f(h(g(x_1,x_2))) = f(h(x_1,D_2f(a_1,a_2)[x_2])) = f(x_1,pr_2 \circ f^{-1}(D_2f(a_1,a_2)[x_2])) $ . So, how to proceed? Where is the error, or what is the necessary idea? I thought about explicitly introducing the projection $ pr_2: E_1 \times E_2 \rightarrow E_2 \equiv (\{0\} \times E_2) \subseteq (E_1 \times E_2) $ at the beginning of the composition: $ f \circ h \circ g \circ pr_2 $ , but unfortunately the projection is no $ C^p $ -diffeomorphism.","['banach-spaces', 'inverse-function-theorem', 'differential-geometry']"
3918473,Show that $\int^{\infty}_{-\infty}\frac{dx}{(x^2-4x+5)^2} = \frac{\pi}{2}$ using residue theory,"I'm trying to evaluate some complex integrals using residue theory. I've read a number of articles with different examples here on Stack Exchange, but I'm still really lost and could use some help. Show that $\int^{\infty}_{-\infty}\frac{dx}{(x^2-4x+5)^2} = \frac{\pi}{2}$ . First, I found the poles of the function as $2-i$ and $2+i$ . Both of these poles are of order $2$ . I am also considering my region as the semicircle of radius $R$ in the upper half-plane with the line segment between $x=−R$ and $x=R$ on the real axis. So I know that I need to evaulate $$\int_{\Gamma}f(z)dz = 2\pi iRes(f,2+i) + 2\pi iRes(f,2-i)$$ At this point, I'm really stuck on what to do from here. I tried evaluating the residues and was getting some really weird answers. My work was really messy and likely completely wrong, so hopefully it's OK if I don't reproduce it here. I'm not sure how to finish solving this. Similarly, I'm having trouble with this integral: $\int_{|z| = 1}z^3e^{1/z}\sin(\frac{1}{z}) dz$ I know that there is a pole at $z = 0$ . So that means I need to evaluate $$ \int_{|z| = 1}z^3e^{1/z}\sin(\frac{1}{z}) dz = 2\pi i Res(f,0)$$ Do I consider this pole of order $1$ or order $2$ and how do I find this residue?",['complex-analysis']
3918489,"Simple proof of ""maximum number of right angles in a convex $n$-gon is 3 for $n\geq 5$"" for a 8th grade student?","I know a proof of ""maximum number of right angles in a convex $n$ -polygon is 3  for $n\geq 5$ "" as follows: Suppose $k$ is the number of right angles. Then $180(n-2)-90k$ is the sum of other $n-k$ interior angles. Now we can perform these $n-k$ angles such that all have equal angle i.e. equal to their average $\frac{180(n-2)-90k}{n-k}=\frac{180(n-k)+90k-360}{n-k}=180+\frac{90(k-4)}{n-k}$ that $\frac{90(k-4)}{n-k}$ must be negative since each angle $<180$ so $90(k-4)<0$ and $k<4$ . But this proof is a bit cumbersome for 8 $^{th}$ grade school student. (The bolded part is also dubious to me and hard to accept it! Let alone the 8 $^{th}$ grade school student). Is there any more simple argument?","['euclidean-geometry', 'angle', 'geometry', 'polygons', 'alternative-proof']"
3918521,ABCD is a cyclic quadrilateral whose two diagonals are perpendicular. [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question ABCD is a cyclic quadrilateral whose two diagonals are perpendicular. If $R$ is the radius of the circumcircle, prove that: $AB$ $2$ + $BC$ $2$ + $CD$ $2$ + $DA$ $2$ = $8R$ $2$ If the centre of the circle is O, I tried drawing radii perpendicular to each of the sides. That way, we get 3 pairs of congruent right triangles.",['geometry']
3918549,ODE $y(x)=xy'(x)-\sqrt{y'(x)-1}$,"I've following differential equation... $$y(x)=xy'(x)-\sqrt{y'(x)-1}$$ I recognised that it's an Clairaut's equation and then I wanted to find the general and the singular solution. First I reformed the ODE like that... $$\frac{d}{dx}y=\frac{d}{dx}dy'-\frac{d}{dx}\sqrt{y'-1}$$ $$y'=xy''+y'-\frac{1}{2\sqrt{y'-1}}*y''$$ $$0=y''(x-\frac{1}{2\sqrt{y'-1}})$$ I can then deduce, that the equation is true if $y''=0$ or $x-\frac{1}{2\sqrt{y'-1}}=0$ I solved 1) like that... $$\int y'' dx = 0 \Leftrightarrow y'=C$$ In our original equation, that gives us... $$y=Cx-\sqrt{C-1}$$ Then I solved 2) $$x-\frac{1}{2\sqrt{y'-1}}=0 \Leftrightarrow y'=\frac{1}{4x^2}+1$$ $$\int y'\, dx=\int \frac{1}{4x^2}+1 \, dx$$ $$y = -\frac{1}{2x}+x+C$$ But I've drawn graphs of these two equations and 1), aka the general solution, doesn't touch 2), aka the singular solution. So i probably have a mistake somewhere, don't I? Can someone point it out to me?","['integration', 'derivatives', 'ordinary-differential-equations']"
3918586,The rate of change in Newton's Law of Cooling,"I know how to solve the following problem but I have a confusion concerning the sentence in bold. A metal bar is taken from an inside room and dropped into a large container of boiling water.The initial temperature of the bar was 20° C. If it is known that the temperature of the the bar increases 2°C in 1 second , How long will it take the bar to reach 98° C? The solution of the problem is as following The differential equation is $\frac{dT}{dt}=k(T_m-T)$ Its solution is $T=Ce^{-kt}+T_m$ Substitute by $T_m=100$ and by the initial condition $T(0)=20$ in the solution of the DE $T(0)=100+C$ Hence $C=-80$ Hence $T=100-80e^{-kt}$ To get the constant k, we should substitute by the condition $T(1)=22$ in the solution $22=100-80e^{-k}$ Hence $k=0.02532$ Then, we can substitute by $T=98$ in the solution to get the required time. My question is: Can we deal with the information ""the temperature of the the bar increases 2° in 1 second"" that this is the rate of change at $T=0$ ? I mean: $\frac{dT}{dt}\big|_{t=0}=2$ Hence, we can substitute in the DE $\frac{dT}{dt}\big|_{t=0}=k(100-T(0))$ $2=k(100-20)$ Hence $k=0.025$ We got finally approximately the same value for the constant k. Note: I think it is not allowed to deal with the given info as a rate because we divided the increase in temperature by the duration which is not exactly the derivative or the rate; we should divide by a duration that is infinitely small.
Also, I think that if the given info is the rate, it should be written in the form  2°c/sec. I felt confused because when I deal with the given info as a rate, it gives me approximately the same solution.","['mathematical-modeling', 'ordinary-differential-equations']"
3918660,Pullback of a vector field under a surjective submersion.,"Let $\pi: M \rightarrow N$ be a smooth map. If we consider a vector field $Y$ on $N$ , I know that, if $\pi$ is a local diffeomorphism, there exists a unique vector field $X$ on $M$ such that $\pi_*X = Y$ . I was wondering if we can weaken this hypothesis: Is there a vector field $X$ with $\pi_*X = Y$ if $\pi$ is only surjective submersion, not a local diffeomorphism? It seems quite intuitive because for each $p$ , as $\pi_{*p}$ is surjective, there is at least one $X_p$ such that $\pi_{*p}X_p = Y(p)$ , but I have some difficulties to show that we can construct a smooth $X$ with $X_p = X(p)$ .","['pullback', 'vector-fields', 'differential-geometry']"
3918666,Probability that $3$ hands of a clock in the same semi-circle?,"I am wondering how to calculate the probability that $3$ hands of a clock are in the same semi-circle ? I know a similar question is that if we randomly choose n points in a circle, then the probability that all of them in the same semi-circle will be $n/2^{(n-1)}$ with $n = 3$ . But when it comes to the dial hands, where I guess there are correlations between them, I am not sure whether my question is equivalent to the previous one.
If they are not equivalent, and this question will be hard to calculate, then whether we can determinate this probability will be greater or smaller than $3/4$ ? Anyone can help? Thanks.","['probability-theory', 'probability']"
3918796,Intersection multiplicity does not go down after restriction to closed subvariety: proof using filtrations,"Let $S$ be a noetherian domain, and $f\in S$ neither a unit nor a zero divisor. Let $P$ be a minimal prime over $f$ , let $I\subset S$ be a prime ideal, suppose the image of $f$ in $S/I$ is neither a unit nor a zero-divisor, and let $Q$ be a minimal prime over $S/(I+f)$ containing $P$ . I'm looking to verify that $\operatorname{length}_{S_P} S_P/(f)\leq \operatorname{length}_{S_Q} S_Q/(I+f)$ . This is basically the statement that if $V\subset W$ are two varieties which both intersect a hypersurface $H$ properly, the intersection multiplicity of $V\cap H$ along any component $V'$ of $V\cap H$ is at least as large as the intersection multiplicity of $W\cap H$ along an irreducible component $W'$ of $W\cap H$ containing $V'$ . I'm looking to prove this before saying something about it in my lecture when covering material related to Hartshorne I.7, but I'm having trouble finishing the argument. The main tool that comes to mind is the fact that any finitely generated module over a noetherian ring $R$ has a finite filtration by submodules $M_i$ so that the subquotients $M_{i+1}/M_i$ are isomorphic to $R/\mathfrak{p}_i$ for $\mathfrak{p}_i$ a prime ideal of $R$ , and the number of times a minimal prime shows up is the length of the module over that minimal prime. Unfortunately, I am a little rusty in this area and I can't quite finish the argument about how the filtrations of $S/(f)$ and $S/(I+f)$ relate to each other. It seems clear that I should take a filtration of one and either push it forward or pull it back along the obvious map $S/(f)\to S/(I+f)$ and argue from there, but I have had no further success. One attempt: Localizing at $Q$ , we get a finite filtration $\{M_i\}$ of $S_Q/(f)$ with subquotients $(S/\mathfrak{p}_i)_Q$ for $P\subset \mathfrak{p}_i\subset Q$ . Under the quotient map $\alpha:S_Q/f\to S_Q/(I+f)$ , we have that $\{\alpha(M_i)\}$ form a filtration of $S_Q/(I+f)$ , and $M_{i+1}/M_i=S_Q/\mathfrak{p}_i$ surjects on to $\alpha(M_{i+1})/\alpha(M_i)$ . If this latter module is nonzero, we're done - it has length at least one, so $\operatorname{length}_{S_Q} S_Q/(I+f)$ is at least the number of terms in the filtration of $S_Q/(f)$ , which is at most the number of times $S_Q/P$ appears in that filtration. But I've been getting turned around in why this ought to be the case for a while, and I could use some help. I'm looking for some help filling in the details of this argument. Alternate methods and suggestions are also welcome!","['commutative-algebra', 'algebraic-geometry', 'abstract-algebra', 'intersection-theory', 'filtrations']"
3918797,"CDF of $\frac{T_1}{T_1+T_2}$ where $T_1,T_2$ are the first two delays in a Poisson process?","$P$ is a Poisson Process with rate $\lambda$ . Let $T_1$ be the time of the first event and let $T_2$ be the time of the from the first to the second event. Let $Y = \frac{T_1}{T_1+T_2}$ . Find the density of $Y$ . I think I should find the CDF first and then take the derivative, but I do not know how to find $P(Y\leq t)$ . Do I need to find the joint density?","['stochastic-processes', 'multivariable-calculus', 'poisson-process', 'probability']"
3918863,"If $x$ is rational and $\sin(x)$ is irrational, then is $\cos(x)$ irrational?","I am wondering if the following implication is true. I know it is not true if $x$ is irrational, as clearly $x =\pi/3$ is a counterexample. $$\text{$x$ is rational and $\sin(x)$ is irrational} \implies \cos(x) \text{ is irrational}$$ I know irrationals are not closed under multiplication or addition. However, this seems true. If there is an obvious counterexample I am missing, I apologize.","['trigonometry', 'functions']"
3918889,Showing that Hopf fibration admits no global sections (not a duplicate!),"Let $\pi: \mathbb{S}^{2n+1} \subset \mathbb{C}^{n+1} \to \mathbb{CP}^n$ be given by $\pi(z_0, \cdots, z_n) = [z_0, \cdots, z_n]$ ( $\pi$ is called the Hopf fibration). Prove that $\pi$ admits no global sections, that is, there does not exist any $s: \mathbb{CP}^n \to \mathbb{S}^{2n+1}$ such that $\pi \circ s = \operatorname{Id}_{\mathbb{CP}^n}$ . I'm aware a similar question has been asked before, but that question does not solve mine and it's only similar, not the exact same. Now, here's how I went about solving this (I want to know if it's all correct): if there were such a section, then the composition $$\mathbb{R} = H^{2n}(\mathbb{CP}^n) \stackrel{\pi^{*}}{\to}H^{2n}(\mathbb{S}^{2n+1}) = 0\stackrel{s^{*}}{\to}  H^{2n}(\mathbb{CP}^n) = \mathbb{R}$$ is the identity on $H^{2n}(\mathbb{CP}^n) $ , which is evidently a contradiction since there's a $0$ in the middle. Here the cohomologies I'm working with are all de-Rham cohomology.","['de-rham-cohomology', 'smooth-manifolds', 'solution-verification', 'algebraic-topology', 'differential-geometry']"
3918907,the limit of $\frac{1}{\sqrt{n}}(1+\frac{2}{1+\sqrt2}+\frac{3}{1+\sqrt2+\sqrt3}+\dots+\frac{n}{1+\sqrt2+\sqrt3+\dots+\sqrt n})$ as $n\to\infty$,"I need to find: $$\lim_{n \to +\infty} \frac{1}{\sqrt{n}}(1+\frac{2}{1+\sqrt{2}} + \frac{3}{1+\sqrt{2}+\sqrt{3}} + \ldots + \frac{n}{1+\sqrt{2}+\sqrt{3}+\ldots+\sqrt{n}}), n \in \mathbb{N}$$ Looking at denominators, I see that [(...) represents any element between] : $$ 1 \le (\ldots) \le 1+\sqrt{2}+\sqrt{3}+\ldots+\sqrt{n}$$ Then I take reverses and get: $$ 1 \ge (\ldots) \ge \frac{1}{1+\sqrt{2}+\sqrt{3}+\ldots+\sqrt{n}}$$ Then I put the other sequence on top of the former one (I see that the rightmost element is still the smallest one) $$ 1 \ge (\ldots) \ge \frac{n}{1+\sqrt{2}+\sqrt{3}+\ldots+\sqrt{n}}$$ Then I take the sum of n elements on every end of inequality (to sum up n times the biggest element and n times the smallest element) and get: $$ n \ge (\ldots) \ge \frac{n^2}{1+\sqrt{2}+\sqrt{3}+\ldots+\sqrt{n}}$$ Ultimately I take into consideration $\frac{1}{\sqrt{2}}$ and get: $$ \frac{n}{\sqrt{n}} \ge (\ldots) \ge \frac{n^2}{\sqrt{n}(1+\sqrt{2}+\sqrt{3}+\ldots+\sqrt{n})}$$ Now I can use the squeeze theorem and get: $\lim_{n \to +\infty} \frac{n}{\sqrt{n}} = \infty$ $\lim_{n \to +\infty} \frac{n^2}{\sqrt{n}(1+\sqrt{2}+\sqrt{3}+\ldots+\sqrt{n})} = \lim_{n \to +\infty} \frac{\frac{n^2}{\sqrt{n}}}{(1+\sqrt{2}+\sqrt{3}+\ldots+\sqrt{n})} \implies Stolz = \lim_{n \to +\infty} \frac{\frac{(n+1)^2}{\sqrt{n+1}}-\frac{(n)^2 }{\sqrt{n}}}{\sqrt{n+1}}$ And that is pretty disappointing - I think that the solution is wrong. Does anybody see an error in my way of thinking? Unfortunately, I can not use integrals while doing that exercise.","['sequences-and-series', 'analysis', 'real-analysis']"
3918993,When does convolution create turning points?,"Suppose $f$ and $k$ are Lebesgue measurable functions $\mathbf R\to\mathbf R$ and satisfy $k(x)\geq 0$ for all $x$ , and $\int_{\mathbf R} k(x)\,dx=1$ . Also suppose $f$ has bounded variation, and $f(-\infty)=0$ and $f(\infty)=1$ . Let $$(k*f)(x) = \int_{-\infty}^{\infty}k(x-y)f(y)dy$$ denote the convolution of $k$ and $f$ . Let $TP(f)$ denote the number of turning points of a measurable function. We can define turning points like follows: let $P=\{a=x_0,x_1,\dots,x_n=b\}$ be a partition of $\mathbf R$ . Then we can define $$ D(f; P) = \left\{ f(x_1)-f(x_0),\dots, f(x_n)-f(x_{n-1}) \right\}$$ $$ TP(f) = \sup_P (\# \text{sign changes of }D(f; P))$$ My question: When does $k*f$ have more turning points than $f$ ? Equivalently, under what conditions will $k*f$ have at most as many turnings points as $f$ ? Since $f$ has bounded variation we know that $TP(f)$ is at most countably infinite. But consider an example like the Gaussian kernel, especially in the context of heat flow. It is intuitively obvious that heat flow will not create new turning points. I cannot seem to prove this in the language of functional analysis. It is easy to show $(k*f)(-\infty)=0$ and $(k*f)(\infty)=1$ , and also that the variation of $k*f$ is not greater than the variation of $f$ . ALso, if $f$ has no turning points then neither does $k*f$ , because if $f$ is monotone then the variation of $f$ is $1$ , and if $k*f$ has a turning point then its variation is strictly greater than $1$ , a contradiction.","['convolution', 'functional-analysis', 'analysis']"
3919091,"Prove function $f:[a, b] \rightarrow \mathbb{R}$ is Riemann integrable if and only if $f$ is bounded and almost everywhere continuous.","I wonder how can we prove the backward direction (function $f:[a, b] \rightarrow \mathbb{R}$ is Riemann integrable, then $f$ is bounded and almost everywhere continuous.) without using the measure theory. My attempt: If $f\in R(\alpha)$ on [a,b], there exists a partition $U(P,f,\alpha)-L(P,f,\alpha)<\epsilon$ . Consider an arbitrary partition $P: {a=x_0<x_1<\ldots<x_n=b}$ . By theorem 6.7(Rudin), If $s,t$ are arbitrary points in $[x_{i-1},x_i]$ , then $\sum_{i=1}^n|f(s)-f(t)|\Delta\alpha_i<\epsilon$ . Since $U(P,f,\alpha)-L(P,f,\alpha)=\sum_{i=1}^n(M_i-m_i)\Delta\alpha_i<\epsilon$ by definition and $f$ is bounded, $M_i-m_i\leq\eta$ where $\eta>0$ . Thus, given $\eta > 0,\exists \delta>0$ s.t. when $|s-t|<\delta$ , $|f(s)-f(t)|<\eta.$ Thus, $f$ is continuous on each $[x_{i-1},x_i]$ . I felt the part "" $M_i-m_i\leq\epsilon$ where $\epsilon>0$ . Thus, given $\epsilon > 0,\exists \delta>0$ "" weird and not sure if what I'm doing is correct. Any help appreciated! Edit: -------------------------- The original problem is : Let $a_{1}, a_{2}, \ldots$ be a strictly increasing sequence in $(a, b],$ and let $p_{n}>0$ be such that $\sum_{n=1}^{\infty} p_{n}=1$ Define $\alpha:[a, b] \rightarrow \mathbb{R}$ as follows: $\alpha(x)=0$ if $a \leq x<a_{1}$ $$
\alpha(x)=\sum_{k=1}^{n} p_{k} \text { if } a_{n} \leq x<a_{n+1}
$$ and $\alpha(x)=1$ if $\sup _{n} a_{n} \leq x \leq b$ I need to Show that a bounded function $f:[a, b] \rightarrow \mathbb{R}$ is Riemann-Stieltjes integrable with respect to $\alpha$ if and only if $f\left(a_{n}^{-}\right)=\lim _{x \rightarrow a_{n}^{-}} f(x)$ exists and $f\left(a_{n}^{-}\right)=f\left(a_{n}\right)$ for all $n,$ and that in this case $$
\int_{a}^{b} f d \alpha=\sum_{n=1}^{\infty} f\left(a_{n}\right) p_{n}
$$ I'm stuck on proving "" $f:[a, b] \rightarrow \mathbb{R}$ is Riemann-Stieltjes integrable, then it's left continuous"" .","['integration', 'riemann-integration', 'analysis', 'real-analysis']"
3919175,Proving identity through combinatorial model,"How to prove this identity using combinatorial argument? $\displaystyle\sum_{r=0}^{n}\binom{n}{r}\binom{p}{r+s}\binom{q+r}{m+n} = \sum_{r=0}^{n}\binom{n}{r}\binom{q}{r+m}\binom{p+r}{n+s}$ Only thing I could think of was to change $\binom{n}{r}$ to $\binom{n}{n-r}$ and then we can probably think of choosing $(n-r+r+s+m+n) = (n+s+m+n)$ people or something but its dead end for me. Please help. Also it seems if I can figure one side, other might trivially follow. Please don't ignore the problem.","['summation', 'combinatorial-proofs', 'binomial-coefficients', 'combinatorics', 'discrete-mathematics']"
3919225,Boundedness in probability of Stochastic Process,"This question is related to some other similar ones I did in the recent past. Let $X_t$ be a Stochastic Process defined through the equation $$\text{d}X_t=f(X_t,t)\text{d}t+\text{d}W_t,$$ where $f$ is a twice differentiable function such that $f(x)<a<0$ for all $x>k$ , and $X_0<k$ . What I am wishing to show is: For all $\epsilon>0$ there is $M>0$ (which we can WLOG assume to be bigger than $k$ ) such that for all $t\ge0$ it holds that $$\mathbb{P}[X_t>M]\le\epsilon.$$ What I tried to do is define $\tau_t$ as $\sup\{0<s<t: X_s=k\}$ , i.e. the last hitting time of the level $k$ before time $t$ , where the sup is set equal to $0$ if the set $\{0<s<t: X_s=k\}$ is empty. This should be a stopping time, even if last hitting times in general are not. Then, we can manipulate the given probability in this way: \begin{align*}\mathbb{P}[X_t>M]&=\underbrace{\mathbb{P}[X_t>M,\tau_t=0]}_{=0}+\underbrace{\mathbb{P}[X_t>M,\tau_t\ne0, X_t<k]}_{=0}+\mathbb{P}[X_t>M,\tau_t\ne0, X_t>k]\\
&\le\mathbb{P}[X_t-X_{\tau_t}>M-k,\tau_t\ne0]\\
&\le \mathbb{P}[a(t-\tau_t)+W_t-W_{\tau_t}>M-k, \tau_t\ne0]\\
&\le \mathbb{P}[a(t-\tau_t)+W_t-W_{\tau_t}>M-k].\end{align*} Since $a<0$ we can bound this probability uniformly over $t$ , and the claim should be proven. However, I think I've been too slick with some steps, and I want to make sure they are correct (in particular, the $\tau_t$ seems to be not a stopping time). Can someone help me find any potential errors? Also, is there a more straightforward way to prove this? Maybe I just didn't see an easier solution. EDIT: the $f$ in my specific problem satisfies the hypothesis $|f(x,t)-k_0|<\theta\cdot |x|+ \mu,$ for some constants $k_0, \theta$ and $\mu$ : is there some kind of comparison principle with the absolute value bound? I can't find it online, but if it exists, then I would be done, as the Ornstein-Uhlenbeck process is bounded in the sense I am looking for.","['solution-verification', 'brownian-motion', 'probability', 'upper-lower-bounds']"
3919253,Hash function and one near hard example?,"Example: Suppose $H:$ { $1,...,n$ } $\rightarrow $ { $1,..,n$ } be a uniform hash function. for input $x$ , $z$ is equal to number of trailing zero in the right side of $H(x)$ . for $0 \leq c \leq 1$ what is the order of probability $ z \geq c \log_2 n$ ? $C$ is constant here. Answer: $O(1/n^c)$ How  this this is can be achieved? Update : The Logarithm base is $2$ not $10$ .","['hash-function', 'calculus', 'probability-theory', 'probability', 'computer-science']"
3919305,ABCD is a cyclic quadrilateral whose diagonals intersect at point P.,"$ABCD$ is a cyclic quadrilateral whose diagonals intersect at point $P$ . Let the centres of the circumcircles of $ABCD$ , $\triangle ABP$ , $\triangle BCP$ , $\triangle CDP$ , $\triangle DAP$ be $O$ , $O$ $1$ , $O$ $2$ , $O$ $3$ , $O$ $4$ in the given order. Prove that $OP$ , $O$ $1$ $O$ $3$ , $O$ $2$ $O$ $4$ intersect at one point. By connecting $O$ $1$ , $O$ $2$ , $O$ $3$ , $O$ $4$ , we can see that $O$ $1$ $O$ $2$ is perpendicular to common cord $BP$ . Similarly, $O$ $3$ $O$ $4$ is perpendicular to common cord $DP$ . Since $D$ , $P$ , and $B$ are on the same line; $O$ $1$ $O$ $2$ is parallel to $O$ $3$ $O$ $4$ . Similarly, $O$ $1$ $O$ $4$ is parallel to $O$ $2$ $O$ $3$ .
So, $O$ $1$ $O$ $2$ $O$ $3$ $O$ $4$ is a parallelogram","['contest-math', 'quadrilateral', 'circles', 'geometry']"
3919413,"Is there really no analogue of the derivative product rule for integrals, or we just haven't found one yet?","For a product rule for integrals, I am not talking about integration by parts. That particular formula uses a integral of products inside the formula itself. The derivative product rule does not use a derivative of products inside the formula. I have never seen a book that proves that there is in fact no analogous formula for the integral of a product. Some books prove that there is no formula for the roots of a general 5th degree or higher polynomial function. Can someone prove that there is no such formula for the integral of a product? There is a formula for the integral of a sum, so maybe someone will discover that there is a formula for the integral of a product. I apologize if my notion of formula is not precise enough, but maybe there is a book that precisely defines what a formula is.","['integration', 'calculus', 'closed-form', 'products']"
3919440,"Another question about ""all odd moments vanish""","[Question inspired by Example of non-degenerate random variable with odd moments = 0 ] Suppose $X$ is a real random variable such that all odd momenents vanish.
That is $\mathbb E[X^{2n+1}]=0$ for $n=0,1,2,3\cdots$ .  Does it follow
that $X$ is symmetrically distributed about $0$ ?  That is, $X$ and $-X$ have the same
distribution. Note: the case where $X$ is bounded is found here: Proof that $\mathbb{E} X^k = 0$ for all odd $k$ implies $X$ symmetric for bounded $X$ without characteristic functions","['expected-value', 'probability-distributions', 'probability-theory']"
3919459,Generalizing Topological Spaces,"I was studying topological spaces having studied metric spaces. I guess the properties of open sets in metric space led into (in terms of open set) definition of Topological spaces. Although there are many equivalent definitions such as Kuratowski closure axioms , the most used definition is in terms of open sets. I know that we can extend these topological spaces into vector spaces or any other algebraic structures by defining appropriate operations in the given topological space. I'm not aware of any generalization of topological spaces which further broadens the definition of topological spaces. For example, if we replace union and intersection by any other two operations (let's call it op1 and op2 ) such that arbitrary op1 and finite op2 are closed in the given class of subsets of a set and conventionally we can call them as open $^*$ sets(I'm calling them open $^*$ to distinguish from the open sets in the normal sense). This idea is itself is worth reflecting because if we consider op1 as intersection and op2 as union then closed sets(in the normal sense) are open $^*$ sets and open sets(in the normal sense) are closed $^*$ sets. If we consider some other operations then the properties will differ entirely. In general, it may not lead us into metric spaces or may not have pleasant properties which always is the aim of mathematicians. Does this kind of generalization of topological space exist? Did anyone work in this direction?","['general-topology', 'soft-question', 'research', 'reference-request']"
3919475,Atoms in a measure space and Sierpinski theorem for non-atomic measures,"I have two questions concerning atoms in a measure space. My first question is about two different definitions I've encountered and I'm not sure if they are equivalent. Definition 1: Given a measure space $(\Omega, \Sigma, \mu)$ we say that an element $A \in \Sigma$ is an atom of $\mu$ if it satisfies that $\mu(A) >0$ and for every $B \in \Sigma$ such that $B \subset A$ either $\mu(B)=0$ or $\mu(A \setminus B)=0$ . Definition 2: Given a measure space $(\Omega, \Sigma, \mu)$ we say that an element $A \in \Sigma$ is an atom of $\mu$ if it satisfies that $\mu(A) >0$ and for every $B \in \Sigma$ such that $B \subset A$ either $\mu(B)=0$ or $\mu(B)=\mu(A)$ . So I know that if $\mu(\Omega)$ is finite then both definitions are indeed equivalent but, are they still equivalent if the measure is not finite? Now, concerning atoms of a measure I'm trying to prove the following theorem, apparently due to Sierpinski: Theorem: If $(\Omega, \Sigma, \mu)$ is a measure space with no atoms, then for every $t \in [0, \mu(\Omega)]$ there exists an $A \in \Sigma$ such that $\mu(A)=t$ . Following Wikipedia's article on atoms , I'm trying to fully proof the sketch of the proof at the end of the page. So I want to prove there is a funciton $S:[0, \mu(\Omega)] \longrightarrow \Sigma$ satisfying: $S$ is monotone, that is, if $t \leq t'$ then $S(t) \subset S(t')$ $\mu(S(t))=t$ for every $t \in [0, \mu(\Omega)]$ because this would directly imply the theorem. To this end, we define $$\Gamma= \left\{  S:D \longrightarrow \Sigma: D \subset [0, \mu(\Omega)], S \textrm{ is monotone}, \mu(S(t))=t, \forall t \in D \right\}$$ Then $\Gamma$ is not empty beacause we can define $S_0:\{0\} \longrightarrow \Sigma$ given by $S(0)=\emptyset$ and is clearly in $\Gamma$ , and we can make it a partially ordered set by establishing that $S_1 \leq S_2$ if, and only if, $\operatorname{graph}(S_1) \subset \operatorname{graph}(S_2)$ . Now, if $\Gamma_c$ is a chain in $\Gamma$ then we define $S_c: D_c \longrightarrow \Sigma$ where $$D_c=\bigcup_{S \in \Gamma_c} \operatorname{dom}(S)$$ and given $t \in D_c$ , we choose any $S \in \Gamma_c$ such that $t \in \operatorname{dom}(S)$ , and define $S_c(t)=S(t)$ . First, $S_c$ is well defined because if we have that given $t \in D_c$ , there are $S_1, S_2 \in \Gamma_c$ such that $t \in \operatorname{dom}(S_1) \cap \operatorname{dom}(S_2)$ , then as $\Gamma_c$ is a chain, we can assume wlog that $S_1 \leq S_2$ and then, $(t,S_1(t)) \in \operatorname{graph}(S_2)$ so $S_2(t)=S_1(t)$ as we wanted to show. Now, we will prove that $S_c \in \Gamma$ . First, observe that given $t_1, t_2 \in D_c$ there exists $S_1, S_2 \in \Gamma_c$ such that $t_i \in \operatorname{dom}(S_i)$ , so as $\Gamma_c$ is a chain, we may assume $S_1 \leq S_2$ and so $\operatorname{dom}(S_1) \subset \operatorname{dom}(S_2)$ , getting that $t_1, t_2 \in \operatorname{dom}(S_2)$ . So, if $t_1 \leq t_2$ as $S_2 \in \Gamma$ we will have that $$S_c(t_1)=S_2(t_1) \subset S_2(t_2) = S_c (t_2$$ so $S_c$ is monotone. On the other hand, it is clear that given $t \in D_c$ if it is $S \in \Gamma_c$ such that $t \in \operatorname{dom}(S)$ , then $$\mu(S_c(t))=\mu(S(t))=t$$ So we have proven that $S_c \in \Gamma$ and it is clear by its construction that it is an upper bound of $\Gamma_c$ . By Zorn's lemma, there exits then some $S: D \longrightarrow \Sigma$ maximal in $\Gamma$ , and the claim is that this function is the one we are looking for, and it suffices to prove that $D=[0, \mu(\Omega)]$ . This last part is the one I cannot prove, and if everything I've done is correct I suppose here is where you should use that $\mu$ is non-atomic. If tried to prove that $D$ is closed and open in $[0, \mu(\Omega)]$ , and as it is conected, we'll have that $D=[0, \mu(\Omega)]$ as we want. First, I've shown by using the maximality of $S$ that $\{0, \mu(\Omega)\} \subset D$ , for example if we suppose $0 \not \in D$ (the other case is analogue), then defining $\bar S: D \cup \{0\} \longrightarrow \Sigma$ as $\bar S(t)=S(t)$ if $t \in D$ and $\bar S(0)=\emptyset$ , it is clear that $\bar S \in \Gamma$ and $S < \bar S$ contradiction with its maximality. Now, to show it is closed, I've taken a sequence $\{t_n\} \subset D$ converging to some $t_0 \in [0, \mu(\Omega)]$ and I've supposed that $t \not \in D$ . Then, we have that $t_0 \in (0, \mu(\Omega))$ so in particular is finite, and we know then there must exists a monotone subsequence of $\{t_n\}$ converging to $t_0$ , so we can suppose that $\{t_n\}$ is monotone. If it is increasing, defining $A_n=S(t_n)$ and $\bar S: D \cup \{t_0\} \longrightarrow \Sigma$ as $\bar S(t)= S(t)$ if $t \in D$ and $\bar S(t_0)= \cup A_n$ we have: If $t <t_0$ then there exists some natural number $n$ such that $t \leq t_n$ and so $$\bar S(t)=s(t) \subset S(t_n) = A_n \subset \bar S(t_0)$$ If $t_0 < t$ then $t_n < t$ for every $n \geq 1$ so $$A_n=S(t_n) \subset S(t), \forall n \geq 1$$ and then $$\bar S(t_0) \subset S(t)$$ Since $\{t_n\}$ is increasing, by the monotonity of $S$ , $\{S(t_n)\}$ is also increasing and we then have $$\mu\left(\bar S(t_0)\right)=\mu\left( \bigcup_{n=1}^\infty S(t_n) \right)=\lim_{n \rightarrow \infty} \mu\left( S(t_n)\right) = \lim_{n \rightarrow \infty} t_n = t_0$$ So, this three facts imply that $\bar S \in \Gamma$ but $S < \bar S$ so we have a contradiction with the maximality. Now, if $\{t_n\}$ is decreasing, we do the same thing but taking $\bar S(t_0)=\cap A_n$ and, as $t$ is finite, we can assume that $\mu(A_1)=\mu(S(t_1))=t_1$ is finite, so we can argue as before arriving at the same contradiction. If everything I've write here is correct, then to show $D$ is open I must use the fact that $\mu$ is non-atomic because I haven't used it yet, so my doubt concerging this part are: If the definitions I gave at the beggining are not equivalent, can this last part be shown with both definitions? In case it is two difficult to show $D$ is open, is there a simple proof that $D=[0,\mu(\Omega)]$ ? And, in that case, can it be shown with both definitions of atoms if they are not equivalent?",['measure-theory']
3919510,"Homogenous PDE, changing of variable","I have an PDE $\dfrac{df}{d \xi}-\xi\dfrac{d x_1}{d \xi}=0$ homogeneous for $\xi$ , where $f:\mathbb{R}^n\to\mathbb{R}$ is function of $x_1,\cdots,x_n:\mathbb{R}\to\mathbb{R}$ , which in turn are functions of $\xi$ (so, $\dfrac{df}{d \xi}$ is a total derivative). I also have $\xi=y/z\in\mathbb{R}$ , where $y,z\in\mathbb{R}$ , and one said that once the PDE is homogeneous in $\xi$ I have $\dfrac{df}{d y}-\xi\dfrac{d x_1}{d y}=0$ . I missed this gap. I imagine that I can multiply the first ODE by $\dfrac{d\xi}{d y}=\dfrac{1}{z}$ , so $$\dfrac{df}{d \xi}\dfrac{d\xi}{d y}-\xi\dfrac{d x_1}{d \xi}\dfrac{d\xi}{d y}=\sum_{i=1}^n \dfrac{\partial f}{\partial x_i}\dfrac{d x_i}{d\xi}\dfrac{d\xi}{d y}-\xi\dfrac{d x_1}{d y}=\sum_{i=1}^n \dfrac{\partial f}{\partial x_i}\dfrac{d x_i}{d y}-\xi\dfrac{d x_1}{d y}=\dfrac{d f}{d y}-\xi\dfrac{d x_1}{d y}=0.$$ But I am not sure. Indeed, I'd like to know how state this only from the fact of homogeneous. Thanks so much. Related: Equality between two total derivatives","['ordinary-differential-equations', 'multivariable-calculus', 'functional-analysis', 'partial-differential-equations', 'homogeneous-equation']"
3919562,Why is Banach-Alaoglu theorem so important?,"According to Lawrence Narici and Edward Beckenstein, the Alaoglu
theorem is a ""very important result - maybe the most important fact
about the weak-* topology - [that] echos throughout functional
analysis."" (Source: Wikipedia) It is a well-known fact (by Riesz) that the compactness of the unit ball with respect to the norm topology characterizes finite dimensional vector spaces. In a infinite dimensional setting, Banach-Alaoglu recovers the compactness of the unit ball in the weak*-topology which seems to come to relief of a lot of analyst.
I have come across a thread recently about the importance of Hahn-Banach which I found very illuminating. I would be interested about the different takes people have on Banach-Alaoglu.
Why is it so important? What if we were not to recover the compactness of the unit ball?",['functional-analysis']
3919565,Misconception about evaluating limits,"Question Find the limit of $$\lim\limits_{x\to\infty}\ \left[\frac 1 3 (3^{\frac 1 x} + 8^{\frac 1 x} + 9^{\frac 1 x})\right]^x\ .$$ My working I know that the limit is $6$ , which can be found through exponentiation. However, I would like to know why the following method (which I tried initially) gives the wrong answer of $1$ . So, I thought that, since $x \rightarrow \infty\ $ , $\frac 1 x \rightarrow 0\ $ . Thus, $\lim\limits_{x\to\infty}\ \left[\frac 1 3 (3^{\frac 1 x} + 8^{\frac 1 x} + 9^{\frac 1 x})\right]^x = [\frac 1 3 (3^0 + 8^0 + 9^0)]^{\infty} = 1^\infty = 1\ $ . Any explanations as to why my original method is incorrect will be greatly appreciated :) Edit Thanks to some helpful comments/answers, I now know that $1^\infty$ is actually not $1$ but indeterminate!","['limits', 'calculus', 'real-analysis']"
3919713,Expectation of sampling integers from a reciprocal distribution without replacement,"I have a reciprocal distribution with PDF $$\frac{1}{x\ ln{N}}$$ I sample $k$ integers from this distribution in the range $[1,N]$ without replacement.
I need to determine the average (expected) number of each integer in $[1,N]$ in my sample. I came across the multivariate Wallenius' noncentral hypergeometric distribution, which deals with sampling weighted colours of ball from an urn without replacement in sequence. The distribution describes the expected number of each colour $i$ as $\mu_i$ in a vector $\mathbf\mu$ , which can be found by solving the following system of non-linear equations $$\left(1-\frac{\mu_1}{m_1}\right)^\frac{1}{\omega_1}
= \left(1-\frac{\mu_2}{m_2}\right)^\frac{1}{\omega_2}
= \cdots
= \left(1-\frac{\mu_c}{m_c}\right)^\frac{1}{\omega_c}$$ $$\sum_{i=1}^c \mu_i=n$$ For my use case, $c=N$ , $m_i=1$ , $n=k$ and $\omega_i=\frac{1}{i\ lnN}$ , so the equations become $$\left(1-\mu_1\right)^{lnN}
= \left(1-\mu_2\right)^{2\ lnN}
= \cdots
= \left(1-\mu_N\right)^{N\ lnN} \tag{1}$$ $$\sum_{i=1}^N \mu_i=k \tag{2}$$ The more general Wallenius' mean is normally approximated through e.g. Newton-Raphson, so I'm hoping that the narrowing of the equations makes them directly solvable.
My work so far is as follows: We can rewrite $(1)$ to put $\mu_i$ in terms of $\mu_j$ $$(1-\mu_i)^{i\ lnN}=(1-\mu_j)^{j\ lnN}$$ Using the identity $a^b=e^{b\ lna}$ $$e^{i\ lnN\ ln(1-\mu_i)}=e^{j\ lnN\ ln(1-\mu_j)}$$ $$i\ lnN\ ln(1-\mu_i)=j\ lnN\ ln(1-\mu_j)$$ $$ln(1-\mu_i)=\frac{j}{i}\ ln(1-\mu_j),\ \ \ N>1$$ $$\mu_i=1-(1-\mu_j)^{j/i}\tag{3}$$ We can then repeatedly substitute $(3)$ into the summation $(2)$ to obtain a formula only in terms of $\mu_i$ $$
\begin{aligned}
k&=\mu_1+\mu_2+\cdots+\mu_N\\
&=(1-(1-\mu_i)^{i/1})+(1-(1-\mu_i)^{i/2})+\cdots+(1-(1-\mu_i)^{i/N})\\
&=N-\sum_{j=1}^N (1-\mu_i)^{i/j}
\end{aligned}
$$ Therefore $$N-k=\sum_{j=1}^N (1-\mu_i)^{i/j}$$ However, I do not know how to proceed. Can this be rearranged for $\mu_i$ ? Am I overcomplicating things?","['systems-of-equations', 'statistics', 'probability-distributions', 'sampling']"
3919769,Rationalizing General Denominators,"I've read on several answers how to rationalize a fraction of the form $$
\frac{1}{\sqrt{a_{1}} + \dots +\sqrt{a_{n}}}
$$ by multiplying top and bottom by the numbers $$\varepsilon_{1}\sqrt{a_{1}} + \dots +\varepsilon_{n}\sqrt{a_{n}},$$ where $\varepsilon_{i} \in \left\{1,-1\right\}$ , avoiding the current denominator, that is, avoiding the number that corresponds to $\varepsilon_{i} = 1$ , for all $i$ . In fact, it is possible to do it restricting ourselves to the family $$\sqrt{a_{1}} + \varepsilon_{2}\sqrt{a_{2}} + \dots +\varepsilon_{n}\sqrt{a_{n}},$$ that is, ignoring the terms where $\varepsilon_{1} = -1$ . My questions are the following ones: Is there a way of doing it involving less products (for n large)? Can somebody give me a book or reference that deals with the more general case $$
\frac{1}{\sqrt[m]{a_{1}} + \dots +\sqrt[m]{a_{n}}}
$$ or, more generally, $$
\frac{1}{\sqrt[m_{1}]{a_{1}} + \dots +\sqrt[m_{n}]{a_{n}}},
$$ including an explicit algorithm and (if possible) a proof of its correctness? Thank you in advance!","['radicals', 'galois-theory', 'algorithms', 'extension-field', 'algebra-precalculus']"
3919812,Riemann integral in two variables,"Let $f:[0,1]\times[0,1]\to \mathbb{R}$ , be defined by: \begin{equation}
    f(x,y)=\begin{cases}
0 & \text{ if  }\; 0\leq x<\frac{1}{2},\\ 
1 & \text{ if  }\; \frac{1}{2}\leq x \leq 1. 
\end{cases}
\end{equation} Show that $f$ is integrable and $\displaystyle\int_{[0,1]\times [0,1]}f=\frac{1}{2}$ . Attempt. Let $P$ a partition of $[0,1]\times[0,1]$ , such that: $\mathcal{P}=(\mathcal{S}_{1},\mathcal{S}_{2})$ , where : \begin{align}
    \begin{split}
        \mathcal{S}_{1}&=\left[0,\frac{1}{2}\right]\times \left[0,1\right]\\
        \mathcal{S}_{2}&=\left[\frac{1}{2},1\right]\times \left[0,1\right]\\
    \end{split}
\end{align} Define \begin{align}
   \begin{split}
     m_{\mathcal{S}_{1}}(f)&=\inf_{x\in \mathcal{S}_{1}}f(x)=0\\
   m_{\mathcal{S}_{2}}(f)&=\inf_{x\in \mathcal{S}_{2}}f(x)=1\\   
   \end{split}
\end{align} and \begin{align}
      \begin{split}
   M_{\mathcal{S}_{1}}(f)&=\sup_{x\in \mathcal{S}_{1}}f(x)=0\\
   M_{\mathcal{S}_{2}}(f)&=\sup_{x\in \mathcal{S}_{2}}f(x)=1\\
      \end{split}
  \end{align} The lower and upper sums: \begin{align*}
    L(f,\mathcal{P})& =\sum_{\mathcal{S}_{1}}m_{\mathcal{S}_{1}}(f).\upsilon(\mathcal{S}_{1})+\sum_{\mathcal{S}_{2}}m_{\mathcal{S}_{2}}(f).\upsilon(\mathcal{S}_{2})\\
    &=\sum_{\mathcal{S}_{1}}0\cdot\frac{1}{2}+\sum_{\mathcal{S}_{2}}\frac{1}{2}\\
    &=\frac{1}{2}
\end{align*} and \begin{align*}
    U(f,\mathcal{P})& =\sum_{\mathcal{S}_{1}}M_{\mathcal{S}_{1}}(f).\upsilon(\mathcal{S}_{1})+\sum_{\mathcal{S}_{2}}M_{\mathcal{S}_{2}}(f).\upsilon(\mathcal{S}_{2})\\
    &=\sum_{\mathcal{S}_{1}}0\cdot\frac{1}{2}+\sum_{\mathcal{S}_{2}}\frac{1}{2}\\
    &=\frac{1}{2}
\end{align*} Then \begin{align*}
\sup{L(f,\mathcal{P})}=\frac{1}{2}=\inf{U(f,\mathcal{P})}.
\end{align*} Hence $f$ is integrable on $[0,1]\times [0,1]$ and $\displaystyle\int_{[0,1]\times [0,1]}f=\frac{1}{2}$ . I have doubts about my solution since my instructor told me that the conclusion is not correct since I calculate the lowest of the upper sums on a particular partition, and by definition of the upper integral it must be calculated on the set of all partitions.
He suggested using the definition of infimum $ \varepsilon $ . I appreciate if someone could help me or give an indication.","['alternative-proof', 'multivariable-calculus', 'vector-analysis', 'real-analysis']"
3919827,Positive elements in Hermitian * Banach Algebras,"Let $A$ be a commutative Hermitian * Banach algebra, that is, a commutative Banach algebra with involution such that every self-adjoint element has real spectrum. It is known that if $x\in A$ , then $y := x^\ast x \geq 0$ , that is, $y^\ast = y$ and $\sigma(y)\subset[0,\infty)$ , see ""Bonsall and Duncan, Complete normed algebras, 41.Th 5. My question is the following: let $y\in A$ be a positive element as before. Can we assume the existence of $x\in A$ such that $y = x^\ast x$ ? I'm particularly interested in the case $A = L^1 (\mathbb{R})$ , the set of integrable functions over the real line, with convolution as multiplication. Note that in this case, $A$ is a $A^\star$ -Banach algebra, that is, it is continuously embedded in a $C^\star$ -algebra. It is well known that the answer to this question is affirmative if $A$ is a $C^\star$ -algebra, but I haven't been unable to find its answer to this more general case. Edit: It seems that the answer to this question is positive if we add the condition that $0 \notin \sigma(y)$ , i.e. $\sigma(y) \subset (0,\infty)$ , see Theorem 11.20 in ""W. Rudin, Functional analysis"". However, I am still interested in the case $0 \in \sigma(y)$ , so any help will be well received.","['c-star-algebras', 'operator-theory', 'functional-analysis', 'banach-algebras']"
3919896,Integrating factor for the ODE $(3x+2y+y^2)dx + (x+4xy+5y^2)dy = 0$,"I was asked to find an integrating factor $\mu = \mu(x+y^2)$ for the ODE $$(3x+2y+y^2)dx + (x+4xy+5y^2)dy = 0.$$ So the natural approach was define $P = 3x+2y+y^2$ and $Q = x+4xy+5y^2.$ Then, $$\partial_yP = 2+2y,~~~\partial_xQ = 1+4y$$ and hence $\partial_yP - \partial_xQ = 1-2y.$ Let us consider then $$\frac{\partial_yP-\partial_xQ}{Q} = \frac{1-2y}{x+4xy+5y^2}$$ and define $z = x+y^2.$ Thus, $$\frac{\partial_yP - \partial_xQ}{Q} = \frac{1-2y}{(z-y^2)(1+4y)+5y^2}.$$ But from here I really don't know how to proceed. Any hint?","['calculus', 'ordinary-differential-equations']"
3919951,"Exhausting integral $ \int_{0}^{\infty}\frac{\ln^3(x)}{(x-1)^3} \, \mathrm{d}x $","I would like to evaluate following integral: $$ \int\limits_{0}^{\infty}\frac{\ln^3(x)}{(x-1)^3} \, \mathrm{d}x $$ by complex means. I integrate $$f(z)=\frac{\ln^4(z)}{(z-1)^3}$$ (where ln is analytic in $\mathbb{C} \setminus \mathbb{R_+}$ with $\ln(-1)=i\pi$ ) over following contour: $[0;R]$ , $C_0(R)$ - circle of radius $R$ , $[R, 1+\varepsilon]$ , semicircle in lower half-plane $C_1(\varepsilon)$ and $[1-\varepsilon,0]$ . After evaluation and taking $\lim\limits_{\varepsilon \to 0}$ I came to correct answer, but my question is: Is it possible to avoid that exhausting evaluation? Thanks in advance.","['integration', 'complex-analysis']"
3919984,vector identities in solving wave equations with different speeds of propagation,"Assume that $u = (u^1, u^2, u^3)$ solves the evolution equations of linear elasticity: $$u_{tt}-µ \Delta u − (λ + µ) D (\nabla\cdot u) = 0$$ in $\mathbf{R}^3 × (0, ∞)$ .
Show that $w := \nabla \cdot u $ and $w := \nabla \times u$ each solve wave equations, but with differing speeds of propagation. This is problem 21 in chapter 2 of Evan's PDE. I am able to do this problem when $ w := \nabla \times u$ to obtain $w_{tt} = \mu \Delta w$ . For $ w:= \nabla \cdot u$ , I am not recognizing how to proceed from $$ w_{tt} = \mu \Delta w + (\lambda + \mu) \nabla (\nabla \cdot w) $$ to $$w_{tt} = \mu (\Delta w) + (\lambda + \mu)(\Delta w)$$ Since $ w = \nabla \cdot u$ is a scalar, I am not sure how the divergence of $w$ is defined here. The identity $\Delta w = \nabla(\nabla \cdot w) - \nabla \times \nabla \times w $ would be useful here but I can't see why the curl of curl of $w$ would be zero in this case or even defined when $w$ is a scalar. Maybe I am misunderstanding something in the statement of the problem? Any help will be appreciated.","['divergence-operator', 'vector-fields', 'curl', 'multivariable-calculus', 'partial-differential-equations']"
3920002,Does $\lim\limits_{n\to+\infty}|\sin{2^n}|^{\frac1n}$ exist?,"More generally, for what real number x does $\displaystyle\lim_{n\to+\infty}|\sin2^nx|^\frac1n$ exist? Actually it's already known that if you replace the $2^n$ with $n$ , then the answer is 1. That is, $$\lim\limits_{n\to+\infty}|\sin n|^\frac1n=1$$ . But when it seems that I could not get the same information about how $\frac{2^n} m$ goes to $\pi$ . To answer this question, one has to consider two things: $\liminf\limits_{n\to+\infty}|2^n-m\pi|$ $=$ ? If the answer to 1) is $0$ , then how fast does the infimum actually come to $0$ ?","['number-theory', 'real-analysis']"
3920005,Is $\mathrm{U}(\mathfrak{a} \oplus \mathfrak{b}) \cong \mathrm{U}(\mathfrak{a}) \otimes \mathrm{U}(\mathfrak{b})$ over a commutative ring?,"Let $\mathbb{k}$ be a commutative ring and let $\mathfrak{g}$ be a Lie-Algebra over $\mathbb{k}$ . Suppose that $\mathfrak{a}$ and $\mathfrak{b}$ are two Lie subalgebras of $\mathfrak{g}$ such that $\mathfrak{g} = \mathfrak{a} \oplus \mathfrak{b}$ as $\mathbb{k}$ -modules. We then have a homomorphism of $\operatorname{U}(\mathfrak{a})$ - $\operatorname{U}(\mathfrak{b})$ -bimodules $$
  \Phi
  \colon
  \operatorname{U}(\mathfrak{a}) \otimes_{\mathbb{k}} \operatorname{U}(\mathfrak{b})
  \to
  \operatorname{U}(\mathfrak{g}) \,,
  \quad
  x \otimes y
  \mapsto
  xy \,.
$$ Question. Is the homomorphism $\Phi$ an isomorphism of bimodules? My thoughts so far: If both $\mathfrak{a}$ and $\mathfrak{b}$ are free as $\mathbb{k}$ -modules (e.g. if $\mathbb{k}$ is a field), then $\mathfrak{g}$ is also free as a $\mathbb{k}$ -module.
One can then use the PBW-theorem to see that $\Phi$ is a bijection on the induced bases.
It is then an isomorphism of $\mathbb{k}$ -modules, and thus an isomorphism of bimodules. If $\mathfrak{a}$ and $\mathfrak{b}$ is are ideals of $\mathfrak{g}$ , then the decomposition $\mathfrak{g} = \mathfrak{a} \oplus \mathfrak{b}$ is one of Lie algebras.
One can then see that both sides of $\Phi$ satisfy the same universal property as $\mathbb{k}$ -algebras, and that the above map $\Phi$ is even an isomorphism of $\mathbb{k}$ -algebras. I think that $\Phi$ is in general still surjective.
One should still be able to get module generating sets by PBW-monomials for the universal enveloping algebras, and then see that $\Phi$ is surjective on the induced generators.
But I’m not sure what happens with the injectivity of $\Phi$ .","['abstract-algebra', 'lie-algebras']"
3920078,Set of all functions $f:\mathbb R\to \mathbb R$ that are non-zero only finitely many times,"I am looking for cardinality of set $S=\{f:\mathbb R\to \mathbb R\mid f(x)\not=0, \text{only in finitely many $x \in \mathbb R$}\}$ . My hunch tells me it is only uncountable. Obviously $F:\mathbb R\to S$ given by: $$F(x)(t)=f_x(t)=\begin{cases}x,&t=1,\\0, & t \not = 1\end{cases}$$ is a well defined injection. For other injection, I thought of mappings such as: $$f \mapsto\left(\sum_{f(x)\not = 0}e^x,\sum_{f(x)\not = 0}e^{|x|},\sum_{f(x)\not =0}e^{-x},\sum_{f(x)\not = 0}f(x),\prod_{f(x)\not = 0}f(x)\right)$$ but I am not sure if $f$ is an injection or not. Any hint would be appreciated.",['elementary-set-theory']
3920080,Fundamental group of the torus from hexagon with opposite sides identified,"The hexagon with opposite sides identified is the topological torus, see here and here . This would suggest to me that the fundamental group of the torus could be written as $\langle x,y,z|xyzx^{−1}y^{−1}z^{−1}\rangle$ , but apparently this is not true (as mentioned here , for example). I don't understand why not. Can the fundamental group of the torus be found starting from the hexagon with opposite sides identified?","['group-presentation', 'group-theory', 'fundamental-groups', 'algebraic-topology']"
3920089,Show that $F^4=I$ if $F$ is the Fourier Transform.,"Let the Fourier Transfor $F:L_2(\mathbb{R})\to L_2(\mathbb{R})$ defined by $\displaystyle Fg(x)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}g(t)e^{ixt}dt\,\,$ for $\,\,g\in L^2(\mathbb{R})$ . I need to show that $F^4=I$ . Can you give me some suggestions to try this please I know that $\displaystyle F^{-1}h(x)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}h(t)e^{-ixt}dt\,\,$ but
but I'm not sure if that could be used to prove that $F^4=I$ .","['fourier-transform', 'functional-analysis']"
3920125,"n married couples, are randomly seated at a round table. Find probability for k couples to sit together.","A total of 2n people, consisting of n married couples, are randomly seated at a round table. Find the probability for k of the couples (k<n) to be seated together. Let us first find the probability for 1 of the couples to be seated together.
Once the 2 persons are seated somewhere, the remaining $2n-2$ can be seated in $(2n-2)!$ ways, plus that the 2 members of the couple can be also seated in $2!$ ways. On the other hand, 2n people in a circular arrangement can be seated in $(2n-1)!$ ways, therefore the probability is $\frac {2!(2n-2)!}{(2n-1)!} = \frac{2}{(2n-1)}$ . Now let's consider a second couple sitting together: Once the 1st couple has seated, we have $2n-2$ free seats, the 1st couple (which we consider as one unit) and a 2nd couple (also as one unit), so $2n-4+1$ can be arranged in $(2n-3)!$ ways, and the 2 couples can also be arranged in $2!.2!$ ways. So the probability for 2 couples to sit together (somewhere in the table) is $\frac{2!.2!.(2n-3)!}{(2n-1)!}$ . For the $k_{th}$ couple, the probability is $\frac{2!.2!.2!...2!.k!.(2n-2k-1)!}{(2n-1)!}$ . Is it correct?
I know this is a very common problem but haven't found this variation (with k couples) anywhere.
Can you help me? Thank you!",['combinatorics']
3920142,Is this a valid way of deriving the area of a circle?,"On the Wikipedia article about deriving the area of a circle, it mentions that the formula $$
\text{area} = \pi r^2
$$ can be derived by evaluating the integral $$
2 \int_{-r}^{r} \sqrt{r^2-x^2} \, dx \, .
$$ However, the article also seems to suggest that this can result in a circular argument if it is not done carefully: This particular proof may appear to beg the question, if the sine and
cosine functions involved in the trigonometric substitution are
regarded as being defined in relation to circles. However, as noted
earlier, it is possible to define sine, cosine, and $\pi$ in a way that is
totally independent of trigonometry, in which case the proof is valid
by the change of variables formula and Fubini's theorem, assuming the
basic properties of sine and cosine (which can also be proved without
assuming anything about their relation to circles). My question is: can the following argument be used to prove that the area of a circle is $\pi r^2$ ? Let $$
I=\int \sqrt{r^2-x^2} \, dx = \int \sqrt{r^2\left(1-\left(\frac{x}{r}\right)^2\right)} \, dx \, = r\int \sqrt{1-\left(\frac{x}{r}\right)^2} \, dx .
$$ Then let $\theta = \arcsin(x/r)$ so that $x=r\sin\theta$ . Since $dx = r\cos\theta \, d\theta$ , the integral is transformed to $$
r^2\int\sqrt{1-\sin^2\theta}\, \cos \theta  \, d\theta = r^2\int \cos^2\theta \, d\theta \, .
$$ Finally, we may use the identity $$
\cos^2\theta \equiv \frac{\cos(2\theta)+1}{2}
$$ to obtain $$
r^2 \int \frac{\cos(2\theta)+1}{2} \, d\theta = r^2 \int \frac{\cos(2\theta)+1}{2} \, d\theta = r^2\left(\frac{\sin(2\theta)}{4}+\frac{\theta}{2}\right) \, .
$$ Note that when $x=r$ , $\theta = \arcsin(1) = \pi/2$ ; when $x=-r$ , $\theta = - \pi/2$ . So the area of a circle is $$
2 \int_{-r}^{r} \sqrt{r^2-x^2} \, dx \, = 2 \left[r^2\left(\frac{\sin(2\theta)}{4}+\frac{\theta}{2}\right)\right]_{-\pi/2}^{\pi/2} = \pi r^2 \, .
$$ Admittedly, this was not the most elegant of proofs, but I'm wondering if it is mathematically rigorous.","['integration', 'area', 'definite-integrals', 'circles', 'calculus']"
3920189,Prove or Disprove an Inequality Raised to the Fifth (UPDATED... Again),"I found this interesting problem: Given two sequences of whole numbers, $a_1, a_2, a_3 \cdots a_n$ and $b_1, b_2, b_3 \cdots b_n$ with the property that: for all $i$ , $a_i + b_i = n-1$ and $\sum_{i=1}^n a_i = \sum_{i=1}^n b_i,$ let $f(x) = \sum_{i=1}^n a_i^x$ and $g(x) = \sum_{i=1}^n b_i^x$ . Given that $f(4) \ge g(4)$ prove or disprove that $f(5) \ge g(5).$ So far, I tried using the info that $f(4) \ge g(4)$ to get: $$\sum_{i=1}^n a_i^4 - b_i^4 = (a_i - b_i)(a_i + b_i)(a_i^2 + b_i^2) \ge 0.$$ Since $a_i + b_i = n-1$ ,we can factor it out and divide by $n-1$ to obtain: $$\sum_{i=1}^n (a_i - b_i)(a_i^2 + b_i^2) \ge 0.$$ Then, letting $a_i^2 + b_i^2 = (a_i+b_i)^2 - 2a_ib_i = (n-1)^2 - 2a_ib_i$ , I got: $$(n-1)^2 \sum_{i=1}^n (a_i - b_i) - 2\sum_{i=1}^n a_ib_i(a_i - b_i) \ge 0.$$ Since $\sum_{i=1}^n a_i = \sum_{i=1}^n b_i, \sum_{i=1}^n (a_i - b_i) = 0.$ That leaves us with: $$\sum_{i=1}^n a_ib_i(a_i - b_i) \le 0.$$ Then, I did something similar with the $f(5) \ge g(5)$ condition. Ultimately, after lots of factoring and manipulation, I am basically left to prove/disprove that: $$\sum_{i=1}^n (a_i - b_i)a_i^2b_i^2 \ge 3(n-1)^2 \sum_{i=1}^n (a_i - b_i)a_ib_i.$$ Bringing everything over to one side and factoring yields: $$\sum_{i=1}^n (a_i - b_i)a_ib_i (a_ib_i -3(n-1)^2) \ge 0.$$ Anyone know how to continue","['algebra-precalculus', 'inequality']"
3920198,Probability problem from CMU contest prep course,"Your sports team is playing a best-of-15 series against a single opponent. Against this opponent, your
team wins with probability 40%, unless it is behind in the series (with strictly fewer wins than losses
so far in these 15-game finals), at which point your team wins with probability 60%. What is the
probability that your team wins? I'm not great at math so I wrote a dynamic programming algorithm to solve it. If my algorithm is correct, the answer would be 40%. I found that if the number of matches is odd, the probability that you win is always 40%, for any number of matches. Can anyone verify this and provide a probabilistic explanation of the solution? Thanks!","['contest-math', 'conditional-probability', 'discrete-mathematics', 'probability']"
3920233,Algebraic Method,"Is there a way to solve the following equation algebraically? $(x+3/5)^{5x-3}=16$ So far, I have figured out a solution to be 7/5. This was solved by comparing the exponential form of 16 but I am not convinced this is the right approach. There is also another solution (negative).","['algebra-precalculus', 'problem-solving']"
3920235,On the definition of a tensor in the study of manifolds,"In the book An introduction to manifolds by Tu, Loring W, it defines the $k$ -tensor as follows Denote by $V^{k}=V \times \cdots \times V$ the Cartesian product of $k$ copies of a real vector space $V $ .  A function $f: V^{k} \rightarrow \mathbb{R}$ is $ k  $ -linear if it is linear in each of its $ k $ arguments: $$
f(\ldots, a v+b w, \ldots)=a f(\ldots, v, \ldots)+b f(\ldots, w, \ldots)
$$ for all $ a, b \in \mathbb{R} $ and $ v, w \in V $ .  Instead of $2$ -linear and $3 $ -linear, it is customary to say ""bilinear"" and ""trilinear."" A $k $ -linear function on $V$ is also called a $ k  $ -tensor on $ V $ . We will denote the vector space of all $ k  $ -tensors on $ V $ by $ L_{k}(V) $ .  If $f $ is a $ k $ -tensor on $ V $ , we also call $ k $ the degree of $ f $ . However in Wikipedia , it defines the $(p,q)$ -tensor as follows In this approach, a type $(p,q)$ tensor $T$ is defined as a multilinear map, $$
T: \underbrace{ V^* \times\dots\times V^*}_{p \text{ copies}} \times \underbrace{ V \times\dots\times V}_{q \text{ copies}} \rightarrow \mathbf{R},
$$ where $V^*$ is the corresponding dual space of covectors, which is linear in each of its arguments. It is obvious that the definition of a $k$ -tensor in An introduction to manifolds is just the $(0,q)$ -tensors in the definition of Wikipedia . My question is , why do not we define the tensors to be $(p,q)$ -tensor in Tu, Loring W? EDIT: I have noticed that there are no $(p,q)$ -tensors in the whole book of Loring Tu. Is it because that the elements from dual space will never be used in tensors in manifolds in the following study (so $p$ always equals to $0$ )? If not, where do we use $(p,q)$ -tensors in the study of manifolds ? (maybe it will appear in tensor fields?) I would really appreciate it if anyone could tell me the connections of these things.","['manifolds', 'linear-algebra', 'tensor-products', 'differential-geometry']"
3920285,Why is the cross product is alternating?,"In the book An introduction to manifolds by Tu, Loring W, it says that The cross product $v\times w$ on $\mathbb R^3$ is alternating. However in the definition of alternating, it says that A $k$ -linear function $f:V^k\to\mathbb R$ is symmetric, if \begin{align*}
    f\left(v_{\sigma(1)}, \ldots, v_{\sigma(k)}\right)=f\left(v_{1}, \ldots, v_{k}\right)
\end{align*} for all permutations $\sigma\in S_k$ ; it is alternating , if \begin{align*}
    f\left(v_{\sigma(1)}, \ldots, v_{\sigma(k)}\right)=(\operatorname{sgn} \sigma) f\left(v_{1}, \ldots, v_{k}\right)
\end{align*} for all $\sigma\in S_k$ . However, I can't understand why the cross product is alternating. I think that the cross product $\times$ can be viewed as a function $\mathbb R^3\times \mathbb R^3\to\mathbb R^3$ , but not any function that maps to $\mathbb R$ . So why this is alternating ?","['manifolds', 'tensor-products', 'differential-geometry']"
3920320,Quasi-coherent sheaves $\supset$ locally free sheaves?,"This question has been completely reformulated following the guidelines in the comments below, to make it clearer where I was able to get and where I can't get out of. Such comments helped me a lot. Let $(X,\mathscr{O}_{X})$ be a ringed space. Let $\mathscr{F}$ be a sheaf of $\mathscr{O}_{X}$ -modules locally free. Show that $\mathscr{F}$ is a quasi-coherent sheaf of $\mathscr{O}_{X}$ -modules. I've been thinking about the following: $\mathscr{F}$ locally free $\Rightarrow$ $X$ can be covered by open sets $U$ for which $\mathscr{F}|_{U}\cong \bigoplus_{i \in I}\mathscr{O}_{X}|_{U}$ , for some set $I$ . As with any $ X $ scheme, the structure sheaf $\mathscr{O}_{X}$ is quasi-coherent, then $X$ can be covered by open affine subsets $U_j=$ Spec $A_j$ , such that for each $j$ there is an $A_j$ -module $M_j$ with $\mathscr{O}_{X}|_{U_j} \cong \widetilde{M_j}$ . So, we have to: $\mathscr{F}|_{U \cap U_j}\cong \bigoplus_{i \in I}\mathscr{O}_{X}|_{U\cap U_j}\cong \bigoplus_{j \in I} \widetilde{M_j}\cong \widetilde{(\bigoplus_{j \in I} M_j)}$ . Thus, $\mathscr{F}$ is quasi-coherent. But now I have a question. A quasi-coherent sheaf $\mathscr{F}$ is primarily a sheaf of $\mathscr{O}_X$ -modules. So so that I can see that $\mathscr{O}_X$ is quasi-coherent, I have to look at $ \mathscr{O}_X(U)$ as $ (\mathscr{O}_X(U), +)$ a group and therefore a $\mathscr{O}_X(U)$ - module, for each open $U$ in $X$ . To make sense of the definition of quasi-coherent in sheaf of rings $\mathscr{O}_{X} $ . Is that correct?","['quasicoherent-sheaves', 'algebraic-geometry', 'sheaf-theory']"
3920332,proving that $f'(x) > g'(x)$ implies $f$ and $g$ cannot intersect more than once.,"I'm not completely certain if the statement is true, but intuitively I think it is. My sort of intuitive explanation would be if there is a point where $f$ and $g$ are equal, then the fact that $f'$ is greater than $g'$ would prevent them from intersecting again. I was thinking of doing something with the mean value theorem and a proof by contradiction: If we assume $f$ and $g$ intersect at two points, say $a$ and $b$ , then $f(a)=g(a)$ and $f(b)=g(b)$ . By the MVT, there must be points $c_f$ and $c_g$ such that $f'(c_f)=\frac{f(b)-f(a)}{b-a}$ and $f'(c_g)=\frac{g(b)-g(a)}{b-a}$ , which implies $f'(c_f)=g'(c_g)$ . But I'm not sure how much this would help since it would only lead to a contradiction if $c_f=c_g$ . Is there a way to continue the proof from here or some other way to prove this statement?",['derivatives']
3920336,"Prove that if $f$ is differentiable at 0, then the function $f(|x|)$ is differentiable at 0 if and only if $f'(0)=0$","I have the following exercise: Prove that if $f$ is differentiable at 0, then the function $f(|x|)$ is differentiable at 0 if and only if $f'(0)=0$ This problem is somewhat similar and the accepted solution seems correct to me. But I would like to know if there is a different way to solve it, also, the difference to the exercise here is that we do not know that $f(0) = 0$ .
I hope you can help me, thank you.","['derivatives', 'real-analysis']"
3920411,Prove that $|\sin 1| + |\sin 2| + |\sin 3| +\cdots+ |\sin 3n| > 8n/5$ [duplicate],"This question already has an answer here : An inequality for a sum of sine functions (1 answer) Closed 3 years ago . So, the question is as follows: Prove that $\left|\sin 1\right| + \left|\sin 2\right| + \left|\sin 3\right| +\cdots+ \left|\sin 3n\right| > 8n/5.$ I have tried quite a few approaches, including using the Taylor expansion or the A.M-G.M inequality, but to no avail. It would be of great help if this problem could be proven. Edit: Angles are in radians, $n$ is a natural number.","['trigonometry', 'summation', 'inequality']"
3920444,Does this proof of the well-ordering principle subtly use induction?,"My impression is that the well-ordering principle and induction are equivalent, so you must have to use induction to prove WOP, right? Does this proof use induction somewhere? Or just subtly fail? Let $S$ be an arbitrary subset of the natural numbers, and take an arbitrary $k \in S$ . Consider the subset $T = \{s \in S \mid s \leq k\}$ . Since $T \subset \{1,2,\dotsc,k\}$ it's a finite set, and so it must have some least element $m$ . We'll show $m$ is the least element of $S$ . Take an arbitrary $s \in S$ . If $s\leq k$ then $s \in T$ and so $m\leq s$ . Otherwise if $s > k$ then $m \leq k < s$ . In either case $m$ is less than your arbitrary element of $S$ , so $m$ is the least element of $S$ .","['elementary-set-theory', 'induction']"
3920468,Derivation of the Entropy of the Johnson-SU distribution,"The pdf of the Normal distribution is $$
f(x) = \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{1}{2}\left(\frac{z-\mu}{\sigma}\right)^{2}}
$$ The pdf of the Johnson-SU distribution is $$
f(x) = \frac{\delta}{\lambda \sqrt{2 \pi}} \frac{1}{\sqrt{1+\left(\frac{x-\xi}{\lambda}\right)^{2}}} e^{-\frac{1}{2}\left(\gamma+\delta \sinh ^{-1}\left(\frac{x-\xi}{\lambda}\right)\right)^{2}}
$$ The differential entropy of the Normal pdf is derived as \begin{align}\label{equation:hN}
h(X) = & -\int_{-\infty}^{\infty} f(x) \ln f(x) \mathrm{d} x\\
=&-\int_{-\infty}^{\infty} \left(2\pi \sigma^2\right)^{-\frac{1}{2}} e^{-(x-\mu)^2 / 2\sigma^2} \ln\left[ \left(2\pi \sigma^2\right)^{-\frac{1}{2}} e^{-(x-\mu)^2 / 2\sigma^2} \right] \mathrm{d} x\\
= &\frac{1}{2} \ln(2\pi \sigma^2) \int_{-\infty}^{\infty} (2\pi \sigma^2)^{-\frac{1}{2}} e^{-(x-\mu)^2 / 2\sigma^2} \mathrm{d} x\\
&+ \frac{1}{2\sigma^2} \left(2\pi \sigma^2\right)^{-\frac{1}{2}} (x-\mu)^2 e^{-(x-\mu)^2 / 2\sigma^2} \mathrm{d} x\\
=& \frac{1}{2} \ln (2\pi\sigma^2) + \frac{1}{2}\\
=& \frac12\ln(2\pi\sigma^2) + \frac12\ln e\\
=& \frac12\left(\ln(2\pi\sigma^2) + \ln e\right)\\
=& \frac{1}{2} \ln (2\pi e \sigma^2)
\end{align} What is the derivation of the Johnson-SU pdf's differential entropy ? First Attempt \begin{align}
h(X) = &-\int_{-\infty}^{\infty} f(x) \ln f(x) \mathrm{d} x\\
=& \mathbb{E}[ -\ln f(x)] \\
=& \mathbb{E} \left[ -\ln \left( \frac{\delta}{\lambda \sqrt{2 \pi}} \frac{1}{\sqrt{1+\left(\frac{x-\xi}{\lambda}\right)^{2}}} e^{-\frac{1}{2}\left(\gamma+\delta \sinh ^{-1}\left(\frac{x-\xi}{\lambda}\right)\right)^{2}} \right) \right] \\
=& \mathbb{E} \left[ -\ln \left( \frac{\delta}{\lambda \sqrt{2 \pi}} \right) -\ln \left(\frac{1}{\sqrt{1+\left(\frac{x-\xi}{\lambda}\right)^{2}}}\right) - \ln \left( e^{-\frac{1}{2}\left(\gamma+\delta \sinh ^{-1}\left(\frac{x-\xi}{\lambda}\right)\right)^{2}} \right) \right] \\
=& \delta \ln \left( \lambda \sqrt{2 \pi} \right) - \mathbb{E} \left[ -\ln \left(\sqrt{1+\left(\frac{x-\xi}{\lambda}\right)^{2}}\right) -\frac{1}{2}\left(\gamma+\delta \sinh ^{-1}\left(\frac{x-\xi}{\lambda}\right)\right)^{2}\right] \\
=& \delta \ln \left( \lambda \sqrt{2 \pi} \right) - \mathbb{E} \left[ -\frac{1}{2} \ln \left(1+\left(\frac{x-\xi}{\lambda}\right)^{2}\right) -\frac{1}{2}\left(\gamma+\delta \sinh ^{-1}\left(\frac{x-\xi}{\lambda}\right)\right)^{2}\right] \\
=& \delta \ln \left( \lambda \sqrt{2 \pi} \right) -\frac{1}{2} \mathbb{E} \left[ - \ln \left(1+\left(\frac{x-\xi}{\lambda}\right)^{2}\right) - \left(\gamma+\delta  \log \left[\left(\frac{x-\xi}{\lambda}\right) + \sqrt{\left(\frac{x-\xi}{\lambda}\right)^2+1}\right] \right)^{2}\right] \\
= & ?
\end{align} Note: $\sinh^{-1} z= \log (z + \sqrt{z^2+1})$ . Is the attempt so far correct? How to simplify and complete the derivation?","['statistics', 'entropy', 'probability-distributions', 'information-theory', 'gaussian']"
3920471,Do I need to analyse sequence given by $ x_{1+n} = \frac{1}{2 + x_{n}}$ without an equation with $0$?,"I have a problem with exercises with sequences given by recursion when I need to ""prove convergence and find limit if it exists"" and I am given a recursion of that kind: $$ x_{1+n} = \frac{1}{2 + x_{n}}, x_1 \in (0 ; \infty)$$ It is fairly easy to find the limit - I just assume that the limit exists in $ \mathbb{R}$ and then use arithmetic properties of limits: $$\lim_{n \to \infty} x_{n+1} = \lim_{n \to \infty} x_{n}$$ $$\lim_{n \to \infty} x_{n} = l, l \in \mathbb{R}>0$$ Taking my recursion: $$l = \frac{1}{2 + l}$$ $$l^2 +2l - 1 = 0$$ $$l_1 = \sqrt{2} - 1 \in D$$ $$l_2 = -1 - \sqrt{2} \notin D$$ So my only possible limit in $ \mathbb{R}$ is $l = \sqrt{2} - 1$ . That is if I can actualy prove that the limit exists - that is: the sequence is monotonous and bounded. And here is my problem - it is just impossible to analyse without computer the difference of: $$x_{1+n} - x_{n} = \frac{1}{2 + x_{n}} - x_{n}$$ In search of limits I just multiply both sides of equation by $ \lim_{n \to \infty} x_{n} = l$ and it is impossible to do so here, so I get: $$x_{1+n} - x_{n} = \frac{-x_{n}^2-2x_n+1}{2 + x_{n}}$$ Then I can't tell when it is bigger than $0$ to analyse monotonicity and I can't the see for which values o $n$ which values of $n+1$ i get (to get the boundary) because min value gets crazy. So I just waneted to ask - am I missing something? Is it possible to make here $x_{1+n} - x_{n} = \frac{-x_{n}^2-2x_n+1}{2 + x_{n}}$ an equality with $0$ and analyse simpler function (red one on the picture)?","['limits', 'sequences-and-series', 'analysis', 'real-analysis']"
3920500,"Borel $\sigma$-algebra is equivalent to the smallest $\sigma$-algebra containing (a,$\infty$)","I'm trying to prove that the Borel $\sigma$ -algebra is equivalent to the smallest $\sigma$ -algebra containing (a, $\infty$ ). The approach I'm trying to use is to show that if A $\subset$ B and B $\subset$ A, then A=B. So far I think I have the first part: (a, $\infty$ ) = $\cup$ (a, n) and since each (a, n) is a finite open interval, then they are included in the Borel $\sigma$ -algebra and since a $\sigma$ -algebra is closed with respect to countable union, then the smallest $\sigma$ -algebra containing (a, $\infty$ ) $\subset$ Borel $\sigma$ -algebra. Looking at (a,b) it seems obvious that it would be contained within (a, $\infty$ ), but I'm not sure if it is sufficient to just say that for the second part of this proof or if there is a formal way to show that (a,b) is contained in (a, $\infty$ ).","['measure-theory', 'solution-verification']"
3920512,Function satisfying the relation $f(x+y)=f(x)+f(y)-(e^{-x}-1)(e^{-y}-1)+1$,"Let f be the differentiable function satisfying the relation $f\left( {x + y} \right) = f\left( x \right) + f\left( y \right) - \left( {{e^{ - x}} - 1} \right)\left( {{e^{ - y}} - 1} \right) + 1$ ; $\forall x,y \in R$ and $\mathop {\lim }\limits_{h \to 0} \frac{{f'\left( {1 + h} \right) + f\left( h \right) - {e^{ - 1}}}}{h}$ exist. The value of $\int\limits_0^1 {f\left( x \right)dx}  = \_\_\_\_\_\_\_$ . My approach is as follow $f\left( {x + y} \right) = f\left( x \right) + f\left( y \right) - \left( {{e^{ - x}} - 1} \right)\left( {{e^{ - y}} - 1} \right) + 1$ $\Rightarrow f\left( {x + y} \right) - f\left( x \right) = f\left( y \right) - \left( {{e^{ - x}} - 1} \right)\left( {{e^{ - y}} - 1} \right) + 1$ $ \Rightarrow \mathop {\lim }\limits_{h \to 0} \frac{{f\left( {x + y} \right) - f\left( x \right)}}{h} = \mathop {\lim }\limits_{h \to 0} \frac{{f\left( y \right) - \left( {{e^{ - x}} - 1} \right)\left( {{e^{ - y}} - 1} \right) + 1}}{h}$ $ \Rightarrow \mathop {\lim }\limits_{h \to 0} \frac{{f\left( {x + y} \right) - f\left( x \right)}}{h} = \mathop {\lim }\limits_{h \to 0} \frac{{f\left( y \right) - \left( {{e^{ - x}} - 1} \right)\left( {{e^{ - y}} - 1} \right) + 1}}{h}$ y=h $ \Rightarrow f'\left( x \right) = \mathop {\lim }\limits_{h \to 0} \frac{{f\left( {x + h} \right) - f\left( x \right)}}{h} = \mathop {\lim }\limits_{h \to 0} \frac{{f\left( h \right) - \left( {{e^{ - x}} - 1} \right)\left( {{e^{ - h}} - 1} \right) + 1}}{h}$ How will I proceed from here","['integration', 'limits']"
3920633,"Solution verification: the minimal/maximal elements of a relation on $\mathcal{P}(\{1,2,3,4\})$","Consider the power set $\mathcal P ( \{ 1 , 2 , 3 , 4 \} ) $ and the partial order $ S $ on $ $ such that $ A \mathrel S B $ is defined as $$A \; S \; B \iff A \cap \{ 1 , 2 \} \subset B \cap \{ 1 , 2 \} $$ $S$ is a partial order and I`m trying to find the minimum and maximum elements of it. Are my results correct? Minimum elements: $$   \{\text{Ø}\},\{3\},\{4\},\{3,4\}  $$ Maximum elements: $$\{1,2\},\{1,2,3\},\{1,2,4\},\{1,2,3,4\} $$","['relations', 'solution-verification', 'discrete-mathematics']"
3920639,Is convolution area-preserving?,"Consider two functions f(x) and g(x) with indefinite integrals (""area under the curve"") A f and A g . Does the convolution f * g preserve the area, i.e. is A f * g = A f * A g i.e. ∫ ( ∫f(x)g(t−x)dx ) dt = (∫f(x)dx) (∫g(x)dx) ? I've learned that convolution in one space is multiplication in the dual space, or more precisely, that direct multiplication in one space corresponds to polynomial multiplication in the dual space. There is also a nice, short video where this intuition is explained, but I'm having difficulty transferring this to the question regarding areas and drawing the right conclusions. To provide some practical context: Image formation in an optical system can be described as Image = Source * PSF with PSF being the point spread function describing the system's impulse response. The PSF is often approximated by a Gaussian and the convolution results in a Gaussian blur. If the Gaussian is normalized to unit area (i.e. a normal distribution), does this equate to the fact that the integrated intensities of source and image are preserved? Meaning that the photons are merely ""redistributed"" (blurring process), but the total photon number is preserved (= energy conservation in an ideal, lossless system)? Conversely, can I add e.g. a 10% loss of photons due to absorption by simply changing the normalization of the PSF from 1.0 to 0.9 (rescaling the Gaussian's normalization factor)?","['integration', 'area', 'improper-integrals', 'convolution']"
3920696,How to simplify out of the gamma function $\Gamma(z)$?,"$$\mathrm{d}t(x_i; \nu) = \frac{\Gamma \bigg(\frac{\nu+1}{2}\bigg) }{\Gamma (\frac{\nu}{2}) \sqrt{\pi\nu}} \Bigg( 1+\frac{x_i^2}{\nu} \Bigg)^{-\frac{\nu+1}{2} } \enspace, i=1,2$$ How to derive a closed-form solution of the above by simplifying out of the gamma function? Is the gamma function that is being used above (for $z>0$ ) the same as $$\Gamma(z)=\int_0^\infty x^{z-1}e^{-x}\mathrm{d}x ?$$","['integration', 'statistics', 'probability-distributions', 'gamma-function', 'copula']"
3920708,What is $\det(A + I)$ when $AA^t = I$ and $\det(A) < 0$?,"Notation: $A^t$ : transpose of matrix $A$ . $\det A$ : determinant of matrix $A$ . $I$ : the identity matrix. We know: $A \in M_{n\times n}(\mathbb{R}), \\
AA^t = I, \ \mathrm{and \ det}(A) < 0$ We want: $\det(A + I)$ From $\det A = \det A^t$ , $\det A^{-1} = \frac{1}{\det A}$ and $\det A < 0$ we can deduce: $\det A = -1$ .  I tried calculating $$\det(A + I) = \det(A(I + A^t)) = \det(A) \det(I + A^t) = - \det(I + A^t)$$ but that didn't lead to anything. I tried using the fact that $A + A^t$ is symmetric and $A - A^t$ is skew-symmetric but I couldn't achieve anything with that either. I thought about interpreting it via a linear transformation but couldn't come up with anything. I learned that matrices, where $AA^t = I$ , are called orthogonal but that's all I know about orthogonal matrices so I'm hoping to find a solution that doesn't get too deep into orthogonality. Hints would be appreciated. Thank you in advance!","['matrices', 'orthogonal-matrices', 'determinant', 'linear-algebra']"
3920709,Prove that any group $G$ with $|G|=588$ is solvable,"I'm stuck trying to solve this problem from my abstract algebra course: Prove that every group of order $588$ is solvable (If you assume that all groups of certain order are solvable, you must prove it too). First I noticed that $588=2^2\cdot 3\cdot 7^2$ , so I guess the prove will use Sylow $p$ -subgroups, but I don't find right what I need to do. Any help or hint will be appreciated. Thanks in advance.","['abstract-algebra', 'p-groups', 'sylow-theory', 'group-theory', 'solvable-groups']"
3920773,Existence of an asymptote for $g(x)=\frac{f(x)f'(x)+f(1)f'(1)}{f'(x)+f'(1)}-f\left(\frac{xf'(x)+f'(1)}{f'(x)+f'(1)}\right)$,"Working with the Slater's inequality (companion of Jensen's inequality) I find this statement : Let $f(x)$ be a continuous, $n$ times differentiable ,convex and non constant on $(0,\infty)$ and increasing on $(1,\infty)$ and finally with  non bounded derivatives then define : $$g(x)=\frac{f(x)f'(x)+f(1)f'(1)}{f'(x)+f'(1)}-f\left(\frac{xf'(x)+f'(1)}{f'(x)+f'(1)}\right)$$ With $g(x)$ strictly increasing on $(1,\infty)$ . Claim: $\lim_{x\to\infty}\frac{g(x)}{x}=constant$ I have not the key to approach the general case but let me try some example : For my first example I take the exponential see here .The function can be decreasing or increasing on a such interval see as example the function $f(x)=x^x$ .As particular case we have $f(x)=x+\frac{1}{x}$ this asymptote is constant.So in fact I have tried only elementary function and their compositions but it would be curious that it works only for them. My question : Have you counter-example (it would be perfect because I have some doubt on this statement) or a proof (which I think is not easy) ? Thanks in advance !","['jensen-inequality', 'examples-counterexamples', 'asymptotics', 'derivatives', 'convex-analysis']"
3920778,"Why is a theorem about extending morphisms $\operatorname{Spec} K\to \mathbb{P}^n_K$ called ""The Heavenly L'Hopitals Rule""?","In Introduction to Schemes , G. Ellingsrud and J. C. Ottem call the result below ""The Heavenly L'Hopitals Rule"". I see absolutely no similarity between this and the usual L'Hopital rule. What do they mean by this name?",['algebraic-geometry']
3920807,How many ways are there in the case that there is ONLY ONE adjacent couple?,"""Three couple will be arranged in a row.How many ways are there in the case that there is ONLY ONE couple which are adjacent."" For example , let $(a_1,a_2),(b_1,b_2),(c_1,c_2)$ be the couples, so they cannot be arranged in the form of $a_1,c_1,c_2,b_2,b_1,a_2$ .  There can be only one couple adjacent such as $b_1,a_1,a_2,c_1,b_2,c_2$ is acceptable. I found this question in my textbook.I tried that $C(3,1).5!.2!-[C(3,2).6.2!.2!.2!+C(3,3).2!.2!.2!.3!]$ but my answer is wrong and I am confused. Can you help me?","['permutations', 'combinatorics', 'problem-solving', 'discrete-mathematics']"
