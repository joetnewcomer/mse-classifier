question_id,title,body,tags
4133566,Problem with understanding Cantor's diagonal argument [duplicate],"This question already has answers here : Why does Cantor's diagonal argument not work for rational numbers? (2 answers) Closed 3 years ago . As shown in Cantor's diagonal argument , we make an assumption that the list of infinite sequences of binary digits is countable. After writing an enumeration, we complemented the diagonal elements to produce an element which isn't in the enumerated list. If the number $s$ is made up from complementing the digits of $s_i$ for $i=1,2,...,n$ , then it is guaranteed that it is not one of the $s_i$ s by construction. My problem is how do we know that it isn't one of the $s_j$ s for $i=n+1,n+2,...$ .
The enumeration doesn't afterall stop at n---It goes like--- ${s_1,s_2,s_3,...,s_n,s_{n+1},s_{n+2},...}$ . So our specially constructed $s$ can(infact must) fall in the list though it doesn't fall in the first $n \hspace{0.1cm}s_i$ s. Contrastingly consider the proof that the rationals are countable, we construct a similar table(by dividing the row and column number of that table), at any step we can construct a rational which is not in that table by choosing a bigger row and or or a bigger column number, so we made a number which wasn't in the list but we don't say then that the set of rationals is uncountable.
(Instead it is said we can make any rational number $c/r$ in finite number of steps.) Note that I have no problem in accepting the fact that the set of reals is uncountable (By Cantor's first argument), it is the diagonal argument which I don't understand. Also I think, this shouldn't be considered an off-topic question although it seems that multiple questions have been asked altogether but these questions are too much related and useful in drawing the (wrong)contradiction to Cantor's argument, so they have been asked. I have seen similar understanding problems with Cantor's diagonal argument in MSE but none match mine(probably) so this isn't a duplicate as well. If you think I missed some, please kindly comment down the link and I will have a look.","['elementary-set-theory', 'real-numbers', 'infinity']"
4133606,"If a function A $\to$ B is bijective then if A is infinite, so it is B (Proof with Dedekind's definition of infinite sets).","Theorem: Be A, B sets that can be be mapped bijectively. If A is infinite, so it is B. Note that we only use Dedekind's definition of infinite sets here: If N $ \subset$ M and f: M $\to$ N bijective then M is infinite. There is a proof in my textbook I do not understand. The proof looks like this: Let A' $ \subset$ A and $f$ : A $\to$ A' bijective. Further we assume $h$ : A $\to$ B bijective.
We assume $g: h \circ f \circ h^{-1}$ : B $\to$ B.
Then $g$ is injective. There's some x $\in$ A - A', so $h(x) \notin rng(g)$ . More precisely it holds: rng( $g$ ) = $h''A' \subset h''A = B$ .
Therefore $g: B \to rng(g) \subset B$ is a witness for the infinity of B. I am too new to set theory to understand this proof. Following you see my so-far attempt to make some sense of the upper proof: Let A' $ \subset$ A and $f$ : A $\to$ A' bijective. Further we assume $h$ : A $\to$ B bijective. (Just the premises of the theorem.) Further we assume a function $g: h \circ f \circ h^{-1}$ , so the mapping goes: B $\to$ A $\to$ A' $\to$ A $\to$ B. Because of A' $ \subset$ A we know that some x $\in$ A - A'. In the case of $h$ : A $\to$ B alone, which is bijective, it means that x has an image $h(x)$ which is in rng( $h$ ) = B, so $h(x)$ $\in$ B. But in case of $g$ the element $h(x)$ $\notin$ rng( $g$ ) because x cannot map from A' going forward, so it cannot be in rng( $g$ ) as $h(x)$ . Therefore rng( $g$ ) $\subset$ B. But then why $g: B \to rng(g)$ is bijective? So basically I need someone to lead me thru the upper proof of my textbook by explaining more what's going on than the short proof tells. I just added my thoughts, so that you can see my thinking/mistakes and also see that I tried.",['elementary-set-theory']
4133668,Are $f(x)=\cot x \tan x$ and $g(x)=1$ equal to each other?,We have two functions $f(x)=\cot x \tan x$ and $g(x)=1$ . Are these function equal to each other? For the function $f(x)$ I know for $x\neq\frac{k\pi}{2}$ it is equal to $1$ but for $x=\frac{k\pi}{2}$ We have $\infty\times0$ and I think it is undefined. So the functions aren't equal to each other. Am I right?,"['algebra-precalculus', 'functions']"
4133738,Integration in cylindrical coordinate system,"Context: I am trying to derive an equation given in a Journal of Fluid Mechanics paper (2.2). It deals with the analysis of an axisymmetric turbulent wake where cylindrical coordinate system has been used (which to me is a little hard to understand as I typically deals in Cartesian system). We start with what is called as a momentum equation (simplified version after certain assumptions): $$U_\infty \frac{\partial}{\partial x} (U - U_\infty) = - \frac{1}{r} \frac{\partial}{\partial r} (r \ \overline{uv})$$ Here, $r$ is the radial direction. The axial ( $x$ ) velocity is defined as $U$ and $U_\infty$ is a constant freestream velocity. The term $\overline{uv}$ is called as a turbulent stress which tends to zero if $r$ tends to infinity (i.e. restricted within a finite radial distance). The authors integrate this equation over a cross-section to yield: $$U_\infty \int_0^\infty (U_\infty - U) r \ dr \approx \theta^2U_\infty^2$$ where $\theta$ is the momentum thickness. I think I can tweak the variables to get the $\theta$ in the equation, but I am not able to understand how exactly would we take a cross-section and then integrate over it? Any leads would be appreciated. PS: Another equation that hasn't been mentioned is a continuity equation that is written as: $$\frac{\partial U}{\partial x} + \frac{1}{r} \frac{\partial}{\partial x} (r V) = 0$$","['integration', 'cylindrical-coordinates', 'partial-differential-equations', 'boundary-layer', 'fluid-dynamics']"
4133755,"Let $n\geq 1$ and let $A$ be a $n\times n$ matrix with real entries such that $A^k=O$, then find value of $\det(I+A)$","Let $n\geq 1$ and let $A$ be a $n\times n$ matrix with real entries such that $A^k=O$ , for some $k\geq 1$ . Let $I$ be the $n\times n$ identity matrix. Then find value of $\det(I+A)$ . My Attempt Since $A^k=O$ we have $\det(A)=0$ and also $A^k=A^{k+1}=A^{k+2}=...=O$ Let $p\in \Bbb N$ where $p\geq k$ $(I+A)^p=I+\binom{p}{1}A+\binom{p}{2}A^2+...+\binom{p}{k-1}A^{k-1}+O$ (since $A^k=A^{k+1}=A^{k+2}=...=O$ ) $A^{k-1}(I+A)^p=A^{k-1}$ $\det(A^{k-1}(I+A)^p)=\det(A^{k-1})=0$ I wonder if I am doing right because I cannot justify further steps","['matrices', 'determinant', 'linear-algebra', 'eigenvalues-eigenvectors']"
4133815,Circle passing through two points and tangent to a line,"""Find the equation of the circle passing through the origin $(0,0)$ , the point $(1,0)$ and tangent to the line $x-2y+1=0$ ."" What I have done: The equation of a circle with radius $R$ and center $(x_0,y_0)$ is $(x-x_0)^2+(y-y_0)^2=R^2$ . Since the circle passes through $(0,0)$ and $(1,0)$ it must be $$\tag{1}  x_0^2+y_0^2=R^2$$ $$ \tag{2} (1-x_0)^2+y_0^2=R^2$$ and since it is tangent to the line $x-2y+1=0$ the distance from the center to this line must equal the radius $R$ hence (using the fact that the distance $d$ from a point $(x_0,y_0)$ to a line $ax+by+c=0$ is $d=\frac{|ax_0+by_0+c|}{\sqrt{a^2+b^2}}$ ) we have $$R=\frac{|ax_0+by_0+c|}{\sqrt{a^2+b^2}}=\frac{|1x_0-2y_0+1|}{\sqrt{1^2+(-2)^2}}=\frac{|x_0-2y_0+1|}{\sqrt{5}} \tag{3}$$ From (1) and (2) we have $$x_0^2 + y_0^2 = (1 - x_0)^2 + y_0^2 \Rightarrow x_0 = \dfrac{1}{2} \tag{4}$$ Equating (1) , (3) and (4) we have $$\dfrac{1}{4}+y_0^2 = \dfrac{(\frac{3}{2}-2y_0)^2}{5} \Rightarrow y_0 = -3 \pm \sqrt{10}$$ So there are two solutions, $$R^2 = \dfrac{1}{4}+(-3+\sqrt{10})^2 \Rightarrow (x - \dfrac{1}{4})^2+(y + 3 - \sqrt{10})^2 = \dfrac{1}{4} + (-3 + \sqrt{10})^2$$ and $$R^2 = \frac{1}{4} + (-3 - \sqrt{10})^2 \Rightarrow (x - \frac{1}{4})^2 + (y + 3 + \sqrt{10})^2 = \dfrac{1}{4} + (-3 - \sqrt{10})^2$$ . Now, when I plot these solutions they appear to be wrong, so I would like to know which mistake(s) I have made.","['analytic-geometry', 'solution-verification', 'geometry']"
4133828,Proving Existence and Uniqueness for Cox-Ingersoll-Ross SDE.,"The Cox-Ingersoll Ross SDE is: $dr_t=a(b-r_t)dt+\sigma\cdot \sqrt{r_t}dB_t$ . I would like to know how to prove existence and uniqueness and that the solution is positive. Øksendal has this result(I simplify it to one dimension): Let $T>0$ and $b(\cdot,\cdot): [0,T]\times\mathbb{R}\rightarrow R,
 \sigma(\cdot,\cdot): [0,T]\times\mathbb{R}\rightarrow \mathbb{R}$ be measurable
functions satisfying: $$|b(t,x)|+|\sigma(t,x)|\le C(1+|x|),$$ for some $C$ . And also $$|b(t,x)-b(t,y)|+|\sigma(t,x)-\sigma(t,y)|\le D|x-y|,$$ for some $D$ . Let $Z$ ve a random variable which is independent of the
sigma-algebra $F_\infty$ generated by $B_s$ and such that $$E[Z^2]<\infty.$$ Then the stochastic differential equation $$dX_t=b(t,X_t)dt+\sigma(t,X_t)dB_t, X_0=Z,$$ has a unique t-continuous solution $X_t(\omega)$ with the property
that $X_t(\omega)$ is adapted to the filtration $\mathcal{F}_t^Z$ generated by $Z$ and $B_s$ , $s\le t$ and $$E\left[\int_0^T |X_t|^2 dt\right]<\infty.$$ Can we use this result to show that the CIR SDE has an unique positive result? The problem is that the growth conditions are not satisfied near zero. An idea is to look at the SDE: $dr_{t,\epsilon}=a(b-r_{t,\epsilon})dt+\sigma\cdot \sqrt{\max(r_{t,\epsilon},\epsilon)}dB_t,$ from what I see this function satisfies the growth and Lipschitz continuity condition for every $\epsilon$ bigger than zero. If we let $\epsilon_n$ be a sequence of positive real numbers converging to zero we get a sequence of processes $r_{t,\epsilon_n}$ . But do we know if these processes converges in some way to the process we want?, and if they converge in some way to a process, is the process an Itö-process that satisfies the CIR SDE?","['stochastic-analysis', 'stochastic-processes', 'stochastic-differential-equations', 'probability-theory', 'stochastic-calculus']"
4133887,"Can a ""dimension reducing"" function be bijective?","Can a function whose image has lower dimensionality than its domain (alas I don't know if there is a special name for that kind of functions) ever be bijective? Consider e. g. $$f:X\to Y:\begin{pmatrix} x_1\\x_2\\x_3 \end{pmatrix} \to\begin{pmatrix} y_1\\y_2 \end{pmatrix}=\begin{pmatrix} x_1 +x_2\\x_2+x_3 \end{pmatrix}$$ which is a function from $\Bbb{R}^3$ into $\Bbb{R}^2$ and is obviously not bijective. My problem is: On the one hand, I think it should be possible to have a bijective function from a ""cube"" into a ""plane"", on the other, it's hard to imagine because it effectively means calculating one result variable from multiple input variables, which would imply that the same image element can always result from different domain elements, thus rendering bijectivity impossible.",['linear-algebra']
4133910,Problem about convergent series,"I'm trying to prove this series converges by using some sort of comparison test. $$\sum_{n=1}^{\infty}\frac{1}{n^{0.51}}-\sin\left(\frac{1}{n^{0.51}}\right)$$ I know by $\sin n\le n$ that the series is positive, so I went with the direction of using the comparison test.
But I can't seem to find a function that is always greater than the expression in the series that also converges..","['limits', 'number-comparison', 'sequences-and-series']"
4133919,A positive integer is special if it has at least two proper divisors and is a multiple of all possible differences between two of them.,"The proper divisors of a positive integer n are all positive integers other than 1 and n which divide $n$ . A positive integer is special if it has at least two proper divisors and is a multiple of all possible differences between two of them. Determine all positive integers that are special. I determined that $6$ , $8$ and $12$ are special. And that any power of $2$ greater that $8$ is not special. But I don't know how to proceed, I would appreciate if you could help me.",['number-theory']
4133958,Uniformly bounded metric,"Let $(V,g)$ be a globally hyperbolic manifold of the type $S \times \mathbb{R}$ , where $S$ is an oriented smooth manifold, $g$ is pseudo-Riemannian with signature $(-,+,...,+)$ and each submanifold $S \times \{t\}$ is spacelike. $g$ is defined to be of the form $$g=-N^2dt^2+ \sigma_{ij}\theta^i \theta^j, \ \theta^i= dx^i+ \beta^i dt$$ where the scalar function $N$ is strictly positive and $\sigma_t$ is Riemannian on each $S_t$ and $\sigma_t$ uniformly equivalent to a complete metric $e$ . In ""Wave maps in general relativity"", the is a theorem, stating a local existence result for wave maps on $V$ : Suppose $M$ is diffeomorphic to $\mathbb{R}^m$ , $(M,h)$ a Riemannian manifold. A mapping $u: V \rightarrow M$ is then
globally defined by a set of $m$ scalar functions $u^A$ on $V$ .
Suppose the metric $h$ is smooth. Suppose $g$ is smooth with uniformly bounded
derivatives of arbitrary order with respect to $t$ and $e$ , i.e., $N$ , $\beta$ , $g$ possess these
properties. Let $\varphi^A \in C^0_b$ , $D \varphi^A \in H_{s-1}$ , and $\psi \in H_{s-1}$ be Cauchy data such that $s > \dfrac{n}{2}+1$ . Then there exists a wave map on $S \times [0,T)$ .... I don't really understand the condition "" with uniformly bounded
derivatives of arbitrary order with respect to "" since I don't understand how to ""measure"" the metric. Isn't it always dependent of the coordinate chart? Also, what does uniformly bounded mean here? My only idea: for an atlas, one can view the union of $g_{ij}$ for all charts as a family of smooth functions and using the supremum norm, on could obtain a uniform bound. Is that the correct idea?","['manifolds', 'differential-geometry']"
4133966,Is a subset of a Borel set of measure zero always Borel?,"I'm pretty sure that a subset of a Borel set (in $\mathbb{R}$ ) of measure zero may not be Borel, but I don't know how to show it. I do know that there are more Lebesgue-measurable sets than Borel sets. I also know that every subset of a Borel set of measure zero is Lebesgue-measurable. I would prefer a general argument (for example, using cardinality) over a counterexample.","['borel-sets', 'measure-theory', 'lebesgue-measure', 'borel-measures']"
4133968,Integral equation with kernel $x-y$,"I am trying to solve the integral equation $$
f(x)=\int_0^x (x-y)f(y)dy, \ 0\leq x\leq 1 
$$ in the space $C([0,1])$ of continuous functions on $[0,1]$ . My reasoning is this: Since $u(y)=(x-y)f(y)$ is continuous, the integral $\int_0^x (x-y)f(y)dy$ is a differentiable function, which means that the solution $f$ will also be differentiable. Differentiating gives $f'(x)=\int_0^xf(y)dy$ . By a similar argument as before $f'$ is differentiable as well, therefore $f''=f$ . This equation has the general solution $$
f(x)=a\exp(x)+b\exp(-x)
$$ for some constants a,b. Now I can use $f(0)=\int_0^0\ldots=0$ to obtain $a=-b$ . My question is whether these are correct and if they are, and how to proceed in determining the constants...","['integration', 'integral-equations', 'ordinary-differential-equations']"
4133985,Statistical survey of two genders in two cities,Suppose there are two cities. In the city A 52% of people are males and 48% are females. In the city B 47% of people are males and 53% are females. We conduct a survey: from each city a simple random sample of size 100 is taken. I need to find the probability that the survey will show greater percentage of males in city B than males in city A. What kind of statistical test should I use? $\chi^2$ or $t$ -test? And how can I find the asked probability?,"['statistics', 'probability']"
4133988,Deriving the Integral for Alternating Harmonic Series Partial Sums,"The partial sums of the harmonic series (the Harmonic Number , $H_n$ ) are given by $$H_n=\sum_{k=1}^{n} \frac{1}{k}$$ and the well known integral representation is $$H_n=\int_0^1 \frac{1-x^n}{1-x}\,dx$$ This can be used to calculate $H_n$ using rational values of $n$ . The partial sums of the alternating harmonic series (the Alternating Harmonic Number , $\widetilde{H_n}$ ) are given by $$\widetilde{H_n}=\sum_{k=1}^{n} \frac{(-1)^{k-1}}{k}$$ Now I am happy with the equivalent integral representation for the alternating harmonic number at integer $n$ : $$\widetilde{H_n}=\int_0^1 \frac{1+(-1)^{n-1}x^n}{1+x}\,dx$$ but at rational values of $n$ this formula gives complex results which I don't believe are correct. The correct formula I think involves the real component of (-1)^z at non-integer values of z, i.e. $\text{Real}[(-1)^z]=\cos(\pi\,z)$ $$\widetilde{H_n}=\int_0^1 \frac{1+(-\cos(\pi\,n))x^n}{1+x}\,dx=\sum_{k=1}^{\infty}(-1)^{k-1} \left(\frac{1}{k}-\frac{\cos(\pi\,n)}{k+n}\right)\tag{1}$$ from which real solutions result for example $$\widetilde{H}_{1/2}=\log 2$$ However is this assertion of mine correct (concerning the integral at non-integer $n$ ) and if so, can it be easily proved (without resorting to complex analysis)? Development of the Argument Lets define the harmonic number in terms of partial zeta sums thus, $H_n=\zeta_n(1)$ and the alternating harmonic number in terms of partial sums of the Dirichlet eta function $\widetilde{H_n}=\eta_n(1)$ by analogy we can define the partial sums of the Dirichlet Lambda function thus, $\lambda_n(1)=\sum_{k=1}^{n} \frac{1}{2k-1}$ and of the Dirichlet Beta function thus, $\beta_n(1)=\sum_{k=1}^{n} \frac{(-1)^{k-1}}{2k-1}$ Now I have been able to find a simple relationship between $\zeta_n(1)$ (i.e. $H_n$ ) and $\lambda_n(1)$ , that is valid for rational $n$ . $$\lambda_n(1)=\zeta_{2n}(1) -\frac{1}{2}\zeta_{n}(1)=H_{2n}-\frac{1}{2}H_n$$ I was therefore wondering whether a similar simple relationship exits between $H_n$ and $\widetilde{H_n}$ for rational $n$ . Obviously that is not possible if $\widetilde{H_n}$ is typically a complex number. For example for integer $n$ we have two formulae for $\widetilde{H_n}$ or $\eta_n(1)$ : $$\eta_{2n-1}(1)=\widetilde{H}_{2n-1}=2\lambda_n(1)-\zeta_{2n-1}(1)$$ $$\eta_{2n}(1)=\widetilde{H}_{2n}=2\lambda_n(1)-\zeta_{2n}(1)$$ and I would like to get to one formula. Update 2: A result of playing with these ideas in Mathematica In Mathematica functions often get simplified in terms of the Generalized Harmonic Number function $H_n^{(s)}=\zeta_n(s)=\sum_{k=1}^n \frac{1}{k^s}$ So I've been trying to represent all the trigonometric functions in terms of the Generalized Harmonic Number function. It's easy for the functions involving $\csc x$ or $\sec x$ but much harder for the pure functions $\sin x$ and $\cos x$ . Anyway I at last managed to find a reasonably simple 4 term candidate for $\cos x$ , utilizing my definition for $\eta_n(1)$ above in (1). $$\cos(x)=1+\frac{x}{\pi}\left(\eta_{-x/\pi}(1)-\eta_{x/\pi}(1)\right)+\frac{x}{\pi}\left(\zeta_{-x/\pi}(1)-\zeta_{x/\pi}(1)\right)$$ But maybe the complex numbers cancel out using the alternative definition.","['integration', 'summation', 'definite-integrals', 'calculus', 'sequences-and-series']"
4133995,Understanding Quasi-self-similarity of the Julia Set (Falconer),"Here's an excerpt from the book: The highly intricate structure of the Julia set illustrated in Figure 0.6 stems from the single quadratic function $f(z) = z^2 + c$ for a suitable constant $c$ . Although the set is not strictly self-similar in the sense that the Cantor set and von Koch curve are, it is quasi-self-similar in that arbitrarily small portions of the set can be magnified and then distorted smoothly to coincide with a large part of the set. I don't know if the author rigorously defines quasi-self-similarity later in the text, but the ""definition"" above seems vague. In the particular example of the Julia set, what is meant by ""arbitrarily small portions of the set can be magnified and then distorted smoothly to coincide with a large part of the set"" ? What exactly are these distortions, and could someone help me understand with pictures how the magnified part on distortion coincides with a part of the set? How much do I need to magnify, and what part? Most of these questions are easy to answer in the case of Cantor sets, von Koch curve, and the Sierpinski triangle - since these are self-similar fractals. Quasi-self-similarity isn't that easy a nut to crack, it seems. I also came across this animation for $f(z) = z^2 - 1$ , but I'm not sure what it's trying to convey. References: Falconer, Fractal Geometry.","['analysis', 'fractals']"
4134053,Prove that $\lim_{n\to\infty} \frac{a_n}{a_{n+1}} = p$ [duplicate],"This question already has answers here : How prove this limit $\lim\limits_{n\rightarrow \infty} \frac{f_n}{f_{n+1}}=a$ given two other limits related to $f_n$ (5 answers) Closed 3 years ago . Given a sequence $\{a_n\}_{n\in \mathbb N}$ of real numbers such that $$\begin{align}
\lim_{n\to\infty}\frac{a_na_{n+1} - a_{n-1}a_{n+2}}{a_{n+1}^2 - a_na_{n+2}} &= p + q && (1)\\[1mm]
\lim_{n\to\infty} \frac{a_n^2 - a_{n-1}a_{n+1}}{a_{n+1}^2 - a_na_{n+2}} &= pq && (2)\end{align}$$ where $|p| < |q|$ , prove that $$\lim_{n\to\infty} \frac{a_n}{a_{n+1}} = p$$ Attempts: Idea #1: Let us denote $b_n = \dfrac{a_n}{a_{n+1}}$ . If we divide both numerators and denominators of $(1)$ and $(2)$ by $a_na_{n+2}$ and $a_{n+1}a_n$ , respectively, we have $$
\begin{align}
\lim_{n\to\infty}\frac{\dfrac{a_{n+1}}{a_{n+2}} - \dfrac{a_{n-1}}{a_{n}}}{\dfrac{a_{n+1}}{a_n}\dfrac{a_{n+1}}{a_{n+2}} - 1} &= p+q && (1') \\[1mm]
\lim_{n\to\infty} \frac{\dfrac{a_n}{a_{a+1}} - \dfrac{a_{n-1}}{a_n}}{\dfrac{a_{n+1}}{a_n} - \dfrac{a_{n+2}}{a_{n+1}}} &= pq && (2')
\end{align}$$ which now can be written as $$\begin{align}
\lim_{n\to\infty}\frac{b_{n+1}b_n - b_nb_{n-1}}{b_{n+1} - b_n} &= p+q  && (1'') \\[1mm]
\lim_{n\to\infty} \frac{b_n - b_{n-1}}{\dfrac{1}{b_{n+1}} - \dfrac{1}{b_{n}}} &= -pq && (2'')
\end{align}$$ Now, every numerator and denominator contains the difference of consecutive terms of some sequence (reminds me of Cesaro-Stolz, but that cannot be applied here). Idea #2: The given conditions can be written as $$\begin{align}
\lim_{n\to\infty}\frac{\begin{vmatrix}a_n & a_{n-1} \\ a_{n+2} & a_{n+1}\end{vmatrix}}{\begin{vmatrix}a_{n+1} & a_{n} \\ a_{n+2} & a_{n+1}\end{vmatrix}} &= p + q && (1')\\[2mm]
\lim_{n\to\infty} \frac{\begin{vmatrix}a_n & a_{n-1} \\ a_{n+1} & a_{n}\end{vmatrix}}{\begin{vmatrix}a_{n+1} & a_{n} \\ a_{n+2} & a_{n+1}\end{vmatrix}} &= pq && (2')\end{align}$$ Now, maybe a bit of Linear Algebra could be incorporated somehow. If we set (B. Grossman) $$\begin{align} x_n = \frac{\begin{vmatrix}a_n & a_{n-1} \\ a_{n+2} & a_{n+1}\end{vmatrix}}{\begin{vmatrix}a_{n+1} & a_{n} \\ a_{n+2} & a_{n+1}\end{vmatrix}}, \quad  y_n =  \frac{\begin{vmatrix}a_n & a_{n-1} \\ a_{n+1} & a_{n}\end{vmatrix}}{\begin{vmatrix}a_{n+1} & a_{n} \\ a_{n+2} & a_{n+1}\end{vmatrix}} \end{align}$$ Then, we have $$\pmatrix{a_{n+1} & a_{n+2}\\a_n & a_{n+1}} \pmatrix{x_n\\y_n} = \pmatrix{a_n\\a_{n-1}}$$ This gives $$\frac{a_{n+1}x_n + a_{n+2}y_n}{a_nx_n + a_{n+1}y_n} = \frac{a_n}{a_{n-1}}$$ Dividing the numerator and the denominator by $a_{n+1}$ , we get $$\frac{x_n + \dfrac{a_{n+2}}{a_{n+1}}y_n}{\dfrac{a_n}{a_{n+1}}x_n + y_n} = \frac{a_n}{a_{n-1}}$$ Noting that $x_n \to p+q$ , $y_n \to pq$ and assuming that $\dfrac{a_n}{a_{n-1}} \to A$ , and sending $n$ to infinity, we get $$\frac{p+q + Apq}{\dfrac{1}{A}(p+q) + pq} = A$$ which simplifies to $$0=0$$ I think I did something wrong somewhere. Any help is appreciated.","['limits', 'calculus', 'sequences-and-series', 'real-analysis']"
4134102,"Is there a ""more easier"" way to integrate by parts?","I found a photo from Facebook (now removed) with the following content: When integrating by parts, students often struggle with how to break up the original integrand into $u$ and $dv$ . $\color{blue}{\rm LIATE}$ is an acronym that is often used to determine which part of the integrand should become $u$ . Here's how it works: let $u$ be the function from the original integrand that shows up on the list below. Logarithmic functions e.g. $(\ln x)$ Inverse trigonometric functions e.g. $(\tan^{-1}x)$ Algebraic functions e.g. $(x^{3} + x - 2)$ Trigonometric functions e.g. $(\cos x)$ Exponential functions e.g. $(e^{x})$ In general, we want to let $u$ be a function whose derivative $du$ is both relatively simple and compatible with $v$ . Logarithmic and inverse trigonometric functions appear first in the list because their derivatives are algebraic, so if $v$ is algebraic, $v\,du$ is algebraic and an integration with ""weird"" functions is transformed into one that is completely algebraic. Note that the LIATE approach does not always work, but in many cases it can be helpful. I tried to use this approach on a relatively simple integral, which is $$\int (\ln x)^{2}\,dx.$$ I am quite confused whether $u$ should be $\ln x$ or $(\ln x)^{2}$ . Is there a more refined way to integrate by parts? Edit to avoid confusion: I found the antiderivative of $(\ln x)^{2}$ , but I am asking for an ""easier"" way.","['integration', 'indefinite-integrals', 'calculus', 'soft-question']"
4134172,Angles inside an equilateral triangle,"We have a point $P$ inside the equilateral triangle $\triangle ABC$ , such that $\angle PAC = x$ , $\angle PCA = 3x$ and $\angle PBC = 2x$ . Find the value of $x$ in degrees. I solved the problem in GeoGebra, and I know the value for $x$ is $6°$ . Also I solved this problem by using the Trigonometry Ceva's Theorem: $$
\sin(60°-2x)\sin(x)\sin(60°-3x)=\sin(2x)\sin(60°-x)\sin(3x)
$$ Where I used WolframAlpha to get $x=6°$ . However, I'm looking for a geometric solution. This is what I have reacher so far. First, draw the circle that lies over $B$ , $P$ and $C$ . Then extend sides $AB$ and $AC$ to get point $E$ and $D$ (we have another equilateral triangle $\triangle AED$ ). Due to properties of angles inscribed in a circle $\angle PDC = 2x$ and $\angle PED = 3x$ . Then trace the bisector $DG$ , we get that $\angle GDP=30°-2x$ and $\angle PGD = 3x$ . Here I got stuck. I think, I should proof that $GP = PD$ , but I don't know how. I would appreciate any contribution to this solution, or if you have a different approach and solution I would glad to hear it.",['geometry']
4134211,Intersection of a conic (only 5 points known) and a line [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I have 5 points of a conic. And a given line. I have to construct from these the intersection(s) of the conic and the line.
From 5 points I can construct more points of the conics. But I am not sure how to construct the points of the intersection of the conic (given by 5 points only) and the line. Could you help me construct their intersection?","['projective-geometry', 'geometric-construction', 'conic-sections', 'geometry', 'abstract-algebra']"
4134222,Show that the sum $\sum_{k=0}^nC_n^k x^k (1-x)^{n-x}(-1)^k f(\frac{k}{n})$ tends to $0$ when $n\to \infty$,"For $f\in C[0,1]$ show that for all $x\in[0,1]$ $\sum_{k=0}^nC_n^k x^k (1-x)^{n-x}(-1)^k f(\frac{k}{n})\to 0$ when $n\to \infty$ I don´t have a idea of how start with the proof, for one hand since our sum is the Bernstein polynomial with a extra $(-1)^k$ I think that must be $0$ since the Bernstein polynomial is a approach of our $f$ and therefore when we take all the terms of the approximation the error must be $0$ . For the other hand I need how fix the problem of the extra term $(-1)^k$ I think that we Can use the absolute value of our sum and find that these limit are $0$ for conclude that our original sum tends to $0$ when $n\to \infty$ . And basically I don´t know how I should continue with a formal proof of this. I belive that I should calculate $$\lim_{n\to \infty}\left( \sum_{k=0}^nC_n^k x^k (1-x)^{n-x}(-1)^k f\left(\frac{k}{n}\right)\right)$$ or find $N\in \mathbb{N}$ such that $$\left|\sum_{k=0}^nC_n^k x^k (1-x)^{n-x}(-1)^k f\left(\frac{k}{n}\right)\right|<\varepsilon$$ Attempt Consider only the index $k\equiv 0\pmod 2$ then we should have $$\left| \sum_{k=0}^nC_n^{2k} x^{2k} (1-x)^{n-x} f\left(\frac{2k}{n}\right)\right|\leq \sum_{k=0}^{n}\left|f\left( \frac{2k}{n}\right) \right|C_{n}^{2k}x^{2k}(1-x)^{n-2k}$$ since the right sumand is at most $1$ then $$\left| \sum_{k=0}^nC_n^{2k} x^{2k} (1-x)^{n-x} f\left(\frac{2k}{n}\right)\right|\leq \sum_{k=0}^{n}\left|f\left( \frac{2k}{n}\right) \right|$$ but when $n\to \infty$ $\frac{2k}{n}\to 0$ and hence since $f(0)=p_n(1)=0$ $$\left| \sum_{k=0}^nC_n^{2k} x^{2k} (1-x)^{n-x} f\left(\frac{2k}{n}\right)\right|\leq \sum_{k=0}^{n}\left|f\left( \frac{2k}{n}\right) \right|=0$$ Finally the case when $k\equiv 1 \pmod{2}$ is analogous.","['limits', 'approximation', 'real-analysis']"
4134229,Find max/min value of a multivariable function,"Determine the maximum and minimum value of: $$x^2+5y^2-4x$$ in the region: $$x^2+y^2<=1 $$ and $$y>=0$$ I was trying to do this question. Firstly I found the gradient and put it equal to 0 and found the point (2,0). However, this point is not in the region and could not be used. So from here, I'm not quite sure how to continue. I do know it has something to do with the edges of the region, to find the corner points and maybe somehow continue from there, but I'm not quite sure.
Thanks","['multivariable-calculus', 'calculus']"
4134259,Find the probability that one card is king and the other is heart.,Two cards are dealt from an ordinary deck of 52 cards (the sampling is without replacement). Find the probability that one card is king and the other is heart. I'm having trouble figure out how to deal with the case where you pick up a king of hearts. Normally I would just multiple the separate probabilities together but as they are not independent? I'm not entirely sure how to proceed.,"['statistics', 'card-games', 'probability']"
4134265,Relation between the fundamental group and projective representations of a Lie group,"For concreteness let's focus on $SO(3)$ . A projective representation is such that, $$U(g_1) U(g_2) = e^{i\theta(g_1, g_2)} U(g_1g_2), \ \ g_1, g_2 \in SO(3).$$ What I'm struggling to fully appreciate is the relation between the phase factor $\theta(g_1, g_2)$ and the fundamental group $\pi_1(SO(3)) = \mathbb Z_2$ . Reading this answer here , there seems to be a connection between $\pi_1(SO(3)) = \mathbb Z_2$ and the kind of projective representations $SO(3)$ can have. I'm not asking how to construct $\pi_1(SO(3))$ , but rather how knowing $\pi_1(SO(3))$ we can know something about $\theta(g_1, g_2)$ . The relationship seems to suggest the following, which are also my questions about this: Can we write $\theta(g_1, g_2)$ as the integration of some quantity over a line connecting $g_1$ , and $g_2$ ? Or more formally $\theta(g_1, g_2) = {\int_{\gamma} A}$ , such that $\gamma$ is a path connecting $g_1$ , and $g_2$ . If the above is true, then we would need to show that the integral is invariant under a smooth change of $\gamma$ , or that $\text{curl} \ A = 0$ . How to show this? I have seen arguments about how projective representations of $SO(3)$ has to correspond to linear representations of the universal cover $SU(2)$ , a statement that I'm also not sure how to prove.","['group-theory', 'representation-theory']"
4134295,Is this set path-connected?,"Consider $\left\{(e^{-x}\cos x, e^{-x}\sin x)\in \mathbb{R}^2: x\geq 0 \right\} \cup \left\{(0,0) \right\}.$ I need to see if this set is path connected. My guess is it is not primarily Because of the point $(0,0)$ . There needs to be a path from $(0,0)$ to any other point in the set. Since the other points are of the form $(e^{-\alpha}\cos \alpha, e^{-\alpha}\sin \alpha)$ for some $\alpha\geq 0,$ I must have that there is some $\alpha$ that will put the point $(e^{-\alpha}\cos \alpha, e^{-\alpha}\sin \alpha)$ arbitrarily close to $(0,0)$ . Since, $\cos x$ and $\sin x$ are never zero at the same time we need to send $\alpha$ to $\infty.$ What we seem to have is the closure of the set $$\left\{(e^{-x}\cos x, e^{-x}\sin x)\in \mathbb{R}^2: x\geq 0 \right\}.$$ Am I going in the right direction? How would I prove that there is no path from $(0,0)$ to any other point in $\left\{(e^{-x}\cos x, e^{-x}\sin x)\in \mathbb{R}^2: x\geq 0 \right\}$ ?","['connectedness', 'path-connected', 'real-analysis', 'limits', 'general-topology']"
4134300,Not understanding the logic behind $2<k+1$ in Induction Proof Problem,"$2^k+1 = 2 · 2^k$ $< 2 · k!$ $< (k + 1)k!$ (because $2 < k + 1$ ) $= (k + 1)!$ (by definition of factorial function.) I am not getting the third line. where did this $2 < k + 1$ appear. This is from the book Discrete Mathematics and Its Application by Kenneth H Rosen chapter 5.1 Mathematical Induction (7th ed) Example 6. Full Problem and it's Solution given below: Use mathematical induction to prove that $2^n < n!$ for every integer $n$ with $n ≥ 4$ . (Note that this inequality is false for $n = 1, 2, and$ $3$ .) Solution: Let $P(n)$ be the proposition that $2^n < n!$ . BASIS STEP: To prove the inequality for $n ≥ 4$ requires that the basis step be $P (4)$ . Note that $P (4)$ is true, because $2^4 = 16 < 24 = 4!$ INDUCTIVE STEP: For the inductive step, we assume that $P (k)$ is true for an arbitrary integer $k$ with $k ≥ 4$ . That is, we assume that $2^k < k!$ for the positive integer $k$ with $k ≥ 4$ . We must show that under this hypothesis, $P (k + 1)$ is also true. That is, we must show that if $2^k < k!$ for an arbitrary positive integer $ k$ where $k ≥ 4$ , then $2^k+1 < (k + 1)!$ . We have $2^k+1 = 2 · 2^k$ (by definition of exponent) $< 2 · k!$ (by the inductive hypothesis) $< (k + 1)k!  $ (because $2 < k + 1$ ) $= (k + 1)!$ (by definition of factorial function.) This shows that $P (k + 1)$ is true when $P (k)$ is true. This completes the inductive step of the proof. We have completed the basis step and the inductive step. Hence, by mathematical induction $P (n)$ is true for all integers $n$ with $n ≥ 4$ . That is, we have proved that $2^n < n!$ is true for all integers $n$ with $n ≥ 4$ .","['induction', 'discrete-mathematics']"
4134316,Possible pattern involving $x$ in the continued fraction expansion of $\frac{1}{\sqrt[3]{x^3+1}-x}$,"Consider the expression $$\frac{1}{\sqrt[3]{x^3+1}-x}$$ Plugging in $10$ for $x$ and using $W|A$ , we find that the continued fraction expansion is $[300; \mathbf{10}, 450, 8, ...]$ . For $x=11$ , it's $[363; \mathbf{11}, 544, ...]$ . The pattern continues (I have only checked up to $21$ , but I assume that it's true) that the second term in the continued fraction expansion is $x$ . I have two questions: Why does this pattern occur and how do we prove that it occurs? Is there a similar pattern with later terms? I haven't found an answer to 2, and I've had no luck as of yet for 1. All I have done so far is simplify it to $x\sqrt[3]{x^3+1}+\sqrt[3]{(x^3+1)^2}+x^2$ . Thanks in advance.","['continued-fractions', 'algebra-precalculus']"
4134365,"Five digit numbers with each digit from the set {1, 2, 3, 4}: numbers with an even digit sum and numbers with more even than odd digits","This is a question in a book that I am studying, and I have attempted to answer it but got it wrong. There were two parts that I wrong. The question is about five digit numbers where the digits are 1, 2, 3, or 4. The first part asked how many numbers were such that the sum of the digits was even. My answer was $$
  \binom{5}{0}
(2^5) +
\binom{5}{2}
(2^2)(2^3)+
\binom{5}{4}(2^4)(2)
$$ My thought process was to add up all of the possible numbers with an even number of odd digits. $\binom{5}{0}$ is for choosing 0 out of the 5 digits to be odd, and the following $2^5$ is for the remaining 5 digits, each of which has two choices for even numbers. $\binom{5}{2}$ is for choosing 2 out of the 5 digits to be odd, followed by $2^2$ to represent the two odd digits having 2 odd numbers to choose from, and the last $2^3$ to represent the remaining 3 digits having 2 even numbers to choose from. The last terms works similarly. The second part asked how many numbers had more even digits than odd digits. My thought process here was that I needed at least 3 even digits, so I needed to choose 3 digits out of 5, assign them even digits, and then freely assign the remaining 2 digits. So, my answer was $$
\binom{5}{3}(2)(4^2)
$$ The book's answer for the first part was to choose 4 digits without restriction and then choose an even or odd last digit to make the sum even: $$(4^4)(2)$$ For the second part, it added up the all 3 even-digit numbers, 4 even-digit numbers, and 5 even-digit numbers: $$
  \binom{5}{3}
(2^3)(2^2) +
\binom{5}{4}
(2^4)(2)+
\binom{5}{5}(2^5)
$$ I understand why the book's answers work, but I'm not sure why my answers do not work. How should I have approached these questions?","['combinatorics', 'discrete-mathematics']"
4134386,Why is $\partial f(x)/\partial x_i$ homogeneous of degree $k-1$ if $f(x)$ is homogeneous of degree $k \geq 1$?,"It's as (simple) as the title says, but I can't figure out why that is. How can I show (I think I need small, tiny steps) that $$\frac{\partial f(tx)}{\partial (tx_i)}t=t^k\frac{\partial f(x)}{\partial x_i}$$ ? My textbook says ""If $f(x)$ is homogeneous of degree $k \geq 1$ , then $\partial f(x)/\partial x_i$ is homogeneous of degree $k-1$ "". How come? Where f is a function $f:\mathbb{R}_+^n \mapsto \mathbb{R}$","['economics', 'functions', 'special-functions']"
4134403,Convergence of $ \sum\limits_{n=1}^{\infty}\frac{1}{n(n+m)} $,"Let $ m \in \mathbb{N} $ . Find to what does the series $ \sum\limits_{n=1}^{\infty}\frac{1}{n(n+m)}  $ converge to My attempt: ( I did as discussed in Infinite Series $\sum 1/(n(n+1))$ ) Let $ N>m $ . $ 
\begin{align}
S_N & = \sum\limits_{n=1}^{N}\frac{1}{n(n+m)}  = \frac{1}{m}\sum_{n=1}^N \left(\dfrac1n - \dfrac1{n+m}\right) = \frac{1}{m}( \sum_{n=1}^N \dfrac1 n - \sum_{n=1}^N \dfrac1{n+m}) = \frac{1}{m}(\sum_{n=1}^N \dfrac1n - \sum_{n=m+1}^{N+m} \dfrac1{n} )\\
& = \frac{1}{m}(\sum_{n=1}^m \dfrac1n + \sum_{n=m+1}^N \dfrac1n - \sum_{n=m+1}^N\dfrac1n -  \sum_{n=N+1}^{N+m}\dfrac1n ) = \frac{1}{m}(\sum_{n=1}^m \dfrac1n  -  \sum_{n=N+1}^{N+m}\dfrac1n ) 
~~\text{ [ from here I got stuck] } \end{align} 
 $","['limits', 'sequences-and-series', 'real-analysis']"
4134527,Large Deviation bound for sum of i.i.d Variables with positive expectation,"Suppose that $X_1, X_2, ... $ are i.i.d copies of some real random variable $X_1$ which has finite exponential moment and positive expectation. I am asked to show that there exists some $k > 0$ such that $$\mathbb{P}(X_1 + ... + X_n \leq kn) \leq e^{-kn}$$ for all $n$ . My first attempt was to apply the Chernoff bound, but that seems to give me a bound of $\inf_{t>0} e^{tkn} (\mathbb{E}e^{-tX_1})^n$ , and I don't see where to proceed from here.","['inequality', 'large-deviation-theory', 'probability-theory']"
4134536,Proving invertibility of $A^{T}A$,"Take the matrix $A$ , which is $m\times n$ and of rank $n$ . Hence, a full column-rank matrix. I need to show that $N(A^{T}A) = N(A)$ , and deduce that $A^{T}A$ is invertible. Since A is a full column-rank matrix, I see that $dim \space N(A) = n-r = n-n =0$ , since $r=n$ . Howver, I'm not sure how to characterize the null-space of $N(A^{T}A)$ in order to arrive at $N(A^{T}A) = N(A)$ . Once this is done, I see how $N(A) = \vec{0}$ implies the invertibility of $A^{T}A$ .","['analysis', 'matrices', 'inverse', 'linear-algebra', 'projection-matrices']"
4134574,Finding vectors in orthogonal complements to create a unique sum,"Take U,W to be subspaces of $\mathbb{R}^{3}$ $U = \operatorname{Lin}\left\{\left(\begin{array}{c}
-1 \\
0 \\
1
\end{array}\right),\left(\begin{array}{l}
1 \\
1 \\
1
\end{array}\right)\right\}$ and $V=\operatorname{Lin}\left\{\left(\begin{array}{l}
1 \\
-2 \\
1
\end{array}\right)\right\}$ . Also : $U \oplus V  = \mathbb{R}^{3}$ . I need to find vectors $u \in U$ and $x \in U^{\perp}$ , such that $\left(\begin{array}{l}
1 \\
1 \\
0
\end{array}\right)$ = $u +x$ That is, find a choice of $u$ and $x$ that is unique and that gives me this vector. Now, my strategy is to find a orthogonal projection matrix that sends $\mathbb{R}^{3}$ to $U$ . This should give me a vector in $U$ such that when added to a vector $x \in V$ , say $\left(\begin{array}{l}
1 \\
-2 \\
1
\end{array}\right)$ , I can obtain the required vector (in a unique way). But I if a build a projection matrix P = $U(U^{T}U)^{-1}U^{T}$ , and then take $u' = P  \left(\begin{array}{l}
1 \\
1 \\
0
\end{array}\right)$ , where $U = \left[\begin{array}{cc}
-1 & 1 \\
0 & 1 \\
1 & 1
\end{array}\right]$ ... it doesn't quite give me the vector that I need. This is is my inital strategy. What am I missing ? What is the right way to approach this ?","['analysis', 'orthogonal-matrices', 'linear-algebra', 'linear-transformations', 'projection-matrices']"
4134575,Hitting time for 2-dimensional brownian motion to hit the unit circle,"Let $\tau=\inf\{t : ||W_t||=1 \}$ for a two dimensional brownian motion $W_t$ .  We want to find the expectation of $\tau$ . My attempt:  So far I know that $X_t=(W_t^1)^2+(W_t^2)^2-2t$ is a martingale and $\tau$ is a stopping time for $X_t$ , so would I just say that $0=E(X_0)=E(X_{\tau})=1-2E(\tau)$ and conclude $E(\tau)=\frac{1}{2}$ ?","['statistics', 'probability-theory', 'probability']"
4134609,"Central Limit Theorem in a win, lose or stay the same scenario","So, I have this problem: Robert bets on a casino game where he estimates that he has a 0.45 probability of winning, a 0.5 probability of losing, and a 0.05 probability of being left with no profit or loss. In each game she bets \$10, which implies that she can win \$10, lose \$10 or stay as he was, with the odds described. If Robert plays this game 100 times independently. What is the approximate probability that he ends up with more money than he started (assuming she has enough money for the first 100 plays)? My attempt: Let $X_i$ be the money he gains or losses in each round. And let $Y=X_1+...X_{100}$ . Then I have $$E(X_i)=10(.45)+(-10)(.5)+0(.05)=-.5$$ $$E(X_i^2)=10^2(.45)+(-10)^2(.5)+0^2(.05)=95$$ $$Var(X_i)=E(X_i^2)-E(X_i)^2=95-(-.5)^2=94.75$$ Then $$E(Y)=100E(X_i)=-50$$ $$Var(Y)=100Var(X_i)=9475$$ Now, if I want to calculate the  probability that he ends up with more money than he started I have to calculate $P(Y>1000)$ . My problem is when I apply the CLT because $P(Y>1000)=1-P(Y\leq1000)=1-P(\frac{Y+50}{\sqrt{9475}}\leq\frac{1000+50}{\sqrt{9475}})=1-\phi(10.78)$ And using a Standard Normal Table that is impossible. So, any help, suggestion or ideas are welcomed and appreciated.","['central-limit-theorem', 'probability-theory', 'probability']"
4134631,Local limit theorems for circular/spherical distributions,"Here are some of the classical density functions for spherical distributions (on the $\mathcal{S}^{d-1}$ sphere, living in the Euclidean space $\mathbb{R}^d$ ): $$\mathbf{x}\mapsto \frac{(\kappa/2)^{d/2-1}}{2 \pi^{d/2} I_{d/2-1}(\kappa)} \exp(\kappa \mathbf{x}^{\top} \boldsymbol{\mu}), \qquad (\text{called the Fisher-von Mises-Langevin density}),$$ $$\mathbf{x}\mapsto \frac{1}{a(\kappa,A)} \exp(\kappa \mathbf{x}^{\top} \boldsymbol{\mu} + \mathbf{x}^{\top} A \mathbf{x}), \qquad (\text{called the Fisher-Bingham density}),$$ $$\mathbf{x}\mapsto \frac{\Gamma(d/2)}{2 \pi^{d/2} M(\frac{1}{2},\frac{d}{2},\kappa)} \exp(\kappa (\mathbf{x}^{\top} \boldsymbol{\mu})^2), \qquad (\text{called the Watson density}),$$ where $\kappa\geq 0$ is a concentration parameter, $\boldsymbol{\mu}\in \mathcal{S}^{d-1}$ is a location parameter, $A$ is a symmetric $d\times d$ matrix, and both $a(\kappa,A)$ and $M(\frac{1}{2},\frac{d}{2},\kappa)$ are the appropriate normalizing constants. I've seen very few central limit theorems in the literature relating to this setting.
In particular, I found absolutely nothing regarding local limit theorems.
If the parameter $\kappa$ approaches some limit ( $0$ or $\infty$ ), do any of these density functions approach a particular limit density (with a properly normalized argument)? $\textbf{Example:}$ As the intensity parameter $\lambda$ of a Poisson $(\lambda)$ distribution tends to $\infty$ , the probability mass function tends to the density of a $\text{Normal}(\lambda,\lambda)$ distribution. Is there any analogous results/conjectures in the context of spherical distributions?","['probability-limit-theorems', 'statistics', 'central-limit-theorem', 'probability']"
4134658,Uncountable chains in an uncountable partially ordered set,"Let $(S,<)$ be a partially ordered set such that $S$ is uncountable and has the property that for all $x\in S$ there exists a $y\in S$ such that $y<x$ . Is it always true that $S$ contains an uncountable chain (with or without assuming the axiom of choice)? My basic approach would be to start with any $x_0\in S$ then pick a $x_1$ guaranteed by the condition such that $x_1<x_0$ and repeat the process indefinitely. However, I don't know if this is valid because this process is sequential and uncountable sets are not. I know that in assuming the axiom of choice, there would also be a well ordering on $S$ but this might not help at all because the poset may not extend to one. Any suggestions or counterexamples?","['elementary-set-theory', 'order-theory', 'combinatorics', 'axiom-of-choice']"
4134755,Name of this polyhedron with 17 faces,"I recently saw a jungle gym in a playground constructed with ropes and small vertices. It formed a really strange polyhedron that I had never seen before. I've searched all over for it, but cannot find it. It: Is convex Consists of 2 pentagons, 10 hexagons, 5 squares, 17 faces total Appears that each polygon is regular (or approximately so) The polyhedron is constructed as follows: Start with a regular pentagon. Attach a regular hexagon to each edge of the pentagon. Fold each hexagon so that it meets with its neighboring hexagon. Mark the free edge of each hexagon that is parallel to the pentagon. Construct a copy of the above. Connect the two halves together where the hexagons are marked. The space between the connected hexagons should form squares","['solid-geometry', 'polyhedra', 'geometry', 'terminology']"
4134775,"Is an algebraic set, all of whose irreducible components are projective varieties, projective too?","Let $X$ be an algebraic set (that is, a non-necessarily irreducible algebraic variety) over a field $k$ . In the language of schemes, let us say that $X$ is a reduced separated scheme of finite type over $\mathrm{Spec}(k)$ . Then $X$ has a finite number of irreducible components which we denote by $X_i$ for $1\leq i \leq r$ . Each of the $X_i$ , with the reduced scheme structure, is a closed subscheme of $X$ and a variety over $k$ . We assume that they are furthermore all projective. Does it follow that $X$ is projective too ? Sure thing, $X$ should be a proper scheme, as the universal closeness of the structure morphism $X\to \mathrm{Spec}(k)$ should follow from that of the irreducible components, since they are in finite number. What about quasi-projectiveness though ?","['algebraic-geometry', 'schemes']"
4134784,"If $B = R^TR$ and $B$ is symmetric positive definite, then $R$ is invertible","I have a problem in my textbook: Assume B is symmetric positive semi-definite. Show that $B=R^TR$ for some square matrix $R$ that is not necessarily symmetric, and that if $B$ is symmetric positive definite, then $R$ is invertible. My approach: Since $B$ is PSD and symmetric, it is diagonalizable, so we can write $B=V\Lambda V^T$ . So, $B=V\Lambda V^T = B=V\Lambda^{1/2}\Lambda^{1/2} V^T = (\Lambda^{1/2}V^T)^T\Lambda^{1/2}V^T=R^TR$ for $R=\Lambda^{1/2}V^T$ Now, for the second part, i'm not quite sure if i'm doing it right: If $B$ is PD, then all eigenvalues are positive. Hence, $R= \begin{bmatrix} v_{11}\sqrt{\lambda_1} & 0 &...&0 \\ 0 & v_{22}\sqrt{\lambda_2} & ... & 0\\0 &...&...&v_{nn}\sqrt{\lambda_n}  \end{bmatrix}$ And it has an inverse: $R^{-1}= \begin{bmatrix} 1/v_{11}\sqrt{\lambda_1} & 0 &...&0 \\ 0 & 1/v_{22}\sqrt{\lambda_2} & ... & 0\\0 &...&...&1/v_{nn}\sqrt{\lambda_n}  \end{bmatrix}$ With which $R^{-1}R=I_n$ . Hence, $R$ is invertible. I am not sure whether the second part is rigorous, or if it's even correct. Any help would be appreciated.","['matrices', 'solution-verification', 'eigenvalues-eigenvectors']"
4134804,Binomial approximation,"A fair die is rolled 800 times. Find the probability to get a 6 at least 150 times. My attempt: The probability to get exactly 150 6's is: $\binom{800}{150}\left(\frac16\right)^{150}\left(\frac56\right)^{800-150}$ The required probability is the sum of getting 150, 151 etc. I understand we must use some approximation here, because we can't do these calculations one by one. Can you please help me?","['probability-distributions', 'combinatorics', 'probability']"
4134839,Continuity and Differentiability at end point of an interval,"Let $ f(x) = x(\sqrt {x} + \sqrt{x+1})  $ The problem had asked to check continuity and Differentiability at $ x = 0 $ First of all I noticed that the Natural Domain of the function is non negative Real Numbers i.e zero is the least point of interval Secondly I noted that $f(0)=0 $ And that the Right Hand Limit is also zero , implying that $f(x)$ is continuous at $0$ . My lecturer had told that to check Differentiability on the end point of an interval we need that the Right Hand Derivative must be finite . When I evaluate the  Right Hand Derivative it comes to be equal to $1$ (and hence finite, and therefore differentiable ) $$  \lim_{h\to 0} \frac {f(0+h) - f(0)}{h} (where  h>0) $$ Which evaluates to $$ \lim_{h\to 0} \frac {h(\sqrt {h} + \sqrt {1+h}) - 0}{h} , $$ the h cancels out and we are left with 1 . But the problem book says that the function is not differentiable at $x=0$ as it not defined from the Left side . I am confused on which is correct ?","['continuity', 'calculus', 'derivatives']"
4134845,Uniqueness of a Complete Minimal Surfaces,"Let $R$ be a complete Riemannian Manifold of dimension $n$ , let $S \subseteq R$ be homeomorphic to the sphere of dimension $k < n$ , and let $M$ be a minimal surface of $R$ which is homeomorphic to the ball of dimension $k + 1$ and has $S$ as its boundary.  If we allow self-intersections, does this minimal surface extend to some unique complete minimal surface?  If $k = 0$ , then $S$ is a pair of points and our $M$ is a geodesic connecting these two points, and this geodesic can be extended past either point in a unique way by moving ""straight"" past the points, and doing this process indefinitely leads to a unique geodesic which is either topologically a line or a circle (possibly with self-intersections), both of which are complete.  I was wondering if this idea extends for higher dimensions. Perhaps the requirements about the boundary being homeomorphic to a $k$ -sphere and the minimal surface being homeomorphic to a $k+1$ -ball could be laxed.","['minimal-surfaces', 'riemannian-geometry', 'differential-geometry']"
4134879,On the construction of a measure $\mu$ on $\mathbb R^n$ by repeated subdivision,"Main Theme of the Question(s): Fractal Geometry by Kenneth Falconer describes the following method to construct a mass distribution $\mu$ (and later, a measure on $\mathbb R^n$ ) on a subset of $\mathbb R^n$ . The author also says that we only concern ourselves with Borel sets. I need help understanding (i) the process of construction of this mass-distribution/measure, and (ii) the proof of a proposition that follows this discussion. Slightly paraphrased from the book: The following method is often used to construct a mass distribution on a subset of $ℝ^n$ . It involves repeated subdivision of a mass between parts of a bounded Borel set $E$ . Let $\mathcal{E}_0$ consist of the single set $E$ . For $k = 1, 2, . . . $ , we let $\mathcal{E}_k$ be a collection of disjoint Borel subsets of $E$ such that each set $U$ in $\mathcal{E}_k$ is contained in one of the sets of $\mathcal{E}_{k−1}$ and contains a finite number of the sets in $\mathcal{E}_{k+1}$ . We assume that the maximum diameter of the sets in $\mathcal{E}_k$ tends to $0$ as $k \to \infty$ . We define a mass distribution on $E$ by repeated subdivision. We let $\mu(E) \in (0,\infty)$ . In general, we assign masses so that $$\sum_i\mu(U_i) = \mu(U)$$ for each set $U$ of $\mathcal{E}_k$ , where $\{U_i\}$ are disjoint sets in $\mathcal{E}_{k+1}$ contained in $U$ . For each $k$ , let $E_k = \bigcup_{A\in \mathcal{E}_k} A$ , and we define $\mu(A) = 0$ for all $A$ with $A\cap E_k = \varnothing$ . Now the author states a proposition, which enables us to determine $\mu(A)$ for any Borel set $A$ . Proposition $1.7.$ Let $𝜇$ be defined on a collection of sets $\mathcal E$ as above. Then the definition of $𝜇$ may be extended to all subsets of $ℝ^n$ so that $𝜇$ becomes a measure. The value of $𝜇(A)$ is uniquely determined if $A$ is a Borel set. The support of $𝜇$ is contained in $E_∞ = \bigcap_{k=1}^\infty \overline{E_k}$ . Note on Proof. If $A \subset \mathbb R^n$ , let $$\mu(A) = \inf\left\{\sum_{i=1}^\infty \mu(U_i): A \cap E_\infty \subset \bigcup_{i=1}^\infty U_i \text{ and } U_i \in \mathcal{E}\right\}$$ It is not difficult to see that if $A\in\mathcal E$ , then $\mu(A)$ above reduces to the mass $𝜇(A)$ specified in the construction. The complete proof that $𝜇$ satisfies all the conditions of a measure
and that its values on the sets of $\mathcal E$ determine its values on the Borel sets is quite
involved, and need not concern us here. As $𝜇(ℝ^n\setminus E_k) = 0$ , we have $𝜇(A) = 0$ if $A$ is an open set that does not intersect $E_k$ for some $k$ , so the support of $𝜇$ is in $E_k$ for all $k$ . I have six questions: In the proposition above, why is $\mu(A)$ defined the way it is (to extend $\mu$ to sets other than those in $\mathcal E$ )? How do I show that $\mu$ as defined above is in fact a measure on $\mathbb R^n$ ? The author says the proof is involved, but I want to see it. I need to prove three things: (a) $\mu(\varnothing) = 0$ , (b) $A \subset B \implies \mu(A) \le \mu(B)$ , (c) $\mu\left(\bigcup_{i=1}^\infty A_i\right) \le \sum_{i=1}^\infty \mu(A_i)$ with equality if $A_i$ 's are disjoint Borel sets. Here's what I've tried: (a) $$\begin{align}
\mu(\varnothing) &= \inf\left\{\sum_{i=1}^\infty \mu(U_i): \varnothing \cap E_\infty \subset \bigcup_{i=1}^\infty U_i \text{ and } U_i \in \mathcal{E}\right\}\\
&= \inf\left\{\sum_{i=1}^\infty \mu(U_i): U_i \in \mathcal{E}\right\}
\end{align}$$ Why should $\inf\left\{\sum_{i=1}^\infty \mu(U_i): U_i \in \mathcal{E}\right\}$ be $0$ ? I'm not sure if $\varnothing\in\mathcal E$ . (b) Suppose $A\subset B$ . Then, $A\cap E_\infty \subset B\cap E_\infty$ . We have $$\mu(A) = \inf\left\{\sum_{i=1}^\infty \mu(U_i): A \cap E_\infty \subset \bigcup_{i=1}^\infty U_i \text{ and } U_i \in \mathcal{E}\right\}$$ and $$\mu(B) = \inf\left\{\sum_{i=1}^\infty \mu(U_i): B \cap E_\infty \subset \bigcup_{i=1}^\infty U_i \text{ and } U_i \in \mathcal{E}\right\}$$ and from what I understand - $A\cap E_\infty \subset B\cap E_\infty$ directly implies $\mu(A) \le \mu(B)$ . This is due to the fact that any cover $\bigcup_{i=1}^\infty U_i$ of $B\cap E_\infty$ is also a cover of $A\cap E_\infty$ , but the converse is not true in general. Since $A\cap E_\infty$ can possibly have more covers of the form $\bigcup_{i=1}^\infty U_i$ for $U_i\in\mathcal E$ , the infimum of $\sum_{i=1}^\infty \mu(U_i)$ over covers of $A\cap E_\infty$ is in general $\le$ the infimum of $\sum_{i=1}^\infty \mu(U_i)$ over covers of $B\cap E_\infty$ . Does this make sense? (c)We have $$\mu(A_i) = \inf\left\{\sum_{i=1}^\infty \mu(U_i): A_i \cap E_\infty \subset \bigcup_{i=1}^\infty U_i \text{ and } U_i \in \mathcal{E}\right\}$$ and $$\mu\left(\bigcup_{i=1}^\infty A_i\right) = \inf\left\{\sum_{i=1}^\infty \mu(U_i): \bigcup_{i=1}^\infty A_i \cap E_\infty \subset \bigcup_{i=1}^\infty U_i \text{ and } U_i \in \mathcal{E}\right\}$$ Clearly, $A_i \subset \bigcup_{i=1}^\infty A_i$ , and from the previous part, i.e. (b), we have $\mu(A_i) \le \mu\left(\bigcup_{i=1}^\infty A_i\right)$ . I'm not sure how this helps, and I'm unable to proceed further. Why does $E_∞ = \bigcap_{k=1}^\infty \overline{E_k}$ contain $\operatorname{spt}\mu$ ? $\color{blue}{\text{Answered.}}$ As $\mu(\mathbb R^n \setminus E_k) = 0$ , by monotonicity, we have $\mu(\Bbb R^n\setminus \overline{E_k}) = 0$ . By the definition of support, $\operatorname{spt}\mu \subset \overline{E_k}$ for all $k$ . So, $\operatorname{spt}\mu \subset E_\infty = \bigcap_{k=1}^\infty \overline{E_k}$ . If $A\in\mathcal E$ , how does $\mu(A)$ (given by the wild expression in the proposition) reduce to the construction of $\mu(A)$ (i.e. the mass distribution earlier)? How do we know that the value of $\mu$ on sets of $\mathcal E$ determines $\mu$ uniquely on all Borel sets? What is the point of all this? What have we achieved? I need help understanding the bigger picture! Thanks a lot for reading, and for your help (in advance)! References: Kenneth Falconer. Fractal Geometry.","['measure-theory', 'borel-measures', 'analysis', 'real-analysis', 'borel-sets']"
4135016,EM algorithm for maximum of 2 normal distribution,"Let $X_i \sim N(\mu_1,\sigma^2), Y_i \sim N(\mu_2,\sigma^2)$ $O_i = \max(X_i,Y_i)$ i need to find $\mu_1, \mu_2$ using EM my attempt: first i defined $Z_i = \left\{\begin{matrix}
1, ~ X_i \ge Y_i
\\
0, ~ X_i < Y_i
\end{matrix}\right.$ we can assume we know Z there for: $O_i = X_i*Z_i + Y_i * (1-Z_i)$ $L(O,Z) = \Pi f_x^{z_i} f_y^{z_i} p^{z_i} (1-p)^{1-z_i} \Rightarrow l :=log-likelihood = ∑z_i  ln⁡(p)+n-∑z_i  ln⁡(1-p)+nln(2√π  σ)-\frac{(∑(o_i-μ_1 )^2 z_i+ (o_i-μ_2 )^2 (1-z_i)}{2σ}  $ where $ p = P(Z=1) = P(Y-X\le0) =P(N(μ_2-μ_1,2 σ^2)≤0)$ E stage: let $e_i=E(Z_i |O_i=k)$ $Q = E(l│O)=∑e_i  ln⁡(p)+n-∑e_i  ln⁡(1-p)+nln(2√π  σ)-\frac{(∑(o_i-μ_1 )^2 e_i+ (o_i-μ_2 )^2 (1-e_i)}{2σ}$ M stage: $\frac{\partial Q}{\partial \mu_1 } = \frac{\sum(O_i + \mu_1)e_i}{\sigma}  = 0 \iff \mu_1 = \frac{\sum O_i e_i }{\sum e_i}$ $\frac{\partial Q}{\partial \mu_2 } = \frac{\sum(O_i + \mu_2)(1-e_i)}{\sigma}  = 0 \iff \mu_2 = \frac{\sum O_i (1-e_i) }{\sum (1-e_i)}$ i do does stages until Q starts to converge i tried this with some given values but the results were wrong, where did i go wrong in my calculations? who should i solve this?","['statistics', 'conditional-expectation', 'machine-learning', 'algorithms', 'optimization']"
4135029,Invariant functions for irreducible representations of $\mathrm{SU}(2)$.,"The orbits of $\mathrm{SU}(2)$ acting irreducibly on $\mathbb{C}^2$ are three-spheres centered around the origin. In other words, an orbit is uniquely specified by the Euclidean norm in $\mathbb{C}^2$ . For the irreducible representation $\rho_n$ of $\mathrm{SU}(2)$ on $\mathbb{C}^n$ ( $n\geq 2$ ), the orbits will generally be three-dimensional so I expect there are $N=2n-3$ functions $f_1,\dots,f_{N}\colon\mathbb{C}^n\to \mathbb{R}$ which are invariant under $\rho_n$ and the orbits of $\rho_n$ are equal to $\{x\in \mathbb{C}^n\mid f_i(x)=c_i\}$ for some $c_1,\dots,c_N\in \mathbb{R}$ .
My question is if what I have said is correct and if so, what else can be said about the functions $f_i$ ?","['algebraic-groups', 'representation-theory', 'algebraic-geometry', 'linear-algebra', 'invariant-theory']"
4135034,Why $\lim\limits_{n \to +\infty} \bigg(\dfrac{n+1}{n+2}\bigg)^n = \frac{1}{e}$?,"I tried to solve this limit: $\lim\limits_{n \to +\infty} \bigg(\dfrac{n+1}{n+2}\bigg)^n$ . My approach was to re-write it as $\lim\limits_{n \to +\infty} \bigg(\dfrac{n}{n+2} + \dfrac{1}{n+2}\bigg)^n$ , and since $\dfrac{n}{n+2}$ tends to 1 and $\dfrac{1}{n+2} \sim \dfrac{1}{n}$ as $n \to +\infty$ , I figured the solution would be $e$ , as $\lim\limits_{n \to +\infty} \bigg(1+\dfrac{1}{n}\bigg)^n = e$ . I suppose I've done something wrong, since by plotting the function I noticed the solution is $\dfrac{1}{e}$ . Where is my error?",['limits']
4135039,"Size of an ""average"" $\epsilon$-net on the unit sphere","Let $\epsilon>0$ and consider constructing a set $S_\epsilon\subseteq S^{d-1}$ of points on the sphere $\{x\in\mathbb R^d \mid ||x||_2=1\}$ , such that $$
\mathbb E\left[\min_{y\in S_\epsilon}||x-y||_2^2\right]\le \epsilon,
$$ where $x$ is a random point on the sphere and the expectation is with respect to the choice of $x$ . How big must $S_\epsilon$ be? I'm looking for a lower bound, although an upper bound may be interesting as well. The motivation for this problem comes from an attempt to prove a lower bound on the number of bits that are needed to send a real-valued vector $x\in\mathbb R^d$ such that the receiver can estimate $x$ to within an ( $\ell_2$ squared) error of $\epsilon||x||_2^2$ . The formulation requires a few extra steps (including Yao's Minimax principle), but this is the component I'm missing.","['geometry', 'expected-value', 'combinatorics', 'upper-lower-bounds', 'probability']"
4135101,How to prove that $PA^2\sin A + PB^2 \sin B + PC^2 \sin C$ takes the same value for all points P on the incircle of the triangle $\triangle ABC$.,"Prove that $PA^2\sin A + PB^2 \sin B + PC^2 \sin C$ takes the same value for all points P on the incircle of the triangle $\triangle ABC$ . The previous part asks that If I is the incentre of the $\triangle ABC$ and $\alpha$ , $\beta$ , $\gamma$ are respectively the angles $BIC$ , $CIA$ and $AIB$ , prove that $$\frac{a \cdot IA}{\sin \alpha} = \frac{b \cdot IB}{\sin \beta} = \frac{c \cdot IC}{\sin \gamma}$$ And I have solve it with trigonometry, but I am not sure how to use it to prove the first statement. The general problem can be found here but I wonder if there is a simpler solution here since it only asks for points on the incircle. An idea is to use the fact that $$PA^2 \sin A = PA \cdot NL$$ etc. But I am not sure how to proceed.","['trigonometry', 'geometry']"
4135153,Proving the stability of the null solution of a differential equation,"so I'm starting to work on Lyapunov functions and I would like to use them to prove the stability of the null solution of the following differential equation: $$\frac{d}{dt}\left(\begin{array}{c}x\\y\end{array}\right) = \left(\begin{array}{c}- \sin(x) - y + sin(y)\\1 - \cos(x) - y + y^2\end{array}\right)$$ I know that for an equation $\frac{dx}{dt} = Ax$ the stability of the null solution can be proved by showing that all eigenvalues of $A$ have negative real part. However I'm not sure in the case above what is $A$ , because of the $cos$ and $sin$ functions inside the given matrix. Thus I was wondering if someone could help my finding this matrix, and since it will be a $2\times 1$ matrix how can I find its eigenvalues ? Moreover this is the only method I've seen to solve this problem, but if there is any other I'm willing to try it, there is no obligation on how to solve this problem. I'm thinking about methods like those in the following links: Stability of the null solution of system of differential equations , How to find a Lyapunov function? and How to pick a Lyapunov function and prove stability? (if they can be applied here of course). Thanks in advance for any help, have a good day!","['lyapunov-functions', 'proof-writing', 'derivatives', 'ordinary-differential-equations']"
4135207,"What does the efficacy of a vaccine mean, i.e. what does it model?","I am aware that the efficacy of a vaccine is calculated with $${\displaystyle VE={\frac {ARU-ARV}{ARU}}\times 100\%,} \text{with}\\
\text{VE=Vaccine efficacy}\\
\text {ARU= Attack rate of unvaccinated people}\\
\text {ARV= Attack rate of vaccinated people}$$ but besides echoing the definition of efficacy, I cannot explain even to myself what does it model. Initially I thought that the efficacy is the probability of you not getting the disease in question. However a close look at the formula reveals that efficacy cannot be a mathematical probability, since it can be a negative value: to my understanding during the pandemic negative efficacies have sometimes been reported regarding proposed vaccines. So what does efficacy model at an individual level, and what, if any, connection efficacy has to probabilities and probability theory?","['mathematical-modeling', 'probability']"
4135225,Eigenvalues of a matrix close to a tridiagonal Toeplitz matrix,"I am trying to find all the eigenvalues of $P$ defined below: $$P=\begin{bmatrix}
0.5&0.5&0&0&\cdots 0&0 \\
0.25&0.5&0.25&0&\cdots 0&0\\
0&0.25&0.5&0.25&\cdots 0&0\\
\vdots \\
0&0&0&0&\cdots 0.5&0.5
\end{bmatrix}_{n\times n}$$ So $P$ has $0.5$ along the main diagonal. It has $0.25$ on diagonals above and below the main diagonal except for the first and last row. Hence $P$ is not exactly a Toeplitz matrix. My attempt: A paper I'm looking at, gives eigenvalues of the following Toeplitz matrix $$
Q=\begin{bmatrix}
b&a \\
c&b&a \\
&\ddots&\ddots&\ddots \\
&&c&b&a \\
&&&c&b
\end{bmatrix}
$$ as $\lambda_j = b+2\sqrt{ca}\cos\left(\frac{j\pi}{n+1} \right)$ So I'm wondering if I will be able to find eigenvalues of $P$ even though its not a Toeplitz matrix precisely?","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
4135233,Prove that there exists no scalar field which has derivative $> 0$ at a fixed point wrt every direction,The problem statement : Prove that there is no scalar field $f$ such that $\nabla_\vec{y}f(\vec{a})>0$ for a fixed vector $\vec{a}$ and every non-zero vector $\vec{y}$ . Here $\nabla_\vec{y}f(\vec{a})$ represents the derivative of $f$ at $\vec{a}$ with respect to $\vec{y}$ . $$ \nabla_\vec{y}f(\vec{a})=\lim_{h\rightarrow 0}\frac{f(\vec{a}+h\vec{y})-f(\vec{a})}{h}$$ I proved it using that if $\vec{y}\neq0$ then $$\nabla_\vec{-y}f(\vec{a})=-\nabla_\vec{y}f(\vec{a})<0$$ This gives a contradiction. Is there any other way to prove this ?,"['scalar-fields', 'multivariable-calculus', 'derivatives', 'real-analysis']"
4135284,"Density of $XY$, where $X,Y$~$Unif(0,1)$","Let $X,Y$ be two independent random variables uniformly distributed in $(0, 1)$ . Compute the density of $W := XY$ . Hint: First compute the joint density of $(X, XY )$ We have the following denisties: $f_X(x)=\mathbb{1}_{(0,1)}(x)$ , $f_Y(y)=\mathbb{1}_{(0,1)}(y) \Rightarrow f_{X,Y}=\mathbb{1}_{(0,1)}(x)\mathbb{1}_{(0,1)}(y)$ Following the hint, let $W=XY$ and $V=X$ , then I define $g(x,y)=(x,xy)=(v,w)\Rightarrow g^{-1}(v,w)=(v,\frac{w}v)$ . Then the Jacobian is: $J_{g^{-1}}(v,w)=\begin{pmatrix} 1&0 \\-\frac{w}{v^2}&\frac1v \end{pmatrix} \Rightarrow \left|\det{J_{g^{-1}}(v,w)} \right|=\frac1{\left|v\right|} $ . Hence we have $f_{V,W}=f_{X,Y}(v,\frac{w}v)\frac1{\left|v\right|}\mathbb{1}_{V\not=0}$ .
Then the marginal should be $f_{XY}(xy)=f_W(w)=\int f_{X,Y}(v,\frac{w}v)\frac1{\left|v\right|}dv=\int f_{X,Y}(x,y)\frac1{\left|x\right|} dx=\int^1_0\frac1{\left|x\right|}dx=\infty$ I don't think this is the right answer, but I don't know what I am missing, can anybody please help me?","['probability-distributions', 'probability-theory', 'probability']"
4135359,(Reference request) Smoothness of desingularized polyharmonic Green kernel on closed manifolds,"Let $(M^n,g)$ be a closed Riemannian manifold with Laplace–Beltrami $\Delta_g$ . It is known that, for $s>0$ , the resolvent $\mathcal G_s$ of $(-\Delta_g)^s$ has integral kernel, say $G_s$ , satisfying $$\left| G_{n/2}(x,y)+ c_n \log d(x,y) \right| \leq C_g$$ for some (unique) universal constant $c_n>0$ and some constant $C_g=C_g(M)$ .
Is there some reference to the fact that the desingularized kernel $$(x,y)\longmapsto G_{n/2}(x,y)+c_n \log d(x,y)$$ is smooth around $x=y$ ? Note: it seems to me that this should follow from some facts in [1], but their arguments are somewhat sketchy, and I would rather like a textbook reference with a solid proof. [1] S. Minakshisundaram, Å. Pleijel, Some Properties of the Eigenfunctions of The Laplace-Operator on Riemannian Manifolds , Canadian J. Math. 1949","['riemannian-geometry', 'reference-request', 'laplacian', 'zeta-functions', 'differential-geometry']"
4135412,Group action on topological spaces.,"Let $X$ be a simply connected topological space, and $G$ a group that acts faithfully on $X$ by homeomorphism (i.e. the map $x \mapsto g \cdot x$ is a homeomorphism for each $g \in G)$ . Let $X / G$ be the orbit space, i.e. the quotient $X / \sim$ where $x \sim y$ iff there is a $g \in G$ such that $x=g \cdot y$ , and assume that the projection $p: X \rightarrow X / G$ is a covering map. Show that then, $G \cong \pi_{1}\left(X / G,\left[x_{0}\right]\right)$ . Below is my attempt to prove the statement, in which I seek an isomorphism between $G$ and $\pi_{1}(X / G,\left[x_{0}\right])$ . I have some questions for both the surjectivity and injectivity parts. For $g \in G$ , let $\tilde{\alpha}_g$ be a path in $X$ with $\tilde{\alpha}_g(0) = x_0$ and $\tilde{\alpha}_g(1) = g \cdot x_0$ . Since $X$ is simply connected, such path exists. Also let $\alpha_g = p \circ \tilde{\alpha}_g$ . We observe that, since the images of $x_0$ and $g \cdot x_0$ under $p$ are in the same orbit by definition, $\alpha_g$ is a loop in $X/G$ based at $[x_0]$ . Thus $\alpha_g \in \pi_1(X/G, [x_0])$ . Now we define $\varphi: G \to \pi_1(X/G, [x_0])$ to be $\varphi(g) = [\alpha_g]$ . We claim that $\varphi$ is an isomorphism. To show that $\varphi$ is surjective, I let $[\gamma] \in \pi_1(X/G, [x_0])$ . So $\gamma(0) = \gamma(1) = [x_0]$ . This implies $p$ send the pre-images of both $\gamma(0)$ and $\gamma(1)$ to the same orbit. So $p^{-1} \circ \gamma(0) = g \cdot x_0$ , and $p^{-1} \circ \gamma(1) = h \cdot x_0$ , for some $g$ and $h$ in $G$ . It is here that I’m stuck, as I can’t say which element of $G$ would give the path between $g \cdot x_0$ and $h \cdot x_0$ . Another approach I could think of is to say we choose a lift $\tilde{\gamma}$ of $\gamma$ that starts at $x_0$ (instead of $g \cdot x_0$ ). But am I allowed to do that? For injectivity, I sort of running into the same problem. Let $g \in G$ be such that $\varphi(g) = [x_0]$ . So I need to show that $g$ is the identity in $G$ . This would be true if $\tilde{\alpha_g}$ is the constant path at $x_0$ in $X$ . But why can’t we have the case where $\tilde{\alpha_g}$ is a path from $x_0$ to $(g \cdot x_0) \neq x_0$ , and each point on the path is $g^\prime \cdot x_0$ for some $g^\prime \in G$ ?","['group-isomorphism', 'fundamental-groups', 'group-theory', 'group-actions', 'algebraic-topology']"
4135433,Expected value of the maximum minimum set,"Pick $m$ numbers in $[0,1]$ , independently and uniformly at random.
It is known that the expected value of the smallest number is $1/(m+1)$ and of the largest number is $m/(m+1)$ . Now, partition the numbers into two sets, such that sum of numbers in the set with the smaller sum is as large as possible (in other words: solve the partition problem on the numbers). What is the expected value of the smaller sum?","['expected-value', 'set-partition', 'probability']"
4135536,"If $\sin^3(\theta)+\cos^3(\theta) = \frac{11}{16}$, find the exact value of $\sin(\theta) + \cos(\theta)$","The equation is $$\sin^3(\theta)+\cos^3(\theta) = \frac{11}{16}$$ and it wants me to find the exact value of $\sin(\theta) + \cos(\theta)$ . I started at first trying to use Pythagorean identities, but those only work for squared trigs. I also tried to expand/use foil, but I'm stuck; not sure if this method is even the right one to use.","['cubics', 'algebra-precalculus', 'trigonometry']"
4135579,Groups acting on semigroups,"Suppose $G$ is a group and $S$ is a semigroup (a set with an associative binary operation). Just as in Groups acting on groups , say a group action of $G$ on $S$ is sensible if $g * (ss') = (g * s)(g * s')$ for all $g \in G$ and $s, s' \in S$ . What are some properties of groups acting (sensibly) on semigroups that are not always true of groups acting on sets? Here is one idea I stumbled upon. Suppose that $F$ is a field and $G$ is a group acting on a semigroup $S$ . There is a corresponding linear permutation representation of $G$ on the semigroup ring $F[S]$ ( $G$ acts by permuting the basis vectors: $g * \sum_{s \in S} c_s \vec{e}_s := \sum_s c_s \vec{e}_{g * s} = \sum_s c_{g^{-1} * s} \vec{e}_s$ ). If $G$ acts sensibly on $S$ , then the linear representation also respects multiplication: $g * (v w) = (g * v)(g * w)$ for all vectors $v, w \in F[S]$ . There is a neat consequence of this result. $S_n$ plainly acts on $[n] := \{ 1, 2, \dots, n\}$ , so there is a corresponding linear permutation representation of $S_n$ on $\mathbb{R}^n$ : $\sigma * (x_1, x_2, \dots, x_n) := (x_{\sigma^{-1}(1)}, x_{\sigma^{-1}(2)}, \dots, x_{\sigma^{-1}(n)})$ . Of course, $S_n$ acts in the same way on $\mathbb{N}^n$ and continues to respect the addition operation of $\mathbb{N}^n$ , so there is a corresponding linear permutation representation of $S_n$ on $F[x_1, x_2, \dots, x_n]$ as $\sigma * h(x_1, x_2, \dots, x_n) = h(x_{\sigma(1)}, x_{\sigma(2)}, \dots, x_{\sigma(n)})$ . (Although it is not immediately clear in this last case, I am talking about the exact same construction of a linear representation from a group action every time I say ""corresponding linear permutation representation."" I think it's awesome that this works out.) Thanks to the preceding paragraph, $\sigma * (fh) = (\sigma * f)(\sigma * h)$ for all $f, h \in F[x_1, x_2, \dots, x_n]$ . Thus $\sigma$ corresponds not only to an invertible linear operator on $F[x_1, x_2, \dots, x_n]$ , but also a ring automorphism that fixes $F$ . So by the universal property of fraction fields, $S_n$ acts linearly on $F(x_1, x_2, \dots, x_n)$ as field automorphisms fixing $F$ .","['group-actions', 'group-theory', 'abstract-algebra', 'semigroups']"
4135580,Finding sum of the roots of $4(x-\sqrt x)^2-7x+7\sqrt x=2$,Find sum of the roots of $$4(x-\sqrt x)^2-7x+7\sqrt x=2$$ By substituting $t=x-\sqrt x$ we have $4t^2-7t-2=0$ $$4t^2-8t+t-2=0$$ $$(4t+1)(t-2)=0$$ So we get $x-\sqrt x=2$ Hence $x=4$ . Or $x-\sqrt x=-\frac14$ then $x-\sqrt x+\frac14=0$ and $(\sqrt x-\frac12)^2=0$ and $x=\frac14$ So sum of the roots is $4+\frac14=\frac{17}{4}\quad$ Or $\quad 4+\frac14+\frac14=\frac92$ (adding $\frac14$ twice)?,"['algebra-precalculus', 'roots']"
4135595,When is $\lim_{t \to \infty} \frac{f(t)}{t}$ equal to $\lim_{t \to \infty} \frac{df}{dt}$?,"Under what circumstances are $\lim_{t \to \infty} \frac{f(t)}{t}$ and $\lim_{t \to \infty} \frac{df}{dt}$ equal? I'm aware that in some cases one or the other limit may not exist (e.g. if $f(t) = sin(t)$ the limit of the derivative does not exist.) More precisely, I'm interested in whether  the limits are equal if $f$ is monotonically increasing and smooth.","['limits', 'calculus']"
4135596,"From game RNG to statistics, how to calculate the expected outcome?","Description I play a war turn-based game, where attacks only have two variables that are visible before a strike: The attacker's military strength $f_{a}$ and the defender's military strength $f_{d}$ . Whether an attack is successful or not is determined by a series of $3$ rolls . Each roll the attacker and defender will be given a random ( uniform ) multiplier $r_{a}$ and $r_{d}$ respectively, where $$0.4<r<1.0$$ For a roll to be successful this must hold: $$f_{a}r_{a}>f_{d}r_{d}$$ At the end of all $3$ rolls the resulting Victory Type will be determined as follows: Rolls Won $3$ / $3$ $2$ / $3$ $1$ / $3$ $0$ / $3$ Victory Type Immense Triumph (I) Moderate Victory (M) Pyrrhic Victory (P) Utter Failure (U) Casualties are also calculated through the rolls, in each roll the attacker's and defender's casualties are given respectively by $$c_{a} = 0.01·f_{d}r_{d} \,\,\,\,\,\,\,\,\,\,\,\, c_{d} = 0.018337·f_{a}r_{a}$$ Importantly, casualties are only applied after all rolls, therefor all rolls are independent events . At the end of the 3 rolls, the number of casualties is rounded to the nearest integer. What I have worked out My knowledge in statistics isn't great, but through experimentation and some intuition I managed to ( after a while ) find that for a given $f_{a}$ and $f_{d}$ where $f_{a} < f_{d}$ the probability of a successful roll can be given by: $$P(f_{a}r_{a}>f_{d}r_{d})=\int_{0}^{1}{\frac{1}{0.6}\int_{0.4}^{max(\frac{f_{a}}{f_{d}}x,\,\,0.4)}{\frac{1}{0.6}\,dt}\,dx}$$ Which can be simplified to: $$P(f_{a}r_{a}>f_{d}r_{d})=\frac{25}{9}\left ( \frac{2f_{d}}{25f_{a}} + \frac{f_{a}}{2f_{d}} - \frac{2}{5} \right )$$ And thus, if $p$ is the probabilty of a successful roll, the probabilities for each victory type given $f_{a} < f_{d}$ are as follows: $$I=\begin{pmatrix}3\\3\end{pmatrix}p^{3}(1-p)^{0}$$ $$M=\begin{pmatrix}3\\2\end{pmatrix}p^{2}(1-p)^{1}$$ $$P=\begin{pmatrix}3\\1\end{pmatrix}p^{1}(1-p)^{2}$$ $$U=\begin{pmatrix}3\\0\end{pmatrix}p^{0}(1-p)^{3}$$ What I haven't worked out So all of this leads me to my question, I have been trying to figure out what the expected number of casualties is for a specified victory type, that is, given $f_{a}$ and $f_{d}$ where $f_{a} < f_{d}$ what are the $E(C_{a}\mid I)$ , $E(C_{d}\mid I)$ , $E(C_{a}\mid M)$ , $E(C_{d}\mid M)$ and so on... So far I understand that if I were to set $r_{a}=1$ this would be true (or at least I think): $$E(C_{a}\mid I) = 3\int_{0.4}^{\frac{f_{a}}{f_{d}}}{\frac{1}{\frac{f_{a}}{f_{d}}-0.4}\,\frac{f_{d}r_{d}}{100}\,dr_{d}}$$ However given that the bounds of $r_{a}$ are dependent on $r_{d}$ and vise versa, since all the possible rolls that would result in a successful roll must follow these: $$0.4<r_{d}<\frac{f_{a}}{f_{d}}r_{a}$$ $$\frac{f_{d}}{f_{a}r_{d}}0.4<r_{a}<1$$ I have not been able to figure out how to calculates these expected values when both $r_{a}$ and $r_{d}$ are free to take all possible values they can. Double checking If anyone knows the solution, I wrote a little Python script that ran through the same scenario $1\times 10^{8}$ times where $f_{a}=3000$ and $f_{d}=4500$ and these were the results: Victory Type Immense Triumph (I) Moderate Victory (M) Pyrrhic Victory (P) Utter Failure (U) Total Outcomes $325513$ $5608873$ $32249407$ $61816207$ Total Attackers Casualties $21321676$ $429922319$ $2831691198$ $6117015520$ Average Attackers Casualties $65.5018$ $76.6504$ $87.8060$ $98.9549$ Total Defenders Casualties $46393770$ $739083939$ $3902378593$ $6814545685$ Average Defenders Casualties $142.5251$ $131.7705$ $121.0062$ $110.2388$ So whatever the right formula is, when $f_{a}=3000$ and $f_{d}=4500$ the expected casualties should match the averages computed above.","['statistics', 'bayes-theorem', 'probability']"
4135598,Propositional equivalence. Duality problem.,I recently have started practicing problems on Discrete Mathematics by Keneth H.Rosen. There seems to be an interesting problem on Propositional Equivalence which states that when is $s^{*} = s$ . Where s is a compound proposition and $s^{*}$ is the dual of $s$ .,"['boolean-algebra', 'propositional-calculus', 'discrete-mathematics']"
4135698,Finding autocorrelation function,"$\{X(t),t>0\}$ be a random variable and have $Y\sim U(0,\pi )$ distribution. Let $X(t)=e^{2Y}t$ . Find autocorrelation function of $X(t)$ . $Y\sim U(0,\pi )\Rightarrow f_Y(y)=\frac 1 \pi $ where $0\le y\le \pi$ then $\begin{align}R_{X}(t_{1},t_{2})&=\Bbb E\left [ X({t_{1}})X(t_{2}) \right ]\\[1ex]&=\Bbb E\left [ e^{2Y}t_{1}\cdot e^{2Y}t_{2} \right ]\\[1ex]&=\Bbb E\left [ e^{4Y}\cdot t_{1}t_{2} \right ]\\[1ex]&=\int_{0}^{\pi }\frac{1}{\pi }e^{4y}(t_{1}t_{2})dy\\[1ex]&=\frac{t_{1}t_{2}}{4\pi }(e^{4\pi }-1)\end{align}$ Is my solution okay? Any answer will be appreciated.","['statistics', 'covariance', 'probability-distributions', 'expected-value', 'probability']"
4135763,question on hitting time for brownian motion,"Let $W_t$ be a one dimensional brownian motion and define $\tau_c=\inf(t \geq 0 : W_t=c)$ .  We are supposed to show that $\tau_c$ is finite a.s., find it's distribution, and show that $E \tau_c = \infty$ .  Here is what I  have so far. For the distribution note that $P(W_t \geq c) = P(W_t \leq c | \tau_c \leq t)P(\tau_c \leq t)+P(W_t \geq c | \tau_c > t)$ .  By the definition of $\tau_c$ and continuity we have $P(W_t \geq c | \tau_c > t)=0$ .  From symmetry we get $P(W_t \leq c | \tau_c \leq t)=\frac{1}{2}$ .  Hence $P(\tau_c \leq t)=2\left( 1-G(\frac{c}{\sqrt{t}}) \right)$ where $G$ is the normal gaussian distribution.  Moreover, we have that $P(A_n)=P(n< \tau_c \leq n+1)=2(G(\frac{c}{\sqrt{n}})-G(\frac{c}{\sqrt{n+1}})$ .  Since $G\left( \frac{c}{\sqrt{t}} \right)\to 0$ as $t \to \infty$ , we have that $P(\cup A_n)=\sum P(A_n) = 2(1-\lim_{t \to \infty} G(\frac{c}{\sqrt{t}})) = 1$ .  Hence $\tau_c$ is bounded a.s. I am not sure how to show that the expectation of $\tau_c$ is infinite however.  Any help would be appreciated.","['statistics', 'brownian-motion', 'probability-theory', 'probability']"
4135773,Intersection of sphere of radius c and a general ellipsoid,"Question: Given the ellipsoid $$E:\ \ \frac{x^2}{a^2}+\frac{y^2}{b^2}+\frac{z^2}{c^2}=1, \ \  0 < a < c < b,$$ let $\pmb{\alpha}$ denotes the intersection of $E\ $ with the sphere $$S^2(c): \ \ x^2 + y^2 +z^2 =c^2.$$ Show that $\pmb{\alpha}$ is the union [of the traces] of two regular curves $\pmb{\alpha_1}$ and $\pmb{\alpha_2}$ . Parametrise either curve and compute its torsion, $\tau$ . Attempts : I've not gotten past the first part.
I've found that a local parametrisation for $E$ is $\mathbf{x}(u,v)=(a\cos u \sin v,\ b \sin u \sin v,\ c \ \cos v)$ which hasn't really made a difference in my confusion.
In both the books I have and on related questions found on this site, the intersection seems to be given specifically for the unit sphere and I'm not sure how to extend the theory to a general sphere. Further, any computation I do seems to return the solution $a=b=c=1$ which clearly contradicts the fact that $0 < a < c < b$ .","['ellipsoids', 'spheres', 'differential-geometry']"
4135825,Height of Square Pyramid created of Spheres [duplicate],"This question already has answers here : Find the height of a square ""pyramid"" *General Form* (3 answers) Closed 3 years ago . A square pyramid is constructed with 1015 spheres, with diameter $6$ , such that the top layer has $1$ sphere, the next has $4$ spheres, then $9$ , etc. How many layers are in the pyramid? What is the height of the pyramid? I calculated $14$ layers, but I need guidance on how to calculate the height of the pyramid. I think I could create a right triangle where the hypotenuse is the distance from the center of a corner sphere to the center of the top sphere. The other legs would be the 1. Height of the pyramid, and 2. Center of corner sphere to center of the middle sphere of the bottom row. Hypotenuse $= 6 \cdot 14 = 84$ Leg 2 $= \frac{1}{2} \sqrt{2} \cdot 84$ Leg 1 must then be $= 3528$ inch $= 294$ feet Let me know whether this seems correct.","['spheres', 'geometry', '3d']"
4135844,"Is $f'$ continuous in a small interval $[0,\epsilon)$?","Let $f:\mathbb R\to\mathbb R$ be differentiable on $[0,\infty)$ . Suppose that $\lim_{x\to0^+}f'(x)$ exists and is finite, and that $f'$ is continuous at $0$ . Must there exist some $\epsilon>0$ such that $f'$ is continuous on $[0,\epsilon)$ ? My hunch is that this is false. My idea is that locally near $x=1/n$ , any positive integer $n$ , the function $f$ is a copy of $\frac{x^2}{n}\sin(1/x)$ . I guess strictly speaking, near $x=1/n$ , $f$ is locally given by $$\frac{(x-1/n)^2}{n}\sin\left(\frac{1}{x-1/n}\right).$$ Then $f'$ is discontinuous at $x=1/n$ , but we should still have $f'\to0$ , as the oscillations are decreasing in amplitude. Could anyone confirm that this works and/or find a simpler counterexample?","['derivatives', 'examples-counterexamples', 'real-analysis']"
4135902,What is the intuition behind Chebyshev's Inequality in Measure Theory,"Chebyshev's Inequality Let $f$ be a nonnegative measurable function on $E .$ Then for any $\lambda>0$ , $$
m\{x \in E \mid f(x) \geq \lambda\} \leq \frac{1}{\lambda} \cdot \int_{E} f.
$$ What exactly is this inequality telling us? Is this saying that there is a inverse relationship between the size of the measurable set and the value of the integral?","['measure-theory', 'lebesgue-integral', 'intuition', 'inequality', 'soft-question']"
4135904,Two definitions of principal fiber,"I have two definition of $G$ -principal fiber bundle when $G$ is a linear algebraic group complex. Let be $X$ a complex variety. A principal fiber bundle on $X$ is a couple ( $\xi,q$ ) where $\xi$ is a complex variety and $q: \xi\rightarrow X$ a morphism such that: i) there is an action of G on $\xi$ such that the fiber $q^{-1}(X)$ are fixed ii) is locally trivial: for each $x \in X$ exists $x \in U \subset X$ neighborhood of $x$ in $X$ and am isomorphism $\psi_{U}:q^{-1}(U) \rightarrow G \times U$ such that $\psi_{U}$ is $G$ -equviariant the $G$ action is free and transitive The second definition is as the first but without the third axiom, so I would like to know if it's a real axiom and if there are some examples of principal fiber bundle which are graphically significative","['algebraic-geometry', 'algebraic-topology', 'principal-bundles']"
4136004,"What's the sequence $3,9,24,21,36,30,75,120,270,462,837,1320,2085,\ldots$?","In order to find a formula of the partition of an integer into 5 parts (see $(6)$ below), I find the sequence $$S_1: 3,9,24,21,36,30,75,120,270,462,837,1320,2085,\ldots \tag1$$ It's clear that $S_1$ is divisible by $3$ , so I got the sequence $$S_2: 1,3,8,7,12,10,25,40,90,154,279,440,695,\ldots \tag2$$ I can't figure out what's this sequence. I tried to divide the sequence into two other sequences: The odd sequence $$S_3: 3,7,10,40,154,440,\ldots\tag3$$ and the even sequence $$S_4: 1,8,12,25,90,279,695,\ldots\tag4$$ but yet I can't find any answer. More details: For example, the formula of the partition of an integer into 3 parts is A069905 $$p_3(n)=\begin{cases}
\frac1{12}n^2 &\text{if}\; n=6k \\[4pt]
\frac1{12}(n^2-1) &\text{if}\; n=6k+1 \;\text{or}\; n=6k+5 \\[4pt] 
\frac1{12}(n^2-4) &\text{if}\; n=6k+2 \;\text{or}\; n=6k+4 \\[4pt]
\frac1{12}(n^2+3) &\text{if}\; n=6k+3
\end{cases}\tag5$$ $p_3(n)$ is a mod $6$ formula. I managed to find also $p_4(n)$ , and it's a mod $12$ formula, A026810 . I suspect that $p_5(n)$ is a mod $15$ formula. While working in the case $n=15k+5$ , I found $$\begin{align}
p_5(5) &=\frac13((\phantom{9}5 \cdot\phantom{99}0)+\phantom{9}\color{red}{3})+0 &&=1 \\[4pt]
p_5(20)&=\frac13((20\cdot\phantom{9}12)+\phantom{9}\color{red}{9})+1 &&=84 \\[4pt]
p_5(35)&=\frac13((35\cdot\phantom{9}57)+\color{red}{24})+1 &&=674 \\[4pt]
p_5(50)&=\frac13((50\cdot156)+\color{red}{21})+4 &&=2611 \\[4pt]
p_5(65)&=\frac13((65\cdot330)+\color{red}{36})+4 &&=7166 \\[4pt]
p_5(80)&=\frac13((80\cdot600)+\color{red}{30})+9 &&=16019 \\[4pt]
p_5(95)&=\frac13((95\cdot987)+\color{red}{75})+9 &&=31289\\[4pt]
\cdots &= \cdots
\end{align} \tag6$$ The sequence of multipliers: $0,12,57,156,330,600,987,\ldots$ is the sequence $3a(n)$ , with $a(n) = \frac16n(n + 1)(7n + 5)$ . The sequence: $0,1,1,4,4,9,9,\ldots$ is the squares sequence ( $0$ for $0$ ). Then the sequence $S_1$ appears highlighted in red. Any answer please?","['integer-partitions', 'sequences-and-series']"
4136006,Why complete space is good?,"I wonder why the complete space is a good space. I know that for a metric space $X$ , if any Cauchy sequence $\{x_n\}$ in $X$ converges, then $X$ is called complete metric space. But why such property is good? Here, 'good' means basically why such property is useful. Also, why such property is desired. Why the 'completion' matters. So yes I need proper motivation for such concept. Ok, every Cauchy sequence converges, so what?","['general-topology', 'complete-spaces', 'analysis']"
4136049,Interesting riddle about heights [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Choose a person randomly on the street, $X$ . Let $N$ denote the random variable representing the number of people that you select randomly from the street before you find someone who's taller than $X$ . What is $E[N]$ ?","['expected-value', 'puzzle', 'probability']"
4136080,"Maximum of $\frac{\sin{(\sin{t}})}{{\cos{(\cos{t})}}}$ on $t\in [0,2\pi]$","I would like to analytically derive the maximum of $\frac{\sin{(\sin{t}})}{{\cos{(\cos{t})}}}$ on $t\in [0,2\pi]$ . I feel like we wouldn't have a closed form, but still I gave it a try. My attempt: I thought this problem can be seen as optimizing the $2$ -variable function $f(x,y) = \sin{x}/\cos{y}$ on $g(x,y)=0$ where $g(x,y)=x^2+y^2-1$ . I used the Lagrange Multiplier method to solve this problem. Since $f_x = \cos{x}/\cos{y}, f_y=\sin{x}\sin{y}/\cos^2{y}$ , we need to solve the following: $\begin{cases} \frac{\cos{x}}{\cos{y}}-\lambda \cdot 2x=0, \\ \frac{\sin{x}\sin{y}}{\cos{y}}-\lambda \cdot 2y=0, \\ x^2+y^2-1=0.\end{cases}$ By first two equations, we have $2xy\lambda = \frac{y\cos{x}}{\cos{y}}=\frac{x\sin{x}\sin{y}}{\cos^2{y}}$ and therefore we have $y\cos{x}\cos{y}=x\sin{x}\sin{y}$ . Now we need to solve $\begin{cases} y\cos{x}\cos{y}=x\sin{x}\sin{y}, \\ x^2+y^2=1.\end{cases}$ $x=\pm1,y=0$ are the trivial solution of this equation. According to Wolfram|Alpha , this seems to have few more solutions, but I couldn't find a way to find them. Any ideas?","['multivariable-calculus', 'calculus', 'optimization', 'lagrange-multiplier']"
4136086,Gauss Sums of Cubic Characters,"Let $p$ be a prime number, $p\equiv 1 \mod 6$ , and $g$ a primitive root modulo $p$ , i.e. a generator of the group $\mathbb F_{p} ^ {\times} \cong C_{p-1}$ . The element $g^3 \in \mathbb F_{p} ^ {\times}$ generates a subgroup of $\mathbb F_{p} ^ {\times}$ denoted by $H$ . $$H=\left \{ 1, g^3, g^6, ... , g^{p-4}\right \}$$ We have 2 cosets $$gH=\left \{ g, g^4, g^7, ... , g^{p-3}\right \} \\g^2H=\left \{ g^2, g^5, g^8, ... , g^{p-2}\right \}$$ Consider the sums $$
{t_1}=\sum_{x\in H}^{}\zeta^x,\;\; {t_2}=\sum_{x\in gH}^{}\zeta^x,\;\; {t_3}=\sum_{x\in g^2H}^{}\zeta^x,\;\; \zeta=e^{\frac{2\pi i}{p}}
$$ Since $p\equiv 1 \mod 6$ , by a theorem of Gauss there are integers $a$ and $b$ such that $$4p=a^2+27b^2$$ Let $a\equiv 1 \mod 3$ , and $b>0$ , then $a$ and $b$ are uniquely determined. Then the sums $t_1, t_2, t_3$ are the roots of polynomial $$t^3+t^2-\frac{p-1}{3}t-\frac{ap+3p-1}{27}$$ This result can be found in the reference book ""H. Hasse, Vorlesungen über Zahlentheorie, 1950. P460/P461"" But this book is in Germany and I cannot find it on line. Can someone show me a brief proof or a reference link in English to explain this result? I also found for $p=9p'$ where $p'$ is $1$ or a prime, and $p'\equiv 1 \mod 6$ , the Euler's totient function of $p$ is \begin{align}
&\varphi(p)=\varphi(9)=6 \;\;\;\text{when }p'=1\\
&\varphi(p)=6(p'-1) \;\;\;\text{when }p'\text{ is a prime}
\end{align} The integers up to $p$ that are relatively prime to $p$ can also be divided into $3$ subgroups $H_1, H_2, H_3$ . When $p'$ is a prime, the count of integers in each subgroup is $2(p'-1)$ . Generating the subgroups is not as simple as that for a prime. We may need $\left \{ g_1, g_2 \right \}$ , a generating set of $(\mathbb{Z}/p\mathbb{Z})^\times \cong \mathrm{C}_{p'-1} \times \mathrm{C}_6$ . Anyway, consider the sums $$
{t_1}=\sum_{x\in H_1}^{}\zeta^x,\;\; {t_2}=\sum_{x\in H_2}^{}\zeta^x,\;\; {t_3}=\sum_{x\in H_3}^{}\zeta^x,\;\; \zeta=e^{\frac{2\pi i}{p}}
$$ Let $a\equiv 0 \mod 3$ , there are integers $a$ and $b$ such that $$4p=a^2+27b^2$$ Then the sums $t_1, t_2, t_3$ are the roots of polynomial $$t^3-\frac{p}{3}t-\frac{ap}{27}$$ I have verified this with some examples $p=9, 63, 117, 171, 279, 333, 387, ...$ I'd like to know how to prove this result.","['number-theory', 'algebraic-number-theory', 'polynomials']"
4136120,$\int_0^{\infty} \frac{e^{-\pi x^2}+e^{-\pi /x^2}}{e^{\pi x}+e^{\pi /x}} dx$ Sum of Coefficients in Cubic Polynomial,"I have been trying to solve this problem and I am stuck half way. My Working: I have solved the integrals and noticed that the value of $\alpha$ comes out to be $$\alpha\approx 94.05$$ So, the cubic equation becomes $$94.05^3A+94.05^2B+94.05C+D=0$$ I am wondering if the question is correct or not. Because I think we cannot solve for four unknowns $$-(A+B+C+D)= ?$$ using 1 equation only, I think we need three more equations. There may be many possible answers for this equation as coefficients are not defined. If it is still solvable please let me know, $\color{red}{\text{I am not asking answer I just want to know is it solvable or not ?}}$ Thanks Edit : $\color{blue}{\text{The original problem has been deleted now}}$ on the site maybe due to wrong question statement and not properly stated or something is missing. Maybe they feel it is not possible to find the value of $A+B+C+D$ However if you still want to try these lovely integrals then you can. Thank you for the support.","['integration', 'calculus', 'solution-verification']"
4136198,Why is it enough to consider limits as $𝛿\to 0$ through $\{𝛿_k\}$ such that $𝛿_{k+1} \ge c𝛿_k$ for some $0 < c < 1$ to find $\dim_B F$?,"To set the stage, let me recall the definition of the box-counting dimension of a set $F \subset \mathbb R^n$ . The lower and upper box-counting dimensions of a subset $F$ of $ℝ^n$ are given by $$\underline{\dim}_B F = \underline{\lim}_{\delta\to 0} \frac{\log N_\delta(F)}{-\log \delta}$$ $$\overline{\dim}_B F = \overline{\lim}_{\delta\to 0} \frac{\log N_\delta(F)}{-\log \delta}$$ and the box-counting dimension of $F$ by $$\dim_B F = \lim_{\delta\to 0} \frac{\log N_\delta(F)}{-\log \delta}$$ (if this limit exists), where $N_\delta(F)$ is the smallest number of sets of diameter at most $𝛿$ that cover $F$ . Now, the author says that to find $\dim_B F$ , it is enough to consider limits as $𝛿\to 0$ through $\{𝛿_k\}$ such that $𝛿_{k+1} \ge c𝛿_k$ for some $0 < c < 1$ , in particular $\delta_k = c^k$ . I have trouble seeing why this is true. Following is the reasoning provided by the author: Note that if $\delta_{k+1} \le \delta < \delta_k$ , then with $N_𝛿(F)$ the least number of sets in a $𝛿$ -cover of F, $$\frac{\log N_\delta(F)}{-\log\delta} \le \frac{\log N_{\delta_{k+1}}(F)}{-\log\delta_k} = \frac{\log N_{\delta_{k+1}}(F)}{-\log\delta_{k+1} + \log{(\delta_{k+1}/\delta_k)}} \le \frac{\log N_{\delta_{k+1}}(F)}{-\log\delta_{k+1} + \log c}$$ and so $$\overline{\lim_{\delta\to 0}} \frac{\log N_\delta(F)}{-\log\delta} \le \overline{\lim_{k\to\infty}} \frac{\log N_{\delta_k}(F)}{-\log\delta_k}$$ The opposite inequality is trivial; the case of lower limits may be dealt with in the
same way. The author says $0 < c < 1$ , so that $\log c < 0$ . I do not understand where the following implication comes from: $$\frac{\log N_\delta(F)}{-\log\delta} \le \frac{\log N_{\delta_{k+1}}(F)}{-\log\delta_{k+1} + \log c} \implies \overline{\lim_{\delta\to 0}} \frac{\log N_\delta(F)}{-\log\delta} \le \overline{\lim_{k\to\infty}} \frac{\log N_{\delta_k}(F)}{-\log\delta_k} $$ For the opposite direction (assuming the implication in (1) is correct), I would do $$\frac{\log N_\delta(F)}{-\log\delta} \ge \frac{\log N_{\delta_{k}}(F)}{-\log\delta_{k+1}} = \frac{\log N_{\delta_{k}}(F)}{-\log\delta_{k} + \log(\delta_k/\delta_{k+1})} \ge \frac{\log N_{\delta_{k}}(F)}{-\log\delta_{k} - \log c}$$ Taking limits (this needs more justification) , $$\overline\lim_{\delta\to 0} \frac{\log N_\delta(F)}{-\log\delta} \ge \overline\lim_{k\to\infty} \frac{\log N_{\delta_k}(F)}{-\log\delta_k}$$ which is the required inequality. Combining this with the previous inequality, stated in the question, we get $$\overline\lim_{\delta\to 0} \frac{\log N_\delta(F)}{-\log\delta} = \overline\lim_{k\to\infty} \frac{\log N_{\delta_k}(F)}{-\log\delta_k}$$ A similar procedure could be adopted for the case of $\underline\lim_{\delta\to 0}$ and $\underline\lim_{k\to \infty}$ (just take lower limits in place of upper ones, everything else unchanged). Is this right? References. Fractal Geometry by Kenneth Falconer.","['measure-theory', 'fractal-analysis', 'proof-explanation', 'analysis', 'fractals']"
4136284,Prove that $\Lambda$ is not compact,"On the Banach space $X=C[0,1]$ , consider the linear operator $\Lambda:X\rightarrow X$ defined by $$
(\Lambda f)(0)=f(0),(\Lambda f)(t)=\frac{1}{t}\int_0^tf(s)ds,\forall t>0
$$ Prove that $\Lambda$ is not compact. My idea: Assume that $\Lambda$ is compact, I am trying to find counterexample. First, let $x_n(t)=t^n$ , the norm of $x_n$ is 1, I hope that $\{\Lambda x_n\}$ is not pre-compact. Unfortunely, $\lim_{n\to \infty}\|\Lambda x_n\|=0$ . Then I do not know what to do.","['compact-operators', 'functional-analysis']"
4136314,How does gluing of affine patches of toric varieties at the examples $\mathbb{P}^2$ and $\mathcal{H}_r$ work?,"I don't fully understand how the gluing of the affine parts of a toric variety exactly works. I have a hard time developing a common sense or any intuition how to tell the result of a gluing morphism immediately and I would appreciate any help in this direction. I'm missing the concrete detailed math behind that as well, would be cool if someone can give more details here. The two examples I am looking at: In both cases, we have the original lattice $N=\mathbb{Z}^2$ and the dual $M=\mathbb{Z}^2$ . Projective Space Let's start with the given fan of $X=\mathbb{P}^2$ : We can concretely calculate the toric varieties $X_{\sigma_i}$ and then glue them together along the walls $\tau_{ij} = \sigma_i \cap \sigma_j$ . In general we have $X_{\sigma} = \text{Spec} (R_{\sigma}) = \text{Spec}(\{f\in \mathbb{C}[z_1^{\pm 1},z_2^{\pm 1}]: \text{supp } f \subset S_{\sigma} = \check{\sigma} \cap M\})$ where $\check{\sigma}$ is the dual of $\sigma\subset N$ in $M$ . So looking at $\sigma_1$ we get $\check{\sigma}_1 = \text{Cone}(-e_1, -e_1+e_2)$ which converts to $R_{\sigma_1} = \mathbb{C}[z_1^{-1},z_1^{-1}z_2]$ and thus $X_{\sigma_1} = \mathbb{C}^2_{(z_1^{-1}, z_1^{-1}z_2)}$ (the coordinates are noted as subscripts). Analogously we can calculate $X_{\sigma_0} = \mathbb{C}^2_{(z_1, z_2)}$ and $X_{\sigma_2} = \mathbb{C}^2_{(z_2^{-1}, z_1z_2^{-1})}$ . Let's start by gluing $X_{\sigma_0}$ and $X_{\sigma_1}$ along $(z_1,z_2) \mapsto (z_1^{-1}, z_1^{-1}z_2)$ . How can I picture this gluing morphism? In my understanding we get that $(z_1,z_2)_{\sigma_0} \sim (z_1^{-1},z_1^{-1}z_2)_{\sigma_1}$ , where $\sim$ is the equivalence relation of the gluing morphism. What are the steps to get that $(X_{\sigma_0} \cup X_{\sigma_1}) / \sim$ is the same as $\mathbb{P}^2 \setminus \{(0:0:1\}$ ? What I thought about is the following:
As we are matching $z_1$ and $z_1^{-1}$ we need $\mathbb{P}^1$ , $\mathbb{C}$ is not sufficient anymore. Therefore I would write the above glued variety as $\{((t_0:t_1), z_2): \frac{t_0}{t_1}=z_1\}\simeq \mathbb{P}^1 \times \mathbb{C}$ . However two questions arise from this set: First, why aren't we taking $\frac{t_1}{t_0}=z_1$ and second where did $z_1^{-1}z_2$ go? Assume I somehow managed this part, I get completely lost with the next gluing! Do I now have to glue $X_{\sigma_2}$ along two rays, do I need to glue all affine patches pairwise and then glue everything together or can I glue $X_{\sigma_2}$ directly with $(X_{\sigma_0} \cup X_{\sigma_1}) / \sim$ and if so, how can I do that? I would really appreciate a detailed answer because everywhere in the literature this seems to be ""trivial"". Hirzebruch surface The calculation of each affine patch is no problem for me. We get $$
X_{\sigma_1} = \mathbb{C}^2_{(z_1,z_2)} \\
X_{\sigma_2} = \mathbb{C}^2_{(z_1,z_2^{-1})} \\
X_{\sigma_3} = \mathbb{C}^2_{(z_1^{-1},z_1^{-r}z_2^{-1})} \\
X_{\sigma_4} = \mathbb{C}^2_{(z_1^{-1},z_1^rz_2)} \\
$$ How can I glue them together? As before we can glue $X_{\sigma_1}$ with $X_{\sigma_2}$ along the second coordinate (that means we identify $(z_1,z_2)_{\sigma_1}$ with $(z_1,z_2^{-1})_{\sigma_2}$ ) and $X_{\sigma_3}$ with $X_{\sigma_4}$ along the second coordinate as well. For simplification define $X_I:=(\bigcup_{i\in I}X_{\sigma_i}) / \sim $ These two gluings give $$X_{12} = \{(z_1, (t_0:t_1)): \frac{t_0}{t_1}=z_2\}\simeq \mathbb{P}^1 \times \mathbb{C}$$ and $$X_{34} = \{(z_1^{-1}, (s_0:s_1)): \frac{s_0}{s_1}=z_1^rz_2\}\simeq \mathbb{P}^1 \times \mathbb{C}$$ Again the (small) question, can we choose $\frac{t_1}{t_0}=z_2$ or $\frac{s_1}{s_0}=z_1^rz_2$ as well? So far so good. We can now glue those two varieties along the first coordinate and that's where I get stuck. I would suggest something of a similar form $\{((u_0:u_1), (s_0:s_1)):\: ...\}$ . However the solutions seems to be $$
X_{1234} = \{((\lambda_0:\lambda_1:\lambda_2), (\mu_0:\mu_1)) : \lambda_0\mu_0^r = \lambda_1\mu_1^r\}
$$ How is this equation derived and what is the intuition behind that? In general, what happens if we don't have the ability to glue some $f(z)$ and $f^{-1}(z)$ together (in above examples it was $f(z)=z_1$ , $f(z)=z_2$ or $f(z)=z_1^rz_2$ ) for a resulting $\mathbb{P}^1$ ? How would be the approach in those cases? I would be glad if someone can clarify those questions.","['projective-varieties', 'algebraic-geometry', 'toric-geometry', 'projective-space']"
4136380,Applying Hadamard's Three Lines Theorem to prove that $f(z) = f(r) (z/r)^k$,"I am currently stuck on a complex analysis exercise, which I find quite difficult, here it is: ""Let $0<r<R$ , define the Annulus as $A_{r,R} := \{z \in \mathbb{C}: r<|z|<R \}$ . Let $f:\overline{A_{r,R}} \rightarrow \overline{A_{\tilde {r},\tilde {R}}}$ be a holomorphic function on $A_{r,R}$ , with $0<r<R<\infty$ , $0<\tilde{r}<\tilde{R}<\infty$ , such that: $|z| =r \implies |f(z)|=\tilde{r}, |z| = R \implies |f(z)| = \tilde{R}$ . Show that there is a natural number $k$ , such that $f(z) = f(r) (z/r)^k$ for all $z \in \overline{A_{r,R}}$ ."" The exercise has a tip: ""Apply Hadamard's Three Lines Theorem on an appropriate function"". I have read something about Hadamard's Three Circle Theorem, which has similar conditions, however, it only says something about the supremums of $|f|$ on specific circles, but here we want to say something about $f$ , so I'm not sure if it's possible to apply it here. My first and only idea was to consider a holomorphic function $g :  S \rightarrow \overline{A_{r,R}}$ , which bijectively maps the strip $ S := \{ z \in \mathbb{C} \cup \{\infty\}: r \leq \Re(z) \leq R\}$ to $\overline{A_{r,R}}$ in a ""Riemann-way"". Then we could consider a map $ \gamma: S \rightarrow \overline{A_{\tilde {r},\tilde {R}}}$ , $\gamma = f \circ g$ . We know that $f$ is bounded by $\tilde{R}$ , so $\gamma$ is a bounded holomorphic function, if we define $\Gamma(x) := \sup \{\gamma(z): \Re(z) = x \}$ , we can apply Hadamard's TLT, so $\Gamma(x) \leq \Gamma(r)^{1-x} \Gamma(R)^x = \tilde{r}^{1-x}\tilde{R}^x$ $\forall x \in [r,R]$ . That's all I have. But what now? I don't see any way to continue, what else could I try, does someone have a tip? Thanks in advance! Edit: We got a solution for this problem, but coming up with the function $g$ doesnt seem very trivial, but the key idea is to apply the Three Lines Theorem on the function $$g(z) := f(r \exp(z \ln(R/r))) / (\tilde{r} \exp(z \ln(\tilde{R} / \tilde{r})))$$ And on its reciprocal $1/g(z)$ to show that $|g(z)| = 1$ . If anyone is interested I can give more details.",['complex-analysis']
4136411,Uniqueness and non-uniqueness of a SDE involving a random time change,"The other day, when I was reading the book of Ethier and Kurtz , I found the following problem (Chapter 6, p.332). Let $W=\{W(s)\}_{s \ge 0}$ be standard Brownian motion (the book doesn't give any more details, but I think $W$ is a one-dimensional Brownian motion). (a) Show that for $\alpha \in (0,1)$ , \begin{align*}
P\left[\int_0^t|W(s)|^{-\alpha}\,ds<\infty,\,t\ge0\right]=1,
\end{align*} and for $\alpha\ge 1$ , \begin{align*}
P\left[\int_0^t|W(s)|^{-\alpha}\,ds=\infty,\,t>0\right]=1.
\end{align*} (b) Show that for $\alpha \ge1$ the solution of \begin{align*}
X(t)=X(0)+W\left(\int_{0}^t |X(s)|^\alpha\,ds\right),\quad t \ge 0
\end{align*} is unique but it is not unique if $\alpha \in (0,1)$ . Problem (c) comes up after this, but for now I'm only interested in this problem (b). I'm not sure if the problem (a) can be solved easily, but this indeed follows from the Engelbert--Schmidt Zero-One law (integrability near the origin of the map $t \mapsto |t|^{-\alpha}$ is important). However, I don't know how to solve (b) (in the first place, the meaning of the  uniqueness is not written in the problem, though). If you know how to solve (b), please let me know.","['stochastic-processes', 'stochastic-differential-equations', 'brownian-motion', 'probability-theory']"
4136418,Complex minimization to minimize $\cos \alpha$,"Let $\alpha,\beta,$ and $\gamma$ be three real numbers. Suppose that $$\cos\alpha+\cos\beta+\cos\gamma=1$$ $$\sin\alpha+\sin\beta+\sin\gamma=1.$$ Find the smallest possible value of $\cos \alpha.$ (Source: HMMT 2018 Algebra and Number Theory #6) I assumed that $\beta = \gamma$ was optimal and got the right answer. However, I was unable to prove afterwards that this was optimal, so I referred to the Official Solutions (Scroll to #6). They let $a = \cos \alpha + i \sin \alpha$ , and so on, and state that $a,b,c$ must be on the unit circle, which I understand. I also understand that we want to minimize the real part of $a$ . However, they then consider a triangle with vertices $a, 1+i$ the origin, and state that $a$ must be as far away from $1+i$ while maintaining a nonnegative imaginary part. They state that this implies the argument of $b$ and $c$ are the same. I don't understand the aforementioned in this paragraph, could someone explain it?","['trigonometry', 'complex-numbers']"
4136445,Relationship between étale maps and polynomials,"Let $k$ be a field, not necessarily algebraically closed, and suppose you have a map of $k$ -schemes $\varphi: \mathbb{A}^n \to \mathbb{A}^n$ , induced by some $\psi: k[y_1, \dots y_n] \to k[x_1, \dots x_n]$ . Let $p_i(x_1, \dots x_n) := \psi(y_i)$ , and define $J(p) := \left(\frac{\partial p_i}{\partial x_j}(p)\right)_{i, j \leq n} \in M_{n \times n}(\kappa(p))$ . Suppose you know that $\varphi$ is étale at some point $p \in \mathbb{A}^n$ , i.e. that $\varphi$ is smooth of relative dimension zero at $p$ . Is it true that $J(p)$ has maximum rank as a matrix in $M_{n\times n}(\kappa(p))$ ? The converse is, to the best of my knowledge, true, and this should also be ""morally"" true (since the $p_i$ s are essentially ""defining"" $\varphi$ ), but I don't know how to prove it. As a bonus: can we formulate a similar statement for $\varphi: X \to Y$ , where this time $X = \operatorname{Spec} k[x_1, \dots x_m]/I \subseteq \mathbb{A}^m$ and $Y = \operatorname{Spec} k[y_1, \dots y_n]/J \subseteq \mathbb{A}^n$ are of finite type (but not necessarily reduced)?","['algebraic-geometry', 'commutative-algebra']"
4136494,Removable Singularity in the Eguchi-Hanson Metric,"I am trying to understand the definition of the Eguchi-Hanson metric, and in particular the nature of the removable singularity at the origin when expressed as a metric on $\mathbb{C}^2$ . Background: On $\mathbb{C}^2$ with complex coordinates $z^1,z^2$ , one looks for a rotationally symmetric Kahler metric of the form $\omega = i\partial\overline\partial \phi$ , where $\phi = F(\rho)$ for $\rho = |z^1|^2+|z^2|^2$ , with vanishing Ricci curvature. The only solutions are $ \omega_{Eucl} = i\partial\partial \rho$ , which (up to scaling) is the Euclidean metric, and (up to some kind of scaling) the Eguchi-Hanson metric: $$ \omega_{EH} = i\partial\overline\partial\left(\sqrt{\rho^2 + 1}+\log(\rho)-\log(1+\sqrt{\rho^2+1})\right) $$ Now this metric has a singularity at the origin (as $\rho \rightarrow 0$ , the $\log(\rho)$ term is unbounded), but many sources say that on passing to the quotient $\frac{\mathbb{C}^2}{\mathbb{Z}_2}$ , where $\mathbb{Z}_2$ acts by multiplication by $-1$ , the singularity disappears. Could anyone explain how to see this using the above formulation?","['kahler-manifolds', 'complex-geometry', 'riemannian-geometry', 'differential-geometry']"
4136526,Compute this Indefinite Integral,"A friend of mine asked me to solve this integral $$\int_0^{\infty}\frac{\tan^{-1}(ax)dx}{x(1+x^2)}, a>0 , a\neq1. $$ but has no idea what is the answer or whether it can be solved or not. I tried pretty much everything I know but I failed so I tried using Residue calculus but got stuck in choosing the particular contour . Since there are 3 singularity points at x = 0 , $i$ , $-i$ , I decided to break the denominator $\frac{1}{x(1+x^2)}$ into partial fractions $\frac{1}{x}-\frac{x}{1+x^2}$ . That way I only had to deal with a singularity at 0 and $i$ seperately but I find it quite hard to navigate my way forward. Please help and also explain the importance of the condition $a>0$ .","['complex-analysis', 'residue-calculus', 'improper-integrals']"
4136600,Why is $\dim_{B}F \le n$ in $\mathbb R^n$? (Upper Bound on Minkowski–Bouligand dimension),"(Please skip to the end for a word on notation) For $F \subset\mathbb R^n$ , where $F\ne \varnothing$ , we have $$0 \le \underline\dim_B F \le \overline\dim_B F \le n$$ and hence $$\dim_B F \le n$$ where $\dim_B F$ denotes the box-counting dimension of $F$ . $\underline\dim_B F$ and $\overline\dim_B F$ denote the lower and upper box-counting dimension respectively. I need help understanding the proof of the last inequality, i.e. the $\le n$ part. The proof in the book is: The first two inequalities are obvious; for the third, $F$ may be enclosed in a large cube $C$ so by counting $𝛿$ -mesh squares $N_𝛿(F) ⩽ N_𝛿(C) ⩽ c𝛿^{−n}$ for some constant $c$ . Since $F\subset C$ , $N_\delta(F) \le N_\delta(C)$ is clear. Why is $N_\delta(C) \le c\delta^{-n}$ for some $c$ ? Notation: About $\dim_B F$ and $N_\delta(F)$ : The lower and upper box-counting dimensions of a subset $F$ of $ℝ^n$ are given by $$\underline{\dim}_B F = \underline{\lim}_{\delta\to 0} \frac{\log N_\delta(F)}{-\log \delta}$$ $$\overline{\dim}_B F = \overline{\lim}_{\delta\to 0} \frac{\log N_\delta(F)}{-\log \delta}$$ and the box-counting dimension of $F$ by $$\dim_B F = \lim_{\delta\to 0} \frac{\log N_\delta(F)}{-\log \delta}$$ (if this limit exists), where $N_\delta(F)$ is any of the following: the smallest number of sets of diameter at most $𝛿$ that cover $F$ ; the smallest number of closed balls of radius $𝛿$ that cover $F$ ; the smallest number of cubes of side $𝛿$ that cover $F$ ; the number of $𝛿$ -mesh cubes that intersect $F$ ; the largest number of disjoint balls of radius $𝛿$ with centres in $F$ . What is a $\delta$ -mesh? $$[m_1\delta, (m_1+1)\delta] \times [m_2\delta, (m_2+1)\delta] \times \ldots \times [m_n\delta, (m_n+1)\delta]$$ where $m_i \in\mathbb Z$ for every $1 \le i \le n$ , is called a $\delta$ -mesh (or a $\delta$ -grid) in $\mathbb R^n$ .","['geometry', 'analysis', 'real-analysis']"
4136655,Cocontinuity of the fiber functor on topological spaces,"Let $X$ be a topological space and $x \in X$ . Is the fiber functor $$\mathbf{Top}/X \to \mathbf{Top},\quad (f : Y \to X) \mapsto f^{-1}(x)$$ cocontinuous? I already checked that coproducts are preserved, but coequalizers are unclear. The composition with the forgetful functor $\mathbf{Top} \to \mathbf{Set}$ is cocontinuous, since the fiber functor $\mathbf{Set}/X \to \mathbf{Set}$ is actually left adjoint to the dependent product along $x : \{\star\} \to X$ , which exists in any topos. I am really interested in $\mathbf{Top}$ here, not a convenient category of spaces .","['topos-theory', 'general-topology', 'limits-colimits', 'category-theory']"
4136721,alternative for Schwarz inequality,"I wish to show that $$\sum_{n=1}^{\infty}|\langle e_n, x\rangle||\langle e_n,y \rangle| \leq \|x\|\|y\|$$ where $\{e_n\}$ is an orthonormal system. But my professor wants me to prove it without using Cauchy-Schwarz (or Hölder) inequality. His sugestion is to use the obvious inequality $(a-b)^2 \geq 0$ for the sequences $a_n = |\langle e_n, x\rangle|$ and $b_n = |\langle e_n, y\rangle| $ . It follows that, in general, $$ \sum_{n=1}^{\infty}\{(a_n - b_n)^2\} \geq 0 \implies \sum_{n=1}^{\infty}\{a_n^2 + b_n^2 - 2a_nb_n\}\geq 0$$ is this right? if so, by Bessel's inequality (i'm allowed to use it lol), $\sum a_n^2 = \sum|\langle e_n, x\rangle|^2 \leq \|x\|$ and $\sum b_n^2 =\sum |\langle e_n, y\rangle|^2 \leq \|y\|^2$ . With this, I get to the inequality: $$ 2\sum_{n=1}^{\infty}|\langle e_n, x\rangle||\langle e_n,y \rangle| \leq  \|x\|^2 + \|y\|^2$$ and... this is not quite what I want. Did i do something wrong or i'm forgetting something? how can I proceeed or restart it? any help will be very appreciated!
thanks in advance.","['cauchy-schwarz-inequality', 'orthonormal', 'linear-algebra', 'functional-analysis']"
4136818,How many ways are there to distribute $18$ distinguishable object into boxes?,"How many ways are there to distribute $18$ distinguisable object into a-) $5$ distinguishable boxes so that the boxes have $1,2,4,5,6$ objects in them, respectively. b-) $5$ distinguishable boxes so that the boxes have $1,2,4,5,6$ objects in them. c-) $5$ distinguishable boxes so that the boxes have $4,2,2,5,5$ objects in them, respectively. d-) $5$ distinguishable boxes so that the boxes have $4,2,2,5,5$ objects in them. e-) $5$ indistinguishable boxes so that the boxes have $1,2,4,5,6$ objects in them, respectively. f-) $5$ indistinguishable boxes so that the boxes have $1,2,4,5,6$ objects in them. g-) $5$ indistinguishable boxes so that the boxes have $4,2,2,5,5$ objects in them, respectively. h-) $5$ indistinguishable boxes so that the boxes have $4,2,2,5,5$ objects in them. My attempt: a-) $ C(18,1) \times C(17,2) \times C(15,4) \times C(11,5) \times C(6,6) $ b-) $ 5! \times (C(18,1) \times C(17,2) \times C(15,4) \times C(11,5) \times C(6,6)) $ c-) $C(18,4) \times C(14,2) \times C(12,2) \times C(10,5) \times C(5,5) $ d-) $(\frac{5!}{2!\times 2! \times 1!})\times(C(18,4) \times C(14,2) \times C(12,2) \times C(10,5) \times C(5,5)) $ e-) I think this question is invalid , because if the boxes are indistinguishable then we cannot talk about respectivity. f-) In this question , i firstly thought them like distinguishable boxes, after that i divide them by $5!$ to obtain indistinguishable form such that $\frac{(C(18,1) \times C(17,2) \times C(15,4) \times C(11,5) \times C(6,6))}{5!} $ g-)I think this question is invalid, because if the boxes are indistinguishable then we cannot talk about respectivity. h-) In this question, i firstly thought them like distinguishable boxes, after that i divide them by $\frac{5!}{2!\times 2! \times 1!}$ such that $\frac{C(18,4) \times C(14,2) \times C(12,2) \times C(10,5) \times C(5,5)}{\frac{5!}{2!\times 2! \times 1!}} $ Is my solutions correct? If not, can you correct me, please ?","['permutations', 'combinations', 'solution-verification', 'combinatorics', 'discrete-mathematics']"
4136838,Show that $K(\sqrt{2\sqrt{2}-1})$ is a degree $4$ Abelian extension of $K=\Bbb Q (i\sqrt{14})$,"I have to show that $K(\sqrt{2\sqrt{2}-1})$ is a a degree $4$ Abelian extension of $K=\Bbb Q (i\sqrt{14})$ . Now, $a=\sqrt{2\sqrt{2}-1}$ has minimal polynomial $f(x)=x^4+2x^2-7$ over $\Bbb Q$ , which can be checked. I have to show that that $f$ is irreducible over $K$ and that the extension is normal. Then it'll follow that it's Galois and hence Abelian since it has degree 4. WolframAlpha tells me that the roots are $\pm \sqrt{2\sqrt2-1},\pm i\sqrt{2\sqrt{2}+1}$ . So, the field is also normal as $\alpha \in L \implies \sqrt{2} \in L$ and then $i\sqrt{2\sqrt{2}+1} = {i\sqrt{14} \over \sqrt{2} \alpha} \in L$ . Then the only thing left to prove is that $f$ is irreducible over $K$ . Please help me out. EDIT : Writing $K_1=K(\sqrt{2})$ , we get that $[K_1:K]=2$ as otherwise $K_1=K$ and $[\Bbb Q{\sqrt{2}}:\Bbb Q]=2=[K:\Bbb Q]$ mean $K=\Bbb Q{\sqrt{2}}$ but that's absurd as $\Bbb Q{\sqrt{2}}$ is real while $K$ is not. So, we'll be done if we can prove that $[L:K_1]=2$ . The obvious choice for minimal polynomial is $x^2+1-2\sqrt{2}$ and it'll be enough if we can show that it is indeed irreducible over $K_1$ .","['irreducible-polynomials', 'number-theory', 'field-theory', 'galois-theory', 'extension-field']"
4136874,Maximum number of triples from $[n]$ so that no two elements appear together more than once [duplicate],"This question already has an answer here : Subsets differing in at least two elements (1 answer) Closed 1 year ago . Let $n$ be a positive integer. What is the maximum number of triples from $[n] :=\{1,2,\ldots,n\}$ one can pick so that no two elements $a$ and $b$ appear both in more than one triple? So far, I have $\frac{n(n-1)}{6}$ as an upper bound and $\frac{n(n-3)}{6}$ as a lower bound. For the upper bound, there are $\frac{n(n-1)}{2}$ pairs of elements whilst each triple contains three pairs (and no pair appears more than once overall). For the lower bound, the triples $(x,y,z)$ with $x+y+z$ divisible by $n$ satisfy the condition -- and when counting, for $x$ there are $n$ choices, for $y$ there are at least $n-3$ and then $z$ is fixed (and we divide by $3!$ because of permutations). Any help appreciated!","['elementary-set-theory', 'combinatorics', 'extremal-combinatorics']"
4136904,Functional Analysis: Kernel-Based Approximation,"First of all, let me give a basic definition and the problem that we want to solve [both taken from Armin Iske's Book ""Approximation Theory and Algorithms for Data Analysis""]: Problem 8.1. On given interpolation points $X = \{x_1, \dots, x_n\}\subset \Omega$ , where $\Omega\subset \mathbb R^{d}$ for $d>1$ , and function values $f_X\in \mathbb R^{n}$ find an interpolant $s\in \mathcal C(\Omega)$ satisfying $s_X = f_X$ , so that $s$ satisfies the interpolation conditions $$s(x_j) = f(x_j) \quad \text{for all $1\leq j\leq n$}.$$ Definition 8.2. [taken from Armin Iske's Book ""Approximation Theory and Algorithms for Data Analysis""]: A continuous and symmetric function $K: \mathbb R^{d}\times \mathbb R^{d} \rightarrow \mathbb R$ is said to be positive definite on $\mathbb R^{d}$ , $K\in \textbf{PD}_d$ , if for any set of pairwise distinct interpolation points $X = \left\{ x_1, \dots, x_n\right\} \subset \mathbb R^{d}$ , $n\in \mathbb N$ , the matrix $$A_{K, X} = \left( K\left(x_k, x_j\right) \right)_{1\leq j, k\leq n}\in\mathbb R^{n\times n}$$ is symmetric and positive definite. Now, here comes the problem that I want to solve: Let $\Phi\left(x-y\right) = K(x,y)$ be positive definite, $K\in \textbf{PD}_d$ , where $\Phi:\mathbb R^{d}\rightarrow \mathbb R$ is even and satisfies, for $\alpha > 0$ , the growth condition $$\left\vert \Phi\left(0\right) - \Phi\left( x\right) \right\vert \leq C \left\vert\left\vert  x\right\vert\right\vert_{2}^{\gamma} \quad \forall x\in B_{R}\left( 0\right)$$ on some ball $B_{R}\left( 0\right)$ around $0$ with radius $R > 0$ and constant $C > 0$ . Prove that no positive definite kernel $K\in \textbf{PD}_d$ satisfies the growth condition for $\gamma > 2$ . This is a homework we got in a class. EDIT : This is the definition of (total) differentiability I know: Let $U\subseteq \mathbb R^n$ be open and $F: U\rightarrow \mathbb R^m$ . (i) The function $F$ is called (totally) differentiable if there exists a linear map $A: \mathbb R^n\rightarrow\mathbb R^m$ such that $$\lim_{\xi\in\mathbb R^n\backslash\{0\},\ \xi\rightarrow 0}\frac{\|F(x+\xi)-F(x)-A(\xi)\|}{\|\xi\|} = 0$$ (ii) The function $F: U\rightarrow\mathbb R^m$ is called differentiable if $F$ is at all points $x\in U$ differentiable.","['approximation', 'functional-analysis', 'reproducing-kernel-hilbert-spaces']"
4136918,Why ignoring the function $f(x)=1/(1+x^2)$?,"I heard that the function $$f(x)=e^{-x^2}$$ Is extremely important in probability and statistics, because it looks like the normal distribution (or something like that). But i noticed that the graph of this function is similar to the function $$g(x)=\frac{1}{1+x^2}$$ Furthermore, the area under this curve is pretty easy to calculate, since $$\int_{0}^z \frac{\,dx}{1+x^2}=\arctan(z).$$ So why the first function is more important than the second one, although they look pretty similar?","['calculus', 'probability']"
4136982,Is there a finite theory of groups whose models are all torsion-free?,"We know from a standard ultrafilter argument (see this and this answer ) that there cannot be a finite first-order theory that is modeled by precisely all the torsion-free groups. But is there a finite theory $T$ that identifies only some torsion-free groups?
Furthermore, is there an example that has at least two models who are not elementarily equivalent?","['group-theory', 'first-order-logic', 'model-theory']"
4137021,Expected number steps in a Birth and Death Chain,"I've been working on this problem for several days now, and I'm completely stuck on what to do next.  Here's the problem: Consider a birth-and-death chain $X_t$ on $S = \{0, 1, 2, . . .\}$ with the following
transition probabilities: $p(0, 1) = 1$ , and $p(j, j + 1) = p$ and $p(j, j − 1) = 1-p$ for $p \in \left(0,\frac{1}{2}\right)$ . Let $T = \min\{t \geq 0 \: | \: X_t=0\}$ and define $\phi(j) = E(T|X_0=j)$ .  Find $\phi(1)$ . The hint that came with the problem suggested the following: Let $N$ be the number of excursions to the right of state $1$ before the first visit to state $0$ .  Find the p.m.f of $N$ and $E(N)$ . Then prove that $\phi(1)=1+E(N)(\phi(1)+1)$ . So, I've concluded that $N-1 \sim \text{Geometric}(1-p)$ . which would give me $E(N) = \frac{2-p}{1-p}$ .  However, I have no idea how $\phi(1) = 1+\frac{2-p}{1-p}(\phi(1)+1)$ . It kind of looks like Wald's equation could be used here, but I dont see how it could be. Any help would be appreciated.","['stochastic-processes', 'markov-chains', 'probability-theory', 'probability']"
4137060,Why do we define Measurable functions by their pre-image?,"Let $(X,\mathcal{M})$ and $(Y,\mathcal{N})$ be measurable spaces. We call the function $f:X\to Y$ to be $(\mathcal{M},\mathcal{N})$ -measurable if $f^{-1}(E)\in \mathcal{M}$ for all $E\in \mathcal{N}$ . Similarly, when we are trying to define ""nearness"" in functions we call a function to be continuous if the preimage of the open set is open. There might be other types of similar definitions (if so, I'd appreciate more examples!), but my question is why we define the particular function always based on the preimage? Thanks in advance!","['measure-theory', 'real-analysis']"
4137094,Vector Space as a variety/scheme,"http://www.math.ubc.ca/~reichst/edtotal.pdf In page 9 of this paper there is a notation that I don't understand: $W_d(k)$ . Let $V$ be a variety, and $k[V]$ be its coordinate ring. Let $W_d$ be the sub (vector) space of $k[V]$ of polynomials up to degree $d$ . I see we can associate to a vector space a projective scheme and/or an affine scheme. But I was wondering if that is what is happening here (since I think the notation for that would be $P(k[V])$ for example)? Usually for a variety/scheme $X$ , $X(k)$ stands for the $k$ -rational points of $X$ . Is $W_d$ being treated as a variety, and is $W_d(k)$ its $k$ -rational points?",['algebraic-geometry']
4137095,Continuity of $f$ and monotone decreasing of $g(x) = \cos(f(x))$ implies uniform continuity of $f$,"Let $f,g : \mathbb{R} \rightarrow \mathbb{R}$ . I want to show uniformly continuity of $f$ under $f$ is continuous and $f(0) = \frac{\pi}{2}$ , $g(x) = \cos(f(x))$ is monotonically decreasing. I know if $f$ is differentiable and $|f'|$ is bounded then $f$ is uniformly continuous. And from $g(x)$ is monotonically decreasing if $g$ is differentiable, then naively I can guess $g'(x) <0$ (decreasing) and deduce $g'(x) = - f'(x) \sin(f(x))<0$ ,  ... But the information given here is not enough to show $g$ and $f$ are differentiable.","['uniform-continuity', 'analysis', 'real-analysis']"
4137105,Find the unique solution to the IVP $x' = Ax$ where $A = \begin{bmatrix} {-3}&{2} \\ {-1}&{-1}\end{bmatrix}$,"I began this problem by evaulating $x' = Ax$ . Let $$ x' = \begin{bmatrix}{x_1} \\ {x_2}\end{bmatrix}.$$ Then we have $$x'_1=-3x_1 + 2x_2, $$ $$x'_2=-x_1 - x_2, $$ $$x_1(0)=1,$$ $$x_2(0)=-2.$$ From the first equation we have $x_2= \frac{1}{2}x'_1+ \frac{3}{2}x_1$ . Substituting this into the second equation yields: $$x''_1+4x'_1-2x_1=0,$$ The characteristic equation of this is $m^2+4m-2=0$ , which has the solutions $m = -2 \pm \sqrt{6}$ . However, I am confused how to proceed to finding the final solution. Any guidance is greatly appreciated!","['initial-value-problems', 'matrix-equations', 'ordinary-differential-equations']"
4137149,The regularity of a piecewise constant function in Sobolev spaces,"I want to know what the ""highest"" regularity is for a piecewise constant function. For example: $$ f(x)=\left\{\begin{align*} &1, & x\in [0,1),\\ &0, & x\in (-1,0).\end{align*}\right. $$ We know that $f\in L^p(\Omega)$ where $\Omega=(-1,1)$ and $p\geq 1$ , but $f\not\in W^{1,p}(\Omega)$ ( $W^{k,p}$ denotes the Sobolev space whose functions and their $k$ -th derivatives are both in $L^p$ space) since $f(0^+)\not=f(0^-)$ . Now, I am wondering may the function $f(x)$ has any higher regularities? such as $f\in W^{\varepsilon,p}(\Omega)$ with $0 < \varepsilon < \frac12$ (or $0<\varepsilon <1$ )? If this is right, how to prove it?","['real-analysis', 'sobolev-spaces', 'functional-analysis', 'regularity-theory-of-pdes', 'piecewise-continuity']"
