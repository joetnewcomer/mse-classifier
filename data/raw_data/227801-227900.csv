question_id,title,body,tags
4711318,"Show $f(x,y)=\left(\frac{\cos x+\cos y-1}{2}, \cos x-\cos y\right)$ has a fixed point using the mean value inequality.","The problem: I am attempting to show $$f(x,y)=\left(\frac{\cos x+\cos y-1}{2}, \cos x-\cos y\right)$$ has a fixed point using the mean value inequality, which states that for any $a \in \mathbb{R}^n$ and $f: B_r(a) \to \mathbb{R}^m$ differentiable on $B_r(a)$ with $\|Df(x)\| \leq M$ for all $x \in B_r(a)$ , we have $|f(b_1) - f(b_2)\| \leq M\|b_1 - b_2\|$ for any $b_1, b_2 \in B_r(a)$ . If I can find such an $M$ on some ball, and show $M < 1$ , then I am done by the contraction mapping theorem. Here's what I've done: Note that $|\frac{\cos x+\cos y-1}{2}| \leq \frac{3}{2}$ and $|\cos x-\cos y| \leq 2$ by applying the triangle inequality, so the fixed point should be on $B_{r}(a)$ with $r=2$ & $a=(0,0)$ . We can calculate that $$Df(x,y)=\pmatrix{
-\frac{\sin x}{2} & -\sin x \\\
-\frac{\sin y}{2} & \sin y
}$$ Now, I'm a bit stuck on the interpretation of the mean value inequality, specifically how to find and get $\|Df(x)\|$ . If I'm interpreting things correctly, I believe this should be using the operator norm, which I'm not confident in using. If so, I know that for $A \in L(\mathbb{R}^n, \mathbb{R}^m)$ $$|A|=\sup_{0 \neq x' \in \mathbb{R}^n} \frac{||Ax'||}{||x'||}$$ I've tried letting $x' = (u,v) \in B_{2}(0)$ be a vector on the ball and calculate $$\sup_{0 \neq x' \in B_{2}(0)} \frac{\sqrt{((\sin^2 x + \sin^2 y) (\frac{1}{2}u+v)^2)}}{\sqrt{u^2+v^2}}$$ (the numerator is $|Df(x,y)x'|$ ) as an upper bound for $M$ . But I can't seem to get this bound lower than $1$ in order to use the contraction mapping theorem, I'm getting things above $1$ . I'm not sure where the problem is, perhaps I'm misunderstanding the theory. Is this calculation wrong? I'm thinking, perhaps, $x'$ should be $(x,y)$ , i.e. the same variables used in $f$ , but I'm not sure if this is correct.","['fixed-point-theorems', 'trigonometry', 'analysis', 'real-analysis']"
4711347,What's the idea of Dirichlet’s Theorem on Arithmetic Progressions proof?,"Dirichlet’s Theorem on Arithmetic Progressions says that if $a, m$ are natural numbers such that $gcd (a,m) = 1$ , then there are infinitely many prime numbers in the arithmetic progression $a + km, k \in \mathbb{N}$ .
I would like to know what is the idea of the proof, and the sketch of the proof, to try to understand it. I understand the Dirichlet L-functions, to obtain the set of complex values such that the Dirichlet L-functions becomes zero, the Dirichlet character, but I heard that ""Dirichlet's theorem is proved by showing that the value of the Dirichlet L-function (of a non-trivial character) at 1 is nonzero"", how is that? Can someone explain how a arithmetic progression is related with a L-function?","['dirichlet-character', 'number-theory', 'arithmetic-progressions', 'l-functions', 'prime-numbers']"
4711348,Why does this trig equation have only 2 solutions and not 4?,"Upon solving this equation: $$
5\sin x - 5\cos x = 2
$$ for the interval $0 \le x < 360^\circ$ , I manipulated it in order to get a solvable trigonometric equation below: $$
\sin 2x = 21/25
$$ Normally this would procure 4 solutions: $28.6^\circ, 61.4^\circ, 208.6^\circ, 241.4^\circ$ (one can easily plot this into Desmos to see that it is true). However, the answers only include the middle 2 solutions: $61.43^\circ, 208.57^\circ$ . Why is this? Why do 2 of the solutions simply disappear? Could it have something to do with the way in which I manipulated the original equation (by squaring and substituting using the appropriate trigonometric identities)? After looking at the original equation, it makes sense to some degree, but then I'm not a fan of such methods, especially since one could encounter more complicated equations in the future. Am I missing/forgetting something fundamental?","['trigonometry', 'problem-solving']"
4711388,Integration using Stokes' theorem,"I have a problem that I can't seem to figure out.
Given a surface $S$ : \begin{cases}
x^2+y^2 \leq 1 \newline
z = y^2
\end{cases} Let $C$ be the edge of $S$ . $C$ is oriented so that the projection of $C$ on the $xy$ -plane runs counter-clockwise. Calculate: \begin{equation}
\oint_{C}y^2dx + xy^2 dy+ xzdz
\end{equation} Directly Using Stokes's theorem I've tried to make a sketch of the area with the orientation: I've at least gotten \begin{equation}
\vec{F} = y^2 \vec{i} + xy^2 \vec{j} + xz \vec{k}
\end{equation} and \begin{equation}
curlF = -z\vec{j} + (y^2-2y)\vec{k}
\end{equation} For calculating the integral directly I've tried a parametrization: \begin{cases}
x = r\cos(t) \newline
y = r\sin(t) \newline
z = y^2 = r^2\sin^2(t)
\end{cases} with $0 \leq r \leq 1$ and $0 \leq t \leq 2\pi$ .
This gave me: \begin{equation}
\int_{0}^{1} \int_{0}^{2\pi} r^2\sin^2(t) + r^2\cos(t)\sin^2(t) + r^3\cos(t)\sin^2(t) dtdr = \frac{1}{3} \pi
\end{equation} But now for $\iint_{R} curlF \cdot N dS$ I can't figure out how to proceed. I can't seem to figure out what $N$ is and what integration bounds I need to use. The examples in my book and my lecture  aren't all that similar and I don't see how to apply those to this situation.
If someone could give me a hint on how to proceed I'd be very grateful.","['integration', '3d', 'curl', 'multivariable-calculus', 'stokes-theorem']"
4711391,Show group defined via quotient of matrix $A$'s columns has order $|\det A|$.,"The problem: Let $M \leq \mathbb{Z}^n$ be the subgroup generated by the rows of an $n \times n$ matrix $A$ with entries in $\mathbb{Z}$ . Show that $G=\mathbb{Z}^n/M$ is finite if and
only if $\det A \neq 0$ , and in that case the order of $G$ is $|\det A|$ . Let's label the rows of $A$ by $v_1, ..., v_n \in \mathbb{Z}^n$ . The if and only if (solution verification) Firstly, I think I understand how to show the if and only if. Here's what I did, if there are any mistakes please correct me. If $\det A = 0$ , then there will be some $v \in \mathbb{Z}$ such that $v \notin \text{span} \left(v_1, ..., v_n \right)$ . Then $v+M, 2v+M, 3v+M, ...$ are all distinct elements of $G$ . Conversely, if $\det A \neq 0$ , we can always write any $v$ as some linear combination $v=\sum_{i=1}^n q_i v_i$ where $q_i \in \mathbb{Q}$ . $q_i$ will have denominator at most the lowest common multiple of the integers in $v_i$ , say $b_i$ , otherwise we'll not get an integer. Then we can write any element of $G$ as $v+M=\sum_{i=1}^n \{q_i\} v_i$ where $q_i \in \mathbb{Q}$ where $\{ \cdot \}$ denotes the fractional part. Thus there's at most $\prod_{i=1}^{n} b_i < \infty$ elements in $G$ . $|G|=|\det A|$ (help) I'm stuck on this. I don't really get how I can relate anything I've done in the first part of this to the determinant, the use of the lowest common multiple or the product $\prod_{i=1}^{n} b_i$ doesn't seem to lend too easily into thinking about determinants, but perhaps I'm missing something. I'm thinking we could perhaps try to show that the group is isomorphic to the same group with $A$ in Smith Normal Form, which would then lead nicely into thinking of the determinant, since it'll be a diagonal matrix, but I'm not really sure how to approach that.","['determinant', 'matrices', 'quotient-group', 'linear-algebra', 'group-theory']"
4711402,Correlation between dependent variables (binomial distribution),"Good afternoon. I have the following problem concerning Poisson and binomial distributions. Let $X_1, ..., X_n, n\geq 2$ be independent random variables, Poisson distributed with a parameter $\lambda >0$ . Let $k \geq 2$ . Let $(Z_1, ..., Z_n)$ be a random vector with the following distribution: $P(Z_1=x_1,..., Z_n = x_n) = P(X_1 = x_1, ..., X_n = x_n|X_1+...+X_n=k)$ . Calculate the correlation coefficient between $Z_1$ and $Z_2$ . I have already calculated $P(Z_1 = t) =P(X_1 = t|X_1+...+X_n = k)$ and found that it has binomial distrubution with parameters $(k, \frac{1}{n})$ . The same holds true for $P(Z_2 =  t) =P(X_2 = t|X_1+...+X_n = k)$ . If I'm right, I should be calculating $E(Z_1 Z_2)$ right now. Can someone please explain to me how to use the fact that these two variables are dependent to calculate that expected value?","['statistics', 'probability']"
4711415,Integral related to binomial coefficient $\frac{1}{2\pi}\int_{-\pi}^{\pi}\frac{(1+e^{ix})^n}{e^{ixk}}~dx$,"I want to show that $$\frac{1}{2\pi}\int_{-\pi}^{\pi}\frac{(1+e^{ix})^n}{e^{ixk}}~dx={n\choose k}$$ My attempts were very poor, since I'm not an expert in contour integration.I thought that, in order to have factorials on the right, the integral could be reduced to a Beta function, but substituting $t=e^{ix}$ doesn't work because of the bounds $-\pi,\pi$ . I am wondering if anyone could give me a shove in the right direction.","['integration', 'definite-integrals', 'complex-analysis', 'contour-integration', 'residue-calculus']"
4711421,Why does the graph of $-\left(x\right)\log_{2}\left(x\right)-\left(1-x\right)\log_{2}\left(1-x\right)$ look like: $\cap$?,"I can understand it algebraically, but I wanted some geometric-esq intuition for why the graph is this way. Like some intuition that I could've used to derive it. This is the plot: I eked out some explanation for what's happening (not sure how correct it is): A log: We want it to swoop back in to zero instead of going to -infinity though. So, we weight it by $x$ (which gets smaller as it goes to $- \infty$ Flip it Now copy it, flip horizontally, and push it to 1 (since we want the graph to be from 0 to 1). My brain is happy with everything above, what it isn't happy with is that when I add both of them together: the part in the middle gets added as expected but the parts at the ends just go away? What witchcraft is this?","['intuition', 'functions', 'graphing-functions', 'logarithms']"
4711471,Let $f$ be a differentiable function with no point $x$ such that $f(x)=0=f'(x)$ show that $f$ has finitely many zeros.,"Let $f: [0, 1] \rightarrow R$ be a differentiable function. Assume there is no
point $x$ in $[0,1]$ such that $f(x) = 0 = f'(x)$ . Show that $f$ has only a finite
number of zeros in $[0, 1]$ . My proof. Assume otherwise. Keep bisecting the interval choosing the subinterval with infinitely many zeros. (This is fairly. standard so I won't go into it). We obtain $(x_n)$ such that $f(x_n)=0$ for all $n$ . Moreover, $x_n\rightarrow x$ as $n\rightarrow \infty$ . We see immediately that $f(x)=0$ . Our goal is to show $f'(x)=0$ as well. We know, $$
\lim_{h\rightarrow 0}\dfrac{f(x+h)}{h}=L
$$ There's a subsequence $x_{n_k}$ , $h_k = x_{n_k}-x\ge 0$ , (if not we will use $x-x_{n_k}$ and the proof will be similar) and we observe for every $h$ , there's $N$ such that if $k\ge N$ , $h_k=x_{n_k}-x\le h$ . Thus we observe, $$
\lim_{k\rightarrow \infty }\dfrac{f(x+h_k)}{h_k}=L=0
$$ The last part is due to the fact $f(x+h_k)=0$ . Contradiction! I am only looking for proof verification. $\textbf{Please only provide hints if my proof is wrong. Complete solutions won't benefit me at all!}$","['solution-verification', 'derivatives']"
4711501,Why Sina.Cosb and Cosa.Sinb are two different identities? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 11 months ago . Improve this question Should not Sina.Cosb and Cosa.Sinb be same if we exchange a and b? Why there is an extra negative sign for Cosa.Sinb ?","['trigonometry', 'functions', 'geometry']"
4711505,Prove the inequality $|z^2 - \pi| < \varepsilon$,"Given $\varepsilon < 1$ , prove that if $|z - \sqrt \pi| < \frac{\varepsilon}{4}$ , then $$|z^2 - \pi| < \varepsilon$$ My attemps In order to prove this inequality, I have been trying different things but I keep getting stuck. First of all, I saw that $|z^2 - \pi| = |z - \sqrt \pi| \cdot |z + \sqrt \pi|$ , so there´s a start. By the triangular inequality, $|z \pm \sqrt \pi| \leq |z| + |\sqrt \pi|$ . Therefore $$|z^2 - \pi| = |z - \sqrt \pi| \cdot |z + \sqrt \pi| < \frac{\varepsilon}{4} \cdot |z + \sqrt \pi| \leq \frac{\varepsilon}{4} (|z| + \sqrt \pi)$$ Since $\sqrt \pi < 2$ , then $$\frac{\varepsilon}{4} (|z| + \sqrt \pi) = \frac{\varepsilon}{4} |z| + \frac{\varepsilon}{4} \pi <  \frac{\varepsilon}{4} |z| + \frac{\varepsilon}{2}$$ If I could prove that $|z| \leq 2$ , then I'll be done but I couldn't figure out how. Other approach I took was trying mean inequalities: $$|z^2 - \pi| = |z - \sqrt \pi| \cdot |z + \sqrt \pi| \leq (\frac{|z - \sqrt \pi| +|z + \sqrt \pi|}{2})^2$$ $$= \frac{|z - \sqrt \pi|^2 + 2|z - \sqrt \pi||z + \sqrt \pi|  +|z + \sqrt \pi|^2}{2}$$ $$= \frac{z^2 + 2 \sqrt \pi z + \pi + 2z^2 - 2\pi + z^2 - 2 \sqrt \pi z + \pi }{4} = z^2$$ I could not reach anywhere neither. Any help is truly appreciated.","['inequality', 'complex-numbers', 'analysis', 'real-analysis']"
4711543,Easiest way to see that $\log X < X^a$ as $X \to \infty$ for fixed $a > 0$,"Let $a > 0$ . It is ""obvious"" that for large enough $X$ , we have $X^a > \log X$ . I would like to prove this using as little machinery as possible. Here are two approaches: One way to do this is to note that our goal is equivalent to $X < \exp\{X^a\}$ as $X\to\infty$ . The Taylor series of $\exp$ at $0$ will have terms of order greater than $X$ , so we can see the aymptotic from that. We could also observe that $$
X^a - \log X = 1 + \int_1^X t^{-1}(t^a - 1)dt.
$$ Let $C$ be a real number such that $C^a \geq 2$ . For $X \geq C$ , we have $$
\int_C^X t^{-1}(t^a - 1) dt \geq \int_C^X t^{-1} = \log X - \log C,
$$ and it follows that $$
X^a - \log X = k + \log X 
$$ for some constant $k$ . Since $\log X \to \infty$ as $X\to \infty$ , it follows that $X^a > \log X$ for large enough $X$ . These arguments both feel a bit unsatisfying to me. The first one uses Taylor series, which feel like quite heavy machinery, and the second one, while more elementary, feels like it doesn't really get to the heart of why $X^a$ is bigger than $\log X$ . Can anyone see a more satisfying argument? I would also accept an answer that convinces me that one of my two arguments is actually a nice solution; currently I don't have much intuition for the aesthetics of these sorts of proofs.","['analysis', 'real-analysis']"
4711617,A non-Riemann integrable function that is Lebesgue integrable,"I'm seeking a function that is non-Riemann integrable yet Lebesgue integrable. Everyone seems to illustrate this phenomenon with the indicator function applied to rational numbers. Is there another such function... perhaps one that would be of greater interest and background to students... one that can be (approximately) plotted? We need an uncountable number of singularities.  A function such as $f(x) = \frac{1}{\sin 1/x}$ on $[0,1]$ (plotted) has an infinite number of discontinuities, but these can be counted.","['integration', 'riemann-integration', 'lebesgue-integral', 'examples-counterexamples']"
4711672,Two conjectured identities of sums involving $\sin(x\sin x)/x^2$,"While tackling this question , I came up with the conjecture that the following identities hold: Conjecture. Let $f(x)=\dfrac{\sin(x\sin x)}{x^2}$ . Then $$ \sum_{k=-\infty}^{\infty} f(k\pi+x) = 1 \qquad\text{and}\qquad \sum_{k=-\infty}^{\infty} (-1)^k f(k\pi + x) = \cos x. $$ Unfortunately, I have absolutely no idea how to tackle this problems. The only evidence I have for this one is a numerical simulation: ( Figure. Left: A comparison between the partial sum $\sum_{|k|\leq n} (-1)^k f(k\pi + x)$ and $\cos x$ / Right: A comparison between the partial sum $\sum_{|k|\leq n} f(k\pi + x)$ and $1$ ) If the conjecture turns out to be true, then we can prove some fun identities, including the original posting that motivated this question. For example, we obtain the following identity as a by-product: $$ \int_{-\infty}^{\infty} \frac{\sin(x\sin x)}{x^2} \, \mathrm{d}x
= \int_{0}^{\pi} \sum_{k=-\infty}^{\infty} f(k\pi + x) \, \mathrm{d}x
= \int_{0}^{\pi} \mathrm{d}x
= \pi $$","['calculus', 'summation', 'real-analysis']"
4711762,Second derivative calculation,"Consider $\boldsymbol{\psi}(\boldsymbol{q})\in R^2$ is a function of vector $\boldsymbol{q}(t)\in R^3$ which is a function of time $t\in R^+$ . The first time derivative of function $\boldsymbol{\psi}$ can be defined as following equation using chain rule: $\dot{\boldsymbol{\psi}}=\frac{\partial{\boldsymbol{\psi}}}{\partial{\boldsymbol{q}}}\dot{q}$ .
In this equation $\frac{\partial{\boldsymbol{\psi}}}{\partial{\boldsymbol{q}}}\in R^{2\times3}$ is the Jacobian matrix. How it is possible to define the second time derivative of $\ddot{\boldsymbol{\psi}}$ using chain rule? For example if the function $\psi$ and $q$ were scalars we could define its second derivative easily by the following equation $\ddot{\psi}=\frac{\partial^2{\psi}}{\partial{q^2}} \dot{q}^2+\frac{\partial{\psi}}{\partial{q}}\ddot{q}$ .
But what about this case? Is there any similar equation for vector forms of $\boldsymbol{\psi} \in R^2$ and $\boldsymbol{q}(t) \in R^3$ ?","['partial-derivative', 'jacobian', 'derivatives', 'chain-rule']"
4711801,Integral with Wiener shift,"Consider the integral $$\int_{- \infty}^{0} e^{s} d B_s(\theta_t \omega)$$ where $B_t$ is Brownian motion and $\theta_t$ is Wiener shift on the canonical probability space.
I know the property that $dB_s(\theta_t \omega) = d (B_{t+s}-B_t)$ . But I cant understand why the above integral is the same as $$\int_{- \infty}^{t} e^{s-t} d B_s(\omega).$$ Can you show the steps to get this integral?","['stochastic-integrals', 'stochastic-processes', 'probability-theory']"
4711806,Trace inequality for operators,"Let T be an trace class operator and A be bounded.
I found on wiki: https://en.wikipedia.org/wiki/Trace_class that $$
|Tr(TA)|\leq || T||  ||A||
$$ Does anyone know how to start the proof?
And more important, does anyone know if this implies that $$
|Tr(TA)|\leq |Tr(T)|  ||A||
$$","['svd', 'operator-theory', 'trace', 'functional-analysis', 'inequality']"
4711823,"Can you fit nine points, whose pairwise distances apart are all greater than 1, in an equilateral triangle with side length 3?","Can you show how to fit nine points, whose pairwise distances apart are all greater than 1, in an equilateral triangle with side length 3 or can you prove that it isn't possible to do so? It is a well-known problem to show, using the pigeonhole principle, that there cannot be five points in an equilateral triangle of side length $2$ whose pairwise distances apart are greater than $1$ , and that there cannnot be ten points in an equilateral triangle of side length $3$ whose pairwise distances apart are greater than $1$ . In the case of side length $2$ , dividing the triangle into four equilateral triangles of side length $1$ and placing points at every vertex of these triangles allows six points to be placed whose pairwise distances apart are no more than $1$ , and it is easy, by placing points at the three vertices and centre of the original triangle, to fit four points whose pairwise distances apart are greater than $1$ . However, in  the case of side length $3$ , while dividing the triangle into nine equilateral triangles of side length $1$ and placing points at every vertex of these triangles allows ten points to be placed whose pairwise distances apart are no more than $1$ , I cannot see how to fit nine points whose pairwise distances apart are greater than $1$ (eight seems quite straightforward). Is this possible, or can you prove that it isn't? I tried taking the ten-point pattern, removing a point and then shifting the remaining nine slightly to increase the distances between them, but there appears to be a residual rigidity in the structure with one point removed that prevents you doing this. Of course, there might be a way of starting from scratch and achieving the desired outcome with a pattern which is not a perturbation of the ten-point one.","['combinatorial-geometry', 'geometry']"
4711827,$\partial^2_{\phi}f(\phi) +(A+B\phi+C\phi^2)f(\phi)=0$,"I have the differential equation $$
\partial^2_{\phi}f(\phi) +(A+B\phi+C\phi^2)f(\phi)=0
$$ where $A, B, C\in\mathbb{C}$ and $\phi\in[0,2\pi]$ . I need the function to be antiperiodic, $f(\phi)=-f(\phi+2\pi)$ . Under what conditions for $A,B,C$ does a solution exist? Is there a name for this family of differential equations. Are there analitical solutions?",['ordinary-differential-equations']
4711843,finding the values of a and b(real numbers) that makes the function differentiable at any point of its domain,"I'm given this function: $$
f(x) = \begin{cases}
(lnx)^4  & 0<x< e \\
ax+b & x≥e
\end{cases}
$$ and I'm asked for which real values of 'a' and 'b' the function is differentiable at any point I understand that for every x≠1 the function is differentiable. and now I'm trying the find the values of 'a' and 'b' that make the function differentiable at x=e this is what I did so far: first I demanded that the function will be Continuous at x=e, meaning I demand that $\lim\limits_{x \to e^{-}} f(x)
= \lim\limits_{x \to e^{+}} f(x) = f(e)$ and I got the equation: $ae+b = e$ next, I demanded that the function will be differentiable at x = e,
I demanded that $\lim\limits_{h \to 0^{-}} \frac{f(e+h)-f(e)}{h}
= \lim\limits_{h \to 0^{+}} \frac{f(e+h)-f(e)}{h}$ so far I got: $\lim\limits_{h \to 0^{+}} \frac{f(e+h)-f(e)}{h} = \lim\limits_{h \to 0^{+}} \frac{(a(e+h)+b) - (ae+b)}{h} = \lim\limits_{h \to 0^{+}} \frac{(ae+ah+b) - (ae+b)}{h} = \lim\limits_{h \to 0^{+}} \frac{ah}{h} = \lim\limits_{h \to 0^{+}} a = a$ but when I tried to calculate $\lim\limits_{h \to 0^{-}} \frac{f(e+h)-f(e)}{h}$ I got stuck, so far I got $\lim\limits_{h \to 0^{-}} \frac{f(e+h)-f(e)}{h} = \lim\limits_{h \to 0^{-}} \frac{(ln(e+h))^4 - (ae+b)}{h} = ???$ now I don't know how to solve this, I'll be glad of any help you can give me, should I calculate the derivative function at $0<x<e$ and at $x>e$ using the chain rule and then demand that $f'(1+) = f'(1-)  $ or should I stick to the limit definition of the derivative?","['calculus', 'parametric', 'limits', 'derivatives', 'piecewise-continuity']"
4711883,Orientation preserving isometries of the sphere,"I know that the isometries of $\mathbb{S}^n\subseteq\mathbb{E}^{n+1}$ are $O(n+1)$ . Intuitively, I think that $SO(n+1)$ should be the orientation-preserving ones. But I'd like to prove this rigourosly. I know that a map is orientation preserving iff its differential at any point is orientation preserving. Clearly if $A\in O(n+1)$ is an isometry, then $d_pA(v)=Av$ but $d_pA$ is NOT an endomorphism (it has different domain and codomain) so I cannot simply say that it's orientation  preserving iff $\det(A)>0$ . Right? How do I solve this?","['orientation', 'linear-algebra', 'riemannian-geometry', 'differential-geometry']"
4711898,Find greatest value of $P = \left |3 i (z_1 + z_2) + 9 - z_1z_2\right |.$,"Let be two complexs numbers $z_1$ , $z_2$ satisfying the conditions $|z_1| = 1$ , $|z_2| = 1$ , $\left| z_1 + z_2\right| =\sqrt{2}$ . Find the least and the greatest value of the value $$P = \left |3 i (z_1 + z_2) + 9 - z_1z_2\right |.$$ Solution. Suppose $z_1 = 1$ , $z_2 = x + i y. $ we find $z_2$ by the system of equations $$\begin{cases}
|z_2| = 1,\\
\left| z_1 + z_2\right| =\sqrt{2}	
	\end{cases}$$ that is mean $$\begin{cases}
x^2 + y^2 = 1,\\
(x+1)^2 + y^2 =2.	
\end{cases}$$ Solve this system, we have $(x,y)=(0,1) $ or $(x,y)=(0,-1) $ , and then $z_2 = i$ or $z_2 = -i$ With $z_1 = 1$ , $z_2 = i$ , we have $$P = \left |3 i (1 + i) + 9 - i\right | = 2\sqrt{10}.$$ With $z_1 = 1$ , $z_2 = i$ , we have $$P = \left |3 i (1 - i) + 9 + i\right | = 4\sqrt{10}.$$ Thus $\max P =4\sqrt{10} $ and $\min P =4\sqrt{10}. $","['extreme-value-theorem', 'complex-analysis', 'maxima-minima', 'calculus', 'complex-numbers']"
4711949,"If $x:[0,1]\rightarrow \mathbb{R}$ verifies $0\leq x(t)\leq \int_0^te^sx(s)^2ds \ \forall t\in [0,1]$ then $x(t)=0 \ \forall t\in[0,1]$","As part of my differential equations homework we have this problem: If $x:[0,1]\rightarrow \mathbb{R}$ verifies $0\leq x(t)\leq \int_0^te^sx(s)^2ds \ \forall t\in [0,1]$ then $x(t)=0 \ \forall t\in[0,1]$ I tried bounding  the integral using that $x$ is continuous and $[0,1]$ compact but I gained no useful info. Reasoning by contradiction doesn't seems useful neither, and I couldn't use any result from this unit as it was all about solving linear ODEs of order $\geq 2$ . Any help or hint is appreciated","['analysis', 'integral-inequality', 'ordinary-differential-equations']"
4711961,How should I understand such recursive definition of rooted tree?,"The Definition is: BASIS STEP : A single vertex r is a rooted tree. RECURSIVE STEP : Suppose that T 1 ,T 2 ,…,T n are disjoint rooted trees with roots
r 1 , r 2 , …, r n , respectively. Then the graph formed by starting with a root r, which is not
in any of the rooted trees T 1 ,T 2 ,…,T n , and adding an edge from r to each of the vertices r 1 , r 2 , …, r n , is also a rooted tree. And it put a picture to illustrate some of the rooted trees formed starting with the basis step and applying the recursive step one time and two times. I cannot get any idea to connect the definition and the illustrated picture. Because, according to the definition, we have only one element of the set of rooted trees when we are in basis step, that is, the rooted tree with only one vertex. And then we applied recursive step one time, we should only produce one new rooted tree to the set, which were formed like the first one in the step 1 in the illustrated picture. Why there were four, even more, new rooted tree produced with applying recursive step only one time? Where does I misunderstand?","['trees', 'definition', 'discrete-mathematics']"
4711970,"What's the intuitive meaning of matrix trace, if any?","3Blue1Brown has great visual explanations of what some commonly used properties of matrices mean, such as determinant, rank, and kernel, but I have absolutely no idea what the trace of a matrix has in linear algebra and what its visual representation (if any) is.","['matrices', 'trace', 'linear-algebra', 'intuition']"
4711971,Can $1!^2+2!^2+3!^2+\dots+n!^2$ be a perfect power when $n\geq2$?,"I know that $S_n:=1!^2+2!^2+3!^2+\dots+n!^2$ cannot be a perfect square because it is equal to $2\pmod{3}$ and it is never a perfect cube because it is equal to $5\pmod{9}$ , but can $S_n$ be a higher odd perfect power? Edit: $S_n$ cannot be also a perfect 5th power when $n\geq10$ , since $17$ is not a 5th power residue $\pmod{100}$ . $S_n$ is also never a perfect 11th power if $n\geq23$ , since $8$ is not a 11th power residue $\pmod{23}$ . $S_n$ is not a perfect 7th power for all $n\geq29$ because $7$ is not a 7th power residue $\pmod{29}$ . $S_n$ is not a perfect 13th power if $n\geq52$ , since $7$ is not 13th power residue $\pmod{53}$ . $S_n$ isn't a perfect 17th power when $n\geq137$ , since $70$ is not a 17th power residue $\pmod{137}$ . $S_n$ isn't a perfect 19th power when $n\geq191$ , since $44$ is not 19th power residue $\pmod{191}$ . $S_n$ is never a perfect 23rd power if $n\geq47$ , since $30$ is not a 23rd power residue $\pmod{47}$ . $S_n$ is not a perfect 29th power for all $n\geq59$ , since $5$ is not 29th power residue $\pmod{59}$ . If $n\geq310$ , then $S_n$ is never a perfect 31st power, since $62$ is not a 31st power residue $\pmod{311}$ . $S_n$ is not a perfect 37th power for all $n\geq148$ , since $88$ isn't a 37th power residue $\pmod{149}$ . $S_n$ cannot be a perfect 41st power for all $n\geq82$ , because $S_{82}\equiv45\pmod{83}$ , and since $x^{41}\equiv45\pmod{83}$ is not solvable, $45$ is not a 41st power residue $\pmod{83}$ .","['number-theory', 'elementary-number-theory', 'perfect-powers']"
4711986,Lebesgue Measure of an uncountable intersection of closed sets,"Consider the probability space $([0,1],\mathcal{B}([0,1]),\lambda)$ , where $\lambda(\cdot)$ is the Lebesgue measure and let $\{F_\gamma\}_\Gamma$ be an uncountable collection of closed sets in $[0,1]$ such that $(\forall \Gamma'\subset\Gamma)(|\Gamma'|<\aleph_0\Rightarrow\lambda(\bigcap_{\gamma\in\Gamma'}F_\gamma)\geq\frac{1}{2})$ .  Show that $\lambda(\bigcap_{\gamma\in\Gamma}F_\gamma)\geq\frac{1}{2}$ . If $\Gamma$ was countable this would be obvious but dealing with the the uncountable case is where I am not sure how to proceed. Since the $F_\gamma$ are closed any intersection of $F_\gamma$ 's are also closed and so Borel measurable and hence they are also Lebesgue measurable so $\lambda(\bigcap_{\gamma\in\Gamma} F_\gamma)$ is well defined. For any $\Gamma'\subset\Gamma$ with $|\Gamma'|=\aleph_0$ , wlog we can say $\Gamma'=\{\gamma_n\}_{n\in\mathbb{N}}$ , so we trivially have the decreasing sequence $A_n= (\bigcap_{i = \gamma_i}^{\gamma_n} F_{\gamma_i})$ and so by continuity of probability measures we have $\lambda(\bigcap_{\gamma\in\Gamma'} F_\gamma)=\lim_n\lambda(A_n)\ge\frac{1}{2}$ . Also just to note as $F_\gamma$ 's (and any intersections of them) are closed in $[0,1]$ they are also compact in $[0,1]$ . My only idea was to consider the outer-measure $\lambda^*$ with closed(hence compact) sets of $[0,1]$ as the generating set of the Borel-algebra of $[0,1]$ .  Then I tried to show that for any cover (of compact sets $\bigcup_n C_n$ ) of $\bigcap_{\gamma\in\Gamma}F_\gamma$ we can find a countable $\Gamma'$ such that $\bigcap_{\gamma\in\Gamma'}F_\gamma\subset \bigcup_nC_n$ which will then give the required result, but I've gotten nowhere with this and don't think it is correct. So any help or feedback with this problem is greatly appreciated.
Thanks in advance.","['measure-theory', 'lebesgue-measure', 'outer-measure']"
4712044,$P(X<Y<Z)$ for exponentially distributed independent random variables,"Let $X$ $Y$ $Z$ independent, exponentially distributed random variables with parameters $a$ $b$ $c$ respectively. Calculate $P(X<Y<Z)$ . One way is to compute integral of joint density. Second way which my textbook describe is: $$P(X<Y<Z)=P(X<\min(Y,Z))P(Y<Z)$$ $\min(Y,Z)$ is exponentially distributed with parameter $b+c$ and the rest is easy integration. However why is $P(X<Y<Z)=P(X<\min(Y,Z))P(Y<Z)$ true?
Why not $P(X<Y<Z)=P(X<Y)P(Y<Z)$ ? (independent question)","['independence', 'probability-distributions', 'exponential-distribution', 'probability']"
4712088,Law of Large Numbers for a Brownian Motion,"I am self-learning introductory stochastic calculus from A first course in Stochastic Calculus by L.P.Arguin. The part(c) of the below exercise problem on the time-inversion property of Brownian motion asks to derive the law of large numbers using the previously developed results. I struggled to write a proof of this. I would like to ask, if my upper bounds and convergence for part (c) make sense, and is technically correct and rigorous. I reproduce my solution to parts (a) and (b) for completeness. Time Inversion. Let $(B_{t},t\geq0)$ be a standard brownian
motion. We consider the process: \begin{align*}
X_{t} & =tB_{1/t}\quad\text{for }t>0
\end{align*} This property relates the behavior of $t$ large to the behavior
of $t$ small. (a) Show that $(X_{t},t>0)$ has the distribution of Brownian motion
on $t>0$ . Proof. Like $B(t)$ , it is easy to show that $X(t)$ is also a Gaussian process. Also, $\mathbb{E}[X_{s}]=0$ . Let $s<t$ . We have: \begin{align*}
Cov(X_{s},X_{t}) & =\mathbb{E}[sB(1/s)\cdot tB(1/t)]\\
 & =st\mathbb{E}[B(1/s)\cdot B(1/t)]\\
 & =st\cdot\frac{1}{t}\\
 & \quad\left\{ \because\frac{1}{t}<\frac{1}{s}\right\} \\
 & =s
\end{align*} Consequently, $X(t)$ has the distribution of a Brownian motion. (b) Argue that $X(t)$ converges to $0$ as $t\to0$ in the sense
of $L^{2}$ -convergence. It is possible to show convergence almost
surely so that $(X_{t},t\geq0)$ is really a Brownian motion for $t\geq0$ . Solution . Let $(t_{n})$ be any arbitrary sequence of positive real numbers
approaching $0$ and consider the sequence of random variables $(X(t_{n}))_{n=1}^{\infty}$ .
We have: \begin{align*}
\mathbb{E}\left[X(t_{n})^{2}\right] & =\mathbb{E}\left[t_{n}^{2}B(1/t_{n})^{2}\right]\\
 & =t_{n}^{2}\mathbb{E}\left[B(1/t_{n})^{2}\right]\\
 & =t_{n}^{2}\cdot\frac{1}{t_{n}}\\
 & =t_{n}
\end{align*} Hence, \begin{align*}
\lim\mathbb{E}\left[X(t_{n})^{2}\right] & =\lim t_{n}=0
\end{align*} Since $(t_{n})$ was an arbitrary sequence, it follows that $\lim_{t\to0}\mathbb{E}[(X(t))^{2}]=0$ . (c) Use this property of Brownian motion to show the law of large
numbers for Brownian motion: \begin{align*}
\lim_{t\to\infty}\frac{X(t)}{t} & =0\quad\text{almost surely}
\end{align*} Proof Sketch. Let $(t_n)$ be an arbitrary sequence such that $(t_n)\to \infty$ . Thus, $\forall n \in \mathbf{N}$ , $\exists t_{k_n}$ , such that $t_{k_n} > n$ . Consider the sequence of random variables $X_n := X(t_n)$ . Let $\epsilon$ be arbitrary. We have: \begin{align*}
\mathbf{P}\left(\left|\frac{X(t_n)}{t_n}\right|>\epsilon\right) &= \mathbf{P}\left[\left(\frac{X(t_n)}{t_n}\right)^4>\epsilon^4\right]\\
&= \mathbf{P}[X(t_n)^4 > t_n^4 \epsilon^4]\\
&\leq \frac{1}{t_n^4 \epsilon^4} \mathbf{E}[X(t_n)^4]\\
& \quad \left\{ \text{ Chebyshev's inequality }\right\} \\
&= \frac{1}{t_n^4 \epsilon^4} \cdot 3t_n^2 \\
& \quad \left\{ \text{ Fourth moment of a standard brownian motion }\right\} \\
&= \frac{3}{\epsilon^4} \cdot \frac{1}{t_n^2} \\
&\leq \frac{3}{\epsilon^4} \cdot \frac{1}{t_{k_n}^2} \\
&\leq \frac{3}{\epsilon^4} \cdot \frac{1}{n^2} \\
\end{align*} Since $\sum \frac{1}{n^2}$ is a convergent series, by the comparison test $\sum_{n=1}^{\infty} \mathbf{P}\left(\left|\frac{X(t_n)}{t_n}\right|>\epsilon\right)$ converges. We know that, if $(\forall \epsilon>0)$ , $\sum_{n=1}^{\infty} \mathbf{P}(|X_n - X| > \epsilon) < \infty$ , then $X_n \to X$ almost surely. Consequently, $\lim_{t_n \to \infty} \frac{X(t_n)}{t_n} = 0$ almost surely. Since, $(t_n)$ was an arbitrary sequence, $\lim_{t \to \infty} \frac{X(t)}{t} = 0$ almost surely.","['solution-verification', 'law-of-large-numbers', 'brownian-motion', 'probability-theory', 'stochastic-calculus']"
4712102,Expectation of the reciprocal of the Gaussian tail,"Let $N(\mu, \sigma^2)$ be the Gaussian distribution. For a real number $l\in\mathbb{R}$ , we further denote by $N(l; \mu,\sigma^2)$ the Gaussian tail distribution, namely, $N(l;\mu,\sigma^2)$ has the density $$
p(x) = \begin{cases}
0 & \text{if } x \leq l,\\
C e^{-\frac{(x-\mu)^2}{2\sigma^2}} & \text{otherwise}. 
\end{cases}
$$ Here, $C$ is a positive number which makes the integral equal to $1$ . Note that if $l>0$ and $X\sim N(l; \mu,\sigma^2)$ , then $\frac{1}{X}$ is well-defined. Suppose $X\sim N(l; \mu,\sigma^2)$ is distributed with respect to the Gaussian tail distribution where $l>0$ . What is $\mathbb{E}[\frac{1}{X}]$ ? I calculated $C$ to be $$
C = \frac{\sqrt{2}}{\mathrm{erfc}(\frac{l-\mu}{\sqrt{2}\sigma})\sqrt{\pi}\sigma},
$$ where $\mathrm{erfc}$ denotes the complementary error function. With this, I have $$
\mathbb{E}[\frac{1}{X}] = C \int_{l}^{\infty} \frac{1}{x} e^{-\frac{(x-\mu)^2}{2\sigma^2}}\mathrm{d} x. 
$$ In the case that $\mu=0$ , this integral can be expressed in terms of the incomplete $\Gamma$ function. If I am not mistaken, the integral equals $\Gamma(0;\frac{l^2}{2})/2$ in this case. However, I was not able to solve the integral for $\mu\neq 0$ . Edit: I want to use the closest integer to $\mathbb{E}[\frac{1}{X}]$ in a code. Hence, I am also happy with a good estimate or a fast algorithm that computes it (preferably, the algorithm does not actually compute the integral). Edit2 : I would also like to add the following diagrams, that explain the behaviour of $\mathbb{E}[\frac{1}{X}]$ in several regimes. In each picture, $\mu$ and $\sigma$ is given in the title, the $x$ -axis represents different values of $l$ and the $y$ -axis is the value of $\mathbb{E}[\frac{1}{X}]$ . The best one is the last one: Here we have $\mu=100$ and $\sigma=2$ . In this case, for small values of $l$ we always have $\mathbb{E}[\frac{1}{X}]\approx 0.01$ , which makes sense.","['probability-distributions', 'probability-theory', 'normal-distribution']"
4712104,Is the right-limit of left-derivatives equal to the right-derivative?,"Let's say $f$ is a convex function, so that $f_L'$ and $f_R'$ exist everywhere. I was wondering if the following is true: $$\lim_{t\to a^+}f_L'(t)=f_R'(a).$$ We know that $f'_L(t)$ decreases as $t\to a^+$ , so $$\lim_{t\to a^+}f'_L(t)=\inf_{t>a}f_L'(t).$$ On the other hand, convexity implies $$f_R'(a)\leq f_L'(t)\qquad\text{for all}\qquad t>a.$$ As such $$f_R'(a)\leq\lim_{t\to a^+}f_L'(t).$$ Should this also be an equality?","['convex-analysis', 'derivatives', 'real-analysis']"
4712155,Sum over 3 distinct indices,"I’m trying to calculate $$\sum_{1 \leq i < j < k \leq n} 1$$ This is because I think that this quantity is the number of alphabetically ordered words of 3 letters taken from n symbols. This is since we can pick 3 letters $ijk$ , but it’s required that $i < j < k$ . I’m not familiar with these kind of sums and the result I’m getting with $n=26$ is not the expected answer via: $$\sum_{i=1}^{n-2} \sum_{j=i+1}^{n-1} \sum_{k=k+1}^n 1$$","['summation', 'combinatorics']"
4712166,Show $\int_0^{\pi/2}\int_0^1 e^{t+t^{\tan\theta}}dtd\theta=\frac{\pi}{4}(e^2-1)$,"A friend gave me this double integral a while ago, and I couldn't figure out how to solve it. $$\int_0^{\pi/2} \int_0^1 e^{t+t^{\tan\theta}}\,dt d\theta=\dfrac{\pi}{4}\left(e^2-1\right)$$ I tried using Fubini's theorem, but I did not know what to do from there $$\int_0^1e^t\int_0^{\pi/2}  e^{t^{\tan\theta}}\,d\theta dt=\int_0^1 e^t\int_0^{\pi/2}\sum_{n=0}^\infty \frac{t^{n\tan\theta}}{n!}\,d\theta dt $$ $$\int_0^{\pi/2}a^{\tan\theta}\,d\theta=?$$ Wolfram Alpha gives a closed form in terms of exponential integral functions, but it seems challenging to solve the sum at the end involving exponential integral functions. Please help","['integration', 'multivariable-calculus', 'definite-integrals', 'fubini-tonelli-theorems']"
4712171,When is the sine angle addition formula actually useful?,"My opinion is that there is no need to remember the formula for sine angle addition, that is, that $\sin(a)\cos(b)+\sin(b)\cos(a) = \sin(a+b)$ . My reasoning is as follows: Case 1: If you have both angles $a$ and $b$ , simply calculate $\sin(a+b)$ directly. Case 2: Let's say you start off with, not the angles, but the actual measurements themselves, to which the sine and cosine of a will be $y_1$ and $x_1$ , and the sine and cosine of $b$ will be $y_2$ and $x_2$ . From here, we can calculate angles $a$ and $b$ via $\arctan(y_1/x_1)$ and $\arctan(y_2/x_2)$ , respectively, and then just do what we did previously in case 1. Additionally, if you have one angle, but you only have the $x/y$ measurements of the other angle, then this is really just case 2 again. Thus, the sine angle addition formula isn't necessary in order to advance ones knowledge of trigonometry, and can be safely done away with (if my reasoning is correct). With this, is it safe to assume that I can file the sine angle addition formula under the ""nice to know"" category? Or is there something right under my nose which I've blindly missed? Honestly, this could very well be the case, hence the reason for this post to begin with.","['trigonometry', 'angle', 'education']"
4712184,How to choose which stronger claim to prove when proving $\sum_{i=1}^n \frac{1}{i^2} \le 2$?,"I am studying an inductive proof of the inequality $\sum_{i=1}^n \frac{1}{i^2} \le 2$ . In the proof, it was decided to prove the stronger claim $\sum_{i=1}^n \frac{1}{i^2} \le 2-\frac{1}{n}$ , as this was easier to prove, and to use this to prove $\sum_{i=1}^n \frac{1}{i^2} \le 2$ . I am wondering what the thought processes would be behind choosing this stronger claim specifically -how can we know that this stronger claim will be correct and furnish a fruitful result before we go on to prove it? What makes us choose $2-\frac{1}{n}$ for the RHS of our stronger claim specifically, rather than any other expression involving $n$ ? Are there any constraints on what we can subtract from 2? I would like to know this to make sure that I can replicate the thought processes of this proof- I am not sure that I would have seen which stronger claim to prove after not getting anywhere with the weaker claim.","['summation', 'elementary-number-theory', 'proof-writing', 'discrete-mathematics', 'inequality']"
4712191,Show $\pi ≈ \frac{4\sum_{i=0}^\infty\left(\left\lfloor \frac{5^k}{4i+1}\right\rfloor-\left\lfloor\frac{5^k}{4i+3}\right\rfloor\right)-2(k+1)}{5^k}$,"Lemma For any non-negative integer $k$ , $x^2+y^2=5^k$ has $4(k+1)$ integer solutions. Pick's Theorem Suppose that a polygon has integer coordinates for all of its vertices. Let $i$ be the number of integer points interior to the polygon, and let $b$ be the number of integer points on its boundary (including both vertices and points along the sides). Then the area $A$ of this polygon is: $$A = i + \frac{b}{2} - 1$$ Gauss Circle Problem For a radius $r$ , the Gauss Circle Problem determines the number of lattice points inside a circle $x^2 + y^2 \leq r^2$ as $$N(r) = 1 + 4\sum_{i=0}^\infty\left(\left\lfloor \frac{r^2}{4i+1}\right\rfloor - \left\lfloor \frac{r^2}{4i+3}\right\rfloor\right)$$ Show $\pi \approx \frac{4\sum_{i=0}^\infty\left(\left\lfloor \frac{5^k}{4i+1}\right\rfloor - \left\lfloor \frac{5^k}{4i+3}\right\rfloor\right) - 2(k+1) }{5^k}$ Proof When the lattice polygon closely approximates the circle, their areas $A$ are nearly equal: $$\pi r^2 \approx i + \frac{b}{2} - 1$$ So $$\pi \approx \frac{i + \frac{b}{2} - 1}{r^2}$$ Let $b \approx 4(k+1)$ , $r=\sqrt{5^k}$ , $i \approx N(\sqrt{5^k}) - 4(k+1)$ \begin{align}
\pi &\approx \frac{i + \frac{b}{2} - 1}{r^2} \\
&= \frac{N(\sqrt{5^k}) - 4(k+1) + \frac{4(k+1)}{2} - 1}{5^k} \\
&= \frac{N(\sqrt{5^k}) - 2k - 3}{5^k} \\
&= \frac{4\sum_{i=0}^\infty\left(\left\lfloor \frac{5^k}{4i+1}\right\rfloor - \left\lfloor \frac{5^k}{4i+3}\right\rfloor\right) - 2(k+1) }{5^k}
\end{align} Question Is that a valid estimate for $\pi$ ? For $k=8$ and $N=250$ , Wolfram gives $\pi \approx 3.1395072$ . Notes When joining the $b=4(k+1)$ boundary lattice points to form the polygon, there are possibly other integer points in-between $$\text{gcd}(|x_2 - x_1|, |y_2 - y_1|) - 1$$ so it's a rough approximation of $b$ . For example, a circle $x^2+y^2=5^2$ with boundary points $(10,5),(5,10)$ has four other points in-between $(9, 6)$ , $(8, 7)$ , $(7, 8)$ , and $(6, 9)$ . Does there exist a formula to compute the exact number of boundary points? Update To hopefully improve the numerical issues for $k>9$ as noted in the answer, I've submitted this question Is there a formula to calculate the number of In-between Points for each adjacent $2D$ lattice polygon vertices? Currently, the $\pi$ estimate uses $b \approx 4(k+1)$ which isn't very accurate. For example, $k=14$ has $b\approx60$ points, however, the true number of boundary points is $b=117540$ . For an N-polygon, the precise total number of boundary points is $$b = \underbrace{4(k+1)}_{\text{Vertices}} + \underbrace{\sum_{i=0}^{N-1}\left(\gcd(|x_{(i+1)} - x_{i}|, |y_{(i+1)} - y_{i}|) - 1\right)}_{\text{Total Points In-between Vertices}}$$ So, perhaps a better $\pi$ estimate is $$\pi \approx \frac{i + \frac{b}{2} - 1}{r^2}$$ where $b = 4(k+1)+\sum_{i=0}^{N-1}\left(\gcd(|x_{(i+1)} - x_{i}|, |y_{(i+1)} - y_{i}|) - 1\right)$ $r=\sqrt{5^k}$ $i = N(\sqrt{5^k}) - b$","['pi', 'geometry', 'sequences-and-series']"
4712241,Evaluate $\int_{0}^{1} \frac{(1+5x^2)\arctan(x)^2}{x^{5/4}(1-x^2)^{1/4}} \text{d}x$,"This is an integral from a chat group: $$
\int_{0}^{1} \frac{(1+5x^2)\arctan(x)^2}{x^{5/4}(1-x^2)^{1/4}}
\text{d}x
=\frac{\sqrt{\sqrt{2}+1 } }{4}\left(
\left ( \sqrt{2 }+1 \right )\pi^2+2\left ( \sqrt{2}-1  \right )
(8-3\ln(2))\pi-\frac{4\sqrt{2}\,\Gamma\left ( \frac34 \right )^2 }{\sqrt{\pi} }
\left ( 2\ln(2)+\left ( 2\sqrt{2}  -1\right ) \pi\right )    \right ).
$$ What I already know is that(integrating by parts) $$
\int_{0}^{1} \frac{(1+5x^2)\arctan(x)^2}{x^{5/4}(1-x^2)^{1/4}}
\text{d}x=8\int_{0}^{1} \frac{(1-x^2)^{3/4}\arctan(x)}{x^{1/4}(1+x^2)}
\text{d}x,
$$ but I don't know how to proceed. So any help would be highly appreciated. ( June 5th,23 ) Expanding $\arctan(x)/(1+x^2)$ into power series, denoting $$
\mathscr{H}_n=\sum_{k=1}^n\frac1{2k-1}
$$ ,
we have $$
\int_{0}^{1} \frac{(1-x^2)^{3/4}\arctan(x)}{x^{1/4}(1+x^2)}
\text{d}x=\frac{3}{8}\Gamma\left ( \frac34 \right )
\sum_{n=1}^{\infty}\frac{(-1)^{n-1}\mathscr{H}_n\,\Gamma\left ( n-\frac18 \right ) }{
\Gamma\left ( n+\frac{13}8 \right ) }.
$$","['integration', 'definite-integrals', 'real-analysis', 'calculus', 'contour-integration']"
4712275,Ideal sheaves pullback via monoidal transform,"Let $X$ be a surface with two divisors $D,E$ . Suppose $D,E$ intersects at $P$ , denote by $Z$ the scheme-theoretic intersection. Let $\mathcal{I}$ be the ideal sheaf corresponding to $Z$ . Now we blow up $X$ at $P$ , $\pi: \tilde{X} \to X$ , with exceptional divisor $E$ . I am curious what $\pi^*\mathcal{I}$ would be. Since $Z$ is supported only at P, $\pi^*\mathcal{I}$ should have some relation with $\mathcal{O}_{\tilde{X}}(E)$ . My first guess is $\pi^*\mathcal{I} = \mathcal{O}_{\tilde{X}}(-E)^{\otimes \mu}$ , where $\mu$ is the local multiplicity of $Z$ at $P$ . But I feel like this may not be correct because the local equations of $D,E$ may not generated the power of maximal ideal $\mathscr{m}_P^{\mu}$ (Or say, $Z \neq \mu P$ as subschemes). So is there a clean expression of $\pi^*\mathcal{I}$ in this case, I mean write in terms of divisors on $\tilde{X}$ ? Thanks!",['algebraic-geometry']
4712298,Prove that $ \det(A+2B)=3\det(B)$ under the condition that $\det(A)$ $=\det(A+B)$ $=\det(A-B)$ $=0$,"Let $A, B\in M_{3\times 3}(R)$ such that $$\det(A)=\det(A+B)=\det(A-B)=0$$ Show that $ \det(A+2B)=3\det(B)$ Observe that $$\det(A+0B)=\det(A+B)=\det(A-B)=0$$ and hence the $3\!-\!\mathrm{degree}$ polynomial of $$\det(A+tB)$$ has roots $t=0,1,-1$ But unfortunately there is nothing more I can tell. Could anyone give me some hint please?","['matrices', 'determinant', 'linear-algebra', 'matrix-calculus']"
4712331,"If $p>3$ is a prime congruent to $3 \bmod 4$, how can I prove that are there only two non-abelian groups of order $4p$?","Take the convention that $D_{2n}$ is the dihedral group of order $2n$ , and $\text{Dic}_{n}$ is the dicyclic group of order $4n$ . I want to show that if $p$ is a prime congruent to $3 \bmod 4$ , $p>3$ , then there are only two non-abelian groups of order $4p$ (which therefore must be $D_{4p}$ and $\text{Dic}_{p}$ , since these are non-isomorphic and non-abelian. In fact, there will be a total of $4$ groups, since the only abelian ones can be $C_2 \times C_2 \times C_p$ , $C_4 \times C_p$ ). This reference confirms that for $p=7$ , $p=11$ , $p=19$ and $p=23$ , the smallest $5$ primes with this property, that this result holds. Note it doesn't hold for all primes. For $p=5$ , there are certainly more than $2$ non-abelian groups of order $20$ , for example. This page on groupprops confirms the result. May I have a hint as to how to start to prove this?","['group-theory', 'abstract-algebra', 'finite-groups']"
4712337,Why is OLS valid for the AR process?,"Is it valid to apply OLS to the AR process? I learn that there exist correlation between explanatory variables and error term in AR model. I'm wondering how can show that $$E(u_t|x_t)\ne0$$ ,where vector of explanatory variable $x_t=(1, y_1,y_2,...,y_{t-1})$ And if AR model can not satisfy the assumption of BLUE, why OLS still used to estimate coefficient of AR model? Statistical proof would be greatly appreciated, and if not, an intuition would be enough. Remind that I'm looking for insights, not all the right answers.","['time-series', 'statistics']"
4712395,Probability of seeing HTTH before THTH in coin flips,"I'm doing a past paper question and trying to use Doob's Optional Stopping to find the probability that for independent identical $(X_n)$ uniform on $\{0,1\}$ we see the pattern $a = (1,0,0,1)$ before the pattern $b = (0,1,0,1)$ , and so wonder whether this is correct, and/or an efficient method since applying Doob's twice doesn't necessarily seem like the quickest way. We consider stopping times $\tau_w = \text{inf}\{n \in \mathbb N \mid (X_{n-3},\dots,X_n) = w\}$ for $w \in \{a,b\}$ , and $\tau = \tau_a \wedge \tau_b$ . As usual we'll set up gamblers entering a casino, all bets double or nothing gambler $A^i$ entering at time $i$ starting with $1$ and betting on the first letter for $a$ , then second, etc., gambler $B^i$ starting with $1$ and betting on consecutive letters for $b$ . Then let $M_n = \sum_{i=1}^n A^i_n + B^i_n - 2n$ earnings of $A^i_n$ and $B^i_n$ , and $N_n = \sum_{i=1}^n A^i_n + 2 \cdot B^i_n - 3n$ earnings for $A^i_n$ and $2 \cdot B^i_n$ . Then it follows that these are martingales as a (infinite) sum of martingales starting at 0 ( $A^i_n - 1$ for instance is such a martingale, and is $0$ for $n < i$ ) since the game is fair. Then we have that $\tau$ has finite expectation since $$
\mathbb P(\tau > 4n) \leq \mathbb P\bigg((X_{4m-3},\dots,X_m) \not = a \ \forall m \leq n\bigg) \leq (15/16)^n
$$ and so since both $M_n$ and $N_n$ have increments bounded by $32 \cdot 4 \cdot 2$ , we apply Doob's twice: At $\tau_a$ , we have a gambler with 4 correct letters on $a$ , one with 1 correct letter on $a$ , and one with 2 correct letters on $b$ . At $\tau_b$ , we have a gambler with 4 correct letters on $b$ , one with 2 correct letters on $b$ , and one with 1 correct letter on $a$ . So applying Doob's to both $M_n^\tau$ and $N_n^\tau$ , writing $p = \mathbb P(\tau = \tau_a)$ , and splitting the first expectation based on whether $\tau = \tau_a$ or $\tau = \tau_b$ : $$
\begin{align*}
0 &= \mathbb E[M_0]= \mathbb E[M_\tau]\\
&= \mathbb E\left[\sum_{i=1}^\tau A^i_n + B^i_n\right] - 2* \cdot \mathbb E[\tau]\\
&= p \cdot (2^4 + 2^1 + 2^2) + q \cdot (2^1+2^4+2^2) - 2 \mathbb E[\tau]\\
&= 2 \cdot (11 - \mathbb E[\tau])
\end{align*}
$$ so that $\mathbb E[\tau] = 11$ , and then doing the same again $$
\begin{align*}
0 &= \mathbb E[N_0]= \mathbb E[N_\tau]\\
&= \mathbb E\left[\sum_{i=1}^\tau A^i_n + 2\cdot B^i_n\right] - 3* \cdot \mathbb E[\tau]\\
&= p \cdot (2^4 + 2^1 + 2\cdot 2^2) + q \cdot (2^1+2\cdot 2^4+2\cdot 2^2) - 3\cdot 11\\
&= 26p +42q - 33\\
&= 42-33-16p
\end{align*}
$$ so that $p = \dfrac{9}{16}$ . The denominator of $16$ makes me think maybe there's a much quicker combinatorial/Markov chain way of getting to the same answer? Any thoughts are appreciated!","['solution-verification', 'martingales', 'stopping-times', 'probability-theory', 'probability']"
4712469,Proof of the Wirtinger Presentation using Van Kampen Theorem,"I have some difficulties understanding a proof of the Wirtinger presentation using the Van Kampen theorem, found in John Stiwell's ""Classical Topology and Combinatorial Group Theory"". I perfectly understand the proof except for its very end (which is crucial) : ""The typical generator of $\pi_1(A \cap B)$ , a circuit round a trench (Figure 161) has the form $a_ia_j^{-1}a_{i+1}^{-1}a_j$ in $\pi_1(A)$ and 1 in $\pi_1(B)$ . Thus, the SVK theorem gives precisely the Wirtinger relations for $\pi_1(A \cup B)$ "". (I understand we use a deform retract argument in the last sentence). What does ""has the form ... in $\pi_1(A)$ "" mean? Using SVK, we compute $\pi_1(A) \star_{\pi_1(A \cap B)} \{1\}$ (since $\pi_1(B) = \{1\}$ ), but I don't understand how the said relations apply to the amalgamated free product. Thank you very much!
Respectfully, AF","['knot-theory', 'group-presentation', 'group-theory', 'free-product']"
4712477,(Frechet) Differentiability of Implicit function in Banach spaces,"I'm looking at the classical implicit function theorem in Banach spaces. So $X,Y,Z$ are Banach spaces and $F: U_{x_0}\times V_{y_0} \to Z$ continuous and continuously differentiable with respect to y. And the inverse linear operator of the Frechet partial derivative is bounded linear operator. Also $F(x_0,y_0)=O.$ Then locally there is unique implicit function $T$ which $F(x,Tx)=O.$ This $T$ is continuous. However if we assume additional smoothness  condition for $F,$ then $T$ also has it.
Ofcouse formal differentiation of $F(x,Tx) = O$ gets us that if $T$ is Frechet differentiable, then $T'x = - F'_y(x,Tx)^{-1} F'_{x}(x,Tx)$ possibly on smaller ball.
So If I assume additionally that $F$ is continuously differentiable, then im trying to prove that indeed $T$ is differentiable by trying to see that $||\omega(x,h)||:=||T(x+h)-Tx+F'_y(x,Tx)^{-1} F'_{x}(x,Tx)h|| = o(||h||)$ as $h \to O.$ And so trying to get to $||\omega(x,h)||\leqq c \varepsilon ||h||.$ But the bound I'm currently getting is something along those lines: ( $\overline{x}:=x+h, y:=Tx, \overline{y}:=T(x+h))$ \begin{align*}
||\omega(x,h)||&= ||-F'_y (x,y)^{-1}||_{L(Z,Y)}.||-F'_y(x,y)(\overline{y}-y)-F'_x(x,y)h|| \\ 
&\leq c_1 ||F(x,y)-F(x,\overline{y})+\nu(x,\overline{y}-y)+F(x,y)-F(\overline{x},y)+\mu(h,x)|| \\
&\leq c_1 ( \varepsilon ||\overline{y}-y|| + \varepsilon ||h|| + ||F(x,\overline{y})-F(x,y)+F(\overline{x},y)-F(x,y)||),
\end{align*} provided that $||h|| < \delta(\varepsilon), ||\overline{y}-\overline{y}|| < \delta(\varepsilon).$ Where $c_1:=||-F'_y (x,y)^{-1}||_{L(Z,Y)}.$ Now the last term in the norm I evaluate the same way with the partial Frechet derivatives \begin{align*}
||F(x,\overline{y})-F(x,y)+F(\overline{x},y)-F(x,y)|| \leqq ||F'_y(x,y)||. ||\overline{y} - y|| +\varepsilon ||\overline{y}-y|| +||F'_x(x,y)||.||h||+\varepsilon .||h||.
\end{align*} Denoting $c_2:=||F'_y(x,y)||_{L(Y,Z)}$ and $c_3:=||F'_x(x,y)||_{L(X,Z)}.$ This then reads $||F(x,\overline{y})-F(x,y)+F(\overline{x},y)-F(x,y)|| \leqq (c_2 + \varepsilon)||\overline{y}-y|| + (c_3 +\varepsilon)||h||$ and in substituing in the inequality for $||\omega(x,h)||$ we get $$
|| \omega(x,h)|| \leqq c_1 (2\varepsilon+c_2)||\overline{y}-y||+(2\varepsilon+c_3)||h||.
$$ Bound for $||\overline{y}-y||$ will get with this inequality and combined with reversed triangle inequality for the definition of omega : $$
| ||\overline{y}-y|| - ||-F'_y(x,y)^{-1}F'_x(x,y)||.||h|| | \leq ||\omega(x,h)||\leqq c_1 [(2\varepsilon+c_2)||\overline{y}-y||+(2\varepsilon+c_3)||h||].
$$ Therefore denoting $c_4:=||-F'_y(x,y)^{-1}F'_x(x,y)||=c_1.c_3$ we get $(1-c_1(2\varepsilon+c_2))||\overline{y}-y|| \leq (c_1 c_3 +c_1 (2\varepsilon +c_2))||h||.$ Thus getting the following absurd (useless)bound $$
||\omega(x,h)|| \leqq \left[ (2\varepsilon+c_3) + \frac{c_1(2\varepsilon+c_2)}{1-c_1(2\varepsilon+c_2)} [c_1 c_3 + c_1(2\varepsilon + c_2)] \right] ||h||.
$$ The problem with this bound is that it must be multiple of $\varepsilon$ and $||h||$ in order to get $o(||h||).$ Any suggestions on how to get a better bound will be much appreciated. For reference: I tried to escape the argument in Deimling's book, where implicit function theorem was proven (without the assertion for differentiability of the implicit function) and then inverse function theorem was proven, and differentiability of the inverse function was established quoite easy. After that Mr. Deimling proves differentiability of the implicit function using the inverse function theorem;
There he assumes $F is C^m$ and considers the following map: $G(x,y):=(x, F'_y(x_0,y_0)^{-1}F(x,y)).$ And claims that since $$
G'(x_0,y_0)(h,k) = (h, k+ F'_y(x_0,y_0)^{-1}F'_x(x_0,y_0)h),
$$ $G'(x_0,y_0)$ must be a homeomorphism (which i dont see how its automatically true) And then $G^{-1}(x,O)=(x,Tx)$ with $T$ exactly the implicit function we are discussing. And  now its clear how differentiability (and $C^m$ ) in inverse function theorem establishes it in the implicit function theorem. So additional question : Is it obvious that $G'(x_0,y_0) = (Id_X, Id_Y +F'_y(x_0,y_0)^{-1}F'_x(x_0,y_0))$ is homeomorphism? Update It is obvious... since the right coordinate is translation with linear bounded operator the inverse will be sign-conjugated one: $$G'(x_0,y_0)^{-1}=(Id_X, Id_Y - F'_y(x_0,y_0)^{-1}F'_x(x_0,y_0)).$$ Now it's clear how the rest follows.","['nonlinear-analysis', 'implicit-function-theorem', 'functional-analysis', 'analysis']"
4712487,Relations between Kolmogorov-Smirnov distance and Wasserstain distance,"Let's have $X,Y \in \mathbb{R}$ with probability measures $\mu, \nu$ , then the Kolmogorov-Smirnov distance is defined as follows $$ d_K(X,Y)=\underset{x \in \mathbb{R}}{sup}\{|F_X(x) - F_Y(x)|\} $$ where $F_X(x)$ is the comulative distribution function of X. If $X,Y$ have finite p-momentum then the p-Wasserstein distance is defined as follows $$d_{W_p}(X,Y) = \Biggr(\underset{\pi \in \mathcal{J}(\mu, \nu)}{inf}\int|x-y|^p d\pi(x,y)\Biggr)^{\frac{1}{p}}$$ where $\mathcal{J}(\mu, \nu)$ denote all joint distribution $\pi$ for $(X,Y)$ that have marginals $\mu$ e $\nu$ . In this paper ( arXiv link ) it tells if Y is a real-valued random variable with
Lebesgue density bounded above by C > 0, then for any real-valued random variable X $$ d_K(X,Y) \leq \sqrt{2Cd_{W_1}(X,Y)} $$ and in this paper ( arXiv link ) I've found that, when $Y \sim \mathcal{N}(0,1)$ , $C = 2$ , so $$ d_K(X,Y) \leq 2\cdot\sqrt{d_{W_1}(X,Y)} $$ My questions is: What is the value of $C$ when $Y$ is a centered non standard gaussian so $Y \sim \mathcal{N}(0,\sigma^2)$ ? (it should be the upper bound to the Lebesgue density of Y, but I don't know what the Lebesgue density is and how to compute it for the centered non standard gaussian distribution)","['measure-theory', 'wasserstein', 'probability-theory', 'probability']"
4712490,Stokes' theorem on a triangle,"I've been given a question that I'm having trouble figuring out:
Calculate \begin{equation}
\oint_{T} xydx + yzdy + zxdz
\end{equation} using Stokes' theorem, where $T$ is the triangle with vertices on $(3,0,0)$ , $(0,-1,0)$ and $(0,0,-2)$ , oriented counter-clockwise from $(0,0,0)$ . Verify your answer by calculating the line integral directly. I have calculated the integral with Stokes' theorem by doing the following:
The triangle $T$ lies on a plane \begin{equation}
\frac{1}{3}x -y - \frac{1}{2}z = 1
\end{equation} which gives a normal unit vector $\vec{N}$ of \begin{equation}
\vec{N} = \frac{2\vec{i} - 6\vec{j} - 3\vec{k}}{7}
\end{equation} F the curl or $\vec{F}$ we have: \begin{equation}
curl(\vec{F}) = -y\vec{i} -z\vec{j} -x\vec{k}
\end{equation} Now for the integral: \begin{equation}
\iint_{T}curl(\vec{F}) \cdot \vec{N}dS = \frac{1}{7}\iint_{T}-2y-6z-3xdS
\end{equation} Using the previous plane for triangle $T$ we can get $z = -2 +\frac{2}{3}x -2y$ , making the integral \begin{equation}
\frac{1}{7}\iint_{T}10y-7x+12dS
\end{equation} My thought was that for the integration limits we have $0 \leq x \leq 3$ and $0 \leq y \leq -\frac{1}{3}x$ , but I'm not sure about the limits for $y$ .
Filling all this in would give: \begin{equation}
\frac{1}{7} \int_{x = 0}^{3}\int_{y=0}^{-\frac{1}{3}x}10y-7x+12dydx = \frac{8}{7}
\end{equation} But now to verify this by calculating the line integral directly I don't know what to do, despite it sounding like it should be quite simple. For previous problems where I had to verify Stokes' theorem I could use a parametrization for the direct calculation, but I don't see how I can parameterize this triangle. I've thought about using Green's theorem, but that seems to work for integrals with a $dx$ and $dy$ part and not also a $dz$ part. I also can't find a single example in my book of a similar problem where they do this direct calculation","['integration', 'multivariable-calculus', 'line-integrals', 'stokes-theorem']"
4712492,Line integral equals zero because the vector field and the curve are perpendicular.,"Let $F: \Omega \subseteq \mathbb R^2 \to \mathbb R^2$ defined as $F(x,y) = (-y,x)$ and $\gamma:[0,2] \to \mathbb R^2$ defined as $\gamma(t) = (R(1-t), 0)$ for some $R>0$ . We have $$ 
\int_{\gamma}\vec F \cdot \vec {d\ell}= \int_{0}^{2}\langle \begin{bmatrix}
0 \\
R(1-t)
\end{bmatrix} 
,
\begin{bmatrix}
-R \\
0
\end{bmatrix} 
\rangle
dt = 0
$$ Supposedly this is because in this case $F$ and $\gamma$ are perpindicular but I'm having trouble seeing why. If I try to visualize the image of $F$ I think it sort of looks like a spiral rotating anti-clockwise (?), and the image of $\gamma$ simply as the segment $[-R, R]$ . How are they perpindicular?","['integration', 'multivariable-calculus', 'real-analysis']"
4712496,Find $\min \int_0^{x\arctan x}\frac{e^{t-\cos t}}{1+t^{2023}}\mathrm dt$,"Find the minimum value of : $$f(x) = \int_0^{x\arctan x}\frac{e^{t-\cos t}}{1+t^{2023}}\mathrm dt$$ Using FTC, we have ; $$f'(x) = \frac{e^{x\arctan x - \cos (x\arctan x)}}{1+x\arctan x}.(\arctan x + \frac{x}{x^2 + 1})$$ For $f$ to be minimum, $f' = 0\implies\arctan x = -\frac{x}{x^2 + 1}$ However, I can't proceed further and don't find it much useful. This question appeared today in my test (JEE) for which I had an average of 3.5 minutes to solve it. But in such short time, I am failing to find a short and elegant method.","['definite-integrals', 'real-analysis', 'maxima-minima', 'calculus', 'derivatives']"
4712503,Is a smooth simple closed curve the union of finitely many arcs?,"This is a problem from Pugh's Real Mathematical Analysis [Chapter 5]. If $C$ is a smooth simple closed curve in the plane, show that it is the union of finitely many arcs $C_l$ , each of which is the graph of a smooth function $y = h(x)$ or $x = h(y)$ , and the arcs $C_l$ meet only at common endpoints. Attempt : Let $C$ be parametrized by $t$ , that is, $(x(t),y(t))$ be its coordinates, where $t$ changes in $[0,1]$ . We can start by letting $t_0$ = $0$ and continue traversing the curve (either clockwise or counter-clockwise) until one of the coordinates of the derivate changes sign. Let's call this point $t_1$ . With continuing the same algorithm, we have partitioned $C$ into a (not necessarily finite) number of arcs with their domain $[t_i,t_{i+1}]$ . But here two things remain to be proved: First : How to revise the algorithm so that the sign-changing points are finite? Or how to show that they are finite, if it is the case? Second : It is easy to show that in each arc either $x$ is a function of $y$ or vice versa. The question is why one of them is an smooth function of the other. Thanks for your suggestions in advance. Note that I'm taking an Advanced Calculus course and I'm not familiar with manifolds or algebraic topology. So keep your answers as simple as possible.","['multivariable-calculus', 'real-analysis']"
4712532,How is the logarithm of an integer analogous to the degree of a polynomial?,"I've recently been reading Serge Lang's Math Talks for Undergraduates, specifically a section about the abc conjecture. Lang starts by stating and proving the Mason-Stothers Theorem: Let $f,g \in \mathbf{C}[t]$ be nonconstant and relatively prime. Then $
\text{deg}(f+g) \leq n_0[fg(f+g)]-1$ , where $n_0$ gives the number of distinct roots of a polynomial. Lang then translates the Mason-Stothers theorem into a theorem about the integers. In doing so, he states ""Experience shows that the analogue of the degree is the logarithm of the absolute value of the integer."" Why is this so? For an integer $a$ and a polynomial $f$ , how is $\log(|a|)$ analogous to $\text{deg}(f)$ ?","['euclidean-domain', 'number-theory', 'abc-conjecture', 'prime-numbers']"
4712559,PDF of product of power law distributed variables,"Consider $X_1 \sim f_1 = ax^{a-1}$ , $X_2 \sim f_2 = bx^{b-1}$ , both independent and defined over the positive domain.
I am interested in the PDF of the product $Z = X_1X_2$ . The know expression of the product distribution is $$
f_Z(z) = \int_{-\infty}^{\infty}f_1(x)f_2(z/x)\frac{1}{|x|} dx\\
=\int_{0}^{1}ax^{a-1} b(z/x)^{b-1}\frac{1}{|x|} dx\\
=ab z^{b-1}\int_{0}^{1}x^{a-b-1} dx\\
\propto z^{b-1}
$$ What I am worried about is the asymmetry: clearly $f_Z(z)$ cannot depend on $b$ alone. More specifically, because of commutativity of product one can also compute the distribution of $X_2X_1$ to be $\propto z^{a-1}$ which is a contraddiction. What is going wrong here?","['statistics', 'probability']"
4712568,Find limit using generalized binomial theorem.,"I'm trying to prove that $$
\lim_{(x,y)\rightarrow (0,0)}\left|\frac{(x+y)^\beta\sin\frac{1}{\sqrt{x^2+y^2}}}{\sqrt{x^2+y^2}}\right| = 0
$$ For every $\beta >1$ . So I'm trying to use that $$
\left|\frac{(x+y)^\beta\sin\frac{1}{\sqrt{x^2+y^2}}}{\sqrt{x^2+y^2}}\right| \leq \frac{(x+y)^\beta}{\sqrt{x^2+y^2}}
$$ And when I put the right limit in WolframAlpha with $\beta$ near to $1$ , always returns that the limit is $1$ so I guess it could be true for every $\beta >1$ . But I don't know how to prove it . I've tried to use some kind of generalization of the binomial theorem to generalize the argument given here But I haven't been able to do it yet. Any hints or counterexample?","['multivariable-calculus', 'real-analysis']"
4712593,"$V=\left \{ C|\space C\in M_{q\times r}(R)\space such \space that\space ACB=O \right \} $, find the dimension of the vector space V","Let $A$ be a $p\times q$ matrix of rank $\alpha$ and $B$ a $r\times s$ matrix of rank $\beta$ .  Let $V=\left \{ C|\space C\in M_{q\times r}(\mathbb{R}) \text{ and } ACB=0_{p\times s}  \right \} $ , find the dimension of the vector space V. Can I solve this by the following...... (1)the range space of B is the domain of C, where rank(B)= $\beta$ (2)the range space of C covers the whole null space of A, where nullity(A)=q-rank(A)=q- $\alpha$ Hence the dimension of C need not to be q $\times r$ , it only takes (q- $\alpha)\times \beta$ in necessary. please give some opinion please!","['matrix-rank', 'linear-algebra', 'vector-spaces']"
4712605,Find all idempotent matrices such that $(A-B)^2 = 0$,"Find all idempotent matrices such that $(A-B)^2 = 0$ We can see that the hypotheses imply that $A+B=AB+BA$ , and if we multiply by $AB$ on the right, we get $AB+BAB=(AB)^2+BAB$ , which also implies that $AB$ is idempotent (and we can also see that $BA$ is idempotent). After that, I am not sure what to do next. I add a necessary condition that $\operatorname{rank} (A) = \operatorname{rank} (B)$ , since $A-B$ is nilpotent, then $\operatorname{Trace} (A-B) = 0$ , so $\operatorname{Trace} (A) = \operatorname{Trace} (B)$ . But $A$ and $B$ are projectors, so $\operatorname{rank} (A) = \operatorname{rank} (B)$ . additional sufficient condition: if $\text{im}(A) = \text{im}(B)$ , then $AB=B$ and $BA=A$ . This implies that $A+B=AB+BA$ , which further implies that $(A-B)^2=0$ .","['matrices', 'matrix-equations', 'linear-algebra', 'idempotents']"
4712622,Question on a (presumably) basic $O_X$-module isomorphism,"I'm working my way through The Rising Sea: Foundations of Algebraic Geometry and want to double-check my work and thinking (and more than likely my notation) on the following question it poses: Exercise: If $\mathcal{F}$ is an $\mathcal{O}_X$ -module, then show that $Hom_{Mod_{\mathcal{O}_X}} (\mathcal{O}_X, \mathcal{F}) \cong \mathcal{F}$ . The text provides the hint that 1 ""generates"" (in a reasonable sense) $\mathcal{O}_X$ . My idea in broad strokes is to use the following: Use the fact that we know there is a canonical isomorphism between $\mathcal{F}_x$ and $Hom_{\mathcal{O}_{X,x}}\left(\mathcal{O}_{X,x},\mathcal{F}_x\right)$ by virtue of the fact that $\mathcal{F}_x$ is a $\mathcal{O}_{X,x}$ -module. A morphism $\Psi:\mathcal{G}\to \mathcal{H}$ of $\mathcal{O}_X$ modules is an isomorphism if and only if it induces an isomorphism of the stalks $\Psi_x:\mathcal{G}_x \to \mathcal{H}_x$ for all $x\in X$ . Since the goal is an isomorphism between $Hom_{Mod_{\mathcal{O}_X}} (\mathcal{O}_X, \mathcal{F})$ and $\mathcal{F}$ , such an isomorphism would require that $$
Hom_{Mod_{\mathcal{O}_X}} (\mathcal{O}_X, \mathcal{F})_x
\cong
Hom_{Mod_{\mathcal{O}_{X,x}}} (\mathcal{O}_{X,x}, \mathcal{F}_x)
$$ as $Mod_{\mathcal{O}_{X,x}}-$ modules.
The right-hand side clearly has the structure of an $\mathcal{O}_{X,x}$ module, and the right-hand side can be seen to have such structure as each $$
Hom_{Mod_{\mathcal{O}_X}} (\mathcal{O}_X, \mathcal{F})(U)
$$ has the structure of a $\mathcal{O}_{X}(U)-$ module, so taking direct limits gives the required structure. To see that $$
Hom_{Mod_{\mathcal{O}_X}} (\mathcal{O}_X, \mathcal{F})_x
\cong
Hom_{Mod_{\mathcal{O}_{X,x}}} (\mathcal{O}_{X,x}, \mathcal{F}_x)
$$ we consider an element $\varphi_x \in Hom_{Mod_{\mathcal{O}_X}} (\mathcal{O}_X, \mathcal{F})_x$ and $U$ an open set containing $x$ such that $$
\varphi \in Hom_{Mod_{\mathcal{O}_X}} (\mathcal{O}_X, \mathcal{F})(U) = Mor_{\mathcal{O}_U}\left(\mathcal{O}_U, \mathcal{F}|_U\right).
$$ Then $\varphi$ , being a morphism of $\mathcal{O}_U-$ modules, induces a morphism on the stalks, i.e an element of $Hom_{Mod_{\mathcal{O}_{U,x}}} (\mathcal{O}_{U,x}, \mathcal{F}_x)$ . Since for all $V\subset U$ we have $\mathcal{O}_U(V) = \mathcal{O}_X(V)$ , and $\mathcal{F}|_U(V)=\mathcal{F}(V)$ , it follows that the induces morphism on stalks is also an element of $Hom_{Mod_{\mathcal{O}_{X,x}}} (\mathcal{O}_{X,x}, \mathcal{F}_x)$ .
Since this takes maps to themselves, it commutes with all required structures to be a morphism of $\mathcal{O}_{X}(U)-$ modules. To see this mapping has to be injective, assume that $\varphi_x \in Hom_{Mod_{\mathcal{O}_{X}}} (\mathcal{O}_{X}, \mathcal{F})_x$ induces the trivial map in $Hom_{Mod_{\mathcal{O}_{X,x}}} (\mathcal{O}_{X,x}, \mathcal{F}_x)$ then it must be the case that $\varphi_x(1) = 0$ , where $1$ is multiplicative identity of the ring $\mathcal{O}_{X,x}$ and $0$ is the identity of the abelian group $\mathcal{F}_x$ . As such, there must be some $V\subset X$ such that $1\in \ker\varphi(V)$ .
But this means that $\varphi(V)$ is the trivial map, thus the map $\varphi_x$ is itself the trivial map.
Thus the only map that can induce the identity of $Hom_{Mod_{\mathcal{O}_{X,x}}} (\mathcal{O}_{X,x}, \mathcal{F}_x)$ is the identity of $Hom_{Mod_{\mathcal{O}_{X}}} (\mathcal{O}_{X}, \mathcal{F})_x$ , so our proposed mapping is injective. To see the desired mapping is surjective, let $\psi \in Hom_{Mod_{\mathcal{O}_{X,x}}} (\mathcal{O}_{X,x}, \mathcal{F}_x)$ .
Then we can uniquely identify $\psi$ with some element $f_x \in \mathcal{F}_x$ (picked as $\psi(1) = f_x \in \mathcal{F}_x$ ).
Thus there is some open set $V$ containing $x$ and some morphism $\tilde{\psi}\in Mor(\mathcal{O}_{V},\mathcal{F}|_V)$ such that $\tilde{\psi}(V)(1) = f \in \mathcal{F}(V)$ .
But since $$
res_{W,V}\circ \tilde{\psi}(V)(1) = \tilde{\psi}(W)(1) =  \tilde{\psi} \circ res_{W,V} (1)
$$ we see that $\tilde{\psi}_x \in Hom_{Mod_{\mathcal{O}_{V,x}}} (\mathcal{O}_{V,x}, \mathcal{F}_x) = Hom_{Mod_{\mathcal{O}_{X,x}}} (\mathcal{O}_{X,x}, \mathcal{F}_x)$ induces the $\psi$ , hence proving surjectivity.
Thus we indeed have $$
Hom_{Mod_{\mathcal{O}_X}} (\mathcal{O}_X, \mathcal{F})_x \cong Hom_{Mod_{\mathcal{O}_{X,x}}} (\mathcal{O}_{X,x}, \mathcal{F}_x)
$$ This indicates that the isomorphism between $Hom_{Mod_{\mathcal{O}_X}} (\mathcal{O}_X, \mathcal{F})$ and $\mathcal{F}$ ought to be of the form $$
\left(\Psi(U)\right)(\varphi) 
=
\left(\varphi(U)\right)(1)
$$ as taking direct limits on either side results in $$
\lim_{x\in U}
\left(\Psi(U)\right)(\varphi) 
=
\lim_{x\in U}
\left(\varphi(U)\right)(1)
$$ where the resulting lefthand side describes the proposed isomorphism $$
Hom_{Mod_{\mathcal{O}_X}} (\mathcal{O}_X, \mathcal{F})_x \cong Hom_{Mod_{\mathcal{O}_{X,x}}} (\mathcal{O}_{X,x}, \mathcal{F}_x)
$$ and the righthand side describes the canonical isomorphism $$
Hom_{Mod_{\mathcal{O}_{X,x}}} (\mathcal{O}_{X,x}, \mathcal{F}_x)
\cong 
\mathcal{F}_x.
$$ Thus the morphism $\Psi$ induces an isomorphism of stalks and thus must be an isomorphism of sheaves. My main question is: ""Is there a more succinct way of proving the result?"" as what I came up with feels very roundabout and vague at times.","['algebraic-geometry', 'abstract-algebra', 'sheaf-theory', 'modules']"
4712624,The successor of 0 in an ordered ring,"The Setup I am investigating properties of the following definition of ordered rings: Let $R$ be a ring. We say $R$ is an ordered ring under the total order relation $\leq$ if, for all $a,b,c \in R$ , $$
(1). a \leq b \text{ implies } a+c \leq b+c, \text{ and}
$$ $$(2). 0 \leq a \text{ and } 0\leq b \text{ implies } 0\leq ab.
$$ We can also say $R$ is strictly ordered by $\leq$ if the above conditions hold with the strict order $<$ replacing $\leq$ throughout. It's been noted on this post and other places that the two notions of strict/non-strict are separate. I found and proved the following theorem while looking at the order topology of $R$ : Let $(R,\leq)$ be a (nonzero) ordered ring with unity. If $0$ has an immediate successor $\epsilon \in R$ , then $\epsilon\leq1$ , and either $\epsilon^2 = 0$ or $\epsilon^2 = \epsilon$ . The proof is simple: we must have $\epsilon \leq 1$ because otherwise $1 \in (0,\epsilon)$ , a contradiction. But then $0 < \epsilon \leq 1$ implies $0 \leq \epsilon^2 \leq \epsilon$ upon multiplication by $\epsilon$ . The Question In the theorem, $\mathbb{Z}$ is a simple example of the latter case, because the "" $\epsilon$ "" is just $1$ . One can show that this is always so in a discrete strictly ordered ring. I also have an example of the former case: the dual integers $\mathbb{Z}[\epsilon] = \{a+b\epsilon : a,b\in\mathbb{Z}, \epsilon^2 = 0\}$ , together with the lexicographic order. Here $\epsilon$ is the immediate successor of zero. My question is this: Is there an ordered ring $R$ (necessarily non-strictly ordered) such that $0$ has an immediate successor $\epsilon \in R$ with $\epsilon^2 = \epsilon$ and $\epsilon \neq 1$ ? My intuition tells me that the answer is no, but I don't have any evidence to back it. Any insight is appreciated!","['ring-theory', 'order-theory', 'abstract-algebra']"
4712662,Alternative solution methods for $y''(x) = e^{y(x)}$,"Some time ago I came across the following odd looking ODE as an exercise in a textbook. I can't seem to find the source. $$
\begin{equation}
\frac{d^2y}{dx^2} = e^{y(x)} 
\end{equation}
$$ Here $y$ is some function of $x$ , I don't think any conditions on $y$ were stated. I've looked around for other stack posts on this but haven't found one. If this has been discussed before I'd be happy for someone to link me to another post. The only way I thought of to solve this was to introduce a bunch of substitutions and reduce the order of the ODE. Can anyone else think of another method to solve this? I guess we are dealing with a non-linear ODE so maybe slick solutions are not to be expected. My method is as follows. First let $u=e^{y(x)}$ $$
\begin{align}
u = e^{y(x)} \implies \frac{du}{dx} = e^y \frac{dy}{dx}\,, \quad \frac{d^2u}{dx^2}= e^y \frac{d^2y}{dx^2} + e^y \Big(\frac{dy}{dx}\Big)^2
\end{align}
$$ Rearranging we have $$
\begin{equation}
\frac{dy}{dx}= \frac{1}{u}\frac{du}{dx}\,, \quad \frac{d^2 y}{dx^2} = \frac{1}{u} \frac{d^2u}{dx^2}-\frac{1}{u^2}\Big(\frac{du}{dx}\Big)^2
\end{equation}
$$ Now looking at our original equation \begin{align}
\frac{d^2y}{dx^2} - e^{y(x)} =0 \\
\end{align} We rewrite our equation in terms of $u$ . \begin{align}
\frac{1}{u} \frac{d^2u}{dx^2}-\frac{1}{u^2}\Big(\frac{du}{dx}\Big)^2 - u &=0
\end{align} Now notice that the dependent varible $x$ does not appear in the equation above. We can introduce a new variable $\alpha$ and use the chain rule to reduce the order to get a first order ODE. let $\alpha = \frac{du}{dx}$ $$
\begin{align}
\frac{d\alpha}{dx} = \frac{d^2u}{dx^2} = \frac{d \alpha}{du} \frac{d u}{dx} = \alpha \frac{d\alpha}{du} 
\end{align}
$$ Skipping some algebra our main ODE becomes $$
\begin{equation}
\frac{\alpha}{u}\frac{d \alpha}{du} - \Big(\frac{\alpha}{u}\Big)^2 - u =0
\end{equation}
$$ Letting $k = \frac{\alpha}{u}$ and doing some more tedious algebra we have a nice simplification. \begin{equation}
k \frac{dk}{du} = 1
\end{equation} This is now easily separable we have \begin{align}
\int k \, dk &= \int du \\
\implies k &= \pm \sqrt{2u +c_1}
\end{align} Here $c_1$ is an arbitrary constant.
In what follows I'll just consider the positive branch. Now backtracking we have the following ODE $$
\begin{equation}
\alpha = \frac{du}{dx} = u \sqrt{2u + c_1}
\end{equation}
$$ Let $\phi = 2u+c_1$ $$
\begin{equation}
\frac{d\phi}{dx} = \sqrt{\phi}(\phi-c_1)
\end{equation}
$$ Introducing yet another substitution... $(\psi = \sqrt{\phi})$ \begin{equation}
2 \frac{d\psi}{dx} = \psi^2 - c_1
\end{equation} This is again separable We have \begin{align}
\int \frac{d \psi}{\psi^2 - c_1} &= \frac{1}{2} \int dx \\
 \implies- \frac{1}{\sqrt{c_1}}\tanh^{-1}\Big(\frac{\psi}{\sqrt{c_1}}\Big) &= \frac{x}{2} + c_2
\end{align} Leading to... \begin{equation}
\psi = - \sqrt{c_1} \tanh\big(\sqrt{c_1}\Big(\frac{x}{2}+c_2\Big)\big)
\end{equation} Back substituting using the definitions of $\psi, \phi$ and $u$ we arrive at \begin{align}
e^y &=  \frac{c_1}{2}\big(\tanh^2\big(\sqrt{c_1}\Big(\frac{x}{2}+c_2\Big)\big) -1 \big) \\
\implies y(x) &= \ln \Big[\frac{c_1}{2}\big(\tanh^2\big(\sqrt{c_1}\Big(\frac{x}{2}+c_2\Big)\big) -1 \big)\Big]
\end{align} There will also be a solution for the negative branch. I also find the method a bit laborious due to all of the new variables I need to define and the mundane algebra. I'm intrigued to know if there is a slicker way.",['ordinary-differential-equations']
4712687,proof : $(\forall z\in \mathbb{C}-\{2\} ) \ \ \overline{f(z)}=-f(z) \Leftrightarrow (z-1+\frac{1}{2}i)(\bar{z}-1-\frac{1}{2}i)-\frac{5}{4}=0 $,"$$f(z)=\frac{z+i}{z-2}\ \  ; \; \; z\in\mathbb{C}-\{2\}$$ proof : $(\forall z\in \mathbb{C}-\{2\} ) \ \ \overline{f(z)}=-f(z) \Leftrightarrow (z-1+\frac{1}{2}i)(\bar{z}-1-\frac{1}{2}i)-\frac{5}{4}=0 $ this is what I did, $$(z-1+\frac{1}{2}i)(\bar{z}-1-\frac{1}{2}i)-\frac{5}{4}=0$$ $$z(\bar{z}-1-\frac{1}{2}i)+(-1+\frac{1}{2}i)(\bar{z}-1-\frac{1}{2}i)-\frac{5}{4}=0$$ $$z\bar{z}-(1+\frac{1}{2}i)z+(-1+\frac{1}{2}i)\bar{z}+(1+\frac{1}{2}i)(1-\frac{1}{2}i)-\frac{5}{4}=0$$ $$z\bar{z}-(1+\frac{1}{2}i)z+(-1+\frac{1}{2}i)\bar{z}=0$$ but I am not sure for what I have to do next, I want to get this result : $$ (\bar{z}-i)(z-2)=-(z+i)(\bar{z}-2) $$ What I have to do next? if there is any other way tell me please.","['complex-analysis', 'geometry', 'complex-numbers']"
4712717,Dimension of a space of holomorphic functions,"I would like to solve the following: for $r$ a positive real number, determine the dimension of the following vector space over $\mathbb{C}$ in terms of $r$ : $$[\text{holomorphic } f:\mathbb{C}\rightarrow\mathbb{C}\text{ where } f<\infty].$$ My idea is to show that all such holomorphic functions are really nice in some way, like polynomials. I've tried playing around with the power series expansion and showing this directly, but I'm not making much progress. Is this the right idea? Any help is appreciated.",['complex-analysis']
4712749,"In a direct proof, do your chain of deductions have to involve the antecedent in any way in order for this to be considered a ""direct proof""?","If I am asked to give a direct proof of the implication ""if the sky is blue, 7 is prime"", would it be considered a valid direct proof to conclude 7 is prime through a chain of implications which have nothing to do with the sky being blue? (the idea being that any implication whose consequent is always true is true, regardless of whether the antecedent is true or false). I am asking because I was taught the definition of a direct proof is a proof which assumes the antecedent, and through a chain of implications concludes the consequent- however, I am not sure if the chain of implications have to be built off of the antecedent, or if they can have no relationship to the antecedent, in order for this proof to be considered a direct proof. So would my proof of the implication I gave constitute a valid direct proof?","['propositional-calculus', 'proof-writing', 'logic', 'definition', 'discrete-mathematics']"
4712752,How do I Derive a Mathematical Formula to calculate the number of eggs stacked on a crate?,"A crate of eggs is stacked with eggs such that eggs are stacked on eggs in the crate until it is fully occupied and no egg can sit on another egg comfortably Derive a mathematical formula to determine the total number of eggs stacked on the crate hence using your formula calculate how many eggs can be stacked on a crate  that can contain 6 eggs per row and 5 eggs per column as the base I was able to solve this in python programming by looking at the problem this way:
For a crate that can contain n eggs by x eggs,
The first layer(ground layer) will contain a total of n*x eggs
The second layer will contain (n-1) * (x-1) eggs
The third layer will contain (n-2) * (x-2) eggs
In that order until n=0 or x=0
Now taking the sum of the total number of eggs on the 1st, 2nd, 3rd,…nth layers, we get the total number of eggs in the stack Now how do I generate a mathematical formula for this problem? Here is my python code def eggCount(row,column):
    totalEggs = 0
    while row>0 and column>0:
        eggs_on_layer = row * column
        totalEggs += eggs_on_layer
        row -= 1
        column -= 1
    return totalEggs

print(eggCount(6,5)) image of eggs stacked on crate","['double-sequence', 'combinatorics', 'sequences-and-series']"
4712860,Path Signatures and Picard iterations,"Recently, I've started studying path signatures and, currently, I'm reading a standard reference, namely ""A Primer on the Signature Method in Machine Learning"" by Ilya Chevyrev and Andrey Kormilitzin. Now, at some point the authors try to show how path signatures naturally emerge in the theory of Controlled Differential Equations, and they do so via Picard iterations. So far, so good. The things that are bothering me are the following: The authors claim that a map $V: \mathbb{R}^e \to L(\mathbb{R}^d,\mathbb{R}^e)$ can be equivalently seen as a map $V: \mathbb{R}^d \to L(\mathbb{R}^e,\mathbb{R}^e)$ . This I am more or less willing to accept. Indeed, if we consider $x \in \mathbb{R}^d$ and $y \in \mathbb{R}^e$ , we observe that $$ y \in \mathbb{R}^e \mapsto V(y) \in \mathbb{R}^{e \times d} \implies V(y)x \in \mathbb{R}^e, $$ yields a map in $\mathcal{L}(\mathbb{R}^e, \mathbb{R}^e)$ by taking $y$ to $V(y)x$ for all $y\in \mathbb{R}^e$ . If we now consider this map to be parameterized by $x$ , we end up with a map $\mathbb{R}^d \to \mathcal{L}(\mathbb{R}^e, \mathbb{R}^e)$ associated to $V$ . This association can be made one-to-one, thus we may equivalently treat $V$ as a linear map $\mathbb{R}^d \to \mathcal{L}(\mathbb{R}^e, \mathbb{R}^e)$ . I think this is what the authors had in mind, but I'd appreciate more insight. Now, my main problem is the claim made in the beginning of page 10. They say: Here $X$ is a path $[a,b] \to \mathbb{R}^d$ and $Y$ is another path $[a,b] \to \mathbb{R}^e$ . How do signatures actually pop out of this iterated integral? Is it just by the linearity of $V$ and the integral? I want to see something of the form $$\int_{a<t_k<b} \int_{a<t_{k-1} < t_k} ... \int_{a < t_1 <t_2} dX_{t_1}^{i_1} ... dX_{t_{k-1}}^{i_{k-1}} dX_{t_k}^{i_k}$$ Could you help me figure out how to formally derive the iterated integral above from the quantity (1.34)?","['integration', 'stieltjes-integral', 'ordinary-differential-equations', 'reference-request', 'notation']"
4712879,Generator for $H^1(S^1)$ (Bott & Tu),"I'm reading Bott and Tu's book on differential forms in algebraic topology and they have the following observation made when computing the de Rham cohomology of $S^1$ . They first conclude that $$H^1(S^1)= \operatorname{coker}(\delta)=\mathbb{R}$$ where $\delta$ is the difference map sending $(\omega,\tau)$ to $(\tau-\omega,\tau-\omega)$ from $H^0(U) \oplus H^0(V)$ to $H^0(U \cap V)$ . It's worth noting that the cover $U \cup V$ is chosen so that the intersection will result in having a small  interval containing the north pole and a small interval containing the south pole. After this they find an explicit generator for $H^1(S^1)$ which is the reason for this post. They state that We now find an explicit representative for the generator of $H^1(S^1)$ . If $\alpha \in \Omega^0(U \cap V)$ is a closed $0$ -form which is not the image under $\delta$ of a closed form in $\Omega^0(U) \oplus \Omega^0(V)$ , then $d^*\alpha$ will represent a generator of $H^1(S^1)$ . As $\alpha$ we may take the function which is $1$ on the upper piece of $U \cap V$ and $0$ on the lower piece Now $\alpha$ is the image of $( - \rho_V \alpha, \rho_U \alpha)$ . Since $-d(\rho_V \alpha)$ and $d\rho_U \alpha$ agree on $U \cap V$ , they represent a global form on $S^1$ ;
this form is $d^*\alpha$ . It is a bump $1$ -form with support in $U \cap V$ . Then they have the following illustrations Now I would appreciate if someone could help me understand what these illustrations represent as I do not see how to tie them together with the reasoning they gave.","['differential-topology', 'homology-cohomology', 'differential-geometry']"
4712880,Does this combinatorial identity hold？ [duplicate],"This question already has answers here : How to prove: $\sum_{k=m+1}^{n} (-1)^{k} \binom{n}{k}\binom{k-1}{m}= (-1)^{m+1}$ (4 answers) Closed last year . I am trying to prove $$\sum\limits_{k=i+1}^m (-1)^{k-1+i}\binom{m}{k}\binom{k-1}{i}=1,$$ where $m\geq 1, 1\leq i\leq m-1$ . Actually this is what I induce when I'm trying to calculate what the tangent map of $\log：\text{PD}_n(\mathbb{R})\rightarrow \text{Sym}_n(\mathbb{R})$ is， where $\text{PD}_n(\mathbb{R})$ is the positive-definite matrix, $\text{Sym}_n(\mathbb{R})$ is the symmetric matrix, and $\log$ is the inverse map of $\exp$ . If this identity holds, I can directly write out the formula of tangent map. But I stuck at this step.","['summation', 'combinatorial-proofs', 'binomial-coefficients', 'combinatorics', 'differential-topology']"
4712882,Local minima and local maxima of a univariate polynomial,"Let $$f(x) = a_n x^n + a_{n-1} x^{n-1} + \ldots + a_1 x + a_0$$ be a real polynomial of degree $n > 0$ . Using the derivative test, the values of $x$ for which the function $f(x)$ attains local minima and local maxima can be determined. However, considering this does not immediately give the values of $f(x)$ at those local minima and local maxima, I was wondering the following: Is it possible for $f(x)$ to attain a higher value at a local minimum than at a local maximum? If so, what is the lowest degree in which this can happen? For the second question, I was thinking about degree $5$ . Such a polynomial can have two local maxima and two local minima, since its derivative has degree $4$ . Order the extrema in increasing value of $x$ at which it is attained. First suppose $a_n > 0$ . I think it could have the first local maximum be lower than the last local minimum if the increase from the first local minimum to the second local maximum is more than the decreases from the local maxima to the next local minima combined (vice versa if $a_n < 0$ ). However, I could not find an example, so I am not sure if the first question is even true.","['real-analysis', 'maxima-minima', 'calculus', 'polynomials', 'derivatives']"
4712890,Help me verify whether my solution of this geometry problem is correct,"Consider the following problem. The Problem Point $B$ lies on the segment $AC$ . A tangent line is constructed from point $A$ to the circle of diameter $BC$ , intersecting it at point $M$ . $K$ is the second point of intersection of line $AM$ with the circle of diameter $AB$ . The extension of segment $MB$ intersects the circle of diameter $AB$ at point $D$ . a) Prove $AD\parallel MC$ . b) Find the area of $DBC$ if $AK=5$ and $MK=25$ . My Drawing My Solution a) $\angle ADB = \angle BMC = 90^\circ$ as inscribed angles subtended by the diameter are always right. As $\angle ADB$ and $\angle BMC$ are alternate interior angles of a transversal intersecting two lines $AD$ and $MC$ , and as they are equal, then, by Proposition 27 of Euclid (one of criterion for parallel lines) these lines are parallel, i.e. $AD\parallel MC$ , QED. b) Because $AD\parallel MC$ , $DAMC$ is a trapezoid with bases $AD$ and $MC$ . As per properties of a trapezoid, its diagonals divide its interior region into four triangles: two triangles that include one of this trapezoid's bases are similar, and two other triangles have the same area. Thus, to find the area of triangle $_\Delta DBC$ , it's sufficient to find the area of a triangle $_\Delta ABM$ , as their areas are equal. In $_\Delta ABM$ , we can immediately find side $AM=5+25=30$ . We now find the altitude to that side in order to find out the area of this triangle. This altitude is $BK$ , as $\angle AKB = 90^\circ$ as an inscribed angle subtended by the diameter. Let $\angle AMB = \alpha$ . This is the angle between tangent $AM$ and chord $MB$ , thus, by the alternate segment theorem, it is equal to the angle in the alternate segment $BM$ . Thus, $\angle BCM = \angle AMB = \alpha$ . Then $\angle CBM=90^\circ - \alpha$ as triangle $CMB$ is a right triangle (established in a) earlier), and sum of acute angles in a right triangle is always $90^\circ$ . Now, $\angle KMB=\angle ABM$ because point $K$ lies on a line $AM$ between points $A$ and $M$ . Therefore, $\angle KBM=\angle CBM=\alpha$ . Because triangle $MKB$ is also a right triangle, then $\angle MBK=90^\circ-\alpha=\angle CBM$ . Thus, by axiom of angle measurement, $\angle CBK=\angle MBK+\angle CBM=90^\circ-\alpha+90^\circ-\alpha=180^\circ-2\alpha$ . Now, as $\angle ABK$ is adjacent to $\angle CBK$ , it's true that $\angle ABK+\angle CBK=180^\circ$ . Therefore, $\angle ABK=2\alpha$ . As triangle $AKB$ is a right triangle as $BK\perp AK$ , it's true that $\angle BAK=90^\circ-2\alpha$ . Let's now express $\tan\alpha$ in terms of some sides. By definition of a tangent of an acute angle of a right triangle, it's the ratio of the opposite leg to the adjacent leg. Therefore, from triangle $MKB$ , $\tan\alpha=\dfrac{BK}{MK}=\dfrac{BK}{25}$ . By one of the phase shift identities, $\tan(90^\circ - 2\alpha)=\cot 2\alpha$ . By the double-angle identity for cotangent, $\cot 2\alpha=\dfrac{1-\tan^2\alpha}{2\tan\alpha}$ . From the right triangle $ABK$ and the definition of tangent, we also have $\tan(90^\circ - 2\alpha)=\tan\angle BAK=\dfrac{BK}{AK}=\dfrac{BK}{5}$ . Therefore, it's true that $\cot 2\alpha=\dfrac{BK}{5}$ . Let $\tan\alpha = x$ , and $BK = y$ . We now solve the system of equations from what we have got earlier: $$
\begin{cases}
x=\dfrac{y}{25}, \\
\dfrac{1-x^2}{2x} = \dfrac{y}{5};
\end{cases} \Rightarrow \begin{cases}
y=25x, \\
y=5\cdot\dfrac{1-x^2}{2x};
\end{cases} \Rightarrow 25x = 5\cdot\dfrac{1-x^2}{2x}.
$$ We now solve the equation for $x$ : \begin{align*}
25x &= 5\cdot\dfrac{1-x^2}{2x} \\
5x &= \dfrac{1-x^2}{2x} \\
10x^2 &= 1-x^2 \\
11x^2 &= 1 \\
x^2 &= \dfrac{1}{11}
\end{align*} That means $\tan^2\alpha=\dfrac{1}{11}$ . We know $\alpha$ is an acute angle, so $\tan\alpha > 0$ . Then, we will take the positive root: $\tan\alpha=\dfrac{1}{\sqrt{11}}$ . Now we can find $BK$ . \begin{align*}
\tan\alpha &= \dfrac{BK}{25} \\
\dfrac{BK}{25} &= \dfrac{1}{\sqrt{11}} \\
BK &= \dfrac{25}{\sqrt{11}}
\end{align*} We can now find the area of a triangle $_\Delta ABM$ : $$
S_{_\Delta ABM} = \dfrac{1}{2}\cdot AM\cdot BK = \dfrac{1}{2}\cdot 30\cdot \dfrac{25}{\sqrt{11}} = \dfrac{375}{\sqrt{11}}
$$ Therefore, as $S_{_\Delta ABM}=S_{_\Delta DBC}$ , we conclude that $S_{_\Delta DBC}=\dfrac{375}{\sqrt{11}}$ . My Question Is my solution correct, or does it have a flaw? The other person got an answer $\dfrac{75\sqrt{11}}{2}$ . Which answer is correct, and why?","['euclidean-geometry', 'solution-verification', 'geometry']"
4712922,How many distinct simply connected shapes can be represented on an n by n binary image?,"This is a problem which I found to be quite challenging. I ask this because I want to be able to losslessly compress simply connected shapes (connected shapes without holes) with fewer bits than $n^2$ , which can have many downstream applications. By connectedness, I do not restrict it to be either 4-connectedness or 8-connectedness, I just would like an answer that gets us to the approximately correct order of magnitude of the number of connected shapes, so an answer may be given for either definition of connectedness, if one of the definitions proves to be easier to work with. I consider two connected shapes to be the same if and only if the two n by n images on which the shapes are represented are exactly the same pixel-wise. Hence, translation and rotation of a shape will likely result in a new shape under my definition. Finally, if an exact, or approximate solution is difficult to come by, I am willing to accept a good upper bound estimate of the form $O(2^{c\times n^k})$ where k should ideally be less than 2, and c is some constant. (If it turns out that the minimum of k must be 2, it is fine, too, if there is a proof.)","['statistics', 'connectedness', 'compression', 'information-theory', 'combinatorics']"
4712941,"evaluate $\iiint\limits_{B}^{} \cosh \left ( x+y+z \right ) dxdydz$, where $B=\left \{ \left ( x,y,z \right ) \in R^3|x^2+y^2+z^2\le 1 \right \} $","Evaluate $\iiint\limits_{B}^{} \cosh \left ( x+y+z \right ) dxdydz$ , where $B=\left \{ \left ( x,y,z \right ) \in R^3|x^2+y^2+z^2\le 1 \right \} $ Since B is a unit ball which centers at the origin,
I think it should be highly usefull to change coordinates to spherical coordinates. And of course, by definition, $$\cosh\left (  x+y+z\right )  = \frac{e^{x+y+z}+e^{-x-y-z}}{2}$$ But after changing coordinates, I still know little about how to integral such thing like...... $$\int_{0}^{2\pi} \int_{0}^{\pi} \int_{0}^{1} \cosh \left ( r\sin \phi \cos \theta+r\sin \phi \sin \theta+r\cos \phi \right )r^2\sin \phi drd\phi d\theta $$ or $$\int_{0}^{2\pi} \int_{0}^{\pi} \int_{0}^{1} \frac{e^{\left ( r\sin \phi \cos \theta+r\sin \phi \sin \theta+r\cos \phi \right )}+e^{\left ( -r\sin \phi \cos \theta-r\sin \phi \sin \theta-r\cos \phi \right )}}{2} r^2\sin \phi drd\phi d\theta $$ Could anyone give some hints how I can do it please?","['multivariable-calculus', 'multiple-integral', 'definite-integrals', 'spherical-coordinates']"
4713021,Average value over an interval vs. between two points,"For a continuous distribution function F, for some $s>0$ , how can I show the following inequality: $$\large\sup_{0\le r\le s/2}\int_r^{s-r}\frac{F(x)}{s-2r}\text{ d}x \leq \sup_{r\in\mathbb{R}}\frac{F(r)+F(s-r)}{2}\quad$$ In words, the inequality would show that the average value of $F(x)$ over $[r,s-r]$ , $r\in[0,\frac{s}{2})$ can always be exceeded by the average value between two points $F(a)$ and $F(s-a)$ , for some $a\in\mathbb{R}$ . To be clear, I assume $F:\Bbb R\to[0,1]$ is continuous and nondecreasing with $\lim_{x\to-\infty}F(x)=0$ and $\lim_{x\to\infty}F(x)=1$ .","['calculus', 'probability-theory', 'real-analysis']"
4713029,How big can a wedge of 2-forms be?,"The comass of a 2-form $\alpha$ is the maximal value of $\alpha(u,v)$ for a pair of unit vectors $u,v$ .  The symplectic form $\alpha$ on $\mathbb R^{2n}$ has the property that $|\alpha^{\wedge n}| = n!$ .
For example, in $\mathbb R^6$ one has $|\alpha^{\wedge 3}| = 6$ .  Are
there 2-forms $a,b,c$ of unit comass on $\mathbb R^6$ such that $|a\wedge b\wedge c|>6$ ? Note.  A solution is provided at https://mathoverflow.net/a/448681/28128","['symplectic-geometry', 'exterior-algebra', 'differential-geometry']"
4713054,Why does ${\lim_{x\to2}}\;(x-4)^x$ not exist?,"Here's a question from the calculus section of AEEE 1997: $${\lim_{x\to2}}\;(x-4)^x$$ I saw this question in a very old paper of the AEEE exam ( $90's$ edition of the JEE exam). So when I saw this question, I obviously, wrote $4$ as the answer. Now the answerkey said Limit does not exist. I believed that maybe it has something to do with left hand and right hand limit and I tried referring some books on limits and understood how RHL and LHL should exist to say limit should exist but all those books have demonstrated this concept with problems involving greatest integer function or cases where limit tends to 0 which is pretty simple and understandable. But I cant seem to apply that concept here as x approaches 2 from Right hand and Left hand side which is all positive. So could anybody explain me the concept behind this. (And I would appreciate graphical representations!!! although not necessary) Also not I'm a high school student and starting calculus so please try to make the answer in simple language. EDIT 1 : After reading some links from mathematics SE, I understood a possible reason for this could be ' If exponentiation $x^y$ of real numbers $x$ and $y$ is defined as $e^{y \log x}$ , then one cannot define things like $(-2)^4$ ' So that creates whole new level of confusion now for me. In a cubic equation say $x^3+....$ , if one of the roots of the solution is -2, then...how can $x=-2$ if $(-2)^3$ isn't possible using the above logic? EDIT 2 (Very important) For anyone answering, please also illustrate if or why $(x+4)^x$ should exist.","['graphing-functions', 'limits-without-lhopital', 'real-analysis', 'calculus', 'limits']"
4713115,Are harmonic coordinates legit coordinates?,"Let $M$ be a smooth manifold. Let $g$ be a Riemannian metric of regularity $C^k(M)$ ; that is, in any coordinates, $g$ 's components' coordinate representations are $C^k$ functions in that chart. When one speaks of ""coordinates"" on a smooth manifold $M$ , they are implicitly assumed to belong to the maximal smooth atlas $\mathscr{A}$ implicit in the definition of $M$ . Harmonic ""coordinates"" can be found using standard elliptic theory: starting from a chart $(U, x)$ on $M$ in which $g$ has $C^k$ components, one obtains $y : U \to \mathbb{R}^n$ solving $\Delta_g y^j = 0$ for $j = 1, \ldots, n$ ( $n = \text{dim} M$ ). However, the resulting theory gives that the coordinates are $C^{k + 1}$ functions of $x$ , and therefore are not guaranteed to be smooth local coordinates - hence not belong to the maximal atlas $\mathscr{A}$ ! So in what sense are these coordinates? Does one have to relax the smooth structure of the manifold $M$ ?","['elliptic-equations', 'differential-geometry']"
4713166,What's the probability that the only coin left is fair given that exactly $n$ tosses were required to obtain heads with each of the first two coins?,"In the box there are $3$ coins. The probabilities of getting heads while tossing are equal to: $\frac{1}{2}$ on the first one, $\frac{1}{3}$ on the second one, $\frac{1}{4}$ on the third one. A man chose randomly $1$ of those $3$ coins and tossed it until getting heads. Then chose randomly another one and similarly - tossed it until getting heads. What's the probability that in the box the only coin that is left is the symmetric one if we know that the man tossed exactly $n$ times? I thought about counting the possibilities for $3 \cdot 2$ pairs of chosen coins in both orders and then the pair that we need in both orders. The first of two expressions would be put in the denominator and the other in the numerator. However, when I started with that approach I got to a point where calculating that all got too complicated. That's why I got to think that there must be more clever way to approach that problem. Any help would be much appreciated. Edit: Using the hint given by @lulu in the comments, I got something like this: $$\mathbb{P} \left( \text{coins were: } \ \frac{1}{3} \text{ and } \frac{1}{4} \ \Big| \ \text{there were n toses} \right) =$$ $$ = \frac{\mathbb{P}(\text{there were n toses} \ \Big| \ \text{2 coins were:} \ \frac{1}{3} \text{ and } \frac{1}{4}) \cdot \mathbb{P}(\text{coins were: } \ \frac{1}{3} \text{ and } \frac{1}{4})}{\mathbb{P}(\text{there were n toses})}$$ $\mathbb{P}(\text{there were n toses}) = $ $\mathbb{P} \left( \text{there were n toses} \ \Big| \ \text{2 coins chosen were those of prob. } \ \frac{1}{2} \text{ and } \frac{1}{3} \right) +$ $\mathbb{P} \left( \text{there were n toses} \ \Big| \ \text{2 coins chosen were those of prob. } \ \frac{1}{3} \text{ and } \frac{1}{4} \right) +$ $\mathbb{P} \left( \text{there were n toses} \ \Big| \ \text{2 coins chosen were those of prob. } \ \frac{1}{2} \text{ and } \frac{1}{4} \right)$ We get: ( $k$ is the number of failed tosses on one coin and $n-k-2$ are failed tosses on another) $1. = \displaystyle \sum_{k=0}^{n-1} \left( \frac{1}{2} \right)^k \frac{1}{2} \left( \frac{2}{3} \right)^{n-k-2} \frac{1}{3}$ $2. = \displaystyle \sum_{k=0}^{n-1} \left( \frac{2}{3} \right)^k \frac{1}{3} \left( \frac{3}{4} \right)^{n-k-2} \frac{1}{4}$ $3. = \displaystyle \sum_{k=0}^{n-1} \left( \frac{1}{2} \right)^k \frac{1}{2} \left( \frac{3}{4} \right)^{n-k-2} \frac{1}{4}$ It looks all good, but now, how do I express $1, 2, 3$ in terms of $n$ , losing $k$ in the process?","['conditional-probability', 'probability']"
4713170,Is the localization at a prime ideal of any polynomial ring always a valuation ring?,"I observed that all examples of localizations of polynomial rings at prime ideals I've encountered have been, so far, valuation rings, and so I started wondering whether this is true in all cases. I think the following might be an argument. We have the field of fractions of the localized polynomial ring: $$\text{Frac}(R[x_1,\dots,x_n]_{\mathfrak p}) = \{ \frac{f_1s_2}{f_2s_1}: f_1,f_2,s_1,s_2 \in R[x_1,\dots,x_n]; s_1, s_2 \notin \mathfrak p \}$$ Consider an arbitrary element of $R(x_1, \dots, x_n)$ . Then the numerator is either in $\mathfrak p$ or not. If it is, then $\text{num} = f \cdot 1$ . If it isn't, then $\text{num} = 1 \cdot s$ . The same argument can be done for denominator, and therefore we have $\text{Frac}(R[x_1,\dots,x_n]_{\mathfrak p}) = R(x_1 \dots, x_n)$ , from which it clearly follows $f \in R(x_1, \dots, x_n)$ implies $f^{-1}$ or $f \in R[x_1,\dots,x_n]_{\mathfrak p}$ , since either the denominator is in $\mathfrak p$ or it is not.","['localization', 'algebraic-geometry', 'ring-theory', 'abstract-algebra', 'commutative-algebra']"
4713176,The divergence operator as an element of $T_pM$ (if it does make any sense),"First of all, I'd like to mention ""in my defense"" that I'm actually a graduate student in physics, but I'm strictly related to math in terms of research area, then naturally, I realized that (in most cases) physicists get to learn some math concepts in a very superficial way. Now let's get to what interests here. I had some contact with the idea of a tangent space $T_pM$ on a point over one n-dim manifold $M$ , I also saw that in terms of a set of local coordinates $x^i$ , we represent an element ${\bf v}\in T_pM$ as $\sum_{i=1}^{n}v_i\frac{\partial}{\partial x^i}$ , meaning that each symbol $\frac{\partial}{\partial x^i} \hspace{0,2cm} (i=1,2,\ldots, n)$ would represent something similar to those L.I. canonical vectors $e^1 = (1,0,\ldots,0), \ldots, e^n = (0, \ldots, 0, 1)$ in $\mathbb{R}^n$ . In this last context, we'd say that an inner product between two vectors $v = (\alpha_1,\ldots,\alpha_n) = \sum_{i=1}^{n}\alpha_i e^{i}$ and $w = (\beta_1,\ldots,\beta_n) = \sum_{i=1}^{n}\beta_i e^{i}$ is numericaly correspondent to $\sum_{i=1}^{n}\alpha_i\beta_i$ , not difficult to verify if we consider $\langle e^i, e^j\rangle = \delta_{ij}$ . Now, by transposing this same ideia to a space where the vectors are represented as ${\bf v}$ above, could I say that $\nabla = \left(\frac{\partial}{\partial x^{1}},\ldots,\frac{\partial}{\partial x^{n}}\right)$ is analogous to something like $(1,\ldots, 1)$ ? I just suspect I can't exactly consider it, the reason is that this wondering came up after one specific paragraph of a paper which says (given a first order polynomial differential system): ""[...] where $\mathcal{X}$ is the vector field associated to the system (1), i.e. $$\mathcal{X} = P_1(x)\frac{\partial}{\partial x_1} + \ldots + P_n(x)\frac{\partial}{\partial x_n}.$$ As usually the divergence of the vector field is defined by $$\textrm{div}\mathcal{X} = \sum_{i=1}^{n}\frac{\partial P_i}{\partial x_i}.""$$ (From Liouvillian Integrability versus Darboux Polynomials - J. Llibre, C. Valls, X. Zhang) Well, considering those observatios I mentioned, wouldn't $\langle \nabla, \mathcal{X}\rangle = \sum_{i=1}^{n} P_i$ ? There might be something I'm not taking into consideration, or maybe I'm trying to see some relation between unrelated things. PS.: I vaguely remember hearing about $\nabla$ not being exactly a vector, but I never got it confirmed by myself. I also apologize for any breaches of mathematical rigor.","['divergence-operator', 'ordinary-differential-equations', 'vector-spaces', 'vector-analysis', 'differential-geometry']"
4713208,Integration by Parts: Product of Many Functions [duplicate],"This question already has answers here : Generalising integration by parts for the product of more than two functions (2 answers) Closed last year . ""Classic"" integration by parts has two functions ( $u$ and $dv$ ; $\int udv = uv - \int vdu$ ). What if there is a product of $n$ functions? In other words, what's the solution to the following? \begin{equation}\label{eq:tosolve}
    F(x)=\int_{a}^{b}\prod_{i=1}^{n}u_{i}(x)\:dx
\end{equation} Wikipedia's integration by parts article mentions this problem and offers an equation, but I think a few extra steps are necessary for a full solution. Wikipedia leaves us with the following product rule for $n$ functions: \begin{equation}
\bigg(\prod_{i=1}^{n}u_{i}(x)\bigg)'=\sum_{j=1}^{n}u_{j}'(x)\prod_{i\neq j}^{n}u_{i}(x).
\end{equation} Integrating, this leads to \begin{equation}\label{eq:onwiki}
    \bigg[\prod_{i=1}^{n}u_{i}(x)\bigg]_{a}^{b}=\sum_{j=1}^{n}\int_{a}^{b}u_{j}'(x)\prod_{i\neq j}^{n}u_{i}(x)\:dx
\end{equation} The object we want to solve for (the RHS of the first equation at the top) does not appear in the expression immediately above. How does one recover it?","['integration', 'multivariable-calculus', 'calculus', 'definite-integrals']"
4713219,"Can someone explain how to integrate it? $\int\frac{(x^2+1)dx}{x\sqrt{x^4+3x^3-2x^2-3x+1}},x>1$","Can someone explain how to integrate it? $$\int\frac{(x^2+1)dx}{x\sqrt{x^4+3x^3-2x^2-3x+1}},x>1$$ I know that I should use a substitution but I can't figure out. I should make some transformations under the root.",['integration']
4713224,"How can we show that $\left(X^{(1)}_t\right)_{t\in[0,\:\tau_1)}$ and $\left(X^{(2)}_t\right)_{t\in[0,\:\tau_2)}$ are independent here?","Let $(\Omega_i,\mathcal A_i)$ be a measurable space; $(E_i,\mathcal E_i)$ be a measurable space with $E_1\cap E_2=\emptyset$ and $(E,\mathcal E):=(E_1\cup E_2,E_1\vee E_2)$ ; $\Delta_i\not\in E_i$ and $(E_i^\ast,\mathcal E_i^\ast)$ denote the one-point extension of $(E_i,\mathcal E_i)$ by $\Delta_i$ $\left(X^{(i)}_t\right)_{t\ge0}$ be an $(E_i^\ast,\mathcal E_i^\ast)$ -valued process on $(\Omega_i,\mathcal A_i)$ and $$\tau_i:=\inf\left\{t\ge0:X^{(i)}_t=\Delta_i\right\};$$ $T_1$ be a Markov kernel with source $(\Omega_1,\mathcal A_1)$ and target $(E_1,\mathcal E_2)$ $p_i$ denote the projection from $\Omega:=\Omega_1\times\Omega_2$ onto the $i$ th coordinate and $$\tilde X^{(i)}:=X^{(i)}\circ p_i$$ $\operatorname P^{(i)}_{x_i}$ be a probability measure on $(\Omega_i,\mathcal A_i)$ with $$\operatorname P_{x_i}^{(i)}\left[X^{(i)}_0=x_i\right]=1$$ for $x_i\in E_i$ $\pi_i(x_i,\;\cdot\;):=\operatorname P_{x_i}^{(i)}$ for $x_i\in E_i$ $[\Delta_i]\in\Omega_i$ with $$X^{(i)}[\Delta_i]\equiv\Delta_i$$ $\tilde\tau_i:=\tau\circ p_i$ Now define $$\operatorname P_{x_1}:=\operatorname P^{(1)}_{x_1}\otimes(T_1\pi_2)$$ for $x_1\in E_1$ and $$\operatorname P_{x_2}:=\delta_{\left[\Delta_1\right]}\otimes\operatorname P^{(2)}_{x_2}$$ for $x_2\in E_2$ . I would like to show that if $f:E\to\mathbb R$ is bounded and $\mathcal E$ -measurable, then $$\int_0^{\tilde\tau_1}f\left(\tilde X^{(1)}_t\right)\:{\rm d}t\tag1$$ and $$\int_0^{\tilde\tau_2}f\left(\tilde X^{(2)}_t\right)\:{\rm d}t\tag2$$ are $\operatorname P_{x_1}$ -independent for all $x_1\in E_1$ . Further assumptions on $T_1$ are obviously necessary. I would like to assume that $T_1(\;\cdot\;,B_2)$ is $\mathcal F_{\tau_1-}$ -measurable for all $B_2\in\mathcal E_2$ . This should be enough. However, I don't know how to proceed. Maybe we can show the stronger claim that $\left(X^{(1)}_t\right)_{t\in[0,\:\tau_1)}$ and $\left(X^{(2)}_t\right)_{t\in[0,\:\tau_2)}$ are $\operatorname P_{x_1}$ -independent. But I struggle to show the latter ... It might be useful to note that $$\operatorname E_{x_1}\left[g\left(\tilde X^{(2)}_0\right)\mid\mathcal F_{\tilde\tau_1-}\right]=T_1\left.f\right|_{E_2}\circ p_1\tag3$$ for all bounded $\mathcal E$ -measurable $g:E\to\mathbb R$ . In particular, if $$T_1(\omega_1,\;\cdot\;)=\mu_1\left(X^{(1)}_{\tau_1-}(\omega_1),\;\cdot\;\right)\;\;\;\text{for all }\omega_1\in\Omega_1$$ (assuming $\mathcal E_i$ is the Borel $\sigma$ -algebra of a topology on $E_i$ and $X^{(1)}$ is left-regular with respect to that topology), then $$\operatorname E_{x_1}\left[g(\tilde X^{(2)}_0)\mid\tilde X^{(1)}_{\tilde\tau_1-}\right]=(\mu_1\left.f\right|_{E_2})\left(\tilde X^{(1)}_{\tilde\tau_1-}\right)\tag4.$$ So, my interpretation is that $\tilde X^{(2)}$ and $\tilde X^{(1)}$ are conditionally independent given $\tilde X^{(1)}_{\tilde\tau_1-}$ (or maybe better $\mathcal F_{\tilde\tau_1-}$ )? Note that it is easy to show that $\tilde X^{(2)}$ and $\tilde X^{(1)}$ are actually $\operatorname P_{x_1}$ -independent for all $x_1\in E_1$ , when $T_1$ does not depend on the first argument (i.e. is simply a measure).","['measure-theory', 'independence', 'stochastic-processes', 'stopping-times', 'probability-theory']"
4713312,Stronger bound on abelianization of 2-transitive group,"Theorem 1 of On abelian quotients of primitive groups states that $$
|G/G'| \leq n 
$$ for any primitive permutation group $ G $ of degree $ n $ . In other words, for primitive permutation groups the size of the abelianization is bounded by the degree. Is it the case that $$
|G/G'|\leq n-1
$$ for any 2-transitive permutation group $ G $ of degree $ n $ ? That slightly stronger bound seems to hold for the 2-transitive group AGL(1,n) of degree $ n $ . Side note: It's interesting to note that the bound $ n $ fails for transitive groups. For example consider the degree $ n=128 $ group $ 2 \wr 2^6 $ , where $ 2^6 $ is an elementary abelian $ 2 $ group. This group has abelianization of size bounded below by $ 2^9 $ (see equation 6.2 of the reference above) and so is certainly not bounded above by the degree $ 2^7 $ .","['permutations', 'group-theory', 'finite-groups']"
4713360,Bound on inverse covariance from covariance in regularized covariance estimation problem,"In this paper by Bickel and Levina , I am confused about result (A15) which claims that since $$
(A14) \qquad \| \text{Var}(\mathbf{X}) - \widehat{\text{Var}}(\mathbf{X})\|_{\max} = O_P(n^{-1/2} \log^{1/2}p)
$$ then it follows that $$
(A15) \qquad \| \text{Var}^{-1}(\mathbf{Z}_j^{(k)}) - \widehat{\text{Var}}^{-1}(\mathbf{Z}_{j}^{(k)})\|_{\max} = O_P(n^{-1/2} \log^{1/2}p)
$$ where $\mathbb{R}^p \ni \mathbf{X} = (X_1,\dots,X_p) \sim N(0,\Sigma_p)$ and $$
\mathbf{Z_j^{(k)}} =(X_{\max(1, j-k)}, \dots, X_j)
$$ is the collection of $X_j$ and $k$ previous neighbours in $\mathbf{X}$ . The estimated quantities are based on an i.i.d. sample $\mathbf{X}_1,\dots, \mathbf{X}_n \sim N(0,\Sigma_p)$ , so $$
\widehat{\text{Var}}(\mathbf{X}) := \frac{1}{n} \sum_{i=1}^n (\mathbf{X}_i - \overline{\mathbf{X}})(\mathbf{X}_i - \overline{\mathbf{X}})^T
$$ and $$
\widehat{\text{Var}}^{-1}(\mathbf{Z}_j^{(k)}) := \left (
\frac{1}{n} \sum_{i=1}^n (\mathbf{Z}_{i,j}^{(k)} - {\overline{\mathbf{Z}}_{j}^{(k)} })(\mathbf{Z}_{i,j}^{(k)} - {\overline{\mathbf{Z}}_{j}^{(k)} })^T\right)^{-1}
$$ where $\mathbf{Z}_{i,j}^{(k)} = (X_{i, {\max(1, j-k)}}, \dots, X_{i,j})$ - i.e. the collection of $X_j$ and its k previous neighbours for the $i$ -th observation. The authors take: $k  \asymp (n^{-1} \log p)^{-1/2(\alpha+1)}$ . For the purpose of this question I think that exact meaning of $\alpha$ is not important and we can think of it as some positive constant.  I am unsure how the bound on the element-wise maximum of the sample covariance matrix in (A14) leads to the same bound on the inverse covariance in (A15). It seems to be a trivial step in their proof so I feel like that I must be missing something obvious here.","['statistics', 'concentration-of-measure', 'covariance', 'machine-learning', 'probability']"
4713366,"How to evaluate $\int\frac{x-1}{(x+1)\sqrt{x^3+x^2+x}}\,dx$","How to evaluate the following pseudo-elliptic integral? $$\int\frac{x-1}{(x+1)\sqrt{x^3+x^2+x}}\,dx$$ I think that I should make $(x+1)^4$ and substract something under the square root. I got $(x+1)^4 -x^4-3x^3-5x^2-3x-1$ but I have no idea what I should do with summands.",['integration']
4713369,Question regarding a qual problem,"I am trying to solve this problem appeared in qualifying exam at University of Michigan in 2023. Problem 5: Let $f(\cdot)$ be an integrable function on $\mathbb{R}^n$ and $Mf$ the corresponding Hardy-Littlewood maximal function $$Mf(x) = \sup_{R > 0} \frac{1}{|B(x,R)|} \int_{B(x,R)} |f(y)|\ dy, \quad x \in \mathbb{R}^n,$$ where $B(x,R)$ denotes the ball centered at $x$ with radius $R$ . Show there is a constant $C_n$ , depending only on $n$ , such that $$m \{ x \in \mathbb{R}^n : Mf(x) > s \} \leq \frac{C_n}{s} \int_{\{x : |f(x)| > s/2\}} |f(y)| \ dy.$$ Hint: Consider the function $f_s$ defined by $f_s(x) = |f(x)|$ if $|f(x)| > s/2$ , $f_s(x) = 0$ otherwise. Solution: Suppose that $Mf(x) > s$ . Then there exists a ball $B(x,R)$ such that $$\frac{1}{|B(x,R)|} \int_{B(x,R)} |f(y)| \ dy = \frac{1}{|B(x,R)|} \int_{B(x,R)} |f_s(y)|\ dy + \frac{s}{2} > s.$$ We conclude that $\{Mf > s \} \subset \{ M f_s > s/2 \}$ . The result now follows from the Hardy-Littlewood inequality applied to $f_s$ . I cant understand how are we getting the Right hand side of the equality in the solution.
I was wondering if someone can help me. Thanks in advance!","['measure-theory', 'real-analysis']"
4713371,Determining $\displaystyle\lim _{x \rightarrow 0} f(x) g(x)$ given conditions on $f$ and $g$,"This was the 4th question that appreared in yesterday's JEE ADVANCED $2023$ : Let $f:(0,1) \rightarrow \mathbb{R}$ be the function defined as $$f(x)=\sqrt{n}$$ for all $x \in\left[\frac{1}{n+1}, \frac{1}{n}\right),$ where $n \in \mathbb{N}$ . Let $g:(0,1) \rightarrow \mathbb{R}$ be a
function such that $$\int_{x^2}^x \sqrt{\frac{1-t}{t}} d t<g(x) <2
\sqrt{x}$$ for all $x \in(0,1)$ . Then $$\lim _{x \rightarrow 0} f(x) g(x)$$ (A) does NOT exist (B) is equal to 1 (C) is equal to 2 (D) is equal to 3 On noticing the domain of $f(x),$ I realised that the limit must be one-sided. But, this being an JEE Advanced exam, which usually asks really difficult questions and is known as the exam of exceptions, I thought that this might be some trap, so I didn't choose option A. Then I observed the definite integral and substituted $t=\cos^2(\theta)$ and obtained $\tan(\theta).$ But I don't have any clue on how to accordingly change the integration limits. What is a good solution strategy or thinking process for this exercise?","['integration', 'limits', 'calculus', 'real-analysis']"
4713382,Partial derivatives and composition with matrices,"Given $f$ and $g$ are each functions sending $(0,0)$ to $(0,0)$ , if you know that $$ [Df]_{(0,0)} = 
\begin{pmatrix}
2 & -1 \\
1 & 0 \\
\end{pmatrix} $$ and $$ [D(f \circ g)]_{(0,0)} = 
\begin{pmatrix}
-1 & 4 \\
2 & 3 
\end{pmatrix} $$ What is the derivative of $g$ at $(0,0)$ ? I believe the strategy is to replace two elements in the equation below (recognizing that $g$ and $f$ are switched from normal convention in the given problem): $ [D(g \circ f)]_{a} = [Dg]_{f(a)} \cdot [Df]_{a} $ Via substitution, I get: $$
\begin{pmatrix}
-1 & 4 \\
2 & 3 
\end{pmatrix} =
\begin{pmatrix}
2 & -1 \\
1 & 0 
\end{pmatrix}
\begin{pmatrix}
? & ? \\
? & ? 
\end{pmatrix}
$$ However, I don't know how to solve the unknown matrix from here. In a more generalized sense, is there a way to find matrix A when AB = C and we know B and C? Or, is there another way to solve this problem? I've only so far seen $Ax = b$ used when $x$ is a column vector. I tried to take the inverse of $A$ and multiply it with $C$ , but reverse checking my work, that didn't work out. (This is problem 2 in Robert Ghrist's textbook in multivariable calculus, volume 2, chapter 5.)","['matrices', 'jacobian', 'multivariable-calculus', 'matrix-calculus', 'partial-derivative']"
4713383,How do we solve this insanely sane sequence question?,"This is question from the algebra section of JEE Advanced 2023 Let $7 \overbrace{5 \cdots 5}^r 7$ denote the $(r+2)$ digit number where the first and the last digits are 7 and the remaining $r$ digits are 5 . Consider the sum $S=77+757+7557+\cdots+7 \overbrace{5 \cdots 5}^{98}7$ . If $S=\frac{7 \overbrace{5 \cdots 5}^{99}7+m}{n}$ , where $m$ and $n$ are natural numbers less than 3000 , then the value of $m+n$ is MY THINKING So my first aim was to calculate the value of S. And my weird intuition made me draw something that is similar to a Pascal's Triangle starting from the last term of S ( I don't know how to draw it here using latex but if anybody could help me by editing thanks! ) which is ..... $7 \overbrace{5 \cdots 5}^{96}7$ $7 \overbrace{5 \cdots 5}^{97}7$ $7 \overbrace{5 \cdots 5}^{98}7$ So we get an equilateral triangle in the above form and on calculating the sum of the right side of the triangle, we place the unit place's digit as the unit place digit of the value of S and carry over the tenth's and the hundredth's placed digits as remainder on to the summation of the terms of the immediate row parallel to the right side. And then I repeated the process. But something happened in between and I failed in calculating the value of S. Could someone help me on how to efficiently solve this one.","['linear-algebra', 'sequences-and-series']"
4713385,Prove that E(t) is increasing in t,"Consider an irreducible continuous-time Markov chain $X_t$ on finite state space with $Q$ -matrix $Q=(q_{ij}, i,j=1,2,\cdots, N)$ , in which $q_{ij}=q_{ji}$ . Fix initial state $j$ , let $P_i(t)$ be the probability that the chain is in $i$ at time $t$ , i.e., $P_i(t)=\mathbb{P}(X_t=i)$ . Define $$E(t)=-\sum_{i=1}^{N}P_i(t)\log P_i(t).$$ The aim is to prove $E(t)$ is non-decreasing in $t$ . I have thought this problem for several days and have no idea. The only thing I am able to do is to take the derivative of $E(t)$ with respect to $t$ , but it seems no use because the expression is more complicated such that it is difficult to judge the sign. I wold appreciate it if someone could give me some useful advice on solving this problem. Thanks very much!","['entropy', 'probability-theory', 'markov-chains']"
4713389,Area-preserving,"A diffeomorphism $\varphi: S \to \bar{S}$ is said to be area-preserving if the area
of any region R \subset S is equal to the area of $\varphi(R)$ . Prove that if $\varphi$ is
area-preserving and conformal, then ϕ is an isometry. I have two ideas for this excersise. First, i consider a parametrization $x: U \to R \subset S$ and $\bar{x}:\varphi \circ x : X(U) \to \varphi(X(U)) $ a parametrization for $\bar{S}$ . We can note that if $\varphi$ is area-preserving then $A(R)=A(\varphi(R))$ . Here, i can use the definition of Area with respect to its partial derivatives and then we have that $$A(R)=\int\int_{R}||x_{u} \times x_{v}||dudv$$ But for $\bar{S}$ $$A(\varphi(R))=\int\int ||(\varphi \circ x)_{u} \times (\varphi \circ x)_{v}||dudv$$ here i'm stuck, because i'dont know how to proceed with $A(\varphi(R))$ or how to describe the partial derivatives of the composition or use the hypotesis that $\varphi$ is conformal, i'm sure that conformal does not implies that coeficients of x and $\bar{x}$ are $\lambda$ - times of another, i.e, $\bar{E}=\lambda  E, \bar{F}= \lambda F, \bar{G}= \lambda G$ , so i don't know what to do here. Another idea for this is use the definition of area with respect to first fundamental form but here the only way to develop this idea is using the wrong assumption about coefficients that i was talking about. I appreciate some idea. UPDATE i want to be sure that this attempt is good. I will proof that $$d\varphi_{q}(x_{u})=\bar{x}_{u}$$ where $x_{u}$ and $\bar{x}_{u}$ are the partial derivatives of x and $\bar{x}$ respectively. Now,for $(u_{0},v_{0}) \in U$ and $q=x(u_{0},v_{0}) $ and $\alpha$ a curve defined as $x(u,v_{0})=\alpha(u)$ , therefore $\alpha'(u)=x_{u}(u,v_{0})$ . Now, we have that $$d\varphi_{q}(x_{u}(u_{0},v_{0}))=d\varphi_{q}(\alpha'(u_{0}))=(\varphi \circ \alpha)'(u_{0})$$ then we have that $$d\varphi_{q}(x_{u}(u,v_{0}))=d\varphi_{q}(\alpha'(u))=(\varphi \circ \alpha)'(u)$$ for all $u$ but now, we can note that $$(\varphi \circ \alpha)(u)=\varphi(\alpha(u))=\varphi(x(u,v_{0})=(\varphi \circ x)(u,v_{0})=\bar{x}(u,v_{0})$$ so, finally we have that $$(\varphi \circ \alpha)'(u,v_{0})=\bar{x}_{u}(u,v_{0})$$ and we can conclude that $$(\varphi \circ \alpha)'=d\varphi_{q}(x_{u})=\bar{x}_{u}$$ this is enought or this proof is wrong?","['geometry', 'analysis', 'differential-geometry']"
4713398,Topological invariant of manifolds equipped with vector field,"Question Given a manifold $M$ equipped with a vector field $\xi$ which has a description in local coordinates. Is it possible to construct a topological invariant which can be used to tell if another manifold equipped with a vector field is equivalent up to coordinate transformation. This question can be equivalently formulated through differential $\alpha_i$ forms where $\alpha_i = \ker \xi$ Motivation Given a ordinary differential equation $\mathcal{E}$ it can be considered as a manifold equipped with the cartan distribution \begin{equation}
D = \partial_{x} + p_1 \partial_{p_0} + \ldots + p_{k-1}\partial_{p_{k-2}} + F(x,p_0,\ldots,p_{k-1})\partial_{p_{k-1}}
\end{equation} Having found equivalence classes of ordinary differential equations of a fixed order and its solution. I want to see if its possible to determine if a given ODE is equivalent to my ODE.","['differential-forms', 'vector-fields', 'ordinary-differential-equations', 'differential-geometry']"
4713407,"How do we evaluate ""a"" in this limit?","If $\quad \lim _{x \rightarrow 0}\left(\frac{a \sin ^4 \sqrt{x}}{\sqrt{\cos x}-1}\right)^{\frac{\tan \left(\ln ^2(1+\sqrt{2} x)\right)}{\ln \left(1+\sin ^2 x\right)}}=16$ ,
sum of all possible values of $a$ is On seeing this , the first thing that immediately ran through my mind is limit does not exist. Becasue $\sqrt x$ is given and when x approaches 0 from left side of 0, it take negative values and thus root x cant be defined. If I'm wrong I don't find any other way to simplify this. Can you help?","['limits', 'calculus', 'real-analysis']"
4713434,Knowing inverse matrix based upon a picture?,"I know how to calculate an inverse matrix (by creating the augmented matrix and Gaussian elimination to get the identity matrix) and I know how to do it for a general $2 \times 2$ matrix by taking two different cases when the determinant is zero or not. And, unless I'm mistaken, I also understand how a matrix is a linear transformation - basically a function - that can be understood as transforming the standard basis vectors. That is, I can see a simple $2 \times 2$ matrix and know what it does geometrically. Is there a way to tell the inverse of a generic $2 \times 2$ matrix my looking at the matrix itself? I looked at Finding inverse of a matrix geometrically but I didn't get it (There were no pictures to help me gain geometric insight). I think that I could do it for some, like the inverse of a matrix that does a stretch would be a matrix that does a compression, or a matrix that rotates counterclockwise would be a matrix that rotates clockwise -- but is there an ""instant"" way of knowing what the inverse matrix is based upon a picture?","['matrices', 'inverse', 'linear-algebra', 'linear-transformations', 'geometric-interpretation']"
4713465,Computing the integral of a certain surface,"I am given the surface $$S:=\{(x,y,z)\in\mathbb{R}^3|\sqrt{x^2+y^2}=2\cosh 2z,z\in[0,1]\}$$ They ask me to find a parametrization for this surface of revolution. This corresponds to the following $$\varphi(u,v)=(2\cosh(2u)\cos(v),2\cosh(2u)\sin(v),u)$$ with $(u,v)\in[0,1]\times[0,2\pi]$ . With this, they ask me to compute the integral over the 2-form $\omega=(z^2+2z)dx\wedge dy+(2x+2yz-z)dx\wedge dz$ if our surface is oriented by the normal vector $\nu(2,0,0)=(-1,0,0)$ . Since $\varphi(0,0)=(2,0,0)$ , $\varphi_u(0,0)=(0,0,1)$ and $\varphi_v(0,0)=(0,2,0)$ , we have that $\nu(0,0)=\varphi_u(0,0)\wedge\varphi_v(0,0)=(-2,0,0)$ and thus we have chosen the correct orientation. Now, $$\int_{S}\omega=\int_{[0,1]\times[0,2\pi]}\varphi^{*}((z^2+2z)dx\wedge dy+(2x+2yz-z)dx\wedge dz)$$ To compute the pullback we do the following $$(u^2+2u)d(2\cosh(2u)\cos(v))\wedge d(2\cosh(2u)\sin(v))+(4\cosh(2u)\cos(v)+4u\cosh(2u)\sin(v)-u)$$ $$d(2\cosh(2u)\cos(v))\wedge du$$ $$= 4(u^2+2u)(2\sinh(2u)\cos(v)du-\sin(v)\cosh(2u)dv)\wedge (2\cosh(2u)\cos(v)dv+4\sin(v)\sinh(2u)du)+2(4\cosh(2u)\cos(v)+4u\cosh(2u)\sin(v)-u)(2\sinh(2u)\cos(v)du-\sin(v)\cosh(2u)dv)\wedge du$$ $$=4(u^2+2u)(4\sinh(2u)\cosh(2u)\cos(v)^2 du\wedge dv + 4\sinh(2u)\cosh(2u)\sin(v)^2 du\wedge dv)+2\sin(v)\cosh(2u)(4\cosh(2u)\cos(v)+4u\cosh(2u)\sin(v)-u)du\wedge dv$$ $$=16(u^2+2u)\sinh(2u)\cosh(2u)du\wedge dv+2\sin(v)\cosh(2u)(4\cosh(2u)\cos(v)+4u\cosh(2u)\sin(v)-u)du\wedge dv$$ But this seemes a super long process, plus I don't really know if I will be able to integrate the expression. Is there a simpler way? Like using Stokes theorem or so?","['integration', 'definite-integrals', 'surface-integrals', 'multivariable-calculus', 'differential-forms']"
4713478,Analysing an integral asymptotically,"Let $A$ be a constant, say larger than $1/2$ . I am interested in the integral $$I_s = \int_0^\infty (A+\varepsilon+\nu^2)^{-s} e^{ix\nu} d\nu$$ where $s$ is a complex variable, ultimately with fixed real part ( $\Re(s) = 2+\delta$ ). Option 1, moving contour By moving the integration line until $\Im \nu = A$ , for which everything in holomorphic, we are led to consider the integral $$I_s = e^{-Ax}\int_0^\infty (\varepsilon + 2iAt + t^2)^{-s} e^{ixt} dt$$ and I do want to see this exponential decay. However, this integral does not converge absolutely (the integrand absolute value is about $t^{\Re(s)}$ ). Question 1. Is this integral convergent? How does it depend on $s$ and on $x$ ? Option 2, obtaining an explicit formula The original integral is a Basset integral for the $K$ -Bessel function (given e.g. here ) which states that it looks like $$K_{s}(zx) \frac{x^{s}}{z^s\Gamma(s)}$$ where $z^2 = A+\varepsilon$ Question 2. Knowing that $\Re(s) = 2 + \delta$ for a small $\delta>0$ , how does this depend upon $s$ (when its imaginary part grows)? How does it depend upon $x$ ? It looks like it blows up vertically in $s$ , since the $\Gamma$ function does by Stirling formula.","['integration', 'analysis', 'complex-analysis', 'contour-integration', 'mellin-transform']"
4713489,"Number of partitions of $\{1, 2, \dots , 25\}$ into $5$ equal parts, where no part is equal to either $\{1, \dots,5\}$, $\cdots$, or $\{21,\dots,25\}$","Find the number of partitions of the set $\{1, 2, \dots , 25\}$ into $5$ equal parts, where no part is equal to either $\{1, \dots , 5\}$ , $\{6, \dots , 10\}$ , $\{11, \dots , 15\}$ , $\{16, \dots , 20\}$ , or $\{21, \dots , 25\}$ . So far I have the following: $\frac{25!}{(5!)^55!}$ , which is the total possible number of arrangements of a 25 string, divided by $5!^5$ for the double counting of equivalent sets, and one more $5!$ for the counting of permutations of the sets themselves. $\binom{5}{1} \frac{20!}{(5!)^44!}$ , which is the number of ways of fixing one unallowed subset, and counting all the remaining arrangements in a fashion similar to above. $\binom{5}{2} \frac{15!}{(5!)^33!}$ $\binom{5}{3} \frac{10!}{(5!)^22!}$ $1$ , which is the number of ways of fixing four disallowed subsets, which automatically fixes the remaining subset, as the remaining 5 elements inadvertently only allow for 1 more combination. And from inclusion exclusion, adding these together I get as an answer $$\frac{25!}{(5!)^55!} - \binom{5}{1} \frac{20!}{(5!)^44!} + \binom{5}{2} \frac{15!}{(5!)^33!} - \binom{5}{3} \frac{10!}{(5!)^22!} + 1$$ Does this logic seem correct? I would appreciate more insight into this problem, as the question I am working with has no provided answer.","['inclusion-exclusion', 'combinatorics']"
4713527,Is the nearest point on a level set always in the direction of the gradient?,"Suppose I have a smooth function $f:\mathbb{R}^2\to \mathbb{R}$ and consider two level sets $$
\begin{align}
    L_1 &= \{x \,:\, f(x) = c_1\} \\
    L_2 &= \{x \,:\, f(x) = c_2\}
\end{align}
$$ Now take a point $p_1\in L_1$ , and consider the nearest neighbour $p_2\in L_2$ . Is it true that $p_2$ is always in the direction of the gradient at $p_1$ ? In other words, there always exists $\gamma \in\mathbb{R}$ such that $$
p_2 = p_1 + \gamma\nabla f(p_1)
$$ Attempted Solution Consider $$
d(p_1, x) = \|p_1 - x\|^2 = \|p_1\|^2 - 2p_1^\top x + \|x\|^2
$$ I want to minimize this with respect to $x$ constrained with $f(x) = c_2$ . The lagrangian is $$
\mathcal{L} = d(p_1, x)  + \lambda(f(x) - c_2)
$$ Take the derivative with respect to $x$ $$
\nabla_x d(p_1, x) = -2p_1 + 2x + \lambda \nabla f(x)
$$ Setting this to zero gives $$
x = p_1 - \frac{1}{2}\lambda \nabla f(x)
$$ But now the gradient is evaluated at $x$ for some reason.. which leads me to believe $$
p_2 = p_1 + \gamma \nabla f(p_2)
$$ rather than $$
p_2 = p_1 + \gamma \nabla f(p_1)
$$ Regarding regions of high curvature","['multivariable-calculus', 'calculus', 'differential-geometry']"
4713536,"How to understand Cartan formula $[L_X,i_Y]=i_{[X,Y]}$ geometrically?","Let $M$ be a smooth manifold, $X,Y$ be the vector field over $M$ , the Lie derivative $L_X$ is defined by $L_X=d\circ i_X+i_X\circ d$ , where $i_X$ means contraction operation of $X$ , then by a complicated computation, we can prove the Cartan formula $[L_X,i_Y]=i_{[X,Y]}$ algebraically, but does anyone have a geometric intuition about why it should be true?",['differential-geometry']
4713544,"Convergence in probability: $X_n \overset{P}{\rightarrow} X,\ Y_n \overset{P}{\rightarrow} Y \implies X_n Y_n \overset{P}{\rightarrow} XY$","According to the Wikipedia article on convergence of random variables, the above property holds. How can we prove it? I guess one way would be to use a second property stated in the article, namely, $$X_n \overset{P}{\rightarrow} X,\ Y_n \overset{P}{\rightarrow} Y \implies (X_n, Y_n) \overset{P}{\rightarrow} (X, Y)$$ and subsequently apply the continuous mapping theorem with $g: (x, y) \mapsto xy$ . However, the proof for this second property requires definitions of convergence in two dimensions and additional properties of the involved distance metrics. Can we prove $X_n \overset{P}{\rightarrow} X,\ Y_n \overset{P}{\rightarrow} Y \implies X_n Y_n \overset{P}{\rightarrow} XY$ only using the one-dimensional random variables $X_n, Y_n, X, Y$ ?",['probability-theory']
4713554,How to prove: $\lim_{y\to\infty}\int_{1}^{\infty}\frac{f(x)}{x^2+y^2}dx=0 $,"Let $f:\left[0,\infty\right]\to \left[0,\infty\right]$ be locally integrable. It is integrable on every compact subinterval $I \subset  \left[0,\infty\right]$ , and assume that the improper integral: $$\int_{1}^{\infty}\frac{f(x)}{x^2}\,dx$$ converges and is finite. Compute: $$\lim_{y\to\infty}\int_{1}^{\infty}\frac{f(x)}{x^2+y^2}dx $$ The limit exists because the integral exists by comparison test for each $y$ and is decreasing function of $y$ . I am sure that the answer is zero but I am not allowed to use the Dominated convergence theorem to solve this question, therefore I need either use $\epsilon$ $\delta$ definition or need to bound this integral and apply squeeze theorem somehow.","['improper-integrals', 'analysis', 'real-analysis', 'limits', 'convergence-divergence']"
4713641,How to define a function with a parameter such that $\int_0^n f_n(x)dx = \int_0^m f_m(x)dx$?,"So, $f_n(x): R \to R$ is a function such that for any integer $n > 1$ and any real $v > 1$ $f_n(0) = v$ $f_n(n) = 1$ $f_n(x) > 0$ for $0 \leq x \leq n$ $f_n(x)$ is continuous and differentiable on $0 \leq x \leq n$ For any arbitrary integers $n$ and $m$ such that $n > 1$ and $m > 1$ the following is true: $\int_0^n f_n(x)dx = \int_0^m f_m(x)dx$ Finding a function that satisfies 1-4 is a fairly simple task. E.g. $f_n(x) = v^{1-\frac{x}{n}}$ , or $f_n(x) = (1 - v) \frac{\log{x+1}}{\log{n + 1}} + v$ , or even $f_n(x) = v + (1 - v)\frac{x}{n}$ . The real problem comes with restriction 5. I spent all day trying different families of functions, but no matter what I try, it just doesn't work. So, the questions are Is it even possible to define $f_n(x)$ ? If it is, what would the definition be? If it is not possible to define, why? Thank you in advance.","['calculus', 'functions']"
4713647,Gluing two disks to get an exotic sphere,"I have found interest in the subject of exotic spheres. Particularly, the 7-sphere. To create such a sphere, you glue two copies $\mathbb{D}^4\times \mathbb{S}^3$ along the boundary identifying $(u,v)$ with $(u,u^hvu^j)$ such that $h+j=\pm1$ . Now my question is, the fact that I am trying to construct such a sphere by gluing two 7-disks along the boundary via a diffeomorphism. However, it seems I cannot find such a diffeomorphism explicitly. By diffeomorphism I mean the map $h:\partial \mathbb{S}^6\to\partial \mathbb{S}^6$ . Any help is appreciated.","['spheres', 'manifolds-with-boundary', 'differential-geometry']"
4713674,Intersections of two cones with parallel axes,"In mathpages, ""On the Ellipse"" , the author starts, as usual, by considering an intersection of a plane and a cone. But, in order to derive further properties of the ellipse, he immediately states after that By symmetry, the locus of intersection between the cone and the plane is also the locus of intersection of two right cones with parallel axes offset from each other, as shown in the figure below. Why should this be the case, a-priori?
Is there any reason to think, without any derivations\algebra, that it should be symmetric about this axis? Or vice versa, that the intersections of two right cones with parallel axes should be planar - without any proof or calculations? The author doesn't regard this point anywhere in this page. Any help is appreciated.","['conic-sections', 'geometry', '3d']"
