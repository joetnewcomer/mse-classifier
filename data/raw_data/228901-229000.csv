question_id,title,body,tags
4739973,Why are we interested in Einstein metrics?,"A Riemannian manifold $(M,g)$ is Einstein if its Ricci curvature tensor satisfies $$ \operatorname{Ric} = \lambda g $$ for some real constant $\lambda$ . In Besse's ""Einstein Manifolds"" I read that this condition is the nicest extension of the concept of constant curvature in dimensions higher than two, and so Einstein Manifolds generalize constant curvature smooth surfaces without being too rigid. I understand this motivation but it seems a bit weak to me to justify all of the efforts in finding existing results of these structures: do Einstein Manifolds have some special properties which make them so interesting?","['manifolds', 'riemannian-geometry', 'differential-geometry']"
4740062,Rectangle inside an ellipse that is inside a rectangle,"Suppose there is a rectangle $ABCD$ with $A=(-2,1), B=(-2,-1), C=(2,-1), D=(2,1).$ Then we have an ellipse $\frac{x^2}{4} + y^2=1$ that is tangent to the rectangle. Then we build the rectangle $EFGH$ which has the corners as the intersection of the ellipse and the diagonals of $ABCD$ as can be seen in . The question is that how do we prove $\textbf{analytically}$ that $\frac{A(ABCD)}{A(EFGH)}=2$ for any ellipse? I already know that it is really easy and cool to prove it by using the homeomorphism of the ellipse and the circle by deforming the question into the easier form.","['area', 'conic-sections', 'geometry']"
4740074,Find an angle in a rectangle. [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 11 months ago . Improve this question A few days ago, I came across a problem that has driving me insane. It seems so simple, but I just can't solve it, it's like a blind spot to me. Here's the problem: A point $N$ is taken on the side $AB$ of a rectangle $ABCD$ , such that $ND=CD$ . Find the measurement of $\angle CND$ if $AB=2BC$ . Thanks in advance!","['contest-math', 'rectangles', 'triangles', 'geometry']"
4740095,Calculate Number of Permissible Permutations,"Consider the set of elements $$
\Sigma =
\{ 
s, a_1, a_2, \dots, a_m
\}
\cup
\alpha_1 \cup \dots \cup \alpha_m
$$ where $\alpha_i = \{ \alpha_{i, 1}, \dots, \alpha_{i, n_i} \}$ for some fixed numbers $n_i, i = 1, \dots, m$ . We are interested in the number of permutations which fulfill the following set of rules: $s$ is the first element. For all $i = 1, \dots, m$ and for all $k = 1, \dots, n_i$ , it holds that $\alpha_{i, k}$ comes after $a_i$ . $a_1$ comes before $a_2$ comes before $a_3$ , $\dots$ , $a_{m-1}$ comes before $a_{m}$ . Idea: Let $|\Sigma| = n$ . Without any condition, there are $n!$ permutations of $\Sigma$ . Taking the first condition into account, there are $$
n! \cdot \frac 1 n = (n - 1)!
$$ permutations since by symmetry, there is an equal number of permutations where each element is in the first position. Similarly, there are $$
(n - 1)! \cdot \prod_{i = 1}^{m} \frac 1 {|\alpha_i| + 1}
$$ permutations taking both condition 1. and 2. into account. Now comes the tricky part where I am unsure: Consider the condition "" $a_k$ before $a_{k + 1}$ "" for some $k$ . By symmetry, only one half of the permutations fulfills this condition since in the other half, $a_k$ comes after $a_{k + 1}$ . If this argument is true, then it could (maybe) be used repetitively, which yields $$
(n - 1)! \cdot \prod_{i = 1}^{m} \left( \frac 1 {|\alpha_i| + 1} \right) \cdot \frac 1 {2^{m-1}}
$$ permutations in total.","['permutations', 'combinations', 'solution-verification', 'combinatorics']"
4740153,"Does $(7,3,6,7)$ occur in the infinite sequence $(1,9,9,3,...)$ in which every component is the unit digit of the sum of its $4$ preceding components?","I'll appreciate solution hints for the following question from a past exam in Discrete Mathematics that was administered in my university. Denote by $\mathbb{N}$ the set of positive integers. For every $n\in\mathbb{N}$ denote by $n\%10$ the remainder of the division of $n$ by $10$ , i.e. $n%10$ is the unique number satisfying: $$
\begin{align}
n\%10&\in\{0,1,2,\dots,9\},\\
n-(n\%10)&=0\mod10.
\end{align}
$$ We postulate that the infinite sequence $n = (n_1, n_2, \dots)$ satisfies: $$
\begin{align}
n_1 &= 1,\\
n_2 &= 9,\\
n_3 &= 9,\\ 
n_4 &= 3,
\end{align}
$$ and for every $i\in\mathbb{N}_1$ $n_{i+4}$ is the unit digit in the decimal representation of the sum of the preceding $4$ components of $n$ : $n_{i+4}=(n_i+n_{i+1}+n_{i+2}+n_{i+3})\%10$ . Is there some $i_0\in\mathbb{N}$ such that the following four equations hold? $$
\begin{align}
n_{i_0}&=7,\\
n_{i_0+1}&=3,\\
n_{i_0+2}&=6,\\
n_{i_0+3}&=7.
\end{align}
$$ My attempts to solve the question I computed $n_1$ to $n_{45}$ : $$
\begin{align}
n':=(&1,9,9,3,2,3,7,5,7,\\
&2,1,5,5,3,4,7,9,3,\\
&3,2,7,5,7,1,0,3,1,\\
&5,9,8,3,5,5,1,4,5,\\
&5,5,9,4,3,1,7,5,6),
\end{align}
$$ and I extended the sequence $(7,3,6,7)$ ""backwards"" to a total of 27 components: $$
\begin{align}
m:=(&4,3,9,5,1,8,3,7,9,\\
&7,6,9,1,3,9,2,5,9,\\
&5,1,0,5,1,7,3,6,7).
\end{align}
$$ I was hoping this would enable me to solve the question by one of the following ways. If it turned out that for some $i\in\{1,2,\dots,42\}$ $n'_i=m_1=4$ , $n'_{i+1}=m_2=3$ , $n'_{i+2}=m_3=9$ , $n'_{i+3}=m_4=5$ , then the answer to the original question would be: yes. However, this is not the case. If I could identify a repeating pattern in $n'$ which didn't manifest in $m$ , or vice versa, then the answer to the original question might be: no. However, I was unable to identify such patterns. If I could show that one of the digits $0, 1, \dots, 9$ didn't occur in $n'$ , but occurred in $m$ , or vice versa, then the answer to the original question might be: no. However, all the digits occur in both $n'$ and $m$ . If I were able to prove that every possible quadruple $(a,b,c,d)\in\{0,1,2,\dots,9\}^4$ eventually occurs in either $(n_1,n_2,n_3,\dots)$ or $(\dots,m_{25},m_{26},m_{27})$ (the infinite extension of $m$ ""backwards""), then the answer to the original question would be: yes. However, I wasn't able to prove this.","['elementary-number-theory', 'discrete-mathematics', 'sequences-and-series']"
4740158,"How to solve ODE's given evaluations and constraints of an equation or its derivatives, but not the equations themselves?","How to begin to solve ODE problems where no equations are known, only the evaluations of equations at certain points? I tried googling this and searching the exchange here, but I'm not even sure what the name for this type of problem is, so I am having difficulty finding relevant results. Seems like all solutions for ODE require having some equation already and starting from there. Here is an example on the simpler side that I am currently working on where I am trying to find f that satisfies all of the following: $$ f'(0) = \infty $$ $$ f'(1) = \infty $$ $$ f'(x) > 0 $$ $$ f(0) = 0 $$ $$ f(1) = 1 $$ $$ f(x) = 1 - f(1 - x) $$ and smooth and continuous for all $x \in [0, 1]$ Simply, a continuously increasing function from 0 to 1 with vertical slope at the edges and symmetry about the middle point. I know the requirements, but I have no equations to start from. And this just an example that I happen to have handy, so I am not necessarily looking for just an answer to this set; I would like an understanding of how to approach this type of problem. (not sure what all tags this should have, feel free to let me know in comments)","['functional-equations', 'functions', 'functional-analysis', 'ordinary-differential-equations']"
4740187,Infinitesimal generator for non-Markov process,"I have read that the infinitesimal generator for a Markov process $(X_t)_{t \geq 0}$ we define the generator $A$ by $$Af(x) := \lim_{t \downarrow 0} \frac{\mathbb{E}^x(f(X_t))-f(x)}{t} $$ whenever the limit exists in $(C_{\infty},\|\cdot\|_{\infty})$ . The definition implies that $$\mathbb E[Af(X)]=0$$ if $X$ is the stationary distribution. I want to know why the definition is limited to Markov processes and not defined for a general stochastic process?","['stochastic-processes', 'markov-process', 'probability-theory', 'probability']"
4740244,Confusion about weak and strong convergence,"This might be rather elementary but I can't seem to find what's wrong with the following argument. I suppose it's something rather trivial.
For operators $T\in B(\mathcal{H})$ We say that $T_i\to T$ weakly if $\forall x,y\in \mathcal{H}$ we have $$\langle T_i x, y\rangle \to \langle T x, y\rangle$$ We say that $T_i\to T$ strongly if $\forall x\in \mathcal{H}$ we have $$\| T_ix-Tx\|\to 0$$ What is wrong with the following argument? since $\langle T_i x, y\rangle \to \langle T_i x, y\rangle$ by linearity this is the same as saying $\langle (T_i-T) x, y\rangle \to 0$ .
Now since this holds for any $x$ and $y$ then it holds in particular in the special case when $x=y$ . But this would mean that as a special case $$\langle (T_i-T) x, x\rangle=\|T_ix-Tx\|^2 \to 0$$ .
This would mean that weak convergence implies strong convergence which is obviously wrong (I know that in fact, it is the other way around). I seem to be confusing something elementary. Where does my confusion stem from?","['operator-theory', 'functional-analysis', 'weak-convergence', 'strong-convergence']"
4740271,Why can't a fiber bundle be written as a cartesian product?,"I am learning about fiber bundles and I understand both the formal definition and the intuitive picture of the space being ""twisted"". However one point I am having trouble with is why we cannot express a fiber bundles in terms of a product. More specifically, let $\pi: E \rightarrow M$ be a fiber bundle with fiber $F$ . What is wrong with viewing $E$ as $M \times F$ , and expressing every $e \in E$ as $(m, f)$ for some $m \in M$ and $f \in F$ ? For example take the Mobius strip. Even though it is ""twisted"", we can describe any point on it by specifying a point in the base manifold $x \in S^1 = M$ and where we are in the fiber at that point by specifying $y \in (-1, 1) = F$ . What is wrong with this view?","['manifolds', 'general-topology', 'fiber-bundles', 'differential-geometry']"
4740279,Does the sum over reciprocals of all sums of squares converge,"There is a useful result when studying lattices that the sum $\sideset{}{'}\sum_{m,n}(m^2 + n^2)^{-r/2}$ converges for $r > 2$ , where the sum is taken over all pairs of integers $(m,n)\neq(0,0)$ . I'm curious whether the condition $r > 2$ is really necessary or not. Does the sum $$\sideset{}{'}\sum_{m,n}\frac{1}{m^2 + n^2}$$ converge? Can we say what happens for fractional powers, or more generally, for which $x > 0$ the sum $$\sideset{}{'}\sum_{m,n}\frac{1}{(m^2 + n^2)^x}$$ converges?","['calculus', 'integer-lattices', 'summation', 'sequences-and-series']"
4740283,Approximating ODE solution with polynomial function,"I am trying to decipher the method for generating non-linear frequency-modulated signals described in this paper by A. W. Doerry .  On page 5, there is an algorithm described that boils down to approximating a solution to the following differential equation $$\omega'(t)=\frac{\omega'(0)}{W(\omega(t)-\omega(0))}$$ with the constraint of $$\int_{-T/2}^{+T/2}\omega'(t)\,dt=\Omega.$$ A few easy to work with forms for $\omega(t)$ are given.  The one I'm focusing on is the polynomial form. $$\tilde\omega(t)=\sum_{n=1}^N c_n\, t^{n-1} $$ The suggested iterative procedure for finding the polynomial (or whichever form) function fit is Start with some initial $\tilde\omega'(t)$ Integrate $\tilde\omega'(t)$ to get $\tilde\omega(t)$ Adjust $\tilde\omega'(t)$ and $\tilde\omega(t)$ to meet $\Omega$ constraint Calculate $W(\tilde\omega(t)-\tilde\omega(0))$ and the new $\tilde\omega'(t)$ Repeat steps 2-4 until the solution converges I am paraphrasing some of this and simplifying the notation a bit.  Either way, I interpreted the above to mean start with some polynomial coefficients, fit the polynomial coefficients in $\tilde\omega'(t)$ to $\tilde\omega'(0)/W(\tilde\omega(t)-\tilde\omega(0))$ based on the coefficients you've got, adjust for constraints, and repeat.  This does not appear to work and I don't really see how it would. Am I interpreting this wrong?  Is the algorithm in this paper referring to some well-known numerical differential equation solution approximation method? Edit $W(x)$ is a weighting function, specifically a sidelobe taper window.  Assume real and symmetric.  For example, a raised cosine window, $$W(x) = 1+\frac{1-\alpha}{\alpha}\cos(Cx)$$ with $C$ being some constant to make the span work out right.  With $\alpha=25/46$ this is the ever-popular Hamming window.","['numerical-calculus', 'numerical-methods', 'ordinary-differential-equations']"
4740287,Is it possible to calculate the angle between two observers given their angles to three of the same points?,"Given the image below, is it possible to calculate the angles E and/or F given the angles $A,B,C$ and $D$ ? The situation is as follows: Two observers each measure the angle between point $1$ and point $2$ (these angles are $A$ and $C$ respectively) and point $2$ and point $3$ ( $B$ and $D$ ). The three points are the same real-world points for both observers, but the exact location (distance etc) of both points is unknown. Is it possible to calculate the angle $E$ and/or $F$ here? It feels like this should be possible but I'm not entirely sure if and how. The reason I think this is that given the two measured angles, there's only one possible position for the observer to be (if the points were in a straight line, there could be another point on the other side of the line but let's disregard that for now). Given that we have the two observer points for which we know that there is only one possible location, it feels like it should be possible to then calculate the angle between the two of them.","['trigonometry', 'angle']"
4740301,"Let $s\in\mathbb{R}$ and $T: C(\mathbb{R})\to C(\mathbb{R})$ defined by $(Tu)(t)=u(t+s)$. Find the resolvent of $T$, $\rho(T)$.","I have the following problem: Let $E=C(\mathbb{R})=\{u\in\mathbb{R}^{\mathbb{R}}\mid \sup_{t\in\mathbb{R}}|u(t)|<\infty\,\land\, $ u $\text{ is continuous}\}$ be equipped with the norm $\|u\|_\infty=\sup_{t\in\mathbb{R}}|u(t)|$ . Define $T\colon E\to E$ by $(Tu)(t)=u(t+s)$ where $s$ is a real non-zero constant. Prove $T\in\mathcal{L}(E)$ and that $\|T\|=1$ . Find $\rho(T)$ (the set of all $\lambda\in\mathbb{R}$ that make $T-\lambda I$ bijective). For any $y\in \mathbb{R}$ , define $g_y\colon\mathbb{R}\to\mathbb{R}$ by $t\mapsto t+y$ . I'll adopt the notation $g_s^+=T$ . The first part is easy: Let $u,v\in E$ and $\lambda \in \mathbb{R}$ , then $g_s^+(\lambda u+v)=(\lambda u+v)\circ g_s=\lambda u\circ g_s + v\circ g_s=\lambda g_s^+(u)+g_s^+(v)$ and $\|g_s^{+} (u)\|=\|u\circ g_s\|=\sup_{t\in\mathbb{R}}|u(t+s)|=\|u\|$ and thus $g_s^+\in\mathcal{L}(E)$ and $\|g_s^+\|=1$ . I'm now trying to prove that, if $|\lambda|>1$ , then $\lambda \in \rho(g_s^+)$ (this was a hint on the exercise) but I don't really know how to show that $g_s^+-\lambda I$ is bijective for such $\lambda$ . I'm looking for a hint or a solution on how to continue.","['banach-spaces', 'spectral-theory', 'functional-analysis', 'analysis']"
4740375,How to show $\int_0^1\frac{\operatorname{Li}_2\left(\frac{1+x^2}{2}\right)}{1+x^2}dx=\ln(2)G$,"I am trying to prove that $$\int_0^1\frac{\operatorname{Li}_2\left(\frac{1+x^2}{2}\right)}{1+x^2}dx=\ln(2)G,$$ where $G$ is the Catalan constant and $\operatorname{Li}_2(x)$ is the dilogarithm function, which is proposed by my friend Sujeethan Balendran and solved in two methods by my friend Cornel Valean in his second book "" More (Almost) Impossible Integrals, Sums, and Series "" page 345. I want to try it differently using as less results and relations as possible and here is my attempt: $$\int_0^1\frac{\operatorname{Li}_2\left(\frac{1+x^2}{2}\right)}{1+x^2}dx=\int_0^1\frac{1}{1+x^2}\left(\int_0^1\frac{-\frac{1+x^2}{2}\ln(y)}{1-\frac{1+x^2}{2}y}dy\right)dx$$ $$=\int_0^1 \ln(y)\left(\int_0^1\frac{dx}{(1+x^2)y-2}\right)dy$$ $$=\int_0^1\ln(y)\left(\frac{1}{y}\sqrt{\frac{y}{y-2}}\arctan \sqrt{\frac{y}{y-2}}\right)dy\quad \text{let}
\quad\sqrt{\frac{y}{y-2}}=ix$$ $$=\int_0^1\frac{-2\operatorname{arctanh}(x)\ln\left(\frac{2x^2}{1+x^2}\right)}{1+x^2}dx$$ $$=\int_0^1\frac{\ln\left(\frac{1-x}{1+x}\right)\ln\left(\frac{2x^2}{1+x^2}\right)}{1+x^2}dx\quad \text{let}
\quad\frac{1-x}{1+x}\to x$$ $$=\int_0^1\frac{\ln(x)\ln\left(\frac{(1-x)^2}{1+x^2}\right)}{1+x^2}dx$$ $$=2\int_0^1\frac{\ln(x)\ln(1-x)}{1+x^2}dx-\int_0^1\frac{\ln(x)\ln(1+x^2)}{1+x^2}dx$$ These two integrals are given here and here respectively and combining them gives $\ln(2)G.$ Question : Is there an elegant way to prove $\displaystyle\int_0^1\frac{\ln(x)\ln\left(\frac{(1-x)^2}{1+x^2}\right)}{1+x^2}dx=\ln(2)G$ without breaking the integrand or is there a different way to evaluate $\displaystyle\int_0^1\frac{\operatorname{Li}_2\left(\frac{1+x^2}{2}\right)}{1+x^2}dx$ ? Thank you,","['integration', 'polylogarithm', 'alternative-proof', 'definite-integrals']"
4740378,Evaluation of a limit involving gamma function,"Basically I try to answer this question . I am almost able to prove it straight on, but I hit a roadblock at the very last step. Namely, the limit $$
\lim_{n\rightarrow \infty} \left( \sqrt[n+1]{\Gamma \left( n+1+\frac{1}{2} \right)}-\sqrt[n]{\Gamma \left( n+\frac{1}{2} \right)} \right) 
$$ It should evaluate to $1/e$ through numerical estimation. One possible direction here is to justify that we just plug in Stirling's formula, due to $$
\sqrt[n]{\Gamma \left( n+\frac{1}{2} \right)}\sim \frac{n}{e}
$$ we have $$
\lim_{n\rightarrow \infty} \left( \sqrt[n+1]{\Gamma \left( n+1+\frac{1}{2} \right)}-\sqrt[n]{\Gamma \left( n+\frac{1}{2} \right)} \right) =\lim_{n\rightarrow \infty} \left( \frac{n+1}{e}-\frac{n}{e} \right) =\frac{1}{e}
$$","['gamma-function', 'limits', 'calculus', 'analysis']"
4740402,Unfamiliar form of coupled ODEs,"I've recently come across a system of coupled ODE's: $$x_2''-x_1''=A-Bx_2$$ $$x_1''-x_2''=C-Dx_1$$ where $x_1, x_2$ are functions of only $t$ . I am sort of stumped. Just by adding these equations together, one gets: $0 = A+C-Bx_2-Dx_1$ which appears to imply, at least to me, that any function will work given that, $x_1 = \frac{A+C-Bx_2}{D}$ However, this is not true when I enter a possible pair of functions, $x_1 = \frac{A+C-Bt^2}{D}$ $x_2 = t^2$ This clearly doesn't work as $x_1''$ and $x_2''$ both wouldn't have dependence on $t$ which implies my assumption that any equations would work as long as they constrained to the $0 = A+C-Bx_2-Dx_1$ equation is false. How do I go about solving this coupled ODE?",['ordinary-differential-equations']
4740410,How can I solve this second order differential equation?,"So, I have to solve the following system $$\frac{d}{dt}x(t) = x(t)y(t)$$ $$\frac{d}{dt}y(t) = x(t)$$ I can just substitute $x(t)$ in the first equation and get $$\frac{d^2}{dt^2}y(t) = y(t)\frac{dy(t)}{dt}$$ but I can't go further. I also tried to use python but gave me a weird function. How can I go on and solve this equation? Edit: I tried integrating both sides and got $$\frac{dy}{dt} = \frac{y(t)^2}{2} + c_1$$ and then integrated again but got the wrong answer.","['calculus', 'ordinary-differential-equations']"
4740420,Finding $ \lim _{n \rightarrow \infty}\left(\int_0^{\frac{\pi}{2}} x \frac{\sin ^4(n x)}{\sin ^4 x} d x-\ln 2 \cdot n^2-\frac{1}{4} \ln n\right)$,"By numerical estimation, I get $$
\lim _{n \rightarrow \infty}\left(\int_0^{\frac{\pi}{2}} x \frac{\sin ^4(n x)}{\sin ^4 x} d x-\ln 2 \cdot n^2-\frac{1}{4} \ln n\right)=\frac{6 \gamma+4 \ln 2+5}{24} .
$$ But I can't prove it yet. I made some attempts： (1)  We can get $$
\lim _{y \rightarrow+\infty}\left(\int_0^y \frac{\sin ^4 x}{x} d x-\frac{3}{8} \ln y\right)=\frac{3 \gamma+2 \ln 2}{8} .
$$ And for sufficiently small $c>0$ ,there is $$
\frac{x}{\sin ^4 x}-\frac{1}{x^3}-\frac{2}{3 x} \leqslant 1, x \in[0, c] .
$$ (2)According to the Riemann Lemma, we have $$
\lim _{n \rightarrow \infty} \int_c^{\frac{\pi}{2}} x \frac{\sin ^4(n x)}{\sin ^4 x} d x=\frac{3}{8} \int_c^{\frac{\pi}{2}} \frac{x}{\sin ^4 x} d x.
$$ (3) By (1), we have \begin{aligned}
\int_0^c x \frac{\sin ^4(n x)}{\sin ^4 x} d x & \leqslant \int_0^c \frac{\sin ^4(n x)}{x^3} d x+\frac{2}{3} \int_0^c \frac{\sin ^4(n x)}{x} d x+\int_0^c \sin ^4(n x) d x \\
& \leqslant n^2 \int_0^{n c} \frac{\sin ^4(x)}{x^3} d x+\frac{2}{3} \int_0^{n c} \frac{\sin ^4(x)}{x} d x+c \\
& =n^2\left(\ln 2-\int_{n c}^{\infty} \frac{\sin ^4(x)}{x^3} d x\right)+\frac{2}{3}\left(\frac{3}{8} \ln (n c)+\frac{3 \gamma+2 \ln 2}{8}\right)+c+o(1) \\
& =\ln 2 \cdot n^2-\int_c^{\infty} \frac{\sin ^4(n x)}{x^3} d x+\left(\frac{1}{4} \ln (n c)+\frac{\gamma+\frac{2 \ln 2}{3}}{4}\right)+c+o(1) 
\end{aligned} i.e. $$
\int_0^c x \frac{\sin ^4(n x)}{\sin ^4 x} d x  \leqslant\ln 2 \cdot n^2-\int_c^{\infty} \frac{\sin ^4(n x)}{x^3} d x+\left[\frac{1}{4} (\ln n +\ln c)+\frac{\gamma+\frac{2 \ln 2}{3}}{4}\right]+c+o(1)
$$ (4) By (2)(3), we have $$
\varlimsup_{n \rightarrow \infty}\left(\int_0^{\frac{\pi}{2}} x \frac{\sin ^4(n x)}{\sin ^4 x} d x-\ln 2 \cdot n^2-\frac{1}{4} \ln n\right)\leqslant \varlimsup_{n \rightarrow \infty}  (\frac{3}{8} \int_c^{\frac{\pi}{2}} \frac{x}{\sin ^4 x} d x-\int_c^{\infty} \frac{\sin ^4(n x)}{x^3} d x+\frac{\ln c}{4}+\frac{\gamma+\frac{2 \ln 2}{3}}{4}+c+o(1)).
$$ I want to prove that $$
\varlimsup_{n \rightarrow \infty}(\frac{3}{8} \int_c^{\frac{\pi}{2}} \frac{x}{\sin ^4 x} d x-\int_c^{\infty} \frac{\sin ^4(n x)}{x^3} d x+\frac{\ln c}{4})= \frac{5}{24}
,\ c\to 0,$$ and $$
\varliminf_{n \rightarrow \infty}\left(\int_0^{\frac{\pi}{2}} x \frac{\sin ^4(n x)}{\sin ^4 x} d x-\ln 2 \cdot n^2-\frac{1}{4} \ln n\right) \geqslant \frac{\gamma+\frac{2 \ln 2}{3}}{4}+\frac{5}{24}.
$$ But I haven't found a way to prove it yet. Can someone give some hints? thanks","['integration', 'limits']"
4740421,The sum of the series by derivatives,"We have a series $$\sum_{k=3}^\infty k \frac{\bigl(\frac{1+\sqrt5}{2}\big)^{k-1}}{2^{k-1}}$$ and I want to compute it like that. Let's say that $a = \bigl(\frac{1+\sqrt5}{2}\bigr)^{k-1}$ . Then we have $$\sum_{k=3}^\infty k \frac{a^{k-1}}{2^{k-1}} = \sum_{k=3}^\infty k (\frac{a}{2})^{k-1}$$ I tried to rewrite it as a derivative: $$\begin{align}
\sum_{k=3}^\infty k \Bigl(\frac{a}{2}\Bigr)^{k-1}
&= \sum_{k=3}^\infty \frac{d(\frac{a}{2})^{k}}{da} \\
&= \frac{d\frac{(\frac{a}{2})^{3}}{(1 - \frac{a}{2})}}{da} \\
&= \frac{3 * (\frac{a}{2})^2 * (1 - \frac{a}{2}) + (\frac{a}{2})^3 * \frac{1}{2}}{(1 - \frac{a}{2})^{2}}
\end{align}$$ But the answer is wrong. What did I do incorrectly?","['integration', 'calculus', 'derivatives', 'sequences-and-series']"
4740425,Check if there is a knot between piecewise linear line connecting finite number of points,"I have a finite number of ordered points connected by a piecewise linear curve, which connects all the points in order and then the last with the first one. Can I check if there is a “knot” in this piecewise linear curve?","['knot-theory', 'general-topology', 'geometry']"
4740454,Is there a standard deviation of a continuous function?,"Suppose we are given a continuous function $f \colon [a, b] \to \mathbb{R}$ . The mean value of $f$ on $[a, b]$ is defined as $$
\mu = \frac{\displaystyle \int_a^b f(x) \; \mathrm{d}x}{b - a}.
$$ Question: is there any way to define a continuous standard deviation of $f$ on $[a, b]$ ?
For example, if $f$ is constant on $[a, b]$ , the continuous standard deviation should be zero.","['integration', 'calculus', 'functions', 'standard-deviation']"
4740461,Asymptotic formula/closed form of $\sum_{r=0}^{n}\frac{(-1)^r\binom{n}{r}\log(n+r+1) }{n+r+1}$,"I need an asymptotic formula/closed form for the sum $$\sum_{r=0}^{n}\frac{(-1)^r\binom{n}{r}\log(n+r+1) }{n+r+1}$$ where $n\in\mathbb{N}$ Denote $$S_n=\sum_{r=0}^{n}\frac{(-1)^r\binom{n}{r}\log(n+r+1) }{n+r+1}$$ $$S_n=\sum_{r=0}^{n}\frac{(-1)^r\binom{n}{r}\log(n+1) }{n+r+1}+ \sum_{r=0}^{n}\frac{(-1)^r\binom{n}{r}\log(1+\frac{r}{n+1}) }{n+r+1} $$ By Wolfram alpha see here $$\sum_{r=0}^{n}\frac{(-1)^r\binom{n}{r}\log(n+1) }{n+r+1}= \frac{\sqrt{\pi}\ n!\ \log (n+1)}{2^{2n+1} \Gamma(n+\frac{3}{2})} $$ $$S_n= \frac{\sqrt{\pi}\ n!\ \log (n+1)}{2^{2n+1} \Gamma(n+\frac{3}{2})}   + \sum_{r=0}^{n}\frac{(-1)^r\binom{n}{r}\log(1+\frac{r}{n+1}) }{n+r+1} $$ Since $\frac{r}{n+1}<1$ so by expansion of logarithm, $$S_n= \frac{\sqrt{\pi}\ n!\ \log (n+1)}{2^{2n+1} \Gamma(n+\frac{3}{2})}   + \sum_{r=0}^{n}\sum_{m=0}^\infty\frac{(-1)^r\binom{n}{r}(-1)^{m+1}(\frac{r}{n+1})^m }{m(n+r+1)} $$ I could not think of an idea to solve the problem. Any help will be highly appreciated.","['summation', 'logarithms', 'real-analysis', 'binomial-coefficients', 'sequences-and-series']"
4740482,Is $(J_{\lambda_n})_n$ convergent in $\mathcal L(H)$?,"Let $(H, \langle \cdot, \cdot \rangle)$ be a real Hilbert space and $|\cdot|$ its induced norm. Let $A: D(A) \subset H \to H$ be a maximal monotone (unbounded linear) operator. Let $I:H \to H$ be the identity map. For every $\lambda>0$ , set $$
J_\lambda=(I+\lambda A)^{-1} \quad \text { and } \quad A_\lambda=\frac{1}{\lambda}\left(I-J_\lambda\right) ;
$$ $J_\lambda$ is called the resolvent of $A$ , and $A_\lambda$ is the Yosida approximation (or regularization ) of $A$ . Then we have $\left\|J_\lambda\right\|_{\mathcal{L}(H)} \leq 1$ for $\lambda>0$ . $\lim_{\lambda \to 0^+} J_\lambda v = v$ for all $v \in H$ . If $\lambda_n, \lambda >0$ such that $\lambda_n \to \lambda$ , then $\|J_{\lambda_n} - J_\lambda\|_{\mathcal L(H)} \to 0$ . On the space $\mathcal L(H)$ , we have strong operator topology (SOT), norm operator topology (NOT), and weak operator topology (WOT). We have SOT is stronger than WOT and weaker than NOT. Now let $\lambda_n >0$ such that $\lambda_n \to 0$ . Does $(J_{\lambda_n})_n$ converge in any of those topologies of $\mathcal L(H)$ as $n \to \infty$ ? If yes, what is its limit? Thank you so much for your elaboration!","['hilbert-spaces', 'operator-theory', 'monotone-operator-theory', 'functional-analysis']"
4740493,Being a member of a set in itself [duplicate],"This question already has answers here : How can a set contain itself? (8 answers) Closed 11 months ago . Hello to all math lovers. I was studying Russell's paradox in set theory and came across something ambiguous that I couldn't justify no matter how hard I tried. The strange thing was: sets that are members of themselves and sets that are not members of themselves. This statement threw me thousands of meters away from the paradox itself. What does Russell mean by this? We know that when something is a part of a set, we say it is a member of that set. But how can a set be a part of itself or not? What unique characteristic makes a set a member of itself or not? I don't understand the concept of being a member of a set in itself. I would appreciate your guidance. Thank you, a math enthusiast :)","['elementary-set-theory', 'paradoxes']"
4740503,Combinatorics - Number of arrangements in a circle,"Let $x_1, x_2, \dots, x_8$ be some distinct people. In how many different ways can we arrange all of them in a circle such that $x_1$ sits in front of $x_2$ and, $x_3$ doesn't sit near $x_4$ ? I am not sure what I'm doing wrong: We can choose out of $x_3, x_4, \dots, x_8$ a subset of 3, let's say $x_3, x_4, x_5$ as those who are placed between $x_1$ and $x_2$ at the right side.
Since each subset defines an arrangement for the left side, we divide by two. Then for each subset, we compute each internal arrangement for the elements in the left side and so for the right side, hence, we end up with: ${6 \choose 3} \cdot 3! \cdot 3!$ Secondly, I compute the number of such arrangements where $x_3$ sits near $x_4$ : I have two options where I want this pair to sit: near $x_1$ or near $x_2$ . Suppose I chose $x_1$ then I have 2 options to choose which one out of $x_3, x_4$ I want to place near $x_1$ . Finally, there are 4 options to place an element near $x_3, x_4$ and taking internal arrangements for the other 3 elements left get me to $2\cdot2\cdot4\cdot3!$ . Now we subtract it from the total number of arrangements. However it seems like the answer is incorrect and I'm curious to know what I've missed. I tried to use analog as the number of circles in a simple graph $G$ which satisfies the given constraints, to which I presumed, would give me intuition but just made it more vague.","['graph-theory', 'combinatorics', 'discrete-mathematics', 'permutations']"
4740569,What value of δ should I pick?,"Given $f(x)=\frac{1}{x-1}$ , find $δ$ , such as if $0<|x-2|<δ$ , then $|f(x)-1|<0.01$ . Original problem is below picture: From figure or formula $f(x) = \frac{1}{x-1}$ we can get $f(2) = \frac{1}{2-1} = 1$ . Because we need to guarantee $|f(x)-1|<0.01$ , now we have know $f(2) = 1$ , so we can get $0.99 < f(x) < 1.01$ when x approach 2, namely $0.99 < f(x) = \frac{1}{x-1} < 1.01$ when x approach 2. Convert a decimal into a fraction, we can follow these steps: $\frac{99}{100} < \frac{1}{x-1} < \frac{101}{100}$ , so we can get two  inequality: $\frac{99}{100} < \frac{1}{x-1}$ and $\frac{1}{x-1} < \frac{101}{100}$ . Solve the first inequality $\frac{99}{100} < \frac{1}{x-1}$ , multiply both sides of an inequality by $\frac{100}{99}$ , we can get $\frac{99}{100}\cdot\frac{100}{99} < \frac{1}{x-1}\cdot\frac{100}{99}$ , so we get $1 < \frac{1}{x-1}\cdot\frac{100}{99}$ , namely $1 < \frac{100}{99\cdot{(x-1)}}$ , so we can get $100 > 99\cdot{(x-1)}$ . Now we can get $100 > 99\cdot{x} - 99$ , namely $199 > 99\cdot{x}$ , solve $x < \frac{199}{99}$ . Solve the second inequality $\frac{1}{x-1} < \frac{101}{100}$ by above approach that we can solve $x > \frac{201}{101}$ . The final range of values for x is $\frac{201}{101} < x < \frac{199}{99}$ . Because we need to find δ that it is related to $0<|x-2|<δ$ . so $\frac{201}{101} -2  < x-2 < \frac{199}{99} -2$ . Taking the absolute value of the inequality yields $|\frac{201}{101} -2| < |x-2| < |\frac{199}{99} -2|$ , namely $|\frac{201-202}{101}| < |x-2| < |\frac{199-198}{99}|$ , so we get $|\frac{-1}{101}| < |x-2| < |\frac{1}{99}|$ or $|\frac{1}{101}| < |x-2| < |\frac{1}{99}|$ . How should I proceed? Take δ = $\frac{1}{99}$ ?","['limits', 'calculus']"
4740575,Function on the real plane that can be expressed in terms of a function on the real line,"Let $f:\mathbb{R}^2\to\mathbb{R}$ such that $f(x,y)+f(y,z)+f(z,x)=0 \text{ } \forall \text{ } x,y,z \in \mathbb{R}.$ Show that there exists a $g:\mathbb{R}\to \mathbb{R}$ such that $f(x,y)=g(x)-g(y).$ This was question $A1$ on the $2008$ Putnam. My attempt is given below. Put in $x=y=z$ to get $3f(x,x)=0.$ This means that $f(x,x)=0$ for all real $x.$ Now, put in $y=z$ to get $$f(x,y)+f(y,y)+f(y,x)=0=f(x,y)+f(y,x).$$ So, $f(x,y)=-f(y,x)$ for all real $x,y.$ In particular, $f(x,0)=-f(0,x).$ Now, let $g(x):=f(x,0).$ This definition makes sense as it's defined on $\mathbb{R}$ and gives out an output that's in $\mathbb{R}.$ Note that $f(x,y)+f(y,0)+f(0,x)=0.$ This gives $f(x,y)=-f(0,x)-f(y,0)=f(x,0)-f(y,0)=g(x)-g(y).$ But, $x$ and $y$ are arbitrary, so this identity holds for all real $x$ and $y,$ as desired. Is this proof fine? I'm skeptical since my proof seems too simple to be the correct answer to a Putnam question. Another thing that troubles me is that we could've set $g(x):=f(x,a)$ for some real $a$ and gotten the same answer. Does this mean there are infinitely many such $g?$","['contest-math', 'functional-equations', 'functions', 'solution-verification']"
4740577,"If $f(x)=0,\,g(x)=\sin(x)\,$ and $h(x)=e^x$ is setting $c_1=k\neq0, c_2=0$ and $c_3=0$ a non trivial linear combination?","Given these $3$ functions, $f(x)=0,\;g(x)=\sin(x)$ and $h(x)=e^x\,,$ I have to find a non-trivial linear combination, that is, three constants that makes the combination of this functions equal to $0$ . $$
c_1\cdot{0}+c_2\cdot{\sin(x)+c_3\cdot{e^x}}=0
$$ The doubt here is with the definition of non-trivial linear combination, which as I understand, means that not all the constants are $0$ . That means that in this example I can put $c_1=1, c_2=0$ and $c_3=0$ but is this a non-trivial linear combination? Thanks.","['linear-algebra', 'ordinary-differential-equations']"
4740596,adjoint operator of divergence operator with respect to $L^2$-norm,"I'm reading a book named The Ricci Flow in Riemannian Geometry . In this book, the author defined a divergence operator: $$
\begin{aligned}
\delta_g: &\Gamma(\operatorname{Sym}^2T^*M)\mapsto \Gamma(T^*M)\\
(\delta_gh)_k&=-g^{ij}\nabla_ih_{jk}.
\end{aligned}
$$ where $g$ is the Riemannian metric. Suppose $M$ is compact without boundary. Then they claimed that the adjoint of $\delta_g$ with respect to $L^2$ -norm is $$
\begin{aligned}
\delta^*_g: &\Gamma(T^*M)\mapsto \Gamma(\operatorname{Sym}^2T^*M)\\
(\delta^*_g\omega)_{jk}&=\frac{1}{2}(\nabla_j\omega_k+\nabla_k\omega_j)=\frac{1}{2}(\mathscr{L}_{\omega^{\sharp}}g)_{jk}.
\end{aligned}
$$ I try to prove this by verifying $\int_M(\delta_gh)^k\omega_k dvol_g=\int_Mh^{jk}(\delta^*\omega)_{jk}dvol_g$ . I try to rewrite the expression under the intergal sign as a divergence of a vecor field then use Stoke's formula. But I failed. Can anyone help to explain the calculation? Thanks in advance.","['riemannian-geometry', 'differential-geometry']"
4740601,Finite stopping time,"Let $(X_k)_{k \in \mathbb{N}}$ be iid uniformly distributed on $(-1,1)$ , $Y_0:=0$ and $$
Y_n := \sum_{k=0}^n \sin(\pi X_k).
$$ We deine another random variable by $Z_K := \inf \lbrace n \in \mathbb{N}: \lvert Y_k \rvert \geq K \rbrace$ . Here, $K>1$ . I am interested in whether $Z_k$ is finite a.s.. I think that it is, since the series $\sum_{k=0}^n \sin(\pi X_k)$ diverges if $X_k \not \rightarrow 0$ . But since the $X_k$ are iid uniformly distributed, this does not hold true.
But this is not very rigorous. Ist this outline even correct and ist there an elegant proof for this? Thank you!","['stochastic-processes', 'martingales', 'probability-theory', 'probability']"
4740626,Confused about Georgiev's definition of $C^{\infty}_c$,"I'm beginning to study Distributions, and I've encountered the following definition in Georgiev's Theory of Distributions : Such definition implies that $C^{\infty}_c$ (with the opology given by the norm) is a normed space; however, to my understanding $C^{\infty}_c$ (with the canonical $LF$ -topology ) is not metrizable, and thus not normable. Why is Georgiev giving $C^{\infty}_c$ a different topology? Will the distributions defined in Georgiev's given topology match distributions as defined in, say, Rudin's functional analysis?","['analysis', 'distribution-theory', 'definition', 'functional-analysis', 'general-topology']"
4740640,On a proof of the non-differentiability of the Blancmange curve,"In my nameless lecture notes, in the construction of a continuous, nowhere differentiable function (the Blancmange curve), we encounter the definition of the sequential derivative and other real-analysis concepts. I have some question about these concepts. Here's a brief background: Part 1: the Blancmange curve Let $$f_1(x)=\begin{cases}
x\qquad \ \text{when } 0\leq x< 1/2,\\
1-x \  \ \text{when } 1/2\leq x< 1.
\end{cases}$$ and define $f_1(x)$ for other values of $x$ so that it becomes periodic with period $1$ . Then we let $$f_2(x)=\frac12 f_1(2x).$$ This is a scaling of $f_1$ ; we get a function with half the period and half the amplitude. In general $$f_{k+1}(x)=2^{-k}f_1(2^kx)\quad \text{for all }x\in\mathbb{R},\quad k=1,2,\ldots.$$ Now sum up all these functions and let $$T(x)=\sum_{k=1}^\infty f_k(x).$$ Since $0\leq f_k(x)\leq 2^{-k}$ , the convergence is uniform according to the Weierstrass $M$ -test and hence $T(x)$ is continuous. $T(x)$ (from Wikipedia)"" /> To understand the next part, here's a lemma from Abbott's Understanding Analysis (second edition), Lemma. Let $f$ be defined on an open interval $J$ and assume that $f$ is differentiable at some $a\in J$ . If $(a_n)$ and $(b_n)$ are sequences satisfying $a_n<a<b_n$ and $\lim a_n = a = \lim b_n$ , then $$f'(a) =\lim_{n\rightarrow\infty}\frac{f(b_n)-f(a_n)}{b_n-a_n}.$$ Part 2: the sequential derivative We shall show that $T(x)$ is not differentiable at any arbitrary point $c$ . For each integer $n$ , we let $m$ be the integer that satisfies $$\frac{m}{2^n}\leq c <\frac{m+1}{2^n},\tag{1}$$ and denote by $I_n$ the interval $[m2^{-n},(m+1)2^{-n}]$ . The length of this interval is $2^{-n}$ which tends to zero as $n\to\infty$ , and the interval contains $c$ . Hence if $T(x)$ is differentiable at $c$ , then the difference quotient $$d_n=\frac{T((m+1)2^{-n})-T(m2^{-n})}{2^{-n}}$$ equals $T'(c)+o(2^{-n})$ which has the limit $T'(c)$ as $n\to\infty$ . Note that my lecture notes have a non-strict inequality, i.e. $\frac{m}{2^n}\color{red}{\leq}c<\frac{m+1}{2^n}$ and I think this is problematic in the proof of the lemma provided here , since we get division by zero if $a_n=a$ , i.e. if $\frac{m}{2^n}=c$ . On the other hand, consider $a=c=\frac14$ and $n=2$ , then there is no integer $m$ satisfying $\frac{m}{4}<\frac14<\frac{m+1}{4}$ . So it seems like we need the non-strict inequality after all ( $m=1$ would do it if the first inequality was a non-strict inequality). This confuses me and I'd be grateful for a comment or two on this. Moreover I think $o(2^{-n})$ should be $o(1)$ (as suggested by this answer ). Part 3: proof We shall prove that $d_n$ has no limit. Consider the corresponding difference quotient for $f_k$ . If $k>n$ , then $f_k$ equals zero at both endpoints of $I_n$ , and then the difference quotient equals $0$ . This cannot happen if $k\leq n$ , because then the difference quotient equals $1$ or $-1$ , since $f_k$ is made up of line segments having these gradients. The sign depends only on the gradient of $f_k$ at the point $a$ . The difference quotient $d_n$ is equal to the sum of these numbers. The difference $d_n-d_{n-1}$ is a difference between two sums the terms of which are equal in pairs except the $n$ th terms. Hence $$d_n-d_{n-1}=\pm 1.$$ This is valid for all $n$ , and therefor the sequence $(d_n)$ has no limit. By ""...corresponding difference quotient for $f_k$ "" I assume they
mean $$\frac{f_k((m+1)2^{-n})-f_k(m2^{-n})}{2^{-n}}.\tag{2}$$ Since $f_{k+1}(x)=2^{-k}f_1(2^kx)$ , the above fraction simplifies to $$\frac{2^{-k+1}f_1(2^{k-1}(m+1)2^{-n})-2^{-k+1}f_1(2^{k-1}m2^{-n})}{2^{-n}}=2^{-p}\left(f_1(2^{p}(m+1))-f_1(2^{p}m)\right),$$ where $p=k-1-n$ . When $k>n$ , then $p\geq 0$ and then $2^{p}(m+1)$ and $2^{p}m$ are integers where $f_1$ is $0$ . I have a hard time
convincing myself of that $(2)$ equals $-1$ or $1$ when $k\leq n$ .
Then $p<0$ , and $2^{p}(m+1)$ and $2^{p}m$ are rational numbers, but what is $f_1$ then? Finally, and probably related to the previous question, I  do not understand why $d_n-d_{n-1}=\pm 1$ . Since $T(x)$ is uniformly convergent on $\mathbb{R}$ and the
difference quotient of $f_k$ equals $0$ for $k>n$ , we have $$\begin{align}
d_n-d_{n-1}&=\frac{T((m+1)2^{-n})-T(m2^{-n})}{2^{-n}}-\frac{T((m+1)2^{-n+1})-T(m2^{-n+1})}{2^{-n+1}}
\tag{3} \\ &= \frac{\sum_{k=1}^\infty f_k((m+1)2^{-n})-\sum_{k=1}^\infty f_k(m2^{-n})}{2^{-n}}-\frac{\sum_{k=1}^\infty f_k((m+1)2^{-n+1})-\sum_{k=1}^\infty f_k(m2^{-n+1})}{2^{-n+1}} \tag{4}\\ &=\sum_{k=1}^n \frac{
f_k((m+1)2^{-n})-f_k(m2^{-n})}{2^{-n}}-\sum_{k=1}^{n-1} \frac{
f_k((m+1)2^{-n+1})-f_k(m2^{-n+1})}{2^{-n+1}}, \tag{5} \end{align}$$ however, I do not see from this why the sums have equal terms up to
the $n$ th term.","['proof-explanation', 'real-analysis', 'continuity', 'uniform-convergence', 'derivatives']"
4740650,Prove that the equalities $A(B\cap C)=AB\cap AC$ and $(A\cap B)C=AC\cap BC$ do not generally hold.,"If $*$ is a binary operation on a set $X$ then it is custom to define $$
\tag{1}\label{1}A\star B:=\{x\in X:x=a*b\text{ with } (a,b)\in A\times B\}
$$ for any $A,B\in\mathcal P(X)$ . So my algebra text ask as exercise to prove that the equality $$
A\star(B\cup C)=(A\star B)\cup(A\star C)
$$ and equality $$
(A\cup B)\star C=(A\star C)\cup(B\star C)
$$ holds
but it seemed to me that actually a more general equalities holds: indeed, it seem to me that if $\mathcal A$ is collection in $X$ then for any $Y$ in $\mathcal P(X)$ the equality $$
\tag{2}\label{2}Y\star\left(\bigcup_{A\in\mathcal A}A\right)=\bigcup_{A\in\mathcal A}(Y\star A)
$$ and the equality $$
\tag{3}\label{3}\left(\bigcup_{A\in\mathcal A}A\right)\star Y=\bigcup_{A\in\mathcal A}(A\star Y)
$$ hold and I even tried to prove it as to follow. So if $x$ is in $Y\star\left(\bigcup_{A\in\mathcal A} A\right)$ then there exist $y$ in $Y$ and $a$ in $\bigcup_{A\in\mathcal A}A$ such that $$
x=y*a
$$ but by axiom of union there exists $A$ in $\mathcal A$ containing $a$ so that $x$ is in $Y\star A$ and thus the inclusion $$
\tag{4}\label{4}Y\star\left(\bigcup_{A\in\mathcal A}A\right)\subseteq\bigcup_{A\in\mathcal A}(Y\star A)
$$ holds. Analogously if $x$ is in $\bigcup_{A\in\mathcal A}(Y\star A)$ then there exists $A$ in $\mathcal A$ such that $x$ is in $Y\star A$ and so there exist $a$ in $A$ and $y$ in $Y$ such that the equality $$
\tag{5}\label{5}x=y*a
$$ holds: however, $a$ is obviously in $\bigcup_{A\in\mathcal A}A$ so that by eq. \eqref{5} $x$ is in $Y\star\left(\bigcup_{A\in\mathcal A}A\right)$ and thus even the inclusion $$
\tag{6}\label{6}\bigcup_{A\in\mathcal A}(Y\star A)
$$ holds. Finally, by incl. \eqref{4} and incl. \eqref{6} we conclude that eq. \eqref{2} holds and moreover by analogous arguments it is possible to prove that even \eqref{5} holds. So we conclude that $\star$ is surely distributive over union but I am wondering to know if $\star$ is distributive over intersection since I tried to prove it but I failed so that afer some time I started to suspect it is not: so first of all I ask if \eqref{2} and \eqref{3} hold since I found them by myself and so I want be sure I did not make a blunder; second I ask to prove or disprove (right and left) distributivity of $\star$ over intersection. So could someone help me, please?","['elementary-set-theory', 'fake-proofs', 'solution-verification', 'examples-counterexamples']"
4740657,Help With Fundamental Theorem of Line Integrals,"I am achieving the wrong answer, but I am unsure where I went wrong about it.
I am given a conservative vector field and the line $C$ : $$F = \langle \frac{4x}{y^2+1}, -\frac{4y(x^2+1)}{(y^2+1)^2} \rangle, C \textrm{ is parameterized by } x=t^3-1, y=t^6-t, 0 \le t \le 1  .$$ Since I know it is conservative, I find the potential function $f$ : $$\frac{4x^2+2}{y^2+1} + c$$ Then I just apply FTLI, $f(x(1), y(1)) - f(x(0), y(0))$ by recognizing $t$ is in between $0$ and $1$ therefore its initial point must be $(-1,0)$ and its terminal point $(0,0)$ . So I get $2 - 6 = -4$ , but $-4$ is deemed wrong. I am not sure when I went wrong in this, I double checked my integrals for $P$ and $Q$ when getting the potential function, did I go wrong in determining the initial and terminal points?","['multivariable-calculus', 'line-integrals']"
4740672,Can (a certain interpretation of) the Ross-Littlewood paradox be formalized in set theory?,"In the Ross-Littlewood paradox , there is a supertask that goes as follows: At step 1, you add 10 balls to a jar labeled 1 through 10 and remove ball #1. At step 2, you add 10 balls to a jar labeled 11 through 20 and remove ball #2. At step 3, you add 10 balls to a jar labeled 21 through 30 and remove ball #3. This process goes on inductively, with each step taking half the time of the previous step (so that all this takes place in finite time). The question is, how many balls are left in the jar after the supertask is done? Based on the literature, it seems there are two sensible conclusions: either you can say the scenario is not physically sensible and thus there is no answer, or you can say there are precisely zero balls left. Now, my question is not about the paradox , so please leave the debate about it aside. Under some suitable interpretation , we can try to think about the process of adding and removing balls in terms of set theory with the final result being zero balls left. When I try to think about this, I am thinking of defining the sets $$ S_{1} = (\varnothing\cup\{1, \ldots, 10\})\setminus\{1\}, \qquad S_{2} = (S_{1}\cup\{11, \ldots, 20\})\setminus\{2\}, \qquad S_{3} = (S_{2}\cup\{21, \ldots, 30\})\setminus\{3\}, \qquad \ldots. $$ In general, $$ S_{n+1} = (S_{n}\cup\{10n+1, \ldots, 10(n+1)\})\setminus\{n+1\}. $$ We can either think of this as an infinite sequence of sets $S_{1}, S_{2}, \ldots$ or as an infinite sequence of operations (union, setminus, union, setminus, etc.) done to sets. Either way, it seems like there ought to be some sort of ""set limit"" which would be the empty set $\{\}$ . Is there any way in which this can be formalized? I am aware there are cases where no ""set limit"" could be assigned. For example, if we define $T_{1} = \{5\}$ , $T_{2} = \varnothing$ , $T_{3} = \{5\}$ , $T_{4} = \varnothing, \ldots$ , then I think there likely can't be any ""set limit"" assigned. However, I don't think this spoils the original question, because there are plenty of sums in real analysis that don't have any result such as $\sum_{n\ge 0} (-1)^{n}$ . The fact that the result of this sum is undefined doesn't mean other sums can't have a well-defined result. Indeed, we can formalize infinite sums by limits (under the epsilon-delta definition of limits) perfectly well and we can even tell which series converge and which don't. Is there anything analogous to this for sets?","['elementary-set-theory', 'recreational-mathematics', 'paradoxes', 'sequences-and-series']"
4740685,"Proving that this relation implies another relation on the Coxeter group [4,3,3,4].","I have a group with five generators $\sigma_i$ , and the following relations: \begin{split}
\sigma_i^2 = \varepsilon \\
|i-j| \neq 1 \implies (\sigma_i\sigma_j)^2 = \varepsilon \\
(\sigma_0\sigma_1)^4 = \varepsilon \\
(\sigma_1\sigma_2)^3 = \varepsilon \\
(\sigma_2\sigma_3)^3 = \varepsilon \\
(\sigma_3\sigma_4)^4 = \varepsilon \\
(\sigma_0\sigma_1\sigma_2\sigma_3\sigma_4\sigma_3\sigma_2\sigma_1)^n = \varepsilon \\
\end{split} Note that without the last relation this is the Coxeter group $[4,3,3,4]$ . So we can think of this as the Coxeter group $[4,3,3,4]$ with an extra relation. Equivalently this is $I_2(n)\wr S_4$ . The task is to prove: $$
(\sigma_0\sigma_4\sigma_1\sigma_3\sigma_2\sigma_1\sigma_3)^{2n} = \varepsilon
$$ or equivalently, that the group $\left\langle\sigma_0\sigma_4,\sigma_1\sigma_3,\sigma_2\right\rangle$ is isomorphic to $I_2(n)\wr S_3$ . Why do I believe this could be true? I've use the GAP system to confirm that this is true for $n < 16$ . While always finite some of these groups begin to get very large, so I think it's very likely this holds for all $n$ . What have I done so far? Besides confirming it holds for cases, I've tried a couple of approaches. The first was to simply hope that $\sigma_0\sigma_1\sigma_2\sigma_3\sigma_4\sigma_3\sigma_2\sigma_1 = (\sigma_0\sigma_4\sigma_1\sigma_3\sigma_2\sigma_1\sigma_3)^2$ as if I could prove that, it would solve the problem. It turns out that is not the case. Next I hoped that $\sigma_0\sigma_1\sigma_2\sigma_3\sigma_4\sigma_3\sigma_2\sigma_1$ and $(\sigma_0\sigma_4\sigma_1\sigma_3\sigma_2\sigma_1\sigma_3)^2$ were conjugates in $[4,3,3,4]$ , since conjugates have the same order in a group. GAP can solve for this and it found they were in fact not conjugates. Jykri Lahtonen pointed out that outer automorphisms preserve order, so I could in addition checking the two values are in the same conjugacy class I could check if applying the dual automorphism to one of the values makes it the conjugate of the other. So if I could show $\sigma_4\sigma_3\sigma_2\sigma_1\sigma_0\sigma_1\sigma_2\sigma_3$ and $(\sigma_0\sigma_4\sigma_1\sigma_3\sigma_2\sigma_1\sigma_3)^2$ are conjugates this would also prove the result. However GAP tells me these are not conjugates either. Where does this problem come from? This problem sort of looks like random nonsense. Just something made from bashing together random generators until something stuck. However it is part of a larger problem which I will try to briefly justify here. The Coxeter group $[4,3,3,4]$ corresponds to the symmetry of the 5-dimensional hypercubic honeycomb, and the additional relation that we give gives a subsymmetry of the 4-torus in 8-dimensional Euclidean space. The 4-torus being a quotient of 4-space. The relation I am aiming to prove shows that this symmetry has a nice relationship to similar symmetries of the 2-torus in 4-space. This relationship is useful to me for proving that certain types of polyhedral embeddings exist in 8-space.","['combinatorial-group-theory', 'group-theory', 'coxeter-groups']"
4740703,"Minimum swaps to put an array into desired order, where some elements are identical/repeated","Inspired by a word game Waffle , see footnotes if interested.  The abstracted problem: You're given an input array of letters, some of which might be identical (i.e. repeated), e.g. GSAAKD .  You are also given a desired outcome, which is a second array containing the same multiset of letters, e.g. ASKGAD .  The challenge is to make the minimum no. of swaps to turn the first array into the second array. In this example: The shortest path is gSAaKD -> ASaGkD -> ASKGAD of $2$ total swaps. The first letter in the desired outcome is A , but it makes a difference which A you move there.  E.g. if your first move is gSaADK -> ASGAKD then you cannot finish in $1$ more swap ( $2$ total swaps). Related / special case: If the letters are all distinct, then this is the well known problem of minimum no. of swaps to turn one permutation into another.  Can someone confirm what I remember about the solution in this case? The min. no. of swaps $= N -$ no. of cycles A greedy algorithm where each step you put one letter into its correct position achieves the minimum. Questions: Bounty will be awarded if you answer either Q1 or Q2.  If one person answers Q1 and another person answers Q2, I will award the full bounty to each. :) Q1: characterization: What is a good way to characterize the answer?  Are cycles even well-defined when there are repeats?  (I'm looking for a characterization beyond the obvious one where we consider all possible mappings of repeated letters to their correct positions and just apply the permutation solution to each.) Q2: greedy algorithm: First let me propose the following greedy algorithm: At every step, make a move s.t. either (A) or (B) happens (or both): (A) a non-repeated letter gets to its (unique) correct position, (B) both swapped letters end up at correct positions (in this case, a swapped letter might be one of a repeated set, and it ends in one of its correct positions in the desired outcome). The greedy algorithm as specified above is ""incomplete"" in the sense that one may be unable to find a move that meets (A) or (B).  However, suppose there is such a sequence of moves (where every step meets (A) or (B)) resulting in the desired outcome.  Is such a sequence guaranteed to achieve the minimum?  Alternatively, can you devise a different greedy algorithm that can reach the minimum? Footnote on motivation: In Waffle, some letters are jumbled in a crossword grid and the challenge is to minimize no. of swaps to make all the words correct.  Waffle is a weird game in that it is both a word game (anagramming) and a combinatorial game (minimizing no. of swaps).  This question is about the latter, i.e. how I can minimize no. of swaps after solving the entire grid and knowing the desired outcome.","['recreational-mathematics', 'puzzle', 'combinatorics']"
4740726,"Finding the derivative of a function with two absolute values within it, using a piecewise function","I'm trying to solve some a problem relating to absolute values. I found online the strategy for solving similar functions from here: https://www.youtube.com/watch?v=eIHtq67nh7w&list=PLGbL7EvScmU7DfRNwONW7JDDcmB98Gjcs&index=22&t=305s&ab_channel=ProfRobBob Using the strategy shown: $y = |2x-3|+1$ solve for $0$ , to find where break point in the line will be: $0 = 2x-3 -> x = 3/2$ By substituting values around the break point, for example: When: $x=0 $ $y = |2(0) -3| + 1 $ $y = |-3| + 1 $ $y = 4$ (absolute value needs to be taken when x < 3/2) When: $x=3 $ $y = |2(3) -3| +1 $ $y = |3| + 1 $ $y = 4$ (The result is the same regardless of taking the absolute value when x> 3/2) We can then represent the first equation to make it piecewise: $y =   2x - 3 + 1 ; x >= 3/2$ $y =  -2x + 3 + 1 ; x < 3/2$ derivatives would be: $y  = 2; x >=3/2  $ $y  =-2; x < 3/2$ HOWEVER,
I am trying to solve a function like this: $y = |x^2 - 3| + |x + 1|$ And Im not sure how to go about it. I tried doing the same as before: $0 = x^2 -1 + x + 4   ->  -3 = x^2 +x$ But this doesn't seem like the right approach, any advice on what to do?","['derivatives', 'absolute-value', 'piecewise-continuity']"
4740753,Is it possible to get the value of angle BAC in this case?,"AB (240 m) is a base of an object  that is tilted over the ramp with angle BCE = 45 degrees. CE is 60 m, ie. the known section of the object on the ground say if ramp had not been there. My aim is to find out the value of BAC, ie the angle at which the object is tilted. I have tried and could find out the length of various segments. But can't get to any way that can relate it with the angle BAC","['triangles', 'trigonometry', 'geometry']"
4740762,"Sufficient condition to apply integration by parts ""infinitely many times""","My question is related to this one .
Suppose we are trying to solve the following integral $$ \int f(x) g(x) dx,$$ where we know $f(x)$ is smooth, all of its derivatives are positive, and the sum of its derivatives $\sum_{n=0}^{\infty} f^{(n)}(x)$ converges.
For the sake of simplicity, let's say $g(x)=e^x$ for now.
Then if we naively apply integration by parts ""infinitely times"" with $u=f(x)$ and $v'=g(x)=e^x$ we get $$ \sum_{n=0}^{\infty} (-1)^n e^x f^{(n)}(x) = e^x \sum_{n=0}^{\infty} (-1)^n f^{(n)}(x). $$ Since the above series converges (by absolute value test and our assumption), were we justified in performing integration by parts infinitely times? In other words, is the condition that a series of a function's derivatives (which are all positive) converges, sufficient to performing integration by parts infinitely times (with $g(x)=e^x$ )? Now what about for a more general function $g(x)$ ?  What conditions are needed on $g(x)$ to allow infinite applications of integration by parts (with the same assumptions on $f(x)$ )?  Thanks to all in advance.","['integration', 'real-analysis', 'absolute-convergence', 'sequences-and-series', 'derivatives']"
4740794,Brezis' theorem 7.7: how to approach to prove $\left |\frac{d u_\lambda}{dt} (T) \right| \le\frac{|u_0|}{T}$?,"Let $(H, \langle \cdot, \cdot \rangle)$ be a real Hilbert space and $|\cdot|$ its induced norm. Let $A: D(A) \subset H \to H$ be a maximal monotone (unbounded linear) operator. Let $I:H \to H$ be the identity map. For every $\lambda>0$ , we define the resolvent $J_\lambda$ and the Yosida approximation $A_\lambda$ of $A$ by $$
J_\lambda=(I+\lambda A)^{-1} \quad \text { and } \quad A_\lambda=\frac{1}{\lambda}\left(I-J_\lambda\right).
$$ We define by induction the subspaces $$
D(A^{k+1}) := \{ v \in D(A^k) :Av \in D(A^k)\}
\quad \forall k \in \mathbb N^*.
$$ Then $D(A^k)$ is a Hilbert space for the inner product $$
\langle u, v \rangle_{D(A^k)} := \sum_{j=0}^k \langle A^j u, A^j v \rangle.
$$ I'm reading Theorem 7.7 at page 194 of Brezis' Functional Analysis , i.e., Let $A$ be a self-adjoint maximal monotone (unbounded linear) operator. Then, given any $u_0 \in H$ there exists a unique function $$
u \in C([0,+\infty) ; H) \cap C^1((0,+\infty) ; H) \cap C((0, +\infty); D(A))
$$ such that $$
\begin{cases}
\frac{d u}{d t}+A u&=0 \quad \text{on} \quad (0,+\infty), \\
u(0)&=u_0.
\end{cases}
$$ Moreover, we have $$
|u(t)| \leq\left|u_0\right| \quad \text { and } \quad\left|\frac{d u}{d t}(t)\right|=|Au(t)| \leq \frac{1}{t} \left|u_0\right| \quad \forall t > 0,
$$ and $$
u \in u \in C^k ((0,+\infty) ; D(A^\ell))
\quad \forall k, \ell \in \mathbb N.
$$ Assume that $u_0 \in D(A^2)$ . For $\lambda>0$ , let $u_\lambda$ be the solution of the problem $$
\begin{cases}
\frac{d u_\lambda}{d t}+A_\lambda u_\lambda&=0 \quad \text{on} \quad [0,+\infty), \\
u_\lambda(0)&=u_0.
\end{cases}
$$ The author then goes on to prove that $$
\left |\frac{d u_\lambda}{dt} (T) \right| \le\frac{|u_0|}{T}
\quad \forall T>0, \quad (*)
$$ by using the following properties $A_\lambda$ is self-adjoint. $u_\lambda \in C^2([0, +\infty) ; H) \cap C^1 ([0, +\infty) ; D(A)) \cap C([0, +\infty); D(A^2))$ . $\frac{d}{dt} (A_\lambda u_\lambda) = A_\lambda (\frac{d u_\lambda}{dt})$ . the map $t \mapsto |\frac{d u_\lambda}{dt} (t)|$ is non-increasing. His proof of $(*)$ is non-trivial by integrating and then combining inequalities. It's like magic coming out of nowhere. I could not get a feeling of how such ideas arise. Could you shed some light on how you approach to prove $(*)$ ?","['ordinary-differential-equations', 'proof-explanation', 'real-analysis', 'hilbert-spaces', 'functional-analysis']"
4740799,Calculating eigenvectors and eigenvalues of a 3x3 matrix,"I am dealing with a task in which I have to calculate the eigenvectors and eigenvalues of the following matrix: $$\begin{pmatrix}
5& 6 &1\\
1 &3& 1\\
2 &1 &2\end{pmatrix}$$ So I have computed the characteristic equation, which turns out to be this three-degree polynomial: $$-λ^3+10\lambda^2 -22\lambda +20$$ This cannot be factored by Ruffini, which leaves me stuck as to finding the roots I need. Have I done something wrong? Thanks in advance.","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
4740805,"Let $g\circ f = h\circ f$ for all $f$ from $A$ to $B$, prove that $g = h$","Let $g:B \to C$ and $h: B \to C$ be functions, so that $g
\circ f = h\circ f$ for all $f: A \to B$ . To prove that $g=h$ . The way I proceeded to prove is as follows: Since $g\circ f = h\circ f$ for all $f$ , I thought it was safe to assume that $f$ is surjective, so that $B \subset ranf$ . Hence: $(x,y) \in g \implies x \in domg \implies x \in B \implies x\in ranf \implies \exists z(z,x) \in f \implies \exists z (z,x) \in f $ and $(x,y) \in g \implies \exists z(z,y) \in g\circ f \implies \exists z(z,y) \in h\circ f \implies \exists v, \exists z (z,v) \in f $ and $(v,y) \in h \implies \exists z, \exists v\in f $ and $(z,x) \in f \implies v=x \implies (x,y) \in h$ . The second half of the proof is completely analogous to the first half, so I don't think it's required to write it down. However, did I prove it correctly? Was there any flaw or falt in my reasoning? I am asking for I require constant confirmation from the more professional since I am only new to set theory. Thank you in advance.","['elementary-set-theory', 'proof-writing', 'solution-verification']"
4740851,Is this inequality about conditional expectations and martingales true?,"Let $(X,Y,Z)$ be a square integrable discrete-time Markov martingale, that is, $E[Z|X]=X$ , $E[Y|X]=X$ , $E[Z|X,Y]=E[Z|Y]=Y$ . Let $U$ be a square integrable random variable independent from $(X,Y,Z)$ . Is it true that $$ E[ (Z- E[Z|X, Z+U])^2 ] \leq  E[ (Z- E[Z|Y, Z+U])^2 ] + E[ (Y- E[Y|X, Y+U])^2 ]$$ This inequality means that the error that one makes when estimation $Z$ based on $X$ and a noisy version of $Z$ is smaller than the sum of the error that one makes when estimation $Z$ based on $Y$ and a noisy version of $Z$ and the error that one makes when estimation $Y$ based on $X$ and a noisy version of $Y$ . The inspiration for this inequality is that, if we remove the noisy terms $Z+U$ and $Y+U$ everywhere, the inequality is true since $$ E[ (Z- E[Z|X])^2 ] = E[ (Z- X)^2 ] \leq E[ (Z- Y)^2 ] + E[ (Y- X)^2 ]$$ Now, adding the noisy terms $Z+U$ and $Y+U$ inside the conditioning is making all the estimates (the conditional expectations) better, i.e., the errors are smaller. Since $U$ is common to all terms and is independent from the martingale, we are making them better ""in the same way"" (hand waving). This is the intuition behind $$ E[ (Z- E[Z|X, Z+U])^2 ] \leq  E[ (Z- E[Z|Y, Z+U])^2 ] + E[ (Y- E[Y|X, Y+U])^2 ]$$ Any thoughts/proof?","['conditional-expectation', 'martingales', 'inequality', 'probability-theory', 'probability']"
4740932,Non trivial $f$ such that $\int_0^1 x^n f(x) dx=a_n\pi+b_n$,"I need a non trivial function $f(x)$ such that $$\int_0^1 x^n f(x) dx=a_n\pi+b_n$$ where $a_n,b_n\in\mathbb{Z}$ and $n\in\mathbb{N}$ We know that $$\pi=4\int_0^1 \sqrt{1-x^2}\ dx$$ By Binomial theorem for rational index $$\pi= 4\int_0^1 \sum_{r=0}^\infty\binom{1/2}{r} x^{2r}\ dx$$ Any help would be highly appreciated. Edit If $f(x)=\sin^{-1} x$ then see here $$\int_0^1 x^n \sin^{-1} x \ dx= c_n \pi +d_n$$ but $c_n,d_n\in\mathbb{Q}$","['integration', 'examples-counterexamples', 'real-analysis', 'calculus', 'pi']"
4740945,How to calculate the radius of an arc segment knowing only a base length?,"While designing a mechanical part, I stumbled upon a geometry problem that I've been unable to solve. The problem seems simple but the answer has been eluding me. In the figure, only the base dimension is known (numerically equal to 1/3). There is a circle segment with radius R, marked by the arc, that is formed according to this known dimension. This base dimension is equal to the distance between the tangent point and the intersection of the line when the radius is prolonged. I need to determine the radius R. I've assigned variables to the problem as best as possible, and found out four equations to the problem. With four variables and four equations, I thought that the problem was over but got lost in the calculations. I've also draw this problem in CAD. The program indicated me that it was well defined, and resulted in R = 0.6667. I'm however unable to get to this result analytically. There is a similar question: How to calculate the radius of a Arc Segment given only the Arc Length and the Height of the arc segment? But note that in the case here the arc length is unknown. Thanks in advance for any assistance.",['geometry']
4740965,for $x \in \mathbb{R}$ find the number of solutions in $3x^2 + 4|x^2 - 1| + x - 1 = 0$. Why we are considering negative values here?,"I was solving some practice problems, from a booklet of math, and one of the problems is like this for $x \in \mathbb{R}$ find the number of roots in the eq . $$3x^2 + 4|x^2 - 1| + x - 1 = 0$$ I know I have find the value of $\Delta = b^2 - 4ac$ , and based on that I can say something about $\#$ of real roots. But I looked at 3 help books, and some online resources as well, and the way they have solved this is they consider two cases - first where $x^2 - 1 \geq 0 $ and the other is where $x^2 - 1 < 0$ I don't understand why these books are considering the negative part, whatever the value is inside it will be positive after taking absolute value
so I think the solution should be equal to the number of real roots for the equation $$3x^2 +4(x^2-1) +x -1 = 0$$ $$3x^2 + 4x^2 - 4 + x - 1= 0$$ $$7x^2 + x - 5 = 0$$ $$\Delta = 1^2 - 4(7)(-5) = 141$$ so give eq. has 2 real roots. I can't provide you the textbook, but in an online website the problem has been solved in same way which can be found here - https://byjus.com/question-answer/for-x-in-mathbb-r-the-number-of-real-roots-of-the-equation-3x-2-3/","['algebra-precalculus', 'quadratics', 'absolute-value']"
4740978,The range of a constraint for function to be surjective,"Consider the following function $f(x)=\dfrac{x^2+2x+a}{x^2+4x+3a}$ Now the question states for us to find the constraint that limits $a$ so that $f(x)$ becomes surjective. My Attempt This can be further written as $f(x)=\dfrac{(x+1)^2+a-1}{(x+2)^2+3a-4}$ Now to make the function surjective, we can ensure two cases:- CASE 1 The function on the top can have some $y$ $\in$ $\mathbb{Q}^{-}$ and for this $a-1<0$ and the function at bottom  can have some $y$ $\in$ $\mathbb{Q}^{+}$ and for this $3a-4>0$ . But there is no solution. CASE  2 The function on the top can have some $y$ $\in$ $\mathbb{Q}^{+}$ and for this $a-1>0$ and the function at bottom  can have some $y$ $\in$ $\mathbb{Q}^{-}$ and for this $3a-4<0$ . So $a\in(1,4/3)$ But this ans is wrong could someone point the mistake.",['functions']
4740997,Under what conditions is $\lim_{x\to a}\left|\varphi\circ f(x)-\tau \circ g(x)\right|=0$ true?,"This question is inspired from another much easier problem I was trying to solve which I tried to generalize. The question is essentially as follows (assuming all the limits exist) If $a\in \mathbb R\cup \{\infty\}$ , and $f$ and $g$ are two functions such that $$\lim_{x\to a}\left|f(x)-g(x)\right|=0$$ and $\varphi$ and $\tau$ are functions such that $$\lim_{x\to \lim_{t\to a}f(t)}\left|\varphi (x)-\tau (x)\right|=0$$ then what is the least amount of conditions necessary to impose so that $$\lim_{x\to a}\left|\varphi\circ f(x)-\tau \circ g(x)\right|=0$$ can be concluded? What about the special case $f=\varphi$ and $g=\tau$ ? In the original problem I had, I was in the special case, and dealing with $a=\infty$ which is why I was only considering those cases. I was under the impression that the continuity of $g$ and the extra condition that $$\lim_{x\to \infty} |f(x)|=\infty$$ would be enough. But, I soon arrived at a counterexample: construct $f$ and $g$ such that $$\lim_{x\to \infty} f(x)= \lim_{x\to \infty} g(x) = -\infty$$ but $$\lim_{x\to -\infty} f(x) = 1,\quad \lim_{x\to -\infty} g(x) = 2$$ which shattered my hopes. Also, it seems strange that there is very little (easily available) literature on such questions about composition of functions. So, does anyone know of any such result that deals with such a problem? Link to MO post: https://mathoverflow.net/q/451283/311366","['analysis', 'function-and-relation-composition', 'calculus', 'functions', 'limits']"
4741059,Radial Limit of a conformal isomorphism - making domain bounded,"I'm currently working through a proof regarding the radial limit of an conformal iso $\psi :\mathbb{D} \rightarrow U$ - it is section 15.3 of Milnor's Dynamics in One Complex Variable (which can be found here https://arxiv.org/abs/math/9201272 ). I understand the majority of the proof, but am unsure on how we are able reduce to the bounded case. For $\psi:\mathbb{D}\rightarrow U$ a conformal isomorphism, this is what Milnor puts: Why does composing by a linear fractional map make it bounded, and why must $\psi(D)$ omit at least two values. If theres any way someone could break down what is happening here, I'd really appreciate it. Thanks","['complex-analysis', 'general-topology', 'functions']"
4741063,"Placing the $21$ two-digit primes into a grid, such that primes in adjacent squares have either the same tens digit or ones digit","This is USAMTS round 3, problem 1 of the 2020-2021 Academic Year. Place the 21 2-digit prime numbers in the white squares of the grid on the right so that each two-digit prime is used exactly once. Two white squares sharing a side must contain two numbers with either the same tens digit or ones digit. A given digit in a white square must equal at least one of the two digits of that square's prime number. They didn't provide a detailed explanation of how they got the (unique) solution on their website, so I was wondering if someone could explain how to come up with the solution? I tried using casework, but there seems to be too many possibilities to consider. Number the white squares from 1 to 21 from left to right and top to bottom and let $c_i$ be the number in square i. Clearly, it seems easier to consider squares containing 2, 4, or 5, but even then I've not sure how to make reasonable deductions. For ease of reference, define the following sets $$\begin{align}
P_1 &:= \{13,11,17,19,31,41,61,71\} \\
P_2 &:= \{23,29\} \\
P_3 &:= \{31,37,23,13,43,53,73,83\}\\  
P_4 &:= \{41,43,47\} \\ 
P_5 &:= \{53,59\} \\
P_6 &:= \{61,67\} \\
P_7 &:= \{73,37,79,71,17,47,67,97\} \\
P_8 &:= \{83,89\} \\ 
P_9 &:= \{19,29,59,79,89,97\}
\end{align}$$ Obviously, $c_1,c_3 \in P_3, c_4 \in P_2, c_{12}\in p_5, c_{19},c_{20}\in P_1, c_{18}\in P_4.$ For instance, squares 5 and 8 both contain the digit 9 in the units digit if $c_4=29$ since $c_3 = 23$ , which immediately implies that $c_4\neq 29$ . So $c_4 = 23$ . $c_3$ cannot equal $29.$ So $c_3$ is either $13,43,53,73,73,$ or 83. Suppose $c_8$ ends in a 3. Then $c_3$ and $c_8$ have distinct tens digits and the same units digit. Thus if $c_7$ does not end in a 3, it must share the tens digit of $c_8$ and $c_3$ , a contradiction. Hence $c_7$ ends in a 3. $c_{11}$ cannot share the 3 with $c_7$ and $c_7$ doesn't contain a 9 as its tens digit. If $c_5=53,$ then one of $c_9$ or $c_{16}$ ends in 3. If $c_{18} = 43,$ then $c_{19} = 41$ or $13$ . If $c_{18} = 47,$ then $c_{19} = 41$ or $17$ . So basically, I've only been able to deduce with certainty that $c_4 = 23$ and it seems hard to find the possibilities for other numbers.","['contest-math', 'puzzle', 'combinatorics', 'discrete-mathematics', 'recreational-mathematics']"
4741111,Understanding equivalent definitions of locally convex spaces,"As stated in the title, I am trying to make sense of the equivalent definitions of locally convex spaces. Especially, I am confused about the part, where I try to prove, that the topologies coincide. Let me first give the definition I am working with: A topological vector space $(X,\tau)$ is called locally convex, if there is a family of seminorms $(p_i)_{i\in I}$ , such that $\tau$ is the initial topology with respect to the quotient mappings $q_i: X \rightarrow X/\ker(p_i)$ , $i\in I$ . The seminorms are separating, i.e. if $x\ne 0$ , there is an $i\in I$ with $p_i(x)\ne 0$ . Let me mention, that for any $i\in I$ , $\epsilon >0$ and $x\in X$ , the sets $$B^i_{\epsilon}(x):=\{y\in X: p_i(y-x)<\epsilon\}$$ form a subbase of the initial topology of the mappings $q_i: X \rightarrow X/\ker(p_i)$ , $i\in I$ . Note: There is a norm $\lVert\cdot\rVert:X/\ker(p_i)\rightarrow \mathbb{R}$ , such that $p_i=\lVert q_i\rVert$ , especially $p_i(y-x)=\lVert q_i(y)-q_i(x)\rVert$ . Then, $B^i_{\epsilon}(x)=q_i^{-1}[ B_\epsilon^{X/\ker(p_i)} (q_i(x))] $ , i.e. can be written as preimage of $q_i$ of an $\epsilon$ -ball in $X/\ker(p_i)$ . I want to prove the following statement: A topological vector space $(X,\tau)$ is a locally convex space, if it contains a neighborhood basis of $0$ of convex sets. Proof of the statement: Every convex neighborhood of $0$ contains an open convex balanced neighborhood of $0$ . Therefore one can find a neighborhood basis of $0$ of open convex balanced sets, which will be denoted as $\mathcal{B}^{disc}$ . Then one can prove, that the family of Minkowski functionals $$\{p_U: U\in\mathcal{B}^{disc}\}$$ is a separating family of seminorms, which are continuous with respect to $\tau$ . Moreover, for any $U\in \mathcal{B}^{disc}$ , it holds $U=\{x\in X: p_U(x)<1\}.$ Now, it is only left to show, that the initial topology with respect to the quotient mappings $q_U: X \rightarrow X/\ker(p_U)$ , $U\in \mathcal{B}^{disc}$ coinicides with $\tau$ . For this, I am not sure, if my proof for $\tau\subset \tau^{init}$ is right. Proof: $\tau^{init}\subset \tau:$ For any $U\in \mathcal{B}^{disc}$ , the seminorm $p_U$ is continuous with respect to $\tau$ . Then, for any $U\in \mathcal{B}^{disc}$ , $\epsilon >0$ , $x\in X$ , one finds $B^U_\epsilon(x)=(\epsilon U)+x\in\tau$ , since translation and scalar multiplication are homeomorphisms, i.e. all subbase sets of $\tau^{init}$ are contained in $\tau$ , such that $\tau^{init}\subset \tau$ . $\tau\subset \tau^{init}:$ Pick any $V\in \tau$ . For any $x\in V$ , the set $V-x$ is an $\tau-$ open neighborhood of $0$ , since translations are homeomorphisms. This means, for any $x\in V$ , one can find a $\tau-$ open $U^x\in\mathcal{B}^{disc}$ , such that $U^x\subseteq V-x$ , leading to $U^x+x\subseteq V$ and $\cup_{x\in V}(U^x+x) = V$ . However, any $U^x+x\in \tau^{init}$ , since $$U^x+x = \{y+x\in X: p_{U^x}(y)<1\}=\{z\in X: p_{U^x}(z-x)<1\}=B_1^{U^x}(x)\in \tau^{init}.$$ This yields $$V = \cup_{x\in V}(U^x+x) = \cup_{x\in V}B_1^{U^x}(x)\in \tau^{init}.$$ Here is what is confusing for me overall: I have shown, that $\tau^{init}\subset \tau$ and that any set in $\tau$ can be written as a union of sets in of the subbase of $\tau^{init}$ . However, wouldn't this imply, that any set of the initial topology could be written as union of sets of the subbase? This shouldn't be true, since a subbase is not necessarily a base, right? Is there anything wrong with my proof, or am I just running in circles for another reason? Further thoughts on this: Please correct me, if I am wrong with this. If $\mathcal{B}^{disc}$ is a open neighborhood basis of $0$ , then $\{U+x:U\in \mathcal{B}^{disc}, x\in E\}$ should be a base of $\tau$ . (Again using, that translation with $x$ is a homeomorphism.) Once again, you can rewrite $$U+x = \{y+x\in X: p_U(y)<1\}=\{z\in X: p_U(z-x)<1\}=B^U_1(x),$$ such that the mentioned subbase contains a base for $\tau$ . Since $\tau^{init}\subset \tau$ , it is a base for $\tau^{init}$ . But then it seems, that the definition I stated above is slightly more general? Is there some definition for something like a convex neighborhood subbasis ? Any hint or help is appreciated! Thank you in advance!","['general-topology', 'functional-analysis', 'locally-convex-spaces', 'partial-differential-equations']"
4741121,Approximating the symplectic flow of Hamiltonian systems with $H = \frac{1}{2}p^TM^{-1}p + V(q)$,"Consider a symplectic map $\phi_t(p,q)\in \mathbb{R}^{2n}$ , that solves or approximates the Hamiltonian system $$\dot{p} = - \nabla V(q),\quad\dot{q} = M^{-1}p,$$ for Hamiltonian $H = \frac{1}{2}p^TM^{-1}p + V(q)$ . That is, the Hamiltonian is separable, quadratic in $p$ , $V(q)$ is an arbitrary differentiable function and $M$ is symmetric and constant. What can we say about $\phi_t$ given this information and how can we leverage the structure of this Hamiltonian to find a better approximation to $\phi_h$ ? My thoughts so far: For example, given this particular form of the Hamiltonian, we can see that it is an even function of $p$ and therefore has time-reversal symmetry , so it's flow must also be time-reversible (i.e., invariant under the time-reversal operator $R:(p,q,t)\rightarrow(-p,q, -t)$ ). So the symplectic map must share this reversing symmetry $R\circ\phi_h = \phi_h^{-1}\circ R$ . To leverage the separability property of the Hamiltonian, we can use the Störmer-Verlet map to construct an explicit symplectic mapping (or any symplectic numerical method), given $V(q)$ . Such numerical methods can also have time-reversal symmetry, but are only accurate for short times $t<<1$ . To leverage the quadratic in $p$ propery , by defining $\phi_t(p,q)=(P, Q)$ , where $P$ and $Q$ can depend on $(p, q, t)$ , then using the definition of a symplectic transformation we require $$P_p^TQ_q - P_q^TQ_p = I,$$ where the subscripts denote a partial derivative. As $Q$ must satisfy $\dot{Q}=M^{-1}P$ , this becomes $$\dot{Q}_p^TQ_q - \dot{Q}_q^TQ_p = M^{-1}.$$ . (preserves the Poisson-bracket) we can then use the chain rule on $\dot{Q}$ to get a complex expression. But I think we need a good ansatz about the form of $Q$ that can solve/approximate the above for $Q$ . I would like to know more maps that can approximate $\phi_h$ by leveraging/preserving the structure of the Hamiltonian, and/or by combining the above ideas. Any references on this would be appreciated too :)","['dynamical-systems', 'symplectic-geometry', 'ordinary-differential-equations', 'differential-geometry']"
4741128,"What is the value of $\frac{1}{2}\int_B\int_B\frac{\rho(x,y,z)\rho(x',y',z')}{4\pi\epsilon_0\sqrt{(x-x')^2+(y-y')^2+(z-z')^2}}dxdydzdx'dy'dz'$?","I am reading a book about electromagnetism by Yousuke Nagaoka. Suppose $R$ is a positive real number. Suppose $Q$ is a positive real number. Let $B:=\{(x,y,z)\in\mathbb{R}^3:\sqrt{x^2+y^2+z^2}\leq R\}$ . Let $\rho(x,y,z):=\frac{Q}{\frac{4}{3}\pi R^3}$ if $(x,y,z)\in B$ and $\rho(x,y,z):=0$ if $(x,y,z)\notin B$ . What is the value of $$U:=\frac{1}{2}\int_B\int_B\frac{\rho(x,y,z)\rho(x',y',z')}{4\pi\epsilon_0\sqrt{(x-x')^2+(y-y')^2+(z-z')^2}}dxdydzdx'dy'dz'$$ ? The author calculated $U=\frac{3Q^2}{20\pi\epsilon_0 R}$ . The author calculated this value using the concept of potential, Gauss's law, etc.. I want to know how to calculate $U$ mathematically.","['integration', 'electromagnetism', 'definite-integrals']"
4741139,Evaluate $\sum\limits_{n=1}^\infty \left(\frac{1}{(k+n)^2+n^2}-\frac{1}{(k+1-n)^2+n^2}\right)$,"I am trying to find the pattern for the coefficients of the closed forms for this series: $$\sum_{n=1}^\infty \left( \frac{1}{(k+n)^2+n^2}-\frac{1}{(k+1-n)^2+n^2} \right)$$ The series seems to converge for every natural $k$ , and it is easy to evaluate each of the two series using this , ending up with a bunch of digamma functions, as shown here . However, when plugging the series in wolfram for some values of $k$ this is what I got: \begin{align}
k=1: &&& \frac{-5+2\pi\tanh\left(\frac{\pi}{2}\right)-\pi\coth(\pi)}{4} \\
k=2: &&& \frac{-42-10\pi\tanh\left(\frac{3\pi}{2}\right)+15\pi\coth(\pi)}{60} \\
k=3: &&& \frac{-341+120\pi\tanh\left(\frac{3\pi}{2}\right)-90\pi\coth(2\pi)}{720} \\
k=4: &&& \frac{-3189-884\pi\tanh\left(\frac{5\pi}{2}\right)+1105\pi\coth(2\pi)}{8840} \\
k=5: &&& \frac{-58076+19890\pi\tanh\left(\frac{5\pi} {2}\right)-16575\pi\coth(3\pi)}{198900} \\
k=6: &&& \frac{-21576583-6277050\pi\tanh\left(\frac{7\pi}{2}\right)+7323225\pi\coth(3\pi)}{87878700}
\end{align} The arguments of $\tanh$ and $\coth$ can be guessed, respectively, to $\frac{k+(k+1\mod2)}{2}\pi$ and $\frac{k+(k\mod2)}{2}\pi$ , but the other coefficients are less easy to confront. The linear sequence in the numerators ( $5,42,341,\dots$ ) does not appear even in the OEIS, and same goes for the denominators. However, it is possible to guess the coefficients of $\tanh$ and $\coth$ . Hence the question: How to show that \begin{align}
\sum_{n=1}^\infty \left( \frac{1}{(k+n)^2+n^2}-\frac{1}{(k+1-n)^2+n^2} \right)= \\
=-\frac{a_k}{b_k}+\frac{\pi}{2k}\tanh\left(\frac{k}{2}\pi\right)-\frac{\pi}{2k+2}\coth\left(\frac{k+1}{2}\pi\right) \text{,} &&&  k\  \text{odd}\\
=-\frac{a_k}{b_k}-\frac{\pi}{2k-2}\tanh\left(\frac{k+1}{2}\pi\right)+\frac{\pi}{2k}\coth\left(\frac{k}{2}\pi\right) \text{,} &&&  k\  \text{even}\\
\end{align} and what are $a_k$ and $b_k$ ? Appearently the result in terms of digamma functions can be simplified a lot to $$\sum_{n=1}^\infty \left( \frac{1}{(k+n)^2+n^2}-\frac{1}{(k+1-n)^2+n^2} \right)=$$ $$=\frac{i}{2k}\left(\psi\left(1+\bar{z}k\right)-\psi\left(1+zk\right)\right)+\frac{i}{2(k+1)}\left(\psi(z-\bar{z}k)-\psi(\bar{z}-zk)\right) \tag{*}$$ where $z=\frac{1+i}{2}$ I feel like it is almost done , but I can't perform the last steps to get rid of the digamma functions.
However, I strongly believe it has to do with the following two identities for $u\in \mathbb{C}$ : $$\psi(1-u)-\psi(u)=\pi\cot(\pi u) $$ $$\psi\left(\frac12+u\right)-\psi\left(\frac12-u\right)=\pi\tan(\pi u)$$ Any ideas on how to finish?","['integration', 'definite-integrals', 'real-analysis', 'calculus', 'sequences-and-series']"
4741173,Clarification of some concepts - calculus of multiple variables,"I am reading from two books a few definitions and a few theorems related to differentiability, differential, directional derivative, etc., and I got a bit confused so I want to clarify if I am understanding these things right. Let's say we have a function $w = f(x,y,z)$ which is defined in some neighborhood $U$ of the point $(a,b,c)$ . As usual, we denote $\Delta{x} = x - a$ $\Delta{y} = y - b$ $\Delta{z} = z - c$ $\Delta{w} = f(a+\Delta{x}, b+\Delta{y}, c+\Delta{z}) - f(a,b,c) $ where $(x,y,z)$ is some other point from $U$ . This is our setup here. The first statement is that when $f'_x(a,b,c), f'_y(a,b,c), f'_z(a,b,c)$ exist, that does not imply that the function $f$ is continuous in the point $(a,b,c)$ . I saw some examples i.e. counterexamples, so OK, I think I understand that. But then I am reading this statement/theorem. If $f'_x(a,b,c), f'_y(a,b,c), f'_z(a,b,c)$ exist and are continuous in the point (a,b,c), then $\Delta{w} = f(a+\Delta{x}, b+\Delta{y}, c+\Delta{z}) - f(a,b,c) $ can be written in this way $\Delta{w} = f'_x(a,b,c) \Delta{x} + f'_y(a,b,c) \Delta{y} + f'_z(a,b,c) \Delta{z} + \epsilon_1 \Delta{x} + \epsilon_2 \Delta{y} + \epsilon_3 \Delta{z} $ , where $\epsilon_1,\epsilon_2,\epsilon_3$ are functions of $a,b,c, \Delta{x}, \Delta{y} ,\Delta{z}$ which tend to $0$ as $(\Delta{x},\Delta{y},\Delta{z}) \to (0,0,0)$ From the conclusion of this theorem, I think it easily follows also that $f$ is continuous in the point $(a,b,c)$ . We just let $\Delta{x}, \Delta{y}, \Delta{z}$ go to zero and we see that $\Delta{w}$ also goes to zero. So it follows that $f$ is continuous in $(a,b,c)$ . Right? Comparing this to 1) I come to the conclusion that the difference in the two situations comes from the fact that in 2) we require also that the partial derivatives are also continuous in the point $(a,b,c)$ , and this makes it possible to claim/prove that $f$ is also continuous. Is this indeed so? I am reading then another definition. We say that $w = f(x,y,z)$ , whose partial derivatives exist in the point $(a,b,c)$ , is differentiable in the point $(a,b,c)$ , if $\Delta{w}$ can be written in the form $$\Delta{w} = f'_x(a,b,c) \Delta{x} + f'_y(a,b,c) \Delta{y} + f'_z(a,b,c) \Delta{z} + \epsilon_1 \Delta{x} + \epsilon_2 \Delta{y} + \epsilon_3 \Delta{z} \ \ \ \ \ (*)$$ where $\epsilon_1,\epsilon_2,\epsilon_3$ are functions of $a,b,c, \Delta{x}, \Delta{y} ,\Delta{z}$ which tend to $0$ as $(\Delta{x},\Delta{y},\Delta{z}) \to (0,0,0)$ From this definition I am drawing the conclusion that the situation in 2) is just one particular case in which the function $f$ is differentiable. I mean, basically 2) is saying that if the partial derivatives exist and are continuous at $(a,b,c)$ , then the function $f$ is differentiable at the point $(a,b,c)$ /differentiable in the sense of definition 3)/. Is this so, i.e. am I understanding this correctly? Also, in other words this definition is just saying that $\Delta{w} = dw + \epsilon_1 \Delta{x} + \epsilon_2 \Delta{y} + \epsilon_3 \Delta{z} $ i.e. the differential of w is ""very good""  approximation of $\Delta{w}$ (very good because these epsilons go to zero as $(\Delta{x}, \Delta{y}, \Delta{z}) \to 0$ ). Right? And when this is so, we say that $f$ is differentiable (differentiable as a whole, not partially) in the point $(a,b,c)$ . Am I getting this right? Finally, there is another theorem which says that if $f$ is differentiable (in the sense of def. 3)) in the point $(a,b,c)$ , then it also has directional derivatives at $(a,b,c)$ in the direction of any unit vector $v = (a_1, b_1, c_1)$ . The proof the authors give
for this one is kind of informal, I didn't like it much. But this statement I was able to prove formally myself by using $(*)$ . It seems very easy to prove by using the definition 3) i.e. by using $(*)$ . So am I understanding all this stuff correctly? Am I drawing the correct conclusions for myself? Actually in the book these defs and theorems were stated for two variables $f(x,y)$ but I stated them here for three variables $f(x,y,z)$ again to ensure that I am understanding all this correctly.","['differential', 'multivariable-calculus', 'calculus', 'derivatives']"
4741179,Why is direct analytic continuation not transitive?,"Consider the following definitions and remarks which will be referenced in the question: Defn ( Direct analytic continuation) A function element in a domain $U$ is a pair $(f, D)$ where $D$ is a subdomain of $U$ and $f$ is an analytic function on $D$ . Two function elements $(f,D)$ and $(g,E)$ are equivalent, write $(f,D) \sim (g,E)$ if $D \cap E \neq 0$ and $f = g$ on $D \cap E$ . We say $(g,E)$ is a direct analytic continuation of $(f,D)$ . Why do we make such a definition? We know the power series $$\sum_{r \geq 0}z^k = \frac{1}{1-z}$$ is defined on $D(0,1)$ and cannot be extended to any larger domain due to natural boundary. However, $\frac{1}{1-z}$ is holomorphic on $\mathbb{C} \backslash \{1\}$ so sometimes the domain forced by the definition of a function is not the maximal possible. In other words, sometimes we are looking at the ""correct"" function with a ""wrong"" domain. Defn (Analytic continuation along path) We say $( g, E )$ is an analytic continuation of $(f, D)$ along $\gamma$ if $\gamma : [0,1] \rightarrow 0$ and there exist function elements $(f_i, D_i), i \in \{ 0, \ldots , n \}$ and $0 = t_o < t_2 < \cdots < t_n = 1$ such that $$(f, D) = (f_0, D_0) \sim (f_1, D_1) \sim \cdots \sim (f_{n - 1}, D_{n - 1}) \sim (f_{n}, D_{n}) = (g, E)$$ and $\gamma \left( \left[ t_j, t_{j + 1} \right] \right) \subseteq D_j$ for $j \in \{ 0, \ldots, n - 1 \}$ . Write $(f, D) \approx_{\gamma} (g, E)$ . Remark As $\mathbb{C}$ has a path-connected basis for the topology, domain are path-connected. Defn (analytic continuation) We say $(g, E)$ is an analytic continuation of $(f,D)$ if there exists a path $\gamma$ such that $(f, D) \approx_{\gamma} (g, E)$ . In this case we write $(f,D) \approx (g, E)$ . Remark If $(f, D) \approx_{\gamma} (g, E)$ and $(f, D) \approx_{\gamma} (h, E)$ then $g = h$ by repeated application of the identity principle. In other words, $g$ is completely determined by $f$ and $\gamma$ . Analytics continuation is an equivalence relation (exercise), but direct analytic continuation is not transitive, even if we require pairwise intersections of the domains to be nonempty. If fact, that is the whole point of analytic continuation along path. The author says that direct analytic continuation is not a transitive property. I am trying to understand why. The obvious thing to me that comes up to mind is that if $ A \cap B \neq \emptyset $ and $B \cap C \neq \emptyset$ , then we could have $A \cap B = \emptyset$ . Is this all there is or am I missing the larger picture?",['complex-analysis']
4741183,Doesn't an integral domain automatically imply that is it is of characteristic zero?,"I was reading about Ring Theory from the book Topics in Algebra by I.N Herstein. I encountered the following definitions: Definition 1 : If $a\neq 0$ is in a  commutative ring $R,$ such that there exists a $b(\neq 0)\in R$ such that $ab=0$ , (,where $0$ is the zero element of $R$ ), then $a$ is called a zero-divisor of $a.$ Definition 2 : A commutative ring is called an integral domain if it has no zero-divisor. Definition 3 : An integral domain $D$ is said to be of characteristic $0$ if the relation $ma=0,$ where $a\neq 0$ is in $D$ and where $m$ is an integer, can hold  only if $m=0.$ I feel that in fact all the integral domains $D$ are of  characteristic $0,$ as since, $D$ is an integral domain it is never possible that $ab=0$ with $a\neq 0 ,b\neq 0.$ So, if say, $pq=0$ then, $p=0$ or $q=0.$ Again, if $p\neq 0$ then, surely $q=0.$ So, isn't it unnecessary to write Definition $3$ ? This is because Definition $2$ implies Definition $3$ we are just giving another name for integral domains, right? Please correct me, if I am mistaken.","['ring-theory', 'definition', 'abstract-algebra']"
4741192,What is this quotient of the free product called?,"I've started thinking about a particular ""equation-preserving"" quotient of the free product of groups (or more generally, coproduct of appropriate algebraic structures). Let $\mathcal{G},\mathcal{H}$ be groups with disjoint underlying sets $G,H$ respectively. Say that a pair of $G\sqcup H$ -words $w(g_1,...,g_m,h_1,...,h_n)$ and $v(g_1,...,g_m,h_1,...,h_n)$ are linked iff we have both $$\mathcal{G}\models \forall x_1,...,x_n[w(g_1,...,g_m, x_1,...,x_n)=v(g_1,...,g_m, x_1,...,x_n)]$$ and $$\mathcal{H}\models\forall x_1,...,x_m[w(x_1,...,x_m, h_1,...,h_n)=v(x_1,...,x_m,h_1,...,h_n).$$ For example, if $g\in Z(\mathcal{G})$ and $h\in Z(\mathcal{H})$ then $gh$ and $hg$ are linked. Linkage gives rise to a congruence $\sim$ on the free product $\mathcal{G}*\mathcal{H}$ , namely the transitive closure of the relation $\sim_0$ given by $x\sim_0y$ iff there are linked words $w,v$ which evaluate to $x,y$ respectively in $\mathcal{G}*\mathcal{H}$ . Let $\mathcal{G}\star \mathcal{H}=\mathcal{G}*\mathcal{H}/\sim$ . Intuitively, the group $\mathcal{G}\star \mathcal{H}$ connects similar ""regions"" of equational behavior in $\mathcal{G}$ and $\mathcal{H}$ . For example, there is a canonical embedding of $Z(\mathcal{G})\times Z(\mathcal{H})$ into $\mathcal{G}\star\mathcal{H}$ , but this isn't true for the free product or even the free product ""modded out by"" the common equational theory of $\mathcal{G}$ and $\mathcal{H}$ . Question : Does this construction have a name? Note that we can generalize this construction to arbitrary algebras (in the sense of universal algebra) in the same signature; really the only tweak needed is to replace ""word"" with ""term"" appropriately. I've phrased this question for groups since I think it's most likely to be known in this setting, but I'm really interested in the universal algebraic version.","['universal-algebra', 'reference-request', 'abstract-algebra', 'group-theory', 'terminology']"
4741232,"Do I understand the definition of limit existence at a point, the definition of continuity at a point, and the difference between them?","I am trying to make sure I understand the definition of limit existence at a point, the definition of continuity at a point, and the exact difference between the two definitions. So below I state the definitions as I understand them, and I hope that someone can tell me whether or not the definitions I have given are correct, specifically if what I have identified as the difference between them is correct. The limit $L$ of a function $f$ exists at a point $p$ if and only if: $\forall\epsilon>0$ , $\exists\delta>0$ such that if $x$ is in the punctured $\delta$ -neighbourhood of $p$ , then $f(x)$ must be in the given $\epsilon$ -neighbourhood of $L$ (because $x$ is never equal to $p$ , it doesn't matter if the $\epsilon$ -neighbourhood does or does not contain $L$ , nor what the value of $L$ is, aslong as it is a real number). A function $f$ is continuous at a point $p$ if and only if: $\forall\epsilon>0$ , $\exists\delta>0$ such that if $x$ is in the unpunctured $\delta$ -neighbourhood of $p$ , then $f(x)$ must be in the given $\epsilon$ -neighbourhood of $L$ (because $p$ must be contained in every $\delta$ -neighbourhood we can create, this implies that all $\epsilon$ -neighbourhoods we choose must always contain $f(p)$ , and thus $f(p)$ must exist, and $f(p)$ must be equal to $L$ , if the statement is to hold true). Is my understanding correct? Are the two definitions essentially identical except for whether or not the $\delta$ -neighbourhood is punctured? EDIT: From Vincent Batens' answer I have learned that my understanding of the first definition (the limit definition) is wrong ; the $\epsilon$ -neighbourhood must be unpunctured because there are many situations in which the punctured $\delta$ -neighbourhood contains an $x$ value such that $f(x)=L$ . For example, if $f(x)$ is a constant function.. or if $f(x)$ is constant in a neighbourhood around $L$ .. or if we have something like the case of $\lim_{x\to0}x\sin(\frac{1}{x})=0$ where $f(x)$ hits $L$ at an ever-increasing frequency as $x$ approaches $0$ . Essentially, in any situation where we can have $f(x)=L$ for $x$ -values inside the punctured $\delta$ -neighbourhood, the limit definition as I have described it breaks down and doesn't work as it should; the definition would say that the limit doesn't exist when it clearly does. Thus it is not true that ""it doesn't matter if the $\epsilon$ -neighbourhood does or does not contain $L$ "".. there are many cases in which it does matter, and so we should specify that, in general, the $\epsilon$ -neighbourhood must be unpunctured, so that we account for such cases in our definition. And of course when we have $f(x)\ne L$ for all $x$ in any punctured $\delta$ -neighbourhood of $p$ we can create, then the fact that there is an $L$ sitting in every $\epsilon$ -neighbourhood given has no bearing on the limit statement; it does no harm.","['limits', 'calculus', 'continuity', 'real-analysis']"
4741262,Has this notation for functions on finite sets been used before?,"In group theory, specifically in groups of permutations, there is a special notation used for functions. I am wondering if a generalization of this notation has been used before. Let our finite set be $\{1,...,n\}$ . Consider a word of length $n$ from that set. Let me give a specific example. Consider the set $\{1,2,3\}$ . The word $112$ represents the function that outputs $1$ to the input $1$ , $1$ to the input $2$ , and $2$ to the input $3$ . The word $232$ represents the function that outputs $2$ to the input $1$ , $3$ to the input $2$ , and $2$ to the input $3$ . I hope the examples I have given make it sufficiently clear what the notation means. My question is, is there any book or paper where this notation has been used? Or am I the first one to use it?","['notation', 'functions']"
4741281,Flux through a tetrahedron,"Im solving a problem and have stumbled can not wrap my head around the solution I get. The problem is formulated: Calculate the flux of $\vec{F}=(x,z,0)$ out from the tetrahedron $x+2y+3z=6$ in the first octant. My solution is the following: Parametrization: $r(x,y)=(x,y,\frac{6-2y-x}{3})$ with $0\leq y\leq 3-\frac{x}{2}, 0\leq x\leq 6$ , which implies $\vec{N}=(\frac{1}{3},\frac{2}{3},1)$ . Then i just evaluate the double integral: $$\int^{6}_{0}\int^{3-x/2}_{0}\vec{F}\cdot \vec{N}dydx=\int^{6}_{0}\int^{3-x/2}_{0}(x,\frac{6-2y-x}{3},0)\cdot(\frac{1}{3},\frac{2}{3},1)dydx=10.$$ I've tried recalculating several times over and can not get the correct answer of 6. Which step am i doing wrong here?","['integration', 'multivariable-calculus', 'calculus']"
4741287,Does this definition of an epimorphism work?,"I'm reading through Aluffi's Algebra Chapter 0 and one of the exercises is to come up with a definition of an epimorphism and show that a map $f \colon A \to B$ is surjective iff it is an epimorphism. In hindsight it's obvious to say that (Definition 1) $f$ is an epimorphism if for all maps $g_1, g_2 \colon B \to Z$ , if $g_1 \circ f = g_2 \circ f$ , then $g_1 = g_2$ (right-cancellation). However, when I first approached the problem I had something about existence in mind and I came up with the following diagram: Here, I wanted to define an epimorphism as follows: (Definition 2) $f$ is an epimorphism if for all maps $g \colon Z \to B$ , there exists a map $h \colon Z \to A$ such that $g = f \circ h$ . In that case, a proof would go as follows (I will assume all sets $A,B,Z$ are nonempty): Suppose $f$ is surjective. Then, for all $b \in B$ , the preimage $f^{-1}(\{b\})$ is nonempty, so we can pick an element $a_b \in A$ such that $a_b \overset{f}{\mapsto} b$ . Now if $z \overset{g}{\mapsto} b$ for $z \in Z$ , then we can let $h(z)=a_b$ . We can repeat this exercise for any $b' \in B$ that is mapped to under $g$ . On the other hand, suppose $f$ is an epimorphism according to Definition 2. Choose an element $b \in B$ and let $g$ be the constant map $g(z) \equiv b$ . Then there exists a map $h \colon Z \to A$ such that $f(h(z))=g(z)=b$ , so there is some $a \in A$ mapping to $b \in B$ under $f$ . Again we can repeat this for any $b' \in B$ . Question 1 Is my proof correct and the definition sound? Question 2 If the answer to Q1 is yes, then I assume I didn't just invent some new property in category theory but rather that this is some property that possibly has a name. (Would it qualify as a universal property?)","['abstract-algebra', 'category-theory']"
4741334,Do I have a misconception about probability?,"I have come across this text recently. I was confused, asked a friend, she was also not certain. Can you explain please? What author is talking about here? I don't understand.   Is the problem with the phrase ""on average""? Innumerable misconceptions about probability. For example, suppose I
toss a fair coin 100 times. On every “heads”, I take one step to the
north. On every “tails”, I take one step to the south. After the 100th
step, how far away am I, on average, from where I started? (Most kids
– and more than a few teachers – say “zero” ... which is not the right
answer.) In a way it is pointless to talk about misconceptions, when you don't explain the misconceptions... Source: https://www.av8n.com/physics/pedagogy.htm Section 4.2 Miscellaneous Misconceptions, item number 5","['average', 'probability']"
4741347,Number of invertible symmetric $3\times 3$ matrices over a finite field $F =\mathbb{F}_q$,"I was trying to find the number of symmetric invertible matrices of size $n\times n$ over a finite field $F= \mathbb{F}_q$ ( $q$ odd for simplicity) For $n=2$ it has to do with counting the number of zeroes of a quadratic form over a finite field, something that was seen before on this site. For $n = 3$ it is a completely new problem. I am trying it in this way: The first subset of (symmetric) invertible matrices are the one having a Gauss decomposition $$A = L D U$$ where $L$ is lower triangular unipotent, $U$ is upper triangular unipotent, and $D$ is diagonal with non-zero diagonal values. If $A$ is moreover symmetric then $L = U^t$ .  Now the number of such matrices is simple to determine. The  matrices with  a Gauss decomposition are those for which $D_1$ , $\ldots$ , $D_n \ne 0$ . where $D_k$ are the leading minors, $1\le k \le n$ . So we got a subset of the set of symmetric invertible matrices. Let's try to determine the number of $n\times n$ symmetric invertible matrices. Now, let $d_{n-1}$ the number of symmetric $(n-1)\times (n-1)$ matrices that are invertible.  From this we can complete the off diagonal elements in $q^{n-1}$ ways, and the last diagonal element in $(q-1)$ ways ( use the Cauchy formula for expanding the determinant ). The problem is what happens if the $D_{n-1}$ leading minor is in fact zero. Now its rank starts to matter.  Perhaps I could cover the case $n=3$ , but it looks a bit confusing. Notes: I am aware there exists a paper that solves this problem ( symmetric matrices with $0$ determinant over the ring $\mathbb{Z}_m$ ), but it seems hard to read.   Even inequalities would be helpful. Any feedback would be welcome!","['matrices', 'finite-fields', 'abstract-algebra', 'finite-groups']"
4741363,Is there a nonmeasurable subset of $\mathbb{R}^2$ that is $1-$dimensional Hausdorff measurable?,"For $n\in\mathbb{N}^*$ and $s\in\mathbb{R}_{\ge 0}$ , the $s-$ dimensional Hausdorff measure $H^s$ is an outer measure over $\mathbb{R}^n$ , and the $\sigma-$ algebra of $s-$ dimensional measurable subsets of $\mathbb{R}^n$ is given by Carathéodory's criterion. It is well-known that $H^0$ is the counting measure and that $H^n$ is the Lebesgue outer measure over $\mathbb{R}^n$ . Giving a subset of $\mathbb{R}^2$ that is $H^2-$ (i.e. Lebesgue) but not $H^1-$ measurable seems natural: take $V$ to be a Vitali set, then $V\times \{0\}$ works. I was wondering if we could have a subset of $\mathbb{R}^2$ that is $H^1-$ but not $H^2-$ measurable?","['measure-theory', 'hausdorff-measure', 'outer-measure']"
4741371,Solve $\int_0^\infty\frac x{e^x-e^{\frac x2}}dx$,"I was able to solve the integral $$\int_0^\infty\frac x{e^x-e^\frac x2}dx=4\left(\frac{\pi^2}6-1\right)$$ I want to see other approaches to solving it. Here is my solution: $$\int_0^\infty\frac x{e^x-e^\frac x2}dx=\int_0^\infty\frac{xe^{-x}}{1-e^{-\frac x2}}dx=\int_0^\infty xe^{-x}\sum_{k=0}^\infty e^{-\frac{kx}2}dx=\sum_{k=0}^\infty\int_0^\infty xe^{-(1+\frac{kx}2)x}dx=\sum_{k=0}^\infty\frac1{(1+\frac k2)^2}\int_0^\infty xe^{-x}dx=\sum_{k=0}^\infty\frac1{(1+\frac k2)^2}=\sum_{k=0}^\infty\frac{4}{(k+2)^2}=4\left(\frac{\pi^2}6-1\right)$$ I came up with this integral when I was trying to prove that $\int_0^\infty\frac x{e^x-1}dx$ converges using the comparison test. I replaced the $1$ with $e^x/2$ while checking on Desmos whether this integral exists or not, but I put $e^{x/2}$ instead. I decided to have a go at this integral.","['integration', 'summation', 'definite-integrals', 'calculus', 'sequences-and-series']"
4741406,Minimum value of $\sec(x)+\sec(y)$.,"If $x+y=2\alpha.$ Then minimum value of $\displaystyle \sec(x)+\sec(y)$ , where $\displaystyle x,y\in\bigg(0,\frac{\pi}{2}\bigg)$ What I try $\displaystyle \sec(x)+\sec(y)=\frac{\cos(x)+\cos(y)}{\cos(x)\cos(y)}$ $\displaystyle =2\cos(\frac{x+y}{2})\cos(\frac{x-y}{2})\cdot \frac{2}{\cos(x+y)\cos(x-y)}$ $\displaystyle =2\cos(\alpha)\cos(\frac{x-y}{2})\cdot \frac{2}{\cos(2\alpha)+\cos(x-y)}$ Put $y=2\alpha-x,$ We get $\displaystyle =\frac{4\cos(\alpha)\cdot \cos(x-\alpha)}{\cos(2\alpha)+\cos(2x-2\alpha)}$ How do I solve it after that , please have  look , Thanks",['trigonometry']
4741444,Example of $\int \frac{df}{dt}dx \neq \frac{d}{dt} \int fdx$,"A sufficient condition $\int \frac{df}{dt}dx = \frac{d}{dt} \int fdx$ holds is Will moving differentiation from inside, to outside an integral, change the result? , or When do the partial derivative and integral of different variables commute? . On the other hand, an example of $\int \frac{df}{dt}dx \neq \frac{d}{dt} \int fdx$ is Derivative commuting over integral . This example, $f$ is discontinuous function. I want to know example of continuous function $f$ such that $x \mapsto f(t, x)$ is integrable for all $t$ , $t \mapsto f(t, x)$ is differentiable for all $x$ , $\int \frac{df}{dt}dx \ $ and $\  \frac{d}{dt} \int fdx$ exist in $\mathbb{R}$ , and $\int \frac{df}{dt}dx \neq \frac{d}{dt} \int fdx$ . i.e. an integrable function $g$ such that $$\left| \frac{\partial f}{\partial t}(t, x) \right| \le g(x).$$ doesn't exist. ( Will moving differentiation from inside, to outside an integral, change the result? ) Any advise would be appreciated.","['integration', 'functions', 'derivatives']"
4741481,Compute the Integral $\int_{\frac{1}{e}}^{e}\frac{1}{\left(1+x^{2}\right)\left(1+x\left(\ln x\right)^{7}\right)}dx$,"$$I=\int_{\frac{1}{e}}^{e}\frac{1}{\left(1+x^{2}\right)\left(1+x\left(\ln x\right)^{7}\right)}dx$$ Well my first thoughts after seeing this problem were substituting $x \to \frac{1}{x}$ and Feynman Integration Technique due to the $7$ as the power of $\ln(x)$ . The substitution leads to: $$I=\int_{\frac{1}{e}}^{e}\frac{x}{\left(1+x^{2}\right)\left(x-\left(\ln x\right)^{7}\right)}dx$$ which according to me does not seem to be leading anywhere. Differentiating under the Integral Sign by Introducing a parameter by replacing $7$ does not help either. $$I(\alpha)=\int_{\frac{1}{e}}^{e}\frac{1}{\left(1+x^{2}\right)\left(1+x\left(\ln x\right)^{\alpha}\right)}dx$$ I did try to numerically integrate it and use Integer Relation Algorithms, but that did not give me any results. Though from what I have numerically calculated $$\lim_{\alpha\to\infty}I(\alpha)=\frac{e}{\pi}$$ But don't take my word on it, as I am not sure about this result. EDIT: Thank you to @ClaudeLeibovici for showing that this result is incorrect. I think the closeness of the values ( $\frac{e}{\pi}$ ) might just be a mere coincidence. Also it seems as $\alpha \to \infty$ , the whole graph falls within the lines $x=\frac{1}{e}$ and $x=e$ which are indeed the limits of integration. $$\lim_{\alpha\to\infty}\int_{\frac{1}{e}}^{e}\frac{1}{\left(1+x^{2}\right)\left(1+x\left(\ln x\right)^{\alpha}\right)}dx \stackrel{?}{=} \lim_{\alpha\to\infty}\int_{0}^{\infty}\frac{1}{\left(1+x^{2}\right)\left(1+x\left(\ln x\right)^{\alpha}\right)}dx$$ EDIT:
I have found the original source for the Integral: Romanian Mathematical Magazine, Page 25 Here is an Image: It seems the solution posted is wrong, in the third line of the solution, the $x^2$ term mysteriously disappears from the numerator. Apologies to everyone who worked on this problem, I do not have any reason to believe that the Integral might have a closed form.","['integration', 'calculus']"
4741491,Is there an exponential lower bound for the chromatic number?,"Let $n$ be a positive integer. Define the Hamming distance $d_H(x,y)$ of $x,y\in\{0,1\}^n$ by $$d_H(x,y)=|\big\{i\in\{0,\ldots,n-1\}:x(i)\neq y(i)\big\}|.$$ For integers $n>1$ and $k$ with $1\leq k<n$ let $G_{n,k}$ be the graph defined on the vertex set $\{0,1\}^n$ such that two nodes $x,y$ are connected by an edge if and only if $d_H(x,y) =k$ . From small scale numerical experiments it appears that for $k$ even and linear in $n$ , the chromatic number of $G_{n,k}$ increases exponentially with $n$ . For odd $k$ it is straightforward to prove that the chromatic number is $2$ . Is there an exponential lower bound for the chromatic number $\chi(G_{n,k})$ when $k$ is $\lfloor n/2 \rfloor$ and even?","['graph-theory', 'additive-combinatorics', 'combinatorics', 'coloring']"
4741499,Random-like walk based on the parity of digits of a normal number: must it return to the origin?,"Using the sequence of digits of a normal number , create a random-like walk: starting with the first term, if a term is odd then move up one unit; if it is even then move down one unit. Will the walk necessarily return to the origin? Example Consider the sequence of digits of the Copeland-Erdos constant , which is formed by concatenating the sequence of prime numbers in base $10$ : $2,3,5,7,\color{red}{1},\color{red}{1},\color{blue}{1},\color{blue}{3},\color{orange}{1},\color{orange}{7},\color{brown}{1},\color{brown}{9},\color{green}{2},\color{green}{3},\dots$ ( A033308 ) Here is the graph of position against step number, for the first $2000$ steps. (Actually, we know that the walk returns to the origin, because it does so on the second step; so just consider the walk to begin after the first two steps. The approximately horizontal section corresponds to the prime numbers from $2003$ to $2999$ .
Numerical data for this example can be found at this question , which my question seeks to generalize.) If the terms in the sequence behave like random numbers, then the answer would be yes . But the sequence is not random; they can be predicted. I don't know what this implies about the whether the walk will return to the origin.","['number-theory', 'random-walk', 'prime-numbers', 'sequences-and-series']"
4741552,Throwing two dice probability problem,"We got the following problem from last week. They say my solution is not correct, but I don't know where I go wrong. Problem: We throw with two dice until the sum of the results equals $10$ two times. What is the probability that we throw exactly eight times a sum smaller than $10$ until this event occurs? (For example if we throw the first time $3$ , $4$ , second time $5$ , $5$ and third time $6$ , $4$ then we finish the experiment. The sums until the second $10$ were $7$ , $10$ and $10$ .) My solution: The probability of throwing exactly $10$ in one throw is $$p_{\left(=10\right)}\overset{\cdot}{=}\left(\frac{1}{6}\right)\left(\frac{1}{6}\right)+\left(\frac{1}{6}\right)\left(\frac{1}{6}\right)+\left(\frac{1}{6}\right)\left(\frac{1}{6}\right)=3\left(\frac{1}{6}\right)^{2}.$$ The probability of throwing more than $10$ in one throw is $$p_{\left(>10\right)}\overset{\cdot}{=}\left(\frac{1}{6}\right)\left(\frac{1}{6}\right)+\left(\frac{1}{6}\right)\left(\frac{1}{6}\right)+\left(\frac{1}{6}\right)\left(\frac{1}{6}\right)=3\left(\frac{1}{6}\right)^{2}.$$ So the probability of throwing less than $10$ in one round is $$p_{\left(<10\right)}\overset{\cdot}{=}1-3\left(\frac{1}{6}\right)^{2}-3\left(\frac{1}{6}\right)^{2}=1-6\left(\frac{1}{6}\right)^{2}.$$ The probability of throwing $\xi=1$ ""exactly $10$ "" and throwing $\eta=8$ ""less than $10$ "" from $N-1=n$ throwings is $$\mathbf{P}\left(\xi=1,\eta=8\right)=\frac{n!}{1!8!\left(n-1-8\right)!}p_{\left(=10\right)}^{1}p_{\left(<10\right)}^{8}\left(1-p_{\left(=10\right)}-p_{\left(<10\right)}\right)^{n-1-8}.$$ The distribution of the number of the last throwing can be calculated as follows: $$\mathbf{P}\left(N=m\right)=\frac{\left(m-1\right)!}{1!8!\left(\left(m-1\right)-1-8\right)!}p_{\left(=10\right)}^{1}p_{\left(<10\right)}^{8}\left(1-p_{\left(=10\right)}-p_{\left(<10\right)}\right)^{\left(m-1\right)-1-8}\cdot p_{\left(=10\right)}$$ So the asked probability using the total law of probabilities is: $$\sum_{n=9}^{\infty}\mathbf{P}\left(\left(\xi=1,\eta=8\right)\cap\left(N-1=n\right)\right)=\sum_{n=9}^{\infty}\mathbf{P}\left(\xi=1,\eta=8\mid N-1=n\right)\cdot\mathbf{P}\left(N-1=n\right)=$$ $$=\sum_{n=9}^{\infty}\frac{n!}{1!8!\left(n-1-8\right)!}p_{\left(=10\right)}^{1}p_{\left(<10\right)}^{8}\left(1-p_{\left(=10\right)}-p_{\left(<10\right)}\right)^{n-1-8}\cdot\frac{n!}{1!8!\left(n-1-8\right)!}p_{\left(=10\right)}^{1}p_{\left(<10\right)}^{8}\left(1-p_{\left(=10\right)}-p_{\left(<10\right)}\right)^{n-1-8}\cdot p_{\left(=10\right)}$$ , which is $\simeq0.0047101892$ according to WolframAlpha. Can you see my mistake?","['probability-distributions', 'probability-theory', 'probability']"
4741563,"How do I prove the ""mean value property"" for Helmholtz equation?","Suppose $u: \mathbb{R}^2 \to \mathbb{R}$ is a $C^2$ function that satisfies the Helmholtz equation $-\Delta u = \lambda u$ for $\lambda \in \mathbb{R}$ . I am trying to prove something that looks like a modified mean value property, namely that $$\frac{1}{2\pi r}\int_{\partial B(z_0,r)} u \,\mathrm{d}\Gamma = \begin{cases} u(z_0) \cdot J_0(\sqrt{\lambda} r) &\quad \text{if } \lambda > 0\\ u(z_0) &\quad\text{if } \lambda = 0\\ u(z_0) \cdot I_0(\sqrt{-\lambda} r) &\quad\text{if }\lambda < 0\end{cases}$$ I used polar coordinates and tried to approach the equation with separation of variables, saying that $u(r, \theta) = R(r) \Theta(\theta)$ . I got $\Theta(\theta) = C_1 e^{i m \theta} + C_2 e^{- i m \theta}$ , from the equation $$ \partial_{\theta}^2 \Theta = - C \Theta $$ For $R(r)$ , I got the (modified / standard, depending on $\lambda$ ) Bessel equation. However, I don't know how to proceed. If $\Theta(\theta)$ was a harmonic function, I could simply integrate and put the Bessel part before the integral and then apply the mean value theorem. However, I am stuck, since it is not a harmonic function. Ultimately, I have three questions; What is the justification for using separation of variables? I have used it, but I don't know if I can, since the statements is phrased for every solution of the Helmholtz equation; what if there are solutions that don't have separated variables? When solving the Bessel equation, we get multiple solutions $J_m$ and when solving the modified Bessel equations, we get multiple solutions $I_m$ . Why are there only $J_0$ and $I_0$ in the final result? As mentioned above, how do I proceed to actually show this property?","['special-functions', 'harmonic-analysis', 'real-analysis', 'complex-analysis', 'bessel-functions']"
4741601,Generalise logarithmic integral $ \int_0^1\ln^n (\frac{1-x}{1+x})\mathrm dx$ related to Zeta function,"Context: I've been recently putting integrals of some random functions in online calculators for fun. An interesting kind of logarithmic integrals that I observed are as follows : \begin{align}
&\int_0^1\ln^2\left(\frac{1-x}{1+x}\right)\mathrm dx= \frac{\pi^2}{3}=2\zeta(2)\\
&\int_0^1\ln^3\left(\frac{1-x}{1+x}\right)\mathrm dx=-9\zeta(3)\\
&\int_0^1\ln^4\left(\frac{1-x}{1+x}\right)\mathrm dx = \frac{7\pi^4}{15}=42\zeta(4)\\
&\int_0^1\ln^5\left(\frac{1-x}{1+x}\right)\mathrm dx=-225\zeta(5)\\
&\int_0^1\ln^6\left(\frac{1-x}{1+x}\right)\mathrm dx=\frac{31\pi^6}{21}= 1395\zeta(6)
\end{align} Question: How can we generalise this for : $$I_n = \int_0^1\ln^n\left(\frac{1-x}{1+x}\right)\mathrm dx$$ Notation: The superscript at logarithm means for exponentiation.","['integration', 'calculus', 'zeta-functions', 'definite-integrals']"
4741687,"When laying trominos on an 8x8, where must the empty square be? [duplicate]","This question already has an answer here : Simple chessboard exercise (1 answer) Closed 11 months ago . 21 tromino tiles (three squares glued together in a row) can together
cover 63 squares on an $8 \times 8$ grid. One of the squares thus
becomes non-covered. What are the possible positions where the empty
square can be? If this is the relevant object to consider, and it should be arranged only horizontally, being an odd number, it cannot fill any row in the 8x8 grid, without leaving 2 squares unfilled. These two squares are left unfilled for each row, and can be filled by putting a tile on ever row on the empty remaining 2 squares. This leaves one square  of the tile outside the 8x8 grid.  How can the question assume that  63 squares are filled out of 64, when this combination covers all 64? Thanks",['combinatorics']
4741712,Selberg's Trace Formula for Hecke Eigenvalues,"I am looking for a reference (if one exists) for an application of Selberg's trace formula after the Hecke operators have been applied. Perhaps to give some context and notation to this, so my question is clear. Perhaps, we should just let $\Gamma=\text{SL}(2,\mathbb{Z})$ (or in general a cocompact lattice acting on $\mathbb{H}$ ). For simplicity, we can let $X=\Gamma\backslash\mathbb{H}$ . Then if $k:\mathbb{H}\times\mathbb{H}$ is a point pair invariant, and $K(z,w)=\sum_{\gamma\in\Gamma}k(\gamma z,w)$ is the averaging of $k$ over $\Gamma$ . Then if we define the operator $T_k:L^2(X)\rightarrow L^2(X)$ defined by $f\mapsto \int_Xk(z,w)d\mu(w)$ . Now if we take $\{u_m(z)\}$ be a complete set of eigenvectors for $T_k$ such that $T_ku_m=\lambda_k u_m$ , then considering the spectral parameters $t_m$ where $\lambda_m=\frac{1}{4}+t_m^2$ , then we will have that $T_k u_m=h(t_m)u_m$ and $h$ is a suitable test function. Now we have that Selberg's pretrace formula says that $$
K(z,w)=\sum_{m=0}^\infty h(t_m)u_m(z)u_m(w)
$$ Now the classical Selberg's trace formula is obtained from the above by setting $z=w$ and integrating over $X$ . This gives $$
\sum_{m=0}^\infty h(t_m)=\int_X K(z,z)d\mu(z)=\int_{\Gamma\backslash\mathbb{H}}\sum_{\gamma\in\Gamma}k(\gamma z,z)d\mu(z)
$$ Then breaking up the righthand side up into conjugacy classes and examining how the integrals work gives the classical version of Selberg's trace formula. What I am interested is a version of Selberg's trace formula involving the Hecke eigenvalues (as the $\lambda_m$ are the Laplace eigenvalues). Thus, if I let $T_n$ be the Hecke operator, and if I choose my basis $u_m$ to also simultaneously be Hecke eigenfunctions such that $T_nu_m=\lambda_m(n)$ (I know notation is confusing, but I will use $\lambda_m(n)$ to be the $n$ th Hecke eigenvalue associated to the function $u_m$ ). Now if we start off with Selberg's pretrace formula, and we apply the Hecke operator $T_n$ (in the variable $z$ ), let's start with the spectral side (i.e. the righthand side): $$
T_n\left(\sum_{m=0}^\infty h(t_m)u_m(z)u_m(w)\right)=\sum_{m=0}^\infty h(t_m)\lambda_m(n)u_m(z)u_m(w)
$$ Then setting $z=w$ , and integrating over $X$ , and using the fact we choose the $u_m$ to be orthonormal, we will have that the left-hand side will become $$
\sum_{m=0}^\infty h(t_m)\lambda_m(n)
$$ Now if we take the right-hand side and we apply $T_n$ , then we find that $$
T_n\left(K(z,w)\right)=T_n\left(\sum_{\gamma\in\Gamma}k(\gamma z,w)\right)=\sum_{\gamma\in R(n)}k(\gamma z,w)
$$ where we have $R(n)$ denote integral matrices of determinant $n$ . Then if we set $z=w$ and integrate we should get the other side of the trace formula. Now, I am curious if there is a reference that goes through this side of the trace formula after applying the Hecke operators (going through the identity, elliptic, parabolic, and hyperbolic contributions). If such a reference is known as I am sure it might be a lot of details I would appreciate to see it.","['number-theory', 'trace', 'automorphic-forms', 'reference-request', 'analytic-number-theory']"
4741718,Compare the piecewise constant interpolation and piecewise linear interpolation,"Let $X$ be a Banach space and $\{u_{k}\}$ be a sequence in $X$ with $i\in\mathbb{N}_{0}$ . Let $h>0$ be a constant and set $t_k=kh$ and $u(t_k)=u_{k}$ with $k\in\mathbb{N}_0$ .
Consider the following two interpolation that is \begin{align*}
\bar{u}(t)&:=u_{i+1}\mbox{ for }t\in(t_i,t_{i+1}],\\
\hat{u}(t)&:=\frac{t_{i+1}-t}{h}u_{i}+\frac{t-t_{i}}{h}u_{i+1}\mbox{ for }t\in(t_i,t_{i+1}).
\end{align*} Now let $p\in[1,\infty]$ , I would like to compare the $L^p$ norm of these two interpolation with respect to time i.e. whether there exists $C_1>0$ and $C_2>0$ such that $$C_1\lVert \lVert\hat{u}\rVert_X\rVert_{L^p(0,\infty)}\leq\lVert \lVert \bar{u}\rVert_X\rVert_{L^p(0,\infty)}\leq C_2\lVert \lVert \hat{u}\rVert_X\rVert_{L^p(0,\infty)}.$$ Could this be true?","['numerical-methods', 'functional-analysis', 'analysis', 'real-analysis']"
4741745,Pullback of $2$-sphere volume form via Gauss map,"This is problem 17.3 from Tu's Differential Geometry text. This post appears to answer my question, but I don't follow many of the explanations, including the construction and usage of $det$ . Problem Let $M$ be a smooth, compact, oriented surface in $\mathbb{R^3}$ with $K$ the Gaussian curvature on $M$ . If $v : M \to S^2 $ is the Gauss map, prove that $$ v^* (vol_{S^2}) = K vol_M $$ Corrected Proof Lemma 1 . If $N$ is an $n$ -dimensional smooth, oriented manifold, $(U, x^1, \ldots, x^n)$ a chart a $p \in N$ with onb $\{e_1, \ldots e_n \} =: e $ , then $$ vol_N = det_e $$ Proof . If $\theta^1, \ldots \theta^n$ is dual to $e_1, \ldots e_n$ , then for any $X_1, \ldots X_n$ in $T_p N$ with $X_j = \sum a^i_j e^i$ \begin{align}
vol_N (X_1, \ldots X_n) &= \theta^n \wedge \cdots \wedge \theta^n (X_1, \ldots X_n) \\
&= \sum_{\sigma \in S_n} sgn(\sigma) a^i_{\sigma(1)} \cdots a^i_{\sigma(n)} \\
&= det_e [ a^i_j ]
\end{align} $\square$ Lemma 2 . There exists $\{ e_1, e_2 \} =: e$ principal vectors at $p$ in $M$ such that $e$ is an onb for $M$ , and $ \{ \kappa_1 e_1, \kappa_2 e_2 \} = \{ dv_p(e_1), dv_p(e_2) \} =: e'$ is an orthogonal basis for $S^2$ . We will use these facts: $dv_p = -L$ (the shape operator at $p$ ) (Problem 5.3), the $L$ has the principal vectors as eigenvectors and principal curvatures as eigenvalues (Prop 5.6), and $L$ is self-adjoint. Note that if $\kappa_1 = \kappa_2$ , all vectors in $T_p M$ are principal, so we can freely choose an onb. So assume they're unequal. Then \begin{align}
\kappa_1^2 \langle e_1, e_2 \rangle &= \langle -L (e_1), -L (e_2) \rangle \\
&=  \langle dv_p (e_1), dv_p (e_2) \rangle \\
&= \kappa_2^2 \langle e_1, e_2 \rangle
\end{align} Because the principal curvatures are not equal, we must have $$\langle e_1, e_2 \rangle = 0 = \langle dv_p (e_1), dv_p (e_2) \rangle = \langle \kappa_1 e_1, \kappa_2 e_2 \rangle.$$ $\square$ Note that Lemma 2 implies that $e$ is an onb for both $T_p M$ and $T_p S^2$ . Now if $K(p) = det(J_v(p)) = 0$ then the claim obviously holds because $v^* vol_{S^2} = 0 = K(p) vol_M$ . So assume $K(p) = det(J_V(p)) \neq 0$ . Applying Lemma 2, Choose onb $e$ for $M$ (which is also onb for $S^2$ ). Let $\Theta^1 \wedge \Theta^2$ be the volume form for $S^2$ (with $\Theta^i$ dual to $e_i$ ). Then for any $X_1, X_2$ vectors in $S^2$ with $X_j = \sum x^{i}_j e_i$ . \begin{align}
v^* (vol_{S^2})(X_1, X_2) &= \Theta^1 \wedge \Theta^2 (dv_p X_1, dv_p X_2) \\
&= \Theta^1(dv_p X_1) \Theta^2(dv_p X_2) - \Theta^1(dv_p X_2) \Theta^2(dv_p X_1) \\
\text{(Lemma 2)} &= (\kappa_1 x^1_1) (\kappa_2 x^2_2) - (\kappa_1 x^1_2) (\kappa_2 x^2_1) \\
&= K det_e(X_1, X_2) \\
\text{(Lemma 1)} &= K vol_M
\end{align} $\square$ Update Special thanks to @cbishop for providing the key insight (Prop 8.2.1) that helped me solve the problem!","['curvature', 'riemannian-geometry', 'differential-geometry']"
4741748,"Methods to evaluate $\int_{0}^{\infty}{e^{-\sigma x^n}\cos(ax^n)\, dx}$","I was trying some things out to evaluate $\int_{0}^{\infty}{e^{-\sigma x^n}\cos(ax^n)\, dx}$ but I couldn't figure out some of the details. For example : $$\begin{align}I&=\int_{0}^{\infty}{e^{-\sigma x^n}\cos(ax^n)\, dx}\\
&=\Re\int_{0}^{\infty}{e^{-x^n(\sigma-ai)}\, dx}\\
&=\Re\frac{1}{n}\int_{0}^{\infty}{e^{-t(\sigma-ia)}t^{1/n-1}\, dt}\\
&=\Re\frac{1}{(\sigma - ia)^{n+1}}\Gamma\left(\frac{1}{n}
+1\right)\end{align}$$ with the last bit using $\int_{0}^{\infty}{e^{-st}t^n\,dt}=s^{-(n+1)}\Gamma(n+1)$ and $n\Gamma(n)=\Gamma(n+1)$ . The formula doesn't seem right, but WFA says $\int_{0}^{\infty}e^{-5x^3}\cos(3x^3)\, dx \approx0.469$ but the formula says it's $-0.000480\dots$ (checked [here][1]) Another attempt : $$
\begin{align}
I&=\int_{0}^{\infty}{e^{-\sigma x^n}\cos(ax^n)\, dx}\\
&=\int_{0}^{\infty}{e^{-\sigma x^n}\sum_{k=0}^{\infty}{(-1)^k}\frac{(ax^n)^{2k}}{(2k)!}\, dx}\\
&=\sum_{k=0}^{\infty}{(-1)^k\frac{a^{2k}}{(2k)!}\int_{0}^{\infty}e^{-\sigma x^n}(x^n)^{2k}\,dx}\\
&=\frac{1}{n}\sum_{k=0}^{\infty}{(-1)^k\frac{a^{2k}}{(2k)!}}\int_{0}^{\infty}{e^{-\sigma t}t^{2k+1/n-1}\, dt}\\
&=\frac{1}{n\sigma}\sum_{k=0}^{\infty}{(-1)^k\frac{a^{2k}}{(2k)!}}\int_{0}^{\infty}{e^{-u}\left(\frac{u}{\sigma}\right)^{2k+1/n-1}\, du}\\
&=\frac{1}{n\sigma^{1/n}}\sum_{k=0}^{\infty}{(-1)^k\frac{\left(\frac{a}{\sigma}\right)^{2k}}{(2k)!}\Gamma\left(2k+\frac{1}{n}\right)}
\end{align}
$$ Can the sum be simplified in this case and how can the first method be amended? EDIT : Found the issue with the first attempt, instead I should've used $\int_{0}^{\infty}{e^{-st}t^{1/n-1}\, dt}=s^{-1/n}\Gamma(1/n)$ which then yields the correct answer.
So $I=\Re\frac{1}{(\sigma - ia)^{1/n}}\Gamma\left(\frac{1}{n}
+1\right)$ Maybe it can be simplified further?
[1]: https://www.wolframalpha.com/input?i=int%20e%5E%28-5x%5E3%29cos%284x%5E3%29%200..inf","['integration', 'laplace-transform', 'fourier-transform', 'real-analysis', 'complex-analysis']"
4741749,How is the fundamental theorem of calculus dependent on orientation?,"Using standard Lebesgue integration, we can write: \begin{equation}
\int_{(a,b)}f'(x) d\lambda = f(b) - f(a)
\end{equation} There's no orientation on the left hand side of the equation, yet on the right we take $f(b) - f(a)$ as opposed to $f(a) - f(b)$ . Due to what exactly is that the case? In what piece is the ""orientation"" $a \rightarrow b$ encoded? // My thoughts:
I'd guess it's due to the fact that the derivative is in some sense defined in a slightly arbitrary way - both $\lim \frac{f(x+h)-f(x)}{h}$ and $\lim \frac{f(x)-f(x+h)}{h}$ make sense. But at the same time, it doesn't seem that arbitrary - it's natural to assume that numerator/enumerator order should be preserved, i.e. $\frac{f(x+h) - f(x)}{(x+h) - (x)}$ is the natural choice. So I think I might be missing something.","['calculus', 'differential-forms', 'real-analysis']"
4741799,How to eliminate $\dot{x}$ term in differential equation $\ddot{x} + p(t) \dot{x} + q(t) x = 0$?,"Vladimir Arnold's Ordinary Differential Equations (page 252) states that ""the more general equation $\ddot{x} + p(t) \dot{x} + q(t) x = 0$ is easily reduced to the form $\ddot{x} + q(t) x = 0$ "". The book doesn't explain how to do this; I guess it's assumed to be obvious, but it's not obvious to me. Can someone please explain?",['ordinary-differential-equations']
4741806,What are the advantages to viewing Hermitian symmetric domains as a moduli for Hodge structures?,"I am reading Milne's An Introduction to Shimura Varieties. At the moment, I am unable to see the forest from the trees with Deligne's approach. Here's the basic set-up. Let $X$ be a Hermitian symmetric domain. Milne claims these are all simply connected (I know of no proof), he considers variation of Hodge structures in which the local systems $H^n(V_s,\mathbb{Q})$ are constant (recall each of these carry a Hodge decomposition / structure and these are required to ary continuations with $s\in X$ ). Going backwards, Milne shows how to give the set of Hodge structures on a vector space $V$ with some conditions the structure of a Hermitian symmetric domain. All questions below are more or less related / different aspects of a bigger question: Why variation of Hodge structures for Shimura variety? Question 1: What are the advantages to this point of view? What are the things a student should be looking out for? Question 2: Going backwards, let me try an example and hopefully someone has insight. Let $X$ be the Hermitian symmetric domain $\mathcal{H}_1$ , the upper half plane. What are the associated variation of Hodge structures one studies here? Question 3: Shimura varieties play a role in the Langlands program. How does / Does the variation of Hodge structures enter play in this story as well? Edit: I am convinced, due to the lack of good references for Shimura varieties, that if I find the correct answer to this, it is worth providing a detailed write-up later. If I have time, I will make this questions more precise.","['number-theory', 'differential-geometry']"
4741891,Abelian subgroups of the group of automorphisms of a finite group,"This is a follow-up question from my post here , which has been moved according to a comment. For context, here is the setup. Let $G$ be a nontrivial finite group. In his book ""Finite Group Theory"", M. Isaacs proves the following two results: Corollary 3.3 : Let $\sigma \in \operatorname{Aut}(G)$ . Then, $o(\sigma) < |G|$ . Corollary 3.4 : Let $P$ be an abelian $p$ -subgroup of $\operatorname{Aut}(G)$ and suppose $p \nmid |G|$ . Then, $|P| < |G|$ . When we consider both of the results together, I'm led to the following: Question : If $A \leq \operatorname{Aut}(G)$ is an abelian group of order $n = p_1^{k_1} \cdots p_m^{k_m}$ such that $p_1, p_2, ..., p_m \nmid |G|$ , is it true that $|A| < |G|$ ? We can, of course, decompose $A = P_1 \times P_2 \times ... \times P_m$ , where $P_j$ is a $p_j$ -group. From 3.4 , we know $|P_j| < |G|$ , so $|A| < |G|^m$ . This suggests that this question is false, since we could, in principle, find ""big"" $p_j$ -subgroups of $\operatorname{Aut}(G)$ and take their direct product, but I don't have any examples to back it up. Any help is appreciated! Thanks in advance!","['automorphism-group', 'group-theory', 'semidirect-product', 'abelian-groups']"
4741894,A somewhat intriguing dilogarithmic integral,"The following problem has been recently proposed by C.I. Valean , $$\int_0^1 \frac{\displaystyle \left(\operatorname{Li}_2\left(\frac{1-x}{2}\right)\right)^2}{1+x} \textrm{d}x$$ $$=\frac{3}{16}\zeta(5)-\frac{1}{4}\zeta(2)\zeta(3)+\frac{5}{8}\log(2)\zeta(4)-\frac{1}{6}\log^3(2)\zeta(2)+\frac{1}{20}\log^5(2).$$ One of the author's solutions makes use of the following ( very useful ) Cauchy product, $$\frac{(\operatorname{Li_2}(x))^2}{1-x}=\sum_{n=1}^{\infty}x^n\left((H_n^{(2)})^2-5 H_n^{(4)}+4\sum_{k=1}^n\frac{H_k}{k^3}\right),$$ which appears in More (Almost) Impossible Integrals, Sums, and Series: A New Collection of Fiendish Problems and Surprising Solutions (2023) , in Sect. 4.5 , page 398 (see the top of the page), which is the sequel of (Almost) Impossible Integrals, Sums, and Series (2019) . We observe the main integral, after the variable change $x\mapsto 1-x$ , may be put in the form $$\frac{1}{2}\int_0^1 \frac{\displaystyle \left(\operatorname{Li}_2\left(\frac{x}{2}\right)\right)^2}{1-x/2} \textrm{d}x,$$ and from here we get manageable results, all easily derived with results from the mentioned books. For example, at this point we might find helpful to know and use generating functions like $$\sum_{n=1}^{\infty} x^n (H_n^{(2)})^2$$ $$=\frac{1}{1-x}\biggr(-4\zeta(4)-4\zeta(3)\log(1-x)-2\zeta(2)\log^2(1-x)-\frac{1}{6}\log^4(1-x)$$ $$+\frac{2}{3}\log(x)\log^3(1-x)+(\operatorname{Li_2}(x))^2+4\log(1-x)\operatorname{Li_3}(x)-3\operatorname{Li_4}(x)$$ $$+4\operatorname{Li_4}(1-x)-4\operatorname{Li_4}\left(\frac{x}{x-1}\right)\biggr),$$ which appears in the first-mentioned book (and visiting further its Sect. 3.51 is definitely illuminating for the next steps to do). Another solution idea: we could start with letting $x\mapsto 2x-1$ and then exploiting the Dilogarithm reflection formula, in an effort to reduce everything to manageable (known) results. The interesting question: Now, as previously seen, the path described above involves the use of harmonic series. May we design ways that avoid the use of harmonic series and then elegantly get the desired closed form?","['integration', 'definite-integrals', 'real-analysis', 'calculus', 'sequences-and-series']"
4741897,Good Book on survival analysis,"I am looking for a comprehensive textbook on survival analysis, either in German or English, from the basics to modern methods including competing risks. It should be easier to understand than the Kalbfleisch & Prentice book ""The Statistical Analysis of Failure Time Data"", which has some terrible notations, e.g. in the unification of discrete, continuous and mixed distributions, many references to derivations later in the book, and inconsistent terms - for example, lambda stands for both the hazard function and the parameter of the exponential distribution, and in places where both are discussed, it is not always clear what is meant.","['statistics', 'book-recommendation', 'analysis', 'reference-request', 'probability']"
4741940,The connectivity of a simple graph,"Let $G = \langle V, E \rangle$ be a simple graph such that $|V| = 7$ and $|E| = 16$ . Denote the number of G's components by $\alpha$ . Is it necessary that $\alpha = 1$ ? I think that the statement is indeed true - by observing the complete graph $K_{7}$ which represents the most simple scenario (.i.e G is $K_{7}$ ), it seems intuitively that I could find a path between any two vertices on the complete graph after ""removing"" the five edges too. However, my struggle is in formalizing it\find the cases which I need to observe between. Obviously that one case is when there exists some $v \in V$ which satisfies $deg(v) = 6$ . Otherwise, I claimed that each vertex is at least of degree 1 and since $|V|$ is odd, by the Handshaking lemma we get that there is some $u\in V$ s.t $deg(v) \in$ { $2, 4$ }. But still, I don't see how it leads into a contradiction. I'd like for some clarification/ideas, thanks.","['graph-theory', 'graph-connectivity', 'combinatorics', 'discrete-mathematics']"
4742000,Difficulty understanding how to deform these oriented circles continuously and make the following links (would like a drawing):,"From Hatcher's Algebraic Topology ( available here ) Page 22 : I don't understand how he manages to get a continuous deformation into the loops without ""cutting"" some things at point $x_0$ for both of the examples (and thus violating the rules of a homeomorphism). He mentions needing to pass the ""string"" through itself to get these, but I would like to see a sequence of intermediate steps. A drawing for either would be nice.","['proof-explanation', 'general-topology', 'fundamental-groups', 'algebraic-topology']"
4742002,"If the complex roots of $x^3-x-2=0$ are $r\pm si$, and $As^6 +Bs^4 + Cs^2 =26$ for integers $A$, $B$, $C$, find $A+B+C$","The question: In the cubic $x^3-x-2=0$ , there is one real root and two complex roots of the form $r\pm si$ , with $r$ and $s$ real. If there exists integers $A,B,$ and $C$ such that $As^6 +Bs^4 + Cs^2 =26$ , find A+B+C. What I've done: I expanded $(x-a)(x-r-si)(x-r+si)$ where $a$ is the real root, and matched coefficients with $x^3-x-2$ to get relationships such as $-2x^2r-ax^2=0$ , upon which $a=-2r$ . Another (I think) important relationship I got was $r^2+s^2=\dfrac{2}{a}$ . Using these, i substituted $r= \dfrac{-a}{2}$ into $r^2+s^2 = \dfrac{2}{a}$ to get that $s^2 = \dfrac{2}{a} - \dfrac{a^2}{4}$ . However, substituting this into $As^6 + Bs^4 + Cs^2 = 26$ , I have not been able to make progress.","['contest-math', 'algebra-precalculus', 'polynomials', 'cubics']"
4742013,"Show that $\bar{Y} - \min(Y_{1}, \dots, Y_{n})$ is independent of $\min(Y_{1}, \dots, Y_{n})$","Suppose that $Y_1, \dots, Y_n$ are i.i.d observations from the density $f(y, \theta, \beta) = \beta e^{-\beta(y - \theta)}I_{[y \geq\theta]}$ where $\beta \gt 0$ , $\theta \in \mathbb{R}$ are unknown parameters. Let $(T_1, T_2) = (\min(Y_1, \dots, Y_n), \bar{Y})$ . I want to show that $T_2 - T_1$ is independent of $T_1$ for all values of $(\theta, \beta)$ . I already proved that $T_1$ is complete sufficient for $\theta$ when $\beta$ is fixed and known, and that $T_2$ is complete sufficient for $\beta$ when $\theta$ is fixed and known. Clearly, $T_2 - T_1$ is an ancillary statistic of $\theta$ . Since $T_1$ is complete sufficient for $\theta$ when $\beta$ is fixed and known, can we just use Basu's theorem to conclude that $T_2 - T_1$ is independent of $T_1$ for all values of $(\theta, \beta)$ ?","['statistical-inference', 'statistics', 'order-statistics', 'sufficient-statistics']"
4742029,Does the Central Limit Theorem Work for a Single Sample?,"This is a question that I am struggling to understand. Suppose there is a university with 100,000 students (population) I take a sample of 100 students and measure the height of each student Then, I take the average height of these 100 students. Finally, I calculate the variance of this sample average and calculate a 95% Confidence Interval for this sample average This leads me to my question: - Even though I interviewed 100 students - effectively, I only have a single sample As I understand, the Central Limit Theorem states that the mean of a (large enough) sequence of samples will be Normally Distributed - Thus, is it correct to use the Central Limit Theorem in this question to construct a Confidence Interval for the average height of the entire population? In this problem, I understand that I can easily calculate the Standard Deviation of the sample itself - that is, how much does the height of any given student in the sample deviate  on average from the average height of all students in the sample. But in this problem, is it really correct to use the Central Limit Theorem to calculate the Confidence Interval of the average height - when all I have is a single sample? I think the Central Limit Theorem would be more applicable if many people each took a single sample of size 100 - and then calculated the average height of students in the sample they took. Then, you would have a sequence of sample means - and in this case, the Central Limit Theorem could be used to calculate the Confidence Interval of the sample mean. Is my reasoning correct? Thanks! Note: I have attempted to illustrate both situations - in Case 1, I think the Central Limit Theorem does not apply. In Case 2, I think the Central Limit Theorem does apply:",['probability']
4742092,Tail field of a sequence of events and relation between sigma algebra and dependence,"before starting I should mention that I have already read (at least) most related questions and it still doesn't make any sense to me.
I've recently started studying probability theory and I'm reading ""A first look at rigorous probability theory"" by Jeffrey Rosenthal.
following definition is stated: Given a sequence of events $A_1, A_2,\cdots$ we define their tail field by $$\tau = \cap_{i=1}^{\infty}\sigma(A_i,A_{i+1},\cdots)$$ Also he mentions In words, an event $A \in \tau$ must have the property that for any $n$ , it depends only on the events $A_n,A_{n+1},\cdots$ ; in particular, it does not care about any finite number of the events $A_n$ . The main problem for me is how generated $\sigma$ _algebra relates to being independent?
Also when I was searching to understand what's going on, I encountered that usually $\sigma$ _algebra is being treated as kind of measuring ""information"". I don't understand what information means here at all. Thanks.",['probability-theory']
4742116,How do you analyse the rank of a matrix depending on a parameter,"I have the matrix $$
\begin{pmatrix}
3-t & 3 & 2t \\
-2 & 0 & -1 \\
1 & 3 & 2+t \\
t+2 & 0 &t
\end{pmatrix}
$$ And I'm asked to evaluate the matrix's rank depending of $t$ , using the determinants and minors of the matrix. However, the solution I see in the answer book is too short and only addresses one minor $$
\begin{pmatrix}
3-t & 3 & 2t \\
-2 & 0 & -1 \\
1 & 3 & 2+t
\end{pmatrix}
$$ and its determinant equals $0$ . But I've tried different minors and they dont give always the same answer. Is it because I'm doing it wrong or I should think of something else? The answer book is this page 85 ex 30","['matrices', 'matrix-rank', 'determinant']"
4742123,Star of David theorem in Pascal's pyramid,"Few days ago I found out the Star of David theorem . Is there an analogue in Pascal's pyramid ? I tried to do some calculations on the base that the star of David is basically two equilateral triangles superimposed, so I tried to do the same with two regular thetraedron, but didn't found the same relation.","['binomial-coefficients', 'combinatorics']"
4742180,"Why are sine, cosine and tangent the ""main"" trig functions?","So I know that the main trig functions are sine, cosine and tangent, while cosecant, secant and cotangent are simply known as the ""reciporicals"", but
wouldn't it make more sense for sine, SECANT and tangent be the main trig functions while COsine, COsecant and COtan be the secondary ones? After all, secant is just the reciprocal of cosine, so everything you can do with cosine you can do with secant.","['trigonometry', 'terminology']"
4742196,Infinitely differentiable function with given zero set?,"For each closed set $A\subseteq\mathbb{R}$, is it possible to construct a real continuous function $f$ such that the zero set, $f^{-1}(0)$, of $f$ is precisely $A$, and $f$ is infinitely differentiable on all of $\mathbb{R}$? Thanks!","['functions', 'real-analysis']"
4742207,"Let $A$ be a skew-symmetric real matrix, prove that there exists a vector $x\ge0$ such that $Ax\ge0$ and $Ax + x > 0$","This is an assignment that I am struggling with. Let $A$ be a skew-symmetric real matrix, prove that there exists a vector $x\ge0$ such that $Ax\ge0$ and $Ax + x > 0$ . Not sure how to proceed here and would appreciate some pointers/solution. My first thought is to: Show there exists a vector $x\ge0$ so that $Ax\ge0$ Use proof by contradiction by starting with the claim: for any $x\ge0$ , either $Ax<0$ or $Ax+x\le0$ , and derive contradiction, but I did not see a way how. and the second thought is to use Farkas' lemma, which states that for any given matrix A and vector b, one and only one of the following statements is true:
(a) There exists a vector $x\ge0$ so that $Ax=b$ ;
(b) There exists a vector $y$ s.t. $A^Ty\ge0$ AND $b^Ty<0$ I start by choosing $A+I$ as the ""A"" matrix and some positive vector $b>0$ as the ""b"" vector here. If I can prove that $y$ does not exist under this situation, I would have proven (a) is correct, which means there exists a $x\ge0$ so that $(A+I)x > 0$ , which proves the second half of the case. But unfortunately I cannot find a way to disprove (b). Appreciate your kind advice. Thanks.","['skew-symmetric-matrices', 'linear-algebra']"
4742214,Creating a function that satisfies the following properties?,"Suppose using the lebesgue outer measure $\lambda^{*}$ , we restrict $A$ to sets measurable in the Caratheodory sense , defining the Lebesgue measure $\lambda$ . Question: Does there exist an explicit and bijective $f:\mathbb{R}\to\mathbb{R}$ , where: The function $f$ is measurable The graph of $f$ , i.e. $\left\{(x,f(x)):x\in\mathbb{R}\right\}$ is dense in $\mathbb{R}\times\mathbb{R}$ The range of $f$ is $\mathbb{R}$ For all real $x_1,x_2,y_1,y_2$ , where $-\infty<x_1<x_2<\infty$ and $-\infty<y_1<y_2<\infty$ : $$\lambda(\left([x_1,x_2]\times[y_1,y_2]\right)\cap\left\{(x,f(x)):x\in\mathbb{R}\right\})>0$$ $\quad\quad\!$ and $$\lambda(\left([x_1,x_2]\times[y_1,y_2]\right)\cap\left\{(x,f(x)):x\in\mathbb{R}\right\})\neq(x_2-x_1)(y_2-y_1)$$ Attempt: I'm not sure how to answer this question but I heard of Conway’s Base-13 function? I have also asked a similar question here with an answer suggesting the function doesn’t exist.","['measure-theory', 'functions', 'definition', 'real-analysis']"
4742255,"Prove if $M^2 =0$, then $\operatorname{rank}(M+M^T)=2\operatorname{rank}(M)$.","Prove if $M^2 =0$ , then $\operatorname{rank}(M+M^T)=2\operatorname{rank}(M)$ . I have used the rank nullity theorem and the fact that rank of matrix and its transpose is same to prove it but I am not sure if it correct or not. I do not how to exactly use the fact of $M^2=0$ . I can think of it means that $M$ is nilpotent and square matrix. In my answer I get less than or equal to but it I have to prove it as equal. Help would be highly appreciated. I am using real square matrices. $\operatorname{rank}(M + M^T)\le\operatorname{rank}(M) +\operatorname{rank}(M^T)$ $\operatorname{rank}(M + M^T)\le2\operatorname{rank}(M)$","['matrix-rank', 'matrices', 'nilpotence', 'linear-algebra', 'transpose']"
4742270,How should I find the circumradius of a triangle without assumption of equilateral triangle?,"Acute triangle ABC has area 845. Let D be the foot of the perpendicular from A to BC, and let E
be the foot of the perpendicular from B to AC. The area of triangle CDE is 125, and the length of
DE is 20. The circumradius of triangle ABC can be expressed in the form a/b, where a and b
are relatively prime positive integers. Find a + b. My work: line segment/ area relationships to get that $\dfrac{CD}{BC} \cdot \dfrac{EC}{AC} \cdot 845 = 125$ . Then, $\dfrac{CD\cdot EC}{BC\cdot AC} = \dfrac{125}{845} = \dfrac{25}{169} = \left(\dfrac{5}{13}\right)^2.$ So, I assumed that $\dfrac{CD}{ED} = \dfrac{BC}{AC} = \dfrac{5}{13}$ for a easier way out, giving a nice similarity. Also, with this, since the formula for a circumradius is $\dfrac{abc}{4A}$ , I set both triangles ABC and CDE as equilateral to get $\dfrac{13^3}{4\cdot 845} = \dfrac{13}{20}$ , so I got 13+20 = 33 as my answer. My question: Do my assumptions lead to a possibly incorrect solution?","['contest-math', 'geometry']"
