question_id,title,body,tags
2650818,"Let $(x,y) \in \Bbb R^+$ Prove that $\Bigr(1+\frac{1}{x}\Bigl)\Bigr(1+\frac{1}{y}\Bigl)\ge \Bigr(1+\frac{2}{x+y}\Bigl)^2$","Let $(x,y) \in \Bbb R^+$ Prove that $$\Bigr(1+\frac{1}{x}\Bigl)\Bigr(1+\frac{1}{y}\Bigl)\ge \Bigr(1+\frac{2}{x+y}\Bigl)^2$$ My try Well, i didn't see a way to factorize this, so i put it in WolframAlpha and i got that we can rewrite it like this, which is obviously true for $(x,y) \in \Bbb R^+$: $$\frac{(y-x)^2(x+y+1)}{xy(x+y)^2}\ge 0$$ But i don't see the way to get there manually, im stuck here: $$\frac{(x+y)(y+1)}{xy}\ge \frac{(x+y+2)^2}{(x+y)^2}$$","['inequality', 'a.m.-g.m.-inequality', 'factoring', 'cauchy-schwarz-inequality', 'algebra-precalculus']"
2650824,"Double integral to polar coordinates, bounds","I want to find $\int_0^2 \int_ {\sqrt{2x-x^2}}^{\sqrt{4-x^2}} \sqrt{4-x^2-y^2}dydx$. My idea is to do a transformation to polar coordinates; $\iint \sqrt{4-r^2} rdrd\theta$, but I'm unsure about the bounds. Plotting the bounds in $f(x,y)$ I realize I'm dealing with the area between a smaller circle centered at (1, 0) and r=1 and a bigger circle centered at (0,0) with r = 2. (Under the ball $\sqrt{4-x^2-y^2}$ Any ideas? Thank you","['multivariable-calculus', 'integration', 'polar-coordinates', 'calculus']"
2650892,Expectation of positive semidefinite random matrix,Let $X$ be a random square symmetric matrix. Assume $X$ is positive semidefinite almost surely. Also assume that the expectation $E[X]$ exists. Does it follow that $E[X]$ is positive semidefinite? All my intuition tells me that this must be true but a rigorous proof eludes me.,"['positive-semidefinite', 'probability', 'random-matrices']"
2650905,What is the value of $\delta $ if $\epsilon=0.01$?,"Let $f(x,y) = \begin{cases} \frac{2x^2y+3xy^2}{x^2+y^2},  & \text{if
 $(x,y)\neq(0,0)$} \\[2ex] 0, & \text{if $(x,y)=(0,0)$ } \end{cases}$ Then the condition on $\delta $ such that $\vert f(x,y)-f(0,0)
\vert<0.01$ whenever $\sqrt {x^2+y^2}<\delta $ is- 1.$\delta <0.01$ 2.$\delta <0.001$ 3.$\delta <0.02$ 4.no such $\delta $ exists solution:since $f(0,0)=0$,then consider $\left\vert \frac{2x^2y+3xy^2}{x^2+y^2}-0\right\vert =\left\vert \frac{xy(2x+3y)}{x^2+y^2}\right\vert\le \frac{(2x+3y)}{2}$ as $xy\le \frac{x^2+y^2}{2}$ From here, how to proceed further...","['multivariable-calculus', 'real-analysis', 'continuity']"
2650938,"Prove ""$ \lim_{n\to\infty}\|x_n\|_1 =0 \iff \lim_{n\to\infty}\|x_n\|_2 =0$"" $\implies \|\cdot\|_1$ equivalent to $\|\cdot\|_2$","Let $X$ be a normed linear space and let $\|\cdot\|_i$ be arbitrary norms on $X$. Prove that the two arbitrary norms are equivalent if the following statement is true: $$\lim_{n\to\infty}\|x_n\|_1 =0 \iff
\lim_{n\to\infty}\|x_n\|_2 =0$$ We want to prove that $\exists m,M$ such that $0<m\leq M$ such that
$$ m\|x\|_1 \leq \|x\|_2  \leq M\|x\|_1 \quad\forall  x\in X$$ By contradiction, suppose $\exists y\in X$ such that $\forall m\leq M \in\mathbb{R}_{>0}$ we have that $$
m\|y\|_1 > \|y\|_2
\quad
and
\quad
M\|y\|_1 < \|y\|_2
$$ Then we have that $$
m\|y\|_1 > M\|y\|_1
$$ which is a contradiction. 
QED. Now I know this must be wrong, because I did not use the statement at all. Where is the mistake and how do I fix this?","['limits', 'normed-spaces', 'proof-verification', 'functional-analysis', 'sequences-and-series']"
2650955,Solving non-homogeneous linear second-order differential equation with repeated roots,"I have the following second-order linear differential equation that I am unable to solve: $y''+4y'+4y=t$. The method for solving homogeneous linear second-order differential equations obviously doesn't work, but I am unsure how to apply the technique to solve non-homogenous linear second-order differential equations because this equation has repeated roots. Is there a particular method I should be applying?",['ordinary-differential-equations']
2650960,A fun Valentine's day chocolate box optimization problem...,"Today I received a small box with chocolate almonds in it. Here's a photo: The top and bottom of the box are held together by a band that ""cuts across"" alternating corners. It got me thinking: ha, I'll make this into a fun little calculus optimization problem: what distance $x$ from the corner should the band cross each edge as to minimize the total length of the band? And more specifically, what is this distance in terms of the length, width, and height of the box? We're defining $x$ as the following (view from the top of the box, bands are red): Let's call $b_T$ the total band length, $b_h$ the sum of the portions of the band on the top and bottom of the box, $b_l$ the sum of the portions of the band on the side $l$ of the box, and $b_w$ the portions of the band on the side $w$ of the box. And of course, $b_T = b_h + b_l + b_w$. Considering just the top and bottom, we can see: $b_h = 4x\sqrt2$. Similarly, since the total side length of two sides of the box are $l$ and the other two are $w$, and the height of the box is $h$, using the Pythagorean Formula we can get the following two formulas: $$b_w=2\sqrt{h^2+(w-2x)^2}$$ $$b_l=2\sqrt{h^2+(l-2x)^2}$$ Thus, our total band length in terms of x is: $$b_T = 4x\sqrt2 + 2\sqrt{h^2+(w-2x)^2} + 2\sqrt{h^2+(l-2x)^2}$$ Since this is an optimization problem and we wish to find the minimum value of $b_T$, we will differentiate and set equal to zero: $$b_T' = 4\sqrt2 + \frac{-4(w-2x)}{\sqrt{h^2+(w-2x)^2}} + \frac{-4(l-2x)}{\sqrt{h^2+(l-2x)^2}}$$ $$0 = 4\sqrt2 + \frac{-4(w-2x)}{\sqrt{h^2+(w-2x)^2}} + \frac{-4(l-2x)}{\sqrt{h^2+(l-2x)^2}}$$ By rearranging a bit more, we can rationalize it to: $$\left(2+ \frac{(w-2x)^2}{h^2+(w-2x)^2} - \frac{(2x-l)^2}{h^2+(l-2x)^2}\right)^2 = \frac{8(w-2x)^2}{h^2+(w-2x)^2}$$ Remember, the goal is to find, given the dimensions of the box, some optimum placement of the band. So we need to rearrange this formula to get $x$ in terms of $l$, $w$, and $h$. This is where I am at a total loss. Distributing out the left side would give us too many terms to keep track of; even substituting each terms for $a$, $b$, and $c$ respectively, distributing that, and then plugging back in results in a ridiculous number of terms. This would be absurd; how can we do this a better way and get $x$ in terms of $l$, $w$, and $h$? (Also, I hope I haven't messed up any of the formulas while typesetting them. I don't think I did, but you never know...)","['algebra-precalculus', 'optimization', 'calculus']"
2650964,Dimension of a range space of a linear operator,"Define $$T: C[0,1] \rightarrow C[0,1]$$ as $\displaystyle T(f(x))= \int_{0}^{1} \sin(x+y)f(y)dy$, then the dimension of the range space of $T$ is?. Can somebody help me to understand how to approach the problem. If it would have been a finite dimensional space i would have found out the nullity and then substracted it from the dimension of the domain but here this is not the case","['functional-analysis', 'linear-transformations']"
2650968,"If $f(t)\in 1 + t\mathbb{Z}[t]$ is irred., could there exist arbitrarily large $n$ such that for some $a\in\mathbb{Z}$, $f(t)+at^n$ splits completely?","Let $f(t)\in 1 +t\mathbb{Z}[t]$ be an irreducible polynomial (not necessarily monic) of degree $\ge 2$. Thus $f(t)$ is also irreducible in $\mathbb{Q}[t]$. Is it possible that there exist arbitrarily large integers $n\ge 0$ satisfying: ""There exist an $a\in\mathbb{Z}$ (possibly 0) such that $at^n + f(t)$ splits completely into linear factors in $\mathbb{Z}[t]$"" ? A second, probably easier question - Can we find a polynomial $f(t) \in 1+t\mathbb{Z}[t]$ such that there do not exist arbitrary large integers $n\ge 0$ satisfying the quoted condition?","['number-theory', 'commutative-algebra']"
2650979,Differential forms: $dx \wedge dy$ vs $dx dy$,"When evaluating the integrals of differential forms over a parameterized manifold, you often end up with something like: $$\int _{|\gamma(s)|} dx\wedge dy + y dx \wedge dz = \iint_{[a,b] \times [c,d]} f(s,t)\,ds\,dt$$ However, this led me to a conceptual contradiction: the left hand side involves the wedge product of differential one forms, and is thus an expression involving differential two-forms. The right hand side, however, has differentials (presumably differential one-forms) just multiplied together. Traditionally, I would interpret $ds dt$ as just representing an ""infinitesimally"" small square in the s-t plane. However, how do I reconcile this notation/concept with that of the differential form?","['multivariable-calculus', 'differential-forms', 'differential-geometry']"
2651005,Asymptotic Distribution of Sample Mean,"Suppose I have a discrete random variable $X$ which follows a geometric distribution on $x=0,1,2,...$ and I take a random sample from this distribution of size $n$.  What is the asymptotic distribution of $\bar X$? I already know that $E(X)=\frac{1-p}{p}$ and $V(X)=\frac{1-p}{p^2}$. This seems like an application of central limit theorem, so I'm sure that $\bar X$ converges to a normal distribution.  However, the part that's tripping me up is calculating the mean and variance of the normal distribution that it's converging to.  How do you do this?","['probability-theory', 'probability-distributions', 'central-limit-theorem', 'statistics', 'probability']"
2651090,A difficult integral of reciprocal of sum of exponentials,"Consider the integral: $$\int_0^\infty \frac{1}{e^{ax}+e^{bx}+e^{cx}} dx$$ where $a, b, c$ are complex numbers. I encounter such integral when dealing with Laplace transform of co-functions. It turns out that the online integral calculator couldn’t even give any result. I suppose the integrand is well-defined since $e^x$ has no branch point on the whole complex plane. Maybe the integrand has poles, but I believe the integral exists in the sense of Cauchy p.v.. I am a bit reluctant to use the substitution $y=e^x$ since $y^p$ in general has branch points for complex $p$. Is there any method to compute such integral? Any help will be appreciated. EDIT: since a result in closed form is very unlikely to exist, a series as an answer will also be appreciated.","['definite-integrals', 'calculus']"
2651182,Closed form of $\sum_{k=0}^p \binom{k+m-1}{m-1}?$,"Question : Is there a closed form for the summation
  $$\sum_{k=0}^p \binom{k+m-1}{m-1}?$$ By writing out the summation, we have 
$$\binom{m-1}{m-1} + \binom{m}{m-1} + \binom{m+1}{m-1}+...+\binom{p+m-2}{m-1}+\binom{p+m-1}{m-1}$$
$$ = 1 + m + \frac{m(m+1)}{2} + \frac{m(m+1)(m+2)}{3!}+...+\frac{m(m+1)...(m+(k-1))}{k!}+...+\frac{m(m+1)...(m+(p-1))}{p!}.$$
However, I do not see a closed form for it.","['combinatorics', 'binomial-coefficients', 'discrete-mathematics']"
2651191,Determine if an Estimator is Biased (Unusual Expectation Expression),"Let $X_1, X_2, \ldots, X_n$ be i.i.d. with a common density function: $$f(x;\theta)=\theta x^{\theta-1}$$
for $0<x<1$ and $\theta>0$.  So this is $\operatorname{BETA}(\theta,1)$ distribution. Given this Maximum Likelihood Estimator (MLE) for $\theta$:  $$\hat \theta=\frac{-n}{\sum_{i=1}^n \ln(X_i)}$$ Determine if $\hat\theta$ is biased.  If it is biased, could you redefine it to make it unbiased? Unfortunately, this is where I get stuck; I have no idea how to evaluate the expectation of $\hat\theta$. $$\operatorname E\left( \frac{-n}{\sum_{i=1}^n \ln(X_i)} \right)$$ How does one calculate this?","['statistics', 'probability', 'expected-value', 'probability-distributions']"
2651199,Generalization of the chain rule using upper derivatives,"I'm having trouble with the following exercise, taken from Chapter 6 of Royden and Fitzpatrick's Real Analysis : Let $f$ be defined on $[a,b]$ and $g$ a continuous function on $[\alpha, \beta]$ that is differentiable at $\gamma \in (\alpha, \beta)$ with $g(\gamma) = c \in (a,b)$. Show that if $g'(\gamma) > 0$, then $\bar D(f \circ g)(\gamma) = \bar D f(c) \cdot g'(\gamma)$ where $$\bar Df(x) = \lim_{h \to 0} \sup_{0 < |t| \leq h} \frac{f(x + t) - f(x)}{t}$$ If $g$ were linear, the problem is straight-forward:
\begin{multline}
\bar D(f \circ g)(\gamma) = \lim_{h \to 0}\sup_{0 < |t| \leq h}\frac{f(c + g'(\gamma)\cdot t) - f(c)}{t} = \\\lim_{h \to 0}\sup_{0 < |s| \leq g'(\gamma)\cdot h}\frac{f(c + s) - f(c)}{s}\cdot g'(\gamma) = \bar D f(c) \cdot g'(\gamma)
\end{multline}
I'm having trouble in the general case, in part because the problem places so little structure on $f$. The error from linearly approximating $g(\cdot)$ at $\gamma$ can be made arbitrarily small, but without a continuity condition on $f$ I'm unsure how to make the approximation error ""go away"". Thank you!","['derivatives', 'real-analysis']"
2651245,Fourier transform of meromorphic function,"Suppose that I have a function $f(z)$ which is meromorphic on the entire complex plane, meaning holomorphic everywhere except for a discrete set of poles. I then take a vertical slice of this function, which we can denote $f(s+it)$ (for constant $s$), and then take the Fourier transform of it, denoted $\mathcal{F}\{f(s+it)\}(\omega)$. Let's assume the function decays quickly enough on this slice for the Fourier transform to be well-defined without distributions having to get involved. My two questions: Does the result always also admit a meromorphic continuation to the entire complex plane? (I believe yes, this is the Laplace transform) If so, does the meromorphic continuation depend on which slice we choose? (Much less sure about this)","['complex-analysis', 'analytic-continuation', 'fourier-analysis', 'meromorphic-functions']"
2651363,How can a point that makes the ODE undefined be included into a domain of any solution?,"To illustrate the problem that I'm having, consider the following ODE: $$y' = \frac{y}{1+x}$$ To solve this ODE, my professor had done the followings: Assuming $y(x) \not = 0$, and $x \not = -1$ (actually, he never stated these), then we can arrange the ODE as $$\frac{dy}{y} = \frac{dx}{1+x} \\
\ln|y| = \ln|x+1| + \ln|c| \\
y = c(x+1). \quad \text{(The General solution)}$$ And later, while introducing some initial values to this ODE, he gave the case where $y(-1) = 0$, and claimed that since when $x = -1$, for any values of $c\in \mathbb{R}$, we get different solutions from the ""general solution"", there exists infinitely many solutions satisfying this initial condition. However, (even if we skip how we found $y = c(x+1)$) to verify that $y = c(x+1)$ is indeed a solution (for some $c$) to this ODE, when we plug it in to the ODE, in order to cancel the terms $1+x$ in the denominator and in the numerator, we need to assume that $x \not =-1$, otherwise it would also mean we are calcelling zero's, which is not valid since, in the algebra that we are doing, zero do not have any multiplicative inverse.Therefore, any solution of the form $y = c_0(x+1)$ cannot contain the the point $x = -1$, so how can we say that ""there are infinitely many solution of this ODE satisfying the initial condition $y(-1) = 0$"" ? With my logic, there exists no such solution satisfying this initial condition, so which one of us is right, and why? A Further Question: As we have done it in this particular case, to reach some solutions of a given ODE, we need to do some arrangements to the form of the ODE, such as collection same variables to the same side, but this generally requires some assumptions, such as $y \not= 0$, so in such cases, the result that we get should not be applicable to the cases, for example, in the above case, it there was any other point other than $x = -1$ where $y = 0$, say $y(x_1) = 0$ the solutions that we got from the equation $y = c(x+1)$ should not contain that point $(x_1, 0)$ also, right ?","['singularity', 'ordinary-differential-equations', 'fundamental-solution']"
2651386,Reference request for a proof of a basic version of normalization lemma,"I was reading some notes on affine algebraic geometry and came across the following fact: If $k$ is a field with $\mathrm{char}~k = 0$, then if $V(f)$ is an irreducible hypersurface in $\mathbf{A}^n_k$, then there is a surjective linear projection $\pi: \mathbf{A}^n_k \to \mathbf{A}^{n-1}_k$ such that the composition $V(f) \to \mathbf{A}^n_k \to \mathbf{A}^{n-1}_k$ is finite. Can someone point me to resource (book/lecture notes) where I can find this statement proved?",['algebraic-geometry']
2651393,Finding the derivative of $\ln(\sin(x))$ using first principles.,"Let $y=f(x)=\ln(\sin(x))$ $f(x+h)=\ln(\sin(x+h))$ $$\frac{d}{dx}(y)=\frac{d}{dx}(f(x))=\lim_{h \to0}\frac{f(x+h)-f(x)}{h}=\lim_{h \to0}\frac{\ln(\sin(x+h))-\ln(\sin(x))}{h}$$
$$=\lim_{h \to0}\frac{\ln\left(\frac{\sin(x+h)}{\sin(x)}\right)}{h}
=\lim_{h \to0}\frac{\ln\left(\frac{\sin(x)\cos(h)+\cos(x)\sin(h)}{\sin (x)}\right)}{h}$$
$$=\lim_{h \to0}\frac{\ln (\cos(h)+\cot(x)\sin(h))}{h}=\lim_{h \to0}\frac{\ln( \cos(h)(1+\cot(x)\tan(h)))}{h}$$ I am stuck at this step. Please help.","['derivatives', 'calculus', 'limits']"
2651401,Find limit of $\{a_n\}$,"The sequence $\{a_n\}$ is determined by $$a_1 = 1, a_{n+1} = \frac{3n-1}{3n} a_n + \frac{1}{n^2}, \quad \forall n\ge 1.$$ Find the limit of $\{a_n\}$ (if it exists). I guess the limit is $0$ by using MATLAB, but the sequence converges really slowly.","['real-analysis', 'sequences-and-series', 'limits']"
2651431,Prove that $a_1!\cdots a_k! < n!$ whenever $a_1+\cdots+a_k < n$,"Prove that for positive integers $a_1,\dots,a_k$ (where $k\geq 1)$  are such that $a_1+\cdots+a_k < n$ , then $a_1!\cdots a_k! < n!$. So far, i've tried adding the condition that suppose they were also in increasing order. I've thought about using induction to prove it, but it seems like this is not the right approach to go about this. Without induction, I am struggling to somehow use the fact that: $$a_1+\cdots+a_k < n \implies (a_1+\cdots+a_k)! < n! $$ and relate $\displaystyle \left(\sum a_i\right)!$ to $\displaystyle \prod a_i!$.  How does one proceed?","['inequality', 'integers', 'discrete-mathematics']"
2651437,Partial of Modulo operator ? (with non-integers),"I am trying to derive gradient for a special neural network, but got stuck on the Modulo Arithmetic . With usual funcitons such as $f(a,x) = a/x$ the partials would be $\frac{1}{x}$ and $-\frac{1}{x^2}$, but am struggling to find such a rule on the internet for Modulo operator. So I have: $$f(A, x) = A\%x$$ 
 or in other words:
  $$f(A, x) = mod(A, x)$$ This means the value A ""wraps around"" the value X several times, and spits out remainder. The trick in my case is A or X are not integers - they can be any real number such as 0.123 etc $$\frac{\partial f}{\partial A}f(A,x) = ?$$ 
  $$\frac{\partial f}{\partial x}f(A,x) = ?$$ Edit: $A\%x$  Will be real number (such as 19.123 etc), can be positive, negative, or zero","['derivatives', 'partial-derivative', 'modular-arithmetic']"
2651442,Any simple group of order $60$ is isomorphic to $A_5$,"There are many famous proofs. However, I cannot find a solution using the following method which is also clear. It’s absolutely correct. Has anyone seen this proof before, and do you have a reference? By the way, why few people seem to prefer this method? As for me, it’s very strong a way to solve many problems. PS : I have also applied this method to the proofs that groups of order $180$ , $540$ , $1080$ are not simple . I will apply the well-known Sylow theorem , strong Cayley theorem , $N/C$ theorem , Burnside's transfer theorem and some basic theories in group-theory. $G$ is a simple group of order $60$ . $|G|=60=2^2\cdot 3\cdot 5.$ So a Sylow $2$ -subgroup would have order $4$ . By Sylow theorem , $n_2\mid 3\cdot 5$ , $n_2\equiv 1~(\text{mod}~2)$ . There are four possibilities: $1$ , $3$ , $5$ , $15$ . And we denote one of the Sylow $2$ -subgroups by $P_2$ . $1)$ If $n_2=1$ , then $P_2$ will be a normal subgroup of $G$ , contradicting the simplicity of $G$ . $2)$ If $n_2=3$ , [ strong Cayley theorem ] $G$ simple, $\ker\Phi$ is $G$ itself or the trivial subgroup $\{e\}$ , furthermore, $\ker \Phi$ must be $\{e\}$ $($ if $\ker \Phi=G$ , then $G\leqslant N_G(P_2)$ , which is ridiculous $)^{[1]}$ . Hence, $G=G/\{e\}=G/\ker\Phi\lesssim S_3$ . It contradicts the fact that $|G|=60>6=|S_3|.$ $3)$ If $n_2=15$ , $|G:N_G(P_2)|=15$ , $|N_G(P_2)|=4$ . However, $|P_2|=4$ , hence abelian, $P_2\trianglelefteq C_G(P_2)\trianglelefteq N_G(P_2)$ , $C_G(P_2)=N_G(P_2)$ . By Burnside's transfer theorem , there exists $K\triangleleft G$ , which contradicts the simplicity of $G$ . Thus, $n_2$ must be $5$ , $G=G/\{e\}=G/\ker\Phi\cong \Phi(G)\leqslant S_5$ , where $|\Phi(G)|=|G|=60$ . Hence $|S_5:\Phi(G)|=2$ , $\Phi(G)$ must be $A_5$ . That is, $G\cong A_5$ . Note: [1] $\Phi$ is a map introduced in strong Cayley theorem ; $G$ is simple, hence $\ker\Phi $ , as a normal subgroup, must be $G$ itself or the trivial subgroup $\{e\}$ . If $\ker \Phi=G $ , then according to strong Cayley theorem, $$G=\ker\Phi=\bigcap\limits_{x\in G} x^{-1}N_G{(P_2)}x\leqslant N_G(P_2).$$ Whereas, $|G:N_G{(P_2)}|>1$ , it will be ridiculous.","['reference-request', 'group-theory', 'simple-groups']"
2651452,The area of circles tangent inside another circle,"Consider the following shape: Three identical circles are tanged inside a circle. Which is greater: The area of the shaded regions. Twice the area of the unshaded regions. My attempt: I tried to solve it approximately. Let $R=2$ be the radius of the big circle, and its area is $4\pi$. The diameter of the smaller circles is about $R$ and their radius is $R/2=1$, the area of each small circle is $\pi$. So, the area of the shaded region is $3\pi$. The area of the unshaded region is about $4\pi - 3\pi =\pi$. Twice of it is about $2\pi$ which is smaller than $3\pi$, the area of the shaded region. However, the correct answer is 2!","['circles', 'euclidean-geometry', 'area', 'geometry']"
2651466,$L^1 \subseteq L^2$?,"Let $(\Omega, \Sigma, \mu)$ - be a measure space. Suppose, $\mathbb{E}(X) < \infty$, i.e. $X \in L^1(\Omega, \Sigma, \mu)$. Given this, it follows $\mathrm{ess}\,\sup \lvert X \rvert < \infty$, otherwise $\mathbb{E}(X^+)$ or $\mathbb{E}(X^-)$ would be $\infty$. So we can deduce $X \in L^{\infty}(\Omega, \Sigma, \mu)$. By Hölder's Inequality it follows $$ \lvert\lvert X\cdot X \rvert\rvert_1 \leq \lvert\lvert X \rvert\rvert_1 \lvert\lvert X \rvert\rvert_{\infty} < \infty$$ which means $\mathbb{E}(X^2) = \lvert\lvert X \rvert\rvert_2^2 < \infty$ I feel like this should not be true. Where is the above reasoning wrong?","['lp-spaces', 'lebesgue-integral', 'measure-theory']"
2651484,Sketching open balls,"When working with metric spaces we usually have to sketch absolute value inequalities. I can determine the open balls and everything but the sketching part is difficult, for example for a metric defined as $$d((x_1,y_1),(x_2,y_2))=|x_1+ y_1- x_2- y_2|+|-x_1+ y_1+ x_2- y_2|$$ if I want the ball $$B_2(0,0)
= \{(x,y) \in \mathbb{R}^2: d((x,y),(0,0))<2\}=\{(x,y) \in \mathbb{R}^2: |x+y|+|-x+y| <2\}$$ I have no idea how to draw these things; although I know what an absolute value function looks like. Any help is appreciated.","['calculus', 'multivariable-calculus', 'absolute-value', 'metric-spaces', 'graphing-functions']"
2651494,Chebyshev's Inequality for conditional expectation,"This is based on Durrett's 5.1.3 Prove Chebyshev's inequality .  If $a > 0$ then $$\mathbb{P}(\lvert
 X \rvert \geq a | \mathcal{F}) \leq a^{-2}\mathbb{E}(X^2 |
 \mathcal{F})$$ First, I need to establish $X^2 \in L^1(\Omega, \Sigma, \mathbb{P})$, so the inequality is possible to have any meaning (otherwise functions are not defined). And i suppose $X \in L^1$, so the left side is defined. But, following $L^1 \subseteq L^2$? I can't deduce anything about $X^2$. Should I just suppose $X^2 \in L^1(\Omega, \Sigma, \mathbb{P})$? Or Durrett works in $L^2$? We just solve problems from the book, so did during my fast-forward search I missed this assumption?","['conditional-expectation', 'lp-spaces', 'measure-theory']"
2651504,"Show that $\bar G=\left\{ \begin{pmatrix} e^{it} & 0 \\ 0 & e^{is} \\ \end{pmatrix}| t ,s \in \Bbb R\right\}$.","Let $a \not \in \Bbb Q$ and let $G$ be the following subgroup of $GL(2, \Bbb C)$: Let $$G=\left\{
    \begin{pmatrix}
    e^{it} & 0 \\
    0 & e^{ita}  \\
    \end{pmatrix}| t \in \Bbb R\right\}
$$ Show that $\bar G=\left\{
    \begin{pmatrix}
    e^{it} & 0 \\
    0 & e^{is}  \\
    \end{pmatrix}| t,s \in \Bbb R\right\}$. I know that I have to use the fact that $\{e^{2\pi ina}: n\in \Bbb Z\}$ is dense in $S^1$. But I can't put all the missing links correctly. What I guess that if I put $t=2\pi n$ and vary $n \in \Bbb Z$ then $e^{it}=1$ and $e^{ita}$ would be dense in $S^1$ then taking closure I would get 
$$\left\{
    \begin{pmatrix}
    1 & 0 \\
    0 & e^{is}  \\
    \end{pmatrix}| s \in \Bbb R\right\}$$ Next is what? Please help","['matrices', 'abstract-algebra', 'matrix-calculus', 'lie-algebras', 'lie-groups']"
2651512,"Number of ways of choosing seven children from a classroom of 32 (15 boys, 17 girls) with at least 1 boy","I know that the correct solution can be calculated as:
$$ \binom {32} {7} - \binom {17}{7}$$ But why is the following solution incorrect? (I am interested in why the following reasoning is incorrect, I realize that the two numbers are not equal):
$$ \frac{15 \binom {31} {6}}{2!} $$ The reasoning is that we first pick a boy ($15$ options) and then pick $6$ children out of $31$ remaining in an arbitrary manner. Finally divide by $2!$ since the order of the two groups does not matter.",['combinatorics']
2651515,"If $y=\mathrm{e}^x\big(a\sin x+b\cos x\big)$, then express $y^{(n)}$ in terms of $y$ and $y'$.","Let $y=e^x(a\sin x+b\cos x)$.  Show $y''=py'+ qy$ for some constants $p$ and $q$; and express all higher derivatives as linear combinations of $y'$ and $y$. I got to $y''=2y'-2y$, but I'm not sure how to do the linear combinations part, I don't know how to reduce the $n$th derivative to just $y'$ and $y$.","['recurrence-relations', 'real-analysis', 'recursion', 'ordinary-differential-equations', 'sequences-and-series']"
2651537,Diophantine quadratic $(5m+3)(3m+1)=n^2$,"Prove that $(5m+3)(3m+1)=n^2$ is not satisfied by any positive integers $m,n$. I have been staring at this for some time (it's the difficult part of a competition problem, I won't spoil it by naming the problem). I tried looking at it modulo 3,4,5,7,8,16 for a contradiction, as well as looking at the discriminant with respect to $m$. I couldn't finish either way. I wonder whether it can somehow be used that $(-1,2)$ is a solution. There is a very small chance that there are actually positive solutions but I've ruled out the first few millions for $m$ with a little Python script and, as I said, this was set in a competition so presumably any solutions would be reasonably small. I've heard somewhere that all diophantine quadratics can be solved by some method and would be interested to read more (though not an entire book, if possible).","['number-theory', 'contest-math', 'diophantine-equations', 'quadratics']"
2651544,Fibered surface is flat if and only if it surjects onto the base,"At the beginning of section 8.3.1 of Liu's ""Algebraic geometry and arithmetic curves"", he defines a fibered surface $\pi\colon  X \to S$ (I am only interested in the case when the base scheme is $S=\mathrm{Spec}\mathbb{Z}$) and immediately remarks that flatness and surjectivity of $\pi$ are equivalent. Why is that so? And does he mean surjectivity onto the underlying space or as a morphism of sheaves or both?","['schemes', 'algebraic-geometry']"
2651571,Contact order of a space curve with one of it's tangent lines,"Definition $1$: Let $\alpha: I \mapsto \mathbb{R^3}$ and $\beta: \overline{I} \mapsto \mathbb{R^3}$ be two regular curves such that $\alpha(t_0) = \beta(t_0)$, where $t_0 \in I \cap \overline{I}$. $\alpha$ and $\beta$ are said to have contact of order $n$ at $t_0$ ($n$ being an integer $\geq 1$) if all derivatives of order $\leq n$ of the functions $\alpha$ and $\beta$ are equal at $t_0$ and derivatives of order $n + 1$ are different. Definition $2$: $\alpha: I \mapsto \mathbb{R^3}$ is said to be a regular curve if $|| \alpha'(t) || \neq 0, \forall t \in I$ Let $\alpha: I \mapsto \mathbb{R^3}$ be a regular unit speed curve. a) If $k(s) > 0, \forall s \in I$, then prove that the tangent line to
  $\alpha$ at $s$ has contact of order $1$ with $\alpha$. b) Give an example of a regular curve that has contact of order $2$ with one of it's tangent lines. c) Is it possible to get a regular curve that has contact of order $\geq n$ with one of it's tangent lines for all integer $n \geq 1$? a) is easy, for b) I think the example (that was) given in one of the answers (a parabola) will do, it's c) I'm having trouble with. I think the only possible example for such a curve is a line, since a curve having contact of order, say, $2$ at $t_0$ with one of it's tangent lines means the curvature at that point is $0$. I'm having trouble proving that, though. Any help would be appreciated.","['curves', 'frenet-frame', 'differential-geometry', 'calculus']"
2651575,What is the exact value of the radius in the Six Disks Problem?,"The disk covering problem : Find the smallest radius $r(n)$ required for $n$ equal disks to completely cover the unit disk . For $n=5,6$ , the best layouts are, $\hskip2.2in$ $\hskip2.2in$ with $r(5) \approx 0.609,\; r(6) \approx 0.5559$ , and $r(5)$ as a root of an octic, $$6^4x^8+2112x^7-3480x^6+1360x^5+1665x^4-1776x^3+22x^2-800x+5^4=0$$ with an unsolvable Galois group. (I already checked.) However, for $n=7,8,9,10$ , the best has a symmetric layout similar to, $\hskip2.2in$ with $\displaystyle r_s(n) = \frac{1}{1+2\cos\big(\frac{2\pi}{n-1}\big)}$ hence is solvable in radicals. Q: But what is the minimal polynomial of $r(6) \approx \frac1{1.798} \approx 0.5559 $ ? I've searched the online literature and I know it is by Károly Bezdek but it seems it's hard to find its minpoly. (Chances are it will also have an unsolvable Galois group but I'm curious to test it.) $\color{red}{Update:}$ Ed Pegg brought to my attention that $n=12$ , $\hskip2.2in$ has $r(12) \approx  \frac1{2.769} \approx  \frac1{x+1} \approx 0.361$ where $x$ is a root of the simple $x^3-2x-2=0$ , hence is again solvable in radicals. Compare to the symmetric but non-optimal $r_s(12) \approx 0.372$ .","['circles', 'discrete-geometry', 'combinatorial-geometry', 'minimal-polynomials', 'geometry']"
2651583,"Finding relationships between angles, a, b and c when $\sin a - \sin b - \sin c = 0$","while I was doing some calculations, I came across the condition where
$$ \sin a - \sin b - \sin c = 0$$
where $0 < b, c < a < \dfrac{\pi}{2}$. Is there a way to find a relationship between the angles without using the trigonometric functions above? I have not been able to even find a specific solution hence any help is extremely appreciated.
Yet, if possible, I will like to avoid approximations, such as by Taylor expanding the above. My(unsuccessful) approaches This is included for the sake of potential elaboration of the question and to just show my thought process. Finding the area on a unit circle. As $\sin 2\theta = 2\sin\theta\cos\theta$, I thought I could somehow correlate it to the area of a rectangle on the unit circle(which is given by $\sin \cos \theta$). Where, if $x = \dfrac{a}{2}, y = \dfrac{b}{2}, z = \dfrac{c}{2}$, and $\cos x = c_0, \sin x = s_0$ and so on,
$$ c_0s_0 -c_1s_1 - c_2s_2 = 0$$
yet I could not figure out how to proceed from here. Trying to associate it with the Euler's formula I tried transforming all the sins to the complex identities. As may be evident, that did not turn out especially well due to the fact that adding exponential functions is not an advisable act(please correct me if I am wrong). I tried to associate it to waves in physics. In particular, I tried to associate it to the triple slit interference pattern where the waves destructively interfere. Yet as I could not find any formula for this that was not an approximation and as I could not think of my own, I dropped this idea.",['trigonometry']
2651586,"Prove that $f(x)=\sin^2 x-x^2\cos x, \forall x\in [0,\frac{\pi}{2}]$ is monotonic increasing","I would like to show that $f(x)=\sin^2 x-x^2\cos x, \forall x\in [0,\frac{\pi}{2}]$ is monotonic increasing. If we can show that $f'(x)>0$, $\forall x\in [0,\frac{\pi}{2}]$, then  $f(x)$ is increasing there. We have $f'(x)=\sin 2x-2x\cos x+x^2\sin x$. How can I show that $f'(x)>0, \forall x\in [0,\frac{\pi}{2}]$","['derivatives', 'monotone-functions', 'calculus']"
2651637,"$a\cdot(b \cdot c) = d \cdot (e \cdot c) \implies b = (\bar{a}\cdot d) \cdot e$, then $(G, \cdot)$ is a group","Let $G$ be a non-empty set, $""\cdot""$ be a binary operation on $G$ and a single operation $x \mapsto \bar{x}$ such that $$a\cdot(b \cdot c) = d \cdot (e \cdot c) \implies b = (\bar{a}\cdot d) \cdot e, \forall a,b,c,d,e \in G.$$ Prove that $(G, \cdot)$ is a group. For $b = c = d = e = a \implies a = (\bar{a} \cdot a ) \cdot a, \forall a \in G$ . For $d = a, e = b \implies b = (\bar{a} \cdot a) \cdot b, \forall a,b \in G$ . Swapping $a$ and $b$ in the hypothesis equation and making $d = b, e = a$ we obtain that $b = (\bar{a} \cdot a) \cdot b$ and $a = (\bar{b} \cdot b) \cdot a$ . I think I need to show that $\bar{\bar{a}} = a, \forall a \in G$ and that $a \cdot \bar{a} = \bar{a} \cdot a = e$ (the identity element), but I don't know how. It would also help If I could prove that the binary operation is associative.","['abstract-algebra', 'group-theory', 'binary-operations']"
2651644,Convergence of a Cesaro sequence,"Let $\{a_n\}_{n=1}^{\infty}$ be a sequence such that $a_i\in[0,1]$ for every $i\in \mathbb{N}$, and suppose that $$\lim_{n \to \infty}\frac{1}{n} \sum_{i=1}^n a_i = p.$$ Does $$\frac{1}{n} \sum_{i=1}^n a_i^2$$ necessarily converge when $n \to \infty$? Clearly, that fact that $$\left|\frac{1}{n} \sum_{i=1}^n a_i\right|\geqslant\left|\frac{1}{n} \sum_{i=1}^n a_i^2\right|$$ is not enough, but together with the fact that the sequence $a_n^2$ is the square of $a_n$, is it enough?","['cesaro-summable', 'convergence-divergence', 'sequences-and-series', 'calculus']"
2651645,Does this probability density function have a name?,I have a random variable distributed according to the density function $$P(x)=|x|e^{-x^2}$$ Does this pdf have a name?,"['statistics', 'probability']"
2651652,Do the Euler Lagrange equations hold meaning for an infinite action? [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 6 years ago . Improve this question This question is edited and migrated from mathstackexchange The Euler–Lagrange equation is an equation satisfied by a function  $q$, which is a stationary point of the functional $S(\boldsymbol q) = \int_a^b L(t,q(t),\dot{q}(t))\, \mathrm{d}t$ Say we have an infinite action over infinite time - the action of a free particle for all time, for exampler. A critical point of the action functional means nothing here, but of course the Euler Lagrange equations still exist for all time.  If we associate the Euler Lagrange equations to a Hamiltonian system, then the path should solve Hamiltons equations iff the Euler Lagrange equations are satisfied. Can we say that these still offer a solution as $t \rightarrow \infty$, even though asking about ""a critical point of the action functional"" no longer makes sense? Is there a way to make sense of it formally?","['functional-analysis', 'classical-mechanics', 'hamilton-equations', 'dynamical-systems']"
2651708,Is there a way to avoid chain rules in finding this derivative of an integral?,"The one technique of differentiation in first-year calculus that is not introduced until after integrals are mentioned is this:
$$
\frac d {dx} \int_a^x f(u) \, du = f(x). \tag 1
$$ I am failing to see how to show the following without using anything other than $(1){:}$
$$
\frac d {dx} \int_0^x \left( \int_0^{x-u} f(u)g(v) \, dv \right) \, du = \int_0^x f(x-v) g(v) \, dv. \tag 2
$$
One can write
$$
\begin{bmatrix} s \\ t \end{bmatrix} = \begin{bmatrix} u+v \\ v \end{bmatrix}, \qquad \begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} s-t \\ t \end{bmatrix}
$$
and then
$$
d(u,v) = \left|\frac{\partial(u,v)}{\partial(s,t)} \right| \, d(s,t) = 1 \, d(s,t) \tag 3
$$
and
\begin{align}
& \iint\limits_{u,v\,:\,0\,\le\,u,\,0\,\le\,v,\, u+v\,\le\,x} f(u)g(v)\, d(u,v) \\[10pt]
= {} & \iint\limits_{s,t\,:\, 0\,\le\,t\,\le\,s\,\le\,x} f(s-t) g(t) \, d(s,t) \\[10pt]
= {} & \int_0^x \left( \int_0^s f(s-t)g(t)\, dt \right) \, ds
\end{align}
and then apply $(1),$ getting $(2).$ However, I would prefer using only a one-variable chain rule rather than $(3),$ or better still, no chain rules. Is there a way to do that?","['derivatives', 'convolution', 'chain-rule', 'jacobian']"
2651714,From ellipse equation to circular cone axis,"From a standard ellipse equation in 2D $ax^2 + bxy + cy^2 + dx + ey + f = 0$, is it possible to retrieve the axis of the corresponding 3D circular cones? I sense there an infinity of possible circular cones, but only two possible axis. However I can't find any information on how to find these axis. I looked into Dandelin spheres but couldn't find any methods on how to construct them from an ellipse.","['differential-geometry', 'geometry']"
2651733,Stronger versions of Wilson's Theorem,"Problem Let $c \in \mathbb{N}$ $;$ $\exists$ a prime $p$ for which: $$p^c \mid (p-1)!+1$$ Does $\exists$ $M$ $\in$ $\mathbb{N}$ $;$ $\forall$ $c \geqslant M$ $;$ $\nexists$ $p$ satisfying the above? When $c$ = $1$ The statement is equivalent to Wilson's Theorem . For every prime $p$ : $$p \mid (p-1)!+1$$ Proof: $\forall$ $x \in {1,2,...,p-1}$ $\exists!$ $ x' \in {1,2,...,p-1}$ ; $x \cdot x'\equiv 1 \pmod{p}$ $x=x' \iff p \mid x^2-1 \iff x = 1, x=p-1$ $(p-1)! = 1 \cdot (p-1) \cdot \prod{(x \cdot x')} \equiv 1^n \cdot (p-1) \equiv p-1 \pmod{p}$ $\implies p \mid (p-1)!+1$ QED When $c$ = $2$ We have the statement: $$p^2 \mid (p-1)!+1$$ The only known primes that satisfy this are $5$ , $13$ and $563$ . $(5-1)!+1 = 25 = 5^2$ $(13-1)!+1 = 479001601 = 13^2 \cdot 2834329$ $563^2 \mid (563-1)!+1 \approx 1.128 \cdot 10^{1303}$ Such primes $p$ are known as Wilson Primes . It is conjectured that there are infinitely many Wilson Primes. However, if there exists a fourth Wilson prime $p$ , then $p>2 \cdot 10^{13}$ . When $c \geqslant 3$ There are no known primes for which $p^3 \mid (p-1)!+1$ as if there is, then $p$ also has to be a Wilson Prime. $(5-1)!+1 = 25 \equiv 25 \pmod{5^3}$ $(13-1)!+1 = 479001601 \equiv 676 \pmod{13^3}$ $(563-1)!+1 \equiv 91921010 \pmod{563^3}$ It is most likely due to following evidence that there exists an upper bound $M$ for which: $$c \geqslant M \implies p^c \nmid (p-1)!+1$$ where $M \geqslant 3$ . We consider $(p-1)!+1 \pmod{p^c}$ We assume that every remainder divisible by $p$ (Wilson's Theorem) is equally probable. Thus, the probability of required remainder $0$ is $\frac{1}{p^{c-1}}$ Thus, probable number of primes for given constant $c$ is: $$\sum{\frac{1}{p^{c-1}}} = P(c-1)$$ where $P(x)$ is the Prime Zeta Function When $c=2$ , the expected number of Wilson primes is $P(1)$ . $$P(1)=\sum{\frac{1}{p}}$$ This sum is divergent. Thus, it is probable that there exist infinitely many Wilson primes. Proof: Define $N(x)$ to be the number of positive integers $n \leqslant x$ for which $p_i \nmid n$ , where $i > j$ for constant $j$ and $p_i$ is the $i$ th smallest prime. Then, we write: $$n=k^2m$$ where $m$ is square-free. As $m$ is square-free, and the only primes that divide it are $p_i$ for $1 \leqslant i \leqslant j$ , it has $2^j$ possibilities. $n^2 \leqslant x \implies n \leqslant \sqrt{x}$ , thus giving $n$ a maximum of $\sqrt{x}$ possibilities. $$\implies N(x) \leqslant 2^j\sqrt{x}$$ Assume the contrary, then for some $j$ : $$\sum_{i=j+1}^\infty \frac{1}{p_i} < \frac{1}{2}$$ We also have $x-N(x)$ is the number of numbers less than or equal to $x$ divisible by one or more of $p_i$ for $i>j$ . $$x-N(x) \leqslant \sum_{i=j+1}^\infty \frac{x}{p_i} < \frac{x}{2}$$ $$\implies 2^j\sqrt{x} > \frac{x}{2}$$ which is untrue for $x \geqslant 2^{2j+2}$ Thus the sum diverges. The divergence is similar to $\log{\log{x}}$ (Which is very slow). Probable Answer To Problem When $c \geqslant 3$ , the sum converges and is less than $1$ . When $c=3$ , $P(c-1) \approx 0.45$ When $c=4$ , $P(c-1) \approx 0.17$ When $c=5$ , $P(c-1) \approx 0.07$ When $c=6$ , $P(c-1) \approx 0.03$ When $c=7$ , $P(c-1) \approx 0.002$ We now go on to show why there most probably exists a constant $M$ such as the one in the problem. Consider: $$\sum_{i=3}^\infty P(i-1)$$ We have: $$\sum_{i=3}^\infty P(i-1) < \sum{\frac{1}{n(n+1)}} = \sum{\biggl(\frac{1}{n}-\frac{1}{n+1}\biggl)} = 1$$ Thus, the probable sum of the number of primes that satisfy the statement for $c \geqslant 3$ , including a prime $p$ , $n-2$ times, if the maximum $c$ satisfied is $n$ , is less than $1$ . However, if the answer to the problem is false, then, the sum would be infinite. Thus, it is highly unlikely for there to be a solution for $c \geqslant 3$ as the probable answer is less than $1$ but the actual answer would be a positive integer. However, it is almost impossible for the answer to the problem to be false, as the probable answer is less than $1$ but the actual answer would be infinite! Any of the following: Any progress or insight Answers conditional on conjectures Polynomial or logarithmic non-trivial bounds on $M$ will be accepted and appreciated. P.S. This question has been posted and answered in Math Overflow for the interested reader. Link: https://mathoverflow.net/questions/311675/stronger-versions-of-wilsons-theorem","['number-theory', 'prime-numbers']"
2651735,At least one of $4$ integers is $0$.,"I encountered this very interesting problem in my work and it sounds like this: Let $a, b, c, d\in\mathbb{Z}$ such that $$ca-3bd=5$$ $$ad+bc=2$$
  Prove that at least one of them is $0$. I tried (using the idea that a perfect square is non-negative) to multiply with $d$ the first relation and then with $c$ the second, and then first with $c$ the first relation and then with $d$ the second and by adding/subtracting I obtained $$a(c^2+3d^2)=5c+6d$$ $$b(c^2+3d^2)=2c-5d$$ but it doesn't help much. How should I approach this problem?","['complex-numbers', 'systems-of-equations', 'algebra-precalculus', 'number-theory', 'quadratics']"
2651744,Axler's LADR. Algebraic multiplicity of an eigenvalue is the number of times it appears on the diagonal of an upper triangular matrix?,"This problem is from Sheldon Axler's Linear Algebra Done Right, Chapter 8.
Let $T \in \mathcal{L}(V)$, where $V$ is a finite dimensional complex vector space. If the matrix of $T$ is upper triangular with respect to any basis of $V$, the number of times $\lambda$ appears on the diagonal of this matrix equals the (algebraic) multiplicity of $\lambda$ as an eigenvalue of $T$. It suffices to show that $\dim \text{null } T^n = \dim G(0, T)$ equals the number of $0$'s on the diagonal, where $G(0, T)$ is the space of generalized eigenvectors of $T$ with respect to $0$. I found this rather nice proof on this blog, which uses induction on the dimension of $V$. I'm interested in finding alternative ways to prove this statement, which may provide a different way of looking at it.","['matrices', 'linear-algebra', 'linear-transformations']"
2651754,Fixed Point Property for a special space?,"Suppose $X$ is a compact connected metric connected space and for every $\epsilon >0$ , there exists a continuous surjective function $f : X \rightarrow [0,1]$, that for all $y \in [0,1]$, the diameter of the set $f^{-1} (y)$ is less than $\epsilon$. Prove that $X$ has the fixed point property; that is, for all continuous $g : X \rightarrow X$, there exists $x_0 \in X$ that $g(x_0) = x_0$. If we know that for a continuous surjective function $f : X \rightarrow [0,1]$ the diameter of set $f^{-1} (y)$ is zero, then we know that $X$ and $[0,1]$ are homeomorphic, so the statement is proven. But I have no idea for that. Hints are appreciated.","['fixed-point-theorems', 'general-topology', 'metric-spaces']"
2651765,Global attractor,"I have three little questions. Let $X$ be a topological space and $f\colon X\to X$ . How is a global attractor defined? In Katok and Hasselblatt’s Modern Theory of Dynamical Systems the following definition of an attractor is given: A compact subset $A\subset X$ is called an attractor for $f$ if there exist a neighbourhood $V$ of $A$ and $N\in\mathbb{N}$ , such that $f^N(V)\subset V$ and $A=\bigcap_{n\in\mathbb{N}}f^n(V)$ . There is no definition of a global attractor in this book but I guess this is an attractor for which $V=X$ ? Assume $X$ is compact and $f$ is continuous. Am I right that $X$ itself as well as $$
	E:=\bigcap_{n\in\mathbb{N}}f^n(X)
	$$ are global attractors? (Using the definition above, I have that $E$ is a compact subset of $X$ and I can choose $V=X$ and $N=1$ .) Assume again that $X$ is compact and $f\colon X\to X$ continuous.  Consider the so-called non-wandering set $$
	\Omega(f)=\left\{x\in X: \text{for each neighbourhoood U of x}\exists~N\geq 1: f^N(U)\cap U\neq\emptyset\right\}.
	$$ It is known that $\Omega(f)$ is $f$ -invariant. Am I right that a global attractor contains every $f$ -invariant set and  therefore $$\Omega(f)\subseteq E=\bigcap_{n\in\mathbb{N}}f^n(X)?$$ (If $E$ is a global attractor...)","['general-topology', 'dynamical-systems']"
2651822,What are $q$-deformations?,"This question has already appeared in a lot of different ways and here is another one. First of all, many people know the typical quantum group $U_q(\mathfrak{sl}_2)$ by generators and relations. This thing is often called a $q$-deformation of $\mathfrak{sl}_2$. The first baby question is whether anyone can give a more general definition of what a $q$-deformation is without generators and relations. Immediately I want to give two remarks: People wrongly call the quantized universal enveloping algebras $U_q(\mathfrak{g})$ where $\mathfrak{g}$ is a finite-dimensional complex semi-simple Lie algebra, Drinfeld-Jimbo algebras. Why do I say that this is wrong? Well, the algebras $U_h(\mathfrak{g})$ defined in a similar fashion are the real Drinfeld-Jimbo algebras. In chapter XVIII of 'Quantum Groups' by Kassel, he defines $h$-deformations. Moreover, there are rigidity-results for these algebras. One can truly say that a Drinfeld-Jimbo algebra is the unique quantization of the underlying Lie algebra in very precise sense (again see Kassel's book). In fact, the Drinfeld-Jimbo algebras have a nice categorical interpretation (see corollary XIX.4.3 of his book). Many people often claim that these rigidity results hold for the $q$-deformations as well. I have yet to see a proof of such claims! Moreover, I believe that people often claim this because they refer to both $q$ and $h$-deformations as Drinfeld-Jimbo algebras. This explains my wrongly . Assuming that these results are easily copied to $q$-setting is not a good hope. The $h$-rigidty results depend on the fact that $h$-deformations $U_h(\mathfrak{g})$ have a quasi-triangular structure. This is simply not true for $U_q(\mathfrak{g})$. (Categorically this correspond to the representation category of $U_h(\mathfrak{g})$ having a braiding whereas $U_g(\mathfrak{g})$-Mod is not braided). With these considerations in mind, any reasonable definition of a $q$-deformation must be seriously different than the one with generators and relations. Indeed, one can only talk about rigidity meaningfully if $q$-deformations aren't intrinsically uniquely defined (specifying generators and relations tells you what the object is, uniquely). Lastly, given a nice Lie algebra $\mathfrak{g}$. One can look at the finite-dimensional representations $U(\mathfrak{g})$-Mod. This category is a monoidal category. One should be able to 'monoidally deform' this category in essentially one way. Applying reconstruction theorems to the deformed category should give you a quantization of $\mathfrak{g}$. What cohomology is used for this monoidal deformation? What is the quantization you recover? Is it an $h$-deformation? Or is this what $q$-deformations should be? After some looking around, I think the so called Davydov-Yetter cohomology might be an interesting cohomology to look at. I'm having troubles understanding this cohomology though. (Etingof's book on tensor categories explains what it is, but not everything is defined). A good reference on this subject would be appreciated.","['homology-cohomology', 'quantum-groups', 'abstract-algebra', 'deformation-theory', 'hopf-algebras']"
2651891,Proof by Contradiction--Product of Lindelöf Spaces is Lindelöf,"Consider the three statements: (i) If $X$ is a set and $\mathcal{A}$ is a collection of subsets of $X$ having the countable intersection property, then there is collection $\mathcal{D}$ of subsets of $X$ such that $\mathcal{D} \supset \mathcal{A}$ is maximal with respect to the countable intersection property. (ii) Suppose that $\mathcal{D}$ is maximal with respect to the countable intersection property. Then countable intersections of elements in $\mathcal{D}$ are in $\mathcal{D}$ . Furthermore, if $A \subseteq X$ intersects every element of $\mathcal{D}$ , then $A \in \mathcal{D}$ . Show that products of Lindelöf  spaces are Lindelöf, and then show that (ii) holds. Okay. This problem is giving some trouble. It's clear that the intention of this problem is to show (i) is false by reductio, and this can be adequately done by considering just the product of two Lindelöf spaces (since I have a counterexample for this case). In the problem before this, I proved that $X$ is a Lindelöf space if and only if for every $\mathcal{A} \subseteq \mathcal{P}(X)$ having the countable intersection property (CIP), it follows that $\bigcap_{A \in \mathcal{A}} \overline{A} \neq \emptyset$ , so I'm going to try using this in proving $X \times Y$ is Lindelöf, where each factor is Lindelöf. Suppose that $\mathcal{A} \subseteq \mathcal{P}(X \times Y)$ has the CIP, but $\bigcap_{A \in \mathcal{A}} \overline{A} = \emptyset$ . Then there exists $\mathcal{D} \supset \mathcal{A}$ which is maximal with respect to the CIP. If all my calculations are right, both $\{\pi_1 (A) \mid A \in \mathcal{A} \}$ and $\{\pi_2(A) \mid A \in \mathcal{A} \}$ have the CIP, which means there exists $\mathcal{D}_1 \subseteq \mathcal{P}(X)$ and $\mathcal{D}_2 \subseteq \mathcal{P}(Y)$ containing the respect set which are maximal with respect to the CIP.... At this point, I don't know what to do. My whiteboard is messy with many failed attempts...I was thinking that considering the set $\{D_1 \times D_2 \mid D_i \in \mathcal{D}_i \}$ might lead to some desired contradiction, perhaps contradicting the fact that $\mathcal{D}$ is maximal in $X \times Y$ , but I presently cannot see how. I could use some help.","['general-topology', 'lindelof-spaces']"
2651925,Is stopping time directed convergence in probability equivalent to a.s. convergence?,"Let $X,X_1,X_2,...$ be random variables on the probability space $(\Omega, \mathcal{F}, P)$. Let $S$ denoted the set of all bounded stopping times on $(\Omega, \mathcal{F}, (\mathcal{F}_n))$, where $\mathcal{F}_n = \sigma(X_1,...,X_n)$. $S$ is a directed set under the pointwise partial order $\leq$, so we can speak of stopping time directed nets of random variables. We write $X_\tau \xrightarrow{p} X$ to mean: for all $\epsilon, \delta > 0$, there exists $\sigma \in S$ such that $\tau \geq \sigma$ implies
$$P\Big( \Big\{\omega: |X_{\tau(\omega)}(\omega) - X(\omega)| > \delta  \Big\} \Big) < \epsilon. $$ It's clear that if $X_n \xrightarrow{a.s.} X$, then $X_\tau \xrightarrow{p} X$. A paper I was reading seemed to claim that the converse is also true and obvious, but I cannot see it. The argument given is that if $X_\tau \xrightarrow{p} X$, then there exists $\tau_1 < \tau_2 < ... \in S$ such that 
$$X_{\tau_n} \xrightarrow{a.s.} \limsup_n X_{\tau_n} = \limsup_n X_n.$$
I don't understand how this proves the claim, and I guess I'm just confused about something simple. Any pointers or hints would be appreciated.","['probability-limit-theorems', 'probability-theory', 'stopping-times']"
2651978,Is Banach Space of Lipschitz Function Separable?,"To simplify the question, let $X=[0,1]$ with Euclidean distance. Denote $$L_f=\sup_{x,y\in X}\frac{|f(x)-f(y)|}{|x-y|},$$
and
$$||f||_L=\max\{L_f,||f||_\infty\}.$$
Consider space $$Lip_0(X)=\{f: f(0)=0,||f||_L<\infty\}.$$
On $Lip_0(X)$, $||.||_L$ is equivalent to $L_f$, because $X$ is bounded.
There are some famous results about this space, for example $Lip_0(X)$ is complete w.r.t. $||.||_L$ and separable w.r.t. $||.||_\infty$. I was wondering if $Lip_0(X)$ is separable under the norm $||.||_L$. And if not, what kind of conditions or modifications are needed to make it separable. Thanks a lot!","['banach-spaces', 'normed-spaces', 'functional-analysis', 'general-topology', 'lipschitz-functions']"
2651991,Maximizing Winnings - Dice Roll Strategy,"Let’s consider a simple dice game. Two fair 6-sided dice are rolled. Let $X$ is the sum of the two dice. If $X = 7$, then the game ends and you win nothing (winnings = 0). If $X \neq 7$, then you have the option of either stopping the game and receiving $X$ (what you rolled on your last roll) or starting the whole process over again. Now consider this strategy to play: pick a number $i$, where $2 \leq i \leq 12$, and stop playing the first time that a value greater than or equal to $i$ is rolled (or until you are forced to stop as a result of rolling a 7). Define $Y_i = $  winnings when you use this strategy with chosen value $i$. We are interested in the value $i$ that maximizes the expected winnings $\mathbb{E}[Y_i]$ over the all possible choices of $i$. To make a long story short, it turns out that the value of $i$ that maximizes the expected winnings $\mathbb{E}[Y_i]$ for the game is $i = 8.$ For this problem, what we actually want is for you to explicitly compute the expected winnings $\mathbb{E}[Y_i]$ for $i = 5, 6, 8$ and $9$ to show why the expected winnings is maximized when $i = 8$. You do not need to consider the cases where $i = 2, 3, 4, 10, 11$ or $12$. Attempt: 
Tried Expressing $\mathbb{E}[Y_i \mid X = 7] = \text{-Winnings}$ $\mathbb{E}[Y_i \mid X < i, X \neq 7] = X + \mathbb{E}[Y_i]$ $\mathbb{E}[Y_i \mid X \geq i, X \neq 7] = X$ $\mathbb{E}[Y_i \mid X = 7] = -(\mathbb{E}[Y_i | X < i, X \neq 7] + \mathbb{E}[Y_i | X \geq i, X \neq 7])$ But no matter what I do, I'm getting an incorrect answer as $8$ should be the maximum, but it's not... Please help!","['expectation', 'probability', 'dice']"
2652008,How many sub-square matrices does a square matrix have and is there a simple formula for it?,"Consider an $n \times n$ matrix $M$. I want to find the determinant for ALL sub-square matrices of $M$. There may be a better way but my method is to find all sub-square matrices and check them individually. How many sub-square matrices does a square matrix have and is there a simple formula for it? The other problem is, after checking several videos I am not sure what counts as a sub-square matrix. I thought I could use the formula for the sum of squares, i.e.
$$S(n) = \frac{n(n+1)(2n+1)}{6}$$ But this gives sixteen $(1 \times 1)$ matrices, nine $(2 \times 2)$ matrices, four $(3 \times 3)$ matrices and one $(4 \times 4)$ matrices, i.e. a total of $30$ sub-square matrices. \begin{pmatrix}
2 & 3 & 1 & 1 \\
1 & 2 & 3 & 1 \\
1 & 1 & 2 & 3 \\
3 & 1 & 1 & 2 
\end{pmatrix} But, for example, I can find thirty-six $(2 \times 2)$ matrices by observation, i.e. six for any pair of rows. For example, the first two rows give:
\begin{equation}
\begin{pmatrix}
2 & 3 \\
1 & 2
\end{pmatrix}
\begin{pmatrix}
2 & 1 \\
1 & 3
\end{pmatrix}
\begin{pmatrix}
2 & 1 \\
1 & 1
\end{pmatrix}
\begin{pmatrix}
3 & 1 \\
2 & 3
\end{pmatrix}
\begin{pmatrix}
3 & 1 \\
2 & 1
\end{pmatrix}
\begin{pmatrix}
1 & 1 \\
3 & 1
\end{pmatrix}
\end{equation}
If I do the same for rows $1$ and $3$, $1$ and $4$, $2$ and $3$, $2$ and $4$, and $3$ and $4$, I get thirty-six sub-square matrices by considering only $(2 \times 2)$ matrices. So now I am wondering if my way of counting sub-square matrices is wrong. Any ideas?","['matrices', 'combinatorics', 'sums-of-squares', 'determinant']"
2652048,Prove that the sequence is bounded and subsequences converge,"Sequence is defined in the following way: $x_1 = \frac{2}{3}$, $x_{2n+1} = \frac{x_{2n}}{3} + \frac{2}{3} $, $x_{2n} = \frac{x_{2n-1}}{3}$ I need to show that the sequence is bounded, i.e  $0<x_{n}<1$, and it does not converge. 
This is what I have done so far: I have separated odd and even elements as the following subsequences: $x_{2n+1} = \frac{x_{2n-1}}{9} + \frac{2}{3} $ $x_{2n} = \frac{x_{2n-2}}{9} + \frac{2}{9} $ And I have found that $x_{2n+1} \to \frac{3}{4} $ and  $x_{2n} \to \frac{1}{4} $ I know that $x_n$ is divergent, as the subsequential limits are not equal, but I have trouble showing that: $x_n$ is bounded from above. Do I need to look at subsequences separately and show that they are monotone or do I need to somehow estimate $x_n$? Showing that $x_{2n}$ and $x_{2n+1}$ are convergent","['real-analysis', 'sequences-and-series', 'limits']"
2652066,"Apparently smooth function, discontinuous derivative","This question is about the existence of discontinuous derivatives, but it doesn't provide much examples except this one and $y = |x|$. The function $$f(x) = \left\{ \begin{array}{lr}
\cos(ax) & 0 \leq x \leq c\\
\cos(ac) e^{-b (x - c)} & x > c
\end{array}
\right.$$ has $[0, +\infty)$ as its domain. $a, b, c \in \mathbb (0,+\infty)$ are real constants. Its first derivative is $$f'(x) = \left\{ \begin{array}{lr}
-a \sin(ax) & 0 \leq x \leq c\\
- b \cos(ac) e^{-b (x - c)} & x > c
\end{array}
\right.$$ $f'(x)$ is continuous only when $a/b = \cot(ac)$ and not in general. $f(x)$ is a continuous function, without a vertical tangent, infinitely differentiable in its domain, and $a,b$ can be chosen such that the function doesn't have corners in $x = c$; despite this, it has a discontinuous derivative. 1) How can (even graphically) the derivative be not continuous where the function is so? 2) Are there any other similar examples that can be done?","['derivatives', 'real-analysis', 'continuity', 'calculus']"
2652095,Mutation probability,"Say we have a sample of DNA from  population A and population B as shown below. (Both samples taken from today). Find the following $$ \mathbb{P}(M>0 | T_2,...,Tn) $$ where $T_i$ is the length of branch $i$ , and $M$ is the total number of mutations happening on the branch with a solid line in the above picture. Hence, find $ \mathbb{P}(M>0) $ . I know that: $T_i$ are independent exponential random variables with parameter $i \choose{2}$ $l$ is the total tree length: $l = 2T_2 + 3T_3 + \ldots nT_n$ . The mutations occur as a Poisson process along the edges of the tree at rate $\frac{\theta}{2}$ (meaning in total it is of rate $\frac{\theta l}{2}$ ) My guess: $$ \mathbb{P}(M>0 | T_2,...,Tn) = 1- \mathbb{P}(M=0 | T_2,...,Tn) = 1-e^{-\theta l/2} \sim exp\left(\frac{\theta l }{2}\right)$$ Is this right? If so how would we find $P(M>0)$ ?","['statistics', 'probability', 'biology']"
2652129,Is this sum rational or not? $1/(q+1)+1/(q+2)(q+1)...$ where $q$ is an integer,$S = \frac{1}{q+1}+\frac{1}{(q+2)(q+1)}+\frac{1}{(q+3)(q+2)(q+1)}...$ I know that $0<S<1$. But is it rational? I took this series from(proof by contradiction that $e$ is is irrational ): http://www.mathshelper.co.uk/Proof%20That%20e%20Is%20Irrational.pdf In the paper it assumes that $e=p/q$. So $q$ is not allowed to be $0$.,"['number-theory', 'real-analysis', 'sequences-and-series']"
2652162,Geometric proof for irrationality of $\pi$,Is there a geometric proof for irrationality of $\pi$? That would be neat.,"['number-theory', 'euclidean-geometry', 'irrational-numbers']"
2652169,Alternating series of primes,"Consider the infinite alternating series: $2-3+5-7+11-13+17...$ taken over all primes.
Partial sums at odd terms gives: $$2-3+5=2^2\\
2-3+5-7+11=2^3\\
2-3+5-7+11-13+\dotsb+23=2^4\\
\vdots$$ Is there a proof that there are infinite partial sums that give as a result a number of the form $2^{k}$ ?","['number-theory', 'prime-numbers', 'sequences-and-series', 'elementary-number-theory']"
2652173,Finding a matrix from its product with its transpose,"Suppose $A$ is a $3 \times 3$ matrix. If $A A^T = B$ and $A^T A = C$, where $B$ and $C$ are known and $B \neq C$, can I uniquely determine A? $A$ has 9 independent elements. Since $B$ and $C$ are symmetric, they have 6 independent entries each. Thus I have an overdetermined nonlinear system of equations with 9 variables and 12 equations. Does a unique solution exist for this system? How can I prove that it does or doesn't?",['matrices']
2652184,Using bounds for an analytically intractable differential equations problem,"Exercise 2.8.6 from Strogatz's Nonlinear Dynamics and Chaos with Applications asks us to consider the initial value problem $\dot x = x+e^{-x}$, $x(0)=0$ $\left(\dot x\, \text{means}\, \frac{dx}{dt}\right)$, which cannot be solved analytically. Part (b) of the exercise states as follows: Using some analytical arguments, obtain rigorous bounds for the value of $x$ at $t=1$. In other words, prove that $a<x(1)<b$ for $a$, $b$ to be determined. By being clever, try to make $a$ and $b$ as close together as possible. I was given a hint from my professor that, in other words, we want to approximate $$\psi(x) \leq x+ e^{-x} \leq \phi(x) $$ for $\phi(x),\,\psi(x) \geq 0$, and that essentially what we want to do is solve the following three IVPs:
$$(1) \qquad\frac{dx}{dt} = \psi(x) \\ \qquad x(t_{0})=x_{0}, $$
$$ (2) \qquad  \frac{dx}{dt} = x+e^{-x} \\ \qquad x(t_{0}) = x_{0},$$
and 
$$ (3) \qquad \frac{dx}{dt} = \phi(x) \\ \qquad x(t_{0})=x_{0} .$$
Then, if we call the solution to $(1)$ $a(t)$, the solution to $(2)$ $x(t)$, and the solution to $(3)$ $b(t)$, the problem boils down to showing that $$a(1) \leq x(1) \leq b(1). $$ In that regard, I am very close to finding $b(1)$. To get there, I considered the Taylor expansion for $$e^{-x} = \sum_{n=0}^{\infty} (-1)^{n} \frac{x^{n}}{n!} =1-x + \frac{x^{2}}{2} - \frac{x^{3}}{3!}+ \cdots$$ so that $x+e^{-x} = x + \sum_{n=0}^{\infty} (-1)^{n}\frac{x^{n}}{n!}.$ As a bound, I decided to show that $x+e^{-x} \leq 1 + \frac{x^{2}}{2}$ $\forall x \geq 0.$ To that effect, I let $h(x) = x + e^{-x} - 1 - \frac{x^{2}}{2}$, and then I considered $h^{\prime}(x) = 1 - e^{-x} - x$ and $h^{\prime \prime}(x) = e^{-x} - 1$. Since $h^{\prime \prime}(x) > 0$ for $x < 0$, $h^{\prime}(x)$ is increasing on $(-\infty, 0)$. Since $h^{\prime \prime}(x) < 0$ for $x > 0$, $h^{\prime}(x)$ is decreasing on $(0, \infty)$. But, $h^{\prime}(x) = 0$ for $x = 0$, so $h^{\prime}(x) < 0$ for $x<0$ and $h^{\prime}(x) < 0 $ for $x > 0$ as well - the only time $h^{\prime}(x)$ is not negative is when $x=0$, and there it is simply $=0$. Therefore, $h$ is everywhere decreasing, and since $h(x)=0$, $h > 0$ for $x<0$ and $h < 0$ for $x > 0$, so we have that $h(x) = x + e^{-x} - 1 - \frac{x^{2}}{2} \leq 0$ for $x \geq 0$, which implies that $x + e^{-x} \leq 1 + \frac{x^{2}}{2} $, as desired. So, then I let $\phi(x) = 1 - \frac{x^{2}}{2}$, and I need to solve Equation $(3)$ above. Using separation of variables, I have that $\frac{dx}{dt} = 1 - \frac{x^{2}}{2}$ becomes $\displaystyle \int \frac{1}{\displaystyle 1 - \left(\frac{x}{\sqrt{2}} \right)^{2}} dx = \int dt$, which comes out to be 
$$\ln \left(1 + \frac{x}{\sqrt{2}} \right) - \ln \left(1 - \frac{x}{\sqrt{2}} \right) = \sqrt{2}t + C, $$ And then, exponentiating both sides, we get $$\frac{1+\frac{x}{\sqrt{2}}}{1-\frac{x}{\sqrt{2}}}= Ce^{\sqrt{2}t} $$ From here, I don't know what to do . The problem tells us what the initial condition when $t=0$ is - it tells us that $x(0) = 0$, but it doesn't tell us what it is for $t=1$. And since we're trying to find bounds for $x(1)$, I'm assuming that for Equation (3) (actually in all three of the equations), we want $t_{0} = 1$, right? The only problem is that I can't figure out what $x_{0}$ is in that case, so how do I solve for $C$? Also, if you could help me out on the left bound (i.e., solving for $\psi(x)$), I would appreciate that very much! Thank you ahead of time for your help.","['ordinary-differential-equations', 'dynamical-systems', 'initial-value-problems']"
2652196,Galois group of $(x^5-3)(x^5-7)$ over $\Bbb Q$,"I am having some trouble with the following question: Let $f(x)=(x^5-3)(x^5-7)\in\Bbb Q[x]$. Find the degree of the splitting field of $f$ over $\Bbb Q$, and determine the Galois group of $f$ over $\Bbb Q$. I noticed that $x^5-3$ and $x^5-7$ are irreducible in $\Bbb Q[x]$, and so their Galois groups are isomorphic to a transitive subgroup of $S_5$. I also found that the splitting fields of these two quintics are $\Bbb Q(\sqrt[5] 3,\alpha)$ and $\Bbb Q(\sqrt[5] 7,\alpha)$ where $\alpha^5=1$. Since $\alpha$ has degree $4$ over $\Bbb Q$ and $\sqrt[5] 3,\sqrt[5] 7$ have degree $5$ over $\Bbb Q$, we have that the degrees of these splitting fields are $20$. Hence, we have $$\operatorname{Gal}(x^5-3/\Bbb Q)\cong F_{20}\cong \operatorname{Gal}(x^5-7/\Bbb Q) $$
But now I'm supposed to use a theorem we proved that $\operatorname{Gal}(f/\Bbb Q)$ is a subgroup of the direct product of the above Galois groups. I am not sure how to do this: i.e., $\color{red}{\text{how to determine which subgroup $\operatorname{Gal}(f/\Bbb Q)$ corresponds to}}$. For the degree of the splitting field of $f$ over $\Bbb Q$, I argued that it is $100$ since the compositum $\Bbb Q(\sqrt[5] 7,\alpha)\Bbb Q(\sqrt[5] 3,\alpha)=\Bbb Q(\sqrt[5] 3,\sqrt[5] 7,\alpha)$ has degree $5\cdot 5\cdot 4=100$ over $\Bbb Q$. $\color{red}{\text{Does this make sense}}$? I also had another question to determine the Galois group of $(x^3-2)(x^4-2)$ over $\Bbb Q$. I showed that $x^3-2$ has Galois group $S_3$ and $x^4-2$ has Galois group $D_4$. Then I argued that the intersection of the two splitting fields, $\Bbb Q(\sqrt[3]2,\omega)\cap\Bbb Q(\sqrt[4]2,i)$ is just $\Bbb Q$, so we have $\operatorname{Gal}((x^3-2)(x^4-2)/\Bbb Q)\cong S_3\times D_4$. $\color{red}{\text{Does this make sense}}$? In order to show $\Bbb Q(\sqrt[3]2,\omega)\cap\Bbb Q(\sqrt[4]2,i)=\Bbb Q$ I just argued that $\sqrt[4]2,i\notin\Bbb Q(\sqrt[3]2,\omega)$, $\color{red}{\text{is this sufficient}}$?","['abstract-algebra', 'galois-theory', 'extension-field']"
2652243,Some convincing reasoning to show that to prove that Ramanujan tau function is multiplicative is very difficult,"I am curious to know (some words or reasoning  about how to justify) why to prove that the so-called Ramanujan $\tau$ is a multiplicative function is (was) very difficult. In this Wikipedia is showed the conjecture due to Ramanujan, and I see that the function $\tau(n)$ is defined from the first formula of the Wikipedia's article. Then I suspect that it was the difficulty to prove that such arithmetic function is multiplicative. But I would like to know a reasoning, if it is possible, to know why is very difficult to prove that this function is multiplicative (since the definition of this arithmetic function ins't explicit in term of prime powers). Question. How do you convince me that from the/a definition of the $\tau(n)$ function is very difficult to show that is multiplicative? Many thanks. Feel free to refer the literature if you need to answer invoking the literature (I can to search and read those literature that you need to refer).","['number-theory', 'analytic-number-theory', 'multiplicative-function']"
2652263,Directional derivative of a function (Munkres),"Let $A\in \mathbb{R^n}$; let $f:A\rightarrow \mathbb R^n.$ Show that if $f'(a,u)$ exists, then $f'(a,cu)$ exists and equals $cf'(a,u).$ This exercise is from Munkres. I suppose $a\in \mathbb R^n$ and $c\in \mathbb R.$
I tried to calculate $f'(a,cu)$ and I got this: $f'(a,cu)=\lim_{t\to 0}\frac{ f(a+t(cu))-f(a)}{t}=\lim_{t\to 0}\frac{ f(a+(tc)u)-f(a)}{t}$ I took $r=ct.$ Then, $r\to 0$ since $t\to 0.$
Then $\lim_{t\to 0}\frac{ f(a+(tc)u)-f(a)}{t}=\lim_{r\to 0}\frac{ f(a+ru)-f(a)}{r}$
and the last limit exists since $f'(a,u)$ does. Is my argument correct? And how can I show the equality?
Thanks in advance!","['multivariable-calculus', 'real-analysis', 'analysis']"
2652305,"7 white balls and 5 black balls. Find the probability that the 6th ball drawn is white, while...","A box contains 7 identical white balls and 5 identical black balls. They are to be drawn randomly, one at a time without replacement, until the box is empty. Find the probability that the 6th ball drawn is white, while before that exactly 3 black balls are drawn. I thought the problem this way: There are $\dfrac{12!}{7!5!}=792$ ways to order the balls in general. Then, if we fix black,black,black,white we have eight other balls to place in the other bins, which is $\dfrac{8!}{6!2!}=28$. Therefore the probability is $\dfrac{28}{792}=\dfrac{7}{198}$. But the book says the answer is $\dfrac{25}{132}$, why tho!?","['combinatorics', 'probability']"
2652321,Given a grammar study its type and the language it generates,"I have to find out if the grammar $G=(\{A,B\},\{x,y\},A)$, where the productions of $P$ are given by $$\begin{array}{ccc}A&\rightarrow &x\:A/y\:B\\B&\rightarrow &x\:B/x\end{array}$$ is type 3 and generates a finite language. When it says ""type 3"" it refers to the Chomsky Hierarchy of the types of grammars, and there are 4 types, where $G_3\subset G_2\subset G_1\subset G_0$. As I think it is in type 3, I am going to write the definition that I have: A grammar is Type 3 if all production $x\to y$: $x \text{ it is a single symbol of the non-terminal elements;}\\
y \text{ is }\left\{\begin{array}{l}\text{The concatenation of 2 symbols, one of them being terminal,}\\\qquad\qquad\text{or}\\\text{A single terminal symbol,}\\\qquad\qquad\text{or}\\\text{It is the null word.}\end{array}\right.$ With this I can affirm that both production $A\to x\: A/y\: B$ and production $B\to x\: B/x$ meet these requirements, and therefore the grammar is type 3. With respect to what generates a finite language, I did not start making the derivation tree, but I understand that the production is being called ""recursively""; that is, it will always continue to generate words (for example, $A$ has two outputs: $x\: A$ - it is generated on itself - and $y\: B$). Therefore, it will never generate a finite language. Is it right? Also, a very short question: If we have for example that a generated language is $L(G)=\left\{0^{\ast} 1^{\ast}\right\}$, is it an infinite language? Thanks!","['formal-grammar', 'formal-languages', 'discrete-mathematics']"
2652330,What is the general solution of $xy' - 2y = -x$?,"I am trying to find the general solution of $xy' - 2y = -x$. I normalize the equation to get $y' - \frac{2}{x}y = -1$. I get the integrating factor as $x^{-2}$, and so $y = x^{2}\int x^{-2}*(-1)\,dx $. Solving for this integral, I get $ y = x + \frac{C}{x^2}$, but this is not the answer I get on wolfram, which is $Cx^2 +x$. I'd like to know where I'm going wrong.",['ordinary-differential-equations']
2652331,Find the group given the presentation,"I have the group presentation $$ G := \langle a,b |a^8=b^8=1,a^{-1}ba=b^{-1},b^{-1}ab=a^{-1} \rangle. $$ Which group is it? Notably, what about its order?","['group-theory', 'group-presentation']"
2652353,"I have a bag with 3 coins in it. One of them is a fair coin, but the others are biased trick coins.","When flipped, the three coins come up heads with probability 0.5, 0.3, 0.6 respectively. Suppose that I pick one of these three coins entirely at random and flip it three times.
1. What is P(HTT)? (i.e., it comes up heads on the first flip and tails on the last two flips.)
2.Assuming that the three flips, in order, are HTT, what is the probability that the coin that I picked was the fair coin?
Don't need to reduce fractions Work:
1. ((.5*.5)/(.5*.5))/3 + ((.3*.5)/(.7*.5))/3+ ((.6*.5)/(.4*.5))/3 - I think this is wrong
2. I dont know how to do",['probability']
2652368,Complexified cotangent bundle,"I have never worked with differential geometry over $\mathbb{C}$, and I feel a little bit confused. If $(M,J)$ is an almost complex manifold of dimension $2n$, we can extend $J_{p}$ to $T_{p}(M)\otimes_{\mathbb{R}}\mathbb{C}$ (which is a $2n$ dimensional $\mathbb{C}$-vector space), by $$J_{p}(v\otimes z)=J_{p}(v)\otimes z.$$
Then, $J_{p}$ is a $\mathbb{C}$-linear map and $J_{p}^{2}=-\text{Id}$, so $J_{p}$ has two eigenvalues: $i$ and $-i$ If $T_{1,0}$ is the $i$-eigenspace and $T_{0,1}$ is the $-i$-eigenspace (which are complex subspaces), it can be proved that $TM\otimes_{\mathbb R}\mathbb{C} \cong T_{1,0}\oplus T_{0,1}$. Similarly, $T^{*}M\otimes_{\mathbb R} \mathbb{C} \cong T^{1,0}\oplus T^{0,1}$.
Then, $\bigwedge^{k}(T^{*}M\otimes_{\mathbb{R}} \mathbb{C})$ equals $\oplus_{m+l=k}(\bigwedge^{m}(T^{1,0})\otimes \bigwedge^{l}(T^{0,1}))$. My question is: I believe that the last tensor product should be taken over $\mathbb{C}$, since $T^{1,0}$ and $T^{0,1}$ are vector spaces over $\mathbb{C}$. Am I right? Moreover, how do elements of $\bigwedge^{m}(T^{1,0})\otimes \bigwedge^{l}(T^{0,1})$ look like? I am following these notes (page 78): https://people.math.ethz.ch/~acannas/Papers/lsg.pdf Thanks in advance!","['complex-geometry', 'differential-forms', 'differential-geometry', 'exterior-algebra']"
2652393,Derivatives of Infinite Product that Diverges to 0,"Consider the function given by $$f_n(x) = \prod\limits_{k=2}^n \left( 1-\frac{x}{k} \right).$$ Then for all $x \in (0,2)$, we have that $$\lim_{n\to\infty}\; f_n(x) = \prod\limits_{k=2}^\infty \left( 1-\frac{x}{k} \right) =0. $$
This follows from the fact that if $q_k \in [0,1)$ for all $k$, then $\prod\limits_{k=1}^{\infty} (1-q_k) = 0$ if and only if $\sum\limits_{k=1}^\infty q_k$ diverges (See for example here and here ). I am interested in the derivatives of this function $f_n$. For instance, let $f_n^{(j)}$ denote the $j$-th derivative of $f_n$. Is it true that $\lim_{n\to\infty} f_n^{(j)}(x) = 0$ for all $x\in(0,2)$? Considering the first derivative, we have that from the product rule:
$$ f_n'(x) =  \sum\limits_{k=2}^n \frac{-1}{k} \prod\limits_{i \neq k} \left(1-\frac{x}{i} \right)\tag1 \label 1 .$$
Now, we have that 
\begin{align}\prod\limits_{i \neq k} \left(1-\frac{x}{i} \right) &\leq \prod\limits_{i =2}^{n-1} \left(1-\frac{x}{i} \right)\\
&= \exp\left\{ \sum_{i=2}^{n-1} \log\left(1-\frac{x}{i} \right)  \right\} \\
&\leq \exp\left\{ \sum_{i=2}^{n-1} -\frac{x}{i} \right\} \tag2  \\
&\leq \exp\left\{-x\log(n)+x \right\}\tag3\\
&= {\left(\frac{e}{n}\right)}^x.\end{align}
Where (2) above follows from the fact that $\log(1-y)\leq -y$ for all $y<1$, and (3) follows from $\sum\limits_{i=2}^{n-1}\frac{1}{i} \geq \log(n)-1$. Now, by noticing all the terms in the sum in $\ref1$ are nonpositive, we have the following
\begin{align*}
 f_n'(x) \geq {\left(\frac{e}{n}\right)}^x\sum\limits_{k=2}^n \frac{-1}{k} \geq -{\left(\frac{e}{n}\right)}^x \log(n)
\end{align*}
Finally, because $f_n'(x)\leq 0$ and $\lim_{n \to \infty}  {\left(\frac{e}{n}\right)}^x \log(n) = 0$, we are able to conclude that $\lim \; f_n'(x) = 0$ for all $x \in (0,2)$. Now, is there a way to show this for the $j$-th derivative $f_n^{(j)}$? I am unable to even currently show that the second derivative is zero (in my simulations it seems to approach zero, but at a rate slower than the first derivative). An idea that I have briefly considered using is considering the differentiation of the infinite product (see here and similarly here ). However in my case my infinite product diverges to zero, so I am not sure how useful this will be. Also, I am interested in the higher order derivatives of this infinite product, not just the first derivative. I appreciate any input on this problem!","['derivatives', 'infinite-product']"
2652460,"Color $27$ unit cube so that by rearranging, they could form a blue $3\times3$ cube, a green one, and a red one?","I searched but there's not much useful information. My instinct is that it is not possible, but I don't know how to show it. To make it clear, there are $27$ unit cubes, that is, $6\times27$ sides to be colored with blue, green, or red. Then you need to arrange them so that they form a $3\times3\times3$ cube, whose surfaces are blue. Then you rearrange unit cubes, so that they form a $3\times3\times3$ cube, whose surfaces are green. Then you rearrange them again, this time get a red $3\times3\times3$ cube. More generally, can you color $n^3$ unit cubes with $n$ colors, so that after arraging, they can form $n$  $n\times n\times n$cubes, each cube's surfaces is in a different color? When $n=1,2$, the answer is yes and the solution is obvious. But when $n$ is bigger, the problem seems complex, and I totally have no clue.","['puzzle', 'discrete-mathematics']"
2652515,isoperimetric inequality proof,"While going through Pressley's Elementary Differential Geometry I came across Isoperimetric inequality.For a simple closed curve $\gamma$ with length $l$ and area of interior $A$ , $4 \pi A \leq l^{2}$. Equality holds if and only if the curve is a circle. Author uses Wirtinger's inequality to prove the result. Firstly he makes assumption that the curve is of parameter $t= \frac{\pi s}{l}$. $\gamma $ is a unit sped curve and having the period $\pi$. Also makes the assumption that curve begins and ends at origin. The he starts with parametric equations $x=rcos \theta$ and $y= rsin \theta$ and proves the inequality. I cannot understand why he uses the the parametric equation of a circle for general curve. If we proceed in this way the later part of the proof follows by routine calculations.","['inequality', 'differential-geometry']"
2652527,"If $x \cos\theta+y\sin\theta=a$ and $x\sin\theta-y\cos\theta=b$, then $\tan\theta=\frac{bx+ay}{ax-by}$. (Math Olympiad) [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I tried to solve it but I can’t get the answer. Please help me in proving this trig identity: If 
  $$x \cos\theta+y\sin\theta=a$$
  $$x\sin\theta-y\cos\theta=b$$ 
  then $$\tan\theta=\frac{bx+ay}{ax-by}$$ I've spent many hours trying.
Thanks in advance.","['algebra-precalculus', 'proof-writing', 'trigonometry', 'systems-of-equations']"
2652532,Understanding the formula for stereographic projection of a point.,"I was wondering about the equation of line I can write which can help me finding the coordinates of Point $P'$ in relation with coordinates of points on the sphere that is $P$ . Let the $P'(X,Y)$ , then how can I find these following coordinates - I tried equating the slope of $NP'$ with that of the slope of $PP'$ .
but could only write $\frac{y}{x} = \frac{Y-y}{X-x}$ . I donot know how it is getting these coordinates - $(X,Y) = (\frac{2ax}{a-z},\frac{2ay}{a-z})$ and $(x,y,z) = (\frac{2aX}{X^2+Y^2+a^2},\frac{2aY}{X^2+Y^2+a^2},\frac{X^2+Y^2-a^2}{X^2+Y^2+a^2})$ ? Here we are following this notation that - points lying on the sphere are represented by $(x,yz)$ and points lying outside the sphere by $X,Y$","['coordinate-systems', 'stereographic-projections', 'spheres', 'geometry', 'surfaces']"
2652535,Convergence in Distribution of Sums of Random Variable,"Suppose I have $X_1,X_2,...,X_n$ random variables that are independent and identically distributed, from ANY distribution.  Suppose that $E(X_i)=\mu$ and $V(X_i)=\sigma^2$. Suppose I define the following random variable: $$Y=\sum_{i=1}^nX_i$$ What is the limiting distribution of $Y$?  That is, as $n$ goes to infinity, what distribution can $Y$ be approximated by? My intuation tells me that $Y\rightarrow N(n\mu,n\sigma^2)$.  In other words, say $200$ was a sufficiently large number for $n$.  Then I could approximate $Y$ by a normal distribution with mean $200\mu$ and variance $200\sigma^2$.  Is this true, and if so, how can you prove it?  If not, what is the limiting distribution of $Y$?","['probability-theory', 'probability-distributions', 'central-limit-theorem', 'statistics', 'probability']"
2652544,Commutators of tensor product of Pauli matrices,"Given tensor product of rank-2 Pauli matrices $\sigma^a$. Each $\sigma^a$ is related to the generator of SU(2) Lie algebra. We know they satisfy $$[\sigma^a, \sigma^b ] = 2 i \epsilon^{abc} \sigma^c$$ Do you know any equality/identity to simplify:
$$
[\sigma^a \otimes \sigma^c, \sigma^b \otimes \sigma^d] = ?
$$
also
$$
[\sigma^a \otimes \sigma^c  \otimes \sigma^e, \sigma^b \otimes \sigma^d  \otimes \sigma^f] = ?
$$
$$
[\sigma^a \otimes \sigma^c  \otimes \sigma^e \otimes \sigma^g, \sigma^b \otimes \sigma^d  \otimes \sigma^f \otimes \sigma^h] = ?
$$
so that the final answers have no commutators? Commutator is defined by default as
$$
[A,B]:=AB-BA
$$","['abstract-algebra', 'linear-algebra', 'lie-algebras', 'tensor-products']"
2652589,"How to show that the set of all non-decreasing functions $f:\mathbb{Z} \rightarrow \{ 0, 1\}$ is countable? [closed]","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Given the set $A = \{f:\mathbb{Z} \rightarrow \{ 0,1 \} |$ if $n \geq m$ then $f(n) \geq f(m) \}$ How would I go about showing $A$ is countable? Thank you!","['real-analysis', 'cardinals', 'elementary-set-theory']"
2652666,"For all sets $A, B, C$, If $A \Delta B = A \Delta C$, then $B \subseteq C$","I get the general reason as to why this is True. I'm just having trouble trying to formulate this into a logically correct proof. My approach was: (1) Assume $A \Delta B = A\Delta C $ (2) Let $x$ be an interger such that $x \subseteq A \Delta B$ and $x \subseteq A \Delta C$, then $x \subseteq A \cup B$ and $x \subseteq A \cup C$ (3) Seperate into cases, where either $x \subseteq A$, or $x \not\subset A$ And this is where I get stuck. I'm not sure if separating into cases is the right approach. Im stuck on linking  $A \Delta B = A\Delta C $ with the fact that if they have the same elements after taking the symmetric difference, all the elements of B must also be in C.","['elementary-set-theory', 'discrete-mathematics']"
2652716,Trace of a matrix product,"For an arbitrary matrix $A$ and a given matrix $B$, is is possible to generate a matrix $C$ such that $\text{Trace}(ABC) = \text{Trace}(AB)$ barring the trivial identity case? If not in the general case, is it possible if $B$ is symmetric? Entries in all the matrices are real. Any tips are appreciated","['matrices', 'trace', 'linear-algebra']"
2652789,Non-contractible space with trivial homotopy groups,"What is an example of a non-contractible space $X$ with $\pi_n(X) = 0$ for all $n\geq 0$ (note in particular $X$ is path connected)? Motivation: Whitehead's theorem implies that no such CW complex $X$ exists. I'd like to know a counterexample to the ""general Whitehead theorem"".","['algebraic-topology', 'general-topology']"
2652799,$O_p$ and $o_p$ notations in asymptotic normality proof,"I'm reading the proof of Theorem 5.21 (asymptotic normality of M-estimators) in van den Vaarts book ""Asymptotic Statistics"" (see the attached picture). (The theorem assumes that $\hat{\theta}_n \to \theta_0$ in probability as $n\to \infty$; i.e., the estimator $\hat{\theta}_n$ is asymptotically consistent.) I do not understand the last paragraph of the proof. How did he go from the $\sqrt{n}$-consistency to the last conclusion? In particular, I do not see where the inequality in page 53 coming from? how the $o_p$ term of the last equation in page 52 vanished (I guess
it      is represented now with the $o_p$ on the right hand side of
that       equation). It seems to me that the presentation (somehow) implies that
$$
O_p(1) + o_p(\sqrt{n} \|\hat{\theta}_n - \theta_0  \|) = O_p(1)\quad ?
$$
Does this hold? My understanding is that this should instead be $O_p(\max(1,\sqrt{n} \|\hat{\theta}_n - \theta_0  \| ))$. However, if it does hold, then it will follow that
 $$\sqrt{n} \|\hat{\theta}_n - \theta_0  \| = O_p(1)$$ 
(we know that this is true if the LHS converges in distribution; however, this is something that we are trying to prove!) and therefore, 
 $$ o_p(\sqrt{n} \|\hat{\theta}_n - \theta_0  \| ) = o_p(O_p(1)) = o_p(1).$$","['statistics', 'asymptotics', 'statistical-inference']"
2652821,Cosine of a matrix,"I came across this question, asked in a competitive exam. It  is as follows. Given a matrix $M = \begin{bmatrix}2&1\\1&2\end{bmatrix}$ what is the value of $cos(πM/6)$? I've tried series expansion but I think there is an alternative way doing it, any help is appreciated. Options given are \begin{bmatrix}1/2&1\\1&1/2\end{bmatrix} \begin{bmatrix}\sqrt3/4&-\sqrt3/4\\-\sqrt3/4&\sqrt3/4\end{bmatrix} \begin{bmatrix}\sqrt3/4&\sqrt3/4\\\sqrt3/4&\sqrt3/4\end{bmatrix} \begin{bmatrix}1/2&\sqrt3/2\\\sqrt3/2&1/2\end{bmatrix}","['matrices', 'matrix-calculus']"
2652824,"Considering the equation, $6 + (2k+1)\sum_{n=1}^{2k+1}p_n^{ \ \ 3}(-1)^{n+1} = x^2$.","I noticed that, $$\begin{align}3(2^3 - 3^3 + 5^3) + 6 &= 18^2 \\ \text{and } \qquad 5(2^3 - 3^3 + 5^3 - 7^3 + 11^3) + 6 &= 74^2.\end{align}$$ These equations are of the form, $$6 + (2k+1)\sum_{n=1}^{2k+1}p_n^{ \ \ 3}(-1)^{n+1} = x^2$$ such that $k\in\mathbb{N}$ and $p_n$ denotes the $n^{\text{th}}$ prime number. Here, $x\in\mathbb{Z}$ by letting $k = 1$ and $k=2$, but $k = 3$ does not hold, i.e. $x\notin\mathbb{Z}$. In fact, it appears as if $k > 2$ does not hold after testing for all $k \leqslant 61.$ Could anyone prove/disprove that there exists another equation like this for some pair $(k, x)$? Does somebody have a computer that could test for values of $k > 61$, because I only use my computer's calculator. (I used to have a program until I updated my computer software and the program lacked support for it.) Thank you in advance. Update: The general equation has been tested for $k < 10,000$ and the only pairs $(k, x) \in\mathbb{N}^2$ are $(1, 18)$ and $(2, 74)$. It is now a conjecture that these are the only solution pairs over the natural numbers. Nonetheless, I did mention in the foregoing that $x \in \mathbb{Z}$, but since we are finding $x^2$ then it is also allowable for $x$ to be natural, i.e. $x\in\mathbb{N}$.","['square-numbers', 'number-theory', 'summation', 'prime-numbers', 'sequences-and-series']"
2652892,Which of the following cannot be possible value for the number of elements of $G?$,"Suppose a finite group $G$ has an element $a$ which is not the identity such that $a^{20}$ is the identity. Which of the following cannot be possible value for the number of elements of $G?$ 1) 12 2) 9 3) 15 4) 20 My work:Possible order for $a$ is $2, 4, 5, 10, 20 $ and we know order of element divides order of the group. From the above no one divide $9$. So option 2 is right answer.","['abstract-algebra', 'group-theory', 'proof-verification']"
2652927,Why isn't minimal sigma-algebra simply the set containing all countable unions and complements [duplicate],"This question already has answers here : Generate the smallest $\sigma$-algebra containing a given family of sets (3 answers) Closed 6 years ago . For $X$ a set and $A\subset X$ the sigma-algebra generated by $A$ is
$$\sigma(A)=\bigcap\{\Sigma: \text{$\Sigma$ $\sigma$-algebra s.t. $A\subset\Sigma$}\}.$$
However why can' t we simply say
$$\sigma(A)=\{\text{all countable unions and complements of elements of $A$}\}\cup \{\emptyset,X\}?$$ What would be a good counterexample?",['measure-theory']
2652978,Kernel density estimation -Effect of bandwidth,"I am trying to learn Kernel density estimation, I need help to understand how the bandwidth $h$ affects the Kernel density estimator. Consider a Gaussian Kernel $k(x)~=~\frac{1}{\sqrt{2 \pi}} e^{-x^2}$. The Kernel density estimator is given by ${\hat{f}}_h (x) ~=~ \frac{1}{h} \sum_{i=1}^{n} K_h(x-X_i)$. Clearly, $k(x)$ is independent of $h$, where does $h$ come in?
What would be ${\hat{f}}_h (x)$? How does $h$ affect the Kernel? Thank you!","['statistics', 'probability-distributions']"
2653002,Open sets on ordered topology in $\mathbb{R\times R}$,"I am having some difficulty grasping the concept of an ordered topology in $\mathbb R \times\mathbb R $. The definition I was given is that this is the topology where $(a,b) < (c,d)$ if $a<c$ or $a= c$ and $b <d$. Would the set $(0, 1)\times(0,1]$ be open in this topology? Or how should I be thinking about this? I read this post , as well as many others, and am still confused.","['general-topology', 'order-topology']"
2653024,The complement of an asymmetric relation is nonsymmetric?,"Asymmetric relation : Given a set A and a relation R in A, R is asymmetric if it is never the case that for any ordered pair (x, y) in R, the pair (y, x) is in R. Nonsymmetric relation : If for some (x, y) in R, the pair (y, x) is not in R then R is nonsymmetric. Complement : The complement of a relation R contains all ordered pairs of the Cartesian product of A which are not members of R. The book that I am reading says that the complement of an asymmetric relation is nonsymmetric. Isn't this a counterexample: The set A is empty and relation R has no ordered pairs (i.e., R is empty). R is asymmetric. The complement of R is R, which is not nonsymmetric. So, is the book incorrect?","['relations', 'elementary-set-theory']"
2653050,Show that $ \sum_{k=0}^{n} \binom{2n+1}{2k} 3^k $ is divisible by $2^n$,"I want to prove that
$$
 \sum_{k=0}^{n} \binom{2n+1}{2k} 3^k = \sum_{k=0}^{2n} \binom{2n}{k} 3^{\lceil k/2 \rceil}
$$
is divisible by $2^n$. I tried induction the following way
\begin{align*}
  \sum_{k=0}^{n} \binom{2n+1}{2k} 3^k
   & = \sum_{k=0}^{n} \left( \binom{2n}{2k-1} + \binom{2n}{2k} \right) 3^k \\
   & = \sum_{k=0}^{n} \left( \binom{2n-1}{2k-2} + \binom{2n-1}{2k-1} + \binom{2n-1}{2k-1} + \binom{2n-1}{2k} \right) 3^k \\
   & = 4\cdot \sum_{k=0}^{n-1} \binom{2(n-1)+1}{2k} 3^k + 2\sum_{k=0}^n \binom{2n-1}{2k-1} 3^k
\end{align*}
now I can apply the induction hypothesis to the first part, hence the entire first summand is divisible by $2^{n+1}$, if I can show that
$$
 \sum_{k=0}^n \binom{2n-1}{2k-1} 3^k
$$
is divisble by $2^{n-1}$ I would be done. But here I failed. So do you have any hints how to show the original claim? EDIT : As mentioned by Will Orrick in the comments, the inductive approach works too, just prove the stronger claim that
$$
 \sum_{k=0}^{n}\binom{2n}{2k}3^k, \quad
 \sum_{k=0}^{n}\binom{2n+1}{2k}3^k, \quad
 \sum_{k=0}^{n}\binom{2n+1}{2k+1}3^k, \quad
 \sum_{k=0}^{n}\binom{2n}{2k+1}3^k \quad
$$
are all divisible by $2^n$. If $n = 1$ we get $4$, $10$, $6$ and $2$ and if $n > 1$ we have
\begin{align*}
 & \sum_{k=0}^{n}\binom{2n}{2k}3^k \\
 & = 3 \sum_{k=0}^n \binom{2(n-1)+1}{2(k-1)+1} 3^{k-1}
     + \sum_{k=0}^n \binom{2(n-1)+1}{2k} 3^k \\
 & = 3 \sum_{k=0}^{n-1} \binom{2(n-1)+1}{2k+1} 3^k + 
       \sum_{k=0}^{n-1} \binom{2(n-1)+1}{2k} 3^k \\
 & = 3 \sum_{k=0}^{n-1} \binom{2(n-1)}{2k} 3^k + 3 \sum_{k=0}^{n-1} \binom{2(n-1)}{2k+1} 3^k
    + \sum_{k=0}^{n-1} \binom{2(n-1)}{2k-1} 3^k + \sum_{k=0}^{n-1} \binom{2(n-1)}{2k} 3^k \\
 & = 4 \sum_{k=0}^{n-1} \binom{2(n-1)}{2k} 3^k + 3 \sum_{k=0}^{n-2} \binom{2(n-1)}{2k+1} 3^k + \sum_{k=1}^{n-1} \binom{2(n-1)}{2k-1} 3^k \\
& = 4 \sum_{k=0}^{n-1} \binom{2(n-1)}{2k} 3^k + \sum_{k=1}^{n-1} \binom{2(n-1)}{2k-1} 3^k + \sum_{k=1}^{n-1} \binom{2(n-1)}{2k-1} 3^k \\
& = 4 \sum_{k=0}^{n-1} \binom{2(n-1)}{2k} 3^k + 6\sum_{k=0}^{n-2} \binom{2(n-1)}{2k+1} 3^k \\
& = 4 \sum_{k=0}^{n-1} \binom{2(n-1)}{2k} 3^k + 6\sum_{k=0}^{n-1} \binom{2(n-1)}{2k+1} 3^k  
\end{align*}
and now by assumption both sums are divisible by $2^{n-1}$, and as they are multiplied with even numbers their sum is divisible by $2^n$. Similar
\begin{align*}
 \sum_{k=0}^{n}\binom{2n+1}{2k}3^k
 & = \sum_{k=0}^n \left( \binom{2n-1}{2k-2} + \binom{2n-1}{2k-1} + \binom{2n-1}{2k-1} + \binom{2n-1}{2k } \right) 3^k \\
 & = \sum_{k=1}^n \binom{2n-1}{2k-2} 3^k + \sum_{k=0}^{n-1} \binom{2n-1}{2k} 3^k + 2\sum_{k=1}^n \binom{2n-1}{2k-1} 3^k \\
 & = 3 \sum_{k=0}^{n-1} \binom{2n-1}{2k} 3^k + \sum_{k=0}^{n-1} \binom{2n-1}{2k} 3^k + 2\sum_{k=1}^n \binom{2n-1}{2k-1} 3^k \\
& = 4 \sum_{k=0}^{n-1} \binom{2n-1}{2k} 3^k +  2\sum_{k=1}^n \binom{2n-1}{2k-1} 3^k \\
& = 4 \sum_{k=0}^{n-1} \binom{2(n-1)+1}{2k} 3^k +  6\sum_{k=0}^{n-1} \binom{2(n-1)+1}{2k+1} 3^k \\
\end{align*}
and for the other sums we have
\begin{align*}
 \sum_{k=0}^{n}\binom{2n+1}{2k+1}3^k
 & = 4 \sum_{k=0}^{n-1} \binom{2(n-1)+1}{2k+1} 3^k +  2\sum_{k=0}^n \binom{2n-1}{2k} 3^k\\
 & = 4 \sum_{k=0}^{n-1} \binom{2(n-1)+1}{2k+1} 3^k +  2\sum_{k=0}^{n-1} \binom{2(n-1)+1}{2k} 3^k\\
 \sum_{k=0}^{n}\binom{2n}{2k+1}3^k
 & = 4 \sum_{k=0}^{n-1} \binom{2(n-1)}{2k+1} 3^k +  2\sum_{k=0}^{n-1} \binom{2(n-1)}{2k} 3^k
\end{align*}
and the induction hypothesis gives the result.","['binomial-coefficients', 'number-theory', 'combinatorics', 'summation', 'discrete-mathematics']"
2653069,Spiral of prime numbers.,"This is a sketch of a spiral of primes.
Up to prime 1427, the number 641 (famous number that divides Fermat numbers) is the only twin prime congruent to 1 mod 4 in the central column containing 2.
Just coincidence?","['number-theory', 'prime-numbers']"
2653081,Factoring Polynomials with four terms and three variables,"If $$\frac{x}{y} + \frac{y}{z} + \frac{z}{x} = 17,$$
  then solve for $x$, $y$ and $z$, where $x$, $y$, and $z$ are positive integers . I got a common denominator of $xyz$, but then after that I was stuck: $$x^2z + xy^2 + yz^2 = 17xyz$$ I also tried the quadratic formula, solving for $x$, but that was also a dead end: $$x = \frac{-y^2+17zy \pm \sqrt{(y^2-17zy)^2-4z^3y}}{2z}.$$ I even wrote a rudimentary program to test a few numbers: // index.js
function solve() {
    const limit = 10000;
    for (x = 1; x < limit; x++) {
        for (y = 1; y < limit; y++) {
            for (z = 1; z < limit; z++) {
                if (x*x*z + x*y*y + y*z*z === 17*x*y*z)
                    return { x, y, z };
            }
        }
    }
}

console.log(solve()); Getting undefined. 
Any help would be appreciated.",['algebra-precalculus']
2653086,On the cubic generalization $(a^3+b^3+c^3+d^3)(e^3+f^3+g^3+h^3 ) = v_1^3+v_2^3+v_3^3+v_4^3$ for the Euler four-square,"We are familiar with the Euler Four-Square identity , $$(a^2+b^2+c^2+d^2)(e^2+f^2+g^2+h^2 ) = u_1^2+u_2^2+u_3^2+u_4^2$$ where, $$u_1 = ae-bf-cg-dh\\
u_2 = af+be+ch-dg\\
u_3 = ag-bh+ce+df\\
u_4 = ah+bg-cf+de$$ or the product of two sums of four squares is itself a sum of four squares. Tinkering about, I came across a cubic version, $$(a^3+b^3+c^3+d^3)(e^3+f^3+g^3+h^3 ) = 6^{-3} (w_1^3+w_2^3+w_3^3+w_4^3)$$ where, $$w_1= 9 + a^3 + b^3 + c^3 + d^3 + e^3 + f^3 + g^3 + h^3\\
w_2= -9 - a^3 - b^3 - c^3 - d^3 + e^3 + f^3 + g^3 + h^3\\
w_3= 9 - a^3 - b^3 - c^3 - d^3 - e^3 - f^3 - g^3 - h^3\\
w_4= -9 + a^3 + b^3 + c^3 + d^3 - e^3 - f^3 - g^3 - h^3$$ Q: Does the cubic version have any number theoretic implications, like the set of sums of four cubes is closed under multiplication? Or is it just an interesting curiosity?","['number-theory', 'ring-theory', 'polynomials']"
2653096,upper semicontinuity of fiber dimension with target dimension 0,"I have problem with Ravi Vakil's FOAG, Theorem 11.4.2. The theorem claims that for a morphism $\pi : X \rightarrow Y$ between finite type k-schemes, the map assigning to a point $p \in X$ the dimension of the largest irreducible component of $\pi^{-1}(\pi(p))$ containing $p$ is a upper semicontinuous function. He proves this by induction on $\operatorname{dim} Y$, and says it is obvious when the dimension is 0. I couldn't figure out the reason why it is obvious. Am I overlooking something?",['algebraic-geometry']
2653102,Is there a closed totally disconnected space meeting every component?,Suppose $X$ is compact Hausdorff and has infinitely many components. It seems there should be a way to select at least one point from each component so the resulting set is totally disconnected and closed. But it seems harder than I thought. Brute force approach: Let $C$ denote the family of components of $X$. For each $c \in C$ choose a point $x(c) \in c$. Then the set $D = \{x(c): c \in C \}$ has only one point from each component. But it might not be closed. So redefine $D$ as the closure of $\{x(c): c \in C \}$. Then $D$ is closed but might not be totally disconnected. Any tips?,"['general-topology', 'compactness', 'connectedness']"
2653110,How to solve a 2D ODE system of the form $\frac{\text{d}\vec{x}}{\text{d}t}=(M-\Delta e^{-\lambda t})\vec{x}+\vec{x}_0$,"Let $x=x(t)$ and $y=y(t)$ for $t>0$ , with $x(0)=y(0)=1$ . \begin{align}
\frac{\text{d}x}{\text{d}t}
%%%
&=
%%%
-\left( A + \alpha e^{-\lambda t}\right)x + By + x_0\\
%%%
\frac{\text{d}y}{\text{d}t}
%%%
&=
%%%
Cx -\left( D + \beta e^{-\lambda t}\right)y + y_0
\end{align} where $A,B,C,D,\alpha,\beta,x_0,y_0\in \mathbb{R}_{>0}$ . We can rewrite this in the following vector format \begin{align}
\frac{\text{d}\vec{x}}{\text{d}t}
%%%
&=
%%%
\left(
M
-
\Delta e^{-\lambda t}
\right)
\vec{x}
+
\vec{x}_0
\end{align} where \begin{align}
%%%
\vec{x} = 
\left(\begin{array}{c}
x \\
y
\end{array}\right),\qquad
%%%
M = 
\left(\begin{array}{cc}
-A  &  B \\
 C  & -D
\end{array}\right),\qquad
%%%
\Delta = 
\left(\begin{array}{cc}
\alpha & 0 \\
0      & \beta
\end{array}\right),\qquad
%%%
\vec{x}_0 = 
\left(\begin{array}{c}
x_0 \\
y_0
\end{array}\right)
%%%
\end{align} where $\vec{x}(0)=(1,1)^T$ . The only issue here, that separates it from a standard non-homogeneous linear system of ODEs ( see here ), is the presence of the exponential vector term. This 2D system is a reduction of a larger ODE system and this is as simple as I can get it. There does exist a small parameter ( $\alpha\simeq\beta\sim O(1/\varepsilon)$ , where $0<\varepsilon\ll1$ and all other terms are $O(1)$ ), however I would rather consider an exact solution as asymptotic analysis on similar systems as this has proved not useful past small/intermediate time in the parameter spaces of interest. If I make any progress on this I will add it below, if this system has been solved before I would really appreciate a reference, or a reference to a more general solution. EDIT 1 Ok, so thank you all for your help. I have consulted with my colleagues, discussed their ideas and your suggestions, we believe we have proven that you can't find a solution in terms of elementary functions, even in a reduced case (which is what I need). With the aid of some biological arguments I can restrict my parameter regimes of interest to ones that allow the following expression to be surprisingly accurate: $$y = \frac{1}{1+\gamma e^{-\lambda t}}$$ where $\gamma>0$ and can be expressed in terms of the other coefficients in this model. This reduces the system to a single ODE, as follows: \begin{align}
\frac{\text{d}x}{\text{d}t}
%%%
&=
%%%
-\left( A + \alpha e^{-\lambda t}\right)x + \frac{B}{1+\gamma e^{-\lambda t}} + x_0
\end{align} with $x(0)=1$ . NOTE 1: If we define $\bar{M}$ as follows \begin{align}
\bar{M} = \left(\begin{array}{cc}
-(A-1)  &  B/\delta \\
\delta C  & -(D-1)
\end{array}\right)
\end{align} where $\delta\in\mathbb{R}_{>0}$ , then the eigenvalues of $\bar{M}$ are $-|\lambda|$ and $-|\mu|$ , where $0<\lambda\ll\mu$ . If we consider the similar system: \begin{align}
\frac{\text{d}\vec{z}}{\text{d}t}
%%%
&=
%%%
\bar{M}
\vec{z}
\end{align} where $\vec{z}=(z_1(t),z_2(t))^T$ and $\vec{z}(0)=(0,1)^T$ . Then this system has approximate solution \begin{align}
\vec{z} &\simeq
\left(\begin{array}{c}
\alpha \\
\beta
\end{array}\right)
e^{-\lambda t}
\end{align} This solution does not hold for $t\ll1$ , as can be seen from the initial conditions, however for all intents and purposes it can be treated as exact. NOTE 2: Here is a plot using suitable parameter values:","['ordinary-differential-equations', 'mathematical-modeling', 'nonlinear-system']"
2653116,Intuition of angle between two planes,"Let theta be the angle between two planes (see the image). I am told that ∠AKB=∠θ if and only if AK is perpendicular to MN and KB is also perpendicular to MN (where MN represents line where two planes intersect). My question is, why does θ will equal only when the angle is created with two perpendiculars (AK and KB) on MN (which intuitively makes sense), and also why does not ∠LKB also equal to ∠θ (which intuitively does not makes sense)?","['solid-geometry', 'geometry']"
2653120,Finding the limit of a sequence of integrals,"Let us define a sequence of function as
  $$f_n(x)=\frac{2nx^{n-1}}{x+1}\;\;\text{for each $x\in [0,1]$ and for all $n\in\mathbb{N}$}$$
  What is $\displaystyle \lim_{n\to \infty} \int_0^1 f_ n(x) dx$ ? How to find the limit? If I can interchange the limit with integral the ans is surely $0$. But can we interchange the limit with integral here. All that I know is Lebesgue's monotone convergence theorem and dominated convergence theorem that allow us to interchange limit and integrals. But this seems not to be useful here. Then how to proceed? Any help would be appreciated. Thanks in advance.","['sequence-of-function', 'real-analysis', 'limits']"
2653127,Is a function periodic $f(x) = \cos (x) +\cos(x^2)$,"This one I gave my students today, nobody solve it. Is a following function periodic $f:\mathbb{R}\to\mathbb{R}$ $$f(x) = \cos (x) +\cos(x^2)$$ If someone is interested I can show a solution later.","['algebra-precalculus', 'trigonometry']"
2653131,"Given the density function, calculate the expected value $E(X)$","Given the density function$$f(x)=\begin{cases} \frac{x^2}{2}&\mbox{if }0\leq x<1\\
-x^2+3x-\frac{3}{2}&\mbox{if }1\leq x<2\\
\frac{(3-x)^2}{2}&\mbox{if }2\leq x<3\\
0&\mbox{else}\end{cases}$$ Calculate $E(X)$ Can you please tell me if I do it correct? Because I would do it like that in the exam and I hope it's good: $$E(X)=\int_{-\infty}^{\infty}xf(x) \;dx = \int_{0}^{1} x\frac{x^2}{2}\;dx + \int_{1}^{2} x(-x^2+3x-\frac{3}{2})\;dx + \int_{2}^{3} x\frac{(3-x)^2}{2}\;dx$$ I leave further calculation away to keep it short, just this part I'm not sure if it's fine like that?","['probability-theory', 'probability', 'expectation', 'density-function']"
