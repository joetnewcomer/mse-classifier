question_id,title,body,tags
3757673,How to minimize this quadratic form?,"For real numbers $x_1,\dots, x_{2n}$ , satisfying $\sum_{i=1}^{2n}x_i = 0$ and $\sum_{i=1}^{2n}x_i^2 = 1$ , how do I show that $$ \sum_{1\le i < j\le n}(x_i-x_j)^2 + \sum_{n+1\le i < j\le 2n}(x_i-x_j)^2 + \sum_{i=1}^n\sum_{j=0}^{m-1} (x_i-x_{n+1+(i+j-1)})^2 \ge 2m$$ where the $(i+j-1)$ in the subscript is taken modulo $n$ and $m\le n$ ? A case of equality would be $x_i = \begin{cases}\frac{1}{\sqrt{2n}} & 1\le i\le n \\ -\frac{1}{\sqrt{2n}} & n+1\le i\le 2n\end{cases}$ .","['optimization', 'multivariable-calculus', 'quadratic-forms', 'inequality']"
3757690,Prove that a graph on $2n$ vertices with at least $n^2+1$ edges has at least $n$ triangles.,"Let $n>1$ be an integer.  A graph $G$ consists of $2n$ vertices and at least $n^2+1$ edges.   Show that there exists at least $n$ triangles. I already have a proof which is induction on $n$ . But is there any direct method to do it? UPDATE:
Sketched inductive proof:
Consider $n=k+1$ case. It can be shown there exists triangle $ABC$ . Suppose in the rest of $2k-1$ points there are $P_A, P_B,P_C$ edges connecting to $A,B,C$ respectively. Case 1: $P_A+P_B+P_C \geq 3k-1$ . It can be shown there are at least $k$ triangles taking one of $AB,BC,CA$ as edge, plus $ABC$ , we get $k+1$ triangles. Case 2: $P_A+P_B+P_C \leq 3k-2$ . Then one of $P_A+P_B, P_B+P_C,P_C+P_A$ must $\leq 2k-2$ . Suppose $P_A+P_B \leq 2k-2$ . One can show the other $2k-1$ points and C form a graph and have at least $(k+1)^2+1-(2k-2)-3=k^2+1$ edges ( $3$ is from triangle $ABC$ ). By induction we have $k$ triangles, plus $ABC$ , and we get $k+1$ .","['graph-theory', 'combinatorics', 'discrete-mathematics']"
3757701,Integrate $\Omega=\int_{-\infty}^{\infty}\frac{\operatorname{arccot}(x)}{x^4+x^2+1}dx$,"A friend of mine got me the problem proposed by Vasile Mircea Popa from Romania, which was published in the Romanian mathematical Magazine . The problem is to find: $$\Omega=\int_{-\infty}^{\infty}\frac{\operatorname{arccot}(x)}{x^4+x^2+1}dx$$ As per the Wolfram alpha the evaluated value is found to be $0$ . The reason is since $\operatorname{arccot}(x)=-\operatorname{arccot}(-x)$ for all $x\in\mathbb C^+$ is an odd function . However, the next answer obtained is $\frac{\pi^2}{ 2\sqrt{3}}$ where  the relations $\text{arccot}(x)=\frac{\pi}{2}-\operatorname{arctan}(x)\cdots(1)$ is used keeping in the view of principal branch of $\operatorname{arccot}(x)$ . The works is as follows: $$\Omega=\int_{-\infty}^{\infty}\frac{\frac{\pi}{2}-\operatorname{arctan}(x)}{x^4+x^2+1}dx=\frac{\pi}{2}\int_{-\infty}^{\infty}\frac{dx}{x^4+x^2+1}-\underbrace{\int_{-\infty}^{\infty}\frac{\operatorname{arctan}(x)}{x^4+x^2+1}}_{\text{odd function}}dx\\\overbrace{=}^{xy=1}\frac{\pi}{2}\int_{-\infty}^{\infty}\frac{x^2 dx}{x^4+x^2+1}=\frac{\pi}{2}\int_{-\infty}^{\infty}\frac{dx}{\left(x-\frac{1}{x}\right)^{2}+3}$$ then, by Cauchy SchlÃ¶milch transformation (Special case of Glasser's Masters  theorem) we obtain $$\Omega= \frac{\pi}{2}\int_{-\infty}^{\infty}\frac{dx}{x^2+3}=\frac{\pi^2}{2\sqrt{3}}$$ Note that former integral can be solved without using aforementioned theorem, by the partial fraction of $x^4+x^2+1=(x^2+x+1)(x^2-x+1)$ . My question is, Which of the above work is correct? In my view the first work is correct. In  the second working, Is the use of Maclaurin series done correctly ?","['integration', 'definite-integrals', 'inverse-function', 'real-analysis', 'trigonometry']"
3757751,"$AD$ has exactly one negative eigenvalue if $v^T A v > 0$ and $D = \mbox{diag}(-1,1,1)$","Let $A$ be $3 \times 3$ real matrix (which is not necessarily symmetric or diagonalizable) such that $v^T A v>0$ for every $v \in \mathbb R^3 - \{0\}$ . Show that $AD$ has exactly one negative eigenvalue, where $D = \mbox{diag}(-1,1,1)$ . I can prove that $AD$ has a negative eigenvalue. If $\det(A) \leq 0$ , then characteristic polynomial $f(t) = \det(tI-A)$ satisfies $f(0) \geq 0$ . Since $f$ is polynomial of degree $3$ and $$\lim_{t \to -\infty} f(t) = -\infty$$ we can find a eigenvalue $\lambda \leq 0$ of $A$ with eigenvector $v$ . Then $v^TAv=\lambda v^Tv \leq 0$ , contradiction. Therefore $\det(AD) = \det(A) \det(D)<0$ . let $g(t)$ be characteristic polynomial of $AD$ . Then $g(0) = - \det(AD)>0$ so same argument produce a result. However, I cannot solve uniqueness part. How to solve it?","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
3757754,Find the greatest integer less than $3^\sqrt{3}$ without using a calculator and prove the answer is correct.,"Find the greatest integer less than $3^\sqrt{3}$ without using a calculator and prove the answer is correct. I'm puzzled on how to solve this problem, any help is appreciated. There was hints about turning the exponents into fractions and picking fractions between : $3^x < 3^\sqrt3 <3^y$ Then I simplified: $x< \sqrt3<y$ $x^2< 3<y^2$ $\sqrt2^2<3<\sqrt4^2$ So $x=\sqrt2$ and $y=\sqrt4=2$ $3^\sqrt2 < 3^\sqrt3 <3^2$","['algebra-precalculus', 'inequality']"
3757773,Is there a simple proof that a non-invertible matrix reduces to give a zero row?,"Let $A$ be a square matrix that is non-invertible. I was wondering if there is a simple proof that we can apply elementary row operations to get a zero row. (For a matrix $C$ to be invertible, I mean there is $B$ such that $CB = BC = I$ .) I can prove this using elementary column operations but I would like a more direct proof that doesnâ€™t appeal to column operations or the fact that row rank equals column rank, or anything to do with transposes, or the existence of RREF, or determinants, etc. The difficulty seems to be that elementary row operations are applied on the row space, whereas invertibility is sort of defined in terms of the column space. You could also use (but it is preferred not to) facts like: A matrix $C$ being invertible is equivalent to null space of $C$ being zero (i.e. injective) is equivalent to $C$ being surjective.","['matrices', 'gaussian-elimination', 'linear-algebra']"
3757779,Significance of Codomain of a Function,"We know that Range of a function is a set off all values a function will output. While Codomain is defined as ""a set that includes all the possible values of a given function."" By knowing the the range we can gain some insights about the graph and shape of the functions. For example consider $$f(x)=e^x$$ By knowing that the range of the function is $(0,\infty)$ ,we can conclude that the graph lies above the $X-axis$ . My Questions Does knowing codomain of a function give any insight/information about the function? Every function has a specific range and it is universal? Is it true for codomain also? What I am trying to say is that range of $\sin x$ is $[-1,1]$ .While as per my understanding codomain is $\mathbb R$ (Real Numbers). But defining codomain of $\sin x$ as say $(-2,2)$ is not going to change anything. So $(-2,2)$ is a valid codomain for $\sin x$ . Am I right? What compelled mathematicians to define codomain, why were they not happy with the concept of range only?","['notation', 'real-numbers', 'definition', 'functions']"
3757811,Find $\lim_{x\to 0^{+}} \frac{\theta(x)}{x}$,"Suppose $f\in C^1([0,1])$ and $f'(0)\neq 0$ . For $x\in(0,1]$ , let $\theta(x)$ be such that $$\int_0^x f(t)dt = f(\theta(x))x$$ Find $$\lim_{x\to 0^{+}} \frac{\theta(x)}{x}$$ I thinking about Taylor series expansion of $$F(x)=\int_0^x f(x)dx$$ but the point is the remainder term. I can't figure out which remainder term should I use. And what is the point of taking only $x\to 0^+$ . Any kind of help is appreciable.","['definite-integrals', 'real-analysis', 'calculus', 'taylor-expansion', 'limits']"
3757844,Proof verification: polynomials $\mathbb R[X]$ are a vector space that is not isomorphic to its dual,"I have never seen an elementary proof of the fact that $V$ need not be isomorphic to $V^*$ that does not require some set theoretic background. I came up with this [most likely incorrect] argument, which does not seem to depend so much on set theoretic arguments, except for basic ideas of cardinality. I wish to have this particular proof vetted. Consider the vector space of polynomials $V \equiv \mathbb R[X]$ as an $\mathbb R$ vector space.
The set $B_V \equiv \{x^i : i \in \mathbb N \}$ is a basis for the vector space $V$ . Given any polynomial $p(x) \in \mathbb R[X]$ , since the polynomial $p$ only has finitely many non-zero coefficients. Thus $p(x)$ must be of the form $p(x) = \sum_{i \in \text{nonzero-powers}(p)} a_i x^i$ where the index set $\text{nonzero-powers}(p)$ has finite cardinality. Hence we can write any polynomial $p(x)$ as a finite linear combination of elements from the set $B_V$ . Next, consider the dual space $V^* \equiv \{ f : \mathbb R[X] \rightarrow \mathbb R \mid f \text{ is a linear function}  \}$ . We have the elements $eval_r$ which evaluate a polynomial at point $r \in \mathbb R$ as elements of $V^*$ .
More formally, $eval_r(p) \equiv p(r); \forall r \in \mathbb R, eval_r \in V^*$ . All the $eval_r$ are linearly independent. Intuitively, this is because we cannot pin down the value of all polynomials by evaluating them at some finite number of points. More formally, suppose that we have that $\sum_{i \in I} a_i eval_{r_i} = 0$ for some finite index set $I$ . So this gives us a way to extrapolate $eval_{i_0}$ from the other $eval_i$ . However, this is absurd, since the value of a polynomial of degree $2|I|$ is not determined by its value at $|I|$ points.  Hence all the $eval_r$ are linearly independent. This means that we have a linearly independent set $L_{V^*} \equiv \{ eval_r : r \in \mathbb R \}$ whose cardinality is that of $|\mathbb R|$ . Wrapping up, we have that the basis of $V$ , $B_V$ has cardinality $|\mathbb N|$ . A linearly independent set of $V^*$ , whose cardinality is a lower bound on the cardinality of $V^*$ , has cardinality $|\mathbb R|$ . Hence the vector spaces cannot be isomorphic since the cardinality of their bases are different. Is this correct?","['solution-verification', 'linear-algebra', 'vector-spaces']"
3757864,Finding a distance between a point in a circle from the center.,"Given a diagram like this, Where $O$ is the center and $OA = \sqrt{50}$ , $AB = 6$ , and $BC = 2$ . The question was to find the length of $OB$ . $\angle ABC = 90^o$ What I've done is so far: I made the triangle $ABC$ and named $\angle BAC = \alpha$ . By trigonometry, I have the values for $\sin{\alpha}$ and $\cos{\alpha}$ . I get $\cos{\alpha}=\frac{6}{\sqrt{40}}$ . Then I made the triangle $OCA$ and named $\angle OAB = \beta$ so $\angle OAC = \alpha + \beta$ . By using the cosinus rule, I have $\cos(\alpha + \beta) = \frac{1}{\sqrt{5}}$ . Using the formula, $\cos(\alpha + \beta) = \cos{\alpha}.\cos{\beta} - \sin{\alpha}.\sin{\beta}$ and making $\sin{\beta} = \sqrt{1 -\cos^2{\beta}}$ I finally get that $\cos{\angle OAB} = \frac{1}{\sqrt{2}}$ . Finally, by using the cosinus rule on the triangle $AOB$ I get $OB = \sqrt{26}$ . My only problem is this takes me way too long!  I am interested in a quicker way to do this (i.e. I now know that $\angle OAB = 45^o$ from trigonometry, but is there a quicker way to recognize it?)","['trigonometry', 'circles', 'geometry']"
3757875,LHS where the argument of the function isn't explicit stated (vector equation),"The Lorentz force is given as $$
\mathbf  F= q\left[\mathbf E(\mathbf r(t),t)+\mathbf v(t)\times \mathbf B(\mathbf r(t),t)\right] \tag 1
$$ where $\mathbf E, \mathbf B:\mathbb R^4\to\mathbb R^3$ are vector fields and $\mathbf r, \mathbf v:\mathbb R\to \mathbb R^3$ are vector-valued functions of one variable, $t\in \mathbb R$ . And $q$ is a constant. Question: In books the argument of $\mathbf F$ is not explicit given, but why? Does it mean it is a constant vector $\mathbf F\in \mathbb R^3$ , i.e. $$
\mathbf F=(F_x,F_y,F_z) \quad ? \tag 2
$$ Or, based on the right hand side, is $\mathbf F$ a vector-valued function, $\mathbf F: \mathbb R\to \mathbb R^3$ , i.e. $$
\mathbf F(t)=\big(F_x(t),F_y(t),F_z(t) \big) \quad ? \tag 3
$$ Or is it maybe a vector field, $\mathbf F: \mathbb R^4\to \mathbb R^3$ , i.e. $$
\mathbf F(\mathbf r(t),t)=\big(F_x(r(t),t)),F_y(r(t),t)),F_z(r(t),t)) \big ) \quad ? \tag 4
$$","['notation', 'multivariable-calculus', 'functions', 'vector-analysis', 'mathematical-physics']"
3757892,Confusion about proving logical implication statements,"I've got four statements* which I'm meant to evaluate as being either true or false. a. If 25 is a multiple of 5, then 30 is divisible by 10. b. If 25 is a multiple of 4, then 30 is divisible by 10. c. If 25 is a multiple of 5, then 30 is divisible by 7. d. If 25 is a multiple of 4, then 30 is divisible by 7. While I'm inclined to just say they're all false, since I can't find any way that the then statements follow from the if statements, I can't help but feel as if I'm missing something, in that we're meant to assume the if statements are correct. The best I can guess is that by accepting the conditionals, the number system must be warped in some way through which the following then statements are analyzed. Anyone able to provide any insight?","['logic', 'discrete-mathematics']"
3757911,Three definitions of 'singleton set'?,"I discovered that there are three definitions of 'singleton set', and that these are at different levels of the set hierarchy. A singleton set... (element level) ...has exactly one element; (set level) ...has exactly one strict subset (viz. the empty set); (family level) ...is an element of every family that covers it. (Here ""F covers A"" means ""F 's union equals A"".  Perhaps this is not official terminology.) My quite vague question: It seems there might be a bigger story behind these different ways of defining this same concept?","['elementary-set-theory', 'definition']"
3757916,"$G$-modules, Group invariants and Tor functor","Let $M$ be a $G$ -module. Then functoriality induces a natural $G$ -module structure on $\text{Tor}_i(M,N)$ , where $N$ is any abelian group. My question is, what can we say about $$\text{Tor}_i(M,N)^G.$$ Is there some way to discribe this using $M^G$ ? For example, if we assume that $M=\mathbb{Z}/n$ , then $$\text{Tor}_1(\mathbb{Z}/n,N)^G=\left(N[n]\right)^G=\left(N^G\right)[n]=N[n]=\text{Tor}_1(\mathbb{Z}/n,N).$$ I'm most interested in ""nice"" criteria for the vanishing of $\text{Tor}_i(M,N)^G.$","['abstract-algebra', 'group-cohomology', 'derived-functors']"
3757936,Pretty conjecture $x^{\left(\frac{y}{x}\right)^n}+y^{\left(\frac{x}{y}\right)^n}\leq 1$,"inspired (again) by an inequality of Vasile Cirtoaje I propose my own conjecture : Let $x,y>0$ such that $x+y=1$ and $n\geq 1$ a natural number then we have : $$x^{\left(\frac{y}{x}\right)^n}+y^{\left(\frac{x}{y}\right)^n}\leq 1$$ First I find it very nice because all the coefficient are $1$ . I have tested with Geogebra until $n=50$ without any counter-examples. Furthermore we have an equality case as $x=y=0.5$ or $x=1$ and $y=0$ and vice versa . To solve it I have tried all the ideas here My main idea was to make a link with this inequality (my inspiration) see here So if you can help me to solve it or give me an approach... ...Thanks for all your contributions ! Little update I think there is also an invariance as in question here Conjecture $a^{(\frac{a}{b})^p}+b^{(\frac{b}{a})^p}+c\geq 1$ Theoretical method Well,Well this method is very simple but the result is a little bit crazy (for me (and you ?)) Well ,I know that if we put $n=2$ we can find (using parabola) an upper bound like $$x^{\left(\frac{1-x}{x}\right)^2}\leq ax^2+bx+c=p(x)$$ And $$(1-x)^{\left(\frac{x}{1-x}\right)^2}\leq ux^2+vx+w=q(x)$$ on $[\alpha,\frac{1}{2}]$ with $\alpha>0$ and such that $p(x)+q(x)<1$ In the neightborhood of $0$ we can use a cubic . Well,now we have (summing) : $$x^{\left(\frac{1-x}{x}\right)^2}+(1-x)^{\left(\frac{x}{1-x}\right)^2}\leq p(x)+q(x)$$ We add a variable $\varepsilon$ such that $(p(x)+\varepsilon)+q(x)=1$ Now we want an inequality of the kind ( $k\geq 2$ ): $$x^{\left(\frac{1-x}{x}\right)^{2k}}+(1-x)^{\left(\frac{x}{1-x}\right)^{2k}}\leq (p(x)+\varepsilon)^{\left(\frac{1-x}{x}\right)^{2k-2}}+q(x)^{\left(\frac{x}{1-x}\right)^{2k-2}}$$ Now and it's a crucial idea we want something like : $$\left(\frac{x}{1-x}\right)^{2k-2}\geq \left(\frac{1-(p(x)+\varepsilon)}{q(x)}\right)^y$$ AND : $$\left(\frac{1-x}{x}\right)^{2k-2}\geq \left(\frac{1-q(x)}{p(x)+\varepsilon}\right)^y$$ Now it's not hard to find a such $y$ using logarithm . We get someting like : $$x^{\left(\frac{1-x}{x}\right)^{2k}}+(1-x)^{\left(\frac{x}{1-x}\right)^{2k}}\leq q(x)^{\left(\frac{1-q(x)}{q(x)}\right)^{y}}+(1-q(x))^{\left(\frac{q(x)}{1-q(x)}\right)^{y}}$$ Furthermore the successive iterations of this method conducts to $1$ because the values of the differents polynomials (wich are an approximation of the initial curve) tend to zero or one (as abscissa). The extra-thing (and a little bit crazy) we can make an order on all the values. My second question Is it unusable as theoretical\practical method ?","['exponentiation', 'conjectures', 'examples-counterexamples', 'inequality', 'derivatives']"
3757943,Differential of gradient on submanifold of Euclidean space,"I'm currently selfstudying (and new to) differential geometry and I came across something, that I can't make sense of at the moment. Let $M$ be a submanifold of $\mathbb{R}^{n}$ and let $u,f:M\longrightarrow\mathbb{R}$ be a function. The gradient $\nabla^{M}$ of $f$ is defined by the property, that $g_{p}(\nabla^{M}f,X)=d_{p}f(X)$ , where $p\in M$ , $X\in TM$ satisfying $X(p)\in T_{p}M$ and $d_{p}f:T_{p}M\longrightarrow T_{f(p)}\mathbb{R}=\mathbb{R}$ . Since the defining property holds for all vector fields $X$ , we get that $\nabla^{M}f(p)=g^{ij}d_{p}f$ , where $g^{ij}$ is inverse matrix of the matrix representing the first fundamental form.
Now, let $u(x)=\nabla^{M}f(x)$ . My questions are: What is the differential $d_{p}u$ of $u$ ? Is there, consequently, a meaningful object for something like the ""second derivative"" aka. the Hessian of $f$ ?
Thanks in advance.","['submanifold', 'riemannian-geometry', 'differential-geometry']"
3757944,How do you move between an observer's frame orbiting a Kerr B-H and the B-H's frame? In context of photon emission from accretion disk.,"I'm struggling with the concept of moving to and from reference frames in GR. I'm doing a problem in Spacetime and Geometry by S. Carroll (Chapter 6, Question 6 - the text for the question is very long) and it's the idea of iron in an accretion disk on geodesics emitting photons of known frequency $$\nu_{0}$$ and then, depending on where the photon is emitted and the direction of rotation of the accretion disk compared to the Kerr B-H, what is the frequency of the photon measured by an observer very far away. One method I tried was using, from earlier in the book, $$\omega = -g_{\mu \nu}U^{\mu}\frac{dx^{\nu}}{d\lambda}$$ then if you assume the observer is stationary, and in the equatorial plane (as given in the question) you get that $$\omega = (1 - \frac{2GM}{r})^{\frac{-1}{2}}E$$ from the Killing vector and the normalisation of U in the Kerr metric. Then I think that the energy will be conserved along the Killing vector so just take $$E = \nu_{0}$$ if hbar = 1. So you end up with the formula $$\frac{\omega}{\nu_{0}} = (1 - \frac{GM}{r})^{\frac{-1}{2}}$$ which is just the Schwarzchild redshift formula right? So clearly this doesn't depend on anything to do with the accretion disk, or even the 'angular momentum' of the Kerr B-H, and I definitely feel like I'm doing something wrong. Furthermore, I have rearranged the Killing vectors for L and E then used them, and the fact the photon is on a null path in the equatorial plane, to get the full form for the dx/d(lambda) in terms of conserved quantities. I want to try doing the idea of moving between frames of a particle in the accretion disk but I'm really struggling with deriving the transformation between the particle's rest frame and then Black-Hole's frame. I'm thinking along the lines of the preservation of the line element and saying that, in a very small vicinity around the particle, spacetime looks like Minkowski spacetime so, using again that we're in the equatorial plane, $$ds^2 = -dt'^2 + dr'^2 + r^2d{\phi'}^2 = -(1 - \frac{2GM}{r})dt^2 + \frac{2GMa}{r}(dtd{\phi} + d{\phi}dt) + \frac{r^2}{\Delta}dr^2 + \frac{\Sigma^2}{r^2}d{\phi}^2$$ where $$\Sigma^2 = [(r^2 + a^2)^2 - a^2{\Delta} ] || \Delta = r^2 - 2GMr + a^2$$ then if you leave all the other components constant, you get the effects of, for example, $$\frac{dr'}{dr} = \frac{r}{\sqrt{\Delta}} $$ I'm really not sure if this is correct. I've also had some ideas like $$U^{\mu'} = \frac{\partial{x^{\mu'}}}{\partial{x^{\mu}}}U^{\mu}$$ where the particle frame is un-primed and the Black-Hole frame is primed, but still I'm struggling to find (convincingly) the partial derivatives. Please can someone give me a push in the right direction? I'm self studying (lockdown and all that) and I feel I'm missing some little conceptual, well basic, ideas like this. I don't know if this is against Stack Exchange policy, but please can you not give a full answer to Carroll's question, I would still like to try and fill in the gaps!","['physics', 'general-relativity', 'differential-geometry']"
3757965,Can the eigenvalues of this block circulant matrix be found?,"I have a matrix of the form $$ M = \begin{pmatrix} A & A^T & & & I\\ I & A & A^T & & \\ &  I & A  & \ddots &\\ & & \ddots & \ddots & A^T\\ A^T & & & I & A \end{pmatrix}$$ where $I$ is an $n \times n$ identity matrix and $A$ is an $n \times n$ -matrix given by $$ A = \begin{pmatrix} 0 & 1 & 0 & \dots & 0\\ \vdots & \ddots& \ddots & \ddots & \vdots\\ 0 & \dots & 0 & 1 & 0\\ 0 & \dots &  & 0 & 1\\ 0 & \dots & & & 0 \end{pmatrix}$$ that is a matrix which has ones on the super diagonal and zeros everywhere else. Is there some way to find the eigenvalues of this matrix? If there is, can it be generalized to a more complicated $A$ ? Since $A$ and $A^T$ don't commute, one cannot diagonalize them simultaneously (also, they are not even diagonalizable), otherwise that would have been a straightforward way to do it. I have tried computing the characteristic polynomial, but I cannot seem to find a way to simplify the determinant.","['eigenvalues-eigenvectors', 'matrices', 'linear-algebra', 'circulant-matrices', 'block-matrices']"
3757972,Why is the answer of $\frac{ab}{a+b}$ always smaller than the smallest number substituted?,"If $\frac{ab} {a+b} = y$ , where $a$ and $b$ are greater than zero, why is $y$ always smaller than the smallest number substituted? Say $a=2$ , $b=4$ (smallest number here is $2$ . Thus, the answer would be smaller than $2$ ) $\frac{2\cdot4}{ 2+4} = 1.\bar 3$ I got this equation from physics. It's for getting total resistance and the miss told us to not waste time in mcq on it because the answer will always be smaller than the smallest number. But I can't explain to myself in words or by intuition why this happens. Any help??","['algebra-precalculus', 'inequality']"
3758028,Help to define an equivalence class,"Consider the array $\{x_{i,n}:1\leq i\leq n, n\in\mathbb{N}\}$ and the set of consecutive indices $A_n$ whose cardinality is $d_n$ , where $d_n<n$ is an increasing positive sequence (slower than $n$ ). Say that $A_n=\{c_n,\dotsc,c_n+d_n-1\}$ with $c_n$ being a positive sequence such that $c_n+d_n-1\leq n$ . Then, for a given $n$ , all of the following $n$ -tuples $$(0,0,c_n,\dotsc,c_n+d_n-1,0,0),(0,0,0,c_n,\dotsc,c_n+d_n-1,0),(0,0,0,0,c_n,\dotsc,c_n+d_n-1,(c_n,\dotsc,c_n+d_n-1,0,0,0,0)$$ are associated with same sum $\sum_{i\in A_n}x_{i,n}$ . So I think of this situation as if there were an equivalence class $S_n$ to represent all these n-tuples. How could I define such equivalence relation of ""interchanging zeros"" n-tuples . With the sets $S_n$ stablished, we see that the sub-array $\{x_{i,n}:i\in A_n, n\in\mathbb{N}\}$ and any other $\{x_{i,n}:i\in I_n, n\in\mathbb{N}\}$ , give the same sum $$\sum_{i\in A_n}x_{i,n}=\sum_{i\in I_n}x_{i,n}, \forall I_n\in S_n$$ in spite of they are unequal arrays. I appreciate any help to elaborate this device, properly.","['elementary-set-theory', 'equivalence-relations']"
3758045,Prove that $13\sqrt{2}$ is irrational.,"I am currently a beginner at proofs and I am having trouble proving this problem... I know that the square root of $2$ is irrational because the square root of $2$ can be expressed as $\frac{p}{q}$ and once both sides are squared it is true that both $p$ and $q$ are even which is a contradiction to the assumption that they have no common factors. I am having trouble proving that $13$ and the square root of $2$ is irrational though and any help would be greatly appreciated! Since we are not dealing with the square root of $13$ , I do not know how to start since we can not set it equal to $\frac{p}{q}$ . Thank you in advance!","['irrational-numbers', 'proof-writing', 'solution-verification', 'discrete-mathematics']"
3758050,Solving $\int \frac{x^3}{(4x^2 + 9)^\frac{3}{2}} dx$,"The integral I must solve is this: $$\begin{equation} \int \frac{x^3 }{(4x^2 + 9)^\frac{3}{2}} dx \tag{1}\end{equation}$$ Now I know that I'm supposed to convert the denominator into some form of $\sqrt{a^2 + x^2}$ in order to apply the substitution $x = a \ tan \ \theta$ . Now I can change the form of the denominator to become easy enough to substitute as follows: $$\bigg (4 \bigg (\ \frac{9}{4}\ +x^2\bigg)\bigg) ^\frac{3}{2} $$ Which makes it clear that $x$ needs to be substituted as $x = \frac{3}{2} \tan \theta $ , and $dx = \frac{3}{2} \sec^2 \theta $ for later use. At this point I can represent $(1)$ in terms of my substituted trignometric function. The only problem comes with the denominator where I get stuck on the power. Here is how I went about solving it: $$\bigg (4 \bigg (\ \frac{9}{4}\ +\left(\frac{3}{2} \tan \theta\right)^2 \bigg)\bigg) ^\frac{3}{2} $$ $$\bigg (4 \bigg (\ \frac{9}{4}\ +\frac{9}{4} \tan^2 \theta) \bigg)\bigg) ^\frac{3}{2} $$ $$\bigg (4 \frac{9}{4}\bigg (\ 1 + \tan^2 \theta) \bigg)\bigg) ^\frac{3}{2} $$ $$ 9^\frac{3}{2}\bigg( \ 1 + \tan^2 \theta \bigg) ^\frac{3}{2} $$ $$ 27\ ( \sec^2 \theta ) ^\frac{3}{2} $$ Now I have no idea how to evaluate this power of $sec$ . The author says that it changes into $sec^3 \theta$ but I just can't fathom how that would go about. If what I understand is correct, the power it is raised to would be added to it's own making it $ \sec^\frac{7}{2} \theta$ . My question is that how exactly is my reasoning wrong here?","['integration', 'indefinite-integrals', 'calculus', 'trigonometry']"
3758097,Are any interesting classes of polynomial sequences besides Sheffer sequences groups under umbral composition?,"Let us understand the term polynomial sequence to mean a sequence $(p_n(x))_{n=0}^\infty$ in which the degree of $p_n(x)$ is $n.$ The umbral composition $((p_n\circ q)(x))_{n=0}^\infty$ (not $((p_n\circ q_n)(x))_{n=0}^\infty$ ) of two polynomial sequences $(p_n(x))_{n=0}^\infty$ and $(q_n(x))_{n=0}^\infty,$ where for every $n$ we have $p_n(x) = \sum_{k=0}^n p_{nk} x^k,$ is given by $$
(p_n\circ q)(x) = \sum_{k=0}^n p_{nk} q_k(x).
$$ An Appell sequence is a polynomial sequence $(p_n(x))_{n=0}^\infty$ for which $p\,'_n(x) = np_{n-1}(x)$ for $n\ge1.$ A sequence of binomial type is a polynomial sequence $(p_n(x))_{n=0}^\infty$ for which $$ p_n(x+y) = \sum_{k=0}^n \binom n k p_k(x) p_{n-k}(y) $$ for $n\ge0.$ A Sheffer sequence is a polynomial sequence $(p_n(x))_{n=0}^\infty$ for which the linear operator from polynomials to polynomials that is characterized by $p_n(x) \mapsto np_{n-1}(x)$ is shift-equivariant. A shift is a mapping from polynomials to polynomials that has the form $p(x) \mapsto p(x+c),$ where every term gets expanded via the binomial theorem. At least since around 1970, it has been known that Every Appell sequence and every sequence of binomial type is a Sheffer sequence. The set of Sheffer sequences is a group under umbral composition. The set of Appell sequences is an abelian group under umbral composition. The set of sequences of binomial type is a non-abelian group under umbral composition. The group of Sheffer sequences is a semi-direct product of those other two groups. For every sequence $a_0, a_1, a_2, \ldots$ of scalars there is a unique Appel sequence $(p_n(x))_{n=0}^\infty$ for which $p_n(0) = a_n$ for $n\ge0.$ For every sequence $c_1, c_2, c_3, \ldots$ of scalars there is a unique sequence $(p_n(x))_{n=0}^\infty$ of binomial type for which $p\,'_n(0) = c_n$ for $n\ge1.$ This can be proved by induction on $n.$ (And in every case $p_0(0)=1$ and $p_n(0)=0$ for $n\ge 1.$ ) So my question is whether Sheffer sequences exhaust the list of interesting classes of polynomial sequences that are groups under this operation? Are there any others of interest?","['group-theory', 'umbral-calculus', 'polynomials']"
3758110,Showing that a given covering is not normal,"This is a question regarding exercise 5.6 of Forster's Lectures on Riemann surfaces. We have $X=\mathbb{C}\setminus\{0,1\}$ , $Y=\mathbb{C}\setminus\{0,\pm i,\pm i\sqrt{2}\}$ , $p\colon Y\to X$ given by $p(z)=(z^2+1)^2$ . It is easy to see that this defines an (unbranched) 4-sheeted covering map, and that $\varphi\colon z\mapsto -z$ is a deck transformation. (1) I want to proof that apart from $\varphi$ and the identity, there are no other deck transformations for $p$ . One way to see this is as follows: We can extend $p$ to a branched holomorphic covering map $\overline{p}\colon \hat{\mathbb{C}}=\mathbb{C}\cup\{\infty\}\to\hat{\mathbb{C}}$ , since $p$ is a meromorphic function. Then one can show that any deck transformation of $p$ extends to a deck transformation of $\overline{p}$ by Riemann's theorem on removable singularities. Now one notes that a deck transformation of a branched covering map must preserve the ramification index to see that our list of deck transformations was exhaustive. My question is:
Is there a way to prove statement (1) without going to the extension of $p$ to an unbranched covering map?**","['complex-analysis', 'riemann-surfaces', 'covering-spaces']"
3758133,Evaluate $\lim_{x \to 0} \frac{\sqrt{1 + x\sin x} - \sqrt{\cos x}}{x\tan x}$,"What I attempted thus far: Multiply by conjugate $$\lim_{x \to 0} \frac{\sqrt{1 + x\sin x} - \sqrt{\cos x}}{x\tan x} \cdot \frac{\sqrt{1 + x\sin x} + \sqrt{\cos x}}{\sqrt{1 + x\sin x} + \sqrt{\cos x}} = \lim_{x \to 0} \frac{1 + x\sin x - \cos x}{x\tan x \cdot(\sqrt{1 + x\sin x} + \sqrt{\cos x})}$$ From here I canâ€™t see any useful direction to go in, if I even went in an useful direction in the first place, I have no idea.",['limits']
3758148,A tough definite integral using contour integration,"For some reason, I am guessing that for any fixed $s_1,s_2>0$ and $\varepsilon >0$ being small, we have \begin{align}&\quad-\int_0^\infty \frac{1}{2\pi} \log\left(\frac{(x-s_1)^2+s^2_2}{(x+s_1)^2+s^2_2}\right)\frac{4x\sin\varepsilon}{x^4-2x^2\cos\varepsilon +1}\,dx\\&= \log\left(\frac{1+s^2_1+s^2_2+2s_2\sin(\frac{\varepsilon}{2})+2s_1\cos(\frac{\varepsilon}{2})}{1+s^2_1+s^2_2+2s_2\sin(\frac{\varepsilon}{2})-2s_1\cos(\frac{\varepsilon}{2})}\right),
\end{align} and I believe this can be shown by some clever contour integration. However, I really didn't figure out the contour that should be used to evaluate the integral... Any help or suggestions would be greatly appreciated!","['integration', 'complex-analysis', 'definite-integrals']"
3758149,"Expectation of $\frac{\langle Gx, Gy\rangle}{\|Gx\|_2\|G y\|_2}$ where $\langle x, y\rangle = \rho$","Let $x$ and $y$ by unitvectors $\in\mathbb R^d$ , such that $\|x\|_2=\|y\|_2=1$ and $\langle x,y\rangle=\rho\in[-1,1]$ . Let $G\in\mathbb R^{k\times d}$ be a matrix with independent Gaussian entries. I'm interested in the mean and variance of $$\rho'=\frac{\langle Gx, Gy\rangle}{\|Gx\|_2\|G y\|_2}.$$ Another way to state this is that $Gx,Gy\in\mathbb R^k$ are Gaussian vectors with covariance matrix $\big(\begin{smallmatrix}1&\rho\\\rho&1\end{smallmatrix}\big)$ .
Thus $E[\langle Gx, Gy\rangle]=k \rho$ and $E[\|Gx\|_2^2]=E[\|Gy\|_2^2]=k$ .
From this we would expect $E[\rho']\approx\rho$ . I can make this more precise using the Gaussian Johnson Lindenstrauss lemma, which says that if $k=\varepsilon^{-2}\log1/\delta$ , then $\langle Gx, Gy\rangle=\rho(1\pm\varepsilon)\|x\|_2\|y\|_2$ with probability at least $1-\delta$ .
We can get similar bounds for $\|Gx\|_2^2=\|x\|_2^2(1\pm\varepsilon)$ and for $y$ . By Cauchy Schwartz we always have $\rho'\in[-1,1]$ , so we get roughly $$
E[\rho'] = \rho\,(1\pm O(1/\sqrt k)).
$$ However, if I series expand $\rho'$ at $\rho=0$ I get $$
\begin{align}
E[\rho']
&= E\left[\frac{g}{\sqrt{\chi^2_{k-1} + g^2}} + \frac{\chi_{k} \chi^2_{k-1} }{(\chi^2_{k-1} +g^2)^{3/2}}\rho
+O(\rho^2)\right]
\\&= \frac{\Gamma(k/2+1/2)^2}{\Gamma(k/2+1)\Gamma(k/2)}\rho
+O(\rho^2)
\\&= \rho(1 + 1/(2k) + O(1/k^2)) +O(\rho^2),
\end{align}
$$ where $\chi_k^2$ and $\chi_{k-1}^2$ are independent Chi-Square distributed random variables, and $g$ is a standard Gaussian. At the same time it seems $f(\rho)=E[\rho']$ for $\rho\ge 0$ is a convex function, that $f(\rho)\le\rho$ , and that $f(1)=1$ .
Hence we must have $f(\rho)\ge \rho(1-1/(2k))$ and $$E[\rho'] = \rho(1+O(1/k)).$$ This is a quadratic improvement over the previous bound.
I would love to have an intuition and a  more rigorous proof.
I also wonder what the variance of $\rho'$ is.","['random-matrices', 'linear-algebra', 'geometry', 'probability']"
3758152,Limit of conditional expectation,"I am working with conditional expectations and am trying to derive a limit property. Consider $(ð‘Œ_ð‘›)_{ð‘›\in\mathbb N}$ a sequence of square integrable random variables, that converge in $L^2$ to a square integrable random variable $Y$ . Additionally assume that $\mathbb E[Y_n\mid Y]=Y_n$ (for example, $Y_n$ is a sequence of discrete quantizers of $Y$ ). Is there anyway at all of guaranteeing that for some other $X$ in $L^2$ , and for some form of convergence ( $L^2, \mathbb P$ etc.) : $$\lim_{n\to+\infty}\mathbb E[X\mid Y_n]=\mathbb E[X\mid Y].$$ I am aware of the following similar question : Conditional expectation of asymptotically independent random variables but in that case $\mathbb E[Y_n\mid Y]=Y_n$ does not hold... With a $L^2$ -projection approach to conditional expectation, and with $Y_n$ converging in $L^2$ to $Y$ , I keep thinking there must be some way of getting this to work... But maybe it just won't. Thank you for any suggestions!","['conditional-expectation', 'convergence-divergence', 'probability-theory']"
3758153,"Evaluating the recurrence $f_k(x)= f'_{k-1}(x)+f_{k-1}(x) f_1(x)$ with $f_0(x)=1$, $f_1(x)=e^x$","Is it possible to recover $f_n(x)$ for any $n$ based off this question ? The relation is: $ \frac{d^n}{dx^n} f_1(x)$ from $\frac{d}{dx} f_{k-1}(x)=f_k(x)-f_{k-1}(x) f_1(x)$ $f_0(x)=1$ and I add $f_1(x)=e^x$ Using this you can find $f_n(x)$ for all $n$ when $n$ is a whole number. $f_0(x)=1,f_1(x)=e^x$ $f_2(x)=e^x+e^{2x}$ $f_3(x)=e^x+3e^{2x}+e^{3x}$ $f_4(x)=e^x+7e^{2x}+6e^{3x}+e^{4x}$ , etc. My question is can you generalize $f_n(x)$ get $n$ to be any number. If so how would you solve $f_{1/2} (x)$ or $f_{i}(x)$","['multivariable-calculus', 'functions', 'stirling-numbers', 'recursion']"
3758182,True or false: Suppose $p$ and $q$ are propositions. Then $\lnot(p\implies q) \equiv p \land q.$,"I am not very familiar with truth tables but I think that the $\lnot$ should get distributed among both $p$ and $q$ making the problem $\lnot p \implies \lnot q$ which does is not the same as $p\land q$ making the statement false. I know that $\lnot q \implies \lnot p$ is the contrapositive of $p \implies q$ which is also equivalent to $\lnot p$ or $q$ , and if we switch the $p$ and $q$ it will still make it false. If anyone can confirm my answer or give more of an explanation that would be great as I am very lost! Thank you to all of the help in advance, it is very appreciated.","['propositional-calculus', 'solution-verification', 'logic', 'discrete-mathematics']"
3758183,Find surface area of part of cylinder.,"I ran into trouble when I'm trying to find a surface area of parts of the cylinder $x^2+z^2=4$ bounded by another cylinder $x^2+y^2=4$ , I simply used a traditional way of double integral, change into polar coordinate calculate $$
\iint\limits_{x^2+y^2=4}
  \sqrt{\left(\frac{\partial z}{\partial x}\right)^2+
        \left(\frac{\partial z}{\partial y}\right)^2+1} \,dx\,dy
 = 
\int_0^{2\pi}\int_{0}^{2} \frac{2r}{\sqrt{4-(r\cos\theta)^2}} \,dr\,d\theta
$$ and eventually this integral diverges. Could anyone tell me where I was wrong ?  thanks a lot.","['multivariable-calculus', 'surface-integrals']"
3758200,"$S \subseteq \mathbb{N}$ and $\forall k \exists x : \forall i,j \in S, |i-j| \notin [x,x+2^k].$ Prove $S$ has zero density in $\mathbb{N}.$","$S \subseteq \mathbb{N}$ and $\forall k \in \mathbb{N},$ there exists $x$ (depending on $k$ ) such that $\forall i,j \in S,$ we have $|i-j| \notin [x,x+2^k].$ How do I show that the density of $S$ in $\mathbb{N}$ is zero, or equivalently that $\lim\limits_{n \to \infty} \frac{S_n}{n} = 0$ where $S_n = |\{x \le n : x \in S\}|$ ? Any individual value of $k$ does not restrict $S$ much because we can take $x = 2^k$ and $S = \{ x : (x \mod 3 \cdot 2^k) \in [0, 2^k) \}.$ On the other hand, we clearly have $|S_n|/|n| \le 0.5$ for $n$ large enough since once $x$ is defined, only one of $a, a+x$ can be in $S$ for all $a \in \mathbb{N}.$ Thus, we need to combine information from multiple values of $k.$ Approach 1: Perhaps we need to prove a statement of the form ""at most one of $a, a+c, 1+2c, \dots, a+nc$ can be in $S$ for all $a \in \mathbb{N}$ "" for some $c$ depending on $n.$ Taking $n \to \infty$ proves the result. However, I have no idea how to proceed past the case $n=1.$ Approach 2: Maybe the statement could be proven by contradiction. If we suppose the limit is not zero, then there exists some $\epsilon > 0$ such that $S_n \ge n\epsilon$ for arbitrarily large values of $n.$ We need to derive a contradiction for $n$ large enough. However, I have no idea how large ""large enough"" would be here, so I have no idea how to proceed. Any other approaches, ideas, or hints?","['number-theory', 'pigeonhole-principle', 'combinatorics']"
3758206,Inclusion exclusion in a combinatorics question,"The question :- Suppose we have an infinite number of Red balls, Green balls, White balls, and Blue balls, and we need to select $10$ balls.  We are required to find the probability that a selection contains balls of all the different colours. (The essence of having an ""infinite"" no. of balls is that the composition remains the same after each draw, so the probabilities aren't affected). Approach-1 :
Suppose the no. of Red,Green,White,Blue balls selected are $r,g,w,b$ . Then : Favourable cases: No. of integer solutions of the equation $r+g+w+b=10$ , such that $r,g,w,b >0$ = $9\choose 3$ = $84$ . Total cases: No. of integer solutions of the equation $r+g+w+b=10$ , such that $r,g,w,b \geq 0$ = $13\choose3$ = $286$ . Which gives the (correct answer) as $42/143$ . Approach 2 : Each selection has $4$ options: i.e select $r,g,w$ or $b$ . Therefore, there are $4^{10}$ total options. By the principle of inclusion-exclusion, the favorable cases must be: $4^{10}$ - $4\choose1$$3^{10}$ + $4\choose2$$2^{10}$ - $4\choose3$$1^{10}$ . However, this approach doesnt give the correct answer. Whats wrong in using the IEP here?","['inclusion-exclusion', 'combinatorics', 'probability']"
3758213,Convergence of $\sum_{p>2} \frac{(-1)^{\frac{p-1}{2}}}{p}$,"Consider the sum $\sum_{p>2} \frac{(-1)^{\frac{p-1}{2}}}{p}$ where $p$ runs only through all odd primes. Show that this sum converges.
The possibly best approach I have until now is via Partial summation, but dealing with number of primes
is troubling, especially for obtaining explicit bounds. Any help appreciated!","['analytic-number-theory', 'numerical-methods', 'prime-numbers', 'sequences-and-series']"
3758237,Proving or disproving this matrix $V$ is invertible.,"Here is a question on the invertibility of a special structured matrix: Notations: Let us take $n\in \mathbb{N}^*$ bins and $d\in \mathbb{N}^*$ balls. Denote the set $B = \{\alpha^1, \ldots, \alpha^m\}$ to be all possible choices for putting $d$ balls into $n$ bins, such as $$\alpha^1 = (d,0,\ldots, 0), ~ \alpha^2 = (0,d,\ldots, 0), \ldots$$ Let us define the matrix $V$ as: $$V = \begin{bmatrix}
(\alpha^1)^{\alpha^1} & \cdots & (\alpha^1)^{\alpha^m}\\
(\alpha^2)^{\alpha^1} & \cdots & (\alpha^2)^{\alpha^m}\\
\vdots & \vdots & \vdots\\
(\alpha^m)^{\alpha^1} & \cdots & (\alpha^m)^{\alpha^m}
\end{bmatrix}$$ where the notation $(\alpha^i)^{\alpha^j} = \displaystyle\prod_{k=1}^{n}(\alpha^i_k)^{\alpha^j_k}$ (under assumption that $0^0=1$ , and $\alpha^i_k$ indicates the $k$ th element of $\alpha^i$ ). Question: "" is the matrix $V$ invertible? "" I have tested several examples, and it seems that $V$ is always invertible, but I have no idea how to prove it or find a counterexample. Here are two facts I understood: all diagonal elements are strictly positives, so the trace of $V$ is strictly positive. the matrix $V$ is not symmetric. Can anyone help prove or disprove the invertibility of $V$ ? Thanks a lot in advance for sharing any idea, any useful discussion and remark. For a better understanding of the problem, I add here an example: Example: Let $n=3$ and $d=2$ , then we have all possible choices for putting $2$ balls into $3$ bins as: $$B = \{(2,0,0),(0,2,0),(0,0,2),(1,1,0),(1,0,1),(0,1,1)\}.$$ The elements in the first row of the matrix $V$ are computed as: $$(\alpha^1)^{\alpha^1} = (2,0,0)^{(2,0,0)} = 2^2\times 0^0\times 0^0 = 4,$$ $$(\alpha^1)^{\alpha^2} = (2,0,0)^{(0,2,0)} = 2^0\times 0^2\times 0^0 = 0,$$ and so on. Thus, we have the matrix $V$ as: $$V = \begin{bmatrix}
4&0&0&0&0&0\\
    0&4&0&0&0&0\\
    0&0&4&0&0&0\\
    1&1&0&1&0&0\\
    1&0&1&0&1&0\\
    0&1&1&0&0&1\\
\end{bmatrix}
$$ which is clearly invertible. Remarks: Note that, the matrix $V$ is not always triangular. as an example, for n=3, d=3, the element $$(2,1,0)^{(1,2,0)} = 2^1\times 1^2 \times 0^0 = 2,$$ and $$(1,2,0)^{(2,1,0)} = 1^2\times 2^1 \times 0^0 = 2$$ which are both non-zeros, thus $V$ will be never invertible in this case. When the elements $(\alpha^i)^{\alpha^j}$ and $(\alpha^j)^{\alpha^i}$ (with $i\neq j$ ) are non-zeros, they may not be equal.
E.g., n=4, d=8, we have $$(1,1,2,4)^{(2,2,2,2)} = 64$$ but $$(2,2,2,2)^{(1,1,2,4)} = 256.$$ As a conclusion: ""the matrix $V$ is neither triangular nor symmetric in general.""","['matrices', 'linear-algebra', 'inverse']"
3758249,Proving equality between two limsups,"Say we have two bounded sequences $a_{n}, b_{n}$ , where $\lim_{n\to\infty}a_{n}=1$ . I have to prove that $\limsup_{n\to\infty}a_{n}b_{n}=\limsup_{n\to\infty}b_{n}$ .
My idea was as following: First $b_{n}$ is bounded, so we can denote $\limsup b_{n}=L$ . There exists a sub-sequence of $b_{n}$ , $b_{n_{k}}$ that satisfies $\lim b_{n_{k}}=L$ .
Now we look at the sub-sequence of $a_{n}b_{n} - a_{n_{k}}b_{n_{k}}$ . $\lim a_{n_{k}}=1$ as a sub-sequence of a converging sequence. So using limit arithmetic rules, $\lim a_{n_{k}}b_{n_{k}}=L=\limsup b_{n}$ . Now whats left is to prove that $\lim a_{n_{k}}b_{n_{k}}=\limsup a_{n}b_{n}$ . Let's assume by contradiction that this is false. So there exists a sub-sequence $a_{n_{j}}b_{n_{j}}$ that satisfies $\lim a_{n_{j}}b_{n_{j}}=K, K>L$ . $a_{n_{j}}$ converges to $1$ and $a_{n_{j}}b_{n_{j}}$ converges to $K$ , so we can assume that $b_{n_{j}}$ converges to $K$ , in contradiction to the assumption that $\limsup b_{n}=L$ . Is my argument correct or am I missing something?","['limsup-and-liminf', 'solution-verification', 'analysis', 'sequences-and-series']"
3758250,"Finding density of $U = \frac{X}{X + Y}$ for $X, \ Y $ ~ $\text{Exp}(\lambda)$ i.i.d [duplicate]","This question already has answers here : X,Y are independent exponentially distributed then what is the distribution of X/(X+Y) (3 answers) Closed 3 years ago . Problem: Given $X, Y$ ~ $\text{Exp}(\lambda)$ i.i.d, find $f_U, \ F_U$ for $U := \frac{X}{X + Y}$ . My approach: For a fixed $u > 0$ , parameterize $\{ (x,y) | \frac{x}{x + y} = u \}$ = $\{ (x,y) | y = \frac{x  (1 - u)}{u} \}$ = $\{ (x,\frac{x  (1 - u)}{u}) | x \geq 0\}$ ( $x \geq 0$ holds by $X$ ~ $\text{Exp}(\lambda)$ ). Then, one can compute: $$\int_0^{+\infty}f_X(x)  f_Y\left(\frac{x (1 - u)}{u}\right) \mathrm{d}x = \int_0^{+\infty} \lambda  e^{-\lambda x}  \lambda  e^{-\lambda \frac{x  (1 - u)}{u}} \mathrm{d}x = \lambda^2 \int_0^{+\infty} e^{-\lambda x  \frac{1}{u}} \mathrm{d}x = \\ \lambda^2  \left(-\frac{u}{\lambda}  e^{-\lambda x  \frac{1}{u}} \biggr{\rvert}_0^{+\infty}\right) = \lambda^2  \left(\frac{u}{\lambda}\right) = \lambda u $$ My problem: Given those computations, I arrived at the conclusion $f_U(u) = \lambda u$ . Although Wolfram Alpha agrees with my computations, the master solution does not, as according to it $F_U (u) = u$ (and therefore $f_U = 1$ ). It'd be great to get some help on where I went wrong. Given that Wolfram Alpha indicates correct computations, I believe my mistake to be conceptual. On a general note: How would you rate my approach; are there better ways to tackle such problems?","['integration', 'probability-distributions', 'exponential-distribution', 'probability']"
3758269,Verifying a solution for the differential equation $(D-m)^3y=0$,"Verify that $y=c_1e^{mx}+c_2xe^{mx}+c_3x^2e^{mx}$ satisfies the differential equation $(D-m)^3y=0$ . $(D= {d\over dx})$ I solved this problem, but our professor said that it was wrong because my method is not general and it is for a specific case, but I failed to understand what is wrong here, can someone please explain why? Here is my professor's explanation: ""You need show that in general. This method is not general. It is for a specific case"" Here is what I did: I substituted given $y$ into the differential equation, then proved it is equal to zero. Consider, $$(D-m)^3(c_1e^{mx}+c_2xe^{mx}+c_3x^2e^{mx})=(D-m)^3c_1e^{mx}+(D-m)^3xc_2e^{mx}+(D-m)^3x^2c_3e^{mx}$$ then I used the fact that $F(D) e^{ax}V(x) =  e^{ax}F(D+a)V(x)$ : $$(D-m)^3(c_1e^{mx}+c_2xe^{mx}+c_3x^2e^{mx})=e^{mx}(D)^31+e^{mx}(D)^3x+e^{mx}(D)^3x^2$$ $$e^{mx}(D)^3+e^{mx}(D)^3x+e^{mx}(D)^3x^2  = e^{mx}(D^31)+e^{mx}(D^3x)+e^{mx}(D^3x^2)=0$$ Thus: $$(D-m)^3(c_1e^{mx}+c_2xe^{mx}+c_3x^2e^{mx})=0$$ Then I said that $y=c_1e^{mx}+c_2xe^{mx}+c_3x^2e^{mx}$ satisfies the differential equation $(D-m)^3y=0$ . Please explain, is it incorrect? Thank you!",['ordinary-differential-equations']
3758308,Does iterating the complex function $z\mapsto\frac{2\sqrt z}{1+z}$ always converge?,"Given $z_0\in\mathbb C\setminus\{-1\}$ , define the sequence $$z_{n+1}=\frac{2\sqrt{z_n}}{1+z_n}$$ where the square root is the one with positive real part (or, if that's not possible, non-negative imaginary part). This is always defined; if ever $z_{n+1}=-1$ , then $$-(1+\sqrt{z_n}^2)=2\sqrt{z_n}$$ $$0=(1+\sqrt{z_n})^2$$ which implies $\sqrt{z_n}=-1$ , a contradiction. So $z_n\neq-1$ for any $n$ . If it converges at all, then it converges to $0$ or $1$ : $$z=\frac{2\sqrt z}{1+z}$$ $$\sqrt z^2(1+\sqrt z^2)=2\sqrt z$$ $$\sqrt z\big(\sqrt z+\sqrt z^3-2\big)=0.$$ The cubic factor has roots $\sqrt z=1$ and $\sqrt z=-\tfrac12\pm\tfrac i2\sqrt7$ , but the latter have negative real part so must be discarded. If $z_n$ is near $0$ , then $1+z_n\approx1$ , and $|z_{n+1}|\approx2\sqrt{|z_n|}>2|z_n|$ ; the sequence gets pushed away from $0$ . But of course if $z_n=0$ exactly, then $z_{n+1}=0$ and it converges trivially. If $z_n=1+\varepsilon$ is near $1$ , then $1+z_n\approx2$ , and $z_{n+1}\approx\sqrt{z_n}\approx1+\tfrac12\varepsilon$ is even closer to $1$ . So, given $z_0\in\mathbb C\setminus\{-1,0\}$ , if the sequence converges it must converge to $1$ . Consider the distance from $1$ : $$1-z_{n+1}=\frac{1+z_n-2\sqrt{z_n}}{1+z_n}=\frac{(1-\sqrt{z_n})^2}{1+z_n}=\frac{(1-z_n)^2}{(1+z_n)(1+\sqrt{z_n})^2}.$$ Since a square root has non-negative real part, $|1+\sqrt{z_n}|>1$ , and thus $$|1-z_{n+1}|<\frac{|1-z_n|^2}{|1+z_n|}.$$ Now consider the distance from $-1$ : $$1+z_{n+1}=\frac{1+z_n+2\sqrt{z_n}}{1+z_n}=\frac{(1+\sqrt{z_n})^2}{1+z_n}$$ $$|1+z_{n+1}|>\frac{1}{|1+z_n|}.$$ We also have $$1-z_{n+1}\!^2=(1-z_{n+1})(1+z_{n+1})=\frac{(1-z_n)^2}{(1+z_n)^2}$$ and $$\frac{1-z_{n+1}}{1+z_{n+1}}=\frac{(1-\sqrt{z_n})^2}{(1+\sqrt{z_n})^2}.$$ I don't know where this is going. Can we show that $\lim_{n\to\infty}|1-z_n|=0$ ? Here's another approach: $$z_{n+1}=\frac{2\sqrt{z_n}}{1+z_n}=\frac{2\sqrt{z_n}(1+z_n^*)}{|1+z_n|^2}$$ $$=\frac{2}{|1+z_n|^2}\big(\sqrt{z_n}+|z_n|\sqrt{z_n}^*\big).$$ Both $\sqrt{z_n}$ and its conjugate $\sqrt{z_n}^*$ have non-negative real part. This expression shows that $z_{n+1}$ is a conical combination of them, so its angle is between their angles, which are half of the original angle of $z_n$ . Thus, with $z_n=r_ne^{i\theta_n}$ , $$|\theta_{n+1}|\leq\frac{|\theta_n|}{2}$$ $$|\theta_n|\leq\frac{|\theta_0|}{2^n}\leq\frac{\pi}{2^n}$$ $$\lim_{n\to\infty}\theta_n=0.$$ I found something interesting, but maybe not useful. If $|z_{n+1}|=1$ then $$|1+z_n|^2=4|z_n|;$$ this equation represents a limacon , with shape parameters $a=4,\,b=\sqrt8$ . If $|z_{n+1}|<1$ then $z_n$ is outside of the curve, or in the tiny loop containing $0$ . If $|z_{n+1}|>1$ then $z_n$ is in the larger inside part of the curve (which includes the unit circle $|z_n|=1$ ). Going in the other direction, if $|z_n|=1,\,z_n=e^{i\theta_n}$ , then $$z_{n+1}=\frac{2}{|1+e^{i\theta_n}|^2}\big(e^{i\theta_n/2}+1e^{-i\theta_n/2}\big)$$ $$=\frac{2}{1+2\cos\theta_n+1}\big(2\cos(\theta_n/2)\big)$$ $$=\frac{\cos(\theta_n/2)}{\cos^2(\theta_n/2)}=\sec(\theta_n/2)>1.$$ And if $z_n=r_n>0$ , then $$z_{n+1}=\frac{2\sqrt{r_n}}{1+r_n}<1$$ because $$2\sqrt{r_n}<1+\sqrt{r_n}^2$$ $$0<\big(1-\sqrt{r_n}\big)^2.$$ My angle argument shows that $z_n$ is in the right half of the plane for $n\geq1$ , so $$|1+z_n|>1$$ and thus $$|1-z_{n+1}|<\frac{|1-z_n|^2}{|1+z_n|}<|1-z_n|^2$$ $$<|1-z_{n-1}|^4<\cdots<|1-z_1|^{2^n}$$ which clearly converges to $0$ , provided that $|1-z_1|<1$ . So we only need to show that the sequence eventually comes within the unit circle around $1$ .","['sequences-and-series', 'inequality', 'convergence-divergence', 'complex-numbers', 'dynamical-systems']"
3758331,Equality of the SchlÃ¤fli's integrals,"I want to show that (ex: (10.9.19) and (10.9.6) in here ) $$ J_{\nu}(z) = \frac{1}{2i\pi} \left( \frac{z}{2}\right)^{\nu}\! \int_{\mathcal{H}} e^{w-\frac{z^2}{4\, w}} w^{-\nu-1} \, dw = \frac{1}{\pi} \int_0^\pi \cos(z \sin\theta - \nu \theta)\,d\theta - \frac{\sin \nu \pi}{\pi} \int_0^\infty e^{-z \sinh t - \nu t} \, dt$$ where $\mathcal{H}$ is the Hankel contour (wrapping around the negative half line, because here I take the convention that $w^{\nu-1} :=e^{(\nu -1) \log w}$ with $\ \log (w) = \ln \lvert w\rvert + i \operatorname{Arg}(w),\ \operatorname{Arg}(w) \in ]-\pi,\pi[$ , i.e. the principal determination of the logarithm defined on $\mathbb{C}\backslash \mathbb{R}_-$ ). In the book ""Fonctions spÃ©ciales de la physique mathÃ©matique"", A. Lesfari p.79 or in Treatise on the Theory of Bessel Functions, G. N. Watson (Reprint 1996, p.176), the first step is to do the following change of variable $$ w = \frac{z}{2} u\quad \Longrightarrow\quad  w -\frac{z^2}{4\, w}  = \frac{z}{2} \left( u -\frac{1}{u} \right) \\\text{and if}\; ``w=\gamma(t)= \frac{z}{2} \tilde{\gamma}(t)""  
 \text{then}\ \left(\frac{z}{2}\right)^{\nu} \gamma^{-\nu -1}\, \gamma'\, dt = \left(\frac{z}{2}\right)^{\nu} \frac{d}{d t}\left(\frac{z}{2} \tilde{\gamma}\right)^{-\nu}\! dt = \tilde{\gamma}^{-\nu -1}\, \tilde{\gamma}'\, dt $$ so that $$ J_{\nu}(z) = \frac{1}{2i\pi} \int_{\left(\frac{z}{2}\right)^{-1}\!\cdot\,\mathcal{H}} e^{\frac{z}{2} \left(u-\frac{1}{u} \right)} u^{-\nu-1} \, du$$ It seems to me indeed that $w\in \mathcal{H}\ \Leftrightarrow\ u\in \left(\frac{z}{2}\right)^{-1}\!\cdot\,\mathcal{H}$ . They however still integrate over $\mathcal{H}$ ... what did I missed? It also seems to me that if I integrate over this new contour, I do not get the final result. The problem does not arise in these notes where $z:=x \in \mathbb{R}$ .","['integration', 'complex-analysis', 'calculus', 'special-functions']"
3758349,In how many ways can 10 people with distinct heights be permuted such that there are no 6 consecutive people in increasing order of height?,"There are 10 people waiting in line at a cafe, and each person has a different height. In how many ways can they be permuted such that there are no 6 consecutive people in increasing order of height (from back to front)? I have been thinking about this question for a while now and I know that without the restrictions the people can be lined up in 10! ways and I think it should be divided by 6! since there can not be 6 consecutive people in increasing order of height (from back to front) standing next to each other. Can anyone confirm my answer or help me fix it if it needs something added? Thank you in advance for any answers or explanations!","['permutations', 'solution-verification', 'combinatorics', 'discrete-mathematics', 'problem-solving']"
3758352,What's the gradient of a vector field?,"Imagine I have the following function $$ \vec{f}(\vec{x}) = x \vec{x}, x = | \vec{x} |, \vec{x} \in R^3 $$ That is, the function is essentially a quadratic function, but contains a vector direction as well. Intuitively from single variable calculus I would expect the gradient $ \nabla \vec{f} = (\partial \vec{f}/ \partial x_1,\partial \vec{f}/ \partial x_2,\partial \vec{f}/ \partial x_3) $ to be proportional to $2x$ , however I also would expect it to be a 3x3 matrix. My most naive attempt would be to do $$ \vec{f} = x_1^2 \vec{e}_1 + x_2^2 \vec{e}_2 + x_3^2 \vec{e}_3 $$ and say that $$ \nabla \vec{f} = 2 x_1 \vec{e}_1 + 2 x_2 \vec{e}_2 + 2 x_3 \vec{e}_3 $$ But it would mean that every gradient w.r.t. a vector would always be a diagonal matrix, which seems wrong to me. What I really want to create is the Jacobian $ \partial \vec{f}_i / \partial x_j $ but I think I get a little bit confused about what I do with the base vectors $ \vec{e_i} $ during the partial derivative.","['partial-derivative', 'jacobian', 'multivariable-calculus']"
3758368,Interchange derivative and expectation operator,"I have a function $$f_\Sigma(x) = c\det(\Sigma^{-1})\exp(-0.5\Vert\Sigma^{-1}x\Vert^2_2)$$ and a function $$g_\Sigma(x - u) = f_\Sigma(x - u)\exp(-0.5\Vert AW_\Sigma a\Vert^2_2),$$ where $W_\Sigma$ is a $n\times n$ diagonal matrix whose $(j,j)$ element is given by $$w_{jj} = \frac{f_\Sigma(x - X_j)}{\sum_{i=1}^nf_\Sigma(x - X_i)}.$$ $A$ is matrix and $a$ some vector. $c$ is a constant and $X_1,X_2,\dots,X_n$ is a collection of iid random variables. I want to verify whether $$\frac{\partial}{\partial\Sigma}\mathbb E[g_\Sigma(x - U)] = \mathbb E\left[\frac{\partial}{\partial\Sigma}g_\Sigma(x - U)\right],$$ where $U$ is a random variable having the same distribution as the $X_i$ .
I found it quite difficult to compute the derivative. Since I only want to verify whether derivative and expectation can be interchanged, I was wondering whether there is an approach to bind the derivative so that the conditions of the DCT are satisfied. Edit : To add some background, $g_\Sigma$ is the multivariate local polynomial density estimator in case of linear fitting ( https://projecteuclid.org/euclid.aos/1032298287 ). I want to study properties of the derivative of this estimator with respect to the bandwidth parameter $\Sigma$ . Eventually, I guess the background is not too important as its more related to an application of DCT to a very very complicated function. I was hoping that someone with more calculus experience has may encountered similar functions and knows a ""trick"" to handle the difficulties arising from the $\exp$ terms.","['expected-value', 'derivatives']"
3758394,"If an infinite set $S$ of positive integers is equidistributed, is $S+S$ also equidistributed?","By $S+S$ , I mean $\{x+y,$ with $x,y \in S\}$ . By equidistributed, I mean equidistributed in residue classes, as defined here (the definition is very intuitive, and examples of such equidistributed sets are provided on that page). My goal here is to prove that if $S$ is equidistributed and contains enough elements (see here ), then $S+S$ covers all the positive integers except a finite number of them. The first step, I think, would be to prove that $S+S$ is equidistributed. I d guess the proof is not difficult and this fact has probably been established, but I could not find a reference. The result seems obvious if you consider ""equidistribution modulo 1"" instead, so I suppose it can also be derived (probably in a similar way) for ""equidistribution in residue classes"", as the two concepts are closely related and based on a similar Weyl criterion (a continuous version for modulo 1, a discrete version for residue classes.) Special case If the infinite set $S$ of positive integers is such that if $x,y\in S$ then $x+y\notin S$ , it is easy to prove that $S$ equidistributed implies $S+S$ is also equidistributed. Note that $S$ is called a Sidon set or sum-free set .
Let $N_S(z)$ be the number of elements of $S$ less or equal to $z$ , and let's assume that $$N_S(z)\sim \frac{a z^b}{(\log z)^c} \mbox{ as } z\rightarrow\infty$$ where $a,b,c$ are non-negative real numbers with $b\leq 1$ . The sets that I am interested in all have $b>\frac{1}{2}$ , for instance, pseudo-primes ( $a=b=c=1$ ) or pseudo-superprimes ( $a=b=1, c=2$ ). Such sets satisfy this conjecture A (see here ) : all but a finite number of positive integers can be written as $z=x+y$ with $x, y \in S$ . That conjecture would imply that $S+S$ is de facto equidistributed, since essentially, in this case $S+S$ it is the set of all positive integers minus a finite number of them. However, that conjecture A is precisely what I would want to prove, so I can not use it as a justification to establish the much weaker result that $S+S$ is supposedly equidistributed if $b>\frac{1}{2}$ and $S$ is equidistributed. If $b\leq \frac{1}{2}$ , conjecture A is not true. Hints to prove the result and win the bounty In case it is not true, a counter-example will do. Assuming it is correct, a sketch of a proof for a simple case is enough. Here is how it could start. Let $T = S+S$ and $$S(n,q) = \{x\in S, x=q \bmod{n}\}.$$ We have $$T(n,q) = \bigcup_{p=0}^{n-1}\Big[S(n,p) + S(n,(q-p) \bmod{n} )\Big]$$ $$T=\bigcup_{q=0}^{n-1}T(n,q)$$ $T(n,q)$ is the subset of elements of $T$ that are equal to $q$ modulo $n$ . The subsets $T(n,q)$ for any given $n>1$ form a partition of $T$ . However the first union for $T(n,q)$ consists of potentially overlapping sets, making the problem non-trivial. Proving the result consists in proving that for any integer $n>1$ and $0\leq q,q'<n$ we have $$\frac{N_{T(n,q)}(z)} {N_{T(n,q')}(z)}\rightarrow 1 \mbox{ as } z\rightarrow \infty.$$ Again $N_T(z)$ counts the number of elements of $T$ less than or equal to $z$ . We can focus on sets $S$ such that $$N_S(z) \sim \frac{a z^b}{(\log z)^c} $$ with $0\leq b \leq \frac{1}{2}$ . I will offer the bounty even if the proof is only for the special case $b=\frac{1}{2}, c=0$ and $n=2$ . For computer experiments (generating such a set $S$ that is supposed to be equidistributed), proceed as follows: the positive integer $k$ belongs to $S$ if and only if $U_k < a/(2\sqrt{z})$ where the $U_k$ 's are independent uniform deviates on $[0, 1]$ . I did some experiments and here are the results, using $n=12, b=\frac{1}{2}$ and looking at all elements of $S$ and $T=S+S$ up to $10^6$ : Equidistribution in $S$ (modulo $n=12$ in this case) means that the ""Ratio_1"" tend to be identical and equal to $\frac{1}{n}$ as you look at more and more elements of $S$ , while equidistribution in $T$ means that the ""Ratio_2"" tend to be identical also converging to $\frac{1}{n}$ . I also did the same test on the set $S$ of perfect squares, which is notoriously not equidistributed. The results are below. Final notes: For perfect squares, $a=1, b=\frac{1}{2}, c=0$ . Even though $T$ , the set of sums of two perfect squares is not equidistributed, the set $T+T$ is the set of all non-negative integers, and is thus equidistributed (that set of course has $a=0, b=1, c=0$ ). The last table suggests that the equations $x^2 + y^2 = 12z+ 3$ , $x^2 + y^2 = 12z+ 7$ , $x^2 + y^2 = 12z+ 11$ do not have integer solutions, while $x^2 + y^2 = 12z$ , $x^2 + y^2 = 12z+6$ and $x^2 + y^2 = 12z+9$ might have only finitely many solutions.","['number-theory', 'additive-combinatorics', 'sumset', 'modular-arithmetic']"
3758403,Counterexample for Converse of Borel-Cantelli Lemma,"The Problem: Let $K$ be a positive integer valued random variable on $(\Omega,\mathcal F,P)$ . Define a sequence $\{X_n\}_{n\in\mathbb N}$ of $\{0,1\}$ -valued random variables on $(\Omega,\mathcal F,P)$ by $$X_n(\omega)=\begin{cases}0&\text{if }n\leq K(\omega)\\1&\text{if }n>K(\omega).\end{cases}$$ Show that by a suitable choice of the distribution of $K,\,\sum_{n=1}^\infty P(X_n\ne1)=\infty$ . In other words, the Borel-Cantelli lemma cannot detect the almost sure convergence $X_n\longrightarrow1.$ My Attempt: Choose a random variable $K$ on $(\Omega,\mathcal F,P)$ with distribution given by $$p_K(k)=\frac{1}{k(k+1)},\quad k\in\{1,2,\dots\}.$$ By construction, $$P(X_n\ne1)=P(n\leq K)=1-\sum_{k=1}^{n-1}\frac{1}{k(k+1)}=1-1+\frac{1}{n}=\frac{1}{n}.$$ Hence, it follows that $$\sum_{n=1}^\infty P(X_n\ne1)=\sum_{n=1}^\infty\frac{1}{n}=\infty.$$ To prove that indeed $X_n\longrightarrow1$ almost surely, note that for a fixed $\omega\in\Omega$ , we have $K(\omega)=k$ for some fixed positive integer $k$ , hence $X_n(\omega)=1$ for all $n>k$ , from which the almost sure convergence follows. Do you agree with my approach and execution presented above? Thank you very much for your time and I really appreciate your most valuable feedback.","['measure-theory', 'solution-verification', 'probability-theory', 'probability', 'random-variables']"
3758418,Image of morphism of sheaves,"Suppose I have a morphism of sheaves $f : E^{\oplus4} \to I_p$ on $X$ a degree $3$ Fano threefold, where $E$ is a rank $2$ vector bundle on $X$ and $I_p$ is an ideal sheaf of a point $p \in X$ . I'm interested in knowing the image of $f$ . For each of the direct summands, I have a morphism $f_i : E \to I_p$ and I know that the image of such a morphism is $I_{D_i}$ where $D_i$ is a degree $5$ curve containing $p$ . Is there a way in which I can use this information to find the image of $f$ ? I'm hoping that this  makes $\mathrm{im}(f) = I_p$ , but I'm not sure if this is true, and if so, how to proceed. Thanks.","['algebraic-curves', 'coherent-sheaves', 'vector-bundles', 'algebraic-geometry', 'sheaf-theory']"
3758457,Can we construct $142$ degrees and $172$ degrees by using only straightedge and compass?,"Can we construct $142$ degrees and $172$ degrees by using only straightedge and compass? I already tried rewrite $142$ to get some angles that can be constructed , such as $90$ , $45$ , $60$ , $30$ , $15$ , $12$ and all multiples of these numbers. But, I still can't construct it. Any hints? Thank you.","['euclidean-geometry', 'angle', 'geometry', 'geometric-construction']"
3758489,Show that a certain space of banded matrices with nonnegative determinants is connected and closed,"Let $n$ and $k$ be two positive integers, with $n \geq 2$ and $k \geq 1$ . Let $l=\{{l_i}\}$ be an $nk$ -dimensional positive real vector. Let $A_{n,k}(l)={(a_{i,j})}_{1 \leq i,j \leq nk}$ be the following $nk \times nk$ matrix: \begin{equation}
a_{i,j}=
\begin{cases}
l_i&\text{if }\quad i=j,\\
-1&\text{if }\quad 1 \leq j-i \leq n-1,\\
-1&\text{if }\quad i \geq nk-n+2\ \text{ and }\ j+nk-i \leq n-1,\\
0&\text{otherwise}.
\end{cases}
\end{equation} That is, $$
A_{n,k}(l) = \pmatrix{
l_1 & -1 & \cdots &-1 &0 &\cdots &0\\
0 & l_2 & -1 &\ddots &-1 &\ddots &\vdots\\
\vdots & 0 & l_3 &-1 &\ddots &\ddots &0\\
0 &\ddots &\ddots &\ddots &\ddots &\ddots &-1\\
-1 &0 &\ddots &\ddots &\ddots &\ddots &\vdots \\
\vdots& \ddots& \ddots&\ddots&0&l_{kn-1}&-1\\
-1 & \cdots & -1 & 0 & \cdots & 0  & l_{kn}},
$$ where the lower triangular bottom-left block has size $(n-1)\times(n-1)$ . Denote the set of $l$ that $\det(A_{n,k}(l)) \geq 0$ be $L$ . I want to show $L$ is a connected and closed set. Any comment and suggestion are welcome.","['matrices', 'general-topology', 'linear-algebra']"
3758557,Curvature of connection in tautological line bundle,"I'm trying to find the mistake in the following computation: let $V$ be a (complex) vector space equipped with a hermitian inner product $\langle\cdot,\cdot\rangle$ , ${\rm P}V$ be the projective space of $V$ and $\mathcal{L} \to {\rm P}V$ be the tautological line bundle. As $\mathcal{L}$ is a subbundle of ${\rm P}V \times V$ , it inherits a fiber metric via $\langle\cdot,\cdot\rangle$ and so we may project the standard flat connection ${\rm D}$ on the trivial bundle to a connection $\nabla$ on $\mathcal{L}$ . Now, sections of $\mathcal{L}$ correspond to homogeneous functions $\mu\colon V\setminus \{0\} \to \Bbb C$ of degree $-1$ via passing $x \mapsto \mu(x)x$ to the quotient. If $X \in \mathfrak{X}({\rm P}V)$ , then the derivative of this map is $$({\rm D}_X\mu)_x = {\rm d}\mu_x(Xx)x+\mu(x)Xx,$$ so since $Xx \in (\Bbb Cx)^\perp$ (since $T_{{\Bbb C}x}({\rm P}V) \cong {\rm Hom}(\Bbb C x, (\Bbb C x)^\perp)$ ), we obtain $(\nabla_X\mu)_x = {\rm d}\mu_x(Xx)$ . And $x\mapsto {\rm d}\mu_x(Xx)$ is also homogeneous of degree $-1$ . The connection $\nabla$ is not flat, because it does not admit any local parallel sections ( $\nabla \mu = 0$ implies ${\rm d}\mu_x|_{(\Bbb C x)^\perp} = 0$ for all $x$ , which contradicts non-integrability of the horizontal distribution of the submersion $\Bbb S^{\dim V-1}\to {\rm P}V$ ). But let's try to compute $R(X,Y)\mu$ . First it seems we have $$\nabla_X\nabla_Y\mu = {\rm d}^2\mu(X,Y) + {\rm d}\mu({\rm d}Y(X)),$$ where ${\rm d}^2\mu$ is ${\rm d}({\rm d}\mu)\colon V\setminus \{0\} \to {\rm Hom}(V,V^*)$ regarded as a (symmetric) bilinear form. This would imply that $$R(X,Y)\mu = {\rm d}\mu({\rm d}Y(X) - {\rm d}X(Y) - [X,Y]).$$ I want to say this is zero because ${\rm D}$ is torsion-free. What's precisely the screw-up here? How to fix it? Thanks.","['differential-geometry', 'riemannian-geometry', 'complex-geometry', 'curvature', 'projective-space']"
3758564,Problem on ratio wrt time distance,"This is a rough translation from a local language so please bear with it, Say we have two people, police $p1$ and thief $p2$ . $p1$ takes $4$ $steps$ and $p2$ takes $5$ $steps$ in the same amount of time . Also the distance $p1$ covers in $6$ $steps$ is equal to the distance $p2$ covers in $8$ $steps$ . What is the ratio of their velocities? What I tried: Let $p1$ and $p2$ cover $d1$ and $d2$ distance in 4 and 5 steps, also let say it takes $t1$ time for $p1$ to take 6 steps and $t2$ time for $p2$ to take 8 steps, so $d1/d2 = 4/5$ and $t1/t2 = 6/8$ , so I get $(d1/t1)/(d2/t2) = 16/15$ In my friend circle we are getting another result $15/16$ (if required I will add the procedure). I just can't wrap my head around the relation of steps with the other units.","['word-problem', 'algebra-precalculus', 'solution-verification', 'ratio']"
3758597,Proof that $\frac{d(\sin x)}{dx} = \cos x$ for $\frac{\pi}{2} < x < \pi$,"Let us assume that $x$ is an angle that lies in the second quadrant i.e. $\dfrac{\pi}{2} < x < \pi$ . We have to prove that $\dfrac{d(\sin x)}{dx} = \cos x$ . I will use the unit circle to prove this. The method will be like the one used by Grant Sanderson of 3Blue1Brown in this video , which is a part of his Essence of Calculus series. The angles are measured in radians. In the diagram below, I have marked the angle $x$ and $dx$ on the unit circle. The angle $dx$ approaches $0$ , so it is very very small but for the sake of clarity, I have made it considerably large. Now, since $dx$ is actually really small, we can approximate arc $AB$ as a straight line approximately perpendicular to $OA$ . We are measuring the angles in radians and we have a unit circle, so its radius is $1 \text{ units}$ . Hence, the length of arc (now line segment) $AB$ is $\dfrac{\theta}{r}$ , where $\theta$ is $\angle AOB$ i.e. $dx$ and $r = 1 \text{ units}$ . So, $AB = \dfrac{dx}{1} = dx$ . Now, $d(\sin x) = \sin(x+dx)-\sin x$ which is the change in the ordinate of $A$ and $B$ . Now, $AP = d(\sin x)$ and $AB = dx$ . Also, $\triangle APB \sim \triangle AOQ$ . So, $\angle BAP = \angle OAQ = \pi - x$ . $\cos(\angle BAP) = \dfrac{AP}{AB} = \dfrac{d(\sin x)}{dx}$ . And $\cos(\angle BAP) = \cos (\pi - x) = -\cos x$ So, $\dfrac{d(\sin x)}{dx} = -\cos x$ which is not the case at all, since $\dfrac{d(\sin x)}{dx}$ will be negative as $\sin(x+dx) < \sin x$ but the sign of $-\cos x$ will be positive as $\cos x < 0$ . So, what mistake did I make here? According to me, the mistake was in assuming that $AP = d(\sin x)$ . I think that $AP$ should be $|d(\sin x)|$ . And as $d(\sin x) < 0 \implies |d(\sin x)| = -d(\sin x)$ . This fixes everything but I still want to verify if this indeed is the cause of the error. Thanks! PS : Let me know if I should justify why $\triangle APB \sim \triangle OQA$ to make the question clearer. PPS : It is necessary to prove that differentiating $\sin x$ with respect to $x$ gives $\cos x$ for all 4 quadrants when proving using the unit circle, right?","['calculus', 'solution-verification', 'derivatives']"
3758626,"$f:[0,1]\to[0,1]$ be a continuous function. Let $x_1\in[0,1]$ and define $x_{n+1}={\sum_{i=1}^n f(x_i)\over n}$.Prove, $\{x_n\}$ is convergent","I have tried a little bit which as follows- Since $f(x_n)\in[0,1]$ , $\{f(x_n)\}$ has a convergent subsequence say $y_n=f(x_{r_n})\ \forall n\in\Bbb{N}$ Let, $\lim y_n=l\implies \lim \frac{y_1+y_2+\cdots+y_n}{n}=l\implies \lim \frac{f(x_{r_1})+f(x_{r_2})+\cdots+f(x_{r_n})}{n}=l$ But I am getting no idea to proceed and prove the convergence of $\{x_n\}$ . I have also tried to prove the sequence to be cauchy which goes- $x_{m+1}-x_{n+1}={\sum_{i=1}^m f(x_i)\over m}-{\sum_{i=1}^n f(x_i)\over n}\le {\sum_{i=1}^m f(x_i)\over n}-{\sum_{i=1}^n f(x_i)\over n}$ (since $m\ge n)$ $\implies |x_{m+1}-x_{n+1}|\le \frac{|f(x_m)|+|f(x_{m-1})+\cdots+|f(x_{n+1})|}{n}\le\frac{m-n}{n}$ (since $f([0,1])\subseteq [0,1])$ Now, what will I get if I tend $m,n\to\infty$ ? But I need to do something more in the 2nd case since I have nowhere used the continuity of $f$ . Can anybody give an idea to prove it? Thanks for assistance in advance.","['continuity', 'convergence-divergence', 'sequences-and-series', 'real-analysis']"
3758671,Bound on truncated trigonometric polynomial,"Let $p(\theta)$ be a real trigonometric polynomial of degree $N>0$ $$
p(\theta) = \sum_{j=-N}^N a_{|j|}e^{\text ij\theta}
$$ and for any $0\le s<N$ define the left trucation of $p$ as $$
p_s(\theta):= \sum_{j=-s}^N a_{|j|}e^{\text ij\theta}.
$$ Is it true that, for any $s$ , $$
\|p(\theta)\|_\infty \le 1 \implies \|p_s(\theta)\|_\infty \le 1?
$$ (Here the infinity norm is just the sup of the absolute value of the funtion on $[-\pi,\pi]$ ) I verified by hand that it is true for $N=1$ , but I am quite sure there exists a counterexample..","['trigonometric-series', 'trigonometry', 'examples-counterexamples', 'upper-lower-bounds']"
3758672,"What are some intuitive ways to find a $3 \times 3$ permutation matrix with $P^3 = I$, $P \ne I $?","Find a $3\times3$ permutation matrix with $P^3 = I$ , $P \ne I$ ? I reduced the above problem to $P^T = P^2$ and tried solving for all $6$ $3 \times 3$ permutation matrices which yielded $$P = \begin{pmatrix} 0&1&0\\0 & 0 & 1\\1&0 & 0\end{pmatrix}$$","['matrices', 'linear-algebra', 'permutation-matrices', 'permutations']"
3758683,"If the number of units of a ring is odd, then the ring has cardinality as a power of two [duplicate]","This question already has an answer here : A ring with few invertible elements (1 answer) Closed 3 years ago . If the number of units of a finite ring is odd, then  does the ring has cardinality as a power of $2$ ? I think yes. For fields, it is trivial. For non-fields, it is a hard question for me. I saw a paper here that sates that an odd number is the cardinality of the group of units of a ring if it is of the form $\prod_i (2^{n_i}-1)$ . But, that proof is quite lengthy, and still the ring need not be a power of $2$ . Any short proof? Thanks beforehand.","['ring-theory', 'group-theory', 'abstract-algebra']"
3758689,"Is it possible to check the validity of the following theorem in Group Theory automatically by a computer? If so, how?","I'm learning right now how to prove the following very basic theorem in Group Theory, which we learn in a Linear Algebra course in my university (I study Computer Science): Let's have a group $(M,*)$ and $a,b\in M$ . Then $x=a^{-1}*b$ is the
only solution to the equation $a*x=b$ . I know how to proof the theorem in a ""Linear Algebra style"". First, I would show that $x=a^{-1}*b$ is a solution to the equation $a*x=b$ , for example like this: $a*(a^{-1}*b) = (a*a^{-1})*b = e*b = b$ . Then I would show that it is the only solution to that equation and the proof would be done (at least my university professor would accept it). However, in the Linear Algebra course we don't use the style of a proof that I have learnt in a Mathematical Logic course that I've just completed (e.g. we were learning to proof a particular logical consequence of a set of first order logic formulas by a method called semantic trees ). I wonder, how this proof of the theorem would look like in this purely ""Mathematical Logic style"", e.g. by using a semantic tree method or some other method which a computer could use to check whether the theorem logically follows from the axioms of the Group Theory. Suppose I have the following definition of a Group Theory (this definition we learnt in a Mathematical Logic course): $\mathscr L=\{f,e\}$ is a language, where $f$ is a binary function symbol and $e$ is a constant called ""neutral element"". The theory of groups has these
axioms: Associativity: $(\forall x)(\forall y)(\forall z)(f(f(x,y),z))=f(x,f(y,z))$ Identity: $(\forall x)(f(x,e)=x)$ Inverse: $(\forall x)(\exists z)(f(x,z)=e)$ If it is possible, how do I check the validity of the theorem in a purely formal logical way (for example by a semantic tree method) so that even a computer can check the validity?","['formal-proofs', 'automated-theorem-proving', 'linear-algebra', 'first-order-logic']"
3758704,Find the area of the largest square that contains exactly $n$ lattice points,"A point $(x,y)$ in the plane is called a lattice point if both $x$ and $y$ are integers. Find the area of the largest square that contains exactly $n$ lattice points in its interior. This question is inspired by 1998 AHSME Problem 29, where $n=3$ . In that case, it suffices to consider squares containing $(0,0), (0,1), (1,0)$ . Note that an optimal square must have parallel sides containing lattice points, and the distance between parallel lines through $(0,-1)$ and $(1,1)$ is $\leq \sqrt{5}$ , so the largest possible area for the square is $5$ . In fact, The following four lines intersect to form a square containing exactly $3$ lattice points with area $5$ : $$2x-y=-2, \quad 2x-y=3, \quad x+2y=-2, \quad x+2y=3.$$ $\hspace{3cm}$ Pick's theorem might be useful if one can translate the lattice so that all the vertices of square fall on points of the new lattice.","['area', 'geometry']"
3758742,How to evaluate $\int \frac{dx}{\sin(\ln(x))}$?,"I am wondering how to evaluate the indefinite integral $$\int \frac{dx}{\sin(\ln(x))} \quad (1)$$ Attempt 1 I tried using Weierstrass substitution. The Weierstrass substitution , (named after K.Weierstrass (1815)), is a substitution used in order to convert trigonometric functions rational expressions to polynomial rational expressions. Integrals of this type are usually easier to evaluate. This substitution is constructed by letting: $$t = \tan\left(\frac{x}{2}\right) \iff x = 2\arctan(t)  \iff dx = \frac{2}{t^2+1}$$ Using basic trigonometric identities it is easy to prove that: $$\cos x = \dfrac{1 - t^2}{1 +  t^2}$$ $$\sin x = \dfrac{2t}{1 + t^2}$$ But I couldn't express $\ln(x)$ in terms of $t$ . Attempt 2 I tried using integration by parts but I couldn't find a workaround, it gets more complicated, really fast. $$ \int \frac{dx}{\sin(\ln(x))} \ = x \sin(\ln(x)) - \int \frac{\cot \left(\ln \left(x\right)\right)}{x\sin \left(\ln \left(x\right)\right)} $$ Attempt 3 The most logical substitution I could think of. It doesn't seem to lead anywhere though. Let, $\ln(x) = u \iff dx = \, e^u du$ $$ (1) \iff \int \frac{dx}{\sin(\ln(x))} = \int \frac{e^u}{\sin(u)} du = \int \frac{(e^u)'}{\sin(u)} du = $$ $$ \frac{(e^u)'}{\sin(u)} - \int e^u \left(\frac{1}{\sin(u)}\right)' = \frac{(e^u)'}{\sin(u)} - \int e^u \frac{\cos(u)}{\sin^2(u)} =  ?$$ Attempt 4 A combination of attempts 1,2, 3. Let $\ln(x) = t$ then $dx = e^t dt$ , therefore, $$\int \frac{dx}{\sin(\ln(x))} dx = \int \frac{e^t }{\sin(t)}dt \quad (1)$$ Let's first evaluate $$ \int \frac{1\:}{\sin\left(t\right)}dt \quad (2)$$ Using the Weierstrass substitution $$ t = \arctan(\frac{x}{2})$$ it is easy to prove that $$ (2) = \int \frac{1\:}{\sin\left(t\right)}dt= \ln \left|\tan \left(\frac{t}{2}\right)\right|+C$$ Therefore, $$ (1) \iff I = \int e^x\left(\ln \:\left|\tan \:\left(\frac{t}{2}\right)\right|\right)'dt = e^x \ln \:\left|\tan \:\left(\frac{t}{2}\right)\right| - \int (e^x)' \ln \:\left|\tan \:\left(\frac{t}{2}\right)\right|dt = $$ $$  e^x \ln \:\left|\tan \:\left(\frac{t}{2}\right)\right| - \left( e^x \ln \:\left|\tan \:\left(\frac{t}{2}\right)\right|  - \int e^x \left(\ln \:\left|\tan \:\left(\frac{t}{2}\right)\right|\right)'dt \right) $$ $$ I = 0 + I \iff 0=0$$ Tautology. No answer here. Attempt 5 Ask a question on MathExchange: Any ideas? Note: A complex-plane solution was proposed in the comments, but I am evaluating this on $\mathbb{R}$","['integration', 'indefinite-integrals', 'calculus']"
3758776,How many subfields are there between $\mathbb{Q}$ and $\mathbb{Q}[\sqrt[16]{2}]$,"Let $\alpha = \sqrt[16]{2}$ be a positive real number and $K = \mathbb{Q}[\alpha]$ be the algebraic extension over $\mathbb{Q}$ by alpha. Find the number of intermediate field $F$ such that $\mathbb{Q} \subseteq F \subseteq K$ . Ok. I have an ugly solution. Suppose that $F$ is such an intermediate field. Consider the irreducible polynomial of $\alpha$ over $F$ , say $f(x)$ . Clearly $f(x) | x^{12}-2$ . Noting that all the fields under consideration are contained in the real field, and $$x^{16}-2 = \left(x-\sqrt[16]{2}\right) \left(x+\sqrt[16]{2}\right) \left(-x^2+2^{9/16}
   x-\sqrt[8]{2}\right) \left(x^2+\sqrt[8]{2}\right) \left(x^2+2^{9/16} x+\sqrt[8]{2}\right)
   \left(-x^4+2^{5/8} x^2-\sqrt[4]{2}\right) \left(x^4+2^{5/8} x^2+\sqrt[4]{2}\right)$$ and observing that all the coefficients are of the form $\alpha^k$ , I have concluded that
(*) $F$ should be of the form $\mathbb{Q}[\alpha^t]$ , $t=0,1,2,4,8,16$ .
My question is the following; Is there an argument to say (*) without actual fatorization? or it could be great if we one can prove the following All the subfields of $\mathbb{Q}[2^{\frac{1}{2^n}}]$ are of the form $\mathbb{Q}[2^{\frac{1}{2^k}}]$ , $0 \le k \le n$ . Thanks for your attention.","['irreducible-polynomials', 'field-theory', 'galois-theory', 'abstract-algebra', 'polynomials']"
3758824,When is the sum of two uniform random variables uniform?,"Suppose that $X$ and $Y$ , two random variables, are both uniformly distributed over $[0,1]$ . Let $Z=\frac{1}{2}X+\frac{1}{2}Y$ . I know that in general, $Z$ is not uniform. For instance, $Z$ is not uniform if $X$ and $Y$ are independent. On the other hand, if $X=Y$ , then $Z$ is uniformly distributed over $[0,1]$ . My question: Suppose $Z$ is uniformly distributed over $[0,1]$ . Is $X=Y$ ? In other words, is $X=Y$ the only case where $Z$ is uniform over $[0,1]$ ?","['probability-distributions', 'uniform-distribution', 'probability-theory', 'random-variables']"
3758828,Understanding the definition of a differential operator on manifolds,"In Christian Bar's ""Geometric Wave Equations"" notes it has this definition of a differential operator. I know what $\frac{\partial f}{\partial x^i}$ means when $f:M\rightarrow \mathbb{R}^n$ is a smooth function. But I don't understand what is meant by $\frac{\partial v}{\partial x^i}$ when $v:M\rightarrow E$ is a smooth section (M and E being manifolds). Any help is appreciated, cheers.",['differential-geometry']
3758836,Sticks and stones but with distinct objects,"The total number of ways which 5 balls of different colors can be distributed among 3 persons so that each person gets at least one ball is..? My attempt: So, first we can choose any three balls to allocate to any person, so $\binom{5}{3}$ and, then distribute them then in 3! ways. And, so we have, $\binom{5}{3} \cdot 3! $ . Now I have two remaining balls and this I can give randomly to any of the three people. For the first ball I have three choices and so do I for the second ball. Hence,  The net number of cases is $ \binom{5}{3} \cdot 3! \cdot 3^2$ Now, I know that like the thing can happen in reverse. For example distributing three then distributing two, I could have some overlapping cases where the items which are distributed in the distributing two case is actually distributed in the distributing three step. How do I account for the overlapping case? Ans: 150 Sticks and stones attempt: We have five objects and introduce two objects as dividers, subtract cases where one guy gets nothing (3 x 6!) and add cases where two people get nothing and third guy everything (3) What I am looking for : A fix to this method such that it gives me the correct answer, in the other stack post linked to this one, it has ample amount of methods based on other ways to solve it but my question here is how to fix this method.",['combinatorics']
3758853,"Let $A,B\in\mathbb{R}^{n\times n}$, where $A$ is PSD and $B$ NSD. If $\mathrm{tr}(AB)=0$, show that $AB=0$.","Let $A,B\in\mathbb{R}^{n\times n}$ , where $A$ is a positive semidefinite matrix and $B$ a negative semidefinite matrix. If $\mathrm{tr}(AB)=0$ , show that $AB=0$ .","['matrices', 'linear-algebra']"
3758860,Are two polynomials which take the same values when evaluated equal?,"Two polynomials are considered equal if they have equal coefficients of corresponding powers of the independent variable, after like terms are combined. If two polynomials are equal in this sense, then they are equal as functions; i.e., they give equal results for equal values of the independent variable.
If, conversely, two polynomial functions differ in their corresponding coefficients, do  they always have different values for the same independent variable? Is this true at least for the infinite field of integers, and where I can find a proof of that?",['abstract-algebra']
3758883,Find $\Omega$ s.t. $L^1_{loc}(\Omega)$ is a normed space,Is there any measurable $\Omega\subset\mathbb{R}^N$ for some $N\in\mathbb{N}$ with infinite Lebesgue-measure such that there exists a norm for $L^1_{loc}(\Omega)$ ?,"['measure-theory', 'functional-analysis', 'real-analysis']"
3758884,Are there identities on integers that be proved by checking finitely many cases?,"Some time ago, I needed to prove some identity on binomial coefficients . Something similar to the following: $$\sum_{m=0}^n \binom m j \binom {n-m}{k-j}= \binom {n+1}{k+1}$$ but with somewhat different coefficients. I could not find a formal proof, but I computed the values for $j,k,n$ up to 1000 and found that it holds. Now, I know that formally this is not sufficient for a proof, since the the identity might fail on 1,001. However, the identity looks so ""simple"" (in that it uses only multiplications and additions - no roots, prime numbers etc.), that it seems impossible that it would start failing after 1,000. Is there a way to make this intuition formal? I.e., is there a class of identities on integers that are considered so simple, that proving an identity from that class for sufficiently many integers is sufficient for proving that it holds for all integers? Alternatively, is there a binomial identity of a similar simple form, which is known to hold up to some very large integer, but fails afterwards?","['alternative-proof', 'binomial-coefficients', 'combinatorics']"
3758924,Why exactly can you change the order of integration in a double (and triple) integral?,"I'm currently studying multivariable Calculus doing double and triple integrals, and I'm slightly confused on why one can change the order of integration for a double integral. I have my own explanation, but it could be wrong. Is the following reasoning correct? Fubini's Theorem states that the double integral over a given 2D region where at least one of the variables has constants as their highest and lowest values (called a horizontally or vertically simple region, depending on which variable has the constants) is equal to the iterated integral where those constants are the outer integral's limits of integration (and the inner integral's limits are functions of the outer variable). My reasoning is: If the 2D region is describable as an equation of your two variables, say x and y, and the equation is separable, then you could simply solve for either x or y to get the inner integral's limits of integration, then see from your separated equation what endpoint values the other variable could take on. The 2D region doesn't necessarily have to have abrupt, flat lines as the endpoints for the outside-integral variable (as is depicted in pictures of these such regions in textbooks), but it does need to have endpoint values in terms of the outside variable that are constants. So, if the region doesn't have these abrupt lines to describe its endpoints for either variable, but either variable's endpoint values could be described as constants, then you could use any order of integration you like as long as your outer iterated integral has constants as the limits of integration. Is this correct, or does Fubini's Theorem explicitly state that any order of integration is usable (as long as the limits of integration are valid)? Also, in the proof for Fubini's Theorem for Triple Integrals, does the same logic essentially hold or is there another reason you can switch the order of integration? Thank you for taking the time to read this post!","['integration', 'multivariable-calculus', 'multiple-integral', 'fubini-tonelli-theorems']"
3758929,Stability analysis of the dynamical system $\ddot{x}+b\dot{x}+K x-\|\dot{x}\| \frac{x-x_i}{\|x-x_i\|^2}=0$ .,"Consider the dynamical system described as: $$\ddot{z}+b\dot{z}+ K z-\|\dot{z}\| \frac{z-z_i}{\|z-z_i\|^3}=0$$ where $z=[x \ \ y]^T$ , $K$ is a positive definite matrix and $b \in \mathbb{R}$ , I made some simulations and based on the numerical results I concluded that: if $b>0$ , the system converges to either $z=0$ , $z=z_i$ or a limit cycle i.e. stable in the sense of lyapunov. if $b>\frac{1}{\|z_i\|}$ ,  the system converges to either $z=0$ or $z=z_i$ i.e. no limit cycles. I was able to only prove that if $b>\frac{1}{\|z_i\|}$ , $z=0$ is a stable fixed point by using lyapunov function as: \begin{align}& V =\frac{1}{2} z^T K z +\frac{1}{2}\dot{z}^T \dot{z}\\
\implies &  \dot{V}=\|\dot{z}\|^2\left(-b+ \frac{cos(\theta)}{\|z-z_i\|^2}\right)
\end{align} where $\theta$ is the angle between $\dot{z}$ and $z-z_i$ , so if $b>\frac{1}{\|z_i\|}\implies \dot{V}|_{z=0} <0$ independent of $cos(\theta)$ in an open neighborhood of the origin so $z=0$ is a stable fixed point. I tried to study the system near $z_i$ by using perturbation and introduced the parameter $\mu$ to the system as: $$\ddot{z}+b\dot{z}+ K z-(\|\dot{z}\|+\mu) \frac{z-z_i}{\|z-z_i\|^3}=0$$ to study the system near $z_i$ , I chose $\mu \gg \|\dot{z}(0)\|$ so the system becomes: $$\ddot{z}+b\dot{z}+ K z-\mu \frac{z-z_i}{\|z-z_i\|^3}=0$$ Choose lyapunov function as: $$\begin{align}&V=\frac{1}{\frac{1}{2} z^T K z +\frac{1}{2}\dot{z}^T \dot{z}+U_i}\\
\implies &\dot{V}=\frac{b\|\dot{z}\|^2}{(\frac{1}{2} z^T K z +\frac{1}{2}\dot{z}^T \dot{z}+U_i)^2}
\end{align}
$$ where $U_i=\frac{\mu}{\|z-z_i\|}$ , so at $z=z_i$ , $V=0$ and $\dot{V}>0$ , so $z=z_i$ is unstable. However, if I check the equilibrium points by letting the derivatives vanish the system is reduced to: $$K z=\mu \frac{z-z_i}{\|z-z_i\|^3}\implies \|z-z_i\|^3 K z=\mu(z-z_i) \text{ and } z \neq z_i$$ The right hand side can be made arbitrary small by choosing $\mu$ arbitrary small,since $K$ is full rank and $z\neq 0$ so it must be that $\|z-z_i\|$ is getting arbitrary small i.e. $z\rightarrow z_i$ . So the system has another equilibrium point $q$ that is getting closer and closer to the unstable node $z_i$ . I believe $q$ is a saddle point (I don't know how to prove it) and so I concluded that $z_i$ in my original system is a bifurcation between an unstable node and a saddle node. My questions are : How to confirm the above claims ? and How to give a qualitative analysis of the behavior of the system near $z_i$ ?","['ordinary-differential-equations', 'control-theory', 'stability-in-odes', 'nonlinear-dynamics', 'dynamical-systems']"
3758934,A counterexample of Riesz Representation Theorem?,"I am working on the exercise 6.B.15 from Axler's book: Linear Algebra Done Right. This problem states that the Riesz Representation Theorem may fail on an infinite-dimensional vector space: Suppose $C_R([-1,1])$ is the vector space of continuous real-valued functions on the interval $[-1,1]$ with inner product given by: $$\left<f,g \right> = \int_{-1}^{1}f(x)g(x)dx$$ for $f, g \in C_R[-1,1]$ . Let $\varphi$ be the linear functional on $C_R[-1,1]$ defined by $\varphi(f)=f(0)$ . Show that there does not exist $g \in C_R[-1,1]$ such that $$
\varphi(f)= \left< f, g \right>
$$ for every $f \in C_R[-1,1]$ . My argument is that I came up with two special sequences of functions in $C_R[-1,1]$ , and proved a contradiction: Consider $f_{n}, h_{n} \subset C_{\mathbb{R}}[-1,1]$ , defined by $$
    f_{n}(x)=
\begin{cases}
    1, ~ x \in [0,1] \\
    n x + 1, ~  x \in [-\frac{1}{n},0] \\
    0, ~  x \in [-1,-\frac{1}{n}]
\end{cases}
$$ and $$
    h_{n}(x)=
\begin{cases}
    1, ~ x \in [-1,0] \\
    -n x + 1, ~  x \in [0,\frac{1}{n}] \\
    1, ~  x \in [\frac{1}{n},1]
\end{cases}
$$ Then $$ f_n \to f $$ and $$h_n \to h$$ where $f = 0$ on $[-1,0]$ , $f = 1$ on $[0,1]$ , while $h = 0$ on $[0,1]$ and $h=1$ on $[-1,0]$ . Then for any $n \in \mathbb{N}$ ,  we have $\varphi(f_{n}) = f_{n}(0) = 1$ and $\varphi(h_{n}) = h_{n}(x) = 1$ . For any constant function $T_{m}(x) = m \neq  0$ on $[-1,1]$ , $\varphi(T_{m}) = \left< T_{m}, g \right> = T_{m}(0)= m = \int_{-1}^{1}m g(x)\,\mathrm{d}x$ . As a result, then $$
    \int_{-1}^{1}g(x)\,\mathrm{d}x = 1.
$$ It implies that $g \in L^{1}[-1,1]$ . So $\left\vert f_{n} g \right\vert \leqslant \left\vert g \right\vert $ and $\left\vert  h_{n} g \right\vert  \leqslant \left\vert g \right\vert $ . By the LDCT, \begin{align*}
    \lim_{n    \to \infty} \int_{-1}^{1}f_{n}(x)g(x)\,\mathrm{d}x = 1 = \int_{-1}^{1}f(x)g(x)\,\mathrm{d}x = \int_{0}^{1}g(x)\,\mathrm{d}x.
\end{align*} The same argument applies to $h_{n}$ , then we have $$
    \int_{-1}^{0}g(x)\,\mathrm{d}x = 1
$$ and then $$
    \int_{-1}^{1}g(x)\,\mathrm{d}x = \int_{-1}^{0}g(x)\,\mathrm{d}x + \int_{0}^{1}g(x)\,\mathrm{d}x = 2
$$ which contradicts with $\int_{-1}^1 g(x) dx = 1.$ My question is, if this argument is legitimate, why such contradiction could arise? Is it because the inner product space is not Hilbert?","['riesz-representation-theorem', 'analysis', 'real-analysis', 'linear-algebra', 'functional-analysis']"
3758952,Euler's method to approximate a differential equation $\frac{dy}{dx} = x - y$,"Question: Use Euler's method to find approximate values for the solution of the initial value $-$ problem $$\frac{dy}{dx} = x-y$$ $$y(0)=1$$ on the interval $[0,1]$ using five steps of size $h = 0.2$ . My attempts: I know that the recurrence relation $y_{n+1} = y_{n} + hf(x_n,y_n)$ however I am unable to see how the interval comes into play. An idea I had was to consider the bounds of the interval and approximate $y(0)$ and $y(1)$ however this does not include $h$ so I am extremely skeptical. Any help or guidance is greatly appreciated!","['approximation', 'ordinary-differential-equations', 'calculus', 'numerical-methods', 'eulers-method']"
3758981,"Suppose $A$, $B$, and $C$ are sets. Prove that $A\cup C\subseteq B\cup C$ iff $A\setminus C\subseteq B\setminus C$.","Not a duplicate of Prove that $A \cup C \subseteq B \cup C$ iff $A \setminus C \subseteq B \setminus C$ Suppose $A$, $B$, and $C$ are sets. Prove that $A âˆª C âŠ† B âˆª C$ iff $A \setminus C âŠ† B \setminus C$. This is exercise $3.5.6$ from the book How to Prove it by Velleman $($$2^{nd}$ edition $)$ : Suppose $A$ , $B$ , and $C$ are sets. Prove that $A\cup C\subseteq B\cup C$ iff $A\setminus C\subseteq B\setminus C$ . Here is my proof: $(\rightarrow)$ Suppose $A\cup C\subseteq B\cup C$ . Let $x$ be an arbitrary element of $A\setminus C$ . This means $x\in A$ and $x\notin C$ . Since $x\in A$ , $x\in A\cup C$ . From $A\cup C\subseteq B\cup C$ and $x\in A\cup C$ , $x\in B\cup C$ . Now we consider two different cases. Case $1.$ Suppose $x\in B$ . Since $x\notin C$ , $x\in B\setminus C$ . Case $2.$ Suppose $x\in C$ which produces a contradiction. From $x\in B\setminus C$ or a contradiction we obtain $x\in B\setminus C$ . Thus if $x\in A\setminus C$ then $x\in B\setminus C$ . Since $x$ is arbitrary, $\forall x(x\in A\setminus C\rightarrow x\in B\setminus C)$ and so $A\setminus C\subseteq B\setminus C$ . Therefore if $A\cup C\subseteq B\cup C$ then $A\setminus C\subseteq B\setminus C$ . $(\leftarrow)$ Suppose $A\setminus C\subseteq B\setminus C$ . Let $x$ be an arbitrary element of $A\cup C$ . This means $x\in A$ or $x\in C$ . Now we consider two different cases. Case $1.$ Suppose $x\in C$ . Ergo $x\in B\cup C$ . Case $2.$ Suppose $x\notin C$ . From $x\in A\cup C$ and $x\notin C$ , $x\in A$ . Ergo $x\in A\setminus C$ . Since $A\setminus C\subseteq B\setminus C$ , $x\in B\setminus C$ which means $x\in B$ . Thus $x\in B\cup C$ . Since the above cases are exhaustive, $x\in B\cup C$ . Thus if $x\in A\cup C$ then $x\in B\cup C$ . Since $x$ is arbitrary, $\forall x(x\in A\cup C\rightarrow x\in B\cup C)$ and so $A\cup C\subseteq B\cup C$ . Therefore if $A\setminus C\subseteq B\setminus C$ then $A\cup C\subseteq B\cup C$ . Ergo $A\cup C\subseteq B\cup C$ iff $A\setminus C\subseteq B\setminus C$ . $Q.E.D.$ Is my proof valid $?$ Thanks for your attention.","['elementary-set-theory', 'proof-writing', 'solution-verification']"
3758984,"Show that $\mathrm{Cov}[g(X), h(X)] \ge 0$ whenever $g$ and $h$ are nondecreasing. [duplicate]","This question already has answers here : Show $\mathbb{E}[f(X)g(X)] \geq \mathbb{E}[f(X)]\mathbb{E}[g(X)]$ for $f,g$ bounded, nondecreasing (2 answers) Closed 3 years ago . Intuitively, the covariance of two nondecreasing functions of a random variable should be nonnegative. However I can't seem to come up with a proof for this. Here is the formal setup: Let $X: (\Omega, \mathcal A)\to (\mathbb R, \mathcal B)$ be a random variable defined on the probability space $(\Omega, \mathcal A, P)$ and let $g$ and $h$ be nondecreasing functions $\mathbb R \to \mathbb R$ . To make sure that everything is well-defined assume that $E[g(X)^2]<\infty$ and $E[h(X)^2]<\infty$ . Question: Is is always true that $$\mathrm{Cov}[g(X),h(X)]\ge 0\,?$$ Some notes: Note that the assertion is equivalent to showing that $E[g(X)h(X)]\ge E[g(X)]E[h(X)].$ I tried reducing the problem to showing that $$E[Xf(X)] \ge E[X]E[f(X)]$$ holds for every nondecreasing $f$ whenever $E[X^2]$ and $E[f(X)^2]$ are finite. To do this, I defined $Y=h(X),$ $f = g \circ h^{-1}$ and wrote $$E[g(X)h(X)]=E[g(h^{-1}(Y)Y] = E[f(Y)Y].$$ But this assumes that $h$ is strictly increasing, which is not necessarily true. Moreover, even in this case I'm not sure how to start.","['measure-theory', 'real-analysis', 'inequality', 'probability-theory', 'probability']"
3758988,Differential of a canonical map,"While studying about curvatures I came up with the following but was unable to work it out fully. Let $M \subset \mathbb{R^n}$ be an embedded submanifold of dimension $k$ . Then there is a natural (smooth?) map $$\alpha: M \rightarrow \text{Gr}(k,\mathbb{R}^n),$$ given by $p \mapsto T_p M$ . I know the grassmanian admits a natural description of its tangent spaces, namely: $$T_V\text{Gr}(k,\mathbb{R}^n) \cong \text{Hom}(V , V^+)$$ where by $V^+$ I mean the orthogonal complement. Is there a nice description of its differential at an arbitrary point $p$ ? I couldnâ€™t work it out. Also, a closesly related question: suppose we look instead of at $p \mapsto T_pM$ at $p \mapsto (T_p M)^+$ , in the codimension 1 case, I think the differential + a choice of normal vector field gives you a bilinear form on $T_pM$ automatically. Intuitively I would expect this is something like the second fundamental form of the manifold , but again, when trying to work it out it becomes a mess. Any ideas? Thanks! P.S. If someone knows a universal property the grassmanians satisfy, Iâ€™m also very interested!","['riemannian-geometry', 'curvature', 'grassmannian', 'tangent-bundle', 'differential-geometry']"
3758996,Is there a formula for $f(x)$ where $f(x)=$ the sum all simplest fractions with the numerator$+$denominator equals $x$,"Is there a closed-form for $$f(n)=\sum\limits_{\substack{k=1 \\ (k,n)=1}}^{n-1} \frac{k}{n-k}$$ For example, $f(5)=1/4+2/3+3/2+4/1= 6+5/12$ ; $f(6)=5+1/5$ The list of $f(x)$ from $x=1$ to $x=8$ is: $(0,1,5/2,10/3,77/12,26/5,223/20,988/105)$ I'm trying to plot this but I would take a while to do it by hand that's why I ask. If there isn't a nice formula could you write a program that plots it I'm not the best at coding.","['coprime', 'functions', 'closed-form']"
3759006,Clarification on definition of a Sheaf,"On Wikipedia, the gluing and locality properties of a Sheaf are defined in terms of elements $s$ of the object $S$ associated with $\mathscr{F}(U)$ . I have two points of confusion. I thought objects in a category don't necessarily have elements so does this definition even makes sense for categories outside of sets with structure? My second question is, assuming $S$ is a set. What is even met by the gluing compatibility conditions, $$res_{V \cap W }(s_i) = res_{V \cap W }(s_j) $$ For instance in the case of the skyscraper sheaf at a point $p$ , give an open covering of $U$ , $s_i$ may only even exist for the $U_i$ containing $p$ . From the definitions, it feels like you need to elements of a set to make the definition to make things work and implicitly a function associated with each element defined for every open subset of $U$ that maps to the empty set over subsets where an element disappears. My thinking must be horribly wrong here but I'm hoping someone can clarify these misconceptions.","['algebraic-geometry', 'category-theory', 'sheaf-theory']"
3759008,Proof for a combinatorial identity,"I have the following formula, which I believe it's true since it works in Mathematica for all values of $N$ I have tried, but I don't know how to prove it: $$\sum_{q=0}^{N} {N \choose q}^2 x^{q} =  \frac{1}{{2N \choose N}} \sum_{k,l=0}^N \; \sum_{s=0}^{\min(m, \ N-M)} \;  \sum_{t=0}^{\min(m, \, N-M)} \\ {N \choose M} {M \choose m-s} {N-M \choose s} {N \choose N-m} {N-m \choose N-M-t} {m \choose t} x^{M-m+s+t} $$ where $m=\min(k,l)$ and $M=\max(k,l)$ , and $x$ can be any complex number. I know one can write the LHS as a Legendre polynomial $ \sum_{q=0}^N { N \choose q }^2 x^q = (1-x)^N P_N \left( \frac{1+x}{1-x} \right)$ , and as a Hypergeometric function $ \sum_{q=0}^N { N \choose q }^2 x^q = \, _2F_1 (-N, -N, 1, x)$ , but apart from that I don't know how to simplify the RHS. I have tried Egorichev method to transform sums involving binomial coefficients into residual integrals, but didn't get much from there. Any ideas? Edit : I have found yet another way of writing the same quantity: $$\sum_{q=0}^{N} {N \choose q}^2 x^{q} = \\
= \frac{1}{ {2N \choose N} } \sum_{p,q=0}^N   \, \sum_{r=\max(0, \, q+p-N)}^{\min (q, \, p)} \, \sum_{s=\max (0, \, q-p)}^{\min (q, \, N-p)} {N \choose p} {N \choose N-p} {p \choose r} {N-p \choose s} {N-p \choose q-r} {p \choose q-s}  x^q $$ This one looks simpler than the previous one, since for instance here $x$ is decoupled from the sums in $s$ and $t$ . Again I have tried Egorychev method on the RHS, which allows you to write the sums in $s$ and $t$ as complex contour integrals, and then you can easily choose your limits in the sum to be whatever is more convenient so that you can actually compute the sums in $r$ and $s$ . But in exchange you now have four complex contour integrals (one for every summation limit you want to ""kill""), so I don't know if this is simpler. I suspect there must be a more general identity relating all three expressions them. Any suggestions?","['summation', 'legendre-polynomials', 'binomial-coefficients', 'combinatorics', 'hypergeometric-function']"
3759032,$\lfloor\frac12+\frac1{2^2}+\frac1{2^3}+\cdots\rfloor\;$ vs $\;\lim_{n\to\infty}\lfloor\frac12+\frac1{2^2}+\cdots+\frac1{2^n}\rfloor$,"Is there any difference between answers of $[1]$ and $[2]$ ? $$\Bigg\lfloor\frac12+\frac1{2^2}+\frac1{2^3}+\cdots\Bigg\rfloor \tag*{$\space.....[1]$}$$ $$
\lim _{n \rightarrow \infty} \Bigg\lfloor\frac{1}{2}+\frac{1}{2^{2}}+\frac{1}{2^{3}}+\cdots+\frac{1}{2^{n}}\Bigg\rfloor \tag*{$ \space.....[2] $}$$ If yes then please do explain that why I canâ€™t write $[2]$ as $[1]$ even if $n$ tends to $\infty$ in $[2]$ (Notice the use of the 'floor' function indicated by the type of brackets.) NOTE- PLEASE donâ€™t unnecessarily edit $[1]$ and $[2]$ . It is exactly  as it should be.","['limits', 'calculus', 'ceiling-and-floor-functions', 'sequences-and-series']"
3759045,Prove that the identity matrix is the only matrix such that $IA = A$ for all $A.$,How do I prove that $I$ is the only matrix such that $IA = A$ for all $A?$ I keep getting tripped up on the index notation. Thanks!,"['matrices', 'proof-writing']"
3759052,"Prove that, for any sets $A$ and $B$, $\mathscr P(A)\cup\mathscr P(B)\subseteq \mathscr P(A\cup B)$.","Not a duplicate of How to prove $P(A) \cup P(B) \subseteq P(A \cup B) $ . This is the exercise $3.5.7$ from the book How to Prove it , by Velleman ( $2^{\text{nd}}$ edition). Prove that, for any sets $A$ and $B$ , $\mathscr P(A)\cup\mathscr P(B)\subseteq \mathscr P(A\cup B)$ . Here is my proof: Let $X$ be an arbitrary element of $\mathscr P(A)\cup\mathscr P(B)$ . This means $X\in\mathscr P(A)$ or $X\in \mathscr P(B)$ . Let $x$ be an arbitrary element of $X$ . Now we consider two different cases. Case $1.$ Suppose $X\in\mathscr P(A)$ . So $X\subseteq A$ and since $x\in X$ , $x\in A$ . Thus $x\in A\cup B$ . Case $2.$ Suppose $X\in\mathscr P(B)$ . So $X\subseteq B$ and since $x\in X$ , $x\in B$ . Thus $x\in A\cup B$ . Since the above cases are exhaustive, $x\in A\cup B$ . Thus if $x\in X$ then $x\in A\cup B$ . since $x$ is arbitrary, $\forall x(x\in X\rightarrow x\in A\cup B)$ and so $X\subseteq A\cup B$ . Ergo $X\in\mathscr P(A\cup B)$ . Therefore if $X\in \mathscr P(A)\cup\mathscr P(B)$ then $X\in \mathscr P(A\cup B)$ . Since $X$ is arbitrary, $\forall X\Bigr(X\in\mathscr P(A)\cup\mathscr P(B)\rightarrow X\in \mathscr P(A\cup B)\Bigr)$ and so $\mathscr P(A)\cup\mathscr P(B)\subseteq \mathscr P(A\cup B)$ . $Q.E.D.$ Is my proof valid $?$ Thanks for your attention.","['elementary-set-theory', 'proof-writing', 'solution-verification']"
3759057,Finding the absolute extrema of $F(x) = 2x + 5\cos(x)$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Find the absolute extrema of $F(x) = 2x + 5\cos(x)$ on the interval $[0,2\pi]$ using the extreme value theorem. Answer should be 2 ordered pairs. I got $\arcsin(2/5)$ for the first value of $x$ , but canâ€™t figure out the second. Thanks in advance.","['trigonometry', 'extreme-value-theorem', 'calculus', 'algebra-precalculus']"
3759085,Does this method work for reflecting over $x^2$?,"I am investigating reflecting over any quadratic. In the graph, I have the simplest scenario (reflecting $y=0$ over $y=x^2$ ). My method was to use the tangent (red dotted line) find the intersect that the tangent makes with $y=0$ and calculate that distance ( $d=\sqrt{x^2+y^2}$ ). After this I found the line perpendicular to the tangent and found point $P'$ by going $d$ down this line. Is this an okay method to use? Also, can a general equation be derived so one doesn't have to manually do all the steps every time?","['reflection', 'algebra-precalculus', 'derivatives']"
3759112,Number of possible configurations by shifting the '2's in $12121212$ to the right.,"I came up with this question while solving another combinatorics problem. Let's say there is a number $12121212$ . Define an operation as swapping any two adjacent digits if the left digit is $2$ . (For example, swapping the $2$ $nd$ and $3$ $rd$ digit to give $11221212$ as a result, but swapping the $3$ $rd$ and $4$ $th$ digit is not allowed.) There is no limit on how many operations you can do on the number(no operation is also possible). How many possible numbers can be formed? Questions Is there a name to this kind of problems? How can it be solved? Extra: What if the original is not $12121212$ but some other numbers(like for example $121212121111111111$ ? Will this make the question very complicated? My attempt I am not sure how to approach this question. My observation is that the final configuration $11112222$ remains unchanged after any operations. So it seems that the first ' $2$ ' originally at $2$ $nd$ position moves to the $5$ $th$ position, the first ' $2$ ' orginally at $4$ $th$ position moves to the $6$ $th$ position and so on. However, some of the cases are invalid but at least I know that number of possible configurations is less than $4\cdot3\cdot2\cdot1 = 24$ . So a possible way will be to enumerate all possible configurations, but it is a pain because I cannot find a way to do it in an organised manner. Therefore, I am curious if there is a way to do it more efficiently and smartly.","['combinations', 'combinatorics']"
3759138,Binomial Expansion Of $\frac{24}{(x-4)(x+3)}$,"Can somebody help me expand $\frac{24}{(x-4)(x+3)}$ by splitting it in partial fractions first and then using the general binomial theorem?
This is what I've done so far: $$\frac{24}{(x-4)(x+3)}$$ $$=\frac{24}{7(x-4)}-\frac{24}{7(x+3)}$$ Now I know I have to find the binomial expansion for this; I just don't know how. Can anyone help me with this?","['algebra-precalculus', 'binomial-theorem', 'partial-fractions']"
3759147,Continuous vector field $F$ with $F(\vec x) - F(\vec y)$ parallel to $\vec x - \vec y$,"In Goldstein's classical mechanics, he makes an interesting claim, that if there is a continuous vector field $F$ where $F(\vec x) - F(\vec y)$ is parallel to $\vec x - \vec y$ , then $F$ must be a constant field. We can attempt a proof by contradiction. If such a non-constant field exists, it's clear that we can first choose some point $\vec x$ and decompose our vector field, into a component $F_{\vec x}^1$ that always points towards (or away from) $\vec x$ and another vector field $F^2_{\vec x}$ that is constant and equal to $F(\vec x)$ . We can then repeat the construction with some other point $\vec y$ . I've used most of the information from the problem hypothesis, except continuity, and I'm not so sure  how continuity and the above paragraph will yield a contradiction. Goldstein's claim is from his chapter on rigid body motion, in a discussion on angular velocity. The claim appears just before equation $5.1$ of the third edition.","['vector-fields', 'multivariable-calculus', 'classical-mechanics']"
3759148,How to prove a norm identity for a Banach space and its dual,"Is the following claim true? It feels like it should be true, but I don't really know how to show it. Let $X$ be a Banach space, and $x \in X$ an element of it. Then there exists a functional $\phi \in X^*$ such that $\| \phi \| = 1$ and $\| x \| = | \phi(x) |$ . If I'm not mistaken, it would suffice to say that there exists a sequence $(\phi_k)_{k = 1}^\infty$ of unit functionals for which $| \phi_k (x) | \to \| x \|$ , since the unit ball in $X^*$ is compact in the weak topology. However, I don't know how to prove the former result. EDIT: I forgot to actually define $\psi$ as $\psi(\lambda x) = \lambda$ . My intuition is that I should be able to invoke Hahn-Banach and define a linear function $\psi$ on $\mathbb{C} x \subseteq X$ bounded by the norm $\rho(x) = \| x \|$ on $X$ , then extend it from $\mathbb{C} x$ to all of $X$ . Is this a correct application of Hahn-Banach?","['banach-spaces', 'solution-verification', 'functional-analysis', 'hahn-banach-theorem']"
3759158,Let $G$ be a group with a free subgroup of rank $2$. Let $H\leq G$ be such that $[G:H]<\infty$. Then $H$ also contains a free subgroup of rank $2$.,I am having difficulties in solving the following problem. Let $G$ be a group with a free subgroup of rank $2$ . Let $H\leq G$ be such that $[G:H]<\infty$ . Then $H$ also contains a free subgroup of rank $2$ . We know by Nielsen-Schreier theorem that a subgroup of a free group is also free. But in this problem $G$ is not necessarily free but contains a free subgroup. How to approach this problem? Any hint or idea will be highly appreciated. Thanks in anticipation.,"['combinatorial-group-theory', 'abstract-algebra', 'free-groups', 'geometric-group-theory', 'group-theory']"
3759232,Why is $\text{Gal}(K/\mathbb{Q}) \cong G_{\mathbb{Q}}/{\{\sigma \in G_{\mathbb{Q}}: \ \sigma|_K=id_K \}}$?,"Here , in page $1$ , the absolute Galois group is defined by $$G_{\mathbb{Q}}:=\text{Gal}(\bar{\mathbb{Q}}/\mathbb{Q})=\{\sigma: \bar{\mathbb{Q} }\to \bar{\mathbb{Q}}, \ \text{field automorphism} \}$$ is a profinite group. Then the article defines for any Galois extension $K$ of $\mathbb{Q}$ , the Galois group by $$\text{Gal}(K/\mathbb{Q}) \cong G_{\mathbb{Q}}/{\{\sigma \in G_{\mathbb{Q}}: \ \sigma|_K=id_K \}}$$ to be the quotient group. My question- Why is $\text{Gal}(K/\mathbb{Q}) \cong G_{\mathbb{Q}}/{\{\sigma \in G_{\mathbb{Q}}: \ \sigma|_K=id_K \}}$ ? Because we know by definition of Galois extension $\text{Gal}(K/\mathbb{Q}) = \{\sigma \in \text{Aut}(K): \ \sigma(a)=a, \ \forall a \in \mathbb{Q} \}$ . So the question- How to see the relation $ \{\sigma \in \text{Aut}(K): \ \sigma(a)=a, \ \forall a \in \mathbb{Q} \} \cong G_{\mathbb{Q}}/{\{\sigma \in G_{\mathbb{Q}}: \ \sigma|_K=id_K \}}$ ? How to see the isomorphism ?","['galois-theory', 'number-theory', 'galois-extensions']"
3759233,How can I understand a hypocycloid as an ideal in a polynomial ring?,"Today I was reading On teaching mathematics by V. I. Arnolâ€™d and came across the following quote. ""Rephrasing the famous words on the electron and atom, it can be said that a hypocycloid is as inexhaustible as an ideal in a polynomial ring. But teaching ideals to students who have never seen a hypocycloid is as ridiculous as teaching addition of fractions to children who have never cut (at least mentally) a cake or an apple into equal parts. No wonder that the children will prefer to add a numerator to a numerator and a denominator to a denominator."" I know hypocycloids as a fascinating geometric object and it is not hard to connect this to group theory and number theory via roots of unity. Also they are related with special unitary group and possibly related with octonions. But never thought and never seen how they related to ideals of a polynomial ring, though this might be obvious to someone who see the picture from the correct angle. Can somebody explain me this connection? And, what is the analogous picture for Epicycloids and ordinary Cycloids? (I am not sure about the suitable tags for this question. You are welcome to edit them as necessary.)","['affine-varieties', 'algebraic-geometry', 'polynomial-rings', 'ideals', 'cycloid']"
3759255,Solving $\int_0^{\infty} {1 \over x^4-81}\ dx$?,"solve the following $$\int_0^{\infty} {dx \over x^4-81}?$$ my approach: $$\int {dx \over x^4-81}=\int {dx \over (x^2+9)(x^2-9)}$$ used partial fractions ${1\over18}\int ({1 \over x^2-9}-{1 \over x^2+9})dx={1\over 18}\int {dx \over x^2-9}-{1\over 18}\int {dx \over x^2+9}$ for first part, used partial fractions $${1\over 18}\int_0^{\infty} {1 \over x^2-9}={1\over 108}\int_0^{\infty} ({1 \over x-3}-{1 \over x+3})dx$$ $$={1\over 108}\lim_{t\to \infty}\int_0^{t} ({1 \over x-3}-{1 \over x+3})dx$$ $$={1\over 108}\lim_{t\to \infty}(\ln|x-3|-ln|x+3|)_0^{t}$$ $$={1\over 108}\lim_{t\to \infty}(\ln|{t-3\over t+3}|)$$ $$={1\over 108}\lim_{t\to \infty}(\ln|{1-3/t\over 1+3/t}|)={1\over 108}\cdot 0=0$$ for second part, I substituted $x=3\tan t$ , $dx=3\sec^2 t\ dt$ $${1\over 18}\int_0^{\infty} {dx \over x^2+9}={1\over 18}\int_0^{\pi/2} {3\sec^2t\ dt \over 9\sec^2t}={1\over 54}\int_0^{\pi/2}dt={\pi\over 108} $$ adding two values, my answer becomes $0+({-\pi\over 108})=-{\pi\over 108} $ Can I do this by other way which is simpler than my method? thank you","['integration', 'calculus', 'definite-integrals']"
3759260,Inverse function in $\mathbb R^2$ [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 3 years ago . Improve this question How do I find the inverse function of $f: X\to Y$ where $X,Y$ both are subsets of $\mathbb R^2$ and $f$ is defined as $f(x,y)=(x+y,x-y)$ .","['functions', 'inverse-function']"
3759283,Properties for analogues of Bessel function,"Consider the unit circle $\mathbb{S}^1 \subset \mathbb{R}^2$ and consider the uniform measure $\nu$ on $\mathbb{S}^1$ normalised so that $\nu (\mathbb{S}^1)=1$ . The collection of functions $\{1,z,\overline{z},z^2,\overline{z^2}\ldots\}$ forms an orthonormal basis for $L^2(\mathbb{S}^1,\nu)$ . For each $n\geq 0$ , we can think of the Bessel function $J_n$ as the Fourier transform of $z^n$ , that is for $w \in \mathbb{R}^2$ $$
J_n(w) = \int_{\mathbb{S}^1} e^{-i\langle w,(\cos \theta, \sin \theta) \rangle} e^{in\theta} d\nu(\theta).
$$ Note that by symmetry, $J_n(w) = J_n(\|w\|)$ and we can think of $J_n$ as just a real valued function $J_n :\mathbb{R} \to \mathbb{R}$ . The Poisson representation formula for $J_n$ is known and it is as follows $$
	J_{n}(r) = \frac{(r/2)^n}{\Gamma(n+\frac{1}{2}) \sqrt{\pi}} \int_{-1}^{1} e^{irt} (1-t^2)^{n-\frac{1}{2}}~ dt. 
$$ It is easy to see the vanishing property of $J_n$ near the origin from the above formula, that is we can conclude $$
|J_{n}(r)| \leq 10 \cdot \frac{(r/2)^n}{\sqrt{2\pi} \left(n/e\right)^n \sqrt{\pi}} \leq \left( \frac{2r}{n}\right)^n.
$$ My question is: Suppose we have an arbitrary probability measure $\mu$ on $\mathbb{S}^1$ (which has no atoms) and let $\{f_1,f_2,\ldots\}$ be an orthonormal basis for $L^2(\mathbb{S}^1,\mu)$ . Is it possible to conclude similar vanishing properties near the origin for the Fourier transform of $f_n$ ? Thanks!","['fourier-transform', 'orthogonal-polynomials', 'measure-theory', 'functional-analysis']"
3759297,Derivative of $L^\infty$ norm of a function,"Let $f: \mathbb R_+ \times \mathbb R \to \mathbb R$ . Under which assumptions does it make sense to compute $\frac{d}{dt}\Vert f(t,\cdot) \Vert_{L^\infty(\mathbb R)}$ and what is it?","['calculus', 'functional-analysis', 'ordinary-differential-equations', 'real-analysis']"
3759337,Find $\sum_{n=1}^{\infty} \frac{1}{\prod_{i=0}^{k} \left(n+i\right)}$,Original question is $$\sum_{n=1}^{\infty} \frac{1}{\prod_{i=0}^{k} \left(n+i\right)}$$ I got it down to $$\sum_{n=1}^{\infty} \frac{(n-1)!}{(k+n)!}$$ Here I am confused.  Possible fraction decomposition but its ugly!  Maybe this approach is not good?  Ideas? Answer is $$\frac{1}{k \cdot k!}$$ I want to know how to proceed with my work though,"['calculus', 'summation', 'sequences-and-series']"
3759344,The quotient scheme $X/\Gamma$ when $X$ is separated and every orbit is contained in an affine.,"I am trying to solve Problem II.4.7(a) of Hartshorne: The only candidate I can think of for $X_0$ would be the quotient scheme $X/\sigma$ . If it exists, it must be unique by the usual argument. First starting with the affine case, taking $A=\mathbb C[x_1,...,x_n]/I$ to be a finitely generated $\mathbb C$ -algebra, then the associated ring morphism that commutes with conjugation would leave us with the invariant algebra being $A^\sigma=\mathbb R[x_1,...,x_n]/\bar I $ where $\bar I$ is the real part of $I$ . Then here it follows that $\text{Spec}A^\sigma\times_\mathbb{R}\mathbb C=\text{Spec}A$ by just tensoring $A^\sigma\otimes_\mathbb{R}\mathbb C=A$ . For the general case, I found a helpful clue from Bosch's AG book.  Exercise 7.1.8 of Bosch says: Let $X$ be a scheme and $\Gamma$ a finite group of automorphisms. The quotient $X/\Gamma$ exists if there is a $\Gamma$ -invariant affine open cover of $X$ . Further,  if $X$ is separated, then the quotient $X/\Gamma$ exists if all the points in any $\Gamma$ -orbit are contained in an open affine. Since by assumption we have that any two points are in an open affine and our orbit has at most two points, this exercise will give us the existence of the quotient. But I want to prove it first. My idea is to first construct the scheme to be locally the invariant rings. That is, if $X=\bigcup_{i=1}^n\text{Spec}A_i$ , take $X/ \Gamma := \bigcup_{i=1}^n \text{Spec}A_i^{\Gamma}$ as a set. Since $X$ is separated, the intersection of any two affines is affine, and define $\text{Spec}A_{ij}:=\text{Spec}(A_i^\Gamma\otimes_\mathbb{C} A_j^\Gamma)=\text{Spec}A_i^{\Gamma}\cap \text{Spec}A_j^{\Gamma}$ , which I naively want to use to glue together. However, the cocycle condition is not necessarily satisfied since we don't seem to have an isomorphism here: $\text{Spec}A_{ij}\cap \text{Spec}A_{ik}=\text{Spec}(A_{ij}\otimes_\mathbb{C} A_{ik})$ , but $\text{Spec}A_{ji}\cap \text{Spec}A_{jk}=\text{Spec}(A_{ij}\otimes_\mathbb{C} A_{jk})$ , and $A_{ij}\neq A_{jk}$ , and from here I'm lost. A second idea is the following: let $U$ be an affine open, and since $\gamma\in\Gamma$ is an automorphism, then $\gamma(U)$ is affine. Then, since intersections of finitely many affines is affine in a separated scheme, we have that $\bigcap_{\gamma\in\Gamma}\gamma(U)$ is nonempty, affine, and $\Gamma$ -invariant. Since every orbit lies in some affine, then we have that open sets of this form actually form an open cover of $X$ . So this proves that the second part of the exercise, once we show the first part.","['algebraic-geometry', 'schemes', 'real-algebraic-geometry']"
3759403,sums and differences of perfect powers,"We have $1=3^2-2^3$ $2=3^3-5^2$ $3=2^7-5^3$ $4=5^3-11^2$ $5=3^2-2^2$ and it is unknown if $6$ is representable as a difference of two perfect powers. Next such undecided example is $14$ . More: http://oeis.org/A074981 However, I found that $6=64-49-9=2^6-7^2-3^2$ and $6=27+4-25=3^3+2^2-5^2$ Similarly $14=27-9-4=3^3-3^2-2^2$ and $14=9+9-4=3^2+3^2-2^2$ My question: Is every positive integer representable in the form: $a_1^{n_1}+a_2^{n_2}-a_3^{n_3}$ or/and in the form $a_1^{n_1}-a_2^{n_2}-a_3^{n_3}$ where $a_1,a_2,a_3,n_1,n_2,n_3$ are natural numbers greater than $1$ with $a_2=0$ also acceptable ? Are these things known? The question is based on my own investigation.","['number-theory', 'recreational-mathematics', 'natural-numbers', 'elementary-number-theory']"
3759442,Intuition for $\tan\theta=\frac{\sin\theta}{\cos\theta}$,"I understand how to prove that using the definitions of the elementary trig functions: $$\frac{\sin\theta}{\cos\theta} = \frac{\dfrac{opp}{hyp}}{\dfrac{adj}{hyp}} = \frac{opp}{adj} = \tan\theta$$ However, I have a hard time visualizing that identity. Is there a way to do so?","['trigonometry', 'intuition']"
3759453,"Bayesian statistics notation: ""$P(\text{event}|x)$"" vs ""$P(\text{event}|\theta, x)$""","Now I am  new to the subject so, having some rotational problems. My issues are - What is the meaning of $P(\text{good bus tomorrow}|x) $ and $P(\text{good bus tomorrow}|\theta, x)$ or what are the differences between these two? Why $ P(\text{good bus tomorrow}|\theta, x)= \theta$ ? Is it because, actually $ P(\text{good bus tomorrow}|\theta, x)= p(\theta)$ but in this case,  the probability of $\theta $ is $\theta$ , i.e. $p(\theta)=\theta$ ? Let me clarify what I am talking about. The problem on page $22$ of the text of Introduction to Bayesian Statistics by Brendon J. Brewer is written as following  - After moving to Auckland, I decided that I would take the bus to work
each day. However, I wasnâ€™t very confident with the bus system in my
new city, so for the first week I just took the first bus that came
along and was heading in the right direction, towards the city. In the
first week, I caught 5 morning buses. Of these 5 buses, two of them
took me to the right place, while three of them took me far from work,
leaving me with an extra 20 minute walk. Given this information, I
would like to try to infer the proportion of the buses that are ""good"", that would take me right to campus. Let us call this fraction $\theta$ and we will infer $\theta$ using the Bayesian framework. Here, $\theta =2/5.$ For example, look the following image - Recall that, if there are $N$ repetitions of a ""random experiment"" and the ""success""
probability is $\theta$ at each repetition, then the number of ""successes"" $x$ . To get the likelihoods, we need to think about the properties of our experiment. In particular, we should imagine that we knew the value of $\theta$ and were trying to predict what experimental outcome (data) would occur. Ultimately, we want to find the probability of our actual data set (2 out of the 5 buses were ""good""), for all of our possible $\theta$ values. $P(\theta|x)$ is the posterior probability. It describes $\textbf{how certain or confident we
are that hypothesis $\theta$ is true, given that}$ we have observed data $x$ . Calculating posterior probabilities is the main goal of Bayesian statistics! $P(\theta)$ is the prior probability, which describes $\textbf{how sure we were that}$ $\theta$ was true,
before we observed the data $x$ . $P(x|\theta)$ is the likelihood. $\textbf{If you were to assume that $\theta$ is true, this is the
probability}$ that you would have observed data $x$ . $P(x)$ is the marginal likelihood. This is the probability that you would have observed data $x$ , whether $\theta$ is true or not. So, $P (\theta|x) = \frac{P (\theta) P(x|\theta)}{P (x)}$ The following part is an excerpt from the same text - In the Bayesian framework, our predictions are always in the form of
probabilities or (later) probability distributions. They are usually
calculated in three stages. First, you pretend you actually know the true value of the parameters,
and calculate the probability based on that assumption. Then, you do this for all possible values of the parameter $\theta$ (alternatively, you can calculate the probability as a function of $\theta$ ). Finally, you combine all of these probabilities in a particular way to
get one final probability which tells you how confident you are of
your prediction. Suppose we knew the true value of $\theta$ was $0.3$ . Then, we would
know the probability of catching the right bus tomorrow is $0.3$ . If
we knew the true value of $\theta$ was $0.4$ , we would say the
probability of catching the right bus tomorrow is 0.4. The problem is, we donâ€™t know what the true value is. We only have the
posterior distribution. Luckily, the sum rule of probability (combined
with the product rule) can help us out. We are interested in whether I will get the good bus tomorrow. There
are $11$ different ways that can happen. Either $\theta=0$ and I get
the good bus, or $\theta=0.1$ and I get the good bus, or $\theta=0.2$ and I get the good bus, and so on. These 11 ways are all mutually
exclusive. That is, only one of them can be true (since $\theta$ is
actually just a single number). Mathematically, we can obtain the posterior probability of catching
the good bus tomorrow using the sum rule: $$P(\text{good bus tomorrow}|x) = \sum_{\theta} p(\theta|x) \times P(\text{good bus tomorrow}|\theta, x) $$ $$=  \sum_{\theta} p(\theta|x) \times \theta$$ This says that the total probability for a good bus tomorrow (given
the data, i.e. using the posterior distribution and not the prior
distribution) is given by going through each possible $\theta$ value, working out the probability assuming the $\theta$ value you are considering is true, multiplying by the probability (given the data)
this $\theta$ value is actually true, and summing. In this particular problem, because $P\text{(good bus  tomorrow}|\theta, x) = Î¸$ , it just so happens that the probability for
tomorrow is the expectation value of $\theta$ using the posterior
distribution. To three decimal places, the result for the probability tomorrow is $0.429$ . Interestingly, this is not equal to $2/5 = 0.4$ .","['statistics', 'statistical-inference', 'proof-explanation', 'bayesian', 'notation']"
3759464,Is it true that $\operatorname{meas}(\partial(\operatorname{supp}(f)))=0$?,"Let $\Omega$ be a open subset of $\mathbb{R}^{d}$ and $C_c^{m}(\Omega)$ the space of $m$ -times continuously differentiable functions with compact support with $0 \leq m \leq \infty$ . Denote by $\partial X$ the boundary of the set $X$ and  by $\operatorname{meas}(X)$ the measure of the set $X$ . My question: Is it true that $\operatorname{meas}(\partial(\operatorname{supp}(f)))=0$ for all $f \in C_c^{m}(\Omega)$ ? Intuitively it seems true. For example, if $n=1$ then $\partial(\operatorname{supp}(f))$ has two points. If $n=2$ , then $\partial(\operatorname{supp}(f))$ looks like a deformed circumference. But I do not know how to give an analytical proof of this fact. Observations: The support of $f$ is the set $\operatorname{supp}(f)=\overline{\{x \in \Omega:f(x) \neq 0\}}^{\Omega}$ . Here $\operatorname{meas} (X)=0$ can be the Lebesgue measure or prove that for all $\varepsilon>0$ there is a sequence of $d$ -dimensional cubes $C_1, C_2, \dots, C_i, \dots$ such that $\operatorname{supp}(f) \subset \bigcup_{i=1}^{\infty} C_i$ and $\sum_{i=1}^{\infty} \operatorname{vol} C_i<\varepsilon.$","['measure-theory', 'smooth-functions', 'real-analysis']"
3759467,Combinatorics With Relations,"The twelvefold way offers a framework for counting functions, under various conditions which can be expressed as n-fold Cartesian Products of the function's domain, function, and codomain attributes.  Using this twelvefold way table (which actually has $16$ entries) as an example, we could structure the various counting problems as: $\{$ domain elements are distinguishable, domain elements are indistinguishable $\} \times \{$ function is left unique, function is not left unique $\} \times \{$ function is right total, function is not right total $\} \times \{$ codomain elements are distinguishable, codomain elements are indistinguishable $\}$ The bijective cases are sometimes dropped, yielding the number $12$ , but we'll keep them. Is it possible to relax the conditions that make the function a function, namely right uniqueness and left totality, and count general relations?  The new structure of counting problems would be: $\{$ domain elements are distinguishable, domain elements are indistinguishable $\} \times \{$ relation is right unique, relation is not right unique $\} \times \{$ relation is left total, relation is not left total $\} \times \{$ relation is left unique, relation is not left unique $\} \times \{$ relation is right total, relation is not right total $\} \times \{$ codomain elements are distinguishable, codomain elements are indistinguishable $\}$ Are there attempts to collect formulas for and study these new cases where the relation may not be a function?  If so, what is known about them?  If not, is it because it has been shown that such cases have no applications?","['number-theory', 'relations', 'functions', 'combinatorics', 'discrete-mathematics']"
3759499,Brezis-Kato regularity argument - Some questions about Struwe's proof,"The following is in Appendix B of Struwe's Variational Methods Let $u$ be a solution of $-\Delta u = g(x, u(x))$ in a domain $\Omega \subset \mathbb R^N$ , $N \geq 3$ , where $g$ is a CarathÃ©odory function with subcritical superlinear growth. Theorem :
Let $\Omega \subset \mathbb R^N$ be a smooth open set  and let $g: \Omega \times \mathbb R \to \mathbb R$ be a CarathÃ©odory function such that $$
|g(x, u(x))| \leq a(x)(1 + |u(x)|) \quad \text{ a.e. in } \Omega
$$ for some $0 \leq a \in L_{loc}^{N/2}(\Omega)$ . Let $u \in H^1_{loc}(\Omega)$ be a weak solution to $-\Delta u = g(x, u)$ . Then $u \in L^q_{loc}(\Omega)$ for all $1 < q < \infty$ . If $u \in H_0^1(\Omega)$ and $a \in L^{N/2}(\Omega)$ , then $u \in L^q(\Omega)$ for all $1 < q < \infty$ . The proof begins as follows: Take $\eta \in C_c^\infty(\Omega)$ , $s \geq 0$ and $L \geq 0$ and let $$
\varphi = u \min \{|u|^{2s}, L^2\} \eta^2 \in H_0^1(\Omega)
$$ Testing the equation against $\varphi$ yields $$
\int_\Omega |\nabla u|^2 \min\{|u|^{2s}, L^2\} \eta^2 \ dx + \frac s2 \int_{\{|u|^s\leq L \}} |\nabla(|u|^2)|^2 |u|^{2s - 2} \eta ^2 \ dx \leq \\
-2 \int_\Omega \nabla u u \min \{|u|^{2s}, L^2\} \nabla \eta \eta \ dx + \int_\Omega a(1 + 2|u|^2)\min \{|u|^{2s}, L^2\}\eta^2 \ dx. 
$$ Why is $\varphi \in H_0^1(\Omega)$ ? How do the second term in the left-hand side of the inequality arises? I tried the following: We want to compute $$
\int_{\{|u|^s \leq L\}} \nabla u u \nabla |u|^{2s} \eta^2 \ dx .
$$ But $$
\nabla |u|^{2s} = \nabla(u^+ - u^-)^{2s} = 2s |u|^{2s - 1} \nabla |u|
$$ so we get $$
\int_{\{|u|^s \leq L\}} \nabla u u \nabla |u|^{2s} \eta^2 \ dx =
2s \int_{\{|u|^s \leq L\}} (\nabla u \nabla |u|) u |u|^{2s - 1} \eta ^2 \ dx 
$$ On the other hand, $$
\frac s2 \int_{\{|u|^s \leq L\}} |\nabla |u|^2|^2 |u|^{2s - 2} \eta^2 \ dx = \frac s2 \int_{\{|u|^s \leq L\}} |2 |u| \nabla |u||^2 |u|^{2s - 2} \eta^2 \ dx \\
= 2s \int_{\{|u|^s \leq L\}} |\nabla |u||^2 |u|^{2s} \eta ^2 \ dx.
$$ How to conclude that these two expressions are the same? Also, what is the intuition for the proof of this theorem? It is seeming like just a lot of calculations. Thanks in advance and kind regards.","['regularity-theory-of-pdes', 'inequality', 'functional-analysis', 'partial-differential-equations']"
