question_id,title,body,tags
1716259,How many students are in the class if every student is represented in the survey?,"A Java Class had $34$ students who passed calculus, $26$ students who passed statistics and $19$ students who passed both calculus and statistics. How many students are in the class if every student is represented in the survey? I have answered this and I was just wondering if my answer is correct. Let $C$ be the set of students who passed calculus. Let $S$ be the set of students who passed statistics. Let $C \cap S$ be the set of students who passed calculus and statistics. Let $x$ be the number of students in class. \begin{align*}
|x| & =|C|+|S|−|C \cap S|\\
    & = 34+26-19\\
    & = 41
\end{align*}
Also, am I using the variables correctly? What would you change?","['combinatorics', 'discrete-mathematics']"
1716279,What is an upper bound for the number of sets in a $\sigma$ - algebra generated by $n$ sets?,"I'm working through G. de Barra's book on measure theory, and have come across this question. The only information the book gives on generated $\sigma$-algebras is: Let $\mathcal{A}$ be a class of subsets of a space $X$. We call the smallest $\sigma$-algebra containing $\mathcal{A}$ the $\sigma$-algebra generated by $\mathcal{A}$. So far so good. Now to the question. Let $\mathcal{A}=\{A_1,\dots,A_n\}$ be our subclass of $n$ sets. Now I know, from other questions on the site, that the next step is to consider the set $\mathcal{K}$ containing elements of the form: $$A_{1}^{\gamma_1}\cap\dots\cap A_{n}^{\gamma_n}$$ 
where $\gamma_i\in \{0,1\}$ to be interpreted that $A_i^1=A_i$ and $A_i^0=A_i^c$. We then take all possible unions of the elements of $\mathcal{K}$ to generate the $\sigma$-algebra generated by $\mathcal{A}$. This gives an upper bound of $2^{2^{n}}$ distinct sets. What I don't quite get is the motivation for taking the elements of $\mathcal{K}$ as the ""atoms"" of the correct $\sigma$-algebra. Intuitively I sense that this does ensure that the gnerated sub-class is closed under complements and countable unions but I'd like a more rigorous reason as to why we first construct $\mathcal{K}$ and then look at its unions. All the answers I've seen on the site treat this step as trivial, but it did not seem to me the obvious place to start. Maybe there's a well-known proof showing that this method generates a $\sigma$-algebra containing $\mathcal{A}$ that I'm not aware of? EDIT: To be clear I'm asking why do other posts choose the set of all possible intersections and complements as their atoms. Is there a pretty way to see that this works, or is it just used because it can be proven (laboriously) that using them as your atoms will lead to the desired $\sigma$-algebra. I am not asking how they get to the upper bound after that step, as that is quite simple.","['real-analysis', 'algebras', 'measure-theory']"
1716289,Does $\frac{1} { ax+b}$ apply when $b$ equals $0$,"This is a dumb question but I'd still like to confirm it here, I just haven't found any information about this in internet. According to any table of integrals $\int \frac{1}{ax+b} \, dx = \frac{1}{a} \ln(ax+b)$ . This doesn't work if if $b=0$, right? Because $\int\frac{1}{ax} \, dx$ should equal to $\frac{1}{a} \int \frac{1}{x} \, dx$ which would be $\frac{1}{a} \ln(x)$.",['integration']
1716298,Proof for convergence in distribution implying convergence in probability for constants,"I'm trying to understand this proof (also in the image below) that proves if $X_{n}$ converges to some constant $c$ in distribution, then this implies it converges in probability too. Specifically, my questions about the proof are: How are they getting $\lim_{n \to \infty} F_{X_{n}}(c+\frac{\epsilon}{2}) = 1$ ? Why do they state the conclusion at the end in this way? They're basically saying that knowing $\lim_{n \to \infty}P(|X_{n} - c| \geq \epsilon) \geq 0$ allow you to conclude that $\lim_{n \to \infty}P(|X_{n} - c| \geq \epsilon) = 0$ but the real reason we can conclude this is because of the whole body of the proof above, right?","['weak-convergence', 'probability']"
1716311,solving linear gradient PDE,"Let $f:\mathbb R^n \to \mathbb R$ be a function that satisfies the equation: $$\nabla^T f = - (\nabla^T \phi ) f $$
where $\phi:\mathbb R^n \to \mathbb R$ is some given function and $\nabla^T$ is the column partial derivatives operator. Then obviousely $f(x)=k e^{-\phi(x)}$ solves the equation. Assume now that we have the equation:
$$Q \nabla^T f = - (\nabla^T \phi ) f $$
for some symmetric positive definite matrix $Q$.
Can it be solved similarly?","['multivariable-calculus', 'vector-analysis', 'partial-differential-equations']"
1716326,Power series solution for ODE,"The ODE I have is $$y'(x)+e^{y(x)}+\frac{e^x-e^{-x}}{4}=0, \hspace{0.2cm} y(0)=0$$
I want to determine the first five terms (coefficients $a_0,\ldots, a_5$) of the power series solution $$y(x)=\sum_{k=0}^{\infty} a_kx^k$$ So far, I know that $$y'(x)=\sum_{k=1}^{\infty} a_kkx^{k-1}$$
Now I plug these back into the equation and get:
$$\sum_{k=1}^{\infty} a_kkx^{k-1} + e^{\sum_{k=0}^{\infty} a_kx^k} + \frac{e^x-e^{-x}}{4}=0$$. Now I'm not sure how to continue with this. Please help.","['power-series', 'ordinary-differential-equations', 'sequences-and-series', 'calculus']"
1716369,Area covered by Moving Circle?,"Consider a situation where we have a point (x,y) moving on a 2-D plane. In fact, the point is function of time x=f(t),y=g(t). Centered around (x,y) is a circle of radius r? Obviously, we can visualize a circle moving in the 2-D plane. Compute the area covered by the circle from the the start of motion. Consider , f=t,g=sin(t) for non-negative t. Please consider the case in case the area covered overlaps.","['area', 'geometry']"
1716392,Nuclear operators,"While studying for my thesis (in dynamical systems) I've encountered multiple times with the concept of nuclear operators and nuclear spaces, often linked with the works of Grothendieck. For example, when studying the generalized transfer operator (or Ruelle operator) for the Gauss Map, Dieter Mayer points out that this operator is in fact nuclear (On the thermodynamic formalism for the Gauss map). While I can understand the definition of a nuclear operator, I still cannot get the real importance of being nuclear of order zero. Usually I'm interested in spectral gap properties for transfer operators, but is there any implication of the nuclear property?
Also, any reference for nuclear operators and Fredholm kernels would be appreciated, since trying to learn directly from Grothendieck's works has been really difficult for me.
Thanks in advance.","['functional-analysis', 'dynamical-systems']"
1716407,Probability that a random $5$-card hand $\dots$,"What is the probability that a random $5$-card hand(from a deck of $52$ cards) has: (a) exactly $3$ of the same number: that is $3$ cards of the same number but not poker or full. (b) $3$ or more of the same number? (c) At least $1$ spade, at least $1$ heart, no diamonds or clubs and the values of the spades are all greater than values of the hearts? I think the answer for a) is 
$$\frac{\dbinom{13}{3}\dbinom{4}{1}\dbinom{12}{2}\dbinom{4}{1}\dbinom{4}{1}}{\dbinom{52}{5}}$$","['combinatorics', 'probability', 'discrete-mathematics']"
1716418,Random Walk of a drunk man,"Problem Statement: From where he stands, one step toward the cliff would send the drunken man over the edge. He takes random steps, either toward or away from the cliff. At any step his probability of taking a step away is 2/3, of a step toward the cliff 1/3. What is his chance of escaping the cliff? My take: Say the probability that he dies from where he stands right now is p. 
Then,
he could comfortably make one step left and end his life with probability 1/3 Or he could take one step away and two step towards and boom...take two steps away and three steps toward...so on and so forth Resulting in p= 1/3 + 2/3 * (1/3)^2 + (2/3)^2 * (1/3)^3 +.... Summing this infinite sequence gives me probability of dying as 3/7 (around 43%). I was rather puzzled when i learnt that the correct probability is 1/2. Cant figure out what are the other 7% ways for my drunken man to die which I missed above?","['puzzle', 'random-walk', 'probability']"
1716430,"Let $f : E \rightarrow \mathbb{R}$. Show that if $|f|$ is measurable on $E$ and the set $\{f > 0\}$ is measurable, then $f$ is measurable on $E$.","I'm learning about Measure Theory (specifically measurable functions) and need help with the following problem: Let $f : E \rightarrow \mathbb{R}$. Show that if $|f|$ is measurable on $E$ and the set $\{f > 0\}$ is measurable, then $f$ is measurable on $E$. What I don't understand is that the implication $$|f| \ \text{measurable}  \implies f \ \text{measurable} \ \ (1)$$ is clearly false. For example, let $A$ be a non-measurable subset of $\mathbb{R}$, and let $f$ be the function $$f(x) = \begin{cases} 1\text{ if }x\in A,\\ -1\text{ if }x\notin A.\end{cases}$$ Then $f$ is not measurable ($f^{-1}[\{1\}] = A$) but $|f|$ is measurable (it's the constant function $1$). How does the additional condition that $\{f > 0\}$ is measurable makes the implication $(1)$ true and how do I prove it?","['real-analysis', 'lebesgue-integral', 'measure-theory']"
1716472,Prove that the Gaussian Integers are an integral domain,"We have the following Theorem: A non-zero commutative ring is an integral domain if and only if for all $a$,$b$ $\neq 0$ $\implies ab \neq 0$. Now, we need to prove that the Gaussian integers form an integral domain. Proof: Let $\Bbb Z[i]$ denote the Gaussian Integers, which is a commutative ring. Take $z,w \in \Bbb Z[i]$ s.t: $z,w \neq 0$ and $z = a + ib$, $w = c + id$. Then, $zw = (ac - bd) + (ad + bc)i \in \Bbb Z[i]$. Since the elements of $\Bbb Z[i]$ are non-zero $\implies zw \neq 0$. QED. I am wondering if this is correct? Thanks.","['abstract-algebra', 'integral-domain']"
1716526,What does it mean to say that a function is integrable with respect to a measure?,"What does it mean to say a measurable function is integrable with respect to a measure, such as $\mu$? I know the definition of integrability, but I'm still not sure what exactly what ""with respect to a measure"" means. For example, with Riemann integration, for a function of one variable, $f(x)$ say, we know we are integrating with respect to $x$ and $x$ is clearly in the integrand. How should I interpret $d\mu$? Or $d\mu(f^{-1}(B))$? This question is to help me understand Radon Nikodym derivatives.","['real-analysis', 'integration', 'measure-theory']"
1716557,Conditional expectation on Function space,"This question is from a notation in section 13.4 of the book ""Linear and Nonlinear Filtering for Scientists and Engineers"" By N U Ahmed In this section, the author is deriving the Zakai Equation. I have a question about the notation, and I will try to formulate my question so that you do not have to refer back to the book. Given the space of continuous functions $C([0,T], \mathbb{R}^n)$ with the sup-norm, let $\mathcal{B}(C)$ denote the $\sigma$-algebra of Borel subsets of $C$.  We have a random variable
$$X: (\Omega,\mathcal{F},P) \rightarrow (C,\mathcal{B}(C))$$
such that $\omega \mapsto X(\cdot, \omega)$. The R.V. will give us a push forward probability measure on the measurable space $(C,\mathcal{B}(C))$ by 
$$\mu_X (E)= P(\{\omega\in \Omega : Y(\cdot,\omega) \in E\}) $$ 
for $E\in \mathcal{B}(C)$. Now the book stated ""Let $E^X$ denote the integrations on $C$ with respect to the measure $\mu_X$"", then it used the following notation
$$E^X[g(X(t)) | \mathcal{H}]$$
where $g$ is a bounded Borel measurable function on $\mathbb{R}^n$ and $\mathcal{H}$ is a sub $\sigma$-algebra of $\mathcal{F}$ in original probability space. I was wondering is this a conditional expectation on the original probability space or the function space? 
Since the $t$ appears in the expression, I would guess it is a conditional expectation on the original probability space, but how does $E^X$ work here?
Maybe this is an abusive notation that I am not aware of. If you would like to look at the book, it is here . This is where they used such notation: The $\mu_X$ I defined is the $\mu_1$ in the book, and my $E^X$ is its $E^1$. We can ignore $\mu_0$ and $E^0$ for now, I was not sure how the middle equality between $E$ and $E^1$ is defined in the picture.","['stochastic-processes', 'probability-theory', 'stochastic-analysis']"
1716583,Prove that $f(x) = x^3 -x $ is NOT Injective,"Okay, so I know there are plenty of injective/surjective (and thus, bijective) questions out there but I'm still not happy with the rigor of what I have done. So I'd really appreciate some help! So the question actually asks me to do two things: (a) give an example of a cubic function that is bijective. Explain why it is bijective. (b) give an example of a cubic function that is not bijective. Explain why it is not bijective. So for (a) I'm fairly happy with what I've done (I think): $$ f: \mathbb R \rightarrow \mathbb R , f(x) = x^3$$ So we know that to prove if a function is bijective, we must prove it is both injective and surjective. Proof: $f$ is injective Let: $$x,y \in \mathbb R : f(x) = f(y)$$ 
$$x^3 = y^3$$            (take cube root of both sides)
$$x=y$$ Proof: $f$ is surjective Let: $$y \in \mathbb R$$ $$x = \sqrt[3]{y}$$ $$f(x) = (\sqrt[3]{y})^3 = y$$ So I believe that is enough to prove bijectivity for $f(x) = x^3$. Keep in mind I have cut out some of the formalities i.e. invoking definitions and sentences explaining steps to save readers time. This is just 'bare essentials'. So for (b) $$f: \mathbb R \rightarrow \mathbb R , f(x) = x^3 - x$$ Now I'm just going to try and prove it is NOT injective, as that should be sufficient to prove it is NOT bijective. Let: $$x,y \in \mathbb R : f(x) = f(y)$$
$$x^3 - x = y^3 - y$$ This is about as far as I get. Send help. Thanks everyone.","['elementary-set-theory', 'polynomials', 'functions']"
1716597,What is the expected distance from the mean of a multivariate Gaussian?,"For a multivariate Gaussian distribution $p(x) = N(x\mid \mu,\Sigma)$, what is $E[\|x-\mu\|]$? I know from this question that $E[|x-\mu|]=\sigma\sqrt{2/\pi}$ for univariate Gaussians. But I couldn't find a definition of standard deviation for multivariate Gaussians. Could it be something like $\|\sqrt{\boldsymbol\lambda}\|\sqrt{2/\pi}$, where $\boldsymbol\lambda$ is the eigenvalues of $\Sigma$?","['normal-distribution', 'multivariable-calculus', 'statistics', 'integration', 'probability']"
1716606,Dimension of Moduli Space of Bundles on Curves,"I think I'm getting conflicting results for the dimension of the moduli space of rank $r$, degree $d$ stable vector bundles on a curve $X$ of genus $g$.  I'm happy to look only at the nice case of $r$ and $d$ coprime such that stable and semi-stable are equivalent.  One can prove that in this case, the moduli space $M_{g}(r,d)$ is a smooth, projective variety of (complex) dimension $r^{2}(g-1) + 1$ for $g \geq 2$. The issue I'm having, comes from an alternative angle.  One can also consider representations of the fundamental group $\rho: \pi_{1}(X) \to G$, where $G$ is compact, connected Lie group.  We can the following set: $R_{d} = \big\{(A_{1}, B_{1}, \ldots, A_{g}, B_{g}) \big| \prod A_{i}B_{i}A_{i}^{-1}B_{i}^{-1} = e^{-2\pi i d/r} I \big\} \subseteq G^{2g},$ where of course the $A_{i},B_{i}$ are the images of the generators $a_{i},b_{i}$ of the fundamental group.  I think it's true that $M_{g}(r,d) \simeq R_{d}/ U(r),$ where $U(n)$ acts on $R_{d}$ by conjugation.  For a general compact Lie group $G$, we have $2g\rm({dim}G)$ degrees of freedom in the $2g$ matrices, we subtract $\rm{dim}G$ corresponding to the constraint equation, as well as another $\rm{dim}G$ corresponding to conjugation symmetry.  All in all, we have a real dimension of $(\rm{dim}G)(2g-2)$.  The real dimension of $U(r)$ is of course $r^{2}$ which suggests that the real dimension of $M_{g}(r,d) = 2r^{2}(g-1)$.  This seems to disagree with the known formula by $\pm 2$. Any idea what I am overlooking here?  Thanks in advance.","['complex-geometry', 'algebraic-geometry']"
1716636,Landau Notation - Practical explanations,"Someone told me this week that the Landau notation is very pratical in general in analysis. Definition : Let the function $\phi$ defined on an open set containing $x_0$.We want to compare $f$ à $\phi$: we want to know if
  $\displaystyle{\left\vert\frac{f}{\phi}\right\vert}$ is bound or if
  the limit is nul at this point; but, we can write the rapport only if
  $ \phi$ doesn't vanish. We say that  $ f\in o(\phi)$ in the neighborhood of $ x_0$ if and only
  if for all  $ \varepsilon >0$ there exists  $ \eta>0$ such that $ \displaystyle{\forall x\in ]x_0-\eta,x_0+\eta[, \vert
 f(x)\vert<\varepsilon \vert\phi(x)\vert}$ If $ \phi$ doesn't vanish, we have : $ \displaystyle{\forall x\in
 ]x_0-\eta,x_0+\eta[,
 \left\vert\frac{f(x)}{\phi(x)}\right\vert<\varepsilon }$ We say in
  this case that $ f$ is negligible in front of $ \phi$ in a
   neighborhood of  $ x_0$. I would like to know why it is important to embrace this notation and master the concept. Does someone could explain to me the importance of this notation (With examples, comparative, etc.)? The question may be silly, but sometimes I wonder about certain topic without being myself able to answer this question. I'm a little young (13 years old), and still have a lot to learn in mathematics. Thanks!","['intuition', 'notation', 'analysis']"
1716651,Roll two dice. What is the probability that one die shows exactly two more than the other die? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Two fair six-sided dice are rolled. What is the probability that one die shows exactly two more than the other die (for example, rolling a $1$ and $3$, or rolling a $6$ and a $4$)? I know how to calculate the probabilities of each event by itself, but I do not know how to proceed with this problem.","['probability', 'discrete-mathematics']"
1716674,Troubles with Cantor's Diagonal Argument,"Hello all and first let me say thanks for reading this, this is my first post on this website. I have some intuitive issues with Cantor's diagonal argument, briefly explained here https://en.wikipedia.org/wiki/Cantor%27s_diagonal_argument if you know it by another name. I understand how it proves that there is no one to one mapping between real numbers and natural numbers, and I believe that is true. I do not like the proof itself, however, and I need someone to correct my understanding. To demonstrate my thought process, I will attempt to construct an example that uses the diagonal argument to prove there is no one to one mapping from natural to natural numbers , which is obviously a contradiction. So, here it is. I begin by defining a bunch of sets that correspond to the natural numbers. They use the prime factorization of the numbers, such that the nth element of the set is the exponent of the nth prime number in the natural number that it corresponds to's prime factorization. That is worded confusingly, so I will show some examples. 2 -> (2^1) (3^0) (5^0)... -> {1, 0, 0, 0, 0, 0...} 45 -> (2^0) (3^2) (5^1)... -> {0, 2, 1, 0, 0, 0 ...} 90 -> {1, 2, 1, 0, 0, 0, ... } Using this, I now have a set for each natural number. I can list them out like this: S1 = {1, 0, 0, ...} S2 = {0, 1, 0, ...} S3 = {2, 0, 0, ...} S4 = {0, 0, 1, ...} S5 = {1, 1, 0, ...} . . . And so on. Those sets are in order, but it does not really matter what order they are in. Using the diagonal argument, I can create a new set, not on the list, by taking the nth element of the nth set and changing it, by, say, adding one. Therefor, the new set is different from every set on the list in at least one way. This is straight from the Wikipedia article if I am not explaining this logic right. Now here is the catch. The new set I created using the diagonal argument must not be on the list because it is different from everything on the list, but because I defined the list as the prime factorization of any natural number, than any set has to be on the list because ""The fundamental theorem of arithmetic says that every positive integer has a single unique prime factorization."" To me, this is a contradiction. The diagonal argument does not make intuitive sense to me, I must be misunderstanding it. Hopefully one of you will understand what I tried to convey here and help me understand what I did wrong.",['elementary-set-theory']
1716680,Simplify $\frac{1-\cos^2(-\theta)}{1+\tan^2(-\theta)}$,"I'm doing practice problems out of Trigonometry 10th ed. Hornsby and ran into a question. Section 5.1 question 71: Write each expression in terms of sine and cosine, and simplify so that no quotients appear in the final expression and all functions are of $\theta$. $\frac{1-\cos^2(-\theta)}{1+\tan^2(-\theta)}$ The book has the answer $\sin^2 \theta \cos^2 \theta$. However, I cannot figure out how to get to this answer. I started by pulling out the negative from $\theta$: $\frac{1+\cos^2\theta}{1-\tan^2\theta}$ Then I changed tangent into sine and cosine:
$\frac{1+\cos^2\theta}{1-\frac{\sin\theta}{\cos\theta}}$ Then multiplied by the reciprocal: $(1+\cos^2\theta)(1-\frac{\cos\theta}{\sin\theta})$ And this is where things got really confusing as I had no idea what I could do with the result of the above expression.",['trigonometry']
1716686,Relationship between incenter and circumcenter,"Let $ABC$ be an acute triangle with circumcenter $O$ and incenter $I$ . Points $E$ , $M$ lie on $AC$ and $F$ , $N$ on $AB$ so that $BE \perp AC$ , $CF\perp AB$ , $\angle ABM = \angle CBM$ and $\angle ACN = \angle BCN$ . Prove that $I$ lies on $EF$ if and only if $O$ lies on $MN$ . I honestly can't make any progress. I tried playing around on geogebra but it seems like when I get a configuration where $O$ and $I$ are on the desired lines, $O$ and $I$ stay on these lines as I move $A$ around the circumcenter...","['circles', 'triangles', 'geometry']"
1716709,Closed-form for $\int_0^\infty {\frac{{\ln \left( {1 + x} \right)}}{{1 + ax}}{e^{ - bx}}{x^n}{\rm{d}}x} $,"I am trying to find the integration of the following $$\int_0^\infty  {\frac{{\ln \left( {1 + x} \right)}}{{1 + ax}}{e^{ - bx}}{x^n}{\rm{d}}x} $$ Here $a>0, b>0$, and $n$ is an integer. I think if we get the Meijer-G representation of $$\frac{{\ln \left( {1 + x} \right)}}{{1 + ax}}$$ we can use Laplace transform to get the closed-form expression.
But I don't know how to express the above function as Meijer-G function. Thanks.","['improper-integrals', 'integration', 'definite-integrals']"
1716720,Verify the identity: $(1-x^2)\frac{\partial^2 \Phi}{\partial x^2}-2x\frac{\partial\Phi}{\partial x}+h\frac{\partial^2}{\partial h^2}(h\Phi)=0$,"The Generating function for Legendre Polynomials is: $$\Phi(x,h)=(1-2xh+h^2)^{-1/2}\quad\text{for}\quad \mid{h}\,\mid\,\lt 1\tag{1}$$
and Legendres' Differential equation is $$\begin{align*}
(1-x^2)y^{\prime\prime}-2xy^{\prime}+l(l+1)y=0\tag{2}
\end{align*}$$ The question in my text asks me to: Verify the identity $$\color{blue}{\left(1-x^2\right)\frac{\partial^2 \Phi}{\partial x^2}-2x\frac{\partial\Phi}{\partial x}+h\frac{\partial^2}{\partial h^2}\left(h\Phi \right)=0}\tag{A}$$ by straightforward differentiation of $(1)$ and some algebra. End of question. My attempt: I started by taking partial derivatives of $(1)$ first with respect to $x$ then with respect to $h$:
$$\color{#180}{\frac{\partial \Phi}{\partial x}}=-\frac12 \left(1-2xh +h^2 \right)^{-3/2}\cdot(-2h)=\color{red}{h\Phi^3}$$ $$\color{#180}{\frac{\partial^2 \Phi}{\partial x^2}}=-\frac32 h\left(1-2xh+h^2\right)^{-5/2}\cdot (-2h)=\color{red}{3h^2\Phi^5}$$ $$\color{#180}{\frac{\partial \Phi}{\partial h}}=-\frac12 \left(1-2xh +h^2 \right)^{-3/2}\cdot(2h-2x)=\color{red}{(x-h)\Phi^3}$$ $$\begin{align}\color{#180}{\frac{\partial^2 \Phi}{\partial h^2}} & = -\frac32 \left(1-2xh+h^2\right)^{-5/2}\cdot (2h-2x)\cdot(x-h)-\left(1-2xh +h^2 \right)^{-3/2}\\ & =3\left(1-2xh+h^2\right)^{-5/2}\cdot(x-h)^2-\left(1-2xh +h^2 \right)^{-3/2}\\&=3\Phi^5(x-h)^2-\Phi^3\\&=\color{red}{\Phi^3\Big(3\Phi^2(x-h)^2-1\Big)}\end{align}$$ I can see the similarity between $(\mathrm{A})$ and $(2)$; but I am confused how to obtain $(\mathrm{A})$. Could someone please help me reach equation $(\mathrm{A})$? EDIT: Now that the question has been answered a special thanks goes to @Semiclassical for the helpful comment and @enthdegree for explaining to me that $$\begin{array}{rcl}\frac{\partial^2}{\partial h^2} h \Phi &=& \frac{\partial}{\partial h} (  \frac{\partial}{\partial h} h \Phi) \\ &=& \frac{\partial}{\partial h} \left( \Phi + h  \frac{\partial}{\partial h} \Phi \right) \\  &=&  \frac{\partial}{\partial h} \Phi + \frac{\partial}{\partial h} \Phi + h\frac{\partial^2}{\partial h^2} \Phi \end{array}$$ I would also like to thank @JJacquelin and especially @Markus Scheuer for giving such a well constructed argument. All the answers given were great; but since I could only accept one answer I think Markus gave the most intuitive answer so the answer credit goes to him. In any case I have up-voted all your answers :) Much to my shame I should have known better that substitution of the $\color{#180}{\mathrm{green}}$ terms into $(\mathrm{A})$ was all that was required to verify the identity $(\mathrm{A})$. Instead of verifying it seems I was trying to derive $(\mathrm{A})$ by noting the striking similarity between $${\left(1-x^2\right)\frac{\partial^2 \Phi}{\partial x^2}-2x\frac{\partial\Phi}{\partial x}+h\frac{\partial^2}{\partial h^2}\left(h\Phi \right)=0}\tag{A}$$ and $$\begin{align*}
(1-x^2)y^{\prime\prime}-2xy^{\prime}+l(l+1)y=0\tag{2}
\end{align*}$$","['derivatives', 'legendre-polynomials', 'partial-derivative', 'calculus']"
1716723,Why do extraneous solutions exist? [duplicate],"This question already has answers here : When do we get extraneous roots? (5 answers) Closed last year . I am currently in a Pre Calculus class at my High School. I have come across the concept of extraneous solutions, particularly when solving absolute value equations, radical equations, and logarithmic equations. My question is, why do these solutions exist? My teacher never explained this, which is understandable given that I am in a High School math class, and there isn't much time for the teacher to go into the actual derivations of everything. I am wondering because I plan to major in Mathematics, and having a conceptual understanding of this is important to me. If anyone could explain the reason extraneous solutions exist for the three examples I noted, it would be of great help to me.",['algebra-precalculus']
1716742,Aa student takes a $10$-question true/false exam and guesses. What is the probability that the student answers every question incorrectly?,Suppose a student takes a $10$-question true/false exam and guesses at every question. What is the probability that... a) the student answers every question incorrectly? b) the students answers at least one question correctly? c) the student answers exactly 5 questions correctly? So far I have solved a. I got $(1/2)^{10}$. For b I'm not sure what to do. For c would the answer be $(1/2)^5$?,"['binomial-distribution', 'probability', 'discrete-mathematics']"
1716797,Divergence of the sequence $n^2-100n$ as $n$ goes to $\infty$.,"State the definition of divergence to $+\infty$. Argue straight from
  this definition that $n^2-100n$ diverges to $+\infty$. The definition is the negation of the definition of convergent sequence which says $\forall \epsilon>0 ~ \exists N \in \mathbb{R}$ s.t. $ |a_n-\ell| < \epsilon$ $\forall n > N$. The negation would then be $\exists \epsilon>0$ s.t.$ ~ \forall N \in \mathbb{R}$, $ |a_n-\ell| \ge \epsilon$ whenever $\exists n > N$. But I don't know how to prove divergence; so far I've only met with far more simple things.","['epsilon-delta', 'real-analysis', 'analysis']"
1716833,A generalized version of inclusion exclusion principle using a binomial identity,"I'm trying to find a way to derive a generalized inclusion exclusion principle for the number of elements which are in the intersection of at least $s$ sets from $A_1,A_2,...,A_n$ using this identity: let $k$ and $s$ be positive integers and let $k\ge s\ge 1$
$$\sum_{i=0}^{k-s} (-1)^i{s-1+i \choose s-1}{k \choose s+i} = 1$$ I'm coming from this question: proof that the binomial sum is equal to 1 It appears from the sum that what determines whether we add or substract the product of binomial coefficient is whether it has even or odd number of elements and it can be applied to every subset of $s. But I don't quite understand the concept of the generalized form of inclusion exclusion principle.","['generating-functions', 'combinatorics', 'inclusion-exclusion', 'discrete-mathematics']"
1716856,Left-continuity of a Lévy filtration,"The natural filtration $(\mathcal{F}_t^X)_{t\geq 0}$ of a Lévy process $X$ is right-continuous, but what about left-continuity? A Lévy process is quasi left-continuous at time $t$ which says that
\begin{align*}
\lim_{s\nearrow t}X_s = X_t, \quad w.p.\; 1,
\end{align*}
so it is almost surely left-continuous at time $t$. It is known that a left-continuous process has a left-continuous natural filtration, so what is the difference between $\mathcal{F}_t$ and $\mathcal{F_{t^-}}$ for Lévy processes? It seems to me that events like $\{X_t\in A\}$, where $A$ is a Borel set, belong to $\mathcal{F}_t$ but not $\mathcal{F_{t^-}}$, but at the same time,
\begin{align*}
\mathbb{E}(X_t \mid \mathcal{F}_t) = X_t = X_{t^-} = \mathbb{E}(X_t \mid \mathcal{F}_{t^{-}}),
\end{align*} 
holds almost surely. Is essentially $\mathcal{F}_t=\mathcal{F}_{t^-}+\text{null sets}$? Can we for a fixed $t_0$ find a modification $X^{(t_0)}$ of $X$, which is left-continuous at $t_0$, and whose natural filtration therefore satisfies $\mathcal{F}_{t_0^{-}}=\mathcal{F}_{t_0}$?","['stochastic-processes', 'probability-theory', 'measure-theory', 'levy-processes']"
1716867,"weird combinatorics, combinations question from cambridge challenge exercise","A child has $10$ identical blocks, each of which is to be painted with one of $4$ colours. 
how many different ways can the $10$ blocks be painted? Answer is $286$ but I have no idea how they got it. From cambridge year ten book $2$","['combinatorics', 'discrete-mathematics']"
1716873,"If $A+B+C = \pi$, show that $\sum_{\text{cyclic}} \tan A \tan B \neq 0$","If $A+B+C = \pi$, and $A,B,C > 0$ show that $$\sum_{\text{cyclic}} \tan A \tan B \neq 0$$ without using the fact that if $A+B+C = \pi$, then $\tan A + \tan B + \tan C = \tan A \tan B \tan C$ My Work I tried to show it as follows: $$\tan(A+B+C) = \dfrac{\tan A + \tan B + \tan C - \tan A \tan B \ tan C}{1-\sum_{\text{cyclic}} \tan A \tan B}$$ For $$A+B+C = \pi$$ $$0 = \dfrac{\tan A + \tan B + \tan C - \tan A \tan B \ tan C}{1-\sum_{\text{cyclic}} \tan A \tan B}$$ I can't proceed from here.","['algebra-precalculus', 'trigonometry']"
1716905,How do I find the solution to the equation $z^2=-81i$?,"This question is from the Powers of Complex Numbers, Precalculus section of KhanAcademy Find the solution to the following equation whose argument is between $90°$ and $180°$ $$z^2=-81i$$ What I understand thus far: I am going to set $r$ and $\theta$ to be the modulus and argument of $z$, respectively. Therefore, $z^{ 2 }=r^{ 2 }[cos(2\cdot \theta )+isin(2\cdot \theta )]$ Now, I can understand how the modulus is $81$, but I do not understand how it was determined that the argument is $270°$ plus any multiple of $360°$. I am quite confused at this point and a hint in the right direction would be the best thing to help me figure out the solution to this problem and ones like it that I will encounter in the future.","['algebra-precalculus', 'complex-numbers']"
1716922,Showing that diffeomorphisms between manifolds preserves orientability,"Here is my view of orientability on a vector space $V$ of dimension $m>0$: let $I(V)$ be the set of linear isomorphisms from $V$ to $\mathbb{R}^m$. Given $\rho,\sigma\in{I(V)}$, we get a linear automorphism $\sigma\circ\rho^{-1}:\mathbb{R}^m\rightarrow\mathbb{R}^m$ with $\det(\sigma\circ\rho^{-1})\not=0$. We write $\sigma\sim\rho$ if $\det(\sigma\circ\rho^{-1})>0$, which defines an equivalence relation on $I(V)$. We define $\mathrm{Or}(V)=I(V)/\sim$, so that $|\mathrm{Or}(V)|=2$, and define an orientation on $V$ to be an element of $\mathrm{Or}(V)$. An oriented vector space is then a (finite dimensional) vector space equipped with an orientation. Now, a basis of an oriented vector space $V$ determines an element of $I(V)$ which sends this basis to the standard basis of $\mathbb{R}^m$. We refer to this basis as positively oriented (with respect to our orientation on $V$) if this map lies in the equivalence class of our orientation, and negatively oriented otherwise. For now I am considering an $m$-manifold embedded in $\mathbb{R}^p$. Then an orientation on such a manifold is as assignment of an orientation to each tangent space $T_x{M}$, such that there is an atlas of charts $\phi_\alpha:U_\alpha\rightarrow{V_\alpha}\subseteq\mathbb{R}^m$ such that for all $\alpha$ and all $x\in{U_\alpha}$, the map $d_x\phi_\alpha:T_x{M}\rightarrow\mathbb{R}^m$ lies in the orientation class of the orientation in $T_x{M}$. We refer to $M$ as an oriented manifold, and say $M$ is orientable if it admits an orientation. Now, I'm aware that the following question has been asked before, but all the answers I've found have used concepts I haven't come across yet, such as differential forms, pullbacks etc. The problem is to show that if $M$ is an orientable manifold and $f:M\rightarrow{N}$ is a diffeomorphism between manifolds (where $N$ is also an $m$-manifold embedded in $\mathbb{R}^q$), then $N$ is also orientable. I've tried using the fact that $\{\phi_\alpha\circ{f}^{-1}\}_\alpha$ will be an atlas for $N$ if $\{\phi_\alpha\}$ is an altas for $M$, but am getting nowhere. I'm thinking the right way to go about it is a basis approach?","['manifolds', 'smooth-manifolds', 'differential-geometry']"
1716937,(Easier) way to determine determinant of a almost-circulant matrix,"Consider the matrix: $$\begin{pmatrix} \color{red}{-x+y} &-1&0&-1 \\ -1&\color{blue}{x+y}&-1&0 
\\ 0&-1&\color{red}{-x+y}&-1 \\ -1&0&-1&\color{blue}{x+y} \end{pmatrix}$$ Is there any easy way to calculate the determinant of this matrix? I have read up on circulant matrices and there is some nice theory on them, however this is not quite circulant, the only difference is that the main diagonal contains 2 different values. So I was wondering if there is a 'short cut' to find the determinant of these kind of almost -circulant matrices.","['matrices', 'linear-algebra', 'determinant']"
1717022,Boundary layers: approximately satisfying BC,"I am working on a boundary layer problem for a second order linear ODE. A simpler problem which I think still illustrates the issue I am having is $$\varepsilon y''-y'+y=0,y(0)=0,y(1)=1$$ where $\varepsilon > 0$ is a small parameter. This problem (unlike my actual problem) in fact admits an exact solution, namely $$\frac{e^{\lambda_2 x}-e^{\lambda_1 x}}{e^{\lambda_2}-e^{\lambda_1}}$$ where $$\lambda_1,\lambda_2=\frac{1 \pm \sqrt{1-4\varepsilon}}{2 \varepsilon}.$$ For small $\varepsilon > 0$, $\lambda_1$ is a large positive number, namely $\frac{1}{\varepsilon}+O(1)$, while $\lambda_2$ is a positive number of order $1$, namely $1+O(\varepsilon)$. Upon sorting out minus signs, this means that the solution above grows very fast near $1$ and is very small far away from $1$. Accordingly, I've developed an ""inner"" boundary layer solution near $x=1$. This goes through the usual way by changing variables to $z=\frac{x-1}{\varepsilon}$, assuming that $\frac{d^2 y}{dz^2},\frac{dy}{dz}$ and $y$ are all $O(1)$ near $x=1$, and balancing the leading order terms. From this I get a leading order solution of $e^{\frac{x-1}{\varepsilon}}$ for $|x-1|=O(\varepsilon)$. Now my problem arises on the rest of the interval. For the leading order outer solution, we consider $y'=y$, and get $y=Ce^x$. My problem is now essentially in identifying $C$. I can take it to be $0$ to match the left boundary condition, but this doesn't seem right, because the composite solution from this procedure is $e^{\frac{x-1}{\varepsilon}}$ over the whole interval, which fails to satisfy the left boundary condition (at least exactly). I can subtract off $e^{-\frac{1}{\varepsilon}}$ to force the left boundary condition to hold (which has a negligible impact on the error in the DE itself), but then the right boundary condition doesn't hold. Should I be thinking about this differently? Do I need to go to higher order in order to get a reasonable result? It seems to me that I will, because a ""reasonable"" first order linear ODE beginning with value zero will always just stay at zero. So to start at zero and not stay there I need to consider a truly second order problem. But this is more difficult in my actual problem, and it seems that the method should be more or less universal.","['asymptotics', 'ordinary-differential-equations', 'perturbation-theory']"
1717030,A graph with an equal number of edges and vertices contains a cycle as a subgraph,"I'm trying to prove A graph with an equal number of edges and vertices contains a cycle as
  a subgraph My attempt Induction on the number of vertices: Clearly holds for $n=3$ Assume true for all graphs on $\le n-1$ vertices. For n: Case 1: There is a vertex of degree 1, say $v$. Then $G = v \cup H$.
  By hypothesis, H contains a cycle, so G necessarily contains a cycle. Case 2: No vertex of degree 1. So every vertex has degree $\ge 2$.
  Take minimum spanning tree of G, which contains no cycles and has at
  most $n-1$ edges. But our hypothesis says that any graph on $\le n-1$
  edges contains a cycle. Contradiction. I'm not sure about my case 2, any verification would be helpful.","['combinatorics', 'graph-theory']"
1717038,Checking units following differentiation,"I'm trying to construct a simple mathematical model for a physical system, but my calculus is rusty and I'm tying myself in knots, especially with regard to the dimensions of the quantities I'm using. The system obeys the equations $$y = \frac{x - a}{\tau} \qquad for \qquad x > a$$
$$y = 0 \qquad for \qquad x \leq a$$ $x$ and $a$ have units of length, $\tau$ has units of time and $y$ has units of $L/T$. $a$ and $\tau$ are constants. The discontinuity at $x = a$ causes me some problems later in my analysis, so I've been approximating it as $$y = \frac{x -a}{\tau} g(x)$$ where $$g(x) = \frac{1}{1 + e^{(a - x)}}$$ The function $g(x)$ switches fairly quickly from $\approx 0$ to $\approx 1$ at $x = a$, which gives a continuous representation of the discontinuity that is sufficiently abrupt for my purposes. In principle, $g(x)$ is just a dimensionless multiplier, but in reality both $a$ and $x$ have units and I'm a bit nervous about having these quantities inside my exponential . In the next step, I need to differentiate $$y = \frac{x - a}{\tau(1 + e^{(a - x)})}$$ with respect to $x$. This took me a depressingly long time by hand, but my answer (which agrees with Wolfram Alpha) is $$\frac{dy}{dx} = \frac{(x - a)e^{(a - x)}}{\tau(e^{(a - x)} + 1)^2} + \frac{1}{\tau(e^{(a - x)} + 1)}$$ When I link all this together with the other ODEs in my system, everything works fine and the results look plausible, which is great. However, because of the way I constructed $g(x)$, the units/dimensions in the above equation no longer make sense, which makes me think I might be getting a sensible answer, but for the wrong reasons . Is my general approach here valid, or is there a better/more rigorous way of thinking about this that would keep the units consistent? Thanks very much!","['derivatives', 'dimensional-analysis']"
1717043,Solve differential equation using 'variation of parameters',"Given the differential equation $L \ u = f$, with $$L \ u = a_2(t) \frac{d^2u}{dt^2}+a_1(t)\frac{du}{dt}+a_0(t)u$$
with $a_i(t)$ sufficiently smooth and $a_2(t) \neq 0$ for every $t$.
Suppose that $u_0$ is a solution to the homogenenous equation $L \ u=0$, with $u_0(t)\neq0$ for every $t$.
 How can I then use the variation of parameters method by Lagrange to find the solution $u =u_0v$ ? And how can I find the general solution if the equation isn't homogeneous? Update : I've substituted $u=u_0v$ into the equation and found a new $2^{nd}$ order differential equation of the form 
$$f = \frac{d^2v}{dt^2}(a_2(t)u_0)+\frac{dv}{dt}(2a_2(t)u_0'+a_1(t)u_0)+va_0(t)u_0$$ which I'm supposed to solve for $w$ using $w=v'$. But I don't seem to be able to proceed as I'm not sure how to solve the system
$$\begin{equation}
\begin{split}
w =&\ v'\\
w'a_2(t)u_0+w(2a_2(t)u_0'+a_1(t)u_0)+v(a_0(t)u_0)=&\ f\ \\
\end{split}
\end{equation}$$",['ordinary-differential-equations']
1717048,"If an orbit $G\cdot x$ is closed in the standard topology, is it Zariski-closed?","Let $G$ be a complex linear algebraic group acting linearly on finite-dimensional complex vector space $V$. If an orbit $G\cdot x$ is closed in the standard topology on $V$, is it also Zariski-closed? Sometimes it is easier to see that an orbit is closed in the usual topology rather than showing that it satisfies some polynomial equations. But I am not sure if this is sufficient to infer that the orbit is Zariski-closed. I couldn't find any counterexample though.","['algebraic-geometry', 'lie-groups']"
1717076,How many ways can 2 person sit in 4 empty chairs?,"I can find the answer using brute force as 12, but what is the formula to calculate this for any combination of person and chairs. Here is the brute force combinations for 2 person, 4 chair: Group where A is always placed before B A,-,-,B,
A,B,-,-
-,-,A,B
-,A,-,B
A,-,B,-
-,A,B,- Group where B is always placed before A B,-,-,A
B,A,-,-
-,-,B,A
-,B,-,A
B,-,A,-
-,B,A,-",['combinatorics']
1717086,Inverse function of $x^2-4$,"The function $h$ is defined by $$h(x)=x^2-4$$. for $$x\leq0$$ Find an expression for $h^{-1}(x)$ My attempt, Let $h^{-1}(x)=a$ $x=h(a)$ $x=a^2-4$ $a=\sqrt{x+4}$ $h^{-1}(x)=\sqrt{x+4}$ Am I wrong? Is the answer $-\sqrt{x+4}$?",['algebra-precalculus']
1717087,Show that $\frac{\Gamma(n+k)}{\Gamma(n)}\sim n^k$ for large values of n,"In order to prove the above result I proceeded as follows: We know that: $\Gamma(n)=(n-1)\Gamma(n-1)$ Using this fact, we have: $\Gamma(n+k)=(n+k-1)\Gamma(n+k-1)\\=(n+k-1)(n+k-2)\Gamma(n+k-2)\\
=...=(n+k-1)(n+k-2)...(n+k-k)\Gamma(n+k-k)\\=n(n+1)...(n+k-1)\Gamma(n)$ So, now we have:
$$\lim\limits_{n\rightarrow\infty}\frac{\Gamma(n+k)}{\Gamma(n)}=
\lim\limits_{n\rightarrow\infty}\frac{n(n+1)...(n+k-1)\Gamma(n)}{\Gamma(n)}$$ Now, if $x$ is a variable and $c$ is a constant, then $x+c\approx x$ (for very large values of $x$) ( I am a little doubtful about this ) Using this fact, can we say that: $\lim\limits_{n\rightarrow\infty}\frac{\Gamma(n+k)}{\Gamma(n)}=\lim\limits_{n\rightarrow\infty}n(n+1)...(n+k-1)\\\sim n.n...n\ (k\ times)\\=n^k$ Thus, we have:
$$\frac{\Gamma(n+k)}{\Gamma(n)}\sim n^k$$ Is this method valid? Is there a more ""mathematically rigorous"" way to prove this? Thanks in advance!","['gamma-function', 'limits']"
1717106,Minimum number of real variables required to describe all n by n matrices that are their own inverse,"This is related to Is the identity matrix the only matrix which is its own inverse? , but that question does not contain an answer to this one. I am attempting to find - probably by using a learning algorithm - the linear transformation in $\mathbb{R}^{300}$ that most closely maps a set of pairs of vectors to each other. Naively, this would require me to perform gradient descent (or some other learning algorithm) on $300\cdot 300 = 90000$ variables - one variable for each cell in my matrix. However, since I know that the matrix must be its own inverse, I suspect I can use a smaller number of variables. Is this the case? Is there some function of fewer than 90000 reals whose range is (or at least contains) all 300 by 300 matrices who are their own inverse? If so, what is that function, and what is the minimum number of reals required in its input?","['matrices', 'linear-algebra']"
1717117,What does it mean to write a linear operator in a particular basis?,"I am working on a past Algebra exam paper and have come across a problem which requires me to write the linear operator associated to a given matrix $M$ in the standard basis of $\mathbb{R}^4$. What does it mean to 'write a linear operator IN a given basis'? Thank you. NB: I have looked at this link , but I am not sure if the concept of writing a matrix in a given basis is synonymous to the concept of writing a linear operator in a given basis. EDIT: I think I should be more specific. The statement of the problem I am working on is as follows: Let $M$ be the matrix: 
$$\begin{bmatrix} 
-2 & 3 & 7 & -3 \\
-6 & 1 & 16 & 1 \\
-2 & 1 & 6 & -1 \\
-2 & -1 & 6 & 3 \\
\end{bmatrix}$$ Write the linear operator associated to $M$ in the standard basis of $\mathbb{R}^4$. The model solution is as follows: The linear operator associated to $M$ is the linear map given by $$M
\begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
x_4 \\
\end{bmatrix} = 
\begin{bmatrix} 
-2x_1 + 3x_2 + 7x_3 -3x_4 \\ 
-6x_1 + x_2 + 16x_3 + x_4 \\ 
-2x_1 + x_2 + 6x_3 - x_4 \\ 
-2x_1 - x_2 + 6x_3 + 3x_4 
\end{bmatrix} $$ My current issue is that I don't fully understand how the model solution is an example of the linear operator written in the standard basis of $\mathbb{R}^4$. In particular, I'm still trying to grasp the concept of writing a linear operator IN a given basis.  What does this really mean?","['matrices', 'linear-algebra', 'linear-transformations']"
1717145,group theory for non-mathematicians,"A very smart non-mathematician friend is looking to learn about groups, and I was wondering if people might have suggestions (this is NOT a duplicate of this question, since a textbook is not what I am looking for, at least not at first).","['reference-request', 'group-theory']"
1717151,"Proof That For Two Uncountable Sets, The Cardinality of the Collection of Their Respective Uncountable Subsets Are Equal.","The proposition I came along is as follows: Assume $S$ & $T$ are uncountable sets. Then if $\mathcal{P}_{u}(S)$ and $\mathcal{P}_{u}(T)$ denotes the respective collection of uncountable subsets of $S$ & $T$,  $|\mathcal{P}_{u}(S)| = |\mathcal{P}_{u}(T)|$. I did write out a proof on if $|S| = |T|$, then $|\mathcal{P}(S)| = |\mathcal{P}(T)|$ for the normal type of Power sets. This conclusion was derived from the fact that due to the existence of bijective function, $f: S \rightarrow T$, and $f^{-1}: T \rightarrow S$, all sets within each power set can be constructed by a mapping from a set $S_{i} \rightarrow T_{i}$ s.t. $S_{i} \subseteq S$ and $T_{i} \subseteq T$. However, the likely apparent connection between this proof and the former is still uncertain for myself.",['elementary-set-theory']
1717187,Solve system inhomogeneous differential equations with variable coefficients,"Given the system of differential equations
$$\frac{d\vec{y}}{dx} = \begin{pmatrix}
        0 & 1 \\
        -1 & 0 \\
        \end{pmatrix}\vec{y} \ + \begin{pmatrix}
        sin(wx) \\
        0\\
        \end{pmatrix} \ \ \ \ (w \neq \pm1) $$
There are two questions which I can't answer. 1. How can I find the general solution? 2. How can I find the periodic solutions (in general). I've tried to solve the following system$$\frac{d\vec{y}}{dx} = \begin{pmatrix}
        0 & 1 \\
        -1 & 0 \\
        \end{pmatrix}\vec{y} $$ to find the complementary solution, which was 
$$\vec{y} = c_1\begin{pmatrix}
        cos(x) \\
        -sin(x)\\
        \end{pmatrix} \ +c_2\begin{pmatrix}
        sin(x) \\
        cos(x)\\
        \end{pmatrix} $$
So how do I proceed?",['ordinary-differential-equations']
1717217,Anybody knows a proof of Uniqueness of the Reduced Echelon Form Theorem?,"The book has no proof showing each matrix is row equivalent to one and only one reduced echelon matrix. Does anybody know how to prove this theorem? ""Theorem  Uniqueness of the Reduced Echelon Form Each matrix is row equivalent to one and only one reduced echelon matrix"" Source: Linear Algebra and Its Applications, David, C. Lay. [EDIT I think the following can be a proof that each echelon matrix is reduced to only  one reduced echelon matrix, but how to show a matrix that is not in echelon form is reduced to only one reduced echelon matrix?] In a $m×n$ matrix in echelon form of a linear system for some positive integers m, n, let the leading entries $(■)$ have any nonzero value, and the starred entries $(☆)$ have any value including zero. Leading entries $■$s in $R_1$ and $R_2$ in an echelon matrix can become leading 1 in a reduced echelon matrix through dividing them by $■$, and the entry ☆ in $R_1$ above $■$ in $R_2$ can be $0$ by subtracting a multiple of $■$. So $R_1$ and $R_2$ in a matrix in echelon form becomes as follows: $\begin{array}{rcl} 
R_1\space & [■ ☆\cdots ☆☆☆☆]\\
R_2\space & [0 ■\cdots ☆☆☆☆]\end{array} \qquad ~
\begin{array}{rcl} R_1\space & [1 0\cdots ☆☆☆☆]\\R_2 &[0 1\cdots ☆☆☆☆]
 \end{array}$ For all integers k with $2≤k<m$, $R_k$, $R_{k+1}$ in the echelon matrix can be expressed as $R_{k}\space$ $[0 \cdots 0 ■☆☆\cdots ☆]$ $R_{k+1}$     $[0 \cdots 0 0 ■☆\cdots ☆]$. Subtracting a multiple of leading entry of $R_{k+1}$ from  $R_k$ can make the entry above leading $■$ in $R_{k+1}$ be zero, and the leading $■$s in $R_k$, $R_{k+1}$ can be 1 through dividing the rows by leading entry $■$s. So the rows in echelon matrix become the following in reduced $m×n$ echelon matrix: $\begin{array}{rcl} 
R_{k}\space & [0 \cdots 0 ■☆☆\cdots ☆]\\  
R_{k+1} &    [0 \cdots 0 0 ■☆\cdots ☆]\\
 \end{array} \qquad 
\begin{array}{rcl} 
R_{k} & [0 \cdots 0 1 0 ☆\cdots ☆]\\
R_{k+1} & [0 \cdots 0 0 1 ☆\cdots ☆]\\
 \end{array}$ Hence, it's found that leading 1s in reduced echelon form of $m×n$ matrix of a linear system correspond to the locations of the leading non-zero values in a $m×n$ matrix in echelon form of the linear system.","['matrices', 'linear-algebra']"
1717220,Quasiconvexity (in the sense of Morrey) implies Rank-One convexity,"I am trying to understand why Quasiconvexity implies Rank-One convexity. In a standard proof of this fact a sequence of functions is constructed, which converges weakly to zero in $W^{1,p}.$ in order to understand why this is useful, I would like to understand why my simpler argument is wrong. Quasiconvexity of a function $f: \mathbb{R}^{m\times d} \rightarrow \mathbb{R}$ means that for any bounded domain $\Omega \subset \mathbb{R}^d:$ $$\int_{\Omega}f(C + \nabla \omega(y))dy \ge | \Omega | f(C), $$ for any $\omega \in W_0^{1,\infty}(\Omega)$. Now let us consider two matrices $A, B$ that are rank one connected, through $A-B = \xi \otimes \eta.$ Wlog we assume that $\eta = e_1 \in \mathbb{R^d}$ (else we rotate everything). We want to prove that $$f (\theta A + (1-\theta)B) \le \theta f(A) + (1-\theta) f(B)$$ So let us define $C = \theta A + (1-\theta)B$ and $ \Omega = (0,1)^d$ and $$ \omega(y) =  \begin{cases}
(1-\theta) (x \cdot e_1) \xi, \text{   if   } 0 \le x \cdot e_1 \le \theta \\ \\
-\theta (x \cdot e_1 -1) \xi, \text{   if   } \theta < x \cdot e_1 \le 1
\end{cases} $$ Now we find that the gradient of $\omega$ is: $$\nabla \omega (y) = \begin{cases}
(1-\theta)\xi \otimes e_1, \text{   if   } 0 \le x \cdot e_1 \le \theta \\ \\
-\theta \xi \otimes e_1, \text{   if   } \theta < x \cdot e_1 \le 1
\end{cases}$$ So eventually we find: $$\theta f(A) + (1-\theta) f(B) = \int_{\Omega}f(C + \nabla \omega(y))dy \ge f(C)$$ where the latter inequality holds by quasiconvexity. A proof of this fact is given for example by Dacorogna in ""Direct methods for calculus of variations,"" but it is actually more complicated.","['multivariable-calculus', 'real-analysis', 'calculus-of-variations', 'convex-analysis']"
1717242,Solving $z^3 e^{1-z}=1$ inside the unit circle,"Prove that $z^3 e^{1-z}=1$ has exactly two roots inside the circle $|z|=1$ . I showed that $z=1$ is a solution on the boundary of the circle, how can I find the other solution?","['complex-analysis', 'roots']"
1717244,Two atlases on a manifold $M$ are equivalent if and only if they determine the same set of smooth functions $f:M\rightarrow\mathbb{R}$,"Suppose $\{\phi_\alpha\}_{\alpha\in\mathcal{A}}$ and $\{\phi_\beta\}_{\beta\in\mathcal{B}}$ are two smooth atlases on a topological manifold $M$. My definition of two such atlases being equivalent is that their union $\{\phi_\alpha\}_{\alpha\in\mathcal{A}\cup\mathcal{B}}$ is also a smooth atlas, that is, the transition maps between the charts of different atlases are smooth. I have shown that if two atlases are equivalent, then they determine the same set of smooth functions $f:M\rightarrow\mathbb{R}$ - i.e. $f$ is smooth with respect to one chart if and only if it is smooth with respect to the other. But I do not know how to prove the converse statement. I would like to do something along the lines of $(f\circ\phi_\beta^{-1})^{-1}\circ(f\circ\phi_\alpha^{-1})=\phi_\beta\circ\phi_\alpha^{-1}$ and conclude from that that the RHS is smooth since both bracketed functions on the LHS are, but I know I can't take the inverse on the LHS like this,since there is no guarantee $f$ is invertible. Any help would be appreciated.","['manifolds', 'smooth-manifolds', 'differential-geometry']"
1717270,Bounds for the integral.,I need help setting up the bounds for this integral.  The question is: Let E be the solid between two cylinders : $x^2 + y^2 = 1$ and $x^2 + y^2 = 16$; and between two planes : $z = 0$ and $z = x+y+8$. Evaluate $\iiint_E x dV$. I have it set up as: $$\int_{0}^{\pi/2}\int_{1}^{4}\int_{0}^{x+y+8} x rdrd\theta$$ But for some reason I don't feel like that's right. Guidance is much appreciated! Thank you!,"['multiple-integral', 'calculus', 'multivariable-calculus', 'integration', 'definite-integrals']"
1717273,How to determine if a sphere is locally isometric to a plane?,"Is a sphere of radius one locally isometric to a plane? (Briefly justify your answer.) How do I go about answering this question. Could someone provide an explanation to what exactly locally isometric means? Also is the radius value of 1 an irrelevant piece of information so basically what they are asking is whether sphere , regardless of the radius locally isometric to a plane? Any help would be much appreciated.","['spheres', 'plane-geometry', 'geometry']"
1717282,Construction of a Borel measurable function mapping Borel set to non-Borel set,"I hope to construct an example of a function $f: \mathbb{R}^1\to\mathbb{R}^1$, which is Borel measurable ($f^{-1}(B) \in \mathcal{B}^1, \forall B \in \mathcal{B}^1$), but does not preserve Borel measurability, i.e., an image of a Borel set is non-Borel. I tried as follows: First pick up a non-Borel set $N \subset [0, 1]$, then I define $f(x) = \sup\{y: y \le x, y\in N\}$. As a piecewise constant non-decreasing function, it is guaranteed that $f(x)$ is Borel measurable. And $f([0,1]) = N$ maps $[0,1]$ to a non-Borel set. Could anyone tell me whether it is a valid example or not? My example seems much simpler than the existing example for this problem, which involves Polish space etc that I cannot understand.","['real-analysis', 'lebesgue-measure', 'measure-theory']"
1717313,Solution of the first order differential equation $y'=\frac{x(x^2+y^2)^2}{4y}$.,I m stuck in finding the solution of the following differential equation $$y'=\frac{x(x^2+y^2)^2}{4y}.$$ Please give me some hint.,['ordinary-differential-equations']
1717315,Find a normal matrix with a given characteristic polynomial,"Find a normal matrix with characteristic polynomial $t^2 + 4$ and eigenspace
$E_{2i} = span  {(1 \ 3i)^t}$ Since any vector in different eignespaces are perpendicular to each other, so i compute $E_{-2i} = span  {(-3 \ i)^t}$. Then I conclude that the matrix is \begin{matrix}
        1 & 3i  \\
        -3 & i\\
        \end{matrix} Then when I try to calculate back the char. polynomial, I didn't get back the same ans. What's wrong with my ans? Thank you!","['matrices', 'eigenvalues-eigenvectors', 'linear-algebra']"
1717330,Parameter estimation - Holt's Two parameter Linear Exponential Smoothing,"The reference for the below equations can be found in the Link . Note that $k$ is the timestamp and $i$ is the $i^{th}$ entry of a vector or $(i,i)^{th}$ entry of a matrix, $F$ in this case Equation 1 \begin{equation}
\tilde{x}_{i}(k+1) = a_{i}(k) + b_{i}(k)
\end{equation}
where
\begin{equation}
\begin{aligned} 
a_{i}(k) &= \alpha_{i}x_{i}(k) + (1-\alpha_{i})\tilde{x}_{i}(k) \\
b_{i}(k) &= \beta[a_{i}(k) - a_{i}(k-1)] + (1-\beta_{i})b_{i}(k-1) \\
\end{aligned}
\end{equation}
Equation 1 can also be written using the dynamic model as \begin{equation} 
\begin{aligned}
\tilde{x}_{i}(k+1) &= F_{i}(k)x_{i}(k)  + G_{i}(k) 
\end{aligned}
\end{equation} From the above equations we have \begin{equation}
\begin{aligned}
F_{i}(k) &= \alpha_{i}(1+\beta_{i}) \\  
G_{i}(k) &= (1+\beta_{i})(1-\alpha_{i})\tilde{x}_{i}(k) - \beta_{i}a_{i}(k-1)+(1-\beta_{i})b_{i}(k-1)
\end{aligned}
\end{equation} I've the data, .i.e the x(k) which is time series data (k = 1,2,$\dots$,n) and I want to estimate the values of $\alpha$ and $\beta$. In my case the values of $\alpha$ and $\beta$ are assumed to be constant throughout the interval $n$. I looked online but couldn't find any suitable reference.  Please help.","['statistics', 'estimation', 'dynamical-systems']"
1717356,Can anyone give a combinatorial proof of the identity ${n \choose m} + 2{n-1 \choose m}+3{n-2 \choose m}+...+(n-m+1){m \choose m}={n+2 \choose m+2}$,Can anyone give a combinatorial proof of the identity $${n \choose m} + 2{n-1 \choose m}+3{n-2 \choose m}+\ldots+(n+1-m){m \choose m}={n+2 \choose m+2}$$ I am finding difficult as $n$ is varying instead of $m$,"['combinatorics', 'summation', 'binomial-coefficients', 'combinatorial-proofs']"
1717358,Example for unbounded Lipschitz function on a bounded domain,"Let $D\subseteq \mathbb{R}^2$ be a bounded open rectangle in the plane.
(You can assume $D=(0,1) \times (0,1)$). Let $f:D \to \mathbb{R}$ be a continuous function which is uniformly-Lipschitz in the second variable $y$, i.e there exists $K>0$ such that $$|f(x,y_2)-f(x,y_1)|\le K|y_1-y_2|  \, \, \forall x,y_1,y_2 \in (0,1)$$ Is it true that $f$ is bounded on $D$?","['multivariable-calculus', 'real-analysis', 'lipschitz-functions']"
1717371,Noetherian rings have only finitely many minimal prime ideals. [duplicate],"This question already has answers here : How to prove a commutative, with unit, Noetherian ring $A$ only has finitely many minimal prime ideals via the following step? (2 answers) Closed 8 years ago . We say that $p$ is minimal prime if It does not contain any other prime. Assume that $A$ is Noetherian ring Question: $A$ has only finitely many minimal primes. any suggestions please","['abstract-algebra', 'noetherian', 'ideals']"
1717388,Abundence of smooth curves on a normal variety?,"If $X$ is a normal variety, and $p \in X$, is it true that there is a curve $C \subset X$ with $p \in C$ a smooth point (on C)? It is obviously false if normality is dropped - take $X$ to be a singular curve. I have no specific reason beyond this to request normality, though the examples I am familiar with pass this test. I somewhat doubt that this could be true, so I am asking for a counter example. (The vague motivation is that I am thinking about the valuative criterion for separatedness recently, and would like to understand the intuition that there are no curves $C$ with double points on a separated scheme - i.e with two centers for the same valuation on $C$. And I like DVRs, though I guess one can just take an arbitrary curve passing through the point and take its normalization to get a discrete valuation on the curves function field with some prescribed center. I am still curious about the geometric question anyway.) The other side of this question: Is there a (normal) variety $X$ with a singularity so ""bad"" that all curves passing through that point acquire that singularity?",['algebraic-geometry']
1717390,"If $f$ is a polynomial and $g(n+1)-g(n)=f(n)$, then $g$ is a polynomial. [closed]","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Assume that $f$ is a polynomial of degree $s$ which is not constant, and that for sufficiently large positive integers $n$, $g(n+1)-g(n)=f(n)$. Here $g$ is defined on the positive integers.  Must $g$ be a polynomial of degree $s+1$? This is an ambiguous statement in Atiyah and MacDonald, page 119.","['polynomials', 'functions', 'commutative-algebra']"
1717398,Why is the sine/cosine of an angle equal to its supplement?,I just got my hands on a trig text and I've been studying the law of sines and cosines so I can solve triangles other than right triangles. Something I've found odd while studying proofs of these theorems are the statements that the sine/cosine of an angle is equal to its supplement. This does not seem intuitive to me and I'm having a hard time understanding how the sine of a 45 degree angle can equal the sine of a 135 degree angle. Can someone please explain this concept to me? Thanks,['trigonometry']
1717409,Collinearity of points in a projective setting,"Let $ ABC$ be a triangle and $P$ a random point on the same plane as the triangle. Let $l$ be a line passing through $P$. Let $A_1,B_1,C_1$ be the intersection points of $BC,CA,AB$ with the reflections of $AP,BP,CP$ across line $l$ in respective order. Prove that $A_1,B_1,C_1$ are collinear. First reflect $\Delta ABC$ across $l$. Suppose the image is $A'B'C'$. Now notice that these two triangles are perspective from the point at infinity. Hence the corresponding sides intersect at points that are collinear. These collinear points lie on $l$. Furthermore, let $PA',PB',PC'$ intersect $BC,CA,AB$ at $A_1,B_1,C_1$ respectively. We have to show that these three points are collinear. After that, I have tried to look for more perspective triangles but haven't been able to find anything that helps me. How do I proceed? Am I overthinking? Thanks in advance.","['projective-geometry', 'geometry']"
1717423,Arzela-Ascoli Theorem on metric spaces,"I've been looking for a proof of one particular direction of this theorem for metric spaces. I've looked online, but everyone seems to use different terminology/notation to state the theorem, so I'd like to ask for an outline of a proof specific to my text's version, which my text ""leaves to the reader"". Let $A \subset M$ where $A$ is compact and $M$ is a metric space. Let $B \subset C_{b}(A,N)$ where $N$ is a metric space and $C_{b}(A,N)$ is the set of all bounded continuous functions from $A$ to $N$. Then $B$ is compact if and only if $B$ is equicontinuous, closed, and pointwise compact. I'm trying to prove the forward direction. The reverse direction (showing compactness) is based on the diagonalization argument, which is described well in the textbook, but the text makes no remarks on the forward direction. I already managed to prove pointwise compactness, and closure, which were trivial, but equicontinuity seems difficult. Could someone provide an outline of the proof, or link me to the proof of this particular version of the theorem? Remarks: - The metric on $B$ is defined by $d_{B}(f,g) = sup(d_{N}(f(x),g(x)|x \in A) $ where $d_{N}$ is the metric on $N$. So compactness of $B$ is defined relative to this metric $d_{B}$. (Showing $d_{B}$ is a metric is trivial).","['arzela-ascoli', 'proof-writing', 'general-topology', 'metric-spaces', 'analysis']"
1717446,How do I manipulate the sum of all natural numbers to make it converge to an arbitrary number?,"I just found out that the Riemann Series Theorem lets us do the following:
$$\sum_{i=1}^\infty{i}=-\frac{1}{12}$$But it also says (at least according to the wikipedia page on the subject) that a conditionally convergent sum can be manipulated to make it look like it converges into any real number. My question is then: Is there a general algorithm for manipulating this series into an arbitrary number? My knowledge about series and number theory is pretty limited so if I'm in over my head or if the answer is just too complicated I'd appreciate some tips on what to read up on! Thanks!","['conditional-convergence', 'summation', 'sequences-and-series', 'convergence-divergence']"
1717453,Non-geometric Proof of Pythagorean Theorem [closed],Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 8 years ago . Improve this question Is there a purely algebraic proof for the Pythagorean theorem that doesn't rely on a geometric representation? Just algebra/calculus. I want to TRULY understand the WHY of how it is true. I know it works and I know the geometric proofs.,['geometry']
1717465,Definition of weak time derivative,"My quesion involves the weak time derivative. In the book: 'Partial Differential Equations' by Evans the time derivative $u'$ of a function $u: [0,T] \rightarrow H^1_0(U)$ is defined by an element $u_t \in L^2(0,T;H^{-1}(U))$ such that
$$ \forall \phi \in C_0^{\infty}([0,T]):  \int_{[0,T]} u \phi' dt = -\int_{[0,T]} u' \phi dt $$
Where $u'\phi$ and $u \phi'$ has to be seen as scalar multiplications. 1) First I want to ask why we have to consider $u'(t)$ to belong to $H^{-1}(U)$ for every $t$ and not as an element of $H_0^1(U)$? 2) Secondly I want to ask how to interpret the above equation. The left-hand side belongs to $H_0^1(U)$ and the right-hand side to $H^{-1}(U)$.
Does the equality in the definition means that
$$ \forall \phi \in C_0^{\infty}([0,T]), \forall w \in H_0^1(U):  \int_U \left( \int_{[0,T]} u \phi' dt \right) w \: dx = \left( \int_{[0,T]} u' \phi dt \right) (w) $$
?
Where I've let each side act on $w$ in the natural way.","['bochner-spaces', 'partial-differential-equations', 'functional-analysis', 'weak-derivatives', 'sobolev-spaces']"
1717471,Geometric intuition of graph Laplacian matrices,"I am reading about Laplacian matrices for the first time and struggling to gain intuition as to why they are so useful. Could anyone provide insight as to the geometric significance of the Laplacian of a graph? For example, why are the eigenvectors of a Laplacian matrix helpful in interpreting the corresponding graph?","['graph-theory', 'matrices', 'algebraic-graph-theory', 'graph-laplacian', 'spectral-graph-theory']"
1717526,Tight lower bound for logarithm function,"Is there a lower bound for the logarithm function which is tighter than,
$$\log(x)\geq 1-x^{-1}$$
that works for all real values of $x>0$?","['logarithms', 'functions']"
1717536,Proving that $\cos\frac{2\pi}{13}+\cos\frac{6\pi}{13}+\cos\frac{8\pi}{13}=\frac{\sqrt{13}-1}{4}$,"How can I prove that: 
$\cos\frac{2\pi}{13}+\cos\frac{6\pi}{13}+\cos\frac{8\pi}{13}=\frac{\sqrt{13}-1}{4}$ Without using complex numbers? I tried to raise by 2 and to multipy by 2, and got: $2y^2=3+3\cos\frac{4\pi}{13}+2\cos\frac{10\pi}{13}+\cos\frac{12\pi}{13}+ 2\cos\frac{14\pi}{13}+2y$ But I'm stuck from here. Thanks.","['algebra-precalculus', 'substitution', 'trigonometry', 'quadratics']"
1717542,A function is convex if and only if its gradient is monotone.,"Let a convex $ U \subset_{op} \mathbb{R^n} , n \geq 2$, with the usual inner product. A function $F: U \rightarrow \mathbb{R^n} $ is monotone if $ \langle F(x) - F(y), x-y \rangle \geq 0, \forall x,y \in \mathbb{R^n}.$ Let $f:U \rightarrow \mathbb{R}$ differentiable. Show that $f$ is convex $\iff \nabla f:U \rightarrow \mathbb{R^n}$ is monotone. My attempt on the right implication: I already proved that if $f$ is convex and 2-differentiable then $f''(x) \geq 0$. But this exercise only says f is 1-differentiable.
Then I tried the following:
$f$ is convex $\iff \forall x,y \in U $ the function $\varphi:[0,1] \rightarrow \mathbb{R}$, defined by $ \varphi(t) = f((1-t)x+ty)$ is convex. Then $\varphi'$ is non-decreasing, then $\nabla \varphi(x) \geq 0$... but I'm stucked here. My attempt on the left implication: $ |\nabla \varphi (x) - \nabla \varphi (y)|| x-y| \geq | \langle \nabla \varphi (x) - \nabla \varphi (y), x-y \rangle | \geq 0$ And so $ |\nabla \varphi (x) - \nabla \varphi (y)| \geq 0 $ then $\nabla \varphi $ is non-increasing and then (By an already proved Theorem)
 it is convex. Can someone please verify what I did and give me a hint? Thanks.","['real-analysis', 'convex-analysis', 'multivalued-functions']"
1717581,Relation between moduli space of flat connections and moduli space of bundles on curves,"I would like to let $X$ be a genus $g$ curve, with $M_{g}(r,d)$ the moduli space of bundles on the curve such that $(r,d)=1$. Alternatively, we can pick a group $G$, consider principal $G$-bundles $P \to X$ with $\mathcal{A}(P)$ and $\mathcal{A}_{\rm{flat}}(P)$ the infinite dimensional spaces of connections, and flat connections on $P$, respectively.  We let $\mathcal{G}(P)$ be the gauge group consisting of all equivariant maps $u:P \to G$.  The holonomy then produces the well-known isomorphism $\coprod _{[P]} \mathcal{A}_{\rm{flat}}(P)/\mathcal{G}(P) \simeq \rm{Hom}(\pi_{1}(X), G) / G$. My first question pertains just to the left-hand side of this expression (i) So I guess $\mathcal{A}_{\rm{flat}}(P)$ and $\mathcal{G}(P)$ are infinite-dimensional spaces but the quotient turns out to be finite dimensional. Then you take the disjoint union over all equivalence classes of bundles $P \to X$.  How do we know this is well-defined, or finite dimensional after taking the disjoint union? My second question is how this relates to the moduli space of stable bundles: (ii) Taking the case where $G=\rm{U}(r)$ or $\rm{SU}(r)$, is it true that $M_{g}(r,d) \simeq \rm{Hom}(\pi_{1}(X), \rm{U}(r))/\rm{U}(r)?$ or perhaps is something similar true, like if we restrict to bundles of fixed determinant, and let $G=\rm{SU}(r)$?  What I have in mind is the commutator map $G^{2g} \to G$ which allows you to identify $M_{g}(r,d)$ as a subspace of $\rm{U}(r)^{2g}$ modulo $\rm{U}(r)$ acting by conjugation. Really, I'm just trying to work out a precise correspondence between the moduli space of flat connections and the moduli space of stable bundles on curves.","['complex-geometry', 'algebraic-geometry']"
1717657,interior of convex hull relatively open,"Consider $k+1$ affinely independent vectors $\left\{p_0,p_1, \dots, p_k \right \}$ in $n$-dimensional euclidean vector space $n>k$ and consider their convex hull. It is known that each point $x$ of it can be written in form $x = \sum_{i=0}^k \lambda_ip_i, \sum_{i=0}^k \lambda_i = 1, \lambda_i \geq 0, 0 \leq i \leq k$. Interior of it is defined requiring for $\lambda_i, 0 \leq i \leq k$  to be positive . How could I prove that this interior is open in relative topology? My try: Pick $x = \sum_{i=0}^k \lambda_ip_i, \sum_{i=0}^k \lambda_i = 1, \lambda_i > 0, 0 \leq i \leq k$. I want to find $\epsilon$ ball around it s.t. $\forall t\in B_\epsilon (x), t=\sum_{i=0}^k \mu_ip_i, \sum_{i=0}^k \mu_i = 1, \mu_i > 0, 0 \leq i \leq k $ We first perturbe $\lambda_0$: Take $\epsilon_0 = \frac{1}{2}\min\left\{\lambda_0, 1-\lambda_0, 1-\sum_{i=1}^k \lambda_i\right\}>0$ (motivation for taking such is the fact $0<\lambda_0<1$) Pick arbitrary $\mu_0$ in $[\lambda_0 - \epsilon_0,\lambda_0 + \epsilon_0]$. It is easy to show that in this case $0<\mu_i<1$ and $\mu_0+\sum_{i=1}^k \lambda_i <1$. Idea is to continue this procedure taking $\epsilon_1 = \frac{1}{2}\min\left\{\lambda_1, 1-\lambda_1, 1-\sum_{i=2}^k \lambda_i - \mu_0 \right\}>0$. In this step I easily show that for $\mu_1$ in $[\lambda_1 - \epsilon_1,\lambda_1 + \epsilon_1]$ we have $0<\mu_1<1$, but struggle to prove $\mu_0 + \mu_1 + \sum_{i=2}^k \lambda_i < 1$. If I eventually prove this and continue to go on like this (changing lambdas), I would then take my $\epsilon = \min_{i=0,...,k} \epsilon_i$ which in max-norm should fit to prove the desired fact. Am I completely wrong? Does anyone have advice/more elegant idea? Since I don't have much theory developed, I have to go to find $\epsilon$-ball around $x$ with such properties, but didn't manage to find more sophisticated thing.","['general-topology', 'convex-geometry', 'real-analysis', 'convex-analysis']"
1717660,How to solve an integratation involved an unknown function?,"Can anyone have any suggestions how to solve this equation for $w_i$, that is, what is the solution of $w_i$?
$$
\int_0^\infty e^{\Phi^{-1}(w_i)ε_i}P(r_i│ε_i )f(ε_i )dε_i=δ
$$ Where, $f(ε_i)$ is the probability density function of $ε_i$, $ε_i$ can be exponential random variable, $\Phi^{-1}(w_i)$ is the inverse of the normal CDF, $P(r_i│ε_i )$ is an unknown function, and all the remaining parameters are constant. I know it may not be possible to have an exact solution. However, there might have some approximation or clever trick, or possibility to get upper and lower limit of $w_i$. Thank you I appreciate your help :) Or, can we write the above equation as
$$
\int_0^\infty e^{\Phi^{-1}(w_i)ε_i}P(r_i│ε_i )f(ε_i )dε_i=δ\int_0^\infty f(ε_i )dε_i
$$
and hence by cancelling out the integrations from the both sides
$$
e^{\Phi^{-1}(w_i)ε_i}P(r_i│ε_i )=δ
$$ 
because $\int_0^\infty f(ε_i )dε_i=1$","['statistics', 'statistical-inference', 'calculus', 'discrete-mathematics']"
1717675,Verlinde Formula in geometric quantization?,"I think I have a fair grasp on the $\rm{SU}(2)$ Verlinde Formula from the algebraic geometry perspective.  I'm hoping to understand better how exactly this relates to the geometric quantization of a symplectic manifold. In algebraic geometry, we consider $N_{g} = N_{g}(2,L)$ to be the moduli space of rank-2 vector bundles over a curve.  One can show that $c_{1} : \rm{Pic}(N_{g}) \to H^{2}(N_{g}, \mathbb{Z})$ is an isomorphism, so there exists a positive generator of the Picard group $\mathcal{L}$, and the famous $\rm{SU}(2)$ Verlinde formula computes the integer $h^{0}(N_{g}, \mathcal{L}^{k})$, using Riemann-Roch, Kodaira Vanishing, etc. However, in geometric quantization, we take a symplectic manifold $X$, a pre-quantum line bundle $\mathcal{L}'$ on $X$ whose curvature precisely coincides with the underlying symplectic structure $[\omega] \in H^{2}(X, \mathbb{Z})$.  We can then hope to compute dimension of Hilbert space, algebra of operators, etc, after quantization. It turns out that $N_{g}$ is a symplectic manifold.  So if we fix its symplectic structure, can we consider that special generating line bundle $\mathcal{L}$ to be a pre-quantum line bundle?  If so, can we choose somehow the curvature to coincide with the symplectic structure?  As far as the Verlinde formula in this other context, am I correct to say it exactly computes the dimension of the Hilbert space (assuming compact)?  I guess you often hear the dimension of the Hilbert space is approximately the volume of the symplectic space.  But I guess Verlinde computes the dimension of Hilbert space exactly, and some limit of the formula gives the volume of phase space? Thanks in advance!","['complex-geometry', 'algebraic-geometry']"
1717685,"Completion of a local ring, Vakil 29.3A","If $p$ is a point of $X$, which is a $\bar{k}$ variety of dimension $1$, $p$ is a node if the completion of $\mathcal{O}_{X,p}$ at $m_{X,p}$ is isomorphic to $\bar{k}[[x,y]]/(xy)$. If now $\text{char}(\bar{k}) \neq 2$, show the curve $y^2-x^2-x^3$ has a node at the origin. So we need to compute the completion of the local ring $(k[x,y]/(y^2-x^2-x^3))_{(x,y)}$ at the maximal ideal generated by $x,y$. I kind of guess it is $k[[x,y]]/(y^2-x^2)$, but do not know how to prove this!","['algebraic-geometry', 'commutative-algebra']"
1717692,What do the closures of cyclic groups in $\textrm{GL}_n$ look like?,"Let $k$ be algebraically closed, $G = \textrm{GL}_n$ in the Zariski topology, and let $g \in G$.  Let $H$ be the subgroup generated by $g$.  Assume that $g$ does not have finite order. Question: What are the possible dimensions of the subgroup $\overline{H}$? If $g$ is diagonalizable, then $H$ sits inside the group of diagonal matrices, hence $\overline{H}$ is of dimension $\leq n$.  In general, we can assume $H$ consists of upper triangular matrices, so the dimension of $\overline{H}$ is bounded above by the dimension of the group of upper triangular matrices. Another thing I noticed is that if $n > 1$, and $k$ is uncountable, then $H$ and hence $\overline{H}$ are never irreducible.  This is because $H$ is countable, and a countable set in Zariski $n$-space is never irreducible unless it is a singleton set (projection onto affine coordinates is a morphism of varieties, allowing you to reduce to the case of a countable irreducible subset of $\mathbb{A}^1(k)$).","['algebraic-groups', 'algebraic-geometry']"
1717713,Show that the eigenvalues of a unitary matrix have modulus $1$,"Show that the eigenvalues of a unitary matrix have modulus $1$ . I know that a unitary matrix can be defined as a square complex matrix $A$ , such that $$AA^*=A^*A=I$$ where $A^*$ is the conjugate transpose of $A$ , and $I$ is the identity matrix. Furthermore, for a square matrix $A$ , the eigenvalue equation is expressed by $$Av=\lambda v$$ If I use the relationship $(u v)^*=v^*{u^*}$ and take the conjugate transpose of this equation then $$v^*A^*=\lambda^*v^*$$ But now I got stuck. Can someone help?","['eigenvalues-eigenvectors', 'matrices', 'unitary-matrices', 'proof-writing', 'linear-algebra']"
1717742,Proving a trigonometric equation,"Knowing that :    $$ \sin t - \cos t = x $$ Prove that :    $$ \cos^3t - \sin^3t = \frac{x}{2}(3-x^2)  $$ I tried to solve it by the important corresponding $$ a^3 - b^3 = (a-b)(a²+ab+b²) $$ 
But I got stuck in the middle and I don't even know if it's correct what I did",['trigonometry']
1717763,Hartshorne Lemma V.1.3 meaning of exact sequence,"I've been trying to make sense of the exact sequence in Lemma 1.3 chapter 5. The Lemma is the following: Let $C$ be a smooth irreducible curve on a smooth projective surface X, and let $D$ be any curve meeting $C$ transversally. Then $$\# (C \cap D) = \text{deg}_C(\mathcal{L}(D) \otimes \mathcal{O}_C)$$ Hartshorne claims the result is deduced from the exact sequence $$0 \to \mathcal{L}(-D) \otimes \mathcal{O}_C \to \mathcal{O}_C \to \mathcal{O}_{C \cap D} \to 0.$$ I'm trying to get the details down for how he gets this exact sequence. Let $i: D \to X$ be the inclusion map. Then we get the exact sequence $$0 \to \mathcal{L}(-D) \to \mathcal{O}_X \to i_*\mathcal{O}_D \to 0.$$ Let $j: C \to X$ be another inclusion map. Then by tensoring with $j_*\mathcal{O}_C$, we get $$0 \to \mathcal{L}(-D) \otimes j_*\mathcal{O}_C \to j_*\mathcal{O}_C \to i_*\mathcal{O}_D \otimes j_*\mathcal{O}_C \to 0.$$ The first map is injective since $C$ and $D$ intersect transversally, so this is indeed an exact sequence. In order to get the result, we need to apply $j^*$ to the exact sequence. Then we get $$0 \to j^*(\mathcal{L}(-D) \otimes j_*\mathcal{O}_C) \to \mathcal{O}_C \to j^*(i_*\mathcal{O}_D \otimes j_*\mathcal{O}_C) \to 0.$$ We can look at the stalks to see that this is exact as well. If $p : C \times_X D \to C$ is the canonical map coming from the fiber product, how do I see that $j^*(i_*\mathcal{O}_D \otimes j_*\mathcal{O}_C) \cong p_*\mathcal{O}_{C \times_X D}$ and $j^*(\mathcal{L}(-D) \otimes j_*\mathcal{O}_C) \cong \mathcal{L}(-D) \otimes \mathcal{O}_C$? Does he mean that $\mathcal{L}(-D) \otimes \mathcal{O}_C = j^*\mathcal{L}(-D)$? If so, I see that the stalks are the same by just dealing with the presheaves, but is there some slick way to see this other than actually writing the maps down? Are there some universal property tricks that will yield the results?",['algebraic-geometry']
1717775,"Fourier transform of a probability measure, and fourier transform of density","I have defined, for a probability measure $\eta$ we have the fourier transform as $\hat{\eta} = \int e^{itx} \ d\eta(x)$, and for a function $h: \mathbb{R} \to \mathbb{R}$ we have that the fourier transform of $h$ is given by $\hat{h}(t) = \int e^{itx} h(x) \ dx$. I then am told that the fourier transform of a probability measure, is the fourier transform of its density if it has one. I don't really understand this statement - what is the density of a measurable function? I thought random variables are those to have densities. Any explanation please, thanks.","['real-analysis', 'measure-theory', 'probability-theory']"
1717787,Is $\mathbb{C}^{\times}/\mathbb{R}^{\times}$ isomorphic to $\mathbb{C}^{\times}/\{\pm 1\}$,"It's known that $\mathbb{C}^{\times}/\mathbb{R}^{\times}$ is isomorphic to $S^1/\{\pm1\}$ (and the isomorphism is given by $\varphi(z)=z/|z|\{\pm 1\})$. Also, $\mathbb{C}^{\times}$ is isomorphic to $S^1$. So a natural question came to me (witohut no natural answer): is true that $\mathbb{C}^{\times}/\mathbb{R}^{\times}$ isomorphic to $\mathbb{C}^{\times}/\{\pm 1\}$? If the answer is yes, how is the isomorphism given (I tried by First Isomorphism Theorem and by writing explicitly without any success), and if not, how to prove it?","['abstract-algebra', 'group-theory', 'group-isomorphism']"
1717811,"Does $\lim_{(x,y)\to(0,0)}[x\sin (1/y)+y\sin (1/x)]$ exist?","This is an exercise from my calculus class.
The function is defined as $x\sin (1/y)+y\sin (1/x)$ if $x\neq0 $ and $y\neq0 $, and $0$ if $x=0 $ or $y=0$. I'm pretty confident the limit exists and should be $0$, because: $$\lim_{(x,y)\to(0,0)}[x\sin (1/y)+y\sin (1/x)]=\lim_{(x,y)\to(0,0)}[x\sin (1/y)]+\lim_{(x,y)\to(0,0)}[y\sin (1/x)]$$ And: $x\leq x\sin(1/y)\leq x$,
so $\lim_{(x,y)\to(0,0)}[x\sin (1/y)]=0$ right?
(The same can be said for $\lim_{(x,y)\to(0,0)}[y\sin (1/x)]$) However, I tried checking my answer, and according to Wolfram Alpha the limit doesn't exist.
Is this because I'm wrong, or is it just because $x\sin (1/y)+y\sin (1/x)$ is undefined for $y\neq0 $ and $y\neq0 $","['multivariable-calculus', 'limits']"
1717903,Probability and Statistics books for a prospective econometrician,"I've recently got accepted into a PhD Program in Economics and I'm looking for textbooks that can help me in my preparation. 
I need a book (or books) that fit my needs and that take into account my background. Needs:
-Mathematical rigor
-Introductory
-Suited to an Economics student
-I'm especially interested in Econometrics, so I would not want a book which focus heavily on topics studied by Statisticians and not by Econometricians. Background:
-I've taken one course on Real Analysis and some introductory lectures on Measure Theory. Topics:
Probability: multivariate random variables, expectation, conditional expectation, limit theorems.
Statistics: Estimation, properties of estimators, hypothesis testing, sample distributions, Asymptotic Theory Thanks!","['reference-request', 'statistics', 'probability']"
1717912,What does closed under convolution mean in Probability Theory?,"I understand what does it mean for a set to be closed under addition or multiplication, i.e. the sum/product of elements in a set, is still in a set. Now, I am a little bit confuse when it says the distribution (stable distribution) is closed under convolution. This is what I think it means, 
Let D be a family of stable distribution. For any $f, g \in D$, then $(f*g) \in D$ where $(f*g)$ is a convolution of $f$ and $g$, defined by $(f*g) = \int_{-inf}^{inf} f(\tau)g(t-\tau) d\tau $.","['probability-theory', 'probability', 'convolution', 'probability-distributions']"
1717915,Generalization of FTA,"I'm sure that this is not any hypothesis, but following came to my mind when I was reading complex analysis. Consider a function $f(z)=z^n+g(z)$, where $g(z)$ is continuous ( not necessary holomorphic) and it holds that $|g(z)/z^n|\rightarrow 0$, as $|z|\rightarrow \infty$. Question : How to proof that there is a $z_0 \in \mathbb{C}$ such that $f(z_0)=0$ ? I belive it must be true since if we set $z=re^{i\theta}$, then we have a closed curve $f_r(\theta)$, when $\theta \in [0,2\pi]$ and $r$ is fixed. Now $f_0(\theta)=a_0=(constant)$, but if $r$ is large enough we have $|f_r(\theta)-a_0|\geq |a_0|$, for all $\theta \in [0,2\pi]$, so due to continuity there must be fixed $r_0$ and $\theta_0$ such that $f_{r_0}(\theta_0)=f(z_0)=0$. I don't know how to make that intuitive proof rigorous but I think complex analysis methods are futile in this case.","['complex-analysis', 'general-topology', 'complex-numbers', 'analysis']"
1717949,Rational Maps Between Curves,"Let $F:C'\rightarrow C$ be a rational map. Then either $F$ is dominating, or $F$ is constant. Furthermore, if $F$ is dominating, then $k(C')$ is a finite algebraic extension of $\tilde{F}(k(C))$. Why is this? I can't seem to grasp what should be a simple question. I feel like there's a result about curves that could be useful, but I'm not seeing it. Any response is much appreciated.","['general-topology', 'algebraic-geometry']"
1717963,find exact value of $\sin(10^\circ)$,"How we can find exact value of $\sin(10^\circ)$? I tried trigonometric ways but I get this equation:
$\ 8y^3-6y+1=0$ and $y = \sin(10^\circ)$ and all the roots are complex . I saw the pages in site, but I can't find the solution.
Thanks",['trigonometry']
1718021,Intuition for volume of a simplex being $\frac 1{n!}$,"Consider the simplex determined by the origin, and $n$ unit basis vectors. The volume of this simplex is $\frac{1}{n!}$, but I am intuitively struggling to see why. I have seen proofs for this and am convinced, but I can't help but think there must be a slicker or more intuitive argument for why this is so than what I have already seen. Any help would be appreciated!","['volume', 'simplex', 'calculus']"
1718041,List all subgroups of $\mathbb Z_6$ and $\mathbb Z_8$,"List all the subgroups of $\mathbb Z_6$ and $\mathbb Z_8$. I think this implies that the operation is addition because that makes the sets above groups. I was thinking that for $\mathbb Z_6$, the groups $\mathbb Z_1$, $\mathbb Z_2$, $\mathbb Z_3$, $\mathbb Z_4$, and $\mathbb Z_5$ are all subgroups because they are all still groups and are subsets of $\mathbb Z_6$.  In addition though, $G={0,2,4}$ is also a subset of $\mathbb Z_6$ and is a group under addition. Is the above correct and exhaustive for $\mathbb Z_6$? Is there any easier way to do this or just to enumerate all possible subsets of the group and see if they satisfy group rules? (I saw something online for finding the subgroups of $\mathbb Z_n$ as a k such that $gcd(n,k)=1$ and then $\langle k \rangle$ is a subgroup but that didn't make much sense.  Is that relevant?)","['group-theory', 'cyclic-groups']"
1718097,A unit square contains 1 million rectangles without any common points. Show that the total area of rectangles is less than 1.,"""A unit square contains 1,000,000 rectangles without common points. Show that the total area of rectangles is less than 1 ."" This statement is somewhat imprecise. Let's say that these are closed rectangles, and ""without common points"" means that the rectangles are pairwise disjoint. This problem seems somewhat trivial to me, but I could be wrong. Is there any formal way to prove this?","['real-analysis', 'problem-solving', 'elementary-set-theory', 'geometry', 'recreational-mathematics']"
1718115,understanding of $d(\log f(z))$ in complex analysis,"In Gameline's Complex Analysis Chapter 8, the notation $d(\log f(z))$ is used: Here are my questions : In the real case, suppose for any $x\in\mathbb{R}$, one has $f(x)\neq 0$ and $f$ is differentiable. Then one has
  $$
d(\log(f(x))=\frac{f'(x)}{f(x)}dx
$$
  by the chain rule. But in the complex case, if the $\log$ function is not differentiable on the curve $\gamma$, how should one makes sense of $d\log f(z)$? Similarly, how should one understand $d\arg(z)$? $fdx+gdy$ is exact if $dh=fdx+gdy$ for some $h$ according to Gameline's book.  But what does ""$d\arg(z)$ is not exact"" mean?","['complex-analysis', 'differential-forms']"
1718166,Write the equation $4x^{2}+4z^{2}=5$ in spherical coordinates.,"Write the equation $4x^{2}+4z^{2}=5$ in spherical coordinates. I used the facts that $$
\begin{align}
x&=ρ\sin\theta\cos\phi\;,\\
z&=ρ\cos\phi\;,
\end{align}
$$ And ended up with: $ 4 (ρ^2 \sin^2(\phi) \cos(\theta) + ρ^2\cos^2(\phi))= 5 $ But it's not simplified enough? I can't use Pythagoras' theorem: $\cos^2(\theta) + \sin^2(\theta) = 1$ inside the parenthesis. So what do?","['multivariable-calculus', '3d', 'spherical-geometry', 'spherical-coordinates']"
1718184,Which are integral domains? Fields?,"Which of the following rings are integral domains? Which ones are fields? (a) $\mathbb{Z}[x]/(x^2 + 2x +3)$ (b) $\mathbb{F}_5[x]/(x^2+x+1)$ (c) $\mathbb{R}[x]/(x^4+2x^3 +x^2 +5x+2)$ For (a), $p(x) = x^2 + 2x + 3$ has no zero in $\mathbb{Z}$, so it is irreducible. This means $p(x)$ is maximal, and then $\mathbb{Z}[x]/p(x)$ is a field, also an integral domain. Similarly in (b), $x^2+x+1$ has no zero in $\mathbb{F}_5$, so $\mathbb{F}_5[x]/(x^2+x+1)$ is a field, also an integral domain. For the part (c), I think $x^4 + 2x^3 +x^2 +5x+2$ is irreducible, but I don't know how to prove it. Also, I am not sure about the way I prove the first two parts is correct or not. So could you please help me to figure it out? Thank you!!!","['abstract-algebra', 'ring-theory', 'integral-domain']"
1718190,$\limsup$ and limits in topological space,"I'm trying to generalise a result that holds for metric spaces. Let $(X,\tau)$ be a topological space and $f:X \to \mathbb{R}$. If $x_0 \in X$ is a limit point of $X$, define $$\limsup_{x \to x_0} f(x)
				= \inf\left\{ \sup_{x \in U \setminus \{x_0\}} f(x)\mid
					U \in \tau, U \setminus \{x_0\} \neq \emptyset\right\}$$ For $f:X \to \mathbb{C}$, is the following statement true? $$\lim_{x \to x_0} f(x) = c \iff \limsup_{x \to x_0} |f(x) - c| = 0$$ Here $\lim_{x \to x_0} f(x) = c$ is interpreted as for every net $\langle x_j \rangle_{j \in M} \to x, \langle f(x_j) \rangle_{j \in M} \to c$. I know that it is true for metric spaces, but can this be generalised for topological spaces? I think $T_2$ condition is necessary on $(X,\tau)$. Known facts that may help:
$f:X \to Y$ is continuous at $x \in X$ iff for every net $\langle x_j \rangle_{j \in M}$ converging to $x$, $\langle f(x_j) \rangle_{j \in M}$ converges to $f(x)$.","['general-topology', 'limsup-and-liminf', 'limits']"
1718202,"Show $\mathbb {R}[x,y]/(y^2-x, y-x)$ is not an integral domain","Let $\mathbb{R}[x,y]$ denote the polynomial ring in two variables $x$, $y$ over $\mathbb{R}$, and let $I = (y^2-x,y-x)$ be the ideal generated by $y^2-x$ and $y=x$. Show that $$\mathbb{R}[x,y]/I$$ is not an integral domain. To be honest, I have no idea to solve this question. I am thinking that if the $\mathbb{R}[x,y]/I$ is not an integral domain, $I$ is neither a prime ideal not a maximal ideal. But I don't know how to prove that. Could you please help me? Thank you very much!","['abstract-algebra', 'integral-domain', 'ideals']"
1718215,Why is cardinality of set of even numbers = set of whole numbers?,"I recently watched a YouTube video on Banach-Tarski theorem (or, paradox) . In it, the presenter builds the proof of the theorem on the basis of a non-intuitve assertion that there as as many even numbers as there are whole numbers, which he 'proves' by showing a 1:1 mapping between the two sets. But would that constitute a valid proof? To me, the number-density (per unit length of the number-line) for whole numbers is clearly more than that for even numbers. And this, I'm sure, can also be trivially proved by mathematical induction. Later, in the same video, it is shown how the interval [0,1] contains as many real numbers as there are in the real number line in its entirety. Once again, using the common-sense and intuitive concept of 'number-density', there would be clearly (infinitely) more real numbers in the entire number line than a puny little section of it. It seems, the underlying mindset in all of this is: Just ""because we cannot enumerate the reals in either set, we'll claim both sets to be equal in cardinality."" In the earlier case of even and whole numbers, just ""because both are infinite sets, we'll claim both sets to be equal in cardinality."" And all this when modern mathematics accepts the concept of hierarchy among even infinites! (First proposed by Georg Cantor?) Is there a good, semi-technical book on this subject that I can use to wrap my head around this theme, generally? I have only a pre-college level of knowledge of mathematics, with the rest all forgotten.","['infinity', 'elementary-set-theory']"
1718276,Moments of the number of roots of polynomials over finite fields,"Let $F:=\{f\in\mathbb{F}_q[X_1,\ldots,X_n]: \textrm{deg}(f)\leq d\}$ be the set containing all $n$-variate polynomials of degree less than or equal to $d$ over finite field $\mathbb{F}_q$ of prime power $q$. Suppose that $f$ is chosen uniformly at random from $F$ and let $N(f)$ be the number of distinct roots of $f$ in $\mathbb{F}_q$, namely $N(f)=\#\{(x_1,\ldots,x_n)\in\mathbb{F}_q^n:f(x_1,\ldots,x_n)=0\}$. I am interested in (the upper bound of) the $m$-th moment $\mathbb{E}[(N(f))^m]$ for $m\geq 1$. By Schwartz–Zippel lemma, there is an upper bound $\mathbb{E}[(N(f))^m]\leq (dq^{n-1})^m$. Is this the best possible upper bound?","['polynomials', 'abstract-algebra', 'probability', 'finite-fields']"
1718300,Use an expression for $\frac{\sin(5\theta)}{\sin(\theta)}$ to find the roots of the equation $x^4-3x^2+1=0$ in trigonometric form,"Question: Use an expression for $\frac{\sin(5\theta)}{\sin(\theta)}$ , ($\theta \neq k \pi)$ , k an integer to find the roots of the equation $x^4-3x^2+1=0$ in trigonometric form? What I have done By using demovires theorem and expanding $$ cis(5\theta) = (\cos(\theta) + i\sin(\theta))^5$$ $$ \cos(5\theta) + i \sin(5\theta) = \cos^5(\theta) - 10\cos^3(\theta)\sin^2(\theta)  + 5\cos(\theta)\sin^4(\theta) +i(5\cos^4(\theta)\sin(\theta)-10\cos^2(\theta)\sin^3(\theta) + \sin^5(\theta)$$ Considering only $Im(z) = \sin(5\theta)$ $$ \sin(5\theta) = 5\cos^4(\theta)\sin(\theta)-10\cos^2(\theta)\sin^3(\theta) + \sin^5(\theta) $$ $$ \therefore \frac{\sin(5\theta)}{\sin(\theta)} = \frac{5\cos^4(\theta)\sin(\theta)-10\cos^2(\theta)\sin^3(\theta) + \sin^5(\theta)}{\sin(\theta)}$$ $$ \frac{\sin(5\theta)}{\sin(\theta)} = 5\cos^4(\theta) -10\cos^2(\theta)\sin^2(\theta) + \sin^4(\theta)  $$ How should I proceed , I'm stuck trying to incorporate what I got into the equation..","['roots', 'trigonometry', 'complex-numbers']"
