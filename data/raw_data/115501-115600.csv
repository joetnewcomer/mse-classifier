question_id,title,body,tags
1693443,Interpretation of Eulerian numbers using the principle of inclusion-exclusion,"Eulerian numbers, denoted $e_{n,k}$, are defined as the number of permutations of $[n]$={1,2,...,n} such that there are k ""ascents"". (For example, the permutation 23541 of [5] would have 3 ascents, 235, 4,  and 1) How can the principle of inclusion exclusion be used so that: $e_{n,k}=\sum_{i=0}^k(-1)^i {n+1 \choose i}(k-i)^n$ I will update this as I make progress. We can think of the permutation as containing k groups of ascents. So, say for $e_{n,2}$, the first value of the alternating sum is ${n+1 \choose 0}(2-0)^n=2^n$ This makes sense since we are placing n objects into 2 groups. Now we need to make up for the fact that each group must be an ascent.","['inclusion-exclusion', 'combinatorics', 'eulerian-numbers']"
1693453,Bounding a solution of an ODE with a small source,"I have an ODE of the form $$ f''(x) + f(x) = \epsilon g(x)$$ with initial conditions $$ f(0) = f'(0) = 0 $$ $g(x)$ is $O(1)$ as $\epsilon \to 0$, and $g(x)$ is as smooth as necessary.  Is there a way I can bound $f(x)$?  In particular, I would like to be able to claim that $f(x)$ must be $O(\epsilon)$ as $\epsilon \to 0$ for all $x>0$, but I'm not sure if this is true or how to approach this.  Any hints/advice would be greatly appreciated. EDIT: I also know that $g(0) = g'(0) = 0$, if that helps with anything. EDIT2: Upon seeing the answers, I think we can see that the values of $g$ and $g'$ at $0$ are irrelevant as long as $g(x) = O(1)$.","['asymptotics', 'ordinary-differential-equations']"
1693478,Asymptotic expression for sum of first n prime numbers?,"Is one known? If not, what are the best known bounds? Is there reason to think that an asymptotic expression is beyond current methods if none exists?","['number-theory', 'analytic-number-theory', 'prime-numbers', 'sequences-and-series']"
1693490,"When is it ok to use ""="" instead of "":=""?","I am having difficulty whilst writing my equations, deciding when I should be using the equals symbol ""="" , and when I should be using the defined as symbol "":="". In particular, I am talking about equations for probabilities, when those probabilities are really approximations to the true probabilities. I am an engineer rather than a mathematician, so these approximations are common! For example, if I was using Bayes rule in my equation, I would write $p(x|y) = \frac{p(y|x)p(x)}{p(y)}$, because the two are equivalent. On the contrary, suppose I was using a very naive and heuristic score, and then using that score as a probability in my future calculations. I might write, for example, $p(x) := 0.5 + 0.5 * (z/10)$ where $z$ are some observations I have made about $x$, and $0 < z < 10$. This is because $p(x)$ is clearly just a score being forced into a probability, and so it would not be accurate to use the ""="" symbol. However, what about the cases between these two examples? Should you only use the ""="" symbol when the probability is derived purely and directly by maths? Or are there cases when the approximation is only small, and therefore ""="" is appropriate? The reason I ask is that in engineering, almost all probabilities are approximations, but it would look odd to have every single equation using the "":="" symbol. For example, suppose I decided to use a naive Bayesian model, where $x = \{a, b, c, d\}$. Would it be ok to write $p(x) = p(a)p(b|a)p(c|a)p(d|a)$, or would I have to write $p(x) := p(a)p(b|a)p(c|a)p(d|a)$? Is there a general rule as to how close to a pure approximation a probability definition has to be if you are going to use ""="" rather than "":=""?","['statistics', 'probability']"
1693493,Maximal twist in braid group product,"Suppose I have the $s_i$ and $s^{-1}_i$ as generators, satisfying the braid relations. I call the $s_i$ ""right twists"" and their inverses ""left twists"". Any element $w$ in the braid group can be written as a product of generators (but not in a unique way). Suppose this representation is reduced, meaning that the number of elements in this product is minimized.
Let $rt(w)$ be the maximal exponent of an element $s_i$
in all possible products representing $w$. 
Think of this as the maximal right-twisting of two strands.
Similarly, $lt(w)$ is the maximal exponent of an $s^{-1}_i$. Now, let $R$ be a product of right twists, meaning it is a product of entries in $\{s_i\}$ but not inverses of these,
and let $L$ be a product of left twists.
Now let $RL$ be the product of $L$ and $R$. I want to prove that $rt(R) \geq rt(RL)$
or equivalently, $lt(L) \geq lt(RL)$. In other words, adding left twists to a braid cannot increase the maximal right twist. A reference would be superb! This is not a homework problem, but for my research but it seems so simple that there should be an easy proof.","['permutations', 'group-theory']"
1693511,"in a non-full rank matrix, how to find the dependent rows especifically in MATLAB?","I have a very large matrix, which its determinant is zero, and hence it is not full rank. Now I wonder is there a way to find which rows are linearly dependent? specifically in MatLab is there any command which outputs the desired result?","['matrices', 'determinant', 'matlab', 'matrix-rank', 'linear-algebra']"
1693538,Ideal sheaf of the intersection of a projective scheme with a hypersurface,"Suppose $X$ is a closed subscheme in $\mathbb{P}^n$ and $H$ is a hypersurface in $\mathbb{P}^n$ with degree $d$, if $H$ does not contain the associate points of $X$, then $H \cap X$ is an effective Cartier divisor in $X$. Is its ideal sheaf in $X$ is $O_{\mathbb{P}^n}(-d)|_X$ (or just $O_X(-d)$ )? but I do not see why, could anyone explain this to me? Or this is true only when $X$ is already a complete intersection?",['algebraic-geometry']
1693543,"""topological operators"" on measurable subsets of $S\times \mathbb{R}$","Let $(S,\mathcal{S})$ be an arbitrary measurable space and $\mathcal{B}$ be the Borel-$\sigma$-Algebra over the reals $\mathbb{R}$. For $U\in\mathcal{S}\otimes \mathcal{B}$, the product-$\sigma$-Algebra over $S\times\mathbb{R}$,
let ${}_s U = \{ u : (s,u)\in U \} \in \mathcal{B}$ be the ""$s$-slice"" of U along $s\in S$ (it is Borel-measurable due to Fubini). For an arbitrary subset $R\subset \mathbb{R}$ denote by $R^{\,\text{int}} = \{ r\in \mathbb{R} : \exists \varepsilon>0 : (r-\varepsilon,r+\varepsilon) \subset R \}$ its interior. $R^{\,\text{cl}} = \{ r\in \mathbb{R} : \forall \varepsilon>0 : (r-\varepsilon,r+\varepsilon) \cap R \neq \emptyset \}$ its closure. $R^{\,\text{iso}} = \{ r\in R : \exists \varepsilon>0 : (r-\varepsilon,r+\varepsilon) \cap R = \{r\} \}$ the set of its isolated points. $R^{\,\text{ni}} = R\setminus R^{\,\text{iso}}$ the set of its non-isolated points. $R^{\,\text{lim}} = (R^{\,\text{cl}})^{\,\text{ni}}$ the set of its limit points. For $U$ one can define $U^{\,\text{int}}$ by applying the operator on every $s$-slice of $U$, i.e. $U^{\,\text{int}} = \bigcup_{s\in S} \{s\} \times ({}_s U)^{\,\text{int}}$; and in the same manner
$U^{\,\text{cl}}$, $U^{\,\text{iso}}$, etc. QUESTION: Which of these sets $U^{\,\text{int}}$, $U^{\,\text{cl}}$, etc. 
inherits the measurability from $U$ (i.e. is an element of $\mathcal{S}\otimes\mathcal{B}$) ? I tried the standard ways of using some monotone class argument, but I'm stuck. Thx. EDIT: mixed up some notations. Question comes from a probability theory context where $(S,\mathcal{S})=(\Omega,\mathcal{A})$ is the underlying measurable space. Any ""Omegas"" should be purged.","['general-topology', 'measure-theory']"
1693564,$41$-th derivative of $\sin(x^2-2x+2)$ at point $1$,"Problem: 
$$\frac{\mathrm d^{41}}{\mathrm dx^{41}}\sin(x^2-2x+2)$$ Can anyone help me with this because I tried to use General Leibniz rule(after I differentiate $\sin(x^2-2x+2)$) and I didn't get much better information and I also tried some proof by induction but also no success 
Without using Taylor.","['derivatives', 'real-analysis', 'calculus']"
1693570,Measurability of the zero-crossing time of Brownian motion,"I have the following random time $\tau = \inf\{t > 0: W_t = 0\}$ where $(W_t)_{t\geq 0}$ is Brownian motion with almost surely continuous paths and $W_0 = 0$ a.s. I need to prove that $\tau$ is measurable (not necessarily a stopping time or an optional time). I first fix $\omega \in \Omega' \subset \Omega$. On $\Omega'$ $W$ is continuous and null at $0$. Then, I claim $$\tau_0(\omega) > s \Leftrightarrow \lvert W_t(\omega) \rvert > 0 \quad \forall{t}, 0 < t \leq s$$
By continuity of $t \mapsto W_t(\omega)$,
\begin{equation}\lvert W_t(\omega) \rvert > 0 \quad \forall{t}, 0 < t \leq s \Leftrightarrow \lvert W_{q_t}(\omega) \rvert > 0 \quad \forall{q_t} \in Q_s \qquad (\triangle)\end{equation}
where $Q_s = \{qs \vert q \in (0,1] \cap Q\}$. Then 
$$\{\tau_0(\omega) > s\} = \underbrace{\cap_{q_t \in Q_s} \underbrace{\{\lvert W_{q_t} \rvert > 0\}}_{\in \mathcal{F}}}_{\in \mathcal{F}, \text{ by countable intersection}}$$ Now I don't think this is correct. In particular, $(\triangle)$ seems wrong. Take $s = 1$ for example. Then consider the function $t \mapsto -t/r + 1$ where $r \in [0,1]\setminus \mathbb{Q}$. Can someone help me fix this proof?","['brownian-motion', 'measure-theory', 'stopping-times']"
1693629,Uniform approximation by even polynomial,"Proposition Let $\mathcal{P_e}$ be the set of functions $p_e(x) = a_o + a_2x^2 +
 \cdots + a_{2n}x^{2n}$, $p_e : \mathbb{R} \to \mathbb{R}$ Show that all $f:[0,1]\to\mathbb{R}$ can be uniformly approximated by
  elements in $\mathcal{P_e}$ Attempt: Since we are talking about polynomials let's try play with Weierstrass Theorem By definition $\mathcal{P_e}$ is dense in  $C^0([0,1], \mathbb{R})$ , if for every funciton $f \in C^0([0,1], \mathbb{R})$, $\exists p_e \in \mathcal{P_e}$ such that $\forall \epsilon > 0, \forall x \in [0,1], ||p_e-f||< \epsilon$ So we want to show $\|p_e - f\| < \epsilon$ Try something like...since every continuous function is approximated by polynomials i.e. $\forall \epsilon > 0,\ \|f - p\| < \epsilon$, therefore let $p$ be a polynomial, $p = a_0 + a_1x + a_2x^2 + a_3x^3 + \cdots$ $$\|p_e - f\| \leq \|f - p\| + \|p - p_e\|$$ $$\Rightarrow \|p_e - f\| < \epsilon + \|p_o\|,$$ where $p_o$ is an odd polynomial $$\Rightarrow \|p_e - f\| < \epsilon + \sup_x|a_1+a_3+\cdots+a_{2_n+1}|$$ Stuck. In any case, $p$ and $p_o$ is poorly defined. What would be the standard approach to prove the proposition?","['real-analysis', 'polynomials', 'functional-analysis', 'approximation-theory', 'weierstrass-approximation']"
1693635,The morphism defined by a linear system associated to a smooth curve of genus 2 on a $K3$ surface has degree 2 and its branch locus is a sextic.,"This is part of Proposition $VIII.13$ in Beauville's ""Complex algebraic surfaces"": Let $S$ be a $K3$ surface and $C \subset S$ a smooth curve of genus $g=2$. Then the morphism $\phi$ defined by the linear system $|C|$ is of degree 2, whose branch locus is a sextic of $\mathcal{P}^2$. The proof is not detailed. He just says that from the fact that $C^2 = 2$ it follows that the degree of $\phi$ is 2. Why is that? Moreover, $C$ is a double cover of a line $l = \phi(C)$ branched in $n$ points of $l \cap \Delta$, where $\Delta$ is the branch locus. Then he just says that $g=2$ implies $n=6$. Could you help me figure out what is implicit in these conclusions? Thanks.",['algebraic-geometry']
1693656,"Analytical solution for rational equality, square root in denominator on both sides","So I was trying to solve a problem I saw in a practice set for a 6th-grade math competition, as far as I can remember it. It was a story problem, but I think the solution is the minimum value of $$
\sqrt{x^2 + 25} + \sqrt{(5-x)^2 + 49}
$$ for $x$ in the interval $[0,5]$. I know the derivative at a minimum should be zero, so $$
{x \over \sqrt{x^2+25}} + {x-5 \over \sqrt{(5-x)^2 + 49}} = 0 
$$ I can see by experimentation that the answer is close to $13$, with $x$ close to $2.08$. However, I get stuck after that. If I write as $$
{x \over \sqrt{x^2+25}} = {5-x \over \sqrt{(5-x)^2 + 49}} 
$$ can I multiply both sides by the product of the denominators? Can I square both sides? When I do that, I get a fourth-degree polynomial on each side, but the fourth-degree and third-degree term cancel out; and the I'm left with a quadratic equation whose roots are nowhere near the interval I need. More generally, is there an analytical solution for $$
\min\left(\sqrt{x^2+y_0^2} + \sqrt{(x_0-x)^2+y_1^2}\right)
$$ where $y_0$, $y_1$, and $x_0$ are positive constants?",['algebra-precalculus']
1693669,Steady periodic solution to $x''+2x'+4x=9\sin(t)$,"Find the steady periodic solution to the differential equation
$x''+2x'+4x=9\sin(t)$ in the form
$x_{sp}(t)=C\cos(\omega tâˆ’\alpha)$, with $C > 0$ and $0\le\alpha<2\pi$. I don't know how to begin. First of all, what is a steady periodic solution? And how would I begin solving this problem?","['ordinary-differential-equations', 'calculus']"
1693685,What is the sum of the reciprocal of all of the factors of a number?,"Suppose I have some operation $f(n)$ that is given as $$f(n)=\sum_{k\ge1}\frac1{a_k}$$ Where $a_k$ is the $k$th factor of $n$. For example, $f(100)=\frac11+\frac12+\frac14+\frac15+\frac1{10}+\frac1{20}+\frac1{25}+\frac1{50}+\frac1{100}=\frac{217}{100}$ $f(101)=\frac11+\frac1{101}=\frac{102}{101}$ $f(102)=\frac11+\frac12+\frac13+\frac16+\frac1{17}+\frac1{34}+\frac1{51}+\frac1{102}=\frac{216}{102}$ I was wondering if it were possible to plot a graph of $f(n)$ and wondered if there were any interesting patterns.  I was also wondering if there is a closed form representation and if $\lim_{n\to\infty}f(n)$ could be evaluated or determined to be finite or not or any other interesting things that might happen in this limit. Secondly, I was wondering about another similar series, which considers $b_k$ as the $k$th prime factor of $n$. $$p(n)=\sum_{k\ge1}\frac1{b_k}$$ What can we determine about this series?","['factoring', 'number-theory', 'recreational-mathematics', 'summation', 'sequences-and-series']"
1693745,Generalizing $f(n)=\int_0^\infty \frac{1}{e^{x^n}+1}=\left(1-2^{(n-1)/n}\right )\zeta(n^{-1})\Gamma(1+n^{-1})$,"I have come up with the following solution to this integral, but is just incomplete to my standards
$$f(n)=\int_0^\infty \frac{1}{e^{x^n}+1}=\left(1-2^{(n-1)/n}\right )\zeta(n^{-1})\Gamma(1+n^{-1})$$ Seems to only work for $x\in\Bbb{N},x\gt 2$ This identity, therefore does not apply to $n=1$, and we all know that $f(1)=\ln 2$ because $\zeta(1)$ diverges. So my question is this: How can you generalize the integral solution I gave to fit the case $n=1$?","['zeta-functions', 'integration']"
1693750,Does equal cardinality and one set being a subset of the other prove equality? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question I'm currently solving a quite specific problem and in the final step I made a statement that can be generalised such that:
$$((|A|=|B|)\wedge(A\subset B)) \implies (A=B)$$
Whilst this is clearly intuitively true for finite sets $A$ and $B$ and probably a safe statement for my purposes I was wondering if: Is it mathematically sound to make such a statement for finite sets or should there be some further proof? Is it the statement mathematically true in general including infinite sets?","['examples-counterexamples', 'elementary-set-theory']"
1693761,What is wrong with my logic and derivatives?,"Since the derivative of a function is analogous to a description on how fast the function grows, I thought that $\lim_{x\to\infty}\frac{f'(x)}{f(x)}=0$ for the following reason. Assume $f(x)$ is monotone increasing on an interval $(a,\infty]$.  If $f'(x)$ is large, that means that $f(x)$ is getting larger and larger.  From this, I've assumed that $f'(x)$ must eventually be outdone by $f(x)$ simply because $f'(x)$ cannot get large without making $f(x)$ even larger. But that's obviously not the case because if $f(x)=\Gamma(x)$, then $\lim_{x\to\infty}\frac{f'(x)}{f(x)}\ne0$. I'm just wondering what is wrong with my logic, that's all.","['derivatives', 'soft-question']"
1693780,Derivative along a curve,"Suppose $M$ is a hypersurface of the sphere $S^n \subset \mathbb{R}^{n+1}$, and denote the riemannian connections of $M$, $S^n$ and $\mathbb{R}^{n+1}$ by $\nabla, \overline{\nabla}$ and $\tilde{\nabla}$, respectively. Given a differentiable curve $\alpha : (-\varepsilon, \varepsilon) \to M$ with $\alpha(0) = p$ and $\alpha'(0) = v \in T_p M$, how can I ""explicitly"" calculate $\overline{\nabla}_v \alpha'$ in terms of the well known derivatives in $\mathbb{R}^{n+1}$? Is it true that $\tilde{\nabla}_v \alpha' = \alpha''(0)$, so that $$ \overline{\nabla}_v \alpha' = \operatorname{proj}_{T_p S^n} (\alpha''(0)) = \alpha''(0) - \langle \alpha''(0), p \rangle p \,\text{ ?} $$ I am having trouble understanding covariant derivatives along curves. Any help will be appreciated.","['connections', 'riemannian-geometry', 'differential-geometry']"
1693782,"In how many dimensions is the full-twisted ""Mobius"" band isotopic to the cylinder?","There is a question on this site about the distinctions between the full-twisted Mobius band and the cylinder, but I would like to ask something different, so I start a new question. Let us call $C$ the standard cylinder embedded in $\mathbb{R}^3$, and call $F$ the full-twisted Mobius band (aka. the Mobius band with two ""half-twists"", or the Mobius band with a 360 degree twist), also embedded in $\mathbb{R}^3$. $C$ and $F$ are topologically homeomorphic to each other, but they are not isotopic within $\mathbb{R}^3$. Now think of $\mathbb{R}^3$ as a subspace of some higher dimensional Euclidean space $\mathbb{R}^n$ where $n\geq 3$, and ask if $C$ and $F$ are isotopic in $\mathbb{R}^n$. Obviously if $n \geq 6$ then $C$ and $F$ are isotopic. But what about $n = 4$ or $5$? Is there any chance that they are still isotopic, or can one prove that $n=6$ is the smallest number of dimensions in which $C$ and $F$ are isotopic? Any help or idea is appreciated. Thanks!","['geometric-topology', 'general-topology', 'mobius-band']"
1693784,determinant of a very large matrix in MATLAB,"I have a very large random matrix which its elements are either $0$ or $1$ randomly. The size of the matrix is $5000$, however when I want to calculate the determinant of the matrix, it is either $Inf$ or $-Inf$. Why it is the case (as I know thw determinant is a real number and for a finite size matrix with finite elements, it cannot be $Inf$) and how can I remedy any possible mistake?","['matrices', 'matlab', 'determinant']"
1693802,Cauchy's theorem for contour integration,"I have to compute $\int_C(z+\frac{1}{z})^{2n}\frac{1}{z}dz$, where $n \in \mathbb{N}$, and $C$ is the unit circle with positive orientation. So let $z(t)=\cos (t) + i \sin (t)$, with $-\pi \leq t < \pi$ $$\begin{align*}\int_C(z+\frac{1}{z})^{2n}\frac{1}{z}dz
&= \int^{\pi}_{-\pi}\left(z(t)+\frac{1}{z(t)}\right)^{2n}\frac{1}{z(t)}dt \\
&= 4^n \left(\int^{\pi}_{-\pi} (\cos (t)) ^{2n+1})dt -i  \int^{\pi}_{-\pi} (\cos (t))^{2n} \sin (t) dt \right)\end{align*}$$ Is there a faster way to compute this integral? Does Cauchy's Theorem help here?","['complex-analysis', 'integration', 'contour-integration']"
1693831,Polar forms of algebraic curves & surfaces,"A paper I'm reading says the following ... With homogeneous coordinates $\mathbf{x} = [x,y,z,w]$, let $F(\mathbf{x}) = 0$ be the equation of a surface of degree $n$. The first polar form of  $F(\mathbf{x})$ at the pole $\mathbf{a} = [a,b,c,h]$ is defined as
  $$
F^1(\mathbf{x}) = \frac1n(aF_x + bF_y + cF_z + hF_w)
$$ And then, later, there's a similar definition ... Let $\mathbf{p}(x)$ be a polynomial curve of degree $n$ and let $\mathbf{p}(x,w)$ denote its homogeneous form. Its first polar with respect to the pole $x_1$ is defined by
  $$
\mathbf{p}^1(x_1 \,|\,x) = \frac1n(x_1\mathbf{p}_x + \mathbf{p}_w)
$$ The paper says these concepts are ""well-known in algebraic geometry"". I'm having trouble understanding what these things mean geometrically. Could someone explain, or provide a reference, please. I'd like the explanation or reference to be something simple and concrete in 2D or 3D space, please; abstraction and generality probably won't help me.","['plane-curves', 'algebraic-geometry', 'geometry', 'algebraic-curves', 'surfaces']"
1693893,Is there a deeper meaning to the identity ${{\sin x}\over x} = \prod_{k=1}^{\infty} \cos\left({x\over{2^k}}\right)$?,"Is there any deeper meaning to trigonometric identity 
$${{\sin x}\over x} = \prod_{k=1}^{\infty} \cos\left({x\over{2^k}}\right)$$
beyond it corresponding to characteristic functions of i.i.d. random variables?","['algebra-precalculus', 'probability-theory', 'trigonometry', 'probability']"
1693912,How to prove $|y|\le |\sin(z)|\le e^{|y|}$ in complex analysis where $z=x+iy$?,How to prove $$|y|\le |\sin(z)|\le e^{|y|}$$ in complex analysis where $z=x+iy$. I don't think I need the entire solution. May be just a set up or approach. I started with sin(z) formula and reached a dead end or a reversed circle. I think the triangle inequality is useful for proving the second inequality and using a theorem if f(0) = g(0) and f(prime) (x) >= g(prime)(x) for x >=0 then f(x) >= g(x) for x>=0,['complex-analysis']
1693923,Solving modulus inequality $|x - 1| + |x - 6|\le11$ geometrically,Find all possible values of $x$ for which $x$ for which the inequality $$|x - 1| + |x - 6|\le11$$ is true. I know this can be easily solved by taking $3$ cases for$x$ and then taking the intersection of those $3$ cases. The solution will be $-2\ge x\le9$. But suppose if I interpret this in this way: What number $x$ satisfy the condition that the distance between $x$ and $6$ plus the distance between $x$ and $1$ is less than or equal to $11$? I would be better to get an idea to solve these types of problems by geometrically by intuition  using number line.,"['algebra-precalculus', 'inequality', 'absolute-value', 'geometry']"
1693945,"Find $\lim\limits_{n\to\infty}{\left(1+\frac{1}{n^k}\right)\left(1+\frac{2}{n^k}\right)\cdots\left(1+\frac{n}{n^k}\right) }$ for $k=1, 2, 3, \cdots$","First of all, I already searched Google, math.stackexchange.com... I know 
$$ \lim_{n\rightarrow\infty} \left( 1+ \frac{1}{n}      \right)      ^n=e$$ That is $$ \lim_{n\rightarrow\infty} \underbrace{\left(1+\frac{1}{n}\right)\left(1+\frac{1}{n}\right)\cdots\left(1+\frac{1}{n}\right) }_{\text{n times}}     =e$$ $$$$
At this time, I made some problems modifying above. $$ \lim_{n\rightarrow\infty} {\left(1+\frac{1}{n}\right)\left(1+\frac{2}{n}\right)\cdots\left(1+\frac{n}{n}\right) } =f(1)   $$ $$ \lim_{n\rightarrow\infty} {\left(1+\frac{1}{n^2}\right)\left(1+\frac{2}{n^2}\right)\cdots\left(1+\frac{n}{n^2}\right) }    =f(2)$$ $$ \lim_{n\rightarrow\infty} {\left(1+\frac{1}{n^3}\right)\left(1+\frac{2}{n^3}\right)\cdots\left(1+\frac{n}{n^3}\right) }    =f(3)$$ $$ \lim_{n\rightarrow\infty} {\left(1+\frac{1}{n^k}\right)\left(1+\frac{2}{n^k}\right)\cdots\left(1+\frac{n}{n^k}\right) }    =f(k)$$ $$$$
After thinking above, I feel I'm spinning my wheels with these limit problems. Eventually, I searched wolframalpha. And the next images are results of wolfram. (I take a LOG, because I don't know COMMAND of n-times product.) $$$$ These result (if we trust wolframalpha) say $$f(1)=\infty$$
$$f(2)=\sqrt{e}$$
$$f(3)=1$$
$$f(30)=1$$ NOW, I'm asking you for help. I'd like to know how can I find $f(k)$ (for $k=1,2,3,4, \cdots$ ). I already used Riemann sum, taking Log...  but I didn't get anyhing. ;-( Thank you for your attention to this matter. ----------- EDIT --------------------------------- The result for $f(1), f(2), f(3), f(30)$ is an achievement of Wolframalpha, not me. I'm still spinning my wheel, $f(1), f(2), f(3)$, and so on...","['infinite-product', 'limits', 'calculus', 'riemann-sum', 'sequences-and-series']"
1693972,"If two norms are not equivalent, then we have a sequence $\|x_n\|/\|x_n\|' \to 0$.","Definition 2.22. We say that two norms $\|\cdot\|$ and $\|\cdot\|'$ are equivalent if there exist $C_1,C_2>0$ such that $$C_1\|x_1\|' \le \|x\| \le C_2 \|x\|',$$ for all $x\in V$. If two norms $\|\cdot\|$ and $\|\cdot\|'$ are not equivalent, then how can we construct a sequence such that $\|x_n\|/\|x_n\|' \to 0$? From the definition, we have for all $C_1, C_2 \gt 0$ there is a $x\in V$ such that $C_1\|x\|'\gt \|x\|$ or $C_2\|x\|'\lt \|x\|$. If we have the first strict inequality for all cases then the construction is obvious, but we could get the second strict inequality, in which case I don't see a way to construct the sequence. So how is this possible? This question stems from the following proof in showing that if two norms are not equivalent then they do not generate the same topology. Now suppose that $\|\cdot\|$ and $\|\cdot\|'$ are not equivalent. Then without loss of generality there exists a sequence of points $x_n\in V$, $n\ge 1$, such that $\|x_n\|/\|x_n\|' \to 0$ as $n\to+\infty$. Set $y_n=x_n/\|x_n\|'$. Then $y_n\to0$ in $(V,\|\cdot\|)$ but $I(y_n)$ does not converge to $0$ in $(V,\|\cdot\|'$ (since $\|y_n\|'=1$ for all $n$). Thus $I$ is not continuous, so there exists $U$ which is $\|\cdot\|'$-open such that $U=I^{-1}(U)$ is not $\|\cdot\|$-open, i.e., the topologies are different.","['functional-analysis', 'normed-spaces']"
1693978,"How to find $a+b$ in square $ABCD,$?","In square $ABCD,$ $AB=1.$ $BEFA$  and $MNOP$ are congruent. $BE= a - \sqrt b.$ Where $a$ and $b$ are both primes. How to find $a+b$? I have no idea how to do this, can this be proved with simple geometry?","['prime-numbers', 'geometry']"
1693985,"a.s convergence is ""probabilistic"" (Breiman, Probability, p. 34)","Let $\{X_n\}$, $\{X'_n\}$ have the same distribution. Prove that if $X_n \xrightarrow{a.s} X$, there is a random variable $X'$ such that $X'_n \xrightarrow{a.s} X'$. Some of my classmates gave me a counterexample to me, but they were wrong to me. I want to prove it by showing that $\{X'_n\}$ is Cauchy. And for this, I need this assertions that I DON'T know whether is true or not : Conjecture. Suppose $X\overset{\mathcal{D}}{=}X'$ and $Y\overset{\mathcal{D}}{=}Y'$. Is it true that $X-Y\overset{\mathcal{D}}{=}X'-Y'$ ?","['probability-theory', 'convergence-divergence', 'probability-distributions']"
1693991,"In $C([0,1],\mathbb{R})$, the sup norm and the $L^1$ norm are not equivalent.","How does the proof here show that the two norms are not equivalent? We have that in the sup norm $f_n$ converges to $1$, and in the $L^1$ norm $f_n$ converges to $0$, but how does this mean that the two norms are not equivalent?","['functional-analysis', 'analysis']"
1694019,"Prove that $\frac{\sqrt{3}}{8}<\int_{\frac{\pi}{4}}^{\frac{\pi}{3}}\frac{\sin x}{x}\,dx<\frac{\sqrt{2}}{6}$","Prove that $$\frac{\sqrt{3}}{8}<\int_{\frac{\pi}{4}}^{\frac{\pi}{3}}\frac{\sin x}{x}\,dx<\frac{\sqrt{2}}{6}$$ My try: Using $$\displaystyle \sin x<x$$ and $$\frac{\sin x-0}{x-0}>\frac{1-0}{\frac{\pi}{2}-0}=\frac{2}{\pi}$$ So we get $$\frac{2}{\pi}<\frac{\sin x}{x}<1$$ So we get  $$\frac{2}{\pi}\int_{\frac{\pi}{4}}^{\frac{\pi}{3}}1\,dx<\frac{\sin x}{x}<1\cdot \int_{\frac{\pi}{4}}^{\frac{\pi}{3}}\,$$ But this is not what I have to prove here.",['integration']
1694112,Sequential definition of differentiation,"Let $f$ be a function continuous and derivable at $x $. Let ${u_n}$ and $v_n$ be two sequence such that   ${u_n}\leq x \leq {v_n}$. Show that $ |u_n - v_n |$ tends to zero implies  $$\frac{f(u_n)-f(v_n)}{u_n-v_n}=f'(x)$$ What I have tried is $ |u_n - v_n |$ tends to zero implies $ {v_n} ,  {u_n} $ converges  to $x$ since $f$ being continuous at $x$. We can use sequential criterion. From that how can we conclude that $$\frac{f(u_n)-f(v_n)}{u_n-v_n}=f'(x)$$","['derivatives', 'real-analysis', 'sequences-and-series', 'calculus']"
1694159,Finding the sum $\sum_{j=k}^n (-1)^{j+k}\binom{n}{j}\binom{j}{k}$,"I am prepping for my mid semester exam, and came across with the following question: Find the closed form for the sum $\sum_{j=k}^n (-1)^{j+k}\binom{n}{j}\binom{j}{k}$, using the assumption that $k = 0, 1,...n$ and $n$ can be any natural number. So what I have done is to note the fact that $$\binom{n}{j}\binom{j}{k}= \frac{n!}{j!(n-j)!}\frac{j!}{k!(j-k)!}=\frac{n!}{(n-j)!\ k!\ (j-k)!}$$ Then we can write the summation as $$\sum_{j=k}^n {(-1)^{j+k}\binom{n}{j}\binom{j}{k}}= \sum_{j=k}^n (-1)^{j+k} \frac{n!}{(n-j)!\ k!\ (j-k)!} = \frac{n!}{k!} \sum_{j=k}^n (-1)^{j+k} \frac{1}{(n-j)!\ (j-k)!} $$ I tried to let $m=j-k$:
$$\frac{n!}{k!} \sum_{m=0}^{n-k} (-1)^{m+2k} \frac{1}{(n-m-k)!\ m!}=\frac{n!}{k!} \sum_{m=0}^{n-k} (-1)^{m} \frac{1}{(n-m-k)!\ m!}$$ But I am not sure what to proceed next. Any help would be highly appreciated!","['combinations', 'combinatorics', 'summation', 'calculus']"
1694161,showing $\|x\| \leq \lim_{n\to \infty} \inf \|x_n\|$,"showing $$\|x\| \leq \lim_{n\to \infty} \inf \|x_n\|$$ where $x_n \to x$ weakly, and we are working under a normed space. I am given a hint that $$\|x\| = \sup_{\|\phi\| = 1} |\phi(x)|$$ where $\phi \in X^\star$. My idea was to choose $\phi$ s.t. $\phi(x) = \|x\|$, and somehow use the hint. The hint however says that for this particular phi we have that $\|x\| \geq |\phi(x)| = \lim_{n \to \infty} |\phi(x_n)|$ which already is in the incorrect direction. Any guidance please.","['functional-analysis', 'normed-spaces', 'real-analysis', 'calculus']"
1694167,"What's the exterior derivative of the two form: $x \,dy \wedge dz + y \,dx \wedge dz + z\,dx \wedge dy$?","What's the exterior derivative of the two form: $x \,dy \wedge dz + y\, dx \wedge dz + z\,dx \wedge dy$? I saw some calculations on this site and I'm pretty sure that the answer is $3\, dx\wedge  dy\wedge dz$ , however, this conflicts with the answer posted on here, Exterior derivative of a 2-form ,  which I get $ dx\wedge  dy\wedge dz$. How do I do this correctly? 
Thanks in advance!","['derivatives', 'differential-forms', 'calculus']"
1694198,How to prove this complex inequality elegantly?,"Question Let $z_{1,2}\in U(0,1)\subset \Bbb C$, prove that 
$$\frac{|z_1|-|z_2|}{1-|z_1||z_2|}\le\left|\frac{z_1+z_2}{1+\overline{z_1}z_2}\right|\le\frac{|z_1|+|z_2|}{1+|z_1||z_2|}$$ Actually I haven't come up with any reasonably good proof so far. All I could do was simply use brute force, i.e., relations like $|z|^2=z\overline z$. When I finished my brute-force proof and rewound it, I found it could be simplified into the following form, which looks not as horrible: Let $w=2|z_1z_2|-z_1\bar {z_2}-z_2\bar{z_1}\ge 0$. Square the inequality, and denote the middle one as $\frac AB < 1$. Then 
$$\frac{A-w}{B-w}\le\frac AB\le \frac{A+w}{B+w}$$
which is the desired result. Seems good. But indeed doesn't. Because it comes in hindsight : it's only after I had brute-forced and rewound that I formulated this short one. So apart from this one, is there any other more elegant or advanced proof? Incidentally, the structure $\displaystyle\frac{|x|\pm|y|}{1\pm|x||y|}$ frequently occurs, is it of any significance?","['alternative-proof', 'complex-analysis', 'inequality', 'complex-numbers']"
1694203,Definition of the total derivative.,"I am trying to understand the following definiton. $f:\mathbb{R}^n \rightarrow \mathbb{R}^m$ .  The total derivative of $f$
  in point $a$ is the unique linear map $Df|_a$ such that $$\lim_{h
\rightarrow 0}\dfrac{f(a+h)-f(a)-Df|_a(h)}{||h||} = 0$$ Could someone explain why this definition works? -Why should we divide by $||h||$? -Why is $Df|_a$ linear? -How should I interpret this linear map $Df|_a$, what is the meaning of the total derivative?","['multivariable-calculus', 'definition', 'calculus', 'derivatives']"
1694259,Conditional Expectation.,"Let $X$ be a random variable in $L^2(\Omega, \Sigma, P)$ and $\mathcal G$ a sub-$\sigma$-algebra of $\Sigma$. Prove that $E[(X-E[X\mid\mathcal G])^2] \le E[(X-E[X])^2]$. As conditional expectation can be viewed as a projection of $X$ from $L^2(\Sigma)$ into a subspace $L^2(\mathcal G)$, the norm $E[(\bullet)^2]$ would be smaller. And the inequality is then obvious. However, this is rather intuitive. I have some trouble in giving a rigorous proof. Can anyone help? Also I am wondering if $E[|X-E[X\mid\mathcal G]|] \le E[|X-E[X]|]$. This seems true from the above reasoning? Let $T$ and $S$ be stopping times and $\mathcal F_T$ and $\mathcal F_S$ be the stopping time $\sigma$-algebra. Prove that $E[E[Y\mid\mathcal F_S]\mid\mathcal F_T] = E[E[Y\mid\mathcal F_T]\mid\mathcal F_S] = E[Y\mid\mathcal F_{T\land S}]$. It seems obvious that the smaller stopping time is the one that matters. But still, I am not able to give the proof. Thanks a lot!","['probability-theory', 'conditional-expectation', 'stopping-times']"
1694268,Two congruent segments does have the same length?,"The answer to the question in the title seems an obvious ''Yes by definition !''. And this really is the definition from Wikipedia : Two line segments are congruent if they have the same length. But in Hilbert's Foundations of Geometry congruence is defined without use of metric notions, by the axioms of Group IV (chapter 6). I don't report here these axioms that can be found on the book (or on the Wiki page )
and essentially say that congruence is an equivalence relation and that congruence is conserved when we add adjacent segments. Usually these axioms are illustrated with a figure that use two segments $AB$ and $A'B'$ that have the same length, as in this case. but I think that this illustration is only dictated by our intuition of congruence as equality of length, and it's not justified by the axioms. Look at the second figure, where $a$,$a'$ and $a''$ are straight lines and $E$ is a point that does not belong to these lines. Projecting from $E$ the points $A,B,C$ of $a$ to the other lines, we can define the relation:
$$
AB \equiv A'B' \quad BC \equiv B'C'
$$ 
and
$$
A'B' \equiv A''B'' \quad B'C' \equiv B''C''
$$ and we can see that this relation satisfies all axioms of congruence. But, clearly, it doesn't satisfies our intuition that congruent segments have the same length. This construction is essentially the same that Hilbert uses (in the chapter 24) for the  construction of an ''Algebra of Segments'', ( where the point $E$ is ''at infinity''). Obviously ''to have the same length'' is a congruence relation, but only in a metric space, and, in the same space, different congruence relations can be defined that are not equivalent to ''have the same length''. And more, we can have different metric (and different ''length'') for a geometry that satisfies the Hilbert's axioms. So my question is, at first, if my reasoning is correct or if I've misunderstood something of fundamental, and, if I'm right, what are the axioms that we need for exactly represent our intuitive concept of congruence? A final notice: this question is related to What really is ''orthogonality''? , where a similar question is posed for angles.","['congruence-relations', 'axioms', 'geometry']"
1694271,Limit with inverse trigonometric functions: $\lim_{x\to 0}\frac{\arctan2x}{\sin3x}$,"$$\lim_{x \to 0} \frac{\tan^{-1}2x}{ \sin 3x}$$ What is the shortest way to do this please? Is there a standard way to solve questions like that with inverse trigonometric functions? P.S. I apologize to those who answered the earlier,  unedited version of this question which had included an arc tan term.  Since I'm new I made a mistake in the formatting there.  It's fixed now.  This is the one I originally meant.  Any answers please?","['real-analysis', 'trigonometry', 'calculus', 'limits']"
1694278,How to write $x \sin A + y \sin B$ as single trigonometric funtion,"I came upon this while doing waves: How do you write $x \sin A+y \sin B$ in the form of $z\sin(\cdots)$? I have absolutely no idea on how to proceed. Edit: It was  $z\sin(C)$ . My mistake.
Maybe there is a mistake in my assumption that it can be expressed in that way. So I'll give the reason behind it Waves can be expressed as $$y_{1}=A_{1}\sin(kx+\omega t+\delta_{1})$$ and $$y_{2}=A_{2}\sin(kx+\omega t+\delta_{2})$$ Let net be $y$ Therefore, $$y=y_{1}+y_{2}$$ Clearly,
 $$\frac{\partial y}{\partial t} = -k\frac{\partial y}{\partial x}$$ So, y can be expressed as a linear combination of x and t (leading to the conclusion that sum of two propogatory waves in the same direction fives another propogatory wave). Its probably a sine or cos function as we are adding up two sines. While we were doing superposition of waves, we assumed same amplitude for mathematical simplicity but I can vaguely remember my teacher telling me it was trivial for different amplitudes but I can't figure it out now. Sorry if I've made a stupid mistake as I am still a beginner in waves. Edit again: -k in partial differential equation.","['algebra-precalculus', 'trigonometry']"
1694290,What is the optimal strategy when driving to my university: Wait or take alternative route and (possibly) wait?,"When bicycling to my university, I'm faced with a difficult decision. The situation is shown here: I have to get to point marked with the $\times$, but in order to do so, I must cross a big road with a traffic light at both places where crossing is possible. If the first traffic light (at the bottom) is green, then there's no debate; I simply cross and go up $A$ with no waiting time at all. The choice arises when it is red (as is always the case anyway), since I can choose to wait for the first light to change, or go up a smaller road $B$ and then try to cross at the second light. Let's assume that the two traffic lights are identical w.r.t. the percent of the time they are red (let's call this quantity $\Theta$), but that the second is phase-shifted with $\tau$ w.r.t. the first, s.t. the second changes to e.g. green a time $\tau$ after the first. The length $L$ of the paths are identical and I bicycle with a constant speed $v$ (although I suspect these last two are irrelevant to the solution). I don't know anything about the light before I arrive at it. The question then becomes If I arrive at a random time during which the first traffic light is red, given $\Theta$ and $\tau$ (and $L$ and $v$), should I wait at the first light and go by path $A$, or should I go by path $B$ in order to minimize waiting time? The type of solution I'm looking for can better be formulated if we first look at two examples: The $\mathbf{I}$'s represent the time during which the traffic light is red - the space in between them represent the green lights. The drawn lines represent my position if I chose to go by $B$. If I arrive at the first red light during the blue stretch, I should go by $B$; if I, on the other hand, arrive during the red stretch, I should go wait and go by $A$. As can be seen from the two scenarios $(a)$ and $(b)$ (where the lines represent my position should I choose path $B$, and where the slope is determined by $L$ and $v$), my choice depends on $\tau$, which is the only parameter that has been changed. If we call the length of the blue strip $t_{\text{go}}$ and the length of the red strip $t_{\text{wait}}$, s.t. $t_{\text{go}}+t_{\text{wait}}=\Theta$, the solution should be of the form $$t_{\text{go}}(\tau)$$ I don't think $v$ and $L$ should play a role, since the effect of changing these can be incorporated into $\tau$, so we could probably set $v=L=1$. I'm guessing there should be a pretty simple geometric way of solving this, but for the time being it is eluding me. Some random observations: If $v=\frac{L}{\tau}$ it doesn't matter what I choose. Also, if the second light was independent of the first, I should always go by $B$, since there I will at least have a $1-\Theta$ chance of getting a green light. Any help is much appreciated!","['recreational-mathematics', 'probability', 'optimization']"
1694328,$\pm$ sign in $y=\arcsin\frac{x}{\sqrt{1+x^2}}$,"If: 
$$y=\arcsin\frac{x}{\sqrt{1+x^2}}$$
Then: 
$$\sin(y)=\frac{x}{\sqrt{1+x^2}}$$
$$\cos^2(y)=1-\sin^2(y)=\frac{1}{1+x^2}$$
$$ \tan^2(y)=\sec^2(y)-1=1+x^2-1=x^2$$
Therefore I would say:
$$\tan(y)=\pm x$$
However, my calculus book says (without the $\pm$):
$$\tan(y)=x$$ Question: Why can we remove the $\pm$?","['trigonometry', 'calculus']"
1694351,Finding all left inverses of a matrix,"I have to find all left inverses of a matrix $$A = \begin{bmatrix}
 2&-1 \\ 
5 & 3\\ 
 -2& 1
\end{bmatrix}$$ I created a matrix to the left of $A$, $$\begin{bmatrix}
a &b  &c \\ 
d &e &f 
\end{bmatrix}  \begin{bmatrix}
 2&-1 \\ 
5 & 3\\ 
 -2& 1
\end{bmatrix} = \begin{bmatrix}
 1&0 \\ 
 0&1 
\end{bmatrix}$$ and I got the following system of equations: \begin{array} {lcl} 2a+5b-2c & = & 1 \\-a+3b+c & = & 0 \\
2d+5e-2f & = & 0 \\ -d+3e+f & = & 1 \end{array} After this step, I am unsure how to continue or form those equations into a solvable matrix, and create a left inverse matrix from the answers of these equations.","['matrices', 'inverse', 'linear-algebra', 'systems-of-equations']"
1694356,How to find the intersection of level curves?,"For two functions $f,g:\mathbb{R}^2\rightarrow\mathbb{R}$, how would I show that the level curves of these two different functions intersect at right angles? I can give the specific functions, but I would like to know in a more general way.","['functional-analysis', 'real-analysis', 'intersection-theory']"
1694385,"Find: $\zeta \left( 3,1,1,1 \right)$","While solving a summation, I came across this: $$\zeta \left( 3,1,1,1 \right)=? $$ I'm new to multiple zeta values. That's why I couldn't find this. So my question is does a closed form exist for this. If yes, what is it? I have no idea to evaluate this. Please help.","['zeta-functions', 'summation', 'integration', 'calculus']"
1694419,Calculating Christoffel symbols using variational geodesic equation,"Given the line element $$ds^2 = e^v dt^2 - e^{\lambda} dr^2 - r^2 d \theta^2 - r^2 \sin^2 \theta d \phi^2$$ we wish to compute the Christoffel symbols $\Gamma^{a}_{bc}$ using the geodesic equation. Our Lagrangian is $$L = g_{ab} {\dot{x}}^a {\dot{x}}^b = e^v \dot{t}^2 - e^{\lambda} \dot{r}^2 - r^2 \dot{\theta}^2 - r^2 \sin^2 \theta \dot{\phi}^2.$$
Applying the Euler-Lagrange equations and dividing by $-2e^v$ yields $$\ddot{t} - \frac{1}{2} \dot{v} \dot{t}^2 + v' \dot{t} \dot{r} + \frac{1}{2} \dot{\lambda} e^{\lambda - v} \dot{r}^2 = 0.$$ I incorrectly conclude that $\Gamma^{0}_{00} = - \frac{1}{2} \dot{v}$. Why? I have a feeling this has something to do with the difference between the geodesic equations for space-like and time-like geodesics.","['tensors', 'semi-riemannian-geometry', 'differential-geometry', 'general-relativity']"
1694424,How do I integrate $\int \frac{dx}{\sin^3 x + \cos^3 x}$? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question How do I integrate the following $$\int \frac{dx}{\sin^3 x + \cos^3 x}$$ ? It appears that I am supposed to break this up into $(\sin x + \cos x)(1-\cos x \sin x)$, but the next thing to do is not apparent to me.","['indefinite-integrals', 'integration']"
1694434,"Is there a positive function $f$ on real line such that $f(x)f(y)\le|x-y|, \forall x\in \mathbb Q , \forall y \in \mathbb R \setminus \mathbb Q$?","Does there exist a  function $f:\mathbb R \to (0,\infty)$ such that $f(x)f(y)\le|x-y|, \forall x\in \mathbb Q , \forall y \in \mathbb R \setminus \mathbb Q$ ?","['real-analysis', 'inequality', 'metric-spaces', 'baire-category']"
1694469,Limit of $x/|x|$,What is the limit of $$\lim_{x\rightarrow 0}  \frac{x}{|x|}$$ My guess is that it is $1$ if approached from $0+$ and $-1$ if approached from $0-$ Thanks,"['calculus', 'limits']"
1694491,"If $\mathcal{M}_1(E)$ is compact, then $M \subset \mathcal{M}_1(E)$ is exponentially tight?","Let $E$ be a metric space, $\mathcal B$ the Borel $\sigma$-algebra on $E$ and $\mathcal M_1(E)$ the set of probability measures on $(E,\mathcal B)$. I'm interested in when and why the following implication holds. If $\mathcal{M}_1(E)$ is compact, then $M \subset \mathcal{M}_1(E)$ is exponentially tight . Is this always true? For what reason? A script I'm studying says something along the lines of ""we do not need to worry about exponential tightness (of $M \subset \mathcal{M}_1(E)$) because $\mathcal{M}_1(E)$ is compact"". Here, $E$ is a bounded, open, connected subset of $\mathbb R^d$ with a smooth boundary and $M$ is the sequence of local times of a Brownian motion. And I see that $\mathcal{M}_1(E)$ is compact in this case, but not why exponential tightness follows. It sounds to me like there's a simple argument that makes this statement true in a general setting, but I couldn't come up with a proof. So I wonder: why and when is it true?","['stochastic-processes', 'large-deviation-theory', 'probability-theory', 'measure-theory']"
1694498,Explaining a simple observation on Terry Tao's blog about the Wiener process,"Quoting Terry Tao's blog : A simple but fundamental observation is that $n$-dimensional Brownian
  motion is rotation-invariant: more precisely, if $(X_t)_{t \in
 [0,+\infty)}$ is an $n$-dimensional Wiener process with initial
  position $0$, and $U \in O(n)$ is any orthogonal transformation on
  ${\bf R}^n$, then $(UX_t)_{t \in [0,+\infty)}$ is another Wiener
  process with initial position $0$, and thus has the same distribution: $(UX_t)_{t \in [0,+\infty)} \equiv (X_t)_{t \in [0,+\infty)}$. This is ultimately because the $n$-dimensional normal distributions $N(0,\sigma^2 I)_{{\bf R}^n}$ are manifestly rotation-invariant (see Exercise 10 of Notes 1). I do not have the solution to Exercise 10 and I do not see directly how it's related. Could someone clarify in more detail how this explains that Brownian motion is rotation-invariant? (To show that all the conditions don't change).","['probability-theory', 'probability', 'brownian-motion']"
1694518,Non reduced cone point in affine cone of a projective variety?,"Let $X$ be an integral closed subscheme of projective $n$ space over a field. Let $X'$ be its affine cone. Is it possible that $X'$ is nonreduced at the cone point? If so, what is an example? Edit: I don't think it can happen. By taking global sections of the twists of the closed subscheme exact sequence, we have an embedding of the homogeneous coordinate ring of $X$ into $R = \oplus_{n \geq 0} \Gamma(X, O_X(n))$. I have convinced myself that this ring $R$ is reduced if $X$ is, and a domain if $X$ is integral. Pf: If $f,g \in R$ are such that $fg = 0$, by choosing highest degree terms we can assume $f \in O(n)$ and $g \in O(m)$. Then we know that $X = V(fg) = V(f) \cup V(g)$, so without loss assume that $V(f) = X$. But then this means that in each trivialization $O(n)$ to $O_X$, the image of $f$ in $O_X$, $\tilde{f}$, vanishes at all primes. Hence $\tilde{f} = 0$, since the local rings are all reduced. Hence $f = 0$, since trivializations are in particular isomorphisms. If $f \in R$ is such that $f^n = 0$, again assume $f \in O(n)$. After passing to some cover of trivializations which send $f$ to $\tilde{f}$, we can consider $\tilde{f}^n$ in each local ring, and reducedness implies that $\tilde{f} = 0$. But then this implies that $f = 0$. Edit: I thought I was making a mistake, but then realized that at any rate this counter example isn't valid. I'll leave it here. If $R = k[x,y]/(xy,x^2)$, then $Spec R$ is reduced at the origin, but $Proj R$, for example computed on the chart $D(y)$ (on $D(x)$ it is empty), is just isomorphic to $Spec k[y]$, so it is reduced. The issue with this counter example: $R$ is not the homogeneous coordinate ring of $Y = Proj(R)$, because $\Gamma_* = \oplus_{n \geq 0} \Gamma(I_Y(n))$ of the ideal sheaf $I_Y$ is larger than $(xy,x^2)$. In particular, $I_Y(1)$ contains $x/y$ on the patch $D(y)$ and $x/x$ on the patch $D(x)$, so we get $x \in \Gamma(I_Y(1))$. (In other words, $(xy,x^2)$ is not saturated.)",['algebraic-geometry']
1694535,Is there an expression for a function that maps integers to one and non-integers to zero?,"Is there a function that can be built with addition, multiplication, exponentiation, trigonometric functions, integrals, (and all of their inverses i.e subtraction, division, taking logarithms, $\arcsin(x)$, derivatives, etc) that would take an integer and output $1$ and take a non-integer and output $0$? I looked at the Dirichlet Function but it separated rationals from irrationals, and the only solution that I could come up with was with the Fourier Series of $$x-\lfloor x \rfloor = \frac 12 - \frac 1\pi \sum_{k=1}^\infty \frac 1k \sin (2 \pi k x)$$
But this uses infinite series. Is there a way to build such a function? If there is not, why not?",['functions']
1694536,Running through all permutations of a Rubik's cube [duplicate],"This question already has answers here : Brute force method of solving the cube: How many moves would it take? (3 answers) Closed 7 years ago . According to Wikipedia a $3 \times3\times3$ Rubik's cube has $43252003274489856000$ permutations. I never tried solving one myself (too tedious), however I wondered, if one could miraculously ""solve"" the cube in color blind mode (all faces of the cube have the same color to him or the person is just blind folded). (Yes, it is tedious, but I guess sooner or later he will just hit the right permutation and hopefully someone tells him, that it's good and he may stop.) I wondered, if there were some kind of ""best"" approach to run all through those  permutations without too many repetitions, for example the algorithm makes sure one gets any permutation at most $n$ times. I don't know the answer yet and probably won't have it anytime soon. However I would be quite interested in seeing this algorithm and thus I hope the question has been asked before and been solved. Any constructive comment/answer is appreciated.","['rubiks-cube', 'combinatorics', 'algorithms', 'reference-request']"
1694543,How to evaluate $\sum_{k=1} ^{n-1} \frac{\sin (k\theta)}{\sin \theta}$,How to evaluate $$\sum_{k=1} ^{n-1} \frac{\sin (k\theta)}{\sin \theta}$$ Any help ? I tried to use difference method. But I'm not getting there.,"['summation', 'trigonometry']"
1694561,differential equation of family of circles passing through origin,"How do I find the DE of all circles passing through origin? I tried something like this The family of circles passing through the origin is given by
     $$
	 (x- r \cos \theta )^2 + (y- r\sin \theta)^2 = r^2 
	 $$Differentiating once, we get
     $$ 
	  2 ( (x- r \cos \theta) + (y - r \sin \theta) y') = 0
	 $$
     Differentiating again, we get
     $$
	  1 + y'^2  + (y - r \sin \theta) y'' = 0 
	 $$
How to get rid of $r$ and $\theta$ algebraically? Is there any other approach?",['ordinary-differential-equations']
1694563,"When does $ \lim\limits_{n\to \infty} \, (\underbrace{f \circ \dots \circ f}_{n}) (\xi)$ exist?","$\newcommand{\coloneqq}{\colon\!=}$
Let $k \in \mathbb{N}$ and $a_0, \dots, a_k \in \mathbb{R}$ and $a_k \neq 0$. Let 
$$\begin{align*}
f: \mathbb{R}\smallsetminus \{0\} &\to \mathbb{R} \\
\forall x \in \mathbb{R}\smallsetminus \{0\} \quad x&\mapsto \sum_{i=0}^{k} \frac{a_i}{x^i}
\end{align*}$$ Let's define $f^{\left[n\right]}$ recursively 
$$ \begin{align*}
f^{\left[1\right]} &\coloneqq f \\
\forall n \in \mathbb{N}\smallsetminus \{0\} \quad f^{\left[n+1\right]} &\coloneqq f \circ f^{\left[n\right]} 
\end{align*}$$
Let $\xi \in \mathbb{R} \smallsetminus \{0\}$ such that $f(\xi)\neq 0$. What conditions on $a_0, \dots, a_k$ and $\xi$ are sufficient for the existence of the following limit?
$$ \lim\limits_{n\to \infty} \, f^{\left[n\right]} (\xi)$$ I have only solved the case $k=1$ so far. If I am not mistaken, the limit exists when $P_2(x):=x^2-a_0x-a_1$ has real roots and $a_0 \neq 0$. Moreover, if the limit exists, then: if $\xi$ is a root of $P_2(x)$, then the limit converges to it, otherwise it is the value of the continued fraction $\left[ \overline{a_0,\frac{a_0}{a_1}} \right]$, which is a root of $P_2(x)$.","['real-analysis', 'limits']"
1694634,Proof of irreducibility of $x^2+1$ over $\mathbb{Z}_p[x]$,"I am trying to prove that $x^2+1$ is irreducible over $\mathbb{Z}_p[x]$. In order to do that, let's say it's not irreducible, then we have: $x^2+1=(x+a)(x+b) \rightarrow x^2+(a+b)x+ab=x^2+1\mod p$, thus we have: $a=-b \mod p$, $ab=1\mod p \rightarrow b^2=-1\mod p$ Now, can I say that the last equation doesn't have any answer in $\mathbb{Z}[x]_p$? If yes, how? Because the last equation implies that $b^2 = p-1\mod p$, and I am not sure how to prove there is no $b$ exist to satisfy this equation","['number-theory', 'irreducible-polynomials', 'modular-arithmetic']"
1694666,How to prove the result of this definite integral?,"During my work I came up with this integral: $$\mathcal{J} = \int_0^{+\infty} \frac{\sqrt{x}\ln(x)}{e^{\sqrt{x}}}\ \text{d}x$$ Mathematica has a very elegant and simple numerical result for this, which is $$\mathcal{J} = 12 - 8\gamma$$ where $\gamma$ is the Euler-Mascheroni constant. I tried to make some substitutions, but I failed. Any hint to proceed?","['integration', 'definite-integrals', 'calculus']"
1694713,Explicit characterization of dual of $H^1$,"Let's start by some well-known facts: $H^1(\mathbb{R})$ is a Hilbert space, hence there holds the Riesz representation theorem, stating that any linear functional on it can be represented as $L = \langle \cdot , v\rangle$, where $v \in H^1(\mathbb{R})$, and $\langle \cdot, \cdot \rangle$ is the pairing in $H^1(\mathbb{R})$:
$$
\langle u, v \rangle = \int_{\mathbb{R}} (u_x v_x + uv) d x.
$$
Here, subscript indicates derivative. Let $w \in L^2(\mathbb{R})$. Since the $H^1$-norm is stronger than the $L^2$-norm, the functional $v \mapsto L(v) := \int_{\mathbb{R}^2} w(x) v(x) d x$ is a continuous linear functional on $H^1(\mathbb{R})$. My question is: is it possible to give the representative explicitly? In other words, find $z \in H^1(\mathbb{R})$ such that $L(v) = \langle v,z\rangle$. I tried mimicking the proof of the Riesz theorem but it did not help. I am sure there is something really easy I don't see, but I seem to have some misconception here. Any help would be appreciated.","['functional-analysis', 'real-analysis', 'sobolev-spaces', 'hilbert-spaces']"
1694721,"A problem involving the product $\prod_{k=1}^{n} k^{\mu(k)}$, where $\mu$ denotes the MÃ¶bius function","Let $\mu$ denote the MÃ¶bius function whereby
$$\mu(k)    = 
  \begin{cases} 
   0 & \text{if $k$ has one or more repeated prime factors} \\
   1 & \text{if $k=1$} \\
   (-1)^j       & \text{if $k$ is a product of $j$ distinct primes}\end{cases}$$ for all $k \in \mathbb{N}$. I have previously noted a surprising connection between products of the form $$\prod_{k=1}^{n} k^{\mu(k)}$$ and harmonic numbers (see http://oeis.org/A130087 ). Letting $H_{n} = 1 + \frac{1}{2} + \cdots + \frac{1}{n}$ denote the $n^{\text{th}}$ harmonic number, I noted that the denominator (in lowest terms) of this product is equal to the denominator (in lowest terms) of $H_{n}^{2}n!$ for all $n < 897$.  For example, letting $n = 11$, we have that $$1^{\mu(1)} \cdot 2^{\mu(2)} \cdot \cdots \cdot 11^{\mu(11)} = \frac{2}{77},$$ and $$H_{11}^{2}11! = \frac{28030126084}{77}.$$ This property does not hold for all $n \in \mathbb{N}$, and the first counterexample occurs in the case whereby $n = 897$. Letting $\text{den}(q)$ denote the denominator in lowest terms of $q \in \mathbb{Q}_{> 0}$, it is not obvious to me why the equality $$\text{den}\left(\prod_{k=1}^{n} k^{\mu(k)}\right) = \text{den}\left(H_{n}^{2}n!\right)$$ holds for the first several hundred natural numbers. So it seems natural to me to ask: (1) Is there some simple number-theoretic explanation as to 'why' this equality holds for the first several hundred natural numbers? (2) Is there some simple number-theoretic interpretation as to 'why' this equality does not hold in general (which specifically explains the counterexample in the case whereby $n = 897)$? (3) Is there a natural combinatorial way of approaching this problem? Note that integers of the form $H_{n}n!$ are unsigned Stirling numbers of first kind, so it is plausible that there is a natural combinatorial interpretation of the expression $\text{den}\left(H_{n}^{2}n!\right)$.","['number-theory', 'mobius-function', 'sequences-and-series']"
1694748,indefinite integral of $\cos(x) / x$?,"How can I evaluate ; $$\int \frac{\cos(x)}{x} \, dx $$ I tried to do partial integration but it became to an infinite loop of partial integrations","['indefinite-integrals', 'trigonometry']"
1694786,Relations on a set. Discrete Mathematics.,"just want to verify that my understanding of relations is correct, grammar and correct logical form.  Thanks! Determine whether the relation R on the set of all Web pages is reflexive, symmetric, antisymmetric, and/or transitive, where $(a, b) \in R$ if and only if every one who has visited Web page a has also visited Web page b. Reflexive: One who has visited Web page a has also visited Web page a. Thus this is reflexive. Symmetric: One who visits Web page a has also visited web page b, however it is not necessarily the case that one who visits Web page b has visited Web page a, thus this is not symmetric. Transitive: If one has visited Web page a, they visited Web page b, and having visited Web page b they also visited Web page c.  Thus, one who has visited Web page a has visited Web page c.  Thus this relation is transitive. Antisymmetric We know one who visits Web page a, also visits Web page b, from this if we know that one who visits Web page b also visits Web page a, then Web page a is Web page b.  Thus, this relationship is antisymmetric.",['discrete-mathematics']
1694790,Obscuring squares of Rubik's cube,"This is a combinatorial question related to Rubik's cube $3\times3\times3$ (and, in the end, its generalizations $n\times n\times n$ ). I assume that the readers are familiar with this puzzle. Let's recall some basic terminology and conventions to rule out possible ambiguities. In the following, we use the words state and configuration interchangeably, as exact synonyms. We are only interested in configurations of Rubik's cube where it has the shape of a cube (so, we consider rotations of its layers by an angle of $90^\circ$ as atomic transformations, and no incomplete rotations are allowed). Also, we consider configurations that can be obtained from one another by solid rotations of the cube as a whole (without any relative movements of its parts) as identical. The cube has $6$ faces, each one of them is a $3\times3$ grid of squares â€” $9$ squares per face, and $54$ squares in total. Each square is of a solid color. Orientation of every single square is irrelevant (in fact, this rule has significance only for central squares). There are $6$ different colors, and there are $9$ squares of each color. There is a single selected state where each face consists of all squares of the same color â€” this state is called the solved state. A state that can be reached from the solved state by a sequence of valid rotations of layers is called a valid state (the solved state, of course, is a valid state too). As a side note, if we allowed disassembling the cube into its parts and reassemling them, then we would get the number of possible states $12$ times larger than the set of valid states (this larger set would be a disjoint union of $12$ equivalence classes under valid rotations, each of the classes called an orbit ). We are only interested in valid states in the following. Usually, it is assumed that colors of all squares on the cube are visible. Let's consider a possibility that some squares may be obscured (e.g. completely covered by an opaque colorless sticker), so that their colors are not visible. In general, obscuring some squares can result in some states becoming visually indistinguishable . As trivial examples, obscuring all $54$ squares makes all states indistinguishable, but obscuring only $1$ square (no matter which one) does not make any states indistinguishable. What maximal number of squares in Rubik's cube $3\times3\times3$ can be obscured without making any states indistinguishable? What maximal number of squares in Rubik's cube $2\times 2\times 2$ can be obscured without making any states indistinguishable? What maximal number of squares in Rubik's cube $n\times n\times n$ can be obscured without making any states indistinguishable? Can we find a general formula, recurrence relation, etc. to compute it for every $n\in\mathbb N$ ?","['rubiks-cube', 'combinatorial-geometry', 'puzzle', 'combinatorics', 'group-theory']"
1694823,"Find polynomials $p_n$ on $[0,1]$ with integral $=3$ and converges pointwisely to $0$","Prove that there  is a sequence of polynomials $\{p_{n}\}$ such that $p_{n} \to 0$ pointwise on $[0,1]$, but such that 
$$\int_{0}^{1}p_{n}(x)dx=3.$$ My thought : 
  I was thinking of working with a seuqence of functions that has pointwise limits to equal to $0$ but the integral to equal $3$. And then using the Weiestrass approximation theorem to say that there is a $p_n$ that would approximate the functions. I just can't think of the functions that would work.","['real-analysis', 'polynomials']"
1694840,How do you find the squares mod 23?,1.) Compute the squares modulo 23 as efficiently as possible. 2.) Show that $y^2 = 23x^2 + 7$ has no integer solutions. This is a two part problem on my review for number theory and I am a bit lost. After going to my professor with this his hint was for part 2 saying to reduce it to mod 23. Having a really tough time with this problem any help is greatly appreciated.,"['number-theory', 'square-numbers', 'modular-arithmetic']"
1694865,help understanding a paragraph from Linear Algebra Done Right,"I am attempting to work my way through the 3rd edition of ""Linear Algebra Done Right"", but there's a paragraph on page 14 that I don't understand.  I have struggled with it myself for a few hours and have come to the conclusion that I need some help.  But first some background. Axler lets $\textbf{F}$ stand for either the set of real or complex numbers.  (He mentions fields but doesn't use them directly.)  He also uses the term ""list"" instead of ""tuple"". The problematic paragraph is preceded by this: If $S$ is a set, then $\textbf{F}^S$ denotes the set of functions from $S$ to $\textbf{F}$. For $f, g \in \textbf{F}^S$, the sum $f + g \in \textbf{F}^S$ is the function defined by
  $$(f + g)(x) = f(x) + g(x)$$
  for all $x \in S$. For $\lambda \in \textbf{F}$ and $f \in \textbf{F}^S$, the product $\lambda f \in \textbf{F}^S$ is the function defined by
  $$(\lambda f)(x) = \lambda f(x)$$
  for all $x \in S$. As an example of the notation above, if $S$ is the interval [0,1] and $\textbf{F} = \textbf{R}$, then $\textbf{R}^{[0,1]}$ is the set of real-valued functions on the interval [0,1]. So far, so good.  This is where I get lost: Our previous examples of vector spaces, $\textbf{F}^n$ and $\textbf{F}^\infty$, are special cases of the vector space $\textbf{F}^S$ because a list of length $n$ of numbers in $\textbf{F}$ can be thought of as a function from {1, 2, ..., $n$} to $\textbf{F}$ and a sequence of numbers in $\textbf{F}$ can be thought of as a function from the set of positive integers to $\textbf{F}$.  In other words, we can think of $\textbf{F}^n$ as $\textbf{F}^{\{1,2,...,n\}}$ and we can think of $\textbf{F}^\infty$ as $\textbf{F}^{\{1,2,...\}}$. The general idea seems straightforward to me.  Let $S$ be the set of all tuples in $\textbf{F}^n$ or $\textbf{F}^\infty$, and now $\textbf{F}^S$ is a vector space.  Is it really that simple?  But what is he saying? [My background: I am a computer programmer who took a ""normal"" linear algebra class in college, who has a new-found love for higher mathematics, but not enough time and money to go back to school.]",['linear-algebra']
1694886,The projection operator on a finite dimensional vector space is diagonalizable,"This question has been answered before, but I want to check if my solution using minimal polynomials is good. A projection matrix satisfies $M^2 = M$, so it satisfies the polynomial equation $M(M-1) = 0$. Thus the minimal polynomial must be either $M$, $M-1$, or $M(M-1)$. Since in all cases the polynomial factors into distinct linear factors, the matrix is diagonaliazable.",['linear-algebra']
1694904,Analytic continuation for $\zeta(s)$ using finite sums?,"$\zeta(s)$ converges for $\sigma >1$ but not for $\sigma =1/2.$ But for some reason for $s = 1/2 + i t $ and fixed finite $N,~$ $\zeta_N(s) =\sum_{n=1}^N\frac{1}{n^s}$ is very close to $\zeta(s)$ as found using analytic continuation for $t$ in some range $$f_1(N) < t < f_2(N). $$ For example, for $N= 1000$ a plot of $\zeta_N(s)$ gives a zero at approximately $t= 568.9,$ and the 319th zeta zero is about $t=568.924$ (per Mathematica). For $N=10000,$ graphically there is a zero at approximately $1783.07,$ and this agrees with Mathematica's 1321st zero, about $1783.08.$ Mathematica has a built-in analytic continuation for $\zeta(s),$ but I don't think that is being engaged here. Does this property evaporate for very large $N$ or is there a reason we should in principle be able to approximate $\zeta(s)$ at $\sigma = 1/2$ for finite $N$ on a suitable range of $t?$ Since this property may not hold for large $N$ there is little point in broadening the question.","['complex-analysis', 'analytic-continuation', 'riemann-zeta']"
1694907,Limit to find convergence of $\int_1^\infty \frac{\arctan x}{x^2} ~dx$,"Show the integral is convergent and find the value it converges to. $$\int_1^\infty \frac{\arctan x}{x^2} ~dx$$ I have found the indefinite integral to be $$-\frac{\arctan x}{x} + \ln|x| -\frac{1}{2}\ln(x^2+1)$$ When I take the limit as t goes to $\infty$, I end up having to deal with this $$\lim_{t\to \infty}\left(-\frac{\arctan x}{x} + \ln|x| -\frac{1}{2}\ln(x^2+1)\right)$$ I know the first term goes to 0, but how do I find the limit of the rest of it? I tried combining the two terms to see if I could use L'Hospital's rule, but I couldn't get anywhere. I got to this and was not sure how to proceed. $$\lim_{t\to \infty}\left(\ln \left(\frac{|t|}{\sqrt{t^2+1}}\right)\right)$$","['real-analysis', 'limits', 'logarithms', 'calculus', 'improper-integrals']"
1694909,"If $A,B\in M_2(\mathbb{R})$, show that $(AB-BA)(AB-BA)$ is a scalar matrix.","$M_2(\mathbb{R})$ is the set of $2\times 2$ matrices. If $A,B\in M_2(\mathbb{R})$, show that $(AB-BA)(AB-BA)$ is a scalar matrix. I'm a bit stuck with this. Until now I know : Matrix multiplication is associative but in general, not commutative. And other basic properties of matrices. I've tried to do it via the perhaps dumb way, that is, working with the matrix elements themselves: And it works, but is there a less cumbersome way? I've tried to write: $$(AB-BA)(AB-BA)\stackrel{?}{=}\lambda I$$ And multiply both sides by $\frac{1}{\lambda}$ to see if something interesting/helpful happens but got nothing. And I've tried to expand: $$(AB-BA)(AB-BA)$$ But also yielded nothing. So is that the only way to do it or there is some more interesting way?","['matrices', 'linear-algebra']"
1694916,Why aren't all matrices diagonalisable?,"I'm wondering what the difference between canonical forms of a matrix are; in particular why they aren't all equivalent to diagonal matrices. For example, we know that all matrices have an upper triangular form. If we use elementary row and column operations, then wouldn't the matrix become diagonal? Not sure how this changes diagonalisability, since the pre and post-multiplication matrices are still both square and invertible. Similarly for other canonical forms such as Jordan Normal Form. Under this logic, why aren't all matrices diagonalisable? Do I have the definition of diagonalisability wrong?","['matrices', 'linear-algebra']"
1694933,"When solving inequalities, does $(x-9)$ count as a negative number?","I know that when solving inequalities, you reverse the sign when dividing or multiplying by a negative number, but right now I need to solve $\frac{x+9}{x-9}â‰¤2$. Do I need to reverse the sign when I multiply $x-9$ across? I'm sorry if this is outrageously obvious...","['algebra-precalculus', 'inequality']"
1694945,Seeing that the second fundamental form is the orthogonal component of the Laplacian,"I have come across the statement a few times that, for a mapping $u:M\to N$ between a Riemannian manifold $(M,g)$ and a submanifold $N$ of Euclidean space $\mathbb{R}^n$, the part of the Laplacian of M orthogonal to the tangent plane of $N$ is given by the second fundamental form $II$ of $N$ in $\mathbb{R}^n$.
$$(\Delta_gu)^\perp=g^{ij}II(u)(\partial_i u ,\partial_j u)$$
I can't find a proof of this fact or see how to demonstrate it myself. Would anyone be able to offer a proof, or a sketch of a proof?",['differential-geometry']
1694949,Generalized knight going from one corner of a box to the other,"Generalize the notion of a knight from chess to a $(x, y)$-knight such that $x \leq y$ and the knight can move any of the $8$ combinations defined by $(\pm x, \pm y)$ or $(\pm y, \pm x)$. Thus a chess knight is a $(1, 2)$-knight. A $(x, y)$-knight starts at $(0,0)$ and wishes to go to the position $(m, n)$ while staying within the rectangle lattice defined by these two opposite corners. Is there any literature about when this is possible for a $(x, y)$-knight? I think there may be something involving $\gcd(x, y)$ here due to the definition of gcd being the minimal positive value of $ux+vy$, but there are some complications using this method that I can't resolve.","['reference-request', 'recreational-mathematics', 'discrete-mathematics']"
1694976,"Exist vector field having only finitely many zeros, all lying in open set of compact connected manifold?","Let $U$ be any open set on the compact connected manifold $X$. Does there exist a vector field having only finitely many zeros, all of which lie in $U$?","['manifolds', 'general-topology', 'differential-geometry', 'differential-topology']"
1694981,How can I create a $1-\alpha$ nonparametric confidence interval for the median using order statistics?,"I need to create a 95% confidence interval for the median based on 9 ordered statistics. I know how to determine the confidence level for a given interval but I admit I'm stuck when it comes to creating the interval for a given level. Here is my work so far: $0.95=P(Y_i<m<Y_j)$ $\implies0.05=P(i-0.5<W<j+0.5)$ adjusting for correction, with $W\sim\mathrm{bin}(9,0.5)$ with $\mu=4.5$ and $\sigma^2=2.25$ By the central limit theorem, we have that $\frac{W-\mu}{\sigma}\approx Z$, where $Z\sim N(0,1)$. Therefore, $0.95=P(\frac{i-5}{1.5}<Z<\frac{j-4}{1.5})$ There must clearly be a more efficient way of finding $i$ and $j$ then by looking in my tables for the normal distribution in order to find a match. Am I doing something wrong? Also, is there a way to solve a problem like this in R?","['statistics', 'confidence-interval']"
1695020,"Do the moments of a distribution ""almost"" determine it?","There exist probability distributions which are not determined by their moments. See, for example, this question . The first answer in the linked question points out that the log-normal distribution
$$
\frac{\sqrt{2\pi}}{x}e^{-(\log{x})^2/2}
$$
has the same moment sequence as
$$
\frac{\sqrt{2\pi}}{x}e^{-(\log{x})^2/2}(1+a\sin{(2Ï€\log{x})}),
$$
which is just the first distribution multiplied by the factor $(1+a\sin{(2Ï€\log{x})})$. Is there any way in which knowing the moments of a distribution whose moment generating function has radius of convergence zero helps us identify the distribution? That is, is there some result of of the form ""If the moments of the distribution $f$ are the same as the moments of $g$ and [some other condition weaker than $f=g$], then $f=g$""? If there are no general results of this form, I would also be interested in examples of conditions for specific probability distributions, if such examples exist.","['probability-theory', 'probability-distributions']"
1695041,Possible cardinalities of the union of two sets,"So the question is: What are the possible cardinalities of the union of the two sets $A$
  (where $[A] = 5$) and $B$ (where $[B] = 9$) So, the smallest $[A \cup B]$ is when all elements of $A$ are also elements of $B$.
Then, $[A \cup B]$ in this case is: (those 5 similar elements) + (the remaining 4 in B) = 9 And the largest $[A \cup B]$ is when no element of A is in B 
Then, $[A \cup B]$ in this case is: $[A] + [B] = 14$ Then the possible cardinalities of $[A \cup B]$ is: 9, 10, ... , 14 I don't understand how my reasoning is incorrect.
My book says that 6 is a possible cardinality.
The only explanation I could think of is that one or both of the sets has duplicate elements. But, wouldn't the cardinality of a set with duplicate elements be the amount of unique elements? Edit: I actually worded the question for the sake of my explanation. The actual question is: We form the union of a set with 5 elements and a set with 9 elements. Which of the following numbers can we get as the cardinality of the union: 4, 6, 9, 10, 14, 20","['cardinals', 'elementary-set-theory']"
1695056,Papoulis 4th Ed. 2-16 - A box contains n identical balls numbered 1 through n. Suppose k balls are drawn in succession.,"I'm having trouble with the following problem: A box contains $n$ identical balls numbered $1$ through $n$ . Suppose $k$ balls
are drawn in succession. (a) What is the probability  that $m$ is the
largest number drawn? (b) What is the probability that the largest
number drawn is less than or equal to $m$ ? The results are (according to the solutions manual), respectively: $$\frac{m-1 \choose k-1 }{n \choose k }$$ $$\frac{m \choose k }{n \choose k }$$ My understanding of the problem is as follows: There are $n$ balls in total, $k$ are drawn, so there are $k$ random numbers from $1$ to $n$ . I'm asked to find if the greatest number between all the ones that were drawn ( $k$ ), equals $m$ (for part a) and $\le m$ (for part b). I get that the binomial coefficient in the denominator are all the different ways to arrange in a group of $k$ the $n$ balls. But I have no clue on how to find the numerator. Thanks in advance.","['combinatorics', 'statistics', 'probability']"
1695096,A function that is one to one but not onto $\mathbb{Z}$ to $\mathbb{N}$,"Can someone please tell me if I can have a function that is one to one but not onto $\mathbb{Z}$ to $\mathbb{N}$? 
I tried these formulas 
\begin{align*}
F(x) & = x^2+1\\ 
F(x) & = x^2 -1
\end{align*} 
But it always gives me a function that is onto but not not one to one, and I don't want it to be like this.",['functions']
1695097,Reference request for an identity involving binomial coefficients,"The identity is $$\sum_{i\ge 0}(-1)^i \binom{\frac{s^k + s^{-k} -
 10}{4}}{i}\binom{\frac{s^k + s^{-k} - 10}{4}+4}{\frac{gs^k +
2g^{-1}s^{-k} - 4}{8}-i}= 0$$ where $k\gt 1$ , $s =
 3+2\sqrt{2}$ and $ g = 2-\sqrt{2}$ exemples 
 $$\sum_{i\ge 0}(-1)^i \binom{6}{i}\binom{10}{2-i}= 0$$
 $$\sum_{i\ge 0}(-1)^i \binom{47}{i}\binom{51}{14-i}= 0$$
 $$\sum_{i\ge 0}(-1)^i \binom{286}{i}\binom{290}{84-i}= 0$$
$$...$$
 ....etc. It comes from this question . Are there other identities like this one, where the integers are obtained from a recursion? A generalisation (in the neater presentation suggested by @JeanMarie) would be, for a given positive integer $p$ $$\sum_{i\ge 0}(-1)^i \binom{u_k}{i}\binom{u_k+p}{v_k-i}= 0$$ This was the case $p=4$. The cases $p\le 3$ are dealt in the previous question . For $p=5$ we have four pairs of sequences $(u_k,v_k)$ $$u_k=18u_{k-1}-u_{k-2}+48\ \  \ v_k=18v_{k-1}-v_{k-2}+8$$ with 
  $$u_0=2,u_1=-2\ \  \ v_0=6,v_1=0$$ $$u_0=-1,u_1=-1\ \  \ v_0=2,v_1=0$$
  $$u_0=-1,u_1=-1\ \  \ v_0=3,v_1=1$$ $$u_0=-2,u_1=2\ \  \ v_0=1,v_1=3$$ For $p=6$ we have two pairs of sequences $(u_k,v_k)$ $$u_k=14u_{k-1}-u_{k-2}+42 \ \  \ v_k=14v_{k-1}-v_{k-2}+6$$ with 
  $$u_0=-1,u_1=-2\ \  \ v_0=4,v_1=0$$ $$u_0=-2,u_1=-1\ \  \ v_0=2,v_1=0$$ For $p=8$ we have two pairs of sequences $(u_k,v_k)$ $$u_k=6u_{k-1}-u_{k-2}+18 \ \  \ v_k=6v_{k-1}-v_{k-2}+2$$ with 
  $$u_0=-1,u_1=-2\ \  \ v_0=5,v_1=1$$ $$u_0=-2,u_1=-1\ \  \ v_0=3,v_1=1$$ Question: Find other pairs of sequences $(u_k,v_k)$, $v_k \lt u_k$ for $p=7$ or $p\ge 9$ (if any).","['recurrence-relations', 'binomial-coefficients', 'reference-request', 'summation', 'sequences-and-series']"
1695132,Simple method to solve a geometry question for junior high school student,"Rencently, my sister asked me a geometry question that came from her mock examination, please see the following graph. Here, $\angle DOE=45Â°$ the length of $DE$ is constant, and $DE=1$. Namely, $OD,OE$ are changeable. $\triangle DEF$ is equilateral triangle. Q: What is the maximum length of $OF$? My solution Denote $OD,OE,\angle ODE$ as $x,y,\theta$, respectively. Via sine theorem
$$
  \begin{cases}
    ED^{2} = OE^{2} + OD^{2} - 2OE \times OD\cos \angle EOD \\[6pt]
    \cos \theta = \dfrac{EO^{2} + ED^{2} - OD^{2}}{2 EO \times ED}
  \end{cases}
$$
$$
\begin{align}
  1^{2} &= x^{2} + y^{2} - 2xy\cos 45^{\circ} \\
  &= x^{2} + y^{2} - \sqrt{2} xy
\end{align}
$$
$$
  \implies
  \begin{cases}
    \color{red}{xy} = \dfrac{x^{2} + y^{2} - 1}{\sqrt{2}} \color{red}{\leq}
    \dfrac{x^{2} + y^{2}}{2} \implies x^{2} + y^{2} \color{red}{\leq}
    2 + \sqrt{2} \\[6pt]
    \cos \theta = \dfrac{x^{2} + 1 - y^{2}}{2x}
  \end{cases}
$$ Via cosine theorem
$$
  \frac{y}{\sin \theta} = \frac{DE}{\sin \angle EOD} = \frac{1}{\sin
    45^{\circ}} \implies \sin \theta = \frac{y}{\sqrt{2}}
$$
$$\begin{align}
  OF^{2} &= EO^{2} + EF^{2} - 2EO \times EF\cos \angle OEF \\
  &= x^{2} + 1^{2} - 2x\cos(\theta + 60^{\circ}) \\
  &= x^{2} + 1 - 2x(\cos \theta \cos 60^{\circ} - \sin \theta \sin
    60^{\circ}) \\
  &= x^{2} + 1 - 2x\left(\frac{x^{2} + 1 - y^{2}}{2x} \frac{1}{2} -
    \frac{y}{\sqrt{2}} \frac{\sqrt{3}}{2}\right) \\
  &= \frac{x^{2} + y^{2} + 1}{2} + \frac{\sqrt{3} xy}{\sqrt{2}} \\
  &= \frac{x^{2} + y^{2} + 1}{2} + \frac{\sqrt{3}}{\sqrt{2}}
    \frac{x^{2} + y^{2} - 1}{\sqrt{2}} \\
  &= \frac{(\sqrt{3} + 1)(x^{2} + y^{2})}{2} + \frac{1 - \sqrt{3}}{2}
  \\
  &\color{red}{\leq} \frac{(\sqrt{3} + 1)(2 + \sqrt{2})}{2} + \frac{1 -
    \sqrt{3}}{2} = \frac{1}{2}(3 + \sqrt{3} + \sqrt{2} + \sqrt{6})
\end{align}
$$ However, for junior high school student , she doesn't learn the following formulae: sine theorem cosine therem $\cos(x+y)=\cos x \cos y-\sin x \sin y$ fundamental inequality $x y\leq \frac{x^2+y^2}{2}$ Question Is there other simple/elegant method to solve this geometry question? Update Thanks for MXYMXY 's hint Here, the line $O'F$ pass the center of the circle . Namely, $O'D=OF$ In Rt $\triangle O'OF$, the inequality $O'F>OF$ holds.","['optimization', 'geometry']"
1695196,INV problem for $ \frac{dy}{dt} = 2y+3\cos4t $ [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Consider the differential equation
$$ \frac{dy}{dt} = 2y+3\cos4t $$
For what initial values $y(0)$ = $y_0$ are the solutions bounded for all t? My approach is to calculate a general solution,which is $y = ke^{2t} - \frac{3}{5}cos4t + \frac{3}{5}sin4t$. But I don't know how to make it bounded. Please help me to solve this question. Thank you!",['ordinary-differential-equations']
1695198,Constructing integers as equivalence classes of pairs of natural numbers,Can you tell me how to construct the integer numbers ($\mathbb Z$) as equivalence classes of pairs of natural numbers ($\mathbb N$)? And also tell me the commutative and associative law by an equivalence relation. Be sure to use only addition and multiplication.,"['elementary-set-theory', 'elementary-number-theory']"
1695208,Is differentiating on both sides of an equation allowed? [duplicate],"This question already has answers here : When is differentiating an equation valid? (2 answers) Closed 8 years ago . Let's say we have $x^2=25$
So we have two real roots ie $+5$ and $-5$. But if we were to differentiate on both sides with respect to $x$ we'll have the equation $2x=0$ which gives us the only root as $x=0$. So does differentiating on both sides of an equation alter it? If it does, then how do we conveniently do it in Integration by substitutions? If not then what exactly is going on here ?",['calculus']
1695214,Problem in understanding definition of absolutely continuous?,"Suppose $(E, \mathcal{A})$ is a measurable space. Let $\mu$ and $\gamma$ be two distinct measures of this space. Now we say that $\gamma$ is absolutely continuous with respect to $\mu$ if for every $A \in \mathcal{A} , \mu(A) = 0 \Rightarrow \gamma(A) = 0 $. I am unable to understand the essence of this definition of ""absolutely continuous"". I mean what is so absolute and continuous about these measures?","['continuity', 'measure-theory', 'terminology', 'definition']"
1695223,Absolute value in integrating factor of First-Order Linear Differential Equation,"Question states:
$$ y' + \frac{y}{x} = 6x+2$$
Obviously x cannot be zero. If we assume that $x$ is positive (i.e. $x>0$), we find the integrating factor as $$u(x)=e^{\int \frac{1}{x} dx}$$ which is equal to $x$. Then the solution is $$y(x)= \frac{1}{u(x)} \int (6x+2)(u(x)) dx = \frac {1}{x} \int 6x^2+2x \ dx = 2x^2+x+\frac{C}{x}.$$
Now, we assumed that $x$ is positive. But I couldn't get the same answer when I didn't make this assumption; that is, the integrating factor is $$u(x)=e^{\int \frac{1}{x} dx} = e^{ \ln \lvert x\rvert} = \lvert x\rvert.$$
Then this problem gets way more complicated, as the solution becomes
$$y(x)= \frac{1}{u(x)} \int (6x+2)(u(x)) dx = \frac {1}{\lvert x\rvert} \int 6x \lvert x\rvert +2\lvert x\rvert \ dx.$$
My calculus textbook omitted the absolute value altogether; that is, the textbook indicated that the integrating factor was just $x$. Because the textbook is written by quite reputable and trustworthy authors (Ron Larson and Bruce Edwards), I was wondering (1) if treating the integrating factor as just $x$ is acceptable, and/or (2) How the solution is still correct if we must use the integrating factor as $\lvert x\rvert$. If we can omit the absolute value sometimes, how do we know when we can omit the absolute value sign and when we shouldn't? (As a side note, I fully understand why there's absolute value sign for the antidervative of $ \frac{1}{x} $).",['integration']
1695229,Understanding how this derivative was taken,"I am pouring water into a conical cup 8cm tall and 6cm across the top.
  If the volume of the cup at time t is $V(t)$, how fast is the water
  level ($h$) rising in terms of $V'(t)$? The solution in the book is: Take the water volume, given by $$\frac{1}{3}\pi(\frac{3}{8})^2h^3$$ Then differentiate with respect to $t$: $$V' = h'\pi(\frac{3}{8})^2h^2$$ Which gives $$h' = \frac{64V'}{9{\pi}h^2}$$ I did not understand how this differentiation happened, when there was no $t$ in the formula to differentiate ! If you differentiate with respect to $h$, though, you get something similar: $$\dfrac{9{\pi}h^2}{64}$$ But I'm not sure how $V'$ fits into this. Thanks in advance!",['derivatives']
1695248,Question on upper triangular matrix with complex eigenvalues with modulus less than 1,"This is problem 16, Section 6.B from Linear Algebra Done Right, 3rd Edition. Suppose the field is $\mathbb{C}$, $V$ is finite-dimensional, $T \in  \mathcal{L}(V)$, all the eigenvalues
of $T$ have absolute value less than 1, and $\epsilon > 0$.
 Show that there exists a positive integer $m$ such that $||T^m v|| < \epsilon ||v|| \; \forall \; v \in V$. At this point in the book Jordan form has not been proved, so all I can use is Schur's Theorem that guarantees upper triangular matrix with eigenvalues on the diagonal. Could anyone give me a hint on how to proceed ? I tried computing powers of $T$, but while I get $\lambda^m$ on diagonal and same upper triangular structure, I am unable to manage the off-diagonal entries and control their growth. Note that in this case if matrix is split as diagonal and strictly upper diagonal matrix as $T = D + U$, $D$ and $U$ do not commute because entries on diagoals of $D$ are unequal.","['matrices', 'normed-spaces', 'eigenvalues-eigenvectors']"
1695275,Simplify the series given by the recurrence relation $na_n=2a_{n-2}$,"If you are given a recurrence relation such that: $$na_n=2a_{n-2}\implies a_n= \begin{cases}
0  & \text{odd} \,n \\
\frac{2}{n}a_{n-2} & \text{even} \,n
\end{cases}$$ My textbook suggests that the series can be simplified by Putting $n=2m$ (since only even terms appear in this series), we get $$a_{2m}=\frac{2}{2m}a_{2m-2}=\bbox[#AF0]{\frac{1}{m}a_{2m-\color{red}{2}}=\frac{1}{m}\color{red}{\frac{1}{(m-1)}}a_{2m-\color{red}{4}}}=\frac{1}{m!}a_0$$ I understand the final equality as $$\frac{1}{m}\frac{1}{(m-1)}\frac{1}{(m-2)}\frac{1}{(m-3)}\cdots=\frac{1}{m!}$$ But I do not understand the highlighted equality. Why does the $\color{red}{\cfrac{1}{m-1}}$ appear when $a_{2m-\color{red}{2}}$ is reduced to $a_{2m-\color{red}{4}}$?","['algebra-precalculus', 'recurrence-relations', 'real-analysis', 'sequences-and-series']"
1695302,Does there exist an injective function $f:\mathbb R^2 \to \mathbb R$ such that $f$ is continuous in one of the variables $x$ or $y$?,Does there exist an injective function $f:\mathbb R^2 \to \mathbb R$ such that $f$ is continuous in one of the variables $x$ or $y$ ? I only know that such an injection cannot be continuous in each real variable $x$ and $y$ . Please help . Thanks in advance,"['multivariable-calculus', 'real-analysis', 'metric-spaces', 'continuity']"
1695304,how to find the following limit,"Problem : Find the following limit $${\displaystyle \lim_{n\rightarrow\infty}\left[\left(1+\frac{1}{n}\right)\sin\frac{\pi}{n^{2}}+\left(1+\frac{2}{n}\right)\sin\frac{2\pi}{n^{2}}+\ldots+\left(1+\frac{n-1}{n}\right)\sin\frac{\left(n-1\right)}{n^{2}}\pi\right]}$$. Attempt: Observe that 
\begin{align*}
 & \lim_{n\rightarrow\infty}\left[\left(1+\frac{1}{n}\right)\sin\frac{\pi}{n^{2}}+\left(1+\frac{2}{n}\right)\sin\frac{2\pi}{n^{2}}+\ldots+\left(1+\frac{n-1}{n}\right)\sin\frac{\left(n-1\right)}{n^{2}}\pi\right]\\
= & \lim_{n\rightarrow\infty}\left[\left(1+\frac{1}{n}\right)\left(\frac{\pi}{n^{2}}+O\left(\frac{\pi^{3}}{n^{6}}\right)\right)+\ldots+\left(1+\frac{n-1}{n}\right)\left(\left(\frac{n-1}{n^{2}}\right)\pi+O\left(\frac{\left(n-1\right)^{3}\pi^{3}}{n^{6}}\right)\right)\right]\\
= & \lim_{n\rightarrow\infty}\left[\left(\sum_{k=1}^{n}\left(1+\frac{k}{n}\right)\frac{k\pi}{n^{2}}\right)+\left(\sum_{k=1}^{n}\left(1+\frac{k}{n}\right)O\left(\frac{k^{3}\pi^{3}}{n^{6}}\right)\right)\right]\\
= & \lim_{n\rightarrow\infty}\left[\left(\sum_{k=1}^{n}\left(1+\frac{k}{n}\right)\frac{k\pi}{n^{2}}\right)+\sum_{k=1}^{n}\left(1+\frac{k}{n}\right)O\left(\frac{1}{n^{3}}\right)\right]\tag{1}
\end{align*}
Note that 
\begin{align*}
\lim_{n\rightarrow\infty}\sum_{k=1}^{n}\left(1+\frac{k}{n}\right)\frac{1}{n^{3}} & =\lim_{n\rightarrow\infty}\left(\frac{1}{n^{2}}+\sum_{k=1}^{n}\frac{k}{n^{4}}\right)\leq\lim_{k\rightarrow\infty}\left(\frac{1}{n^{2}}+\frac{1}{n^{3}}\right)=0.
\end{align*}
Hence 
\begin{align*}
\left(1\right) & =\lim_{n\rightarrow\infty}\left[\sum_{k=1}^{n}\left(1+\frac{k}{n}\right)\frac{k\pi}{n^{2}}\right]=\pi\lim_{n\rightarrow\infty}\left[\sum_{k=1}^{n}\left(\frac{k}{n}+\left(\frac{k}{n}\right)^{2}\right)\frac{1}{n}\right]\\
 & =\pi\int_{0}^{1}x+x^{2}dx=\frac{5\pi}{6}.
\end{align*} Question I don't know if I did it correctly.","['proof-verification', 'real-analysis', 'calculus', 'limits']"
1695308,Support and stalks at generic points,"Let $X$ be an noetherian scheme, $Y$ an irreducible closed subscheme of $X$ with generic point $y$ and $\mathscr G$ a coherent sheaf of $\mathscr O_X$-modules. Consider the following statement: If $\mathscr G_y\ne 0,$ then $\operatorname{Supp}(\mathscr G)=Y.$ I know this holds for $X=Y$ but I think it isn't correct otherwise because I can take $Y=y$ to be a closed point and $\mathscr G = \mathscr O_X$ and violate the statement. But unless I am reading it wrong, the proof of the ""lemme de dÃ©vissage"", Theorem 3.1.2 in EGA III, and even more explicitly the statement of Corollary 3.1.3 which follows it, both claim just the above statement. I am confused; could someone please explain how I should understand what's written in EGA (or is it perhaps a mistake and if so, how should one correct it)? Thanks a lot! Edit: Let me elaborate on what bugs me. Theorem 3.1.2 only contains the condition that for every irreducible closed subset $Y\subset X$ with a generic point $y$ there should exist a sheaf $\mathscr G\in \mathbf K'$ such that $\mathscr G_y$ is a one-dimensional vector space over $k(y).$ But then in the proof when $\mathscr G$ is called upon (second paragraph on p. 116) there is a passage: Par hypothÃ¨se, il y a un $\mathscr O_X$-Module cohÃ©rent $\mathscr  G$ (nÃ©cessairement de support $Y$) tel que $\mathscr G_y$ soint un $k(y)$-espace vectoriel de dimension $1.$ Then in the course of the proof this is used in the proof and the fact that $\mathscr G^m$ has support inside $Y$ is reiterated.
Then finally the statement of Corollary 3.1.3 literally contains the following: ... est remplacÃ©e par $\mathscr G_y\ne 0$ (ce qui Ã©quivaut Ã  $\operatorname{Supp}(\mathscr G) = Y$). From all of that it's hard not to come to the conclusion that the statement I wrote above is being implied.","['schemes', 'coherent-sheaves', 'algebraic-geometry']"
1695309,"If $f(x)=\lim_{n\to\infty}[2x+4x^3+\cdots+2nx^{2n-1}]$, $0<x<1$, then find $\int f(x)\mathrm{d}x$","If $f(x)=\lim_{n\to\infty}[2x+4x^3+\cdots+2nx^{2n-1}]$, $0<x<1$, then find $\int f(x)\mathrm{d}x$ $$f(x)=\lim_{n\to\infty}2x[1+2x^2+\cdots+nx^{2n}]$$
$$S=\frac{f(x)}{2x}=1+2x^2+\cdots+nx^{2n}$$
$S$ is an AGP. I used the general method for finding $S$.
$$x^2S=x^2+2x^4+\cdots+nx^{2n+2}$$ $$(1-x^2)S=1+(x^2+x^4+\cdots+x^{2n})+nx^{2n+2}$$
$$(1-x^2)S=\frac{1-x^{2n+1}}{1-x^2}+nx^{2n+2}$$
$$S=\frac{1-x^{2n+1}}{(1-x^2)^2}+\frac{nx^{2n+2}}{1-x^2}$$
$$f(x)=\lim_{n\to\infty}\left(2x\frac{1-x^{2n+1}}{(1-x^2)^2}+2x\frac{nx^{2n+2}}{1-x^2}\right)$$
I got stuck here.","['integration', 'sequences-and-series', 'calculus', 'limits']"
1695381,How to simplify the nested radical $\sqrt{1 - \frac{\sqrt{3}}{2}}$ by hand?,I was solving a Mock Mathcounts Contest Mock contest (.pdf) written by a user on the Art of Problem Solving Forums. In problem #24 the only thing I couldn't do by hand was simplify the radical mentioned above. Note that the contest should involve math topics accessible to a high performing Mathcounts middle school student. How do you simplify $$\sqrt{1 - \frac{\sqrt{3}}{2}}$$,"['algebra-precalculus', 'contest-math', 'nested-radicals']"
1695394,"if $f(x + y) = f(x)f(y)$ is continuous, then it has to be injective.","Let $f$: $\Bbb R$  $\rightarrow$ $\Bbb R$ be a non-constant function such that $f(a + b) = f(a)f(b)$ for all real numbers $a$ and $b$. Prove that if $f(x + y) = f(x)f(y)$ is continuous, then it has to be injective. I derived a few useful properties from this function such that
$f(x)\neq0;f(0) = 1 ; f(x) = \frac{1}{\mathrm f(-x)}$ In order to prove $f$ is injective, I need to prove that
$f(x) = f(y)$ implies that $x = y$ I suppose that there are $x, y \in \Bbb R$ such that $f(x) = f(y)$
$$f(x) = \frac{1}{f(-y)}$$
$$f(x)f(-y) = 1$$
$$f(x - y) = 1$$
To finish the proof I need to prove $f(x) = 1$ implies $x = 0$. This is where I got stuck. I don't know if I'm on the right track. Any suggestion is much appreciated.","['continuity', 'exponential-function', 'functions', 'functional-equations']"
1695395,Does the function $\log(1+\exp(x))$ have a conventional name?,"Does the function $\log(1+\exp(x))$ (or the function $\log(1+\exp(-x))$) have a conventional or at least fairly common name? Alternatively, is it closely related to some reasonably well-known, named function? [It arises in a problem I'm working on and it would be nice to use a reasonably standard terminology and notation rather than either writing $\log(1+\exp(g(x)))$ each time (where $g$ is a fairly complicated set of terms), or coming up with a non-standard notation for it (like $\omega(g(x))$ say) and then discovering I could have used a symbol and term for it that are already recognized). It would also help in terms of locating properties, should I end up needing any beyond the simple ones I've already derived.]","['terminology', 'functions']"
