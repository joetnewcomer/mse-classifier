question_id,title,body,tags
4729087,Show $Q(\eta_n) := \mathbb P \{\exists m < \infty : \eta_m = 0 |\eta_0 = \eta_n\}$ is a martingale,"I am trying to solve this problem Let $\eta_n$ be a homogeneous Markov chain on the countable state
space $S : =\{ 0,1,2,...\}$ and $\mathcal F_n := \sigma(\eta_j; 0 \le j \le n), n \ge 0$ its natural
filtration. For $i \in S$ denote by $Q(i)$ the probability that the
Markov chain starting from site $i$ ever reaches the point $0 \in S$ : $Q(i) := \mathbb P \{\exists m  < \infty : \eta_m = 0 |\eta_0 = i\}$ Prove that $Z_n := Q(\eta_n)$ is an $(\mathcal F_n)_{n\ge 0}$ -martingale I read the definition of Markov chain, transition probability and homogeneous Markov chain, apart from that I don't know much about Markov chains. Still I am not making much sense of this problem To prove the martngale property, I don't know how to manage $Q$ in $E[Q(\eta_{n+1})|\mathcal F_n)]=E[\mathbb P \{\exists m  < \infty : \eta_m = 0 |\eta_0 = \eta_{n+1}\}|\mathcal F_n)]$ How  am I supposed to compute the probability of a probability(That is why I need to compute the expectation I guess) or equivalently the expectation of a probability? $Q(\eta_n)$ does not seem like a random variable Moreover I am not sure how to check the other conditions either. $Z_n$ does not seem adapted as it depends on future events, i.e. on some $m>n+1$ , i.e. $\eta_m$ which is not $\mathcal F_n$ -measurable.  I am not sure how to check $E[|Z_n|]< \infty $ either I am not sure if this makes a lot of sense: $ Q(\eta_{n+1})= \mathbb P \{\exists m  < \infty : \eta_m = 0 |\eta_0 = \eta_{n+1}\}$ then $n+1=0 $","['martingales', 'markov-chains', 'probability-theory', 'probability']"
4729126,"Show that $C([0,1])$ is dense in $L^p_w([0,1])$, for an integrable function $w\ge0$","Let $1 \leq p < \infty$ and $w$ be a non-negative integrable function on $[0,1]$ . Show that the space of continuous function $C([0,1])$ is dense in $L^p_w([0,1])$ with the following norm $$\Vert f \Vert_{L^p_w([0,1])} := \left( \int_0^1 |f(x)|^p w(x) dx \right)^{1/p}$$ My attempt: I am able to show the following Given any interval $I =[a,b] \subset [0,1]$ and $\epsilon >0$ there exists a continuous function $\phi$ on $[0,1]$ such that $\phi \geq \chi_I$ and $\int_0^1 |\phi|^p w dx \leq \int_a^b w dx + \epsilon$ . So, suppose $f \in L^p_w([0,1])$ and $\epsilon > 0$ be given then $|f|^p w \in L^1([0,1])$ and non-negative and taking $\mu(dx) = |w(x)| dx$ , there exists a continuous $\phi$ on $[0,1]$ so that $\phi \geq 1$ on $[0,1]$ and $$\int_0^1 |\phi(x)|^p |f(x)|^p w(x) dx \leq \int_0^1 |f(x)|^p w(x) dx + \epsilon $$ $$\iff \int_0^1 |\phi|^p |f|^p  d\mu \leq \int_0^1 |f|^p  d \mu + \epsilon.$$ However, I am not sure how to estimate $\int_0^1 |\phi - f|^p  d \mu$ using the above inequality. Any hints are appreciated.","['integration', 'measure-theory', 'normed-spaces', 'real-analysis']"
4729128,"Form of invariant measures for SDsE on the toroidal domain $[0,1)^d$","Consider the SDE $${\rm d}X_t=b(X_t){\rm d}t+\sigma(X_t){\rm d}W_t\tag1$$ with Lipschitz continous $b:\mathbb R^d\to\mathbb R^d,\sigma:\mathbb R^{d\times d}\to\mathbb R$ and a $d$ -dimensional Brownian motion $(W_t)_{t\ge0}$ . Now, let $$\iota:\mathbb R^d\to[0,1)^d\;,\;\;\;x\mapsto x-\lfloor x\rfloor,$$ where $\lfloor x\rfloor:=(\lfloor x_1\rfloor,\ldots,\lfloor x_d\rfloor)$ . Question : Can we prove existence of a solution to an SDE similar to $(1)$ , but evolving in $[0,1)^d$ (with toroidal wrap arround at the boundary) instead of $\mathbb R^d$ ? Would the correct formulation of that SDE be $$X_t=\iota\left(X_0+\int_0^tb(X_s)\:{\rm d}s+\int_0^t\sigma(X_s)\:{\rm d}W_s\right)\tag2?$$ If that's correct, does $(2)$ admit an invariant measure? My hope would be that existence of an invariant measure would be easier to verify here, since one problem in $\mathbb R^d$ is that ""all the probability mass could go to infinity""; which is not possible in $[0,1)^d$ with the wrap around. Can we also give an explicit form of (the density of) that measure? EDIT : Let me concretize my question: In my actual application, $b,\sigma$ are only defined on $[0,1)^d$ . I guess this is not a problem, since we can extend them periodically to $\mathbb R^d$ by $$b(x):=\sum_{k\in\mathbb Z^d}1_{[k,\:k+1)}(x)b(x-k)\tag3$$ and analogously for $\sigma$ . Next, I've also got a probability density $p:[0,1)^d\to[0,\infty)$ and would like to see how I need to choose $b$ and $\sigma$ such that the wrapped solution of the SDE has an invariant measure with density $p$ . The paper Langevin diffusions on the torus: estimation and applications is close to what I'm asking, but I don't really understand why Proposition 1 holds (asked for that here ) and it is somehow different in the treatment of $p$ .","['measure-theory', 'stochastic-analysis', 'stochastic-processes', 'stochastic-differential-equations', 'probability-theory']"
4729181,Proving that quotient of orthogonal polynomials is a Padé approximant of Stieltjes transform.,"Conext and notation. In the question below, $R_n(z) / S_n(z)$ denotes the n-th convergent of the continued fraction $$ \frac{1|}{|z-b_1} + \frac{-a_2|}{|z-b_2} + \dots + \frac{-a_n|}{|z-b_n} + \dots,$$ where $a_n,b_n$ are the coefficients of a generic orthogonal sequence of monic polynomials $\{p_n(z)\}$ that satisfies the three term recurrence relation $$ p_n(z) = (z-b_n)p_{n-1}(z) - a_np_{n-2}(z), \quad \text{ for } \, \, n= 1,2,\dots$$ The problem. Define the function $$ \hat w(z) = \int_a^b \frac{1}{z-t}w(t) dt,$$ which is normally known as the Stieltjes transformation. I wish to prove that \begin{equation} \tag{1} \hat w(z) - \frac{R_n(z)}{S_n(z)} = \frac{1}{p_n(z)}\int_a^b \frac{p_n(t)}{z-t}w(t) \, dt.\end{equation} and \begin{equation} \tag{2} \hat w(z) - \frac{R_n(z)}{S_n(z)} = \frac{k_n^1}{z^{2n+1}} + \frac{k_n^2}{z^{2n+2}} + \dots, \quad |z| > R,\end{equation} where $R$ is big enough to guarantee uniform convergence of the series $\displaystyle{\sum_{j=0}^\infty \frac{t^j}{z^{j+1}}}.$ My attempt. I was able to prove $(1)$ with ease. Identity $(2)$ gave me quite some more problems. Follows my attempt: We have that \begin{equation*}
 \begin{split}
 \hat w(z) - \frac{R_n(z)}{S_n(z)} &= \frac{1}{p_n(z)} \int_a^b
 \frac{p_n(t)}{z-t}w(t) \, dt \\[.25cm]
&= \frac{1}{p_n(z)} \int_a^b \sum_{j=0}^\infty \frac{t^j}{z^{j+1}}p_n(t) w(t) \, dt \\[.25cm]
&= \frac{1}{p_n(z)} \sum_{j=0}^\infty \frac{1}{z^{j+1}} \boxed{\int_a^b t^j p_n(t) w(t) \, dt}.
\end{split}
\end{equation*} So, all we have to do is study the boxed integral above. From the theory of orthogonal polynomials, we know that for $j<n$ this integral is zero and for $j=n$ we have that $$ \int_a^b t^np_n(t) w(t) \, dt = \gamma_n h_n, $$ where $\displaystyle{h_n = \int_a^b p_n^2(t)w(t) \, dt}$ and $\gamma_n$ is such that $\displaystyle{t^n = \sum_{i=0}^n \gamma_i p_i(t)}$ (recall that $\{p_0(t),\dots,p_n(t)\}$ forms a basis for the vectorial space of the polynomials in one variable of degree equal or smaller than $n$ ). So everything we have to do is to study the boxed integral for values of $j$ such that $j > n.$ For this cases, it is clear that $\{ p_0(t),\dots,p_j(t)\}$ forms a basis for the vectorial space of the polynomials in one variable of degree equal or smaller than $j$ . Therefore, we can find scalars $\delta_i$ such that $$ t^j = \sum_{i=0}^j \delta_i p_i(t). $$ Then, $$ \int_a^b t^j p_n(t) w(t) \, dt = \sum_{i=0}^j \delta_i \int_a^b p_i(t)p_n(t) w(t) \, dt = \delta_n h_n. $$ Therefore, we have that $$ \hat w(z) - \frac{R_n(z)}{S_n(z)} = \frac{1}{p_n(z)}\left[ \frac{\gamma_n h_n}{z^{n+1}} + \sum_{j=n+1}^\infty \frac{\delta_n h_n}{z^{j+1}} \right].$$ Thanks for any help in advance.","['stieltjes-integral', 'real-analysis', 'complex-analysis', 'orthogonal-polynomials', 'continued-fractions']"
4729241,Show that $ \left\{ \sum_{k=0}^\infty(-1)^{k+1}\frac{2^{2k}-1}{2^{2k-1}}\zeta(2k) \right\}=\frac{\pi}{2}\tanh\frac{\pi}{2}-1 $,"I want to prove the following: $$ \left\{ \sum_{k=0}^\infty(-1)^{k+1}\frac{2^{2k}-1}{2^{2k-1}}\zeta(2k) \right\}=\frac{\pi}{2}\tanh\frac{\pi}{2}-1 $$ where $ \{ x \} $ is the fractional part of $x$ . In fact, this is an abuse of notation, since actually the series doesn't converge. What it does is oscillate between two values that are exactly an integer apart (as you can see here ), so they have the same fractional part, hence the notation. It would have been more correct to write: $$\lim_{N \rightarrow \infty}\left\{\sum_{k=0}^N(-1)^{k+1}\frac{2^{2k}-1}{2^{2k-1}}\zeta(2k) \right\}=\frac{\pi}{2}\tanh\frac{\pi}{2}-1$$ However, I attacked the problem in several ways, including: using $\zeta(s)=\sum_{n=1}^\infty\frac{1}{n^s}$ and then exchanging the order of summation we get: $$\sum_{k=0}^N(-1)^{k+1}\frac{2^{2k}-1}{2^{2k-1}}\zeta(2k)=\sum_{k=0}^N(-1)^{k+1}\frac{2^{2k}-1}{2^{2k-1}}\sum_{n=1}^\infty\frac{1}{n^{2k}}=$$ $$=\sum_{n=1}^\infty\sum_{k=0}^N(-1)^{k+1}\frac{2^{2k}-1}{2^{2k-1}}\frac{1}{n^{2k}}$$ which doesn't seem much easier. using $\zeta(2s)=\frac{(-1)^{s+1}B_{2s}(2\pi)^{2s}}{2(2n)!}$ : $$\sum_{k=0}^N(-1)^{k+1}\frac{2^{2k}-1}{2^{2k-1}}\zeta(2k)=\sum_{k=0}^N\frac{(2^{2k}-1)B_{2k}\pi^{2k}}{(2k)!}$$ but I don't recognise this to be a known sum with Bernoulli numbers. Plus notice the fact that $$\frac{\pi}{2}\tanh\frac{\pi}{2}=\frac{i}{2}\left[\psi\left(\frac{1-i}{2}\right)- \psi\left(\frac{1+i}{2}\right) \right]$$ which didn't help me at all but perhaps it can be useful. The real problems are that all these sums are not converging, but rather oscillating, and it's not clear how and when to use the fractional part to get the desired result. Maybe using some integral representation of the zeta function? Any ideas?","['integration', 'analysis', 'real-analysis', 'calculus', 'sequences-and-series']"
4729319,The closure of the union of a countable disjointed family of metric balls is the union of the closures,"Let $(X,d)$ be a metric space and let $\{x_n\}_{n\in\mathbb{N}}\subseteq X$ and $ \{r_n\}_{n\in\mathbb{N}}\subseteq (0,+\infty)$ sequences such that $\overline{B(x_n,r_n)}\cap\overline{B(x_m,r_m)}=\emptyset$ for $m, n\in\mathbb{N}$ with $m\ne n$ . My question is the following one : is the equality $\overline{\bigcup_{n\in\mathbb{N}} B(x_n,r_n)}=\bigcup_{n\in\mathbb{N}}\overline{B(x_n,r_n)}$ true? If it's, true how can I prove it? If it's false, it would be true if I suppose that $X$ is separable? Can anyone help me please? Proof Attempt. Clearly $\overline{\bigcup_{n\in\mathbb{N}} B(x_n,r_n)}\supseteq\bigcup_{n\in\mathbb{N}}\overline{B(x_n,r_n)}$ . The problem is to show the remaining inclusion, i.e. $\overline{\bigcup_{n\in\mathbb{N}} B(x_n,r_n)}\subseteq\bigcup_{n\in\mathbb{N}}\overline{B(x_n,r_n)}$ .","['general-topology', 'geometry', 'metric-spaces']"
4729361,$\alpha$-quantum wedge in Liouville Quantum Gravity,"We consider a proper simply connected subdomain $D\subset\mathbb{C}$ and let $\overline{\mathcal{D}}(D)$ be the space of smooth functions in $D$ with finite Dirichlet energy, which we consider modulo constants. The closure of $\overline{\mathcal{D}}(D)$ with respect to the Dirichlet inner product $$\langle f,g\rangle_\nabla:=\frac{1}{2\pi}\int_D\nabla f\cdot\nabla g$$ will be denoted by $\overline{H}^1(D)$ . Let $\{\overline{f}_j\}_{j\in\mathbb{N}}$ be an orthonormal basis for $\overline{H}^1(D)$ and let $\{X_j\}_{j\in\mathbb{N}}$ be independent $\mathcal{N}(0,1)$ random variables. The random series $$\overline{h}_n:=\sum_{j\in\mathbb{N}}X_j\overline{f}_j$$ converges almost surely in the space of distributions modulo constants, and the law of the limit does not depend on the choice of orthonormal basis $\{\overline{f}_j\}_{j\in\mathbb{N}}$ . The Neumann Gaussian free field (NGFF) is defined to be the limit $h$ of this random series. Let $S=\mathbb{R}\times(0,\pi)$ be an infinite strip. We can decompose $\overline{H}^1(S)$ as follows: Let $\overline{\mathcal{H}}_{\text{rad}}$ be the subspace of $\overline{H}^1(S)$ obtained by closing the smooth functions which are constant on each vertical segment, modulo constants and let $\mathcal{H}_{\text{circ}}$ be the subspace obtained by closing off the smooth functions which have mean zero on all vertical segments. We have that $$\overline{H}^1(S)=\overline{\mathcal{H}}_{\text{rad}}\oplus\mathcal{H}_{\text{circ}}.$$ This allows us to write the NGFF on $S$ as $h=h_{\text{rad}}^{\text{GFF}}+h_{\text{circ}}^{\text{GFF}}$ , where $h_{\text{rad}}^{\text{GFF}},h_{\text{circ}}^{\text{GFF}}$ are independent, $h_{\text{rad}}^{\text{GFF}}(z)=B_{2s}$ if $\text{Re}(z)=s$ , where $B$ is an independent standard Brownian motion modulo constants, and $h_{\text{circ}}^{\text{GFF}}(z)$ has mean zero on each vertical segment. Now let $\tilde{\mathcal{M}}$ be the space of signed Radon measures $\rho$ on $S$ with finite and equal positive and negative mass, so that $\int_D\rho(dx)=0$ , such that $$\overline{f}\mapsto\int_D\overline{f}(x)\rho(dx)$$ defines a continuous linear functional on $\overline{H}^1(S)$ . We define $$\mathcal{M}:=\{\rho:\rho=\tilde{\rho}+f\text{ with }\tilde{\rho}\in\tilde{\mathcal{M}}\text{ and }f\in\mathcal{D}_0(S)\},$$ where $\mathcal{D}_0(S)$ is the space of smooth test functions that are compactly supported in $S$ . An $\alpha$ -quantum wedge is defined as follows: Let $$h_{\text{rad}}=\begin{cases}B_{2s}+(\alpha+Q),&\text{if Re}(z)=\text{ and }s\geq0;\\\widehat{B}_{-2s}+(\alpha-Q)s,&\text{if Re}(z)=s\text{ and }s<0,\end{cases}$$ where $B$ is a standard Brownian motion, and $\widehat{B}$ is an independent Brownian motion conditioned so that $\widehat{B}_{2t}+(Q-\alpha)t>0$ for all $t>0$ and $Q$ is just a constant $Q=\frac{2}{\gamma}+\frac{\gamma}{2}$ and $\gamma\in[0,2)$ . Let $h_{\text{circ}}$ be a stochastic process indexed by $\mathcal{M}$ that is independent of $h_{\text{rad}}$ and has the same law as $h_{\text{circ}}^{\text{GFF}}$ . We call $h=h_{\text{rad}}+h_{\text{circ}}$ an $\alpha$ -quantum wedge in $S$ . Can someone help me understand why the $\alpha$ -quantum wedge is important for LQG? To my eyes the definition includes a lot of formalities that don't necessarily form a good picture or vision in my head. Why is this construction necessary/useful? Edit: I want to explain why this is a math question, and not a physics question, since I've been asked. Liouville quantum gravity (LQG) is a ""poorly"" named field of math. It was inspired by Polyakov's 1984 seminal work The Quantum Geometry of Bosonic Strings . From that paper blossomed the field of Liouville conformal field theory (LCFT), a 2D theory of quantum gravity. When physicists first tried to solve LCFT, they did so through a nonrigorous analytic continuation argument, from which they found the DOZZ formula -- A sort of good guess. When mathematicians came to tackle the problem (just like how the Feynman path integral is formalized by Brownian bridges), they discovered that a two dimension version of the Brownian bridge, called the Gaussian free field (GFF), was involved. Now, LQG has branched off significantly from LCFT: The rigorous proof of the DOZZ formula was recently completed, and the next goal is to work on the conformal bootstrap (this is work in LCFT; I'll be publishing notes soon on the rigorous proof of the DOZZ formula, since it takes hundreds of pages across multiple papers and books). LQG on the other hand, focuses more on the random geometry. We consider things like random models on graphs, and discover that their scaling limits are Liouville quantum gravity surfaces, explicitly construct a random metrics, and as you saw above, notions of GFF, etc. This is only the tip of the iceberg for LQG, which has deep connections to imaginary geometry, SLE and other random processes, etc., and even the motivating LCFT.","['analysis', 'complex-analysis', 'stochastic-processes', 'brownian-motion', 'probability-theory']"
4729384,"Let $T: \mathbb{Z}\rightarrow\{0\}\cup X, X$ is a finite set. Only a finite number of integers are not sent to $0$. The set of all $T$ is countable?","My tentative: For some $T$ , let $S$ be the finite set of numbers that are not sent to $0$ . Then we can write $T$ as: $$ T(x)=\begin{cases}
T(x), & \text{ if } x \in S \newline
0,& \text{ otherwise}  
\end{cases}$$ So counting the $T$ functions is the same that counting the $T_S$ functions ( $T$ restricted to $S$ ) for all $S$ Then exists a finite number of $T_S:S\rightarrow X$ for a fixed $S$ Since $S$ is a finite subset of $\mathbb{Z}$ , the set of all possible $S$ is countable. Then total number of $T$ is a series with infinite countable finite terms, so the number of $T$ is countable. Is this correct?","['elementary-set-theory', 'functions', 'analysis']"
4729441,Can the Stinespring Dilation Theorem extend to *unbounded* operators?,"I have some (possibly basic) questions about $C^*$ -algebras, the Stinespring Theorem (Theorem 3.6 in Takesaki's book ), and unbounded operators. This is motivated by quantum mechanics, where unbounded operators are common. For example, the momentum operator $\hat{p}$ (which is related to $-i \partial^{\,}_x$ ), or the boson number operator $\hat{n}$ , whose eigenvalues are $n \in \mathbb{N}^{\,}_0$ . For reference, I have come across these two works from 2008 and from 2020 , which seem relevant, though I lack the background to understand them fully in finite time. Do the two references above generically extend the Stinespring Theorem to unbounded operators? In other words, if a Hilbert space $\mathcal{H}$ admits unbounded operators, and I replace the usual set of bounded operators $\mathcal{B}(\mathcal{H})$ with the set  End( $\mathcal{H}$ ) of all (possibly self-adjoint) linear operators on $\mathcal{H}$ , does the following aspect of the Stinespring Theorem still hold: Let $\mathcal{H}$ and $\mathcal{K}$ be Hilbert spaces, with corresponding operator sets ${\rm End}(\mathcal{H})$ and ${\rm End}(\mathcal{K})$ . If the adjoint map $\psi^* = \phi: \mathcal{A} \to {\rm End}(\mathcal{H})$ is completely positive and unital, then there exists a unital *-homomorphism $\pi : \mathcal{A} \to {\rm End} (\mathcal{K})$ and an isometry $V : \mathcal{H} \to \mathcal{K}$ such that, $ \forall A \in \mathcal{A}$ , $\phi (A) = V^* \pi (A) V$ , up to multiplication by a unitary. The statement above is the part of the Theorem relevant to me, and please correct me if I've been sloppy. For quantum mechanics, the $C^*$ -algebra $\mathcal{A}$ is unital, the map $\psi$ and its adjoint $\phi$ are generally completely positive and trace preserving (CPTP), we can restrict to self-adjoint operators in End( $\mathcal{H}$ ), and/or require that dim( $\mathcal{K}$ ) ≥ dim( $\mathcal{H}$ ), if helpful. If these papers are not familiar—and because I am new to *-algebras—it would also be helpful to me to know, e.g., what goes wrong in trying to construct $C^*$ -algebras with unbounded operators, if anything? If there are issues, do they persist if we restrict to unital algebras / CPTP maps / self-adjoint operators? Or, are things like ""completely positive"" or other aspects of $C^*$ -algebras difficult to define with unbounded operators? Lastly, do unbounded operators pose a particular obstacle to proving the Stinespring Theorem beyond the considerations above? If this is a bad question in any respect, please let me know how I could improve it / what is missing. I did not find any similar questions while searching Stack, and my more general search came up with the two papers referenced above. Still, if you think another answer might be relevant, please comment with a link.","['c-star-algebras', 'linear-algebra', 'functional-analysis', 'unbounded-operators']"
4729448,Difference Between Riemann Integrals and Definite Integrals,"I'm currently using a textbook that covers Calculus and Analytic Geometry, however, the author seems to have confused me with notations and definitions. He first proves that the area under the curve within the interval $[a,b]$ can be calculated by allowing $n$ to approach infinity in $\Sigma_{k = 1}^ n f(c_k) \Delta x$ , ... (1) where $c_k$ is the minimum of the function at the $k$ th interval, and $\Delta x$ is the length of each interval (so that $\Delta x = \frac{b – a}{n}$ ). He first defines (1) to be the ""Area under the graph"". Later in the text, he calls (1) specifically the ""lower bound"", because it approaches the actual graph from below. And so $\Sigma_{k = 1}^ n f(c_k) \Delta x$ , where $c_k$ is the maximum of the function at the $k$ th interval is called the ""upper bound"", because it approaches the actual graph from below. A question arises for me: although the upper bound and lower bound are equal as $n$ approaches infinity, is the notion of ""Area under the graph"" specifically defined as the lower bound? However, this question is only subsidiary to my next. Later, the author begins considering arbitrary intervals instead of fixed ones. He defines the ""norm"" to be the length of the greatest interval. He proves that $\Sigma_{k = 1}^ n f(c_k) \Delta x_k$ , (where $c_k$ is a randomly chosen point in the interval $\Delta x_k$ ) is equal to the upper and lower bound as the norm approaches $0$ . He calls this expression the Riemann Integral (named adter Georg Friedrich Bernhard Riemann), and denotes it by $\int_a^b f(x) dx$ . He then says that the area we defined earlier in (1) is the Riemann Integral over the interval $[a,b]$ . He then says that the Integral $\int_a^b f(x) dx$ is called the Definite Integral. I was completely thrown off. So are the ""area under the graph"", the Riemann Integral, the Definite Integral all identical concepts? Is there a difference between the terminologies? I seek thorough guidance. Thank you in advcalance.","['integration', 'definite-integrals', 'calculus', 'riemann-sum', 'algebra-precalculus']"
4729489,Evaluate: $\tan^2 \frac{\pi}{18}+\tan^2 \frac{5\pi}{18}+\tan^2 \frac{7\pi}{18}$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 11 months ago . Improve this question Evaluate: $$\tan^2 \frac{\pi}{18}+\tan^2 \frac{5\pi}{18}+\tan^2 \frac{7\pi}{18}$$ Tried in various ways but to no avail , $$\tan\left(\frac{\pi}{2}-\frac{2\pi}{18}\right) = \tan^2 \frac{7\pi}{18}$$ , $$\tan\left(\frac{\pi}{2}-\frac{4\pi}{18}\right) = \tan^2 \frac{5\pi}{18}$$ , $$\tan\left(\frac{\pi}{2}-\frac{8\pi}{18}\right) = \tan^2 \frac{\pi}{18}$$","['algebra-precalculus', 'trigonometry']"
4729639,Topological version of second isomorphism theorem,"Is there a version of the second isomorphism theorem for topological spaces? That is, given $X$ and $Y$ topological spaces, is there a condition such that $(X \cup Y)/Y$ is homeomorphic to $X/(X \cap Y)$ ? What about an isomorphism of homology/fundamental groups?","['general-topology', 'abstract-algebra', 'homotopy-theory', 'algebraic-topology']"
4729644,How to prove that $\sum_{k=1}^n k! \cdot \binom{n}{k}=\lfloor e \cdot n! - 1 \rfloor$,"The integer sequence A007526 is a sequence with many equivalent formulations. I have noticed the following expression, which seems to be correct. However, I am not sure how to prove it. Prove that $$\sum_{k=1}^n k! \cdot \binom{n}{k}=\lfloor e \cdot n! - 1 \rfloor$$ How is the constant $e$ derived? Another question: If I define $s = \lfloor e \cdot n! - 1 \rfloor$ , can I solve for $n = f(s)$ ? I try this:
We can let $e\cdot n！-1=s+\alpha $ where $0\le \alpha<1$ . Then $n!=\frac{s+\alpha +1}{e}$ . But I do not know if there is a symbolic representation for the inverse operation of factorial (n!).","['summation', 'binomial-coefficients', 'combinatorics', 'discrete-mathematics']"
4729660,Simplification of trig(arctrig()) expressions like $\sin(2\arcsin(x))$ or $\sec(\arctan(x))$?,"Doing integration practice, I've seen lots of cases where I end up with expressions like $\sin(2\arcsin(x))$ or $\sec(\arctan(x))$ . Checking integral-calculator.com, it turns out that these expressions simplify to: $\sin(2\arcsin(x)) = 2x \sqrt{1-x^2}$ $\sec(\arctan(x)) = \sqrt{x^2 + 1}$ There are many other examples such as $\sin(\sec^{-1}(x))$ , $\tan(\sec^{-1}(x))$ , etc. How are these kinds of expressions generally simplified? Is there a general method? Thanks!","['algebra-precalculus', 'trigonometry', 'trigonometric-integrals']"
4729747,"Derivative of integrals of $L^p([a,b])$ functions","Question: Let $f,g \in L^p([a,b]),1 < p < \infty$ . Show that the function $I \colon \mathbb{R} \to \mathbb{R}$ defined by $$I(t) := \int_a^b |f(x) + t g(x)|^p\, dx$$ is differentiable at $t = 0$ and compute its derivative. My attempt: Using Minkowski's inequality the function $I$ is defined for all $t \in \mathbb{R}$ and $I(0) =  \int_a^b |f(x)|^p \, dx$ . I have consider using Lebesgue-dominated convergence theorem by letting $\{t_n\}\subset \mathbb{R} - \{0\}$ so that $t_n \to 0$ and defining $$I_n(t) := \int_a^b \frac{|f(x) + t_n g(x)|^p - |f(x)|^p}{t_n}\, dx$$ however I am not too sure how to carry on from here. Alternatively, I suspect the derivative at $t = 0$ should be the following expression $$A := \int_a^b p  |f(x)|^{p-1} |g(x)| \, dx$$ so i tried to consider proving the following $$\lim_{t \to 0} \int_a^b \frac{|f(x) + t g(x)|^p - pt |f(x)|^{p-1}|g(x)| - |f(x)|}{t}\, dx = 0.$$ However, I am not sure of any inequalities to estimate the integrand. Any hints will be appreciated.","['integration', 'measure-theory', 'lebesgue-integral', 'real-analysis']"
4729755,"Topology, Normal Spaces - Proving a function is continuous knowing that any composition is continuous.","my question is regarding topology. Given two topological spaces $X, Y$ and we know that $Y$ is $T_4$ . Let $f:X \rightarrow Y$ be a function such that for any continuous function $\phi:Y \rightarrow \mathbb{R}$ , the composition $\phi \circ f$ is continuous . Show that $f$ it self is continuous. My attempt: we want to show that for any open set $V$ , $f^{-1}[V]$ is open. If we could find a function $g$ such that $g:Y \rightarrow \mathbb{R}$ would be zero in exactly $Y \setminus V$ and positive else where, then we could take the preimage of the open set $(0,\infty)$ which would be open in $X$ and be exactly $f^{-1}[V]$ . As I looked around, it seems we can build such a function when knowing that a set is $G_{\delta}$ , and the space is normal but here we don't know that the set is $G_{\delta}$ . In the exam from which I took the question, the first question was to quote Urysohn's lemma , so the solution probably uses the lemma. Any suggestions?",['general-topology']
4729832,$\lim_{x \to 0} (\cos x)^{\cot x}$,The following question is from cengage calculus . Illustration 2.95 but the explanation isn't clear to me $\lim_{x \to 0} (\cos x)^{\cot x}$ It is to be solved by using the identity : $\lim_{x \to 0} (1+x)^{\frac{1}{x}} = e$,"['limits', 'limits-without-lhopital']"
4729843,"How to describe all semigroups $(S, \, \cdot)$ based on a choice operation?","Let $S$ be a non-empty set. We say that a binary operation $f \, \colon S \times S \to S$ is a choice operation if it always returns one of its arguments. In other words, $\forall \, a \in S \, \colon f(a,a)=a$ and $\forall \, a, b \in S, \, a \ne b \, \colon f(a,b) \in \{ a, b \}$ . If $f$ is associative , it defines a semigroup on $S$ . We'll use more simple and common to semigroups notation $a \cdot b$ instead of $f(a,b)$ further. Several notable (and, in some sense, exhaustive) examples of such operations are $a \cdot b \equiv \max (a,b)$ , $a \cdot b \equiv \min (a,b)$ on some totally ordered set and $a \cdot b \equiv a$ ( ""leftArg"" ), $a \cdot b \equiv b$ ( ""rightArg"" ) on any set. The first two are commutative, while the last two are not. How to describe all semigroups $(S, \, \cdot)$ based on a choice operation? It is not hard to prove that such semigroup is commutative if and only if there is a total order on $S$ such that $a \cdot b \equiv \max (a,b)$ (or, alternatively, $a \cdot b \equiv \min (a,b)$ for the reverse order). That's why I've said that $\max (a,b)$ is exhaustive. What about non-commutative semigroups? My conjecture is that $(S, \, \cdot)$ is a semigroup with a choice operation $\cdot$ if and only if there exist a totally ordered set $(T, \, \leq)$ and two functions $r \, \colon S \to T$ (rank) and $s \, \colon T \to \{\mathcal L, \mathcal R\}$ (side) such that $\cdot$ is a combination of the above max, leftArg and rightArg functions in the following sense: $$ a \cdot b \equiv
\begin{cases}
a, & \text{if $r(b) < r(a)$} \\
b, & \text{if $r(a) < r(b)$} \\
a, & \text{if $r(a) = r(b)$ and $s(r(a)) = \mathcal L$} \\
b, & \text{if $r(a) = r(b)$ and $s(r(a)) = \mathcal R$}
\end{cases}$$ Is it true?","['order-theory', 'abstract-algebra', 'associativity', 'semigroups']"
4729861,What is the meaning of $z \to \infty $ when working with complex numbers?,"Let $z=x+iy$ .  If I am evaluating $f(z)$ as $z$ tends to infinity, I should assume this means $x$ tends to infinity and $y$ tends to infinity? I think this is the case but wanted to confirm.  I realized this when checking the solution to a problem asking for the limit of $f(z)=e^z$ as $z$ tends to infinity.  At first I simply thought infinity was the answer as I was ""thinking"" of $z$ as infinity $+ 0i$ , but I see that it's periodic when thinking of $x$ and $y$ both tending to infinity.  Obviously if it said the limit as $z$ tends to $5$ , the answer would be $e^5$ , but I suppose when thinking of infinity we consider both the real and imaginary parts to tend to infinity?","['complex-analysis', 'infinity', 'functions', 'complex-numbers']"
4729867,Area of loop in graph [duplicate],"This question already has answers here : Find the area of curve (3 answers) Closed 12 months ago . The graph of $$r = \frac{3\sec(θ)\tan(θ)}{1 + \tan^3(θ)}$$ is shown below. Find the area of the loop. I know that the equation for finding the area enclosed under a polar curve is A = (1/2) ∫[a, b] [r(θ)]^2 dθ where r(θ) is the polar equation of the curve and [a, b] represents the interval of θ values that correspond to the desired region, but I am unsure about how I can find the values of a and b in the case of this graph. I think the equation is kind of daunting. And I'm honestly not even sure if that integral can apply to a case like this. Could I have some help?","['calculus', 'polar-coordinates']"
4729907,"Evaluate the integral $\int_0^1 \frac{\ln(1 - x)}{x} \,dx$ by contour integration","$$\int\limits_{\mathrm{0}} ^{\mathrm{1}} \frac{\ln\left(\mathrm{1}−{x}\right)}{{x}}\:{\mathrm dx} \\ $$ I am trying to evaluate the above definite integral by contour integration. I have tried computing the above integral using a keyhole unit circle contour but the result is coming wrong . Please help in sorting my mistake .
The contour is attached below : My attempt to the integral is as follows : $${I}\:=\:\int_{\mathrm{0}} ^{\mathrm{1}} \frac{{ln}\left(\mathrm{1}−{x}\right)}{{x}}{dx} \\ $$ $${I}_{{C}} =\:\int_{{C}} \frac{{ln}^{\mathrm{2}} \left(\mathrm{1}−{z}\right)}{{z}}{dz} \\ $$ $$\bullet{Branch}\:{point}\Rightarrow{z}\:=\mathrm{1} \\ $$ $$\bullet{Pole}\Rightarrow{z}\:=\:\mathrm{0}\:\left({removable}\:{pole}\right) \\ $$ $${By}\:{Cauchy}'{s}\:{Theorem}\:, \\ $$ $$\int_{{C}} =\int_{\psi_{\mathrm{1}} } +\int_{\psi_{\mathrm{2}} } +\int_{\gamma} +\int_{\Gamma} =\:\mathrm{0}\:\rightarrow\left(\mathrm{1}\right) \\ $$ $$\bullet{Branch}\:{cut}\Rightarrow\left(−\infty,\mathrm{1}\right) \\ $$ $$\bullet{arg}\left(\mathrm{1}−{z}\right)\epsilon\:\left(\mathrm{0},\mathrm{2}\pi\right) \\ $$ $${Now}, \\ $$ $$\bullet\int_{\psi_{\mathrm{1}} } =\int_{\mathrm{0}} ^{\mathrm{1}} \frac{{ln}^{\mathrm{2}} \left(\mathrm{1}−{x}\right)}{{x}}{dx}={I}_{\mathrm{0}}  \\ $$ $$\bullet\int_{\psi_{\mathrm{2}} } =\int_{\mathrm{1}} ^{\mathrm{0}\left(\right.} \frac{\left.{ln}\left(\mathrm{1}−{x}\right)−\mathrm{2}\pi{i}\right)^{\mathrm{2}} }{{x}}{dx} \\ $$ $$\:=−{I}_{\mathrm{0}} +\int_{\mathrm{0}} ^{\mathrm{1}} \frac{\mathrm{4}\pi^{\mathrm{2}} }{{x}}{dx}+\mathrm{4}\pi{iI} \\ $$ $$\therefore\:{Im}\left(\int_{\psi_{\mathrm{1}} } +\int_{\psi_{\mathrm{2}} } \right)\:=\:\mathrm{4}\pi{I}\:\:\:\rightarrow\:\left(\mathrm{2}\right) \\ $$ $$\int_{\Gamma} \Rightarrow\:{z}\:=\:\mathrm{1}+{e}^{{i}\theta}  \\ $$ $$\Rightarrow\:{dz}\:=\:{ie}^{{i}\theta} \:{d}\theta\: \\ $$ $$\theta\:=\:\mathrm{0}\:{to}\:\mathrm{2}\pi \\ $$ $$\int_{\Gamma} =\int_{\mathrm{0}} ^{\mathrm{2}\pi} \frac{{ln}^{\mathrm{2}} \left(−{e}^{{i}\theta} \right)\centerdot{ie}^{{i}\theta} }{\mathrm{1}\:+\:{e}^{{i}\theta} }{d}\theta \\ $$ $$\int_{\mathrm{0}} ^{\mathrm{2}\pi} \frac{\left({i}\left(\pi\:+\:\theta\right)\right)^{\mathrm{2}} \centerdot{ie}^{\frac{{i}\theta}{\mathrm{2}}} }{\mathrm{2}\:{cos}\left(\frac{\theta}{\mathrm{2}}\right)}{d}\theta \\ $$ $$\left\{\mathrm{1}+{e}^{{i}\theta} \:=\:\mathrm{2}{cos}\left(\frac{\theta}{\mathrm{2}}\right){e}^{{i}\left(\frac{\theta}{\mathrm{2}}\right)} \right\} \\ $$ $$\therefore\:\:\:\mid{Im}\left(\:\int_{\Gamma} \right)\mid \\ $$ $$=\int_{\mathrm{0}} ^{\mathrm{2}\pi} \frac{\left(\pi\:+\:\theta\right)^{\mathrm{2}} }{\mathrm{2}}{d}\theta \\ $$ $$\:\:\:\:\:\:\:\:=\:\:\frac{\mathrm{13}\pi^{\mathrm{3}} }{\mathrm{3}}\:\rightarrow\:\left(\mathrm{3}\right) \\ $$ $${From}\:\left(\mathrm{1}\right),\left(\mathrm{2}\right)\:{and}\:\left(\mathrm{3}\right),\:{we}\:{get} \\ $$ $$\Rightarrow\mathrm{4}\pi{I}\:+\:\frac{\mathrm{13}\pi^{\mathrm{3}} }{\mathrm{3}}\:=\:\mathrm{0} \\ $$ $$\Rightarrow\:{I}\:=\:−\frac{\mathrm{13}\pi^{\mathrm{2}} }{\mathrm{12}}\: \\ $$ $$ \\ $$ $$ \\ $$ $$ \\ $$","['integration', 'definite-integrals', 'complex-analysis', 'calculus', 'complex-numbers']"
4729911,Computing an Integral Involving Rational and Bessel Functions,"I tried to compute the following integral by using Contour integration method. $$
    \int_0^{\infty}\frac{x^2}{x^4+1}J_0(ax) dx
$$ where $J_0$ is Bessel function of the first kind and $a$ is a positive real constant. Based on the table of integrals (p. 672), we expect to have $-\operatorname{kei}(a)$ as the answer, where $\operatorname{kei}$ is Kelvin function . I wanted to compute it myself but I ended up with a different answer. Here is what I did: The poles on upper half-plan are $\sqrt{i}$ and $i\sqrt{i}$ . Because $\frac{x^2}{x^4+1}J_0(ax)$ is an even function, by using residue theorem I simply expect that $$
    \int_0^{\infty}\frac{x^2}{x^4+1}J_0(ax) dx =\frac{2\pi i}{2}\Big(\frac{J_0(a\sqrt{i})}{4\sqrt{i}}+\frac{J_0(a  i \sqrt{i})}{4i \sqrt{i}}\Big) 
$$ Which is not definitely $\operatorname{kei}$ function. Also, we simply put $a=2$ (remember $a$ is a positive real number) and compute the integral by wolfram alpha, we get a different answer , which is neither matched with my computations nor with the table of integrals. I appreciate any help! Edit: As @Gonçalo points out in the comments, the integral on the curve $Re^{i \theta}$ does not vanish. So, my idea on using residue theorem was completely wrong! However, I am still seeking for an appropriate way to deal with this type of integrals and write it down as a sum of Bessel functions for instance.","['improper-integrals', 'definite-integrals', 'complex-analysis', 'residue-calculus', 'bessel-functions']"
4729926,Prove the (path-) connectedness of the graph of a compact- and convex-valued upper hemi-continuous correspondence.,"Let $\Gamma: [0, 1] \to \mathbb{R}$ be a compact- and convex-valued, upper hemi-continuous correspondence. Prove that the graph of $\Gamma$ is a connected set. Is it path-connected? This is what I have so far: Proof $\space$ We first show that $Gr(\Gamma)$ is connected. Assume to the contrary that $Gr(\Gamma)$ is not connected. Then $Gr(\Gamma) = A \cup B$ where $\overline{A} \cap B = \phi$ and $A \cap \overline{B} = \phi$ , $A \neq \phi$ , and $B \neq \phi$ . Since $\Gamma$ is compact- and convex-valued, $\Gamma(x)$ us a closed interval or a singleton for all $x \in [0, 1]$ . Since $\Gamma$ is upper hemi-continuous, $Gr(\Gamma)$ is closed. Let $(x, y)$ be a limit point of $Gr(\Gamma)$ . Without loss of generality, assume $(x, y) \in A$ , where $y \in \Gamma(x)$ . Then $(x, y) \notin \overline{B}$ . Take a sequence $\{x_n\}$ such that $x_n \to x$ . Then there exists a subsequence ${x_{n_k}}$ such that $(x_{n_k}, y_{n_k}) \in \overline{B}$ , $y_{n_k} \in \Gamma(x_{n_k})$ , and $x_{n_k} \to x$ . However, no subsequence of $\{y_{n_k}\}$ can converge to $y$ , for otherwise $(x, y)$ would be in $\overline{B}$ . This contradicts the sequential characterization of upper hemi-continuous correspondence. Therefore, $Gr(\Gamma)$ is connected. I would really appreciate it if someone could check if my proof of the connectedness is correct. In addition, I have difficulties to determine whether $Gr(\Gamma)$ is path-connected or not. I tried to use the $Gr(\Gamma)$ = Deleted Comb Space as a counterexample, but it turns out that $Gr(\Gamma)$ is not closed, because its closure is the Comb Space. By the closed-graph theorem, if $\Gamma$ is upper hemi-continuous, $Gr(\Gamma)$ would be closed. Hence $\Gamma$ is not upper hemi-continuous, a contradiction. This is a question about set-valued analysis and topological spaces . I would like to provide some related definitions and results first, because they are usually not covered in a standard math course in analysis or topology: Definition $\space$ Let $X$ and $Y$ be two sets. If with each element $x$ of $X$ we associate a subset $\Gamma(x)$ of $Y$ , we say that the correspondence $x \to \Gamma(x)$ is a mapping of $X$ into $Y$ ; the set $\Gamma(x)$ is called the image of $x$ under the mapping $\Gamma$ . Definition $\space$ Let $\Gamma$ be a mapping of a topological space $X$ into a topological space $Y$ , and let $x_0$ be any point of $X$ . We say that $\Gamma$ is lower hemi-continuous at $x_0$ if for each open set $G$ meeting $\Gamma(x_0)$ , there is a neighborhood $U(x_0)$ such that \begin{equation}
x \in U(x_0) \Longrightarrow \Gamma(x) \cap G \neq \phi.
\end{equation} We say that $\Gamma$ is upper hemi-continuous at $x_0$ if for each open set $G$ containing $\Gamma(x_0)$ there exists a neighborhood $U(x_0)$ such that \begin{equation}
x \in U(x_0) \Longrightarrow \Gamma(x) \subset G.
\end{equation} We say that the mapping $\Gamma$ is continuous at $x_0$ if it is both lower and upper hemi-continuous at $x_0$ . Definition $\space$ We say that $\Gamma$ is lower hemi-continuous in $X$ (l.h.c. in $X$ ) if it is lower hemi-continuous at each point of $X$ . We say that $\Gamma$ is upper hemi-continuous in $X$ (u.h.c. in $X$ ) if it is upper hemi-continuous at each point of $X$ and if, also, $\Gamma(x)$ is a compact set for each $x$ . If $\Gamma$ is both lower hemi-continuous in $X$ and upper hemi-continuous in $X$ , then it will be called continuous in $X$ . Definition $\space$ The correspondence $\Gamma$ is closed-valued if for each $x \in X$ , $\Gamma(x)$ is closed in $Y$ . Definition $\space$ The correspondence $\Gamma$ is compact-valued if for each $x \in X$ , $\Gamma(x)$ is compact in $Y$ . Definition $\space$ The correspondence $\Gamma$ is convex-valued if for each $x \in X$ , $\Gamma(x)$ is a convex set in $Y$ . Definition $\space$ The graph of the correspondence $\Gamma$ is the set $Gr(\Gamma) = \{(x, y) \in X \times Y | y \in \Gamma(x)\}$ . Lemma $\space$ [Sequential characterization of lower hemi-continuous] A correspondence $\Gamma: X \to Y$ is lower hemi-continuous at $x_0 \in X$ if and only if, for any sequence $\{x_m\}$ converges to $x_0$ and any $y \in \Gamma(x_0)$ , there exists a sequence $\{y_m\}$ converges to $y$ such that $y_m \in \Gamma(x_m)$ for all $x_m$ . Lemma $\space$ [Sequential characterization of upper hemi-continuous] Let $\Gamma: X \to Y$ be a correspondence. If, for every sequence $\{x_m\}$ in $X$ that converges to $x_0 \in X$ and for every sequence $\{y_m\}$ such that $y_m \in \Gamma(x_m)$ , there exists a subsequence of $\{y_m\}$ that converges to a point in $\Gamma(x_0)$ , then $\Gamma$ is upper hemi-continuous at $x_0$ . If $\Gamma$ is compact-valued, then the converse is true. Theorem $\space$ [Closed graph theorem] Let $\Gamma: X \to Y$ be a correspondence. If $\Gamma$ is closed-valued and upper hemi-continuous, then $Gr(\Gamma)$ is closed.","['solution-verification', 'set-valued-analysis', 'functional-analysis', 'multivalued-functions', 'general-topology']"
4729935,Does $|e^{f(z)}| = 1$ on parallel lines imply that $f$ is constant?,"Let $f : \mathbb C \to \mathbb C$ be a holomorphic function. Suppose that for some $a<b<c \in \mathbb R$ we have $$
|e^{f(z)}| = 1
$$ for all $z = x+iy, x \in \mathbb R, y \in \{ a,b,c \}$ . That is, $|e^f|$ is constant one for such $x$ and $y$ . Can we conclude that $e^f$ is globally constant, i.e. $f(z)=iu$ for some real $u$ ? I was thinking to apply a Phragmen Lindelöf argument, but failed.",['complex-analysis']
4729950,Determining whether $\sum_{i=1}^\infty \frac{t^{i}}{{i}!} (\sum_{j=0}^{i-1} k^j)$ converges,"Let's consider a map $L$ between complex matrices: $L(A) = kA + B$ for some real value $k$ and matrix $B$ . I am trying to calculate $e^{tL}(A)$ for some real value $t>0$ . Here is my attempt: Since $L^n(A) = k^nA + \big(\sum_{i=0}^{n-1}k^i\big)B$ , we have: $e^{tL}(A) = A + tL(A) + \frac{t^2}{2!}L^2(A) + \frac{t^3}{3!}L^3(A) + \dotsc $ $= A + t(kA + B) + \frac{t^2}{2!}\big(k^2A + (k+1)B\big) + \frac{t^3}{3!}\big(k^3A + (k^2+k+1)B\big) + \dotsc $ $= \big(1 + k + \frac{(kt)^2}{2!} + \dotsc\big)A + \big(t + \frac{t^2}{2!}(k+1) + \frac{t^3}{3!}(k^2+k+1) + \dotsc \big)B$ $= e^{kt}A + \big(t + \frac{t^2}{2!}(k+1) + \frac{t^3}{3!}(k^2+k+1) + \dotsc \big)B$ . I am not sure how to calculate the coefficient for the matrix $B$ , which is $\sum_{i=1}^\infty \frac{t^{i}}{{i}!} (\sum_{j=0}^{i-1} k^j)$ . Does it converge/diverge? If it diverges, is it possible to obtain a condition on $k$ such that it converges?","['convergence-divergence', 'sequences-and-series']"
4729967,"Basic closed subsets of Stone topological group presented as inverse limit, inner automorphism group of profinite group","Take a profinite group $G=\varprojlim G_\alpha$ . We know that the inner automorphism group $\text{Inn}(G)$ of $G$ is profinite since $\text{Inn}(G)=G/Z(G)$ , and the quotient of a profinite group by a closed normal subgroup (which $Z(G)$ must be as the intersection of centralizers) is profinite. I'm considering another object $\widehat{\text{Inn}}(G)=\varprojlim\text{Inn}(G_\alpha)$ . Now I would like to show that $\text{Inn}(G)=\widehat{\text{Inn}}(G)$ . A priori there may be things in $\widehat{\text{Inn}}(G)$ that are not in $\text{Inn}(G)$ , so I believe that showing this is a worthwhile goal. There is an embedding $\text{Inn}(G)\hookrightarrow\widehat{\text{Inn}}(G)$ and I've managed to show that $\text{Inn}(G)$ is dense in $\widehat{\text{Inn}}(G)$ . It remains to show that $\text{Inn}(G)$ is closed in $\widehat{\text{Inn}}(G)$ . I believe that one way to go about this is to take a limit point $\varphi$ of $\text{Inn}(G)$ and show that any open subset of $\widehat{\text{Inn}}(G)$ that contains $\varphi$ must intersect $\text{Inn}(G)$ . Can someone help me with this argument? Another possible way involves considering what the basic closed sets look like in a Stone topological space presented as an inverse limit. I know what the basic open sets look like, but what do basic closed sets look like? Edit: I want to assume all the maps $f:G_\alpha\to G_\beta$ are surjective.","['profinite-groups', 'general-topology', 'infinite-groups', 'group-theory']"
4729986,Subsets of $\mathbb{Z}$ satisfying the factorial property,"Consider the subset $S \subseteq \mathbb{Z}$ given by: $$S = \{ 2^i 3^j : i,j \ge 0 \}$$ Define the sequence $(a_k)$ to be the elements of $S$ in increasing order (with the standard order on $\mathbb{Z}$ ), and further define: $$ [a_n]! = \prod_{k = 1}^n a_k$$ I am interested to know if this generalisation of the factorial satisfies the analogous fundamental property: $$\frac{[a_{m+n}]!}{[a_m]![a_n]!} \in S$$ and more generally, if this relation would hold given $S$ generated as the multiplicative span of any finite set of primes. This seems very plausible, but I haven't found it to be immediate. It is obvious if $S$ is generated by only one prime. It may not hold if $S$ is generated by infinitely many primes, as the case $S = \{1,3,5,7,9,...\}$ demonstrates. I am aware of the Bhargava factorial corresponding to sets $S \subseteq \mathbb{Z}$ , an important recently-discovered generalisation of the factorial which satisfies the property above. Unfortunately, the Bhargava factorial of $S$ is not equal to the function I have given above. But it's possible that this factorial could be realised as a Bhargava factorial of some other subset of $T \subseteq \mathbb{Z}$ . For reference, the first terms of the sequence $(a_k)$ for $S = \{2^i 3^j\}$ are $1,2,3,4,6,8,9,12,16,18,24,...$ . This gives: $$[a_0]! = 1$$ $$[a_1]! = 1$$ $$[a_2]! = 2$$ $$[a_3]! = 6$$ $$[a_4]! = 24$$ $$[a_5]! = 144$$","['number-theory', 'combinatorics', 'prime-numbers', 'factorial']"
4729994,Find ratio formed by the perpendicular bisector of an angle bisector,"Question: As shown in the figure, $AD$ is the angle bisector of $\triangle ABC$ , the perpendicular bisector of $AD$ intersects the extension of side $BC$ at point $F$ , and $AB, AC$ at points $E$ and $G$ respectively. If $\frac{AC}{AB}=\frac{3}{4}$ , compute $\frac{CF}{BF}$ . Here's what I've done so far: Without loss of generality, assume that $AC=3$ and $AB=4$ . Using the angle bisector theorem, we can let $DC=3x$ and $DB=4x$ . This means that to find the ratio $\frac{CF}{BF}$ , we need to find $CF$ in terms of $x$ . If we let $\overline{EF} \cap \overline{AD}=H$ , then $\triangle AHF \cong \triangle DHF$ (by SAS) After doing some angle bashing, $\triangle EBF \sim \triangle GAF$ and $\triangle EAF \sim \triangle GCF$ . I think what I'm supposed to do here is consider the ratios $\frac{BF}{AF}$ and $\frac{AF}{CF}$ , since multiplying them  will yield $\frac{BF}{CF}$ , but I'm not sure how to find the value of either of these ratios.","['contest-math', 'geometry']"
4730020,"$x_1=1$, $x_2=2$, $x_{n+2}=\frac{x_{n+1}+x_n}{x_{n+1}-x_n}$ is divergent?","In my analysis class they let us find out if the following sequence converges or not: $$x_1=1,\quad x_2=2,\quad x_{n+2}=\frac{x_{n+1}+x_n}{x_{n+1}-x_n}$$ This sequence appears in the Bartle but the exercise does not ask to prove convergence. The whole class has had problems with proving that the sequence diverges (because according to Wolfram the sequence is divergent). We have tried to do operations of the type $x_{n+1}-x_{n+2}$ and try to prove that it converges to two different limits and thus conclude the divergence, but it doesn't seem to work $$x_{n+1}=\frac{x_n+x_{n-1}}{x_n-x_{n-1}},\quad x_{n+2}=\frac{x_{n-1}+x_n(1+x_n-x_{n-1})}{x_{n+1}-x_n(-1+x_n-x_{n-2})}$$ where $x_{n+2}\to x$ and $\frac{x_{n-1}+x_n(1+x_n-x_{n-1})}{x_{n+1}-x_n(-1+x_n-x_{n-2})}\to \frac{2x}{2x}=1$ But it seems that attacking the problem from this point is not convincing, does anyone have any suggestions? EDIT (Progress according to a user's proposal, I think you have good ideas)--------------- The idea is to first assume that $x_n=0$ for some $n\in\mathbb{N}$ , and let $N=\min\{ n\in\mathbb{N}\;:\; x_n=0\}$ , then $\displaystyle x_{N+1}=\frac{x_N+x_{N-1}}{x_N-x_{N-1}}=\frac{x_{N-1}}{-x_{N-1}}=-1$ and $\displaystyle  x_{N+2}=\frac{x_{N+1}+x_N}{x_{N+1}-x_N}=\frac{-1}{-1}=1 $ in this way we have the sequence behaves $0, -1, 1, 0, -1, 1,...$ and diverges. Now assuming that $x_n\neq 0$ for all $n\in\mathbb{N}$ and suppose that $(x_n)$ converges to $L$ , then $\displaystyle x_{n+2}(x_{n+1}-x_n)=x_{n+1}+x_n \Rightarrow $ $L(L-L)=L+L$ and $L=0$ . Now, let $a_n=\frac{x_{n+1}}{x_n}$ notice that $\displaystyle x_{n+2}=\frac{x_{n+1}+x_n}{x_{n+1}-x_n}=1+\frac{2x_n}{x_{n+1}-x_n}=1+\frac{2}{a_n-1}$ therefore $ \lim_{n\to\infty} \frac{2}{a_n-1}=-1$ and $\lim a_n=-1$ . This is what I have, any idea to end the demonstration or some other way?",['sequences-and-series']
4730041,Is $\Bbb Z^3$ a one-relator group?,"I understand that: $\Bbb Z^0 = \langle a  \mid a \rangle$ $\Bbb Z^1 = \langle a, b \mid b \rangle$ $\Bbb Z^2 = \langle a, b  \mid aba^{-1}b^{-1} \rangle$ but is it possible for $\Bbb Z^3$ to be represented with just one relator? I can’t think of a way in which it would be possible but I am also unsure how to prove it would be impossible.","['group-presentation', 'combinatorial-group-theory', 'abstract-algebra', 'free-groups', 'group-theory']"
4730052,Is the following piecewise-defined function smooth $\in C^\infty(\mathbb{R})$?,"Is the function $$q(x)=\begin{cases} 1& x=0\\ 0& |x|\geq 1\\ \dfrac{1}{1+\exp\left(\dfrac{1-2|x|}{x^2-|x|}\right)}&\text{otherwise}\end{cases}$$ a smooth function class $C^\infty(\mathbb{R})$ ? I found this function as is shown in Wiki by choosing $f(x)=\exp\left(-\frac{1}{x}\right)\theta(x)$ with $\theta(x)$ the Heaviside step function , then making $g(x)=\frac{f(x)}{f(x)+f(1-x)}$ and then lets $q(x)=g\left(\frac{x-a}{b-a}\right)\cdot g\left(\frac{d-x}{d-c}\right)$ with $a=-1,\ b=c=0,\ d=1$ , as could be seen in Desmos . But later in this question I found in the answers and comments it have some issues at specific points were is undefined, so now I tried to fix them by defining the function piecewise. A simular construction is used in this answer in MSE to built an example of a smooth transition function so in the case of this $q(x)$ , in principle, the points were could be some issues should be only $x\in\{-1, 0,1\}$ , but since its defined piecewise now I am not sure if indeed this solve the situation making $q(x)\in C^\infty(\mathbb{R})$ a smooth function, or it still have issues that make it being non infinitely differentiable. I was able to check it possitively until the 7th derivative in Wolfram-Alpha . Motivation I am trying to understand the properties in order to figure out this other question .","['smooth-functions', 'real-analysis', 'continuity', 'limits', 'schwartz-space']"
4730099,Calculus Functional Equation,"Given a function $F: \mathbb{R} \rightarrow \mathbb{R}$ defined by: $$F(x-y) = F(x)F(y), \forall x,y \in \mathbb{R}.$$ There are several questions: Prove: If there is $a \in \mathbb{R}$ s.t. $F(a) \not = 0$ , then $F(0) = 1$ . My attempts: Choose $x = 0 =y$ , we obtain $F(0) = [F(0)]^2$ . So the value of $F(0) = 1$ . If $F(0) = 0$ , the function $F(x)$ will be identically zero. Determine F(3). My attempts: Choose $x = 3 = y$ . Then $$F(3-3) = F(3)F(3) \Longrightarrow F(0) = [F(3)]^2 \Longrightarrow F(3) = 1 \vee F(3) = -1.$$ Is it correct? Thanks in advance.","['calculus', 'functions']"
4730104,Is Schwartz space canonical in any sense?,"Schwartz space $\mathcal S(\mathbb R^n, \mathbb C)$ is a dense subspace of $L^2(\mathbb R^n,\mathbb C)$ that appears frequently in physics, because the position operators $X_i$ and momentum operators $P_i$ , defined by $(X_if)(x) = x_if(x)$ and $P_if=-i (\partial/\partial x_i)f$ are defined and well behaved on it. However, there are many other spaces of test functions that could be used (and sometimes are used) in this context, and elsewhere in functional analysis. Still the Schwartz functions seem to appear the most often, so I wonder if they are ""natural"" or ""canonical"" in any sense. Are there any natural characterizations of $\pmb{\mathcal S(\mathbb R^n, \mathbb C)}$ ? For example, something like ""Schwartz space is the unique subspace of $L^2$ such that ..."" or ""Schwarz space is the largest subspace of $L^2$ such that ..."". I think I once saw a such a characterization in terms of the position and momentum operators above, along the lines of ""the largest subspace such that $X_i$ and $P_i$ are simultaneously defined,"" or something like that. But I cannot find it now, and I may be misremembering completely.","['definition', 'schwartz-space', 'functional-analysis', 'distribution-theory']"
4730131,"More methods to evalute this integral ; $I=\int_0^2 x(8-x^3)^{\frac{1}{3}} \, dx$","$$I=\int_0^2 x(8-x^3)^{\frac{1}{3}} \, dx$$ Being inept with integrals, this is my try, $$\int x(8-x^3)^{\frac{1}{3}} \, dx = (-1)\int x(x^3-8)^{\frac{1}{3}} \, dx $$ Using the substitution method (Chebyshev); for $\int x^m(a+bx^n)^p \,dx$ , where $m, n, p ∈ Q$ , If $\frac{m+1}{n}+p ∈ Z$ , then substitute $(ax^{-n}+b)=t^K$ , where $K$ is the denominator of $p$ ; $$\frac{(x^3-8)^{\frac{1}{3}}}{x}=t ,\, dx=\frac{dt}{\left(\frac{x}{(x^3-8)^{\frac{2}{3}}}-\frac{(x^3-8)^{\frac{1}{3}}}{x^2}\right)}$$ $$\int \frac{8t^3}{(t-1)^2(t^2+t+1)^2} \, dt=8 \left( \frac{1}{3} \int \frac{1}{t^3-1}\,dt-\frac{1}{3}  \int \frac{t}{t^3-1} \, dt\right)$$ Continuing the partial fractions, the final result is; $$\frac{8}{3} \left( \frac{\ln(t-1)}{3}-\frac{t}{t^3-1}-\frac{\arctan \left(\frac{2t+1}{\sqrt3}\right)}{\sqrt3} \right)-\frac{4}{9} \ln(t^2+t+1) $$ On putting the bounds, $I=\frac{16\pi}{(\sqrt3)^5}$ However, this is a very tedious method. In search for alternate method, i tried for beta functions, it resembled the format (almost). To bring it into the beta function form; $B(p,q) = \int_0^1 x^{p-1}(1-x)^{q-1} \, dx$ ; I tried with $x^3=8y$ , which gave $I=\frac{8}{3} B\left(\frac{2}{3},\frac{4}{3}\right)$ and using up almost all the properties; $B(p,q)=\frac{\Gamma_p \Gamma_q}{\Gamma_{p+q}}$ $\Gamma_{p+1}=p(\Gamma_p)$ $\Gamma_p \Gamma_{1-p}=\frac{\pi}{sinp\pi}$ , you get $I=\frac{16\pi}{(\sqrt3)^5}$ . I am interested in learning more ways to evaluate this integral.","['integration', 'calculus']"
4730174,Construction of parallel lengths in given ratio,"An angle is given and two points $M$ and $N$ inside it. Through these two points, draw  parallel lines $m$ and $n$ so that lengths formed by their intersections with the angle arms are in  ratio 1:3. I tried solving it with homothety but it didn't quite work out. I would appreciate any hints. One solution :","['euclidean-geometry', 'angle', 'geometry']"
4730251,Calculate the expectation of a random variable,"Let $A$ be a $d\times d$ non-negative definite matrix. Let $\{{\bf{u}}_i\}_{i=1}^{n+1}$ be i.i.d. random vectors sampled uniformly at random from the unit sphere $\mathbb S^{d-1}\subset \mathbb R^d$ . Define $Q_n=\frac{1}{\sqrt n} \sum_{i=1}^n \textbf{u}_i^T A \textbf{u}_{i+1}.$ I want to calculate the expectations $\mathbb E[Q_n]$ and $\mathbb E[(Q_n)^2]$ in terms of $n,d, (a_{i,j})_{1\leq i,j \leq d}$ . But I don't know how to proceed. My attempt: The pdf of every $\textbf{u}_i$ shall be $f_{\textbf{u}_i}(x_1,x_2,...,x_d)= \frac{1}{B}$ for all $(x_1,...,x_d) \in \mathbb S^{d-1}$ and equals zero otherwise, where $B$ is the area of $\mathbb S^{d-1}$ . We may write $\textbf{u}_i$ as $(u_{i,1}, u_{i,2}, ..., u_{i,d})$ and $Q_n$ shall be a sum of terms of the form $u_{i,j} a_{j,k} u_{i+1,k}$ over $i,j,k$ , where $1\leq i,j,k \leq d$ . We know $\{\textbf{u}_i\}_{i=1}^{n+1}$ is i.i.d. Can we conclude that $u_{i,j}$ and $u_{i+1,k}$ are independent, for all $i,j,k$ with $1\leq i,j,k \leq d$ ? (If it is true, can you give me a reference on this inference?) If they are really independent, then $\mathbb E[Q_n]=0$ . In the expression of $(Q_n)^2$ , there will be terms like $u_{i,j} a_{j,k} u_{i+1,k} u_{i',j'} a_{j',k'} u_{i'+1,k'}$ for $1\leq i, i', j, j', k, k' \leq d$ . I don't know how to calculate $\mathbb E[u_{i,j} u_{i+1,k} u_{i',j'}u_{i'+1,k'}]$ . Thanks for help.","['expected-value', 'independence', 'probability-theory', 'random-variables']"
4730268,"What's $\kappa=\min\{\nu : [0, \omega_1)\text{ embedds into }\mathbb{R}^\nu\}$?","What's the least $\kappa$ such that $[0, \omega_1)$ embedds into $\mathbb{R}^\kappa$ ? We have $\aleph_1\leq \kappa\leq \mathfrak{c}$ since $\mathbb{R}^{\aleph_0}$ is metrizable and $|C([0, \omega_1))| = \mathfrak{c}$ . Thanks.","['general-topology', 'set-theory']"
4730326,"Greatest Integer, Least Integer functions","Two positive real numbers, $a$ and $b$ , are expressed as the sum of $m$ positive real numbers and $n$ positive real numbers respectively as follows: $a = s_1 + s_2 + s_3 + s_4 + \cdots + s_m$ $b = t_1 + t_2 + t_3 + t_4 + \cdots + t_n$ For any real number $x$ , $\lfloor x\rfloor$ denotes the greatest integer less than or equal to $x$ and $\lceil x\rceil$ denotes the least integer greater than or equal to $x$ . If $\lfloor a\rfloor = \lceil s_1\rceil + \lceil s_2\rceil + \lceil s_3\rceil + \cdots + \lceil s_m\rceil - 11$ and $\lceil b\rceil = \lfloor t_1\rfloor + \lfloor t_2\rfloor + \lfloor t_3\rfloor + \cdots + \lfloor t_n\rfloor +7$ What is the minimum possible value of $m+n$ ? My attempt so far :- I have no clue on what properties to apply , so I started by checking with some values Since $a>0$ and also $\lfloor a\rfloor \ge 0$ I took a case for minimum value of when $\lfloor a\rfloor = 0$ which would imply $0\le a<1$ ,  now intuitively by taking $s_1, s_2, s_3,\ldots,s_{12}$ to be very small values between $0$ and $1$ , I should be able to create the sum to lie between $0$ and $1$ , and also each of $12 \lceil s\rceil$ would give value as $1$ which would satisfy our condition, so $m=12$ would be the smallest value I have no clue on how to fit values for the second set of equations involving $b$ and $t$ . All I can say is $\lceil b\rceil \ge 7$ Also, I am extremely skeptical about my initial approach for finding $\min(m)$ as well. Please guide me on how to solve this.","['algebra-precalculus', 'ceiling-and-floor-functions']"
4730365,Find the missing angle formed by the intersection of a diagonal and an equal line inside the square.,"(Apologies for the vague title, I could not figure out how to describe it adequately within the word limit) I came across this problem in an online forum a few months ago, with no answers under it. The diagram looks like this: We have a square and another right angle at its lower left vertex, and the question is to find the missing angle formed by the intersection of the diagonal of this square, and another line segment of equivalent length. This geometry problem is quite interesting and unique compared to what I’ve posted before, there is not a lot of information given. I will post my solution below as an answer. Please let me know if my answer is correct, and if there are any problems in my approach. Also, please post your own answers as well, preferably using different methods in order to ensure my answer is accurate.","['contest-math', 'euclidean-geometry', 'geometry', 'solution-verification', 'trigonometry']"
4730385,Uniform convergence in distribution implies convergence of moments,"I am reading a paper in which the author wants to prove the convergence of the moments. He transforms the object of interest $\varepsilon^{-1} (\vartheta_\varepsilon^*-\vartheta_0)$ into \begin{align*}
\varepsilon^{-1} (\vartheta_\varepsilon^*-\vartheta_0)=\Delta_\varepsilon +   \frac{\bar{\vartheta}_{\tau_\varepsilon}-\vartheta_0}{\varepsilon} R_\varepsilon^* 
\end{align*} Now, he shows that $\Delta_\varepsilon$ converges uniformly to the normal distribution $N(0,I(\vartheta_0)^{-1})\overset{d}{=}:\zeta$ (using the uniform CLT) and that \begin{align*}
\sup_{\vartheta_0 \in \mathbb{K}}\mathbb{E}_{\vartheta_0} \left| \frac{\bar{\vartheta}_{\tau_\varepsilon}-\vartheta_0}{\varepsilon} R_\varepsilon^* \right|^p \underset{\varepsilon \rightarrow 0}{\longrightarrow} 0
\end{align*} Hence, $\varepsilon^{-1} (\vartheta_\varepsilon^*-\vartheta_0) \rightarrow N(0,I(\vartheta_0)^{-1})$ as $\varepsilon \rightarrow 0$ . But he also claims that the moments converge (as a consequence of this proof!), i.e. that for any $p>0$ : $\mathbb{E}_{\vartheta_0} |\varepsilon^{-1} (\vartheta_\varepsilon^*-\vartheta_0)|^p \rightarrow \mathbb{E}_{\vartheta_0}|\zeta|^p $ . My question: Why does this hold? I am of course familiar with the well-known counterexamples of ""convergence in distribution implies convergence of moments"" and the ability to use uniform integrability. But here the author seems (as he claims so) to have shown the moment convergence already; but I am unable to see this connection.","['statistics', 'central-limit-theorem', 'stochastic-analysis', 'uniform-integrability', 'probability']"
4730389,Does this property of a function $f : 2^A \rightarrow A$ have a name?,"I've got this property: For all $S\in 2^A$ and all partitions $P$ of $S$ , $f(S) = f\left(\left\{f(M)\mid M \in P\right\}\right)$ , i.e. $f$ maps a set of values to a single value and gives the same result whether we map the set ""at once"" or whether we map subsets first and then map the set of the results. Examples for $f$ would be sums of finite sets of numbers, or the least upper bound of subsets of a complete lattice. So, if $A = \mathbb{N}\cup\left\{\infty\right\}$ and $f(S) = \sum_{s \in S} s$ , the property is fulfilled, as, for example $f(\{1,2,3\}) = f(\{f(\{1,2\}),f(\{3\})\}) = 3+3 = 6$ Is that a well-known property? If so, what's it called so I can read up more on it?","['elementary-set-theory', 'functions']"
4730397,Find estimates for the solution $x=x_n$ of the equation $(1+x)^n =1+(n+1)x$,"It is clear that for every $n> 0$ , and $\delta> 0$ ,  we have $(1+x)^n < 1+ (n+\delta) x$ , for $x>0$ small. In particular we have $(1+x)^n< 1 + (n+1) x$ for $x> 0$ small enough.  How small is just the question.  I have approximated the logs on both sides with their quadratic terms and got $x_n\simeq \frac{2}{n^2+n+1}$ for the solution of the equation $$(1+x_n)^n = 1+(n+1) x_n$$ Now it turns out that we  have $$x_n > \frac{2}{n^2}$$ equivalently $$\left(1+\frac{2}{n^2}\right)^n < 1+\frac{2(n+1)}{n^2}$$ This is not hard to show with some calculus ( take the second derivative). I am   interested in better estimates for $x_n$ defined above. I am thinking about large $n$ 's Any feedback would be appreciated! Got some great feedback. @Somos solution finds it as a power series in $\frac{1}{n}$ .  It seems that all of the coefficients are positive.  I wonder how one could prove that.
Also @Somos expressed the problem as an inversion of a function. In @Claude Leibovici solution it seems we find the Pade approximants.  They themselves have expansion in $\frac{1}{n}$ with positive coefficients it seems.  Intriguing. This is still evolving.  The way I see it now: Let $t = \frac{1}{n}$ , $t$ small.  Also take $k$ a parameter ( assumed positive). The equation in $x$ with parameters $k$ and $t$ $$(1+ x)^{\frac{1}{t}} - (1 + (\frac{1}{t} + k) x)$$ has a solution for $x$ that is approximately $2 k t^2$ .  So write $x = 2 k t^2 \cdot y$ and consider the equation in $y$ $$\frac{ \log( 1+ 2 k t^2 y) - t \log (1 + (\frac{1}{t} + k) 2 k t^2 y)}{t^3} =0$$ or $f(k,t,y) = 0$ .  In this setup $f$ is analytic (in $k$ , $t$ , $y$ ), $f(k, 0, 1) = 0$ , and moreover, $\frac{\partial f(k,0,1)}{\partial y} = 2 k^2 \ne 0$ (if $k\ne 0$ ).  So now we  get a solution $$x= x(t) = 2 k t^2[ 1+ \frac{1}{3}( 3 - 2k)t + \frac{1}{9} ( 9+ 5 k^2) + \frac{1}{135}( 135 + 90 k - 30 k^2 - 68 k^3)t^3 + \cdots ]$$ This series is convergent for small $t$ . For $k=1$ ( the original problem) it seems the coefficients are positive, still not elucidated. With calculus ( using the known series expansion of @Somos) we could show that $$x(t) = x(1,t) > 2 t^2( 1 + 1/3 t + 14
/9 t^2)$$ by showing that $$f(t) \colon = t \log ( 1+ (\frac{1}{t} + 1)2 t^2( 1 + 1/3 t + 14/9 t^2)) - \log ( 1 + 2 t^2( 1 + 1/3 t + 14/9 t^2))>0$$ For this, check that $f(0) = f'(0) = 0$ , and $f''(t) > 0$ for $t> 0$ ( see this WA link )","['calculus', 'asymptotics', 'inequality']"
4730443,"Solutions behavior of i.v.p $y'(x)+e^x(2+\sin x)y(x)=\frac{1}{x^2+1},\ y(0)=a,\ x\geq 0 $","Consider the initial value problem (i.v.p) $$y'(x)+e^x(2+\sin x)y(x)=\frac{1}{x^2+1},\ y(0)=a,\ x\geq 0\ \ (a\in\mathbb{R}). $$ The unique solution of this problem has the form $$y(x)=e^{-\int_0^x e^s(2+\sin s)ds}\left(a+\int_0^x \frac {1}{1+t^2}e^{\int_0^t e^s(2+\sin s)ds}dt\right),\ x\geq 0 $$ Firstly, I proved that for every $a\in\mathbb{R}$ it holds that $\displaystyle\lim_{x\to+\infty} y(x)=0$ (note that $e^x(2+\sin x)\geq 1,\forall\ x\geq 0$ and $\frac{1}{x^2+1}\to 0$ , as $x\to +\infty$ ). However, I have a problem with those two questions: $\bullet$ Can I tell with certainty, that there exist a solution $y$ of the i.v.p such that $y(x)<0,\forall x\geq x_0$ (for some $x_0\geq 0$ )? $\bullet$ Can I tell with certainty, that there exist an oscillative solution $y$ (i.e solution that has infinite countable roots) of the i.v.p? Is it possible the answer lies hidden at the asymptotic behavior of those integrals? Hint: Calculation of integrals isn't necessary. Thanks for your help!! I appreciate it","['calculus', 'ordinary-differential-equations']"
4730446,Existence of paths obeying partial ordering,"Consider a partially ordering on $\mathbb{R}^n$ that forms a lattice, with meet and join continuous w.r.t. the standard topology (i.e. a topological lattice ). Can we choose a path $\gamma(t)$ with arbitrary distinct endpoints also of our choosing $\gamma(0) \leq \gamma(1)$ which preserves order; i.e., $t \leq t' \implies \gamma(t) \leq \gamma(t')$ for any $t$ and $t'$ ? I'm pretty sure this is true for Riesz spaces and am wondering if it holds more generally. If the above holds with additional conditions (complete, complemented, distributive, etc) that would also be interesting. It would also be o.k. if $t \leq t' \implies \gamma(t) \leq \gamma(t')$ held in a dense subset of $[0, 1]$ . It feels like this should have been studied somewhere -- I'd appreciate any results or references! Update: As an attempt, I've been trying to start with an arbitrary path $\gamma(t)$ and using completeness to construct another path $ \bar \gamma(t) = sup_{t' \leq t} \gamma(t') $ . But there's no reason to believe that the new $\bar \gamma$ is a continuous path in $\mathbb{R}^n$ . It also seems that a necessary condition to this problem is that we can find an uncountably infinite chain in $\mathbb{R}^n$ . This is true for certain partial orders; say, for example, the Riesz space where meet is componentwise min and join is componentwise max. But is it always true for any continuous meet and join operations? Update 2: I've dug up the following result from ""A Compendium of Continuous Lattices"" (Gierz et. al.) which is close: Unfortunately the results in this text rely heavily on compactness. But the result is also stronger than required: it shows that there's an arc-chain (the correct terminology for the above) between any $x < y$ , while I'm only looking for the existence of any arc-chain. Perhaps we can find a compact sublattice of $\mathbb{R}^n$ .","['general-topology', 'path-connected', 'lattice-orders']"
4730468,Is state space representation useful for nonlinear control systems?,"I understand that the state space representation is mathematically equivalent to the transfer function representation for linear systems, and that it allows us to solve the corresponding DE by finding the eigenvalues of a matrix. However, for nonlinear systems, the transfer function can only represent a linear approximation, while the state space form can represent the full system. But what's the advantage of using state space form for nonlinear systems, if we can't generally solve them by matrix methods? How does state space representation help us analyze or design nonlinear control systems any better than we could by sticking with the original DE representation? Some background: My impression was that the state space form of linear systems is essentially just syntactic sugar for the final result of transforming a nth order DE into a system of n first order DE's, and writing that system as a single matrix equation. It ""hides"" the derivatives under the extra parameterization variables. But for nonlinear systems, we can't just get a system of linear equations and write it as a single matrix equation that doesn't explicitly involve derivatives. So I don't see how the state space form simplifies anything for nonlinear systems.","['ordinary-differential-equations', 'applications', 'control-theory', 'nonlinear-system', 'dynamical-systems']"
4730506,Is the difference between additive groups and multiplicative groups just a matter of notation?,"Groups are sometimes written additively, like $(G,+)$ , and sometimes they are written multiplicatively, like $(G,\times)$ . But is there really a difference? Is it all just a matter of notation, nothing more? If it is something beyond notation, what is the difference, exactly?","['notation', 'group-theory', 'abstract-algebra']"
4730537,Does weak convergence to a continuous distribution imply strong convergence of measures?,"Let $X_n$ be a sequence of real-valued random variables with cumulative distribution function $F_n$ . Let $X$ be another real-valued random variable with CDF $F$ . Here it is stated that $F_n$ converges strongly to $F$ if $F_n(x) \to F(x)$ for every $x\in\mathbb{R}$ . The book cites this paper as a reference. However, the paper defines strong convergence of measures (say $\mu_n$ converges strongly to $\mu$ ) if $\mu_n(A) \to \mu(A)$ for every set $A$ in the sigma field on which measures are defined. Are these two equivalent i.e. strong convergence of cdfs implies strong convergence of measures defined by the cdfs ?","['measure-theory', 'weak-convergence', 'probability-distributions', 'probability-theory', 'probability']"
4730540,Solving a Lamé differential equation with a parameter out of boundaries.,"I am trying to get rid of the following homogeneous ode. \begin{equation}
	\begin{split}
u''(z)+\frac{1}{2} \left(\frac{1}{z}+\frac{1}{z-1}+\frac{1}{z-1}\right) u'(z)+\frac{2\left(A+B\right)- \left(2 B\right)z }{4 z
   (z-1) (z-1)}u(z)=0
	\end{split}
\end{equation} The form of this equation is very particular and reminds me the algebraic representation of the Lamé equation, as shown in http://dlmf.nist.gov/29.2.E2 , which is : \begin{equation}
	\begin{split}
		\frac{{\mathrm{d}}^{2}w}{{\mathrm{d}\xi}^{2}}+\frac{1}{2}\left(\frac{1}{\xi}+%
\frac{1}{\xi-1}+\frac{1}{\xi-k^{-2}}\right)\frac{\mathrm{d}w}{\mathrm{d}\xi}+%
\frac{hk^{-2}-\nu_1(\nu_1+1)\xi}{4\xi(\xi-1)(\xi-k^{-2})}w=0
	\end{split}
\end{equation} For $0<k<1$ and $\xi={\operatorname{sn}}^{2}\left(z,k\right)$ then the Lamé equation can be expressed in this form: \begin{equation}
	\begin{split}
{
\frac{{\mathrm{d}}^{2}w}{{\mathrm{d}\zeta}^{2}}+\left[h-\nu(\nu+1) k^2 sn^{2}(\zeta,k)\right] w(\zeta)=0
}
	\end{split}
\end{equation} Thus,when comparing my equation and Lame's equation the only way to match is the case in which $k=1$ , but that leaves $k$ out of the parameters. If I ignored the fact that $k$ can't have such value, I could proceed and see how it looks like: \begin{equation}
	\begin{split}
{
\frac{{\mathrm{d}}^{2}u}{{\mathrm{d}z}^{2}}+\left[2(A+B)-2B   sn^{2}(z,1)\right] u(z)=0
}
	\end{split}
\end{equation} Actually, the Jacobi elliptic function $sn(z,1)$ is reducible into $tanh(z)$ And finally I end up with this equation. \begin{equation}
	\begin{split}
{
\frac{{\mathrm{d}}^{2}u}{{\mathrm{d}z}^{2}}+\left[2(A+B)-2B   \tanh^{2}(z)\right] u(z)=0
}
	\end{split}
\end{equation} This equation seems to be solvable in terms of Associated Legendre Polynomials. And some questions have arisen to me: My original equation can be considered a legitimate Lamé differential equation? If so, it will be correct to suggest that Lamé differential equation for the parameter value k=1 degenerates into some form of Associated Legendre differential equation? I would really appreaciate any comments on this, and I apologize for any mistake I could have done while typing this thread.","['complex-analysis', 'homogeneous-equation', 'ordinary-differential-equations', 'hypergeometric-function']"
4730576,Is $X$ adapted not sufficient for measurability of $X_T$ in $\mathcal F_T$ (the σ-algebra of a stopping time)?,"Let $X$ be a stochastic process in continuous time and $T$ be a stopping time. All the standard texts I've looked at so far, as well as this excellent blog , introduce the assumption that $X$ is progressively measurable to prove that $X_T$ is $\mathcal F_T$ -measurable. Why is merely adapted $X$ not enough? Indeed the proof in the linked article seems to use ""progressive"" solely to claim ""adapted"" for the conclusion.","['stochastic-processes', 'measure-theory', 'stopping-times']"
4730606,Determining the distance to the origin from three points on a circle,"Scenario: A measuring tool consists of a round measuring bung (red circle) equipped with three probes (labeled $a$ , $b$ , and $c$ ). These probes are evenly spaced at angles of $120^\circ$ from each other ( $\theta = 120^\circ$ ). The master part (blue circle), in which the measuring bung is seated, has a known inner bore diameter of $D$ . Refer to the diagram below (figure 1): However, it is important to note that the provided diagram does NOT accurately represent the real-world situation . In practical terms, it is nearly impossible to mount each probe perfectly tangential to the measuring bung. Consequently, each probe is mounted at a unique and unknown starting distance from the center of the measuring bung. These starting distances are denoted as $r_a$ , $r_b$ , and $r_c$ for probes $a$ , $b$ , and $c$ , respectively. For a clearer representation, please refer to the revised diagram below (figure 2): The following variables are known : $a$ , $b$ , $c$ - the distances measured by the 3 probes $\theta$ - the probe mounting angle around the center of the    measuring bung $D$ - the bore diameter of the master part The following variables are unknown : $r_a$ , $r_b$ , $r_c$ - the probe mounting distances from the center ofthe measuring bung Objective: The objective is to find $r_a$ , $r_b$ , and $r_c$ . These distances will be determined during the calibration process (using the master part). Once these distances are determined, the master part can be replaced with a new part having an unknown diameter, such that the diameter of any part can be accurately measured. Measurement Process: The calibration process involves taking any number of measurements $(K)$ with the tooling seated in various configurations. Each time, the tooling was seated in a new position. Using CAD software, I can determine the probe measurements ( $a_k, b_k, c_k$ ) for each measurement $(K)$ . $D = 25.0;$ $R = 12.5;$ $θ = 120^\circ;$ $K=01 :\qquad   a_{01} = 5.2491254 ;\quad  b_{01} = 3.6421203 ;\quad  c_{01} = 5.5479800 ;$ $K=02 :\qquad   a_{02} = 3.5261623 ;\quad  b_{02} = 4.0069957 ;\quad  c_{02} = 6.9097135 ;$ $K=03 :\qquad   a_{03} = 5.2425714 ;\quad  b_{03} = 2.2366199 ;\quad  c_{03} = 6.9231890 ;$ $K=04 :\qquad   a_{04} = 5.6053228 ;\quad  b_{04} = 4.0762025 ;\quad  c_{04} = 4.5958384 ;$ $K=05 :\qquad   a_{05} = 2.6211544 ;\quad  b_{05} = 4.3330685 ;\quad  c_{05} = 7.3330685 ;$ $K=06 :\qquad   a_{06} = 5.6305936 ;\quad  b_{06} = 1.7881182 ;\quad  c_{06} = 6.8966301 ;$ $K=07 :\qquad   a_{07} = 3.1218340 ;\quad  b_{07} = 5.3989306 ;\quad  c_{07} = 5.7511286 ;$ $K=08 :\qquad   a_{08} = 2.8136425 ;\quad  b_{08} = 3.7336225 ;\quad  c_{08} = 7.7652347 ;$ $K=09 :\qquad   a_{09} = 6.7312779 ;\quad  b_{09} = 1.6301678 ;\quad  c_{09} = 5.7985227 ;$ $K=10 :\qquad  a_{10} = 3.7744161 ;\quad  b_{10} = 5.6724511 ;\quad  c_{10} = 4.7375570 ;$ Find $r_a; r_b;r_c;$ (Prove that $r_a = 8;r_b = 9;r_c = 6;$ for this example) Solution based on answers from Fedja This is my understanding on the solution provided by Fedja... from the perspective of a learner. If you are reading this and you are not Fedja, I recommend you read his answer instead of this. We will deploy the least-squares method to find numerical approximations of the $r_a$ , $r_b$ , and $r_c$ . Let's establish a few important functions: A mathematical expression for the radius, in terms of the variables a,b,c,ra,rb, and rc. I recommend referring to the answer provided by Joshua Wang for a detailed understanding. $R=\sqrt{\frac{\left(x^2+y^2+xy\right)\left(x^2+z^2+xz\right)\left(y^2+z^2+yz\right)}{3x^2y^2+3x^2z^2+3y^2z^2+6x^2yz+6{xy}^2z+6xyz^2}}$ The mean quadratic error G. This function returns the square root of the sum of the squared errors for each measurement point $(k)$ . It provides a measure of how well the given guess ( $r_a$ , $r_b$ , $r_c$ ) fits the measurements. This is the function that is being minimized in order to find the values of $r_a, r_b$ and $r_c$ that provides the best fit to the given measurements. $G\ =\ \sqrt{\frac{1}{K}\ \sum_{k=0}^{K}\left(Error_k\right)^2}$ Where, $K$ is the number of measurements. $Error_k=\sqrt{\frac{\left({x_k}^2+{y_k}^2+x_k\ast y_k\right)\left({x_k}^2+{z_k}^2+x_kz_k\right)\left({y_k}^2+{z_k}^2+y_kz_k\right)}{3{x_k}^2{y_k}^2+3{x_k}^2{z_k}^2+3{y_k}^2{z_k}^2+6{x_k}^2y_kz_k+6x_k{y_k}^2z+6\ast x_ky_k{z_k}^2}}-R$ Where $x_k=a_k+r_a $ $y_k=b_k+r_b $ $z_k=c_k+r_c $ $F(x,y,z)$ – This function is used as a constraint in the algorithm to ensure that the calculated values of $r_a$ , $r_b$ , and $r_c$ satisfy the given measurements. $F\left(x,y,z\right)=3x^2y^2+3x^2z^2+3y^2z^2+6x^2yz+6{xy}^2z+6xyz^2-\frac{\left(x^2+y^2+xy\right)\left(x^2+z^2+xz\right)\left(y^2+z^2+yz\right)}{R^2}$ The partial derivatives of $F\left(x,y,z\right)\ - F_x\left(x,y,z\right)$ , $F_y\left(x,y,z\right)$ , $F_z\left(x,y,z\right)$ with respect to $x,\ y,\ z$ .
The partial derivatives are an essential to optimization process as they directly affect magnitude and direction that $r_a$ , $r_b$ , and $r_c$ is incremented with each iteration ${(dr}_a{,dr}_b{,dr}_c)$ . $F_x(x,y,z) = 6xy^2+ 6xz^2+ 12xyz + 6y^2 z + 6yz^2 - \frac{(2x + y)(x^2+ z^2+ xz)(y^2+ z^2+ yz)}{R^2} -\frac{(x^2+ y^2+ xy)(2x + z)(y^2+ z^2+ yz)}{R^2} $ $F_y\left(x,y,z\right)=F_x\left(x,y,z\right) $ $F_z\left(x,y,z\right)=F_x\left(x,y,z\right) $ (Due to the symmetry of $F(x,y,z)$ ) An system of linear equations for the increments ${(dr}_a{,dr}_b{,dr}_c)$ .
These equations represent the conditions that need to be satisfied for the increments ${(dr}_a{,dr}_b{,dr}_c)$ to ensure that the changes in the function $F\left(x,y,z\right)$ with respect to each variable are zero at the specific point ${(x}_k{,y}_k{,z}_k)$ . $F_x\left(x_k,y_k,z_k\right)dr_a{+F}_y\left(x_k,y_k,z_k\right)dr_b+F_z\left(x_k,y_k,z_k\right)dr_c=-F(x_k,y_k,z_k)$ The left-hand side of this equation represents change in $F$ at the point ${(x}_K{,y}_K{,z}_K)$ caused by the increments ${(dr}_a{,dr}_b{,dr}_c)$ , while the right-hand side is the negative value of $F$ at that point. By setting them equal to each other, you are essentially trying to make the net change in $F$ equal to zero. This equation ensures that the increments ${(dr}_a{,dr}_b{,dr}_c)$ are chosen in a way that counteracts the change in $F$ caused by variations in $(x, y, z)$ and moves the solution towards the desired minimum or maximum of $F$ with each iteration. $\frac{f(x_K,y_K,z_K)}{R}\ dr_a=0 $ $\frac{f(x_K,y_K,z_K)}{R}\ dr_b=0 $ $\frac{f(x_K,y_K,z_K)}{R}\ dr_c=0 $ These equations represent additional constraints on the increments ${(dr}_a{,dr}_b{,dr}_c)$ .
These equations ensure that the increments ${dr}_a{,dr}_b$ and ${dr}_c$ are constrained in a way that aligns with the behaviour of the function $F(x_K,y_K,z_K)$ and its relationship with $R$ . By satisfying these conditions, the algorithm can find solutions that minimize the error and converge towards the desired result. With the required functions established lets define the steps of the iterative method. The following steps will be executed: Initialize the values of $r_a$ , $r_b$ , and $r_c$ to some initial guess, such as $R/2$ . Construct the matrices $U$ and $V$ by evaluating the partial derivatives and function values for each measurement point $k$ . Solve the system of linear equations to obtain the increments $V$ $V\ =\left(U^TU\right)^{-1}U^TV$ Update $r_a$ , $r_b$ , and $r_c$ by adding the corresponding elements of $V$ to the previous values. Check if any of the updated values of $r_a$ , $r_b$ , or $r_c$ are negative. If so, revert them back to their previous values. Calculate the mean quadratic error $G$ based on the updated values of $r_a$ , $r_b$ , and $r_c$ Compare the new mean quadratic error $G$ with the previous minimum error $e$ If it is lower, update $e$ and store the values of $r_a$ , $r_b$ and $r_c$ as the new suspected minimizer. Repeat steps 2-7 until the termination condition is met (e.g., when $e$ is within the desired tolerance or after a maximum number of iterations). We can now implement these steps. (Implemented in Asymptote): //Measurements from CAD
real[] a = {5.2491254, 3.5261623, 5.2425714, 5.6053228, 2.6211544, 5.6305936, 3.1218340, 2.8136425, 6.7312779, 3.7744161};
real[] b = {3.6421203, 4.0069957, 2.2366199, 4.0762025, 4.3330685, 1.7881182, 5.3989306, 3.7336225, 1.6301678, 5.6724511};
real[] c = {5.5479800, 6.9097135, 6.9231890, 4.5958384, 7.3330685, 6.8966301, 5.7511286, 7.7652347, 5.7985227, 4.7375570};

//Masterpart radius
real R = 12.5;

//Number of measurements
int K = a.length;

//Define the mean quadratic error G
real G(real ra, real rb, real rc, real[] a, real[] b, real[] c)
{
  real s = 0;
  for (int k = 0; k < K; ++k)
  {
    real x = ra + a[k];
    real y = rb + b[k];
    real z = rc + c[k];
    real error = sqrt(((x^2 + y^2 + x * y) * (x^2 + z^2 + x * z) * (y^2 + z^2 + y * z)) / (3 * x^2 * y^2 + 3 * x^2 * z^2 + 3 * y^2 * z^2 + 6 * x^2 * y * z + 6 * x * y^2 * z + 6 * x * y * z^2)) - R;
    s += abs(error)^2;
  }
  return sqrt(s / K);
}

//Define the main function F
real f(real x, real y, real z)
{
  return 3 * x^2 * y^2 + 3 * x^2 * z^2 + 3 * y^2 * z^2 + 6 * x^2 * y * z + 6 * x * y^2 * z + 6 * x * y * z^2 - (x^2 + y^2 + x * y) * (x^2 + z^2 + x * z) * (y^2 + z^2 + y * z) / R^2;
}

//Define the partial derivatives
real Fx(real x, real y, real z)
{
  return 6 * x * y^2 + 6 * x * z^2 + 12 * x * y * z + 6 * y^2 * z + 6 * y * z^2 - (2 * x + y) * (x^2 + z^2 + x * z) * (y^2 + z^2 + y * z) / R^2 - (x^2 + y^2 + x * y) * (2 * x + z) * (y^2 + z^2 + y * z) / R^2;
}

real Fy(real x, real y, real z)
{
  return Fx(y, x, z);
}

real Fz(real x, real y, real z)
{
  return Fx(z, x, y);
}

// Initialize variables
real e = G(R / 2, R / 2, R / 2, a, b, c);
real ra = R / 2;
real rb = R / 2;
real rc = R / 2;

int maxIterations = 50;
real tolerance = 0.0001; // Tolerance for termination condition
int iteration = 0;

// Perform the iterations
while (iteration < maxIterations && e > tolerance)
{
  iteration = iteration + 1;

  // Construct the matrices U and V
  real[][] U, V;
  for (int k = 0; k < K; ++k)
  {
    real x = ra + a[k];
    real y = rb + b[k];
    real z = rc + c[k];

    U[k] = new real[] {Fx(x, y, z), Fy(x, y, z), Fz(x, y, z)};
    V[k] = new real[] {-f(x, y, z)};
    U[k + K] = new real[] {abs(f(x, y, z)) / R, 0, 0};
    V[k + K] = new real[] {0};
    U[k + 2 * K] = new real[] {0, abs(f(x, y, z)) / R, 0};
    V[k + 2 * K] = new real[] {0};
    U[k + 3 * K] = new real[] {0, 0, abs(f(x, y, z)) / R};
    V[k + 3 * K] = new real[] {0};
  }

  // Calculate the least squares solution
  V = inverse(transpose(U) * U) * transpose(U) * V;

  real rra = ra;
  real rrb = rb;
  real rrc = rc;

  // Update ra, rb, rc
  ra += V[0][0];
  rb += V[1][0];
  rc += V[2][0];

  // Check for negative values and update if necessary
  if (ra < 0) ra = rra;
  if (rb < 0) rb = rrb;
  if (rc < 0) rc = rrc;

  // Update the minimum value if the new error is lower
  if (G(ra, rb, rc, a, b, c) < e && ra > 0 && rb > 0 && rc > 0)
  {
    e = G(ra, rb, rc, a, b, c);
  }
}

  write(""iterations completed= "",iteration);
  write(""ra = "", ra);
  write(""rb = "", rb);
  write(""rc = "", rc);
  write(""error = "", e); Results: After executing the script we get the following results: We found the solution in only 6 iterations and the mean error of the calculated radius is $1.647\times10^{-8}$ mm.","['euclidean-geometry', 'circles', 'geometry', 'least-squares', 'trigonometry']"
4730612,Differentiation on an arbitrary set,"The subject of this question is differentiability of a function on an arbitrary set. It doesn't matter if we exclude isolated points from the domain of a function. So, let $X$ be a subset of $\mathbb{R}$ whose every point is an accumulation  point of $X$ . For a function $f:X\rightarrow\mathbb{R}$ and a point $p\in X$ , we can define $f’(p)=\lim\limits_{x\to p}\frac{f(x)-f(p)}{x-p}$ . If this limit exists at all points $p\in X$ , $f$ is differentiable . Two other definitions are known for the differentiability of a function on an arbitrary subset. The question is that if $f$ is differentiable in the above sense, is it differentiable in each of the following senses? (1) $f$ is differentiable if there is an open set $U$ including $X$ and a differentiable function $\tilde{f}:U\rightarrow \mathbb{R}$ such that $f=\tilde{f}|X$ . (2) $f$ is differentiable is for all $x \in X$ , there is an open set $U$ containing $x$ and a differentiable function $\tilde{f}:U\rightarrow \mathbb{R}$ such that $f|U\cap X=\tilde{f}|U\cap X$ . This is my original problem, not a homework.","['calculus', 'derivatives', 'real-analysis']"
4730633,Stone-Weierstrass theorem for non-polynomials,"Let $(X,\rho)$ be a compact metric space and let $C(X)$ be the set of the continuous real-valued functions on $X$ equiped with the maximum norm (which makes $C(X)$ a Banach Space). If a closed set subset $A\subset C(X)$ is such that: $f,g\in A\Rightarrow f+g\in A$ , $f\cdot g\in A$ ; $f\in A$ , $c\in\mathbb R\Rightarrow cf\in A$ ; There is some non-zero constant function in $A$ ; If $x,y\in X$ , $x\ne y$ , then there's some function $f\in A$ whith $f(x)\ne f(y)$ . Then we can conclude that $A=C(X)$ . This is how the Stone-Weierstrass theorem is stated on the book Foundations of Modern Analysis by Avner Friedman (Theorem 3.7.1). If we remove the closed hypothesis from $A$ , then the closure of $A$ is $C(X)$ , that is, every continuous function defined on $X$ can be approximated (uniformly) by functions from $A$ . The main application of this theorem is that every function defined on a compact subset $X\subset\mathbb R^n$ can be approximated by polynomials. My question is: Can we find a set $A$ of continuous functions defined on a compact subset $X\subset\mathbb R^n$ that has no polynomial functions, with the exception maybe of the constant ones, but still every continuous function defined on $X$ can be uniformly approximated by functions from $A$ .","['continuity', 'analysis', 'compactness']"
4730638,Is there a finite abelian group which is not isomorphic to either the additive or multiplicative group of a field?,"Does there exist a finite abelian group $G$ , such that for no field $F$ is it the case that $G$ is isomorphic to either the additive group of $F$ nor the multiplicative group of non-zero elements of $F$ ? I would prefer a counterexample of minimum cardinality.","['field-theory', 'abelian-groups', 'group-theory', 'finite-groups']"
4730659,Question of Venn Diagrams and Subsets on a Book,"In this book , Section 1.2.1 on page 7 introduce Sets and a diagram on page 8 Here, I am confused about the hightlighted area. Is there an error here?
Should A ∩ Bc = {s : s ∈ A and s ∈ B} be A ∩ Bc = {s : s ∈ A and s ∈ Bc} Isn't A ∩ Bc the waning cresent(moon phase term) of A ?",['elementary-set-theory']
4730695,A closed form for a triple integral involving Heron's formula,"Let $$S(x,y,z)=\frac14\sqrt{(x+y+z) (-x+y+z) (x-y+z) (x+y-z) }\tag1$$ (note that it's Heron's formula for the area of a triangle with sides of lengths $x,y,z$ ). I'm trying to evaluate the following integral in a closed form: $$\mathcal U=\int_0^1\int_0^x\int_{x - y}^{x + y}\sqrt{S(x,y,z) } dz dy dx.\tag2$$ I wasn't able to evaluate it symbolically (either manually or using Mathematica ), but using numerical integration and heuristic methods, I found a plausible form: $$\mathcal U\stackrel{\color{gray}?}=\frac\pi{12\sqrt{2 }}+\frac{\Gamma\left(\frac14\right)^4\sqrt{10 }}{1440 \pi^2}\left(\left(\small\frac12-\frac1{\sqrt{5 \phi }}\right)\textbf K(\alpha)^2-\textbf K(\alpha) \textbf E(\alpha)\right)\\ \quad\quad\;\approx0.117599420842157114228246644831065494814051852697…,\tag3$$ where $\textbf K(\cdot),\textbf E(\cdot)$ are the complete elliptic integrals of the first and the second kind, and $\alpha=\frac1{\phi \sqrt2}+\frac1{\sqrt{2 \phi}}$ . Can we prove that $(3)$ is indeed the true value of the integral? Update: You might ask, what heuristic methods could possibly give us the conjectured value? Here is a brief explanation. First, calculate a sequence of values of $\int\!\int\!\int S^{2n}dz dy dx,n\in\mathbb N;$ their evaluation is straightforward and they are all rational numbers. Then, use FindSequenceFunction to find a possible recurrence relation for that sequence. We need at least $30$ elements of the sequence to get a result: $$\small{(5+2 n)^2 (7+4 n) (9+4 n) (11+4 n) (27+20 n)\cdot a(n+2)=\\4 (2+n) (7+4 n) \left(6750+16947 n+15764 n^2+6416 n^3+960
   n^4\right)\cdot a(n+1)+\\128 (1+n)^2 (2+n) (3+2 n) (3+4 n) (47+20 n)\cdot a(n)}\tag4$$ Of course, there is no guarantee that this relation holds for all larger $n$ , but we cross our fingers 🤞 and conjecture that it does. Next, use FunctionExpand to find a possible explicit expression for the general term determined by this recurrence relation; it's complicated and involves generalized hypergeometric functions; we also need to manually adjust a periodic factor to ensure the result remains real even for non-integer $n$ . After some simplifications we arrive at this: $$\small\tfrac{4^n (20 n+27) \Gamma (2 n+2)\sqrt\pi}{(n+1) (16 n+12) \Gamma \left(2 n+\frac{7}{2}\right)}\,{_5F_4}\left(1,n+\tfrac{3}{2},n+\tfrac{3}{2},n+\tfrac{3}{2},n+\tfrac{47}{20};n+\tfrac{27}{20},n+\tfrac{7}{4},n+2,n+\tfrac{9}{4};-4\right)\tag5$$ Then we cross our fingers again 🤞 and conjecture that the same formula holds for all non-integer $n$ as well. Next, substitute $n=1/4$ and use FunctionExpand again to expand the generalized hypergeometric functions in terms of elliptic integrals. The result is quite large and unwieldy ( $19$ terms), but it appears to match the integral numerically, which is a sign we might be on the right track. We cross our fingers one more time 🤞 and conjecture that only a few terms of the expression are actually linearly independent over $\mathbb Z,$ and the result can be significantly simplified. Using FindIntegerNullVector and high enough numeric precision, we find a possible basis, and a linear combination of its elements that numerically matches the integral. After some radical denestings (using ResourceFunction[""RadicalDenest""] ) and other simplifications, this gives us the conjectured value $(3).$","['integration', 'definite-integrals', 'calculus', 'closed-form', 'elliptic-integrals']"
4730737,Expanding a Probability Tree,"Consider the following situation: Take some integer $N$ Each day there is a: $p_1$ probability that $N$ will increase by $n_1$ % of its current value $p_2$ probability that $N$ will decrease by $n_2$ % of its current value $p_3 = 1- p_1+ p_2$ probability that $N$ keep its current value My Question: On any given day, I am trying to find out what possible values $N$ can assume - and what are the probabilities of assuming these values. To solve this question, I first wrote the possible values that $N$ can take on the first day: $$N_{1} = N \times [ p_3 + p_1(1+n_1) + p_2(1-n_2)]^{1}$$ On the second day, we can write: $$N_{2} = N \times [ p_3 + p_1(1+n_1) + p_2(1-n_2)]^{2}$$ $$  N_{2} = N \times [p_3^2 + 2p_3p_1(1+n_1) + 2p_3p_2(1-n_2) + p_1^2(1+n_1)^2 + 2p_1(1+n_1)p_2(1-n_2) + p_2^2(1-n_2)^2]$$ By analyzing the above expression, I can indirectly see that on the second day, $N$ can have 9 possible values: $N$ can happen one possible way with probability $p_3^2$ $N*(1+n_1)$ can happen two possible ways with a total probability $2*p_3p_1$ $N*(1-n_2)$ can happen two possible ways with a total probability $2*p_3p_2$ $N*(1+n_1)^2$ can happen one possible way with probability $p_1^2$ $N*(1+n_1)(1-n_2)$ can happen two possible ways with a total probability $2*p_1p_2$ $N*(1-n_2)^2$ can happen one possible way with probability $p_2^2$ Based on this, I can observe that : $$(p_1 + p_2 + p_3)^2 = p_3^2 + 2p_3p_1 + 2p_3p_2 + p_1^2 + 2p_1p_2 + p_2^2 = 1$$ Thus, it would appear that I could find out all possible values that $N$ can assume on any given day along with the corresponding probabilities using the relationship: $$N_{k} = N \times [ p_3 + p_1(1+n_1) + p_2(1-n_2)]^{k}$$ Is my understanding correct? Can this above logic be used to expand any Probability Function corresponding to any Discrete Random Variable - and thus find out the possible values and corresponding probabilities that a Discrete Random Variable can take in the future? And is there a more compact way (in mathematical notation) to represent this expansion in the general case? Thanks! Note: The following relationship (Multinomial Theorem : https://en.wikipedia.org/wiki/Multinomial_theorem ) can be useful here: $$(x_1 + x_2 + ... + x_m)^n = \sum_{k_1=0}^n \sum_{k_2=0}^n ... \sum_{k_m=0}^n \binom{n}{k_1, k_2, ..., k_m} x_1^{k_1} x_2^{k_2} ... x_m^{k_m} = \sum_{k_1=0}^n \sum_{k_2=0}^n ... \sum_{k_m=0}^n \frac{n!}{k_1! k_2! ... k_m!} x_1^{k_1} x_2^{k_2} ... x_m^{k_m}
$$",['probability']
4730803,Double limit and dominated convergence theorem,"I am interested in the double limit of a particular function $f_m(x,\varepsilon)$ of the form $$f_m(x,\varepsilon) = \frac{1}{m}\sum_{n=0}^{m-1}g(x,\varepsilon).$$ I want to compute $$\lim_{\varepsilon \to 0}\lim_{m\to \infty}\frac{1}{m}\sum_{n=0}^{m-1}g_n(x,\varepsilon).$$ Without the $1/m$ normalisation the limit reduces to $$\lim_{\varepsilon \to 0}\sum_{n=0}^{\infty}g_n(x,\varepsilon)$$ where $|g_n(x,\varepsilon)|\leq h_n(x)$ where $\sum_{n=0}^\infty h_n(x)$ does not necessarily converge for all choices of $x$ . My question is the following. Knowing that $\lim_{m\to \infty} \frac{1}{m}\sum_{n=0}^{m-1}h_n(x)$ converges for almost every $x$ , are we allowed to say that $$\lim_{\varepsilon \to 0}\lim_{m\to \infty}\frac{1}{m}\sum_{n=0}^{m-1}g_n(x,\varepsilon)=\lim_{\varepsilon \to 0}\lim_{m\to \infty}\sum_{n=0}^{\infty}\frac{1}{m}g_n(x,\varepsilon)$$ and apply dominated convergence here by arguing that $|\frac{1}{m}g_n(x,\varepsilon)|\leq \frac{1}{m}h_n(x)$ where now in the dominated convergence theorem we are interested in whether $\lim_{m\to \infty} \frac{1}{m}\sum_{n=0}^{\infty}h_n(x)$ converges which we know it does by assumption? Is my idea nonsense?","['analysis', 'real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
4730835,Is it possible to solve this first order differential equation analytically?,"I've been trying to solve the equations for the path of a projectile moving under constant gravity and affected by air resistance and I come up against this nasty FODE. Is there a means by which it could be solved analytically for $y(x)$ or $x(y)$ ? $$\frac{\mathrm dy}{\mathrm dx}\sqrt{1+\left(\frac{\mathrm dy}{\mathrm dx}\right)^2} + \operatorname{arsinh}\left(\frac{\mathrm dy}{\mathrm dx}\right)=K-\frac{a}{x^{2}},$$ where $K$ and $a$ are real, positive constants.","['calculus', 'ordinary-differential-equations']"
4730912,System of Pell's equations,"Let $a$ , $b$ and $k$ be positive integers.
I want to prove that the only solution of the system $$\left\{\begin{array}{rclr}
2k^2+1&=&a^2&(1)\\
6k^2+1&=&b^2&(2)
\end{array}
\right.$$ is $(k,a,b)=(2,3,5)$ . I checked by computer that there was no solution with $3\leq k\leq10^8$ . I know that the values of $k$ which satisfy $(1)$ are the terms of the sequence $(k_n)$ defined by $$\left\{\begin{array}{l}
k_0=2\\
k_1=12\\
k_{n+2}=6k_{n+1}-k_n
\end{array}
\right.$$ and that the values of $k$ which satisfy (2) are those of the sequence $(k'_n)$ defined by $$\left\{\begin{array}{l}
k'_0=2\\
k'_1=20\\
k'_{n+2}=10k'_{n+1}-k'_n
\end{array}
\right.$$ but I cannot prove that if $m\geq1$ and $n\geq1$ , then $k_n\neq k'_m$ .","['number-theory', 'pell-type-equations', 'elementary-number-theory', 'diophantine-equations']"
4730931,"Geometrical interpretation of $\operatorname{Spec}(\mathbb{R}[x,y]/(x^2+y^2))$","$\def\Spec{\operatorname{Spec}}$ If am not mistaken, the prime spectrum of $\Spec(\mathbb{R}[x,y]/(x^2+y^2))$ consists of the points $(\overline{x},\overline{y})$ , $(\overline{x}-a)$ , $(\overline{y}-b)$ , where $a,b\in\mathbb{R}\setminus\{0\}$ (I think one can prove this by using the description of $\Spec k[x,y]$ explained here ). However, I wonder: why is this spectrum? Does it bear some geometric interpretation? I heard that its points are to be interpreted as “two lines that intersect at the origin.” I guess reflecting somehow the fact that over the complex numbers we can factor $x^2+y^2=(x+iy)(x-iy)$ , and that $\Spec(\mathbb{C}[x,y]/(x^2+y^2))$ is effectively the classical complex affine variety equal to $V(x+iy)\cup V(x-iy)\subset\mathbb{A}_\mathbb{C}^2\ (=\mathbb{C}^2)$ . My questions are: Is this interpretation of $\Spec(\mathbb{R}[x,y]/(x^2+y^2))$ correct? Is any of this pointing out to any general phenomenon/a going on behind the scenes? If I'm not being precise is because I don't know what I'm looking for. I guess I would be happy with (a) knowing about some relation between a $k$ -variety (in the scheme-theoretic sense) and its base change to $\overline{k}$ (whenever the later is still a variety), (b) some explanation or indications of literature about how schemes extend geometry to polynomials that don't have solutions or don't have that many solutions over non-algebraically closed fields (I am not talking about the usage of nilpotents in modern algebraic geometry, everything I would like to find out more about is in the reduced case). (The most general definition of “scheme-theoretic $k$ -(pre)variety” I know is that of a reduced (separated) $k$ -scheme of finite type.)","['affine-schemes', 'algebraic-geometry', 'affine-varieties']"
4730998,Problem with First-Order nonlinear Differential Equation,"I am trying to derive an analytical expression for a solution to the following differential equation: $$\frac{\mathrm dy}{\mathrm dx} - Be^{-y} = \cos(2x) + D,$$ where $B$ and $D$ are constants. Can someone point me in the right direction as to how this can be done? If it cannot be solved analytically, is there an approximation that can be made (e.g. power series expansion) to allow an approx. analytical expression to be derived? I am not interested in solving this problem numerically. Thank You","['integration', 'derivatives', 'ordinary-differential-equations']"
4731017,Can infinite union of finite sets be finite? [duplicate],"This question already has an answer here : Infinite countable union of finite sets (1 answer) Closed 12 months ago . Intuitively, I can say no! Because just look at following example: singleton sets $\{ n\}$ where $n \in \mathbb N$ and their infinite union $\cup_{n\in \mathbb N} n$ is infinite. My question is can I find finite sets whose infinite union is finite?",['elementary-set-theory']
4731067,How to prove there is not a polynomial $P(x)$ that is a good approximation to $\cos(x)$ for all $x$?,"The problem: ""True or false: there exists a polynomial $P$ such that $|P(x) - \cos(x)| \leq 10^{-6}$ for all real $x$ ."" I know the answer (it is false) but I do not understand the proof given in the book. The proof: Suppose $P(x) = a_Nx^N + \sum_{n=0}^{N-1}{a_nx^n}$ where $N\geq 1$ and $a_N > 0$ .
It is enough to show that $P(x) > 2$ for some value of $x$ . (I do not understand the following steps in the proof, could someone
please explain them.) A number $x$ can be found so large that $a_Nx^N > N|a_n|x^n + 2$ for each integer $n$ where $0 \leq n \leq N-1$ . The smallest possible value of $P(x)$ for any given positive $x$ would be achieved if all the coefficients in the sum were negative. Thus $$ P(x) \geq a_Nx^N - \sum_{n=0}^{N-1}{|a_n| x^n} > a_Nx^N + (2 - a_Nx^N) = 2 $$ Link to the book (the problem in question is problem 10): https://books.openbookpublishers.com/10.11647/obp.0181.pdf This is the proof given","['approximation', 'proof-explanation', 'functions', 'taylor-expansion', 'trigonometry']"
4731073,"Sending a message in bit form, calculate the chance that the message is in its original form after transfer","A message consists of 100 bits (either 0 or 1), of which every bit can change (from 0 to 1
or the other way around) during the data transfer with probability p = 0,001 (independently
of other bits). What is the probability that a message is in its original form after 10 data
transfers? Is my reasoning correct here (my answer):
In order to calculate the probability that a message is in its original form after 10 data transfers we first have to calculate the probability of one bit being transferred correctly and then do that for the next 100 bits to lastly calculate the probability of doing this 9 more times (10 total). so: P(X=100) = (1-p)^100 = (0.999)^100 and for 10 total transfers we get P(X=10) = ((1-p)^100)^10 == (0.999)^1000 ~= 0.3677 so the probability that the message is received in its original form is 0.3677 (~36.8%)",['probability']
4731079,Can an open and closed function be neither injective or surjective.,"I know if a function $f: X \to Y$ is bijective, it is open iff it is closed as both are equivalent to $f^{-1}$ being continuous. Now I wonder if a function $f$ on two topological spaces is open and closed, must it be bijective? Furthermore, can it be neither injective nor surjective? I suspect that there is some easy counterexample, but I cannot think of any.","['general-topology', 'functions', 'metric-spaces', 'real-analysis']"
4731119,Show this set has measure zero,"I have a measurable space $(X, \Sigma)$ with $\mu, \nu: \Sigma \rightarrow [0, \infty)$ finite measures. Let $\lambda = \mu + \nu$ . Let $f: X \rightarrow \mathbb{R}$ be a $\Sigma$ -measurable function such that $\nu(E) = \int_E f\ d\lambda$ for all $E \in \Sigma$ . In this problem, I was asked to first show that $0 \leq f \leq 1$ a.e. and $\mu(f^{-1}(\{1\})) = 0$ , both of which I have successfully done. I'm struggling with the proving last part, which says that if $ A\subseteq \{x: 0 \leq f(x) < 1\}$ and $\mu(A) = 0$ , then $\nu(A) = 0$ . I know that if $\mu(A) = 0$ , then $\lambda(A) = \nu(A)$ . But we also have $\nu(A) = \int_A f\ d\lambda$ . So $\lambda(A) = \int_A f\ d\lambda$ . From here, I don't know where to go. I feel that $\mu(f^{-1}(\{1\})) = 0$ will come into play at some point, but I can't seem to incorporate it. I would appreciate any hints.","['measure-theory', 'lebesgue-integral', 'real-analysis']"
4731150,"Are there three rational numbers $(a,b,c)$ such that $r(a)+r(b)=r(c)$ where $r(q) = \frac{2q(1-q^2)}{(1+q^2)^2}$?","For a given rational number $0<q<1$ let $r(q)$ be $$
r(q) = \frac{2q(1-q^2)}{(1+q^2)^2}, \quad q \in (0, 1)
$$ Is there a triple $(a, b, c)$ of rational numbers such that $$
r(a)+r(b)=r(c), \quad a,b,c \in (0,1)?
$$ ( Edit 1: This question arose when I was playing with magic squares of squares, which, to my knowledge, is an unsolved problem. I am not very familiar with elliptic curves and such, that's why I tried to brute force search a solution and didn't find any after trying about a $1000$ rationals ( $10^9$ triples). Some insight on the problem could be very helpful.) Edit 2: As @ThomasAndrews correctly mentioned in the comments, the range of $r$ is in fact the set of values $uv$ such that $u^2+v^2=1$ .
The problem above actually follows from the system $$ \begin{cases}
s_1^2+r_1^2=1,\\
s_2^2+r_2^2=1,\\
s_3^2+r_3^2=1,\\
s_3r_3=s_1r_1+s_2r_2.\\
\end{cases}
$$ I found that all rational points on the unit circle can be parameterized as $$
s = \frac{2q}{1+q^2}, \quad r = \frac{1-q^2}{1+q^2}, \quad q \in \mathbb{Q},
$$ hence the last equation is the real problem in the system. Edit 3: The fact that the set $r([0,1])$ is dense in the interval $\left(0, \frac{1}{2}\right)$ ( since $r$ is a continious function ) also might help.","['number-theory', 'polynomials', 'elliptic-curves', 'rational-numbers']"
4731161,orthogonal vs orthonormal matrices - what are simplest possible definitions and examples of each ??,"I'm trying to understand orthogonal and orthonormal matrices and I'm very confused.  Unfortunately most sources I've found have unclear definitions, and many have conflicting definitions! Some sites like for example https://en.wikipedia.org/wiki/Orthogonal_matrix seem to imply these are the same thing, but most others at least imply if not directly state that they are different. Before someone marks this question as a duplicate I've already consulted Difference between orthogonal and orthonormal matrices and the provided answers do not make the definitions of orthogonal and orthonormal matrices clear, at least not to me. Also, I've been able to find various definitions and verbiage, but I've been able to find very few, if any, examples of matrices that are or aren't orthogonal/orthonormal/both/neither. Additionally I should clarify I'm concerned with the definition of orthogonal and orthonormal matrices, not orthogonal/orthonormal vectors. Based on the sources available and my current understanding of them, this seems to be the definition of each: def. of orthogonal matrix: $AA^T = I$ and: $A^{-1} = A^T$ def. of orthonormal matrix: meets the definition of orthogonal (above) and also: $determinant(A) = 1$ So my questions are: Are these definitions correct?  Please don't throw complicated math formulas at me here, I'm ideally looking for simple logic that I could code in a Python/NumPy if statement (I'm an engineer not a math person!).  If these definitions are not correct, what should they be? What would be examples of matrices that are: a) not orthogonal or orthonormal b) orthogonal but not orthonormal c) orthonormal but not orthogonal (if this is possible, my current understanding is it's not) d) orthogonal and orthonormal","['matrices', 'orthogonality', 'orthonormal']"
4731172,"Why does the exponent rule [If $a^b = a^c,$ then $b=c$] not apply to imaginary numbers [duplicate]","This question already has answers here : What are the Laws of Rational Exponents? (12 answers) Closed 12 months ago . So I came across this video: https://www.youtube.com/watch?v=R476CTKUIr4 in which the creator shows an incorrect proof of π = 0 and the mistake made. The video proves this using the exponent rule $(a^b)^c = a^{bc}$ and shows that it cannot be applied to imaginary numbers. Here are the first few steps from the video which lead me to prove π = 0 (a different way) and I am wondering why this way is also wrong. e^(iπ) +1 = 0 e^(iπ) = -1 [e^(iπ)]^2 = 1 e^(2iπ) = 1  -> True, checked using WolframAlpha e^(2iπ) = e^0 2iπ = 0 iπ = 0 So either i = 0 or π = 0, and both are incorrect. I'm assuming this is because I used the exponent rule from the title here (as the step before it is true). Why is this the case?","['exponentiation', 'fake-proofs', 'pi', 'eulers-number-e', 'trigonometry']"
4731190,"For every matrix $A$, can one find a matrix $B \ne O$ such that $\det(A+B)=\det(A)+\det(B)$?","Since we can't say that $\det(A+B)=\det(A)+\det(B)$ for every matrix $A$ and $B$ , we can ask a different question. Clearly, for every matrix $A$ we have $\det(A+O)=\det(A)+\det(O)$ .
So can we find for every matrix $A$ a matrix $B\neq O$ such that $\det(A+B)=\det(A)+\det(B)$ ? If $A$ is singular we get $\det(A-A)=\det(A)+\det(-A)$ and we can find many other ways to obtain our desired result. How do we deal with the case of $A$ being invertible?","['matrices', 'determinant', 'linear-algebra']"
4731206,"Integrating factor for non-exact ODE $y(1+2x^2+2y^2)\; \mathrm dx + x(1-2x^2-2y^2)\; \mathrm dy=0,$","If we have a non-exact ODE, then to convert it to an exact ODE we multiply the ODE with an integrating factor $\mu(x,y)$ . Lets us say we have the following ODE: $$M(x,y)dx+N(x,y)dy=0,$$ and let us denote $\frac{\partial M(x,y)}{\partial y}=M_y$ and $\frac{\partial N(x,y)}{\partial x}=N_x$ . Since the ODE is not exact, $M_y-M_x =f(x,y)\neq 0$ . We know that if $\frac{f(x,y)}{N(x,y)}$ depends only on $x$ then $e^{\int\frac{f(x,y)}{N(x,y)}dx}$ is the integrating factor of the ODE and if $\frac{f(x,y)}{M(x,y)}$ only depends on $y,$ then $e^{\int\frac{f(x,y)}{-M(x,y)}dy}$ is the integrating factor. But what is the integrating factor of the ODE when both $\frac{f(x,y)}{M(x,y)}$ and $\frac{f(x,y)}{N(x,y)}$ are functions of $x$ and $y$ and neither of them are independent of any variable? For example, in the following ODE: $$y(1+2x^2+2y^2)\; \mathrm dx + x(1-2x^2-2y^2)\; \mathrm dy=0,$$ $M_y-N_x=f(x,y)=8x^2+8y^2$ and neither of $\frac{f(x,y)}{M(x,y)}$ and $\frac{f(x,y)}{N(x,y)}$ are independent of any variable i.e both of them depend on $x$ and $y$ both. So in such cases how do we determine the integrating factor?","['integration', 'calculus', 'integrating-factor', 'ordinary-differential-equations']"
4731225,Diagonalization of specific symmetric tridiagonal matrix,"I am wondering if there is a way to get an explicit expression for the eigenvalues (and possibly the eigenvectors) of a symmetric tridiagonal matrix with the following peculiar structure: $$
\left(
\begin{array}{ccccc}
0 & k\sqrt{1} & 0 & 0 & \dots \\
k\sqrt{1} & 1 & k\sqrt{2} & 0 & \dots \\
0 & k\sqrt{2} & 2 & k\sqrt{3} & \dots \\
0 & 0 & k\sqrt{3} & 3 & \dots \\
\vdots & \vdots & \vdots & \vdots & \ddots
\end{array}
\right)
$$ in the limit where the size of the matrix goes to infinity.
The context of this problem is quantum physics, and by some arguments that are shown here , the answer should be something like $\lambda_n = n - k^2$ . I know that the characteristic polinomial can be written recursively as $\lim_{n\to\infty}p_n(\lambda)$ , where $$
p_n(\lambda)=a_n(\lambda) p_{n-1}(\lambda) - b_{n-1}^2 p_{n-2}(\lambda),
\;\;\;\;\;
p_{0}(\lambda) = 1
\;\;\;\;\;
p_{-1}(\lambda) = 0,
$$ $a_n(\lambda)$ being the diagonal term, so $a_n=n-1-\lambda$ and $b_n$ being the off diagonal term, thus $b_n=k\sqrt{n}$ , but I am stuck here. Can anyone help me with this? Thanks!","['matrices', 'diagonalization', 'tridiagonal-matrices', 'eigenvalues-eigenvectors']"
4731238,Show series $\sum_{k=1}^{\infty} \frac{(2k-2) !\left(\frac{1}{4}\right)^{k-1}\left(1-\frac{1}{4}\right)^k}{k !(k-1) !}=1.$,"I have the following series, together with its result $$\sum_{k=1}^{\infty} \frac{(2k-2) !\left(\frac{1}{4}\right)^{k-1}\left(1-\frac{1}{4}\right)^k}{k !(k-1) !}=1.$$ This can be further simplified to $$\sum_{k=1}^{\infty} \frac{2^{-4 k} \times 3^k\left(\begin{array}{c}
2 k \\
k
\end{array}\right)}{2 k-1}=\frac{1}{2}.$$ It is simple enough to verify that the series is (absolutely) convergent (ratio test), however, I have no idea how to prove the equality. Any ideas for a solution? Any tips for what could usually come in useful in this case?","['integration', 'analysis', 'calculus', 'taylor-expansion', 'sequences-and-series']"
4731240,Show the decreasing of a recursive sequence,"Consider the sequence $(x_n)$ defined by $x_1 = \frac{1}{4}$ and $x_{n+1} = \frac{x_n^{1-\frac{1}{n}}}{x_n+1}$ . It seems that the sequence $(x_n)_{n\geq 2}$ is decreasing based on a program, but I'm unsure how to prove it.
We have $\frac{x_{n+1}}{x_n} = \frac{1}{(x_n +1)x_n^{\frac{1}{n}}}$ , but it's not obvious how to demonstrate that $(x_n +1)x_n^{\frac{1}{n}} \geq 1$ for all $n\geq 2$ . My attempt: By induction we can prove that $\,\forall n\geqslant2\,,\;0<x_n<1\;.$ Let $\,f_n(x)=(1+x)^nx\,.$ $f’_n(x)>0\;\;\forall n\geqslant2\,,\,\forall x\in]0,1[\,.$ How could I continue ?","['calculus', 'sequences-and-series']"
4731297,Vanquishing a dragon using one dimensional random walk with variable path lengths,"My friend posed a question a couple of days back which states the following. There is a dragon which starts with 3 heads, and we can cut one head at a time. Everytime we cut one head, either nothing happens, two heads grow back or three heads grow back. We are asked to find the probability of cutting off all heads. So I modelled this as a random walk on $\mathbb Z$ with an absorbing barrier at $x=0$ . We either go back one step, go forward one step or go forwards two steps, each with equal probability. Let us take $P_k$ to be the probability of starting at $x=k$ and hitting $x=0$ eventually. We then need to find $P_3$ . The recursion I am looking at is $$3P_k=P_{k-2}+P_{k-1}+P_{k+1}$$ with the obvious boundary condition $P_0=1$ . The solution to this recurrence is $$P_k=A+B(1-\sqrt2)^k+C(1+\sqrt2)^k$$ with $A+B+C=1$ . I want my probabilities to be bounded, so $C$ must be $0$ . But here I am stuck. I would need one more boundary condition to proceed, which I am not sure what it might be. I am trying to look at the fact that the mean motion is $2/3$ steps to the right and hence the probability of eventually getting absorbed must decrease (and hence $A=0$ ? But then the probabilites are negative at odd values of $k$ ). I am at a loss how to proceed, or maybe I have done something wrong. Any help is appreciated.","['random-walk', 'markov-chains', 'recurrence-relations', 'probability']"
4731301,Does function $f(x) = kf(x-a) + (1-k)f(x-b)$ for $a / b \notin \mathbb{Q}$ always have a limit?,"(This is my attempt to simplify or generalize my previous unanswered question Proving technique for exchanging lim and (infinite) sum where the sum converges very slowly. ) Consider a function defined on positive real numbers $f: \mathbb{R_+}\to\mathbb{R}$ with the following properties for all $x>a$ , $f(x) = kf(x-a) + (1-k) f(x-b)$ , for some constants $k$ , $a$ , and $b$ where $0<k<1$ and $0<b<a$ $a / b \notin \mathbb{Q}$ (otherwise a counter-example would be $f(x) = \sin(2\pi x/\gcd(a,b))$ )) $f(x)$ is bounded $f(x)$ is continuous  (otherwise a counter-example would be the indicator function $f(x) = \mathbb{1}_{a\mathbb{Z}+b\mathbb{Z}}$ )(can we weaken this to ""discontinuity is nowhere dense""?) Can we conclude that the limit $$\lim_{x\to\infty} f(x)$$ exists? If not, what other condition we need to add to ensure the limit exists? Intuitively, this is true to me. The first functional equation means that each function value is a weighted average of some previous values, so overall it should converge to some sort of average of the ""seeding region"" $x\in(0,a]$ . But I can't really convert this intuition to a concrete proof. Another attempt I had was to try solving the functional equation. I can find the following solution basis $$f(x) = e^{tx},$$ where $t$ is a (potentially complex) solution to the equation $$ke^{-at} + (1-k) e^{-bt} = 1.$$ I don't know if this is the entire solution space, nor how to solve the coefficient for each term. Nevertheless, this equation always has a solution $t = 0$ , whose corresponding term in $f(x)$ would be a constant, which is the limit if it exists. All other solutions $t$ have negative real components (but can be arbitrarily close to 0), meaning their corresponding term in $f(x)$ shrinks and has individual limit of 0. However, I don't think I can directly say the limit of the function is the sum of the limit of each term in this case. Another path I tried to explore is to consider the function on a grid $F: \mathbb{Z}^2\to\mathbb{R}$ , $F(x, y) = f(ax+by)$ . It would be a great first step to prove that $\lim_{\mathbf{v}\to(+\infty,+\infty)}F(\mathbf{v})$ exists, and then we can ""fill in"" other values for $f(x)$ using continuity. On the grid, it can be seen that what contributes to $F(x, y)$ is a structure similar to binomial expansion: \begin{aligned}
F(x, y) &= kF(x-1, y) + (1-k) F(x,y-1)\\
&= k^2 F(x-2, y) + 2k(1-k) F(x-1,y-1) + (1-k)^2 F(x,y-2)\\
&= k^3 F(x-3, y) + 3k^2(1-k) F(x-2,y-1) + 3k(1-k)^2 F(x-1, y-2) + (1-k)^3 F(x,y-3)\\
...
\end{aligned} As good as it looks, it doesn't help me with the question.","['limits', 'calculus', 'functions', 'functional-equations']"
4731329,Signed Permutations and Coxeter Groups,"Context: (most of which is pulled from comments and answers to https://mathoverflow.net/questions/431964/signed-permutations-and-operatornameson ) The diagonal subgroup $ C_2^n $ of $ O_n(\mathbb{Z}) $ is a generated by reflections. The permutation subgroup $ S_n $ of $ O_n(\mathbb{Z}) $ is also generated by reflections. Thus the subgroup of $ O_n(\mathbb{Z}) $ consisting of all signed permutation matrices, also known as the Weyl group $ W(B_n) $ , is also generated by reflections. Viewing these statements abstractly we have that $ C_2^n $ is a Coxeter group, $ S_n $ is a Coxeter group and the group of signed permutation matrices $ W(B_n) $ (which has shape $ 2^{n}:S_n $ ) is also a Coxeter group. It is also known that $ W(D_n) $ , the index $ 2 $ normal subgroup of $ W(B_n) $ consisting of monomial matrices with an even number of sign changes, is a Coxeter group. It has shape $ 2^{n-1}:S_n $ . What about the subgroup of $ W(B_n) $ of matrices with determinant $ +1 $ ? We will denote this subgroup of $ SO(n) $ by $ W_n $ . This is an a priori different index $ 2 $ normal subgroup of $ W(B_n) $ than $ W(D_n) $ . It is the case that $ W(D_3)=W(A_3)=S_4=W_3 $ . Strange that while $ W_2=C_4 $ on the contrary $ W(D_2)=C_2 \times C_2 $ . Something similar must happen to $ W_4 $ and $ W(D_4) $ with $ W_4 $ having more elements of order $ 4 $ . In general I guess $ W_n $ will have more elements of order $ 4 $ than $ W(D_n) $ (with the exception of $ n=3 $ when the groups coincide). $ W_n $ has shape $ 2^{n-1}.S_n $ . $ W_n $ , for $ n \geq 3 $ , is generated by involutions using blocks of the form $$
\begin{bmatrix} 0 & 1 & 0\\ 1 & 0 & 0 \\ 0 & 0 & -1 \end{bmatrix} 
$$ however such involutions have two $ -1 $ eigenvalues instead of one so they are not reflections in a codimension 1 hyperplane and so it is still hard to say if $ W_n $ is a coxeter group or not. As previously stated, it is the case that $ C_2^n,S_n,W(B_n),W(D_n) $ are all Coxeter groups. But what about $ W_n $ ? Question: Is it the case that $ W_n \cong W(D_n) $ if and only if $ n $ is odd? And moreover is it the case that $ W_n $ is a Coxeter group if and only if $ n $ is odd?","['group-theory', 'finite-groups', 'coxeter-groups']"
4731330,How to evaluate $\lim\limits_{x \to-\infty}\frac{\sqrt{16x^6-x^2}}{6x^3+x^2}$,How I approach this problem:- $\displaystyle \lim_{x  \to -\infty} \frac{\sqrt{16x^6 - x^2}}{6x^3+x^2}$ $= \displaystyle \lim_{x  \to -\infty} \frac{\sqrt{x^6(16 - \frac{x^2}{x^6}})}{x^2(6x+1)}$ $= \displaystyle \lim_{x  \to -\infty} \frac{x^3\sqrt{16 - \frac{1}{x^4}}}{x^2(6x+1)}$ $= \displaystyle \lim_{x  \to -\infty} \frac{x\sqrt{16 - \frac{1}{x^4}}}{6x+1}$ $= \displaystyle \lim_{x  \to -\infty} \frac{\sqrt{16 - \frac{1}{x^4}}}{6+\frac{1}{x}}$ Now the answer to this problem is $-\frac{2}{3}$ but it doesn't seems to match with the last expression. What I observe in the initial step is the numerator is positive because of the even powers and denominator is negative because cubic power dominates square power and we get negative in the denominator. Still what if we didn't knew the initial step and knew only the last one. How one will conclude it's $-\frac{2}{3}$ . Whether $x \to -\infty \;\text{or}\; \infty$ the last expression always comes out to be $\frac{\sqrt{16}}{6}$ or my steps are wrong?,"['limits', 'calculus']"
4731348,Prove that $xax^{-1} = yay^{-1}$ iff $C_a x = C_a y$.,"I'm trying to prove an exercise in a book on abstract algebra I am reading. The setup is that for any element $a$ of a group $G$ , we define the centraliser $C_a = \{ x \in G : xa = ax \}$ , so all elements that commute with $a$ . Now, we must prove that if $xax^{-1} = yay^{-1}$ , that $C_a x = C_a y$ . My progress is that $xax^{-1} = yay^{-1}$ implies that $(y^{-1}x)a = a(y^{-1}x)$ , so $y^{-1}x \in C_a$ . However, this proves that $C_a y^{-1} = C_a x^{-1}$ , which I do not think is equivalent to $C_a x = C_a y$ (as $C_a x = C_a x^{-1}$ is not generally true). Am I missing something obvious here?","['group-theory', 'abstract-algebra']"
4731384,"If $4ax+3by+12c$ is normal to ellipse $\frac{x^2}{a^2}+\frac{y^2}{b^2}=1$, then how are $c$, $a$, $e$ related?","Here we have equation of normal of ellipse, $4ax+3by=12c$ and ellipse $\frac{x^2}{a^2}+\frac{y^2}{b^2}=1$ What is the relation between $(c, a, e)$ ? (a) $c=a^2e^2\quad$ (b) $c=5a^2e^2\quad$ (c) $5c=a^2e^2\quad$ (d) None of these The answer is (c) $5c=a^2e^2$ . I tried to solved it with formula of normal and by putting normal points in it. ( $ae,\pm\dfrac{b^2}{e}$ ), so here $\dfrac{a^2x}{x_1}+\dfrac{b^2y}{y_1}=a^2-b^2$ , but couldn't find the relation.","['analytic-geometry', 'conic-sections', 'geometry']"
4731466,What is the limit $a_{n+1}=\frac{1+c_n}{1+\frac{1}{b_n}}$ when n goes to infinity?,"Given that $\left\{
\begin{matrix}
a_{n+1}=\frac{1+c_n}{1+\frac{1}{b_n}}\\
b_{n+1}=\frac{1+a_n}{1+\frac{1}{c_n}}\\
c_{n+1}=\frac{1+b_n}{1+\frac{1}{a_n}}\\
\end{matrix}
\right.
$ where $a_0b_0c_0=1$ and $a_0, b_0, c_0>0$ Find $\displaystyle\lim_{n\to\infty} a_n$ , $\displaystyle\lim_{n\to\infty} b_n$ and $\displaystyle\lim_{n\to\infty} c_n$ My work: $a_nb_nc_n=1$ , it can be proved by induction By checking the first 100 terms with excel, $a_n, b_n$ and $c_n$ probably converge to $1$ . The sequence is not monotonic.","['limits', 'sequences-and-series']"
4731499,More examples of morphisms of ringed spaces that aren't local?,"$\def\Spec{\operatorname{Spec}}$ All questions and answers that I've found in MSE regarding a morphism of ringed spaces between affine schemes that isn't a morphism of locally ringed spaces are the same example, namely, this one . On the other hand, there are infinite counterexamples for l.r.s. that aren't affine schemes: given any non-local morphism of local rings $R\to S$ , the morphism of ringed spaces $(\{*\},R)\to(\{*\},S)$ is not local. But can we produce more counterexamples between affine schemes different from the linked one? In Example 6.17 here (p. 29) one finds a morphism of ringed spaces $\Spec\mathbb{Q}\to\Spec\mathbb{Z}$ that is not local. More generally, given a PID $R$ and a prime $(p)\in R$ , we can similarly produce a non-local ringed spaces morphism $\Spec Q(R)\to\Spec R$ that sends $*\in\Spec Q(R)$ to $(p)$ . However, this idea reduces to that of the first linked case with the DVR $R_{(p)}$ , by factoring the last morphism as $\Spec Q(R)\to\Spec R_{(p)}\to\Spec R$ (where the second arrow is the morphism of l.r.s. induced by $R\to R_{(p)}$ ). So: can we cook up some genuinely new kind of morphism of ringed spaces $\Spec B\to\Spec A$ that is not of l.r.s.? Maybe this has been answered here before, but I have been unable to find it. (My question is motivated by, assuming $B$ is Jacobson, trying to find a morphism of ringed spaces $\Spec B\to\Spec A$ that is not of l.r.s. but such that the composite $\operatorname{Spm}B\to\Spec B\to\Spec A$ is a morphism of l.r.s. If such a morphism $\Spec B\to\Spec A$ were to exist, this would imply that at least one of “ $A$ is Jacobson,” “ $A\to B$ is of finite type” is false.) EDIT: I just found a proof that if $B$ is Jacobson and the composite $\operatorname{Spm}B\to\Spec B\to \Spec A$ is a morphism of l.r.s., then so is $\Spec B\to\Spec A$ . However, I'm still interested on the original question of constructing novel examples of non-local morphisms of ringed spaces between affine schemes.","['affine-schemes', 'ringed-spaces', 'algebraic-geometry', 'examples-counterexamples']"
4731557,Probability of winning a match given probability of winning a game,"If a team has probability p of winning each game, and if the team wins/loses a match when it is 2 games ahead/behind the other team, is there an analytical formula or approximation for the probability that the team wins the match, as a function of p? One can of course resort to Monte Carlo. Here is what I tried: p = prob of A winning each game
p1 = prob of A winning when match tied
p2 = prob of A winning when ahead by 1 game
p3 = prob of A winning when behind by 1 game

p1 = p*p2 + (1-p)*p3 = p*p2 + p3 - p*p3
p2 = p + (1-p)*p1    = p + p1 - p*p1
p3 = p*p1 + 1 - p    = p*p1 + 1 - p

plug last into first:

p1 = p*p2 + (1-p) * (p*p1 + 1 - p)

plug 2nd into above

p1 = p*(p + p1 - p*p1) + (1-p) * (p*p1 + 1 - p)
   = p^2 + p*p1 - p^2*p1 + p*p1 + 1 - p - p^2*p1 - p + p^2

p1 = (2*p^2 - 2*p + 1)/(2*p^2 - p + 1) But for p=0 , this gives p1=1 , which is wrong.",['probability']
4731621,"For three vectors in any inner product space, prove the following inequality.","Let $x, y, z$ be three unit vectors in a real inner product space. Assume that we are given the inner products $\langle x, y \rangle = a \in \mathbb{R}$ and $\langle y, z \rangle = b \in \mathbb{R}$ . How do you obtain a bound on the value of $\langle z, x \rangle$ , in particular, prove the following? $$ \langle z, x \rangle \geq ab - \sqrt{1 - a^2} \sqrt{1 - b^2} $$ My Approach I was able to prove this inequality for the vector space $\mathbb{R}^n$ geometrically. I assumed that the angle between $x$ and $y$ is $\theta$ and the angle between $y$ and $z$ is $\phi$ . Thus $a = \cos{\theta}, b = \cos{\phi}$ . The inner product of $x$ and $z$ is minimized when the angle between them is the largest and this happens when all three vectors are coplanar and $x$ and $z$ are on opposite sides of $y$ $$\therefore \min \langle z, x \rangle = \cos{(\theta + \phi)} = \cos{\theta} \cos{\phi} - \sin{\theta} \sin{\phi} = ab - \sqrt{1 - a^2} \sqrt{1 - b^2}$$ I however want to approach this question from a purely algebraic perspective without involving trigonometric or visualization, using purely the results from the algebra of inner product spaces. Thanks in advance!","['inner-products', 'inequality', 'linear-algebra']"
4731673,Function where order of integration cannot be reversed,"Our professor gave us the following problem to give us an example where we cannot switch the order of integration. However, I am stuck on the last part, so maybe someone can give me an answer. Let $i = 1,2,\dots$ and $\phi_i: \mathbb{R} \rightarrow \mathbb{R}$ be continuous functions with $\mathrm{supp}(\phi_i) \subset (2^{-i}, 2^{1-i})$ and $\int_{\mathbb{R}}\phi_i = 1$ .
Then we define $$f(x,y) := \sum_{i = 1}^{\infty}(\phi_i(x) - \phi_{i+1}(x))\phi_i(y).$$ I already managed to prove that $f$ is continuous on $\mathbb{R}^2 - \{(0,0)\}$ . The main problem is the following, however; we are meant to show that $$ \int_{\mathbb{R}}{\left(\int_{\mathbb{R}}f(x,y)\mathrm{d}x\right)\mathrm{d}y} = 0,$$ but $$ \int_{\mathbb{R}}{\left(\int_{\mathbb{R}}f(x,y)\mathrm{d}y\right)\mathrm{d}x} = 1.$$ The first result is what I also obtain, but I seem to be doing something wrong, as I obtain 0 for the second integral as well.","['integration', 'multivariable-calculus', 'real-analysis']"
4731696,Proof that $V$ is an open set,"Let $f:\mathbb{R}^{n}\times \mathbb{R}^{n}\to \mathbb{R}$ be of class $C^{k+2}$ , $k \ge 0$ and suppose that, for each $(x,y) \in \mathbb{R}^{n}\times \mathbb{R}^{n}$ , the matrix: $$\bigg{[}\frac{\partial^{2}f}{\partial y_{i}\partial y_{j}}(x,y)\bigg{]}_{1\le i,j\le n}$$ is positive. In other words, for each fixed $x \in \mathbb{R}^{n}$ , the mapping $y \mapsto f_{x}(y) = f(x,y)$ is strictly convex. For each fixed $x \in \mathbb{R}^{n}$ and $p \in \mathbb{R}^{n}$ , define $\varphi_{x,p}: \mathbb{R}^{n}\to \mathbb{R}$ by: $$\varphi_{x,p}(y) = \langle y, p \rangle - f(x,y).$$ It follows from the hypothesis on $f$ that $\varphi_{x,p}$ has at most one critical point and, any such critical point (if it exists) is a global maximum. I want to prove the following: Proposition: The set $V =\{(x,p)\in \mathbb{R}^{n}\times \mathbb{R}^{n}: \mbox{$\varphi_{x,p}$ has a global maximum}\}$ is nonempty and open. Attempted Proof: Note that $(x,p) \in V \iff \exists y_{0} \in \mathbb{R}^{n}$ such that $\nabla_{y}\varphi_{x,p}(y_{0}) = 0 \iff \nabla_{y}f(x,y_{0}) = p$ . Hence: $$V = \{(x,p) \in \mathbb{R}^{n}\times \mathbb{R}^{n}: \mbox{$ y_{0} \in \mathbb{R}^{n}$ such that $\nabla_{y}f(x,y_{0}) = p$}\}$$ For each $x \in \mathbb{R}^{n}$ fixed, $\nabla_{y}f(x,\cdot)$ is injective and, hence, invertible. Let $p$ be an element of its range. In this case, there exists a unique $y_{0} \in \mathbb{R}^{n}$ with $\nabla_{y}f(x,y_{0}) = p$ , so that $V \neq \emptyset$ . My question is: how to prove $V$ is open? I sketched the following proof. Let $V_{x} = \{p \in \mathbb{R}^{n}: \mbox{$p=\nabla_{y}f(x,y_{0})$ for some $y_{0}$}\}$ . This is open because it is the pre-image of $\nabla_{y}f(x,\cdot)$ . Now $V = \bigcup_{x\in \mathbb{R}^{n}}\{x\}\times V_{x}$ . But is this set open? Note: the notation $\nabla_{y}f(x,y)$ means $(\frac{\partial f}{\partial y_{1}}(x,y),...,\frac{\partial f}{\partial y_{n}}(x,y))$ .","['multivariable-calculus', 'solution-verification', 'convex-analysis', 'real-analysis']"
4731710,Reference request: Lorentzian Ricci flow,"I have been studying some aspects of Ricci flow, namely existence, uniqueness, finite time extinction, the preservation of curvature bounds via the maximum principle, and the modifications of Ricci flow that lead to long time solutions with limiting geometries of interest like constant curvature metrics. As a physicist, I naturally wanted to see what the analogue of Ricci flow is for Lorentzian manifolds. A couple of problems arise: (1) globally hyperbolic spacetimes are not closed, (2) non compact manifolds without further conditions may render the flow ill-defined for existence of solutions, and (3) the Laplacian in this case is not elliptic given the metric is not positive. Does anyone have any good references where progress is made on some of these problems in the Lorentzian setting? I would also appreciate any insight from the community here, as I am a bit out of my depth.","['semi-riemannian-geometry', 'ricci-flow', 'reference-request', 'differential-geometry']"
4731730,Expectation value of stochastic differential equation using Fokker-Planck distributions,"Suppose we have a stochastic variable $x(t)$ with known steady-state Fokker-Planck distribution $P(x)$ , i.e. for any $f(x)$ , its expectation value is $$\overline{(f(x))}=\int dx f(x) P(x) $$ My question is what happens to $f(x)=\dot{x}(t)$ ? Is it correct to assume that $$\int dx \dot{x}(t) P(x) = \overline{\dot{x}(t)}$$ I do know the form of $\dot{x}(t)=-a x(t)+b \eta(t)$ , where $\overline{\eta(t)}=0$ and $\overline{\eta(t)\eta(t')}=\delta(t-t')$ . What can I say about $\overline{\dot{x}}$ ?","['stochastic-processes', 'statistics', 'partial-differential-equations']"
4731736,A Gaussian process that is continuous in probability but not almost surely,"Let $(X_t)_{t\in [0,1]}$ be a Gaussian process. $(X_t)$ is continuous in probability if and only if its mean and autocovariance function are continuous on $[0,1]$ . Continuity in probability does not imply continuity almost surely. But I cannot find an example of an $(X_t)$ that is continuous in probability but not almost surely. What is a classical counterexample?","['continuity', 'measure-theory', 'probability-theory', 'normal-distribution']"
4731764,Closed form of the integral of $r \ln (r^2 + \rho^2 - 2r \rho \cos (\theta - t))$,"I have the following integral that needs to be evaluated. $$
\int_{0}^{2\pi} \int_0^{\frac 1 {\sqrt{4 \cos^2 \theta  + 9 \sin^2 \theta}}} \frac r 2 \ln (r^2 + \rho^2 - 2r \rho \cos (\theta - \phi)) dr d\theta
$$ I tried using Mathematica but it gives the following output -((\[Rho] (-2 E^(2 I \[Theta]) Sqrt[
        26 - 5 E^(-2 I \[Theta]) - 
         5 E^(2 I \[Theta])] + (5 - 26 E^(2 I \[Theta]) + 
          5 E^(4 I \[Theta])) \[Rho] ArcTan[
         Cot[\[Theta] - \[Phi]]] Cos[
         2 \[Theta] - 2 \[Phi]] Csc[\[Theta] - \[Phi]] - (5 - 
          26 E^(2 I \[Theta]) + 5 E^(4 I \[Theta])) \[Rho] ArcTan[
         Cot[\[Theta] - \[Phi]] - (
          Sqrt[2] Csc[\[Theta] - \[Phi]])/(\[Rho] Sqrt[
           13 - 5 Cos[2 \[Theta]]])] Cos[
         2 \[Theta] - 2 \[Phi]] Csc[\[Theta] - \[Phi]] - 
       10 \[Rho] Cos[\[Theta] - \[Phi]] Log[\[Rho]] + 
       52 E^(2 I \[Theta]) \[Rho] Cos[\[Theta] - \[Phi]] Log[\[Rho]] \
- 10 E^(4 I \[Theta]) \[Rho] Cos[\[Theta] - \[Phi]] Log[\[Rho]] + 
       5 \[Rho] Cos[\[Theta] - \[Phi]] Log[(
          5 \[Rho]^2 + 5 E^(4 I \[Theta]) \[Rho]^2 - 
           2 E^(2 I \[Theta]) (2 + 13 \[Rho]^2))/(
          5 - 26 E^(2 I \[Theta]) + 5 E^(4 I \[Theta])) - (
          4 \[Rho] Cos[\[Theta] - \[Phi]])/Sqrt[
          26 - 5 E^(-2 I \[Theta]) - 5 E^(2 I \[Theta])]] - 
       26 E^(2 I \[Theta]) \[Rho] Cos[\[Theta] - \[Phi]] Log[(
          5 \[Rho]^2 + 5 E^(4 I \[Theta]) \[Rho]^2 - 
           2 E^(2 I \[Theta]) (2 + 13 \[Rho]^2))/(
          5 - 26 E^(2 I \[Theta]) + 5 E^(4 I \[Theta])) - (
          4 \[Rho] Cos[\[Theta] - \[Phi]])/Sqrt[
          26 - 5 E^(-2 I \[Theta]) - 5 E^(2 I \[Theta])]] + 
       5 E^(4 I \[Theta]) \[Rho] Cos[\[Theta] - \[Phi]] Log[(
          5 \[Rho]^2 + 5 E^(4 I \[Theta]) \[Rho]^2 - 
           2 E^(2 I \[Theta]) (2 + 13 \[Rho]^2))/(
          5 - 26 E^(2 I \[Theta]) + 5 E^(4 I \[Theta])) - (
          4 \[Rho] Cos[\[Theta] - \[Phi]])/Sqrt[
          26 - 5 E^(-2 I \[Theta]) - 
           5 E^(2 I \[Theta])]]) Sin[\[Theta] - \[Phi]])/(5 - 
     26 E^(2 I \[Theta]) + 5 E^(4 I \[Theta]))) Does closed form of the above integral exists? What approaches might I try to solve this integral? Is this possible to express this integral in terms of special functions?","['integration', 'calculus', 'definite-integrals', 'polar-coordinates']"
4731771,Evaluate $\sum_{k\ge1}\frac{(-1)^{k+1}}{k^2}\log\left[ \frac{\Gamma\left(1+\frac{k}{2}\right)}{\Gamma\left(\frac12+\frac{k}{2}\right)} \right]$,"I am struggling to evaluate the following series: $$\mathcal{S}=\sum_{k=1}^\infty\frac{(-1)^{k+1}}{k^2}\log\left[ \frac{\Gamma\left(1+\frac{k}{2}\right)}{\Gamma\left(\frac12+\frac{k}{2}\right)} \right]=-0.13360...$$ From what I tried, I figured out that, if a closed form does exist, it must be really difficult to find. Here are my attempts: Attempt (1): using the fact that, for $\Re(z)>0$ and $\Re(z-x)>0$ $$\frac{\Gamma(z+1)}{\Gamma(z+1-x)}=e^{-\gamma x}\prod_{n=1}^\infty\left[ \left(1-\frac{x}{z+n}\right)e^{\frac{x}{n}} \right]$$ $$\implies \frac{\Gamma(z+1)}{\Gamma\left(z+\frac12\right)}=e^{-\frac{\gamma}{2}}\prod_{n=1}^\infty\left[ \left(1-\frac{1}{2(z+n)}\right)e^{\frac{1}{2n}} \right]$$ $$\implies \frac{\Gamma\left(1+\frac{k}{2}\right)}{\Gamma\left(\frac12+\frac{k}{2}\right)}=e^{-\frac{\gamma}{2}}\prod_{n=1}^\infty\left[ \left(1-\frac{1}{k+2n}\right)e^{\frac{1}{2n}} \right]$$ $$\implies \log\left[ \frac{\Gamma\left(1+\frac{k}{2}\right)}{\Gamma\left(\frac12+\frac{k}{2}\right)} \right]=-\frac{\gamma}{2}+\sum_{n=1}^\infty \left[\frac{1}{2n}+\log\left(1-\frac{1}{k+2n}\right)\right]$$ and here the tough part is the double sum that we end up with after plugging this value into the original $\mathcal{S}$ . Notice that all these steps where valid only for $k>1$ , so we would get $$\mathcal{S}=\log\left(\frac{\sqrt{\pi}}{2}\right)+\frac{\gamma}{2}\left(1-\frac{\pi^2}{12} \right) +\sum_{k=2}^\infty \sum_{n=1}^\infty \frac{(-1)^{k+1}}{k^2}\left[\frac{1}{2n}+\log\left(1-\frac{1}{k+2n}\right)\right]$$ which had me stuck. Attempt (2): I tried splitting the sum into even and odd terms, in order to get rid of the gamma functions. For instance, the odd terms form the following series: $$\mathcal{S_1}=\sum_{k=0}^\infty\frac{1}{(2k+1)^2}\log\left[ \frac{\Gamma(k+1+\frac12)}{\Gamma(k+1)} \right]$$ Now, using the fact that $$\frac{\Gamma(n+\frac12)}{\Gamma (n)}={{2n} \choose n}\frac{n}{4^n}\sqrt{\pi}$$ we have $$\mathcal{S_1}=\sum_{k=0}^\infty\frac{1}{(2k+1)^2}\log\left[{{2k} \choose k}\frac{2k+1}{4^k}\frac{\sqrt{\pi}}{2} \right]$$ $$=\frac{\pi^2}{8}\log\left(\frac{\sqrt{\pi}}{2}\right)+\sum_{k=0}^\infty\frac{1}{(2k+1)^2}\log\left[{{2k} \choose k}\frac{2k+1}{4^k} \right]$$ but now this series has this $\log$ term that is difficult to handle. We can't even split it using $\log$ properties, since the resulting series wouldn't converge at all. Attempt (3): It can be shown that $$\int_0^1\int_0^1\frac{\log(1+x^a)}{1+x}\text{d}x \text{d}a=2\log^22+\frac{\pi^2}{24}\log\pi+\mathcal{S}$$ and notice that here, changing the order of integration is allowed. I couldn't evaluate this double integral either in the end. EDIT: Attempt (4): Using this we can rewrite $\mathcal{S}$ as: $$\mathcal{S}=-\frac{\pi^2}{24}\log\pi-\int_0^1\frac{\text{Li}_2(-x)+\frac{\pi^2}{12}}{(x+1)\log x}\text{d}x$$ and this integral is similar to the ones that appear here and here , so maybe it can be expressed as a series involving harmonic numbers as well?","['integration', 'analysis', 'real-analysis', 'calculus', 'sequences-and-series']"
4731820,Geometric Intuition for Hamiltonian Actions,"Let $G$ be a connected Lie group acting on the symplectic manifold $(M,\omega)$ . In the definition of a Hamiltonian action one requires that the moment map $\mu\colon M\xrightarrow{} \mathfrak{g}^\ast$ satisfies $$\omega(X_\xi,\cdot) = d\langle \mu,\xi\rangle$$ for all $\xi\in\mathfrak{g}$ . In other words, we want $X_\xi$ to be the Hamiltonian vector field of the function $\langle \mu,\xi\rangle$ . While I understand what the definition says, I don't really have a geometric understanding of it. The second condition in the definition is that $\mu$ should be $G$ -equivariant, which is more intuitive to me. What exactly does the above condition ensure? Is there a more geometric way to think about it?","['symplectic-geometry', 'lie-algebras', 'group-actions', 'lie-groups', 'differential-geometry']"
4731827,Is the textbook WRONG on the branch cut?,"As defined in the textbook, it takes the branch cut along positive real axis. So the argument angle is $[0,2\pi)$ . This is no problem. But in the example, it sets $f(z)=\frac1{\sqrt{2-z}}$ and claims $$\frac1{\sqrt{2-z}}=\frac i{\sqrt{z-2}}$$ and the phase factor of $f(z)$ is also $-1$ . If use the branch cut from $2$ to infinity, then $2-z=(z-2)e^{i\pi}$ , so $$f(z)=\frac1{\sqrt{2-z}}=\frac1{\sqrt{(z-2)e^{i\pi}}}=\frac{-i}{\sqrt{z-2}}$$ Is the textbook wrong?","['complex-analysis', 'branch-points', 'branch-cuts']"
4731829,Doubt on base point for a linear system,"I'm studying divisors on Riemann surfaces and I got stuck on a thing said by the professor at lesson. Probably I'm getting lost in a glass of water, or there is something wrong in my notes: if this is so please let me notice what is the error. Consider a linear system $V$ on a (compact) Riemann surface $X,$ i.e. a subset of a complete linear system $$|D|=\{E\in \operatorname{Div}(X) \mid E\geq 0,\ \exists f:X\to\mathbb{C}\text{ meromorphic such that }E=\operatorname{div}(f)+D  \}$$ we say that $p\in X$ is a base point for $V$ if $p$ is in the support of each divisor in $V$ (with this statement I mean that the coefficient $E(p)$ of $p$ in each divisor $E\in V$ is nonzero). I know that $V$ is in correspondence with a linear subspace $\overline{V}$ of $\mathbb{P}(L(D)),$ where $$L(D)=\{f:X\to\mathbb{C} \mid \operatorname{div}(f)\geq -D \}$$ In the notes there is this statement: $p$ is a base point for $V$ if and only if for each $f\in\overline{V}$ we have $f(p)=0$ My attempt: I know that the divisors in the linear system $V$ are exactly those of the form $$\operatorname{div}(f)+D$$ with $f\in\overline{V}$ . If $p$ is a base point for $V$ I have that $E(p)\neq0$ for each $E\in V:$ hence we must have $E(p)\geq 1,$ since each divisor $E\in|D|$ satisfies $E\geq0.$ If we take a function $f\in\overline{V}$ we can consider the divisor $E=\operatorname{div}(f)+D\in V:$ using the previous observation and we must have $$\operatorname{ord}_p(f)+D(p)\geq 1$$ To conclude I must prove that $\operatorname{ord}_p(f)\geq1,$ and this is certainly true if $D(p)\leq0,$ but I don't see how to continue from this point if $D(p)\geq 1.$ I don't think this is a difficult thing to prove, since this is a simple observation in the notes, but I don't see how to conclude...","['riemann-surfaces', 'divisors-algebraic-geometry', 'algebraic-geometry']"
4731830,Another integral representation of Catalan's Constant?,"The integral in question: $$I=\int_{0}^{1} {\frac{\arctan\left(\frac{x-1}{\sqrt{x^2-1}}\right)}{\sqrt{x^2-1}}}dx.$$ So I have a solution and it's rather straightforward, but I don't much like it. I'm not sure how to tackle this integral head-on, as you'll see, and I was wondering if anyone had another way of arriving at the result. My Solution: Let $$I(a)=\int_{0}^{\frac{\pi}{2}} {\ln{(\sin{(x)}+a)}}\hspace{1pt}dx.$$ Then $$I'(a)=\int_{0}^{\frac{\pi}{2}} {\frac{1}{\sin{(x)}+a}}dx=\frac{2}{\sqrt{a^2-1}}\arctan{\left(\frac{a-1}{\sqrt{a^2-1}}\right)}.$$ To show this one can apply the Weierstrass Substitution and complete the square in the denominator. Now consider the following $$\int_{0}^{1} {\frac{2}{\sqrt{x^2-1}}\arctan{\left(\frac{x-1}{\sqrt{x^2-1}}\right)}}dx=I(1)-I(0).$$ Noticing the famous result $I(0)=-{\pi}\ln(2)/2$ (check 1 for more details) we have $$I=\frac{\pi}{4}\ln{(2)}+\frac{1}{2}\int_{0}^{1} {\ln{(\sin{(x)}+1)}}\hspace{1pt}dx.$$ Using $$\sin{(x)}+1=2\sin{\left(\frac{1}{2}\left(x+\frac{\pi}{2}\right)\right)}\cos{\left(\frac{1}{2}\left(x-\frac{\pi}{2}\right)\right)}$$ we arrive at the following $$I=\frac{\pi}{2}\ln{(2)}+2\int_{0}^{\frac{\pi}{4}} {\ln{(\cos{(x)})}}\hspace{1pt}dx$$ and now by 2 we have $I=C$ , where $C$ is Catalan's Constant. If you have seen this integral before please let me know where and again if you have a different solution that would be much appreciated. :)","['integration', 'calculus', 'catalans-constant', 'definite-integrals']"
4731876,Why do complex numbers lend themselves to rotation?,"In the introductory complex analysis course I am taking, nearly every theorem relates to rotation and argument. Why do complex numbers love doing this so much?
I can understand why these theorems work; however, aside from basic knowledge of polar coordinates, I do not intuitively understand what property of complex numbers make rotation and angle such a common/convenient idea.","['complex-analysis', 'angle', 'complex-numbers', 'rotations']"
4731910,How to Create a Mathematical Model of an Elevator?,"This is a problem I have been thinking about for a while. Everyday, I take an elevator and observe that people get on and off. Given the current situation (i.e. what floor I am on and how many people are currently in the elevator) - I find myself always trying to guess: When the elevator will reach my destination? What floor will the elevator stop on next? How many people will enter and exit the elevator on the next stop? While I wait in the elevator, I always think: Suppose if we had an entire elevator log that detailed every elevator trip that was made over a year , and this included information how many people got on/off at each floor: How could we create a mathematical model to represent this system and answer the above questions? To begin, let's state some of the following conditions: The elevator has a maximum capacity of $n_k$ people : once $n_k$ people have entered the elevator, no one outside the elevator can request the elevator to stop. The elevator can only stop when someone inside requests the elevator to stop or once the elevator has reached the final floor. When the elevator starts, there are currently $n_0$ people on the elevator (where $n_0$ is some positive integer) There are $m$ floors in total On the $j^{th}$ floor (where $ j\leq m$ ) , the number of people that enter can be denoted by $x_j$ and the number of people that exit can be denoted by $y_j$ The immediate future of the elevator is only decided by the current situation of the elevator (i.e. what floor we are on and how many people are in the elevator) - i.e. the elevator has the memoryless property The time of day does is not being taken into consideration and is being considered as irrelevant in this problem (i.e. peak hours like lunch break, end of day, etc. are not important). The first thing that comes to mind is to use (Discrete Time) Markov Chains. As such, I think the elevator can be thought of to contain two underlying Markov Chains: Markov Chain for Floor Transitions: A Markov Chain can be created which will look at the probability of the elevator stopping at any other floor given the current floor Markov Chain for Elevator Capacity: A Markov Chain (similar to a queue/birth-death process) can be created which will look at the net change in capacity given the current capacity My Question: Given access to the elevator log - I know how to create two independent Markov Chains to model floor transitions and elevator capacity independently. Using conditional probabilities, I could analyze the number of times the elevator was on the $i^{th}$ floor - and of those times, how many times was the next stop made at the $i^{th}$ floor: this information would serve as an estimate of $p_{i,j}$ , the probability of directly transitioning from the $i^{th}$ floor to the $j^{th}$ floor (note that $p_{j,i}$ does not need to be equal to $p_{i,j}$ ). Using conditional probabilities, I could also analyze the probability of the change in capacity. If the elevator current has 5 people - I could calculate what is the probability that after the next stop, there will now be 9 people. But it is unclear to me how to ""jointly"" model both of these Markov Chains together. For example, if the elevator reaches capacity at the 3rd last floor - it might be very likely that no one will request a stop until the final stop. However, if the elevator reaches maximum capacity early on - it might be more likely that someone on the elevator will have to go to an intermediate floor and give the elevator another chance to start accepting more passengers and take on more possible trajectories. I thought perhaps I could combine both of these chains together into a single chain - but the number of states in this single chain would explode in size , for example: State 1: 1st Floor, 0 People State 2: 1st Floor, 1 Person State 3: 1st Floor, 2 people State 4: 1st Floor, 3 people ... State 1-k: 1st Floor, $n_k$ people ... State k-m: $n_k$ people, $m^{th}$ floor Apart from resulting in a very large number states and make all computations difficult - more concerning would be the fact that I might not have enough data (i.e. joint combinations of floor transitions at varying capacity levels) to reliably estimate all state probabilities Can someone please advise me on how I can use Markov Chains to jointly model both of these processes (i.e. floor transitions and capacity)? Thanks!","['stochastic-processes', 'probability']"
4731959,Intuition behind Liouville's theorem,"This post , currently closed, asks  for the intuition behind Liouville's theorem , which states that every bounded entire function is constant; I find the answers unsatisfactory and hand-wavy. I am looking for a more focused, precise description of what is going on. Is there a geometrical understanding of what is going on? I try to visualize a non-constant analytic function, to understand why unboundedness is necessary along some direction, but it isn't working for me.","['complex-analysis', 'intuition']"
4731985,"Evaluate: $\int \frac{{\sin(x)}}{{\sin(x) - \cos(x)}}\,dx$","I'm trying to evaluate the following integral: $$\int \frac{{\sin(x)}}{{\sin(x) - \cos(x)}}\,dx$$ Here's my approach so far: $$$$ I've multiplied the numerator and denominator by $-csc^3(x)$ , resulting in $$\int -\frac{{\csc^2(x)}}{{\cot(x)\csc^2(x) - \csc^2(x)}}\,dx$$ Next I substituted $u = cotx $ , giving me $$\frac{{du}}{{dx}} = -csc^2(x), du =  -csc^2(x)dx$$ therefore in $\int -\frac{{\csc^2(x)}}{{\cot(x)\csc^2(x) - \csc^2(x)}}\,dx$ by replacing $cotx$ with $u$ ,  I obtained, $$\int \frac{-1}{{u\left(u^2+1\right) - \left(u^2+1\right)}}\,du$$ $$\int (\frac{1}{{4\left(u+1\right)}} - \frac{1}{{4\left(u-1\right)}} - \frac{1}{{2\left(u-1\right)^2}})\,du$$ I would greatly appreciate any insights or techniques that could help me make progress with this integral. Are there specific strategies or mathematical tools that I should consider? Are there any useful properties or identities related to this type of expression? Thank you for your attention and assistance.","['integration', 'calculus', 'solution-verification', 'indefinite-integrals', 'trigonometry']"
4732007,Tangents from a point to two given circles that form equal angles with a line passing through that point,Line $p$ and circles $k$ and $l$ are given. Construct  point $T$ on line $p$ such that  tangents to circles $k$ and $l$ from  point $T$ form equal angles with line $p$ . How many solutions are there? I don't know why but I just drew a line $t$ parallel to $p$ passing through the center of circle $l$ . Then I found the perpendicular bisector of $IJ$ where $I=t\cap l$ and $J=t \cap k$ . I thought the interesction of that perpendicular bisector with the line $p$ would be point $T$ . But apparently it's not even tho it's close. Any hints would be appreciated. What I'm trying to achieve:,"['euclidean-geometry', 'geometric-construction', 'tangent-line', 'circles', 'geometry']"
