question_id,title,body,tags
2973198,"Proving that $\frac{D(a,b,c)}{D(x,y,z)}=1$","In deriving continuity equations using Lagrangian . We consider the element of fluid which occupied a rectangular parallelopiped having its centre at the point $(a,b,c)$ and its edges $\delta a$ , $\delta b$ , $\delta c $ parallel to the axes . At the time $t$ the same element for an oblique parallelepiped . The centre now has for its co-ordinates $x$ , $y$ , $z$ ; and the projections of the edges on the co-ordinate axes are respectively $$ \frac{\partial x}{\partial a} \delta a \ , \ \frac{\partial y}{\partial a} \delta a \ , \ \frac{\partial z}{\partial c} \delta a$$ $$\frac{\partial x}{\partial b} \delta b \ , \ \frac{\partial y}{\partial b} \delta b \ , \ \frac{\partial z}{\partial b} \delta b$$ $$\frac{\partial x}{\partial c} \delta c \ , \ \frac{\partial y}{\partial c} \delta c \ , \ \frac{\partial z}{\partial c} \delta c$$ How can i get these projections ?
The volume of the parallelepiped is therefore $$\begin{vmatrix}
\frac{\partial x}{\partial a} & \frac{\partial y}{\partial a} & \frac{\partial z}{\partial a} \\ 
\frac{\partial x}{\partial b} & \frac{\partial y}{\partial b} & \frac{\partial z}{\partial b} \\ 
\frac{\partial x}{\partial c} & \frac{\partial y}{\partial c} & \frac{\partial z}{\partial c}
\end{vmatrix} \delta a \delta b \delta c$$ or as its often written $$\frac{D(x,y,z)}{D(a,b,c)} \delta a \delta b \delta c$$ since the fluid mass is unchanged and the fluid is incompressible we have $$\frac{D(x,y,z)}{D(a,b,c)} =1$$ Is there a way to prove that $$\frac{D(a,b,c)}{D(x,y,z)}= 1$$ without expanding the determinant ?","['multivariable-calculus', 'derivatives', 'vector-spaces', 'fluid-dynamics']"
2973209,Are there products in the category of $\sigma$-algebras and (reversed) $\sigma$-homomorphisms?,"Let $\mathcal{A}$ be a $\sigma$ -algebra on a set $X$ , and $\mathcal{B}$ be a $\sigma$ -algebra on a set $Y$ . A map $h\colon\mathcal{B}\to\mathcal{A}$ is called a $\sigma$ -homomorphism if $h(\emptyset)=\emptyset$ , $h(Y\setminus B)=X\setminus h(B)$ for any $B\in\mathcal{B}$ , and $h\big(\bigcup B_n\big)=\bigcup h(B_n)$ for any sequence $\{B_n\}_{n\in\omega}$ in $\mathcal{B}$ . Let us consider a category where objects are pairs of the form $(X,\mathcal{A})$ , $\mathcal{A}$ being a $\sigma$ -algebra on $X$ , and morphisms $(X,\mathcal{A})\to(Y,\mathcal{B})$ are $\sigma$ -homomorphisms $h\colon\mathcal{B}\to\mathcal{A}$ . Note that the arrows are reversed. Question: Are there products in this category? Note: Original version of this question confusingly uses the term ""measurable space"" for the objects of the category in concern. The question is now rewrtitten to make clear the difference. There is a standard notion of ""the category of measurable spaces"" where objects are the same pairs as above and morphisms $(X,\mathcal{A})\to(Y,\mathcal{B})$ are measurable maps $f\colon X\to Y$ . Measurability means that $f^{-1}[B]\in\mathcal{A}$ for any set $B\in\mathcal{B}$ ; it is easy to check that then $f^{-1}\colon\mathcal{B}\to\mathcal{A}$ is $\sigma$ -homomorphism. However, not all $\sigma$ -homomorphisms are of this form; see here . So our category has the same objects but more morphisms than the standard category of measurable spaces. A natural candidate for a product of $(X,\mathcal{A})$ and $(Y,\mathcal{B})$ in our category of $\sigma$ -algebras is $(X\times Y,\mathcal{A}\times\mathcal{B})$ , where $\mathcal{A}\times\mathcal{B}\,$ is the $\sigma$ -algebra generated by rectangles of the form $A\times B\,$ for $A\in\mathcal{A}$ and $B\in\mathcal{B}$ , together with $\sigma$ -homomorphisms $f_1\colon\mathcal{A}\to\mathcal{A}\times\mathcal{B}$ and $f_2\colon\mathcal{B}\to\mathcal{A}\times\mathcal{B}$ defined by $f_1(A)=A\times Y$ and $f_2(B)=X\times B$ . Taking another $\sigma$ -algebra $\mathcal{C}$ and $\sigma$ -homomorphisms $g_1\colon\mathcal{A}\to\mathcal{C}$ , $g_2\colon\mathcal{B}\to\mathcal{C}$ , we have to prove that there exists a unique $\sigma$ -homomorphism $h\colon\mathcal{A}\times\mathcal{B}\to\mathcal{C}$ such that $g_1=h\circ f_1$ and $g_2=h\circ f_2$ . We could define $h(A\times B)=g_1(A)\cap g_2(B)$ and then try to extend the definition of $h$ to the whole $\sigma$ -algebra $\mathcal{A}\times\mathcal{B}$ . Is this approach working? How one can prove the existence and the uniqueness of $h$ ?","['boolean-algebra', 'measure-theory', 'category-theory']"
2973293,Perimeter of an equilateral triangle drawn with respect to a square.,"Here's a question that was asked in the International Kangaroo Math Contest 2016 . The question goes like this: If the perimeter of the square in the figure is 4 units, then what is the perimeter of the equilateral triangle? What I did: Well I tried something very na√Øve and it was the supposition that the equilateral triangle cuts the top side of the square at its midpoint. Hence giving the following result. By Pythagoras' Theorem, $$\overline{AB} = \overline{MC} = \sqrt{\overline{BC}^2 + \overline{BM}^2} = \sqrt{\left(1\right)^2 + \left(\frac12\right)^2} = \frac{\sqrt5}{2}$$ So perimeter of the triangle is: $$\begin{align}
P&=\overline{AF} +\overline{FM}+\overline{MC}+\overline{CD}+\overline{DE}+\overline{EA}\\
&= \frac12+\frac12+\frac{\sqrt5}2+1+\frac12+\frac{\sqrt5}2\\
&= \frac52+\sqrt5
\end{align}$$ However this is not the correct answer and I know that the problem is with the supposition that $M$ is the midpoint of $\overline{AB}$ . So what is the correct method and answer? Thanks for the attention.","['analytic-geometry', 'geometry']"
2973319,Random Game Using Real Numbers,"The original question: Alice and Bob play a game. To win the game, Alice needs $a$ points, and Bob needs $1$ point ( $a$ is fixed for each game). In each round of the game, Alice picks a real number $x$ . Then a coin is flipped, and Alice is awarded $x$ points if the coin comes up heads, and Bob is awarded $x$ points if the coin comes up tails. Assume Alice always plays optimally. Your task is to find the probability that Alice wins given she is using the best strategy. This problem is a little difficult to formalize, so I will offer the following formalization. First, note that at the beginning of any round, we can normalize the game so that Bob needs exactly $1$ more point. We will define a strategy $s:\mathbb{R^+}\to\mathbb{R^+}$ as a function that takes as an input the number of points Alice needs, and outputs her choice $x$ . Define $p_s(a)$ to be the probability that Alice wins the game needing $a$ points and playing strategy $s$ . Let $S$ be the set of all strategies. Find $$p, p(a)=\sup_{s \in S}p_s(a).$$ First, we note that $p(a) = 1$ if $a = 1 - \epsilon < 1$ , as Alice can choose a sufficiently small $x$ and play it repeatedly until she wins. I have included a formal proof below. Second, we note that $p(1)$ still equals $1$ . Pick $\epsilon, 2\epsilon, 4\epsilon, 8\epsilon, \dots$ until she has won one of these. For sufficiently small $\epsilon$ , this will always happen before she is defeated by Bob. At this point, she is ahead of Bob by $\epsilon$ , and the argument for $a < 1$ applies. Again, I have included a formal proof below Clearly this also imples that $\lim_{a \to 1^+}p(a)=1$ . These are the easy cases. Now intuition suggests that $\lim_{a \to \infty}p(a)=\frac 12$ (she is forced to choose $x=a$ when $a$ is sufficiently large). I have not tried to prove this, but it does not seem too difficult. Finally, I can show that $p(2) \geq \frac 34$ . This is easily done by first having Alice choose $x=1-\epsilon$ for sufficiently small $\epsilon$ . If she wins the points, invoke the $\lim_{a \to 1^+}$ case to win with probability $1$ , and if not then choose $x=2$ to win with probability $\frac 12$ . This strategy can be expanded to any $n \in \mathbb{N}$ by simply picking $1-\epsilon$ repeatedly, $n-1$ times. This shows that $p(n) \geq \frac 12 + 2^{-n+1}$ for natural numbers $n$ . Question: Can this bound be improved, shown to be tight, or extended to all real numbers? Proof for a < 1: Consider the game in which Bob needs $1$ point and Alice needs $1-\epsilon, \epsilon > 0$ points. We will show that for every $\delta > 0$ , there exists a strategy that allows Alice to win with probability at least $1-\delta$ . The strategy is simple: Alice will repeatedly choose $x=k$ , where $k=\frac{\epsilon^2}{8(2-\epsilon)\log \delta}$ . We note that once $2-\epsilon$ points have been awarded, one of the two players must have won. As a result, one of the two players will always have won after $\frac{2-\epsilon}{k}$ rounds have been played. Let $X_i, 1 \leq i \leq \frac{2-\epsilon}{k}$ be the indicator random variable that is $1$ if Bob wins round $i$ and $0$ otherwise, and let $X=\sum X_i$ . Then clearly Bob is awarded a total of $Xk$ points, and so he wins only if $X \geq \frac 1k.$ Then, $$\begin{aligned}
\Pr [\text{Bob wins}] =& \text{Pr }\left[X \geq \frac 1k\right] \\
=& \Pr \left[X \geq \frac 1k - \frac \epsilon {2k} + \frac \epsilon {2k}\right] \\
=& \Pr \left[X \geq \mathbb{E} [X] + \frac{\epsilon}{2k}\right] \\
=& \Pr \left[X \geq \mathbb{E} [X] \left(1+ \frac{\epsilon}{2-\epsilon}\right)\right] \\
\leq & \text{exp }\left(-\frac{2-\epsilon}{8k}\left(\frac{\epsilon}{2-\epsilon}\right)^2\right) \hspace{20 px} (*)\\
= & \delta
\end{aligned}$$ where $(*)$ is due to Chernoff Bounds. Proof for a=1: We will describe a set of strategies that allow Alice to win the game in which both she and Bob need $1$ point with probability $\geq 1-\delta$ for any $\delta > 0$ . Let $\delta ' = \frac \delta 2$ , and $r=\frac{\delta '}2$ . Alice will begin by choosing $x=r, 2r, 4r, 8r, \dots$ until either Bob wins the game, or until Alice wins any of these coin flips. It is easy to verify algebraically that the probability that Bob wins in this stage is $\leq \delta '$ . Once Alice wins any coin flip, she is ahead of Bob by exactly $r$ points. At this point, she can abandon her current betting scheme, normalize Bob's points and play a strategy for $a<1$ that gives her a win chance of $1-\delta '$ . By union bound, the probability of Bob winning is then $\leq 2\delta ' = \delta$ , as desired.","['game-theory', 'statistics', 'probability']"
2973358,The number of Hamiltonian paths in a tournament is always odd?,"A tournament is defined as an orientation of a complete graph. Prove or disprove the following statement: In a tournament, there are exactly an odd number of Hamiltonian paths. In all cases I‚Äôve tried, the statement seems true, so I guess it‚Äôs always true. But to prove it, I really got confused. Two ideas have come up but get nowhere. Please help. Induction on number of vertices; Show that changing the direction of one edge won‚Äôt change the parity.","['graph-theory', 'discrete-mathematics', 'hamiltonian-path']"
2973381,Cardinality of all non-increasing functions from $\mathbb{N}$ to $\mathbb{N}$,"Let $g:\mathbb N\to\mathbb N$ be a non-increasing function
(which means $x\leq y\implies g(x)\geq g(y)\ \forall x,y\in \mathbb N$ ). I'm interested in the cardinality of the set which contains all such functions like $g$ . My educated guess would be that this set is countable, but how can I prove it?","['elementary-set-theory', 'integers', 'real-analysis']"
2973390,"If $f(x)$ is a twice differentiable function and given that $f(1) = 2, f(2) = 5$ and $f(3) = 10$ then","Q. If $f(x)$ is a twice differentiable function and given that $f(1) = 2, f(2) = 5$ and $f(3) = 10$ then (A) $f''(x) = 2$ for all $x\in (1,3)$ (B) $f''(x) = f'(x) = 2$ for some $x\in (2,3)$ (C) $f''(x) = 3$ for all $x\in (2,3)$ (D) $f''(x) = 2$ for some $x\in (1,3)$ I assumed $$f(x) = ax^2 + bx + c$$ and compared and found $a,b,c$ . That way, I get $f''(x) = 2$ for all $x$ . But, seems like that's not how it is done. I am aware that it could be a thrce differentiable function or a trigonometric one. But, I thought that a general case should satisfy it and hence I could atleast deduce the answer. Why is it wrong? What should I do?","['calculus', 'functions', 'derivatives']"
2973511,"Cantor's diagonalization argument - define second sequence, that is also not in the list.","I now how the standard construction of a sequence which can not be in the list, but how can i construction  another sequence which is not in the list ? 
More generally, is it possible to list all sequence which are not in the list ( of course their are infinitely many).",['elementary-set-theory']
2973531,A holomorphic map between complex tori (proposition 1.3.2 in Diamond‚ÄìShurman),"The following is from Diamond and Shurman's A First Course in Modular Forms book: I had studied Munkres Topology a few years ago but for lifting I had to review the materials again, but I still don't see the connection to this theorem! 
Especially, could someone at easiest possible way explain why $f_{\lambda}$ becoming constant? And, how $m \Lambda \subset \Lambda'$ holds? (They are underlined blue in the image above)","['modular-forms', 'general-topology', 'holomorphic-functions', 'integer-lattices']"
2973572,Let $f:\mathbb{R}\to \mathbb{R}$ be a $C^1$ function such that $f(n)=0$ for all $n \in \mathbb{Z}$. Solutions of $x'=f(x)$.,I'm trying to do this exercise: Let $f:\mathbb{R}\to \mathbb{R}$ be a $C^1$ function such that $f(n)=0$ for all $n \in \mathbb{Z}$ . Prove that the maximal solutions of $x'(t)=f(x(t))$ are defined and bounded for all $t \in\mathbb{R}$ . I really don't know how to attack this problem. I'd appreciate any hint. Thanks for your time.,"['calculus', 'ordinary-differential-equations']"
2973575,Divergence of the solution.,"I have two functions $f,g:\mathbb{R}\longrightarrow{\mathbb{R}}$ , both continuous and with $g$ bounded. I also have the following Cauchy problem $$
\begin{cases}x'=f(t)g(x)\\x(t_0)=x_0
\end{cases}
$$ If $\phi$ is the solution of the system defined in $(-\infty, b)$ , I have $$
\lim_{t \to{+}b}{\phi'(t)=f(t)g(\phi(t))=f(b)k}
$$ with $k \geq g(\phi(t))$ since g is bounded. And now the question: in my notes it claimed that $\phi(t)$ goes to $\infty$ in $b$ since is not defined there. Is it true? I don't see why $\phi \to \infty$ in $b$ . I think it is simply not defined but I don't see why it should go to infinity.
Showing this I would get a contradiction since $\phi(t)'$ is bounded, hence proving that $\phi$ is defined in $\mathbb{R}$ Regards. Edit ------- Original problem: Let $f,g:\mathbb{R}\longrightarrow{\mathbb{R}}$ , both continuous and with $g$ bounded. Prove that for all $(t_0,x_0)\in \mathbb{R^2}$ , the maximal solutions of the Cauchy problem $$
\begin{cases}x'=f(t)g(x)\\x(t_0)=x_0
\end{cases}
$$ are defined for all $\mathbb{R}$ . The attempt of solution is done by contradiction; supposing $\phi : (-\infty,b) \to \mathbb{R}$ is a solution, we want to show the derivative of $\phi$ is bounded (as I noted at the begining of this post) while $\phi$ goes to $\infty$ in $b$ . This last part is what I don't get. I don't see why is going to $\infty$ .","['ordinary-differential-equations', 'real-analysis']"
2973577,Backpropagation with two hidden layers - matrix dimension doesn't add up,"I'm currently trying to create a neural network with 2 hidden layers from scratch. The input layer has 784 dimensions (MNIST dataset) The first hidden layer has 100 neurons using sigmoid activator The second hidden layer has 10 neurons using sigmoid activator The output layer has 10 possible outcomes (digit 0-9) using softmax activator I easily computed the output (third) layer derivative with respect to the final (third) weight matrix: $$\frac{\partial L}{\partial W_3} = \frac{\partial L}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial z_3}  \frac{\partial z_3}{\partial W_3} = [1\ H_2]^T(\hat{y} - y)$$ $\frac{\partial L}{\partial z_3} = (\hat{y} - y)$ is the partial derivative of the loss function with respect to the softmax input. $\frac{\partial z3}{\partial z_3} = \frac{\partial}{\partial z_3} [1\ H_2]W_3 = [1\ H_2]^T$ is the partial derivative of the softmax input with respect to the weight matrix. I checked the dimension, and since $[1\ H_2]^T \in \mathbb{R}^{11\times1}$ and $(\hat{y} - y) \in \mathbb{R}^{1\times10}$ , $\frac{\partial L}{\partial W_3}\in \mathbb{R}^{11\times10}$ , which has the same dimension as $W_3$ . Thus, it would be possible to perform gradient descent. Next up, I want to compute the derivative of loss function with respect to the second layer's weight matrix: $$\frac{\partial L}{\partial W_2} = \frac{\partial L}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial z_3}  \frac{\partial z_3}{\partial H_2} \frac{\partial H_2}{\partial z_2} \frac{\partial z_2}{\partial W_2}$$ $\frac{\partial L}{\partial z_3} = (\hat{y} - y) \in \mathbb{R}^{1\times10}$ $\frac{\partial z_3}{\partial H_2} = \frac{\partial}{\partial H_2} [1\ H_2]W_3=W_3^T \in \mathbb{R}^{10\times10}$ $\frac{\partial H_2}{\partial z_2} = \frac{\partial}{\partial z_2} \sigma(z_2)=\sigma(z_2)(1-\sigma(z_2))=H_2(1-H_2) \in \mathbb{R}^{10\times1}$ $\frac{\partial z_2}{\partial W_2} = \frac{\partial}{\partial W_2} [1\ H_1]W_2=[1\ H_1] \in \mathbb{R}^{1\times101}$ I can't merge these derivatives using chain rule into $Dim(W_2) \in \mathbb{R}^{101\times10}$ no matter what, which is needed for gradient descent. I feel like I'm missing something in my $\frac{\partial L}{\partial W_2}$ . Could someone please give me some insights?","['derivatives', 'calculus', 'linear-algebra', 'neural-networks']"
2973590,Compute $E[ \sqrt{Q} e^{-tQ} ]$ where $Q$ is non-central chi-square random variable.,"Let $Q$ be non-central chi-square random variable of degree $k$ and centrality parameter $\mu$ . How to compute  the following expectation \begin{align}
E\left[ \sqrt{Q} e^{-tQ} \right], t>0 
\end{align} I was thinking this can be computed by using the fact that $Q= \sqrt{ \sum_{i=1}^k Z_i^2 }$ where $Z_i \sim \mathcal{N}(\mu_i,1)$ and $\mu= \sqrt{\sum_{i=1}^k \mu_i^2}$ , and $Z_i$ 's are independent. Using this observation we get that \begin{align}
E\left[ \sqrt{\sum_{i=1}^k Z_i^2 } e^{-t\sum_{i=1}^k Z_i^2 } \right]= E\left[ \sqrt{\sum_{i=1}^k Z_i^2 }  \prod_{i=1}^k  e^{-tZ_i^2 } \right].
\end{align} However, it is not clear to me how such exressions can be  manipulated or expanded. I also tried direct integration but expression seem very long, so I stopped.","['expected-value', 'probability-theory']"
2973594,"Prove that $\liminf \frac{M_n}{\sqrt{2\log n}}\geq 1$, where $M_n=\max_{1}^n X_i$ and $(X_i)$ is a sequence of i.i.d standard normal random variables","Question Let $(X_n)_{n\geq 1}$ be an i.i.d sequence of standard normals. Show that with probability one $\liminf \frac{M_n}{\sqrt{2\log n}}\geq 1$ , where $M_n=\max_{1}^n X_i$ . My attempt Given $\varepsilon >0$ , it suffices to show that $P(\frac{M_n}{\sqrt{2\log n}}<1-\varepsilon \quad \text{i.o.})= 0$ . To this end put $A_n=(\frac{M_n}{\sqrt{2\log n}}<1-\varepsilon)$ and we attempt to use the Borel-Cantelli Lemma. So $$
\sum_1 ^\infty P(A_n)=\sum_{1}^\infty P(X_1<c_n)^n=\sum_{1}^\infty (1-\bar{\Phi} (c_n))^n
$$ where $c_n=(1-\varepsilon )(\sqrt{2\log n})$ and $\bar{\Phi}=1-\Phi$ is the survival function. At this point since the normal distribution has no nice form for its cdf I have to use some argument involving asymptotics of this sum. To this end, I know that $$
\bar{\Phi}(x)\sim\frac{\phi (x)}{x} 
$$ as $x\to \infty$ where $\phi$ is the density of a standard normal. More precisely, $$
\left(\frac{1}{x}-\frac{1}{x^3}\right)\leq \frac{\bar{\Phi}(x)}{\phi(x)}\leq \frac{1}{x}.\tag{1}
$$ We can write $$
\sum_{1}^\infty P(X_1<c_n)^n\leq \sum_{1}^\infty(1-(c_n^{-1}-c_n^{-3})\phi(c_n))^n
$$ using $1$ , but I am not sure how to argue that this is finite.","['real-analysis', 'borel-cantelli-lemmas', 'convergence-divergence', 'probability-theory', 'probability']"
2973630,Finding value of this expression.,"If $f(x)$ is a differentiable function and $f''(x)<0 ~ \forall x \in
 \mathbb R$ then the value of: $$\left[\dfrac{
 \displaystyle\sum_{r=0}^{n-1}f\left(\frac r n\right)\dfrac 1n + 
 \displaystyle\sum_{r=1}^{n}f\left(\frac r n\right)\dfrac 1n }{2
\displaystyle\int_0^1 f(x)dx}\right] $$ is equal to ? (where $[\cdot]$ is the greatest integer function) My attempt I tried solving it for $n\rightarrow \infty$ and got the answer as 1. But the answer is given as 0. Also while discussing on this question with my fellow mates I heard the term 'Reimann sum' which I am clueless about. Please help me understanding what is Reimann sum? How does it answer this question? Is it possible to solve this without using the concept of Reimann sum?","['calculus', 'functions']"
2973638,$i^{1/n}$ when $n \to \infty$,What would happen when $n \to \infty$ for $i^{1/n}$ . Would it still be an imaginary number or would it simply be $i^0=1$ ?,"['calculus', 'complex-numbers']"
2973669,"Proving that a sequence $a_n: n\in\mathbb{N}$ is (not) monotonic, bounded and converging","$$a_n = \left(\dfrac{n^2+3}{(n+1)^2}\right)\text{ with } \forall n\in \mathbb{N}$$ $(0\in\mathbb{N})$ Monotonicity: To prove, that a sequence is monotonic, I can use the following inequalities: \begin{align}
a_n \leq a_{n+1}; a_n < a_{n+1}\\
a_n \geq a_{n+1}; a_n > a_{n+1}
\end{align} I inserted some $n$ 's to get an idea on how the sequence is going to look like. I got: \begin{align}
a_0&=3\\
a_1&=1\\
a_2&=\frac{7}{9}\approx 0.\overline{7}\\
a_3&=\frac{3}{4}=0.75
\end{align} Assumption: The sequence is monotonic for $\forall n\in \mathbb{N}$ Therefore, I show that \begin{align}
a_n \leq a_{n+1}; a_n < a_{n+1}\\
a_n \geq a_{n+1}; a_n > a_{n+1}
\end{align} I am having problems when trying to prove the inequalities above: \begin{align}
& a_n \geq a_{n+1}\Longleftrightarrow \left|\frac{a_{n+1}}{a_n}\right |\leq 1\\
& = \left|\dfrac{\dfrac{(n+1)^2+3}{(n+2)^2}}{\dfrac{n^2+3}{(n+1)^2}}\right|\\
& = \frac{4 + 10 n + 9 n^2 + 4 n^3 + n^4}{12 + 12 n + 7 n^2 + 4 n^3 + n^4}\\
& = \cdots \text{ not sure what steps I could do now}
\end{align} Boundedness: The upper bound with $a_n<s_o;\; s_o \in \mathbb{N}$ is obviously the first number of $\mathbb{N}$ : \begin{align}
a_0=s_o&=\frac{0^2+3}{(0+1)^2}\\
&=3
\end{align} The lower bound $a_n>s_u;\; s_u \in \mathbb{N}$ $s_u$ should be $1$ , because ${n^2+3}$ will expand similar to ${n^2+2n+1}$ when approaching infinity. I don't know how to prove that formally. Convergence Assumption (s.a) $\lim_{ n \to \infty} a_n =1$ Let $\varepsilon$ contain some value, so that $\forall \varepsilon > 0\, \exists N\in\mathbb{N}\, \forall n\ge N: |a_n-a| < \varepsilon$ : \begin{align}
\mid a_n -a\mid&=\left|\frac{n^2+3}{(n+1)^2}-1\right|\\
&= \left|\frac{n^2+3}{(n+1)^2}-\left(\frac{n+1}{n+1}\right)^2\right|\\
&= \left|\frac{n^2+3-(n+1)^2}{(n+1)^2}\right|\\
&= \left|\frac{n^2+3-(n^2+2n+1)}{(n+1)^2}\right|\\
&= \left|\frac{2-2n}{(n+1)^2}\right|\\
&= \cdots \text{(how to go on?)}
\end{align}","['analysis', 'real-analysis', 'upper-lower-bounds', 'sequences-and-series', 'convergence-divergence']"
2973689,Types of group action observing the table of $G$ acting in $X$,"I'm studying the group action of a group $G$ on a set $X=\{1,\dots,n\}$ For example taking a straightforward example with $G \leq S_4$ : $$X = \{1,2,3,4\} \quad G=\{e_4,(1,2),(3,4),(1,2)(3,4)\}$$ We can construct a group multiplication table based on the action of $G$ in $X$ to determine the group action type. $$\begin{array}{|l|llll|}
\hline
& 1 & 2 & 3 & 4 \\ \hline
\sigma_1 & 1 & 2 & 3 & 4 \\ 
\sigma_2 & 2 & 1 & 3 & 4 \\
\sigma_3 & 1 & 2 & 4 & 3 \\
\sigma_4 & 2 & 1 & 4 & 3 \\ \hline
\end{array}$$ At first glance is not transitive since $\sigma.x\neq y \quad x,y\in X$ for example $\nexists \sigma\in G$ such that $\sigma \cdot 1 = \sigma(1) = 4$ . It is faithful: given $\sigma,\rho \in G$ there exists $x$ such that $\sigma\cdot x \neq \rho\cdot x$ , formally, every row is different by at least one element. It is not free, so it doesn't satisfy $\sigma \cdot x = \rho \cdot x \iff \sigma = \rho = e$ . For example $\sigma_2(1) = \sigma_4(1)$ breaks the rule. Summarizing , I want a general understanding of group action types. It seems that types of group actions follow a pattern if we observe the table of the action of $G$ on $X$ . Transitive: each column has each $x\in X$ once. Faithful: each row is different by at least one element. Free: each row is entirely different to others. (As a consequence every permutation is a derangement since it has no fixed points) Regular: All the listed above, so the table forms a symmetric matrix. I can give you a group with regular action, so the table follows the aforementioned criteria: $$G=\{e_4, (1,2)(3,4),(1,3)(2,4),(1,4)(2,3)\}$$ $$\begin{array}{|l|llll|}
\hline
& 1 & 2 & 3 & 4 \\ \hline
\sigma_1 & 1 & 2 & 3 & 4 \\ 
\sigma_2 & 2 & 1 & 4 & 3 \\
\sigma_3 & 3 & 4 & 1 & 2 \\
\sigma_4 & 4 & 3 & 2 & 1 \\ \hline
\end{array}$$ As you can see it satisfied the criteria above. Q: What if $X=G$ ? Would be that an automorphism ( $G \times G \to G$ )? If it's the case, then do bijections from $G$ to $G$ give the corresponding permutation? Q:All the groups above were subgroups of the symmetric group on n symbols. If that wasn't the case, how would an example look like? Q: Can we determine the action type by observing if the properties are satisfied in the table? Q: Is there any alternative method to identify the group action of a G-set?","['proof-verification', 'abstract-algebra', 'symmetric-groups', 'group-theory', 'group-actions']"
2973709,Boundary Trace in Half Space,"Let $\Omega$ denote the upper half-plane in $\mathbb{R}^2$ , i.e. $$
\Omega = \left\{ (x_1,x_2) : x_2 > 0\right\}. 
$$ Is it possible to find a function in $H^1_0(\Omega)$ whose boundary trace vanishes in $L^2(\partial \Omega)$ but not pointwise? I saw this question in a set of notes and, honestly, I'm not even sure how to understand the question itself. Functions in $H^1(\Omega)$ are actually equivalence classes of functions equal almost everywhere. So how does one even go about discussing pointwise convergence of the trace? Edit It turns out that one can find a function $u \in H_0^1(\Omega)\cap C(\Omega)$ whose boundary trace vanishes in $L^2(\partial \Omega)$ , with the property that $$
\lim_{\substack{x \to x_0\\ x \in \Omega}} u(x) \neq 0
$$ for at least one point $x_0 \in \partial\Omega$ . Still, I do not see how to construct such a function $u$ .","['sobolev-spaces', 'functional-analysis', 'trace-map']"
2973710,Reference request: arranging distinct numbers into a full rank matrix,"I thought of the following problem: Let $n\ge 2$ .  Suppose you have $n^2$ distinct numbers in some field. 
  Is it necessarily possible to arrange the numbers into an $n\times n$ matrix of full rank (ie, nonsingular or invertible)? (I am able to solve the problem, for example using the combinatorial nullstellensatz .) I was wondering whether this problem was previous stated elsewhere, perhaps even on this site? My original motivation for the problem was in fact quite similar to this question , but I was rearranging the primes.","['matrices', 'linear-algebra', 'reference-request']"
2973720,Derivative of polynomial roots with respect to a parameter,"Consider the polynomial $$p(s;\tau,a)=3(56-56a\tau^2+a^2\tau^4)-112(-6+a\tau^2)\tau s-14(-36+a\tau^2)\tau^2s^2+112\tau^3s^3+7\tau^4 s^4,$$ I want to examine that how the roots of $p$ change with respect to the parameter $\tau$ . I determined the derivative of a root with respect to $\tau$ using $$ \frac{\partial s}{\partial \tau}(s_i;\tau,a)=- \frac{ \frac{\partial p}{\partial \tau}(s_i;\tau,a)}{ \frac{\partial p}{\partial s}(s_i;\tau,a)},$$ where $s_i$ is one of the roots of $p$ . Is it correct to define the derivative $\frac{\partial s}{\partial \tau}$ ? And can we use the implicit function derivative formula?","['parametric', 'implicit-differentiation', 'derivatives', 'polynomials']"
2973722,Greatest Integer Function Linear Equality.,Number of natural numbers satisfying the equation $$\left[\dfrac{x}{49}\right]=\left[\dfrac{x}{51}\right]$$ (where $[\cdot]$ denotes Greatest Integer Function)?,"['functions', 'linear-algebra']"
2973741,"Given an anti-chain of the powerset of a finite set, is there a disjoint maximal chain?","Denote by $X^{(r)}$ the subset of $\mathcal{P}(X)$ such that every element has cardinality $r$ where $ 1 \leq r \leq n = |X|$ Suppose we have an antichain $\mathcal{A}$ of the partial ordering of $\mathcal{P}(X)$ by inclusion, that is not of the form $X^{(r)}$ for some $r$ . Must there exist then a maximal chain that is disjoint from $\mathcal{A}$ ? I am very stuck on this, however I suspect that it is true. I have a two ideas of how one might go about showing this that I can't seem to complete: For contraposition, we supposed that for an antichain $\mathcal{A}$ we do not have any maximal chains that are disjoint from it. We want to show that $\mathcal{A}$ is then $X^{(r)}$ for some $r$ . I've thought about considering the set $C$ of chains that are disjoint from $\mathcal{A}$ , and considering the subset $C' \subset C$ containing all chains of length maximal in $C$ . Then we can look at $\mathcal{B} = \{c \in X^{(|C'|)} \mid \exists A \in C' \text{ such that } c \in A\}$ . This is the subset of $X^{(|C'|)}$ that contains all the last elements of the chains in $C'$ . My thinking was to either show that $|C'| = n$ or that you can extend some chain in $C'$ , contradicting maximality. However, I can't think of a good way to do either of these things. For contradiction, suppose we have $\mathcal{A}$ an antichain not of the form $X^{(r)}$ . Then I wanted to show that there exists a maximal chain that is disjoint from $\mathcal{A}$ . Consider then $\mathcal{A}_r = \mathcal{A} \cap X^{(r)}$ . Then we know that $\mathcal{A}_r$ is strictly contained in $X^{(r)}$ for every $r$ . I want to then construct a maximal chain through the gaps in every layer $X^{(r)}$ that is not occupied by $\mathcal{A}$ . However, I am also unsure how I might go about proving this. I wanted to ask if I am on the right track and how I might be able to finish these proofs off. More importantly, is my suspicion that this result is true correct, or is there a counter example that I didn't find?",['combinatorics']
2973819,A magic trick - find out the fifth card if four is given,"Here is a magic trick I saw. My question is how the magician and his partner did it. Given the simple French deck of cards, with $52$ cards. A person from the audience chooses randomly five cards from the deck and gives it to the partner (the partner works with the magician), without showing it to the magician. Then the partner (who sees the five cards) chooses four cards from the five cards, and gives it to the magician one by one (so the order of the four cards matters). From that the magician knows the fifth card. The partner and the magician can‚Äôt communicate during the trick. How did they do it? I thought that amoung the five cards there will be two with the same sign (Spades,Hearts,Diamonds or Clubs) and one of these two cards will be the fifth, and the other will be the first card to give to the magician...","['contest-math', 'graph-theory', 'matching-theory', 'combinatorics', 'discrete-mathematics']"
2973843,Prove $\frac{d}{dx}(e^x)=e^x$ with Taylor series and power rule,"Prove $\dfrac{d}{dx}\left(e^x\right)=e^x$ with Taylor series and power rule. So far I have $$1+x+\frac{1}{2}x^2+\frac{1}{3\cdot2}x^3+\cdots+\frac{1}{\infty-1}x^\infty$$ note: Also that is what I got after I took the derivative of the Taylor series, fyi for those who are telling me to take the derivative of the Taylor series. Basically, it goes through an infinite regression. Thence I applied the power rule upon the Taylor series of $e^x$ . I'm pretty sure I'm supposed to use the binomial theorem to get $(1+\frac{x}{n})^n$ ; however $n$ in this case is $\infty$ , and thus I cannot rewrite the series. Help s'il vous pla√Æt. Merci.","['calculus', 'derivatives', 'taylor-expansion']"
2973866,Can one explicitly solve the shifted harmonic oscillator,"The quantum harmonic oscillator is described by a Hamiltonian $$H=-\Delta + \left\lvert x \right\rvert^2.$$ By decomposing the eigenvalue problem $H\psi= E\psi$ into its angular and radial part one obtains essentially the ODE $$-\varphi''(r) + \frac{2}{r} \varphi'(r) + \frac{l(l+1)}{r^2} \varphi(r)+r^2\varphi(r)=E\varphi(r).$$ Please let me know if you have any questions. This ODE can be solved explicitly, i.e. one obtains the usual eigenfunctions and eigenvalues I wonder whether one can also solve the shifted version $$-\varphi''(r) + \frac{2}{r} \varphi'(r) + \frac{l(l+1)}{r^2} \varphi(r)+(r-r_0)^2\varphi(r)=E\varphi(r)$$ for some $r_0>0.$ It does not seem possible to map this one immediately back to the original one that's why I am asking. Please let me know if you have questions or remarks.","['ordinary-differential-equations', 'analysis', 'real-analysis', 'functional-analysis', 'mathematical-physics']"
2973884,Prove that $\mathbb{Q}_+$ can be enumerated as $(q_n)$ such that $\lim\sqrt[n]{q_n}$ exists.,"Prove that the set of positive rationals can be enumerated as $(q_n),$ such that $\lim\sqrt[n]{q_n}$ exists. Comment. I don't know if I should be looking for a certain ""formula"" on $q_n's$ , or a way, a method to enumerate the positive rationals, in order to get the desired result. Thanks for the help!","['limits', 'calculus', 'rational-numbers', 'sequences-and-series']"
2973886,Justification for notation of line integrals,"I am having trouble with notation, specifically how certain substitutions are rigorously made when going back and forth between a line or contour integral and a definite integral. For example if: $$
\phi(\tau) = t \\ \alpha \leq \tau \leq \beta
$$ then: $$
\int_a^bw(t)dt = \int_{\alpha}^{\beta}w(\phi(\tau))\phi'(\tau)d\tau
$$ It appears that $dt = \phi(\tau)d\tau$ But I always thought $dt$ was just a symbol that told us which variable we are integrating with respect to. So it seems very wishy-washy to make this substitution. Question: How can I understand this rigorously enough that I don't feel guilty making the substitution when I am working problems?
Also, how do I justify the change of variables on the bounds? To clarify my question, I am seeking a rigorous algebraic explanation. I vaguely understand the larger idea behind the paramaterization, but I want to see all the substitutions made rigorously, step by step with no hand waving. This seems to have been neglected in my classes, and it's killing me.","['complex-analysis', 'multivariable-calculus']"
2973889,A compact space whose proper compact subspaces are finite,A recent deleted question asked: Are there infinite compact spaces whose only compact subsets are finite or itself?,"['general-topology', 'compactness']"
2973895,Square root of $-I_2$,"I would like to get all matrices $N \in M_2(\mathbb R)$ such that $N^2 = -I_2$ . To start with, I know that $N_0=\begin{bmatrix}
0 & -1 \\
1 & 0
\end{bmatrix}$ works, and we can prove that every matrices $N$ that are similar to $N_0$ work. $i.e.$ Let $N \in M_2(\mathbb R)$ if $\exists P \in \mathrm{GL}_2(\mathbb R)$ such that $N = PN_0P^{-1}$ , then $N^2 = -I_2$ . My question is, is the converse true? Are all matrices $N \in M_2(\mathbb R)$ such that $N^2 = -I_2$ similar to $N_0$ ?","['matrices', 'linear-algebra']"
2973978,"Proof that if $a \gt 1$ and $r \gt s$, then $a^{r} \gt a^{s}$","I'm trying to prove the sentence above but I'm not getting results, I'm learning precalculus yet. Could someone help me find the starting point to prove the sentence above?",['algebra-precalculus']
2974052,Characterization of finite cyclic totally ramified extension of local fields with prime power degree,"Definition Let $G_K$ be the absolute Galois group of a local field $K$ . We will call a group homomorphism $\chi: G_K \to \mathbb{C}^*$ with finite image a character on $K$ . Since every finite subgroup of $\mathbb{C}^*$ is cyclic, it is generated by a primitive root of unity. So in our case, every character $\chi$ corresponds to a unique cyclic Galois extension $F/K$ of degree $n$ , the cardinality of the image of $\chi$ , and an isomorphism $$\bar{\chi}: \operatorname{Gal}(F/K) \xrightarrow{\sim} \langle \xi_n \rangle \subseteq \mathbb{C}^*$$ where $\xi_n$ is a primitive $n$ -th root of unity. Let $\chi$ be a character which is induced by a cyclic Galois extension $F/K$ with prime power degree. Furthermore, let $\operatorname{Frob}_K$ be a Frobenius element in $G_K$ , i.e. an element whose image is $x \mapsto x^{|\kappa(K)|}$ under the restriction homomorphism $G_K \to G_{\kappa(K)}$ where $\kappa(K)$ is the residue field of $K$ . By $F^{nr}$ , we denote the maximal unramified subextension of $F/K$ . I want to show that the following statements are equivalent: $F/K$ is totally ramified, $F^{nr} = K$ , the image of $\operatorname{Frob}_K$ in $\operatorname{Gal}(F/K)$ is the identity element, $\chi(\operatorname{Frob}_K)=1$ . I think I was able to show the equivalences 1. $\Leftrightarrow$ 2. and 3. $\Leftrightarrow$ 4., so I am interested in the characterization from 1./2. to 3./4 and vice versa. Ideas : $\bar{\chi}$ is injective on $\operatorname{Gal}(F/K)$ . The image of $\operatorname{Frob}_K$ under $G_K \to \operatorname{Gal}(F/K) \simeq G_K/\operatorname{Gal}(\bar{K}/F)$ is the Frobenius element in $\operatorname{Gal}(F/K)$ (Usually, a Frobenius element is unique up to conjugacy. But since $F/K$ is cyclic, it is unique indeed.). It should be the generator of $\operatorname{Gal}(F/K)$ . The inertia subgroup $I_{F/K}$ of $\operatorname{Gal}(F/K)$ is the unique cyclic subgroup of order $e$ , the ramification index of $F/K$ . We can identify $\operatorname{Gal}(F/K)/I_{F/K}$ with $\operatorname{Gal}(F^{nr}/K)$ . A generator of this group is the image of $\operatorname{Frob}_K$ , I think. Could you please help me to establish the remaining connections? Thank you! Edit : I found a crucial mistake! $F/K$ must have prime power degree, otherwise the equivalences won't work!","['algebraic-number-theory', 'ramification', 'local-field', 'abstract-algebra', 'group-theory']"
2974062,Why exactly limit in polar coordinates isn't sufficient to find the limit in two variables?,"I'm currently facing the following problem, my math teacher told us that the following statement is true: $\lim_{x\to(a,b)} f(x)=L \iff \forall \theta \lim_{r\to0} f(a+r\cos\theta, b+r\sin\theta)=L$ And in an attempt to better understand why this is true, I researched here for related questions and attempted to prove this myself. But I failed in finding any proof and my research led me to believe that the statement is actually false. Here is a link to one of the questions: Limit $\frac{x^2y}{x^4+y^2}$ is found using polar coordinates but it is not supposed to exist. In this question, he shows what I think is a counterexample to the statement above: $$\lim_{(x,y)\to(0,0)}\frac{x^2y}{x^4+y^2}$$ in which using polar coordinates to find the limit returns $0$ regardless of the value of $\theta$ . But by making $y=x^2$ the limit is equal to $1/2$ . Therefore, the limit doesn't exists. The intuitive explanation for this, given in the accepted answer is that using polar coordinates only checks the limit in the directions of straight lines. But that explanation doesn't satisfy me at all, I still can't explain why checking the limit in every straight direction isn't sufficient. I feel that if I could come up with counterexample on my own that would mean I truly understand. So here are my first questions: Could you build your own counterexamples? And how would/did you go about doing this? Are there any counterexamples you know? Am I really correct in assuming that the statement is actually false? Since the polar limit checks in every straight direction, the following statement is true? $\forall y\in\mathbb{R}^2\lim_{t\to0} f((a,b)+ty)=L \iff \forall \theta \lim_{r\to0} f(a+r\cos\theta, b+r\sin\theta)=L$ Don't know if that is particularly useful.","['limits', 'multivariable-calculus', 'polar-coordinates', 'real-analysis']"
2974166,Trigonometric Identities: Given that $2\cos(3a)=\cos(a)$ find $\cos(2a)$,"Given that $2\cos(3a)=\cos(a)$ find $\cos(2a)$ . $2\cos(3a)=\cos(a)$ I converted $\cos(2a)$ into $\cos^2(a)-\sin^2(a)$ Then I tried plugging in. I know this is not right, but I have no clue how to solve this. Hints please? edit: 
Because I got that $\cos(2a) = 4\cos^2(3a)-1$","['trigonometry', 'proof-writing']"
2974178,How to evaluate the integrals in the cylindrical coordinates,"Evaluate the following integral in cylindrical coordinates $$\int^{1}_{-1}\int^{\sqrt{1-x^2}}_{0}\int^{2}_{0}\dfrac{1}{1+x^2+y^2}dzdydx$$ My try: I first took the boundaries as $$-1\le x\le1\\0\le y\le\sqrt{1-x^2}\\0\le z\le2$$ and I know the formula that $$D=\{(r,\theta,z):g(\theta)\le r\le h(\theta),\alpha\le\theta\le\beta,G(x,y)\le z\le H(x,y)\}$$ $$\int^{}_{}\int^{}_{D}\int^{}_{}f(r,\theta,z)dV=\int^{\beta}_{\alpha}\int^{h(\theta)}_{g(\theta)}\int^{H(r\cos\alpha,r\sin\theta)}_{G(r\cos\theta,r\sin\theta)}f(r,\theta,z)dzdrd\theta$$ But how to apply this formula and change the boundaries of the integrals? Can anyone please explain this.","['integration', 'cylindrical-coordinates', 'definite-integrals', 'multivariable-calculus', 'calculus']"
2974181,Polynomial Forms on Dual Projective Spaces,"Let $E$ be a finite dimensional complex vector space. Let $\mathbb{P}(E)$ be the projective space of lines through the origin of $E$ . Fulton, in his book ""Young Tableaux"", then defines the $\textit{dual projective space}$ to be the collection of all hyperplanes in $E$ , $\mathbb{P}^*(E)$ . He mentions that the motivation for this is that then $E$ then becomes the space of linear forms on $\mathbb{P}^{*}(E)$ , and, likewise, $\mathsf{Sym}_{\bullet}(E)$ is the algebra of polynomial forms on $\mathbb{P}^{*}(E)$ . My question is, how exactly does this work? That is, how is an element $v \in E$ a linear form on $\mathbb{P}^{*}(E)$ ?","['algebraic-geometry', 'linear-algebra', 'projective-space']"
2974223,Is weakly continuous function with values in Banach space strongly measurableÔºü,"Let $X$ be a non-separable Banach space and a map $f$ from real line into $X$ is called weakly continuous if it is continuous with respect to the weak topology of $X$ . A function with values in $X$ is called strongly measurable if it is weakly measurable and separably valued(Pettis theorem). It seems that a weakly continuous function is automatically weakly measurable. Is it also separably valued(i.e., its range lies in a closed separable subspace of $X$ )?","['measure-theory', 'measurable-functions']"
2974274,Functions - Odd numbers and surjectivity [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 5 years ago . Improve this question I'm going through my textbook here and a lot of the examples show surjective functions that only map, say $$ \mathbb{R} \rightarrow \mathbb{R} $$ But it just occurred to me what if I wanted to come up with a function that is surjective but more restrictive? Like Odd -> Odd? I'm not sure why but I'm having some real trouble coming up with one and it's bugging me. Where would I even start?","['functions', 'relations']"
2974287,Inflection point for function with fractional exponents,"Show that $f(x) = 4x^{1/3}-x^{4/3}$ has an inflection point at $x=1$ . I correctly get $$f'(x) = \frac{4(1-x)}{3x^{2/3}}\implies f''(x)=-\frac{4(x+2)}{9x^{5/3}}$$ It is clear to me that there is an inflection point at $x=-2$ since this value of $x$ makes the second derivative zero. The text shows that there is also an inflection point at $x=1$ .  I see that this value makes the first derivative $=0$ , but I don't understand why this causes an inflection point.  Can anyone help clarify this point?","['calculus', 'derivatives', 'stationary-point']"
2974298,"How to prove $\frac{1}{2n+2}<\int_0^{\frac{\pi}{4}}\tan^nx\,dx<\frac{1}{2n}$","How to prove $$\frac{1}{2n+2}<\int_0^{\frac{\pi}{4}}\tan^nx\,dx< \frac{1}{2n}$$ Set $A_n=\int_0^{\frac{\pi}{4}}\tan^nx\,dx$ , then we have $A_n+A_{n+2}=\frac{1}{n+1}$ and we have $A_{n+2} < A_n$ ,so we can get $$\frac{1}{2n+2}< \int_0^{\frac{\pi}{4}}\tan^nx\,dx < \frac{1}{2n-2}$$ But how to show that $$\int_0^{\frac{\pi}{4}}\tan^nx\,dx < \frac{1}{2n}$$","['integration', 'definite-integrals', 'analysis', 'integral-inequality', 'inequality']"
2974318,Show that $\lim_{k\to\infty}\lim_{j\to\infty}\cos^{2j}k!\pi x=0$ [duplicate],"This question already has answers here : Double limit of $\cos^{2n}(m! \pi x)$ at rationals and irrationals (3 answers) Closed 5 years ago . The Dirichlet function is defined as the indicator function of rational numbers. I have also seen this function described by: $$f(x)=\lim_{k\to\infty}\lim_{j\to\infty}\cos^{2j}k!\pi x$$ How does this limit act as the indicator, and how does it yield an answer if cosine is limited to Infinity?","['rationality-testing', 'limits', 'irrational-numbers', 'rational-numbers']"
2974320,Expected number of offers until house is sold,"I am selling my house, and have decided to accept the first offering
exceeding $K$ dollars. Assuming that offers are independent rv with
common distribution $F$ , find the expected number of offers received
before I sell the house. Try I call $X$ to be number of offers receiver before house is sold. Suppose we have $n$ such offers and call them $X_1,X_2,...,X_n$ . Therefore, $X = \sum X_i$ . We have that $$ E(X) = E(X_1) + .. + E(X_n) $$ Since all $X_i$ have common distribution $F$ , then $$ E(X_i) = \int\limits_0^K x f(x) $$ where $f = F' $ . So, $$ E(X) = n \int\limits_0^K x f(x) $$ is this correct?",['probability']
2974370,Questions on Proof of Intermediate Value Theorem,"I am trying to follow a proof of the Intermediate Value Theorem in Ross's real-analysis textbook, but do not understand several of the steps. I'm going to replicate the proof as much as I can (sometimes adding additional detail, so if I state something incorrectly without knowing, please tell me) and pause at these questions. Theorem. If $f$ is a continuous real-valued function on an interval $I$ , then $f$ has the intermediate value property on $I$ : Whenever $a, b \in I$ , $a < b$ , and $y$ lies between $f(a)$ and $f(b)$ [i.e., $f(a) < y < f(b)$ or $f(b) < y < f(a)$ ], there exists at least one $x$ in $(a,b)$ such that $f(x) = y$ . -- Proof. Suppose that $f(a) < y < f(b)$ . We define a set $S = \{x \in [a,b]: f(x) < y\}$ . Since $f(a) < y$ by assumption, $a \in S$ , so $S$ is nonempty. Furthermore, $[a,b] = \{x \in \mathbb{R} : a \leq x \leq b\}$ by definition, so it follows, it follows that $x \leq b, \forall x \in S$ , so $S$ is bounded above by $b$ . By the completeness axiom, $S$ has a supremum, $x_0$ . For any $n \in \mathbb{N}$ , $x_0 - \frac{1}{n} < x_0$ , so $x_0 - \frac{1}{n}$ is not an upper bound for the set of $S$ , by the definition of the supremum. The only thing I am confused on here is whether we are able to assert that $x < b$ . We know $f(b) > y$ , so $b$ is not in $S$ , which suggests that we can make this greater assertion. Because $x_0 - \frac{1}{n}$ is not an upper bound of $S$ , there exists some element in $S$ greater than it. So, for every $n \in \mathbb{N}$ , there exists some $s_n \in S$ such that $x_0 - \frac{1}{n} < s_n \leq x_0$ . Here, I am not completely sure on the strategy. He seems to take $s_n$ as not a particular element of $S$ , but to treat $S$ as a sequence (we take the limit in the next step) and then take $s_n$ to be a subsequence where we create a value for each natural number. Is this the correct way to think about it? Therefore, $\lim s_n = x_0$ . Does this follow from the squeeze lemma? Limits preserve $\leq$ inequalities, so $x_0 - \frac{1}{n} < s_n \leq x_0$ implies $\lim \left(x_0 - \frac{1}{n} \right) \leq \lim s_n \leq \lim \left(x_0 \right)$ and that $x_0 \leq \lim s_n \leq x_0$ , so by the squeeze lemma $\lim s_n = x_n$ . That $f(s_n) < y$ for all $n \in \mathbb{N}$ , and since $f$ is continuous, it follows that $f(x_0) = \lim f(s_n) \leq y$ . Assuming that I am correct in thinking that each $s_n$ is an element of the set that we are treating as a subsequence of $S$ (meaning that when we talk about the limit of $s_n$ , we are are talking about the sequence, but when we say $f(s_n) < y$ , we are talking an individual element), then I believe i understand this. Define $t_n = \min\left(b, x_0 + \frac{1}{n}\right)$ . I am assuming, again, that this is a subsequence of $S$ . Observe that $x_0 \leq t_n \leq x_0 + \frac{1}{n}$ . I do not understand completely the second of these inequalities. If $t_n = b$ , since $b$ is an upper bound of $S$ and $x_0$ is its supremum, it follows that $x_0 \leq t_n$ , so I understand this. But $n$ is strictly positive, obviously, so $x_0 + \frac{1}{n} > x_0$ . If $x_0 = t_n$ , then $x_0 + \frac{1}{n} > t_n$ . I suppose we have $t_n = x_0 + \frac{1}{n}$ is $x_0 + \frac{1}{n} < b$ and thus $t_n = x_0 + \frac{1}{n}$ . Is this correct? Therefore, $\lim t_n = x_0$ . I assume this is also a result of the squeeze lemma, correct? Observe that each $t_n$ belongs to $[a,b]$ but not to $S$ . If $t_n = b$ , then it surely doesn't belong to $S$ since $f(b) > y$ . Is the additional argument then that since $x_0$ is the supremum of $S$ , then if $x_0 + \frac{1}{n}$ were in $S$ , then $x_0$ couldn't be its supremum (a contradiction), then $x_0 + \frac{1}{n}$ cannot be in the set? I think I understand why $t_n$ is not in $S$ , but not why they are still in $[a,b]$ . Surely $b$ is in $[a,b]$ , and I suppose in the case where $x_0 + \frac{1}{n}$ is greater than $b$ , then $t_n = b$ , so we stay in $[a,b]$ . Is that correct? Therefore, $f(t_n) \geq y$ for all $n$ . Therefore, since $f$ is continuous, $f(x_0) = \lim f(t_n) \geq y$ . Since $f(x_0) \leq y$ and $f(x_0) \geq y$ , it follows that $f(x_0) = y$ . Therefore, $\exists x \in (a,b)$ such that $f(x) = y$ . I understand everything here. I would greatly appreciate any insights on the above points or on something I missed. Thanks in advance.","['proof-explanation', 'real-analysis']"
2974375,Related Rates Calculus Trigonometric Problem,"I've been stuck on this related rates problem for a while now and I just can't figure out how to even approach it. The problem goes something like this: The above diagram shows two objects moving at different speeds. Both objects are 0.5 miles from the origin. The blue object is moving at 50mph. The straight-line distance between the blue object and the red object is increasing at a rate of 35 mph. Find the speed of the red object. I tried to solve it using Pythagorean Theorem and finding the derivative of the top side of the triangle. Anyway, I ended up getting a negative number, and even ignoring the sign, the answer I got was obviously wrong. I know the speed of the red object is obviously greater than the blue object because the distance between the objects is increasing. I just really don't know how to calculate the magnitude of said number. There's also a variation of this problem where the straight-line distance is changing at a rate of -35mph but I think that should be doable once I understand how to go about solving the original. Any responses would be appreciated!",['calculus']
2974378,Equicontinuity of the Image of a set of Functions under an Integral Operator.,"Let $X = C([0,1])$ be the Banach space of continuous functions on $[0,1]$ (with the supremum norm) and define a map $F:X\rightarrow X$ by $$F(f)(x)=\int^{x}_{0} \cos(f(t)^{2})dt, \space x \in [0; 1].$$ Show that $FX=\{F(f):f \in X \} \subset X$ is relatively compact. This comes down to using the Arzela-Ascoli Theorem. I have shown pointwise boundedness. I am stuck on showing equicontinuity. So far I have tried the following $$\begin{align} |(F(f)-F(g))(x)| &= \Big{|} \int^{x}_{0} \cos(f(t)^{2}-\cos(g(t)^{2}) dt \Big{|} \\ &\leq \int^{x}_{0}| \cos(f(t)^{2})-\cos(g(t)^{2})| dt\\ &= \int^{x}_{0} \Big|f(t)^{2}-g(t)^{2}| dt \end{align}.$$ I am stuck at this point, as I do not know how to bound the term inside the integral. Any hints on how to proceed from here or whether this approach is incorrect, and if so, which approach to take? Thank you in advance.","['banach-spaces', 'operator-theory', 'functional-analysis', 'real-analysis']"
2974382,Martingale theory: Collection of examples and counterexamples,"The aim of this question is to collect interesting examples and counterexamples in martingale theory. There is a huge variety of such (counter)examples available here on StackExchange but I always have a hard time when I try to locate a specific example/question. I believe that it would be a benefit to make this knowledge easier to access. For this reason I would like to create a (big) list with references to related threads. Martingale theory is a broad topic, and therefore I suggest to focus on time-discrete martingales $(M_n)_{n \in \mathbb{N}}$ . I am well aware that this is still a quite broad field. To make this list a helpful tool (e.g. for answering questions) please make sure to give a short but concise description of each (counter)example which you list in your answer. Related literature: Jordan M. Stoyanov: Counterexamples in Probability, Dover. Joseph P. Romano, Andrew F. Siegel: Counterexamples in probability and statistics, CRC Press.","['martingales', 'big-list', 'probability-theory', 'examples-counterexamples']"
2974471,Convergence of the series $\sum a_n$ when $\sqrt[n]{a_n}\leq 1-\frac{1}{n^\alpha}$ for $0<\alpha<1$.,"Examine the convergence of the series $\sum a_n$ , where: $$\sqrt[n]{a_n}\leq 1-\frac{1}{n^\alpha}$$ for all $n$ ( $0<\alpha<1$ ). Attempt. Since $$\limsup \sqrt[n]{a_n}\leq \limsup\left(1-\frac{1}{n^\alpha}\right)=1$$ we can not use the root test. Comparison test also doesn't work, since $\sum(1-n^{-\alpha})$ diverges to $+\infty$ . Thanks in advance.","['calculus', 'sequences-and-series', 'real-analysis']"
2974476,Conserved Current for a PDE,"Let $(x,t) \in \mathbb{R}^2$ , $W(x)$ be a (smooth enough) real-valued function and consider the following partial differential equation for the real-valued function $U(x,t)$ \begin{equation}
\frac{\partial^2 U}{\partial t^2} = - \frac{\hbar^2}{4 m^2} \frac{\partial^4 U}{\partial x^4}+ \frac{W}{m} \frac{\partial^2 U}{\partial x^2} +\frac{W‚Äô}{m} \frac{\partial U}{\partial x} + \left( \frac{W‚Äô‚Äô}{2m} - \frac{W^2}{\hbar^2} \right) U \qquad (I),
\end{equation} where $m$ and $\hbar$ are positive constants. In the following we shall be quite sloppy, and we shall assume that given (smooth enough) initial conditions $U(x,0)$ and $\frac{\partial U}{\partial t}(x,0)$ (lying in some space) there exists a unique (smooth enough) solution $U$ (lying in some space) to (I). Let us call the set of solutions $\mathcal{E}$ . Let $D_{x}^k F$ be the set of all partial derivatives of $F$ with respect to $x$ from order 1 to order $k$ . I ask whether there exist (smooth enough) real-valued functions $p \geq 0$ and $j$ such that, by setting \begin{equation}
P(x,t)=p \left(U(x,t),(D_{x}^k U)(x,t), \frac{\partial U}{\partial t}(x,t), \left(D_{x}^{k} \frac{\partial U}{\partial t}\right)(x,t) \right), \\
J(x,t)=j \left(U(x,t),(D_{x}^k U)(x,t), \frac{\partial U}{\partial t}(x,t), \left(D_{x}^{k} \frac{\partial U}{\partial t}\right)(x,t) \right),
\end{equation} the following properties hold: (i) if $U$ is the solution of (I) corresponding to a given function $W(x)$ and given initial conditions $U(x,0)$ and $\frac{\partial U}{\partial t}(x,0)$ , and $\tilde{U}$ is the solution of (I) corresponding to the same initial conditions, but to $W(x)+c$ , with $c \in \mathbb{R}$ , then $P(x,t)$ is the same when computed for $U$ and $\tilde{U}$ ; (ii) for every $U \in \mathcal{E}$ the following conservation law holds \begin{equation}
\frac{\partial P}{\partial t} + \frac{\partial J}{\partial x}=0;
\end{equation} (iii) $p$ is not the constant function. The answer I think should be negative, but I don't how to ""prove"" this: since we have not formulated the problem in a rigorous way, we do not expect to get a rigorous proof, but some heuristic, but convincing argument in this direction. NOTE (1) This problem, as the notation shows, has a physical background, and the mathematical formulation of the problem that I give here is my personal interpretation of a physical exposition given by the great XXth century physicist David Bohm in his wonderful treatise $\mathit{Quantum}$ $\mathit{Theory}$ published in 1951. For all the physical details about this problem see my post Nonexistence of a Probability for Real Wave Functions . NOTE (2) Bohm's physical discussion is not very clear, so that it can admit different mathematical interpretations. A simpler interpretation of Bohm's original statement is the following. Consider the following equation \begin{equation}
\frac{\partial^2 U}{\partial t^2} = - \frac{\hbar^2}{4m^2} \frac{\partial^4 U}{\partial x^4} \qquad{(II)},
\end{equation} and let $\mathcal{F}$ the set of all (smooth enough) solutions of this equation.
Do there exists (smooth enough) real-valued functions $p \geq 0$ and $j$ such that, by setting \begin{equation}
P(x,t)=p \left(U(x,t),(D_{x}^k U)(x,t), \frac{\partial U}{\partial t}(x,t), \left(D_{x}^{k} \frac{\partial U}{\partial t}\right)(x,t) \right), \\
J(x,t)=j \left(U(x,t),(D_{x}^k U)(x,t), \frac{\partial U}{\partial t}(x,t), \left(D_{x}^{k} \frac{\partial U}{\partial t}\right)(x,t) \right),
\end{equation} the following properties hold: (i) for every $U \in \mathcal{F}$ we have \begin{equation}
\frac{\partial P}{\partial t} + \frac{\partial J}{\partial x}=0;
\end{equation} (ii) for the special solution $U(x,t)=\cos\left(\sqrt{\frac{2m \omega}{\hbar}}x-\omega t \right)$ , we have that $P(x,t)$ is independent of $\omega > 0$ ; (iii) $p$ is not a constant function? Maybe this mathematical problem is more easily seen to have a negative answer than the one I formulated above.","['partial-differential-equations', 'mathematical-physics', 'real-analysis']"
2974517,Integral $\int_0^\infty \frac{x-\sin x}{x^3(x^2+4)} dx$,"The following integral appeared on the $8$ th Open Mathematical Olympiad of the Belarusian-Russian University. $$I=\int_0^\infty \frac{x-\sin x}{x^3(x^2+4)} dx$$ I used power series: $$x-\sin x = \sum_{n=1}^\infty \frac{(-1)^{n+1}x^{2n+1}}{(2n+1)!}\rightarrow I=\sum_{n=1}^\infty \frac{(-1)^{n+1}}{(2n+1)!} \int_0^\infty \frac{x^{2n-2}}{x^2+4}dx$$ Taking the inner integral and substituting $\displaystyle{x^2=4t \rightarrow dx=\frac{dt}{\sqrt{t}}}$ gives: $$\int_0^\infty \frac{x^{2n-2}}{x^2+4}dx=4^{n-2}\int_0^\infty \frac{t^{n-1-\frac12}}{t+1}dt$$ $$=4^{n-2} B\left(n-\frac12, 1-n+\frac12\right)=4^{n-2}\Gamma\left(n-\frac12\right)\Gamma\left(1+\frac12-n\right)$$ And using Euler's reflection formula: $$\Gamma\left(n-\frac12\right)\Gamma\left(1+\frac12-n\right)=\pi \csc\left({n\pi-\frac{\pi}{2}}\right)=-\pi\sec(n\pi)=(-1)^{n+1}\pi$$ $$I=\pi\sum_{n=1}^\infty \frac{4^{n-2}}{(2n+1)!}=\frac{\pi}{32} \sum_{n=1}^\infty \frac{2^{2n+1}}{(2n+1)!}=\frac{\pi}{32}(\sinh 2 -1)$$ I did not found the official solution, but the answer given $\displaystyle{\frac{\pi}{32}\left(\frac{e^2-1}{e^2}\right)},\,$ doesn't match. Can you help me find my mistake? And maybe share some different methods to solve this integral?","['integration', 'proof-verification', 'definite-integrals']"
2974520,Find the limit of $\frac{3^{2n + 1} + 2^{3n + 1}}{7^{n+2} + 9^n}$ when $n$ goes to infinity,"When doing my test prep, I stumbled upon this particular exercise: $$\lim_{n \to \infty} \frac{3^{2n + 1} + 2^{3n + 1}}{7^{n+2} + 9^n} $$ I tried to solve it through some algebraical juggling, but without luck. Even trying l'Hospitals rule (which I officially don't know yet) didn't yield a solution. When I inputted the task into Wolfram, I found out the solution is $3$ . Even after that, I've not been able to find the steps which are needed to get that solution. Judging by the value 3, it would seem that I somehow want to ""reduce"" the fraction so that I get only $$\lim_{n \to \infty} \frac{3^{2n + 1}}{9^n} $$ from which 3 would easily follow, but I'm not sure how to do that.",['limits']
2974525,"Is the problem VIII, 1.6 of Dungundji's Topology right?","On the page 178 of Dungundji's Topology, Problem 6 of Section 1 says: A grating of a space $Y$ is a family $\{V_{\alpha}\;|\;\alpha \in A\}$ of disjoint open sets such that $\{\overline{V_{\alpha}}\;|\;\alpha \in A\}$ is a covering of $Y$ and $\;Int(\overline{V_{\alpha}}) = V_{\alpha}\;$ for each $\alpha \in A.\;$ Let $\{V_{\alpha}\;|\;\alpha \in A\}$ and $\{W_{\beta}\;|\;\beta \in B\}$ be two gratings of $Y$ . Prove: $$\{V_{\alpha}\cap W_{\beta}\;|\;(\alpha, \beta) \in A\times B\}$$ is a grating of $Y$ . So far I've succeeded in proving $$Int(\overline{V_{\alpha}\cap W_{\beta}}) = V_{\alpha}\cap W_{\beta} \quad\forall \alpha, \beta$$ , but couldn't prove $\{\overline{V_{\alpha}\cap W_{\beta}}\;|\;(\alpha, \beta) \in A\times B\}$ covers $Y$ using given conditions only. My question is: Is the statement of the problem right? I think $\{(-\infty,0), (1, \infty)\}\cup\{(\frac{1}{n+1},\frac{1}{n})\;|\;n \in \mathbf{N}\}$ and $\{(-\infty,-1), (0, \infty)\}\cup\{(-\frac{1}{n},-\frac{1}{n+1})\;|\;n \in \mathbf{N}\}$ are both gratings of $\mathbf{R}$ , but their intersection $$\{(-\infty,-1), (1,\infty)\}\bigcup\left\{(-\frac{1}{n},-\frac{1}{n+1})\;\big|\;n \in \mathbf{N}\right\}\bigcup\left\{(\frac{1}{n+1},\frac{1}{n})\;\big|\;n \in \mathbf{N}\right\}$$ is not a grating since $0$ does not belong to any of their closures. Am I thinking right? Or is there a way to prove coveredness of $Y$ ?",['general-topology']
2974534,Evaluate a limit (and prove it exists),"I am unable to evaluate the following limit, nor is my computer algebra system able to produce a float approximation. But the graphical plot it produces shows a pattern that is indicative of convergence to a specific constant value, and I realise that this is by no means a guarantee that indeed the limit exists, but it does allow us to eliminate a few of the commonly known types of divergence.(see figure 1 below): $$\lim_{n \rightarrow \infty}\Biggl(\frac{1}{2n}\Biggl\lfloor\ln(2) \Bigl(\Bigl\lfloor\frac{3^{1/3}n}{3^{1/3}-1}\Bigr\rfloor+1\Bigr)\Biggr\rfloor\Biggr)$$ Also for the sake of a sort of comparative analysis, see the function plotted in figure 2, which likewise is very suggestive of having a limit as $n$ is made infinite. relevant question previously asked","['limits', 'real-analysis']"
2974566,Bias adjustment for the Box-Cox back-transformation,"I'm learning time series analysis and I don't understand why the back-transform of Box-Cox transformation outputs the median instead of the mean of the forecast distribution. The family of Box-Cox transformations is defined as follows: $$\tag{1} w_t  =
    \begin{cases}
      \log(y_t) & \text{if $\lambda=0$};  \\
      (y_t^\lambda-1)/\lambda & \text{otherwise}.
    \end{cases}$$ Hence the normal back-transform would be: $$\begin{equation}
\tag{2}
  y_{t} =
    \begin{cases}
      \exp(w_{t}) & \text{if $\lambda=0$};\\
      (\lambda w_t+1)^{1/\lambda} & \text{otherwise}.
    \end{cases}
\end{equation}$$ In the book that I'm reading ""Forecasting: Principles and Practice"". It says that: One issue with using mathematical transformations such as Box-Cox transformations is that the back-transformed point forecast will not be the mean of the forecast distribution. In fact, it will usually be the median of the forecast distribution (assuming that the distribution on the transformed space is symmetric). For many purposes, this is acceptable, but occasionally the mean forecast is required. For example, you may wish to add up sales forecasts from various regions to form a forecast for the whole country. But medians do not add up, whereas means do. For a Box-Cox transformation, the back-transformed mean is given by: $$\begin{equation}
\tag{3}
y_t =
  \begin{cases}
     \exp(w_t)\left[1 + \frac{\sigma_h^2}{2}\right] & \text{if $\lambda=0$;}\\
     (\lambda w_t+1)^{1/\lambda}\left[1 + \frac{\sigma_h^2(1-\lambda)}{2(\lambda w_t+1)^{2}}\right] & \text{otherwise;}
  \end{cases}
\end{equation}$$ where $\sigma_h^2$ is the $h$ -step forecast variance. The larger the forecast variance, the bigger the difference between the mean and the median. Could you please help me explain why the point forecast in equation (2) is the median and the bias-adjusted forecast in equation (3) is the mean of the forecast distribution and what is the formula for $\sigma_h^2$ in equation (3)? Many thanks in advance for your help!","['time-series', 'statistics']"
2974633,Proof explanation: convexity of the numerical range of an operator (Toeplitz-Hausdorff Theorem),"Let $\mathcal{H}$ be a complex Hilbert space and $\mathcal{B}(\mathcal{H})$ be the algebra of all bounded linear operators on $\mathcal{H}$ . Theorem: Let $T\in \mathcal{B}(\mathcal{H})$ , then $W(T)$ is a convex subset of $\mathbb{C}$ , where $$W(T):=\{\langle T x\mid x\rangle;\;x \in \mathcal{H}\;\;\text{with}\;\|x\|=1\}.$$ Proof: Given $\lambda,\mu\in W(T)$ such that $\lambda\neq\mu$ and we will prove that $\eta=t\lambda+(1-t)\mu \in W(T)$ whenever $t\in [0,1]$ . Clearly we have $$\eta\in W(T)\Leftrightarrow t\in W(\alpha I+\beta T),$$ with $\alpha=-\frac{\mu}{\lambda-\mu}$ and $\beta=\frac{1}{\lambda-\mu}$ . Let $S=\alpha I+\beta T$ . We observe that $\eta\in W(T)$ if and only if $t\in W(S)$ for all $t\in [0,1]$ . Since $\lambda,\mu\in W(T)$ , then there exist unit vectors $x,y\in \mathcal{H}$ such that $$\lambda=\langle Tx\mid x\rangle\;\text{and}\;\mu=\langle Ty\mid y\rangle.$$ A simple calculation shows that $$\langle Sx\mid x\rangle=1\;\text{and}\;\langle Sy\mid y\rangle=0.$$ Define $g:\mathbb{R}\to \mathbb{C}$ by $$g(\theta)=\langle Sy\mid x\rangle e^{-i\theta}+\langle Sx\mid y\rangle e^{i\theta},\;\theta\in \mathbb{R}.$$ Obviously $g(\theta+\pi) = -g(\theta)$ for all $\theta\in \mathbb{R}$ . Further, $\Im m g(0)=-\Im m g(\pi)$ . Since $g$ is a continuous function, then there exists $\theta_0\in [0,\pi]$ such that $\Im m g(\theta_0)=0$ . Now observe that the vectors $y$ and $\hat{x}=e^{i\theta_0}x$ are linearly independent. Otherwise, we write $y=\alpha \hat{x}$ for some $\alpha\in \mathbb{C}$ , then $|\alpha|=1$ and $0=\langle Sy\mid y\rangle=|\alpha|^2\langle S\hat{x}\mid \hat{x}\rangle=\langle Sx\mid x\rangle=1$ which is a contradiction. Hence, $\|(1-t)y+t\hat{x}\|\neq 0$ for all $t\in [0,1]$ . Put $$z_t=\frac{(1-t)y+t\hat{x}}{\|(1-t)y+t\hat{x}\|},\;t\in [0,1].$$ Clearly $\|z_t\|=1$ for all $t\in [0,1]$ . Moreover, one can see that $\langle Sz_0\mid z_0\rangle=0$ and $\langle Sz_1\mid z_1\rangle=1$ . This implies that $0,1\in W(S)$ . To finish the proof, define a continuous function $f$ on $[0,1]$ by $$f(t)=\langle Sz_t\mid z_t\rangle,\;t\in [0,1].$$ A straightforward calculation shows that $f$ is a real-valued function with $f(0)=0$ and $f(1) = 1$ . Hence $[0,1]\subset W(S)$ . In this proof I don't understant the following facts: $\eta\in W(T)\Leftrightarrow t\in W(\alpha I+\beta T).$ Why we use $\Im m g$ in order to find $\theta_0$ . Why we use $f$ in order to conclude that $[0,1]\subset W(S)$ ?","['operator-theory', 'functional-analysis', 'real-analysis']"
2974651,Good books on Function fields for self-study,The first 7 chapters of the book Making Transcendence Transparent by Burger & Tubbs is very easy to read with not much prerequisite. But there is a sudden change of level of difficulty in the 8th (the last) chapter. The followings are the titles of materials: So what are some of books that are suitable for self-study? I have limited background in algebra (just an undergraduate book). All suggestions are welcome! Thank you!!,"['number-theory', 'book-recommendation', 'reference-request']"
2974658,Algebraic 1-cocycles and Galois gerbs,"We have the following set up: $K/F$ is Galois, $D$ is an algebraic group of mult. type and $E$ is an extension of groups: $$1\to D(K)\to E\to Gal(K/F)\to 1$$ Now take a linear algebraic group G over F. An algebraic 1-cocycle is a map $w \mapsto x_w$ from $E$ to $G(K)$ s.t. $x_{w_1 w_2}= x_{w_1} w_1(x_{w_2})$ , where $E$ acts on $G(K)$ via $E\to Gal(K/F)$ . Furthermore we have a Galois action on morphisms $v:D(K)\to G(K)$ via $\sigma(v)(d)=\sigma(v(\sigma^{-1}(d))$ (cf here section 2.2/2.3 for more details). In the above paper it is claimed then that for $w\in E$ which maps to $\sigma$ $$x_w\cdot\sigma(v)\cdot (x_w)^{-1}=v$$ as a cosequence of the cocycle condition. Here $v$ is the restriction of the cocycle to $D$ (i.e. $v(d)=x_d$ ). But my calculation only gives me: $$(x_w\cdot\sigma(v)\cdot (x_w)^{-1})(d)=x_w\cdot\sigma(x_{\sigma^{-1}(d)})\cdot (x_w)^{-1}=x_{w\sigma^{-1}(d)}\cdot \sigma( x_{w^{-1}}) =x_{w\sigma^{-1}(d)w^{-1}}$$ and I am not able to conclude the desired result.","['algebraic-groups', 'galois-theory', 'algebraic-geometry', 'group-cohomology', 'arithmetic-geometry']"
2974729,Need to find the equation of a curve from a Hand drawing [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. This question is not about mathematics, within the scope defined in the help center . Closed 5 years ago . Improve this question My uncle makes hand drawings of furniture on a large piece of chart paper at 1:1 scale. He has recently passed away. I have the task of converting those hand drawings into Autocad drawings. The straight lines are easy to draw. But the curves of the furniture are very hard to replicate exactly. I need to be as precise as possible. Currently, I mark points on the curve and measure the distance from a reference line and then recreate that on the computer. This is extremely time-consuming. Is there a mathematical way in which I can solve this problem? Either by getting the equation of the curve or something?","['curves', 'geometry', 'applications']"
2974738,Set Theory In Construction of Naturals,"My understanding is that the naturals may be constructed via algebraic terms as per Peano axioms and they may also be constructed based on set theory. I would like to improve my intuitions regarding set theoretical construction approaches. On the algebraic side, I believe I can intuit the successor function adding a multiplicative identity element to an empty set or to a set containing other instances of that element. With set theory, I have difficulty intuiting the meaning of $$1 := 0 \cup \{0\} = \{0\} = \{\emptyset\}\quad \text{or}\quad 2 := 1 \cup \{1\} = \{0, 1\} = \{\emptyset, \{\emptyset\}\}.$$ I am able to recognize the nesting of prior/lesser sets within those of a given nonzero number but my newb intuition tells me that the constituent elements of those sets are a whole lot of nothing, which makes it difficult for me to intuitively recognize them as representing differing quantities of something . Any words of advice or recommended resources would be much appreciated.","['elementary-set-theory', 'natural-numbers']"
2974740,Find the maximum value of $ \int_{0}^{y}\sqrt{x^{4}+(y-y^{2})^{2}}dx$,Find the maximum value of $$\displaystyle \int_{0}^{y}\sqrt{x^{4}+(y-y^{2})^{2}}dx$$ for $0 \leq y\leq 1$ . Try: Let $$I(y) = \int^{y}_{0}\sqrt{x^4+(y-y^2)^2}dx$$ Then $$I'(y)=\sqrt{y^{4}+y^{2}(1-y)^{2}}+y(1-y)(1-2y)\int_{0}^{y}\frac{dx}{\sqrt{x^{4}+y^{2}(1-y)^{2}}}$$ Now did not find any clue how I find $$\int_{0}^{y}\frac{dx}{\sqrt{x^{4}+y^{2}(1-y)^{2}}}$$ Could some help me find it? Thanks.,"['integration', 'maxima-minima', 'multivariable-calculus', 'definite-integrals']"
2974761,Weak convergence of sequence of random variables,"Suppose that $(X_n)_{n \in \mathbb{N}}$ is a sequence of real-valued random variables such that their weak limit $X$ exists and is not constant almost surely. Furthermore, suppose that there exist a sequence of positive real numbers $(a_n)_{n \in \mathbb{N}}$ and a sequence of real numbers $(b_n)_{n \in \mathbb{N}}$ such that $a_n X_n + b_n$ converges weakly to another random variable $Y$ . I want to show that in the above setting there exist $a\geq 0$ and $b \in \mathbb{R}$ such that $a_n \to a$ and $b_n \to b$ . Moreover, if $Y$ is also not constant almost surely, then $a$ is supposed to be positive. I am grateful for any help!","['probability-theory', 'probability']"
2974833,"$\min f(x)=\frac{1}{2}x^t Bx + c^t x$ subject to $Ax=b, x\ge 0$ then $f(\overline{x})=\frac{1}{2}(c^t\overline{x}+b^t\overline{\lambda})$","$$\min f(x)=\frac{1}{2}x^t Bx + c^t x\\\mbox{s.t.} \\Ax=b\\ x\ge 0$$ Let $\overline{x}$ be a regular solution of the problem and $\overline{\lambda}$ the vector of Lagrange Multipliers associated to
  the equality restrictions. Prove that $f(\overline{x})=\frac{1}{2}(c^t\overline{x}+b^t\overline{\lambda})$ This problem is the same as $$\min f(x)=\frac{1}{2}x^t Bx + c^t x\\\mbox{s.t.} \\Ax-b\le 0\\ -x\le 0$$ I can't use advances method like KKT or whatever, so to solve problems with inequality constraints I must create a matrix with all the inequalities and analyze the intersection points. That is, the points $x$ such that $a_ix-b_i= 0$ and $a_jx-b_j= 0$ for some $i$ and $j$ or that $a_ix-b_i= 0$ and $-x\le 0$ . I must investigate if the points $x$ in the intersection are written like this: $\nabla f(x) = B_I\lambda$ , where $B$ is the matrix of all inequalities, and $B_I$ is the submatrix of $B$ that contains only the inequalities of the intersection. However I have some questions. The minimum point will always be in the intersections? If it's true, how do I arrive at the final result? If $\overline{x}$ is a point in the intersection of two inequalities and satisfy $\nabla f(\overline{x})=B_I\overline{\lambda}$ for negative $\lambda_i$ for all $i$ then how $f(\overline{x})=\frac{1}{2}(c^t\overline{x}+b^t\overline{\lambda})$ ?","['lagrange-multiplier', 'linear-programming', 'multivariable-calculus', 'linear-algebra', 'optimization']"
2974841,If $ \int_{0}^{1} \frac{\ln x}{1-x^2} dx = -\frac{œÄ^2}{\lambda} $ find $\lambda$ given that $\sum_{n=1}^{\infty} \frac{1}{n^2} = \frac{œÄ^2}{6} $,"If $$\sum_{n=1}^{\infty} \frac{1}{n^2} = \frac{œÄ^2}{6} $$ then $$ \int_{0}^{1} \frac{\ln x}{1-x^2} dx = -\frac{œÄ^2}{\lambda} $$ then the value of $\lambda$ equals? My attempt - I tried using integrating by parts to the integral. As a result, I'm left with $$-\frac{1}{2} \int_{0}^{1} \frac{\ln\left(\frac{1+x}{1-x}\right)}{x} dx $$ Now I'm stuck in here! I don't know how to move on from here! Or maybe integrating by parts was a bad option? If it is, please guide me to a solution or Please help me on how to continue from here! 
Any help would be appreciated.","['integration', 'summation', 'definite-integrals', 'riemann-zeta']"
2974848,Right triangle with 2 equilateral triangles,"""Be a right triangle ABC with $\angle B=90¬∫$ . Two equilateral triangles $ABD$ and $BEC$ are drawn externally in the legs of the triangle $ABC$ . Be $G,H$ and $F$ the midpoints of $BE$ , $BC$ and $DC$ . If the area of $ABC$ is $32$ , then the area of $GHF$ is?"" I made the drawing with an arbitrary triangle $6,8,10$ in GeoGebra because i didn't know how to start in this problem, and i got that the area of $ABC$ is $4$ times the area of $GHF$ (the triangle $GHF$ is right too), so the answer will be $8$ , but i want to know how to get this mathematically without trigonometry. Any hints?","['euclidean-geometry', 'triangles', 'geometry']"
2974891,what is an ‚àû-group?,"I was reading on nLab and I found the term infinity group . The definition is awfully abstract: An ‚àû-group is a group object in ‚àûGrpd . Equivalently (by the delooping hypothesis) it is a pointed connected ‚àû-groupoid. Under the identification of ‚àûGrpd with Top this is known as an A‚àû-space, for instance. Then I tried to look up the term "" group-object "" and obtained: A group object in an (‚àû,1)-category generalizes and unifies two familiar concepts: it is the generalization of the notion of groupal Stasheff A‚àû-space from Top to more general (‚àû,1)-sheaf (‚àû,1)-toposes: an object that comes equipped with an associative and invertible monoid structure, up to coherent homotopy, and possibly only partially defined (see also looping and delooping for more on this) ; it generalizes the notion of equivalence relation ‚Äì or rather the internal notion of congruence ‚Äì from category theory to (‚àû,1)-category theory. At this point I don't even know enough to specify what an answer should look like. As I recall, a group $G$ is a set with a binary operation $\times$ so that $(G,\times)$ should have three properties: an identity element $e \in G$ with $e \times g = g \times e = g$ every element $g \in G$ has an inverse $g^{-1}$ so that $g \times g^{-1}= g^{-1} \times g = e$ multiplication is associative $a \times (b \times c) = (a \times b) \times c$ for all $a,b,c \in G$ . So our infinity group looks nothing like the object that i am familiar with.","['group-theory', 'homotopy-theory', 'category-theory', 'groupoids']"
2974940,"$\mu,\nu$ $\sigma$-finite and $\nu \le \mu$. Then there exists a $\mu$-almost-everywhere unique function $f$ with $0 \le f \le 1 \ \mu \ a.e.$","Let $\mu,\nu$ be $\sigma$ -finite on $(\Omega,\mathcal{A})$ and $\nu  \le \mu$ . Then there exists a $\mu$ -almost-everywhere unique function $f=\frac{d\nu}{d\mu}$ with $0 \le f \le 1 \ \mu \ a.e.$ As both measures are $\sigma$ finite and $\nu \le \mu \Rightarrow \nu << \mu$ the Radon-Nikodym-Theorem tell us that there exists such a function $f$ which fulfils $f\ge 0$ $\mu \ a.e.$ and this $f$ is unique $a.e.$ The only thing that is left to show is that $f \le 1 \ a.e.$ .
For this I tried a proof by contradiction: Let $A \in \mathcal A$ with $\mu (A)>0$ and $\forall x \in A:f(x)>1$ Then $\nu(A)=\int_A f d \mu>\int_A 1 d \mu=\mu(A)$ which is a contradiction because $\nu \le \mu$ and therefore $f \le 1 \ a.e$ . Is my way to show $f \le1$ correct?",['measure-theory']
2974962,Assume I'm unhappy with the Riemann Rearrangement Theorem. How else can I define a series?,"I have been explaining the Riemann Rearrangement Theorem to a friend of mine, and they feel as though the definition of series using partial sums ""doesn't work"" for conditionally convergent sequences. I understand how they feel: the RRT feels counter-intuitive, and so the result should be denied as a contradiction and the partial-sum definition rejected. However, the definition of a series as $$\sum_{k=1}^{\infty} a_k = \lim_{n\to\infty} \left(\sum_{k=1}^n a_k\right)$$ feels like the most natural approach to take. If I wanted to add infinitely many numbers together by hand, this is how I would have to do it. Is there an alternative (but not equivalent) definition of an infinite sum that agrees with the partial-sum definition on absolutely convergent sequences, but where the RRT doesn't hold? My guess is that if you define some value for an infinite series agreeing on absolutely convergent sequences then this method of definition must imply the RRT, but I don't see how that proof could go.","['convergence-divergence', 'definition', 'sequences-and-series', 'real-analysis']"
2974989,"Counting four-digit numbers that start with an even (possibly zero), end with an odd, and have no repetitions","Question: In a certain company, employee IDs are strings of 4 digits (that may begin with a $0$ ). How many IDs begin with an even digit, end with an odd digit, and have no repeated digit? Answer: First digit: $5$ choices (either $0$ , $2$ , $4$ , $6$ , or $8$ ) Last digit: $5$ choices (either $1$ , $3$ , $5$ , $7$ , or $9$ ) Second digit: $8$ choices ( $5+5 - 2$ ) Third digit: $7$ choices ( $5+5 - 3$ ) Total number of IDs = $5 \times 5 \times 8 \times 7$ Am I right?","['permutations', 'combinatorics', 'discrete-mathematics']"
2975018,"Are there any non-algebraic, non-transcendental complex numbers? Is $0$ a pure imaginary number?","I came across this question here: Difference between imaginary and complex numbers The top answer contains this diagram: Here we see numbers like $e - \pi i$ and $\pi + 3i$ existing outside of transcendental and algebraic numbers but within the realm of complex numbers. Is this accurate or should they technically be in the transcendental area? Are there any complex non-transcendental non-algebraic numbers? We also see $0$ as a whole number, an integer, a rational number, a real number, an algebraic number, and a complex number, but is it not also a pure imaginary number?","['number-theory', 'real-analysis', 'complex-analysis', 'definition', 'complex-numbers']"
2975020,Question about 1-forms on $S^3$,"I am an undergrad student, and doing an introduction to smooth manifolds course. I came across the following question:
Let $\theta_1,\theta_2,\theta_3 \in \Omega^1(S^3)$ be 1-forms on $S^3\subset \Bbb{R}^4$ given by: $\theta_1 := ‚àíy dx + x dy + t dz ‚àí z dt,\\
\theta_2 := ‚àíz dx ‚àí t dy + x dz + y dt,\\
\theta_3 := ‚àít dx + z dy ‚àí y dz + x dt$ (where i use conventions, dx and the other forms on the right hand
side are the restrictions from $\Bbb{R}^4$ to $S^3$ just that we do not write $dx|_{S^3}$ etc all the
time).
now they ask me to show that: $$d\theta_1=-2\theta_2\wedge\theta_3$$ where $d: \Omega^k(S^3)\to\Omega^{k+1}(S^3)$ is DeRham differential and $\cdot \wedge \cdot: \Omega^k(S^3)\times \Omega^l(S^3) \to \Omega^{k+l}(S^3)$ is the wedge product. I have worked on this for a while now but can't seem to figure it out. I feel like that if: for $p \in S^3$ , $p:=(x,y,z,t)$ and $w \in T_pS^3$ , $w=(w_1,w_2,w_3,w_4)$ , $$tw_3-yw_1+xw_2-zw_4=0$$ i would be much closer to the anwser, but i don't see why/if this is true.
a hint or something in the right way would be much appreciated.","['smooth-manifolds', 'differential-geometry']"
2975032,Odd of the form $a^2+b^2+c^2+ab+ac+bc$.,"The computation below shows that (for $a,b,c \in \mathbb{N}$ ) the form $$a^2+b^2+c^2+ab+ac+bc$$ covers every odd integer less than $10^5$ except those in $$I= \{ 5,  15,  23,  29,  41,  53,  59,  65,  101,  107,  149,  155,  165,  167,  221,  231,  239,  305,  317,  341,  371,  401,  413,  479,  623,  629,  659,  767,  905,  1001,  1031,  1115,  1397,  1409,  1439,  1751,  1991,  2459,  2543,  4355,  5909,  5969\}.$$ Question : Is it true that the above form covers every odd integer, except those in $I$ ? Observation: $2(a^2+b^2+c^2+ab+ac+bc) = (a+b)^2+(a+c)^2+(b+c)^2$ . Application : this answer proves that the form $\| A\|^2$ covers every natural number for $A \in M_3(\mathbb{Z})$ . A positive answer to the above question would prove this result for $A \in M_3(\mathbb{N})$ . Computation sage: oddmixed(317)
{5, 15, 23, 29, 41, 53, 59, 65, 101, 107, 149, 155, 165, 167, 221, 231, 239, 305, 317, 341, 371, 401, 413, 479, 623, 629, 659, 767, 905, 1001, 1031, 1115, 1397, 1409, 1439, 1751, 1991, 2459, 2543, 4355, 5909, 5969} Code # %attach SAGE/3by3.spyx

from sage.all import *

cpdef oddmixed(int r):
    cdef int a,b,c,n,i
    cdef list L
    L=[]
    for a in range(r):
        for b in range(r):
            for c in range(r):
                n=a**2+b**2+c**2+a*b+a*c+b*c
                if not n in L:
                    L.append(n)
    return set([2*i+1 for i in range(r*r/2)])-set(L)","['sums-of-squares', 'number-theory', 'additive-combinatorics', 'elementary-number-theory', 'combinatorics']"
2975109,How to convert Euler angles to Quaternions and get the same Euler angles back from Quaternions?,"I am rotating n 3D shape using Euler angles in the order of XYZ meaning that the object is first rotated along the X axis, then Y and then Z . I want to convert the Euler angle to Quaternion and then get the same Euler angles back from the Quaternion using some [preferably] Python code or just some pseudocode or algorithm. Below, I have some code that converts Euler angle to Quaternion and then converts the Quaternion to get Euler angles. However, this does not give me the same Euler angles. I think the problem is I don't know how to associate yaw, pitch and roll to X, Y an Z axes. Also, I don't know how to change order of conversions in the code to correctly convert the Euler angles to Quaternion and then convert the Quaternion to Euler angle so that I am able to get the same Euler angle back. Can someone help me with this? And here's the code I used: This function converts Euler angles to Quaternions: def euler_to_quaternion(yaw, pitch, roll):

        qx = np.sin(roll/2) * np.cos(pitch/2) * np.cos(yaw/2) - np.cos(roll/2) * np.sin(pitch/2) * np.sin(yaw/2)
        qy = np.cos(roll/2) * np.sin(pitch/2) * np.cos(yaw/2) + np.sin(roll/2) * np.cos(pitch/2) * np.sin(yaw/2)
        qz = np.cos(roll/2) * np.cos(pitch/2) * np.sin(yaw/2) - np.sin(roll/2) * np.sin(pitch/2) * np.cos(yaw/2)
        qw = np.cos(roll/2) * np.cos(pitch/2) * np.cos(yaw/2) + np.sin(roll/2) * np.sin(pitch/2) * np.sin(yaw/2)

        return [qx, qy, qz, qw] And this converts Quaternions to Euler angles: def quaternion_to_euler(x, y, z, w):

        import math
        t0 = +2.0 * (w * x + y * z)
        t1 = +1.0 - 2.0 * (x * x + y * y)
        X = math.degrees(math.atan2(t0, t1))

        t2 = +2.0 * (w * y - z * x)
        t2 = +1.0 if t2 > +1.0 else t2
        t2 = -1.0 if t2 < -1.0 else t2
        Y = math.degrees(math.asin(t2))

        t3 = +2.0 * (w * z + x * y)
        t4 = +1.0 - 2.0 * (y * y + z * z)
        Z = math.degrees(math.atan2(t3, t4))

        return X, Y, Z And I use them as follow: import numpy as np
euler_Original = np.random.random(3) * 360).tolist() # Generate random rotation angles for XYZ within the range [0, 360)
quat = euler_to_quaternion(euler_Original[0], euler_Original[1], euler_Original[2]) # Convert to Quaternion
newEulerRot = quaternion_to_euler(quat[0], quat[1], quat[2], quat[3]) #Convert the Quaternion to Euler angles

print (euler_Original)
print (newEulerRot) The print statements print different numbers for euler_Original and newEulerRot which I don't want to be the case. For example if euler_original contains numbers like (0.2, 1.12, 2.31) in radians I get this Quaternion --> [0.749, 0.290, -0.449, 0.389] and converting the Quaternion to Euler angles gives me this --> (132.35, 64.17, 11.45) which is pretty wrong. I wonder how I can fix this? Although I'm interested in getting the above code to work by making changes to it but, I would rather learn how to set up the equations correctly. This way I would know how I can get the correct Quaternions even if the order of rotations (XYZ --> YZX etc) for applying Euler angles is changed.","['python', 'coordinate-systems', 'geometry', 'quaternions', 'rotations']"
2975137,"Show that $f_n \to f$ over $\| \cdot \|_\infty$ implies $f_n \to f$ over $\| \cdot \|_2$ and is $(C[a,b], \|\cdot\|_2)$ Banach?","Exercise : Over the space $C[a,b]$ we define the norm $$\|f\|_2 = \sqrt{\int_a^bf(x)^2\mathrm{d}x}$$ (i) Show that if the sequence $(f_n)$ converges to $f$ with respect to $\| \cdot \|_\infty$ , then it also converges to $f$ with respect to $\| \cdot \|_2$ . (ii) Is $(C[a,b], \| \cdot \|_2)$ a Banach space ? Attempt : (i) Let $(f_n)$ be a sequence defined over $C[a,b]$ that converges to $f$ with respect to the norm $\| \cdot \|_\infty$ . Then, this means that $\exists n_0 \in \mathbb N$ : $$\|f_n-f\|_\infty< e \Leftrightarrow \max_{x \in [a,b]}|f_n(x) - f(x)| < \epsilon \; \forall n \geq n_0$$ Now, let's consider the quantity $\| f_n - f \|_2$ . This is defined as : $$\|f_n - f\|_2 = \sqrt{\int_a^b (f_n-f)^2(x)\mathrm{d}x}=\sqrt{\int_a^b[f_n(x)-f(x)]^2\mathrm{d}x}$$ Now, the last expression can be rewritten as : $$\sqrt{\int_a^b[f_n(x)-f(x)]^2\mathrm{d}x}=\sqrt{\int_a^b |f_n(x) - f(x)|^2\mathrm{d}x}$$ But, it is : $$|f_n(x)-f(x)|<\max_{x \in [a,b]}|f_n(x)-f(x)| < \epsilon \; \forall n \geq n_0$$ Now, since $|f_n(x)-f(x)| \geq 0$ and $\epsilon >0$ , we yield : $$|f_n(x)-f(x)|<\epsilon \Rightarrow |f_n(x)-f(x)|^2 < \epsilon^2 \equiv \epsilon' \; \forall n\geq n_0$$ And by integrating from $a$ to $b$ since $f_n, f$ are defined over $C[a,b]$ : $$\int_a^b|f_n(x)-f(x)|^2\mathrm{d}x < \int_a^b\epsilon'\mathrm{d}x=\epsilon'(b-a) \equiv \epsilon'' \forall n\geq n_0$$ Thus, we have $\|f_n-f\|_2 < \epsilon'' \; \forall n\geq n_0$ , which means that $(f_n)$ converges to $f$ with respect to the $\|\cdot\|_2$ norm. Question : Is my approach to (i) correct ? How would one approach (ii) though ?","['banach-spaces', 'normed-spaces', 'functional-analysis', 'real-analysis']"
2975161,triple dual space and more and more,"Let $X$ be a normed space. And Define $X^{(n)}$ by $X^{\overbrace{*****....}^{n\ times}}$ where $X^*$ means the dual space of $X$ My question is : Is there some space $X$ such that a sequence with the initial space $X$ get bigger and bigger infinitely? i.e for all $n>m$ , $X^{(n)}>X^{(n)}$ or, indepedent of choice of $X$ , does $X^{(n)}$ become a reflexive space for some $n$ ?. I'm very wondering this, but I don't know how to search about this. Please let me know some results, search-keyword or anything about this.","['functional-analysis', 'dual-spaces']"
2975164,Reference book on differential geometry,Which books should a physics major student read before undertaking a course in differential geometry ? Which books on differential geometry are most suitable for a first reading ?,"['multivariable-calculus', 'calculus']"
2975172,"Compact convergence and compact-open topologies on $\mathcal{C}(X,Y)$ coincide when $Y$ is metric [Proof Verification]","As the title states, I'm currently working through the following exercise: Let $(X,\tau)$ be a topological space and $(Y,d)$ a metric space. Then, the compact convergence and compact-open topologies on $\mathcal{C}(X,Y)$ coincide. For completeness sake: the compact-open topology is generated by the sub-basis $$
\{S(K,U) : K \subseteq X \text{ compact}, U \subseteq Y \text{open}\}
$$ with $S(K,U) = \{ f \in \mathcal{C}(X,Y) : f(K) \subseteq U\}$ , and the compact convergence topology is generated by the basis of open balls restricted to each compact set, $$
\{B_K(f,\varepsilon) : \varepsilon > 0 , K \subseteq X \text{ compact}\}
$$ with $B_K(f,\varepsilon) = \{f \in \mathcal{C}(X,Y) : d(f(x),g(x)) < \varepsilon \ (\forall x \in K)\}$ . What follows is my attempt. I would highly appreciate any comments on whether it is correct, and in that case, on how to make this argument more elegant. Let's denote $\tau_{co}$ and $\tau_{c}$ for the compact-open and compact convergence topologies respectively and see both inclusions. ( $\tau_{co} \subseteq \tau_c$ ): let $S(K,U) \in \tau_{co}$ . It suffices to see that $S(K,U)$ is open with respect to the compact convergence topology. Let $f \in S(K,U)$ . Now, by defnition this means that $f(K) \subseteq U$ and so $f(K) \cap U^c = \emptyset$ . Since $K$ is compact, $f(K)$ is compact and so it is closed in $Y$ (because it is metric). Hence we have that $\delta_f := d(f(K),U^c)$ is positive. This implies that $ B_K(f,\delta_f) \subseteq S(K,U)$ . In effect, by the contrapositive if $g \in \mathcal{C}(X,Y)$ is such that $g(K) \cap U^c$ is non empty, then taking $g(x) \in g(K) \cap U^c$ we get $$
d(f(x),g(x)) \geq d(f(K),U^c) = \delta_f.
$$ This proves that $B_K(f,\delta) \subseteq S(K,U)$ and so $$
S(K,U) = \bigcup_{f \in S(K,U)}B_K(f,\delta_f)
$$ is open for $\tau_c$ . ( $\tau_{c} \subseteq \tau_{co}$ ): with the same idea, let $B_K(f,\varepsilon)$ be an open set for $\tau_{co}$ and let's see that for each $g \in B_K(f,\varepsilon)$ , there exists $S(K',U') \in \tau_{co}$ with $g \in S(K',U') \subseteq B_K(f,\varepsilon)$ . Since $K$ is compact and the mapping $$
K \hookrightarrow X \xrightarrow{1 \times 1} X \times X \xrightarrow{(f,g)} Y \times Y \xrightarrow{d} \mathbb{R}_{\geq 0}
$$ is continuous, it attains a maximum, $$
\delta_g :=  \max_{x \in K}d(f(x),g(x)) < \varepsilon.
$$ Now, let $\mu = \varepsilon - \delta_g$ , and note that the set $$
U := \{y \in Y  : d(y,g(k)) < \mu \ , \ (\forall k \in K)\}
$$ is open: if $y\in U$ then $m_y := \max_{k \in K}d(y,g(k)) < \mu$ . Thus, if $d(z,y) < \mu -m_y$ , $$
d(z,g(k)) \leq d(z,y) + d(y,g(k)) < \mu - m_y + m_y = \mu
$$ and therefore $B_{m_y}(y) \subseteq U$ . Finally if $h \in S(K,U)$ , then for each $x \in K$ we have that $$
d(h(x),f(x)) \leq d(h(x),g(x)) + d(g(x),f(x)) < \mu + \delta_g = \varepsilon,
$$ which proves that $S(K,U) \subseteq B_K(f,\varepsilon)$ . Thoughts?",['general-topology']
2975238,Can a product of unbounded functions be a bounded function?,"I am supposed to answer the question if a product of unbounded functions can be a bounded function. I have found two unbounded functions (direct proportionality and inverse proportionality) and their product is a bounded function, thus I have found a counter example. Is my solution correct? Thanks","['functions', 'functional-analysis']"
2975304,Largest circle between lines through point [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question Calculate circle through a point, with its center on a line I am trying to make a program in java that calculates the largest circle between two tangential lines through a point that is not on one of the lines. I have the coordinates of A, B, C and D (the point where the circle goes through), the angle between AB and AC and I already know how to get the bisector. If I could calculate the centers of both circles, I could calculate their radii, so I know which circle is the biggest. So my question is: How can I get the coordinates of the center of both circles?","['trigonometry', 'circles']"
2975305,Inverse of a primitive recursive bijection [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . The community reviewed whether to reopen this question last year and left it closed: Original close reason(s) were not resolved Improve this question Is it true or false that the inverse of a primitive recursive bijection $f: \mathbb{N} \to \mathbb{N}$ is also primitive recursive (pr)?","['computability', 'discrete-mathematics', 'computer-science']"
2975306,Proving the determinant of the derivative of a function can not be zero on any open set.,"I'm working on the following problem for my multivariable calculus course: Let $f: R^2 \to R^2 $ be a $C^1$ function such that for each $y \in R^2$ , the set $f^{-1}(y)$ is finite. Show that $\det Df(x)$ cannot vanish identically on any open subset of $R^2$ I'm assuming I need to use the inverse function theorem somewhere, but I cannot find a place to apply it.","['multivariable-calculus', 'analysis']"
2975356,What is the graph coloring problem that Paul Erd≈ës offered $25 to solve circa 1979?,"I know that what I am asking is not a mathematics question, but a question about a mathematics question, so if it is inappropriate and you're thinking to downvote, I would appreciate a comment first, so that I can delete it or change it (I don't have a lot of reputation here). I heard something about a \$25 prize offered by Paul Erd≈ës for determining the number of colors required for coloring graphene, and I wanted to know what the exact question is, because I thought Erd≈ës didn't spend a lot of money (and \$25 back then was about \$100 now). However, the only thing I could find about it was some text in German, and I tried Google translate but the precise description of the problem is not clear: 1979 l√∂ste er ein Problem von Paul Erd≈ës √ºber die F√§rbung von Graphen
  (von Erd≈ës mit 25 Dollar dotiert). Er benutzte einen Computer, um eine
  ebene Menge mit 6448 Punkten ohne gleichseitige Dreiecke der L√§nge 1
  zu konstruieren, deren zugeh√∂riger Graph (Punkte wurden jeweils
  verbunden falls Abstand 1) nicht mit drei Farben f√§rbbar war
  (chromatische Zahl 4), entgegen der Vermutung von Erd√∂s und zu dessen
  √úberraschung. I searched so many things such as ""graphene chromatic number Erdos"" and ""graphene graph coloring Erdos"" and similar searches but absolutely nothing came up!","['coloring', 'graph-theory', 'combinatorics', 'discrete-mathematics', 'math-history']"
2975358,Is there an idempotent element in a finite semigroup?,"Let $(G,\cdot)$ be a non-empty finite semigroup . Is there any $a\in G$ such that: $$a^2=a$$ It seems to be true in view of theorem 2.2.1 page 97 of this book (I'm not sure).  But is there an elementary proof? Theorem 2.2.1. [R. Ellis] Let $S$ be a compact right topological semigroup. Then there exists an idempotent in it. This theorem is also known as Ellis‚ÄìNumakura lemma .","['finite-semigroups', 'idempotents', 'alternative-proof', 'abstract-algebra', 'semigroups']"
2975376,"Intuitively, how is the proof of IVT with connectedness equivalent to the proof with completeness?","I just learned the proof of IVT in general topology and it seems very different than the one I learned in Analysis (actually, the Analysis IVT is a bit stronger, I guess, since it asserts that the point occurs in some interval). Does this mean that connectedness somehow encompasses the notion of completeness?","['general-topology', 'analysis']"
2975390,Change of variables to make this differential equation separable: $\frac{dy}{dx}=F\left(\frac{ax+by+c}{dx+ey+f}\right)$,"I'm trying to define a change of variables to make the next differential equation separable: $$\frac{dy}{dx}=F\left(\frac{ax+by+c}{dx+ey+f}\right)$$ assuming that $ae-bd=0$ . I tried setting $u=ax+by+c$ and $v=dx+ey+f$ to make the right side of the equation homogeneous-like, but then i got confused trying to compute $\frac{dv}{du}$ (using that $ae-bd=0$ ). I also tried the substitution $u=\frac{ax+by+c}{dx+ey+f}$ , and in this case I used the hypothesis that $ae-bd=0$ to deduce the next expresions for the derivatives of $u$ : $$\frac{du}{dx}=\frac{af-cd}{(dx+ey+f)^2}$$ and $$\frac{du}{dy}=\frac{bf-ce}{(dx+ey+f)^2}$$ but I don't know how to use this info. Any hints will be appreciated.",['ordinary-differential-equations']
2975395,How to find the general solutions of the following second-order ODE?,"I was wondering if there is a general solution of the following second-order homogeneous ODE. $$
ty''(t)+(2-t)y'(t)=(1+\frac{\lambda}{t})y(t)\quad t>0
$$ where $\lambda$ is a positive constant.",['ordinary-differential-equations']
2975469,do 3D perpendicular lines have negative reciprocal slopes?,"it's stated in this answer that the directional vector is a nice analog for the slope in $3D$ space $‚ü®x,y,z‚ü©=‚ü®x_0,y_0,z_0‚ü©+t‚ü®m_x,m_y,m_z‚ü©$ now suppose I have two perpidicular lines $L1$ and $L2$ where $L_1$ is $(a,b,c) + t (m_x,m_y,m_z)$ . The slopes of the perpendicular lines should be the negative reciprocals of each other thus I tried to define $L2$ as $‚ü®x_2,y_2,z_2‚ü© = (a,b,c) + t (-1/m_x,-1/m_y,-1/m_z)$ for many reasons, clearly that is not true, e.g. the dot product between the two directional vectors would equal $-3$ not $zero$ ,  is the directional vector the same thing as the slope? do the perpendicular lines only have negative reciprocal slopes in $2D$ space? if not, what would be a nice analog to explain that fact in $3D$ ?","['algebra-precalculus', 'geometry']"
2975509,Probability Question for 2-Sided Coin (Verification),"Suppose you and your friends have a two-sided coin each. Your coin lands Heads with probability $\frac{1}{6}$ , while your friend's coin lands Heads with probability $\frac{3}{4}$ . The two coins are independent of one another. Suppose you play a game where you both flip your coins once, and if they both land on the same side (i.e., both Heads or both Tails) you get $x from your friend, but if they land on different sides, then your friend gets 2 dollars from you. What is the minimum integer value of $x$ for which your expected total winnings after 3 rounds of this game are positive (i.e., you are expected to make money rather than lose some)? Hint: Let $W$ denote your total winnings after 3 rounds of this game. What values of $W$ are possible? This is what I have so far: Your Coin: Probability of Heads P(H1) = $\frac{1}{6}$ ; P(T1) = $\frac{5}{6}$ Your friends coin: Probability of Heads P(H2) = $\frac{3}{4}$ ; P(T2) = $\frac{1}{4}$ For a given round: $W$ = you win : Both heads or both sides $L$ = you lose : landing on different side P(W) = Probabilty of Both heads or Both tails = P(H1H2) + P(T1T2) = P(H1)P(H2) + P((T1)P(T2) = $\frac{1}{6}$ ‚Ä¢ $\frac{3}{4}$ ‚Ä¢ $\frac{5}{6}$ ‚Ä¢ $\frac{1}{4}$ = $\frac{1}{3}$ P( $L$ ) = Probability of landing different sides = 1- Probabilty of Both heads or Both tails = 1- $\frac{1}{3}$ = $\frac{2}{3}$ P( $W$ ) = $\frac{1}{3}$ P( $L$ ) = $\frac{2}{3}$ The game is played for three rounds : Let $Y$ be the number of rounds you win Then possible values of $Y$ = 0,1,2,3; $W$ : Total winning after three rounds when $Y$ = 0 ; You lose 3 rounds and lose 3*2 = 6$ ; W = 0x - 6 = -6 Y=1 ; You win one round and lose 2 round ; W = 1x - 4 = x - 4 Y=2 ; You win 2 rounds and lose one round : W= 2x -2 Y=3 ; You win all three rounds W = 3x - 0 = 3x If Y is random variable representing number of wins in 3 rounds with probability of winning a round: p= $\frac{1}{3}$ and q =1 -p = $\frac{2}{3}$ ; If expected value of winning after 3 rounds to be positive [ex) x - 4 > 0 = x > 4] Then, when x > 4 then expected value of winning after 3 rounds to be positive when value of x = 5 ; then expected value of winning is x-4 ex) 5-4 = 1 Minimum integer value of x > 4 is 5 Minimum integer value of x = 5 for which you expected total winnings after 3 rounds of this game are positive.","['statistics', 'proof-verification', 'probability-distributions', 'probability-theory', 'probability']"
2975602,"In a shuffled deck, are the probabilities of finding one pair of cards and a second pair of cards independent events?","This question caused a massive fight in a number theory class I was in a few years ago. Class was split, and no one changed their opinion. Curious if it is obvious to outsiders, or if anyone can offer any proof. Course was split with most poker and blackjack players arguing these are dependent events, with most others claiming they were independent and you could simulate relevant events with an infinite deck. Take a standard deck with no jokers. 52 cards. Four aces, four deuces, four threes, and so on. Shuffle the deck. Draw cards until you spot a match, ie, a card with the same numerical value as the last card drawn. Does drawing such a match affect the odds of drawing a subsequent match later in the deck? Clarifying note: I call a match a ""pair"" in the title, but ""match"" is more precise. Drawing three in a row counts as two matches, but most people would not call that two pairs.","['conditional-probability', 'poker', 'combinatorics', 'probability']"
2975606,Using termwise (term-by-term) differentiation on an infinite series to satisfy a differential equation.,"I have a question which asks me to use termwise differentation on the series $$\sum_{n=0}^\infty\frac{(-1)^nx^{2n}}{(n!)^22^{2n}}$$ to show that it satisfies the differential equation $$x^2y''+xy'+x^2y=0$$ I dont understand what this question is asking me to do. I have found the interval and radius of convergence and the first 3 terms of this in previous questions if that is relevant at all?
Can someone explain this to me or the method etc so that I know how to do complete it?","['ordinary-differential-equations', 'taylor-expansion', 'sequences-and-series', 'power-series', 'derivatives']"
2975655,What is the best way to tessellate sphere into equal area in any level of detail? HEALPix or Geodesic Grid or another method?,"I want to tessellate sphere into a grid in my 3D world map. There was 2 ways that I was consider right now, HEALPix and Geodesic If I use it specifically for world map that could be zoom into any level of detail; What would be the better method and what is drawback or limitation of each method?","['spheres', 'geodesic', 'tessellations', 'geometry', 'cartography']"
2975734,Show that the limit exists or does not exist [duplicate],"This question already has an answer here : Show that the limit does not exist $\lim_{(x, y) \to (0,0)}\frac{5x^2}{x^2 + y^2}$ [duplicate] (1 answer) Closed 5 years ago . $\lim_{(x, y) \to (0,0)} \frac{5x^2}{x^2 + y^2}$ let $y = 0$ $\lim_{x \to 0} \frac{5x^2}{x^2} = 5$ let $y = x$ $\lim_{x \to 0} \frac{5x^2}{2x^2} = \frac{5}{2}$ Since different values the limit does not exist. Would this be right?",['multivariable-calculus']
2975798,"The absolute value function $|\cdot|$ is elementary, but not differentiable?","As usual, define the absolute value function $|\cdot|:\mathbb R \rightarrow \mathbb R$ by $$|x| = \left\{
\begin{array}{ll}
      x & \text{for } x \geq 0,\\
      -x & \text{for } x < 0.\\
\end{array} \right.$$ Observe that the absolute value function can also defined by: $$|x| = \sqrt {x^2}.$$ And so by ProofWiki's definition of elementary functions , the absolute value function is the composition of two elementary functions and is itself elementary. According to Wikipedia , the set of elementary functions ""is also closed under differentiation"". I believe this implies the claim that every elementary function is differentiable. But I know that the absolute value function isn't. What is the flaw/error in the above argument? Addendum: I also found in Edwards and Larson ( Calculus , 2018 ) the claim that ""you can differentiate any elementary function"".","['elementary-functions', 'derivatives']"
2975802,$f = g$ almost everywhere implies $f =g$ for any continuous function $f$ and $g$.,"Let $f,g : [0,T] \to X$ be continuous $X$ -valued functions for Banach Space $(X,||\, \cdot\,||)$ . Suppose that $f = g$ almost everywhere in $C([0,T],X)$ , then $f = g$ in $C([0,T],X)$ . This is my attempt so far : First, $f = g$ almost everywhere in $C([0,T],X)$ means $\forall t \in [0,T]\backslash M, f(t) = g(t)$ for some $M$ measure zero set. Therefore, our claim is $M \neq \emptyset$ . By contradiction, suppose there exists $t_{1} \in M\subset [0,T]$ such that $f(t_{1}) \neq g(t_{1})$ . Then, we know that $||f(t_{1}) - g(t_{1})|| \neq 0$ . Furthermore, from our assumption, we know that $\forall t \in [0,T]\backslash M, ||f(t) - g(t)|| = 0$ . However, since $f$ and $g$ are in $C([0,T],X)$ , then $\forall \varepsilon > 0, \exists \delta > 0$ such that $\forall t \in (t_{1}-\delta, t_{1}+\delta), f(t)\neq g(t)$ . Therefore, $\forall t \in (t_{1}-\delta, t_{1}+\delta), ||f(t) - g(t)||\neq 0$ . Fix $\varepsilon = \varepsilon_{1}$ to obtain $\delta = \delta_{1}$ . Then, $(t_{1}-\delta_{1},t_{1}+\delta_{1}) \subset M$ which means measure of $M$ is not zero. This is a contradiction and therefore $M = \emptyset$ which implies $\forall t \in [0,T], f(t) = g(t)$ in $C([0,T],X)$ . So my question is where did I make any mistake? Also, I want to make this argument more rigorous so I would like to ensure $f(t) \neq g(t)$ in $(t_{1} - \delta_{1}, t_{1} + \delta_{1}$ ). Is my reasoning enough to ensure that? I tried to use $\varepsilon_{1} = \frac{1}{2}||f(t_{1}) - g(t_{1})||$ but I am unsure whether I am correct or not. Any help is very much appreciated! Thank you!","['continuity', 'measure-theory', 'banach-spaces', 'almost-everywhere']"
2975828,"Computing Fr√©chet derivative of $F(f)(x) = \int^{x}_{0} \cos(f(t)^{2})dt, x \in [0,1]$","Let $X = \mathcal{C} \left( [0,1] \right)$ be the Banach space of continuous functions on $[0,1]$ (with the supremum norm) and define a map $F : X \rightarrow X$ by $$F(f)(x) = \int^{x}_{0} \cos(f(t)^{2})dt, x \in [0,1].$$ Show that $F$ is Fr√©chet differentiable and compute the Fr√©chet derivative $DF|_{f}$ for each $f \in X.$ So far I have the following. Using the identity $\cos(A+\varepsilon B)=\cos(A)-\varepsilon B\sin(A)+\mathcal{O}(\varepsilon^2).$ We have \begin{align*}F(f+h)(x)
&=\int^{x}_{0} \cos((f(t)+h(t))^{2})dt \\
&=\int^{x}_{0}\cos(f^2(t))-h(t)[2f(t)h(t)]\sin(f^2(t))+\mathcal{O}(h^2(t))dt \\
&=F(f)(x)+\int^{x}_{0}-2f(t)(h(t)\text{sin}(f^2(t))-h^2(t)\sin(f^2(t))+\mathcal{O}(h^2(t))dt \\
\end{align*} Let $T(h)=\int^{x}_{0}-2f(t)(h(t)\sin(f^2(t))dt$ . I am able to show that $T(h)$ is a linear map. Then we have $$F(f+h)(x)-F(f)(x)=T(h)+\int^{x}_{0}\mathcal{O}(h^2(t))dt $$ I am confused about the Big- $\mathcal{O}$ notation and I do not understand what it would mean to integrate over it. Also, am I on the right track? Thank you in advance for any help provided.","['operator-theory', 'asymptotics', 'real-analysis', 'frechet-derivative', 'functional-analysis']"
2975830,Minimal sufficient statistics for Cauchy distribution,"I'm trying to find the minimal sufficient statistics for a Cauchy distributed random sample $X_1,...,X_n$ , here \begin{equation}
f(x|\theta) = \frac{1}{\pi[1+(x-\theta)^2]}
\end{equation} I begin by guessing that the order statistics are the minimal sufficient statistics (first of all, they are sufficient). Then I try to prove that \begin{equation}
\frac{f(X|\theta)}{f(Y|\theta)} = \prod_{i=1}^n\frac{1+(y_i-\theta)^2}{1+(x_i-\theta)^2}=C
\end{equation} Where $C$ is a constant of $\theta$ , I want to prove that the above equation holds iff $T(X)=T(Y)$ , where $T(X)$ is the order statistics of $X$ . I am really stuck and don't know how to show why that if it holds then $T(X)=T(Y)$ , can anyone help me on this proof? p.s., a similar thread minimal sufficient statistic of Cauchy distribution discusses the problem but offers no proof for the minimal sufficiency.","['statistical-inference', 'statistics', 'polynomials', 'order-statistics']"
2975845,Why is a simulation of a probability experiment off by a factor of 10?,"From a university homework assignment: There are $8$ numbered cells and $12$ indistinct balls. All $12$ balls are randomly divided between all of the $8$ cells. What is the probability that there is not a single empty cell ( $i.e.$ each cell has at least $1$ ball)? The answer is $\large\frac{\binom{11}{7}}{\binom{19}{7}}$ which is about $0.0065$ . I reached this result independently, and it was confirmed by the official homework solution of the university. A friend of mine and I independently wrote Python simulations that run the experiment many times (tested up to $1,000,000$ ). We used both Pythons' random generator and several randomly generated lists from www.random.org. Results were similar and consistently hovering around $0.09$ which is a factor of $10$ or even a bit more off from the expected theoretical result. Have we made some wrong assumptions?
Any ideas for this discrepancy? P.S.: Here is the Python code that I wrote, and maybe there is some faulty logic there. def run_test():
    global count, N

    def run_experiment(n_balls, n_cells, offset):
        cells = [0] * n_cells
        # toss balls randomly to cells:
        for j in range(n_balls):
            cells[random.randrange(0, n_cells)] += 1
            # cells[int(lines[offset + j])] += 1
        cells = sorted(cells)
        # print(cells)

        # check if there is an empty cell. if so return 0, otherwise 1:
        if cells[0] == 0:
            return 0
        return 1

    count = 0
    N = 1000000
    offset = 0
    N_CELLS = 8
    N_BALLS = 12
    # iterate experiment
    for i in range(N):
        result = run_experiment(N_BALLS, N_CELLS, offset=offset)
        count += result
        offset += N_CELLS

    print(""probability:"", count, ""/"", N, ""(~"", count / N, "")"")","['python', 'simulation', 'probability']"
2975861,Prove that two sequences of integers that have the same sum and product must be the same.,"Given two sequences of nondecreasing distinct positive integers such that $$x_1 + x_2 + ... + x_i = y_1 + y_2 + ... + y_i , i>0$$ and that $$x_1x_2 ... x_i = y_1y_2 ... y_i$$ Prove/disprove that the sequences are equal i.e. $$x_1 = y_1, x_2 = y_2, ... , x_i = y_i$$ I started with Let $x_1x_2 ... x_i$ be $A$ .
If $A$ is prime, $x_1 = A = y_1$ (since $A$ cannot be factored any more) and we are done. What I don't know is what happens when $A$ is not prime. Intuitively, it sounds true, and I cannot find any counter examples.","['number-theory', 'discrete-mathematics']"
2975881,Proof of a bizarre identity,"Prove That $$ \prod_{r=1}^{n} (1-x^r) = 1 - \sum_{r=1}^{n} (1-x)\cdot(1-x^2)\cdot \ldots\cdot(1-x^{r-1})x^r $$ I found this identity through experimentation with Wolfram Alpha while trying to find a formula for the product on the LHS. I have been able to prove it using induction, but I'm interested in how one would derive such identities. I'm seeking a direct proof without induction. Any help will be appreciated. Thanks.","['products', 'combinatorics', 'sequences-and-series']"
2975898,Functional equation - Cyclic Substitutions,"Please help solve the below functional equation for a function $f: \mathbb R \rightarrow \mathbb R$ : \begin{align}
&f(-x) = -f(x) , \text{ and } f(x+1) = f(x) + 1, \text{ and } f\left(\frac 1x\right) = \frac{f(x)}{x^2} \\
&\text{ for all } x \in \mathbb R \text{ and } x \ne 0 .
\end{align} I know this will be solved by cyclic substitutions, but I'm unable to figure out the exact working. Can someone explain step wise?","['functional-equations', 'systems-of-equations', 'substitution', 'functions', 'algebra-precalculus']"
2975924,why average value of a function is not calculated by using the formula $\frac{f(a)+f(b)}{2}$?,"I know the formula for calculating the average value of a function as $$ \frac{1}{b-a}\int_a^b f(x)\,dx$$ But in elementry level maths and physics problems we generally use a very simple approach to find the average of a value,by taking sum of two values and then dividing them from 2. (for example,if the initial velocity of a car is ${10ms^{-1}}$ and after 10 seconds the final velocity is ${20ms^{-1}}$ then the average velocity will be ${ \frac {10 + 20}{2}=15ms^{-1}}$ ) Let us took a very simple identity function ${f(x)=x}$ ,we want to find the average value between ${2}$ and ${4}$ then by integration; $$ \frac{1}{b-a}\int_a^b f(x)\,dx$$ $$= \frac{1}{4-2}\int_2^4 f(x)\,dx$$ $$= \frac{1}{2}\left(\frac{4^2-2^2}{2}\right)$$ $$= \left(\frac{16-4}{4}\right)=3$$ But I can also calculate this by using a general formula for average of two values $$\frac{f(a)+f(b)}{2}$$ $$\frac{4+2}{2}=3$$ By both methods we get the same value,so my question is why we don't use $\frac{f(a)+f(b)}{2}$ for calculating the average value of a function (which seems to be very simple) ?",['functions']
2975943,Expected value estimate.,"If we have $P(a< x<b)=1$ , where $P$ denotes probability,and $0<a<b$ then $$E(X)E(\frac{1}{X})\le \frac{(a+b)^2}{4ab}$$ I have tried some trivial cases , uniform distribution and the case $P(x=a)=P(x=b)=\frac{1}{2}$ , I found that in the later case we get the maximum value of right hand side of the inequality we wanted,or it‚Äôs the extreme case.ÔºàAlthough the case doesn‚Äôt satisfy the condition, I don‚Äôt know if it is useful or not.) But I don‚Äôt know how to deal the general cases.
Even though the special case:the maximum value of the following functional. $$\max_{\int_a^bf(x)dx=1}\int_a^b xf(x)dx\int_a^b\frac{1}{x}f(x)dx$$ Any suggestion will be appreciated.","['calculus', 'analysis', 'probability', 'real-analysis']"
