question_id,title,body,tags
2779148,Bounds for $\int_0^1 x^{\cos x+\sin x} dx$,"Hello I am trying to show that $\frac{2}{5} \le \int_0^1 x^{\cos x+\sin x}dx \le \frac{1}{2}$ or to get better bounds. Here is my try:$$I=\int_0^1 x^{\cos x}\cdot x^{\sin x} \,dx$$ By Cauchy inequality $$I^2\le \int_0^1 x^{\cos x} \,dx \cdot \int_0^1 x^{\sin x} \,dx$$ for $x\in [0,1]\,$ $$ 0\le \cos x \le 1$$ $$\,0\le \sin x \le 1$$ thus $$x^{\cos x} \le x$$ $$x^{\sin x} \le x$$ so $$\int_0^1 x^{\cos x} \,dx \le \frac{1}{2}$$ $$\int_0^1 x^{\sin x} \,dx \le \frac{1}{2}$$  Therefore $$I\le \sqrt{\frac{1}{2}\cdot \frac{1}{2}} \le \frac{1}{2}$$ Is this correct? And what about the lower bound?","['integration', 'upper-lower-bounds']"
2779228,Simplify a double integral,"I'm having trouble with proving that the double integral $$\int_{-1}^1\int_{x^2}^1 f\Big(\frac{y}{x}\Big)\mathrm{d}y \mathrm{d}x $$ can be simplified to:
 $$\int_{-1}^1 f(t) \frac{t^2}{2} \mathrm{d}t + \int_{-1}^1 f\Big(\frac{1}{t}\Big) \frac{1}{2} \mathrm{d}t $$ This exercice is part of the section ""double integrals in polar coordinates"", but I can't see the link.
Thank you in advance!","['real-analysis', 'integration']"
2779275,Theoretical integral curve - fields lemma proof,"I need help or a thorough proof on understanding the following theoretical lemma regarding integral curves and fields. This is a part of the ""Introduction to Partial Differential Equations with Applications - Zachmanoglou"" . First of all, where to begin from for proving part (a) of the lemma-protasis ? Secondly, I can't comprehend part (b). Let $C$ be an integral curve of the vector field $V = (P,Q,R)$ and suppose that $C$ is given parametrically by the equations 
  $$x=x(t), \; \; y = y(t), \; \; z = z(t) \; ; \; t \in I$$
  where the functions $x(t), y(t)$ and $z(t)$  are in $C^1(I)$ and the tangent vector 
  $$T(t) = \bigg(\frac{\mathrm{d}x(t)}{dt},\frac{\mathrm{d}y(t)}{dt}, \frac{\mathrm{d}z(t)}{dt}\bigg)$$
  never vanishes for $t \in I$. (a) Show that there exists a function $μ(t)$ in $C^1(I)$ such that for every $t \in I$ it is :
  $$μ(t) \neq 0 \; \; \text{and} \; \; V(x(t),y(t),z(t)) = μ(t)T(t)$$ (b) Let $t = τ(t)$ be a solution of the differential equation
  $$\frac{\mathrm{d}t}{\mathrm{d}τ}=μ(t)$$
  where $τ$ varies over some interval $I'$ as $t$ varies over $I$. Set
  $$\bar{x}(τ)=x(t(τ)), \; \bar{y}(τ)=y(t(τ)), \; \bar{z}(τ)=\bar{z}(t(τ))$$
  Show that in terms of the new parametric representation 
  $$x = \bar{x}(τ), \; \; y = \bar{y}(τ), \; \; z = \bar{z}(τ) \; ; \; t \in I'$$
  the curve $C$ is a solution of the system of equations associated with $V$
  $$\frac{\mathrm{d}x}{\mathrm{d}τ}=P, \; \frac{\mathrm{d}y}{\mathrm{d}τ}=Q, \; \frac{\mathrm{d}z}{\mathrm{d}τ}=R$$","['integration', 'ordinary-differential-equations', 'partial-differential-equations']"
2779295,"Proof that (S(X),$\circ$) is a group.","$X$ is a non-empty set and $S$(X) := { f : X $\rightarrow$ X : f is bijective}. Investigate if (S(X),$\circ$) is a group. Notes: f is bijective which means that the function is also injective and surjective. My solution: Let f, g and h $\in$ S(X) be arbitrary. ( f $\circ$ g )$\circ$ h = f $\circ$( g $\circ$ h ) (Associative). Let x $\in$ X be arbitrary so that:
(( f $\circ$ g ) $\circ$ h ) (x) = ( f $\circ$ ( g $\circ$ h ))(x) = f $\circ$ g ( h (x)) = f ( g ( h (x)) and ( f $\circ$ ( g $\circ$ h ))(x) = f ( g ( h (x)). Now for the neutral element: Let f be arbitrary and let's assume that there exists I$_X$ (Identity of X). ( f $\circ$ I$_X$) (x) = f (I$_X$(x)). And (I$_X$ $\circ$ f ) (x) = I$_X$( f (x)). I don't know how to continue at this point. I somehow think that my solutions aren't correct and I still need to prove that (S(X),$\circ$) has an inverse element but I don't know how. Any hints or solutions guiding to the right direction I much appreciate.","['abstract-algebra', 'functions']"
2779307,Confusing of the last line of proof for Fatou's lemma,"I am reading Royden's proof on Fatou's lemma. Let ${f_n}$ be a sequence of nonnegative measurable functions on $E$ converging to $f$ pointwise almost everywhere on $E$, then $\int_E f \leq \liminf\int_E f_n$. In the very last line of Royden's proof, it says By the definition of the integral of $f_n$ over $E$, $\int_E h_n \leq \int_E f_n$ (where $h_n = \min (h , f_n)$ and $h$ is a bounded measurable function with finite support with $h \leq f$) Thus
  $$\int_E h = \lim \int_E h_n \leq \liminf \int_E f_n.$$ And I don't see how the last statement is implied, how does he apply the first line to imply the last? Just because $\int_E h_n \leq \int_E f_n$ doesn't tell me that $\inf \int_E f_n$ has to be greater than any particular $\int h_n$. How do we know that $\int_E f_{n+1}$ isn't smaller than $\int_E h_n$?","['real-analysis', 'measure-theory', 'proof-explanation']"
2779311,Examples of analysis results using probability theory,"Sometimes, nice results from analysis appear unexpectedly in probability theory. Here are a couple of examples: $1.$ If $Z \sim \mathcal{N}(0,1)$, then $Z^2 \sim \Gamma(1/2,2)$ When we want to prove this, we find that $Z^2$ has density function $x \mapsto \sqrt{2\pi}^{-1} x^{-1/2} e^{-x/2}$ for $x \geq 0$
and comparing this to the density function of the gamma $(1/2,2)$ distribution, and using the fact that $\int_{-\infty}^{+ \infty} f(x)dx = 1$ for a density function $f$, it follows that $\boxed{\Gamma(1/2) = \sqrt{\pi}}$ $2.$ If $X \sim \Gamma(\alpha_1, \beta), Y \sim \Gamma( \alpha_2, \beta)$ and $X,Y$ are independent, then $X+Y \sim\Gamma(\alpha_1 + \alpha_2, \beta)$ While proving this, one can find the identity $$\boxed{\int_0^1 u^{\alpha_1 -1}(1-u)^{\alpha_2 -1}du = \frac{\Gamma(\alpha_1)\Gamma(\alpha_2)}{\Gamma(\alpha_1 + \alpha_2)}}$$ So my question is: what are other examples where we can find interesting results from analysis (or other other branches of mathematics) using probability theory?","['big-list', 'probability-theory', 'probability', 'soft-question', 'analysis']"
2779312,Score-based rating system,"I am looking for a rating system to rate people based on their score on a competition and their previous rating. Codeforces is using a variant of Elo rating system which is pretty cool (you can read about it here ), but it is rating people by their ""rank"", not their ""score"". Has anyone encountered such a system?","['statistics', 'estimation', 'analysis']"
2779318,Evaluating $\lim_{n\rightarrow\infty} \int_{0}^{\pi} \frac {\sin x}{1+ \cos^2 (nx)} dx$,"Greetings I want to evaluate $\displaystyle\lim_{n\rightarrow\infty} \int_{0}^{\pi} \frac {\sin x}{1+ \cos^2 (nx)} dx$. Here is my try: We have that $x\in[0,\pi]$ so $$\cos(n\pi)\le \cos(nx) \le 1.$$ Here I am not sure, but if it's correct then it gives: $$\frac{1}{2}\le\frac{1}{1+\cos^2(nx)}\le \frac{1}{1+\cos^2(n\pi)},$$ giving $$\lim_{n\rightarrow\infty} \frac{1}{2}\int_0^{\pi} \sin x dx \le \lim_{n\rightarrow\infty} \int_{0}^{\pi} \frac {\sin x}{1+ \cos^2 (nx)} dx \le \lim_{n\rightarrow\infty} \int_{0}^{\pi} \frac {\sin x}{1+ \cos^2 (n\pi)} dx.$$ Since $$\int_0^{\pi} \sin x dx =2$$ By squeeze theorem we may conclude that $\displaystyle \lim_{n\rightarrow\infty} \int_{0}^{\pi} \frac {\sin x}{1+ \cos^2 (nx)} dx=1$. Could you help me evaluate this, if it's wrong?","['real-analysis', 'limits', 'proof-verification', 'integration', 'definite-integrals']"
2779332,Inner and outer factors in Hardy spaces,"Let $f(z) = \sin(z),$ viewed as a function in the hardy space $H^1.$ How do I factor $f$ into inner and outer factors? I know that the formula for the outer factor is 
$$Q_f(z)= \exp(\dfrac{1}{2\pi} \int_{-\pi}^\pi \dfrac{e^{it}+z}{e^{it}-z} \log|\sin(e^{it})| dt)$$ for $z$ in the open unit disk. This looks like a mess to compute. Is there a way to explicitly find the inner and outer factors of $\sin(z)?$ Obviously, we need to factor out the zeros of $\sin(z)$ in the unit disk, and there is precisely one zero in the unit disk, namely at $0.$So the inner factor $M_f$ contains $z.$ Now, I believe that since $\sin(z)/z$ is analytic in a larger disk than the unit disk, $\sin(z)/z$ is already an outer function. Does this mean that $Q_f(z) = \sin(z)/z?$ How does one in general find the outer and inner factors of a function $f \in H^1?$ What are some good examples to keep in mind?","['complex-analysis', 'hardy-spaces', 'harmonic-analysis']"
2779336,How to derive the put-call parity?,"The following solution seems quite vague to me as I am not too sure how they thought of putting the terms on the right hand side together and similarly for lhs. I know that max(..) is the payoff, but why are we adding the strike to it? Any help would be really appreciated!","['probability-theory', 'finance', 'probability', 'statistics']"
2779348,sum of alternating binomial,"Compute the sum $\sum_{k=0}^{n}(-1)^k k^n\binom{n}{k} $ I've seen a solution along the following lines here, page 3 : Consider $(1+x)^n=\sum_{k=0}^{n}\binom{n}{k}x^k$. ($\star$) We prove by induction that $\sum_{k=0}^{n}(-1)^k k^t\binom{n}{k}=0$ for $t< n$. We prove this by differentiating ($\star$) t times, setting $x=-1$ and using the inductive step. Now if we differentiate $(\star)$ n times we get: $n!=\sum_{k=0}^{n} k \cdot (k-1) \dots \cdot (k-(n-1)) \binom{n}{k}x^{k-n}=\sum_{k=0}^{n} k^n\binom{n}{k}x^{k-n}$ (by the inductive step). So $n!=\sum_{k=0}^{n} k^n\binom{n}{k}x^{k-n}$ and by setting $x=-1$ and multiplying by $(-1)^n$ we get $\sum_{k=0}^{n}(-1)^k k^n\binom{n}{k}=n! (-1)^n $. My question is, starting from $(1+x)^n=\sum_{k=0}^{n}\binom{n}{k}x^k$, if we differentiate it n times, most of the RHS terms will vanish ,leaving us with $n!=n!$, not $n!=\sum_{k=0}^{n} k \cdot (k-1) \dots \cdot (k-(n-1)) \binom{n}{k}x^{k-n}$. How is that a valid step and also how can the sum in question be evaluated?","['combinatorics', 'summation', 'binomial-coefficients']"
2779357,Solving for $x$ in $A = \arctan \left(\frac{\sin (x)}{ -\tan(f)\sin (e) + \cos (e)\cos(x) }\right)$,I'm trying to solve below equation and creating a formula for $x$. $$A = \arctan \left(\frac{\sin (x)}{ -\tan(f)\sin (e) + \cos (e)\cos(x) }\right)$$ where $A$ is a constant e.g. $A=5$ and $f$ and $e$ are also some given numerical values (e.g. $f=2$ and $e=3$). So far what steps I've tried: Given equation: $$A=\arctan \left(\frac{\sin (x)}{ -\tan(f)\sin (e) + \cos (e)\cos(x) }\right)$$ Moved atan to LHS: $$\frac{\sin (x)}{ -\tan(f)\sin (e) + \cos (e)\cos(x) } = \tan (A)$$ $\sin (x) = \tan (A) (-\tan(f)\sin (e) + \cos (e)\cos(x))$ $\sin (x) = -\tan (A)\tan(f)\sin (e) + \tan (A)\cos (e)\cos(x)$ $\sin (x) - \tan (A)\cos (e)\cos(x) = -\tan (A)\tan(f)\sin (e)$ Got stuck now to solve further the L.H.S part (even though I know $\tan (A)\cos (e)$ would be some numerical value after putting value of $A$ and $e$ (e.g. $2.333$) i.e. $\sin (x) - 2.333 \cos(x)$.,"['problem-solving', 'trigonometry']"
2779398,"For $n\ge4,$ prove that $F_n+1$ is not prime, where $F_n$ is $n^{th}$ Fibonacci number.","For $n\ge4,$ prove that $F_n+1$ is not prime, where $F_n$ is $n^{th}$ Fibonacci number What is the idea of the proof? I tried it by contradiction by 
letting $(1+F_n)$  to be prime $\implies$ $F_n$ is not prime $\implies$ WHAT NEXT ?","['number-theory', 'fibonacci-numbers', 'prime-numbers', 'elementary-number-theory']"
2779465,Is any group the automorphism group of some (commutative) monoid?,"The same question with groups instead of monoids is answered negatively, see here . From this question, it is true that any finite group is the automorphism group of some monoid. Moreover, what happens if we require the monoid to be commutative ? (I believe that my question with ""semigroup"" instead of ""monoid"" has the same answer – removing or adding the identity element shouldn't change so much).","['group-theory', 'monoid']"
2779471,Normal distribution governed by a Bernoulli distribution,"How would I find the distributional characteristics (mean, variance) of the following scenario: A Bernoulli random variable $X \sim B(1,p)$. If the $X = 1$, then $Y \sim N(\mu_1, \sigma_1^2)$. If the $X = 0$, then $Y \sim N(\mu_0, \sigma_0^2)$. One random variable is conditional on another.
I know the mean of this scenario is $p \mu_1 +(1-p) \mu_0$, but what is the variance? Thank you so much. edit -- based on further research, this is what I have come up with: $Y | X=1 \sim N(\mu_1, \sigma_1^2)$ $Y | X=0 \sim N(\mu_0, \sigma_0^2)$ $E(Y) = E(E(Y|X)) = p \times E(Y|X=1) + (1-p) \times E(Y|X=0) = p \mu_1 + (1-p)  \mu_0$ And, $Var(Y) = E(V(Y|X)) +V(E(Y|X))$ $E(V(Y|X)) = p \sigma_1^2 + (1-p) \sigma_0^2$ $V(E(Y|X)) = E(E(Y|X)^2) - E(E(Y|X))^2 =E(E(Y|X)^2) - E(Y)^2$ $ = p \mu_1^2 + (1-p) \mu_0^2 - (p \mu_1 + (1-p) \mu_0)^2$ $ = p(1-p) \mu_1^2 + p(1-p) \mu_0^2 - 2p(1-p) \mu_1 \mu_0 $ Hopefully this is correct?","['statistics', 'probability']"
2779480,Centralizer of $U(n)$ inside $U(nm)$,"Let $n$ and $m$ be two positive integers. There is a canonical inclusion $U(n) \rightarrow U(nm)$ given by the tensor product with the unit matrix $\mathbf{1}_m$. What is the centralizer of $U(n)$ seen as a subgroup of $U(nm)$? Edit : I was asked to provide context, so here it is, but I'm not sure it will be very illuminating for a non-physicist. Context : In physical language, you can consider a field theory with $nm$ free complex scalar fields $\phi_i$, with $i=1,\dots,nm$. The Lagrangian density is then $\sum_i \partial_\mu \phi_i^* \partial^{\mu} \phi_i$. It is left invariant under the action of the unitary group $U(nm)$, the vector $(\phi_i)_{i=1 , \dots , nm}$ being in the fundamental representation. Now you might want to gauge partially this big symmetry. Here this means you identify a subgroup $U(n) \subset U(nm)$ and consider the combinations of the $\phi_i$ that are invariant under $U(n)$. A way to visualize that is to see the fields $\phi_i$ as the entries of an $n \times m$ where $U(n)$ acts by left multiplication. Then it is obvious that there is also a group $U(m)$ which acts by right multiplication on this matrix, and the two actions commute. Therefore, the centralizer of $U(n)$ contains $U(m)$. My question can then be re-formulated as: is the centralizer equal to $U(m)$, or bigger?","['group-theory', 'classical-groups', 'lie-groups']"
2779539,Monotonic solution of first order ODE,"Let $f:\mathbb{R} \to \mathbb{R} $ be a continuous function. Let $x'(t)=f(x(t))$ be a maximal solution to the first order ODE. Show that the solution is a monotonic function. This is the classical problem when you have the intuition, but you don't know how to write it down. My first idea was to show that if it's not monotonic, then you will have two points, say $a$ and $b$ with $x(a)=x(b)$ but $x'(a)> 0$, $x'(b)<0$.","['real-analysis', 'ordinary-differential-equations']"
2779545,Ask for the rational roots of $\frac{a}{b+c}+\frac{b}{a+c}+\frac{c}{a+b}=4.$,"I consider the problem: ask the rational roots of $$\frac{a}{b+c}+\frac{b}{a+c}+\frac{c}{a+b}=4.$$
I try to use the theory of ellipic curves (If we have two rational roots, we could have the third collinear one) and calculte the genus of $\Sigma_{p}:=\{[a,b,c]: p(a,b,c)=0\}$, where $$p(a,b,c)=a(a+c)(a+b)+b(b+c)(b+a)+c(c+a)(c+b)-4(a+b)(a+c)(b+c).(*)$$ How to get the genus? I know I should use: The Riemann-Hurwitz theorem: $$2g(\Sigma)-2=B_{p}(f)-2\deg(f),$$
  where $g(\Sigma)$ is the genus of Riemann surface $\Sigma$ and $B_{p}(f)$ branch numbers of $f$ at $p$. I try to bring $(*)$ into the form $y^2=x(x-1)(x-\lambda), \lambda \ne0,1,$  since I know the genus is one. Maybe help?
$p(a,b,c)=a^3+b^3+c^3-3a^2b-3ab^2-3a^2c-3ac^2-3cb^2-3bc^2.$","['riemann-surfaces', 'algebraic-geometry', 'algebraic-number-theory', 'number-theory', 'algebraic-curves']"
2779590,What is the Galois group of sinus?,"I am trying to define the Galois group of complex functions.
For example for sinus we have the product representation: $$\sin(x) = x \prod_{k=1}^\infty \left( 1-\frac{x^2}{k^2\pi^2} \right)$$
The roots of $\sin$ are $0,\pm k \pi$, $k\in \mathbb{N}$. It is known that $\pi$ is transcendent, hence the field $\mathbb{Q}(\pi)$ is isomorphic as fields to $\mathbb{Q}(x)$. If we define $\sigma(\pi)=-\pi$ and extend it to a $\mathbb{Q}(\pi)/\mathbb{Q}$ automorphism, then the Galois Group of $\sin$ over $\mathbb{Q}$ should be $C_2$ the cyclic group. 1) Any suggestions how to define a Galois group of a complex function as a group of permutations of the roots of these functions? 2) Do you know of any other function where the ""Galois group"" as defined below gives a non trivial group other than $C_2$? For instance: What is the group defined below, for the Ramanujan theta function at $b=1$ ( https://en.wikipedia.org/wiki/Infinite_product#Product_representations_of_functions ) :
$$f(a,1) = \prod_{n=0}^\infty (1+a^{n+1})(1+a^n)(1-a^{n+1})$$ It seems that it has something to do with roots of unity. Preliminary definition:
Let $\Lambda := \{ z \in \mathbb{C} | f(z) = 0 \}$. Let $\mathbb{Q}(\Lambda)$ be the smallest subfield of $\mathbb{C}$ which contains $\Lambda$. Define the Galois group of $f$ to be
$$ Gal(f/\mathbb{Q}) := \{ \sigma \in Aut(\mathbb{Q}(\Lambda)/\mathbb{Q})| z \in \Lambda \rightarrow \sigma(z) \in \Lambda \text{ and } \sigma^{-1}(z) \in \Lambda\}$$
Notice that for polynomials, the last property is automatically fullfilled, as automorphisms commute with the polynomial in the following sense:
$ \sigma(p(z)) = p(\sigma(z))$.
The above example for $\sin$ shows that $C_2$ is a subgroup of the Galois group. Is it possible to prove given the above definition that $C_2$ equals the Galois group? Edit :
By the comments below, we should only consider the automorphisms of the field extension which leave $\mathbb{Q}$ unchanged and permute the roots of the function. In the example above we have for any root $ z = k\pi, k \in \mathbb{Z}$ that:
$$ 0 = \sigma(0) = \sigma(\sin(z)) = \sin(z) = \sin(-z) = \sin(\sigma(z))$$ 
Hence $\sigma$ maps roots to roots and is an automorphism of $\mathbb{Q(\pi)}$ which leaves $\mathbb{Q}$ unchanged.
I don't think one needs the continuity here. Or did I miss something?
I think in general ( for $z \in \mathbb{Q}(\pi)$ ) we do not have $\sigma(\sin(z)) = \sin(\sigma(z))$, so @Micah is right with his argument.
Don't know if this still gives something interesting or not. Anyway, it is not clear how $\sigma(\sin(z))$ should be defined, as it might be the case that $\sin(z)$ is not an element of $\mathbb{Q}(\pi)$ although $z$ is. (For example $\sin(\pi/4) = 1/\sqrt{2}$ is not an element of $\mathbb{Q}(\pi)$, so in general it does not make sense to define $\sigma$ on such elements.)","['complex-analysis', 'galois-theory']"
2779596,Manipulation of generating functions+quadratic equation with generating functions,Generating functions really give me a hard time. I'm trying to understand this proof. There are two things I don't see. How do you get the quadratic equation with the $P(z)$ (with straightforward manipulations)? Secondly why can one derive $P(z) = z^2 C(z)^2$?,"['generating-functions', 'discrete-mathematics']"
2779600,"Appeal for clarification of an isomorphism between $\operatorname{Aut}_c(G)$ and $\operatorname{Hom}(G,Z(G))$","I am reading an older paper by Jamali and Mousavi. On the second page there is the following proposition 2.2 I marked fourplaces in red. The first one seems like a typo: "".. for every $f$ in $\operatorname{Hom}(G,Z(G))$"" makes more sense to me. The second one "".. is an isomorphism"" - why? The map is certainly a bijection, but an isomorphism needs groups as domain and range and $\operatorname{Hom}(G,Z(G))$ is no group. What am I missing? The third says ""..$\operatorname{Hom}(G,Z(G)) \cong \operatorname{Hom}(G/G',Z(G))$.."" - again why? There is no group on either side. But even if it is only a bijection: is this obvious? Fourth mark: what implies this conclusion? All in all I am certainly missing something essential - perhaps something obvious and/or easy? Can you tell me what it is? Thank you!!","['finite-groups', 'reference-request', 'group-theory', 'automorphism-group']"
2779617,Why is this function $h(x)$ invalid?,"Let there be two functions, $f(x,y)$ and $g(x)$, such that
$$f:\Bbb{R} \times \Bbb{R} \rightarrow \Bbb{R}$$
$$g:\Bbb{R} \rightarrow \Bbb{R}$$
Our initial set is $\{0\}$. If we define the functions like this
$$f(x,y)=x \cdot y$$
$$g(x)=x+1$$
we can use them to generate $\Bbb{N_0}$ from $\{0\}$ (mathematical induction). Now, lets create function $h(x)$ such that it maps
$$\Bbb{N_0} \rightarrow S$$
where $S$ is set.
Now, why is the function $h(x)$ defined like this invalid?
$$h(0)=0$$
$$h(f(x,y))=f(h(x),h(y))$$
$$h(g(x))=h(x)+2$$
As you can see, I'm trying to use $h(x)$ (which is a recursive function) to generate $S$ from $\Bbb{N_0}$.
I know that these rules are somehow contradicting each other, but I don't know how. That's my question: why are these rules contradicting each other? This problem is taken from the book A Mathematical Introduction To Logic [2nd edition] (p. 38) by Herbert Enderton . This problem is not ""copy-pasted"" from the book, so I screenshotted it for those of you who want to see the original. You can see it here .","['elementary-set-theory', 'logic', 'functions']"
2779632,Can one know the rank of a matrix product given the rank of one of them?,"Suppose we have some matrix $Q\in\mathbb{R}^{m\times n}$. We have another matrix $T\in\mathbb{R}^{n\times n}$, which happens to be full-rank. I'm wondering if it is possible to know the rank of the matrix built as $$\hat{Q}=QT$$ My intuition says that, due to $T$ being an injection, $\mathrm{rank}(\hat{Q})=\mathrm{rank}(Q)$, but I'm not sure about this. I find it easy to arrive to this conclusion if the matrix was $\hat{Q}=TQ$, with $Q\in\mathbb{R}^{n\times m}$ but not with the $T$ at the right. Any ideas?","['matrices', 'matrix-rank', 'linear-algebra']"
2779665,Variance of X vs Variance of a binary function of X,"Let $X$ be a random variable in $[0, 1]$ and $m$ its median such that $P(X \le m) = P(X \ge m)$. Define $\beta(X)$ as
$$\beta (x) = 
\left\{ 
\begin{array}{c}
\begin{align*}
1&,\space X \ge m; \\ 
0&,\space otherwise.
\end{align*}
\end{array}
\right. 
$$
(a) Is it true that $Var(X) \le Var(\beta(X))$? (b) What if $X$ is continuous? Where I got stuck: If $X$ is a discrete random variable, $\beta(X)$ is just Bernoulli with $p = 0.5$ and $Var(\beta(X)) = 0.25$. I couldn't come up with any discrete X whose variance is bigger than that. Tried simple Bernoulli; $X = 0.5^{i-1}$ with $p(x_{i})=0.5^i$. All variances are smaller than or equal to 0.25. However, I couldn't come up with a formal proof either. Reasoning for (b) will depend on the proof/counterexample with (a) I guess. Please help! P.S. First time poster hear. Apologies if something's wrong with my post","['variance', 'probability', 'random-variables', 'probability-distributions']"
2779681,Expected number of die rolls to get 6 given that all rolls are even.,"A fair 6-sided die is rolled repeatedly in till a 6 is obtained. Find the expected number of rolls conditioned on the event that none of the rolls yielded an odd number I had tried to figure out what will be the conditional distribution of $\frac{X}{Y}$ but I can't solved it yet
Where $X $ is the face numbered 6 is obtained and $Y$ is the event only even number is occured","['expectation', 'conditional-expectation', 'probability']"
2779686,If $ f(x)= \int_0^x x^2\sin(t^2)dt$ then $f'(x)= ?$,"Since the integral doesn't depend on $x$, I take $x^2$ out of the integral and take the derivative of a constant times a function. The final result is $x^2\sin(x^2)$. I saw one suggested answer which gives a different answer: $x^2\sin(x^2)+2xf(x)$, which is the derivative using the product rule. Which one is correct? Please explain, thanks.","['derivatives', 'integration', 'calculus']"
2779719,How to solve this particular difference equations?,"Considere a real polynomial function $P_n\left(x\right)$ of two variable, when $n$ is a discrete variable and $x$ a continuous variable where $P_0\left(x\right)=1$ and satisfies the following recurrence relations: $$P_{n+1}\left(x\right)=\left(x+1\right)\cdot P_n\left(x\right)+x\cdot\frac{d}{dx}\left(P_n\left(x\right)\right).$$ How could I find the explicit solution $P_n\left(x\right)$ for this expression? And what name do these types of equations receive?","['recurrence-relations', 'ordinary-differential-equations', 'delay-differential-equations']"
2779779,$A+3I$ is nonsingular,"If $A^2=A+2I$, show that $A+3I$ is nonsingular. If $p$ is a polynomial that annihilates A, how does the minimum poly of A and $p$ relate? Can someone help me with this? I’m not quite familiar with annihilating polynomial.","['minimal-polynomials', 'linear-algebra', 'inverse']"
2779847,Does $\sum_{n=1}^\infty \frac{T(n)}{2^{2^n}}$ converge?,"Let $T(n)$ be the number of distinct topologies on a set with $n$ elements. Does $\displaystyle\sum_{n=1}^\infty \displaystyle\frac{T(n)}{2^{2^n}}$ converge? There is not much context to this unfortunately. It's a problem I came up with myself, when counting the number of topologies on an $n$-element set for $n=2,3$ (I am a beginner in topology). I am not sure of the difficulty of this problem but any progress toward a solution would be appreciated.","['general-topology', 'combinatorics', 'real-analysis', 'sequences-and-series']"
2779892,Suggestions for a reading list in Statistics and intro to machine learning,"I have built a reading list based on recommendations and suggestions found on several posts from this community. My intention is to use this reading list to acquire a good understanding about statistics and also about some topics on machine learning. Being my final goal to start a PhD in Statistics (in around 2 years). I'm familiar with basic calculus and basic Linear Algebra (almost all the concepts covered by Gilbert Strang books, even though in a practical level). Furthermore, I'm familiar with basic concepts of probability and statistics. Additionally, I'm highly proficient in R and Python (and I have some experience using numpy, matplotlib, pandas and scikit-learn). For this reason, I would appreciate further suggestions and recommendations, in order to improve this reading list. The books on this reading list range from introductory to more advanced level, being the following: Gilbert Strang - Calculus Daniel J. Velleman - How to Prove It: A Structured Approach Gilbert Strang - Linear Algebra and Its Applications Larry Wasserman - All of Statistics James, Witten, Hastie and Tibshirani - An Introduction to Statistical Learning Kreyszig - Introductory Functional Analysis with Applications [chapters 1 to 3] Golub and Van Loan - Matrix Computations Stein and Shakarchi - Real Analysis: Measure Theory, Integration, and Hilbert Spaces Jan R. Magnus - Matrix Differential Calculus with Applications in Statistics and Econometrics Fitzpatrick - Advanced Calculus Wakerly, Mendenhall and Scheaffer - Mathematical Statistics with Applications Casella and Berger - Statistical Inference Boyd and Vandenberghe - Convex Optimization Hastie, Tibshirani and Friedman - The Elements of Statistical Learning: Data Mining, Inference, and Prediction Gelman et al. - Bayesian Data Analysis Kevin Murphy - Machine Learning: A Probabilistic Perspective Click this link to see a visual representation of the previous reading list Thanks in advance! Edit 1: Added Hans Engler Feedback Edit 2: Added All of Statistics as suggested by Wanshan Edit 3: Added paf feedback","['advice', 'machine-learning', 'statistics', 'book-recommendation', 'soft-question']"
2779916,What is the probability that both children are boys if at least one is a boy born on a Tuesday?,"A family has two children. Given that at least one of the children is a boy who was born on a Tuesday, what is the probability that both children are boys? The day of birth is independent of the gender P(both are boys $\mid $ at least one boy) = P(both are boys) / P(at least one boy) $= P(\text {both are boys}) / [1 - P(\text{both are girls}$)] $= 0.5^2/(1-0.5^2)$ $= 0.25/0.75$ $= 0.3333$",['probability']
2779918,Intuition for Formal Definition of Linear Independence,"I learned about this a long time ago but it never really clicked, which led me to these questions: How the formal definition (at the bottom) works. I have a rough intuition: linear independence is where the variables are independent and don't affect each other. But I don't follow the formal definition. I would like to have a deep understanding of the formal definition based on these linear combination equations. I'm not sure how a linear combination constructed in a certain way can tell you the variables are independent or not. Why set the linear combination equations to $\vec{0}$. I don't see how setting to zero helps determine independence or not. Why choose $a_i$ to be non-zero in one case. It seems arbitrary. From Wikipedia : A subset $S=\{{\vec {v}}_{1},{\vec {v}}_{2},\dots ,{\vec {v}}_{n}\}$ of a vector space $V$ is linearly dependent if there exist a finite number of distinct vectors ${\vec {v}}_{1},{\vec {v}}_{2},\dots ,{\vec {v}}_{k}$ in $S$ and scalars $a_{1},a_{2},\dots ,a_{k}$, not all zero, such that $$a_{1}{\vec {v}}_{1}+a_{2}{\vec {v}}_{2}+\cdots +a_{k}{\vec {v}}_{k}={\vec {0}}$$ where ${\vec {0}}$ denotes the zero vector. The vectors in a set $T=\{{\vec {v}}_{1},{\vec {v}}_{2},\dots ,{\vec {v}}_{n}\}$ are linearly independent if the equation $$a_{1}{\vec {v}}_{1}+a_{2}{\vec {v}}_{2}+\cdots +a_{n}{\vec {v}}_{n}={\vec {0}}$$ can only be satisfied by $a_{i}=0$ for $i=1,\dots ,n$. So my understanding is, there are two subsets $S$ and $T$ of $V$. In one of them, the coefficients are not all zero, in the other they are all zero. In one case they are linearly dependent, in the other not. I don't understand why though; that's as much as I understand. Not sure why the equations were constructed like this in the first place.","['intuition', 'linear-algebra', 'vector-spaces']"
2779929,Smooth partition of unity: trouble verifying detail in Folland,"I have consulted the book Introduction to Partial Differential Equations by Folland (1995, Second Edition, Princeton University Press) for the proof of a certain theorem, but I am having trouble verifying a minor detail. Before I present what I am having trouble with, let me present the theorem and the proof as it is written by Folland on page 13 of the aforementioned book. (If $V$ is an open subset of $\mathbb{R}^n$, Folland writes $C_c^\infty(V)$ for the space of smooth functions on $\mathbf{R}^n$ whose support is compact and contained in $V$.) (0.19) Theorem Let $K \subset \mathbb{R}^n$ be compact and let $V_1, \dotsc, V_N$ be bounded open sets such that $K \subset \bigcup_1^N V_j$. Then there exist functions $\zeta_1, \dotsc, \zeta_N$ with $\zeta_j \in C_c^\infty(V_j)$ such that $\sum_1^N \zeta_j = 1$ on $K$. Proof Let $W_1, \dotsc, W_N$ be as in Lemma (0.18). [Comment: this means that the $W_j$ are open, cover $K$, and $\overline{W_j} \subset V_j$.] By Theorem (0.17), we can choose $\phi_j \in C_c^\infty(V_j)$ with $0 \le \phi_j \le 1$ and $\phi_j = 1$ on $\overline{W_j}$. Then $\Phi = \sum_1^N \phi_j \ge 1$ on $K$, so we can take $\zeta_j = \phi_j/\Phi$, with the understanding that $\zeta_j = 0$ wherever $\phi_j = 0$. For my applications I am only interested in demonstrating that the $\zeta_j$ belong to $C_c^1(V_j)$, and I am having trouble verifying that they are continuously differentiable ; everything else is fine. What is more, in my applications I do not assume that the $V_j$ are bounded, and I would appreciate an answer not relying upon their boundedness. It should be noted that the $\overline{W_j}$ may still be taken compact in this case. What follows is my progress. Pick $j \in I$, and define $A_j = \{ x \in \mathbb{R}^n : \phi_j(x) > 0 \}$, an open set. Note that $\text{supp } \phi_j = \overline{A_j}$, since $\phi_j \ge 0$ by construction. Firstly, $\phi_j = 0$ on the open set $\mathbf{R}^n \setminus \overline{A_j}$, hence the same is true of $\zeta_j$. We see that $\zeta_j$ is continuously differentiable on $\mathbf{R}^n \setminus \overline{A_j}$. Secondly, one has $\phi_j > 0$ and $\Phi > 0$ on $A_j$, and both of these functions are continuously differentiable. Therefore, the same must be true of $\zeta_j$ on the open set $A_j$. What remains is to prove that the partial derivatives of $\zeta_j$ exist and are continuous on the boundary $\partial A_j$, and this is where I am having trouble. However, I have demonstrated that $\zeta_j = 0$ on $\partial A_j$. For consider a point $x \in \partial A_j$, and pick a sequence $(x_m)_1^\infty$ in $\mathbb{R}^n \setminus A_j$ converging to $x$. Using continuity of $\phi_j$, one has $0 = \phi_j(x_m) \to \phi_j(x)$, and so $\phi_j(x) = 0$, thus $\zeta_j(x) = 0$. Can anyone help me verify that the partial derivatives of $\zeta_j$ exist and are continuous on $\partial A_j$?","['derivatives', 'real-analysis', 'calculus']"
2779962,What area of math develops an overaching theory of transforms?,"I am working on some things that require Laplace, Fourier, and Mellin transforms (and a few others lurking in the background). Simply put, if seems a transform is any function $G$ such that $F(g)=F(f(x))$, often in the form $F(g)=\int_{-\infty}^\infty f(x)q(x, g)dx$ or some such. We could have simpler transforms: for any complex number $Z=u+iv$ we could have a transform $F(Z)\to(u+1)+i(v+1)$  It migh not have much use (maybe it does), but it is obvious it has certain useful qualities - e.g., it is bi-directional, unique, and has an inverse.  On the other hand $F(Z)\to(u\cdot 0)+i(v+1)=i(v+1)$ ""loses"" a whole lot of information for no good purpose. Is there a braoder theoretical basis for transforms - particularly but not only integral transforms of complex numbers - that drives their usefulness, or explains why 'transforms' are useful and well-behaved?  Obviously, each example I gave has its mechanical benefits (turning convolution into multiplication, say), but I am seeking an understanding of the deeper abstract logic or structre that lets these things arise in the first place.","['complex-analysis', 'fourier-analysis', 'laplace-transform', 'transformation']"
2779965,"Finding min and max of $f(x,y)=-(x-y)^2+x$ in the region $(x,y)\in [0,1]\times[0,1]$.","I have a two variable function: 
$$f(x,y)=-(x-y)^2+x\, .$$
I need to find its absolute minimum and maximum under the constraints: $(x,y)\in [0,1]\times[0,1]$. The partial derivatives of$f$ do not vanish in the same points and then the maximum (and minimum) have to be to find on the edge: $E_1=\{x\in[0,1], y=0\}$, $E_2=\{x\in[0,1], y=1\}$, $E_3=\{y\in[0,1], x=0\}$, $E_4=\{y\in[0,1], x=1\}$. Then we must consider $f(x,0)$, $f(x,1)$ etc and find min and max of them. My problems are in the corners of the square $(x,y)\in [0,1]\times[0,1]$. What shall I say about $(0,0)$, $(0,1)$, $(1,0)$ and $(1,1)$? Thanks in advance","['multivariable-calculus', 'optimization']"
2779985,Sum of random variables is equal to zero infinitely often,"$X_1,\dots ,X_n$ are i.i.d random variables with $P(X_1=1)=p$, $P(X_1=-1)=1-p$, $p\neq\frac{1}{2}$. And $S_n=\sum_{k=1}^nX_k$. I need to show that $P(\limsup_{n\to\infty}\{S_n=0\})\in\{0,1\}$. It seems to me that $\limsup_{n\to\infty}\{S_n=0\}$ is not in tail sigma-algebra. Without this, i don't know how to proceed with it.","['probability-theory', 'probability']"
2780012,Is there a generalization of the free group that includes infinitely long words?,"The free group over a set $S$ only includes finitely-long words made up of letters from $S$ and their inverses. It seems natural to me to also allow infinitely long words. While this would obviously be impossible to operationalize on a computer, in principle these infinitely-long group elements seem perfectly well-defined. Is there some fundamental logical problem with including infinitely long words? If not, has this concept ever been studied? (I know that such a generalization would no longer count as being ""generated"" by $S$ - since by definition every group element needs to be reachable by a finite number of generator multiplications - so you'd need to find a new name.)","['abstract-algebra', 'group-theory', 'free-groups']"
2780055,Every conservative vector field is irrotational,"I have done an example where I needed to show that every conservative $C^2$ vector field is irrotational. However, there is something unclear in the solutions: Namely, I am uncertain what does the following sentence at the end of the solution mean: ""since second partial derivatives are independent of the order (for smooth functions)"", and I was wondering how does that imply that the equality before that is 0?","['multivariable-calculus', 'vector-fields', 'calculus']"
2780080,"Solving $\cos(2x)-\sin(x)= 0$ within the domain $[0,2\pi]$. Why am I missing some solutions?","I‘m trying to figure this one out: Solve for $x$ within the domain $[0,2\pi]$:
  $$\cos(2x)-\sin(x)= 0$$ I figured this is $\sin(x)=-1$ and $\sin(x) = \frac 12$, which, when taking the $\arcsin$, gives $x=\frac{\pi}6$ and $x=-\frac{\pi}2$. Now my question is: I‘m supposed to get $\dfrac{\pi}6$, $\dfrac{5\pi}6$, $\dfrac{3\pi}2$, but adding $2\pi$ gets me only $\dfrac{\pi}6$, $\dfrac{3\pi}2$, $\dfrac{7\pi}2$ within the $[0,2\pi]$ domain. Any idea what I‘m doing wrong here?","['algebra-precalculus', 'trigonometry']"
2780123,Are polynomials dense in $H^\infty$,Let $H^\infty(D)=\{f:D\longrightarrow \mathbb{C}: f \;\text{is bounded and analytic on}\; {D}\}$ where $D=\{z\in\mathbb{C}: |z|<1\}$. Are the polynomials dense in $H^\infty(D)$? I know that they are dense in  other Hardy spaces $H^p(D)$ for $0<p<\infty$.,"['functional-analysis', 'hardy-spaces', 'measure-theory']"
2780138,"There are 4 cups of liquid. Three are water and one is poison. If you were to drink 3 of the 4 cups, what is the probability of being poisoned?","In Season 5 Episode 16 of Agents of Shield, one of the characters decides to prove she can't die by pouring three glasses of water and one of poison; she then randomly drinks three of the four cups. I was wondering how to compute the probability of her drinking the one with poison. I thought to label the four cups $\alpha, \beta, \gamma, \delta$ with events $A = \{\alpha \text{ is water}\}, \ a = \{\alpha \text{ is poison}\}$ $B = \{\beta \text{ is water}\},\ b = \{\beta \text{ is poison}\}$ $C = \{\gamma \text{ is water}\},\ c = \{\gamma \text{ is poison}\}$ $D = \{\delta \text{ is water}\},\ d = \{\delta \text{ is poison}\}$ If she were to drink in order, then I would calculate $P(a) = {1}/{4}$.
Next $$P(b|A) = \frac{P(A|b)P(b)}{P(A)}$$ Next $P(c|A \cap B)$, which I'm not completely sure how to calculate. My doubt is that I shouldn't order the cups because that assumes $\delta$ is the poisoned cup. I am also unsure how I would calculate the conditional probabilities (I know about Bayes theorem, I mean more what numbers to put in the particular case). Thank you for you help.",['probability']
2780146,Number of $5$-card hands containing exactly two aces,"I have the above problem and I came up with the solution: $${4 \choose 2}{12 \choose 3}4^3$$ which is to choose $2$ aces from $4$ aces, choose $3$ values for the $3$ remaining and choose the suits for those $3$ cards. However, another solution that I came up with is: $${4 \choose 2}{48 \choose 3}$$ which is to choose $2$ aces from $4$ aces and choose the remaining $3$ cards out of remaining $48$. These two solutions give different results, but I don't know which one is right. Can someone clarify it? Thank you.","['combinatorics', 'discrete-mathematics']"
2780160,How to find the eigenvalues of $A+I$ given the eigenvalues of $A$?,"Say the $3 \times 3$ matrix $A$ has eigenvalues $a$, $b$ and $c$. Given $I$ is the $3 \times 3$ identity matrix, how to find the eigenvalues of $A+I$? I think it could be $a+1$, $b+1$ and $c+1$ but I don't have any basis at all. Thanks in advance.","['eigenvalues-eigenvectors', 'linear-algebra']"
2780173,Gauss Bonnet theorem and general relativity,"In the text : Vector calculus - Marsden, 
Gauss Bonnet theorem can be the motivation of the two dimensional 
Einstein Hilbert action. But, I could not find well these contents
in many texts. Is there any good reference about the relation between Gauss bonnet theorem
and the 2-dimensional General relativity?","['differential-geometry', 'general-relativity']"
2780197,"Separating compact, non-convex sets in $\mathbb R^n$","Let $m>1$. For disjoint compact subsets $E$ and $F$ of $\mathbb{R}^n$ we can define $$d(E,F) := \sup_\phi \inf \{\phi(x)-\phi(y)| x\in E, y\in F\},$$ 
where the supremum is taken over all bounded $\phi\in C^\infty(\mathbb R^n; \mathbb R )$ with $\|D^\alpha\phi\|_\infty \leq 1$ for $1\leq |\alpha| \leq m$. We easily get $d(E,F)\leq n^{1/2} \tilde d (E,F),$ for $\tilde d$ being the Euclidean distance. Since for compact convex sets we can always find a linear function with gradient of norm 1 such that $$\tilde d(E,F) = \inf \{\phi(x)-\phi(y)| x\in E, y\in F\},$$ we can approximate it by a sequence of bounded $C^\infty$ functions to obtain $\tilde d(E,F)\leq d (E,F),$ whenever $E, F \subseteq \mathbb R^n$ are two disjoint, compact and convex sets. The question I have been struggling with is: can we relax the condition that both sets need to be convex to obtain the above inequality (with possibly some extra, but uniform, constants)? I am particularly interested in the case when $E$ is the closed unit ball and $F$ is $\{x\in \mathbb R^n| 2\leq|x|\leq 4\}$. First I took a linear function as above separating $E$ and $B= \{x\in \mathbb R^n| |x-3e_1|\leq 1\}$ a compact and convex subset of $F$. This can't work though, because if I allow $y\in F\setminus B$ the infimum becomes negative. Should it be possible to construct some kind of a cut-off function with the above properties? I will be grateful for help! PS. the answer to the above question for $m=1$ is yes, since the distance function is Lipschitz continuous. Update: The claim holds for any disjoint closed subsets of $\mathbb R^n$. The idea of the following proof comes from Lemma 2.3 in this paper . Let $s=\tilde d(E,F) >0$ and $\psi \in C^\infty_c(\mathbb R^n)$, supported in the unit ball, with $\int \psi =1$. We consider the rescaled function $\psi_\varepsilon = \varepsilon^{-n}\psi(\cdot/\varepsilon)$ for $\varepsilon>0$. Let us fix $\varepsilon = \frac{s}{4}$ and denote the $\delta$-neighbourhood of $E$ w.r.t. $\tilde d$ by $\tilde E_\delta$. Define $C_\psi = \sum_{1\leq|\alpha|\leq m} \int |D^\alpha\psi|.$ We distinct two cases: Case 1. $\varepsilon\ge 1$ Let $\phi = \frac{\varepsilon}{C_\psi}\chi_{\tilde E_\varepsilon} \ast \psi_\varepsilon$. Then $\phi$ satisfies $\|D^\alpha\phi\|_\infty \leq 1$ for $1\leq |\alpha| \leq m$. 
We obtain for $x\in E, y\in F$ $$ \begin{align} \phi(x)-\phi(y)=\phi (x) & = \frac{\varepsilon}{C_\psi}\int_{B(x,\varepsilon)} \psi_\varepsilon(x-z) dz \\ & = \frac{\varepsilon}{C_\psi}\ge C\tilde d(E,F)\end{align}. $$ Case 2. $\varepsilon\leq 1$ Let $\delta=\varepsilon^{\frac{1}{m}}$ and we set $\phi = \frac{\varepsilon}{C_\psi}\chi_{\tilde E_\delta} \ast \psi_\delta$. Then one checks that $\phi$ also satisfies $\|D^\alpha\phi\|_\infty \leq 1$ for $1\leq |\alpha| \leq m$.
We have for $x\in E, y\in F$ $$ \begin{align} \phi(x)-\phi(y)=\phi (x) & = \frac{\varepsilon}{C_\psi}\int_{B(x,\delta)} \psi_\delta(x-z) dz \\ & = \frac{\varepsilon}{C_\psi}\ge C\tilde d(E,F)\end{align}. $$","['functional-analysis', 'special-functions', 'real-analysis', 'euclidean-geometry']"
2780215,What is the rationale behind transformation of a subspace in machine learning,"Suppose we have a random vector $\textbf{x}\subset \mathbb{R}^n$ with known probabilistic characteristics. We want to find a $\theta$ which makes the vector $z=H\theta$ as close as possible to $\textbf{x}$ where $H\in\mathbb{R}^{n\times p}$ is a full rank matrix with $p<n$ and $\theta\in\mathbb{R}^{p\times 1}$. One can find the desired $\theta$ by writing the least square errors as
$$J(\theta)=(\textbf{x}-H\theta)^T(\textbf{x}-H\theta)$$
The answer for the best $\theta$ which minimizes the above norm is $$\hat\theta=(H^TH)^{-1}H^T\textbf{x}$$ which means the best $\theta$ is the one that makes orthogonal projection of $\textbf{x}$ onto the subspace spanned by $H$. However, the weighted form of the error can be written as 
$$J(\theta)=(\textbf{x}-H\theta)^TW(\textbf{x}-H\theta)$$
If $W$ is a positive definite matrix, by substituting $W=LL^T$ (Cholescky decompostion) the above can be written as
$$J(\theta)=(L\textbf{x}-LH\theta)^T(L\textbf{x}-LH\theta)$$
Having the above notion in mind, and letting $y=L\textbf{x}$ one can say, the best $\theta$ is the one that makes orthogonal projection of $y$ onto the subspace spanned by $LH$. The last step means we have a subspace spanned by $H$ but to find the least error we change the subspace to Range($LH$) and our measurement to $L\textbf{x}$ and try to find the orthogonal projection of new measurement onto the new subspace. Why do we change our subspace to new subspace? What is the best $W$ or $L$? Does this change relate to data shape?","['optimization', 'machine-learning', 'statistics', 'geometry', 'linear-algebra']"
2780241,Is there a function that gives unique values when a unique sequence of numbers is given as input?,"Consider a random sequence of numbers, like 1, 4, 15, 21, 27, 15... There are no constraints on what numbers may appear in the sequence. Think of it as each element in the sequence is obtained using a random number generator. The question is, do we have a function that will give unique output by performing mathematical operations on this sequence? By unique, I mean when the function is applied on sequence A, it must output a value that's different from the output obtained by applying the same function on any other sequence (or the same sequence but numbers placed in different order) in the world. If we don't have such functions, can you tell me if it is even possible? Do we have anything that gets close?","['sequences-and-series', 'functions']"
2780293,Stochastic Processes - Bass exercise 1.6,"I'm working on this problem similar to one from Bass' book on stochastic processes and I cannot for the life of me make any progress. Let $X$ be a stochastic process and suppose that each sample path of $X$ is cadlag. Put $\mathcal{F}^X_t = \sigma(X_s:s \leq t)$, $\Delta X_t = X_{t} - X_{t^-}$, $\mathcal{F}_{\infty} = \sigma(\mathcal{F}^X_t:t\geq 0 )$, and $A_c = \{\omega \in \Omega: \text{ for some }t > 0\text{, } \Delta X_t(\omega) > c\}$. Show that $A_c \in \mathcal{F}_{\infty}$. So far I know that for $t>0$, $\Delta X_t(\omega) > c$ if and only if there exists $K \in \mathbb{N}$ such that for each $n \in \mathbb{N}$, there exists $m \in \mathbb{N}$ such that for each $t_1 \in (0,t) \cap \mathbb{Q}, t_2 \in (t,\infty)\cap \mathbb{Q}$, if $|t_1 - t_2| < 1/m$, then $X_{t_2}(\omega) - X_{t_1}(\omega)>c + 1/K -1/n$. If I put $B_t = \{\omega \in \Omega: \Delta X_t(\omega) > c\}$, $R_{t_1, t_2, m} = \{\omega \in \Omega: |t_1 - t_2| < 1/m \}$, $S_{K, t_1,t_2,n} = \{\omega \in \Omega: X_{t_2}(\omega) - X_{t_1}(\omega) > c +1/K - 1/n\}$, then $R_{t_1, t_2, m}$, $S_{K, t_1,t_2,n} \in \mathcal{F}_\infty$. So $B_t = \bigcup\limits_{K \in \mathbb{N}}\bigcap\limits_{n \in \mathbb{N}} \bigcup\limits_{m \in \mathbb{N}} \bigcap\limits_{t_1 \in (0,t)\cap \mathbb{Q}} \bigcap\limits_{t_2 \in (t, \infty)\cap \mathbb{Q}} R_{t_1, t_2, m}^c \cup S_{K, t_1, t_2, n}$ and hence $B_t \in \mathcal{F}_{\infty}$. Now I'm stuck trying to answer the question since $A_c = \bigcup\limits_{t>0}B_t$ and I cannot figure out how to get a countable union. I thought a little about how cadlag functions have at most countably many discontinuities, but I couldn't figure a way to work that in. I'm sort of expecting this entire process to be an unhelpful direction to begin at, but it was the best that I could come up with. The question reminds me of exercise 1.7 in Karatzas and Shreve. However in that problem I was able to get a uniform characterization of continuity to be able to get a countable union/intersection. With this one, I'm not sure if that's possible, so feel free to offer any other approach to solving the problem. Thanks everyone.","['stochastic-processes', 'real-analysis', 'measure-theory', 'probability-theory']"
2780301,Find value of $\tan\big(\frac{\pi}{25}\big)\cdot \tan\big(\frac{2\pi}{25}\big)\cdots\tan\big(\frac{12\pi}{25}\big)$,"Find value of: $$\displaystyle \tan\bigg(\frac{\pi}{25}\bigg)\cdot \tan\bigg(\frac{2\pi}{25}\bigg)\cdot \tan\bigg(\frac{3\pi}{25}\bigg)\cdots\cdots \tan\bigg(\frac{12\pi}{25}\bigg)$$ The solution I tried: Assume 
$$P =  \tan\bigg(\frac{\pi}{25}\bigg)\cdot \tan\bigg(\frac{2\pi}{25}\bigg)\cdot \tan\bigg(\frac{3\pi}{25}\bigg)\cdots\cdots \tan\bigg(\frac{12\pi}{25}\bigg),$$
with the help of $\tan(\pi-\theta)=-\tan \theta$, then
$$P=\tan\bigg(\frac{13\pi}{25}\bigg)\cdot \tan\bigg(\frac{14\pi}{25}\bigg)\cdot \tan\bigg(\frac{15\pi}{25}\bigg)\cdots\cdots \tan\bigg(\frac{24\pi}{25}\bigg)$$
which gives
$$P^2=\prod^{24}_{r=1}\tan\bigg(\frac{r\pi}{25}\bigg).$$ How do I proceed from here?",['trigonometry']
2780335,Upper & Lower Bound on P-Value using printed t table,"Suppose we have a sample size of $n=25,$ and want to test $H_0 :\mu = 0$ against $H_A : \mu > 0$ using a t test at the 5% level of significance. The first question was Suppose the t-statistic were calculated to be $t = 1.972.$ What is the greatest upper & lower bound for the corresponding p-value that is implied by your critical value table? And my answer was $0.025 <$ p value $< 0.050.$ However, the next part ask Using a normal approximation to the null distribution of the t statistic, what is the greatest upper & lower bound for the p-value that is implied? I'm not sure how to do this? Do I use the z table but not quite sure how.","['statistics', 'hypothesis-testing', 'normal-distribution']"
2780348,Do continuous weak derivatives imply continuity?,"Let $U \subseteq \mathbb{R}^n$ be an open set. Let $f \in W^{1,p}(U)$ and suppose all the weak derivatives of $f$ are continuous. Is $f$ itself continuous? It is a classic fact that if we assume a-priori $f$ is also continuous, then it is in $C^1$. Here, however I do not assume $f$ is continuous.","['weak-derivatives', 'real-analysis', 'sobolev-spaces']"
2780356,"Is $\text{SL}(2,\mathbb{R}) \to \mathbb{R}^2 \setminus \{0\},\; A \mapsto Ae_1$ a closed map?","Question: Consider the map from the title
$$f \colon \text{SL}(2,\mathbb{R}) \to \mathbb{R}^2 \setminus \{0\},\qquad A \mapsto A\begin{pmatrix}1 \\ 0 \end{pmatrix}.$$ Is it a closed map, i.e. does it map closed sets $A \subseteq \text{SL}(2,\mathbb{R})$ to closed sets $f(A) \subseteq \mathbb{R}^2 \setminus \{0\}$? What I've tried : According to Wikipedia , this is equivalent to the condition that for all subsets $A \subseteq \text{SL}(2,\mathbb{R})$, we have
$$
\overline{f(A)} \subseteq f(\overline{A}).
$$ So, for example, if I take the sequence of matrices
$$
a_n =\begin{pmatrix}
2 & n \\ 1/n & 1
\end{pmatrix},
$$
then we have $f(a_n) \to \begin{pmatrix}2 \\ 0 \end{pmatrix}$ but $(a_n)_n$ itself does not converge. So maybe one can construct a set $A$ containing the image of this sequence, but its closure does not contain a matrix with first row $\begin{pmatrix}2 \\ 0 \end{pmatrix}$?","['matrices', 'closed-map', 'general-topology']"
2780381,How do I show that $n$ is prime if and only if $\phi(n) = n − 1$?,"I am trying to construct a proof that shows both the 'forward' and 'backward' directions for the statement $n$ is prime if and only if $\phi(n) = n − 1$, however I cannot figure out what specific proof type to use for both directions. Thanks.","['totient-function', 'proof-verification', 'elementary-number-theory', 'prime-numbers', 'discrete-mathematics']"
2780396,Absolute continuity of probability measure wrt Lebesgue measure,"I am a bit dumbfounded by this problem as it has always been given in the build up to an application of the Radon-Nikodym theorem. Let $(\Omega, \mathcal{A}, P)$ is a probability space. Suppose that
  $Y:\Omega \rightarrow \mathbb{R}$, $Y(\Omega) = \mathbb{R}$, is a continuous random variable with
  density $f(y)$. Define $$\mathcal{C} = \{Y^{-1}(B): B \in \mathcal{B}\}$$ where $\mathcal{B}$ is the Borel $\sigma$-field
  defined on $\mathbb{R}$. Next, define $\mu(Y^{-1}(B)) = \lambda(B)$
  for any $Y^{-1}(B) \in \mathcal{C}$ where $\lambda$ is the Lebesgue
  measure in $\mathbb{R}$. Show that $P \ll \mu$. We know that $P \ll \mu$ if $\mu(A) = 0$ implies $P(A) = 0$ but I am not certain how to prove this relationship without more context about $P$. Thanks in advanced for the help!","['probability-theory', 'lebesgue-measure', 'measure-theory']"
2780425,"Which polynomials are ""transitive"" with respect to $\mathbb{Z}/r\mathbb{Z}$?","Given a polynomial $P(x) \in \mathbb{Z}[x]$ and a ring $R$, call $P$ transitive with respect to $R$ if and only if for all $r \in R$, there exists a natural number $n$ such that $P^n(0)=r,$ where $P^n$ refers to the $n$-fold composite $$P^n = \underbrace{P \circ \cdots \circ P}_n.$$ Observe, for example, that the degree-$1$ polynomial $P(x) = x+a$ is transitive with respect to $\mathbb{Z}/r\mathbb{Z}$ if and only if $a$ and $r$ are coprime. I'm looking for a generalization of this statement. Question. Is there a useful characterization of which polynomials $P(x) \in \mathbb{Z}[x]$ are transitive with respect to $\mathbb{Z}/r\mathbb{Z}$ generalizing the above comment, that's at least somewhat easier than simply repeatedly applying $P$?","['commutative-algebra', 'abstract-algebra', 'polynomials', 'elementary-number-theory']"
2780443,Difference of convexity and strict convexity,"I know what is a function convex and what is a strict convex function. But I was wondering what is concretely the difference, haw can we differentiate both on a graph ? For example, I know that a convex function is always upper that any tangent, i.e. $f(x)\geq f(y)+f'(y)(x-y)$ for all $x$ and all $y$. I unfortunately don't thing that it make sense to says that it's strictly upper it's tangent since for $x=y$ we always have $f(x)=f(y)+f'(y)(x-y)$. So, is there a way to distinguish convexity and strict convexity just by watching the graph of a function ? And if yes, how ? What can be the specific characteristic of a strict convex function that a convex function doesn't have ? (despite the strict inequality at the definition).","['real-analysis', 'convex-analysis']"
2780451,Calculate max Data,"Over the last few days, I have been dealing in detail with numbers and number ranges, as well as with the conversion of a value into different number ranges. [X] Some information: I got a dataField that includes an specific amount of dataBlobs (int N like e.g 10 or 102 , ...). Every single dataBlob can hold 16 different combination options. I got a dataString like ""102"" or ""10101010"" I want to encode in the data field . The amount of unique characters in dataString will be the baseValue I'm encoding from. ( If dataString is like ""101010101"" [2 unique chars ""1"", ""0""] I will encode dataString from base 2 into base 16 ) ( If dataString is like ""102"" [3 unique chars ""1"", ""0"", ""2""] I will encode dataString from base 3 into base 16 ) The encoding of the dataString into Base16 will give back a array like [""0"", ""15"", ""4"", ""7"", ""1"", ""1"", ...] (all values between 0 and 15) and for every element in the encodedData I will add one Blob with the specific dataValue (see in the image below) [X] My question: I'd like to calculate the maximum length of the dataString a dataField with N dataBlobs(16combinations) can hold. What is given : The maximum amount of dataBlobs can used (N) The count of unique chars in dataString ( = the base-value I will encode from ) dataString = ""10101010"" | (from BASE 2 to BASE 16 ) will get [10, 10] --> Needs 2x dataBlobs dataString = ""102"" | (from BASE 3 to BASE 16 ) will get [6] --> Needs 1x dataBlob dataString = ""2143567908"" | (from BASE 10 to BASE 16 ) will get [7, 15, 12, 4, 4, 0, 2, 4] --> Needs 8x dataBlobs How can I calculate the amount of characters dataString (with a given Base value) can hold if I want to use N dataBlobs ?","['number-theory', 'recreational-mathematics', 'integers', 'discrete-mathematics']"
2780457,How to evaluate $\int_{0}^{1}\arccos\sqrt{1-x}\ln(x-x^2)\ln\left(\frac{1}{x}-1\right)dx?$,How may one demonstrate this integral is equal to this result? $$\int_{0}^{1}\arccos\sqrt{1-x}\ln(x-x^2)\ln\left(\frac{1}{x}-1\right)\mathrm dx=\pi\zeta(2)-2\pi[1-\ln^2(2)]$$ Where do I start from?,"['special-functions', 'integration', 'definite-integrals', 'calculus']"
2780467,Looking for all strictly concave functions unbounded in $\mathbb{R}^+$,"Looking for all functions $\left.g:[k,\infty )\to \left[k',\infty \right.\right)$, with $k, k'>0$ that satisfy the following conditions: $$g'(x)>0, g''(x)<0 \ \text{for all $x$ in domain}$$ $$\lim _{x\to \infty }g(x)=+\infty.$$ $$g^{-1} (x) \ \text{is unique (g is invertible) in domain.}$$ So far I got: $$g=x^\alpha, 0<\alpha<1.$$ $$g= \log(x)$$","['real-analysis', 'convex-analysis', 'calculus', 'functions', 'functional-analysis']"
2780482,Hilbert function of a primary ideal,"I'm reading Geramita's lectures on fat points, but I found a passage a bit confusing. (I'll post the reference as soon as I'll find the link).
I read that a fat point (of order $t$, defined by the ${p}$-primary ideal $p^t$) in $\mathbb{P}^n$ behaves like $\binom{n+t-1}{n}$ distinct points (in the sense of the Hilbert function). In order to prove this, the paper states that $$ H(S/p^t,s) =  \binom{n+t-1}{n} $$
if $s\geq t$, with $S=k[x_0,x_1,\ldots,x_n]$. To prove this observation, they observe that $ F \in p^{t+1}$ $\iff$ all the partial derivatives of order $\leq t$, vanish at $P$ (the point defined by the ideal). But I can't understand why $H(S/p^t)$ is equal to that binomial coefficient, I mean it should be equal to 
$$H(R/p^t,s)=\dim_k R_s-\dim_k (p^t)_s=\binom{n+s}{s}-\dim_k (p^t)_s$$
and I can't find an easy formula for the second dimension, I guess it involves some combinatoric tool I can't see.
Any help would be much appreciate.","['algebraic-geometry', 'hilbert-polynomial', 'abstract-algebra', 'commutative-algebra', 'ideals']"
2780495,compute $\lim\limits_{n\to \infty}\sum_{i=1}^{n}\sum_{j=1}^{n}\frac{n}{(n+i)(n^2+j^2)}$,"$\lim\limits_{n\to \infty}\sum_{i=1}^{n}\sum_{j=1}^{n}\frac{n}{(n+i)(n^2+j^2)}$ the form of limits reminds me $\iint\limits_{D} f(x,y)dxdy =  \lim\limits_{\lambda\to 0}\sum_{i=1}^{\infty}\sum_{j=1}^{\infty}f(x,y)dxdy$
$D=\{(x,y)|x\in [1,\infty],y\in [1,\infty]\}$ but that is all I can think about this problem","['multiple-integral', 'limits']"
2780514,"Does $A$ open, non-empty, $\frac {A+A} 2=A+1$ imply $A=\mathbb R$?","This is elementary and it may have an easy proof. Does $A$ open,non-empty in $\mathbb R$, $\frac {A+A} 2=A+1$ imply $A=\mathbb R$? There are many sets satisfying this equation: $\mathbb Q$, dyadic rationals etc. I can show that if $A$ satisfies this equation and $A$ is closed then $A=\mathbb R$. I believe that the same is true if $A$ is open. I can show that $\sup A =\infty$ and $\inf A=-\infty$, but I have not been able to show that $A=\mathbb R$. Notations: $\frac {A+A} 2 =\{\frac {a+b} 2:a,b \in A\}$, $A+1=\{a+1:a \in A\}$. Thanks in advance.",['general-topology']
2780521,Is there an error in this exercise about martingales?,"I'm currently hearing a probability theory course, we have the following exercise: An urn contains a red and a blue ball, at each (integer) time-step a ball is drawn from the urn and placed back into the urn along with an additional ball of the same colour. Let $R_n, B_n$ denote the random variables that count how many red and blue balls have been drawn at step $n$ . Let $\mathcal F_n$ be the smalles $\sigma$ -Algebra so that $R_n, B_n$ are measurable and let $M_n$ be the fraction of blue balls in the urn. Show that $(M_n,\mathcal F_n)$ is a martingale. I think this exercise has two errors , the first is that $(\mathcal F_n)$ as defined here do not actually give a filtration (ie $\mathcal F_n \subset \mathcal F_{n+1}$ does not hold), the second is that even if $\mathcal F_n$ is modified in the obvious way, that $M_n$ is not a martingale. The first can be corrected by asking that $\mathcal F_n$ be the $\sigma$ -Algebra generated by $\{R_1,..,R_n, B_1,..,B_n\}$ , so I don't see this as a big deal, I'd just be interested in whether I am correct in thinking that the above definition does not make $\mathcal F_n$ a filtration. For the second I note that $M_n = \frac{B_n +1}{n+2}$ and $\mathbb E[B_n\mid \mathcal F_{n-1}]= B_{n-1}+\frac12$ , so $$\mathbb E[M_n\mid \mathcal F_{n-1}]= \frac{B_{n-1}+\frac32}{n+2}=\frac{(n+1)(M_{n-1}-\frac1{n+1})}{n+2}+\frac{3}2\frac1{n+2},$$ which is not the same as $M_{n-1}$ . Am I correct in thinking that the exercise has an error? I have not included actual justifications, since I think they would harm readability of the question. However I can provide these if my conclusions are not correct and somebody would like to identify my errors.","['probability-theory', 'martingales']"
2780550,Evaluate $\lim_{x\to 0}\frac{x-\sin x}{x\sin x}$ without to use L'Hopital,"Evaluate $$\lim_{x\to 0}\frac{x-\sin x}{x\sin x}$$ Without L'Hopital's Rule $$\lim_{x\to 0}\frac{x-\sin x}{x\sin x}=\lim_{x\to 0}\frac{x(1-\frac{\sin x}{x})}{x\sin x}=\lim_{x\to 0}\frac{1-\frac{\sin x}{x}}{\sin x}$$ But I can not find a way to deal with $\sin x$ that does not result with a limit of the type $""\frac{0}{0}""$","['real-analysis', 'limits-without-lhopital', 'limits']"
2780582,Is it possible to shorten the solution for this 2014 RMO question?,"I was solving a question from the Regional Math Olympiad (RMO) 2014. Find all positive real numbers $x,y,z$ such that $$2x-2y+\frac1z=\frac1{2014},\quad2y-2z+\frac1x=\frac1{2014},\quad2z-2x+\frac1y=\frac1{2014}$$ Here's my solution: These expressions are cyclic. Therefore all solution sets must be unordered. This implies that $x=y=z$ . Thus, $x=2014$ and the solution is $$x=2014\quad y=2014\quad z=2014$$ Here's the official solution: Adding the three equations, we get $$\frac1x+\frac1y+\frac1z=\frac3{2014}$$ We can also write them as $$2xz-2yz+1=\frac z{2014},\quad2xy-2xz+1=\frac x{2014},\quad2yz-2xy+1=\frac y{2014}$$ Adding these, we get $$x+y+z=3\times2014$$ Therefore, $$\left(\frac1x+\frac1y+\frac1z\right)(x+y+z)=9$$ Using $\text{AM-GM}$ inequality, we therefore obtain $$9=\left(\frac1x+\frac1y+\frac1z\right)(x+y+z)\ge9\times(xyz)^{\frac13}\left({1\over xyz}\right)^{\frac13}=9$$ Hence equality holds and we conclude that $x=y=z$ . Thus we conclude $$x=2014\quad y=2014\quad z=2014$$ What I wonder is if there is something wrong with my approach. If yes, what is it? If no, then why is the official solution so long winded?","['algebra-precalculus', 'contest-math', 'alternative-proof', 'proof-verification']"
2780601,Is it true that the convex hull of a finite union compact convex sets compact?,"Currently I am studying Behrend's $M$-structure and Banach-Stone Theorem . 
He introduced the following notation. Notation : Consider a Banach space $X.$
Fix $x\in X$ and $r\geq 0.$
Consider the set 
$$K(x,r)=\{(x^*,x^*(x)+r)\in X^*\times \mathbb{R}:x^*\in B_{X^*}\}.$$ It is not hard to show that $K(x,r)$ is a compact convex set. However, the author quoted the following at page $45.$ convex hull of $\bigcup_{i=1}^nK(x_i,r_i)$ is compact because convex hull of finite unions of compact convex sets is compact. I fail to prove the above statement. 
Any hint is appreciated.","['functional-analysis', 'convex-hulls', 'real-analysis', 'convex-analysis']"
2780613,Elementwise maximum of two positive definite matrices,"Assume that $A$ and $B$ are real, symmetric, positive definite matrices of the same size, that is, $$A \succ 0, B\succ 0.$$ Let $\operatorname{elmax(A,B)}$ be the element-wise maximum matrix, consisting of scalar maxima of elements of $A$ and $B$. Is it always the case that $$\operatorname{elmax}(A,B)\succ 0?$$ That is, does the element-wise maximum operation preserve positive definiteness?","['matrices', 'positive-definite', 'convex-analysis']"
2780617,"Rolling $k$ identical dice, how many possibilities are there?","I understand that the solution is $\binom{6-1+k}{k}$, the same as the solution to distributing $k$ balls into $6$ boxes, but my first thought was to solve it differently, and I can't understand why it's incorrect. My thought was, to describe the result of throwing $k$ dice as a string of $k$ letters, each letter representing the result of one die.
So we get $6^k$ different strings, but the order of the letters doesn't matter because the dice are identical, so we divide by $k!$. Why is this incorrect?","['combinatorics', 'discrete-mathematics']"
2780699,Possible explanation why roots of $f(x)$ and $g(x)$ are very similar,"Made a small observation, and I'm looking for a possible explanation. Suppose we have $f(x)$ and we define $g(x)$ as: $$g(x):=x-\frac{f(x)}{f'(x)}$$
Why are the roots of $g'(x)$ and $f(x)$ almost identical? Almost because although some are numerically equivalent, all the roots are not the same. Consider $f(x):=x^2+3 x-1$, then $g'(x)$ is $\frac{2 \left(x^2+3 x-1\right)}{(2 x+3)^2}$, both functions' roots are: $$\left\{x\to \frac{1}{2} \left(-\sqrt{13}-3\right)\right\},\left\{x\to \frac{1}{2} \left(\sqrt{13}-3\right)\right\}$$ Consider $f(x):=x^5-2 x^4+2 x^3-2 x^2-x+\frac{1}{2}$, then the real roots are: $$\{x\to -0.52325\},\{x\to 0.33225\},\{x\to 1.66996\}$$ The roots of $g'(x)$ are: $$\{x\to 0.756324\},\{x\to -0.52325\},\{x\to 0.33225\},\{x\to 1.66996\}$$","['algebra-precalculus', 'functions']"
2780731,Uniqueness of solution of differential equation $y'=y$ [duplicate],"This question already has answers here : Prove that $C e^x$ is the only set of functions for which $f(x) = f'(x)$ (9 answers) Closed 6 years ago . In school, I have recently been learning about simple differential equations. We know that the solution of $y'=y$  is $y=Ae^x$, where $A$ is a constant. But how can we know that it is the only solution? The only thing I can figure out is that $y$ is continuously differentiable. Help me, please.",['ordinary-differential-equations']
2780734,What polyhedron is the Dayan Gem VI?,"The Dayan Gem VI is a twisty puzzle, and there are some pictures of it here: https://www.amazon.com/DaYan-Gem-Cube-VI-black/dp/B00PDVZ6YQ The polyhedron consists of 6 octagons arranged like cube faces, with 24 pentagons surrounding the octagons, with 8 triplet of pentagons arranged like cube vertices. This is not a Johnson solid, so pentagons and/or octagons are not regular polygons. Is there a name for this polyhedron?","['polyhedra', 'geometry']"
2780736,$f'(x) = f(x-1)$ then $f$ is not bounded,"Let $f:\mathbb{R} \rightarrow \mathbb{R}$. Then consider the following delay equation : 
$$f'(x) = f(x-1)$$ Let $S$ be the set of solution ot this equation. Then I would like to prove that : $\forall f \in S -\{ x \mapsto 0\}$, $f$ is not bounded. What I've done so far is that : The set of solutions is an infinite dimensional space, hence it's hard to get a general form of all solutions, hence I don't think that calculating the characteristic equation is a good idea. I've noticed that this is true for all equations of the form : $f'(x) = f(x-a)$ where $-1 \leq a \leq 1$. 
Yet this is not true for $a = 3\pi/2$, because $x \mapsto \sin(x)$ is a trivial solution and clearly $x \mapsto \sin(x)$ is bounded. Maybe using the mean value is a good idea, in order to get a contradiction if we suppose that $f$ is bounded. But the problem with this approach is that I don't get enough informations on $a$. We also have the formula : 
$$f^{(n)}(x) = f(x-n)$$ Thank you !","['real-analysis', 'ordinary-differential-equations', 'delay-differential-equations', 'calculus']"
2780766,If $A$ and $B$ are bounded simply connected open subsets of $\mathbb{R}^2$ then a connected component $C$ of $A\cap B$ is simply connected?,If $A$ and $B$ are bounded simply connected open subsets of $\mathbb{R}^2$ and  if  $ A \cap B \neq \emptyset$  then a connected component $C$ of $A\cap B$ is simply connected? I think it must be true intuitively but how can I prove it?,"['algebraic-topology', 'general-topology']"
2780796,"If ""P implies (Q or R)"" and ""Q holds true"", what can be said about R?","I encountered the above situation when I was working with these set of equations
$$\begin{cases}
a^2+bc&=&0 \\
b(a+d)&=&0 \\
c(a+d)&=&0 \\
d^2+bc&=&0
\end{cases}$$ where $a,b,c,d\in \mathbb{C}$. Suppose $a+d\ne 0$. Then from $(2)$ and $(3)$, we have $b=c=0$. Also from $(1)$ and $(4)$, we have $a^2=0 \Rightarrow a=0$ and $d^2=0 \Rightarrow d=0$. But then $a+d=0$ which contradicts our assumption. Hence, it must be the case that $a+d=0$. We know that if $b(a+d)=0$ then $b=0$ or $a+d=0$. Since, we showed that $a+d=0$, can $b$ be any arbitrary complex number? Seems intuitive but I cannot seem to show it.","['algebra-precalculus', 'systems-of-equations', 'discrete-mathematics']"
2780815,"Describing $\langle x,y : x^{2} , y^{3} , [x,y] , x^{6}y^{6}\rangle$.","I am trying to identify the following presentation $\langle x,y : x^{2} , y^{3} , [x,y] , x^{6}y^{6}\rangle$ I substituted  the first relation in the final one and got $x^{6}=1$ so the group is cyclic of order 6
The problem is why we can't say that the schreier transversal is $\{x^{0}.....x^{5}\}$ so we omit the other generator $y$. The correct answer in fact is claiming that the transversal is $\{x^{i}y^{j}\}$. But in another case for The presentation of the cyclic group of order 7 we omit the other generator.","['combinatorial-group-theory', 'group-theory', 'group-presentation']"
2780832,Proving that $\sin(z+w) = \sin(w) \cos(z) + \sin(z) \cos(w)$ using complex exponentials,"Let's define: $$\sin(z) = \frac{\exp(iz) - \exp(-iz)}{2i}$$
 $$\cos(z) = \frac{\exp(iz) + \exp(-iz)}{2}$$ We are to prove that
  $$\sin(z+w)=\sin(w) \cos(z) + \sin(z)\cos(w), \forall_{z,w \in \mathbb{C}}$$
  using only the following statement: $\exp(z+w) = \exp(w)\exp(z)$. I managed only to show that:
$$\sin(z + w) = \frac{\exp(iz)\exp(iw)}{2i} - \frac{\exp(-iz)\exp(-iw)}{2i}.$$ Where can I go from here?","['complex-analysis', 'trigonometry']"
2780860,Convergent series after rearrangement becomes divergent,"Show that $$1-\frac{1}{\sqrt{2}}+\frac{1}{\sqrt{3}}-\frac{1}{\sqrt{4}}+\frac{1}{\sqrt{5}}-\cdots$$
is convergent but by rearrangement the following series
 $$\left(1+\frac{1}{\sqrt{3}}-\frac{1}{\sqrt{2}}\right)+\left(\frac{1}{\sqrt{5}}+\frac{1}{\sqrt{7}}-\frac{1}{\sqrt{4}}\right)+\cdots $$
 is divergent. Attempt: The 1st series $$\sum_{n=1}^\infty (-1)^{n+1} \frac{1}{\sqrt{n}}$$ can be proved to be convergent by Leibnitz Test as (1) $u_n= \frac{1}{\sqrt{n}}\to 0$ as $n\to\infty$ (2) $\{u_n\}$ is monotone decreasing But please help me to show that the second series, after rearrangement, is divergent.","['real-analysis', 'sequences-and-series', 'convergence-divergence']"
2780865,Expected value and Standard deviation- Cards,"Suppose that we've decided to test Clara, who works at the Psychic Center, to see if she really has psychic abilities. While talking to her on the phone, we'll thoroughly shuffle a standard deck of $52$ cards (which is made up of $13$ hearts,$13$ spades,$13$ diamonds, and $13$ clubs) and draw one card at random. We'll ask Clara to name the suit (heart, spade, diamond, or club) of the card we drew. After getting her guess, we'll return the card to the deck, thoroughly shuffle the deck, draw another card, and get her guess for the suit of this second card. We'll repeat this process until we've drawn a total of $14$
cards and gotten her suit guesses for each. Assume that Clara is not clairvoyant, that is, assume that she randomly guesses on each card. a.Estimate the number of cards in the sample for which Clara correctly guesses the suit by giving the mean of the relevant distribution (that is, the expectation of the relevant random variable). Do not round your response. b.Quantify the uncertainty of your estimate by giving the standard deviation of the distribution. Round your response to at least three decimal places. My answer a) $E(X)=np= 14(1/4) =3.5.$ b) $SD(X) = \sqrt{npq}=\sqrt{14(1/4)(3/4)}= 1.620.$","['statistics', 'probability-distributions', 'probability', 'discrete-mathematics']"
2780868,Definition of differential forms on $\mathbf P^n$.,"Consider the projective space $\mathbf P^n$ over some field $k$, with assumptions on the field as necessary. There is a complex of $k[x_0,\ldots,x_n]$-modules $\Lambda^0\xrightarrow{\mathrm d}\cdots\xrightarrow{\mathrm d}\Lambda^n$, where $\Lambda^i = \bigwedge^i\Omega_{k[x_0,\ldots,x_n]/k}$ for $\Omega_{k[x_0,\ldots,x_n]/k}$ the dual space of the $k$-vector space $\langle\frac{\partial}{\partial x_0}, \ldots, \frac{\partial}{\partial x_n}\rangle$. The operation $\mathrm d: \Lambda^i\to \Lambda^{i+1}, f\mathrm dx_{j_1}\wedge\ldots\wedge\mathrm dx_{j_i} \mapsto \sum_l \frac{\partial f}{\partial x_l} \mathrm dx_{l}\wedge\mathrm dx_{j_1}\wedge\ldots\wedge\mathrm dx_{j_i}$ is a derivation. There is another derivation $\Delta:\Lambda^i\to \Lambda^{i-1}$ with $\Delta(\mathrm d\alpha)+\mathrm d(\Delta\alpha)=\alpha \deg\alpha$ for homogeneous $\alpha$ wrt. the grading $\deg f\mathrm dx_{j_1}\wedge\ldots\wedge\mathrm dx_{j_i} = \deg f + i$. Questions: What does $\Delta$ do? In degree $0$, it seems to me that the contraction $\omega\mapsto\sum_l x_l \frac{\partial}{\partial x_l}\lrcorner\omega$ has the desired property. What is it in higher degrees? The text I am trying to read defines $M^i=\ker\Delta|_{\Lambda^i}$ and calls $\tilde M^i$ the sheaf of differential $i$-forms. Why doesn't one take all of $\Lambda^i$? Especially for question 2., answers appealing to intuition rather than rigorousness are highly welcome!","['differential-geometry', 'algebraic-geometry', 'commutative-algebra']"
2780910,Is there such a thing as an Even Matrix?,"An even function is one in which $f(x)=f(-x)$. For two variables I believe this is $f(x,y)=f(-x,-y)$ If I wish to make a 2D even matrix how would I do this? $$ \begin{matrix} (0,0) & (0,1) \\
                  (1,0) & (1,1) \end{matrix}$$ Looking at the indices I can't see any pattern that would allow a even matrix.","['matrices', 'linear-algebra', 'even-and-odd-functions']"
2780974,Is there a name for $\sum\left\lfloor\frac{n}{p^i}\right\rfloor$,"Is there a name for this function?
$$f(n,p)=\sum_{i=1}^\infty\left\lfloor\frac{n}{p^i}\right\rfloor, \quad n,p\in \mathbb{N}$$
It's left unnamed in the OEIS , Wolfram Alpha doesn't recognise it , but it's a common theme in some Project Euler problems and it seems important.","['terminology', 'sequences-and-series']"
2781063,Limit of $\frac{\sqrt[n]{(n+1)(n+2)\cdots(2n)}}n$ [duplicate],"This question already has answers here : How to prove that $\lim \frac{1}{n} \sqrt[n]{(n+1)(n+2)... 2n} = \frac{4}{e}$ (5 answers) Closed 3 years ago . Compute the limit
$$
\lim_{n \to \infty} \frac{\sqrt[n]{(n+1)(n+2)\cdots(2n)}}n
$$ How can this be done? The best I could do was rewrite the limit as
$$
\lim_{n \to \infty} \left(\frac{n+1}n \right)^{\frac 1n}\left(\frac{n+2}n \right)^{\frac 1n}\cdots\left(\frac{2n}n \right)^{\frac 1n} 
$$ Following that log suggestion in the comments below:
\begin{align}
&\ln \left(\lim_{n \to \infty} \left(\frac{n+1}n \right)^{\frac 1n}\left(\frac{n+2}n \right)^{\frac 1n}\cdots\left(\frac{2n}n \right)^{\frac 1n} \right) \\ &= \lim_{n \to \infty} \ln \left( \left(\frac{n+1}n \right)^{\frac 1n}\left(\frac{n+2}n \right)^{\frac 1n}\cdots\left(\frac{2n}n \right)^{\frac 1n} \right) \\
&= \lim_{n \to \infty} \left(\ln \left(\frac{n+1}n \right)^{\frac 1n} + \ln\left(\frac{n+2}n \right)^{\frac 1n} + \cdots + \ln\left(\frac{n+n}n \right)^{\frac 1n} \right) \\
&= \lim_{n \to \infty} \sum_{i=1}^n \ln \left(\frac{n+i}n \right)^{\frac 1n} \\
&= \lim_{n \to \infty} \frac 1n \sum_{i=1}^n \ln \left(1 + \frac in \right) \\
&= \int_1^2 \ln x \, dx \\
&= (x \ln x - x)\vert_1^2 \\
&= (2 \ln 2 - 2)-(1 \ln 1-1) \\
&= (\ln 4-2)-(0-1) \\
&= \ln 4-1 \\
&= \ln 4 - \ln e \\
&= \ln \left( \frac 4e \right)
\end{align} but I read from somewhere that the answer should be $\frac 4e$.",['limits']
2781158,Question about proof in Neukirch's Algebraic Number Theory,"I was reading Proposition 2.2 in chapter I of Neukirch (page 6 in my edition), which states the following for an extension of rings $A\subseteq B$: (2.2) Proposition. Finitely many elements $b_1,\dots, b_n\in B$ are all integral over $A$ if and only if the ring $A[b_1,\dots,b_n]$ viewed as an $A$-module is finitely generated. Neukirch begins the proof by showing that if $b\in B$ is integral over $A$ then $A[b]$ is a finitely generated $A$-module. To do this, he notes that $b$ integral means there is some monic $f(x)\in A[x]$ of degree $n\geq 1$ such that $f(b)=0$. The claim is that $\{1,b,\dots,b^{n-1}\}$ form a generating set for $A[b]$. Neukirch proceeds to take a polynomial $g(x)\in A[x]$ (so that $g(b)$ is an arbritary element in $A[b])$ and states that ""we may then write
$$
g(x)=q(x)f(x)+r(x)
$$
for some $q(x),r(x)\in A[x]$ with $\deg(r(x))<n$"". Here is my problem : $A[x]$ is not a Euclidean domain in general. If $A$ is a field then sure, but if $A=\mathbb{Z}$ then $\mathbb{Z}[x]$ is not Euclidean so it would seem this step in the proof is not justified. What am I missing here?","['number-theory', 'abstract-algebra', 'euclidean-domain', 'algebraic-number-theory']"
2781183,"Maximal number of subsets of $\{1,\cdots,n\}$ with odd cardinals and even intersections","What is the maximal $k$ such that there exists $E_1,\cdots,E_k$, subsets of $\{1,\cdots,n\}$, such that $|E_i|$ is odd for every $i$ and $|E_i \cap E_j|$ is even for $i \neq j$? The way I have tried to approach it is by induction over $n$ then over the size of the smallest of the $E_1, \cdots, E_k$. I have proven that $k\geq n$ and believe that $k=n$. If there is a certain $E_i$ with only element, say $E_1 = \{n\}$ up to a reordering and permutation, then the rest of the subsets do not contain $n$, and therefore are a valid family of subsets for $n-1$. If there is not, I have tried proving it for there being $E_1 = \{1,2,3\}$ (and therefore $n\geq 4$). This does not really lead me anywhere, only providing me with the fact that every other $E_i$ is either (exclusively): Included in $\{4,\cdots,n\}$ A subset of $\{4,\cdots,n\}$ with $\{1,2\}$ added A subset of $\{4,\cdots,n\}$ with $\{2,3\}$ added A subset of $\{4,\cdots,n\}$ with $\{1,3\}$ added This has not led me to much, other than showing that the elements that fall in the first bullet point cannot be the subset of $\{4,\cdots,n\}$ in the other bullet points.","['combinatorics', 'discrete-mathematics']"
2781188,Find a normal matrix which commute with $A$.,Consider $$A=\begin{pmatrix}1&1&0&0\\-1&1&0&0\\0&0&2&2\\0&0&-2&2\end{pmatrix}.$$ I want to find a matrix $B$ such that $B$ is normal. $AB=BA$ . $B\neq I$ and $B\neq \alpha A$ ( $\alpha \in \mathbb{R}^*$ ).,"['matrices', 'linear-algebra']"
2781202,Compute $\lim_{n\to\infty}3\frac {{n!}^{\frac 1n}}{n}.$,Evaluate $\lim_{n\to\infty}3\frac {{n!}^{\frac 1n}}{n}.$ I tried forcing a riemann sum: rewrite $\lim_{n\to\infty}3\frac {{n!}^{\frac 1n}}{n}=\lim_{n\to\infty}3(\frac{n!}{n^n})^{\frac 1n}=L.$ Apply $\ln$ on both sides and get: $$\lim_{n\to\infty}\ln3+ \frac 1n\ln(\frac {n!}{n^n})=\lim_{n\to\infty} \ln3+ \sum_{k=1}^{k=n}\frac1n\ln(\frac kn)=\ln3+\int_0^1\ln(x)dx=\ln(\frac 3e)\to L=\frac 3e.$$ I actually did right I had a typo... Sorry for wasting your time.,"['real-analysis', 'limits-without-lhopital', 'calculus', 'limits']"
2781284,"If $a_i\geq 1\ \forall i\in\{1,...,120\}$ and $\sum_{i=1}^{120}a_i\leq 180$, there exist indices $i,j$ such that $\sum_{k=i}^ja_k=59$","Let's consider 120 numbers $a_1,a_2,...,a_{120}\in \mathbb{N}$ such
  that $a_i\geq 1\ \forall i\in\{1,...,120\}$ and
  $\sum_{i=1}^{120}a_i\leq 180$. Prove that there exist indices $i, j$
  such that $\sum_{k=i}^j a_k = 59$. I think that I might be able to use the pigeonhole principle somehow, but I am not sure what would correspond to the pigeons here... The nests could possibly be 60 
and the pigeons 59, solely by intuition, but I am not sure of anything else. Any help?",['discrete-mathematics']
2781293,A function in terms of another function,"I have two functions that are polynomials. For example:
$$F=x^2+2x+1  \hspace{5mm} \text{ and } \hspace{5mm} G=2x^2-x+2$$
I need to write one of these two functions in terms of the other one. For the example above the answer would be:
$G$ as a function of $F$ is: $G=2F-5F^{1/2}+5$
As you know, depending on the complexity of $F$ and $G$, doing such a manipulation can be very difficult. Therefore, I am wondering if there is a known mathematical way to do this algebraic manipulation. Please note that the general format of $F$ and $G$ is always as below:
$$F=\sum(ai × xi^i)|i=0,1,2,…,N$$
$$G=\sum(bi × xi^i)|i=0,1,2,…,N$$ 
Thank you!",['functions']
2781343,Covariance of two expectation estimators that used different numbers of samples,"Say I have two estimates of the mean of two functions:
$$Q^1_{N_1}=\frac{1}{N_1}\sum_{i=1}^{N_1}f^1(X_i), \quad Q^2_{N_2}=\frac{1}{N_2}\sum_{i=1}^{N_2}f^2(X_i),$$
where each sample $X_i$ is identical when $i \leqslant \min(N_1, N_2)$ and is selected from the probability distribution $X$, and I want to calculate the covariance of the two estimators $\mathrm{Cov}(Q^1_{N_1}, Q^2_{N_2})$. Lemna 3.2 of this paper attempts to prove that
$$\mathrm{Cov}(Q^1_{N_1}, Q^2_{N_2})=\frac{\mathrm{Cov}(f^1(X), f^2(X))}{\max(N_1,N_2)}=\frac{\sqrt{\mathrm{Var(f^1(X)) \mathrm{Var}(f^2(X))}}}{\max(N_1,N_2)}\rho_{12},$$
where $\rho_{12}:=\dfrac{\mathrm{Cov}(f^1(X), f^2(X))}{\sqrt{\mathrm{Var}(f^1(X)) \mathrm{Var}(f^2(X))}}$. I am pretty sure that the important piece there is
$$\mathrm{Cov}(Q^1_{N_1}, Q^2_{N_2}) = \frac{\mathrm{Cov}(f^1(X), f^2(X))}{\max(N_1,N_2)}.$$ This has been a hard proof for me to follow , so I am asking this community for help. I would say my biggest source of confusion is where the $\max(N_1, N_2)$ term comes from. Here is what I have tried so far. My result is simillar to that in the paper, except I have a minimum instead of a maximum. Can anyone help give me insight as to where the maximum operator comes into play? The covariance of $Q^1_{N_1}$ and $Q^2_{N_2}$ may be calulated as
\begin{align*}
&\mathrel{\phantom{=}}{} \mathrm{Cov}(Q^1_{N_1}, Q^2_{N_2})\\
&=\frac{1}{\min(N_1,N_2)}\sum_{i=1}^{\min(N_1, N_2)}\left(\left(f^1(x_i) - \frac{1}{N_1}\sum_{j=1}^{N_1}f^1(x_j)\right)\left(f^2(x_i) - \frac{1}{N_2}\sum_{j=1}^{N_2}f^2(x_j)\right)\right),
\end{align*}
where the minimum of $N_1$ and $N_2$ are the number of samples common to both estimators. Only $\min(N_1,N_2)$ samples are common to both estimators, so we must use at most $\min(N_1,N_2)$ samples to estimate the covariance. Assuming the number of samples is large, we can say
$$\mathrm{Cov}(Q^1_{N_1}, Q^2_{N_2})= \frac{1}{\min(N_1,N_2)}\mathrm{Cov}(f^1(X), f^2(X))=\frac{\sqrt{\mathrm{Var(f^1(X)) \mathrm{Var}(f^2(X))}}}{\min(N_1,N_2)}\rho_{12}.$$","['probability', 'covariance']"
2781350,morphisms to the skyscraper sheaf,"I am trying to understand when the module $Hom(F,k)$ is non empty, where $F$ is a rank $r \geq 2$ torsion-free sheaf on a 3-fold $X$ , and $k$ the skyscraper sheaf of one point on $X$ . While $Hom(F,k)$ is non empty for $F$ locally free or reflexive, this seems not to be the case for any torsion-free sheaf. Indeed, since $F$ is torsion-free and $X$ a 3-fold, there exists a locally free resolution of $F$ of the form: $$0\to L_2 \to L_1 \to L_0 \to F$$ which applying functor $\mathcal{Hom}(,k)$ on the above sequence gives : $\dim Hom(F,k) = r + \dim H^0(\mathcal{ext}^1(F,k)) - H^0(\mathcal{ext}^2(F,k)) $ while $H^0(\mathcal{ext}^2(F,k)) = 0$ if $F$ is reflexive or locally free, this can not be true if $F$ is only torsion-free, in this case, I don't know any condition that prevents that $\dim H^0(\mathcal{ext}^1(F,k)) - H^0(\mathcal{ext}^2(F,k)) = -r$ . But this will be very strange, so I would like to know if there is one example of torsion-free sheaf such that $Hom(F,k) = 0$ .","['coherent-sheaves', 'sheaf-theory', 'algebraic-geometry']"
2781351,(Non-) Convergence of $\frac{1}{n} \sum_{k=0}^{n - 1} \exp\left(2i \pi [\frac{3 + \sqrt{5}}{2}]^k\right)$ when $n \to +\infty$,"Let be $$\forall n > 0, S_n = \dfrac{1}{n} \sum\limits_{k=0}^{n - 1} \exp(2i\pi u_k),\quad  \forall k \geq 0,  u_k = \left(\dfrac{3 + \sqrt{5}}{2}\right)^k$$ I would like to prove or disprove the convergence of $S_n$ as $n \to +\infty$. What I have tried: First, I tried to express $u_k$ as $(\phi^{2k})_k$ with $\phi$ the golden ratio and use $\phi^2 = 1 + \phi$ in the exponential, but with no success. Second, I tried to establish lower / upper bounds of $S_n$ or study $S_{2n}, S_{2n + 1}$ with no success. I think I could make use of the irrationality of $\phi$ but would prefer to avoid a proof based on equipartition (as this is what I'm proving in the end). Also, this problem is whether $(\exp(2i\pi u_k))_k$ is Cesaro-summable.","['cesaro-summable', 'real-analysis', 'golden-ratio', 'convergence-divergence']"
2781366,ultrafilter convergence versus non-standard topology,"I have recently been reading about the non-standard characterisation of topological spaces, by saying which points of ${^*X}$ are infinitesimally close to which standard points. The theory looks a lot like that of ultrafilter convergence. Let me clarify: A point $x$ lies in the closure of $A$ iff there is an ultrafilter $\mathcal{U}$ on $A$ with $\mathcal{U}\to x$ A point $x$ lies in the closure of $A$ iff there is a non-standard $y \in {^*A}$ with $y\simeq x$ Here, ultrafilters play the same role as non-standard points, and convergence plays the same role as the $\simeq$ relation. Another example: X is Hausdorff iff for every ultrafilter $\mathcal{U}$ we have $(\mathcal{U}\to x \mbox{ and } \mathcal{U}\to y) \implies x=y$ X is Hausdorff iff for every non-standard $z$ we have $(z\simeq x \mbox{ and } z\simeq y) \implies x=y$ Again we just have to change ultrafilters to non-standard points and convergence to $\simeq$ to get the non-standard version of the property. This analogy also works for compactness and products, yielding the same easy proof of Tychonov. We have the following correspondence (let $A\subseteq X, x\in X$):
$$\begin{aligned}
y\in {^*X} &\leftrightarrow \mathcal{U} \mbox{ ultrafilter on } X\\
y \simeq x &\leftrightarrow \mathcal{U} \to x\\
y\in {^*A} &\leftrightarrow \mathcal{U} \mbox{ ultrafilter on } A \mbox{ (i.e. containing $A$)}\\
^*A &\leftrightarrow \{\mathcal{U}\mbox{ ultrafilter } \mid A \in \mathcal{U} \}\\
^*x &\leftrightarrow \mbox{ the principal filter on $x$}
\end{aligned}
$$ My first question is, how far does this analogy go? Is the ultrafilter characterisation a special case of non-standard topology? Can we then also use it to describe the star of functions, relations, ... , i.e.  does the above correspondence induce a monomorphism? My second question is, if we apply this analogy to the space $({^*X}, \mathcal{T}_S)$ carrying the standard topology (which is compact if the extention is saturated), then we get the space of all ultrafilters on $X$ with basis open sets $\{\mathcal{U} \mid G \in \mathcal{U} \}$ for all opens sets $G \subseteq X$. Is this space compact (like the analoguous space in the non-standard setting)? Does this construction have a name, in the ultrafilter setting? According to this analogy, its Hausdorff reflection should be the Stone-Cech compactification $\beta X$. This does agree with the ultrafilterconstruction of $\beta X$ for discrete $X$ Remark: It is very difficult to google this question because ultrafilters (on a different set) are used to construct non-standard extentions, so any search containing ultrafilters and non-standard analysis leads me to that construction and not to the use of ultrafilter convergence to describe topology. A link to a book or paper on this topic, or even a title would be a satisfactory answer.","['nonstandard-analysis', 'general-topology', 'compactification', 'filters']"
2781379,Parametric Curve Derivative Formula Misunderstanding,"I was reading this site 
on determine the derivative and tangents to a parametric curve. However, he begins by saying: ""suppose that we were able to eliminate the parameter from the parametric form and write the parametric equations in the form $y=F(x$)"". I thought the whole point of parametric equations was that this isn't usually possible (e.g. a circle), and so his argument that follows from there is thus redundant, or am I missing something?","['parametric', 'parametrization', 'calculus', 'functions']"
2781446,Why fully characteristic subgroup is normal?,"Let's take a group $G$. We say that the subgroup $H \leq G $ is fully characteristic if
$$\forall \phi \in \mathrm{End} (G) : \phi(H) \subseteq H.$$ Is this the fully characteristic   subgroup normal? A first thought is to apply the Theorem : $$\forall g \in G, \forall h \in H :ghg^{-1}\in H. $$
But how could this help as? I have definitely stuck. Update: Could we find a normal subgroup $H\trianglelefteq G$ such that $H$ is not strictly characteristic? (so the reverse statement is not valid) Thank you. P.S.: I apologize for not having any other progress, but I don't know how to continue.","['abstract-algebra', 'normal-subgroups', 'group-theory']"
2781449,Factorial of odds [duplicate],This question already has answers here : Proving formula for product of first n odd numbers (3 answers) Closed 5 years ago . I am trying to find a simple factorial of all the preceding odd numbers. If $9$ were to be picked the equation would read $9\times 7\times 5\times 3\times 1$ (only odd numbers can be picked). Would the following fraction work? $$\dfrac{x!}{2^{\left(\frac{x-1}{2}\right)}\left(\frac{x-1}{2}\right)!}$$,"['functions', 'products', 'combinatorics', 'factorial', 'fractions']"
2781456,Infinite convergent sum of differentiable functions always differentiable?,"Let $\{f_i\}_{i \in \mathbb{N}}$ be a sequence of differentiable real-valued functions in $\mathbb{R}$. Assume that for any $x \in \mathbb{R}$, the sum $N(x) = \sum\limits_{i=0}^\infty f_i(x)$ is convergent. Is there a general theorem that states that $N(x)$ is also a differentiable function?","['derivatives', 'real-analysis', 'sequences-and-series']"
2781488,What is the difference between $(u \cdot \nabla)v$ and $u\cdot(\nabla v)$ when written in Einstein notation?,"What is the difference between $(u \cdot \nabla)v$ and $u\cdot(\nabla v)$ when written in Einstein notation? I understand that they are different, but I'm not quite sure how. I've proven that $u \cdot (\nabla u)=\frac{1}{2}\nabla (u\cdot u)-u \times (\nabla \times u)$ . I want to prove stuff with $(u \cdot \nabla)v$ but I don't know how to write it in Einstein notation. Both $u$ and $v$ are vector fields.","['multivariable-calculus', 'index-notation']"
2781496,partial differentiability implies differentiability?,"Let $f(x_1, x_2, \ldots x_n)$ be a real-valued function in $\mathbb{R}^n$, $n \in \mathbb{N}$. Assume that for any integer $i \in [1,n]$, and for any collection of real values of $(x_j)_{j \neq i}$, the function 
$$
F(x_i) = f(x_1, \ldots, x_i, \ldots x_n)
$$
is differentiable in $\mathbb{R}$ (which means, we keep the coordinates $x_j$ for $j \neq i$ fixed and we consider $f$ as a function only of $x_i$). Does it necessarily mean that $f(x_1, \ldots, x_n)$ is differentiable in $\mathbb{R}^n$?","['derivatives', 'real-analysis', 'partial-derivative', 'complex-analysis', 'ordinary-differential-equations']"
2781537,Precise conditions on Sturm-Liouville Theorems,"In Sturm-Liouville (SL) theory ( https://en.wikipedia.org/wiki/Sturm-Liouville_theory ), there are three fundamental theorems concerning the solutions of the SL differential equation, $ \frac{\mathrm{d}}{\mathrm{d}x}\left[p(x)\frac{\mathrm{d}y(x)}{\mathrm{d}x}\right]+q(x)y(x)=\lambda y(x)$. They are (SL Theorems): The set of eigenvalues $\lambda=\{\lambda_1, \lambda_2,\ldots\}$ are all real, countable and distinct. The set of eigenfunctions $y(x)=\{y_1(x),y_2(x),\ldots\}$, forms a orthogonal system in some interval $(a,b)$ of the real line, so that they satisfy $\int_a^b y_m(x)y_n(x)\mathrm{d}x=K_n\delta_{m,n}$, where $K_n$ are non-null constants. This set of eigenfunctions forms a basis for the vector space of square integrable functions. Now, it is very known that a second-order differential equation of the form $f(x)y''(x)+g(x)y'(x)+h(x)y(x)=\lambda y(x)$ can be put into the SL form after we multiply it by some integrating function $w(x)$ that satisfies the first-order differential equation $\frac{\mathrm{d}}{\mathrm{d}x}\left[w(x)f(x)\right] = w(x)g(x)$. In this case, the solutions of this differential equation will obey the SL theorems, if we agree that the solutions are orthogonal with respect to the weight function $w(x)$, i.e., they satisfy $\int_a^b y_m(x)y_n(x)w(x)\mathrm{d}x=K_n\delta_{m,n}$. My question is: what are the precise conditions (on the functions $p,q,f,g,h,w$, on the interval of orthogonality etc.) for the SL Theorems above to hold? I ask this motivated by the following related problem: it is very known that some second-order differential equations admit an infinite sequence of orthogonal polynomials on the real line -- these are called the Classical Orthogonal Polynomials (COP) and comprehends the Jacobi, Hermite and Laguerre polynomials). There is a theorem due to Bochner (see for instance the book of T. Chihara, ""An Introduction to Orthogonal Polynomials"", p. 150) that these three sequences of COP are the only ones satisfying a second-order differential equation that are orthogonal on the real line (up to linear transformations). However, in Bochner proof appears another infinite sequence of polynomials -- called nowadays as Bessel polynomials -- that although are not orthogonal on the real line, they are orthogonal on the complex unit circle. These Bessel polynomials satisfy the differential equation (see Krall, H.L., and Frink, O. ""A new class of orthogonal polynomials: The Bessel polynomials."" Transactions of the American Mathematical Society 65.1 (1949): 100-115.): $x^2y''(x) + 2(x+1)y'(x)=n(n+1)y(x)$. It can be verified that $w(x)=\exp(-2/x)$ is the integrating function for this differential equation (hence the weight function), so it can be put into the SL form. However, I can't see why the SL Theorems do not hold for it (they seem to not hold, since, for instance, the Bessel polynomial of degree 2, $B_2(x)=3x^2+3x+1$, has imaginary roots, while the roots of any orthogonal polynomial sequence on the real line are all real). I will appreciate any comment about these questions.","['bessel-functions', 'sturm-liouville', 'ordinary-differential-equations', 'orthogonal-polynomials']"
2781542,ODE: $u'' +\frac {(1-4z)}{z^2(1-2z)}u' =0$,"I have got the following ODE:
$$u'' +\frac {(1-4z)}{z^2(1-2z)}u' =0$$
The answer book rearranges as follows:
$$\frac{u''}{u'} = -\frac{1-4z}{z^2(1-2z)}$$ I know I have to split the fraction on the RHS. But what is happening on the LHS:$\frac{u''}{u'}$
Can this be integrated at once?","['integration', 'ordinary-differential-equations']"
