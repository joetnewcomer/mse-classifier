question_id,title,body,tags
1774084,Is $n^{1/\sqrt{n}}$ convergent?,"I think it is convergent to $1$ because as $n$ tends to $\infty$ , $1/\sqrt(n)$ tends to $0$. Is it true? Thanks!","['real-analysis', 'analysis']"
1774095,Product of inexact differential forms is inexact,"Suppose we have a product manifold $M = M_1 \times M_2$. Let $\omega$ be a closed but inexact form on $M_1$ and $\eta$ a closed but inexact form on $M_2$. Then the claim is that
$$\omega \wedge \eta$$
is inexact on the manifold $M$. I $\require{enclose}\enclose{horizontalstrike}{\mathrm{suspect}}$ know that the general statement ""a product of inexact forms is inexact"" is false, but I don't know how to go about proving the above claim. The context of this question is in de Rham cohomology: I'm trying to prove Künneth's formula, and as a prelude I want to show that differential forms of the form above correspond to non-trivial elements of the cohomology groups of $M$.","['homology-cohomology', 'differential-geometry']"
1774100,Is there another way to describe the category $\mathbf{Top}$ of topological spaces?,The category $\mathbf{Top}_\ast$ of pointed topological spaces can be viewed as the comma category $(\bullet\downarrow\mathbf{Top})$. The objects of the category $\mathbf{Top}$ are topological spaces and the morphisms are the continuous maps between them. The composition of morphisms is the usual composition of maps and the identity morphisms are the identity maps. Is there exists another way to describe the category $\mathbf{Top}$ of topological spaces with other concepts in category theory?,"['category-theory', 'general-topology']"
1774117,A version of Ampère's law,"The most common proof that I have found of the fact that Ampère's law is entailed by the Biot-Savart law uses the fact that , if $\boldsymbol{J}:\mathbb{R}^3\to\mathbb{R}^3$, $\boldsymbol{J}\in C_c^2(\mathbb{R}^3)$, is a compactly supported twice continuously differentiable field such that $\nabla\cdot\boldsymbol{J}\equiv 0 $ and $\Sigma$ is a smooth surface satisfying the assumptions of Stokes' theorem, then $$\oint_{\partial^+ \Sigma}\left(\frac{\mu_0}{4\pi}\int_{\mathbb{R}^3}\frac{\boldsymbol{J}(\boldsymbol{x})\times(\boldsymbol{r}-\boldsymbol{x})}{\|\boldsymbol{r}-\boldsymbol{x}\|^3}d\mu_{\boldsymbol{x}}\right)\cdot d\boldsymbol{r}=\mu_0\int_\Sigma \boldsymbol{J}\cdot\boldsymbol{N}_e \,d\sigma\quad(1)$$where $\mu_0$ is any constant (the magnetic permeability in the physical interpretation), $\boldsymbol{N}_e$ is the surface's external normal unit vector and $\mu_{\boldsymbol{x}}$ is Lebesgue $3$-dimensional measure. Nevertheless, common exercises and applications of Ampère's law found in books of physics use current densities $\boldsymbol{J}\notin C_c^2(\mathbb{R}^3)$, one example being $\boldsymbol{J}$ constant on an infinite cylinder and constantly $\mathbf{0}$ outside the infinite cylinder. Do mathematically rigourous formulations of Ampère's law $(1)$ exist under more relaxed assumptions on $\boldsymbol{J}$, like the quoted case of $\boldsymbol{J}$ constant on a (bounded or unbounded) region and null outside of it, and, if they do, how can they be proved? I have thought about approximating such a $\boldsymbol{J}$ with $\boldsymbol{J}_n\in C_c^2(\mathbb{R}^3)$, but it is not easy to see that the required sequence really exists. I heartily thank any answerer!","['real-analysis', 'physics', 'mathematical-physics', 'integration', 'vector-analysis']"
1774130,Why do we need the infinite field hypothesis in this cohomology calculation?,"I've just finished my very first calculation with sheaf cohomology. It's exercise III.2.1(a) in Hartshorne, and it says Let $X = \mathbb{A}_K^1$ be the affine line over an infinite field $K$. Let $P, Q$ be distinct closed points of $X$ and let $U = X\backslash\left\{P, Q\right\}$. Show that $H^1(X,\mathbb{Z}_U)\neq 0$ where $\mathbb{Z}_U$ is the sheaf $\mathbb{Z}\vert_U$ extended by zero. A quick description of my proof: $X$ is a Noetherian topological space of dimension $1$, so Grothendieck's vanishing theorem says $H^i(X,\mathbb{Z}_U) = 0$ for all $i>1$. We have a short exact sequence of sheaves on $X$: $$0\to \mathbb{Z}_U \to \mathbb{Z} \to \mathbb{Z}_Y \to 0$$ where $\mathbb{Z}_Y$ is the extension by zero of the constant sheaf $\mathbb{Z}$ on the closed set $Y = \left\{P, Q\right\}$. Since $X$ is irreducible the constant sheaf $\mathbb{Z}$ is flabby and therefore $H^1(X, \mathbb{Z}) = 0$. Then the long exact sequence in cohomology is $$0\to \mathbb{Z}_U (X)\to \mathbb{Z}(X)\to\mathbb{Z}_Y (X) \to H^1 (X,\mathbb{Z}_U)\to 0$$ Now $\mathbb{Z}_Y (X) = \mathbb{Z}\vert_Y (Y) = \mathbb{Z}\oplus\mathbb{Z}$. Furthermore $\mathbb{Z}(X) = \mathbb{Z}$ because $X$ is irreducible. If $H^1(X,\mathbb{Z}_U)=0$ then we'd have a surjection $\mathbb{Z}\to \mathbb{Z}\oplus\mathbb{Z}$. Therefore it must not be zero. Although this seems reasonable to me, I cannot see why the assumption that $K$ is infinite is required. Can anyone expand on this to show what's gone wrong?","['sheaf-cohomology', 'field-theory', 'algebraic-geometry']"
1774158,Is affine GIT quotient necessarily an open map?,"Let $k$ be a field, $X=$Spec$A$ be an affine scheme with A a f.g. $k$-algebra. $G=$Spec$R$ is a linearly reductive group acting rationally on A. (i.e. every element of $A$ is contained in a finite dimensional $G$-invariant linear subspace of $A$.) By Nagata's theorem, $A^G$ is a f.g. $k$-algebra. We have the affine GIT quotiont $X \rightarrow X//G :=$Spec$A^G$ induced by the inclusion $A^G \rightarrow A$ of $k$-algebras. Question: is the affine GIT quotient, viewed as a map of the underlying topological spaces, necessarily an open map? (it doesn't need to be an open immersion of schemes.) If not, any counterexample?","['geometric-invariant-theory', 'algebraic-groups', 'invariant-theory', 'algebraic-geometry']"
1774163,Form all numbers from limited digits,"Let $M$ be a subset of the digits $\{0,1,\ldots,9\}$, and $N$ the set of numbers formed with digits in $M$. Suppose that all numbers from $1 $ to $99999999$ either belong to $N$ or can be written as a sum of two numbers in $N$. What is the minimum size of $M$? If $M$ contains $0,1,3,4,5$, then it already suffices because $2=1+1,6=3+3,7=4+3,8=4+4,9=5+4$, so every digit can be formed.",['algebra-precalculus']
1774164,General Methods to Solve First-Order PDE,"Question is as simple as: What are the different methods for solving a first-order PDE? I'm aware of nearly all forms of Method of Characteristics - Lagrange Method, Charpit's Method. I'm learning PDE, and was curious if there are any other methods since most of the links suggest (only) method of characteristics. If there are any, it'll be great if you can give an outline of it or a link where I can read about it, to learn! I read through this unanswered question , I'm not considering higher dimensions, I'm talking about simpler cases.","['ordinary-differential-equations', 'characteristics', 'partial-differential-equations']"
1774174,The topology of $y^2=(x-1)(x-2)(x-3)(x-4)$,"Andreas Gathmann's lecture notes on algebraic geometry start by considering the curve $C_n=\{(x,y): y^2 = (x-1)(x-2)\cdots(x-2n)\} \subset \mathbb{C}^2$. He claims that the topology of this curve is the same as that obtained by removing from two copies of $\mathbb{C}$ the real intervals $[1,2],...,[2n-1,2n]$ and gluing the two copies along those intervals in a reverse manner, identifying opposite sides, as indicated by his following diagram: I hope to understand this through some clarifications: After gluing, what corresponds to the two points of $C_n$ having value $x=1.7$? It seems we lost them in the process of removing the intervals. Answered: they are present in the A part of each sheet and not in B, and after the gluing they are the horizontal perimeter of the circular hole Why do we need to glue the intervals in opposite sides? (sides A and B in the picture have non-compatible labels in the two copies, so the gluing is supposedly more contrived than is shown to the right). Answered: We don't glue A to A. We glue instead A to B, to match the part with the crossing points to the part without them. This gives us the picture shown on the right. In what exact sense does it characterize the topology of the curve? Do we say that $C_n$ as a topological space with the euclidean topology is homeomorphic to the glued space? Can we say what are all the (euclidean) open sets in $C_n$? (In other words, what did we achieve?) Does this example even have any reference or bearing to the usual Zariski topology that we give to algebraic curves? What is the Zariski topology on the curve $C_n$?","['algebraic-curves', 'zariski-topology', 'curves', 'algebraic-geometry']"
1774199,Prove that $\operatorname{p.v.}(k\;*f)$ does not exist if $k(x)=|x|^{-n+i\gamma}$ and $f\in\mathcal{C}_c^1$,"I put a bounty only because I need quickly a solution, NOT because I know it's difficult - maybe it is, maybe not. I'm trying to do it, but without results. If I get some ""intermediate result"" (something I think it could be useful) I will write it here immediately. Consider the function $k(x)=|x|^{-n+i\gamma}$ defined on $\Bbb R^n$, where $\gamma$ is any nonzero real number and $i$ is the imaginary unit. $k$ is homogeneous of degree $-n+i\gamma$. I have to show that
$$
\operatorname{p.v.}(k\;*f)(x):=\lim_{\epsilon\to0^+}
\int_{\{|t|>\epsilon\}}|t|^{-n+i\gamma}f(x-t)\;dt
%(k\chi_{\{|x|>\epsilon\}})*f
$$
does NOT exists for $f\in\mathcal C_c^1(\Bbb R^n)$ (i.e. $\mathcal{C}^1$ functions with compact support), but we can find a sequence $\epsilon_j\to0$ such that
$$
\lim_{j\to+\infty}\int_{\{|t|>\epsilon_j\}}|t|^{-n+i\gamma}f(x-t)\;dt
%(k\chi_{\{|x|>\epsilon_j\}})*f
$$
exists for such $f\;$. We exclude the trivial case $f\equiv 0$. I don't know where to start, can someone help me please?","['functional-analysis', 'harmonic-analysis', 'real-analysis']"
1774211,Is there an interpretation of the hyper skewness?,"Let $X$ be a random variable. The standardized $n$th moment of $X$ is defined as
$$\frac{E[(X-\mathbb{E}[X])^n]}{\mbox{Var}[X]^{n/2}}. $$
Special cases are the skewness ($k=3$) and the kurtosis $k=4$. The skewness is a measure for the asymmetry of a distribution while the kurtosis measures how peaked the distribution is. In my financial engineering project, I have to work with $k=5$ which is referred to as hyper skewness . As a benchmark, it is common to consider the normal distribution which has zero skewness and kurtosis equal to three. However, the hyper skewness of the normal distribution is also equal to zero, so at first sight it does not tell anything more about the distribution. I was wondering if there is a useful interpretation of the hyper skewness? Does someone know any literature about this feature? If there is no any available literature then I can perhaps compute the hyper skewness for a variety of distributions and try to find an interpretation. However, some known literature would spare me some time. Thanks in advance!","['statistics', 'finance', 'probability']"
1774229,Prove that $\angle{BED} = \angle{CFD} = 30^{\circ}$,"Let $D, E$ and $F$ be on $\overline{BC}, \overline{AC},$ and $\overline{AB},$ respectively, such that $\overline{AD}, \overline{BE},$ and $\overline{CF}$ are the angle bisectors of $\triangle {ABC}$. If $\angle{EDF}=90^{\circ}$, prove that $\angle{BED} = \angle{CFD} = 30^{\circ}$. I have a few conjectures on how prove this. Firstly, it seems that $DE$ is an angle bisector of $\angle{CDA}$ and similarly $DF$ is an angle bisector of $\angle{BDA}$. Also it seems that $\overline{EF} \parallel \overline{BC}$, so that might help, but I don't see how to show $\angle{BED} = \angle{CFD} = 30^{\circ}$.",['geometry']
1774249,Exponential integration,"I am working for an investment institution and we need to use upper partial moments. To evaluate them, I need to integrate $$  \int_a^\infty x^2 e^{-ax^2} dx  $$ with $a>0$. I found this formula online : $$ \int x^2 e^{-ax^2} dx = \frac{1}{4} \sqrt{\frac{\pi}{a^3}}erf \left(x\sqrt{a} \right) - \frac{x}{2a}e^{-ax^2} $$ I was hoping someone can help me prove it so I can be convinced before I use it. Thanks!",['integration']
1774319,Relationship to weak toplogy (Lévy metric),"By $P(\Omega)$, denote the space of all probability measures on $(\mathbb{R},\mathcal{B})$. Let $F_{\mu}$ denote the distribution function of $\mu\in P(\Omega)$. Let, 
$$
d_L(\mu,\nu):=\inf\left\{\varepsilon\geq 0: F_{\mu}(x-\varepsilon)-\varepsilon\leq F_{\nu}(x)\leq F_{\mu}(x+\varepsilon)+\varepsilon~\forall x\in\mathbb{R}\right\}
$$
This is the so-called Lévy metric . Show that $\lim_{n\to\infty}F_{\mu_n}(x)=F_{\mu}(x)$ for all points $x\in\mathbb{R}$ in which $F$ is continuous $\Leftrightarrow d_L(\mu_n,\mu)\to 0$ Its a bit confusing to me, did not find a start.","['metric-spaces', 'measure-theory']"
1774367,"Showing that $\|.\|$ is a norm of the space of 1-forms $\Omega^1(U)$, where $U\subset\mathbb{R}^n$.","Let $U\subset\mathbb{R}^n$ and let $\Omega^p(U)$ denote the vector space of $p$-forms ($p\in\mathbb{N}$). Define the isomorphism $\Phi:\Omega^{1}(U)\to\Omega^{n-1}(U)$ as $$\Phi\left(\omega=\sum_{i=1}^nf_idx_i\right)=\sum_{i=1}^n(-1)^{i-1}f_idx_1\ldots dx_{i-1}dx_{i+1}\ldots dx_n$$ 
 It can be easily shown that $\Phi$ is an isomorphism. Now define for $\omega\in\Omega^{1}(U)$,
$$\|\omega\|=\int_U\omega\:\Phi(\omega)$$
We want to show that $\|.\|$ is actually a norm on $\Omega^1(U)$. So I started by the following. I want to show that $\|\omega\|\geq 0$. Notice that $$\int_U\omega\:\Phi(\omega)=\int_U\sum_{i=1}^n\sum_{j=1}^n(-1)^{j-1}(f_if_j)dx_idx_1\ldots dx_{j-1}dx_{j+1}\ldots dx_n$$
Now notice that the term $(-1)^{j-1}(f_if_j)dx_idx_1\ldots dx_{j-1}dx_{j+1}\ldots dx_n\not=0$ only if $i=j$. If not, there will be two $dx_i$'s and term will be $0$. So the sum can be rewritten 
$$\int_U\omega\:\Phi(\omega)=\int_U\sum_{i=1}^n(-1)^{i-1}f_i^2dx_idx_1\ldots dx_{i-1}dx_{i+1}\ldots dx_n$$. Now in order to rearrange $dx_idx_1\ldots dx_{i-1}dx_{i+1}\ldots dx_{n}$ to $dx_1\ldots dx_n$ we need to do $i-1$ swaps and we get
\begin{align*}\int_U\omega\:\Phi(\omega)&=\int_U\sum_{i=1}^n(-1)^{i-1}\cdot f_i^2\cdot (-1)^{i-1}dx_1\ldots dx_n\\&=\int_U\sum_{i=1}^n f_i^2 dx_1\ldots dx_n\\&=\sum_{i=1}^n\int_Uf_i^2dx_1\ldots dx_n\geq 0\end{align*} Also is easy to show that $\|\omega\|=0$ if and only $\omega=0$. I got stuck when showing the triangle inequality and scaling. Here my try :to show the triangle inequality, let $\omega,\eta\in\Omega^1(U)$ and using the fact that $\Phi$ is a homomorphism, \begin{align*}\|\omega+\eta\|&=\int_U(\omega+\eta)\Phi(\omega+\eta)=\int_U(\omega+\eta)\left(\Phi(\omega)+\Phi(\eta)\right)\\&=\int_U\omega\Phi(\omega)+\eta\Phi(\omega)+\omega\Phi(\eta)+\eta\Phi(\eta)\\&=\|\omega\|+\|\eta\|+\int_U\eta\Phi(\omega)+\omega\Phi(\eta)\end{align*} I got suck here. Can anyone help with the triangle inequality and scaling? EDIT : If we redefine $\|\omega\|=\sqrt{\int_U\omega\:\Phi(\omega)}$ then $\|\omega\|$ is still greater than 0, equal if and only if $\omega=0$. And for scaling we get 
$$\|c\omega\|=\sqrt{\int_U c\omega\:\Phi(c\omega)}=\sqrt{c^2\int_U \omega\:\Phi(\omega)}=|c|\sqrt{\int_U \omega\:\Phi(\omega)}=|c|\|\omega\|$$
and what remains to show is the triangle inequality. Any help?","['multivariable-calculus', 'differential-forms', 'linear-algebra']"
1774396,Is a $90\%$ confidence interval really $90\%$ confident?,"Let's say you are estimating a population proportion, which you model as binomial. One source of error already is using the normal approximation to the binomial when getting your critical values. But what bothers me more is that the theoretically sound interval uses the true population proportion in computation of the interval width. This is usually approximated by the sample proportion, but doesn't this no longer make the confidence level accurate? (It seems like a pretty Bayesian assumption for a frequentist approach to get away with.) As a common tactic, I see people use the upper bound for the population variance (by assuming the proportion is $1/2$), and use that to determine their intervals. Is this preferable to using the sample proportion to estimate the population variance? At least in this upper bounding case, we can say with mathematical soundness, that our confidence level is at least $90\%$confident (assuming normal perfectly approximates binomial).","['statistics', 'confidence-interval']"
1774425,is every function defined on natural numbers considered a sequence?,"I wonder if every function that operates over the set of natural numbers (or its finite subset ranging from $1$ to $k$) is considered a sequence? It seems to me that the answer is yes, at least when one looks at the common definition of a sequence: $$a: \mathbb{J} \to X$$
where: $\mathbb{J}$ is either $\mathbb{N}$ or $\{1, 2,..., k\}$","['sequences-and-series', 'functions']"
1774465,What's the second Fréchet derivative of a function $\mathbb R^d\to\mathbb R$,"Let $u:\mathbb R^d\to\mathbb R$ be twice Fréchet differentiable. What's the second Fréchet derivative ${\rm D}^2u$ of $u$? It's clear that ${\rm D}u$ is a mapping$^1$ $\mathbb R^d\to\mathfrak L(\mathbb R^d,\mathbb R)$ and ${\rm D}^2u$ is a mapping $\mathbb R^d\to\mathfrak L(\mathbb R^d,\mathfrak L(\mathbb R^d,\mathbb R))$. From that, it's easy to see that $${\rm D}u(x)y=\nabla u(x)\cdot y=\sum_{i=1}^dy_i\frac{\partial u}{\partial x_i}(x)=(y\cdot\nabla)u(x)\;\;\;\text{for all }x,y\in\mathbb R^d\;.\tag 1$$ How can we obtain ${\rm D}^2u$? I'm sure ${\rm D}^2u$ can be expressed somehow in terms of the Hessian $\nabla^2u$, maybe we've got $${\rm D}^2u(x)yz=\nabla^2u(x)y\cdot z\;\;\;\text{for all }x,y,z\in\mathbb R^d\;,$$ but I'm unsure. $^1$ Let $\mathfrak L(A,B)$ be the set of bounded, linear operators from $A$ to $B$.","['derivatives', 'partial-derivative', 'operator-theory', 'functional-analysis', 'frechet-derivative']"
1774514,"How did the name ""The Calculus"" come about, was there a reason or just good marketing?","This is a historical and lighthearted question about etymology. The area of mathematics that deals with limiting processes over real numbers (Real Analysis) or real vector spaces, or even complex vector spaces (I think it depends on who you ask) is variably referred to as ""analysis"" or "" the Calculus"". My question is, why did this particular area of math get the somewhat grandiose name The Calculus , which, if taken literally, means something like ""The way to reason"". It seems like linear algebra, geometry, logic, and many other formal systems are just as worthy of this title in their areas of application, yet the mathematics that was developed by Newton, Leibniz, Weierstrass & Co. has taken this moniker without contest. Any anyone fill in the history of why this particular branch of mathematics got this title? I know calculus is a great achievement, has lots of applications, etc...but I don't think that is the reason. Half-jokingly, maybe it was just good marketing on the developers of Calculus...give it a great name so people pay attention to it ;-)","['math-history', 'soft-question', 'calculus']"
1774544,"Function field, finite extension, isomorphism implies isomorphism?","Let $A$ be a function field in $1$ variable over $\mathbb{C}$, and let $B$ be a finite extension of $A$ of degree $[B : A]$. If $B \cong \mathbb{C}(x)$ over $\mathbb{C}$, then does it necessarily follow that $A \cong \mathbb{C}(x)$ over $\mathbb{C}$? Here, $\mathbb{C}(x)$ is the field of rational functions in $x$ whose coefficients are in $\mathbb{C}$. Thought. Perhaps we want to use the fact that a function field in $1$ variable over an algebraically closed field $k$ is of genus $0$ if and only if $F \cong k(x)$ over $k$, somehow?","['algebraic-geometry', 'abstract-algebra', 'algebraic-number-theory', 'number-theory', 'complex-analysis']"
1774561,Recast the scalar SPDE $du_t(Φ_t(x))=f_t(Φ_t(x))dt+∇ u_t(Φ_t(x))⋅ξ_t(Φ_t(x))dW_t$ into a SDE in an infinite dimensional function space.,"Let$^1$ $(\Omega,\mathcal A,\operatorname P)$ be a probability space $U$ be a separable Hilbert space $Q\in\mathfrak L(U)$ be nonnegative and symmetric operator on $U$ with finite trace $(W_t)_{t\ge0}$ be a $Q$-Wiener process on $(\Omega,\mathcal A,\operatorname P)$ $H:=\mathbb R^d$ for some $d\in\mathbb N$ and $\xi:\Omega\times[0,\infty)\times\mathbb R^d\to\operatorname{HS}(U_0,H)$, where $U_0:=Q^{1/2}U$. Suppose we're concerned with an SPDE $${\rm d}u_t\left(\Phi_t\left(x\right)\right)=f_t\left(\Phi_t\left(x\right)\right){\rm d}t+\nabla u_t\left(\Phi_t\left(x\right)\right)\cdot\xi_t\left(\Phi_t\left(x\right)\right){\rm d}W_t\;\;\;\text{for all }t\ge 0\text{ and }x\in H\;,\tag 1$$ where $u:\Omega\times[0,\infty)\times H\to\mathbb R$ $\Phi:\Omega\times[0,\infty)\times H\to H$ $f:\Omega\times[0,\infty)\times H\to\mathbb R$ are suitable. Can we recast $(1)$ into an equation in $\tilde H:=L^2(\mathbb R^d;\mathbb R^d)$? I want to get rid of the second parameter of $u$, i.e. I want to turn the finite-dimensional multiparameter SDE $(1)$ indexed by time and space into an infinite dimensional single parameter SDE indexed by time only. $^1$ Let $\mathfrak L(A,B)$ be the set of bounded, linear operators from $A$ to $B$. Moreover, Let $\operatorname{HS}(A,B)$ be the set of Hilbert-Schmidt operators from $A$ to $B$.","['stochastic-processes', 'stochastic-pde', 'probability-theory', 'stochastic-calculus', 'stochastic-differential-equations']"
1774564,Confusion about Doob–Dynkin lemma,"Doob–Dynkin lemma: Let $X$,$Y$ $:\Omega \rightarrow R^n$ be random elements and $\sigma(X)$ be the $\sigma$ algebra generated by $X$. Then $Y$ is $\sigma(X)$-measurable if and only if $Y=g(X)$ for some Borel measurable function $g:R^n\rightarrow R^n$. Now I want to use this lemma to simplify $\sum_{n=0}^{\infty}E[f(X_{n+1}) \vert \sigma(X_n)] 1_{\{N=n\}}$ where $N<\infty$ is any discrete stopping time and $f$ is a bounded measurable function. I know that by definition, conditional expectation $E[f(X_{n+1}) \vert \sigma(X_n)]$ is $\sigma(X_n)$-measurable. So, by applying Doob–Dynkin lemma, $E[f(X_{n+1}) \vert \sigma(X_n)]= g(X_n)$ and then,
\begin{equation}
\sum_{n=0}^{\infty}E[f(X_{n+1}) \vert \sigma(X_n)] 1_{\{N=n\}}
= \sum_{n=0}^{\infty} g(X_n) 1_{\{N=n\}}   \hspace{5mm}(1)
\end{equation} But I am not sure about the next step. I think it is wrong to substitute $n$ in $g(X_n)$ by $N$ because in that case, it means that $E[f(X_{N+1}) \vert \sigma(X_N)]= g(X_N)$ which doesn't seem correct to me because without fixing the value of $N$, the definition of $\sigma(X_N)$ is not clear to me. If we do this seemingly wrong substitution, then $\sum_{n=0}^{\infty}E[f(X_{n+1}) \vert \sigma(X_n)] 1_{\{N=n\}}
= \sum_{n=0}^{\infty} g(X_n) 1_{\{N=n\}} =  g(X_N) \sum_{n=0}^{\infty} 1_{\{N=n\}} =  g(X_N).
$ But As I mentioned above, I think this substitution is wrong and (1) at the end should also depend on $N$. Am I right? Any idea for simplifying (1)?","['probability-theory', 'conditional-expectation', 'measure-theory']"
1774595,What is the difference between the projection onto the column space and projection onto row space?,"If I see a question that asks ""find the projection a vector $b$ onto a matrix $A$"" I would either solve by using $A^TA\hat x =A^Tb$ and then the projection would equal $A\hat x$, and if the matrix $A$ was orthogonal then I would use $proj_bA = \frac{b \cdot q_1}{q_1 \cdot q_1}q_1 + ... + \frac{b \cdot q_k}{q_k \cdot q_k}q_k$ where $q_k$ represents the $k^{th}$ vector in matrix $A$. My question is what if a question says find the projection of some vector $b$ onto the column/row space of matrix $A$? What does this mean and what would I need to do differently to calculate it?",['linear-algebra']
1774604,Why are the irreducible components $T$-stable?,"I'm having trouble with part of a proof (7.1.5) in Springer's Linear Algebraic Groups .  Let $r: G \rightarrow \textrm{GL}(V)$ be a rational representation of a linear algebraic group $G$, $B$ a Borel subgroup of $G$, and suppose that there exists a nonzero $v \in V$ such that $V$ is spanned by $g \cdot v : g \in G$, and $$B = \{ g \in G: gv \in kv\}$$ Let $T$ be a maximal torus of $G$ which is one dimensional, $\lambda: k^{\ast} \rightarrow T$ an isomorphism, $e_1, ... , e_n$ a basis for $V$, such that for some integers $m_1 \geq \cdots \geq m_n$, not all equal, we have $$\lambda(a)e_i = a^{m_i}e_i$$ Now $r$ induces an isomorphism of $G/B$ onto a closed subvariety $Z$ of $\mathbb{P}(V)$, the set of one dimensional subspaces of $V$, via $gB \mapsto [g\cdot v]$.  Let $U_1$ be the affine open set in $\mathbb{P}(V)$ consisting of points with nonzero first coordinate (with respect to the basis $e_i$).  Then $\mathbb{P}(V) \setminus U_1$ is closed, and $$[\mathbb{P}(V) \setminus U_1] \cap Z$$ is closed and clearly $T$-stable.  Springer claims that the irreducible components of $[\mathbb{P}(V) \setminus U_1] \cap Z$ are also $T$-stable, but gives no indication of why.  Why should this be the case?","['algebraic-groups', 'algebraic-geometry']"
1774608,Convergence in Probability - Definition in Billingsley,"In defining convergence in probability on p. 70, Billingsley considers the complement of the event $\{\omega \in \Omega : \lim_{n \to \infty} X_n(\omega) = X(\omega)\}$ and writes this as
$$
\left\{ \lim_{n \to \infty} X_n = X\right\}^c = \bigcup_{\epsilon > 0} \left\{ |X_n - X| \geq \epsilon \,\text{ i.o.}\right\}.
$$
This is fine, but he then states, ...the union can be restricted to rational (positive) $\epsilon$ because the set in the union increases as $\epsilon$ decreases. I'm trying to work out why this is so.  For any $\epsilon > 0$ let $A_\epsilon := \left\{ |X_n - X) \geq \epsilon \,\text{ i.o.}\right\}$.  Then for any $\epsilon_1 < \epsilon_2$ we have $A_{\epsilon_1} \supset A_{\epsilon_2}$, and I suppose want to use this with the fact that $(0,\infty) \cap \mathbb{Q}$ is dense in $(0,\infty)$, but I'm having trouble seeing the way forward. Updated attempt: Select any $\epsilon > 0$ and the corresponding $A_{\epsilon}$.  Then there is an $n \in \mathbb{N}$ such that $\frac{1}{n} < \epsilon$.  Thus $A_\epsilon \subset A_{1/n} \subset \cup_n A_{1/n}$.  Now select any $n \in \mathbb{N}$ and the corresponding $A_{1/n}$.  Then there is an $\epsilon > 0$ such that $\epsilon < \frac{1}{n}$.  Thus $A_{1/n} \subset A_\epsilon \subset \cup_\epsilon A_\epsilon$.","['real-analysis', 'probability-theory']"
1774639,Is it true that finite intersection distributes over arbitrary unions?,"I have come across the problem of showing that $$\bigcap_{i=1}^n \Big ( \bigcup_{\alpha\in A} X_\alpha^{(i)}\Big) = \bigcup_{\alpha\in A} \Big ( \bigcap_{i=1}^n X_\alpha^{(i)} \Big)$$ for some family of sets $\{ A_\alpha^{(i)} \}$. Is this statement even true for arbitrary sets? If not, what conditions do I need to assert this.","['general-topology', 'elementary-set-theory']"
1774642,"Trigonometry, obtuse angles and a negative length?","I've been looking for an answer as to why when $\cos x<0$ and $\tan x<0$ the angle is obtuse. I found a few identical explanations online where, a right-angle triangle is formed in the second quadrant from the corner (of the obtuse angle triangle) where the hypotenuse and opposite side meet. Here they show that the length of the adjacent side of the newly formed right angled triangle is negative, which as a result causes $\cos x<0$ and $\tan x<0$ since $\cos x=\frac{-a}{h}$ and $\tan x=\frac{o}{-a}$. But how can a length be negative? Is the explanation wrong? Or am I missing something?",['trigonometry']
1774668,Frechet derivative in a Hilbert space,"Let $\mathcal{H}$ be a Hilbert space and $A$ a self-adjoint operator.
With $(\, ,\, )$ denoting the inner product and $\psi\in \mathcal{H}$, I want to formally show that the Frechet derivative of the expression $F[\psi]=\left(\psi, \, A\psi \right)$ at $\varphi$ is equal to $d_\varphi F[\psi]=\left(\varphi, \, A\psi \right) + \left(A \psi, \, \varphi \right) $. The way I show it is the following. For some $\epsilon\in \mathbb{R}$, I write
\begin{align}
F[\psi+\epsilon\varphi]=\left(\psi+\epsilon \varphi, \, A(\psi+\epsilon\varphi) \right)= \left(\psi, \, A\psi \right) +\epsilon \left(A\psi, \, \varphi \right)+\epsilon \left(\varphi, \, A\psi \right)+\epsilon^2 \left(\varphi, \, A\varphi \right).
\end{align}
Finally,
\begin{equation}
\lim_{\epsilon\rightarrow 0} \frac{F[\psi+\epsilon\varphi]-F[\psi]}{\epsilon}=\left(A\psi, \, \varphi \right)+ \left(\varphi, \, A\psi \right)= d_\varphi F[\psi].
\end{equation} Is this sufficient for my claim or have I missed anything? 
Moreover, assuming this is correct, how would I extend the above argument to something like $f(F[\psi])$, where $f$ is some function from $\mathbb{R}$ to $\mathbb{R}$. Essentially, I would like to know what would be the chain rule in this case, and what are the restrictions(if any) on the function $f$, for the Frechet derivative to exist? Thanks in advance!","['functional-analysis', 'frechet-derivative', 'operator-theory', 'inner-products']"
1774670,Looking for examples of Discrete / Continuous complementary approaches,"Among many fascinating sides of mathematics, there is one that I praise, especially for didactic purposes : the parallels that can be drawn between some ""Continuous"" and  ""Discrete"" concepts. I am looking for examples bringing a help to a global understanding... Disclaimer : Being driven, as said above, mainly by didactic purposes, I am not in need for  full rigor here although I do not deny at all the interest of having a rigorous approach in other contexts where it can be essential to show in particular in which sense the continuous ""object"" is the limit of its discrete counterparts. I should appreciate if some colleagues can give examples of their own, in the style ""my favorite one is..."", or references to works about this theme. Let me provide, on my side, five examples : 1st example: How to obtain the equations of certain epicycloids, here a nephroid : Consider a $N$ -sided regular polygon $A_1,A_2,\cdots A_N$ with any integer $N$ large enough, say around $50$ . Let us connect every point $A_k$ to point $A_{3k}$ by a line segment (we assume a cyclic numbering). As can be seen on Fig. 1, a certain envelope curve is ""suggested"". Question : which (smooth) curve is behind this construction ? Answer : Let us consider two consecutive line segments like those represented on Fig. 1 with a larger width : the evolution speed of $A_{3k} \to A_{3k'}$ where $k'=k+1$ is three times the evolution speed of $A_{k} \to A_{k'}$ , the pivoting of the line segment takes place at the point (of the line segment) which is 3 times closer to $A_k$ than to $A_{3k}$ (the weights' ratio 3:1 comes from the size ratio of ''homothetic'' triangles $P_kA_kA_k'$ and $P_kA_{3k}A_{3k'}$ .) Said in an algebraic way : $$P_k=\tfrac{3}{4}e^{ika}+\tfrac{1}{4}e^{3ika}$$ ( $A_k$ is identified with $e^{ika}$ with $a:=\tfrac{2 \pi}{N}$ ). Replacing now discrete values $ka$ by a continuous parameter $t$ , we get $$z=\tfrac{3}{4}e^{it}+\tfrac{1}{4}e^{3it}$$ i.e., a parametric representation of the nephroid, or the equivalent real equations : $$\begin{cases}x=\tfrac{3}{4}\cos(t)+\tfrac{1}{4}\cos(3t)\\
y=\tfrac{3}{4}\sin(t)+\tfrac{1}{4}\sin(3t)\end{cases}$$ Fig. 1 : The nephroid as an envelope. It can be viewed as the trajectory of a point of a small circle with radius $\dfrac14$ rolling inside a circle with radius $1$ . Remark: if, instead of connecting $A_k$ to $A_{3k}$ , we had connected it to $A_{2k}$ , we would have obtained a cardioid, with $A_{4k}$ an astroid, etc. 2nd example: Coupling ''second derivative $ \ \leftrightarrow  \ \min \ $ kernel'' : All functions considered here are at least $C^2$ , but function $K$ . Let $f:[0,1] \rightarrow \mathbb{R}$ and $K:[0,1]\times[0,1]\rightarrow \mathbb{R}$ (a so-called ""kernel"") defined by $K(x,y):=\min(x,y)$ . Let us associate $f$ with function $\varphi(f)=g$ defined by $$\tag{1}g(y)=\int_{t=0}^{t=1} K(t,y)f(t)dt=\int_{t=0}^{t=1} \min(t,y)f(t)dt$$ We can get rid of "" $\min$ "" function by decomposing the integral into : $$\tag{2}g(y)=\int_{t=0}^{t=y} t f(t)dt+\int_{t=y}^{t=1} y f(t)dt$$ $$\tag{3}g(y)=\int_{t=0}^{t=y} t f(t)dt - y F(y)$$ where we have set $$\tag{4}F(y):=\int_{t=1}^{t=y}f(t)dt \ \ \ \ \ \ \ \ \text{Remark:} \ \ \ F'(y)=f(y)$$ Let us differentiate (4) twice : $$\tag{5}g'(y)=y f(y) - 1 F(y) - y f(y) = -F(y)$$ $$\tag{6}g''(y)= -f(y)  \ \ \Longleftrightarrow \ \ f(y)=-g''(y)$$ Said otherwise, the inverse of transform $f \rightarrow \varphi(f)=g$ is: $$\tag{7}\varphi^{-1} = \text{opposite of the second derivative.}$$ This connexion with the second derivative is rather unexpected... Had we taken a discrete approach, what would have been found ? The discrete equivalents of $\varphi$ and $\varphi^{-1}$ are matrices : $$\bf{M}=\begin{pmatrix}1&1&1&\cdots&\cdots&1\\1&2&2&\cdot&\cdots&2\\1&2&3&\cdots&\cdots&3\\\cdots&\cdots&\cdots&\cdots&\cdots&\cdots\\\cdots&\cdots&\cdots&\cdots&\cdots&\cdots\\1&2&3&\cdots&\cdots&n
\end{pmatrix} \ \ \textbf{and}$$ $$\bf{D}=\begin{pmatrix}2&-1&&&&\\-1&2&-1&&&\\&-1&2&-1&&\\&&\ddots&\ddots&\ddots&\\&&&-1&2&-1\\&&&&-1&1
\end{pmatrix}$$ that verify matrix identity: $\bf{M}^{-1}=\bf{D}$ in analogy with (7). Indeed, Nothing to say about the connection of matrix $\bf{M}$ with coefficients $\bf{M}_{i,j}=min(i,j)$ with operator $K$ . tridiagonal matrix $\bf{D}$ is well known (in particular by all people doing discretization) to be ""the"" discrete analog of the second derivative due to the classical approximation: $$f''(x)\approx\dfrac{1}{2h^2}(f(x-h)-2f(x)+f(x+h))$$ that can easily be obtained using Taylor expansions. The exceptional value $1$ at the bottom right of $\bf{D}$ is explained by discrete boundary conditions. Remark: this correspondence between ""min"" operator and second derivative is not mine ; I known for a long time but  I am unable to trace back where I saw it at first (hopefully in a signal processing book). If somebody has a reference ? Connected : the eigenvalues of $D$ are remarkable ( http://www.math.nthu.edu.tw/~amen/2005/040903-7.pdf ) In the same vein : computation of adjoint operators . 3rd example : the Itô integral. One could think that the Lebesgue integral (1902) is the ultimate theory of integration, correcting the imperfections of the theory elaborated by Riemann some 50 years before. This is not the case. In particular, Itô has defined (1942) a new kind of integral which is now essential in probability and finance. Its principle, roughly said, is that infinitesimal ""deterministic"" increments ""dt"" are replaced by random increments of brownian motion type ""dW"" as formalized by Einstein (1905), then by Wiener (1923). Let us give an image of it. Let us first recall definitions of brownian motion $W(t)$ or $W_t$ , ( $W$ for Wiener), an informal one, and a formal one: Informal : A ""particle"" starting at $x=0$ at time $t$ , jumps ""at the next instant"" $t+dt$ , to a nearby position; either on the left or on the right, the amplitude and sign of the jump being governed by a normal distribution $N(x,\sigma^2)$ with an infinitesimal fixed standard deviation $\sigma.$ $\text{Formal}: \ \ W_t:=G_0 t+\sqrt{2}\sum_{n=1}^{\infty}G_n\dfrac{\sin(\pi n t)}{\pi n}$ , with $G_n$ iid $N(0,1)$ random variables. (Other definitions exist. This one, under the form of a ""random Fourier series"" is handy for many computations). Let us now consider one of the fundamental formulas of Itô's integral, for a continuously differentiable function $f$ : $$\tag{8}\begin{equation}
\displaystyle\int_0^t f(W(s))dW(s) = \displaystyle\int_0^{W(t)} f(\lambda)d \lambda - \frac{1}{2}\displaystyle\int_0^t f'(W(s))ds.
\end{equation}$$ Remark: The integral sign on the LHS of (8) defines Itô's integral, whereas the integrals on the RHS have to be understood in the sense of Riemann/Lebesgue. The presence of the second term on the RHS is rather puzzling, isnt'it ? Question: how can be understood/justified this second integral ? Szabados has proposed (1990) (see ( https://mathoverflow.net/questions/16163/discrete-version-of-itos-lemma )) a discrete analog of formula (8). Here is how it runs: Theorem: Let $f:\mathbb{Z} \longrightarrow \mathbb{R}$ . let us define : $$
\tag{9}\begin{equation}
F(k)=\left\{
\begin{matrix}
\dfrac{1}{2}f(0)+\displaystyle\sum_{j=1}^{k-1} f(j)+\dfrac{1}{2}f(k) & if & k \geq 1 & \ \ (a)\\
0  & if & k = 0 & \ \ (b)\\
-\dfrac{1}{2}f(k)-\displaystyle\sum_{j=k+1}^{-1} f(j)-\dfrac{1}{2}f(0)  & if & k \leq -1 & \ \ (c)
\end{matrix}
\right.
\end{equation}
$$ Remarks: We will work only on (a) and its particular case (b). (a) is nothing else than the ""trapezoid formula"" explaining in particular factors $\dfrac{1}{2}$ in front of $f(0)$ et $f(k)$ . Let us now define a family of Random Variables $X_k$ , $k=1, 2, \cdots $ , iid on $\{-1,1\}$ with $P(X_k=-1)=P(X_k=1)=\frac{1}{2}$ , and let $$
\begin{equation}
S_n= \displaystyle\sum_{k=1}^n X_k. 
\end{equation}
$$ Then $$
\tag{10}\begin{equation}
\forall n, \ \ \displaystyle\sum_{i=0}^{n}f(S_i)X_{i+1} = F(S_{n+1})-\dfrac{1}{2}\displaystyle\sum_{i=0}^{n}\dfrac{f(S_{i+1})-f(S_{i})}{X_{i+1}}
\end{equation}
$$ Remark : Please note analogies : between $\frac{f(S_{i+1})-f(S_{i})}{X_{i+1}}$ and $f'(S_i)$ . between $F(k)$ and $\displaystyle\int_{\lambda=0}^{\lambda=k}f(\lambda)d\lambda$ . For example, a) If $f$ is identity function ( $\forall k \  f(k)=k$ ), definition (9)(a) gives : $$
\begin{equation}
F(k)=\frac{1}{2}(k-1)k+\frac{1}{2}k=\dfrac{1}{2}k^2.
\tag{11}
\end{equation}
$$ which doesn't come as a surprise : the 'discrete antiderivative' of $k$ is $\frac{1}{2}k^2$ ... (the formula in (11) remains in fact the same for $k<0$ ). b) If $f$ is the ""squaring function"" ( $\forall k, \ f(k)=k^2$ ), (9)(a) becomes : $$
\begin{equation}
\text{If} \  k>0, \ \ \  F(k)=\frac{1}{6}(k-1)k(2k-1)+\frac{1}{2}k^2=\dfrac{1}{3}k^3+\dfrac{1}{6}k.
\tag{12}
\end{equation}
$$ This time, a new term $\dfrac{1}{6}k$ has entered into the play. Proof of the Theorem: The definition allows to write : \begin{equation}F(S_{i+1})-F(S_i)=f(S_i)X_{i+1}+\frac{1}{2}\dfrac{f(S_{i+1})-f(S_i)}{X_{i+1}}
\tag{13}
\end{equation} In fact, proving (11) can be split into two cases: either $X_{i+1}=1$ , or $X_{i+1}=-1$ . Let us consider the first case (the second case is similar): the RHS of (13) becomes $f(S_i)+\frac{1}{2}(f(S_{i+1})-f(S_i))=\frac{1}{2}(f(S_{i+1})+f(S_i))$ which is the area variation in the trapezoid formula ; Summing all equations in (10) gives the desired identity. An example of application : Let $f(t)=t$ ; we get $$\displaystyle\sum_{i=0}^{n}S_iX_{i+1} = F(S_{n+1})-\frac{n}{2}=\dfrac{1}{2}S_{n+1}^2-\frac{n}{2}.$$ which appears as the discrete equivalent of the celebrated formula: $$
\begin{equation}
\displaystyle\int_0^t W(s)dW(s) = \frac{1}{2}W(t)^2-\frac{1}{2}t.
\end{equation}
$$ One can establish the autocorrelation of $W_t$ process is $$cov(W_s,W_t)=E(W_sW_t)-E(W_s)E(W_t)=\min(s,t),$$ (see ( Autocorrelation of a Wiener Process proof )) providing an unexpected connection with the second example... Last remark : Another kind of integral based on a discrete definition : the gauge integral ( https://math.vanderbilt.edu/schectex/ccc/gauge/ ). 4th example (Darboux sums) : Here is a discrete formula : $$\prod_{k=1}^{n-1}\sin\frac{k \pi}{n} = \frac{n}{2^{n-1}}$$ (see a proof in Prove that $\prod_{k=1}^{n-1}\sin\frac{k \pi}{n} = \frac{n}{2^{n-1}}$ ) Has this formula a continuous ""counterpart"" ? Taking the logarithm on both sides, and dividing by $n$ , we get : $$\tfrac1n \sum_{k=1}^n \ln \sin \tfrac{k \pi}{n}=\tfrac{\ln(n)}{n}-\ln(2)\tfrac{n-1}{n}$$ Letting now $n \to \infty$ , we obtain the rather classical integral : $$\int_0^1 \ln(\sin(\pi x))dx=-\ln(2)$$ 5th example : bivariate cdfs (cumulative probability density functions). Let $(X,Y)$ a pair of Random Variables with pdf $f_{X,Y}$ and cdf : $$F_{X,Y}(x,y):=P(X \leq x \ \& \ Y \leq y).$$ Take a look at this formula : $$P(x_1<X \leq x_2, \ \ y_1<Y \leq y_2)=F_{XY}(x_2,y_2)-F_{XY}(x_1,y_2)-F_{XY}(x_2,y_1)+F_{XY}(x_1,y_1)\tag{14}$$ ( https://www.probabilitycourse.com/chapter5/5_2_2_joint_cdf.php ) It is the discrete equivalent of the continuous definition of $F_{XY}$ as the mixed second order partial derivative of $F_{X,Y}$ , under the assumption that $F$ is a $C^2$ function : $$f_{XY}(x,y)=\dfrac{\partial^2 F_{X,Y}}{\partial x \partial y}(x,y).\tag{15}$$ Do you see why ? Hint : make $x_2 \to x_1$ and make $y_2 \to y_1$ and assimilate the LHS of (14) with $f(x_1,y_1)dxdy$ . Final remarks : A remarkable text about this analogy in Physics : https://www.lptmc.jussieu.fr/user/lesne/MSCS-Lesne.pdf In linear algebra, continuous analogs of some fundamental factorizations ( https://royalsocietypublishing.org/doi/pdf/10.1098/rspa.2014.0585 ). A similar question on MathOverflow mentionning in particular the following well written book ""Computing the continuous discretely"" by  by Beck and Robins. There are many other tracks, e.g., connections with graphs ""Discrete and continuous : two sides of the same."" by L. Lovàsz or this one ( http://jimhuang.org/CDNDSP.pdf ), discrete vs. continuous versions of the logistic equation ( https://math.stackexchange.com/q/3328867 ), etc. In the epidemiology domain: ""Discrete versus continuous-time models of malaria infections"" lecture notes by Lucy Crooks, ETH Zürich . Another example in probability: the connection between a discrete and a continuous distribution, i.e., the Poisson( $\lambda$ ) distribution and the $\Gamma(n)$ distribution which is well treated in [this answer] ( https://math.stackexchange.com/q/2228023 ).","['big-list', 'probability', 'linear-algebra', 'calculus']"
1774676,Provide examples or explain why it is impossible,"a) A continuous function defined on an open interval with range equal to a closed interval. My example: $f(x)=\frac{1}{2}\sin(4\pi x)+\frac{1}{2}$ on $(0,1)$ to $[0,1]$.
Note: I am not considering $\mathbb{R}$ an interval. b) A continuous function defined on a closed interval with range equal to an open interval. I think this is impossible if we exclude $\mathbb{R}$. Edit: we must also exclude unbounded intervals. By the Extreme Value Theorem, any  continuous function on a compact set attains a maximum and a minimum. Yet, the set of the points in an open interval doesn't include its supremum and infimum, a contradiction. c) A continuous function defined on an open interval with range equal to an unbounded closed set different from $\mathbb{R}$. My example: $f(x)=\sqrt{|x|}$ on $\mathbb{R}$ to [0,$\infty$). Is there another function that works and has a different domain than $\mathbb{R}$? d) A continuous defined on all of $\mathbb{R}$ with range equal to $\mathbb{Q}$. I was thinking maybe map the natural numbers to $\mathbb{Q}$ and use the rest to ""fill in the gaps."" Evidently, I need most help with d). Thanks in advance!","['general-topology', 'real-analysis', 'continuity', 'limits']"
1774682,"What is the most useful/intuitive way to define $\Bbb{C}$, the complex set?","I was wondering whether there is a more common or 'best' way to define/think about the set of all complex number in complex analysis? The first way I can think of is: $\Bbb{C} \triangleq \Bbb{R} \times \Bbb{I} = \{(a,b) \mid a\in\Bbb{R}, b\in\Bbb{I}\}$ Or is it more useful, down the road, to think of $\Bbb{C}$ as the set of all vector sums: $\Bbb{C} \triangleq \{\vec{a}+\vec{b} \mid \vec{a}\in\Bbb{R}^1, \vec{b}\in\Bbb{I}^1\}$ and just remembering that you cannot add members/1-D vectors of $\Bbb{R}$ to members/1-D vectors of $\Bbb{I}$. I guess another question would be how do you guys think about them?","['complex-analysis', 'complex-numbers']"
1774698,Show that $\lim_{n\to \infty}\prod_{i=n}^{Bn}\frac{\arctan(i\phi)}{\arccos\left(\frac{\phi}{i}\right)}=B^{\frac{2}{\pi}}$,"Inspired from Gosper's formula $$\lim_{n\to \infty}\prod_{i=n}^{2n}\frac{\pi}{2\arctan(i)}=4^{\frac{1}{\pi}}$$ (See Pi Formulas on MathWorld ) Through mathematical experimental we found another formula similar to that of Gosper $\phi=\frac{1+\sqrt5}{2}$ B is an integer, $B\ge2$ $$\lim_{n\to \infty}\prod_{i=n}^{Bn}\frac{\arctan(i\phi)}{\arccos\left(\frac{\phi}{i}\right)}=B^{\frac{2}{\pi}}$$ We need help on proving it. Can anyone help us to prove it.","['trigonometry', 'limits']"
1774756,Continuous maps in topology; the definition?,"I am just wondering, given the definition of continuous maps as follows, A functionn $f:X \to Y$ is continuous if for every open subset $U $ of $Y$ the preimage $f^{-1}U$ is open in $X$. I guess mathematically, this doesn't necessarily mean that ""an open subset of $X$ is mapped to an open subset in $Y$""? It's only that the open subset of $Y$ must originate from an open subset in $X$, but not necessarily that every open $V$ of $X$ will be mapped to some open $U$ of $Y$. Is this understanding correct?",['general-topology']
1774785,Invertibility of a linear transformation without knowing its matrix,"Let $\mathbb{V}$ be a finite-dimensional inner product space, and let $\mathbb{W} \subset \mathbb{V}$ be a subspace. Define $T:\mathbb{V} \rightarrow \mathbb{V}$ by $$T(\overrightarrow v)=\overrightarrow v + Proj_{W}\overrightarrow v$$ Show that $T$ is invertible. My approach is to show that $T$ is injective and since $T$ goes from $\mathbb{V}$ to $\mathbb{V}$, that would imply that it is bjective and therefore invertible. I let an arbitrary vector $\overrightarrow v \in KerT$. $T(\overrightarrow v)=\overrightarrow 0$ $\Rightarrow$ $\overrightarrow v=-Proj_{W}\overrightarrow v$ This would mean that $\overrightarrow v$ is a linear combination of vectors which pertain to a basis of $\mathbb{W}$.
$\therefore Proj_{W}\overrightarrow v=\overrightarrow v$ $\therefore \overrightarrow v=-\overrightarrow v \Rightarrow \overrightarrow v=\overrightarrow 0$ Therefore $T$ is bijective and invertible. Is my approach correct?","['linear-algebra', 'inverse']"
1774792,$\displaystyle\lim _{x\to 0}\frac{\sqrt{1+x+x²}-1}{\sqrt{1+x}-\sqrt{1-x}}$,"First I need to prove that this limit; $\displaystyle\lim _{x\to 0}\frac{\sqrt{1+x+x^2}-1}{\sqrt{1+x}-\sqrt{1-x}}$ converges, then I have to find its limit. Now I don't know how to prove that it converges (these epsilon proofs are still something that I'm trying to learn). And to actually find the limit; I tried to rewrite it by multiplying by $\displaystyle{\sqrt{1+x}+\sqrt{1-x}\over \sqrt{1+x}+\sqrt{1-x}}$ but I didn't get any further.. Edit: I know that it converges to $1/2$, but that's what wolfram alpha says :|","['real-analysis', 'analysis', 'limits']"
1774794,"Prove that $a^2b+b^2c+c^2a \leqslant 3$ for $a,b,c >0$ with $a^ab^bc^c=1$","Let $a,b,c >0$ and $a^ab^bc^c=1$ . Prove that $$a^2b+b^2c+c^2a \leqslant 3.$$ I don't even know what to do with the condition $a^ab^bc^c=1$ . At first I think $x^x>1$ , but I was wrong. This inequality is true, following by the verification from Mathematica","['real-analysis', 'inequality']"
1774826,"When is $\nabla^2 f (x, y, z)= $ probability measure?","When is $\nabla^2 f(x, y, z) $= probability density function ?
That is $\nabla^2 f(x, y, z)= \mu (x, y, z)$ $\int \mu (x, y, z) dxdydy = 1 $ What conditions must $f(x,y,z) $ satisfy? It is known to physicist as 'Poisson's equation' where $ f(x, y, z)$  is the potential. But it will be of practical interest to state conditions necessary or sufficient which the potential must satisfy.","['probability-theory', 'probability', 'probability-distributions']"
1774843,Fiber of morphism homeomorphic to $f^{-1}(y)$,"I want to solve exercise 3.10 (a) of Hartshorne's book, chapter II, which asks to prove the following: Let $f\colon X\to Y$ be a morphism of schemes and let $y\in Y$, then $X_y=X\times_Y \operatorname{Spec}k(y)$ is homeomorphic to $f^{-1}(y)$ with the induced topology. The idea is clear to me and I proved the statement for affines. Now, choosing some open affine $V=\operatorname{Spec}A$ of $Y$ containing $y$, it follows that $X_y\cong f^{-1}(V) \times_V \operatorname{Spec}k(y)$. Thus, it suffices to prove that the projection of the latter one to $f^{-1}(V)$ induces a homeomorphism with $f^{-1}(y)$. Cover $f^{-1}(V)$ by open affines $U_i=\operatorname{Spec}B_i$. By the construction of the fiber product and in particular of the projection by gluing, the projection $p\colon f^{-1}(V) \times_V \operatorname{Spec}k(y) \to f^{-1}(V)$ is obtained by gluing the projections $p_i\colon \operatorname{Spec}(B_i \otimes_A k(y)) \to \operatorname{Spec}B_i \hookrightarrow f^{-1}(V)$. $\textbf{The problem is:}$ I don't see why $p$ should remain injective. Let $z,z'$ be two points of $f^{-1}(V) \times_V \operatorname{Spec}k(y)$, then $z \in \operatorname{Spec}B_i \otimes_A k(y)$ and $z'\in \operatorname{Spec}B_j \otimes_A k(y)$ for some $i,j$. Now, $p(z)=p(z')$ implies that $z \in p^{-1}(f^{-1}(y)\cap \operatorname{Spec}B_i \cap \operatorname{Spec}B_j)=p^{-1}(\operatorname{Spec}B_i \cap \operatorname{Spec}B_j)$. Thus, I $\textbf{need to prove}$ that $p^{-1}(\operatorname{Spec}B_i \cap \operatorname{Spec}B_j)= \operatorname{Spec}(B_i \otimes_A k(y)) \cap \operatorname{Spec}(B_j \otimes_A k(y))$, where I fail to see that ""$\subseteq$"" holds. I appreciate any kind of help.","['schemes', 'general-topology', 'algebraic-geometry']"
1774844,Average prime value in n factorial.,"I was wondering about the (weighted) average prime value in the factorisation of $n!$. $\\$
 If we call $f(n)$ the average prime value in $n!$, then $f$ seems to increase rather linear. Is there a clear explanation for that? Anything known about asymptotic behavior? I know that the number of primes smaller then $x$ is approximately $x/\log(x)$ 
, but here im somewhat intrigued. A friend made a plot for me of $f(n)$. 
Check the following link: EDIT: Since my question wasnt clear to everyone: I meant the weighted average of primes. I mean explicitly the fraction $$\frac{\sum_{p\leq n}p \sum_{k\geq 1}{[n/p^k]}}{\sum_{p\leq n}\sum_{k\geq 1}[n/p^k]} $$ https://sagecell.sagemath.org/?q=smlmgo Thanks for any suggestions in advance.","['number-theory', 'combinatorics', 'analytic-number-theory']"
1774845,Evaluate the improper integral with residues.,"$$\int_0^{\infty} \frac{x^2+1}{x^4+1}dx$$ What i've found are the singularities at: $e^{\pi/4+\pi/2n}$ for $n=0,1,2,3$. But i'm unsure how to calculate the integral since I don't want to include the singularity at $x_2=\frac{-1+i}{\sqrt{2}}$ since then I would be calculating the integral from negative to positive infinity. My idea was to construct a contour $\gamma : x=ti$ where $R<t\leq0$ and $\lim_{R\rightarrow \infty}$. My reckoning was that since: $$\int_{\gamma}\frac{x^2+1}{x^4+1}dx=i\int_R^0\frac{1-t^2}{t^4+1}dt$$ doesn't have any singularities along the positive real numberline, it is therefore bounded and thus: $$i\int_R^0\frac{1-t^2}{t^4+1}dt\leq \Big| i\int_R^0\frac{1-t^2}{t^4+1}dt\Big|\leq\int_R^0\frac{|1-t^2|}{|t^4+1|}dt\leq\frac{|R^2|+1}{|R^4|-1}\rightarrow0 \text{ when } R\rightarrow \infty.$$ The answer would then be: 
$$\lim_{R\rightarrow \infty}\int_0^{R}\frac{x^2+1}{x^4+1}dx=2\pi iRes(f;e^{\pi/4})-\int_0^{\pi/2}f(x)dx-i\int_R^0f(x)dx=\frac{\pi}{2\sqrt{2}}.$$ But sadly I am mistaken, what is wrong?","['residue-calculus', 'complex-analysis', 'improper-integrals', 'complex-numbers']"
1774846,"Prove that $\lim_{x\to\infty}\frac{1}{x}\int_{0}^xf(t)\,dt=a$ if $f$ is continuous and $\lim_{x\to\infty}f(x)=a$",Suppose $f$ is continuous and $\displaystyle \lim_{x \to \infty} f(x) = a$. Prove that $$\displaystyle \lim_{x \to \infty}\dfrac{1}{x} \int_{0}^x f(t) dt = a.$$ I don't see why in the solution below they take the integral from $1$ to $N$ instead of $0$ to $N$ and how that proves the result. Also is $M$ considered to be positive? Book solution:,"['real-analysis', 'calculus']"
1774896,How to integrate |z| dz?,"As the title says, how do we integrate $|z|dz$ on a straight line on the complex plane? Suppose that I've already known the parametrization. If it were on reals, we would break the integral down to multiple parts where z changes sign, but how do I do that on the complex plane? Is it just simply the formula for absolute value (square root of the sum of the squares of real and imaginary parts)?","['complex-analysis', 'integration', 'complex-integration']"
1774980,Evaluation of $\cos\left(\frac{\pi}{2n}\right)\cdot \cos\left(\frac{2\pi}{2n} \right)....\cos\left(\frac{(n-1)\pi}{2n}\right)$,"Evaluation of $$\lim_{n\rightarrow \infty}\left(\tan \frac{\pi}{2n}\cdot \tan \frac{2\pi}{2n}\cdot \tan \frac{\pi}{3n}\cdot ...............\tan \frac{(n-1)\pi}{2n}\right)^{\frac{1}{n}} = $$ without using Limit as a sum. $\bf{My\; Try::}$ Using the formula $$\displaystyle \sin\left(\frac{\pi}{n}\right)\cdot \sin\left(\frac{2\pi}{n} \right)....\sin\left(\frac{(n-1)\pi}{n}\right) = \frac{n}{2^{n-1}}$$ Replace $n\rightarrow 2n$ $$\displaystyle \sin\left(\frac{\pi}{2n}\right)\cdot \sin\left(\frac{2\pi}{2n} \right)....\sin\left(\frac{(2n-1)\pi}{2n}\right) = \frac{2n}{2^{2n-1}}$$ Now How can I calculate $$\displaystyle \sin\left(\frac{\pi}{2n}\right)\cdot \sin\left(\frac{2\pi}{2n} \right)....\sin\left(\frac{(n-1)\pi}{2n}\right)$$ and also How can I calculate $$\displaystyle \cos\left(\frac{\pi}{2n}\right)\cdot \cos\left(\frac{2\pi}{2n} \right)....\cos\left(\frac{(n-1)\pi}{2n}\right)$$ Help required, Thanks","['integration', 'calculus', 'limits']"
1775002,"If $f: \mathbb{R}\setminus\mathbb{Q} \to \mathbb{R}_S\times\mathbb{R}_S$ is continuous, then the image has empty interior.","Please, don't write the entire answer. I am looking for hints only. Here $\mathbb{R}_S\times\mathbb{R}_S$ is the Sorgenfrey plane. My attempt so far was limited by Suppose it is continuous and it has a non-empty interior. Let $p$ be an interior point. Define $g(x)=f(x)-p$ so that $(0,0)$ belongs to its interior now. The diagonal of the Sorgenfrey plane has some properties, so I was thinking about using it. Suppose you could extend it to $\mathbb{R}$, so $\tilde{f}(\mathbb{R})$ would be connected. However, if $A$ is a basic open set inside the image, one can show that it is not connected and the question is finished. Could you help me? :) 
Remember, just a hint. Cheers.",['general-topology']
1775030,Bartoszyński's results on measure and category and their importance,"I have seen this interesting paragraph on a talk page of the Wikipedia article about Polish mathematician Tomek Bartoszyński : Tomek's paper ""Additivity of measure implies additivity of category, Trans. Amer. Math. Soc., 281 (1984), no. 1, 209--213"" as the first to show that there is (in ZFC) an asymmetry between measure and category. Before that essentially everybody believed that no such asymmetry exists (cf Oxtoby's book Measure and Category), and in my very personal opinion, Tomek's result shook the world and initiated the research that quickly led to Cichon's diagram, among others. This paragraph sounds quite intriguing. And it came from a user who definitely knows a lot about set theory, judging by their contributions on Wikipedia . 1 Would it be possible to outline some basic information of Bartoszyński's results on measure and category and their importance? (Ideally in a way readable by a non-expert in the field.) Adding some references suitable for somebody who wants to start to learn a bit more about this topic would be appreciated too. 1 This was a part of discussion about Bartoszyński notability in the sense in which this word is used on Wikipedia. The above comment was added by the Wikipedia with the username Kope , who is most probably Péter Komjáth , although I am not sure whether he officially confirmed this somewhere. He is also active on MathOverflow (and to a lesser extent on Math.SE).","['real-analysis', 'baire-category', 'measure-theory', 'set-theory', 'infinitary-combinatorics']"
1775041,Prove $(x_n)$ defined by $x_n= \frac{x_{n-1}}{2} + \frac{1}{x_{n-1}}$ converges when $x_0>1$,"$x_n= \dfrac{x_{n-1}}{2} + \dfrac{1}{x_{n-1}}$ I know it converges to $\sqrt2$ and I do not want the answer. I just want a prod in the right direction. I have tried the following and none have worked:  $x_n-x_{n-1}$ and this got me nowhere. I have tried $\dfrac{x_{n+1}}{x_{n}}$ with no luck, and I was expecting this  to help,","['real-analysis', 'fixed-point-iteration', 'convergence-divergence', 'numerical-methods', 'sequences-and-series']"
1775058,"Find all real numbers $a,b$ such that $|a|+|b|\geq\frac{2}{\sqrt{3}}$ and $|a\sin x+b\sin{2x}|\leq 1$ for all real $x$.","Find all real numbers $a,b$ such that $|a|+|b|\geqslant\frac{2}{\sqrt{3}}$ and $|a\sin(x)+b\sin(2x)|\leqslant 1$ for all real $x$. We could write the inequality as
$$
\left|\frac{a}{\sqrt{a^2+b^2}}\sin (x)+\frac{b}{\sqrt{a^2+b^2}}\sin(2x)\right|\leqslant \frac{1}{\sqrt{a^2+b^2}}
$$
and let
$$
\cos (y)=\frac{a}{\sqrt{a^2+b^2}}
$$
and
$$
\sin (y) = \frac{b}{\sqrt{a^2+b^2}}.
$$
The inequality is equivalent to
$$
\left|\cos (y)\cos\left(\frac{\pi}{4}-x\right)-\sin (y)\sin(2x)\right|\leqslant \frac{1}{\sqrt{a^2+b^2}},
$$
but the terms are different, so we cannot use the cosine addition formula.","['algebra-precalculus', 'inequality', 'trigonometry']"
1775129,Mean Value Theorem involving second derivative,"If $f:[a,b]\to\mathbb R$ such that $f'(x)>0,f''(x)>0$ for all $x\in[a,b]$ , then $$\int_a^b e^{f(x)}\,dx\le (b-a)\frac{e^{f(b)}-e^{f(a)}}{f(b)-f(a)}$$ My progress so far: We have $\frac{1}{b-a} \int\limits_{a}^{b} e^{f(x)} =e^{f(\epsilon)}$ for some $\epsilon \in (a,b)$ . Also, take the function $e^x$ int the RHS, and apply Lagrange MVT to get $e^c$ for some $c \in (f(a),f(b))$ SO, we are left to show that $e^{f(\epsilon)} \le e^c$ or $f(\epsilon) \le c$ . I am stuck here. I don't know how to use the fact that $f''(x)>0$ .","['real-analysis', 'calculus']"
1775140,How to prove this trigonometric integral?,"$$ \displaystyle \int_{-\pi/4}^{\pi/4} {{\left(\dfrac{\cos x - \sin x}{\cos x + \sin x}\right)}^{\cos(2t)} \ dx} = \frac{\pi}{2 \sin(\pi \cos^2 t)}$$ I could simplify it to $\displaystyle \int_0^1 {\left(t^n + \frac{1}{t^n}\right) \ \frac{dt}{1+t^2}}, \ n = \cos 2t $ From here, I can think of expanding into sums but that doesn't seem a good option. Also, getting back to trigonometric form is also an option but it would get us to reduction formula which will be messy. What is a straight, neat and easy approach to solve it?","['definite-integrals', 'integration', 'trigonometry']"
1775145,Diophantine $4x^3-y^2=3$,"I am interested in how to tackle this Diophantine equation: $$4x^3-y^2=3$$ The solutions I have found so far are $(1,1)$ and $(7,37)$. Are there any more? I have looked up various material on cubic Diophantines but most of what I’ve found is on equations where the coefficients of $x^3$ and $y^2$ are the same. In this particular problem, if both coefficients were equal to $1$, it would just be a nice Mordell’s equation. But the coefficient of the cubic variable is not $1$ – which is why it’s so frustrating. Still, would I be right in saying that if the solutions were to lie on an elliptic curve, there would only be finitely many of them? What if they don’t lie on an elliptic curve? Will the number of solutions still be finite?","['diophantine-equations', 'mordell-curves', 'number-theory', 'cubics', 'elementary-number-theory']"
1775152,For which values of $p$ the series $\sum_{n = 2}^{\infty}\frac{1}{\ln^p{n}}$ converges?,"I'm trying to find all values of $p$ for which the following series converges: $$\sum_{n = 2}^{\infty}\frac{1}{\ln^p{n}}$$ So my first approach was to use the integral test because $\frac{1}{\ln^p{n}}$ is monotone decreasing, continuous and non-negative, but I don't know how to calculate such an integral... In my second approach I found that in the interval $[0, 1]$, we have $\frac{1}{n} \leq \frac{1}{\ln^p n}$ and I know that $\frac{1}{n}$ diverges and also non-negative so for $p \in [0, 1]$ the series diverges. But that's not enough... How should I solve this?","['real-analysis', 'calculus', 'summation', 'convergence-divergence', 'analysis']"
1775233,The canonical base point for Weil algebras,"Kock defines, after (16.2), the canonical base point of a small object $\operatorname{Spec}_R(W)$ to be $$\mathbf 1\overset{\operatorname{Spec}_R (\pi)}{\longrightarrow}\operatorname{Spec}_RW$$where $\pi:W\rightarrow R$ is the augmentation map of the Weil algebra $W$, and the $\mathbf 1$ is just $\operatorname{Spec}_RR$. Thing is, I don't understand what $\operatorname{Spec}_R$ does to arrows as a functor, since there are no prime ideals to take inverse images of. So, what does $\operatorname{Spec}_R$ do as a functor, and how does it follow that in the case of the dual numbers, it chooses zero?","['category-theory', 'synthetic-differential-geometry', 'algebraic-geometry']"
1775240,Problem with a proof on Conway's book,"Now assume that $C_b(X)$ is separable. Thus $({\rm ball}\,C_b(X)^*,{\rm wk}^*)$ is metrizable (5.1). Since $X$ is homeomorphic to a subset of ${\rm ball}\,C_b(X)^*$ (6.1), $X$ is metrizable. It also follows that $\beta X$ is metrizable. It must be shown that $X=\beta X$ . Suppose there is a $\tau$ in $\beta  X\setminus X$ . Let $\{x_n\}$ be a sequence in $X$ such that $x_n\to\tau$ . It can be assumed that $x_n\neq x_m$ for $n\neq m$ . Let $A=\{x_n:n\,{\rm is\, even}\}$ and $B=\{x_n:n\,{\rm is\, odd}\}$ . Then $A$ and $B$ are disjoint closed subsets of $X$ (not closed in $\beta X$ , but in $X$ ) since $A$ and $B$ contain all of their limit points in $X$ . Since $X$ is normal, there is a continuous function $f:X\to[0,1]$ such that $f=0$ on $A$ and $f=1$ on $B$ . But then $f^\beta(\tau)=\lim f(x_{2n})=0$ and $f^\beta(\tau)=\lim f(x_{2n+1})=1$ , a contradiction. Thus $\beta X\setminus X=\varnothing$ . $\hspace{10cm}\blacksquare$ The quote above comes from the end of p. 140 on Conway's A course in functional analysis . The assumption is only that $X$ is completely regular, plus the one in the first sentence of course. My problem is: how does he deduce $\beta X$ is metrizable? He indicates no reference for this fact, and I can't seem to be able to do it myself. I thought of the completion theorem, so if I prove the completion of the metric space $X$ is compact then it is $\beta X$ by uniqueness and $\beta X$ is therefore metrizable. Indeed, if the completion $X'$ is compact, supposing $f:X\to\mathbb{R}$ is continuous, let $y\in X'\smallsetminus X$ , then we have $x_n\to y$ a sequence in $X$ , and by continuity I can prove $f(x_n)$ converges for any such sequence to a unique limit which I call $f(y)$ , and this extends $f$ to $X'$ , right? So $X'$ has the extension property for continuous functions which characterizes $\beta X$ , hence $X'=\beta X$ . But is it true that $X'$ is compact? And how can I prove it? And if otherwise, why is $\beta X$ metrizable? Update OK the above argument works for continuous functions to complete metric spaces, but not any compact Hausdorff space, so I'd have to chuck it away since it doesn't get to $\beta X$ . I was thinking maybe the weak-* closure of $X$ in $C_b(X)^\ast$ is a candidate? But again, how do I extend functions to prove that thing is $\beta X$ ? Clarification The last question in the update seems a bit cryptic, so let me expand. $\beta X$ is characterized by being compact, having $X$ as a dense subspace, and being such that for any $f:X\to K$ , with $K$ ha compact hausdorff space, there exists $\tilde f:\beta X\to K$ an extension of $f$ to a continuous map. So if we take the weak-* closure of $X$ in $C_b(X)^\ast$ , it is compact because the ball by Banach-Alaoglu is and that closure is closed in the ball, it has $X$ as a dense subspace by definition, but what about that extension property?",['functional-analysis']
1775268,Change of Variables in a Second Order Linear Homogeneous Differential Equation,"Consider the differential equation
$$\frac{d}{dx} \left( x \frac{dy(x)}{dx}\right) + \frac{\lambda}{x} y(x) = 0$$
This a Sturm-Liouville problem where $\lambda \in \mathbb{R}$ corresponds to the (eventual) eigenvalues of the SL operator. To solve the differential equation we preform the following change of variables
$$v(x) = \ln (x) \implies \frac{dy(x)}{dx} = \frac{dv(x)}{dx} \frac{dy(v)}{dv} = \frac{1}{x} \frac{dy(v)}{dv}$$
Plugging in and multiplying through by $x \not = 0$ we find that
$$ \frac{d^2 y(v)}{dv^2} + \lambda y(x) = 0$$
The change of variables in the case of the derivative happens naturally in the definition of the derivative and derivation of the chain rule, but how do the variables change in the case of $y(x)$? As an example (not related to the exercise at hand) if $y(x) = \chi_{[0,1]}$ (the indicator on the unit interval) then clearly 
$$ y(e) = 0, \ y(v(e)) = y(1) = 1$$
So we cannot have $y(x) = y(v(x))$. However in a suitable domain we could have
$$ y(x) = y(\ln(e^x)) = y(v(e^x))$$
At this point I am convinced that there is some fundamental error in my understanding of the change of variables method.",['ordinary-differential-equations']
1775325,Finding limit without using limit,If we have to find the value of $$ \lim_{x \to 0} \frac{e^x-1}{x}$$ I tried to solve this by using series i.e by expanding $e^x$ and got the result. But if there is another method to solve this,"['exponential-function', 'sequences-and-series', 'functions', 'limits']"
1775353,Calculate limit with integral,"Hi I have a problem with following limit:
$$\lim_{x\rightarrow\infty}e^{-x}\int_{0}^{x}\int_{0}^{x}\frac{e^u-e^v}
{u-v}\ \mathrm du\ \mathrm dv$$ as a hint i got that i should use de l'Hospital. So:
$$\lim_{x\rightarrow\infty}\frac{\int_{0}^{x}\int_{0}^{x}\frac{e^u-e^v}
{u-v}\ \mathrm du\ \mathrm dv}{e^{x}}$$ And now we want to calculate derivative of up and down. But now i am not sure how to calculate derivative from $$\frac{\mathrm d}{\mathrm dx}\left( \int_{0}^{x}\int_{0}^{x}\frac{e^u-e^v}
{u-v}\ \mathrm du\ \mathrm dv\right)$$
I will be very glad for help","['multivariable-calculus', 'lebesgue-integral', 'calculus', 'limits']"
1775382,Question involving functions and permutations,"If $A=\{1,2,3,4\},B=\{a,b,c\}$, how many functions $A\to B$ are not onto? My Try: so one element in $B$ shouldn't have a preimage in A so one element is excluded(for convenience) so for $4$ elements in $A$ there are $2$ in B hence total ways are $16$ then $2$ elements in $B$ don't have a pre image so $2^3$ ways. Thus total ways are $16+8=24$ . So this can be done for all $3$ elements. Hence total ways are $72$ but answer given is $45$","['combinations', 'functions']"
1775403,Is every complete metric space closed?,"I know that if $A\subset X$ where $X$ is a complede metric space, and $A$ is closed $\iff$ it's complete. However is every metric space closed? E.g., can I take $X\subset X$ and since $X$ is complete, can I conclude it's closed?","['real-analysis', 'metric-spaces']"
1775469,Finding the Exponential of a Matrix that is not Diagonalizable,"Consider the $3 \times 3$ matrix $$A =
\begin{pmatrix} 
1 & 1 & 2 \\ 
0 & 1 & -4 \\ 
0 & 0 & 1  
\end{pmatrix}.$$ I am trying to find $e^{At}$. The only tool I have to find the exponential of a matrix is to diagonalize it. $A$'s eigenvalue is 1. Therefore, $A$ is not diagonalizable. How does one find the exponential of a non-diagonalizable matrix? My attempt: Write
$\begin{pmatrix} 
1 & 1 & 2 \\ 
0 & 1 & -4 \\ 
0 & 0 & 1  
\end{pmatrix} = M  + N$,
with $M = \begin{pmatrix} 
1 & 0 & 0 \\ 
0 & 1 & 0 \\ 
0 & 0 & 1  
\end{pmatrix}$ and $N = \begin{pmatrix} 
0 & 1 & 2 \\ 
0 & 0 & -4 \\ 
0 & 0 & 0  
\end{pmatrix}$. We have $N^3 = 0$, and therefore $\forall x > 3$, $N^x = 0$.  Thus: $$\begin{aligned}
e^{At}
&= e^{(M+N)t} = e^{Mt} e^{Nt} \\
&= \begin{pmatrix} 
e^t & 0 & 0 \\ 
0 & e^t & 0 \\ 
0 & 0 & e^t  
\end{pmatrix} \left(I + \begin{pmatrix} 
0 & t & 2t \\ 
0 & 0 & -4t \\ 
0 & 0 & 0  
\end{pmatrix}+\begin{pmatrix} 
0 & 0 & -2t^2 \\ 
0 & 0 & 0 \\ 
0 & 0 & 0  
\end{pmatrix}\right) \\
&= e^t \begin{pmatrix} 
1 & t & 2t \\ 
0 & 1 & -4t \\ 
0 & 0 & 1  
\end{pmatrix} \\
&= \begin{pmatrix} 
e^t & te^t & 2t(1-t)e^t \\ 
0 & e^t & -4te^t \\ 
0 & 0 & e^t  
\end{pmatrix}.
\end{aligned}$$ Is that the right answer?","['matrices', 'linear-algebra', 'analysis']"
1775471,a continued fraction related to pythagoras theorem $a^2+b^2=c^2$,"For our purpose,let $a,b,c$ and $x\gt2$ be natural numbers such that the positive integers $a,b$ and $c$ form a special pythagorean triple $(a,b,c)$,then it is conjectured that the following is true $$G(x)=\cfrac{2}{3x+\cfrac{(-1)(7)} {9x+\cfrac{(2)(10)}{15x+\cfrac{(5)(13)}{21x+\cfrac{(8)(16)}{27x+\ddots}}}}}$$ $$G(x)=\frac{a}{2b}+\frac{c}{b}\left[
{}_2F_1\left( -\frac13, \frac13 ; \frac12; \frac{c+a}{2c}\right)
-   {}_2F_1\left( -\frac13, \frac13 ; \frac12; \frac{c-a}{2c}\right)
\right]$$ where, $$a=4x(x^2-1)\\b = x^4-6x^2+1\\c=(x^2+1)^2$$ such that, $$a^2+b^2=c^2=(x^2+1)^4$$ Corollaries Here are some of its closed forms with their related not necessarily primitive pythagorean triples next to them $$G(3)=\frac{12}{7}+\frac{25}{7}\left[
{}_2F_1\left( -\frac13, \frac13 ; \frac12; \frac{49}{50}\right)
-   {}_2F_1\left( -\frac13, \frac13 ; \frac12; \frac{1}{50}\right)
\right] ; (7,24,25)$$ $$G(4)=\frac{120}{161}+\frac{289}{161}\left[
{}_2F_1\left( -\frac13, \frac13 ; \frac12; \frac{529}{578}\right)
-   {}_2F_1\left( -\frac13, \frac13 ; \frac12; \frac{49}{578}\right)
\right] ;(161,240,289)$$ $$G(5)=\frac{60}{119}+\frac{169}{119}\left[
{}_2F_1\left( -\frac13, \frac13 ; \frac12; \frac{289}{338}\right)
-   {}_2F_1\left( -\frac13, \frac13 ; \frac12; \frac{49}{289}\right)
\right] ;(119,120,169)$$ $$G(6)=\frac{420}{1081}+\frac{1369}{1081}\left[
{}_2F_1\left( -\frac13, \frac13 ; \frac12; \frac{2209}{2738}\right)
-   {}_2F_1\left( -\frac13, \frac13 ; \frac12; \frac{529}{2738}\right)
\right] ;(840,1081,1369)$$ $$G(7)=\frac{168}{527}+\frac{625}{527}\left[
{}_2F_1\left( -\frac13, \frac13 ; \frac12; \frac{961}{1250}\right)
-   {}_2F_1\left( -\frac13, \frac13 ; \frac12; \frac{289}{1250}\right)
\right] ;(336,527,625)$$ $$G(8)=\frac{1008}{3713}+\frac{4225}{3713}\left[
{}_2F_1\left( -\frac13, \frac13 ; \frac12; \frac{6241}{8450}\right)
-   {}_2F_1\left( -\frac13, \frac13 ; \frac12; \frac{2209}{8450}\right)
\right] ;(2016,3713,4225)$$ $$G(9)=\frac{360}{1519}+\frac{1681}{1519}\left[
{}_2F_1\left( -\frac13, \frac13 ; \frac12; \frac{2401}{3362}\right)
-   {}_2F_1\left( -\frac13, \frac13 ; \frac12; \frac{961}{3362}\right)
\right] ;(720,1519,1681)$$ $$G(10)=\frac{1980}{9401}+\frac{10201}{9401}\left[
{}_2F_1\left( -\frac13, \frac13 ; \frac12; \frac{14161}{20402}\right)
-   {}_2F_1\left( -\frac13, \frac13 ; \frac12; \frac{6241}{20402}\right)
\right] ;(3960,9401,10201)$$ $$G(11)=\frac{660}{3479}+\frac{3721}{3479}\left[
{}_2F_1\left( -\frac13, \frac13 ; \frac12; \frac{5041}{7442}\right)
-   {}_2F_1\left( -\frac13, \frac13 ; \frac12; \frac{2401}{7442}\right)
\right] ;(1320,3479,3721)$$ $$G(12)=\frac{3432}{19873}+\frac{21025}{19873}\left[
{}_2F_1\left( -\frac13, \frac13 ; \frac12; \frac{27889}{42050}\right)
-   {}_2F_1\left( -\frac13, \frac13 ; \frac12; \frac{14161}{42050}\right)
\right] ;(6864,19873,21025)$$ and so on for all values of $x\gt2$... Question :Can the conjecture be rigorously proved,so that the connection between the continued fraction and the primitive pythagorean triples can be established?","['number-theory', 'hypergeometric-function', 'continued-fractions', 'pythagorean-triples']"
1775483,Application of Hölder's inequality on $\mathbb R^n$,"Let $1 \leq p < q \leq \infty$ ($p$ and $q$ are not otherwise related). Given $\|x\|_q\leq\|x\|_p $ $\forall$ $ x \in \mathbb R^n$ how can I use Hölder's inequality to show $\|x\|_p\leq n^{\frac{1}{p}-\frac{1}{q}}\|x\|_q$ . I can see this link is a related topic but I could not recognize Hölder's inequality usage in there. Am I missing anything? I am assuming I need to say $\|x\|_p \leq \|x\|_1\leq...$ and here I need to find functions $f$ $g$ such that $\|x\|_1 \leq \|fg\|_1$. Is there a general ""rule of thumb"" for picking functions on these cases?",['functional-analysis']
1775532,Probability: 10th ball is blue,"The following is a question I've made myself, but I need help in solving it: Suppose there are 100 balls in a box. 20 balls are blue, 30 balls are green and 50 balls are yellow. Now we randomly pick out 10 balls out of the box (one ball after the other) and we don't put the balls back in the box. What's the probability of the 10th ball being picked having color blue? I tried thinking of all the possibilities of the the 10th ball being blue, divided by all of the possible combinations. I tried for several hours and couldn't figure out neither of them. Help is appreciated!","['combinatorics', 'probability']"
1775571,"the ""real spectrum"" of an operator acting on a real Banach space","Let $X$ be a Banach space over the field $\mathbb{R}$, and denote by $\mathcal{L}(X)$ the space of continuous linear operators acting on $X$.  The spectrum $\sigma(T)$ of an operator $T\in\mathcal{L}(X)$ is defined as the set of all $\lambda\in\mathbb{C}$ for which the complexification shift $\lambda-T_\mathbb{C}$ is not invertible in $\mathcal{L}(X_\mathbb{C})$.  (See here for the definition of a complexification.) However, I am interested in invertibility in the real setting. The real spectrum of $T$, denoted $\sigma_\mathbb{R}(T)$, is the set of all $a\in\mathbb{R}$ such that $a-T$ does not have an inverse in $\mathcal{L}(X)$.  It is the complement of the real resolvent , defined as
\begin{equation}\rho_\mathbb{R}(T):=\left\{a\in\mathbb{R}:\text{ there is }\;S\in\mathcal{L}(X)\;\text{ such that }\;S(a-T)=(a-T)S=I_X\right\},\end{equation}
where $I_X\in\mathcal{L}(X)$ is the identity operator ($I_Xx=x\;\forall x\in X$). Question 1. I suspect (although I haven't verified it yet) that $\sigma_\mathbb{R}(T)=\mathbb{R}\cap\sigma(T)$. Is there a reference for this? Question 2. Suppose $r\in\partial\sigma_\mathbb{R}(T)$ and $(a_n)_{n=1}^\infty\subseteq\rho_\mathbb{R}(T)$ with $a_n\to r$.  Is it true that $\|(a_n-T)^{-1}\|\to\infty$? I would also be interested in any good literature on the real spectrum, if it is available. Thank you!","['functional-analysis', 'banach-spaces', 'spectral-theory']"
1775572,Olympiad Inequality $\sum\limits_{cyc} \frac{x^4}{8x^3+5y^3} \geqslant \frac{x+y+z}{13}$,"$x,y,z >0$ , prove $$\frac{x^4}{8x^3+5y^3}+\frac{y^4}{8y^3+5z^3}+\frac{z^4}{8z^3+5x^3} \geqslant \frac{x+y+z}{13}$$ Note:
Often Stack Exchange asked to show some work before answering the question. This inequality was used as a proposal problem for National TST of an Asian country a few years back. However, upon receiving the official solution, the committee decided to drop this problem immediately. They don't believe that any students can solve this problem in 3 hour time frame. Update 1: In this forum, somebody said  that BW is the only solution for this problem, which to the best of my knowledge is wrong. This problem is listed as ""coffin problems"" in my country. The official solution is very elementary and elegant. Update 2: Although there are some solutions (or partial solution) based on numerical method, I am more interested in the approach with ""pencil and papers."" I think the approach by Peter Scholze in here may help. Update 3: Michael has tried to apply Peter Scholze's method but not found the solution yet. Update 4: Symbolic expanding with computer is employed and verify the inequality. However, detail solution that not involved computer has not been found. Whoever can solve this inequality using high school math knowledge will be considered as the ""King of Inequality"".","['algebra-precalculus', 'contest-math', 'inequality', 'cauchy-schwarz-inequality']"
1775593,Prove that if $2n+1$ and $3n+1$ are both perfect squares then $40|n$.,"Prove that if $2n+1$ and $3n+1$ are both perfect squares then $40|n$. First, I took
$$2n+1 \equiv x^2 \equiv 0, 1 \pmod 4$$
which showed that $n$ was even. Now, 
$$3n + 1 \equiv y^2 \equiv 0, 1, 4 \pmod 8$$ But since $n$ is even, we get that $8|n$. So, now any square $\equiv 0, 1, 4, 5, 6, 9 \pmod{10}$. So, I tested $2n+1$ and $3n+1$ for all numbers from 0 to 9. For only two, 0 and 5, were both of them ending with the residues above mentioned. So, I finally proved that $5|n$. So, $40|n$. Is my proof correct? Also, my proof is too roundabout and lengthy. I had to write many programs to take different modulos. Can anyone suggest a more elegant proof especially for the second part when I have to show that $5|n$? Thanks.","['number-theory', 'alternative-proof', 'proof-verification', 'elementary-number-theory']"
1775594,Proof of the Addition and Scalar multiplication for linear maps,"Let $f, g : U\rightarrow V$ be linear maps and $\lambda\in  F$ . Then the maps $f + g : U\rightarrow V$ and $\lambda f : U \rightarrow V$ are linear. My attempt at the proof for the first statement is as follows: Let $u,z\in U$ and $a\in F$ , using a linearity check by deﬁnition of $f + g$ $$(f + g)(au + z) = f(au + z) + g(au + z)$$ by linearity of $f$ and $g$ $$= (af(u) + f(z)) + (ag(u) + g(z)) $$ by basic properties of vector spaces $$= af(u) + ag(u) + f(z) + g(z)$$ by an axiom of vector spaces $$= a(f(u) + g(u)) + (f(z) + g(z))$$ by deﬁnition of $f + g$ . $$= a(f + g)(u) + (f + g)(z)$$ Hence, $f + g$ is linear Is this the correct approach. What is the proof of $\lambda f : U \rightarrow V$ to be linear?",['linear-algebra']
1775619,Let $A$ be a finite set and $\mathcal R$ an equivalence relation in A. Prove that exists a function ...,"Let $A$ be a finite set and $\mathcal R$ an equivalence relation in A. Prove that exists a function $f: A \to \Bbb N$ such that $\forall a, b \in A$ the following is true: $$a \mathcal R b \iff f(a) = f(b)$$ I've already proved that $f(a) = f(b)$ is an equivalence relation. $\mathcal R$ is a subset of the cartesian product $A \times A$, and I know that for $f$ to be a function each element $a \in A$ has to have one and only one image. If any two elements in the domain have the same image I could think of a constant function, that maps every element to the same $n \in \Bbb N$ ... but if this is right, I still don't know how to prove it. So, is ok to think the problem like this? If it is, how would you prove it? I would appreciate your help or tips, thanks!","['functions', 'proof-writing', 'equivalence-relations', 'elementary-set-theory', 'relations']"
1775624,Finding Conjugate harmonic of $u = \frac{1}{2} ln(x^2 + y^2)$,"this is a nice community; I've been facing a hard time answering this question, a detailed answer would be splendid. $u = \frac{1}{2} ln(x^2 + y^2)$
find conjugate harmonic, and harmonic function. Thanks very much Edit: this is my solution so far
found ∂u∂x, it's square, and ∂u∂y with its square tried using this equation (F(z)= integration(∂u∂x(z,0)- j∂u∂y(z,0))dz + jc)
....but couldn't get a sensible result of (v) my results was F(z)=ln(z) +jc = ln(x+jy) + jc ..... as you can see this is not a satisfying solution","['calculus', 'functions']"
1775635,If $g(x) = f(-x)$ then $g'(x) = -f'(-x)$,"I am doing two exercises using Derivatives. Prove that if $f$ is even , then $f'(x) = -f(-x)$ Prove that if $f$ is odd, then $f'(x) = f'(-x)$. Now, I found the answer for the exercises, but there is a statement : if $g(x) = f(-x)$ then $g'(x) = -f'(-x)$. How can I prove that statement using the Derrivate's Definition. I tried it, but everytime I get $g'(x) = -f'(x)$ instead of $g'(x) = -f'(-x)$. Thanks","['derivatives', 'calculus']"
1775651,Find the derivative of $F(x) = \int_{a}^b \frac{x}{1+t^2+\sin^2{t}}dt$,Find the derivative of $$F(x) = \int_{a}^b \dfrac{x}{1+t^2+\sin^2{t}}dt.$$ Attempt: We use the product rule since $\displaystyle \int_{a}^b \dfrac{x}{1+t^2+\sin^2{t}}dt = x  \int_{a}^b \dfrac{1}{1+t^2+\sin^2{t}}dt$ to get that $$F'(x) = \int_{a}^b \dfrac{1}{1+t^2+\sin^2{t}}dt.$$,"['derivatives', 'calculus']"
1775692,Find the value of : $\lim_{n \rightarrow \infty} \prod_{k=1}^n \left(1+\ln\left(\frac{k+\sqrt{k^2+n^2}}{n}\right)^{\frac{1}{n}}\right)$,"Compute $\displaystyle \lim_{n \rightarrow \infty} \prod_{k=1}^n \left(1+\ln\left(\frac{k+\sqrt{k^2+n^2}}{n}\right)^{\frac{1}{n}}\right)$ Note that $\frac{k+\sqrt{k^2+n^2}}{n}\geq 1$ so we're dealing with positive terms. Taking $\log$ of the product, we're interested in the limit of $\displaystyle \sum_{k=1}^n \ln\left(  1+ \exp \left(\frac 1n\ln\ln \left( \frac kn + \sqrt{\left( \frac{k}{n} \right) ^2 +1}\right)\right) \right)$ which look very much like a Riemann sum. Setting $f(x) =\ln\ln(x+\sqrt{x^2+1})$, the sum rewrite as $\displaystyle \sum_{k=1}^n \ln\left(  1+ \exp \left(\frac 1nf \left(\frac kn\right)\right) \right)$ I've been trying to use the usual bounds on $\ln$ and $\exp$ to exhibit a Riemann sum, but it's quite messy.","['sequences-and-series', 'limits']"
1775719,How to solve this algorithmic math olympiad problem?,"So, today we had a local contest in my state to find eligible people for the international math olympiad ""IMO"" ... I was stuck with this very interesting algorithmic problem: Let $n$ be a natural number ≥ 2, we take the biggest divisor of $n$ but it must be different from $n$ itself, subtract it from $n$. We repeat this until we get $1$. Example : Let $n = 30$. Then we have to subtract its biggest different divisor, that is, 15. So 30 - 15 = 15, now we do the same: 5 is the biggest divisor for 15, so 15 - 5 = 10 5 is the biggest divisor for 10, so 10 - 5 = 5 1 is the greatest divisor for 5, so 5 - 1 = 4 2 is the biggest divisor for 4 so 4 - 2 = 2 1 is the biggest divisor for 2, so 2 - 1 = 1 . And we're done ! it took 6 steps to get 1. If  $n = 2016^{155}$ how many steps we have to get 1 at the end ? I'm a programmer, and I used to rock with logical puzzles, but this time I'm completely lost. So please help me.","['algorithms', 'divisibility', 'sequences-and-series', 'elementary-number-theory']"
1775747,$f$ is holomorphic in Ω such that $|f|^2$ is harmonic; we need to show that $f$ is constant.,"$f$ is holomorphic in Ω such that $|f|^2$ is harmonic; we need to show that $f$ is constant. solution of the question In the solution attached, I don't really understand the transition between $∆|f(z)|^2 = 4|f'_z(z)|^2$.
It would be great someone could answer this.","['complex-analysis', 'holomorphic-functions', 'harmonic-functions']"
1775749,"Right triangle on an ellipse, find the area","Beginning note: Please wait until the animations load. The loading might take some time depending on your internet connection. Secondly, the title and the content of the question might not be well understood. Any edit is welcome. Suppose that I have a right triangle $\triangle ABC$ where the median of $BC$ is located on the origin $O$. The triangle has the following properties: $\angle ABC = 90^{\circ}$, $B=(r\cos(\alpha), t\sin(\alpha))$, $A=(-\sqrt{r^2\cos^2(\alpha) + t^2\sin^2(\alpha)},0)$, $C=(\sqrt{r^2\cos^2(\alpha) + t^2\sin^2(\alpha)},0)$, where $\alpha = \angle BOC$ and $r,t$ some arbitrary real numbers. Now I am going to let Geogebra draw trace of the segments $\overline{AB}$ and $\overline{BC}$ while $\alpha$ changes from $0^\circ$ to $180^\circ$ for the following cases: In this case $r=t=1$. Point $B$ changes location whereas the points $A$ and $C$ stay the same because $$\sqrt{r^2\cos^2(\alpha) + t^2\sin^2(\alpha)}=\sqrt{r^2\cos^2(\alpha) + r^2\sin^2(\alpha)}=r$$ It can easily be verified that by interpolation the traces lead to a semi-circle of an area $\pi r^2/2$. In this case $r=1$, $t=1.5$; as you can verify by the equations, the points $A,B,C, O$ change location. Particularly the point $B$ moves on an ellipse. Finally, let's come to the problem: I tried but couldn't calculate the area beneath the traces like the first case for the second case. How can I formulate the area? Any help is appreciated.","['conic-sections', 'triangles', 'geometry']"
1775764,A counterexample of Riemann mapping theorem in high dimension,"There is an exercise (1.1.16) in Huybrechts: the polidisc $B_{(1,1)}(0)\subset\mathbb C^2$ and the unit disc $D$ in $\mathbb C^2$ can not be biholomorphic. The hint is to compare the automorphisms of these two sets. The unitary matrices is a subgroup of the group of biholomorphic maps of $D$ which leaves the origin fixed. Clearly, the group of unitary matrices of $\dim 2$ is not abelian and the group of biholomorphic maps of $B_{(1,1)}(0)$ is transitive. If we can show the group of biholomorphic maps of $B_{(1,1)}(0)$ which leave invariant the origin is abelian, then this will complete the proof. If we set $f_1(z_1,z_2)=(z_2,z_1)$ and $f_2(z_1,z_2)=(e^{i\pi/3}z_1,e^{i2\pi/3}z_2)$, then these are clearly in biholomorphic maps of $B_{(1,1)}(0)$. However, $f_1\circ f_2\neq f_2\circ f_1$. Please tell me where is the problem. Any hint is helpful. Thx.","['complex-analysis', 'several-complex-variables', 'complex-geometry']"
1775767,Conditional Extremes/Lagrange multipliers: proving: $\frac{1}{x_1}+...+\frac{1}{x_n} \geq \frac{n^2}{x_1+...+x_n}$,"$$\frac{1}{x_1}+...+\frac{1}{x_n} \geq \frac{n^2}{x_1+...+x_n};x_i>0.$$ This is supposed to be proven using conditional extremes. I tried the main function being $f(x_1,...,x_n)=\frac{1}{x_1}+...+\frac{1}{x_n} $and the condition:$x_1+...+x_n=s$ but when setting up the Lagrange equation I cannot get $x_i$ to be dependant upon $s$ and $n$ for reasons which are not clear. Help is appreciated.","['multivariable-calculus', 'lagrange-multiplier', 'calculus', 'analysis']"
1775773,Proof verification : a very useful theorem (in measure theory),"Let  $\bigcup_{n=1}^\infty E_n=E$ and $ E_{n}  \subseteq  E_{n+1} $ then $\lim\limits_{n\mapsto \infty} \mu^*(E_n) = \mu^*(E) $ even if each $E_n$ is a non-measurable set, where $\mu^*$ is outer measure. $E$ is a bounded set .
Please could you verify the proof given here? The theorem is true for measurable sets. Proof for the general case: There is a subsequence { $ E_k $} such that $\mu^* (E_{k+1} ) - \mu^* (E_k ) \le \frac {\epsilon}{2^{k+1}} $ Lets first construct such subsequence : $ \lim\limits_{n\mapsto \infty}\mu^*(E_n) \ge \mu^*(E_{n+1}) \ge \mu^*(E_n)$ Choose $E_1$ such that $ \lim\limits_{n\mapsto \infty}\mu^*(E_n) -\mu^*(E_1) \le \frac {\epsilon}{2} $ Choose $E_2$ such that $E_1 \subseteq E_2$ and  $ \lim\limits_{n\mapsto \infty}\mu^*(E_n) -\mu^*(E_2) \le \frac {\epsilon}{2^3}  $ Choose $E_3$ such that $E_2 \subseteq E_3$ and  $ \lim\limits_{n\mapsto \infty}\mu^*(E_n) -\mu^*(E_3) \le \frac {\epsilon}{2^4}  $ 
Then use induction Step 1: cover $E_k$ with union of open intervals $\bigcup_{i=1}^\infty I_i = L_k $
Such that $ \mu^* (L_k) \le \mu^* (E_k) +  \frac {\epsilon}{2^k} $ By Caratheodory condition $ \mu^* (E_{k+1} \bigcap L_k^c ) = \mu^* (E_{k+1} ) - \mu^* (E_{k+1} \bigcap L_k ) $
$\mu^*(E_k) \le  \mu^*(E_{k+1} \bigcap L_k ) \le \mu^*(L_k) $ Therefore  $ \mu^*(E_{k+1} \bigcap L_k^c ) \le \mu^*(E_{k+1}) - \mu^* (E_k) \le \frac {\epsilon}{2^{k+1}}$ Step 2 : Let $ G_{k+1} = E_{k+1} \bigcap L_k^c $ $ L_k \bigcup G_{k+1}$ contains $E_{k+1}$ Now cover  $ L_k \bigcup G_{k+1}$ with union of intervals $\bigcup_{i=1}^\infty I_i = H_{k+1} $
Such that $\mu^* (H_{k+1}) \le \mu^*( L_k \bigcup G_{k+1}) + \frac {\epsilon}{2^{k+1}} $ $\mu^* (H_{k+1}) \le \mu^*( L_k) + \mu^*( G_{k+1})  \le \mu^*(E_k) +  \frac { \epsilon}{2^k} +\frac { 2\epsilon}{2^{k+1} }$ Now using $H_{k+1} $ as the cover for $ E_{k+1} $ As seen  $\mu^*(E_{k+1})  \le \mu^*(H_{k+1})$  $\le \mu^*(E_{k+1})$ $+  \frac {\epsilon}{2^k}$ $+\frac {2\epsilon}{2^{k+1}}  $ Now apply step 1 and 2 to $ E_{k+1}$  and $ E_{k+2}$ and get : $  \mu^*(E_{k+2})  \le \mu^* (H_{k+2})  \le \mu^*(E_{k+2})  +  \frac { \epsilon}{2^k} +\frac { 2\epsilon}{2^{k+1} } +\frac { 2\epsilon}{2^{k+2} }$ It is obvious $ E \subseteq \bigcup_{i=1}^\infty H_k   $ $H_k \subseteq H_{k+1}$ $  \mu^*(E_{k})  \le \mu^* (H_{k})  \le \mu^*(E_{k}) + 4 \epsilon $ Notice that the theorem is valid for $H_k$ as it is a measurable set (union of intervals)
So $\lim\limits_{k\mapsto \infty} \mu^*(H_k) \ge  \mu^*(E) $ $\lim\limits_{k\mapsto \infty}\mu^*(E_{k})  \le  \lim\limits_{k\mapsto \infty} \mu^*(H_k)    \le \lim\limits_{k\mapsto \infty}\mu^*(E_{k})  + 4\epsilon $ $\lim\limits_{k\mapsto \infty}\mu^*(E_{k}) \le \mu^*(E)     \le \lim\limits_{k\mapsto \infty}\mu^*(E_{k})  + 4\epsilon $ Because $\epsilon $ is arbitrary the proof is complete. Remark: The proof is straight forward for measurable sets but not so for arbitrary sets. Actually it is true and it was given as an exercise in 'The Integrals of Lebesgue, Denjoy , Perron , and Henstock (Graduate Studies in Mathematics Volume 4 )' by Russell A. Gordon . It is theorem 1.15 in the book. This theorem allows the short proofs of Dominated convergence theorem, Vitali Convergence Theorem, Monotone Convergence Theorem , Egorov's theorem and Luzin's theorem without dwelling much on the machinery of measure theory.","['alternative-proof', 'lebesgue-measure', 'measure-theory', 'proof-verification']"
1775791,Probability to pick a certain amount of balls of some color,"Suppose there are 100 balls in a box. 20 balls are blue, 30 balls are green and 50 balls are yellow. Now we randomly pick out 10 balls out of the box (one ball after the other) and we don't put the balls back in the box. What's the probability of picking exactly 2 blue balls, 3 green balls and 5 yellow balls? My wrong attempt of a solution is $(\frac{20}{100} *  \frac{19}{99}) + (\frac{30}{98 }* \frac{29}{97 }* \frac{28}{96}) + (\frac{50}{95 }* \frac{49}{94 }* \frac{48}{93 }* \frac{47}{92 }* \frac{46}{91})$ the correct solution seems to be $(\frac{20}{100} *  \frac{19}{99}) * (\frac{30}{98 }* \frac{29}{97 }* \frac{28}{96}) * (\frac{50}{95 }* \frac{49}{94 }* \frac{48}{93 }* \frac{47}{92 }* \frac{46}{91})  *(\frac{10!}{2!3!5!})$ Thanks for your help fellas!","['combinatorics', 'probability']"
1775805,Tropicalization of a line in the projective plane $\mathbb P^2$,"Lets assume that our field $K$ is the Puiseux series. I have been working with tropicalization from the book ""Introduction to tropical geometry"" link : https://homepages.warwick.ac.uk/staff/D.Maclagan/papers/TropicalBook20.4.14.pdf The fundamental theorem of tropical algebraic geometry (page 99) tells us how we tropicalize a variety within an algebraic torus $T^n$. I am interested in the tropicalization of the line $V(ax+by+c)$ where $a,b,c\in K$ within $\mathbb{P}^2$. My problem is I have no idea on how to approach this exercise even thou I know how the set looks. Any help?","['abstract-algebra', 'tropical-geometry', 'algebraic-geometry']"
1775811,Past exam paper question on Reduction of order.,"Verify that $U_1(x)=x$ is a solution to the differential equation and solve $$x^2y''-x(x+2)y'+(x+2)y=0$$ Where $x>0$ . I've done most of the method, i'm at the point where you reduce the problem to a first order system, and then solve by using the separable method. It sounds stupid but i'm not sure what to do with this integral $\int (1/w) {dw} = \int (1) {dx}$ and what the final answer should be. Then use the Wronskien to show that the two solutions gained are linearly independent.
(sorry that I couldn't write my entire methodology, i'm not that good with HTML yet).",['ordinary-differential-equations']
1775830,Prove that the line integral of a vector valued function does not depend on the particular path,"Let C denote the path from $\alpha$ to $\beta$.
  If $\textbf{F}$ is a gradient vector, that is, there exists a differentiable function $f$ such that
$$\nabla f=F,$$ then
\begin{eqnarray*}
\int_{C}\textbf{F}\; ds &=& \int_{\alpha}^{\beta} \textbf{F}(\vec{c}(t)).\vec{c'}(t)\; dt \\
&=&\int_{\alpha}^{\beta} \nabla f(\vec{c}(t)).\vec{c'}(t)\; dt \\
&=&\int_{\alpha}^{\beta}  \frac{\partial  f(\vec{c}(t))}{\partial t}\; dt \\
&=&f(\vec{c}(\beta))- f(\vec{c}( \alpha))
\end{eqnarray*}
That is, the integral of $\textbf{F}$ over $C$ depends on the values of the end points $c(\beta)$ and $c(\alpha)$ and is thus independent of the path between them. This proof is true if and only if  $\textbf{F}$ is a gradient vector, what if not ?",['multivariable-calculus']
1775840,Is this proof valid? The claim is $2^{k} < (k+1)!$ for $k \geq 2$,"Hey guys so I think I have completed this proof but I'm not sure if its valid. Here it is: Prove that 
$
2^n < (n+1)! \quad\text{for}\quad n >= 2
$ Here is my proof: Base Case P(2) = $ 4 < 6 $ Inductive Hypothesis (IH) P(k) = $ 2^k < (k+1)! $ Proof P(k+1) = $ 2(k+1) < (k+2)! $ $ 2 * 2^k < (k+1)! * (k+2) $ I have already shown that $ 2^k < (k+1)! $ is true by IH. With $2$ being multiplied on the left and $(k+2)$ being multiplied on the right, if I can prove that $2 < (k+2) $ than the whole equation is true. 2 is always less than $k+2$ because k must be greater than or equal to 2 so the equation at minimum is $2 < 4$ . End Proof Is this valid? And if not, what am I missing? why is this approach wrong?","['induction', 'proof-verification', 'discrete-mathematics']"
1775870,Lebesgue–Radon–Nikodym Theorem Explanation,"From Folland, the theorem is as follows: The Lebsgue–Radon–Nikodym Theorem Let $\nu$ be a $\sigma$-finite signed measure and $\mu$ a $\sigma$-finite positive measure on $(X,\mathcal{M})$. There exists unique $\sigma$-finite signed measure $\lambda,\rho$ on $(X,\mathcal{M})$ such that $\lambda\perp \mu$, $\rho\ll\mu$, and $\nu=\lambda+\rho$. Moreover, there is an extended $\mu$-integrable function $f: X\to\mathbb{R}$ such that $d\rho=f\,d\mu$, and any two functions are equal $\mu$-a.e. The statement of the theorem itself seems straightforward. However, I am confused as to the later claim in Folland: In the case where $\nu\ll\mu$, the theorem says that $d\nu = f \, d\nu$ for some $f$ I would really appreciate a simple and intuitive explanation of this.","['derivatives', 'measure-theory']"
1775914,"Probability $P(X>Y,X>Z)$ for independent normal random variables $X$, $Y$, $Z$","There are several answers already given for working out the probability of one random variable being greater than another, but I can't make the leap to working out the probability of one random variable being greater than several others. My random variables are independent and normally distributed.
For example: Let $X$ , $Y$ and $Z$ be independent normal random variables. What is $P(X>Y,X>Z)$ ? The obvious (to me) answer, being to just multiply the two probabilities $P(X>Y)$ and $P(X>Z)$ does not work because the difference random variables $(X-Y)$ and $(X-Z)$ are not independent. Edit For $P(X>Y)$ the answer is: $$
{\rm P}(X  > Y )  = \Phi \left(\frac{\mu_X - \mu_Y }{\sqrt{\sigma_Y^2 + \sigma_Y^2}}\right).
$$ I'm hoping for a way of adding a third normal random variable to the equation. If this is possible I presume the answer can be easily expanded to add further random variables.","['probability', 'normal-distribution']"
1775917,Pendulum loss/gain of time per day given : $\ddot{\phi}+\frac{g}{l}\sin{\phi}=0$ and max displacement $5^{\angle}$,"Here is what i am given: The oscillations of a pendulum are described by the equation:
$$\ddot{\phi}+\frac{g}{l}\sin{\phi}=0$$
where $\phi$ is the angle between the pendulum and the vertical axis, $l$ is the length of the pendulum, and $g$ is the acceleration of gravity.
The pendulum of a longcase clock swings to a maximum angle of $5^{\angle}$ from the vertical. How many seconds does the clock gain or lose each day (1day=86400sec) if the clock is adjusted to keep perfect time when the angular swing is infintesimaly small? Hints: (1) Since the swing angle in the problem are small, simplify the equation for the pendulum while still keeping it nonlinear. (2)The relavite loss or gain of the clock is proportional to the relative change in the frequencies of the oscillations. (3) Don't forget to convert angles to radians. So i'm having trouble with the last few steps of the problem. I'll catch you up on where i am. Since the angle is small i used the first 2 terms of the MacLauren series to estimate $\sin\phi=\phi-\frac{\phi^3}{6}$ so our problem becomes 
$$\ddot{\phi}-\omega^2(\frac{\phi^3}{6}-\phi)=0$$
where $\omega^2=\frac{g}{l}$ and $\omega$ is the angular frequency in $s^{-1}$. I dropped $\omega^2$ for now and will introduce it at the end since it is a constant and inserted a small parameter $\epsilon$. Rearranging gives:
$$\ddot{\phi}+\phi=\epsilon\frac{\phi^3}{6}$$
From here i followed the steps for a nonlinear oscillator (like the Van der Pol oscillator) where we look for a solution in the form $\phi(t)=a(t)\cos(t+\psi(t))$ with $\dot{\phi}(t)=-a(t)\sin(t+\psi{t})$ where a(t) is the amplitude which is a function of time $t$ and $\psi(t)$ is a (phase?) constant dependent on initial conditions. I'm skipping a lot of steps here but a vague description of what i did was: differentiate, substitute into original problem, solve for $a'(t)$ and $\psi'(t)$  and then take their averages over one period ($2\pi$). If you're familiar with the method of averaging and certain solution methods for the Van der Pol or Wayleigh oscillators you probably understand what i did. 
I end up with 
$a'_{avg}(t)=0$ so that $a(t)=A$ (some constant dependent on initial conditions) and 
$$\psi_{avg}'(t)=-\epsilon\frac{(a(t))^2}{16}$$. 
This is a separable differential equation to which i find solution
$$\psi(t)=\epsilon\frac{(a(t))^2}{16}(t+k)$$
where $k$ is an integration constant $k=\psi(0)$ (i think). 
And since we determined that $a(t)=A$ i use this substition in $\psi$ also. Substituting this solution of $\psi$ back into our desired solution form gives
$$\phi(t)=A\cos\left(t-\epsilon\frac{A^2}{16}(t+k)\right)$$
And since Simple Harmonic Oscillators generally have solutions in the form of $x(t)=A\cos(\omega t)$ i reinsert $\omega$ inside the solution to get
$$\phi(t)=A\cos\left(\omega(t-\epsilon\frac{A^2}{16}(t+k))\right)$$ So that is where i am... I'm very UNconfident about those last couple steps and assumptions. I don't know how i'm supposed to find the loss (or gain) in time over the course of day. I'm also a bit unsure about that integration constant $k$. One of my classmates said that $k=0$ but i don't know how he got that and he can't justify it. I'm not sure how to use the given information to find a solution. I'm leaning toward this: Since we are given that ""the clock swings to a maximum angle of $5^{\angle}$ from the vertical"" it must be that $\phi(0)=5^{\angle}$ since the clock must have its maximum amplitude at $t=0$. But then i run into the problem with the ""$k$"" so i think i made some incorrect assumptions in the last few steps. Also, i'm not sure how that would help me find the time gain/loss per day. Is anyone familiar with this equation and this method for solving it?","['physics', 'ordinary-differential-equations', 'perturbation-theory']"
1775924,Laurent expansion of a function,"Consider the function $$f(z) = \frac{e^z+1}{e^z-1}$$ This function has a Laurent expansion about $0$ of the form $$f(z) = \frac{a}{z} + \sum_{n=0}^\infty b_nz^n$$ for constants $a, b_1,\ldots, b_n$. Show that $b_n=0$ for all even $n$. What is the best way to do this? All I can think to do is find the laurent expansion and then show that a few are zero but obviously this doesn't prove it for all even $n$, just a few. If we split $f(z)$ into even and odd parts we have $$f(z) = \frac az +\sum_{k=0}^\infty b_{2k}z^{2k}+\sum_{k=0}^\infty b_{2k+1}z^{2k+1}$$ and how the problem turns into showing the first summation is zero.",['complex-analysis']
1775958,Functor of points definition of a space modeled on a site,"I'm trying to find a definition of a space modeled on a site which is: (i) plausible and natural in the context of general sites (ii) subsumes common examples. Let $(C,J)$ be a grothendieck site and $Sh(C,J)$ the corresponding sheaf topos. Let $P$ be a property of morphisms in $C$ which containes all isomorphisms and is stable under composition and pullbacks. The aim is to define a space as a $J$ -sheaf with a cover by representable sheaves that glue by $P$ morphisms. Definition 1: A ""nice"" $P$ -local space is a sheaf $F \in Sh(C,J)$ s.t. there exists a representable sheaf $h_U$ and an epimorphism $\varphi: h_U \to F$ satisfying the following: $\varphi: h_U \to F$ is representable and has property $P$ (meaning all pullbacks to representable sheaves have property $P$ ). Example 1: If $C= Aff$ , $J=Zariski$ and $P=\text{open immersion}$ , we get a a scheme. Example 2: If $C=Aff$ , $J= \text{etale}$ and $P= \text{etale}$ we get an algebraic space (I hope. Separability conditions for algebraic spaces vary so I'm not so sure about the conventions here). Example 3: If $C= \text{CartSp}$ , $J = \text{open immersion}$ and $P =C^{k}$ we get a $C^k$ -manifold. Here is the wierd part. The definition above (although rather natural) only gives us some subcategory of ""nice"" schemes/algebraic spaces. Question 1: What subcategory of schemes does Example 1 correpond to? If I require that the covers be finite coproducts of affine scheme do I get qcqs schemes? To get the category of schemes we need an extra step: Definition 2: A general $P$ -local space is a sheaf $F \in Sh(C,J)$ s.t. there exists a representable sheaf $h_U$ and an
epimorphism $\varphi: h_U \to F$ satisfying: $h_U \times_F h_U$ is a ""nice"" $P$ -local space. Both projections $\pi_i : h_U \times_F h_U \to h_U$ are representable by a $P$ morphism. Question 2: Is the above condition equivalent to requiring that the
diagonal $h_U \to h_U \times h_U$ be representable by a separated $P$ -local space? Now wer'e fine since any intersection of affines is a quasi-affine scheme which satisfies definition 1. Question 3: Why is this step reasonable from this point of view? Does this have something to do with closure under pullbacks? Does ""definition 1"" generate ""definition 2"" by taking arbitrary pullbacks?.","['category-theory', 'sheaf-theory', 'topos-theory', 'algebraic-geometry']"
1775977,Selling oranges when people queue up in a line,"We have $a$ oranges to give to $b$ people. Each person has a value $f(n)$ for receiving $n$ oranges, where $f$ is a nondecreasing, nonnegative function that is the same for everyone. Let $X$ be the maximum total value possible (summing up everybody's value). Suppose that we set a price $p$ for each orange, and people queue up in a line. If there are $k$ oranges left, the next person will choose the number $l\in[0,k]$ that maximizes the profit $f(l)-p\cdot l$. Let $Y$ be the total value obtained this way (if there are ties, break them in a way that $Y$ is as low as possible.) It is obvious that $Y\le X$. Is there a positive constant $r$ such that for any $a,b,f$, we can find $p$ so that $Y\ge rX$?","['algebra-precalculus', 'optimization']"
1775986,connected sum of surfaces is well defined proof attempt,"Suppose $S_1$ and $S_2$ are compact surfaces (connected 2-dimensional manifolds). If we cut out of them two closed disks, and glue the surfaces along disk boundaries we get new surface, their connected sum, denoted by $S_1\#S_2$. I have precise definition in Massey's Algebraic topology , but couldn't find proof that definition is well: Take disks $D_1 \subset S_1$ and $D_2 \subset S_2$, and define $S_i' = S_i \setminus Int(D_i)$ for $i=1,2$. Choose homeomorphism $h$ sending boundary of $D_1$ to boundary of $D_2$. Then $S_1 \# S_2$ is defined as a quotient space of $S_1' \cup S_2'$ with $x$ and $h(x)$ being identified for $x \in \partial D_1$. My attempt (not finished): Take now $\hat{D}_1 \subset S_1, \hat{D}_2 \subset S_2$, $\hat{S}_i' = \hat{S}_i \setminus Int(\hat{D}_i) $ for $i=1,2$ and another homeomorphism $\hat{h}$ mapping $\partial \hat{D}_1$ onto  $\partial \hat{D}_2$. 
Construct a quotient space of $\hat{S}_1' \cup \hat{S}_2'$ where $x$ and $\hat{h}(x)$ are identified for $x \in \partial D_1$. We must be able to construct homeomorphism $g$ between those two quotient spaces. We know that there exist homeomorphism $f$ mapping boundary of $ \hat{D}_2$ onto boundary of $D_2$. I want to define some homeomorphism on certain subsets of first space and then proceed by gluing lemma. Now I propose $g_1(x) = (h^{-1} f \hat{h})(x)$ for $x$ in the equivalence class of $ \partial D_1$ and $g_2(x) = (h^{-1} f) (x)$ for $x$ in the equivalence class of $\partial D_2$.
And now I don't have idea what to do with other points and have concern since both of those boundaries are closed as sets and what remains in complement is open... So I'm stuck here, since gluing lemma couldn't be applied, even if I  would manage to define anything on the rest of the space. Any idea, comment?","['algebraic-topology', 'compact-manifolds', 'general-topology', 'manifolds']"
1776014,"What are the differences between empty set, zero set and null set?","What are the differences between empty set, zero set and null set? If i'm right empty set and null set is the same which is {}
but zero set is {0} ?",['discrete-mathematics']
1776117,How do fixed point arguments for PDE work?,"My understanding (which I'm not altogether sure of) is that a a ""fixed point"" argument often used in PDE goes something like this: If we have some PDE like
$$u=F(u),$$
we consider a sequence of functions $u^{(n)}$ recursively defined so
$$u^{(n+1)}=F(u^{(n)}).$$
Then you compute an estimate like
$$\|u^{(n+1)}\|_X\leq g(\|u^{(n)}\|_X)$$
(where the mathematician knows what function space $X$ should be) and then you solve the recurrence equation to find say
$$\|u^{(n)}\|_X\leq c$$
and then use some kind of compactness argument to say the $u^{(n)}$ have a particular kind of limit in $X$ which by construction solves the desired equation. Is my understanding of this strategy broadly correct? There are some details I don't understand. For instance, what is the particular kind of compactness argument we need? Say $X=H^s$. A ball in $H^s$ is not compact in the norm topology, right? So how does one make this work? I ask this question because it seems that people who do PDE use things like this all the time so they skim over the technical details.","['functional-analysis', 'partial-differential-equations']"
1776138,"In proving A = B, A, B are sets, do you always have to show $\subseteq$ and $\supseteq$?","I am trying to show the DeMorgan's Law $X \backslash \bigcup_{\alpha \in I} A_\alpha = \bigcap_{\alpha \in I}
(X \backslash A_\alpha)$ It seems I could directly approach this as follows: $X \backslash \bigcup_{\alpha \in I} A_\alpha = X \bigcap (\bigcup_{\alpha \in I} A_\alpha)^c  = X \bigcap (\bigcap_{\alpha \in I} A_\alpha^c) = \bigcap_{\alpha \in I} X \backslash A_\alpha$ The last line follows from distributivity But I thought in set theory proofs of the type Prove $A = B$ , you have to show that $ A \subseteq B$ and $B \subseteq A$. But in this case it seems I have directly proved that the two are equivalent without resorting to $A \subseteq B$ and $B \subseteq A$... Can someone enlighten me as to whether my approach is correct? I am not very well versed in set theory proofs.","['proof-verification', 'proof-writing', 'elementary-set-theory', 'proof-explanation', 'general-topology']"
1776145,limit of the function of two variables ??,"Here is the function :
\begin{equation}
f(x,y)=a^2 \left(\frac{x}{a^2-3 x^2}-\frac{y}{3 y^2-a^2}\right).
\end{equation}
We know that $y\geq x \geq 0$, $~a$ is a constant and $a^2=x^2+xy+y^2$. I want to derive the limit of the function in the case $x\rightarrow a/\sqrt{3}=r_0$ and  $y\rightarrow a/\sqrt{3}=r_0$. One method is to take $x=r_0-\delta$ and $y=r_0+\delta$ and $a=\sqrt{3}r_0$. Then substituting them into $f(x,y)$, I can obtain 
\begin{equation}
f(r_0,\delta)=-\frac{2 r_0^3}{4 r_0^2-\delta ^2}.
\end{equation}
Thus in the limit of $\delta \rightarrow 0$, we can get the result $-r_0/2$. The another method is to substitute $a^2=x^2+xy+y^2$ into $f(x,y)$ directly, I get 
\begin{equation}
-\frac{(x+y) \left(x^2+x y+y^2\right)}{(2 x+y) (x+2 y)},
\end{equation}
then set $x=y=r_0$. I get the limit is $-2r_0/3$. Obviously, the two results are different. So which method is reliable?","['multivariable-calculus', 'limits']"
1776182,What is the Most General Setting in Which Limits Commute with Continuous Functions?,"In metric spaces, we have that limits commute with continuous functions . In Hausdorff spaces, the limits of nets are always unique. Seemingly the second fact is necessary for the proof of the first. However, it is not clear to me that metric structure is necessary for the proof of the first either (for example, why would uniform structure not be sufficient). This might, however, just be a result of the fact that I don't understand the concept of limit very well in spaces more general than metric spaces. Question: What is the most general type of space for which a function is continuous if and only if it commutes with the limits of sequences? Somewhere strictly between Hausdorff and metrizable spaces? ( Note: I'm very surprised that this question seems not to be a duplicate -- I could not find an equivalent question or the answer when Googling. If you know where to find the answer, please just close the question without downvoting and comment with the link.) Background: Inspired by this question , in which an elegant proof using this property is given. Naturally I am curious to see the full generality to which this proof can be extended.","['general-topology', 'metric-spaces', 'analysis']"
1776194,Why do people accept the axiom of choice given the well ordering principle?,"We know without any doubt that the axiom of choice implies (in fact is equivalent to) the well ordering principle. The well ordering principle can't be true! If we take the open interval $(0,1)$ for example, there can't be a least (or most) element. If you give me any element of this set, I could always find one that is greater than or smaller than the one you give me. Isn't this enough to conclude that the axiom of choice - though intuitive - should be viewed as untrue? Thanks.","['order-theory', 'elementary-set-theory', 'axiom-of-choice']"
1776196,Concatenation of strings is not in the set,"A set $M$ contains some strings of $0$s and $1$s of length no more than $n$, in a way that if $a,b\in M$ (possibly $a=b$), then their concatenation $ab$ doesn't belong to $M$. What is the maximum size of $M$? As an example, if $M$ only contains strings of size at least $s=\lfloor n/2\rfloor + 1$, then any concatenation has length more than $n$, so doesn't belong to $M$. This gives a set $M$ of size $2^s+2^{s+1}+\dots+2^n=2^{n+1}-2^s$.",['combinatorics']
1776205,How to Prove that a (Centered) Gaussian Process is Markov if and only if this Equation Holds?,"A centered Gaussian process is Markov if and only if its covariance
  function $\Gamma: \mathbb{R}\times\mathbb{R} \to \mathbb{R}$ satisfies
  the equality: $$\Gamma(s,u)\Gamma(t,t)=\Gamma(s,t)\Gamma(t,u)\ \ \ \ (1)$$ for all $s<t<u$. My Question: How can you prove this? It turns out to be difficult to find a clear proof of the above fact. One proof which I found but did not understand well was: Example 4.5, p.119 of Random Processes for Engineers by Bruce Hajek, or Example 4.7 on pp. 120-121. The notation used in that book made very little sense to me. Background: Inspired by my answer to this question . I had to try and prove the statement for a homework assignment, but was not very successful. A solution was explained in class, but it relied on the rather implausible Claim: A function $h: \mathbb{R}\times\mathbb{R} \to \mathbb{R}$ satisfies equation (1) if and only if it is of the form $$h=\max(f(s,t),g(s,t))\min(f(s,t),g(s,t))$$for some two functions $f,g:\mathbb{R}\times\mathbb{R} \to \mathbb{R}$. The sufficiency of the claim to me is clear, but the necessity is not, nor was it proven in class. Moreover, I did not understand anyway the subsequent proof which was supposed to show that the desired equation follows from this claim. What I Have Tried: (Copy-pasted from the TeX file from my homework ""solution"", which I already handed in and got a grade for) Let $X_t$ be a centered Gaussian process. Specifically, $\forall t$,
  $\mathbb{E}(X_t)=0$ and $X_t \sim \mathscr{N}(0,\sigma_t)$. Now assume that ab initio $\Gamma(s,u)\Gamma(t,t)=\Gamma(s,t)\Gamma(t,u)$ for any $s<t<u$, but that we have not necessarily shown that $X_t$ is Markov. Then $\Gamma(s,u)=\frac{\Gamma(s,t)\Gamma(t,u)}{\Gamma(t,t)}$. Because the process is centered Gaussian, this is true if and only if (see Example 4.5, p.119 of Random Processes for Engineers by Bruce Hajek, or Example 4.7 on pp. 120-121.) $X_u$ and $X_s$ are conditionally independent given $X_t$. This equivalence is due to the fact that the above equation implies that
  $$\mathbb{E}[(X_s | X_t)(X_u |X_t)]=0,$$ i.e. that the two conditional expectations are uncorrelated, and since the two random variables are Gaussian distributed, this implies even that they are independent. This is equivalent to the Markov property (past and future are independent given the present).","['stochastic-processes', 'probability-theory', 'conditional-expectation', 'markov-process']"
1776232,Geometric interpretation for eigenvalues and eigenvectors of the cross product's representation as a linear map,"Fix ${\bf x} = (x_1,x_2,x_3) \in \Bbb R^3\setminus\{{\bf 0}\}$. We can look at the cross product as a linear map ${\bf x}\times: \Bbb R^3 \to \Bbb R^3$ which is represented in the standard basis by $$\begin{bmatrix} 0 & -x_3 & x_2 \\ x_3 & 0 & -x_1 \\ -x_2 & x_1 & 0\end{bmatrix}.$$Also, it is easy to compute its charcteristic polynomial $p(t) = t(t^2 + \|{\bf x}\|^2)$. Then $0$ is an eigenvalue for which the associated eigenspace is the line spanned by ${\bf x}$ itself. But we can write $$p(t) = t(t-i\|{\bf x}\|)(t+i\|{\bf x}\|),$$and continue the analysis. Assuming I didn't screw up computations, I get that the a complex eigenvector associated to $i\|{\bf x}\|$ is $${\bf v} = \left(-x_1x_3 - x_2\|{\bf x}\|i, -x_2x_3+x_1\|{\bf x}\|i, x_1^2+x_2^2\right).$$We have $${\rm Re}({\bf v}) = (-x_1x_3,-x_2x_3,x_1^2+x_2^2) \quad\mbox{and}\quad {\rm Im}({\bf v}) = (-x_2\|{\bf x}\|,x_1\|{\bf x}\|,0).$$Then I noticed that: $${\bf x}\times {\rm Re}({\bf v})  = \|{\bf x}\|\,{\rm Im}({\bf v}) \quad\mbox{and}\quad {\bf x}\times {\rm Im}({\bf v}) = -\|{\bf x}\|\,{\rm Re}({\bf v}).  $$ Even more, ${\rm Re}({\bf v})$ and ${\rm Im}({\bf v})$ are orthogonal. I am bewildered by my little discovery. However, I can't quite interpret this geometrically, and I guess that extra factor of $\|{\bf x}\|$ is related to the $i\|{\bf x}\|$ eigenvalue. Can someone explain what's behind these computations? I just found this question, and I apologize for not searching well enough before asking - yet, there is no satisfactory answer there, and they didn't use real and imaginary parts of the eigenvectors like I pointed here - it might make something easier to see, so please don't vote to close as duplicate (yet?).","['analytic-geometry', 'diagonalization', 'cross-product', 'linear-transformations', 'linear-algebra']"
1776238,limit of $ x/\sin(\pi x)$ as $x$ approaches zero?,"I reorganized as $x\csc(\pi x)$ and input $0$, which would be $0$, but the answer is apparently wrong? It says the answer is $1/\pi$. Any help?","['trigonometry', 'limits']"
1776260,Simplifying the result formula for depressed Cubic,"After understanding the Cardano's formula for solving the depressed cubic (of the form $x^3+mx=n$, of course), I tried to find the solution of the equation $$x^3+6x=20.$$
After plugging into the formula
$$x=(n/2+\sqrt{ \frac{n^2}{4}+ \frac{m^3}{27} })^{1/3}+(-n/2+\sqrt{ \frac{n^2}{4}+ \frac{m^3}{27} })^{1/3}$$
where $m=6$ and $n=20$, we get
$$x=(10+ \sqrt{108})^{1/3}-(-10+ \sqrt{108})^{1/3}.$$
However, we notice that, without using Cardano's formula, that $x=2$ is the solution for the equation $x^3+6x=20.$
My question is: how does the equation $$x=(10+ \sqrt{108})^{1/3}-(-10+ \sqrt{108})^{1/3}$$ get simplified to $x=2$? P.S. I understand that it was Niccolo Fontana who first figured out how to solve depressed cubic, to give one the proper credit.",['algebra-precalculus']
