question_id,title,body,tags
4130746,Finding the smallest subgraph of a given chromatic number,"If I have a graph whose chromatic number I know to be say, 5, is there a method then for finding the smallest subgraph of chromatic number 4? Here is an example where the graph has a chromatic number of 4 and I want to find the smallest subgraph of chromatic number 3: Here I have the moser spindle whose chromatic number is 4, and the smallest subgraph with chromatic number 3 would be the complete graph $K_3$ : I suppose the question can be generalized as: Given a graph with a chromatic number $k$ , can one find the smallest subgraph with chromatic number $k-1$ ? Or is this something that would have to be brute-forced? Is there any optimization that could be made knowing the chromatic number of the super-graph? Thank you!","['graph-theory', 'coloring', 'combinatorics', 'discrete-mathematics']"
4130839,Line integral of a Vector Field around a Closed path,"This is just a little question. Suppose you want to evaluate an integral around a closed path formed by a curve $C(t) $ (only one curve), I suspect that the result would be $0$ , because you will do an integral from the point $P$ to the same point. so for example if $P=C(a)$ , then your integral is $$\int_CF=\int_a ^a F(C(t))\cdot C'(t)\,dt = 0$$ Is that true?","['multivariable-calculus', 'calculus', 'vector-analysis']"
4130853,"Prove if $\{X_n\}$ is a sequence of independent r.v. and converges to $X$ in probability, then $X$ is a constant almost surely.","Prove that if $\{X_n\}$ is a sequence of independent random variables and converges to $X$ in probability, then $X$ is a constant almost surely. My attempt: To prove that $X$ is a constant a.s., it suffices to show that $\mathbb{P}(X<x)=0 \text{ or } 1$ . Notice that $X_n\xrightarrow[]{\mathbb{P}}X$ implies $X_n\xrightarrow[]{D}X$ . Thus, $\mathbb{P}(X\leq x)=\lim_{n\to \infty}\mathbb{P}(X_n \leq x)\leq\mathbb{P}(\lim\sup_{n\to\infty}\{X_n\leq x\}).$ By Borel's 0-1 law, we have $\mathbb{P}(\lim\sup_{n\to\infty}\{X_n\leq x\})=0\text{ or } 1$ . Similarly, $\mathbb{P}(X\leq x)\geq\mathbb{P}(\lim\inf_{n\to\infty}\{X_n\leq x\})=0\text{ or }1.$ I notice that if both $\mathbb{P}(\lim\sup_{n\to\infty}\{X_n\leq x\})$ and $\mathbb{P}(\lim\inf_{n\to\infty}\{X_n\leq x\})$ always have the same value(0 or 1), then we can prove that $\mathbb{P}(X\leq x)=0 \text{ or }1$ . But somehow I find it very hard to prove. Is there something I have not thought of? Edit:
OK, I have figured it out. In this post , we have the conclusion for convergence almost surely. As for convergence in probability, we can always find its subsequence that convergences almost surely to $X$ . Then, we can apply the conclusion for convergence almost surely.","['probability-theory', 'probability']"
4130856,Quotients of multivariate polynomial rings - is $k[x][y]/(y-x^2) \cong k[y][x]/(y-x^2)$?,"This question is motivated by exercise 1.1 in Hartshorne Algebraic Geometry. One has to prove that $k[x,y]/(y-x^2)$ is isomorphic to a polynomial ring in one variable. I know that this can be done by viewing $k[x,y]/(y-x^2)$ as the image under the surjection $k[x][y] \to k[x][y]/(y-x^2)$ given by $x \mapsto x$ , $y \mapsto x^2$ (that is, $f(x,y) \mapsto f(x,x^2)$ ). Since $x^2 \in k[x]$ , the image under this evaluation map is just $k[x]$ . My question is, why does it apparently not work to view $k[x,y]/(y-x^2)$ as $k[y][x]/(y-x^2)$ , the image under the map $k[y][x] \to k[y][x]/(y-x^2)$ given by $x \mapsto \sqrt{y}$ , $y \mapsto y$ ? Since $\sqrt{y} \notin k[y]$ , I think the evaluation can't produce a ring isomorphic to $k[y]$ . But it seems to me that if $k[x,y]/(y-x^2)$ is isomorphic to $k[x]$ , then it should be isomorphic to $k[y]$ also, and therefore cannot be isomorphic to the ring obtained by this evaluation.","['ring-homomorphism', 'algebraic-geometry', 'polynomials', 'commutative-algebra']"
4130873,Cayley Transform of self-adjoint operator is unitary,"If $A$ is a self-adjoint operator, I want to show that the Cayley transform of $A$ defined as the operator $U=(A-iI)(A+iI)^{-1}$ is unitary. Here's my trial:
For all $x,y \in \mathcal{H}$ , keeping in mind that $A$ and $I$ have adjoints: $$\langle Ux,Uy \rangle = \langle U^*Ux,y \rangle$$ All i need to show now is that $U$ is an Isometry, i.e, $U^*=U^{-1}$ and I'm all set (right?). Therefore: $$ U^*=[(A-iI)(A+iI)^{-1}]^* = [(A+iI)^{-1}]^*(A-iI)^*$$ But i cannot go further because $(A+iI)$ is not self-adjoint, so I can't switch the inverse and the adjoint operations. Does somenone have some tip on how to get the inverse of this operator or an alternative way to show that $U^*=U^{-1}$ ? all the help will be appreciated.","['hilbert-spaces', 'self-adjoint-operators', 'functional-analysis']"
4130874,Prove that $G$ is a block if and only if given a vertex and an edge of $G$ there exists a cycle containing them.,"Prove that $G$ is a block if and only if given a vertex and an edge of $G$ there exists a cycle containing them. A block of a graph $G$ is a maximal connected subgraph of G that has no cut-vertex. Attempt: Let $u\in V(G)$ and $a\in E(G)$ , $a=wv$ . By hypothesis, there exists a cycle $C$ containing $u$ and $w$ . As for $v$ there are two possibilities: that it is part of the vertices of the cycle or that it is not. If $v\in V(C)$ we obtain a cycle $C'$ containing $u$ and edge $a$ as follows: \begin{align*}
C&=u, u_1, \ldots , w, w_1, \ldots v, v_1, \ldots u \\
C'&=u, u_1, \ldots , w, v, v_1, \ldots u  
\end{align*} That is, we replace the part $C$ between $w$ and $v$ by the edge $a$ .
If $v\notin V(G)$ we have a situation equal to the previous demonstration and we can find two paths from $u$ to $v$ , one of which contains edge $a$ .","['graph-theory', 'graph-connectivity', 'discrete-mathematics']"
4130953,Has anyone found this formula before? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I found this formula. \begin{equation}
\sum_{\lambda \vdash n}\frac{2^{l(\lambda)}}{z_{\lambda}}=n+1
\end{equation} where $\lambda \vdash n$ means that $\lambda$ is an integer partition of $n$ , $l(\lambda)$ is the length of $\lambda$ and if $\lambda=(1^{m_{1}},2^{m_{n}},\cdots,n^{m_{n}})$ , $z_{\lambda}=m_{1}!m_{2}!\cdots m_{n}! 1^{m_{1}}2^{m_{2}}\cdots n^{m_{n}}$ .\ Has anyone found it before?","['combinatorics', 'algebraic-combinatorics']"
4130997,Manifolds and Schemes from Idempotent Splitting,"Recently I read on the nlab , that the category $\mathsf{SmoothMf}$ of smooth manifolds can be realized as the Karoubi-envelope / Cauchy-completion of the category $\mathsf{SmoothOpen}$ of open subsets of euclidean spaces $\mathbb R^n$ and smooth maps between them. The proof invokes the tubular neighborhood theorem , which (according to a rather coarse search of the internet) seems to be a special property of $\mathsf{SmoothMf}$ . Regardless I am wondering, whether we can identify the category of topological manifolds $\mathsf{TopMf}$ with the Cauchy-completion of $\mathsf{TopOpen}$ , using continuous maps instead of smooth ones. Similarly, is the (or some nice subcategory, say smooth ones) category of schemes $\mathsf{Sch}$ realizable as the Cauchy-completion of the category $\mathsf{Aff} = \mathsf{CRing}^{op}$ of affine schemes? I feel like this question is natural to ask, so it should be answered already, but didn't find any positive nor negative results. Unfortunately I don't really know enough about the differences between $\mathsf{SmoothMf}$ and $\mathsf{TopMf}$ nor do I know much about Cauchy-completions. So I apologize, if there are more than obvious obstructions I just didn't notice yet. Anyway, thank you for your time.","['geometry', 'category-theory', 'smooth-manifolds', 'manifolds', 'schemes']"
4131011,Is there a standard name for this definition?,"Consider an n-ary function from a set $S$ to itself. For a specific example consider the function from $\mathbb{R}^2$ to $\mathbb{R}$ defined by the formula $xy^2$ . I define a reduction of that function to be a function where you hold zero or more variables constant. So for example, the reductions of that function are $xy^2$ , $cx^2$ (where $c$ is an arbitrary real number), $px$ (where $p$ is an arbitrary non-negative number), and nullary functions $r$ , where $r$ is an arbitrary real number. Is there a standard name for this definition?","['functions', 'terminology']"
4131072,Good introduction to free groups and free products,"In my undergraduate research project, I am going to study a paper on free products in division rings. To do this, however, I, of course, need to learn about free groups and free products. Right now, the only reference I have is Rotman's ""An Introduction to the Theory of Groups"". Is this a good reference? Or is there a better book to get the intuition and the main theorems behind free groups? Please have in mind that I am self-studying and that, being an undergrad, if it is possible to avoid too much Category Theory, it would be best. Thanks in advance!","['book-recommendation', 'reference-request', 'free-groups', 'free-product', 'group-theory']"
4131084,What is a non binary adjacency Matrix?,I am going through the implementation of a graph convolutional neural network. I came across a non-binary adjacency matrix in the case of a directed graph. The particular issue is discussed here in the following https://github.com/tkipf/pygcn/issues/3 Can someone explain what a non-binary adjacency matrix looks like? How can an adjacency matrix have something other than 0 or 1?,"['matrices', 'adjacency-matrix', 'graph-theory', 'symmetric-matrices']"
4131095,Using Stokes theorem to find the line integral over the boundary of a paraboloid in the first octant opening downward the z-axis,"I've been trying at this problem on my homework, but I think I am going about it the wrong way. I tried breaking it down into the line integrals of the boundaries of the surface, but I think I might have the wrong idea about how Stokes' Theorem works. Can someone please give me a step by step solution to this problem? Edit I will include my work here: I tried several different things and none of them turned out right.","['curl', 'multivariable-calculus', 'calculus', 'multiple-integral', 'stokes-theorem']"
4131109,If x =-1 for the derivative of ln(x)=1/x does that imply that ln(-1) = -1?,"I wanted to find the solution of x for y=ln(x)-1/x=0 and my first intuition was to rearrange the equation to ln(x)=1/x and find the first derivative of the equation which gave me 1/x=-x^-2. I rearranged this equation to get x and it got me to x=-1. this made me ask if x=-1, does that imply that ln(-1)=-1? I understand that there are other ways to find x which is true for the equation. but I don't understand why this doesn't work and why ln(-1) is not equal to -1 even when x=-1. Does anybody have any ideas as to why this is?","['calculus', 'derivatives', 'logarithms']"
4131137,Calculate $\lim_{n \to \infty} \dfrac{\ln (\log_a (n))-\ln (\log_n (a))}{\ln (n)}$,"I have to calculate $\lim_{n \to \infty} \dfrac{\ln (\log_a (n))-\ln (\log_n (a))}{\ln (n)}$ My idea Let $a_n = \dfrac{\ln (\log_a (n))-\ln (\log_n (a))}{\ln (n)}$ , then $$e^{a_n}= (\log_a (n))^{1/\ln(n)} \cdot \left( \dfrac{1}{\log_n (a)} \right)^{1/\ln(n)} $$ And I don't know to continue","['limits', 'limits-without-lhopital']"
4131145,Examples of bijective map from $\mathbb{R}^3\rightarrow \mathbb{R}$,Could any one give an example of a bijective map from $\mathbb{R}^3\rightarrow \mathbb{R}$? Thank you.,"['elementary-set-theory', 'examples-counterexamples', 'real-analysis']"
4131291,Is the total derivative defined for a function of multiple independent variables?,"It seems clear enough to define the total derivative of a function $f=f(x(t),y(t))$ of multiple ""intermediate variables"" $x$ and $y$ , who themselves depend on one independent variable $t$ , to be: $$\frac{df}{dt}=\frac{\partial f}{\partial x} \frac{dx}{dt} + \frac{\partial f}{\partial y} \frac{dy}{dt}$$ (One could say this is simply an application of the multivariable chain rule.) I also find it reasonable when multiple independent variables $t$ and $u$ are present (yielding $f=f(x(t,u),y(t,u))$ to calculate the partial derivatives by: $$\frac{\partial f}{\partial t}=\frac{\partial f}{\partial x} \frac{\partial x}{\partial t} + \frac{\partial f}{\partial y} \frac{\partial y}{\partial t}$$ (similarly for $\frac{\partial f}{\partial u}$ .) However, I am confused if such as thing as the ""total derivative"" $\frac{d f}{d t}$ can be defined when their are multiple independent variables $t$ and $u$ present, and how it would relate to the partial derivative $\frac{\partial f}{\partial t}$ . Note: I understand that in the simple case of a function depending explicitly on multiple independent variables, such as $f=f(t,u)$ , we do have total derivatives with respect to each variable: $$\frac{df}{dt}=\frac{\partial f}{\partial t} + \frac{\partial f}{\partial u} \frac{du}{dt}  \text{  and  }  \frac{df}{du}=\frac{\partial f}{\partial t} \frac{dt}{du} + \frac{\partial f}{\partial u}$$ I'm not sure how this can be generalized to the case with $f$ depending on intermediate variables $x(t,u)$ and $y(t,u)$ . I attempted to solve this myself, but I ended up with a strange result. Here was my approach: Take the total differential of $f$ : $$df=\frac{\partial f}{\partial x} dx + \frac{\partial f}{\partial y} dy$$ being aware that since $x=x(t,u)$ , then $dx=\frac{\partial x}{\partial t} dt + \frac{\partial x}{\partial u} du$ and similarly for $dy$ . Dividing the total differential by $dt$ (not rigorous, I know), we obtain: $$\frac{df}{dt}=\frac{\partial f}{\partial x} \frac{dx}{dt} + \frac{\partial f}{\partial y} \frac{dy}{dt}$$ We can evaluate $\frac{dx}{dt}$ as $\frac{dx}{dt}= \frac{\partial x}{\partial t} \frac{dt}{dt} + \frac{\partial x}{\partial u} \frac{du}{dt} = \frac{\partial x}{\partial t}$ because $\frac{du}{dt}=0$ , and similarly for $\frac{dy}{dt}$ : $$\frac{df}{dt}=\frac{\partial f}{\partial x} \frac{\partial x}{\partial t} + \frac{\partial f}{\partial y} \frac{\partial y}{\partial t}$$ But I must have made a mistake somewhere, because this expression is equal to $\frac{\partial f}{\partial t}$ , which should only be true when $t$ is the only independent variable. What have I done wrong, and what is the true expression for $\frac{df}{dt}$ (if it exists)? Thank you for your time.","['partial-derivative', 'derivatives', 'differential']"
4131322,Second order linear ODE with nonconstant coefficients: Why can we not use the ansatz $e^{\lambda x}$?,"For a constant-coefficient case, it is guaranteed that the solutions we get from taking $e^{\lambda x}$ to be the ansatz and finding out the value of $\lambda$ are valid. Generally, for a homogeneous equation $$ay''+by'+cy=0$$ it is equivalent to taking the linear differential operator as $$\mathcal{L}=aD^2 + bD + c$$ where $D = d/dx$ , and saying that $$\mathcal{L}y=0$$ So I would say that $e^{\lambda x}$ is a basis for the eigenspace of $\mathcal{L}$ which corresponds to the eigenvalue of $$\Lambda=a\lambda^2 + b\lambda +c$$ henceforth making the equation $\mathcal{L}y=\Lambda y$ . However, this method does not work for a nonconstant-coefficient case. But I don't see why it doesn't work. Instead of talking about general cases, I will use a specific example from now on. Consider the equation $$y''+xy'+y=0$$ I found that it is entirely valid up to carrying out the characteristic equation. $$\lambda^2 e^{\lambda x} + x \lambda e^{\lambda x} + e^{\lambda x}=0$$ $$(\lambda^2+x\lambda+1)e^{\lambda x}=0$$ Since $e^{\lambda x}>0$ , $$\lambda^2+x\lambda+1 = 0$$ Then we can find $\lambda$ using the quadratic formula ( admitting that $\lambda$ is a function of $x$ ): $$\lambda_1(x) = \frac{-x + \sqrt{x^2-4}}{2},\ \lambda_2(x) = \frac{-x - \sqrt{x^2-4}}{2}$$ Since the equation is linear, we can guarantee that the span of $\{e^{\lambda_1 x}, e^{\lambda_2 x}\}$ is a solution. Thus, $$y(x) = e^{-x/2} \times [Ae^{+\frac{\sqrt{x^2-4}}{2}x} + Be^{-\frac{\sqrt{x^2-4}}{2}x}]$$ Except for the fact that the eigenvalue $\lambda$ is a function of $x$ there is nothing peculiar about it. I thought it is fine, because what we are taking as ""vectors"" is the solution function $y$ , not the independent variable $x$ underlying at the back of this function.","['eigenfunctions', 'solution-verification', 'ordinary-differential-equations', 'eigenvalues-eigenvectors']"
4131334,Paradigm shifts from $2\to 3$,"Recently, I've been thinking about a common theme that I've seen all over mathematics. One often finds, that when the number of dimensions/degrees of freedom in a given scenario/problem changes from $2$ to $3$ , that some fundamental shifts in the solution or resulting behavior occur. I'll list five examples of what I'm talking about, three of which have to do with differential equations. (What can I say? Read my bio.) Before I do so, my questions are What are some other examples of this theme? Is there a universal reason why this happens? Does it have to do with $3$ being the first odd prime? Or perhaps something else? 1: Poincaré-Bendixson Theorem For those that don't know, the Poincare-Bendixson theorem is a deep result in ODEs/dynamical systems. Consider the autonomous ODE $$\dot{\mathrm{x}}=\mathrm{u}(\mathrm{x})\tag{1}$$ Where $\mathrm{x}:\mathbb{R}\to\mathbb{R}^n~;~\mathrm{x}:t\mapsto \mathrm{x}(t)$ , and $\mathrm{u}:\mathbb{R}^n\to\mathbb{R}^n$ . When $n=1$ the behavior of the solution is typically very easily to analyze heuristically, and in particular, it is rather obvious that only monotonic solutions exist. When $n=2$ , things obviously get a lot more complicated but there is in fact the powerful P-B theorem: Let $\Omega\subset \mathbb{R}^2$ be closed and bounded. If $u$ is $C^1$ in $\Omega$ , $\Omega$ contains no fixed points, and $\exists x_0\in\Omega$ such that the solution of the IVP $$\dot {\mathrm{x}}=\mathrm{u}(\mathrm{x})~~;~~\mathrm{x}(0)=\mathrm{x}_0$$ Is entirely contained in $\Omega$ , then there is at least one closed orbit in $\Omega$ . Quite a remarkable theorem if you ask me. The consequence of this theorem is that there is no chaotic behavior in two dimensions. In the plane, any solution of $1$ is either unbounded bounded and approaching a periodic limit cycle periodic So solutions that are bounded , but do not approach a stable limit cycle , i.e strange attractors , are impossible in the plane. However, when we jump from dimension two to dimension three, and any subsequent dimension, no similar result exists. Solutions of autonomous ODEs in $n>2$ dimensions can be as ""strange"" as you like. 2: Fermat's Last Theorem Consider the simple equation $$a^n+b^n=c^n\tag{2}$$ Where $a,b,c\in\mathbb{Z}\setminus \{0\}$ and $n\in\mathbb{N}$ . The question is, given $n$ , how many solutions $(a,b,c)$ exist to $\boldsymbol{(2)}$ ? When $n=1$ it is obvious that there are infinitely many solutions - the sum of any two integers is an integer. When $n=2$ , proving that infinitely many solutions exist is still rather easy, and known thousands of years ago to the Greeks. We can simply let $r,k\in\mathbb{N}$ with $k>r$ and observe that $$(k^2-r^2)^2+(2rk)^2=(k^2+r^2)^2$$ Since there are infinitely many pairs of positive integers $(k,r)$ with $k>r$ there are infinitely many solutions. However, as I am sure you are all aware, the general answer was not known until 1995, when Andrew Wiles published his complete and peer-reviewed proof of the problem, $358$ (!) years after the problem's conception by Fermat. His result was that For $n>2$ , no solutions to $\boldsymbol{(2)}$ exist. 3: The Three Body Problem Take a system of $n$ particles in $\mathbb{R}^3$ with positions $\mathrm{r}_1,\dots ,\mathrm{r}_n$ and masses $m_1,\dots, m_n$ and consider the the coupled vector IVP $$\ddot{\mathrm{r}}_{i}=\sum _{j\in \{1,\dotsc ,n\} \setminus \{i\}}\frac{-Gm_{i} m_{j}}{\| \mathrm{r}_{i} -\mathrm{r}_{j} \| ^{3}}(\mathrm{r}_{i} -\mathrm{r}_{j})$$ $$\mathrm{r}_i(0)=\mathrm{r}_{i,0}~~,~~\dot{\mathrm{r}}_i(0)=\dot{\mathrm{r}}_{i,0}$$ Where $i\in\{1,\dots ,n\}$ . When $n=1$ we just have a single stationary body. When $n=2$ things get a lot more interesting, but still the equations are easy to analyze and their behavior is easy to predict with numerical simulation - it is why we are able to predict solar ecplipses years in advance. In fact, taking the limiting case $m_2 \gg m_1$ some very precise equations for the motion of the bodies have been known for hundreds of years, namely Kepler's laws. However, when $n\geq 3$ the system becomes chaotic, with the particles exhibiting no obvious or predictable behavior. Is this because, unlike two points, one cannot in general find a line that goes through three arbitrary points? This reminds me a lot of example 1. 4: Commutative Division Algebras over $\mathbb{R}$ (Frobenius's Theorem) My shortest entry on this list, due to my extreme lack of knowledge about abstract algebra. There is (trivially) a one dimensional commutative division algrebra over $\mathbb{R}$ , namely $\mathbb{R}$ itself. There a two dimensional commutative division algebra over $\mathbb{R}$ , namely the complex numbers $\mathbb{C}$ .
But there is in fact no commutative division algebra over $\mathbb{R}$ when $n>2$ . Once again, we see that changing the dimension from $2$ to $3$ completely changes the behavior. 5: The fundamental solution of Laplace's equation We seek to solve the equation $$(\boldsymbol{\triangle}u)(\mathrm{x})=\delta(\mathrm{x})$$ Here $u:\mathbb{R}^n\to\mathbb{R}$ , $\mathrm{x}\in\mathbb{R}^n$ , and $\delta$ is Dirac's delta distribution. It can be shown that, letting $V_n=\frac{\pi^{n/2}}{\Gamma(1+n/2)}$ be the volume (where, by volume I really mean the $n$ dimensional measure) of the unit $n$ ball, the solution is $$\Phi_n:\mathbb{R}^n\setminus \{0\}\to\mathbb{R}$$ $$\Phi _{n}(\mathrm{x}) =\begin{cases}
\frac{1}{2} |\mathrm{x} | & n=1\\
\frac{1}{2\pi }\log |\mathrm{x} | & n=2\\
\frac{-1}{n( n-2) V_{n}} \ \frac{1}{|\mathrm{x} |^{n-2}} & n\geq 3
\end{cases}$$ This is actually more interesting - going from $n=1,2,3$ we start with a power law in $|\mathrm{x}|$ , then a logarithm, and then again a power law. However, once again we see a stark change in behavior when $n$ goes from $2\to 3$ . If you made it this far, thanks for reading. Consider leaving an answer giving other examples or perhaps a hand-wavy explanation.","['ordinary-differential-equations', 'abstract-algebra', 'partial-differential-equations', 'soft-question', 'differential-geometry']"
4131339,Expectation of product of jointly normal variables conditional on inequality,"X, Y and Z are jointly normal with mean zero and a positive definite covariance matrix $\Sigma$ . I know if I wanted to compute $E[X|Y+Z-X<A]$ (where A is a constant), I could do a change of variables and use a truncated normal (i.e. let $V\equiv Y+Z-X$ and $\sigma_V$ be the variance of $V$ . Then, $E[X|Y+Z-X<A]=\frac{Cov\left(X,\frac{V}{\sigma_V}\right)}{Var\left(\frac{V}{\sigma_V}\right)}\mathbb{E}\left[\frac{V}{\sigma_V}\middle|\frac{V}{\sigma_V}<\frac{A}{\sigma_V}\right]$ , which can be easily computed using a truncated standard normal). How would I approach computing $E[X^2|V<A]$ and $E[XY|V<A]$ in a similar way?","['probability-distributions', 'conditional-expectation', 'expected-value', 'probability-theory', 'probability']"
4131353,"If $a_n \neq 0$ for all natural numbers n , $\sum_{n=1}^{+\infty} a_n $ converges $\lim_{n\to+\infty} a_n/b_n =1\sum_{n=1}^{+\infty} b_n$ converges [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question If $a_n \neq 0$ for all natural numbers $n$ , $\sum_{n=1}^{+\infty} a_n $ converges $\lim_{n\to+\infty} \frac{a_n}{b_n} =1 $ then $\sum_{n=1}^{+\infty} b_n$ converges I am trying to find counterexamples to this proposition but I'm unable to do so, is it likely that it is true? Thank you for the help.","['calculus', 'sequences-and-series', 'analysis', 'real-analysis']"
4131411,Prove or disprove: $\lim_{x→0} f(x) = 0$,"Let $f : \mathbb{R} → \mathbb{R}$ be a function such that for any $r ∈
 \mathbb{R}$ , we have $$\lim_{n\rightarrow \infty }f\left ( \frac{r}{n} \right )= 0$$ Prove or disprove: $\lim_{x→0} f(x) = 0$ My claim: The statement is true because $r ∈ \mathbb{R}$ . In other words, I think since we can pick any $r ∈
 \mathbb{R}$ , with $\lim_{n\rightarrow \infty} \frac{r}{n}$ , we can cover any neighbor of $0$ . (Either rational or irrational numbers near $0$ ). But, since the reasoning does not seem strong enough for me, I also thought of any counterexample of $f$ . So, I tried to construct a discontinuous function at $x=0$ , but I couldn't think of any counterexample. Is the statement really true? Then, how should I change (or improve) my reasoning to write it in clear mathematical terms? If the statement is false, what should I notice to have a counterexample?","['limits', 'continuity', 'real-analysis']"
4131426,Non-existence of a holomorphic map on the disc,"Is there a function $f$ that is holomorphic on the punctured unit disc such that $f^{\prime}$ has a pole of order $1$ ? My answer is: No. The following post has an answer to my question. Show that there is no function $f$ that is analytic in punctured unit disc and $f'$ has a simple pole at $0$. Here's my reasoning: Let $f$ be such a function. Then $f^{\prime} (z) = \frac{g(z)}{z}$ for some holomorphic function $g$ on the pictured disc and $g(0)$ is NOT equal to $0.$ Now, taking integral both sides along the boundary of a small disc that is contained in the unit disc, we have $0 = g(0),$ which is a contradiction. Is this a valid argument? Any help would be appreciated.","['complex-analysis', 'analyticity', 'laurent-series']"
4131432,Proof that $\mathbb{R}^2 \cong \mathbb{R}$,"I'm trying to prove that $\mathbb{R}^2$ and $\mathbb{R}$ have the same cardinality. Here is my attempt. I will be taking the Schröder-Bernstein theorem for granted. Define the map $f: \mathbb{R} \to \mathbb{R}^2$ sending $x \longmapsto (x,0)$ . I claim that $f$ is an injection. Indeed, if $f(x) = f(y)$ for $x,y \in \mathbb{R}$ , then $(x,0) = (y,0)$ , so $x = y$ . Therefore, $|\mathbb{R}| \leq |\mathbb{R}^2|$ . By the Schröder-Bernstein theorem, it suffices to prove that $|\mathbb{R}^2| = |\mathbb{R}|$ . Lemmma 1 : $\mathbb{R} \cong (0,1)$ . Proof of Lemma 1. Notice that $\tan(x)$ is a bijection from $\left(-\frac{\pi}{2}, \frac{\pi}{2}\right) \to \mathbb{R}$ since $\tan$ has period $\pi$ ; furthermore, the existence of the $\arctan$ function guarantees that the function is surjective, hence bijective. Notice that there is a one-to-one correspondence $\left(0,1\right) \to \left(- \frac{\pi}{2}, \frac{\pi}{2}\right)$ sending $x \longmapsto \left(x - \frac{1}{2}\right)\pi$ . Composing bijections gives a bijection $\left(0,1\right) \to \mathbb{R}$ . Lemma 2 : Given sets $A, B, C,D$ such that $A \cong C$ and $B \cong D$ , we have $A \times B \cong  C \times D$ . Proof of Lemma 2. Since $A \cong C$ and $B \cong D$ , there exist bijections $f: A \stackrel{\sim}{\to} C$ and $g: B \stackrel{\sim}{\to} D$ . Then define the map $$ 
h: \underset{(a,b)}{A \times B} \underset{\longmapsto}{\to} \underset{(f(a), g(b))}{C \times D}.
$$ Given $(a_1, b_1), (a_2, b_2) \in A \times B$ for which $h(a_1, b_1) = h(a_2, b_2)$ , we have $(f(a_1), g(b_1)) = (f(a_2), g(b_2))$ . So $f(a_1) = f(a_2)$ and $g(b_1) = g(b_2)$ . The former implies $a_1 = a_2$ by injectivity of $f$ , while the latter implies $b_1 = b_2$ by injectivity of $g$ . So $(a_1, b_1) = (a_2, b_2)$ , so $h$ is injective. Given $(c,d) \times C \times D$ , by surjectivity of $f$ , there exists $a \in A$ such that $f(a) = c$ ; by surjectivity of $g$ , there exists $b \in B$ such that $g(b) = d$ . We then have $$ 
(c,d) = (f(a), g(b)) = h(a,b),
$$ so $h$ is surjective, hence bijective, so $A \times B \cong C \times D$ . Corollary. It suffices to prove that $(0,1) \times (0,1) \cong (0,1)$ . Proof. We know $\mathbb{R} \cong (0,1)$ by Lemma 1. By Lemma 2, that implies that $\mathbb{R} \times \mathbb{R} \cong (0,1) \times (0,1)$ . So if we prove $(0,1) \times (0,1) \cong (0,1)$ , we have $$ 
\mathbb{R} \times \mathbb{R} \cong (0,1) \times (0,1) \cong (0,1) \cong \mathbb{R},
$$ which is the intended result. Claim. $(0,1) \times (0,1) \cong (0,1)$ . Proof of Claim. We use the Schröder-Bernstein theorem. There is an obvious injection $(0,1) \times (0,1) \times (0,1)$ sending $x \longmapsto \left(x, \frac{1}{2}\right)$ . This is an injection regardless of whether we allow an infinite string of $9$ 's. Concordantly, we can define an injection $(0,1) \times (0,1) \to (0,1)$ as follows. Given $(x,y) \in (0,1) \times (0,1)$ , choose a decimal expansion for $x,y \in (0,1)$ that does not contain an infinite string of nines, which we know to be unique. Write $$ 
x = 0.x_1 x_2 x_3 \ldots \\
y = 0.y_1 y_2 y_3 \ldots 
$$ Then define $$ 
f(x,y) = 0.x_1 y_1 x_2 y_2 x_3 y_3 \ldots 
$$ Since neither $x,y$ contained an infinite string of nines, $f(x,y)$ likewise does not. I claim that $f$ is an injection. Indeed, suppose that $f(a,b) = f(c,d)$ for $(a,b), (c,d) \in (0,1) \times (0,1)$ . Defining the decimal expansions for $a,b,c,d$ similarly, we then have $$ 
0.a_1 b_1 a_2 b_2 a_3 b_3 \ldots = 0.c_1 d_1 c_2 d_2 c_3 d_4 \ldots.
$$ Since neither of these expressions contain an infinite string of $9$ 's, we can equate corresponding digits, so $a_i = c_i$ for all $i \geq 1$ and $b_i = d_i$ for all $i \geq i$ , so $a = c$ and $b = d$ , so $(a,b) = (c,d)$ , so $f$ is injective. By the Shroder-Bernstein theorem, $(0,1) \times (0,1) \cong (0,1)$ . -- How does this proof look? The only fact I am not totally sure I understand fully is why the decimal expansion not terminating in a string of nine's is unique. I believe we can assert that every element of $(0,1)$ has exactly two decimal expansions. What is the canonical way to go about showing this?","['elementary-set-theory', 'solution-verification']"
4131473,Finding isometries of a manifold $M$ which has Euclidean geometry,"How do you find the isometries of a manifold $M$ with metric $ds^2=\frac{du^2}{u^2}+\frac{dv^2}{v^2}?$ Observe that $ds^2$ is of the form $ds^2=g(u)du^2+f(v)dv^2.$ This means that the metric must describe a Euclidean geometry. In fact with more work we can show that $ds^2$ is a transported metric from the familiar $\Bbb R^2$ to $M:=\Bbb R^2_+$ via $\exp.$ I calculated the pullback via an immersion here: \begin{align}
 (f^{-1})^* (dx^2 + dy^2) &=( d((f^{-1})^*x))^2 + ( d((f^{-1})^*y))^2 \\
&= (d (x\circ f^{-1}))^2 +(d (y\circ f^{-1}))^2 \\
&= (d \log u)^2 +(d\log v)^2 \\
&= \left( \frac{1}{u} du\right)^2 +  \left( \frac{1}{v} dv\right)^2 \\
&= \frac{1}{u^2} du^2 + \frac{1}{v^2} dv^2.
\end{align} Now I know that the isometries of $M$ are basically the isometries of $\Bbb R^2$ in disguise. I figured out that translation in $\Bbb R^2 \implies (x+a,y+b)$ has analogue in $M \implies (ax,by).$ Rotation in $\Bbb R^2 \implies
\begin{bmatrix}
\cos \theta & -\sin \theta \\  
\sin \theta & \cos \theta \\
\end{bmatrix}(x,y)$ has what analogue in $M?$ I couldn't figure this one out.","['euclidean-geometry', 'analytic-geometry', 'isometry', 'differential-geometry']"
4131549,Finding $\lim_{n\to\infty} \frac{((1!)(2!)(3!).....(n!))^{1/n^2}}{n^{\alpha}}$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question if $\alpha ,\beta$ belong to $\mathbb{R}$ , $\beta$ is not equal to zero, $n$ belong to $\mathbb{N}$ and $$\lim_{n\to\infty} \frac{((1!)(2!)(3!).....(n!))^{1/n^2}}{n^{\alpha}} = \beta$$ then
please help me. Any help will be appreciated. Thanks
I think this can be solved using Squeeze theorem but i m not able to apply it ....
how to apply and solve the problem
how to apply and solve the problem
i tried using different technique but fail",['limits']
4131567,How many solution has this equation? $x_1 + x_2 + x_3 + x_4 = 32$,"$x_1 + x_2 + x_3 + x_4 = 32$ and $x_1 > x_2 > x_3 > 0$ and $0<x_4\le25$ . There are how many solutions for this equation? I was trying to enumerate the solutions one-by-one but that didn't work my try: so from this $x_1 > x_2 > x_3 > 0$ we have $x_3\ge1 , x_2\ge2, x_1\ge3$ and from $0<x_4\le25$ we have $x_4\ge1$ so we can write : $(x_1-3)+(x_2-2)+(x_3-1)+(x_4-1)=32-3-2-1-1=25$ .
then ${25+3 \choose 3}={28 \choose 3}$ . because of $x_4\le25$ we have ${28 \choose 3}-1$ .",['combinatorics']
4131623,Galois module theory: from global to local,"Let $L/\mathbb{Q}$ be a finite Galois extension with Galois group $G$ . It is well known that the ring of integers $\mathcal{O}_L$ is free over its associated order $\mathfrak{A}_{L/\mathbb{Q}}=\{x\in \mathbb{Q}[G]\mid x\mathcal{O}_L\subseteq \mathcal{O}_L\}$ if $G$ is abelian (Leopoldt, 1959); $G$ is dihedral of order $2p$ , where $p$ is a rational prime (Bergé, 1972); $G$ is the quaternion group of order $8$ , and the extension is wild (Martinet, 1972). In some other papers, I have found written that also the local counterparts are true ( $L/\mathbb{Q}_p$ finite with the same Galois group as before), and it seems that the authors suggest that these results are naturally implied by the global ones. But while they seem ""folklore"", these implication are not immediate to me. A couple of observations (I wish to thank Fabio Ferri for the precious discussion). It seems to me that only the number fields case is considered in the papers. Probably the key is to repeat the proof as it is for the $p$ -adic case; for example, this works in the Martinet's case. There are other ways to get these results, like for example using Lettl's work on absolutely abelian extensions for Leopoldt's local case, or realise local extensions as completion of global ones with the same Galois group (here $p\ne 2$ , we refer to Henniart, 2001). But I am interested in knowing if there is a ""direct way"". Summarising, my question is: is it immediate that the global cases imply the local cases? (More generally, a question could be: if every Galois extension of number fields with a certain type of Galois group admits freeness of the ring of integers over the associated order, then the same also holds for every Galois extension of local fields with the same Galois group?)","['galois-theory', 'number-theory', 'algebraic-number-theory', 'local-field']"
4131645,Find prime numbers with given property [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I've been working in the following number theory problem: find all prime numbers $p,q,r$ such that $$p^2+q^2+r^2-1$$ is a perfect square. Does anyone has some hint for the problem? Thanks in advance.","['number-theory', 'prime-numbers']"
4131781,Multiplying or adding constants within $P(X \leq x)$?,"Is it always true, that, for a positive constant $c$ , we have $P(X \leq x) = P(c X \leq cx)$ for some continuous random variable $X$ ? Further is it always true that $P(X + c \leq x + c) = P(X \leq x)$ ? If so, what governs the equality?",['probability']
4131809,difference between quotient rule and product rule,"Product rule : $$\frac{d}{dx} \big(f(x)\cdot g(x)\big)=f'(x)\cdot g(x)+f(x)\cdot g' (x)$$ Quotient rule : $$\frac{d}{dx} \frac{f(x)}{g(x)}=\frac{f'(x)\cdot g(x)-f(x)\cdot g'(x)}{[g(x)]^2}$$ Suppose, the following is given in question. $$y=\frac{2x^3+4x^2+2}{3x^2+2x^3}$$ Simply, this is looking like Quotient rule. But, if I follow arrange the equation following way $$y=(2x^3+4x^2+2)(3x^2+2x^3)^{-1}$$ Then, we can solve it using Product rule. As I was solving earlier problems in a pdf book using Product rule. I think both answers are correct. But, my question is, How does a Physicist and Mathematician solve this type question? Even, is it OK to use Product rule instead of Quotient rule in University and Real Life?","['calculus', 'derivatives']"
4131840,How do probability measures on $\mathbb{Z}$ look like?,"Let $\mu: \mathcal{P}(\mathbb{Z}) \rightarrow [0,1]$ be a finitely additive $\mathbb{Z}$ -invariant probability measure on $\mathbb{Z}$ . Such measures exist because $\mathbb{Z}$ is amenable and one can indeed describe examples of such measures as ultralimits of sequences of asymptotic densities computed with respect to a Følner sequence; for example, see here and here . I have two questions which I hope are elementary, but I was not able to figure out. Does $\mu$ have to generalize the notion of asymptotic density of a subset of integers? In other words, does $\lim_{n \rightarrow \infty} \frac{|A \cap [-n,n]|}{2n+1}=r$ imply that $\mu(A)=r$ ? The answer is clearly yes for infinite arithmetic progressions by finite additivity of $\mu$ . On the other hand, sets of integers with asymptotic density can look very different than such sets, so I do not see how to extend the result to these sets. $\mu$ is necessarily not continuous from above or below; otherwise it would be a $\sigma$ -additive measure as shown, for example, here . Indeed, we have that $\lim_{n \rightarrow \infty} \mu(\mathbb{Z}-[-n,n]) = 1 \neq 0 = \mu\left(\bigcap_{n \in \mathbb{N}} \mathbb{Z}-[-n,n]\right)$ . My question this time is a bit vague. Is it possible to give a (useful) sufficient condition for a class of subsets of $\mathbb{Z}$ that guarantees the continuity of $\mu$ from above and below for sequences in this class? This would allow us to compute the probability of infinitely many events happening simultaneously in certain cases. Perhaps I should mention the motivation behind this question. It is often written that ""there is no way to choose an integer at random"" due to the lack of a $\sigma$ -additive measure on $\mathbb{Z}$ which chooses each integer with equal probability. But there are finitely additive such measures. So why don't we use these measures to model the situations where we need to choose an integer at random? If we are to do that, such measures need to satisfy certain intuitive properties; for example, it needs to generalize asymptotic density (hence my first question) and it should satisfy some basic identities that allow us to play with it (hence my second question). Some more motivation : Let me also add how I ended up thinking about this in the first place. It is well-known that the probability that two randomly chosen integers from $\{1,2,\dots,n\}$ are coprime goes to $6/\pi^2$ as $n \rightarrow \infty$ . Let us try formalize the idea given in this Wikipedia article using such a measure $\mu$ on $\mathbb{Z}\times\mathbb{Z}$ and try to exactly prove that ""two randomly chosen integers are coprime with probability $6/\pi^2$ ."" Let $\mu: \mathcal{P}(\mathbb{Z}\times\mathbb{Z}) \rightarrow [0,1]$ be a finitely additive $\mathbb{Z}\times\mathbb{Z}$ -variant probability measure. For each prime $p$ , set $A_p=\{(m,n) \in \mathbb{Z}\times\mathbb{Z}: p \nmid a \text{ or } p \nmid b\}$ . Then, finite additivity implies that $\mu(A_p)=1-\frac{1}{p^2}$ for any prime $p$ . It follows that $$\mu(\{(m,n) \in \mathbb{Z}\times\mathbb{Z}: gcd(m,n)=1\})=\mu\left(\bigcap_{p \text{ prime}} A_p\right) \leq \prod_{p \text{ prime}} \mu(A_p)=\frac{6}{\pi^2}$$ The lack of continuity of $\mu$ from above prevents us to conclude equality as it may be that $\prod_{p \text{ prime}} \mu(A_p)=\lim_{n \rightarrow \infty}\mu(A_2 \cap A_3 \cap \dots \cap A_{p_n}) \neq \mu\left(\bigcap_{p \text{ prime}} A_p\right)$ . This is partly why I am interested in the second question; I'd be happy to see if this were actually equality, which, we know in our hearts, must be true! Clearly the specific problem here regarding coprimality is just a distraction. What I really want to see is if ""taking limits of probabilities obtained on the set $\{1,2,\dots,n\}$ "" actually corresponds to something meaningful in this finitely additive setting.","['amenability', 'measure-theory']"
4131869,"In $\triangle{ABC}$, $\angle ABC=45^ \circ$. $X$ is a point on $BC$ such that $BX=\frac{1}{3}BC$ and $\angle AXC=60^ \circ$. Find $\angle ACB$.","Problem In $\triangle{ABC}$ , $\angle ABC=45^ \circ$ . $X$ is a point on $BC$ such that $BX=\frac{1}{3}BC$ and $\angle AXC=60^ \circ$ . Find $\angle ACB$ . The problem looks easy. Though I couldn't solve it in an efficient way. Finally I solved it using trigonometry. Trig solution Let $BX = a$ units, then $BC = 3a$ and $XC = 3a-a= 2a$ units. $\angle AXC =60^ \circ$ and $\angle ABC= 45^ \circ$ , then $\angle BAX= 60^ \circ -45^ \circ = 15^ \circ$ . Applying sine rule in $\triangle ABX$ , $$\frac{BX}{\sin \angle BAX} = \frac{AX}{\sin \angle ABC}$$ $$\implies \frac{a}{\sin 15°} = \frac{AX}{\sin 45°} \tag{1}$$ In $∆AXC$ , let $\angle ACB = \theta$ , then $\angle XAC = (120 - \theta)$ and by sine rule, $$\frac{XC}{\sin \angle XAC} = \frac{AX}{\sin \angle ACB}$$ $$\implies \frac{2a}{\sin (120 - \theta)} = \frac{AX}{\sin \theta} \tag{2}$$ Dividing $(1)$ by $(2)$ , $$\frac{\sin (120-\theta)}{2\sin 15^ \circ}= \frac{\sin \theta}{\sin 45°}$$ $$\implies 2\sin 15°\cdot\sin \theta = \sin 45°\cdot\sin (120-\theta)$$ $$\implies \frac{\sqrt{3}–1}{\sqrt 2}.\sin \theta = \frac{1}{\sqrt 2}.(\sin120°.\cos \theta - \cos 120°.\sin \theta).$$ $$\implies (\sqrt{3}–1).\sin \theta = \frac{\sqrt 3}{2}.\cos \theta + \frac{1}{2}.\sin \theta$$ $$\implies \tan \theta= 2+\sqrt 2$$ $$\implies \theta=75^ \circ$$ Thus, $\angle ACB = 75°$ . This solution is impossible without knowing the values of $\sin 15^ \circ$ and $\tan 75^ \circ$ . And I find trigonometry boring. So, can this problem be solved in some other ways?","['contest-math', 'euclidean-geometry', 'geometry', 'solution-verification', 'trigonometry']"
4131873,How to obtain the bound $\lvert R_{n}(\omega)\rvert \leq \beta d(2^{n+1})^{d-1}$ in the Ising Model,"From Chapter 3, page 85 of Friedli and Velenik, Statistical Mechanics of Lattice Systems: A classical mathematical introduction https://www.unige.ch/math/folks/velenik/smbook/Ising_Model.pdf The proof of the existence of the free energy in the Ising model is given. In this proof we initially look at the ""cubes"" $D_{n}:=\{1,...,2^{n}\}^{d}$ and let $n$ pass through $\mathbb N$ . Note that by the neat construction, we get that $D_{n+1}$ contains $2^{d}$ ""cubes"" of $D_{n}$ Now let inverse temperature $\beta$ and magnetic field $h$ be given, and consider the following decomposition of the hamiltonian on the cube $D_{n+1}$ : $\mathcal{H}_{D_{n+1}}=\sum\limits_{i=1}^{2^{d}}\mathcal{H}_{D_{n}}^{(i)}+R_{n}$ where $\mathcal{H}_{D_{n}}^{(i)}$ is the Hamiltonian on $D_{n}^{(i)}$ , i.e. the i-th cube of the decomposition and further where $R_{n}$ is the interaction between particles from separate cubes. Note the hamiltonian in the Ising model is defined in the following way: $\mathcal{H}_{\Lambda}(\omega):=-\beta\sum\limits_{i,j\in \Lambda\; \lvert i-j\rvert = 1}\omega(i)\omega(j)-h\sum\limits_{i\in \Lambda}\omega(i)\;\;$ where $\omega \in \{-1,+1\}^{\Lambda}$ and $\Lambda \subset \mathbb Z^{d}$ . Now my problem is that in the proof, they claim that the bound $\lvert R_{n}(\omega)\rvert \leq \beta d(2^{n+1})^{d-1}$ works. I assume that the bound comes about in the following way: $\beta \times (\text{number of pairs that can interact with one another from separate cubes)}$ "" $\text{number of pairs that can interact with one another from separate cubes}$ "" actually means that the pairs that are nearest neighbours but not in the same cube. I do not see (despite help with 3d drawings) why "" $\text{number of pairs that can interact with one another from separate cubes}$ "" $=d(2^{n+1})^{d-1}$ . Is there a mathematical proof behind this or explanation?","['euclidean-geometry', 'geometry', 'real-analysis', 'physics', 'probability-theory']"
4131875,Relationship between $\text{Pois}(\mu)$ and $\text{Pois}(\ell)$. Choose $\mu=\ell\delta_1$?,"Let $\mu$ denote a finite measure on $(\mathbb{R}, \mathcal{B}(\mathbb{
R}))$ . The General Poisson measure w.r.t $\mu$ is defined as $$
\text{Pois}(\mu)(A) \equiv \exp({-\mu(\mathbb{R})})\sum_{k=0}^\infty \frac{\mu^{*k}(A)}{k!}
$$ for every $A \in \mathcal{B}(\mathbb{
R})$ . Here $\mu^{*k}$ denotes the convolution of $\mu$ with itself $k$ times. The Poisson distribtion with rate $\ell > 0$ is defined as $$
\text{Pois}(\ell)(A) \equiv \exp({-\ell})\sum_{k \in A} \frac{\ell^k}{k!}
$$ for every $A \in \mathcal{P}(\mathbb{N})$ . My question is how these two are related? My gut is telling me it is something along the lines of choosing $\mu=\ell\delta_1$ . Then $\exp({-\ell\delta_1(\mathbb{R})})=\exp({-\ell})$ as we want. However it seems to me that even for $k \in A$ we have $\mu^{*k}(A)\neq \ell^k$ . Also it would have to be $\mu^{*k}(A) = 0$ for $k \notin A$ . How to fix this? Or is the chosen $\mu$ wrong?","['probability-distributions', 'probability-theory']"
4131916,Can we almost cover any shape in the plane by disjoint/tangent disks of prescribed radii?,"Let $(a_n)_{n \in \mathbb{Z}}$ be some given sequence of positive numbers, such that $\lim_{n \to -\infty}  a_n=0,\lim_{n \to \infty}  a_n=+\infty$ . Let $\Omega \subseteq \mathbb{R}^2$ be a bounded, connected, open subset, with Lipschitz boundary. Let $\epsilon >0$ . Does there exist a countable collection of closed disks $B(x_k,r_k)$ with the following properties: For each $k$ , $B(x_k,r_k) \subseteq \Omega$ and $r_k \in \{a_n\}$ . For every two distinct disks, either their intersection is empty, or they are tangent to each other. (The interiors of the disks do not intersect in any case). $\Omega \setminus\cup_{k} B(x_k,r_k)$ has Lebesgue measure less then $\epsilon$ . Here $B(x_k,r_k)$ denotes the closed Euclidean disk with radius $r_k$ , centered around $x_k$ . It's possible that we only need the assumption $\lim_{n \to -\infty}  a_n=0$ . I am fine with assuming higher regularity of the boundary $\partial \Omega$ , say $C^1$ . Comment: I am not sure if this cane be done for any such sequence $a_n$ . One could suppose e.g. that perhaps certain bounds on the ratios between the different $a_n$ are needed. Perhaps we could start with an easier question: If there was no restriction on the radii at all, could such a disjoint\tangent cover be constructed? This question resembles in a sense Apollonian gaskets .","['euclidean-geometry', 'measure-theory', 'real-analysis', 'plane-geometry', 'differential-geometry']"
4131973,Find a line that crosses multiple line segments,"I'm looking for a formula, or algorithm, that would allow me to figure out if there's a line that cross multiple line segments (those are always parallel to the y axis), and if there is, the equation for that line. For example, in the following diagram, I'm looking for any line that touches the 4 vertical bar, such as the red line shows. It can touch the bars anywhere. I can figure out how to do it with 3 line segments, but anything over that I have no idea. I'm also not a mathematician so I probably got most terminology wrong. EDIT: a possible approach is recursive, by drawing the bow-tie of possible lines for the left-most two segments, and seeing if the third segment touches the bow-tie. If so, create a narrower bow-tie of acceptable lines for the first three, and so on.","['geometry', 'algorithms']"
4131985,"If $f: \mathbb{R}^{n} \to \mathbb{R}$ satisfies $|f(x)| \leq ||x||^{2}$, then, $f$ is differentiable at the origin.","For a function $f: \mathbb{R}^{n} \to \mathbb{R}$ , I want to prove that if $f$ satisfies $|f(x)| \leq ||x||^{2}$ . Then, $f$ is differentiable at the origin. So far, I try to reduce this problem to the case $n=2$ , that is $f: \mathbb{R}^{2} \to \mathbb{R}$ is such that $|f(x_{1},x_{2})| \leq x_1^{2}+ x_2^{2}$ . And according to Wikipedia , I need to prove that there exists a linear map $J:\mathbb{R}^2 \rightarrow \mathbb{R}$ such that $$\lim_{(x_{1},x_{2}) \rightarrow 0}\frac{|f(x_1,x_2)-f(0,0)-J(x_1,x_2)|}{\|(x_1,x_2)\|_{\mathbb{R}^2}}=0$$ . So by taking $J: \mathbb{R}^{2} \to \mathbb{R}$ equal to the linear map zero I got $$\frac{|f(x_1,x_2)-f(0,0)|}{\sqrt{x_1^{2}+x_2^{2}}}.$$ But as $f(0,0)=0$ by the first answer we got that $$\frac{|f(x_1,x_2)-f(0,0)|}{\sqrt{x_1^{2}+x_2^{2}}}=\frac{|f(x_1,x_2)|}{\sqrt{x_1^{2}+x_2^{2}}} \leq \frac{(x_{1}^{2} + x_{2}^{2})}{\sqrt{x_1^{2}+x_2^{2}}}.$$ But limit of the last term of this inequality is zero as $(x_{1}, x_{2})$ aproaches to zero. And in this step is where maybe I need to use my hypothesis $|f(x_{1},x_{2})| \leq x_1^{2}+ x_2^{2}$ . But Im run out of ideas about how to proceed from here. Also Im not sure if the trick changes for $f: \mathbb{R}^{n} \to \mathbb{R}$ ?","['multivariable-calculus', 'calculus', 'derivatives', 'real-analysis']"
4132007,Large gap Sparse Rulers,"A marked integer ruler has length $n$ and integer marks from $0$ to $n$ .  It can measure any integer length from $1$ to $n$ .  The two ends of the ruler, $0$ and $n$ are considered marks. A sparse ruler can also measure any integer length from $1$ to $n$ , but has a minimal number of marks.  For example, for length $24$ , the nine marks at $(0, 1, 13, 14, 16, 18, 20, 22, 24)$ suffice for measuring all distances from $1$ to $24$ . For any $n$ , a sparse ruler of length $n$ can be constructed with $E+\lceil {\sqrt {3n+{\tfrac {9}{4}}}}\rfloor $ marks, $E$ being the the $0$ or $1$ excess of the ruler, with OEIS A326499 giving the best known excess values, known to be true up to 213. In the example above, the ruler has a gap longer than a third of the length of the ruler, making it a large gap sparse ruler. Here are more examples of sparse rulers with large gaps: The length $322$ large gap sparse ruler is currently the longest known example. $(0, 1, 2, 3, 14, 15, 16, 17, 22, 151, 165, 170, 175, 180, 189, 198, 207, 216, 225, 234, 243, 252, 261, 270, 279, 288, 297, 306, 310, 314, 318, 322)$ Can anyone find a length $n>322$ ruler with $1+\lceil {\sqrt {3n+{\tfrac {9}{4}}}}\rfloor $ marks that can measure all lengths from $1$ to $n$ and has a gap of length greater than $n/3$ ? Slightly harder: Is there a longer large gap sparse ruler?  More explicitly: Can anyone find a length $n>322$ ruler with $E+\lceil {\sqrt {3n+{\tfrac {9}{4}}}}\rfloor $ marks that can measure all lengths from $1$ to $n$ and at least gap of length greater than $n/3$ , and $E$ being the the $0$ or $1$ excess value for $n$ listed in OEIS A326499 ? The second largest of length 262 has marks at these locations: $(0, 1, 2, 3, 4, 5, 6, 7, 131, 136, 141, 146, 154, 162, 170, 178, 186, 194, 202, 210, 218, 226, 234, 242, 250, 253, 256, 259, 262)$ , representable in the five part gap-repetition form $1^7 124^1 5^3 8^{13} 3^4$ . This is the longest known sparse ruler with a five part gap-repetitions.  The longest known with three part gap-repetitions has length 69 with $1^6 8^7 7^1$ . There are infinite sparse rulers with six or more gap-repetitions, the Wichmann-like rulers . EDIT:  Re-running some code managed to improve the best known large gap sparse ruler slightly. The value $344$ is now the value to beat. Here are the marks for the pictured sparse rulers. {    
 {0,1,2,3,4,5,6,7,8,141,153,164,170,178,187,196,205,214,223,232,241,250,259,268,277,286,295,296,301,308,316,324},
 {0,1,2,3,4,5,6,7,8,142,148,154,165,168,178,187,196,205,214,223,232,241,250,259,268,277,286,295,303,311,318,324},
 {0,1,4,5,8,11,12,15,16,151,164,171,177,186,195,204,213,222,231,240,249,258,267,276,285,294,303,305,308,319,322,324},
 {0,1,4,17,20,21,30,39,48,57,66,75,84,93,102,111,120,129,138,147,153,161,172,307,309,311,312,314,317,319,322,324},
 {0,1,2,6,7,8,12,13,14,150,153,170,173,182,191,200,209,218,227,236,245,254,263,272,281,290,299,305,307,309,321,324},
 {0,1,3,4,5,7,8,11,15,151,159,167,175,184,193,202,211,220,229,238,247,256,265,274,283,292,301,302,312,314,322,324},
 {0,1,2,6,7,8,12,13,14,150,154,170,173,182,191,200,209,218,227,236,245,254,263,272,281,290,299,305,307,310,321,324},
 {0,1,4,5,7,8,11,12,15,151,159,167,175,184,193,202,211,220,229,238,247,256,265,274,283,292,301,302,312,314,322,324},
 {0,1,4,5,7,8,11,12,15,151,161,166,172,177,186,195,204,213,222,231,240,249,258,267,276,285,294,303,307,309,322,324},
 {0,1,4,5,6,7,8,11,12,151,161,166,172,177,186,195,204,213,222,231,240,249,258,267,276,285,294,303,307,309,322,324},
 {0,1,2,3,8,13,14,15,16,159,164,169,178,187,196,205,214,223,232,241,250,259,268,277,286,295,299,303,316,319,320,324},
 {0,1,2,3,4,5,6,7,8,139,150,158,165,175,184,193,202,211,220,229,238,247,256,265,274,283,292,297,305,313,321,325},
 {0,1,2,3,4,5,6,7,8,144,154,165,170,179,188,197,206,215,224,233,242,251,260,269,278,287,296,300,308,315,320,325},
 {0,1,2,6,7,8,12,13,14,151,154,171,174,183,192,201,210,219,228,237,246,255,264,273,282,291,300,306,308,310,322,325},
 {0,1,2,3,4,5,6,7,8,150,162,173,179,187,196,205,214,223,232,241,250,259,268,277,286,295,304,313,314,319,326,334,342},
 {0,1,2,3,4,5,6,7,8,151,157,163,174,177,187,196,205,214,223,232,241,250,259,268,277,286,295,304,313,321,329,336,342},
 {0,1,4,5,8,11,12,15,16,160,173,180,186,195,204,213,222,231,240,249,258,267,276,285,294,303,312,321,323,326,337,340,342},
 {0,1,4,17,20,21,30,39,48,57,66,75,84,93,102,111,120,129,138,147,156,162,170,181,325,327,329,330,332,335,337,340,342},
 {0,1,2,6,7,8,12,13,14,159,162,179,182,191,200,209,218,227,236,245,254,263,272,281,290,299,308,317,323,325,327,339,342},
 {0,1,3,4,5,7,8,11,15,160,168,176,184,193,202,211,220,229,238,247,256,265,274,283,292,301,310,319,320,330,332,340,342},
 {0,1,2,6,7,8,12,13,14,159,163,179,182,191,200,209,218,227,236,245,254,263,272,281,290,299,308,317,323,325,328,339,342},
 {0,1,4,5,7,8,11,12,15,160,168,176,184,193,202,211,220,229,238,247,256,265,274,283,292,301,310,319,320,330,332,340,342},
 {0,1,4,5,7,8,11,12,15,160,170,175,181,186,195,204,213,222,231,240,249,258,267,276,285,294,303,312,321,325,327,340,342},
 {0,1,4,5,6,7,8,11,12,160,170,175,181,186,195,204,213,222,231,240,249,258,267,276,285,294,303,312,321,325,327,340,342},
 {0,1,2,3,8,13,14,15,16,168,173,178,187,196,205,214,223,232,241,250,259,268,277,286,295,304,313,317,321,334,337,338,342},
 {0,1,2,3,4,5,6,7,8,148,159,167,174,184,193,202,211,220,229,238,247,256,265,274,283,292,301,310,315,323,331,339,343},
 {0,1,2,3,4,5,6,7,8,153,163,174,179,188,197,206,215,224,233,242,251,260,269,278,287,296,305,314,318,326,333,338,343},
 {0,1,2,6,7,8,12,13,14,160,163,180,183,192,201,210,219,228,237,246,255,264,273,282,291,300,309,318,324,326,328,340,343},
 {0,1,2,3,4,5,6,7,8,148,160,172,176,183,193,202,211,220,229,238,247,256,265,274,283,292,301,310,319,324,332,339,344},
 {0,1,2,6,7,8,12,13,14,158,160,179,182,186,195,204,213,222,231,240,249,258,267,276,285,294,303,312,321,324,327,341,344},
 {0,1,2,6,7,8,12,13,14,161,164,181,184,193,202,211,220,229,238,247,256,265,274,283,292,301,310,319,325,327,329,341,344},
 {0,1,2,6,7,8,12,13,14,161,163,180,183,192,201,210,219,228,237,246,255,264,273,282,291,300,309,318,324,325,328,341,344}
 }","['recreational-mathematics', 'combinatorics', 'discrete-mathematics']"
4132042,Can existence of cartesian products be proven without power set axiom?,"Can it be proven that for any two sets $S$ and $T$ , the cartesian product $S \times T$ exists, without using the power set axiom? (But using the other axioms of ZFC is fine)",['elementary-set-theory']
4132051,Parameterization of the Hawaiian Earring,"I've been trying to find a way to parameterize the Hawaiian Earring, i.e. to find a parametric function $[0,1]\to\mathbb{H}$ . I  would like this function to be continuous and differentiable. So far I only know that I can express a circle of radius $\frac{1}{n}$ for $n\in\mathbb{N}$ as $$t\mapsto \frac{1}{n}(\cos(2t\pi)+1,\sin(2t\pi))$$ but am struggling with finding a way to somehow cover all $n$ 's with just one function. I think that I should somehow express $n$ as $n(t)$ , which would be a step function on smaller and smaller parts of the $[0,1]$ interval, but am very much stuck on how to do such a thing. Would this kind of approach work for this? If so, any hints on how to proceed? Or should I be thinking about this in some other way? Thanks for any help.","['differential-geometry', 'real-analysis']"
4132060,Compute number of regular polgy sides to approximate circle to defined precision,"I am trying to approximate a circle with a regular polygon for a drawing program. I would like to compute how many sides are needed for a regular polygon to approximate a circle of radius $R$ such that at the point where the regular polygon is furthest from the true circle it is less than some specified tolerance $D$ . It seems like there should be a clear solution to this, but I have no idea where to begin. Thanks for taking a look at my question!",['geometry']
4132085,"A regular tetrahedron can be dissected into $1,2,3,4,6,8,12,$ or $24$ congruent pieces. Is this it?","By placing a tetrahedron on a face and making vertical cuts centered at the ""top"" vertex, it is easy to dissect the tetrahedron into $1, 2, 3,$ or $6$ congruent pieces. By cutting the tetrahedron into four identical pyramids meeting at the center, one for each face, we can further subdivide these pyramids to yield a total of $4, 8, 12,$ or $24$ congruent pieces. However, it's not obvious to me how to do anything more interesting than this in a way that yields dissections into other numbers of pieces - unlike the triangle, which can be subdivided into many smaller equilateral triangles, the tetrahedron does not decompose into congruent smaller regular tetrahedra, so there is no natural way to 'bootstrap' these constructions to higher numbers of pieces. In particular, I'm most interested in the question of whether a tetrahedron can be dissected into arbitrarily high numbers of pieces. Some options that come to mind: Some of the pieces resulting from one of the above dissections might have a further dissection I haven't thought of. Polyforms on the tetrahedral-octahedral honeycomb could potentially work to tile a tetrahedron of large side length, just as some polyominoes tile a square and some polyiamonds tile a larger triangle. (However, since the ratio of tetrahedra to octahedra in a given finite tetrahedral portion of the honeycomb monotonically decreases with the side length of said tetrahedron, any given polyform will work with at most one scale.) See also this question on MSE . Perhaps something like Dehn invariants could to used to attempt a proof of impossibility, somehow? I'd be skeptical, though.","['dissection', 'tiling', 'geometry', '3d']"
4132132,"Global maxima and minima of $f(x,y)=x^2 + y^2 + \beta xy + x + 2y$","I am self-learning basic optimization theory and algorithms from An Introduction to Optimization by Chong and Zak. I would like someone to verify my solution to this problem, on finding the minimizer/maximizer of a function of two variables, or any tips/hint to proceed ahead. For each value of the scalar $\beta$ , find the set of all stationary points of the following two variables $x$ and $y$ $$f(x,y) = x^2 + y^2 + \beta xy + x + 2y$$ Which of those stationary points are local minima? Which are global minima and why? Does this function have a global maximum for some value of $\beta$ . Solution. We have: \begin{align*}
f(x,y) &= x^2 + y^2 + \beta xy + x +2y\\
f_x(x,y) &= 2x+\beta y + 1\\
f_y(x,y) &= \beta x + 2y + 2\\
f_{xx}(x,y) &= 2\\
f_{xy}(x,y) &= \beta \\
f_{yy}(x,y) &= 2
\end{align*} By the first order necessary condition(FONC) for optimality, we know that if $\nabla f(\mathbf{x})=0$ , then $\mathbf{x}$ is a critical point. Thus, \begin{align*}
f_x(x,y) &= 2x+\beta y + 1 = 0\\
f_y(x,y) &= \beta x + 2y + 2 = 0
\end{align*} Solving for $x$ and $y$ , we find that: \begin{align*}
x = \frac{\begin{array}{|cc|}
-1 & \beta \\
-2 & 2
\end{array}}{\begin{array}{|cc|}
2 & \beta \\
\beta & 2
\end{array}}=\frac{-2+2\beta}{4-\beta^2}=\frac{2\beta-2}{4 -\beta^2}
\end{align*} \begin{align*}
y = \frac{\begin{array}{|cc|}
2 & -1 \\
\beta & -2
\end{array}}{\begin{array}{|cc|}
2 & \beta \\
\beta & 2
\end{array}}=\frac{-4+\beta}{4-\beta^2}=\frac{\beta -4}{4 - \beta^2}
\end{align*} The second order necessary and sufficient conditions for optimality are based on the sign of the quadratic form $Q(\mathbf{h})=\mathbf{h}^T \cdot Hf(\mathbf{a}) \cdot \mathbf{h}$ . The Hessian of $f$ is given by, $$Hf(\mathbf{x})=\begin{array}{|c c|}
2 & \beta \\
\beta & 2
\end{array}$$ Thus, $d_1 = 2 > 0$ and $d_2 = 4 - \beta^2$ . Thus, $f$ has a local minimizer if and only if $4 - \beta^2 > 0$ . $g(\beta) = 4 - \beta^2$ is a downward facing parabola. So, the values of this expression positive, if and only if $-2 < \beta < 2$ . The function $f$ has no global maximum. Question. How do I find the actual global minima?","['nonlinear-optimization', 'maxima-minima', 'multivariable-calculus', 'solution-verification', 'optimization']"
4132148,"Show that $\{(x_1,x_2):x_1x_2\geq4,x_1>0,x_2>0\}$ is a convex set","Trying to show that the set $S = \{(x_1,x_2):x_1x_2\geq4,x_1>0,x_2>0\}$ is convex. I tried proving by definition, let $x=(x_1,x_2)$ and $y=(y_1,y_2)$ and $\lambda\in[0,1]$ We want to show that $\lambda x + (1-\lambda)y \in S$ for $\lambda=1$ and $\lambda=0$ the answer is trivial. for $\lambda \in (0,1)$ I got a bit stuck What I tried was $\lambda x + (1-\lambda)y = \lambda(x_1,x_2) + (1 - \lambda)(y_1,y_2) = \lambda(x_1,x_2) - \lambda(y_1,y_2) + (y_1,y_2) = (\lambda x_1 - \lambda y_1 + y_1, \lambda x_2 - \lambda y_2 + y_2)$ So now if we can prove that $(\lambda x_1 - \lambda y_1 + y_1)\times(\lambda x_2 - \lambda y_2 + \lambda y_2) \geq 4$ we are done. I got stuck here, I tried looking for a way with the mean inequality but with no success. I think I am way off base, perhaps Jensen's inequality or something in that domain might be what I am looking for.","['convex-optimization', 'optimization', 'calculus', 'convex-analysis']"
4132168,"Prove convexity of $f(x_1,x_2,x_3)=\frac{x_1^2+x_1+1}{x_2+x_3}$ over $\{(x_1,x_2,x_3) : x_2,x_3\gt0\}$","Prove convexity of $$f(x_1,x_2,x_3)=\frac{x_1^2+x_1+1}{x_2+x_3}$$ over $\{(x_1,x_2,x_3) : x_2,x_3\gt0\}$ I am looking for an easy (or relatively easy) way to show $f$ is convex.
I tried to split the function into: $$\frac{x_1^2}{x_2+x_3}+\frac{1}{x_2+x_3}+\frac{x_1}{x_2+x_3}$$ The first and the second functions are convex: The first is convex because it's quadratic-over-linear which is convex; The second is convex because it's linear change of variables of $\frac{1}{x}$ which is convex over $x>0$ . I stuck proving the third one. I found it's Hessian matrix. Let $g(x)=\frac{x_1}{x_2+x_3}$ . $$\nabla ^2g(x)=
\begin{bmatrix}
0 & \frac{-1}{(x_2+x_3)^2} & \frac{-1}{(x_2+x_3)^2} \\
\frac{-1}{(x_2+x_3)^2} & \frac{2x_1}{(x_2+x_3)^3} & \frac{2x_1}{(x_2+x_3)^3} \\
\frac{-1}{(x_2+x_3)^2} & \frac{2x_1}{(x_2+x_3)^3} & \frac{2x_1}{(x_2+x_3)^3}
\end{bmatrix}
$$ It suffices to show $\nabla g(x)\succeq0$ , but I failed to do so. Last hope is calculating eigenvalues, but it seems too difficult. By the way, I can't use level-sets .
Maybe there's an eaier way to show its convexity. Please advise. Thank you.","['convex-optimization', 'nonlinear-optimization', 'multivariable-calculus', 'optimization', 'convex-analysis']"
4132175,How to find explicit formula for a family of polynomials defined by $f_0(x)=0$ and $f_{n+1}(x)=(xf_n(x)+1)^2$?,"So I have a family of polynomials defined as $$f_0(x)=0$$ $$f_{n+1}(x)=(xf_n(x)+1)^2$$ for $n\geq 0$ .
I am wondering if and how I would find the explicit formula for $f_n(x).$ I tried listing out the first couple of functions $$\begin{align}
f_0(x)&=0 \\
f_1(x)&=1 \\
f_2(x)&=(x+1)^2 \\
f_3(x)&=(x(x+1)^2+1)^2 \\
f_4(x)&=(x(x(x+1)^2+1)^2+1)^2
\end{align}$$ and so on. It is clear from this that the degree of $f_n(x)$ is $2^n-2$ . However, I am having difficulty determining the coefficients. Trivially, the first and last coefficients are both $1$ . Perhaps through Vieta's formulas we can determine other coefficients like the coefficient of the $2$ nd term, but I don't see a clear way to get other coefficients/terms. After doing an OEIS search, I discovered that the coefficients line up exactly with A202019 . Figuring out why...","['algebra-precalculus', 'functions', 'polynomials']"
4132178,How to rigorously prove from set theory that functions can be composed?,This is a follow up to my previous question about existence and uniqueness of piecewise functions. Suppose we are given two functions $f:S \rightarrow T$ and $g:T \rightarrow U$ . How does one prove from ZFC set theory that there is a unique function $h:S \rightarrow U$ which is the composition of those two functions?,"['elementary-set-theory', 'functions']"
4132201,Proving using definition that groups of orders up to $4$ are abelian.,"I want to prove that the Groups of order upto 4 are abelian. I am not allowed to use concept of order of an element of a group. Let $\displaystyle G=\{e,a,b,c\}$ be a group of order $\displaystyle 4$ . By closure, $\displaystyle ab\in G$ . Clearly $\displaystyle ab\neq a,b$ ( as $\displaystyle ab=a\Longrightarrow b=e$ and similarly $\displaystyle ab\neq b$ ) so either $\displaystyle ab=e$ or $\displaystyle ab=c$ . Case 1: $\displaystyle ab=e$ It follows that $\displaystyle a^{-1} =b$ and thus our $\displaystyle G$ becomes $\displaystyle \{e,a,a^{-1} ,c\}$ . Again by closure $\displaystyle ac\in G\Longrightarrow ac\neq a,c,e$ . Since $\displaystyle ac=e\Longrightarrow a^{-1} =c$ , which is not possible as $\displaystyle a^{-1}$ and $\displaystyle c$ are distinct. So we must have $\displaystyle ac=a^{-1} \Longrightarrow c=a^{-2}$ . Thus our $\displaystyle G$ finally becomes $\displaystyle \{e,a,a^{-1} ,a^{-2} \}$ , which is clearly abelian. Case 2: $\displaystyle ab=c$ so that our $\displaystyle G$ becomes $\displaystyle \{e,a,b,ab\}$ . Clearly, $\displaystyle a^{2} \neq a,ab$ . So either $\displaystyle a^{2} =b$ or $\displaystyle a^{2} =e$ . If $\displaystyle a^{2} =b$ , then $\displaystyle G$ becomes $\displaystyle \{e,a,a^{2} ,a^{3} \}$ , which is abelian. And if $\displaystyle a^{2} =e$ , then $\displaystyle ab^{2} \neq ab,b$ so either $\displaystyle ab^{2} =e$ or $\displaystyle a$ . If $\displaystyle ab^{2} =e$ then $\displaystyle a^{2} b^{2} =a=b^{2}$ , and the group becomes $\displaystyle \{e,b^{2} ,b,b^{3} \}$ , which is abelian. And if $\displaystyle ab^{2} =a$ then $\displaystyle b^{2} =e$ . $\displaystyle ( ab)( ba) =ab^{2} a=aea=e\Longrightarrow ( ab)^{-1} =ba$ . $\displaystyle ba\neq a,b$ and $\displaystyle ba\neq e$ as $\displaystyle ba=e\Longrightarrow ab( ba) =ab=e$ , which is not possible. So $\displaystyle ba=ab$ . Therefore, $\displaystyle G=\{e,a,b,ab\}$ is abelian. Groups of order $1,2$ and $3$ are trivially abelian. Is my proof correct? Thanks.","['abelian-groups', 'group-theory', 'solution-verification', 'finite-groups']"
4132214,"On the proof of the Künneth Formula (Bott, Tu, Proposition 9.12)","I'm currently going through a proof of the Künneth formula via the Cech-de Rham complex. More precisely, the statement is the following: If $M$ and $F$ are two manifolds and $F$ has finite-dimensional cohomology, then $H^{\bullet}(M\times F)=H^{\bullet}(M)\otimes H^{\bullet}(F)$ . Let $\mathfrak{U}=\{U_{\alpha}\}_{\alpha\in A}$ be a countable good open cover of $M$ (which we suppose to be ordered). Then we know that there is an isomorphism $H^{\bullet}(M)=H^{\bullet}(C^{\bullet}(\mathfrak{U},\Omega^{\bullet}))$ . Choose closed forms $\omega_{1},...,\omega_{r}\in \Omega^{\bullet}(F)$ such that $\{[\omega_{1}],...,[\omega_{r}]\}$ is a basis of $H^{\bullet}(F)$ . The authors then define a map $\pi_{\mathfrak{U}}^{*}\colon H^{\bullet}(F)\otimes C^{\bullet}(\mathfrak{U},\Omega^{\bullet})\to C^{\bullet}(\mathfrak{\pi^{-1}\mathfrak{U}},\Omega^{\bullet})$ given by $[\omega_{i}]\otimes \eta \to \rho^{*}\omega_{i}\wedge\pi^{*}\eta$ , where $\rho\colon M\times F\to F$ and $\pi\colon M\times F\to M$ are the projections. Here is the confusing part to me: the authors claim that $H^{\bullet}(F)\otimes C^{\bullet}(\mathfrak{U},\Omega^{\bullet})$ can be seen as a (double, I think) complex with cohomology $H^{\bullet}(F)\otimes H^{\bullet}(C^{\bullet}(\mathfrak{U},\Omega^{\bullet}))$ by using the differential of the second factor. That is, if we denote by $d$ and $\delta$ the differentials of $C^{\bullet}(\mathfrak{U},\Omega^{\bullet})$ , then $1\otimes d$ and $1\otimes \delta$ are the differentials of $H^{\bullet}(F)\otimes C^{\bullet}(\mathfrak{U},\Omega^{\bullet})$ . At first, this made sense to me, but later it is stated that $\pi^{*}_{\mathfrak{U}}$ induces an isomorphism in cohomology. But if we take some $\eta\in C^{p}(\mathfrak{U},\Omega^{q})$ , then $\rho^{*}\omega_{i}\wedge \pi^{*}\eta\in C^{p}(\pi^{-1}\mathfrak{U},\Omega^{q+\operatorname{deg}\omega_{i}})$ , so $\pi^{*}_{\mathfrak{U}}$ is not a homomorphism of double complexes. What am I missing here? Is it that the structure of complex in $H^{\bullet}(F)\otimes C^{\bullet}(\mathfrak{U},\Omega^{\bullet})$ is different than the one I'm thinking of? Thank you in advance! EDIT: I believe the problem here lies on the structure given to $H^{\bullet}(F)\otimes C^{\bullet}(\mathfrak{U},\Omega^{\bullet})$ . I thought that maybe we can consider it as a product complex by using the fact that $H^{\bullet}(F)$ is finite-dimensional, but I'm not sure on how to proceed.","['homology-cohomology', 'differential-forms', 'differential-geometry']"
4132217,Distance between a point on the edge of a circle to a point on a equilateral triangle [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Equilateral triangle $ABC$ has side length 6. Let $D$ be the point on segment $BC$ such that $BD=4$ . The circle passing through points $A, B, C$ intersects line $AD$ at $A$ and at another point $E$ . The length of $DE$ can be expressed in simplest radical form as $\frac{A\sqrt B}C$ , where $A$ , $B$ and $C$ are positive integers. What is $A+B+C$ ? I keep on getting $15$ , but the answer key says the answer is $18$ . I don't know if the link to the image will work, but what I have tried so far is $R-r$ $\left(\frac{6\sqrt3}3-\frac{6\sqrt3}6\right)$ . However, I realize that $R$ is calculated assuming that you are going from the center of the circle inscribed in the equilateral triangle to a point on the outside circle, and in this problem the line going through points $AD$ doesn't go through the middle of an inscribed circle. My question is what other methods should I try to find the length of $DE$ where the answer can be expressed in terms of $\frac{A\sqrt B}C$ .","['euclidean-geometry', 'circles', 'geometry', 'triangles', 'trigonometry']"
4132263,Understanding Induction as it Applies to Proving $A \cup (\cap_iB_i) = \cap_i(A \cup B_i)$,"I am grappling with the concept of induction, and I'm trying to apply the concept to different proofs. I seem to be missing a piece of the conceptual puzzle though. Consider the following proof. (Question at the end) Given $A \cup (B \cap C) = (A \cup B) \cap (A \cup C)$ , then for an arbitrary index set $I$ , $A \cup (\cap_iB_i) = \cap_i(A \cup B_i)$ . Proof :
Let $\mathcal{A}(n)$ be the assertion that $A \cup (\cap_nB_n) = \cap_n(A \cup B_n)$ . Clearly $\mathcal{A}(1)$ is true, since $A \cup (B_1 \cap B_2) = (A \cup B_1) \cap (A \cup B_2)$ . Suppose now that $\mathcal{A}(n)$ is true. Then we may consider: \begin{align*}
  (A \cup ( \cap_nB_n)) \cap B_{n+1} &= (A \cup B_{n+1}) \cup \big((\cap_nB_n) \cap B_{n+1}\big) \\
                                     &= (A \cap B_{n+1}) \cup (\cap_{n+1}B_{n+1}) \\
\implies x \in A \cap B_{n+1} \;\; \text{or} \;\; x \in \cap_{n+1}B_{n+1} &\implies x \in A \cup (\cap_{n+1}B_{n+1})
\end{align*} Thus we have $\big(A \cup (\cap_nB_n)\big) \cap B_{n+1}= A \cup (\cap_{n+1}B_{n+1})$ , and by induction, $ A \cup (\cap_{n+1}B_{n+1}) = \cap_{n+1}(A \cup B_{n+1})$ .\ $\therefore A \cup (\cap_iB_i) = \cap_i(A \cup B_i)$ . $\blacksquare$ My Question : We used induction to show that the left hand side of the equality was true. Do we also need to perform the same manipulation to show the right hand side is true, or does the ""suppose step"" allow us to immediately make the deduction once we've shown that it is true for the left hand side? If so, why? Basically, am I correct in saying and by induction... in the above proof? My fear is that we need to show the right hand side also works with our induction step to prove equality, but I can't really explain why I think that's true (or not true). It seems like the proof above is sufficient, since our ""suppose step"" should be immediately applicable. I appreciate any guidance. Thanks!","['elementary-set-theory', 'proof-explanation', 'induction', 'solution-verification']"
4132318,Number of lines of $3$ points in an arrangement of points and lines,It is well known that a finite set of $n$ points cannot form more than $$\bigg\lfloor \frac{n(n-3)}{6} \bigg\rfloor+1 $$ lines that include $3$ points. Would this result still hold if we assume that the set $P$ of $n$ points has some sort of central symmetry i.e. $k$ -fold symmetry for some $k\in \mathbb{N}$ ? Or would the upper bound be smaller?,"['graph-theory', 'additive-combinatorics', 'combinatorics', 'discrete-mathematics', 'algorithms']"
4132340,"Finding all $f:\Bbb{R}\to\Bbb{R}$ such that $f(x+y)=f(x)+f(y)$ for all real $x$ and $y$, and $f(1)=0$","If $f:\Bbb{R}\to\Bbb{R}$ satisfies $f(x+y)=f(x)+f(y)$ for all real $x$ and $y$ , and $f(1)=0$ , find $f(x)$ . By definition, $$f'(x)= \frac{f(x+h)-f(x)}{h}$$ when $h$ tends to zero.
Using the given statement, $$f'(x)= \frac{f(h)}{h}$$ Since $f(0)=0$ , $$f'(x)= f(h)+\frac{f(0)}{h}$$ or, $$f'(x)=f'(0)$$ which means $f(x)=mx+C$ . However, I am not able to determine these constants. Moreover, is there any way to do it without calculus? Please help. Thanks.",['functions']
4132346,"On which subspace $W\subset C^{\infty}[0,1]$ is $D(f)=xf'(x)$ a bounded operator provided all functions in $W$ are flat functions?","First we introduce the concept ""Flat function"": Definition: A smooth function $f:\mathbb{R}\to \mathbb{R}$ is called a flat function at origin if $f^{(k)}(0)=0$ for all $k=0,1,2,\ldots $ . Let $V=\{f\in C^{\infty}[0,1]\mid \text{$f$ is flat at origin}\}$ . We equip $V$ with $\|\cdot\|_{\infty}$ . Is there an infinite dimensional subspace $W\subseteq V$ which is invariant under the differential operator $D(f)=xf'(x)$ and $D$ is a bounded operator on $W$ ?
The motivation comes from page 43 remark 2 of the following paper: http://mcs.qut.ac.ir/article_243944.html","['differential-operators', 'operator-theory', 'functional-analysis', 'real-analysis']"
4132380,Is the honeycomb lattice the double graph of another graph?,"Associated to any planar graph $G$ there is another graph called its double graph , denoted by $\mathcal{H}(G)$ , defined according to the following rules (see 'Trees and Matchings' section 2, page 3): Embed $G$ and its dual $G^{\ast}$ simultaneously in the plane in such a way that any vertex $v$ of $G$ should lie inside its dual face $F_{v}^{\ast}$ in $G^{\ast}$ and any two dual edges of $G$ and $G^{\ast}$ intersect exactly once. Declare all the new edge intersections from step 1 as new vertices and color them in white and all the remaining vertices coming from $G$ and $G^{\ast}$ as black vertices. Denote the resulting graph $\mathcal{H}(G)$ and notice that by construction is bipartite. Question: Does the bipartite honeycomb lattice arise as the double graph $\mathcal{H}(G)$ of any other graph $G$ ? Note: It is indicated in 'Trees and Matchings' that the double graph construction can be generalized to directed graphs and in this case the bipartite honeycomb lattice appears as the double graph a directed triangular lattice, however, I'm interested in undirected graphs.","['graph-theory', 'combinatorial-geometry', 'combinatorics', 'discrete-mathematics', 'planar-graphs']"
4132394,Do generic smooth functions have no degenerate points?,"In this question, $\mathbb{N}$ includes $0$ . Let $M$ be a compact $D$ -dimensional $C^\infty$ -smooth manifold, and fix a finite atlas $\,\mathcal{U}=\{\phi_j \,\colon U_j \to \mathbb{R}^D\}_{j=1,\ldots,n}$ of $C^\infty$ charts on $M$ . Let $A=\{\boldsymbol{\alpha}_1,\ldots,\boldsymbol{\alpha}_{D+1}\}$ be a set of $D+1$ distinct multi-indices $\boldsymbol{\alpha}_i \in \mathbb{N}^D$ , and let $K=\max\{\|\boldsymbol{\alpha}_1\|_1,\ldots,\|\boldsymbol{\alpha}_{D+1}\|_1\}$ . Let $\mathcal{F}$ be the set of all $C^\infty$ functions $f \colon M \to \mathbb{R}$ with the property that for every chart $\,\phi_j \, \colon U_j \to \mathbb{R}^D$ in $\,\mathcal{U}$ , for each $\mathbf{x} \in \phi_j(U_j)$ , at least one of the multi-indices $\boldsymbol{\alpha}_i \in A$ has $\partial_{\boldsymbol{\alpha}_{i\!}}(f \circ \phi_j^{-1})(\mathbf{x}) \neq 0$ . Is it the case that for every $k \in \{K,K+1,\ldots\} \cup \{\infty\}$ , $\mathcal{F}$ is dense in $C^k(M,\mathbb{R})$ ? My intuition is that the answer is yes , and furthermore that this result – or something similar – should be a well-known result (probably with a name), or should at least exist in some textbook. Is there a reference for this result, or a name for this result?","['multivariable-calculus', 'differential-geometry', 'reference-request', 'real-analysis']"
4132414,Proving that $\mathbb{R} \cong \mathbb{R}^{\mathbb{N}}$,"I am trying to prove that the real numbers have the same cardinality as the set of real sequences. I'm treating this as proving that $\mathbb{R} \cong \mathbb{R}^{\mathbb{N}}$ since the set of real sequences can be identified with the set of functions $\mathbb{N} \to \mathbb{R}$ . Indeed, given such a sequence $(s_n)$ in $\mathbb{R}$ , we identify it with the mapping $s: \mathbb{N} \to \mathbb{R}$ such that $s(n) = s_n$ . I don't know of a way to prove this directly, so my only thought is to use the Schröder–Bernstein theorem. One direction is rather obvious. Given $c \in \mathbb{R}$ , we can define the constant function (or the constant sequence) $f: \mathbb{N} \to \mathbb{R}$ sending $f(n) = c$ for all $n$ . So $\mathbb{R} \hookrightarrow \mathbb{R}^{\mathbb{N}}$ . It suffices to find an injection in the opposite direction $\mathbb{R}^{\mathbb{N}} \hookrightarrow \mathbb{R}$ , but I can't figure out how to do. I could perhaps identify $\mathbb{R}$ with $\mathcal{P}(\mathbb{N})$ , but that would require proving that $\mathbb{R} \cong \mathcal{P}(\mathbb{N})$ , wich I know to be true, but also don't know how to prove. Any help or guidance would be appreciated.","['elementary-set-theory', 'proof-explanation']"
4132447,Why do keyhole contours work?,"When evaluating integrals on $(0,\infty)$ , we often extend the integral into the complex plane by using a ""keyhole contour"" with inner radius $\epsilon$ and outer radius $R$ : Credit for the image goes to Linda J. Cummings . My question is - why do we do this? When we take $R\to\infty$ and $\epsilon\to 0$ , the contribution from the circular arcs typically vanish, but what we have remaining is two integrals, i.e $\int\limits_0^\infty$ and $\int\limits_\infty^0$ . But these will just cancel out right? When we integrate over the same line in the opposite direction, the result is negated, is it not? I have not seen anyone rigorously justify this. More worrying is a section on Wikipedia where they conclude $$\int\limits_R^\epsilon \frac{\sqrt{z}}{z^2+6z+8}\mathrm{d}z=\int\limits_R^\epsilon \frac{-\sqrt{z}}{z^2+6z+8}\mathrm{d}z=\int\limits_\epsilon^R \frac{\sqrt{z}}{z^2+6z+8}\mathrm{d}z$$ But from this surely the integral is trivially zero?! Why is this not a completely pointless endeavor? Does it have to do with the fact we are taking a limit as the upper bound of the integral goes $\to\infty$ ? Or does domain reversal work differently in the complex plane? I'd appreciate if someone could explain this.","['integration', 'complex-analysis']"
4132614,"If $F,G:\mathbb{R}\to\mathbb{R}$ are injective functions then $F+G$ is injective.","I wrote this proof (or rather the beginning of it), but it sounds a bit ""illogical"" to me, so I couldn't finish. Let $F,G:\mathbb{R}\rightarrow\mathbb{R}$ be injective functions. Let $a,b$ in $\mathbb{R}$ . Assume $(F+G)(a)=(F+G)(b)$ , and prove that $a=b$ . $(F+G)(a) = F(a) + G(a) = x + y$ ( $F, G$ injective so there exist one and only one element $a$ [respectively] in $\mathbb{R}$ so that $F(a) = x$ and $G(a) = y$ ) $(F+G)(b) = F(b) + G(b) = c + d$ (Same as before) Since $(F+G)(a)=(F+G)(b)$ , we get that $x+y = c + d$ Since $F,G$ injective, and $x + c$ (here's my problem with this statement, since we don't REALLY know that $F(a) = x$ and $F(b) = c$ , since we only know about the SUM of $F(a)+G(a)$ , hence sounds a bit illogical to me How do you continue from here? Sorry for my bad English, it's far from my native language! Also, sorry for the lack of formatting - I'm new to all of this. Thanks!","['calculus', 'functions']"
4132617,"Finding the global minimum of $f(\mathbf{x})=\|(1-x_1,x_1-x_2,x_2-x_3,\ldots,x_{n-1}-x_n,x_n-2)\|_2^2$","I am self-learning optimization algorithms. A certain assignment problem is as follows: Show that the $n$ -dimensional function $f(\mathbf{x})=\lvert \lvert (1-x_1,x_1-x_2,x_2-x_3,\ldots,x_{n-1}-x_n,x_n-2\rvert\rvert_2^2$ has exactly one stationary point which is a global minimum. Compute this minimum. I would like someone to verify my optimal solution; does the math checkout? Solution. We have, \begin{align*}
f(\mathbf{x})= (1-x_1)^2+(x_1-x_2)^2+\ldots+(x_{n-1}-x_{n})^2 + (x_n-2)^2
\end{align*} The partial derivatives of $f$ are as follows: \begin{align*}
f_{x_1}(\mathbf{x}) &= 2(1-x_1)(-1)+2(x_1-x_2) = -2 + 4x_1 - 2x_2\\
f_{x_2}(\mathbf{x}) &= 2(x_1-x_2)(-1)+2(x_2-x_3) = -2x_1 + 4x_2 -2x_3 \\
f_{x_3}(\mathbf{x}) &= 2(x_2-x_3)(-1)+2(x_3-x_4) = -2x_2 + 4x_3 -2x_4 \\
\vdots\\
f_{x_{n-1}}(\mathbf{x}) &= 2(x_{n-2}-x_{n-1})(-1)+2(x_{n-1}-x_n) = -2x_{n-2} + 4x_{n-1} -2x_n \\
f_{x_n}(\mathbf{x}) &= 2(x_{n-1}-x_n)(-1)+2(x_n-2) = -2x_{n-1} + 4x_n -4
\end{align*} The critical points of $f$ are given by, $$\nabla f(\mathbf{x}) = 0$$ Therefore, we have the system of equations: \begin{align*}
\begin{bmatrix}
4 &-2 & 0 & 0 & 0 & \ldots & 0 & 0 & 0\\ 
-2& 4 &-2 & 0 & 0 & \ldots & 0 & 0 & 0\\
0 &-2 & 4 &-2 & 0 & \ldots & 0 & 0 & 0\\
0 & 0 &-2 & 4 &-2 & \ldots & 0 & 0 & 0\\
0 & 0 & 0 &-2 & 4 & \ldots & 0 & 0 & 0\\
0 & 0 & 0 & 0 &-2 & \ldots & 0 & 0 & 0\\
\vdots\\
0 & 0 & 0 & 0 & 0 & \ldots &-2 & 4 &-2\\
0 & 0 & 0 & 0 & 0 & \ldots & 0 &-2 & 4
\end{bmatrix}\begin{bmatrix}
x_1\\
x_2\\
x_3\\
x_4\\
x_5\\
x_6\\
\vdots\\
x_{n-1}\\
x_n
\end{bmatrix}=\begin{bmatrix}
2\\
0\\
0\\
0\\
0\\
0\\
\vdots\\
0\\
4
\end{bmatrix}
\end{align*} These are $n$ equations in $n$ unknowns. This tridiagonal system has a unique solution vector $\mathbf{x}$ . Applying Gaussian elimination to the augmented matrix $[A \quad b]$ , by hand, we have: \begin{align*}
\begin{bmatrix}
4 &-2 & 0 & 0 & 0 & \ldots & 0 & 0 & 0 &\bigm| & 2\\ 
-2& 4 &-2 & 0 & 0 & \ldots & 0 & 0 & 0 &\bigm| & 0\\
0 &-2 & 4 &-2 & 0 & \ldots & 0 & 0 & 0 &\bigm|  & 0\\
0 & 0 &-2 & 4 &-2 & \ldots & 0 & 0 & 0 &\bigm|  & 0\\
0 & 0 & 0 &-2 & 4 & \ldots & 0 & 0 & 0 &\bigm|  & 0\\
0 & 0 & 0 & 0 &-2 & \ldots & 0 & 0 & 0 &\bigm|  & 0\\
\vdots &  &  &  & &  &  &  & &\bigm|  & \vdots\\
0 & 0 & 0 & 0 & 0 & \ldots &-2 & 4 &-2 &\bigm|  & 0\\
0 & 0 & 0 & 0 & 0 & \ldots & 0 &-2 & 4 &\bigm|  & 4\\
\end{bmatrix}
\end{align*} Applying \begin{align*}
\{R_2 \rightarrow 2R_2 + R_1\}
\end{align*} \begin{align*}
\begin{bmatrix}
4 &-2 & 0 & 0 & 0 & \ldots & 0 & 0 & 0 &\bigm| & 2\\ 
0 & 6 &-4 & 0 & 0 & \ldots & 0 & 0 & 0 &\bigm| & 2\\
0 &-2 & 4 &-2 & 0 & \ldots & 0 & 0 & 0 &\bigm|  & 0\\
0 & 0 &-2 & 4 &-2 & \ldots & 0 & 0 & 0 &\bigm|  & 0\\
0 & 0 & 0 &-2 & 4 & \ldots & 0 & 0 & 0 &\bigm|  & 0\\
0 & 0 & 0 & 0 &-2 & \ldots & 0 & 0 & 0 &\bigm|  & 0\\
\vdots &  &  &  & &  &  &  & &\bigm|  & \vdots\\
0 & 0 & 0 & 0 & 0 & \ldots &-2 & 4 &-2 &\bigm|  & 0\\
0 & 0 & 0 & 0 & 0 & \ldots & 0 &-2 & 4 &\bigm|  & 4\\
\end{bmatrix}
\end{align*} Applying \begin{align*}
\{R_3 \rightarrow 3R_3 + R_2\}
\end{align*} \begin{align*}
\begin{bmatrix}
4 &-2 & 0 & 0 & 0 & \ldots & 0 & 0 & 0 &\bigm| & 2\\ 
0 & 6 &-4 & 0 & 0 & \ldots & 0 & 0 & 0 &\bigm| & 2\\
0 & 0 & 8 &-6 & 0 & \ldots & 0 & 0 & 0 &\bigm| & 2\\
0 & 0 &-2 & 4 &-2 & \ldots & 0 & 0 & 0 &\bigm| & 0\\
0 & 0 & 0 &-2 & 4 & \ldots & 0 & 0 & 0 &\bigm| & 0\\
0 & 0 & 0 & 0 &-2 & \ldots & 0 & 0 & 0 &\bigm| & 0\\
\vdots &  &  &  & &  &  &  & &\bigm|  & \vdots\\
0 & 0 & 0 & 0 & 0 & \ldots &-2 & 4 &-2 &\bigm| & 0\\
0 & 0 & 0 & 0 & 0 & \ldots & 0 &-2 & 4 &\bigm| & 4\\
\end{bmatrix}
\end{align*} Applying \begin{align*}
\{R_4 \rightarrow 4R_4 + R_3\}
\end{align*} \begin{align*}
\begin{bmatrix}
4 &-2 & 0 & 0 & 0 & \ldots & 0 & 0 & 0 &\bigm| & 2\\ 
0 & 6 &-4 & 0 & 0 & \ldots & 0 & 0 & 0 &\bigm| & 2\\
0 & 0 & 8 &-6 & 0 & \ldots & 0 & 0 & 0 &\bigm| & 2\\
0 & 0 & 0 &10 &-8 & \ldots & 0 & 0 & 0 &\bigm| & 2\\
0 & 0 & 0 &-2 & 4 & \ldots & 0 & 0 & 0 &\bigm| & 0\\
0 & 0 & 0 & 0 &-2 & \ldots & 0 & 0 & 0 &\bigm| & 0\\
\vdots &  &  &  & &  &  &  & &\bigm|  & \vdots\\
0 & 0 & 0 & 0 & 0 & \ldots &-2 & 4 &-2 &\bigm| & 0\\
0 & 0 & 0 & 0 & 0 & \ldots & 0 &-2 & 4 &\bigm| & 4\\
\end{bmatrix}
\end{align*} Continuing in this fashion, we obtain after $\{R_n\rightarrow n R_n + R_{n-1}\}$ , \begin{align*}
\begin{bmatrix}
4 &-2 & 0 & 0 & 0 & \ldots & 0 & 0 & 0 &\bigm| & 2\\ 
0 & 6 &-4 & 0 & 0 & \ldots & 0 & 0 & 0 &\bigm| & 2\\
0 & 0 & 8 &-6 & 0 & \ldots & 0 & 0 & 0 &\bigm| & 2\\
0 & 0 & 0 &10 &-8 & \ldots & 0 & 0 & 0 &\bigm| & 2\\
0 & 0 & 0 &0 & 12 & \ldots & 0 & 0 & 0 &\bigm| & 2\\
0 & 0 & 0 & 0 &0 & \ldots & 0 & 0 & 0 &\bigm| & 2\\
\vdots &  &  &  & &  &  &  & &\bigm|  & \vdots\\
0 & 0 & 0 & 0 & 0 & \ldots & 0 & 2n &-2(n-1) &\bigm| & 2\\
0 & 0 & 0 & 0 & 0 & \ldots & 0 &0 & 2n+2 &\bigm| & 4n+2
\end{bmatrix}
\end{align*} By back-substitution, \begin{align}
(2n+2)x_n &= 4n+2\\
x_n &= \frac{2n+1}{n+1} \tag{1}
\end{align} Substituting the value of $x_n$ in $2n x_{n-1} -2(n-1)x_n = 2$ , we have: \begin{align}
2n x_{n-1} &= 2 + 2(n-1)x_n\\
x_{n-1} &= \frac{1}{n}\left[1 + (n-1)x_n\right]\\
&= \frac{1}{n}\left[1 + (n-1)\frac{2n+1}{n+1}\right]\\
&= \frac{1}{n}\left[\frac{(n+1)+(n-1)(2n+1)}{n+1}\right]\\
&= \frac{1}{n}\left[\frac{(n+1)+n(2n+1)-(2n+1)}{n+1}\right]\\
&= \frac{1}{n}\left[\frac{n + 1 + 2n^2 + n - 2n - 1}{n+1}\right]\\
&= \frac{1}{n}\left[\frac{2n^2}{n+1}\right]\\
&= \frac{2n}{n+1} \tag{2}
\end{align} As the recurrence relationship stays the same for $n \in \{1,2,3,4,\ldots,n-1\}$ , we have: \begin{align*}
x_{n-1} &= \frac{2n}{n+1}\\
x_{n-2} &= \frac{2n-1}{n+1}\\
x_{n-3} &= \frac{2n-2}{n+1}\\
\vdots\\
x_3 &= \frac{n+4}{n+1}\\
x_2 &= \frac{n+3}{n+1}\\
x_1 &= \frac{n+2}{n+1}
\end{align*} Now, the Hessian of $f$ , $Hf(\mathbf{x})$ is the same tridiagonal matrix $A$ as above. If we find the sequence of determinants $det(B_k)$ , where $B_k$ is a $k \times k$ sub-matrix of $A$ in the upper-left corner, we see that $det(B_1) = 4, det(B_2) = 8, det(B_3) = 16, \ldots, det(B_n)=2^{n+1}$ . Consequently, the critical point $\mathbf{x}$ is a global minima. The minimum value of $f$ is given by, \begin{align}
\min \{f(\mathbf{x}):\mathbf{x}\in \mathbf{R}^n \} &=\left(1-\frac{n+2}{n+1}\right)^2 + \left(\frac{1}{n+1}\right)^2 + \ldots +  \left(\frac{2n+1}{n+1}-2\right)^2\\
&= \frac{(n+1)}{(n+1)^2}\\
&= \frac{1}{n+1}
\end{align}","['optimization', 'multivariable-calculus', 'solution-verification', 'maxima-minima']"
4132657,How to write $f(x)=x^n+(1-x)^n$ in terms of $r=x(1-x)$,"The functions $f_n(x) = x^n + (1-x)^n$ with $n\in\mathbb N$ obviously have $f_n(x)=f_n(1-x)$ . Thus we can write $f_n(x)=\sum_k a_{n,k} r^k$ with $r=x\cdot(1-x)$ . Is there a nice way to determine the $a_{n,k}$ ?","['symmetry', 'polynomials', 'analysis']"
4132666,Find the length of the segment $QA$ in acute-angled $\triangle ABC$,"Problem Let $ABC$ be an acute-angled triangle. The external bisector of $\angle BAC$ meets the line $BC$ at point $N$ . Let $M$ be the midpoint of $BC$ . $P$ and $Q$ are two points on line $AN$ such that, $\angle PMN=\angle MQN=90^{\circ}$ . If $PN=5$ and $BC=3$ , find the length of $QA$ . ( BdMO 2021 Higher Secondary P6 ) My attempt Some angle chasing leads to $$\triangle PMN \sim \triangle QMN \sim \triangle PQM.$$ Then, $$\frac{PN}{MN}=\frac{MN}{QN} \implies MN^2=5QN \tag {1}$$ and $$\frac{PN}{PM}=\frac{PM}{PQ} \implies PM^2=5PQ \tag {2}$$ I couldn't achieve something useful from them. I also tried adding the internal bisector of $\angle BAC$ , then if the bisector is $AD$ we have $AD \parallel QM$ and $\triangle ADN \sim \triangle QMN$ . But these are also not useful.(At least I can't proceed further) Edit: After Intelligenti pauca 's comment, I tried the problem in Geogebra and found that $A,B,C,P$ are concyclic. But I am unable to prove this. Here is the figure. Can someone prove this? Also, Dr. Mathva provided a solution using projective geometry . But as I don't know projective geometry, I need a solution with Euclidean Geometry . So, how to solve this problem? Thanks in advance.","['contest-math', 'euclidean-geometry', 'projective-geometry', 'geometry', 'trigonometry']"
4132667,Is $f(x)$ continuous at $x=0$? For $f(x) = \frac{x^3+x}{x}$.,"Here's my work flow. I've used this graph from Khan Academy, while it's indeterminate at first glance, it seems like we'd be able to find it's limit by factorising. I also graphed it to see that there's any break in the graph, but can't seem to find a hole/break in the graph that makes is ""not continuous"", I've thus assumed that it IS continuous. MY QUESTION: What did I do wrong and can you help me identify/bridge my knowledge gap please? Here's the original question, which doesn't even mention limit... : If I cheated and used Wolfram alpha using limits it shows there's a break in limit, how did they get the graph like that?","['limits', 'calculus']"
4132685,Planar analogues of complete graphs,"In this question, the word graph means simple graph with finitely many vertices. We let $\subseteq$ denote the subgraph relation. A characterization of complete graphs $K_n$ gives them as "" $n$ -universal"" graphs that contain all graphs $G$ with at most $n$ vertices as subgraphs: For any graph $G$ with at most $n$ vertices, we have $G \subseteq K_n$ , and given any graph $H$ that contains all graphs $G$ with $|V(G)| \leq n$ as subgraphs, we have $K_n \subseteq H$ . Question 1. (the probably-easy question) Are there planar graphs $U_n$ that are "" $n$ -universal"" in the sense that For any planar graph $G$ with at most $n$ vertices, we have $G \subseteq U_n$ , and given any planar graph $H$ that contains all planar graphs $G$ with $|V(G)| \leq n$ as subgraphs, we have $U_n \subseteq H$ ? Obviously, there is a $4$ -universal planar graph. I suspect the answer is negative for $n > 5$ . If so, is there a quick proof? Question 2. (the soft question) Are there any important results about sequences $n\in\mathbb{N} \mapsto S_n$ of planar graphs such that $S_n$ contains all at-most- $n$ -vertex planar graphs as subgraphs? Most importantly: Known exact or asymptotic bounds on the minimum vertex number of $S_n$ as a function of $n$ ? Such exact bounds are known for trees. [1] Constructions of graphs achieving said bounds. [1] F. R. K. Chung, R. L. Graham and D. Coppersmith: On trees which contain all small trees . The Theory and Applications of Graphs (ed. G. Chartrand) John Wiley and Sons, 1981, 265–272.","['graph-theory', 'combinatorics', 'extremal-combinatorics', 'planar-graphs']"
4132689,$\sum\left(\frac{ n^2 + 1}{n^2 +n + 1}\right)^{n^2} $ converges or diverges?,"The original question is to show that $\;\sum\left(\dfrac{ n^2 + 1}{n^2 +n + 1}\right)^{n^2} $ either converges or diverges. I know it diverges but I'm having difficulty arriving at something useful for $  a_n $ . Here's what I did: $  a_n = \left(\dfrac{ n^2 + 1}{n^2 +n + 1}\right)^{n^2} \geq\left( \dfrac{ n^2 + 1}{n^2 +2n + 1}\right)^{n^2}  \geq\left( \dfrac{ n^2 - 1}{n^2 +2n + 1}\right)^{n^2} = \left(\dfrac{(n-1)(n+1)}{(n+1)^2}\right)^{n^2}  = \left(\dfrac{n-1}{n+1}\right)^{n^2} = \left( 1 - \dfrac{2}{n+1}\right)^{n^2} $ Do you have idea how to continue from here? ( my idea was to arrive to some expression that goes to infinity and deduce by the comparison test that $ a_n $ diverges ). I feel like something is right infront of my eyes but I can't see it. Edit : The series converges. However, how do you propose I should continue from where I've left?","['limits-without-lhopital', 'convergence-divergence', 'sequences-and-series', 'real-analysis']"
4132696,taking an integral without trigonometric substitution,"I needed to evaluate $$\int_{-\infty}^\infty\frac{1}{x^2+a^2}dx$$ I looked around and found it's solved by trigonometric substitution $x = a\tan\theta$ and the answer is $\frac{\pi}{a}$ . I understand the derivation but have been curious: supposing I didn't know the ""trick"", are there more staightforward methods (even if more laborious) to find this integral? For example, I figured out the power series $$\frac{1}{x^2+a^2} = \frac{1}{a^2} - \frac{1}{a^4}x^2 + \frac{1}{a^6}x^4 - ...$$ I can integrate the power series term-by-term. But what I end up with obviously diverges as $x\to \infty$ or $x \to -\infty$ . Is there a way to continue with this approach and end up with the value of the integral? Or some other way, perhaps?","['integration', 'definite-integrals']"
4132718,"$f''(0)$ exists, what is $\lim_{x\to 0} f(x)/x^2$?","Given a function $f:\mathbb{R}\to\mathbb{R}$ twice-derivable at $x=0$ with $f(0)=0$ . Define $g(x):=\frac{f(x)}{x}$ for all $x\neq 0$ and $g(0)=f'(0)$ . Clearly, $g$ is continuous since $$ \lim_{x\to 0} g(x) =\lim_{x\to 0}  \frac{f(x)}{x} = \lim_{x\to 0}  \frac{f(x) - f(0)}{x-0} = f'(0).   $$ Solving a problem, I got the limit $\lim_{x\to 0}  \frac{g(x)}{x} =\lim_{x\to 0}\frac{f(x)}{x^2} $ , which I can't manage. I suppose that it exists and is related with $f''(0)$ . Could anyone show how to compute $\lim_{x\to 0}  \frac{g(x)}{x}$ ? Thanks in advance. EDIT: In a solution-book I read $g'(0)=f''(0) /2$ . Can this be  typo? Maybe they forgot some assumptions in the problem.","['limits', 'derivatives', 'real-analysis']"
4132746,"Can we say ""$\lim_{n\rightarrow \infty }\frac{1}{n}\sum_{k=1}^{n}\sup_{x\in [t_{k-1},t_{k}]}f(x)=U(f)$""?","Let $f:[0,1]\rightarrow \mathbb{R}$ be bounded and integrable
function. Let $R_n=\frac{1}{n}\sum_{k=1}^{n}f\left ( \frac{k}{n} \right )$ . I need to prove "" $\lim_{n\rightarrow \infty }R_n=\int_{0}^{1}f(x)dx$ "". I will write down my approach first, so you can clearly see what the question in the title means. My approach: Let partition $P=\{0,\frac{1}{n},\frac{2}{n},...,1\}$ Then note that, $$\inf_{x\in \left [ 0,\frac{1}{n} \right ]}f(x)\leq f\left ( \frac{1}{n} \right )\leq \sup_{x\in \left [ 0,\frac{1}{n} \right ]}f(x)$$ $$\inf_{x\in \left [ \frac{1}{n},\frac{2}{n} \right ]}f(x)\leq f\left ( \frac{2}{n} \right )\leq \sup_{x\in \left [ \frac{1}{n},\frac{2}{n} \right ]}f(x)$$ $$\cdot \cdot \cdot $$ $$\inf_{x\in \left [ \frac{n-1}{n},1 \right ]}f(x)\leq f\left ( \frac{n}{n} \right )=f(1)\leq \sup_{x\in \left [ \frac{n-1}{n},1 \right ]}f(x)$$ So, $$\sum_{k=1}^{n}\inf_{x\in \left [ t_{k-1},t_{k} \right ]} f(x)\leq \sum_{k=1}^{n}f\left ( \frac{k}{n} \right )\leq \sum_{k=1}^{n}\sup_{x\in \left [ t_{k-1},t_{k} \right ]}f(x)$$ $$\Rightarrow \frac{1}{n}\sum_{k=1}^{n}\inf_{x\in \left [ t_{k-1},t_{k} \right ]} f(x)\leq \frac{1}{n}\sum_{k=1}^{n}f\left ( \frac{k}{n} \right )\leq \frac{1}{n}\sum_{k=1}^{n}\sup_{x\in \left [ t_{k-1},t_{k} \right ]}f(x)\; \; \; \cdot \cdot \cdot \bigstar $$ $$\Rightarrow \lim_{n\rightarrow \infty }\frac{1}{n}\sum_{k=1}^{n}\inf_{x\in \left [ t_{k-1},t_{k} \right ]} f(x)\leq \lim_{n\rightarrow \infty }\frac{1}{n}\sum_{k=1}^{n}f\left ( \frac{k}{n} \right )\leq \lim_{n\rightarrow \infty }\frac{1}{n}\sum_{k=1}^{n}\sup_{x\in \left [ t_{k-1},t_{k} \right ]}f(x)$$ From this step, I wanted to show that $$\lim_{n\rightarrow \infty }\frac{1}{n}\sum_{k=1}^{n}\inf_{x\in \left [ t_{k-1},t_{k} \right ]} f(x)=L(f)\; \; \; \text{and} \lim_{n\rightarrow \infty }\frac{1}{n}\sum_{k=1}^{n}\sup_{x\in \left [ t_{k-1},t_{k} \right ]}f(x)=U(f)$$ so that this could automatically implies $\lim_{n\rightarrow \infty }R_n=\int_{0}^{1}f(x)dx$ . However, I am not sure where to start to show them. One idea that I have is substracting $\frac{1}{n}\sum_{k=1}^{n}\inf_{x\in \left [ t_{k-1},t_{k} \right ]} f(x)$ from each side in step $\bigstar $ because ""f is integrable"" implies "" $\forall \varepsilon > 0,\; \exists  P$ s.t $U(f,P)-L(f,P)<\varepsilon $ "" Can I get some help? Update: As we substract $\frac{1}{n}\sum_{k=1}^{n}\inf_{x\in \left [ t_{k-1},t_{k} \right ]} f(x)$ from each side, then we get $$\frac{1}{n}\sum_{k=1}^{n}\left (\sup_{x \in [t_{k-1},t_k]}f(x)-\inf_{x \in [t_{k-1},t_k]}f(x)  \right )=U(f,P)-L(f,P)$$ on the right hand side. Thus, $$\left | \frac{1}{n}\sum_{k=1}^{n}f\left ( \frac{k}{n} \right ) \right |\leq U(f,P)-L(f,P)
$$ We can take $N$ large enough to make it satisfy the following condition, $$\forall \varepsilon >0\: \: \exists N \: \: s.t\: \:  |U(f,P)-L(f,P)|<\varepsilon \; \; \forall n>N$$ because there always some refinement of $P$ that can make difference smaller. Therefore $\lim [U(f,P)-L(f,P)]=0$ And, thus $$\lim L(f,P) = L(f) = U(f) = \lim U(f,P)$$ This gives us, $$\lim_{n\rightarrow \infty }R_n=\int_{0}^{1}f(x)dx$$ Does my updated part look right?","['integration', 'limits', 'real-analysis']"
4132751,How is this function surjective? (Double Counting),"We call a positive integer $n$ ""good"" if and only if , in a $6 \cdot 6$ checkerboard , no matter how we put $n$ $1 \cdot 2$ dominoes in the board, there will be a space for another domino (There will be a $1 \cdot 2$ or $2 \cdot 1$ space). Find the maximum ""good"" integer. I think the answer is $11$ . Here's my work. Section 1 : 12 isn't ""good"" . As you can see, this is a figure that shows $12$ isn't ""good"". Section 2 : 11 is ""good"" (Where I'm stuck) So, I learn this and my teacher solution is as follows, First, assume some counterexample exists. Let $A$ be sets of all spaces in the upper $5 \cdot 6$ board. Let $B$ be sets of all spaces in the lower $1 \cdot 6$ board. If $|B| \geq 4$ ; there will be two spaces that are connected , a contradiction. So, $|B| \leq 3$ Since there are a total of $36 - 2(11) = 14$ spaces. Hence, $|A| \geq 11$ . Let $C$ be sets of all dominoes that are fully included in the lower $5 \cdot 6$ board. Now, assume that , there exists some space in $A$ that there is no domino below it. This will create a $1 \cdot 2$ space which we can put another domino. A contradiction. So, there will always be a domino below a space in $A$ . We define a function $f : A \rightarrow C$ such that $f(t) = $ domino below $t$ $\space$ $\forall t \in A$ Now, my teacher says that this function is bijective , so $|C| = |A| \geq 11$ . But maximum number of dominos is $11$ , $|C| = 11$ . But if we consider the upper $1 \cdot 6$ board, we can easily put a domino in it. Hence, a contradiction. So, $11$ is ""good"". My question From Section 2, I think I can show the injective part, Let $f(a_1) = f(a_2) =$ domino ""m"" Case I : ""m"" is horizontal. Now, if $a_1$ is the left space above ""m"" and $a_2$ is the right space of ""m"" , we can put another domino above ""m"", a contradiction. So, $a_1 = a_2$ . Case II : ""m"" is vertical. This case is obvious. Thus, $f$ is injective. I know we can finish the problem using only injectivity and change $|C| = |A|$ to $|C| \geq |A|$ and finish the problem. But I can't figure out how $f$ is surjective. Can anyone give me a hint or a solution. Thanks in advance.","['chessboard', 'functions', 'combinatorics']"
4132784,"Explicit formula of the solution of $u_{tt}=au_{xx}$ and for which values of $a$ is this a ""wave equation""?","Let $a\in\mathbb R\setminus{0}$ and $u\in C^2((0,\infty)\times\mathbb R)$ be a solution of $$u_{tt}=au_{xx}\tag1.$$ I'm trying to find an explicit formula of $u$ using the ansatz $$u(t,x)=v(t)w(x);\tag2$$ and understand in which sense $(1)$ is a ""wave equation"" for appropriate values of $a$ . For 1.: By $(1)$ and $(2)$ , $$v''(t)w(x)=av(t)w''(x)\tag3$$ and hence $$\frac{v''(t)}{v(t)}=a\frac{w''(x)}{w(x)}=\lambda\tag4$$ for some constant $\lambda\in\mathbb R\setminus\{0\}$ and all $(t,x)\in(0,\infty)\times\mathbb R$ with $u(t,x)\ne0$ . $^1$ The first system, $v''=\lambda v$ , can be solved using the ansatz $v(t)=e^{\alpha t}$ . We easily see that if $\lambda>0$ , then $$v(t)=c_1e^{\alpha_1t}+c_2e^{-\alpha_1t}\tag5;$$ if $\lambda=0$ , then $$v(t)=c_1+c_2t\tag6;$$ if $\lambda<0$ , then $$v(t)=c_1e^{{\rm i}\alpha_1}+c_2e^{-{\rm i}\alpha_1}=\tilde c_1\cos(\alpha_1t)+\tilde c_2{\rm i}\sin(\alpha_1x)\tag7.$$ For the second system, $w''=\frac\lambda a$ , we obtain solutions of precisely the same form by considering the cases $\frac\lambda a>0$ , $\lambda=0$ and $\frac\lambda a<0$ . So, the solution is finally a product of the terms in $(5)$ - $(7)$ in $t$ and the corresponding terms in $x$ . For example, if $a,\lambda>0$ , then $$u(t,x)=c_1e^{\alpha_1t+\alpha_2x}+c_2e^{-\alpha_1t+\alpha_2x}+c_3e^{\alpha_1-\alpha_2x}+c_4e^{-\alpha_1t-\alpha_2x}\tag8.$$ Is there anything more we can do? And how do we need to approach question 2? $^1$ I'm not sure how we subsequently need to argue for $(t,x)\in(0,\infty)\times\mathbb R$ with $u(t,x)=0$ .","['wave-equation', 'ordinary-differential-equations', 'partial-differential-equations']"
4132820,Compute minimum amount of 60s to obtain a 9-darts finish in 501 double darts out,"In 501 double out darts, it is possible to reach the final score of 501 with only 9 darts. The dartboard has fields 1 - 20, and also double and triple fields and the semi-bull (25) and bullseye (50).
The last dart needs to hit a double field, i.e. either of: [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 50] I already know all possible 9-dart-finish combinations (you can look them up on the internet). I have noticed that you need a minimum of three 60s to obtain a 9-darts finish. Now I wonder: is there a way to prove this? Or is there at least a way to see that you need to hit a 60 at least once in order to obtain a 9-darts finish? Because in 301 darts you can obtain a 6-darts finish without ever hitting a 60. Would anyone care to show an example calculation?","['statistics', 'combinatorics', 'probability']"
4132872,Finding $|A|^2+|B|^2$ if $A=\text{adj} (B)-B^T$ and $B=\text{adj} (A)-A^T$,"Consider two $3\times3$ matrices $A$ and $B$ satisfying $A=\text{adj} (B)-B^T$ and $B=\text{adj} (A)-A^T$ (where $C^T$ denotes transpose of matrix $C$ ). If $A$ is a non-singular matrix, then find $AB$ and $|A|^2+|B|^2$ (where $|C|$ denotes determinant of matrix $C$ ). Also, find $|\text{adj} (2B^{-1})|$ . Given, $A=\text{adj} (B)-B^T$ . Taking transpose on both sides, I get $A^T=(\text{adj} B)^T-B$ . Putting $B=\text{adj} (A)-A^T$ in it, I get, $A^T=(\text{adj}B)^T-\text{adj(A)}+A^T\implies\text{adj} A=(\text{adj} B)^T$ . Taking determinant on both sides, I get $|A|^2=|B|^2$ (because $|\text{adj} C|=|C|^{n-1}$ where $n$ is the order of $C$ and $|C|=|C^T|)$ . Also, since $A$ is non-singular, if I divide $B=\text{adj} (A)-A^T$ by $|A|$ , I get $\dfrac B{|A|} = A^{-1} - \dfrac{A^T}{|A|} \implies  \dfrac{B + A^T}{|A|} = A^{-1}$ . Taking determinant on both sides, I get $\dfrac{|B+A^T|}{|A|^3} = \dfrac1{|A|} \implies |B + A^T| = |A|^2$ (because $|kC| = k^n|C|$ , where $k$ is any constant (here $\frac1{|A|}$ ) and $n$ is order of matrix $C$ . Also, $|C^{-1}|=\frac1{|C|}$ ). Not able to proceed next. EDIT: After @Upayan De's answer, $|B|=8$ . Now, $\text{adj}(kC)=k^{n-1}\text{adj}C$ , where $k$ is a constant and $n$ is the order of matrix $C$ . So, $\text{adj}(2B^{-1})=4\text{adj}(B^{-1})$ . So, $|\text{adj}(2B^{-1})|=|4\text{adj}(B^{-1})|=64|\text{adj}(B^{-1})|$ . Now, $|\text{adj}(B^{-1})|=(|\text{adj}B|)^{-1}=(|B|^2)^{-1}=1/64$ . So, $|\text{adj}(2B^{-1})|=1$","['matrices', 'determinant']"
4132891,"Pointwise convergence of $f_n(x) = \sum_{i=1}^n \left( 1_{ [ \frac{2i-2}{2n},\frac{2i-1}{2n})}(x)-1_{ [ \frac{2i-1}{2n},\frac{2i}{2n}]}(x) \right)$","Consider the following sequence of functions: \begin{align}
f_n(x) = \sum_{i=1}^n  \left( 1_{ [ \frac{2i-2}{2n},\frac{2i-1}{2n})}(x)-1_{ [ \frac{2i-1}{2n},\frac{2i}{2n}]}(x) \right).
\end{align} We are interested in understanding how does this function convergence pointwise as $n \to \infty$ for all $x\in [0,1]$ .
That is, what is \begin{align}
f(x)=\lim_{n \to \infty} f_n(x)
\end{align} Some thoughts: This is a periodic function that oscillates between $1$ and $-1$ .  As $n$ increases the period shrinks.
I have a feeling that the limit might not exist here for all $x$ .","['limits', 'real-analysis']"
4132905,Friends exchange gifts,"Classmates at the end of school decided to exchange gifts. There are more than $6$ classmates. Each classmate exchanged gifts with exactly $3$ other people. Show that all classmates can be divided into $2$ non-empty groups such that each member of the group has exchanged gifts with at least two other members of the group to which he belongs. It is easy to show that there are even numbers of students. It is also easy to show that there is a cycle in the graph. However, I cannot go further, please tell me how","['graph-theory', 'combinatorics']"
4132910,"How to find the critical points of $f(x,y,z) = x^4 + y^4 + z^4 - 4xyz$?","So I'm trying to find the critical points of this function $f(x,y,z) = x^4 + y^4 + z^4 - 4xyz$ , to do that I try to find the points where the gradient of f is equal to $(0,0)$ , though I can't solve the systems of equations, is this even the right way to do it? Here's the system of equations that I'm trying to solve: $4x^3-4yz = 0$ $4y^3-4xz = 0$ $4z^3-4xy = 0$ I first do this: $x^3 = yz$ $y^3 = xz$ $z^3 = xy$ Then this: $y = \dfrac{x^3}{z}$ $z = \dfrac{y^3}{x}$ $x = \dfrac{z^3}{y}$ But then I just go in circles by replacing each variable, I'm probably doing something wrong but I can't see it...","['multivariable-calculus', 'calculus', 'systems-of-equations']"
4132921,"Are the finite subsets of $\mathcal{P}(\mathbb{N})$, finite?","I am trying to split up the finite subsets of $\mathcal{P}(\mathbb{N})$ into two disjoint groups $X \sqcup Y$ so that no two neighbouring sets are in the same group. (we introduce the term 'neighbouring': Two sets $A,B \subseteq \mathbb{N}$ are neighbouring, if $A$ is obtainable by adding an element to $B$ , i.e. $A=B \cup \{c\}$ for a $c \not\in B$ or the other way around). In my proof, I am trying to use the compactness theorem of propositional logic to find a partitioning for the finite subsets of $\mathcal{P}(\mathbb{N})$ , to deduce the fact that there exists a partitioning for $\mathcal{P}(\mathbb{N})$ as a whole. Thus far, I could obtain a partition for the finite subsets $T:= \{1,...n\} \in \mathcal{P}(\mathbb{N})$ with $$X':= All \ subsets \ of \ T \ with \ an \ odd \ number \ of \ elements \\  Y':= All \ subsets \ of \ T \ with \ an \ even \ number \ of \ elements$$ so that $\mathcal{P}(T)=X' \sqcup Y'$ . Now, theoretically, from that partitioning, with the compactness theorem it should also follow that $\mathcal{P}(\mathbb{N})=X \sqcup Y$ . But my two main concerns are: The compactness theorem is merely applicable in the sense of considering the finite subsets of the power set. But what if those subsets are, in itself, infinite? I.e., what if the subsets themselves do not have a finite number of elements. In light of 1, how would we be able to measure the ""length"" of those subsets? For instance, $\{even \ numbers\}$ and $\{1, \ even \ numbers\}$ would be neighbouring according to our definition. But the ""length"" of those sets would neither be odd or even, and would thereby not fit into either partition. In this thread somebody has already tried to construct this partitioning, but used algebraic definitions. I feel like my proof thus far with the compactness theorem is right, but that last tiny bit is missing, or maybe I am overreacting and it is just fine as it is. (In addition: My understanding of how the compactness theorem would apply here. Our formula $\Phi$ is satisfied iff there exists a partitioning $\mathcal{P}(\mathbb{N})=X \sqcup Y$ . Now, applying the compactness theorem, we merely take the finite subsets of $\mathcal{P}(\mathbb{N})$ , which we have definied as $\mathcal{P}(T)$ , so that we have a partitioning for the finite subsets $\mathcal{P}(T)=X' \sqcup Y'$ . Which is nothing else than a partition for the finite subsets (or, formulae) of our initial formula $\Phi$ . And by definition of the compactness theorem, if every finite subset of the formula $\Phi$ is satisfied (we satisfy them by allocating them to either one of the disjunct groups), and we thus have a model for those formulae, we can conclude that it is also a model for $\Phi$ itself.) Thank you so much in advance for any help!
Lin","['propositional-calculus', 'logic', 'abstract-algebra', 'solution-verification', 'elementary-set-theory']"
4132930,Why $X$ is a del Pezzo surface?,"I'm reading the notes for a birational geometry course. There is a statement I do not understand: Suppose $X$ is a smooth projective rational surface $G\subset \text{Aut}(X)$ is a finite subgroup such that $\text{rk}(\text{Pic}(X)^G)=1$ . Then $X$ is a dell Pezzo surface. It says that to see this the reader should ""intersect the orbit of the curve with the canonical divisor and find a positive number"". I tried that and got: $$g(\sum C_i)= \sum C_i \Rightarrow \sum C_i =a K_X-$$ since the fixed part of the Picard group is generated by the canonical divisor (here $\sum C_i$ is the orbit of some curve). So after intersecting we have: $$\sum C_i\cdot K_X=a K_X \cdot K_X=a\cdot K_X^2$$ I do not see how is gives that $-K_X$ is ample (i.e. $X$ a dell Pezzo surface). I will appreciate any help!","['algebraic-geometry', 'birational-geometry']"
4132936,"Action of $S^1$ as a Lie Group on $S^2$ by ""rotation""","I'm studying Lie Groups and their actions on manifolds and I was looking for a ""concrete"" example of an action. All the stuff about $GL(n;\mathbb{R})$ acting on $\mathbb{R}^n$ is ok but I can't visualise it well, so I thought of an action by the Lie group $S^1$ on $S^2$ by ""rotation"" in this sense: every element of $S^1$ can be identified by and angle $\alpha$ given by the anti-clockwise rotation around the center $(0,0)$ ; in the same way we can parametrize $S^2$ using latitude and colatitude. The action that I came up with is simply $$
\begin{gather}
S^1 \times S^2 \longrightarrow S^2 \\
(\alpha,(\theta, \varphi))\longmapsto (\alpha + \theta, \varphi)
\end{gather}
$$ which ""spins"" $S^2$ around the vertical axis.
Is this actually an example of a Lie group acting on $S^2$ ? Are there any other more ""concrete"" examples I can look at?","['group-actions', 'lie-groups', 'differential-geometry']"
4132947,Proving $0/0$ case of L'Hospital's rule,"In the proofs of the $0/0$ case of L'Hospital's rule that I have seen so far, if we are interested in determining $$\lim_{x \to a}\dfrac{f(x)}{g(x)}$$ and given that $f, g$ are differentiable at $x = a$ and their derivatives are continuous at $x = a$ , one always assumes that $f(a) = g(a) = 0$ , but isn't the requirement; $$\lim_{x \to a} f(x) = \lim_{x \to a}g(x) = 0$$ is enough, meaning that the functions doesn't necessarily have to be defined at this point. The proof I'm referring to goes as follow: Given that $f(a) = g(a) = 0$ , we know that $$
\lim_{x \rightarrow a} \frac{f^{\prime}(x)}{g^{\prime}(x)}=L
$$ that means we can find a $\delta$ -neighborhood around $a$ such that if $x \in V_{s}(a)$ implies this limit is $\epsilon$ close to $L$ . Now, if we apply the GMVT to some points $x, a \in V_{\delta}(a)$ , WLOG $x>a$ , then we have $$
\frac{f(x)-f(a)}{g(x)-g(a)}=\frac{f(x)}{g(x)}=\frac{f^{\prime}(c)}{g^{\prime}(c)}
$$ for some $c \in(a, x) \subseteq V_{\delta}(a)$ . Since this $c$ is also in $V_{\delta}(a)$ , we conclude that $$
\left|\frac{f(x)}{g(x)}-L\right|=\left|\frac{f^{\prime}(c)}{g^{\prime}(c)}-L\right|<\epsilon
$$ which proves that $\lim _{x \rightarrow a} \frac{f(x)}{g(x)}=\lim _{x \rightarrow a} \frac{f^{\prime}(x)}{g^{\prime}(x)}=L$ What would one have to change such that the proof is also applicable if the functions $f$ and $g$ are not defined at $x = a$ and we only have $\lim\limits_{x \to a}f(x) = \lim\limits_{x \to a}g(x) = 0$ ? What would one have to change if $L = \infty$ ?","['limits', 'calculus', 'real-analysis']"
4133038,How do I prove that the intersection of all $A_j$ is non-empty?,"Let $A_j \subseteq X, j = 1, 2,\dots, N$ some sets of size $k$ each, different from each other and such that the intersection of any $k + 1$ of the sets $Aj$ is non-empty. How do I prove that the intersection of all $A_j$ is non-empty? The solution I came up with is to start with the claim that the intersection of all $A_j$ is empty. If $A_1 = \{x_1, x_2,\dots, x_k\}$ , then because the intersection of any $k + 1$ of the sets $A_j$ is non-empty, there will be some set from $A_2,\dots, A_N$ that don't include $x_1$ , that don't include $x_2$ etc, then the claim that the intersection of all $A_j$ is empty is wrong. So, the intersection of all $A_j$ is non-empty. Do you think this is correct, or should I think of something else?","['combinatorics', 'discrete-mathematics']"
4133043,Why do we need Noetherianness to conclude that a connected scheme is integral iff its local rings are?,"I'm trying to proof/understand the following Statement: If $X$ is a Noetherian and connected scheme, then $X$ is integral if and only if $X$ has integral stalks. It can quickly by verified that if $X$ is integral, all stalks have to be integral. It is the reverse implication that troubles me. Of course for a scheme being integral is the same as being irreducible and reduced. If $X$ has integral stalks, it has reduced stalks, and reducedness is a stalk-local property. Hence, if $X$ has integral stalks, it is reduced. It remains to show that $X$ is irreducible. If $Y\neq Z$ are two irreducible components of $X$ which meet at a point $p$ , and if $p$ is contained in some affine open $\operatorname{Spec} A$ , then $Y \cap \operatorname{Spec} A$ is a irreducible component of $\operatorname{Spec} A$ , hence corresponds to some minimal prime $\mathfrak p_Y$ of $A$ . But the same applies to $Z$ , so we have two different minimal primes $\mathfrak p_Y$ and $\mathfrak p_Z$ of $A$ , so $(0)$ isn't prime in $A$ , and $A$ is not integral. If $\mathfrak p$ is the prime corresponding to the point $p$ of $\operatorname{Spec} A$ , both $\mathfrak p_Y$ and $\mathfrak p_Z$ are contained in $\mathfrak p$ , therefore there are two different minimal prime ideals in $A_{\mathfrak p} = \mathcal O_{X,p}$ . Hence we have shown: If two different irreducible components meet at a point $p$ , $\mathcal O_{X,p}$ is not integral. But since all stalks are integral by assumption, every pair of irreducible components of $X$ has empty intersection, and since $X$ is connected there can only one irreducible component. My problem is that I don't know where we used Noetheriannes, but I guess it was in the last step. If $X$ is Noetherian this step will work for sure, as we can write $X$ as a finite union of irreducible components. But why would this fail if we had an infinite union of irreducible components?","['algebraic-geometry', 'schemes', 'noetherian']"
4133062,Complex power series converging at only one point on the unit circle,"The complex power series $$\lambda(z) = \sum_{n=0}^\infty \frac{z^n}{n}$$ Has radius of convergence $R=1$ , and converges for all complex $z$ on the unit circle except for $z=1$ . Does anyone know of a complex power series $$\sum_{n=0}^\infty a_n z^n$$ for which the opposite is true - so that it also has radius of convergence $R=1$ , but diverges for all complex $z$ on the unit circle except $z=1$ ? Or is there a reason why no such power series could exist?","['complex-analysis', 'convergence-divergence', 'power-series']"
4133092,A confusing test with $n$ questions with $k$ choices that can only be guessed,"Suppose there is a test with $n\geq1$ questions. For each question, there are no descriptions for either the questions or the choices for the answer, just $k\geq1$ choices to choose from, in which exactly one of them is correct. (This condition is to make sure that the answers can only be guessed.) For each submission of the test, any number of questions can be answered, and the score for that submission would be announced, but not which answers are correct or wrong. What are the optimal strategy and the maximum number of submissions in the worst case needed to get a full mark for any $n$ and $k$ ?",['combinatorics']
4133134,Proving $\sum_{j=0}^{n}\binom{n}{j}j^{n-2}(-1)^{n-j+1} = 0$,"Prove $$\sum_{j=0}^{n}\binom{n}{j}j^{n-2}(-1)^{n-j+1} = 0$$ My wrong proof try is this: $\sum_{j=0}^{n}\binom{n}{j}j^{n-2}(-1)^{n-j+1} = \sum_{j=0}^{n}\binom{n}{j}1^jj^{n-2}(-1)^{n-j}(-1) 
=-\sum_{j=0}^{n}\binom{n}{j}1^j(-1)^{n-j}j^{n-2}$ We know: $(\forall x,y \in R)(\forall n \in N) (x+y)^n = \sum_{k=0}^n \binom{n}{k} x^k y^{n-k}$ Therefore: $\sum_{j=0}^{n}\binom{n}{j}1^j(-1)^{n-j} = (1-1)^n = 0$ And $\sum_{j=0}^{n}\binom{n}{j}1^j(-1)^{n-j}j^{n-2} \leq \sum_{j=0}^{n}\binom{n}{j}1^j(-1)^{n-j}n^{n-2} \\
= n^{n-2}\sum_{j=0}^{n}\binom{n}{j}1^j(-1)^{n-j} \\
= 0$ And $
\sum_{j=0}^{n}\binom{n}{j}1^j(-1)^{n-j}j^{n-2} 
= \sum_{j=1}^{n}\binom{n}{j}1^j(-1)^{n-j}j^{n-2} \\
\geq \sum_{j=0}^{n}\binom{n}{j}1^j(-1)^{n-j}1^{n-2} \\
= 1\sum_{j=0}^{n}\binom{n}{j}1^j(-1)^{n-j} \\
= 0$ Therefore $0 \leq \sum_{j=0}^{n}\binom{n}{j}1^j(-1)^{n-j}j^{n-2} \leq 0 \Rightarrow \\
\sum_{j=0}^{n}\binom{n}{j}1^j(-1)^{n-j}j^{n-2} = 0$ And $\sum_{j=0}^{n}\binom{n}{j}j^{n-2}(-1)^{n-j+1} = 0$ I later realised that the comparison cant be made due to the $(-1)^n$ I have also tried by testing for n even and odd numbers. I can prove it for odd numbers but not for even. Do you have any other ideas?","['algebra-precalculus', 'summation', 'sequences-and-series']"
4133172,For which values of $\theta$ does $\sin(\theta)$ etc have closed form solutions?,"There are special arguments to trigonometric functions which produce closed-form results, e.g. $\sin(\frac{\pi}{4}) = \frac{1}{\sqrt 2}$ . I also recently learned that $\cos(\frac{\pi}{5})=\frac{\phi}{2}$ as shown here . We can extend these results, for example by using the angle-sum formule and half-angle formulae and get closed-form results for e.g. $\sin({\frac{1}{2}\frac{\pi}{5}})$ or $\sin({\frac{\pi}{4}+\frac{\pi}{5}})$ . Is there a completely-known set of all the closed-form values of the trig functions?",['trigonometry']
4133177,Solve over the positive integers: $7^x+18=19^y.$,"Solve over the positive integers: $$7^x+18=19^y.$$ Progress:-
I first took $\mod 7,$ so we get $5^y\equiv 4 \mod 7$ since $5$ is a primitive root of $7$ and $5^2\equiv 4\mod 7.$ So we get $y\equiv 2\mod 6.$ And then took $\mod 9$ So we get $7^x\equiv 1\mod 9.$ Since residues of $7^x$ are $\{7,9,1\}.$ We get $x\equiv 0\mod 3.$ Then I couldn't get any progress, I tried Zsigmondy,etc stuff and also noticed $7^x-1=19(19^{y-1}-1)$ Any hints? Thanks in advance.","['contest-math', 'modular-arithmetic', 'exponential-diophantine-equations', 'number-theory', 'elementary-number-theory']"
4133286,When to suppress parameter in function notation,"For example, a differential equation of the form $$\frac{dy}{dx}+P(x)y=f(x),$$ has the general solution $$y=\mu^{-1}\int\mu f(x)\,dx +C\mu^{-1},$$ where $\mu$ is the integrating factor $\mu(x)=\exp{(\int P(x)\,dx)}$ . So the way I've written this, the parameter for the integrating factor is suppressed when I write it down in the solution but when I defined it, I explicitly say it is a function of $x$ . However, for $f(x)$ , I write the parameter explicitly in the solution because it felt wrong otherwise. Further, for $y,$ I also suppressed the parameter. My question, then, is whether or not this is the best way to write it. Is there a general consensus on when to explicitly state the independent variable in the notation? And is it a problem that I'm somewhat inconsistent on when I did it?","['notation', 'functions']"
4133290,Prove that a function f is injective,"I will refer to Exercise 18 of Section 5.2 of Velleman's 2nd edition book. I am struggling proving the following: Suppose $A,B,C$ are sets and $f: B\rightarrow C$ . Suppose that $A\neq\varnothing$ , and for all functions g and h from $A$ to $B$ , if $f\circ g= f\circ h$ then $g=h$ . Prove that $f$ is one-to-one. I have tried two different approaches: 1) Using the assumptions just as they are, letting some $b,b'\in B$ and $f(b)=f(b')$ and trying to prove that $b=b'$ . However, I don't see how to prove that, since I have the impression I need to know more about the functions $g$ and $h$ . Since there is some $a\in A$ , I used this to some function $g$ and $h$ , but I don't really see how this helps me. 2) I tried proving by contradiction, but still think I need some functions $g$ , $h$ . I would appreciate any hints/guide you could provide me as to how to proceed with this proof. Thank you.","['proof-writing', 'functions']"
4133297,"Proving that a set with associative binary operation and satisfying two other given conditions, is a group.","Suppose that $\displaystyle G$ is a set with an associative binary operation such that Given $\displaystyle a,y\in G,$ there exists $\displaystyle x\in G$ such that $\displaystyle ax=y$ ; and Given $\displaystyle a,w\in G$ , there exists $\displaystyle u\in G$ such that $\displaystyle ua=w$ Then it is to be proven that $\displaystyle G$ is a group. I started as follows: By (1), there exists $\displaystyle x\in G$ such that $\displaystyle ax=a$ and by (2),there exists $\displaystyle u\in G$ such that $ $$\displaystyle ua=a$ . We have $\displaystyle ax=( ua) x=u( ax)$ and now by property (1), there exists $\displaystyle t\in G$ such that $\displaystyle ( ax) t=x$ . It follows that \begin{equation}
x=( ax) t=( u( ax)) t=u(( ax) t) =ux\ \  \tag{A}
\end{equation} $\displaystyle \ \ $ Similarly, \begin{equation}
ua=u( ax) \Longrightarrow u=ux \tag{B}
\end{equation} By (A) and (B), we get $\displaystyle u=x$ and we conclude that $\displaystyle \forall a\in G,\exists x\in G\ ( ax=xa=a)$ Now I claim that $\displaystyle x$ is the universal identity. For any $\displaystyle b\in G,\exists y\in G$ such that \begin{equation}
by=yb=b \tag{B'}
\end{equation} Now by using properties (1) and (2), we have \begin{gather}
xb=xyb\Longrightarrow xy=xy^{2} \Longrightarrow y^{2} =y^{3} \Longrightarrow x=xy\Longrightarrow y=y^{2} \tag{C}\\
bx=byx\Longrightarrow yx=y^{2} x=y( yx) \Longrightarrow x=yx=xy \tag{D}\\
ya=yxa\Longrightarrow yx=yx^{2} =( yx) x=( xy) x=x( yx) \Longrightarrow y=xy=x \tag{E}
\end{gather} Therefore, for any arbitrary $\displaystyle b\in G$ , we have $\displaystyle bx=xb=b$ . Therefore, by definition of identity $\displaystyle x\in G$ is the identity. Let's denote $\displaystyle x$ by $\displaystyle 1$ . Existence of inverse: By properties (1) and (2), for any $\displaystyle b\in G,\exists ( t,s) \in G^{2}$ such that $\displaystyle bt=1$ and $\displaystyle sb=1$ \begin{equation*}
t=1t=sbt=s( bt) =s( 1) =s
\end{equation*} By definition of inverse elements, every $\displaystyle b\in G$ has an inverse $\displaystyle s$ . Let's denote it by $\displaystyle b^{-1}$ . Explanation for (C): $\displaystyle xb=x( yb)$ (by (B')) By property (1), there exists $\displaystyle g\in G$ such that $\displaystyle bg=y$ . It follows that $\displaystyle xbg=xy=( x( yb)) g=xy^{2} \Longrightarrow xy=xy^{2}$ and then by property (2), there exists $\displaystyle g'\in G$ such that $\displaystyle g'x=y$ whence it follows that $\displaystyle y^{2} =y^{3}$ etc. Similarly for (D) and (E). Is my proof correct? Thanks. \begin{equation*}
\end{equation*} \begin{equation*}
\end{equation*}","['group-theory', 'solution-verification', 'semigroups']"
4133300,Proving Chebychev's Inequality using Markov's Inequality,"I am trying to prove: Suppose $(X, \mathcal S, \mu)$ is a measure space with $\mu(X) = 1$ and $h\in \mathcal L^1(\mu)$ . Prove that \begin{align}
    \mu\left(\left\{ x\in X: \left|h(x) - \int h\; d\mu\right|\ge c
    \right\}\right) \le
    \frac{1}{c^2} \left(\int h^2 \; d\mu - \left(\int h \; d\mu
    \right)^2\right)
\end{align} for all $c>0$ . Attempt: Suppose $(X, \mathcal S, \mu)$ is a measure space with $\mu(X) = 1$ and $h\in \mathcal L^1(\mu)$ . Fix $c>0$ . Markov's Inequality: $\mu\left(\left\{x \in X: |f(x)|\ge c
    \right\}\right) \le \frac{1}{c^2} \|f\|_1 = \frac{1}{c^2} \cdot \int\left|f\right|\; d\mu$ . Take $f(x) = h(x) - \int h \; d\mu$ in Markov's inequality: \begin{align*}
    \mu\left(\left\{x \in X: \left|h(x) - \int h \; d\mu\right|\ge c \right\}\right) 
    &\le \frac{1}{c^2} \int\left|h(x) - \int h \; d\mu\right|\; d\mu \\
    &\le \frac{1}{c^2} \int\left(h(x) - \int h \; d\mu\right)^2\; d\mu\\
    &= \frac{1}{c^2} \int\left[ h^2(x) +\left(\int h \; d\mu\right)^2 - 2h(x) \int h(x) \; d\mu\right] d\mu \\
    &= \frac{1}{c^2}\left( \int h^2(x)\; d\mu +\int\left[\left(\int h \; d\mu\right)^2 - 2h(x) \int h(x) \; d\mu\right] d\mu\right)
\end{align*} I am not sure how to proceed from here. I also know that I still have to use $\mu(X) = 1$ . Intuitively, I think I should be using the property that integration is homogeneous, but $h(x)$ is not independent of $x$ , of course. Can someone please provide some hints?","['measure-theory', 'proof-writing', 'solution-verification', 'real-analysis']"
4133339,ODE solution is monotonic with respect to initial value?,"Consider the ODE $$\frac{dy}{dt}=f(y,t), \quad y(0)=a,$$ where we may assume that $f$ is Lipschitz continuous and there is a unique solution $y(t)$ on $[0,2]$ . My questions is following: Is $y(1)$ a monotone (more precisely, increasing) function of $a$ ? I believe the claim holds true because the direction fields never cross, and thus if a solution curve $y_1(t)$ is above $y_2(t)$ at $t=0$ , then it will always be above $y_2$ . If you look at a simple example like logistic equation, you'll see that $y(1)$ is indeed increasing with respect to $y(0)$ . The continuous dependence on initial data has been well-studied but I can not find any result or even discussion on the monotonicity.","['initial-value-problems', 'ordinary-differential-equations', 'dynamical-systems']"
4133403,"Let $f,g$ be differentiable and continuous functions on $[a,b]$, and let $f(a)=f(b)=0$, do we always have $c \in (a,b)$ s.t $g'(c)f(c)+f'(c)=0$?","I tried to prove this with the fact "" $f(x)g(x)=h(x)=0$ "" when $x=a,b$ . So, by Rolle's theorem, we know there exists $c \in (a,b)$ s.t $h'(c)=f'(c)g(c)+f(c)g'(c)=0$ . Then, for such $c$ , $g'(c)f(c)=-f'(c)g(c)$ , so $f'(c)+g'(c)f(c)=f'(c)(1-g(c))$ . But, if we say $f'(c)(1-g(c))=0$ is true, then this implies $f'(c)=0$ because we have no information for $g(x)$ . Here is my question: [If we know that $f(a)=f(b)=0$ , then we know there exists some $c_1 \in (a,b)$ s.t $f'(c_1)=0$ . Combining with this fact, I wonder if this $c_1$ is guaranteed to be equal to $c$ from above?]","['derivatives', 'real-analysis']"
4133436,Definition of Upper/Lower Riemann Sums,"Let $f: [a,b] \rightarrow \mathbb R$ be a function defined on a closed interval $[a,b]$ of the real numbers, $\mathbb R$ , and $\mathcal P = \left \{[x_0,x_1],[x_1,x_2],\dots,[x_{n-1},x_n] \right \}$ , be partition of the closed interval $[a,b]$ , where $ a=x_0<x_1<x_2<\cdots<x_n=b $ . One, perhaps the , way of defining the Riemann upper sum is as follows. $ U = \displaystyle{\sum_{i=1}^{n} f(x_i^*)\, \Delta x_i }$ where $ \Delta x_i=x_i-x_{i-1} $ , $ x_i^*\in[x_{i-1},x_i]$ , and $f(x_i^*) = \sup f([x_{i-1},x_i]) $ (that is, the supremum of $f$ over $[x_{i-1},x_i]$ ). My question is how does this work for a function with a discontinuity at what would be it's maximum in a particular sub-interval $[x_{i-1},x_i]$ ; say the function $f$ has a `hole' at the point $c\in [x_{i-1},x_i]$ ? For example, take $f(x) = -x^2 + 1$ on $[-1,1]$ with $(0,1)$ removed, and consider a partition with one of the sub-intervals $[-\epsilon, \epsilon]$ , $\epsilon >0$ . In this situation, what value do you pick for the $x^*$ value in the sub-interval? I mean, it's clear that you'd take a rectangle of height 1 for this bin, since that is the supremum of the set of values of $f$ on this interval, but what value do you take for $x^*$ ? Does it matter, since the supremum for the set doesn't depend on the chosen value for $x^*$ ? I think my misunderstand is how to interpret choosing an $x^*_i \in [x_{i-1},x_i]$ when we are really using the supremum of the set of values of $f$ on $[x_{i-1},x_i]$ . Does it matter which value we pick for $x_i^*$ , since the supremum of the set is fixed?","['integration', 'analysis', 'real-analysis', 'riemann-sum', 'riemann-integration']"
4133445,Why do we need to use opposite categories/contravariant functors,"I know this is obviously a really dumb question apologies, but I’m currently trying to learn Category Theory and I’m struggling to find a resource/textbook that doesn’t just define these concepts in a really unmotivated way and then move on. Looking at an opposite category doesn’t seem to add any new or interesting structure to the things you’re studying, and the correspondence between morphisms is so natural and trivial that it seems really unclear what you can actually “do” with the opposite category that you can’t just do with C. Contravariant functors then just seem to be less intuitive ways to talk about basically the same stuff Clearly this is just me being dumb/not finding the right material yet but can someone point out what I’m missing? Even just an example where thinking in terms of opposite categories/contravariant functors is actually much more natural/illuminating?","['abstract-algebra', 'category-theory']"
4133449,$1/E(x)$ vs. $E(1/x)$,"Question concerning MVUE and geometric distribution, trying to apply Rao-Blackwell Theorem here.
We know that the geometric distribution is a regular exponential class with $$Y = \sum x$$ as our sufficient and complete statistic. However $$ E(Y) = \frac{n(1- \theta)}{\theta} $$ is not an unbiased estimator because it does not equal theta. Since 1/E(X) is usually not E(1/X), I tried $$ E(\frac{1}{y}) = \sum\frac{1}{y}\theta(1-\theta)^\frac{1}{y} $$ but that's where I got royally stuck. Is there a way around this summation so that I can get $$ E(Y) = \theta $$","['expected-value', 'statistics', 'probability-distributions', 'probability']"
4133456,Carroll's interpretation of 1-forms,"This question is crossposted from 1 . Carroll writes in his Spacetime and Geometry book on page 68 that ""[...] in fact, however, we could just as well have begun with an intrinsic definition of one-forms and used that to define vectors as the dual space. Roughly speaking, the space of one-forms at $p$ is equivalent to the space of all functions that vanish at $p$ and have the same second partial derivatives. In fact, doing it that way is more fundamental, if anything, since we can provide intrinsic definitions of all $q$ -forms (totally antisymmetric tensors with $q$ lower indices), etc."" Could somebody explain this equivalency in a bit more detail? When I asked this question on 1 I got several replies but the more I thought of the consensus answer, namely that it must be a ""typo"" the less satisfied I was. So I wrote to Prof. Carroll and asked him directly but his answer was quite laconic and cryptic ""Nope, it is correct. If the first derivatives were the same, they'd be the same one-forms."" Now I am totally confused, and I have another question: what does Carroll mean by this?","['differential-forms', 'differential-geometry']"
4133484,Surjective function combinatorics,"I think I'm having a math block these days with things I shouldn't have problems with. Having $|A| = n$ (the number of elements of A is n), and $|B| = p$ , and ( $n \geq p$ ), how many surjective functions should I have? I was developing the following reasoning: $\sum _{k=1}^{p}x_{k}=n$ is the problem associated with solving n arrows inside p boxes, since we have to complete the counterdomain, I should transform this problem with the change of variables $y_{k}=x_{k}-1$ . This gets me to $\sum _{k=1}^{p}y_{k}=(n-p)$ , and the result $\frac{(n-1)!}{(n-p)!(p-1)!}=C_{n-1}^{n-p}$ . I tried some inclusion-exclusion with it, but I can't construct a sequence... The book says the answer is $\sum _{k=0}^{p}(-1)^{k}C_{p}^{k}(p-k)^n$ Obs.: $C_{n}^{p}=\frac{n!}{(n-p)!(p)!}$","['inclusion-exclusion', 'combinatorics']"
4133504,Continuous function nowhere Differentiable,"Let $D(x):\mathbb{R}\rightarrow\mathbb{R}$ be: $$ D(x)=\sum^\infty_{k=1}\frac{1}{k!}\sin((k+1)!x)$$ Prove that $D(x)$ is nowhere differentiable. What I've done is that I supposed that exist a $x\in \mathbb{R}$ where $D(x)$ is differentiables, so: $$\lim_{h\rightarrow0} \frac{D(x+h)-D(x)}{h}=a \Rightarrow$$ $$\lim_{h\rightarrow0} \frac{\sum^\infty_{k=1}\frac{1}{k!}\sin[(k+1)!(x+h)]-\sum^\infty_{k=1}\frac{1}{k!}\sin((k+1)!x)}{h}=$$ $$\lim_{h\rightarrow0} \frac{\sum^\infty_{k=1}\frac{1}{k!}[\sin[(k+1)!(x+h)]-\sin((k+1)!x)]}{h}=$$ $$\sum^\infty_{k=1}\frac{1}{k!}\lim_{h\rightarrow0}\frac{ 2\sin(\frac{(k+1)!(x+h)-(k+1)!x}{2})\cos(\frac{(k+1)!(x+h)+(k+1)!x}{2})}{h}=$$ $$\sum^\infty_{k=1}\frac{1}{k!}\lim_{h\rightarrow0}\frac{ 2\sin(\frac{(k+1)!h}{2})\cos(\frac{(k+1)!(2x+h)}{2})}{h}=\frac{0}
{0} $$ Using L'hopital: $$ \sum^\infty_{k=1}\frac{1}{k!}\lim_{h\rightarrow0}\frac{\cos(\frac{(k+1)!h}{2})(\frac{(k+1)!}{2})\cos(\frac{(k+1)!(2x+h)}{2})-\sin(\frac{(k+1)!h}{2})(\frac{(k+1)!}{2})\sin(\frac{(k+1)!(2x+h)}{2})}{1} =$$ $$ \sum^\infty_{k=1}\frac{1}{k!}\lim_{h\rightarrow0}[(\frac{(k+1)!}{2})\cos(\frac{(k+1)!(2x+h)}{2})] =$$ $$ \sum^\infty_{k=1}\frac{1}{k!}(\frac{(k+1)!}{2})\cos((k+1)!(x)) = \sum^\infty_{k=1}\frac{k+1}{2}\cos((k+1)!(x)) $$ I don't know how to continue.","['differential', 'analysis', 'real-analysis']"
4133551,Find $\limsup\limits_{x\rightarrow\infty}\ (\sin(x)+\sin(\pi x))$,I'm studying for my Qualifying exams and this was one of the questions in the question bank under real analysis section. I'm currently stuck on this question. I think the answer is 2 but don't have a rigorous proof. Find $\limsup\limits_{x\rightarrow\infty}\ (\sin(x)+\sin(\pi x))$ . My attempt: I tried to look at the sequence $x_n=\frac{1}{2}+2n$ but not sure how to calculate further and find the $\limsup$,"['limsup-and-liminf', 'sequences-and-series', 'real-analysis']"
4133565,Prove that $h(x)= f(x) - g(x)$ is even degree [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question If $f$ , $g$ are polynomial functions such that $f(x) = g(x)$ and $f''(x) = g''(x)$ have no real roots i) Prove that $h(x) = f(x) - g(x)$ is a polynomial of even degree ii) $f'(x)= g'(x)$ has exactly one real root I find it difficult to approach the first question. Any help would be highly appreciated. Thank you in advance","['calculus', 'derivatives', 'polynomials']"
