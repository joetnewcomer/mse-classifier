question_id,title,body,tags
4537416,Regular submanifolds in practice.,"A subset $S$ of a smooth $m$ -manifold $M$ is called regular submanifold of codimension $k$ if for every $p\in S$ , there is a coordinate neighborhood $(U, \phi)= (U, x^1, \dots, x^m)$ in the atlas of $M$ such that $x^1 = \dots = x^k =0$ on $U \cap S$ . Then, there are theorems like: Let $g: M\to \mathbb{R}$ be a $C^\infty$ -function. Then a non-empty regular level set $S=g^{-1}(c)$ is a regular submanifold of $M$ of codimension $1$ . Question : (From someone ignorant about differential geometry!) What is the practical use of knowing that a certain subset $S\subseteq M$ is a regular submanifold? We know it is a manifold in some natural way, but do we actually understand its structure? It is my understanding that the charts of such a submanifold are constructed using abstract results like the inverse function theorem, so we don't really understand how an atlas of this submanifold looks like. Say, for example, we are given a regular submanifold $S\subseteq M$ which we know is a submanifold by the above theorem. We have a function $S \to N$ where $N$ is some other manifold. We don't have an explicit atlas for $S$ , so how do we for example show that the function $S\to N$ is smooth?","['manifolds', 'submanifold', 'smooth-manifolds', 'differential-geometry']"
4537426,Example for which the Donaldson-Futaki invariant is $0$,"Background: Let $X$ be a Fano variety, and consider a test configuration for $(X,-K_X)$ , tha is a pair $(\mathcal{X}, \mathcal{L})$ where: $\mathcal{X}$ is a normal variety, endowed with a $\mathbb{C}^*$ -action; there is a flat $\mathbb{C}^*$ -equivariant morphism $f:X\to \mathbb{P}^1$ , with $\mathbb{C}^*$ acts on $\mathbb{P}^1$ as $[tx:y]$ ; $\mathcal{L}$ is an $f$ -ample line bundle on $\mathcal{X}$ , and there is a $\mathbb{C}^*$ -equivariant isomorphism $$(\mathcal{X}\setminus \mathcal{X}_0, \mathcal{L}|_{\mathcal{X}\setminus \mathcal{X}_0})\simeq (X\times (\mathbb{P}^1\setminus 0), \text{pr}_1^\star(-K_X)),$$ where $0=[0:1]$ and $\text{pr}_1 : X\times \mathbb{P}^1\to X$ . With this setting, one defines the Donaldson-Futaki invariant as $$DF(\mathcal{X},\mathcal{L})=\frac{1}{(-K_X)^n}(\mathcal{L}^n\cdot K_{\mathcal{X}|\mathbb{P}^1}+\frac{n}{n+1}\mathcal{L}^{n+1}),$$ with $n=\dim X$ , and one says that the Fano variety is semistable if for every test configuration one has $DF(\mathcal{X};\mathcal{L})\geq 0$ ; polystable if it is semistable, and $DF(\mathcal{X},\mathcal{L})=0 \iff (\mathcal{X},\mathcal{L})$ is of product type, i.e. it holds $\mathcal{X}\setminus \mathcal{X}_\infty\simeq X\times (\mathbb{P}^1\setminus \infty)$ , with $\infty=[1:0]$ . Question: From this definition it seems easy to see that, if a test configuration is of product type, then its Donaldson-Futaki invariant is $0$ . However, I am having troubles proving it, or at least even convincing myself it should be true. Seeing the expression, $DF=0$ should correspond to having $\mathcal{L}=-K_{\mathcal{X}\mid \mathbb{P}^1}$ , but still I don't see why we have the $\frac{n}{n+1}$ factor. Moreover, so far most of the proofs of stability of Fano varieties I've seen use different approachs, instead of computing the DF-invariant. Any help would be much appreciated!","['complex-geometry', 'algebraic-geometry', 'birational-geometry', 'differential-geometry']"
4537433,Numerical integration of functions of bounded variation,"Let $f: \mathbb{R} \rightarrow \mathbb{R}$ be such that $f\in BV(\mathbb{R}) \cap L^1(\mathbb{R}).$ Now, since $f\in BV(\mathbb{R})$ pointwise values makes and consequently we can define numerical approximations of $\int\limits_{\mathbb{R}}f(x)dx,$ via various well known formulas such as rectangle rule, midpoint rule, trapozoidal rule, simsons rule etc. Do they still converge  ( $f$ need not be continuous)  as in the case of sufficiently smooth $f$ ? If so, what is the rate of the convergence?
Is it same as what we get in the smooth case? P.S.: I feel it works at-least for rectangle and midpoint rule because $BV$ functions satisfy $\left| \int f(x) dx - \int f(x-h)dx \right|\leq \int\limits_{\mathbb{R}}|f(x+h)-f(x)| \leq Ch.$ Thanks in advance.","['integration', 'analysis', 'real-analysis', 'numerical-calculus', 'numerical-methods']"
4537441,Question about Taylor series expansion in complex numbers involving branch cut,"I want to consider a Taylor series expansion over complex numbers of $(i + z)^{-1/2}$ around $0$ .
Using the usual formula I get $$
(i + z)^{-1/2} = i^{-1/2} - \frac{1}{2} i^{-3/2} z + .... 
$$ I can see that the series converge around $|z| < 1$ . When I was computing this, I just used the formula. But then I realized $i^{-m/2}$ is apriori not well defined since there are more than one choice for this... so the above formula doesn't make sense as it is. How can I fix this issue? Choosing arbitrary choice of $i^{-m/2}$ for each $m$ doesn't sound a good idea.","['complex-analysis', 'power-series', 'taylor-expansion']"
4537531,What can you say about the limit of a series as a linear transformation?,"I have a question.
Let $V$ be the space of convergent real infinite sequences.
I know that the following function: $lim: V \rightarrow \textbf{R}, lim(a_n) = lim_{n\rightarrow \infty} a_n$ is a linear transformation (Because $lim(a_n + b_n) = lim(a_n) + lim(b_n), and\ lim(ca_n) = clim(a_n)$ ). Is there anything special about this from a linear algebra perspective? The only thing I could find is that it's a functional. Is there anything special you can say about this from an abstract algebra persepctive? Thanks!","['limits', 'abstract-algebra', 'linear-algebra', 'real-analysis']"
4537610,Beilinson's theorem over any base.,"I am not a specialist in spectral sequences and I am trying to apply Beilinson's approach to vector bundles on $\mathbb{P}^1_A$ , where $A$ is some good enough ring (e.g. $\mathbb{Z}$ or PID). Where does the proof of Beilinson's theorem about spectral sequences fail when we take as a base of $\mathbb{P}^n$ not a field, but a ring? We first construct the Koszul resolution for the diagonal, apparently this is true for any base (e.g. by direct coordinate calculations using Euler sequence as in the second proof on pg. 65 in https://johncalab.github.io/stuff/beilinson.pdf ). After that, we build and calculate hyperdirect images of second projection of our complex, as in the book by Okonek-Schneider-Spindler, which also seems to be independent of the base, and as a result we obtain Beilinson's Theorems. If this is not true, then is it possible to point out a place where our proof fails for the simplest case – $F$ is an (indecomposable) locally free sheaf of rank 2 on $\mathbb{P}^1_{\mathbb{Z}}$ with $H^1(\mathbb{P}^1_\mathbb{Z},F)=0$ . Or is everything fine and this theorem is also true for rings?","['spectral-sequences', 'algebraic-geometry']"
4537646,How efficiently can the first-order theory of posets recognize the open set poset of $\Bbb R^n$?,"This question requires me to write quite a bit of background, so I apologize. Suppose our domain of discourse is the set of open subsets of a topological space. This means that, by $\forall A$ I mean ""for all open sets $A$ "" and by $\exists A$ I mean ""there exists an open set $A$ "". Consider the following property: \begin{align}
\forall A_1,A_2,\color{red}{[(\exists Y_1:Y_1\subseteq A_1}&\color{red}{{}\land\lnot(Y_1=A_1))\land(\exists Y_2:Y_2\subseteq A_2\land\lnot(Y_2=A_2))]\implies}\\
[\exists X_1,X_2:X_1\ne X_2\land{}(
&(A_1\subseteq X_1\land A_1\subseteq X_2\land A_2\subseteq X_1\land A_2\subseteq X_2)\lor{}\\
&(X_1\subseteq A_1\land X_2\subseteq A_1\land X_1\subseteq A_2\land X_2\subseteq A_2))]\\
\end{align} In words: for any two $\color{red}{\text{nonempty}}$ open sets $A_1$ and $A_2$ , there are two distinct open sets either containing both or contained in both. For what topological spaces is this valid? Take a moment to consider. Spoiler: It turns out that a topological space satisfies this condition iff the space is connected. (EDIT: Originally I had forgotten to add in a requirement forcing $A_1$ and $A_2$ to be nonempty. The formula is much less elegant now.) This is a formula in the first-order theory of posets , applied to the open set poset of a topological space. The first-order theory of posets has only one relation, namely the order (which I denote by $\subseteq$ ), and has no constants or functions. Every topological space has an associated poset of open sets, ordered by inclusion. I am interested in which properties of topological spaces are first-order expressible in this language. Here are some other first-order expressible properties: $X={\rm all}\quad$ iff $\quad\forall A,X\subseteq A\implies X=A$ $X=\emptyset\quad$ iff $\quad\forall A,A\subseteq X\implies X=A$ $X={\rm all}\setminus\overline{\{p\}}$ for some $p\quad$ iff $\quad X\ne{\rm all}\land\forall A,X\subseteq A\implies X=A\lor A={\rm all}$ (Note that for T1 spaces, $\overline{\{p\}}=\{p\}$ .) $X=A\cap B\quad$ iff $\quad X\subseteq A\land X\subseteq B\land(\forall Y,Y\subseteq A\land Y\subseteq B\implies Y\subseteq X)$ $X=A\cup B\quad$ iff $\quad A\subseteq X\land A\subseteq Y\land(\forall Y,A\subseteq Y\land B\subseteq Y\implies X\subseteq Y)$ $X$ is connected $\quad$ iff $\quad\lnot\exists A_1,A_2:A_1\ne\emptyset\land A_2\ne\emptyset\land A_1\cap A_2=\emptyset\land A_1\cup A_2=X$ (If you fully expand that out for the special case $X={\rm all}$ , then you get the example I led with.) We cannot talk about points, arbitrary subsets, functions, or families of open sets in this language. Consider, finally, this property: $$\exists A_1,A_2:A_1,A_2\text{ are connected}\land(A_1\cup A_2={\rm all}\setminus\overline{\{p\}}\text{ for some }p)\land{}\\\lnot(A_1\cap A_2\text{ is connected})$$ In words, there are two connected open sets whose union is everything but a point and whose intersection is disconnected. This is a first-order property because of the formulas I wrote out above. (Technically I am not allowed to quantify over points $p$ , but this is OK because it is really shorthand for one of the formulas above.) I will not prove it, but this property is true of $\Bbb R^2$ and false of $\Bbb R^3$ ! Take a moment to convince yourself of this. In fact, I believe this should be true of $\Bbb R^2$ but false of $\Bbb R^m$ , $m\ne2$ . Out of all Euclidean spaces, this formula picks out the plane. I believe I can generalize this to pick out $\Bbb R^n$ for all $n$ ; that is, for any $n$ , I can give you a formula that's true of $\Bbb R^n$ but false of $\Bbb R^m$ , $m\ne n$ . The only issue is, as far as I can tell, $n$ grows, so does the quantifier rank . The quantifier rank of a formula is the depth of nesting quantifiers. (That very first formula I wrote, about a space being connected, has quantifier rank 4, since the deepest part has the form $\forall\forall\exists\exists$ .) To finally arrive at my question: What's the most efficient way to do this? Specifically: For each $n$ , what's the minimal quantifier rank of a first-order formula that's true of $\Bbb R^n$ but false of $\Bbb R^m$ , $m\ne n$ ? I would also be interested in answers involving other ways to measure simplicity than quantifier rank.","['predicate-logic', 'first-order-logic', 'logic', 'order-theory', 'general-topology']"
4537674,$\exists f:\mathbb{R}\to\mathbb{R}$ such that $\frac{f(X_1)}{X_1+X_2}$ is independent of $X_1$?,"Let $X_1, X_2$ be independent real random variables. Does there exist a non-zero measurable function $f:\mathbb{R}\to\mathbb{R}$ such that $$\frac{f(X_1)}{X_1+X_2}\text{ is independent of }X_1?$$ I am especially interested in the case when $X_1, X_2$ are continuous, but we do not assume anything else about them. My ideas are the following: if $\frac{f(X_1)}{X_1+X_2 }$ does not depend on $X_1$ , then $\frac{f(X_1)}{X_1+X_2}=g(X_2)$ . Rewriting this we get $X_1+X_2 = f(X_1)g^{-1}(X_2)$ . But on the left-hand side, we have a sum, and on the right side a product. Hence, that's a contradiction.  However, these steps are not rigorous and I am not really sure if I can make them. Especially, the existence of $g$ probably does not hold.","['measure-theory', 'measurable-functions', 'probability-theory', 'probability', 'random-variables']"
4537733,Understanding Lang's application of two-minus sign products,"This is very basic stuff from the first chapter of Lang's Basic Mathematics , but I can't get past it. Lang here uses a previously established rule to prove that (-1)(-1) = 1 but I cannot reason out his steps. How is what he's worked out an application of the rule he cites? What's a and what's b here? I tried doing it on my own and couldn't reason through it.","['proof-explanation', 'algebra-precalculus']"
4537737,Limit of Exponential and its Integral,"Consider a function $h : \mathbb{R} \rightarrow \mathbb{R}$ without singularities such that $h(x) \nearrow \infty$ with $x \rightarrow \infty$ . Let $g(x)$ defined by $$
g(x) = \exp(-h(x)) \int_0^x \exp(h(s)) ds.
$$ Playing around with Desmos, it seems that if $h(x)$ grows at approximately a linear rate, that is to say $\lim_{x \rightarrow \infty} \frac{h(x)}{x} = c \in \mathbb{R}$ , then $g(x)$ remains bounded away from both $0$ and $\infty$ . Meanwhile, if $h(x)$ grows faster than every line, then $g(x) \rightarrow 0$ , and if $h(x)$ grows slower than every line, $g(x) \rightarrow \infty$ . I believe I have a (partial) proof in the case that $h(x)$ is strictly increasing and differentiable, but I suspect this holds in a much more general setting. How can this be shown? Does there exists results that are related to functions of this form? Edit : This was my approach for the case $h(x)$ being differentiable and strictly increasing at some minimum rate $\alpha > 0$ . Write $h(x) = ax + k(x)$ , for $a > 0$ and for some $k(x)$ differentiable and $o(x)$ . Since $h(x)$ is non-singular, we have that $\sup_{x \in \mathbb{R}} k'(x) \leq m_1$ ( this seems questionable, actually ). Thus, we have that $$
\frac{d}{dx} e^{h(x)} \leq (a + m_1) e^{h(x)}.
$$ Integrating this becomes $$
e^{h(x)} - e^{h(0)} \leq \int_0^x (a + m_1) e^{h(s)} ds \qquad \Rightarrow \qquad g(x) = e^{-h(x)} \int_0^x e^{h(s)} ds \geq \frac{1 - e^{h(0) - h(x)}}{a + m_1}.
$$ Since $h(x)$ is everywhere increasing with a minimum rate $\alpha > 0$ , we have that $\inf_{x \in \mathbb{R}} k'(x) \geq m_2$ with $a > m_2$ . Performing the same as above and taking $x \rightarrow \infty$ , we finally obtain that $$
0 < \frac{1}{a + m_1} \leq \lim_{x \rightarrow \infty} g(x) \leq \frac{1}{a + m_2} < \infty.
$$ I have yet to show that this behaves as predicted if $h(x)$ grows at non-linear rates. Edit 2 : As EnEm pointed out in the comments, it is not difficult to find a function $h(x)$ that grows faster than any line but for which $g(x)$ does not go to $0$ . It still remains very likely that $h(x)$ with linear growth still forces $g(x)$ to avoid $0$ and $\infty$ under fairly weak conditions.","['integration', 'limits', 'calculus', 'asymptotics']"
4537762,Noetherian Rings in Nonstandard Framework,"I have been trying to go through some algebraic geometry using the nonstandard framework. Noetherian rings are of course fundamental in this subject and it is characterized by the attribute that every ideal is finitely generated. Let $R$ be a ring and $I$ is an ideal in $R$ . Let $^*R$ be the enlargment of $R$ . There are two entities created in the enlargment. $^*I = $ the extension of $I$ in this enlargment. $(I) = $ the smallest ideal in $^*R$ containing $I$ . It seems that characterization of Noetherian rings mentioned above equates to saying that $(I) = {}^*I$ . The proof of this is are simply applications of the transfer and idealization principles. I think this is interesting because usually during enlargments, the resulting extensions of sets not explicitly known and you may get a crazy number of additional points. The Noetherian property basically limits how crazy these additional points can get. I'm sure I'm not the first to come across this characterization but I cannot find any resource on using nonstandard analysis to study noetherian rings, even in the context of algebraic geometry. Maybe I'm not looking in the right direction. If anyone can suggest some resources, or if you find this characterization interesting, can suggest how I should continue further in this nonstandard Noetherian ring direction, it would be much appreciated.","['algebraic-geometry', 'nonstandard-analysis', 'noetherian']"
4537770,"Integration of $\int\sin^{3}2x\, dx.$ using IBP","I am attempting to solve this integral: $$\int\sin^{3}2x\, dx.$$ Using an identity, I have manipulated the integral into this: $$\int\frac{1}{2}\left(1-\cos4x\right)\sin2x dx.$$ From here, using IBP, I let $u$ = $1-cos4x$ and $v'$ = sin2x However, I obtained an answer of $\cos2x+\frac{1}{2}\cos2x\cos4x\ -\ \frac{1}{6}\cos6x$ , which is nowhere near the intended answer in the solutions of $\frac{1}{6}\cos^{3}2x-\frac{1}{2}\cos2x$ . Any help on why my solution/method is incorrect will be appreciated.","['integration', 'indefinite-integrals']"
4537812,Evaluate $\int_0^\infty \frac{t^2 - \sin^2 t}{t^4} dt$,"Evaluate $\int_0^\infty \frac{t^2 - \sin^2 t}{t^4} dt$ . The integrand is clearly positive as for all $t > 0, t > |\sin t|.$ I'm not sure if finding the Fourier series of some functions might be useful. Or maybe a Taylor expansion or telescoping sum might be useful? $\sin^2 t$ has period $\pi$ , so we can split the integral into the sum $\sum_{k=0}^{\infty} \int_{k\pi}^{(k+1)\pi} \frac{t^2 - \sin^2 t}{t^4} dt.$ This sum, even if valid, doesn't seem very useful. In hindsight, it seems the question was fairly straightforward, provided one can accept the fact that $\int_0^\infty (\sin x)/xdx = \pi/2.$ Many proofs are provided here for instance.","['integration', 'calculus', 'fourier-analysis', 'real-analysis']"
4537850,How do I evaluate $\displaystyle \lim_{x \to -\infty}\left(\sqrt{x^2-8x+1}-x\right)$?,"As asked in the title.
I want to know how can I show that the value approaches to positive infinity as x approaches to negative infinity,without looking at its graph.
Besides, I know that I can use the formula $\displaystyle (a+b)(a-b)=a^2-b^2$ , but then I don't know what should I do next.","['limits', 'calculus']"
4537859,Finite state Markov chain must come back to absorption states,"Consider Markov chain $\{X_n\}_{n=0}^\infty$ on a finite state space $V = [k] = \{1,...,k\}.$ Let $\mathcal{A}$ be the set of absorbing states ( $i \in \mathcal{A}$ $\Rightarrow$ $p_{i,i} = P(X_1 = i |X_0=i) = 1$ ). Assume that for all $i \notin \mathcal{A}$ and all $j \in V$ there is $n \geq 1$ such that $P(X_n=j|X_0=i) > 0$ , then $P(X_n \in \mathcal{A} \text{ for all large n} |X_0=i) = 1$ for all $i \in V$ . How to start? What we know is that starting at any state (absorbing or not) we can get to an absorbing state and never leave once we are there. If $i \in \mathcal{A}$ we are done since once we are in here we never leave. If $i \notin \mathcal{A}$ , then there $n \geq 1$ such that $P(X_n \in A|X_0=i) > 0$ and once we are in here then again we do not leave. But we are only given that this probability is positive, not necessary guaranteed to happen. Also this is not rigorous at all, how could I write this more rigorous? Edit: I stated the question incorrently but it is fixed now.","['conditional-probability', 'markov-chains', 'probability-theory', 'probability']"
4537878,"Example of a group action of group $D_6$ on set $A = \{1,2,3\}$","I am an undergraduate currently taking Abstract Algebra. We have learned about groups and examples of groups (dihedral, symmetric, general linear, etc.). We are just starting to learn about subgroups in more detail. I am reading through Dummit and Foote's Abstract Algebra 3ed, and I will refer to their section 1.7 on Group Actions. I am confused because an arbitrary group action is a map $\eta: G \times A \rightarrow A$ (written $\eta(g, a) = g \cdot a$ ) that satisfies: $$g_2 \cdot (g_1 \cdot a) = (g_1g_2) \cdot a,$$ $$1 \cdot a = a.$$ I understand that $\cdot$ is not the group operation of $G$ . However, what can it be? How can we guarantee that an operation defined by us can consistently combine elements $g \in G$ and $a \in A$ to produce another element of $A$ ? Or, is this just a natural challenge of coming up with group actions? In particular, let's say that I want to define a group action that describes how elements of $D_6$ permute the vertices of a regular triangle. I will represent the vertices of the regular triangle by a set $A = \{1,2,3\}$ . Is it possible to write the group action I am describing with just the elements of $D_6$ ? Or, must we observe that $D_6$ is isomorphic to $S_3$ and define our action using $S_3$ (because $S_3$ specifically contains all bijections from the set A to A and so elements of $S_3$ can naturally ""affect"" elements of $A$ ). Additionally, what is this process of representing an initial group by another group in order to more naturally define a group action?","['group-theory', 'group-actions']"
4537923,What does $f'(c) >0 $ mean at a point $x=c$?,"I always understood that $f'(c) >0 $ at a point $x=c$ means that there exists a $\delta>0$ so that f is monotonically increasing in the interval $(c-\delta,c+\delta)$ . However in the book A basic course in real analysis they have given an example of the following form $f(x)=x+2x^2sin(1/x)$ for $x \ne 0$ and $f(0)=0$ they have asked to prove that $f'(0)=1$ but $f$ is not monotonic in any interval around $0$ . This example has made me even more confused. Can someone explain what does $f'(c)>0$ at the point $x=c$ even mean?","['continuity', 'derivatives', 'real-analysis']"
4537929,"If two points can be joined by a causal curve that is not a null geodesic, can they be joined by a timelike curve?","Suppose $p$ and $q$ are connected by a causal (i.e. its tangent vectors have non-positive norm) curve $\gamma$ . If $\gamma$ is not a null geodesic, can it be deformed into a smooth, timelike curve still connecting $p$ and $q$ ? This is in Hawking and Ellis The large scale structure of space-time , Proposition 4.5.10. I write the proof below with as many details as I could fill in. Denote by $D_t$ the covariant derivative along $\gamma$ induced by the Levi-Civita connection $\nabla$ . We know $\gamma$ is a null geodesic if the acceleration vector $D_t \gamma '(t) = 0$ and $\langle \gamma'(t), \gamma'(t)\rangle = 0$ everywhere. Thus, if $\gamma$ is not a null geodesic,
there must exist a point $t_0$ where $D_t\gamma'(t_0) \neq 0$ or where $\langle \gamma'(t_0), \gamma'(t_0) \rangle < 0$ . By continuity, if either of these cases hold, then
they hold on an open interval $I = (t_1, t_2)$ . However, if $\langle \gamma'(t), \gamma'(t) \rangle < 0$ on $I$ then $\gamma$ is already timelike, so there is nothing to prove. Consequently, we assume that $D_t
\gamma' \neq 0$ on $I$ . Here it is claimed in the book that $$
\langle D_t \gamma'(t), \gamma'(t) \rangle = \frac 12 \partial_t \langle \gamma'(t), \gamma'(t) \rangle = 0,
$$ but I cannot see why this is so. Can we reparametrize any causal curve so its velocity has constant norm even it at some points the curve becomes null? Taking this for granted, we deduce that $D_t \gamma'$ is spacelike and thus $a(t) ^2 := \langle D_t \gamma'(t), D_t \gamma'(t)\rangle > 0$ on $I$ . We can Fermi transport a vector with positive inner product against $\gamma'$ to obtain a vector
field $W$ along $\gamma|_I$ s.t. $c(t) := -\langle W, \gamma'(t)\rangle > 0$ on $I$ and generate the variation $$ \Gamma(s, t) = \exp_{\gamma(t)}(s (x W + y D_t \gamma'(t))), \quad S = \partial_s \Gamma(s, t),
\quad T = \partial_t \Gamma(s, t)
$$ where $x$ and $y$ are functions to be chosen momentarily but keeping in mind the requirement that $x(t_1) =
x(t_2) = y(t_1) = y(t_2) = 0$ to ensure $\Gamma_s(t_1) = \gamma(t_1)$ and $\Gamma_s(t_2)
= \gamma(t_2)$ . Then, $\Gamma_s(t)$ will be timelike if $\langle T, T \rangle < 0$ everywhere, which
would hold if the the derivative with respect to $s$ at $s = 0$ were negative: \begin{align}
  - 1 &= \frac{1}{2} \partial_s|_0 \langle T, T \rangle = \langle D_s T, T \rangle
  \\
      &= \langle D_t S, T \rangle & \nabla \text{ is torsion free,}
  \\
      &= \partial_t \langle S, T \rangle - \langle S, D_t T\rangle & \text{metric compatibility,}
  \\
  &= \partial_t (x \langle W, \gamma' \rangle) - x \langle W, D_t \gamma'\rangle - y \langle D_t \gamma', D_t \gamma'
    \rangle & T|_{s = 0} = \gamma',
  \\
  &= u' + \langle W, D_t \gamma'\rangle c ^{-1} u - y a(t) ^2 & \text{Letting } u = - x c.
\end{align} With the integrating factor $b(t) = -\int_{t_1} ^t \langle W, D_t \gamma'(s) \rangle c(s) ^{-1}
\mathrm{d} s$ we have \begin{align}
(u e ^{-b})' e ^{b} = y a(t) ^2 - 1 &\Rightarrow u = e ^b \int_{t_1} ^t e ^{-b(s)} (y(s) a(s) ^2 - 1)
                      \mathrm{d} s
  \\
  &\Rightarrow x = c(t) ^{-1}e ^{b(t)} \int_{t_1} ^t e ^{-b(s)} (1 - y(s) a(s) ^2)
                      \mathrm{d} s.
\end{align} To satisfy our earlier constraints, we could let $y$ be a parabola with roots ata $t_1$ and $t_2$ . By scaling and applying the intermediate value theorem, we could find an appropriate factor
s.t. $$ \int_{t_1} ^{t_2} e ^{-b(s)}(1 - y(s) a(s) ^2) \mathrm{d} s = 0$$ because $a(s)$ does not vanish. However, in the book, they instead define $$
x = c^{-1} e ^b \int_{t_1}^t e^{-b}(1 - a^2 y/2) \text d s
$$ and I do not understand where this extra factor of a half is coming from. I think I may be misunderstanding the proof. Could someone point me in the right direction?","['geodesic', 'riemannian-geometry', 'differential-geometry']"
4537963,Is this method of solving this differential inequality correct?,"If $ P(1)=0 , and \frac{d(P(x)}{dx} >P(x),\forall x \geq 1$ , I have been asked to prove that $P(x)>0 ,\forall x\geq 1$ my attempt:- $dx = \frac{dP(x)}{P(x)}$ $\int(dx) \geq \int{\frac{dP(x)}{P(x)}}$ $x+C \geq ln(P(x))$ so $e^{x+c} \geq P(x)$ and as $e^x$ is greater than zero, for all x, this inequality, must be true my book solved this by multiplying both sides by $e^{-x}$ , and then solving the ODE. Is my method right too, and if not, where is it wrong?","['integration', 'solution-verification', 'derivatives', 'ordinary-differential-equations']"
4538015,"If $\lim_{x\to x_0} f(x)g(x)=0$, then $\lim f(x)=0$ or $\lim g(x)=0$ at $x_0$. Is my counterexample fine?","True/false: If $\lim_{x\to x_0} f(x)g(x)=0$ , then $\lim f(x)=0$ or $\lim g(x)=0$ at $x_0$ . I think the claim is not true! Can we define functions $f, g$ as follows: $$
f = \begin{cases}
    1 & \text{if $x \in Q$}\\
    0 & \text{otherwise}
\end{cases}
\\
g = \begin{cases}
      0 & \text{if $x \in Q$}\\
      1 & \text{otherwise}
\end{cases}
$$ So both $f, g$ diverge as Dirichlet function, but $fg=0$ which converges to $0$ at any $x_0$ . Is this counterexample right?
Thanks!","['calculus', 'solution-verification']"
4538061,Which map of the Earth corresponds to the usual parametrization of the sphere?,"The unit sphere $S$ in $\mathbb{R}^3$ , given by the equation $x^2+y^2+z^2 = 1$ , can be parametrized by $$(u,v)\mapsto (\cos u \cos v, \sin u \cos v, \sin v).$$ Under the above parametrization, points of $S \setminus \{(0,0,1),(0,0,-1)\}$ are in 1-to-1 correspondence with $(u,v) \in [-\pi,\pi)\times (-\pi/2,\pi/2)$ . Assuming the Earth is a perfect sphere, there should be a well-known map of the Earth (minus north and south poles) associated with this correspondence. Which one is it?","['cartography', 'geometry', 'differential-geometry']"
4538068,Proving $\lim _{x \to \infty }\frac{1 - 2x^2}{x^2 + 3}\ = -2$,I solved it using the definition of limit to the $\left|\frac{-7}{x^2 +3}\right| < \varepsilon$ Then $\varepsilon > \frac{7}{x + 3} > \frac{7}{x^2 +3}$ Then $x > \frac{7}{\varepsilon} - 3$ But my teacher said this was wrong and I have to fix it to $x >  \sqrt{\left|\frac{7}{\varepsilon} - 3\right|}$ Can anyone please explain why am I wrong here? Thank you very much,['limits']
4538072,"Skew a value in the range [0.0, 1.0].","I have a variable v that can take a value from 0.0 to 1.0 . I want a function to left-skew the value, producing w . By ""left skew"" I mean that any value of v will be increased, but more so for lower values and less so for higher values, so that there will be more values in w ""bunched together"" the nearer you get to 1.0 . (My math is a but rusty so I apologize if I'm not asking this using the correct terminology.) For an intuitive example (not to scale; these are arbitrary values to explain the idea ), a value v of 0.4 might yield a value w of 0.5 , while a value v of 0.91 might only yield a value w of 0.92 . (Ideally there would be some other variable that controlled the amount of skew, and perhaps another variable that controlled the weight or slope of the skew. But I want to start simple!) However an input v of 0 or 1 would still yield an output w of 0 , and 1 , respectively (i.e. the bounds of the range are fixed). Another way to look at this is to assume that the values in v are right-skewed, and we want to ""unskew"" them to make a uniform distribution. This is probably Math 101 or Trigonometry 101, and there's probably some simple sin/cos function or something like that. Thank you in advance for helping my brain get oriented to review these elementary concepts.","['functions', 'uniform-distribution']"
4538079,How can I compute the distribution function of $\Bbb{E}(Z|N)$?,"Let $(X_i)$ be a collection of i.i.d. r.v. in $L^1$ and $N\sim \mathrm{Poi}(1)$ independent from $(X_i)$ . We define $Z=\sum_{i=1}^N X_i$ . I need to compute the distribution function of $\Bbb{E}(Z|N)$ . From the lecture I know that $\Bbb{E}(Z|N=n)=n\Bbb{E}(X_1)$ . Now I know that I need to compute $\Bbb{P}(\Bbb{E}(Z|N)\leq x)$ for all $x\in \Bbb{R}$ . But somehow I don't know where to start.
I know that by definition $\Bbb{E}(X|Y=y)=\frac{\Bbb{E}(X1_{Y=y})}{\Bbb{P}(Y=y)}$ therefore I thought about doing a partition in $\Bbb{E}(Z|N)$ to get $\Bbb{E}(Z|N)=\sum_k \Bbb{E}(Z|N)1_{N=k}=\sum_k\Bbb{E}(Z|N=k)1_{N=k}$ . But I do not get something nice. Can maybe someone help me? Thanks a lot","['poisson-distribution', 'probability-distributions', 'conditional-expectation', 'probability-theory', 'probability']"
4538155,Getting derivative by dividing partial derivatives (in ODE linear stability analysis),"In short: I'm trying to understand how dividing two partial derivatives gives a ""non-partial"" derivative. Let me give the context, since there might be notational peculiarities specific to that domain. Given the following system of ordinary differential equations: $$\frac{du}{dt} = f(u,v)$$ $$\frac{dv}{dt} = g(u,v)$$ and let $(u_0, v_0)$ be a steady solution: $f(u_0, v_0) = g(u_0, v_0) = 0$ . Murray ( pp. 226-227 ) draws the picture of some possible reaction null clines for $f=0$ , $g=0$ at a steady state $(u_0, v_0)$ (see picture below)
and states that at $(u_0, v_0)$ , the ""gradient on $g=0$ "" fulfils $$ \frac{dv}{du} \Bigg]_{g=0} = - \frac{\frac{\partial g}{\partial u}}{\frac{\partial g}{\partial v}} $$ . How does one get this identity? Why is there a negative sign?","['partial-derivative', 'stability-in-odes', 'stability-theory', 'ordinary-differential-equations']"
4538159,"Lagrange multiplier $f(x,y)=x^2+2xy^2+y^2$. Show that $(-1,1)$ is the minimal point of $f$.","Given the function $f(x,y)=x^2+2xy^2+y^2$ with the restriction $x+2y-1=0$ I can't justify why the point (-1,1) is the minimum point of $f$ . I used the lagrange multiplier and found the following maximum and minimum candidates: $(-1,1), (-1/2,2/3)$ , and $(0,2/3)$ .","['lagrange-multiplier', 'analysis', 'maxima-minima', 'multivariable-calculus', 'functional-analysis']"
4538162,Are horizontally shifting and stretching a graph in y-direction always interchangeable?,"For example, to plot $g(x) = 2(x-2)^3$ , you could Take $f_1(x) = x^3$ . Stretch it by the factor 2 in y-direction: $f_2(x) = 2 \cdot f_1(x) = 2x^3$ and then shift it by 2 units on the x-axis in positive direction: $f_3 = f_2(x - 2) = 2(x-2)^3$ . $f_3 (x) = g(x)$ . Take $f_1(x) = x^3$ . Shift it by 2 units on the x-axis in positive direction: $f_2(x) = (x-2)^3$ and then stretch it by the factor 2 in y-direction: $f_3(x) = 2 \cdot f_2(x) = 2(x-2)^3$ . $f_3(x) = g(x)$ . (The only case I noticed where horizontally shifting and stretching in y-direction aren't interchangeable is for lines, where horizontally shifting is equivalent to vertically shifting. In that case, stretching would also stretch the shift, so you should first stretch and only then shift.)
(Edit: even for lines, the two operations are interchangeable) Now my question is, are horizontally shifting and stretching in y-direction always interchangeable(, with the one exception being lines?)","['algebra-precalculus', 'functions', 'graphing-functions']"
4538166,A question related to the inverse image of a set,"This may seem like (... or, may be it is) a very trivial question. Consider a function $f\colon X\to Y$ (Assume that both $X$ and $Y$ are non-empty). Consider the inverse image of $Y$ under $f$ , defined by $\:f^{-1}(Y):=\{x\in X:f(x)\in Y\}$ . Then is it necessarily true that $f^{-1}(Y)=X$ ? My thoughts: Cleary, $f^{-1}(Y)\subset X$ (follows from the definition). Now given $x\in X$ , $f(x)\in Y$ , so $x\in f^{-1}(Y)$ . Hence $X\subset f^{-1}(Y)$ , so $X=f^{-1}(Y)$ . Is this reasoning correct? I feel like I am missing something.","['elementary-set-theory', 'functions']"
4538168,How does $\frac {(x^2 + 2x) - (a^2 + 2a)}{x-a}$ reduce to $x + a + 2$?,"With the given function: $$f(x) = x^2 + 2x $$ I am trying to evaluate the following expression: $$\frac {f(x) - f(a)}{x-a} $$ I've been informed that the solution is: $x + a + 2$ , where $x \ne a$ But I don't know how to get there. Can someone help me understand the logic behind the solution and where I'm going wrong? I hope this question is appropriate. It's my first on the site. I started by replacing the functions with their bodies respecting the given inputs. $$\frac {(x^2 + 2x) - (a^2 + 2a)}{x-a}$$ Then, it appears to me that one could factor out the X's and the A's. $$\frac {x(x + 2) - a(a + 2)}{x - a} $$ At this point I'm actually stumped. I can see there's an $x$ and an $a$ with subtraction between them, but I don't see how they're related. But given the solution, I guess the $x-a$ in the denominator cancels out the $x-a$ in the numerator (somehow?). Then I'd be left with $$(x+2)+(a+2)$$ Adding this up I get $x+a+4$ . I'm not sure what problems like these are called, so I'm not sure what to search to find what I'm missing. Thank you in advance for helping me.","['fractions', 'algebra-precalculus', 'functions']"
4538195,Does 'constant' mean independent of the main variables?,"What is the meaning of 'constant' when given as an unspecified term in an expression? I'm aware of the idea of a 'parameter' we give a term that defines a function or a set of functions, so given $f(x)=ux$ , we can consider it giving us a new function $f$ for each $u$ , if $f$ retains it's identity, the number multiplied by $x$ must also do so. However, in many cases we are told things like the following: 'If $a,b,c$ are constants then $ax^2+bx+c=0$ is a quadratic equation in $x$ ' What makes them 'constant' here? They can be independent, is it that for a given $a,b,c$ we get a particular equation to be solved? Does it simply mean 'independent' of all other variables (or at least independent of the arguments and unknowns), because of course if $a=f(x)$ and we get a different equation in $x$ and with any parameter it must be independent of the 'arguments'. For example if I state that the set of values defined at $y_0$ is given by the equation $f(x)=y_0$ where $y_0$ is a constant, is it a correct understanding that $y_0$ may change from point to point, but must be independent of $x$ .","['notation', 'calculus', 'functions', 'algebra-precalculus', 'terminology']"
4538233,Double Integrals - Region delimited by triangle in counterclockwise,"A triangle with vertices (0,0), (0,1) and (1,2), counterclockwise, delimits a region. Determine an integral over this region of the following expression: $$\int_{\Omega}(x-y)dx+e^{x+y}dy$$ This kind of exercises fit the line integral, but I don't know how to start solving it, I thought I'd use Green's Theorem. The key is to integrate x first, then y. $$\int_{\Omega}(x-y)dx+e^{x+y}dy=\int_{\Omega}\left[\dfrac{\partial}{\partial x}(e^{x+y}) - \dfrac{\partial}{\partial x}(x-y)\right]\,dx\,dy =\int_{\Omega}\left[e^{x+y} +1\right]\,dx\,dy $$ What would the extremes of integration look like? I don't know how to continue","['integration', 'multivariable-calculus', 'calculus', 'change-of-variable']"
4538243,No nontrivial independent sets of events in $2^\mathbb{N}$,"From ""Theorems and Counterexamples in Mathematics"" Exercise 4.1.2 We say that a set $\mathcal{E} = \{A_\lambda\}_{\lambda \in \Lambda}$ of events $A_\lambda$ is independent iff for every subset $\{A_n : A_i \neq A_j \text{if } i\neq j, 1 \leq n \leq N\}$ satisfies $$P\left(\bigcap_{n=1}^N A_n \right) = \prod_{n=1}^N P(A_n).$$ The trivial instance of independence is that for any event $A$ , the collection $\{\emptyset,A,X\}$ is independent where $X$ is the whole space. Let $P$ in the measure situation $(\mathbb{N},2^\mathbb{N},P)$ be the discrete measure $$ P(n) \overset{\text{def}}{=} \begin{cases} 1 - \sum_{n=2}^\infty 2^{-n!} & \text{if } n = 1 \\ 2^{-n!} & \text{if } n \geq 2. \end{cases}$$ I want to show that ""trivial instances aside, there are no independent sets of events in $2^\mathbb{N}$ "". I know that I need to show that for any pair of distinct events $A,B \notin \{\emptyset,\mathbb{N}\}$ we have $P(A \cap B) \neq P(A)P(B)$ . There is a first part that I can solve that suggests I should use the fact that if $1 < m,n \in \mathbb{N}$ then there is no $k \in \mathbb{N}$ such that $k! = m! + n!$ . However, I can make no sense of how to solve this problem.","['independence', 'probability-distributions', 'probability-theory']"
4538271,Constrained Maximization problem involving integer variables,The maximization problem may be seen as a problem of integer programming problem and may be solved by Gomory cut method or branch and bound technique. Is there any other way to see this problem?,"['optimization', 'inequality', 'analysis']"
4538276,Can someone help me understand the difference between gradient vector and directional derivative?,"Okay so here's what I understand: If we have a surface, then the directional derivate in the direction of a unit vector $\vec{u}$ at a point $P_{0}$ is the slope of the curve on the surface going through $P_{0}$ in the direction of $\vec{u}$ And $D_{\vec{u}} = f_{x}(x,y)a + f_{y}(x,y)b$ where $\vec{u} = \langle a, b \rangle$ and $\nabla f(x,y) = \langle f_{x}(x,y), f_{y}(x,y) \rangle $ So $D_{\vec{u}} = \nabla f(x,y) \cdot u$ But then $\nabla f(x,y)$ is in the direction of maximum ascent? how does that come to be? what is happening here?",['multivariable-calculus']
4538301,What exactly is symmetry?,"I mean, I know intuitively what symmetry is. But if we want to be exact, is it defined as a mapping from an object to itself preserving the structure? Or it is defined as a property that an object has, that is, the object remains unchanged under a set of operations? Even the Wikipedia article about symmetry provides these two ""definitions"" one after another. But I cannot help but feel that they are different things. I would say that symmetry is property of invariance under a mapping, not the mapping itself. For the context, I mainly work with symmetries in terms of groups. (I.e. rotational, reflectional symmetry etc.) Maybe this adds to my confusion, since there is a symmetric group $S_n$ with clear definition and operation, but also symmetry group (group of symmetries) which is a group of transformations of a given abstract object that preserve its structure. In summary, how should be symmetry best defined/described? Thank you for your insights on this. May be relevant: A similar question on this forum, describing symmetry as a condition .","['symmetric-groups', 'group-theory', 'definition', 'symmetry']"
4538322,Reducing $ax^6-x^5+x^4+x^3-2x^2+1=0$ to a cubic equation using algebraic substitutions,"Use algebraic substitutions and reduce the sextic equation to the cubic equation, where $a$ is a real number: $$ax^6-x^5+x^4+x^3-2x^2+1=0$$ My attempts. First, I tried to use the Rational root theorem, when $a$ is an integer $x=\pm 1$ , but this implies $a=0$ and this is not always correct. Then I realized that, $$x^4-2x^2+1=(x^2-1)^2$$ is a perfect square. So, I tried to write the original equation as $$ax^4-x^3+x+\bigg(x-\frac 1x\bigg)^2=0$$ $$x^2\bigg(ax^2-x+\frac 1x\bigg)+\bigg(x-\frac 1x\bigg)^2=0$$ But I failed again. I couldn't spot the palindromic property.","['cubics', 'irreducible-polynomials', 'roots', 'polynomials', 'algebra-precalculus']"
4538331,What is the difference between the sets $A \times (B \times C)$ and $A \times B \times C$?,"How are the sets $A \times D$ (where $D = B \times C$ is the cartesian product of $B$ and $C$ ) and $A \times B \times C$ different? Aren't each element in both sets equal? In particular, what is the difference between $\bigl(a, (b, c)\bigr)$ and $(a, b, c)$ ? If there is a difference, why is it significant?",['elementary-set-theory']
4538337,Weird system of PDEs defined on a sphere,"Let $x$ and $y$ be functions defined on a simply connected (open or closed) portion of the surface of a (unit) sphere, and consider the following system of PDEs: \begin{align}
\|\nabla x\|^2 =
 \|\nabla y\|^2 = \| \nabla x \times \nabla y \|^2
\end{align} (Some suitable conditions may be provided.) Are there non-constant solutions for such a system ? Any ideas about how to reduce it to single equations for $x$ and $y$ ?","['vector-fields', 'analysis', 'calculus', 'partial-differential-equations', 'differential-geometry']"
4538344,$\sum_{n=1}^\infty \frac{a_n}{n^{1+\delta}}$ converges,"Let $\left\{a_n\right\}$ be real sequence, $b_n=\frac{a_1+\cdots+a_n}{n}, n\geq 1$ . Suppose that $\left\{b_n\right\}$ is bounded, show that for any $\delta>0$ , the $\sum_{n=1}^\infty \frac{a_n}{n^{1+\delta}}$ converges. My attempt: let $c_k=a_k/k$ , then by $a_k=kb_k-(k-1)b_{k-1}$ , we see readily that $$|\sum_{i=1}^n c_i|=|b_n+\sum_{k=2}^n \frac{b_{k-1}}{k}$$ But this could not prove that $|\sum_{i=1}^n c_i|$ is bounded, or else, we could use Dirichlet test... Any ideas? or other proof?","['calculus', 'sequences-and-series']"
4538411,Finding poles of $f(z) = z/(1-\cos z)$,"I'm trying to self-learn complex analysis and am working on a question (from a practice exam) that asks me to find all the poles of $$f(z) = {z\over 1-\cos z}$$ and compute the residues at each pole. Now if $z_0 = 2\pi n$ where $n$ is a nonzero integer, then $$\lim_{z\to z_0} {(z-z_0)^2 z\over 1-\cos z} =
\lim_{z\to z_0} {2(z-z_0) z + (z-z_0)^2 \over \sin z} =
\lim_{z\to z_0} {4z - 2z_0 + (z-z_0) \over \cos z} = 2z_0,
$$ which is neither zero nor infinity, so $f$ has a pole of order $2$ at all of these points. If you try this with $z_0 = 0$ and $m=1$ , you get $$\lim_{z\to 0} {z^{2} \over 1-\cos z} = \lim_{z\to 0} {2z\over \sin z} = \lim_{z\to 0} {2\over \cos z} =2,$$ which says that $0$ is a pole of order $1$ and also tells us that the residue at $0$ is $2$ . I decided to check if these are the correct poles and typed this into WolframAlpha: https://www.wolframalpha.com/input?i=poles+of+z%2F%281-cos+z%29 It says that there is no pole at $z=0$ , which is strange. Am I misunderstanding the concept of pole? My definition of pole (I'm learning out of Complex Variables by Francis J. Flanagan) is $z_0$ such that $\lim z\to z_0 |f(z)| = \infty$ , and by this definition, $0$ is a pole. (By the way, I still have to compute the residues here which is a huge mess according to the formula I know: $$ {\rm Res}(f; z_0) = \lim_{z\to z_0} ((z-z_0)^2 f(z))'$$ so I guess a subquestion would be: Is there any way to compute these residues without differentiating the inside of that and then using l'Hospital's rule over and over? WolframAlpha says the answer should be $2$ at every pole.)",['complex-analysis']
4538424,Strange polynomial analog of the Bell numbers,"Let $\vec{x} = (x_0, x_1, x_2, \dots)$ and $\vec{y}=(y_1,y_2,y_3, \dots)$ be two systems of parameters/variables. The Motzkin polynomials $P_n(\vec{x},\vec{y})$ for $n \geq 0$ are defined by the following quadratic recursion \begin{equation}
P_n(\vec{x},\vec{y}) \ = \ x_0 P_{n-1}(\vec{x},\vec{y})
\, + \, \sum_{i \, = \, 0}^{n-2} \, y_1P_i(\vec{x}_+, \vec{y}_+) P_{n-i-1}(\vec{x},\vec{y}) \end{equation} where $P_0(\vec{x},\vec{y}) := 1$ and $P_1(\vec{x},\vec{y}) := x_0$ are the initial polynomials and where $\vec{x}_+:= (x_1, x_2, x_3, \dots)$ and $\vec{y}_+:=(y_2,y_3,y_4, \dots)$ are the respective one-step shifts of the sequences $\vec{x}$ and $\vec{y}$ . From an enumerative standpoint, the Motzkin polynomial $P_n(\vec{x},\vec{y})$ is the multi-variate generating function of all Motzkin paths with $n$ steps: Each horizontal step (at level $k$ ) is weighted $x_k$ , each ascending step (at level $k$ ) is weighted $y_k$ , and the overall weight of the path is the product of weights of all horizontal steps and ascents which are taken. I should add that the generating
function $\sum_{n \geq 0} P_n(\vec{x},\vec{y})z^n$ can be be formally expressed as the following Jacobi continued fraction involving the parameters $\vec{x}$ and $\vec{y}$ \begin{equation}
J_{\vec{x}, \vec{y}} \, (z) \ := \
{1 \over {1 - x_0z - {\displaystyle y_1z^2 \over {\displaystyle 1 - x_1 z - {\displaystyle y_2z^2 \over 
{\displaystyle 1 - x_2 z - {\displaystyle y_3z^2 \over {\ddots } } } } } } } }
\end{equation} I'm concerned with the following specialization. For integers $k \geq 1$ let $x_{k-1} = \sigma + k - 1$ and $y_k = \sigma + k -1$ where $\sigma$ is an indeterminate. Since $\sigma$ is the operative variable I shall, for brevity's sake, write $P_n(\sigma)$ instead of $P_n(\vec{x},\vec{y})$ . Brute force calculations reveal that \begin{equation}
\begin{array}{l}
P_0(\sigma) \, =1 \\
P_1(\sigma) \, = \sigma \\
P_2(\sigma) \, = \sigma^2 + \sigma\\
P_3(\sigma) \, = \sigma^3 + 3\sigma^2  + \sigma \\
P_4(\sigma) \, = \sigma^4 + 6\sigma^3 + 6\sigma^2 + 2\sigma \\
P_5(\sigma) \, = \sigma^5 + 10\sigma^4 + 20\sigma^3 + 16\sigma^2 + 5\sigma \\
P_6(\sigma) \, = \sigma^6 + 15\sigma^5 + 50\sigma^4 + 71\sigma^3 + 51\sigma^2 + 15\sigma \\
P_7(\sigma) \, = \sigma^7 + 21\sigma^6 + 105\sigma^5 + 231\sigma^4 + 281\sigma^3 + 186\sigma^2 + 52\sigma
\end{array} 
\end{equation} Evidence seems to suggest that $P_n(1) = B_n$ where $B_n$ is the $n$ -th Bell number while $P_n(2)$ counts the number of irreducible set partitions of size $n$ (see sequence A074664 at the OEIS). Furthermore \begin{equation}
\begin{array}
\big[\sigma] \, P_n(\sigma)
&\displaystyle = \, B_{n-2} \ \text{for $n \geq 2$} \\
\big[\sigma^{n-2}\big] \, P_n(\sigma)
&\displaystyle = \, {1 \over {12}} \, n^2(n^2-1) \ \text{for $n \geq 3$} \\
\big[\sigma^{n-1}\big] \, P_n(\sigma)
&\displaystyle = \, \binom{n}{2} \ \text{for $n \geq 2$} \\
\end{array}
\end{equation} where $[\sigma^k] \, P_n(\sigma)$ denotes the coefficient of $\sigma^k$ occurring in $P_n(\sigma)$ .
For $0 \leq n \leq 3$ we have $P_n(\sigma) = T_n(\sigma)$ where $T_n(\sigma)$ is the Touchard polynomial but this coincidence ceases for $n \geq 4$ . Question: Are the $P_n(\sigma)$ polynomials known? Does the generating function $\sum_{n \geq 0} P_n(\sigma) z^n$ have a nice form? thanks, jeanne. Up-date: Rephrasing the calculation in Somos' post and also Following Qiaochu Yuan's suggestion
to use the continued fraction expansion of the generating function $J(\sigma, z):= \sum_{n \geq 0} P_n(\sigma) z^n$ we get
the functional equation \begin{equation} 
J(\sigma, z) \ = \
{1 \over {1 - \sigma z - \sigma z^2 J(\sigma +1 ,\ z)  }}
\end{equation} which we can re-write in terms of
linear fractional transformations
as \begin{equation}
\begin{array}{ll}
\begin{pmatrix}
1 - \sigma z & -1 \\ \sigma z^2 & 0 
\end{pmatrix} \cdot J(\sigma, z)
&\displaystyle = \ {(1 - \sigma z)J(\sigma,z) \, - \, 1 \over{\sigma z^2 J(\sigma, z)}} \\ \\
&= \
J(\sigma +1 ,z)
\end{array}
\end{equation} The term ""nice form"" might entail having a differential-recursive formula for the polynomials $P_n(\sigma)$ analogous to the Rodrigues formula for the Touchard polynomials, namely: \begin{equation}
T_{n+1}(x) \ = \ x \Big(1 \, + \, {d \over {dx}} \Big) T_n(x)
\end{equation}","['bell-numbers', 'combinatorics', 'stirling-numbers', 'generating-functions']"
4538426,Hardy's inequality proof using Doob's inquality,"Consider a probability space $([0,1],\mathcal{B}([0,1],\lambda),p>1$ and $f \in L^p(]0,\infty[).$ We want to prove Hardy's inequality using martingale theory and Doob's maximal inequalities.
Let $\mathcal{F}_n$ be the $\sigma$ -algebra generated by $]k2^{-n},(k+1)2^{-n}],k=0,...,2^n-1.$ I managed to find, for an integrable function $h,$ $E[h|\mathcal{F}_n](x): E[h|\mathcal{F}_n]=2^n\sum_{k=0}^{2^n-1}\int_{k2^{-n}}^{(k+1)2^{-n}}h(y)dy1_{]k2^{-n},(k+1)2^{-n}]}(x),x \in [0,1].$ It is sufficient to prove $$\left(\int_0^1\left|\frac{1}{x}\int_0^xf(y)dy\right|^pdx\right)^{1/p} \leq \frac{p}{p-1} \left(\int_0^1\left|f(x)\right|^p dx\right)^{1/p},$$ since the general result follows from this and the monotone convergence theorem (applied to $f_n(x)=f(nx),x \geq 0$ ). How to relate $E[h|\mathcal{F}_n](x)$ (for a convenient $h$ ) to $\frac{1}{x}\int_0^xf(y)dy$ ?","['measure-theory', 'real-analysis', 'stochastic-processes', 'martingales', 'probability-theory']"
4538446,Integration by parts using Dirac delta functions,"From my notes I have some background information on Canonical Commutation Relations: I can prove the first question. Though I cannot prove the next question: I will typeset this question also just in case the image is a bit hard to read: Perform the following change of variables in the integral $p^{\prime} = p + \hbar s$ and then expand the integrand in powers of $\hbar$ . Prove that to ﬁrst order in $\hbar$ we have $$\left(A_c \ast B_c\right)(x, p)=A_c\left(x,p\right)B_c\left(x,p\right)-i\hbar\frac{\partial A_c}{\partial p}\frac{\partial B_c}{\partial x}+\mathcal{O}\left(\hbar^2\right)\tag{9}$$ (Hint, you will need to remember that the Dirac delta function can be represented by $$\delta(x-x^{\prime})=\int_{-\infty}^{\infty}\frac{dk}{\left(2\pi\right)}\,e^{ik\left(x-x^{\prime}\right)}\tag{10}
$$ You will also need to perform an integration by parts over the variable $x^{\prime}$ .) Here is my attempt at trying to prove eqn. $(9)$ : The previous result, (eqn. $(8)$ ) can be re-written as - $$\left(A_c \ast B_c\right)(x, p)=\int_{-\infty}^{\infty}dx^\prime\int_{-\infty}^{\infty}\frac{dp^{\prime}}{2\pi \hbar}A_c\left(x, p^{\prime}\right)B_c\left(x^\prime,p\right)e^{i\left(p^\prime-p\right)\left(x-x^\prime\right)/\hbar}$$ Now $p^{\prime}=p+hs$ , where $s$ is the new integration variable, therefore, $dp^{\prime}=\hbar ds$ , here $p$ is a constant as it is not being integrated over. Therefore, $$\left(A_c \ast B_c\right)(x, p)=\int_{-\infty}^{\infty}dx^{\prime}\int_{-\infty}^{\infty}\frac{ds}{2\pi}A_c\left(x, p+\hbar s\right)B_c\left(x^{\prime},p\right)e^{is\left(x-x^{\prime}\right)}$$ Taylor expanding the integrand in powers of $\hbar$ , $$\left(A_c \ast B_c\right)(x, p)=\int_{-\infty}^{\infty}dx^{\prime}\int_{-\infty}^{\infty}\frac{ds}{2\pi}\Big[A_c\left(x, p\right)+\hbar s\frac{\partial}{\partial p}\Big(A_c \left(x,p\right)\Big)+\mathcal{O}\left(\hbar^2\right)\Big]B_c\left(x^{\prime},p\right)e^{is\left(x-x^{\prime}\right)}$$ $$=\frac{1}{2\pi}A_c\left(x,p\right)\int_{-\infty}^{\infty}dx^{\prime}B_c\left(x^{\prime},p\right)\int_{-\infty}^{\infty}ds\,e^{is\left(x-x^{\prime}\right)}\tag{a}$$ $$+\frac{1}{2\pi}\hbar\frac{\partial}{\partial p}\Big(A_c \left(x,p\right)\Big)\int_{-\infty}^{\infty}dx^{\prime}B_c\left(x^{\prime},p\right)\color{blue}{\int_{-\infty}^{\infty}ds\,se^{is\left(x-x^{\prime}\right)}}\tag{b}$$ $$+\frac{1}{2\pi}\mathcal{O}\left(\hbar^2\right)\int_{-\infty}^{\infty}dx^{\prime}B_c\left(x^{\prime},p\right)\int_{-\infty}^{\infty}ds\,e^{is\left(x-x^{\prime}\right)}\tag{c}$$ Where I have taken out factors that don't depend on the integration variables $x^{\prime}$ and $s$ and multiplied out the integral into $3$ terms, it is the integrals over $s$ I would like to focus on now: I note that $$i\frac{\partial}{\partial x^{\prime}}\left(e^{is\left(x-x^{\prime}\right)}\right)=se^{is\left(x-x^{\prime}\right)}\tag{d}$$ and the hint given in the question is, $$\delta(x-x^{\prime})=\frac{1}{2\pi}\int_{-\infty}^{\infty}ds\,e^{is\left(x-x^{\prime}\right)}\tag{e}$$ Starting with the first term, I can make use of eqn. $(e)$ ,
so $(\mathrm {a})$ becomes, $$\frac{1}{2\pi}A_c\left(x,p\right)\int_{-\infty}^{\infty}dx^{\prime}B_c\left(x^{\prime},p\right)2\pi\,\delta\left(x-x^{\prime}\right)=A_c\left(x,p\right)B_c\left(x,p\right)$$ by the sifting property of the Dirac delta 'function', which sets $x^\prime=x$ , and is the required first term in eqn. $(9)$ . Using the same logic for the third term, $(\mathrm{c})$ , things do not work out as required: $$\begin{align}\frac{1}{2\pi}\mathcal{O}\left(\hbar^2\right)\int_{-\infty}^{\infty}dx^{\prime}B_c\left(x^{\prime},p\right)\int_{-\infty}^{\infty}ds\,e^{is\left(x-x^{\prime}\right)}&=\frac{1}{2\pi}\mathcal{O}\left(\hbar^2\right)\int_{-\infty}^{\infty}dx^{\prime}B_c\left(x^{\prime},p\right)2\pi\delta\left(x-x^{\prime}\right)\\&=\mathcal{O}\left(\hbar^2\right)B_c\left(x,p\right)\end{align}$$ It gets worse for the second term, $(\mathrm{b})$ , and I can't figure out what to do. I'm sure this should be done via integration by parts and using eqn. $(\mathrm{d})$ somehow along with eqn. $(\mathrm{e})$ . Just for clarity, I will focus solely on the integral of the product of functions of $s$ , marked blue in $(\mathrm{b})$ , this is what I find: $$\begin{align}\color{blue}{\int_{-\infty}^{\infty}ds\,se^{is\left(x-x^{\prime}\right)}}&=\tag{f}\bigg[-\frac{i}{x-x^{\prime}}se^{i\left(x-x^{\prime}\right)}\bigg]_{s=-\infty}^\infty-\int_{s=-\infty}^{\infty}ds\bigg[-\frac{i}{x-x^{\prime}}e^{is\left(x-x^\prime\right)}\bigg]\\&=\tag{g}\bigg[\frac{1}{x-x^{\prime}}\frac{\partial}{\partial x^{\prime}}\left(e^{is \left(x-x^{\prime}\right)}\right)\bigg]_{s=-\infty}^\infty+\frac{i}{x-x^{\prime}}\int_{s=-\infty}^{\infty}ds\,e^{is\left(x-x^\prime\right)}\\&=\tag{h}\bigg[\frac{1}{x-x^{\prime}}\frac{\partial}{\partial x^{\prime}}\left(e^{is \left(x-x^{\prime}\right)}\right)\bigg]_{s=-\infty}^\infty+\frac{2\pi i}{x-x^{\prime}}\delta\left(x-x^\prime\right)\end{align}$$ In $(\mathrm{f})$ I have used the formula for integration by parts, $$\int_{v=-\infty}^\infty udv=\Big[uv\Big]_{v=-\infty}^{\infty}-\int_{v=-\infty}^\infty vdu$$ Where, in the first term on the RHS of $(\mathrm{g})$ I used eqn. $(\mathrm{d})$ , $$se^{is\left(x-x^{\prime}\right)}=i\frac{\partial}{\partial x^{\prime}}\left(e^{is\left(x-x^{\prime}\right)}\right)$$ Lastly, in the second term of $(\mathrm{h})$ I made use of eqn. $(\mathrm{e})$ , $$\int_{-\infty}^{\infty}ds\,e^{is\left(x-x^{\prime}\right)}=2\pi\,\delta(x-x^{\prime})$$ Of course, this has all gone horribly wrong as even if I reinstate the full form of $(\mathrm{b})$ , $$\frac{1}{2\pi}\hbar\frac{\partial}{\partial p}\Big(A_c \left(x,p\right)\Big)\int_{-\infty}^{\infty}dx^{\prime}B_c\left(x^{\prime},p\right)\color{blue}{\int_{-\infty}^{\infty}ds\,se^{is\left(x-x^{\prime}\right)}}$$ $$=\frac{1}{2\pi}\hbar\frac{\partial}{\partial p}\Big(A_c \left(x,p\right)\Big)\int_{-\infty}^{\infty}dx^{\prime}B_c\left(x^{\prime},p\right)\color{blue}{\Bigg\{}\bigg[\frac{1}{x-x^{\prime}}\frac{\partial}{\partial x^{\prime}}\left(e^{is \left(x-x^{\prime}\right)}\right)\bigg]_{s=-\infty}^\infty+\frac{2\pi i}{x-x^{\prime}}\delta\left(x-x^\prime\right)\color{blue}{\Bigg\}}$$ both the terms in the blue curly braces are undefined, the first because of the limits and the second because the Dirac delta function sets $x=x^{\prime}$ which causes problems for the denominator. Please may I have some hints or tips on how to get the correct terms, $(\mathrm{b})$ and $(\mathrm{c})$ and hence reach the correct expression, $(9)$ ? In other words, how can I show that $$\left(A_c \ast B_c\right)(x, p)=A_c\left(x,p\right)B_c\left(x,p\right)-i\hbar\frac{\partial A_c}{\partial p}\frac{\partial B_c}{\partial x}+\mathcal{O}\left(\hbar^2\right)\,\,?$$ For reference, the part of the handwritten solution involving the Taylor expansion is written as $$\left(A_c \ast B_c\right)(x, p)=\int_{-\infty}^{\infty}dx^{\prime}\int_{-\infty}^{\infty}\frac{ds}{2\pi}\Big[A_c\left(x, p\right)+\hbar s\frac{\partial}{\partial p}\Big(A_c\left(x,p\right)\Big)\Big]B_c\left(x^{\prime},p\right)e^{is\left(x-x^{\prime}\right)}+\mathcal{O}\left(\hbar^2\right).$$ But this just seems nonsensical to me since the question specifically asked for the integrand to be Taylor expanded, so putting $\mathcal{O}\left(\hbar^2\right)$ outside the integral is not a Taylor expansion of the integrand. What have I overlooked here?","['integration', 'dirac-delta', 'calculus', 'solution-verification', 'taylor-expansion']"
4538480,Why is reduction map $\phi: End(E)\rightarrow End(\widetilde{E})$ a ring homomorphism?,"Let $E/L$ be an elliptic curve over a number field $L$ . Let $\mathfrak{B}$ be a maximal ideal of the ring of integers, and let $E$ have good reduction at $\mathfrak{B}$ . In Silverman's Advanced Topic in the Arithmetic of Elliptic Curves, Proposition II.4.4 yields that the natural reduction (wrt $\mathfrak{B}$ ) morphism $\phi: End(E)\rightarrow End(\widetilde{E})$ is injective. As far as I can see the proof only yields that $\phi$ is an injective morphism of sets. However soon after in the text, e.g. in Lemma II.5.2 it is heavily insinuated that $\phi$ is a morphism of rings. I don't see a priori why this should hold, especially when $End(E)\supsetneq \mathbb{Z}$ . Question : Why is reduction map $\phi: End(E)\rightarrow End(\widetilde{E})$ a ring homomorphism? Bonus points if this can be shown in as low-tech a way as possible.","['algebraic-curves', 'algebraic-geometry', 'elliptic-curves']"
4538534,"Show that $f''(x)=e^xf(x)$ with $f(a)=f(b)=0$ makes $f\equiv 0$ $\forall x \in [a,b]$","Define $f \in C^{2}\left[a,b\right]$ satisfying $f''(x)=e^xf(x)$ . Show that $f''(x)=e^xf(x)$ with $f(a)=f(b)=0$ makes $f\equiv 0$ $\forall x\in [a,b]$ . Actually, I figure out a solution as follows:
(just taking about the idea) We can prove a general conclusion：if $f \in C^{2}\left[a,b\right]$ satisfying $f''(x)=g(x)f(x)$ where $g(x) \in C^{0}\left[a,b\right]$ satisfying $g(x)>0$ , and $f(a)=f(b)=0$ , we have $f\equiv 0$ $\forall x \in [a,b]$ . The idea is to prove that if there exists $x_0\in (a,b)$ such that $f(x_0)\ne0$ (let's assume that $f(x_0)>0$ ), we can prove that there exists $x_1\in (a,b)$ such that $f(x_1)>0$ , $f'(x_1)>0$ and $f''(x_1)>0$ . And through this conclusion we can easily get that $f(x)$ will be strictly monotonically increasing in the interval $\left[x_1,b\right]$ . So my questions are: Is there any other solution of this problem? Can we solve this differential equation problem?","['ordinary-differential-equations', 'real-analysis']"
4538597,For what values of $a$ does $\prod\limits_{k=1}^n a|\sin{k}|\to\infty$ as $n\to\infty$?,"For what values of $a$ does $P=\prod\limits_{k=1}^n a|\sin{k}|\to\infty$ as $n\to\infty$ ? Experimenting on desmos, it seemed that if $a>2$ then $P\to\infty$ , but some strange cases like $\prod\limits_{k=1}^{120000} 2.0001|\sin{k}|\approx 4\times10^{-17}$ made me doubt it. Either there exists a critical value for $a$ such that $P\to\infty$ , or $P\not\to\infty$ for all $a$ . Either way, I think it's astounding.","['infinite-product', 'limits', 'trigonometry']"
4538609,Limit of 2 variable function,"I'm trying to determine the limit of this function: $$\lim_{(x,y)\to (1,2)} \frac{xy^2-4xy-y^2+4x+4y-4}{x^2+y^2-2x-4y+5}$$ I tried to approach in many different ways, such as $$\lim_{t\to 1} f(t,2t) \quad, \quad\lim_{t\to 1} f(t,2) \quad, \quad \lim_{t\to 2} f(1,t) $$ But i got that the limit is 0 for all of them, tried with polar coordinates but it seems hopeless to get the limit!
How should I think there? Thanks in advance!","['multivariable-calculus', 'limits', 'calculus', 'polar-coordinates']"
4538654,"Is $A\setminus \{x_1,\dots,x_n\}$ an open set if $A$ is open?","let $A$ be an open set of the metric space $(X,d)$ . Now I wanted to prove that for any $x_1,\dots,x_n\in X$ also the set $$A\setminus \{x_1,\dots,x_n\}$$ is open. My idea was the following. Let $a\in A\setminus \{x_1,\dots,x_n\}$ . Since $A$ is open there exists $r>0$ such that $B(a,r)\subset A$ . Now let me define $$r' = \min \left \{ r, \min_{k=1, \dots,n} d(a,x_k)\right\}$$ then clearly $r'\leq r$ and hence $B(a,r')\subset U$ . But, by construction, $x_k\notin B(a,r')$ for all $k$ , hence $B(a,r') \subset A \setminus \{x_1,\dots,x_n\}$ . Does this work or did I miss something? If it is wrong could you maybe show me why?","['metric-spaces', 'analysis', 'real-analysis', 'solution-verification', 'functional-analysis']"
4538686,"How to determine at which point a polynomial function with even and odd x-exponents is symmetrical, algebraically?","Say you have a polynomial function like $f(x) = 2x^3 - 6x^2 + 8$ . This function won't be point symmetrical over $(0, 0)$ , nor will it be symmetrical over the y-axis, but after plotting it, you can see that it's point symmetrical over the point $(1, 4)$ . For a polynomial function like $g(x) = (x+1)^3$ , you can tell that it was shifted by one unit to the left on the x-axis, $h(x) = x^3$ is symmetrical over the point $(0, 0)$ , so $g(x)$ will be symmetrical over the point $(-1, 0)$ .
I don't really see how you could use this approach for $f(x)$ yet though. One approach I found that works for $f(x)$ is to take the second derivative and set it to $0$ . $$f'(x) = 6x^2 - 12x$$ $$f''(x) = 12x - 12$$ $$f''(x) = 0$$ $$12x = 12$$ $$\fbox{x = 1}$$ Now to find the corresponding y value, plug in $x = 1$ into $f(x)$ . This second derivative approach doesn't seem to work for all functions though, there could be a point where the graph changes concavity but still isn't point symmetrical over. Now my question is, is there a way to determine over which point any function like this will be symmetrical over without plotting it? In the comments, it was stated that for a point of symmetry to exist, all the derivatives of the function must be (anti-)symmetric with respect to a line x = a. How can you check if a function is (anti-)symmetric to this line algebraically?","['graphing-functions', 'calculus', 'functions', 'polynomials', 'algebra-precalculus']"
4538724,What did I do wrong solving this 2nd order differential equation?,"I'm trying to find the solution of this differential equation; $$y \cdot \frac{\partial{f}}{\partial{y}} -2x \cdot \frac{\partial{f}}{\partial{x}} = 2xy^4, \quad f(1,y) = \frac{1}{2}y^4 $$ But I think I went wrong somewhere because when I try to differentiate my solution I don't get the right answer! This is my solution,
I substituted $u = xy^2$ and $v=y$ then: \begin{align*}
\frac{\partial{f}}{\partial{x}} &= y^2 \cdot \frac{\partial{f}}{\partial{u}}\\[10pt]
 \frac{\partial{f}}{\partial{y}} &= 2xy \cdot \frac{\partial{f}}{\partial{u}} + \frac{\partial{f}}{\partial{v}}
\end{align*} Then I get: $$ \frac{\partial{f}}{\partial{v}} = 2xy^3 \implies f(v) = 2xy^3\cdot v + C(u)$$ \begin{align*}
f(x,y) &= 2xy^4 + C(xy^2)\\[5pt]
f(1,y) &= 2y^4 + C(y^2) = \frac{1}{2} y^2
\end{align*} $$\implies C(y^2) = \frac{-3}{2}y^4 \implies C(xy^2) = \frac{-3}{2}x^2y^4
$$ $$ f(x,y) = 2xy^4 - \frac{3}{2}x^2y^4$$ Which apparently is wrong, what did I miss here?","['multivariable-calculus', 'calculus', 'characteristics', 'partial-differential-equations']"
4538736,Number of non-negative integral solutions has two different formulae and gives same result?,"The problem I'm trying to solve: Determine the number of integer solutions to the equation $x_1 + x_2 + x_3 + x_4 = 7$ , where $x_i \ge 0\,\,\,\forall  i = 1,2,3,4$ . Solution using the formula mentioned in my text book: $={n+r-1 \choose r},$ where $n=4,\,r=7;$ $={4+7-1 \choose 7} = {10 \choose 7} = \boldsymbol{120}$ Now everything was fine till here until, I checked for examples on Youtube and Google. The problem there, is that most of the problems are being solved using a different formula and weirdly it gives the same result. I'm completely confused and there's no mention of the below formula in my textbook Solution using formula NOT mentioned in my text book: $={n+r-1 \choose r-1}$ where $n=7,\,r=4;$ $={7+4-1 \choose 4-1} ={10 \choose 3}=\boldsymbol{120}$ My question to the Math community is: What is this formula that is being used instead of my textbook version? Why are the values of n and r just being interchanged like that? Is there any correlation between the formula that I'm using and the one that's being used over the other mediums? Is the formula used over the other mediums correct?","['combinations', 'combinatorics']"
4538754,"Probability of choose point in interval (0,1)","Choose a point random uniformly in $(0,1)$ . Then, this point divides the interval (0,1) into two sub-intervals. Compute the expected length of the interval containing a fixed point $s \in [0,1]$ . Compute the expected distance of the randomly chosen point from $s$ . My approach: My intuition is that the expected length of the interval containing $s$ is 1/4, since on average the sub-intervals should be about length 1/2 each, and $s$ is in exactly one of them. I don't know how to mathematically prove this intuition if it's correct. I have no idea how to approach 2.","['statistics', 'probability', 'real-analysis']"
4538771,Find $\lim_{x \to \infty} x^{(2-\sin(\frac{2}{x}))}(x\sin(\frac{2}{x})-2)$ with L'Hospital's Rule,"I try to find $\lim_{x \to \infty} x^{(2-\sin(\frac{2}{x}))}(x\sin(\frac{2}{x})-2)$ with L'Hospital's Rule but get stuck. Here is my attempt. Let substitute $y = \dfrac{1}{x}$ \begin{align*}
\lim_{x \to \infty} x^{(2-\sin(\frac{2}{x}))}(x\sin(\frac{2}{x})-2) &= \lim_{y \to 0} \frac{\left(\frac{\sin(2y)}{y}-2\right)}{y^{(2-\sin(2y))}} \\
&= \lim_{y \to 0} \frac{\sin(2y)-2y}{y^{(3-\sin(2y))}} \\
\text{(L'Hospital's Rule)} &= \lim_{y \to 0} \frac{2\cos(2y)-2}{-y^{(2-\sin(2y))}(\sin(2y) + 2y\ln(y)cos(2y) - 3)}
\end{align*} After I use L'Hospital's Rule, everything seem to get messier. I think the next step is using L'Hospital's Rule again but everything will only get messier. I think I go in the wrong way since L'Hospital's Rule should not make everything worse. Where do I do wrong ?",['limits']
4538778,Necessary and sufficient condition for a quadrilateral to be split into four triangles of equal area.,"If I am given a quadrilateral, how can I prove that by there exists a point $P$ in the interior of the quadrilateral so that one can draw straight lines from this point $P$ to all the vertices of the quadrilateral (these do not necessarily form diagonals) in a manner that this produces four triangles all of whom have equal areas if and only if two edges of the quadrilateral are parallel lines. First, I am starting by assuming that all areas of the triangles are equal to $1$ . Now, let $b_k$ be the base of triangle $k$ for $k\in\{1,2,3,4\}$ and $h_k$ be the height. Then clearly $\frac{1}{2}h_kb_k=1$ for each $k\in \{1,2,3,4\}$ . However, we do not have sufficient information only by this. I think a geometric argument would be needed. EDIT: seems like my initial claim is false. It would be interesting to see if this is in fact possible for all convex quadrilaterals and if it is not, then for what type of convex quadrilaterals does the claim fail for?","['euclidean-geometry', 'geometry', 'discrete-mathematics']"
4538828,"What distinct rational multiples of $\pi$ are solutions of $\sin\alpha\sin\beta=\sin\gamma\sin\delta\,$?","When considering “Langley's Adventitious Angles” , I noticed that $$\sin40^\circ\sin50^\circ=\sin30^\circ\sin80^\circ.$$ What other distinct rational multiples of $\pi$ radians, say $0<\alpha, \beta, \gamma, \delta<\frac12\pi$ , satisfy such an equation, namely $$\sin\alpha\sin\beta=\sin\gamma\sin\delta,$$ if any? Edit: $\;$ As J.G. points out in a comment, $$\sin x\sin(\tfrac12\pi-x)=\sin x\cos x=\tfrac12\sin2x=\sin\tfrac16\pi\sin2x$$ provides a more general template for many solutions. So we are looking for solutions not of this type.","['trigonometry', 'angle']"
4538876,Properties of cosets; $aH = bH \iff b^{-1}a \in H$.,"I've found the concept of cosets to be strange when I've encountered them. I want to make sure that I'm understanding how to work with them. Claim: Given a subgroup $H \leq G$ and $a,b \in G$ , we have $aH = bH \iff b^{-1}a \in H$ . Attempt: Assume first that $aH = bH$ . This means that as sets $$
\{g \in G \mid g = ah, h \in H\} = \{g \in G \mid g = bh, h \in H\}.
$$ Therefore if $aH = bH$ then $ah_1 = bh_2$ for some $h_1,h_2 \in H$ . This implies that $b^{-1}a = h_2h_1^{-1} \in H$ . Conversely assume that $b^{-1}a \in H$ . This means that there exists some $h \in H$ such that $b^{-1}a =h$ . Then it follows that $a = bh \in bH$ and that $b = ah^{-1} \in aH$ . If we pick any arbitrary $h' \in H$ we must have that $ah' = b(hh') \in bH$ but this means that $aH \subseteq bH$ . Identically $bh' = a(h^{-1}h') \in aH$ so $bH \subseteq aH$ and we have $aH = bH$ . Is this overkill? Whenever I read problems or questions the manipulations on cosets seem to be much faster. How can I stop thinking of them as sets, or the quotient group $G/H$ as a ""set of sets""? Thanks in advance.","['quotient-group', 'group-theory', 'abstract-algebra']"
4538890,Find the general formula for the generating function $g(x) = \frac{1}{1-x} \frac{1}{1-x^5} \frac{1}{1-x^{10}} \frac{1}{1-x^{25}} \frac{1}{1-x^{50}}$,"Background: I'd like to solve the Counting change problem by generating function. The problem is described here : How many different ways can we make change of $1.00, given half-dollars, quarters, dimes, nickels, and pennies? More generally, can we write a procedure to compute the number of ways to change any given amount of money? There are many possible ways, e.g. tree recursion, dynamic programming, SMT solver, etc. My attempt is to use a generating function: This problem can be modeled as counting the number of nonnegative integral solutions of the equation: $\displaystyle{ e_1 + 5e_2 + 10e_3 + 25e_4 + 50e_5 = n}$ Thus the generating function is: $\displaystyle{ g(x) = \frac{1}{1-x} \frac{1}{1-x^5} \frac{1}{1-x^{10}} \frac{1}{1-x^{25}} \frac{1}{1-x^{50}}}$ but how to find the general formula $h(n)$ for $g(x)$ ? i.e. $g(x) = \sum_{n=0}^\infty h(n) x^n $ AFAIK, the traditional method is to use partial fractions, i.e. $ \displaystyle{\frac{1}{1-x} \frac{1}{1-x^5} \frac{1}{1-x^{10}} \frac{1}{1-x^{25}} \frac{1}{1-x^{50}} = \frac{c_1}{1-x} + \frac{c_2}{1-x^5} + \frac{c_3}{1-x^{10}} + \frac{c_4}{1-x^{25}} + \frac{c_5}{1-x^{50}}}  $ then simplify it and solve $c_1$ , $c_2$ , $c_3$ , $c_4$ , $c_5$ . However, for this example it seems very difficult ... (or better method?) My question is that Is there any mathematical tool that can find the general formula of the  generating function directly? I have tried the generating function in wolfrmalpha, it cannot get the general formula (although it can get Taylor series). Thanks. P.s. I believe that once we get the general formula, computing the number of ways to change any given amount of money will be very fast.","['math-software', 'combinatorics', 'generating-functions']"
4538944,"Meaning of ""$X$ is a sample from population $P \in \mathcal{P}$"" in Jun Shao's ""Mathematical Statistics""","I am reading the textbook ""Mathematical Statistics"" 2nd edition written by Jun Shao http://www.mim.ac.mw/books/Mathematical%20statistics%202nd%20edition.pdf . In precise terms, what does the author mean by "" $X$ is a sample from population $P \in \mathcal{P}$ ""? To phrase my question in another way, how is $X$ related to probability measure $P \in \mathcal{P}$ ? Lots of theorems are started with this sentence and no precise definition was found. I need to sort it out before moving on. Here are the earliest related definitions I can find: On page 91-92: In statistical inference and decision theory, the data set is viewed as a realization or observation of a random element defined on a probability space $(\Omega, \mathcal{F}, P)$ related to the random experiment. The probability measure $P$ is
called the population. The data set or the random element that produces the data is called a sample from $P$ . A population $P$ is known if and only if $P(A)$ is a known value
for every event $A \in \mathcal{F}$ . Later on page 92: In statistical inference and decision theory, the data set, $(x_1, ..., x_n)$ , is
viewed as an outcome of the experiment whose sample space is $\Omega = \mathcal{R}^n$ .
We usually assume that the $n$ measurements are obtained in n independent trials of the experiment. Hence, we can define a random $n$ -vector $X = (X_1, ..., X_n)$ on $\prod_{i=1}^n(\mathcal{R}, \mathcal{B},P)$ whose realization is $(x_1, ..., x_n)$ . The
population in this problem is $P$ (note that the product probability measure
is determined by $P$ ) and is at least partially unknown. But later, the author does not seem to be very certain about this. For example, on page 96, there is a sentence saying "" $\Omega$ is usually $\mathcal{R}^k$ . On page 100, the beginning of chapter 2.2 says ""Let us assume now that our data set is a realization of a sample $X$ (a
random vector) from an unknown population $P$ on a probability space."" The paragraph on page 92 seems to suggest that $X = (X_1, \cdots, X_n)$ has nothing to do with $P$ : it just seems to be a random variable defined on a measurable space $\prod_{i=1}^n(\mathcal{R}, \mathcal{B})$ . The way the sentence in question constructed seems to suggest that $X$ and $P$ are somehow related: and they should be related as many important theorems (like theorem 2.2 on page 104) all start with this sentence. My guess to make sense of it: Whenever the sentence "" $X$ is a sample from population $P \in \mathcal{P}$ "" is mentioned, assume the following: There is a probability space $(\Omega, \mathcal{F}, \mu)$ and $X = (X_1, \cdots, X_n): \Omega \to \mathcal{R}^n$ is a Borel function (i.e. $\mathcal{R}^n$ valued random variable or $n$ -dimensional random vector). $\mathcal{P}$ is a (given) collection of Borel probability measures on $\mathcal{R}$ . The probability measure $\mu$ on $\mathcal{F}$ is on a different space and is a priori assumed to be unrelated to $\mathcal{P}$ . $X_1, \cdots, X_n$ are independent and identically distributed with common law $P$ and it just happens that $P \in \mathcal{P}$ . Is my guess correct ?","['statistical-inference', 'measure-theory', 'probability-theory', 'statistics']"
4538963,How to show $ \int_{-\infty}^{\infty} \frac{e^{-(x+1)^2}}{1+e^{-x}}\mathrm{d}x = \frac{\left(2\sqrt[4]{e} -1 \right)\sqrt{\pi}}{2e}$?,"I was recently looking at this post where the following formula is shown: $$
\int_{-\infty}^{\infty} \frac{E(x)}{1+\mathcal{E}(x)^{O(x)}}\mathrm{d}x= \int_0^{\infty} E(x) \mathrm{d}x
$$ where $E(x), \mathcal{E}(x)$ are even functions and $O(x)$ is an odd function. One nice application of this formula would be the integral $$
\int_{-\infty}^{\infty} \frac{e^{-x^2}}{1+e^{-x}}\mathrm{d}x = \frac{\sqrt{\pi}}{2}
$$ where the problem reduces to the evaluation of the Gaussian integral. I then wondered what would happen if I made slight alterations to the above integral, like changing $x^2\to (x+1)^2$ . WA evaluates said integral as: $$
 \int_{-\infty}^{\infty} \frac{e^{-(x+1)^2}}{1+e^{-x}}\mathrm{d}x = \frac{\left(2\sqrt[4]{e} -1 \right)\sqrt{\pi}}{2e}
$$ The even/odd formula can't be applied since the $+1$ makes the function not even anymore. Recalling that $\int^\infty_{-\infty} e^{-(ax^2 + bx+c)}\mathrm{d}x=\sqrt{\frac{\pi}{a}}e^{\frac{b^2}{4a}-c}
$ I attempted to evaluate the integral using geometric series $$
\int_{-\infty}^{\infty} \frac{e^{-(x+1)^2}}{1+e^{-x}}\mathrm{d}x =  \sum_{n\ge 0}(-1)^n \int_{-\infty}^{\infty}e^{-(x^2+(n+2)x+1)}\, \mathrm{d}x = \frac{\sqrt{\pi}}{e}  \sum_{n\ge 0}(-1)^n e^{\frac{(n+2)^2}{4}}
$$ but the resulting series is divergent, so this method won't work. Does anyone have any ideas on how to evaluate this integral? Thank you!","['integration', 'definite-integrals', 'calculus', 'closed-form', 'gaussian-integral']"
4538972,Find upper bound for T(n) in terms of n and prove with Master Theorem,"Sorry about the formatting. Had nothing but my cellphone! Problem : Suppose this is an algorithm that runs in T(n) time, where T(n) is the following recurrence relation: T(n) = $2T($$ \frac n3$$) + Θ(n),  x > 2$ $Θ(1), x ≤ 2$ Draw a tree and find the upper bound of T(n) in terms of n. Prove upper bound using Master Theorem. My tried solution : The recursion tree that I ended up with; Levels n $$ \frac n3 \frac n3 $$ $$ \frac n9 \frac n9 \frac n9 \frac n9 $$ ..... $$(1) (1) ... (1)$$ Work for each level. n $$ \frac n3 + \frac n3 = \frac 23n$$ ... k nodes. Since 2T, I got $2^k$ with work $$\frac {n}{3^k}$$ , which
in the end would be $$ \left(\frac{2}{3}\right)^kn $$ Since there are $$log_3 (n)$$ levels, I ended up with $$n = \frac{n}{1-\frac{2}{3}} = 3n$$ EDIT: which in the end would be O(n). When I try to prove this using Master Theorem, I can't for the life of me get it to work for any of the 3 cases. For example, for case 1: I try to prove T(n) ≤ c*n and my end result is T(n) ≤ $$\frac 23n + n $$ Is my tree completely wrong or is my understanding of the master theorem wrong? Any insight would be greatly appreciated, thanks!","['asymptotics', 'recurrence-relations', 'analysis-of-algorithms', 'discrete-mathematics', 'algorithms']"
4538987,"T/F: If $(x_n)$ is a positive real sequence s.t. $\sum x_n$ converges, then $\exists N$ s.t. $x_{\left\lceil\frac{1}{x_N }\right\rceil}<\frac{1}{N}.$","Proposition : Suppose $(x_n)$ is a positive real sequence such that $\displaystyle\sum_n x_n$ converges, then there exists $N\in\mathbb{N}$ such that $\ \large{ x_{ \left\lceil \frac{1}{x_N } \right\rceil } } < \frac{1}{N}. $ Attempt $1$ : $\ \displaystyle\sum_n x_n$ converges $\implies \neg \left( x_n \geq \frac{1}{n}\  \forall n\in\mathbb{N} \right). $ Therefore $\exists N\ $ such that $x_N < \frac{1}{N},\ \implies \left\lceil \frac{1}{x_N } \right\rceil > N.$ Now what? I'm also also not making much progress via proof by contradiction. But it feels like it must be true somehow...","['recreational-mathematics', 'real-analysis', 'sequences-and-series', 'convergence-divergence', 'problem-solving']"
4539018,Assistance with idempotent matrices,"I am taking linear algebra for the first time and am struggling with the concept of idempotent matrices. I know that $A = A^2$ is the concept behind it, but I can't seem to understand HOW one would find the entries, and the explanations given confused me quite a lot. I was hoping someone could give me the gist of this concept and point me in the right direction. For example, I am faced with the question of ""Find all $2\times 2$ matrices such that $A^2=A$ "" Currently, I know that with the matrix $A = \begin{bmatrix}a&b\\c&d\end{bmatrix}$ that: $a = a^2 + bc\\
b = ab + bd\\
c = ca + cd\\
d = bc + d^2$ but I'm now unsure of how to determine entries off of that.","['matrices', 'linear-algebra', 'idempotents']"
4539101,"For Hirzebruch Surfaces, does the tangent exact sequence split?","Consider the projection $\pi:\mathbb{F}_n \rightarrow \mathbb{P}^1$ . Is it true that the following exact sequence $$0 \rightarrow T_{\pi} \rightarrow T_{\mathbb{F}_n} \rightarrow \pi^*T_{\mathbb{P}^1} \rightarrow 0$$ splits? Since $n=0$ case is trivial, I would like to see what happens when $n \ge 1$ . I thought there might be some standard source for this but I could not find it on top of my head. (Edited): I found a way to check it by using brute-force method from toric charts, but would like to find a simple intrinsic way of doing it. For instance, the Ext space for this exact sequence has positive dimension by using Hirzebruch-Riemann-Roch theorem on $\mathbb{F}_n$ , so what makes tangent bundle correspond to the zero element of the Ext?","['exact-sequence', 'algebraic-geometry', 'tangent-bundle', 'toric-varieties']"
4539132,"prove that there exist $x_1\neq x_2, x_1,x_2\in [0,1]$ so that $\int_{x_1}^{x_2} f(x)dx = (x_1-x_2)^{2002}$","Let $f : [0,1]\to\mathbb{R}$ be an integrable function so that $0 < |\int_0^1 f(x)dx| \leq 1$ . Prove that there exist $x_1\neq x_2, x_1,x_2\in [0,1]$ so that $\int_{x_1}^{x_2} f(x)dx = (x_1-x_2)^{2002}$ . Let $F : [0,1]\to \mathbb{R}$ be given by $F(x) = \int_0^x f(t)dt.$ Note that if we have $|F(b)-F(a)| \ge |b-a|^{2002}$ for all $a\neq b\in [0,1],$ then we can just choose $x_1 = a, x_2 = b$ or vice versa (so that the sign of $F(x_2)-F(x_1)$ is positive) so that $1\ge |F(1)-F(0)| \ge 1.$ If $|F(b)-F(a)|\leq |b-a|^{2002}$ for all $b\neq a$ in $[0,1]$ , then for an arbitrary $a\in [0,1],$ letting $b\to a,$ we get that $|F'(a)| \leq \lim\limits_{b\to a} |b-a|^{2001} = 0$ and hence $F'(a) = 0$ for all a. Hence $F$ is constant, contradicting $0 < |F(1)-F(0)|.$ Thus there exist $a,b,c,d\in [0,1]$ with $|F(b)-F(a)| < |b-a|^{2002}$ and $|F(d)-F(c)| > |d-c|^{2002}$ . But I'm not sure how to proceed from here. Perhaps using the Intermediate Value Theorem might be useful?","['integration', 'real-analysis', 'calculus', 'inequality', 'derivatives']"
4539145,Locus of points traced by the Chebyshev lambda linkage,"I was looking around on the interwebs when I found a GIF of the locus of points traced by this mechanism called the Chebyshev lambda linkage , and I was curious if there is an equation that describes the baguette-shaped locus of points traced by it. Can you give me some pointers on how to go about deriving such a formula? Here's that GIF I mentioned:","['curves', 'locus', 'geometry']"
4539154,can one select 102 17-element subsets of a 102-element set so that the intersection of any two of the subsets has at most 3 elements,"Can one select 102 17-element subsets of a 102-element set so that the intersection of any two of the subsets has at most 3 elements? I'm not sure how to approach this problem. I think it might be useful to try to find a generalization of the result so one can work with smaller numbers and try to find useful lemmas/properties. In particular, $17$ is prime, so one could replace that with $p$ . Then $p(p+1)/3 = 102.$ So it might be reasonable to guess that one can always select $p(p+1)/3$ $p$ -element subsets of a $p(p+1)/3$ -element set so that the intersection of any two of the subsets has at most $3$ elements. Also, $17\cong 2\mod 3, 17\cong 2\mod 5.$ For $p=2,3$ the problem is straightforward. For $p=5,$ we need to find $10$ $5$ -element subsets of a 10-element set so the intersection of any two has at most $3$ elements. I'm not sure how to find the subsets in this case. It might be useful to consider something related to modular arithmetic modulo the prime p.","['contest-math', 'modular-arithmetic', 'number-theory', 'elementary-number-theory', 'combinatorics']"
4539161,Question about the definition of a homomorphism,"In Fraleigh's abstract algebra book, he gives definitions on how structure carries over between two isomorphic binary structures. The first definition is given relatively early in the book (Section 3) as the fourth criteria for defining an isomorphism : $$ \phi(x\space *\space y) = \phi(x) \space \bar* \space \phi(y)$$ Where $*$ and $\bar *$ are potentially two different operations. Later on, in Section 13 he gives the definition of a homomorphism as such: A map $\phi$ of a group G into a group $\bar G$ is a homomorphism if the homomorphism property $$ \phi(ab) = \phi(a)\phi(b)$$ Holds for all $a,b \in G$ Unlike for his property of an isomorphism , where $*$ and $\bar *$ are implied to be different operations, his definition of a homomorphism implies that $$ \phi(a \cdot b) = \phi(a) \space \cdot \space \phi(b)$$ Where the multiplicative notation implies that the operation $\cdot$ must be the same in both groups. Does this mean then for example, that defining an isomorphic relation between two binary structures allows something of the form $$ \phi(a \space + \space b) = \phi(a) \space \times \space \phi(b) $$ Where $+$ and $\times$ are different operations, but I must define a homomorphism with the form $$\phi(a \space \times \space b) = \phi(a) \space \times \space \phi(b) $$ Where $\times$ is the same operation for both groups? If that is the case, why can an isomorphism use two different operations, but a homomorphism must use the exact same operation? Thank you for your time.","['group-homomorphism', 'group-isomorphism', 'abstract-algebra', 'binary-operations', 'group-theory']"
4539162,Counterexample that the arbitrary union of compact sets is compact,"I want to show that it is not true that for any metric space $(X,d)$ , the arbitrary union of compact subsets of $X$ is always compact. For this I use the following counterexample: Consider the metric space $(\mathbb{R},d)$ with $d$ being the euclidean metric. Let $\mathcal{A}=\left\{[a,a]:a\in\mathbb{R}\right\}$ , note that every element in $\mathcal{A}$ is a closed and bounded interval, then by the Heine-Borel theorem, each element of $\mathcal{A}$ is compact in this metric space. Note that $\cup\mathcal{A}=\bigcup_{a\in\mathbb{R}}[a,a]=\mathbb{R}$ , which is not compact. Then we found a metric space for which there is an arbitrary union of compact subsets that is not compact. Is this counterexample right? I'm a bit dubious about the use I'm making of the notion of ""arbitrary"" union, since I'm not positive that $\mathbb{R}$ is actually an arbitrary index set (since at the end I'm choosing the index set). In other words, I think I'm implying that ""arbitrary"" and ""uncountable"" are equivalent notions in this context, fact that I'm not sure of. Thank you!","['solution-verification', 'metric-spaces', 'real-analysis']"
4539195,Significance of Hahn-Banach theorem.,"I am doing a course on functional analysis. One of the pillars in functional analysis is Hahn-Banach theorem. But in different books I find different versions of the theorem and it is not easy to check that they are equivalent. So, I want to understand why Hahn-Banach theorem is important. Also the proof uses Zorn's lemma which is equivalent to axiom of choice, so I cannot understand it by concrete examples. The statement given by our instructor for Hahn-Banach theorem is as follows: Let $X$ be a normed linear space and $X_0$ be a subspace of $X$ ,let $f_0$ be a bounded linear functional on $X_0$ ,then there is a bounded linear functional $f$ on $X$ such that $f|_{X_0}=f_0$ and $||f||=||f_0||$ i.e. $\sup\limits_{||x||=1,x\in X}|f(x)|=\sup\limits_{||x||=1,x\in X_0}|f_0(x)|$ . I want to understand the usefulness of the fact that $||f||=||f_0||,$ i.e. its physical significance and why it is good to have such an extension. Can someone give me some idea?","['normed-spaces', 'motivation', 'functional-analysis', 'intuition', 'hahn-banach-theorem']"
4539209,Trouble with tedious algebra (Oxford 1992 Admissions Test 2 1992),"(i) Show that the condition that the points $P$ $(a\cos A,b\sin A)$ and $Q$ $(a\cos B,b\sin  B )$ should subtend a right angle at O is $$a^2\cos A\cos B+b^2\sin A\sin  B=0$$ (ii) Let S be a circle centre $O$ and radius $C$ . Find the equation of the tangent to S at
the point $(C\cos  t, C\sin t)$ . (iii) If $C = \dfrac{ab}{\sqrt{a^2+b^2}}$ , show that the points where a tangent to S cuts the ellipse $$\dfrac{x^2}{a^2}+\dfrac{y^2}{b^2}=1$$ subtend a right angle at $O$ . Part (i) can be done by considering gradients of lines from $O$ to each of the points, multiplying them and setting equal to $-1$ Part (ii) gives the result $x\cos t+y\sin t=C$ With part (iii), I have attempted to simply rearrange for $y$ in the tangent equation, subbing into the ellipse equation and trying to solve for $x$ but this results in a huge amount of tedious algebra and I am unable to simplify it properly. I then attempted to parameterize the ellipse in the form $x=a\cos T, y=b\sin T$ but I am unsure how to go from there to solve for the points. I did attempt to use harmonic addition and this does give some exact solutions in terms of $a,b,t$ but to simplify requires identities for $\sin(\arccos(x))$ and other similar identities like $\arctan\left(\frac{b}{a}\tan t\right)$ . Some useful information may be that we can let the points of intersections be $R(a \cos P,b\sin P)$ and $L(a\cos Q,b\sin Q)$ and substitute these points into the equation for the tangent. Now my problem is taking those equations and getting the equation we want from (i)","['conic-sections', 'parametric', 'circles', 'geometry']"
4539222,"If $x=\frac{1}{2^2}+\frac{1}{3^2}+\cdots+\frac{1}{2015^2}$, show that $\frac{201}{403}<x<\frac{2014}{2015}$","If $$x=\frac{1}{2^2}+\frac{1}{3^2}+\cdots+\frac{1}{2015^2}$$ show that $$\frac{201}{403}<x<\frac{2014}{2015}$$ So, I manage to do the RH inequality using that $$x<\frac{1}{2\cdot3}+\frac{1}{3\cdot4}+\cdots+\frac{1}{2014\cdot2015}=1-\frac{1}{2015}=\frac{2014}{2015}$$ Unfortunately I can't find a way to do the LH inequality. I saw that $$\frac{1}{2015}+\frac{1}{2015}+\cdots+\frac{1}{2015}$$ for 1005 times is equal to $\frac{201}{403}$ but I don't see why $$x> \frac{1}{2015}+\frac{1}{2015}+\cdots+\frac{1}{2015}$$ thx!","['riemann-sum', 'algebra-precalculus', 'inequality']"
4539319,How to go from Radon–Nikodym derivative to classical derivative in change of variables formula of p.d.f.?,"Let $X$ be an absolutely continuous real-valued random variable whose  distribution is $\mu_X$ and whose p.d.f. is $p_X$ . Let $f:\mathbb R \to \mathbb R$ be differentiable such that $f'(t)>0$ for all $t\in \mathbb R$ . This implies $f$ is strictly increasing. Let $Y := f(X)$ . Clearly, the distribution $\mu_Y$ of $Y$ is absolutely continuous w.r.t. Lebesgue measure $\lambda$ . Let $F_Y,p_Y$ be the c.d.f. and p.d.f. of $Y$ respectively. Clearly, $\mu_Y$ is absolutely continuous w.r.t. $\mu_X$ . According to this Wikipedia page , $$
p_Y := \frac{\mathrm d  \mu_Y}{\mathrm d  \lambda} = \frac{\mathrm d  \mu_Y}{\mathrm d  \mu_X} \frac{\mathrm d  \mu_X}{\mathrm d  \lambda}.
$$ Here $\frac{\mathrm d  \mu_Y}{\mathrm d  \mu_X}$ is the Radon–Nikodym derivative of $\mu_Y$ w.r.t. $\mu_X$ . Similarly, $\frac{\mathrm d  \mu_X}{\mathrm d  \lambda}$ is the Radon–Nikodym derivative of $\mu_X$ w.r.t. $\lambda$ . At page $14$ of this lecture note , the author mentioned that $$
p_Y (t) = \frac{\mathrm d F_Y (t)}{\mathrm d t} = \frac{\mathrm d  F_X (f^{-1} (t))}{\mathrm d  t} \frac{\mathrm d  f^{-1} (t)}{\mathrm d t} =\frac{p_X (f^{-1} (t))}{f'(f^{-1} (t))}.
$$ Clearly, the map $F_\lambda:\mathbb R \to \mathbb R, t \mapsto t$ is the corresponding c.d.f. of $\lambda$ . Obviously, the p.d.f. of $\lambda$ is $p_\lambda:\mathbb R \to \mathbb R, t \mapsto 1$ . It seems to me the author meant by $\frac{\mathrm d  F_X (f^{-1} (t))}{\mathrm d  t}$ the value of $p_X :=\frac{\mathrm d  \mu_X}{\mathrm d  \lambda}$ at $f^{-1} (t)$ , i.e., $$
\frac{\mathrm d  F_X (f^{-1} (t))}{\mathrm d  t} = \frac{\mathrm d  \mu_X}{\mathrm d  \lambda} (f^{-1} (t)) =p_X (f^{-1} (t)).
$$ Could you elaborate on how to transfer from the Radon–Nikodym derivative $\frac{\mathrm d  \mu_Y}{\mathrm d  \mu_X}$ to the classical derivative $\frac{\mathrm d  f^{-1} (t)}{\mathrm d t}$ , i.e., $$
\frac{\mathrm d  \mu_Y}{\mathrm d  \mu_X} (t) = \frac{\mathrm d  f^{-1}}{\mathrm d t} (t)?
$$","['measure-theory', 'probability-distributions', 'real-analysis', 'probability-theory', 'radon-nikodym']"
4539328,How to solve $\frac{dy}{dx}-1=\frac{x^2}{y}$,How to solve $$\frac{dy}{dx}-1=\frac{x^2}{y}$$ I tried letting $y^2=u$ which gives $$y\frac{dy}{dx}=\frac{1}{2}\frac{du}{dx}$$ So the equation is now $$\frac{du}{dx}-2\sqrt{u}=2x^2$$ Any help?,"['algebra-precalculus', 'ordinary-differential-equations', 'substitution']"
4539379,Length of a super-circle,"In a youtube video titled ""Generalizing the circumference"" Prof. Michael Penn works out the integral $$
L(n)=4 \int_{0}^{\pi/2}\sqrt{n^2 \cos^{2n-2}(\theta) \sin^2(\theta)+n^2 \sin^{2n-2}(\theta) \cos^{2}(\theta)} \, d\theta,
$$ which gives the arclength of $(x^2)^{\frac1n}+(y^2)^{\frac1n}=1$ ""supercircle"" for various values of $n$ .  In particular the values $L(1)=2\pi$ is the circumference of the circle, $L(2)=4\sqrt{2}$ the circumference of a diamond shape, and $L(3)=6$ .  At the end of the video it is stated that the value when $n\geq 4$ has no closed form, but converges towards a known value.  Surely, that's the case, but it seems that at least Wolfram alpha can work out some of the values for $n\geq 4$ .  I got for example $$
L(4) = 4 -\sqrt{2}\log(1-\frac{1}{\sqrt{2}})+\sqrt{2}\log(1+\frac{1}{\sqrt{2}})\approx 6.49
$$ Similarly, for $n=5$ we get an explicit form $$
L(5)=\frac{5}{6}\left(6+\sqrt{3}\log{\left(2+\sqrt{3}\right)}\right)\approx 6.90
$$ My follow up question is how do you derive the values for $n=4$ and $n=5$ and can you derive values for other values of $n$ as well?  It seems integer values of $n\geq 4$ are possible?","['integration', 'arc-length']"
4539393,Every diagrammatic map has only finitely many double points,"Definition: A smooth map $\gamma\colon \Bbb S^1\to \Bbb R^2$ is called diagrammatic if the following conditions are satisfied: $(1)$ the map is an immersion, i.e., derivative at each point is non-zero, $(2)$ if $z\neq w\in \Bbb S^1$ satisfy $\gamma(z)=\gamma(w)$ , then $\gamma'(z)$ and $\gamma'(w)$ are linearly independent, $(3)$ given any $p\in \gamma(\Bbb S^1)$ , the preimage $\gamma^{-1}(p)$ consists of either one or two points. Furthermore, any $p\in \gamma(\Bbb S^1)$ for which there exist $z\neq w\in
 \Bbb S^1$ with $\gamma(z)=p=\gamma(w)$ is called a double point of $\gamma$ . I am solving the following Problem: Problem: Every diagrammatic map has only finitely many double points. My Idea: Suppose not, then there is a sequence $\{p_n\}$ of distinct double points of $\gamma$ . Passing to a subsequence, if needed, and using compactness of $\gamma(\Bbb S^1)$ , we may assume that $p_n\to \ell\in \gamma(\Bbb S^1)$ . Write $\{z_n,w_n\}=\gamma^{-1}(p_n)$ . Passing to subsequence and compactness of $\Bbb S^1$ , let $z_n\to z$ . Thus $\gamma(z)=\ell$ . Now, $\gamma'(z_n)\to \gamma'(z)$ and $\gamma'(w_n)\to \gamma'(z)$ as $\gamma$ is $C^\infty$ -smooth. Since $\gamma'(z_n)$ and $\gamma'(w_n)$ are linearly independent, $\gamma'(z)=0$ , a contradiction to the assumption that $\gamma$ is an immersion. Is my idea correct??","['smooth-manifolds', 'knot-theory', 'differential-topology', 'algebraic-topology', 'differential-geometry']"
4539411,Prove the commutation of smallest sigma-algebra and inverse,"Let $f:Ω \toΩ'$ , and let $G$ be a class of subsets of $Ω'$ . Show that $\sigma(f^{-1}(G))=f^{-1}(\sigma(G))$ . It is not hard to verfify that $f^{-1}(\sigma(G))$ is a $\sigma$ -algebra, and also $G\subset\sigma(G)$ , so $f^{-1}(G)\subset f^{-1}(\sigma(G))$ so we have $\sigma(f^{-1}(G))\subset f^{-1}(\sigma(G))$ , but how to prove the other direction?","['elementary-set-theory', 'real-analysis']"
4539422,"In multivariable calculus, why do we normalize $\frac{\partial}{\partial \theta}$ in polar coordinates?","I'm TA'ing multivariable this semester, and I just noticed that we always tend to normalize all our basis vectors when using polar coordinates. This is in stark contrast what I'm used to in differential geometry, as we'd prefer that our coordinate basis to transform by the law \begin{align*} 
\frac{\partial}{\partial x}&=\frac{\partial r}{\partial x}\frac{\partial}{\partial r}+\frac{\partial \theta}{\partial x}\frac{\partial}{\partial \theta}\\
&=\cos\theta\frac{\partial}{\partial r}-\frac{\sin\theta}{r}\frac{\partial}{\partial \theta},
\end{align*} and similarly with $\frac{\partial}{\partial y}$ . This $\frac{1}{r}$ factor makes up for the fact that if we travel in the angular direction, we cover more ground the further we are from the origin. So for example, the gradient in these ""geometric"" polar coordinates would take on the form $$\nabla f |_{(r,\theta)}=(\frac{\partial}{\partial r},\frac{1}{r^2}\frac{\partial}{\partial \theta})$$ which agrees with the usual way of defining gradients locally by $\nabla f=g^{ij}(\partial_if)\partial_j$ . This in opposition to the more common $\frac{1}{r}$ factor which comes using the normalized polar coordinate system. So why are we normalizing these coordinates? If you insist on working in an orthonormal frame, why not call it a polar frame instead of polar coordinates to avoid bad practices in the future? Edit: Let me put in an explicit computation in with the ""geometric"" (which I learned is called holonomic) basis. Consider $$f(x,y)=\frac{x}{x^2+y^2},$$ so that in polar coordinates, $$f(r,\theta)=\frac{\cos\theta}{r}.$$ One sees: \begin{align*}
    \nabla f&=\frac{\partial f}{\partial x}\bigg\vert_{(r,\theta)}\frac{\partial}{\partial x}+\frac{\partial f}{\partial y}\bigg\vert_{(r,\theta)}\frac{\partial}{\partial y}\\
    &=\frac{\sin^2\theta-\cos^2\theta}{r^2}\left(\cos\theta\frac{\partial}{\partial r}-\frac{\sin\theta}{r}\frac{\partial}{\partial \theta}\right)-\frac{2\cos\theta\sin\theta}{r^2}\left(\sin\theta\frac{\partial}{\partial r}+\frac{\cos\theta}{r}\frac{\partial}{\partial \theta}\right)\\
    &=\frac{\sin^2\theta\cos\theta-\cos^3\theta-2\cos\theta\sin^2\theta}{r^2}\frac{\partial}{\partial r}+\frac{-\sin^3\theta+\cos^2\theta\sin\theta-2\cos^2\theta\sin\theta}{r^3}\frac{\partial}{\partial \theta}\\
    &=-\frac{\cos\theta}{r^2}\frac{\partial}{\partial r}-\frac{\sin\theta}{r^3}\frac{\partial}{\partial \theta}\\
    &=\frac{\partial f}{\partial r}\frac{\partial }{\partial r}+\frac{1}{r^2}\frac{\partial f}{\partial \theta}\frac{\partial}{\partial \theta}.
\end{align*}","['coordinate-systems', 'multivariable-calculus', 'polar-coordinates', 'education', 'soft-question']"
4539432,Composition of infinite comparison functions,"A $\mathcal{K}$ function $\alpha: \mathbb{R}_+ \rightarrow \mathbb{R}_+$ obeys $\alpha(0) = 0$ and $\alpha(a) > \alpha(b)$ for $a > b$ . An identity function $\mathrm{id}: \mathbb{R}_+ \rightarrow \mathbb{R}_+$ obeys $\mathrm{id}(s) = s$ . Suppose we have some $\mathcal{K}$ function $\alpha < \mathrm{id}$ , i.e., $\alpha(s) < \mathrm{id}(s)$ for all $s > 0$ . Given an $s \in \mathbb{R}_+$ , do we have $\lim_{k \rightarrow \infty}\underbrace{\alpha\circ\alpha\circ \cdots \circ \alpha}_{k}(s) = 0$ (here $\circ$ denotes composition of functions)? My intuition is that the above claim may not be true, and the answer could be $s - c$ with any constant $c < s$ . But I have no idea how to proceed the proof, and I can not come up with some example.","['stability-theory', 'functions', 'control-theory', 'real-analysis']"
4539456,A question about the theorem of characteristic function,"The statement is: Let $X$ be a real-valued random variable with characteristic function $\phi_X(.)$ .Let $Z=N(0,1)$ be independent of $X$ .For each $\sigma>0$ the random variable $X_{\alpha}=X+\sigma Z$ has a density $f_{\alpha}$ given by, $$f_{\alpha}(x)=\frac{1}{2\pi}\int_{-\infty}^{\infty}e^{itx}\phi_{X}(t)e^{\frac{-\sigma^2t^2}{2}}dt$$ proof: Fix $\sigma>0$ using the independence of $X$ and $Z$ we have $$P(X_{\sigma}{\leq\alpha})=\int_{\mathbb{R}}F_Z(\frac{\alpha-x}{\sigma})d\mu_X(x)$$ $$\int_{\mathbb{R}}\int_{-\infty}^{\frac{\alpha-x}{\sigma}}\frac{1}{\sqrt{2\pi}}e^\frac{-a^2}{a}da d\mu_X(x)$$ I have marked the step where I am getting confused. What I have understood is that $$P(X_{\sigma}\leq\alpha)=P(X+\sigma Z\leq \alpha)$$ now I am confused how we got this equals to the integral given in the proof.","['measure-theory', 'probability-distributions', 'probability-theory', 'probability', 'random-variables']"
4539462,(Short proof verification) Problem related to distance to a set,"I have this previous result: Let $(E, d)$ be a metric space and $K\subset E$ a compact subset. Then, for all $x\in E$ there is a $k_x \in K$ such that $d(x, K)=d(x, k_x)$ , where $d(x, K)=\inf\{d(x, k):k\in K\}$ The problem is to show that in case $E=\mathbb{R}^n$ with the usual distance, the previous statement holds for $C \subset \mathbb{R}^n$ (a closed not empty subset) instead of a compact one. My proof is as follows: Let $x \in \mathbb{R}^n$ . Take $c_0 \in C$ . Now set $r=d(x, c_0)$ , and consider $W=C \cap \bar{B}(x, r)$ (that is the closed ball of center $x$ and radius $r$ ).  Because $W$ is the intersection of two closed sets, it is closed. It is also bounded, because it is contained in a ball. In $\mathbb{R}^n$ a closed and bounded subset is compact, so apply the previous result to get $k_x \in W$ such that $d(x, k_x)=d(x, W)$ . To see that $d(x, k_x)=d(x, C)$ we consider the definition of distance from a point to a set and the following inequalities: $d(x, k_x) \leq d(x, c_0)=r \leq d(x, k) \ \forall k \in C\backslash W$ , so $d(x, k_x)=d(x, C)$ As requested in the comments, I edit the last sentence:
As $d(x, k_x)=\inf \{d(x, w) : w \in W\}$ , and $c_0 \in W$ , $d(x,k_x) \leq d(x, c_0)=r$ . However, $d(x, c)>r \ \forall c \in C$ such that $c\notin W$ , so $d(x, k_x)=d(x, C)$ Thanks in advance.","['normed-spaces', 'metric-spaces', 'analysis', 'general-topology', 'compactness']"
4539465,Does the inverse function theorem require continuity as a hypothesis?,"This question is about the inverse function theorem for real-valued functions. Suppose $f$ is a one-to-one, that $a$ is in the domain of $f$ , and that $f$ is defined on an open interval containing $a$ . Suppose further that $f$ is differentiable at $a$ , and $f'(a)\neq0$ . Does it follow that $f^{-1}$ is differentiable at $f(a)$ , and $$
\bigl(f^{-1}\bigr)'\bigl(f(a)\bigr)=\frac{1}{f'(a)} \, ?
$$ I ask this question because some presentations of the inverse function theorem (e.g. in Spivak's Calculus ) seem to additionally require that $f$ is continuous on an open interval containing $a$ . I see three possibilities: That the hypotheses given above imply that $f$ is continuous on an open interval containing $a$ , and so it is redundant to state this as a hypothesis. That the hypotheses given above do not imply that $f$ is continuous on an open interval containing $a$ , but the theorem holds anyway. That the hypothesis that $f$ is continuous on an open interval containing $a$ is in fact necessary, and so there is a counter-example to the ""theorem"" stated above.","['inverse-function', 'inverse-function-theorem', 'real-analysis', 'calculus', 'derivatives']"
4539484,Does there exist a real $\alpha$ such that series $\{n^\alpha x_n\}$ converges to a non-zero number?,"If $x_{n+1}=x_n(1-x_n^2)$ , $x_1 \in (0,1)$ , does there exist a real $\alpha$ such that series $\{n^\alpha x_n\}$ converges to non-zero number? I have written a program to check if the series converges. Plugging in $\alpha=1/2$ it seems that $\lim_{n\to\infty}{(\sqrt{n}~x_n)}$ always converges to $\frac{\sqrt{2}}{2}$ , but I couldn't find an elegant way to prove that. So far, I have proved that $\lim{\frac{x_{n+1}}{x_n}}=1$ and $x_{n+1}<x_n$ , but I didn't find that very useful. Any help is appreciated!","['limits', 'sequences-and-series', 'real-analysis']"
4539557,Find $\cos\frac{\pi}{12}$ given $\sin(\frac{\pi}{12}) = \frac{\sqrt{3} -1}{2 \sqrt{2}}$,"Find $\cos\frac{\pi}{12}$ given $\sin(\frac{\pi}{12}) = \frac{\sqrt{3} -1}{2 \sqrt{2}}$ From a question I asked before this, I have trouble actually with the numbers manipulating part. Using trigo identity, $\sin^2 \frac{\pi}{12} + \cos^2 \frac{\pi}{12} = 1$ so , $\cos^2 \frac{\pi}{12} = 1- \sin^2 \frac{\pi}{12}$ To find $\cos \frac{\pi}{12} = \sqrt{1- \sin^2 \frac{\pi}{12}}$ $\sin^2 \frac{\pi}{12} = (\frac{\sqrt{3} -1}{2 \sqrt{2}})^2 = \frac{(\sqrt{3}-1)^2}{(2\sqrt{2})^2} = \frac{2- \sqrt{3}}{4}$ $\cos \frac{\pi}{12} = \sqrt{1-(\frac{\sqrt{3} -1}{2 \sqrt{2}})^2} $ $\cos \frac{\pi}{12} = \sqrt{1- \frac{2-\sqrt{3}}{4}}$ $\cos \frac{\pi}{12} = \frac{\sqrt{2+\sqrt{3}}}{2}$ What is wrong with my steps?",['trigonometry']
4539561,Projection of Direct Product of Nonabelian Simple Groups,"Given nonabelian simple groups $G_1,\ldots,G_n (n\ge 2)$ . If $H$ is a subgroup of $G=G_1\times \cdots \times G_n$ such that the projection of $H$ to each $G_i \times G_j$ is surjective. Prove that $H=G$ . I am trying the following approach: for any $a,b\in G_1$ , there exists $h_a = (a,1,*,\cdots)$ and $h_b = (b,*,1,\cdots)$ in $H$ , because of the surjectivity onto $G_1\times G_2$ and $G_1\times G_3$ . Then the commutator $[h_a,h_b]\in H$ has the form $([a,b],1,1,
\cdots)$ . Since $G_1$ is nonabelian simple, for any $g\in G_1$ , we can write $g=[a,b]$ for some $a,b\in G_1$ . Repeating this will show that $(g,1,\cdots,1)\in H$ for any $g\in G$ . This question is from a competition and it looks like this approach is a false proof because I (very likely) didn't get credits from this question. Please help me point out where it goes wrong. Edit: Thanks for the comments. I looked up my answers and found that I didn't mention one can write $g=[a,b]$ explicitly, but went straight with a conclusion that for any $g\in G_1$ there is an element in $H$ in form of $(g,1,1,*,\cdots)$ , using the fact that $G_1$ is nonabelian simple. So I guess the submitted version isn't quite wrong, but I didn't really understand everything, and I deserve a punishment for being too sloppy.","['group-theory', 'simple-groups']"
4539596,Generalizing $\sum_{n=1}^\infty \frac{\Gamma(n)\Gamma(x)}{\Gamma(n+x)} = \frac1{x-1}$ to double sum,"Using the definition of the beta function, it's easy to show that $$\begin{align*}
\sum_{n=1}^\infty \frac{\Gamma(n)\Gamma(x)}{\Gamma(n+x)} &= \sum_{n=1}^\infty \operatorname{B}(x,n) \\[1ex]
&= \sum_{n=1}^\infty \int_0^1 t^{x-1} (1-t)^{n-1} \, dt \\[1ex]
&= \int_0^1 t^{x-1} \sum_{n=1}^\infty (1-t)^{n-1} \, dt \\[1ex]
&= \int_0^1 t^{x-2} \, dt \\[1ex]
&= \frac1{x-1}
\end{align*}$$ I was wondering if we could generalize this to the case of a double sum, $$\sum_{m=1}^\infty \sum_{n=1}^\infty \frac{\Gamma(m) \Gamma(n) \Gamma(x)}{\Gamma(m+n+x)}$$ I've considered rewriting $$\frac{\Gamma(m) \Gamma(n) \Gamma(x)}{\Gamma(m+n+x)} = \begin{cases} \operatorname{B}(m,x) \operatorname{B}(m+x,n) \\ \operatorname{B}(n,x) \operatorname{B}(n+x,m) \\ \operatorname{B}(m,n) \operatorname{B}(m+n,x) \end{cases}$$ The third form seems the most useful, and applying the method above leads me to $$\begin{align*}
\sum_{m=1}^\infty \sum_{n=1}^\infty  \operatorname{B}(m,n) \operatorname{B}(m+n,x) &= \int_0^1 (1-t)^{x-1} \sum_{m=1}^\infty \sum_{n=1}^\infty \operatorname{B}(m,n) t^{m+n-1} \, dt
\end{align*}$$ $\operatorname{B}$ is symmetric, so $$\begin{align*}
\sum_{m=1}^\infty \sum_{n=1}^\infty \operatorname{B}(m,n) t^{m+n-1} &= \sum_{m=1}^\infty \operatorname{B}(m,m) t^{2m-1} + 2 \sum_{m=2}^\infty \sum_{1 \le n < m} \operatorname{B}(m,n) t^{m+n-1} \\[1ex]
\end{align*}$$ and we can rewrite the first sum as $$\begin{align*}
\sum_{m=1}^\infty \operatorname{B}(m,m) t^{2m-1} &= \sum_{m=1}^\infty \frac{\Gamma(m)^2}{\Gamma(2m)} t^{2m-1} \\[1ex]
&= \sum_{m=1}^\infty \frac{t^{2m-1}}{\binom{2m}m} \\[1ex]
&= \frac{4\arcsin\left(\frac t2\right)}{\sqrt{4-t^2}}
\end{align*}$$ The subsequent integral over $t\in[0,1]$ seems doable if $x$ is a positive integer; not so sure otherwise. I also don't know what else could be done with the other sum. Mathematica gives a result in terms of hypergeometric functions but is unable to simplify any further. $$\sum_{m=1}^\infty \sum_{n=1}^\infty \frac{\Gamma(m)\Gamma(n)\Gamma(x)}{\Gamma(m+n+x)} = \frac1{x^2} {}_3F_2\left(\left.\begin{array}{c|c}1,1,x\\1+x,1+x\end{array}\right\vert 1\right)$$","['gamma-function', 'beta-function', 'sequences-and-series']"
4539641,Is function application itself a function?,Is function application itself a function? So given a function $f$ and an element $x$ we can define the function that takes $f$ and it's element $x$ to the value of $f$ at $x$ ?,"['notation', 'multivariable-calculus', 'calculus', 'functions']"
4539643,Proof: $n < \aleph_0$,"I want to prove that any finite cardinality $n$ (basically $n$ is the number of elements of the finite set in question) is smaller than the cardinality of the set of natural numbers, in short: $n < \aleph_0$ . This is my proof: $n < \aleph_0$ , because $|\{0, 1, …, n\}| < |\mathbb N|$ , because $\exists f: \{0, 1, …, n\} \to \mathbb N$ injective, e.g. $f(x) = x$ , but $\lnot \exists f: \{0, 1, …, n\}\to \mathbb N$ surjective, because $n+1 \in \mathbb N$ and $n+1 \notin \{0, 1, …, n\}$ , so that there is always an additional element in $\mathbb N$ that cannot have a preimage in $\{0, 1, …, n\}$ . Would that be enough to be a proof accepted by mathematicians? Or should one add: … because assume a bijective $f$ then we have $n$ -pairs of $(x,y)$ but also $n+1$ -pairs of $(x,y)$ , contradiction.","['elementary-set-theory', 'cardinals']"
4539659,Prove a group of order 351 is not simple without Sylow's Theorems,"I am trying to prove a group of order $351$ is not simple. I know this has many answers that involve Sylow's Theorems, but I was trying to do this using an alternative method. My idea is the following:
Let $G$ be a group such that $|G| = 351$ .
If I can prove the existence of a subgroup $H$ of order $117$ , then, by Lagrange's Theorem $[G: N] = 3$ . Since $3$ is the smallest prime diving the order of $G$ , then this would show that $H$ is normal in $G$ . This of course shows that $G$ is not simple. The problem is that I do not know if a subgroup of order $117$ necessarily exists. If it does exist, I do not know how to show it. Are Sylow's Theorem's the only way to solve this problem or will my method work?","['group-theory', 'abstract-algebra', 'finite-groups']"
4539680,"How do you ""guess"" a candidate for an UMVUE?","The following distribution is given $X \sim \operatorname{BIN}(1, p)$ , then $\mathbb{E}(X)=p$ and $\mathbb{V} \operatorname{ar}(X)=p(1-p)$ . The CRLB is computed with: $$
\begin{aligned}
f(x ; p) &=p^x(1-p)^{1-x} \\
\ln f(x ; p) &=x \ln p+(1-x) \ln (1-p) \\
\frac{\partial}{\partial p} \ln f(x ; p) &=\frac{x}{p}-\frac{1-x}{1-p}=\frac{x-p}{p(1-p)} \\
\mathbb{E}\left(\frac{\partial}{\partial p} \ln f(X ; p)\right)^2 &=\mathbb{E}\left(\frac{X-p}{p(1-p)}\right)^2=\frac{\mathbb{E}(X-p)^2}{p^2(1-p)^2}=\frac{\operatorname{Var}(X)}{p^2(1-p)^2}=\frac{1}{p(1-p)}
\end{aligned}
$$ The CRLB is now obtained as $$
\frac{\left[\tau^{\prime}(p)\right]^2}{n \mathbb{E}\left(\frac{\partial}{\partial p} \ln f(X ; p)\right)^2}=\frac{1}{\frac{n}{p(1-p)}}=\frac{p(1-p)}{n}
$$ The subsequent task is to find the UMVUE. Algebraically I know how to check wheter an estimator is an UMVUE, but I find it hard to do this the ""other way around"", so to come up with an UMVUE yourself. The following is a snippet from the solution manual. Looking at your answer for part (a) you should recognize that the CRLB
coincides with $\operatorname{Var}(X) / n$ . As an educated guess we
therefore try $\hat{p}=\bar{X}$ . First, from $\mathbb{E}(X)=p$ The ""educated guess"" part makes me uncomfortable, because there are so many possebilities to choose from. How do you know that you pick the right one? Feedback on this would be very much appreciated. Questions: Why is $p$ an educated guess? How to make an educated guess yourself? Do you have general advice to come up with an UMVUE yourself?",['statistics']
4539681,"$f(x)=\begin{cases}x^{n+1}, & x \text{ irrational}\\ 0, & x \text{ rational}\end{cases}$ is differentiable at $0$ and discontinuous at $x\neq 0$.","I have proved that For every $n\in\mathbb{N}$ , the function $f(x)=\begin{cases}x^{n+1}, & x \text{ irrational}\\ 
                   0, & x \text{ rational}\end{cases}$ is differentiable at $0$ and discontinuous at every $x\neq 0$ . but I am not sure about the correctness of my proof, especially the last part, where I show that it is discontinuous at every non-zero rational number, so I would appreciate if someone would check it out. I welcome comments. Thanks Proof. Let $n\in\mathbb{N}$ and $h\neq 0$ . $|\frac{f(0+h)-f(0)}{h}|\leq|\frac{h^{n+1}}{h}|=|h|^{n}\xrightarrow{h\to 0}0$ so $\lim\limits_{h\to 0}\frac{f(0+h)-f(0)}{h}=0$ thus $f$ is differentiable at $0$ and $f'(0)=0.$ Suppose $a$ is irrational and take $\varepsilon:=\frac{|a|^{n+1}}{2}$ . Then, since the rationals are dense in the reals, we have that however we choose $\delta>0$ there exists $x_q\in (a-\delta,a+\delta)\setminus\{a\}\cap\mathbb{Q}$ such that $|f(x_q)-f(a)|=|0-a^{n+1}|=|a|^{n+1}>\varepsilon$ so $\lim\limits_{x\to a}f(x)\neq f(a)$ i.e. $f$ is not continuous at $a$ . Suppose now that $a$ is a non-zero rational number. Pick $|a|>d>0$ and set $\ell:=\inf\{|f(x)|:x\in (a-d,a+d)\setminus\{a\}\cap\mathbb{R}\setminus\mathbb{Q}\}$ . Then if we take $\varepsilon:=\frac{\ell}{2}$ we have that for every $\delta>0$ there exists some $x_p\in (a-d,a+d)\setminus\{a\}\cap (a-\delta,a+\delta)\setminus\{a\}$ such that $|f(x_p)-f(a)|=|f(x_p)|>\varepsilon$ , so $\lim\limits_{x\to a}f(x)\neq f(a)$ i.e. $f$ is not continuous at $a$ . $\square$","['real-analysis', 'continuity', 'calculus', 'solution-verification', 'derivatives']"
4539706,Scheme satisfying certain condition is affine. Exercise 3.5 Qing Liu,"Let $Y$ be a scheme such that the natural morphism $$ Hom_{Schemes}(X,Y) \rightarrow Hom_{Rings}(\mathcal{O}_Y(Y),\mathcal{O}_X(X))$$ is bijective for all affine schemes $X$ . Prove $Y$ is affine. I have no idea how to prove this. Since $Y$ is scheme we can cover it by open affine subsets $Y=\bigcup_iU_i$ , but I'm not sure how I can utilise the $U_i$ . I know that $$ Hom_{Schemes}(U_i,Y) \rightarrow Hom_{Rings}(\mathcal{O}_Y(Y),\mathcal{O}_Y(U_i))$$ is bijective, but that doesn't seem to help.","['affine-schemes', 'algebraic-geometry', 'schemes']"
4539739,"Another interesting property of $y=2^{n-1}\prod_{k=0}^n \left(x-\cos{\frac{k\pi}{n}}\right)$: product of arc lengths converges, but to what?","Here is the curve $y=2^{n-1}\prod\limits_{k=0}^n \left(x-\cos{\frac{k\pi}{n}}\right)$ , shown with example $n=8$ , together with the unit circle centred at the origin. Call the arc lengths between neighboring roots $l_1, l_2, l_3, ..., l_n$ . What is the exact value of $L=\lim\limits_{n\to\infty}\prod\limits_{k=1}^n l_k$ ? Desmos suggests that $L$ exists and is approximately $2.94$ . Maybe $\frac{8}{e}$ ? Context I have studied this curve, and found that it has several interesting properties. The curve is tangent to the unit circle at $n$ points, which are uniformly spaced around the circle. The magnitude of the gradient at each root inside the circle is $n$ ; the magnitude of the gradient at $x=\pm1$ is $2n$ . The total area of the regions enclosed by the curve and the x -axis is $1$ . As $n\to\infty$ , the volume of revolution of those regions about the x -axis approaches $\frac{1}{2}$ of the volume of the unit sphere, and the volume of revolution of those regions about the y -axis approaches $\frac{1}{\pi}$ of the volume of the unit sphere. As $n\to\infty$ , if the curve is magnified so that the average area of those regions is always $2$ , then the product of those areas approaches $4\cosh^2{\left(\frac{\sqrt{\pi^2-8}}{2}\right)}\approx6.18$ , as shown here . I recently discovered that the product of arc lengths between neighboring roots seems to converge to a positive number as $n\to\infty$ . Hence, my question. (If you know any other interesting properties of this curve, feel free to add them in the comments.) My attempt The part of the curve inside the circle can be expressed as $y=-\sqrt{1-x^2}\sin{(n\arccos{x})}$ . So $$L=\lim\limits_{n\to\infty}\prod\limits_{k=1}^n \int_{\cos{\frac{k\pi}{n}}}^{\cos{\frac{(k-1)\pi}{n}}}\sqrt{1+\left(n\cos{(n\arccos{x})+\frac{x\sin{(n\arccos{x})}}{\sqrt{1-x^2}}}\right)^2}dx$$ I do not know how to evaluate this limit. I tried taking the log of the product, without success. I tried to approximate each integral as areas of triangles (hoping that that approximation would become equality with the limit) and a rectangle at the bottom, multiplying each triangle's area by $\frac{4}{\pi}$ (which is the ratio of areas under sine or cosine to the area of an inscribed triangle), but that resulted in a different limit. EDIT Further numerical analysis strongly suggests that $L=\frac{8}{e}$ . I noticed that when $n$ doubles, the ratio of the two products is a certain number (which is close to $1$ ), and when $n$ is doubled again, the ratio's distance to $1$ is approximately halved. So then I projected that the product indeed approaches $\frac{8}{e}$ . (I don't have Mathematica; anyone who has it is welcome to confirm this.) I have simplified the expression of $L$ . Letting $x=\cos{\frac{u}{n}}$ , and ignoring the $1$ in the $\sqrt{1+(...)^2}$ (I think this is OK since $n\to\infty$ ), we get $$L=\lim\limits_{n\to\infty}\prod\limits_{k=1}^n \int_{k\pi}^{(k-1)\pi}\sqrt{\left(n\cos{u}+(\sin{u})\cot{\frac{u}{n}}\right)^2}\left(-\frac{1}{n}\sin{\frac{u}{n}}\right)du$$ $$\space{}=\lim\limits_{n\to\infty}\prod\limits_{k=1}^n \int_{(k-1)\pi}^{k\pi}\left|(\cos{u})\sin{\frac{u}{n}}+\frac{1}{n}(\sin{u})\cos{\frac{u}{n}}\right|du$$ So why is this equal to $\frac{8}{e}$ ?","['arc-length', 'circles', 'polynomials', 'infinite-product', 'limits']"
4539778,Diagonalization of a symmetric bilinear form over the integers,"Suppose $H$ is an $n\times n$ symmetric matrix with integer entries. We can view $H$ as a symmetric bilinear form $H:\Bbb R^n\times \Bbb R^n\to \Bbb R$ defined by $H(v,w)=v^tAw$ . It is then well known that $H$ is diagonalizable ( http://www.math.toronto.edu/~jkamnitz/courses/mat247_2014/bilinearforms2.pdf , Theorem 1.4, for example), i.e. there is an invertible matrix $P$ such that $P^t HP$ is diagonal. On the other hand, we can view $H$ as a symmetric bilinear form $\Bbb Z^n\times \Bbb Z^n\to \Bbb Z$ . Suppose $\det H=\pm 1$ and that $H$ is negative definite. Then can we find an invertible matrix $P\in GL(n,\Bbb Z)$ such that $P^t HP$ is diagonal?","['matrices', 'bilinear-form', 'linear-algebra', 'symmetric-matrices', 'positive-definite']"
4539828,Can someone please verify my solutions for this probability question on bayes' theorem?,"Assume a COVID test can identify the presence of COVID, given that the person has COVID, with probability $p_d$ . Assume the test assigns false positives with probability $p_f$ (a test will be positive but the person does not have COVID). Let $p_{\theta}$ be the prior probability a person has COVID. a) Calculate the probability that a test subject has COVID, given a test was positive. Do the same given the test was negative. Calculate the probability the test subject doesn’t have COVID given the test was positive, and then do the same for a negative test. *** $P(COVID|+)=\frac{P(+|COVID)P(COVID)}{P(+)}$ Using law of total probability for $P(+)$ : $P(+)=P(+|COVID)P(COVID)+P(+|NOCOVID)P(NOCOVID)$ $=p_dp_{\theta}+p_f(1-p_{\theta})$ $\implies P(COVID|+)=\frac{p_{d}p_{\theta}}{p_dp_{\theta}+p_f(1-p_{\theta})}$ *** $P(COVID|-)= \frac{P(-|COVID)P(COVID)}{P(-)}$ $P(-)=P(-|COVID)P(COVID)+P(-|NOCOVID)P(NOCOVID)$ $= (1-p_d)p_{\theta}+(1-p_f)(1-p_{\theta})$ $\implies P(COVID|-)=\frac{(1-p_d)p_{\theta}}{(1-p_d)p_{\theta}+(1-p_f)(1-p_{\theta})}$ *** $P(NOCOVID|+)= 1-P(COVID|+) = 1 - \frac{p_{d}p_{\theta}}{p_dp_{\theta}+p_f(1-p_{\theta})}$ *** $P(NOCOVID|-) = 1 - P(COVID|-) = 1 - \frac{(1-p_d)p_{\theta}}{(1-p_d)p_{\theta}+(1-p_f)(1-p_{\theta})}$ b) Assume $p_d=0.8$ and $p_f=0.5$ , and assume that the prior probability any person has COVID is 0.1. Given a person has COVID, how many positive tests in a row do they need to take to be 99% confident they have it? 99.9%? $1-P(COVID|+)^{n} \geq 0.99$ $\implies 1-\Big[ \frac{(0.8)(0.1)}{(0.8)(0.1)+(0.5)(1-0.1)} \Big]^{n}\geq 0.99$ $\implies 1-(0.15)^n\geq 0.99$ $n\geq 2.4$ Need at least three tests. For 99.9%, perform the same calculation but with 0.999 instead of 0.99. We get $1-(0.15)^n \geq 0.999$ $\implies n \geq 3.6$ Need at least 4 tests","['bayes-theorem', 'probability']"
4539859,ZFC and the Axiom of Choice.,"I've been given the following problems in set theory, specifically, ZFC: Every infinite set has a countable infinite subset. Conclude that every Dedekind finite is also finite. Every infinite set is the union of two infinite disjoint sets. Every infinite set is the countable union of infinite sets. If there exists a surjection from the set $A$ to the set $B$ . Then, there exists an injection from $B$ to $A$ . If $A$ is infinite, $B \subseteq A$ and $|B| < |A|$ . Then, $\left|A \setminus B\right| = |A|$ . My thoughts and questions; This proof was taken from https://proofwiki.org/wiki/Infinite_Set_has_Countably_Infinite_Subset#:~:text=Now%2C%20suppose%20that%20that%20there,countably%20infinite%20subset%20of%20S , the third proof, and left me with some questions. Let $S$ be a infinite set. By the axiom of choice, there exists a choice function $f: \mathcal{P}(S)\setminus\{\emptyset\}\to S$ such that $$\forall s \in \mathcal{P}(S)\setminus\{\emptyset\}:f(s)\in s$$ Let $C = \left\{X \subset S\;\vert\; X \mbox{ is finite } \right\}$ , which is a set by the theorem of separation. If $A\in C$ , it follows by assumption that $S\setminus A \neq \emptyset$ , then, $S\setminus A \in \mbox{Dom}(f)$ . Define $g:C \to C$ by $$g(A) = A\cup f(S \setminus A)$$ Define $h:\omega \to C$ (here in the proof, it is said that $h$ is a recursion on $g$ , is this necessary? Can't we just define $C$ and then $h$ ?) by $$h(x)= \left\{ \begin{array}{rcl}
\emptyset & \mbox{if}
& x=0 \\
 h(x)\cup f\left(S \setminus h(n)\right) & \mbox{if} & x = n^{+}
\end{array}\right.$$ Consider now the function $j: \omega \to S$ given by $$\forall n \in \omega: j(n)=f\left(S \setminus h(n) \right)$$ which has the properties (1) $\forall n \in \omega: j(n)\notin h(n)$ (2) $\forall n \in \omega : j(n) \in h(n^{+})$ (3) $(\forall n,m \in \omega)(n\leq m) \Rightarrow h(n) \subseteq h(m)$ (4) $(\forall n,m \in \omega)(n <m) \Rightarrow j(n) \neq j(m)$ . Since $j(n) \in h(m)$ but $j(m) \notin h(m)$ . From (4), it follows that $j$ is an injection, if $\mbox{Img}(j) = B \subset S$ , then, $j: \omega \to B$ is a bijection. Thus, the set $\mbox{Img}(j) \subset S$ is countably infinite. In ZFC, every infinite set is also Dedekind infinite. Thus, if a set $X$ is Dedekind finite, $X$ is itself finite. Let $S$ be infinite. Take $a_0 \in S$ and let $A_0 = \{a_0\}$ and take $b_0 \in S\setminus A_0$ , then $B_0 = \{b_0\}$ . Take $a_1 \in S\setminus A_0 \cup B_0$ , then $A_1 = A_0\cup\{a_1\}$ , take $b_1 \in S \setminus A_1 \cup B_0$ , then $B_1 = B_0 \cup\{b_0\}$ . With this, we can construct \begin{align*}
A &= \bigcup_i A_i  \\
B &= \bigcup_j B_j
\end{align*} This process can't be carried indefinitely, and thus we need the axiom of choice. However, I can't think of how to precisely build this, any help would be appreciated. I'm having quite some trouble in solving problems which require the use of the axiom of choice, any tip on how to develop a better problem-solving technique for those specific cases? I'm not sure, but I believe this would follow similarly from the previous problem. If not, where does the idea breaks down and how to solve it? Taken from If there is a surjection $A\to B $ and another $B\to A$ then $A $ and $B$ are in bijection . Let $A$ and $B$ be sets and let $f: A \to B$ be a surjection. For each $b \in B$ , we can (by the axiom of choice), take an element $a\in f^{-1}(b)$ . Thus, we have an injection $g: B \to A$ by defining $$g(a) = b$$ for every $b$ . How can this be written more rigorously using a choice function? Not sure it this counts as a duplicate of Let $A, B$ be some sets such that $A\setminus B$ is infinite while $B$ countable or finite. Prove that $A\setminus B\sim A$ . Let $A$ be infinite, $B \subseteq A$ and $|B|<|A|$ . If $B$ is finite, it is trivial that $|A \setminus B| = |A|$ . If $B$ is infinite, I can see how to relate the axiom of choice to this problem. Again, any help is extremely appreciated. I'm really struggling to construct proofs using choice functions and the axiom of choice.","['proof-writing', 'solution-verification', 'elementary-set-theory', 'axiom-of-choice', 'set-theory']"
4539860,Residue of $e^{\frac {1} {\sin z}}$ at $z = 0$,"I am a beginner in complex analysis.
I have come across a problem that, in essence, asks me to find the residue of $e^{\frac{1}{\sin z}}$ at the isolated essential singularity $z=0$ . Until now, I have only seen problems where the sine is multiplied or divided to an analytic function, in which case it sufficed to expand the Taylor series of $\sin z$ , or even consider only the linear term of it. However, in this case, $\sin z$ is not only inverted but also exponentiated, which made it too difficult for me to directly write the Laurent series of the function.
I briefly tried writing $$ \csc z = \sum_{n=-1}^{\infty} a_n z^n$$ because it has a simple pole at $z = 0$ , but exponentiating it lead to a representation of the residue as an infinite sum including $a_n$ , which I had no idea how to calculate using the inductive formlua for $a_n$ . I would like to ask what kind of techniques can be used in such problems, when complicated formulae are composed and inverted and such, making a direct derivation of Laurent series inapplicable. Thank you in advance.","['complex-analysis', 'residue-calculus']"
4539873,"Can we express the value of $b^a$ in terms of $c$ , where $c=a^b$?","We know that ; If $a+b = c$ , then $b+a = c$ If $a-b = c$ , then $b-a=-c$ If $ab = c$ then $ba = c$ If $\dfrac{a}{b} = c$ then $\dfrac{b}{a} = \dfrac{1}{c}$ Now, I am curious to know that If $a^{b} = c$ , then what is $b^a$ in terms of $c$ ? I tried using logarithms but failed to get the desired answer in terms of $c$ . EDIT : It has been made clear from the answers that $b^a$ is not unique because $a^b$ can also be expressed as $x^y$ where $x,y$ can take infinite values. However, what happens if $a$ and $b$ are not changed? I want to emphasize more on the numerical value of $b^a$ rather than preserving numerical value of $a^b$ and then changing it to for some other forms of $x,y$ such that $x^y=c$ . For example, $2^6 = 64$ and $6^2=36$ which is unique. It is clear that $64 = 4^3$ ; but we do not wish to change $a$ and $b$ by other possible values which also happen to be some solutions of the $c$ . Or in more mathematical terms, I am intrested in ; If for some $a,b$ we have : $a^b = c$ then find $y^x$ given that : 1) $x^y = c$ as well as 2) $x=a$ , $y=b$ If it is impossible to find, then please also provide a proof . Thanks!","['exponentiation', 'algebra-precalculus', 'exponential-function', 'logarithms']"
4539933,Proving $\sum_{n=1}^{\infty}\frac{1}{1+n^2\pi^2} = \frac{1}{e^2-1}$,"I hope I'm allowed to ask this question here, but I have to prove that $\sum_{n=1}^{\infty}\frac{1}{1+n^2\pi^2} = \frac{1}{e^2-1}$ using the following Fourier series: $$
		1-\frac{1}{e} + \sum_{n=1}^{\infty}\frac{2}{1+n^2\pi^2}\left[1-\frac{1}{e}(-1)^n\right]\cos(n{\pi}x) =
		\begin{cases} 
      		e^x & x\in[-1,0) \\
      		e^{-x} & x\in[0,1]
   		\end{cases}
$$ This is my progress so far: Let $x=0$ : \begin{align*}
		\therefore 1-\frac{1}{e} + \sum_{n=1}^{\infty}\frac{2}{1+n^2\pi^2}\left[1-\frac{1}{e}(-1)^n\right] &= 1 \\
		\sum_{n=1}^{\infty}\frac{2}{1+n^2\pi^2}\left[1-\frac{1}{e}(-1)^n\right] &= \frac{1}{e} \\
		\sum_{n=0}^{\infty}\frac{2}{1+(2n+1)^2\pi^2}\left(1+\frac{1}{e}\right) + \sum_{n=1}^{\infty}\frac{2}{1+(2n)^2\pi^2}\left(1-\frac{1}{e}\right) &= \frac{1}{e}
\end{align*} Does anyone know what I should do next?","['fourier-series', 'fourier-analysis', 'sequences-and-series']"
4539953,Infer a set from its differences to other sets,"There is a set $S$ . We don't know the exact elements of $S$ , but only know its cardinality $|S|$ . Now we have some guesses to $S$ , i.e., $\{ S_i \}_{i=1}^n$ . For each $S_i$ , we know its exact elements, and the cardinalities $|S_i \backslash S|$ and $|S \backslash S_i|$ . The goal is to infer all the possible set $S$ that satisfies the given guesses. Note that the universe from which $S$ and all guesses are chosen is given, but may be arbitrarily large. So, is this problem well-defined/studied in set theory domain? And is there any elegant algorithm to do so, instead of brute-force search? Here we give an example: Already know that $|S|=4$ . Guess 1, $S_1 = \{1,2,3,4,5\}$ , $|S_1\backslash S|=1$ , $|S\backslash S_1|=0$ . Then $S$ has 5 possibilities (all subsets of $S_1$ with 4 elements). Guess 2, $S_2 = \{2,3,4,5\}$ , $|S_2\backslash S|=1$ , $|S\backslash S_2|=1$ . Then $S$ is reduced to only 4 possibilities ( $\{2,3,4,5\}$ is impossible). Guess 3, $S_3 = \{0,2,3,5\}$ , $|S_3\backslash S|=1$ , $|S\backslash S_3|=1$ . Then $S$ is must be $\{1,2,3,5\}$ .",['discrete-mathematics']
4539966,Can a graph of a continuous function intersect its vertical asymptote?,"I know a graph can intersect its horizontal asymptote ( 1 , 2 , 3 ). I think it's possible for a graph of a function to intersect its vertical asymptote. Example: Define $f:\mathbb{R}\rightarrow \mathbb{R}$ by $f(x)=1/x$ for $x\neq 0$ and $f(x)=0$ for $x=0$ . Then $x=0$ is a vertical asymptote for and intersects $f$ . (Correct me if this example is mistaken.) I think though that it's not possible for a graph of a continuous function to intersect its vertical asymptote. Is this true? How do I prove this?","['graphing-functions', 'real-analysis', 'continuity', 'functions', 'limits']"
