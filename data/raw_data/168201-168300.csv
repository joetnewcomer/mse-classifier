question_id,title,body,tags
2944522,Then value of $\alpha^2 +4\alpha$ in Infinite series,"If $\displaystyle \alpha = \frac{5}{2!\cdot 3}+\frac{5\cdot 7}{3!\cdot 3^2}+\frac{5\cdot 7 \cdot 9}{4!\cdot 3^3}+\cdots \cdots \infty.$ Then value of $\alpha^2 +4\alpha$ is Try: Let $$S = \frac{5}{2!\cdot 3}+\frac{5\cdot 7}{3!\cdot 3^2}+\frac{5\cdot 7 \cdot 9}{4!\cdot 3^3}+\cdots \cdots $$ $$S+1 = 1+\frac{5}{2!\cdot 3}+\frac{5\cdot 7}{3!\cdot 3^2}+\frac{5\cdot 7 \cdot 9}{4!\cdot 3^3}+\cdots \cdots $$ Now camparing with $$(1+x)^n = 1+nx+\frac{n(n-1)x^2}{2!}+\frac{n(n-1)(n-2)x^3}{6\cdot 3!}+\cdots \cdots$$ So $\displaystyle nx=\frac{5}{6}$ and $\displaystyle \frac{n(n-1)x^2}{2}=\frac{35}{27}$ So $$\frac{nx(nx-x)}{2}=\frac{5}{12}\cdot \frac{5-6x}{6}=\frac{35}{27}$$ So $\displaystyle x=-\frac{41}{18}$ and $\displaystyle n=-\frac{15}{41}$ I am getting $\displaystyle S+1=\bigg(1-\frac{41}{18}\bigg)^{-\frac{15}{41}}$ but answer of $\alpha^2+4\alpha = 23$ which is not possible from my answer. could some help me how can i solve it, thanks",['sequences-and-series']
2944603,Higher total derivatives (Frechét derivatives),"This issue I just cannnot resolve, so I'd highly appreciate your help. 
Let $a_1, ... , a_n \in \mathbb{R}^k$ with $k$ a natural positive number. If we consider the function $$ W: \mathbb{R}^k \to \mathbb{R}, x \to \sum_{i=1}^n {(x-a_i)}\cdot {(x-a_i)}$$ ( $\cdot$ being the standard inner product) then we can write $ W = \sum_{i=1}^n N \circ T_{a_i}$ , where $$N(x) := x \cdot x, T_{a_i}(x) := x - a_i$$ giving $$D_x N(h) = 2 (x \cdot h), D_x T_{a_i}(h) = h$$ With the linearity of the total differential and the chain rule ( $D_p (f\circ g) = D_{g(p)}f \circ D_p g$ ) we obtain $$D_xW(h) = \sum_{i=1}^n (D_x(N\circ T_{a_i}))(h) = \sum_{i=1}^n (D_{T_{a_i}(x)} N \circ D_x T_{a_i}) (h) = \sum_{i=1}^n D_{T_{a_i}(x)} N(h) =  \sum_{i=1}^n 2 (x-a_i) \cdot h$$ Now you can supposedly take the second derivative: $$D(D_xW(h))(g) = \sum_{i=1}^n 2D_x [(T_{a_i} (g)) \cdot h] = 2 \sum_{i=1}^n g \cdot h$$ I don't understand 
1.  how the first step (equality) above follows and 
2.  why this simply doesn't equal zero, since for a function $f:\mathbb{R}^k \to \mathbb{R}$ evaluating the total derivative $D_pf$ just gives $D_pf(x) \in \mathbb{R}$ for $p, x \in \mathbb{R}^k$ , so that $D(D_xW(h))(g)= (D_xW(h))'(g) = 0$ . Or if there is a better alternative to calculating the second derivative, please share! (For context this came up showing that $W$ has a minimum in $x_0 := \frac 1 n \sum_{i=1}^n a_i$ - supposedly $W$ arises naturally in the context of the method of least-squares.) I hope everything is clear and again, I appreciate your efforts. Edit: I hope this warrants the ""functional analysis""/""Frechét derivative"" tag, as (if I understand correctly from the wikipedia article) they are concerned with infinite dimensional vector spaces, but in the finite dimensional case like the one at hand it just seems to be the derivative that I've learned.","['frechet-derivative', 'multivariable-calculus', 'multilinear-algebra', 'functional-analysis', 'chain-rule']"
2944608,If $V= x^nlogx$ then prove that $V_n = nV_{n-1} + (n-1)!$ where $V_n $ is the nth derivative of $V$,"If $V= x^nlogx$ then prove that $V_n = nV_{n-1} + (n-1)!$ where $V_n $ is the nth derivative of $V$ My attempt : $V=x^nlogx$ $\therefore \, V_1 = nx^{n-1}logx + x^{n-1}$ $\therefore \, V_1= \dfrac{nV}{x}+x^{n-1}$ $\therefore \, xV_1=nV+x^n$ Differentiating $(n-1)$ times, $\therefore \, xV_n+(0)V_1=nV_{n-1}\,+ (n-1)!$ $\therefore \, xV_n=nV_{n-1}\,+ (n-1)!$ I don't know how an extra $x$ appears in the LHS in my answer.","['calculus', 'derivatives']"
2944641,norm of positive semi-definite complex matrix,"Suppose $0\neq X_n\in \mathbb{M}_{k(n)}(\mathbb{C})$ ,if $\lim_{n \to \infty}tr(X_n^*X_n)=0$ ,can we conclude that $\lim_{n \to \infty}\|X_n^*X_n\|=0$ ,where $tr$ is the standard trace on complex matrix, $\|·\|$ is the norm of matrix. My thought: the eigenvalues of $X_n^*X_n$ is non-negative, $ tr(X_n^*X_n) $ is equal to the sum of non-negative eigenvalues,so each non-negative eigenvalue is small enough,the norm of $X_n^*X_n$ is equal to the product of eigenvalues,so it tends to zero.Is my idea correct?","['matrices', 'adjoint-operators', 'linear-algebra', 'functional-analysis']"
2944724,"Is there a unnecessary part in this definition of ""not connected set"" in complex plane?","In the book ""Introduction to complex analysis"" - Junjiro Noguchi, there is a definitoin of not connected set in complex plane as follows Here, $A$ is a subset of complex plane. I think the part $\cap A$ in $U_1\cap U_2\cap A=\emptyset$ is not necessary. Am I wrong?","['complex-analysis', 'general-topology', 'connectedness']"
2944728,"Proving that $q(A,S)>0$ if $A>0$, $S<0$ and $p_i(A,S)>0$","Consider the polynomials $$p_1=3(56-56A+A^2)-112(-6+A)S-14(-36+A)S^2+112S^3+7S^4,$$ $$p_2=-112(-6+A)-28(-36+A)S+336S^2+28S^3,$$ $$p_3=-28(-36+A)+672S+84S^2,$$ $$p_4=672+168S,$$ $$p_5=168,$$ $$q=-168S-252S^2-84S^3-7S^4+84A+84AS-3A^2+14AS^2.$$ I want to prove that if $A>0$ , $S<0$ and $p_i>0$ then $q>0$ . I have observed that $\frac{\partial p_i}{\partial S}(A,S)=p_{i+1}(A,S)$ and $q=-p_1+p_2-p_3+p_4-p_5$ , so we can express $q$ as $$q=\sum_{k=0}^4(-1)^{k+1}\frac{\partial^k p_1}{\partial S^k},$$ but I don't know if it can be used to prove the above statement.","['inequality', 'derivatives', 'polynomials']"
2944752,Extension of Borel-Cantelli in Probability Theory,"I am working on a problem regarding an extension of the Borel-Cantelli lemma that goes as follows: Let $E_1, E_2, ...$ be an arbitrary sequence of sets. It is known that $\lim_{n \to \infty}P(E_n) = 0$ and $\sum_n P(E_n \cap E_{n+1}^c) < \infty$ . Show that $P(E_n \text{ infinitely often})=0$ I have tried a few different attempts at the answer, with the most current one given below: \begin{align*}
      &P(\limsup_n E_n)&&\\
      &= P(\cap_{n=1}^\infty \cup_{k=n}^\infty E_n)&&\\
      &= \lim_{n \nearrow \infty} P(\cup_{k=n}^\infty E_n)&&\text{intersection is monotone decreasing}\\
      &= \lim_{n \nearrow \infty} P\bigg(\cup_{k=n}^\infty \big[[E_n \cap E_{n+1}^c] \cup [E_n \cap E_{n+1}]\big]\bigg)&&\\
      &= \lim_{n \nearrow \infty} P\bigg(\big(\cup_{k=n}^\infty [E_n \cap E_{n+1}^c]\big) \cup \big(\cup_{k=n}^\infty [E_n \cap E_{n+1}]\big)\bigg)&&\\
      &= \lim_{n \nearrow \infty} P\bigg(\big(\cup_{k=n}^\infty [E_n \cap E_{n+1}^c]\big) \cup \big(\cup_{k=n}^\infty E_n \cap \cup_{k=n}^\infty E_{n+1}]\big)\bigg)&&\\
      &= \lim_{n \nearrow \infty} P\bigg(\big(\cup_{k=n}^\infty [E_n \cap E_{n+1}^c]\big) \cup [\cup_{k=n}^\infty E_{n+1}]\bigg)&&\\
      &= \lim_{n \nearrow \infty} \bigg[P\big(\cup_{k=n}^\infty [E_n \cap E_{n+1}^c]\big)+ P\big(\cup_{k=n}^\infty E_{n+1}\big)\bigg]&&\text{disjoint additivity}\\
      &= \lim_{n \nearrow \infty} P\big(\cup_{k=n}^\infty [E_n \cap E_{n+1}^c]\big)+ \lim_{n \nearrow \infty} P\big(\cup_{k=n}^\infty E_{n+1}\big)&&\\
      &\leq \lim_{n \nearrow \infty} \sum_{k=n}^\infty P\big(E_n \cap E_{n+1}^c\big)+ \lim_{n \nearrow \infty} P\big(\cup_{k=n}^\infty E_{n+1}\big)&&\text{subadditivity}\\
    \end{align*} The main problem with this answer, as well as the other ones I have drafted, is that I can't seem to get rid of the union over $E_n$ . Because of this I can't make any statements about the limiting probability of the union going to $0$ (e.g. if $P(E_n) = 1/n$ the conditions of the problem are fulfilled but the probability of the union does not converge). Am I missing something in the way I am breaking up the sets? I have been banging my head against this for days with no success - any resources you all could suggest/ direction that you could give would be highly appreciated.","['measure-theory', 'real-analysis', 'sequences-and-series', 'borel-cantelli-lemmas', 'probability-theory']"
2944754,Wasserstein Distance with Translations,"I am studying this book about Optimal Transport, and in Remark 2.19 it talks about translation in Variance, where it is stated that a nice property of Wasserstein Distances is the ability to factor out translations: $$\mathcal{W}_2 (T_{\tau\#}\alpha \; , \; T_{\tau'\#}\beta)^2= \mathcal{W}_2(\alpha,\beta)^2-
	2\langle \tau -\tau' \; , \; \mathbf{m}_\alpha - \mathbf{m}_\beta \rangle + \|\tau -\tau'\|^2$$ where $\mathbf{m}_\alpha \triangleq \int_\mathcal{X}xd\alpha(x) \in \mathbb{R}^d$ is the mean of a measure $\alpha$ . The proceeded to write that: $$	\mathcal{W}_2(\alpha,\beta)^2=\mathcal{W}_2(\bar{\alpha},\bar{\beta})^2+\|\mathbf{m}_\alpha - \mathbf{m}_\beta \|^2$$ Where $\mathcal{W}_2(\bar{\alpha},\bar{\beta})^2=\mathcal{W}_2 (T_{\tau\#}\alpha \; , \; T_{\tau'\#}\beta)^2$ . Now, I can't wrap my head around how the term $\|\mathbf{m}_\alpha - \mathbf{m}_\beta \|^2$ came to be in the equation. I've tried to trace backwards, but I got stuck. Here is my attempt: \begin{align}
		2 \langle \tau-\tau',\mathbf{m}_\alpha-\mathbf{m}_\beta\rangle  - \|\tau -\tau'\|^2 &= \|\mathbf{m}_\alpha-\mathbf{m}_\beta\|^2 \\
		2 \langle \tau-\tau',\mathbf{m}_\alpha-\mathbf{m}_\beta\rangle  &= \|\tau -\tau'\|^2 + \|\mathbf{m}_\alpha-\mathbf{m}_\beta\|^2 \\
		  \langle \tau-\tau',\mathbf{m}_\alpha-\mathbf{m}_\beta\rangle &= \frac{\|\tau -\tau'\|^2 + \|\mathbf{m}_\alpha-\mathbf{m}_\beta\|^2}{2}
\end{align} I haven't studied analysis or measure theory, but I am trying to understand as much as I could. I thought that this is an inner product and that $\tau-\tau'$ is the difference between translations, which could be viewed as some vector, and similarly for the difference between the means of the discrete measures which I assume that they yield the center of the discrete measures $\alpha$ and $\beta$ . I am pretty sure something is wrong in my understanding, or at least my basics. So, my questions are: where did I go wrong? is that an inner product? what is the definition of this $\langle \cdot , \cdot \rangle$ notation? and what is the definition of these norms $\|\cdot\|^2$ in the context of
the Wasserstein distances? Thanks EDIT: I am using $\triangleq$ as the equal with a ""def"" on top. EDIT: The purpose is self-study. I am not enrolled anywhere so it isn't a homework. EDIT: From Remark 2.19 in the book: $(\bar{\alpha},\bar{\beta})$ are the ""centered"" zero mean measures $\bar{\alpha}=T_{\mathbf{m}_\alpha \#} \alpha$","['measure-theory', 'optimal-transport', 'metric-spaces', 'analysis']"
2944761,"Let $f:[0,1] \to R$ be continuous such that $|f(x)| \le \int_0^xf(t)dt$ for all $x \in [0,1]$.","Let $f:[0,1] \to R$ be continuous such that $|f(x)| \le \int_0^xf(t)dt$ for all $x \in [0,1]$ . Then, a. Such $f$ does not exist b. $f(x)=0$ for all $x \in [0,1]$ c. $f(x)=c$ for all $x \in [0,1]$ and some constant $c$ d. None of the above is true. What I've tried doing : $$
f(x) \le |f(x)| \le \int_0^xf(t)dt
$$ and hence $$
f(x)-\int_0^xf(t)dt \le 0
$$ Let $g(x)=f(x)-\int_0^xf(t)dt$ $$
g'(x)=f'(x)-f(x)
$$ I have no idea what to do now. Any hints?","['integration', 'calculus', 'functional-analysis']"
2944795,Find the values of $p$ and $q$,"If $p^3+p=q^2+q$ where $p$ and $q$ are prime numbers, 
  Find all the solutions (p, q) I tried to solve this exercise using that: $p^2 = -1(\text{mod} \, q)$ and $q = -1(\text{mod} \, p)$ ; 
   So: $q+1=ap$ and $p^2+1=bq$ , where $b$ and $q$ integers. Then I tried to solve a quadratic equation, but I could not finish the problem","['elementary-number-theory', 'diophantine-equations', 'polynomials', 'algebra-precalculus', 'prime-numbers']"
2944797,"If $\int_0^x f^2(t)dt \le f(x)$ for all $x \in [0,1]$, then $\min_{[0,1]} f(x) \le 1$?","Suppose that $f$ is a continuous function on $[0,1]$ and $$\int_0^x [f(t)]^2dt \le f(x) \quad \text{for all} \quad x \in[0,1].$$ Prove or disprove $$\min_{0\le x\le 1} f(x) \le 1.$$ In case the desired inequality does not hold, what is the best upper bound? Thanks.",['calculus']
2944853,Finding the probability that an inner product is positive,"Let $x$ be a fixed vector in $\mathbb{R}^n$ .
Let $a=[a_1,\cdots,a_n]^T$ be a random vector whose entries are iid random variables, say, $a_i \sim P$ . I would like to compute $$
P(\langle a, x \rangle > 0) 
$$ Here is my attempt.
Without loss of generality, let $x_1 > 0$ . 
Then \begin{align*}
P(\langle a, x \rangle > 0) = \int_\mathbb{R} \dots \int_{\mathbb{R}} \int_{a_1 > -\frac{\sum_{j=2}^na_jx_j}{x_1}} p(a_1)\cdots p(a_n)da_1\cdots da_n.
\end{align*} If $P$ is an uniform distribution on $[-M,M]$ , the above becomes \begin{align*}
P(\langle a, x \rangle > 0) &= \frac{1}{(2M)^n}\int_{-M}^M \dots \int_{-M}^M \int_{a_1 > -\frac{\sum_{j=2}^na_jx_j}{x_1}} da_1\cdots da_n \\
&=\frac{1}{(2M)^n}\int_{-M}^M \dots \int_{-M}^M \left(M + \frac{\sum_{j=2}^na_jx_j}{x_1}\right) da_2\cdots da_n \\
&= \frac{1}{2}.
\end{align*} If $a_i \sim \mathcal{N}(0,\sigma^2)$ , since $\langle a, x \rangle = \sum_{i=1}^na_ix_i \sim \mathcal{N}(0,\|x\|^2\sigma^2)$ ,
one can easily conclude $P(\langle a,x \rangle > 0) = 0.5$ . Question In what class of distribution $P$ , can we derive $$P(\langle a,x \rangle > 0) = 0.5?$$ I thought the symmetric distribution around 0 could result in the same result, however, it is unclear to me. Any comments/answers will very be appreciated.","['inner-products', 'probability-distributions', 'probability']"
2944908,Is this proof of$ (A \cup B) \cap (A \cup C) \subseteq A \cup (B \cap C)$ right?,"My try: Suppose $x \in (A \cup B) \cap (A \cup C)$ . We know $x \in A \cup B$ and $x \in A \cup C$ . We know $x \in A$ or $x \in B$ and $x \in A$ or $x \in C$ .
Let's divide into cases: 1 - Suppose $x \in A$ .
In this case we know $ x \in A \cup (B \cap C)$ 2 - Suppose $x \in B$ and $x \in C$ . We know $x \in B \cap C$ , by the definition of interessection. Hence, $x \in A \cup (B \cap C)$ .
So we always have $x \in A \cup (B \cap C)$ . Hence, $(A \cup B) \cap (A \cup C) \subseteq A \cup (B \cap C)$ , by the definition of subset.","['elementary-set-theory', 'proof-writing', 'proof-verification']"
2944937,Proving pointwise convergence almost everywhere,"I would like to prove the following statement: Let $(X, F, \mu)$ be a measure space, not necessarily finite. Suppose that for every $\epsilon > 0$ there exists a natural number $N$ such that $ \mu(\bigcup_{n = N}^{\infty} \{x \in X : |(f_n(x) - f(x)| > \epsilon \}) < \epsilon $ . Then $f_n \to f$ pointwise almost everywhere. My idea was to prove the statement by contradiction, i.e. assuming the negation of pointwise convergence almost everywhere: Suppose there exists an $\epsilon'$ such that for all $N$ there exists an $n \geq N$ with $|f_n(x) - f(x)| > \epsilon$ for almost every $x \in X$ . By assumption we know that for any $\epsilon$ , in particular for $\epsilon'$ , we can find an $N_0$ such that for all $n \geq N_0$ we have $|f_n(x) - f(x)| > \epsilon'$ for almost every $x \in X$ . I thought this would imply that $\mu(\bigcup_{n = N_0}^{\infty} \{x \in X : |(f_n(x) - f(x)| > \epsilon' \}) = \mu(X)$ , but even if that were the case, I do not see a way to reach a contradiction as I cannot assume that $\mu(X) > \epsilon$ , I've probably made a mistake, but I cannot see where. Is there any way I could complete this proof, or is there a better way? Thank you in advance.","['measure-theory', 'convergence-divergence']"
2944970,High Order Derivative,"Find the ninth derivative of the following function at $x=0$ : $$f(x) =\frac{\cos\left(4x^4\right)-1}{x^7}$$ So I did all the manipulations and I got the following Maclaurin Series: $$\sum _{n=0}^{\infty }\,(-1)^n\frac{16x^{8n-7}}{(2n)!}-1$$ So to have $8n-7=9$ , I got that $n=2$ . Therefore, I thought that: $$(-1)^2\frac{16x^9}{24}-1=\frac{16x^9}{24}-1$$ And, therefore, the ninth derivative would be: $$\frac{16\cdot 9!}{24}-1=241919$$ But this was incorrect. Any help?","['calculus', 'derivatives', 'taylor-expansion']"
2945001,Conjugate Prior for Gamma Distribution,"This is very basic, but I have been stuck on this problem for a while. Suppose $Y_1, \dots, Y_n|\alpha,\beta\sim Gamma(\alpha, \beta)$ is iid with $\alpha$ known. I want conjugate prior for $\beta$ and the posterior. My work: $p(\beta|y_1, \dots, y_n)=p(y_1, \dots, y_n|\beta)p(\beta)=(\prod_{i=1}^{n} y_i)^{\alpha-1} \exp(-\beta\sum_{i} y_i) p(\beta)$ $\propto \exp(-\beta\sum_{i} y_i) p(\beta)$ . Therefore, the conjugate prior for $\beta$ would be gamma $(\alpha_0, \beta_0)$ . In this case, we can derive the posterior as: $p(\beta|y_1,\dots, y_n)\propto \beta^{\alpha_0 -1} \exp(-\beta(\sum_{i} y_i+\beta_0))$ . So, the posterior is gamma $(\alpha_0 , \sum_{i} y_i+\beta_0)$ . However, Wikipedia says the posterior should be gamma $(\alpha_0 +n\alpha, \sum_{i} y_i+\beta_0)$ . I don't understand where does $n\alpha$ come from. I also don't understand why my derivation is not correct. Thanks in advance!","['statistics', 'conditional-probability', 'probability-distributions', 'bayesian', 'gamma-distribution']"
2945055,Covariant derivative vs. Ehresmann connection,"I know about Ehresmann connections on fiber bundles and covariant derivatives as an (equivalent) way to define linear Ehresmann connections on vector bundles. My question is: Is there any notion of covariant derivative equivalent to Ehresmann connection in the most general setting concerning fiber bundles? When I say ""the most general setting"", I am emphasizing that the fiber bundle do not have any further structure than being just a fiber bundle (it may not be a vector bundle nor a principal bundle). Thanks in advance, Diego PS: I'm concerning the case when the fiber bundles are smooth. I don't worry about the non smooth case.","['connections', 'fiber-bundles', 'differential-geometry']"
2945057,A way of solving this PDE? (Other than method of characteristics),"I've been trying to work with this coupled PDE for some time. Here, $P = P(k,t)$ $$\frac{\partial{P}}{\partial t}-\frac{\sin(k)}{t}\frac{\partial{P}}{\partial k}=\left(\cos(k)-1\right){P}$$ To solve this PDE, I looked at the ODE $$\frac{dP}{dt}=\left(\cos(k)-1\right)P$$ We know that $$\frac{dP}{dt}=\frac{\partial P}{\partial t} + \frac{dk}{dt}\frac{\partial P}{\partial k}$$ So we can see that whenever $$\frac{dk}{dt}=\frac{-\sin(k)}{t}$$ the solution to the ODE gives us a solution to the PDE. But is there another way to get a more general solution than this? Thanks for the help :)","['ordinary-differential-equations', 'partial-differential-equations']"
2945103,Does group of symplectomorphisms preserve Hopf fibration,"Consider the closed ball of radius c := $B^4(c)$ , with the usual symplectic form coming from $\mathbb{R}^4$ . In the proof of Lemma 2.1 of the following paper( https://arxiv.org/pdf/math/0207096.pdf ) the authors go on to claim that that the group of symplectomorphisms of $B^4$ which acts naturally on ${B^4}$ preserving the characteristic foliation on $S^3 = \partial{B^4}$ . How does one go about proving this claim?","['symplectic-geometry', 'differential-geometry']"
2945214,"If $f(z)$ is continuous and nonzero at a point $z=z_0$, then there is a neighbourhood of $z=z_0$ in which $f(z)$ is nonzero.","Here is my proof. (Note that $z$ refers to complex number.) Since $f(z)$ is continuous at $z=z_0$ , by definition, for any $\epsilon > 0$ , there exists $\delta > 0$ such that $|f(z)-f(z_0)| <\epsilon$ whenever $|z-z_0|<\delta$ . (or equivalently, there exists a neighbourhood $N(z_0;\delta)$ .) Then for $\epsilon=|f(z_0)|>0$ ( $|f(z_0)|\neq 0$ as $f(z_0)\neq 0$ ), there exists $N(z_0;\delta)$ such that $$\big||f(z)|-|f(z_0)|\big|\leq |f(z)-f(z_0)|<\epsilon$$ $$\Rightarrow\big||f(z)|-|f(z_0)|\big|<\epsilon$$ $$\Rightarrow -\epsilon+|f(z_0)|<|f(z)|<\epsilon+|f(z_0)|$$ $$\Rightarrow 0<|f(z)|<2|f(z_0)|$$ $$\Rightarrow |f(z)|>0$$ $$\Rightarrow f(z)\neq 0$$ as required. Is my proof correct?","['complex-analysis', 'proof-verification']"
2945222,Understanding a non-autonomous ODE,"Consider $x'(t) = a(t)x$ a) Find a formula involving integrals for the solution of this system.
b) Prove that your formula gives the general solution of this system. I am new to ODE's and we have gone over integrating the system... $x'/x = a(t)$ in order to attempt to isolate x in terms of $t$ .  I thus get $log(x) = 0.5at^2+C \; \text{(a constant)}$ , which yields $x(t) = e^(.5at^2+C)$ , not sure if I'm right here.  I'm also not sure why it says ""involving integrals,"" making it sound like the solution needs integral signs in it? For part b, I have no idea how to even start it, not sure what I need to show. Any help appreciated!",['ordinary-differential-equations']
2945263,$\lim_{n\to \infty} \prod_{k=1}^n \left( \frac {2k}{2k-1}\right) \int_{-1}^{\infty} \frac {(\cos x)^{2n}}{2^x} dx$,Evaluate $$\lim_{n\to \infty} \prod_{k=1}^n \left( \frac {2k}{2k-1}\right) \int_{-1}^{\infty} \frac {(\cos x)^{2n}}{2^x} dx$$ My try: $$\lim_{n\to \infty} \prod_{k=1}^n \left( \frac {2k}{2k-1}\right) \int_{-1}^{\infty} \frac {(\cos x)^{2n}}{2^x} dx=\lim_{n\to \infty} \prod_{k=1}^n \left( \frac {2k}{2k-1}\right) \int_{-1}^{\infty} \frac {e^{i2nx}(1+e^{-i2x})^{2n}}{2^{2n}e^{x\ln 2}} dx$$ I write this using that $\cos x=\frac {e^{ix}+e^{-ix}}{2}$ and $2^x=e^{x\ln 2}$ We also know that $$\prod_{k=1}^n \frac {2k}{2k-1}=\frac {2^{2n}(n!)^2}{(2n-1)!}$$ Using this along with binomial theorem we get $$\lim_{n\to \infty} \prod_{k=1}^n \left( \frac {2k}{2k-1}\right) \int_{-1}^{\infty} \frac {e^{i2nx}(1+e^{-i2x})^{2n}}{2^{2n}e^{x\ln 2}} dx=\lim_{n\to\infty} \frac {(n!) ^2}{(2n-1)!}\left(\sum_{r=0}^{2n} \binom {2n}{r}\left(\int_{-1}^{\infty} e^{x(2i(n-r)-\ln 2)} dx\right)\right) $$ $$=\lim_{n\to\infty} \frac {(n!) ^2}{(2n-1)!}\left(\sum_{r=0}^{2n} \binom {2n}{r} \left[\frac {e^{x(2i(n-r)-\ln 2}}{ 2i(n-r)-\ln 2)} \right]_{-1}^{\infty}\right)$$ And now I am stuck here. Any suggestions or a different method are openly welcomed.,"['integration', 'definite-integrals', 'calculus', 'limits', 'complex-numbers']"
2945310,Differences between derivatives and strong derivatives,"Definition: Let $f$ be a real valued function. We say $f$ is $\mathbf{strongly}$ $\mathbf{differentiable}$ at $x = a$ if the following limits exists and is finite: $$ \lim_{x \to a, y \to a, x \neq y} \frac{ f(x)-f(y)}{x-y} = f^*(a) $$ and we can $f^*(a)$ the strong derivative of $f$ at $a$. Why is this definition of derivative different than the usual one? What is the main crucial point to understand here that makes it different?","['calculus', 'real-analysis']"
2945331,Does $\sum\limits_{k=1}^\infty \sum\limits_{n=k}^\infty \frac{(-1)^{n+k}}{n}$ diverge?,"Does $\displaystyle\sum_{k=1}^\infty \sum_{n=k}^\infty \frac{(-1)^{n+k}}{n}$ diverge? It is clear that the alternating Harmonic series converges: $$\sum_{n=1}^\infty \frac{(-1)^{n+1}}{n}=\log 2.$$ Thus, $S_k=\displaystyle\sum_{n=k}^\infty \frac{(-1)^{n+k}}{n}$ converges for each $k$ . For each $k$ , the sum could be expressed either as $\log 2-\alpha$ or $\alpha -\log 2$ . So, we're really only interested in how much $S_k$ deviates $(\alpha)$ from the alternating series $S_1$ . The numerators of $S_k$ follow for even $k$ and for odd $k$ . However, it seems that the partial sums may slowly go towards infinity as shown in this Wolfram plot here . Reasonably, since the difference between $S_1$ and $S_k$ probably behaves like $O(1/k)$ , the sum probably diverges. What would be the best way to show convergence/divergence?",['sequences-and-series']
2945411,How to define affine/projective varieties over fields which are not algebraically closed?,"In general, you come across affine and projective varieties as defined over algebraically closed fields, such as in Hartshorne's Algebraic Geometry , who defines an algebraic variety as an irreducible algebraic subset of $\mathbb{A}^n$ , endowed with the induced topology. a projective variety as an irreducible algebraic subset of $\mathbb{P}^n$ , endowed with the induced topology. My questions: Where exactly do I need that the field is algebraically closed for this definition (or maybe for other definitions necessary for this one)? How would I need to alter these definitions in case of a non-algebraically closed field? 
(I was thinking that it has to do sth with the definition of an algebraic set based on this post Affine variety over a field which is not algebraically closed can be written as the zero set of a single polynomial , i.e I do not have whole set of polynomials such that my variety equals the vanishing locus over them, but only one polynomial?) Thank you!","['affine-varieties', 'algebraic-geometry']"
2945446,Understanding the chain rule in the Wirtinger calculus,"The Wirtinger differential operators are defined by: \begin{equation}
\frac{\partial}{\partial z} = \frac{1}{2}\left(\frac{\partial}{\partial x} - i\frac{\partial}{\partial y}\right) \\
\frac{\partial}{\partial \bar{z}} = \frac{1}{2}\left(\frac{\partial}{\partial x} + i\frac{\partial}{\partial y}\right)
\end{equation} These satisfy the following chain rule: \begin{equation}
\frac{\partial}{\partial z}(f \circ g) = \left(\frac{\partial f}{\partial z}\circ g\right)\frac{\partial g}{\partial z} + \left(\frac{\partial f}{\partial \bar{z}}\circ g\right)\frac{\partial \bar{g}}{\partial z}.
\end{equation} Usually I think about partial derivatives as forming the components of the Jacobian (a.k.a differential/total derivative) and the chain rule for them as a matrix representation of the relation: \begin{equation}
 \mathbf{D}(f \circ g) = (\mathbf{D}f \circ g)\cdot\mathbf{D}g.
\end{equation} How can one interpret the chain rule for the Writinger differential operators in this light? I would particularly enjoy a formalism that allows me to understand why $\frac{\partial{f}}{\partial \bar{z}} = 0$ iff $f$ is analytic, or makes this seem like a really natural definition to begin investigation of the properties of analytic functions. Alternatively I would like formalism that is closely related to the idea of complexifying the tangent bundle of $\mathbb{R}^2$ with its standard complex structure, with an explanation of the connection.","['complex-analysis', 'chain-rule']"
2945485,"On the integral $\int_0^\pi\sin(x\sin(x\sin(x\cdots)))\,dx$","This is a follow-up question to the one with addition instead of multiplication . Consider $f_1(x)=\sin(x)$ and $f_2(x)=\sin(xf_1(x))$ such that $f_n$ satisfies the relation $$f_n(x)=\sin(xf_{n-1}(x)).$$ To what value does $$L:=\lim_{n\to\infty}\int_0^\pi f_n(x)\,dx$$ converge, where it exists? If it does not exist, what are the values of $$L_e:=\lim_{k\to\infty}\int_0^\pi f_{2k}(x)\,dx,\quad L_o:=\lim_{k\to\infty}\int_0^\pi f_{2k-1}(x)\,dx$$ for $k=1,2,\cdots$ ? The following diagram shows the values of $L_i$ for even and odd $i$ . The odd $i$ all have $x$ -coordinate $0.2$ and the even $i$ all have $x$ -coordinate $0$ . We can see that if the limits exist, it will be extremely unlikely that they will be the same for even and odd $i$ ; hence why I asked the final part of the question. I have tried to use @Tianlalu's method as in my previous question. If we define $t=\text{Sa}(x)$ as the inverse function of $y=t\sin t$ on $[0,\pi]$ , then $$t\sin t=x\implies t=\text{Sa}(x)$$ If the limit exists, then $$f_\infty=\sin(xf_\infty)\implies xf_\infty\sin(xf_\infty)=xf_\infty^2\implies f_\infty=\frac{\text{Sa}(xf_\infty^2)}x$$ which is not at all useful since we cannot write $f_\infty$ purely in terms of $x$ . Any ideas on how to continue?","['integration', 'limits', 'convergence-divergence']"
2945517,Solving system of ODE with initial value problem (IVP),"I have a question about a system of ODE. 
If we have: $\frac{dx}{dt}=x+2y$ $\frac{dy}{dt}=3x+2y$ with $x(0)=6$ and $y(0)=4$ , how come the solution to the IVP is: $x(t)=4e^{4t}+2e^{-t}$ $y(t)=6e^{4t}-2e^{-t}$ I tried doing integral by separating the variable but I didn't get that solution. That example & solution are from my numerical method book, please see the attached image example & solution","['differential', 'ordinary-differential-equations']"
2945557,Doe a smooth function map positive measure sets to positive measure sets,"Suppose $f: X \subset R^n \to R^n$ is a smooth function (for example $C^2$ function), and for each $y \in R^n$ , the set $f^{-1}(y)$ is finite. Do we have $f(A)$ is a positive measure set if $A$ has positive measure? If $C^2$ function is not enough, how about analytic function?","['measure-theory', 'lebesgue-measure', 'analyticity', 'geometric-measure-theory', 'general-topology']"
2945593,"$R$ be a relation on $\mathbb{Z}\times\mathbb{N}$ by $(n,m)R(n',m')$ iff $nm'=n'm$","Let $R$ be a relation on $\mathbb{Z}\times\mathbb{N}$ by $(n,m)R(n',m')$ iff $nm'=n'm$ I've shown that this defines an equivalence relation. I want to describe the equivalence classes. I've noticed that the relation happens if $\frac{n}{m}=\frac{n'}{m'}$ . So my guess is $[(n,m)]=\{\frac{n\cdot a}{m\cdot a}, a\in\mathbb{Z}\setminus\{0\}\}$ ? Also I'm asked to give a bijection $(\mathbb{Z}\times\mathbb{N})/R\rightarrow \mathbb{Q}$ . For this could we pick $[(n,m)]\mapsto \frac{n\cdot a}{m\cdot a}$ for $a\in\mathbb{Z}\setminus\{0\}$ . Also I'm a little in doubt about the notation - is $[(n,m)]$ correct to write?","['equivalence-relations', 'discrete-mathematics']"
2945637,Show that $\lim_{\alpha \to \infty} \int_{\mathbb{R}^n} f(x) \phi_{\alpha}(x - x_0) dx = f(x_0)$,"Given is a integrable function $\phi: \mathbb{R}^n \mapsto \mathbb{R}$ with the property $\int_{\mathbb{R}^n} \phi  dx = 1$ . We define for $\alpha > 0$ the re-scaled function $\phi_\alpha(x):= \alpha^n \phi(\alpha x) $ . Now I should show that for every continuous and bounded function $f: \mathbb{R}^n \mapsto \mathbb{R}$ and for every $x_0 \in \mathbb{R}^n$ the following holds: $\lim_{\alpha \to \infty} \int_{\mathbb{R}^n} f(x) \phi_{\alpha}(x - x_0) dx = f(x_0)$ So I've done some research and found that the properties of $\phi$ are very similar to something called the dirac-delta function $\delta$ . One property of $\delta$ is that $|\alpha| \delta(\alpha x) = \delta(x)$ . I will now prove that something similar holds for $\phi$ , which means I will show that $\alpha^n \phi(\alpha x) = \phi(x)$ ( $\alpha$ already is $>0$ so I can leave out the absolute value). Using multivariable substitution ( $u = \alpha x$ ): $\int_{\mathbb{R}^n} \alpha^n \phi(\alpha x) dx = \int_{\mathbb{R}^n} \alpha^n \phi(u) \frac{1}{\alpha^n} du = \int_{\mathbb{R}^n} \phi(u)du = 1$ If this is correct, my problem becomes easier and I only have to show that $\int_{\mathbb{R}^n} f(x)\phi(x-x_0) = f(x_0)$ I am not sure how to continue from this point on, but my guess would be that using the property of $f$ being bounded might prove useful. 
Also please correct me on mistakes I have done thus far. EDIT: Thanks to help in the comments I maybe figured out how to correct my mistakes and continue: Using multivariable substitution ( $u = \alpha x - \alpha x_0$ ): $\lim_{\alpha \to \infty} \int_{\mathbb{R}^n} f(x) \alpha^n \phi(\alpha x) dx = \lim_{\alpha \to \infty} \int_{\mathbb{R}^n} f(x_0 +\frac{u}{\alpha}) \phi(u) du$ Now if it is possible to get $\lim$ inside of the integral we have: $\int_{\mathbb{R}^n} \lim_{\alpha \to \infty}  f(x_0 +\frac{u}{\alpha}) \phi(u) du = \int_{\mathbb{R}^n} f(x_0) \phi(u) du = f(x_0)\int_{\mathbb{R}^n} \phi(u) du = f(x_0)$ Is this correct? If yes, which theorem can I use to get the limit inside the integral?","['integration', 'multivariable-calculus', 'real-analysis']"
2945641,Algebraic proof of Kodaira-Spencer isomorphism,"Let $X$ be a smooth scheme over $\mathbb{C}$ . Let us consider first-order deformations of $X$ over $S:=\operatorname{Spec}\mathbb{C}[t]/t^2$ i.e. flat surjective morphisms $\pi\colon \widetilde{X} \rightarrow S$ , such that $\widetilde{X} \times_S \operatorname{Spec}(\mathbb{C}) \simeq X$ . 
I do understand that to any such a morphism we may associate a short exact sequence $0 \rightarrow T_X \rightarrow T_{\widetilde{X}}|_X \rightarrow \pi^*(T_S)|_X \rightarrow 0$ that produces an element $\xi \in H^1(X,T_X)$ called Kodaira-Spencer class of the deformation $\pi$ . The question is the following: how to prove that the map $(\widetilde{X},\pi) \mapsto \xi$ is a bijection. I do understand the complex-analytic proof of this statement (for example from Claire Voisen's book). It is very natural but it uses the fact that there exists a covering of $\widetilde{X}$ by open subsets $\widetilde{U}_i$ s.t. the deformations $\pi|_{\widetilde{U}_i}\colon \widetilde{U}_i \rightarrow S$ are trivial. The existence of such covering is not obvious for me in the algebraic context.","['homological-algebra', 'complex-geometry', 'algebraic-geometry', 'deformation-theory']"
2945650,A question on a $2\times 2$ matrix differential equation,"Suppose we consider an equation $A f=\lambda f^*$ . Here $A$ is a differential operator with the general form $$A=a\frac{d^2}{dt^2}+b g(t)$$ where $a,b\in\mathbb{C}$ and $g(t)$ is a complex function of real variable $t$ . $f$ is complex function $f=f(t)$ , and $f^*$ is its complex conjugate. $\lambda$ is a real number. We have a complex-conjugated equation $A^* f^*=\lambda f$ . In total, we have the matrix form $$\pmatrix{\ 0\ \ A^*\\
A\ \ 0}\pmatrix{f\\ f^*}=\lambda\pmatrix{f\\ f^*}.$$ It is very easy to see that we have another equation $$\pmatrix{0\  \ A^*\\
A\ \ 0}\pmatrix{if\\ -if^*}=-\lambda\pmatrix{if\\ -if^*}.$$ Now I will conclude that $$-\lambda^2=\det\pmatrix{0\ \ A^*\\
A\ \ 0}=-\det(A^*A).\tag{1}$$ All these look fine to me up to now. But I tried another way in the following and found a disagreement. Writing $A=U+iV$ , $f=x+iy$ , then we have the following matrix equation $$\pmatrix{U\ \ -V\\
-V\ \ -U}\pmatrix{x\\ y}=\lambda\pmatrix{x\\ y}.$$ This equation is also associated with another equation $$\pmatrix{U\ \ -V\\
-V\ \ -U}\pmatrix{-y\\ x}=-\lambda\pmatrix{-y\\ x}.$$ And I conclude $$-\lambda^2=\det\pmatrix{U\ \ -V\\
-V\ \ -U}=-\det (U^2+V^2).$$ However $A^*A=U^2+V^2+i[U,V]$ . If both methods are correct then there must be $$\det (U^2+V^2+i[U,V])=\det(U^2+V^2).\tag{2}$$ Is this true? Or there is something wrong in one of these two methods or in both? For Eq.(2), one possibility that it may be true is that we know $$\det(AA^*)=\det(A^*A)$$ which gives $$\det(U^2+V^2+i[V,U])=\det(U^2+V^2+i[U,V]).$$ Somehow the part $i[V,U]$ or $i[U,V]$ does not contribute in the determinant?","['operator-theory', 'determinant', 'ordinary-differential-equations']"
2945659,"Armed with a sword and a shield, a proud knight combats a monstrous hydra with 100 heads.","Hi so I am battling with this question at the moment. ""Armed with a sword and a shield, a proud knight combats a monstrous hydra with 100 heads. When he slashes, he removes 17 heads and 5 heads grow back. When he slices, he removes 6 heads and 33 grow back. When he cuts, 14 heads fall and 8 grow back. When he stabs, 2 heads fall and 23 grow back. In order to kill the hydra, all its heads must be removed at some point, in which case no heads will grow back. Can the knight’s sword and courage triumph against this mythological monster?"" I am assuming the hydra cannot grow more than the original 100 heads, and that you must cut off the exact amount (you can't slash 9 times). My current working consists of: rule a = shrink 6 rule b = shrink 12 rule c = grow 21 rule d = grow 27 b can be discounted as it's just aa as 100 is the start point, and 6 is the minimum reduction some sum of 100 + c(x) + d(y) = a(z), or 100 + (21 * x) + (27 * y) must equal a multiple of 6. 100 + (21 * x) + (27 * y) = 6 * z where x, y, z are positive integers simplifies to z = (7x/2) + (9y/2) + (50/3). For all positive integers x, y, there is no integer solution for z I am not convinced this is enough proof.","['proof-verification', 'linear-algebra', 'puzzle']"
2945671,The definition of a rational function and $f(x) = {1}/{x}$,I have a small question: Can one state that $$f(x) = \frac{1}{x}$$ is a rational function because it is the quotient between a polynomial with degree 0 and a polynomial with degree 1? Thank you.,"['algebra-precalculus', 'definition', 'rational-functions']"
2945690,"On the integral $\int_{-\pi/2}^{\pi/2}\sin(x/\sin(x/\sin(x/\sin\cdots)))\,dx$","This question is the final one out of the set (see I and II ), I promise! Consider $f_1(x)=\sin(x)$ and $f_2(x)=\sin\left(\frac x{f_1(x)}\right)$ such that $f_n$ satisfies the relation $$f_n(x)=\sin\left(\frac x{f_{n-1}(x)}\right).$$ To what value does $$L:=\lim_{k\to\infty}\int_{-\pi/2}^{\pi/2} f_{2k-1}(x)\,dx$$ converge, for $k=1,2,\cdots$ ? Here is a very nice graph showing the likely convergence of $f_n$ : The $R^2$ value is extremely close to $1$ , and the best fit curve is given by the equation $$y=\frac{0.2091}{e^x-0.5226}+2.411$$ which implies that $$L\approx2.411$$ Are there any analytic techniques to prove this?","['integration', 'limits', 'convergence-divergence', 'regression']"
2945733,Why does $\int_0^{n\pi}\frac{dx}{1+\tan^{2k}(x)}=n\frac\pi2$ hold for all non-negative integer $k$?,"I thought of putting $t=\tan x, x=\arctan t, dx=\frac{dt}{1+t^2}$ but then I erroneously get $$\int_{tan(0)}^{\tan(n\pi)}\frac{dt}{(1+t^2)(1+t^{2k})}$$ which is $0$ I believe I should write the integral as $\int_0^\pi(\cdot)+\int_\pi^{2\pi}(\cdot)+\cdots+\int_{(n-1)\pi}^{n\pi}(\cdot)$ and then see that each one is $\pi/2$","['integration', 'definite-integrals', 'real-analysis', 'calculus', 'trigonometry']"
2945783,intuitively obvious integration identity,"Let $\mu$ denote the Lebesgue measure on $[-\infty,\infty]$ . For each measurable subset $E\subset(0,\infty)$ , define the nondecreasing function $m_E:(0,\mu(E))\to(0,\infty)$ by the rule $$m_E(t)=\inf\left\{s\in(0,\infty):\mu\left(E\cap(0,s)\right)=t\right\}.$$ Question 1. I would like to show the following:  If $f:(0,\infty)\to[0,\infty]$ is a (nonnegative) measurable function, then $$\int_Ef(t)\;dt=\int_0^{\mu(E)}(f\circ m_E)(t)\;dt.$$ Discussion. My intuition tells me it should work, but I'm too rusty on my measure theory to prove it. The analogy here is to the $\ell_1$ norm of subsequences.  For example, if $B\subset\mathbb{N}$ , then let $$i_B(n)=\left\{i\in B:\#\left(B\cap[0,i]\right)\leq n\right\}.$$ It follows that, for any sequence of nonnegative scalars $(a_n)_{n=1}^\infty$ , we have $$\sum_{n\in B}a_n=\sum_{n=1}^{\#B}a_{i_B(n)}.$$ In other words, we have ""pushed"" $(a_n)_{n\in B}$ down to $(a_{i_B(n)})_{n=1}^{\#B}$ so that their $\#$ -integrals are the same. I would like to construct an analogous transformation to work with nonnegative functions on $(0,\infty)$ . Thanks!","['integration', 'measure-theory', 'lebesgue-measure', 'lebesgue-integral', 'real-analysis']"
2945794,Example of a concrete irrationality test,"Can you give some example of an irrational number that can be proved to be irrational with this theorem? Theorem .
Given $a\in \mathbb{R}$ , if there exists a sequence of integers $u_n,v_n \rightarrow \infty$ such that: $a$ is not equal to any $u_n/v_n$ . The quotien sequences aproximates $a$ in this sense: $$\lim_{n\rightarrow \infty}v_na-u_n=0 $$ then the number $a$ is irrational. The proof is in: https://math.stackexchange.com/q/898420","['number-theory', 'irrational-numbers']"
2945795,Is it possible to demonstrate two groups are isomorphic without specifying an isomorphism between them?,"Usually, when there are two finite groups of small order, we can check if they are isomorphic by trying to impose an isomorphism. But suppose we have two very large finite groups, what are some conditions on the two groups I can scrutinise so as to conclude an isomorphism exists between them or not? One condition I can think of is they must have the same order. My another crude guess is If an isomorphism $\phi$ exists between two finite groups $G_1$ and $G_2$ , then $\phi$ must map element of order $a$ in $G_1$ to element
  of order $a$ in $G_2$ too. So there are two main questions in this post: 1.What are some conditions on the two groups one can scrutinise so as to conclude an isomorphism exists between them or not 2.Whether my guess is true, and whether there is a counterexample.","['group-theory', 'group-isomorphism', 'finite-groups']"
2945808,Proof regarding the golden succession [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question Considering the golden number $\varphi=\frac{1+\sqrt 5}2$ and the succession defined by $$r_n=1 +\frac 1{r_{n-1}}$$ for all $n \geq 2$ where $r_1=1$ How do I prove that $r_n=\frac{f_n}{f_{n-1}}$ , where $f_n$ is the Fibonnaci sequence?","['fibonacci-numbers', 'discrete-mathematics']"
2945821,What does a notation $R^X$ generally mean for a semiring $R$ and a set $S$?,"While going through a lecture note by a speaker named-Sam Payne, entittled-Tropical Scheme Theory (Idempotent Semirings), i encounter a notation $R^X$ , saying that if $R$ is an idempotent semiring and $S$ is set then $R^X$ is an idempotent semiring. I couldn't figure out what does the notation $R^X$ mean?","['elementary-set-theory', 'semiring']"
2945862,Limit Epsilon Delta,"everybody! I am a new user here. Please correct me if I make any mistakes. Show that for any $\epsilon$ >0 there exists N such that for all n $\geqq$ N it is true that $|x^n - 0|$ < $\epsilon$ x $\in$ (-1,1) $x^n \to$ 0 as n $\to$ ∞ I tried to solve this problem $lim_{x\to ∞}$ $x^n$ = 0 | $x^n$ -0|< $\epsilon$ $x^n$ < $\epsilon$ But, I am not sure how to continue the proof. I also have another question: $x_n$ = $\frac {a^n - b^n}{a - b}$ $(\frac{b}{a})^n$ $\to$ 0 as n $\to$ ∞ Show that for any integer k $\geqq$ 1 $\frac {x_n+_k}{x_n}$ $\to$ $a^k$ as n $\to$ ∞ This is what I did: And I got $lim_{n\to ∞}$ $\frac {a^n{^+}^k - b^n{^+}^k}{a^n - b^n}$ Again, I am stuck as I don't know how to finish it. Can someone please direct me step-by-step? I need to understand this topic well. Thank you very much.","['limits', 'calculus', 'proof-writing']"
2945864,Find the shortest path from a point to curve,"Problem Find the shortest length from the point $(2,8)$ to the curve $C=\{(x,y)|y=x^{2/3}+8, x \ge 0\}$ Attempt to solve Here is the associated plot of the situation: By drawing a triangle to this image we can form a length function of $x$ by Pythagora's theorem. This function $L(x)$ is: $$ L^2(x)=(2-x)^2+(x^{2/3}+8)^2 $$ Now since $\frac{d}{dx}L^2(x)=0 \iff \frac{d}{dx}L(x)=0$ $$ L'(x)=2x+\frac{4 \sqrt[3]{x}}{3}-4 $$ Now I want to solve when $$ L'(x)=0 \implies 2x + \frac{4\sqrt[3]{x}}{3}-4=0 $$ $$ \implies 2x+\frac{4\sqrt[3]{x}}{3}=4 $$ $$ \implies 6x+4\sqrt[3]{x}=12 $$ $$ \implies \sqrt[3]{x}=3-\frac{6x}{4} $$ $$ \implies x = (3-\frac{6x}{4})^3 $$ $$ \implies 27x^3-162x^2+332x-216=0 $$ $$ \implies x= \frac{2}{9}(9-\frac{2}{\sqrt[3]{\sqrt{737}-27}})+\sqrt[3]{\sqrt{737}-
27} \approx 1.2768 $$ Now if you take a look at the image this is probably wrong since I would approximate just by looking at the image that the shortest length is in $x \in [1.5,2]$ .","['optimization', 'calculus', 'derivatives']"
2945880,How is the discrete metric continuous?,"It is proven that any metric $d$ is continuous. 
Consider the metric space $(\mathbb{R}, d)$ where: $$d:\mathbb{R}\times\mathbb{R}\rightarrow \mathbb{R}$$ $$d(x, y) =  \begin{cases} 
      0 & x=y \\
      1 & x\neq y
   \end{cases}
$$ Let $x_n \rightarrow x=0$ and $y_n \rightarrow y=0$ . If you take $d(x_n, y_n)\rightarrow 1 \neq 0=d(x, y)$ . This shows that this metric is discontinuous. What is wrong with my reasoning?","['continuity', 'metric-spaces', 'real-analysis']"
2945893,(Logic) can a set be both reflexive and asymmetric? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question I am just starting to learn logic at undergraduate level. My exercise book is asking me to: ""Specify a relation and a set $S$ such that the relation is reflexive on $S$ and asymmetric"". How can a set be both reflexive and asymmetric?","['elementary-set-theory', 'logic', 'relations']"
2945926,Help with proof by induction and divisibility,"I have a question about induction (I'm a little fuzzy on it). Prove $3n+1<n^2$ for all integers $n\geq 4$ . Assume $k^2+k$ is an even integer for any $k\in\mathbb{Z}$ . Prove $n^3-n$ is divisible by 6 for all $n\in\mathbb{N}$ . For 1, I have: Base Case: Let $n=4$ . Evaluate the inequality, returning $13<16$ . Thus, the base case is valid. Induction step: Suppose that, given $k\geq 4$ , $3k+1<k^2$ is true for $n=k$ . Then, $3(k+1)+1<(k+1)^2$ . I am stuck from here. For 2, I have: Base case: Let $n=1$ (the first natural number in this case). Evaluate the expression $n^3-n$ as $0$ . Now I am stuck. Am I supposed to evaluate at $n=2$ ? Induction step: take $k$ to be true such that $k^3-k$ is divisible by 6 (am I allowed to do this?). Then, $(k+1)^3-(k+1)$ , or $k^3+3k^2+3k+1-(k+1)$ , or $(k^3-k)+3(k^2+k)$ . Now I am stuck. Help is appreciated!","['induction', 'proof-writing', 'telescopic-series', 'discrete-mathematics']"
2945975,Confusion in determining an argument's validity?,"I understand that an argument is basically an implication in the sense that: $$premise1 \land premise2 \land premise3.... \to conclusion$$ And an argument would be considered valid when such an implication is a tautology. What I don't understand is do we consider the actual truth value of the statements when validating an argument? Lets say this is an argument: Premise: 36 is divisible by 9 Conclusion: 36 is divisible by 3 Now this argument, when taken as $p \to q$ where $p$ is a premises and $q$ is a conclusion is invalid. Whereas the intuitive sense by understanding this argument says that the argument holds according to mathematical rules. My question is when we formulate a truth table of the above $p \to q$ , there is one condition when $p = T$ , $q = F$ when the argument fails. But intuitively and in all practical senses, $p = T$ , $q = F$ is not a possible combination and hence why we can ignore it saying the argument is valid? Or an argument is in fact when we don't talk in concrete statements and rather abstract statement variables, which for any statements should work? Lastly, what is the difference between an argument being valid and implication (of the form $p \land q \land r \to z$ ) being true?","['propositional-calculus', 'first-order-logic', 'logic', 'discrete-mathematics', 'logic-translation']"
2945995,Existence of infinite iteration of functions $f_\infty$?,"Given a sequence of functions $\{f_n\}$ satisifying an iterated relation such as $f_n(x)=g(x+f_{n-1}(x))$ $f_n(x)=g(xf_{n-1}(x))$ $f_n(x)=g(x/f_{n-1}(x))$ Where $g:=f_1$ is continuous on the interval $[a, b]$ (or differentiable on $(a,b)$ for stronger assumptions) Question: How to prove the existence of $f_\infty(x):=\lim\limits_{n\to \infty}f_n(x)$ ? AND Are there any methods to prove such $f_\infty$ does not exist? The question comes from the problems $\displaystyle\int_0^\pi\sin(x+\sin(x+\sin(x+\cdots)))\,\mathrm dx=2$ and $\displaystyle\int_0^\pi\sin(x\sin(x\sin(x\cdots)))\,\mathrm dx$ and $\displaystyle\int_{-\frac\pi2}^{\frac\pi2}\sin\frac x{\sin\frac x{\sin\frac x{\sin\cdots}}}\,\mathrm dx=\frac34 \pi$ . Let $g(x)=\sin x$ . I ""proved"" the $1^{\rm{st}}$ and  the $3^{\rm{rd}}$ integral by assuming the exsistence of $f_\infty$ . @Sangchul Lee think $f_\infty$ in the $2^{\rm{nd}}$ integral does not exist due to the chaotic behavior . If $f_\infty$ in the $2^{\rm{nd}}$ integral exists, then $$L=\int_0^\alpha \sin y\,\mathrm d\left(\frac y{\sin y}\right) =1.86006...$$ where $\alpha=2.31373...$ is the positive root of $\dfrac t{\sin t}= \pi$ . Some thoughts so far: If we could prove $f(t)=g(x_0+t)$ is a contraction mapping on $[a,b]$ for every $x_0\in[a,b]$ , that is, if $t_0$ (depending on $x_0$ ) is the only fixed point on $[a,b]$ , then the result is intuitively true from Banach Fixed Point Theorem (similar to the case $f(t)=g(x_0t)$ and $f(t)=g(x_0/t)$ ). However, we could not apply the theorem for any $f$ , one example is $f(t)=\sin(x_0+t)$ in the $1^{\rm{st}}$ integral.","['limits', 'convergence-divergence', 'chaos-theory', 'fixed-point-theorems']"
2946014,Separation Properties in Topology,"This is Remark 54.5 from Kasriel's Topology book pg. 110 ""Suppose X is a metric space Then subsets A and B are mutually separated subsets of  X IFF A and B are both closed (or equivalently both open) if A $\cup$ B are disjoint."" I am trying to understand this remark. First of all I thought there can be open subsets that are mutually separated but they're neither open or closed. Second, why in this case A and B being both open and closed are equivalent? Munkres's book also states that if subsets A and B form a separation in Y then A is both open and closed. So there must be something I am misunderstanding.","['general-topology', 'metric-spaces']"
2946046,Example of a dense but not schematically dense open subset,"We have the following theorem in algebraic geometry Let $X$ be a reduced $S$ -scheme, $Y$ a separated $S$ -scheme and $U\subseteq X$ a dense open subset. Then two morphisms $f,g:X\rightarrow Y$ that are equal over $U$ must be equal. I am trying to understand why these hypothesis are needed. It is easy to see why $Y$ must be separable, otherwise I can take the affine line with double point at the origin and the two different inclusions of the usual line on it. These morphisms are equal on $\mathbb{A^1}\setminus \{0\}$ but not on the entire double line. Basically this example is the same as the one we have in general topology when $Y$ is not Hausdorff and the reason amount to the fact that the equalizer of $f$ and $g$ must not be closed. But it has been harder for me to understand why the hypothesis for $X$ is needed. It appears the reason is the existence of open subsets $U\subseteq X$ that are dense in the topological level but not in the sheaf level. So you can embed them inside a closed immersion $Z\hookrightarrow X$ where $Z=X$ as sets but $\mathcal{O}_Z=\mathcal{O}_X/\mathcal{J}$ with $\mathcal{J}\subseteq \mathcal{Nil}(X)$ . I tried to construct such an open set but I failed. Anyway, I looked in the literature and I saw that the right notion of dense open subset for schemes is called schemetically dense (or scheme theoretically dense), as can be seen in the book of Görtz, Wedhorn or the Stack Project and are exactly the ones that avoid this problem. So now my question is What is an example of a dense open subset $U\subseteq X$ that is not schematically dense? I tried to construct it with an affine $X$ and $U$ principal but it appears it must be something more involved than that.","['algebraic-geometry', 'schemes']"
2946059,Random Bridge Hand w Cards of exactly two suits,"Question: What is the probability that a random bridge hand contains cards of exactly two suits? My Attempt At A Solution Bridge hands consist of $13$ cards, and a suit contains $52$ cards, so the way to pick a random bridge hand would be $$\frac{{26\choose 13}-2}{{52 \choose 13}} $$ As user @Lord Shark the Unknown hinted: there are $26\choose 13$ ways to choose hands of two suits but one of those suits is only say hearts, and another only spades, so we must compensate for those. Thank you for any corrections/hints.","['combinations', 'combinatorics', 'card-games', 'probability']"
2946069,What can be added to a non positive definite matrix to make it positive definite?,"I'm reading about the Newton Algorithm for optimization, and when the hessian is not positive definite, it says that I can add a matrix $E_k$ such that $B_k = \nabla^2 f(x_k) + E_k$ is sufficiently positive definite. What does that mean? How does $E_k$ looks like?","['matrices', 'optimization', 'nonlinear-optimization', 'linear-algebra']"
2946085,Integral of the form $\int_a^b \frac{\ln(c+dx)}{P(x)}dx$,"I found here a ""great theorem"" which states that: $$\int_a^b \frac{\ln(c+dx)}{P(x)}dx =\frac{\ln((ad+c)(bd+c))}{2}\int_a^b\frac{dx}{P(x)}$$ I don't know how to prove this, but I am pretty sure that we should work by symmetry with a substitution of the form $\frac{mx+n}{sx+p}$ , then add the result with the initial integral. An easier case which shows this idea is the well-known integral $\int_0^1 \frac{\ln(1+x)}{1+x^2}dx$ which can be dealt with the substitution $\frac{1-x}{1+x}$ which produces $\int_0^1 \frac{\ln 2 -\ln(1+x)}{1+x^2}dx$ and adding this with the initial integral simplifies the logarithm. In our case, after finding the magic substitution we will have: $$\int_a^b \frac{\ln(c+dx)}{P(x)}dx=\int_a^b \frac{\ln((ad+c)(bd+c)) - \ln(c+dx)}{P(x)}dx$$ Unfortunately I dont know what $P(x)$ is, but it's hard to believe that it can be any polynomial of the form $x^2+sx+p$ . I would appreciate some help to prove this ""great theorem"".","['integration', 'definite-integrals']"
2946105,"Finding the support of the CDF of $(X,Y)$","Assume $X$ to be standard normal random variable, and define $Y$ as $$Y=\begin{cases}X,&\text{if }⌊X⌋\text{ is even}\\-X,&\text{if }⌊X⌋\text{ is odd}\end{cases}.$$ I am trying to show that $X$ and $Y$ are mutually completely dependent. For this I want to find the support of the copula of $(X,Y)$ : $$ C(u,v)=\mathbb{P}( X \leq F^{-1}(u), Y \leq G^{-1}(v)),$$ and conclude by finding the probability mass concentrated on some locus. Here $F(x) = \Phi(x)$ is the standard normal distribution of $X$ , and $G(y)$ is the distribution of $Y$ . However, I am not sure how to technically tackle the $Y$ , that is, how to properly split it between cases odd/even integer values. My approach: I am adding an excerpt from another question of mine (which is linked) with my approach. If my work is correct, I worked out that $$G(y)=\frac{1}{2}[1 + F(y) - F(-y)].$$ Further, \begin{align*}
C(u,v) &= \mathbb{P}(X \leq F^{-1}(u), Y \leq G^{-1}(v))\\
&= \frac{1}{2}\left[\mathbb{P}(X \leq F^{-1}(u), X > -G^{-1}(v)) + \mathbb{P}(X \leq F^{-1}(u), X \leq G^{-1}(v)) \right]\\
&=\frac{1}{2}\left[\mathbb{P}(X \leq \min\{F^{-1}(u), G^{-1}(v)\}) + \mathbb{P}(X \in [-G^{-1}(u), F^{-1}(v)])  \right]\\ 
&= \frac{1}{2}\left[ v - F(G^{-1}(v)) + F(\min\{F^{-1}(u), G^{-1}(v)\})  \right]. \tag{1}
\end{align*} It seems I might be able to untangle this as a function of $(u,v)$ if I find $F(G^{-1}(v))$ . However, here I'm not too sure that the usual approach of finding the inverse works. $$ y = G(G^{-1}(y)) =\frac{1}{2}[1 + F(G^{-1}(y)) - F(-G^{-1}(y))] $$ $$ 2y -1  = F(G^{-1}(y)) - F(-G^{-1}(y)) $$ $$ F^{-1}(2y -1)  = F^{-1}\left(  F(G^{-1}(y)) -F(-G^{-1}(y)) \right) \tag{2}$$ And it seems I am stuck here . I am thinking that it is enough to simplify $(1)$ to some convenient form, here my idea is that finding the expression for $G^{-1}(y)$ would help, but I got stuck at $(2)$ . Maybe another approach is better? Would it be easier to show that $\mathbb{P}(X = g(Y))= 1$ , where $g$ is some function of $Y$ ? But I still need to find the support of $C$ for following exercises. Would appreciate any hints or suggestions on how to proceed!","['probability-distributions', 'probability-theory', 'probability', 'random-variables']"
2946197,Why is the bounded functions not a sheaf?,"I know that the presheaf of bounded functions is not a sheaf but I don't see why. I checked in wikipedia they say that this presheaf does not verify the axiom of ""Glue"" . For me it verifies this axiom. Indeed, if U and V are open sets, and f and g are bounded functions on U and V respectively, and they agree on the intersection, then combining them in the obvious way - let h(x) = f(x) if x is in U, g(x) if x is in V, is a bounded function with the bound being max(|f|, |g|). Do you think I don't understand well what gluing means? Thanks.","['algebraic-geometry', 'sheaf-theory']"
2946244,There are three lights which can be in one of three states. Can we get the system of lights into a specific state?,"If there is someone who can come up with a better title for this please edit the title. There are three lights in a line.  Each light can be in one of three states:  off, light red, and dark red.  There is a cycle of states:  OFF, then LIGHT RED, then DARK RED, then back to OFF. There are three switches which control the lights like so: Switch A - advances the cycle for the first two lights
Switch B - advances the cycle for the all three lights
Switch C - advances the cycle for the last two lights. If we start with all three lights in the off state can the switches be pushed in some order so that the three lights in the line are in:  OFF-LIGHT RED-DARK RED? I'm trying to model this with linear algebra.  Where A,B,C are the lights in a row and we push A x times, B y times, and C z times.  Of course the numbers are mod 3 because after 3 pushes we wrap back to the off state. Any suggestions?","['linear-algebra', 'modular-arithmetic']"
2946270,$f_* f^* \mathcal{G}= \mathcal{G}$ and $f^* f_* \mathcal{F}= \mathcal{F}$ for Quasicoherent Sheaves,"Let $f: X \to Y$ a morphism between schemes, $\mathcal{F}$ a quasicoherent $\mathcal{O}_X$ module, $\mathcal{G}$ a quasicoherent $\mathcal{O}_Y$ module. My question is what are the weakest possible conditions for $f$ , $\mathcal{F}$ and $\mathcal{G}$ such that $f_* f^* \mathcal{G}= \mathcal{G}$ and $f^* f_* \mathcal{F}= \mathcal{F}$ hold? (*) Considerations: $f^*$ and $f_*$ are connected via adjunction $$Hom_{\mathcal{O}_X}(f^*\mathcal{G},\mathcal{F})= Hom_{\mathcal{O}_Y}(\mathcal{G}, f_*\mathcal{G})$$ and this gives arise for natural unit and counit trafos $\mathcal{G} \to f_* f^* \mathcal{G}$ and $\mathcal{F} \to f^* f_* \mathcal{F}$ . But it seems that  in generally the adjunction don't provider more informations. Does there exist a suitable theorem providing sufficent conditions for the above property (*)? Background of my question: I want to find under which conditions the induced map $f^*: Pic(Y) \to Pic(X)$ is injective? Indeed, if I know that $f_* f^* \mathcal{G}= \mathcal{G}$ holds I get the desired injectivity.","['algebraic-geometry', 'coherent-sheaves', 'quasicoherent-sheaves']"
2946304,What's the area enclosed by $x^4-x^2y^2+y^4=1$,What's the area enclosed in $x^4-x^2y^2+y^4=n$ ? The image above has $n=1$ . We can convert to polar coordinates of course. $r^4[\cos^4(\theta)-\cos^2(\theta)\sin^2(\theta)+\sin^4(\theta)]=n$ Not exactly sure how that might help us. I can approximate the area by creating some random points between $0$ and $2$ and asking how often they fall in the space enclosed by $x^4-x^2y^2+y^4=1$ . This leads me to an area of $\approx 4.3$ . Can I get an exact value for this?,"['multivariable-calculus', 'calculus']"
2946305,What are practical examples of Toeplitz matrices?,"A Toeplitz matrix is one in which each descending diagonal from left to right is constant. Given that structure, matrix operations are sometimes much faster. Where are Toeplitz matrices likely to occur?","['toeplitz-matrices', 'big-list', 'applications', 'matrices', 'linear-algebra']"
2946338,How is the sequence $x_{n+1} = \frac{(x_n)^{2} + 5}{ 6}$ going to converge to 1,"Having some trouble understanding that if $x_{1} = 4$ and the sequence where n is defined as $x_{n+1} = \frac{(x_n)^{2} + 5}{ 6}$ how is it going to converge to 1. I have solved using the L as limit and using the quadratic i get two possibilites that are 5, or 1.","['limits', 'convergence-divergence', 'sequences-and-series']"
2946370,"The order of $(g,h)$ equals the least common multiple of ord(g) and ord(h)?","Let $G$ and $H$ be two groups and let $g∈G$ and $h∈H$ be two elements of  finite orders. Is the order of $(g,h)$ equals the least common multiple of $\operatorname{ord}(g)$ and $\operatorname{ord}(h)$ ? That is $$\operatorname{ord}(g,h)=\operatorname{lcm}(\operatorname{ord}(g),\operatorname{ord}(h))$$ It is my conjecture, and I am not sure if it is correct or not. If it's true, how can I formally prove it?","['group-theory', 'abstract-algebra']"
2946424,"Extension Classes $Ext^1 _X (\mathcal{F},\mathcal{G})$ of Sheaves","Let $k$ be a field and $X$ be a proper $k$ scheme. Futhermore let $\mathcal{H}$ be a coherent $\mathcal{O}_X$ -module. Grothendieck's Finiteness Theorem says that for all $i \ge 0$ the cohomology groups $H^i(X, \mathcal{H})$ are finite vector spaces . Let $\mathcal{F}, \mathcal{G}$ be two inverible sheaves. Consider $H^1(X, \mathcal{F}^{\vee} \otimes \mathcal{G}) = k^n$ . Under some conditions for $X$ we have the identification $H^1(X, \mathcal{F}^{\vee} \otimes \mathcal{G}) = Ext^1 _X(\mathcal{O}_X, \mathcal{F}^{\vee} \otimes \mathcal{G})= Ext^1 _X (\mathcal{F},\mathcal{G})$ . $Ext^1 _X (\mathcal{F},\mathcal{G})$ describes the extension classes of $\mathcal{F}$ by $\mathcal{G}$ . By definition two extensions $\mathcal{L}, \mathcal{K}$ are equivalent if there exist a sheaf isomorphism $d: \mathcal{L} \to \mathcal{L}$ such that following diagram commutes: $$
\require{AMScd}
\begin{CD}
0 @>{} >> \mathcal{F} @>{a}  >> \mathcal{L} @>{a} >> \mathcal{G} @>{}  >> 0\\
@VV0V @VVidV  @VVdV  @VVidV @VV0V \\
0 @>{} >> \mathcal{F} @>{b}>> \mathcal{K} @>{a} >> \mathcal{G} @>{}  >> 0;
\end{CD}
$$ Now the question: Generally, $Ext^1 _X (\mathcal{F},\mathcal{G})$ is only a set consisting of all extension equivalence classes. But here - since
it is isomorphic the vector space $k^n$ - I'm curious if this extra vector space
structure tells ""more"" about the shape of the extension classes. For example: How are the equivalence classes related to each other which 
belong in $k^n$ to the same line $l \subset k^n$ . Is there a special
connection between them in the commuting diagram? Or same subspace? Or: What can we say about $d$ between two aquivalence classes lying in the same line?
In $k^n$ they can be transformed to each other but simple scalar multiplication $ \cdot k$ . Does this tell something about $d$ ?","['algebraic-geometry', 'sheaf-theory']"
2946435,Evaluation of $\int_0^1\frac{\ln(x)\ln(x+1)\ln(x^2+x+1)}{(1-x)(1+x^2)}dx$,"I originally saw this logarithmic integral pop up on the Integrals and Series forum but due to the inactivity there no conversation has developed. I originally tried generalizing the integral to $$I(\alpha,\;\beta)=\int_0^1\frac{\ln(x)\ln(\alpha x+1)\ln(\beta(x^2+x)+1)}{(1-x)(1+x^2)}dx$$ and differentiating under the integral sign with respect to $\alpha$ and $\beta$ but it didn't seem to go anywhere. However, I did get the related integrals $$\int_0^1\frac{(x^2+x)\ln(x)}{(1-x)(1+x^2)(x^2+x+1)}dx=-\frac{17\pi^2}{432}$$ and $$\int_0^1\frac{(x^2+x)\ln(x)}{(1-x^2)(1+x^2)(x^2+x+1)}dx=\frac{-1296G-115\pi^2+192\psi\left(\frac{1}{3}\right)-96\psi\left(\frac{2}{3}\right)}{2592}.$$ My questions are: What is the closed form of this integral, if there is any? And are there any methods or techniques that would be effective at attacking integrals with multiple logarithms in the numerator of the integrand? Edit: G is Catalan's constant and psi is the digamma function.","['complex-analysis', 'definite-integrals', 'special-functions']"
2946471,Random Gift Giving at a Party - Combinatorics Problem,"Each of $10$ employees brings one (distinct) present to an office party. Each present is given to a randomly selected employee by Santa (an employee can get more than one present). What is the probability that at least two employees receive no presents? Firstly, there are $10^{10}$ total ways to give the $10$ employees the $10$ presents. So this is our denominator. My attempt was to consider the complement and consider the number of ways that either $0$ employees receive no presents (every employee gets a present) or $1$ employee receives no present. Case 1: $0$ employees There are $10$ employees and $10$ presents. So there are $10^{10}$ ways to give the presents. Case 2: $1$ employee Step 1: Decide which employee receives no presents: $10$ possibilities. Step 2: Distribute the $10$ presents to the remaining $9$ employees: $9^{10}$ ways. So the number of ways in which at least $2$ employees receive no presents is: $1-(10^{10}+9^{10}$ ). So my final answer is: $1-\displaystyle\frac{(10^{10}+9^{10})}{10^{10}}$ . However, this answer does not match the answer in my textbook. Which is: $1-\displaystyle\frac{10!-10\times 9 \times \frac{10!}{2!}}{10^{10}}$ Where did my attempt go wrong and how can I correct it?","['combinatorics', 'probability']"
2946494,Is there an elementary method of finding this missing angle?,"Let a point $P$ lie in a triangle $\triangle ABC$ such that $\angle BCP = \angle PCA = 13^\circ$ , $\angle CAP = 30^\circ$ , and $\angle BAP = 73^\circ$ .  Compute $\angle BPC$ . I have an ugly trig solution that looks something like this: Let $\angle PBC = \theta$ .  It follows that $\angle PBA = 51-\theta$ .  From trig Ceva, we see that: $$\frac{\sin(30)}{\sin(73)}*\frac{\sin(51-\theta)}{\sin(\theta)}*\frac{\sin(13)}{\sin(13)} = 1$$ Observe that $90-73=17$ , and conveniently $17*3=51$ .  This inspires the following manipulations: $$\frac{1}{2\sin(73)} * \frac{\sin(51-\theta)}{\sin(\theta)} = 1$$ $$\frac{1}{2\cos(17)} * \frac{\sin(51)\cos(\theta)-\cos(51)\sin(\theta)}{\sin(\theta)} = 1$$ $$\sin(51)\cos(\theta)-\cos(51)\sin(\theta)= 2\cos(17)\sin(\theta) $$ $$\sin(51)\cos(\theta) = 2\cos(17)\sin(\theta) + \cos(51)\sin(\theta)$$ $$\sin(51)\cos(\theta) = \sin(\theta)(2\cos(17) + \cos(51))$$ $$\tan(\theta) = \frac{\sin(51)}{2\cos(17) + \cos(51)}$$ Proceeding with triple-angle formulae: $$\tan(\theta) = \frac{3\sin(17)-4\sin^3(17)}{2\cos(17) + 4\cos^3(17)-3\cos(17)}$$ $$\tan(\theta) = \frac{\sin(17)}{\cos(17)} * \frac{3-4\sin^2(17)}{4\cos^2(17)-1}$$ $$\tan(\theta) = \tan(17) * \frac{3-4(1-\cos^2(17))}{4\cos^2(17)-1}$$ $$\tan(\theta) = \tan(17) * \frac{4\cos^2(17)-1}{4\cos^2(17)-1}$$ $$\tan(\theta) = \tan(17)$$ We conclude that $\theta = 17$ and $\boxed{\angle BPC = 150}$ . This is simply horrific.  Is there a more elegant method?  I notice that $73 = 13 + 60$ , but I don't see where I would put an equilateral triangle.","['triangles', 'geometry']"
2946504,Completing the proof for a combinatorics question from OIM 1994,"The question states: In every square of an $n × n$ board there is a lamp. Initially all the lamps are turned off. Touching a lamp changes the state of all the lamps in its row and its column (including the lamp that was touched). Prove that we can always turn on all the lamps and find the minimum number of lamps we have to touch to do this. How do I complete the proof for the even case below? My attempt: If we spit the problem into odd $n \times n$ cases and even $n \times  n$ cases we see the following: Odd:
We can turn every lamp on in a board by simply touching every lamp in one column. Suppose we sequentially touch every lamp in column $C1$ . This turns on all lamps in each row (and since they are not affected by consequent moves remain on). The lamps in the column will eventually all be on too because they are affected an odd number of times. Thus we have a strategy taking $n$ moves. We cannot affect all the lamps in fewer moves and this must be the best strategy for an odd $n$ . Even: I present a strategy for turning on all lamps: We can split any $n \times n$ board into blocks of $2\times 2$ . For each $2 \times 2$ if we touch each lamp in a clockwise fashion, we turn on all the lamps in that block without affecting all other lamps. We can thus use this strategy for all blocks and turn on all lamps in $n^2$ moves. Now I have a feeling we cannot beat this strategy (I get this sense because I cannot beat it in the 2x2 case) but am not sure how to prove this strategy cannot be beaten in a more general fashion.","['contest-math', 'combinatorics']"
2946528,Proving a very simple uniqueness.,"Apologizes for the sloppy proof, but this I've just started doing these in uni. I feel this is inaccurate, so can some of you help me with why it might not be good? Much appreciated. Prove there is a unique real number x such that the floor of x = the ceiling of x = 8. So I know one way of showing uniqueness is to Find at least one answer that satisfies what I'm looking for. Assume there is another distinct solution. Find a contradiction. So for this problem I wrote: When x=8, the floor of x = the ceiling of x = 8. Assume there is a second solution, x ≠ 8. 
However, (floor of x = the ceiling of x) iff x is an integer.
This means that (floor of 8 = the ceiling of 8  = 8) iff x = 7.
This contradicts the second solution, x ≠ 8. This concludes the proof.",['elementary-set-theory']
2946548,Fredholm Alternative for Singular ODE,"Consider the following inhomogeneous boundary value problem, $$t^2 u'' + tpu' +qu = f(t), \ t \in [-1,1], \ \ u(1) = \alpha, \ u(-1) = \beta,$$ where $p$ and $q$ are constants. I would like to determine a condition for the existence of a solution to this problem using the Fredholm alternative. To use it, I need to express the ODE above in self-adjoint form, $$-(a(t)u'(t))' + b(t)u = \tilde{f}.$$ However, doing so involves steps which are irreversible, i.e multiplying both sides of the differential equation by a power of $t$ , which may take the value $t = 0$ . This means that the self-adjoint equation is not equivalent to the original. Does this render the Fredholm alternative unusable? How could I determine a condition for the existence of a solution to this BVP?","['boundary-value-problem', 'self-adjoint-operators', 'ordinary-differential-equations']"
2946550,Summation of natural numbers,"How would you prove this without induction? Prove the following statement for a collection of natural numbers $$ x_1, x_2, . . . , x_n $$ and the set $$ I = \{1, 2, . . . , n\} $$ Statement : $$ (x_1 + x_2 + · · · + x_n) >
\frac{n(n + 1)}{2} → (∃i ∈ I, x_i > i) $$",['discrete-mathematics']
2946559,Dice game - how do find the expected number of rounds?,"I've been struggling trying to find an analytical solution to this problem. Let's say we have a dice game, played with n players rolling n , k sided dice, with k >= n .  The dice determine the order of winning players:  each player is assigned a number, from 1 to n, and all roll their k-sided dice simultaneously.  Winners are selected each round if the number they roll is unique amongst the n players.  If a players roll is matched by another player, then both must continue to the next round, and there can be more than one winner each round, or zero winners.  All players roll their dice even if they have already won.  All players keep rolling until all have won at least once.  The problem is to find the average and expected number of rounds each games last as a function of n and k . For example, if we had a game of 4 players, each rolling 4-sided dice, a potential game might proceed like this: Roll 1 - (1,1,3,3) : no winning players, since no player rolled a unique number Roll 2 - (1,2,2,3) : player 1 and player 4 win, since each rolled a unique number Roll 3 - (1,1,3,4) : player 3 and player 4 win Roll 4 - (1,3,3,4) : player 1 and 4 win Roll 5 - (2,3,2,2) : player 2 wins Each player has won at least once, so this game ends in 5 rounds. It would make sense that as k becomes >> n, then the expected number of rounds would converge at 1 (since the probability of there being any matching number for a large sided dice tends towards zero for a large number of sides). This is a simple problem to simulate, and running each game 1 million times shows the following result: 4 players, rolling 4-sided dice -> average number of rounds = 4.17346 4 players, rolling 10-sided dice -> average number of rounds = 1.806924 6 players, rolling 6-sided dice -> average number of rounds = 5.225997 6 players, rolling 10-sided dice -> average number of rounds = 3.043941 6 players, rolling 100-sided dice -> average number of rounds = 1.15475 This does not seem to be model-able with a Markov Chain, since the number of states is not fixed from game to game with a given n and k . NOTE: this is not homework, but I came across this game in a PDF full of dice games and haven't been able to find or work-out a solution. EDIT: with @SteveKass's suggestion, I realize that we can model this game as a Markov process, with $n+1$ states.  Each state represents the number of players who have 'won' - so state $S_0$ means no players have won, and state $S_i$ means exactly i players have won.  Thus state $S_n$ means the game is done, which is an absorbing state. So the state transition probabilities for 4 players with 4 dice from state zero can be computed: P(no progress) = P(all the same number) + P(even number of collisions) So these rolls have the form 1111, or 2222, or 1122, or 3434.  So there are 10 combinations of collision rolls:  4 where all numbers match, and 36 where two numbers appear an even number of times.  So the state transition matrix probability for S(0,0) (meaning we are at the beginning of the game, but stay in state zero because the rolls don't advance the game), is $\frac{40}{256}$ (there are ${4 \choose 1}$ ways to roll all the same number, and ${4 \choose 2}$ ways to select two numbers, and ${4 \choose 2}$ ways to distribute those numbers into rolls, hence $4+36=40$ over $4^4$ total possible rolls). However I see no generalized way of counting the number of collision possibilities.  For 4 players, it is only AAAA, AABB, or ABAB.  For 5 players, it is AABBB, or AAAAA, AAABB, or ABBAAA, etc.  For 10 players, it is any even partitioning of the rolls that results in an even number of conflicts, like AABBBBBBBB, AAAABBBBBB, AAAAAABBBB, AAAAAAAABB, AABBAAAAAA, and AAAABBAAAA, etc.  I don't know of any equation that will tell me the number of possible collision possibilities as $n$ becomes large.","['expected-value', 'dice', 'combinatorics', 'probability', 'random-variables']"
2946579,How to prove the range of $AA^T$ is the same as range of $A$?,"I have seen quite a number of questions regarding similar issues, like this and this . However, all the answers were trying to approach the topic via a non-straightforward way, that is to prove the statement by proving $N(A) = N(AA^T)$ . This method is fine and do be easy to understand. But I am actually wondering if there is a straightforward way that we can prove this? Like if we assume $x \in R(A)$ , then if we can somehow show $x \in R(AA^T)$ holds, we proved the statement. I'd like to do this but can't quite push $x \in R(A)$ towards $x \in R(AA^T)$ .","['matrices', 'linear-algebra', 'linear-transformations']"
2946583,Probability that a 5 occurs first,"Suppose we roll pair of dice until a sum of either 5 or 7 appears.
What is the probability that a sum of 5 occurs first? Try: Let $A$ be the event that a sum of $5$ occurs on the ith roll and $B$ that the sum of 7 occurs on the ith roll. We are interested on the event $A | A^c \cup B^c $ . We have $$ P(A | A^c \cup B^c) = \dfrac{ P(A \cap (A^c \cup B^c))}{P(A^c \cup B^c)} = \frac{P(A \cap B^c)}{1 - P(A \cap B)} = \frac{P(A \cap B^c)}{1-0} = P(A \cap B^c)$$ Is this approach correct so far?",['probability']
2946613,(Vishik's Normal Form) Behavior of a vector field near the boundary of a manifold,"I'm trying to prove a special case of Vishik's Normal Form. Consider $T^2 := \frac{1}{\sqrt{2}}\cdot\mathbb{T}^2 \subset \mathbb{S}^3$ , let $h: \mathbb{\mathbb{S}^3}\to \mathbb{R}$ be the function $h(x_1,x_2,x_3,x_4) = x_1^2+x_2^2 -\frac{1}{2}$ , and $\mathbb{S}^+ :=\{x \in \mathbb{S}^3; h(x)\geq 0\}$ . We say that $X$ is a vector field on $\mathbb{S}^+$ if $$X:\mathbb{S}^+ \to \mathbb{R^4} $$ is a smooth function and $X(p)$ $\in$ $T_p\mathbb{S^3} \subset \mathbb{R}^4$ , $\forall p \in \mathbb{S}^+$ . For our purposes we will always consider $X(p) \neq 0$ , $\ \forall \ p \in T^2$ . Notations: $Xh(p)= \nabla h(p) \cdot X(p)$ and $X^2 h(p)= \nabla Xh(p) \cdot X(p).$ Definition: Let $X$ be a vector field on $\mathbb{S}^+$ and $p$ $\in$ $T^2$ . If $Xh(p) = 0$ and $X^2 h(p) \neq 0$ , $p$ is called a fold point. The theorem that I am trying to demonstrate is as follows: Theorem (Vishik's Normal Form) Let $X$ be a vector field on $\mathbb{S}^+$ such that, all the points on $T^2$ that satisfy $Xh(q) =0$ are fold points. Then, if $p$ $\in T^2$ is a fold point, there exists an open set $V_p \subset \mathbb{S}^3$ and a local chart $\varphi: V_p \to U_0 \subset \mathbb{R}^3$ $(\varphi(p) = 0)$ such that $\varphi_*X|_{\varphi(V_p)}$ is a germ at $\{0\}\times \mathbb{R}^2 \cap \varphi (V_p)$ of the vector field given by: $$\left\{\begin{array}{l}
\dot{x}_1=\varepsilon x_2,\\
\dot{x}_2=1,\\
\dot{x}_3=0. \\
\end{array}\right.$$ Where $\varepsilon =\text{sgn}(X^2 h(p))$ and $\varphi^{-1}(\{0\}\times \mathbb{R}^2 \cap \varphi (V_p) )= V_p\cap T^2$ . I do not have many ideas of how to prove this theorem and I do not know any good reference. Does anyone know how to prove this theorem or can give me some hints (or references)? My ideas Note that $h$ is a function such that $0$ is regular value, then using local submersion theorem there exists a local coordinate system $\phi: V_0 \subset \mathbb{R}^3 \to U_p \subset \mathbb{S}^3$ ( $\phi(0) = p$ ), such that $h \circ \phi (x,y,z) = x$ . Defining $Y(q) = D\phi^{-1}(\phi(q)) \cdot X(\phi(q))$ , we are able to study the problem in an open neighborhood of $0$ in the topological space $\mathbb{H}^3 = \{(x,y,z); 0\leq x\}$ , with some calculation is possible to show that if $f=h\circ \phi$ , then $Y f(q) = Y_1(q)$ (where $Y=(Y_1,Y_2,Y_3))$ , implying by the definition of $X$ , that $Yf(0) =0$ and $Y^2 f(0) = Y(0) \cdot \nabla Y_1(0) \neq 0$ . Once $Yf(0,0,0) =0$ and, by hypotesis, $0\neq Y^2 f(0,0,0) = Y(0,0,0) \cdot \nabla Y_1(0,0,0)$ , we are able to conclude that either $$\frac{\partial}{\partial y} Y_1(0,0,0) \neq 0\  \text{or} \ \frac{\partial}{\partial z} Y_1(0,0,0) \neq 0,$$ assuming without loss of generality $\frac{\partial}{\partial y} Y_1(0,0,0)\neq 0$ , and using implicit function theorem, there exists $\tau: (-\varepsilon,\varepsilon)^2 \to (-\delta,\delta)$ , such that $Y(x,\tau(x,z),z) =0$ . Defining the local diffeomorphism $\psi (x,y,z) := (x,y+\tau(x,z),z)$ we can (by changing the coordinates and and shrinking the definition domain of $Y$ ) assume that $Y = (Y_1,Y_2,Y_3)$ , with $Y_1(x,0,z) =0$ (redefining $Y$ as $\psi^{-1}_* Y$ ). It is clear that $$\frac{\partial Y_1}{\partial y}(0)\neq 0 \ \text{and}\ \frac{\partial Y_1}{\partial x}(0)  = \frac{\partial Y_1}{\partial x}(0) = 0. $$ Using this question A special change of coordiantes of a Vector Field , we can make a change of coordinates $\zeta (x,y,z) = (x,\pi_{(2,3)} \circ \varphi (y,(x,0,z) )^{-1}$ , where $\varphi(t,z)$ is the solution of the ODE $$
\left\{\begin{array}{l}
\dot{x} = Y(x)\\
x(0)=z,\\
\end{array}\right.
$$ (we are able to extend $Y(x)$ to a open neighboorhood of $(0,0,0)$ ), implying that $$\zeta_* Y(x,y,z) = \text{d}\zeta (\zeta^{-1} (x)) Y(\zeta^{-1}(x))=  \left(\begin{array}{c}
Y_1\circ \zeta^{-1}(x,y,z)\\
1\\
0 \\
\end{array}\right)$$ Note that we still have $Y_1(\zeta^{-1}(x,0,z)) = Y_1(x,\pi_{(2,3)}\circ \varphi(0,(x,0,z)) = Y_1(x,0,z) =0,$ and consequently $$\frac{\partial Y_1(\zeta^{-1})}{\partial x}(0) = \frac{\partial Y_1(\zeta^{-1})}{\partial z}(0) =0 \ \text{and}\ \frac{\partial Y_1(\zeta^{-1})}{\partial y}(0)\neq 0.$$ The vector field is almost in the desired form, but I don't know how to proceed, any hints? Another thing that I noticed but didn't lead me to anywhere, is that we can define the local diffeomorphism $\theta (x,y,z) = (x, Y_1\circ \zeta^{-1}(x,y,z),z)$ and we can easily check that \begin{align*}
\theta_* ( \zeta_* Y)(x,y,z) &=  \left(\begin{array}{c}
y\\
\frac{\partial Y_1\circ \zeta^{-1} }{\partial x}\left(\theta^{-1}(x,y,z)\right) \cdot  Y_1\circ \zeta^{-1} \circ \theta^{-1} (x,y,z) + \frac{\partial Y_1\circ \zeta^{-1}}{\partial y}(\theta^{-1}(x,y,z)) \\
0 \\
\end{array}\right)\\
&=  \left(\begin{array}{c}
y\\
\nabla Y_1(\zeta^{-1})(\theta^{-1} (x,y,z)) \cdot \zeta_* Y (\theta^{-1}(x,y,z)) \\
0 \\
\end{array}\right)\\
&=  \left(\begin{array}{c}
y\\
  (\zeta_* Y)^2 \pi_1 \  (\theta^{-1}(x,y,z)) \\
0 \\
\end{array}\right)\\
&=  \left(\begin{array}{c}
y\\
  (X^2 h) \  (\theta \circ \zeta \circ \phi )^{-1}(x,y,z)) \\
0 \\
\end{array}\right)\\
\end{align*} however $\frac{\partial Y_1\circ \zeta^{-1} }{\partial x}\left(\theta^{-1}(x,y,z)\right) \cdot  Y_1\circ \zeta^{-1} \circ \theta^{-1} (x,y,z) + \frac{\partial Y_1\circ \zeta^{-1}}{\partial y}(\theta^{-1}(x,y,z))$ does not seem like a constant.","['ordinary-differential-equations', 'vector-fields', 'smooth-manifolds', 'differential-topology', 'dynamical-systems']"
2946616,Reverse Holder Inequality $\|fg\|_1\geq\| f\|_{\frac{1}{p}}\|g\|_{-\frac{1}{p-1}}$,"Let $p\in(1,\infty)$ and $(X,\mathcal{F},\mu)$ a measure space such that $\mu(X)\not=0$ . Let $f,g:X\to\mathbb{R}$ be such that $g\not=0$ a.e., $\|fg\|_1<\infty$ and $\|g\|_{-\frac{1}{p-1}}<\infty$ . Then prove: $$\|fg\|_1\geq\| f\|_{\frac{1}{p}}\|g\|_{-\frac{1}{p-1}}$$ I tried searching for ""reverse holder inequality"" and I found I thousand things that do not look similar to this. I just wrote the definitions of the norms and couldn't do nothing because the only thing I thought that I could use was Holder inequality but that is in the reverse direction of the inequality above (laugh). I would aprecciate some hint to how I should proceed.","['measure-theory', 'holder-inequality']"
2946620,"A box contains 10 balls, which are 6 W and 4 B. Assume you pick one by one, without replacement","What is the probability that only $1$ out of the first $4$ you picked are black? So it is clear that we have picked $4$ balls. Our $4$ spots can be as follows: $\mathrm{BWWW}$ $\mathrm{WBWW}$ $\mathrm{WWBW}$ $\mathrm{WWWB}$ $4$ ways to orient this. There are $6$ W balls, from those we must pick $3$ . There are $4$ B balls, from those we must pick $1$ . Thus: $$\frac{\displaystyle\binom{6}{3}\binom{4}{1}}{\displaystyle\binom{10}{4}}$$ Is this correct?","['statistics', 'probability-distributions', 'probability']"
2946664,Is the derivative of a continuously differentiable function always integrable?,"I know that $f(x)$ is bounded by the extreme value theorem, because $f(x)$ is defined on a closed bounded interval. Now, how can I prove that its derivative is integrable knowing that it is continuous? Or, is there a counterexample?","['integration', 'continuity', 'derivatives', 'analysis']"
2946683,Differentiability of $\frac{xy^2(x+iy)}{x^2+y^4}$ at 0,"This function is defined to be 0 at z=0  and the value at the rest of the points is given by that formula I tried to use the Cauchy Riemann equations. Here, $u=\frac{x^2y^2}{x^2+y^4}$ $u_x=\frac{2y^6x}{(x^2+y^4)^2}$ I'm getting $u_x=\frac{0}{0}$ at x=0, y=0. Similarly, I calculated $v_y$ and that's also 0/0 at z=0. I think I'm not doing it right. Please give me some hint.",['complex-analysis']
2946707,How to prove that two $p$-adic lattices are isomorphic?,"Let $\mathbb{Z}_{p}$ be the ring of p-adic integers.
A pair $(L,<>)$ is called lattice if $L$ be a free $\mathbb{Z}_{p}$ module of finite rank and $<>:L×L \to \mathbb{Z}_{p}$ be a nondegenerate symmetric bilinear
form on $\mathbb{Z}_{p}$ . Two lattices $L_1,L_2$ is called isomorphic if there exist  isomorphism of $\mathbb{Z}_{p}$ module $L_{1} \to L_{2}$ preserving $<>$ . Let $X_1,X_2$ be 2-adic lattices of rank 2 determined by matrices $\begin{pmatrix}0&2^k&\\2^k&0&\end{pmatrix},\begin{pmatrix}2^{k+1}&2^k&\\2^k&2^{k+1}&\end{pmatrix}$ . How to prove $X_{1}\oplus X_{1} \cong X_{2}\oplus X_{2}$ and write this isomorphism explicitly ?","['number-theory', 'p-adic-number-theory', 'integer-lattices', 'abstract-algebra', 'quadratic-forms']"
2946747,Induction proof: $n! > 2^n$ for all $n \geq 4$ [duplicate],"This question already has answers here : Prove the inequality $n! \geq 2^n$ by induction (3 answers) Closed 5 years ago . Prove: $n! > 2^n$ for all $n \geq 4$ I already proved the base case: $24 > 16$ . Then I assume this holds for $n=k$ , and start proving it for $n=k+1$ : $(k+1)k! > 2^{k+1}$ $(k+1)k! > 2^{k}(2^1)$ After this I'm not sure how to proceed. Any help is appreciated, thanks in advance.","['induction', 'proof-writing', 'discrete-mathematics']"
2946778,Why does $1^{\aleph_0}=1$ but $\aleph_0^{\aleph_0}\neq\aleph_0$?,"Note: $k$ is finite number. $1\cdot 1=1$ $1^k=\underbrace{1\cdot1\dots1}_{k}=1$ $1^{\aleph_0}=\underbrace{1\cdot1\dots1}_{\aleph_0}=1$ $\aleph_0\cdot\aleph_0=\aleph_0$ $\aleph_0^k=\underbrace{\aleph_0\cdot\aleph_0\dots\aleph_0}_{k}=\aleph_0$ $\aleph_0^{\aleph_0}=\underbrace{\aleph_0\cdot\aleph_0\dots\aleph_0}_{\aleph_0}=\aleph\neq\aleph_0$ I don't understand why statement 3 give the same result as statement 1 and statement 2 because in the infinite (like statement 3) I will expect to be different behavior like the examples i gave in statements 4, 5 and 6. I hope the question is clear.","['elementary-set-theory', 'cardinals']"
2946791,Neighborhood basis in the discrete metric space and first countability,"Consider the topological space $\mathbb{R}$ with discrete metric $$d(x,y) = \begin{cases} 1 & x\neq y\\ 0 & x=y \end{cases}$$ We know that the metric space is first countable. So for each $x\in \mathbb{R}$ , there exists a countable neighborhood basis at $x$ . My question is what is that basis? I think $\{\{x\}\}$ is a local base at each $x$ , so there is only one element in this basis? How about if I consider the open ball with $r>1$ , then the number of elements in this basis is infinite. It becomes uncountable. I am confused about how to find the neighborhood basis of $x$ .","['general-topology', 'first-countable', 'metric-spaces']"
2946796,Are the eigenvalues of the sum of two positive definite matrices increased?,"Let $A$ and $B$ be two $n \times n$ (symmetric) positive definite matrices, and denote the $k$ th smallest eigenvalue of a general $n \times n$ matrix by $\lambda_k(X)$ , $k = 1, 2, \ldots, n$ so that $$\lambda_1(X) \leq \lambda_2(X) \leq \cdots \leq \lambda_n(X).$$ I guess the following relation holds: $$\lambda_k(A + B) > \max\{\lambda_k(A), \lambda_k(B)\}, \; k = 1, 2, \ldots, n.$$ This looks intuitive but I have difficulty to prove it, any hints?","['linear-algebra', 'eigenvalues-eigenvectors']"
2946822,"If X is independent of Y and X is independent of Z, then X is independent of Y+Z","X,Y,Z are random variables, we know (Y+Z) is also a r.v.,
if X is independent of Y and X is independent of Z, is it guaranteed that X is independent of (Y+Z)? sorry that I had my probability course without much mention of measure theory, my thought is to get the p.d.f of (Y+Z)  as the marginal p.d.f of the joint p.d.f of $f_{Y+Z,Y}(u,v)$ but it doesn't work. Answers with the usage of measure theory are also fine, I would try to understand them.","['independence', 'probability-theory', 'probability', 'random-variables']"
2946885,Why does Wolfram|Alpha make a mistake here?,"We want to evaluate $$\lim_{x \to -8}\frac{\sqrt{1-x}-3}{2+\sqrt[3]{x}}.$$ The solving process can be written as follows: \begin{align*}\lim_{x \to -8}\frac{\sqrt{1-x}-3}{2+\sqrt[3]{x}}&=\lim_{x \to -8}\left[\frac{(\sqrt{1-x}-3)(\sqrt{1-x}+3)}{(2+\sqrt[3]{x})(4-2\sqrt[3]{x}+\sqrt[3]{x^2})}\cdot \frac{4-2\sqrt[3]{x}+\sqrt[3]{x^2}}{\sqrt{1-x}+3}\right]\\&=\lim_{x \to -8}\left[\frac{-(x+8)}{x+8}\cdot \frac{4-2\sqrt[3]{x}+\sqrt[3]{x^2}}{\sqrt{1-x}+3}\right]\\&=-\lim_{x \to -8} \frac{4-2\sqrt[3]{x}+\sqrt[3]{x^2}}{\sqrt{1-x}+3}\\&=-2.\end{align*} But when I input this lim\frac{\sqrt{1-x}-3}{2+\sqrt[3]{x}} as x to -8 into Wolfram|Alpha, it gives the limit $0$ . Why is Wolfram|Alpha making a mistake here?","['limits', 'calculus', 'wolfram-alpha']"
2946963,How to take the derivative of something with respect to something else?,"Just when I thought I understood taking derivatives, a textbook example of taking a derivative the equation $$[x(t)]^2+4000^2=[s(t)]^2$$ with respect to time shows that the derivative is $$x\frac{dx}{dt} = s\frac{ds}{dt}$$ However, after  applying the power rule and the chain rule, I come up with $$2x(t)\cdot x'(t)=2s(t)\cdot s'(t)$$ .
What concept am I missing about taking the derivative with respect to time? To elaborate a bit, I'm confused as to where the $\mathbf{x}$ and the $\mathbf{s}$ come from in $$\mathbf{x}\frac{dx}{dt} = \mathbf{s}\frac{ds}{dt}$$ from the example. Furthermore, to me it seems like they forgot to apply the power rule. Example: An Airplane Flying at a Constant Elevation https://cnx.org/contents/ [email protected] :74vQD30u@6/Related-Rates","['calculus', 'derivatives']"
2946992,Integer midpoint,"Two points $(c, d, e),(x, y, z) \in \mathbb{R}^3$ , we say that the midpoint of these two points is the point with coordinates $\left(\frac{c+x}{2}
,
\frac{d+y}{2}
,
\frac{e+z}{2}\right)$ Take any set $S$ of nine points from $\mathbb{R}^3$ with integer coordinates. Prove that there must be at least one pair of points
  in $S$ whose midpoint also has integer coordinates. I've tried to do an example with set $$S=\{(2,2,2),(1,8,2),(3,4,5),(5,2,2),(4,2,9),\\(2,1,4),(6,8,2),(0,0,0),(5,2,3)\}$$ So taking $2$ points, $(3,4,5)$ and $(5,2,3)$ so $(3+5)/2=4$ , $(4+2)/2=3$ , $(5+3)/2 = 4$ , which are integers. I'm wanting this argument to hold in general and I'm finding it tricky to prove this does anyone have suggestions would be grateful!","['combinatorics', 'discrete-mathematics']"
2947034,How can I derive this fast-converging series formula for $\zeta(4)$?,"Let $\zeta(n)$ denote the Riemann Zeta function for positive integers $n>1$ as usual by: $$
\zeta(n)=\sum_{m=1}^{\infty}m^{-n}.
$$ There are fast-converging series for $\zeta(2)$ and $\zeta(3)$ , but not others. In the spirit of Apéry's $$
{\displaystyle {\begin{aligned}\zeta (3)&={\frac {5}{2}}\sum _{k=1}^{\infty }{\frac {(-1)^{k-1}}{{\binom {2k}{k}}k^{3}}}\end{aligned}}},
$$ quick numerical computations show that Conjecture 1. $$ {\displaystyle {\begin{aligned}\zeta (4)&={\frac {36}{17}}\sum _{k=1}^{\infty }{\frac {1}{{\binom
{2k}{k}}k^{4}}}\end{aligned}}}.$$ However, I am unable to prove this statement. I have tried using $$
2(\sin^{-1}x)^2 =\sum_{k=1}^{\infty}{\frac{(2x)^{2k}}{{\binom {2k}{k}}k^{2}}},
$$ but to no avail. Any help is appreciated.","['riemann-zeta', 'sequences-and-series']"
2947048,Is Dirichlet's theorem on arithmetic progressions true for ring of Gaussian integers?,"Is Dirichlet's theorem on arithmetic progressions true for ring of Gaussian integers $\mathbb{Z}[i]$ ? By Dirichlet's theorem I mean the fact that if some line $an+b$ ( $a, b\in Q$ and $n\in\mathbb{Z}$ is variable) contains two different primes in the ring $Q$ , then there are infinitely many primes on it.","['number-theory', 'prime-numbers']"
2947056,Decrease in algebraic degree on multiplying a real number with a root of unity,"Let $\omega$ be a $\textbf{root of unity}$ with algebraic degree(degree of its minimal polynomial over $\mathbb{Q}$ ) $d_1$ over $\mathbb{Q}$ and $r$ be a $\textbf{real number}$ with algebraic degree $d_2$ over $\mathbb{Q}$ . Can $r\omega$ have algebraic degree $= d_3$ strictly less than $d_1$ over $\mathbb{Q}$ ? Is there an explicit non-trivial lower bound for $d_3$ in terms of $d_1$ and $d_2$ ? Note that if both multiplicands were real numbers then decrease in algebraic degree over $\mathbb{Q}$ is possible, example: $a, b = \sqrt{2}$ . Similarly, if both multiplicands were roots of unity then decrease in algebraic degree over $\mathbb{Q}$ is possible, example: $a, b = i$ .","['galois-theory', 'number-theory', 'galois-extensions', 'algebraic-number-theory']"
2947072,On the functional equation of the theta function,"Given $\tau$ in the upper half-plane, we define its theta function as $$ \theta(z) := \sum_{n \in \mathbb{Z}} e^{\pi i (n^2\tau + 2nz)}.$$ This is an entire function. It is well-known that the theta function satisfies the functional equation $$ \theta(z+\tau)=e^{-\pi i (\tau+2z)}\theta(z).$$ I am trying to show this, but I start from $$ \theta(z+\tau) = \sum_{n \in \mathbb{Z}} e^{\pi i (n^2\tau + 2nz)} e^{\pi i 2n\tau} $$ and then I have no idea how to proceed. Developing the exponential into a series doesn't look good, it only makes things messier. Any hint?",['complex-analysis']
2947150,Why is there no derivative in an absolute value function?,"For example the function $f(x)=|x|$ , the graphic will be something like the letter ""V"". The function is continuous in $x=0$ . However, the slope is different for $x < 0$ than for $x>0$ , just like in quadratic functions, so I don’t see how that explains it. Can’t I draw a tangent line to the graph in $x=0$ which coincides with the $x$ axis? If i am able to do that, why isn’t the derivative (in $x=0$ ) equal to zero as in quadratic functions?","['calculus', 'derivatives']"
2947211,"Using this definition of ordinals, do I need foundation?","Definition. A set is an ordinal if it is a transitive set of transitive sets. This is the simplest definition of an ordinal I have ever encountered, and I happen to like it a lot for this reason. However, it seems to have one downfall: I am having a lot of trouble showing that the class of ordinals is well-ordered without using the axiom of regularity (foundation). Do I need to use regularity to show this? Are there models of $ZFC-(\text{regularity})$ in which the transitive sets of transitive sets are not well-founded?","['elementary-set-theory', 'foundations']"
2947218,Question on the notation $ds^2=dx^2+dy^2$ : what does it really mean?,"I know that if a curve $C$ is parametrize as $$s(t)=(x(t),y(t)),\quad  t\in [a,b]$$ then $$\ell(C)=\int ds=\int_a^b \sqrt{\dot x^2(t)+\dot y^2(t)}dt.\tag{*}$$ In the french wikipedia , it's written that $(*)$ can be written as $$ds^2=dx^2+dy^2.$$ I really don't understand the logical. Could someone explain ? What is the link between $(*)$ and $ds^2=dx^2+dy^2$ ? I tried $$ds^2=dx^2+dy^2\implies ds=\sqrt{dx^2+dy^2}\implies \int ds=\int\sqrt{dx^2+dy^2},$$ but I can't give an interpretation of the RHS... So it should be wrong.","['differential-geometry', 'real-analysis']"
2947248,Why isn't this problem in Real and Complex Analysis really easy?,"In Real and Complex Analysis, Walter Rudin proposes the following problem: Let $\{f_n\}$ be a sequence of continuous functions in $[0,1]$ with $0\leq f_n\leq 1$ and such that for all $x\in [0,1]$ , $f_n(x)\to 0$ . Show that $$\lim_{n\to\infty}\int_0^1 f_n(x)\:\mathrm{d}x=0.$$ Also try to prove this without using anything from Lebesgue's theory. (This exercice is meant to show how powerful Lebesgue's theory is.) (This is my translation since I own the french version of the book.) Since Rudin does this remark in the exercice, it seems to me that this ought to be a hard problem to do without Lebesgue's machinery. However, I thought about the following solution: Since $[0,1]$ is compact and $f_n\to 0$ pointwise, we have that $f_n\to 0$ uniformly. Then $$\lim_{n\to\infty}\int_0^1 f_n(x)\:\mathrm{d}x=\int_0^1\lim_{n\to\infty}f_n(x)\:\mathrm{d}x=0.$$ Is this wrong? If so, why?","['analysis', 'real-analysis']"
2947253,State-space representation of a nonlinear MIMO system,"Question: Obtain a state-space representation of nonlinear multiple-input multiple-output (MIMO) system: $$\dddot{y}_1 + 2\dot{y_1} + 3y_2 + 2 = u_1 y_2 \tag{1}$$ $$\ddot{y}_2 - 2 \dot{y}_2 + \dot{y}_1^3 + y_2 + y_1 = (u_2 - u_3)y_1 \tag{2}$$ I find it difficult solving the above equations. I have the following queries: What do I do with $(dy_1/dt)^3$ ? How do I represent it in state space model? Are $u_1, u_2$ and $u_3$ control inputs or just constants (coefficients of $y_1$ and $y_2$ )? Does the constant $2$ in equation come in $\mathbf B$ (i.e., $\mathbf A x + \mathbf B u)$ ? Do I have to convert these equations into linear equations?","['nonlinear-system', 'control-theory', 'ordinary-differential-equations']"
2947286,Solving a certain system of differential equations,"Let us consider the system of differential equations: $$x'(t)= y(t)^2, y'(t) = x(t)^2, x(0) = a, y(0)=b, a, b \in \mathbb{R}.$$ How would one go about solving this system of differential equations?",['ordinary-differential-equations']
2947299,Proving ${\frac{n+2}{2n+3}} $ converges to $\frac{1}{2}$,"I'd just like to verify whether I've done it correctly. Proof strategy. First we determine what we need to set $N$ equal to; $$\left|\frac{n+2}{2n+3}-\frac12\right|=\left|\frac{2n+4-(2n+3)}{4n+6}\right|= \frac1{4n+6}<\epsilon$$ Some rewriting get us, $$n>\frac{1}{4\epsilon}-\frac{6}4{}$$ When $\epsilon$ is $\frac{1}{6}$ $N$ will be 0 but this is not allowed since N should be a positive integer. We notice: $$n>\frac1{4\epsilon}>\frac1{4\epsilon} -\frac64$$ Hence choose $N=\lceil\frac1{4\epsilon}\rceil$ Proof: Let $\epsilon>0$ and choose $N=\lceil1/4\epsilon\rceil$ . Let $n>N$ where $n \in \mathbb{Z}$ . Then $n>\frac1{4\epsilon}>\frac1{4\epsilon}-\frac64$ . Thus, rewrite to get, $ \frac{1}{4n+6} < \epsilon$ Therefore, $$\left|\frac{n+2}{2n+3}-\frac12\right|=\left|\frac{2n+4-(2n+3)}{4n+6}\right|=\frac1{4n+6}<\epsilon$$ Hence the sequence converges to $\frac{1}{2}$ . $$\space \blacksquare$$","['proof-verification', 'real-analysis']"
2947306,Proof that CDF is continuous for continuous random variables.,"I have to show that, For any continuous random variable $X$ , the cumulative distribution function(CDF) $F : \mathbb{R} \to [0,1]$ is continuous. My attempt Assume $F$ has a discontinuous point $x \in \mathbb{R}$ . Since CDF should be non-decreasing, i.e. monotone increasing, so it follows that $f(x-) < f(x+)$ . Since CDF is right-continuous, $f(x)=f(x+)$ , thus $X$ has point mass of size $f(x+) - f(x-)$ at the point $x$ . This contradicts the fact that $P(X=x) = 0$ for any $x \in \mathbb{R}$ , because $X$ : continuous random variable. Is my proof OK?","['probability-theory', 'probability']"
2947309,A probability measure is nonatomic iff its distribution function is continuous,"Given the probability space $(\Omega, \mathcal{F}, \mathbb{P})$ and the r.v. $X$ defined on it, the (pushforward) measure $\mu_X$ is defined as $\mu_X(B) := PX^{-1}(B), \forall B \in \mathcal{B}(\mathbb{R})$ , and the distribution function $F_X(x)$ is defined as $F_X(x) := \mu_X((-\infty, x])$ . $\mu_X$ is nonatomic if $$\forall A_1 \in \mathcal{B}(\mathbb{R}): \mu_X(A_1)>0 \rightarrow \exists A_2\in \mathcal{B}(\mathbb{R}) \quad \& \quad A_2\subset A_1: \mu_X(A_2)>0.$$ If such a set $A_2$ does not exists, $A_1$ is an atom of $\mu_X$ . There is also this fact that $F_X$ is continuous iff $\mu_X(\{x\}) = 0, \forall x \in \mathbb{R}.$ To prove ""if $F_X$ is continuous $\rightarrow$ $\mu_X$ is nonatomic"", this is how I proceed: Suppose not:  there exists $A \in \mathcal{B}(\mathbb{R})$ , an atom of $\mu_X$ with $c := \mu_X(A) > 0$ . Restricting $\mu_X$ to $A$ , we can consider the measurable space $(A, \mathcal{B}(\mathbb{R})_A)$ . Let $F(x) = \mu_X((-\infty, x]) \in [0, c]$ . $F$ is a distribution function and hence is non-decreasing, $F(x) \downarrow 0$ as $x \rightarrow -\infty$ , and $F(x) \uparrow c$ as $x \rightarrow \infty$ . Continuity of F implies $F(x) \in (0, c)$ for some $x \in \mathbb{R}$ , then the set $B = (-\infty, x]$ gives a contradiction to the fact that $A$ is an atom. Questions: 1- Is there anything about this proof not rigorous enough that needs modification? 2- How can I prove the converse of the claim? Any other proof strategy is appreciated as well.","['measure-theory', 'probability-distributions', 'probability-theory']"
