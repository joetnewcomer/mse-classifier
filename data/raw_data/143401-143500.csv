question_id,title,body,tags
2330803,Why is $x^x$ only defined for $x>0$,Consider the real function $x^x$. I understand that $0^0$ is undefined so $x \neq 0$ but $x$ values like $-1$ and $-2$ have well defined function values (although curiously opposite sign). Why isn't the curve well defined for $x<0$? Note: the domain that I would like to investigate this function for is x is an element of the real numbers,"['functions', 'limits']"
2330825,"Let $A \in \Bbb M_3(\Bbb R)$ be such that $A^4= I, A \neq \pm I.$ Then is it true that $A^2 + I=0$?","Let $\lambda$ be an eigenvalue of $A$ . Then $\lambda^4$ is an eigenvalue of $A^4$ . Let $v$ be the corresponding eigenvector of $\lambda^4$ . Then $A^4 v = \lambda^4 v \Rightarrow Iv=\lambda^4 v \Rightarrow \lambda^4=1 \Rightarrow \lambda= \pm 1,\pm i$ . Since $A$ is a real matrix, if $i$ (or $-i$ ) is it's eigenvalue, then $-i$ (respectively $i$ ) is also it's eigenvalue. Also $A \neq \pm I$ . Therefore possible characteristic polynomials of $A$ are $(x-1)(x+1)^2,(x+1)(x-1)^2,(x-1)(x^2+1),(x+1)(x^2+1)$ . Now if possible, $x^2+1$ will be a minimal polynomial of $A$ for the characteristic polynomials $(x-1)(x^2+1),(x+1)(x^2+1)$ only. But any such minimal polynomial must have factors $(x-1)$ and $(x+1)$ respectively. Hence $A^2+ I=0$ is not possible. Are there any mistakes in my proof? I am learning to handle proofs containing characteristic polynomials, minimal polynomials, eigenvalues etc. Thanks.","['matrices', 'contest-math', 'linear-algebra', 'proof-verification']"
2330861,Projection of oblique segment onto another,"How can I find the length of segment a , given the length of segment b , and given that the angle between a and b is x ? Alternatively, how can I find the length of segment a , given r and angle y ? OBS: segments r and b are perpendicular.",['geometry']
2330869,Condition on a matrix so that its determinant is $2$,"Is the following statement true or false? There exists $A \in M_{3,3}(\mathbb{Z})$ with determinant $2$ such that $$A\begin{pmatrix}2\\1\\4\end{pmatrix} = \begin{pmatrix}4\\-8\\16\end{pmatrix}$$ I first thought of eigenvalues but it doesn't look like the second vector is a multiple of the first. What are the conditions on $A$ so that it's ""true""? Or is it always false?","['matrices', 'diophantine-equations']"
2330879,Use of Extended Cauchy Integral Formula,"I am having some trouble with the following question: Show
$$\int_C \frac{2+3\sin(\pi z)}{z(z-1)^2}\mathrm dz = -6\pi^2 i $$
with $C$ the circle of radius $6$, centre $0$, positively oriented. I am assuming I use Cauchy's extended integration formula which states: $$\int_C f(z) \mathrm dz = 2\pi i \sum_{k=1}^n \operatorname{Res}_{z=z_k}  f(z)$$ However I am not sure what I should use as my $f(z)$. If for one part $$f(z) = \frac{2+3\sin(\pi z)}{z} = \frac{1}{z} [2 + 3(1 - \frac{(\pi z)^3}{3!}+...)]$$ I have calculated already that the residue at $z = 0$ is equal to $5$. I am not sure if my $f(z)$ should just be the original function though, or how I incorporate the $z = 1$ singularity.","['cauchy-integral-formula', 'complex-analysis']"
2330898,Philosophy behind Riesz–Markov–Kakutani representation theorem,"The Riesz–Markov–Kakutani representation theorem tells us that, if $X$ is a locally-compact Hausdorff space, then monotone linear functionals $\mathcal{C}_{\mathrm{cs}}(X) \rightarrow \mathbb{R}$ are the same thing as regular measures on $X$. I don't know about you, but the correctness of this theorem kind of messes with some broad intuitions I have about these kinds of things. In particular, the Riesz–Markov–Kakutani representation theorem violates the pseudotheorem that being finitely-linear isn't enough to establish being countably-linear in any sense of the word. Question. Can anyone offer any intuition or justification for why it's reasonable to expect this pseudotheorem to fail here?","['functional-analysis', 'riesz-representation-theorem', 'measure-theory']"
2330907,"Proof of property of normals to a star shaped region (""elementary"" vector calculus lemma in Evans' PDE)","I cannot follow the proof of the following Lemma (Evans's PDE , p. 515). Lemma. If $U\subset\mathbb{R}^n$ is an open star-shaped region with $\partial U\in C^1$, then $x\cdot\nu(x)\ge0$ $\forall x\in \partial U$, where $\nu$ denotes the outward normal. Proof. We assume $U$ is star-shaped with respect to the origin $0$.
Since $\partial U$ is $C^1$, if $x\in\partial U$ then for each $\varepsilon >0$ there exists $\delta >0$ such that $|y-x|<\delta$ and $y\in\bar{U}$ imply 
$$\nu(x)\cdot\frac{y-x}{|y-x|}\le\varepsilon.$$ Why is that? In particular $$\limsup_{y \to x, y \in \overline{U}} \nu(x) \cdot \frac{y-x}{|y-x|} \le 0.$$ Let $y = \lambda x$ for $0 < \lambda < 1$. Then $y \in \overline{U}$ since $U$ is star-shaped  with respect to the origin $0$. Therefore $$\nu(x)\cdot\frac{x}{|x|} = - \lim_{\lambda \to 1^-} \nu(x) \cdot\frac{\lambda x-x}{|\lambda x-x|}.$$ Why is that? But we have shown that the last term is $\ge 0$, hence we are done.","['real-analysis', 'partial-differential-equations', 'calculus', 'multivariable-calculus', 'differential-geometry']"
2330936,Looking for a rigorous analysis book,"I'm a mathematics undergrad student who finished his first university year succesfully. I got courses of calculus, but these weren't very rigorous. I did learn about stuff like epsilon and delta proofs but we never made exercises on those things. The theory I saw contained proofs but the main goal of the course was to succesfully learn to solve integrals (line integrals, surface integrals, double integrals, volume integrals, ...), solve differential equations, etc. I already took proof based courses like linear algebra and group theory, so I think I am ready to start to learn rigorous real analysis, so I'm looking for a book that suits me. I want the book to contain the following topics: The usual analysis stuff: a construction of $\mathbb{R}$ or a system that takes $\mathbb{R}$ axiomatically for granted rigorous treatment of limits, sequences, derivatives, series, integrals the book can be about single variable analysis, but this is no requirement exercises to practice (I want certainly be able to prove things using epsilon and delta definitions after reading and working through the book) Other requirements: The book must be suited for self study (I have 3 months until the next school year starts, and I want to be able to prepare for the analysis courses). I have heard about the books 'Real numbers and real analysis' by Ethan D. Block and 'Principles of mathematical analysis' by Walter Rudin, and those seem to be good books. Can someone hint me towards a good book? If you want me to add information, feel free to leave a comment.","['reference-request', 'real-analysis', 'book-recommendation']"
2330977,"Canonical morphism $H^{n}(Y,f_* \mathscr{F}) \to H^{n}(X, \mathscr{F})$ in sheaf cohomology.","Given a scheme morphism $f:X \to Y$ and a sheaf of abelian groups $\mathscr{F}$ on $X$ we can consider an injective resolution of this sheaf by a complex $I^{\bullet}$ namely an exact sequence, $$0 \to \mathscr{F} \to I^0 \to I^1 \to I^2 \to \cdots $$ with each $I^{n}$ an injective sheaf. Then since $f^{-1}$ is an exact functor between the categories of abelian sheaves on $Y$ and the category of abelian sheaves of $X$ we obtain that $f_* I^{n}$ is also injective for every $n$. Then if we denote by $J^{\bullet}$ an injective resolution of $f_* \mathscr{F}$ we can obtain by basic homological algebra ( dual to Weibel Theorem 2.2.6) a morphism of resolutions $f_* I^{\bullet} \to J^{\bullet}$ where $f_* I^\bullet$ denotes the pushfoward of the complex $I^\bullet$. Then it is clear that if we take global sections and compute cohomology we obtain the desired canonical homomorphism since it is easy to show that does not depend on the injective resolution taken. My question is: How can we define this canonical map when we are working with $\mathscr{F}$ a quasi-coherent sheaf on $X$? We will assume that $f_* \mathscr{F}$ is also quasi-coherent (for example if $f$ is quasi-compact and quasi-separated). In this case we need to work with $f^{*}$ insted of $f^{-1}$ and I cannot assure that $f_* I^{n}$ are going to be injective $\mathcal{O}_Y$ modules.","['sheaf-cohomology', 'algebraic-geometry']"
2330997,How can I evaluate $ \int{\sqrt{1-k \cos{x}} dx} $?,"When $ k = 1 $ the solution is simple to find. But how can I do when $ k \neq 1 $? With the aid of WolframAlpha I realized that this primitive involves the E elliptic integral (i.e. second kind), but how can I reduce my problem to this?
Thanks.","['indefinite-integrals', 'calculus', 'analysis']"
2331069,"How many solutions are there in equation $x_1+x_2+\cdots+x_{10}=15\space, \space x_1\geq 1\space ,\space 0\leq x_i\leq9$ for $i=1$ to $10$","Question How many solutions are there in equation $$x_1+x_2+\cdots+x_{10}=15\space, \space x_1\geq 1\space ,\space 0\leq x_i\leq9$$ for $i=1$ to $10$ My Approach $$x_1+x_2+\cdots+x_{10}=15\space, \space x_1\geq 1\space ,\space 0\leq x_i\leq9$$ Let us find the restriction for $x_{1}\geq 1$ Let $x_{1}=x_{1}^{'}+1$ So our solution boils down to $$x_1^{'}+1+x_2+\cdots+x_{10}=15\space$$ $$x_1^{'}+x_2+\cdots+x_{10}=14\space$$ Number of Solution = $$\binom{10+14-1}{14}=\binom{23}{14}$$ Among $$\binom{23}{5},$$ we have to subtract the restriction for $0 \leq x_{i} \leq 9$ if we take $i=2$ ,i.e $0 \leq x_{i} \leq 9$ i.e Subtract $$x_{2}\geq10$$ from $$\binom{23}{5},$$ calculating for restriction $$x_{2}\geq10$$ using the above approach, we get $$x_1+x_2^{'}+\cdots+x_{10}=5\space$$ = $$\binom{10+5-1}{5}=\binom{14}{5}$$ We will find the restriction for every $i=1$ to $10$ and then subtract it from $$\binom{23}{5},$$ Hence required number of solution= $$\binom{23}{5}-10*\binom{14}{5}$$ But the answer says that it is $${{23}\choose{9}} - {{14}\choose{9}} - 9{{13}\choose{9}}$$ I am confused.Please help me out where i am getting wrong ??","['combinatorics', 'discrete-mathematics']"
2331080,How many five digit number can be formed such that their numbers counted from left to right creates a decreasing sequence?,"How many five digit number can be formed such that their numbers counted from left to right creates a decreasing sequence? Numbers $= {0,1,2,3,4,5,6,7,8,9}$ Example: $54321$ and $96310$ If we would count the amount of five digit numbers that can be formed such that their number counted from left to right is INCREASING we would get: $\binom{9}{5}=126$ Since the amount of distinct set can be sorted so $n_1>n_2>n_3>n_4>n_5$
Were $n_1$ is the digit with the highest value. My attempt at the real question in hand is the following: We sort every digit so that $n_1<n_2<n_3<n_4<n_5$ We would now get $\binom{10}{5}=252$ decreasing five digit number.
This is clearly wrong since I don't account for the zero.
I need to divide this problem into different parts in some way, any got any suggestions? 
Thanks beforehand!",['combinatorics']
2331084,Real Numbers are Uncountable/Set-Theoretical Approach: Proof 2,"https://proofwiki.org/wiki/Real_Numbers_are_Uncountable/Set-Theoretical_Approach:_Proof_2 How do we know it's a great idea to use the characteristic function rather than every other function that exists? If the sequence $d_i$ does not terminate in an infinite sequence of 1s, does that mean $d_i$ terminates in an infinite sequence of 0s? I disagree that there are $2$ cases, I think either the sequences terminates in infinite sequence of 0s, 1s, or doesn't terminate infinitely in 1 or 0.","['real-numbers', 'cardinals', 'elementary-set-theory', 'proof-explanation']"
2331138,$\lim_{m \to \infty}{\prod_{i=0}^{tm-1}(1+\frac{r(i)}{m})} = e^{\int_{0}^{t} r(s)ds}$?,"How can I deduce the following equation ?: $$
\lim_{m \to \infty}\prod_{i = 0}^{{\large tm - 1}}
\left[\,1 + \frac{r\left(\,i\,\right)}{m}\,\right] =
\exp\left(\,\int_{0}^{t}r\left(\,s\,\right)\,\mathrm{d}s\,\right) $$ where $t > 0$ and $r\left(\,i\,\right)$ a real valued function. I can figure out the series expansion of $\mathrm{e}$, and it makes somehow sense as you can think of $r$ as the average ""rate of interest"" for example, but what exactly is the math behind it ?.","['exponential-function', 'analysis']"
2331159,Example of an operator with purely residual spectrum,Do you know an example of a linear bounded operator acting on a Banach (or even Hilbert) space whose residual spectrum is non-empty but the point and continuous spectrum are empty?,"['functional-analysis', 'spectral-theory']"
2331163,Derivative of absolute function,"In the video lecture that I am watching, the instructor compared an example of finding the derivative of the $f(x) = |x|$ vs $f(x) = x|x|$, where $f(x) = x|x|$, derivative is $0$, whereas $f(x) = |x|$, does not have a derivative. The instructor highlighted that the absolute value function does not have a derivative compared to $f(x) = x|x|$. If I would to apply the power rule: $f(x) = nx^{n-1}$, where $f(0) = 1*0^{1-1}$. Because 0 to the power of 0 is undefined. Is it because $0$ to the power of $0$ is undefined, therefore the absolute value function does not has a derivative?","['derivatives', 'calculus']"
2331204,Question in proof about subgroups of cyclic groups,"I'm studying the proof of the theorem that says: A subgroup of a cyclic group is cyclic The proof goes as follows. Let $G = \langle g \rangle$ be a cyclic group. Let $H$ be a subgroup of $G$.
For $H = \{e\}$, the statement is trivial. So suppose $H \neq \{e\}$ en let $g^k \in H$ with $k$ minimal in $\mathbb{N_0}$ (so $k$ is the first non zero natural number such that $g^k \in H$). We now show that $H = \langle g^k \rangle$, from which the result follows. Let $g^n \in H$. Write $n = qk+r$ with $0 \leq r <k$. Then, $g^r = g^{n-qk} = g^n(g^k)^{-q} \in H$, from which must follow that $r = 0$ (because we took $k$ minimal and $r < k$). So $g^n \in \langle g^k \rangle$, hence $H = \langle g^k \rangle$. Now, I'm wondering why the proof doesn't work if $H = \{e\}$ if I would not treat the case $H = \{e\}$ separately. I think it is because the proof would fail if $G$ is generated by an element of infinite order, so we cannot take $g^k \in H$ with $k$ minimal. Can someone verify whether this is correct? Thanks in advance.","['abstract-algebra', 'group-theory', 'cyclic-groups']"
2331206,Computing fiber of morphism between affine schemes,"I found these examples in Eisenbud-Harris' Geometry of Schemes and I am rather stuck understanding the algebra machinery under them. Suppose $k$ is an algebraically closed field. Let $X=\mathrm{Spec}(k[x,u]/(xu))$ be the union of two intersecting lines in the affine plane and let $\varphi:X\longrightarrow \mathrm{Spec}(k[t])$ be the morphism induced by $k[t]\longrightarrow k[x,u]/(xu),t\mapsto x+u+(xu)$. I want to show that the fibre of $\varphi$ above $p=(t-a)\in \mathrm{Spec}(k[t])$ consists of two distinct points (i.e. $\mathrm{Spec}(K\times K)$) if $a\neq 0$ and a single double point if $a=0$, i.e. the scheme $\mathrm{Spec}(K[t]/(t^2)$). I don't want to use the definition via fiber products, as in the book this is given later and at this stage the only definition it provides is ""scheme-theoretic preimage"" of a morphism $\varphi : \mathrm{Spec}(R)\longrightarrow \mathrm{Spec}(S)$ at a point $\mathfrak p\in \mathrm{Spec}(S)$ as the affine scheme $\mathrm{Spec}(R/e(\mathfrak p))$ where $e(\mathfrak p)$ is the extended ideal via the ring morphism $S\longrightarrow R$ associated with $\varphi$. My attempt: Here we have $\mathfrak p =(t-a)$ and the first problem is calculating the extension. The generic element in $e(t-a)$ should be
$$\sum_{i}(f_i+(xu))\cdot \varphi((t-a)\cdot g)=(x+u-a)\cdot g(x+u)\cdot \sum_i f_i +(xu)$$
where $g\in k[t],f_i\in k[x,u]$. If I am correct, then $e(t-a)=(x+u-a)$ in the quotient ring $k[x,u]/(xu)$. I'm not really convinced of that but I can't fault it. It follows then that
$$\varphi^{-1}(\mathfrak p)=\mathrm{Spec}\left(\frac{k[x,u]/(xu)}{(u+x-a)/(xu)}\right)$$
as sets. But then
$$\frac{k[x,u]/(xu)}{(x+u-a)/(xu)}\simeq k[x,u]/(x+u-a)$$ I'm stuck from here and I am neither very convinced about the above. Could you please help me?","['schemes', 'affine-schemes', 'algebraic-geometry', 'commutative-algebra']"
2331215,"If $(x^2-4x+7)^3+p(x^2-4x+7)^2+q(x^2-4x+7)+r=0$ has no real roots, Then $p+2q+r$ is","If $f(x)=x^3+px^2+qx+r$ has three distinct non negative integer roots and $(x^2-4x+7)^3+p(x^2-4x+7)^2+q(x^2-4x+7)+r=0$ has no real roots, Then $p+2q+r$ is $\bf{Attempt:}$ Assuming $f(x) = x^3+px^2+qx+r = (x-a)(x-b)(x-c)$ , where $a,b,c \geq 0$ and $a,b,c \in \mathbb{Z}$ Then $f(x^2-4x+7) = (x^2-4x+7)^3+p(x^2-4x+7)^2+q(x^2-4x+7)+r $ $=(x^2-4x+7-a)(x^2-4x+7-b)(x^2-4x+7-c)$ Could some help me how to solve it , thanks",['algebra-precalculus']
2331231,How to construct the forms of $\mathbb{G}_m$?,"This is a sort of sequel to a question I asked before: Forms of the multiplicative group . My question is how to construct the forms of $\mathbb{G}_m$ using Galois cohomology. If $k$ is a field (assume it's of characteristic 0), and $X$ is a $k$-scheme, a form of $X$ is defined to be a $k$-scheme $X'$ such that $X'_{\overline{k}}$ is isomorphic to $X_{\overline{k}}$. Let $\Gamma=Gal(\overline{k}/k)$. Then there is a natural bijection between isomorphism classes of forms of $X$ with $H^1(\Gamma,Aut_{\overline{k}}(X))$. I want to understand both directions of the correspondence and how it works for $X=\mathbb{G}_m$, and how to construct explicitly the forms of $\mathbb{G}_m$. I understand how the map works in one direction. Given a form $X'$ with an isomorphism $\phi:X_{\overline{k}} \rightarrow X'_{\overline{k}}$, we want to define a cocycle from $\Gamma$ to $Aut_{\overline{k}}(X)$. Let $\sigma \in \Gamma$ be given. Then $\sigma$ induces another isomorphism $\sigma \phi:X_{\overline{k}} \rightarrow X'_{\overline{k}}$. To get a cocycle, send $\sigma$ to $\phi^{-1} \circ (\sigma \phi)$. (Here I'm thinking of $\sigma \phi$ as ""upstairs"" and I'm starting from the lower left and going around clockwise.) I don't know how the reverse direction goes, though: given a cocycle, how do you get a form? How does it work for $\mathbb{G}_m$? Looking back at the question I asked before (the link is above), I still don't see how one gets $T_a$. Basically, it seems that there should be algorithm to get the forms but I don't know how it works for this example, which should be the simplest case (non-split rank one tori)","['algebraic-groups', 'algebraic-geometry']"
2331255,Killing Fields on Euclidean Spaces,"Let $K$ be a set of all Killing vector fields on $\mathbf R^n$ (with the Euclidean metric $\bar g$) which vanish at the origin. (A vector field $V$ on a Riemannian manifold $(M, g)$ is said to be a Killing vector field if the flow of $V$ acts by isometries of $M$. This is equivalent to saying that $\mathcal L_Vg=0$). If $V\in K$, then by using $\mathcal L_V\bar g=0$, we get that the matrix $[\partial V^i/\partial x^j]$ is anti-symmetric, where $V^i$ are the components of $V$ in the standard coordinates. Define a map $T:K\to \mathfrak o(n)$ as $$T(V)= \left[\frac{\partial V^i}{\partial x^j}(0)\right]$$
where $\mathfrak o(n)$ is the Lie algebra of $O(n)$, which is ""same"" as the space of $n\times n$ real anti-symmetric matrices. Problem. To show that $T$ is injective. I am quite lost here.","['riemannian-geometry', 'differential-geometry']"
2331258,Is Frobenius' method generally useful?,"Regarding the differential equation
$$ y'' + p(z)y' + q(z)y = 0,\quad z\in\mathbb{C}, $$
we can find solutions of the form 
$$ \sum_{n=0}^\infty c_n (z-z_0)^n, \quad c_n\in\mathbb{C}, $$
given that $p(z)$ and $q(z)$ are analytic in $z=z_0$. Here $|z-z_0|<R_1$ for some $R_1>0$ must hold. Suppose $p(z)$ and $q(z)$ aren't analytic at $z=z_0$, but $(z-z_0)p(z)$ and $(z-z_0)^2 q(z)$ are, then we can find solutions of the form 
$$ \sum_{n=0}^\infty c_n (z-z_0)^{n+r}, \quad c_n\in\mathbb{C}, $$
where $r$ satisfies $r(r-1)+[zp(z)]_{z=z_0}r+[z^2q(z)]_{z=z_0}=0$. Here $|z-z_0|<R_2$ for some $R_2>0$ must hold. The latter is Frobenius' method. Now to illustrate my question suppose $z_0=0$ is a singular point of $p(z)$ but not of $zp(z)$ so we would apply Frobenius' method. Then suppose it turns out that $R_2$ is really small. 
In my understanding we could also have looked for solutions around some analytic point. Suppose $z_0=2$ is analytic, then we would've looked for solutions of the form $\sum_{n=0}^\infty c_n (z-2)^n$ and then maybe the corresponding $R_1$ would have turned out to be more satisfactory. So if what I said in the above is right, my question is: can we know beforehand if applying Frobenius' method is useful?","['complex-analysis', 'frobenius-method', 'ordinary-differential-equations']"
2331310,Tricky inequality on norms of vectors,"Let $v_0=(v_{0x},0),v_1=(v1x,0),w_0=r_0e^{i\theta_0},w_1=r_1e^{i\theta_1}\in \mathbb{R}^2$ be any four non-zero vectors such that: $v_{0x}>0$ and $v_{1x}>0$ $0<\theta_0<\pi$ and $0<\theta_1<\pi$ the following inequalities are satisfied:
$$\displaystyle{\frac{||v_1||}{||v_0||}> \frac{||w_1||}{||w_0||}}\quad \textit{and }\frac{||v_1||}{||v_0||}> \frac{||v_1+w_1||}{||v_0+w_0||}$$ Now define, for every $t\in [0,1]$, the following vectors: $$\displaystyle{v_t=((1-t)v_{0x}+tv_{1x},0)}$$ $$\displaystyle{w_t=(r_0(1-t)+tr_1)e^{i((1-t)\theta_0+t\theta_1)}}$$ Is the following inequality $$\displaystyle{\frac{||v_{t_1}||}{||v_{t_0}||}\ge \frac{||v_{t_1}+w_{t_1}||}{||v_{t_0}+w_{t_0}||}}$$ verified for every $t_0,t_1\in [0,1],t_0<t_1$? If the answer is no, I'm searching for additional conditions on $v_1,v_2,w_1,w_2$ which will guarantee that this inequality is always verified. My attempt: I've tried many examples without findind a counterexample. But I couldn't find a general proof.","['vector-spaces', 'euclidean-geometry', 'linear-algebra', 'geometry']"
2331321,if $A$ is Real skew Symmetric Matrix Then Prove that $I-A$ is Non singular,if $A$ is Real skew Symmetric Matrix Then Prove that $I-A$ is Non singular. i have taken $2 \times 2$ matrix and proved it but is there a formal way to prove it?,"['matrices', 'linear-algebra', 'determinant']"
2331330,Probability / Permutation of large number of events.,"This seems like a very silly and simple question, but I'm here to learn. So here it is: How can I compute the probability of getting heads x times when tossing a coin a large number of times?  For example, what is the probability of getting heads 181 times when tossing a coin 300 times? I understand how to do this using permutations, factorials, and traditional (elementary) probability methods. My problem is that my calculator won't go that high. I'm unable to use (300!)/[(181!)(119!)] because I get an overflow error. The question I'm solving doesn't specifically ask for the probability, but I'd like to learn. Thanks!","['algebra-precalculus', 'statistics', 'probability', 'permutations']"
2331403,Computation of definite integral,"Value of $$\text{I}=\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}\sqrt{\cos(x)-\cos^2(x)}dx$$ $$\text{Attempt}$$ Using symmetry :- $$I=2\int_0^{\frac{\pi}{2}}\sqrt{\cos(x)(1-\cos(x))}dx$$.Letting $\cos(x)=u$ thus $du=-\sin(x)dx$ and using $1-\cos(x)=2\sin^2(\frac{x}{2}),\sin(\frac{x}{2})=\frac{2\sin(x)}{\cos(\frac{x}{2})},\cos(\frac{x}{2})=\sqrt{\frac{1+\cos(x)}{2}}$ the integral changes to $$\text{I}=2\int _0^1\sqrt{\frac{u}{1+u}}du$$.Now letting $u=\tan^2(t)$ we can solve the integral.But these are a lot of calculations and manipulations. Is there any elegant way to calculate the integral?","['definite-integrals', 'trigonometry']"
2331417,When do we have $\sup_{x}\int fdy=\int \sup_{x}fdy$?,"When do we have $$\sup_{x}\int fdy=\int (\sup_{x}f)dy$$? Here, $f$ is a function of $x$ and $y$. Some say: Use The monotone convergence theorem, but I really don't understand that hint.","['measure-theory', 'analysis']"
2331436,"$3^{4a+1} = 2b^2+1$ diophantine equation, $(a,b) \in \mathbb{N}^2$","I would like to solve the following diophantine equation : 
$$3^{4a+1} = 2b^2+1$$ Yet I don't know how to proceed. What I found so far is just that $$(a, b) = (0,1)\ ,\  (1,11)$$ seem to be the only solutions... Any ideas ?","['number-theory', 'diophantine-equations', 'elementary-number-theory']"
2331439,Integral over a curve in the sphere,"Fix $r\in(0,2)$, and consider arbitrary points $\mathbf{x},\mathbf{q}\in\mathbf{S}^2\subseteq\mathbf{R}^{3}$ such that $q_z > 1-r$ and $x_z > 1-r$. Consider the path $\gamma:[0,2\pi)\to\mathbf{S}^2$ given by $$\gamma(t)=(R\cos(t),R\sin(t),1-r),$$ where $R=\sqrt{r(2-r)}$. I want to compute $$\int_{\gamma([0,2\pi])}\frac{\ln(1-\mathbf{y}\cdot\mathbf{q})}{1-\mathbf{x}\cdot\mathbf{y}}~d\ell(\mathbf{y}).$$
I've been having a lot of trouble actually working this out by hand, and I'm not sure if there are any clever techniques that I could use to evaluate the above expression. I came across this integral when working out a representation formula for a solution to some sort of Dirichlet problem, and it would be very nice to have a closed form expression of the above. If there isn't any nice closed form solution for such an expression, any ideas on numerical techniques for computing the above integral quickly would be also appreciated.","['integration', 'differential-geometry']"
2331446,Show that coefficients of power series expansion are the fibonacci sequence,"Show that the power series expansion of $f(z)=\frac{1}{1-z-z^2}$ in $z_0=0$ is $\sum_n F_n z^n$, where $F_n$ is the Fibonacci sequence. Proof . Since $f$ is holomorphic it has a taylor expansion where $F_n=\frac{f^{(n)}(0)}{n!}$. By the Cauchy Integral Formula this equals $\displaystyle F_n=\frac{1}{2\pi i}\int_{|z|=r}\frac{f(z)}{z^{n+1}}dz$ Let us worry about the starting value later. If $F_n$ is the Fibonacci sequence we have $\displaystyle F_n=F_{n-1}+F_{n-2}$ so by inserting $F_n$ and the linearity of the integral we have to show $\displaystyle\frac{f(z)}{z^{n+1}}-\frac{f(z)}{z^{n}}-\frac{f(z)}{z^{n-1}}=0$ which is equal to $\displaystyle\frac{f(z)-z\cdot f(z)-z^2\cdot f(z)}{z^{n+1}}=0$ so $\displaystyle f(z)-z\cdot f(z)-z^2\cdot f(z)=0$ for $z\neq 0$. By inserting $f$ I get the contradiction $1=0$. What am I doing wrong? The mistake is in step 3 where I conclude $\int f=0\implies f=0$. Continuing with the integral proofs the statement.","['complex-analysis', 'functions']"
2331453,Sum of products of numbers with specific sum,"I came across a problem recently which involved finding $$\sum_{a_1+a_2+...+a_m=n}a_1a_2a_3...a_m$$ given $n$ and $m$ where all $a_i$ are positive integers.  While I was able to try different values and find the answer to be
${n+m-1}\choose{n-m}$, I don't understand why this answer is correct.  How would one go about solving this equation without just plugging in values and guessing?","['combinatorics', 'discrete-mathematics']"
2331466,Can you map the open unit disk conformally onto $\{ z: 0 < |z| < 1 \}$?,"I only find that is not possible from the punctured disk to the unit disk, but in the other direction is possible or not ? If not, counter example, if yes please provide the mapping. This is problem 15 from section 3 chapter 3, functions of complex variables Conway book.","['complex-analysis', 'analytic-functions', 'conformal-geometry']"
2331480,Short exact sequence makes exact triangle in derived category,"I am reading through the appendix of Commutative Algebra - with a View Towards Algebraic Geometry and I came across essentially the following question: 
Suppose that 
$\require{AMScd}$
\begin{CD}
    0 @>>> F' @>\alpha>> F @>\beta>> F'' @>>> 0
\end{CD}
is a short exact sequence of complexes from a category $\mathcal{A}$, show that it induces an exact triangle in $D(\mathcal{A})$, but not necessarily in $K(\mathcal{A})$. As a hint, it says to prove that $F''$  is quasi-isomorphic to $M(\alpha)$ (the mapping cone of $\alpha$) by showing that 
$\require{AMScd}$
\begin{CD}
    0 @>>>M(\alpha') @>>> M(\alpha) @>>> F'' @>>> 0
\end{CD}
is a short exact sequence of complexes, where $\alpha'$ is the isomorphism from $F'$ to $\alpha(F')\subset F$. I can show that the latter is indeed a short exact sequence, but that's it. I am quite new to derived categories and the concept of exact triangles in general is quite confusing to me.","['derived-categories', 'homological-algebra', 'algebraic-geometry']"
2331484,Computing how long it takes for something to decrease with differential equations of calculus,"​When a condenser discharges electricity, the instantaneous rate of change of the voltage is proportional to the voltage in the condenser.
Suppose you have a discharging condenser and the instantaneous rate of change of the voltage is 1/100 of the voltage (in volts per second). How many seconds does it take for the voltage to decrease by 90%? What I have gotten so far: v'[x] = instantaneous rate of change of the voltage v[x] = voltage in the condenser v'[x] = r v[x] v[x] = k e^(rx) I don't fully understand why v[x] = k e^(rx). I just know it is true. How do you solve this question?","['ordinary-differential-equations', 'calculus']"
2331498,"Does $\sum_{n=1}^\infty \frac{1}{n!}{n\brace \varphi(n)}$ converge, where ${n\brace m}$ are Stirling numbers of second kind?","When I was playing with the function StirlingS2[n, m] in Wolfram Alpha online calculator, that corresponds with the implementation of Stirling numbers of the second kind (see the definition in this MathWorld ), I've thought about doing experiments with number-theoretic functions like the Euler's totient function $\varphi(n)$ , the prime counting function $\pi(n)$ or the sum of divisor function $\sigma(n)$ . With the purpose of  ensuring the convergence of the series in my experiements I've multiplied by a factor $\frac{1}{n!}$ (and after I did my experiments I've seen that some series seems convergent, but other seems divergent). I don't know if these kind of questions were in the literature, I am saying deduce the convergence of series with same form of next question (with $0<a_n<b_n$ two arithmetic functions as inputs of the function StirlingS2[n, m] ). Question. Let $\varphi(n)$ the Euler's totient function and we denote with ${n\brace m}$ our Stirling numbers of second kind. Do you know how deduce the convergence of $$\sum_{n=1}^\infty \frac{1}{n!}{n\brace \varphi(n)}?$$ Thanks in advance.","['stirling-numbers', 'sequences-and-series', 'convergence-divergence', 'totient-function']"
2331502,Help proving an inequality with arcsin function using Mean value theorem,"So i have an inequality: $$ \frac{b-a}{\sqrt{1-a^2}} < \arcsin(b) - \arcsin(a) < \frac{b-a}{\sqrt{1-b^2}}$$
 $$0\leq a < b < 1$$ So I kinda see that both sides look similar to derivative, but how can use it in here? This is probably going to use Lagrange mean value theorem, or am I mistaken? I wonder how can I use the fact: $$\frac{d}{dx}(arcsin x)= \frac{1}{\sqrt{1-x^2}}$$ So I kinda see that $$(b-1)(\arcsin(a))' ?<\arcsin(b)-\arcsin(a)<(b-1)(\arcsin(b))'$$ The right inequality, should work since it's Lagrange theorem usage on it? right? I wonder that to to the left side, since I put a question mark there. Is it just the same just because of the fact: $a<b$ that it works the other way around as the right side one? Any help regarding my solution would be helpful, if it's right or wrong.","['derivatives', 'inequality', 'calculus']"
2331530,Infinite sum with number system conversions,"Any number $x \in \mathbb{Z}$ can be written as $\pm\sum\limits_{i=0}^n x_i2^i$ where $x_i \in \text{{0,1}}$ (signed binary representation).
It can also be written as $\sum\limits_{i=0}^n x_i(-2)^i$ where $x_i \in \text{{0,1}}$ (negabinary representation). We are given two functions $B(x)$ and $N(x)$, where $B(x)$ interprets the binary representation of the decimal input as a negabinary input, which it converts to binary and then to decimal and where $N(x)$ converts the binary representation of the decimal input to negabinary, which is then interpreted as binary and then as decimal. For example: $B(3_{10}) \rightarrow B(11_{-2}) = -1_{2}=-1_{10}$ and $N(2_{10}) = N(10_{2})=110_{-2} \rightarrow 110_2=6_{10}$ Now the following is true: $\sum\limits_{i=1}^{\infty} \frac{1}{B(2n+1)^2}=\frac{\pi^2}{4}$ because $B(2n+1)^2$ contains the square of all odd numbers twice and the sum of the recipricals of all odd numbers is $\sum\limits_{i=1}^{\infty} \frac{1}{(2n+1)^2}= \frac{\pi^2}{8}$ The question now is, what is
$\sum\limits_{n=1}^{\infty} \frac{1}{N(n)^2}$ equal to?","['binary', 'number-systems', 'sequences-and-series']"
2331531,"Calculate $\lim_{n\rightarrow\infty} \int_0^1 nx^2(1-x^2)^n \, dx$","I have to calculate $$\lim_{n\rightarrow\infty} \int_0^1 nx^2(1-x^2)^n \, dx$$ I've created the series $(f_n)_{n\in \mathbb{N}}$ with $f_n:[0,1]\rightarrow \mathbb{R}, f_n(x)=nx^2(1-x^2)^n$. I considered $x\in[0,1]$ a scalar and proceed to calculated the limit $\lim_{n\rightarrow\infty}f_n(x)$, which equals to $0, \forall x\in[0,1)$ (if I'm not wrong), but I'm getting stuck at calculating the limit for the particular case of $x=1$. Thanks for help!","['real-analysis', 'definite-integrals', 'sequences-and-series', 'limits']"
2331536,Tiling a floor with squares,"I have a rectangular room that has square linoleum tiles.  When the tiles were put in the contractor said that it would take twice as many cuts to have the tiles at a 45 degree angle from the walls than to have them perpendicular to the walls. While the contractor probably just meant it would take a good deal more, and not exactly twice as many cuts I began to think about how many cuts it would take to tile a rectangular room with the two patterns.  To formalize this a bit we will say that all cuts must be straight lines that start and end outside of the tile we are cutting.  We can also reuse the scrap created by cutting tiles (but we can't stitch tiles back together), for example if we need two half tiles we can use a single cut one one tile. There are some rooms that take 0 cuts to be tiled by pattern $a$, in particular rooms with integer side lengths (we are considering the tiles to be $1\times 1$).  These rooms it is clearly a good idea to use Pattern $a$ over Pattern $b$. However if both side lengths of the room (we'll call them $n$ and $m$) are multiples of $\sqrt{2}$ we can tile the room using pattern $b$ making only $\dfrac{n+m}{\sqrt{2}}$ cuts.  (for this we can cut $n+m$ tiles along the diagonals and use the half tiles along the edges).  The best it seems you can get in this situation using pattern $b$ it seems is $\lceil n\rceil+\lceil m\rceil$ (correct me if I'm wrong here). Given a general room is it possible to figure out which tiling method will require us to make the least cuts?","['tiling', 'geometry']"
2331546,Riesz's Lemma on $l_\infty$,"Riesz's Lemma says that for a Banach space $X$ , a proper closed subspace $Y\subset X$ , and $\epsilon\gt 0$ there exist $x\in X$ with $\|x\|=1$ and $d(x,Y)\ge 1-\epsilon$ . I have to find a closed subset in $l_\infty$ that demonstrates that the above result is not true for $\epsilon=0$ . That is, construct $Y$ such that for all $x\in l^\infty$ with $\|x\|_{l^\infty}=1$ it holds $d(x,Y)<1$ . The problem is that I'm not suposed to use weak topology, so I can't use Jame's Theorem (I've read a solution using it).","['functional-analysis', 'sequences-and-series']"
2331583,How do I prove or disprove this prime number conjecture?,"How would I go about proving something like this? Prove or disprove: If $p$ and $q$ are prime numbers for which $\ p < q$, then $\ 2p+q^2$ is odd. I'm assuming its definitely true because and even $+$ odd is always odd, and odd $\times$ odd is always odd too. So I know it's true, but how do I prove it?","['proof-writing', 'prime-numbers', 'discrete-mathematics']"
2331586,Number of combinations with sets of identical items,"I was trying to write a python program which takes a string and prints out all possible combinations of characters from it and in turn figure out the number of such possible combinations of a string to verify it's correctness This made me think about the following problem:
Given N total objects of k distinct kinds such that for i = 1 to k, r i objects are of the same kind , what are the total number of combinations of containing R objects? The problem of permutations containing all N objects is much simpler and intuitive, however, I am interested in R < N . I did some research and could only find this article discussing the problem, where it was suggested that the answer would be the coefficient of
x R in the following equation: (1+x+x 2 +..+x r 1 )(1+x+x 2 +..+x r 2 ).....(1+x+x 2 +..+x r k ) Is this the only way to solve this problem? Also how would one solve the problem when the combination length is specified to be at most R instead of exactly R?","['combinatorics', 'binomial-coefficients']"
2331600,Prove that $\int_0^\infty \frac{1+2\cos x+x\sin x}{1+2x\sin x +x^2}dx=\frac{\pi}{1+\Omega}$ where $\Omega e^\Omega=1$,"Whilst reading this Math SE post, I saw that the OP mentioned the integral
$$\int_0^\infty \frac{1+2\cos x+x\sin x}{1+2x\sin x +x^2}dx=\frac{\pi}{1+\Omega}$$
where $\Omega$ is the unique solution to the equation
$$xe^x=1$$
However, the question was about how to approximate the integral numerically. This is not a duplicate question; I would like to know how to exactly prove the above equality without approximation. Does anybody know how to do this? Thanks!","['lambert-w', 'integration', 'definite-integrals', 'closed-form']"
2331632,How to evaluate $\int_0^{2\pi} \frac{d\theta}{(A+B\cos\theta)^2}$?,"How to evaluate $\displaystyle\int_0^{2\pi} \frac{d\theta}{(A+B\cos\theta)^2}$?
I know that I can evaluate the integral using residue theorem, but which is the result with passages? Please",['complex-analysis']
2331644,Complex matrix decomposition into the sum of a diagonalizable and a nilpotent matrix.,"Let $A$ be any $n \times n$ complex matrix. Prove that $A$ can be written as $A = B + N$
where $B$ is diagonalizable, $N$ is nilpotent (some power is the zero matrix) and the
matrices $B$ and $N$ commute.","['matrices', 'linear-algebra']"
2331657,Prove that $\frac{1}{2\pi i}\int_\mathcal{C} |1+z+z^2+\cdots+z^{2n}|^2~dz =2n$ where $\mathcal{C}$ is the unit circle,"On the generalization of a recent question , I have shown, by analytic and numerical means, that $$\frac{1}{2\pi i}\int_\mathcal{C} |1+z+z^2+\cdots+z^{2n}|^2~dz =2n$$ where $\mathcal{C}$ is the unit circle. Thus, $z=e^{i\theta}$ and $dz=iz~d\theta$. There remains to prove it, however. What I have done: consider the absolute value part of the integrand, $$
\begin{align}
|1+z+z^2+\cdots+z^{2n}|^2
&=(1+z+z^2+\cdots+z^{2n})(1+z+z^2+\cdots+z^{2n})^*\\
&=(1+z+z^2+\cdots+z^{2n})(1+z^{-1}+z^{-2}+\cdots+z^{-2n})\\
&=(1+z+z^2+\cdots+z^{2n})(1+z^{-1}+z^{-2}+\cdots+z^{-2n})\frac{z^{2n}}{z^{2n}}\\
&=\left(\frac{1+z+z^2+\cdots+z^{2n}}{z^n} \right)^2\\
&=\left(\frac{1}{z^n}\cdots+\frac{1}{z}+1+z+\cdots z^n \right)^2\\
&=(1+2\cos\theta+2\cos 2\theta+\cdots+2\cos n\theta)^2\\
\end{align}
$$ We now return to the integral, $$
\begin{align}\frac{1}{2\pi i}\int_C |1+z+z^2+\cdots z^n|^2dz
&=\frac{1}{2\pi}\int_0^{2\pi}(1+2\cos\theta+2\cos 2\theta+\cdots+2\cos n\theta)^2 (\cos\theta+i\sin\theta)~d\theta\\
&=\frac{1}{2\pi}\int_0^{2\pi}(1+2\cos\theta+2\cos 2\theta+\cdots+2\cos n\theta)^2 \cos\theta~d\theta
\end{align}$$ where we note that the sine terms integrate to zero by virtue of symmetry. This is where my trouble begins. Clearly, expanding the square becomes horrendous as $n$ increases, and even though most of the terms will integrate to zero, I haven't been able to selectively find the ones that won't. The other thing I tried was to simplify the integrand by expressing it in terms of $\cos\theta$ only using the identity $$\cos n\theta=2\cos (n-1)\theta\cos\theta-\cos(n-2)\theta$$ but this too unfolds as an algebraic jungle very quickly. There are various other expressions for $\cos n\theta$, but they seem equally unsuited to the task. I'll present them here insofar as you may find them more helpful than I did. $$
\cos(nx)=\cos^n(x)\sum_{j=0,2,4}^{n\text{ or }n-1} (-1)^{n/2}\begin{pmatrix}n\\j\end{pmatrix}\cot^j(x)=\text{T}_n\{\cos(x)\}\\
\cos(nx)=2^{n-1}\prod_{j=0}^{n-1}\cos\left(x+\frac{(1-n+2j)\pi}{2n} \right)\quad n=1,2,3,\dots
$$ where $\text{T}_n$ are the Chebyshev polynomials. Any suggestions will be appreciated.","['complex-analysis', 'complex-numbers', 'complex-integration']"
2331671,Decompose a simple function into the sum of two simple functions,"Let $(\Omega, \mathscr F)$ be a measurable space and let $f,g : \Omega \to \mathbb R$ be non-negative Borel functions. Suppose that $\psi$ is a non-negative simple function, i.e. $\psi = \sum_{i=1}^n a_i \mathbf 1_{A_i}$ for some partition $\{A_i\}$ of $\Omega$ and some non-negative real numbers $\{a_i\}$, and that $\forall \omega \in \Omega$ $0 \leq \psi(\omega) \leq f(\omega) + g(\omega)$. By $\mathbf 1_{A}$ I mean the indicator function for the set $A$. I want to decompose $\psi$ into the sum of two simple functions $\varphi$ and $\varphi'$ where $0 \leq \varphi \leq f$ and $0 \leq \varphi' \leq g$. I believe this to be possible but I'm finding it difficult to prove. One example that I've been thinking about is taking $\Omega = [0, \pi/2]$, $f(x) = \sin(x) + 1$, and $g(x) = -\sin(x) + 1$. This means that $f + g = 2$, so I can take $\psi(x) = 1.99 \cdot \mathbf 1_{[0,\pi/2]}$. I can split this $\psi$ up, but I end up needing $\varphi$ and $\varphi'$ to be Riemann sum-like functions involving a large number of steps. This suggests that a constructive proof will be very difficult since $f + g$ and $\psi$ may be very simple, yet $f$ and $g$ separately can be extremely complicated. I've considered trying to show that we can find $\varphi$ and $\varphi'$ such that their sum is arbitrarily close to $\psi$, but that would require some kind of norm on my simple functions and I haven't introduced that yet. I haven't even mentioned a measure. I'm hoping there's a proof that doesn't require introducing new machinery like that. Update @Hagen von Eitzen has pointed out that this is not generally true if $\psi \leq f + g$ is allowed, so I will require $\psi < f + g$.","['real-analysis', 'measure-theory']"
2331696,Does multiplying with a unitary matrix change the spectral norm of a matrix?,"We know that the spectral norm of a matrix $A \in \Bbb K(n,n)$ $$\left \| A \right \| _2=\sqrt{\lambda_{\text{max}}(A^{^*}A)}$$ I need to prove that multiplying with an unitary matrix $U \in U(n)$ from the left or right does not change the value of the norm i.e.
$$\left \| A \right \| _2 = \left \| UA \right \| _2 = \left \| AU \right \| _2$$
I was able to prove that 
$$\left \| UA \right \| _2=\sqrt{\lambda_{\text{max}}((UA)^{^*}(UA))}$$
$$=\sqrt{\lambda_{\text{max}}(A^{^*}U^{^*}UA)}$$
$$=\sqrt{\lambda_{\text{max}}(A^{^*}A)}=\left \| A \right \| _2$$
since for unitary matrices $$U^{^*}U = I$$ I have no idea how to prove the the argument when multiplying from the right. This has been my approach.
$$\left \| AU \right \| _2=\sqrt{\lambda_{\text{max}}((AU)^{^*}(AU))}$$
$$=\sqrt{\lambda_{\text{max}}(U^{^*}A^{^*}AU)}$$
I've really got no idea on what to do next. Any help is much appreciated.",['matrices']
2331734,Infinitely many positive integers $n$ such that $n^2+1 \mid 1 \cdot 2 \cdot 5 \cdot 10 \cdots ((n-1)^2+1)$,"Prove that there exist infinitely many positive integers $n$ such that $$n^2+1 \mid 1 \cdot 2 \cdot 5 \cdot 10 \cdots ((n-1)^2+1).$$ We need to prove that there are infinitely many $n$ such that $\prod_{m=0}^{n-1}(m^2+1)$ is divisible by $n^2+1$. Some examples I found for when it is divisible are $n = 3,7,8,13,17,18,21$, but I didn't see how to find infinitely many.",['number-theory']
2331769,Hilbert scheme is isomorphic to moduli space of sheaves over a CY3,"I  a question regarding example 1.2 of the paper "" Cohomological Donaldson-Thomas theory "" by Balazs Szendroi, which I reproduce below: Let $Y$ be a CY3. For the case of sheaves of rank 1, one can be more explicit about the stability condition involved. Given a torsion-free sheaf $E$ of rank 1 on $Y$ , there is an injective map of sheaves $E\to\det E$ to the determinant of $E$ , which must be an invertible sheaf. Up to tensoring by this invertible sheaf, we can assume that the determinant is trivial, and then we get an embedding $E\to\mathcal{O}_Y$ with cokernel the structure sheaf $Z\subset Y$ of a subscheme of $Y$ , necessarily supported in codimension two. The parameter space of such embedded subschemes is the Hilbert scheme of codimension-two subschemes of $Y$ ; under the assumption $H^1(\mathcal{O}_Y)=0$ , the moduli scheme of sheaves is isomorphic to the Hilbert scheme [...] My main question is how to see the isomorphism mentioned at the end. I think there is a bijection between both spaces. Indeed, given a codimension two subscheme $Z\in Hilb$ we map it to its ideal sheaf in $\mathcal{M}$ , and conversely given a torsion-free rank 1 sheaf we apply the construction above to get a codimension-two subscheme. However, I don't know enough about the construction of these moduli spaces to know how to show these functions induce scheme maps. (1) Can someone comment on this? Once we show there is a scheme map defining a bijection I believe the standard method to show it's an isomorphism is showing the tangent spaces are isomorphic. I've read that the tangent space of the Hilbert scheme at some $Z\subset Y$ is $H^0(Z,N_{Z|Y})$ and the tangent space at $E$ of the moduli space of sheaves is $\operatorname{Ext}^1(E,E)$ . Here's where I believe the equality $H^1(\mathcal{O}_Y)=0$ comes into play. (2) In the same spirit as above, is there a straightforward way to present how the induced map on the tangent spaces looks? (3) How does $H^1(\mathcal{O}_Y)=0$ come into play here? I'm not looking for a fully detailed proof, some comments highlighting the main points of it would be great and much appreciated.","['moduli-space', 'algebraic-geometry']"
2331771,"If $\alpha'=f(\alpha)$ for some periodic $f:\Bbb R\to\Bbb R_{>0}$, then $\alpha(x+\ell)=\alpha(x)+A$ for some fixed $\ell$.","Let $f:\Bbb R\to\Bbb R_{>0}$ be some periodic Lipschitz function, $A>0$ some constant, and suppose that $f$ is $A$-periodic. Then the theory of ODEs tells us that we have global solutions $\alpha:\Bbb R\to\Bbb R$ to $$\alpha'(s)=f(\alpha(s))$$ for all initial values. Now, I suspect that if we set $$\ell=\int_x^{x+A}\frac{\mathrm{d}u}{f(u)}$$ for any $x\in\Bbb R$, then $\alpha(x+\ell)=\alpha(x)+A$ for all $x\in\Bbb R$ (this problem comes from the theory of curves, in which we try to find a unit-length parametrization for a closed immersed plane curve). But why is this true?","['real-analysis', 'ordinary-differential-equations']"
2331802,Jacobian matrix vs. Transformation matrix,"Given is a coordinate system $\{x_1,x_2,...,x_n\}$ and another, second coordinate system $\{y_1,y_2,...,y_n\}$, where $x_1=x_1(y_1,y_2,...,y_n)$ $x_2=x_2(y_1,y_2,...,y_n)$ ... $x_n=x_n(y_1,y_2,...,y_n)$ Then the Jacobian matrix is $${\mathbf J}=\begin{pmatrix}
\frac{\partial x_1}{\partial y_1} &  \frac{\partial x_1}{\partial y_2} & ... & \frac{\partial x_1}{\partial y_n}\\
\frac{\partial x_2}{\partial y_1} &  \frac{\partial x_2}{\partial y_2} & ... & \frac{\partial x_2}{\partial y_n}\\
... \\
\frac{\partial x_n}{\partial y_1} &  \frac{\partial x_n}{\partial y_2} & ... & \frac{\partial x_n}{\partial y_n}\\
\end{pmatrix}$$ Now, consider vector $\vec{u}$. Its coordinates in $\{x_k\}$ are
$\vec{u}=(u_{x1},u_{x2},...,u_{xn})$, while its coordinates in $\{y_k\}$ are 
$\vec{u}=(u_{y1},u_{y2},...,u_{yn})$, where $$\begin{pmatrix}
u_{x1}\\
u_{x2}\\
... \\
u_{xn}\\
\end{pmatrix}=
{\mathbf A} \cdot 
\begin{pmatrix}
u_{y1}\\
u_{y2}\\
... \\
u_{yn}\\
\end{pmatrix}$$ What is the difference between the Jacobian matrix ${\mathbf J}$ and the Transformation matrix ${\mathbf A}$? How are they related? Please, write the expression that connects them. ------------EXAMPLE----------- Cartesian and cylindrical coordinates are related via $x=r\cos\theta$ $y=r\sin\theta$ $z=z$ Then the Jacobian is $${\mathbf J}=\begin{pmatrix}
\frac{\partial x}{\partial r} &  \frac{\partial x}{\partial \theta} &  \frac{\partial x}{\partial z}\\
\frac{\partial y}{\partial r} &  \frac{\partial y}{\partial \theta} & \frac{\partial y}{\partial z}\\
\frac{\partial z}{\partial r} &  \frac{\partial z}{\partial \theta} &  \frac{\partial z}{\partial z}\\
\end{pmatrix}=
\begin{pmatrix}
\cos\theta &  -r\sin\theta &  0\\
\sin\theta &  r\cos\theta & 0\\
0 &  0 &  1\\
\end{pmatrix}$$ and $\det({\mathbf J})=r$. Because ${\mathbf J}$ is orthogonal when $r=1$, ${\mathbf J}^{-1}={\mathbf J}^{T}=\begin{pmatrix}
\cos\theta &  \sin\theta &  0\\
-\sin\theta &  \cos\theta & 0\\
0 &  0 &  1\\
\end{pmatrix}$ The Cartesian basis vectors are $\hat{\mathbf i} = \begin{pmatrix}
1\\
0\\
0\\
\end{pmatrix}$; 
$\hat{\mathbf j} = \begin{pmatrix}
0\\
1\\
0\\
\end{pmatrix}$; 
$\hat{\mathbf k} = \begin{pmatrix}
0\\
0\\
1\\
\end{pmatrix}$; The cylindrical basis vectors are $\hat{\mathbf r}=
\begin{pmatrix}
\cos\theta\\
\sin\theta\\
0\\
\end{pmatrix}$; 
$\hat{\mathbf \theta}=
\begin{pmatrix}
-\sin\theta\\
\cos\theta\\
0\\
\end{pmatrix}$; 
$\hat{\mathbf z} = \begin{pmatrix}
0\\
0\\
1\\
\end{pmatrix}$. It seems that $\hat{\mathbf r}={\mathbf A}\cdot \hat{\mathbf i}$ $\hat{\mathbf \theta}={\mathbf A}\cdot \hat{\mathbf j}$ $\hat{\mathbf z}={\mathbf A}\cdot \hat{\mathbf k}$ if the transformation matrix ${\mathbf A}$ is given by ${\mathbf A}=\frac{1}{\det({\mathbf J})}{\mathbf J}^{-1}$. Is this conclusion true? Is this the relationship between ${\mathbf A}$ and ${\mathbf J}$?","['coordinate-systems', 'multivariable-calculus', 'linear-algebra', 'vectors', 'vector-analysis']"
2331821,Prove that a subset of $L^1$ is closed under pointwise convergence.,"I'm currently studying for an analysis written qual and came across the following question: Let $m$ denote the Lebesgue measure on $\mathbb R$. Prove that the subset $A$ of $L^1(m)$ defined by $A := \{f \in L^1(m)\ :\ \int_{\mathbb R} |f|dm \leq 1\}$ is closed under pointwise convergence. I took a topological approach, using the fact that $L^1$ is a complete metric space, under the metric $\rho(f, g) = \int |f - g|dm$. As $A$ is the closed ball of radius 1 centered at $f = 0$, $A$ is also complete. That plus total boundedness implies $A$ is compact. So if $\{f_n\}_{n=1}^\infty$ is a sequence in $A$ that converges pointwise to $f$, it has a subsequence converging in $A$ which implies $f \in A$. My questions are: 1) is this solution valid? and 2) is there a more measure theoretic approach that works? My first instinct was to use Lebesgue dominated convergence but I couldn't come up with a dominating function.","['real-analysis', 'lebesgue-integral', 'measure-theory', 'analysis']"
2331826,Intermediate value theorem and the Riemann integration,"Previously I posted this question asking for a review of the proof  , but I realize the proof it's wrong at all because I can't have the continuity of $f$. So here is another attempt for the proof. If $f\in R$ on $[a,b]$ and $g$ is a monotonous function on $[a,b],$ then there exist $\epsilon \in [a,b]$ such that $$\int_a^bfg=g(a)\int_a^{\epsilon}f+g(b)\int_{\epsilon}^bf..................   (*)$$ Proof: (attempt) Let $F(x)=g(a)\int_a^{x}f+g(b)\int_{x}^bf.$ Thus $F$ is continuous. Also, $F(a)=g(b)\int_a^bf$ and $F(b)=g(a)\int_a^bf$. Now if $\int_a^bfg$ is between $F(a)$ and $F(b)$ and If I apply the Intermediate value theorem then I'll get something like $(*)$. But how can I find a 'bound' for $\int_a^bfg$ such that $\int_a^bfg$ is between $F(a)$ and $F(b)$?. Is this idea for the proof correct? Note: I can't use mesure theory for the proof because I haven't seen nothing (haven't taken a course) about mesure theory.","['real-analysis', 'riemann-integration', 'proof-writing', 'integration', 'analysis']"
2331833,Isomorphism of group actions,"Let $G$ be a group acting on a set $X$, and let $H$ be a group acting on a set $Y$. I'm wondering if there is standard terminology or notation to express the following condition: there exist an isomorphism $\varphi:G\rightarrow H$ and a bijection $\sigma:X\rightarrow Y$ such that $$\sigma(gx)=\varphi(g)\sigma(x)$$
for all $g\in G$ and all $x\in X$. Basically, I want to say that not only are $G$ and $H$ isomorphic as abstract groups, but that they act in the same way on the corresponding sets.",['group-theory']
2331843,How to formally represent a function whose elements are sets?,"Let us suppose that I have two sets: A and B. $A = \{0,2,4,6,8\}$
$B = \{1,3,5,7,9\}$ Let us suppose that I have a function f that maps elements of A and elements of B to the natural numbers ($\mathbb{N}$). I think that I can formalize this function in this way: $f: A \times B \to \mathbb{N}$ However, how can I formalize a function t that maps subsets of A and subsets of B to subsets of $\mathbb{N}$? I think that I can do this, by replacing the sets A, B, and $\mathbb{N}$, of the formal definition of f, by their respective power sets, in this way: $t: \mathcal{P}(A) \times \mathcal{P}(B) \to \mathcal{P}(\mathbb{N})$ Am I correct?","['elementary-set-theory', 'functions', 'discrete-mathematics']"
2331874,Determining asymptotic bounds on $T(n) = \sqrt{n}T(\sqrt{n})+n$,"Note: this is from JeffE's Algorithms notes on Recurrences, page 5: http://jeffe.cs.illinois.edu/teaching/algorithms/notes/99-recurrences.pdf (1). So we define the recurrence $T(n) = \sqrt{n}T(\sqrt{n})+n$ without any base case. Now I understand that for most recurrences, since we're looking for asymptotic bounds, the base case wouldn't matter. But in this case, I don't even see where we could define the base case. Is there any number we are guaranteed to hit as we keep taking square roots starting from any integer Do we just define $T(n) = a$ for $n<b$, for some reals $a$, $b$? (2). On page 7, Erickson gets that the number of layers in the recursion tree L will satisfy $n^{{2}^{-L}} = 2$. Where is this coming from? I have no idea. I see that the number of leaves in each level of the tree should sum to $\sqrt(n)\sqrt(n) = n$, but I have no idea where to go from there. (3). From the result mentioned in (2). Erickson derives $T(n) = \theta(n\lg\lg(n))$. But unrolling the recurrence yields 
     $$T(n) = n^{\sum\limits_{i=1}^k  \frac{1}{2^{i}}}T(n^{\frac{1}{2^k}})+kn \leq (k+1)n $$ For any integer k. Wouldn't this mean $T(n) = O(n)$, contradicting $T(n) = \theta(n\lg
\lg(n)$? Where is my reasoning wrong? (Please know that I ask with complete confidence that it is wrong). This is also posted on the computer science stack exchange, because of the overlap in topics. Any help is appreciated!","['recurrence-relations', 'discrete-mathematics']"
2331898,Probability that $n$ vectors drawn from arbitrary independent prob. distributions are linearly independent,"Let $X_1, X_2, \ldots, X_n \in \mathbb{R}^n$ be continuous random variables in $\mathbb{R}^n$ that are pairwise and elementwise independent. By continuous random variables, it is meant that their distributions do not include any Dirac delta fnc $\delta_x$. My question is: in this general situation, are $X_1, \ldots, X_n$ linearly independent (LI) with propbability 1? That is, is it true that 
\begin{equation}
\operatorname{Prob}\big ( \operatorname{span} \{ X_1, \ldots, X_n\} = \mathbb{R}^n \big ) = 1\text{?}
\end{equation} This is a general version of the topic: Probability that $n$ vectors drawn randomly from $\mathbb{R}^n$ are linearly independent since here we consider a general independent prob. distribution, rather than uniform (or Gaussian) distribution. By referring the related posts above and below: The probability that two vectors are linearly independent. I think the statement above is true as shown in the short proof below. First, the probability of ""$X_1 = 0$"" is zero since the singleton $\{ 0 \}$ is a set of measure zero. Hence, $X_1$ is LI with probability 1. Next, assume that $X_1,\ldots,X_r$ are LI with probability 1 for some $r \in \{1,2,\ldots, n-1\}$. Then, with probability 1, they span an $r$-dimensional subspace of $\mathbb{R}^n$ which is, however, a set of measure zero, too. Hence, the next vector $X_{r+1}$ lies on $\mathbb{R}^n$ outside the subspace with prob. $1$. By this process, $\operatorname{span}\{X_1,\ldots,X_n\} = \mathbb{R}^n$ with prob. $1$. But, I'm just not sure that this proof is correct and has no problem. Many thanks in advance for your comments and discussions! Edited: I think this can be extended to a more general case where the RVs are stochastically dependent. The proof seems to be also true in this dependent case as well---it is relevant to the fact that the distributions include no Dirac delta fnc, not to the stochastic independence.","['measure-theory', 'probability-theory', 'probability', 'linear-algebra']"
2331906,"Classic birthday problem turned on its head: With $N$ people, how many are likely to share most common birthday?","I have a unique opportunity to present to a very large group of people ($2{,}000$ in a theatre hall) about how chance works and how human intuition can be way off to guess likeliness. Rather than present the classic birthday problem to them (that is discussed many times very well in Math SE), I wanted to have some audience participation instead to illustrate the issue of chance and human intuition on the answer more directly with them, by asking a series of questions (see below) to get to the most common birthday in that entire audience and count how many hands are up for that. If I were to do this, how many people are likely to share that most common birthday ? Obviously, the many permutations among $2{,}000$ people mean that every single person will share so many birthdays with so many other people, and some birthdays will be less likely than others, but what number will I see specifically for that most common one? You can take any kind of confidence criteria that would be reasonable, such as a minimum number to expect with $50\%$ certainty. That way when I know the day-of exactly how many people are actually attending, I can update the final guess appropriately, and of course the answer can be more universally applicable to any crowd of $N$ people similarly. Edit: Since the number isn't likely going to be very high, a follow-up suggestion that I liked was the question ""Given that answer, how many birthdates should I look to ask for, such that $y$ people raise their hands?"" Then I may see that asking just $10$ dates gets over $100$ people, or $15$ gets $300$ etc. and I can get that impressive number I'm looking for. To get the final answer, the questions I'd ask would be: ""Whose birthday is in the first half of the month?"", then list the 6 months that seems to get more hands and pick the winner, then ask of those ""Whose birthday is among days 1-10? 11-20? 21-28/30/31?"", then ask whichever of those 3 groups has the most hands up ""Is it odd or even?"" and then list the 4-6 options and count each until we have a winner. I'll have assistants all over the theatre to help with the counting. I'd appreciate a comment if there's a more efficient way to do this that wouldn't be confusing. (Edit: See comments; this actually isn't as effective as I thought, so other suggestions welcome!) I think this approach for such a large crowd would be most effective, since intuition without any statistics that would lead someone thinking you'd need $183$ people to have a $50/50$ chance that two share a birthday and be shocked to hear it's $23$, could apply the opposite direction and I could suggest that since $2000/365 = 5{.}47$, maybe $5$ or $6$ people would share the most common birthday to add more of an impact when we see the actual answer. I could just pick a random date or my birthday and see the number of hands, but I think this ""most common birthday"" approach could be really effective.","['birthday', 'probability']"
2331919,The value of $\sum_{n=1}^{\infty}(-1)^{n-1}\frac{\log n}{n}$,"How to compute the following convergent series? or some hint!
$$\sum_{n=1}^{\infty}(-1)^{n-1}\frac{\log n}{n}.$$
The Wolfram MATHEMATICA9.0 gives the result is $1/2(\log 2)^2-\gamma\log 2$,
where $\gamma$ is tha Euler constant!",['sequences-and-series']
2331928,Explain this amazing cancellation of 4 terms to 40 digits,"Define the following four rational numbers.
$$
a = \frac{4243257079864535154162785598448178442416}{41016602865234375}
\\
b = -\frac{143308384621912247542172258992236503771301}{1210966757832031250}
\\
c = \frac{350687399375274064088342133116344593437371021}{4109863607096484375000}
\\
d = -\frac{762284492856611655417326017768244278005511063}{12085448243163671875000}
$$
Let
$$
p = \frac{501}{10}, \qquad m = \frac{499}{10}.
$$
Compute
$$
\mathrm{Result} = a \cos [p] + b \cos [m] + c \sin [p] + d \sin [m].
$$
Each term in this sum is roughly $10^{23}$. There is a curious cancellation (using 40 digits) happening amongst these four terms; the correct answer is $\mathrm{Result}=7.32 \times 10^{-18}$. My question: Where does this cancellation come from, analytically? Can you massage the terms into a form where the cancellation is manifest, and machine precision can evaluate the answer with a semblance of accuracy? (If you're curious, the result came from the analytic integral of a highly oscillatory function.)","['catastrophic-cancellation', 'trigonometry']"
2331930,How to solve this integro-differential equation?,"I came across this integro-differential equation to solve
$$\frac{du(x;t)}{dt}=-\lambda\int_0^xu(\xi;t)\;d\xi\tag{1}$$
under the initial condition $u(x;0)=f(x)$ where $x$ is a parameter, $\lambda$ is a constant, and $0<t<\infty$. My first thought is that I can just directly integrate the equation to obtain
$$u(x;t)=-\lambda\int_0^t\int_0^xu(\xi;\tau)\;d\xi\,d\tau\;.\tag{2}$$
This equation is highly implicit and an explicit expression is desired. So then I thought of using Laplace transforms instead. Let $U(\cdot)$ be the Laplace transform of $u(\cdot)$, and $s$ be the complex co-variable of the real variable $t$, then
$$s\,U(x;s)-f(x)=-\lambda\,\int_0^xU(\xi;s)\,d\xi\tag{3}$$
which can be converted to the differential equation
$$s\,U'(x;s)+\lambda\,U(x;s)=f'(x)\tag{4}$$
where the derivative is now with respect to $x$. Equation $(4)$ is easily solvable. Though I am uncertain if I did the following Laplace transform correctly,
$$\mathscr{L}\left\{\int_0^xu(\xi;t)\,d\xi\right\}=\int_0^xU(\xi;s)\,d\xi\;.\tag{5}$$
I figured since the integration is over the parameter instead of the transforming variable I could bring it into the integral under the heuristic that the Laplace transform of a sum is the sum of the Laplace transforms, but I am unsure of this. Any insight on any of this or alternative methods to solve Equation $(1)$ is welcome.","['integro-differential-equations', 'ordinary-differential-equations', 'laplace-transform']"
2331945,"Proving/disproving that differences $X_2 - X_1, \ldots, X_n - X_{n-1}$ of independent RVs $X_1, \ldots, X_n$ are also independent.","Let $X_1,\ldots,X_n$ be pairwise independent RVs in a Euclidean space.  My question is: Is the set of differences $\{X_{j+1} - X_j \}_{j=1}^{n-1}$ also pairwise independent? (Edited: by the discussion, it turns out that the proof below is wrong--see the answers) If they are all Gaussian, then 
\begin{equation}
    X_j, \, X_k \textrm{ independent } \; \Longleftrightarrow \; X_j, X_k \textrm{ uncorrelated } \; \Longleftrightarrow \; \mathbb{E}[X_jX_k] = \mathbb{E} [X_j] \mathbb{E} [X_k], 
\end{equation}
so the above statement can be proven as follows. Let $\mu_i = \mathbb{E}[X_i]$ for  $i = 1,2,\ldots,n$. Then, since $X_{j-1}$, $X_j$, and $X_{j+1}$ are all uncorrelated, 
\begin{align}
\mathbb{E}\big [ \, (X_j - X_{j-1}) \cdot (X_{j+1} - X_j) \, \big ] 
  &= \mu_j \mu_{j+1} - \mu_j^2 - \mu_{j-1}\mu_{j+1} + \mu_{j-1} \mu_j \\
  &= (\mu_j - \mu_{j-1}) \cdot (\mu_{j+1} - \mu_j ) \\
  &= \mathbb{E}[X_j - X_{j-1}]\cdot \mathbb{E}[X_{j+1} - X_j].
\end{align}
Hence, $X_j - X_{j-1}$ and $X_{j+1} - X_j$ are also uncorrelated. Since $X_j$'s are all Gaussian, so are $X_{j+1} - X_{j}$ for all $j=1,2,\ldots,n-1$. Therefore,  $\{X_{j+1} - X_j \}_{j=1}^{n-1}$ is independent. (Edited: The statement below regarding non-Gaussian RVs is also not true--see the answer) In general non-Gaussian situations, the same proof-line shows that $\{X_{j+1} - X_j \}_{j=1}^{n-1}$ is uncorrelated, but not necessarily independent since independent RVs are always uncorrelated, but not vice versa in general. I wonder if there is any other specific non-Gaussian distributions that make the statement true? A counter example? Or, is there any general proof of the statement w/o assuming RVs are Gaussian? Many thanks in advance for your discussion and comments.","['probability-theory', 'probability', 'probability-distributions']"
2331951,"If a compact set and a closed set have no intersection, is it true that the distance between the two set always positive?","Let $A \subseteq \mathbb{R}^n$ be compact, and $B \subseteq \mathbb{R}^n$ be closed. Assume $A \cap B=\emptyset$.Prove that there is a M>0 such that $||\vec{a}-\vec{b}|| \geq c \quad \forall \vec{a} \in A,\vec{b} \in B$. My solution is as follows: Suppose to the contrary that there exist $\vec{c} \in A,\vec{d} \in B$ such that for all c>0 we have $||\vec{c}-\vec{d}||<M$. But this would imply that $\vec{c}=\vec{d}$ which contradicts the fact that  $A \cap B=\emptyset$. I think my proof is wrong because i did not use the fact that A is compact and B is closed. Which step is wrong?","['calculus', 'analysis']"
2331972,"$\operatorname{span}\{x_1,\ldots,x_n\} = \mathbb{C}^n$?","If $A$ is a $n \times n$ Hermitian matrix, then is it right that,
\begin{equation}
\operatorname{span}\{x_1,\ldots,x_n\} = \mathbb{C}^n \text{?}
\end{equation}
where $x_i \ne 0$ is the eigenvector associated with the eigenvalue $\lambda_i$ of $A$. That is, $A x_i = \lambda_i x_i, i=1,\ldots,n$.","['matrices', 'eigenvalues-eigenvectors', 'linear-algebra']"
2332007,Find intersection of hyperbola and ellipse.,"Given $E: 2x^2-xy+y^2+y=4$ and $H: 2x^2-y^2=1$, find intersection. All the times I had to deal with problems of intersection I would always use substitution method, but now it seems a tedious way of dealing with this problem. Is there a better approach?","['analytic-geometry', 'conic-sections', 'geometry']"
2332008,Why the definition of topology is what it is?,"I recently attended an interview for PhD. One of panel members asked me definition of topology which I answered and next question was why the definion supposed to be that way. 
   I explained through an example. Is it right to explain definition using example? What could be a better answer?
Any help will be deeply acknowledged.",['general-topology']
2332039,The heat kernel as a distance metric on manifolds,"I recently came across Varadhan's formula (see e.g. [1] , [2] , [3] , [4] , [5] ):
$$
{d_{\text{g}}(x,y)^2}{} = -\lim_{t \rightarrow 0} 4 t \log K_t(x,y)
$$
where $d_\text{g}$ is the geodesic distance and $K_t$ is the heat kernel (i.e. the fundamental solution to the manifold heat equation $\Delta_g u = \partial_t u$). Essentially, it gives a way to compute the geodesic distance between two points, using the heat kernel. Completely separately, there is a notion of a diffusion map (see e.g. [1] , [2] , [3] , [4] ). The idea is to use a ""similarity"" function $\kappa(x,y)$ to construct the transition matrix of a Markov process representing a random walk on a set of points (from a manifold): $$ p(x,y) = \dfrac{\kappa(x,y)}{\sqrt{\left(\sum_u\kappa(x,u)\right)\left(\sum_v\kappa(y,v)\right)}} $$
which is the chance of the walk moving from $x$ to $y$ in one step.
The chance of moving from $x$ to $y$ in $n$ steps is given by the matrix power $p_n(x,y)=p^{(n)}(x,y)$.
The diffusion distance is then:
$$ d_m(x,y)^2 = \sum_{z} || p_m(x,z) - p_m(y,z) ||^2_d $$
where $m$ is fixed and $||\cdot||_d$ is some distance metric. Informally, the diffusion distance is related to the length of all paths between the two points . This is quite distinct from the geodesic distance. Of course, the quantity $K_t(x,y)$ is akin to a similarity function (since it is the probability of getting from $x$ to $y$ in time $t$ for a random walk), and could also be used to construct a notion of distance directly, I suppose. Questions Is there a clear mathematical connection between these different ways to measure distances between points on a manifold using the heat kernel? What other connections are there between the heat kernel, diffusion, and paths/distances between points on a manifold?","['riemannian-geometry', 'partial-differential-equations', 'heat-equation', 'manifolds', 'differential-geometry']"
2332060,"Is it true that $m=n\implies A$ is invertible, for an $m\times n$ matrix satisfying $(AA^T)^r=I$","If $m,n,r\in \Bbb N$ and $A$ is an $m\times n$ matrix satisfying $(AA^T)^r=I$ is it true that $m=n\implies A$ is invertible. I think it's true since $\det (AA^T)^r=1\implies \det A^{2r}=1\implies (\det A)^{2r}=1\implies \det A\neq 0\implies A $ is invertible. So the result is true. Is this correct?","['matrices', 'linear-algebra', 'proof-verification']"
2332090,$\arctan(\tan(-\frac{3\cdot \pi}{5}))+\operatorname{arccot}(\cot(-\frac{3\cdot \pi}{5}))=?$,"We know that $$\DeclareMathOperator{\arccot}{arccot}\arctan(-x)=-\arctan(x)$$ $$\arccot(-x)=\pi-\arccot(x)$$ $$\cot(-x)=-\cot(x)$$ $$\tan(-x)=-\tan(x)$$So I have figured out that $$\arctan(\tan(-\frac{3\cdot \pi}{5}))=\arctan(-\tan(\frac{3\cdot \pi}{5}))=-\arctan(\tan(\frac{3\cdot \pi}{5}))=-\frac{3\cdot \pi}{5},$$ $$\arccot(\cot(-\frac{3\cdot \pi}{5}))=\arccot(-\cot(-\frac{3\cdot \pi}{5}))=\pi-\arccot(\cot(\frac{3\cdot \pi}{5}))=\pi-\frac{3\cdot \pi}{5}.$$Coming from these, I have solved the problem as follows:$$\arctan(\tan(-\frac{3\cdot \pi}{5}))+\arccot(\cot(-\frac{3\cdot \pi}{5}))=-\frac{3\cdot \pi}{5}+\pi-\frac{3\cdot \pi}{5}=-\frac{6\cdot \pi}{5}
+\pi=-\frac{\pi}{5}.$$But the book says it is incorrect. Could somebody point out where I have made a mistake?","['trigonometry', 'inverse-function']"
2332108,Solving non-linear ordinary differential equation: $(y'')^2-y'y'''=\left(\frac{y'}{x}\right)^2$,"$$(y'')^2-y'y'''=\left(\frac{y'}{x}\right)^2$$ I have been struggling to solve this equation by doing following simplifications:
$$y'''y'=(y'')^2-\left(\frac{y'}{x}\right)^2$$
Dividing by $y'$:
$$y'''=\frac{(y'')^2}{y'}-\frac{y'}{x^2}$$
Dividing by $y''$:
$$\frac{y'''}{y''}=\frac{y''}{y'}-\frac{y'}{y''x^2}$$
I can't figure out what goes next since I have never encountered with the last fraction. I would really appreciate any help on the matter.",['ordinary-differential-equations']
2332150,analytic solution of a zombie model,"in the following Paper a ODE model of a zombie infection is given.
\begin{equation}
\begin{aligned}
	\frac{dS}{d\tau} &= -\frac{SZ}{N} \\
	\frac{dZ}{d\tau} &= (1-\alpha)\frac{SZ}{N}\\
	\frac{dR}{d\tau} &= \alpha\frac{SZ}{N} \\\\
                  N &= S + Z + R = const \quad \dots\\
\text{initial condition: } S(0) &= S_0,\, Z(0) = Z_0,\, R(0) = 0
\end{aligned}
\end{equation}
The system is non-linear and there exits a analytical solution. \begin{equation*}
\begin{aligned}
	P &\equiv Z_0 + (1-\alpha)S_0 \\
	\mu &\equiv \frac{P}{Z_0}-1\\
	f(\tau) &\equiv \frac{P\mu}{\exp{(\tau P/N)} + \mu}\\
	Z(\tau) &= P - f(\tau) \\
	S(\tau) &= \frac{f(\tau)}{1-\alpha}
\end{aligned}
\end{equation*} Unfortunatly I don't know how to solve the system to get the analytical solution.
My Idea is to substitute X = SZ but if I calculate $\dot{X}$ I can't eliminate S or Z from the equation.
\begin{equation}
\dot{X} = \dot{S}Z + \dot{Z}S
\end{equation} 
I'm out of ideas how to solve the system and I hope for some suggestions","['ordinary-differential-equations', 'nonlinear-system']"
2332172,Speeding up the convergence of $\zeta(2)$,"Let us denote by $S$ the sum of the series $\displaystyle\zeta(2)=1+\frac1{2^2}+\frac1{3^2}+\cdots$ Yes, I know (and you know) that $S=\frac{\pi^2}6$, but that is not relevant for the question that I am about to ask. This series converges slowly. In fact, the sequence$$\left(S-\sum_{k=1}^n\frac1{k^2}\right)_{n\in\mathbb N}$$converges to $0$ at about the same rate as $\left(\frac1n\right)_{n\in\mathbb N}$. My question is about speeding up the rate of convergence of this series. More precisely, it is this: prove that there is a number $K\in(0,1]$ such that$$(\forall n\in\mathbb{N}):\left|S-\frac2{2n+1}-\sum_{k=1}^n\frac1{k^2}\right|\leqslant\frac K{n^3}.$$ Added note: Because of some comments that I got, I want to make this clear: I know an answer to this question.","['sequences-and-series', 'calculus', 'convergence-acceleration']"
2332199,If $A$ and $B$ are matrices such that $AB^2=BA$ and $A^4=I$ then find $B^{16}$.,"If $A$ and $B$ are matrices such that $AB^2=BA$ and $A^4=I$ , then find $B^{16}$ . My Method: Given $$AB^2=BA \tag{1}$$ Post multiplying with $B^2$ we get $$AB^4=BAB^2=B^2A$$ Hence $$AB^4=B^2A$$ Pre Multiplying with $A$ and using $(1)$ we get $$A^2B^4=(AB^2)A=BA^2$$ hence $$A^2B^4=BA^2 \tag{2}$$ Now post multiplying with $B^4$ and using $(2)$ we get $$A^2B^8=B(A^2B^4)=B^2A^2$$ hence $$A^2B^8=B^2A^2 \tag{3}$$ Now Pre Multiply with $B^2$ and use $(3)$ we get $$B^2A^2B^8=B^4A^2$$ $\implies$ $$A^2B^8B^8=B^4A^2$$ $$A^2B^{16}=B^4A^2$$ Now pre multiply with $A^2$ and use $(2)$ we get $$A^4B^{16}=A^2B^4A^2$$ $\implies$ $$B^{16}=BA^4=B$$ is there any other approach to solve this?","['matrices', 'abstract-algebra', 'determinant', 'proof-writing', 'linear-algebra']"
2332216,"MVUE of Bernoulli r.v. when $p\in [1/2,1)$","$X_1, X_2\dots , X_n$ are independent Bernoulli r.v. s.t. $P(X_i=1)=p$ , $i=1,2,\dots , n$ , where $p\in [1/2,1)$ . (a) Is $T=\frac{1}{n} \sum_{i=1}^{n}X_i$ MVUE of $p$ ? Justify it. It can be easily shown that $T$ is MVUE if $p\in (0,1)$ , since $T$ is complete sufficient statistics(as belongs to exponential family) and unbiased estimator by Lehmann-Scheffe theorem. But here $p\in(1/2,1]$ . I don't think the result would be same(or may be same). I am confused!! What I did is that: Let $T^*=a\space \text{if $T<.5$ and}\\ \space\space\space\space\space\space\space\space bT\space \text{if $T\geq.5$}$ $a,b$ are such that $T^*$ is u.e. of $p.$ Then I tried to find $a,b$ such that $Var(T^*)<Var(T)$ . But I don't think this is right.","['parameter-estimation', 'statistics', 'statistical-inference']"
2332271,"sequence of positive numbers satisfying $a_{n+1}=\frac{2}{a_n+a_{n-1}}$, prove it converges","Assume that $(a_n)$ is a sequence of positive real numbers satisfying $a_{n+1}=\frac{2}{a_n+a_{n-1}}$ for $n=2,3,\dots$. Prove that $(a_n)$ is convergent and find the limit. I have no idea how to prove convergence.
I tried to prove that $(a_n)$ is bounded and monotone, but both trials failed. If convergence is proven, the limit is easy: if $g=\lim a_n$, then $g\ge 0$ because all $a_n$'s are positive, the equivalent relation $a_{n+1}(a_n+a_{n-1})=2$ gives $g(g+g)=2$, so finally $g=1$.","['convergence-divergence', 'sequences-and-series', 'calculus', 'limits']"
2332277,Is there $n>1$ such that $n^{n+1} \equiv 1 \mod (n+1)^n$?,"First of all, note that $\frac{n^{n+1}}{(n+1)^n} \sim \frac{n}{e}$. Question : Is there $n>1$ such that $n^{n+1} \equiv 1 \mod (n+1)^n$? There is an OEIS sequence for $n^{n+1}\mod (n+1)^n$: https://oeis.org/A176823 . $0, 1, 8, 17, 399, 73, 44638, 1570497, 5077565, 486784401, 22187726197,
 166394893969, 13800864889148, 762517292682713, 9603465430859099,
 803800832678655745, 3180753925351614970, 947615093635545799201$","['number-theory', 'congruences', 'modular-arithmetic', 'elementary-number-theory']"
2332297,Intuition for the Lipschitz-condition in Picard-Lindelöf,"In an exercise class I taught recently, someone came up to me after class and asked me about the intuition behind the Lipschitz-condition in Picard-Lindelöf. I am aware of the examples which show, that one cannot weaken the Lipschitz-condition in the Picard-Lindelöf Theorem. But why is the Lipschitz-condition really there? Is there some intuition from physics behind it? Or is it merely a technical tool to prove the theorem?","['real-analysis', 'ordinary-differential-equations', 'calculus']"
2332326,Bounded Laplacian and function implies bounded gradient,"Let $f:\mathbb R^n\to \mathbb R$ be a smooth function. Suppose that in a neighbourhood $U$ of $0$, we have two bounds:
$$
|f(x)|\leq A
$$
and
$$
|\Delta f(x)|\leq B
$$
Is it true that we have some bound of the form
$$
|\nabla f(x)|\lesssim A^\theta B^{1-\theta},
$$
where $x\in U$? The one-dimensional case is easier to verify. But I wonder if there is some high dimensional analogue.","['real-analysis', 'interpolation', 'calculus', 'ordinary-differential-equations', 'analysis']"
2332351,Relative homology group of a translation surface,"Let $X$ be a Riemann surface of genus $g\ge 2$, $\omega$ an holomorphic differential on $X$ and $\Sigma(\omega)$ the set of zeroes of $\omega$. We can equip $X$ with the singular flat metric $\omega\overline{\omega}$: the couple $(X,\omega\overline{\omega})$ is often called a translation surface. Consider the relative homology group $H_1(X,\Sigma(\omega),\mathbb{C})$. Let $\gamma$ be a saddle connection on $X$: by definition it is a geodesic segment for $\omega\overline{\omega}$ which encounters the points of $\Sigma(\omega)$ only at its initial and terminal points. Consider $[\gamma]\in H_1(X,\Sigma(\omega),\mathbb{C})$ (the class of $\gamma$ in the relative homology group); I know that, since the metric $\omega\overline{\omega}$ is negatively curved, $\gamma$ is the only geodesic representative of $[\gamma]$. My question is: what is the geodesic representative of $k[\gamma]$ for $k\in \mathbb{R}$?","['differential-topology', 'algebraic-geometry', 'algebraic-topology', 'general-topology', 'differential-geometry']"
2332358,Are probability distributions for sample or population?,"When we take a sample, does the relative frequencies of sample  follow sample probability distributions or the population probability distribution ? Also, why does events have to follow a certain distribution ? What natural laws force them to follow those underlying distribution ?","['statistical-inference', 'probability-theory', 'probability-distributions', 'statistics', 'probability']"
2332381,Meaning of functional differentiability,"I just began studying variational calculus, and I'm having some issues getting a conceptual grasp on functional differentiability. Let $J[y]$ be a functional defined on some normed linear space, and let
  $$\Delta J [h] = J [y+h]-J [y] $$
  Suppose that
  $$\Delta J[h] = \phi [h] + \epsilon ||h||$$
  Where $\phi [h] $ is a linear functional and $\epsilon \to 0$ as $||h|| \to 0$. Then $J [h]$ is said to be differentiable . From the definition of $\Delta J [h] $, it is clear that we're concerned with finding how $J $ varies as its argument changes (very analogous to traditional differentiation). But I don't think we could graph this result, as infinitely many functions will have the same norm (graphing the tangent line is generally the easiest way to understand a derivative in Real Analysis), so I'm struggling to see the full meaning of this definition. For our norm, we are using $||h|| = \max_{a \le x \le b} |h(x)|$. If $||h|| \to 0$, then $h$ approaches the zero function, so that $y+h \to y$, which is consistent with ordinary derivatives. But why would the fact that we can write $\Delta J[h] = \phi [h] + \epsilon  ||h||$ correspond to differentiability (perhaps I'm trying to relate it too much to real analysis here)? It seems to say that the if a (generally) nonlinear functional is differentiable, we can relate 'small' changes in $J$ to some linear functional  (sort of similar to a linear approximation, I suppose), as: $$\frac {\Delta J [h]-\phi [h]}{||h||} = \epsilon $$
If $||h|| \to 0$ and $\epsilon \to 0$, it necessarily demands that $\Delta J [h] - \phi [h] \to 0$ What is the purpose of having this class of functionals and how does it relate to how a general functional changes with respect to its variable?","['derivatives', 'gateaux-derivative', 'calculus-of-variations', 'frechet-derivative']"
2332412,Interactive problem to demonstrate non-intuitive probability results to a large crowd?,"I will be presenting to a large crowd of about 2,000 people on how our intuition does not align with reality in many situations, as I've always been impressed with how statistical analysis can show us how the world really works, to get us to accept that our intuition on probability can be way off. What I'm looking for is Suggestions on specific questions/riddles or situations that I can give to this crowd that would demonstrate this in the most impressive way by them actually answering the questions & the result being counted right there Some suggested Statistical Mathematics applied to that problem, to demonstrate a likely expected (with, let's say 80% confidence, or whatever would be more appropriate) end-result given a sample size of N , so I can know ahead of time the number I should expect the result to be close to. The tone of this event doesn't allow me to give any Mathematical explanations, so a demonstration would be more powerful to illustrate the point. As such, questions I'd ask the crowd would need to be straightforward for most people to follow and to allow for a show of hands to be counted, or for them to have discussions with those around them (in groups of x , that could be defined in outlining the problem). I'll have about two dozen helpers to potentially count hands or to ask groups what the result is, and we'll be doing it quickly, so precision can be sacrificed for the sake of simply getting an impressive final result. The classic example that demonstrates this well for explanatory purposes (without relying on an actual live implementation) would be the birthday problem , but that only needs 23 people for the odds to be over 50/50 of the same birthday, and even if you tested it on a group of 23 people, doing it once will be a bad way to demonstrate your point because of how probability actually works meaning that maybe there won't be that shared birthday this time. But with a large group of people, we can be more confident to get close to a particular number. At first, I thought that I could scale up the birthday problem and simply ask a small number of questions to get to the most common birthday for the entire crowd, but in posing another question here to see what the expected number may be , I'm reminded why I'm asking here in the first place in how rusty my statistics knowledge is, given that doing so would face the law of big numbers and actually give the intuitively (much less impressive) expected result. I could, of course, give a simple twist to a classical probability situation, such as the Monty Hall problem , by getting a show of hands for who would do what, or who would expect what odds. That result won't be as impressive though since the large number isn't really a result, so much as psychological proof that most people's intuition is off. If nothing else works, I'd do this and simply explain the issue, but that would work for a small crowd just as well, and I'd like to take advantage of having such a big audience, and having the law of big numbers work for me to be confident on getting a particular result or very close. Here are a couple of my quick ideas on doing this, but I'd like to see if someone can give me a demonstrable proof of one of these or another demonstration you may have in mind, which would give a large final result, where intuition (for the layperson unfamiliar with statistics) may not expect it: Attempt to split the crowd into groups of approximately 23 (or slightly higher if seating arrangements make that easier), and to give them 2 minutes to figure out if there are any shared birthdays. When they are finished, count the number of successes, which should be approximately half (about 43 of ~87 groups that would form). [Would this work as expected, or am I forgetting something?] The top answer on this question has multiple interesting suggestions that I'll be looking into to see which could be most entertaining, but can we find an expected answer given my criteria, and from that, know which one is going to be most impressive?","['probability', 'soft-question']"
2332425,Is there an Elementary Theory of the Category of Groups?,"I am aware that the category of categories and category of sets has been axiomatized. Is there such an axiomatization of the category of groups? (and am I correct in thinking ""axiomatization of the category of groups"" and ""elementary theory of the category of groups"" refer to the same thing?)","['category-theory', 'reference-request', 'group-theory', 'foundations']"
2332476,Showing that the Fatou's lemma inequality can be strict.,"Let $X = \mathbb{R}$ and $M$ denote the $\sigma$-algebra of Lebesgue measurable subsets of $\mathbb{R}$, while $m$ denotes the outer Lebesgue measure restricted to $M$. Consider $f_n = \chi_{(1,2]}$ (characteristic function of $(1,2])$ when $n$ is even and $f_n = \chi _{[0,1]}$ when $n$ is odd. Since both $(1,2]$ and $[0,1]$ are Lebesgue measurable, we have that for each $n$, $f_n$ is a measurable function. Now when $n$ is odd, $\int_X f_n dm = m(1,2] = 1$ and $m[0,1] = 1$ for even $n$. Hence for each $n$ the integral is $1$. Hence,  $\lim \inf \int_X f_n dm = 1$. However clearly, $\lim\inf f_n = 0$. So $\int_X \lim \inf f_n dm = 0$. Hence the inequality in Fatou's lemma can be strict. Is this a correct argument, or have I made a mistake somewhere?","['real-analysis', 'limsup-and-liminf', 'measure-theory', 'proof-verification']"
2332485,Stirling Number of the second kind recurrence combinatorial proof,"I am working with the Stirling number of the Second kind problem from the book of Principles and Techniques in Combinatorics by Chen Chuang-Chong and Koh Khee-Meng.
Page 73 problem 84: Given $r,n\in \mathbb{Z}$ with $0 \le n\le r$, the Stirling number $S(n,k)$ of the second kind is defined as the number of ways of distributing $r$ distinct objects into $n$ identical boxes such that no box is empty. Show that ( i ) $S(r,3)=\frac{1}{2}(3^{r-1}+1)-2^{r-1}$; ( ii ) $S(r,r-1)=\binom{r}{2}$. My answer for ( ii )Exactly one box will contain two objects while all the other boxes contain exactly one boxes. It suffices to consider which two elements belong to the same box, and there are $\binom{r}{2}$ ways to do so.
For ( i ) I dont know how to do the combinatorial proof. I am trying to understand so that I can get the RHS but I failed.. ANy Help is very much appreciated. Thank you so much..","['recurrence-relations', 'combinatorial-proofs', 'stirling-numbers', 'combinatorics', 'discrete-mathematics']"
2332495,Gelfand Triples / Rigged Hilbert Spaces - Reflexivity necessary?,"There have been several questions asked on various aspects of Gelfand triples. However, I have not yet found an answer to the following question: Let $V$ be a Banach space, $H$ be a Hilbert space such that we have a dense embedding
$$
i :V \hookrightarrow H.
$$
We then consider the ""adjoint map""
$$
i^{*} : H^{*} \rightarrow V^{*}
$$
which is defined via
$$
_{V^{*}}\langle i^{*}(\phi), v \rangle_{V} :=~ _{H^{*}} \langle \phi, i(v) \rangle_{H}
$$
(intuitively, if $V \subset H$ as sets, one can think of the action of $i^{*}$ as being the restriction of a functional on $H$ to a functional on $V$). We would like this map $i^{*}$ to have three properties: continuous injective with dense image, i.e. $i^{*}(H^{*}) \subset V^{*}$ is a dense subset w.r.t. the norm on $V^{*}$. If we have established this, we can proceed to find the Gelfand triple 
$$
V \hookrightarrow H \equiv H^{*} \hookrightarrow V^{*}
$$ 
with dense embeddings. The first property is easy to check, the second follows from the density of $i$. However, for the third property, one needs (or rather: seems to need) something more. Brezis, in his functional analysis book, for example, assumes reflexivity (Rem. 5.2.3 on p. 136 in the 2011 edition) and so do Liu/Röckner ( SPDEs: An Introduction , Ch. 4.1, p. 69), Zeidler ( Linear Monotone Operators , Ch. 23.4, p. 416) and all other books I could get my hands on. As I will show below, reflexivity is sufficient indeed, but here is my question : Is reflexivity also necessary for property 3? That it works if $V$ is assumed to be reflexive can be seen as follows: Let $c$ denote the canonical map
$$
c : V \rightarrow V^{**}, ~ v \mapsto \psi_{v}, ~ \psi_{v}(v^{*}) := v^{*}(v),
$$
i.e. $c(v)(v^{*}) := v^{*}(v)$. If $V$ is reflexive, $c$ is surjective. Now to prove the density of $i^{*}(H^{*}) \subset V^{*}$ we need to check that if a continuous functional $\psi$ on $V^{*}$, i.e. $\psi \in V^{**}$, is zero on the image, then it must already be the zero functional, i.e. $\psi = 0$. So the condition we need to check is
$$
_{V^{**}} \langle \psi, i^{*}(\phi) \rangle_{V^{*}} = 0 \quad \forall \phi \in H^{*} \quad \Rightarrow \quad \psi = 0.
$$
By reflexivity, $\psi = c(v)$ for a $v \in V$. So let $\phi \in H^{*} 
$ be arbitrary. By the definition of $c$ and $i^{*}$ we get
\begin{align*}
0 &= ~_{V^{**}} \langle \psi, i^{*}(\phi) \rangle_{V^{*}} = ~_{V^{**}} \langle c(v), i^{*}(\phi) \rangle_{V^{*}} = ~_{V^{*}} \langle i^{*}(\phi), v \rangle_{V} \\
&= ~_{H^{*}} \langle \phi, i(v) \rangle_{H}.
\end{align*}
Since this holds true for every $\phi \in H^{*}$, by Hahn-Banach (or rather, since $H^{*}$ separates points on $H$), this implies $i(v) = 0$ which by the injectivity of $i$ yields $v = 0$ and hence by linearity of $c$ that $\psi = c(v) = 0$.","['banach-spaces', 'hilbert-spaces', 'partial-differential-equations', 'adjoint-operators', 'functional-analysis']"
2332536,Integrate Airy function from 0 to $\infty$,"From the integral representation of Airy function
$$\mathrm{Ai}(x)=\int_{-\infty}^{\infty} \frac{\mathrm{d} \tau}{2\pi} \exp(-\mathrm{i}\tau x)\exp(-\mathrm{i}\frac{\tau^3}{3}),$$
It is easy to see that $\int_{-\infty}^{\infty} \mathrm{d} x\mathrm{Ai}(x) =1$. However, I am wondering how to find 
$$\int_{0}^{\infty} \mathrm{d}x \mathrm{Ai}(x).$$
From this website , the result of the above integral is $\frac{1}{3}.$ I could not follow the method in the reference given by that website. Could anyone give an alternative (more straightforward) derivation of  $\int_{0}^{\infty} \mathrm{d}x \mathrm{Ai}(x)=\frac{1}{3}$?","['special-functions', 'airy-functions', 'definite-integrals', 'calculus']"
2332537,Soft question: Why use optimization algorithms instead of calculus methods?,"I know that some functions are multivariate and therefore take a long time to compute zeros for the gradient, and then test for minima, maxima, etc. However, some optimization algorithms are not tractable either, and numerically they are never analytical and they don't give an analytic solution. So what is the purpose of not simply using the analytical methods we have from calculus eg solving gradient zeros vs possibly incorrect and at best approximate optimization algorithms?","['calculus', 'algorithms', 'numerical-optimization', 'numerical-methods', 'analysis']"
2332562,Prove that every compact Riemann surface is an algebraic curve.,"I am currently studying Riemann surfaces and I find a theorem stating ""Every compact Riemann surface is an algebraic curve."" but I don't find a proof. Thanks in advance for helping by providing a proof...","['algebraic-curves', 'riemann-surfaces', 'algebraic-geometry']"
2332585,$\lim_{|x|\rightarrow\infty}(f\cdot g)(x)=0$,"suppose $f,g\in L^1(\mathbb{R}^d)$ and $g$ bounded. then $\lim_\limits{{|x|\to\infty}}(f\cdot g)(x)=0$? since $|(f\cdot g)(x)|=|\int_{\mathbb{R}^d}f(x-y)\,g(y)\,dy|\leq ||g||_\infty\int_{\mathbb{R}^d}|f(x-y)|\,dy=||g||_\infty||f||_1<\infty$, at least the limit is bounded. I guess I need to know if $\int_{\mathbb{R}^d}|f(x-y)|\,dy$ goes to $0$ as $|x|\rightarrow \infty$, but I got nothing until now. if $x$ moves finite distance, then the integral won't change. would it be different near $\infty$?","['real-analysis', 'limits', 'convolution', 'calculus', 'measure-theory']"
2332589,7 friends are going to the cinema. They will be sitting in a row with 7 seats. What is the probability that John and Mary don't sit together?,"To watch a movie, John, Mary and 5 friends will sit randomly in a row with 7 seats. What is the probability John and Mary won't sit together? $$(\mathbf A)\ \frac{2\times5!}{7!}\qquad(\mathbf B)\ \frac{5!}{7!}\qquad(\mathbf C)\ \frac27\qquad(\mathbf D)\ \frac57$$ I did: $$1-\left(6\cdot 2\cdot\left(\frac{2}{7}\cdot\frac{1}{6}\right)\right) = \frac{3}{7}$$ But my book states the solution is D). I tried not multiplying by 2 and I get D), however I don't know exactly why the 2 is wrong. You can make 2 permutations with Mary(M) and John(J), MJ and JM. Then if you imagine the 2 of them as a block of 2 seats they can sit in $^6C_1=6$ places. Why doesn't my book count those 2 permutations of JM and MJ?","['combinatorics', 'probability']"
2332659,Test whether or not $\sum _{n=1}^{\infty }\frac{1}{n\left(1+\frac{1}{2}+...+\frac{1}{n}\right)}$ converge?,"$$\sum _{n=1}^{\infty }\frac{1}{n\left(1 +\frac{1}{2}+...+\frac{1}{n}\right)}$$ I am trying to use limit comparison test to test this series, Let $a_n = \frac{1}{n\left(1 +\frac{1}{2}+\ldots+\frac{1}{n}\right)}$ and $b_n = \frac{1}{n}$ $\lim_\limits{{n\to \infty }}\left(\frac{a_n}{b_n}\right)$ $=\lim _\limits{{n\to \infty }}\left(\frac{\frac{1}{n\left(1+\frac{1}{2}+\ldots+\frac{1}{n}\right)}}{\frac{1}{n}}\right)$ $=\lim _\limits{{n\to \infty }}\left(\frac{1}{\left(1+\frac{1}{2}+\ldots+\frac{1}{n}\right)}\right)$ This limit will always larger than $0$, therefore $\sum _{n=1}^{\infty }\:\frac{1}{n}$ diverges, $\sum _{n=1}^{\infty }\frac{1}{n\left(1 +\frac{1}{2}+\ldots+\frac{1}{n}\right)}$ diverges. Is this working valid or there is a better solution for this question?",['sequences-and-series']
2332664,"Topological space with countable open cover $\{U_\alpha\} $ with each $U_\alpha$ second-countable, is second countable","I'm trying to prove the following statement: Let $X$ be a topological space with a countable open cover $C= \{U_\alpha : \alpha \in A\}$. If each $U_\alpha$ is second-countable then $X$ is second-countable. However I'm stuck on how to proceed, so any hints on how to tackle it would be highly appreciated.",['general-topology']
2332684,Show $(\frac{\partial \ln p}{\partial \ln V})_{T} = \frac{V}{p}(\frac{\partial p}{\partial V})_{T}$,"Question: In thermodynamics, the pressure of a system, $p$, can be considered as a function of the variables $V$ (volume) and $T$ (temperature) or as a function of the variables $V$ and $S$ (entropy). (i) By expressing $p(V,S)$ in the form $p(V,S(V,T))$ evaluate $(\frac{\partial p}{\partial V})_{T} - (\frac{\partial p}{\partial V})_{S}$ in terms of $(\frac{\partial S}{\partial V})_{T}$ and $(\frac{\partial S}{\partial p})_{V}$. (ii) Hence, using $TdS = dU + pdV$ (conservation of energy with $U$ the internal energy), show that: $(\frac{\partial \ln p}{\partial \ln V})_{T} - (\frac{\partial \ln p}{\partial \ln V})_{S} = (\frac{\partial (pV)}{\partial T})_{V} [ \frac{p^{-1}(\frac{\partial U}{\partial V})_{T}+1}{(\frac{\partial U}{\partial T})_{V}}]$. [Hint: $(\frac{\partial \ln p}{\partial \ln V})_{T} = \frac{V}{p}(\frac{\partial p}{\partial V})_{T}$] (This question is from http://www.damtp.cam.ac.uk/user/examples/A3a.pdf .) ------------------------------------------------------------------------------------------- My Attempt: (i) By Chain Rule: $dp = (\frac{\partial p}{\partial V})_{S}dV + (\frac{\partial p}{\partial S})_{V}dS$ =>$(\frac{\partial p}{\partial V})_{T} - (\frac{\partial p}{\partial V})_{S} = (\frac{\partial p}{\partial S})_{V} (\frac{\partial S}{\partial V})_{T} = \frac{(\frac{\partial S}{\partial V})_{T}}{(\frac{\partial S}{\partial p})_{V}}$ (by reciprocal rule) (ii) $TdS = dU + pdV$ => $T (\frac{\partial S}{\partial V})_{T} = (\frac{\partial U}{\partial V})_{T} + p$ and $T (\frac{\partial S}{\partial p})_{V} = (\frac{\partial U}{\partial p})_{V}$ So $(\frac{\partial p}{\partial V})_{T} - (\frac{\partial p}{\partial V})_{S} = p [\frac{(p^{-1}\frac{\partial U}{\partial V})_{T} + 1}{(\frac{\partial U}{\partial p})_{V}}]$ Then observe $dS = (\frac{\partial S}{\partial V})_{T}dV + (\frac{\partial S}{\partial T})_{V}dT $ So $dp = [(\frac{\partial p}{\partial V})_{S} + (\frac{\partial p}{\partial S})_{V}(\frac{\partial S}{\partial V})_{T}]dV + (\frac{\partial p}{\partial S})_{V}(\frac{\partial S}{\partial T})_{V}dT$ => $(\frac{\partial p}{\partial U})_{V} = (\frac{\partial p}{\partial S})_{V} (\frac{\partial S}{\partial T})_{V} (\frac{\partial T}{\partial U})_{V}$ Then by Chain Rule (using our expression for $dS$) $(\frac{\partial S}{\partial p})_{V} = (\frac{\partial S}{\partial T})_{V} (\frac{\partial T}{\partial p})_{V}$ => $(\frac{\partial p}{\partial T})_{V} = (\frac{\partial p}{\partial S})_{V}(\frac{\partial S}{\partial T})_{V}$ And so (using the results above and the Reciprocal Rule): $(\frac{\partial p}{\partial V})_{T} - (\frac{\partial p}{\partial V})_{S} = p [\frac{p^{-1}(\frac{\partial U}{\partial V})_{T} + 1}{(\frac{\partial U}{\partial p})_{V}}] =  p (\frac{\partial p}{\partial U})_{V} [{p^{-1}(\frac{\partial U}{\partial V})_{T} + 1}] = p (\frac{\partial p}{\partial S})_{V} (\frac{\partial S}{\partial T})_{V} (\frac{\partial T}{\partial U})_{V} [{p^{-1}(\frac{\partial U}{\partial V})_{T} + 1}] = p (\frac{\partial p}{\partial T})_{V} [\frac{ {p^{-1}(\frac{\partial U}{\partial V})_{T} + 1}}{(\frac{\partial U}{\partial T})_{V}}]$ (This is where I am stuck) ------------------------------------------------------------------------------------------- Comments: The issue I have with the question (I assume) is that I am unable to derive the expression in the hint, i.e. showing that $(\frac{\partial \ln p}{\partial \ln V})_{T} = \frac{V}{p}(\frac{\partial p}{\partial V})_{T}$. I would assume that a very similar expression exists when keeping $S$ constant.","['partial-differential-equations', 'partial-derivative', 'calculus', 'multivariable-calculus', 'ordinary-differential-equations']"
2332689,Functional Derivative of Fourier Transform,"Say I have a functional $F$ which depends on the Fourier transform of $g^{2}(x_{1},x_{2},x_{3})$: $$F[g] = \int \mathcal{F}(g^{2}) d^{3}x$$ The Fourier transform is given by: $$\mathcal{F}(g^{2})=\int g^{2}e^{-i\vec{k}\vec{r}} d^{3}k$$ I was wondering if the following logic was sound and mathematically legal: $$\frac{\delta F}{\delta g} = \frac{\partial \mathcal{F}(g^{2})}{\partial g}$$ by Euler Lagrange Equation $$\frac{\partial \mathcal{F}(g^{2})}{\partial g}=\mathcal{F}(2g)$$ $$\frac{\delta F[g]}{\delta g} = \mathcal{F}(2g)$$","['derivatives', 'calculus-of-variations', 'fourier-analysis', 'calculus']"
2332698,Quadratic residues in finite field,"For an integer $a$ and a finite field $F_{q}$ of odd order, what is the efficient algorithm   to determine $a$ is Quadratic residue or not?","['finite-fields', 'quadratic-reciprocity', 'number-theory', 'algorithms', 'quadratic-residues']"
2332710,"Use Inclusion-Exclusion to find the number of arrangements of 4 A's, 5 B's, 6 C's, 7 D's , 8 E's with exactly 2 adjacent C's","I found the number of arrangements in the problem stated above by arranging the letters other than C and then counting the number of ways to insert the C's into the gaps, giving an answer of $\displaystyle\hspace{1.2 in}\frac{24!}{4!5!7!8!}\binom{25}{5}\binom{5}{1}=281625478590456000$, but I would like to find out how to work this problem using Inclusion-Exclusion instead.","['inclusion-exclusion', 'combinatorics']"
2332722,Hartshorne Example V.1.4.1 - Why $C^2=\deg_C(\mathcal{L}(C)\otimes\mathcal{O}_C)$?,"In the book Algebraic Geometry by Hartshorne, Example V.1.4.1 says $C^2=\deg_C(\mathcal{L}(C)\otimes\mathcal{O}_C)$ holds due to Lemma V.1.3. Here, $C$ is a nonsingular curve on a nonsingular projective surface $X$ . The statement of Lemma V.1.3 is Lemma 1.3. Let C be an irreducible nonsingular curve on $X$ , and let $D$ be any curve meeting $C$ transversally. Then $$\#(C\cap D)=\deg(\mathcal{L}(D)\otimes\mathcal{O}_C).$$ However, since $C$ and $C$ itself does not meet transversally and also $\#(C\cap C)$ does not make a sense, we have to find a curve $D\in|C|$ that is transversal to C. If we find such $D$ , then we may conclude $C^2=C\cdot D=\#(C\cap D)=\deg(\mathcal{L}(D)\otimes\mathcal{O}_C)=\deg(\mathcal{L}(C)\otimes\mathcal{O}_C)$ . So I'm trying to find such $D\in|C|$ . One sufficient condition is to find a very ample divisor $E\in|C|$ . In that case, almost curves $D\in|C|$ are transversal to $C$ by Lemma V.1.2. So my question is, If $C$ is an irreducible nonsingular curve on a nonsingular projective surface $X$ , is there always a very ample divisor $E\in|C|$ ? If the question 1 is not the case, why is there a curve $D\in|C|$ that meets $C$ transversally?","['intersection-theory', 'algebraic-geometry']"
2332754,Distribution of $i$th largest entry in multinomial sample,"I have a $k$-class multinomial distribution with vector of probabilities $(p_1, p_2, \ldots, p_k)$, from which I draw a size-$N$ sample $(c_1, c_2, \ldots, c_k), \sum\limits_{s=1}^k c_s = N$. Assume for simplicity that $c_j \geq 1$ for all $j$. Next, I sort the sample in descending order by count to obtain $(c_{(1)}, c_{(2)}, \ldots, c_{(k)})$ where $c_{(\ell)} \geq c_{(\ell+1)}$. Are there any research papers or other resources that describe the distribution over $c_{(i)}$, the $i$th largest count in a multinomial sample? For example, the distribution over $c_{(1)}$ is the distribution over the largest count and $c_{(k)}$ is the distribution over the smallest. Or, asked a different way, what is the distribution describing the probability that the $j$th class's count is the $i$th largest in the sample? I've been googling around for a while but haven't really found anything relevant.","['probability', 'probability-distributions']"
