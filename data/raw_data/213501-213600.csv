question_id,title,body,tags
4318153,Recurrence relation on a set of integers. Do we have $v_{n+q} = \sum_{j=0}^{q-1}\lambda_{j}v_{n+j}$?,"Order the set of positive integers of the form $2^{a}3^{b}$ $(a,b \in \mathbb{N}\cup\{0\})$ as $v_{1}<v_{2}<...$ . I.e $v_{1} = 1, v_{2} = 2^{1}3^{0} = 2, v_{3} = 2^{0}3^{1} = 3, v_{4} = 4, v_{5} = 6$ and so on. Does there exist $q \in \mathbb{N}$ and constants $\lambda_{0},...,\lambda_{q-1} \in \mathbb{R}$ so that $$v_{n+q} = \sum_{j=0}^{q-1}\lambda_{j}v_{n+j}$$ whenever $n \geq 1$ ? I asked this question because I wanted to know whether we can get additive relations on certain sequences where the prime factorization properties of the studied sequence are somewhat well known (thereby somewhat linking addition and prime factorization). Edit: We know that $\lim_{n\rightarrow \infty}\frac{v_{n+1}}{v_{n}} = 1$ (as mentioned by Apass.Jack) and prove this result down here; Since $\log_{2}(3)$ is irrational, the numbers $\log_{2}(3), 2\log_{2}(3),...$ are equidistributed modulo $1$ . Hence for each $k \in \mathbb{N}$ and $u \in \{0,1,...,k-1\}$ there exists $j_{u,k} \in \mathbb{N}$ for which $$j_{u,k}\log_{2}(3) \equiv \eta_{u,k} \mod 1$$ where $\eta_{u,k} \in [\frac{u}{k},\frac{u+1}{k}]$ . Next choose $r_{u,k} \in \mathbb{Z}$ so that $r_{u,k}+j_{u,k}\log_{2}(3) = \eta_{u,k}$ . Hence for all $n$ large enough for which $v_{n} \geq \max_{u \in \{0,...,k-1\}}\{3^{j_{u,k}}\}$ there exists $\tau(n,k)\in \mathbb{N}$ for which $\log_{2}(v_{n+\tau(n,k)}) - \log_{2}(v_{n}) \leq \frac{2}{k}$ Hence $$\limsup_{n\rightarrow \infty} \frac{v_{n+1}}{v_{n}} \leq 2^{\frac{2}{k}}$$ By taking $k$ to infinity we know that the limit is 1.","['number-theory', 'recurrence-relations', 'elementary-number-theory']"
4318165,"Let $|G|=735$. If the number of Sylow $7$-subgroups are more than $1$, then show that there exists a normal Sylow $5$-subgroup.","Let $|G|=735$ . If the number of Sylow $7$ -subgroups are more than $1$ , then show that there exists a normal Sylow $5$ -subgroup. To solve this problem we need to show that if there are more than 1 sylow 7-subgroup then there is only one sylow-5 subgroup. I tried by assuming that G has more than 1 sylow 7-subgroup and sylow 5-subgroup and I hoped that by assuming this I will get no. of elements will be more than 735 , it didn't happen.","['group-theory', 'normal-subgroups', 'finite-groups', 'sylow-theory']"
4318210,Confusion with what the metric gives when mapping a surface to the complex plane,"Tristan Needham Visual Differential Geometry,pg-32 In the beginning, I understood the metric as the factors by which the length of displacement on surface and on the plane relate. But, in the book the following formula is given and suggests to me that separation vectors on the surface are related to separation vectors in the plane: $$ d \hat{s} = \lambda(z, \gamma)dz$$ With, $dz = e^{i \gamma} ds$ My doubt is that the separation vector on the surface is existing in the tangent plane at the base point $\hat{z}$ and is three dimensional, so how could it possibly be related to the complex separation vector $dz$ existing in the flat plane?",['differential-geometry']
4318245,How degree of freedom of lines in 3D space is 4?,"I asked this question that degree of freedom of lines in 2D plane is 2. Proof:- Suppose take two points $(x_1,y_1), (x_2,y_2).$ So therefore degree of freedom is 4.But subtract 2 because their position on the line is irrelevant. My question is what does mean of ""subtract 2 because their position on the line is irrelevant ""? And I also read from that question degree of freedom of line in 3D space is 4. My approach is, we take two arbitrary points to draw line in 3D.It has 6 degrees of freedom. But why we subtract 2 from 6? Don't understand. N. B. - I want to understand intuition rather than complex mathematics.","['geometry', '3d', 'plane-geometry']"
4318280,Verify that $X_n$ is a martingale,"Let $P_{ij}=e^{-i}i^j/j!,\;i,j=0,1,...$ be the transition probabilities for a Markov chain $X_n$ . We consider $P_{00}=1$ . (a) Verify that $X_n$ is a martingale. (b) Derive the inequality $$\Pr\{\max_{0\leq n<\infty} X_n\geq a\mid X_0=i\}\leq i/a$$ for $i,a,=1,2,...$ . (c) Prove that $\lim_{n\rightarrow\infty}X_n=0$ with probability one. (a) I am having trouble showing that $X_n$ is a martingale. Can I get some help with this part? I know I need to show that $E[X_{n+1}\mid \mathcal{F}_n]=X_n$ . Then using the definition of expectation, I'd get $E[X_n]=\sum_j^\infty jP(j)$ . How do I know that I sum over $j$ and not over $i$ ? For part (b), since $X_n$ is a martingale, we can consider a stopping time $T=\max_{0\leq a<\infty}(X_n)$ and the stopped martingale $X_{T\wedge a}$ . Then we have $$i=X_0=E[X_{T\wedge a}]=E[X_{T\wedge a}\cap T\geq n]+E[X_{T\wedge a}<n]\geq aP(T\geq n)+0$$ Thus, we have $P(T\geq a)\leq \frac{1}{a}$ . (c) I am not sure about this part but I would like to understand part (a) first because I would like to see how to verify that $X_n$ is a martingale from the transition probabilities.","['stochastic-processes', 'statistics', 'probability-theory', 'martingales']"
4318375,"Prove $ \int_0^t 2X_s \ dX_s = X_t^2-X_0^2-\langle X, X\rangle_t $ WITHOUT Ito's formula","Suppose $X$ is a continuous local martingale (on some complete filtered probability space). Then I want to prove that $$
\int_0^t 2X_s \ dX_s = X_t^2-X_0^2-\langle X, X\rangle_t
$$ where $\langle X , X \rangle$ denotes the quadratic variation process of $X$ . One way to go about this is using Ito's formula but I want to see if I can give a direct proof of this just using properties of the stochastic integral. What I have thought about so far is that by definition of the quadratic variation process the above yields that $$
\int_0^t 2X_s \ dX_s - X_0^2
$$ is a continuous local martingale. Maybe this could be used along with the fact that the quadratic variation process is unique to show the relation? It almost also looks like that $(a+b)^2=a^2+b^2+2ab$ could play a role. Maybe this would lead to too many terms but some of them could be indistinguashable from 0 (by showing they were both a continuous local martingale and a finite variation process). The textbook I have is ""Brownian Motion, Martingales, and Stochastic Calculus"" by Jean-Francois Le Gall so any result prior to the section on Ito's formula is available.","['stochastic-integrals', 'martingales', 'probability-theory']"
4318386,A triangle has one vertex at a circle's center and two vertices on the circle. Can the three enclosed regions have rational areas?,"A triangle has one vertex at a circle's center and two vertices on the circle. Can the three enclosed regions have rational areas? Let $r=$ radius of circle, $\theta=$ angle at vertex of triangle at center of circle. Assume the three regions have rational areas. The area of the circle is rational, so $r^2$ is a rational multiple of $1/\pi$ . Then (since the area of the triangle is rational) $\sin\theta$ is a rational multiple of $\pi$ , and (since the area of the segment is rational) $\theta$ is a rational multiple of $\pi$ . So I think the question is equivalent to: Can $\theta$ and $\sin\theta$ both be rational multiples of $\pi$ ? ( $0<\theta<\pi$ ) I thought about Niven's Theorem , but it doesn't seem to help. (I suspect the answer is no.)","['number-theory', 'trigonometry', 'geometry', 'real-analysis']"
4318473,"How to solve this estimate $m\{t \in[a, b]:|f(t)| \leq \varepsilon\} \leq 4\left(q ! \frac{\varepsilon}{2 \beta}\right)^{\frac{1}{q}}$","(a) Assume that the function $f:[a, b] \rightarrow \mathcal{R}$ with $a<b$ is differentiable and satisfies $\left|f^{\prime}(t)\right| \geq \beta$ for all $t \in[a, b]$ for some $\beta>0 .$ Prove the following estimate $$
m\{t \in[a, b]:|f(t)| \leq \varepsilon\} \leq \frac{2 \varepsilon}{\beta} \quad \text { for } \varepsilon>0
$$ where $m\{B\}$ denotes the Lebesgue measure of set $B$ . (b) Assume that the function $f:[a, b] \rightarrow \mathcal{R}$ with $a<b$ is $q$ -times continuously differentiable and satisfies $\left|f^{(q)}(t)\right| \geq \beta$ for all $t \in[a, b]$ for some positive integer $q$ and $\beta>0 .$ Prove the following estimate $$
m\{t \in[a, b]:|f(t)| \leq \varepsilon\} \leq 4\left(q ! \frac{\varepsilon}{2 \beta}\right)^{\frac{1}{q}} \quad \text { for } \varepsilon>0
$$ where $m\{B\}$ denotes the Lebesgue measure of set $B$ . I have proved $(a)$ . Noting that we can assume that $f^{\prime}(t)\geq \beta$ and $a= 0$ , so $f(t)\geq \beta t+f(0)  $ $(*)$ . However, it's easy to show that for any linear function with slope $\beta$ , $(a)$ is correct. By $ (*)$ one can prove $(a)$ for all differentiable $f$ with $\left|f^{\prime}(t)\right| \geq \beta$ . For (b), the method we use in (a) is out of work, since we cannot get the inequality like $(*)$ . I
also tried to prove it by induction but falied. May anyone give me some hint or correct solution? Thanks in advance!!!","['integration', 'measure-theory', 'lebesgue-measure', 'analysis', 'real-analysis']"
4318497,Line integral of non conservative vector field,"I'm having trouble finding the line integral of this problem. I have been given a vector field $F=(2x\sin(\pi y)-e^z,\pi x^2\cos(\pi y)-3e^z,-xe^z)$ Where the curve $C$ intercepts between $z=\ln(1+x)$ and $y=x$ from $(0,0,0)$ to $(1,1,\ln(2)$ . So I try to do this by solving with: $\int_{C}^{} F \cdot dr$ I started of by determining conservativity and it is non conservative, $\frac{\partial f_1}{\partial y} = \frac{\partial f_2}{\partial x}=2x\pi \cos(\pi y)$ $\frac{\partial f_1}{\partial z} = \frac{\partial f_3}{\partial x}=-e^z$ $-3e^z=\frac{\partial f_2}{\partial z} \ne \frac{\partial f_3}{\partial y}=0$ Next I found the vector function by using the two coordinats $(0,0,0)$ & $(1,1,\ln(2)$ , $r(t)=(t)\hat{i}+(t)\hat{j}+(\ln(2)t)\hat{k}$ From this vector function we could say that, $x=t$ , $y=t$ & $z=\ln(2)t$ Now usually I believe I should substitute $(x,y,z)$ into $F$ and then take the dot product between $F$ and $r'(t)$ . But I'm not sure this correct, because I also have to take $z=\ln(1+x)$ and $y=x$ into consideration. So now I'm stuck on how to proceed. What am I supposed to do with $z=\ln(1+x)$ and $y=x$ ? And is it even the correct approach to this problem?","['multivariable-calculus', 'calculus', 'line-integrals', 'vector-fields']"
4318528,Normal distribution of the mean of a uniform distribution,"I have $\bar{X}$ which is the mean of the numbers from the uniform distribution of $[0, 1]$ with $n = 100$ . I know that $\mu = \frac{1}{2}$ and $\sigma^2 = \frac{1}{1200}$ , thus, $\sigma \approx 0.028 $ , I need to find the probability of $\bar{X}$ having a value between $[0.47, 0.53]$ . Calculating the normal distribution, I find $1 - 2(0.3508) = 0.2984$ , however the book says the answer is $0.7016$ . It is easy to see that $1 - 0.2984 = 0.7016$ , and it makes me sure that I'm in the right path. I just don't know why I should subtract the value I found in the distribution from $1$ , and it makes me think the book forgot one step before finishing the exercise. Could someone clarify this for me?","['statistics', 'normal-distribution']"
4318564,Calculate and prove the limit $\lim\limits_{n\to\infty }(1+\frac{1}{a_n})^{a_n}$ Given $a_n$ is an increasing monotone sequence of integers,"$\lim\limits_{n\to\infty }(1+\frac{1}{a_n})^{a_n}$ given $(a_n)$ is an increasing sequence of integers NOTE - I noticed these 2 questions here (1) and here (2) but I believe that my question is a bit different because these questions answer only the case of $a_1 \geq 0$ according to the information we can understand that if $a_1 \geq 0$ then it is immediately solved as $\lim\limits_{n\to\infty }(1+\frac{1}{a_n})^{a_n} =e$ since then we can look at $a_n$ as in $a_n=n$ and then the limit would be just an identity $\lim\limits_{n\to\infty }(1+\frac{1}{n})^{n}=e$ otherwise what if $a_1 <0$ ? we will need to show that  there exists an $N \in \Bbb N$ such that $a_N \geq 0$ we can also understand that $a_{n+1}-a_n \geq 1$ then for ever $n$ we get $a_{n+1}-a_n = (a_{n+1}-a_n)+(a_n-a_{n+1})...+(a_2 - a_1) \geq 1+1+1...+1 =n$ I do not know how to continue from here.. I usually have more ideas and stuff I tried on my posts but I really cannot figure out what to do here. thanks for any help and tips!
edit - Thanks to all the comments I tried and cannot figure out on how to actually prove it , I do realize what the limit is worth now and why but I am struggling to prove it as I stated before EDIT (point of the edit is to solve it organized and in a better way according to information I have and from the comments) -
Organizing information a. $(a_n)$ is an increasing sequence therefore we get $(1)$ $a_{n+1} > a_n$ , $(2)$ thanks to nejimban an increasing sequence of integers must tend to $\infty$ $(3)$ an increasing sequence of integers therefore $a_{n+1}-a_n \geq 1$ b. $\lim\limits_{n\to\infty }(1+\frac{1}{n})^{n}=e$ c. Need to prove that there is an $N \in \Bbb N$ such that for every $n>N$ we get $a_n>0$ Solving - posted as an answer to the question","['real-analysis', 'solution-verification', 'eulers-number-e', 'limits', 'sequences-and-series']"
4318662,Proof that Dirichlet series $\sum_{n=1}^{\infty}\frac{2^{\omega(n)}}{n^2}=\frac{5}{2}$,"So I want to prove the following: $$\sum_{n=1}^{\infty}\frac{2^{\omega(n)}}{n^2}=\frac{5}{2},$$ where $\omega(n)$ is the number of distinct prime factors of $n.$ I computed it to $10^{10}$ and it does seem to be slowly approaching $\frac{5}{2}.$ Also, I am aware of the following result: $$\sum_{n=1}^{\infty}\frac{\omega(n)}{n^2}=\zeta(2)P(2),$$ where $P(2)$ is the prime zeta function. I am not quite sure how exactly to go about this, there doesn't seem to be a way to get from the known result to the one I'm trying to solve.","['dirichlet-series', 'number-theory', 'elementary-number-theory', 'sequences-and-series']"
4318709,Prove that every ray is a polyhedron,"I am reading a book on linear optimization and I am stuck with the following problem: Prove that every ray in $\mathbb{R}^n$ is a polyhedron. The book defines a ray as follows: For a point $x_0\in\mathbb{R}^n$ and a vector $d\in\mathbb{R}^n$ , a ray is the set $$\{x_0+\lambda d \mid \forall \lambda \in \mathbb{R} \, \text{such that} \, \lambda \ge 0\}$$ It also defines a polyhedron as a set of solutions to a system of linear inequalities. My idea is to find a linear transformation $A$ , whose kernel is the subspace spanned by $d$ . Consequently, the set of solutions to the equation $Ax=Ax_0$ will be a line containing the ray. But there is two problems. First I do not know how to find such a $A$ . Second, I do not know what to do next. Any help is appreciated.","['convex-analysis', 'polyhedra', 'linear-algebra', 'linear-programming']"
4318715,"Dual space of $\mathcal{C}^n [a,b]$.","I just started reading a few days ago about Banach algebras using the Kaniuth's book. In this, it is said that the space $\mathcal{C}^n [a,b]$ of $n$ -times continuously differentiable functions is a Banach algebra with the norm $$||f|| := \sum_{k=0}^n \frac{1}{k!} ||f^{(k)}||_\infty, \quad \forall f \in \mathcal{C}^n [a,b].$$ So I have a question, what is the (topological) dual of this space? It is simple curiosity, maybe it is isomorphic to a known space, or to the direct sum of known spaces, I don't have idea.","['banach-spaces', 'complete-spaces', 'banach-algebras', 'functional-analysis', 'dual-spaces']"
4318763,"In calculus, how should I interpret the -1 superscript in trigonometric functions?","In calculus, and in the context of differentiating functions for practice, how should I interpret the following expression (i.e., what is the convention here) $$f(x) = \tan^{-1}(x)$$ Should I treat it as the inverse of tan or just $$\frac{1}{\tan(x)}$$","['notation', 'calculus', 'inverse-function', 'trigonometry']"
4318803,"A variant of the ode $\dot x=y, \dot y=-x$","The ordinary differential equation $\dot x(t)=y(t)$ and $\dot y(t)=-x(t)$ clearly has a family of solutions given by $$
x(t)= r\cos(t), y(t)=-r\sin(t), t\in\mathbb R
$$ Now, let $f:(0,\infty)\to(0,\infty)$ be a positive smooth function. I want to study a modified differential equation system $$
\dot x=f(x^2+y^2) \cdot y \\
\dot y=-f(x^2+y^2) \cdot x
$$ We cannot find explicit solutions, but I was wondering if we could still prove that there exist solutions for all $t\in\mathbb R$ .","['ordinary-differential-equations', 'partial-differential-equations']"
4318839,Taking a limit inside an integral,I would like to see a rigorous proof that: $$\lim_{R\rightarrow \infty}\int_1^R \left(\frac{1}{\sqrt{x^2-1}}-\frac{1}{x}\right)\frac{dx}{\sqrt{1-x^2/R^2}}=\int_1^\infty \left(\frac{1}{\sqrt{x^2-1}}-\frac{1}{x}\right)dx.$$ Note that the second integral does converge (to $\log 2$ ). This probably follows from some basic theorems in integration theory but I am rusty in that area...I cannot see how to do it with the dominated convergence or monotone convergence theorems. Some reference to the relevant theorems would be great. Thanks for any help.,"['integration', 'limits']"
4318841,Asymptotic expansion of inhomogenous differential equation,"Consider $$\tag{1}
y'(x)+y(x)=\frac{1}{x}
$$ For reference, the exact solution is $$\tag{2}
y(x)=e^{-x}(C+\operatorname{Ei}(x))
$$ Where $\operatorname{Ei}$ is the exponential integral and $C$ is the integration constant. I want to study the/a particular solution of (1) as $x \to 0^+$ . Using dominant balance, I have found $y \sim \ln x$ . This matches the logarithmic singularity carried by $\operatorname{Ei}$ . Question : is it possible to say anything about the next to leading order terms of the particular solution using asymptotic analysis? Ie. by manipulating (1) and not just expanding (2). Expanding $\operatorname{Ei}$ suggests the next term should be $\gamma$ , Euler's constant . My thoughts : I think the answer is 'no' because a constant term could be absorbed into the constant of integration, $C$ . This leaves me vaguely uneasy. Working : There are three dominant balances to consider  in (1). The consistent one uses $y\ll x^{-1}$ $$
y'\sim x^{-1} \qquad, \qquad x \to 0
$$ This can be directly integrated $$
y(x)=\ln(x)+A(x)
$$ Where $A'(x)\ll x^{-1}$ as $x\to 0$ . We may substitute into (1) to find a differential equation for $A$ $$
A'+A=-\ln(x)
$$ The dominant balance that neglects $A$ leads to $$
A(x)=-x \ln(x)+x+B(x)
$$ Where $B'\ll \ln x$ . The differential equation for $B$ is $$\tag{3}
B'+B=x \ln(x)-x
$$ And now there are two consistent dominant balances to consider, and the RHS vanishes at zero. At this point I think something must have been missed, because no constant terms appear in our expansion (4) between the singular $\ln(x)$ and finite $x \ln(x)$ terms $$\tag{4}
y \sim \ln(x) - x \ln(x) +x \qquad ,\qquad x \to 0
$$ After playing around with it, I noticed that continuing to neglect the non-derivative terms in (3) and beyond leads to repeated integrals over $x \ln(x)$ , which can be done. Spotting the pattern then summing up the terms, I find $$
y \sim p(x)+e^{-x}\ln(x) \qquad , \qquad x \to 0
$$ Where $p(x)$ is a series in only positive powers of $x$ . Context : The equation (1) comes from this question .","['asymptotics', 'ordinary-differential-equations']"
4318846,"If $X$ and $Y$ are independent standard normal random variables, and $U=X+Y$, $V=\frac{X}{Y}$ then $f_V(v) = \frac{1}{\pi (1+v^2)}$","If $X$ and $Y$ are independent standard normal random variables, and $U=X+Y$ , $V=\frac{X}{Y}$ prove that $$f_V(v) = \frac{1}{\pi (1+v^2)}$$ Solution: I know that $f_{xy}(x,y)=\frac{e^{-\frac{x^2+y^2}{2}}}{2\pi}$ Also, $$u=x+y \ \ \ \ \ \ AND \ \ \ \ \ \ v = \frac xy$$ Then $\frac{uv}{1+v}=x$ and $\frac{u}{v+1}=y$ Then the Jacobian would be $J=\frac{-u}{(v+1)^2}$ So: $$f_{u,v}=f_{xy}(h^{-1}(u,v),g^{-1}(u,v))*|J|=$$ $$\frac{e^{-\frac{u^2(1+v^2)}{2(1+v)^2}}}{2\pi}*\frac{u}{(v+1)^2}$$ My question now is, how can I prove that: $$f_V(v)=\int^{\infty}_{-\infty}\frac{e^{-\frac{u^2(1+v^2)}{2(1+v)^2}}}{2\pi}*\frac{u}{(v+1)^2}du=\frac{1}{\pi(1+v^2)}$$","['statistics', 'normal-distribution', 'density-function']"
4318864,"I need help understanding the concept of ""at most one "" using quantifers","$$∀x∀y ((\text{Cube}(x) ∧ \text{Cube}(y)) → x = y)$$ To me, this statement means that for all cubes x and y, if you pick up a cube from the box, it will always be the same cube. I understand this to mean that there is exactly one cube in the box. I don't understand how the statement means that there is at most one cube in a box.","['logic', 'discrete-mathematics', 'logic-translation']"
4318869,"Is it possible for a set of random variables to each be highly correlated with another variable, but not highly correlated with each other?","Let $X_1, ..., X_n$ and $Y$ be random variables. Is it possible for the $X_i$ 's to all have a high magnitude of correlation against $Y$ (absolute value of Pearson's $r$ ), but not be strongly correlated with each other?","['correlation', 'statistics', 'probability', 'random-variables']"
4318877,It is possible for a scalar finite-duration continuous system to achieve an infinite speed (in finite-time)? How if true? Why not if false?,"It is possible for a scalar finite-duration continuous system to achieve an infinite speed (in finite-time)? How if it true? Why not if it false? (Please read first the restrictions of the system I am interested in). Since new information has rise, now I am actually more interested in the ""Added Later"" part I have change the term ""time-limited"" for ""finite-duration"" since is more accurate and widely used, but it means that the scalar one-variable function $f(t)$ has an starting time $t_0$ for which $f(t) = 0\,\forall t<t_0$ , and also an ending time $t_F$ from which $f(t) = 0\,\forall t>t_F$ , with $t_0 < t_F$ To avoid some ""bad-behaved"" functions, I am thinking in functions $f(t)$ as a position of a classical object that change with time, with the function describing its position restricted as: $f(t)$ is continuous, including it been so in the following way: $\forall\,t_d,\,f(t_d^-) = f(t_d^+)$ so there is no ""jump-like discontinuities"" in the function $f(t)$ , so no ""teleportation"" is allowed. With this, I am avoiding the existence of delta functions components $\delta(t)$ in the derivative $f'(t)$ because of jump discontinuities (I don´t know if could have them because of other reasons). Also, I am thinking in well-behaved ""classic"" continuous functions, not things like a Brownian motion which is continuous and nowhere differentiable (I don´t know how to ""formally"" described this restriction - I hope you get the idea). The function $f(t)$ is of finite-duration, so compact-supported since is a one variable function (scalar). Since is continuous and compact-supported is also bounded, so $\sup\limits_t |f(t)| = \|f(t)\|_\infty < \infty$ . The function $f(t)$ has a well defined Fourier transform $F(w)$ (analytic, and follows the Riemann-Lebesgue Lemma). Since for a compacted-supported function the Fourier transform could have some ""issues"" at the boundaries of it domain $\partial t = \{t_0,\,t_F\}$ , you could assume that $f(t_0)=f(t_F)=0$ if that makes it more easy to work with ( here I found a way to overcome this issues for arbitrary finite $\{f(t_0),\,f(t_F)\}$ so generality is sustained). The function $f(t)$ have finite energy $\int_{t_0}^{t_F} |f(t)|^2 dt < \infty$ , and is also Lebesgue integrable $\int_{t_0}^{t_F} |f(t)| dt < \infty$ (later I learned that if continuous and compacted supported, then is bounded, and this imply that the Fourier Transform is analytic, and then adding is Lebesgue Integrable implies then is of finite energy since is bounded, and I believe all this implies then the Riemann-Lebesgue follows, but not quite sure about the last affirmation) . I believe that if the function $f(t)$ have sharp-edges/tips/spikes like the absolute-value function at the origin, the first derivative (speed) $f'(t)$ will be discontinuous, but since $f(t)$ is bounded the ""jump"" in the derivative will be also bounded: as example, let $f(t) = \sin(|t|\pi),\,|t|\leq 1$ (plot here ), it have a sharp edge at $t=0$ so its derivative have a jump-discontinuity, but $\sup\limits_t |f'(t)| = \pi$ . At first glance, I thought that to have a spike with $\sup\limits_t |f'(t)| \to \infty$ at some time $t_d$ it has to have some $|f(t_d^-)| = |\lim\limits_{t \to t_d^-} f(t)| \to \infty$ and/or $|f(t_d^+)| = |\lim\limits_{t \to t_d^+} f(t)| \to \infty$ making the function $f(t)$ discontinuous, which is not allowed (as counterexample, $f(t) = 1/|t|$ has an ""infinite jump"" in $f'(t)$ ), but then I found the function $f(t) = \left|t \cdot \displaystyle{\frac{\log(t^2)}{2}}\right|,\,|t|\leq 1$ which have a sharp edge where an infinite speed is achieved with an infinite-size jump-discontinuity on $f'(t)$ without being $f(t)$ a discontinuous function (it also has bounded variation equal to $4/e$ even given it has a discontinuity on its derivative). Nevertheless, note that this functions can achieve these infinite speeds only at isolated/disjointed single points (of measure zero), or it will have a sudden position change with zero time, creating a jump-discontinuity in $f(t)$ which is not allowed from the assumptions (continuity part). With this in mind, the second derivative (acceleration) $f''(t)$ , must be allowed to have infinite jump discontinuities, or no abrupt changes of direction could be allowed, like crashing and bouncing from a wall, so the jump-discontinuity in $f'(t)$ will become a delta function $\delta(t) \equiv \infty$ in the acceleration profile (as example for $f(t) = \sin(|t|\pi),\,|t|\leq 1$ , its second derivative is $f''(t) = 2\pi\delta(t)-\pi^2 \sin(|t|\pi),\,|t|\leq 1$ ). So acceleration is necessarily unbounded. Also, speed must be allow to have discontinuities or no sudden changes in direction would be achieved - meaning this that smoothness is a too restrictive for this function, discarding Analytic solutions! . So, If I am right, an unbounded acceleration makes possible to achieve an infinite speed on a finite time, but it doesn´t mean that there are other laws making impossible to it to be unbounded - law which I am looking for. This behavior can be seen in the following examples: $f(t) = \sqrt{1-t^2},\,|t|\leq 1$ , which starts/ends with $|f'(t)| \to \infty$ as can be seen on its plot here . $f(t) = t \cdot \displaystyle{\frac{\log(t^2)}{2}},\,|t|\leq 1$ , which ""softly"" achieve $|f'(t)| \to \infty$ at $t=0$ as can be seen on its plot here . $f(t) = \left|t \cdot \displaystyle{\frac{\log(t^2)}{2}}\right|,\,|t|\leq 1$ , which achieve $|f'(t)| \to \infty$ at $t=0$ with an infinite-size jump-discontinuity in $f'(t)$ , as can be seen on its plot here . So the main question of the beginning can be divided as: It is possible for a real-life classic mechanical system to behave as the last examples? So achieving and infinite speed in a finite time? Here, I think that a model that can achieve an infinite speed, even if is only in one point in time, will violate every possible physics model with a finite speed for causality, but I haven´t found yet how causality conditions will restrict the derivative of these finite-duration functions. Can you think of examples of real life classic mechanic systems that behaves as the last examples? (I can´t made yet by myself an idea of an example). There are other ways of achieving an unbounded maximum rate of change for a classic mechanic system that are not included in the scenarios I mention?? (keeping the restrictions over $f(t)$ ). If is not possible, Which physics laws are avoiding it to happen? (here meaning that the maximum rate of change must be bounded because of these laws) . Here is where the problem of the bounds of the domain can make struggles in the Fourier Transform, since $\sup\limits_t |f'(t)| \leq \frac{1}{2\pi} \int_{-\infty}^\infty |w F(w)|dw$ which can fictitiously diverge because of the effect of the discontinuities at the ""edges"" of the time-limited compact-support, but they can be avoided as is explained here by using instead $\sup\limits_t |f'(t)| \leq \frac{1}{2\pi} \int_{-\infty}^\infty |iw F(w)+f(t_F)e^{-iwt_F}-f(t_0)e^{-iwt_0}|dw$ . If there are mistakes in my argumentation, please let me know what assumptions/lines-of-thought are wrong. Added Later I have found recently two papers from the author V. T. Haimo (1985), that analyze finite-duration differential equations [ 1 ] and [ 2 ]. From them, I realize I could be mixing two things into the question: first, math actually could stand any function cropped at some compact-domain to be treated as a finite-duration function, and second, that it is not really what I want to know, actually what I am looking for are for solutions to differential equations which are of finite-duration . On the papers the author explain that: ""One notices immediately that finite time differential equations cannot be Lipschitz at the origin. As all solutions reach zero in finite time, there is non-uniqueness of solutions through zero in backwards time. This, of course, violates the uniqueness condition for solutions of Lipschitz differential equations."" Since, linear differential equations have solutions that are unique, and finite-duration solutions aren´t, finite-duration phenomena models must be non-linear differential eqs. to show the required behavior (non meaning this, that every non-linear dynamic system support finite-duration solutions). Also, since the system ""dies"" at the end of the domain, the solutions will have the same issues than compacted-supported functions in this ending point, which will leads that finite-duration solutions cannot be Analytic in the whole time domain (maybe using functions defined piece-wise could work, like common bump-functions $\in C_c^\infty$ are defined, but no restricting the starting point to be also zero - which is a requirement for bump-functions for keeping smoothness). Note: discarding ""whole-domain analytic functions"" like Power Series, and also Linear ODEs, actually ""discards"" almost-all the maths knowledge I acquire on engineering, so this is totally new for me. The papers also show which conditions must fulfill the non-linear differential equation to support finite-duration solutions, at least for first and second order scalar ODEs. And in [1] on Theorem 2 point (i), it is said that, without losing generality by considering that the ending time of the finite-duration solution happens at $t_F = 0$ , for a second order dynamical system described by $\ddot{x}(t) = g(x(t),\dot{x}(t))$ such $g(0,0)=0$ (the system dynamics ""die"" at $t_F = 0$ ), with $g \in C^1(\mathbb{R}\setminus \{0\})$ , then for the system to support finite-duration solutions, the following another differential equation must have solutions: $$q(z)\frac{dq(z)}{dz} = g(z,q(z)),\,q(0)=0$$ Honestly the papers are bit advanced to my mathematical skills, but if I didn´t made any mistakes, what the author is doing is splitting the second derivative of the scalar one-variable function $x(t)$ as: $$\ddot{x} = \frac{d}{dt}\frac{dx(t)}{dt} = \frac{d}{dt}\frac{dx}{dx}\frac{d\,x(t)}{dt} = \frac{dx}{dt}\frac{d}{dx}\frac{dx}{dt} = q(z)\frac{dq(z)}{dz}$$ by using the change of variable $z=x(t)$ and $q(z)=\dot{x}$ . I don´t really understand why this transformation leads to another differential equation that ""tells"" how the original equation will behave, so the following analysis is probably wrong, but I want to share it with you so you can correct me: Since I am looking from the maximum speed $\sup_t |\dot{x}|$ , and from the papers looks like I can figure out the behavior of $\dot{x}$ from $q(z)$ , I believe whatever it achieve a maximum, the values obtained should be the same, so finding $\sup_t |\dot{x}| \equiv \sup_z |q(z)|$ . With this, since $q(z)=0$ is not really a value I ""care"", I could use first order conditions to look for the maximum value of $q(z)$ , so I need to find $z$ such $$\frac{dq(z)}{dz}=0 \rightarrow z^* \rightarrow q(z^*)$$ So, since I am interested in $q(z) \neq 0$ , looking for the first order conditions is equivalent to looking for $q(z)\frac{dq(z)}{dz}=0$ , which is indeed the same that looking for $\ddot{x} = 0$ (if my assumption of interchangeability of equations is right - which I believe is not), so it would be meaning that finite-duration solutions of differential equations only can achieve their maximum speeds at inflection points of the acceleration profile where it is equal to zero $\ddot{x}=0$ , which instantly discard situations as the example $f(t) = t \cdot \displaystyle{\frac{\log(t^2)}{2}},\,|t|\leq 1$ , which ""softly"" achieve $|f'(t)| \to \infty$ at $t=0$ but at this points it second derivative is non-zero (actually diverges to infinity). This is quite an aggressive affirmation (so, probably wrong), since it can be reformulated as: finite-duration solutions to scalar-second-order differential equations, which are of unlimited bandwidth because of having finite-duration so they could achieve infinite speeds in principle, are actually restricted by being solutions of finite-time differential equations  (with $g(x, \dot{x}) \in C^1(\mathbb{R}\setminus \{0\})$ ) so they can achieve their maximum speeds only when it acceleration is zero discarding it of happening at discontinuities, so their maximum speeds are actually bounded .... this is actually too good to be true (maybe it happens because of the restriction on $g(x,\dot{x})$ ), but it is quite interesting to see which restrictions could rise for mechanical systems described by these finite-time differential equations, and I did not find too much information related to them so I think are quite unknown. So far I have only found nunerical representatuons of this finite-duration solutions, and maybe the mentioned paper is only reviewing the behavior near the ending point, but anyway, any example of a function that is a finite-duration solution of a differential equation will be preciated (I don't even know if a close-form is possible). Hope you can comment about.","['nonlinear-dynamics', 'ordinary-differential-equations', 'solution-verification', 'physics', 'finite-duration']"
4318902,If $f$ is an entire function of order $\lambda$ then $f'$ also has order $\lambda$,"If $f$ is an entire function of order $\lambda$ then $f'$ also has order $\lambda$ Can someone show me how to prove this or point me in the right direction? My definition is an entire function $f$ has finite order $\lambda$ if for $\epsilon>0$ we have $|f(z)| < $ exp $(|z|^{\lambda + \epsilon})$ for all $|z|$ sufficiently large. I also have that $\lambda = $ inf $\{ a : |f(z)| < $ exp $(|z|^{a}) $ for $|z|$ sufficiently large}, though I'm not sure if that will be useful here. I had to show the order of the sum of two functions is less than or equal to the max of the order of each function in an earlier problem, but I'm not sure how to go about this for $f'$ . Thanks for any help.","['complex-analysis', 'entire-functions']"
4318909,Laplace Transform piecewise function with domain from 1 to inf,"I have been asked to compute the Laplace Transform of the following piecewise function \begin{equation}
f(t) = \begin{cases}
t - 1 \quad 1 \leq t < 2 \\
3 - t \quad 2 \leq t < 3 \\
0 \quad t \geq 3
\end{cases}
\end{equation} I could not find an example where the domain of the piecewise function to transform does not begin at $0$ , I was wondering if it could be possible to shift the function to the left $f(t + 1)$ , so that the domain begins at $0$ , rewrite in terms of the unit step function and transform it. Is that something valid to do? or how should these cases be handled?","['laplace-transform', 'ordinary-differential-equations']"
4318959,Proving that a set generates $\Gamma_0(4)$,"I want to show that $$\Gamma_0(4) = \biggl\{\gamma = \pmatrix{a&b\cr c&d}\in {\rm SL}_2({\bf Z}) : 
c\equiv 0\pmod 4\biggr\}$$ is generated by the three matrices $$\pmatrix{1&1\cr 0&1},\quad\pmatrix{1&0\cr 4&1},\quad\hbox{and}\quad\pmatrix{-1&0\cr 0&-1}.$$ I tried to do this by showing that note that for any $\gamma = \bigl( {a\atop c}{b\atop d}\bigr) \in \Gamma_0(4)$ , we have $$\pmatrix{a&b\cr c&d}\pmatrix{1&-n\cr 0&1} = \pmatrix{a & b-na\cr c&d-nc}\qquad\hbox{and}\qquad
\pmatrix{a&b\cr c&d}\pmatrix{1&0\cr -4n&1} = \pmatrix{a-4nb & b\cr c-4nd&d}.$$ If $c$ is $0$ we are done, since $\gamma$ is in $\Gamma_0(4)$ in this case. Otherwise, if $|c|<|d|$ ,
we can apply the division
algorithm to get $q$ and $d'$ such that $|d| = |c|q + d'$ with $|d'| < |c|/2$ , and the first transformation
applied $q$ times produces a matrix whose bottom-left entry is $c$ and whose bottom-right entry is $d'$ . On
the other hand, if $d\ne 0$ and $|c|>4|d|$ , then we can find $q$ and $c'$ such that $|c| = 4|d|q + c'$ where $|c'| < 2|d|$ and we apply the second transformation $q$ times to get a matrix with bottom-left entry $c'$ . In each case, we have strictly reduced the quantity $\min\bigl\{|c|, 2|d|\bigr\}$ , so the process must terminate with $|c| = 0$ or $|d|=0$ . In the first case we have found an element of $\Gamma_0(4)$ , and the second case cannot happen, since it would imply that $c = \pm 1$ . I think this is the right idea except that we're missing the case where $|d|\le |c|\le 4|d|$ (correct me if there are any other holes in the proof other than this). I'm just not sure what to do in this case.","['matrices', 'modular-forms', 'group-theory']"
4318960,Understanding axiom of choice from examples,"$
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
$ I thought I understood AC (axiom of choice), but I am now totally confused through some discussions. Let me examine whether each of the following examples needs AC. Assume ZF. Choose $ x \in (0, 1) $ . AC is unnecessary. (One existential quantification over an interval.) Let $ n \in \N $ . Choose $ x \in (0,1)^n $ . AC is unnecessary because $x$ can be chosen by finite existential quantification. Let $ A = \{[a, b]| a,b \in \R,~a\le b\} $ . There is a function $f:A \to \R$ such that $ f(I) \in I$ for all $ I \in A$ . AC is unnecessary because we can declare a choice function $c: A \to \R$ defined by $ c(I) = \min I \in I $ for all $I \in A$ though $A$ is infinite. Let $ A = \{(a, b)| a,b \in \R,~a < b\} $ . There is a function $f:A \to \R$ such that $ f(I) \in I$ for all $ I \in A$ . AC is necessary because there is no finite quantification over $ A $ that specifies the choice of $x$ . Let $ A = \{[a, b] \cap \Q| a,b \in \R,~a < b\} $ and $ B = \{(a, b) \cap \Q| a,b \in \R,~a < b\} $ . Either there is a function $f:A \to \R$ such that $ f(I) \in I$ for all $ I \in A$ or there is a function $f:B \to \R$ .... AC is necessary by the same reasoning in 4. Let $ A = \{ (a,b)|a,b \in \Q,~a < b \} ​$ . There is a function $f:A \to \R$ such that $ f(I) \in I$ for all $ I \in A$ . AC is necessary by the same reasoning in 4 though $A$ is countable. I considered this example from the discussion by Karagila , which seems to be inconsistent with what I learned. Does countability assume some important role here? Would you figure out what I am missing about the notion of AC? Edit: I think I overlooked some simple factors in making examples. :) I would like to add one more example: Let $A \subseteq P(\R)$ . There is a function $f:A \to \R$ such that $ f(I) \in I$ for all $ I \in A$ . Then, is 7. the only case AC actually needed?","['elementary-set-theory', 'axiom-of-choice']"
4318969,Using the Dominated convergence theorem in a sequence of Indicator functions,"Let $Z_t\sim WN(0,\sigma^2)$ be a white noise. Consider a $\text{MA}(q)$ process: \begin{equation}
X_t^q = \sum_{j=0}^{q} \theta_j Z_{t-j}, \quad X_t = \sum_{j=0}^{\infty} \theta_j Z_{t-j}
\end{equation} where $\sum_{j=0}^{\infty} \theta_j^2 < \infty$ . Fix any $t$ and any $x$ , I want to show that: $$\lim_{q \to \infty}P(X_t^q \leq x ) = P(X_t \leq x)$$ For this, I tried the Dominated convergence theorem: Define $f_q = I_{[\,X_q \, \leq \, x\,]}$ and $f = I_{[\,X \, \leq \, x\,]}$ . It's easy to show that: $$\int f_q\,dP = P(X_t^q \leq x ), \quad \int f\, dP = P(X_t \leq x ) $$ Also, it's easy to show that $|f_q| \leq 1$ .
It only remains to show that  the sequence $f_q$ converges pointwise to $f$ and I'm having a little trouble showing this. I think that the solution have to do with this two items questions: $X^q_t \to X_t$ pointwise? How I can show this? The first item implies that $f_q \to f$ pointwise? Some help, pls!","['stochastic-processes', 'measure-theory']"
4319011,Conditions on stopping time being finite,"Let $(Y_n)_{n \in \mathbb{N}}$ be independent random variables taking values in $\{-1, 0, 1\}$ such that $EY_n = 0$ . Let the process $(X_n)_{n \in \mathbb{N}}$ with $X_n = \sum_{k = 1}^n Y_k$ . Let $\tau = \inf \{n : X_n = 1\}$ . For what conditions on $Y_n$ is $\tau$ finite almost surely? I know if $Y_n$ are iid, then if $P(Y_n = 0) < 1$ , then $\tau$ will be finite a.s. I am having trouble with this exercise though. It's seems possible to me that the condition is $\sum (1 - P(Y_n = 0)) = \infty$ . Any pointers?","['martingales', 'stopping-times', 'probability-theory', 'probability']"
4319034,Shortest path in conformal maps of a surface,"My intuition tells me that the shortest distance between two points on the surface corresponds to a line segment joining the two points on the map of said surface, because, the path on the surface is same as the shortest path in the map. However, this turns out to be wrong. Take for instance, the Beltrami-Poincare half-plane model of $\mathbb{H}^2$ , the shortest path between two points seems to be an arc of a semi circle centered at somewhere on the horizon. Picture: Why is the shortest distance not a straight line in the map here? Probably I am missing something quite basic, but I just can't seem to figure it out.",['differential-geometry']
4319062,Show that any two closed and bounded interval are homeomorphic in $\mathbb{R}$,"Any two closed and bounded intervals are homeomorhpic in $\mathbb{R}$ : If we want to show that the two sets $[a,b]$ and $[a_1,b_1]$ are homeomorphic we can consider the following map $f(x) \mapsto \frac{(x-a)}{b-a}(b_1-a_1) + a_1$ . This map is continuous and bijective and the inverse of the map  is also continuous. However here are the follow up questions I have asked myself and I am stuck with - 1)What if the intervals are not bounded - Is $[a,b]$ hoemeomorphic to $[a_1,\infty)$ ? I don't think it would hold because $[0,1]$ is probably not homeomorphic to $[1,\infty)$ . 2)Is there a way to generalize this in $\mathbb{R}^n$ - Any set in $\mathbb{R}^n$ (under the product metric ) is closed iff it is of the form $A_1 \times \cdots \times A_n$ where each $A_i$ is closed. Assume that each $A_i,B_i$ is closed and bounded then is $A_1 \times \cdots \times A_n$ homeomorphic to $B_1 \times \cdots \times B_n$ ? I think this would hold true because $A_i$ and $B_i$ are closed and bounded in $\mathbb{R}$ and let us consider $f_i:A_i \mapsto B_i$ where $f_i$ is a homeomorphism. Then $f(x) = (f_1,\cdots f_n(x))$ might be the homeomorphism we are looking for.","['continuity', 'general-topology', 'metric-spaces']"
4319087,Verification for proof by strong induction of $a^n - 1 = (a - 1)(a^{n-1} + a^{n-2} + a^{n-3}+···+a + 1)$,"I'm trying to prove by strong induction that $$a^n - 1 = (a - 1)(a^{n-1} + a^{n-2} + a^{n-3}+···+a  + 1),$$ for $n \geq 1$ . By strong induction, I'd like to know if my solution is valid. What I did was: proof for $n = 1$ $$a - 1 = (a-1)(1)$$ $n = k$ $$a^k - 1 = (a - 1)(a^{k-1} + a^{k-2} + a^{k-3}+···+a  + 1)$$ proof for $n = k + 1$ We want to show that: $$a^{k+1} - 1 = (a - 1)(a^k + a^{k-1} + a^{k-2} + a^{k-3}+···+a  + 1)$$ as we know, $$a^{k+1} - 1 = a^{k+1} - a^k + a^k - 1$$ so we rewrite the statement and use that fact that for $n = k$ the property holds. \begin{eqnarray*}
a^{k+1} - 1 &=& a^{k+1} - a^k +a^k - 1\\ 
&=& a^{k+1} - a^k + (a - 1)(a^{k-1} + a^{k-2} + a^{k-3}+···+a  + 1),
\end{eqnarray*} now factorizing $a^k$ we get: $$a^{k+1} - 1 = a^k (a-1) + (a - 1)(a^{k-1} + a^{k-2} + a^{k-3}+···+a  + 1)$$ so we have $$a^{k+1} - 1 = (a - 1)(a^k + a^{k-1} + a^{k-2} + a^{k-3}+···+a  + 1)$$ as desired.","['algebra-precalculus', 'solution-verification', 'induction']"
4319104,"Prove $\frac1{2πi}\int_C\frac{ζ^2(1-n)z^{-n}}{2\cos(nπ/2)}\,{\rm d}n=-γ-\frac12\log z-\frac1{4πz}+\frac zπ\sum\limits_{n=1}^{+∞}\frac{τ(n)}{z^2+n^2}$","I'm wondering on how one can go about proving that $$\frac{1}{2\pi \imath} \int_{\left(\mathcal{C}\right)} \frac{\zeta^2(1-n)\,z^{-n}}{2\cos\left(n\pi /2\right)}\,\mathrm{d}n = -\gamma -\frac12 \log z - \frac{1}{4\pi z}+\frac{z}{\pi}\sum_{n= 1}^{+\infty} \frac{\tau(n)}{z^2+n^2}$$ where $\tau(n)$ represents the divisor function, $\gamma$ denotes the Euler-Mascheroni constant, $\zeta$ represents the Riemann zeta function, $1<\mathcal{C}<2$ and $\int_{(\mathcal{C})}$ denotes the line integral $\int_{\mathcal{C}-\imath \infty}^{\mathcal{C} + \imath\infty}$ . So far I tried complex analytic methods (Residue theorem and contour integration) but no progress. Any help would be highly appreciated.","['complex-analysis', 'number-theory', 'sequences-and-series']"
4319106,"Order 12 group with 3 generators, can I reduce to 2 generators?","I'm just getting back into group theory after studying it quite a few years ago. I ran into a seemingly-simple question as I was getting started, looking for advice. I was looking at the dihedral group with 6 elements (D6), and made a Cayley diagram of it ( see below ) with two concentric triangles: one triangle on the inside with a group generator ""a"" of order 3, with arrows going clockwise, and then one triangle on the outside with operator a's arrows going i n the counterclockwise direction, and finally connected the vertices of each triangle with another generator ""x"", of order 2. Simple enough so far. Now, I was curious what group I might obtain if I extended this Cayley diagram in a natural way, not by increasing the order of a, but instead by adding additional concentric triangles around the outside. Eventually, in order to make a valid-looking Cayley diagram, I added a total of two more concentric triangles, so I have 4 triangles total. For the new outer triangles, I repeated the pattern of connecting the vertices inside each triangle with operator a, continuing the pattern of alternating arrow directions: counterclockwise on one outer triangle, and then clockwise on the next, so that the arrow patters are alternating for each triangle. Now, in order to connect the vertices of each triangle to the next outer triangle, I introduced one more generator, ""y"" of order 2, and made sure each point in the diagram has arrows with all three operators (for x and y, I don't draw arrowheads, since these are order 2). I have x and y alternating as we move between triangles, so I think the Cayley diagram here is valid and I assume this corresponds to a group. The group must be nonabelian since axy != xay. I know from a Google search there are only 3 nonabelian groups of order 12, so this must be isomorphic to one of them: A4, D12, or Dic3. My question: I constructed this using 3 generators, a, x, and y, but for these other groups I see them each presented with just 2 generators . Furthermore, I don't see an obvious isomorphism just from looking at the Cayley graph structure. When I studied group theory previously, I also recall getting stumped by this question of understanding which graph I'm looking at, when it's defined in terms of different generators. So, what group is it that I've drawn? Is there an obvious way to see A4, D12, or Dic3 hiding in this structure?","['group-theory', 'group-isomorphism', 'cayley-graphs', 'quaternions']"
4319109,Recognising Chern-Weil forms,"Given a smooth vectorbundle $E\to B$ with connection $\nabla$ , the (real or complex) characteristic classes of $E$ are the cohomology classes of the Chern-Weil forms associated to $\nabla$ . Suppose $E$ is complex, and that we have a form $\omega\in \bigoplus_i \Omega^{2i}(B;\mathbb R)$ which represent $ch(E)$ . Is there a connection $\nabla$ on $E$ such that $\omega=ch(\nabla)$ ? I'm pretty sure the answer is no; if I take the Chern-character forms of one connection in some degrees, and of another connection in other degrees, it seems unreasonable to expect there to be a third connection with the resulting combination of Chern-character forms. I would love to see a concrete example though. Also, it clearly sometimes happens that a form is the Chern-Weil form of a connection, and I wonder when: Are there any known conditions we can impose on $\omega$ to ensure the existence of a connection $\nabla$ with $ch(\nabla)=\omega$ ?","['vector-bundles', 'characteristic-classes', 'differential-geometry']"
4319187,"What does this notation mean, concerning a vector?","I am currently reading a paper, in which the following system is defined: $$
\dot x(t) = f(x) + \varepsilon\sigma(x)\xi(t) \tag{1}
$$ Where $x(t)$ is an $n$ -vector, $f(x)$ is some $n$ -vector transformation function, $\xi(t)$ us an $m$ -dimensional standard white Gaussian noise with parameters $\langle \xi(t)\rangle = 0,~\langle\xi(t)\xi^\mathsf{T}(\tau)\rangle = \delta(t-\tau)I$ , $\varepsilon$ is the scalar parameter of the noise intensity, and $\sigma(x)$ is an $n\times m$ -matrix-valued function of the random disturbances. And here, I do not understand, what does the notation $\langle\xi(t)\rangle$ mean, once $\xi(t)$ is a vector. I mean, I have already seen such notation, but its meaning was different, as for example, for some vectors $\boldsymbol{u}$ and $\boldsymbol{v}$ , $\langle \boldsymbol{u}, \boldsymbol{v}\rangle$ would be a dot-product of theirs, and $\langle\boldsymbol{u}, \boldsymbol{v}\rangle = 0$ only once $\boldsymbol{u} \perp \boldsymbol{v}$ . Or alternatively, as far as I know, in group theory, $\langle g \rangle$ where $g\in G$ , indicates the cyclic subgroup of $G$ , generated by its element of $g$ . In $(1)$ I completely fail to understand, what the notation means. I have rather a guess, that it may indicate ""kind-of"" expected value of a vector, since we assume the Gaussian white noise, but I have never seen such operations with vectors before, so that I cannot expand my thought on this any further.. I would be glad, if anyone could help me out with identifying, what such a notation may mean. Thank you in advance!","['notation', 'statistics', 'noise', 'vectors']"
4319229,"Upper bound of number of triangles in an edge-colored graph, given that none of the triangles are rainbow.","Let $G$ be a simple edge-coloured graph with $m$ edges. Assume furthermore that it does not contain a rainbow triangle, (i.e a triangle such that each edge has different color) and each color class in $G$ has size at most $k$ . Let $t(G)$ be number of triangles contained in $G$ . Prove: $$
t(G) \leq \frac{1}{2} m(k-1)
$$ Since the triangles are not rainbow, each one of them has a dominant color, i.e., at least two of the three edges in the triangle are in that color. This also gives a partition of the triangles based on the dominant colors, so I’m thinking of upper bounding the number of triangles $t_i$ dominant in color $i$ , summing up the bounds across all colors, and hopefully get a bound on $t(G)$ . Now any $i$ -dominant triangle use at least $2$ of the $m_i$ edges having color $i$ . And we know $m_i \leq k$ for all $i$ . So we have: $$t_i \leq {m_i \choose 2} = \frac{1}{2}m_i(m_i - 1) \leq \frac{1}{2}m_i(k-1)$$ We note that the $m_i$ ’s partition $m$ , so when summing up the $t_i$ ’s across all color classes, we get: $$t(G) = \sum t_i \leq \frac{1}{2} m (k-1)$$ Note: The following is my first attempt, which was wrong. Also there was a typo in the question that I was not aware of. I let $[r]$ be the color classes, and single out a color $i$ . The classes $[r] \setminus \{i\}$ have at least $k(r-1)$ edges in total, so $i$ has at most $m - k(r-1)$ edges. Therefore: $$t_i \leq \frac{m_i}{2} \leq \frac{1}{2} (m - k(r-1)),$$ where $m_i$ is the number of edges colored $i$ , and equality of the first $\leq$ is achieved when each of the $i$ -dominant triangles contains $2$ edges colored $i$ . Summing up the above bound across all colors, we get: $$t(G) \leq \frac{1}{2}r(m - k(r-1))$$ While I can upper bound $r$ with $\lfloor \frac{m}{k} \rfloor$ , I’m stuck at lower bounding it to make the above work. How should I proceed?","['coloring', 'graph-theory', 'solution-verification', 'combinatorics', 'discrete-mathematics']"
4319303,A real vector space with an almost complex structure vs its complexification,"Suppose $(V, J)$ is a real vector space of dimension $2n$ equipped with an almost complex structure $J$ such that $J^2 = -Id$ . $V$ can be realized as a complex vector space by setting $iv = J(v)$ for any $v \in V$ . On the other hand one can consider the complexification of $V$ , $V \otimes_{\mathbb R} \mathbb C$ and the complexification can be viewed as a complex vector space naturally. There are two almost complex structures on the complexification, one is the $\mathbb C-$ linear extension of $J$ and the other is multiplication by $i$ . What is the motivation behind considering the complexification in terms of doing complex geometry? Why can't we just consider the original tangent space? I understand the complexification can be decomposed into eigenspaces of $J$ , but what confuses me a bit is what is the benefit of doing so.","['complex-analysis', 'complex-geometry', 'hodge-theory', 'linear-algebra']"
4319374,"Exercise 2.3.5 in Grafakos, Classical Fourier Analysis","I got stuck in Exercise $2.3.5$ (c) of Grafakos, Classical Fourier Analysis for a few days. Notation here $\mathcal{S}$ is the Schwartz Class. $\mathrm{C}_0^\infty$ means smooth functions with compact support. $\tau^y$ means translation by $y$ , that is, $(\tau^yf)(x):=f(x-y).$ $\partial_j$ means the partial derivative on the $j$ -th component. For multi-index $\beta=(\beta_1,\beta_2,\cdots,\beta_n)$ and $x=(x_1,\cdots,x_n)\in\mathbb{R}^n$ , $|\beta|:=\beta_1+\beta_2+\cdots+\beta_n,$ and $$\partial^\beta f(x):=\frac{\partial^{|\beta|}}{\partial^{\beta_1}_{x_1}\cdots\partial^{\beta_n}_{x_n}}f(x).$$ $e_j$ is  the elementary vector in $\mathbb{R}^n$ : $1$ on the $j$ -th component and $0$ on the others. And $e_j$ can also be viewed as a multi-index. $\hat{f}$ is the Fourier transform of $f$ . Problem here $2.3.5.$ Let $f\in\mathcal{S}(\mathbb{R}^n)$ and $\varphi\in\mathrm{C}_0^\infty(\mathbb{R}^n)$ be identically equal to 1 in a neighbourhood of the origin. Define $\varphi_k(x)=\varphi(x/k)$ as in the proof of Proposition $2.3.23$ . (a)Prove that $(\tau^{-he_j}f-f)/h\to\partial_jf$ in $\mathcal{S}$ as $h\to 0.$ (b)Prove that $\varphi_k f\to f$ in $\mathcal{S}$ as $k\to\infty.$ (c)Prove that the sequence $\varphi_k\widehat{\varphi_kf}$ converges to $\hat{f}$ in $\mathcal{S}$ as $k\to\infty.$ (It doesn't matter if you know nothing about that proposition. Grafakos left some details in that proposition as exercise here. And if you need further explanations for anything above, please comment to let me know.) The following is what I have done. To prove (a), for any multi-indices $\alpha,\beta$ , $x\in\mathbb{R}^n$ , $$|x^\alpha\partial^\beta(\frac{\tau^{-he_j}f(x)-f(x)}{h}-\partial_jf(x))|=|x^\alpha\cdot\frac{\partial^\beta f(x+he_j)-\partial^\beta f(x)-\partial^{\beta+e_j}f(x)h}{h}|$$ Use the mean value theorem and this semi-norm of $f$ in $\mathcal{S}$ : $\sup\limits_{x\in\mathbb{R}^n}|x^\alpha\partial^{\beta+2e_j}f(x)|$ to prove the result. $~$ To prove (b), suppose $\varphi$ equals to $1$ on $B(0,R):=\{x\in\mathbb{R}^n:|x|<R\}$ . Then for multi-indices $\alpha,\beta$ , we split the semi-norm into two parts after using Leibniz's rule: $$|x^\alpha\partial^\beta(\varphi_kf-f)|=|x^\alpha(\sum\limits_{0<\gamma\leq\beta}\tbinom{\beta}{\gamma} \partial^\gamma\varphi_k ~ \partial^{\beta-\gamma}f)+x^\alpha(\varphi_k-1)\partial^\beta f|.$$ The sum is taken over multi-index $\gamma$ , where (the partial order on multi-indices) $\gamma\leq\beta$ means $\gamma_j\leq\beta_j$ for every $j\in\{1,2,\cdots,n\}$ , and $\gamma>0$ means $\gamma\geq0$ as multi-index but $\gamma\not=0.$ Use the fact that $\varphi_k-1$ and $\partial^\gamma\varphi_k(\gamma>0)$ vanish on $B(0,kR)$ to help prove the result. $~$ Now prove (c). It is really tempting to prove (c) with (b), because (b) says that the sequence $\{\varphi_k\}_{k\geq1}$ approximates the identity map. So, it seems that $\widehat{\varphi_kf}$ is almost $\hat{f}$ . Then $\varphi_k\widehat{\varphi_kf}$ is almost $\varphi_k\hat{f}$ , and thus almost $\hat{f}.$ But actually it doesn't make sense! To be rigorous, consider the sequence with two indices: $\{\varphi_j(\widehat{\varphi_kf})\}_{k,j\geq1}$ . Fix any one of $k,j$ and let the other $\to\infty$ , for example fix $k$ and let $j\to\infty$ , we get the limit $\widehat{\varphi_kf}$ , then we let $k\to\infty$ to get $\hat{f}.$ The same result if we first let $k\to\infty$ then let $j\to\infty$ . With this intepretation, what we care about is the diagonal subsequence where $k=j$ . Let's simplify this problem. Consider an infinite diagonal matrix $(a_{j,k})_{j,k\geq1}$ , where all entries on the diagonal equal $1$ . Then the limit of every row and column regarded as a sequence is $0$ , and the limits of these two sequences of row/column limits are both $0$ . But the limit of the diagonal sequence is $1$ . They are not equal... $~$ Also, I have tried to split $x^\alpha\partial^\beta(\varphi_k\widehat{\varphi_kf})$ into two parts as I did in (b), but it points to the same ""diagonal sequence"" problem. So I want to know how to prove (c), or how to deal with this ""diagonal"" problem. Is there anything I've missed or any tool I need? Thanks in advance.","['harmonic-analysis', 'fourier-analysis', 'functional-analysis', 'real-analysis']"
4319456,Exsistence of limit $\lim_{k\to \infty} \prod_{i=1}^{k}P(A_i)$ and $\lim_{k\to \infty}P(\bigcap_{i=1}^{k}A_i)$,Let $(A_k)_{k\in\mathbb{N}}$ be a sequence of events. Argue that both limits exist $$\lim_{k\to \infty} \prod_{i=1}^{k}P(A_i)\quad \quad \lim_{k\to \infty}P(\bigcap_{i=1}^{k}A_i)$$ I am not sure on how to answer this at all. I would have thought that they didn't as my book states the definition of independent probability only for finite values of $J\subseteq I$ $$P(\bigcap_{i \in J}A_i)=\prod_{i\in J}P(A_i)$$ Now the question above is considering cases where $I$ is the natural numbers and and the cardinality of $J$ is $\infty$ . How do I argue that these limits exists?,"['conditional-probability', 'elementary-set-theory', 'limits', 'probability-theory', 'probability']"
4319494,Measure of an interval contained in a Borel set,"Let $\mathbb{Q}\cap [0,1]=\{x_1,x_2,x_3,\dots\}$ and define the open interval $$G_n=\left(x_n-\frac{1}{2^{n+2}},x_n+\frac{1}{2^{n+2}}\right),\,n\in\mathbb{N}.$$ Putting $G=\bigcup_{n=1}^{\infty}G_n$ , define the Borel set $B$ as $$B=[0,1]\cap G^c.$$ Consider the Lebesgue measure space $(\mathbb{R},\Lambda,\lambda).$ If $I\subset B\cup E$ , where $I$ is an interval and $\lambda(E)=0$ , then $\lambda(I)=0$ . The book in whick this question appears provides the following hint: ""If $I\subset [0,1]$ and $I$ is open, then $I\cap B^c\neq\emptyset$ "". To solve this exercise, I suspect it would be necessary to use the following result (which I've already proved): Let $G\subset \mathbb{R}$ be an open set and $E\subset G$ such that $\lambda(E)=0$ . Then $G\backslash E$ is dense in $G$ . However, I can't see how to proceed from here. Thanks in advance!","['measure-theory', 'lebesgue-measure']"
4319575,"If a nilpotent group has an element of prime order $p$, so does its centre.","This is Exercise 5.2.1 of Robinson's, ""A Course in the Theory of Groups (Second Edition)"" . According to Approach0 , it is new to MSE. The closest I could find is the following: Prove that in a nilpotent group every normal subgroup of prime order is contained in the center. The Details: (This can be skipped.) Robinson's definition of the following is equivalent to the one given on proof wiki : Let $G$ be a group whose identity is $e$ . A normal series for $G$ is a sequence of normal subgroups of $G$ : $$\{e\}=G_0\lhd G_1\lhd\dots\lhd G_n=G,$$ where $G_{i-1}\lhd G_i$ denotes that $G_{i-1}$ is a proper normal subgroup of $G_i$ . On page 122 of Robinson's book, Definition: A group $G$ is called nilpotent if it has a central series , that is, a normal series $1=G_0\le G_1\le \dots \le G_n=G$ such that $G_{i+1}/G_i$ is contained in the centre of $G/G_i$ for all $i$ . The length of a shortest central series of $G$ is the nilpotent class of $G$ . The Question: If a nilpotent group has an element of prime order $p$ , so does it ( sic ) centre. Thoughts: Let's denote by $g$ the element of $G$ of order $p$ . I'm not particularly well-versed in nilpotent groups. Cauchy's Theorem for groups springs to mind: Theorem (Cauchy): For a finite group $G$ , if a prime $p$ divides $\lvert G\rvert$ , then $G$ has an element of order $p$ . I don't know whether it would help. I doubt it. There's nothing to say $G$ in the question is finite. If $G$ is abelian, then $G=Z(G)$ , so the question is trivial in this case. Examples of infinite, nonabelian nilpotent groups can be found here . An example of a nonabelian, nilpotent finite group is the quaternion group $Q_8$ . Its order is $8$ , so Cauchy's Theorem suggests that $Q_8$ has an element of order two. It is not difficult to see that that element is $-1$ . We have $Z(Q_8)=\{1,-1\}$ . So the result holds here. The last time I gave Robinson's book a good read was about three weeks ago. I need to get stuck in again if I stand a chance of doing the exercises. I have an exam coming up in a few weeks, too, so I can't give each exercise as much time as I would normally. Please help :)","['nilpotent-groups', 'group-theory', 'normal-subgroups']"
4319630,Limiting distribution of binary variable (Central limit theorem fails),"Suppose we have a random variable $$Y_i = i \text{ with probability } \frac{1}{i}$$ and $0$ otherwise. Here all the $Y_i$ are independent.
We can redefine $X_i = Y_i -1 $ so that $E(X_i)=0$ .
Then the variance of $X_i$ is $(i-1)^2\cdot 1/i + (-1)\cdot(1-1/i) = i-1$ and $s_n=\sum_{i=1}^{n}(i-1)=\frac{n(n-1)}{2}$ Define $S_n = \sum_{i=1}^{n}X_i$ . One can check that the CLT does not apply here and that $\frac{S_n}{s_n}$ does not converge to the standard normal distribution. Any thoughts on what is the limiting distribution and how to get it?","['statistics', 'probability-distributions', 'probability-theory', 'central-limit-theorem']"
4319653,Measurability of the max function.,"Could someone help me with the following problem? Let $S$ be a compact metric space, $\mu$ be a Borel measure on a Souslin space, $f:S\times X\to\mathbb{R}$ be a function continuous in the first argument, and Borel measurable in the second argument. Show that there is a Borel measurable function $F:X\to S$ such that $F(x)$ is a point of maximum of $f(x,s)$ over $s\in S$ . I was given two hints: We are looking for the function $F$ such $f(F(x),x)=\max\limits_{s\in S}f(x,s)$ We may use the fact that a Borel measurable mapping between Souslin spaces has a Borel measurable right inverse. This question might seems as duplicate of the question Measurability of supremum over measurable set , but the answer looks like an overkill. I'm looking for a more down to Earth solution.","['borel-sets', 'maxima-minima', 'measure-theory']"
4319759,Finding the area of a square inside a quarter of a circle,"Here's the problem: This problem could be easy, were I to know if the small pink square divided the arc length of a quarter circle into 3 pieces (identical). What I'm trying to say is, if my guess is correct, the ratio of the length of $\dfrac{\alpha\beta}{BD}$ is $\frac13$ . But, is it correct? I want to assure myself if this hypothesis is correct. How do you prove it? This is the important key to find the small square. Because, if that's so, I can use this formula (below) to find the side of the small square: $$\text{side} = 2r\sin\left(\frac{t}{2}\right)$$ Where $r$ is the radius of the circle and $t$ is the angle of the sector circle excribed the small square. In conclusion, my point is just I'm asking whether it's true or not that the ratio $\dfrac{\alpha\beta}{BD}$ is $\frac13$ . Or perhaps you have another simpler way to find the small pink square?",['geometry']
4319761,Symmetric matrix with positive entries being invertible?,"Let $u_1,\dots,u_n\in S^{n-1}$ be $n$ linearly independent unit vectors (thus a basis for $\mathbb{R}^n$ ). Let $a_{ij}:=u_i^\top u_j$ . Then $a_{ii}=1$ and $a_{ij}=a_{ji}$ . My question is: Is the following matrix always invertible $$A=(a_{ij}^2)_{1\leq i,j\leq n}$$ For $n=2$ it is, but for general $n$ I think there are counterexamples. Since the entrywise squareroot of $A$ is invertible, I guess we can start with some invertible(maybe unitary?) matrix $P$ with $P^2$ (entrywise) not invertible, and then modify it to find the corresponding vectors. But I failed to modify it to find such unit vectors...","['matrices', 'linear-algebra']"
4319771,Martingale majorizing a bounded supermartingale,"Let $S_1, S_2, \dots, S_n$ be a supermartingale, such that $$|S_i| \le C$$ almost surely, for some positive constant $C$ and all $1\le i \le n$ . My question is: Does there always exist a martingale $M_1, M_2, \dots, M_n,$ such that $$S_1=M_1$$ $$S_i \le M_i$$ and $$|M_i|\le C$$ for all $1\le i \le n$ ? In other words: if we have a bounded supermartingale, can we always ""improve"" it while not validating the given bound $C$ ? My intuition is that it should be true. I think that it is (more or less) obvious for $(S_i)_{i=1}^{n}$ with finite support. Is it also true without this constraint? What would be a short, yet formally sound argument?","['conditional-expectation', 'martingales', 'probability-theory', 'probability']"
4319882,Taylor expansion of a non-Taylor series,"Show that the following function is arbitrarily differentiable and find its Taylor expansion zero-centered. $$g:\mathbb{R}\to\mathbb{R}, \hspace{5mm} g(x):= \sum_{n=0}^{\infty} \frac{\cos(n^2x)}{2^n}.$$ Solution: At the moment, I don't know (with formal proof) if the symbols $\frac{\mathrm{d}}{\mathrm{d}x}$ , $\sum_{n=0}^{\infty}$ switch. I proceed with one of the formal definition of the derivative: $$\frac{\mathrm{d}}{\mathrm{d}x}(g) = \lim_{x \to x_0} \frac{g(x)-g(x_0)}{x-x_0} = \lim_{x \to x_0} \left(\sum_{n=0}^{\infty} \frac{\cos(n^2x)-\cos(n^2x_0)}{2^n(x-x_0)}\right) \hspace{5mm} \text{if the limit exists.}$$ I'm stuck at this point. Also I don't know if the symbols $\lim_{x\to x_0}$ and $\sum_{n=0}^{\infty}$ switch. By the way, I tried with the following expansion: $$\cos:\mathbb{R}\to\mathbb{R}, \hspace{5mm} \cos(x):= \sum_{k=0}^{\infty} \frac{(-1)^{k}}{(2k)!} x^{2k}.$$ But I don't see any advantage of use this expansion. I would be grateful for any help/hint.","['power-series', 'derivatives', 'taylor-expansion', 'real-analysis']"
4319929,Show that this structure is an affinely regular polygon,"Let $\{x_1,\cdots,x_n\}$ be a set of convex points labeled in a cyclic order. I am trying to show that the following structure is equivalent with an affinely regular polygon: Fix $j$ for some $j\in \{1,\cdots,n\}$ . Then we know the following to be true: $$\{x_j,x_{j-1}\}\parallel \{x_{j+1},x_{j-2}\},\quad \{x_{j},x_{j-2}\}\parallel \{x_{j+1},x_{j-3}\}$$ where indices are modulo $n$ , and in general $\{x_j,x_{j-a}\}\parallel \{x_{j+1},x_{j-a-1}\}$ for each $a\in \mathbb{Z}_{+}$ . Here $\parallel$ denotes parallelity among the given lines. How would I go about showing that it is an affinely regular polygon? I know that by Nizette we know that if for all $j\in \mathbb{Z}$ it holds that $\{x_j,x_{j+1}\}\parallel \{x_{j-1},x_{j+2}\}$ and $\{x_j,x_{j+2}\}\parallel \{x_{j-1},x_{j+3}\}$ then the structure is equivalent to an affinely regular polygon. However, the problem in my case is that $j$ is fixed to be some integer in $\{1,\cdots,n\}$ . If there could be a way of showing that Nizette's argument is true based on this given information, then we would be done. However, I am not sure how I would go about this.","['combinatorics', 'geometry', 'discrete-mathematics']"
4319939,What does a set of functions from $A$ to $B$ belong to?,"To understand the set-theoretic definition of functions, I tried to find a set that contains a set of all functions from $A$ to $B$ for (possibly empty) sets $A$ and $B$ . $
\newcommand{\eqv}{\Leftrightarrow}
\newcommand{\imply}{\Rightarrow}
\newcommand{\powset}{\mathcal{P}}
$ My approach (possibly informal): Because a function is a subset of binary relation, $$
f \in A \to B
\imply
f \subseteq A \times B
\imply
f \in \powset(A \times B)
$$ where $A \to B$ denotes the set of all functions from $A$ to $B$ .
Therefore, $$
\forall f: f \in A \to B \imply f \in \powset(A \times B)
$$ i.e., $A \to B \subseteq \powset(A\times B)$ . Finally, we have $$
A \to B \in \powset(\powset(A\times B))
$$ Is this correct? What is the domain of discourse of $f$ here? (concerning $\forall f$ )","['elementary-set-theory', 'functions']"
4319942,Prove $C(X^\mathrm{T}X+C^\mathrm{T}C)^{-1}C^\mathrm{T}= I$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Consider the linear model $Y = X \beta + \epsilon$ , where $\beta \in \Bbb R^{p}$ . Suppose that $r := \mbox{rank}(X) < p $ and let $C \in \Bbb R^{m\times p}$ be a matrix satisfying the condition $\mbox{rank}(C) = p-r$ , $\mathcal{R}(X^\mathrm{T})\cap \mathcal{R}(C^\mathrm{T})=\{0\}$ . then $$(X^\mathrm{T}X+C^\mathrm{T}C)^{-1} \text{ is a generalized inverse of } C^\mathrm{T}C \tag{a}$$ $$C(X^\mathrm{T}X+C^\mathrm{T}C)^{-1}C^\mathrm{T}=I \tag{b}$$ I think one can be obtained by another, since if $C(X^\mathrm{T}X+C^\mathrm{T}C)^{-1}C^\mathrm{T}=I$ , then $C^\mathrm{T}C(X^\mathrm{T}X+C^\mathrm{T}C)^{-1}C^\mathrm{T}=C^\mathrm{T}I$ , which implies $$C^\mathrm{T}C(X^\mathrm{T}X+C^\mathrm{T}C)^{-1}C^\mathrm{T}C=C^\mathrm{T}IC=C^\mathrm{T}C,$$ Hence we prove the (a). now I am not sure how to prove (b), thank you for help me to figure it out.","['statistical-inference', 'statistics', 'linear-algebra']"
4320018,Divergence of curl is zero (coordinate free approach),"I'm TAing a vector calculus course and the professor has asked the students to prove that $\nabla \cdot (\nabla \times \vec{F}) = 0$ . I'm meant to teach this problem in recitation tomorrow and I think the problem is incorrect. The specific problem statement is: Problem. Let $C$ be a simple, closed curve, $S_1, S_2$ two surfaces whose boundary is $C$ and $\vec{F}$ a vector field that is defined and differentiable throughout a simply connected region containing $C, S_1$ , and $S_2$ . Use Stokes' theorem and the divergence theorem to show that $\nabla \cdot (\nabla \times F)$ is zero. This is obviously super easy to do if one uses Euclidean coordinates ( for example , on page 3). And since we're dealing with curl, I think it's safe to assume that the domain is $\mathbb{R}^3$ , so that Euclidean coordinates are certainly available. But proof by computation in coordinates does not require the divergence theorem or Stokes' theorem, so I think the professor has a coordinate-free approach in mind. Specifically, I think he's thinking that the shared boundary $C$ will be oriented in opposite directions when considered as the boundary of $S_1$ versus $S_2$ . Therefore, one can compute \begin{align*}
\iiint_V \nabla \cdot (\nabla \times \vec{F}) dV &= \iint_{S_1 \cup S_2} (\nabla \times \vec{F}) \cdot d\vec{A} \\
&= \iint_{S_1} (\nabla \times \vec{F}) \cdot d\vec{A}_1 + \iint_{S_2} (\nabla \times \vec{F}) \cdot d\vec{A}_2 \\
&= \oint_{C^+} \vec{F} \cdot d\vec{r} + \oint_{C^-} \vec{F} \cdot d\vec{r} \\
&= 0,
\end{align*} where $V$ is the region enclosed by $S_1 \cup S_2$ , where $d\vec{A}_1$ and $d\vec{A}_2$ are outward normal to $S_1$ and $S_2$ respectively, and where $C^+$ indicates $C$ oriented counterclockwise while $C^-$ indicates clockwise orientation. However, that computation only shows that the integral of $\nabla \cdot (\nabla \times \vec{F})$ is zero, not that the function itself is zero. Is there some way to show from here that the function itself is zero? Another approach that I am missing? Should I just tell the students to do the computation in coordinates and ignore the divergence/Stokes stuff? Thanks in advance!","['divergence-theorem', 'multivariable-calculus', 'stokes-theorem', 'vector-analysis']"
4320032,Why do we work on the Borel sigma algebra and not on the Lebesgue sigma algebra?,"In most measure theory text books one derives the Lebesgue anh Borel-Lebesgue measure from Caratheodory's extension to outer measures by first proving that the set of $\lambda^*$ measurable sets is a sigma algebra (the Lebesgue sigma algebra) and that $\lambda^*$ restricted to that sigma algebra is a measure, the Lebesgue measure. This measure space is even complete. However, then still one continues to show that $\lambda^*$ restricted to the sigmal algebra generated by the ring (on which the pre-measure was defined that was used to obtain $\lambda^*$ via Caratheodorys extension) is also a measure, the Borel-Lebesgue measure, and that the generated sigma algebra is the Borel sigma algebra. So my question is, why is the Borel sigma-algebra ""better"" than the Lebesgue sigma algebra, because most of the time text books continue to work only on the Borel sigma algebra, even though the Lebesgue sigma algebra is its completetion and has some other favorable properties? I.e., I am just missing an argument in all the lecture notes and text books why we continue to work on the Borel sigma-algebra after having shown that the Lebesgue sigma algebra is larger (and after all we do all the extension from a pre-measure to an outer measure and then restricting to a measure because we want to get a larger set than just the ring on which we initially defined the pre-measure).","['borel-sets', 'measure-theory', 'lebesgue-measure']"
4320041,Find the minimum of $\frac{\sin x}{\cos y}+\frac {\cos x}{\sin y}+\frac{\sin y}{\cos x}+\frac{\cos y}{\sin x}$,"Let $0<x,y<\frac {\pi}{2}$ such that $\sin (x+y)=\frac 23$ , then find the minimum of $$\frac{\sin x}{\cos y}+\frac {\cos x}{\sin y}+\frac{\sin y}{\cos x}+\frac{\cos y}{\sin x}$$ A) $\frac 23$ B) $\frac 43$ C) $\frac 89$ D) $\frac {16}{9}$ E) $\frac{32}{27}$ My attempts: I think that the all possible answers are wrong. Because, by Am-Gm inequality we have $$\frac{\sin x}{\cos y}+\frac{\cos y}{\sin x}+\frac {\cos x}{\sin y}+\frac{\sin y}{\cos x}≥2+2=4.$$ But, Wolfram Alpha gives us a different result : The Global Minimum doesn't exist. However, ​the local minimum must be $6.$ But, still the problem is not solved. Because Wolfram's graph shows that the minimum can be less than $6$ . I also tried Let $$\sin x=a,\cos y=b,\cos x=c,\sin y=d$$ with $$ab+cd=\frac 23≥2\sqrt{abcd}\implies abcd≤\frac 19\\ a^2+c^2=b^2+d^2=1 $$ then I need $$\min \left(\frac ab+\frac ba+\frac cd+\frac dc\right)$$ But, I can't do anything from here.  Finally, I attach the graph drawn by WA.","['contest-math', 'inequality', 'maxima-minima', 'trigonometry', 'algebra-precalculus']"
4320044,Help proving that signed volume of n-parallelepiped is multilinear,"Overview I am trying to build some intuition about the volumes of parallelepipeds and determinants. I would like to define the determinant as the unique function of $N$ vectors in $\mathbb{R}^N$ which is multilinear, anti-symmetric, and normalized and then show that the signed volume of the $N$ -parallelepiped satisfies these conditions without using any facts about determinants . The challenging one here for me is multi-linearity. Furthermore, I would like the signed volume to be defined in terms of Lebesgue integrals. Problem Setup Let $v_1, \ldots, v_N \in \mathbb{R}^N$ . A parallelepiped $P = P(v_1, \ldots, v_N) \subset \mathbb{R}^N$ is the set $$
P = P(v_1, \ldots, v_N) = \left\{\sum_{i=1}^N t_i v_i \mid 0 \le t_i \le 1 \text{ for $i$ from 1 to $N$} \right\}
$$ My goal is to define a function $\text{svol}(P) = \text{svol}(v_1,\ldots, v_N)$ which is the signed volume of the parallelepiped $P$ . The volume $\text{vol}$ of the parallelepiped is defined as follows. Let $1_P$ be the indicator function on the set $P$ . $$
\text{vol}(P) = \text{vol}(v_1, \ldots, v_N) = \int_{\mathbb{R}^N} 1_P dV
$$ Where the integral is the Lebesgue integral. We should have $$
|\text{svol}(P)| = \text{vol}(P)
$$ We must additionally define, or provide an algorithm, to set the sign of $\text{svol}(P)$ . It is apparently the case, once everything above has been set up correctly, that for $a, b \in \mathbb{R}$ and $w \in \mathbb{R}^N$ \begin{equation}
\tag{*}
\text{svol}(av_1 + bw, v_2, \ldots v_N) = a\cdot \text{svol}(v_1, v_2, \ldots, v_N) + b\cdot \text{svol}(w, v_2, \ldots v_N)
\end{equation} The Direct Questions My questions are How should $\text{svol}(v_1,\ldots, v_N)$ be defined such that (1) it is defined in terms of an integral over $\mathbb{R}^N$ and (2) we are able to prove $(*)$ . Given the definition that answers the previous question, how do we prove $(*)$ ? An Illustrative Image The answer to this question includes a beautiful illustration the depicts exactly what I am trying to prove. It is intuitively clear to me from this diagram and Cavalieri's principle that $(*)$ should hold. But of course, for me, the illustration does not constitute a fully rigorous proof following the conditions I laid out above. An Almost Solution The closest I have come to a satisfactory proof is as follows. It is possible to define the volume of the $n$ -parallelepiped inductively. We say the volume of the 1-parallelepiped $\overline{\text{vol}}(P) = ||v||$ . The volume of the $n$ -parallelepiped is then defined by $\overline{\text{vol}}_N(v_1, \ldots, v_N) = \overline{\text{vol}}_{N-1}(v_1, \ldots, v_{N-1}) \cdot ||v_{N, \perp}||$ where $v_{N, \perp}$ is the component of $v_N$ which is orthogonal to the span of $\{v_1, \ldots, v_{N-1}\}$ . This component may be called the $N^{\text{th}}$ altitude of the parallelepiped. Under this definition the proof follows because, from $(*)$ , $||v_{1, \perp}|| + ||w_{\perp}|| = ||(v_1 + w)_{\perp}||$ . This proof is essentially a conversion of the image above into a more rigorous definition and proof. I think the quick proof I've given here lacks something defining the sign of $\overline{\text{vol}_N}$ . The problems with this proof are the lack of control of the sign of the volume and that the definition of $\overline{\text{vol}}_N$ is not based on an integral. One appropriate answer to the question I am asking in this thread would be a way to control the sign of $\overline{\text{vol}_N}$ and relate $\overline{\text{vol}_N}$ to the integration based definition of volume. This would essentially be an equation relating $\text{svol}$ and $\overline{\text{vol}_N}$ . Comment About Sign I realize that to define $\text{sgn}(\text{svol}(v_1,\ldots, v_N))$ that we must make some convention choice. This convention choice should be consistent with $\text{sgn}(\text{svol}(e_1,\ldots,e_N)) = +1$ where $\{e_1, \ldots, e_N\}$ is the standard basis for $\mathbb{R}^N$ . Beyond that, I'm not sure how to define the sign in a way that does not use any facts about determinants . I don't know if there's a way to determine if two sets of $N$ vectors, $\{v_1, \ldots, v_N\}$ and $\{w_1, \ldots, w_N\}$ have the same orientation other than computing the sign of the determinant. If someone has an answer here I would appreciate it. If not I would be comfortable with the following. Let $$
D(v_1, \ldots, v_N) = \sum_{\sigma \in S_N} \text{sgn}(\sigma) \prod_{i=1}^N v_{i, \sigma(i)}
$$ where $S_N$ is the symmetric group of size $N$ and if $j=\sigma(i)$ then $v_{i, j}$ indicates the $j^{\text{th}}$ component of vector $v_i$ . Then let $$
\text{sgn}(\text{svol}(v_1, \ldots, v_N)) = \text{sgn}(D(v_1, \ldots, v_N))
$$ so that $$
\text{svol}(v_1, \ldots, v_N) = \text{sgn}(D(v_1, \ldots, v_N)) \cdot \text{vol}(v_1, \ldots, v_N)
$$ This last section essentially serves as an answer to my first question above. The question of how to prove $(*)$ from this definition still remains.","['integration', 'determinant', 'linear-algebra', 'volume']"
4320128,Has anyone studied the quasimetric induced by the KL divergence?,"Recall that, for probability measures $p$ and $q$ on a finite set $X$ , the Kullback-Leibler divergence $$
D(p||q) = \sum_{x\in X} p(x) \log \dfrac{p(x)}{q(x)}
$$ is famously not a metric, in particular it does not satisfy a triangle inequality. However, in principle one could look at the following quantity $$
d(p,q) := \inf_{n\in \Bbb{N}} \inf_{p_1,\dots,p_n} \Big( d(p,p_1) + d(p_1,p_2) + \dots + d(p_n, q) \Big) ,
$$ which does satisfy the triangle inequality, it is the largest quasimetric which is less or equal than $D(p||q)$ . Has this quantity been studied anywhere? Is it trivial (i.e. zero)?","['statistics', 'metric-spaces', 'information-geometry', 'information-theory', 'probability']"
4320132,Explicit Jacquet-Langlands correspondence,"Jacquet-Langlands correspondence gives a 1-to-1 correspondence between automorphic forms on $\mathrm{GL}_{2}(\mathbb{Q})$ and automorphic forms on $\mathrm{GL}_{1}(D)$ , where $D$ is a division algebra over $\mathbb{Q}$ .
As one can consider (holomorphic) modular forms as $\mathrm{GL}_{2}(\mathbb{Q})$ automorphic forms, I think there should exists some kind of automorphic functions on $\mathrm{GL}_{1}(D)$ corresponds to the modular form.
However, I can't imagine how the automorphic function on division algebra should look like. According to this note, the corresponding function might be a function on $\mathfrak{h}^{n}$ , where $\mathfrak{h}$ is a complex upper half plane and $n$ is the number of infinite places which $D$ split, with suitable transformation laws.
Could anyone give more detailed explanations or references in this direction? For example, what kind of function may corresponds to the discriminant form $\Delta(z) \in S_{12}(1)$ ?","['number-theory', 'automorphic-forms', 'modular-forms']"
4320173,Theorem 10.2 Rudin,"$\mathscr b(X)$ denotes the set of all complex-valued, continuous, bounded functions with domain $X$ . I don't understand why is $L(h)$ equal of $\prod_{i=1}^k $ $\int_{a_i}^{b_i} h_i(x_i)dx_i$ and then why it's equal of $L'(h)$ . As I know $\int f(x)g(x)dx$ is not equal of $\int f(x) dx \int g(x) dx $ . The proof of is theorem is absolutely ununderstandable for me.
I also  don't understand why is $L(g)$ equal of $L'(g)$ for all $ g $ $\in$ $\mathscr A $ . I would be grateful for any kind of help","['integration', 'continuity', 'abstract-algebra', 'analysis']"
4320219,"Continuity and differentiability of f(x,y) at the origin [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Let $f$ be a function defined by $$f(x,y) = \begin{cases} \dfrac{\sin x - \sin y}{x-y} & x \neq y \\ \cos x & x = y \end{cases}$$ Study the continuity and differentiability of $f$ in the origin.","['continuity', 'derivatives', 'analysis', 'real-analysis']"
4320245,Are there functions $f(t)$ with $||f'(t)||_\infty < \infty$ such as their Fourier transform $F(w)$ makes $\int_{-\infty}^\infty|wF(w)|dw \to \infty$??,"Are there any time-limited and continuous one-variable functions $f(t)$ with bounded derivative $||f'(t)||_\infty < \infty$ (not meaning here they are also necessarily differentiable), such as their Fourier transform $F(w)$ makes diverge the following integral $\int\limits_{-\infty}^\infty|iwF(w)+f(t_F)\,e^{-iwt_F}-f(t_0)\,e^{-iwt_0}|\,dw \to \infty$ ?? Or these kind of functions are an empty set (for each of the following scenarios)? The different terms from the questions of the tittle are just for avoiding the effects of the discontinuity on the edges of the compact-support $\partial t = \{t_0,\,t_F\}$ (starting and ending times), since they introduce Dirac's Delta functions $\delta(t)$ in the derivative $f'(t)$ (""artificially"" in my opinion, since to model time limited phenomena I am interested only in what is happening ""within"" the compact support). If you feel uncomfortable with them, just assume also that the functions $f(t)$ begins and finishes at zero $f(t_0)=f(t_F)=0$ . From the following, I will use both definitions as equivalent since the problem is avoidable (I explained one way to overcome it here ). Please keep it in mind, or it will make harder to find counterexamples since this edges-discontinuities will make the standard $\int_{-\infty}^\infty|wF(w)|dw$ always diverge, since the derivative will be unbounded because of these delta functions, as I will explain now. I am trying to understand the figure of the integral $\int_{-\infty}^\infty|wF(w)|dw$ which is an upper bound for the maximum rate of change of the function $f(t)$ : $$ \sup\limits_t \left| f'(t)\right| \leq \int_{-\infty}^\infty|wF(w)|dw$$ It has an individual name? (as the Dirichlet Energy, as example), this for being able to look for its properties by myself. Any references are welcome. Directly from the inequality I know that if the derivative is unbounded $||f'(t)||_\infty \to \infty \Rightarrow \int_{-\infty}^\infty|wF(w)|dw \to \infty$ will always diverge, and conversely, if ""this"" integral is bounded $\int_{-\infty}^\infty|wF(w)|dw < \infty \Rightarrow ||f'(t)||_\infty < \infty$ the maximum rate of change will be bounded (even when time-limited functions has unlimited bandwidth on the frequencies), but I want to know if there exists any cases of functions that lie in-between these two scenarios (I have already looked unsuccessfully for counterexamples by myself). I am specially interested in these five scenarios (from less to more restrictive - I believe): General time-limited and continuous one-variable functions $f(t)$ , as is already asked Time-limited continuous one-variable functions which are also absolutely integrable $\int\limits_{t_0}^{t_F}|f(t)|\,dt < \infty$ and energy finite $\int\limits_{t_0}^{t_F}|f(t)|^2 dt < \infty$ Functions that fulfill (1) and (2) and are also have their absolute value
of its Fourier Transform bounded $\int\limits_{-\infty}^{\infty} |F(w)| dw < \infty$ Functions that fulfill (1) to (3) and also have finite Dirichlet Energy $\int\limits_{t_0}^{t_F} |f'(t)|^2 dt < \infty$ Functions that fulfill (1) to (4) and there also of bounded total variation $V_{[t_0,\,t_F]}(f(t)) < \infty$ I want to know if any of these intermediate conditions stages makes the integral $\int_{-\infty}^\infty|wF(w)|dw$ becomes bounded, or if are totally unrelated . Please notice that neither of these conditions are requiring to $f(t)$ to be differentiable. But I am not interested in ""bad-behaved"" things like nowhere-differentiable functions as Brownian motions, or fractals, or Cantor or Weierstrass functions, and things like that (at least not this time) Any counterexample will be welcome either. Beforehand, thanks you very much.","['fourier-analysis', 'fourier-transform', 'real-analysis', 'complex-analysis', 'derivatives']"
4320361,Density of integers represented by a binary quadratic form is zero,"Let $q(x_1,x_2):=ax_1^2+bx_1x_2+cx_2^2$ be a positive-definite quadratic form with $a,b,c\in\Bbb{Z}$ . Let $r_q(n)$ be the number of natural numbers $k\leq n$ that are represented by $q$ , i.e., such that $q(x_1,x_2)=k$ for some $x_1,x_2\in\Bbb{Z}$ . Is it necessarily true that the probability of a random number being representable by $q$ is zero, in the sense that $\lim_{n\to\infty}\frac{r_q(n)}{n}=0$ ? In the nice example $q(x_1,x_2)=x_1^2+x_2^2$ , we have that $r_q(n)$ is proportional to $\frac{n}{\sqrt{\log n}}$ (according to this article), so $\frac{r_q(n)}{n}$ is proportional to $\frac{1}{\sqrt{\log n}}$ , which tends to zero. Less pleasant examples like $q(x_1,x_2)=3x_1+7x_2^2-x_1x_2$ seem to be much more restrictive on possible representations, so it looks like the conjecture should be true. Given that quadratic forms are intensely studied since Gauss (at least), I imagine this must be something well-known.","['analytic-number-theory', 'number-theory', 'quadratic-forms']"
4320386,How to prove that a Compact Riemann Surface is a projective algebraic curve?,"There are already several questions on the subject, the most interesting being this one . But I do not find the answers there completely satisfying. The end of the proof is missing. Here is how I see the main steps of the proof. Let $X$ be a compact connected Riemann surface of genus $g$ . Prove the Riemann-Roch theorem for $X$ . I am aware that this part is quite hard. It needs some serious work in analysis/fuctional analysis to establish even the existence of one non-constant meromorphic function, and still further non-trivial work to prove the Riemann-Roch theorem. But this is well-documented, for instance in Foster's book ""Lectures on Riemann SUrfaces"" or Gunning's book ""Riemann Surfaces"" or Miranda's ""algebraic curves and Riemann Surfaces"". Use Riemann-Roch to show that a line bundle of degree $\leq 2g+1$ defines an embedding of the curve into $\mathbb{P}^N$ for some $N$ . This is easier, and the proof for Riemann surfaces is exactly the same as for algebraic curves, so to the references above we can add Harthsorne II.7 and IV.3, etc. Conclude. Now this is the part where I ask for some help and reference. It is not done in any of the reference I have cited (not even in Miranda's, despite its title). Here is how I can see part 3. Using any divisor of degree at least $2g+1$ (e.g. $2g+1$ times a point), we get an embedding of the curve $X$ into $\mathbb{P}^N$ . Call $Y$ its image. Show that $Y$ is an analytic subvariety of $\mathbb{P}^N$ . Then use Chow's theorem, using that $Y$ is compact since $X$ is, to prove that $Y$ is an algebraic curve in $\mathbb{P}^N$ . However, this is a long and hard road. Chow's theorem is a difficult and long to prove theorem, either with its original proof or with Serre's GAGA (since GAGA is itself quite advanced). It requires introducing the notion of analytic subvarieties, which I'd like to avoid. I am looking for the simplest possible proof of this, as this is for a graduate course I am teaching. Is there some shorter way to do this? Is there a reference that does it?","['riemann-surfaces', 'algebraic-geometry']"
4320409,How to construct submodules with GAP / MeatAxe?,"Let $G= \langle g_1, g_2 \rangle$ be a finite group. Let $k$ be a finite field with ${\rm char}(k)=p>0$ such that $p \mid |G|$ . Let the $kG$ -module $M$ be a MeatAxe-module in GAP. The generators of $M$ are given by the two matrices $m_1$ and $m_2$ , respectively, which reflect the actions of $g_1$ and $g_2$ , respectively. I'd like to ask the following two (related) questions: 1.) If ${\rm dim}_k(M)=n$ and one has a finite set $S=\{v_1,v_2,...\}$ of row vectors (where each vector has $n$ entries), how can one construct the submodule of $M$ generated by $S$ with GAP/MeatAxe ? 2.) Given a fixed element $f\in{\rm End}_{kG}(M)$ via a matrix in GAP, how can one construct the image and the kernel of $f$ as submodules of $M$ with GAP/MeatAxe ? Thank you very much for the help.","['gap', 'representation-theory', 'modules', 'algorithms', 'group-theory']"
4320437,Domain and Co-domain of a linear transformation not defined over the same Field,"I was thinking about linear tramsformations and i came up with this example: $$f:\mathbb{R}^n \to \mathbb{C}^n\\ 
f(x)=ix$$ for this example, domain and co-domain are not defined over the same field and all linear transformations that i encountered by now had domain and co-domain defined over the same field. I was wondering that if this is a valid linear transformation or not? and if not, why did we put such a constraint? also, if it is possible, keep the explanation simple because i'm pretty new in pure math. thank you in advance.","['linear-algebra', 'linear-transformations']"
4320481,Let $z$ be a complex number. The number $1$ is written on a board. You perform a series of moves. When can you make terms tend to $0$?,"Let $z$ be a complex number. The number $1$ is written on a board. You perform a series of moves, where in each move you may either replace the number $w$ written on the board with $zw$ ,
or replace the number $w$ with a different complex number $w'$ so that $$\max(\lvert\operatorname{Re} w\rvert, \lvert\operatorname{Im} w\rvert) = \max(\lvert\operatorname{Re} w'\rvert, \lvert\operatorname{Im} w'\rvert).$$ After some time, a positive real number less than $0.001$ is written on the board. The set of all $z$ for which this is possible forms a region $A$ in the complex plane. What is the area of $A$ ? So I'm baffled on where to begin this problem. I suppose defining $z$ = a + bi. I do not know where to go from here. I understand the max notation and the $\rm{Re}$ (Real) and $\rm{Im}$ (Imaginary) notation. This problem was sent to me by my math coach, so this may be from a book.","['complex-analysis', 'abstract-algebra', 'geometry', 'complex-numbers']"
4320512,Nowhere continuous functions $f: \mathbb{R} \to \mathbb{R}$ such that $f\bigl(f(x)\bigr) = \frac{f(2x)}{2}$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved Improve this question I'm looking for a function $f: \mathbb{R} \to \mathbb{R}$ that is continuous at no point and satisfies the identity $$f\bigl(f(x)\bigr) = \frac{f(2x)}{2}$$ for all $x \in \mathbb{R}$ . This is not a homework question, but rather a curiosity of mine. There is absolutely no context, I just thought of it. $f(x)=x$ is the example that made me think of this problem, but there are other solutions of the functional equation obviously, for example $f(x)=0$ for all $x$ . I was only able to notice that if $f$ is not identically $0$ then $f(\mathbb{R})$ is an infinite set.","['functional-equations', 'real-analysis', 'continuity', 'calculus', 'functions']"
4320590,Random Variable - Birthday Problem,"How many people are needed so that the probability that at least two of them were born on the same day of the week is at least 1/2? (Assume that the days of the week are equally likely to be the birthday of each person.) How many people are needed so that the probability that at least two of them were born in the same month of the year is at least 3/4? (Assume that the months are equally likely to be the birth month of each person.) How many people, none of whom was born on February 29, are needed so that the probability that at least two of them were born on the same day of the year is at least 7/8? (Assume the days of the year are equally likely to be the birthday of each person.) I read about this under the concept ""Random Variable"". The steps to follow are : Calculate the probability of $P_n$ for all different birthdays. The book suggests that this is calculated by the formula $$\frac{365}{366}.\frac{364}{366}...\frac{367-n}{366}$$ This value is very hard to calculate. So, the probability among $n$ people having the same birthday is : $1 - P_n = 1 - \frac{365}{366}.\frac{364}{366}...\frac{367-n}{366}$ Since I got all the answers wrong, I am trying to reverse engineer the solution to get the value of $X$ which is $$1 - P_n = X * \frac{367 - n}{366} $$ I searched this site and found that for question 1 (above) we have to consider 7 days instead of the 366 used in the bigger problem. My question is how do we arrive at $n = 4$ $$1 - P_n = X * \frac{8 - n}{7} $$ I have tried $$1 - P_n = 1 - \frac{6}{7}*\frac{5}{7}*\frac{4}{7}*\frac{3}{7}*\frac{2}{7}*\frac{1}{7}*\frac{8-n}{7} $$ Please help me with one solution, for the first one, I will reverse engineer and understand this for the other two.","['birthday', 'discrete-mathematics', 'random-variables']"
4320609,Generalizing Euler's infinite product of cosines,"The following formula is attributed to Euler: $$\frac{\sin(2 k)}{2 k} = \prod_{n=0}^\infty \cos(k \frac{1}{2^n})$$ This can be shown through $m$ applications of $\sin(x) = 2 \sin(x/2) \cos(x/2)$ to find $\sin(x) = 2^m \sin(x/2^m) \prod_{n=1}^m \cos(\frac{x}{2^n})$ . Taking $m \rightarrow \infty$ and replacing $x$ with $2k$ completes the proof. There are simple generalizations for other powers of $2$ inside the parentheses, such as $$\frac{\sin(2 k)}{2 k}\frac{\sin(\sqrt{2} k)}{\sqrt{2} k} = \prod_{n=0}^\infty \cos( k\frac{1}{\sqrt{2}^n})$$ which follow from separating the terms with even and odd powers $n$ . There are yet more generalizations that start from $$\frac{\sin(x)}{x} = \prod_{n=1}^{\infty} \frac{\sin(\frac{x}{q^{n-1}})}{n \sin(\frac{x}{q^{n}})}$$ and are most easily seen by taking the upper bound on the product to be $m$ , telescoping the terms in the product, and then taking $m \rightarrow \infty$ . Now for the question. Consider the following function: $$f_{\lambda, r}(k) = \prod_{n=0}^\infty\left(\cos(k\lambda^n)+ i r \sin(k\lambda^n) \right)$$ Note that it is clear that when $r=1$ , the formula above reduces to $e^{i \frac{k}{1-\lambda}}$ , and when $r=0$ and $\lambda=1/2, 1/\sqrt{2}$ , the formula reduces to the two cases at the top of the question. Are there specific cases of $\lambda$ and $r$ when one can simplify $f_{\lambda, r}(k)$ to some simple function of $k$ ? In particular, I would like the formula to hold for all $k$ , but I am happy with fixed, nontrivial choices of $\lambda$ and $r$ ( $\lambda \neq -1,0,1$ , $r\neq -1,0,1$ ). As an example, an evaluation for the case $r=1/2$ and $\lambda = 1/2$ would suffice.","['complex-analysis', 'infinite-product']"
4320664,"Let $H\leq G$. Prove $x^{-1}y^{-1}xy\in H\text{ }\forall x,y\in G$ iff $H\trianglelefteq G$ and $G/H$ is abelian.","Question: Let $H\leq G$ .  Prove $x^{-1}y^{-1}xy\in H\text{ }\forall x,y\in G \iff H\trianglelefteq G \text{ and } G/H \text{ is abelian}$ . my thoughts: In the forward direction, if $x^{-1}y^{-1}xy\in H$ for all $x,y\in G$ , then $y^{-1}xy\in xH\subseteq G$ , so, since $y\in G$ , I just need to show that $x\in H$ to show that $H\trianglelefteq G$ , right?
In the backwards direction, since $H\trianglelefteq G$ , I can consider the cosets $xH$ , $yH$ , $x^{-1}H$ , and $y^{-1}H$ where $x,y\in G$ , thus so are their inverses.  Then, I consider $(xH)(x^{-1}H)(yH)(y^{-1}H)$ , but I am not quite sure how to finish from here.  Any help is greatly appreciated!  Thank you.","['quotient-group', 'normal-subgroups', 'abstract-algebra', 'group-theory', 'abelian-groups']"
4320693,Is an reflexive relation also antisymetric?,"I've already seen a similar question here: Is an Anti-Symmetric Relation also Reflexive? But my question is rather, if you know that a relation is reflexive, then, can this relation also be antisymetric? As far as I know, by definition, a relation is antisymetric if for two elements in R , xRy and yRx then x=y . For this to be true, I should only need x=y to be true for the relation to be antisymetric. In that case, the relation is antisymetric if we know it is reflexive already. Am I wrong?","['relations', 'discrete-mathematics']"
4320705,Are there infinitely many unique-period prime numbers?,"Wolfram.mathworld.com defines a unique prime in the following way: ""Following Yates (1980), a prime $p$ such that $\frac{1}{p}$ is a repeating decimal with decimal period shared with no other prime is called a unique prime. For example, $3$ , $11$ , $37$ , and $101$ are unique primes."" OEIS has unique-period primes, and the list they give is finite. My question is whether we know if there are infinitely many unique primes, or if there is an easy way to prove that there are finitely many. Quick clarification: I know that there are infinitely many primes already and how to prove that. I am particularly focused on unique primes .","['number-theory', 'elementary-number-theory']"
4320723,Real Schur decomposition of orthogonal matrix,"The real Schur decomposition theorem states that for any matrix $A\in\mathbb R^{n\times n}$ , there exists an orthogonal matrix $Q$ and a ""quasitriangular"" matrix $T$ such that $A=QTQ^T$ . Here, ""quasitriangular"" means that $T$ has the form $$T=\begin{pmatrix}B_1&&&*\\&B_2&&\\&&\ddots&\\0&&&B_n\end{pmatrix},$$ where all $B_i$ are either $1\times1$ or $2\times2$ matrices. These matrices form the ""quasidiagonal"". I want to prove that in the specific case of real Schur decomposition of an orthogonal matrix, $T$ must be a ""quasidiagonal matrix"", i.e. all entries above the quasidiagonal are zero. This is claimed without proof in this answer . It's easy to see that $T$ must be orthogonal. In the special case where $T$ is upper triangular, it's simple to prove that the norm of the $i$ -th column equals that of the $i$ -th row, which then yields the desired result by a simple induction. However, my attempt at adapting this proof to the quasidiagonal case failed. This result seems to me like it should be well-known. A reference would also suffice.","['schur-decomposition', 'reference-request', 'matrices', 'orthogonal-matrices', 'linear-algebra']"
4320733,Do I catch all solutions for the generalized Pell-equation $a^2+b^2 = 2 c^2$ by this matrix-method?,"As one question in a somewhat bigger analysis I want to characterize the set of solutions of the generalized Pell-equation in the title: $$a^2+b^2 = 2 c^2 \tag 1$$ I'm not much fluent with the Pell-solving technology, and from some initial brute-force solutions I constructed the following scheme, which seems to me as if it could produce all solutions by changing a vector of parameters. My question is, whether the set of solutions that I find is exhaustive or not - tests in small numbers seem to suggest that. I introduce two matrices; $$T= \begin{bmatrix} 1&0&0\\0&3&2\\0&4&3 \end{bmatrix} \tag 2$$ $$C= \begin{bmatrix} 0&1&0\\1&0&0\\0&0&1 \end{bmatrix} \tag 3$$ The starting solution is the vector $A_0=[a,b,c]=[1,1,1]$ . With this I create further solutions by $$A_k=A_0 \cdot T^k \qquad k \in \mathbb Z \tag {4.1}$$ which gives a list with all solution having $a=1$ .
But from each of those solutions one can derive a new initial $A_0$ from where then again a complete list of solutions can be drawn: $$A_{k_1,k_2,k_3}=A_0 \cdot T^{k_1} \cdot C \cdot T^{k_2}\cdot C \cdot T^{k_3} \tag {4.2}$$ and so on, for arbitrarily many indexes/exponents $k_j$ . Using Pari/GP I implemented a function, which takes the vector of arbitrary length containing the indexes/exponents as argument in the following form $$  A_x = \operatorname{Pell2Sol}([k_1,k_2,...,k_n]) \tag 5$$ I don't know, whether the resulting structure can be called a tree or whether it has another name. Q: As I said above, it seems to me that this indexing of solutions is exhaustive (assuming the number of indexes $n$ is taken from $1$ to $\infty$ ). Does someone know whether this is true? Context: if this is a valid method, I'll try to extend/adapt it to some similar equations which occur in a combined set of equations in quadratics. Here is a simple Pari/GP-implementation: \\ Update: initial vector A0 is configurable to have a gcd-common factor
\\ vector E takes the indexes
\\ env=0 says: give only that solution, 
\\ env=1 says: printout the set of neigboured solutions towards index +- env.
\\ A0 can be configured by a common factor in the function call
{Pell2Sol(E,env=0,gcd=1)=my(A0=gcd*[1,1,1], T=[1,0,0;0,3,2;0,4,3],C=[0,1,0;1,0,0;0,0,1]);
     if(#E==0,return(A0));
     A0*=prod(k=1,#E,C*T^E[k]); 
     if(!env,return(A0));
     return(Mat(vectorv(2*env+1,r,A0 * T^(r-1-env))));} Giving Pell2Sol([1,0],3) shows the following sequence of solutions: a    b    c
 --------------
  7  -601  425
  7  -103   73
  7   -17   13
  7     1    5
  7    23   17
  7   137   97
  7   799  565 Here the negative signs should not be significant, since we use the squares anyway. But the signs are significant when computing the lists of solutions using the powers of the matrix T and the matrix C . I didn't supress the signs here to show that internal effect. and Pell2Sol([1,5,1,0],3) gives a         b        c
  76793  -5375863  3801697
  76793   -920801   653365
  76793   -148943   118493
  76793     27143    57593
  76793    311801   227065
  76793   1843663  1304797
  76793  10750177  7601717 Perhaps a better picture than the two above is the following, (where I also draw the motivating connection to the problem of finding magic squares-of-squares)  (Screenshot):","['number-theory', 'pell-type-equations', 'elementary-number-theory']"
4320835,Quantification and the existence of a function,"I am trying hard to understand what exactly the axiom of choice (AC) is. I think much of the confusion comes from taking AC obvious not knowing what exactly ZF implies. AC for nonempty indexed families $(B_x )_{x \in A}$ is represented by $$
\forall x \in A:B_x \neq \varnothing \Rightarrow \exists f \in A \to \cup_{x \in A}B_x:\forall x \in A: f(x) \in B_x
$$ where $X \to Y$ denote the set of all functions from $X$ to $Y$ . For example, for sets $X$ and $Y$ , and a predicate $P$ , the statement $$
\forall x \in X: \exists y \in Y: P(x,y)
\Rightarrow
\exists f \in X \to Y: \forall x \in X: P(x,f(x))
$$ is a direct consequence of AC as we can define $B_x = \{y \in Y|P(x,y)\}$ . My question is, does the statement really need AC in ZF?","['elementary-set-theory', 'first-order-logic']"
4320848,Finding the $n$th derivative of $f(x) = e^{\sin(x)}$,"I have a function $$f(x) = e^{\sin(x)}$$ I want to expand it in a infinte series using Maclaurin's theorem and for that I need to know if the remainder term $$R_n = \frac{x^n}{n!}f^{(n)}(\theta x),\quad (0<\theta<1)\quad[\text{Lagrange's from}] $$ converges as $n \to \infty$ but for that I need to know the $f^{(n)}(x) = e^{\sin(x)}$ the $n$ th derivative of exponential trigonometric function. I know how to find $n$ -th derivative of functions like $\sin(x)$ or $ e^{ax}$ where you can  see the pattern but I cant see anything in this function. $$\begin{align}
f'(x) &= \phantom{-}e^{\sin(x)}\cos(x) \\
f''(x) &= -e^{\sin(x)}(\sin(x)-\cos^2(x)) \\
f'''(x) &= -e^{\sin(x)}\cos(x)(3\sin(x)-\cos(2x)+1) \\
f^{(4)}(x) &= \phantom{-}e^{\sin(x)}(3\sin^2(x)+(1-6\cos^2(x))\sin(x)+\cos^4(x)+4\cos^2(x))
\end{align}$$ How am I supposed to find the $n$ th derivative of function like these?","['exponential-function', 'calculus', 'derivatives', 'trigonometry']"
4320849,Finding the range of $y =\frac{x^2+2x+4}{2x^2+4x+9}$ (and $y=\frac{\text{quadratic}}{\text{quadratic}}$ in general),"I had this problem in an exam I recently appeared for: Find the range of $$y =\frac{x^2+2x+4}{2x^2+4x+9}$$ By randomly assuming the value of $x$ , I got the lower range of this expression as $3/7$ . But for upper limit, I ran short of time to compute the value of it and hence couldn't solve this question. Now, I do know that one way to solve this expression to get its range is to assume the whole expression as equals to K, get a quadratic in K, and find the maximum/minimum value of K which will in turn be the range of that expression. I was short on time so avoided this long winded method. Another guy I met outside the exam center, told me he used an approach of $x$ tending to infinity in both cases and got the maximum value of this expression as $1/2$ . But before I could ask him to explain more on this method, he had to leave for his work. So, will someone please throw some light on this method of $x$ tending to infinity to get range, and how it works. And if there exists any other efficient, and quicker method to find range of a function defined in the form of a ( quadratic / quadratic ).","['quadratics', 'calculus', 'functions', 'derivatives']"
4320875,Is there a shortcut to Miller's algorithm?,"Consider an elliptic curve $E: y^2 = x^3 + ax + b$ over some finite field $F_{q^k}$ and a point $P$ on $E$ of order $n$ . Miller's algorithm tells us how to efficiently construct a rational function $f_{n,P}(x,y)$ on $E$ with divisor $div f = n [P]  - n [\mathcal{O}]$ . I have worked through examples and exercises and have understood how and why Miller's algorithm works. But I cannot answer the following question: Why can't we just take $f_{n,P} = (y-y_P)^n$ ? It obviously has an $n^{\rm th}$ order root at $P$ and $n^{\rm th}$ order pole at $\mathcal{O}$ . It's also obvious that it doesn't have any other poles or roots, because only the point $P$ has $y$ -coordinate $y_P$ . What am I missing?","['algebraic-geometry', 'elliptic-curves']"
4320912,How many secret codes can be made by assigning each letter of the alphabet a (unique) different letter?,"The letter A can be assigned in 26 ways The letter B can be assigned in 25 ways . . . The letter Z can be assigned in 1 ways So the answer is 26! and in Euler constant form is $e^{61.26170}$ However, the answer in the text book is $≈ (26!)^2/e$ Could you please help me with this problem?","['combinatorics', 'discrete-mathematics']"
4320927,Conditional expectation given an event and a $\sigma$-algebra,"Let $X$ be an integrable real random variable on the probability space $(\Omega,\mathcal A,P)$ . If $A\in \mathcal A$ is an event with probability $0<P[A]<1$ then we have that $$E[X|\sigma(A)]=1_A \frac{E[1_AX]}{P[A]}+1_{A^c} \frac{E[1_{A^c}X]}{P[A^c]} \quad P\text{-a.s.} $$ Now suppose $\mathcal F$ a sub- $\sigma$ -algebra of $\mathcal A$ . Am wondering if the following holds: $$E[X|\sigma(A,\mathcal F)]=1_A \frac{E[1_AX|\mathcal F]}{P[A|\mathcal F]}+1_{A^c} \frac{E[1_{A^c}X|\mathcal F]}{P[A^c|\mathcal F]} \quad P\text{-a.s.} \quad \quad (1)$$ assuming that $0<P[A|\mathcal F]<1$ $P$ -a.s. (I don't see how to dispense with this assumption). It seems to me that the answer is  yes based on the following argument: Let $Y$ denote the RHS of $(1)$ .Then $Y$ is $\sigma(A,\mathcal F)$ measurable. We check that $Y$ is integrable using the conditional Jensen's inequality,the law of total expectation, and the pull-out property: $$ E\Bigg[\Bigg|1_A \frac{E[1_AX|\mathcal F]}{P[A|\mathcal F]}\Bigg|\Bigg]\leq E\bigg[1_A \frac{E[1_A |X|\mid\mathcal F]}{P[A|\mathcal F]}\bigg]= E\bigg[E[1_A|\mathcal F] \frac{E[1_A |X|\mid\mathcal F]}{P[A|\mathcal F]}\bigg]=E[1_A|X|]< \infty$$ and similarly $E\Bigg[\Bigg|1_{A^c} \frac{E[1_{A^c}X|\mathcal F]}{P[A^c|\mathcal F]}\Bigg|\Bigg]<\infty$ . Now, we verify that $\sigma(A,\mathcal F)=\Big\{(A\cap B_1) \cup (A^c\cap B_2) : B_1,B_2\in\mathcal F \Big\}$ , and therefore by additivity it is is sufficient to check that $E[1_{A\cap B}Y]=E[1_{A\cap B}X]$ and $E[1_{A^c\cap B}Y]=E[1_{A^c\cap B}X]$ for all $B\in\mathcal F$ . This we verify by direct computation, using again the law of total expectation and the pull-out property: $$E[1_{A\cap B}Y]=E\bigg[1_A\frac{E[1_{A\cap B}X|\mathcal F]}{P[A|\mathcal F]}\bigg]=E\bigg[E[1_A|\mathcal F]\frac{E[1_{A\cap B}X|\mathcal F]}{P[A|\mathcal F]}\bigg]=E[1_{A\cap B}X]$$ and similarly $E[1_{A^c\cap B}Y]=E[1_{A^c\cap B}X]$ . Am I missing something? Is there a way to ensure that $0<P[A|\mathcal F]<1$ $P$ -a.s.? Thanks a lot for your help. Proposition. A necessary and sufficient condition for $0<P[A|\mathcal F]<1$ a.s. is that $P[A\cap B]>0$ and $P[A^c\cap B]>0$ for all $B\in \mathcal F$ with $P[B]>0$ . In other words, the partition $\{A,A^c\}$ of $\Omega$ must split each non-null set of $\mathcal F$ into two non-null parts. Necessity. Suppose $0<P[A|\mathcal F]<1$ a.s.. Let $B\in \mathcal F$ with $P[B]>0$ . Then $$P[A\cap B]=E[1_{A\cap B}]=E[P[A|\mathcal F] 1_B]>0$$ for otherwise $P[A|\mathcal F] 1_B=0$ a.s. and so $P[A|\mathcal F]=0$ on an $\mathcal F$ -measurable set of positive measure, contradiction. Similarly, $P[A^c\cap B]>0$ , for otherwise $P[A|\mathcal F]=1$ on an $\mathcal F$ -measurable set of positive measure. Sufficiency. Suppose $P[A\cap B]>0$ and $P[A^c\cap B]>0$ for all $B\in \mathcal F$ with $P[B]>0$ . Suppose first by contradiction that $P[A|\mathcal F]=0$ on a set $B\in \mathcal F$ with $P[B]>0$ . Note that $P[B]<1$ , for otherwise $P[A|\mathcal F]=0$ a.s. which implies $P[A]=0$ , contradiction. Then $$E[1_A|\mathcal F]=1_BE[1_A|\mathcal F]=E[1_{A\cap B}|\mathcal F]$$ and so $E[1_{A\cap B^c}|\mathcal F]=E[1_A|\mathcal F]-E[1_{A\cap B}|\mathcal F]=0$ a.s.. This implies $P[A\cap B^c]=0$ , contrary to hypothesis. Next suppose by contradiction that $P[A|\mathcal F]=1$ on a set $B\in \mathcal F$ with $P[B]>0$ . Note that $P[B]<1$ , for otherwise $P[A|\mathcal F]=1$ a.s. which implies $P[A]=1$ , contradiction. Then $E[1_A|\mathcal F]\geq 1_B$ a.s. and so $$E[1_{A\cap B}|\mathcal F]=1_B E[1_A|\mathcal F]\geq 1_B=E[1_B|\mathcal F]$$ which implies $0\geq E[1_{A\cap B}-1_B|\mathcal F]\geq 0$ or $E[1_{A^c\cap B}|\mathcal F]=0$ a.s.. Hence $P[A^c\cap B]=0$ , contrary to hypothesis.","['measure-theory', 'conditional-expectation', 'solution-verification', 'probability-theory', 'probability']"
4320975,Attracting fixed point of $f$ if and only if repelling fixed point of $f^{-1}$,"While working on some dynamical system problems, I came across an interesting exercise that has left me stumped for quite a few weeks now. I have a solution, but I don't think it holds enough water yet, so I was hoping to get some feedback by posting it here. First, some (perhaps strange) definitions (for simplicity we will assume that everything is in regards to locally compact metric spaces): A fixed point $p$ of a continuous map $f$ is called topologically attracting if it has a neighbourhood $U$ in which the iterates $f^n$ are all defined on $U$ and the sequence $\{f^n|_U\}$ converges uniformly to the constant map $U \to \{p\}$ . A fixed point $p$ of a continuous map $f$ is called topologically repelling if it has a neighbourhood $U$ so that for all $x \neq p$ in $U$ , there exists some $n \geq 1$ such that $f^n(x) \notin U.$ Hence, the only infinite orbit that completely lies in $U$ is the orbit of the fixed point itself. The problem that I am attempting is as follows: Let $f: (X, d) \to (X, d)$ be a continuous map on a locally compact metric space with fixed point $p$ . Suppose that $f$ maps a compact neighbourhood $K$ of $p$ homeomorphically onto a compact neighbourhood $K'$ . Then the restriction $f: K \to K'$ is topologically repelling at $p$ if and only if its inverse $g: K' \to K$ is topologically attracting at $p$ . I believe I have the forward direction, but the backwards direction seems to escape me. This is what I have tried (feedback is much appreciated)! ( $\impliedby$ ) Suppose that $g$ is topologically attracting at $p$ , and suppose for a contradiction that $f$ is not topologically repelling. Because $g$ is topologically attracting, there exists a compact neighbourhood $N' \subseteq K'$ of $p$ such that the iterates $\{g^n\}$ converge uniformly on $N'$ to the constant map $N' \to \{p\}$ . Since $f$ is not topologically repelling, then for every compact neighbourhood $N \subseteq K'$ of $p$ , there exists $x_0 \neq p$ in $N$ such that for all $n \geq 1$ , $f^n(x_0) \in N$ . Take $N = N'$ . Then after passing to a subsequence, we may assume that $f^n(x_0)$ converges to some limit $\hat{x}_0 \in N'$ , for $N'$ is compact in a metric space. Next, fix $n \in \mathbb{N}$ . Then as the iterates $g^m$ uniformly converges, we have \begin{equation*}
\lim_{m \to \infty} d(g^m(f^n(x_0)), p) = 0.
\end{equation*} Moreover, we may interchange limits to obtain \begin{align*}
0 ={} & \lim_{n \to \infty} \lim_{m \to \infty} d(g^m(f^n(x_0)), p)
																				\\
={} & \lim_{m \to \infty} \lim_{n \to \infty} d(g^m(f^n(x_0)), p)
																				\\
={} & \lim_{m \to \infty} d(g^m(\hat{x}_0), p),
\end{align*} where the last equality follows from continuity of $g^m$ . Hence, for sufficiently large $m$ , we have $g^m(\hat{x}_0) = p = g^m(p)$ , which by injectivity implies that $\hat{x}_0 = p$ . Thus, $f^n(x_0) \to p$ as $n \to \infty$ . But since $p$ is a fixed point of $f$ , we also have that $f^n(x_0) = f^n(p)$ for sufficiently large $n$ . Therefore, $x_0 = p$ by injectivity of $f$ , a contradiction. (I have major misgivings about the last paragraph here...)","['general-topology', 'fixed-points', 'dynamical-systems']"
4320991,Recurrence relation for Kravchuk polynomials,"I'm reading this article, where they use the following equivalent expressions for the Kravchuk polynomials: \begin{equation}
    \begin{split}
        K_j(i) &= \sum_{h=0}^j (-1)^h(q-1)^{j-h} \binom{i}{h}\binom{d-i}{j-h} \\
        &= \sum_{h=0}^j (-q)^h(q-1)^{j-h} \binom{i}{h}\binom{d-h}{j-h} \\
        &= \sum_{h=0}^j (-1)^h q^{j-h} \binom{d-i}{j-h}\binom{d-j+h}{h}
    \end{split}
\end{equation} In the paper, they claim that the following recurrence relation holds for $i,j\geq 1$ without any proof: \begin{equation}
    (q-1)(d-i)K_j(i+1) - (i+(q-1)(d-i)-qj)K_j(i) + i K_j(i-1) = 0
\end{equation} Could anyone help me figuring out how to prove this?","['combinatorics', 'polynomials', 'recurrence-relations', 'recursion']"
4321006,A circle and two perpendicular lines enclose four regions. Can the regions have distinct rational areas?,"A circle and two perpendicular lines enclose four regions. Can the regions have distinct rational areas? (I stipulate distinct to eliminate trivial cases in which one of both of the perpendicular lines go through the center of the circle.) My approach was to let the circle be $x^2$ + $y^2$ = $1/\pi$ (so the area of the circle is one), then let the perpendicular lines be $y=a$ and $x=b$ , then express the four areas in terms of $a$ and $b$ and try to show that the areas cannot all be rational. But I quickly got lost in very complicated expressions for the areas. I've tried to work with this , but I have come up with nothing. Here is another question about four regions in a circle, which has not received any answer yet; I'm not sure if it would help answer my question. This resolved question makes me suspect that the answer to my question is no.","['number-theory', 'trigonometry', 'geometry', 'real-analysis']"
4321008,Number of irreducible DNFs of cyclic boolean function,"Let us say that cyclic boolean function is a function such that we can connect all its True vertices with edges (in boolean cube) and get a cycle. Let the number of edges in this cycle equals $n$ . I need to find the number of irreducible DNFs (disjunctive normal forms) for the cyclic boolean function with cycle length n (maybe asymptotically). I've already have a solution of the problem connected to that: for snake boolean function (such that all its True vertices connected with edges generate a chain of length $n$ ) there are asymptotically $c \cdot \lambda^n$ irreducible DNFs where $c$ is a constant and $\lambda$ is the a real root of equation $x^3 - x - 1 = 0$ . Moreover, it is said that for cyclic function I will have the same asymptotic but another constant $c$ . Can you please help me to find the solution?","['boolean-algebra', 'discrete-mathematics', 'disjunctive-normal-form']"
4321052,Understanding a definition of the exterior derivative,"I'm trying to prove the exact same formula as in this question : $U \subset \mathbb{R^n}$ an open set, $\omega$ a k-differential form on $U$ , $X_0, \dots, X_k$ vector fields on U, i. e. elements of $C^{\infty}(U)^n$ . I am supposed to show (for k = 1, 2): $$d\omega(X_0, ... , X_k) = \sum_i (-1)^i X_i(\omega(X_0, ... , \hat{X_i}, ... , X_k)) + \sum_{i < j}(-1)^{i+j}\omega([X_i, X_j], X_0, ... , \hat{X_i}, ... , \hat{X_j}, ..., X_k)$$ In the lecture I'm following we did not learn about Cartans magic formula. I don't even know what $(\mathcal{L}_Y \omega)$ means. My main problem is: I'm struggling to understand what $X_i(\omega(X_0, ... , \hat{X_i}, ... , X_k))$ is: I thought $\omega(X_0, ... , \hat{X_i}, ... , X_k)$ is an element of $C^{\infty}(U)$ . How can we feed it to $X_i$ ?","['vector-fields', 'differential-forms', 'differential-geometry']"
4321093,"If $F=\{(x,y,z)\in\mathbb R^3:f(x,y,z)=0\}$ is non-empty and $\frac{\partial f}{\partial x}\neq0$ on $F$, can $F$ be finite?","In our analysis exam, we were given the question Let $f:\mathbb R^3\to \mathbb R$ is a $\mathcal C^\infty$ -smooth function. Let $F=\{(x,y,z)\in \mathbb R^3: f(x,y,z)=0\}$ is non-empty and $\frac{\partial f}{\partial x}\neq 0$ on $F$ . Prove or give a counterexample- $F$ can never be a collection of finitely many points in $\mathbb R^3$ . On one hand, I tried to figure out a lot of counterexamples none of which worked. And, on the other hand, I couldn't figure out any way to show that one point in $F$ forces more of them. I can't show much of my work except some counterexamples, which I thought were quite close $f(x,y,z)=x^2+y^2+z^2$ which gives only one point in $F$ which is unfortunately the point where $\frac{\partial f}{\partial x}=0$ . $f(x,y,z)=x^2+y^2+z^2+1$ which satisfies all other properties except that $F$ is not non-empty. I couldn't figure out any more.","['multivariable-calculus', 'calculus', 'analysis']"
4321258,Pullbacks and isomorphisms of vector bundles,"I want to clarify certain details regarding pullbacks of vector bundles. I suspect my question will be equivalent in every possible setting: topological, differentiable and holomorphic vector bundles. Setting Let $X$ be a manifold and $E$ a vector bundle over $X$ . We consider automorphisms of $X$ , namely those isomorphisms in the appropriate category (homeomorphisms, diffeomorphisms, biholomorphisms...). Let $g:X\rightarrow X$ be one such automorphism. I can construct the pullback bundle by either declaring transition functions from those of $E$ $$
E \equiv \{U_i, \psi_{ij}\} \implies g^*E \equiv \{g^{-1}(U_i), \psi_{ij}\circ g\}
$$ or even more explicitly by $$
g^*E = \{ (x,e)\in X\times E| g(x)=\pi(e)\}
$$ I already know that there is a commutative diagram involving these bundles $$\begin{array}
*g^*E & \stackrel{\hat{g}}{\longrightarrow} & E \\
\downarrow{p_1} & & \downarrow{\pi} \\
X & \stackrel{g}{\longrightarrow} & X  
\end{array}
$$ and $g(x,e)=e$ covers the automorphism $g$ . Question I want to clarify if it is possible that $g^*E$ is isomorphic to the original bundle, meaning there is a commutative diagram as above covering the identity on X . I know this is a stronger condition, but as I have $g$ an automorphism of $X$ , I suspect this could be possible in some situations. For example, from the classification of line bundles over $\mathbb{P}_\mathbb{C}^1$ , I can use the pullback of a connection and show that the degree/first Chern number is invariant under the action of $g\in SL(2,C)\subset Aut(\mathbb{P}_\mathbb{C}^1)$ , and thus $g^*L\simeq L$ for every line bundle over the projective line. I am trying to come up with a proof that either implements explicitly an identity-covering-vector-bundle-map or a proof involving Cech cohomology, but I would really prefer the former. If this is false, I would like to build an explicit counterexample.","['vector-bundles', 'algebraic-geometry', 'differential-geometry']"
4321349,The convolution formula for 3 distribution functions,"I am trying to find a generalization of the convolution formula between distribution functions. In the case of 2 functions, I know that: $$F_0*F_1\,(x) = \int_{-\infty}^{+\infty}F_0(x-x_1) d\,F_1(x_1) $$ Where $d\,F_1(x_1)$ stresses the Lebesgue-Stieltjes integral notation. I would like generalize to 3 functions. I can do: $$F_0*F_1*F_2\,(x) = \int_{-\infty}^{+\infty}F_0(x-x_1) d\,F_1*F_2\,(x_1) $$ Where, according to the firts equation: $F_1*F_2 \,(x_1)= \int_{-\infty}^{+\infty}F_0(x_1-x_2) dF_2(x_2)$ . But I don't know how to find: $$d\, F_1*F_2\,(x_1) = ? $$ to after replace on the second equation and then find my forumula. How can I do this? Update: I tryed something like this. Let $f$ be the density of some distrib. function $F$ , then $ dF(x) = f(x)dx$ . So, we can do $$d F_1*F_2(x_1) = f_1 * f_2 (x_1)dx_1$$ Where $f_1 * f_2 (x_1) = \int_{-\infty}^{+\infty}f_1(x_1 - x_2)f_2(x_2) dx_2$ . So, \begin{align}
F_0*F_1*F_2\,(x) &= \int_{-\infty}^{+\infty}F_0(x-x_1) d\,F_1*F_2\,(x_1)\\
&=\int_{-\infty}^{+\infty}F_0(x-x_1) f_1 * f_2 (x_1)dx_1\\
&=\int_{-\infty}^{+\infty}F_0(x-x_1) \left[\int_{-\infty}^{+\infty}f_1(x_1 - x_2)f_2(x_2) dx_2\right] dx_1\\
&=\int_{-\infty}^{+\infty}\left[\int_{-\infty}^{+\infty}F_0(x-x_1) f_1(x_1 - x_2)f_2(x_2) dx_2\right]dx_1\\
&=\int_{-\infty}^{+\infty}\left[\int_{-\infty}^{+\infty}F_0(x-x_1) f_1(x_1 - x_2)f_2(x_2) dx_1\right]dx_2\\
&=\int_{-\infty}^{+\infty}\left[\int_{-\infty}^{+\infty}F_0(x-x_1) f_1(x_1 - x_2) dx_1\right]f_2(x_2)dx_2\\
&=\int_{-\infty}^{+\infty}\left[\int_{-\infty}^{+\infty}F_0(x-x_1) f_1(x_1 - x_2) dx_1\right]d F_2 (x_2)      
\end{align} From here on, I don't know how to get a closed equation.","['cumulative-distribution-functions', 'measure-theory', 'probability-distributions', 'convolution']"
4321371,Show that the dimention of the intersection of projective linear sub-spaces of dimentions $d_1$ and $d_2$ of $\mathbb{P}^n$ is bigger than $d_1+d_2-n$,"Proposition: Let $L$ and $M$ linear projective subspaces of $\mathbb{P}^n$ of dimention $d_1$ and $d_2$ respectively. Prove that $\operatorname{dim}(L\cap M)\geq d_1+d_2-n$ (we consider the dimention of empty set is $-1$ ). When occur the equalty? Remark: Let $L\subset \mathbb{P}^n$ a projective sub-variety defined by $L=(f_1,f_2,\ldots, f_s)$ where the $f_i$ are homogenous polynomials of degree $1$ and linearly independent.Such $L$ is called linear projective subspace . Attempt: I use the fact if $L$ is a linear projective space then $L\simeq \mathbb{P}^{n-s}$ , in particular $\operatorname{dim}L=n-s$ .Let us consider $L=V(f_1,f_2,\ldots,f_{d_1})$ and $M=V(g_1,g_2,\ldots,g_{d_2})$ two linear projective varietys where $f_i,g_j$ are homogenous polynomials of degree $1$ and linearly independent. Now $$L\cap M= V(f_1,f_2,\ldots,f_{d_1})\cap V(g_1,g_2,\ldots,g_{d_2})=V((f_1,f_2,\ldots,f_{d_1})+(g_1,g_2,\ldots, g_{d_2}))$$ and hence $$L\cap M=V(f_1,f_2,\ldots, f_{d_1},g_1,g_2,\ldots, g_{d_2})$$ And by the lemma $\operatorname{dim}(L\cap M)=n-d_1-d_2$ as well. But this doesn´t match with the result. I don´t figure out where I get a fault and also how I should procede to prove that inequality.
Any suggestion was useful. Thanks in advice.","['projective-schemes', 'vector-spaces', 'krull-dimension', 'algebraic-geometry', 'projective-space']"
4321389,Is the set of matrices with constrained condition numbers a convex set?,"Let $\mathbb{S}_{\tau}^{+,p}$ indicates the $p\times p$ real symmetric positive-semidefinite matrix, whose condition number, defined as the ratio of maximum eigenvalue and minimum eigenvalue, is less or equal to $\tau$ . Is $\mathbb{S}_{\tau}^{+,p}$ convex? I attempted to use the definition of convexity to prove it. Suppose $M_1,M_2\in \mathbb{S}_{\tau}^{+,p}$ , and $\forall v\in(0,1)$ , I would like to show $vM_1+(1-v)M_2\in\mathbb{S}_{\tau}^{+,p}$ . I understand that $$\lambda_{\text{min}}\left[\nu M_1+(1-\nu)M_2\right]\geq\lambda_{\text{min}}[\nu M_1]+\lambda_{\text{min}}[(1-\nu) M_2]\geq\min\{\lambda_{\text{min}}[M_1], \lambda_{\text{min}}[M_2]\}$$ and $$\lambda_{\text{max}}\left[\nu M_1+(1-\nu)M_2\right]\leq\lambda_{\text{max}}[\nu M_1]+\lambda_{\text{max}}[(1-\nu) M_2]\leq\max\{\lambda_{\text{max}}[M_1], \lambda_{\text{max}}[M_2]\},$$ but they do not guarantee that $$\frac{\lambda_{\text{max}}\left[\nu M_1+(1-\nu)M_2\right]}{\lambda_{\text{min}}\left[\nu M_1+(1-\nu)M_2\right]}\leq \tau$$ If the statement is not true, can you give a simple counterexample? Thanks!","['condition-number', 'eigenvalues-eigenvectors', 'matrices', 'linear-algebra', 'convex-analysis']"
4321390,Symmetric random walk on the integers,"I found an exercise in a paper by Anupam Gupta, Sahil Singla that i couldn't solve, sadly my knowledge in Markov processes is pretty limited. The exercise is the following: Suppose we have a symmetric random walk of length n on $\mathbb{Z}$ starting at point 0 conditioned on ending at the origin. Define $I_t$ as the position on $\mathbb{Z}$ after $t$ steps. Use a Chernoff-Hoeffding bound to show that $\max_t I_t-\min_t I_t$ is at most $O(\log n)$ with probability $1-\frac{1}{\text{poly}(n)}$ . I really do not know how to approach this, i would appreciate some help.","['probability-theory', 'random-walk']"
4321414,recurrence involving permutations,"Let $K_n$ be the number of permutations $\pi$ of $\{1,2,\cdots, n\}$ so that $|i-j|=1$ implies $|\pi(i)-\pi(j)| \leq 2$ for all $i,j$ in $\{1,2,\cdots, n\}$ . Show that for $n\ge 2, K_{n+5} - K_{n+4} - K_{n+3} + K_n = 4.$ I know recurrence relations that can be used to derive the answer. However, I don't understand how those recurrences were obtained, so could someone elaborate or provide a proof on that? Below are the recurrences I'm confused about. Let $A_n$ be the number of permutations $\pi$ of $K_{n+1}$ with $\pi(1) = 1$ , where $K_1 := 1$ . Then $K_n = 2(A_0+A_1+\cdots + A_{n-3}+A_{n-1})\,\forall n\ge 2$ and $A_n = A_{n-1}+A_{n-3} + 1$ for $n\ge 3$ . Also, one has the following recurrences: Assume $n\ge 3$ . Let $U_n$ be the number of permutations counted by $K_n$ that end with $n-1,n$ , let $V_n$ be the number ending in $n,n-1$ , let $W_n$ be the number starting with $n-1$ and ending in $n-2,n$ , let $T_n$ be the number ending in $n-2,n$ but not starting with $n-1$ , and let $S_n$ be the number that has $n-1,n$ consecutively in that order but not at the beginning or end of a permutation. (Why is it true that) every permutation $\pi$ counted by $K_n$ either lies in exactly one of the sets counted by $U_n, V_n, W_n, T_n, S_n$ or is the reverse of such a permutation. Thus $P_n = 2(U_n + V_n + W_n+ T_n+S_n)$ . Also one has the recurrence relations $U_{n+1} = U_n + W_n + T_n, V_{n+1 } = U_n, W_{n+1} = W_n, T_{n+1} = V_n, S_{n+1} = S_n + V_n$ . I get that $W_n = 1$ for all $n$ ; $n-1$ must be beside $n-3$ , $n-2$ must follow $n-4,$ etc. So a permutation in $W_n$ would have to be of the form $(n-1,n-3,n-5,\cdots, n-4, n-2,n)$ . If $n$ is odd, in the middle of the permutation is $2,1$ and if n is even there is $1,2$ . Clearly every permutation counted by $U_n, W_n, V_n, S_n, T_n$ lies in the set of permutations counted by $K_n$ . Now suppose $\pi$ is a permutation counted by $K_n$ that does not lie in $U_n, W_n, V_n, S_n, $ or $T_n$ and is not the reverse of a permutation lying in $U_n, W_n, V_n, S_n, $ or $T_n$ . $n$ must be paired with $n-1$ or $n-2$ in any permutation counted by $K_n$ . We cannot have $\pi(n) = n$ or $\pi(1) = n$ (otherwise $\pi$ is counted by $U_n, W_n$ or $T_n$ or is the reverse of a permutation counted by $U_n, W_n,T_n$ ). So $\pi(i)=n$ for some $2\leq i\leq n-1$ . By the above observation, either $\pi(i-1) = n-1$ or $\pi(i-1) = n-2$ . If it's the former then $\pi$ is counted by $S_n$ or is the reverse of a permutation counted by $V_n$ and if it's the latter then it's either counted by $V_n$ or is the reverse of a permutation counted by $S_n$ . This covers all possibilities, so we are done the proof of this claim. So to summarize, this question asks to justify/prove the recurrences for $K_n$ (just the first one if the proof above is right; permutations in the sets of permutations counted by $U_n, V_n, W_n, S_n, T_n$ are in bijection with their reverses), the recurrence for $A_n$ and the recurrences for $U_n, W_n, V_n, S_n, T_n$ . As an example to clarify the sets $U_n, V_n, W_n, T_n, S_n$ for $n=3$ , we have $U_n = \{(1,2,3)\}, V_n=\{(1,3,2)\}, W_n=\{(2,1,3)\}, T_n=\emptyset, S_n=\emptyset$ .","['elementary-set-theory', 'combinatorics', 'recurrence-relations', 'permutations']"
4321448,Show inequality using the Jensen inequality,"X is a non-negative random variable, with $\mathbb{E}(X) < \infty $ . My goal is to show this inequality: $$\sqrt{1+(\mathbb{E}(X))^2} \leq \mathbb{E}(\sqrt{1+X^2})$$ x² is a convex function, so with the Jensen inequality I get that: $$\sqrt{1+(\mathbb{E}(X))^2} \leq \sqrt{1+\mathbb{E}(X^2)} = \sqrt{\mathbb{E}(1+X^2)}$$ But when I use the Jensen inequality a second time, for the concave $\sqrt{ }$ function, I get that: $$\sqrt{1+(\mathbb{E}(X))^2} \leq \sqrt{1+\mathbb{E}(X^2)} = \sqrt{\mathbb{E}(1+X^2)}  \geq \mathbb{E}(\sqrt{1+X^2})   $$ where the inequality is in the wrong direction. Did I make a mistake? Or is more than the Jensen inequality needed to show this?","['expected-value', 'statistics', 'jensen-inequality', 'inequality']"
4321474,Least Square Estimation of Linear Regression,"I have a linear regression model: $$y = \beta_0 + \beta_1x + \epsilon$$ with $\epsilon$ as a random noise. The least squares estimation is given by: $$S(\beta_0, \beta_1) = \sum^n_{i=1} (y_i - \beta_0 - \beta_1x_i)^2$$ By setting partial derivatives with respect to $\beta_0$ and $\beta_1$ , I obtained: $$\hat \beta_1 = \frac{\sum x_iy_i - \frac{\sum x_i \sum y_i}{n}}{\sum x_i^2 - \frac{(\sum x_i)^2}{n}} (1)$$ The final result for $\hat \beta_1$ is: $$\hat \beta_1 = \frac{\sum(x_i - \bar x)(y_i - \bar y)}{\sum (x_i - \bar x)^2} (2)$$ with $\bar x = \frac{\sum x_i}{n}$ and $\bar y = \frac{\sum y_i}{n}$ I am stuck at getting from (1) to (2), which is the final result. How should I proceed for this case?","['linear-regression', 'statistics']"
4321531,Proof of weak maximum principle for heat-type equations,"Here is the statement of the scalar weak maximum principle on Riemanninan manifolds with boundary. Theorem (Scalar weak maximum principle): Let $M$ be a compact manifold with boundary and $g_t$ a smooth family of Riemannian metrics on $M$ for $t\in [0,T]$ . Suppose that for $u\in C^2(M\times[0,T])$ and $U\in C^1([0,T])$ we have \begin{align}
        \partial_t u &\leq \Delta_{g_t} u + f(u,t)\,, && \text{on } M\times[0,T]\tag{1}\\
        u &\leq U &&\text{on }(M\times\{0\})\cup (\partial M\times[0,T])\tag{2}\\
        \partial_t U(t) &\geq f(U(t),t) && t\in [0,T]\tag{3}
\end{align} where $f:\mathbb{R}\times[0,T]\longrightarrow \mathbb{R}$ is Lipschitz in the first variable and continuous in the second. Then, $u \leq U$ on $M\times[0,T]$ . The standard proof can be found in the book The Ricci Flow: An Introduction by Chow and Knopf. Here is an alternative proof I found in lecture notes for a course by Richard Bamler (found here ). His notes don't go into details, so I have tried filling them out myself. proof: First assume that we have strict inequalities in (2) and (3). Choose $t^*\in [0,T]$ maximal so that $u(\cdot, t) \leq U(t)$ for all $t\in [0, t^*]$ . Since (2) is assumed to be a strict inequality, we have that $t^*>0$ . Suppose that $t^*<T$ . Then there exists $x^*\in M\setminus \partial M$ such that $u(x^*,t^*) = U(t^*)$ . Since $x^*$ is a local maximum of $u(\cdot, t^*)$ , we have $\nabla u(x^*,t^*) = 0$ and $\Delta u(x^*,t^*)\leq 0$ . Moreover $\partial_t u(x^*,t^*)\geq \partial_tU(t^*)$ . Then combining (1) - (3) and evaluating at $(x^*,t^*)$ gives $$
    \partial_t U(t^*) \leq \Delta u(x^*,t^*) + f(u(x^*,t^*),t^*) \leq f(U(t^*),t^*) < \partial_tU(t^*)
$$ which is a contradiction. Define $U_\epsilon(t) = U(t) + \epsilon t + \epsilon^2$ . Then $u(x, t) < U_\epsilon(t)$ for $(x,t)\in \partial_{\text{par}}(M\times[0,T])$ . Since $f$ is Lipschitz, we have for some $C>0$ \begin{align*}
    f(U_\epsilon(t), t) &\leq f(U(t),t) + C(\epsilon t + \epsilon^2)\\
    &\leq \partial_tU(t) + C(\epsilon t+\epsilon^2)\\
    &= \partial_t U_\epsilon(t) + C(\epsilon t+\epsilon^2) -\epsilon 
\end{align*} For sufficiently small $\epsilon$ choose $\tau\in (0,T]$ independent of $\epsilon$ such that $\tau < 1/C-\epsilon $ . Then whenever $t\leq\tau$ we have $$
    C(\epsilon t+\epsilon^2)-\epsilon \leq C(\epsilon \tau+\epsilon^2)-\epsilon < C(\epsilon (1/C-\epsilon)+\epsilon^2)-\epsilon = 0\,.
$$ Thus, for $t\in [0,\tau]$ we have $f(U_\epsilon(t),t)<\partial_tU_\epsilon(t)$ . By the previous case we therefore have that $u \leq U_\epsilon$ on $M\times [0,\tau]$ . Letting $\epsilon \rightarrow 0$ , we have $u \leq U$ on $M\times [0,\tau]$ . Clearly the subset of $u-U\leq 0$ is closed in $M\times [0,T]$ , thus we may assume $\tau$ is chosen maximally. If $\tau < T$ then we can repeat the above procedure with the time shift $t\mapsto t-\tau$ and $T\mapsto T-\tau$ . Thus $\tau = T$ . Question: Is this proof correct? My issue is the inequality $\tau < 1/C$ . This seems to put a constraint on the maximum time $T$ . For example, if $1/C<T$ , then it wouldn't be possible to have $\tau = T$ .","['riemannian-geometry', 'solution-verification', 'maximum-principle', 'partial-differential-equations', 'differential-geometry']"
4321547,Explicit series expansion for inverse of $e^{-x}\left(\frac{x^2}2+x+1\right)$,"Intro: Remember that the W-Lambert function has the following series expansion: $$xe^x=y\implies x=\text W(y)=\sum_{n=1}^\infty\frac{(-n)^{n-1}y^n}{n!},|y|<\frac1e$$ so how about a series expansion for inverse of $e^{-x}\left(\frac{x^2}2+x+1\right)$ ? This question is inspired from: How close are we to solving for a closed form of $$\sum\limits_{n=0}^\infty \frac{(pn+q)^{rn+s}Γ(An+B,Cn+D)}{Γ(an+b)}$$ with the Incomplete Gamma function? where one idea was to notice how $\text W(xe^x)=x$ is the definition. This definition reminds us of the Regularized Incomplete Gamma function and Incomplete Gamma function defined as $$Q(a,z)=\frac{\Gamma(a,z)}{\Gamma(a)}=\frac{\int_z^\infty t^{a-1}e^{-t}dt }{\Gamma(z)}$$ an inverse function for the incomplete argument, z, has been defined known as the Inverse of the Regularized Gamma function with this Taylor Series at $z=1$ : $$Q(a,Q^{-1}(a,z))=z, Q^{-1}(a,z)=((1-z)\Gamma(a+1))^\frac1a+\frac{((1-z)\Gamma(a+1))^\frac2a}{a+1}+\frac{(3a+5)((1-z)\Gamma(a+1))^\frac3a}{2(a+1)^2(a+2)}+…$$ where the series expansion seems to have a large radius of convergence. Here are some special cases: $$Q(1,z)=e^{-z}\implies Q^{-1}(1,z)=-\ln(z)\\Q(2,z)=e^{-z}(z+1)\implies Q^{-1}(2,z)=-\text W_{-1}\left(-\frac ze\right)-1\\Q(3,z)=e^{-z}\left(\frac{z^2}2+z+1\right)\implies Q^{-1}(3,z)=((1-z)\Gamma(3+1))^\frac13+\frac{((1-z)\Gamma(3+1))^\frac23}{3+1}+\frac{(3\cdot3+5)((1-z)\Gamma(3+1))^\frac33}{2(3+1)^2(3+2)}+…=(-6)^\frac13(z-1)^\frac13+\frac{(-6)^\frac23}4 (z-1)^\frac23-\frac{21}{40}(z-1)-\frac{461 (-6)^\frac13}{480}(z-1)^\frac43-\frac{3049 (-6)^\frac23(z-1)^\frac53}{33600}+\frac{11013(z-1)^2}{44800}+…=\sum_{n=1}^\infty a_n (-6)^\frac n3(z-1)^\frac n3$$ this shows that we have a generalization of the W-Lambert function . Maybe this function has an interesting series expansion? The function appears in finding the Median of a Gamma Distribution . Here is an attempt at using the Lagrange Inversion theorem with a series expansion around $x=1$ since around $x=0$ would have $f’(0)=0$ which does not work with the formula. Any other value inside $0\le x\le 1$ may make the formula more complicated and any outside this interval would not be in the defined domain of the Inverse Regularized Incomplete Gamma function. The domain comes from it’s statistics application: $$y=f(x)=e^{-x}\left(\frac{x^2}2+x+1\right)=Q(3,x) \implies f^{-1}(y)=Q^{-1}(3,y)=x=1+\sum_{n=1}^\infty \frac{d^{n-1}}{dy^{n-1}}\left(\left(\frac{y-1}{e^{-y}\left(\frac{y^2}2+y+1\right)-\frac5{2e}}\right)^n\right)\frac{\left(x-\frac5{2e}\right)^n}{n!}=1-2e\left(x-\frac5{2e}\right)-\frac{4e^2}{2!}\left(x-\frac5{2e}\right)^2-\frac{32e^3}{3!} \left(x-\frac5{2e}\right)^3-\frac{384e^4}{4!}\left(x-\frac5{2e}\right)^4= 1-2e\left(x-\frac5{2e}\right)-2e^2\left(x-\frac5{2e}\right)^2-\frac{16e^3}{3} \left(x-\frac5{2e}\right)^3-16e^4\left(x-\frac5{2e}\right)^4 -…=\sum_{n=0}^\infty a_n e^n \left(x-\frac5{2e}\right)^n= \sum_{n=0}^\infty a_n \left(e x -\frac52\right)^n $$ with this graphical demo . Is there an explicit series or closed form of $$a_n=\left\{1,2,2,\frac{16}3,16,…\right\}=\frac{\frac{d^{n-1}}{dy^{n-1}}\left(\left(\frac{y-1}{e^{-y}\left(\frac{y^2}2+y+1\right)-\frac5{2e}}\right)^n\right)}{n!}$$ ? Here is a plot of our goal goal inverse function: $$Q^{-1}(3,x):$$ Please correct me and give me feedback! Motivation from inspired question: Just as a side note, here is some motivation since the Inverse of the Regularized Incomplete Gamma function is a generalized version of $\text W_{-1}(x)$ . The linked question reminds us of the Product Logarithm/ W-Lambert function : $$\text W(x)=\sum_{n=1}^\infty \frac{(-n)^{n-1}x^n}{n!},|x|<\frac1e$$ Now let’s use this property to derive more general ones. From now on the interval of convergence will be assumed for simplicity: $$\text W^{k-1}(ax)\text W(ax) x^b= \text W^{k-1}(ax) x^b\sum_{n=1}^\infty \frac{(-n)^{n-1}(ax)^n}{n!}$$ This is not very surprising, but integrating gives a familiar series expansion with a closed form with the Incomplete Gamma function and Generalized Exponential Integral : $$\int \text W^k(ax)x^bdx=\int \sum_{n=1}^\infty\frac{(-n)^{n-1}\text W^{n-1}(x)x^{n+k}}{n!}dx= x^b e^{-b \text W(a x)} \text W^k(a x) (-(b + 1)\text W(a x))^{-b - k} ((b + 1) Γ(b + k + 1, -(b + 1) \text W(a x)) - Γ(b + k + 2, -(b + 1)\text W(a x)))\frac1{a (b + 1)^2} +C =C+\sum_{n=1}^\infty\frac{ ((-n)^{n - 2} n x^{k + n}\text W(a x)^{k + 1} e^{-(k + n)\text W(a x)} (-(k + n + 1)\text W(a x))^{-(2 k + n + 1} (Γ(2 k + n + 1, -(k + n + 1) \text W(a x)) - (k + n + 1) Γ(2 k + n, -(k + n + 1) \text W(a x)))}{a n!} $$ or for a simpler case with the Imaginary Error function : $$\int \sqrt{\text W(x)}dx=\frac{\sqrt\pi}{4}\text{erfi}\big(\sqrt{\text W(x)}\big)+x\sqrt{\text W(x)}-\frac{x}{2\sqrt{\text W(x)}}+C=C+\sum_{n=1}^\infty \frac{(-n)^{n - 1} x^n \text W^\frac32(x) \left((n + 1) Γ\left(n + \frac12, -(n + 1) \text W(x)\right) - Γ\left(n + \frac32, -(n + 1) \text W(x)\right)\right)}{(-(n + 1) \text W(x))^{n +\frac32} e^{n \text W(x)} n!}  $$ Be careful not to cancel like terms when $x=0$ . The problem here is that when another function was tried, there was no closed form: $$e^{\text W(x)+x}=e^ {\text W(x)}\sum_{k=0}^\infty \frac{x^k}{k!}\implies \int e^{\text W(x)+x} dx=\int e^ {\text W(x)}\sum_{n=0}^\infty \frac{x^n}{n!}dx=\sum_{k=1}^\infty \frac{(-1)^nx^k ((k + 2) Γ(k + 1, -(k + 2) \text W(x)) - Γ(k + 2, -(k + 2)\text W(x)))}{(k + 2)^{k+2} \text W^k(x) e^{k\text W(x)} k!} =?$$ which has no closed form in terms of any official special functions, but may possibly have a closed form with $Q^{-1}(a,z)$ function because: $$Q^{-1}\left(3,e^{-x}\left(\frac {x^2}2+x+1\right)\right)=x,0\le x\le 1$$ Goal: @skbmoore’s solution is excellent, but is there an explicit expression for the coefficients $$a_n=\left\{1,2,2,\frac{16}3,16,…\right\}=\frac{\frac{d^{n-1}}{dy^{n-1}}\left(\left(\frac{y-1}{e^{-y}\left(\frac{y^2}2+y+1\right)-\frac5{2e}}\right)^n\right)}{n!}$$ Also see this related post where the solution includes this function: The positive root of the transcendental equation $$\ln x-\sqrt{x-1}+1=0$$","['special-functions', 'inverse-function', 'lagrange-inversion', 'sequences-and-series', 'recreational-mathematics']"
4321639,Number of triangles $\Delta ABC$ with $\angle{ACB} = 30^o$ and $AC=9\sqrt{3}$ and $AB=9$?,"I came across the following question just now, A triangle $\Delta ABC$ is drawn such that $\angle{ACB} = 30^o$ and side length $AC$ = $9*\sqrt{3}$ If side length $AB = 9$ , how many possible triangles can $ABC$ exist as? Here is a diagram for reference: Here is what I did: I used the Law of Sines to find angle $\angle ABC$ $\to \frac{9}{\sin(30^o)} = \frac{9*\sqrt{3}}{\sin(\angle ABC)}$ $$\to \angle ABC = 60^o$$ So, therefore, $\Delta ABC$ can only exist as a $1$ triangle with angles: $30^o, 60^o$ and $90^o$ . But the answer says $2$ triangles are possible. So my question is: what is the second possible triangle?","['geometry', 'triangles', 'plane-geometry', 'trigonometry', 'algebra-precalculus']"
4321675,Deriving Finsler geodesic equations from the energy functional,"I'm struggling to derive the Finsler geodesic equations. The books I know either skip the computation or use the length functional directly. I want to use the energy. Let $(M,F)$ be a Finsler manifold and consider the energy functional $$E[\gamma] = \frac{1}{2}\int_I F^2_{\gamma(t)}(\dot{\gamma}(t))\,{\rm d}t\tag{1}$$ evaluated along a (regular) curve $\gamma\colon I \to M$ . We use tangent coordinates $(x^1,\ldots,x^n,v^1,\ldots, v^n)$ on $TM$ and write $g_{ij}(x,v)$ for the components of the fundamental tensor of $(M,F)$ . We may take for granted (using Einstein's convention) that $$F^2_x(v) = g_{ij}(x,v)v^iv^j, \quad \frac{1}{2}\frac{\partial F^2}{\partial v^i}(x,v) = g_{ij}(x,v)v^j, \quad\frac{\partial g_{ij}}{\partial v^k}(x,v)v^k = 0.\tag{2} $$ Setting $L(x,v) = (1/2) F_x^2(v)$ , and writing $(\gamma(t),\dot{\gamma}(t)) \sim (x(t),v(t))$ , the Euler-Lagrange equations are $$0 = \frac{{\rm d}}{{\rm d}t}\left(\frac{\partial L}{\partial v^k}(x(t),v(t))\right) -\frac{\partial L}{\partial x^k}(x(t),v(t)),\quad k=1,\ldots, n=\dim(M).\tag{3}$$ It's easy to see (omitting application points) that $$\frac{\partial L}{\partial x^k} = \frac{1}{2}\frac{\partial g_{ij}}{\partial x^k}\dot{x}^i\dot{x}^j\quad\mbox{and}\quad \frac{\partial L}{\partial v^k} = g_{ik}\dot{x}^i,\tag{4}$$ so $$\frac{\rm d}{{\rm d}t}\left(\frac{\partial L}{\partial v^k}\right) = \frac{\partial g_{ik}}{\partial x^j}\dot{x}^j\dot{x}^i +{\color{red}{ \frac{\partial g_{ik}}{\partial v^j} \ddot{x}^j\dot{x}^i }}+ g_{ik}\ddot{x}^i\tag{5}$$ Problem: I cannot see for the life of me how to get rid of these $v^j$ -derivatives indicated in red, even using the last relation in (2), as the indices simply don't match. I am surely missing something obvious. Once we know that this term does vanish, then (4) and (5) combine to give $$ g_{ik}\ddot{x}^i + \left(\frac{\partial g_{ik}}{\partial x^j} - \frac{1}{2}\frac{\partial g_{ij}}{\partial x^k}\right)\dot{x}^i\dot{x}^j =0\tag{6}$$ as in the Wikipedia page .","['finsler-geometry', 'multivariable-calculus', 'euler-lagrange-equation', 'differential-geometry']"
4321686,In a naive attempt to calculate the derivative of $x^x$,"Consider the function $f(x)=x^x$ with $x>0$ . One has the following derivatives $$
(x^n)'=nx^{n-1},\quad (a^x)'=a^x\ln a\;.
$$ Neither of these applies to the function $f$ . In other words, it is neither $
f'(x)=xx^{x-1}
$ nor $
f'(x)=x^x\ln x.
$ But if we naively just add them together, we get $$
f'(x)=xx^{x-1}+x^x\ln x\tag{1}
$$ By definition, $f(x)=\exp(x\ln x)$ and thus by the chain rule: $$
f'(x) = f(x)\cdot (x\ln x)' = x^x (\ln x+x\cdot \frac{1}{x})=x^x(\ln x+1)\tag{2}
$$ One can see that (1) and (2) are the SAME. Is this a coincidence? Is there any underlying math to reveal the coincidence?","['calculus', 'derivatives']"
4321724,"$\forall n,\lim_{x\to 0}\frac{f(x)}{x^n}=0\Leftrightarrow\forall n,\lim_{x\to 0}\frac{f'(x)}{x^n}=0$?","Suppose $f$ is continuous function satisfying $f(0)=0$ and $f$ is differentiable at anywhere except $0$ . $(\Rightarrow)$ Assume that $$
\lim_{x\to 0}\frac{f'(x)}{x^n}=0
$$ for all $n\in\mathbb{N}$ . Then by L'Hospital's rule, $$
\lim_{x\to 0}\frac{f(x)}{x^n}=\lim_{x\to 0}\frac{f'(x)}{nx^{n-1}}=0
$$ for all $n\in\mathbb{N}$ because $\lim_{x\to 0}\frac{f(x)}{x}=\lim_{x\to0}f'(x)=0$ too. $(\Leftarrow)$ I'm tried to show this, but I struck by showing existence of $\lim_{x\to 0}\frac{f'(x)}{x^n}$ . If $\lim_{x\to 0}\frac{f'(x)}{x^n}$ exists, then obviously $\lim_{x\to 0}\frac{f'(x)}{x^n}=0$ . Can we show this? Any ideas will be appreciated.","['limits', 'calculus', 'derivatives', 'real-analysis']"
4321737,How do I evaluate $\sum_{n=1}^{\infty}\frac{(-1)^{n+1}H_n}{2n+1}?$,"So, I am coming from this question. I managed to bring the answer to this series: $$\sum_{n=1}^{\infty}\frac{(-1)^{n+1}H_n}{2n+1}$$ where $H_n$ is the $n$ th harmonic number. However, as seen in the comments, the integral is $\frac{\pi\ln(2)}{2}-C$ where $C$ is Catalan's constant. Certainly, the series looks somewhat like the series for Catalan's constant, but I'm not sure how to deal with the harmonic numbers. The series does seem to be numerically correct, as it slowly converges to the desired value. Any help is appreciated.","['harmonic-numbers', 'calculus', 'taylor-expansion', 'sequences-and-series', 'catalans-constant']"
4321739,How do I show that $\frac{M_n-E(M_n\mid F_{n-1})}{\xi_n-E(\xi_n\mid F_{n-1})}$ is predictable?,"Show that $\frac{M_n-E(M_n\mid F_{n-1})}{\xi_n-E(\xi_n\mid F_{n-1})}$ is predictable. I want to know if this is true, if $M_n$ is martingale adapted to $F_n=\sigma({\xi_1,\dots,\xi_n})$ , and $\xi_i$ is iid. It seems that this is related to the theorem ""predictable representation property.""
This is essentially the converse of martingale transform. I have been studying this for four hours to no avail. Could someone give me any insight? Even if it's not the answer, I would greatly appreciate any suggestions. I will post the original question if needed. Thank you.",['probability-theory']
