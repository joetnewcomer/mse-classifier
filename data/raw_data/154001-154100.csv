question_id,title,body,tags
2594908,Usage of mean value theorem ; bounded derivative and open interval,"Let $f : (0,1) \to \mathbb{R}$ be a function such that $ |f'(x)| \leq 5 $ on the open interval $(0,1)$. Prove that $\lim_{x \to 1^-} f(x)$ exists. It involves the derivative and the actual function itself, so I think I have to somehow use the mean value theorem.. Also, $f$ is continuous on $(0,1)$ and differentiable on $(0,1)$ ( because the derivative exists there ). But then, the function is defined on the open interval, so the requirements for the mean value theorem aren't satisfied. I'm guessing we have to consider intervals of the form $(a,b)$ with $a > 0$ and $b < 0$. Lastly, I don't see the significance of the $5$ ... Is it only there to establish that the derivative is bounded, or does the number itself have some signifiance ( would the same thing hold if we had $3$ for example? ). Please give me a hint, not the solution. Something like ""consider the mean value theorem on intervals of the form ... "" would be very helpful.","['derivatives', 'real-analysis', 'calculus']"
2594914,Finding extrema of general function of n variables without constrained optimization,"In our calculus textbook we are given the following exercise: Given function $f(x_1, x_2, \dots, x_n) = \sum_{i = 1}^{n}\sin(x_i) +
 \sin\left(\sum_{i=1}^{n}x_i\right)$ with $x_i \in (0, \pi)$ and
  $\sum_{i = 1}^{n}x_i \in (0, \pi)$ find it's extrema points and determine their type using only gradient and Hessian matrix. Taking partial derivatives for two arbitrary variables $x_i$ and $x_j$ we get
$$\partial_{i}f = \cos(x_i) + \cos\left(\sum_{i = 1}^{n} x_i\right) = 0,$$
$$\partial_{j}f = \cos(x_j) + \cos\left(\sum_{i = 1}^{n} x_i\right) = 0.$$
Subtracting them we get that
$$\cos(x_i) = \cos(x_j),$$
and so all minima lie at point where all values will be the same. Now trying to find points of interest gives me the condition
$$\cos(x_i) = \cos(nx_i).$$ From that point and on, I'm stuck. How can I proceed there?","['multivariable-calculus', 'maxima-minima', 'optimization', 'derivatives']"
2594923,Can the conjugate $\phi^{-1} \circ A \circ \phi$ be linear for nonlinear $\phi$?,"More specifically, let $f：ℂ^n\to ℂ^n, f(z)=Az$ be linear and $ϕ：ℂ^n → ℂ^n$ entire and bijective, i.e. $ϕ∈\text{Aut}(ℂ^n)$ . Is it possible that $ϕ^{-1}∘f∘ϕ$ is linear again when $ϕ$ is non-linear? For example if $ϕ(z) = Lz+c$ is affine, then $$ ϕ^{-1}∘f∘ϕ (z) = L^{-1}ALz + L^{-1}(A-I)c $$ is linear if and only if $c \in \ker(A-I)$ . Are there more sophisticated examples where $\phi$ is truly non-linear, in the sense of having non-constant derivative? Question: Does there exist a triple $(A, B, ϕ)$ , such that $A≠B$ are complex $n×n$ matrices. $ϕ∈\text{Aut}(ℂ^n)$ with $ϕ'$ non-constant $ ϕ^{-1} ∘ A ∘ ϕ=B $ $A∘ϕ$ is nonlinear (this avoids certain trivial examples) In particular, is there an example where $A$ and $B$ are non-similar? EDIT 2022: Non-similarity is impossible as per Dap's comment ✔ I added the 4th condition to rule out certain trivial examples (see comments in Qiaochu Yuan's answer) Dap provided an example in the comments with $A=B$ Note that if $(A, B, ϕ)$ is a valid triple, with $B=S^{-1}AS$ , then we can actually non-linearly conjugate $A$ to any matrix $C=T^{-1}AT$ similar to $A$ via $ψ = ϕ∘S^{-1}∘T$ . So we immediately get an example with $A≠B$ out of Dap's contraction. An interesting follow-up question would be: Can we construct examples for generic $A$ ? So far we only have an example where $A$ is similar to a diagonal matrix and all eigenvalues have magnitude $1$ .","['complex-analysis', 'linear-algebra']"
2594928,Show that $e^{1-n} \leq \frac {n!}{n^n}$,How can I show that for a $n \in \mathbb N$ $$e^{1-n} \leq \frac {n!}{n^n}$$ I tried using the binomial theorem like this $$n^n \le (1+n)^n = \sum_{k=0}^n \binom nk n^k \le \sum_{k=0}^\infty \binom nk n^k = \sum_{k=0}^\infty \frac{n!}{k!(n-k)!} n^k \le \sum_{k=0}^\infty \frac{n!}{k!} n^k = n! \sum_{k=0}^\infty \frac{n^k}{k!} = n! \cdot e^n$$ which would give me $$\frac{1}{e^n} \le \frac{n!}{n^n}$$ But I'm missing the factor of $e$ on the left side. Can you give me a hint?,"['real-analysis', 'inequality', 'binomial-theorem']"
2594975,Fourier series formula proof,"Is there any proof for this formula? $$f(x)=a_{0}+\sum_{n=1}^{\infty} \Big[a_{n}\cos\Big(\frac{n \pi x}{L}\Big)+b_{n}\sin\Big(\frac{n\pi x}{L}\Big)\Big]$$
It seems no matter how hard I look, this is just a given in any papers on the Fourier series. Can anyone direct me to a proof of this formula or show how it is derived please. Thank you.","['fourier-series', 'fourier-analysis', 'partial-differential-equations', 'ordinary-differential-equations', 'fourier-transform']"
2594983,Choose 3 vectors of length $\geq 1$ so that sums of each two have length $< 1$,"Is it possible to choose three vectors $v_1, v_2, v_3\in\mathbb{R}^2,\vert v_i\vert\geq1$, so that
$$\vert v_1+v_2\vert<1$$
$$\vert v_2+v_3\vert<1$$
$$\vert v_3+v_1\vert<1$$ where $\vert v\vert$ is the euclidean norm of $v$? I could not find a counterexample, but I also did not manage to prove it.
Since each $v_i$ only has two coordinates, I tried to prove it by distinction of all possible combinations of negative and positive coordinates for each vector, but this did not work out (and it would be a very clumsy proof if it worked).
I also tried a similar approach using polar coordinates, which also did not work. Do you have an counterexample or an approach for a proof?","['vectors', 'geometry']"
2594990,"Determine bijective conformal self maps of $\Bbb C \setminus \{0,1\}$","I'm asked how to determine the bijective conformal self maps of $\Bbb C \setminus \{0,1\}$. My attempt is to use the fact that Moebius transformations are uniquely determined by where I send $3$ points (where I'm allowed to pick $\infty$) and are invertible and conformal. Hence by seeing $\{0,1\}$ to $\{0,1\}$ and $\infty$ to $\infty$ I have a family of conformal bijective self maps of $\Bbb C \setminus \{0,1\}$. How to prove that they exhaust all the possible self maps of $\Bbb C \setminus \{0,1\}$?",['complex-analysis']
2594991,A possible converse to the Cayley-Hamilton theorem?,"Happy new year MSE! During my holiday vacation I had an interesting idea! The Cayley-Hamilton theorem states that if $f:\mathbb C^n\to\mathbb C^n$ is a linear function, then it is a root of its own characteristic polynomial $\chi_f(f) = 0$. So I wondered: what if we had a function $f:\mathbb C^n \to \mathbb C^n$ satisfying a polynomial functionial equation (PFE), i.e. there is some polynomial $p=\sum a_k x^k$ such that $p(f)=0$, where we interpret $$ f^k  = \underbrace{f\circ\ldots\circ f}_{k \text{ -times}}$$ and $f^0 = \text{id}$. (replace product of variables with composition of functions) E.g. if $p=x^2+ax+b$ then $p(f)=0$ iff $f(f(x)) + af(x) +bx=0$ for all $x$. This is motivated by the fact that after all matrix multiplication is nothing but the composition of linear functions. Quesiton: Under which conditions would we be able to conclude that $f$ must be a linear function? Here a few things are important to keep in mind If $f$ solves the PFE $p(f)=0$, then so does $\phi^{-1} \circ f\circ \phi$ for any bijective function $\phi$. The way we write down $p$ matters, e.g. although $x(x-1) = x^2 -x$, the resulting PFE $f(f(x)-x) = 0$ and $f(f(x)) - f(x)= 0$ are  different. (This raises an interesting side question about under which conditions their solutions must coincide) General solutions to functional equations can be messy if no additional regularity assumptions are made (cf. Cauchy's equation ). With these caveats in mind I would question the validity of the following Conjecture: Let $f\colon\mathbb C^n\to\mathbb C^n$ be an entire function satisfying a PFE $p(f) =  0$. Then $f$ is conjugate linear, i.e. there exists a holomorphic bijection $\phi\in\text{Aut}(\mathbb C^n)$ such that $\phi^{-1}\circ f\circ\phi$ is linear. I did some digging in the literature and found this wonderful paper by Ahern and Rudin . They consider holomorphic $f$ that are functional roots of unity $f^m =\text{id}$ (also known as Babbage's equation), which is equivalent to the PFE given by $p=x^m-1$. Among other things they prove: If $f^m = \text{id}$ and $f$ is affine, i.e. $f(z)= Lz+c$, then $f$ is conjugate linear. If $f^m = \text{id}$ and $f$ has a fixed point, then $f$ is conjugate linear locally around it. If $f^m = \text{id}$ and $f$ is $\mathbb C^2\to\mathbb C^2$ and a finite composition of overshears , then $f$ is conjugate linear. Here and overshear is a map of the form $$\begin{pmatrix}x\\ y\end{pmatrix}
\longrightarrow
\begin{pmatrix}g(y)x+h(y)\\ y\end{pmatrix}$$ with $g,h$ entire and $g(y)\neq 0$ for all $y$; or more generally $f(x_i) = x_i$ for $i\neq j$ and $f(x_j) = g(x) x_j + h(x)$ where $g,h$ are entire, independent of $x_j$ and $g(x)\neq 0$ for all $x$. It is known that the set of all finite compositions of overshears forms a dense subgroup of $\text{Aut}(\mathbb C^n)$. There are also some known negative results of non-linearizable holomorphic automorphisms (e.g. Derksen 1997 ) but I don't understand enough of the advanced algebra to really fathom this paper and its possible implications on the question at hand. There are some simpler sub-problems that might be easier to track: Problem 1: Let $f(z)=Lz+c$ be affine and satisfy a PFE $p(f)=0$. Does $f$ admit a fixed point? In this case $f$ is conjugate linear by choosing $\phi$ to be the translation onto the fixed point. If false, this might be the easiest route towards a counter example. If true the next logical step should be to try Problem 2: If $f:\mathbb C^n \to \mathbb C^n$ is entire and solves the PFE $p(f)=0$, then $f$ admits a fixed point. Finally, a neat little observation I made is the following: if $f$ solves the PFE $p(f)=0$, and there exists a non-zero vector $v$ and entire function $g$ such that $f(\lambda v) = g(\lambda) v$ for all $\lambda \in \mathbb C$, then $f^k(\lambda v) = g^k(\lambda) v$, hence $g$ is a scalar function solution to the PFE $p(g)=0$. Maybe this indicates that some sort of Eigenvalue theory is possible? Anyway, it seems like some of this stuff is still untapped terrain so it might be worth some further investigation. Thanks for reading!","['complex-analysis', 'linear-algebra', 'functional-equations']"
2594993,Show that every normal subgroup can be written as an intersection of $\ker \chi$'s for $\chi \in \operatorname{Irr}(G).$,"The question is as follows: This can be the third part of this question Linked to the results : Let $G$ be a finite group and $N$ a normal subgroup of $G$. c) Show that $N$ is the intersection of the sets of the form $\ker \xi$ that contain $N$ with $\xi \in \operatorname{Irr}(G)$. Some attempts: For the finite group $G$ and $\rho$ a representation with induced character $\chi_{\rho}$, we have $\ker \rho = \{ g \in G: \chi_{\rho}(g) = \chi_{\rho}(e) \} $ by Lemma 15.17; Isaacs ""Algebra: A Graduate Course"". Then it makes sense to define the kernel of the character $\chi$, denoted $\ker \chi$ by $\ker \chi = \{ g \in G: \chi(g) = \chi(e) \}$. In particular, we know that for every character $\chi$ one has that $\ker \chi \unlhd G.$ For the irreducible characters $\chi^{(\alpha)}$, $\alpha \in \hat{G}$, we give the special symbols $N^{(\alpha)}$ for $\ker \chi^{(\alpha)}$. Now what we are going to show is that knowing $N^{(\alpha)}$ for every $\alpha \in \hat{G}$ enables one to know $\ker \chi$ for every character $\chi$. Indeed, if we let $\chi$ be a character with representation as a linear combination of the irreducible characters $\chi = \sum_{\alpha \in \hat{G}} m^{(\alpha)}\chi^{(\alpha)}$, then we have 
$$\ker \chi =\bigcap \{ N^{(\alpha)} :  m^{(\alpha)} > 0 \}.$$
Because if $\chi^{(\alpha)} (g) = d_{\alpha}$ for every $\alpha$ such that $m^{(\alpha)} > 0$ one sees that 
$$\chi (g) = \sum_{\substack{\alpha \in \hat{G}\\ m^{(\alpha)} > 0 }} m^{(\alpha)} \chi^{(\alpha)} (g) =  \sum_{\substack{\alpha \in \hat{G}\\ m^{(\alpha)} > 0 }} m^{(\alpha)} \chi^{(\alpha)} (e) = \chi (e) $$
and so $g \in \ker \chi$. Conversely, since one evidently has that $|\chi^{(\alpha)} (g)| \le d_{\alpha}$ for every $\alpha \in \hat{G}$ we see that for $g \in \ker \chi$ one has that 
\begin{align} 
|\chi (g)|
&= \left| \sum_{\substack{\alpha \in \hat{G}\\ m^{(\alpha)} > 0 }} m^{(\alpha)} \chi^{(\alpha)} (g) \right|\\
&\le \sum_{\substack{\alpha \in \hat{G}\\ m^{(\alpha)} > 0 }} m^{(\alpha)} \left| \chi^{(\alpha)} (g) \right| \\& \le  \sum_{\substack{\alpha \in \hat{G}\\ m^{(\alpha)} > 0 }} m^{(\alpha)} d_{\alpha} \\
&= \chi(1) 
= \chi(g) 
\end{align}
from where it follows from this that $\chi^{(\alpha)} (g)$ must be real and so if $\chi^{(\alpha)} (g) \le d_{\alpha}$ for any $\alpha \in \hat{G}$ then this would induce a strict inequality for $\chi(g)$  and $\chi(1)$. It follows that $\chi^{(\alpha)} (g) = d_{\alpha}$ for any $\alpha \in \hat{G}$ such that $m^{(\alpha)} >0$. Then the conclusion follows. Can someone please let me know if I am wrong and we cannot get the result through what I wrote? Thanks!","['finite-groups', 'abstract-algebra', 'group-theory']"
2595033,Can unbounded functions be Riemann integrable?,"I need to prove or disprove that the next function is Riemann integrable on $[0,2]$: $$ f(x) = \begin{cases} \dfrac{1}{x} &x > 0 \\ 0 &x = 0 \end{cases} $$ My intuition is that it's not, because $f$ is unbounded in that interval, so $U(f,Pn) = L(f,Pn) = \infty$ So I have two questions: Am I right? Can unbounded functions be Riemann integrable?",['integration']
2595060,What is the expected value of sample mean?,"I have a simple question. 
$X$ is a random variable with mean $μ$, and there is a sample of size $n$: $X_1, X_2, \cdots, X_n$. Then what is the expected value of the sample mean $\overline{X}$? This is what I thought: $$\overline{X} = \frac{1}{n}(X_1+X_2+X_3+\cdots+X_n),$$ thus $\overline{X}$ is a certain value (constant), therefore $E(\overline{X}) =  \overline{X}$. Similarly, $\overline{X}^2$is also a certain value (constant) and $E(\overline{X}^2) = \overline{X}^2$. But it turns out there's something wrong. Could someone please explain it to me? Thanks in advance.",['statistics']
2595073,For which real $x$ is the matrix $ A_{ij} = x^{|i-j|}$ invertible?,"For which real $x$ is the $n\times n$ matrix with $ A_{ij} = x^{|i-j|}$  invertible? Omitting the trivial cases $x=1,0$ I thought since it was real symmetric matrix I could diagonalize and look at its eigenvalues, but it quickly got messy and I couldn't come up with a closed solution","['matrices', 'eigenvalues-eigenvectors', 'linear-algebra', 'inverse']"
2595084,Probability of a Full House for five-card hand.,"So I know this can be solved easily by counting the total ways to make a full house and dividing that by the total possible hands, but I want to know why another way I thought of to solve it is wrong. My calculation is: $$1 \times \frac{3}{51} \times \frac{2}{50} \times \frac{48}{49} \times \frac{3}{48} \times 5!$$ To break this down, the first card can be any. The second card must be the same number as the first ($\frac{3}{51}$) and the third card must also be the same number ($\frac{2}{50}$). The fourth card can be any from the deck with the exception of whatever card makes a 4-of-a-kind ($\frac{48}{49}$). And the fifth card must be the same number as the fourth card ($\frac{3}{48}$). Since order should not matter for a hand of cards, I multiply this probability by $5!$. I can't figure out where I went wrong, but evidently this does not give me the correct answer. Can anyone help me find my error?","['combinatorics', 'poker', 'probability', 'card-games']"
2595102,Show that $2x^5+3x^4+2x+16$ has exactly one real root,"It's clear that this function has a zero in the interval $[-2,-1]$ by the Intermediate Value Theorem. I have graphed this function, and it's easy to see that it only has one real root. But, this function is not injective and I'm having a very hard time proving that it has exactly one real zero. I can't calculate the other 4 complex roots, and my algebra is relatively weak. I have also looked at similar questions, where the solutions use Rolle's Theorem, but I can't seem to apply it to this problem.","['algebra-precalculus', 'polynomials']"
2595109,Prove that T (tree) is a path iff $d(v) = 2$,"Let $T$ be a tree with n vertices, $n \geq 4$, and $v$ is a vertex with maximum degree in $T$.  Prove that $T$ is a path iff $d(v)  = 2$. My attempt:
Logically,  if a vertex has maximum degree more than $2$, then the tree will contain branches (no cycle of course) and that's why it is not a path then. In generalization,  $(v_0, v_1, \cdots,  v_i,  \cdots,  v_n)$ where $v_i$ has more than degree $2$,  then we can't found a single last vertex $v_n$ (branches). Somehow I think that we should use induction.  But I myself can't proceed. So,  do you have any idea?  Regards. 
Sorry for my bad English.","['graph-theory', 'discrete-mathematics']"
2595117,Taking the derivative of the following integral,"I have the following double integral which I want to differentiate one times with respect to $1$. Or simply I want to obtain $h'(1)$. $h(x)=\int_{0}^{x}\int_{0}^{x} f(u,v)du dv$ The following answer has been provided. $\int_{0}^{1}(f(1,t)+f(t,1))dt$ I am not able to understand how this has been obtained. Although, I know how to take derivative of the integral when only one integral is provided. Please help. Thanks in advance.","['derivatives', 'definite-integrals']"
2595121,Eigenvalues of Symmetric Matrix Plus Diagonal Matrix,"Let $M$ be a real symmetric matrix and let $D$ be a real diagonal matrix. Can we say something about the eigenvalues of $M+D$ in terms of the
  eigenvalues of $M$? For example, if $D$ is a constant multiple of $I$ ($D = cI$), then the eigenvalues of $M+D$ is just $c$ plus the eigenvalues of $M$. Can we get bounds on the locations of the eigenvalues of $M+D$ ?","['eigenvalues-eigenvectors', 'numerical-linear-algebra', 'linear-algebra']"
2595128,When is product of right-invertible matrix and left-invertible matrix invertible?,"I have a strong suspicion this is a textbook linear algebra problem, but I have been unsuccessful in finding an answer. Let $A$ be an $n \times m$ matrix and let $B$ be an $m \times n$ matrix where $n < m$.  Suppose than $A$ has rank $n$ ($A$ has a right-inverse) and $B$ also has rank $n$ ($B$ has a left-inverse). When is $AB$ invertible? Here is what I have so far.  Say that $AB$ is invertible and the inverse is $C$.  Then, $\left(CA\right) B = I_{n}$ And, $A\left(B C\right) = I_{n}$. Or: $BC$ must be a right-inverse of $A$ and $CA$ must be a left-inverse of $B$. $CA = B^{-1}_{left} \implies C = B^{-1}_{left} A^{-1}_{right}$ $BC = A^{-1}_{right} \implies C = B^{-1}_{left}A^{-1}_{right}$ So, if $C$ exists, then I know what it is.  If I know there is a $C$ such that $CA$ is a left-inverse of $B$ and $BC$ is a right-inverse of $A$, then the product is invertible.  What I'm looking for are more primitive conditions on the matrices $A$ and $B$ that guarantee such a matrix exists.",['linear-algebra']
2595131,Axiomatic set theory book recommendation,"I was playing around with a theorem in combinatorics about finite sets and was trying to find a meaningful generalization of it to non-finite sets. At one point without thinking I wrote that: $$\text{ The power-set of }X=\mathcal{P}(X)=\bigcup_{n=0}^{\infty}\{S\subseteq X:|S|=n\}$$ Then about five seconds later I realized it was false, because my set $X$ might contain a subset whose cardinality is not a natural number. So I tried to salvage this partially, using my essentially zero knowledge on ordinal numbers and more advanced set theory, basically I wrote something akin to:
 $$\mathcal{P}(X)=\bigcup_{\substack{\gamma\leq |X|\\\gamma\text{ a cardinal number}}}\{S\subseteq X:|S|=\gamma\}$$ However I'm not even sure if this makes sense. To make it formal I would have to reform that lower index to something like ""the set of cardinals"", but I don't even know if that's a set?? I'm starting to feel really stupid, I mean throughout most of my time studying I've never really made use of any sets with cardinality say greater then $|\mathbb{R}|$ maybe $|\mathcal{P}(\mathbb{R})|$ when working with function spaces, though even then I never really made use of it. Also I've tried to play around with stuff like this before, where I see something cool and want to make a generalization for non-finite sets and sometimes I can manage by manipulating things weirdly and finding bijections to establish equinumerosity. But I imagine if I had some more tools up my sleeve like a familiarity with cardinal arithmetic etc. I could probably do this much faster and likely find results I couldn't have got before because I lacked the ability to manipulate sets with arbitrary cardinality in a nice way. So in short can someone recommend me some reading on ""advanced set theory"" (no idea what to call it, just want to make sure its not a book on naive set theory etc. which I'm fine with).","['reference-request', 'book-recommendation', 'elementary-set-theory']"
2595144,A limit involving some binomials,"Let $C_k^i=\frac{k!}{i!(k-i)!}$. Show that $$\lim_{k\to\infty}\frac{C_k^i+C_k^{n+i}+\cdots+C^{([k/n]-1)n+i}_k}{2^k}=\frac{1}{n}$$ for any $1\leq i<n$. Here $i,n$ be positive integers. As is well-known, $\sum_{i=0}^k C_k^i=2^k$. But how to prove the above limit? Choose $C_k^i$ after $n$ blocks.",['limits']
2595183,Quotient Group of $\mathbb{Z}^4$ and a Lattice,"I am working on problem 2 of the Rutgers 2017 Fall Algebra Qualifier where we are tasked with determining the structure of $\mathbb{Z}^4 /S$ where $S$ is the group generated by the vectors $(5,-2,-4,1)$, $(-5,4,4,1)$, $(0,6,0,6)$. So the first thing I noted was that $$(5,-2,-4,1) + (-5,4,4,1) = (0,2,0,2).$$ So it follows the third vector given $(0,6,0,6)$ is in the span of first two, and so the question remains to show: Find $\mathbb{Z}^4 / \lbrace a (5,-2,-4,1)  + b (-5,4,4,1), a, b \in \mathbb{Z} \rbrace$ Now I tried to look for similar problems to this to make sense of it and came across the following: Is $\mathbb{Z}\times\mathbb{Z}/((6,5),(3,4))$ is finitely generated? But I'm not sure how to use the matrix techniques there correctly and rigorously. So I now I'm working with equivalence classes but the ease with which one can declare $$ \mathbb{Z} / k\mathbb{Z} = \mathbb{Z}_k$$ seems to be lost when I move into the 2 basis vector situation.","['abstract-algebra', 'linear-algebra']"
2595359,"If $f$ is integrable, $g$ measurable, and $f = g$ a.e., then $g$ is integrable","The precise statement is if $f$ is integrable, $g$ measurable, and $f = g$ almost everywhere, then $g$ is integrable, and the integrals coincide. I use the following definition of integrable: $f$ is integrable if there is a sequence of (simple) integrable functions $\{f_n\}$ such that $f_n \to f$ almost everywhere, and it is Cauchy in the mean. Then $\lim_n f_n = f = g$ almost everywhere, and it seems that I can use the same sequence for both $f$ and $g$, and then there is hardly anything to do. What am I missing? Thanks in advance.","['simple-functions', 'integration', 'measure-theory']"
2595388,How to determine which sets are open in a topology?,"Given $A_n = \{n, n+1, \dots\} , n \in \mathbb{N} $ and $\tau = \{ \emptyset, A_1, A_2, \dots \} $, I would like to determine which sets are open in $(\mathbb{N}, \tau) $ By definition: $A_n$ is open and any intersection/union of finitely/infinitely many $A_n$ is also open. Furthermore: $A_n$ is infinite, ${A_n}^C$ is finite. However this does not tell or really help me deciding, what are any other open/closed sets regarding this topology? For example: Is set of odd/even numbers open? What about an arbitrary set? etc. It seems I am missing a very basic property of topologies. Therefore how does one approach above problem? What am I missing?","['general-topology', 'elementary-set-theory']"
2595398,Is this a combinatorial identity: $ \sum_{k=1}^{n+1}\binom{n+1}{k} \sum_{i=0}^{k-1}\binom{n}{i} = 2^{2n} $?,"$$
\sum_{k=1}^{n+1}\left(\binom{n+1}{k} \sum_{i=0}^{k-1}\binom{n}{i}\right) = 2^{2n}
$$
This is my first question, please feel free to correct/guide me. While solving a probability problem from a text book l reduced the problem to the above LHS. I couldn't reduce it any further. I tried a few values of n and it holds. I gave a half hearted attempt at induction before I gave up. Does this hold? Is there a combinatorial proof to it(assuming it holds) i.e count something one way and count the same thing other way and then equate them. Is there a name to it? Most importantly how to Google such questions? To provide further context, the problem is as follows:-
Alice and Bob have a total of $2n+1$ fair coins. Bob tosses $n+1$ coins while Alice tosses $n$ coins. Tosses are independent. What is the probability that Bob tossed more heads than Alice? It is from a standard textbook ""Introduction to Probability"" by Dmitri and N John.",['probability']
2595567,Which rings arise as a group ring?,"Let $R$ be an arbitrary associative ring with identity. When does there exist a group $G$ and a field $F$ such that $F[G] = R$? Do we obtain more rings as $F[G]$ if we loosen the condition that $F$ be a field? When can we get $G$ to be a finite group? There seem to be some obvious restrictions: The characteristic of $F$ is equal to that of $R$, for instance. Also, when $F$ and $G$ are finite, then so is $R$. Also, all elements of $G$, considered as a subset of $R$, are units in $R$. If $R$ is commutative, $G$ has to be abelian. But I fail to see much more.","['ring-theory', 'group-theory', 'group-rings']"
2595596,Relation between Riemannian metric and Hermitian structure.,"Let $M$ be a complex manifold with almost complex structure $J$, and $TM$ it's real tangent space, $TM^{\mathbb{C}}=TM \otimes \mathbb{C}$ its complexified tangent space. Now I'm getting confused with the metrics on this space, and so I will try and list what I think is going on and where I'm confused. • A Riemannian metric on $M$ is a real , symmetric, positive definite sesquilinear form
$$g : TM \times TM \to \mathbb{R}$$
such that it is $J$-invariant (i.e. $g(X,Y)=g(JX, JY), \, \forall X,Y \in \Gamma (TM$)). • A Hermitian structure on M is a map
$$h: TM^{\mathbb{C}} \times TM^{\mathbb{C}} \to \mathbb{C}$$
such that it is a $J$-invariant, symmetric (i.e. $h(X,Y)=\overline{h(Y,X)}, \, \forall X, Y \in \Gamma(TM^{\mathbb{C}})$ ), positive definite, sesquilinear form. • Now the texts usually say something like: given a Riemannian metric g, there exists a Hermitian structure
$$h(X,Y) = g(X,Y)-ig(X,JY)$$ But this is only defined on $TM$ and we need h to be defined on $TM^{\mathbb{C}}$? Do we then extend it linearly in the first argument and anti-linearly on the second to define it on all of $TM^{\mathbb{C}}$ and thus defining h as a Hermitian structure? Also, why can't we define $h(X,Y) = g(X,Y)$? I don't see why this would not define a Hermitian structure.","['almost-complex', 'tangent-bundle', 'differential-geometry']"
2595602,Definitions of an additive functor,"In my Rings and Modules class, we defined additive functors this way: A (covariant/contravariant) functor $F$ is called additive if: (i). $F0 = 0$, where $0$ is the zero-module. (ii). For any modules $M, N$ we have $F(M \oplus N) = FM \oplus FN$. I looked around online and I saw this definitions, which looks the same but uses morphisms instead of modules: A (covariant/contravariant) functor $F$ is called additive if: (i). $F0 = 0$, where $0$ is the zero-map. (ii). For any modules $M, N$ and morphisms $\phi, \psi: M \longrightarrow N$ we have $F(\phi + \psi) = F \phi + F \psi : FM \longrightarrow FN$. Now it seems clear, intuitively, how these two concepts are equivalent. However, I always have a lot of trouble proving elementary stuff with functors, because I always think if what I'm doing is allowed. I was trying to prove that the first two conditions imply the other two. So I tried to rephrase them in terms of compositions of homomorphisms. (i). Let $\phi : M \longrightarrow N$ be the zero-map. Then $\phi$ is the same as $\iota \circ \phi_0$, where $\phi_0 : M \longrightarrow 0$ and $\iota : 0 \longrightarrow N$. Then $F \phi = F(\iota \circ \phi_0) = F \iota \circ F \phi_0$ is the zero-map, because we are assuming that $F0 = 0$. (ii). Let $\phi, \psi: M \longrightarrow N$ be homomorphisms. Then $\phi + \psi$ is the same as the following sequence:
$$ M \overset{id_M \oplus id_M}{\longrightarrow} M \oplus M \overset{(\phi, \psi)}{\longrightarrow} N \oplus N \overset{id_N + id_N}{\longrightarrow}N.$$
At this point, however I am stuck. I don't know how to justify that applying $F$ to this sequence gives $F\phi + F\psi$, since natural stuff such as inclusions and projections are not necessarily preserved. How should I go on? I haven't yet tried proving the other direction...","['abstract-algebra', 'additive-categories', 'homological-algebra', 'commutative-algebra', 'functors']"
2595603,Maximal chain with upper bound has at least one element,"Let $\left({E, \preceq}\right)$ be a poset, and let $K \subseteq E$ be a maximal chain in $E$ such that $K$ has an upper bound in $E$.
Then $K$ has at least one element. Proof : $K$ is a maximal chain. That is, for every element
$x \in E \setminus K$ there is a $y \in K$ such that
$(x,y) \notin \, \preceq$ and $(y,x) \notin \, \preceq$. This means that if an $x \in E$ belongs to $E \setminus K$, we surely have an element of $K$ that isn't comparable with this $x$ (and so no element of the chain is comparable with $x$, because of transitivity). Now let $m \in E$ be an upper bound for $K$; we can compare $m$ with any element the chain. But this is the same as saying that $m \notin E \setminus K$ and so $m \in K$. $\blacksquare$ In other words, if $m$ is an upper bound for $K$, it must belongs to $K$ because it has to be comparable with all elements of $K$ itself, and we said that $K$ is maximal (if we are able to compare $m \notin K$ with every element of the chain, it's reasonable to ""expand"" it including $m$, in contrast with the hypothesis that $K$ is maximal). Is it a correct proof?","['relations', 'order-theory', 'elementary-set-theory', 'proof-verification']"
2595701,Ergodicity and Positive Recurrence,"If a stochastic process $\{ X_t \}_{t \in \mathbb{N}}$, that takes values only in $\mathcal{X} = \{0, 1\}$, is ergodic, then what can we say about its recurrence times? If $T_k$ is the $k$th recurrence time for the state $1$, then is $\mathbb{E}[T_k]$ finite? What about $\mathbb{E}[T^{2}_{k}]$, or other moments of $T_{k}$?","['stochastic-processes', 'probability-theory', 'ergodic-theory']"
2595727,Permutation of a sum - generating functions,"How many ways can we combine $1, 2$ and $5$ dollars into a vending machine, in a total sum of $n$ - no need to get a closed form expression, just show the generating function. differentiate between ordered and unordered cases. (Without using exponential generating function) My Attempt - So the unordered case feels kind of obvious - I define $x_1 = 1+x+x^2+..., x_2 = 1+x^2+x^4+...,x_5=1+x^5+x^{10}+...$
, $x_1 + x_2 + x_5 = n$. And the generating function as such - $f(x) = \lambda x\in \Bbb{R}. \dfrac{1}{1-x} \cdot \dfrac{1}{1-x^2} \cdot \dfrac{1}{1-x^5}$. The second case is a little bit more confusing, i thought about something like - $f(x) = \lambda x\in\Bbb{R}.\sum\limits_{i=0}^n(1+x+x^2+x^5)^i$ I really am not sure this is a good generating function for that case, any thoughts ?","['generating-functions', 'discrete-mathematics']"
2595768,A counterintuitive problem in conditional probability/combinatorics,"Please help me to finish/validate my solution for the following problem: Three players $A,B,C$ play a game. A game is played between two
  players. After a game the winner plays against player who did not play
  last time. Everyone has equal winning chance of $\frac12$. To become a
  champion a player have to win 2 games in a row. Find the probabilities
  of being a champion for each player, if the first game is played
  between $A$ and $B$. Ok, my attempt: Let ${PL}_n$ is the probability of an event ""player $PL$ became a champion after game $n$"". Then it is easy to get that $A_1=B_1=C_1 =C_2=0,A_2=B_2=\frac14$, $C_3=\frac14$. With the probability $\frac14$ $4$-th game is played between $A$ and $B$, so the game tree with winning leaves cut off will repeat itself with a period $3$. So $A_{3n}=B_{3n}= {\frac14(\frac14)}^{n-1}={(\frac14)}^{n}$,$C_{3n+1}= {\frac14(\frac14)}^{n-1}={(\frac14)}^{n}$. Then $A_{\infty}=B_{\infty}=$ sum of infinite sequence with first term $\frac14$ and factor $\frac14$, so  $A_{\infty}=B_{\infty}=\frac13$. Since the probability the no one will will win the game after game $3n+1$ is equal to ${(\frac14)}^{n}\to0$ for large $n$, we get $C_{\infty}=1-A_{\infty}-B_{\infty}=\frac13$. I do not see any errors in my solution and I like it and believe it is correct, but the result looks counterintuitive - it looks like players $A$ and $B$ should have better chance of being a champion. Is my solution correct? Thanks a lot for your help!","['combinatorics', 'probability']"
2595778,Find $\sum_{n=1}^{\infty}$ $\frac{n^{2}}{\left(n+1\right)\left(n+2\right)\left(n+3\right)\left(n+4\right)}$,"$$\sum_{n=1}^{\infty}\frac{n^{2}}{\left(n+1\right)\left(n+2\right)\left(n+3\right)\left(n+4\right)}$$ MY Approach $\sum_{n=1}^{\infty}$$\frac{n^{2}}{\left(n+1\right)\left(n+2\right)\left(n+3\right)\left(n+4\right)}$
= Lim$_{k\rightarrow\infty}$$\sum_{k=1}^{n}\frac{k^{2}}{\left(k+1\right)\left(k+2\right)\left(k+3\right)\left(k+4\right)}$ $\frac{k^{2}}{\left(k+1\right)\left(k+2\right)\left(k+3\right)\left(k+4\right)}$=
$\frac{1}{6\left(n+1\right)}$-$\frac{2}{\left(n+2\right)}$+$\frac{9}{\left(n+3\right)2}$-$\frac{8}{\left(n+4\right)3}$ I don't think i can telescope it, And i don't know any other method","['real-analysis', 'limits', 'reference-request', 'convergence-divergence', 'sequences-and-series']"
2595781,Nonsingular matrix problem,"Let $A$, $B$ be $n\times n(n\ge 2)$ nonsingular matrices with real entries. a)If $A^{-1}+B^{-1}=(A+B)^{-1}$,then prove that $\det A=\det B$ b)Find the examples of matrices $A$,$B$ satisfying $A^{-1}+B^{-1}=(A+B)^{-1}$. c)Find the examples of matrices $A,B$ with complex entries such that $A^{-1}+B^{-1}=(A+B)^{-1}$, but $\det A\ne \det B$ I tried the first part I think it may be done by some multiplication right and left side but i failed.And for example i can't derive any example.Is their any process for thinking this type of problem??","['matrices', 'determinant']"
2595821,"Whether $f(x)= x^3 \sin \frac{1}{\sqrt{|x|}}$, $x\neq 0$ and $0$ if $x=0$ is differentiable everywhere.","I think that this function is not differentiable at points where $\frac{1}{\sqrt{|x|}}=n\pi$, $n \in \mathbb{Z}$. But I don't know whether it is correct or not. Of course, the function is differentiable at $x=0$.
$\lim_{x\to 0+} \frac{f(x)-f(0)}{x-0}= \lim_{x\to 0+} x^{2}\sin \frac{1}{x} =0 $
Similarly,  $\lim_{x\to 0-} \frac{f(x)-f(0)}{x-0}=0.$ How to discuss the differentiability of this function at other points.
Thanks in advance.","['derivatives', 'real-analysis', 'calculus']"
2595841,Derivative of $e^{2x^4-x^2-1}$ with limit definition of derivative,"Let $f:\mathbb{R}\to\mathbb{R}$ be defined as
$f(x) = e^{2x^4-x^2-1}$. I have to find the derivative using the defintion:
$$f'(x) = \lim_{h \to 0} \frac{f(x+h)-f(x)}{h}$$ My approach:
$$
\begin{align}
&\lim_{h \to 0} \frac{\exp({2(x+h)^4-(x+h)^2-1})-\exp({2x^4-x^2-1})}{h}\\
&=\lim_{h \to 0} \frac{\exp(2 h^4 + 8 h^3 x + h^2 (12 x^2 - 1) + h (8 x^3 - 2 x) + 2 x^4 - x^2 - 1) - e^{2 x^4 - x^2 - 1}}{h}
\end{align}$$ How can I remove $h $from denominator?","['derivatives', 'analysis']"
2595864,Is there a combinatorial proof to this inequality?,"I verified that this inequality: $$
\sum_{i=0}^{k-1} \sum_{j=0}^{k+1} {3 k-3\choose i} {3 k+3\choose j}
\geq
\sum_{i=0}^{k} \sum_{j=0}^{k} {3 k\choose i} {3 k\choose j}
$$
holds for all $k$ between 1 and 100. It seems that, if this inequality is true, it ""should"" have some combinatorial proof, since in both sides, we have numbers of ways of picking at most $2 k$ items from among $6 k$ items. So, does this inequality have a combinatorial proof? EDIT: some non-combinatorial proofs for this inequality are in: Recent Question in American Math Monthly, proposed by Donald Knuth","['combinatorics', 'inequality']"
2595890,"Prove that for every even $n\ge 4 $, there exists a planar, connected and 3-regular graph with $n$ vertices","I know that, if there exists such a graph, its number of faces $C$ satisfy that $C=n/2 +2$, though I don't know whether it is helpful in this proof or not. I thought of using induction, starting from a graph 3-regular and connected with $n=2\cdot (k+1)$ , then remove two vertices to apply the induction hypothesis, however by doing that the graph wouldn't be 3-regular.","['combinatorics', 'graph-theory', 'planar-graphs', 'discrete-mathematics']"
2595913,How to show that all trajectories of this dynamical system end in a disk?,"Consider the system: $$\begin{alignat}{2}
			x_{1}' &=&~ -ax_2 &+ x_{1}\left(1-x_{1}^2-x_{2}^2\right),\\
			x_{2}' &=&   ax_1 &+ x_{2}\left(1-x_{1}^2-x_{2}^2\right)-b,
	\end{alignat}
$$ where $a,b$ are real numbers. The task is to show that there is a disk which eventually contains every orbit of the system and that there is a limit cycle only if $b$ is zero. Up to this point, I was trying to construct a function $V(x_1,x_2)$ with negative derivative along the solution curves on a disk but it’s not getting anywhere. Maybe a proof by assuming the opposite plus a theoretical argument would do the job, but I would welcome any hints here.","['ordinary-differential-equations', 'dynamical-systems']"
2595950,Calculate probability using the central limit theorem,"A multiple choice test consists of 25 questions with 3 answers for every question. Only one of the three answers is correct for every question.
With which probability can someone, that is only guessing the test, get a positive grade, when more than half of the questions must be answered correctly to pass the test? We have to use the central limit theorem.
As the questions are independant from each other, I think it should be possible to approximate it by: $$ P(a \le S_n \le b) \sim \phi\left( \frac{b+\frac{1}{2}-np}{\sqrt{np(1-p)}}  \right) - \phi\left( \frac{a-\frac{1}{2}-np}{\sqrt{np(1-p)}}  \right).$$ We've $25$ questions, and every answer can be correct with a probability of $ \frac{1}{3}$ as there are $3$ answers given for every question. So I got the values: $n=25$, $p = \frac{1}{3}$, $a=13$, and $b=25$. When I calculate it with these values, I get $\phi(7.28)-\phi(1.77)$ which results in $3.84\%$ and doesn't seem to be correct. So how can I do it the right way?","['normal-distribution', 'independence', 'statistics', 'central-limit-theorem', 'probability']"
2595952,Can $15x^3 + 7x^2 - 8x - 27$ be written as a product of two polynomials with integer coefficients? Explain.,"i tried factoring it like this : $x$ $(15x^2 + 7x - 8) - 27$ $x$ $(15x^2 + 15x - 8x - 8) -27$ $x$ $(15x (x+1) - 8(x+1))-27$ $x$ $((x+1)(15x-8)) - 27$ I don't know if it could be factored any further, and i also don't know how to explain. And i'm sorry i'm learning this on my own so i'm kinda lost, why is the question about integer coefficients? i'm trying to do an exercise about introduction to Direct Proof and Counterexample in a discrete mathematics textbook and this question came right after a question about factoring and parity.","['irreducible-polynomials', 'polynomials', 'factoring', 'cubics', 'discrete-mathematics']"
2595961,"(Poisson limit theorem) Random variable $X_n$ ~ Bin$(n,p_n)$ convergences to $Z$ ~ Poisson($\lambda$)","$(\star)$ : Let $ X_n $ ~ Bin( $n,p_n$ )  ( $ n \in \mathbb{N} $ ) $ n \cdot p_n  \rightarrow \lambda $ for $ n \rightarrow \infty $ . Then $ X_n$ convergences to Poisson $Z$ ~ Poisson( $\lambda$ ). I have to use the following steps : a) First of all solve this exercise for $ n \cdot p_n = \lambda $ $ \forall n \in \mathbb{N}$ . b) Let $[A,B]$ be a bounded interval.  Show $ \forall k \in \mathbb{N}$ $ \forall a,b  \in [A,B]$ : $| a^k - b^k | = | \int_{min(a,b)}^{max(a,b)} k \cdot x^{k-1} dx | \le k \cdot| a-b| \cdot $ max{ |A|,|B| } $^{k-1}$ c) Let $[A,B]$ be a bounded interval. Show there is a $D > 0 $ , such that $\forall a,b \in [A,B]$ $ \forall n \in \mathbb{N} $ : | $(1+\frac{a}{n})^n - (1+\frac{b}{n})^n | \le D|a-b| $ . d) Now proof  ( $\star$ ). Attempt : a) $n \cdot p_n  = \lambda  \Rightarrow  p_n = \frac{\lambda}{n}$ . So: $\lim_{n \rightarrow \infty} P(X_n = k) = \lim_{n \rightarrow \infty} \binom{n}{k} p_n^k(1-p_n)^{n-k} =$ $\lim_{n \rightarrow \infty} \binom{n}{k} (\frac{\lambda}{n})^k(1-\frac{\lambda}{n})^{n-k} = \frac{\lambda^k}{k!} \lim_{n \rightarrow \infty} \frac{n}{n} \frac{n-1}{n} ... \frac{n-k+1}{n}(1-\frac{\lambda}{n})^n \frac{1}{(1-\frac{\lambda}{n})^k} = \frac{\lambda^k}{k!} \cdot 1\cdot e^{-\lambda} \cdot 1 = e^{-\lambda} \frac{\lambda^k}{k!}  $ . b) Without loss of generality: $ a \le b$ . $| \int_{min(a,b)}^{max(a,b)} k \cdot x^{k-1} dx |  = | \int_{a}^{b} k \cdot x^{k-1} dx | = | x^k|_{a}^{b} | = |b^k -a^k| = |a^k - b^k| $ . second part:
for $a = b$ we have $ 0 \le 0 $ and for $ a < b $ we use the mean-value theorem: $\frac{|a^k - b^k|}{|a-b|} = k \cdot |c|^{k-1} \le k \cdot  $ max{|A|,|B|} $^{k-1}$ because $c \in (a,b) \subset [A,B]$ . c) Again the mean-value theorem? $| \frac{(1+\frac{a}{n})^n - (1+\frac{b}{n})^n}{a-b} | = |(1+\frac{c}{n})^{n-1}| $ but why is this $ \le D $ ? I know that $c \in (a,b) $ . d) So now I have to show  that for random variables $X_n$ ~ Bin $(n,p_n)$ with $n \cdot p_n \rightarrow \lambda$ for $ n \rightarrow \infty $ $\Rightarrow $ $X_n $ convergences to $Z$ ~ Poisson( $\lambda$ ). Here is my problem. Unfortunately I don't know how I can use a), b) and c) for this exercise. I hope that your answer can help me. Important remark : It is allowed to use: Let $X_n$ be random variables with values in $\mathbb{N_0}$ . If the sequence of functions $g_n(t) := g_{X_n}(t)$ convergences pointwise to $g(t)$ on a open interval $I$ with $0 \in I $ , then $\exists$ a random variable $X$ with $g(t) = g_X(t) $ and $ \lim_{ n \rightarrow \infty } P (X_n = k) = P(X=k)$ $ \forall k \in \mathbb{N_0}$ .","['limits', 'random-variables', 'binomial-distribution', 'probability', 'poisson-distribution']"
2595966,"How to prove that $\frac{a+b}{1+a+b} \leq \frac{a}{1+a} + \frac{b}{1+b}$ for non-negative $a,b$?","If $a, b$ are non-negative real numbers, prove that
$$
\frac{a+b}{1+a+b} \leq \frac{a}{1+a} + \frac{b}{1+b}
$$ I am trying to prove this result. To that end I added $ab$ to both denominator and numerator as we know
$$
\frac{a+b}{1+a+b} \leq \frac{a+b+ab}{1+a+b+ab}
$$
which gives me
$$
\frac{a}{1+a} + \frac{b}{(1+a)(1+b)}
$$ How can I reduce the second term further, and get the required result?","['algebra-precalculus', 'inequality']"
2595968,"If the solution is too simple, it must be incorrect?","Prove that to each $\epsilon > 0$ there exists a $\delta > 0$ such that $\displaystyle \int_E |f| d \mu < \epsilon$ whenever $\mu (E) < \delta$, where $f \in L^1(\mu)$. I found this question asked on MSE here , here , and here , but some of the suggestions seem relatively complicated (at least compared to what I did). When I was solving the problem, no such ideas/theorems found in those links came to my mind; indeed, I just followed my nose by playing with inequalities. But, after looking at some other solutions, I am beginning to doubt that mine is correct. Here is what I did: First note that by definition \begin{align}
\int_{E} |f|d \mu &= \sup \{\sum_{i=1}^m a_i \mu (A_i \cap E) \mid m \in \Bbb{N}, 0 \le a_i \le |f|, E = \bigsqcup_{i=1}^m A_i, A_i \mbox{measurable} \} \\
&\le \sup\{\max \{a_i\} \sum_{i=1}^m \mu(A_i \cap E) \mid ... \} \\
&= \sup\{\max \{a_i\} \mu(E) \mid ... \} \\
&= \mu(E) \cdot \sup\{\max \{a_i\} \mid ... \} \\
&\le \mu(E) \inf_{x \in E} |f(x)| \\
\end{align} The first inequality follows simply because the elements in the one set are larger than the other. The second because $0 \le a_i \le |f|$ on $E$ implies $a_i \le \inf_{x \in E} |f(x)|$ and therefore $\max \{a_i \} \le \inf_{x \in E} |f(x)|$. Now, if $\inf_{x \in E} |f(x)| = 0$, then any $\delta > 0$ will do, since the above together with this implies $\int_{E} |f| d \mu =0$. If not, then $\displaystyle \delta = \frac{\epsilon}{\inf_{x \in X} |f(x)|}$ will work. I can't spot the error.","['real-analysis', 'measure-theory']"
2596000,Second-derivative finite-difference approximation: What is the correct order?,"The ""standard"" second-derivative centered finite-difference approximation is given by LeVeque as
\begin{equation}
u''(x)\approx\frac{u(x+h)+u(x-h)-2u(x)}{h^2}\,.
\end{equation}
So if I insert
\begin{equation}
u(x+h)=u(x)+h u'(x)+\frac{1}{2} h^2 u''(x)+\frac{1}{6} h^3 u^{(3)}(x)+\frac{1}{24} h^4 u^{(4)}(x)+\mathcal{O}\left(h^5\right)
\end{equation}
and
\begin{equation}
u(x-h)=u(x)-h u'(x)+\frac{1}{2} h^2 u''(x)-\frac{1}{6} h^3 u^{(3)}(x)+\frac{1}{24} h^4 u^{(4)}(x)+\mathcal{O}\left(h^5\right)
\end{equation}
into the first equation, the $u(x)$, $u'(x)$, and $u'''(x)$ terms cancel, and the rest are divided by $h^2$ to give
\begin{equation}
u''(x)+\frac{1}{12} h^2 u^{(4)}(x)+\mathcal{O}\left(h^3\right)\,.
\end{equation}
Here I am assuming that
\begin{equation}
\frac{\mathcal{O}(h^5)}{h^2}=\mathcal{O}(h^3)\,.
\end{equation}
But LeVeque says the result should be
\begin{equation}
u''(x)+\frac{1}{12} h^2 u^{(4)}(x)+\mathcal{O}\left(h^4\right)\,.
\end{equation}
Where have I gone wrong?","['derivatives', 'numerical-methods', 'taylor-expansion']"
2596012,"Show that $\int_0^1 \ln(x) \,\mathrm dx$ converges without carrying out the integration","Integrating by parts, I know how to calculate $$\int_0^1 \ln(x) \,\mathrm dx$$ Is there a way to show that it converges without carrying out the integration?","['integration', 'definite-integrals', 'convergence-divergence', 'calculus']"
2596075,Infinite group acts on a set such that an orbit of any length exists.,"Give an example of an infinite group $G$ which acts on a set $S$ such that for each $n\in \mathbb{N}$ there is an orbit of this length. Has anyone got an idea? I've been trying something rotations in $\mathbb{C}$ of powers of matrices, something with eigenvalues, ...
I haven't found any example yet..","['group-actions', 'abstract-algebra']"
2596084,Flow on a Torus is Transitive iff it is Incommensurate,"Consider the system
$$
\dot{\mathbf{x}}(t)=\mathbf{a}
$$
where $t\in\mathbb{R},\mathbf{x}\in\mathbb{R}^n,$ and $\mathbf{a}\in\mathbb{R}^n.$
It is well known that the flow on the n-torus $\mathbb{T}^n$ generated by 
$$\varphi_t:\mathbb{T}^n\to\mathbb{T}^n
$$
given by 
$$\varphi_t\left(\mathbf{x}_0\right):=t\mathbf{a}+\mathbf{x}_0
$$
is transitive if and only if it is incommensurate.
This paper http://fe.math.kobe-u.ac.jp/FE/FE_pdf_with_bookmark/FE01-10-en_KML/fe07-091-102/fe07-091-102.pdf references several papers early on as a proof of this. However, I'm looking for a direct proof that doesn't rely on any measure theoretic notions or ergodic theory for pedagogical purposes. I think I have a proof, yet I haven't been able find one anywhere else despite this being an important example in integrable dynamical systems. Has anyone come across a direct proof of this well-known result?","['dynamical-systems', 'reference-request', 'integrable-systems', 'ergodic-theory', 'ordinary-differential-equations']"
2596098,Proving Identity for Derivative of Determinant,"For a square matrix $A$ and identity matrix $I$, how does one prove that $$\frac{d}{dt}\det(tI-A)=\sum_{i=1}^n\det(tI-A_i)$$ Where $A_i$ is the matrix $A$ with the $i^{th}$ row and $i^{th}$ column vectors removed?","['matrices', 'determinant', 'linear-algebra', 'derivatives']"
2596109,$f$ is differentiable $\Rightarrow$ we can find $I \subset \mathbb{R}$ such that $f$ is monotonic on $I$,"Let $f:\mathbb{R} \rightarrow \mathbb{R}$ Is it true that : $f$ is differentiable $\Leftrightarrow$ we can find $I \subset \mathbb{R}$ such that $f$ is monotonic on $I$ where $I = [a, b]$ ($a \ne b$ and $a, b \in \mathbb{R}$). If this is true I really don't know how to prove such result...
My intuition is saying that this is true because by definition if $f$ is differentiable then the function has a tagent line on every point of $\mathbb{R}$ and hence is monotonic, but I am not sure .","['derivatives', 'real-analysis']"
2596177,What is the simplest accurate approximation of $\sin^{-1}x$ using a (polynomial) function of $x$ and $\sqrt{1-x^2}$? [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 6 years ago . Improve this question What is the simplest accurate approximation of $\sin^{-1}x$ using a (polynomial) function of $x$ and $\sqrt{1-x^2}$? The most well-known definition in this subject is Euler's formula: $$\sin^{-1}x=-i\ln\left(\sqrt{1-x^2}+ix\right) \tag{1}$$ I'm looking for a bivariate polynomial approximation $$\sin^{-1}x\approx p_N(x,\sqrt{1-x^2}) \tag{2}$$ where $N$ is the degree of the polynomial. For example, for $N=3$, I discovered a formula $$\sin^{-1}x\approx x+(1-\sqrt{1-x^2})^2(1-0.43x) \tag{3}$$ which gives a uniformly good approximation over $x\in[0,1]$. More discussions of the problem can be found here . Are there general ways to achieve such a polynomial approximation?","['trigonometry', 'approximation']"
2596205,"Advice on when $f(x,y)=\frac{x^4 + y^4}{(x^2+y^2)^k}$ is continuous/differentiable?","I have the function $f(x,y) =
\begin{cases}
\frac{x^4 + y^4}{(x^2+y^2)^k},  & x \ne0 \\
0, & x=0
\end{cases}$ I'm trying to find the the real $k$ such that this function is continuous and differentiable at $(0,0)$. I note that $f(\lambda x,\lambda y) = \lambda ^{4-2k}f(x,y)$  implying that for $k>2$ the function blows up when it approaches $(0,0)$. But I can't seem to properly bound the function at that point when $k\leqslant2$ to show continuity or differentiability. Would anyone be able to help?","['multivariable-calculus', 'real-analysis', 'continuity', 'derivatives']"
2596230,Find $ \lim_{n\rightarrow \infty} \frac{1}{n^2} + \frac{2}{(n+1)^2}+\dots+\frac{n+1}{(2n)^2} $,"I want to find the following limit: 
$$ \lim_{n\rightarrow \infty} \frac{1}{n^2} + \frac{2}{(n+1)^2}+ \frac{3}{(n+2)^2}+\dots+\frac{n+1}{(2n)^2} $$
I guess it converges to 0 and I tried to prove it using the squeeze theorem but it doesn't seem to work.
Any ideas?","['sequences-and-series', 'limits']"
2596232,How do I integrate these functions?,"I deleted my last question, since some of you wanted me to rewrite the question properly. I feel sorry for inconvenience, but please understand that this is the first time I use $\texttt{MathJax}$. Up to now, I tried every method I know in integration, like substituition, partial fractions, uv-method, etc. But seems like nothing works. I would appreciate to have your help. Thanks. $$
\int_{0}^{1}\frac{\mathrm{d}x}{\left(x + 1\right)
\left[x^{2}\left(1 - x\right)\right]^{1/3}}\,,
\qquad\qquad\int_{0}^{1}\frac{\mathrm{d}x}{\left(x^{2} + 1\right)
\left[x^{2}\left(1 - x\right)\right]^{1/3}}
$$","['complex-analysis', 'integration', 'contour-integration', 'complex-integration']"
2596269,Proof Check: Prove $A\cup \varnothing = A$ and $A\cap \varnothing = \varnothing$.,"Can someone please verify whether my proofs are logically correct? :-) $A\cup \varnothing = A $ Proof: Let $x\in A \cup \varnothing$. Then $x \in A$ or $x \in \varnothing$. If $x \in A$, then $A \subseteq A$. If $x \in \varnothing$, this forms a contradiction (since the empty set is empty). Therefore $x \in A$ and so $A \cup \varnothing \subset A$. Now let $x \in A$. Then $x \in A \cup \varnothing$ by disjunctive amplification. Then $A \subset A \cup \varnothing$, and so $A\cup \varnothing = A$. $\square$ $A\cap \varnothing = \varnothing$ Proof: Let $x \in A \cap \varnothing$. Then $x \in A$ and $x \in \varnothing$. This forms a contradiction, since the empty set is empty. Therefore, there does not exist such an $x \in A \cap \varnothing$, and so $A\cap \varnothing =\varnothing$. $\square$","['elementary-set-theory', 'proof-verification']"
2596307,"Let $M$ be the set of all $2\times2$ real matrices, and define $f:M\rightarrow M$ by $f(A)=A^3$. Is $f$ surjective?","I know that in a finite field, injectivity implies surjectivity, but we are working in a ring, not a field. Also $f$ is not injective. I'm sure an argument could be made using a system of equations of an arbitrary cube, but there must be a more elegant solution. Something using Cayley-Hamilton, maybe? Any advice on how to approach this is appreciated.","['matrices', 'linear-algebra']"
2596329,$Re(f)=Re(g)$ implies $f(z)=g(z)+ic$,"Let $f$ and $g$ be analystic in a region $G$. If $\Re(f)=\Re(g)$ in $G$, then prove $$f(z)=g(z)+ic \tag{for all $z \in G$}$$ where $c$ is a real constant. By cauchy riemann we have $$\int \frac{\partial}{\partial y}Re(g)=-\int\frac{\partial}{\partial x}Re(g)$$ Hence, $Im(f)$ must be some constant?",['complex-analysis']
2596332,$f(z)=0$ for all $z \in \mathbb{R}$,Prove that there is no non-constant analytic function $f$ on $\mathbb{C}$ such that $f(z)=0$ for all $z \in \mathbb{R}$ . Doesn't this follow immediately by the identity theorem? There is a sequence along the real axis that converges to 0... Attempt Suppose $f$ is entire and $f(z)=0$ for all real numbers. Now let $E=\mathbb{D}$ . Since the set of points of $f=0$ has a limit point in $E$ we conclude $f$ is identically 0.,"['complex-analysis', 'analytic-functions']"
2596341,Finding radius and interval of convergence of unique series,"Here is the problem: Find the radius and the interval of convergence of $$\sum_{n\geq 1}\frac{2\cdot 4\cdot 6\cdots (2n)\,x^{n}}{1\cdot 3\cdot 5\cdots (2n-1)}.$$ I thought that I could write the numerator as $n!2^n$, and the denominator as $\frac{(2n)!}{(n!2^n)}$, but I am not sure that either of these are correct.  If you have any advice on how to solve the problem, it would be much appreciated! :) Thanks!","['factorial', 'sequences-and-series', 'limits']"
2596347,Showing that a space is Hausdorff,"Suppose $X$ is compact and Hausdorff and that $f:X \to Y$ is continuous, closed, and surjective. How can I show that $Y$ is Hausdorff?","['closed-map', 'general-topology', 'compactness']"
2596360,Example 9.7.1 in Hartshorne,"Let $f:X\to Y$ be the normalization of a curve $Y$ with a node. Then this example 9.7.1 (Hartshorne's book Algebraic Geometry) want to show $f$ cannot be flat. But I don't understand the following step: If $f$ were flat then $f_* \mathcal O_X$ would be a flat sheaf of
  $\mathcal O_Y$ modules. Then the Definition in page 254 tells that this means, for each point $x\in X$, the local ring $(f_*\mathcal O_{X})_x$ is a flat $O_{Y,f(x)}$-module. (Actually, the definition of flat sheaves is not pointed out in the book, but we may define it in analogy to the one in page 254)
On the other hand, the same definition tells that the flatness of $f$ can only implies that $\mathcal O_{X,x}$ is a flat $\mathcal O_{Y,f(x)}$-module via $f^\#: \mathcal O_{Y,f(x)}\to \mathcal O_{X,x}$, slightly different from the previous one.","['sheaf-theory', 'algebraic-geometry', 'flatness']"
2596407,Proof verification: prove $A\cup (B\cap C) = (A\cup B) \cap (A \cup C)$.,"Can someone please verify whether my proof is logically correct? :) Proof: Let $x\in A \cup (B\cap C)$. Then $x\in A$ or $x\in B\cap C$. If $x\in A$, then $x \in A\cup B$ and $x\in A\cup C$ (since for any sets A and B, $A\subseteq A\cup B$). If $x\in B\cap C$, then $x\in B$ and $x\in C$. Then $x\in A\cup B$ and $x\in A\cup C$. In either case, $x\in (A\cup B) \cap (A \cup C)$. Then $A \cup (B\cap C) \subset (A\cup B) \cap (A \cup C)$. Let $x\in (A\cup B) \cap (A \cup C)$. Then $x\in A\cup B$ and $x\in A\cup C$. Then $x\in A$ or $x\in B\cap C$. Then $x\in A \cup (B\cap C)$. Then $(A\cup B) \cap (A \cup C) \subset A \cup (B\cap C)$. Therefore, $A\cup (B\cap C) = (A\cup B) \cap (A \cup C)$. $\square$","['elementary-set-theory', 'proof-verification']"
2596430,How do you derive a normal vector from the equation of a line?,"I've seen that to find a normal vector to a line such as $3x+4y-1=0$ people take the coefficients and say $(3,4)$ is a normal vector? Why does this work? How are the coefficients related to the normal?","['analytic-geometry', 'geometry']"
2596499,Series $\sum_{n=1}^{\infty} \left[K_0\left(\sqrt{[n\beta-it]^2+s^2 }\right)+K_0\left(\sqrt{[n\beta+it]^2+s^2}\right)\right]$,"Let $\beta > 0$ and $t, s \in \mathbb{R}$. Furthermore, suppose that $-t^2 + s^2 > 0$. Define the following function:
$$
F( \beta, t, s )\ : = \ \sum_{n=1}^{\infty} \left[K_0\left(\sqrt{[n\beta-it]^2+s^2 }\right)+K_0\left(\sqrt{[n\beta+it]^2+s^2}\right)\right]
$$ where $K_0$ is the modified Bessel function of the second kind (McDonald function) of order $0$. I know that $K_0(z) \sim - \log(z)$ for $z=0$ (this is where the function blows up). Also, I know that $K_0(z) \sim \frac{e^{-z}}{\sqrt{z}}$ for $z\to\infty$ where the function dies away to $0$. I am interested in the following questions: Is there a way to evaluate this series? (I am guessing probably not. I have found nothing helpful in Gradshteyn+Ryzhik nor DLMF) How does this function look like in the limit $t \to 0$? (Asymptotically) How does this function look like in the limit $t \to \infty$? I am more interested in the latter 2 questions. Because $F$ is a series I am not sure how to deal with this.","['bessel-functions', 'functions', 'special-relativity', 'special-functions', 'sequences-and-series']"
2596530,Why did Magma had an error(?) for $x\big(x+807^2\big) \big(x+811^2\big) = y^2$,"In this post , we considered the elliptic curve, $$x\big(x+(m-1)^2\big) \big(x+(m+1)^2\big) = y^2\tag1$$ Online Magma was able to solve the case $m=2^7$ but timed-out on $m=5^7$, $$x\big(x+78124^2\big) \big(x+78124^2\big) = y^2$$ However, Allan MacLeod in this answer was able to find a rational point on it. This paper "" Finding rational points on elliptic curves using $6$-descent and $12$-descent "" (2007) by Fisher includes the elliptic curve, $$x\big(x+(n-2)^2\big) \big(x+(n+2)^2\big) = y^2\tag2$$ This has at least five torsion points, $$u = 0,\quad -(n-2)^2,\quad  -(n+2)^2,\quad  \pm(n-2)(n+2)$$ I tried the case $n=809$ of $(2)$, $$x\big(x+807^2\big) \big(x+811^2\big) = y^2$$ on online Magma using the commands, Q<x> := PolynomialRing(Rationals()); 
n:=809;
E00 := EllipticCurve(x *(x + (n + 2)^2)*(x + (n - 2)^2)); 
Q00 := Generators(E00); 
Q00; and just got [ (-651249 : 0 : 1), (654477 : 1058943786 : 1) ] which are two of the torsion points. However, according to the paper, $n=809$ does have a non-torsion rational point of huge height. Q: What are the assumptions Magma uses to find the non-torsion point of $m=2^7=128$, but not $n=809\,$? Why didn't it just time-out like the case $m=5^7=79125$, or is it a bug?","['number-theory', 'magma-cas', 'elliptic-curves']"
2596549,"Explicit formula for a reversible function $f: [0,1] \rightarrow \mathbb{R}$","There is a bijection between $[0,1]$ and $\mathbb{R}$ (because they have a same cardinality). Can we write an explicit formula for such a function? (or at least a reversible function $f$ whose domain is $[0,1]$ and its range cover all real numbers?)","['general-topology', 'real-analysis', 'elementary-set-theory', 'calculus']"
2596568,Distance of nearest zero of an analytic function,"Prove that the distance of the nearest zero of the function $f(z)={\sum}_{s=0}^{\infty}\: c_nz^n$ to the point $z=0$ is not less than $\frac{r|c_0|}{M+|c_0|}$, where r is any number not exceeding the radius of convergence of the series, and $M(r)= \max\limits_{|z|=r}|f(z)|$. I followed the hint to first establish that the function $f(z)$ has no zero in the domain where $|f(z)-c_0|<|c_0|$. The next hint is to estimate  $|f(z)-c_0|$ using Cauchy's inequality. But, I am getting $|c_0|<M$ using that inequality and unable to proceed further. Please suggest the way forward. Thanks.","['cauchy-integral-formula', 'complex-analysis', 'analytic-functions', 'holomorphic-functions']"
2596587,"""Taylor"" approximation of a function, based on 2 rather than 1 base points?","Normally, when we want to approximate a function $f(x)$ close to $x=a$, we do a Taylor approximation around $a$: $$f(x)\approx f(a)+f'(a)(x-a)+...$$ However, what if we want to approximate a function $f(x)$ on the interval $[a,b]$, by an approximating function $g(x)$ where we require the following: Contrary to Taylor approximation, where we only necessarily have that $g(a)=f(a)$ for some $a$. We now require two endpoints to coincide. That is, we need $g(a)=f(a)$ and $g(b)=f(b)$ Ideally: g(x) should be simple, easily integrable , analytically tractable, and (just like Taylor approximation) allow for arbitrary degrees of precision. Preferably polynomial. We only care about the approximation on the interval $[a,b]$ . That is, we need $g(x)\approx f(x)$ on $x\in[a,b]$ only. We care whether the integrals of $f(x)$ and $g(x)$ on $[a,b]$ are approximately equal for arbitrary functions $f(x)$. Moreover, the approximation should ideally be the ""best"" approximation in its class $\mathcal C$ . By $\mathcal C$ I mean for example, the class of polynomial functions of degree $n$. If we want more precision, we can then increase $n$. I am open to different definitions of ""best"", but I am thinking of something like ""the integral of $g(x)$ on $[a,b]$ should be the closest to that of $f(x)$ on that interval out of all possible g(x) in class $\mathcal C$"". Is there a canonical technique, similar to Taylor approximation, that satisfies these requirements? Or satisfy some of them? The most simple approximation I can come up with is simply, $g(x)=A+Bx$ with $A,B$ such that $g(a)=f(a), g(b)=f(b)$. This determines a unique function.  However, when we add a quadratic term, there are now an infinite amount of functions satisfying the boundary conditions, so which one of them is the ""best""?","['functional-analysis', 'integration', 'taylor-expansion', 'approximation']"
2596603,Vector space over F (set of functions),"$\mathbb F^{n}$ is a set of tuples of size $n$ which is obtained by Cartesian product of $\mathbb F$ $n$ times, where $\mathbb F$ is a field. It's clear. I can imagine that in my mind ))). But I don't understand how the same approach applies to $\mathbb F^{S}$ where S is a set. This is the definition: If S is a set, then $\mathbb F^{S}$ denotes the set of functions from S to F. Here $\mathbb F^{S}$ is a vector space over $\mathbb F$. For instance, $\mathbb R^{[0,1]}$ is vector space where elements are real-valued functions on [0,1]. I understand what vector space is and that $\mathbb F^{S}$ is a set of functions, but I don't understand how it correlates with $\mathbb F^{n}$. Is it a Cartesian product of $\mathbb F$ $S$ times (set times)?","['functions', 'vector-spaces']"
2596617,Fast Summation over $\frac{n^2}{k^2}$,"How to find $\sum\limits_{k=1}^n \lfloor\frac{n^2}{k^2}\rfloor$ where n is of order $10^{18}$? I have been able to calculate it for n of order $10^{14}$ by using fast-summation, but it does not seem to be just fast enough for $10^{18}$.","['number-theory', 'contest-math', 'summation', 'discrete-mathematics']"
2596665,"$X$ and $Y$ are defined on the same probability space $(\Omega, \mathcal{F}, \mathbb{P})$? Can they have different CDFs?","Suppose $X$ and $Y$ are defined on the same probability space $(\Omega, \mathcal{F}, \mathbb{P})$. Is possible that $X$ and $Y$ have different CDF?
I know the answer is yes, I don't understand why. I don't understand the link between the measure $\mathbb{P}$ and the CDFs $P_X(X \le x), P_Y(Y \le y) $ Intuitively what doest it mean for two RVs to be defined on the same probability space? Could you provide an example?","['intuition', 'probability-theory', 'probability-distributions', 'measure-theory', 'random-variables']"
2596675,CLT for triangular array of finite uniformly distributed variables,"I am interested in central limit theorems for the following rather simple setup of finite uniform distributions: Let $X_{ni}$ for $i \leq n$ and $n \in \{1,2,3,\ldots\}$ be independent discrete random variables such that $X_{ni}$ is uniformly distributed on the interval $[-a_{ni},a_{ni}] \subset \mathbb{Z}$.
Let $s_n^2 = \mathbb{V}(X_n) = \sum_i \mathbb{V}(X_{ni})$ be the variance of $X_n = \sum_i X_{ni}$. Are there necessary and sufficient conditions on the array $(a_{ni})$ implying a central limit $\frac{X_n}{s_n} \rightarrow N(0,1)$ ? It is well-known that under the assumption $\frac{1}{s_n^2}\max_i \mathbb{V}(X_{ni}) \rightarrow 0$ for $n \rightarrow \infty$, the Lindeberg condition is both necessary and sufficient.
My question thus can be given in two parts: Assuming $\frac{1}{s_n^2}\max_i \mathbb{V}(X_{ni}) \rightarrow 0$. Is it known how the Lindeberg condition translate to a (hopefully simple) condition that can be directly written in terms of the parameters $(a_{ni})$ ? Assuming $\frac{1}{s_n^2}\max_i \mathbb{V}(X_{ni}) \not\rightarrow 0$. Is it anyways possible for $X_{ni}$ to satisfy a central limit? I finally add that I have very little background on probability theory. References to books/papers answering these or related questions are highly appreciated.","['reference-request', 'probability-distributions', 'central-limit-theorem', 'probability', 'discrete-mathematics']"
2596747,"If $b_n$ is convergent, then $a_n$ is also convergent","Let $(a_n)_{n\geq 1}$ and $(b_n)_{n \geq 1}$ be two sequences of real numbers such that $$b_n=a_{n+2}-5a_{n+1}+6a_{n}, \: \forall n \geq 1$$
  Prove that if $(b_n)$ is convergent, then $(a_n)$ is also convergent. I defined $c_n=a_{n+1}-2a_n$ and the relation became $b_n=c_{n+1}-3c_n.$ Then I tried to prove that $c_n$ is convergent by expressing $c_n$ only in terms of $b_n, b_{n-1}, \dots b_1$ and $c_1$, but the convergence doesn't follow from here and I got stuck. EDIT: As proven below, this statement is false !","['recurrence-relations', 'real-analysis', 'limits', 'convergence-divergence', 'sequences-and-series']"
2596813,"Show that $f(x,y) = (x^2y-\frac13y^3, \frac13x^3-xy^2)$ is totally differentiable and calculate its derivative.","I'm working on the problem Show that $f(x,y) = (x^2y-\frac13y^3, \frac13x^3-xy^2)$ is totally differentiable and calculate its derivative. And arived at the answer $Df(x,y)=(x^2+2xy-y^2, x^2-2xy-y^2)$. But I have doubts about the method I used to get there so I'd love someone to verify my work. (In specific, I'm not entirely certain I'm allowed to just casually ignore all terms with an $h$ in them.) Solution: Consider $$\lim_{h\rightarrow 0} \frac{|f(x+h, y+h) - f(x,y) - A(h,h)|}{|(h,h)|}=0.$$ For conveinience, we'll focus our attention to the enumerator of this fraction and leave out the limit. (Leaving out a few algebraic steps for conciseness) We can then write this as $$\left|(x^2y+2xyh+yh^2+hx^2+h^3-\frac13y^3-y^2h-yh^2-\frac13h^3, \frac13 x^3+x^2h+xh^2+h^3-xy^2-2xyh-xh^2-hy^2-2yh^2-h^3) - (x^2y-\frac13y^3, \frac13x^3-xy^2)-A(h,h)\right|.$$ Note how all terms which are not multiplied by $h$ at least once cancel. This means that we can write this as $$\left|(h,h)\right|\left|(2xy+yh+x^2+h^2-y^2-yh-\frac13h^2, x^2+h^2-2xy-y^2-2yh-h^2)-
 A\right|$$
But remembering our limit, we know that we're going to have to divide by $|(h,h)|$ and that $h\rightarrow 0$ so that we have that $$|(2xy+x^2-y^2, x^2-2xy-y^2) - A| = 0,$$ so $A = (x^2+2xy-y^2, x^2-2xy-y^2)$. Another question: The place I got this from says that the problem should take anywhere from 10 to 15 minutes to solve, but with this method it takes longer. What alternative methods should I consider in solving these kinds of problems, or what should my first line of attack be? For context, another they had listed within the same time frame was: Show that $f(x,y) = |x+\pi|^3e^{3y}$ is totally differentiable and calculate its gradient.","['derivatives', 'real-analysis', 'alternative-proof', 'proof-verification']"
2596832,"What does the vector space R^[0,1] mean?","While reading a book Linear Algebra Done Right , I came to knew that a vector space $\mathbf{R}^n$ represents a space with dimensions as $(x_1, x_2, ...,x_n)$, but there were other vector spaces that I could not understand. There was a statement as Ref: 1.35 The set of continuous real-valued functions on the interval $[0,1]$ is a subspace of $\mathbf{R}^{[0,1]}$ What kind of space does $\mathbf{R}^{[0,1]}$ represent? Is this a space that can continuously be from $0$ dimension to $1$ dimension? Another statement, Ref: 1.35 The set of differentiable real-valued functions on $\mathbf{R}$ is a subspace of $\mathbf{R}^\mathbf{R}$ What kind of space is $\mathbf{R}^\mathbf{R}$? Similarly, there were other subspaces as, $\mathbf{R}^{(0,\ 3)}$ and $\mathbf{R}^{(-4,\ 4)}$ Explain me how can I visualize such spaces. If you can explain with the proof too, that will be great.","['notation', 'linear-algebra', 'functions', 'vector-spaces']"
2596839,What's the biggest circle that can fit between 2 Gaussian curves?,"What's the biggest radius possible for a circle to fit completely between the curve $y = e^{-x^2}$ and $y = -e^{-x^2}$ ? This isn't homework, I was just thinking about this randomly. I know calculus will be involved to find the radius, but I just can't figure out how to proceed. r=1 is too big",['calculus']
2596845,Prove that there is an even number of faces whose vertices get all three labels in a graph that is triangular,"Consider a planar drawing of a graph all whose faces, including the
outer one, are triangular (i.e., have 3 vertices). To each vertex, we arbitrarily assign one of the labels 1,2,3. Prove that there is an even number of faces whose vertices get all three labels. I'm trying to prove the above and I got very close with my intuition but I'm not yet there. This is what I did: I assigned odd degree with having all the three vertices. The red one is the only one that has all of them and the outer one i guess. We know that it can't only be the red one because by handshake lemma we know that the number of vertices with an odd degree is even but is this a good proof? And can I assign stuff like I did?","['graph-theory', 'discrete-mathematics']"
2596859,How to solve this polynomial optimization problem using KKT conditions?,"I am trying to find a general recipe for a nonlinear constrained optimization problems. Please correct where wrong or where parts are missing. $$
\begin{aligned}
&\min &f(x,y)&=xy+y^3\\
&\text{such that }&x &\geq y^2\\
&&x+y&\leq 2
\end{aligned}
$$ Write in standard maximization problem form $$
\begin{aligned}
&\max &&-xy-y^3\\
&\text{such that }&y^2-x &\leq 0\\
&&x+y&\leq 2
\end{aligned}
$$ Find feasible points where Kuhn-Tucker CQ (constraint qualification) does not hold. $$
\text{gradient 1st constraint: } (-1, 2y)\\
\text{gradient 2nd constraint: } (1,1)
$$ gradient 1st constraint is never linearly dependent. (never equal to $(0,0)$ for any $y$) gradient 2nd constraint is never linearly dependent. Both gradients of constraints are linearly dependent if $y=-\frac{1}{2}$ Note that the 2nd constraint in this case is binding if $x=2\frac{1}{2}$. Then the 1st constraint is not binding: $y^2-x=\frac{1}{4}-2\frac{1}{2}\neq 0$. As CQ qualification does only apply to binding constraints (and it is not binding here), we conclude CQ holds for all feasible points. What do you do if you find a feasible point where CQ does not hold? (where the point is binding for all(?) constraints) If there are more than 2 contraints, do you check them pairwise too? Write down the KT conditions First write down the Lagrangian: $\mathcal{L}=-xy-y^3-\lambda(y^2-x)-\mu(x+y-2)$ Take partial derivatives, set them equal to $0$ $$
\begin{aligned}
\text{KT conditions: }&-y +\lambda-\mu&=0\\\
&-x-3y^2-2\lambda y-\mu&=0\\
&\lambda,\mu&\geq 0\\
&\lambda(y^2-x)&=0\\
&\mu(x+y-2)&=0
\end{aligned}
$$ Find all points satisfying the KT conditions Especially the 3rd constraint is important, it induces 4 cases: (a) $\lambda =0,\mu=0$ (b) $\lambda =0,\mu>0$ (c) $\lambda>0, \mu=0$ (d) $\lambda>0, \mu>0$ (a): $\lambda =0,\mu=0\stackrel{eq. 1}{\implies} y=0\stackrel{eq. 2}{\implies}x=0$ which satisfy all conditions. (b):$\lambda =0,\mu>0\stackrel{eq. 5}{\implies}x+y=2\iff -x = y-2$. Also $eq. 1$ implies $y=-\mu$. We substitute this in the 2nd equation. $y-2-3y^2+y=0$, solving this gives, $$
3y^2-2y+2=0\implies y=\frac{2\pm\sqrt{4-24}}{6}\notin\mathbb{R}\text{ contradiction}
$$ (c) $\lambda>0, \mu=0\stackrel{eq. 4}{\implies} y^2-x=0\implies -x=-y^2$. Also $eq. 1$ implies $\lambda = y$. Plug this in $eq. 2$ gives, $$
-y^2-3y^2-2y^2=0\iff y=0, \text{ contradiction as }y=\lambda>0
$$ (d) $\lambda>0, \mu>0$, implies $y^2-x=0\implies x=y^2$ and $x+y-2=0$. Plug in the first in the second equation here, $y^2 + y - 2=0$ $$
y^2+y-2=0\implies y=\frac{1}{2}(-1\pm\sqrt{9})=1\text{ or } 2.
$$ Then $x=1$ or $4$. Thus $(x,y)=(1,1)\text{ or }(4,-2)$. Let $(x,y)=(1,1)$, $\lambda-\mu=1\implies \lambda=1+\mu$. Plug in into $eq. 2$ $$
-1-3-2\lambda-\mu=0\\
\mu=-2 \text{ contradiction as }\mu>0
$$ Let $(x,y)=(4,-2)$, then $-\mu=-2-\lambda$ substituting gin $eq. 2$ gives $\lambda=6,\mu=8$ We now have 2 points: $(0,0)$ with $\lambda=0, \mu=0$ and $(4,-2)$ with $\lambda=6, \mu=8$ Apply Extreme Value Theorem (EVT) if possible The feasible set is compact and $xy+y^3$ is continuous so EVT applies. $$
f(0,0)=0, \quad f(4,-2)=-16\implies \min \text{ at } (4,-2)
$$ Done. (questions are in italic )","['nonlinear-optimization', 'optimization', 'multivariable-calculus', 'maxima-minima', 'non-convex-optimization']"
2596870,Sigma algebra generated by a sigma algebra and another set,"I'm really stuck. If $\mathcal{A}$ is a sigma-algebra in $X$ and $C\subset X$ does not belong to $\mathcal{A}$, how do I need to show that the sigma-algebra $\sigma(\mathcal{A}\cup \{C\})$ (so generated by $\mathcal{A}$ and $C$) is equal to the collection of sets of the form $K=(A \cap  C) \cup (B \cap C^c) $, with $A,B \in \mathcal{A}$. Hope somebody can help me how to solve this problem.",['measure-theory']
2596884,Reference - basic ODE global uniqueness theorem,"Unless I am very badly mistaken it should hold that any solution $f:[0,\infty)\to \mathbb{R}^n$ ($n \in \mathbb{N}$) of the equation $$f'(t)=g(f(t)) \quad \quad (t\in [0, \infty))$$
with a given initial condition
$$f(0)=x_0$$ is unique (globally on all of $[0,\infty)$) provided that $g$ is a globally Lipschitz continuous function. However, I don't have an access to a library at the moment and surprisingly I had trouble finding a reference on Google even though it is a fairly basic result. I guess you can get the global uniqueness from the Picard-Lindelöf Theorem which gives me an interval on which the solution is unique whose length should only depend on the Lipschitz constant so I could then glue the unique solutions together to show global uniqueness...But I'd rather use a simple reference to the result I cite above. Thanks for links to any references (preferably those that can be accessed online but inaccessible books are also OK, I can access a physical library, just not every day).","['lipschitz-functions', 'ordinary-differential-equations']"
2596894,"Does $\frac{(x^2 + y^2) y}{x}$ have a limit at $(0,0)$?","Does $\frac{(x^2 + y^2) y}{x}$ have a limit at $(0,0)$? Recently, someone asked whether a function from $\mathbb{R}^2$ to $\mathbb{R}$ had a limit at $(0,0)$.  The question was easy and answered in the negative by showing that approaching $(0,0)$ on different lines led to different limits. This prompted a question: is there such a function which has a limit when restricted to any straight line through $(0,0)$ and the limit is the same in all cases yet the function does not have a limit at $(0,0)$? This led me to consider this function: $$
  f(x, y) =
\begin{cases}
\frac{(x^2 + y^2) y}{x},  & \text{if $x \neq 0$} \\
0, & \text{if $x = 0$}
\end{cases}
$$ This looks a bit nicer in polar coordinates with $x = r \sin \theta$ and $y = r \cos \theta$ $$
  f(x, y) =
\begin{cases}
r^2 \tan \theta,  & \text{if $\theta \neq \pm \frac{\pi}{2} $} \\
0, & \text{if $\theta = \pm \frac{\pi}{2} $}
\end{cases}
$$ So, if the function is restricted to a straight line through $(0,0)$ then the function clearly has the limit $0$ since $\tan \theta$ will be a constant. However, it is not continuous at $(0,0)$ as within any radius of $(0,0)$, it takes arbitrarily large values. So, here is my question: is the above right or have I made a mistake?  (I am rather rusty in this area.) Additional clarification I know that I don't need to restrict myself to straight lines when testing limits.  In fact, that was the point of the exercise: to show that straight lines may disprove a limit but testing only straight lines will not prove a limit.  I wanted an example that had a limit along all straight lines yet still failed to have a limit. Simpler examples that demonstrate this would be welcome.",['limits']
2596954,Geometric proof for $\tan{(3x)}=\tan{(x)}\tan{\left(\frac{\pi}{3}-x\right)}\tan{\left(\frac{\pi}{3}+x\right)}$,"Are there geometric proofs for the identitity
$$\tan{(3x)}=\tan{(x)}\tan{\left(\frac{\pi}{3}-x\right)}\tan{\left(\frac{\pi}{3}+x\right)}$$ My try: Thank in advances.","['euclidean-geometry', 'trigonometry', 'geometry']"
2596959,Expected number of games in the Baseball World Series,"Consider the following problem, from Understanding Probability by Henk Tijms. In the World Series Baseball, the final two teams play a series
  consisting of a possible seven games until such time that one of the
  two teams has won four games. In one such final, two unevenly matched
  teams are pitted against each other and the probability that the
  weaker team will win any given game is equal to $p=0.45$. Assuming that
  the results of the various games are independent from each other,
  calculate the probability of the weaker team winning the final. What
  are the expected value and the standard deviation of the number of
  games the final will take? The probability of the weaker team winning is:
$$
\sum_{k=0}^3 \binom{7}{k} (1-p)^kp^{7-k}\simeq 0.3917.
$$
For the expected value of the number of games, I consider the following table, where I list the number of possible wins for each of the two teams:
$$
\begin{array}{cc}
\text{strong team} & 4 & 4 & 4 & 4 & 0 & 1 & 2 & 3 \\
\text{weak team} & 0 & 1 & 2 & 3 & 4 & 4 & 4 & 4 \\
\end{array}
$$
which leads me to the expression:
$$
\sum_{k=0}^3 (4+k)\binom{4+k}{k}p^k(1-p)^4 + \sum_{k=0}^3 (4+k)\binom{4+k}{k}p^4(1-p)^k \simeq 8.622
$$
the result, however, is wrong. The only thing that I can think of is that I am counting in a wrong way the number of games with a given number of wins for the two teams. I have found the solution of this problem here , but why is my approach wrong?","['probability', 'random-variables']"
2596965,Asymptotic equivalence of Akaike Information Criterion and cross-validation?,"I've been studying information criteria for a while now from the book of Konishi et al. and I came across a chapter where the authors refer to publication made by Stone in 1977 , in which he proves the claim that Akaike Information Criteria (AIC) and cross-validation are asymptotically equivalent . In my reference book this proof is also demonstrated in a more general information criterion case. This claim, as I've understood, should be true both in linear and nonlinear regression, but the form of the information criterion is not the same in all cases. I was shown an example where the claim made by Stone seems not to hold and I wish to better understand what's going on under the hood. Here is the problem: Take the following classifier model: $$f(x)=\text{sign}(\sin(tx)),\;\;t>0.$$ Our task is to select such a frequency parameter $t$ such that a given data set $(x_1, y_1), ...(x_n, y_n)$, where $x_i\in\mathbb{R}$ and $y\in\{-1, 1\}\;\forall\,i$ is classified as well as possible. The values of the $y_i$s are labels of the data points and are selected randomly. It is known that no matter what data set we have, we can always select such a value of $t$ so that we get a perfect classification. In below is an example illustration of the function $f(x)$ which correctly classifies all data points: The argument of this example is that because such a $t$ always exists that produces a perfect classification, so the squared error term in $AIC$ is zero and we left with only the bias term, which in this case would be $2*1=2$ ($t$ only parameter), so $AIC=2$. In case of a leave-one-out cross-validation (LOOCV), we would get asymptotically a classification error where approximately $50$% of the data points were classified wrongly, since the classifier can not guess the labels of the test points due the random phenomena. So AIC-value would be $2$ and LOOCV would have a value of $CV=\sum_{i=1}^n (y_i-f(x_i))^2 = $ $4n/2=2n\;$ ($50$% of data points misclassified) no matter how many data points we have. My question: Does this prove that Stone's claim is wrong? Or is there a fundamental misunderstanding on how AIC should be applied here? P.S. please ask me if you need more information.","['information-theory', 'statistics', 'probability', 'mathematical-modeling']"
2596984,Distributing 10 euros among 5 people,"In how many ways can you distribute 10 euros among 5 people by using 1, 2, 5, 10, 20, 50 cents and 1 and 2 euros (with an unlimited supply of every coin). In part A of the question it doesnt matter what kind of coins you give the people. At first I thought I could use the possible combinations of getting $a+b+c+d+e=10000$. But this can only be used when $a , ... , e$ can have every value which is not the case. Then I thought about using generating functions:
Since every person gets 2 euros the number of ways for every person to get the 2 euros should be the same I assume we are looking for the coefficient of $[x^{200}]$, and the generating function is: $$g(x)=(x^0+x^1+x^2+...)(x^0+x^2+x^4+...)(x^0+x^5+...)(x^0+x^{10}+...)(x^0+x^{20}+...)(x^0+x^{50}+...)(x^0+x^{100}+...)(x^0+x^{200}+...)$$ To find the coefficient here we can rewrite this as:
$$g(x) = \frac{1}{1-x}\frac{1}{1-x^{2}}\frac{1}{1-x^{5}}\frac{1}{1-x^{10}}\frac{1}{1-x^{20}}\frac{1}{1-x^{50}}\frac{1}{1-x^{100}}\frac{1}{1-x^{200}}$$ For which we can make a combinatorial expression $$g(x)=\sum_{k=0}^{\infty}\binom{n+k-1}{k}x^{k}\sum_{k=0}^{\infty}\binom{n+k-1}{k}x^{2k}\sum_{k=0}^{\infty}\binom{n+k-1}{k}x^{5k}\sum_{k=0}^{\infty}\binom{n+k-1}{k}x^{10k}\sum_{k=0}^{\infty}\binom{n+k-1}{k}x^{20k}\sum_{k=0}^{\infty}\binom{n+k-1}{k}x^{50k}\sum_{k=0}^{\infty}\binom{n+k-1}{k}x^{100k}\sum_{k=0}^{\infty}\binom{n+k-1}{k}x^{200k} $$ But to find the coefficient of $x^{200}$ here wil take ages. Which makes me rethink my strategy. There must be some possible simplification somewhere. Does anyone have an idea here?? Maybe a variation on the first appoach? For question B the choice of the coins does matter per person (for instance person A doesnt want any 1 cent coins). So for this problem I will have to use the generating function.. But still how..? Any help will be appreciated!","['generating-functions', 'combinatorics']"
2597082,How does this limit with a square root behave as it approaches infinity? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question $$
\lim_{x\to \infty}\frac{2x}{\sqrt{4x^2+1}}
$$ Can I substitute $\sqrt{4x^2}$ for $2x$ in the denominator to show that this limit approaches $1$?","['calculus', 'limits']"
2597099,Every homogeneous polynomial of positive degree has nontrivial zero,"Let $\mathbb{K}$ be an algebraically closed field, and $n,m \geq 1$. If $F(T_0,T_1,...,T_n)$ is an homogeneous polynomial of degree m, then $F$ has nontrivial zeros. I would like to prove the previous statement using algebraic geometry arguments. I know it's possibile to show it via induction on $n$, as explained here . Maybe it follows directly from Projective (or affine) Hilbert's Nullstellensatz, but I cannot find any correct attempt. If I assume, on the contrary, that $V(F)=\{(u_0:u_1:\cdots:u_n)\in \mathbb{P}^n\,\,:\,\,F(u_0,...,u_n)=0\}=\emptyset$, then by Hilbert's Nullstellensatz,  exist $k\geq 1$ such that the principal ideal generated by $F$ contains the k-th power of the irrelevant ideal $R^{+}=(T_0,...,T_n)$. Is it possible from this to show a contradiction?? Hope someone can give me a hint.","['polynomials', 'algebraic-geometry']"
2597163,Real integral of $\sqrt{x}/(x^4+1)$ over the positive real axis computed as a contour complex integral,"I'm asking to evaluate the following integral
$$ \int_0^{+\infty} \frac{\sqrt{x}}{x^4+1}dx$$ And my idea was to compute it on the half upper disk of radius $R$ with a little smaller half disk around the origin. I make a branch cut on the negative imaginary axis which means that I restrict the arguments to $(-\pi/2,3/2\pi)$ According to the solution provided the result is $$\frac{\pi \cos(\pi/8)}{2+\sqrt{2}}$$ but I get $\frac{\pi}{2}(\sin(\frac{\pi}{8})+\cos(\frac{\pi}{8}))$. Here is my attempt: 
We have two simple poles in our domain, $e^{i\pi/4}$ and $e^{i3/4\pi}$, and their residues are $-\frac{1}{4}e^{i 3/8\pi}$ and $-\frac{1}{4}e^{i \pi/8}$. Hence by the Residue theorem the integral along the circuit must be equal to $$ 2\pi i (-\frac{1}{4}e^{i 3/8\pi}-\frac{1}{4}e^{i \pi/8})$$ which is equal to $$-\frac{i\pi}{2}(\cos(\pi/8)+\sin(\pi/8)+i(\cos(\pi/8)+\sin(\pi/8)))$$ Now by ML estimates I believe I can show that the integrals along the two semicircles are going both to $0$, while the integral along the negative real axis is $i$-times the one along the positive real axis, hence purely imaginary. By taking the real part the result is $$ \frac{\pi}{2}(\cos(\pi/8)+\sin(\pi/8))$$ which sadly is not the result given. Am I making any conceptual error? (maybe with the branch cut?) what's wrong? I've double checked my computations but I can't find the error","['complex-analysis', 'residue-calculus']"
2597171,Prove a norm inequality by Lagrange multipliers,I would like to prove that $\left \| x \right \|_1 \le \sqrt{n} \left \| x \right \|_2$ for all $x \in \mathbb{R}^n$ using Lagrangian multipliers. Thank you all for your help!,"['real-analysis', 'inequality', 'normed-spaces', 'lagrange-multiplier', 'analysis']"
2597209,Can one use properties of polynomials in order to generalize the generalized Cauchy-Schwartz inequality?,"Sorry about the edits guys, I forgot to add binomial coefficients, I hope I didn't cause any needless confusion. Edit(again): I've been thinking about this a bit and perhaps I should clarify the question a bit more. I'm essentially asking the following question: Does generalizing the inner product allow one to use properties of polynomials, such as discriminants, in order to derive analogues of the Cauchy-Schwartz inequality by means of inequality $(II)$? If my talk of ""generalizing the generalized Cauchy Schwartz"" is utter nonsense please focus on this question instead. Another edit: I've been pondering this question all morning and it occurs to me that it may be rather trivial, most of the people who use this website are mathematicians right? This means that something like ""generalizing the inner product and exploring its consequences"" has surely been thought of before. This tells me that my question is probably highly imperfect, and even possibly unnatural. I apologize if this is the case. If this more general framework exists and allows one to come to a better understanding of a variety of questions posed which are similar to mine, could someone perhaps reveal this to me. Stated differently, what branch of mathematics can I study that would allow me to see just how naive this question is? Again, I'm very sorry if I have offended anyone by not knowing more mathematics. Take the following proof of Cauchy-Schwartz. The intuition behind this proof becomes clear when viewed from the perspective of an inner product. Start with an inner product, that is, a function $f:V^2\rightarrow\mathbb{R}$, $V$ a vector space, that satisfies the following axioms for scalars k. $(i) f(u,v) = f(v,u)$ $(ii) f(ku,v) = kf(u,v)$ $(iii) f(u+v,w) = f(u,w) + f(v,w)$ $(iv) f(u,u) \ge 0$
Where equality in $(iv)$ holds iff $u = 0$. Leveraging each of these axioms gives us the following inequality.
$$f(u,v)^2 \le f(u,u)f(v,v) \tag{A}$$
since for scalars $t$
$$0 \le f(ut+v,ut+v)$$
implies
$$0 \le f(u,u)t^2 + 2f(u,v)t + f(v,v)\tag{I}$$
whose discriminant must be  less than or equal to zero. If $f(u,v) = \sum u_kv_k $ then $(A)$ reduces to Cauchy-Schwartz. Now, generalize the inner product as follows, let $n$ be even and $f:V^n \rightarrow \mathbb{R}$ satisfy the following axioms $(i*) f(u_1,u_2,\cdots,u_n) = f(v_1,v_2,\cdots,v_n)$ whenever $(v_1,v_2,\cdots,v_n)$ is a permutation of $(u_1,u_2,\cdots,u_n)$ $(ii*) f(u_1,u_2,\cdots,ku_m, \cdots, u_n) = kf(u_1,u_2,\cdots,u_m, \cdots, u_n)$ $(iii*) f(u_1,u_2,\cdots,u_m + u_m', \cdots, u_n) = f(u_1,u_2,\cdots,u_m, \cdots, u_n) + f(u_1,u_2,\cdots,u_m', \cdots, u_n)$ $(iv*) f(u,u,\cdots,u) \ge 0$ where equality holds iff $u=0$ Such functions obviously exist, take $f(u,v,w,s) = u_1v_1w_1s_1 + u_2v_2w_2s_2 + \cdots + u_rv_rw_rs_r$ for example. Now, axiom $(iv*)$ tells us that for scalars $t$. $$0 \le f(ut+v,ut+v,\cdots,ut+v)$$ which implies (using axioms $(i*),(ii*)$, and $(iii*)$) that
$$0 \le f(u,u,\cdots, u)t^n + \binom{n}{1}f(v,u,\cdots, u)t^{n-1} + \cdots + f(v,v,\cdots, v) \tag{II}$$ Notice that $n$ in $V^n$ must be even, otherwise inequality $(II)$ is contradictory. If $n$ were odd then the image of RHS must be $\mathbb{R}$, which is absurd. Also notice that letting $n = 2$ reduces $(II)$ to $(I)$ To give a specific example suppose that n=4 so that
$$0 \le f(u,u,u,u)t^4 + 4f(v,u,u,u)t^3 + 6f(v,v,u,u)t^2 + 4f(v,v,v,u)t + f(v,v,v,v)$$ I'm wondering if one can use, say in the case of n=4, well known properties of quartic polynomials in order to derive more fundamental inequalities than Cauchy Schwartz. Perhaps this could tell us something about the function
$u_1v_1w_1s_1 + u_2v_2w_2s_2 + \cdots + u_rv_rw_rs_r$
for example?
More generally, can one use properties of polynomials, such as discriminants (I'm not very familiar with discriminants to be honest) in order to derive more fundamental inequalities than Cauchy-Schwartz on the basis of $(II)$? If not, what kind of useful and important inequalities can be produced by means of $(II)$. Just how far does inequality $(II)$ go? In the case of $n=2$ we obtain Cauchy-Schwartz, which is quite profound. What about $n=4, 6, 8, \ldots$? Just what does $(II)$ reveal to us in these cases?","['real-analysis', 'inequality', 'polynomials', 'cauchy-schwarz-inequality', 'functional-analysis']"
2597210,How many minimal and maximal elements does this partial-ordering relation have?,"We are given set $S = \{A \in \mathcal P(\{1, \dots 10 \}) \mid 2 \le |A| \le 7 \}$ and have to find the number of maximal and minimal elements in this relation: $(A, \subseteq)$ My idea to solve this is to simply say that all of the two-element sets will be minimal and all seven-element sets will be maximal. And so, the number of minimal elements will be $\frac{10 \cdot 9}{2}$ and the number of maximal elements will be $\frac{10 \cdot 9 \cdot 8 \cdot 7 \cdot 6 \cdot 5 \cdot 4}{7 \cdot 6 \cdot 5 \cdot 4 \cdot 3 \cdot 2} = \frac{10\cdot9\cdot8}{3 \cdot 2}$ Am I missing something or it's that simple?","['relations', 'elementary-set-theory', 'order-theory']"
2597227,Proof that the generalized inverse of an increasing right-continuous function is also right-continuous,"Let $a:[0,\infty) \to [0,\infty]$ be an increasing right continuous function, and $\tau:[0,\infty) \to [0,\infty]$ be the generalized inverse, i.e. $\tau(s):=\inf\{t\ge 0: a(t)>s\}, \inf \emptyset = \infty$. Prove that $\tau$ is right continuous. Proof. $$\{t:a(t)>s\} = \inf_{\epsilon>0} \{t:a(t)>s+\epsilon\}.$$ Therefore, $\inf\{t\ge 0: a(t)>s\} = \inf_{\epsilon>0} \inf \{t\ge 0: a(t)>s+\epsilon\}$ proving right continuity. I have trouble figuring out why this last equality proves right-continuity. Is there a way to see this immediately? I would greatly appreciate any help.","['continuity', 'real-analysis', 'analysis', 'limits']"
2597261,"Equivalence relation class of $X=\{0,1\}^{\mathbb{N}}$","Consider $X=\{0,1\}^{\mathbb{N}}$, i.e. the set of functions $\Bbb N \to \{0,1\}$. Let $R$ be a relation on $X$ such that for every $f, g \in X$, $fRg$ iff the set $\{n\in\mathbb{N}|f(n)\neq g(n)\}$ is finite. I proved that the relation $R$ is an equivalence relation. Now I am trying to find the cardinality of $X/R$, the set of equivalence classes of $X$ modulo $R$. But I have some trouble to understand how to do so.",['elementary-set-theory']
2597292,Inverse of a specific lower triangular matrix,"How can I find the inverse of a n-by-n lower triangular matrix with diagonal elements all equal to 2, and lower triangular elements all equal to one? \begin{bmatrix}
  2&&&& \\
  1&2&&&\\
  1&1&2&&\\
  \vdots& \ddots & \ddots & \ddots &\\
  1&\dots&1&1&2
  \end{bmatrix}","['matrices', 'inverse']"
2597325,Two definitions of holomorphic vector bundle,"I am stuck in the task of understanding of the following. I am trying to learn about holomorphic vector bundle. So far, I have found two definitions. Definition 1: One starts with the complex (smooth or topological, does not matter) vector bundle $\pi: E \rightarrow M$ over the complex manifold $M$ , assuming there is a local trivialization of $E$ whose transition maps are holomorphic maps. Definition 2: One again starts with complex vector bundle $\pi: E \rightarrow M$ over the complex manifold $M$ , only assuming that there is a complex structure on $E$ making $\pi: E \rightarrow M$ into a holomorphic map of complex manifolds. The implication D1 $\Rightarrow$ D2 is quite clear, as one can use the local trivialization together with holomorphic atlas on $M$ to define the holomorphic atlas on $E$ , such that $\pi$ is locally just a projection. The definition D1 is quite common. For example, I use the Lectures on Kähler Geometry . On the other hand, in lecture notes by Sean Pohorence, they use D2 . In the first reference, they give the equivalence D1 $\Leftrightarrow$ D2 as an excercise. I am completely stuck in the D2 $\Rightarrow$ D1 part. Excercise 2.0.7. in the second reference hints one to show that based on D2 , every local trivialization map $\phi: U \times \mathbb{C}^{k} \rightarrow \pi^{-1}(U)$ is automatically holomorphic. Clearly, this would prove D1 . Can someone hint me on this one? Some good reference (at least stating this precisely) would be fine. Interestingly, there is already some discussion in this topic here , where people suggest that this implication may not be true, which confuses me even more... With kind regards, Jan Vysoký","['complex-geometry', 'vector-bundles', 'differential-geometry']"
2597342,Pointwise and uniform convergence of $\sum\limits_{n=1}^{+\infty}\big({\frac{x}{1+x^n}}\big)^n$,"Examine the convergence of  the series of functions
$$\displaystyle\mathop{\sum}\limits_{n=1}^{+\infty}\Big({\frac{x}{1+x^n}}\Big)^n$$
 a) pointwise   in $[0,1]$, b) uniformly in $[0,1]$. My attempt for  pointwise convergence: For all $x\in[0,1)$ exists $n_0(x)\in{\mathbb{N}}$ such that for all $n\in\mathbb{N}$ with $n\geqslant n_0(x)$ :
$$\displaystyle\Big|\Big({\frac{x}{1+x^n}}\Big)^n\Big|<\frac{1}{n^2}\,.$$
Because $\sum_{n=1}^{+\infty}\frac{1}{n^2}=\frac{\pi^2}{6}$, we have that the series  $\sum_{n=1}^{+\infty}\big({\frac{x}{1+x^n}}\big)^n$ converges pointwise in  $[0,1)$. Also for  $x=1$ :  $\sum_{n=1}^{+\infty}\big({\frac{1}{1+1^n}}\big)^n=\sum_{n=1}^{+\infty}\big({\frac{1}{2}}\big)^n=1$.
So, the series converges pointwise in $[0,1]$. I have no answer for uniform convergence. edit: This is not an answer for the uniform convergence issue. I'm just giving two plots which shows the behavior of the partial sums sequence $S_n=\sum_{k=1}^{n}\big({\frac{x}{1+x^k}}\big)^k$ near $1$, where is possible the non-uniform convergence of the series $\sum_{n=1}^{+\infty}\big({\frac{x}{1+x^n}}\big)^n$, for helping others to procced further. In the rest of the interval the series looks that converges uniformly.","['pointwise-convergence', 'sequences-and-series', 'uniform-convergence']"
2597347,Integral solutions to the equation $\left(\frac{1}{n}\right)^{-1/2}=\sqrt{a+\sqrt{15}}-\sqrt{a-\sqrt{15}}.$,"Find all integral solutions to the equation 
  $$\left(\frac{1}{n}\right)^{-1/2}=\sqrt{a+\sqrt{15}}-\sqrt{a-\sqrt{15}}.$$ Clearly, $\displaystyle \left(\frac{1}{n}\right)^{-1/2}=\sqrt{n}$, so $n\geq 0$, and the equation becomes
$$\sqrt{n}=\sqrt{a+\sqrt{15}}-\sqrt{a-\sqrt{15}}.$$
Squaring both sides (which I am aware may introduce additional solutions) gives
$$n=2a-2\sqrt{a^2-15}.$$
We require $a+\sqrt{15}\geq 0$ and $a-\sqrt{15}\geq 0$, so $a\geq 4$. Is it possible to find an upper bound for $a$, or bounds on $n$? Rearranging and squaring again (which may also introduce superfluous solutions) gives
$$4a^2-4an+n^2=4a^2-60,$$
or 
$$n^2-4an+60=0.$$
Perhaps now writing 
$$n(4a-n)=60$$
helps, since we know $n,4a-n\in\mathbb{Z}$, and the integral factors of $60$ are $1,2,3,4,5,6,10,12,15,20,30,60,-1,-2,-3,-4,-5,-6,-10,-12,-15,-20,-30,-60$. Since I have already shown $a$ and $n$ are non-negative, the negative factors can be excluded. Moreover, $n$ must be even, since if $n$ is odd, both $n$ and $4a-n$ are odd, so their product cannot be $60$. From here it is tedious (but easy) to find the solutions and then double check that they satisfy the original equation. Questions: 1) Does this solution appear to be correct? 2) Is there an easier solution? I would welcome any suggestions for alternative solutions.","['algebra-precalculus', 'radicals', 'roots', 'solution-verification']"
2597356,Vector bundle associated to a locally free sheaf,"I am starting to study vector bundles over schemes and I have encountered two different definitions of the vector bundle associated to a locally free sheaf. I tried to understand why this was the case, but the reason eludes me. Eisenbud and Harris say more or less the following about it in 3264 and all that (they say it about the projectivization of vector bundles, but I assume the comment applies equaly good to vector bundles themselves): Some sources define the vector bundle associated to a locally free sheaf of $\mathcal{E}$ to be $\text{Spec}(\text{Sym}(\mathcal{E}))$ instead of $\text{Spec}(\text{Sym}(\mathcal{E}^{\vee }))$. This convention is better adapted to the generalization from locally free sheaves to arbitrary coherent sheaves. What is the precise meaning of this last sentence? Is there an explicit example showing this phenomenon? A vector bundle of rank $r$ over a schene $B$ is a scheme $E$ together with a morphism $\pi \colon E\to B$ together with local trivialisations $\pi^{-1}(U_{i})\cong U_{i}\times \mathbb{A}^{r}$ with the corresponding conditions on the transition maps (restricted to any affine open $V=\text{Spec}(A)\subseteq U_{i}\cap U_{j}$ they are given by a linear automorphism of $A[x_{1},...,x_{r}]$. My attempt to understand this: I tried to think of this in the case of vector bundles over a smooth affine variety over $\mathbb{C}$. For example, over $\mathbb{A}^{1}=\text{Spec}(\mathbb{C}[x])$. The closed points of $\mathbb{A}^{1}$ correspond then to maximal ideals in $\mathbb{C}[x]$, which is the ring of regular functions on $\mathbb{A}^{1}$. Then suppose we have the trivial rank $2$ vector bundle $\pi \colon E=\mathbb{A}^{1}\times \mathbb{A}^{2} \to \mathbb{A}^{1}$. Then there is a natural locally free sheaf to consider, namely the sheaf $\mathcal{E}$ of local sections of $\pi$. A closed point of $E$ is a maximal ideal in $\mathbb{C}[x,y,z]$ and a global section of $\mathcal{E}$ is a morphism $\mathbb{A}^{1}\to \mathbb{A}^{1}\times \mathbb{A}^{2}$, which (I think) corresponds to the choice of two global sections $f(x)$ and $g(x)$ in $\mathcal{O}_{\mathbb{A}^{1}}$. So it seems to me that the ring of global sections is isomorphic to the ring $(\mathbb{C}[x])^{2}$. Hence the global sections of the corresponding symmetric algebra $\text{Sym}_{\mathcal{O}_{\mathbb{A}^{1}}(\mathbb{A}^{1})}(\mathcal{E}(\mathbb{A}^{1}))=\text{Sym}_{\mathbb{C}[x]}(\mathbb{C}[x]^{2})=\mathbb{C}[x][y,z]=\mathbb{C}[x,y,z]$ is just $\mathcal{O}_{E}(E)$. Hence, closed points in $E$ really correspond to maximal ideals in $\text{Spec}(\text{Sym}(\mathcal{E}))$, which was supposed to be the less natural definition (at least the less classical according to Eisenbud and Harris). I suspect that I made a mistake somewhere in this reasoning, because Eisenbud and Harris suggest that the points of this scheme should correspond to the fibers, and not to points of the vector bundle (and because I am not really familiar with any of the concepts that I am using). I suspect that a mistake could be that I don't see the difference between $\text{Sym}(V)$ and $\text{Sym}(V^{\vee})$ , besides covariant and contravariant. I tried to find out the difference here , but I still cannot see any clear difference. In that question, a user suggests that bigger differences appear on finite fields. Can anybody show this with an example? And how would I describe $\text{Spec}(\text{Sym}(\mathcal{E}^{\vee}))$ in an equally explicit way? In particular, the place where I get stuck is when trying to describe the group of $\mathcal{O}_{\mathbb{A}^{1}}$-module morphisms from $\mathcal{E}$ to $\mathcal{O}_{\mathbb{A}^{1}}$ (but I have the feeling that there should be an easy way to describe them that I am just not seeing right now).","['abstract-algebra', 'coherent-sheaves', 'vector-bundles', 'algebraic-geometry']"
2597364,Maximum likelihood estimate and Wald interval for binomial success probability $p$,"In an interview $63\%$ of $100$ randomly chosen people gave an positive answer. $p$ is this amount from all people.
What is the maximum likelihood estimation of $p$? Calculate the wald interval of $p$! I understand it so, that $p = 0.63 $. 
I also want to give the steps of the deviation of the formula of the likelihood. So I tried to start with this formula for the likelihood: $L(p)=\prod \limits_{i=1}^{n}p(x_i, p) = \prod \limits_{i=1}^{n}P(X_i=x_i) = \prod \limits_{i=1}^{100}P(X_i =0.63)$ $\hat{p} = \frac{1}{n} * \sum\limits_{i=1}^{n} x_i = \frac{1}{100} * 63 = 0.63$ And for the wald interval: $\hat{p} \pm Z_{1-\frac{\alpha}{2}} * \sqrt{\frac{\hat{p} (1-\hat{p} )}{n}}$ When I use $0.63$ for $\hat{p}$ I get the interval $[0.54; 0.72]$. So I don't really understand how to get from the formula of $L(p)$ to the value of $\hat{p}$. I'm also not sure if this is already my final result for the likelihood. I got this with the help of a book, and there seem to be some steps missing. I'm also not sure if this is in general the right way of solving it and if I used the right formula and values for the Wald interval.","['maximum-likelihood', 'statistics', 'probability', 'confidence-interval']"
2597411,Rank of a combinatorial matrix,"The following -- seemingly purely combinatorial -- problem arose in a cohomology computation: Let $n$ be an integer and denote by $C^1$ the set of (ordered) $2$-tuples with entries in $0,\dotsc,n$ and by $C^2$ the set of (ordered) $3$-tuples. In the following, we want to consider $C^1$ and $C^2$ as free $\mathbb{Z}$-modules. Consider that map $d\colon C^1\to C^2$ that sends a tuple $(i,j)$ to $f(i,j)\cdot(l,m,n)$ where
$$f(i,j)=\begin{cases}1,&\text{if $i=l$ and $j=m$,}\\-1,&\text{if $i=l$ and $j=n$,}\\1,&\text{if $i=m$ and $j=n$,}\\0,&\text{otherwise.}\end{cases}$$
We can then describe this map via an $\binom{n+1}{3}\times\binom{n+1}{2}$ matrix $M_n$. For example: For $n=2$ there are three $2$-tuples $(0,1), (0,2)$ and $(1,2)$ and one $3$-tuple $(0,1,2)$. Therefore $M_2=(1,-1,1)$. For $n=3$ there are six $2$-tuples $(0,1), (0,2), (0,3), (1,2), (1,3), (2,3)$ and four $3$-tuples $(0,1,2), (0,1,3), (0,2,3), (1,2,3)$. Therefore
$$M_3=\begin{pmatrix}1&-1&0&1&0&0\\1&0&-1&0&1&0\\0&1&-1&0&0&1\\0&0&0&1&-1&1\end{pmatrix}.$$ The rank of $M_2$ is obviously $1$, the rank of $M_3$ is $3$ and $M_4$ is a $10\times 10$ matrix of rank $6$. I claim that the rank of $M_n$ is always $\binom{n}{2}$ but I see no easy/down-to-earth combinatorial proof of this and would be very grateful for any suggestions or references.","['homology-cohomology', 'combinatorics']"
2597429,Meaning of cross terms in multivariable Taylor expansion,"The cross terms in the Taylor expansion of $f = f(x,y)$ in $(x_0,y_0)$ 
$$
f(x,y) = f(x_0,y_0) + \ldots + \frac{1}{2!}\bigg( \frac{\partial^2 f}{\partial x^2} (\Delta x)^2 + \color{green}{2\frac{\partial^2 f}{\partial x \partial y} \Delta x \Delta y} + \frac{\partial^2 f}{\partial y^2}(\Delta y)^2 \bigg) + \ldots \tag{1}
$$ can be seen as to have arosen from the cross terms of 
$$
f(x,y) = \sum_{n=0}^\infty \frac{1}{n!} \bigg[ \bigg( \Delta x \frac{\partial}{\partial x} + \Delta y \frac{\partial}{\partial y}\bigg)^n \ f(x,y)\bigg]_{x_0,y_0} , \tag{2}
$$ 
where we consider the partial derivatives to only operate on $f$. I'm missing the interpretation of these cross terms. $(2)$ somewhat explains where the (algebraic structure of) these cross terms comes from, but it doesn't give me any insight as to why we need to consider the product of two changes. Moreover, I also don't grasp how my text arrives at $(2)$. Why are the cross terms included? I understand the operation of mixed derivatives in terms of calculations, but I want to know how I can interpret them. Furthermore, I also understand the concept of stationary points of multi-variable functions. I don't think the concept of stationary points are relevant here. I'm not necessarily looking for a geometric interpretation; as long as the relevance of the mixed partial derivative terms is made evident.","['multivariable-calculus', 'taylor-expansion', 'partial-derivative']"
