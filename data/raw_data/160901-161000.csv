question_id,title,body,tags
2784476,General solutions to first-order differential equations with disitrubutional coefficients,"Consider the first-order differential equation $$\dot{x}+p(t)x=q(t).$$ This can be generally solved using an integrating factor $$a(t)=\exp\left(\int p(t)dt\right)$$ and the solution is $$x(t)=\frac{1}{a(t)}\left(\int a(t)q(t)dt+C\right)$$ (I'm going to skip over why this works - I can more information if people don't recognize this). The point is that this is a general method - it should work for any smooth functions $p(t)$ and $q(t)$. What I'm interested in is if a similar thing is true for the same differential equation, $$\dot{x}+P(t)x=Q(t),$$ but where $P(t)$ and $Q(t)$ are distributions, that is, linear functions on test functions. One wouldn't normally think techniques on smooth functions would extend to distributions, but since the final solution basically just depends on integrals of the distributions, I would be optimistic that it might work for all distributions. Are there any results in this area? I do have a specific case in mind, in which $P$ and $Q$ are made up of delta functions and it's derivatives, but the question is more general, and I would love a specific reference (which is hopefully readable :-) )","['dirac-delta', 'distribution-theory', 'ordinary-differential-equations']"
2784489,Galois cover ramified at some points,"I have seen statements like this "" Fermat curve is a wild Galois cover of $\mathbb{P}_{k}^{1}$ ramified at $\{0,1, \infty\}$."" Here, $k$ is a field of characteristic $p>0$. I would like to understand what it precisely means. Here by Fermat curve $F_n$ we mean $\operatorname{Proj}(k[x,y,x]/(x^n+y^n-z^n)$. Now, the usual map from $\phi:F_n \to \mathbb{P}_{k}^{1}$ which is flat and unramified at all points except at $\{0,1, \infty\}$. Now since this cover is Galois, I expect  $Aut(F_n,\mathbb{P}_{k}^{1}) = \operatorname{deg}(\phi)$. Is the above interpretation correct? Also, what is the group $Aut(F_n,\mathbb{P}_{k}^{1})$ here?","['algebraic-geometry', 'abstract-algebra', 'etale-cohomology', 'number-theory', 'commutative-algebra']"
2784492,Six People Permutation in Linear Arrangement With Exceptions,"I can't seem to wrap my head around this. My professor hasn't done anything like this in class, and all I've found on the internet on this are circular arrangements. We have Arnie, Betty, Carl, Debbie, Ernie, and Felicia Ernie and Felicia refuse to be seated by each other AND Arnie and Betty refuse to be seated beside each other. How many line-ups are possible?","['permutations', 'discrete-mathematics']"
2784523,Is this Galois Group is isomorphic to $S_5$,"We want $\text{Gal}_\mathbb{Q}(f) \cong S_5$ where $f = x^5 + 2x + 2$.  This polynomial has 5 roots, one real and 4 imaginary, call them $a,b_1,b_2,c_1,c_2$ where $b_1, b_2$ and $c_1, c_2$ are conjugates.  Then $|\text{Gal}_\mathbb{Q}(f)| = [\mathbb{Q}(a,b_1,c_1): \mathbb{Q}(a, b_1)][\mathbb{Q}(a, b_1): \mathbb{Q}(a)][\mathbb{Q}(a): \mathbb{Q}]$.  I think this equals $2 \times 2 \times 5$, but I'm not sure and I'm not sure how to show it.  In this case $|\text{Gal}_\mathbb{Q}(f)|=20$ and therefore is isomorphic to a subgroup of $S_5$ with 20 elements, which I guess would be $S_3 \times \mathbb{Z}_2$.  Is this correct and how would I show that those are the degrees of the extensions?","['abstract-algebra', 'galois-theory', 'group-theory']"
2784531,Distribution of joint Gaussian conditional on their sum,"Let $X = (X_1, X_2, \dots, X_n)$ be jointly Gaussian with mean vector $\mu$ and covariance matrix $\Sigma$ . Let $S$ be their sum. I know that the distribution of each $X_i \mid S = s$ is also Gaussian. When $n=2$ , I know that $$
E\left( X_1\mid S = s \right) = s \frac{\sigma_1^2}{\sigma_1^2 + \sigma_2^2}
$$ and $$
V\left(X_1\mid S = s \right) = \frac{\sigma_1^2\sigma_2^2}{\sigma_1^2 + \sigma_2^2}
$$ (see here and here ). I could probably work out analogous expressions for an arbitrary $n$ if I sat down with a pencil and paper and worked at it for a bit. What I want to know is, what is the distribution of $X$ given $S = s$ ? I know that this can't be Gaussian, since the sum is bounded. It's clearly not Dirichlet or anything Dirichlet-esque, since the marginal distributions are Gaussian. But beyond that I don't have a clue.","['statistics', 'probability', 'normal-distribution', 'probability-distributions']"
2784567,How can I calculate the number of permutations of an irregular rubik's cube?,"Recently I made a 1x2x3 Rubik's cuboid (example here ) for my little brother, thinking that the number of possible permutations of the cuboid would be small enough that he could solve it simply by twisting randomly. Sure enough, he can (as can any one else). However, I wanted to know the exact number of possible permutations this little cuboid can have, and an extensive internet search revealed nothing. I found sites capable of giving me the number of permutations of any regular (nxnxn) Rubik's cube, but giving me the formula used or any information about irregular cuboids. So, what formula can I use to calculate the number of possible permutations in any irregular cuboid, and more specifically the 1x2x3? Also, forgive the elementary nature of this question. While most mathematicians would most likely find this combinatorics problem simple, I have never studied probability and cannot find a simple solution to this online; therefore I deemed this question pertinent to Math SE.","['puzzle', 'combinatorics', 'rubiks-cube']"
2784580,Can ideals of the coordinate ring describe Cartier divisors?,"I'm implementing basis computation of Riemann-Roch spaces in Sage, and I'm puzzling over how to specify the divisor being input.  Assume we're given a projective (but not necessarily non-singular) curve. Places on the curve's function field are in one-to-one bijection with prime ideals of the maximal finite order (for finite places) and the maximal infinite order (for infinite places).  This is proposition 5.3 in [He01], and is the natural form to provide input to [He01]'s algorithm for Riemann-Roch basis space calculation.  A divisor is specified using a pair of fractional ideals, one for finite places, and one for infinite places. Another common way of specifying places is to use ideals of the coordinate ring.  So, the ideal $(x-2z, y-3z)$ would correspond to the point $(2:3:1)$ if the curve was, say, $y^2z - x^3 - z^3$. I'm trying to figure if there's some straightforward way to convert from the second representation to the first one.  I'm starting to think that there is none. Here's my logic:  Ideals in the coordinate ring correspond to (effective) Weil divisors.  Why?  Well, we can do a primary decomposition on them and get a set of associated prime ideals.  Those correspond to subvarieties, and that implies a Weil divisor. Yet the Riemann-Roch theorem, and the whole theory of Riemann-Roch spaces really deals with Cartier divisors, right?  We need to work with a non-singular model of the curve for Riemann-Roch to hold, and Cartier divisors give us the ability to distinguish between multiple places over a singularity, while for Weil divisors, a singularity is just a single point. Since the [He01] formulation deals with ideals of orders of the function field , this gives the power to express Cartier divisors, while ideals of the coordinate ring are only able to express Weil divisors. So there's probably no way to easy way to make that conversion, and specifying ideals of the coordinate ring isn't a very good way to specify a divisor, because we can only specify Weil divisors that way. Does this make sense?  (and thanks for taking the time to read it) [He01]: Florian Hess, Computing Riemann-Roch spaces in algebraic function fields and related topics , J. Symbolic Computation 33 (2002) 425-445. doi:10.1006/jsco.2001.0513","['algebraic-curves', 'algebraic-geometry']"
2784591,Bounded sequence in a Hilbert such that all subsequences that weakly converge do so to the same limit,"Let $H$ be a Hilbert space, $a \in H$ and $(x_n)_n$ a bounded sequence in $H$ such that every subsequence of $H$ that converges weakly converges to $a$. How do you prove that $(x_n)_n$ converges weakly to $a$? The only thing I know is that there is indeed at least one subsequence of $(x_n)_n$ that converges weakly.","['functional-analysis', 'weak-convergence', 'hilbert-spaces']"
2784646,Finding period of rational function of trigonometric functions,"I want to find the period of the function
 $$f(x)=\frac{2\cos (3x)\cdot |\sin(5x)|\cdot 3|\tan (8x)|\cos^4(x)}{7\tan\left( \frac{x}{9}\right)\cdot \sec^3\left(\frac{2x}{3}\right)}$$
which is a rational function of trig functions. So far I have found that: the period of $\displaystyle \cos(3x)$ is $\frac{2\pi}{3};$ the period of $\displaystyle |\sin (5x)|$ is $\frac{\pi}{5};$ the period of $\displaystyle \cos^4(x)$ is $\pi$; the period of $\displaystyle \tan(x/9)$ is $9\pi$; and the period of $\displaystyle \sec^3(2x/3)$ is $3\pi$. How do I find the total period?","['periodic-functions', 'functions']"
2784661,Iterated Limits Proof,"If $\lim\limits_{(x,y) \to (a,b)} f(x,y) = L$, and if the one-dimensional limits
$~\lim\limits_{x \to a} f(x,y), ~\lim\limits_{y \to b} f(x,y)~$ both exist, prove that
$$\lim\limits_{x \to a} \Bigg(\lim\limits_{y \to b} f(x,y)\Bigg) = \lim\limits_{y \to b} \Bigg(\lim\limits_{x \to a} f(x,y)\Bigg) = L$$ Proof. Since  both one-dimensional limits exist, let $~\lim\limits_{x \to a} f(x,y) = L_a, ~ \lim\limits_{y \to b} f(x,y) = L_b$. For every $\epsilon > 0$ there exist $\delta > 0$ such that if the points $(x,y)$ are in the neighborhood $V_\delta (a,b)$, the points $f(x,y)$ are in the neighborhood $V_\epsilon (f(a,b))$, in other words $$0 < |(x,y) - (a,b)| < \delta$$ implies $$|f(x,y) - L| < \epsilon/2$$ Also, for every $\epsilon > 0$ there exist $\delta > 0$ such that if $0 < |x-a| < \delta$, then $|f(x,y) - L_a| < \epsilon/2$ From the properties of inequality, $|x| - |y| \leq |x-y|$, $$|L_a - L| - |L_a - f(x,y)| \leq |f(x,y) - L|$$ $$|L_a - L| \leq |f(x,y) - L| + |L_a - f(x,y)| < \epsilon/2 + \epsilon/2 = \epsilon$$ Thus, for every $\epsilon > 0$ there exist $\delta > 0$ such that if $0 < |y-b| < \delta$, then $|L_a - L| < \epsilon$. This implies that, $$\lim\limits_{x \to a} \Bigg(\lim\limits_{y \to b} f(x,y)\Bigg) = L$$ By appyling the same argument with the other limit, we conclude that $$\lim\limits_{y \to b} \Bigg(\lim\limits_{x \to a} f(x,y)\Bigg) = L$$ Can anyone comment on my work? Have I missed something?","['real-analysis', 'calculus', 'limits']"
2784697,Find the solutions to:$\frac{d^2y}{dx^2}=\left(\frac{dy}{dx}\right)^2$.,"Find the solutions to:$\displaystyle\frac{d^2y}{dx^2}=\left(\frac{dy}{dx}\right)^2$. I got the following solutions:- $\left(\frac{dy}{dx}\right)=0\Rightarrow y=c_1$ is a solution $\left(\frac{dy}{dx}\right)=1\Rightarrow y=x+c_2$ is another solution Are there any other solutions? I dont have any idea about how to solve a $2^{nd}$ order non linear DE. As far as i Know , a $2^{nd}$ order linear DE could be solved with the help of auxillary equations , Is there any such similar methods applicable to this problem",['ordinary-differential-equations']
2784710,British Maths Olympiad (BMO) 2004 Round 1 Question 4 alternative solutions?,"The question states: Alice and Barbara play a game with a pack of $2n$ cards. On each of which is written a positive integer. The pack is laid out in a row, with the numbers facing upwards. Alice starts, and the girls take turns to remove one card from either end of the row, until Barbara picks up the final card. Each girl’s score is the sum of the numbers on her chosen cards at the end of the game. Prove that Alice can always obtain a score at least as great as Barbara’s. My solution uses the idea that if we label the cards: $a_1,a_2 \cdots a_{2n} $ We can show that Alice can force Barbara to either pick up all the evens, or all the odds. It then follows naturally that either the sum of the two sequences are equal or one is greater than the other. What are some other solutions to this problem that use elementary methods? Describing the motivation for a solution would also be highly appreciated.","['algebra-precalculus', 'contest-math', 'optimization', 'combinatorial-game-theory']"
2784741,prove this inequality with $63$,"Let $x,y,z,w>0$, and such $x^2+y^2+z^2+w^2=1$. show that
  $$x+y+z+w+\dfrac{1}{63xyzw}\ge\dfrac{142}{63}\tag{1}$$ I know
$$x^2+y^2+z^2+w^2\ge 4\sqrt[4]{x^2y^2z^2w^2}\Longrightarrow xyzw\le \dfrac{1}{16}$$
so we have
$$\dfrac{1}{63xyzw}\ge\dfrac{16}{63}$$
But other hand
$$(x^2+y^2+z^2+w^2)\ge\dfrac{1}{4}(x+y+z+w)^2\Longrightarrow x+y+z+w\le 2$$
So the above solution is not correct, so how do you prove this inequality $(1)$","['mixing-variables', 'real-analysis', 'inequality', 'multivariable-calculus', 'uvw']"
2784776,"If $|a_1|>1$, then the series $\sum\frac{a_1^n+\cdots+a_k^n}{n}$ diverges","Let $a_1,...,a_k$ be complex numbers different from $1$. I am trying to prove that if at least one of them has a module strictly greater than $1$, say $|a_1|>1$, then the series $\displaystyle\sum\frac{a_1^n+\cdots+a_k^n}{n}$ diverges. I have tried to prove that $\dfrac{a_1^n+\cdots+a_k^n}{n}$ doesn't converge to $0$ as it should for the series to converge, and for that I've rewritten it as: $$
\frac{|a_1^k+\cdots+a_n^k|}{k}=
\frac{|a_1|^k}{k}\Big|1+\Big(\frac{a_2}{a_1}\Big)^k
+\cdots+\Big(\frac{a_n}{a_1}\Big)^k\Big|
$$
I thus have to prove that the sum after the $1+\cdots,\,$ doesn't converge to $-1$, but I can't seem to do it.","['complex-analysis', 'sequences-and-series', 'convergence-divergence']"
2784897,"Is there a matrix $A$ such that for every other matrix $B$, we have $\mbox{tr}(AB) = 0$?","I'm struggling to prove/disprove this notion. I've figured if such matrix exists, it has to be nilpotent and non-invertible, and the sum of its eigenvalues is 0. Can anyone chip in? Edit : Oh yeah, $A \neq 0$","['matrices', 'linear-algebra']"
2784961,Poisson Sample Mean Wald Test,"Let $X_1,\ldots,X_n\sim\text{Poisson}(\lambda)$, $H_0:\lambda=\lambda_0$, and $H_1:\lambda\neq\lambda_0$ for $\lambda_0>0$. Compute the size $\alpha$ Wald test, estimating $\lambda$ by $\overline{X}_n$. The size $\alpha$ Wald test rejects $H_0$ if and only if $\left|\tfrac{\hat{\lambda}-\lambda_0}{\operatorname{se}(\hat{\lambda})}\right| > \Phi^{-1}\left(1-\dfrac{\alpha}{2}\right)$. $\hat{\lambda}=\overline{X}_n$, so $\mathbb{V}(\hat{\lambda})=\tfrac{\mathbb{V}(X_i)}{n}=\tfrac{\lambda}{n}$, so $\operatorname{se}(\hat{\lambda})=\sqrt{\tfrac{\lambda}{n}}$, which we can estimate by $\sqrt{\tfrac{\hat{\lambda}}{n}}=\sqrt{\tfrac{\overline{X}_n}{n}}$. I notice in The Wald test with Poisson distribution the second answer uses $\sqrt{\tfrac{\lambda_0}{n}}$ instead, which seems fine because we're assuming $H_0$. Of course if we assume $H_0$ completely then we'd know $\lambda=\lambda_0$, but the second answer only assumes $H_0$ partially, if that makes sense. Are both of our methods valid?","['means', 'hypothesis-testing', 'maximum-likelihood', 'statistics', 'poisson-distribution']"
2784980,Finding $\sin 2x+\sin 4x+\cdots+\sin 22x$ [duplicate],"This question already has answers here : How can we sum up $\sin$ and $\cos$ series when the angles are in arithmetic progression? (8 answers) Closed 6 years ago . I have to find this sum:
$$S= \sin(2x)+\sin(4x)+\sin(6x)+\cdots+\sin(22x).$$
I tried to multiply the $S$ by $i$ then add a so called ""$T$"" where
$$T = \cos 2x + \cos 4x +\cdots+\cos 22x.$$
From here I obtained: 
$$T+iS=(\cos x+i\sin x)^2+(\cos 2x+i\sin2x)^2+\cdots+(\cos11x+i\sin11x)^2.$$
What should I do next?",['trigonometry']
2784985,An example of a a convolution of singular distribution and a Gaussian distribution that has a 'simple' pdf,"I am looking for a nontrivial example of a singular distribution that when convolved with a Gaussian distribution has a pdf of a 'simple'  form. I let 'simple' be something that you interpret yourself. Singular distributions are an import class of distributions that are often 'swept under the carpet.'   I would like to see a nice illustrative example of how to work with such distributions. One way to do this is to give a characteristic function $\phi(t)$ that when multiplied by $e^{-t^2/2}$ has a simple Fourier inverse. However, I  don't have a good choice  of the characteristic function $\phi(t)$ that would lead to  a meaningful result. For example, for the Cantor distribution, the characteristic function is given by \begin{align}
\phi(t)=e^\frac{it}{2} \prod_{i=1}^\infty \cos \left( \frac{t}{3^k} \right).
\end{align} However, it and is not easy to work with this characteristic function \begin{align}
\phi(t) e^{-\frac{t^2}{2}}.
\end{align} In particular, it is difficult to fuind its Fourier inverse. Edit: By singular distributions I mean:  A singular distribution is a probability distribution concentrated on a set of Lebesgue measure zero, where the probability of each point in that set is zero. Edit 2: Another approach we can take is to look at the convolution directly. That is look at the $U=X+V$ where $V$ is has a singular distribution and $X$ is Gaussian, in this case, the pdf of $U$ is given by \begin{align}
f_U(u)=E\left[ \frac{1}{\sqrt{2 \pi}}  e^{-\frac{(u-V)^2}{2}} \right].
\end{align} I was wondering if we can come up with a sequence of random variables $V_n$ that converges in distribution to some $V$ with a singular distribution, for which we can compute the limit \begin{align}
\lim_{ n \to \infty}E\left[ \frac{1}{\sqrt{2 \pi}}  e^{-\frac{(u-V_n)^2}{2}} \right].
\end{align}","['probability-theory', 'convolution', 'singular-measures']"
2784987,"What is $\ \overline{\bigcup_{p≥ 1}\ \{A\in M_n(\mathbb C), \ A^p = I_n\}} \ $?","Let $\Gamma_p = \{A\in M_n(\mathbb C), A^p = I_n\}$ and let $\Gamma = \bigcup_{p≥ 1}\ \Gamma_p$. What is the closure of $\Gamma$ ? (This is from an oral exam). Let $B \in M_n(\mathbb C)$ such that there exists a sequence $(A_k)_k$ of elements of $\Gamma$ such that $A_k \to B$. $\chi_{A_k} \to_{k\to\infty} \chi_B$ ($\chi$ is the characteristic polynomial). We will prove that each eigenvalue $\lambda$ of $B$ satisfies $|\lambda | =1$. Writing $\chi_{A_k} = \prod_{1 ≤ i ≤ n}(X-a_{i,k})$, with $|a_{i,k}|=1$ for all $i$ and $k$ ; all $(a_{i,k})_k, 1≤i≤n,$ are bounded so we can apply Bolzano-Weierstrass theorem in $\mathbb C^n$ to prove there exists $\phi$ an extraction such that $a_{i, \phi(k)} \to_{k\to\infty} l_i$ for all $i$, $1≤i≤n$. $\chi_{A_{\phi(k)}} \to_{k \to \infty} P$ and the roots of $P$ are the limits $l_i$ of $a_{i, \phi(k)}$, hence $|l_i|=1$ for all $i$. Otherwise $\chi_{A_{\phi(k)}} \to \chi_B$, hence $ \chi_B = P$. Reciprocally, suppose $B$ is such that for all eigenvalue $\lambda,$ $|\lambda| =1$. $B = P^{-1}TP$ with $T$ being upper triangular. For every diagonal coefficient $t_i$ of $T$, $t_i$ is one of the eigenvalue of $B$, hence $|t_i|=1$. For all $t_i$, we consider a sequence of roots of $1$ $(a_{i,k})_k$ such that $a_{i,k} \to_{k\to\infty} t_i$ and we choose these sequences such that for all $k$ and for all $(i,j), \ 1 ≤ i < j≤ n$, $a_{i,k} \neq a_{j,k}$ (using the density of the roots of $1$ in the unit circle). Let $A_k$ be the upper triangular matrix with diagonal coefficient $i$ equal to $a_{i,k}$ for  $1 ≤ i ≤ n$ and the other coefficients equal to the ones of $T$. We will prove that for all $k$, there exists $p_k ≥ 1$ such that $A_k^{p_k} = I_n$ : For all $i$ there exists $p_{i,k}$ such that $a_{i,k}^{p_{i,k}} = 1$. Let $p_k$ be a common multiple of the $p_{i,k}, 1 ≤ i ≤ n$. $\chi_{A_k} = \prod_{1 ≤ i ≤ n}(X-a_{i,k})$ vanishes $A_k$ and the $a_{i,k},  \ 1 ≤ i ≤ n$ are distinct, hence $X^{p_k} - 1$ vanishes $A_k$ i.e. $A_k^{p_k} = I_n$. Finally, $(P^{-1}A_kP)_k$ is a sequence of $\Gamma$ (because $(P^{-1}A_kP)^{p_k} = I_n$)  that converges towards $B$ since $A_k \to T$. Is that correct?","['eigenvalues-eigenvectors', 'matrices', 'general-topology', 'linear-algebra', 'vector-spaces']"
2784990,Show that every solution $x(t)$ of $x'(t)= A(t)x(t)$ converges to some limit,"(Long-time asymptotics). Suppose $$\int_0^∞\|A(t)\|\,dt < ∞.$$ Show that every solution $x(t)$ of $x'(t)= A(t)x(t)$ converges to some limit:
$\lim_{t→∞} x(t) = x_∞.$ (Hint: First show that all solutions are bounded and then use the corresponding integral equation.) I was able to solve a first part: I used that the solutions are limited by $\|\phi(t,t_0)\| \leq e^ {\int_0^∞\|A(t)\|\,dt}$. As $\int_0^∞\|A(t)\|\,dt < ∞$, then $\|\phi(t,t_0)\| \leq K $. The problem now is to show that the above limit always exists. Someone can help",['ordinary-differential-equations']
2785009,"If $gh=hg$ and $\rho$ is a representation of $G$, then the matrices $\rho(g)$ and $\rho(h)$ are simulatenously diagonalizable","Let $G$ be a finite group, $|G|=n$, and $g,h\in G$ such that $gh=hg$. If $\rho: G \rightarrow GL(V)$ is a representation, where $V$ a vector space over $\mathbb{C}$,  prove that there exists a basis of $V$ in which both $\rho(g)$ and $\rho(h)$ are diagonal. I could prove this result using general methods of linear algebra , as follows: Since $G$ is finite, $|G|=n$,  it follows that $\rho(g)^n =\rho(g^n) = \rho(1) = I$. If $p(X) = X^n-1,$ the condition before shows us that $p(\rho(g))=0$, which ensures that the minimal polynomial for $\rho(g)$ must divide $X^n-1$. Now, $X^n-1$ has no repeated roots and therefore so does the minimal polynomial. We conclude from this that $\rho(g)$ is diagonalizable. The same argument runs for $\rho(h)$. Now, both are diagonalizable and commuting. By linear algebra, they are simultaneously diagonalizable. My question is: is there a way to prove this claim using only representation theory methods? or a proof which is heavily based on representation theory methods?","['representation-theory', 'linear-algebra']"
2785037,"What are the semi-norms on $M_n(\mathbb C)$ such that $\| AB\| = \| BA\|$, for all $A$ and $B$?","What are the semi-norms on $M_n(\mathbb C)$ such that $\| AB\| = \| BA\|$, for all $A$ and $B$ in $M_n(\mathbb C)$, $n ≥ 2$? I came across this exercise (an oral exercise), and I have thought about this : Is there a semi-norm that respects matrix similitary? Do you have another idea to solve this problem? It does not seem really natural to directly think of similitary and does not allow to conclude apparently: are these the only such semi-norms?","['matrices', 'normed-spaces', 'linear-algebra']"
2785093,Plucker relations imply Leibniz formula?,"Let $E$ be a finite set, $n \in \mathbb{N}$ and $f:E^n \rightarrow \mathbb{Q}$ be an alternating function such that
$$\sum_{i=0}^n (-1)^i f(y_i,x_2, \dots, x_n)f(y_0, \dots , \hat{y_i}, \dots, y_n)=0, $$
for all $(x_2,\dots, x_n)\in E^{n-1}$ and $(y_0, \dots, y_n) \in E^{n+1}$.
I claim that for all $(a_1,\dots, a_n)\in E^{n}$ and $(b_1, \dots, b_n) \in E^{n}$ the following formula holds:
$$\sum_{\sigma \in \mathfrak{S}_n}(-1)^{\operatorname{sgn} \sigma} \prod_{i=1}^n f(a_1, \dots a_{i-1}, b_{\sigma(i)}, a_{i+1}, \dots, a_n)=f(a_1, \dots, a_n)^{n-1} f(b_1, \dots, b_n).$$
Is it true? can you prove it?",['combinatorics']
2785107,How to find the point on an ellipse that is closest to the point A outside of the ellipse,"I am working on a project where I need a formula to find the point $B(x,y)$ on the ellipse $x^2+4y^2=r^2$ that is closest to the point $A(x_0,y_0)$, where $A$ is known and is outside the ellipse. I have been trying to use the distance formula $d=\sqrt{(x-x_0)^2+(y-y_0)^2}$ but I have been running into some problems trying differentiate. Any help would be greatly appreciated.","['derivatives', 'conic-sections', 'calculus']"
2785131,How can I compute $\mathbb P\{T_n<T_0\}$?,"Let $(X_n)_{n}$ a random walk over $\mathbb Z$ starting at $0$, i.e. $\mathbb P\{X_0=0\}=1$. I denote $T_k=\inf\{n\geq 1\mid X_n=k\}$. I suppose that $$\mathbb P\{X_{n+1}=X_n+1\mid X_n,...,X_0\}=p\quad \text{and}\quad \mathbb P\{X_{n+1}=X_n-1\mid X_n,...,X_0\}=q.$$
Remark that $q=1-p$. How can I compute $\mathbb P\{T_n<T_0\}$, i.e. the probability to touch $n$ before touching $0$ ? In fact I have problem to interpret $\{T_n<T_0\}$ using $(X_n)_n$. I tired as follow : $\mathbb P\{T_1<T_0\}=\mathbb P\{X_1=1\mid X_0=0\}=p$. $\mathbb P\{T_2<T_0\}=\mathbb P\{X_1=1\mid X_0=0\}+\mathbb P\{X_2=2\mid X_1=1\}=2p$ $\mathbb P\{T_3<T_0\}=\mathbb P\{X_1=1\mid X_0=0\}+\mathbb P\{X_2=2\mid X_1=1\}+\mathbb P\{X_3=3\mid X_2=2\}+\mathbb P\{X_3=2\mid X_2=2\}(\mathbb P\{X_4=3\mid X_3=2\}+\mathbb P\{X_4=1\mid X_3=2\})$ I know that the last one is not clear at all, but I don't know how to interpret that fact that the walker can past many time between 1 and 2 many times before arriving at $3$. Any explanation would be appreciated.","['random-walk', 'probability-theory', 'probability']"
2785132,Convergence of the random harmonic series $\sum_{n=1}^{\infty}\frac{X_{n}}{n}$,"Let $(X_{n})_{n \in \mathbb{N}}$ be independent with Rademacher distribution : \begin{equation}
\mathbb{P}(X_{n} = -1) = \frac{1}{2} = \mathbb{P}(X_{n} = 1).
\end{equation} I have to investigate \begin{equation}
\sum_{n=1}^{\infty}\frac{X_{n}}{n}
\end{equation} for convergence. It was given in a textbook and I'm very interested in the solution. It is something between the harmonic series $\sum_{n=1}^{\infty}\frac{1}{n}$ and the series $\sum_{n=1}^{\infty}\frac{(-1)^{n}}{n}$ , but I know the sign changes randomly.","['rademacher-distribution', 'independence', 'probability-theory', 'convergence-divergence', 'sequences-and-series']"
2785237,Tangent vectors on Lie Groups,"I am studying Lie groups and I am aware that we can think of a Lie group as a differentiable manifold which means we can talk about functions, curves and tangent spaces. For an abstract manifold $M$, if we have an open neigbourhood about a point $p \in M$ we can define a chart $\phi: M \rightarrow \mathbb{R}^n $. We define the tangent space at the point $p$ as the vector space of directional derivative operators to curves passing through the point $p$ which act on functions $f:M \rightarrow \mathbb{R}$. In the chart $\phi$ with coordinate functions $\{x^i\}$, I can define tangent vector to the curve $\gamma$ at $p$, given by $V$, as $$ V(f) = \frac{d}{dt}(f \circ \gamma) \bigg|_{t=0} \\
= \frac{dx^i}{dt}\bigg|_{t=0} \frac{\partial (f \circ \phi^{-1})}{\partial x^i}\bigg|_{\phi(p)} \\
= \frac{dx^i}{dt}\bigg|_{t=0} \frac{\partial}{\partial x^i}\bigg|_p f. $$ So we make the identification that the vector is given by 
$$ V = \frac{dx^i}{dt}\bigg|_{t=0} \frac{\partial}{\partial x^i}\bigg|_p. $$ Now in my studies of Lie algebras I have been introduced to the idea that the Lie algebra of a Lie group $G$ is the tangent space at the identity $T_e(G)$ equipped with a bracket. In my lectures we were told that in order to make this Lie group to Lie algebra transition, it's easiest to use matrices and we perform the mapping from $G \rightarrow GL(n,F)$ as $$ \frac{\partial}{\partial x^i} \mapsto \frac{\partial g(t)}{\partial x^i}, $$ where $g(t)$ is some curve through the matrix Lie group. I have some questions regarding this: In order to take the derivative of elements $g(t)$ of the Lie group, do we have to assume that it's also a vector space because we need the notion of addition and scalar multiplication? How do I know that this is in the tangent space of the matrix group? How do these basis vectors act on functions? i.e. above I can say that the abstract basis vectors act on functions as 
$$ \frac{\partial}{\partial x^i}\bigg|_{p} f = \frac{\partial (f \circ \phi^{-1})}{\partial x^i}\bigg|_{\phi(p)}.$$
What would $ \frac{\partial g(t)}{\partial x^i}\bigg|_p f $ be?","['differential-geometry', 'lie-algebras', 'lie-groups']"
2785266,Proving $\partial ^ 2 = 0 $ for the case of Morse-Complex with $\mathbb{Z}$ using orientation of the moduli space,"I was going through the book Morse theory and Floer homology by Audin-Damian and got stuck where they talk about defining the complex for $\mathbb{Z}$ coefficient. Assume that $a,b,c$ are critical points of index $k,k-2,k-1$. By definition 
$$\partial u = \sum_{\text{v is a critical point of 1 dimension less that a}} \eta (u,v) v$$ where $\eta (u,v)$ is the signed sum of the orientations of the elements in the moduli space of flow-lines from $u$ to $v$. I know that we can compactify the moduli space using broken-flow lines, but what is bugging me is that how does the orientation at the boundary (ie. the broken flow-lines) match up. I need some result of the sort that if $\lambda_1 $ is a flowline between $a$ and $c$ and $\lambda_2$ is a flowline between $c$ to $b$ then orientation of the boundary point ($\lambda_1,\lambda_2$) in the oriented  1-manifold with boundary $\bar {\mathcal{M}}(a,b)$ is the product of the orientations of $\lambda_1 $ and $ \lambda_2$.
How do I prove this ? I tried working it out by keeping track of the co-orientation and orientation but I am getting stuck at moving (orientation) data from the stable manifold of $c$ to the unstable manifold of $c$","['homology-cohomology', 'differential-geometry', 'differential-topology', 'morse-theory']"
2785274,Convergence in probability of conditional expectation,"Suppose I have a sequence of random variables $X_n$ and $\sigma$-fields $\mathcal{F_n} \subseteq \mathcal{F}_n'$. Suppose that $\mathbb{E}[X_n \mid \mathcal{F}_n']$ converges to a constant $c$ in probability. Does it follow that $\mathbb{E}[X_n \mid \mathcal{F}_n]$ also converges to $c$ in probability? It seems the answer should be ""yes"" since $\mathbb{E}[X_n \mid \mathcal{F}_n]$ is ""less random"" than $\mathbb{E}[X_n \mid \mathcal{F}_n']$, but I can't prove this.","['random', 'probability-theory', 'filtrations', 'conditional-expectation', 'convergence-divergence']"
2785296,Polyominoes with the most reflex exterior angles,"What is the polyomino with the largest number $n$ of reflex (i.e., $270^\circ$) exterior angles that can fit in a $L \times L$ grid? How does $n$ scale with $L$?","['combinatorics', 'polyomino', 'extremal-combinatorics']"
2785298,How to solve this differential equation in Mathematica?,"I am trying to solve a differential equation in Mathematica:
    $$y'' + 2\frac{y'}{x} + (1 - \frac{e^{-x}}{x} - \frac{l(l+1)}{x^2})y = 0$$
I have initial conditions at $x=0$ as:
    $$y(0) = a$$
    $$y'(0) = b$$
$a$ and $b$ are some known constants. But, how do I implement it, because $x=0$ will blow up in the differential equation? I tried assigning values near zero, like assigning at $x=0.00001$, but then there appears a discontinuity near $x=0$ in the function. After finding the function, I have to normalize it and have to find value at $x=0$.","['mathematica', 'ordinary-differential-equations']"
2785348,Is there a name or a reference for these aperiodic rhomboidal tilings?,"Fill space with unit cubes and then remove all cubes that are not completely within a given half space.  An isometric view of the remaining cubes will look like the following image. This is in general an aperiodic tiling of the plane by rhombi that all have the same shape.  (Of course for certain half spaces it is periodic.) My question is whether these tilings have a name, and whether they are discussed anywhere in the literature.  I've seen discussions of this in relation to 'voxelizing' planes (a generalization of Bresenham's Algorithm), but not in relation to generating or analyzing tesselations, even though I think the construction must be well known. (Interactive version of the above picture is at https://codepen.io/brainjam/full/QrZgLP/ )","['reference-request', 'tiling', 'tessellations', 'geometry']"
2785349,About a continuous function satisfying a given integral equation,"Question: Let $f(x):[0,2] \to \mathbb{R}$ be a continuous function, satisfying the equation
  $$
\int_{0}^{2} f(x)(x-f(x)) \,dx = \frac{2}{3}.
$$
  Find $2f(1)$. The solution took $f(x)=\frac {x}{2}$. Yes, I know it does not contradict the condition but how can we be sure that $f(x)=\frac{x}{2}$ and not any other function?","['real-analysis', 'integration', 'functions']"
2785355,What's the nonstandard way to argue $\lim_{n\to\infty}\sum_{k=0}^n\binom n k (\frac x n)^k = \sum_{k=0}^\infty \frac{x^k}{k!}$,"First, the equality holds, as:
$$\lim_{n\to\infty}\sum_{k=0}^n\binom n k \left(\frac x n\right)^k  =\lim_{n\to\infty}\left(\left(1+\frac{x}{n}\right)^n\right) = e^x =
 \sum_{k=0}^\infty \frac{x^k}{k!} $$ Having a limes over a series, this should be a prime target for nonstandard-reasoning, as we can just substitute in $h$ for $n$ where $h$ is an infinite hyperreal and drop the limit: $$\sum_{k=0}^h\binom h k \left(\frac x h\right)^k \tag{i}$$ 
Now all that should be left to do should be to show that this sum is infinitesimally close to $\displaystyle  \sum_{k=0}^\infty \frac{x^k}{k!}$ So, by transforming $(i)$, we get $$\displaystyle\sum_{k=0}^h\binom h k \left(\frac x h\right)^k  = 
\sum_{k=0}^h \frac{h!}{k!(h-k)!} \left(\frac x h\right)^k
= 
\sum_{k=0}^h \frac{h!}{h^k(h-k)!} \frac {x^k} {k!} $$ Now, we split the sum in two parts: The natural summands, and the (infinite) hypernatural summands. The natural summands should be infinitesimally close to our target sum , and the hypernatural summands should be infinitesimally close to 0. Here's where my argumentation turns grotesque. As far as I know, comparing infinite hyperreals is a fruitless endeavor, as their difference/ratio can be any hyperreal value. So, I have to argue using the known limits of regular sequences: $$ 
\lim_{k\to\infty} \frac {x^k}{k!} = 0  \qquad
\lim_{k\to\infty} \frac {n!}{n^k} = 0  
$$
Using these both limits, we can argue (Let $d_1,d_2$ be infinitesimals): $$
\sum_{k\in\mathbb{^*N/N}\\k\le h}^h \frac{h!}{h^k(h-k)!} \frac {x^k} {k!} \approx 
\sum_{k\in\mathbb{^*N/N}\\k\le h}^h \frac{h!}{h^k(h-k)!} d_1 \approx
\sum_{k\in\mathbb{^*N/N}\\k\le h}^h \frac{d_1 d_2}{(h-k)!}\ \approx 0 $$ The argumentation here is that every summand $a_k$ is infinitely smaller than $h$, so even the infinite sum is infinitesimally close to $0$. (Question 1: Is this argumentation correct, and can it be made easier?) And for the natural part of the sum, using $\frac {h-i}{h} \approx \frac {h}{h}\approx 1$ , for $i\in\mathbb{R}$: $$
\sum_{k\in\mathbb{N}} \frac{h!}{h^k(h-k)!} \frac {x^k} {k!} =
\sum_{k\in\mathbb{N}} \frac{(h\cdot (h-1)\cdots(h-k+1))}{h^k} \frac {x^k} {k!}
\underbrace{\approx}_{(1)}
\sum_{k\in\mathbb{N}} \frac{h^k}{h^k} \frac {x^k} {k!} = 
\sum_{k\in\mathbb{N}} \frac {x^k} {k!}$$ Note that (1) holds as there's only finitely many $\approx$-transformations, i.e. the error introduced can't sum up to a significant amount. (Question 2: Is this argumentation correct? It feels fishy) Reacting on the spotted mistakes I've tried to fix the reasoning. Instead of splitting the sum $\displaystyle\sum_{k=0}^h$ in $\displaystyle\sum_{k\in\mathbb{N}}$ and $\displaystyle\sum_{k\in\mathbb{^*N/N}}$ I'm now trying a split into these two sums: 
$\displaystyle\sum_{k=0}^{h_1}$ and $\displaystyle\sum_{k=h_1}^{h_2}$ where $h_1,h_2$ are both infinite hyperreals. Now, the responding subsets of $\mathbb{^*N}$ should be internal. The idea behind this approach is that $h_1 << h_2$, so that $\displaystyle\sum_{k=0}^{h_1}$ can take a similar role to the sum  $\displaystyle\sum_{k\in\mathbb{N}}$. Therefore, we choose $h_1$ so that $\frac{h_1}{h_2} \approx 0$. We can do this without loss of generality, as we can find such an infinite hyperreal $h_1$ for any infinite hyperreal $h_2$ (e.g. $h_1 := \lfloor\sqrt{h_2}\rfloor$). Now the argumentation translates rather analogue: $$
\sum_{k=0}^{h_2} \frac{{h_2}!}{{h_2}^k({h_2}-k)!} \frac {x^k} {k!} = \bigg(\sum_{k=0}^{h_1} \frac{{h_2}!}{{h_2}^k({h_2}-k)!} \frac {x^k} {k!}\bigg) + 
\bigg(\sum_{k=h_1}^{h_2} \frac{{h_2}!}{{h_2}^k({h_2}-k)!} \frac {x^k} {k!}\bigg)
$$ $$
\sum_{k=0}^{{h_1}} \frac{{h_2}!}{{h_2}^k({h_2}-k)!} \frac {x^k} {k!} =
\sum_{k=0}^{{h_1}} \frac{({h_2}\cdot ({h_2}-1)\cdots({h_2}-k+1))}{{h_2}^k} \frac {x^k} {k!}
=\\
\sum_{k=0}^{h_1} \frac{{h_2}^k}{{h_2}^k} \frac {x^k} {k!} +\Delta_k= 
\sum_{k\in\mathbb{N}} \frac {x^k} {k!} + \Delta_k$$ Here, $\Delta_k$ is supposed to be the difference that is defined so that the equation holds.  We now have to show $\sum_{k=0}^{h_1} \Delta_k \approx 0$: $$|\Delta_k| \underbrace{\le}_{\#1} \frac 2 {h_2} \cdot \frac {x^k}{k!} \underbrace{\le}_{\#2} \frac 2 {h_2} \implies \sum_{k=0}^{h_1} |\Delta_k| \le\frac{(h_1+1)\cdot 2} {h_2} \approx 0$$ #1: This one is a long calculation with a few estimations, but it should be rather clear. #2: This inequality holds for almost all $k$, as $\frac{x^k}{k!}$ tends to 0. And therefore, $\sum_{k=0}^{h_1} \Delta_k \approx 0$ holds. Finally, for the sum $\displaystyle \sum_{k=h_1}^{h_2} \frac{{h_2}!}{{h_2}^k({h_2}-k)!} \frac {x^k} {k!}$ we again use analogous arguments: $$
\sum_{k=h_1}^{h_2} \frac{{h_2}!}{{h_2}^k({h_2}-k)!} \frac {x^k} {k!} \approx
\sum_{k=h_1}^{h_2} \frac{d_1 \cdot {h_2}!}{{h_2}^k({h_2}-k)!} \approx
\sum_{k=h_1}^{h_2} \frac{d_1 d_2}{({h_2}-k)!} =\\
d_1 d_2 \sum_{k=h_1}^{h_2} \frac{1}{({h_2}-k)!} =
d_1 d_2 \sum_{k=0}^{h_2-h_1} \frac{1}{k!} \approx d_1 d_2 e \approx 0
$$","['nonstandard-analysis', 'sequences-and-series', 'proof-verification']"
2785392,Is every strongly continuous semi-group uniformly continuous?,"I think the answer is YES. Yes, I thought of an alleged demonstration like this: How, by hypothesis $\{T(t)\: ; \: t\geq 0\}$ is the $C_0$ semigroups,in $X$, (where $X=(X,|\cdot|)$ is a Banach space) then, by definition, we have
$$\lim_{t\rightarrow 0}T(t)x=x,\: \forall \: x\in X,$$
that is, given any $\epsilon>0$, exists $\delta>0$ such that if $t\geq0$ and $|t|<\delta$ then $$ |T(t)x-x|<\epsilon,\: \forall\:x\in X. $$ In particular, for all $x\in X$ such that $|x|=1$, we have
$$ |T(t)x-x|<\epsilon \: \Rightarrow \: \sup_{\stackrel{\scriptstyle x\in X}{|x|=1}} |T(t)x-x|  <\epsilon  $$
but,
$$ \|T(t)-I\|=\sup_{\stackrel{\scriptstyle x\in X}{|x|=1}} |T(t)x-x| <\epsilon ,\: \forall \: \epsilon>0,\;|t|<\delta ,$$ 
where $I$ is identity operator, in $X$. Therefore, $\displaystyle \lim_{t\rightarrow 0} \|T(t)-I\|=0;$ that is, $\{T(t)\: ; \: t\geq 0\}$ is a uniformly continuous semigroup. Is that correct?","['functional-analysis', 'semigroup-of-operators']"
2785438,Schemes and locally ringed spaces,"I've been reading Vakil's notes on Algebraic Geometry on locally ringed spaces and there's a part that confuses me. There's an exercise on page 135 that says that says that the stalk of an affine scheme Spec$R$ at $p$ is the local ring $R_p$. This is clear. What is not, is that a few lines later Vakil says that this shows that all schemes are locally ringed spaces. Could you please help me see it? Perhaps this makes more clear what it is that confuses me. If $p \in X$ has two different affine neighborhoods then what would be the maximal ideal of the stalk?","['schemes', 'algebraic-geometry']"
2785485,Applying Sherman-Morrison-Woodbury to obtain rank 2 update,"I was reading Nocedal and Wright, and it is stated that one may use the Sherman-Morrison-Woodbury formula 
$$(A+ YGZ^*)^{-1} = A^{-1} - A^{-1}Y(G^{-1}+Z^*A^{-1}Y)^{-1}Z^*A^{-1}$$ On the hessian $H_{k+1}$ inverse approximation from the BFGS:
 $$ H_{k+1}=  (I - \gamma s_k y_k ^T)H_k(I-\gamma_k y_k s_k^T) + \gamma_k s_k s_k^T$$ To obtain an update formula for the Hessian $B_{k+1}$ for the BFGS: $$B_{k+1} = B_k - \frac{B_ks_ks_k^T B_k}{s_k^TB_k s_k } + \gamma_k y_ky_k^T $$ Where the setting here is $\gamma_k = \frac{1}{y_k^Ts_k}$ and $H_k = B_k^{-1}$ and $H_k$ and $B_k$ are positive definite symmetric matrices. My issue is that I do not see how to apply the Sherman-Morrison-Woodbury formula.
What I see is that I want 
$H_{k+1} = A+ YGZ^*$ and that would give me $B_{k+1}$ Then the question is what is $A$, $Y$, $G$, and $Z$. My thoughts are that $A \not = \gamma_k y_ky_k^T$ as that is singular. Thus that leaves $A = (I - \gamma s_k y_k ^T)H_k(I-\gamma_k y_k s_k^T)$, but that is a mess to invert and when I tried it I did not get anything resembling the desired expression and also then what do I define $G,Y,Z$ to be?","['matrices', 'linear-algebra', 'optimization']"
2785498,Finding the limit of the integral,"Let $f$ be continuous in [0, 1]. Assume $$0 < a < b$$ Prove that the following limit exists and determine what it is: $$\lim_{\delta\rightarrow 0}\int_{\delta a}^{\delta b} \frac{f(x)}{x} dx$$ I got stuck on this question and I can't seem to figure it out. It seems to me that there's no reason for this limit to always exist. For instance, if I take $$f(x) = 1$$ The integral of the harmonic function diverges near 0, so by Cauchy's critrion, the limit shouldn't exist. So what am I missing? Does anyone have an idea?","['real-analysis', 'riemann-integration', 'limits', 'calculus', 'integration']"
2785513,Non-existence of weak solution in one dimension,"Let $\Omega=(1,\infty).$ Then for any given $f\in L^2(\Omega),$ the equation 
$$
-u''=f\,\,\text{in}\,\,\Omega,
$$
does not admit any weak solution in $W_{0}^{1,2}(\Omega).$ I tried the solution by contradictory argument. Indeed, suppose such a weak solution exists, say $u.$ Then for every $\phi\in H_0^{1}(\Omega),$ we have
$$
\int_{\Omega}\,u'\phi'\,dx=\int_{\Omega}f\phi\,dx
$$
I tried to construct $\phi_n\in C_c^{\infty}(\Omega)$ such that the above inequality become false. I think this idea will work, but I am still unable to construct such $\phi.$ If anyone can help me regarding this or with some other idea on solving this problem, it will be very grateful for me. Thanks.","['linear-pde', 'regularity-theory-of-pdes', 'analysis', 'partial-differential-equations']"
2785515,"Can we find all solutions to the equation $\frac{\phi(n)}{\phi(n-1)}=5$, where $\phi(n)$ denotes the totient function?","The solutions of the equation $$\frac{\phi(n)}{\phi(n-1)}=5$$ upto $n=10^8$ , where $\phi(n)$ denotes the totient function, are : ? for(n=2,10^8,if(eulerphi(n)/eulerphi(n-1)==5,print(n,""  "",factor(n-1),""   "",fa
ctor(n))))
11242771  [2, 1; 3, 1; 5, 1; 7, 1; 11, 1; 31, 1; 157, 1]   [1171, 1; 9601, 1]
18673201  [2, 4; 3, 3; 5, 2; 7, 1; 13, 1; 19, 1]   [2161, 1; 8641, 1]
77805001  [2, 3; 3, 2; 5, 4; 7, 1; 13, 1; 19, 1]   [1801, 1; 43201, 1]
? In the two last solutions, the two $n-1$ -numbers share the same prime factors, but the first solution is completely different with a squarefree $n-1$ -number. Can we somehow classify all the solutions of this equation ? What, if we replace $5$ by another positive integer ?","['number-theory', 'divisibility', 'totient-function', 'elementary-number-theory']"
2785551,Keep a signal that depends on an external input in some interval without using two cases or the max function,"This problem regards keeping a signal x between the bounds of the interval $[D, B]$, by specifying the derivative of $x$ with respect to time as a function of some external input, say a real between $-1$ and $1.$ For example, if there were no bounds, this equation could be $dx/dt = \text{input}.$ When bounds $D$ and $B$ are present, the equation is: $\dfrac{dx}{dt} = (B - x)*\max(0, \text{input}) + (D - x)*\max(0, -\text{input})$ Obviously, what this achieves is that when input is positive, $x$ grows towards $B$ and becomes saturated close to it, and if it was larger than $B,$ it gets lowered back. Vice versa for $D.$ My questions is whether a similar behavior (boundaries, saturation, proportional correction when $x$ is beyond boundaries) can be achieved without splitting the input signal into a positive and negative case using the max function, that is, using only basic algebraic operations? For example, at first I incorrectly thought that a second-degree polynomial in the form $-(x - B)(x - D)*\text{input}$ may be used, because it has roots $D$ and $B$ between which it is positive, so the signal $x$ would increase for a positive input, and decrease for a negative input, and also if $B>x$ a positive signal would knock it back; but this falls apart once a negative signal is given when $x$ is over $B,$ or a positive signal is given when $x<D.$ I gave this example just to illustrate what could be an acceptable alternative that wouldn't use two cases and max--the appearance of the function could change however as long as those essential properties of controlling $x$ remain.","['derivatives', 'signal-processing', 'functions']"
2785558,Trace of positive semidefinite matrix,Let $A =X +B$ with $X= (X_1+X_2) $all three semidefinite positive and B definite positive.  How can i proove that $$ trace A^{-1}X \leq trace( (X_1+ B)^{-1}X_1  +(X_2 +B)^{-1}X_2)$$,"['inequality', 'matrices', 'positive-semidefinite', 'trace', 'linear-algebra']"
2785603,Is this a known formula? $ \prod_{k=0}^n \left(1 - \frac{a_k}{N}\right)$,"I try to quantify a partition, are there any known indicators/caracteristic numbers? Something which came to my mind was
$$ \prod_{k=0}^n \left(1 - \frac{a_k}{N}\right), $$ with the following condition $$ \sum_{k=0}^n a_k = N .$$ Is this a known formula? I'd like to have an indicator which tells me if the partition is well spread or concentrated on some few numbers.
I hope my question is clear as I know not a lot about partitions. Thank you for your help. Edit: If $a_0=N$ and all others $a_k$ are 0 this formula gives 0. If all $a_k$ are 1 and $n=N$ tends to infinity this formula goes to $\frac{1}{e}$. So I am wondering too if $\frac{1}{e}$ is the upper bound for a finite $N$ for all partitions. Edit2: Thanks a lot for different proofs that $\frac{1}{e}$ is the upper bound.
I still like to know if someone knows something more about this formula. If someone has an interessing fact, that would be nice.","['number-theory', 'probability']"
2785608,show integrability of random variable,"Let $X_1,X_2,\ldots$ be iid random variable with $E[X_i]=0$ and $P(X_i>0)>0$. Let $S_0=x>0$ and define $S_n=S_0+\sum_{i=1}^nX_i$. Then define $T=\inf\{n \geq 0: S_n \leq 0 \text{ or } S_n \geq b\}$. I have shown that $T$ has finite expectation, and am trying to show $X_T$ is integrable. Any hint or help would be great!","['martingales', 'probability-theory', 'probability', 'measure-theory']"
2785622,"How to prove that generating function of series ($1 ,\binom{m}{1},\binom{m+1}{2},\binom{m+2}{3},...$ ) equals $\frac{1}{(1-z)^m}$ .","How to prove that generating function of series ($1 ,\binom{m}{1},\binom{m+1}{2},\binom{m+2}{3},...$ ) equals $\frac{1}{(1-z)^m}$ . How do I do this? I know that I have to use some differentiation somewhere  but what series do I use to start?","['generating-functions', 'combinatorics', 'sequences-and-series', 'discrete-mathematics']"
2785638,Probability of $\lim \sup$ of intersection,"I'm given two sequences of events $\{A_n\}$ and $\{B_n\}$ with $
\mathbb{P}(\liminf_n A_n) = \mathbb{P}(\limsup_n B_n) = 1$ . Can we deduce that $\mathbb{P}(\limsup_n A_n \cap B_n) = 1$ ? My attempt was to write $$
\mathbb{P}(\liminf_n A_n) = 1 \Rightarrow
\mathbb{P}\left( \left\{ w : \sum_n 1_{A_n}(w) \text{ finite} \right\} \right) = 0
$$ Additionally, we know that $$
\mathbb{P}(\limsup_n (A_n \cap B_n)) = \mathbb{P}\left(\left\{w : \sum_{n=1}^{\infty} 1_{A_n}(w) \cdot
1_{B_n}(w) =\infty \right\}\right) \\=
\mathbb{P}\left(
\left\{ w : \sum_n 1_{A_n}(w) = \infty, \sum_n 1_{A_n}(w) \cdot 1_{B_n}(w) =
\infty
\right\} \cup \left\{
w : \sum_n 1_{A_n}(w) < \infty, \sum_n 1_{A_n}(w) \cdot 1_{B_n}(w) = \infty
\right\}
\right) \\ = 
\mathbb{P}\left(\left\{
   w: \sum_{k=1}^{\infty} 1_{B_{n_k}}(w) = \infty
\right\}\right) + 0 = \mathbb{P}(\limsup_n B_n) = 1
$$ where in the above we made use of the fact that since $w \in \liminf_n A_n$ , the terms in the sum $\sum_n 1_{A_n} 1_{B_n}(w)$ have $1_{A_n} = 0$ for only a finite set of indices. Is that correct, or am I missing something? Additionally, does this hold when I change the condition of $\mathbb{P}(\liminf_n A_n) = 1$ to $\mathbb{P}(\limsup_n A_n) = 1$ ? I've been trying to find a counterexample to no avail.","['limsup-and-liminf', 'probability', 'measure-theory', 'proof-verification']"
2785663,Proving that a random walk that diverges to infinity may not become negative,"Consider a random walk $S_n= \sum_{k=1}^n X_k$, where $\{X_k\}_{k=1}^\infty$ are independent and identically distributed random variables. Assume that $S_n \rightarrow \infty$ almost surely as $n \rightarrow \infty$. Let $$\tau = \inf\{n \geq 1: S_n \leq 0\}.$$ In the book ""Stopped Random Walks - Limit Theorems and Applications"" by Allan Gut I found a theorem stating that under the assumptions above $\tau$ is defective, i.e., $$\mathbb{P}(\tau = \infty)>0.$$ However, no proof for the theorem is provided. Could anyone provide any hints on how to prove this theorem? Also, does anyone know if there are any generalizations of the theorem for the case that $\{X_k\}_{k=1}^\infty$ are not i.i.d.? Thank you for your time!","['stochastic-processes', 'probability-theory', 'random-walk', 'stopping-times']"
2785666,Uniqueness of solutions of $y''+y=0$,Recently I was asked about the uniqueness of the solutions of the equation $y'=y$. It could be obtained by multiply $e^{-x}$. This time I was wondering about the same question regarding a different equation. We know that the solutions of $y''+y=0$ are of the form $a\sin(x)+b\cos(x) $. How can we explain that these are the ONLY solutions?,"['ordinary-differential-equations', 'calculus']"
2785744,$\sum_{k=1}^{n} \frac{1}{n+k} < \ln 2$,"$\sum_{k=1}^{n} \frac{1}{n+k}  < \ln 2$ Here are some proofs for this inequality but I have another approach. Can anyone verify my proof? Thanks. $$\sum_{k=1}^{n} \frac{1}{n+k}< \int_0^n \frac{1}{n+x}dx=\ln (n+x)\mid ^n_0 = \ln(2n)-\ln(n) = \ln 2.$$ Does the first inequality of this proof hold? In general, what's the relationship between a series and its integral form? Is the series always less than its integral form? Is it the same for a finite summation?","['inequality', 'integral-inequality', 'calculus', 'integration', 'solution-verification']"
2785769,"Proof of ""every finite dimensional vector space has a finite basis""","Finite dimensional implies the existence of a finite set that spans the vector space. Let V be one such vector space and let S be a finite set that spans V. The text I am following has a theorem that Theorem 1: Any minimal spanning set of V is a basis of V. Since we have a spanning set to begin with, we can keep on removing the linearly dependent vectors till we are left with a minimal spanning set which should then be a basis for that vector space. However, the text I am following has the following theorem. Theorem 2: Every linearly independent list of vectors in a finite-dimensional vector space can be extended to a basis of the vector space Then the text mentions the following corollary Corollary to Theorem 2: Every finite dimensional vector space has a finite basis The proof is not given for the corollary. Is it really that straight forward? Does it involve something like the empty set of basis vectors, which by definition, is the basis of the set {0}, can be extended to a basis of V? That would then imply V has a basis. I feel that something is missing. All we know up until this point is the if a basis exists, then it is a minimal spanning set, maximal linearly independent set, and that any two sets basis vectors must have the same number of elements (which is where motivation to define dimension will start to emerge). We have not yet shown that a finite dimensional vector space has a basis and hence, we cannot assume that V has a finite basis. So my question is how can we prove Theorem 2 without referring to any finite list of basis of V? Line of proof for Theorem 2 given in the text: Let W be a subspace of V with basis vectors $\{w_1,w_2,...,w_k\}$ . Choose a vector $v_{k+1}$ from V-W. Then the set $\{w_1,w_2,...,w_k,v_{k+1}\}$ is linearly independent. Let the span of this new set be $W_1$ . Then choose any vector from V- $W_1$ , say $v_{k+2}$ , and add it to the set of linearly independent vectors to get the new set $\{w_1,w_2,...,w_k,v_{k+1},v_{k+2}\}$ . We can keep on going like this but
can the process go on forever? This is where the text simply mentions that this process has to terminate because ""the vector space is finite dimensional."" To me, this is the statement that does not make sense. All we know is There is a finite set of vectors, say S, which spans V, and we know that There is a subset W of V with some basis, say $\{w_1,w_2,...,w_k\}$ . How can we use just the above facts (and maybe also some of the aforementioned theorems about basis vectors if they existed) to prove Theorem 2? I would greatly appreciate feedback to the above query.",['linear-algebra']
2785782,Existence of Complementary Subtorus,"Suppose that $T$ is a closed subtorus (compact, connected, abelian subgroup) of $\mathbb{T}^d:=\mathbb{R}^d/\mathbb{Z}^d$. Is there a closed subtorus $T^\perp$ of $\mathbb{T}^d$ such that $\mathbb{T}^d=T^\perp \oplus T$? To elaborate more, I know that $T$ would then be isomorphic as a Lie group to $\mathbb{T}^k$ for some $k$, hence is a divisible group. Therefore, I know that $T$ is a summand in $\mathbb{T}^d$. So my question is whether or not the complementary summand is also a closed Lie subgroup. Thanks in advance for the help!","['abstract-algebra', 'topological-groups', 'group-theory', 'lie-groups']"
2785804,Bayesian and frequency tail estimation.,"The tail probability can be estimated by two methods: In Bayesian method:
$$P_B(X>a)=\int^{\infty}_{-\infty}\pi(\theta|x)[1-F(a|\theta)]d\theta$$ In Plug-in frequency method:
$$P_F(X>a)=1-F(a|\hat{\theta})$$ 
where $\hat{\theta}$ is the MLE of $\theta$. The numerical results show that it's always $$P_B \geq P_F$$
no matter what the distribution is. Any ideas or any resources related to this topic to explain why is that? Many thanks~","['bayesian', 'statistics', 'bayes-theorem', 'statistical-inference']"
2785856,"If $\frac{\partial z}{\partial x}=\frac{\partial z}{\partial y}$, then $z\left(x,y\right)$ must be differentiable?","$z=z\left(x,y\right)$ is defined on region $D$, $\frac{\partial z}{\partial x}$ and $\frac{\partial z}{\partial y}$ both exists, and $\frac{\partial z}{\partial x}=\frac{\partial z}{\partial y}$ for all $\left(x,y\right)\in D$. If $z$ is differentiable we can get $z=f\left(x+y\right)$, i.e. z is only dependent on $x+y$. But what if $z$ is not differentiable? Do there exist some cases when $z$ is not differentiable? I guess that $z\left(x,y\right)$ must be differentiable if the conditions above are true, but anyway I cannot prove it. It's probable that I made a wrong guess. Now it remains a problem: if $\frac{\partial z}{\partial x}=\frac{\partial z}{\partial y}$ for all $\left(x,y\right)\in D$ and $z\left(x,x\right)\equiv 0$, then $z\left(x,y\right)\equiv 0$? I think it would be true. But without given ""$z$ is differentiable"", it is hard to prove.","['multivariable-calculus', 'real-analysis', 'analysis']"
2785869,Commutability transformation [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Suppose that V is a finite dimensional vector space and $f\in {\rm End}\ (V)$ is diagonalizable with ${\rm dim}\ V$ distint eigenvalues. Show for
$Z(f):=\lbrace g\in {\rm End}\ (V)| fg=gf\rbrace$ we have ${\rm dim}\ Z(f)={\rm dim }\ V$","['linear-algebra', 'linear-transformations']"
2785882,Discontinuous function with bounded variation,"Can $\gamma:[0,1]\rightarrow\mathbb{C}$ be discontinuous and have bounded variation ? The discontinuation is making it difficult for me to calculate the variation of any function I can think of. Any suggestions?","['continuity', 'complex-analysis', 'bounded-variation']"
2785893,Rep-tiles of order 4 and order 9,"Rep-tiles are figures which are dissected by the same figures as itself. As you can see, the rep-tile of order 4 is also a rep-tile of order 9 in the above figures: Compare the L-shaped figure at row 2 column 1 and one at row 3 and column 1, and compare two trapezoids at row 1 and row 3. As I know, Golomb found that a few rep-tiles of order 9 which are not of order 4. But, those examples are not simple closed curves. So, my questions: (1) Is any rep-tile of order 4 always of order 9? (2) If a rep-tile of order 9 is a simple close curve, is it also of order 4?","['recreational-mathematics', 'tiling', 'geometry']"
2785909,Solve for $x$ if $\tan^{-1}{\frac{\sqrt{1+x^2}-1}{x}}=\frac{\pi}{45}$,"It given that $$\tan^{-1}{\frac{\sqrt{1+x^2}-1}{x}}=\frac{\pi}{45}$$
Solve for $x$ So I am pretty new to inverse trigonometric functions so I don't really know what to do over here. Please help",['trigonometry']
2785912,Is there a proof for $\lim_{x \to a} \frac{1}{x-a} = \infty$?,"I am an adult software developer who is trying to do a math reboot. I am working through the exercises in the following book. Ayres, Frank , Jr. and Elliott Mendelson. 2013. Schaum's Outlines Calculus Sixth Edition (1,105 fully solved problems, 30 problem-solving videos online) . New York: McGraw Hill. ISBN 978-0-07-179553-1. So far as I can tell, the following question either has a misprint or the book does not cover the material.  It is entirely possible that I failed to grasp a key important sentence. Chapter 7 Limits, problem 24. Use the precise definition to prove: $$
\text{a)  }\lim_{x \to 0} \frac{1}{x} = \infty \\
\text{b)  }\lim_{x \to 1} \frac{x}{x-1} = \infty \\
$$ My understanding. It is possible to prove $\lim_{x \to 0^+} \frac{1}{x} = +\infty$ or $\lim_{x \to 0^-} \frac{1}{x} = -\infty$, but not $\lim_{x \to 0} \frac{1}{x} = \infty$ because $\frac{1}{x}$ is a hyperbola with no limit at 0.  A similar argument can be made for $\frac{x}{x-1}$ at 1. Is there a proof for $\lim_{x \to a} \frac{1}{x-a} = \infty$?","['calculus', 'limits']"
2785917,Smallest variety containing $\mathbb{Z}$,"I've been told that the answer is all abelian groups, but I don't see how. I know that the class of all nilpotent groups of degree 1 is a group variety and that a group being nilpotent of degree 1 is equivalent to that group being abelian so the class of all abeliani groups is indeed a variety.  I also know that the integers are a group when considered with addition as the operation and that a class of algebraic structures of the same signature is a variety if and only if it is closed under the taking of homomorphic images, subalgebras and (direct) products, but I'm not quite sure how to use this here or if this is even the way to go about this. I think that the main structure of this proof should be to show that $\mathbb{Z}$ is contained inside the variety of all abelian groups and then show that its the smallest one that could possibly contain $\mathbb{Z}$.  I know that in the class of abelian groups, the language is $(+, 0, \frac{1}{})$ and the identities are 
$$(x+y)+z = x + (y+z),$$ 
$$x+y = y+x,$$ 
$$x + 0=x,$$ 
$$x+(-x) = 0,$$ 
all of which obviously hold on $\mathbb{Z}$.  Is that enough to say that the class of abelian groups contains $\mathbb{Z}$? How would I show nothing smaller could contain $\mathbb{Z}$?","['universal-algebra', 'abstract-algebra']"
2785944,Do oriented null cobordant manifolds admit spin structures?,Let $M$ be an oriented null cobordant manifold. Since $M$ is oriented its first Stiefel-Whitney class vanishes. Since $M$ is null cobordant all of its Stiefel-Whitney numbers vanish. Is it known if this implies that the second Stiefel-Whitney class vanishes so that $M$ admits a spin structure? Or are there known counterexamples?,"['spin-geometry', 'differential-topology', 'algebraic-topology', 'geometric-topology', 'differential-geometry']"
2786026,convergence in distribution to a non standard random variable,"Let $(X_{n})_{n \in \mathbb{N}}$ be independent random variables with \begin{equation}
\mathbb{P}(X_{n} = -n^{3/2}) = \mathbb{P}(X_{n} = n^{3/2}) = \frac{1}{2n},~ \mathbb{P}(X_{n} = 0) = 1 - \frac{1}{n}.
\end{equation} Let $S_{n} = \sum_{k=1}^{n}X_{k}$. It is to show that \begin{equation}
\frac{S_{n}}{\sqrt{var(S_{n})}} \xrightarrow d S,
\end{equation}
for $n \to\infty$ where $S$ is an non standard random variable. Now the professor give us a solution but it is about 2 pages long. I understand the proof. Is there a shorter proof?","['weak-convergence', 'probability-theory', 'probability']"
2786052,Expected maximum pairwise distance for $n$ points on a circle?,"Place $n$ points uniformly at random on a circle of circumference $1$. What is the expected maximum distance between any pair $x_i$, $x_j$ of those points? I'm defining distance as distance on the circle, i.e., the length of the smallest path from point $X$ to point $Y$ which does not leave the circle.","['order-statistics', 'probability', 'geometric-probability', 'expected-value']"
2786054,Farm auction distribution,"At a farm auction, various pieces of farm equipment and livestock are bid on and sold. Today, there are $7$ distinct items up for auction, and there are $3$ farmers bidding on them. One of the farmers declares that he will bid on at least $2$ of the items (not specifying which ones), and he won't be outbid. If this farmer is telling the truth, then how many ways can the items be distributed among farmers? Assume that all items are sold. My solution : $$\binom{7}{5} \cdot 3^{5}$$ Choose $5$ out of $7$ items (the other two stay with the farmer) and multiply by the number of ways of distributing $5$ distinct items to $3$ distinct people. It's wrong and I am over counting, but I don't understand how?",['combinatorics']
2786063,Distinguishing independent Bernoulli trials from a 2-state Markov Chain,"Given is a sequence of zeros and ones $x=(x_1,\dots,x_n)$ that could have been be drawn in one of two ways: 1) as success indicators in Bernoulli experiments with probability of success p (e.g. by tossing a biased coin), or 2) as sequence of states (0 or 1) in a random realisation of a Markov chain with {a transition probabilities matrix $\mathbf{P}.$ The challenge is to determine which way the given data were actually produced and how to estimate the corresponding parameters $p$ of $\mathbf{P}.$","['statistics', 'probability']"
2786122,Martingale ganerated by random walk,"Let $(\Omega,\mathcal{F},P)$ be a probability space. Moreover let $\tau_x\colon \Omega \to \Omega$ for $x \in \mathbb{Z}$ be an ergodic group of tranformations that preserves $P$. By ergodic we mean that, up to sets of measure $0$, the only invariant measurable sets under the action of $(\tau_x)_{x \in \mathbb{Z}}$ are $\emptyset$ and $\Omega$. Let $p_1,p_{-1}\colon \Omega \to [0,+\infty)$, $x \in \mathbb{Z}$ be two bounded and nonnegative functions such that
$$
p_1(\omega) + p_{-1}(\omega) = p_{-1}(\tau_1 \omega) + p_1(\tau_{-1} \omega)
$$
for almost every $\omega$. Assume that $X(t)$ is a continuous time Markov process with state space $\mathbb{Z}$. We define now a random walk $X_t^\omega$, $t \geq 0$ (on some other probability space) with jump rate from $y$ to $y+1$ (resp. from $y$ to $y-1$)
$$
p(y,y+1,\omega) := p_1(\omega) \quad \text{resp}.\ p(y,y-1,\omega) := p_{-1}(\omega)
$$
for $y \in \mathbb{Z}$. Finally, we introduce environment process
$$
\eta(t) := \tau_{X_t^\omega}(\omega).
$$
It is not so hard to prove that $\eta$ is a Markov process. Now we can write
$$
X_t^\omega = N_1(t) - N_{-1}(t),
$$
where $N_1(t)$ (resp. $N_{-1}(t))$ is the number of times that the process $\eta(t)$ jumped from a state $\eta$ to $\tau_1 \eta$ (resp. $\tau_{-1} \eta$) in time interval $[0,t]$. I would like to show that
  $$ m_1(t) := N_1(t) - \int_0^t p_1(\eta(s)) ds
$$
  is a martingale (and similarly for $m_{-1}$) with respect to
  natural filtration. Could you give me any hints how I should do this?","['stochastic-processes', 'random-walk', 'probability-theory', 'markov-chains', 'martingales']"
2786138,Solve in prime numbers the equation $p^q+q^r=r^p$,"Find all triples of prime numbers $(p,q,r)$ such that $$p^q+q^r=r^p.$$ I proved that when $r=2$ , the equation becomes $$p^q+q^2=2^p.$$ Then I tried to use reciprocity laws and Fermat's little theorem. I could prove that $p\equiv 7\pmod 8$ and that $p>q$ . The equation appeared in some olympiad . They asked to prove that $r=2$ . So I am trying to find at least one triple.","['diophantine-equations', 'number-theory', 'contest-math', 'prime-numbers', 'elementary-number-theory']"
2786144,If $B \subset \mathbb R^m$ is compact and $x\in \mathbb R^n$ then $\{x\} \times B \subset \mathbb R^{n+m}$ is compact.,If $B \subset \mathbb R^m$ is compact and $x\in \mathbb R^n$ then $\{x\} \times B \subset \mathbb R^{n+m}$ is compact. If $ \mathcal O$ is open cover of  $\{x\} \times B \subset \mathbb R^{n+m}$ then as $B$ is compact there is finite cover $ \mathcal O'$ of $B$ and hence finite open sets $U$ s.t. $ \{x\} \times U' \subset U \in \mathcal O $ where $U' \in \mathcal O'$. Is this argument correct?,"['multivariable-calculus', 'real-analysis', 'compactness']"
2786151,Rank of Laplacian matrix,"Let $ L $ be a Laplacian matrix of a balanced and strongly connected digraph having $n $ nodes.
$ L[r]$ is a submatrix of $L$ which is obtained  by deleting $rth$ row and $rth$ column of Laplacian matrix $L$. It is observed for any $r$ the rank of $ L[r]$ is full$(n-1)$. How can I prove this fact?
 Thanks in advance for your suggestions.","['matrices', 'graph-theory', 'matrix-rank', 'algebraic-graph-theory']"
2786168,Example of series such that $\sum a_n$ is divergent but $\sum \frac{ a_n}{1+ n a_n}$ is convergent,Example of series such that $\sum a_n$ is divergent but $\sum \frac{ a_n}{1+ n a_n}$ is convergent. I got one example from online that $a_n =\frac {1}{n^2} $for n is non square term and  $a_n =\frac {1}{\sqrt n} $ for $n$ is square term . I know that $\sum a_n$ is divergent term but not convience with  convergence of  $ \sum \frac{ a_n}{1+ na_n}$. Any help will be appreciated.,"['real-analysis', 'examples-counterexamples', 'sequences-and-series', 'analysis']"
2786212,How do I prove that $\sum_{i=1}^m \cos^2\left(\frac{2\pi i}{m}\right) = \sum_{i=1}^m \sin^2\left(\frac{2\pi i}{m}\right) = \frac{m}{2}$? [duplicate],"This question already has answers here : How can we sum up $\sin$ and $\cos$ series when the angles are in arithmetic progression? (8 answers) Closed 6 years ago . I haven't had any good ideas nor found any helpful identities so far, so I'd appreciate some help. Also, here $m > 2$. Update: Thanks to the hints and to this previous post I managed to get to the conclusion I wanted. Thanks guys.","['complex-numbers', 'trigonometry', 'calculus', 'summation', 'sequences-and-series']"
2786216,"If $(X_n)$ are i.i.d random variables, then $P(X_1\geq 0)=1$ and $P(X_1 >0)>0$ implies $\sum X_n =\infty$","Let $(X_n)$ a sequence of i.i.d random variables, such that $P(X_1 \geq 0)=1$ and $P(X_1 >0) > 0$. Show that $\sum_{n \geq 1}X_n =\infty$ almost surely. We have that $P(X_1 > 0)= \underset{n \rightarrow \infty}{\lim}P(X_1 > \frac{1}{n})$. So by the property of the limit, there exists a sufficiently big $N \in \mathbb{N}$ such that $\forall n \geq N$ we have $P(X_1 > \frac{1}{n})>0$. Let then put $A_n = \{X_1 \geq \frac{1}{N} \}$. We then have: $$\sum_{n \geq 1} X_n \geq \sum P(A_n) \geq M + \sum_{n\geq N} P(X> \frac{1}{N}) = M + P(X>\frac{1}{N})\sum_{n\geq N}1$$
Which equals to infinity. Is my reasoning correct? I am very unsure, as $\sum_{n\geq 1}X_n$ is ambiguous to me, I am unsure what it exactly represents.","['probability-theory', 'summation', 'probability', 'sequences-and-series']"
2786220,"Calculate an abstract algebra ""sum"" $\frac 12 * \frac13 *...*\frac 1{1000}$.","Let $x*y= \frac {x+y}{1+xy}$, $x,y\in(-1,1)$. Calculate $\frac 12 * \frac13 *...*\frac 1{1000}$. My attempt: First I tried to find some inductive formula but I get something like this: $\frac 12*\frac 13=\frac 57$ $\frac 57*\frac 14=\frac 9{11}$ $\frac 9{11}*\frac 15=\frac 78$ didn't got anywhere... Then I thought maybe if I let $G=(-1,1)$ then $(G,*)$ is an abelian group. That means I have to find another group that can be an isomorphism with this one and try to calculate using that composition. So I found that: 
$$f:(0,\infty)\to (-1,1),f(x)=\frac {x-1}{x+1}$$ is an isomorphism with $G$ from $(\mathbb{R},\times)$ to $(G,*).$ Then: $f(xy)=f(x)*f(y)$. So: $$f^{-1}:(-1,1)\to(0,\infty), f^{-1}(x)=-\frac {x+1}{x-1}.$$ Is is true that $f^{-1}(x*y)=f^{-1}(x)f^{-1}(y)?$ If for $f$ happens this then it happens for $f^{-1}$ too? If this is true then I think I solved my exercise because: $$f^{-1}\left(\frac 12*\frac 13*\ldots*\frac 1{1000}\right)=f\left(\frac {1}{2}\right) f\left(\frac{1}{3}\right)\ldots f\left(\frac{1}{1000}\right)= \frac {\frac 32}{-\frac 12}\frac {\frac 43}{-\frac 23}...\frac {\frac {1000}{999}}{-\frac {998}{999}}\frac {\frac {1001}{1000}}{-\frac {999}{1000}}=500\cdot 1001=500500.$$ then $\frac 12*\frac 13*...*\frac 1{1000}=f(500500)=\frac {500499}{500501}$",['abstract-algebra']
2786268,Measurability of piecewise defined function on a product space,"Let $(\Omega,\mathcal A)$ and $(X,\mathcal X)$ be measurable spaces $g_n:\Omega\times X\to\mathbb R$ be $\mathcal A\otimes\mathcal X$-measurable for $n\in\mathbb N$ Assume that for all $x\in X$, there is a $N_x\in\mathcal A$ such that $(g_n(\omega,x))_{n\in\mathbb N}$ is convergent for all $\omega\in\Omega\setminus N_x$. Now, let $$g(\omega,x):=\begin{cases}\displaystyle\lim_{n\to\infty}g_n(\omega,x)&&\text{, if }\omega\in\Omega\setminus N_x\\0&&\text{, otherwise}\end{cases}$$ for $(\omega,x)\in\Omega\times X$. How can we conclude that $g$ is $\mathcal A\otimes\mathcal X$-measurable? My problem with this task is the dependence of $N_x$ on $x$. How do we need to argue?","['product-space', 'real-analysis', 'measure-theory']"
2786270,Proof for the pointwise limit of functions with bounded derivatives being Lipschitz,"Let $f_n\colon \mathbb{R} \rightarrow \mathbb{R}$ be differentiable for
 each $n \in \mathbb{N}$ with $\lvert f'_n(x)\rvert \le 1$ for all $n$ and $x$.
 Let $\lim_{n \rightarrow \infty} f_n(x) = g(x)$ for all $x$. Prove
 that $g\colon \mathbb{R} \rightarrow \mathbb{R}$ is Lipschitz-continuous. I know one can prove this using the triangle inequality, e.g. as suggested here . I wanted to check if the following, similar argument is also valid: Let $x$,$y \in \mathbb{R}$ and $n$ be arbitrary. By the mean value theorem, there exists $c \in (x,y)$ such that
$$ f_n(x)-f_n(y)= f_n^{'}(c) \cdot (x-y).$$
Using this, we obtain a bound for the differential quotient of $f_n(x)$:
$$\frac{\mid f_n(x)-f_n(y)\mid}{\mid x-y \mid} = \frac{\mid f_n^{'}(c)\mid \cdot\mid x-y \mid}{\mid x-y \mid} \leq 1.$$ 
Therefore, we obtain the following:
$$\frac{\mid g(x)-g(y)\mid}{\mid x-y \mid} = \frac{\mid \lim f_n(x)- \lim f_n(y)\mid}{\mid x-y \mid} =\frac{\mid \lim (f_n(x)-f_n(y))\mid}{\mid x-y \mid} = \lim\frac{\mid f_n(x)-f_n(y)\mid}{\mid x-y \mid} \leq 1 ,$$
where the first equality follows by definition, the second by limit laws for addition, the third as $\mid {}\cdot{}\mid$ is continuous and the inequality as if a convergent sequence is bounded, then the limit is bounded in the same way.","['derivatives', 'real-analysis', 'lipschitz-functions', 'proof-verification']"
2786286,Text about connections between complex analysis and partition theory?,"I hope this is enough about maths to ask here. As part of my degree I need to do a project consisting of a 7,000-word report on some area of maths (quite a general guideline). I've noticed in studying complex analysis that quite frequently, studying a function's residues winds up giving you an expression for an infinite sum or a generating function for some partition. For instance, there's a well-documented example of using elliptic functions to prove Jacobi's Triple Product. I also know a little bit about partitions, having read the book Integer Partitions by Andrews and Eriksson and done a previous report summarising a large part of it. So I'd like this project to combine the two areas, and I know -- mostly from casual references -- that it's generally agreed that complex analysis and additive number theory are 'closely related'. The two subjects also seem well-adapted for combining because doing the working out in complex analysis is elegant, but the final answer might not be very exciting, while it's the other way round in number theory, where the working out can be very painful but the final answer pretty neat. Can you recommend any online source that explores connections between complex analysis and partition theory? If there's an especially good textbook, that would be welcome too. I've noticed Complex Analysis in Number Theory by Anatoly A. Karatsuba, but it doesn't seem like the right fit based on the Amazon description. EDIT: And if there's a source that links general number theory results with complex analysis, that's cool too. It doesn't have to be partitions. I'm offering a bounty for this now because responses have been extremely scant and it would be really helpful to find a good source. EDIT 2: I should emphasise that this is a project for my final year of an undergraduate degree. Also, I've found a potentially useful source in the textbook Complex Analysis by Eberhard Freitag and Rolf Busam. Chapter 8 is all about additive number theory. But I'll leave the question open in case there are more sources out there.","['number-theory', 'complex-analysis', 'soft-question', 'integer-partitions']"
2786332,"Evaulating the trigonometric integral $\int \frac{1}{(x^2+1)^2} \, dx$","Problem: Evaluate the following integral:
\begin{eqnarray*}
\int \frac{1}{(x^2+1)^2} \, dx \\
\end{eqnarray*} Answer: To do this, I let $x = \tan u$. Now we have $dx = \sec^2 u du$.
\begin{eqnarray*}	\int \frac{1}{(x^2+1)^2}  \, dx &=& \int \frac{\sec^2{u} \, du}{(\tan^2{u} + 1)^2} \\
\int \frac{1}{(x^2+1)^2}  \, dx &=& \int \frac{1}{\sec^2{u}} \, du
	= \int \cos^2{u} \, du \\
\int \frac{1}{(x^2+1)^2}  \, dx &=& \int \frac{\cos{(2u)} + 1}{2} \, du 
	= \frac{\sin(u)}{4} + \frac{u}{2} \\
\int \frac{1}{(x^2+1)^2}  \, dx &=& \frac{\sqrt{1 - \cos^2{u}}}{4} + \frac{u}{2} \\
\end{eqnarray*}
Now, I think I am right so far but I do not know have to get rid of the $u$ in
the $\cos^2(u)$ term. Please help. Thanks Bob","['integration', 'trigonometry', 'calculus']"
2786367,Probability that polygon formed by n points on a circle contain the center of the circle?,"I have seen similar questions being asked on this forum, but couldn't find this exact problem. So there are n points selected uniformly randomly on a circle. What is the probability that the polygon of these n points contains the center of the circle? Now, taking cue from a similar question of probability that all these n points lie within a semicircle, Suppose we mark the bottom most point of the circle as zero. From there on, we move to the right and find the first point, lets say point i, at a distance x along the circumference. Now, the probability that the next n-1 points lie within the arc length $(x, x+\frac12)$ is $P = { (\frac { 1 }{ 2 } ) }^{ n-1 }$, which is the probability that these n points lie within a semicircle. The probability that they don't lie in the same semicircle then becomes $1-P.$ Clearly, if these n points lie within a semicircle, their polygon doesn't contain the center of the circle. Next, the point i could be any of those n points. So we need to account for all the n possibilities being the first point. But, should the final probability be $1-nP$, or $n(1-P)$?","['probability', 'geometric-probability']"
2786438,$AB + BA = A \implies A$ and $B$ have a common eigenvector,"Let $A$ and $B$ in $M_n(\mathbb C)$ with $n$ being odd . Suppose $AB + BA = A$. Prove that $A$ and $B$ have a common eigenvector. (It is from an oral exam.) I ask for a hint for this, not a complete solution. Thank you.","['matrices', 'eigenvalues-eigenvectors', 'linear-algebra']"
2786448,Curl and Conservative relationship specifically for the unit radial vector field,"Consider the vector field $$\vec{F} = \frac{x}{r}\hat{x} + \frac{y}{r}\hat{y} + \frac{z}{r}\hat{z}$$ with $r = \sqrt{x^2 + y^2 + z^2}$. If I can find a function $V$ such that $\vec{F} = \nabla V$, then the unit radial vector field, by definition, will be called conservative. I find that $V = \sqrt{x^2 + y^2 + z^2}$ works and therefore, $\vec{F}$ is conservative. Now let's attack the problem using the curl. Can we use the curl to determine if $\vec{F}$ is conservative? Yes, however in addition to the curl being zero, we have to add in the necessary condition that the domain of $\vec{F}$ be simply connected. I find that $\nabla \times \vec{F} = \vec{0}$. Therefore I conclude on any simply connected domain not containing the origin, there is some potential $V$ out there. The largest simply connected domain that I could have could be, for instance, all of $\mathbb{R}^3$ except half the plane $x = 0$ (the full plane $x = 0$ is too much but if we remove just half of it, the domain is still simply connected). Question 1: In my first paragraph, I found a potential function. However, my textbook lacks details on where I can call $\vec{F}$ conservative. It basically says, if you can find a $V$, then $\vec{F} = \nabla V$ is called conservative on it's domain. It's domain is all 3-tuples of real numbers except the origin. So should I call $\vec{F}$ conservative over all of $\mathbb{R}^3$ except the origin? But my second paragraph (this one) says that I should call $\vec{F}$ conservative over any simply connected domains? Which one do I go with? Question 2: My 2nd issue comes when I look at a counterclockwise vector field divided by $r^2$ $$\vec{G} = \frac{-y}{r^2} \hat{x} + \frac{x}{r^2}\hat{y}$$ I find that $\nabla \times \vec{G} = \vec{0}$. This is good. So over any simply connected domain not containing the origin, I can call $\vec{G}$ conservative and find some potential function for it. Now let's consider for fun $$ \oint_{\text{unit circle}} \vec{G} \cdot d\vec{s} $$ For 1 revolution, this integral is $2\pi$. For $n$ revolutions, this integral is $2\pi n$. For conservative vector fields, any circulation should always give $0$. This shows us (at least somewhat) why $\vec{G}$ can't be called conservative on domains that contain the origin. But interestingly $$\oint_{\text{unit circle}} \vec{F} \cdot d\vec{s} = 0 $$ Does this mean, even though the theory says that we call $\vec{F}$ conservative over simply connected domains, the unit radial field is an exception? We can call the unit radial field conservative over all of $\mathbb{R}^3$ , including the origin ?","['multivariable-calculus', 'vector-fields', 'curl']"
2786454,How to understand the definition of limit?,"Definition 1: Let $f(x)$ be defined on an interval that contains $x=a$. Then we say that, $$\lim_{x\to a}f(x) = L$$
  if for every number $\epsilon$ there is some number $\lambda$ such that
  $$|f(x) - L|<\epsilon \text{ whenever    } 0<|x-a|<\lambda.$$ I understand what this theorem is trying to tell me in terms of mathematical conditions. I also understand the vertical and horizontal line test. The range mapped by the domain has to be within something called 'the pink area', but I still don't see the big picture of it or I don't see the importance of that so called 'pink area'. What does close enough mean?. I have taken several calculus classes and still don't understand a single thing about it. What was the original problem that led to this problem?","['intuition', 'calculus', 'limits']"
2786459,Fibonacci numbers proof [duplicate],"This question already has answers here : Fibonacci Numbers Proof: $ f_n = \binom n0 + \binom{n-1}1 +\dots+ \binom{n-k}k$ (3 answers) How to show that this binomial sum satisfies the Fibonacci relation? (6 answers) Relation between Pascal's triangle and fibonacci series. [duplicate] (1 answer) Closed 6 years ago . Is there any proof that $\sum_{k=0}^{n}$ ${n-k}\choose{k}$ $=f_{n+1}$, when $f_{n}$ is a Fibonacci number? (I tried to write in some $n$'s and it looked true, but I don't really know how should I prove it.","['algebra-precalculus', 'combinatorics', 'fibonacci-numbers']"
2786506,Non-wandering set (definitions),"For a map $f\colon X\to X$, a point $p\in X$ is called non-wandering , provided for every neighborhood $U$ of $p$ there exists an integer $n>0$ such that $f^n(U)\cap U\neq\emptyset$. Sometimes, I also read the following definition: For either maps or flows a point $p\in X$ is said to be a nonwandering point, if given any neighborhood $U$ of $p$ there exists a sequence of times $t_n\to\infty$ such that $f^{t_n}(p)\in U$ for all $n$. Hence, non-wandering points are points whose neighborhoods the map visits infinitely often. Are these definitions equivalent?
Does the first definition also imply that each neighborhood of $p$ is visited infinitely often?","['topological-dynamics', 'dynamical-systems', 'analysis']"
2786507,$L^2$-valued integral as parameter integral,"Setting Let us regard the Hilbert space $L^2(0,1)$ and the $C_0$-semigroup $(T(t))_{t\geq 0}$ defined by
$$
T(t):\left\{
\begin{array}{rml}
L^2(0,1) & \to & L^2(0,1), \\
f &\mapsto &\left(x \mapsto
\begin{cases}
f(x+t), & \text{if}\; x+t<1\\
0, & \text{else}
\end{cases}
\right).
\end{array}
\right.
$$
It is easy to verify that this is indeed a $C_0$-semigroup. Therefore, the mapping $t \mapsto T(t)f$ is a continuous mapping from $L^2(0,1)$ to $L^2(0,1)$. Consequently the $L^2(0,1)$-valued integral
$$
g := \int_0^1 T(t)f \,\mathrm{d}t
$$
exists. Question In order to get some information about the behavior of $g$ it would be nice to regard $g$ as a parameter integral. Hence I am interested in the following equality
$$
g(x) = \Big(\int_0^1 T(t)f \,\mathrm{d}t\Big) (x)\stackrel{?}{=} \int_0^1 \big(T(t)f\big)(x)\,\mathrm{d}t
.
$$
Or with a different notation
$$
g = 
\int_0^1 \big(x \mapsto \big(T(t)f\big)(x) \big)\,\mathrm{d}t
\stackrel{?}{=}
\Big(x\mapsto\int_0^1 \big(T(t)f\big)(x)\,\mathrm{d}t\Big)
.
$$
The evaluation mapping is neither continuous nor well-defined on $L^2$. So I think it is not trivial to justify this step. It seems quite common to evaluate such $L^2(0,1)$-valued integrals by interpreting it as a parameter integral, so I guess that there is a theorem which justifies that. It would be really great if someone had a reference. Solution for this special case In this particular case I think I have a solution. I know that every convergent sequence in $L^2$ has a subsequence which converges even point-wise a.e.. Since
$$
g_n := x\mapsto \sum_{i=1}^{n} \frac{1}{n} \Big(T\Big(\frac{i}{n}\Big)f\Big)(x)
$$
converges to $g$ and every subsequence of $g_n(x)$ converges in $\mathbb{R}$ to the same limit for a.e. $x\in (0,1)$, the point-wise limit of $g_n$ has to coincide with $g$ a.e..","['functional-analysis', 'lp-spaces', 'integration']"
2786510,How to explain this point transformation?,"Yesterday we were having a lecture on point coordinates after rotation. The prof. explained that the position of a point after a counterclockwise rotation is obtained from the following formula, $x=x_0 \operatorname{Cos}(\theta)-y_0 \operatorname{Sin}(\theta), \qquad y=y_0 \operatorname{Cos}(\theta)+x_0\operatorname{Sin}(\theta)$, where $\theta$ is the angle of rotation. And then, out of nowhere, for an exercise he used the following formula for the displacements of the point after rotation, $u_x=(-x_0 \operatorname{Cos}(\frac{\pi-\theta}{2})-y_0\operatorname{Sin}(\frac{\pi-\theta}{2}))\times 2\operatorname{Sin}(\frac{\theta}{2}), \qquad \\ u_y=(x_0 \operatorname{Sin}(\frac{\pi-\theta}{2})-y_0\operatorname{Cos}(\frac{\pi-\theta}{2}))\times 2\operatorname{Sin}(\frac{\theta}{2}).$ Why the signs are different? why instead of just $\theta$ he has used $\frac{\pi-\theta}{2}$? and most importantly what is the role of $2\operatorname{Sin}(\frac{\theta}{2})$? Thanks in advance.","['rotations', 'coordinate-systems', 'trigonometry', 'linear-transformations', 'geometry']"
2786552,Show function is measure preserving,"Given $\Theta:[0,1]\to [0,1]$ of the following form, if we express $x = (x_1,x_2,x_3,x_4,...)$ in binary where $x\in [0,1]$, then $$\Theta(x)=(x_2,x_1,x_4,x_3,...)$$
I.e. $\Theta$ transposes the binary digits of $x$. I want to show that $\Theta$ is measure preserving. My idea was to show that it preserves (Borel) measure on some simple generating set of the Borel algebra. I.e. intervals of form $(0,t)$. But in practice this seems extremely hard, and I am finding it very hard to visualize how $\Theta$ behaves on open intervals.","['real-analysis', 'ergodic-theory', 'measure-theory']"
2786567,Prove range$(A)$ is the space spanned by the columns of $A$,"Prove range$(A)$ is the space spanned by the columns of $A$. Since both of these things are sets we traditionally show that both sets are a subsets of the each other. But why doesn't the following proof work? proof Let $A \in \mathbb{C}^{m\times n}$. Then range$(A)$ is the set of all vectors $Ax$, $x \in \mathbb{C}^n$. Furthermore $Ax$ can be written $$Ax = x_1a_1 + \cdots x_na_n \tag{$\star$}$$ where $x_i$ are the components of the vector $x$ and $a_i$ are the columns of $A$. Since $x$ is arbitrary so are its components. Therefore the right side of $(\star)$ represents all possible linear combinations of the column vectors $a_i$. That is $$\text{range}(A) = \{Ax: x \in \mathbb{C}^n\} = \text{span}\{a_1, \dots a_n\}$$ Edit : Note that the book I'm studying out of basically does what I do above but then includes a ""conversely"" statement that starts by letting some vector be in the span of the columns of $A$. That's why I think mine is wrong.","['linear-algebra', 'elementary-set-theory', 'vector-spaces']"
2786581,Inverse-Square vector fields have both a divergence and curl of $0$?,"Consider an inverse- square vector field $$ \vec{F} = \frac{x}{r^3}\hat{x} + \frac{y}{r^3}\hat{y} + \frac{z}{r^3}\hat{z} = \frac{\hat{r}}{r^2}$$ where $r = \sqrt{x^2 + y^2 + z^2}$. The curl $\nabla \times \vec{F} = \vec{0}$, therefore we might go looking for a potential $V$. I find that $V = -1/r$ works and therefore one can say that $\vec{F} = \nabla V$ is derivable from a potential function $V$. I'll point out right now that $\vec{F}$ is undefined at the origin. The divergence $\nabla \cdot \vec{F} = 0$. Therefore, we might go looking for a vector potential $\vec{A}$ such that $\vec{F} = \nabla \times \vec{A}$. One would say that $\vec{F}$ is derivable from a vector potential $\vec{A}$. But I'm having trouble seeing that an inverse-square vector field is derivable from both a vector potential and a scalar potential. So I know we have a trouble point at the origin. Yet this trouble point doesn't really seem to affect the ""conservativeness"" or path-independence of the vector field. But this trouble point does seem to affect the surface-independence of the vector field. As long as the surface doesn't wrap around the origin, I'd expect the inverse-square vector field to be surface-independent for a given boundary curve. Can an inverse-square vector field be derivable from both a scalar potential and a separate a vector potential? (Helmholtz theorem comes to mind. But the question I'm asking involves two separate equations. One $\nabla V$ gives $\vec{F}$ and another $\nabla \times \vec{A}$ gives $\vec{F}$ as well).","['multivariable-calculus', 'vector-fields', 'curl', 'divergence-operator']"
2786590,"On Halmos' proof that ""there is no universe""","On pp. 6-7 of his Naive Set Theory , Paul Halmos proves a result that I know very well, but I have a hard time following the argument he gives here.  This question is about Halmos' argument (and not about the result itself, which I understand well enough). I'll quote Halmos at length to make sure I don't miss anything.  (I'll be as faithful as possible, but I will change his notation slightly, since he uses $\epsilon$ for $\in$ , and $\epsilon^\prime$ for $\notin$ ; I will stick to $\in$ and $\notin$ ).  The excerpt below begins right after he has given the Axiom of specification .  (In the excerpt, I'll use boldface to indicate what I'm having trouble with.) ...To indicate the way $B$ is obtained from $A$ and from $S(x)$ it is customary to write $$B = \{x \in  A:S(x)\}$$ To obtain an amusing and instructive application of the axiom of specification, consider, in the role of $S(x)$ , the sentence $$\mathrm{not}\;(x \in  x).$$ It will be convenient, here and throughout, to write $``x \notin  A""$ ... instead of $``\mathrm{not}\;(x \in  A)""$ ; in this notation, the role of $S(x)$ is now played by $$x \notin  x.$$ It follows that, whatever the set $A$ may be, if $B = \{x\in A:x\notin x\}$ , then for all $y$ , $$(*)\;\;\;\;\;y\in B\;\mathit{\;if\;and\;only\;if\;} \;(y\in A\;\;\mathit{and}\;\;y\notin y).$$ Can it be that $B \in A$ ?  We proceed to prove that the answer is no.  Indeed, if $B\in A$ , then either $B\in B$ also (unlikely, but not obviously imposssible), or else $B\notin B$ . If $B\in B$ , then, by $(*)$ , the assumption $B\in A$ yields $B\notin B$ —a contradiction.  If $B\notin B$ , then, by $(*)$ again, the assumption $B\in A$ yields $B\in B$ —a contradiction again.  This completes the proof that $B\in A$ is impossible, so we must have $B\notin A$ . (In the subsequent discussion, Halmos argues that this result means that ""there is no universe [of discourse]"" , etc., etc.) Now, this is what I don't get.  If I replace $y$ with $B$ in $(*)$ , I get $$B\in B\;\mathit{\;if\;and\;only\;if\;} \;(B\in A\;\;\mathit{and}\;\;B\notin B).$$ This means that $B\in B$ and $(*)$ together imply $B\notin B$ .  Contrary to what Halmos writes, ""the assumption $B\in A$ "" is not needed to draw this conclusion. What am I missing? (By the way, I would have worded the second part of the proof a bit differently, namely: If $B\notin B$ , then, by $(*)$ again, the assumptions $B\in A$ and $B\notin B$ jointly imply that $B\in B$ . I mention this only in case this rewording gives a clue about where my confusion lies.)",['elementary-set-theory']
2786600,Invert the softmax function,"Is it possible to revert the softmax function in order to obtain the original values $x_i$ ? $$S_i=\frac{e^{x_i}}{\sum e^{x_i}} $$ In case of 3 input variables this problem boils down to finding $a$ , $b$ , $c$ given $x$ , $y$ and $z$ : \begin{cases}
\frac{a}{a+b+c} &= x \\
\frac{b}{a+b+c} &= y \\
\frac{c}{a+b+c} &= z
\end{cases} Is this problem solvable?","['machine-learning', 'exponential-function', 'probability', 'logistic-regression']"
2786611,Continuous functional calculus of multiplication operator in $L_2$,"I would like to calculate the continuous calculus of the multiplication operator by an essentially bounded function $\varphi : X \rightarrow \mathbb{R}$ in $L_2 (X, \mu)$, where $\left( X, \mu \right)$ - finite measure space. But I have one problem. I know that $\sigma \left( M_{\varphi} \right) = \mathrm{ess.im} \left( \varphi \right)$, but to define $\gamma: C (\sigma) \rightarrow B \left( L_2 (X, \mu)\right)$ we need the domain of the function $f \in C (\sigma) $ to be $\mathrm{im} (\varphi)$. I know, that if $$\forall A \subset X \quad \varphi(A) \cap \mathrm{ess.im} \left( \varphi \right) = \varnothing \implies \mu(A)=0$$ but I don't know how to prove this. Thank's for the help!","['functional-analysis', 'functional-calculus', 'measure-theory', 'operator-theory']"
2786648,Integral inequality $\frac{1}{2M}\le\int_0^1xf(x)dx\le1-\frac{1}{2M}$,"Let $f:[0,1]\rightarrow[0,\infty)$ be a continuous function such that $\int_0^1f(x)dx=1$, and let $M=\max f(x)$. Show that:
$$\frac{1}{2M}\le\int_0^1xf(x)dx\le1-\frac{1}{2M}$$ We have $1=\int_0^1f(x)dx\le\int_0^1Mdx=M$, so $M\ge1$. I tried several inequalities, but none of them seem to be working in this case. I could use a hint. Thank you!","['integration', 'integral-inequality', 'calculus']"
2786679,Wallis' axiom for parallel lines,"I want to prove, using the typical tools from a Hilbert plane, that the Wallis' axiom implies ($P_{\leq 1}$), where Wallis' axiom: Given a triangle $\Delta ABC$ and given a line segment $DE$, there exists a similar triangle $\Delta A'B'C'$, having side $A'B' \geq DE$. $P_{\leq 1}$: For each line $l$ and for each point $P\notin l$, there is at most one line containing $P$ that is parallel to $l$ I have already proved Proclo's axiom is equivalent to $P_{\leq 1}$, but I got no idea how to solve this problem... Any help would be appreciate.",['geometry']
2786680,Squaring a real valued function,"There is a function f(x) defined as 
$$f(x)= \sqrt{x+1}$$
And we need to find its square that is $$f^2(x)$$
Or in other words we have to find $$f(x) × f(x)$$
What I am doing is :-
First I found the domain of f(x)
that is $$[-1,\infty)$$
Then I found the intersection of domains of f(x) and f(x) that is also equal to $$[-1,\infty)$$
And then we can write as :-
$$f^2:[-1,\infty)\to$$ R is defined by
$$f^2(x)=\sqrt{x+1}×\sqrt{x+1}$$
$$f^2(x)=x+1$$
My question is that in $f^2(x)$ we calculated the domain as $[-1,\infty)$ but we get $f^2(x)=x+1$ where we can put any real number so its domain should be R(real numbers). What am I misssing here???
Any help is highly appreciated.",['functions']
2786703,About the properties of a type of inclusion structure,"I am interested in learning about the properties of a particular type of set that is relevant to my (non-mathematical) research. I apologize for my totally informal language below (I can read answers in set notation but it's a bad idea for me to try to write with it): Elements of the set have inclusion relations, they can either overlap (they have an intersection that is less than either element); or one can include the other (they have an intersection that is equal to one element). There are three clear axioms: Almost every element of the set overlaps with at least one other element. For every pair of overlapping elements, there is an element that exactly includes this pair (there is a ""union element"" for every such pair). The exception to 1. is that there is a single element that is only a ""union element"" and that overlaps with no one (and, from 2., therefore is included by no one). The best that I understand is that this is a specific type of partially-ordered set but that's a pretty general category. It seems very similar to a topological set but it requires unions rather than intersections. Is it familiar to anyone? Or is it nonsense? If this is a familiar structure, I'd like to know if there are proofs for a few properties that I think must be true; primarily, that every element will have a unique upward and downward inclusion substructure, and that any subset of non-overlapping elements (excluding the Axiom 3 ""top"" element) will have a common upward including element. Thanks!","['order-theory', 'general-topology', 'elementary-set-theory']"
2786722,"Find the largest $n \in \mathbb{N_+} $ such that $\{ (2+\sqrt 2)^n\} < \frac{7}{8}$, where $\{x\}$ denotes the fractional part of $x$.","Problem Find the largest $n \in \mathbb{N_+} $ such that $\{ \left(2+\sqrt{2}\right)^n\} < \dfrac{7}{8}$ , where $\{x\}$ denotes the fractional part of $x.$ My Solution First, we can prove that $a_n=(2+\sqrt{2})^n+(2-\sqrt{2})^n$ is an integer sequence. For this purpose, we may apply the mathematical induction. However,in fact,by setting up the characteristic equation $x^2-4x+2=0$ ，we may confirm that the equality above of $a_n$ really gives the general term formula of the recursion sequence as follows $$a_1=2,a_2=12,a_{n+2}=4a_{n+1}-2a_{n}(n=1,2,\cdots).$$ Now, it's clear that $a_n$ are a series of integers. Moreover, notice that $0<(2-\sqrt{2})^n<1.$ We can obtain $\{(2+\sqrt{2})^n\}=1-(2-\sqrt{2})^n.$ Thus, the problem is to ask us to find the largest $n \in \mathbb{N_+}$ such that $$\left(\frac{2+\sqrt{2}}{2}\right)^n<8.$$ But the left side increases with the increasing $n$ . Hence, we only need to test the critical value. Since $$2^{3/4}=\sqrt{2 \cdot \sqrt{2}}<\frac{2+\sqrt{2}}{2}<\frac{2+2}{2}=2,$$ we have $\left(\dfrac{2+\sqrt{2}}{2}\right)^4>2^3=8$ , and $\left(\dfrac{2+\sqrt{2}}{2}\right)^3<2^3=8.$ As a result, the largest $n$ is $3$ . Please correct me if I'm wrong! And I hope to see another new solution. Thanks!",['sequences-and-series']
2786727,"If $x \in R $ and $\frac{(x-a)(x-c)}{(x-b)}$ can assume any real value, then which of the following may be correct","If $x \in R $ and $\frac{(x-a)(x-c)}{(x-b)}$ can assume any real value, then which of the following may be correct $a>b>c$ $a>c>b$ $b>c>a$ $b>a>c$ I have tried simplifying this equation like - 
$$\frac{(x-a)(x-c)}{(x-b)} = y$$
$$x^2 -xc -ax + ac = xy - by$$
$$x^2 -x(c+a-y) + (ac+by) = 0$$
Since this is always real, $D\ge 0$
$$(c+a-y)^2 - 4(ac+by) \ge  0$$
$$c^2 + a^2 + y^2 + 2(ac -cy-ay) - 4ac - 4by \ge 0$$
However, I have no idea how to continue from here. Can someone possibly advice on how to continue?","['algebra-precalculus', 'inequality', 'polynomials', 'quadratics']"
2786924,"How to evaluate this integral $\int_0^1 \frac{x\log ^2(\sqrt{x^2+1}+1)}{\sqrt{1-x^2}} \, dx$","How to evaluate $$A=\int_0^1 \frac{x \log ^2\left(\sqrt{x^2+1}+1\right)}{\sqrt{1-x^2}} \, dx$$See the details here from a similar question: Evaluating $\int_0^1 \frac{z \log ^2\left(\sqrt{z^2+1}-1\right)}{\sqrt{1-z^2}} \, dz$ .
 By applying the same process, I find:
 $$A=\int_0^1\ln^2\left(\sqrt{2-y^2}+1\right)dy=\int_1^{\sqrt2}\frac {x\log^2 (x+1)}{\sqrt{2-x^2}}dx=\sqrt2 \int_{0}^{\pi/4}\cos\varphi\ln^2\left(\sqrt2 \cos\varphi+1\right)d\varphi.$$ $$A=\ln^22+32(1+\sqrt2)\int_{0}^{\sqrt2-1}\frac{t^2}{\left(1+t^2\right)^2(t+1+\sqrt2)(-t+1+\sqrt2)}\ln\left(\sqrt2\, \frac{1-t^2}{1+t^2}+1\right)dt.$$
 Now,put $$A=\int_0^1 \frac{x \log ^2\left(\sqrt{x^2+1}+1\right)}{\sqrt{1-x^2}} \, dx,  B=\int_0^1 \frac{x \log ^2\left(\sqrt{x^2+1}-1\right)}{\sqrt{1-x^2}} \, dx.$$ We have: $$A-B=4\int_0^{1}\frac {x\log x\log(\sqrt{1+x^2}+1)}{\sqrt{1-x^2}}dx-4\int_0^{1}\frac {x\log^2 x}{\sqrt{1-x^2}}dx$$
$$A+B=-2\int_0^{1}\frac {x\log (\sqrt{1+x^2}-1)\log(\sqrt{1+x^2}+1)}{\sqrt{1-x^2}}dx+4\int_0^{1}\frac {x\log^2 x}{\sqrt{1-x^2}}dx$$
 $$\int_0^{1}\frac {x\log^2 x}{\sqrt{1-x^2}}dx=\ln^22-\frac{\pi^2}{12}-2\ln2+2.$$
But how to deduce these integrals.(using Mathematica?)","['special-functions', 'integration', 'definite-integrals', 'calculus']"
2786955,Question about Bounded Operators,"Let $H$ be a separable Hilbert Space. Suppose $B$ is a linear map with the property that if $u_n \to u$ and $B(u_n) \to v$, then $v=B(u)$. Show that $B$ is bounded. So the issue here is to get some sequence that $B$ will be continuous on and then proceed by some sort of scaling, but I don't see how to find such a sequence...perhaps involving the basis? Any tips helpful.","['functional-analysis', 'real-analysis', 'analysis']"
2786966,Example of a continuous but no where differentiable R to R function that is not expressed as a series of functions?,"A famous example is the Weierstrass function, which can be found here https://en.wikipedia.org/wiki/Weierstrass_function . I'm curious as to if there exists such functions that are not a series of functions. Especially, are there elementary functions that are continuous but no where differentiable? https://en.wikipedia.org/wiki/Elementary_function","['real-analysis', 'calculus', 'functions']"
2786974,"If a function is log-lipschitz, then it is $\alpha$-Hölder for all 0<$\alpha$<1 and isn't Lipschitz continuous","Consider $C^{0,\alpha}$ the class of $\alpha$-Hölder continuous functions. Let $\Omega \subset \mathbb{R}$ be a bounded subset and let $u: \Omega \rightarrow \mathbb{R}$ a continuous function. Prove that if there is $C\in \mathbb{R}$ such that 
    $$|u(x)-u(y)| \leq C \left|x-y\right| \ln {\frac{1}{\left|x-y\right|}}$$
For $|x-y|<\frac{1}{2}$, then $u\in C^{0,\alpha}$, i.e., $\forall \alpha \in (0,1)\exists K_{\alpha}\in \mathbb{R}$ such that
    $$|u(x)-u(y)| \leq K_{\alpha} \left|x-y\right|^{\alpha}$$
My doubt: First: How I prove that $K$ depends on $\alpha$? Second: I think that we can use the subbaditive property of $ln$ function. But I can't do this Third: How I prove that it isn't Lipschitz continuous? Forth: why is the supremum of the distance $1/2$? Can I improve the distance, i.e. $\exists d>1/2$ such that the inequality is true for $1/2<|x-y|<d$? Or, can I suppose that this inequality are well-defined at $|x-y|<1$? This definition I found in this link , page 80.","['continuity', 'lipschitz-functions', 'holder-spaces', 'analysis']"
