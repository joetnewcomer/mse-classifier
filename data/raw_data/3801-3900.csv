question_id,title,body,tags
27437,Solving in positive integers an equation containing exponentials,"I stumbled upon this number theory problem while I was solving another problem. Here is the equation: $$3^kn + 3^{k-1} + 2^m(3^{k-1} + 2h) = 2^{m+l}n$$ where $k \geq 3, h,l,m,n\in\mathbb{N}$, $n$ is odd and $n$ is not a multiple of $3$. My impression is that it does not have a solution. However, I have not progressed on the problem anymore than that. Could you please help? Thanks.","['diophantine-equations', 'number-theory']"
27443,"Does this calculation have a name, or a generic formulation?","Background Informatiom I would appreciate help in identifying or explaining this operation: To calculate each of the $n$ values of $f(\Phi)$ : Sample from the distribution of each of $i$ parameters, $\phi_i$ Calculate the $i$ values of $g(\phi_{i})$ Subtract each $g_i(\phi_{i,n})$ from $g_i(\hat{\phi_i})$ (these are deviation) Take the sum of these deviations Add the sum of these deviations to the median in summary, this is the calculation: $$f(\Phi_n)=g(\hat{\phi}) + \sum_i(g_i(\phi_{i,n})-g_i(\hat{\phi_i}))$$ $\phi_i$ is the distribution of each of $i$ parameters $\hat{\phi}$ is a vector of the parameter medians $g$ is a vector of $i$ univariate splines, one for each parameter estimated by evaluating a multivariate model across the range of $\phi_i$ while all $\phi_{\text{not}i}$ held at their medians (a univariate sensitivity analysis of a computationally intensive prognostic model) $g_i(\hat{\phi_i})$ is the $i^{th}$ spline evaluated at the median of $\phi_i$ Questions: Is there a name or simple way to describe this calculation? My first attempt to describe the above operation: The spline ensemble is calculated as the sum of deviations from the median for for each parameter added to the median. An alternative suggestion: The spline ensemble is calculated based on the univariate anomalies for each parameter. Is there a simplified form of this computation, or expression of the equation?","['statistics', 'algorithms', 'functions', 'terminology', 'probability']"
27446,Eigenvalues of the differentiation operator,"I have a linear operator $T_1$ which acts on the vector space of polynomials in this way:
$$T_1(p(x))=p'(x).$$ How can I find its eigenvalues and how can I know whether it is diagonalizable or not?","['linear-algebra', 'eigenvalues-eigenvectors']"
27455,Is a sequence of disjointly supported functions in $L^\infty$ complemented?,"Let $(f_n)_{n \geq 1}$ be disjointly supported sequence of functions in $L^\infty(0,1)$. Is the space $\overline{\mathrm{span}(f_n)}$ (the closure of linear span) complemented in $L^\infty(0,1)$? By complemented we mean that $L^\infty(0,1) = \overline{\mathrm{span}(f_n)} \oplus X$, where $X$ is a subspace of $L^\infty$ and $\oplus$ is direct sum. Equivalently, we can ask if there exists a projection $P\colon L^\infty(0,1) \to \overline{\mathrm{span}(f_n)}$? It is quite easy to prove this in $C[0,1]$. Indeed, let $(f_n)$ be disjointly supported sequence in $C[0,1]$ and fix $x_n \in \mathrm{supp}(f_n)$, $n \in \mathbb{N}$. Then the space $C[0,1]$ can be written as
$$
C[0,1] = \overline{\mathrm{span}(f_n)} \oplus \{f \in C[0,1]\colon f(x_n) = 0, n = 1,2,\dots \}.
$$","['functional-analysis', 'banach-spaces']"
27468,Finding a Lower Bound of a Function,"Let
$$
f(x, y) = \frac{x + y}{2e}
$$ and $$
g(x, y) = \frac{(x+y)+\sqrt{x^2 + y^2 + 6xy}}{2e^{\frac{\sqrt{x^2 + y^2 + 6xy}}{2(x + y)}}} 
$$ I am looking for the largest $K$ so that, $$ \forall x, y > 0, \frac{g(x, y)}{f(x, y)} > K$$ By numerical computation I found that $K$ must be around $3.3$. But the best thing I could prove is about 2.28 for $K$ using the following method, $$
x^2 + y^2 + 2xy < x^2 + y^2 + 6xy < (\sqrt{3}x)^2 + (\sqrt{3}y)^2 + 2(\sqrt{3}x)(\sqrt{3}y)
$$ Which results in, $$
(x+y) < \sqrt{x^2 + y^2 + 6xy} < \sqrt{3}(x+y)
$$ So, $$
\frac{g(x, y)}{f(x, y)} > \frac{\frac{x+y}{e^{\sqrt{3}/2}}}{\frac{x+y}{2e}} > 2.28
$$ I'm wondered if anyone can propose a way to prove a larger K? Note: This is the improvement of the network throughput based on the method I proposed (g(.)) over an old method (f(.)). So, this is important for me to show that how much better my proposed method works. Thanks in advance :)","['calculus', 'functions']"
27471,Proof of Legendre's theorem on the ternary quadratic form,"Theorem (Legendre): Let $a,b,c$ coprime positive integers, then $ax^2 + by^2 = cz^2$ has a nontrivial solution in rationals $x,y,z$ iff $\left(\frac{-bc}{a}\right)=\left(\frac{-ac}{b}\right)=\left(\frac{ab}{c}\right)=1$. I'm trying to prove this theorem using Lemma Modified global square theorem: The rational number $cz$ is a $c$ times a square iff it is a $c$ times a square in $\mathbb{Q}_p$ for every prime $p$. So far, it is possible to show that the equation can be solved locally for all primes powers except $2^r$: To solve this equation $\pmod {abc}$ just put $x = bc$, $y = ac$, $z = ab$. Reducing this gives a solution for every odd prime $p | abc$. Hensel lifts it to $\mathbb{Q}_p$. If an odd prime $p \not| abc$ then put $z = 1$ so that we have $ax^2 \equiv c - by^2 \pmod p$, $x$ and $y$ can take on $(p+1)/2$ values each so they must have an intersection which solves this congruence. Hensel lifts it to $\mathbb{Q}_p$ again. It's easy to solve when $p=2$ but I think it $p=4$ needs to be done for Hensel to apply. My question is how to get a rational number out of this, so that I can apply the global square theorem and conclude? It seems like the $p$-adic numbers have to have a finite expansion but that seems just as hard to prove as anything. Also if anyone has a hint for the $2^r$ case that would be great too! Thanks a lot.","['quadratic-forms', 'number-theory']"
27489,The discrete Fourier transform of a Dirichlet charachter,"I usually work in number theory so I am not familiar with Fourier transforms, I have read up on them and know the basics but it never seems to be in number theory language. I am trying to find the transform of a primitive Dirichlet character $\chi(n) \bmod q$. I know this is a periodic function and $\chi(n)=\exp\left(\frac{Kv(n)}{\phi(p^\alpha)}\right)$ but I have no idea have to find its transform or the transform of $f(n)\chi(n)$ Yes you are right, say how would you calculate $\sum_(n\epsilon Z) f(n)\chi(n)$","['fourier-analysis', 'number-theory']"
27496,Question about the integral of $1/(1+9x^2)$,This might be a stupid question but what is the integral of $\frac{1}{1 + 9x^2}$? I want to think it's $\tan^{-1}(3x) + c$ but the book I'm working in says $\frac{1}{3}\tan^{-1}(3x) + c$. How did they get $1/3$?,"['calculus', 'integration']"
27506,Counting orbits under $\operatorname{Sym}(3)$,"I'd like a closed form expression $x(n)$ for the number of orbits of the symmetric group on $3$ points acting on the triples in $\{ (a,b,c) \mid a,b,c \in \Bbb{Z}, 1 \leq a,b,c \leq n, c = 2n−a−b \}$. I feel like this should be a really basic problem, but my standard method of attack fails: look it up in OEIS and prove the known formula.  My backup plan of ""think about it"" has failed: I don't know how to deal with the restriction on $c$. (Without the restriction, I happen to have learned this is the same thing as counting $1 \leq a \leq b \leq c \leq n$, and I happen to have learned this is Binomial($n+2$, $3$) because you need to place two bars between $n$ stars, but I have no general context for this.) I suspect this is a pretty standard counting problem even with the restriction, but I never really learned how to count (fish, fish, fish, …, fish, fish, …, fish). The counts $x(n)$ for $n=1$ to $30$ are: $0, 1, 2, 3, 4, 6, 7, 9, 11, 13, 15, 18, 20, 23, 26$, $29, 32, 36, 39, 43, 47, 51, 55, 60, 64, 69, 74, 79, 84, 90$. I think there are $(n-1)\cdot(n+4)/2$ triples, but even that is a little fuzzy (increase increases by $1$ each time).  I have no idea how many of them have $2$ equal components.",['combinatorics']
27507,Transformation of state-space that preserves Markov property,"I am solving a problem in Mathematical Statistics by Jun Shao Let $\{X_n \}$ be a Markov chain. Show that if $g$ is a one-to-one Borel function, then $\{g(X_n )\}$ is also a Markov chain. Give an example to show that $\{g(X_n )\}$ may not be a Markov chain in general. I have a hard time on solving it, even though I have been staring it and thinking about it for a whole day. For the first part, which is to show that any
one-to-one Borel $g$ preserves
Markov property of a Markov chain, I
guess using the formula for density function under the change
of random variables by $g$, learned in elementary probability course, might
help, but I am not sure how to use it, or maybe the tools needed to solve the problem are not that simple? For the second part, I really have no idea of how to construct some $\{ X_n\}$ so that $\{g(X_n )\}$ is not a Markov chain, for example, when $g(x)=x^2$? Here are also some extended thoughts and questions: If $g$ is not one-to-one, is
$\{g(X_n )\}$ always not a Markov chain for any Markov chain $\{ X_n\}$? How about if $\{X_t \}, t \in
    \mathbb{R}$? Does any one-to-one $g$
also preserve Markov property of
continuous-time stochastic
processes? Thanks a lot!","['probability-theory', 'markov-chains', 'markov-process']"
27524,fair value of a hat-drawing game,"I've been going through a problem solving book, and I'm a little stumped on the following question: At each round, draw a number 1-100 out of a hat (and replace the number after you draw). You can play as many rounds as you want, and the last number you draw is the number of dollars you win, but each round costs an extra $1. What is a fair value to charge for entering this game? One thought I had was to suppose I only have N rounds, instead of an unlimited number. (I'd then let N approach infinity.) Then my expected payoff at the Nth round is (Expected number I draw - N) = 50.5 - N. So if I draw a number d at the (N-1)th round, my current payoff would be d - (N-1), so I should redraw if d - (N-1) < 50.5 - N, i.e., if d < 49.5. So my expected payoff at the (N-1)th round is 49(50.5-N) + 1/100*[(50 - (N-1)) + (51 - (N-1)) + ... + (100 - (N-1))] = 62.995 - N (if I did my calculations correctly), and so on. The problem is that this gets messy, so I think I'm doing something wrong. Any hints/suggestions to the right approach?",['probability']
27526,$S=1+10+100+100+10000+... = -1/9$? How is that,"its written that if you multiply the sum above by -9 and use the distributive law, all terms except ""1"" will cancel. I can't see that. I know that this is a divergent series. (the article I was reading was a layman's introduction to zeta regularization) Similarly, how is $S=1+2+3+4... = -1/12$ Even if the series were to terminate somewhere, I dont see these value. This is completely baffling me. Using $\frac{1}{1-x}$ I can put x=10 but that is not fair,isn't it?",['sequences-and-series']
27535,"How to find center of an arc given start point, end point, radius, and arc direction?","Given an arbitrary arc, where you know the following values: start point $(x_0, y_0)$, end point $(x_1, y_1)$, radius ($r$) and arc direction (e.g. clockwise or counterclockwise from start to end), how can I calculate the arc's center?  I know from this previous post (thanks!) that the center lies on the perpendicular bisector between the two points, but don't know how to calculate it. thanks!","['geometry', 'algebra-precalculus', 'circles']"
27546,zeroes of holomorphic function,"I know that zeroes of holomorphic functions are isolated,and I know that if a holomorphic function has zero set whic has a limit point then it is identically zero function,i know a holomorphic function can have countable zero set, does there exixt a holomorphic function which is not identically zero, and has uncountable number of zeroes?",['complex-analysis']
27555,geodesics on a surface of revolution,"I'm having problems with exercise 1 of chapter 3 of do Carmo's ""Riemannian Geometry"".  Here is the background: Let $(u,v)$ be the coordinates on $\mathbb{R}^2$.  Let $f,g\in C^\infty(\mathbb{R})$, and observe that $\varphi:\mathbb{R}^2\rightarrow \mathbb{R}^3$ given by $\varphi(u,v)=(f(v)\cos u,f(v)\sin u,g(v))$ is an immersion assuming $f'(v)^2+g'(v)^2\not= 0$ and $f(v)\not= 0$.  The image is the surface of revolution generated by the curve $(f(v),g(v))$ being rotated about the $z$-axis.  The induced metric is
$$ (g_{ij})=\left( \begin{array}{cc} f^2 & 0 \\ 0 & f'^2+g'^2 \end{array} \right), $$
and the local equations of a geodesic $\gamma$ are
$$ \left\{ \begin{array}{l} \frac{d^2 u}{dt^2} + \frac{2ff'}{f^2}\frac{du}{dt}\frac{dv}{dt}=0 \\ \frac{d^2 v}{dt^2}-\frac{ff'}{f'^2+g'^2}\left( \frac{du}{dt} \right)^2 + \frac{f'f'' + g'g''}{f'^2+g'^2} \left( \frac{dv}{dt} \right)^2 = 0. \end{array} \right. $$ Then, do Carmo says: Obtain the following geometric meaning of the equations above: the second equation is, except for meridians ($u=u_0$) and parallels ($v=v_0$), equivalent to the fact that the ""energy"" $|\gamma'(t)|^2$ of a geodesic is constant along $\gamma$; the first equation signifies that if $\beta(t)$ is the oriented angle, $\beta(t)<\pi$, of $\gamma$ with a parallel $P$ intersecting $\gamma$ at $\gamma(t)$, then $r\cos \beta$ is constant, where $r$ is the radius of $P$. This last paragraph is what's confusing me.  First of all, I've seen the energy of a path as $\int_a^b |\gamma'(t)|^2 dt$, so maybe that's why he put ""energy"" in quotes.  But also, geodesics have constant speed!  So this should be constant along all geodesics too (regardless of whether they're meridians or parallels).  But I figured that maybe I should just blindly plug and chug since that seems to work scarily often in Riemannian geometry, so I found $|\gamma'(t)|^2$, took its $t$-derivative, and substituted in the second equation in the system for geodesics.  Here it is:
$$ |\gamma'(t)|^2 = \left\langle \frac{d \gamma}{dt} , \frac{d \gamma}{dt} \right\rangle = u'^2 g_{11} + 2u'v'g_{12} + v'^2 g_{22} = u'^2f^2+v'^2(f'^2+g'^2)$$
so
$$ \frac{d}{dt} |\gamma'(t)|^2 = 2u'u''f^2+2u'^2ff' + 2v'v''(f'^2+g'^2)+v'^2(2f'f''+2g'g'') $$
$$ = 2u'u''f^2+2u'^2ff' + 2v'(ff'u'^2-(f'f''+g'g'')v'^2)) + 2v'v''(f'^2+g'^2)+2v'^2(f'f''+g'g'')$$
$$ = 2u'u''f^2+2u'^2ff'(1+v')+2v'v''(f'^2+g'^2)+2v'^2(f'f''+g'g'')(1-v')$$ and this looks hopeless.","['riemannian-geometry', 'differential-geometry']"
27556,Simpler way to determine terms in arithmetic progression,"I was given this question on a college assessment pre-test.  I got the correct answer in a reasonable amount of time, but mostly because I worked backwards and double checked my answer.  After I was done, I tried to find math on the net to solve it correctly, but I feel there is probably a simpler solution than this. Is there an easier way to solve problems in this template? Given the first and n -th values in an arithmetic progression, and the sum of the progression up to n (inclusive), give the first x terms of the series. The actual question on the quiz In an arithmetic series, the terms of the series are equally spread out. For example, in
  1 + 5 + 9 + 13 + 17, consecutive terms are 4 apart. If the first term in an arithmetic series is 3, the last term is 136, and the sum is 1,390, what are the first 3 terms? This one lended itself to intuition and backwards-work because I was reasonably certain that d would be an integer, and 133 ( 136 - 3 ) is evenly divisible by 7.  But I would like to find a simpler way to solve problems like this in the future. The work I did to solve the question I used the formulas: S n = ½ n(a 1 + a n ) a n = dn + c Where n is the count of values in the sequence, d is the common difference (distance between values in the sequence), and c is an unknown constant For the first equation, I solved for n : 1390 = ½ n(3 + 136) 1390 = ½ n(139) 10 = ½ n 20 = n For the second equation, I first solved for c , by making two equations out of the values I had for a n , and the previously calculated value for n : 3 = d + c, 136 = 20d + c (with the first): d = 3 - c (substitute): 136 = 20(3 - c) + c 136 = 60 - 19c 76 = -19c -4 = c Then I solved for d : (substitute): 3 = d - 4 d = 7 (double-check): 136 = 20d - 4 140 = 20d 7 = d The final answer was 3, 3 + d, 3 + 2d: 3, 10, 17 Which I did a brute force double-check on.",['sequences-and-series']
27557,Sum of two squares proof,"Find two pairs of relatively prime positive integers $(a,c)$ so that $a^2 + 5929 = c^2$.
Can you find additional pairs with $gcd(a,c) > 1$? What I know: $gcd(a,c) = 1$ implies that there are some $x$ and $y$ such that $ax + cy = 1$. Since $a d$oes not divide $c$, I'm guessing that $a^2$ does not divide $c^2$ as well (need confirmation). In that case we then have $gcd(a^2, c^2) = 1$ so there are some x and y such that $a^2 x + c^2 y = 1$. I'm not 100% sure if that leads us anywhere but it does give an equation that is somewhat matching the question. Let $c^2 = d$ We know that a number $d$ can be written as a sum of two squares if all its prime factors are either 2 or congruent to $1 (mod  4)$. We have $\sqrt(5929) = 77$. So we have that if $d = a^2 + 5929$, $d$ must be a product of distinct primes that are congruent to $1 (mod  4)$ or 2. What am I missing from here that is keeping me back from answering this? It doesn't seem like a very difficult question yet I'm having trouble with it. Maybe its midnight speaking :(.",['number-theory']
27559,Finding points on two linear lines which are a particular distance apart,"I have two linear, skew, 3D lines, and I was wondering how I could find a points on each of the lines whereby the distance between the two points are a particular distance apart? I'm not after the points where the lines are the closest distance apart, nor the furthest distance apart, nor the point at which the lines cross! I'm after the points on the lines where the two lines are a particular distance apart. I'd like to also be able to change this distance and find the new points. Thanks very much in advance! :)","['geometry', 'linear-algebra']"
27572,Teaching myself differential topology and differential geometry,"I have a hazy notion of some stuff in differential geometry and a better, but still not quite rigorous understanding of basics of differential topology. I have decided to fix this lacuna once for all. Unfortunately I cannot attend a course right now. I must teach myself all the stuff by reading books. Towards this purpose I want to know what are the most important basic theorems in differential geometry and differential topology. For a start, for differential topology, I think I must read Stokes' theorem and de Rham theorem with complete proofs. Differential geometry is a bit more difficult. What is a connection? Which notion should I use? I want to know about parallel transport and holonomy. What are the most important and basic theorems here? Are there concise books which can teach me the stuff faster than the voluminous Spivak books? Also finally I want to read into some algebraic geometry and Hodge/Kähler stuff. Suggestions about important theorems and concepts to learn, and book references, will be most helpful.","['differential-topology', 'book-recommendation', 'reference-request', 'differential-geometry']"
27577,"""Boundary"" of convergence of  $\frac{1-(1-c^{n})^{2n}}{(1-c)^{2n}}$","I ran across this confounding limit I am wondering about.  It is as follows: $$\displaystyle \lim_{n\to \infty}\frac{1-(1-c^{n})^{2n}}{(1-c)^{2n}}, \;\ 0<c<1$$ I played around with this on Maple and found that if c is less than approximately .382 (but greater than 0), it converges to 0. If c is greater than .382 (but less than 1), it diverges. What is it about .382?. .382 is an approximation. By playing around more I could have taken it out to more decimal places. The actual problem asks to prove that the above limit is < $\frac{1}{p(n)}$, where p(n) is a polynomial. I was mainly wondering how to solve the limit and why .382 is so significant. Thank you all very much. You are always a big help.","['real-analysis', 'limits']"
27578,the relationship between eigenvectors and matrix multiplication,"If A has eigenvector $\mathbf{v}_1$ so that $A\mathbf{v}_1=\lambda_1\mathbf{v}_1$and B has eignenvector  $\mathbf{v}_2$ so that $B\mathbf{v}_2=\lambda_2\mathbf{v}_2$, then what can you say about AB? can you say $AB\mathbf{v_3}=\lambda_3\mathbf{v}_3$? and what would be the relationship between $\mathbf{v}_1,\mathbf{v}_2,\mathbf{v}_3$ and what would be the relationship between $\lambda_1,\lambda_2,\lambda_3$? Edit $A,B$ are 3 by 3 matrices and $\lambda_1,\lambda_2,\lambda_3$ can be real or complex numbers and $\mathbf{v}_1,\mathbf{v}_2,\mathbf{v}_3$ is a triple.",['linear-algebra']
27579,Non-degenerate alternating bilinear form on a finite abelian group,"Ciao! Let $A$ be a finite abelian group, and let $ \psi : A \times A \to \mathbb{Q}/\mathbb{Z} $ be an alternating, non-degenerate bilinear form on $A$. Maybe I should say what I mean by these words; bilinear means it is linear in each argument separately; alternating means that $\psi(a,a) = 0$ for all $a$; non-degenerate means that, if $\psi(a,b) = 0$ for all $b$, then $a$ must be $0$. Why must $A$ have square cardinality? I believe it will follow from the following theorem in Linear algebra: Theorem. Let $V$ be a finite dimensional vector space over a field $K$ that has an alternating, non-degenerate bilinear form on it (from $V \times V \to K$). Then dim $V$ is even. My idea was to proceed as follows: If the size of $A$ is not square, then for some prime $p$, $A(p)$ is not square, where $A(p)$ means the $p$-primary part of $A$. The original $\psi$ induces a map on $A(p)$ that is non-degenerate, alternating and bilinear. I then wanted to say that $A(p)$ is an $\mathbb{F}_p$-vector space, and then applying the theorem I am done, but this is not true, e.g, $\mathbb{Z}/25\mathbb{Z}$ is not an $\mathbb{F}_5$-vector space. Any pointers anyone?","['linear-algebra', 'abstract-algebra', 'abelian-groups']"
27591,Algebraic notation of floor function in an equation,"I have the following equation as a question for homework: $\lfloor 2x \rfloor = 2\lfloor x\rfloor$ I know what the solution is by deducting to the possibilities.
Meaning this equation is true for any x which is between n (an integer) and $n+y$ where $0\leq y < \frac{1}{2}$ putting it simple:
\begin{align}
x \in \lbrace n + y | n \in \mathbb{Z} , y \in [0, 0.5) \rbrace
\end{align}
I just don't know how to algebraically get to this solution.
Help will be appreciated!","['algebra-precalculus', 'roots', 'ceiling-and-floor-functions']"
27598,Krylov-like method for solving systems of polynomials?,"To iteratively solve large linear systems, many current state-of-the-art methods work by finding approximate solutions in successively larger (Krylov) subspaces. Are there similar iterative methods for solving systems of polynomial equations by finding approximate solutions on successively larger algebraic sets?","['computational-mathematics', 'algebraic-geometry', 'numerical-methods', 'polynomials']"
27609,"Given $n! = c$, how to find $n$?","I'm dealing with a time-complexity problem in which I know the running time of an algorithm: $$t = 1000 \mathrm{ms} .$$ I also know that the algorithm is upper bounded by $O(n!)$. I want to know the approximate size of the input $n$ based on this: $$ f(n) = n! = t $$
$$ f^{-1}(t) = n = ? $$","['factorial', 'algebra-precalculus']"
27616,Number of abelian groups of order $p^n$,"If $p$ is prime, determine the number of abelian groups of order $p^n$ for each $1\leq n\leq8$ (I assume that ""up to isomorphism"" should be included somewhere in the question for the sake of precision...) Could someone please review/confirm my work? n = 1: $\mathbb{Z}_p $ n = 2: $\mathbb{Z}_{p^2}$ and $\mathbb{Z}_p\times \mathbb{Z}_p$ n = 3: $\mathbb{Z}_{p^3}$,  $\mathbb{Z}_{p^2}\times \mathbb{Z}_p$, and $\mathbb{Z}_p\times \mathbb{Z}_p \times\mathbb{Z}_p$ n = 4: $\mathbb{Z}_{p^4}$, 
  $\mathbb{Z}_{p^3} \times \mathbb{Z}_p$, $\mathbb{Z}_{p^2}\times \mathbb{Z}_{p^2}$,  $\mathbb{Z}_{p^2}\times \mathbb{Z}_p \times \mathbb{Z}_p$, and $\mathbb{Z}_p\times \mathbb{Z}_p \times\mathbb{Z}_p \times \mathbb{Z}_p$ et cetera I am simply considering all the options for when the largest exponent of $p$ is $n$, then $n-1$, and so on. How does this look? Thanks! (Apparently I don't know how to ""end a quote""...)","['abelian-groups', 'abstract-algebra', 'group-theory', 'finite-groups', 'p-groups']"
27621,Basis for $\mathbb{Z}^2$,"Let $x = (a, b), y = (c, d) \in \mathbb{Z}^2$. What is the condition on $a, b, c, d$ so that ${x, y}$ is a basis? My answer: $ad\neq bc$ and $gcd(a, c) = gcd(b, d) = 1$. The first condition ensures that they aren't the same vector; the second ensures that we can actually ""get"" all of the integer values/lattice points. Is this correct? Thanks.","['abstract-algebra', 'gcd-and-lcm']"
27625,Cycles of Specified Length in a Graph,"Let $G=(V,E)$ be a graph, and $A$ be its adjacency matrix. Define $n = |V|$. Given $A$ and a natural number $m \le n$, I'm interested in the following problem: How many simple cycles of length $m$ exist in $G$? By simple cycle, I mean no repeated vertices along the cycle is allowed (other than the starting and ending vertices, which coincide). The problem is NP-hard. However, I'm not asking its complexity; I'm merely interested in whether there is a closed-form expression for computing it. (Thus, computing the expression can be NP-hard.)","['graph-theory', 'combinatorics']"
27629,Topology exercises,"Can anyone suggest a collection of (solved) exercises in topology? Undergrad level, as a companion to Dugundji's Topology (although excellent it doesn't provide the solutions to the problems). Thanks.","['general-topology', 'book-recommendation', 'reference-request']"
27633,distance between consecutive primes (related  to Polignac's conjecture),"Is there an elementary(or not) proof that there are at least two consecutive primes which have difference $2n$ for every natural number $n$? i remind that Polignac's conjecture states that there should be infinite such pairs for every $n$ so it is a much more easy question to ask and of course should be valid. In addition, for every $n\in\mathbb N$, is there any $m_n\in\mathbb N$ such that there are  two natural numbers $k_1,k_2$ so that $k_2-k_1=2n$, $k_1, k_2$ are not divisible by $p_1,\dots p_{m_n}$, where $p_i$ is the $i$-th prime, and for every $k\in\{k_1+1,\dots k_2-1\}$ there exists a $p\in\{p_1,\dots p_{m_n}\}$ such that $p|k$? (elementary proof)","['prime-numbers', 'number-theory']"
27647,Is the sub-field of algebraic elements of a field extension of $K$ containing roots of polynomials over $K$ algebraically closed?,"If I have a field $K$ and an extension $L$ of $K$ such that all (non-constant)
polynomials in $K[X]$ have a root in $L$, is the set of algebraic elements 
of $L$ over $K$ (the sub-field of all the elements of $L$ which are roots of a polynomial in $K[X]$) algebraically closed ? Do you have a counterexample ?","['abstract-algebra', 'field-theory']"
27650,Finding the modified Green function for the Helmholtz equation,"I've been wrestling with this question for quite some time now, and the result was like 20 leaves of paper packed with scribbling...anyway, here's the question: I need to find the solution to the following source excited Helmholtz equation: $$(\nabla^2 + k^2)G_1(r,a) = -\delta(r-a)$$
With the boundary condition:
$$\left. G_1(r,a)\right|_{z=0} = 0$$ Now, the solution is given by the integral:
\begin{multline}
G_1(r,a)=\iiint G(r,r') \delta(r'-a) dV' \\ + \iint [G(r,r')\nabla'G_1(r',a)-G_1(r',a)\nabla'G(r,r')]\cdot \hat{n} dS'
\end{multline}
(The surface integral is on a closed surface, for some reason MathJaX doesn't understand what \oiint is)
The second term in the surface integral drops since it's $0$ on all the boundaries of the volume that interests us (which is $z \geq 0$). HOWEVER, I do not know what the gradient of $G_1$ is on $z=0$, and I understand that I'm free to chose it however I see fit as long as it only affects the $z<0$ region. Even simplifying the problem to a one dimensional problem I still can't solve it. I do know what the solution is, because I've been thought many years ago in introduction to Electromagnetics that when encountering a boundary over which the electrical potential is $0$, I need to mirror all my sources and give them a negative sign, and if I plug that solution back into the equation is does satisfy it and the boundary condition. But how THAT solution is supposed to be inferred from the mathematical problem is beyond me. Any help/hints/insights would be most welcome. (Sorry for the paucity of tags, I wanted to use ""Helmholtz"" and ""Green"" but I'm still a newbie so I can't create new tags...)","['ordinary-differential-equations', 'partial-differential-equations']"
27652,Find $\frac{\mathrm d^{100}}{\mathrm d x^{100}}\frac{x^2+1}{x^3-x}=$?,"$$f(x)=\frac{x^2+1}{x^3-x}$$
$$f^{(100)}(x)=?$$ I tried differnetiating once and twice, but did not see any pattern emerging and can't guess what the 100th derivative should be. EDIT so decomposing this as $$f(x)=-\frac{1}{x}+\frac{1}{x+1}+\frac{1}{x-1}
$$  does the job. Thanks for the hints! (edit: Sivaram has a complete calculation) Although a similar approach would greatly simplify this (next) problem can someone tell me what is wrong with my approach My usual line of attack is to use Taylor expansion. For example the next problem in the same list asks for the $100^{th}$ derivative of $$\frac{1}{x^2-3x+2}$$ at $x=0$ within 10% relative error. NOTE :The above is a mistype, the following attempt is for $\frac{1}{x^2+3x+2}$. A better general approach, which is what I was looking for is described in the answer posted below. I know I can expand in a Maclaurin series $$\frac{1}{x^2+3x+2}=\frac{1}{2} (1+\frac{x^2+3x}{2} + (\frac{x^2+3x}{2})^2 +\cdots)$$ After taking 100 derivatives I would be left to differentiate the following. $$\frac{1}{2}((\frac{x^2+3x}{2})^{50}+(\frac{x^2+3x}{2})^{51}+\cdots)$$ $$=\frac{1}{2}\left(\frac{\sum_{k=0}^{50}{{50}\choose{k}}3^{50-k} x^{50+k}
}{2^{50}}+\frac{\sum_{k=0}^{51}{{51}\choose{k}}3^{51-k} x^{51+k}
}{2^{51}}+\cdots\frac{\sum_{k=0}^{100}{{100}\choose{k}}3^{100-k} x^{100+k}
}{2^{100}}\right)$$ Because anything on either side of these values would disappear when i take the hundreth derivative at $x=0$ . And it is also easy to sea that I will get exactly one term from each of the sums, so I get an answer, $$=100!\sum_{k=0}^{50}\frac{3^{2k}}{2^{50+k}}$$ Which is wrong, well because the answer is too huge and Im to find a number within 10%. Can someone tell me where I went wrong, and if there is a cleaner way to approach these problems.",['calculus']
27656,Eigenvalues of product of matrices,"If $\mathbf{A}_{n\times n}$ is a positive semi-definite matrix with eigenvalues $\{\alpha_k\},\ k\in\{1,...,n\}$, and $\mathbf{B}_{m\times n}$ is an arbitrary matrix with singular values $\{\beta_k\},\ k\in\{1,...,\min(m,n)\}$, can anything be said about the singular values $\{\gamma_k\},\ k\in\{1,...,\min(m,n)\}$ of the matrix $\mathbf{\Gamma}=\mathbf{BA}$? Is there a way I can relate $\gamma_k$ to $\alpha_k$ and $\beta_k$?","['matrices', 'eigenvalues-eigenvectors']"
27660,Why do the Localization of a Ring,"This question may be a bit vague, but neverthless, i would like to see an answer. Wikipedia tells me that: In abstract algebra, localization is a systematic method of adding multiplicative inverses to a ring. It is clear that for integral domains , we have the Field of Fractions , and we work on it. It's obvious that a Field has multiplicative inverses. Now if we consider any arbitrary ring $R$, by the definition of localization, it means that we are adding multiplicative inverses to $R$ thereby wanting $R$ to be a division ring or a field . My question, why is it so important to look at this concept of Localization. Or how would the theory look like if we never had the concept of Localization.","['ring-theory', 'abstract-algebra']"
27665,When is an affine bundle trivial?,"Let $k$ be a field, not necessarily algebraically closed, not necessarily of characteristic 0 (actually, the example I have in mind is $k=F_2$).  Let $V,W$ be varieties over $k$, and $W\to V$ a morphism defined over $k$, such that every fiber is isomorphic to an affine space of the same dimension. What conditions imply that $W$ is actually $V\times A^n$?",['algebraic-geometry']
27667,"For finite abelian groups, show that $G \times G \cong H \times H$ implies $G \cong H$","Let $G$ and $H$ be finite abelian groups such that $G \times G \cong H \times H$. Then $G \cong H$. I was going to just write the hypothesis as $G^2 \cong H^2$ and take square roots on both sides, but I don't think that would suffice (and neither would saying ""true"" a la Myself !)... The hypothesis tells us that there is an isomorphism, say $f: G \times G\to H \times H$. I would like to use this to come to the conclusion that there is a bijective homomorphism $g: G \to H$.  (I will be using additive notation...) Since $f((a, b) + (c, d)) = f(a,b) + f(c, d)$ for all $a,b \in G$ and $c, d \in H$, I was thinking about choosing an arbitrary $a, b \in G$ and calculating: $$
f((a, 0) + (b, 0)) = f(a, 0) + f(b, 0) \\ \Rightarrow
f(a + b, 0) = f(a, 0) + f(b, 0).
$$ I basically need to define g in such a way that it extracts the first dimension from the equation. Clearly (I think!), g will invoke f in some way. Can I have a tip on this? It's probably very simple, but I'm not sure how to express it symbolically. I might not need to show that g is bijective if I can say something to the effect of ""this routine verification is straightforward and left to the reader"", but I'm afraid I might not be able to perform such a verification if put on the spot! Here's a stab: f injective $\Leftrightarrow f(a,b) = f(c,d) \Rightarrow (a, b) = (c, d) \Rightarrow a = c \wedge b = d$ So by definition of g (forthcoming...), g(a) = g(c) implies that a = c. Surjective : For all $x, y \in H$, there exists $a, b \in G : f(a,b) = (x, y)$ Man, this really seems trivial, but without my definition of g, I feel like I'm handwaving... Thanks again, guys!","['finite-groups', 'group-theory', 'abstract-algebra', 'abelian-groups']"
27690,convergence of the maximum of a series of identically distributed variables,"My friend and I have been stumped on this problem for a little while and I thought asking for tips couldn't hurt (we did ask the teacher, but we got other problems after) Here is the question : Let $\{X_n\}_{n \geq 1}$ be a sequence of random variables defined on the same probability space $(\Omega, F, \mathbb{P})$ with the same law of finite expected value (E(|X_1|)<\infty ). Let $Y_n = n^{-1} \max_{1 \leq i \leq n} |X_i|$. Show that $\lim_{n\rightarrow \infty} E(Y_n) = 0$ and $Y_n \rightarrow 0$ almost surely. We have ideas of many parts of the proof, for example for the first one it would suffice to show that the expected value of the max of all $|X_i|$ is finite... and since the max is one of the $|X_i|$ for each $\omega \in \Omega$ it seems reasonable but we're not sure how to show it. We also tried splitting the integral for the expected value into a partition of $\Omega$ considering the sets on which $X_i$ is the max, but didn't get too far with that. For the second part, I think we could show it if we knew that $X_i(\omega)$ diverges for only a measure 0 set, but it's not that obvious (I think). Any pointers to the right direction appreciated!",['measure-theory']
27692,How can I find the nth exponent of the matrix using the diagonalization algorithm?,"Hey guys, simple question in linear algebra. I want to find the nth exponent of this matrix: $$
\left[
\begin{array}{cc}
 1 & 1 \\
0 & 2
\end{array}
\right]
$$ I'm trying to use the diagonal algorithm, first by finding the eigenvalues, so I get $t_1=1$, and $t_2=2$, and than I realize that the matrix is not diagonalizable... So how can I still calculate the nth exponent? Thanks","['linear-algebra', 'eigenvalues-eigenvectors']"
27713,Metric connection,"How to prove that any vector bundle with a fiber metric g admits a metric connection? It seems I should use partition of unity, but I have no idea how to proceed. Also it seems there are two definitions of connection on a vector bundle E, one is that 
$\nabla: \Gamma(TM) \times \Gamma(E) \to \Gamma$(E),
and the other one is that $\nabla: \Gamma(E) \to \Gamma(E) \otimes\Gamma(T^\ast M)$, and they should be equivalent. But I don't see how to use this to show the following equations are equivalent: If there is a fiber metric g on vector bundle E, then $d(g(u,v))=g(\nabla u,v)+g(u,\nabla v)$ for all $u,v\in\Gamma(E)$ is equivalent to $X(g(u,v))=g(\nabla_X u,v)+g(u,\nabla_X v)$ for all $X\in TM$, is it because that
$d(g(u,v))(X)=X(g(u,v))$? 
I am just not sure about how vector fields act on metrics.",['differential-geometry']
27719,"What is $\gcd(0,a)$, where $a$ is a positive integer?","I have tried $\gcd(0,8)$ in a lot of online  gcd (or hcf) calculators, but some say $\gcd(0,8)=0$, some other gives $\gcd(0,8)=8$ and some others give $\gcd(0,8)=1$. So really which one of these is correct and why there are different conventions?","['elementary-number-theory', 'group-theory', 'abstract-algebra', 'gcd-and-lcm']"
27730,Finding SVD efficiently for $AB^T$,"I posted this on cs theory yesterday but did not get an answer and hence I am posting here. I have a low rank matrix given as $AB^T$ where $A,B \in \mathbb{R}^{n \times p}$ and $p \ll n$. (I know $A$ and $B$ separately) One efficient way to get the Singular Value Decomposition is to do a reduced QR on $A$ and $B$ i.e. $A = Q_A R_A$ and $B = Q_B R_B$. The above can be done in $\mathcal{O}(p^2n)$ cost. I could then compute the svd of $R_A R_B^T = U_1 \Sigma V_1^T$ which is $\mathcal{O}(p^3)$. Hence this will give me $$AB^T = (Q_A U_1) \Sigma (Q_B V_1)^T$$ The total cost is $\mathcal{O}(np^2)$. However, I am wondering if there are other efficient ways to go about doing this to reduce the coefficient infront of $p^2n$. If I am right, the coefficient infront of $np^2$ is $3$ if we go about doing QR. The reason why I am interested in minimizing the coefficient is that my $n$ is really ginormous. So if I were to implement it, a cost cutting on the coefficient of the $\mathcal{O}(n)$ could be significant. I am wondering if we could exploit the fact that the left singular vectors must span $A$ and the right singular vectors span $B$ i.e. we know that if $AB^T = U \Sigma V^T$, then $$A = U \alpha \text{ and } B = V \beta$$ and we want $\alpha \beta^T$ to be diagonal and $U$,$V$ needs to be unitary. EDIT What I essentially want is an exact rank $r<p$ approximation to the product $AB^T$ in the form $U_r V_r^T$. I am not looking for approximate or probabilistic algorithms.","['matrices', 'numerical-linear-algebra', 'algorithms']"
27731,Isoperimetric inequalities of a group,"How do you transform isoperimetric inequalities of a group to the of Riemann  integrals of functions of the form $f\colon \mathbb{R}\rightarrow G$ where $G$ is a metric group so that being $\delta-$hyperbolic in the sense of Gromov is expressible via Riemann integration? In other words, how do you define ""being $\delta-$hyperbolic group"" by using  integrals in metric groups? (Note: I am not interested in the ""Riemann"" part, so you are free to take commutative groups with lebesgue integration etc.)","['integration', 'geometric-group-theory', 'metric-spaces', 'group-theory', 'gromov-hyperbolic-spaces']"
27741,Largest $\sigma$-algebra on which an outer measure is countably additive,"If $m$ is an outer measure on a set $X$, a subset $E$ of $X$ is called $m$-measurable iff 
$$
m(A) = m(A \cap E) + m(A \cap E^c)
$$
for all subsets $A$ of $X$. The collection $M$ of all $m$-measurable subsets of $X$ forms a $\sigma$-algebra and $m$ is a complete measure when restricted to $M$. Is $M$ the largest $\sigma$-algebra on $X$ on which $m$ is a measure (i.e., on which $m$ is countably additive)?  If not, what is? Is $M$ the largest $\sigma$-algebra on $X$ on which $m$ is a complete measure?  If not, what is? I am especially interested in the case when $X$ is $\mathbb{R}$ or $\mathbb{R}^n$ and $m$ is the Lebesgue outer measure.  In this case $M$ is the Lebesgue $\sigma$-algebra. ADDED: Julián Aguirre (thanks!) has shown in his response below that the answer to the first question is yes when $X$ is $\mathbb{R}^n$ and $m$ is the Lebesgue outer measure.  Hence the answer to the second question in this situation is also yes.","['measure-theory', 'real-analysis', 'analysis']"
27743,"Does a ""Newton differential equation"" always have a solution?","One reason why the iteration $$x_{n+1}=x_n-\tan\;x_n$$ converges quickly for appropriate starting values is that this is nothing more than the Newton-Raphson iteration for $\sin\;x$. This got me thinking: given some arbitrary function $g(x)$, is there always a function $f(x)$ such that $$\frac{f(x)}{f^\prime(x)}=g(x)$$ or are there restrictions on the nature of $g(x)$ so that the differential equation has a solution?",['ordinary-differential-equations']
27753,Given a matrix $A$ find a matrix $C$ such that $C^3$=$A$,"This is a question I had on a test, we were told not to use brute-force and figure out a smart way to solve the problem. We have a matrix $A =$ $\displaystyle\begin{bmatrix}
2 & 3\\ 
3 & 2
\end{bmatrix}$ . Find a matrix $C$ such that $C^{3}$ = $A$ . What is the 'smart not brute-force' way to solve this, without picking numbers, looking for patterns and so on? it was in eigenvalues section"" in the end?",['linear-algebra']
27775,Why do we restrict the definition of Lebesgue Integrability?,"The function $f(x) = \sin(x)/x$ is Riemann Integrable from $0$ to $\infty$, but it is not Lebesgue Integrable on that same interval.  (Note, it is not absolutely Riemann Integrable.) Why is it we restrict our definition of Lebesgue Integrability to absolutely integrable?  Wouldn't it be better to extend our definition to include ALL cases where Riemann Integrability holds, and use the current definition as a corollary for when the improper integral is absolutely integrable?","['improper-integrals', 'integration', 'measure-theory', 'lebesgue-integral', 'intuition']"
27792,"Positioning three circles, all of them touching each other","There are three circles, all of them touching each other. The bottom two circles are laying on an imaginary floor, such that they touch the line g=-r as well. Given are all three radii, r1 (A), r2 (B) and r3 (C).
Assuming circle A has its center on (0/0), B has its center on (2 sqrt(r1 * r2), r2 - r1). I am now supposed to find the coordinates of C. Is this a know problem and has an easy/straighforward solution? I can't seem to find a nice approach.","['trigonometry', 'triangles', 'circles']"
27794,Non-integer exponents,Can you use noninteger powers Like is $x^{8.3} / x^{2.2} = x^{6.1}$?,"['exponentiation', 'algebra-precalculus']"
27804,Applications of the wreath product?,"We recently went through the wreath product in my group theory class, but the definition still seems a bit unmotivated to me. The two reasons I can see for it are 1) it allows us to construct new groups, and 2) we can use it to reconstruct imprimitive group actions. Are there any applications of the wreath product outside of pure group theory?","['wreath-product', 'applications', 'group-theory']"
27805,Is it possible to simplify $\frac{2^{2x-1} - 2^{x-1}}{2^{2x-1}}$?,Is it possible to simplify this expression? $\frac{2^{2x-1} - 2^{x-1}}{2^{2x-1}}$,"['exponentiation', 'algebra-precalculus']"
27808,A linear operator commuting with all such operators is a scalar multiple of the identity.,"The question is from Axler's "" Linear Algebra Done Right "", which I'm using for self-study. We are given a linear operator $T$ over a finite dimensional vector space $V$. We have to show that $T$ is a scalar multiple of the identity iff $\forall S \in {\cal L}(V), TS = ST$. Here, ${\cal L}(V)$ denotes the set of all linear operators over $V$. One direction is easy to prove. If $T$ is a scalar multiple of the identity, then there exists a scalar $a$ such that $Tv = av$, $\forall v \in V$. Hence, given an arbitrary vector $w$, $$TS(w) = T(Sw) = a(Sw) = S(aw) = S(Tw) = ST(w)$$ where the third equality is possible because $S$ is a linear operator. Then, it follows that $TS = ST$, as required. I am, however, at a loss as to how to tackle the other direction. I thought that a proof by contradiction,  ultimately constructing a linear operator $S$ for which $TS \neq ST$, might be the way to go, but haven't made much progress. Thanks in advance!","['vector-spaces', 'linear-algebra', 'self-learning']"
27841,"How to calculate $E[(\int_0^t{W_sds})^n], n \geq 2$","Let $W_t$ be a standard one dimension Brownian Motion with $W_0=0$ and $X_t=\int_0^t{W_sds}$.
With the help of ito formula, we could get
$$E[(X_t)^2]=\frac{1}{3}t^3$$ 
$$E[(X_t)^3]=0$$ When I try to employ the same method to calculate the general case $E[(X_t)^n]$, I got stuck.
I guess $X_t$ should be normal distribution since it could be the limit of the following
$$\lim_{n\rightarrow \infty}{\sum_{i=0}^{n-1}{W_{t_i}(t_{i+1}-t_i)}},$$ where $ W_{t_i}\sim norm(0,\sqrt{\frac{t_i}{n}}).$ If it is true, the problem would be trivial. Update: Thanks for all the suggestions. Now I believe $X_t$ is a Gaussian process. How about for this integral
$$Y_t=\int_0^t{f(W_s)ds}$$
if we assume that $f$ is some good function, say polynomial or exponential, i.e 
$$Y_t=\int_0^t{e^{W_s}ds}$$
$$Y_t=\int_0^t{[a_n(W_s)^n+a_{n-1}(W_s)^{n-1}+...+a_0]ds}$$","['stochastic-processes', 'probability-distributions', 'probability']"
27842,Help with definition of group realization,"I am reading a document it says A group realization is a map from elements of G to transformations of a space M that is a group homomorphism , i.e. it preserves the group multiplication law. Thus if $T:G \rightarrow T(M): g \mapsto T(g)$ where $T(g)$ is some transformation on M, then T is a homomorphism if $T(g_1 g_2) = T(g_1)T(g_2)$. I have got a bad feeling about this definition. If T(M) is transformations on M then how can $T$ also be a map from $G$ to $T(M)$? This document page 4.","['notation', 'group-theory']"
27859,Arc length of the Cantor function,"How does one find the arc length of the Cantor function? Wikipedia says that the length is $2$ . I can ""see"" that the length is at most $2$ by a simple triangle inequality argument. I am struggling to come up with a partition $P$ such that the arc length is at least 2. I tried a partition of the form $\{  1/ 3^n : 0 \le k \le n \}$ but I guess I am making some mistake in my calculation so that I get the length as $3/4$ instead of close to $2$ .","['real-analysis', 'cantor-set']"
27865,What's the connection between the Laplace transform and the Fourier transform?,"Both the Laplace transform and the Fourier transform in some sense decode the ""spectrum"" of a function. The Laplace transform gives a power-series decomposition whereas the Fourier transform gives a harmonic (or loop-based) decomposition. Are there deep connections between these two transforms? The formulaic connection is clear, but is there something deeper? (Maybe the answer will involve spectral theory?)","['spectral-theory', 'fourier-analysis', 'functional-analysis', 'integral-transforms']"
27871,Cardinalities of $\mathbb{R^{2}}$ and $\mathbb{C}$ and isomorphisms,"Using the tools of linear-algebra it seems like  $\mathbb{R^{2}}$ or $\mathbb{R}$ $\times$ $\mathbb{R}$ is isomorphic to $\mathbb{C}$, since both of the spaces are of dimension 2. Does this mean that the these sets are equinumerous? $\mathbb{R}$ is a subspace of $\mathbb{C}$, but I am wondering how could one prove that they are equinumerous without using isomorphisms (assuming they imply equinumerosity); would this require cardinal arithmetic? Thanks.","['linear-algebra', 'elementary-set-theory']"
27877,Galois groups of polynomials and explicit equations for the roots,Lets say I have calculated the galois group of some polynomial and I also have the subgroup structure. What's an effective procedure to turn the group into equations for the actual roots of the polynomial in terms of its coefficients assuming the galois group of the polynomial is solvable?,"['roots', 'abstract-algebra', 'polynomials', 'galois-theory', 'group-theory']"
27880,Isosceles trapezoid,"I was solving an exercise on Isosceles trapezoid whose diagonal was given, and I note that If I draw a diagonal in the isosceles trapezoid I got two triangles To determine the area of the triangles I draw their heights, which are perpendicular to the diagonal. The problem arises when I suppose that the sum of the height of the two triangles is equal to the diagonal. Funny thing I got the correct anwser doing that way, however it was only intuitive, not accurate. But how to prove if $(a = b + c)$  is true or false ?","['geometry', 'euclidean-geometry']"
27905,Convergence of the series $\sum\limits_{n=1}^{\infty }(-1)^{n+1} \frac{\sin^2(n)}{n}$,I would like to know how to prove the convergence (or not) of the following serie: $\sum\limits_{n=1}^{\infty }(-1)^{n+1} \frac{\sin^2(n)}{n}$ Thank you in advance for any suggestion.,['sequences-and-series']
27907,Countability of local maxima on continuous real-valued functions,"I am working through a bank of previous exams and couldn't figure a problem out to my satisfaction. Let $f(x) : \mathbb{R} \to \mathbb{R}\,$ be a continuous function. Show that $f$ can have at most countably many strict local maxima. Assume that $f$ is not monotone on any interval. Then show that the local
  maxima of $f$ are dense in
  $\mathbb{R}$.","['real-analysis', 'analysis']"
27914,Intuition behind arc length formula,"I understand the arc length formula is derived from adding the distances between a series of points on the curve, and using the mean value theorem to get: $ L = \int_a^b \sqrt{ 1 + (f'(x))^2 } dx $ But is there an intuition here I'm missing?  Something about taking the integral of the derivative seems like it should mean something..","['calculus', 'intuition', 'functions']"
27916,Squared binomial coefficient,"I've got the following finite sum:
$s_{n}=\sum\limits_{k=0}^{n}\binom{n}{k}^2p^k$ (esp. if $p$ is a function of $n$, like $p=\frac1{n}$), which can be rewritten as $s_{n}=\sum\limits_{k=0}^{n}\binom{n}{k}\sqrt p^k\binom{n}{k}\sqrt p^k$. Using the generating function approach (from Graham, Knuth and Patashnik) with each of the polynomials in the sum, I get the expression $(1+\sqrt p x)^{2n}$ and the coefficient at the n-th term $x$ turns out to be $\binom{2n}{n} \sqrt p^n$. But comparing this result to the computational value, it turns out to be incorrect. Where did I make a mistake? Are there certain limitations on the generating function method?","['generating-functions', 'binomial-coefficients', 'combinatorics']"
27921,Joint moments of Brownian motion,"My approach to this SE question uses the following joint moments of
Brownian motion. For $n=1,2$ they are obvious and well-known, the others 
are not terribly hard to work out. Is there a reference where these 
formulas are given, or/and is there a  pattern to the coefficients? Fix $t_1\leq t_2\leq t_3\leq\cdots \leq t_n$.
For odd values of $n$ we have $\mathbb{E}[W(t_1)\ W(t_2) \cdots W(t_n)]=0$
while for even values of $n$ we get \begin{eqnarray*}
\mathbb{E}[W (t_1)\ W(t_2)]&=& t_1 \cr
\mathbb{E}[W (t_1)\ W(t_2)\ W(t_3)\ W(t_4)]&=& 2t_1 t_2+t_1t_3 \cr
\mathbb{E}[W (t_1)\ W(t_2)\ W(t_3)\ W(t_4)\ W(t_5)\ W(t_6)]&=& 2t_1t_2t_5+t_1  t_3  t_5 +4 t_1  t_2  t_4 +2 t_1  t_3  t_4 +6 t_1  t_2  t_3 
\end{eqnarray*} I suppose everything about Brownian motion
has been worked out, but I can't find this in any of my books. 
It's not very important, but I'm just curious!","['reference-request', 'stochastic-processes', 'probability', 'brownian-motion']"
27922,A five-part problem that uses ends and the Cantor set to prove that there are $c$ non-homeomorphic connected open subsets of $\mathbb{R}^2$,"My question comes from Spivak's ""Comprehensive Introduction to Differential Geometry Vol 1"" (It's Chapter 1, Problem 24). Background : Let $X$ be a connected, locally connected, locally compact, and hemicompact Hausdorff space. And end of $X$ is defined to be a function $e$ that assigns to each compact set $C$ of $X$ a connected component of $X-C$, in such a way that if $C \subset D$, then $e(D) \subset e(C)$. Let $E(X)$ be the set of all ends of X. In a previous problem, I've shown that the set $X \cup E(X)$ can be given a topology with basis elements being the open sets of X together with sets $N(C,e)=e(C) \cup \{f\in E(X) | e(C)=f(C)\}$ for each end $e$ and compact set $C$. (Thanks to Henno Brandsma here for suggesting the hemicompactness condition that makes this work). This topology is compact and Hausdorff. Problem : The problem has five parts (a through e). I'd like to find a proof of b,d, and e . I think I have a solution for a and c, described further below. a) Show that it is possible for $\mathbb{R}^2-A$ and $\mathbb{R}^2-B$ to be homeomorphic even though $A$ and $B$ are non-homeomorphic closed subsets. b) If $A \subset \mathbb{R}^2$ is closed and totally disconnected, then $E(\mathbb{R}^2-A)$ is homeomorphic to $A$. Hence if $A$ and $B$ are non-homeomorphic totally disconnected closed subsets, $\mathbb{R}^2-A$ and $\mathbb{R}^2-B$ are non-homeomorphic. c) The derived set $A'$ of a set $A$ is defined to be the set of non-isolated points of $A$. Show that for each $n$, there is a subset $A_n$ of $\mathbb{R}$ such that the $n$'th derived set ${A_n}^{(n)}$ of $A_n$ consists of a single point. d) There are $c$ non-homeomorphic closed, totally disconnected subsets of $\mathbb{R}^2$.(Hint: Let $C$ be the cantor set, and $c_1<c_2<c_3\ldots$ a sequence of points in $C$. For each sequence $n_1<n_2<n_3\ldots$, one can add a set $A_{n_i}$ such that its $n_i$'th derived set is $\{c_i\}$.) e) There are $c$ non-homeomorphic connected open subsets of $\mathbb{R}^2$. What I've got so far :
For part a , $A=$point, and $B=$closed disk should solve the problem.
For part c , I think that we can take the $1/n$ sequence (and 0) and add smaller such sequences that converge to each of the points of the original. This can be done recursively. For part b , I have a feeling that the statement of the problem should be to prove that $E(\mathbb{R}^2-A)$ is homeomorphic to the one-point compactification of $A$ (call it $\tilde{A}$). (The reason I think this is because $A$ might not be compact, but $E(\mathbb{R}^2-A)$ always is. Using the one-point compactification should also take care of the unbounded end.) I think the idea is to define a function $\tilde{A}\to E(\mathbb{R}^2-A)$ that takes a point $a$ to an end $e$ defined by $e(C)$= the component of $\mathbb{R}^2-A-C$ whose closure in $\mathbb{R}^2$ contains $a$, and takes $\infty$ to the end $e$ defined by $e(C)=$the unique unbounded component of $\mathbb{R}^2-A-C$ (I'm not sure why there's a unique one, but it feels like removing a closed totally disconnected set from a connected open set should leave a connected open set). But I couldn't prove that this function is well-defined, much less bijective, continuous, or open. If my guess about the one-point compactification in right, this may mess up the ""hence"" part of b, since I remember reading somehere that it's possible for two non-homeomorphic spaces to have isomorphic one-point compactifications. For part d , I believe we can take the cantor set, and add the sets in part c vertically over the $c_i$. But I couldn't prove that the result is totally disconnected or closed, or that there are $c$ non-homeomorphic ones. For part e , I believe that it's enough to prove that the complements of the sets used in part d are connected. Edit: Beni Bogosel has provided a nice answer to part c below.",['general-topology']
27928,Find the value of 'x' in a product of exponentiated logarithms,"Find the value of '$x$' if, $$\large \left(\frac{1}{2^{\log_x 4}} \right) \cdot \left( \frac{1}{2^{\log_x 16}} \right) \cdot \left(\frac{1}{2^{\log_x 256}} \right) \cdots = 2 $$ I tried to make it simple by resulting series is not converging, the suggested answers in my module is either ($2,\frac{1}{2},4,\frac{1}{4}$) Also I tried this and this but  not the correct answer in either case.","['roots', 'logarithms', 'sequences-and-series', 'algebra-precalculus']"
27929,How to find $f(x)$ if $df(x)/dx=f(x)$,"How to find $f(x)$ if $\frac{df(x)}{dx}=f(x)$?
I know $c e^x$ is a solution, but how does one find it and how to prove it is the complete solution?","['ordinary-differential-equations', 'calculus']"
27938,Simple stats question-correlation coefficient,"Let's say we have two exams, each out of 50 points. The correlation rate between them is 0.75. If the teacher decides to add 10 points to the results of the first test, what will happen to the corr. rate? The way I see it, the correlation should decrease, but by how much? Would it decrease by 1/5=20%? And the result would have been the same even if she subtracted 10 points from the first test, correct?",['statistics']
27944,The Use of the Axiom of Choice in an Elementary Proof,"I wanted to give some of the new undergrad analysis students the following problem: given the real numbers (with the standard topology, as they'd expect) one cannot have an uncountable set such that every point is an isolated point.  A few of my fellow grad students attempted solutions first. A sketch of a potential proof was given as follows: take each point and create as large a ball as is possible without it containing any other point of the set, so say $(a-\alpha, a+\alpha)$.  Then construct corresponding balls with a smaller radius: $(a-\frac{\alpha}{2}, a+\frac{\alpha}{2})$.  These new balls will intersect each other trivially.  Then choose a rational point from each of these new balls.  This set will be in one-to-one correspondence with the balls which are in one-to-one correspondence with the points of the original set. After presenting this proof, it was argued that it requires the (non-finite) axiom of choice (as we are picking one point from a potentially uncountable set).  We modified the proof by using the density of the rationals to pick rationals $p_{a},q_{a}$ such that $(a-p_{a},a+q_{a})\subseteq (a-\frac{\alpha}{2}, a+\frac{\alpha}{2})$, and then have our function pick the ""left-most"" end-point.  It was still argued that this used the axiom of choice. Because I am not entirely familiar with what constitutes use of the axiom of choice, I wanted to open the question up to all of you.  Do these proofs require the (non-finite) axiom of choice?  If so, is there a proof you know of which does not require it?","['general-topology', 'axiom-of-choice']"
27947,Topologies on the space $\mathcal D'(U)$ of distributions,"In my analysis lecture I am given a topology on the space of distributions as follows: Let $u_k$ be a sequence in $\mathcal D'(u)$, $u \in \mathcal D'(u)$. We say $u_k \rightarrow u$, if $\forall \phi \in \mathcal D(u) : u_k(\phi) \rightarrow u(\phi)$. This is the weak-$*$-topology on $\mathcal D'(u)$. It seems lecturers don't care too much about the topology of $\mathcal D'(u)$, hence I wonder whether there are stronger topologies on $\mathcal D'(u)$.","['distribution-theory', 'functional-analysis']"
27958,Adjunction space is a pushout,"I would like to show that the diagram $$\begin{array}{}
A & \stackrel{f}{\longrightarrow} & Y \\
i \downarrow &  & \downarrow {\phi_2} \\
X & \stackrel{\phi_1}{\longrightarrow} & X \coprod_f Y \end{array}$$ where $i:A \to X$ is an inclusion is a pushout. Here $A$ is a closed subset of $X$, all maps given are continuous and $X \coprod_f Y$ is the disjoint union $X \coprod Y$ quotient by the equivalence generated by $\{(a,f(a)) \in (X \coprod Y) \times (X \coprod Y): a \in A\}$ (call the equivalence relation $\sim$) So I start with $\nu: X \coprod Y \to X \coprod_f Y$ (the natural map) and define $\phi_1 = \nu | X$, and  $\phi_2 = \nu | Y$. Then for $a \in A$, $\phi_1(i(a)) = \nu(i(a)) = \nu(f(a)) = \phi_2(f(a))$ and the diagram is commutative. The other part is to show that this is unique. So let $Q$ be another space such that there exists $\alpha_1:X \to Q$, $\alpha_2:Y \to Q$. We seek a $u: X \coprod_f Y \to Q$ Define the function $\Theta:X\coprod_f Y \to Q$ with $\Theta | X = \alpha_1$ and $\Theta | Y = \alpha_2$. Then for $a \in A$, $\Theta(i(a)) = \alpha_1(i(a)) = \alpha_2(f(a)) = \Theta(f(a))$. So this means that $\Theta$ maps elements of the equivalence class $\sim$ from $X \coprod_f Y \to Q$ (maybe I am not saying the last bit clearly, but I think it is clear what I mean!) Is this all reasonable? I only ask because this whole commutative diagram thing is very new to me...","['general-topology', 'category-theory']"
27963,Difficulty on understanding conditional probability,"I have some confusion when understanding the concept of conditional probability. Given any two random variables $X$ and $Y$ and any two constants $m$ and $k$, Is it true that $$P(Y-X=m | Y > X) = P(Y = m+k | Y > k)? $$ My guess is it is not always true,
because $$\begin{align*}
P(Y-X=m | Y > X) & = \sum_{k} P(Y-X=m, X=k | Y > X) \\
& = \sum_{k}  P(Y-X=m | X=k, Y > X) P(X=k | Y > X) \\
& = \sum_{k}  P(Y-k=m | Y > k) P(X=k | Y > X).
\end{align*}$$ What if $X$ and $Y$ are i.i.d.?
Even further, how about when $X$ and $Y$
are i.i.d. with some memoryless
distribution, i.e. exponential or
geometric distribution? In my previous post under the
setting of  $X$ and $Y$ being i.i.d.
with geometric distribution, Henry
wrote $$P(Y-X=m | Y > X) = P(Y = m+k
| Y > k),$$ about which he said in his comment it is true because  ""$k$ is
merely shorthand for"" $X$. But I
still don't quite understand that yet. If Y is a random variable subject to
a memoryless distribution, i.e.
exponential or geometric
distribution and X is any other
random variable with any
distribution, is it true that
$$P(Y>X+m | Y > X) = P(Y > m)$$ Thanks for your help!","['probability-theory', 'probability']"
27967,Curvature of planar implicit curves,"I am trying to understand how the curvature equation $$\kappa = -\frac{f_{xx} f_y^2-2f_{xy} f_x f_y + f_x^2 f_{yy}}{(f_x^2+f_y^2)^{3/2}}$$ for implicit curves is derived. These curves arise from equalities such as $f(x,y)=0$. I found this on the net: http://www.cad.zju.edu.cn/home/zhx/GM/001/00-rep_dg.pdf I can follow almost everything here until pg 49, then the author jumps to the final equation and I have no idea how he's done it. Can anyone help, or point to other possible derivations? I understand the parametric form of curvature equation which is $\kappa = | \frac{d\vec{T}}{ds} |$ where $\vec{T}$ is unit tangent, if any parallels need to be made to that subject, just in case. And one more question: How do I expand the term below? $$\frac{\partial}{\partial x} \bigg( \frac{f_y}{\sqrt{f_x^2 + f_y^2}} \bigg)$$ Do I have to use the Quotient Rule? $$\frac{d}{dx}(\frac{u}{v}) = \frac{v \frac{du}{dx} - u \frac{dv}{dx}}{v^2}$$ and in that case, I guess I would need to derive $\frac{\partial}{\partial x}(\sqrt{f_x^2+f_y^2})$. Would this be $\frac{1}{2}\frac{2f_x f_{xx} + 2f_y f_{yx}}{\sqrt{f_x^2+f_y^2}}$ Thanks again","['plane-curves', 'curvature', 'differential-geometry']"
27985,How does a bounded irrotational vector field imply continuity of the scalar potential,Supose we have a vector field $E:R^3\rightarrow R^3$ with the property $\nabla\times E=0 \Longleftrightarrow E=-\nabla \phi$ where $\phi:R^3\rightarrow R$ How does the boundedness of $E$ imply the continuity of $\phi$ I can solve this physically for a certain case by assuming a rectangular curve through a surface across which $E$ is discontinuous as $\nabla\times E=0 \Longleftrightarrow \oint E.l dl=0$. So even though $E$ is discontinuous its associated scalar ($\phi$) is still continuous. But the argument above has been hinted to apply in general and I am having trouble getting it mathematically.,['multivariable-calculus']
27989,Time until a consecutive sequence of ones in a random bit sequence,"This a reformulation of a practical problem I encountered. Say we have an infinite sequence of random, i.i.d bits. For each bit $X_i$, $P(X_i=1)=p$. What is the expected time until we get a sequence of $n$ 1 bits? Thanks!","['stochastic-processes', 'probability']"
28000,Prove that 2 of 3 triangles sharing one side overlap,"Let $C, D, E$ be three non-degenerate triangles in $\mathbb R^2$. Let $c, a, b$ be the vertices of $C$, let $d, a, b$ be the vertices of $D$, and let $e, a, b$ be the vertices of $E$. I want to show that there is one point contained in the interior of at least two of the given triangles. Here are my thoughts: If two of the triangles are the same then we're done, so suppose without loss of generality that $C$ and $D$ are different. Think of the triangles as simplices. By definition, this means that the sets $\{c - a, b - a\}$, $\{d - a, b - a\}$, and $\{e - a, b - a\}$ are linearly independent. Since $C$ and $D$ are different, this must mean that $\{c - a, d - a\}$ is also linearly independent, hence it is a basis of $\mathbb R^2$. This means I can write $e - a = \gamma(c-a) + \delta(d-a)$ for some appropriate $\gamma,\delta$. Now any point on the triangle $E$ can be written as $\alpha a + \beta b + \epsilon e$ where $\alpha, \beta, \epsilon$ are nonnegative and sum to 1. Using the fact that $e - a = \gamma(c-a) + \delta(d-a)$, we can write any point on the triangle $E$ as $(\alpha + \epsilon(1-\gamma-\delta))a + \beta b + \gamma\epsilon c + \delta\epsilon d$. I was hoping to make this latter sum into a convex sum with just $a, b, c$ or $a, b, d$ by picking $\epsilon$ appropriately and therefore showing that $E$ overlaps with $C$ or $D$, but alas that doesn't work. So, is this the right way of going about this? Or is there a better approach?",['geometry']
28001,Common terms in general Fibonacci sequences,"Mathworld notes that ""The Fibonacci and Lucas numbers have no common terms except 1 and 3,"" where the Fibonacci and Lucas numbers are defined by the recurrence relation $a_n=a_{n-1}+a_{n-2}$. For Fibonacci numbers, $a_1=a_2=1$; for Lucas numbers, $a_1=1$, $a_2=3$. How do you prove mathworld's statement?","['recurrence-relations', 'sequences-and-series', 'elementary-number-theory', 'lucas-numbers', 'fibonacci-numbers']"
28018,On distributions over $\mathbb R$ whose derivatives vanishes,"Let $I \subset \mathbb R$ be open, $u \in \mathcal D'(I)$ be a distribution whose distributional derivatives vanishes (i.e. is zero for all test functions, which we may assume to be complex valued ). We show $\forall c \in \mathbb C: \forall \phi \in \mathcal D(I) : u(\phi) = \int c\cdot\phi dx$. (EDIT: Correctly, $c$ should be quantified with $\exists$. My question has been why the following proof doesn't allow for arbitrary complex $c$, which explains the preceding statement.) Proof: Let $\Psi \in D(I)$. $\Psi$ is the derivative of a test function iff $\int \phi dx = 0$. In that case $u(\Psi) = 0$. Let $h \in D(I)$ be arbitrary with $\int h dx = 1$. Now for every test function $\phi \in D(I)$ we see: $\phi - \int \phi dx \cdot h \in D(I)$ and $\int ( \phi - \int \phi dx h ) dx = 0$. therefore $u( \phi - \int \phi dx h ) = 0$, i.e. $u(\phi) = u(h) \int \phi dx$ $\square$ If this is not wrong, how can I interpret the fact that $h$ has been arbitrary? Note: This is part of a larger proof, which shows the same for non-one-dimensional domains.","['distribution-theory', 'analysis']"
28028,Proving that a series is convergent,"If I can show that $\displaystyle \sum_{k\geq1}a_k^2<\infty$, then is this an acceptable proof that $\displaystyle \sum_{k\geq1}a_k^2$ is convergent?","['sequences-and-series', 'real-analysis']"
28036,Relationship between eigendecomposition and singular value decomposition,"Let $A \in \mathbb{R}^{n\times n}$ be a real symmetric matrix. Please help me clear up some confusion about the relationship between the singular value decomposition of $A$ and the eigen-decomposition of $A$. Let $A = U\Sigma V^T$ be the SVD of $A$. Since $A = A^T$, we have $AA^T = A^TA = A^2$ and:
$$A^2 = AA^T = U\Sigma V^T V \Sigma U^T = U\Sigma^2 U^T$$
$$A^2 = A^TA = V\Sigma U^T U\Sigma V^T = V\Sigma^2 V^T$$ Both of these are eigen-decompositions of $A^2$. Now consider some eigen-decomposition of $A$ $$A = W\Lambda W^T$$ Then $$A^2 = W\Lambda W^T W\Lambda W^T = W\Lambda^2 W^T$$ So $W$ also can be used to perform an eigen-decomposition of $A^2$. So now my confusion:
It seems that $A = W\Lambda W^T$ is also a singular value decomposition of A. But singular values are always non-negative, and eigenvalues can be negative, so something must be wrong. What is going on?","['eigenvalues-eigenvectors', 'svd', 'symmetric-matrices', 'matrices', 'linear-algebra']"
28038,Expressing the product Ax as a linear combination of the column vectors of A,"Expressing the product Ax as a linear combination of the column vectors of $A$=
$\begin{bmatrix}
4 & 0 & -1\\ 
3 & 6 & 2\\ 
0 & -1 & 4
\end{bmatrix}$ $\vec{x}$=$\begin{bmatrix}
-2\\ 
3\\ 
5
\end{bmatrix}$ I get it now. They just want me to multiply the two vectors together. I end up with $\begin{bmatrix}
-13\\\ 
22\\\ 
17
\end{bmatrix}$",['linear-algebra']
28039,What is the easiest way to see that there are no nonconstant holomorphic forms on the Riemann Sphere?,"I've heard this result bandied about many times, and I know that it follows from e.g. the theory of divisors, but I'd like to see some simpler, straightforward ways of proving this fact.",['complex-analysis']
28045,expected area of a triangle determined by randomly placed points [duplicate],This question already has an answer here : The expected area of a triangle formed by three points randomly chosen from the unit square (1 answer) Closed 4 years ago . Three points are placed at independently and at random in a unit square. What is the expected value of the area of the triangle formed by the three points?,['probability']
28052,Comparing the Lebesgue measure of an open set and its closure,"Let $E$ be an open set in $[0,1]^n$ and $m$ be the Lebesgue measure. Is it possible that $m(E)\neq m(\bar{E})$, where $\bar{E}$ stands for the closure of $E$?","['measure-theory', 'examples-counterexamples', 'real-analysis']"
28061,Does $\det(A) \neq 0$ (where A is the coefficient matrix) $\rightarrow$ a basis in vector spaces other than $R^{n}$?,"I know that for a set of vectors $\{ v_{1}, v_{2}, \ldots , v_{n} \} \in \mathbb{R}^{n}$ we can show that the vectors form a basis in $\mathbb{R}^{n}$ if we show that the coefficient matrix $A$ has the property $\det(A) \neq 0$, because this shows the homogeneous system has only the trivial solution, and the non-homogeneous system is consistent for every vector $(b_{1}, b_{2}, \ldots , b_{n}) \in \mathbb{R}^{n}$. Intuitively, this concept seems applicable to all polynomials in $\mathbf{P}_{n}$ and all matrices in $M_{nn}$. Can someone validate this? edit: I think to make the intuition hold, $A$ must be defined as follows in $M_{nn}$: Let $M_{1}, M_{2}, ... , M_{k}$ be matrices in $M_{nn}$. To prove these form a basis for $M_{nn}$, we must show that $c_{1}M_{1} + c_{2}M_{2} + ... + c_{k}M_{k} = 0$ has the only trivial solution, and that every $n \times n$ matrix can be expressed as $c_{1}M_{1} + c_{2}M_{2} + ... + c_{k}M_{k} = B$. So I believe that for $M_{nn}$, $A$ must be defined as a $n^{2} \times n^{2}$ matrix where each row vector is formed from all the $(i, j)$ entries taken from $M_{1}, M_{2}, ... , M_{k}$ (in that order.) $\text{e.g. } A = \begin{pmatrix}
M_{1_{1,1}} & M_{2_{1,1}} & ... & M_{k_{1,1}} \\
M_{1_{1,2}} & M_{2_{1,2}} & ... & M_{k_{1,2}} \\
... & ... & ... & ... \\
M_{1_{n,n}} & M_{2_{n,n}} & ... & M_{k_{n,n}}
& 
\end{pmatrix}$ However, I am not sure about this.","['vector-spaces', 'linear-algebra', 'determinant']"
28069,Q is dense in R and Completions,"What is the relation between the fact that $\mathbb{Q}$ is dense in $\mathbb{R}$ and the fact that the completion of $\mathbb{Q}$ is $\mathbb{R}$? Or in general, that is if $A$ is dense in a metric space $B$, what's the relation between the completion of $A$ and $B$?","['general-topology', 'real-analysis']"
28076,Calculating the median in the St. Petersburg paradox,"I am studying a recreational probability problem (which from the comments here I discovered it has a name and long history). One way to address the paradox created by the problem is to study the median value instead of the expected value. I want to calculate the median value exactly (not only find bounds or asymptotic values). I have found a certain approach and I am stuck in a specific step. I present my analysis and I would like some help on that specific step. [Note: Other solutions to the general problem are welcome (however after the revelation of the long history I found a lot of material) but what I really want is to know the answer to the sub-problem that my approach raises.] The problem We have the following game: I toss a coin as many times is needed to get tails. Then I count the number of consecutive heads that preceded (call it h) and I give you $2^h$ dollars. How much are you willing to pay to play such a game? In other words, what is the maximum buy-in for that game you are willing to pay? Note also, that we can play this game any amount of finite times (each time with you paying the buy-in). A naive answer One straightforward way to answer this is to calculate the expected value of one game. This should be the upper limit for the buy-in. The expected value is the infinite sum of the return of each case times the probability of each case. More specifically
$$\sum_{i=0}^\infty (2^{i-1}\cdot\frac{1}{2^i}) = \sum_{i=0}^\infty \frac{1}{2} = \infty$$  This might seem counter-intuitive but it is true: Whatever constant and finite amount you bet per game, you are expected to win on the long run! Why is this so counter-intuitive though? Would you be willing to play this in practice with say 1000 dollars per game? The answer is no, because you would need an immensely large amount of games to actually win. So if we care about a more practical measure, the expected value is of no help. What we need is the median (or any other percentile value). If we know the median return for N games, we can at least know that if the buy-in is $\frac{median}{N}$, half of the possible cases you will lose and for half you will win. We will not know how much we will win or lose (we do have an upper bound on the losses though) but at least we know the chances to win or lose for a finite N number of games. Finding the median So how do you calculate the median return from N games (or more generally any ith percentile)? If we play only one game (N=1) then it is trivial. The median is 1. For N=2 it starts getting more complicated. With probability 0.25 we'll get back 1+1, with 0.125 1+2, with 0.125 2+1. These 3 cases already bring us to a total of 0.5, so the median is 3 (and so the maximum bet is 1.5 per game). For any N, how do we enumerate all the cases and find the 50% point (or any i% point)? I realized that this is (partly) an ordering problem. We do not want just to enumerate random cases, we have to order them, starting from the case with the smallest possible return, then getting the one(s) with the next smallest return and so on. As we are doing this ordering we are adding the probabilities of these cases. When we reach 50% (or i%) we stop. The return value for that case is our median value (ith percentile value). The ordering is where I am stuck. Sub-problem formulation We can depict the possible space of returns with a matrix where the N columns are the N games and the infinite rows are the return for each game:
$$\begin{array}{c} \text{row 1} \\ \text{row 2} \\ \text{row 3} \\ \vdots \\ \text{row i} \\ \vdots \end{array} \;\;\;\; \overbrace{\begin{array}{cccc} 1 & 1 & \cdots & 1 \\ 2 & 2 & \cdots & 2 \\ 4 & 4 & \cdots & 4 \\ \vdots & \vdots & \ddots & \vdots \\ 2^{i-1} & 2^{i-1} & \cdots & 2^{i-1} \\ \vdots & \vdots & & \vdots \end{array}}^N$$ A series of N games consists of picking values for each column (i.e., picking a game outcome for each game). The smallest possible total return is when all game outcomes are 1. So total return = N. The next possible one is when we get one outcome from the second row (total return N+1). The next smallest total return is N+2 (2 game outcomes from the second row). Notice though that for total return N+3 we have two ""configurations"": 1) cases where we have N-3 outcomes from the first row and 3 from the second row, OR 2) cases where we have N-1 outcomes from the 1st row and 1 outcome from the 3rd row! So ordering is not such an easy process. Configurations vs. cases Notice how I talked about ""configurations"" instead of individual cases. An individual case is a sequence of game outcomes (which are completely described by the game returns). For example a case of 4 games could be (1, 1, 16, 8) for a total return of 26. A configuration on the other hand is a more general construct which specifies how many outcomes we have from each row. A configuration completely determines the total return, but not the individual order that the outcomes happened. For example, the case given above is part of the configuration ""2 outcomes from row 1, 1 outcome from row 4, 1 outcome from row 5"". Cases (1,16,1,8) and (8,1,1,16) belong to the same configuration. From a configuration I can calculate how many distinct cases it has and what is the probability of each case. For example, for the configuration "" $N_i$ outcomes from row i, $N_j$ from row j, $N_k$ from row k"" we have: The number of distinct cases is ${N\choose {N_i}}\cdot{{N-N_i}\choose{N_j}}\cdot{{N-N_i-N_j}\choose{N_k}}$ The probability for each of these cases is $2^{-(i\cdot N_i + j\cdot N_j + k\cdot N_k)}$ The total return value for any of these cases is $N_i \cdot 2^{i-1}+N_j \cdot 2^{j-1}+N_k \cdot 2^{k-1}$ The example above shows a configuration with 3 rows, just to get a taste of the complexity of the problem. I can generalise the formulas to find distinct cases, their probabilities and their total returns for any given configuration. The problem is ordering the configurations . Can we find an algorithm that orders and lists the configurations based on their total return value? Let's describe each configuration as a series of pairs {(x,i), (y,j), ...} where the first number of a pair denotes the row number and the second number of a pair denotes how many outcomes do we have from that row. For example, {(1,4), (3,1), (4,2)} means that we get 4 outcomes from row 1, 1 outcome from row 3, and 2 outcomes from row 4. This also means that we played 4 + 1 + 2 = 7 games. I manually computed the first terms of the ordered configurations list, for N games. I give the configuration(s) on the left and the total return on the right. Note that some total returns have more than one configurations that produce them. $\begin{array}{ll} \text{Configurations} & \text{Total return} \\ 
\{(1,N)\} & N \\
\{(1,N-1),\; (2,1)\} & N+1 \\ 
\{(1,N-2),\; (2,2)\} & N+2 \\
\{(1,N-3),\; (2,3)\},\;\; \{(1,N-1),\; (3,1)\} & N+3 \\ 
\{(1,N-4),\; (2,4)\},\;\; \{(1,N-2),\; (2,1),\; (3,1)\} & N+4 \\  
\{(1,N-5),\; (2,5)\},\;\; \{(1,N-3),\; (2,2),\; (3,1)\} & N+5 \\
\{(1,N-6),\; (2,6)\},\;\; \{(1,N-4),\; (2,3),\; (3,1)\},\;\; \{(1,N-2),\; (3,2)\} & N+6 \\
\end{array}$ If I can produce this order algorithmically then I will be able to calculate the median (or ith percentile) for any N. I would also appreciate any help in formulating the problem in more accepted/mainstream terms. I believe that the formulation is valid and clear(?), but if we use a formulation from an established subfield maybe it will point to the solution too. Thanks!","['number-theory', 'probability', 'median', 'combinatorics']"
28086,Prove that $ \frac{n^2}{\sum_1^n{\frac{1}{a_i}}} $ is convergent.,"Suppose that $a_n > 0$ for all $n\geq 1$, and define $S_n = \sum_{i=1}^n{a_i}$. If $S_n$ is convergent, prove that 
$$ \frac{n^2}{\sum_{i=1}^n{\frac{1}{a_i}}} $$ 
is also convergent. Thanks.",['sequences-and-series']
28092,Finding the optimum supply quantity when there is uncertainty in forecast,"This is actually a quiz that will be needed in a real life food stall! I need to decide how much stock to supply for my pumpkin soup stall. I sell each soup for $5$ dollars a cup, and let's say my ingredients cost is $1$ dollar. Therefore an outcome of under-forecasting is $4$ dollars per unit, while an outcome of over-forecasting is 1 dollar per unit. My forecast isn't so simple, however. I'm guessing that the most probable number of sales is $150$ units, but I'm very unsure, so there's a normal distribution behind this prediction with a standard deviation of $30$ units. 
This is harder than I expected. Intuitively I would prepare ingredients for $180$ units, at which point I'd guess that the likely opportunity costs that would come with understocking would roughly meet the likely costs of overstocking. But given this is such a common dilemma, I thought that someone must be able to find a precise solution, and would then hopefully be able to explain it in layman's terms.","['statistics', 'optimization']"
28123,If g(f(x)) is one-to-one (injective) show f(x) is also one-to-one (given that...) [duplicate],"This question already has answers here : Closed 13 years ago . Possible Duplicate: Injective and Surjective Functions If $g(f(x))$ is one-to-one (injective) show $f(x)$ is also one-to-one given that $f$ is a function from $A$ to $B$ and $g$ a function from $B$ to $C$. I've just started my Discrete math course and I'd like some help on this. I'm pretty sure we're supposed to use set theory laws to prove this. So far I know the three conditions that satisfy an injective function (sorry, having difficulties typing all this TeX markup so I'll skip that). Any help?","['elementary-set-theory', 'functions']"
28139,How to solve for $x$ in $\frac{200}{x+10} = \frac{200}{x} -1$?,"I am really confused how to calculate the following. The answer is $40$, but what are the steps to get $40$?
$$\frac{200}{x+10} = \frac{200}{x} - 1.$$
Thank you.","['algebra-precalculus', 'problem-solving']"
28145,Axiom of choice and automorphisms of vector spaces,"A classical exercise in group theory is ""Show that if a group has a trivial automorphism group, then it is of order $1$ or $2$."" I think that the straightforward solution uses that a exponent two group is a vector space over $\operatorname{GF}(2)$, and therefore has nontrivial automorphisms as soon as its dimension is at least $2$ (simply transposing two basis vectors). My question is now natural: Is it possible, without the axiom of choice, to construct a vector space $E$ over $\operatorname{GF}(2)$, different from $\{0\}$ or $\operatorname{GF}(2)$, whose automorphism group $\operatorname{GL}(E)$ is trivial?","['linear-algebra', 'axiom-of-choice']"
28153,Numbers satisfying $\binom{n}{k} = m!$,"Let $k,m,n\in \mathbb{N}$ where $1 < k < n-1$. Consider the equation $$\binom{n}{k} = m!$$ which can also be equivalently written as $$n!=(n-k)!k!m!$$ The only instances I found are $\binom{4}{2} = 3!$ and $\binom{10}{3} = 5!$ I do not see any pattern coming out. As I went far out, it seemed that it is hard to find other examples as the second instance seems to be related to the problem of  consecutive numbers being composed of only small primes.  Is it true that these are the only instances? Thanks.","['combinatorics', 'number-theory']"
28157,How to solve $(5-2\sqrt6)^n + (5+2\sqrt6)^n = 98$,$$\sqrt{(5+2\sqrt6)^x}+\sqrt{(5-2\sqrt6)^x}=10$$ So I have squared both sides and got: $$(5-2\sqrt6)^x+(5+2\sqrt6)^x+2\sqrt{1^x}=100$$ $$(5-2\sqrt6)^x+(5+2\sqrt6)^x+2=100$$ I don't know what to do now,"['recurrence-relations', 'algebra-precalculus', 'discrete-mathematics', 'elementary-number-theory', 'exponentiation']"
28166,"Find limit of $\sqrt[n]{a^n-b^n}$ as $n\to\infty$, with the initial conditions: $a>b>0$","With the initial conditions: $a>b>0$; I need to find $$\lim_{n\to\infty}\sqrt[n]{a^n-b^n}.$$ I tried to block the equation left and right in order to use the Squeeze (sandwich, two policemen and a drunk, choose your favourite) theorem.","['calculus', 'limits']"
28179,An ant walk on the edges of a cube where it can select any of the 3 adjoining vertices with equal probability.,There is a cube and an ant is performing a random walk on the edges where it can select any of the 3 adjoining vertices with equal probability. What is the expected number of steps it needs till it reaches the diagonally opposite vertex?,"['graph-theory', 'expected-value', 'probability']"
28187,Evaluate $\int_{0}^{\frac{\pi}{4}}\ln(\cos t)dt$,"$$\int_{0}^{\frac{\pi}{4}}\ln(\cos t)dt=-\frac{{\pi}}{4}\ln2+\frac{1}{2}K$$ I ran across this integral while investigating the Catalan constant. I am wondering how it is evaluated.
I know of this famous integral when the limits of integration are $0$ and $\frac{\pi}{2}$ , but when the limits are changed to $0$ and $\frac{\pi}{4}$ , it becomes more complicated. I tried using $$\cos(t)=\frac{e^{it}+e^{-it}}{2},$$ then rewriting it as: $$\int_{0}^{\frac{\pi}{4}}\ln\left(\frac{e^{it}+e^{-it}}{2}\right)=\int_{0}^{\frac{\pi}{4}}\ln(e^{it}+e^{-it})dt-\int_{0}^{\frac{\pi}{4}}\ln(2)dt.$$ But, this is where I get stuck. Maybe factor out an $e^{it}$ and get $$\int_{0}^{\frac{\pi}{4}}\ln(e^{it}(1+e^{-2it}))dt=\int_{0}^{\frac{\pi}{4}}\ln(e^{it})dt+\int_{0}^{\frac{\pi}{4}}\ln(1+e^{-2it})dt$$ I thought maybe the Taylor series for ln(1+x) may come in handy in some manner. It being $\ln(1+x)=\sum_{k=1}^{\infty}\frac{(-1)^{k+1}x^{k}}{k}$ Giving $\int_{0}^{\frac{\pi}{4}}\sum_{k=1}^{\infty}\frac{(-1)^{k+1}e^{-2kit}}{k}$ Just some thoughts. I doubt if I am on to anything. I used a technique similar to this when solving $\int_{0}^{\frac{\pi}{2}}x\ln(\sin(x))dx$ . But, how in the world would the Catalan constant come into the solution?. $K=\sum_{k=0}^{\infty}\frac{(-1)^{k}}{(2k+1)^{2}}\approx .916$ Your learned input is appreciated.","['definite-integrals', 'trigonometric-integrals', 'integration', 'catalans-constant']"
28189,Freedoms of real orthogonal matrices,"I was trying to figure out, how many degrees of freedoms a $n\times n$-orthogonal matrix posses.The easiest way to determine that seems to be the fact that the matrix exponential of an antisymmetric matrix yields an orthogonal matrix: $M^T=-M, c=\exp(M) \Rightarrow c^T=c^{-1}$ A antisymmetric matrix possesses $\frac{n(n-1)}{2}$ degrees of freedom. BUT: When I also thought about how to parametrize these freedoms explicitly (without the exponential) I remembered, that rotations in $\mathbb{R}^n$ can be parametrized using $n-1$ angles or cosines. I dont' understand, where the remaining parameters are hidden? My guess is, that a orthoganl transformation in $n>3$ can be more complicated than a rotation or that there are different types of rotation (containing reflectiong or such things) and that all the combination of these different types accounts for the rest of the parameters. Thanks you for your help!","['matrices', 'orthonormal', 'rotations']"
28190,Counting and Ordering of Numbers,"Are there differences between 'counting' and 'ordering'? As such, the whole of rational number is countable, or they order-able too? In what cases counting and ordering are same or not?","['order-theory', 'elementary-set-theory', 'soft-question', 'intuition', 'combinatorics']"
28223,Sample randomly from (or computably enumerate) non-decreasing sequences from 1 to n of length k,"I have an oracle that samples uniformly from sets of size up to around n or n 2 . How do I efficiently approximate a uniform distribution on the set of non-decreasing sequences of length k whose terms are from {1,2,…,n} (using the oracle)? This set has about (n+k−1) choose k elements, O(n^k), and is too big to directly use the uniform oracle. Sample values that give trouble are (n=31,k=26) and (n=31,k=100).  In particular, k will always be a bit large compared to n, so there is a substantial difference between non-decreasing (what I want) and increasing. Things that don't work: Pick the first term uniformly, then pick the second term uniformly from the [a 1 …n], then pick the third term uniformly from [a 2 …n], etc.  The resulting distribution is too heavily skewed to be useful (1≤1≤…≤1 has probability (1/n) k , while n≤n≤…≤n has probability 1/n). Sampling uniformly from {1,2,…,n} k and then sort the result is also too heavily skewed.  For n=k, 1≤2≤…≤n is n! times more likely than 1≤1≤…≤1. Sampling uniformly from {1,2,…,n} k and discarding unsorted sequences is not efficient enough (something in the range of k! oracle calls per sample produced; for n=31, k=26, it is 89704535825984961898313 calls per sample). Another approach that is fine by me: What is an efficiently computable bijection between {1,2,…,Binomial(n+k−1,k)} and the set of non-decreasing sequences of length k whose terms are positive integers less than or equal to n? I can sample uniformly at random from {1,2,…,m} for m up to n k using a different oracle.  Hence if I can efficiently convert from positive integers to sequences, then that is a good solution.","['probability-distributions', 'combinatorics']"
