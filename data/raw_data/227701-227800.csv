question_id,title,body,tags
4708827,What's the probability of $HTHT$ occuring before $HHTT$?,"What's the probability of $HTHT$ occuring before $HHTT$ in a stream of $H$ 's and $T$ 's (both equally likely) that will stop if either of those occur? What's the mean number of throws such that $HHTT$ occurs? Hello, I figured out that this is an instance of Penney's game . This question is also related. What makes this question hard for me is the fact that I have to somehow account for the fact that $HTHT$ occures before $HHTT$ . I have to solve this using Markov chains, does someone have an idea?","['markov-chains', 'probability-theory', 'probability']"
4708866,what does $\frac{\partial}{\partial x}$ means without the f ( $\frac{\partial f}{\partial x}$),"I was reading in calculus 3 book , but I am stuck here I don't understand what $\frac{\partial}{\partial x}$ means (I know it is partial derivative but I don't know what it means without f like $\frac{\partial f}{\partial x}$ )
and I don't understand how does $\frac{\partial}{\partial x}*Q=\frac{\partial Q}{\partial x}$ and what does $\nabla $ means without any function ?
It is like multiplying a number with a sign, which doesn't make any sense
I heard from 3b1b video that this is not only a notional trick but there is a relation between curl and cross product but I didn't understand what he means
it is in this video at 11:00","['multivariable-calculus', 'calculus', 'vectors']"
4708890,"Multivariable calculus: strict local minimum of $f(x,y) = y^2 - 3x^2y+2x^4$","Task Let $p:=(0,0)$ and $f: \mathbb{R}^{2} \longrightarrow \mathbb{R}$ be the function $$f(x, y):=y^{2}-3 x^{2} y+2 x^{4}$$ Show: a) For every $a=\left(a_{1}, a_{2}\right) \in \mathbb{R}^{2} \backslash\{(0,0)\}$ , the function $g_{a}: \mathbb{R} \longrightarrow \mathbb{R}$ $$g_{a}(t):=f(p+t a)$$ has a strict local minimum at $t=0$ . b) $f$ has no local minimum at $p$ .
Hint: Consider $f$ along the curve $\gamma: t \mapsto\left(t, \sqrt{2} t^{2}\right)$ passing through $p$ . My solution:
a) We have $$g_{a}(t)=f(p+ta)=f((0,0)+t\cdot(a_1,a_2))=f(ta_1,ta_2)=t^2a_2^2-3t^3a_1^2a_2+2t^4a_1^4$$ $$g_a'(t)=2ta_2^2-9t^2a_1^2a_2+8t^3a_1^4$$ Necessary condition: $g_a'(t)=0$ , $0=2ta_2^2-9t^2a_1^2a_2+8t^3a_1^4$ , $0=t(2a_2^2-9ta_1^2a_2+8t^2a_1^4)$ Case 1 $t_1=0$ Case 2 $$\begin{aligned}
2a_2^2-9ta_1^2a_2+8t^2a_1^4 &= \frac{2a_2^2}{8a_1^4}-\frac{9a_1^2a_2}{8a_1^4}t+t^2 \\
&= \frac{a_2^2}{4a_1^4}-\frac{9a_2}{8a_1^2}t+t^2, \\
t_{2,3} &= \frac{9a_2}{16a_1^2}\pm\sqrt{\frac{81a_2^2}{256a_1^4}-\frac{64a_2^2}{256a_1^4}} \\
&= \frac{9a_2}{16a_1^2}\pm\frac{\sqrt{17}a_2}{16a_1^2}, \\
t_2 &= \frac{a_2(9+\sqrt{17})}{16a_1^2}, \\
t_3 &= \frac{a_2(9-\sqrt{17})}{16a_1^2}
\end{aligned}$$ Sufficient condition $$g_a''(t)=2a_2^2-18a_1^2a_2t+24t^2a_1^4$$ Then, $$g_a''(t_1)=2a_2^2>0$$ and $$\begin{aligned}0<t^2a_2^2-3t^3a_1^2a_2+2t^4a_1^4,\\ 
0<t^2(ta_1^2-a_2)(2ta_1^2-a_2)\end{aligned}(*)$$ $$\Rightarrow \text{True statement}$$ $$\Rightarrow t_1=0 \text{ is a strict local minimum}$$ Problems: I'm struggeling with my approach of (*). And for b) I don't really have an idea but I substituted $(t,\sqrt{2}t^2)$ into $f$ and get $f(t,\sqrt{2}t^2) = t^4(\sqrt{16}-\sqrt{18})< 0$ . Thanks so much for your help!","['maxima-minima', 'multivariable-calculus', 'calculus', 'solution-verification', 'derivatives']"
4708912,Showing that $\{ e_n \}_{n\in\mathbb Z}$ is an orthonormal basis,"I have to show that $ \{ e_n \}_{n \in \mathbb Z} $ , defined by $$ e_n(x) := \frac{1}{\sqrt 2} \mathrm e^{\mathrm i \pi n x} $$ is an orthonormal basis of $L^2([-1,1])$ , equipped with the scalar product $\langle f, g \rangle = \int_{-1}^1 \overline f g $ . I already proved that it is an orthogonal set and it remains to show completeness, i.e. that $0$ is the only vector that is orthogonal to all $e_n$ . As a hint, it is given to show that $ \mathrm{span}\{e_n\}_{n\in\mathbb Z}$ is dense in $C := \{ f \in C([-1,1]) : f(-1) = f(1) \} \cong C(S^1)$ w.r.t. the supremum norm by using Stone-Weierstrass. I don’t want a full proof, rather I want to understand the rough idea. I already do not understand how proving denseness as recommended in the hint helps me in the proof of completeness. I mean, i have to get from „Assume $\langle e_n, f \rangle = 0 \, \forall n \in \mathbb Z$ .“ somehow to „That implies $f = 0$ .“ What’s the idea in between and how does denseness of $C$ play a role here?","['hilbert-spaces', 'functional-analysis', 'analysis']"
4708947,"When I can truncate a function space to a subspace, in a way that non-negative functions stay non-negative?","When I can truncate a function space to a subspace, in a way that non-negative functions stay non-negative? How I got here (a simple concrete example): I was working with point-process intensity functions $\rho(x)$ on $x\in\mathbb R^n$ . I wanted to find some ""nice"" convolutional filters $f(x)$ that: Send all Fourier frequencies $\omega \in \mathcal \Omega$ above $\omega_0$ to identically zero, that is $\|\omega\|>\omega_0 \Rightarrow \{\hat f\cdot \hat \rho\}(\omega)=0$ (to not worry about high-frequency details). Are themselves non-negative $f(x)\ge 0$ (to interpret $f*\rho$ as the PDF resulting from addition of improper random variables). $f(x)$ and $\hat f(\omega)$ are ""as nice as possible"" in any interesting sense. Probably things like ""unimodal"", and ""peak at zero"" ""as concentrated as possible near zero"". You can get some very nice low-pass $f(x)$ with compact support on $\|\omega\|\ge\omega_0$ , achieve their maximum at $x=0$ , are reasonably concentrated near $x=0$ , and have $\nabla f(x)=0$ for $x=0$ . For example, $f(x) \propto \operatorname{sinc}(\alpha x)^4$ is nice. I wasn't able to find any such $f(x)$ that decayed monotonically from $x=0$ ; Can such a function exist? I realized I didn't have the tools to reason about this. My attempts to generalize This seemed like a special case of something that may have relevant results in functional analysis.
Here is my best-attempt at generalizing the problem: Let $\rho \in \mathcal R:\mathcal X\mapsto \mathbb R_{\ge 0}$ be non-negative functions on $x \in \mathcal X$ . Let $f \in \mathcal F:\mathcal X\mapsto \mathbb R$ be scalar functions on $x \in \mathcal X$ . Let $\mathcal M$ a function space contining $\mathcal R$ and $\mathcal F$ . Let $U:\mathcal M\mapsto \mathcal M$ be operators that preserve non-negativity: $U\rho \subseteq \mathcal R$ Let $K:\mathcal M\mapsto \mathcal M$ be operators that send a non-trivial subspace of $\mathcal M$ to zero (have a nontrivial kernel). Q: How can we find operators that are in both $U$ and $K$ ? The definitions above should be considered ""purely formal"", ""vague"", or ""confused, please help me fix it"". I'm interested in keywords, the names of mathematical objects, well-known results, topics and sub-fields that I can search for to learn more? What theorems, concepts, and results on this topic might I find empowering?","['banach-spaces', 'hilbert-spaces', 'linear-algebra', 'functional-analysis', 'reproducing-kernel-hilbert-spaces']"
4708964,What is the geometrical difference between $Z_{\alpha/2}$ and $E_{a}$?,"What is the geometrical difference between $Z_{\alpha/2}$ and $E_{a}$ ? Let's say we have a generator of toys and the weight is distributed with a standard deviation of $4kg$ and a mean of $5kg$ for example. I know that the interval of confidence is defined as $[\mu+E_a,\mu-E_a]$ being $E_a=Z_{\alpha/2}\frac{\sigma}{\sqrt{n}}$ the random error and $\sigma$ the standard deviation. Which then the extreme values of that interval, are $\mu+E_a$ and $\mu-E_a$ , which then I suppose $E_a$ is a value with units of $kg$ , and then since those are the extreme values, I'm assuming that $E_a$ are these values that would be showed in the graph that limit the area $1-\alpha$ : But then I think, then what is $Z_{\alpha/2}$ ? Since I thought that it was literally the limit that would separate the area $1-\alpha$ . Is $Z_{\alpha/2}$ just the equivalent to the above in a distribution that represents $f(X)$ and not $f(\overline{X})$ ? Or is it just the equivalent to the above in a normal distribution $N(0,1)$ ? Also I notice that the graph it is delimited by $X_{\alpha/2}$ , so is $X_{\alpha/2}=E_a$ and then $Z_{\alpha/2}$ is just $Z_{\alpha/2}=\frac{X_{\alpha/2}-\mu}{\sigma}$ . In general this question is for me to join spots and make it have sense.","['statistics', 'confidence-interval', 'normal-distribution', 'standard-deviation']"
4708973,Prove by induction $n^2 + n \geq 42$ when $n \geq 6$ and $n \leq -7$ (induction with two different intervals),"I am asked the following: Prove by induction $n^2 + n \geq 42$ when $n \geq 6$ and $n \leq -7$ The first part of the exercise seems ok: Base case P(6): $\quad LHS = 36+6=42\geq 42$ Inductive hypothesis $\quad P(k) : k^2 + k \geq 42$ Inductive step $$ (k+1)^2 + (k+1) = k^2 + 2k + 1 + k + 1 = k^2 + k + (2k+2) > k^2 + k \geq 42$$ Now, for the second part: Base case P(7): $\quad LHS = 49-7=42\geq 42$ Inductive hypothesis $\quad P(k) : k^2 + k \geq 42$ Inductive step $$\quad (k+1)^2 + (k+1) = \cdots$$ (not sure how to go from here) My intuition says I need to use the fact that I have a negative $n$ and use it to my favor, however, I am not sure how. Any help is highly appreciated.","['algebra-precalculus', 'induction']"
4708983,Non effective divisor on a smooth quartic surface,"Let $E$ be a nonzero, non effective divisor on a smooth quartic surface $X \subset \mathbb P^3$ (i.e. $H^0(X, E) =0$ ). Then is it possible that $H^1(X, E) \neq 0$ ? Euler Characteristic computation suggests that it is possible iff $E^2 \neq -8$ . Are there any concrete examples to this $E$ ? Thanks in advance.",['algebraic-geometry']
4709020,"Proof of $\inf\left \{ \frac{\mathrm{d} (n^2)}{\mathrm{d} (n)} \; \bigg| \; n \in \mathbb{N} \right \}=0$, where $d(n)$ is the sum of digits of $n$","So I wanted to find the infimum of the set described in the title, and I'm pretty confident on what the subsequence should be to ensure a 0 infimum. $[4899,4899899999,4899899999899999999999, \dotsc]$ , where the next term in the sequence is the previous + '8' + '9' * $(2s+1)$ , where $s$ is the length of the previous set of consecutive nines. In the sequence I provided, it goes 2,5,11... (I'm fairly confident 1,3,7,15... also works). But a proof of this eludes me. I tried some funky stuff with the multinomial theorem and binomial theorem $(10^6 4899 + 899999)^2$ and tried to chunk it based on the first section and second but I really couldn't go anywhere due to the $2\cdot 899999 \cdot 10^6 \cdot 4899$ term. EDIT: 49 is a good starting term to look at.","['number-theory', 'decimal-expansion', 'recreational-mathematics', 'supremum-and-infimum', 'sequences-and-series']"
4709056,Probability of Sisyphus laboring forever,"Zeus has decreed that Sisyphus must spend each day removing all the rocks in a certain valley and transferring them to Mount Olympus. Each night, each rock Sisyphus places on Mount Olympus is subject to the whims of Zeus: it will either be vaporized (with probability 10%), be rolled back down into the valley (with probability 50% ), or be split by a thunderbolt into two rocks that are both rolled down into the valley (with probability 40%). When the sun rises, Sisyphus returns to work, delivering rocks to Olympus. At sunrise on the first day of his punishment, there is only one rock in the valley and there are no rocks on Mount Olympus. What is the probability that Sisyphus must labor forever? I tried approaching this problem as a random walk, but the fact that if Sisyphus has more than one rock in the valley then there is a chance of increasing the number of rocks by more than one is throwing me off. Does anyone have any insight as to how to approch this kind of problem? Thanks!","['markov-process', 'random-walk', 'probability']"
4709123,"Prove that if $A$, $B$ and $C$ are sets then $(A \Delta B) \cup C = (A \cup C) \Delta (B \setminus C) $","I've been trying to solve this problem for hours and I can't come up with a solution that isn't so complicated and ugly to the point where I feel like I'm doing something wrong. Here are my attempts: Trying to show that both sets are equal to $((A \cap B) \setminus C)^c$ , and then show that that implies that the sets are equal. This doesn't work out in practice because that would require showing that set $x$ is a subset of this, and showing that set $y$ is not a subset, etc. Showing that both are subsets of each other. This is the conventional way but this would require showing that some subset of $(A \Delta B) \cup C$ and also a subset of $(A \cup C) \Delta (B \setminus C)$ , and so on. There are many more attempts but I'm just here with a question: Is there a practical way of proving this?",['elementary-set-theory']
4709130,What is the motivation behind the definition of Cech cohomology?,"From Hartshorne's Algebraic Geometry, chapter 3.4: The Cech cohomology of a sheaf topological space wrt an open cover is defined as $\frac{ker(d_{i+1})}{im(d_i)}$ , where $d$ is some operator involving alternating sums (I won't type out the full definition here as it's quite long, see the Wikipedia  article for the full definition) . While I understand the definition, where does this formula come from and how did mathematicians arrive at this definition? There is theorem III.4.5 (isomorphism between Cech cohomology and derived functor cohomology in the case of a Noetherian separated scheme , quasicoherent sheaf and affine open cover). This is essentially the Cech to derived functor spectral sequence. Is it possible to reverse-engineer the definition from the spectral sequence? Edit: I'm familiar with derived functor cohomology from chapter 3.1 of Hartshorne and chapter 17 from Dummit and Foote (including the construction of Ext and Tor). I think that's well-motivated in terms of making a long exact sequence from a short exact sequence. I know some algebraic topology but only a little. I know about the fundamental group, homotopy equivalence, and deck transformations but that's it.","['motivation', 'algebraic-geometry', 'homology-cohomology']"
4709139,Solve ODE by series given a strange solution,"While studying for a test, I had this following question: Solve with series substituion: $$y''-x^2y'-3xy=0$$ So, doing $y=\sum_{n=0}^{\infty}a_nx^n$ , I got: $2a_2+\sum_{n=0}^{\infty}[(n+3)(n+2)a_{n+3}-(n+3)a_n]x^{n+1}=0$ So, $a_2=0$ and $a_{n+3} = \frac{a_n}{n+2}$ Is this correct?
I can't see any pattern in this terms.
Thanks for attention.","['taylor-expansion', 'ordinary-differential-equations', 'sequences-and-series']"
4709204,Does the sequence converge? $\displaystyle a_n=n\sum_{k=1}^\infty\frac{1}{4^{2k+1}}\left(1-\frac{1}{4^k} \right)^{n-2}$,"For every $n\geq 3$ , let $$
a_n=n\sum_{k=1}^\infty\frac{1}{4^{2k+1}}\left(1-\frac{1}{4^k} \right)^{n-2}
$$ So for example, $$
a_3=3\sum_{k=1}^\infty 2^{-4k-2}-2^{-6k-2}=\frac{3}{4}\left(\frac{1}{15}-\frac{1}{63}\right)
$$ I am curious if $a_n$ converges as $n\rightarrow\infty$ , and if so, to what limit. One thing I've tried is to simplify the terms using the binomial expansion: $$\begin{align}
a_n&=n\sum_{k=1}^\infty2^{-2(2k+1)}\sum_{m=0}^{n-2}{n-2\choose m}(-2^{-2k})^m\\&=\frac{n}{4}\sum_{m=0}^{n-2}{n-2\choose m}(-1)^m\sum_{k=1}^\infty 2^{-(2m+4)k}\\&=\frac{n}{4}\sum_{m=0}^{n-2}{n-2\choose m}\frac{(-1)^m}{4^{m+2}-1}\end{align}
$$ But I don't know how to simplify it further and to evaluate whether or not it converges. Any hint would be greatly appreciated.","['analysis', 'real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
4709229,"If $A, B$ is a non-trivial partition of $S^1$, is it possible that $R_\theta(A) \cap B$ has measure zero for all rotations $R_\theta$?","As in the question title, let $A, B$ be a partition of the unit circle $S^1$ , equipped with the Haar measure. Here, we do not require $A, B$ to be measurable. Also, assume neither $A$ nor $B$ is of measure zero, so they are either both non-measurable or both of positive measure. Is it possible, then, for $R_\theta(A) \cap B$ to be measurable and has Haar measure zero for all $\theta$ , where $R_\theta$ is the rotation by degree $\theta$ ?","['measure-theory', 'lebesgue-measure', 'rotations']"
4709260,Do $X$ and $kX$ have the same compact subsets only if $X$ is weak Hausdorff?,"I have a doubt on compactly generated spaces. The majority of the question is here for context, you can skip through the definitions and the facts mentioned to the question, if you're familiar with them. Thanks for any clarify. Let $X$ be a topological space. $X$ is said weak Hausdorff if every compact subset is closed; a subset $S\subset X$ is compactly closed if, for every continuous map $K\to X$ from a compact space $K$ , $f^{-1}(S)\subset K$ is closed; $X$ is said a $k$ -space if every compactly closed subset is closed. Let $kX$ be the topological space defined as follows: the underlying set is (the same of) $X$ ; a subset $S\subset kX$ is closed if and only if $S$ is compactly closed in $X$ . Fact 1. The identity  map on the underlying sets $i:kX\to X$ is continuous, and $S\subset kX$ is closed if and only if $i(S)\subset X$ is compactly closed. Fact 2. $kX$ is a $k$ -space. Let $C\subset kX$ be compactly closed, and let $f:K\to X$ be a continuous map from a compact space $K$ . $f$ factors as $i\circ f'$ , for a (unique) continuous map $f':K\to kX$ : the definition on the underlying sets is evident, and it is continuous because, for any closed $S\subset kX$ , we have $f'^{-1}(S)=f^{-1}(i(S))$ , which is closed as $i(S)$ is compactly closed. Now $f^{-1}(i(C))=f'^{-1}(C)$ , which is closed as $C$ is compactly closed, yielding the thesis. Fact 3. If $X$ is weak Hausdorff, also $kX$ is. Let $C\subset kX$ be compact. $i(C)$ is compact too, hence closed by hypothesis. So $C=i^{-1}(i(C))$ is closed. Fact 4. If $X$ is weak Hausdorff, if $C\subset X$ is compact also $i^{-1}(C)$ is. We show that the continuous map $i':i^{-1}(C)\to C$ induced by $i$ is a homeomorphism: it is sufficient to prove that $i'$ is closed. Let $S'\subset i^{-1}(C)$ be closed, so that $S'=S\cap i^{-1}(C)$ for some closed $S\subset kX$ . Then $i'(S')=i(S)\cap C$ , which is closed in $C$ since $i(S)$ is compactly closed. Question. Is it necessary that $X$ is weak Hausdorff for Fact 4 to be true? It was assumed on the text I'm reading, but it doesn't seem necessary. The point is that, in any space $X$ , holds this: ( $*$ ) if $S\subset X$ is compactly closed, then $S\cap C\subset C$ is closed for every compact $C\subset X$ ; if $X$ is weak Hausdorff, we know that, if $S\subset X$ satisfies ( $*$ ), then $S$ is compactly closed, but we don't need this direction in the proof of Fact 4, I think.","['general-topology', 'algebraic-topology', 'compactness']"
4709270,What's the significance of Mean Squared Error? Why not something else?,"Background: Masters in CS/Math. I'm brushing up on statistics I see Mean Squared Error(MSE) everywhere. As a student I took it for granted, but now when I tried to find the reasons for why it's so prevalent I am told: simplicity, emphasis on outliers and mathematical properties like differentiability. So what? It's not the only function with those properties. So why is it used so widely? Are there situations where it's probably the best function to use? Are there situations where there are other functions that are probably better to use? Say I am designing my own heuristic, and I have an error I want to minimize on. How do I know that squaring the error is the best way forward?","['statistics', 'mean-square-error', 'machine-learning', 'functions', 'error-function']"
4709294,How to solve the first order ODE,"I have a differential equation: $$y(x)\frac{dy(x)}{dx}+nxy(x)=C,$$ where $n$ and $C$ are constants. Is there a way to get $y(x)$ ?","['nonlinear-system', 'ordinary-differential-equations']"
4709370,You have $n$ rectangles of area $1$ (and variable height). Pack as many as possible in a semicircle of area $n$. How many leftovers will there be?,"You have $n$ rectangles of area $1$ (and variable height). Pack as many of these rectangles as possible in a semicircle of area $n$ . How many leftover rectangles will there be, in terms of $n$ ? How to pack the rectangles I believe that the most efficient way to pack the rectangles is to stack them (so that each rectangle has two vertices touching the arc), as shown below. What about arranging the rectangles side by side? A simple argument shows that the stacked arrangement is more efficient than the side by side arrangement. In each arrangement, consider a quarter circle (for example the right half of the semicircle). In the stacked arrangement the rectangles have area $1/2$ , whereas in the side by side arrangement the rectangles have area $1$ . The smaller the rectangles, the more efficient the packing. Expressing the problem in terms of a sequence Let $a_k$ be the sequence of the $x$ -coordinates of the upper-right vertex of each rectangle, from bottom to top. We have $\alpha_1=$ $\large{[}$ largest real root of $2x\sqrt{\frac{2n}{\pi}-x^2}=1$ $\large{]}$ $\alpha_{k+1}=$ $\large{[}$ largest real root of $2x\left(\sqrt{\frac{2n}{\pi}-x^2}-\sqrt{\frac{2n}{\pi}-{\alpha_k}^2}\right)=1$ $\large{]}$ So the number of leftover rectangles is $f(n)=n-$ (number of terms in sequence $\alpha_k$ ). I am looking for an exact or asymptotic closed form expression for $f(n)$ . Further thoughts I have found experimentally that $f(18)=1$ and $f(19)=2$ . Based on Gauss's circle problem , I would guess something like $f(n)\approx n^{\theta}$ for some $\theta<1$ . I guess my problem should be easier than Gauss's circle problem, because my problem just depends on finding the number of terms in the well-defined sequence $\alpha_k$ . The functions $y=2x\sqrt{\frac{2n}{\pi}-x^2}-1$ and $y=2x\left(\sqrt{\frac{2n}{\pi}-x^2}-\sqrt{\frac{2n}{\pi}-{\alpha_k}^2}\right)-1$ have the following kind of appearance. (This example is with $n=19$ .) If we can find a pattern among the local maximum values, then we can predict how many curves there are, which equals the number of rectangles packed in the semicircle. On the top curve, $y=2x\sqrt{\frac{2n}{\pi}-x^2}-1$ , the coordinates of the maximum point are $\left(\sqrt{\frac{n}{\pi}},\frac{2n}{\pi}-1\right)$ . The gaps between the local maximum values slightly decrease from top to bottom.","['rectangles', 'circles', 'asymptotics', 'packing-problem', 'sequences-and-series']"
4709372,"Is there an incompressible vector field $\mathbf{F}$ with $\nabla \times \mathbf{F} = (y^2, z^2, x^2)$?","I want to find an incompressible vector field $\mathbf{F}$ such that $\nabla \times \mathbf{F} = (y^2,z^2,x^2)$ . After some attempts to find such $\mathbf{F}$ , I think the vector field $\mathbf{F}$ with given conditions may not exist. However even for this side, I do not have an idea to proceed. Since $\mathbf{F}$ is incompressible we have $\mathbf{F} = \nabla \times \mathbf{G}$ for some $\mathbf{G}$ , which leads to $\nabla \times(\nabla \times \mathbf{G}) = (y^2,z^2,x^2)$ . However there is no additional property that I know about double curl to prove nonexistence of such $\mathbf{F}$ . Thanks in advance for any form of help, hint, or solution.","['vector-fields', 'multivariable-calculus', 'vector-analysis']"
4709381,How to solve this ODE $(1-2xy)y'=y(y-1)$?,"It appears in the section of first-order linear equations, and may only need a variable substitution, but I tried many and couldn’t get the desired result. Can anyone help me ? Thank you!",['ordinary-differential-equations']
4709424,How to solve for $x$ in $2^x+4^x=8^x$,"So I was bored, and decided to do some math for fun. This was mostly to see if I could still do fairly complex math. After a while, I came up with this to see if I could still do some reasonably simple Algebra involving logarithms: $$\text{Solve for }x\text{: }2^x+4^x=8^x$$ which I thought that I might be able to do. Here is my attempt at solving for $x$ in $2^x+4^x=8^x$ : $$2^x+4^x=8^x$$ $$\implies1+2^x=4^x$$ $$\because2^x+4^x=8^x\equiv\require{cancel}\cancel{2^x}(1+2^x)=\cancel{2^x}(4^x)\gets1+2^x=4^x$$ $$2^x+1-(4^x)=\cancel{4^x-4^x}$$ $$-(4^x)+2^x+1=0$$ $$2^{2x}-2^x-1=0$$ $$(2^x)^2-2^x-1=0$$ $$a^2-a-1=0\quad\text{substituting }a\text{ for }2^x$$ $$a=\dfrac{1\pm\sqrt{1+4}}{2}$$ $$a=\dfrac{1\pm\sqrt5}{2}$$ $$2^x=\dfrac{1\pm\sqrt5}{2}$$ $$2^x=(1\pm\sqrt5)2^{-1}$$ $$x\ln(2)=\ln(1\pm\sqrt5)-\ln(2)$$ $$\therefore x=\dfrac{\ln(1\pm\sqrt5)}{\ln(2)}-1$$ $$\text{Or }x\approx0.6942419136306,-0.6942419136306+4.3523601i$$ My question Is the solution I have achieved correct, or what could I do to attain the correct solution more easily? Mistakes I might have made Simplifying the logarithms (which honestly is a mistake I always seem to make) The conversion to the quadratic formula (is that even a legal move I made there? I'm not sure. :\) Incorrect tags on my question Incorrectly using math notation","['algebra-precalculus', 'solution-verification', 'logarithms']"
4709452,Finding a matrix and its inverse in terms of another one.,I have a symmetric matrix $A$ . I wanted to modify its first row (thus it is first column). Let's call the new row $c^T$ . How do I write the modified matrix $B$ in terms of $A$ and $c$ . What is the inverse of $B$ in terms of $A$ and $c$ ?,"['matrices', 'linear-algebra', 'inverse']"
4709484,"Let $f:Y\to X$ be the proper birational morphism, prove $H^0(Y,\mathcal{O}_Y(D)) \subset H^0(X,\mathcal{O}_X(f_*(D)))$","Let $f:Y\to X$ be the proper birational morphism between smmoth projective variety, $D$ be an (effective) divisor, prove $H^0(Y,\mathcal{O}_Y(D)) \subset H^0(X,\mathcal{O}_X(f_*(D)))$ . I saw this statement in Kollar and Mori Lemma 3.40 using this result. I am not sure if this statement is a consequence of some exact sequence? Or can we use something projection formula to deduce it? Sorry I don't have good idea to handle the push foward of divisor？","['algebraic-geometry', 'commutative-algebra']"
4709492,"Prove: if $x^2 + y^2 < \frac{1}{2}$, then $\cos(x+y) > 0$","For an exercise about finding the minimum of a function I have to prove that $cos(x+y) > 0$ for $x^2 + y^2 < \frac{1}{2}$ . Which means that $-\frac{\pi}{2}< x + y < \frac{\pi}{2}$ . However, I am struggling with drawing any conclusions about $x + y$ from the expression with the squares. Does anyone have tips for that? only thing I have so far, is this: $-\frac{\pi}{2} < -\frac{1}{\sqrt{2}} < -\sqrt{x^2 + y^2} \leq \sqrt{x^2 + y^2} < \frac{1}{\sqrt{2}} < \frac{\pi}{2}$","['analysis', 'polar-coordinates', 'functions', 'trigonometry', 'extreme-value-analysis']"
4709499,Let $A\in M_3[\mathbb{R}]$ such that $A^{2019}+A=$,"Let $A\in M_3[\mathbb{R}]$ such that $A^{2019}+A=\begin{bmatrix}
2 & 2 & 0 \\ 0 & 2 & 2 \\ 0 & 0 & 2\end{bmatrix}.$ Find $tr(A^{2019})$ and $\det(A).$ My work: Let characteristic equation of $A$ be $x^3-tx^2+sx-d=(x-\lambda_1)(x-\lambda_2)(x-\lambda_3)=0$ $\implies x^{2019}=Q(x)(x-\lambda_1)(x-\lambda_2)(x-\lambda_3)+ax^2+bx+c,\text{ where }a\lambda_{i}^2+b\lambda_i+c=\lambda_i^{2019}\,\,\forall\,\, i=1,2,3.$ $\implies A^{2019}+A=aA^2+(b+1)A+cI_3=\begin{bmatrix}
2 & 2 & 0 \\ 0 & 2 & 2 \\ 0 & 0 & 2\end{bmatrix}$ $\implies a\times tr(A^2)+b\times tr(A)+3c=6$ $\implies a\sum\lambda_i^2+(b+1)\sum\lambda_i+3c=6$ $\implies \sum (a\lambda_i^2+(b+1)\lambda_i+c)=6$","['matrices', 'matrix-equations']"
4709539,Line integral of closed and otherwise non-intersecting curves on a cylindric surface,"In my multi variable/vector calculus textbook I encountered a problem which led to more questions than answers when i solved it. Let me start by stateing the problem: The problem Let $\gamma$ be a curve on the cylinder surface $x^2+y^2=1$ . $\gamma$ is closed but otherwise it does not intersect with it self.
Let $\mathbf{F}=(x+y,2yz,y^2)$ be a vector field. Show that $\oint_\gamma \mathbf{F}\bullet d\mathbf{r}$ only can take on 3 different values depending on how $\gamma$ runs along the cylinder surface. Determine these values and describe the different cases. My attempt and initial thoughts My inital thought was: How many types of closed curves can there be on a cylindric surface given the conditions. The intersect between the cylinder and a plane could give $\gamma$ either as a circle or a ellipse and still be closed. A plane $\Pi : ax+by+cz=d$ where c=0 cannot as I see it give a closed curve $\gamma$ . I also thought about how different geometric shapes could intersect with the cylinder and give a closed curve, spheres e.t.c. But I decided to start by just examine the case of a plane intersecting the cylinder. Let $\Pi$ be the plane $ax+by+cz=d$ with $a,b,c\in\mathbb{R}$ and where $x^2+y^2\leq 1$ .
Let $\gamma$ be the intersect curve between the cylinder $x^2+y^2=1$ and $\Pi$ .
Then the unit normal of $\Pi$ is $\mathbf{N}=\frac{(a,b,c)}{\sqrt{a^2+b^2+c^2}}$ . In preperation I calculated $\text{curl}(\mathbf{F})=\nabla \times\mathbf{F}=(0,0,-1)$ By Stoke´s theorem: $$\oint_\gamma \mathbf{F}\bullet d \mathbf{r}=\iint_\Pi (\nabla \times\mathbf{F})\bullet\mathbf{N} dS=\frac{-c}{\sqrt{a^2+b^2+c^2}}\iint_\Pi dS=\frac{-c}{\sqrt{a^2+b^2+c^2}}\mu (\Pi)$$ Where $\mu (\Pi)$ is the area of $\Pi$ I then proceed to calculate this area, interpreting the plane as a equipotential surface of $f(x,y)=\frac{d-ax-by}{c}$ , where $x^2+y^2\leq 1$ . Then: $$\mu (\Pi)=\iint_{x^2+y^2\leq 1} \sqrt{1+(f^´_x)^2+(f^´_y)^2}dxdy=\iint_{x^2+y^2\leq 1} \sqrt{1+(\frac{-a}{c})^2+(c{-b}{c})^2}dxdy=\iint_{x^2+y^2\leq 1} \frac{\sqrt{a^2+b^2+c^2}}{|c|}dxdy=\frac{\sqrt{a^2+b^2+c^2}}{|c|} \int_{0}^{2\pi}\int_{0}^{1} rdrd\theta=\ldots=\frac{\pi\sqrt{a^2+b^2+c^2}}{|c|}$$ Then: $$W=\oint_\gamma\mathbf{F}\bullet d\mathbf{r}=\frac{-c\pi}{|c|}$$ Thus: $c>0$ then $\gamma$ has negative orientation around the z-axis and $W=-\pi$ $c=0$ then $\gamma$ does not go around the z-axis and $W=0$ $c<0$ then $\gamma$ has positive orientation around the z-axis and $W=\pi$ Textbook answer sheet $\gamma$ has negative orientation around the z-axis and $W=-\pi$ $\gamma$ does not go around the z-axis and $W=0$ $\gamma$ has positive orientation around the z-axis and $W=\pi$ Questions First of the answer in the textbook is clearly not the same as mine but quite simular, why do they dont have c in their answer? Then, the first and the third case makes sense to me given the problem statement, but second as I meantioned earlier does not. Am I wrong if I think that if $c=0$ then $\gamma$ would just be two straight lines on the of the cylinder? And in the bigger picture, to me it didn´t seem like I covered all possible curves given the conditions. Did I? And if not which most probabilly is the case, how would I do it? For example consider the intersection of the cylinder and the sphere $(x-\frac{1}{2})^2+y^2+z^2=1$ . This is my first post ever and I hope I have been clear and thorough:)","['analytic-geometry', 'multivariable-calculus', 'vector-analysis']"
4709565,how to solve $\lim_{x\to0}\frac{e^x-e^{-x}-2x}{x^3}$ without L'Hopital rule or Taylor series,"I want to compute $\lim_{x\to0}\frac{e^x-e^{-x}-2x}{x^3}$ without L'Hospital rule  or Taylor series my second question there is a part that I don't understand which is $$\lim_{x\to0}\frac{e^x-e^{-x}-2x}{x^3}$$ $$=\lim_{x\to0}\frac{2e^{-x}(e^{2x}-1)}{2xx^2}-\frac{2}{x^2} $$ as $$=\lim_{x\to0}\frac{(e^{2x}-1)}{2x}=1$$ then $$=\lim_{x\to0}\frac{2e^{-x}(e^{2x}-1)}{2xx^2}-\frac{1}{x^2} $$ = $$\lim_{x\to0}\frac{2e^{-x}}{x^2}-\frac{2}{x^2} $$ which is infinity but this answer is wrong the answer is $\frac{1}{3}$ but why my solution was wrong ? isn't $\lim(a+b)=\lim(a)+\lim(b)$ and $\lim(ab)=\lim(a)*\lim(b)$ ?
so where does the error comes from
i think it comes from the derivative step but I don't understand why","['limits', 'calculus']"
4709572,What is the Hessian of $x \mapsto\log \det \left( A^T A + R^T \operatorname{diag}(x)^{-1} R \right)$?,"This is a follow-up to a previous question I asked regarding the hessian of a similar log determinant. The log determinant I am considering is given by $$
L(\vec{x}) = \log \det \left( A^T A + R^T D_x^{-1} R \right),
$$ where $x \in \mathbb{R}^q$ with all positive entries, $D_x = \operatorname{diag}(x)$ is a diagonal matrix, and $A \in \mathbb{R}^{p \times n}$ , $B \in \mathbb{R}^{q \times n}$ . What is the Hessian of $L(x)$ with respect to $x = (x_1, \ldots, x_q)^T$ ? I have determined that the gradient can be written as $$
\nabla_x L(x) = - \operatorname{diag}\left( D_x^{-1} R \left( A^T A + R^T D_x^{-1} R \right)^{-1} R^T D_x^{-1}  \right).
$$ Using an identity for the $\operatorname{diag}$ operator, this can also be written as $$
\nabla_x L(x) = - \left( \left( D_x^{-1} R  \right) \odot \left( D_x^{-1} R  \right) \right) \operatorname{diag}\left( \left( A^T A + R^T D_x^{-1} R \right)^{-1}  \right).
$$ Any advice on how to proceed from here? I am thinking the next step to find the gradient is to apply a product rule to this expression.","['scalar-fields', 'matrices', 'multivariable-calculus', 'matrix-calculus', 'hessian-matrix']"
4709582,Real valued continuous function on a closed disc,"Assume that $D\subseteq \mathbb R^2$ is a closed disc and $S$ is its boundary circle. Moreover $x,y\in S$ are distinct points, which determine exactly two arcs $I,J\subseteq S$ , and $f:D\rightarrow \mathbb R$ is a continuous function such that $f(x)<0$ , $f(y)>0$ . I want to prove the following: There exist points $p\in I$ , $q\in J$ and a connected set $C\subseteq f^{-1}(0)$ such that $p,q\in C$ . To me it seems intuitively true, but I have no serious idea how to prove it. Any help appreciated!","['general-topology', 'real-analysis']"
4709600,Show that in gradient descent the gradient goes against 0 (Under certain conditions),"So we have gradient descent: $$x^{(i+1)} = x^{(i)} - \tau \nabla f(x^{(i)})$$ And we gotta show that $$\left|\nabla f\left(x^{(j)}\right)\right| \to 0$$ The conditions are: $f: \mathbb R^n \to \mathbb R$ is differentiable $\nabla f: \mathbb R^n \to \mathbb R^n$ is lipschitz continuous, so there exists an $L > 0$ such that: $$|\nabla f(x)-\nabla f(y)| \leq L|x-y|,\ \forall x,y \in \mathbb R^n$$ $\tau < \frac{1}{L}$ I've been trying to solve this for like an hour and can't get any further, can someone point me in the right direction? I've been trying the following: Assume there exists an $\epsilon > 0$ , such that $|\nabla f(x^{(j)})| \geq \epsilon$ for infinitely many j but I haven't been able to find any contradiction with the assumptions.","['numerical-optimization', 'lipschitz-functions', 'continuity', 'multivariable-calculus', 'gradient-descent']"
4709618,How do I get the differential of a function that depends on the sum of itself?,"I have a very long and complicated formulation of an equation $X(P,T)$ and I want to calculate its differential. I will try to make it the simplest as possible. If I am missing anything, please, don't hesitate to ask me. I managed to have it in the form : $$dX = \frac{\partial X}{\partial P}dP + \frac{\partial X}{\partial T}dT + \frac{\partial X}{\partial k}dk$$ where I know: $$\frac{\partial X}{\partial P} = A.X$$ $$\frac{\partial X}{\partial T} = B.X$$ $$\frac{\partial X}{\partial k} = C.X$$ with (A,B,C) are functions that I know and depends or $P,T,k$ as : $A=A(P)$ , $B=B(T,k)$ and $C=C(T,k)$ . The part that is causing me troubles is the $k$ function because it is a function of $X$ : $$k=\sum_{i=1}^N k_i X_i$$ and $$dk = \sum_{i=1}^N k_i dX_i$$ $k_i$ are constants and $dX_i$ is the $i^{th}$ element of $dX$ from the 1st equation. How can I get rid of the $dk$ term and get something that would look like $dX = f(P,T,X)dP + g(P,T,X)dT$ ? Edit: $P$ and $T$ variables are floats while $X,k$ are 1-D matrices as well as $A,B,C$ . Edit: Below is the formulation of the vector X, which is computed in an iterative way: $$X_i^{n+1} = exp\left( \frac{H_i}{T} + (1+\alpha x) \left(\frac{M_i}{1+xe^{\alpha x}} - \frac{k_i}{k} \right) \right) $$ where $H_i, M_i$ are constant matrices, $\alpha$ is a constant, $n$ is the iteration number, $x=\frac{k}{PT}$ , $k=\sum_{j=1}^N k_j X_{j}^n$ with $k_j$ constants. My goal is to differentiate X to obtain the form : $dX = f(P,T,X)dP + g(P,T,X)dT$ . Thank you all in advance for your help.","['summation', 'functions', 'derivatives']"
4709693,What does $\sum_{1\le k\le n}$ mean in this context?,"Why is: $$\sum_{k=1}^\infty \sum_{n=k}^\infty \frac{1}{n(n+1)} k\, a_k
=\sum_{1\le k\le n}\frac{1}{n(n+1)} k\, a_k=\sum_{n=1}^\infty \frac{1}{n+1}\; \frac{1}{n}\sum_{k=1}^n k\, a_k$$ What does the middle term mean and why is it equal to the first? I thought $\sum_{1\le k\le n}=\sum \limits_{k=1}^{n}$ .","['notation', 'summation', 'analysis']"
4709757,"Probability, potential theory and complex analysis","The connection between Markov processes and Potential Theory is well known, as is conformal invariance of Brownian motion which allows probabilistic proofs of statements in Complex Analysis, like Picard's theorem.
What are some results in these areas (Potential Theory, Complex Analysis, maybe Complex or Differential Geometry) that admit probabilistic proofs, and possibly for which no other proof is known?
To what extent are probabilistic methods in these areas actually crucial in proving new theory and to what extent are they more of an interpretation of facts that are already known?
It would be great if there were some survey about this. Thanks in advance for your answers.","['complex-analysis', 'potential-theory', 'probability-theory', 'probability', 'differential-geometry']"
4709767,"If $f:\mathbb R\to\mathbb R$ is continuous and $f(x)f(x+2)+f(x+1)=0$ for all $x$, prove $f(x)=0$ for infinitely many $x$","Let $f:\mathbb{R}\to\mathbb{R}$ be a continuous function such that $f(x)f(x+2)+f(x+1)=0$ for all $x\in\mathbb{R}$ . Prove that there are infinitely many real values of $x$ such that $f(x)=0$ . Here is my approach (by inspection): For $x=0$ we have $f(0)f(2)+f(1)=0$ (Eq1). Now, if $f(0)=0$ , then $f(1)=0$ . From $f(1)f(3)+f(2)=0$ for $x=1$ we can also conclude that $f(2)=0$ . In this case it's not hard to prove that $f(n)=0\ \ \forall n\in\mathbb{N}$ . On the other hand, if $f(0)>0$ , then Eq1 let us conclude that $f(1)$ and $f(2)$ have opposite signs, say $f(1)<0<f(2)$ . In this case, the IVT guarantes that there exists a $c\in (1,2)$ such that $f(c)=0$ . Again, it's not hard to prove that $f(n+c)=0\ \ \forall n\in\mathbb{N}$ . I am stuck with the case where $f(0)<0$ . In this case $f(1)$ and $f(2)$ have the same sign and I couldn't figure out what to do next. Any hint to this case or a different approach would be great. Thanks in advance.","['continuity', 'functions', 'functional-equations', 'real-analysis']"
4709773,Unique fixed point in a group action,"I have been doing algebra exercises recently and I stumbled across this problem that I struggled to solve. Suppose a finite group $G$ acts on a finite set $A$ so that for every nontrivial $g \in G$ there exists a unique fixed point (i.e., there is exactly one $a \in A$ , depending on $g$ , such that $g(a) = a$ ). Prove that this fixed point is the same for all $g \in G$ . I had a feeling that we were supposed to use the orbit stabilizer theorem but I didn't know how. So I made the following attempt: Suppose $|G|=n$ and suppose all $g_i\in G$ such that $g_i(a_0)=a_0\in A$ except for $m$ of them. Then $G=\{g_1,...,g_{k1},...,g_{km},...,g_n\}$ acts on $a_0$ will have the orbit $\{a_0,...,a_{k1},...,a_{km},...,a_0\}$ . Then we let $G$ acts on $a_{k1}$ and get the orbit $\{a_1,a_2,...,a_0,a_{k1},...,a_n\}$ . $a_0$ is in it because we have $g_{k1}^{-1}$ and $a_{k1}$ exists because these two orbits have $a_0$ in common $\Rightarrow$ they are the same orbit. Moreover, $g_{k1}$ can't fix $a_{k1}$ therefore $\exists g_{kx}(a_{k1})=a_{k1}$ . This is the furthest I can get. I'm trying to force a contradiction by assuming the negative but got really stuck. Could you give me some hints to push me forward? If this direction is totally wrong, what would be the correct path? Thanks a lot!","['finite-groups', 'fixed-points', 'abstract-algebra', 'group-theory', 'group-actions']"
4709806,"If $f(x+y)=f(x)+f(y)$ and $\lim_{x \rightarrow 0} f(x)$ exists, prove $\lim_{x \rightarrow 0} f(x) = 0$ and $\lim_{x \rightarrow p} f(x) \ \forall p$","I'm trying to solve the problem 19 in Manfred Stoll's ""Introduction to Real Analysis"", Exercises 4.1 Let $f : \mathbb{R} \rightarrow \mathbb{R}$ satisfy $f(x+y)=f(x)+f(y) \ \forall x, y \in \mathbb{R}$ . If $\lim\limits_{x \rightarrow 0} f(x)$ exists, prove (a) $\lim\limits_{x \rightarrow 0} f(x) = 0$ and (b) $\lim\limits_{x \rightarrow p} f(x)$ exists $\forall p \in \mathbb{R}$ . I solved (a) by $\begin{align}
f(2x) & = f(x)+f(x) \\
\lim_{x \rightarrow 0} f(2x) & = \lim_{x \rightarrow 0} f(x) + \lim_{x \rightarrow 0} f(x) \\
\lim_{x \rightarrow 0} f(x) = \lim_{2x \rightarrow 0} f(2x) & = \lim_{x \rightarrow 0} f(x) + \lim_{x \rightarrow 0} f(x) \\
\therefore \lim_{x \rightarrow 0} f(x) & = 0,
\end{align}$ but I have no idea to solve (b). Is there any hint for (b)?","['limits', 'real-analysis']"
4709817,Why only one singularity is involved? $\int_{0}^{2\pi} \frac{1}{13+5\sin(\theta)}~d\theta$,"I solved the integral $$\int_{0}^{2\pi} \frac{1}{13+5\sin(\theta)}~d\theta$$ with the residue theorem and Cauchy’s Integral Formula. The following is the solution, but I am unsure why in the end we consider only one of the residues which is a singularity point in the unit circle. Making the substitution $z=e^{i\theta}$ , $d\theta=\frac{dz}{iz}$ , with $z$ being the unit circle in the complex plane: $$ \int_{0}^{2\pi}\frac{1}{13+5\sin(\theta)}d\theta = 2\oint_{C}\frac{1}{5z^2+26iz-5}dz = \frac{2}{5}\oint_{C}\frac{1}{(z+\frac{i}{5})(z+5i)}dz$$ Therefore, the singular points are $-i/5$ and $-5i$ . Now this is where I’m confused. Since we made the substitution on the unit circle, the only relevant singularity point is $-i/5$ , and thus we ignore the other residue when using Cauchy’s Integral Formula. $$ \frac{2}{5}\oint_{C}\frac{1}{(z+\frac{i}{5})(z+5i)}=\frac{2}{5}\cdot2\pi i\cdot \operatorname{Res}\left(z=-\frac{i}{5}\right)=\frac{\pi}{6}$$ My question is, since we arbitrarily used the unit circle for our substitution, isn’t it just as valid to use a substitution for a circle with a larger radius? Could the larger radius lead to all singular points of $f(z)$ being inside this larger circle, resulting in two residues? Or will the substitution with the inclusion of radius $R$ at the beginning lead to the same outcome?","['cauchy-integral-formula', 'complex-analysis', 'contour-integration', 'residue-calculus', 'complex-integration']"
4709836,"A novice substitution in differentiation, why is it wrong?","Edit: It is given that $f$ is continuously differentiable. One of my colleague's student wrote this: Let $x=2-h,$ $$\lim_{h\to0}\frac{f(2+3h)-f(2-h)}{h}=4\lim_{h\to0}\frac{f(x+4h)-f(x)}{4h}=4f'(2)$$ We feel repelled to accept the latter equality, but aren't sure why, other than that we've always used the more common method of making up extra terms $-f(2)+f(2)$ in the nominator. I've tried to find an example where the student's treatment of two related variables in a single limit would lead to an error, but nothing can seem to go wrong. What would you say as a teacher of calculus?","['analysis', 'continuity', 'calculus', 'limits', 'derivatives']"
4709840,Probability 5-card hand contains exactly 2 pairs,"What is the probability that a five-card poker hand contains two pairs (that is, two of each of two different kinds and a fifth card of a third kind)? The textbook answer is 198/4165 But why doesn't it work that: Since we know the first card can be any card, it will be 52/52, the next card must be one of the same kind which is just 3/51, since there's 4 cards of a kind and one has already been picked. Then repeat for the next card excluding the remaining 2 cards of the last kind, 48/50, and the next being one of the same here 3/49. The last card being any other card 44/48 Multiply them all together (52 * 3 * 48 * 3 * 44) / (52 * 51 * 50 * 49 * 48) * 5!, the possible permutations  = 1584/4165, why does this process not work when the logic seems reasonable?","['combinatorics', 'probability']"
4709849,Successive dice pool probabilities,"So, I cannot wrap my head around how to calculate this. Here is my problem:
In Warhammer you roll a pool of attacks, the attacks hit (for example) on a result of 4+ on a six sided dice. Then all the hits form a new dice pool and get rolled again transforming into wounds on a 3 or more.
How do I calculate my probability to get an arbitrary number of wounds? Calculating the average is fairly straightforward but it seems to me that the fact that size of the second pool of dice is dependent on the result of the first should have some kind of bearing on the formula that I am not seeing.",['probability']
4709859,How many ways can divide 12 books among 4 children,"How many ways can divide 12 different books among 4 children, so that each child reaches 3 books? each of two older children get 4 books and each of two younger children get 2 books? I think the answer is: 1) We can think of this as arranging the 12 books in a row and then dividing them into groups of 3, where the order within each group does not matter. We can use the formula for combinations to find the number of ways to do this: C(12,3) x C(9,3) x C(6,3) x C(3,3) = 739,200. we can approach this problem by dividing it into two steps. First, we choose the 4 books for each of the two older children, and then we choose 2 books for each of the two younger children. To choose the 4 books for each of the two older children (order in which we choose the books does not matter): C(12,4) x C(8,4) = 34,650 We have 4 books left. For the first younger child, we have 4 books to choose from, so we have C(4,2) = 6 . For the second younger child, we have 2 books left, so we have C(2,2) = 1 . Finally, We get 34,650 x 6 I'm not sure my answer is right.","['solution-verification', 'combinatorics', 'discrete-mathematics']"
4709925,Does the double integral diverge? $\iint_{\Omega} \sqrt{\frac{x^2 + y^2}{xy(1 - x^2 - y^2)}} ~ dx dy$,"Does the double integral converge $$\iint_{\Omega}  \sqrt{\frac{x^2 + y^2}{xy(1 - x^2 - y^2)}} ~ dx dy$$ I tried using polar coordinate but then half way I realised it was not a good idea so what do I do? I got $$\iint_{\Omega} \frac{r}{\sqrt{\cos(\theta)\sin(\theta) (1-r^2)}}drd\theta = \int \frac1{\sqrt{\cos(\theta)\sin(\theta)}}d\theta \times \int \frac{r}{\sqrt{(1-r^2)}},dr$$ $$\Omega = \left\{ (x, y) \in \mathbb{R}^2 : x^2 + y^2 < 1, x > 0, y > 0 \right\}$$ But I don't know how to continue, I haven't written the bounds yet because I was trying to simplify it.","['integration', 'definite-integrals', 'improper-integrals', 'multivariable-calculus', 'calculus']"
4709927,When is a solution of a differential matrix Riccati equation positive definite?,"The question is in the context of linear control systems. Let $A(t)$ , $R(t)$ , and $Q(t)$ be time-varing square $n\times n$ matrices of reals. For all $t$ , $R(t)\ge 0$ and $Q(t)\ge 0$ are semi positive-definite. Consider the differential matrix Riccati equation $$\dot{X}(t) = A(t) X(t) 
+ X(t) A^\top(t) - X(t) R(t) X(t) + Q(t)$$ with a positive-definite initial condition $X(0)>0$ . My question are When $X(t)$ is bounded for all $t$ ? Is it enough to say that $R(t)> 0$ or it should be $R(t) \ge R_0 > 0$ ? When $X(t)$ is non-singular for all $t$ ? Is it enough to say that $Q(t)> 0$ or it should be $Q(t) \ge Q_0 > 0$ ? When (necessary, sufficient, iff) does there exist $X_0>0$ such that $X(t) \ge X_0$ for all $t$ ? Usually, control books consider the algebraic equation, i.e. for $\dot{X}(t)=0$ , or start with assumptions on uniform controllability and observability of $R$ and $Q$ implying they are positive definite. Are you aware of books/papers that coniser semi definite cases?","['ordinary-differential-equations', 'control-theory', 'matrix-calculus', 'linear-control', 'matrix-equations']"
4709944,Integrating $\int_0^\infty\frac{u^2}{\sqrt{u^2+a^2}}J_1(ru)e^{-z\sqrt{u^2+a^2}}du$,"I came across the following non-trivial improper integral while I was elaborating on a fluid mechanical problem involving porous media: $$
f(r,z,a) = \int_0^\infty \frac{u^2}{\sqrt{u^2+a^2}} J_1(ru) e^{-z\sqrt{u^2+a^2}} \mathrm{d}u , 
$$ wherein $r$ , $z$ , and $a$ are positive real numbers.
Physically, $a$ is a measure of the resistance of the porous medium against flow (impermeability coefficient.)
Using a different solution route of the original problem (which does not involve integral transforms), I was able to find that $f(r,z,a)$ is actually given by $$
f(r,z,a) = \frac{r(1+aR)}{R^3} e^{-aR} ,
$$ with $R = \sqrt{r^2+z^2}$ .
I can check that this expression is correct by numerical substitution but I have no clue how this could be rigorously proven. What I tried is to use Poisson's representation of the Bessel function but this does not lead to the desired result.
It would be highly appreciated if someone here could be of help to clarify how this could be the case.
Thank you! In particular, it can readily be checked that in the limit case $a=0$ both expressions lead to the same value, namely, $$
f(r,z,0) = \frac{r}{\left( r^2+z^2\right)^{3/2}} .
$$","['integration', 'improper-integrals', 'definite-integrals', 'calculus', 'bessel-functions']"
4709951,The number $2$ in cohomology theories,"I've started feeling this rather curious mystique coming from an unaddressed - at least in my experience - excessive presence of the number $2$ in a few different areas of maths. My curiosity really sparked the moment I realised there was a mismatch between different explanations lurking in my knowledge of cohomology: Of course, the singular cohomology of a complex variety should vanish in degrees greater than double the dimension of the variety, since complex varieties are effectively of real dimension twice their complex dimension, and this is because the index of the field extension $\mathbf{C}/\mathbf{R}$ is two - singular co/homology is a theory fundamentally built from simplices which are inherently "" real "" objects. Singular cohomology really isn't about simplices: these are just a way of accessing it concretely; singular cohomology is really just sheaf cohomology with the simplest possible choice of coefficients. Even when you consider cohomologies which aren't quite modelled on the same sorts of spaces, such as the étale cohomology of schemes or that of spaces which locally look nothing like $\mathbf{C}$ or $\mathbf{R}$ , like adic spaces or diamonds, somehow the main point is that these are formalisms which give back the same or analogous results you're supposed to get for the "" complex varieties analogue "" of whatever it is you're computing. I felt particularly stumped when I learned that the ubiquitous number $2$ appearing in the representation theory of semisimple Lie algebras and algebraic groups can also be given a cohomological explanation, via the cohomology of flag varieties - somehow the $2$ appearing there corresponds to the intersection theory of their singular subvarieties (via their Chow groups/perverse sheaves) and the fact that "" the intersection of a pair of (real) even-dimensional varieties gives back an even-dimensional variety "". It might be because I'm only somewhat acquainted with only a few of these theories, being still just a student, but I feel like there's something I'm not seeing here that all my teachers do: how can it possibly be that $\mathbf{P}^n_{\overline{\mathbf{Q}}}$ with coefficients in $\underline{\mathbf{Z}/n}\in \text{Sh}_{\text{Ab}}(\mathbf{P}^n_{\overline{\mathbf{Q}},\text{ét}})$ has exactly the same cohomology as $\mathbf{C}P^n$ with coefficients in $\mathbf{Z}/n$ if the latter theoretically depends on the equality $[\mathbf{C}:\mathbf{R}] = 2$ whereas the former has no mention of either the real or complex numbers...? I apologise if my question is a little vague and rather soiled with my little experience; I'd really appreciate any piece of wisdom (however relevant) you might have. Thanks for reading! :D","['etale-cohomology', 'complex-geometry', 'representation-theory', 'algebraic-geometry', 'big-picture']"
4710006,A limit question involving summation $\sum_{k=1}^{n} k \ln(\frac{n^2+(k-1)^2}{n^2+k^2})$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question $$\lim_{n\to \infty } \frac 1n \sum_{k=1}^{n} k  \ln\left(\frac{n^2+(k-1)^2}{n^2+k^2}\right)$$ Basically, First thought I had was to convert the limit into definite integral but couldn't do it.I don't have any other idea to solve it. Thanks for any help in advanced.","['limits', 'calculus', 'summation']"
4710032,"For every matrix $A$ mod $r$, is there a nonzero vector $\vec x$ so that $A \vec x$ has 0 mod $r$ nonzero entries?","Definitions: Let $\lVert \vec x \rVert_0$ be equal to the number of nonzero entries in $\vec x$ . I'll write $\mathbb{Z} / r\mathbb{Z}$ , the ring of integers mod $r$ , as $\mathbb{Z}_r$ for brevity. What I'd like to prove: For all $r \geq 2 \in \mathbb{N}$ (not necessarily prime), for all sufficiently large $n \in \mathbb{N}$ , for all $m \in \mathbb{N}$ , and for all $A \in \mathbb{Z}_r^{m \times n}$ , there exists some nonzero $\vec x \in \mathbb{Z}_r^{n \times 1}$ such that $\lVert A \vec x  \rVert_0 \equiv 0 \mod r$ , where both the matrix multiplication and the 0-""norm"" are taken mod $r$ . Current progress: Certainly when $m < n$ , $A$ is singular, so the statement holds. When $r$ is prime, the full statement holds: we can use Fermat's Little Theorem to rewrite $\lVert A \vec x \rVert_0 \mod r$ as a polynomial in the entries of $\vec x$ . The degree of the polynomial (in terms of the components of $\vec x$ ) is $r-1$ . But any polynomial that is nonzero exactly on nonzero inputs (that is, a polynomial representing $OR$ over $\mathbb{Z}_r$ ) has degree at least $n$ : the unique polynomial that is 1 only on $\vec 0$ and 0 otherwise is $NOR(\vec x) = \prod_i (1-x_i^{r-1})$ , which has degree $n(r-1)$ . If we had a polynomial representing $OR$ of degree $< n$ , we could write $NOR(\vec x) = 1 - OR(\vec x)^{r-1}$ , which would have degree $< n(r-1)$ , a contradiction. So, as soon as $n \geq r$ there must be a nonzero $\vec x$ that produces a 0-norm congruent to 0 mod $r$ . I'm pretty sure when $r$ is a prime power, similar polynomial tricks will also work. But when $r$ is composite with distinct prime factors, I run out of tricks and get stuck.","['matrices', 'number-theory', 'linear-algebra', 'combinatorics']"
4710047,How to solve this ODE: $(x^2-y^2+y)dx+x(2y-1)dy=0$?,"I tried to rewrite it as $$(x^2-y^2)dx+ydx-xdy+2xydy=0,$$ divide by $y^2$ ,then get $$(\frac{x^2}{y^2}-1)dx+d(\frac{x}{y})+2\frac{x}{y}dy=0,$$ let $\frac{x}{y}=t, $ and $\frac{dx}{dy}=t+yt'$ ,then $$t^3+yt^2t'+t-yt'+t'=0,$$ But I don’t know how to calculate it later. Can anyone help me? Thank you!","['integration', 'calculus', 'ordinary-differential-equations']"
4710073,A confusing proof-outline for the Schröder-Bernstein Theorem.,"I have seen a number of  proofs of the much popular Schroder–Bernstein Theorem. But a particular proof outline interested me. My version of the Schroder–Bernstein Theorem is : Assume there exists a $1–1$ function $f : X → Y$ and another $1–1$ function $g : Y → X.$ Then there exists a $1–1,$ onto function $h : X → Y $ The proof outline was stated as follows: (a) The range of $f$ is defined by $f(X) = \{y\in Y:y=f(x) \text{ for some} x ∈ X\}.$ Let $y ∈ f(X).$ (Because $f$ is not necessarily onto, the range $f(X)$ may not be all of $Y .$ ) Explain why there exists a unique $x ∈ X$ such that $f(x) = y.$ Now define $f ^{−1}(y) = x,$ and show that $f^{ −1}$ is a $1–1$ function from $f(X)$ onto $X.$ In a similar way, we can also define the $1–1$ function $g^{−1} : g(Y) → Y .$ (b) Let $x ∈ X$ be arbitrary. Let the chain $C_x$ be the set consisting of all elements of the form $$(1) \space\space\space\space\space\space\space... ,f ^{−1}(g^{−1}(x)), g^{−1}(x), x, f(x), g(f(x)), f(g(f(x))),... .$$ Explain why the number of elements to the left of $x$ in the above chain may be zero, finite, or infinite. (c) Show that any two chains are either identical or completely disjoint. (d) Note that the terms of the chain in $(1)$ alternate between elements of $X$ and elements of $Y .$ Given a chain $C_x$ , we want to focus on $C_x ∩Y $ , which is just the part of the chain that sits in $Y .$ Define the set $A$ to be the union of all chains $C_x$ satisfying $Cx ∩ Y ⊆ f(X).$ Let $B$ consist of the union of the remaining chains not in $A.$ Show that any chain contained in $B$ must be of the form $y, g(y), f(g(y)), g(f(g(y))),... ,$ where $y$ is an element of $Y$ that is not in $f(X).$ (e) Let $X_1 = A ∩ X, X_2 = B ∩ X, Y_1 = A ∩ Y ,$ and $Y_2 = B ∩ Y .$ Show that $f$ maps $X_1$ onto $Y_1$ and that $g$ maps $Y_2$ onto $X_2.$ Use this information to prove the existence of a bijective mapping from $X$ to $Y.$ However, I am facing some problems while proceeding with this proof-outline. I could solve the claims in $a,b$ and $c.$ Problem 1(Solved): Regarding this problem, currently I have no issues with this anyomore. Please check the other problem i.e, $\color{green}{\text{Problem 2 (Unsolved)}}$ . But the problem lies with part $d.$ What I infered was, that : If $\exists C\in  B$ then, $\exists y\in Y$ and $y\in C$ such that $y\notin f(X)$ . But we note that $g^{-1}(g(Y))=Y$ so, $y\in g^{-1}(g(Y))-f(X).$ I found the proof given for part $d.$ in the manual as : (d) Let $C$ by a chain in $B.$ Then $C\cap Y$ is not a subset of $f(X),$ so there exists $y\in Y$ with $y\in C$ but $y\notin f(X).$ Note that $C$ and $C_y$ have a point in common, so they must be equal. But I feel that something has gone awry with the logic of the proof in the manual. This is only because, up until now, we defined chains only for the elements in $X$ and as $y\in Y(\implies y\notin X$ ) so, I don't understand what is meant by $C_y.$ Further, if I am assume, that $C_y$ is the sequence $y, g(y), f(g(y)), g(f(g(y))),... ,$ where $y$ is an element of $Y$ , there is still some problem. In part $b$ of the proof, we have proved either $C_x$ or $C_{x'}$ are completely disjoint or equal. I think, WLOG, we  can say that this assertion holds good for $C_y$ and $C_{y'}$ (where, $y,y'\in Y$ ) as well. But in this case (i.e in $d$ ) it seems that the author asserts the same thing to hold true for $C_x$ and $C_y$ (,where $x\in X,y\in Y$ ) which is, $C_x$ and $C_{y}$ are either completely disjoint (or $C_x\cap C_{y}=\phi$ ) or equal. But how can we say this, just like that, without a rigorous proof ? Problem 2(Unsolved): The proof for for claim in part $d.$ of the proof outline, is given as: Note that $Y_1\in f(X).$ To show $f : X_1\to Y_1$ is onto we pick a $y_1\in Y_1$ and show there exists $x_1 \in X_1$ with $f(x_1) = y_1.$ Well, if $y_1\in Y_1,$ we know there exists $x_1\in X$ such that $f(x_1) = y_1.$ But we must be sure that $x_1\in X_1.$ However, $C_{x_1}$ contains $y_1$ which is an element of some chain in $A.$ Since chains that intersect must be identical,  so, $C_{x_1}\subset A$ , and $x_1\in X_1.$ Now we have to show $g : Y_2\to X_2$ is onto. To do this, we pick $x_2\in X_2$ and show that there exists $y_2\in Y_2$ with $g(y_2) = x_2.$ Since $x_2\in X_2\subset  g(Y )$ we know there exists $y_2\in Y$ such that $g(y_2) = x_2.$ Now we need to show that $y_2\in B$ because if $y_2 \in Y$ and $y_2\in B$ then $y_2\in B \cup Y = Y_2.$ We know that $C_{x_2}$ contains $y_2$ which is an element of some chain $B.$ Since chains that intersect
are identical $C_{x_2} \subset B, y_2 \in B,$ and hence $y_2 \in Y_2.$ Finally, to prove $X$ and $Y$ has a bijection between them, define $h : X\to Y$ by $$h(x) =f(x) \text{ if } x\in X_1$$ $$h(x)=g^{-1}(x) \text{ if } x\in X_2$$ Because $X = X_1\cup X_2$ and $f$ and $g^{-1}$ are $1–1$ and have disjoint ranges on these respective spaces, we get that $h$ is $1–1.$ Because $Y = Y_1\cup Y_2$ and $f$ and $g^{-1}$ are respectively onto, it follows that $h$ is onto as well. So, $h$ is a bijection. But the thing that confuses me, is that: (i) In the portion, where it proves, that $f:X_1\to Y_1$ , how are they so sure, of the fact that $\forall x\in X_1$ , $f(x)\in Y_1.$ I know that $Y_1\subset f(X)$ but it is never proved that $Y_1=f(X)$ till then ? It might happen for a particular $x\in X_1$ that, $f(x)\in f(X)-Y_1.$ (ii) In the portion where it proves, that $g:Y_2\to X_2,$ I have the same concern as in (i) above i.e how are they so sure, of the fact that $\forall y\in Y_2$ , $g(y)\in X_2.$ I know that $X_2\subset g(Y_2)$ but it is never proved that $X_2=g(Y_2)$ till then ? It might happen for a particular $y\in Y_2$ that, $g(y)\in g(Y_2)-X_2.$ Any clarifications regarding this would be much appreciated.","['elementary-set-theory', 'proof-explanation']"
4710083,$\bigcap\limits_{\varphi\in E^*}\ker(\varphi)$ and the Axiom of Choise,"Context. Give a nonzero $K$ -vector space $E$ , it is known that $\displaystyle \bigcap_{\varphi\in E^*}\ker(\varphi)=0$ under AC. It is also known that, without AC, there are models of ZF in which some non zero vector space do not have nonzero linear forms. For such an $E$ , $\displaystyle \bigcap_{\varphi\in E^*}\ker(\varphi)=E.$ So my question is: Question. Are there models of ZF for which the dimension of $\displaystyle \bigcap_{\varphi\in E^*}\ker(\varphi)$ can have any prescribed cardinality $\kappa$ ? (the model may depend on $\kappa$ , even if it would be supernice to have a single model which works for any cardinality). Or, do we have necessarily the alternative $\displaystyle \bigcap_{\varphi\in E^*}\ker(\varphi)=0$ or $E$ ? I am also interested in the weaker question: can $\displaystyle\bigcap_{\varphi\in E^*}\ker(\varphi)$ be a nonzero proper subspace of $E$ ? Side remark. There is no precise reason why I am interested in this, just plain curiosity. These questions popped out because I realized that I needed the existence of some  complement of a given line of $E$ to prove that the intersection above is zero.","['axiom-of-choice', 'linear-algebra', 'set-theory']"
4710117,Math for fun: Finding the exact value of $\sin(10\unicode{xB0})$,"$\def\a#1{#1\unicode{xB0}}$ So I was looking through the homepage of Youtube to see if there were any math problems that I thought that I might be able to solve when I came across this video by Blackpenredpen which was about finding the exact value of $\sin(10\unicode{xB0})$ knowing that $\sin\left(\dfrac x2\unicode{xB0}\right)=\pm\sqrt{\dfrac12(1-\cos(x))}$ which I thought that I might be able to do. Here is my attempt at finding the exact value of $\sin(\a{10})$ : $$\sin(\a{30})$$ $$=\sin(\a{10}+\a{20})$$ $$=\sin(\a{10})\cos(\a{20})+\cos(\a{10})\sin(\a{20})$$ $$=\sin(\a{10})(1-2\sin(\a{20}))+2\sin(\a{10})\cos(\a{10})\cos(\a{10})$$ $$=\sin(\a{10})-2\sin^3(\a{10})+2\sin(\a{10})-2\sin^3(\a{10})$$ $$=3\sin(1\a0)-4\sin^3(1\a0)$$ Now, letting $$1\a0=\dfrac x3$$ $$3\sin(1\a0)-4\sin^3(1\a0)\gets3\sin\left(\dfrac x3\right)-4\sin^3\left(\dfrac x3\right)$$ Now, we set what we have gotten equal to $\sin(x)$ : $$\sin(x)=3\sin\left(\dfrac x3\right)-4\sin^3\left(\dfrac x3\right)$$ Then, $$-4\left(\sin\left(\dfrac x3\right)\right)^3+3\left(\sin\left(\dfrac x3\right)\right)-\sin(x)=0$$ $$4\left(\sin\left(\dfrac x3\right)\right)^3-3\left(\sin\left(\dfrac x3\right)\right)+\sin(x)=0$$ $$\left(\sin\left(\dfrac x3\right)\right)^3-\dfrac34\left(\sin\left(\dfrac x3\right)\right)+\dfrac14\sin(x)=0$$ Now, for any $y^3+\color{red}{p}y+\color{red}{q}=0$ , $$y=\sqrt[3]{-\dfrac{\color{red}{q}}2+\sqrt{\dfrac{\color{red}{q}^2}4+\dfrac{\color{red}{p}^3}{27}}}+\sqrt[3]{-\dfrac{\color{red}{q}}2-\sqrt{\dfrac{\color{red}{q}^2}4+\dfrac{\color{red}{p}^3}{27}}}$$ $$\therefore\sin\left(\dfrac x3\right)=\sqrt[3]{-\dfrac12\left(\dfrac14\sin(x)\right)+
\sqrt{\dfrac14\left(\dfrac14\sin(x)\right)^2+\dfrac1{27}\left(-\dfrac34^3\right)}}$$ $$+\sqrt[3]{-\dfrac12\left(\dfrac14\sin(x)\right)^2-\sqrt{\dfrac14\left(\dfrac14\sin(x)\right)^2+\dfrac1{27}\left(-\dfrac34\right)^3}}$$ $$=\dfrac12\sqrt[3]{-\sin(x)\pm\sqrt{-\cos^2(x)}}$$ $$\dfrac12\sqrt[3]{-\sin(x)\pm i\cos(x)}$$ Meaning that $\sin\left(\dfrac x3\right)$ is equal to $$\dfrac12\sqrt[3]{-\sin(x)\pm i\cos(x)}$$ Now, when we plug in $\a{30}$ for $x$ (since $\dfrac{30}3=10$ ) we get $$\sin(\a{10})=\dfrac12\sqrt[3]{-\sin(\a{30})\pm i\cos(\a{30})}$$ $$=\dfrac12\sqrt[3]{-\dfrac12\pm\dfrac{i\sqrt3}2}$$ $$=\dfrac12\sqrt[3]{-\dfrac{1\pm i\sqrt3}2}$$ $$\sin(1\a0)=\dfrac i2\sqrt[3]{\dfrac{1\pm i\sqrt3}2}$$ My question Is my solution for the exact value of $\sin(1\a0)$ correct, or what could I do to attain the correct solution/attain it more easily? Mistakes that I might have made Trigonometric identities Cubic formula Definition of the cubic formula for any $y+py+q=0$ Incorrect tags that I put","['trigonometry', 'solution-verification']"
4710124,where I'm wrong with direct cosine vs dot product?,"$\cos \theta = \frac{av}{|a||v|}$ (1) (from dot product) $\cos \theta = \frac{|a_v|}{|a|}$ (2) (from cosine definition) combining (1) and (2): $\frac{av}{|a||v|} = \frac{|a_v|}{|a|}$ , so $av = |a_v||v|$ (3) but $a_v v = |a_v||v|\cos0=|a_v||v|$ (4) (by dot product) now if combine (3) and (4), I have: $av=a_vv$ and $a = a_v$ which is false by assumption. What's wrong? There must be some obvious explanation, but I checked it multiple times and can't see it.","['linear-algebra', 'geometry']"
4710131,How to write $\frac{1}{3\ln^23} + \frac{1}{4\ln^24} + \frac{1}{5\ln^25} +\dots$ as a short expression?,"I have a snippet of series that I have to note as a short expression: $$\frac{1}{3\ln^23} + \frac{1}{4\ln^24} + \frac{1}{5\ln^25} +\dots$$ I am not sure if it would be more reasonable to write it as follows: $\displaystyle\sum_{n=1}^{\infty} \frac{1}{(n+2)\ln^2(n+2)}$ or : $\displaystyle\sum_{n=3}^{\infty} \frac{1}{n\ln^2(n)}\,.$ Would it even make a difference?","['notation', 'sequences-and-series']"
4710169,"Convergence a.e. of $\sum_{n=1}^{\infty} \frac{a_n}{\sqrt{\left|x-r_n\right|}}$, where $\sum_{n=1}^{\infty}\left|a_n\right|<\infty$","Let $\left\{a_n\right\}_{n=1}^{\infty}$ and $\left\{r_n\right\}_{n=1}^{\infty}$ two sequences of real numbers, and suppose that: (i) $\sum_{n=1}^{\infty}\left|a_n\right|<\infty$ ; (ii) $r_i \neq r_j$ for all $i \neq j$ . Prove that the series $$
\sum_{n=1}^{\infty} \frac{a_n}{\sqrt{\left|x-r_n\right|}}
$$ converges absolutely Lebesgue almost everywhere on $\mathbb{R}$ . I tried to compute, for each $n\geq1$ , $$\int_{\mathbb{R}} \frac{a_n}{\sqrt{\lvert x-r_n \rvert}}dx, $$ but this integral is divergent, so I'm out of ideas. Any hints?","['integration', 'measure-theory', 'lebesgue-measure', 'lebesgue-integral', 'sequences-and-series']"
4710217,To what extent can we determine the simplicity or non-simplicity of groups based on their prime decompositions?,"This question may have more of a vague, less objective answer than usual for this site, so I apologise if it difficult to answer definitively. Below, $p$ , $q$ and $r$ are distinct primes. A group of order $p$ is simple (these groups are precisely the prime-order cyclic groups.) A group of order $p^n$ is not simple, for $n \geq 2$ . ( Here is a
proof from this site. ) A group of order $pq$ is not simple. ( Proof. ) A group of order $p^2q$ is not simple. ( Proof. ) A group of order $p^2q^2$ is not simple. ( Proof. ) A group of order $pqr$ is not simple. ( Proof. ) It is not always possible to classify the simplicity this way - consider $p^2qr$ . $60=2^2 \cdot 3 \cdot 5$ . $A_5$ is simple, $\mathbb{Z}_{60}$ is not. Is there a more general statement that can classify whether a group is either a) definitely simple, b) definitely not simple, c) could be either simple or not simple, in terms of the decomposition of the prime factors of the order, or certain results that rule out many more cases?","['group-theory', 'simple-groups', 'finite-groups']"
4710234,Does a contractible locally connected continuum have an fixed point property?,"I'm surprised that I can't find any research on this topic. Maybe it's too obvious? Kinoshita proved that contractible continuum do not have FPP , but his example is not locally connected. Maybe if we add this to the conditions it will have FPP? UPD: Continuum as a nonempty compact connected metric space","['fixed-points', 'fixed-point-theorems', 'geometric-topology', 'general-topology', 'algebraic-topology']"
4710247,Relation between the level and the determinant of a matrix,"Let $ \mathcal{D}_k$ be the set of $k \times k$ integer , positive definite matrices with even diagonal . For $A \in \mathcal{D}_k$ we define the level of $A$ as the smallest integer $n_A \in \mathbb{N}$ such that $n_A A^{-1} \in \mathcal{D}_k$ . I was reading the chapter on theta functions on Iwaniec's ""Topics in classical automorphic forms"" and it seems like he uses the following property of $n_A$ (e.g. in Theorem 10.9): If a prime $p$ divides det $(A)$ , then $p$ divides $n_A$ . Although it seems like something elementary, I have not been able to prove it. I tried to use the formula $$A^{-1} = \frac{1}{\det (A)} \operatorname{adj} (A)$$ but didn't arrive anywhere. I have also worked some concrete examples, but all of them have $n_A = 2 \det (A)$ , for example: $$ A =\begin{pmatrix} 
2 & 1 & 0\\
1 & 2 & 1 \\
0 & 1 & 2
\end{pmatrix}$$ so is not very helpful. Any help would be appreciated.","['matrices', 'symmetric-matrices', 'positive-definite', 'quadratic-forms']"
4710255,Unable to find all angles of a trigonometric triangle,"I have been stuck on this problem for a while now. And have gotten close to slowing it but I am unsure what small mistake I am doing. This is the question
[Here is the Question, it is in Norwegian but translated it says ""In the $\triangle ABC$ $AC=8$ , $\angle B=30^\circ$ and the length of the Normal $BP$ from $B$ thru the line $AC$ is $6$ . Find the angles of $ABC$ ""[1] https://i.sstatic.net/gFieI.png [Solution]
[2]: https://i.sstatic.net/GB3jl.png This has been my idea so far: $sin(30) = \frac 8x, x= 16$ such that I find the length of line CB $$\sin(c) = \dfrac{6}{16}, x= 0.365$$ $$\sin^{-1}(\sin(0.366)) = 22.02$$ I don't bother to do anything more since finding the other angel will of course result in the wrong result Thank you for the help in advance","['trigonometry', 'triangles']"
4710268,Does the SVD work on an incomplete field?,"Does the singular value decomposition (SVD) require a complete field? SVD clearly can't work on $\mathbb Q$ , since we need square roots.  But can it work on $\mathbb E$ , the smallest Euclidean field containing $\mathbb Q$ ? I ask because, to my surprise, my synthetic geometry proof of polar decomposition required completeness, and would not, as far as I can tell, work for $\mathbb E$ .  (This is startling, because, it is generally accepted, as Dedekind himself wrote when introducing his cuts, that Euclidean geometry does not require, and would not be changed, by completeness, as long as it includes all constructible numbers.) Additionally, when proving SVD, Trefethen & Bau make use of compactness to show the existence of a maximum, which of course requires a complete field. And, from Harvard's fabled 55a , another use of completeness: Compactness... gives us another way to prove the spectral theorem: we can find an eigenvector for $T$ by seeing where the function $w \mapsto \langle Tw,w \rangle$ achieves its maximum on the unit sphere. Thus, can the SVD be done in a field that admits square roots but is not complete?  And, if yes: What aspect of complete fields finds its way into all three of these proofs? Is SVD somehow different or ""stronger"" in complete fields?","['complete-spaces', 'svd', 'real-analysis', 'field-theory', 'linear-algebra']"
4710282,"Dual space of $\mathcal{P}(M)$, when regarded as an $\mathbb{F}_2$-vector space.","This question stems from the well-known observation that the power set $\mathcal{P}(M)$ of any set $M$ can be given an $\mathbb{F}_2$ -vector space structure by the symmetric difference operation.
Any powerset is then naturally a dual space, namely of the subspace of the finite-element subsets, as can be seen from the pairing $$
\mathcal{P}(M)\times \mathcal{P}^{\mathrm{fin}}(M) \to \mathbb{F}_2, (A,B)\mapsto |A\cap B| \mod 2
$$ The question now is: Can we find a similarly explicit description for $\mathcal{P}(M)^*$ ? More ambitiously: Does the following construction encompass the whole space? For any ultrafilter $U$ on $M$ , we obtain an element in this space via ""integration against $U$ "", i.e. as the map $$\phi_{U}:\mathcal{P}(M) \to \mathbb{F}_2, A \mapsto \begin{cases} 1 & \text{if $A\in U$ and} \newline
0 & \text{otherwise.}\end{cases}$$ It is not difficult to show that the $\phi_U$ are linearly independent in $\mathcal{P}(M)^*$ : For different ultrafilters $U_1,...,U_n$ , we may always find subsets $A_1,...,A_{n-1}$ of $M$ such that $A_i\in U_n $ but $A_i\notin U_i$ for each $i$ . Then the intersection $A_1\cap...\cap A_{n-1}$ still lies in $U_n$ but in no other $U_i$ - consequently $\phi_{U_n}$ is not a linear combination of the other $\phi_{U_i}$ . Since $M$ supports $2^{2^{|M|}}$ different ultrafilters (assuming AC), the space spanned by the $\phi_U$ at least has the correct dimension (again assuming AC). (Note also that it contains the image of $\mathcal{P}^{\mathrm{fin}}(M)$ under the double dual embedding: Every element gets identified with the principal ultrafilter generated by its singleton.) I have no clue how to show that these maps generate the whole space though. Do you know an argument? Alternatively, can you give a description of a functional that is not of this form?","['filters', 'linear-algebra', 'set-theory']"
4710325,"How to graph $\int_0^ \pi \int_0^{2\cos(\theta)}r\, dr\,d\theta$","I want to graph the region that the integral $\int_0^ \pi \int_0^{2\cos(\theta)}r\, dr\,d\theta$ integrates over, which  is a little weird because I don't understand how $\theta$ goes from $0$ to $\pi$ . It would be obvious if $\theta$ goes from $-\frac{\pi}{2}$ to $\frac{\pi}{2}$ . I checked answers in the back of the book and it was a full circle with center of $(1,0)$ and radius of $1$ .","['multivariable-calculus', 'calculus', 'multiple-integral']"
4710361,Find the values of $a$ for which the function $f(x)=x^3+x^2+a\sin x$ is one-one(injective),"Find the values of $a$ for which the function $$f(x)=x^3+x^2+a\sin x$$ is one-one(injective). My Attempt Since $f(x)$ is differentiable one can say that the derivative must be either always non-negative or non positive i.e. $f'(x)\geq 0$ $\Rightarrow 3x^2+2x+a\cos x\geq 0$ Since minimum value of $\cos x$ is $-1$ we can say that if $3x^2+2x-a\geq 0$ then $f'(x)\geq 0$ for all $x\in \mathbb{R}$ So,for $3x^2+2x-a\geq 0$ ,the discriminant of the quadratic must be non-positive i.e. $4+12a\leq 0$ . Thus $a\in \left(-\infty,-\frac{1}{3}\right]$ But when I take $a=-1$ and plot the graph $y=x^3+x^2-\sin x$ I see that $f(x)$ is NOT one-one. What mistake I am making.","['graphing-functions', 'real-analysis', 'calculus', 'functions', 'algebra-precalculus']"
4710406,Is there explicit formula for the smallest enclosing ball in $SO_3$,"In Euclidean Space, given some points, we can find the smallest enclosing ball which contains all the points. And the center is known as the Chebyshev Center. But now I encounter a problem related to rotation and I would like to find the smallest enclosing ball on the manifold $SO_3$ , using the metric of geodesic distance ( 1 ): $$
d(R, Q)=\operatorname{acos}\left(\frac{\operatorname{tr}\left(R^T Q\right)-1}{2}\right)
$$ To make this problem make sense, we may consider the points contained in a big ball with radius $\pi$ . I'm thinking about this question for quite a while, but I can't find much progress. Now I can solve this problem with some iterative algorithms, but due to this problem's simple form, I'm wondering if there is an explicit solution. I'm wondering if I can use something like stereographic projection to simply the problem. Or, if I only consider the circumstances of 3 or 4 points on the $SO_3$ , can I hope to get an explicit solution to this Chebyshev center problem?(To verify my iterative method). Any hints and help will be appreciated, thanks a lot in advance! Edit 2023/6/3: After some searchings, I find that the geodesic distance above is the bi-invariant metric defined on $SO_3$ and the angular distance in $S^3$ is also bi-invariant. The bi-invariant metric is unique up to scaling if the Lie group is compact and Lie algebra is simple. We can conclude that both these two bi-invariant metrics are unique up to scaling. Now if I consider a diffeomorphism $F: SO_3 \to RP^3$ and locally the $RP^3$ looks the same as $S^3$ , maybe we can consider the homomorphism: $\rho: SO_3 \to S^3$ and somehow prove that after the homomorphism, the new metric is still bi-invariant (I'm not sure if I'm right). If this holds, then maybe I can just study about the problem in $S^3$ ? But I'm still not sure if I'm right and how to interprete these in a precise way. Also I think my previous thought of using the stereographic projection to simply the problem is not valid, for the smallest ball in $R^3$ after stereographic projection isn't the smallest on the $S^3$ . So I'm still wondering how to tackle this.","['lie-algebras', 'riemannian-geometry', 'smooth-manifolds', 'lie-groups', 'differential-geometry']"
4710410,The practical use of reflexive relations [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question I understand what reflexive relation on a set is, but I struggle to come up with a practical use of it - why would you even bother defining it? I would really appreciate a couple examples, where reflexions are used, so it would be a little easier to understand - why they are needed?","['elementary-set-theory', 'relations']"
4710421,Variance of cards without replacement,"So let's say that we label a deck of cards as 1-13. There is a 1-13 of hearts, diamonds, spades, and clubs. What is the variance of the sum of picking 3 cards (without replacement)? If this was with replacement it's obviously just 3 times the variance of a single draw since variance is additive like that. I already know that the variance of a singe draw is 14 because variance is just $$ E[X^2]-E[X]^2 $$ For all purposes, can I just neglect the fact that they are not replaced? It makes sense that if one card was removed at random that the expected value of the next draw is the same (assuming the one removed was random and not some Baye's theorem situation where you were told which one was removed) so therefore the variance would also be treated the same way? There was another post on this already: Variance of sum of cards with and without replacement But I don't quite understand part of one of the solutions given: ""In the case of ""with replacement"" 𝖤(𝑋𝑌) will equal $𝖤(𝑋)^2$ because 𝑋,𝑌 will be independent and identically distributed."" How are these independent if one is picked after the other? Also wouldn't E[XY] be equal to $E[X^2]$ and not $E[X]^2$ ?","['statistics', 'probability-distributions', 'variance', 'card-games']"
4710433,Trouble with Knuth's proof that surreal numbers lie between their left and right sets,"I'm reading through Donald E. Knuth's book, Surreal Numbers , and I've been struggling for days now with one little step in a single proof.
Specifically, I'm trying to work through the proof that every surreal number is bounded by the elements of its left and right sets. That is, for a surreal number $x = \{ X_L \,|\, X_R \}$ , prove that $x_L \le x$ and that $x \le x_R$ for all numbers $x_L \in X_L$ and $x_R \in X_R$ . The proof starts by way of contradiction, assuming that there exists some number $x_l \in X_L$ such that $x_L \not\le x$ . From this statement, the second axiom of surreal numbers gives us two possible cases; either there exists a number $x_{LL} \in X_{LL}$ (where $X_{LL}$ is $x_L$ 's left set), with $x_{LL} \ge x$ , or else there exists a number $x_R \in X_R$ with $x_L \ge x_R$ . This second case is an obvious contradiction of the first axiom of surreal numbers, so we go on checking the sole remaining case with $x_{LL} \ge x$ . The proof then seems to try to chain this relation and another together with the transitive rule, eventually producing a contradiction. The transitive rule has been proved prior to this point in the book for the $\ge$ relation, so we just need a second relation to use it. And this is where I hit my wall.
The proof goes on to say that $x_{LL} \le x_L$ , and I cannot for the life of me figure out the justification. Here's the wording for that statement in the book (Just after Ch 5 begins): B. (continuing from above proof) ...But what can we do with $x_{LL}$ ? I don't like double subscripts. A. Well, $x_{LL}$ is an element of the left set of $x_L$ . Since $x_L$ was created earlier than $x$ , we can at least assume that $x_{LL} \le x_L$ , by induction. B. Lead on. That seems to be the only justification for the statement; from here, Alice and Bill continue on down their productive and insightful train of reasoning without me. I know how induction works, but I have absolutely no idea where this induction comes from or what it's doing. It all reads like a wholly obvious fact that follows perfectly naturally from what I already know, but I'm stuck. I'm convinced I'm forgetting something or that I""m on the wrong track. Either way, I need it explained to me more fully and simply than this. Thanks.","['elementary-set-theory', 'proof-explanation', 'surreal-numbers']"
4710448,Asymptotics of orthonormal basis in $\ell^2$,"Is it true that for all orthognal basis $\{e_n\}_{n=1}^\infty$ of $\ell^2(\mathbb N)$ , we have $$\sup_{n}n^2 \sum_{k=1}^\infty  (e_n(k))^2 k^{-2}<\infty? $$ where $e_n=(e_n(1),e_n(2),\cdots,e_n(k),\cdots)\in \ell^2(\mathbb N)$ and $$ \langle e_n,e_m \rangle_{\ell^2}=\sum_{k=1}^\infty e_n(k)e_m(k)=\delta(m,n).$$ For the special case: the standard orthognal basis $\bar {e_n}(k)=\delta(k,n),$ it's clearly true $$ n^2 \sum_{k=1}^\infty  (\bar {e_n}(k))^2 k^{-2}=1,\quad \text{for all $n$.}$$ For general orthognal basis, I think it's not true, but I can't find a counter example. Update: At first I thought this argument is far too strong, construct some other ONB and reorder them would make a counterexample, but I'm not so sure about it now. Note this truth, define $T\in \mathscr L(\ell (\mathbb N))$ : $$T(\bar{e_n})=n^{-1}\bar {e_n},$$ it's straightforward that $T$ is a Hilbert–Schmidt operator: $$\|T\|^2_{HS}=\sum_n \|T({e_n})\|^2=\sum_n \|T(\bar{e_n})\|^2=\sum_n n^{-2}<\infty. $$ Note that the asymptotic above is $$\sum_{k=1}^\infty  (e_n(k))^2 k^{-2}=\|T({e_n})\|^2,$$ and the question turns to $$\|T({e_n})\|=O(n^{-1})?$$ That is, is the ""tail"" of the Hilbert–Schmidt norm of $T$ decrease in the same speed no matter what ONB we choose?","['compact-operators', 'asymptotics', 'real-analysis', 'functional-analysis', 'sequences-and-series']"
4710622,Solving the differential $\frac{y'y'''}{y''} = x$,"I've been trying to solve the differential equation: $\frac{f'(x)f'''(x)}{f''(x)} = x$ , $x\in \mathbb{R}$ My initial attempt was to integrate by parts, something like this: $$\int_{}^{} \frac{f'(x)f'''(x)}{f''(x)} dx = \int_{}^{}x dx \Leftrightarrow \\ ln(f''(x))f'(x) - \int_{}^{} \ln(f''(x)) f''(x)dx  = \frac{x^2}{2} + C1$$ but I soon came to realize that there was absolutely no way to proceed. Even if I try to reduce it to a second-order nonlinear ODE by letting $u(x) = f'(x)$ , I can't seem to solve it by simple integration. After some trial and error, I managed to figure out that $f(x) = \frac{x^3}{3} + C ,x\in \mathbb{R}$ (as noted by @Vasili) is indeed a solution (if not the only solution) to this differential. And here I ask you: How do we mathematically prove that $f(x) = \frac{x^3}{3}$ + C is a solution? How do we reach that conclusion without simply making guesses? Thanks in advance for any insight!","['integration', 'ordinary-differential-equations', 'differential', 'analysis', 'calculus']"
4710624,How can I find the limit of a multivariable function?,"I'm a student that start learning Calculus 3 and I came across this problem: $$\lim_{(x,y)\to (0,0)}\frac{\log\left(x^2+2^{y^2}\right)} {\sqrt{x^2+4y^2}}$$ I need to find the limit. I've tried approaching the origin from the $x$ and $y$ axis: Approaching from the $x$ axis: $$\lim_{(x,0)\to (0,0)}\frac{\log\left(x^2+1\right)} {\sqrt{x^2}}$$ Use the L'Hopital's rule: $$\lim_{(x,0)\to (0,0)}\frac{\frac{2x}{x^2+1}} {\frac{2x}{2\sqrt{x^2}}}=\lim_{(x,0)\to (0,0)}\frac{2\sqrt{x^2}}{x^2+1}=0$$ Approaching from $y$ axis and use L'Hopital's rule i got: $$\lim_{(0,y)\to (0,0)}\frac{\log\left(2^{y^2}\right)} {\sqrt{4y^2}}=\lim_{(0,y)\to (0,0)}\frac{\frac{y^2\,2^{y^2-1}}{2^{y^2}}} {\frac{8y}{2\sqrt{4y^2}}}=\lim_{(0,y)\to (0,0)}\frac{2^{y^2-1}2\sqrt{4y^2}}{16y}=\pm \frac18$$ This would mean that the limit doesn't exist. But I'm not so sure about the the way I've handled this problem so can you tell me if I made a mistake somewhere and is there another way to solve problem like this one?","['multivariable-calculus', 'limits', 'calculus']"
4710648,How to prove: $\left|\int_0^1 f(x) d x\right| \leq \frac{1-a}{2} \max _{0<x<1}\left|f^{\prime}(x)\right|$,"I found this question on the 1998 Roorkee Paper during my crazy hunt for mind-blowing calculus questions. Let $f:[0,1] \rightarrow \mathbb{R}$ be a continuous
function, differentiable on $(0,1)$ such that $\int_0^{a}
 f(x) dx=0$ for some $a \in(0,1)$ , then (1) $\left|\int_0^1 f(x) d x\right| \leq \frac{1-a}{2} \max
 _{0<x<1}\left|f^{\prime}(x)\right|$ (2) $\left|\int_0^1 f(x)dx\right|>\frac{1-\mathrm{a}}{2} \max_{0<x<1}\left|f'(x)\right|$ (3) $\left|\int_0^1 f(x) d x\right|=\frac{1-a}{2} \max_{0<x<1}\left|f^{\prime}(x)\right|$ for some function $f(x)$ (4) none of the other options is correct And ofcourse I have no idea on how to solve this.
However on carefully reading the options and noticing $\frac{1-a}{2}$ , I thought of making a connection with the concept of ' Definite Integral as Limit of Sum ' where $\frac{b-a}{h}$ could  be $\frac{1-a}{2}$ here. Then I also believe there is an involvement of Lagranges Mean Value Theorem somewhere as the questions says continuous and differentiable. I presented my observations to my teacher and he gave me a hint: ""Put $x=at$ and change limits accordingly and try to apply your concepts."" Could someone kindly help me with this one.","['integration', 'definite-integrals', 'analysis', 'real-analysis', 'inequality']"
4710651,Examples of solutions to strictly nonlinear differential equations,"Examples of solutions to linear differential equation are (from Wikipedia): exponential function, logarithm, sine, cosine, inverse trigonometric functions, error function, Bessel functions and hypergeometric functions. So in this sense these are linear functions (for some generalized notion of ""linear""). But I wonder if there are examples of functions that are ""strictly nonlinear"", in the sense that they are the solution of some nonlinear differential equation but not of any linear differential equation.","['ordinary-differential-equations', 'partial-differential-equations']"
4710659,How many five digit numbers that have the sum of digits equal to 25 with generating function,"Question is : Find all $5$ -digit numbers that sum of digits is $25$ . I write this: All numbers like $\overline {a_{1}a_{2}a_{3}a_{4}a_{5}}$ that $a_{1}+a_{2}+a_{3}+a_{4}+a_{5}=25$ . constraint is: $ a_1 = 1,\cdots, 9\qquad a_i = 0,\cdots,9 \quad ; \quad\forall i\in\{2,3,4,5\}$ . The related generating function is: \begin{align}
f(x)&=\left(x+x^2+\cdots+x^9\right)\cdot\left(1+x+x^2+\cdots+x^9\right)^4 \\
&=x\cdot\left(1+x+x^2+\cdots+x^8\right)\cdot\left(1+x+x^2+\cdots+x^9\right)^4\\
&=x\cdot\frac{1-x^9}{1-x}\cdot\left(\frac{1-x^{10}}{1-x}\right)^4\\
&=x\cdot\left(1-x^9\right)\cdot\left(1-x^{10}\right)^4\cdot\frac{1}{(1-x)^5}\\
&=x\cdot\left(1-x^9\right)\cdot(1 - 4 x^{10} + 6 x^{20} - 4 x^{30} + x^{40})\cdot
\sum_{k=0}^{\infty}{\left[\binom{4}{0}+\binom{5}{1}x+\binom{6}{2}x^2+\binom{7}{3}x^3+\cdots\right]}.
\end{align} Coefficient of $x^{25}$ : Product $x$ $1$ $1$ $\binom{28}{24}x^{24}$ $\binom{28}{24} \times x^{25}$ $x$ $1$ $-4x^{10}$ $\binom{18}{14}x^{14}$ $-4 \times \binom{18}{14} \times x^{25}$ $x$ $1$ $6x^{20}$ $\binom{8}{4}x^{4}$ $6 \times \binom{8}{4} \times x^{25}$ $x$ $-x^9$ $1$ $\binom{19}{15}x^{15}$ $-\binom{19}{15} \times x^{25}$ $x$ $-x^9$ $-4x^{10}$ $\binom{8}{4}x^{4}$ $4 \times \binom{8}{4} \times x^{25}$ So, the answer is: $$
\binom{28}{24} - 4\binom{18}{14} + 6\binom{8}{4} - \binom{19}{15} + 4\binom{8}{4} = 5059
$$ But, I write all answers of the question in Excel and true answer is $5283$ . What part of above answer is wrong?","['discrete-mathematics', 'generating-functions']"
4710694,On a definition of subgroup,"I know that a subset $H$ of a group $G$ is a subgroup if (a)If $a$ and $b$ are in $H$ , so is $ab$ . (b)1(the identity) is in $H$ (c)If $a$ is in $H$ , so is $a^{-1}$ . I am wondering if the condition $(b)$ is unnecessary. The definition seemed to assume that the $a^{-1}$ in $H$ is the same as the $a^{-1}$ in $G$ .Thus, $1$ serves as the identity in both $H$ and $G$ . Is there any gap in my understanding?","['group-theory', 'abstract-algebra']"
4710716,"Crazy fact(?) about circles drawn on base of triangle between cevians: they always fit, no matter what their order?","Take any triangle, and draw any number of cevians from the top vertex to the base, with any spacing between the cevians. In each sub-triangle thus formed, inscribe a circle. Now rearrange the order of the circles from left to right (but don't change their radii). Adjust the cevians so that each cevian touches two neighboring circles. The circles always seem to fit perfectly in the sub-triangles, with no gaps, no matter what their order! Is this true, and if so, why? I tested this with different size triangles and circles, and it always seems to work. I doubt it's just a coincidence. At first I thought there must be a simple explanation, but I haven't found one. (I asked a coworker who is quite good at geometry about this, and he didn't know what to think.) Notice that the angle at the top vertex subtended by say the blue circle, is different in the different arrangements. Possibly useful: the inradius of a triangle is $\frac{2\times \text{area}}{\text{perimeter}}$ .","['permutations', 'conjectures', 'circles', 'geometry', 'triangles']"
4710734,Maximum of $27abc+\sum\limits_{\mathrm{cyc}}a\sqrt{a^2+2bc}$ for positives $a+b+c=\frac{1}{\sqrt{3}}$,"Maximum possible value of $27abc+a\sqrt{a^2+2bc}+b\sqrt{b^2+2ca}+c\sqrt{c^2+2ab}$ if real positive numbers $a, b, c$ satify $a+b+c=\frac{1}{\sqrt{3}}$ This is a problem from an Inequality paper that they gave us. I believe that we have to use Cauchy-Schwarz.
I think that we somehow need to change the under root to a perfect square as the 2ab, 2ac... are giving us that clue. Although I tried to go through that direction, I ended up not knowing how to transfrom it into that, neither knowing if it is solved like this.","['contest-math', 'algebra-precalculus', 'inequality']"
4710737,Show that the fractional Laplacian operator is closed,"Consider the fractional Laplacian defined by $$(-\Delta)^s u(x) = P.V. \int_{\mathbb{R}^N} \frac{u(x) - u(y)}{|x - y|^{N + 2s}}dy, \ s \in (0,1).$$ Also consider that $$D((-\Delta)^s) = \{u \in H^s(\Omega); (-\Delta)^su \in L^2(\Omega)\},$$ for some $s \in (0,1)$ .
I want to show that the operator $(-\Delta)^s : D((-\Delta)^s) \subset L^2(\Omega) \to L^2(\Omega)$ is closed. Here, $\Omega \subset \mathbb{R}^N$ is a bounded and smooth domain with $u = 0$ in $\mathbb{R}^N \backslash \Omega$ . Attempt: Let $(u_n) \subset D((-\Delta)^s) $ with $u_n\to u$ in $L^2(\Omega)$ and $(-\Delta)^su_n \to v$ in $L^2(\Omega)$ . By Fatou's Lemma, I was able to show that $u \in H^s(\Omega)$ , where $$H^s(\Omega) = \left\{u \in L^2(\Omega); \int \int_{\Omega \times \Omega}\frac{|u(x) - u(y)|^2}{|x - y|^{(N + 2s)}}\right\}.$$ Now, I need to show that $\int_{\Omega}|(-\Delta)^su(x)|^2 dx < \infty$ .
I have tried in different ways, but without success. For example, the most I can do is show, by Fatou's Lemma, that $\int_{\Omega}|(-\Delta)^su(x)|^2 dx \geqslant \|v\|^2_{L^2(\Omega)}$ .","['measure-theory', 'functional-analysis', 'fractional-sobolev-spaces', 'fractional-calculus']"
4710786,Can I use a function argument to prove invertibility of matrices?,"If all matrices can represent a linear transformation, can I refer to some general notion of the transformation associated with the matrix to make conclusions about the matrix? For example, I want to prove a simple true/false statement, ""If A and B are square matrices such that AB is invertible, then both A and B must be invertible."" I believe that the statement is true for the following reason: Assuming AB is well-defined and A and B are square, A and B have the same dimension. The product, AB is square as well then. AB is invertible and square, so the transformation AB represents is an
isomorphism, thus the transformation associated with AB is a
bijection. The transformation associated with AB is a bijection, so the
transformation B represents is surjective and the transformation A
represents is injective. Since both A and B have square matrices representing their linear
transformations, injective and surjective respectively, the transformations must also
be isomorphisms. Hence the transformation associated with A and B has
an inverse, and thus A and B (the matrices) are invertible. This feels convoluted and likely isn't the simplest solution. I am just wondering if it is OK to bring in the notion of the transformations the matrices could represent like this? Am I missing some subtlety?","['matrices', 'linear-algebra', 'linear-transformations']"
4710810,Compute residues for $\int_{-\infty}^{\infty} \frac{x \sin x}{1+x^4}dx$,"I am trying to compute $$\int_{-\infty}^{\infty} \frac{x \sin x}{1+x^4}dx$$ This question has been asked here , but the asker and answerer skip over the calculation of the residues. Define the curve $\gamma$ to be the semicircle in the top half plane with radius $R$ and straight edge on the real axis from $-R$ to $R$ . Then by the residue theorem and Jordan's lemma, taking $R \to \infty$ , I know that $$\int_{-\infty}^{\infty} \frac{z e^{iz}}{1+z^4}dz = \oint_{\gamma} \frac{ze^{iz}}{1+z^4} dz = 2 \pi i \sum_{\alpha \text{ roots of } 1+z^4, \Im(\alpha)>0} \text{Res}\left(\frac{z e^{iz}}{1+z^4}, \alpha \right)$$ The relevant roots are $\alpha=e^{\frac{\pi i}{4}}, e^{\frac{3 \pi i}{4}}$ . How can I go about calculating these residues? I know that it's the coefficient of $z^{-1}$ the Laurent series at $\alpha$ , or also $\frac{1}{2 \pi i} \oint_{\partial D(\alpha, r)} \frac{z e^{iz}}{1+z^4} dz$ . I'm struggling to calculate either of these. What do I do?","['integration', 'improper-integrals', 'analysis', 'complex-analysis', 'contour-integration']"
4710836,Evaluate the integral:$ \int_0^1\left[\left(\tan ^{-1} x\right)^2-\frac{\ln \left(1+x^2\right)}{1+x^2}\right] d x $,"$$
\int_0^1\left[\left(\tan ^{-1} x\right)^2-\frac{\ln \left(1+x^2\right)}{1+x^2}\right] d x
$$ What  I did : Substitute $$
x=\tan \theta
$$ $$
\int_0^{\pi / 4}(\theta)^2 \sec ^2 \theta -\ln \left(\sec ^2 \theta\right) d \theta
$$ But now I can't integrate the first term.
Thanks in advance.","['integration', 'definite-integrals', 'analysis', 'real-analysis', 'calculus']"
4710840,"Consider the group $\mathbb{Z}_2 \wr \mathbb{Z}$, what is $\mathbb{Z}_2 \wr 2 \mathbb{Z}$?","Consider the (Lamplighter) group $(\bigoplus_{n=-\infty}^{n=\infty}\mathbb{Z}_2) \rtimes_\phi\mathbb{Z}$ , where $\phi(1)$ ""shifts"" every element in $\bigoplus_{-\infty}^{\infty}\mathbb{Z}_2$ to the right by $1$ . I was wondering if its subgroup, $(\bigoplus_{n=-\infty}^{n=\infty}\mathbb{Z}_2) \rtimes_\phi2 \mathbb{Z}$ isomorphic to a wreath product of a finite abelian group with $\mathbb{Z}$ . I know that $\mathbb{Z}_2 \wr \mathbb{Z} \not \cong \mathbb{Z}_2 \wr 2 \mathbb{Z}$ . However, $\mathbb{Z}_2 \wr 2 \mathbb{Z}$ is a wreath product of a finite abelian group with a group isomorphic to $\mathbb{Z}$ , does that implies that it is isomorphic to a wreath product of a finite abelian group by $\mathbb{Z}$ ?","['infinite-groups', 'finitely-generated', 'wreath-product', 'semidirect-product', 'group-theory']"
4710851,$\frac{\partial \psi}{\partial \mu}$ where $\psi$ is the solution of $\ddot{x}+\tanh{(2\dot{x}+x)}=\mu e^t$,"I am asked to find $\frac{\partial \psi}{\partial \mu}(t, t_0, x_0, \dot{x}_0, µ) \ $ at the point $ \ (t, t_0, x_0, \dot{x}_0, µ) = (t, 0, 0, 0, 0)$ where $\psi$ is the solution of $\ddot{x}+\tanh{(2\dot{x}+x)}=\mu e^t \ $ . I transformed the equation into $$
\begin{cases}
& \dot{x}_1=x_2 \newline
& \dot{x}_2=-\tanh{(2x_2+x_1)}+\mu e^t
\end{cases}
$$ So $\frac{\partial \psi}{\partial \mu}$ is the solution of $\dot{X}=AX+b(t)$ , $X(0)=
\begin{pmatrix}
0 \newline
0
\end{pmatrix}
$ where $A(t)=Df = 
\begin{pmatrix}
0 & 1\newline
-\frac{1}{cosh^2{(2x_2+x_1)}} & -\frac{2}{cosh^2{(2x_2+x_1)}}
\end{pmatrix}
$ evaluated in a solution of the system (with $\mu = 0$ in this case) but I don't know how to get a solution (I didn't see any similar problem in class before). How can i get a solution? Or is there any other way to find the partial derivative?","['numerical-methods', 'ordinary-differential-equations']"
4710870,Explicit enumerations of n-tuples of naturals $f:\mathbb{N}^n \to \mathbb{N}$?,"For $n=2$ there is a simple and nice enumeration: $$(i,j) \mapsto f(i,j)=\frac{(i+j)(i+j+1)}{2}+j$$ Obviously, we can use this formula to obtain a general enumeration, for $n=3$ , we have: $$(i,j,k) \mapsto (f(i,j),k) \mapsto f(f(i,j),k)$$ defining $F_n:\mathbb{N}^2\times\mathbb{N}^{n-1} \to \mathbb{N}^n$ as $(a,b,\boldsymbol{p})\mapsto (f(a,b),\boldsymbol{p})$ , we obtain enumerations as: $$H^{(n)}:\mathbb{N}^n\to \mathbb{N}, \qquad H^{(n)}(i_1,\dots,i_n) = (F_{n-1}\circ\dots\circ F_1)(i_1,\dots,i_n)$$ Then, some things came up that surprised me a bit: In terms of 3-tuple boxes, $\mathbb{N}^3$ would have a ""volume"" so I would have expected there to be enumeration that was a cubic polynomial (the volume of a cone and the volume of a pyramid is a cubic polynomial of their maximum dimensions). However, the result is a rather lackluster quartic polynomial. Does this fact have an intuitive explanation? What other more elegant ways are there to give enumerations, at least for $\mathbb{N}^3$ ?","['combinatorics', 'natural-numbers', 'examples-counterexamples']"
4710894,Uniqueness of Jacobian,"Consider the simple stick-breaking function $$f : [0, 1]^2 \to S^3 : (v_1, v_2) \mapsto (x_1, x_2, x_3) = \big(v_1, (1 - v_1) v_2, (1 - v_1) (1 - v_2)\big),$$ where $S^N = \left\{(x_1, x_2, \ldots, x_N) \mid \sum_i x_i = 1, \forall i : x_i \geq 0\right\}$ is the $N$ -dimensional probability simplex. The inverse of this function should be $$f^{-1}(x_1, x_2, x_3) = \Big(x_1, \frac{x_2}{1 - x_1}\Big)$$ However, there is an equivalent formulation that makes use of the fact that $x_1 + x_2 + x_3 = 1$ $$f^{-1}_\mathrm{eq}(x_1, x_2, x_3) = \Big(x_1, \frac{x_2}{x_2 + x_3}\Big)$$ Both of these formulations should represent the same function, but if we compute the Jacobians for both functions, we get $$\begin{align*}
  \mathcal{J}_{f^{-1}}(x_1, x_2, x_3) &= \begin{pmatrix}1 & 0 & 0 \\ \frac{x_2}{(1 - x_1)^2} & \frac{1}{1 - x_1} & 0 \end{pmatrix} \\
  \mathcal{J}_{f^{-1}_\mathrm{eq}}(x_1, x_2, x_3) &= \begin{pmatrix}1 & 0 & 0 \\ 0 & \frac{x_3}{(x_2 + x_3)^2} & \frac{-x_2}{(x_2 + x_3)^2} \end{pmatrix} = \begin{pmatrix}1 & 0 & 0 \\ 0 & \frac{1}{1 - x_1} - \frac{x_2}{(1 - x_1)^2} & \frac{-x_2}{(1 - x_1)^2} \end{pmatrix},
\end{align*}$$ which seems to suggest that there is no unique Jacobian for $f^{-1} = f^{-1}_\mathrm{eq}$ . Note that the difference between the second rows in the Jacobian is a constant.
For higher-dimensional variants of these functions, I noticed that this constant offset is different for every row. I am aware that constants are unique up to a constant term, but I thought derivatives would be unique (cf. this post with an answer for single-output functions).
Therefore, I started wondering: are Jacobians actually unique?
If yes, how should the example above be interpreted?
If not, is there some specific notion of uniqueness like ""unique up to ...""?","['multivariable-calculus', 'calculus', 'jacobian']"
4710896,"Help with negating proposition $P: \forall a\in\Bbb R, \forall b\in\Bbb R, f(a)=f(b)\rightarrow a=b$","Let $\,f(x)=x^2-2x-8\,,\,$ $\forall x\in\Bbb R$ . Consider the proposition $P: \forall a\in\Bbb R,\;\forall b\in\Bbb R,\, f(a)=f(b)\rightarrow a=b$ Find $f(x)=0$ . Negate the proposition $P$ . Find truth values for $P$ . So, I started by finding the values for $f(x)=0$ , which the $x$ values are $4$ and $-2$ .
I started trying to negate it, but I wasn't sure how to. Do I negate the $\forall a$ ? And if I do, where does the negation symbol go?
I’m not sure how to go about negation here.","['quantifiers', 'discrete-mathematics']"
4710902,"Proof for $\sqrt{2}$, $\sqrt{3}$, $\sqrt{5}$ can never be the terms of a single arithmetic progression with non zero common difference.","Proof for $\sqrt{2}$ , $\sqrt{3}$ , $\sqrt{5}$ can never be the terms of a single arithmetic progression with non zero common difference. Here is what I have tried. Let: $$ax +c=\sqrt{2}$$ $$bx +c=\sqrt{3}$$ $$dx +c=\sqrt{5}$$ $$\frac{\sqrt2-c}{a}=\frac{\sqrt3-c}{b}=\frac{\sqrt5-c}{d}$$ What property can I use next?","['arithmetic-progressions', 'sequences-and-series']"
4710913,Is right-continuity of $c$ necessary for $a(u)=\inf \left\{ t \in \mathbb{R_+} \colon c(t)>u \right\}$ to be right-continuous?,"There are similar posts, such as this one or this one , but I can't see where the right-continuity assumption is required. Let $c:\mathbb{R}_+ \rightarrow \mathbb{R}_+$ be a non-decreasing function, right-continuous function. Let $a:\mathbb{R_+} \rightarrow \mathbb{R_+}$ be defined by $a(u)=\inf \left\{t \in \mathbb{R_+}  \,\colon \; c(t) > u \right\}$ . The claim is that $a$ is increasing and right-continuous. As for right-continuity, one should prove that for every sequence $(u_n)_{n=1}^{\infty} \subset \mathbb{R_+}$ such that $u_n \searrow u \in \mathbb{R_+} $ , $ \; a(u_n) \searrow a(u)$ . So, let $(u_n)_{n=1}^{\infty}$ be such a sequence, and define, for each $n \geq 1$ , $A_{u_n}=\left\{t \in \mathbb{R_+} \; \colon c(t) > u_n \right\}=c^{-1}\big(u_n, +\infty\big)$ . It is immediate to see that $(A_{u_n})_{n=1}^{\infty}$ is an increasing sequence of sets, and that for any $u \in \mathbb{R_+}$ , $ \; a(u)=\inf \left\{ A_{u} \right\}$ . So, $A_{u_n} \nearrow \bigcup_{n=1}^{\infty}A_{u_n}=\bigcup_{n=1}^{\infty}c^{-1}(u_n, \infty)=c^{-1}(\bigcup_{n=1}^{\infty}(u_n, \infty))=c^{-1}\big(u, \infty \big)=\left\{t \in \mathbb{R}_+ \colon c(t) > u \right\}=A_u$ . My issue is that I can't see where this list of equalities makes use of the right-continuity assumption on $c \,$ : $\bigcup_{n=1}^{\infty}A_{u_n} = \bigcup_{n=1}^{\infty}c^{-1}(u_n, \infty)$ follows by definition of $A_{u_n}$ ; $\bigcup_{n=1}^{\infty}c^{-1}(u_n, \infty)=c^{-1}(\bigcup_{n=1}^{\infty}(u_n, \infty))$ follows from measurability of $c$ with respect to the Lebesgue measure on $\mathbb{R_+}$ ; $c^{-1}(\bigcup_{n=1}^{\infty}(u_n, \infty))=c^{-1}(u, \infty)$ follows from the fact that $u_n \searrow u_n$ . Once the equality $\bigcup_{n=1}^{\infty} A_{u_n} = A_u$ is proven, the rest of the claim follows smoothly: By definition, $a(u_n)=\inf\left\{ A_{u_n} \right\}$ , hence \begin{align}
\lim_{n \rightarrow \infty}a(u_n)=\lim_{n \rightarrow \infty} \inf A_{u_n}=\inf \left\{ \lim_{n \rightarrow \infty} A_{u_n} \right\} = \inf \left\{ \bigcup_{n=1}^{\infty}A_{u_n} \right\}=\inf \left\{ A_u \right\}=a(u)
\end{align} I am just not sure why it is possible to interchange $\inf$ and $\lim$ in \begin{align}
\lim_{n \rightarrow \infty} \inf \left\{ A_{u_n} \right\} =\inf \left\{ \lim_{n \rightarrow \infty} A_{u_n} \right\}
\end{align} but my main concern is about where the right-continuity of $c$ is required in the proof. Thanks to anyone who can help!","['cumulative-distribution-functions', 'limits', 'continuity', 'real-analysis']"
4710930,Book recommendations: Olympiad Geometry,"As a 10th grader who'll take the ICSE exam in Q1 2024, I am planning to attempt the Indian Olympiad Qualifier in Mathematics next year, and quite hopefully RMO, INMO, and IMO afterward. I have found good sources to learn about other topics like number theory, algebra, and combinatorics. However, I am struggling to find books from where I can learn about Olympiad geometry. Any recommendations for books that explain the theoretical aspects of geometry and combinatorial geometry properly?","['contest-math', 'book-recommendation', 'combinatorial-geometry', 'geometry']"
4710933,Finding $n$ points on a cube such that the distance between them is maximized.,"Suppose a unit cube. How can I place $n$ points on it (or in it, although I suspect that that will only be optimal for a large $n$ ) such that the sum of the distances between all pairs of points is maximized? For $n=2$ this is trivially the opposite vertices. For $n=8$ it seems like it is obviously all vertices. But how do I find this for general $n$ ?","['optimization', 'calculus', 'geometry']"
4710939,How do we use the right hand rule for Stokes' theorem?,"Let $C$ be the intersection curve between the plane $z = 10 - x - y$ and the cylinder $x^2+y^2 = 1$ , oriented such that the projection of the curve onto the xy-plane is positively oriented. Determine the work done by the force field $\mathbf{F} = (x, x^3, z^3)$ in circulating around the curve $C$ . Long story short I parameterized the curve using polar coordinate: $$r(t,r) = (r\cos(t), r\sin(t), 10 - r\cos(t) - r\sin(t))$$ Then I partial differentiated with respect to t and r and then took their cross product to get the normal vector: $$r_{t} \times r_{r} = (-r, -r, -r)$$ The curl of $F$ became $F = (0,0, 3x^2) = (0,0, 3r^2\cos^2(t))$ and the dot product between the normal and $F$ is then $-3r^3\cos^2(t)$ . $$\iint_{D} -3r^3\cos^2(t) = -\frac{3\pi}4$$ The correct answer is positive, and it got me thinking is because it says in the question that curve projection on the xy-plane has a positive orientation and using the right hand rule, the normal points upwards. This means that I've to change the direction of my normal vector right? Is this the only reason? While we are on the subject, I don't need to use Jacobian determinant right? As in multiply by ""r"" when I parametrize it. Is it because I am already calculating the change in the area caused by the variable change when I take the cross product?","['definite-integrals', 'greens-theorem', 'multivariable-calculus', 'calculus', 'stokes-theorem']"
4710963,"Maximum possible number of 1012-element subsets of {1,2,...,2024} such that no three intersect at more than one element","I came across the following problem: At most how many $1012$ -element subsets of $\lbrace 1,2,\dots,2024 \rbrace$ may be chosen such that the intersection of any three subsets has at most one element? I tried small cases. For example, I considered $3$ -element subsets of $\lbrace 1,2,3,4,5,6 \rbrace$ . By playing around (see table below), I think I got that for this case, the maximum is 8 subsets: 1 2 3 4 5 6 x x x x x x x x x x x x x x x x x x x x x x x x Essentially, we reach the point that for all triples $(a,b,c)$ at least one of $a-b, b-c, a-c$ are already in two subsets together. However, I have no idea how to prove that this is the maximum amount for even the $\lbrace 1,2,3,4,5,6 \rbrace$ case, and am stumped on how to proceed with the general or $2024$ case. Intuitively, I feel as though there must be some way to upper-bound the number of subsets and then give a construction that satisfies that upper bound, but I am lost on how to proceed. Any help would be greatly appreciated. Thank you!","['recreational-mathematics', 'puzzle', 'combinatorics', 'discrete-mathematics']"
4710964,Sufficient condition for a profinite group to be topologically finitely generated,"Is it true that if $G = \varprojlim G_i$ is a profinite group such that every $G_i$ has a generator set $S_i$ whose cardinality is uniformly bounded for all $i$ , then $G$ is (topologically) finitely generated? I found it in the survey linked here: https://arxiv.org/pdf/math/0703885.pdf forth line, page number 3, with no reference and I could not prove it nor disprove it. The converse is of course true and this sounds intuitive for me, from the perspective that $G$ tends to be the limit of the $G_i$ 's. However, a clear obstruction is that no every lift of an element in $G_i$ should work to topologically generate everything.","['group-theory', 'profinite-groups']"
4710992,Log power law confusion with squares,"I have this equation: $$\log_{10} [(𝑥+1)^2]=2$$ Applying the power law it becomes: $$2\log_{10} (𝑥+1)=2$$ Solving for $x$ : $$\begin{align*}
\log_{10} (𝑥+1)&=\frac{2}{2}\\
\log_{10} (𝑥+1)&=1\\
x+1 &= 10^1 \\
x&=9 \end{align*}$$ So I have one solution when using power law. However, without using the power law: $$\begin{align*}\log_{10} [(𝑥+1)^2]&=2\\
(𝑥+1)^2&=10^2\\
(𝑥+1)^2&=100\end{align*}$$ If $x=-11$ , $$\begin{align*}(-11+1)^2&=100\\
(-10)^2&=100\end{align*}$$ If $x=9$ , $$\begin{align*}(9+1)^2&=100\\
(10)^2&=100\end{align*}$$ Thus I can get $2$ solutions for $x$ without using the power law since I am able to square the negative value too. What am I doing wrong?","['algebra-precalculus', 'logarithms']"
4711034,"""General"" Solutions to $\sin{x} = \cos{x}$","I need to find the general solution to sin( ${\alpha}$ )=cos( ${\alpha}$ ). The question did not mention in which quadrant ${\alpha}$ was in. Below is my solution: $\sin({\alpha})=\cos({\alpha})$ $\sin({\alpha})=\sin\left(\frac{\pi}{2} - {\alpha}\right)$ -- Since the general solution for $sin({\alpha}) = sin({\beta})$ is ${\alpha} = \pi n + {(-1)^n}({\beta})$ -- ${\alpha} = \pi n + {(-1)^n}\left(\frac{\pi}{2} - {\alpha}\right)$ This the answer I got. However, the book did it a different way. Instead of converting cos to sin as I did, the book converted sin to cos, which gave a completely different answer. Here is what the book did: $\sin({\alpha})=\cos({\alpha})$ $\cos(\frac{\pi}{2} - {\alpha})=\cos({\alpha})$ -- Since the general solution for cos( ${\alpha}$ ) = cos( ${\beta}$ ) is ${\alpha} = 2\pi n \pm {\beta}$ -- ${\alpha} = 2\pi n \pm \left(\frac{\pi}{2} - {\alpha}\right)$ This the answer the book got. From this, I am unsure weather to convert sin to cos (like the book did) or cos to sin (like I did), since they both seem like valid steps. Any help is appreciated. Thanks! ---Edited---
Thank you for your answers! I have modified the question. Could you please look at the picture and help?",['trigonometry']
4711045,Does This Equation Have a Closed Form Solution?,"In a previous question I asked ( Lagrange Method With Random Variables ), someone suggested to me (in the comments) that I can use Maximum Likelihood Estimation to arrive at the same solution for an Optimization Problem involving the Lagrange Method ( https://en.wikipedia.org/wiki/Inverse-variance_weighting ). Here is the original problem: Consider a generic weighted sum $Y=\sum_i w_i X_i$ , where the weights $w_i$ are normalized such that $\sum_i w_i = 1$ . If the $X_i$ are all independent, the variance of $Y$ is given by $\text{Var}(Y) = \sum_i w_i^2 \sigma_i^2$ Now, here is my attempt to arrive at the same solution using Maximum Likelihood Estimation (as per the suggestion in the comments): Suppose $Y = \sum_{i} w_iX_i$ , where $w_i$ are the weights, $X_i$ are independent random variables, and $\sum_{i} w_i = 1$ . We want to find the optimal weights that minimize the variance of $Y$ . If $X_i$ follows a Normal Distribution with mean $\mu_i$ and variance $\sigma_i^2$ , then we can define some new random variable $Y$ and write the distribution of $Y$ as: $$Y \sim N\left(\sum_{i} w_i\mu_i, \sum_{i} w_i^2\sigma_i^2\right)$$ Now, the logarithm of the likelihood function can be written as: $$\log(L) = \log\left(\prod_{i} \frac{1}{\sqrt{2\pi\sum_{j} w_j^2\sigma_j^2}} \exp\left(-\frac{1}{2\sum_{j} w_j^2\sigma_j^2}(y - \sum_{i} w_i\mu_i)^2\right)\right)$$ $$= -\frac{1}{2}\log(2\pi) - \frac{1}{2}\log\left(\sum_{j} w_j^2\sigma_j^2\right) - \frac{1}{2\sum_{j} w_j^2\sigma_j^2}(y - \sum_{i} w_i\mu_i)^2$$ To find the maximum likelihood estimate, we differentiate (using chain rule) the log-likelihood with respect to $w_i$ and set it to zero: $$\frac{\partial}{\partial w_{i}} \log (L) = -\frac{w_i\sigma_i^2}{(\sum_j w_j^2\sigma_j^2)^2} + \frac{(\sum_j w_j^2\sigma_j^2)(y - \sum_{i} w_{i} \mu_{i})\mu_i + (y - \sum_{i} w_{i} \mu_{i})^2w_i\sigma_i^2}{(\sum_j w_j^2\sigma_j^2)^2} = 0$$ But I am not sure if the above system of likelihood equations has a closed form solution for $w_i$ . But perhaps this system of likelihood equations might have a closed form solution if all $\mu_i$ are equal to each other ? My Question: Can someone please tell me if I am doing this correctly? Can Maximum Likelihood Estimation really be used to arrive at the same optimal solutions for $w_i$ as compared to the Lagrange Method? Thanks! Note : If $Y=\sum_i w_i X_i$ , then I don't think that variance of $Y$ is given by $\text{Var}(Y) = \sum_i w_i^2 \sigma_i^2$ unless each $X_i$ has the same mean $\mu$ . This suggests that the ""objective function"" (i.e. $\text{Var}(Y)$ being optimized would actually be a completely different function with a completely different optimal solution of $w_i$ I think actually asked a question about this same point over here: Simplifying the Formulas for Weighted Means","['optimization', 'derivatives', 'probability', 'maximum-likelihood']"
4711069,Does $\sum_n x_{\lceil \alpha^n\rceil} < \infty $ imply $x_n \to 0$?,"Suppose we have a (nonnegative if you like) sequence $x_n$ in $\mathbb{R}$ or a general Banach space.
If for all $\alpha > 1$ , $\sum_{n = 1}^\infty x_{\lceil \alpha^n\rceil} < \infty$ , does this imply that $x_n \to 0$ ? This question is inspired by the ""sub-sub-sequence"" characterization of convergence (e.g. this question and duplicates). (Application: I am trying to reduce a question of stochastic sequence convergence to stochastic series convergence.) My thoughts: the most likely counterexample to this claim would be a sequence that doesn't converge to zero, but has its fluctuations super-exponentially far apart, e.g. $x_n = 1$ if $n = m!$ for some $m$ , and $0$ otherwise. In any case, if $x_n > \epsilon$ infinitely often, I am not sure whether such $n$ certainly have an infinite intersection with the indices $\{\lceil \alpha^n\rceil\}$ for sufficiently small $\alpha$ . (Maybe I am missing something obvious here.) Edit 0: The prototype sequence I have in mind is $x_n = n^{-1}$ . Edit 1: The summability criterion implies $\lim \inf x_n = 0$ . Intuitively it ought to imply more, as summability is stronger than convergence to zero. Perhaps we can find a creative way to combine series at varying $\alpha$ ?","['convergence-divergence', 'sequences-and-series', 'real-analysis']"
4711071,Are these Hankel matrices positive semidefinite?,"While working on a quantum information project, I encountered the following two Hankel matrices $$ a_{i,j} = (i+j)!(2n-(i+j))! ,\qquad b_{i,j} = (i+j+1)!(2n-(i+j))! $$ where $0 \le i,j \le n$ and $!$ denotes the factorial. I would like to know if the matrices are positive semidefinite for any integer $n \geq 2$ . Their positive semidefiniteness is related to the quantum entanglement of some systems. Computing the eigenvalues via Mathematica up to $n \approx 30$ seems to affirm that these matrices are indeed positive definite. How would I prove it for all $n$ ?","['positive-semidefinite', 'hankel-matrices', 'matrices', 'linear-algebra', 'combinatorics']"
4711105,Jacobian criterion for singularity types,"Let $I = (f_1,...,f_m) \subset k[x_1,...,x_n]$ be a prime ideal where $k$ is an algebrically closed field and let $V = \text{Spec}(k[x_1,...,x_n]/I)$ be the corresponding algebraic variety. If we take any $p \in V$ , we can define the Jacobian matrix $$J_p = \begin{pmatrix}\frac{\partial f_i}{\partial x_j}(p)\end{pmatrix}.$$ The Jacobian criterion tells us that $V$ is smooth at $p$ if and only if $\text{rk}(J_p) = n - \dim V$ . Certainly, if $rk(J_p) < n - \dim V$ then $p$ is singular. However, is there a way of using the Jacobian matrix to tell what type of singularity $p$ is if $p$ was to be a singular point? In particular, can we tell if $p$ is a nodal singularity or not using the Jacobian matrix? By definition $p$ is nodal if and only if $\hat{\mathcal{O}}_{V,p} \cong k[[x,y]]/(xy)$ but this completion might be hard to compute if $I$ was complicated. But even for a simple example $V:y^2 - x^2 - x^3$ , is there a way of using the partial derivatives to tell that $V$ the singularity at the origin is nodal? If there is such a criterion, a explanation or a link to a reference would be appreciated.",['algebraic-geometry']
4711129,Is the problem NP-hard?,"Let $GF(p) = ({\mathbb Z}_p, +, \times)$ be the Galois field where $p>2$ is prime and let $$
H=\{1,2,\cdots, \frac{p-1}{2}\}.$$ I need an algorithm (subexponential in terms of $\log_2 p$ ) that computes at least one element inside the following expression (or, indicate that it is empty) $$
\bigcap_{i=1}^n a_i H
$$ where $a_1, \cdots, a_n\in {\mathbb Z}_p\setminus \{0\}$ are given as inputs and $aH=\{a\times x\mid x\in H\}$ .
I believe that this problem is NP-hard. Can someone give any hints, links, maybe related problems.","['number-theory', 'np-complete', 'prime-numbers']"
4711267,Why plot of a sinusoid with a large phase appears like a staircase?,I plotted the following in MATLAB and Desmos: y = cos(x + 6998666554443343)    (1) The plot is shown here: Plot of (1) This staircase behaviour seems to appear with any large number. The following plot with an extra digit in its phase is another example: y = cos(x + 69986665544433456)    (2) Plot of (2) I think that I am crossing the limits of resolution somehow. Can someone explain the reason behind this 'staircase' behaviour?,"['trigonometry', 'machine-precision', 'matlab']"
