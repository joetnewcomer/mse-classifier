question_id,title,body,tags
3631296,Prove for sets $A$ and $B$ that $A\cup{B}=B\cup{A}$.,"Prove for sets $A$ and $B$ that $A\cup{B}=B\cup{A}$ . Here is my attempt on proving this, by the definition of subset theorem we have $(A\cup{B})\subseteq(B\cup{A})$ and $(B\cup{A})\subseteq(A\cup{B})$ then $x\in({B\cup{A}})$ , $x\in({B\cup{A}})$ therefore $A\subseteq{B\cup{A}}\land{B\subseteq{A\cup{B}}}$ this implies $x\in{A}\lor{x\in{B}}\iff{x\in{B}}\lor{x\in{A}}$ . I know there is no much work can be done here, I just want to improve my proof's written.","['elementary-set-theory', 'proof-writing']"
3631308,Proof that $\{x\in\Bbb Z\mid x=4p-1 \text{ for some }p\in\Bbb Z\}$ and $\{y\in\Bbb Z\mid y =4q-5 \text{ for some }q\in\Bbb Z\}$ are equal,"I have a homework question that asks the following: $A=\{x\in\mathbb{Z}\mid x=4p-1 \text{ for some }p\in\mathbb{Z}\}$ $B=\{y\in\mathbb{Z}\mid y =4q-5 \text{ for some }q\in\mathbb{Z}\}$ Prove that $A = B$ I have tried the following: Let $D \subseteq  A $ | $ 1\leq p \leq 5 $ $= \left\{3,7,11,15,19\right\}$ Let $E \subseteq B$ | $ 1\leq q \leq 5$ $= \left\{-1, 3,7,11,15\right\}$ I can see that if I shifted D so that $p = p-1$ , $D = E$ , and since $D \subseteq A$ and $E \subseteq B$ , it shows that $A = B$ . Would induction be the best way to prove this formally? If so, since there are two variables, $p \text{ and } q$ , would I set both to zero for base case, or just one? Then for the induction step, would I set it for $p+1$ and $q+1$ ?",['elementary-set-theory']
3631412,Find general solution of $xx''=(x')^3$,"I'm trying to solve ordinary differantial equation that looks like this: $xx''=(x')^3$ I'm using the substitution $u(x)=x'$ and now I have $ \frac{uu'}{u^3}=\frac{1}{x}$ After solving it I obtain with: $u=-\frac{1}{lnx} +c$ and  going back to x' I have: $ x'=-\frac{1}{lnx} +c$ How can I proceed from here? Integrating both sides doesn't look good, am I doing something wrong?",['ordinary-differential-equations']
3631449,"Can one divide a convex polygon into two convex polygons of the same area, with parallelly moving a given straight line on a plane?","I was wondering if one can divide a convex polygon into two convex polygons of the same area, with parallelly moving a given straight line on a plane. Drawing some figures to test, it seems like always possible. It's just my intuition, though. Could one prove or disprove this?",['geometry']
3631463,Space of ergodic measures is $G_\delta$ in space of invariant measures,"I'm reading an old paper by Varadarajan (1963) titled ""Groups of automorphisms of Borel spaces"" and I'm trying to fulfill the details.
There are many questions I thought about for quite a long time with no success.
For the moment, I confine myself to the first part of the proof of Theorem 4.2 (possibly, the remaining one would be treated in another question). Notation and Terminology: Suppose $G$ lcsc (=locally compact second countable) group.
By measure we mean a Borel probability measure.
A measure on a Borel space $X$ is said to be invariant if $\mu(A)=\mu(gA)$ for all borel subsets $A$ and $g\in G$ and we write $\mu\in\mathscr{I}$ .
It is ergodic if all $G$ -invariant Borel subsets are either null or conull.
The set of ergodic measures is denoted by $\mathscr{E}$ . Q: Suppose $Y$ is a compact metric $G$ -space ( $G$ acts continuously on $Y$ ), so that the space $\mathscr{I}_Y$ of $G$ -invariant measures on $Y$ is compact wrt the weak-topology (the one which makes continuous the functions $\mu\mapsto \int_Y g\,d\mu$ forall bounded continuous real-valyued functions $g\in C_b(Y)$ ).
Then, referring to a paper by Choquet titled ""Existence et unicité des représentations intégrales"", he claims $\mathscr{E}_Y$ is a $G_\delta$ subset.
Unfortunately, I have not found such a paper, so I would like to know how to prove that $\mathscr{E}_Y$ is $G_\delta$ (any useful reference on the subject is appreciated, even better if a book). Edit: I have made the notion of ergodicity more explicit.","['measure-theory', 'borel-measures', 'ergodic-theory', 'borel-sets', 'probability-theory']"
3631506,Derivative of a map $ f:\mathbb{R^n\times R^n}\rightarrow\mathbb{R}$,"I want to calculate the derivative of a function $$f:\mathbb{R^n×R^n}\rightarrow\mathbb{R}$$ defined by $$f(x,y)=\langle Ax,y \rangle,$$ where $A$ is any $n\times n$ matrix over set of reals $\mathbb{R}$ . I have never seen such types questions to calculate derivative in which domain is $\mathbb{R^n\times R^n}$ . Basically my question is that, is the derivative of $f$ is same  as that of 
  function $$g:\mathbb{R^{2n}}\rightarrow\mathbb{R}$$ defined by $$g(x_1,x_2,.......x_n,y_1,y_2,.....y_n)=\langle Ax,y \rangle,$$ where $x=(x_1,x_2......x_n)$ , $y=(y_1,y_2.....y_n)$ . Your help would be precious to me, thanks in advance!","['multivariable-calculus', 'calculus', 'functional-analysis', 'real-analysis']"
3631512,Finding a multiple of a given number which can be expressed as 1+2+...+x,"An unrelated problem I came across in the domain of computer science reduced to the following mathematical problem: For a given number $ n\in \mathbb{N} $ , I need to find if any multiple of that number can be expressed as a series of the first $x$ natural numbers. Further, if such multiples exist, I need to find the least such multiple. That is, for a given $n$ , I need the lowest values for $k, x$ that satisfy the equation: $$
n \times k = \frac{x\times \left(x+1  \right)}{2}, n\in \mathbb{N}, k\in \mathbb{N}, x\in \mathbb{N}
$$ I understand that this is a diophantine equation, and while I could find ways to solve linear and quadratic diophantine equations, I could not find a general form that could be applied to the problem above, especially since there are two unknowns in the equation. I also considered that one way to solve the problem would be to try and factorize $2 \times n$ into two consecutive numbers as indicated by the rearranged equation: $$
k = \frac{x\times \left(x+1  \right)}{2 \times n}, n\in \mathbb{N}, k\in \mathbb{N}, x\in \mathbb{N}
$$ Finally, since the problem originated in the context of computer programs, I figured that if I couldn't find a mathematical approach to solve this equation, I could simply try for all values of x till I found an appropriate value. The problem with that approach (apart from the less than ideal computational time needed) is that I do not know if the $n$ I'm solving this for has such a multiple or not, hence I have no way of knowing if the brute-force algorithm would terminate. So I also tried (unsuccessfully) to find a method to determine if such a value for $k, x$ exists for a given $n$ . Does such a method exist? I would appreciate any help in trying to solve this problem.","['number-theory', 'gcd-and-lcm', 'diophantine-equations', 'factoring', 'sequences-and-series']"
3631542,$y(x+y^3)dx=x(y^3-x)dy$,"Solve $y(x+y^3)dx=x(y^3-x)dy$ Attempt: $$\frac{dy}{dx}=\frac{y(x+y^3)}{x(y^3-x)}.$$ Let $y^3=xt.$ Then $$3y^2\frac{dy}{dx} = t+x\frac{dt}{dx}.$$ Therefore $$\frac{(t+x\frac{dt}{dx})}{(3y^2)} = \frac{t(t+1)}{y^2(t-1)}$$ and hence $$x\frac{dt}{dx}=\frac{2t^2+4t}{t-1}.$$ Integrating, $$\ln|cx|=\frac{3}{4}\ln|t+2|-\frac{1}{4}\ln|t|.$$ Simplyfing, $$cx^2=\frac{y^3+2x}{y}$$ but the given answer is $y=cx^{1/3}$ . Where have I made a mistake?","['integration', 'calculus', 'ordinary-differential-equations']"
3631546,How is a multivariable function integrated,"I am unsure about how integration is handled in the context of area integrals. For example, $$ \int_0^1 \!\int_0^{y=x-1} \! (x-y) dydx $$ Does the inner integral evaluate to $ xy - \frac{1}{2}y^2$ or to $-\frac12(x-y)^2$ - or are they the same thing? Indeed expanding out $\frac12(x-y)^2$ gives $xy -\frac12 y^2 - \frac12 x^2$ with the additional $\frac12 x^2$ , which makes sense as $x$ is held as a constant in the inner integral. This example is from a book wherein $\!\int_0^{y=x-1} \! (x-y)dy$ is evaluated to $[\frac12 (x - y)^2]$ . However, in a later example $\!\int_0^{y=x-1} \! (x^2-y^2)dy$ is evaluated to $[x^2y - \frac13y^3]$ . Why are they treated in different ways?","['integration', 'multivariable-calculus', 'area']"
3631561,Conjecture on a functional equality and nested radical of Ramanujan :$3=\sqrt{1+f(2)\sqrt{1+f(3)\sqrt{1+\cdots}}}$,"Hi i was wondering something about the great Ramanujan : I think moreover I'm not the only one who propose this kind of problem (so if you have a link related to this subject). We have : $$3=\sqrt{1+2\sqrt{1+3\sqrt{1+\cdots}}}$$ Now the problem : Let $f(x)$ be a positive,continuous  and differentiable on function $(0,\infty)$ and non-constant  then the functional equation : $$3=\sqrt{1+f(2)\sqrt{1+f(3)\sqrt{1+\cdots}}}$$ Have as unique solution $f(x)=x$ I have tried to build a counter-example without success .First it seems obvious for strictly increasing\decreasing functions such that $f(x)>x$ or $f(x)<x$ . I would like to create an counter-example of the form : $$f(x)=x+g(x)$$ Where $g(x)$ is a periodic function . I have tried more general representation without success . If it's true we can see how Ramanujan was great . Any helps are highly appreciated . Thanks a lot .","['nested-radicals', 'conjectures', 'functions', 'examples-counterexamples']"
3631580,Find a partition of set $\mathbb{Z}$ in 4 parts | all parts are infinite in size,"Find a partition of the set $\mathbb{Z}$ with four parts that are infinite in size. A collection of sets $A$ can be considered a partition if: 1) $A =  A_1 \cup A_2 \cup ... \cup A_n$ AND 2) $A_1, A_2, ... , A_n $ are mutually disjoint. Could these four sets be considered a partition? $A_1 = \{n \in \mathbb{Z} : n = 2p \text{ for } p \in \mathbb{Z} : P \geq 0\}$ $A_2 = \{n \in \mathbb{Z} : n = 2p -1 \text{ for } p \in \mathbb{Z} : P > 0\}$ $A_3 = \{n \in \mathbb{Z} : n = -2p  \text{ for } p \in \mathbb{Z} : P > 0\}$ $A_4 = \{n \in \mathbb{Z} : n = -2p +1 \text{ for } p \in \mathbb{Z} : P > 0\}$ If $A_1$ contains all positive even numbers including 0, $A_2$ contains all odd positive numbers, and $A_3$ and $A_4$ contain the negative equivalents, am I covering all bases here?",['elementary-set-theory']
3631606,Show that $\sum_{k=1}^n \frac{X_k}{k^2}$ converges a.s,"Good morning everyone, I am new to this website so I hope everything is going to be ok. Here is a question for my homework : Let $(X_n)_{n \geq 1}$ be a sequence of independent random variables. For every $k$ greater than or equal to $1$ , $X_k$ has a density $f_X(t) = \frac{1}{2} \exp(-|t|) $ with respect to the Lebesgue measure on $\mathbb{R}$ . Show that : $$S_n = \sum_{k=1}^n \frac{X_k}{k^2}$$ converges almost surely. We can calculate $\mathbb{P}(|X_k| \geq 2 \log(k))$ for every $k \geq 1$ . I tried to use Borel-Cantelli lemmas but I don't know the limit of this series. I also tried to use the law of large numbers, without success. All I did is : $$\mathbb{P}(|X_k| \geq 2 \log(k)) = \mathbb{P}(|X_1| \geq 2 \log(k)) = \frac{1}{k^2}$$ Hence : $$S_n = \sum_{k=1}^n \frac{X_k}{k^2} = \sum_{k=1}^n X_k \cdot \mathbb{P}(|X_1| \geq 2 \log(k))$$ What can I do now ? Thank you ! (PS : I'm not english so I apologize for the mistakes...).","['convergence-divergence', 'probability-theory']"
3631648,Exponential Likelihood Function,"Suppose $X_1, ..., X_n \stackrel{iid}{\sim}$ Exponential(rate = $\lambda$ ) independent of $Y_1, ..., Y_n \stackrel{iid}{\sim}$ Exponential $(1)$ . Define $Z_i \equiv \min\{X_i, Y_i\}$ I want to find the maximum likelihood estimator for $\lambda$ in the following scenario:  I observe $Z_1, ..., Z_n$ and $Y_1, ..., Y_n$ but NOT any of the $X_i$ . First I need to determine the likelihood and then maximize it over $\theta > 0$ , but I'm not really sure of the right approach.  I calculate the joint cdf as follows: $$P(Z_i \leq z, Y_i \leq y) = \begin{cases} P(Y_i \leq y), & y \leq z \\ P(Y_i \leq z, Y_i \leq X_i) + P(Y_i \leq y, X_i \leq z, X_i < Y_i), & y > z\end{cases} \\
= \begin{cases} 1- e^{-y}, & y \leq z \\
1-e^{-z} + (e^{-z}-e^{-y})(1-e^{-\lambda z}), & y > z \end{cases}$$ This is because $Z_i \leq Y_i$ always.  Would the likelihood function therefore be: $$L(\lambda |Y_i, Z_i, i \in \{1,...n\}) = \prod_{\{i : Y_i = Z_i\}} (1-e^{-Y_i}) \prod_{\{i:Y_i > Z_i\}}  \lambda e^{-Y_i}e^{-\lambda Z_i}$$ splitting into the ""discrete"" and ""continuous"" parts?  Or am I getting this wrong?  Or should I be doing something like here or here ?  I should note my scenario is different than theirs, as intuitively at least, observing the magnitude of the difference between the minimum and the maximum (in the cases where $Z_i$ and $Y_i$ differ) should give us more information about $\lambda$ , right?","['statistical-inference', 'statistics', 'probability-distributions', 'exponential-distribution', 'maximum-likelihood']"
3631670,Does there exist a sequence of sets such that the sequence of their cardinalities is strictly decreasing?,"Does there exist a sequence of sets such that the sequences of their cardinalities is strictly decreasing? More explicitly, does there exist a sequence of sets $S_1,S_2...$ such that for each $i$ there exists an injection from $S_{i+1}$ into $S_i$ yet $S_i$ and $S_{i+1}$ is no bijection? In other words, each $S_{i+1}$ is strictly smaller (in cardinality) than $S_i$ .",['elementary-set-theory']
3631710,How to sketch a curve given its intrinsic equation,"I'm wondering what the best method of sketching a curve is, if you know the coordinates of a point on the line and  its intrinsic equation in the form: $$
s=f(\psi)
$$ where s is arc length from the origin to a point on the curve and where $$tan(\psi )=\frac{dy}{dx}$$ When attempting a problem I tried to convert the equation to cartesian following a method online but I ended up getting an integral which is unsolvable with my level of maths I'm wondering if there is another method of sketching this curve. Thank you for any replies","['multivariable-calculus', 'calculus', 'graphing-functions', 'geometry']"
3631769,Motivation of geometric analysis,Geometric analysis in some sense tries to make sense of calculus on spaces where we don't require any smoothness (for example metric measure spaces). What are the type of problems that motivate these kind of studies?,"['geometric-measure-theory', 'metric-spaces', 'analysis']"
3631869,Improving set intersection lexicographically,"Let $A=\{1,2,\dots,n\}$ , and let $A_1,\dots,A_m$ be subsets of $A$ of the same size.
  Let $k$ be a fixed positive integer.
  We want to choose $B\subseteq A$ of size $k$ such that $\min(|A_1\cap B|,\dots,|A_m\cap B|)$ is maximized. I'm not sure whether a direct approach to find such $B$ exists, so one way is to proceed greedily. First choose an arbitrary set $B$ of size $k$ . Then, if possible, we try to ""improve"" it by replacing an element in $B$ with an element outside it. If the criterion for improvement is that $\min(|A_1\cap B|,\dots,|A_m\cap B|)$ should increase, we may get stuck . For example $n=4$ , $m=k=2$ , $A_1=\{1\}$ , $A_2=\{2\}$ . Initially $B=\{3,4\}$ , and we cannot improve using this criterion. Yet a better $B$ is $B=\{1,2\}$ . So, how about the improvement criterion being that the sequence $(|A_1\cap B|,\dots,|A_m\cap B|)$ improves lexicographically? For sequences $(a_1,\dots,a_r)$ and $(b_1,\dots,b_r)$ , where we sort $a_1\leq\dots\leq a_r$ and $b_1\leq\dots\leq b_r$ , we say that the latter improves the former if, for the first $i$ such that $b_i\neq a_i$ , we have $b_i>a_i$ . In the example above, we would change $B=\{3,4\}$ to $B=\{1,4\}$ , then to $B=\{1,2\}$ , resulting in a desired set $B$ . Does this algorithm always end with a desired set $B$ , or can it get stuck?","['combinatorics', 'algorithms']"
3631895,How to argue that discrete random variables do not have a Radon–Nikodym density?,"Suppose I have a discrete random variable $X\in\{1,2,3,4\}$ with a probability mass $\mu(X=k)=1/4$ for $k=1,2,3,4$ . How to rigorously argue that it doesn't have a Radon-Nikodym density with respect to Lebesgue measure $\lambda$ ? One argument I have in mind is as follows: as having a Radon–Nikodym density is equivalent to being absolutely continuous with respect to the Lebesgue measure $\lambda$ , so we look at whether $X$ is absolutely continuous. It's obvious that $X$ is not absolutely continuous, because for measurable set $[1,1]=1$ , the Lebesgue measure is $\lambda([1,1])=\lambda(1)=0$ , which doesn't imply $\mu(1)=0$ . Is this argument correct? If not, how to correct it (and make it fully rigorous)? Thanks! (another related question: does there exist any measure $c$ , such that a discrete random variable have a density with respect to $c$ ? Thanks!)","['measure-theory', 'lebesgue-measure', 'probability-distributions', 'probability-theory', 'density-function']"
3631903,"Solve $\tan x = \frac{p}{q}$, where $p, q\in\mathbb{Z}$ such that $3\cos x-4\sin x=-5$","A Calculus A level trigonometry problem: Solve $\tan x = \dfrac{p}{q}$ where $p,q\in\mathbb{Z}$ such that $$3\cos x\ - 4\sin x = -5$$ I tried moving terms to one side, but that doesn't help much. Any ideas?","['algebra-precalculus', 'trigonometry']"
3631914,How to check these terms in $\int{x^x dx}$ power series are correct?,"I've got an infinite power series for $\int{x^x}{dx}$ over $x>0$ and I suspect it might have this form: $$\int{x^x}{dx} = \sum_{n=1}^{\infty}{\frac{x^n}{A(n)}}\sum_{k=0}^{n-1}{B(n,k) \log^k(x)}$$ $$A(n)=\mathrm{lcm}(n^n,n!)$$ $$B(n,k)=\begin{cases}(-1)^{1+n+k}\times\frac{n!\times n^k}{\gcd(n^n,n!)\times k!} & n > k\\0 & \mathrm{otherwise}\end{cases}$$ The first bunch of terms in the sum look like this: $$x
+\frac{x^2}{4} (-1 + 2 \log(x))
+\frac{x^3}{54} (2 - 6 \log(x) + 9 \log^2(x))
+\frac{x^4}{768} (-3 + 12 \log(x) - 24 \log^2(x) + 32 \log^3(x))
+\frac{x^5}{75000}((24 - 120 \log(x) + 300 \log^2(x) - 500 \log^3(x) + 
   625 \log^4(x))) + \dots $$ I checked OEIS and the terms followed patterns related to A055774, A051696, and A095996. Is anybody aware of any existing derivation of this series and if not, how can I go about proving the general formula is correct and converges to the integral over $x>0$ ? I've checked using Mathematica to produce the terms. It matches out to 20 terms so far but I'm not sure what to do next to verify it as $n\rightarrow\infty$ . denom[n_] := LCM[n!, n^n]
logterm[n_, k_] := 
 If[n > k, (-1)^(k + n + 1) Denominator[n^n/n!]*n^k/k!, 0]
myseries[x_, n_] := 
 Sum[x^i/denom[i] Sum[logterm[i, j]*Log[x]^j, {j, 0, i - 1}], {i, 1, 
   n}]
(* evaluate and compare *)
Normal[Series[Integrate[x^x, x], {x, 0, 20}]] == myseries[x,20] A related question Finding $\int x^xdx$ explores this integral but it does not feature this particular power series expansion. -- edit --
 I realized the $\mathrm{lcm}(n^n,n!)$ in $A(n)$ can be pulled into the inner sum combining with the $\gcd(n^n,n!)$ in $B(n,k)$ as $n^n \times n!$ so this simplifies things a bit and the series is now: $$
\int{x^x}{dx} = \sum_{n=1}^{\infty}{x^n}\sum_{k=0}^{n-1}{B(n,k) \log^k(x)}
\\
B(n,k)=\begin{cases}(-1)^{1+n+k}\times\frac{n^{k-n}}{k!} & n > k\\0 & \mathrm{otherwise}\end{cases} 
$$","['integration', 'number-theory', 'power-series']"
3631924,Prove trigonometric inequality $\sin x\leq 1-\left(\frac{2x}{\pi}-1\right)^2$,"I was working on a trigonometric inequality and after some manipulations I needed to prove that: $$\sin x\leq 1-\left(\dfrac{2x}{\pi}-1\right)^2, \enspace \forall x\in \left[0,\pi\right).$$ My idea was to move the square on a side and then square root and prove what we got. But I failed. Please help me solve this! Thank you! Please don't use calculus for the proof.","['trigonometry', 'inequality']"
3631965,Finding primitive element of field extension in characteristic 2 corresponding under Galois correspondence to the group $G_f\cap A_n$,"Let $F$ be a field and let $f(X)\in F[X]$ be a separable polynomial over $F$ of degree $n$ . Let $F_f$ be the splitting field of $f$ . Then the Galois group $Gal(F_f/F)=G_f$ acts as a group of permutations on the roots of $f$ and we can consider $G_f$ as a subgroup of the symmetric group $S_n$ . Define $$
SG_f = G_f\cap A_n,
$$ where $A_n$ is the alternating group. Let $$
\Delta(f) = \prod_{1\leq i < j\leq n} (\alpha_i - \alpha_j),
$$ where $\alpha_1,\dots,\alpha_n$ are the distinct roots of $f(X)$ in $F_f$ . It is easy to prove that $\sigma\Delta(f)=\mbox{sgn}(\sigma)\Delta(f)$ for every $\sigma\in S_n$ , and thus we have, if $\mbox{char}(F)\neq 2$ , that $\sigma\in S_n$ fixes $\Delta(f)$ if and only if $\sigma\in A_n$ . Thus under the Galois correspondence, we have that the group $SG_f = G_f\cap A_n$ corresponds to the field $F[\Delta(f)]$ . The condition that $\mbox{char}(F)\neq 2$ is essential, for if $\mbox{char}(F)=2$ then every element of $S_n$ fixes $\Delta(f)$ . Here is my question: How can I find some element $\delta\in F_f$ such that, under the Galois correspondence, the group $SG_f=G_f\cap A_n$ corresponds to the field $F[\delta]$ when $\mbox{char}(F)=2$ ? Added: It is easy to prove that such $\delta_f$ exists: $A_n$ is a normal subgroup of $S_n$ and thus $SG_f$ is a normal subgroup of $G_f$ . If $L=F_f^{SG_f}$ is the fixed subfield of $F_f$ corresponding to de group $SG_f$ , then we have that $L/F$ is a Galois extension and by the primitive element theorem there we have that $L=F[\delta_f]$ for some $\delta_f\in L$ . But it is an existence proof, I need an explicit expression for such $\delta_f$ . Even by following the proof of the primitive element theorem, I am unable to determine such an expression for $\delta_f$ .","['field-theory', 'galois-theory', 'abstract-algebra', 'splitting-field', 'galois-extensions']"
3631976,"What are $\aleph_0$, $\omega$ and $\mathbb{N}$ and how are they related to each other?","I have seen these three symbols, $\aleph_0$ , $\omega$ and $\mathbb{N}$ , a lot in my reading (mostly in analysis, I have very limited experience in set theory). I have seen in various places they are used interchangeably, which is confusing for me. There is no problem that the symbol $\mathbb{N}$ denotes the set of natural numbers. (By convention, the number $0$ may or may not be in the set.) The aleph null $\aleph_0$ is defined as the "" cardinality "" of the set $\mathbb{N}$ . This Wikipedia article says that $\omega$ is the first infinite ordinal. I have seen people use $\mathbb{R}^\omega$ for the set of all real sequences (see, for instance, Munkres's Topology ); some people use $\mathbb{R}^{\mathbb{N}}$ instead, which suggests that $\omega$ and $\mathbb{N}$ may be the ""same"" in some sense. On the other hand, I have never seen $\mathbb{R}^{\aleph_0}$ . The definitions of these three concepts are quite different, yet they seem to be closely related. So my question is: how exactly are they related to each other and in what sense they are (possibly) the same?","['cardinals', 'ordinals', 'notation', 'elementary-set-theory', 'soft-question']"
3631995,"If you know the Diagonal and Area of a Rectangle, can you find the sides of the rectangle? [duplicate]","This question already has an answer here : How to find the dimensions of a rectangle given a) the area and the diagonal or b) the perimeter and the diagonal (1 answer) Closed 4 years ago . If you know the Diagonal and Area of a Rectangle, can you find the sides of the rectangle? I was doing strange math yesterday, and I can across the realization that two different rectangles can’t have the same area and same diagonal. But I haven’t been able to solve a equation that shows this. I also haven’t be able to prove myself wrong? So maybe higher math gods will help? $$\text{Diagonal}=\sqrt{x^2+y^2}$$ $$\text{Area}=xy$$","['algebra-precalculus', 'geometry']"
3632034,Does Murphy prove that states separate the points of a C*-algebra?,"Im currently reading chapter 3.3 of Murphy's book on C*-algebras . This chapter is about positive linear functionals. On the internet I read somewhere (I lost the source) that states (i.e. positive linear functionals with norm one) separate the points of a C*-algebra $A$ . If I'm not mistaken, this means that for any $x,y\in A$ there exists a state $\tau$ on $A$ such that $\tau(x)\neq\tau(y)$ . I can't find this result in Murphy's book, is this right? I need a source (preferably Murphy) that I can refer to. Or else, does it follow easily from the results Murphy presents?. Thanks in advance!","['c-star-algebras', 'operator-algebras', 'operator-theory', 'reference-request', 'functional-analysis']"
3632077,Geodesics on paraboloid self-interesect in an infinite number of points,"I'm trying to solve an exercise of Do Carmo's Riemannian geometry. Specifically, I have to prove that on a paraboloid(that is the revolution surface of a paraboloid $\{(v\cos u,v\sin u,v^2):v\in(0,\infty),u\in(0,2\pi)\}$ ) the geodesics which are not meridians (that is $u\neq $ constant)self-intersects in an infinite amount of points. Using Clairaut's relation it is possible to show that geodesics are characterized (at least locally) by the following ODE's system: \begin{cases}
(1+4v(t)^2)v'(t)^2+u'(t)^2=c_0\\
u'(t)v(t)^2=c_1
\end{cases} where $c_0,c_1$ are unkown constants. This system reduces to the equation $(1+4v(t)^2)v'(t)^4-c_0v(t)^2+c_1=0$ Which I have absolutely no idea on how to solve. Any hint is very much appreciated. P.S: Do Carmo suggest using Clairaut's relation, which I've already used, but it is possible there is a more tricky application.","['geodesic', 'differential-geometry']"
3632164,Show that $\lim _{x\to \infty }\left(\sqrt{x+\sqrt{x+\sqrt{x}}}-\sqrt{x}\right) =1/2$,"I don't know how to start. Is it simple algebraic manipulation where, if, let $a=\sqrt{x+\sqrt{x+\sqrt{x}}} $ and, $b=\sqrt{x}$ the above equation can be manipulated as $\implies a-b$$.\:\frac{a+b}{a+b}=\frac{a^2-b^2}{a+b}$ giving, $\frac{\sqrt{x+\sqrt{x}}}{\left(\sqrt{x+\sqrt{+x\sqrt{+x}}}+\sqrt{x}\right)\:}$ Now, my mind can't think of any method to solve further.","['limits-without-lhopital', 'calculus', 'limits', 'radicals', 'derivatives']"
3632243,Is the union of independent events independent,"If $A_1,A_2,....$ are events all independent of an event $B$ . Do we also have that $\bigcup_{n\geq1}A_n$ is independent of $B$ ?","['elementary-set-theory', 'probability-theory', 'probability']"
3632268,Can we write limits variable-free?,"When first learning the subject and when doing simple calculations, it's convenient to describe derivatives in terms of variables, i.e. $$ \frac{d}{dx} f(x) = f'(x)$$ and we say the derivative takes a ""function"" (expression) and maps it to another ""function"" (expression). But more rigorously, we can say that given an $n$ -manifold $M$ , a chart $x:U\subseteq M\rightarrow\mathbb{R}^n$ , and a function $f:M\rightarrow \mathbb{R}$ , $$ \frac{\partial}{\partial x_i} f \equiv \partial_i(f\circ x^{-1})\circ x$$ This is just one way of reformalizing derivatives, but what we have done is taken out the reliance on a naive notion of ""variables"". I was wondering if we have made any similar re-formulations for the limit. From the above equivalence, we need the notion of a limit to fully describe the partial derivative on $\mathbb{R}^n$ . I.e. for a function $g:\mathbb{R}^n\rightarrow\mathbb{R}$ , $$ \partial_ig(a_1,...,a_i,...,a_n)\equiv \lim_{h\to 0} \frac{g(a_1,...,a_i-h,...,a_n)-g(a_1,...,a_i,...,a_n)}{h} $$ You could use an ""epsilon-delta"" definition of limits if $n=1$ and you want to totally-order $\mathbb{R}$ or you could define limits using neighborhoods or open sets, but the notation is what I'm stuck on. Is there a way to formalize the limit as a map $$ \lim_{i,\ b}:\ C^0(\mathbb{R}^n)\ \to\ C^0(\mathbb{R}^n) $$ $$ \left(\lim_{i,\ b}g\right)(a_1,...,a_i,...,a_n)\equiv\ ``\,\lim_{a_i\to b}\left(g(a_1,...,a_i,...,a_n)\right)"" $$ similar to the many ways we have reformatted the derivative? For clarity, my question is this: Question: Is there a definition of the limit on Euclidean $\mathbb{R}^n$ space that doesn't require variables? If not, is there simply a notion of a limit that doesn't use variables in its notation?","['multivariable-calculus', 'calculus', 'limits', 'differential-topology', 'derivatives']"
3632298,I Need help creating an intuitive answer to the sum of $1(1!) + 2(2!) + 3(3!) +\cdots+ n(n!)$,"Given the sum $1+\sum_{i=1}^n i(i)! = (n+1)!$ , is there an intuitive way to think about this sum? I understand the algebraic manipulation to get to that answer, and also how to use induction to prove the answer, but say could we use a combinatorial proof to get to the same answer? I cant seem to think of a way to think of anything that could intuitively explain this. I tried thinking about it as the number of ways to create license plates with restrictions on the number of different characters that are allowed to be used at any given spot in the license plate, whereas the right-hand side counts them all at once, but that didn't get me to an answer. Any help is greatly appreciated.","['summation', 'combinatorics', 'combinatorial-proofs', 'factorial']"
3632312,2 edge-disjoint trees in graph,"Under what conditions can we construct 2 edge-disjoint trees in a finite undirected graph? I think might be addressed by  ""On the Problem of Decomposing a Graph into $n$ Connected Factors"" by W. T. Tutte, but I can't find a free pdf of the paper online. If somebody can present the results for the case of 2 trees here, I'd really appreciate it!","['graph-theory', 'trees', 'combinatorics', 'discrete-mathematics']"
3632324,"Rudin, Riesz Representation Step X: Why do we need $|a|$?","My question has to do with the inequalities at the end. However, I will summarize the step to give some context to my question. We want to show that for some complex functional $\Lambda $ on $C_c(X) $ , that $\Lambda f \le \int_X f d \mu$ when $f$ is real. Let $K$ be the support of $f$ .  Choose $[a, b]$ to be an interval containing the range of $f$ .  choose $\epsilon > 0$ and choose $y_i$ such that $y_i - y_{i-1} < \epsilon$ and $$y_0 < a < y_1 < \cdots < y_n = b.$$ Let $E_i = \{ x : y_{i-1} < f(x) \le y_i \} \cap K$ .  Then there are open sets $E_i \subset V_i$ such that $$\mu (V_i) < \mu (E_i) + \frac{\epsilon}{n}$$ and $f(x) < y_i + \epsilon$ for $x \in V_i$ .  Then using previous steps we find functions $h_i \prec V_i$ such that $\sum h_i = 1$ .  Then $\mu (K) \le \sum \Lambda h_i$ , and $h_i f \le (y_i + \epsilon)h_i$ .  Now we get a long string of equalities and inequalities. $$\hspace{-.5 in}\Lambda f = \sum^n \Lambda (h_if) \le \sum^n (y_i + \epsilon) \Lambda h_i \\
 \hspace{.15in}= \sum ^n(|a| + y_i + \epsilon)\Lambda h_i - |a|\sum^n \Lambda h_i \\
\hspace{.55in}\le \sum^n(|a| + y_i + \epsilon)[\mu (E_i) + \frac{\epsilon}{n}] - |a| \mu(K) \\
\hspace{1.16in}= \sum^n(y_i - \epsilon)\mu(E_i) + 2\epsilon \mu(K) + \frac{\epsilon}{n}\sum^n(|a| + y_i + \epsilon) \\
\hspace{.15in}\le \int_X f d \mu + \epsilon[2 \mu(K) + |a| + b + \epsilon].$$ since this is true for any $\epsilon > 0$ , we have $$\Lambda f \le \int_x f d \mu.$$ My question is why do we inject $|a|$ into the inequality? What purpose does it serve? It appears to me that if we never introduced $|a|$ , second sum on line four would be $\frac{\epsilon}{n}\sum(y_i + \epsilon)$ which is less than $\frac{\epsilon}{n} \sum(b + \epsilon) = \epsilon(b + \epsilon.)$","['riesz-representation-theorem', 'proof-explanation', 'lebesgue-integral', 'analysis', 'functional-analysis']"
3632349,Continued Fraction using all Perfect Squares,"What is known about the infinite continued fraction $$1 + \cfrac{1}{4 + \cfrac{1}{9 + \cfrac{1}{16 + \cdots}}} $$ whose terms include all perfect squares in order? Do we have a closed form expression for the value of this number? Is it known to be transcendental, or satisfy any other interesting properties?","['number-theory', 'abstract-algebra', 'arithmetic', 'sequences-and-series', 'continued-fractions']"
3632420,"Eigenvalues of an $n\times n$ matrix where all rows are equal to $[1,2,...,n]$","I'm stuck on a problem to calculate the eigenvalues for a matrix: \begin{equation*}
A_{n,n} = 
\begin{bmatrix}
1 & 2 & 3 &\ldots& n \\
1 & 2 & 3 & \ldots& n \\
1 & 2 & 3 &\ldots& n \\
1 & 2 & 3 &\ldots& n \\
\vdots&\vdots&\vdots&\ddots&\vdots\\
1 & 2 & 3 &\ldots& n 
\end{bmatrix}
\end{equation*} Been trying to go via: \begin{equation}
|A_{n,n} - \lambda| = 0 \\
\end{equation} Not sure if QR decomposition be a better route to go? Any help would be appreciated!","['linear-algebra', 'eigenvalues-eigenvectors']"
3632426,Possible error in a STEP Paper: Matrix Group (STEP 1991 P2 Q9),"I have been doing some STEP papers and I think I found a mistake on one of the papers. The paper is from 1991 (STEP 2) and the question (Q9) goes as follows: Let $G$ be the set of all matrices of the form $
\
  \left[ {\begin{array}{cc}
   a & b \\
   0 & c \\
  \end{array} } \right]
\
$ where a,b and c are integers modulo $5$ , $a\neq0\neq b$ . Show that $G$ forms a group under matrix multiplication (which may be assumed to be associative). I've proven that the identity exists and is unique, however, I can't prove that the inverses are going to exist within G and ,moreover, that G is closed. Here are my arguments Inverses Suppose there exists $A$ such that $A \times
\
  \left[ {\begin{array}{cc}
   a & b \\
   0 & c \\
  \end{array} } \right]
\
$ $=
\
  \left[ {\begin{array}{cc}
   1 & 0 \\
   0 & 1 \\
  \end{array} } \right]
\
$ . It follows that $A=
\
  \left[ {\begin{array}{cc}
   a & b \\
   0 & c \\
  \end{array} } \right]^{-1}
\
$ $=\frac{1}{ac}
\
  \left[ {\begin{array}{cc}
   c & -b \\
   0 & a \\
  \end{array} } \right]
\
$ . The issues that arise are that $\frac{c}{ac}$ is not nesseraly an intiger for intiger c and similarly $\frac{a}{ac}$ is also not necceserally an intiger. Thus for $a\neq 1\neq c$ the inverses are not a member of $G$ . Closure Let $
\
  \left[ {\begin{array}{cc}
   a & b \\
   0 & c \\
  \end{array} } \right]
\
$ and $
\
  \left[ {\begin{array}{cc}
   d & e \\
   0 & f \\
  \end{array} } \right]
\
$ be members of $G$ . In doing matrix multiplication we get $
\
  \left[ {\begin{array}{cc}
   a & b \\
   0 & c \\
  \end{array} } \right]
\
\times
\
  \left[ {\begin{array}{cc}
   d & e \\
   0 & f \\
  \end{array} } \right]
\
=
\
  \left[ {\begin{array}{cc}
   ad & ae+bf \\
   0 & cf \\
  \end{array} } \right]
\
$ We see that if $d$ is a multiple of $5$ then $ad\equiv0 \mod{5}$ . However, if the new matrix is formed $ad$ must obey $ad \neq 0$ which it obviously doesn't. Thus it is not closed. Can someone have a look at my arguments and let me know what I have done wrong or if I am right ? Here is the link to the full paper: https://pastpapercache.blob.core.windows.net/ppppapers/step/STEP%20II%201991.pdf Thanks in advance","['matrices', 'group-theory', 'solution-verification']"
3632454,"Can we characterise regularly homotopic closed curves $a$, $b$ for which the homotopy $(t,c) \mapsto c\cdot a(t)+(1-c) \cdot b(t)$ is not regular?","For many two closed curves $c, d: \mathbb R \to \mathbb R^n$ that are regularly homotopic to each other, the map $$ \tag{1}
h: \mathbb R \times [0, 1] \to \mathbb R^n, \ 
(t, u) \mapsto (1 - u) c(t) + u \cdot d(t)
$$ is a regular homotopy between them.
An example is $c(t) := (\cos(t), \sin(3t))$ and $d(t) := (\cos(t), \sin(t))$ . Question. How can we characterise the pairs of two closed curves $c, d: \mathbb R \to \mathbb R^n$ that are regularly homotopic to each other, but for which the map $(1)$ is not regular? An example for two closed parametrised curves (unfortunately, they are the same closed curve) is $(c, 2 \pi)$ and $(d, 2 \pi)$ with $c(t) := (\cos(t), \sin(t))$ and $d(t) := \left(\cos\left(t \cdot e^{t - 2 \pi}\right), \sin\left(t \cdot e^{t - 2 \pi}\right)\right)$ , which are both the positively traversed unit circle but traversed with different ""speed"", so they are regularly homotopic, but $(1)$ is not regular for $u \approx 0.27$ and $u \approx 0.83$ (haven't calculated the exact values yet). Another example are the following curves with period $2 \pi$ $$
c(t) := (\cos(t), \sin(t)), \qquad
d(t) := (\sin(t), \cos(t)),
$$ which represent the unit circle being traversed negatively resp. positively.
They are regularly homotopic, but the homotopy $(1)$ is a straight line for $u = 1/2$ , which is not closed. Definition 1. A closed parametrised curve is a pair $(c, p)$ where $c: \mathbb R \to \mathbb R^n$ is parametrised curve with period $p$ , i.e. $c(t+p)=c(t)$ holds for all $t \in \mathbb R$ . Definition 2. A closed curve is an equivalence class of closed parametrised curves, where $(c,p) \sim (d,q)$ if and only if there exists a bijective smooth map $\phi: \mathbb R \to \mathbb R$ such that $d = c \circ \phi$ and $\phi'(t) > 0$ and $\phi(t + p) = \phi(t) + q$ hold for all for all $t \in \mathbb R$ Definition 3. Two closed curves $(c, p)$ and $(d, q)$ are regularly homotopic if there exists a regular homotopy between them, i.e. a map $$
h: \mathbb R \times [0, 1] \to \mathbb R^n
$$ such that $h(t,0) = c(t)$ and $h(t,1) = d(t)$ hold for all $t \in \mathbb R$ , all curves are regular, i.e. $\frac{\partial}{\partial t} h(t,u) \ne 0$ holds for all $t \in \mathbb R$ and all $u \in [0, 1]$ and all curves are closed, i.e. there exists a continuous map $u \mapsto \tau_u$ such that $h(t + \tau_u, u) = h(t,u)$ holds for all $t \in \mathbb R$ and all $u \in [0, 1]$ and we have $\tau_0 = p$ and $\tau_1 = q$ .","['plane-curves', 'homotopy-theory', 'differential-geometry']"
3632473,Partitioning a Cartesian Product Yields Sierpiński Triangle... ish?,"I wanted to find an efficient method to partition  a Cartesian product of $n$ sets $S_i$ of varying sizes into maximum size subsets that are defined by all tuples in the partition differing in at least $k$ places. I failed to come up with a non-bruteforce algorithm. But I was pretty sure there should be a better approach. So I computed a few partitions by brute force and visualized them to maybe get a new idea. I found that for the case where $k = 2$ using 3 sets of size 100 each, the result seems to be something like what is apparently called tetrix , a 3-D Sierpiński Triangle. I don't understand this result and it did not help me in coming up with an algorithm. But since this is a fractal I am now even more sure there needs to be a non-bruteforce computation to generate this fractal and maybe also a generalization for all possible setups of sets $S_i$ and $k$ . Can anyone explain the result to me and maybe knows how to non-brutoforce generate these partitions? This is the code: import seaborn as sns, numpy as np, pandas as pd, random
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from tqdm import tqdm

L = [list(range(100)), list(range(100)), list(range(100))]

from itertools import product
def n_distinct_product(iterables, n_distinct):
    elements_n_distinct = set()
    pr = list(product(*iterables))
    for s1 in tqdm(pr):
        if all(sum([w1 != w2 for w1, w2 in zip(s1, s2)]) >= n_distinct
               for s2 in elements_n_distinct if s1 != s2):
            elements_n_distinct.add(tuple(s1))
    return elements_n_distinct


k = 2
data = n_distinct_product(L, k)
data = list(data)

data = np.vstack(data)

df = pd.DataFrame(data=data, columns = ['x', 'y', 'z'])


sns.set_style(""whitegrid"", {'axes.grid' : False})

fig = plt.figure(figsize=(6,6))

ax = Axes3D(fig)


g = ax.scatter(df.x, df.y, df.z, c=df.z, marker='.', depthshade=True, cmap='Paired')
ax.set_xlabel('X Label')
ax.set_ylabel('Y Label')
ax.set_zlabel('Z Label')

# produce a legend with the unique colors from the scatter
legend = ax.legend(*g.legend_elements(), loc=""lower center"", title=""X Values"", borderaxespad=-10, ncol=4)
ax.add_artist(legend)

plt.show()

sns.pairplot(data=df)

sns.scatterplot('x', 'y', data=df, hue='z') Description of algorithm: Define a set $M = \{\}$ . Iterate over Cartesian product in lexicographic order, starting with $(0,0,0)$ . For each tuple: If it differs from all tuples in $M$ in at least $k$ places, add it to $M$ .","['set-partition', 'number-theory', 'equivalence-relations', 'discrete-mathematics', 'fractals']"
3632479,Exterior derivative of a restricted 1-form,I had a question about the exterior derivative of a restricted 1-form. Let S be a surface in 3-space. Suppose we define a 1-form on an open set containing the surface such that the restricted 1-form is zero. How do you then prove that the restricted exterior derivative is zero? Thanks.,"['differential-topology', 'differential-geometry']"
3632511,Long exact sequence of sheaf cohomology from the normal bundle of $\mathbb{P}^1$ in $\mathbb{P}^2$,"Let $i: \mathbb{P}^1 \to \mathbb{P}^2$ denote the inclusion $[x_0,x_1] \mapsto [x_0,x_1,0]$ . In other words, we identify $\mathbb{P}^1$ as the subvariety defined by ${x_2 = 0}$ in $\mathbb{P}^2$ . The inclusion induces the following short exact sequence $$0 \to T_{\mathbb{P}^1} \to i^*T_{\mathbb{P}^2} \to N_{\mathbb{P}^1/\mathbb{P}^2} \to 0,$$ where the normal bundle $N_{\mathbb{P}^1/\mathbb{P}^2} \cong \mathcal{O}_{\mathbb{P}^2}(1)|_{\mathbb{P}^1}$ .
Then we have a long exact sequence of sheaf cohomology $$0 \to H^0(\mathbb{P}^1,T_{\mathbb{P}^1}) \to H^0(\mathbb{P}^1,i^*T_{\mathbb{P}^2})\to H^0(\mathbb{P}^1,\mathcal{O}_{\mathbb{P}^2}(1)|_{\mathbb{P}^1}) \to H^1(\mathbb{P}^1,T_{\mathbb{P}^1}) \to \cdots.$$ It's known that $H^1(\mathbb{P}^n,T_{\mathbb{P}^n}) = 0$ in general. So the above sequence reduces to $$0 \to H^0(\mathbb{P}^1,T_{\mathbb{P}^1}) \to H^0(\mathbb{P}^1,i^*T_{\mathbb{P}^2})\to H^0(\mathbb{P}^1,\mathcal{O}_{\mathbb{P}^2}(1)|_{\mathbb{P}^1}) \to 0.$$ I want to describe the above short exact sequence explicitly over the affine open set $U_0 = \{[x_0 \neq 0,x_1]\}$ with coordinate $x = x_1/x_0$ . I know $H^0(\mathbb{P}^1,T_{\mathbb{P}^1})$ has a basis of three vector fields $$\frac{\partial}{\partial x}, x \frac{\partial}{\partial x}, x^2 \frac{\partial}{\partial x}.$$ I also know $H^0(\mathbb{P}^1,\mathcal{O}_{\mathbb{P}^2}(1)|_{\mathbb{P}^1})$ has a basis $x_0,x_1$ ( $x_2$ is not one of them because it vanishes on $\mathbb{P}^1$ ). Over $U_0$ , they become two functions $$1,x.$$ But I'm not sure how to write global sections from $H^0(\mathbb{P}^1,i^*T_{\mathbb{P}^2})$ . How to identify $i^*T_{\mathbb{P}^2}$ as a vector bundle of rank $2$ on $\mathbb{P}^1$ so we can write down a basis of $H^0(\mathbb{P}^1,i^*T_{\mathbb{P}^2})$ ?","['exact-sequence', 'algebraic-geometry', 'sheaf-cohomology']"
3632517,Fourier transform of measure on $\mathbb{T}$,"Following is problem 8.39 from Folland. $\mu$ is a positive Borel measure on $\mathbb{T}=[0,1)$ with $\mu(\mathbb{T})=1$ , then for its Fourier transform $\hat{\mu}(k)=\int_{\mathbb{T}}e^{-2\pi ikx}d\mu(x)$ , prove that $|\hat{\mu}(k)|<1$ for any $k \neq 0$ unless $\mu$ is a linear combination of point mass. I don't even know how to start with. I was trying to use the Radon-Nikodym theorem and thus decompose the measure w.r.t Lebesgue measure but fail to push it further. Also, I can't see there's any general way to conclude to the point mass case... Any comment and help is appreciated.","['fourier-analysis', 'fourier-transform', 'real-analysis']"
3632572,"a problem between $C[0,1]$ subspaces.","Given $X=\left\{x\in C[0,1], x(0)=0\right\}$ and $Y=\big\{y\in X, \int_{0}^{1}y(t)dt=0\big\}$ , $\|x\|=\sup\{|x(t)|,t\in [0,1]\}$ . Prove that, $\forall x\in X$ with $\|x\|=1 $ exists $y_0\in Y$ such that $\|x-y_0\|<1$ . I tried to prove by contradiction: $\exists x_0\in X, \|x_0\|=1, \|x_0-y\|\geq1, \forall y\in Y$ . Then considering $d(x_0,Y)\geq 1$ and I tried to consider a sequence in $Y$ ... But I don't know how to finish it... I also tried to use Riesz's lemma (because $Y$ is closed) and try a sequence $x_n\in X$ that $\|x_n-y\|\geq 1-1/n$ , but I think that doesn't help...","['functional-analysis', 'metric-spaces']"
3632573,Understanding limit superior and limit inferior for sets,"I am currently following Jürgen Elstrodt ""Maß- und Integrationstheorie"", 7th edition. On p. 9 he defines the limes superior $\overline{\lim}_{n \rightarrow \infty}$ and limes inferior $\underline{\lim}_{n \rightarrow \infty}$ by Def. Let $(A_{n})_{n \geq 1}$ be a sequence of subsets of a set $X$ . We define the limes superior by $$\overline{\lim_{n \rightarrow \infty}} A_{n} := \{ x \in X : x \in A_{n} \text{ for infinitely many } n \in \mathbb{N} \setminus \{ 0 \} \},$$ and the limes inferior by $$\underline{\lim_{n \rightarrow \infty}} A_{n} := \{ x \in X :  \text{ there exists } n_{0}(x) \in \mathbb{N} \setminus \{ 0 \} \text{, such that } x \in A_{n} \text{ for all } n \geq n_{0}(x) \}.$$ How should I think of these concepts? Is there some intuition behind it? According to just the words (and what I know about $\sup$ , $\inf$ , and $\lim$ ) the limes superior would seem to be something like a limit from above and in particular, the least upper bound limit. And similarly, the limes inferior would then be the greatest lowest bound limit. Then, the following three relations are presented without proof $$\overline{\lim_{n \rightarrow \infty}} A_{n} = \bigcap_{n = 1}^{\infty} \bigcup_{k = n}^{\infty} A_{k}, $$ $$\underline{\lim_{n \rightarrow \infty}} A_{n} = \bigcup_{n = 1}^{\infty} \bigcap_{k = n}^{\infty} A_{k}, $$ $$\underline{\lim_{n \rightarrow \infty}} A_{n} \subseteq \overline{\lim_{n \rightarrow \infty}} A_{n}.$$ So I tried proving these and I am uncertain whether I am on the right track or even perhaps if I completed the proof: We prove $\overline{\lim_{n \rightarrow \infty}} A_{n} = \bigcap_{n = 1}^{\infty} \bigcup_{k = n}^{\infty} A_{k}$ by showing $\overline{\lim_{n \rightarrow \infty}} A_{n} \subseteq \bigcap_{n = 1}^{\infty} \bigcup_{k = n}^{\infty} A_{k}$ and $\overline{\lim_{n \rightarrow \infty}} A_{n} \supseteq \bigcap_{n = 1}^{\infty} \bigcup_{k = n}^{\infty} A_{k}$ . We prove first $\overline{\lim_{n \rightarrow \infty}} A_{n} \subseteq \bigcap_{n = 1}^{\infty} \bigcup_{k = n}^{\infty} A_{k}$ . Let $x \in \overline{\lim_{n \rightarrow \infty}} A_{n}$ . This means that $x \in X$ such that $x \in A_{n}$ for infinitely many $n \in \mathbb{N} \setminus \{ 0 \}$ . We want to show that $x \in \bigcap_{n = 1}^{\infty} \bigcup_{k = n}^{\infty} A_{k}$ , which is true if for all $n \in \mathbb{N} \setminus \{ 0 \}$ we have $x \in \bigcup_{k = n}^{\infty} A_{k}$ , which in turn is true if for all $n \in \mathbb{N} \setminus \{ 0 \}$ there exists $k \geq n$ (or $k = n$ ? or is it $k \in \{ n, n + 1, n + 2, \ldots \}$ ?) such that $x \in A_{k}$ . And this is where I am stuck on this part. We have that $x \in X$ such that $x \in A_{n}$ for infinitely many $n \in \mathbb{N} \setminus \{ 0 \}$ and we want to show that for all $n \in \mathbb{N} \setminus \{ 0 \}$ there exists $k \geq n$ (or $k = n$ ? or is it $k \in \{ n, n + 1, n + 2, \ldots \}$ ?) such that $x \in A_{k}$ . Perhaps these statements are the same and I am just not understanding it (linguistically or logically). Now, for the other inclusion, i.e. we show $\overline{\lim_{n \rightarrow \infty}} A_{n} \supseteq \bigcap_{n = 1}^{\infty} \bigcup_{k = n}^{\infty} A_{k}$ . Let $x \in \bigcap_{n = 1}^{\infty} \bigcup_{k = n}^{\infty} A_{k}$ . This means that for all $n \in \mathbb{N} \setminus \{ 0 \}$ there exists $k$ (again here I am confused as to $k \geq n$ or $k = n$ ) such that $x \in A_{k}$ . We want to show that $x \in \overline{\lim_{n \rightarrow \infty}} A_{n}$ , which is true if $x \in X$ such that $x \in A_{n}$ for infinitely many $n \in \mathbb{N} \setminus \{ 0 \}$ . So now we have that for all $n \in \mathbb{N} \setminus \{ 0 \}$ there exists $k$ (again here I am confused as to $k \geq n$ or $k = n$ ) such that $x \in A_{k}$ and we want to show that $x \in \overline{\lim_{n \rightarrow \infty}} A_{n}$ , which is true if $x \in X$ such that $x \in A_{n}$ for infinitely many $n \in \mathbb{N} \setminus \{ 0 \}$ . I believe that if I understand how to prove the first relation, then I will be able to do the second one.","['measure-theory', 'solution-verification', 'real-analysis']"
3632574,Equivalence of measures,"Let $\nu$ and $\mu$ be two measures defined on a space $\mathcal{X}$ . It is given that they are equivalent, i.e., $\nu \ll \mu$ and $\mu \ll \nu$ . I am interested in finding necessary and sufficient conditions to conclude that there exist constants $\alpha, \beta >0$ such that $\alpha \mu(X) \leq \nu(X) \leq \beta \mu(X)$ holds for all measurable sets $X \in \mathcal{X}$ . It is clear to me that equivalence is a necessary condition but I am not sure of its sufficiency. If it is sufficient, then how can one go about finding the constants $\alpha$ and $\beta$ ? An obvious sufficient condition is $ \alpha \leq \frac{d \nu}{d \mu} \leq \beta$ , i.e., the Radon-Nikodym derivative is bounded from both above and below. However, I am not sure if this is necessary. I believe the answer is related to certain properties of the derivative but I can't figure out the best way to state it. If it helps, you can assume that the measures are finite and $\mu$ is the Lebesgue measure (with $\mathcal{X}$ being a bounded set, say a unit ball). Any references or hints would be appreciated. Thanks!",['measure-theory']
3632681,"Is $\int_{\mathbb{R}} |F(x)-G(x)|^2(f(x)-g(x))dx =0$ true, for $f,g$ probability densities and $F,G$ the corresponding distribution functions?","I came across the following inequality while doing a problem. I guess it should be true, but I am not able to prove it. Here is the situation. Let $f$ and $g$ be two probability densities on $\mathbb{R}.$ Let $F$ and $G$ be the distribution function corresponding to $f$ and $g,$ respectively. I am looking at the following integral $$I=\int\limits_{\mathbb{R}} |F(x)-G(x)|^2(f(x)-g(x))dx.$$ I guess that $I=0.$ The particular case I came across has further simplifications. For example, in my case, $g$ is uniform density on $[0, 1]$ and $f$ is a density supported on $[0, 1].$ The problem, in that case, reduces(?) to $$\int\limits_{0}^{1}|F(x)-x|^2(f(x)-1)dx=0.$$ Unfortunately, I could not come up with a proof. Any ideas?","['analysis', 'probability-theory', 'probability', 'real-analysis']"
3632685,"Similar concept for ""nilpotent"" matrix but gives the identity and not 0?","Is there a term for a square matrix $E$ such that $E^k=I$ for some positive integer $k$ ?
To provide context: I was experimenting with permutation matrices and discovered that they satisfy the interesting property above. I have not proved/disproved this claim, I am looking for some hints and suspect that it has a nice name. Initially I thought it would be called ""unipotent"" (like how ""nilpotent"" is defined) but that's not it.","['matrices', 'nilpotence', 'permutations']"
3632773,How to prove that the first $43$ digits of $43!$ is a prime number?,"I saw this problem posted by someone in twitter , and this really just boggled up my mind . Check this here:- https://twitter.com/fermatslibrary/status/1235215179115360257/photo/1 Now i am really thinking , how can I ever prove this fact ?
Any ideas ? Also the next number n by which the first $n$ digits of $n!$ is prime is $93$ .","['number-theory', 'elementary-number-theory', 'prime-numbers']"
3632915,Derivative of row-wise softmax matrix w.r.t. matrix itself,"Define matrix $\mathbf{M}' \in \mathbb{R}^{n \times k}$ as the result of the row-wise softmax operation on matrix $\mathbf{M} \in \mathbb{R}^{n \times k}$ . Hence, $$\mathbf{M}'_{ij} = \frac{\exp{\mathbf{M}_{ij}}}{\sum\limits_{b=1}^k \exp{\mathbf{M}_{ib}}}.$$ Now, I look at the derivative of a scalar function, e.g., the Frobenius norm, with respect to $\textbf{M}$ , namely $$
\frac{\partial E}{\partial \textbf{M}} = \frac{\partial \left\Vert \textbf{X} - \textbf{M}'\textbf{H}\right\Vert_F}{\partial \textbf{M}}.
$$ I don't have any problem calulating the derivative of the above function w.r.t. $\textbf{M}'$ . However, I am interested in finding the derivative w.r.t. $\textbf{M}$ , which means that I somehow have to deal with the row-wise softmax operation. Since softmax is a vector function, but I am interested in finding the derivative w.r.t. the whole matrix $\textbf{M}$ at once, I don't know how to deal with it best. Do I need to calculate the derivative w.r.t. each vector $\textbf{M}_{i:}$ seperately? Also, the derivative of the softmax would yield a Jacobian matrix of dimensionality $k \times k$ . Getting one Jacobian for each row vector $\textbf{M}_{i:}$ seems to mess up the dimensionality, assuming I would need to concatenate all those Jacobians... I am not sure where my mistake is. However, it feels like I am stuck. It would be great if you could help me out :) Thanks in advance and best regards.","['scalar-fields', 'matrix-calculus', 'derivatives']"
3632927,Research the roots of complex equation $x^n(x-2) = 1$,"I need to prove that only one of roots of equation $ x^n(x-2) = 1 \:n>1 $ is lying outside unit circle. By researching how function on the left behave on real line i have found that root exists between 2 and 3. Then i tried really hard to imagine how function $ x^n $ looks like and how multiplying on $ x-2 $ or even $x - n$ would deform it but still no luck at it and I had spent on it so much time that I simply can't spend any more. The only ""results"" i had got are something trivial, like multiplying on $x-n$ will compress values inside unit circle centered at n. So, I'm yet again asking for your help and tips.",['complex-analysis']
3632933,"Sufficient conditions for square map to be group isomorphism, with basic tools","Let G be a finite abelian group. I am asked to find sufficient conditions for $$ \begin{array}{cccc} 
\alpha: & G & \rightarrow & G & \\
& g & \mapsto & g^{2}
\end{array} $$ to be a group isomorphism. I already know that if the order of the group is odd, then $ \alpha $ is injective, and thus bijective. Is there any way to prove this without using cyclic groups and Lagrange's Theorem? I am supposed to use just basic things like definitions and equivalence relations.","['group-theory', 'abstract-algebra', 'group-isomorphism']"
3632948,A sufficient condition for differentiability of $f:\mathbb{R}^n\rightarrow\mathbb{R}$,"There is a theorem in my textbook which says: Theorem: consider the function $f:\mathbb{R}^2\to\mathbb{R}$ such that both of its partial derivatives at a point exist and at least one of them is continuous, then $f$ is differentiable at that point. I want to know whether the theorem is true in general for a function $f:\mathbb{R}^n\to\mathbb{R}$ , i.e whether the following is true or not: Consider the function $f:\mathbb{R}^n\to\mathbb{R}$ such that all its partial derivatives exist at a point and at least one of these partial derivatives is continuous at that point,then $f$ is differentiable at that point.","['multivariable-calculus', 'calculus', 'real-analysis']"
3632955,Is Hausdorff measure regular?,"Let $E$ be a Borel set in $\mathbb{R}^n$ and $\mathcal{H}^k$ be a $k$ -dimensional Hausdorff measure.
Is it true that $\mathcal{H}^k (E)=\sup \{\mathcal{H}^k (S)\mid S\subset E \text{ is compact}\}$ ?","['measure-theory', 'real-analysis']"
3632967,Question about inertia groups and unramified extensions,"Let $K$ be a number field, and $v$ a finite place. If $\bar{K}$ is a separable closure of $K$ , then in $G_K=\text{Gal}(\bar{K}/K)$ we can find the decomposition group of (a place over) $v$ , which is isomorphic to the Galois group of $\bar{K_v}/K_v$ , with $K_v$ the completion at $v$ . It is well know that the fixed field of the inertia $I_v$ in $\bar{K_v}$ is the maximal unramified extension of $K_v$ . Is it also true that the fixed field of the inertia in $\bar{K}$ is the maximal extension of $K$ unramified at $v$ ? I think this is true, since we can easily move to the finite case where is true, but also I could appreciate a check. After this, if we consider the maximal extension of $K$ unramified at $v$ and $v'$ , with $v\ne v'$ , then it is the intersection of the maximal extension unramified at $v$ with the one unramified at $v'$ (it is true? It seems to me obvious), therefore, by Galois correspondence, the product of the inertia $I_vI_{v'}$ is the group corresponding to that field. But what happens if we consider the maximal extension unramified outside a finite set of places, so unramified at an infinite set of places? The infinite intersection would correspond to an infinite product of subgroups, which of course make no sense. So have we to compute it with, maybe, inverse limit, or something like this? (My final goal is to understand a proof in Rubin's book Euler systems: he proved that, given a Galois representation $T$ with coefficients in the valuation ring $O$ of a finite extension of $\mathbb{Q}_p$ , and a finite set of primes $\Sigma$ containing all primes where $T$ ramifies, primes above $p$ and infinite places, then the Selmer group $S^{\Sigma}(K,T)$ is equal to $H^1(K_{\Sigma}/K,T)$ , where $K_{\Sigma}$ is the maximal extension unramified outside $\Sigma$ . The proof is the following: $$\begin{split}S^{\Sigma}(K,T)&\overset{(1)}{=}\ker \left(H^1(K,T)\to \prod_{v\not\in\Sigma} H^1(K_v,T)/H_f^1(K_v,T)\right)=
\\&\overset{(2)}{=}\ker\left(H^1(K,T)\to\prod_{v\not\in\Sigma}\text{Hom}(I_v,T)\right)=\\&\overset{(3)}{=}\ker\left(H^1(K,T)\to H^1(K_{\Sigma},T)\right){=}H^1(K_{\Sigma}/K,T).
\end{split}$$ (1) is the definition. In (2) we need $\text{Hom}(I_v,T)^{Fr}$ , the fixed points of Frobenius, but not a big deal, since we want the kernel,so we can enlarge the codomain. But my big problem is in (3), with which the question is related).","['number-theory', 'galois-cohomology', 'algebraic-number-theory']"
3633029,Prove that this set is a ring,"Let $X$ be a set and $\mathcal{B}$ a subset of the set of maps from $X$ to $\mathbb{Z}_2$ . For any subset $A\subset X$ we define the characteristic mapping of A as the mapping $\chi_A:X\rightarrow \mathbb{Z}_2$ given by $\chi_A(x)=1$ if $x\in A$ and $\chi_A(x)=0$ if $x\notin A$ . Give $\mathcal{B}$ the operations: $\odot: \mathcal{B} \times \mathcal{B} \rightarrow \mathcal{B}$ given by $(\chi_A \odot \chi_B)(x)=\chi_A(x) \cdot \chi_B(x)$ and $\oplus : \mathcal{B} \times \mathcal{B} \rightarrow \mathcal{B}$ given by $(\chi_A \oplus \chi_B)(x)=\chi_A(x) + \chi_B(x) - \chi_A(x) \cdot \chi_B(x)$ . Show that $\mathcal{B}$ whith these operations is a ring . As a part of this exercise, we have to see that $(\mathcal{B},\oplus)$ is a group. I have considered the map $\chi_\emptyset$ as the neutral element for the addition $\oplus$ since $\chi_\emptyset (x)=0$ $\forall x \in X$ . But I have a problem when I try to find an opposite element because $\chi_A(x) + \chi_B(x) - \chi_A(x) \cdot \chi_B(x)$ has to be $0$ but it only happens if $x\in A^c \cap B^c$ . So, what is our candidate for be an opposite of $\chi_A$ and how can I denote it? Is it correct to consider the map $\chi_\emptyset$ as the neutral element?","['abstract-algebra', 'characteristic-functions']"
3633096,Number of ways we can paint the fence pickets,"Problem: Find the number of ways we can paint the fence containing n vertical pickets with 4 different colors. The problem is that, before we start painting, we adjust the buckets of colors in one row. Each time we paint a picket, the next picket can only be painted with colors from the neighbor buckets. Example: If we have colors red blue green yellow, and we start painting with blue, the next picket can only be painted with red or green. How I tried solving the problem I tried counting the overall possible ways to paint the fence via stars and bars method using theorem two $$\binom{n+k-1}{k-1}$$ where n is the number of pickets and k is number of paints and then dividing the result by 2. The correct answer: We have to use the following recurrence relation to solve this problem: $$f(n) = f(n-1) + f(n-2)$$ Why do we need to do this? Can someone explain how we came to this answer?","['combinatorics', 'recurrence-relations', 'discrete-mathematics']"
3633100,Conditional expectation $\mathbb{E}(X|\mathcal{G})$ where $\mathcal{G}$ is $\sigma$-algebra of countable and co-countable sets.,"We have $\left((0,1), \mathcal{B}(0,1), \lambda|_{(0,1)} \right)$ and r.v. $X(\omega) = \cos (\pi \omega)$ and $\mathcal{G} = \{A \subset (0,1): \text{A is countable or $A^C$ is countable}\}$ . What is the $\mathbb{E}(X|\mathcal{G})$ ? We have $\lambda(A) \in (0, 1)$ due to the properties of $\lambda$ -measure. Can I just assume that $\mathbb{E}(X|\mathcal{G}) = \mathbb{E}(X) $ and then just calculate that as $ \int_\Omega X d \lambda = \int_0^1cos(\pi x)dx = 0$ ? If yes, why? This is related to the this SE question but I could not get the grasp of the answer there.","['measure-theory', 'probability-theory', 'probability']"
3633109,Is a characteristic polynomial we consider in Linear Algebra a polynomial or a polynomial function?,"In Linear Algebra, we consider characteristic polynomials. Is a characteristic polynomial we consider in Linear Algebra a polynomial or a polynomial function? I think it is a polynomial function. I am reading ""Introduction to Linear Algebra"" (in Japanese) by Kazuo Matsuzaka. In this book, the characteristic polynomial of a linear map $F$ is defined by $\det(A - \lambda I)$ , where $A$ is a matrix which represents $F$ . And in this book, the author defines a determinant only for a matrix whose elements belong to a some field $K$ . If $\det(A - \lambda I)$ is a polynomial, then the elements of $A - \lambda I$ are polynomials too. But the author didn't define a determinant for a matrix whose elements are polynomials.","['definition', 'linear-algebra', 'polynomials', 'characteristic-polynomial']"
3633155,geodesic line on intersecting planes,"I have a problem and I don't understand how to solve it, please help! Prove that if two surfaces in $R^3$ intersect along a curve that is geodesic on both surfaces, and the tangent planes to the surfaces at any point of the curve do not coincide (in this situation, the surfaces are said to intersect transversely), then this curve is straight.",['differential-geometry']
3633183,"Relation between Kuratowski's and Wagner's theorem, re: Wikipedia","On the Wikipedia page for Wagner's theorem , we find the following description of the relation to Kuratowski's Theorem : Wagner published both theorems in 1937, subsequent to the 1930 publication of Kuratowski's theorem, according to which a graph is planar if and only if it does not contain as a subgraph a subdivision of one of the same two forbidden graphs $K_5$ and $K_{3,3}$ . In a sense, Kuratowski's theorem is weaker than Wagner's theorem: a subdivision can be converted into a minor of the same type by contracting all but one edge in each path formed by the subdivision process, but converting a minor into a subdivision of the same type is not always possible. However, in the case of the two graphs $K_5$ and $K_{3,3}$ , it is straightforward to prove that a graph that has at least one of these two graphs as a minor also has at least one of them as a subdivision, so the two theorems are equivalent. Am I crazy, or is the ""In a sense, Kuratowski's theorem is weaker than Wagner's theorem"" assertion completely backwards? Of course, since both theorems are if-and-only-ifs, there are ""two directions"" to each theorem. But in both cases, one direction is more or less trivial: it is obvious that a planar graph cannot have a non-planar minor nor a subdivision of a non-planar graph as a subgraph. The nontrivial direction in either case is to show that if a graph is non-planar then it must have a [ $K_5$ or $K_{3,3}$ minor] / [subdivision of $K_5$ or $K_{3,3}$ as a subgraph]. Since, as the paragraph explains, any subdivision of $H$ as a subgraph can immediately be converted to an $H$ minor by contracting the ""extraneous"" vertices on each edge, it would seem that showing a graph has a subdivision of $K_5$ or $K_{3,3}$ as a subgraph is strictly harder than showing it has a $K_5$ or $K_{3,3}$ minor. Is there some basic logical point I'm getting tripped up by here? At any rate, as the quoted paragraph explains, I'm sure it's not hard to deduce one of these theorems from the other. But it would be good to correct Wikipedia on this if I am not mistaken. EDIT : Looking at the history of the page, about a year ago the paragraph in question was edited to say ""weaker"" where it had said ""stronger."" I think this is the culprit: an erroneous edit by someone who didn't understand. I'll change it back to ""stronger"" now.","['graph-theory', 'combinatorics', 'planar-graphs']"
3633195,On a result of Mazur about convergence in locally convex spaces,"My question is about the following result from Simon (2011) (Theorem 5.3). Let $X$ be a locally convex space and $Y$ its space of continuous [linear] functionals. Let $\{x_n\}$ be a sequence in $X$ with $x_n \to x_\infty$ in the $\sigma(X,Y)$ -topology. Then, $$ x_\infty = \bigcap_n \mathrm{cch}(\{x_m\}_{m\ge n}). $$ Here, $\sigma(X,Y)$ is the weak topology on $X$ with respect to $Y$ . For a set $A$ , $\mathrm{cch}(A)$ is the closed convex hull of $A$ . Note that $X$ may be endowed with a topology stronger than the weak topology (subject to the requirement that this topology gives $Y$ as its dual). The proof establishes fairly quickly that $x_\infty \in \cap_n \mathrm{cch}(\{x_m\}_{m\ge n})=:A$ . It is, however, silent on the (non-)existence of other points in $A$ . Thus, my question . How does one show that $x_\infty$ is the only member of $A$ ? For reference, here is the proof from the text. Let $C_n=\mathrm{cch}(\{x_m\}_{m\ge n})$ . If $x_\infty \notin C_n$ , there exists $y\in Y$ such that $ \langle y,x_\infty \rangle > \sup_{x\in C_n} \langle y,x\rangle \ge \sup_{m \ge n} \langle y, x_m \rangle$ . This is incompatible with $\langle y, x_n \rangle \to \langle y, x_\infty \rangle$ . If $X$ were a Fréchet space, then the result might follow from a version of Cantor's Intersection Theorem. However, $X$ may not be metrisable. Moreover, showing that the sequence $\{C_n\}$ has vanishing diameter might require that $x_n \to x_\infty$ in the original topology, and this need not be the case.","['weak-convergence', 'topological-vector-spaces', 'functional-analysis', 'locally-convex-spaces', 'convex-analysis']"
3633217,Solution of $f(x) = \mu x f'(x) + \lambda x f''(x)$,"While there is a simple solution to the Euler equation $f(x) = \mu x f'(x) + \lambda x^2 f''(x)$ as pointed out in my previous question (see here ), the one discussed here seems much more challenging. I don't think there is a simple solution for the general case, but the case $\mu = 1$ leads to an interesting result. I was wondering if the result I am presenting here (see below) is correct. I proceeded as follows: Step 1 Let $f(x) = x\phi(x)$ . Step 2 This leads to $(x+2\lambda) \phi'(x) + \lambda x \phi''(x) = 0$ , that is $$\frac{d \log\phi'(x)}{dx} = -\frac{1}{\lambda}-\frac{2}{x}.$$ Step 3 Simple integration (twice) leads to $$f(x) = x\phi(x) = Ax + Bx\int \frac{1}{x^2}\cdot\exp\Big(-\frac{x}{\lambda}\Big) dx.$$ So we have the general solution if $\mu =1$ . It is trivial to verify that $f(x) = Ax$ is indeed a particular solution. Step 4 Using WolframAlpha or by other means, the solution can be expressed in terms of the exponential integral (Ei) function: $$f(x) = Ax - \frac{B}{\lambda} \Big\{ x \mbox{Ei}\Big(-\frac{x}{\lambda}\Big) + \lambda\exp\Big(-\frac{x}{\lambda}\Big)\Big\}.$$ Is this final result correct (if $\mu=1$ )? Is this a well known differential equation?","['integration', 'calculus', 'special-functions', 'ordinary-differential-equations']"
3633255,Can we reparametrise a closed curve such that its derivative looks like the original curve?,"When playing around with closed planar curves centered at the origin such as ellipses and circles in ""standard"" parametrisation (i.e. $(a \cos(t), b \sin(t))$ and period $2 \pi$ ) I noticed that they are their own derivatives.
So I asked myself for which other closed curves this holds.
For a curve like $(c, 2 \pi)$ , where $c(t) := (\cos(t), \sin(2t))$ , the derivative $c'$ obviously does not coincide with $c$ . But can we reparametrise $c$ (in a way that is its speed towards this bumps is slowed down) such that the derivative coincides with $c$ ? Another example: The derivative of unit circle in standard parametrisation coincides with the unit circle. But if we reparametrise the unit circle as $(\cos(t \cdot e^{t - 2 \pi}), \sin(t \cdot e^{t - 2 \pi}))$ with still period $2 \pi$ , the derivative doens't coincide. Another factor to consider is translated versions of curves. If a circle centered at the origin is translated to another position, its derivative will be centered at the origin. Thus my question is: Let $(c,p)$ be a closed planar curve.
  Does there exists a reparametrisation of $c$ such that $c'$ and $c$ look the same modulo translation? Definition 1. A closed parametrised curve is a pair $(c, p)$ where $c: \mathbb R \to \mathbb R^n$ is parametrised curve with period $p$ , i.e. $c(t+p)=c(t)$ holds for all $t \in \mathbb R$ . Definition 2. A closed curve is an equivalence class of closed parametrised curves, where $(c,p) \sim (d,q)$ if and only if there exists a bijective smooth map $\phi: \mathbb R \to \mathbb R$ such that $d = c \circ \phi$ and $\phi'(t) > 0$ and $\phi(t + p) = \phi(t) + q$ hold for all for all $t \in \mathbb R$","['plane-curves', 'curves', 'derivatives', 'differential-geometry']"
3633346,On the subgroup generated by a part of a group,"I am reading Undergraduate Algebra , by S.Lang. I can't understand the bold part of the following paragraph: There is a general way of obtaining subgroups from a group. Let $S$ be a subset of a group $G$ , having at least one element. Let $H$ be the set of elements of $G$ consisting of all products $x_1\cdots x_n$ such that $x_i$ or $x_i^{-1}$ is an element of $S$ for each $i$ , and also containing the unit element . Then $H$ is obviously a subgroup of $G$ , called the subgroup generated by $S$ . My thoughts: if $x$ is in $S$ , then $x\cdot x^{-1}=1$ is in $H$ , so why to explicitely require the presence of $1$ in $H$ ?",['group-theory']
3633390,Proving slight extension of the Monotone Convergence Theorem,"I am trying to do the following exercise in the Measure Theory book by Cohn Prove that the Monotone Convergence Theorem still holds if the assumption that the functions $f_1, f_2, ...$ are non-negative is dropped, and the assumption that $f_1$ is integrable is added (note that in this case the integrals of the functions $f$ and $f_2, f_3, ...$ exist, but may equal $\infty$ .) I think I was able to solve it, but it took two cases, and case $2$ seems a bit contrived. I was wondering if my solution is correct, and if there is a better way? Thank you. $\\$ $\\$ Case 1: $f$ is integrable Since $f_1 \le f_2 \le \cdots$ , the sequence $(f_n-f_1)_{n=0}^{\infty}$ is increasing, non-negative, and converges to $f-f_1$ . Now we may apply the MCT to get $$\lim_{n \to \infty} \int (f_n-f_1) = \int(f-f_1)$$ Since each $f_n$ satisfies $f_1 \le f_n \le f$ and both $f_1$ and $f$ are integrable, each $f_n$ is integrable, so we may split the integral on the left. Since $f$ and $f_1$ is integrable, we may split the integral on the right. Cancelling, we get $$\lim_{n \to \infty} \int f_n = \int f$$ Case 2: $f$ is not integrable Since $f_1$ is integrable, $\int(f_1)_- < \infty$ , so $\int (f_n)_- < \infty$ and $\int (f)_- < \infty$ . Therefore, since $f$ is not integrable, we must have that $\int f = \int f_+ = \infty$ . Now apply the MCT to the sequence $(f_n)_+$ which converges to $f+$ . We get $$\lim_{n \to \infty} \int (f_n)_+ = \int (f_n)_+ = \infty$$ Now $$\lim _{n \to \infty} \int f_n = \lim _{n \to \infty} \left[ \int (f_n)_+ - \int (f_n)_- \right ]= \infty - \lim_{n \to \infty} \int (f_n)_- = \infty.$$","['measure-theory', 'analysis', 'real-analysis']"
3633401,"Calculating determinant of a symmetric matrix where the $k$th row is given by $[a_{k-1},a_k,...,a_0,a_1,...,a_{n-(k-1)}]$","For $j = 0,...,n$ set $a_{j} = a_{0} + jd$ , where $a_{0}, d$ are fixed real numbers. Calculate the determinant of the $(n+1)\times (n+1)$ matrix $$A = \begin{pmatrix}
    a_{0}   & a_{1} & a_{2} &  \dots & a_{n}\\
    a_{1} & a_{0}  & a_{1} & \dots & a_{n-1}\\
    a_{2} & a_{1}  & a_{0} & \dots & a_{n-2}\\
    \ldots & \ldots & \ldots & \ldots & \ldots\\
    a_{n} & a_{n-1}  & a_{n-2} & \dots & a_{0} 
\end{pmatrix}.$$ How to calculate that? I haven't found any property of determinant of symmetric matrix which could help.
I've tried to use Gaussian elimination (subtracting each row from the row above it), but it didn't work Gaussian elimination(subtracting each row from the row above it)  brings to the matrix: $$\begin{pmatrix}
    -d   & d & d &  ... & d\\
    -d & -d  & d & ... & d\\
    -d & -d  & -d & .... & d\\
    \ldots & \ldots & \ldots & \ldots & \ldots\\
    a_{n} & a_{n-1}  & a_{n-2} & ... & a_{0} 
\end{pmatrix} = d^{n-1} \cdot \begin{pmatrix}
    -1   & 1 & 1 &  ... & 1\\
    -1 & -1  & 1 & ... & 1\\
    -1 & -1  & -1 & .... & 1\\
    \ldots & \ldots & \ldots & \ldots & \ldots\\
    a_{n} & a_{n-1}  & a_{n-2} & ... & a_{0} 
\end{pmatrix}$$","['matrices', 'determinant', 'linear-algebra']"
3633481,Showing that: $A$ maximal monotone $\Longleftrightarrow A^*$ monotone.,"For the purposes of my thesis, I am interested in proving the following: Let $A: D\left(A\right) \subset H \to H$ be an operator, where $H$ is a Hilbert space with $H^* = H$ . Then, the following hold: \begin{align*} A \; \text{maximal monotone} \; &\Longleftrightarrow A^*  \; \text{maximal monotone} \\ &\Longleftrightarrow A \; \text{is closed, D(A) is dense and} \; A,A^* \; \text{are monotone.} \end{align*} Edit: My question evolved around proving the equivalence: $$A \; \text{maximal monotone} \Longleftrightarrow A \; \text{is closed, D(A) is dense and} \; A,A^* \; \text{are monotone.}$$ Specifically, I was interested in the $(\Leftarrow)$ direction, which seem to trouble me a lot, whereas $(\Rightarrow)$ was pretty straightforward. After a lot of research, I found out that the proof was a rather long and hard result by Brezis and Browder. In the answer section, I provide a sketch of the proof.","['operator-theory', 'monotone-functions', 'hilbert-spaces', 'functional-analysis', 'spectral-theory']"
3633559,Checking if some infinite trigonometric sum is convergent [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question How can I check if this sum (on the natural numbers) is convergent? $$\sum_{n\mod90\neq0}\frac{\tan^2n}{n^3}$$","['trigonometric-series', 'trigonometry', 'convergence-divergence']"
3633736,Prove if $\lim_{x\to +\infty}(f(x)+f'(x))=0$ then $\lim_{x\to +\infty} f(x)=0$ [duplicate],"This question already has answers here : Show that $\lim_{x \to +\infty}\left(f(x)+f'(x)\right)=0 \Rightarrow \lim_{x \to +\infty} f(x)=0$ (5 answers) Closed 4 years ago . Let $f$ be a real function  continuously differentiable at $\Bbb R$ such that $$\lim_{x\to +\infty}(f(x)+f'(x))=0$$ prove that $$\lim_{x\to +\infty} f(x)=0$$ I tried tu use exponential function knowing that $$\frac{d}{dx}f(x)e^x=(f(x)+f'(x))e^x$$ but I got nothing.
thanks in advance for an answer or un idea","['limits', 'real-analysis']"
3633750,Discrete Math: Writing Proofs,"1) How do I prove the following: Let A = {6a + 4b ∈ Z : a, b ∈ Z} and B = {2a ∈ Z : a ∈ Z}. Show that A = B. Thank you all for the help!",['discrete-mathematics']
3633759,3-set-lemma in (naive) set theory,"By accident I came across the following curious, very general statement: Let $X$ be a non-empty set and let $f:X \to X$ be fixpoint-free (that is $f(x) \neq x$ for all $x\in X$ ). Then there are subsets $X_1, X_2, X_3 \subseteq X$ with $X_1\cup X_2\cup X_3 = X$ and $$X_i \cap f(X_i) = \emptyset$$ for $i \in \{1,2,3\}$ . A simple example shows that there cannot be a ""2-set-lemma"" : let $X = \{0,1,2\}$ and let $f$ be the fixpoint-free bijection with $0\mapsto 1\mapsto 2\mapsto 0$ . But I can't prove (or refute) the statement highlighted above. Can anyone give me a hint? I would also be glad for a reference, where did this statement appear, does it have a proper name?","['elementary-set-theory', 'functions', 'combinatorics']"
3633873,Dimension of a Component of a Topological Space,"I am reading about topological manifolds from An Introduction to Manifolds by Loring Tu (Second Edition, page no. 48). He defines topological manifolds as follows. Definition 5.1. A topological space $M$ is locally Euclidean of dimension $n$ if every point $p$ in $M$ has a neighborhood $U$ such that there is a homeomorphism $\phi$ from $U$ onto an open subset of $\mathbb R^n$ . ... ... Definition 5.2. A topological manifold is a Hausdorff, second countable, locally Euclidean space. It is said to be of dimension $n$ if it is locally Euclidean of dimension $n$ . In the next paragraph, he says, Of course, if a topological manifold has several connected components, it is possible for each
  component to have a different dimension. I understand what a component of a topological space is and also know that a component is connected as well as closed in it. But I don't understand what the dimension of a component is. My Question: What is the definition of dimension of a component of a topological space? How can we determine the dimension? Can you provide a simple example to illustrate it?","['manifolds', 'general-topology', 'differential-geometry']"
3633901,"How does the Axiom of Regularity apply to $A=\{1,2,3\}$?","I cannot understand the axiom of regularity. It says that any generic non-empty set $A$ contains an element $X$ such that $ X\cap A = \emptyset $ . How can this be true? If, for example I have a set $A = \{1,2,3\}$ , according to this axiom there must be an element inside of this set such that its intersection with $A$ is the empty set, but $1,2,3$ are part of $A$ , how can the intersection be an empty set?","['elementary-set-theory', 'foundations']"
3633931,Solution verification: Calculating expected value of sum of three unusual dice,"Problem: We have three fair dice. Their numbering is unusual, the $6$ numbers on the respective dice are: $$\text{Die}\,\,\#1:\,1,3,5,7,9,10,\quad\text{Die}\,\,\#2:\,1,2,2,3,3,3,\quad\text{Die}\,\,\#3:\,2,2,4,4,4,4.$$ We roll all three of these dice, and denote by $X$ the sum of the three numbers that are showing. Find the expected value $X$ . My Attempt: I have a hunch that the indicator approach might somehow work here but I do not see how to execute it. Therefore, I will attempt to solve the problem the hard way, by finding the probability mass function of $X$ . Note that $X$ takes values in the set $A=\{4,5,6,7,8,9,10,11,12,13,14,15,16,17\}.$ Now we have the following calculations using the independence of the die rolls and some basic counting rules $$P(X=4)=\frac{1}{6}\cdot\frac{1}{6}\cdot\frac{2}{6}=\frac{2}{6^3},\, \\P(X=5)=\frac{1}{6}\cdot\frac{2}{6}\cdot\frac{2}{6}=\frac{4}{6^3},\, \\P(X=6)=\frac{1}{6}\cdot\frac{1}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{3}{6}\cdot\frac{2}{6}+\frac{1}{6}\cdot\frac{1}{6}\cdot\frac{2}{6}=\frac{12}{6^3},$$ $$P(X=7)=\frac{1}{6}\cdot\frac{2}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{2}{6}\cdot\frac{2}{6}=\frac{12}{6^3},\, \\P(X=8)=\frac{1}{6}\cdot\frac{3}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{1}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{3}{6}\cdot\frac{2}{6}+\frac{1}{6}\cdot\frac{1}{6}\cdot\frac{2}{6}=\frac{24}{6^3},$$ $$P(X=9)=\frac{1}{6}\cdot\frac{2}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{2}{6}\cdot\frac{2}{6}=\frac{12}{6^3},\, \\P(X=10)=\frac{1}{6}\cdot\frac{3}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{1}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{3}{6}\cdot\frac{2}{6}+\frac{1}{6}\cdot\frac{1}{6}\cdot\frac{2}{6}=\frac{24}{6^3},$$ $$P(X=11)=\frac{1}{6}\cdot\frac{2}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{2}{6}\cdot\frac{2}{6}=\frac{12}{6^3},\, \\P(X=12)=\frac{1}{6}\cdot\frac{3}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{1}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{3}{6}\cdot\frac{2}{6}+\frac{1}{6}\cdot\frac{1}{6}\cdot\frac{2}{6}=\frac{24}{6^3},$$ $$P(X=13)=\frac{1}{6}\cdot\frac{2}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{2}{6}\cdot\frac{2}{6}+\frac{1}{6}\cdot\frac{1}{6}\cdot\frac{2}{6}=\frac{14}{6^3},$$ $$P(X=14)=\frac{1}{6}\cdot\frac{3}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{1}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{3}{6}\cdot\frac{2}{6}+\frac{1}{6}\cdot\frac{2}{6}\cdot\frac{2}{6}=\frac{26}{6^3},$$ $$P(X=15)=\frac{1}{6}\cdot\frac{2}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{1}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{3}{6}\cdot\frac{2}{6}=\frac{18}{6^3},$$ $$P(X=16)=\frac{1}{6}\cdot\frac{3}{6}\cdot\frac{4}{6}+\frac{1}{6}\cdot\frac{2}{6}\cdot\frac{4}{6}=\frac{20}{6^3},\, \\P(X=17)=\frac{1}{6}\cdot\frac{3}{6}\cdot\frac{4}{6}=\frac{12}{6^3}.$$ One can see that $\sum_{k=4}^{17}P(X=k)=1$ , so we have a legitimate probability mass function. Now let us calculate the expectation of $X$ . Using the definition we have $$E[X]=\sum_{k=4}^{17}k P(X=k)=\frac{1}{6^3}[4\cdot2+5\cdot4+6\cdot12+7\cdot12+8\cdot24+9\cdot12+10\cdot24+11\cdot12+12\cdot24+13\cdot14+14\cdot26+15\cdot18+16\cdot20+17\cdot12]=11.5$$ Could anybody please give me some feedback about my approach? If my exposition is unclear, please let me know and I will do my best to improve it. In addition, if anyone has a simpler way of tackling the problem, any hints on how to go about such a method would be much appreciated. Thank you very much for your time and feedback.","['expected-value', 'solution-verification', 'probability']"
3633949,Prove Looped line is Hausdorff,"Definition of Looped line At each point $x$ of the real line other than the origin, the basic neighborhoods of $x$ will be the usual open intervals centered at $x$ . Basic neighborhoods of the origin will be the sets $(-\epsilon,\epsilon)\cup(-\infty,-n)\cup(n,\infty)$ for all possible  choices $\epsilon >0$ and $n \in \mathbb{N}$ . This gives a topology on the line. Problem . Prove the looped line is Hausdorff. I know, by definition of Hausdorff space, with different points $x,y$ and differents of zero, we can disjoint open. But when, for example, $x=0$ there's no exists a open of $y$ such that both are disjoints. Could you guide me to solve this problem?",['general-topology']
3633960,Quartic polynomials of a complex variable,"I want to answer the following: The equation $$
    a \bigg(z + \frac{1}{z}\bigg)^2
    + b \bigg(z + \frac{1}{z}\bigg)
    + c = 0
$$ has four solutions. Find which quartics can be put in this form after apply a linear change of vars.","['complex-analysis', 'complex-numbers']"
3633982,Curvature of orthogonal distribution in a Riemannian principal space,"Let $P \to M$ be a principal $G$ -bundle and $\langle\cdot,\cdot\rangle$ be a $G$ -invariant Riemannian metric on $P$ . We have an Ehresmann connection for this bundle defined by $${\rm Hor}_p(P) \doteq {\rm Ver}_p(P)^\perp,$$ for any $p \in P$ . So the projection $TP \to {\rm Ver}(P) \to \mathfrak{g}$ defines a connection $1$ -form and so we may consider the associated curvature $2$ -form $\Omega = {\rm D}\omega$ , where ${\rm D}$ stands for the exterior covariant derivative given by the connection. I'd expect $\Omega$ to be somehow related to the Levi-Civita connection $\nabla$ of $\langle\cdot,\cdot\rangle$ , or to the $\nabla$ -horizontal connection on the vector bundle $TP\to P$ . In a very broad sense, I think my question is: how to compute $\Omega$ ? I guess I'm just a bit stumped about how to concretely use the definition of ${\rm Hor}_p(P)$ as an actual orthogonal complement. I'd like to see how this particular situation is treated or, if you know a reference I can look at, I'd be happy enough (I have already skimmed through Kobayashi and Nomizu, before you suggest it -- unless I actually missed it and you have a specific page in mind). Thanks.","['principal-bundles', 'connections', 'riemannian-geometry', 'differential-geometry']"
3634043,Riemann-Hurwitz formula $\chi(X)=d(F)\cdot\chi(Y)-R_F$,"I read the book of Miranda on Algebraic curves and Riemann surfaces, I read the proof of Riemann-Hurwitz formula page 52-53. I mean for holomorphic map $F:X\to Y$ between Riemann Surfaces, we have $$\chi(X)=d(F)\cdot\chi(Y)-R_F$$ with $\chi(X)$ is the Euler characteristic and $d(F)$ is the degree of $F$ and $R_F=\sum_{p \in X} (e_p(F)-1)$ . This result is proved in three steps: First, we prove the set of ramification points is finite (easy), so $R_F$ is a finite sum. And we take a triangulation of $Y$ , such that each branch point of $F$ is a vertex. I don't understand this, is this because any triangle is triangulated? I want to prove it formally. Step 2: Assume there are $v_Y$ vertices, $e_Y$ edges, and $t_Y$ triangles. Lift this triangulation to $X$ via the map $F$ (how we can do it?), and there are $v_X$ (resp. $e_X , t_X$ ) vertices (resp. edges, triangles) on $X$ . Since there are no ramification points over the general point of any triangle, each triangle of $Y$ lift to $\deg(F)$ triangle in $X$ (why, really two days to understand that and I can't). Thus $t_X = t_Y . d(F)$ Similarly $e_X =e_Y .d(F)$ . Now fix a vertex $q\in Y$ . The number of preimages of $q$ in $X$ is $F^{-1}(q)$ , which we can rewrite $$|F^{-1}(q)|=d(F)+\sum_{p\in F^{-1}(q)}1-e_p (F).$$ Therefore the total number of preimages of vertices of $Y$ , which is the number $v_X$ is $$v_X = \sum_{vertex \ q \ of \ Y}(d(F)+\sum_{p\in F^{-1}(q)}1-e_p (F))=d(F)\cdot v_Y - \sum_{vertex \ q\ of \ Y}\sum_{p \in F^{-1}(q)}e_p(F)-1)\\ = d(F)\cdot v_Y - \sum_{vertex \ p \ of \ X}e_p(F)-1$$ Also, I think the last inequality is true because the set of vertex $p$ of $X$ is the set of preimages of $q$ vertex in $Y$ . It's true? Last step is to calculate $\chi(X)$ , we have $$\chi(X)=v_X -e_X + t_X=d(F)v_Y - d(F) e_Y + d(F)\cdot t_Y-\sum_{p \ vertex \ of \ X}e_p (F)-1,$$ I think last inequality holding because every ramification point of $F$ is a vertex of $X$ .","['complex-analysis', 'riemann-surfaces', 'algebraic-geometry', 'algebraic-topology']"
3634213,"Consider the system $x'=-y-x^2$, $y'=x.$ Find a reversor $S$, identify Fix$(S)$ and show that the origin is a nonlinear center. [closed]","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 4 years ago . Improve this question Can I please have help solving the following? I am not quite sure how to approach this problem and was wondering if someone could please help. Thanks! Consider the system $$x'=-y-x^2$$ $$y'=x.$$ Find a reversible $S$ , identify Fix $(S)$ and show that the origin is a nonlinear center.","['systems-of-equations', 'linearization', 'ordinary-differential-equations', 'nonlinear-system', 'dynamical-systems']"
3634214,Recursive definition of sets,"I'm having trouble understanding the following problem: I have three sets, the first one contains $$S=\{a,b,c\}$$ The second contains operators (plus, times, bullet) $$O=\{+, \times, \bullet\}$$ The third one, E is a set of ""well written expressions"" defined by the following two rules: $$(1) \text{ if } x \in S \text{, then } x \in E$$ $$(2) \text{ if } X,Y \in E \text{ and } * \in O \text{ then } *XY \in E$$ Now I have multiple expressions and I must verify if they are ""correct"". The issue is when I try to interpret the rules. The first one stipulates that for any letter in S it exists in E. the second says that for an expression in E and a symbol in O the combination also exists in E? How can I try to mathematically prove that a given expression is in E ex: $$+ a\times bc$$ Thanks!","['discrete-mathematics', 'recursion']"
3634216,Proving an equivalence relation in regards to sum of absolute values.,"Prove that the relation R is an equivalence relation on the set of real numbers. $$(x,y) \in R \iff |x+y| = |x| + |y|$$ I did prove the reflexivity as well as the symmetry, but I am stuck on how to prove the transitivity of this relation.","['elementary-set-theory', 'equivalence-relations', 'relations']"
3634238,Prove that if $f(f(x)) = x-1$ then $f$ is bijective,"I have this statement: Let $f: \mathbb{R} \to \mathbb{R},$ and $f(f(x)) = x-1$ prove that $f$ is bijective My attempt was: $(1)$ Use the fact that if a composite function $gof$ is bijective then $f$ is bijective. In this case, if $fof$ is bijective, then $f$ is bijective. First, prove $fof$ is injective. To prove: $f(f(x_1)) = f(f(x_2)) \to x_1 = x_2$ $x_1 -1 = x_2 - 1 \to x_1 = x_2$ and proved. Second, prove $fof$ is surjective. Using the fact that the codomain of $gof$ is equal to the codomain of $g$ , then in this case,  the codomain of $f$ is $\mathbb{R}$ and therefore the codomain of $fof$ is $\mathbb{R}$ and noting that the range of $x -1$ is the same, we can conclude that $fof$ is surjective. And using $(1)$ , i proved that $f$ is bijective. My teacher told me that my development was wrong and i dont know why. Thanks in advance.","['functions', 'solution-verification']"
3634280,Solve $x+yp=ap^2$ where $p=\frac {dy}{dx}$,Solve $x+yp=ap^2$ where $p=\frac {dy}{dx}$ My Attmept: $$x+yp=ap^2$$ $$x=ap^2-yp$$ This is solvable for $x$ so differentiating both sides w.r.t $y$ $$\frac {dx}{dy} = 2ap\cdot \frac {dp}{dy} - y\cdot \frac {dp}{dy} -p$$ $$\frac {1}{p} + p = (2ap-y) \cdot \frac {dp}{dy}$$ $$\frac {1+p^2}{p} = (2ap-y) \cdot \frac {dp}{dy}$$ $$\frac {dy}{dp} = \frac {2ap^2}{1+p^2} - \frac {yp}{1+p^2}$$ $$\frac {dy}{dp} + \frac {p}{1+p^2} \cdot y = \frac {2ap^2}{1+p^2}$$ $$\textrm {This is Linear so}$$ $$\textrm {Integrating Factor}=e^{\int \frac {p}{1+p^2} dp}=\sqrt {1+p^2}$$ So we have $$y\sqrt {1+p^2} = \int \frac {2ap^2}{1+p^2} \times \sqrt {1+p^2} dp + c$$ $$y\sqrt {1+p^2} = 2a \int \frac {p^2}{\sqrt {1+p^2}} dp + c $$ I could not solve further from here.,"['calculus', 'ordinary-differential-equations']"
3634295,Is the equivalence between tangent and cotangent bundles canonical for algebraic topologists?,"Let $M$ be a smooth manifold. Choosing a metric tensor $g$ on $TM$ one gets a vector bundle isomorphism $g^\flat\colon TM\to T^*M$ . Changing $g$ we get different maps $g^\flat$ . I would like to understand the statement and its consequences for algebraic topology: Every two metric choices are homotopic. My interpretation would be that the metric tensor is a (positive-definite) section $g\colon M\to \mathrm{Sym}^2(T^*M)$ and every two of these are homotopic (through positive-definite sections) by $$H_t(x) = t g_1(x) + (1-t)g_2(x)$$ Similarily if I treat $g^\flat$ as a section of the bundle $\mathrm{Hom}(TM, T^*M)$ which is an isomorphism on each fiber, the above homotopy yields a homotopy (through isomorphisms as well) between any $g_1^\flat$ and $g_2^\flat$ . This would mean that from the point of view of algebraic topology, the equivalence between $TM$ and $T^*M$ is ""canonical"" (induced isomorphisms on (co)homology and similar concepts do not depend on the metric chosen). Have I got this right?","['differential-topology', 'algebraic-topology', 'differential-geometry']"
3634374,Test for nonexistence of subgroups with given order?,"There are useful theorems for the existence of subgroups, such as the Sylow-theorem and the Hall-theorem. But even if the desired order divides the group order, subgroups need not exist. Given a group and an order dividing the group order. What is the easiest way to verify whether a subgroup with the desired order exists. In particular, how can we show that there is no such subgroup ? Are there any useful sufficient conditions for the non-existence of such a subgroup ?","['group-theory', 'finite-groups']"
3634388,"If $f(x)=x^3+x^2+ax+4\sin x$ is increasing for all real values of $x$, find the interval in which the variable $a$ lies.","I have this problem If $f(x)=x^3+x^2+ax+4\sin x$ is increasing for all real values of $x$ , then $a$ lies in the interval So what I did was to first differentiate the function and equate it to greater than equal to zero ie $f'(x)=3x^2+2x+a+4\cos x \geq 0$ I also know that the determinant of this value should be less than 0. But I don't know what to do after this. I asked my teacher and some other sources and they just substituted $\cos x=-1$ and wrote the equation as : $f'(x)=3x^2+2x+a-4 \geq 0$ I don't know what to do after this and why did my teacher do so. If you can guide me then it would be great.","['maxima-minima', 'calculus', 'derivatives', 'trigonometry']"
3634416,"find $ \lim\limits_{n\to\infty}\int_{0}^{1}1/(1+x^{n})\,dx $","First of all, English is not my native language, but Chinses is. I tried to spilt the integration interval into 2 pieces: $ [0, 1-1/n] $ and $ [1-1/n, 1] $ . In both intervals I use the mean value theorem: $$
	\int_{0}^{1-1/n}\frac{1}{1+x^{n}}\,dx=\frac{1}{1+\xi_{n}^{n}}\left( 1-\frac{1}{n} \right), \qquad \text{and} \qquad \int_{1-1/n}^{1}\frac{1}{1+x^{n}}\,dx=\frac{1}{1+\eta_{n}^{n}}\frac{1}{n},
	$$ where $ \xi_{n}\in(0, 1-1/n), \eta_{n}\in(1-1/n, 1) $ .I found that the latter formula has a limit of $ 0 $ when $ n\to\infty $ . However I can't handle the previous formula. Does anyone has some thoughts?","['calculus', 'analysis']"
3634447,Is the optimum of this constraint optimization problem smooth everywhere except at one jump?,"Let $f:\mathbb R^+ \to \mathbb R$ be a smooth function, satisfying $f(1)=0$ , and suppose that $\,|f(x)|$ is strictly increasing when $x \ge 1$ , and strictly decreasing when $x \le 1$ . For any $s>0$ , define $$
F(s)=\min_{xy=s,x,y>0} f^2(x)+ f^2(y). 
$$ If I am not mistaken, the map $s \to F(s)$ is continuous. Question: Does there exist an $s^* >0$ such that $F|_{(0,s^*)},F|_{(s^*,\infty)}$ are smooth? I ask if $F(s)$ is piecewise smooth, with at most one ""jump"" point. Can there be more than one? Here is an example which shows that we must allow for at least one point of non-regularity: Linear penalization: Take $f(x)=x-1$ . In that case $$F(s) =
\begin{cases}
2(\sqrt{s}-1)^2,  & \text{ if  }\, s \ge \frac{1}{4} \\
1-2s, & \text{ if  }\, s \le \frac{1}{4}
\end{cases}
$$ $F$ is $C^1$ but not $C^2$ . (for a more involved example, see here ). Of course, $F(s)$ can be smooth, e.g. when $f(x)=\log x$ . In that case $ F(s)=2f^2(\sqrt s)=\frac{1}{2}(\log s)^2.$","['nonlinear-optimization', 'smooth-functions', 'real-analysis', 'multivariable-calculus', 'optimization']"
3634458,Looking for a Real Analysis textbook with solutions for self study,I am wondering if there are any good texts out there that also have a solution easily available. I've found solutions to Mathematical Analysis by Apostol and Principles of Mathematical Analysis by Rudin but found them a bit too dense for my background (single-variable calculus with an emphasis on theory/proofs instead of application). I also came across Understanding Analysis by Abbott but I'd prefer something that covers a more extensive range of topics (like that of the first two). I might be asking for too much but is there a book that: Has a good exposition of the content (I'd prefer more hand-holding over less) Has solutions either online (not necessarily by the auther of the book) or in the back of the book Covers more content like Lebesgue Integrals or perhaps Multivariable Calculus My initial plan was to go with Understanding Analysis since it covers 1) and 2) but then I'd face the issue with finding the same characteristics in a book that picks up where Understanding Analysis left off as most textbooks don't have easily accessible solutions which I find incredibly useful especially since I'll be self-studying.,"['self-learning', 'book-recommendation', 'analysis', 'reference-request', 'real-analysis']"
3634504,"Where do the integral signs come from when you solve differential equations and have a ""dx"" (or similar) on its own?","Ok so I can solve (some!) differential equations, but I dont quite understanding what's happening.  For example: $$
\frac{dv}{dt} = \frac{2}{3}v^{-2}
$$ We can rearrange to get all the $v$ 's on the left and the $t$ 's on the right so we can solve it: $$
\int 3v^2 dv = \int 2 dt
$$ My question is:  Where did those integral signs come from?!  At first I thought ah well I guess if you have something multiplied by $dt$ (or similar), then that refers to an integral - but in integration by substitution we find $dx$ in terms of $du$ without having to stick an integral sign on the front!  So, what's going on here.  How do we know it's an integral.
Thank you!","['integration', 'calculus', 'ordinary-differential-equations']"
3634544,Is the minimum of this optimization problem essentially unique?,"Let $h:\mathbb R^{>0}\to \mathbb R^{\ge 0}$ be a smooth function, satisfying $h(1)=0$ , and suppose that $h(x)$ is strictly increasing on $[1,\infty)$ , and strictly decreasing on $(0,1]$ . Let $s>0$ be a parameter, and define $
F(s)=\min_{xy=s,x,y>0} g(x,y),
$ where $g(x,y):=h(x)+ h(y)$ . Question: Can the minimum be obtained at two essentially different points? That is, suppose that $F(s)=g(x,y)=g(\tilde x,\tilde y)$ , for some $x,y,\tilde x,\tilde y>0$ satisfying $xy=\tilde x \tilde y=s$ . Is it true that $$ (x,y)=(\tilde x,\tilde y) \, \, \, \text{ or } \,\, (x,y)=(\tilde y,\tilde x)?$$ By symmetry, we can assume W.L.O.G that $x \le \sqrt{s}$ . It is not hard to see that the minimum must be obtained at a point where $x, y \le 1$ (if $s \le 1$ ) or $x,y \ge 1$ (if $s \ge 1$ ). Thus, if $s \le 1$ , then we have $x,y=\frac{s}{x} \le 1$ , which implies $s \le x \le \sqrt{s}$ . Edit: I tried to produce counter-examples by using $g$ which are invariant under some automorphism of the hyperbola $xy=s$ . (Then the set of minimizers is closed under the operation of this automorphism). I couldn't find such an automorphism which preserve the special additive structure of $g$ . Here is a partial analysis of the question for local minima: Set $\psi(x)=h(x)+h(\frac{s}{x})$ . Then $$\psi'(x)=h'(x)-h'(\frac{s}{x})\frac{s}{x^2}, \tag{1}$$ and $$\psi''(x)=h''(x)+h''(\frac{s}{x})\frac{s^2}{x^4}+2h'(\frac{s}{x})\frac{s}{x^3}. \tag{2}$$ Now, suppose $x$ is a local minimum of $\psi$ . Then, equations $(1),(2)$ imply that $$
h'(x)=h'(\frac{s}{x})\frac{s}{x^2} \, \, , \, \, h''(x)+h''(\frac{s}{x})\frac{s^2}{x^4}+2\frac{h'(x)}{x} \ge 0\tag{3}.
$$ Subquestion: Suppose that $x,y$ satisfy $(3)$ . Does $x=y$ or $x=\frac{s}{y} $ hold?","['symmetric-functions', 'global-optimization', 'real-analysis', 'multivariable-calculus', 'optimization']"
3634653,Is this expression OK? $d(xy)=xdy+ydx$,"Is it OK to write $d(xy)=xdy+ydx$ ? I thought it should be written as $\frac{d(xy)}{dx}=x\frac{dy}{dx}+y$ to clarify with respect to which variable the equation is differentiated.
Is the first expression widely accepted in mathematics? What would happen if you combine it with an integral? $\int{\frac{d(xy)}{dx}dx}=\int{d(xy)}=xy$ ?","['calculus', 'derivatives']"
3634666,"Good analogies between types in programming and structures, spaces and fields in mathematics?","I am a programmer, and so my knowledge of math is limited to bare-bones calculus, linear algebra etc. Basically I know how to do the basic stuff, but never learned how to look at it in a more abstract sense. I am trying to see if I can leverage my understanding of concepts within programming to get a better intuition about concepts in mathematics. I am well aware that there is no 1-to-1 mapping. So I am not looking for precise analogies but very rough analogies to help me build some intuition and make my journey into math-world a bit easier. In programming , we operate with hierarchies of types, some which are abstract and others which are concrete. Concrete types can be instantiated to create objects. We can do thing with objects based on the type they belong to. So if I have an abstract type A, which concrete subtypes B and C, then instances of B and C will have some shared abilities because they share an abstract type A. I am trying to map this thinking into my math reading. In programming , there is often a base type for everything called e.g. Object or Any . It seems you have the same in math. Everything is a mathematical object . In many programming languages , types are also objects themselves. There seems to be an analogy in mathematics. mathematical structures look like types to me. As a programmer , I would say these have mathematical objects as instances. However , the mathematical structure is itself also an object, just like a type can be an object in programming. And like programming there is a type hierarchy. If I understand correctly an algebraic structure is kind of like a subtype of refinement of the mathematical structure concept. When I look at things like fields and spaces , this reminds of parameterized types in programming. E.g. Vector2D{Integer} or Vector2D<Int> depending on the language you use are 2D vectors parameterized on the Integer type, meaning their x and y coordinates are integers, rather than say floating point. As far as I understand , you express vector spaces in a similar fashion. You say a vector space over some field. The field seem a bit analogous to a type parameter to me. The field would be the natural numbers, real numbers of whatever. The field defines the components making up the  individual vectors in the vector space as well as the numbers which you may multiply a vector with e.g. Again I know these kinds of analogies are flawed, but if we were to attempt to create analogies, is there some truth to what I am guessing here or do I have a profound misunderstanding of these concepts?","['matrices', 'abstract-algebra', 'linear-algebra', 'soft-question', 'computer-science']"
3634695,"Is there a $4$-by-$4$, rank $3$, positive semidefinite matrix with $a_{ii}=3$, $|a_{12}|\neq 1$, and principal minors having minimal eigenvalue $1$?","Could anyone help me search for a positive semidefinite matrix $\left(a_{i,j}\right)_{4\times4}$ of rank 3 with $a_{i,i}=3$ and its all 3 by 3 principal minor matrices having minimal eigenvalue $\lambda_{\min}=1$ , but $\left|a_{1,2}\right|\ne1$ , or could anyone explain why such a matrix would not exist? Thanks a lot.","['discrete-geometry', 'positive-semidefinite', 'matrices', 'combinatorial-geometry', 'linear-algebra']"
3634700,"If $u_i\cdot u_j<0$ for all $u_0,...,u_n\in\Bbb R^n$, then the $u_i$ cannot lie in the same halfspace?","Given a set of $n+1$ vectors $u_0,...,u_n\in\Bbb R^n$ with pair-wise negative inner product, that is, $u_i\cdot u_j<0$ for $i\not=j$ . Question: what is a quick and clean way to see that the $u_i$ cannot all lie in the same half-space, that is, that there is no $y\in\Bbb R^n \setminus \{ 0 \}$ with $y\cdot u_i\ge 0$ for all $i\in\{0,...,n\}$ ?","['euclidean-geometry', 'discrete-geometry', 'inner-products', 'geometry', 'linear-algebra']"
3634705,What does $\Phi\vert_S$ mean?,"In Abstract Algebra (Dummit and Foote) Exercise 11 in Section 6.3 (A Word on Free Groups) it says: ... Prove that $A(S)$ has the following universal property: if $G$ is any abelian group and $\varphi:S\rightarrow G$ is any set map, then there is a unique homomorphism $\Phi:A(S)\rightarrow G$ such that $\Phi\vert_S=\varphi$ . ... The notation is not introduced in the book and I wouldn't like to take any wrong assumption.","['notation', 'abstract-algebra', 'free-groups', 'group-theory', 'abelian-groups']"
3634709,Fibonacci recurrence relation - Principle of Mathematical Induction,"The problem: Let $F_n$ be the nth term of the Fibonacci sequence: $F_0 = 0$ $F_1 = 1$ $F_n = F_{n-1} + F_{n-2}$ for $n\geq2$ Prove that $\sum_{i=1}^{n} F_i^2 = F_nF_{n+1}$ for all $n \in\mathbb{N}$ My attempt to prove this using the induction hypothesis is: 1) With $n = 1$ , the equation holds true: $F_{1}^2 = F_{1}F_{n+1}$ because $1^2 = 1*1$ . 2) Now we have to prove that $\sum_{i=1}^{n} F_i^2 = F_nF_{n+1} \implies \sum_{i=1}^{n+1} F_i^2 = F_{n+1}F_{n+2}$ We know that $\sum_{i=1}^{n+1} F_i^2 = \sum_{i=1}^{n} F_i^2 + F_{n+1}^2$ , and if we assume that the antecedent is true we get: $\sum_{i=1}^{n+1} F_i^2 = F_{n}F_{n+1} + F_{n+1}^2$ If we replace this last equation in the consequent we get: $F_{n}F_{n+1} + F_{n+1}^2 = F_{n+1}F_{n+2}$ Finally, if we divide both sides by $F_{n+1}$ we end up with the Fibonacci recurrence equation: $F_{n+2} = F_{n+1} + F_{n}$ We know this holds for all $n\in\mathbb{N}$ , because $F_n$ is defined as the nth term of the Fibonacci sequence by the premises of the problem. Therefore we have proven (2). Thus, by the Principle of Mathematical Induction : $\sum_{i=1}^{n} F_i^2 = F_nF_{n+1}$ for all $n \in\mathbb{N}$ QUESTION : Is this proof correct? And if not, where is the mistake?","['fibonacci-numbers', 'logic', 'recurrence-relations', 'discrete-mathematics', 'induction']"
3634732,Independence of some events with respect to Haar measure,"In order to fulfill details in a proof, I came across with the following fact which, whenever true, allows me to conclude the line of reasoning. Here is the stage:
I have an abelian group $G=\Bbb{Q}^n/\Bbb{Z}^n$ (here $n$ is a positive integer) and it is equipped with the discrete $\sigma$ -algebra.
Furthermore, let us denote by $\Gamma_n$ the dual of $G$ , i.e. the group of all group homomorphisms form $G$ to the complex unit circle $T=\{z\in\Bbb{C}\colon \lvert z\rvert=1\}$ and denote by $\mu$ the probability Haar measure on $\Gamma_n$ . What I want to prove is the following:
the events $A=\{\psi\in\Gamma_n\mid \psi(a+\Bbb{Z}^n)=0\}$ and $B=\{\psi\in\Gamma_n\mid \psi(b+\Bbb{Z}^n)=0\}$ are $\mu$ -independent whenever $a,b\in\Bbb{Q}^n\setminus \Bbb{Z}^n$ are linearly independent over $\Bbb{Z}$ , i.e. $$\mu(A)\cdot \mu(B)=\mu(A\cap B).$$ Although I spent on this much time, I got the least in the sense that I'm stuck.
In some sense,  that I'm not able to make more precise, I expect all the sets $A$ , $B$ and $A\cap B$ have $0$ measure wrt $\mu$ . Any suggestion or contribution is appreciated. Edit As @AlexRavsky remarked below, the correct (and more general) notion of independence to be used here is the following: $a,b$ are lineraly independent iff $ma+kb=0$ implies $m=k=0$ , not the one I used in my original question.","['measure-theory', 'independence', 'topological-groups', 'haar-measure', 'probability-theory']"
3634770,Intuition for Cohomology and Holes in a Space,"I'm learning that the dimension of the $k^{th}$ De-Rham Cohomology Group (as a vector space over $\mathbb{R}$ ) of space tells us the number of $k$ -dimensional holes in the space. I always found this quite strange, given that the De-Rham Cohomology Group deals with differential forms, a very algebraic object, whereas holes are very geometric. So my question is this: Is there an intuitive reason why differential forms on a space and holes in the space have anything to do with each other? Does the presence of holes force the differential form to ""dodge"" the hole, which in turn changes its properties in ways that we can detect? I know how to formally prove that forms detect holes; that's not what I'm looking for. I am looking for a deeper philosophical answer to the question: what is the intuitive relationship between differential forms and holes? Thanks in advance. Appreciate the help!","['differential-forms', 'algebraic-topology', 'differential-geometry']"
3634797,"Find the directional derivative of $x^2-xy-2y^2$ at $(1,2)$ along the vector that goes from this point to $(3,4)$","I used this formula (in blue, you have to scroll down a bit), and did this: $$\lim_{h \rightarrow 0} \frac{f(1+3h,2+4h)-f(1,2)}{h} = \\
\lim_{h\rightarrow 0}\frac{(1+3h)^2-(1+3h)(2+4h)-2(2+4h)^2+11}{h} = \\
\lim_{h\rightarrow 0} \frac{2-36h-35h^2}{h} = \lim_{h \rightarrow 0 } 2/h - 36 - 35h = \infty$$ What went wrong?","['multivariable-calculus', 'calculus']"
3634801,"What's the difference between the Wiener measure on $C^0[0,1]$ and the distribution function of a Brownian motion?","Let $(\Omega,\mathcal A, P)$ be a probability space and $B_t$ a standard Brownian motion on that space ( $t \in [0,1]$ ). I am trying to understand what is called the ""Wiener measure"". I never had a course on that, but I always thought that what is called the Wiener measure is simply the dynamical Gaussian measure associated with the Brownian motion, that is $\mu_t(A)= P(B_t \in A)=\int_A \frac{1}{\sqrt{2\pi t}} e^{-x^2/2\sqrt{t}} \, dx$ $\forall A \in \mathcal B(\mathbb R)$ , so for each $t>0$ , $\mu_t$ is a Gaussian measure (the cumulative distribution function/pushforward measure of the Brownian motion). But I just discovered that it is not exactly the case. The Wiener measure is a measure on $(C^0[0,1], ||.||_\infty)$ . So a measure on an infinite dimensional space. How do we even build such measure ? Intuitively, $\mu_t$ is ""the projection on one dimension"" of the Wiener measure. So the Wiener measure is able to measure much more than just sets in $\mathcal B(\mathbb R)$ . It can measure sets in $\mathcal B(C^0[0,1])$ . I have many questions on the Wiener measure, call it $W$ : How is $W(A)$ defined for $A \in \mathcal B(C^0[0,1]) $ ? Does it have a density ? With regard to what measure ? Is it enough the give all ""one dimensional projection"" of a measure on an infinite dimensional space to define this measure ? So in that case, is knowing all $\mu_t$ enough to build $W$ ? How to do that ? Is knowing $W$ enough to build all $\mu_t$ ? If so, how to do that ? Same question but for a general measure on an infinite dimensional space, not necessarily $W$ . What's the connection between $W$ and $B$ besides through the one dimensional projections ? Is $W$ the pushforward measure of a Brownian motion on $\mathbb R ^\infty$ ? Is the latter well defined ?","['probability-theory', 'brownian-motion', 'functional-analysis', 'measure-theory']"
3634855,Definition of Tensor Laplacian,"Let $(M,g)$ be an orientable Riemannian manifold, $\nabla$ its Levi-Civita connection and $\epsilon$ its volume form. Let $f\in C^\infty(M)$ be a scalar field. Then we know that we can define its Laplacian by $$\nabla^2f=\star d\star df.\tag{1}$$ Now let $T \in \Gamma(T_k^0M)$ be a tensor field $$T:\Gamma(TM)\times\cdots \times \Gamma(TM)\to C^\infty(M),\tag{2}$$ how does one define the Laplacian $\nabla^2 T$ ? I've googled about it and the only thing I found was this page and unfortunatelly I didn't manage to make sense of it because I found their notation quite confusing (I've never got used to that semicolon notation for covariant derivatives and, moreover, the rule I know is to sum over indices repeated indices appearing twice , and in their notation the same index appears even thrice).","['laplacian', 'definition', 'riemannian-geometry', 'differential-geometry']"
