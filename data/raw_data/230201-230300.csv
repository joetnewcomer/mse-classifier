question_id,title,body,tags
4773790,How to show that the random variable $Y$ is independent of $\sum\limits_{i} X_i^2-Y^2$.,"I am trying to solve a problem in probability theory which is as follows: Problem: Suppose $X_1,...,X_n$ are i.i.d. with distribution $\mathcal{N}(0,1)$ and $a_1,...,a_n\in \mathbb R$ are such that $\sum\limits_{i} a_i^2=1$ . Let $Y=\sum\limits_{i}a_iX_i$ . Then show that $Y$ is independent of $\sum\limits_{i} X_i^2-Y^2$ . I have managed to show that $Y$ follows the standard normal distribution. But I am unable to show why the two random variables are independent of each other. Can someone help me?","['measure-theory', 'density-function', 'probability-theory', 'random-variables']"
4773803,Find the Expected Travel Distance of the Ambulance (Where did I go wrong?),"Problem: The county hospital is located at the center of a square whose sides are 3 miles wide. If an accident occurs within this square, then the hospital sends out an ambulance. The road network is rectangular, so the travel distance from the hospital, whose coordinates are (0, 0), to the point (x, y) is |x|+|y|. If an accident occurs at a point that is uniformly distributed in the square, find the expected travel distance of the ambulance. My Attempt: $E[D] = E[X] + E[Y]$ ; Where D is the travel distance, X is the travel distance on the x axis and Y is the travel distance on the y axis. Because it's uniformly distributed and its a square I can do this: $E[D] = 2E[X]$ . Then $$(1) \space E[X] = \int_{a}^{b} xf_X(x) \,dx$$ $$(2) \space f(x, y) = |x| + |y|, 0 \leq x, y \leq 3/2$$ $$(3) \space f_X(x) = \int_{0}^{3/2} |x| + |y| \,dy = \frac{3}{2}x + \frac{9}{8}$$ (Because we're only working with positive values, there's no negative, so it's similar to integrating x + y) $$(4) \space E[X] = \int_{0}^{3/2} x(\frac{3}{2}x + \frac{9}{8}) \,dx = \frac{189}{64}$$ $$(5) \space E[D] = 2E[X]; E[D] = 2(\frac{189}{64}) = \frac{378}{64}$$ So the answer obviously doesn't make sense, as it's larger than the max travel distance possible, which is 3. \ The answer is 3/2 and this doesn't make sense to me either and I'll explain why. $E[X] = \mu$ which is the average. If the average is 3/2 then on average the ambulance will drive 0.75 miles on the x axis and 0.75 miles on the y axis. If we take the hospital as the center point, then we have a square of 1.5 by 1.5 mile surrounding the hospital. The area of that square is $\frac{9}{4}$ and the area of the other square which was 3 by 3, is 9. Then we have $9 - \frac{9}{4} = \frac{27}{4}.$ Considering we took the mean and the chance is uniform, meaning that every point in the square has an equal chance of being chosen, the area within the mean and the area outside of the mean should be equal, no?","['integration', 'uniform-distribution', 'expected-value', 'probability-theory', 'probability']"
4773843,On a locally-compact group $h(x^{-1})$ constant a.e. $\implies h(x)$ constant a.e. (proof of uniqueness of Haar measure),"I'm trying to understand the final step in a proof of uniqueness of left Haar measures on locally-compact groups as presented in Knapp's ""Advanced Real Analysis"" (pg. 224-225) or this note by Pedersen (pg. 5). Both proofs proceed roughly as follows Let $\mu,\lambda$ be left Haar measures on the $\sigma$ -compact LCG $G$ , wlog assume $\mu\leq\lambda$ (otherwise replace $\lambda$ by $\mu+\lambda$ ). By Radon-Nikodym there exists $h:G\to[0,\infty)$ with $d\mu=hd\lambda$ . For $g$ arbitrary fixed and $f\geq0$ arbitrary by translation-invariance (here $L_gf(x)=f(g^{-1}x)$ ) $$\int f\cdot L_gh\,d\lambda=\int L_{g^{-1}}f\cdot h\,d\lambda=\int L_{g^{-1}}f\,d\mu=\int f\,d\mu=\int f\cdot h\,d\lambda$$ $\implies L_gh\,d\lambda=h\,d\lambda$ as measures $\implies$ for any $g$ holds $L_gh=h$ $\lambda$ -a.e.. It follows that the function $g\mapsto\int|L_gh-h|\,d\lambda(x)$ is zero, so by Tonelli $$\iint|L_gh-h|\,d\lambda(g)d\lambda(x)=\iint|L_gh-h|\,d\lambda(x)d\lambda(g)=0,$$ hence for $\lambda$ -almost every choice of $x$ holds $$\int|h(g^{-1}x)-h(x)|\,d\lambda(g)=\int|h(g^{-1})-h(x)|\,d\lambda(g)=0.$$ Fix such an $x_0$ , then $h(g^{-1})=h(x_0)$ for $\lambda$ -almost every $g\implies h(x^{-1})$ is constant $\lambda$ -a.e. $\implies h(x)$ is constant $\lambda$ -a.e.. I don't understand the highlighted step, how does $h(x^{-1})$ being costant a.e. imply the unmodified $h(x)$ is constant a.e.? Both Knapp and Pedersen gloss over this detail. Knapp : [...] and $h(g^{-1}x)=h(x)$ for almost every $x\in G$ . We can regard $h(g^{-1}x)$ and $h(x)$ as functions of $(g,x)\in G\times G$ ; for each $g$ , they are equal for almost every $x$ . By Fubini’s Theorem they are equal for almost every pair $(g, x)$ (with respect to the product measure), and then for almost every $x$ they are equal for almost every $g$ . Pick one such $x$ , say $x_0$ , then it follows that $h(x) = h(x_0)$ for almost every $x$ . Pedersen :  Fubini’s theorem, applied to the product integral and the function $(x,z)\mapsto h(z^{-1}x)-h(x)$ , now shows that the function $x\mapsto\int|h(z^{-1}x)-h(x)|dz=\int|h(z^{-1})-h(x)|dz$ equals zero almost everywhere. It follows that $h$ is constant almost everywhere, as desired.","['integration', 'measure-theory', 'harmonic-analysis', 'representation-theory', 'functional-analysis']"
4773877,Do mollifiers exist in dimensions higher than 1?,"A mollifier is defined as a function $\varphi: \mathbb{R}^n \rightarrow \mathbb{R}$ such that $$\int_{\mathbb{R}^n} \varphi(x) dx = 1$$ $\varphi$ has compact support $$\lim_{\epsilon \rightarrow 0} \varphi_\epsilon = \lim_{\epsilon \rightarrow 0} \frac{1}{\epsilon^{n}} \varphi\big(\frac{x}{\epsilon}\big) = \delta_0$$ Thus we may mollify any function $f: \mathbb{R}^n \rightarrow \mathbb{R}$ by computing the convolution $f * \varphi_\epsilon$ . I would like to do the same for a function $g: \mathbb{R}^n \rightarrow \mathbb{R}^m$ where $m > 1$ . To do this would require the mollifiers $\varphi$ to be extended to higher dimensions, that is $\varphi: \mathbb{R}^n \rightarrow \mathbb{R}^m$ for $m > 1$ . Does such an object exist? If not, what about the three criteria above fails in higher dimensions? I could not find it in any textbooks.","['convolution', 'fourier-analysis', 'functional-analysis', 'analysis']"
4773898,Compare the growth rate of functions $f(n)=(2^{n+1}-1)^{1/\log n}$ and $n^{\log n}$,"How do I compare the growth rate of functions $f(n)=(2^{n+1}-1)^{1/\log n}$ and $g(n)=n^{\log n}$ My Attempt \begin{align}
\lim_{n\to\infty}\frac{\log f(n)}{\log g(n)}&=\lim_{n\to\infty}\frac{\log\Big(2^{n+1}-1\Big)^{1/\log n}}{\log 2^{(\log n)^2}}\\
&\approx\lim_{n\to\infty}\frac{\frac{1}{\log n}\log\big(2^{n+1}\big)}{{(\log n)^2}}\quad\Bigg[2^{n+1}-1=2^{n+1}\text{ for large }n\\
&=\lim_{n\to\infty}\frac{n+1}{(\log n)^3}\\
&=\lim_{n\to\infty}\frac{1}{3(\log n)^2.\frac{1}{n\ln 2}}\bigg[\text{Applying L'Hospital rule}\\
&=\lim_{n\to\infty}\frac{n\ln 2}{3(\log n)^2}\\
&=\lim_{n\to\infty}\frac{\ln 2}{6(\log n).\frac{1}{n\ln 2}}\\
&=\lim_{n\to\infty}\frac{(\ln 2)^2.n}{6\log n}=\infty\\
\end{align} But when I tried to plot the graphs in Mathematica I am getting $g(n)$ to be larger than $f(n)$ : Note: $\log n=\log_2 n$ and $\ln 2=\log_e 2$ Mathematica code: $\text{Plot}\left[\left\{f(n)=\left(2^{n+1}-1\right)^{\frac{1}{\log _2(n)}},g(n)=n^{\log _2(n)}\right\},\{n,0,10\},\text{PlotLabels}\to \text{Expressions},\text{ImageSize}\to \text{Large}\right]$","['limits', 'functions', 'functional-analysis', 'logarithms']"
4773906,Independence of random pairs and conditional expectation,"Let $(X_1,Y_1)$ and $(X_2,Y_2)$ be two independent and identically distributed random pairs. Does it hold that $$E(Y_1Y_2|X_1,X_2)=E(Y_1|X_1)E(Y_2|X_2)?$$ Why? Comments By hypothesis, the two sigma-algebras are independent $\sigma(X_1,Y_1)\perp \sigma(X_2,Y_2)$ . It implies that $\sigma(Y_1)\perp \sigma(X_2,Y_2)$ and $\sigma(Y_2)\perp \sigma(X_1,Y_1)$ . So $E(Y_1Y_2|X_2)=E(Y_1)E(Y_2|X_2)$ and $E(Y_1Y_2|X_1)=E(Y_2)E(Y_1|X_1)$ . Is everything correct so far?
If so, my intuition says that $E(Y_1Y_2|X_2,X_1)=E(Y_1|X_1)E(Y_2|X_2)$ but I'm not quite sure how to show it.","['statistics', 'independence', 'conditional-expectation']"
4774013,Can the union of several curves in the plane intersect every line in exactly two points?,"A teacher once challenged the students to find a subset of $\mathbb R^2$ which intersects every line in finitely many points (not zero). The trick was to think about graphs instead of subsets. In fact, the subset $\{(x, x^3): x\in\mathbb R\}\subset\mathbb R^2$ intersects every line in $1$ or $3$ points. Recently, in a set theory class, I've seen the construction of a subset of $\mathbb R^2$ that intersects every line in exactly two points. The construction used extensively the Axioma of Choice, so there was no hope for visualizing the set. Thinking about it, I came to the conclusion that no curve could have this property: If a line cross a curve in two points, then one of the semi-planes determined by the line must contain a limited section of the curve, therefore, there is a parallel line in this semi-plane that doesn't cross the curve at all. But what if we have a set of curves? Can the union of several curves in the plane intersect every line in exactly two points? Or are sets with this property inherently complex?","['recreational-mathematics', 'geometry']"
4774031,"How many different ""syntactic trees"" exist related to an $m$ word sentence?","Before talking about the main topic, I would like to say that
I'm not in major of mathematics, nor any of mechanics. So, there could be an issue of defining concepts or words not being strict or even be ambiguous about what I'm actually saying. Please, take that into consideration while reading this. Now, let's get started. Let's say, there's a sentence with $3$ words, $a$ , $b$ , and $c$ . We can analyze this sentence by structuring ""syntactic trees"" in $3$ ways. Then, I deleted all the middle nodes between a root and $a,b$ , and $c$ .
Here's a picture of this process. Of course, there can be a tree which looks like this. But I would like to say that the 3rd tree in the first picture and this tree in the 2nd picture are same,
because I can say that both node $a'$ and node $a''$ are referring to exactly same thing which is the node $a$ . Every time I do the analysis, I will delete all the nodes from the tree except the root and last nodes without any child. When I apply this rule to analyze $4$ 'word' sentence,
I will get these trees. Here's the question. How many different ""syntactic trees"" are there of an $m$ word sentence? Thinking of this, I first thought of the number of partitions of $m$ : the number of ways to write $m$ as a sum of positive integers. (For $4$ , there's $5$ ways to do this: $1+1+1+1$ , $2+1+1$ , $2+2$ , $3+1$ , $4$ ) In 'syntactic trees' however, we do care about order. Investigating the topic with my friend, I finally able to show all the possible ways to write $m$ as a sum of positive integers, where order matters. First, get all the possible ways about small number; for example $3$ . Then, for all the possibilities, do $2$ kinds of operations in each possibility. One is about summing $1$ to the leftmost term of a equation, for example, $1+1+1 \rightarrow (1+1)+1+1 \rightarrow 2+1+1$ . Second is about making a new $1$ next to the leftmost term of a equation, for example, $1+1+1 \rightarrow (1)+1+1+1 \rightarrow 1+1+1+1$ . By doing these operations to every equation, I could get all the possibilities of making $4$ without any duplications. Obviously, all the possibilities related to $m$ can be represented as $2^{m-1}$ . But problem was, I needed to take care of all the possibilities within every term of each equation. For bigger numbers, this will definitely become insane. I couldn't think of good way to make a formula for counting trees related to $m$ words. Even figuring out how to do the operation faster or coming up with more clever methods was too much for me. I had no choice but to use a brute force method. Using Python, I was able to count number of trees of an $m$ word sentence, and until $m=20$ it was not a big deal. However, the time it took to calculate further terms seemed to increase exponentially, making this approach infeasible for higher values. One thing I noticed was that between every step, the ratio between the number of $m$ word trees, and the number of $m+1$ word trees seemed to converge to some constant. I don't understand why this happens. Here I've plotted the successive ratio of each step in graph. I want to ask if there's any explicit formula that counts the number of trees for each $m$ word sentence. I would also like to know why the ratio of successive terms seems to converge to some constant. I would be very grateful if anyone gives an explanation and helps me figure out the problem. Thank you. Furthermore: Here's the list of the numbers I got.
Given that $m$ is amount of words in a sentence. Define the function $C(m)$ , which returns the number of ""syntactic trees"" related to an $m$ word sentence. $$\begin{split}
C( 1 ) &= 1\\
C( 2 ) &= 1\\
C( 3 ) &= 3\\
C( 4 ) &= 11\\
C( 5 ) &= 45\\
C( 6 ) &= 197\\
C( 7 ) &= 903\\
C( 8 ) &= 4279\\
C( 9 ) &= 20793\\
C( 10 ) &= 103049\\
C( 11 ) &= 518859\\
C( 12 ) &= 2646723\\
C( 13 ) &= 13648869\\
C( 14 ) &= 71039373\\
C( 15 ) &= 372693519\\
C( 16 ) &= 1968801519\\
C( 17 ) &= 10463578353\\
C( 18 ) &= 55909013009\\
C( 19 ) &= 300159426963\\
C( 20 ) &= 1618362158587\\
C( 21 ) &= 8759309660445\\
C( 22 ) &= 47574827600981\\
C( 23 ) &= 259215937709463\\
C( 24 ) &= 1416461675464871\\
C( 25 ) &= 7760733824437545\\
C( 26 ) &= 42624971294485657\\
C( 27 ) &= 234643073935918683\\
\end{split}$$","['graph-theory', 'trees', 'combinatorics']"
4774083,Applications of Category Theory in Abstract Algebra,"Almost every text on Category theory uses categories such as Ab , Grp , and so on as examples to work with but can category theoretic methods actually help us understand the structures better? In particular, does Category Theory aid us in proving some significant abstract algebraic results that are otherwise tedious to prove? Furthermore, is there any structural insight about abstract algebraic objects that is not readily apparent from algebra itself but becomes crystal clear with a category theoretic approach? I do not have a specific format for the answer in mind. As category theory is quite general, the answer may also include ways to say use results from other branches such as analysis or topology, so as to prove an abstract algebraic result which was tedious to prove using an algebraic approach. In this light, any insight about the relationship between category theory and abstract algebra is welcome.","['abstract-algebra', 'category-theory']"
4774098,Connection between Lame equation with Weierstrass and elliptic sine,"Lame function is the solution of the following equation, $$\frac{d^2w}{dz^2}+\left(A+B\wp(z)\right)w=0,$$ where $A$ and $B$ are constants and $\wp(z)$ is the Weierstrass elliptic function. Wiki says that the most important case corresponds to $B\wp(z)=-\kappa^2\mathrm{sn}^2(\kappa,z)$ , where $\mathrm{sn}(\kappa,z)$ is the elliptic sine function. Moreover, the Lame equation with elliptic sine function can be quite simply transformed into the Mathieu equation by setting $\kappa\rightarrow 0^{+}$ , which gives $$\kappa^2\mathrm{sn}^2(\kappa,z)=\kappa^2\sin z+\mathcal{O}(\kappa^3).$$ I would like to understand the following: Is it possible to obtain the elliptic sine function from the Weierstrass elliptic function? Is it possible to obtain the sine/cosine function from the Weierstrass elliptic function? I have a feeling that it is tightly related to the Inozemtsev limit, which corresponds to setting one of the periods in Weierstrass function to infinity with some additional tricks.","['elliptic-functions', 'ordinary-differential-equations']"
4774112,"About infimum of a subgroup of $(\mathbb{R},+)$","Let $(A,+)$ be a subgroup of $(\mathbb{R},+)$ and $A \neq \left\{0\right\}$ . Consider the set $A_+ = \left\{a \in A: a>0\right\}$ . Prove that $\alpha = \inf A_+$ is in $\mathbb{R_+}$ (non-negative real numbers). (have proved) Show that whenever $\alpha > 0$ then it is in $A_+$ . Deduce $A = \alpha \mathbb{Z}$ . Show that whenever $\alpha =0$ , $A$ is dense subset of $\mathbb{R}$ . This is an interesting problem which is an intersection between Analysis and Algebra. I hope everyone can sit here and discuss it with me. I will also try to put the solution below whenever I have an idea.","['abstract-algebra', 'analysis', 'real-analysis']"
4774141,"What are some calculus, linear algebra and probability and statistics books you would recommend that focus on applications with minimum proofs?","I am a machine learning engineer and I decided I wanted to brush up on the topics relevant for machine learning: calculus linear algebra probability and statistics I will be doing this in my free time, alongside my full-time job. My college education entailed calculus (although I didn't do any multiple integrals, for example). I also had linear algebra and discrete mathematics. I had statistics, but not probability, although I did self-study probability, but I never did probability with calculus for example (like calculating the surface under a probability density function using an integral). I want to emphasize that the books should be focused on applications and not proofs. Actually, the less theory oriented the book, the better. I don't mind going through a few crucial proofs, but that's it. I want to focus on the ""mechanics"". Maybe in a later pass through these topics I will find myself reading some more theory oriented books, but for now, my plan is to read application-oriented books with minimal or no focus on theory and try to solve as many exercises as I can. Having said this, what are your recommendations for books on calculus, linear algebra and probability and statistics that focus on applications with minimum proofs?","['statistics', 'reference-request', 'calculus', 'linear-algebra', 'probability']"
4774167,Prove $\sum_{k=i}^{n} {k-1 \choose i-1} p^i (1-p)^{k-i} = \sum_{k=i}^{n} {n \choose k} p^k (1-p)^{n-k}$,"Prove that the following two summations are equal for any positive integers $i\leq n$ , and any real number $p$ between $0$ and $1$ : $$
\sum_{k=i}^{n} {k-1 \choose i-1} p^i (1-p)^{k-i} = \sum_{k=i}^{n} {n \choose k} p^k (1-p)^{n-k}
$$ I know the equation is originated from Binomial distribution and its insight. That is, $$
P\{X(i,p) > n\} = P\{B(n,p) < i\},
$$ which implies $$
1 - \sum_{k=i}^{n} {k-1 \choose i-1} p^i (1-p)^{k-i} = \sum_{k=0}^{i-1} {n \choose k} p^k (1-p)^{n-k}.
$$ But I do not know how to prove it mathematically using some transformations in combinatorics.","['geometric-distribution', 'binomial-distribution', 'binomial-coefficients', 'combinatorics', 'probability']"
4774175,Help for evaluating the line integrals with Green's Theorem,"Earlier, when I scrolled the Instagram posts I found a mathematical problem uploaded by The Vegan Math Guy like the following because this problem looks interesting to me to be solved. The mathematical problem is to find $\displaystyle \int\limits_C x^4 \,\mathrm dx+ xy \,\mathrm dy$ where $C$ is the area shown by the following graph. In my solution, I'd use Green's Theorem because I've seen this line same as told in Ron Larson's calculus book at Theorem 15.8. Green's Theorem says that: Let $R$ be a simply connected region with a piecewise smooth boundary $C$ , oriented counterclockwise (that is $C$ is traversed once so that the region $R$ always lies to the left). If $M$ and $N$ have continuous first partial derivatives in an open region containing $D$ , then $$\displaystyle \int\limits_C M\,\mathrm dx+N\,\mathrm dy=\iint\limits_R\left( \frac{\partial N}{\partial x}-\frac{\partial M}{\partial y} \right)\,\mathrm dA$$ Then I follow the steps as demonstrated in Example 1 on p. 1025, like this: Examine the conditions Is $C$ positively oriented? Yes. Is $C$ piecewise smooth? Yes. Is $C$ closed and simply connected? Yes. Find the partial derivative the function have continuous first partial derivatives. Let $M=xy$ and $N=x^4$ . \begin{align*} \frac{\partial M}{\partial y}\implies\frac{\partial }{\partial y}\left[ xy \right]&=x\frac{\partial}{\partial y}[y]\\&=x \\ \frac{\partial N}{\partial x}\implies\frac{\partial }{\partial x}\left[ x^4 \right]&=4x^3 \end{align*} Then taking Green's Theorem, I evaluated like this: \begin{align*} \displaystyle \int\limits_C x^4\,\mathrm dx+xy\,\mathrm dy&=\iint\limits_D\left( \frac{\partial N}{\partial x}-\frac{\partial M}{\partial y} \right)\,\mathrm dA \\&=\int\limits_0^1\int\limits_{0}^{1-x}\left( 4x^3 \right)-x\,\mathrm dy\mathrm dx\\&= \int\limits_0^1 \left[ 4x^3y-xy \right]^{1-x}_0\,\mathrm dx\\&=\int\limits_0^1\left( 4x^3(1-x)-x(1-x) \right)\,\mathrm dx\\&=\int\limits_0^1 4x^3-4x^4-x+x^2\,\mathrm dx\\&=\left[ x^4-\frac{4x^5}{5}-\frac{x^2}{2}+\frac{x^3}{3} \right]_0^1\\&=1-\frac{4}{5}-\frac12+\frac13\\&=\dfrac{1}{30} \end{align*} After evaluating the line integral, I see why my line integral is different than the one of the comments on Instagram @a_generic_nerd2 that shares his solution. This user got $\frac16$ while me $\frac{1}{30}$ , because I making that taking the partial derivative of $x^4$ with respect to $x$ for $N$ and for $M$ , I taking the partial derivative of $xy$ with respect to $y$ , but this user does reverse from what I do on the partial derivative. In your opinion, am I doing wrong? Here's the link that leads to the source I mentioned in this problem.","['integration', 'greens-theorem', 'multivariable-calculus', 'calculus', 'line-integrals']"
4774185,Strapping down a cylinder,"I want to strap down a big heavy cylinder on a flatbed truck. The strap is attached to the truck bed as shown in the picture and also behind. Will the strap slip off as in the next picture? PS. This is a purely geometrical question about the length of the strap and the shape of the cylinder. Please consider that the block cannot move sideways and is blocked in place by some metal ""foot"" at the bottom.",['geometry']
4774191,Proving that there exists a $\xi\in \mathbb{R} $ such that $\ln f(\xi) = \xi$.,"Given a function $f(x)$ that is differentiable over $(-\infty,+\infty)$ , and $|f'(x)| < k f(x)$ for some $0 < k < 1$ .
Prove that there exists a $\xi\in  \mathbb{R}$ such that $\ln f(\xi) = \xi$ . I attempt a proof by contradiction but couldn't proceed. First, we assume that $F(x) = \ln f(x) - x<0$ ( or $>0$ ) consistently. Since $$\frac{f'(x)}{f(x)} = \frac{d}{dx} \ln f(x)$$ we obtain $$|\frac{d}{dx} \ln f(x)| < k < 1,$$ which lead to $F'(x) < 0$ consistently, indicating a strictly decreasing function. However, at this point, I struggle to establish $\ln f(x) < x$ consistently, resulting in a contradiction. I would appreciate it if someone could provide some insights. Thank you very much.","['derivatives', 'analysis']"
4774197,If a closed curve has no antipodal points it must be contained in some open hemisphere,"Whilst doing an exercise, on bounds on total absolute curvature I encountered the following obstacle: Let $ \mathit c$ be a closed curve, whose trace is contained in $\mathbb S^2$ , if c has no antipodal points, does this imply, that c is contained in some open hemisphere?
Intuitively I would argue, that if $ \pmb P, Q$ are the points on the curve that are the furthest apart, one takes the great circle parallel to the line connecting the two points, and then the curve should be contained, in one of the open hemispheres whose equator is formed by this great circle. I'm not sure if the claim is even true, and if yes how to make it rigorous. Any help would be greatly appreciated.","['curves', 'geometry', 'differential-geometry']"
4774203,If $f'(x)$ is continuous at $a$ then prove that $\lim_{h\to 0}\frac{f(a+h)-f(a-h)}{2h}=f'(a)$,"Prove that, $\lim_{h\to 0}\frac{f(a+h)-f(a-h)}{2h}=f'(a)$ if $f'(x)$ is continuous at $a.$ This was how the question was presented in a regional book focusing upon Mean Value Theorems and no other details were given. My solution is as follows: We have, $$\lim_{x\to a}\frac{f(x)-f(a)}{x-a}=f'(a)=\lim_{h\to 0}\frac{f(a+h)-f(a)}{h}=\lim_{h\to 0}\frac{f(a-h)-f(a)}{-h}=\lim_{h\to  0}\frac{f(a)-f(a-h)}{h}.$$ We note that, $$\lim_{h\to 0}\frac{f(a+h)-f(a)-f(a-h)+f(a)}{h}=\lim_{h\to 0}\frac{f(a+h)-f(a-h)}{h}=2f'(a)\implies \lim_{h\to 0}\frac{f(a+h)-f(a-h)}{2h}=f'(a),$$ as required. However, the solution given in the book is, as follows: From Lagrange's Mean Value Theorem, we have, $f(a+h)=f(a)+hf'(a+\theta h),\theta\in (0,1)$ and $f(a)=f(a-h)+hf'(a-\theta ' h),\theta '\in (0,1).$ Adding these relations and rearranging we have, $$f(a+h)-f(a-h)=h(f'(a+\theta h)+f'(a-\theta ' h))\implies \frac{f(a+h)-f(a-h)}{2h}=\frac 12[f'(a+\theta h)+f'(a-\theta ' h)].$$ Taking limit $h\to 0$ on both sides, we have, $$\lim_{h\to 0}\frac{f(a+h)-f(a-h)}{2h}=\frac 12\lim_{h\to 0}[f'(a+\theta h)+f'(a-\theta ' h)]=\frac 12\times 2f'(a)=f'(a).$$ However, I feel the approach given in the book has a major flaw. First of all, how did they apply Lagrange's Mean Value Theorem just like that without checking whether $f$ is continuous at $[a,a+h]$ and $[a-h,a].$ Also, in order to apply this theorem we need to have $f$ being differentiable in $(a,a+h)$ and $(a-h,a).$ They never guaranteed such claims holding true. I think, this makes the proof given in the book incorrect. Also, the information that, $f'(x)$ is continuous at $a$ seems totally useless and unnecessary! Lastly, is my solution at the beginning a valid one?","['real-analysis', 'alternative-proof', 'solution-verification', 'limits', 'derivatives']"
4774232,Compute $f(z)$ and show it's well defined.,"I'm given the function $f$ in integral form as follows $$f(z)=\int_{\gamma} \frac{dw}{w-z},$$ where $\gamma(t)=t, \, 0<t<1$ and $z \notin [0,1]$ . I'm asked to compute this integral and show that $f$ is well defined. So my approach was to compute the integral by definition, $$f(z)=\int_0^1 \frac{dt}{t-z}= \log(1-z)-\log(-z)=\log(|1-z|)-\log(|z|)+i(\arg(1-z)-\arg(-z)).$$ Since $z \notin [0,1]$ there is no risk of the log's of the real part to explode. Now I just have to prove that the imaginary part is not multivalued for any branch of the logarithm I choose. How can I do that? Any hints?","['complex-analysis', 'complex-integration', 'complex-numbers']"
4774339,How can I smoothly connect two points on the surface of a sphere with a pair of arcs passing through the points in certain directions?,"Given two points on the surface of a sphere $\vec a$ and $\vec b$ and directions tangent to the sphere at those points, $\vec {a'}$ and $\vec {b'}$ how can I connect them with a pair of circular arcs that lie in the sphere $x^2 + y^2 + z^2 = 1$ pass through respectively $\vec a$ and $\vec b$ are tangent to $\vec {a'}$ at $\vec a$ and to $\vec {b'}$ at $\vec b$ are tangent to each other There is a family of solutions over which the ratio of the size of the arcs changes. At the extremes the points $\vec a$ and $\vec b$ are connected by a single arc that passes through $\vec a$ and $\vec b$ and is tangent to one of $\vec {a'}$ or $\vec {b'}$ (and you could imagine another arc at the other point of radius $0$ ). I have previously solved this problem on a plane, and the points of intersections of the solutions lie on the circle that passes through both points and is tangent to the two extreme solutions. Let $\vec v$ be a point on the sphere $\vec {c_a}$ be the center of a circle on the sphere passing through $\vec v$ , $\vec a$ , and tangent to $\vec {a'}$ at $\vec a$ $\vec {c_b}$ be the center of a circle on the sphere passing through $\vec v$ , $\vec b$ , and tangent to $\vec {b'}$ at $\vec b$ Then $$
\lVert \vec v \rVert = 1
$$ $\vec v$ and $\vec a$ are on the same circle around $\vec {c_a}$ , and likewise for $\vec b$ . $$
\vec v \cdot \vec {c_a} = \vec a \cdot \vec {c_a}
$$ $$
\vec v \cdot \vec {c_b} = \vec b \cdot \vec {c_b}
$$ The circles are tangent to each other. I'm sure there's a way of saying this without resorting to the $\mathbb R^3$ specific cross product just to construct a basis. $$
(\vec v - \vec c_a) \times \vec c_a = s*(\vec v - \vec c_b) \times \vec c_b
$$ Things I know: A circle: passing through a point $\vec v$ passing through $\vec b$ , and tangent to $\vec {b'}$ at $\vec b$ lies in the plane containing $\vec v$ , $\vec b$ , and $\vec b + \vec {b'}$ . Such a circle is on the sphere if both points are on the sphere and the tangent is tangent to the sphere. The center of a circle on the sphere passing through $\vec a$ tangent to $\vec {a'}$ is located on the plane passing through $\vec a$ and perpendicular to $\vec {a'}$ . $$
\vec {c_a} \cdot \vec {a'} = 0
$$ $$
\vec {c_b} \cdot \vec {b'} = 0
$$ The center of a circle on the sphere passing through $\vec a$ tangent to $\vec {a'}$ is located on the circle halfway between $\vec a$ and the circle around the sphere perpendicular to $\vec {a'}$ at $\vec {a}$ . Single solution If you choose the circle at one end, you can construct a pair of circles that are tangent to it and the other end. Let $\vec c_a$ be the center of a circle $C_a$ tangent to $\vec {a'}$ at $\vec a$ . Project $\vec b$ along $\vec {b'}$ onto the plane of the circle $C_a$ , which we'll call $\vec b_{img}$ . Find lines tangent to the circle and passing through the projected point $\vec b_{img}$ . The points of tangency $\vec v_1$ and $\vec v_2$ are the points were the circle $C_a$ intersect a pair of circles $C_{b1}$ and $C_{b2}$ respectively that are both in the sphere and tangent to $\vec {b'}$ at $\vec b$ . The circles $C_{b1}$ and $C_{b2}$ are the intersections with the sphere of a plane passing through $\vec b$ , $\vec b_{img}$ and one of $\vec v_1$ or $\vec v_2$ . Only one of the two solutions is correct and connects $\vec a$ and $\vec b$ in the directions of $\vec {a'}$ and $\vec {b'}$ , rather than in the opposite direction at one end. The whole family? The intersection points of the family of solutions appear to lie on two circles around the sphere. Something about ellipsoids For a solution with intersection point $\vec v$ , the directions $$
(\vec v - \vec a) \times \vec {a'}
$$ $$
(\vec v - \vec b) \times \vec {b'}
$$ are perpendicular to the planes the circles $C_a$ and $C_b$ lie in. Therefore the direction $$
((\vec v - \vec a) \times \vec {a'}) \times ((\vec v - \vec b) \times \vec {b'})
$$ is perpendicular to both of those planes, and is the direction a tangent to two circles in those two planes must be in. For them to be tangent at $\vec v$ they must be perpendicular to $\vec v$ $$
(((\vec v - \vec a) \times \vec {a'}) \times ((\vec v - \vec b) \times \vec {b'})) \cdot \vec v = 0
$$ If we expand this out as far as possible $$
((\vec v \times \vec {a'} - \vec a \times \vec {a'}) \times (\vec v \times \vec {b'} - \vec b \times \vec {b'}) ) \cdot \vec v = 0
$$ $$
((\vec v \times \vec {a'} - \vec a \times \vec {a'}) \times (\vec v \times \vec {b'}) - ((\vec v \times \vec {a'} - \vec a \times \vec {a'}) \times (\vec b \times \vec {b'}) ) \cdot \vec v = 0
$$ $$
(
  (\vec v \times \vec {a'}) \times (\vec v \times \vec {b'})
- (\vec a \times \vec {a'}) \times (\vec v \times \vec {b'}) 
- (\vec v \times \vec {a'}) \times (\vec b \times \vec {b'})
+ (\vec a \times \vec {a'}) \times (\vec b \times \vec {b'})
) \cdot \vec v = 0
$$ Cubic Term First we deal with the seemingly cubic term $((\vec v \times \vec {a'}) \times (\vec v \times \vec {b'})) \cdot \vec v$ with the vector triple product $\vec u \times (\vec v \times \vec w) = (\vec u \cdot \vec w) \vec v - (\vec u \cdot \vec v) \vec w$ $$
(
((\vec v \times \vec {a'}) \cdot \vec {b'}) \vec v -
((\vec v \times \vec {a'}) \cdot \vec v) \vec {b'}
) \cdot v
$$ We observe that $(\vec v \times \vec {a'}) \cdot \vec v = 0$ $$
((\vec v \times \vec {a'}) \cdot \vec {b'}) \vec v \cdot \vec v 
$$ and $\vec v \cdot \vec v = 1$ . This seemingly cubic term is only linear: $$
(\vec {a'} \times \vec {b'}) \cdot \vec v
$$ Quadratic Terms Next we deal with the quadratic terms $$
- ((\vec a \times \vec {a'}) \times (\vec v \times \vec {b'})) \cdot \vec v
$$ $$ 
+ ((\vec b \times \vec {b'}) \times (\vec v \times \vec {a'})) \cdot \vec v
$$ Which become $$
- (((\vec a \times \vec {a'}) \cdot \vec {b'}) \vec v - ((\vec a \times \vec {a'}) \cdot \vec v) \vec {b'}) ) \cdot \vec v
$$ $$ 
+ ((\vec b \times \vec {b'}) \cdot \vec {a'}) \vec v - ((\vec b \times \vec {b'}) \cdot \vec v) \vec {a'}) ) \cdot \vec v
$$ The first half of each term is a constant, again because $\vec v \cdot \vec v = 1$ . I think the second half of each term can be rearranged to be of the form ${\vec v}^T M \vec v$ where $M$ is an outer product. $$
{\vec v}^T \left[ (\vec a \times \vec {a'}) {\vec {b'}}^T \right] \vec v
$$ $$
{\vec v}^T \left[ (\vec b \times \vec {b'}) {\vec {a'}}^T \right] \vec v
$$ I hoped to be able to read something out of this equation $$
{\vec v}^T \left[
(\vec a \times \vec {a'}) {\vec {b'}}^T + 
(\vec b \times \vec {b'}) {\vec {a'}}^T
\right] \vec v
 + 
(\vec {a'} \times \vec {b'} +
(\vec a \times \vec {a'}) \times (\vec b \times \vec {b'})
) \cdot \vec v
- (\vec a \times \vec {a'})
+ (\vec b \times \vec {b'}) = 0
$$","['circles', 'geometry', 'spherical-geometry']"
4774375,Negative definite forcing of an ODE ? (soft question),"Given an ODE in $\mathbb{R}^n$ $$\frac{d}{dt}x(t)=b(x(t)).$$ How can the assumption that $$\langle D b \xi,\xi\rangle \leq - c |\xi|,~~~~~\forall \xi \in \mathbb{R}^n$$ be interpreted? Does this stop the trajectory going too wild in some sense? What would this condition be called?  (Here $Db$ is the differential of $b$ ).","['coercive', 'positive-semidefinite', 'ordinary-differential-equations', 'functions', 'soft-question']"
4774383,Having troubles to understanding differentiation of multivariable vectors,"What doesn't seem to get around my head is how differentiation with vectors work. I have seen my (physics) teacher differentiate the unitary vector $\vec{u_r}$ using the following: $$\frac{d\vec{u_r}}{dt}=\frac{\partial \vec{u_r}}{\partial r}\dot{r}+\frac{\partial \vec{u_r}}{\partial \theta}\dot \theta+\frac{\partial \vec{u_r}}{\partial \phi}\dot \phi$$ for the  unitary vectors for the spherical coordinates, which depend on those three variables $\vec{u_r}=\vec{u_r}(r(t),\theta(t),\phi(t))$ . But then for the position vector itself, $\vec{r}=r\vec{u_r}$ (by definition), he used the following to differentiate: $$\frac{d\vec r}{dt}=\dot r\vec{u_r}+\dot{\vec{u_r}}r$$ How?? I know that the chain rule for multivariable scalar functions like $f=f(x(t),y(t))$ is $\dot f=\frac{\partial f}{\partial x}\frac{dx}{dt}+\frac{\partial f}{\partial y}\frac{dy}{dt}$ but is that rule also comparable and usable for the vectorial functions? One thing that really helps me is the tree drawing of the derivatives and it's variables, to see if I have to add or not, is there a vectorial function version of that?","['multivariable-calculus', 'derivatives']"
4774405,Show $\sum_{n = 1}^N \frac{\log(n) + 1}{n + 1} = O\left(\log(N)^2\right)$,"I am not good with inequalities or big-O notation. Here is my attempt: It suffices to show that $\sum_{n = 1}^N \frac{\log(n) + 1}{n + 1} \leq \log(N)^2$ . We have \begin{align*}
\sum_{n = 1}^N \frac{\log(n) + 1}{n + 1} &\leq \sum_{n = 1}^N \frac{\log(n) + 1}{n}\\
&= \sum_{n = 1}^N \frac{\log(n)}{n} + \sum_{n = 1}^N \frac{1}{n}\\
&\leq \sum_{n = 1}^N \frac{\log(n)}{n} + 1 + \log(N)
\end{align*} I'm not sure if I'm making the right moves, but at any rate I'm stuck. Thanks.","['asymptotics', 'analysis', 'real-analysis', 'calculus', 'inequality']"
4774415,Probability that a restaurant is better than another,"Consider two restaurants $A$ and $B$ that both have a set of online reviews $S_A$ and $S_B$ , and each review can have an integer score from 1 to 5. What is the probability that a new review for $B$ , $r_B$ , will have a higher score than a new review for $A$ , $r_A$ ? I tried to solve this problem by considering the distribution of reviews for each restaurant, that is $$P(r_B > r_A) = \sum_{i=1}^{5}P(r_A = i)P(r_B > i)$$ Where $P(r_A = i)$ would be the fraction of $S_A$ that is equal to $i$ and $P(r_B > i)$ the fraction of $S_B$ that is bigger than $i$ . However, I've noticed that this is only useful if the number of reviews for both $A$ and $B$ is sufficiently big. That's because if $A$ had 1000 reviews and an average score of 4.8 and $B$ had 1 review with a score of 5, then $B$ , by the above equation, would be considered the best option, even though it has only 1 review. That means there is some uncertainty associated to the distribution of reviews. In this sense, how does one take this uncertainty into account?","['statistical-inference', 'statistics', 'probability-distributions', 'probability']"
4774429,Bacterial growth,"Marco works in a pharmaceutical laboratory and has to study the action
of an antibiotic on the behavior of a certain type of bacterium that duplicates every 15 minutes. a. If there is 1 bacterium at the beginning of the experiment, how many bacteria will be there after 7 hours (assuming
that no factors take over to alter the growth rate)? b. After 7 hours the antibiotic is used on the obtained culture of bacteria; it is observed that from this
instant the number of bacteria in the culture halves every 20 minutes. How many bacteria will be there
after 2 hours since the antibiotic was used?
Leave all results expressed as powers of 2. I have read that for bacterial growth the number of microorganisms in an exponentially growing population is always given by $2^{n}$ , where $n$ is the number of generations. I do not understand this part. The given solution: For part a, generally, the number $N_t$ of cells at time $t$ and the initial number $N_0$ of cells are related by $N_t=N_0\times 2^n. \tag 1$ We have $n=\frac{7\ \text{hours}}{15\ \text{minutes}}=\frac{420\ \text{minutes}}{15\ \text{minutes}}=28,$ so at the end we have $2^{28}$ bacteria. For part b, we have $n'=\frac{20\ \text{hours}}{20\ \text{minutes}}=\frac{120\ \text{minutes}}{20\ \text{minutes}}=6,$ so at the end we have $$2^{28-6}=\frac{2^{28}}{2^6}=2^{22}$$ bacteria. I do not understand the last step above. Is it more correct to write $2^{28}/2^6=2^{22}$ bacteria? Are the above formulas and steps correct? I await other solutions or a different formulation of the problem.","['algebra-precalculus', 'exponential-function']"
4774435,Proof about Markov kernels and absolute continuity,"Assumptions: $(\mathsf{X}, \mathcal{X})$ is a measurable space. $M_n$ and $L_{n-1}$ are Markov probability kernels for $n=2, \ldots, P$ . $\mu_n$ be probability measures on $(\mathsf{X}, \mathcal{X})$ for each $n=2, \ldots, P$ . $\nu$ be a probability measure on $(\mathsf{X}, \mathcal{X})$ with $\mu_1 \ll \nu$ . Does $$
\mu_n(dx_n)\prod_{k=1}^{n-1} L_{k}(x_{k+1}, dx_{k}) \ll \nu(dx_1)\prod_{k=1}^{n-1}M_{k+1}(x_k, dx_{k+1}) \qquad \forall\, n=2, \ldots, P
$$ imply this? $$
\mu_n(dx_n) L_{n-1}(x_n, dx_{n-1}) \ll \mu_{n-1}(dx_{n-1}) M_n(x_{n-1}, dx_n)
$$ I have a feeling it has to do with the chain rule of Radon-Nikodym derivatives and transitivity. Attempt Since the equation below holds $$
\mu_n(dx_n)\prod_{k=1}^{n-1} L_{k}(x_{k+1}, dx_{k}) \ll \nu(dx_1)\prod_{k=1}^{n-1}M_{k+1}(x_k, dx_{k+1}) \qquad \forall\, n=2, \ldots, P,
$$ then there must exist a Radon-Nikodym derivative $$
R_n(x_{1:n}) = \frac{d(\mu_n \otimes \prod_{k=1}^{n-1} L_{k})}{d(\nu \otimes \prod_{k=1}^{n-1}M_{k+1})}(x_{1:n}) \qquad \forall\, n=2, \ldots, P.
$$","['measure-theory', 'markov-chains', 'absolute-continuity', 'probability', 'radon-nikodym']"
4774445,"if $\sum_{n=1}^{\infty}a_n$ converges, then $\sum_{n=1}^{\infty} a_{2n-1}$ converges?","I think this statement is wrong.
Consider $a_1=1, a_2=-1, a_3=1/2, a_4=-1/2, a_5=1/3, a_6=-1/3, \dots$ Is my logic correct?","['calculus', 'convergence-divergence', 'sequences-and-series']"
4774511,"The Expected Value of ""Descending Dice Problem"" and Harmonic Numbers","I named the ""Descending Dice Problem"", but not sure if there is another name to it. This is how it goes: You start with a N sided dice. Throw it, suppose you rolled R as your result, and then throw another dice with R sides. Continue this process until you get a 1. You only roll one dice at a time. Once you rolled a dice and got a number different from 1 you forget about this dice and then only focus on the next one. What is the expected value of dice rolls until I get a 1 if I started with a N sided dice? I searched just for the problem's description and find nothing. Even less about its solution. After some days of thinking I got the exact result for the $N = 2, 3, 4$ cases and started looking for patterns. It seems that the solution is: $E(Dice(n)) = 1 + H(n-1)$ Where $H(n)$ is the n-th Harmonic Number . After checking with some python code the results were pretty much exact. Can you help me prove that this is actually the formula?","['expected-value', 'harmonic-numbers', 'dice', 'probability']"
4774545,Does $f(x)=\frac1x\int_0^x\frac{1}{\sin t+\sin (\pi t)+2}dt$ have a supremum?,"Does $f(x)=\frac1x\int_0^x\frac{1}{\sin t+\sin (\pi t)+2}dt$ have a supremum? (This question was inspired by What is the mean value of $|\sin x +\sin (\pi x)|$ ? .) My thoughts: $\frac{1}{\sin t+\sin (\pi t)+2}$ can be arbitrarily large, but that doesn't tell me whether its average value from $t=0$ to $t=x$ , for all $x\in\mathbb{R}$ , has a supremum. Wolfram does not evaluate $\int \frac{1}{\sin t+\sin (\pi t)+2}dt$ . Here is the graph of $y=\frac1x\int_0^x\frac{1}{\sin t+\sin (\pi t)+2}dt$ from $x=-30$ to $x=30$ .","['definite-integrals', 'real-analysis', 'trigonometry', 'average', 'supremum-and-infimum']"
4774548,"Deduce, the Taylor series expansion of $\arcsin x$ from the Taylor Series expansion of $e^{a\arcsin x}$","Find the Taylor Series expansion of $e^{a\arcsin x}$ and hence deduce, the Taylor series expansion of $\arcsin x$ . I could find the Taylor Series expansion of $e^{a\arcsin x}$ as $$1+ax+\frac{a^2x^2}{2}+\frac{a^3+a}{6}x^3+\cdots $$ However, I have no idea how to deduce the series for $\arcsin x$ from the above expansion. I tried writing, $e^{a\arcsin x}$ as $$1+a\arcsin x+\frac{(a\arcsin x)^2}{2!}+\cdots$$ and comparing the coefficients with the one above, but it was not of any help because , I only got the inference that, $x=\arcsin x$ which was quite obvious. Any help regarding solving this issue will be highly appreciated.","['inverse-function', 'calculus', 'taylor-expansion', 'trigonometry', 'exponential-function']"
4774551,"Prove a hypergeometric Identity: $\frac{\Gamma(\frac14)^2 }{8\sqrt{\pi}}\,_3F_2(.) +\frac1{2\pi}\,_3F_2(.) =\frac{\Gamma( \frac14 )^6}{32\pi^{7/2}}$","Are there any direct ways to prove the following relation between two $\,_3F_2$ s? $$
\frac{\Gamma\left ( \frac14 \right )^2 }{8\sqrt{\pi} } 
\,_3F_2\left ( \frac14,\frac14,\frac12;1,1;1 \right ) 
+\frac{1}{2\pi}\,_3F_2\left ( \frac34,\frac34,1;\frac32,\frac32;1 \right )
=\frac{\Gamma\left ( \frac14 \right )^6}{32\pi^{7/2}}.
$$ The non-direct proof is based on observing the two equalities: \begin{aligned}
&\,_4F_3\left ( \frac14,\frac12,\frac12,\frac12;1,1,\frac54;-1 \right )  = \frac{\Gamma\left ( \frac14 \right )^2 }{8\sqrt{\pi} } 
\,_3F_2\left ( \frac14,\frac14,\frac12;1,1;1 \right ), \\
&\,_4F_3\left ( \frac14,\frac12,\frac12,\frac12;1,1,\frac54;-1 \right )=-\frac{1}{2\pi}\,_3F_2\left ( \frac34,\frac34,1;\frac32,\frac32;1 \right )
+\frac{\Gamma\left ( \frac14 \right )^6}{32\pi^{7/2}},
\end{aligned} where the second one refers to arXiv:1806.08411 (p16). I would appreciate it if you provide me with some ideas.","['hypergeometric-function', 'sequences-and-series']"
4774604,Strong consistency of kernel density estimator,"I am studying the book Nonparametric and Semiparametric Models written by Wolfgang Hardle and have difficulty with the following exercise: $\textbf{Exercise 3.13}$ Show that $\hat{f_h}^{(n)}(x) \xrightarrow
{a.s.}f(x)$ . Assume that $f$ possesses a second derivative and $\Vert
> K \Vert_2 <\infty$ . where $\hat{f_h}^{(n)}(x)=\dfrac{1}{n}\sum\limits_{i=1}^n K_h(x-X_i)$ is the density kernel estimator, $\Vert K \Vert_2=(\int_{\mathbb{R}}K^2(s)ds)^{\frac{1}{2}}$ and we set $h=O(n^{-\frac{1}{5}})$ which is the optimal bandwidth parameter in this exercise. Here is my attempt: I want to use Borel-Cantelli lemma to prove the almost sure convergence. By Chebyshev inequality we have $$
\mathbb{P}(\vert \hat{f_h}^{(n)}(x)-f(x)\vert>\epsilon)\le \dfrac{\mathbb{E}(\hat{f_h}^{(n)}(x)-f(x))^2}{\epsilon ^2}=\dfrac{MSE(\hat{f_h}^{(n)}(x))}{\epsilon^2},
$$ and it is known that $$
MSE(\hat{f_h}^{(n)}(x))=\dfrac{f(x)}{nh}\Vert K \Vert_2^2+\dfrac{1}{4}(f''(x))^2h^4(\mu_2(K))^2+o(h^4)+o(\dfrac{1}{nh})
$$ where $\mu_2(K)=\int_{\mathbb{R}}s^2K(s)ds$ . By replacing $h$ with $h_{opt}=O(n^{-\frac{1}{5}})$ we derive that $$MSE(\hat{f_h}^{(n)}(x))=O(n^{-\frac{4}{5}})$$ However, $\sum\limits_{n=1}^{\infty} n^{-\frac{4}{5}}=\infty$ and I was stuck here. Can anyone help me with this?","['nonparametric-statistics', 'statistics', 'probability-limit-theorems', 'strong-convergence']"
4774630,"Best approximation of an irrational number by primes $2,3,5,7$","Let $G$ be the multiplicative group $(\mathbb{R}_+,\cdot)$ , which is the positive real numbers equipped with the usual distance. Let $P$ be the subgroup of $G$ generated by $2,3,5,7$ , i.e. $$P = \bigcup\limits_{n=1}^\infty\{2,3,5,7,\frac12,\frac13,\frac15\frac17\}^n$$ I have a few questions. Is the subgroup $P$ dense in $(\mathbb{R}_+,\cdot)$ ? What is the best approximation of $\sqrt2$ or $\pi$ in $P$ given the constraint that, for example, the total number of primes used is less than $1000$ ? I'm allowed only to use $2,3,5,7$ and no more. I know that a nontrivial additive subgroup of $(\mathbb R,+)$ is either dense or discrete, so we can transform $2,3,5,7$ into $\log 2,\log3,\log5,\log7$ and consider the subgroup of $(\mathbb R,+)$ generated by $\log 2,\log3,\log5,\log7$ , which I believe is dense. Then we can approximate $\log \sqrt2=\frac12\log2$ by an integer combination of these elements. However, I've done some simulations and the best approximation I can find is $\frac{7^3}{3^5}\approx 1.411522633744856$ , which makes me suspect it is not dense. Where did I go wrong? Does there exist an algorithm for finding such combinations? Edit: I just found out that $\sqrt2$ is a very special case because $\log\sqrt2=\frac12\log2$ . What if I want to approximate $\pi$ ? I used @lulu's method and got $$3^a=2^b\pi\implies a\log3=b\log2+\log\pi\implies \frac{a}{b+\frac{\log\pi}{\log2}}=\frac{\log2}{\log3}$$ Or I want to use all of $2,3,5,7$ . $$2^a3^b5^c7^d=\sqrt2$$ $$a\log2+b\log3+c\log5+d\log7=\frac12\log2$$ $$2b\log3+2c\log5+2d\log7=(1-2a)\log2$$ $$\frac{2b+2\frac{\log5}{\log3}c+2\frac{\log7}{\log3}d}{1-2a}=\frac{\log2}{\log3}$$ but I couldn't proceed.","['number-theory', 'group-theory', 'abstract-algebra', 'approximation']"
4774678,Help with solving 3 simultaneous sine equations.,"Is it even possible to solve $3$ sine equations with $3$ unknowns? I tried to do so, but i always reach a dead end. For example, i have these $3$ equations that I am stuck on: $2.3 = \sin(2.93a-b)+c$ $1.91 = \sin(3.55a-b)+c$ $1.99 = \sin(3.93a-b)+c$ I've tried subtracting equation $1$ from equation $2$ , and then applying various trig rules. However, I didn't solve much this is what I've tried to do so far If it is not possible to solve a sine system of equations, why is that so?","['trigonometry', 'systems-of-equations']"
4774691,Prove $\int_{0}^{1}f^{3}(x)dx\geq 4\int_{0}^{1}x^{2}f(x)dx\int_{0}^{1}xf^{2}(x)dx$,"Let $f:[0,1] \to [0,\infty)$ continuous function. Show that $$\int_{0}^{1}f^{3}(x)dx\geq 4\int_{0}^{1}x^{2}f(x)dx\int_{0}^{1}xf^{2}(x)dx$$ This problem is from American Mathematical Monthly, 2015. I proved that $$\int_{0}^{1}x^{2}f(x)dx-\int_{0}^{1}xf^{2}(x)dx\leq \frac{1}{16}$$ This is a result from a Putnam problem. By Cauchy-Schwarz ineqaulity (using the fact that $f$ is positive), it is easy to show $$\int_{0}^{1}x^{2}f(x)dx\int_{0}^{1}f^{3}(x)dx\geq \left ( \int_{0}^{1}xf^{2}(x)dx \right )$$ and $$\int_{0}^{1}f^{3}(x)dx\geq \int_{0}^{1}f^{2}(x)dx\int_{0}^{1}f(x)dx$$ I don't know if these results are helpful for this problem. Another interesting thing that I found is $$\int_{0}^{1}xf^{2}(x)dx-\int_{0}^{1}f^{3}(x)dx\leq \frac{1}{12}$$ Maybe that $4$ in the main ineqality is from a quadratic equation discriminant? Maybe we need to make some parameters and use the Cauchy-Schwarz inequality? Can somemone give me a hint for this problem? I would appreciate!","['integration', 'inequality', 'cauchy-schwarz-inequality', 'definite-integrals']"
4774710,"A ""New"" Special Point in a Triangle.","I was playing with the software Geometry Expressions and I was exploring generalizations of special points in triangles (centroid, orthocenters, etc.) when I stumbled upon this construction. J is always located at the intersection of the three lines but can either be inside or outside the triangle depending upon its shape.
If one positions A"",B"",C"" at the critical distance from C,B, A, one can construct a second point J"". J and J'on the same figure: Different triangles, with J inside or outside: Proof: The way I constructed the point J:
Position A',B',C' such that: $$ \overrightarrow{CA'} =k \overrightarrow{CA} $$ $$ \overrightarrow{BB'} =k \overrightarrow{BC} $$ $$ \overrightarrow{AC'} =k \overrightarrow{AB} $$ Define J: $$ \overrightarrow{BC'}  .  \overrightarrow{C'J} =0 $$ $$ \overrightarrow{AA'}  .  \overrightarrow{A'J} =0 $$ $$  \overrightarrow{BB'}  .  \overrightarrow{B'J} =0 $$ Solve for k and find: $$ k= \frac{ a^{2} +b^{2}+c^{2}}{2a+2b+2c} $$ Properties Following ""MyMolecule""'s hint, O, circumcenter of the triangle is the midpoint of JJ"".(A is not on the line).
The software returns for the distance OJ: $$OJ=\dfrac{\sqrt{a}\cdot \sqrt{b}\cdot \sqrt{-c^{5}+c^{4}\cdot (2\cdot a+2\cdot b)+c^{3}\cdot \left (-2\cdot a^{2}-a\cdot b-2\cdot b^{2}\right )+c^{2}\cdot \left (2\cdot a^{3}-a^{2}\cdot b-a\cdot b^{2}+2\cdot b^{3}\right )+c\cdot \left (-a^{4}+2\cdot a^{3}\cdot b-2\cdot a^{2}\cdot b^{2}+2\cdot a\cdot b^{3}-b^{4}\right )}}{(a+b+c)\cdot \sqrt{(a+b-c)\cdot (a-b+c)\cdot (-a+b+c)}}$$ Let I be the incenter. A"", B' are concyclic and both belong to the circle of center I. More generally, the center of any circle passing through A"", B' is located on the line IC. Have J,J' ever been named in classical geometry? Is there a theory about all the points that can be constructed with a straight edge and a compass out of a triangle (intersection of three lines, three circles, etc.)?","['euclidean-geometry', 'triangles', 'triangle-centres', 'geometry']"
4774739,"Prove that if every sequence of functions converge everywhere also converges in measure, the measure is countably additive","The Varadhan's probability theory text mentions that countable additivity is key for a sequence of functions that converges almost everywhere to converge in measure (The proof is on p.12 here ). Now I am curious how to prove the other direction: if every sequence of functions that converge almost everywhere also converges in measure, then the measure is countably additive. The approach I am thinking of is to use reductio: first assume that every $f_n$ that converges to $f$ almost everywhere also satisfies $f_n \rightarrow f$ in measure, and then assume the probability measure $P$ on the probability space $\langle \Omega, \Sigma, P \rangle$ is not countably additive, and we try to derive a contradiction. In fact, there is an answer giving an example of a sequence of functions that converges a.e. but does not converge in measure when the measure is not countably additive. (And this does seem to fit in with my reductio approach.) Does this suffice to prove the result we want? Is there any approach that does not rely on a counterexample?","['measure-theory', 'convergence-divergence', 'probability-theory', 'probability']"
4774772,Determination of circle given diameter and a point,"I am doing my mathematics homework on circles.
One of the questions asks to find the equation of circle(s) given that it touches the x-axis, passes through $(1, 1)$ and has the line $x + y = 3$ as diameter.
We may solve this question, but while drawing the diagram I asked if only one such circle exists? My question is to prove the following: Given equation of a straight line and an arbitrary point not on the straight line, a unique circle exists such that it passes through the point and the chord cut on the straight line by the circle is its diameter. Is it really the case or infinite circles pass or does it depend on something? The point of finding how many circles exist  is that, the question asks to find equation of circle(s). Notice the (s) .","['analytic-geometry', 'circles', 'geometry']"
4774833,Do the Exponent Properties Apply in Constructive Mathematics?,"I have been studying smooth infinitesimal analysis which depends entirely on the set $\Delta$ of numbers whose square (and all higher powers) are equal to $0$ . Now, I know smooth infinitesimal analysis is a branch of constructive mathematics, and I was just wondering if the properties of exponents like, $a^x \cdot a^y = a^{x+y}, (a^x)^y = a^{xy}, \text{ and } a^{1/n} = \sqrt[n]{a}$ would hold if $a \in \Delta$ . Also, is this a valid proof of $\epsilon = 0$ where $\epsilon \in \Delta$ and $\epsilon \geq 0$ : Assume $\epsilon \geq 0$ is an element of $\Delta$ . Clearly, $\epsilon^1=\epsilon^{2/2} = \sqrt{\epsilon^2}$ . But, $\epsilon^2 = 0$ as $\epsilon \in \Delta$ and $x,y \in \mathbb{R}$ , so we have $\epsilon = 0$ . I do not have a great amount of experience in constructive mathematics and have only learned smooth infinitesimal analysis because of its applications (like deriving the formula for the surface area of a sphere) up to this point, so any guidance on how to properly apply constructive mathematics would be useful. I am also new to the site, so I will gladly accept advice on how to improve my question.","['constructive-mathematics', 'analysis']"
4774834,How can I show that these two topologies are the same?,"$G$ is a topological group and $H$ an open subgroup of $G$ I want to show that the quotient topology of $G/H$ , $\tau_{G/H}$ is equal to the discrete topology $\tau_{\text{discrete}}$ . I mean clearly we know that $\tau_{G/H}\subseteq \tau_{\text{discrete}}$ since $\tau_{\text{discrete}}$ is the finest topology. But now to show the other implication, I'm a bit lost. Since in $\tau_{\text{discrete}}$ every subset of $G$ is open I know that if $U\in \tau_{\text{discrete}}$ , then $U\subset G$ . But to show that $U\in \tau_{G/H}$ , I need to show that $\pi^{-1}(U)\subset G$ is open. Where $\pi:G\rightarrow G/H$ is the projection. But now I'm confused what topology we have on $G$ because since it is a topological group I only know that the product map and the inverse map are continuous. Can someone help me?","['general-topology', 'analysis', 'quotient-spaces']"
4774836,Sharp constant in the $L^p$ regularity estimate?,"Problem : Let us denote $\mathbb{W}^{2,p}(\mathbb{R}^2)$ the space of Sobolev functions in the plane. Let us denote with $\Delta$ the classic Laplacian operator. We know that there exists a constant $C>0$ , depending only on $p$ and $d$ , such that if $f \in \mathbb{L}^p(\mathbb{R}^2)$ , $u \in \mathbb{W}^{2,p}(\mathbb{R}^2)$ and: $$
-\Delta u = f
$$ in the weak sense, then it holds: $$
\|\partial_i \partial_j u \|_{\mathbb{L}^{p}(\mathbb{R}^2)} \leq C \|f \|_{\mathbb{W}^{2,p}(\mathbb{R}^2)} \quad \text{ for all } i,j \in \{1,2\}
$$ I want to find the sharp constant $C$ with this property. In other words I want to find the least possible constant $C$ such that the property above holds. Attempt: I tried using Fourier theory and the Hardy–Littlewood–Sobolev theorem but this does not work if $\alpha=2$ is equal to the dimension $d=2$ , where $\alpha$ is the power associated to the $\Delta$ operator seen as a Fourier multiplier. Any help or any reference will be appreciated.","['fourier-analysis', 'poissons-equation', 'laplacian', 'sobolev-spaces', 'functional-analysis']"
4774840,Symmetric algebra on a graded vector space,"Let $G$ be an abelian group and let $V=\oplus_{g \in G}V_g$ be a $G$ -graded vector space over $\mathbb C$ . Then what is the symmetric algebra on $V$ ? What is a canonical basis for the symmetric algebra on $V$ ? When $G=\mathbb Z_2$ , then $V=V_0 \oplus V_1$ is a super vector space and $Sym(V) \cong Sym(V_0) \otimes \Lambda (V_1)$ .","['representation-theory', 'algebraic-geometry', 'abstract-algebra', 'linear-algebra', 'commutative-algebra']"
4774881,An identity related to $q$-series,"While studying Ramanujan's theta functions, I encountered a q-series $(q;q)_\infty^2\phi(q)$ . I calculated the first few terms of $(q;q)_\infty^2\phi(q)$ and observed that it seems to have the following form: $$(q;q)_\infty^2\phi(q)=\sum_{\genfrac{}{}{0pt}{}{n\geq1}{n\ \text{odd}}}\left(\frac n3\right)nq^{(n^2-1)/12},\tag1$$ where $(q;q)_\infty=\prod_{n=1}^\infty(1-q^n)$ and $\phi(q)=\sum_{n=-\infty}^\infty q^{n^2}$ . I have attempted to find a reference for this series but have not made any progress. My question is: How to prove (1)? Any comments will be appreciated.","['q-series', 'number-theory', 'theta-functions']"
4774904,Ito integral of $\int_0^t B_{s/2} d B_s$,"Are the more or less closed formula for Ito integral $$
\int_0^t B_{s/2} d B_s
$$ where $B_s$ is standard 1-dimensional Brownian motion? For example, $\int_0^t B_s d B_s = (B_t^2 - t) / 2$ , are there similar result for integral above?","['stochastic-integrals', 'brownian-motion', 'probability-theory', 'stochastic-calculus']"
4774906,Is there a polynomial whose cube is a symmetric polynomial?,"The discriminant of variables $x_1, \dots, x_n$ , $$d=d(x_1, \dots, x_n)=\prod_{i<j}(x_i-x_j)$$ is a classical example of a polynomial whose square $d^2$ is symmetric in its entries. This turns out to be very useful for understanding what do some field extensions look like. So the question is: Does there exist a polynomial $p(x_1, \dots,x_n)$ which is not symmetric but $p^3$ is? More generally, for some $k>1$ , is there a polynomial such that $p, p^2, \dots, p^{k-1}$ are not symmetric and $p^k$ is?","['abstract-algebra', 'symmetric-polynomials']"
4774925,Geometric interpretation of a generalized Euler sequence,"Although similar arguments work for $\mathbb P^n$ for any $n$ , let us deal with $X=\mathbb P^1_k$ for simplicity. Recall there is a so-called Euler sequence on $X$ (twisted by $\mathcal O_X(1)$ ): $$ 0 \to \Omega_X(1) \to \mathcal O_X \oplus \mathcal O_X \to \mathcal O_X(1) \to 0.$$ Now, consider projections $p_1,p_2:X \times_kX\to X$ and the following maps $$p_1^*(\mathcal O_X \oplus \mathcal O_X) \to p_1^*\mathcal O_X(1), \quad p_2^*\Omega_X(1) \to p_2^*(\mathcal O_X \oplus \mathcal O_X),$$ which yields a map $p_2^*\Omega_X(1) \to p_1^*\mathcal O_X(1)$ and hence a map $$\mathcal E:= p_2^*\Omega_X(1) \otimes_{\mathcal O_{X\times X}}p_1^*\mathcal O_X(-1) \to p_1^*\mathcal O_X(1) \otimes_{\mathcal O_{X\times X}}p_1^*\mathcal O_X(-1) = \mathcal O_{X\times_k X}.$$ Beillinson showed the map fits into the following short exact sequence ( e.g. ): $$0 \to \mathcal E \to \mathcal O_{X\times_k X} \to \mathcal O_{\Delta} \to 0$$ where $\Delta \subset X\times_k X$ denotes the diagonal. Now, since everything is flat, we get the following distingushed triangle $$ 
\mathcal E \to \mathcal O_{X\times_k X} \to \mathcal O_{\Delta} \to \mathcal E[1]
$$ in $\operatorname{Perf} X$ and for any $\mathcal F \in \operatorname{Perf}X$ , we get the following distingushed triangle $$
\Phi_{\mathcal E}(\mathcal F) \to \Phi_{\mathcal O_{X\times_k X}}(\mathcal F) \to \Phi_{\mathcal O_{\Delta}}(\mathcal F) \to \Phi_{\mathcal E}(\mathcal F)[1]
$$ where $\Phi$ denotes the Fourier-Mukai transform. Now, by the projection formula and flat base change (e.g. Section III.2 ), the distingushed triangle is isomorphic to the following: $$
\oplus_{i \in \mathbb Z}H^i(X,\mathcal F(-1))[-i] \otimes_k \Omega_X(1) \to \oplus_{i \in \mathbb Z}H^i(X,\mathcal F)[-i] \otimes_k \mathcal O_X \to \mathcal F \overset{+1}{\to}. 
$$ In particular, if we apply this to $\mathcal F = \mathcal O_X(k)$ (say $k\geq 2$ ), then we get $$
H^0(X,\mathcal O_X(k-1))\otimes_k\Omega_X(1) \to H^0(X,\mathcal O_X(k))\otimes_k \mathcal O_X \to \mathcal O_X(k) \overset{+1}{\to},
$$ which looks like some generalization of the Euler sequence. So, my questions are: Is this indeed a generalization of the Euler sequence in geometric sense? If so, is there a nice way to obtain the triangle without using those heavy machines? Thank you in advance!","['homological-algebra', 'algebraic-geometry', 'commutative-algebra', 'derived-categories']"
4774970,The isomorphism theorem of measure theory for atomic measures.,There is the famous isomorphism theorem for measure spaces: All separable nonatomic normalized measure algebras are mutually isomorphic. what about an atomic normalized measure algebras? Is it possible to establish an isomorphism or a measure-preserving transformation from an atomic normalized measure algebra into the measure algebra of the unit interval? These theorems would be helpful in understanding my qeustion. Any help would be appreciated.,"['measure-theory', 'lebesgue-measure', 'ergodic-theory', 'analysis', 'functional-analysis']"
4775006,Does this prove the union of countably many countable sets is countable?,"I am wondering if the following argument proves that the union of countably many sets is countable. Number each set  in the union by a number of zeroes, assigning an arbitrary set $0$ , the next set we pick $00$ , the next $000$ etc. And then assign each element in each of the sets a number of $1$ 's in the same manner- and then have an injection from this union and the set of all binary strings, by our function conjoining the set whence an element came and its position in that set to get a binary string (if an element is in multiple sets we arbitrarily pick one set and its position therein to map it to). Then this union is countable due to the set of finite binary strings being countable. I am unsure because proofs of this statement I have seen, such as this , use an argument which is more complicated, leading me to believe there is a fallacy in this seemingly simpler argument.","['elementary-set-theory', 'solution-verification', 'discrete-mathematics']"
4775108,"Proving $n$ subsets $A_1, ..., A_n$ of size $\geq 2$ must pairwise intersect.","Let $A_1, ..., A_n \subseteq [n]$ be $n$ subsets of $[n]$ with $|A_i|\geq 2$ . Suppose further that for every $B \subseteq [n], |B|=2$ , that there exists a unique $i$ with $B\subseteq A_i$ . Prove that $A_i \cap A_j \neq \emptyset $ for all $i,j$ . Attempt: I can get the equality: $$\sum_{i=1}^n {|A_i| \choose 2} = {n \choose 2}$$ Using a double counting argument, but not sure how to use that to prove $A_i, A_j$ intersect for all $i,j$ .","['pigeonhole-principle', 'combinatorics', 'extremal-combinatorics']"
4775113,Extension of equivalent norm in subspace to the whole space,"I'm trying to prove the following result: Let $(X, \|\,\cdot\,\|_X)$ be a normed space and $Y$ a subspace of $X$ . If $\|\,\cdot\,\|_{YY}$ is an equivalent norm to $\|\,\cdot\,\|_Y$ (the inherited norm from $X$ ) in $Y$ , then there exists a norm $\|\,\cdot\,\|_{XX}$ in $X$ which is equivalent to $\|\,\cdot\,\|_X$ and induces the norm $\|\,\cdot\,\|_{YY}$ into $Y$ . I've checked the proof in several books and post in this forum and it is esentially the following argument: As $\|\,\cdot\,\|_Y$ and $\|\,\cdot\,\|_{YY}$ are equivalent in $Y$ , there exists a $C>0$ such that $B_X \cap Y \subset C B_{YY}$ , where $B_X,B_X \cap Y$ and $B_{YY}$ are respectively the closed unit ball in $X$ , the closed unit ball in $Y$ with the inherited norm, and the closed unit ball in $Y$ for the norm $\|\,\cdot\,\|_{YY}$ . Defining $$B_{XX}=\text{conv}\left\{\dfrac{1}{C} B_X \cup B_{YY}\right\}$$ it is easy to see that it is convex, balanced, bounded and it has zero as an interior point. So, Minkowski's functional for this set is an equivalent norm in $X$ , which we will call $\|\,\cdot\,\|_{XX}$ , such that is closed unit ball is the closure of $B_{XX}$ . Lastly, it is seen that $B_{XX} \cap Y = B_{YY}$ and from that, every proof concludes that the norm $\|\,\cdot\,\|_{XX}$ induces in $Y$ the norm $\|\,\cdot\,\|_{YY}$ . I understand every setp in the proof but there is something in the last step that I don't see and I don't know if it is a mistake. To prove that the norm defined by Minkowski's functional induces in $Y$ the desiered norm we should check that $$\text{cl}(B_{XX}) \cap Y = B_{YY}$$ Because $\text{cl}(B_{XX})$ is the closed unit ball in $(X, \|\,\cdot\,\|_{XX})$ , not $B_{XX}$ . If $B_{XX}$ was a close set everything would be fine but I cannot prove it and, in fact, I don't see why it should be true in the case of infinite-dimensional spaces. Another option would be that $B_{XX} \cap Y = \text{cl}(B_{XX}) \cap Y$ , but I cannot proof this (without assuming that $B_{XX}$ is closed of course). In no proof of this fact is there any mention to this problem in the proof. Am I skipping something? Thanks for the answers.",['functional-analysis']
4775121,Non abelian group with some nice properties,"Are there examples of finite non-abelian groups of order $k$ where the number of conjugacy classes is $O(\log{k})$ ? Or is there any reason this is not obviously possible? I know that there lower bounds on the number of conjugacy classes as a function of the order of the group (see here ). Thanks in advance! EDIT: I'm also interested in groups where the number of conjugacy classes grows slowly as a function of the order, i.e, $\kappa(G) = O(\log^c{k})$ for any constant $c$ that is independent of other parameters of the group.","['group-theory', 'representation-theory']"
4775189,Prove that there is no holomorphic map $s: \mathbb{P}^n \to \mathbb{C}^{n+1} \setminus \{0\}$ with $p \circ s = \operatorname{id}$.,"Let $p:\mathbb{C}^{n+1} \setminus \{0\} \to \mathbb{P}^n$ be the natural projection. Prove that there is no holomorphic map $s: \mathbb{P}^n \to \mathbb{C}^{n+1} \setminus \{0\}$ with $p \circ s = \operatorname{id}$ . I'm seeking for intuition for the problem. I started to think about what the map $s$ would  have to be in order for this to hold and for example if $s([z_0:\dots:z_n]) = (z_0,\dots,z_n)$ , the the map would give the identity, but this $s$ isn't even well-defined  let alone holomorphic since $[z_0:\dots:z_n]=[\lambda z_0:\dots:\lambda z_n]$ for any non-zero $\lambda$ , but clearly $(z_0,\dots,z_n) \ne (\lambda z_0,\dots, \lambda z_n) $ unless $\lambda = 1$ . Now my understanding of the projective space is very limited so I don't have any intuition as to why such map wouldn't exist and I would appreciate if anyone could shed some light in to why this shouldn't be true?","['complex-analysis', 'projective-geometry', 'complex-numbers', 'projective-space']"
4775193,"For which set $A$, Alice has a winning strategy?","Alice and Bob are playing a game. They take an integer $n>1$ , and partition the set $\{1,2,...n\}$ into two non-empty subsets $A,B$ . Alice takes the set $A$ and Bob takes the set $B$ . They take a paper and write $0$ on it. Alice plays first, and the rules are: Each turn, let $x$ be the number written on the paper. The player has two choices: Erase $x$ and rewrite $0$ on the paper, or Choose a number $y>x$ to remove from their set, then erase $x$ and write $y$ on the paper. The game ends if a player's set becomes empty, and that player wins the game. Question: For which subsets $A$ of $\{1,2,...n\}$ does Alice have a winning strategy, and what is the number of such sets? I think the best strategy for both players is to choose the smallest element possible that can be played from their set, except if the other player's set has only $1$ element, in which case they choose the largest element in their set. But I can't prove it or describe the winning sets for Alice using that strategy. Update: The above strategy doesn't work, as @aschepler points out: for $n=6$ , $A=\{1,4,5\}$ , $B=\{2,3,6\}$ , Alice wins by initially choosing $4$ or $5$ , but not $1$ .","['game-theory', 'combinatorics']"
4775217,Find integers $x$ that satisfy $\tan{x}>10^{30}$,"I have a positive integer $x$ . I want to find all values of $x$ that satisfy the following inequality: $$\tan{x}>10^{30}$$ I don't have much experience with number theory. How would one normally approach a problem like this? I tried brute forcing it by calculating $\tan(x)$ for increasing values of $x$ , but the largest value I found was $\tan(122925461)\approx 10^9$ , which is far smaller than the required $10^{30}$ . Is there a more mathematical approach that could solve this problem?",['number-theory']
4775219,Does this simple branching random walk on $\mathbb Z$ satisfy a central limit theorem?,"Consider the following simple branching random walk on $\mathbb Z$ in discrete time: At stage 0, we start with one token at the site 0. At each time step, we randomly, independently split each token into either 2 tokens to the left, or 3 tokens to the right with probabilities $\frac35$ , $\frac25$ respectively. (I.e., a given token at site $p$ will split, uniformly randomly into either $2$ tokens on the site $p-1$ , or $3$ tokens on the site $p+1$ .) Denote the number of tokens at site $p$ at the $n$ th stage by $X_n(p)$ , and denote the total number of tokens by $|X_n|$ . Question: Does the observed distribution of tokens at the $n$ th stage, $$\mu_n = \frac1{|X_n|} \sum_{p \in \mathbb Z} X_n(p) \delta_{p},$$ almost-surely converge to a Gaussian distribution? (After a suitable rescaling.) My numerical play suggests that it does, but I'm at a loss as to how to show it.","['branching-process', 'probability-limit-theorems', 'probability-theory', 'random-walk']"
4775224,Root test application question (usefulness of root test for the series $\sum 1/n^n$),"Is there a simple way to conclude whether the following series converges or diverges using the ratio test? $$
\sum_{n=1}^\infty   \frac{1}{n^n}
$$ My level is multi-variable calculus. ( Note : I know this can be easily found with the root or comparison tests)","['multivariable-calculus', 'calculus', 'sequences-and-series']"
4775235,$L^2$-ness of the finite difference operator,"Is there a proof of the fact that the operator $\mathfrak{D}$ defined by $\mathfrak{D}[f](x,y):=\dfrac{f(x)-f(y)}{x-y}$ is continuous from $H^n(\mathbb{R})$ to $H^m(\mathbb{R}^2)$ or even $L^2(\mathbb{R}^2)$ assuming that $n$ is sufficiently big (i guess $n\geq m+1$ would suffice) ? Some insights why there exists $C>0$ such that for all $f\in H^n(\mathbb{R})$ , $\|\mathfrak{D}[f]\|_{L^2(\mathbb{R}^2)}\leq C \|f\|_{H^1(\mathbb{R})}$ might be very useful to guess how to show the result for derivatives of $\mathfrak{D}[f]$ . Also some formula for the derivatives of $\mathfrak{D}[f]$ hold, it basically has the form of a sum of Taylor sums divided by $(x-y)^{\alpha-j}$ where $\alpha=m_1+m_2+1$ where $m_1$ (resp. $m_2$ ) is the order of the derivative with respect to $x$ (resp. $y$ ).","['operator-theory', 'normed-spaces', 'compact-operators', 'functional-analysis']"
4775337,"Is this function differentiable at $(1,0)$?","Let $f:\mathbb{R}^{2}\to\mathbb{R}^{2}$ be defined by $$f(x,y)=\left\{ \begin{array}[ll] (\left(\dfrac{\sin(x^2 + y^2 -1)}{x^2+y^2-1}, \cos(x^2 + y^2 -1)\right), & \mbox{if }x^2+y^2-1\neq0 \\ (1,1), & \mbox{if  }x^2+y^2-1=0\end{array} \right.$$ Is $f$ differentiable at $(1,0)$ ? I've already proved that $\dfrac{\partial f}{\partial x}(1,0)=(0,0)$ and $\dfrac{\partial f}{\partial y}(1,0)=(0,0)$ . I know the next step is to prove the continuity of $\dfrac{\partial f}{\partial x}$ and $\dfrac{\partial f}{\partial y}$ , but I don't know how to deal with the limit of two variables with such complicated form like this. Could anyone advise? Thank you.","['limits', 'multivariable-calculus', 'derivatives', 'analysis']"
4775346,Find the limit in $\mathbb R^2$,"Let $x_1 = y_1 = 0$ and $$
(x_{n+1}, y_{n+1}) = \bigg(\bigg(1-\frac{2}{n}\bigg)x_n -\frac{y_n}{n} + \frac{4}{n}, \bigg(1-\frac{1}{n}\bigg)y_n - \frac{x_n}{n} + \frac{3}{n}\bigg).
$$ Find the $\lim_{n \to \infty} (x_n, y_n)$ . Unfortunately, I have no idea how to solve this problem. Expressions like $$
x_{n+1} - x_n = -3\frac{x_n}{n} - \frac{1}{n}\sum_{k = 1}^{n-1} \frac{x_k}{k} + \frac{H_{n-1}}{n} + \frac{3}{n},
$$ that can be derived from the definition of a sequence do not seem to be helpful. $H_n$ is $n$ -th partial sum of Harmonic series. Would be happy to see some suggestions.","['limits', 'analysis']"
4775352,All DFT of binary numbers subsets of prime length are nonzero,"Let $p$ be a prime. Consider a sequence $S$ of $p$ binary numbers $x_n \in \{ 0, 1 \}$ , i.e. $S = \{x_1, x_2, \cdots, x_p\}$ , where the number of zeroes in $S$ is neither $0$ nor $p$ .  Then the conjecture is: for all such sequences, all discrete fourier transformation (DFT) components $\hat S_k$ of $S$ are nonzero. That is, show that, $\forall k = 1\cdots p$ , one obtains $\hat S_k = \sum_{n=1}^p x_n \exp (2 \pi i \frac{ n k }{p}) \ne 0$ . A way to the solution might be the visualization that a vectorial sum of $p$ evenly distributed spokes in a wheel (the unit circle) is performed, where only those spokes with $x_n = 1$ are existent, and where every $k$ th spoke (modulo $p)$ is counted, and the vector sum will not disappear. It is  clear that the conjecture only holds if $p$ is prime. Since if it weren't, write $p= q \cdot r$ , and consider a sequence $S^q$ where every $q$ th entry is $x_n = 1$ and all others are $0$ . The DFT components $\hat S^q_k$ of $S^q$ will then be $$\hat S^q_k = \sum_{n=1}^p x_n \exp (2 \pi i\frac{ n k }{q r}) =  \sum_{n=1}^r \exp (2 \pi i \frac{ n q k }{q r})  =  \sum_{n=1}^r \exp (2 \pi i \frac{ n  k }{r})  =  0.$$","['elementary-number-theory', 'algebraic-graph-theory', 'fourier-analysis', 'discrete-mathematics']"
4775378,Compute $\sum^{2024}_{k=1} \frac{2023-2022 \cos \left(\frac{\pi(2k-1)}{2024} \right)}{2021-2020 \cos \left(\frac{\pi(2k-1)}{2024} \right)}$,"Question: Compute $$\sum^{2024}_{k=1} \frac{2023-2022 \cos \left(\frac{\pi(2k-1)}{2024} \right)}{2021-2020 \cos \left(\frac{\pi(2k-1)}{2024} \right)}$$ I began by rearranging the sum as follows: $$\sum^{2024}_{k=1} \frac{2023-2022 \cos \left(\frac{\pi(2k-1)}{2024} \right)}{2021-2020 \cos \left(\frac{\pi(2k-1)}{2024} \right)}$$ $$=\sum^{2024}_{k=1} \left(1+\frac{2-2\cos \left(\frac{\pi(2k-1)}{2024} \right)}{2021-2020 \cos \left(\frac{\pi(2k-1)}{2024} \right)} \right)$$ $$=2024+\frac{1}{1010} \sum^{2024}_{k=1} \frac{2020-2020\cos \left(\frac{\pi(2k-1)}{2024} \right)}{2021-2020 \cos \left(\frac{\pi(2k-1)}{2024} \right)}$$ $$=2024+\frac{1}{1010} \sum^{2024}_{k=1} \left(1-\frac{1}{2021-2020 \cos \left(\frac{\pi(2k-1)}{2024} \right)} \right)$$ $$=\frac{1011 \cdot 1012}{505}-\frac{1}{1010} \sum^{2024}_{k=1} \frac{1}{2021-2020 \cos \left(\frac{\pi(2k-1)}{2024} \right)}$$ $$=\frac{1011 \cdot 1012}{505}-\frac{1}{2 \cdot 1010^2} \sum^{2024}_{k=1} \frac{1}{\frac{2021}{2020}-\cos \left(\frac{\pi(2k-1)}{2024} \right)}$$ So, I then turned my attention to trying to find $\sum^{2024}_{k=1} \frac{1}{\frac{2021}{2020}-\cos \left(\frac{\pi(2k-1)}{2024} \right)}$ . Complex numbers I considered $z_k=e^{\frac{i \pi (2k-1)}{2024}}$ then observed $(z_k)^{2024}+1=0$ . Then, I (mistakenly) believed that it was sufficient to find the real part of $$\sum^{2024}_{k=1} \frac{1}{\frac{2021}{2020}-z_k}$$ For chuckles, I'll give a quick sketch of how I calculated this. Define $b_k$ to be the summand, and solve for $z_k$ in terms of $b_k$ . Substitute this into the equation $(z_k)^{2024}+1=0$ and use Vieta's to find $\sum^{2024}_{k=1} b_k$ . Considering the derivative Define $$f(x)=\prod^{2024}_{k=1} \left(x-\cos \left(\frac{(2k-1)\pi}{2024} \right) \right)$$ Then, we know that $$\sum^{2024}_{k=1} \frac{1}{x-\cos \left(\frac{(2k-1)\pi}{2024} \right)}=\frac{f'\left(\frac{2021}{2020} \right)}{f \left(\frac{2021}{2020} \right)}$$ For the denominator, we can represent it in terms of Chebyshev polynomials as follows: $$f\left(\frac{2021}{2020} \right)=\frac{1}{2^{2022}} \left(T_{1012} \left(\frac{2021}{2020} \right) \right)^2$$ where $T_n$ is the $n$ th Chebyshev polynomial of the first kind. It's annoying to calculate, but I think I could use the binomial theorem for this. However, I have no idea how to compute $f'\left(\frac{2021}{2020} \right)$ , so I'm at a dead end here as well.","['contest-math', 'algebra-precalculus', 'trigonometry', 'summation']"
4775426,Calculating a formula to express the volume of a rhombic dodecahedron (BEE PRISM WITH HEXAGONAL BASE),"I was trying to optimize the cells in a honeycomb structure and I eventually came across the rhombic dodecahedron (I am aware other figures such as the bitruncated octahedron entail greater efficiency). Bees make (let us assume it is their intention) hexagonal prisms with an apex enclosure as the one in the image to minimize the wax used. I wanted to utilize a Lagrangian function that took into consideration the surface area and volume (equated to a constraint of 0.35) in a multivariable system of equations, but need an actual formula for the volume to be able to carry out the task. The surface area I found was 3s(2h+(s√2)/2) (calculating already for the angle to be 54.7º). I have tried to use integrals of volume and have tried to estimate the irregularities of this prism with regular prisms, but none of these seem to be the correct approach (or at least I have not figured out how to use them appropiately). To sum up, my question is: ¿How can I derive an expression for the volume of a rhombic dodecahedron that has got an hexagonal base as the one shown in the figure? Any help would be appreciated. Edit: I am now looking to optimize the sides of the shape as previously suggested. The area is 3s(2h+(s√2)/2) and the volume is ((3√3)/2 s^2 )(h-s/(2√2))=0.35 (0.35 is the constraint). Utilizing Lagrange I get sides s and h to be approximately 0.4567 and 0.8073 respectively. This yields a surface area of approximately 2.6547. The thing is that I did it as well with a regular hexagonal prism and the surface area was surprisingly less; approximately 2.254. The area used in that case was 3sa+6sh (becuase the hexagonal aperture is open) and the volume 3ash=0.35. I thought this might be because in the long run the rhombic dodecahedron might end up tessellating space more efficiently, but I am not sure. Did I go wrong in my operations, or is there a suitable explanation?","['multivariable-calculus', 'calculus', 'geometry', 'volume']"
4775522,Why is $ \mathfrak{Mod}(A_{Y}/f) $ a thick subcategory of $\mathfrak{Mod}(A_{Y})$?,"Let $f: Y \to X$ be a continuous map and $\mathfrak{Mod}(A_{Y}/f)$ be the full subcategory of $\mathfrak{Mod}(A_{Y})$ (categories of $A_{Y}$ -modules, with $A$ a fixed ring) whose sheaves $\mathcal{F}$ satisfy $\mathcal{F}|_{f^{-1}(x)}$ is locally constant, for all $x \in X$ . I'm trying to prove the following statement: $ \mathfrak{Mod}(A_{Y}/f) $ is a thick subcategory of $\mathfrak{Mod}(A_{Y})$ So, as defined in ""Sheaves on Manifolds"" (which is where this statement is from), it must be checked that for any $Y,Y',Z,Z' \in \mathfrak{Mod}(A_{Y}/f) $ and $X \in \mathfrak{Mod}(A_{Y})$ , if there is an exact sequence $$ Y \to Y' \to X \to Z' \to Z $$ then $X \in \mathfrak{Mod}(A_{Y}/f)$ But, since the sequence is exact, so is $ Y\vert_{f^{-1}(x)} \to Y'\vert_{f^{-1}(x)} \to X\vert_{f^{-1}(x)} \to Z'\vert_{f^{-1}(x)} \to Z\vert_{f^{-1}(x)}$ Since the restrictions are locally constant sheaves, (let us assume $f^{-1}(x)$ is connected) they have the same stalk at each point. This implies that the locally constant sheaf $M_{f^{-1}(x)}$ (where $M := X_{y}$ , for some chosen $y \in f^{-1}(x)$ ), when sustituted for $X\vert_{f^{-1}(x)}$ preserves the exactness of the sequence. If we had a map $M_{f^{-1}(x)} \to X\vert_{f^{-1}(x)}$ , we could use the five lemma do conclude the proof. But in general, I don't see a way to construct this map. And from what I know, the locally constant sheaf isn't fully determined by its stalks (right?). That is, at least in general topological spaces. Am I going wrong somewhere in my reasoning? How is that subcategory thick in relation to $\mathfrak{Mod}(A_{Y})$ ? Edit: in truth, we could drop the restriction to the fibers of $f$ , since the question seems to have the same complexity as that of: Is the category of constant (or locally constant) sheaves $\mathfrak{Const}(A_{Y})$ a thick subcategory of $\mathfrak{Mod}(A_{Y})$ ? Thank you for all the help in advance :).","['abelian-categories', 'analytic-geometry', 'algebraic-geometry', 'sheaf-theory']"
4775563,The logic of P values and rejecting,Could someone please explain the logic of the reasoning behind why we reject the $H_0$ if the $\alpha$ exceeds the $p$ value What I understand is that $\alpha$ is the probability of making a type 1 error if $H_0$ is true. $p$ value is the likelihood of getting a value more to the right or left of your test stat if the $H_0$ is true (probability of getting a test stat that favors $H_a$ more) So if $\alpha>p$ value means we reject $H_0$ Then it must mean that if the probability of making an error while $H_0$ is true is larger than the probability of getting a value more the to the right of your test statistic then reject $H_0$ . How does that logic add up? Could someone explain the reason behind the why we reject $H_0$ ? Thank you,['statistics']
4775601,Gambler's ruin in the limit (only stopping rule ruin),"Imagine a classical Gambler's ruin with winning probability p and losing probability q = 1-p. You start at 1\$ and lose once you reach 0\$. The only stopping rule is that the game is over when the gambler loses. No restriction on p besides the usual $0\leq p\leq1$ . My question is, what is the probability of the gambler losing. My intuition would be that $P(lose) = 1$ , or at least very close to one. The question, I believe, is whether there are values of $p$ , where even when considering an infinite time horizon, the Gambler never reaches 0.","['probability-limit-theorems', 'random-walk', 'probability-distributions', 'limits', 'probability']"
4775613,Classification of finite groups of order $p^3qr$ up to isomorphism,"Does there exist a reference which aims to solve this or a similar problem? For distinct primes $p,q,r$ . The closest thing I found was a paper classifying groups of order $p^2qr$ . From a cursorary glance it seems to use Burnside transfer theorem which I've never seen before so I wasn't sure if it could be modified for this case. I was curious about this problem as it came up when I was trying to count groups upto isomorphism of order $2024=2^3\cdot 11 \cdot 23$ . Weirdly enough GAP didn't have an entry for it in SmallGroups. But running FiniteGroupCount[2024] on Mathematica returned a result of 46. So 43 non abelian groups. 2024 in particular is small enough that I didn't need to consider extensions of Alternating or Lie type groups. So maybe it could be a subset of all possible semi direct products between the underlying cyclic groups? But that seems very loosey goosey. Since I know extensions need not be semi direct products.","['group-theory', 'abstract-algebra', 'reference-request']"
4775625,Proving a Sierpiński result on partitions of the unit interval into closed sets,"From Are the Sierpiński cardinal ˊn and its measure modification ˊm equal...? , I seem to have rediscovered a result from Sierpinski: Theorem (Sierpiński, 1921). For any countable partition of the unit interval $[0,1]$ into closed subsets exactly one set of the partition is non-empty. (1) Does the proof below work? (2) It appears from How strong is Sierpiński theorem about continua? and Partitioning [0,1]
into pairwise disjoint nondegenerate closed intervals that we can replace $[0, 1]$ with any continuum (nonempty compact connected Hausdorff space).
Is there a complete proof of this on the web? The first answer to the second question above has most of a proof, but missing a lemma. (There's more discussion in Show [0,1]≠⋃∞𝑛=1𝐼𝑛... .) [EDIT: for general continua, in addition to the answer below, there's one in an answer to Is [0,1] a countable disjoint union of closed sets? . For the $[0, 1]$ result, there's also some nice discussion in Why are the integers with the cofinite topology not path-connected? ] Proof . Suppose not: then take just the nonempty closed sets, and the partition has size at least two. If the partition is finite, it disconnects $I = [0, 1]$ . So the partition is countably infinite, say $(A_i)_0^\infty$ . We'll get a contradiction by constructing a point of $I$ which is not in $\bigcup A_i$ , by building a decreasing sequence of open intervals which stay away from the $A_i$ . For any $j$ , take $B_j$ = $\bigcup_{i = 0}^j A_j$ . $B_j$ is a closed proper subset of $I$ . Get $j_0$ with $0, 1 \in B_{j_0}$ , and get $x_0 \in I - B_{j_0}$ . Since $B_{j_0}$ is closed, there's an open interval $(t, u)$ disjoint from $B_{j_0}$ with $x_0 \in (t, u)$ . Take $p_0$ to be the $\inf$ of all such $t$ , and $q_0$ to be the $\sup$ of all such $u$ : then $(p_0, q_0)$ is still disjoint from $B_{j_0}$ . Note that $p_0 \in B_{j_0}$ : otherwise $p_0$ has a neighborhood disjoint from $B_{j_0}$ and $p_0$ would not be minimal. Similarly, $q_0 \in B_{j_0}$ . Take the smallest index $j'_0$ such that $A_{j'_0}$ meets $(p_0, q_0)$ (note that $j'_0 > j_0$ ). I claim there is some $u \in (p_0, q_0)$ with $(p_0, u)$ disjoint from $A_{j'_0}$ : otherwise there's a sequence $u_k \to p_0$ with all $u_k$ in $A_{j'_0}$ , which is impossible since $A_{j'_0}$ is closed and does not contain $p_0$ . Take $q_1$ to be the $\sup$ of all such $u$ : then $(p_0, q_1)$ is disjoint from $A_{j'_0}$ and $q_1 \in A_{j'_0}$ as above. In fact $(p_0, q_1)$ is disjoint from $B_{j'_0}$ , because $j'_0$ is minimal. Similarly, on the other side, take the smallest $j_1$ such that $A_{j_1}$ meets $(p_0, q_1)$ , and take $p_1$ to be the $\inf$ of all $t \in (p_0, q_1)$ such that $(t, q_1)$ is disjoint from $A_{j_1}$ . Now $j_1 > j_0$ , $p_0 < p_1 < q_1 < q_0$ , $(p_1, q_1)$ is disjoint from $B_{j_1}$ , and $p_1, q_1 \in B_{j_1}$ . Repeat this to get $j'_1$ and $q_2$ , and $j_2$ and $p_2$ , with $p_1 < p_2 < q_2 < q_1$ , $(p_2, q_2)$ disjoint from $B_{j_2}$ , and $j_2 > j_1$ . Continue inductively to get $j_i$ and $(p_i, q_i)$ for all $i$ . The sequence $(j_i)$ approaches infinity, so $\bigcup B_{j_i} = I$ . $(p_i)$ is increasing and $(q_i)$ is decreasing. Taking $p = \sup {p_i}$ and $q = \inf {q_i}$ , we have $p \le q$ , when means $p$ is in all the $(p_i, q_i)$ . But then $p$ is in none of the $B_{j_i}$ , which is impossible. Q.E.D.","['general-topology', 'continuum-theory', 'real-analysis']"
4775658,Find $f(x) \ $: $f'(x) = f(x)^2 + f^{-1}(x) + \int_{-\infty}^{x} \frac{e^{t}}{t} dt$,$$f'(x) = f(x)^2 + f^{-1}(x) + \int_{-\infty}^{x} \frac{e^{t}}{t} dt$$ I suspect this question is a typo because differential equations typically require initial or boundary conditions or more information about the properties of the function $f(x)$ . Without any other information it's not possible to find $f(x)$ . Am I right?,"['calculus', 'functions', 'derivatives', 'ordinary-differential-equations']"
4775661,Show by counterexample that $\mathbb{N} \times \mathbb{Z}^{+} \subseteq \mathbb{Z}^{+} \times \mathbb{N}$ is false,"We are studying discerete mathematics and our professor has assigned us the task to give counterexample to the following false statement. Statement : $\mathbb{N} \times \mathbb{Z}^{+} \subseteq \mathbb{Z}^{+} \times \mathbb{N}$ . My Issue : My concern is that I believe the above statement is true because $\mathbb{Z}^{+}$ and $\mathbb{N}$ represent same set of numbers $\{1,2,3....\}$ and hence L.H.S and R.HS of the statement represent same set. Since we know that each set is improper subset of itself, hence the statement is true not false. Am I wrong or the professor? Please guide.","['proof-explanation', 'set-theory', 'discrete-mathematics']"
4775743,Solving $f'(x)g(x)-f(x)g'(x)=c \in \mathbb{C}-\{0\}$,"Let $c \in \mathbb{C}$ . How to find a general form of $f=f(x),g=g(x) \in \mathbb{C}[x]$ that satisfy $f'(x)g(x)-f(x)g'(x)=c$ ? I think I can solve this algebraically by writing $f=a_nx^n+\cdots+a_1x+a_0$ and $g=b_mx^m+\cdots+b_1x+b_0$ , where $n,m \geq 0$ , $a_i,b_j \in \mathbb{C}$ , with $a_nb_m \neq 0$ . Then $c=f'g-fg'=(a_nx^n+\cdots+a_1x+a_0)'(b_mx^m+\cdots+b_1x+b_0)-(a_nx^n+\cdots+a_1x+a_0)(b_mx^m+\cdots+b_1x+b_0)'=
(na_nx^{n-1}+\cdots+a_1)(b_mx^m+\cdots+b_1x+b_0)-(a_nx^n+\cdots+a_1x+a_0)(mb_mx^{m-1}+\cdots+b_1)=a_nb_m(n-m)x^{n+m-1}+\cdots$ . There are two cases: (1) $n+m-1=0$ and $n-m \neq 0$ : Then $n+m=1$ and $n-m \neq 0$ .
This implies $n=1,m=0$ or $n=0,m=1$ .
Therefore, $f=a_1x+a_0, g=b_0$ with $a_1b_0=c$ or $f=a_0, g=b_1x+b_0$ with $-a_0b_1=c$ . (2) $n+m-1 > 0$ , hence $n-m=0$ , so $n=m$ ,
and also we should check all the other conditions until $a_1b_0-a_0b_1=c$ .
This yields, for example: $f=x+a, g=x+b$ with $b-a=c$ . However, I wonder if there is a solution involving differential equations .
I wish to integrate both sides, but do not know how:
Divide both sides of $f'(x)g(x)-f(x)g'(x)=c$ by $g^2(x)$ ( $c \neq 0$ implies that $g(x) \neq 0$ ), so $(\frac{f}{g})'=\frac{f'(x)g(x)-f(x)g'(x)}{g^2(x)}=\frac{c}{g^2(x)}$ . But now I am not sure if it will help multiplying both sides by $-g'(x)$ since this will 'ruin' the LHS. Perhaps this will help somehow. Any help in solving this equation not algebraically is welcome; thank you! Edit: In case (2) I have not brought all the details; here is an example where $n=m=2$ : $f=x^2+ax+b$ and $g=x^2+cx+d$ , there are no such $f,g$ of degree $2$ (the original $c$ is now changed to $\tilde{c}$ , since we already use $c$ as one of the coefficients of $g$ ): $\tilde{c}=f'g-fg'=(c-a)x^2+2(d-b)x+(ad-bc)$ ,
hence: $c=a$ , $d=b$ and $ad-bc=\tilde{c} \neq 0$ ,
but applying the first and second conditions in the third yield: $0 \neq \tilde{c}=ad-bc=ab-ba=0$ , a contradiction. Similarly, I have checked $n=m=3$ , writing $f=x^3+ax^2+bx+c$ , $g=x^3+dx^2+ex+f$ , and again the last condition, based on former conditions, yields a contradiction. So it seems that a general form is necessarily that of case (1) : $f=a_1x+a_0, g=b_0$ with $a_1b_0=c$ or $f=a_0, g=b_1x+b_0$ with $-a_0b_1=c$ .
(Since case (2) is impossible).","['commutative-algebra', 'derivatives', 'polynomials', 'ordinary-differential-equations']"
4775808,Find the product of all irreducible polynomials over $\mathbb{F}_p$ of degree $n$,"Let $p$ be a prime number and $n \in \mathbb{N}^+$ . Let $H_n$ be the product of all monic irreducible polynomials in $\mathbb{F}_p[T]$ whose degree is equal to $n$ . What is known about $H_n$ ? Is there an explicit formula? Where does it appear in the literature? It is well-known that the product of all monic irreducible polynomials whose degree divides $n$ is equal to $T^{\large p^n}-T$ . It follows that, if $\ell$ is a prime number, then $$H_{\ell} = (T^{\large  p^\ell} - T)/(T^p - T).$$ More generally, if $n = \ell^s$ is a power of a prime number, then $$H_{\large \ell^s} = (T^{\large p^{\ell^s}} - T)/(T^{\large p^{\ell^{s-1}}} - T).$$ For the product of two primes it is also easy, but for $n = 12 = 2^2 \cdot 3$ we need the principle of inclusion-exclusion to find (if I am not mistaken) $$H_{12} = \dfrac{(T^{\large p^{12}} - T) (T^{\large p^2}-T)}{(T^{\large p^4}-T)(T^{\large p^6}-T)}.$$ I assume that a similar approach works in general. I am looking in particular for a good reference.","['irreducible-polynomials', 'finite-fields', 'reference-request', 'combinatorics', 'commutative-algebra']"
4775927,Derivative with respect to vectorization,"I refer to the vectorization $\mathrm{vec}$ of a matrix $A\in\mathbb{R}^{n\times n}$ as the vector in $\mathbb{R}^{n^2}$ with the columns of $A$ stacked one on top of each other. I have a function $g(x,A) = (A^T\otimes CA)x$ , for a fixed matrix $C\in\mathbb{R}^{n\times n}$ , and with $x\in\mathbb{R}^{n^2}$ . I want to compute the derivative of $g(x,A)$ with respect to $\mathrm{vec}(A)$ . I expect this to be a matrix of size $n^2\times n^2$ of course. If instead of one of the two $A$ s I had a $B\in\mathbb{R}^{n\times n}$ , I would know what to do since, for example for $h(x,A)=(A^T\otimes  CB)x$ I know $$
g(x,A)=\mathrm{vec}(CB\mathrm{Mat}(x)A)=(I_n\otimes CB\mathrm{Mat}(x))\mathrm{vec}(A)
$$ and hence the derivative is simply given by $(I_n\otimes CB\mathrm{Mat}(x))$ . A similar reasoning applies when I fix the first $A$ . Here by $\mathrm{Mat}$ I intend the opposite of $\mathrm{vec}$ , i.e. $\mathrm{vec}(\mathrm{Mat}(x))=x$ . To differentiate the whole of $g(x,A)$ I would be tempted to sum these two contributions obtained rewriting $g(x,A)$ in two different and convenient ways. I have however the worry this is not correct. How can I think about solving this problem?","['matrices', 'matrix-calculus', 'derivatives', 'vector-analysis']"
4775933,Uniqueness of Sturm-Liouville like problem,"the following is an exercise taken from the written exam of the functional analysis course that I am following Let $f \, : \, [0,1] \times \mathbb{R} \to \mathbb{R}$ be a $C^1$ function that satisfies $$ \lim_{|\xi|\to \infty}{\frac{f(x,\xi)}{\xi}} = 0 $$ uniformly with respect to $x \in [0,1]$ . Prove that the problem $$ \begin{cases} &-u^{''} = f(x,u(x)) \\ &u(0) = u(1) = 0\end{cases} $$ has a unique classical solution I managed to prove the existence of the solution but I didn't managed to prove the uniqueness of the solution. I would like to demonstrate uniqueness by not using tools external to the course, whcih are : Sturm-Liouville theory, a bit of Sobolev spaces, Nemitski operators, Fourier transform, Functional calculus and spectral theorem, Lax-Milgram theorem, Schauder-Fixed point theorem, Implicit function theorem for Banach spaces, Inversion theorem for Banach spaces and Lagrange multipliers for Banach spaces. This is how I proved the existence. PROOF OF EXISTENCE : Consider the operator $\tilde{T} : L^2([0,1]) \to H^1_0{([0,1])}$ that maps $g \in L^2([0,1])$ in the unique solution $\tilde{T}(g) = v$ of the Sturm-Liouville problem $$\begin{cases} &-v^{''} = g \\&v(0) = v(1) = 0\end{cases}$$ one can prove that $\tilde{T}$ is continuos because multiplying by $\tilde{T}(g)$ , integrating and using integration by parts we find $$ \int_{0}^{1}{ g \cdot (\tilde{T}g) dx} = -\int_{0}^{1}{(\tilde{T}g)^{''}\cdot(\tilde{T}g)dx} = \int_{0}^{1}{ (\tilde{T}g)^2 dx}$$ Combining this with the Cauchy-Schwarz inequality we find ( from now on I will omit $([0,1])$ in the functional spaces for simplicity ) $$ ||(\tilde{T}g)' ||^2_{L^2} \leq ||(\tilde{T}g)' ||_{L^2}||g ||_{L^2}$$ dividing we find $$ ||(\tilde{T}g)' ||_{L^2} \leq ||g ||_{L^2}$$ which proves the continuity by the Poincarè inequality. Now let $i \; : \; H^1_0 \to L^2$ be the inclusion operator, we know it is compact thus $T = i \circ \tilde{T}$ is compact Consider the Nemitski non-linear operator $L^2 \ni u \mapsto Gf = f(x,u(x)) \in L^2$ ( I won't prove $Gf$ is in $L^2$ since we did it in class ) Now let $$\begin{split} F \; : \; L^2([0,1]) &\to L^2([0,1])\\ L^2 \ni u &\mapsto F(u) := T(G(u)) \in L^2 \end{split}$$ it's easy to show that $u$ solves the problem if and only if $F(u) = u$ . I show an important inequality for $G$ , let $N > 0$ ,
let $$M = M(N) := \max_{\substack{%
 			0 \leq x \leq 1\\
 			|y| \leq N}}{|f(x,y)|} \hspace{0.5cm} \epsilon = \epsilon(N) := \sup_{\substack{%
 			0 \leq x \leq 1\\
 			|y| \geq N}}{\frac{|f(x,\xi)|}{|\xi|}}$$ clearly $M = M(N)$ is finite for all $N > 0$ and $\lim_{N \to \infty}{\epsilon(N)} = 0$ Then we find $$||G(u)||^2_{L^2} = \int_{0}^{1}{|f(x,u(x))|^2 dx} = \int_{ |u(x)| \leq N }{ |f(x,u(x)|^2 dx} + \int_{|u(x)| > N}{ |f(x,u(x)|^2 dx} \leq \int_{ |u(x)| \leq N }{ M^2 dx} + \int_{ |u(x)| > N }{ \epsilon^2 \cdot |u(x)|^2 dx} \leq  M^2 + \epsilon^2 ||u||^2_{L^2}$$ this shows that $G$ sends bounded sets in bounded sets, therefore $F = T \circ G$ is compact since $T$ is compact. Now let $N >> 0$ and $R >> 0$ ( $R$ possibly depending on $N$ ) be such that $||T||^2(M^2 + \epsilon^2 R^2) \leq R^2$ then since $||F(u)||_{L^2} \leq ||T||\cdot||G(u)||_{L^2}$ I have that the set $$D := \{ u \in L^2([0,1]) \; : \; ||u||_{L^2} \leq R \}$$ is mapped into itself by $F$ , meaning $||u||_{L^2} \leq R \implies ||F(u)||_{L^2} \leq R$ , therefore if I restrict $F$ to $D$ by Schauder fixed point theorem there exists $u \in L^2$ such that $F(u) = u$ , which proves the existence I didn't managed to make any progress from here","['sturm-liouville', 'functional-analysis', 'fixed-point-theorems', 'weak-derivatives']"
4775958,Is there a notion of best $\mathbb{K}$-approximations where $\mathbb{K}$ is an algebraic number field?,"A known approximation of $\pi$ is $\sqrt{2}+\sqrt{3}=\color{green}{3.14}\color{red}{6264...}$ But recently I came across a refinement of this approximation $$\pi\approx\sqrt{2}+\sqrt{3}+\frac{\sqrt{2}-\sqrt{3}}{68}=\color{green}{3.14159}\color{red}{0292...}$$ Knowing this, I tried to find interesting approximations of $\pi$ on $\mathbb{Q}(\sqrt{2},\sqrt{3})$ and I've been able to obtain $$\boxed{\pi\approx\frac{42(132\sqrt{2}+375\sqrt{3}-19^2)}{5\left(\left(5^2+2^2\cdot\sqrt{2}\right)\left(10^2+3^2\cdot\sqrt{3}\right)-2273\right)}=\color{green}{3.1415926535897}\color{red}{63336...}}$$ But this made me thinking: Q: Similar to how we know that the best $\mathbb{Q}$ -aproximations of a given real number $x>0$ are given via the partial fractions of its continued fraction representation, is there any notion of best $\mathbb{K}$ -approximations where $\mathbb{K}$ is an algebraic number field (i.e. a finite field extension of $\mathbb{Q}$ ) like $\mathbb{K}=\mathbb{Q}(\sqrt{2})$ or $\mathbb{K}=\mathbb{Q}(\sqrt{2},\sqrt{3})$ ? I know that, for $\mathbb{Q}$ , we can say that $\frac{a}{b}\in\mathbb{Q}$ is a best approximation of $x>0$ if $$\forall a'/b'\in\mathbb{Q}\text{ with }b'\leq b\text{ we have }\left|x-\frac{a}{b}\right|\leq\left|x-\frac{a'}{b'}\right|$$ However, we can't extend this to (say) $\mathbb{Q}(\sqrt{2})$ naively expressing numbers of $\mathbb{Q}(\sqrt{2})$ as $\frac{a+b\sqrt{2}}{c}$ since $\mathbb{Z}(\sqrt{2})=\{a+b\sqrt{2}:a,b\in\mathbb{Z}\}$ is dense on $\mathbb{R}$ so, for a fixed $c$ , we can find $\frac{a+b\sqrt{2}}{c}$ arbitrarily close to $x>0$ .","['diophantine-approximation', 'number-theory', 'reference-request', 'field-theory', 'abstract-algebra']"
4776048,How can I calculate $\mathbf{X}\mathbf{X}^{\rm T} \mathbf{v}$ in the most effective way without storing $\mathbf{X}\mathbf{X}^{\rm T}$,"I have a matrix $\mathbf{X} \in \mathbb{R}^{n \times m}$ and a vector $\mathbf{v} \in \mathbb{R}^n$ with $n \gg m$ . I would like to compute $\mathbf{X}\mathbf{X}^{\rm T} \mathbf{v}$ in the most effective way, but I cannot store $\mathbf{X} \mathbf{X}^{\rm T}$ . Do you have an idea how to perform these multiplications as fast as possible? Of course, some partial results (like some rows of $\mathbf{X} \mathbf{X}^{\rm T}$ ) can be temporarily stored.","['matrices', 'linear-algebra', 'linear-transformations']"
4776051,A daunting double integral with a simple closed form,"In the following, I'll present a curious double integral that despite its daunting look has a very nice closed form, $$\int _0^{\pi/2}\int _0^{\pi/2}\cot (x) \csc ^2(y) \log (\cos (y)) \log \left(1-2 \sin (x)+\sin ^2(x) \csc ^2(y)\right)\textrm{d}x \textrm{d}y$$ $$=\frac{\pi^3}{6}-\pi.$$ The integral was recently proposed by C. I. Valean . He exploited a new double integral representation of the Dilogarithm he recently derived and presented in The Dilogarithm: A New Representation in Terms of a Double Integral . The solution flow (in large steps) is as follows: Exploiting that $\displaystyle \operatorname{Li}_2(u)=-\frac{1}{\pi}\int _0^{\pi/2}\int _0^{\pi/2}\frac{(1-u)\log \left(1-2 \sin (x)+ \csc ^2(y) \sin ^2(x)\right)}{\tan (x) \left(\cos ^2(y)+(1-u)^2 \sin ^2(y)\right)}\textrm{d}x \textrm{d}y, \ u<1$ , if we multiply both sides by $\pi$ and then integrate from $u=0$ to $u=1$ , we have $$\small \int _0^{\pi/2}\left(\int _0^{\pi/2}\cot(x)\log \left(1-2 \sin (x)+ \csc ^2(y) \sin ^2(x)\right)\left(-\int_0^1\frac{(1-u)}{\cos ^2(y)+(1-u)^2 \sin ^2(y)}\textrm{d}u\right)\textrm{d}x\right)\textrm{d}y$$ $$\small =\frac{1}{2}\int _0^{\pi/2}\left(\int _0^{\pi/2}\cot(x)\log \left(1-2 \sin (x)+ \csc ^2(y) \sin ^2(x)\right)\csc^2(y) \log(\cos^2(y)+(1-u)^2 \sin^2(y)\biggr|_{u=0}^{u=1}\textrm{d}x\right)\textrm{d}y$$ $$\small =\int _0^{\pi/2}\left(\int _0^{\pi/2}\cot (x) \csc ^2(y) \log (\cos (y)) \log \left(1-2 \sin (x)+\sin ^2(x) \csc ^2(y)\right)\textrm{d}x\right)\textrm{d}y$$ $$
=\pi \int_0^1 \operatorname{Li}_2(u) \textrm{d}u=\pi \int_0^1 u' \operatorname{Li}_2(u) \textrm{d}u=\pi \underbrace{u\operatorname{Li}_2(u)\biggr|_{u=0}^{u=1}}_{\displaystyle \pi^2/6} +\pi \underbrace{\int_0^1 \log(1-u)\textrm{d}u}_{\displaystyle -1}$$ $$=\frac{\pi^3}{6}-\pi.$$ I would enjoy a lot to see different approaches (possibly without using the Dilogarithm). Given the simplicity of the closed form one might be tempted to ponder over the possibility of getting other elegant ways of performing the calculations. EDIT_1 : Cornel says the double integral representation of $\operatorname{Li}_2$ exploited in the solution can be reduced to the following (fascinating) integral in one variable: $$\operatorname{Li}_2(u)=\frac{\pi ^2}{6}-\frac{1}{\pi }\int_0^{\pi/2} \arctan((1-u) \tan (x)) (\pi+2 \cot (x) \log (\cot (x))) \operatorname{d}x,$$ which, if you ask me, looks too beautiful to be true (how can such a beautiful thing exist?). The solution is straightforward at this point if we differentiate with respect to $u$ . EDIT_2 : The transformation from the double integral representation of $\operatorname{Li}_2$ to the single integral representation above takes place by proving and using that $$\int _0^{\pi/2}\cot (x) \log \left(1-2 \sin (x)+\sin ^2(x) \csc ^2(y)\right)\textrm{d}x$$ $$=-\frac{\pi ^2}{3}+\int_y^{\pi/2} (\pi +2 \cot (x) \log (\cot (x)) ) \textrm{d}x,$$ and at the same time, this fact can also be employed separately and directly to the main integral to get a second solution. EDIT_3 : Here is another example where the given double integral representation of $\operatorname{Li}_2$ plays a crucial part and immediately allows us to connect the integral with known resulting integrals: $$\int _0^{\pi/2}\int _0^{\pi/2} \operatorname{arctanh}(\sin(y))\csc (y)\cot (x) \log \left(1-2 \sin (x)+\sin ^2(x) \csc ^2(y)\right)\textrm{d}x \textrm{d}y$$ $$=2\log(2)\pi G-\frac{9}{8}\log^2(2)\pi^2-\frac{\pi^3}{6}-\frac{17}{96}\pi^4+12 \pi \Im\biggr\{\operatorname{Li}_3\left(\frac{1+i}{2}\right)\biggr\}.$$ EDIT_4 : More generally, we have the following polylogarithmic representation, $n\ge0$ , $$ \operatorname{Li}_{n+2}(u)$$ $$\small =(-1)^{n-1}\frac{1}{\pi}\frac{1}{n!}\int _0^{\pi/2}\int _0^{\pi/2}\frac{(1-u)\log^n(\sin(x))\log \left(1-2 \sin (x)+ \csc ^2(y) \sin ^2(x)\right)}{\tan (x) \left(\cos ^2(y)+(1-u)^2 \sin ^2(y)\right)}\textrm{d}x \textrm{d}y, \ u<1.$$ EDIT_5 : In view of the generalization above, which can be proved by exploiting a similar idea to the one stated at EDIT_2 , we obtain a double integral very similar to the initial one, and so nice: $$\small \int _0^{\pi/2}\int _0^{\pi/2}\cot (x) \csc ^2(y) \log(\sin(x))\log (\cos (y)) \log \left(1-2 \sin (x)+\sin ^2(x) \csc ^2(y)\right)\textrm{d}x \textrm{d}y$$ $$=\frac{\pi^3}{6}-\pi-\pi \zeta(3).$$ EDIT_6 : The Trilogarithmic version can also be approached like the Dilogarithm version, presented in the link attached in the beginning. Essentially, we combine $$\int_0^1 \frac{\log \left(\csc ^2(x) t^2 -2 t+1\right)\log(t)}{t} \textrm{d}t$$ $$=-\sum_{n=1}^{\infty}  \left(\frac{H_n^2}{n}+\frac{H_n^{(2)}}{n}+2\frac{H_n\overline{H}_n}{n}+2\frac{\overline{H}_n^{(2)}}{n}-2\frac{1}{n}\sum_{k=1}^n (-1)^{k-1}\frac{H_k}{k}\right)\cos(2 n x)$$ $$= -\sum_{n=1}^{\infty}\left(\frac{H_n^2}{n}+\frac{H_n^{(2)}}{n}+2\frac{1}{n}\sum_{k=1}^n\frac{\overline{H}_k}{k}\right)\cos(2 n x)$$ and $$-\operatorname{Li}_3\left(\frac{2x}{x-1}\right)$$ $$= \sum_{n=1}^{\infty}  x^n\left(\frac{1}{2}\frac{H_n^2}{n}+\frac{1}{2}\frac{H_n^{(2)}}{n}+\frac{1}{n}\sum_{k=1}^n\frac{\overline{H}_k}{k}\right)$$ $$=\sum_{n=1}^{\infty}  x^n\left(\frac{1}{2}\frac{H_n^2}{n}+\frac{1}{2}\frac{H_n^{(2)}}{n}+\frac{H_n\overline{H}_n}{n}+\frac{\overline{H}_n^{(2)}}{n}-\frac{1}{n}\sum_{k=1}^n (-1)^{k-1}\frac{H_k}{k}\right)$$ , which appear in Analogues of the established Landen-type identities in the form of series and some related Cauchy products by C.I. Valean and Deriving Special Fourier Series Involving the Inverse Tangent Integrals of Order Two and Three, and Other Curious Functions by C.I. Valean . EDIT_7 : Cornel says that we can perfectly consider and exploit the ideas in the paper The Dilogarithm: A New Representation in Terms of a Double Integral (for the Polylogarithm version) where the dilogarithmic version is proved. For higher order when the complicated coefficients of the key powers series and Fourier series pop up, we can write them generically by using say, $a_n$ and $c_n \cdot b_n$ , and this is enough (of course, they turn out to be complicated as we consider polylogarithms of higher orders). These details will be explained later in another paper. EDIT_8: Such strategies are very powerful in deriving difficult results, as we may see in More (Almost) Impossible Integrals, Sums, and Series: A New Collection of Fiendish Problems and Surprising Solutions (2023), pages 73-74 $$\int_0^{\pi/2}\frac{\log^4(\cos(\theta))}{\cos^2(\theta)+y^2 \sin^2(\theta)}\textrm{d}\theta=\frac{1}{16}\int_0^{\infty} \frac{\log^4(1+x^2)}{1+y^2 x^2}\textrm{d}x$$ $$= \frac{\pi}{96}(7\pi^4+24 \log^2(2)\pi^2+48\log^4(2)-288\log(2)\zeta(3))\frac{1}{y}$$ $$-\frac{1}{2}\log(2)\pi(4\log^2(2)+3\pi^2)\frac{1}{y}\log\left(\frac{2y}{1+y}\right)+\frac{\pi^3}{2}\frac{1}{y}\log^2\left(\frac{2y}{1+y}\right)$$ $$-3\log(2)\pi \frac{\log(y)}{y}\log^2\left(\frac{2y}{1+y}\right)+3\log(2)\pi \frac{\log(1-y)}{y}\log^2\left(\frac{2y}{1+y}\right)$$ $$+\frac{3}{2}\log(2)\pi \frac{1}{y}\log^3\left(\frac{2y}{1+y}\right)+\frac{\pi}{2}\frac{\log(y)}{y}\log^3\left(\frac{2y}{1+y}\right)-\frac{\pi}{2}\frac{\log(1-y)}{y}\log^3\left(\frac{2y}{1+y}\right)$$ $$
-\frac{\pi}{8}\frac{1}{y}\log^4\left(\frac{2y}{1+y}\right)+\frac{\pi}{4}(12\log^2(2)+\pi^2)\frac{1}{y}\operatorname{Li}_2\left(\frac{1-y}{1+y}\right)+3\log(2)\pi \frac{1}{y}\operatorname{Li}_3\left(\frac{1-y}{1+y}\right)$$ $$+6\log(2)\pi \frac{1}{y}\operatorname{Li}_3\left(\frac{2y}{1+y}\right)-\frac{3}{2}\pi\frac{1}{y} \operatorname{Li}_4\left(\frac{1-y}{1+y}\right)-3\pi\frac{1}{y} \operatorname{Li}_4\left(\frac{2y}{1+y}\right)$$ $$
-3\pi \frac{1}{y} \operatorname{Li}_4\left(\frac{y-1}{2y}\right), \ y>0.$$ EDIT_9: probably the final edit. Thanks Cornel for sharing fantastic calculations.","['integration', 'definite-integrals', 'real-analysis', 'calculus', 'sequences-and-series']"
4776056,Counting Permutations with Product Rule,"I am working on my homework and I solved this problem and I just wanted someone to double check to see if my answer makes sense. 5.4.3: Lining up club members for a photo. Ten members of a club are lining up in a row for a photograph. The club has one president, one VP, one secretary, and one treasurer. (c)
How many ways are there to line up the ten people if the president must be next to the secretary and the VP must be next to the treasurer? I did 2 * 2 * 8! because the count goes down to 8 after grouping P - S together and T - VP and we have to multiply it by 2 twice because the order of p-s s-p t-
vp vp-t. Does it make sense? Can somebody help me? thank you!","['permutations', 'combinations', 'combinatorics', 'discrete-mathematics']"
4776059,I do not understand why do we divide with the k! that is used in nCk when multiplying two combinations or more combinations that has the same k value,"I recently started studying Permutations and Combinations and following is one interesting question I came across. There are 8 students in a class. The class teacher wants to divide divide those students into four teams. The sizes of teams need not all be equal and a team may consist of even one person. Show that the required team can be formed in 1701 ways. How I approached the problem We can select students in following 5 ways, 5 Students, 1 Student, 1 Student, 1 Student 4 Students, 2 Students, 1 Student, 1 Student 3 Students, 3 Students, 1 Student, 1 Student 3 Students, 2 Students, 2 Students, 1 Student 2 Students, 2 Students, 2 Students, 2 Students Then we can find combinations as below for the given five cases, $\binom{8}{5} \cdot \binom{3}{1} \cdot \binom{2}{1} \cdot \binom{1}{1}$ $\binom{8}{4} \cdot \binom{4}{2} \cdot \binom{2}{1} \cdot \binom{2}{1}$ $\binom{8}{3} \cdot \binom{5}{3} \cdot \binom{2}{1} \cdot \binom{2}{1}$ $\binom{8}{3} \cdot \binom{5}{2} \cdot \binom{3}{2} \cdot \binom{1}{1}$ $\binom{8}{2} \cdot \binom{8}{2} \cdot \binom{8}{2} \cdot \binom{8}{2}$ But above answer has been wrong and the correct answer is $\binom{8}{5}$ $\binom{8}{4} \cdot \binom{4}{2}$ $\binom{8}{3} \cdot \binom{5}{3}$ divided by 2! $\binom{8}{3} \cdot \binom{5}{2} \cdot \binom{3}{2}$ divided by 2! $\binom{8}{2} \cdot \binom{8}{2} \cdot \binom{8}{2} \cdot \binom{8}{2}$ divided by 4! Now I have 2 questions, How come second one is correct? Why aren't we choosing in the scenarios where 1 student is present? Why are we dividing with 2! and 4! etc... in second secnario? More importantly how to recognise when to divide with 2! and 4! etc in problems...?","['permutations', 'combinations', 'combinatorics', 'binomial-theorem', 'probability']"
4776063,What is the difference between addition and product in abstract algebra?,"This might seem like a very stupid question and it probably is, but hopefully not as much as one could expect. My question in fact is the following: Does the difference between sum and product lie only in the distinction of their respective neutral elements (i.e. ""0"" for addition and ""1"" for multiplication) and in the ""one-way"" distributive property of multiplication over addition? As a newbie coming to this subject my first instinct while reading up on the basics of abstract algebra (AA from here on) was to take literally the ""abstract"" in AA, which — as I take it — doesn't really have to do with numbers per se , but ""the structures arising from different ways of combining things together"", of which numbers are a very useful and recurring instance, but not the only one. For instance i have learned (from a 3b1b video on youtube) that the set of isomorphic rotations of a polygon is a group, since it is closed under addition, where addition is computed by applying one rotation after the other (also commutativity applies, there exists a ""zero"" element, and for every non-zero element there exists the additive inverse of that element). Here ""addition"" seemed to me to have nothing to do with the familiar algebraic summation of numbers, except for the properties that it holds. Is this the right way of interpreting the subject? That is, am i right in believing that the definition of an operation (a mapping of a set upon itself of the kind $S\times S \longrightarrow S$ ) exactly coincides with its properties and nothing more? (setting aside the fact that we can elaborate different methods of computing the output of these binary operations on different objects)",['abstract-algebra']
4776141,Product ideals are the kernel of what ring homomorphism?,"As I learned here , since the very beginning of humanity/Kummer's development of ideals, the idea that they are ""numbers"" that we can ""modulo by""/do ""modular arithmetic by"" has been central to the concept. An example that was given in the linked post is that the ideal $(2,1+\sqrt{-5})$ in $\mathbb Z[\sqrt{-5}]$ can be thought of as (the kernel of) the ring homomorphism $$f:\mathbb Z[\sqrt{-5}] \to \mathbb Z/2\mathbb Z, f(a+b\sqrt{-5}) := a+b+2{\mathbb Z}.$$ It took until the work of Dedekind (and even then, the later revisions) that the notion of ideals as sets of elements of a ring took center stage, especially in the seemingly-naive but miraculously-suitable definition of multiplying 2 ideals. (Indeed, somehow all the stars align and we get unique factorization of ideals in terms of the aforementioned definition of product ideal in, for instance, all number rings. Although using Kummer's ring isomorphism conception of ideals we can talk about ideals dividing/divisible by other ideals, we seem to be missing the notion of multiplicity of ""how many times"" an ideal divides another --- perhaps this is some intuition for the alternative definition of Dedekind domain: Noetherian domain s.t. all localizations at prime ideals are DVRs, where discrete valuations are exactly the framework for talking about multiplicities [and Noetherianity is there to guarantee finiteness of the prime factorization process]). Question: I'm wondering if there's any way to interpret the multiplication of ideals ""in Kummer's original conception of the ideal"", or more concretely, is there somehow a way to construct, using ring homomrphisms that define $I_1,I_2$ , a ring homomorphism whose kernel is $I_1\cdot I_2$ ? I'm aware that the Chinese remainder theorem does this if $I_1,I_2$ are relatively prime, but it doesn't seem to apply in general.","['algebraic-number-theory', 'ring-theory', 'abstract-algebra', 'ideals', 'commutative-algebra']"
4776173,If $|f'(x)-e^{2x}|\leq 3$ for all $x\in\mathbb{R}$. Then evaluate the limit $\lim_{x\to +\infty}\frac{f(x)}{e^{2x}}$,"Let $f:\mathbb{R}\to\mathbb{R}$ be a differentiable function such that $f'$ is continuous and satisfies $|f'(x)-e^{2x}|\leq 3$ for all $x\in\mathbb{R}$ . Then evaluate the limit $$\lim_{x\to +\infty}\frac{f(x)}{e^{2x}}$$ . My Attempt: From the given inequality we have $$|f'(x)e^{-2x}-1|\leq 3e^{-2x}$$ $$\lim_{x\to+\infty}|f'(x)e^{-2x}-1|\leq\lim_{x\to+\infty} 3e^{-2x}$$ $$\lim_{x\to+\infty}|f'(x)e^{-2x}-1|\leq 0$$ $$\lim_{x\to+\infty}|f'(x)e^{-2x}-1|= 0$$ $$\lim_{x\to+\infty}\left(f'(x)e^{-2x}-1\right)= 0$$ $$\lim_{x\to+\infty}f'(x)e^{-2x}=1$$ Now to evaluate $\lim_{x\to +\infty}\frac{f(x)}{e^{2x}}$ it is very tempting to use the
L'Hopital Rule and then using $\lim_{x\to+\infty}f'(x)e^{-2x}=1$ we get the answer as $\frac{1}{2}$ But how do we show that $\lim_{x\to+\infty}f(x)=\infty$ since condition to use L'Hopital rule is that the limit must be of form $\frac{0}{0}$ or $\frac{\infty}{\infty}$","['limits', 'calculus', 'limits-without-lhopital', 'real-analysis']"
4776206,Finding area under the cycloid without parametrizing [duplicate],"This question already has answers here : Inverse of $f(x)=\sin(x)+x$ (4 answers) Closed 9 months ago . I tried to calculate the area under the cycloid without parametrizing the $x$ & $y$ coordinate in angle $\theta$ . Let's say the radius of circle is $R$ and we are rolling $2\pi$ rad. If I assume that $y(\theta) = R(1-\cos(\theta))$ can be written as $y(x) = R(1-\cos(\theta(x)))$ , the area under the cycloid of $2\pi$ cycle should be the following integral: $$\int_0^{2\pi R}  R(1-\cos(\theta(x))dx = 3\pi R^2 $$ Then, $$\int_0^{2\pi R}  R(1-\cos(\theta(x))dx = \int_0^{2\pi R} \ R  dx - \int_0^{2\pi R}  R\cos(\theta(x))dx $$ $$ = 2\pi R^2 - \int_0^{2\pi R} R\cos(\theta(x))dx$$ $$ = 3\pi R^2$$ Which gives us: $\int_0^{2\pi R} \cos(\theta(x))dx = -\pi R$ But, since $\cos(\theta(x)) \in [-1,1]$ , this restricts the radius R to be: $-\frac{1}{\pi} \le R \le \frac{1}{\pi}$ . (I find this interesting because setting $\theta(x)$ now lead us to some constraint on radius $R$ ) We know $x(\theta) = R(\theta - \sin(\theta))$ . Taylor expanding $\sin(\theta)$ then subtracting $\theta$ gives: $$\frac{x}{R} = \sum_{i=0}^\infty \frac{(-1)^n \theta^{2n+3}}{(2n+3)!} $$ If there is a function $F(\theta) = \sum_{i=0}^\infty \frac{(-1)^n \theta^{2n+3}}{(2n+3)!}$ , then $\theta(x) = F^{-1}(\frac{x}{R})$ . Is there such a function $F$ that allows us to integrate this cycloid situation in a closed form? If not, why?","['integration', 'geometry', 'real-analysis', 'functions', 'cycloid']"
4776270,"Is the set of polynomials of odd degree dense in $\mathcal{C}^{0}([a,b])$?","Let $$E=\{\sum\limits_{k=0}^{n}a_{k}\,x^{2k+1}\, | \, n\in\mathbb{N}\cup\{0\}\}$$ be the set of polynomials of odd degree in each term defined on $[1,2]$ . (a) Show that $E$ is not closed in $\mathcal{C}^{0}([1,2])$ , where $\mathcal{C}^{0}([1,2])$ is the space of continuous function from $[1,2]$ to $\mathbb{R}$ with sup norm. (b) Is $E$ dense in $\mathcal{C}^{0}([1,2])$ ? Here's my proof: (a) Consider the partial sum of the taylor series of $\sin(x)$ , then every partial sum are in $E$ obviously, i.e., there is a sequence $\{f_{i}(x)\}$ of $E$ , However, the limit of the sequence $f(x)=\sin(x)$ is not in $E$ . This shows that $E$ is not closed. (b) I've tried to prove it using Stone-Weierstrass Theorem, but the theorem only states the polynomial space $\mathbb{R}[x]$ dense in $\mathcal{C}^{0}([a,b])$ , i.e., $$\forall \epsilon >0 \mbox{ and } \forall f(x)\in\mathcal{C}^{0}([a,b]), \exists \, p(x)\in\mathbb{R}[x] \mbox{ s.t. } \|f(x)-p(x)\|_{\infty}<\epsilon.$$ Thus I want to use triangle inequality to find a polynomial with odd degree $p'(x)$ such that $\|p(x)-p'(x)\|<\epsilon$ , but it is impossible since not every polynomial function is the odd function. If you control the distance of two polynomials when $x>0$ , another side must have large distance. So I'm stuck, and I guess the answer is NOT. Can anyone give me some hints? Thanks!","['dense-subspaces', 'polynomials', 'analysis']"
4776322,Sums of three cubes of form $a^3+b^3+c^3=(c+1)^3$?,"Let all variables be positive integers . Consider the forms, $$a^2+b^2=(b+1)^2\tag1$$ $$a^3+b^3+c^3=(c+1)^3\tag2$$ The smallest solutions are the well-known, $$3^2+4^2=5^2$$ $$3^3+4^3+5^3=6^3$$ All solutions of Eq.1 are covered by only one polynomial parameterization. On the other hand, not all solutions of Eq.2 are covered by its three known parameterizations. I. Family 1 $$(3 n^2)^3 + (6 n^2 - 3n + 1)^3 + (9 n^3 - 6n^2 + 3n - 1)^3 = (9 n^3 - 6n^2 + 3n)^3$$ $$3^3 + 4^3 + 5^3 = 6^3\\
12^3 + 19^3 + 53^3 = 54^3\\
27^3 + 46^3 + 197^3 = 198^3$$ Note: $F(n) = (6, 54, 198)/6 = 1, 9, 33,\dots$ is A005920 , or the ""tricapped prism numbers"". II. Family 2 $$(3 n^2)^3 + (6 n^2 + 3n + 1)^3 + (9 n^3 + 6n^2 + 3n)^3 = (9 n^3 + 6n^2 + 3n + 1)^3$$ $$3^3 + 10^3 + 18^3 = 19^3\\
12^3 + 31^3 + 102^3 = 103^3\\
27^3 + 64^3 + 306^3 = 307^3$$ Note: $F(n) = (18, 102, 306)/3 = 6, 34, 102,\dots$ is A067389 . III. Family 3 $$(n)^3 + (3n^2 + 2n + 1)^3 + (3n^3 + 3n^2 + 2n)^3 = (3n^3 + 3n^2 + 2n + 1)^3$$ $$1^3 + 6^3 + 8^3 = 9^3\\    
2^3 + 17^3 + 40^3 = 41^3\\ 
3^3 + 34^3 + 114^3 = 115^3$$ Note: $F(n) = 3n^3 + 3n^2 + 2n = 8, 40, 114,\dots$ is A143943 , or ""the Wiener index of a chain of $n$ squares"". Update : (Jan 22, four months later) Courtesy of an insight by Adam Bailey in this post , given $x^3+y^3+z^3=(z+1)^3$ we now realize that $(x,y)$ of Family 2 and 3 are just lattice points on the same ellipse. Do the substitution, $z = (k + 1)(x - 1) + k y\,$ where $k = \dfrac{3n^2+1}{3n}$ and, after removing a trivial factor, we get the ellipse, $$3n^2(x^2 - xy + y^2) - (1 + 6n + 12n^2 + 18n^3 + 9n^4)x - (1 + 3n^2 + 9n^4)y + (1 + 3n + 9n^2 + 9n^3 + 9n^4) = 0$$ where two lattice points are $(x_1, y_1)$ and $(x_2, y_2)$ from Family 2 and 3, respectively. For example, let $n=3$ so, $$27(x^2 - x y + y^2) - 1342x - 757y + 1063 = 0$$ which for this $n$ has only 2 lattice points, thus we can use these coordinates, $$27^3 + 64^3 + 306^3 = 307^3\;\\
3^3 + 34^3 + 114^3 = 115^3$$ and so on for infinitely other $n$ . IV. Unknown Families? $$14^3 + 23 ^3 + 70^3 = 71^3\\ 
21^3 + 46^3 + 188^3 = 189^3\\
16^3 + 51^3 + 213^3 = 214^3\\ 
\;9^3 \,+\, 58^3 + 255^3 = 256^3\\  
15^3 + 64^3 + 297^3 = 298^3$$ These and the families above are the primitive solutions with $c<307$ . V. Similar equalities By reverse-engineering those identities, $$(an^2+bn+c)^3+(dn^2+en+f)^3+(pn^3+qn^2+r)^3=(pn^3+qn^2+rn+s)^3$$ collecting powers of $n$ , equating everything to zero, it seems those three are the only quadratic-cubic identities. Turns out there is also the form $a^3+b^3+c^3 = (c\color{blue}{+3})^3$ , $$(m^2)^3 + (2 m^2 + 3m + 3)^3 + (m^3 + 2m^2 + 3m)^3 = (m^3 + 2m^2 + 3m \color{blue}{+3})^3$$ $$(m^2)^3 + (2 m^2 - 3m + 3)^3 + (m^3 - 2m^2 + 3m \color{blue}{-3})^3 = (m^3 - 2m^2 + 3m)^3$$ which, for $m=3n$ , can be factored out into $a^3+b^3+c^3 = (c+1)^3$ . VI. Question Just like $a^3\pm b^3 \pm c^3 = 1$ has infinitely many polynomial parameterizations (where degrees are getting higher and higher), does $a^3+b^3+c^3 = (c+1)^3$ also have other families with degree higher than the cubic polynomials in this post?","['number-theory', 'diophantine-equations', 'sequences-and-series']"
4776351,Can $!1+!2+!3+\cdots+!n$ be a perfect power?,"Can $!1+!2+!3+\cdots+!n$ be a perfect power if $n\geq3$ ? Note that $!n$ is a subfactorial. I do know that $1!+2!+3+\cdots+n!$ is only a perfect power if $n=1, 3$ , since when $n\geq9, 1!+2!+3!+\cdots+9!=9\pmod{27}$ , so it cannot be a perfect cube, or any higher perfect prime power, and this is never a perfect square if $n\geq5$ . But I don’t see any pattern to $!1+!2+!3+\cdots+!n$ , so I cannot know if this can be a perfect power if $n\geq3$ . For example, $!1+!2+!3+\cdots+!16\equiv1\pmod{5,7},0\pmod{9},0\pmod{16}$ , so it can a perfect square or a higher perfect power. On the other hand, $!1+!2+!3+\cdots+!17\equiv8\pmod{9}$ , so it is not a perfect square, but it can be a odd perfect power. Are there any ways to find the remainder of $!1+!2+!3+\cdots+!n$ when divided by $p\geq n$ to determine if $!1+!2+!3+\cdots+!n$ can be a perfect power?","['number-theory', 'square-numbers', 'perfect-powers']"
4776364,Intersection of subspace of cyclical rotations with orthant,"In an $N$ -dimensional real Euclidian space, let an orthant be specified by a vector $\underline{x}_0 = \{x_1, x_2, \dots, x_N\}$ where the components $x_k$ are binary in  the sense that $x_k = \pm 1$ . Now let $\underline{x}_j$ be cyclical rotations of  that vector $\underline{x}_0$ , such that the components are $x_{k+j}$ where the indices are understood modulo $N$ .  Span a space of vectors $V = {\rm span} \{ \underline{x}_1, \underline{x}_2, \dots, \underline{x}_d \} $ . What is the minimum number of $d$ such that $V$ intersects the original orthant indicated by $\underline{x}_0$ ? This question can either be understood when notion of the original $\underline{x}_0$ is available, or in an average sense over randomly picked $\underline{x}_0$ where $N$ is large. In the latter case, it is reasonable to make the vector $\underline{x}_0$ almost mean-free, i.e. by imposing $|\sum_{k=1}^N x_k| \le 1$ . Also, conditions on $N$ may be useful, i.e. demanding that $N$ is prime. Background 1 :
If the vectors $ \{ \underline{x}_1, \underline{x}_2, \dots, \underline{x}_d \} $ were randomly picked and in general position, it is known that, as $N$ turns large, the probability of the span intersecting any particular orthant approaches 1 as $d \ge 0.50 N$ (Thomas Cover, Geometrical and Statistical Properties of Systems of Linear Threshold Devices, 1964). In the present non-random but cyclical case, although the vectors could still be assumed in general position, the probability of the span intersecting the original orthant is, by large scale simulations, known to approach 1 as $d \ge 0.59 N$ , i.e. the subspace dimensionality $d$ must be ca. 18 % larger.  (M. Schröder, W. Kinzel and I. Kanter: Training a perceptron by a bit sequence: storage capacity, J. Phys. A 29 7965-7972, 1996). To my knowledge, there is no explanation for this published. Background 2 : One could demand a stronger condition, namely that $\underline{x}_0 = \sum_{j=1}^d w_j \underline{x}_j$ holds, with freely chosen $w_j$ . However, it can be shown by Discrete Fourier Transformation that this condition can be met only in very special cases, and when $N$ is prime and $\underline{x}_0$ is almost mean-free, it is impossible to achieve. The argument is the following: Let $\bf{W}$ be a circulant matrix where the generating vector is composed from the components $w_j$ , and otherwise zero. Then the condition is equivalent to $\bf{W} \cdot \underline{x}_0 = \underline{x}_0 $ . Let $\bf{I}$ be the unit matrix and $\underline{0}$ be a vector of zeros, then equivalently $(\bf{W} - \bf{I})\cdot \underline{x}_0  = \underline{0}$ . So $\bf{V}  = \bf{W} - \bf{I}$ is also a circulant and we have $\bf{V} \cdot \underline{x}_0  = \underline{0}$ . Taking DFTs, $
\underline{\hat x} = {\rm{DFT}}(\underline{x}_0 )$ and $\bf{\Lambda}$ is the diagonal (since $\bf{V} $ is circulant) matrix of eigenvalues of $\bf{V} $ . So, after taking DFT, we have $\bf{\Lambda} \cdot \underline{\hat x}= \underline{0}$ . In components, we have that $\lambda_j {\hat x}_j = 0$ . Now if $N$ is prime, all DFT components ${\hat x}_j $ are nonzero (for a proof see here ), which means that it is required that all $\lambda_j = 0$ . This is only possible if all $v_j =0$ , in other words, $w_0 = 1$ and all other $w_j = 0$ . However, the task has it that $w_0 = 0$ , so this condition cannot be met. If $N$ is not prime, it is still possible that (almost) all DFT components ${\hat x}_j $ are nonzero, so a similar argument holds. Background 3 : Following the discussion above, the span $V$ intersects the original orthant indicated by $\underline{x}_0$ if we define a diagonal matrix $\bf{A}$ with positive elements only, and $(\bf{W} - \bf{A})\cdot \underline{x}_0  = \underline{0}$ . So $\bf{V}  = \bf{W} - \bf{A}$ is a ""circulant plus diagonal"", which is disturbing the argument in Background 2.  This may be a handle to augment the discussion in Background 2 with a perturbation treatment.","['analytic-geometry', 'fourier-analysis', 'algebraic-geometry', 'linear-algebra', 'algebraic-topology']"
4776396,Fubini's theorem and reparameterisation,"Consider the accepted answer to the following question which says It's Fubini's theorem .  The domain of the double integral is: $(\omega,y)\in\Omega\times [0, |X(\omega)|]$ , however the outer
integral is with respect to the measure of the outcomes. $\mathrm
 d\mathbb P\equiv \mathbb P(\mathrm d \omega)$ .  So when we apply
Fubini's theorem the new inner integral is simply the cumulative
measure. Via: $$\begin{align}\mathbb{E}|X|^p &
=\int_{\Omega}p\left(\int_0^{|X(\omega)|}y^{p-1}\,\mathrm d y \right)\,\mathbb{P}(\mathrm d\omega) \\ & = p\int_0^\infty
 y^{p-1}\left(\int_{\Omega:y<|X(\omega)|} \mathbb{P}(\mathrm
 d\omega)\right)\,\mathrm d y \\ & = p\int_0^\infty y^{p-1}\mathbb
 P(y<|X|)\,\mathrm d y \end{align}$$ I understand the first equality, this is just rewriting $|X|$ . However, I do not understand how one goes from the first to the second line. The author suggests that this is due to Fubini. But this is not immediate according to the version I have: Theorem 4.22 (Fubini's theorem) (i) If $f$ is non-negative measurable,
then $$ \int_E f \mathrm{~d} \mu=\int_{E_1}\left(\int_{E_2}
 f\left(x_1, x_2\right) \mu_2\left(\mathrm{~d} x_2\right)\right)
 \mu_1\left(\mathrm{~d} x_1\right) . $$ In particular, we have $$
 \int_{E_1}\left(\int_{E_2} f\left(x_1, x_2\right)
 \mu_2\left(\mathrm{~d} x_2\right)\right) \mu_1\left(\mathrm{~d}
 x_1\right)=\int_{E_2}\left(\int_{E_1} f\left(x_1, x_2\right)
 \mu_1\left(\mathrm{~d} x_1\right)\right) \mu_2\left(\mathrm{~d}
 x_2\right) . $$ This is sometimes known as Tonelli's theorem. (ii) If $f$ is integrable, and $$ A=\left\{x_1 \in E:
 \int_{E_2}\left|f\left(x_1, x_2\right)\right| \mu_2\left(\mathrm{~d}
 x_2\right)<\infty\right\} . $$ then $$ \mu_1\left(E_1 \backslash
 A\right)=0 . $$ If we set $$
f_1\left(x_1\right)=\left\{\begin{array}{ll} \int_{E_2} f\left(x_1,
 x_2\right) \mu_2\left(\mathrm{~d} x_2\right) & x_1 \in A \\ 0 & x_1
 \notin A \end{array},\right. $$ then $f_1$ is a $\mu_1$ integrable
function and $$ \mu_1\left(f_1\right)=\int_E f \mathrm{~d} \mu $$ My issue is that $E_2$ should not depend on $E_1$ as in the theorem. But in the given application the author seems to do the following Take their union (in 2-D) Reparameterise it in the form of the second line Claim that this is due to Fubini. I am struggling to see how this makes sense. Question: Could someone help me understand why $$\begin{align}\int_{\Omega}p\left(\int_0^{|X(\omega)|}y^{p-1}\,\mathrm d y \right)\,\mathbb{P}(\mathrm d\omega) & = p\int_0^\infty
 y^{p-1}\left(\int_{\Omega:y<|X(\omega)|} \mathbb{P}(\mathrm
 d\omega)\right)\,\mathrm d y  \end{align}$$ In particular, how does this follow from Fubini's Theorem?","['integration', 'measure-theory']"
4776419,"Permutations of the word ""EXAMINATION"" which include the word ""EXAM""","I am trying to understand the answer to the following problem: The eleven letters of the word E-X-A-M-I-N-A-T-I-O-N are written on eleven separate
pieces of card. Find the probability that the four letter word E-X-A-M will appear in one of
these eleven letter arrangements. My attempt was as follows: EXAM may be considered as 1 ""unit"" so there are 8 spaces it can move in to around the other 7 letters. There are 2 I s and 2 N s, and 7 letters which can be moved so the number of possible arrangements is $\frac{7!}{2!2!}$ x 8 = 10,080 however the solution gives 5040 saying that swapping the 2 A s makes no more arrangements, so there are 3 pairs of duplicates, not 2. (I am not worried about getting the probability at this point because it is very, very easy to do once you have the number of arrangements) I couldn't really understand this so I made up the following smaller example: Find the number of arrangements of the word ""THREE"" with the letters ""HRE"" together. Treat ""HRE"" as 1 ""unit"" again and move it around the 3 available spaces .The remaining letters T and E can be swapped to produce another solution, for example: HREET and HRETE. Hence we have 3x2! = 6 solutions. According to the logic of the solution for the examination question, we should have $\frac{2!}{2!}$ x 3 solutions because of the double E but that is untrue:
1.HREET
2.HRETE
3.THREE
4.EHRET
5.ETHRE
6.TEHRE What am I missing?",['combinatorics']
4776507,Computation of $\displaystyle{\sum_{n=1}^{\infty}\frac{\sin nx \cdot \sin ny}{n^2}}$,"First I used the identity $$\sin nx \cdot \sin ny=\cos(n(x-y))-\cos(n(x+y))$$ and the sum turned into the following $$\sum_{n=1}^{\infty}\frac{\sin nx \cdot \sin ny}{n^2}=\sum_{n=1}^{\infty}\frac{\cos(n(x-y))}{n^2}-\sum_{n=1}^{\infty}\frac{\cos(n(x+y))}{n^2}$$ because both series converge. And now we have to calculate the two sums above that have the form $$\sum_{n=1}^{\infty}\frac{\cos(na)}{n^2}$$ for $a\in \mathbb R$ . Then I tried the following: $$\sum_{n=1}^{\infty}\frac{\cos(na)}{n^2}=\textrm{Re}\sum_{n=1}^{\infty}\frac{e^{ina}}{n^2}$$ but then things are getting a bit difficult because the function $\textrm{Li}_2(z)$ appears and I don't know how to handle it from there...
Thanks for the help in advance!","['summation', 'calculus', 'sequences-and-series', 'derivatives', 'complex-numbers']"
4776534,Cohomology ring of symmetric products (of manifolds),"Let $S_g$ be a closed, orientable surface of genus $g$ (new notation in light of the first comment). I am looking for results to determine explicitly the (co)homology groups and/or cohomology ring structure (in integer or rational coefficients) of the $n$ -fold symmetric product of $S_g$ , denoted by $SP^n\left(S_g\right)$ . In this direction, I’m aware of a theorem of IG MacDonald for algebraic curves, but I don’t know a reference to a useful, general result for $S_g$ . More generally, is there a good reference to computing the space structure of $SP^n(M)$ or the ring structure of cohomology ring $H^*\left(SP^n(M)\right)$ for $k-$ dimensional closed, orientable topological manifolds $M$ ? In particular, I’m interested in the case when $M$ is a finite product of spheres. I know that DV Gugnin has a result on the functoriality of the homology of symmetric products with integer coefficients but the result is purely theoretical and not helpful for my purpose of determining explicitly the ring structure of $H^*\left(SP^n(M)\right)$ . I also know that Ozsvath-Szabo showed that $H_{1}\left(S_g \right) \approx H_{1} \left(SP^g\left(S_g\right) \right)$ , but I’m looking for more information about either the space $SP^n(M)$ or the (co)homology of $SP^n(M)$ for arbitrary $n$ so that I can find explicitly the cohomology ring $H^*\left(SP^n(M)\right)$ . Any help or references would be appreciated.","['manifolds', 'algebraic-geometry', 'homology-cohomology', 'algebraic-topology']"
4776569,Well-urned balls,"You want to maximize the number of red balls you get. Any red ball you draw you may keep. Any green ball you draw ends the game. Urn A contains one red and one green ball. You just draw.
You get an expectation value $E(A)=1/2$ ( $p=1/2$ you get the red and draw the green, $1-p=1/2$ you immediately draw a green one). Urn B also contains one red and one green ball, but if you draw
the red one, another red goes into B. You can do the infinite
sum in your head, the expectation value is $1$ ball. Urn C contains two balls, where the content is equidistributed over the number of red balls. This means with $p=1/3$ both balls are red and you get both, with $q=1/3$ both are green and you get none, and with $1-p-q=1/3$ it contains one red and one green as in A,
giving a red ball with fifty-fifty chance again. Makes $(2+0+1/2)/3=5/6$ balls. Urn D is like C, but has the first ball being red with $p=1/2$ and the other too.
You get both red balls with $p=1/2*1/2=1/4$ , none with $q=1/4$ and a half
with $1-p-q=1/2$ . Makes $3/4$ balls. Putting back would make no sense for C and D, since with finite probability
you have only red balls and the expectation value thus is $\infty$ . Observe $E(A)<E(D)<E(C)<E(B)$ . Is this true generally for Urn A and B containing $n$ red and $n$ green balls, Urn C $2n$ balls equidistributed over the number of red balls and Urn D as C, only having each ball the probability $p=1/2$ that it is red? (I sorta doubt it, the advantage of Urn B gets close to zero with large $n$ .) Please give general formulae for any $n$ .","['statistics', 'probability-distributions']"
4776713,Distribution of $Y = X \bmod 2\pi$ with $X$ being a Cauchy distribution,"Let $X$ be a Cauchy distribution with parameter $\theta$ , that is to say, its density function is: $$
f(x;\theta) = \frac{\theta}{\pi(x^2 + \theta^2)}
$$ I'm asked to get the distribution of $Y$ where $Y = X \bmod 2\pi$ . I've tackled this problem but I feel like I'm stuck. My attempt so far has been the following: Let's compute the distribution function for $Y$ : $$
F(y) = P(-\pi \leq Y \leq y) = P(\bigcup_{n=-\infty}^{\infty} \{ -\pi + 2\pi n \leq X \leq y + 2\pi n \} )
$$ Integrating $X$ 's density function I reach the following series: $$
F(y) = \sum_{n = -\infty}^{\infty} \int_{-\pi + 2\pi n}^{y + 2\pi n} \frac{\theta}{\pi (x^2 + \theta^2)} = \sum_{n = -\infty}^{\infty} \frac{1}{\pi}\left[ \arctan{\frac{2\pi n + y}{\theta}} - \arctan{\frac{2\pi(n-1)}{\theta}} \right]
$$ I can now differentiate to get the density function of $Y$ : $$
f(y) = \sum_{n=-\infty}^{\infty} \frac{1}{\pi}\frac{1}{1+\left(\frac{2\pi n + y}{\theta} \right)^2}
$$ However, my professor has told me that he expects a closed form solution. In order to do that he has told me that I should use the Poisson kernel, but I'm not exactly sure how should I proceed with that information. I've tried using the logarithmic expression for the inverse tangent to no avail. I've noticed that the density function of $X$ IS the Poisson kernel for the upper half-plane, but I'm not entirely how that could help. I've tried expressing either of the two series I've provided in a form that allows to apply the Poisson kernel summation formula (the one for the circle) but I haven't had success with that approach. If you could help me in any way I would really appreciate it. Finally, I know it is a trope but I have to say: English is not my mother tongue, so I'm sorry if there are any typos. Thanks for your attention. Edit: I wish I could accept both answers because they are helpful in different ways, but the system is what it is. Thank you both for your thorought responses!","['statistics', 'probability-distributions', 'analysis', 'sequences-and-series', 'density-function']"
4776717,"Solving $\frac{8}{1-\cos(4x)} = \frac{5}{\tan^2(2x)} + 3$, I keep getting only one answer","I am trying to solve the following and I keep only getting one answer. $$\dfrac{8}{1-\cos(4x)} = \dfrac{5}{\tan^2(2x)} + 3$$ Firstly, I see that one can set the denominators of the two fractions each equivalent to $1$ . I tried that and I got the same answer both times: $\dfrac{\pi}{8}$ . Then, re-writing the problem as: $$\dfrac{8}{2\sin^2(2x)} = \dfrac{5\cos^2(2x)}{1-\cos^2(2x)} + 3 \tag1$$ and again as: $$\dfrac{4}{\sin^2(2x)} = \dfrac{5\cos^2(2x)}{\sin^2(2x)} + \dfrac{3\sin^2(2x)}{\sin^2(2x)} \tag2$$ and again as: $$4 = 5\cos^2(2x) + 3\sin^2(2x) \tag3$$ and then solving, I got: $$\sin(2x) =  \dfrac{\sqrt2}{2} \tag4$$ which again solved as $$\dfrac{\pi}{8} + \dfrac{\pi k}{4} \tag5$$ The answer says that there is an additional answer: $$\pm \dfrac{2\pi}{3} + \dfrac{\pi k}{2} \tag6$$ but I can't seem to figure out where I lost that answer along the way. Any ideas?","['algebra-precalculus', 'trigonometry']"
4776724,Eigenvalues and Eigenvectors of a block matrix,"I need to find the eigenvalues and eigenvectors of the following matrix: $$D = \begin{bmatrix}
1 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & -i & 0 & 0 \\
0 & 0 & i & 0 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & -1  \\
\end{bmatrix} $$ Now I'm sure my professor does not want me to make some sterile computations, therefore I've tried analyzing the matrix first.
First of all, I noticed the matrix is Hermitian, which means all its eigenvalues must be real numbers. I've looked up the properties of Hermitian matrices but nothing useful came out. Then I noticed I can rewrite $D$ as a block matrix composed of $2\times2$ blocks: $$ \begin{bmatrix}
A & [0] & B \\
[0] & \sigma_2 & [0] \\
C & [0] & F \\
\end{bmatrix}$$ where $\sigma_2$ denotes the second Pauli matrix (I do not believe this to be a mere coincidence).
The upside is that the eigenvalues of all the $2 \times 2$ matrices are trivial or I have already computed them (same thing for the eigenvalues). The main problem is that I've never actually worked with block matrices in any of my courses, so my personal knowledge of the topic is close to zero. I've seen something on Wikipedia about the determinant of matrices of the form: $$ \begin{bmatrix} A & B \\
C & F 
\end{bmatrix}$$ But this does not apply to my case, so maybe the block matrix route is unlikely the right one. Is there some trick to compute the eigenvalues and eigenvectors of such a  peculiar matrix that I'm most likely missing? I'm honestly at a dead end this time so any input is much appreciated.","['matrices', 'linear-algebra', 'block-matrices', 'eigenvalues-eigenvectors']"
4776725,"Why is a certain projective limit of weighted symmetric Fock space, namely $\bigcap\limits_{\tau \in T, p\ge 1 } \mathcal{F}(H_\tau,p)$, separable","I have a question regarding separability of a certain locally convex space. Let $H_{\tau}:=H^{\tau_1}(\mathbb{R}^n,\tau_2(x)dx)$ the weighted Sobolev Hilbert space with $\tau_1 \in \mathbb{N}, \tau_2(x)\in C^\infty(\mathbb{R}^n)$ and with $\tau_2 \ge 1$ . We denote $\mathcal{D}(\mathbb{R}^n)=\bigcap_\tau H_\tau$ , when $\tau$ is arbitrary with stated properties above (and call $T$ the index set of all these $\tau$ ), endowed with the projective topology with respect to all the embeddings $\iota_\tau: \mathcal{D}(\mathbb{R}^n) \longrightarrow H_\tau$ , i.e. the coarsest topology for which all these embeddings are continuous. This space is a nuclear and separable. The folowing construction will be the case, when $n=1$ . Also consider the symmetrical Fock spaces $$\mathcal{F}(H_\tau):=\bigoplus^\infty_{n=0}H_{\tau, \mathbb{C}}^{\hat \otimes n}$$ of nth-symmetrized Hilbert space tensor products of the Sobolev spaces introduced above. This is the direct sum in of a countable collection of Hilbert spaces. We now consider the weighted Fock spaces $$\mathcal{F}(H_\tau,p)=\lbrace (f_n) \in \mathcal{F}(H_\tau):\sum_n \|f_n\|^2_{H_{\tau,\mathbb{C}}^{\hat \otimes n}}\cdot p_n < \infty \rbrace,$$ for sequences $(p_n)$ with $p_n\ge 1$ for all $n\in \mathbb{N}$ . The space I am interested in is $$\mathcal{F}_{\text{fin}}(\mathcal{D})=\bigcap\limits_{\tau \in T, p\ge 1 } \mathcal{F}(H_\tau,p)$$ the locally convex space as the projective limit of the weighted Fock spaces endowed with the projective topology, i.e. the initial topology with respect to all the embeddings $\iota_{\tau, p}:\mathcal{F}_{\text{fin}}(\mathcal{D}) \longrightarrow \mathcal{F}(H_\tau,p)$ . I am wondering why this space is separable.. The separability is necessary to apply the so called projection spectral theorem in its most general form see [Y.M. Berezansky et al. Spectral Methods in Infinite-Dimensional Analysis, Chapter 3, Theorem 2.7]. This space gets used in the paper [Yu. M. Berezansky and V. A. Tesko. “The investigation of Bogoliubov func-
tionals by operator methods of moment problem”. In: Methods Funct. Anal.
Topology 22.1 (2016), pp. 1–47. issn: 1029-3531] as if it is separable, but the author does not specify why this is the case. My current thoughts lead to the following: If I can show, that $$\bigcap_{\tau \in T}(H^{\tau_1}(\mathbb{R},\tau_2(x)dx))^{\otimes n}=\bigcap_{\tau \in T}H^{\tau_1}(\mathbb{R}^n,\tau_2^{\otimes n}(x)dx)=\mathcal{D}(\mathbb{R}^n),$$ then $\mathcal{F}_{\text{fin}}(\mathcal{D})=\bigoplus_k \mathcal{D}(\mathbb{R}^k)_{\text{symm},\mathbb{C}}$ as a set, i.e. the set of finite sequences. And since $\mathcal{D}(\mathbb{R}^k)$ is separable for all $k \in \mathbb{N}$ , we can conclude $$\bigoplus_k \mathcal{D}(\mathbb{R}^k)_{\text{symm},\mathbb{C}}$$ is separable as a topological sum. Now If we additionally show that the topology of the sum is finer, than the topology of the projective limit, we are done.","['locally-convex-spaces', 'topological-vector-spaces', 'functional-analysis']"
4776729,Trace convergence of finite dimensional truncations?,"Suppose $P, Q$ are two positive, self-adjoint trace-class operators on $\ell^2$ . Let $V_k \colon \ell^2 \to \mathbb{R}^k$ denote the projection onto the first $k$ coordinates. Suppose also that $Q$ is strictly positive in the sense that $\langle Qx, x \rangle > 0$ for all $x \neq 0$ . Consider the sequence of operators: $$
P_k = V_k P V_k^\ast, \quad \mbox{and} \quad Q_k = V_k Q V_k^\ast, 
\quad \mbox{for}~k \geq 1.  
$$ I am wondering if it is true that $$
\lim_{k \to \infty} \mathrm{tr}(Q_k (P_k Q_k + I)^{-1}) = \mathrm{tr}(Q (P Q + I)^{-1}). 
$$ The operators $I$ above are (slightly abusing notation) the identity on $\mathbb{R}^k$ and $\ell^2$ , respectively. In the case where $P, Q$ are diagonal I can see that this is true, but otherwise I am unsure.","['operator-theory', 'linear-algebra']"
4776740,Do the roots of this function all reside on the line $\Re(s) = \frac12$?,"In this video , the following nice integral expression is derived: $$\int_0^{\infty}\frac{\cos(x)}{(x^2+1)} \,\mathrm{d}x = \frac{\pi}{2\,\mathrm{e}}$$ Wondered about the more general form: $$f(s) = \int_0^{\infty}\frac{\cos(x)}{(x^2+1)^s} \,\mathrm{d}x$$ and managed to derive: $$g(s) = f(s)\,\Gamma(s)\,2^s = {}_2F_0 \left([s, 1-s],[], -\frac12\right)\cdot \frac{\pi}{\mathrm{e}} \qquad s \in \mathbb{C}$$ which has the functional equation: $$g(s) = g(1-s)$$ All roots of $g(s)$ seem to be complex and to reside on the critical line with $\Re(s) = \frac12$ , e.g.: 0.5000000000 + 2.962548535*I
0.5000000000 + 4.534490718*I
0.5000000000 + 5.879867200*I
0.5000000000 + 7.107583837*I
0.5000000000 + 8.258936409*I
0.5000000000 + 9.355093826*I
... Since these roots look quite regularly spaced, could the conjecture that they all reside on the critical be in reach of a proof?","['integration', 'number-theory', 'roots', 'hypergeometric-function']"
4776774,Is the notation $\bigcup S$ for unions well known outside set theory?,"Let $X$ be a set and $S\subseteq\mathcal{P}(X)$ be a family of subsets of $X$ . I'd like to know if I can replace this commonly used notation $$\bigcup_{Y\in S}Y$$ by this one $$\bigcup S$$ and be understood by any mathematician outside of set theory; that is, I want to know if the second notation is as well-known as the first one. I saw it used in a famous set theory book so my guess is it's well spread among set theorists. However every other mathematician I've seen uses the first notation. (The requirement ""any mathematician outside of set theory"" is naturally an wishful exaggeration. I'll be satisfied if more than 90% of mathematicians outside of set theory understand me. Damn it, I'll pay if 90% of just topologists and analysts understand the notation.)","['elementary-set-theory', 'notation']"
4776813,Book recommendations for functional analysis and analysis on manifolds to study optimization.,"I'm about to start a PhD in an optimization lab, and I'd like to study some analysis beforehand. So far, I've studied linear, non-linear, and integer optimization during my undergrad courses, and they were pretty theoretical. However, I've heard that functional analysis and analysis on manyfolds also hold many important results for optimization (mostly for non-linear), and I'd like to take a look before the PhD starts. For functional analysis, my uni uses the Kreyszig's book, and a random one for analysis on manyfolds. However, I'm aware that only a few chapters in these books are directly relevant to optimization. For instance, in Kreyszig's book, I know that the sections on metric spaces and Banach's fixed point theorem are essential, but I'm uncertain about the rest. In a nutshell: Could you guys recommend me some books and chapters in each book that will be most helpful for this purpose? :)","['optimization', 'book-recommendation', 'functional-analysis', 'analysis']"
4776850,Best estimator of a matrix signal with binary entries,"Setup: Given that we have a noisy matrix signal $\breve{B}\in\mathbb{R}^{p\times L}$ of the true signal $B\in\mathbb{R}^{p\times L}$ , where the empirical distribution of the rows of $B$ converge to $\bar{B}\in\mathbb{R}^L$ where $\bar{B}\sim\text{Categorical}(\pi)$ , $\pi\in\mathbb{R}^L$ , and $\sum_{l=1}^L\pi_l=1$ (i.e., $\pi$ is a probability vector). The rows of $B$ are generated by sampling independently from the distribution $\text{Categorical}(\pi)$ . So the rows of $B$ are independent one-hot vectors. The goal is to estimate $B$ from $\breve{B}$ . We have additional statistical information about $\breve{B}$ , namely: The empirical distribution of the rows of $\breve{B}$ converges to $M\bar{B}+G$ , where: $M\in\mathbb{R}^{L\times L}$ is a non-random component; $G\sim \mathcal{N}(0,\Sigma)$ , where $\Sigma\in\mathbb{R}^{L\times L}$ , is the random noise component. Note that we have access to $\pi$ , $M$ , and $\Sigma$ . To be more precise, the convergence of rows means $$
\frac{1}{p}\sum_{j=1}^p\breve{B}_j
\rightarrow
\mathbb{E}\big[M\bar{B}+G\big],
$$ where $\breve{B}_j$ denotes the $j$ th row of $\breve{B}$ . Question: What is the best estimator $\widehat{B}_j=f(\breve{B}_j)$ ? Note that here, estimation is done row wise with $j\in\{1,\dots,p\}$ . The best that I can think of is $$
f(\breve{B}_j)=\mathbb{E}\Big[\bar{B}\,\Big|\,M\bar{B}+G=\breve{B}_j\Big].
$$ Can we do better than this since we know that rows of $B$ are one-hot vectors (and we also know the prior distribution of the signal $B$ )? Thanks.","['statistical-inference', 'statistics', 'estimation', 'signal-processing', 'probability']"
