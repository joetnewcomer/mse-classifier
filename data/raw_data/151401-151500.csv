question_id,title,body,tags
2528141,Determine if $\lim_{n\rightarrow \infty }\int_{-\infty}^{\infty}\frac{(\sin(x))^{n}}{x^2+1}d\lambda(x)$ exists and calculate its value if it exists.,"Determine if the next limit exists and calculate its value if it exists.
$$\lim_{n\rightarrow \infty }\int_{-\infty}^{\infty}\frac{(\sin(x))^{n}}{x^2+1}d\lambda(x)$$
where $\lambda$ is the Lebesgue measure in $\mathbb{R}$ My attepmt: We consider $f_{n}(x)=\frac{(\sin(x))^{n}}{x^2+1}$, and $g(x)=\frac{1}{x^2+1}$. Note that $g$ is absoltely improper Riemann integragle, that is, $\int_{-\infty}^{\infty}|g(x)|dx<\infty$ because
$$\int_{-\infty}^{\infty}|g(x)|dx=\int_{-\infty}^{\infty}g(x)dx=\int_{-\infty}^{\infty}\frac{1}{x^2+1}dx=2\int_{0}^{\infty}\frac{1}{x^2+1}dx=\pi<\infty.$$
Therefore, $g$ is Lebegsue integrable in $[-\infty,\infty]$ and 
$$\int_{-\infty}^{\infty}g(x)d\lambda(x)=\int_{-\infty}^{\infty}g(x)dx=\pi <\infty.$$ Note that $|f_{n}(x)|\leq g(x)$ for all $x\in [-\infty,\infty]$. Also, $\{f_{n}(x)\}_{n}$ converge for each $x\in [-\infty,\infty]$, then we define $f$ as
$$f(x)=\lim_{n\rightarrow \infty }f_{n}(x)=\left\{\begin{array}{ll}1 & \mathrm{if}\:x=\frac{\pi}{2}+2m\pi \mbox{for some }m\in\mathbb{N} \\
-1 & \mathrm{if}\:x=\frac{3\pi}{2}+2m\pi \mbox{ for some }m\in\mathbb{N}\\ 0 & \mbox{otherwise} \end{array}\right.$$ We have that $\int_{-\infty}^{\infty}f d\lambda(x)=0$ because $f$ is not zero in a discrete set. Is it correct? Then, by Dominated Convergence Theorem we have that $f$ is Lebesgue integrable and $$\begin{array}{rcl}\lim_{n\rightarrow \infty }\int_{-\infty}^{\infty}\frac{(\sin(x))^{n}}{x^2+1}d\lambda(x)&=&\lim_{n\rightarrow \infty }\int_{-\infty}^{\infty}f_{n}(x)d\lambda(x)\\ &=&\int_{-\infty}^{\infty}\lim_{n\rightarrow \infty }f_{n}(x)d\lambda(x) \\ &=&\int_{-\infty}^{\infty}f(x) d\lambda(x)\\&=&0.\end{array}$$ The question: Is my attempt correct? Do I have an error? and if I have it, what is it?","['real-analysis', 'lebesgue-measure', 'proof-verification', 'lebesgue-integral', 'measure-theory']"
2528201,Find the marginal distributions (PDFs) of a multivariate normal distribution,"Let $\mathbf{x}\in\Bbb{R}^n$ be a multi-variate normal vector with mean $\bar{\mathbf{x}}\in\Bbb{R}^n$ and covariance matrix $\Sigma\in\Bbb{S}_{++}^n$, where $\Bbb{S}_{++}^n$ denotes the space of all $n\times n$ symetric posotive definite matrices with real entries. Also, let $f$ denote the probability density function of $\mathbf{x}$, i.e., 
$$
f(\mathbf{x})
=
\frac{1}{(2\pi)^{\frac{n}{2}}\vert\Sigma\vert^{\frac{1}{2}}}
\exp
\left(
-\frac{1}{2}
(\mathbf{x}-\bar{\mathbf{x}})^\top
\Sigma^{-1}
(\mathbf{x}-\bar{\mathbf{x}})
\right).
$$
In general we suppose that $\Sigma$ is a non-diagonal matrix. Thus, the elements of $\mathbf{x}=(x_1,\ldots,x_j,\ldots,x_n)^\top$, i.e., the variables $x_j$, $j=1,\ldots,n$ are dependent normal variables. I am interested in the computation of the marginal densities of $x_j$, $j=1,\ldots,n$. In the corresponding Wikipedia's article , it states that: To obtain the marginal distribution over a subset of multivariate normal random variables, one only needs to drop the irrelevant variables (the variables that one wants to marginalize out) from the mean vector and the covariance matrix. It also gives the following example: Example: Let $\mathbf{x}=(x_1, x_2, x_3)^\top$ be multivariate normal random variables with mean vector $\bar{\mathbf{x}}=(\bar{x}_1,\bar{x}_2, \bar{x}_3)^\top$ and covariance matrix $\Sigma$ (standard
  parametrization for multivariate normal distributions). Then the joint
  distribution of $\mathbf{x}^\prime=(x_1, x_3)^\top$ is multivariate
  normal with mean vector $\bar{\mathbf{x}}^\prime=(\bar{x}_1, \bar{x}_3)^\top$ and covariance matrix  $$ \Sigma^\prime = \begin{bmatrix} \Sigma_{11} & \Sigma_{13} \\  \Sigma_{31} & \Sigma_{33} \end{bmatrix}. $$ Does this mean that, if we want to obtain the marginal probability density function of each of $x_j$, for all $j=1,\ldots,n$, we just use the diagonal entries of $\Sigma$ as the variances of the respective random variables? What bugs me is that this would be the same as if $\Sigma$ was diagonal (and thus $x_j$'s independent). Does the solution lie in the parentheses of the above example (i.e., standard parametrization for multivariate normal distributions )?","['probability', 'density-function', 'normal-distribution']"
2528211,"If C is a chain of set S, $C \subseteq S$ or $C \subseteq S \times S$?","I'm doing an exercise, which asks us to prove that ""every infinite set $S \subseteq \mathbb N^n_0$ contains an infinite chain $C\subseteq S$."" But since C is a chain of set S, C is supposed to be a relation on S, which means, suppose P is a poset of S, $P=(S,\preceq)$, $P \subseteq S \times S$, and $C \subseteq P$, so actually $C \subseteq S \times S$. Or maybe in this context it's also understandable to write $C \subseteq S$?
Could anyone please tell me what's wrong here? I think I've misunderstood some concepts. Thank you very much.","['intuition', 'elementary-set-theory']"
2528216,Polarization Operators identity and $GL_{\ell}(\mathbb{R})$,"( This question is also posted in Mathoverflow: https://mathoverflow.net/questions/286539/polarization-operators-and-the-action-of-gl-ell-mathbbr-on-mathcalr ) Let $X$ be a matrix of variables $x_{ij}$ of size $\ell\times n$:
\begin{equation*}
X=\left(\begin{array}{cccc} x_{11}&x_{12} &\dots & x_{1n}\\
x_{21}&x_{22} &\dots & x_{2n}\\
\vdots& \vdots & \ddots & \vdots \\
x_{\ell 1}&x_{\ell 2} &\dots & x_{\ell n}\\\end{array}\right).
\end{equation*}
The ring $\mathcal{R}_{n}^{(\ell)}=\mathbb{R}[X]$ of polynomials in $\ell$ sets of $n$ variables is defined as the $\mathbb{R}$-vector space generated by the monomials:
$$X^{A}=\prod_{i=1}^{\ell}\prod_{j=1}^{n}x_{ij}^{a_{ij}}$$
The multidegree of a monomial in this ring is given by:
\begin{equation*}
\deg\left(X^{A}\right):=\left(\sum_{j=1}^{n}a_{1j},\sum_{j=1}^{n}a_{2j},\dots,\sum_{j=1}^{n}a_{\ell j}\right)
\end{equation*}
So an element $f\in\mathcal{R}_{n}^{(\ell)}$ have the form
\begin{equation*}
\displaystyle{f(X)=\sum_{A\in\mathbb{N}^{\ell\times n}}f_{A}X^{A}}
\end{equation*}
The multidegree of $f$ is given by:
\begin{equation*}
\deg\left(f(X)\right):={\max}_{{\rm grlex}}\left\{\deg\left(X^{A}\right):\ f_{A}\neq 0\right\}.
\end{equation*}
where the maximum is taken w.r.t. the graded lexicographic order in $\mathbb{N}^{\ell}$. So the ring $\mathcal{R}_{n}^{(\ell)}$ is $\mathbb{N}^{\ell}$ graded. Soit $Q$ la matrice diagonale suivante:
\begin{equation*}
Q=\left(\begin{array}{cccc}
q_1&0&\dots&0\\
0&q_2&\dots&0\\
\vdots&\vdots&\dots&\vdots\\
0&0&\dots&q_{\ell}\\
\end{array}\right)
\end{equation*}
A polynomial $f(X)\in\mathcal{R}_{n}^{(\ell)}$ is said to be homogeneous of multidegree ${\bf d}$ if the following condition holds:
\begin{equation*}
f(QX)={\bf q}^{\bf d}f(X).
\end{equation*}
where ${\bf q}=(q_1,\dots,q_{\ell})$, ${\bf d}=(d_1,\dots,d_{\ell})$ , ${\bf q}^{\bf d}=q_1^{d_1}\dots q_{\ell}^{d_{\ell}}$. It's well known that For a matrix of variables $Y=(y_{{ij}})$ of size $\ell\times n$ and $f\in\mathcal{R}_{n}^{(\ell)}$ an homogeneous polynomial of multidegree ${\bf d}$, then for every matrix $M$ of size $\ell\times\ell$ we have
  \begin{equation*}
f(MX)=\sum_{\big\{K\in\mathbb{N}^{\ell\times\ell}:\ \big\vert{K}\big\vert={\bf d}\big\}}
\frac{M^{K}}{K!}\,\prod_{i=1}^{\ell}\prod_{j=1}^{\ell}P_{j,i}^{k_{ij}}
\big(f(Y)\big),
\end{equation*}
  where $$K!:=\displaystyle{\prod_{i=1}^{\ell}\prod_{j=1}^{\ell}k_{ij}!}$$ $$\displaystyle{M^{K}=\prod_{i=1}^{\ell}\prod_{j=1}^{\ell}m_{i,j}^{k_{ij}}}$$ and the polarization operator $P_{ik}$ is given by
  $$P_{i,k}:=\displaystyle{\sum_{j=1}^{n}x_{ij}\frac{\partial\ }{\partial y_{kj}}}.$$ The notation $\ \big\vert{K}\big\vert={\bf d}$ represents the set of all squares matrices $K$ of order $\ell$ such that $\displaystyle{\sum_{j=1}^{n}k_{ij}=d_{i}}$, for all $i$ such that $1\leq i\leq\ell$. My question: Reading Claudio Procesi book I saw that the ring $\mathcal{R}_{n}^{(\ell)}$ is closed under polarization operators $P_{ik}$ if and only if is closed under the action of the general linear group $GL_{\ell}(\mathbb{R})$. Using the formula for $f(MX)$ above I can understand why geing closed under the $P_{ik}$ implies that $\mathcal{R}_{n}^{(\ell)}$ is closed under the right side action of $GL_{\ell}(\mathbb{R})$. But, how to find the matrices $M$ to show the reciprocal of this result. Thank for any hint on this.","['combinatorics', 'algebraic-groups', 'invariant-theory', 'generalized-polarization-modules']"
2528225,How can I calculate $\alpha=\arccos\left(-\frac{1}{4}\right)$ without using a calculator?,"How can I calculate $\alpha$, without using a calculator? $\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad$ I know $x = -\frac{1}{4} \implies y= \frac{\sqrt{15}}{4}, $ now how can I calculate $$\arccos\left(-\frac{1}{4}\right) = \alpha,\quad \arcsin\left(\frac{\sqrt{15}}{4}\right) = (180° - \alpha),$$  without using a calculator? How did the Greeks to calculate the angle?","['numerical-methods', 'trigonometry', 'geometry']"
2528378,What is known about the space of geodesic vector fields on a surface?,"Let $M$ be a compact two-dimensional manifold. I'm interested in geodesic vector fields on $M$: smooth vector fields $v$ that have the property that every integral curve of the vector field is (locally) a geodesic. In other words
$$\nabla_v v = \phi v$$
where $\phi$ is a function $M\to\mathbb{R}$. What is known about the space of such vector fields? Is there a way to characterize or parameterize them? For example, I imagine one possible construction is to pick a closed subset $S$ of $M$, construct the signed distance function $d_S$ to $S$, and set $v = \nabla d_S$. This vector field is undefined at the cut locus of $S$, but presumably that can be fixed by adjusting the magnitude of $v$ so that it vanishes near the cut locus.","['differential-geometry', 'geodesic']"
2528391,When can we extend a countably-additive set function on a Dynkin system to be a (probability) measure?,"Let $X$ be an arbitrary set. A class of subsets $\mathcal{A}$ of $X$ is called a $\lambda$-system (or Dynkin system) if (1) $X\in\mathcal{A}$ (2) $A\in\mathcal{A}\Rightarrow A^c\in\mathcal{A}$ (3) $A_1,A_2,\cdots,\in\mathcal{A}$ and $A_n\cap A_m=\phi\ \forall\ m\neq n$ imply $\cup_nA_n\in\mathcal{A}$ Suppose $\mu:\mathcal{A}\rightarrow [0,1]$ satisfies (a) $\mu(X)=1$ and $\mu(\phi)=0$. (b) $\mu(\cup_nA_n)=\sum_n\mu(A_n)$ if $A_n\cap A_m=\phi$ for all $n\neq m$ Is it true that $\mu$ can be extended to a probability measure (or just finitely additive probability measure) on the sigma algebra generated by $\mathcal{A}$? That is, does there exist a probability measure $p$ on $(X,\sigma(\mathcal{A}))$ such that $p(A)=\mu(A)\ \forall\ A\in\mathcal{A}$? If not, are there sufficient conditions ensuring the existence of such extension?","['probability-theory', 'examples-counterexamples', 'measure-theory']"
2528397,$A$ be a nonzero finite abelian group then $A$ is not a projective or injective $\Bbb Z$ module.,To prove that if $A$ be a nonzero finite abelian group then $A$ is not a projective or injective $\Bbb Z$ module. Ref: Dummit Foote Sec 10.5 Q7.,"['abstract-algebra', 'ring-theory', 'modules']"
2528435,Show that the Moore plane is not normal,"Definition: A Hausdorff space is normal (or: $T_4$) if each pair of disjoint closed sets have disjoint neighborhoods. Then, we have Exercise 5, pg. 158, Dugundji's Topology: Let $X$ be the upper of the Euclidean plane $E^2$, bounded by the $x$-axis. Use the Euclidean topology on $\{(x,y)\,|\, y>0\}$, but define neighborhoods of the points $(x,0)$ to be $\{(x,0)\}\cup [\text{open disc in $\{(x,y)\,|\,y>0\}$ tangent to the $x$-axis at $(x,0)$}]$. Prove that this space is not normal. It is easy to see that $X$ is Hausdorff. So, what we need is to find a pair of closed sets that fail to satisfy the definition given above. Here , Alice Munro says that $A=\{(x,0)\,|\, x\in \Bbb Q\}$ and $B=\{(x,0)\,|\, x\in \Bbb R-\Bbb Q\}$ are such sets. I can see that they are closed, but how can I show that they do not admit disjoint neighborhoods? (intuitively true, but I'm having difficulty to write it down...)","['general-topology', 'smooth-manifolds', 'separation-axioms']"
2528440,I'm calculating exterior derivatives and my answer is off by a factor of -1. What am I doing wrong?,"For example. I want to find the exterior derivative of $$\omega=(-3x-2y-z)dx +(2x^2+3y^3+4z^4)dy + (x+2y+3z)dz$$ Here's what I did: $$ \begin{align} 
d\omega &= \left[\frac{\partial}{\partial x}(-3x-2y-z)\ dx+\frac{\partial}{\partial x}(2x^2+3y^3+4z^4)\ dy+\frac{\partial}{\partial x}(x+2y+3z)\ dz\right]dx \\
&+ \left[\frac{\partial}{\partial y}(-3x-2y-z)\ dx+\frac{\partial}{\partial y}(2x^2+3y^3+4z^4)\ dy+\frac{\partial}{\partial y}(x+2y+3z)\ dz\right]dy \\
&+ \left[\frac{\partial}{\partial z}(-3x-2y-z)\ dx+\frac{\partial}{\partial z}(2x^2+3y^3+4z^4)\ dy+\frac{\partial}{\partial z}(x+2y+3z)\ dz\right]dz \\
&= (-3\ dx +4x\ dy +1\ dz)\ dx +(-2\ dx+9y^2\ dy+2\ dz)\ dy+(-1\ dx+16z^3\ dy+3\ dz)\ dz \\
&= 4x\ dy\ dx+ dz\ dx-2\ dx\ dy+2\ dz\ dy-1\ dx\ dz+16z^3\ dy\ dz \\
&= -4x\ dx\ dy-2\ dx\ dy-1\ dx\ dz-1\ dx\ dz-2\ dy\ dz+16z^3\ dy\ dz \\
&= (-4x-2)\ dx\ dy-2\ dx\ dz+(16z^3-2)\ dy\ dz
\end{align} $$ However, Maple says this: Maple says I'm off by a factor of -1? What am I doing wrong? Thank you!","['multivariable-calculus', 'differential-forms', 'differential-geometry', 'exterior-algebra']"
2528453,Question about the relation between the Weierstrass equation and weighted projective space,"I am reading a review on Toric Geometry for string theorists by Harald Skarke ( arXiv:hep-th/9806059 ). In section 3, the author says A standard way of describing an elliptic curve is by embedding it into $\mathbb{P}^2 = (\mathbb{C}^3\setminus\{0\})/(\mathbb{C}\setminus\{0\})$.....The elliptic curve is embedded in this space via the Weierstrass equation
  $$ \label{eq:w1}y^2 z = x^3 + a x z^2 + b z^3$$ An alternative description can be given in terms of the weighted projective space $\mathbb{P}^{(2,3,1)}$ defined just like $\mathbb{P}^2$, but with the equivalence relation changed to $$(x, y, z) \sim (\lambda^2 x, \lambda^3 y, \lambda z) \quad \text{  for any } \lambda \in \mathbb{C}\setminus\{0\}$$ and the Weierstrass equation changed to $$\label{eq:w2}y^2 = x^3 + a x z^4 + b z^6$$ Questions Given the description of the two projective spaces as embeddings in $\mathbb{C}^3$, how does one obtain the two Weierstrass equations? Is there a way to transform the first Weierstrass equation into the other? Thanks in advance!","['algebraic-geometry', 'string-theory', 'toric-geometry', 'elliptic-curves', 'projective-space']"
2528478,Find an equation of a tangent line,"Find an equation of a the tangent line to the graph of $x^2 - xy - y^2= 1$ when $(x,y) = (2,1)$. Here's what I have so far: $$ x^2 - xy - y^2 = 1
$$ $$\implies x^2 - xy - y^2  - 1 = 0
$$ $$\implies y' = 2x - xy' - 2yy'
$$ $$\implies -2x = y'(-x -2y)
$$ By trying to simplify the equation so that $y'$ is on one side, is that the correct way to begin solving the problem in the first place? If so, is my simplification correct as shown above?","['derivatives', 'tangent-line', 'calculus']"
2528486,Vector Field vs. Gradient Field?,"Suppose we have a gradient field $\vec{F}$. Is there thus a vector field $\vec{G}$ such that the curl of ($\vec{G}$) = $\vec{F}$? So, I'm trying to find examples/counter-examples. If we take a vector field and div(curl($\vec{F}$)) $\ne$ 0, then we know this isn't the case. This is my thinking. Can anyone guide me through?","['multivariable-calculus', 'vectors', 'calculus', 'vector-spaces']"
2528493,Ergodicity of a measurable transformation on $\mathbb{T}^2$,"Let the dynamical system $(\mathbb{T}^2,\mathcal{B}_{\mathbb{T}^2},m_{\mathbb{T^2}},T)$ where $m_{\Bbb{T}^2}=m_{\Bbb{T}} \times m_{\Bbb{T}}$ the product measure of the Lebesgue measure $m$ on the circle $\Bbb{T}$ and $$T(x,y)=(x+a,y+2x+b) \text{mod1}$$ It is not difficult to prove using Fubini's theorem that $T$ is measure preserving For which $a,b \in \Bbb{R}$ is $T$  ergodic? I'm new to ergodic theory and i would appreciate any help solving this problem. My thoughts are to use the theorem which states that: If $f:\Bbb{T^2} \to \Bbb{T}^2$ such that $f \in L^2(\Bbb{T}^2)$ and  $f(T(x,y))=f(x,y) \Rightarrow$ $f$ is constant almost everywhere,then $T$ is ergodic. To use this i took the Fourier expansion of $f$ in $\Bbb{T^2}$ (which converges to $f$ with respect to the $L^2$ norm) to find the form of the values $a,b$ but i could not solve it. Can someone help me? Thank you in advance.","['ergodic-theory', 'dynamical-systems', 'fourier-analysis', 'measure-theory']"
2528495,"Product of limits: If $\lim_{n\to\infty} u_nv_n=0$, does it mean that $\lim_{n\to\infty} u_n=0$ or $\lim_{n\to\infty} v_n=0$?","I was doing a homework and while I was trying different solutions, I found this instance that puzzled me. If $\lim_{n \rightarrow +\infty} u_n * v_n = 0$ is it true that: $\lim_{n \rightarrow +\infty} u_n = 0$
or 
$\lim_{n \rightarrow +\infty} v_n = 0$. Can I get a counterexample or a proof please ?","['examples-counterexamples', 'sequences-and-series', 'limits']"
2528496,Ideal class group of $\mathbb{Q} ( \sqrt{-23} ) $ is $\cong \mathbb{Z}/3$ by Minkowski bound.,"I am trying to show the above result. First, I compute that the Minkowski bound must be strictly lower than 4. Therefore, for every ideal class, there is a representative, so that its norm is at most 3. Now every ideal $a$ can be decomposed into prime ideals $p_1, \ldots, p_k$. Noting that every rational prime $p$ is contained in some $p_i$, so $N(p_i)$ is a power of $p$. Now $N(a) = N(p_1) \ldots N(p_k) \leq 3$. Therefore $p \leq 3$. Next I would try to decompose (2) and (3) and look at their factors to get the result. My problem actually is to decompose them into prime ideals. How do I do that?",['number-theory']
2528529,Definition of a function and its graph,"I understand that the definition of a function, mathematically, goes beyond that of explicit functions that most physicists and engineers deal with. However, I came across the definition for a function in a specific book which seems circular. It goes as follows. First the book states in English that essentially a function is an assignment from one set to another. Then they define the graph (Gr) of a function to be 
Gr( f ) = {(x, f (x)): x ∈ X}. After introducing Gr, then the book states, ""we may now give
an entirely rigorous definition of a function by saying that a function is a subset,
G, of X × Y which satisfies the condition that for each element x of X there
exists exactly one y in Y such that (x, y) is in G."" But, the y in (x,y) is defined as f(x)....so is this not circular?","['elementary-set-theory', 'functions']"
2528534,When is a matrix the cosine of another matrix?,"Knowing that every nonsingular matrix in $M_n(\mathbb{C})$ (the set of size $n$ matrices with complex entries) is an exponential of some matrix in $M_n(\mathbb{C})$, what can be said to answer the question: For which matrices $A$ in $M_n(\mathbb{C})$, does the equation $$\cos X = A$$ have a solution $X$ in $M_n(\mathbb{C})$?","['matrices', 'matrix-equations', 'matrix-calculus', 'linear-algebra']"
2528535,"Show that $\det(P(t))=\det(P_0) e^{\int_0^t \operatorname{Tr} A(s)\, ds}$","Let $A(t)$ be a continuous family of $n \times n$ matrices and let $P(t)$ be the matrix solution to the initial value problem $P'(t)=A(t)P$, where $P(0)=P_0$. Show that $$\det(P(t))=\det(P_0) e^{\int_0^t \operatorname{Tr} A(s)\, ds}.$$ If we consider $A(t)=
    \begin{pmatrix}
    a & b  \\
    c & d  \\   
    \end{pmatrix}$, $P(t)=
    \begin{pmatrix}
    x_1 & y_1  \\
    x_2 & y_2  \\   
    \end{pmatrix}$, and $W(t)=\det (P(t))$ then $W'=\operatorname {Tr}(A) W$. My question is how do I solve the last system? And I just consider  $2\times 2$ matrix, but the result follows easily for an $n\times n$ matrix; am I right?","['derivatives', 'ordinary-differential-equations', 'initial-value-problems', 'systems-of-equations']"
2528545,Fourier Transform of $\displaystyle \frac{x}{x+ic}$,"$$\int_{-\infty} ^\infty \frac{x}{x+ic}e^{ikx}dx$$ where $c$ is some positive real constant. I've tried substituton $u=x+ic$ giving: $$\int_{-\infty}^\infty \frac{u-ic}{u}e^{ik(u-ic)}du \\
= \int_{-\infty} ^\infty \frac{u-ic}{u}e^{iku}e^{kc}du \\
= e^{kc}\left(\int_{-\infty} ^\infty e^{iku}du-ic\int_{-\infty} ^\infty\frac{e^{iku}}{u}du\right) \\
= e^{kc}\left(\left.\frac{e^{iku}}{ik}\right|_{-\infty} ^\infty -ic\int_{-\infty} ^\infty\frac{e^{iku}}{u}du\right) $$ I don't know how to evaluate the second integral, and I'm not sure if there's just an easier way to do the whole calculation that I'm missing.","['integration', 'fourier-analysis', 'fourier-transform']"
2528547,Can a connected Eulerian graph have an even number of vertices and an odd number of edges?,"I know that the requirements for a Eulerian graph are that all vertex degrees are even and that it is connected. But I am not sure how that would work with the amount of vertices and edges. So, if this is possible, how would we draw a graph like that? If it isn't possible, how can we prove that? Thank you!","['eulerian-path', 'graph-theory', 'discrete-mathematics']"
2528562,Even and odd functions with Fourier series,"I have a general question in partial differential equations. Can we say that when an even function is expressed as a Fourier series, the Fourier cosine series is also the Fourier series? My thinking is that a Fourier series has the form, $$f(x) = \frac{a_0}{2}+\sum^{\infty}_{n=1} a_n cos(nx) + \sum^\infty_{n=1}b_nsin(nx)$$ where $$a_0 = \frac{1}{\pi}\int ^\infty _{-\infty}f(x)dx$$, $$a_n = \frac{1}{\pi}\int ^\infty _{\infty}f(x)cos(nx)dx$$, $$b_n = \frac{1}{\pi}f(x)sin(nx)dx$$ where $cos$ is a even function and $sin$ is a odd function. Then if $f(x)$ is even, multiplying a even and odd function together gives a odd function which is $0$ which would eliminate the $b_n$ term, leaving just $a_0$ and $a_n$ which is the fourier cosine series. Therefore yes this is true","['fourier-series', 'ordinary-differential-equations', 'fourier-analysis']"
2528608,"Find the number of solutions of $|\cos(x)|=\cot(x)+\frac{1}{\sin(x)}$ that lie in $[-4\pi,4\pi]$.","I came across this problem Find the number of solutions of $|\cos(x)|=\cot(x)+\frac{1}{\sin(x)}$ that lie in $[-4\pi,4\pi]$ in a PreCalculus book and I am looking for a simple solution. What I can say: let $\displaystyle f(x)=|\cos(x)|$ and $\displaystyle g(x)=\cot(x)+\frac{1}{\sin(x)}$ The functions $f$ and $g$ are $2\pi$-periodic so it is enough to study the problem on $[0,2\pi]$. The function $g$ is undefined when $x$ is a multiple of $\pi$. $g(x)=\frac{1+\cos(x)}{\sin(x)}$, so $g$ has the same sign than $\sin$. Therefore, there is no solution on $(\pi,2\pi)$ and e can assume that $x\in(0,\pi)$. Let $c=\cos(x)$. Then $c$ is a root of $P(c)=c^4+2c+1$ and $c\in[-1,1]$. By Descartes' Rule of Signe, $P$ has no positive root and $0$ or $2$ negative roots (counted with multiplicity). $c=-1$ is one of them and Synthetic Division gives $P(c)=(c+1)(c^3-c^2+c+1)$. Let $Q(c)=c^3-c^2+c+1$. $Q(-1)\neq 0$, so $P$ has another negative root. By the Lower Bound Rule, $c=-1$ is a lower bound for the roots of $P$ and the other root, say $a$ is in $(-1,0)$. This means that the original equation has at most one solution ($x=\arccos(a)$) in $(\frac{\pi}{2},\pi)$. On the other hand, $f$ is decreasing $(\frac{\pi}{2},\pi)$, $g(x)=\cot(\frac{x}{2})$, so $g$ is decreasing on $(\frac{\pi}{2},\pi)$. Since $f(\frac{\pi}{2})=0<1=g(\frac{\pi}{2})$ and $f(\pi)=1>0=\cot(\frac{\pi}{2})$, we obtain that $f(x)=g(x)$ for exactly one value in $(\frac{\pi}{2},\pi)$. Finally, there is exactly one solution in $[0,2\pi]$, so there are $4$ solutions in the given interval. That's quite convoluted, so I was hoping for a way to shortcut the solution. Any idea?","['algebra-precalculus', 'trigonometry']"
2528609,Prove $\det(1+tA)=1+t\cdot tr(A)+O(t^2)$,"I need help proving $\det(1+tA)=1+t\cdot \operatorname{tr}(A)+O(t^2)$ I'm not really sure where to start due to the $(1+tA)$, the $1$ is throwing me off.","['matrices', 'trace', 'linear-algebra', 'determinant']"
2528639,Constructing a particular Borel set,"I'm working on a question that requests the following: Given $\alpha \in (0,1)$, Construct a Borel set $E_\alpha \subseteq [-1,1]$ such that $\lim_{r\to 0^+} \dfrac{m(E_\alpha \cap [-r,r])}{2r} = \alpha$. I'm totally stumped.  This question is in the same chapter that discusses antiderivatives, but I can't figure out how to use that to my advantage here.","['real-analysis', 'measure-theory']"
2528668,"Does formal smoothness of $R \to R[x,y]/(f)$ imply $(f,f_x,f_y)=R[x,y]$?","I came across this question about formal smoothness implying smoothness in a specific example. That question was old and unanswered, so I'm taking the opportunity to ask it again. I tried to use the hint in Mariano's comment in the above-linked question to show that $(f_x,f_y)=(1)$ in the ring $R[x,y,s,t]/(f(x+s,y+t),s^2,st,t^2)$, but I essentially got nowhere. I also came across some other similar questions here, such as this question , but none of them have concrete answers. I looked at theorem 30.3 (p.233) of Matsumura's Commutative Ring Theory and also this link , but I just cannot see how to adapt those local arguments to this example. So is it possible to show the Jacobian criterion holds when $R$ is this general explicitly (that is, without having to appeal to the conormal sequence and differentials)? Is it easier to show if $R$ is a field?","['algebraic-geometry', 'commutative-algebra']"
2528675,Inverse of a lower triangular matrix,"I got the following question to solve: Given the lower triangular matrix \begin{bmatrix}
    A_{11} & 0 \\
    A_{21} & A_{22}
  \end{bmatrix} of size $n \times n$ (n is a power of 2) where $A_{11}$, $A_{21}$ and $A_{22}$ are matrices of size $(n/2) \times (n/2)$, show that the inverse is, \begin{bmatrix}
    A_{11}^{-1} & 0 \\
    -A_{22}^{-1}A_{21}A_{11} & A_{22}^{-1}
  \end{bmatrix} how do I go about to solve this problem? Edit: the matrix is invertible. Edit: the second matrix should be changed to:
\begin{bmatrix}
    A_{11}^{-1} & 0 \\
    -A_{22}^{-1}A_{21}A_{11}^{\color{red}{-1}} & A_{22}^{-1}
  \end{bmatrix} The inverse was missing.","['matrices', 'linear-algebra', 'inverse']"
2528699,How do we change the basis of a function?,"Consider vector space $\mathbb{F}$ all funcitions with domain and range $\mathbb{R}$ Now, consider the subset of functions, $$f_{k}(x)= 1; x=k$$ $$=0; x\neq k$$ One such function is defined for each real $k$, so there are an infinite number of these functions. Clearly, all these functions (one for each $k$) cannot be linearly combined to give a net $0$ result. That means an infinite number of elements of $\mathbb{F}$ can be linearly independent. So, the dimensionality of $\mathbb{F}$ is infinity. Clearly, any other function $g(x)$ in $\mathbb{F}$ can be represented as a linear combination of the $f_{k}(x)$ functions, with the component of $g(x)$ in $f_k(x)$ being $g(k)$. So, am I right in saying that this subset of $f_{k}(x)$ functions is a basis for the vector space of functions, kind of like the $i, j, k$ unit vectors for arrows? If yes, then, how do we find the other non-obvious basis for functions? I mean it was obvious that all $f_{k}(x)$ are linearly independent. If any infinite set of functions is given, how do we find out that they are linearly independent? And if they are linearly independent, then how do we calculate the components of any other function in $\mathbb{F}$ in that basis?","['change-of-basis', 'functions', 'vector-spaces']"
2528711,"Fourier series of $f(x) = 1$ for $x\in[-\pi,0]$, $f(x) = -1$ for $x\in[0,\pi]$.","I am attempting to solve a Fourier series problem where we have the question defined by a piecewise function: $$f(x) = \begin{cases} 1 & -\pi \leq x \leq 0 \\ -1 & 0 \leq x \leq \pi \end{cases}$$ I can solve it out where I calculate that $A_{n} = 0$, and $A_{0} = 1$, and I then integrate 
$$B_{n} = \frac{2}{\pi} \, (1-\cos(\pi \,n))$$ At this point when trying to solve for an equation, it doesn't match up to any of the example solutions online I can find.","['fourier-series', 'ordinary-differential-equations']"
2528720,"Values of $a,b,c$ such that $\lim_{x \to 0}\frac{x(a+b-\cos x)-c\sin x}{x^5}=1$","Find the values of $a,b,c$ such that $$\lim_{x \to 0}\frac{x(a+b-\cos x)-c\sin x}{x^5}=1$$ Here's what I have got so far Using L'Hospital's rule,
$$\lim_{x \to 0}\frac{a+b-\cos x+x\sin x-c\cos x}{5x^4}=1$$ So,$a+b-1=0$
Again,
$$\lim_{x \to 0}\frac{\sin x+\sin x+x\cos x+c\sin x}{20x^3}=1$$ But the solution is given as $a=120,b=60,c=180$ There is no way that $a+b=1$","['derivatives', 'trigonometry', 'calculus', 'limits']"
2528730,Resources for probability and counting,"I'm currently taking a Discrete probability course at my University.It isn't going to well,the grad student taking the place of the professor is not able to teach the material effectively.Nor are there any real resources provided by the professor for self learning.Basically I feel defeated.
These are some of the problems included on our last test to give you an idea      of what i'm looking for. Test problems , More , More . I've tried Trev tutor videos and A First Course in Probability by Sheldon Ross but I haven't found them to be too helpful.Are there any other resources that you can suggest to help me?Do you believe I need to just try harder,I've been trying to put in 3 hours a day to study some of these problems but I get stuck a lot and I can't find clear examples that  are similar to the professors work.These seem to be traditional problems but I've been having trouble googling similar questions to them.Any advice would help,thanks a lot.","['reference-request', 'probability', 'advice', 'discrete-mathematics']"
2528731,$\frac{1}{x+1} + \frac{2}{x^2+1} + \frac{4}{x^4+1} + \cdots $ up to $n$ terms in terms of $x$ and $n$.,There's a series which I can't seem to find a way to sum. Any help would be highly appreciated. It goes as follows      $$\frac{1}{x+1} + \frac{2}{x^2+1} + \frac{4}{x^4+1} + \cdots $$ up to $n+1$ terms.   The sum is to be expressed in terms of $x$ and $n$. I tried setting a formula for the $n$-th term and setting it into a difference but ran into a dead end.,"['summation', 'sequences-and-series', 'closed-form']"
2528761,To evaluate $\lim_{x \to 0^-}({\frac{\tan x}{x}})^\frac{1}{x^3}$,"Evaluate $$\lim_{x \to 0^-}({\frac{\tan x}{x}})^\frac{1}{x^3}$$
I tried taking log on both sides and then using L'Hospital rule but its giving complex results.Are there any simpler methods to approach this?","['calculus', 'limits']"
2528814,Is this graph Hamiltonian?,"I know that a Hamiltonian graph has a path that visits each vertex once. But I am not sure how to figure out if this one does. Obviously I can try and trace various different paths to see if one works but that is incredibly unreliable. So my question is, if this graph is Hamiltonian, where would the Hamilton cycle be? And if it is not Hamiltonian, how can we prove it? Thank you!","['graph-theory', 'hamiltonian-path', 'discrete-mathematics']"
2528875,"If $f$ is a diffeomorphism and $g$ a homeomorphism, is $g\circ f\circ g^{-1}$ a diffeomorphism?","Let $X\subset\mathbb{R}^n$ open. Let $f,g:X\rightarrow X$ be functions such that $f$ is a diffeomorphism and $g$ is a homeomorphism. Is $g\circ f \circ g^{-1}$ a diffeomorphism? I'm currently working through Lee's Intro to Smooth Manifolds, trying to show that any non-degenerate manifold that admits a smooth structure admits uncountably many distinct smooth structures. If the above question is true, it would enable me to complete my proof. My intuition for the special case in $\mathbb{R}$ is as follows: Let $x\in X\subset \mathbb{R}$, and $(x_n)$ a sequence converging to $x$. Choose $(y_n)$ so that $y_n = g(x_n)$ for each $n$, and let $y = \lim_{n\rightarrow\infty} y_n$. Then 
\begin{align}
&\lim_{n\rightarrow\infty}\frac{(g\circ f\circ g^{-1})(x_n) - (g\circ f\circ g^{-1})(x)}{x_n - x}\\
= &\lim_{n\rightarrow\infty}\frac{g(f(g^{-1}(x_n))) - g(f(g^{-1}(x)))}{g(g^{-1}(x_n)) - g(g^{-1}(x))}\\
= &\lim_{n\rightarrow\infty}\frac{g(f(y_n)) - g(f(y))}{g(y_n) - g(y)}
\end{align}
But here I reach a problem because I don't see how I can unwrap this any further. Moreover, there is no guarantee that $y$ exists. ($y$ may not be in X.)","['derivatives', 'real-analysis', 'differential-topology', 'calculus', 'differential-geometry']"
2528941,On dispersion of random variables,"Let $X$ and $Y$ be two i.i.d. random variables.
I am interested in the quantity
$$m(X):=\mathbb{E}[X|X>Y] - \mathbb{E}[X|X < Y].$$
It is not hard to see that $m(X) \geq 0$ and that $m(X) = 0$ if and only if $X$ is a constant, almost surely. So, one may think about $m(X)$ as measuring the dispersion of $X$ in a sense. My question is there a relation between $m(X)$ to other known measures of dispersion such as variance of mean absolute deviation?","['statistics', 'conditional-expectation', 'probability', 'expectation']"
2528959,Characterization of Almost-Everywhere convergence,"Given a $\sigma$-finite measure $\mu$ on a set $X$ is it possible to formulate a topology on the space of functions $f:X \rightarrow \mathbb{R}$ that gives convergence $\mu$-almost everywhere? I can't seem to find any way to write this and am suspecting that no such topology exists! Is this true?  If so, is there some generalisation of a topological space where one can make sense of convergence without having open sets? Any comments, references or tips would be greatly appreciated.","['general-topology', 'measure-theory', 'analysis']"
2528974,Sum of odd terms of a binomial expansion: $\sum\limits_{k \text{ odd}} {n\choose k} a^k b^{n-k}$,"Is it possible to find a closed form expression for the sum
$$\sum_{k \text{ odd}} {n\choose k} a^k b^{n-k}$$
in terms of $a$ and $b$ ?","['algebra-precalculus', 'combinatorics', 'binomial-theorem', 'calculus']"
2528991,"Does ""limit not equal X"" mean that the limit exist?","If we say that $\lim_{n\to \infty}a_n\not=X$, does it implicitly mean that the limit exists? In other words, does this negation means: ""it's not true that 'X is the limit' "" (and maybe there is not a limit at all) or "" the limit doesn't equal X"" (and therefore, there is a limit, because otherwise, we couldn't speak about it)","['terminology', 'limits']"
2529058,Why does a fiber bundle connection satisfy Leibniz rule?,"Let $M$ be a smooth manifold and $\pi:TM\rightarrow M$ be the projection map. Define $V=ker(d\pi)$ and define $H$ to satisfy $TTM=V\oplus H$ and suppose $H$ is invariant under the tangent map induced by the multiplication maps $m_\lambda:TM\rightarrow TM:x\rightarrow \lambda x$. Let $\rho_V:TTM\rightarrow ker(d\pi)$ be the vertical projection map. Define $\bigtriangledown_X Y:=\rho_V\circ dY \circ X$ for smooth vector fields $X,Y:M\rightarrow TM$. I was trying to prove that it satisfies that $\bigtriangledown_X (fY)= X(f) Y + f\bigtriangledown_X Y$ where $f$ is a smooth function, but I don't know how to prove this. How do I prove this?","['connections', 'smooth-manifolds', 'differential-geometry']"
2529068,Resnick - Probability Path - Exercise 6.16 (c),"I'm trying to solve the following exercise from Resnick's books: For any sequence of random variables {$X_n$} set $S_n = \sum_{i=1}^{n}X_i$ (c) Show $X_n \xrightarrow{P} 0$ does NOT imply $S_n/n \xrightarrow{P} 0$. Hint: Try $X_n = 2^n$ with probability $n^{-1}$ and $=0$ with probability  $1-n^{-1}.$ I could show the first part, that $X_n \xrightarrow{P} 0$, by taking: $\lim_{n\to\infty}P(X_n = 0) = \lim_{n\to\infty}(1-\frac{1}{n}) = 1$ $\Rightarrow \lim_{n\to\infty}P(|X_n - 0|\leq \epsilon) =1 $ $\Rightarrow \lim_{n\to\infty}P(|X_n - 0|> \epsilon) =0 $, by taking complements. How can I show the second part, that $S_n/n \xrightarrow{P} 0$ does not apply?","['probability', 'convergence-divergence', 'random-variables']"
2529141,Cauchy sequence in mean,"Suppose $\{f_n\}$ is a sequence of functions in $L_1$ . Show that $\{f_n\}$ is a Cauchy sequence in mean if and only if $\int_E f_n \, d\mu=x_n$ is a Cauchy sequence for real numbers for every measurable set $E,$ and $\{f_n\}$ is a Cauchy sequence in measure. I am able to prove the first part: ( $\Rightarrow$ ) Since Cauchy sequence in $p$ th mean implies Cauchy sequence in measure, we only need to prove that the sequence $\{x_n\}$ is a Cauchy sequence (We can assume that $E=\Omega$ is the entire space), but that is precisely the definition of being a Cauchy sequence in $L_1$ , so we are done I'm having trouble to proving the second part, since I cannot estimate $\int_E |f_n-f_m| \, d\mu$ using $|x_n-x_m|=|\int_E( f_n-f_m ) \, d\mu|$ . I was tying to use one Theorem, too: Theorem: Suppose $\{f_n\}$ is a sequence of functions is $L_p$ and let $\nu_n(E)$ = $\int_E |f_n|^p \, d\mu$ . Then $\{f_n\}$ is a Cauchy sequence in pth mean if an only if $\{f_n\}$ is a Cauchy sequence in measure and the family $\{\nu_n\}$ is equicontinuous at $\varnothing$ . Since $\nu_n(\Omega)<\infty$ , then we only need to show that the family is absolutely continuous with respect to $\mu$ , and is precisely here where I'm stuck.",['measure-theory']
2529144,What is the difference between p ↔ q and p ≡ q?,Im currently new to understanding the propositional logic and I am really confused on the difference between p ↔ q and p ≡ q.,"['notation', 'logic', 'discrete-mathematics']"
2529147,Find the real roots for $\displaystyle \sqrt[4]{386-x}+\sqrt[4]{x}=6.$,"Find the real roots for $\displaystyle \sqrt[4]{386-x}+\sqrt[4]{x}=6.$ Question from a Math Olympiad (ES, 2005). Answer: $(3\pm \sqrt{2})^4$ . My attempt: I will make my attempt below, but I think the approach might be too complicated... are there simpler approaches? I started making the substitution $$u=\sqrt[4]{386-x},~~v=\sqrt[4]{x}$$ so that $u^4+v^4=386$ . Therefore we can define a system of 2 equations, in order to hopefully find $u$ and $v$ : $$\left\{ 
\begin{array}{l}
u+v=6 \\ 
u^4+v^4=386\\ 
\end{array}
\right.
$$ If we use $p_k=u^k+v^k$ , $e_1=(u+v)$ and $e_2=uv$ , by Newton-Girard identities we get $$
\left\{ 
\begin{array}{l}
p_4=e_1p_3-e_2p_2\\
p_3=e_1p_2-e_2p_1\\
p_2=e_1p_1-e_2\times 2\\
\end{array}
\right.
$$ But as we know that $p_1=e_1=u+v=6$ , we can make the substitutions backwards, 
so that $$
\left\{ 
\begin{array}{l}
p_2=36-2 uv\\
p_3=6(36-2uv)-6uv=216-18uv\\
p_4=6(216-18uv)-uv(36-2 uv)\\
\end{array}
\right.
$$ But, as we know that $p^4=u^4+v^4=386$ , so making the substitution $z=uv$ and some simplifications the last equation is equivalent to $$-z^2+72z-455=0.$$ Solving this last equation we find the roots $7$ and $65$ , therefore $uv$ will potentially have these two values. Now as we know that $u+v=6$ we can set a system to solve for $u$ and $v$ : $$
\left\{ 
\begin{array}{l}
u+v=6\\
uv=7~~\text{or}~~65\\
\end{array}
\right.
$$ That can be solved noticing the $u$ and $v$ are the roots of $P(z)=z^2-(u+v)z+uv$ . When $uv=65$ , $P(z)$ has only complex roots. When $uv=7$ , $P(z)$ has 2 real roots. 
When using $uv=7$ the roots $(u,v)$ for $P(z)$ are, by the symmetry, either $$(3-\sqrt{2},3+\sqrt{2})~~\text{or}~~(3+\sqrt{2},3-\sqrt{2}).$$ But as $x=v^4$ the final solution would be $$x=(3\pm \sqrt{2})^4=193\pm 132\sqrt{2}.$$ Both solutions comply with the initial restritions for the argument of the radicals: $0\le x\le 386$ . Questions: (a) is the development correct? at least the final answer matches Wolphram Alpha; (b) are there other approaches? other solutions are welcomed. Sorry if this is a duplicate.","['radicals', 'polynomials', 'systems-of-equations', 'algebra-precalculus', 'contest-math']"
2529149,"closed form evaluation of $\int_{-1}^{1} (x;x)_{\infty}\,dx$","This was mostly provoked by the graph on http://reference.wolfram.com/language/ref/QPochhammer.html which suggests
that the integral $$\int_{-1}^1 (x;x)_\infty\,dx\approx 1.28830088867\ldots$$
is convergent. Is it convergent to something tidy? I've used $x$ instead of $q$ here because $q$ is usually associated with being in the unit disc on the complex plane. Seeing the fractional part start with $0.288$ is somewhat suggestive, given that $(1/2,1/2)_{\infty}=0.2887880950866024\ldots$ (entry 25 on http://mathworld.wolfram.com/TreeSearching.html )","['integration', 'definite-integrals']"
2529155,Isomorphism via birational equivalence,"I'm looking at Exercise 1.13 in Rational Points of Elliptic Curves by Silverman and Tate. I know, from Part (a) of the question, that if $u$ and $v$ satisfy the relation $u^3 + v^3 = \alpha$ , then the quantities $$x = \frac{12\alpha}{u+v} \;\;\;\; \text{and} \;\;\;\; y=36\alpha\frac{u-v}{u+v}$$ satisfy the relation $y^2 = x^3 - 432\alpha^2$ , giving a birational transformation from the curve $u^3 + v^3 = \alpha$ to the curve $y^2 = x^3 - 432\alpha^2$ . How would we prove that the birational transformation described in Part (a) is an isomorphism of groups? I'm wondering how to approach this. Do I need to set up the map, say $\phi$ , between the two curves and then show that $\phi(P+Q) = \phi(P) + \phi(Q)$ ? Any guidance would be much appreciated!","['abstract-algebra', 'group-isomorphism', 'group-theory', 'algebraic-geometry']"
2529173,"If two set systems' elements all have even intersection, then the product of their cardinality is $\le 2^n$","I'm trying to solve the following problem: Let $\mathcal{A}, \mathcal{B} \subset \mathcal{P}(n)$ be two set systems such that $|A \cap B|$ is even for each $A \in \mathcal{A}, B \in \mathcal{B}$. Prove that $|\mathcal{A}||\mathcal{B}| \le 2^n$. My idea for solving it is to construct an injection $f: \mathcal{A} \times \mathcal{B} \to \mathcal{P}(n)$, and I think the function given by $(A, B) \mapsto A \cup B$ should work. However, I am unsure how to prove this. I've been looking at the symmetric difference of two sets in $\mathcal{A}$ to try and help with this (because I was given a hint to use this), but I can't make this work. Does anybody have any clues?","['combinatorics', 'extremal-combinatorics']"
2529177,Question about distributive law in definition of a ring,"In the definition of a ring $R$, one has $a(b+c) = ab + ac$ and $(a+b)c = ac + bc$ for all $a,b,c\in R$ My question is (just out of curiosity) if one really needs both of these. I can't think of an example of something that is not a ring that only satisfies one of the sides of the distributive law. So can one prove that if $a(b+c) = ab + ac$ for all $a,b,c$, then $(a+b)c = ac + bc$ for all $a,b,c$. Edit: I maybe should add that all rings in my definition have a unity $1$.","['abstract-algebra', 'ring-theory', 'definition']"
2529189,A surprising result about the product of Blaschke matrices,"I have verified analytically the conjecture described bellow up to $n=4$, but have had no success trying to prove it. Any help would be much appreciated. Setup Let $\{\lambda_i\}_{i=1}^n$ be real numbers and $g_i:\mathbb R \to \mathbb R$ for all $i\in \mathbb N $. Consider the following recurrence $$ g_{i+1}(x)=\frac{1+g_{i}(\lambda_{i})g_{i}(x)}{g_{i}(\lambda_{i})-g_{i}(x)}\frac{x-\lambda_{i}}{1-\lambda_{i}x}. $$ with $$ g_1(x) = a+bx .$$ Next, let $$ \mathbf M_i(x) = \begin{bmatrix} 1 & 0 \\ 0 & \frac{x-\lambda_i}{1-\lambda_i x}\end{bmatrix}\begin{bmatrix} g_i(\lambda_i) & -1 \\ 1 & g_i(\lambda_i)\end{bmatrix}, $$ and $$ S(x) = \begin{bmatrix} 0 & 1 \end{bmatrix}\begin{bmatrix} g_{n+1}(0) & -1 \\ 1 & g_{n+1}(0)\end{bmatrix}\left[\mathbf M_n(x)\mathbf M_{n-1}(x)\dots\mathbf M_{2}(x)\mathbf M_1(x)\right]\begin{bmatrix} 0 \\ 1\end{bmatrix}.$$ Conjecture For all $n\ge 2$ and for all $i,j\in\{1,\dots,n\}$, $$\frac{S(\lambda_i)}{S(\lambda_j)} = \frac{\lambda_j\left(1+g_1(\lambda_j)g_1(1/\lambda_j)\right)}{\lambda_i\left(1+g_1(\lambda_i)g_1(1/\lambda_i)\right)}.$$ The reason I consider this surprising is that the analytical formulas for $S(x)$ become extremely complicated very quickly as $n$ increases, yet these ratios continue to satisfy this simple equation. Background The motivation for this question comes from the following. Suppose $\mathbf A (x)$ is a $2\times 2$ matrix such that $\det (\mathbf A (x))$ has $n+1$ roots (inside the unit circle): $\{\lambda_i\}_{i=1}^n$ and $0$. To solve some forecasting problems (in which $\mathbf A (x)$ represents the signal structure) it is useful to find a matrix $\mathbf B (x)$ such that $\det(\mathbf A (x)\mathbf B (x))$ has no roots (inside the unit circle). A procedure to obtain the matrix $\mathbf B (x)$ is as follows. To remove the first root, $\lambda_1$, you multiply $\mathbf A (x)$ by an unitary matrix times the Blaschke matrix for this root, i.e. $$ \mathbf A^* (x) = \mathbf A (x) \mathbf W_1 \begin{bmatrix} 1 & 0 \\ 0 & \frac{x-\lambda_1}{1-\lambda_1 x}\end{bmatrix}$$ where $\mathbf W_1$ is a matrix whose columns are the left singular vectors of the of $\mathbf A(\lambda_1)$. To remove the second root, $\lambda_2$, you repeat this step using $\mathbf A^*(x)$ instead of $\mathbf A(x)$. Then, one can simply repeat this step for each of the other roots. The matrices $\mathbf M_i(x)$ above are (up to a constant), the transpose of the matrices that are multiplied to $\mathbf A^*(x)$ in each step described above. One simplifying aspect of the problem I am working with is that the second row of $\mathbf A(x)$ is equal to $0$ when evaluated at any of the roots.","['matrices', 'complex-analysis', 'recurrence-relations', 'linear-algebra']"
2529197,Zolotarev's Lemma and Quadratic Reciprocity,"The law of quadratic reciprocity is unquestionably one of the most famous results of mathematics. Carl Gauss, often called the ""Prince of Mathematicians"", referred to it as ""The Golden Theorem"". He published six proofs of it in his lifetime. To date over 200 proofs of this result have been found. The single most frustrating thing about this theorem is that there are no easy proofs of it, at least when measured relative to the simplicity of the statement and the mathematics it involves. For someone like myself, who prides themselves on being able to find very slick proofs, it can drive you insane. As an undergraduate, when confronted with the lattice-point proof in an introductory number theory class, I refused to learn it. I thought to myself there's no way I need to go through all that just to prove something so simple. There must be an easier way. Zolotarev's Proof of the law has been to date the simplest, and quite frankly most elegant, proof that I can find. The crucial step involves equating the value of the Legendre symbol with the signature of the permutation on $\mathbb{Z}_q$ induced by left multiplication. It can take a little bit longer going through it the first time, but winds up being one of those proofs you can just remember without needing to re-reference it. I had a difficult time finding the result from a single source, at least in a satisfactory form, and had to compile different results from different sources. I thought others might similarly struggle, and so I've typed it below as a resource for them.","['number-theory', 'quadratic-reciprocity']"
2529219,A variation of the square peg problem,"This question spawned from this recent thread . The notorious square peg problem states that any continuous, simple and closed curve $\gamma$ in the plane contains the vertices of some square. It has been proved under many different regularity assumptions, but it still remains an open problem in its original formulation. I realized that the solution I proposed in the linked thread contains the main ideas of a solution of the $C^2$ case of the square peg problem: for any $B\in\gamma$ , by considering a sequence of circles centered at $\gamma$ with slowly increasing radii we get a solution of $A,C\in\gamma$ , $AB=BC$ and $AB\perp BC$ . Given an isosceles and right triangle $ABC$ , we may define $B'$ as the symmetric of $B$ with respect to the midpoint of $AC$ . If for some $B\in\gamma$ we have $B'\in\gamma$ the square peg problem is solved, and solving the square peg problem with suitable smoothness assumptions boils down to showing that two curves intersect (like in Gauss' original proof of the fundamental Theorem of Algebra). $\hspace{1in}$ The green curve and the ellipse intersect, hence the square peg problem can be solved in an ellipse. Context's over. Now I propose the following variation: let $\gamma$ be a (simple,) closed curve of class $C^2$ without any inflection point. Given some $P\in\gamma$ , we define $P'$ as the symmetric of $P$ with respect to the center of curvature of $P$ . We denote as $\gamma'$ the set $\{P':P\in\gamma\}$ . Q1 . Is it true that $\gamma$ and $\gamma'$ always have an intersection? Q2 . Has this problem already been studied in the literature?","['fixed-point-theorems', 'plane-curves', 'curvature', 'geometry']"
2529245,Tangent spaces and local approximations of manifolds,"I'm approaching differential geometry from a physicist's perspective in the hope of understanding GR more thoroughly. I've been told that, intuitively, the tangent space $T_{p}M$ to a point $p$ on a manifold $M$ is "" the best linear approximation to the manifold $M$ at that point "". What is meant by this? Is it meant in the sense that the tangent vectors at that point provide the best linear approximation of functions on the manifold at that point? Does this extend for a sufficiently small neighbourhood around a given point? In the context of GR, is this a mathematical implementation of the equivalence principle, in the sense that the $T_{p}M$ is flat and so the laws of physics are those of special relativity (SR) on $T_{p}M$. The laws of physics on $M$ are therefore SR for a sufficiently small neighbourhood of $M$ around a given point?","['riemannian-geometry', 'differential-geometry']"
2529262,Value of $e$ given that $\frac{a+b+c+d+e}{5} = 2 = \sqrt{\frac{a^2 + b^2 + c^2 + d^2 + e^2}{5}}$,"I have five real numbers $a,b,c,d,e$ and their arithmetic mean is $2$. I also know that the arithmetic mean of $a^2, b^2,c^2,d^2$, and $e^2$ is $4$. Is there a way by which I can prove that the range of $e$ (or any ONE of the numbers) is $[0,16/5]$. I ran across this problem in a book and  am stuck on it. Any help would be appreciated.","['cauchy-schwarz-inequality', 'polynomials', 'sequences-and-series', 'symmetric-polynomials']"
2529313,Convergence of $a_0 \cdot a_1 \cdot ... \cdot a_n$,"Let $(a_n)_{n \geq 0}$ be a sequence with $a_0,a_1 \in (0,1)$ and $$a_{n+1}=a_n^2a_{n-1}-a_na_{n-1}+1$$
  Find $$\lim_{n \to \infty} (a_0 \cdot a_1 \cdot ... \cdot a_n)$$ I managed to prove that $a_n \in (0,1)$ and that the sequence is increasing and convergent to $1$. Then I denoted $b_n=a_0 \cdot a_1 \cdot ... \cdot a_n$ and since it is strictly decreasing and bounded, it must be convergent to a number $l$. Using the relation given I obtained $$b_{n+1}=\frac{b_n^3}{b_{n-1}b_{n-2}}-\frac{b_n^2}{b_{n-2}}+b_n \iff b_{n+1}-b_n=\frac{b_n^2}{b_{n-1}b_{n-2}}(b_n-b_{n-1})$$ and thus got $$b_{n+1}-b_n=\frac{b_n^2 \cdot b_{n-1}}{b_1\cdot b_0}(b_2-b_1)$$
Applying limits yields $0=\frac{l^3(b_2-b_1)}{b_1b_0}$, hence $l=0$. My question is whether this problem could have been done differently, maybe without the notation of $b_n$. At first I tried to write $$a_0\cdot a_1 \cdot ... \cdot a_n <a_n^{n+1}$$ and then $\lim_{n \to \infty}a_n^{n+1}=e^{\lim_{n \to \infty} (a_n-1)(n+1)}$ and tried to prove that $$\lim_{n \to \infty}n(a_n-1)=-\infty$$ using Cesaro-Stolz, but didn't get anything...","['real-analysis', 'limits', 'sequences-and-series', 'calculus', 'convergence-divergence']"
2529343,"Find the tangent plane to the surface with parametric equations $x=u^2, y=v^2, z=u+2v$ at the point $s(1, 1, 3)$.","My prof in the class got $x=1+2s$, $y=1+2t$ and $z=3+s+2t$ but in the book says 
$x+2y-2z+3=0$. Both of them were first solving for $r(u)$ and $r(v)$ and I get that part but in the book they were using the cross product then and my prof wasn't, he was substituting $s$ and $t$. Can someone tell me what the correct answer is or if they are both correct?","['multivariable-calculus', 'tangent-line', 'parametrization', 'vectors']"
2529347,Inequality of means for functions,"According to Wikipedia one can define the arithmetic mean of an integrable function $f: M \to \mathbb{R}$ over a relatively compact domain $M$ as: $$A(f) = \frac{1}{\mu(M)}\int_M f$$ where $\mu(M)$ is the measure of $M$. Moreover, if $f > 0$ one can also define its geometric mean as: $$G(f) = \exp\left(\frac{1}{\mu(M)}\int_M \log f\right)$$ The inequality of the arithmetic and geometric means is a well-known result in the discrete case. ¿Is it also true in this context? That is, is it true that : $$A(f) \geq G(f)$$ for every positive integrable function $f$?","['means', 'lebesgue-integral', 'measure-theory']"
2529366,Trigonometric equation application,"Find all values of $x$ between 0 and 4 for which $\sin(2x+1) = \cos (2x+1) $ I made the equation to become - $$\tan (2x+1) = 1$$ Which I believe is correct. Now the problem is , the range . If the range is $0$ to $2\pi$ I would've known how to do but what about the $0$ to $4$ ?",['trigonometry']
2529375,Epsilon neighbourhood of an injective path,"By an $\epsilon$-neighbourhood of $A\subseteq\mathbb{R}^n$ I understand the set $$A_\epsilon=\big\{v\in\mathbb{R}^n\ \big|\ \lVert v-w\rVert<\epsilon\mbox{ for some }w\in A\big\}$$ I'm looking for a proof or counterexample for the following statement: Let $\lambda:[0,1]\to\mathbb{R}^n$ be an injective continuous path. Then for any $\delta>0$ there exists $\epsilon >0$ such that $\epsilon<\delta$ and $\overline{\mbox{im}(\lambda)_\epsilon}$ is homeomorphic to a closed ball.","['general-topology', 'real-analysis']"
2529378,If roots of $x^6=p(x)$ are given then choose the correct option,"If $x_1,x_2,x_3,x_4,x_5,x_6$ are real positive roots of the equation $x^6=p(x)$ where $P(x)$ is a 5 degree polynomial where $\frac{x_1}{2}+\frac{x_2}{3}+\frac{x_3}{4}+\frac{x_4}{9}+\frac{x_5}{8}+\frac{x_6}{27}=1$ and $p(0)=-1$, then choose the correct option(s): $(A)$ $x_5-x_1=x_3x_4$ $(B)$ Product of roots of $P(x)=0$ is $\frac{6}{53}$ $(C)$ $x_2,x_4,x_6$ are in Geometric Progeression $(D)$ $x_1,x_2,x_3$ are in Arithmetic Progression Now
$x_1,x_2,x_3,x_4,x_5,x_6$ are real positive roots of the equation $x^6=p(x)$, so I wrote it as $x^6-p(x)=(x-x_1)(x-x_2)(x-x_3)(x-x_4)(x-x_5)(x-x_6)$ but I am not getting how to use the condition $\frac{x_1}{2}+\frac{x_2}{3}+\frac{x_3}{4}+\frac{x_4}{9}+\frac{x_5}{8}+\frac{x_6}{27}=1$ to get the answer. Could someone please help me with this?","['roots', 'polynomials', 'sequences-and-series']"
2529418,Tensor product of two distributions $u \in \mathcal{D}(X)$ and $v \in \mathcal{D}(Y)$,"I'm following the proof of the theorem 4.3.3 p. 47 of ""Introduction to the distributions theory"" by Friedlander and Joshi. We have the following identity \begin{align*}
\textbf{(1)}  \displaystyle \langle u \otimes v, \varphi \rangle = \langle v(y), \langle u(x), \varphi(x,y) \rangle \rangle = \langle u(x), \langle v(y), \varphi(x,y) \rangle \rangle
\end{align*} where $\varphi \in \mathcal{D}(X\times Y)$. Ok I understand that this identity is worth. However, the same theorem, there is the next point $\textbf{(4)}$ The tensor product is a separately continuous bilinear form on $\mathcal{D}'(X) \times \mathcal{D}'(Y)$ and in the proof it says that $(4)$ is immediate consequence of $(1)$. Sincerely, I did not understand what it means ""separately"" and how $(1)$ implies $(4)$. Thank you for each reply.","['functional-analysis', 'distribution-theory', 'locally-convex-spaces']"
2529434,$X_k$ uniformly integrable iff $\lim_{n \to \infty}\limsup_{k\to\infty}\mathbb E(\vert X_k \vert \mathbb 1_{\vert X_k \vert \ge n})=0$,"A sequence $X_k$ of real random variables on $(\Omega,F,\mathbb P)$ is uniformly integrable iff all $X_k$ are integrable and $$\lim_{n \to \infty}\limsup_{k\to\infty}\mathbb E(\vert X_k \vert \mathbb 1_{\vert X_k \vert \ge n})=0$$ $\textbf{Definition}:$
A familiy of real valued random variables $X_t, t\in I$ ($I$ arbitrary index set) is called uniformly integrable, if $$\lim_{n \to \infty}\sup_{t \in I}\int_{\{\vert X_t \vert \ge n\}}\vert X_t \vert \, d\mathbb P=0$$ Therefore '$\Leftarrow$' is clear. Since $\limsup_{k \to \infty}$ can be 'substitued' by  $\sup_{t \in I}$. The problem is the other direction. Help is much appreciated!","['uniform-integrability', 'probability-theory', 'random-variables']"
2529464,Pseudo-inverse with minimal number of non-zero entries,"I'm looking for a way to compute the pseudo-inverse of a matrix (not the Moore-Penrose, but any other) with the minimal number of non-zero entries (maximum number of zero entries). In MATLAB, the the mldivide() operator does that, but I cannot find any documentation on how this operator evaluates my matrix so that it inds this particular pseudo-inverse. Is there a way to do this ""by hand""? Note: I know, Moore-Penrose minimize the Euclidean norm, but I explicitely do not want to minimize that norm, but to maximize the zero-entries in my pseudo-inverse.","['matrices', 'matlab', 'inverse', 'pseudoinverse', 'linear-algebra']"
2529479,Levi-Civita connections from metrics on the orthogonal frame bundle,"Following Kobayashi and Nomizu, a connection on a manifold is given by a establishing a notion of horizontal vector in the tangent space of a frame bundle.   (Alternative approaches make covariant differentiation foundational.) An important step in developing Riemannian geometry consists of isolating the Levi-Civita connection as that connection with zero torsion that preserves the metric. Could an alternative approach to defining the Levi-Civita connection go like this:  Given a manifold $M$ with Riemannian metric, construct some natural (family of?) Riemannian metrics on the orthogonal frame bundle of $M$.  Then simply define ""horizontal"" to mean orthogonal (in the sense of the constructed metric) to vertical? Pedagogically, this might offer a bypass around defining and studying torsion. About my ""family of"" hedge.  There may be no canonical way to compare the scale of vertical vectors, essentially elements of the Lie algebra of the orthogonal group, with more general vectors. If this is worked out anywhere, I'd appreciate a reference.  If there's some obstruction to this approach, I'd appreciate an explanation.","['connections', 'differential-geometry']"
2529488,"Conditional expectation, martingale convergence theorem, and inequalities","I have the following question. We have a random variable $X:\Omega \to\mathbf{X} \subseteq \mathbb{R}^d$ and a continuous function $f:\mathbf{X}\to\mathbb{R}$ such that $f(X)$ and $X$ are both integrable. Introduce n-component partition of $\mathbf{X} \subseteq \mathbb{R}^d$ by $\mathbf{X}^{(n)} = \{\mathbf{X}^{(n)}(k): k =1,\dots,n\}$ and assume the
generated sigma-algebra $\sigma^{(n)} = \sigma(\{X\in\mathbf{X}^{(n)}(k)\}:k=1,\dots,n)$ satisfies $\sigma(X) = \sigma(\cup_{n\in\mathbf{N}} \sigma^{(n)})$. Then from the martingale convergence theorem and the continuous mapping theorem, we know that both random variables $E[f(X):\sigma^{(n)}]$ and $f(E[X:\sigma^{(n)}])$ converge to $f(X)$ almost surely. My question is whether there exists a sequence of random variables $(Y^{(n)})_{n\in\mathbb{N}}$ where $\sup_{n\in\mathbb{N}} Y^{(n)}$ is integrable and $\lim_{n\to\infty}Y^{(n)} = 0$ almost surely such that
$$
|f(X)| + Y^{(n)} \geq |f(E[X:\sigma^{(n)}])| 
$$
and
$$
|E[f(X):\sigma^{(n)}]| + Y^{(n)} \geq |f(E[X:\sigma^{(n)}])| 
$$
holds??? Thank you.","['probability-theory', 'conditional-expectation', 'convergence-divergence']"
2529501,Transforming a function $f: \mathbb N^2 \to A$ to a function $f:\mathbb N\to A$ in a zig-zag manner,"We know that the rationals are countable since we can list them via zig-zagging: $$\begin{matrix}
\frac{1}{1}   && \frac{1}{2} &\color{red}\to &\frac{1}{3} &&\frac{1}{4} & \color{red}\to & \frac{1}{5}\\
\color{red}\downarrow   & \color{red}\nearrow && \color{red}\swarrow && \color{red}\nearrow &&\color{red}\swarrow\\
\frac{2}{1}   && \frac{2}{2} && \frac{2}{3} \\
& \color{red}\swarrow && \color{red}\nearrow\\
\frac{3}{1}&&\frac{3}{2}&&& \ddots\\
\color{red}\downarrow&\color{red}\nearrow\\
\frac{4}{1}
\end{matrix}$$ Now suppose we have a function $f: \mathbb N^2 \to A$ for some set $A$, where we list the outputs of the function in the same way:
$$\begin{matrix}
f(1,1) && f(1,2) && f(1,3)  && f(1,4) && f(1,5)\\
~\\
f(2,1) && f(2,2) && f(2,3) \\
~\\
f(3,1) && f(3,2) &&& \ddots\\
~\\
f(4,1)
\end{matrix}$$ Is there a way to define a function $f' : \mathbb N \to A$ such the resulting sequence of outputs travels along this grid in the same way? $$\begin{matrix}
f(1,1)   && f(1,2) &\color{red}\to &f(1,3) &&f(1,4)& \color{red}\to & f(1,5)\\
\color{red}\downarrow   & \color{red}\nearrow && \color{red}\swarrow && \color{red}\nearrow &&\color{red}\swarrow\\
f(2,1)   && f(2,2) && f(2,3) \\
& \color{red}\swarrow && \color{red}\nearrow\\
f(3,1) && f(3,2) &&& \ddots\\
\color{red}\downarrow&\color{red}\nearrow\\
f(4,1)
\end{matrix}$$ That is, defining $f'$ in terms of $f$ such that $$ f'(1) = f(1,1)\\
f'(2) = f(2,1) \\
f'(3) = f(1,2) \\
f'(4) = f(1,3) \\
f'(5) = f(2,2) \\
f'(6) = f(3,1) \\
 \vdots$$ Given $n\in\mathbb N$ I need to find a way to map to the corresponding pair in $\mathbb N^2$, expressing this only in terms of $n$, but I'm not sure how to do this. I appreciate any assistance.","['analysis', 'functions']"
2529527,Continuous Bijective map,"Which of the following statements are false ?? a) There exists a continuous bijection $f:[0,1]\to[0,1]\times[0,1]$ b) There exists a continuous map $f:S^1\to \mathbb{R}$ which is injective, where $S^1$ stands for the unit circle in the plane c) There exists a continuous map $f:[0,1]\to SL_2(\mathbb{R})$ which is surjective. I think a) is false as it is similar to construct a continuous bijective map from side to square (Space-filling curve which is continuous but not bijective) not possible, but how do I prove that? For b ) i have no any idea For c) as we know any continuous image of a compact space is compact and here $[0,1]$ is compact where $SL_2(\mathbb{R})$ is not compact as it is not bounded",['general-topology']
2529531,Multiple soft step function,"I want to create a function that have multiple (infinite) steps like this one: $x+\sin x$ But i want to have control of two things: how quickly it increases and when the (soft) steps occur. For example: for the function $\frac{1}{(1+e^{-x})}$ which has the following graph: We can make the step rise faster by multiplying x by a big number. One possible way to write such a function is writing it as a sum of previous function:
$\sum_{i=0}^4 \frac{1}{1+e^{-(x-i)*5}}$ Unfortunately I can't use this kind of series to solve my problem.
How can I obtain such function?",['functions']
2529610,"If $d\mid2n^2$, could $n^2+d$ be a square of a natural number? Here $d, n \in \Bbb N$.","If $d\mid2n^2$, could $n^2+d$ be a square of a natural number? Here $d, n \in \Bbb N$. What I tried so far: $2n^2 = kd, k \in N$ $$d = 2\frac{n^2}{k}$$
$$n^2 + d = $$
$$n^2 +2\frac{n^2}{k}= n^2\left(\frac{k+2}{k}\right) = \frac{kd}{2}\left(\frac{k+2}{k}\right) = \frac{d(k+2)}{2}$$ I don't see where this could lead me.",['number-theory']
2529614,"How to evaluate $\int_{0}^{\infty} \frac{x^{-\mathfrak{i}a}}{x^2+bx+1} \,\mathrm{d}x$ using complex analysis?","We were told today by our teacher (I suppose to scare us) that in certain schools for physics in Soviet Russia there was as an entry examination the following integral given $$\int\limits_{0}^{\infty} \frac{x^{-\mathfrak{i}a}}{x^2+bx+1} \,\mathrm{d}x\,,$$ where $a \in \mathbb{R}$, $b \in [0,2)$, and $\mathfrak{i}$ is the imaginary unit. And since we are doing complex analysis at the moment, it can, according to my teacher, be calculated using complex methods. I was wondering how this could work? It seems hard to me to find a good curve to apply the residue theorem for this object, I suppose. Is there a trick to compute this integral?","['complex-analysis', 'real-analysis', 'integration', 'analysis']"
2529793,Splitting field of $f(X)=X^4-X^3-5X+5$ and its degree.,"Let $K$ be a splitting field in $\mathbb C$ of the polynomial $f(X)=X^4-X^3-5X+5$ over $\mathbb Q$. Construct the splitting field $K$ and find the degree of the extension $K:\mathbb Q$. $f(X)=(X-1)(X^3-5)$, hence we have roots $\sqrt[3]{5},\sqrt[3]{5}\zeta_3$ and $\sqrt[3]{5}\zeta_3^2$, where $\zeta_3$ is the primitive 3rd root of unity. Thus our field extension is $\mathbb{Q}(\sqrt[3]{5},\zeta_3)$. By the Tower Law $[\mathbb{Q}(\sqrt[3]{5},\zeta_3)]=[\mathbb{Q}(\sqrt[3]{5},\zeta_3):\mathbb Q(\sqrt[3]{5})]\cdot[\mathbb{Q}(\sqrt[3]{5}):\mathbb Q]=6$. Have I missed anything important out? Find the order and structure of $Gal(K:\mathbb Q)$. The order of $Gal(K:\mathbb Q)$ is also 6 because the extension is normal and separable. I believe the six automorphisms are: $id: \sqrt[3]{5} \mapsto \sqrt[3]{5} $
, $\zeta_3 \mapsto \zeta_3$ $\alpha: \sqrt[3]{5} \mapsto \zeta_3\sqrt[3]{5} $
, $\zeta_3 \mapsto \zeta_3$ $\alpha: \sqrt[3]{5} \mapsto \zeta_3^2\sqrt[3]{5} $
, $\zeta_3 \mapsto \zeta_3$ $\beta: \sqrt[3]{5} \mapsto \sqrt[3]{5} $
, $\zeta_3 \mapsto \zeta_3^2$ $\beta: \sqrt[3]{5} \mapsto \zeta_3^2\sqrt[3]{5} $
, $\zeta_3 \mapsto \zeta_3^2$ which is isomorphic to the symmetric group $S_3$? Find all subfields of $K$ via Galois correspondence. I'm trying to get my head around fixed fields and Galois correspondence, could anyone show me clearly how this part is done? Find all constructible numbers in $K$. I assume this leads on from the previous part, I know constructible numbers must be of a degree which is a power of 2? So would it be all the elements of $K$ with such an order? Hope my attempts weren't too hard to follow, any help would be great!","['irreducible-polynomials', 'galois-theory', 'polynomials', 'group-theory']"
2529824,"Region of attraction of : $x'=-y-x^3,y'=x-y^3$ via Lyapunov Function","PROBLEM : $1)$ Show that the stationary point $O(0,0)$ is asymptotic stable $2)$ Find a region of attraction for the system : $$x'=-y-x^3$$ $$y'=x-y^3$$ given the Lyapunov's function: $$V=x^2+y^2$$ First, I differentiate my Lyapunov's function and i take: $$ \dot{V}=-2 \cdot (x^4+y^4)<0 $$ So, the stationary point $$O(0,0)$$ is asymptotic stable as $$\dot{V}<0$$ everywhere outside the origin. For the second question of my problem, I believe that the region of attraction is the circle : $$x^2+y^2=c $$ Is this right?How  can I find $c$ and the boundary  of the estimation of the region of attraction? If i am right with the circle can anyone help me to write it in a good mathematical way? Last days I am trying to understand how regions of attraction work so , I would really appreciate a thorough solution and explanation about how to find this region of attraction. Thanks in advance!","['basins-of-attraction', 'dynamical-systems', 'stability-theory', 'stability-in-odes', 'ordinary-differential-equations']"
2529862,Proof that a function is bijective if and only if it is both surjective and injective,"I am given the usual definitions for surjectivity and injectivity, but I am introduced with an alternative formulation of bijectivity: Suppose $X$ and $Y$ are sets and $f:X\rightarrow Y$ a mapping. This mapping is said to be bijective if $\exists g:Y\rightarrow X$ such that $\forall x\in X,\ y\in Y$, $f(g(y))=y$ and $g(f(x))=x$. I have to proof that in this sense, bijectivity is equivalent to simultaneous injectivity and surjectivity. Now from bijectivity, I found it quite easy to prove the other two conditions. The other way around, however, poses some difficulty. My proof: Suppose $f:X\rightarrow Y$ is surjective and injective. I define the function $g:Y\rightarrow X$ by $g(y)=x\Longleftrightarrow f(x)=y$. Surjectivity of $f$ implies $\forall y\in Y\ \exists x\in X$ such that $f(x)=y$, thus $f(g(y))=y$. Now suppose $x,z\in X$ and $f(x)=f(z)$. Along with the definition of $g$ this implies $g(f(z))=x$. From injectivity follows $x=z$, thus $g(f(x))=x$. First of all, initially, I have not shown that $g$ is well-defined. I am not sure how to actually prove this, or if it is even necessary here. However, using surjectivity of $f$ is it easy to see that $g$ maps every value of $Y$. Can I conclude from this that the function is well-defined, or is there more to say on the matter? I suppose I would also have to show that $g$ cannot take on two different values of $x$ for the same $y$. Also, suppose I have proven that $g$ is indeed well-defined, is my proof as presented above correct? Thank you for your help! EDIT: From a discussion in the comments of an answer, I have come to realise that perhaps I have misused the term ""well-defined"". Since I don't really understand the formal definition, I will re-state a part of my question as follows: can I directly use $g$ as defined above, or do I have to prove that it is ""okay"" to use it? I'm really not sure how to say this anymore... intuitively, I would say that ""okay"" means that the definition itself does not produce any inconsistencies. If it is necessary to prove something about it prior to using it, what is it?","['elementary-set-theory', 'functions', 'proof-verification']"
2529891,Can there be a power series that converges for all reals but not for the complex numbers?,"Let $z = x + i y$, and consider the following function: $$
f(z) = e^{\frac{1}{1 + z^2}} \qquad z \in \mathbb{C}
$$ Note that at $z = \pm i$ the function does not converge.
We see that on $\mathbb{R}$ when $y = 0$, this becomes: $$
f(x) = e^{\frac{1}{1 + x^2}} \qquad x \in \mathbb{R},
$$ which converges nicely for all values $x \in \mathbb{R}$.
Furthermore, this function is analytic on $\mathbb{R}$, and so there exists some power series of form: $$
f(x) = \sum_{n = 0}^{\infty} a_n (x - x_0)^n \qquad a_n, x, x_0 \in \mathbb{R}
$$ that converges for all $x \in \mathbb{R}$.
Does this power series naturally extend to the complex plane if we have $x \mapsto z$, i.e.: $$
f(z) = \sum_{n = 0}^{\infty} b_n (z - z_0)^n \qquad z \in \mathbb{C}, \quad z_0 = x_0 + i0, \quad b_n = a_n + i 0
$$ In this case, would the function shown above thus demonstrate that a complex-valued power series that converges on all of $\mathbb{R}$ not necessarily converge in $\mathbb{C}$?","['complex-analysis', 'power-series']"
2529905,Dividing a square into two regions with minimal interface,"We want to divide a unit square into a black region of given area $A\in[0,1]$ and a white region of area $1-A$, while minimizing the interface perimeter between the two regions. Let's say both regions must be contiguous. Note that without loss of generality, we may consider the restricted range $A\in\left[0,\frac{1}{2}\right]$, because we can always swap the black and white regions. A simple way of dividing the square into two regions is to draw a straight line parallel to two of the sides (right figure). This works for any area, and has an interface perimeter of $P=1$, regardless of the area. Another solution* is to draw a quadrant, centered at one of the square's corners (left figure). Clearly, we can enclose more than half of the area with this method, so it can be used for any given $A$. Using simple geometry, we can show that to enclose an area $A<\frac{1}{2}$, the interface perimeter will be $P = \sqrt{\pi} \sqrt{A}$. This means that the quadrant is a better solution than the line when $\sqrt{\pi}\sqrt{A}<1$, or $A<\frac{1}{\pi}$, and a candidate optimal (?) solution would be: My question is: Is there a solution better than the one above? If not, how can we
prove that this solution is optimal? Bonus question: Does the optimal solution change if the regions are not required to be contiguous? *Other solutions such as a full circle, a semi-circle using the border as a diameter, a square using the borders as two of the sides, and a diagonal line at 45 degrees all do worse than the quadrant for any $A$, and therefore cannot feature in an optimal solution.","['nonlinear-optimization', 'optimization', 'calculus', 'geometry', 'area']"
2529918,Galois covering induces an isomorphism on the level of (co)homology,"The setting is the following : we have a smooth Galois cover of manifolds $p : Y \to X$, with (Galois) automorphism group $G$. Denote by $\Omega^*(X)$ and $\Omega^*(Y)$ the spaces of differential forms on $X$ and $Y$ respectively. Somehow we want to express the cohomology of $X$ and $Y$ in terms of each other (motivating example : expressing the cohomology of projective space in terms of that of the sphere). This question is strongly related, but here we are concerned with the technical details of the claim. For now we are only interested in homology. Firstly we want a nice action of $G$ on $\Omega^*(Y)$. I think we can define one as follows : if $g$ is a covering space automorphism of $Y$, we can associate an automorphism $g^* : \Omega^*(Y) \to \Omega^*(Y)$. The action induced by $G$ is an action by such automorphisms. Now denote by $\Omega^*(Y)^G$ the subalgebra of differential forms on $Y$ that are fixed by all automorphisms $g^*$. Claim : The induced map $p^*:\Omega^*(X) \to \Omega^*(Y)^G$ is an isomorphism. This is where I encounter some problems. Firstly I can show that $p^*(\Omega^*(X)) \subset \Omega^*(Y)^G$ (the covering map $p$ and some automorphism $g$ of covering spaces form a commutative diagram. This gives us a ""reverse"" commutative diagram in homology with $p^*$ and $g^*$ from which we can deduce the inclusion). However the reverse inclusion seems difficult. We may have to use at some point the fact that $p$ is Galois ($Y=X/G$) but it gets more confusing as I go forward. Moreover we should be able to find an explicit inverse for $p^*$ but I am unable to do so. Is my group action even reasonable ? Any help in either direction would be greatly appreciated.","['homology-cohomology', 'differential-topology', 'algebraic-topology', 'general-topology', 'differential-geometry']"
2529921,How to integrate $|x| \cdot x$,"How to integrate this manually?
$$
\int |x|\cdot x ~dx 
$$ My tries so far: $$
\int |x|\cdot x ~dx = (x^2/2)\cdot|x| - \int (x²/2)\cdot \mathop{\mathrm{sign}}(x) ~dx 
$$
Trying it again, but using sign(x) as first parameter, because sign(x) is not derivable further.
$$
 \int \mathop{\mathrm{sign}}(x)\cdot(x²/2) ~dx =|x|\cdot (x^2/2) - \int |x|\cdot x ~dx 
$$
Great, as nothing would have been done. Next try, using the signum function
$$
|x|\cdot x = \mathop{\mathrm{sign}}(x)\cdot x^2
$$ $$
 \int \mathop{\mathrm{sign}}(x)\cdot |x| ~dx = x^2-\int|x|\cdot x^2~dx
$$ $$
 \int |x|\cdot x^2 ~dx =x²\cdot \mathop{\mathrm{sign}}(x)\cdot x^2-\int x^2\cdot \mathop{\mathrm{sign}}(x)\cdot 2x ~dx
$$ which seems to be a never ending chain again.
Any ideas?",['integration']
2529924,Proof of Schwarz's theorem on partial derivatives for vector-valued functions,"I really appreciate your help in checking the below proof (especially the last section). Let $(E_{i})_{i=1 \dots n}$ and $F$ be normed vector spaces and $f: \prod{E_{i}}\rightarrow F$ a twice-differentiable function (no hypothesis is made on the continuity of partial derivatives). We want to prove that $\forall a,h,k \in \prod{E_{i}}, f''(a)(h)(k)=f''(a)(k)(h)$. We write $\Delta(a,h,k)=f(a+h+k)-f(a+h)-f(a+k)+f(a)$. Then: $||\Delta(a,h,k) - f''(a)(k)(h)|| \leq$
$||\Delta(a,h,k) - f'(a+k)(h)+f'(a)(h)||+||f'(a+k)(h)-f'(a)(h)-f''(a)(k)(h)||$ Let $(1)$ denote the first summand and $(2)$ the second. I can show that $(2)=o(||h||.||k||)$ using the definition of $f''$ therefore I will now try to do the same with $(1)$ (from there we can conclude by applying the same reasoning to $||\Delta(a,h,k) - f''(a)(h)(k)||$ and using the linearity of the derivatives: I am not interested in that part of the proof). $(1) = ||\Delta(a,h,k) - f'(a+k)(h)+f'(a)(h)||=$ $||\delta(a,h,k)-\delta(a,h,0)|| \leq $ $||k||.Sup_{x \in B(0,||k||)}{||f'(a+h+x)-f'(a+x)-f''(a+x)(h)||}$ where $\delta(a,h,k)= f(a+h+k)-f(a+k)-f'(a+k)(h)$, $B(0,||k||)$ the closed ball with center $0$ and radius $||k||$, and using the Mean Value theorem.
Since $f'$ is differentiable it is continuous and the $Sup$ is reached at some point say $x0$. $(1) \leq ||k||.||f'(a+h+x0)-f'(a+x0)-f''(a+x0)(h)|| \leq $ $||k||.||f''(a+x0)(h)|| \leq$ $||k||.||h||.||f''(a+x0)|| = o(||h||.||k||)$ P.S.: Is there any useful, simple way to weaken the hypotheses?","['derivatives', 'normed-spaces', 'proof-verification']"
2529932,What is an example of function f:Z→N that is a bijection?,"I tried to look at the cases and find a function, but I could not find a bijective function. I know that we should check the cases when x is a positive number and when x is negative. Can you help me to find one?",['discrete-mathematics']
2529950,"Find all $C^{1}$ functions $f: (0,+\infty) \to (0, +\infty)$ such that $f(x)^{f'(x)}=x$, $f(1)=1$.","As the question title says, I'm trying to find all $C^1$ functions $f:(0, +\infty) \to (0, +\infty)$ which satisfy $f(x)^{f'(x)} = x$, and $f(1)=1$. I know that $f(x)=x$ is one solution. When I put everything into the exponent, I get $f'(x) \ln{f(x)} = \ln{x}$, which gives me the implicit solution $f(x)(\ln{f(x)}-1) = x(\ln{x}-1)+C$, $C \in \mathbb{R}$. By inserting $(1,1)$ into the implicit solution, I get that the solution must satisfy $f(x)(\ln{f(x)}-1) = x(\ln{x}-1)$. The problem here is that I can't use Picard's theorem and claim uniqueness, because the expression $f'(x) = \frac{\ln{x}}{\ln{f(x)}}$ isn't defined for $(x, f(x))=(1,1)$. Is there a different way to prove uniqueness, or is there another solution to this equation?",['ordinary-differential-equations']
2529956,Prove the maximum of convex functions is also convex using this def. [duplicate],"This question already has answers here : Proving that the maximum of two convex functions is also convex (2 answers) Closed 6 years ago . Could someone assist me in proving that the maximum of a convex function is also convex using the below definition? I cannot figure out how this proof would work and would appreciate some help! $$
f(\lambda x +(1-\lambda)y) \le \lambda f(x)+(1-\lambda)f(y)
$$ $$
\lambda \in[1,0]
$$ Thank you!","['matrices', 'proof-writing', 'functions']"
2529978,Is any regular algebraic curve in the plane union of closed curves?,"This is related to the attempt I am doing trying to prove that any polynomial lemniscate is the union of regular closed curves. Recall by the way that a polynomial lemniscate is a curve of the form $\{z\in\mathbb C: |p(z)|=K\}$, where p is a polynomial of one complex variable with complex coefficients and $K>0$. I proved that any polynomial lemniscate is the union of regular curves defined on open intervals. However, I have no idea of how to prove that these regular curves are indeed closed. The following picture represents a lemniscate of 8 foci (the polynomial is of degree 8). Although there is a self-intersecting component, it can be parametrized regularly. I conjecture that something like the following could be true: If a bounded algebraic curve on the plane is the union of regular curves, these curves are indeed closed. Thank you so much in advance.","['algebraic-curves', 'differential-geometry', 'algebraic-geometry']"
2529988,Removing points from the discrete spectrum of a self-adjoint operator,"I'm currently following the chapter 6 of Gerald Teschl's book ""Mathematical Methods in Quantum Mechanics"" and got stuck in something that seems irrelevant but is killing me anyway. As a example of how inestable is the discrete spectrum, it is stated in the book that: Given an self-adjoint operator $A$ and $\lambda_0\in \sigma_{dis}(A)$, we can easily remove this eigenvalue with a finite rank perturbation of arbitrarily small norm. In fact, consider 
$$A+\varepsilon P_{\{\lambda_0\}}(A).$$ While it is easy to see that such perturbation is of finite rank and its norm is arbitrarily small, understanding why $\lambda_0$ can not be an eigenvalue anymore is not clear at all. I'd appreciate any help with this.","['functional-analysis', 'mathematical-physics', 'operator-theory', 'spectral-theory']"
2529995,Uniform convergence of Fourier series in terms of modulus of continuity,"In the section Uniform Convergence in https://en.wikipedia.org/wiki/Convergence_of_Fourier_series it is given the result of D. Jackson which states that if $f \in C^p$ is $2\pi$-periodic and $f^{(p)}$ has modulus of continuity $\omega$, then 
$$
|f(x) - (S_N f)(x)| \leq K \dfrac{\log N}{N^p}\omega(2\pi/N)
$$
for $K > 0$ a constant independent of $N,f,p$. Here $S_N f$ denotes the $N$-th partial sum of the Fourier series expansion of $f$. I have been unable to find the proof of this and was wondering if anyone would be kind enough to show it? Thanks!","['fourier-series', 'fourier-analysis', 'uniform-convergence', 'calculus', 'analysis']"
2530006,"Equation via matrix, having no solution, one solution and infinite solutions.","$$ \left[
\begin{array}{cc|c}
  1&-2&4\\
  a&4&5
\end{array}
\right] $$ I came across this question on one of my course slides, and I am having trouble understanding the whole concept of an equation having no solution, one solution or infinitely many solutions. $$ \left[
\begin{array}{cc|c}
  1&-2&4\\
  0&4+2a&5-4a
\end{array}
\right] $$ this was the resulting matrix. What I don't understand is how you get $(4+2a)$ and $(5-4a)$? And what needs to be done so that the same equation has no solution, one unique solution, and infinitely many solutions. Can anybody please help?","['matrices', 'linear-algebra']"
2530015,Eigenvalues and eigenvectors of the second derivative,"Consider the $1$-dimensional eigenvalue problem $$\begin{cases}\frac{d^2u}{dt^2}=\lambda u \\ u(0)=u(1)=0\end{cases}$$ I want to compute the eigenvalues and eigenfunctions. I found something related in wikipedia. Here is the link: https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors_of_the_second_derivative#The_discrete_case In wiki, there are two cases. In the continuous case, the $j$th eigenvalue and eigenvector are $$\begin{align} &\lambda_j=-j^2 \pi^2 ~~(\text{In my problem $L=1$}) \\ &v_j=\sqrt{2}\sin(j\pi t)\end{align}$$ In the discrete case, the $j$th eigenvalue and eigenvector are 
$$\begin{align} &\lambda_j=-\frac{4}{h^2}\sin^2\left(\frac{\pi j}{2(n+1)}\right) \\ &v_{i,j}=\sqrt{\frac{2}{n+2}}\sin\left(\frac{ij \pi}{n+1}\right)\end{align} $$(the index i represents the $i$th component of the eigenvector). I think $n$ should be number of grid points and $h$ should be the length of a subinterval in the partition. What's the relation of between the two $\lambda_j~'s$?  Why do the eigenvalues in continuous and discrete cases look very different?","['eigenvalues-eigenvectors', 'partial-differential-equations', 'functional-analysis', 'eigenfunctions', 'ordinary-differential-equations']"
2530070,"For a finite support distribution $p$, $p^k \rightarrow U(X),\quad \vert X \vert={{k}\choose{p_1k,\ldots,p_nk}}$","I'm trying to show that for a distribution with a finite support with probabilities $p_1,\ldots,p_n$, we get that with $k\rightarrow\infty$ the probability $p^k$  becomes uniform over ${k}\choose{p_1k,\ldots,p_nk}$ size support. My final goal is to define another distribution over this support, and to show that the $L_1$ norm distance between them over the $p^k$ support approaches zero. I tried to use the law of big numbers, but either I use the almost surely convergence, which implies that the event $P(\lim_{k\rightarrow\infty}\frac{1}{k}\{\text{number of times }p_i\text{ happened}\}=p_i)=1 $ but gives me no direct information of the probabilities of the events which aren't in the ${k}\choose{p_1k,\ldots,p_nk}$ subspace, for which I wish to find a boundary that approaches $0$ as $k\rightarrow\infty$, neither tells me that $\lim_{k\rightarrow\infty}P(\frac{1}{k}\{\text{number of times }p_i\text{ happened}\}=p_i)=1$, or I use the $P$ convergence which tells me $\forall \varepsilon>0 \quad \lim_{k\rightarrow\infty} P(\vert\{\text{number of times }p_i\text{ happened}\}-p_ik\vert > k\varepsilon)=0$ but these constant ""margins"" make it impossible for me to say that as $k\rightarrow\infty$, the sum of probabilities of events outside of the sub-support ${k}\choose{p_1k,\ldots,p_nk}$ vanishes. Actually, I believe it's not quite true, as I tried a different approach: They probability of each event in the ${k}\choose{p_1k,\ldots,p_nk}$ subspace is $\prod_{i=1}^n p_i^{p_ik}$, thus the probability of an event in this subspace is: $${k \choose p_{1}k,p_{2}k,\ldots,p_{s}k}\prod_{i=1}^{s}p_{i}^{p_{i}k}=\frac{k!}{\prod_{i=1}^{s}kp_{i}!}\prod_{i=1}^{s}p_{i}^{p_{i}k}
\\
\frac{k!}{\prod_{i=1}^{s}kp_{i}!}\prod_{i=1}^{s}p_{i}^{p_{i}k}\sim\frac{\sqrt{2\pi k}\left(\frac{k}{e}\right)^{k}}{\prod_{i=1}^{s}\sqrt{2\pi kp_{i}}\left(\frac{kp_{i}}{e}\right)^{kp_{i}}}\prod_{i=1}^{s}p_{i}^{p_{i}k}
\\
=\frac{\sqrt{2\pi k}\left(\frac{k}{e}\right)^{k}}{\prod_{i=1}^{s}\sqrt{2\pi kp_{i}}\left(\frac{k}{e}\right)^{kp_{i}}}
\\
=\frac{\sqrt{2\pi k}\left(\frac{k}{e}\right)^{k}}{\left(\frac{k}{e}\right)^{\underbrace{\left(\sum_{i}p_{i}\right)}_{=1}k}\prod_{i=1}^{s}\sqrt{2\pi kp_{i}}}
\\
=\frac{\sqrt{2\pi k}}{\prod_{i=1}^{s}\sqrt{2\pi kp_{i}}}\rightarrow0,\quad k\rightarrow\infty$$ However, I know that the $p^k$ distribution does approach a uniform distribution over this subspace as $k\rightarrow\infty$. Where have a I gone wrong, and how can I prove that the probabilities outside this sub-support vanish?","['probability-limit-theorems', 'probability-theory']"
2530081,$f$ is a complex constant function,"If $f$ is analytic in a domain $D$ and is there exists $z_0 \in D$ such that $|f(z)|\geq |f(z_0)|>0$ is true for every $z\in D,$ then $f$ is constant. It's kind of strange this result because it's like the Maximun Principle, but the inequality is backwards... Is possible this result? I tried to prove that if $f=u+iv,$ then $u$ is bounded below, but I got $|f(z_0)|\leq |u|.$  Thanks for your help!","['complex-analysis', 'maximum-principle']"
2530091,Surfaces of revolution perpendicular to the plane xOy,"Let $S$ be a surface of revolution parametrized by $(\varphi(v) \cos u, \varphi(v) \sin u, \delta(v))$. Assume $S$ has constant Gaussian curvature equal to 1. Since $K = -\frac{\varphi''}{\varphi}$, $\varphi'' + \varphi = 0$. If we solve this differential equation we obtain that 
\begin{equation}
\varphi(v) = c_1 \cos(v) + c_2 \sin(v).
\end{equation}
My question is the following: if we know that $S$ intersects perpendicularly the plane $xOy$, can we say that $\varphi(v) = c_1 \cos(v)$? Apparently, this is true, but I don't know why. I found the Gaussian map and at points $p$, where $p$ is in the intersection of $S$ and the plane $xOy$, $\varphi'(v) = 0$. I try to play with this but nothing came up. Please if you can help me, I will really appreciate it. Thanks!","['parametrization', 'differential-geometry', 'surfaces']"
2530129,"If $A = \{ 2, \frac{1}{2} , 3, \frac{1}{3} , 4, \frac{1}{4} , 5, \frac{1}{5} , . .\}$. Are $P(A \cap [0,1])$ and $P(A \cap [1,2])$ countable","If $A = \{ 2, \frac{1}{2} , 3, \frac{1}{3} , 4, \frac{1}{4} , 5, \frac{1}{5} , . .\}$. Are the following sets countable (using the definition of countable meaning to not be finite and have the same cardinallity as $\mathbb{N}$) : $P(A \cap [0,1])$ and $P(A \cap [1,2])$ Attempt at solution: For $P(A \cap [0,1])$: $A \cap [0,1]$ is the same as $\{\frac{1}{2} ,\frac{1}{3} ,\frac{1}{4}... \}$ which is countable so the power set of a countable must be uncountable For $P(A \cap [1,2])$: $A \cap [1,2]$ is the same as $\{2\}$ and the power set of $\{2\}$ is $\{\{2\}, \emptyset\}$which is finite so it is uncountable. I am not sure if my solutions are correct","['elementary-set-theory', 'discrete-mathematics']"
2530186,Find the sum of the series $ \ \sum_{n=1}^{\infty} \frac{1}{n^2+l^2} \ $ [duplicate],"This question already has answers here : How to sum $\sum_{n=1}^{\infty} \frac{1}{n^2 + a^2}$? (4 answers) Closed 6 years ago . Find the sum of the series $ \ \sum_{n=1}^{\infty} \frac{1}{n^2+l^2} \ $ , where $ \ l=constant \ $ Answer The given series is convergent clearly . $ \ \sum_{n=1}^{\infty} \frac{1}{n^2+l^2} \\ = \frac{1}{1^2+l^2}+\frac{1}{2^2+l^2}+......+\frac{1}{n^2+l^2}+........ \\ = \frac{1}{l}  [ d \tan^{-1}(\frac{1}{l})+ d \tan^{-1}(\frac{2}{l})+........+ d \tan^{-1}(\frac{n}{l})+.....] \\ = \frac{1}{l} \lim_{n \to \infty}  \sum_{k=1}^{n}[d \tan^{-1}(\frac{1}{l})+ d \tan^{-1}(\frac{2}{l})+........+ d \tan^{-1}(\frac{k}{l})]    \\ = \frac{1}{l} \ d (\lim_{n \to \infty}  \sum_{k=1}^{n}[ \tan^{-1}(\frac{1}{l})+  \tan^{-1}(\frac{2}{l})+........+  \tan^{-1}(\frac{k}{l})]) $ but now i  can't proceed to find the sum of the series. Help me out.",['sequences-and-series']
2530213,when can we interchange integration and differentiation,"Let $f$ be a Riemann Integrable function over $\mathbb{R}^2$. When can we do this? $$\frac{\partial}{\partial\theta}\int_{a}^{b}f(x,\theta)dx=\int_{a}^{b}\frac{\partial}{\partial\theta}f(x,\theta)dx$$ (Here, $a$ and $b$ are not a function of $\theta$.) In the problem, which I am solving recently, are like this: $f_{\theta}(x)$, here $\theta$ is constant and $\theta\in\mathbb{R}$ (usually). For example $f_{\theta}(x)=x^2\theta$. So, I am blindly interchanging integration and differentiation because of continuity over $\theta$. But I want to know little bit more. Also, what happens if $a$ and $b$ are function of $\theta$? Thanks.","['derivatives', 'integration', 'calculus', 'partial-derivative']"
2530221,Continuously varying homeomorphisms in a homogeneous space,"Let $X$ be a topological space, and equip its homeomorphism group $\text{Homeo}(X)$ with the compact-open topology. I probably need to restrict $X$ to be locally compact for it to play off with $\text{Homeo}(X)$ nicely. The usual definition of $X$ being homogeneous is that given $x,y \in X$ there exists $f \in \text{Homeo(X)}$ with $f(x) = y$. My question: When can this choice of $f$ be made continuous? More precisely, is there a natural class of homogeneous spaces for which there is always a continuous map $$\theta: \{(x,y): x \neq y\} \rightarrow \text{Homeo}(X)$$
such that $\theta(x,y)$ carries $x$ to $y$. Is it true for connected manifolds?","['manifolds', 'general-topology']"
2530234,Conjugacy Classes in Finite Nilpotent Group,I have been doing some studying and I was wondering if there was any literature regarding the conjugacy classes size in a finite nilpotent group in determining if a group is a p-group or not.  It seems to be that there is not; I was wondering if anyone knew of any literature (I can not seem to locate anything through online resources and my library). If there is no literature I am curious as to why because it seems that this would be a studied question. EDIT: To be more precise. The question is Let G be finite nilpotent group. Given the largest class size can one determine if G is of prime power order.,['group-theory']
2530282,Hypergeometric function discontinuity,"The hypergeometric function$\ _2F_1(a,b;c;z)$ has a branch point at $z=1$. How do I compute the discontinuity around the point? In particular, how do I compute the following? $$\lim_{\epsilon\rightarrow 0^+} \ _2F_1(1,b;b+1;1+\epsilon)-\ _2F_1(1,b;b+1;1-\epsilon)$$ Mathematica says that it is $-ib\pi$.","['special-functions', 'complex-analysis', 'hypergeometric-function', 'continuity']"
2530301,"Evaluate the double integral for $f(x,y)=ye^x$ given $R = [2,4] \times [1,9]$","So I did first did it by integrating with respect to $y$ first then $x$ and eventually got the answer of $40(e^4-e^2)$, which is correct. But when I attempted to apply Fubini's theorem and switch the order of integration, I should get the same answer but I'm not. Possible made an error somewhere? What I did: \begin{align}\text{Double integral} &= \int_1^9 \int_2^4 ye^x\, dx\, dy \\
&= \int_1^9 \left.ye^x \right\vert_{x=2}^{x=4}\, dy \\
&= \int_1^9 (ye^4 - ye^2) \,dy\\
&= \left[\left[\frac{y^2}2\right]e^4 - \left[\frac{y^2}2\right]e^2 \right]_ {y=1}^{y=9}\\
&= 
\end{align}
and I got similar numbers as to when I integrated with $dx$ then $dy$, but couldn't get $40(e^4-e^2)$. Can someone show their algebra for after integrating?",['multivariable-calculus']
2530310,Surfaces of revolution with curvature 0,"I am trying to find all the surfaces of revolution with Gaussian curvature $K \equiv 0$. This is what I got so far. If we assume the surface of revolution is parametrized by $(\varphi(v) \cos u, \varphi(v) \sin u, \delta(v))$. Then since $\varphi'' + 0\cdot\varphi = 0$, $\varphi(v) = C\cdot v$ which implies that $\delta(v) = \int_0^v \sqrt{1 - C^2}dv = \sqrt{1 - C^2}\cdot v$. I do not know how to continue from this point. Any ideas? Thanks!","['solid-of-revolution', 'curvature', 'differential-geometry', 'surfaces']"
2530338,How to evaluate the integral $\int_0^{\pi}\frac{a^n\sin^2x+b^n\cos^2x}{a^{2n}\sin^2x+b^{2n}\cos^2x}dx$?,"Evaluate the integral $$\int_0^{\pi}\frac{a^n\sin^2x+b^n\cos^2x}{a^{2n}\sin^2x+b^{2n}\cos^2x}dx.$$ I have no idea. $$\int_0^{\pi}\dfrac{a^n\sin^2x+b^n\cos^2x}{a^{2n}\sin^2x+b^{2n}\cos^2x}dx=\int_0^{\pi/2}\dfrac{a^n\sin^2x+b^n\cos^2x}{a^{2n}\sin^2x+b^{2n}\cos^2x}dx=\int_0^{\pi/2}\dfrac{a^n\tan^2x+b^n}{a^{2n}\tan^2x+b^{2n}}dx$$ I try the substitution $\tan{x}=t$, but it doesn’t work.","['integration', 'calculus']"
2530343,$(AB)^p =A^p B^p$?,"Let $A,B$ be two positive bounded linear operators on a Hilbert space. If $A$ commutes with $B$, do we have $(AB)^p =A^p B^p$ for any $p>0$? Or more general, $f(AB) = f(A)f(B)$ for any Borel function $f$.","['functional-calculus', 'hilbert-spaces', 'operator-theory', 'functional-analysis', 'operator-algebras']"
2530441,Why invent the definition of pseudovector?,"According to Wikipedia: a pseudovector (or axial vector) is a quantity that transforms like a vector under a proper rotation, but in three dimensions gains an additional sign flip under an improper rotation such as a reflection. It seems that pseudovectors ""are not real vectors"". But if you think about it, every vector in $\mathbb{R}^3$ can be written as a cross product of two vectors. Let $\vec{v}_1 = (a,b,c)$, then $\vec{v}_2 = \frac{1}{\sqrt{a}}(-c,0,a), \vec{v}_3 = \frac{1}{\sqrt{a}}(-b,a,0)$ satisfy 
$$
\vec{v}_3\times\vec{v}_2 = (a,b,c)
$$
So we get that $\vec{v}_1$ is a pseudovector. But this means every vector is a pseudovector, so this definition seems empty to me in 3D.","['cross-product', 'linear-algebra']"
2530475,Find the set of values for $c\in \mathbb R$ that allows real solution for $\sqrt{x}=\sqrt{\sqrt{x}+c}$,"Given $\{x,c\}\subset \mathbb R$, $\sqrt{x}=\sqrt{\sqrt{x}+c}~~~~ (1)$. Find: set of values for $c$ such that $(1)$ has solution in $\mathbb R$. Question from the Brazilian Math Olympiad 2004. No solution provided. I'm not sure whether I'm setting the right constraints to find the asked set. Hints and full solutions are appreciated. My attempt: as an initial observation, the solution must consider at least 2 constraints: $(c_1)$ $x\ge 0;$ and, $(c_2)$ $\sqrt{x}\ge -c$, to avoid negative arguments in the square root. By assuming ($c_1$) and ($c_2$) hold, and squaring both terms in (1) we get
$$\sqrt{x}=\sqrt{\sqrt{x}+c}\Leftrightarrow x=\sqrt{x}+c\Leftrightarrow x-c=\sqrt{x}.$$ Now considering an addicional constraint $(c_3)$ $x-c\ge 0$, and squaring both terms, we get
$$x-c=\sqrt{x}\Leftrightarrow x^2-2cx+c^2=x \Leftrightarrow x^2-(2c+1)x+c^2=0$$
To solve this last equation, notice that 
$$\triangle=(2c+1)^2-4c^2=4c+1$$
Therefore, another constraint, for real roots, is $\triangle\ge 0$ or $(c_4)$ $c\ge -1/4$. And the tentative solution for $x$, before checking constraints, will be given by
$$x=\frac{2c+1\pm\sqrt{4c+1}}{2}.$$
Now the set of values for $c$ that leads to a real solution in $x$, will be the set resulting from the intersection of 4 conditions:
$$\left\{ 
\begin{array}{l}
(c_1)~~x\ge 0 \Leftrightarrow \frac{2c+1\pm\sqrt{4c+1}}{2}\ge 0\Leftrightarrow 2c+1\ge\pm\sqrt{4c+1}\\ 
(c_2)~~\sqrt{x}\ge -c \Leftrightarrow \sqrt{\frac{2c+1\pm\sqrt{4c+1}}{2}}\ge -c\\
(c_3)~~x\ge c\Leftrightarrow \frac{2c+1\pm\sqrt{4c+1}}{2}\ge c\Leftrightarrow 1 \ge \pm\sqrt{4c+1}\Leftrightarrow 1 \ge \sqrt{4c+1}\Leftrightarrow c \le 0\\ 
(c_4)~~c\ge -1/4\\
\end{array}
\right. 
$$
Question: (a) is this last step the right set of conditions to be intersected to give the asked set for $c$? (b) how to solve for the intersection of the conditions? Hints and full answers are appreciated. Maybe I'm just complicating something that is a lot easier.","['algebra-precalculus', 'contest-math', 'inequality', 'radicals']"
