question_id,title,body,tags
2373097,Condition on sigma algebra,"Say I toss two distinct coins. Let A be event: there are two heads, $\{HH\}$ Let $\sigma$ be a sigma algebra $\big\{\emptyset,\Omega,\{HH,HT\},\{TT,TH\}\big\}$ How does one understand $P(A|\sigma)$, $E(A|\sigma)$? 
How does the meaning differ if sigma algebra is chosen differently, say trivial, or full algebra?","['probability-theory', 'probability', 'measure-theory']"
2373108,How many bit strings can be made with six 0's AND eight 1's?,"This was a question I had on an exam the other night. I was really stumped by this question on the exam, and I'm still having trouble figuring it out. When doing this type of problem, there's usually a set length, but this question asks for any length, and two different elements. I thought this could've been a stars and bars problem C(n+r-1, n-1), but I don't think that works. The question is also kind of confusing. Is it asking for any type of bit string as long as you are choosing from six 0's and eight 1's. Does anyone have any idea on how to do this? Edit: Just wanted to clarify that when I asked the professor about this question during the exam, he elaborated that it can be of any length, including the null string.","['combinatorics', 'discrete-mathematics']"
2373116,"How does proof for ""differentiability imply continuity"" work?","Here's the well known proof: $$\lim_{x \to c} \left[f(x) - f(c) \right] = \lim_{x \to c} (x - c) \frac{f(x) - f(c)}{x - c}$$
$$ = \left[\lim_{x \to c} (x - c)\right]\left[ \lim_{x \to c} \frac{f(x) - f(c)}{x - c}\right]$$
 $$ = 0 \cdot f ^{\prime} (c)$$
$$ = 0$$ a) Therefore, $\lim_{x \to c} f(x) = f(c)$, $f$ is continuous at $c$. The problem I know how the equation works, but I don't get how it proves explicitly that differentiability implies continuity. To me, as long as $\lim_{x \to c} (x - c) = 0$, then why should $f ^{\prime} (x)$ even matter? a) would be remain true anyways, right? I must be looking at it wrong. To me, it just looks like an equation saying that continuity implies continuity. As long as $\lim_{x \to c} (x - c) = 0$, which is the continuity equation, then of course the equation would imply that f(c) is continuous. But how does differentiability explicitly imply continuity? It just looks like modifications were made to the continuity equation to make the differentiability equation appear, then it just went back to the continuity equation. But how exactly does the differentiability equation that popped up imply continuity? I mean, it just looks like the continuity equation proving itself, but I don't see how the differentiability equation is ""proving"" anything. It started from the continuity equation and it went back to the continuity equation? I'm so frustrated with this. I keep attending the same classes and the teachers seem to see something I can't.","['derivatives', 'limits', 'calculus', 'continuity', 'ordinary-differential-equations']"
2373124,"Show that $f(x) = \sqrt{x}$ for $0 \leq x \leq 1$ is absolutely continuous on $[0,1]$","According to the Royden 4th p.119, Definition) A real-valued function $f$ on a closed, bounded interval $[a,b]$ is said to be absolutely continuous on $[a,b]$ provided for each $\epsilon >0$, there is a $\delta>0$ such that for every finite disjoint collection $\{ (a_k,b_k)\}_{k=1}^{n} $ of open intervals in $(a,b)$, if $\sum_{k=1}^{n}[b_k-a_k] < \delta$, then $\sum_{k=1}^n |f(b_k)-f(a_k) | < \epsilon$. I am trying to prove that $f(x) = \sqrt{x}$ for $0 \leq x \leq 1$ is absolutely continuous on $[0,1]$. I want to check that my approach is correct. Proof) Claim 1 For given $0<\epsilon<1$, $f(x) = \sqrt{x}$ is absolutely continuous on $[\epsilon,1]$. Let $0 < \epsilon <1$ and consider finite disjoint collection $\{ (a_k,b_k)\}_{k=1}^{n} $ of open intervals in$(\epsilon,1)$. Note that $\sum_{k=1}^{n} |f(b_k)-f(a_k)| $ =  $\sum_{k=1}^{n}|\sqrt{b_k}-\sqrt{a_k}|$= $\sum_{k=1}^{n} \frac{b_k-a_k}{\sqrt{b_k}+\sqrt{a_k}}$ < $\frac{1}{2\sqrt{\epsilon}}$ $\sum_{k=1}^{n}[b_k-a_k]$. Let $\delta = 2\epsilon \sqrt{\epsilon} = 2\epsilon^{\frac{3}{2}}$. Now, if $\sum_{k=1}^{n}[b_k - a_k] < \delta = 2\epsilon^{\frac{3}{2}}$, then $\sum_{k=1}^{n} |f(b_k)-f(a_k)| $ = $\sum_{k=1}^{n} \frac{b_k-a_k}{\sqrt{b_k}+\sqrt{a_k}}$ < $\frac{1}{2\sqrt{\epsilon}} (2\epsilon^{\frac{3}{2}})$ = $\epsilon$. Thus, $f(x) = \sqrt{x}$ is absolutely continuous on $[\epsilon,1]$. # Claim 2 $f(x) = \sqrt{x}$ is absolutely continuous on $[0,1]$. Let $\epsilon>0$ be given and choose $\epsilon'>0$ such that $0<\sqrt{\epsilon'}<\frac{\epsilon}{2}$. By Claim 1 , we can find $\delta>0$ as the response to the $\epsilon'$ challenge regarding the criterion for the absolute continuity of $f(x)=\sqrt{x}$ on $[\epsilon',1]$. Consider $\{ (a_k,b_k) \}_{k=1}^{n}$ such that $\sum[b_k-a_k]< \delta$. Divide a disjoint collection $\{ (a_k,b_k) \}_{k=1}^n$ in $(0,1)$ into two parts: $\{ (a_{k1},b_{k1}) \}_{k1=1}^{n1}$ in $(0,\epsilon')$ and $\{ (a_{k2},b_{k2}) \}_{k2=1}^{n2}$ in $(\epsilon',1)$, where $n1+n2=n$. (If some $(a_k,b_k)$ contains $\epsilon'$, then divide it into two parts: $(a_k,\epsilon')$ and $(\epsilon',b_k$).) Now, $\sum_{k2=1}^{n2}[b_{k2}-a_{k2}] < \sum_{k=1}^{n}[b_{k}-a_{k}] < \delta$. Since $\delta>0$ responds to the $\epsilon'>0$ challenge on $[\epsilon',1]$, $\sum_{k2=1}^{n2} |\sqrt{b_{k2}} - \sqrt{a_{k2}}| < \epsilon' < \frac{\epsilon}{2}$. Also, $\sum_{k1=1}^{n1} |\sqrt{b_{k1}} - \sqrt{a_{k1}}| < \sqrt{\epsilon'}-0 < \frac{\epsilon}{2}$. Thus, $\sum_{k=1}^{n} |\sqrt{b_{k}} - \sqrt{a_{k}}| \leq$ $\sum_{k1=1}^{n1} |\sqrt{b_{k1}} - \sqrt{a_{k1}}|$ + $\sum_{k2=1}^{n2} |\sqrt{b_{k2}} - \sqrt{a_{k2}}|$ < $\frac{\epsilon}{2}$ + $\frac{\epsilon}{2}$ < $\epsilon$. Hence, $f(x) = \sqrt{x}$ is absolutely continuous on $[0,1]$. #","['real-analysis', 'analysis']"
2373140,Can we use chi-square distribution and central limit theorem to find the approximate normal distribution?,"If $X_1,\ldots,X_i,\ldots,X_n$ are same normal distribution, $X_i \sim \operatorname{Normal}(0,σ^2)$ ,
and they are independent. $$
Z = \frac{\sum_{i=1}^n X_i^2} n.
$$ What is the distribution of the square of the normal distribution?like $X_i^2$ ,and,what is it mean and variance? I am trying to turn this Z into a normal distribution can we use chi-square distribution and central limit theorem to find the approximate normal distribution ? How to do it？ I do not quite understand the chi-square distribution and central limit theorem,
could you answer this question in detail? Any help would be much appreciated! re-edit： I do this works: $$
Z = \frac{\sum_{i=1}^n X_i^2} n= σ^2\sum_{i=1}^n \left(\frac{X_i}{σ}\right)^2.
$$ this is a chi-square distribution,and mean $= nσ^2$ , var ${}=2nσ^2$ . is this right? and how to use CLT to find the approximate normal distribution?","['statistics', 'central-limit-theorem', 'chi-squared', 'normal-distribution']"
2373154,Large sample confidence interval,Can someone please be so kind to check if my answer is correct,"['statistics', 'confidence-interval', 'normal-distribution']"
2373158,Paving a rectangular area where the area of the perimeter stones is equal to the area of the interior.,"Given a rectangular area that needs to be completely paved using 1 metre square paving stones (for simplicity's sake), the number of stones that are used around the perimeter of the rectangle must equal the number used for the interior of the rectangle. In other words, the area of the perimeter stones must equal the area of the interior. What are the dimensions of such a rectangular area? What I know: - There are two possible answers. They are 12-by-5 and 8-by-6. - Because you are using 1 metre square paving stones, the dimensions of the rectangular area therefore must be a whole number. I managed to work out the first dimension as I was given that the perimeter was equal to 30, and solving for simultaneous equations resulted in 12-by-5. The 8-by-6 was given as another alternative answer and claimed to be the only other such possible dimension. So my question is, is there a way to find these two possible answers algebraically without going into something convoluted like going through every possible whole number or something? ~Sorry if this is in the wrong area or if this question has already been asked before. Cheers~ ^_^","['algebra-precalculus', 'geometry']"
2373225,Integrating factor for a differential equation,"Find the integrating factor for the equation: $(3x+\frac{6}{y})\mathrm{d}x+(\frac{x^2}{y}+\frac{3y}{x})\mathrm{d}y=0$. Write $P_1(x,y)=2x+\frac{6}{y}$, $Q_1(x,y)=\frac{x^2}{y}$, $P_2(x,y)=x$, and $Q_2(x,y)=\frac{3y}{x}$, then $P_1(x,y)\mathrm{d}x+Q_1(x,y)\mathrm{d}y+P_2(x,y)\mathrm{d}x+Q_2(x,y)\mathrm{d}y=0$. For $P_1(x,y)\mathrm{d}x+Q_1(x,y)\mathrm{d}y$, notice that $\frac{1}{P_1(x,y)}(\frac{\partial Q_1(x,y)}{\partial x}-\frac{\partial P_1(x,y)}{\partial y})=\frac{1}{y}$, and the integrating factor for this part is $\mu_1(y)=\mathrm{e}^{\int\frac{1}{y}\mathrm{d}y}=y$. Hence, by computing $\int_{x_0}^x\mu_1(y)P_1(x,y)\mathrm{d}x+\int_{y_0}^y\mu_1(y)Q_1(x_0,y)\mathrm{d}y$, one obtains $\Phi_1(x,y)=x^2y+6x$. Similary, for $P_2(x,y)\mathrm{d}x+Q_2(x,y)\mathrm{d}y$, notice that $\frac{1}{Q_2(x,y)}(\frac{\partial P_2(x,y)}{\partial y}-\frac{\partial Q_2(x,y)}{\partial x})=\frac{1}{x}$, and the integrating factor for this part is $\mu_2(x)=\mathrm{e}^{\int\frac{1}{x}\mathrm{d}x}=x$. Hence, by computing $\int_{x_0}^x\mu_2(y)P_2(x,y)\mathrm{d}x+\int_{y_0}^y\mu_2(y)Q_2(x_0,y)\mathrm{d}y$, one obtains $\Phi_2(x,y)=\frac{x^3}{3}+\frac{3y^2}{2}$. In order to find the integrating factor for the whole equation, one needs to find two smooth functions $g_1(t)$ and $g_2(t)$ such that $\mu_1g_1(\Phi_1(x,y))=\mu_2g_2(\Phi_2(x,y))$, i.e., $yg_1(x^2y+6x)=xg_2(\frac{x^3}{3}+\frac{3y^2}{2})$. However, it seems impossible, does not it?","['integrating-factor', 'ordinary-differential-equations']"
2373282,Ignoring the constant of integration $C$ in the integrating factor method for solving Linear ODE,"$$\dfrac{dy}{dx} + p(x)y = f(x)$$
Solving the linear dif. equation, we can use integrating factor method. We know integrating factor: $exp(\int p(x) dt) = exp(P(x) + C)$. But we ignore the constant of integration $C$. How can we explain why the constant was ignored?",['ordinary-differential-equations']
2373293,"Showing that the ""left-hand limit function"" is left continuous","Let $f: [0, \infty) \rightarrow \mathbb{R}$. Define the value of the left-hand limit of $f$ at $t>0$ to be $f(t^-) = \lim_{x \rightarrow t^-} f(x)$. Define the ""left-hand limit function"" of $f$ as $f^-: (0, \infty) \rightarrow \mathbb{R}, f^-(t) = f(t^-) = \lim_{x \rightarrow t^-} f(x)$. Prove that $f^-$ is left continuous at each $t>0$. Let $t>0$. To prove $f^-$ is left continuous at $t$, I need to show that $\lim_{t \rightarrow t^-}f^-(t) = f^-(t) = f(t^-)$. My question is, how can I use the sequential definition (not the $\epsilon-\delta$ definition) of left continuity to prove this question? In other words, let $(s_n)$ be a sequence contained in $(0, \infty)$ such that $s_n<t$ for all $n$ and $s_n \rightarrow t$. I need to show that $\lim_{n \rightarrow \infty} f^-(s_n) = f^-(t) = f(t^-)$. I have also been given a hint (but I have no idea where to incorporate it): If $f(t^-)$ exists and is finite, then it is equivalent to:
$$f(t^-) = \sup \inf_{s<t} \{f(v): s \le v < t\} = \inf \sup_{s <t} \{f(v) : s \le v < t\} $$ Any help would be greatly appreciated!","['real-analysis', 'proof-writing', 'analysis', 'limits']"
2373322,$\sin (n^2)$ diverges,"One can prove that $\sin n$ diverges, using the fact that the natural numbers modulo $2\pi$ is dense. However, the case for $\sin (n^2)$ looks much more delicate since this is a subsequence of the former one. I strongly believe that this sequence is divergent, but cannot prove it. In general, can one prove that $\sin (n^a)$ diverges for $a>0$?","['equidistribution', 'trigonometry', 'convergence-divergence', 'limits']"
2373357,What is an intuitive approach to solving $\lim_{n\rightarrow\infty}\biggl(\frac{1}{n^2} + \frac{2}{n^2} + \frac{3}{n^2}+\dots+\frac{n}{n^2}\biggr)$?,"$$\lim_{n\rightarrow\infty}\biggl(\frac{1}{n^2} + \frac{2}{n^2} + \frac{3}{n^2}+\dots+\frac{n}{n^2}\biggr)$$ I managed to get the answer as $1$ by standard methods of solving, learned from teachers, but my intuition says that the denominator of every term grows much faster than the numerator so limit must equal to zero. Where is my mistake? Please explain very intuitively.","['algebra-precalculus', 'limits-without-lhopital', 'calculus', 'limits']"
2373378,Computing the Moore-Penrose pseudoinverse of a $2 \times 2$ matrix,"I am facing difficulties in calculating the Moore-Pensore pseudoinverse of a positive semidefinite matrix $A$, where $A$ is self-adjoint and $\langle A u, u \rangle \geq 0$ for all $u \in \mathcal{H}$, where $\mathcal{H}$ is a complex Hilbert space. For example, $$A = \begin{bmatrix} 1&-1\\ -1&1\end{bmatrix}$$ is a positive semidefinite matrix. How to calculate the Moore-Penrose pseudoinverse of $A$?","['matrices', 'pseudoinverse', 'linear-algebra']"
2373421,A Combinatorial Game,"The following is a combinatorial game that I read about in a mathematical olympiad preparation book some time ago. I have forgotten the exact rules of this game, but the general idea of the problem goes something like this: Two players A and B take turns placing counters on a 5 by 5 grid. Player A always goes first. Each turn, the player has the choice of placing either 1, 2 or 3 counters on the grid. The winner of the game is the person who has an even* number of tokens after all the squares on the grid have been used up. *Due to the fact that I first read this problem several years ago, I am unable to remember whether the winner of the game was the one with an even or an odd number of tokens at the end of the game. However, I am fairly certain that there was a winning strategy for player B. I have been unable to prove why that should be so for either case or find the original statement of the game. Has anyone seen this problem before? Does anyone have any idea of how to find a winning strategy for either case? Any help would be greatly appreciated.","['game-theory', 'combinatorics', 'contest-math', 'combinatorial-game-theory']"
2373457,Confusion in understanding why empty set is a subset of every set [duplicate],"This question already has answers here : Is ""The empty set is a subset of any set"" a convention? (7 answers) Closed 6 years ago . statement 1 : If $x \in \emptyset$ then $x \in A$. statement 2 : If $x \in \emptyset$ then $x \notin A$. I know that both statements are true since the hypothesis is false. But first statement says that $\emptyset \subset A$ while second statement says that $\emptyset$ is not a subset of $A$. My question is why we prefer $\emptyset \subset A$ over the other implication that $\emptyset$ is not a subset of $A$? Thanks.","['proof-writing', 'logic', 'elementary-set-theory', 'proof-explanation']"
2373459,"What is common math notation for ""fanout"" combination of functions?",Let's say we have $f_1 : A \to X$ and $f_2 : A \to Y$. What is the most canonical way to denote $f : A \to X \times Y$ that combines $f_1$ and $f_2$ by outputting pair of their respective values for same argument? Sort of like this haskell operation . For sure mathematicians must have some shortcut for such common operation.,['functions']
2373491,Is w*-sequential closure idempotent?,Suppose that $X$ is a Banach space and $F$ is a closed subspace of $X^{**}$. Consider $K(F)$ to be linear subspace of $X^{**}$ consisting of weak*-limits of w*-convergent sequences from $F$. Is it true that $$K(K(F)) = K(F)? $$ I couldn't find any immediate counterexamples to this claim.,"['functional-analysis', 'banach-spaces', 'weak-convergence']"
2373509,"Why is $(-\infty,1)$ the domain of the solution to $y'=y^2$, $y(0)=1$ instead of $(-\infty,1)\cup (1,\infty)$","Consider the ODE $$y'=y^2$$ with initial condition $$y(0)=1$$. A solution is $$y(t)=\frac{-1}{t-1}$$ Which the text I'm reading mentions is only valid on $(-\infty,1)$, which includes the left branch of the graph of this hyperbola. They justify this by saying that the initial condition is included in the left-hand side of the graph of this function, so that we cannot extend the solution to the entire real line. My question is, what is wrong with saying that the solution is $y(t)$ on $(-\infty,1)\cup (1,\infty)$? Why can we not include the rest of the domain where the solution $y(t)$ is defined? Thanks EDIT: They mention that this is the ""maximum interval on which the solution curve is defined"", perhaps by calling it a solution ""curve"" they are implicitly trying to say we want the greatest domain on which the solution is continuous?","['ordinary-differential-equations', 'calculus']"
2373511,How that $\int_0^x\int_0^{\infty}\exp\left(-\frac{t^b+s^a}{2}\right) dtds \leq \int_0^x\int_{0}^{+\infty}\exp\left(-\frac{t^a+s^b}{2}\right)dtds$,"How to show the following inequality
\begin{align}
\int_{0}^{x}\int_{0}^{+\infty}\exp\left(-\frac{t^b+s^a}{2}\right)\,dt\,ds \leq \int_{0}^{x}\int_{0}^{+\infty}\exp\left(-\frac{t^a+s^b}{2}\right)\,dt\,ds
\end{align} where $0<a<b$ and all $x>0$. I learned This is the inequality from this question . However, I was not able to follow the proof.","['multivariable-calculus', 'real-analysis', 'calculus']"
2373533,"coins on chessboard, who has the winning strategy","The game begins with empty $n\times n$ chessboard and a fixed number $m\in\{1,2,\dots,n\}$. Two players are making moves alternately, each move is placing a coin on one empty square, each row and column can contain at most $m$ coins, the guy who cannot put a coin when he is to play, loses. Who has the winning strategy? In the original problem there was $n=2011$ and $m=1005$. My solution: The first guy wins. First move: a coin in the centre, then symetrical reflections of opponent's moves. After solving the problem, I generalised it. My above solution works for all $n,m$ both odd. If $n$ is even, then the second guy wins by symetrical reflections. What about remaining cases?","['combinatorial-game-theory', 'combinatorics', 'contest-math', 'recreational-mathematics']"
2373544,Domain of a complex-valued function,"Let $f:\mathbb{R}^2 \to \mathbb{C}$ be function from the real plane to a complex plane which we denote by $f(x,y)$. If we introduce the new variables $z = x + iy$ and $\bar{z} = x - iy$, what is the domain of the new function $g(z,\bar{z}) = f(x,y)$? Is it $g:\mathbb{C} \to \mathbb{C}$ or $g:\mathbb{C}^2 \to \mathbb{C}$?","['complex-analysis', 'functions']"
2373548,"I can't find what't wrong with this proof, it's from a discrete mathematics slide",This is a proof in Discrete Mathematics.,"['logic', 'proof-verification', 'discrete-mathematics']"
2373592,Does every continuous map from $\mathbb{Q}$ to $\mathbb{Q}$ extends continuously as a map from $\mathbb{R}$ to $\mathbb{R}$?,"Given a continuous function $f:\mathbb{Q}\to\mathbb{Q}$ ,does there exist a continuous function  $g:\mathbb{R}\to\mathbb{R}$ such that   $g|_{\Bbb Q} = f$? What I have no Idea about how to attempt this Question! Any suggestion will be very helpful.",['real-analysis']
2373599,Estimate function f(x) in high-dimensional space,"I'm working on a problem of estimating a function $y=f(x): \mathbb{R}^d \rightarrow \mathbb{R}$. Namely, I have an unknown function $f(x)$ (like a black box), what I can do is to input $x^{(i)}$ to it, and obtain $y^{(i)}$ ($i=1,2,\cdots, N$). Then I get a dataset $(x^{(i)}, y^{(i)})$ and am able to fit a function on it. The simplest/dumbest way is to use a uniform grid of $x$, namely sampling $m$ points on each dimension ($x_i, i=1,2,\cdots, d$) and form a grid with $m^d$ points. The number of samples would explode with high dimension (very large $d$). A better choice might be using Latin Hypercube or some low discrepancy sequences. I'm wondering are there any literature with rigorous analysis on using those sampling methods to estimate functions? How can I use as less samples as possible (i.e. small $N$) to get a good estimate of $f(x)$. For simplicity, we could assume $f(x)$ is infinitely differentiable ($f \in C^\infty$).","['sampling', 'functions', 'probability-distributions', 'monte-carlo', 'parameter-estimation']"
2373604,A pushout exists for a diagram on three groups?,"From Rotman's Algebraic Topology, I have a question about a particular part of this proof. If $f_1: B \rightarrow A_1$ and $f_2:B \rightarrow A_2$ is a diagram of groups then a pushout exists. Proof: Let $C$ be the pushout defined as $C=(A_1 * A_2)/N$, where $A_1 * A_2$ is the free product and $N$ is the normal subgroup generated by $\{f_1(b)f_2(b^{-1}): b \in B\}$. Define $g_i(a_i) = a_iN$ as a map between $A_i \rightarrow C$, then it should follow that the diagram commutes as $C$ is a solution. I see that I would have to show that $g_1f_1 = g_2f_2$, but I'm not sure how I'd show that $f_1(b)N = f_2(b)N$. I can see that $N=\langle \prod_{b \in B} f_1(b)f_2(b^{-1})\rangle$, but I'm not sure how to proceed. Anyone have any ideas?","['algebraic-topology', 'abstract-algebra', 'category-theory', 'group-theory']"
2373633,Determine all local rings containing $\mathbb{C}$ and having dimension 5 as $\mathbb{C}$-vector space,"Let $R$ be a local ring containing $\mathbb{C}$ and $\dim_{\mathbb{C}}R = 5$ (as $\mathbb{C}$-vector space).
  Let $\mathfrak{m}$ be the maximal ideal of $R$ and $d=\dim_{\mathbb{C}}\mathfrak{m}/\mathfrak{m}^2$. (1) Prove that $1\leq d  \leq4$. (2) For each $d=1,2,3,4$ determine all $R$ up to isomorphism. (1) is easy, but I'm stuck in (2). I know that $R$ is finitely generated $\mathbb{C}$-algebra and $R$ is Artinian. In the case $d=1$, I think $R\cong \mathbb{C}[x]/(x^5)$, but I can't prove it.
In the case $d=2,3,4$, I have no idea. How to solve (2)?","['abstract-algebra', 'ring-theory', 'commutative-algebra']"
2373639,Closed set mapped to itself in a compact Hausdorff space,"Let $X$ be a compact Hausdorff space and let $f:X\to X$ be continuous. Prove that there exists a non-empty closed set $C$ such that $f(C) = C$. Proving by contradiction seems hard to me, since if $f(C)\neq C$ for all closed sets $C$, then we can have either $f(C)\subset C$, $f(C)\cap C\neq\varnothing$ or $f(C)\cap C = \varnothing$. The only thing I might have thought about is to set $C = f(X)$. Since $X$ is compact, $f(X)$ is compact and since $X$ is Hausdorff, $f(X)$ is closed. Then we have that $f(C)\subseteq C$ by definition. I was not able to prove that $C\subseteq f(C)$, namely that $f(X) \subseteq f(f(X))$. It is not obvious for me that in general it is even true, so maybe the choice $C = f(X)$ does not help.","['continuity', 'general-topology', 'compactness']"
2373651,"$a \in (0,1]$ satisfies $a^{2008} -2a +1 = 0$ and we define $S$ as $S=1+a+a^2+a^3........a^{2007}$. The sum of all possible value(s) of $S$ is?","This is homework . Let $a \in (0,1]$ satisfies the equation $$a^{2008} -2a +1 = 0$$ 
and we define $S$ as $$S=1+a+a^2+a^3........a^{2007}$$ The sum of all possible value(s) of $S$ is? My Attempt $a=1$ is obviously a solution.Hence one value of $S$ is $2008$. To find other values of $S$, I need all the other solutions of $a$ lying between $0$ and $1$. When I graphed the function here , the other root was approximately coming out to be $0.5$. But I think that such a problem has to be solved exactly and no approximations are needed. Hence, I am stuck here. Solutions should preferably not involve a calculator or any computer tool and use college level math only (since this problem was found in a college level book).","['polynomials', 'sequences-and-series', 'algebra-precalculus', 'summation', 'geometric-progressions']"
2373696,Cosecants of half angles,"The tangent of an angle is $2.4$. Find the cosecant of half the angle. My tries: As $\tan A=2.4=\dfrac{12}{5}\implies \sin A=\pm\dfrac{12}{13}=\dfrac{1}{\csc A}$ Also: $\sin \frac{A}{2}+\cos \frac{A}{2}=\pm\sqrt{1+\sin A}=\pm\dfrac{5}{{\sqrt{13}}}$ , considering $\sin A=\dfrac{12}{13}$ $\sin \frac{A}{2}-\cos \frac{A}{2}=\pm\sqrt{1-\sin A}=\pm\dfrac{1}{{\sqrt{13}}}$, considering $\sin A=\dfrac{13}{13}$ Adding them gives: $\sin \frac{A}{2}=\pm\dfrac{3}{\sqrt{13}}=\dfrac{1}{\csc{\frac A2}}$ I did same by considering $\sin A=-\dfrac{12}{13}$ then also it gave the same result. But answer provided by the author is $\dfrac{\pm\sqrt{13}}{2}$ and $\dfrac{\pm\sqrt{13}}{3}$. So what did I miss? please help.","['algebra-precalculus', 'trigonometry']"
2373719,"Prob. 4, Chap. 6, in Baby Rudin: If $f(x)=0$ for all irrational $x$ and $f(x)=1$ for all rational $x$, then $f$ is not integrable on $[a, b]$","Here is Prob. 4, Chap. 6, in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: If $f(x) = 0$ for all irrational $x$, $f(x) = 1$ for all rational $x$, prove that $f \not\in \mathscr{R}$ on $[a, b]$ for any $a < b$. My Attempt: By Theorem 1.20 (b) in Baby Rudin, 3rd edition, between any two real numbers, there is at least one (in fact, infinitely many) rational numbers. From this we can also prove that between any two real numbers, there is at least one (in fact infinitely many) irrational numbers. Thus between any two real numbers, there are infinitely many rational numbers and infinitely many irrational numbers. Now let $P = \left\{ \ x_0, x_1, \ldots, x_n \ \right\}$ be an arbitrary  partition of the interval $[a, b]$, where 
  $$ a = x_0 < x_1 < \cdots < x_n = b. $$ Now for each $i = 1, \ldots, n$, there are infinitely many rational numbers and infinitely many irrational numbers in the sub-interval  $\left[ x_{i-1}, x_i \right]$; so we must have 
  $$ \sup \left\{ \ f(x) \ \colon \ x_{i-1} \leq x \leq x_i \ \right\} = 1, $$
  and 
  $$ \inf \left\{ \ f(x) \ \colon \ x_{i-1} \leq x \leq x_i \ \right\} = 0.  $$
  Therefore, 
  $$ 
\begin{align}
U (P, f) &= \sum_{i=1}^n \sup \left\{ \ f(x) \ \colon \ x_{i-1} \leq x \leq x_i \ \right\} \cdot \left( x_i - x_{i-1} \right) \\ 
&= \sum_{i=1}^n 1 \cdot \left( x_i - x_{i-1} \right) \\ 
&= x_n - x_0 \\
&= b-a, 
\end{align}
$$
  and 
  $$ 
\begin{align}
L (P, f) &= \sum_{i=1}^n \inf \left\{ \ f(x) \ \colon \ x_{i-1} \leq x \leq x_i \ \right\} \cdot \left( x_i - x_{i-1} \right) \\ 
&= \sum_{i=1}^n 0 \cdot \left( x_i - x_{i-1} \right) \\ 
&= 0.
\end{align}
$$
  So if $\varepsilon$ be any real number such that $0 < \varepsilon < b-a$, then we see that 
  $$ U(P, f) - L(P, f) = b-a > \varepsilon $$
  for any partition $P$ of the interval $[a, b]$. Therefore (by Theorem 6.6 in Baby Rudin, 3rd edition) we can conclude that 
  $f \not\in \mathscr{R}$ on $[a, b]$. Is my proof good enough logic, rigor, and presentation-wise?","['real-analysis', 'proof-verification', 'integration', 'definite-integrals', 'analysis']"
2373726,"Differential 1-form for $f(x,y) = \sin(x^2 + y^2)$ on vector field $\mathbf{A}=x\partial_x + y\partial_y$","I was doing some research into differential 1-forms and came across a problem which asks for the differential 1-form $\omega = df$ on the vector field $\mathbf{A}=x\partial_x + y\partial_y$ if $f(x,y) = \sin(x^2 + y^2)$. So I undertand that if $\omega$ is a differential 1-form then it can be expressed as $\omega=df=F(x,y)dx+G(x,y)dy=2x\cos(x^2+y^2)dx + 2y\cos(x^2+y^2)dy$. But $dx$ and $dy$ have vector arguments, so as a function of $\textbf{A}$ this is $\omega(\textbf{A})=2x\cos(x^2+y^2)dx(\textbf{A}) + 2y\cos(x^2+y^2)dy(\textbf{A})=2x\cos(x^2+y^2)A_x + 2y\cos(x^2+y^2)A_y$ In this case, the solution to the problem reads that $A_x = x$ and $A_y = y$, but why is this so, and what is the meaning of $\partial_x$ and $\partial_y$ in this problem (i.e. what are we taking partial derivatives of)?","['derivatives', 'differential-geometry', 'calculus', 'geometry']"
2373738,Coordinate free proof that det is algebraic,"If V is a vector space of dimension N, and $f\colon V \to V$ is an endomorphism, one defines its determinant to be the scalar corresponding to the induced map $\wedge^N V \to \wedge^N V$. I like this definition, but I can't really say I know how to work with it. For example, how do you prove that det is an algebraic map? I'd like to show that GL(V) is a variety, without picking a basis for V. Edit: As pointed out in the comments, det is a map End(V) $\to$ End($\wedge^N V$) = k, where k is the base field. This map is far from being linear, in fact by picking a basis for V one can look up in any textbook that it is a polynomial of degree N in the entries of the corresponding matrix, i.e. an element of Sym$^N$End(V)$^*$. So the question boils down to identifying det as an element of SymEnd(V)$^*$ = SymEnd(V$^*$).","['category-theory', 'linear-algebra', 'algebraic-geometry', 'lie-groups']"
2373775,What topological properties are invariant under diffeomorphism?,"In General Topology if we a topological space $(X, \mathcal{T})$, that is homeomorphic to another space $(Y, \mathcal{K})$, there are a number of topological properties, such as compactness, connectedness, path-connectedness, that are invariant under homeomorphism. In Algebraic Topology if we have homeomorphic topological spaces $(X, \mathcal{T})$ and $(Y, \mathcal{K})$, then we can conclude that they have isomorphic fundamental groups $\pi_1(X) \cong \pi_1(Y)$ But in Differential Topology, the question of what topological properties are preserved by diffeomorphisms seems to be something that I can't quite answer at the moment. Certainly diffeomorphisms are stronger versions of homeomorphisms, so all the things we expect to be invariant under homeomorphisms (compactness, connectedness etc.) are also invariant under diffeomorphism. However I would like to know if there are further topological (or perhaps non-topological) properties that are invariant under diffeomorphism?","['general-topology', 'differential-topology']"
2373801,"For which $q$ there exists a Steiner system $S(2, q, q^2)$?","I encountered the title question answering this question . It is well-known (see, for instance [vdW, $\S$ 43]) when $q$ is a power of a prime number there exists a finite field of order $q$. In this case a Steiner System $S(2, q, q^2)$ can be realized as a finite affine plane . I guess that the answer for general case may be already known, so I googled for it, but failed to find it. References [vdW] B. L. van der Waerden, Algebra (Russian edition).","['combinatorial-designs', 'combinatorics', 'reference-request']"
2373820,"Prob. 5, Chap. 6, in Baby Rudin: If $f^2$ is integrable, does it follow that $f$ is integrable too? What about $f^3$?","Here is Prob. 5, Chap. 6, in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $f$ is a bounded real function on $[a, b]$, and $f^2 \in \mathscr{R}$ on $[a, b]$. Does it follow that $f \in \mathscr{R}$? Does the answer change if we assume that $f^3 \in \mathscr{R}$? My Attempt: The Riemann-integrability of $f^2$ need not imply the integrability of $f$. For example, let $f$ be defined on $\mathbb{R}$ by 
  $$
f(x) \colon= 
\begin{cases} 1 \qquad & \mbox{ if } x \in \mathbb{Q}, \\ -1 \qquad & \mbox{ if } x \not\in \mathbb{Q}. \end{cases} 
$$
  Then $f^2(x) = 1$ for all $x \in \mathbb{R}$, and so 
  $$ \int_{-r}^{+r} f(x) \ \mathrm{d} x = 2r $$
  for any real number $r > 0$. However, this function $f$ is not integrable on $[a, b]$, where $a$ and $b$ are real numbers such that $a < b$. The details are as follows: If $P$ be any partition of $[a, b]$, then we have 
  $$ L(P, f) = -(b-a), \ \mbox{ and } \ U(P, f) = b-a, $$
  and so 
  $$ U(P, f) - L(P, f) = 2(b-a) > \varepsilon $$
  for any real number $\varepsilon$ such that $0 < \varepsilon < 2(b-a)$, and thus the condition of Theorem 6.6 in Baby Rudin is violated. Is what I've stated above correct? If so, then is my counter-example the right one? And, have I managed to present this argument correctly as well? On the other hand, if $f^3$ is integrable on $[a, b]$, then so is $f$. Am I right? The proof is as follows: As $f$ is a bounded real function on $[a, b]$, so the supremum and infimum of the range of $f$ exist in $\mathbb{R}$. Let us put 
  $$ M \colon= \sup \left\{ \ f(x) \ \colon \ a \leq x \leq b \ \right\}, \ \mbox{ and } \ m \colon= \inf \left\{ \ f(x) \ \colon \ a \leq x \leq b \ \right\}. $$ Since the function $t \mapsto t^3$ is a continuous one-to-one mapping of $\mathbb{R}$ onto $\mathbb{R}$, therefore it is also a continuous one-to-one mapping of any finite interval $[c, d ]$  onto the interval $\left[c^3, d^3 \right]$. Moreover, the interval $[ c, d ]$  is compact.  So by Theorem 4.17 in Baby Rudin, the function $t \mapsto t^{1/3}$ is a continuous (one-to-one) mapping of $[c^3, d^3 ]$ onto  $[c , d ]$. Now as $f^3 \in \mathscr{R}$ on $[a, b]$, $m^3 \leq f^3 \leq M^3$ on $[a, b]$, and the map $\phi$ defined on $\left[m^3, M^3 \right]$ by 
  $\phi(t) = t^{1/3}$ is continuous, so by Theorem 6.11 in Baby Rudin the function $h = \phi \circ f^3 $ is also integrable on $[a, b]$. Is my reasoning in this proof correct? If so, then have I presented this proof correctly and lucidly enough too?","['real-analysis', 'calculus', 'integration', 'definite-integrals', 'analysis']"
2373844,"If $f: \mathbb{C} \to \mathbb{C}$ is continuous and analytic off $[-1,1]$ then is entire.","This is a problem from Complex Variable (Conway's book) 2nd ed. (Section 4.4) 9. Show that if $f: \mathbb{C}\to\mathbb{C}$ is a continuous function such that $f$ is analytic off $[-1,1]$ then $f$ is an entire function. I already have a solution by Morera's theorem that split this problem in 5 cases. I think this solution is too long and I'm trying to solve this using a different approach. Any ideas ?","['continuity', 'complex-analysis', 'analytic-functions', 'entire-functions']"
2373907,The limit of a sequence,"Given $$x_n=\frac{1}{n^2+1}+\frac{1}{n^2+2}+\frac{1}{n^2+3}+\cdots+\frac{1}{n^2+n}$$ Verify if there is or no a limit. Find it if affirmative. Let $a_n=\frac{n}{n^2+1}$ (The biggest portion of the sum $n$ times)  and $b_n=\frac{n}{n^2+n}$ (the smallest portion of the sum $n$ times)
then
$$b_n\le x_n \le a_n$$
since$$\lim \frac{n}{n^2+1}=\lim \frac{n}{n^2+n}=0,$$
we have that
$$\lim x_n=0.$$ Is this wrong? why? If it is, any tips on how to find $\lim x_n$? Grateful for any help. **Edited","['limits', 'sequences-and-series', 'calculus', 'summation', 'fractions']"
2373967,Why is the Sasaki metric natural?,"Let $(M,g)$ be a Riemannian manifold with $\text{dim}(M)=n$. Then, there is a ""natural"" metric $\tilde{g}$ on the tangent bundle $TM$, so that $(TM,\tilde{g})$ is a Riemannian manifold, called the Sasaki metric, where a line element is written  (with local coordinates of $TM$ given by $(x,v)$):
$$
d\sigma^2 = g_{ij}\,dx^idx^j + g_{ij}\, Dv^iDv^j
$$
where $D$ represents covariant differentiation:
$$ Dv^i = dv^i + \Gamma_{jk}^iv^jdx^k $$
In components, letting indices range over $1$ to $n$, this is:
\begin{align}
\tilde{g}_{jk} &= g_{jk} + g_{\alpha \gamma}\Gamma_{\mu j}^\alpha\Gamma_{\eta k}^\gamma v^\mu v^\eta =: g_{jk}+A_{jk} \\
\tilde{g}_{j(n+k)} &= g_{kd}\Gamma^d_{\lambda j}v^\lambda =: B_{jk}\\
\tilde{g}_{(n+j)(n+k)} &= g_{jk}
\end{align}
Or, as a matrix:
$$
\tilde{g} = \begin{bmatrix}
g+A & B \\ B^T & g
\end{bmatrix}
$$ Question: intuitively speaking, why is this ""natural""? I am aware of other ""natural"" metrics on the tangent bundle; this question is specifically about this one, and the geometric intuition for why it is a good choice of metric. I can't seem to picture it. Related: [1] , [2] , [3]","['riemannian-geometry', 'tangent-bundle', 'manifolds', 'vector-bundles', 'differential-geometry']"
2373989,Using Partial Summation with an Infinite Sum,"I'm looking at a proof of Brun's theorem. Let $\mathcal{P}=\{p  : p+2 \text{ is also prime}\}$. Then we want to show that the sum $$\sum_{p\in \mathcal{P}} \frac{1}{p} < \infty .$$ The proof given just says ""by partial summation."" I'm comfortable using partial summation, but usually it would be with a sum over $p\leq x$ or something. I know we don't actually know if the set $\mathcal{P}$ is infinite (since that would be the twin prime conjecture), but it's the lack of an $x$ that's got me confused. I was looking at either defining a multiplicative function 
$$a(n)= \begin{cases}
     1 & \text{if $n$ and $n+2$ are both prime}\ \\
      0 & \text{otherwise}
    \end{cases}$$ and letting $f(n)=1/n$, then $$\sum_{p\in \mathcal{P}} \frac{1}{p} = \sum_{n} a(n) f(n) .$$ I also thought about multiplying and dividing by $\log(p)$ (or both), but I'm still stuck with the lack of an $x$. I don't need help with showing the series converges, it's specifically using partial summation on a sum of this kind that I need help with. Edit: For anyone interested, the proof was from An Introduction to Sieve Methods and Their Application by Alina Cojocaru and Ram Murty. I still don't know for sure what they intended, but I decided to impose a limit on $p$ and use partial summation over the sum of primes $p \leq x$, then let $x$ tend to infinity to get the result.","['number-theory', 'prime-numbers']"
2374000,Differentiability of a supremum of a family of functions with respect to a parameter,"Let $f : \Bbb R \times M \to [0, \infty)$ be smooth, with $M$ a compact smooth manifold. Let $g : \Bbb R \to [0, \infty)$ be given by $g(t) = \sup _{x \in M} f(t,x)$. Is it true that $g$ is smooth? The issue here is the permutation of $\lim$ and $\sup$ - what techniques should I apply? Uniform convergence is out of the question. If instead of $\sup$ I had had an integral, I would have tried to use the dominated convergence theorem, or any other result from this family of theorems, but what to try here? Since $\sup$ is not additive, I cannot view it as a positive linear functional, therefore I cannot use the Riesz-Markov representation theorem on it (which would have given me a positive measure to which the dominated convergence theorem would have applied).","['functional-analysis', 'derivatives', 'continuity', 'limits']"
2374019,How to start with $\pi$ defined as the area of the unit circle.,"In IV-1, Example 3 of Advanced Calculus of Several Variables, by C. E. Edwards Jr. the reader is told ""Since $\pi$ is by definition the area of the unit circle,..."". The example provides a proof that $A=\pi r^2$, so that result cannot be used to justify the (unconventional) definition. Every definition of $\pi$ I have seen in mathematical literature amounts to: Given a circle of diameter $D$ and circumference $C$, the real number $\pi$ is defined as $\pi=\frac{C}{D}$. I could propose a number of alternative ways of defining $\pi$. For example, Wallis's product: https://en.wikipedia.org/wiki/Wallis_product .  From there one must demonstrate that $\pi$ so defined satisfies $\pi=\frac{C}{D}$. Similarly, the definition promulgated by Edwards must result in $\pi=\frac{C}{D}$.  I'm sure I could provide persuasive arguments toward this end.  What I would like to know is whether there is an established convention for demonstrating that Edwards's definition of $\pi$ is equivalent to the traditional form I stated above. In particular, I am interested to know how this has been done without the use of trigonometry nor calculus. Edit to add: This is my heuristic argument showing $2\pi r=C$. Assuming $A=\pi r^2$. For a small change in radius $\Delta r$ there will be a corresponding change in circumference $\Delta C$. The change in area will be the area of an annulus $\Delta A=\pi(r-\Delta r)^{2}-\pi r^{2}=\pi(2r\Delta r+\Delta r^{2})=(C+\varepsilon)\Delta r$. Where $0<\varepsilon<\Delta C$. The last expression follows from the observation that $(C+\Delta C)\Delta r>\Delta A>C\Delta r$. That is, $\Delta A$ falls somewhere between the area of a rectangle $C\times\Delta r$ and the area of a rectangle $(C+\Delta C)\times\Delta r$. The last two expressions in the above equivalence result in $2\pi r+\pi\Delta r=C+\varepsilon$. Clearly $\varepsilon\to0$ as $\Delta r\to0$. So what remains is $2\pi r=C$.","['axioms', 'geometry']"
2374042,Continuous mapping Theorem,"I want to show that $$\lim_{N \rightarrow \infty} \int_\Theta q(\theta) \log\frac{p(\theta\mid X^N)}{\mathcal{N}(\theta\mid\theta_0,N^{-1}I(\theta))} \, d\theta = 0.$$ I know that $p(\theta\mid X^N) \rightarrow \mathcal{N}(\theta\mid \theta_0, N^{-1}I(\theta))$ w.p $1$. And using continuous mapping theorem $|\log (p(\theta\mid X^N))- \log(\mathcal{N}(\theta\mid\theta_0,N^{-1}I(\theta)))| \rightarrow 0$ w.p $1$. I want to use the argument that since for large $N$   $$\ \ \log\frac{p(\theta\mid X^N)}{\mathcal{N}(\theta\mid\theta_0,N^{-1}I(\theta))}< \varepsilon \Rightarrow \int_ \Theta q(\theta) \log\frac{p(\theta\mid X^N)}{\mathcal{N}(\theta\mid \theta_0,N^{-1} I(\theta))} \, d\theta < \varepsilon \int_\Theta q(\theta) \, d\theta = \varepsilon.$$ Hence my proposition is correct.","['probability-theory', 'measure-theory']"
2374061,Is it possible to create a screwless cube made out of 12 individual planks?,"The planks all have to be identical. But I'm trying to figure out if there is a configuration in which it can be created. Below is an example of a screwless square shape. This cube needs to be able to be assembled, not just fit together. Image",['geometry']
2374066,Interesting identity about Trig Functions and Complex Numbers,"I recently discovered the beautiful identity
$$(\cos\theta+i\sin\theta)^n=\cos n\theta+i\sin n\theta$$
and I proved it using induction. However, I can't figure out if this identity also applies to non-integer values of $n$. How can I determine this, and how do I prove it? Certainly not with induction... NOTE: If I could prove the original identity without induction, then that should do it... but I'm not sure how to do that. Thanks!","['induction', 'trigonometry', 'complex-numbers']"
2374091,"Proof in do Carmo's ""Differential Forms and Applications""","I've trying to figure this out for a while and I am finally desperate enough to post to stack exchange In do Carmo's Differential Forms and Applications Proposition 2 on pg 92 (this is the proof that the Gaussian curvature is well-defined, independent of choice frame and coframe). I post the statement and proof for context: Proposition 2 Let $M^2$ be a Riemannian manifold of dimension two. For each $p\in M$, we define a number $K(p)$ by choosing a moving [orthonormal] frame $\{e_1,e_2\}$ around $p$ and setting
  $$
d\omega_{12}(p):=-K(p)(\omega_1\wedge\omega_2)(p).
$$
  [Here $\{\omega_1,\omega_2\}$ is the coframe associated to $\{e_1,e_2\}$.] Then $K(p)$ does not depend on the choice of frames, and is called the Gaussian curvature of $M$ at $p$. Proof. Let $\{\bar{e}_1,\bar{e}_2\}$ be another moving [orthonormal] frame around $p$. Assume first that the orientations of the two moving frames are the same. Then
  $$
\omega_{12}=\bar{\omega}_{12}-\tau.
$$
  [Here $\tau=fdg-gdf$, where $f$ and $g$ are differentiable functions such that $f^2+g^2=1$; this was shown in a earlier lemma -- Lemma 4 on pg 90.] Since $\tau =fdg-gdf$, $d\tau =0$, hence $d\omega_{12}=d\bar{\omega}_{12}$ [this is the part I don't understand; I will elaborate afterwards]. It follows that
  $$
-K\omega_1\wedge\omega_2=d\omega_{12}=d\bar{\omega}_{12}=-\bar{K}\bar{\omega}_{1}\wedge\bar{\omega}=-\bar{K}\omega_1\wedge\omega_2
$$
  hence $K=\bar{K}$, as we wished. If the orientations are opposite, we obtain
  $$
d\omega_{12}=-d\bar{\omega}_{12},\hspace{.2 in}\omega_1\wedge\omega_{2}=-\bar{\omega}_1\wedge\bar{\omega}_2
$$
  and the same conclusion holds. This is slightly embarrassing since I have been working with differential forms for a few years now, but I don't understand why $d\tau=0$. In my line of thinking (interpreting $fdg$ and $gdf$ as wedge products between 0-forms and 1-forms and writing out in full detail)
\begin{align*}
d\tau &=d(fdg-gdf)\\
&=df\wedge dg+f d^2g-dg\wedge df-gd^2f=2df\wedge dg\\
&=df\wedge dg+f\cdot 0-dg\wedge df-g\cdot 0\\
&=df\wedge  dg-dg\wedge df\\
&=2df\wedge dg
\end{align*}
which is not zero, unless there is some extra information about $f$ and $g$ I don't know about. It does seem like Differential Forms and Applications has quite a few typos, so I was thinking it was supposed to be $fdg+gdf$, but I went through the work where this ""$\tau$"" first popped up, and it seems like this is the correct form to be working with. Does anyone have any insight on what could be going wrong here? Thanks","['differential-forms', 'differential-geometry', 'curvature']"
2374104,Local behavior of Brownian Motion,"Let $(\Omega,\mathcal A,\Bbb P)$ be a complete probability space.
Let us consider a standard Brownian Motion $B=(B_t)_{t\ge0}$ on this space. We know that its trajectories are $\alpha$-Holder continuous for every $\alpha<1/2$, and moreover that a.s. $B_0=0$. Thus I was asking myself (even looking at the many realizations that can be found on the web):  is it possible to control the behaviour of the trajectories near $0$? I mean, I conjectured that $\forall \epsilon>0\;\;\exists\delta>0$ such that
$$
t^{1/2+\epsilon}\le|B_t|\le t^{1/2-\epsilon}\;\;\forall t\in[0,\delta].
$$
This seems reasonable, but I don't know how to prove or disprove it. EDIT: GENERALIZATION If $B^H=(B_t^H)_{t\ge0}$ is a fractional Brownian Motion of Hurst parameter $0<H<1$, is it possible to prove that
$$
t^{H+\epsilon}\le|B_t^H|\le t^{H-\epsilon}\;\;\forall t\in[0,\delta]?
$$","['real-analysis', 'holder-spaces', 'brownian-motion', 'probability-theory']"
2374227,"Integral $\int _{0}^{\pi /2}x\cos (8x)\ln \tan x \, \text{d}x$","Inspired by this topic , can I easily prove the result below? $$\int _{0}^{\pi /2}x\cos (8x)\ln \tan x \, \text{d}x=\frac{13}{36}$$ The elementary antiderivative exists, but it seems masochist if one wants compute it.","['integration', 'definite-integrals', 'calculus']"
2374230,Involution that brings sets to disjoint sets,"Let $A$ be a collection of subsets of $\{1,2,\dots,n\}$ that is closed under taking subsets (that is, if $U\in A$ and $V\subseteq U$ then $V\in A$). Is there always an involution $f:A\to A$ such that $f(V)\cap V=\emptyset$ for all $V\in A$? I'm guessing yes. Note that if $A=\mathcal P(\{1,2,\dots,n\})$, then taking the complement works. Also note that if $|A|$ is odd, we can send the empty set to itself (the empty set is disjoint from itself, isn't that weird?). I tried working through a few small examples. I haven't found a counterexample, but I also haven't found a proof.","['graph-theory', 'discrete-mathematics', 'elementary-set-theory', 'combinatorics', 'involutions']"
2374232,3 disks covering the triangle?,"Let $H$ be the orthocentre of $ABC$ triangle. Does the sum of disks of diameters $AH,BH,CH$ cover the whole triangle? It seems so, each disk covers the part of triangle nearest to appropriate vertex, all disks meet ""close to the centre of the triangle""  and cover this region. But this is an informal observation, not a formal proof. How it can be proven?","['triangles', 'geometry']"
2374244,Proving a set is infinite,"Question: Let $B$ be a proper subset of a set $ A$, and let $f$ be a bijection from $A$ to $B$. Prove that $A$ is an infinite set. My attempt: Proof by contradiction: Assume $A$ is an finite set. Then $ B$ is a finite set. 
Since $B$ is a proper subset of $A$, we have $ |A|>|B|$. This is a contradiction since $ f : A\to B$ is a bijection and |A| = |B|.","['cardinals', 'elementary-set-theory', 'proof-verification']"
2374270,Inference about standard deviation of normal sample,"An experiment was conducted to investigate the filling capability of packing equipment at a winery. 20 bottles were randomly selected, and the fill volume $(in$ $ml)$ was measured. Assume that the fill volume has a normal distribution. The data is shown below: \begin{array}{|c|c|c|c|c|}
\hline
753& 751 & 752 & 753 & 753 \\ \hline
 753& 752& 753&  754& 754\\ \hline
 752&  751& 752&  750& 753\\ \hline
 755&  753& 756&  751& 750\\ \hline
\end{array} $a)$  Does the data support the claim that the standard deviation of fill volume is less than 1 ml? Use $α=0.05$. $b)$  Find a 95% two-sided confidence interval on the standard deviation of fill volume.
$$\\$$
This is what I have so far: $a)$ $H_o:σ = 1$ $H_a:σ < 1$ where: $\bar{X}$ = 752.55  and  $S_x$ = 1.538112309 $$\\$$ $\chi^2 =  \frac{(n-1)s^2}{\sigma^2}$  = $\frac{(19)1.538112309^2}{1^2}$ = 44.950 Is what I did correct? & I don't know how to find the confidence interval on the standard deviation, I would really appreciate your help!","['statistics', 'confidence-interval', 'hypothesis-testing']"
2374291,Problems while solving the differential equation.,"$$x^2\frac{d^2y}{dx^2}+x^2\frac{dy}{dx}-2y=0$$ $x=0$ is a regular singular point. $$y=\sum_{n=0}^\infty c_nx^{n+r}$$ $$\frac{dy}{dx}=(n+r)\sum_{n=0}^\infty c_nx^{n+r-1}$$ $$\frac{d^2y}{dx^2}=(n+r)(n+r-1)\sum_{n=0}^\infty c_nx^{n+r-2}$$ $$(n+r)(n+r-1)\sum_{n=0}^\infty c_nx^{n+r}+(n+r-1)\sum_{n=1}^\infty c_{n-1}x^{n+r}-2\sum_{n=0}^\infty c_nx^{n+r}=0$$ Taking out a few terms $$(r)(r-1)c_0x^r+-2c_0x^r+(n+r)(n+r-1)\sum_{n=1}^\infty c_nx^{n+r}+ (n+r-1)\sum_{n=1}^\infty c_{n-1}x^{n+r}-2\sum_{n=1}^\infty c_nx^{n+r}=0$$ The incidal equation is $$r^2-r-2=0$$ The recurrence formula is, $$(n+r)(n+r-1)c_n+(n+r-1)c_{n-1}-2 c_n=0$$ Bigger roots Let $r=r_1=2$ $$(n+2)(n+1)c_n+(n+1)c_{n-1}-2c_n=0$$ $$c_n=\frac{-(n+1)c_{n-1}}{n^2+3n}$$ $$c_1=\frac{-c_0}{2},$$ $$c_2=\frac{-c_{1}}{6}$$ Taking the smaller root, $r=r_2=-1$ $$(n-1)(n-2)c_n+(n-2)c_{n-1}-2c_n=0$$ $$c_n=\frac{(2-n)c_{n-1}}{n^2-3n}$$ How shall I continue this any further? Any help would be appreciated. Can someone hint me on this question.",['ordinary-differential-equations']
2374308,Minimal hypersurfaces and covering maps,"I'm studying minimal surfaces. In a variety of sources people seem to be using the following two results which I can't prove. 1. Let $(M,g)$ be a compact $n$-dimensional riemannian manifold and $\pi:(\tilde{M},\tilde{g}) \to (M,g)$ a covering map where $\tilde{g}$ is the riemannian metric in $\tilde{M}$ given by the pullback of $g$ by $\pi$, i.e., $\tilde{g}=\pi^*g$. Question: If $f:(\Sigma,h) \to (\tilde{M},\tilde{g})$ is a minimal isometric immersion where $(\Sigma,h)$ is a $(n-1)$-dimensional riemannian manifold with $h=f^*g$, then $\pi\circ f: \Sigma \to M$ is a minimal isometric immersion? 2. Let $(M,g)$ be a $n$-dimensional riemannian manifold and $f: (\Sigma,h) \to (M,g)$ an minimal isometric immersion with $(\Sigma,h)$ a $(n-1)$-dimensional riemannian manifold and $h=f^*g$. Question: If $\pi : \tilde{\Sigma} \to \Sigma$ is a covering map such that $\tilde{\Sigma}$ is endowed with the metric $\tilde{g}=\pi^*g$ then $f\circ \pi : \tilde{\Sigma}\to M$ is a minimal isometric immersion? Any help or reference would be highly valuable. Thank you.","['minimal-surfaces', 'riemannian-geometry', 'differential-geometry']"
2374341,What is the probability that a red ball is chosen before the black ball?,"A box contains 2 white balls, 2 red balls and a black ball. Balls are chosen without replacement from the box. What is the probability that red ball is chosen before the black ball? I am quite confused about the question because this is a exercise arranged in ""combination"" section, however I intuitively think it as a ""permutation"" problem. The red balls and the white balls have to be different, and as a red is chosen before the black, then its order should be accounted with. Furthermore, the question is that ""the red"", thus the red ball labeled 1 and the red ball labeled 2 can be chosen without considering their orders. Can anyone give me a clue to solve this kind of problem?","['combinatorics', 'probability']"
2374343,"Proof of $Y=F_X(X)$ being uniformly distributed on $[0,1]$ for arbitrary continuous $F_X$","This question is related to Showing that Y has a uniform distribution if Y=F(X) where F is the cdf of continuous X , with the difference being that $F_X$ (the probability distribution function of random variable $X$) is an arbitrary continuous distribution function, not necessarily strictly increasing. I think the proof is similar, but we have to take care of the possibility that $F_X$ may not be $1$-to-$1$.  I list my attempt below, and would appreciate it if someone can confirm if it's correct, and, in particular, if it can be improved.  Thanks a lot! The goal is to show $F_Y(y)=y$ for any $y \in [0,1]$.  To do so, note that $F_Y(y)\triangleq\mathbb P(\{Y\le y\})$, and $\{Y\le y\}=\{F_X(X)\le y\}=\{X\in F_X^{-1}([0, y])\}.$ Since $F_X$ is continuous, $F_X^{-1}([0,y])$ must be closed.  So it follows that $\sup F_X^{-1}([0, y])=\max F_X^{-1}([0, y])=\max F_X^{-1}(\{y\}),$ which let's denote by $a$. Therefore, $F_Y(y)=\mathbb P(\{X\le a\})=F_X(a)=y.\quad$  (Q.E.D.)","['probability-theory', 'probability', 'analysis', 'proof-verification']"
2374403,Reference request for complete probability text without measure theory,"I'm looking for a complete probability reference text, covering the majority of standard probability and stochastic process topics that can be covered without the use of measure theory. I've already had a basic course in probability and in stochastic processes, so I'm looking for more of a desk reference type of book. The three that have been recommended to me are Probability, Statistics, and Random Processes by Papoulis Probability for Statistics and Machine Learning by DasGupta Probability By Feller I'm not too interested in Feller, since it seems that vol 2 has a fair bit of measure theory and vol 1 is only discrete. Any other recommendations?","['stochastic-processes', 'reference-request', 'statistics', 'book-recommendation', 'probability']"
2374410,How to fit 5 balls into a can.,"You have 5 balls of radii 5 cm. You have a cylindrical can with radius 8 cm. What is the optimal placement of the balls within the can to minimise the height of the can (Distance from top of top ball to bottom of bottom ball) and what is this height. So far, I believe that alternating balls in a 2d plan would be the most efficient but I cannot show this and I cannot figure out what the heigh would be. [Edit]
The other option that I have considered is a spiral-like placement, where each ball is rotated 360/5° around the centre of the can from the last leading to their centres forming a pentagon when looked at from above. How would you calculate the height or show which one would be optimal without just measuring it?","['3d', 'geometry']"
2374423,On $17^7+76271^3=d^2$ and coprime solutions to $x^6+y^3=z^2$?,"This post got me curious. The Diophantine equation with $x\neq1$, $$x^n+y^3 = z^2\tag1$$ has infinitely many coprime solutions when $n\leq5$. Are there at least a few co-prime ones when $n>5$? $n=6:$ $\color{red}{??}$ No solutions with $x<100$ and $0<y<100000$. (I checked.) $n=7:$ $$2^7+17^3=71^2$$
$$17^7+76271^3=21063928^2$$ $n=8:$ $$43^8+96222^3 = 30042907^2$$ Q: Does $n=6$ in fact have a coprime solution? (It seems strange that $n=7,8$ has but $n=6$ doesn't. A larger search radius might yield a result.) P.S.: From work by Darmon and Granville, if $1/p+1/q+1/r<1$, then the equation $ax^p+by^q+cz^r=0$ has only finitely many coprime solutions. So $(1)$ has only finitely many for $n\geq7$.","['number-theory', 'diophantine-equations']"
2374430,Every polynomial's image contains $0$ or $1$ in a field $\Bbb F$,"This question talks about fields in which every polynomials are almost surjective, while I am interested in the following case: $\Bbb F$ is a field such that for every non-constant polynomial $f$ over $\Bbb F$, $f$ or $f-1$ has a root in $\Bbb F$. If $\Bbb F$ is not the field with two elements, must $\Bbb F$ be algebraically closed?","['abstract-algebra', 'field-theory']"
2374455,The Hausdorff property versus closedness of the diagonal in the context of convergence spaces,"Given a topological space $X$, the following are equivalent: Given points $x$ and $y$, there exist neighborhoods $A$ and $B$ of $x$ and $y$ respectively satisfying $A \cap B = \emptyset$. Every proper filter converges to at most one point. The diagonal set $\{(x,x) : x \in X\}$ is a closed subset of $X \times X$. We learn as undergraduates that a topological space with any, and hence all, of these properties is said to be Hausdorff. Now suppose we're trying to generalize from topological spaces to convergence spaces . The standard definition of being Hausdorff in this context is (2). Presumably, (1) is just the wrong definition in this context, and probably not worth thinking about too much. But (3) still looks interesting enough; for example, it's equivalent to being able to think of equality as a function $X \times X \rightarrow \Omega$, where $\Omega$ is the Sierpinski space. Question. Is there a relationship between conditions (2) and (3) at the level of convergence spaces?","['general-topology', 'separation-axioms', 'filters']"
2374491,Explicit formula for the recurrence relation $x_n = a x_{n-1} + b (n-1) x_{n-2} +c$,"Let $a,b,c$ be non-zero constants. How can we find an explicit formula for a sequence obeying a recurrence relation of the following type? $$ x_n = a x_{n-1} + b (n-1) x_{n-2} +c , \text{ for } n \geq 2$$ The initial conditions $x_0$ and $x_1$ are given, and can be assumed to be non-zero.","['recurrence-relations', 'sequences-and-series']"
2374503,Do the Fibonacci numbers contain any run of digits?,"Related to this question and inspired by this challenge on PPCG . The challenge is as follows: for a given natural number $x \in \mathbb{N}$, find the first Fibonacci number $F_n$ that contains $x$. With containing I mean that the run of digits of the base-10 representation of $x$ is present in the base-10 representation of $F_n$. For example: for $x = 32$, the first Fibonacci number containing $32$ is the $30^{th}$: $832040$. The question is, is there a solution for any $x$?","['fibonacci-numbers', 'sequences-and-series', 'elementary-number-theory']"
2374531,Integral of $\ln f(x)$,"I know that $\displaystyle\int\ln(f(x))dx=x\ln(f(x))-\int\frac{xf'(x)}{f(x)}dx$ which seemingly cannot be further compacted (i.e. it requires to know $f$ ). In my problem $f$ is (pretty well form) as follows: $$f(x)=c+d(1-\frac{\cosh\frac{x}{a}}{\cosh\frac{b}{2a}})$$ for $\displaystyle-\frac{b}{2}<x<\frac{b}{2}$ . I'm struggling to obtain a compact solution for $$\int_{-\frac{b}{2}}^{\frac{b}{2}}\ln(f(x))dx$$ In fact, the second term ( $\displaystyle\int_{-\frac{b}{2}}^{\frac{b}{2}}\frac{xf'(x)}{f(x)}dx$ ) ends up being so ugly. Any effort on finding a compact form would be highly appreciated. PS While one may use Euler’s representation or Taylor expansion, it is more desirable to have the final answer in a compact hyperbolic form. Any clever idea for approximating the integral is also very welcome (rough ranges: $c=350, d=5, b=1e-4, a=4e-6$ ). I'm not sure if it helps but FYI: $f$ is obtained by solving $f''-\frac{(f-c)}{a^2}=-\frac{d}{a^2}$ and compacting the solution nicely.","['derivatives', 'taylor-expansion', 'logarithms', 'integration', 'approximation']"
2374536,Pointwise convergence of a sequence of polynomials,"Consider the identity function $f(x) = x$ and let $\{h_n;n \in \mathbb{N}\}$ be a sequence of polynomials, which are defined on $[0,a]$ with some fixed $a<1$, and are of the form $h_n(x) = \sum_{i=1}^n c_{i,n} x^i$ where $c_{i,n}$ is the $i$th coefficient in the $n$th polynomial $h_n$. Further assume that: (1) $0 \leq h_n(x) \leq x$ and $h_n(0) = 0$ for every $n$; (2) each $h_n$ is monotonically increasing; (3) $\lim_{n \to \infty} h_n(x) = x$ uniformly for all $x \in [0,a]$. From (1) we know that $c_{1,n} = h_n'(0) \leq x'|_{x=0} = 1$ for every $n$. My question is: from conditions (1), (2) and (3), is it necessarily true that $\lim_{n \to \infty} c_{1,n} = 1$? If I impose the extra condition $\sup_{n \in \mathbb{N}} \{\sum_{i=2}^n |c_{i,n}|\} < M$ for some absolute constant $M > 0$, then the claim holds. But this is a too stringent condition. But I am not sure if the claim is unconditionally true. If not, what could be a minimal set of conditions that I need to impose? Thanks very much.","['real-analysis', 'polynomials', 'calculus', 'measure-theory', 'convergence-divergence']"
2374620,On the convergence of $\sum_{n = 1}^\infty\frac{\sin\left(n^a\right)}{n^b}$,"Given the infinite series $$\begin{aligned}\sum_{n = 1}^{\infty}\end{aligned} \frac{\sin\left(n^a\right)}{n^b}$$ with $a,\,b \in \mathbb{R}$ , study when it converges and when it diverges. Easy cases $\forall\,a \in \mathbb{R}$ we have $\left|\frac{\sin\left(n^a\right)}{n^b}\right| \le \frac{1}{n^b}$ so the series $\color{green}{\text{converges}}$ for $b > 1$ . If $a \le 0$ we have $\frac{\sin\left(n^a\right)}{n^b} \le \frac{1}{n^{b-a}}$ so the series $\color{blue}{\text{diverges}}$ for $b \le a + 1$ and $\color{green}{\text{converges}}$ for $b > a + 1$ . If $a > 0 \, \land \, b \le 0$ we have $\not\exists \begin{aligned}\lim_{n \to \infty} \frac{\sin\left(n^a\right)}{n^b} \end{aligned}$ so the series $\color{blue}{\text{diverges}}$ . If $a = 1\, \land \, b > 0$ the series $\color{green}{\text{converges}}$ by Abel-Dirichlet's test . Hard cases If $0 < a < 1\, \land \, 0 < b \le 1-a$ the series $\color{blue}{\text{diverges}}$ by proof of i707107 . If $0 < a < \frac{1}{2}\, \land \, a < b \le 1-a$ the series $\color{blue}{\text{diverges}}$ by proof of RRL . If $0 < a < 1\, \land \, b > 1-a$ the series $\color{green}{\text{converges}}$ by proof of i707107 . If $a > 0\, \land \, b > \max(a,\,1-a)$ the series $\color{green}{\text{converges}}$ by proof of RRL . If $k \in \mathbb{Z}_{\ge 2}, $ $k-1 < a < k\, \land \, b > 1 - \frac{k-a}{2^k-2}$ the series $\color{green}{\text{converges}}$ by proof of i707107 . If $a > 0 \, \land \, b = 1$ the series $\color{green}{\text{converges}}$ by proof David Speyer (+ i707107 in the comments). If $a = 2 \, \land \, 0 < b \le \frac{1}{2}$ the series $\color{blue}{\text{diverges}}$ ( Theorem 2.30 by Hardy&Littlewood ). $\color{red}{\textbf{Open cases}}$ $a = \frac{3}{2} \land b = \frac{1}{4}$ : $a = \frac{3}{2} \land b = \frac{1}{2}$ : $a = \frac{3}{2} \land b = \frac{3}{4}$ : $a = 2 \land b = \frac{3}{4}$ : $a = \frac{5}{2} \land b = \frac{1}{2}$ :","['convergence-divergence', 'divergent-series', 'sequences-and-series', 'calculus']"
2374639,Is there any educational value into reading the original work of the authors who discovered certain theorems or concepts?,"I am interested in reading original work of some authors of theorems or concepts in mathematics because I believe that there is also an educational value to this and it might help me understand better those concepts because I can see where and how they came from from the author's minds. For example I would be really curious into reading the work of Leibnitz and Newton regarding calculus, differentials, and integrals. Do you think it could help me better understand those concepts? If yes, can anyone help me to find those original works?","['derivatives', 'math-history', 'calculus', 'analysis']"
2374645,How many points distant from each other can fall in a ball?,"Let's say we have a closed ball  $B(x,r)\subset\mathcal{R}^n$ centered at $x$ with radius $r$. The question I want to ask is: how many distinct points in $\mathcal{R}^n$, in which the distance between any two points is larger than or equal to $r$, can fall in $B(x,r)$? When $n=2$, the question is easy to visualise. But to imagine the case generally when $n$ is large is not so easy. I think the points must be located on the boundary of $B(x,r)$ to maximise the number. Therefore it is possible to derive an upper bound by estimating the area each point would occupy on the boundary of $B(x,r)$. But the computation gets complicated because the area occupied by several points depends on the location of these points. Thanks!",['geometry']
2374685,Functions that are always less than their derivatives,"I was wondering if there are functions for which $$f'(x) > f(x)$$ for all $x$. Only examples I could think of were $e^x - c$ and simply $- c$ in which $c > 0$. Also, is there any significance in a function that is always less than its derivative? Edit: Thank you very much for all the replies. It seems almost all functions that apply are exponential by nature... 
Are there more examples like - 1/x? Again are there any applications/physical manifestations of these functions? [for example an object with a velocity that is always greater than its position/acceleration  is always greater than its velocity]","['derivatives', 'inequality', 'calculus', 'integration', 'ordinary-differential-equations']"
2374714,Is there a solution for $\sum_{k=1}^{n}\cos(kx) = a$?,"I am trying to predict the phase error in a chip based on its DFT components. Therefore it would be amazing if there is a specific solution of x for the following equation:
$$
\sum_{k=1}^{n}\cos(kx) = a
$$
For the domain of x such that there is a unique mapping from the LHS to the RHS. In the general case this condition is for sure satisfied in the domain of $x = [0, \frac{\pi}{n}]$. However this domain could be larger. For example in the case of n = 3, WolframAlpha told me that there is a unique mapping between LHS and RHS for the domain of $x = [0, \sim 0.4\pi]$. The values of $a$ are only valid in the range $a = [n, ?)$, but I don't think it is of that much importance to know the exact lower bound of $a$ at this time. I tried to solve this equation by looking at a specific case of this general equation, which can be found in this question on StackExchange . The given solution is valid since the identity $\cos(a)\sin(b)=\frac{1}{2}(\sin(a+b)−\sin(a−b))$, which results in a factor of $a$ (namely $\frac{1}{2}$) on the LHS such that the $a\sin{\frac{x}{2}}$ on both sides of the equal sign cancel. In the general case, this is of course not the case since $|a|$ is not by definition equal to $\frac{1}{2}$. Therefore: Does anyone know if this equation could have a specific solution (and why or why no)? If the answer to 1. is Yes: What will be the solution to this equation? Also the solution to a more specific case, like n = 3 or 4 (or preferable 8, because I perform an 8 sample DFT) would we very welcome. I am looking forward to some brilliant ideas :)",['trigonometry']
2374718,Matrix differentiation: Combination of vectors and matrices,"I want to differentiate: $f(w) = w^TF^TFw - w^TF^Tt- t^TFw$ with respect to w. F is a $n*d$ matrix, w is a $d*1$ vector, y is a $n*1$ vector. I read sometimes that $(w^TF^Tt)'$ = $(F^Tt)^T$, and sometimes that it is  $(F^Tt)$ - why is that? Furthermore, I know that generally $(w^TAw)' = w^T(A+A^T)$. Should it not follow that $w^TF^TFw = w^T(F^TF+F^TF)$?","['matrices', 'matrix-calculus', 'linear-algebra', 'derivatives']"
2374765,is there a formula for this binomial sum?,"Are there any formulas to calculate the below sum? $$\sum_{n=1}^{1918}n\binom{2017-n}{99}$$ Or, more generally, $$\sum_{n=1}^{1918}n\binom{2017-n}{k}$$","['combinatorics', 'summation', 'binomial-coefficients']"
2374768,Minimum of the given expression,"For all real numbers $a$ and $b$ find the minimum of the following expression. $$(a-b)^2 + (2-a-b)^2 + (2a-3b)^2$$ I tried expressing the entire expression in terms of a single function of $a$ and $b$. For example, if the entire expression reduces to $(a-2b)^2+(a-2b)+5$ then its minimum can be easily found. But nothing seems to get this expression in such a form, because of the third unsymmetric square. Since there are two variables here we can also not use differentiation. Can you please provide hints on how to solve this?","['optimization', 'algebra-precalculus', 'maxima-minima', 'quadratic-programming', 'linear-algebra']"
2374771,When the set of morphism between two objects is an object in the category?,"In the category of R-Modules, for a given ring R, The set of all morphism between two objects is an object in the category. Obviously this is also true in the category of sets. Are there more categories in which this is true?","['category-theory', 'abstract-algebra']"
2374810,Concentration inequality for covariance,"Is there any concentration inequality for the covaraince of two scalar random variables? For example, how can I found a tight upper bound for the following probability? $$\Pr\left( {\left| {{\mathop{\rm cov}} (x,y) - \overline {{\mathop{\rm cov}} (x,y)} } \right| \le \varepsilon } \right)$$ where $\overline {{\mathop{\rm cov}} (x,y)}  = E\left( {x - E(x)} \right)E\left( {y - E(y)} \right)$ is the actual covariance and ${\mathop{\rm cov}} (x,y) = \frac{1}{N}\sum\limits_{n = 1}^N {\left( {{x_i} - \hat x} \right)\left( {{y_i} - \hat y} \right)} $ is the sample covariance.","['concentration-of-measure', 'probability-theory', 'covariance', 'probability', 'random-variables']"
2374898,Does eigenvectors of a matrix change during matrix operations?,"If I have a matrix $A$ with two eigenvectors $x$ and $y$. 
What will be the eigenvectors of $$A^2 - 3A + 4I ?$$ I know that if we take powers of $A$ then the eigenvectors remain unchanged. But I am not quite sure about the above matrix .","['matrices', 'eigenvalues-eigenvectors', 'linear-algebra']"
2374902,How to find stability of fixed point of a PDE?,"I have a system of coupled PDEs which have both time and spatial dependence with the form: \begin{align*} a_t &= D_aa_{xx}+f(a, b) \\ b_t &= D_bb_{xx}+g(a, b) \end{align*} where $f(a, b)$ and $g(a, b)$ are linear functions in terms of $a$ and $b$ and $D_a$ and $D_b$ are diffusion coefficients. When there is no spatial dependence I can easily calculate the fixed points, find the Jacobian and determine the stability of the fixed points. My question is then, how can I generalise this method when I have spatial dependence too? I was told I could Fourier Transform the system and turn it into a set of ODEs so that I have \begin{align*} \tilde{a}_t &= -k^2D_a\tilde{a}+f(\tilde{a}, \tilde{b}) \\ \tilde{b}_t &= -k^2D_b\tilde{b}+g(\tilde{a}, \tilde{b}) \end{align*} and find the Jacobian of this. However, I struggle to see how finding the Jacobian of the Fourier transformed equations could be related to the stability of my system? I was also told that the Jacobian of my FT equations is equal to the Jacobian of my non transformed system though I can't figure out how. Also: $$\tilde{a}(k, t) = \int a(x, t)e^{ikx}\,\mathrm{d}x$$","['fixed-points', 'ordinary-differential-equations', 'nonlinear-system', 'partial-differential-equations']"
2374910,complex-valued continuous function on $\mathbb{D}$.,"Would anyone mind providing a hint for the following exercise: Assume $f$ is continuous on the unit disk $\mathbb{D}$  and $\text{Re}(\overline{z}f(z)) > 0$ for all $|z| = 1$. Show that $f(z) = 0$ for some $z$ in the disk. I attempted using convexity of the disk, but this didn't lead me very far.","['complex-analysis', 'analysis']"
2374918,Local ring with finite maximal ideal is finite,"Let $(R,  m)$ be a commutative local ring which is not a field such that $m$ is finite. Then is it true that $R$ is finite ? I can see that $R$ has finitely many ideals and all proper ideals are finite; so in particular $R$ is Artinian. Moreover $m=R\setminus U(R)$ is finite where $U(R)$ denotes the group of units of $R$ . To show $R$ is finite it would be enough to show either $U(R)$ is finite or that $R/m$ is finite. But I am unable to conclude either. Is the claim at all true ? Please help. Thanks in advance.","['abstract-algebra', 'maximal-and-prime-ideals', 'commutative-algebra', 'local-rings', 'ideals']"
2374936,Questions about Spivak's proof that $\sqrt{2} + \sqrt[3]{2}$ is irrational,"I'm aware that there are already two questions about this specific proof (namely this one and this one ), but I'm specifically puzzled by Spivak's approach in his Answer Book . The basic idea is clear enough: one needs to find a polynomial expression of the form $x^6 + a_5 x^5 + a_4 x^4 + a_3 x^3 + a_2 x^2 + a_1 x + a_0$ (with $a_0, \dots, a_5$ integers) with $\sqrt{2} + \sqrt[3]{2}$ as a root; then, by Gauss's lemma, since $\sqrt{2} + \sqrt[3]{2}$ is not an integer, it is not rational either. The question thus reduces to finding the appropriate $a_0, \dots, a_5$. And here is where I got confused. Spivak's idea is the following: let $x = \sqrt{2} + \sqrt[3]{2}$ and $n = 2^{\frac{1}{6}}$. We can then write the first powers of $x$ in terms of $n$ as follows: $x^0 = n^0$ $x^1 = n^2 + n^3$ $x^2 = 2n^0 + n^4 + 2n^5$ $x^3 = 2n^0 + 6n + 6n^2 + 2n^3$ $x^4 = 4n^0 + 2n^2 + 8n^3 + 12n^4 + 8n^5$ $x^5 = 40n^0 + 40n + 20n^2 + 4n^3 + 2n^4 + 10n^5$ $x^6 = 12n^0 + 24n + 60n^2 + 80n^3 + 60n^4 + 24n^5$ This gives us the following table: $\begin{array}{l | c | r}
    & n^0 & n^1 & n^2 & n^3 & n^4 & n^5 \\
x^0 & 1   & 0   & 0   & 0   & 0   & 0   \\
x^1 & 0   & 0   & 1   & 1   & 0   & 0   \\
x^2 & 2   & 0   & 0   & 0   & 1   & 2   \\
x^3 & 2   & 6   & 6   & 2   & 0   & 0   \\
x^4 & 4   & 0   & 2   & 8   & 12  & 8   \\
x^5 & 40  & 40  & 20  & 4   & 2   & 10  \\
x^6 & 12  & 24  & 60  & 80  & 60  & 24
\end{array}$ (Note that there's a missing $4$ in the second column of Spivak's own table on p. 14, with a corresponding missing term in the first equation below) This is the part I don't really understand. Spivak then claims that the integer coefficients $a_0, \dots, a_5$ are the integers satisfying the system of linear equations generated by the columns of the above table, i.e. satisfying: $a_0 + 2a_2 + 2a_3 + 4a_4 + 40a_5 + 12 =0$ $6a_3 + 40a_5 + 24=0$ $a_1 + 6a_3 + 2a_4 + 20a_5 + 60 = 0$ $a_1 + 2a_3 + 8a_4 + 4a_5 + 80 = 0$ $a_2 + 12a_4 +2a_5 + 60 =0$ $2a_2 + 8a_4 + 10a_5 + 24 = 0$ I checked and indeed the solution to this system of equations indeed gives us the right answer. But I don't understand why . That is, I don't understand (a) what is the heuristics behind the solution, i.e. how he came up with this idea and, more importantly, (b) why taking the columns in the above table as coefficients in a system of linear equations gives us the correct solution. If someone could explain this to me, I would be immensely grateful!","['polynomials', 'linear-algebra', 'irrational-numbers']"
2374943,Conditional expectation and stopping times,"I think I found a way to prove a result but I have doubts about it. I may have made a mistake. ($X$ is a geometric Brownian motion and I do not want to use the strong Markov property) I have proved that for any $t \in [0,\infty)$ 
$$
\mathbb E\bigg[\int_t^\infty G(X_u)~\bigg|~F_t\bigg]= U_t \tag 1
$$
for a certain function $G$ such that it is well defined and a certain process $U$. Can I conclude that for any finite stopping time $\tau$:
$$
\mathbb E\bigg[\int_\tau^\infty G(X_u)~\big|~F_\tau] 
= U_\tau \text{ ?}
$$
I think it is true since for any $\omega \in \Omega$ and (I am not sure about this part because it looks too obvious) we get: \begin{align}
\mathbb E\bigg[\int_\tau^\infty G(X_u)~\bigg|~F_\tau\bigg](\omega)&= \mathbb E\bigg[\int_{\tau(\omega)}^\infty G(X_u)~\bigg|~F_{\tau(\omega)}\bigg](\omega) \tag 2 \\ &= U_{\tau(\omega)}(\omega)
\end{align}
since $\tau$ is finite and by $(1)$. But is $(2)$ true and do you know how to prove it? I read on one forum that it is true and it is called ""local property of conditional expectation"" but I did not find it anywhere else. Thank you in advance for your help. Edit: Does anyone know? It is important because I proved a result using this property and my proof will be wrong if this property is not correct.","['stochastic-processes', 'conditional-expectation', 'stopping-times', 'probability', 'brownian-motion']"
2374962,Two topologies are equal if they have the same filter convergence,"A major drawback with sequential convergence in topological spaces is that two different topologies can have the same convergent sequences e.g. the discrete and cofinite topologies on $\mathbb{R}$. Filters are meant to be better convergent structures in topological spaces, which leads to my question: If $\tau_1$ and $\tau_2$ are two topologies on a set $X$ with the same ultrafilter convergence i.e. an ultrafilter $\mathcal{F}\rightarrow x $ in $\tau_1 \iff \mathcal{F}\rightarrow x $ in $\tau_2$, then is it true that $\tau_1 = \tau_2$?","['general-topology', 'filters']"
2374970,What is the maximal size of a subset of consecutive integers such that they are all coprime?,"Suppose that $a$,$b$, and $n$ are positive integers such that $a = b - n$. In terms of $n$, what is the largest number of integers $z_1,z_2,z_3,\cdots, z_m$ all between $a$ and $b$ such that all $z$ are coprime to each other? I feel like this is something I should be able to find online somewhere, but I couldn't find it. I'm not looking for a proof so much as just looking for the current proven metric. I know it cannot be more than roughly $\frac n2$.","['number-theory', 'prime-numbers', 'elementary-number-theory']"
2374977,A question about integers of the form $\frac{x}{y}+\frac{y}{z}+\frac{z}{x}$,"This post ( Find all possible values of $x+y+z$. ) got me curious.
Consider this problem: 
Let $x,y,z \in \mathbb{N}$ such that $\frac{x}{y}+\frac{y}{z}+\frac{z}{x}=t$ is an integer. Which integers $t$ are expressible in this way? (Are there infintely many when $\gcd(x,y,z)=1$?) Edit :
Conjecture: If $b=\gcd(x,y),c=\gcd(y,z),a = \gcd(x,z)$ then
$(x,y,z) = (a^2b,b^2c,c^2a)$ and then $xyz=(abc)^3$ is always a cube, and
$x/y+y/z+z/x = \frac{a^3+b^3+c^3} {abc}$ Proof: There exist $k,l,m$ such that $x = bak, y=bcl,z=acm$.
From this it follows that (by plugging in $x,y,z$ in $x/y+y/z+z/x=t$ and rearranging: $a \cdot m(tbckl-abk^2-c^2lm) = k \cdot b^2cl^2$ (1) Since $\gcd(ak,cl)= \gcd(bk,cm) = \gcd(bl,am)=1$
It follows by the equation (1) that $k|a$ and $a|k$ hence $a=k$.
Similarily we get: $l=b,m=c$ and the conjecture is proved.
Hence:
$(x,y,z) = (a^2b,b^2c,c^2a)$ Also, I found the OEIS sequence: http://oeis.org/A072716",['number-theory']
2374978,Solve the differential equation $x^2\frac{d^2y}{dx^2}+x\frac{dy}{dx}+(x-1)y=0$,"$$x^2\frac{d^2y}{dx^2}+x\frac{dy}{dx}+(x-1)y=0$$ $$\sum_{n=0}^{\infty}(n+r)(n+r-1)c_nx^{n+r}+\sum_{n=0}^{\infty}(n+r)c_nx^{n+r}+\sum_{n=1}^{\infty}c_{n-1}x^{n+r}-\sum_{n=0}^{\infty}c_nx^{n+r}$$ The exponents of the equation is then, $$r^2-1=0$$ $$r_1=1,r_2=-1$$ The first solution is given by $$c_n=\frac{c_{n-1}}{1-(n+1)^2}$$ $$c_1=-\frac{c_0}{3}$$ $$c_2=-\frac{c_1}{8}$$ $$c_3=-\frac{c_2}{15}$$ $$y_1(x)=1+2\sum_{n=1}^{\infty}\frac{(-1)^n(x^n)}{(n!)(n+2)!}$$ I can't seem to find the second solution, $$r=r_2=-1$$ The recurrence formula is given by $$c_n=\frac{c_{n-1}}{1-(n-1)^2},n\neq 2$$ We then have linearly independent solution For $n>2$ $$c_3=-\frac{c_2}{3}$$ $$c_4=-\frac{c_3}{8}$$ $$c_5=-\frac{c_4}{15}$$ $$c_6=-\frac{c_5}{24}$$ I notice that the two are the same. How shall I find the linearly independent $y_2$ with log in it. Any help would be appreciated.",['ordinary-differential-equations']
2374991,Why is $\lfloor n/3 \rfloor + \lceil 2n/3\rceil = n?$,Why is $\lfloor n/3 \rfloor + \lceil 2n/3\rceil = n?$ It feels like I'm just doing basic fraction addition since $$\frac{n}{3}+\frac{2n}{3}=n$$ So then how does ceil and floor play a role in addition?,"['algebra-precalculus', 'ceiling-and-floor-functions']"
2374995,The limit of ideals / schemes (Eisenbud and Harris),"I am reading Geometry of Schemes By Eisenbud and Harris. In section II.3.1 They talk about schemes that are affine varieties, except that they are nonreduced. They say that schemes with double points can arise as limits of reduced schemes. In the book an explanation of the meaning of the limit they are talking about is given, but I don't seem to have the right knowledge to understand what is ment. Below I will give the specefic example I was reading. Let $a(t),b(t) \in k[t]$ by polynomials such that $a(0) = b(0) = 0$, and define the following ring and affine scheme: $$
S_t
\quad :=  \quad
{ 
k[x,y]
\over
(x,y)(x-a(t),y-b(t))
}
$$
and
$$
X_t
\quad := \quad
\text{Spec}(S_t) 
\ = \ 
\{(0,0),(a(t),b(t))\} 
\ \subseteq \
\mathbb{A}_k^2.
$$
Now the limit is defined by defining the limit of $(x-a(t), y-b(t))$, but I don't know what this means. Should one think of a limit as in category theory? This is what the book says about it: Of course, this only shifts the burden to describing what is the limit of a
  family of ideals! But this is easy: in the current case, for example, we can
  take their limit as codimension-2 subspaces of $K[x, y]$, viewed as a vector
  space over $K$. That this limit is again an ideal follows from the continuity
  of multiplication. I don't know what the limit of vector spaces is. I could imagine how we can take the limit of this using category theory, but I don't know how to interpret it as an ideal again, it would just be $k^2$. I also could imagine that if $k$ is a metric space we could make some sort of metric on linear spaces of a fixed ambient space as well, but for many $k$ that wouldn't work. Of course I also just googled to find out what it could mean, but I didn't find anything. I think the rest of II.3.1 just explains what this limit means intuitively, so that wouldn't really answer my question. So what is this limit? please tell me.","['algebraic-geometry', 'reference-request', 'affine-schemes', 'limits-colimits', 'definition']"
2375956,Lifting a convergent net through a quotient map,"Throughout, $\pi : X \to Y$ will denote a topological quotient map. It is sometimes desirable to understand the topology of $Y$ in terms of nets. At first, one might guess the following holds Claim 1: If $y_i \to y$ in $Y$, then we can write $y_i = \pi(x_i)$, $y = \pi(x)$ such that $x_i \to x$ in $X$. but easy examples show this is false, even if $X$ and $Y$ are very simple spaces. Example 1: Let $X = [0,1]$ and $Y=S^1$ with $\pi$ the map which glues the endpoints together. Let $(a_n)$ and $(b_n)$ be sequences in $(0,1)$ converging to $0$ and $1$ respectively. Then, $\pi(a_1), \pi(b_1), \pi(a_2), \pi(b_2), \pi(a_3), \ldots$ is a convergent sequence in $Y$ whose unique lift to $X$ does not converge. In the above example, $a_1,b_1,a_2,b_2,a_3, \ldots$ does, however, have convergent subsequences. So, one might guess the following is true. Claim 2: If $y_i \to y$ in $Y$, then, for some subnet $y_{i_k}$ of $y_i$, we can write $y_{i_k} =\pi(x_k)$, $y=\pi(x)$ such that $x_k \to x$ in $X$. This claim is also false, though it took me a while to think of a counterexample. Example 2: Let $X = \mathbb{R}$, in the standard topology. Define an equivalence relation $\sim$ on $X$ such that: The integers $\mathbb{Z}$ are an equivalence class. Each open interval $(n,n+1)$, $n \in \mathbb{Z}$ is an equivalence class. Let $Y = X / \sim$ in the quotient topology. The topology on $Y$ is as follows: Each $y_n = (n,n+1)$ is an open point of $Y$. $y_\infty = \mathbb{Z}$ has all of $Y$ as its only open neighbourhood. Then, $y_1,y_2,y_3,\ldots$ converges to $y_\infty$ in $Y$, but cannot be lifted to a convergent sequence in $X$. I am unsatisfied with this example, however, because $Y$ is a not even a Hausdorff space. Question: Can we violate Claim 2 using nicer spaces $X$ and $Y$? Can they be Hausdorff? Compact Hausdorff? Compact metrizeable? In the positive direction, we do have that Claim 2 holds when the quotient map is open. Proposition: Suppose the quotient map $\pi : X \to Y$ is open. Let $y_i \to y$ in $Y$. Then, for any lift $x$ of $y$ there is a subnet $y_{i_k}$ of $y_i$ and lifts $x_k$ of $y_{i_k}$ such that such that $x_k \to x$. Proof: Let $\mathscr{U}_x$ be the collection of all open sets $U \subseteq X$ with $x \in U$. Let $K = I \times \mathscr{U}_x$, where $I$ is the index set of $y_i$. Define $k \mapsto i_k : K \to I$ and $k \mapsto x_k : K \to X$ as follows. For $k=(i,u)$: Choose $i_k$ so that $i_k \geq i$ and $y_{i_k} \in \pi(U)$. Choose $x_k$ so that $x_k \in U$ and $\pi(x_k) = y_{i_k}$. It is simple to see that $y_{i_k}$ is a subnet of $y_i$ and $x_k \to x$. So, in any answer to my question, $\pi : X \to Y$ cannot be an open map.","['general-topology', 'examples-counterexamples', 'nets', 'quotient-spaces']"
2376006,Is this induced topology the product topology?,"Consider the pairs $(A_i,a_i)$ where $A_i$ is a set and $a_i: U(A_i)\rightarrow A_i$ is a relation where $U(A_i)$ is denotes the set of ultrafilters on $A_i$. Now consider the topological spaces $(A_i,\tau_i)$ where $\tau_i$ is the topology generated as follow: $U\in \tau_i \iff$ for every $x\in U$ whenever $(\mathcal{F},x)\in a_i
$ we have $U\in \mathcal{F}$ (here $\mathcal{F}$ is an ultrafilter
  on $A_i$). $\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
 \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
 \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \
 \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (*)$ Now consider the pair $(\Pi_{i\in I}A_i, a_p)$ where $a_p: U(\Pi_{i\in I}A_i)\rightarrow \Pi_{i\in I}A_i$ is a relation between ultrafilters on the product and the product itself (as before). Suppose that the relation $a_p$ is described as follows: $$\big(\mathcal{F},(x_i)_{i\in I}\big)\in a_p \iff ({\pi_i}_*(\mathcal{F}),x_i) \in a_i \text{ for every } i\in I$$ Here ${\pi_i}_*(\mathcal{F}) = \{X\subseteq A_i: \pi_i^{-1}(X)\in\mathcal{F}\} $ as usual where $\mathcal{F}$ is an ultrafilter on the product. Question: is the topology generated by $a_p$ on $\Pi_{i\in I}A_i$ (by using the characterization of open sets in $(*)$ in the orange block) the product topology? Background : the reason I suspect this is that if $\tau$ is the product topology on $\Pi_{i\in I}A_i$, because ultrafilter convergence in the topological sense satisfies the same relation as $a_p$.","['general-topology', 'filters']"
2376016,What does the set notation $A\setminus B$ mean?,If $A$ and $B$ are two sets. What does the notation $A\setminus B$ denote for? I found this notation in the inclusion and exclusion principle.,"['notation', 'elementary-set-theory']"
2376054,Satisfying the following determinant inequality,"I would like to find least restrictive conditions on $W = W^T \succ 0, \ V = V^T \succ 0$ (which are $\mathbb{R}^{n \times n}$ positive definite matrices) such that the following inequality is satisfied: $$ \text{det} \bigg( W^{-1} \Gamma W^{-1} + A^T V A \bigg) \geq 1  \tag{*}  $$ where $\Gamma = \Gamma^T \triangleq W - B P B^T \in \mathbb{R}^{n \times n} $, but is not necessarily a positive definite matrix, with $P = P^T \succ 0$ being a $\mathbb{R}^{n \times n}$ positive definite matrix. Further, the matrices $A, B \in \mathbb{R}^{n \times n}$ are arbitrary. I was hoping that (*) can be simplified by using appropriate determinant inequalities, for example using Minkowski's determinant inequality (here: https://mathoverflow.net/questions/251646/reverse-minkowski-and-related-determinant-inequalities ), but this requires that $\Gamma$ be positive definite--though it is symmetric $\Gamma = \Gamma^T$--so not sure. Further, the second term would need to be positive definite as well. I would not want to impose conditions on $A,B$ though.","['matrices', 'normed-spaces', 'linear-algebra', 'determinant']"
2376077,Morphisms of $\textbf{Set}$ are functions not general relations?,"In the category $\textbf{Set}$, where objects are sets and morphisms are functions between sets, why can the morphisms not be any arbitrary relation? The morphisms have to preserve the structure of the objects, but what structure do sets have that functions can preserve but other relations can't? Is it anything to do with there being an infinite number of relations between any two sets while between finite sets there are only a finite number of distinct functions? If it makes it simpler to not have to consider infinite sets, the same questions still hold for the category of finite sets $\textbf{Set}_{fin}$, where the morphisms are still specificially those relations which are also functions.","['elementary-set-theory', 'category-theory', 'relations', 'functions']"
2376094,"Finding the minimum value of $\cot^2A + \cot^2B+ \cot^2C$ where $A$, $B$ and $C$ are angles of a triangle.","The question is: If $A+B+C= \pi$, where $A>0$, $B>0$, $C>0$, then find the minimum value of $$\cot^2A+\cot^2B +\cot^2C.$$ My solution: $(\cot A + \cot B + \cot C)^2\ge0$   //  square of a real number $\implies \cot^2A +\cot^2B + \cot^2 +2 \ge0   $  //Conditional identity used: $\cot A \cot B + \cot B \cot C + \cot A \cot C =1$ $\implies \cot^2A +\cot^2B + \cot^2 C \ge -2$ Thus according to me the answer should be $-2$. However, the answer key states that the answer is $1$. Where have I gone wrong?","['inequality', 'trigonometry', 'algebra-precalculus', 'maxima-minima', 'triangles']"
2376117,Finding the general Taylor Series of a function,"I have to find the general Taylor Series Expansion, about $0$, of the following function. $$ \sqrt{x^4 -6x^2+1} $$ I have tried to use the identity $$ \left(1+t\right)^{1/2} = \sum_{n\ge 0}\frac{(-1)^{n+1}}{4^n (2n-1)} \binom{2n}{n}t^n $$. and then substitute for $t$ accordingly, but to no avail, since the resulting expression becomes messy. Any help will be appreciated. Thanks.","['taylor-expansion', 'calculus']"
2376163,"Simple algebra of sets ""proof""",I just wanted to ask about this equality $A\cap B\cap C = (A\cap B)\cap (A\cap C)$ Can this easily be proven by using the associative property of sets which states? $(A\cap B)\cap (A\cap C) = (A\cap A)\cap (B\cap C) = A\cap (B\cap C) = A\cap B\cap C$ Or is this not a valid mathematical proof,"['elementary-set-theory', 'proof-verification']"
2376164,First and second derivative or Moore's Law,"I am tasked with taking the first and second derivative of a polynomial function. I chose Moore's law which states that every two years, the amount of transistors double. The equation for this law is: $$
P_n = P_0\times2^{1/2n}
$$ $P_n$ is the computer processing power n years after the current year. $P_0$ is the computer processing power in the current year. $n$ is the number of years after the current year. My question is to take the derivative of this do I drop the $P_n$ because it is a constant? Or is this a matter of using Leibniz's notation vs Lagrange's notation? Should I be using implicit differentiation because I have more than one variable? I know the rules of differentiation I am just not sure of the format because I am used to seeing equations like $f(x)= ...$ Thank you for your help.","['derivatives', 'calculus']"
2376212,Interpreting Combinations for Four of a Kind,"To calculate the number of ways to make a four of a kind in a five card poker hand, one could reason as follows. There are 13 values you can select for the four of a kind: ${13 \choose 1}$ The fifth can be any of the 52 - 4 remaining cards: ${52 - 4 \choose 1}$ So the total number of combinations is simply ${13 \choose 1}{48 \choose 1} = 13 \times 48 = 624$ I've also seen this equivalent calculation, but I'm not sure how to interpret it: ${13 \choose 1}{12 \choose 1}{4 \choose 1} = 624$","['combinations', 'combinatorics']"
2376245,Derive expression for the gradient of an arbitrary scalar function,"I'm trying to do the following problem but I have no idea how to start and was wondering if someone could point me in the right direction? Start from the expression for the metric (the square of the infinitesimal
line element) in an arbitrary orthogonal curvilinear 3-dimensional coor-
dinate system, and obtain an expression for the gradient, grad$\phi$, of an arbitrary  differentiable  scalar  function $\phi$. The square of the infinitesimal line element I think is: $(ds)^2=h_1^2(dx_1)^2+h_2^2(dx_2)^2+h_3^2(dx_3)^2$ Any help would be greatly appreciated!","['differential', 'coordinate-systems', 'scalar-fields', 'multivariable-calculus', 'differential-geometry']"
2376257,Converse of the fundamental theorem of Riemannian geometry?,"The fundamental theorem of Riemannian geometry says that for a manifold with a given metric, there is a unique torsion-free connection. Suppose instead that we are given a connection. According to answers to this question , a metric exists that induces that connection.  How much non-uniqueness is there in the metric? It seems that there is at least an ambiguity up to a constant factor, because if the connection is metric-compatible with the metric $g$ (i.e., $\nabla g=0$), then it's also compatible with $cg$. Does any of this change in the semi-Riemannian case?",['differential-geometry']
2376262,How to relate the solutions to a Fuchsian type differential equation to the solutions to the hypergeometric differential equation?,"Consider a Fuchsian type differential equation written as
$$\frac{d^2 y}{dz^2} + \frac{p(z)}{(z - z_1)(z - z_2) \cdots (z - z_m)} \frac{dy}{dz} + \frac{q(z)}{(z - z_1)^2 (z - z_2)^2 \cdots (z - z_m)^2} y = 0, \quad m \geq 2,$$
where $z_1, z_2, \ldots, z_m$ are distinct regular singular points, and $p(z)$ and $q(z)$ are polynomials. How does one relate the solutions to this differential equation to the solutions to the hypergeometric differential equation
$$z(1 - z) \frac{d^2 y}{dz^2} + [c - (a + b + 1)z] \frac{dy}{dz} - aby = 0$$
around each regular singular points? The case $m = 2$ essentially reduces to Riemann's differential equation (if $z = \infty$ is a regular singular point) whose solutions can be written in terms of the hypergeometric functions. Can this be done in general, or for at least four or five regular singular points? Update : From what I have gathered so far, all homogeneous linear differential equations of the second order having four regular singularities in the extended complex plane, can be transformed into Heun's differential equation whose solutions are related to the hypergeometric functions.","['complex-analysis', 'hypergeometric-function', 'special-functions', 'ordinary-differential-equations', 'power-series']"
2376264,Find the inverse laplace transform of complicated rational function,"Find the inverse laplace $$\mathcal{L}^{-1}\left\{ \frac{2s + 1}{2s^2 + s + 2} \right\}$$ I don't see a simple way to take the inverse laplace transform, convolution theorem?","['ordinary-differential-equations', 'laplace-transform']"
