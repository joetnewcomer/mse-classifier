question_id,title,body,tags
3901230,How to solve this equation with matrices,"can you please give me some hints to solve the following? I really don't know how to start. $$X^2= \begin{pmatrix}
6 & 2 \\ 3 & 7
\end{pmatrix}.$$ I tried to express this matrix as $4\cdot I + \begin{pmatrix}
2 & 2 \\ 3 & 3
\end{pmatrix}$ And somehow solve it, but I really have no clue. Please some help.","['matrices', 'matrix-equations', 'linear-algebra']"
3901239,"Find out functions of the form $g(x,y) = \int f(x,t) f(y,t) \lambda(dt)$","I am interested in the following question. Given a symmetric function $g: \mathbb R^n \times \mathbb R^n \rightarrow \mathbb R$ or $\mathbb R_{+}^{n}\times \mathbb R_{+}^{n} \rightarrow 0$ .  I am interested in finding out whether $g$ can be written as the following form: $$g(x,y) = \int f(x,t) f(y,t) \lambda(dt),$$ where $\lambda$ is some measure but not necessarily the standard Lebesgue measure. For example, $g(x,y) = \min\{|x|,|y|\}$ can be written as the above form for $f(x,t) = \mathbb I(0<t<|x|)$ . $g(x,y) = \frac{1}{|x|+|y|}$ can also be written as above for $f(x,t) = e^{-|x|t}\mathbb I(t>0)$ . I am wondering if there is any necessary or sufficient condition to describe the set of functions which satisfies the above assumption. One necessary condition is $g(x,y)$ needs to be positive symmetric definite. Thanks a lot!!","['symmetric-functions', 'functional-analysis', 'probability', 'real-analysis']"
3901339,solve for the Laurent series of the function $f(z) = \frac{z-12}{z^2+z-6}$ that is valid for $1<|z-1|<4$.,"I am about to solve for the Laurent series of the function $f(z) = \frac{z-12}{z^2+z-6}$ that is valid for $1<|z-1|<4$ . What I did is I use the partial fraction decomposition to rewrite the $f(z)$ into $f(z) = \frac{3}{z+3}+\frac{2}{z-2}$ .  Now based on the region to where the function $f$ should be valid on is that $1< |z-1|< 4$ , I'm not sure if I did it right when I assumed that $\left|\frac{2}{z}\right|<1$ and $\left|\frac{3}{z}\right|<1$ . (I just did it with brute force to arrive on the two inequalities). Can you help me with that? Assuming my two inequalities are correct, I have now $\frac{3}{z+3} = \frac{3/z}{1+3/z} = 3/z \sum_{n=0}^{\infty} \left(- \frac{3}{z}\right)^n = \sum_{n=0}^{\infty} (-1)^{n-1}3^nz^{-n} = \sum_{n=1}^{\infty} (-1)^n3^{n+1}z^{-(n+1)}$ , and $\frac{2}{z-2} = \frac{2/z}{1-2/z} = 2/z \sum_{n=0}^{\infty}  \left(\frac{2}{z}\right)^n = \sum_{n=0}^{\infty} \frac{2^{n+1}}{z^{n+1}}.$ Then after that, I combined which I have the result as $f(z) = \frac{z-12}{z^2+z-6} = \sum_{n=0}^{\infty} \frac{2^{n+1}}{z^{n+1}} + \sum_{n=1}^{\infty} (-1)^n(3)^{n+1}z^{-(n+1)}$ . Now, did I do it correctly? or I did wrong on the inequalities of the region to where the laurent should be valud to?  Thanks for those who can help.","['complex-analysis', 'laurent-series', 'analysis']"
3901351,Definition of $k$-times differentiable on $S$?,"I'm trying to come up with a good definition of being $k$ -times differentiable on a subset $S$ . Here's the definitions I'm working with. Let $f$ be a function from $X\subseteq\mathbb{R}$ to $\mathbb{R}$ . Let $L\in\mathbb{R}$ . If $x_0\in X$ is a limit point of $X$ , we say that $f$ has derivative $L$ at $x_0$ iff $\lim_{x\to x_0}\frac{f(x)-f(x_0)}{x-x_0}=L$ , i.e. iff $$\forall\epsilon>0,\exists\delta>0,0<|x-x_0|<\delta\implies\left\lvert\frac{f(x)-f(x_0)}{x-x_0}-L\right\rvert<\epsilon.$$ If $S$ is a subset of $X$ , we say that $f$ is differentiable on $S$ iff the derivative exists at $x_0$ for all $x_0\in S$ (in particular, every point of $S$ must be a limit point of $X$ ). We say that $f$ is differentiable iff it is differentiable on its whole domain. Are these definitions standard so far? Now the definition I've come upon for "" $k$ -times differentiable"" is as follows. Let $f$ be a function from $X\subseteq\mathbb{R}$ to $\mathbb{R}$ . We say that $f$ is $1$ -times differentiable iff $f$ is differentiable, and the first derivative of $f$ is the function $f^{'}:X\to\mathbb{R}$ , also denoted $f^{(1)}$ , defined in the obvious way. For $k\geq 1$ , we say that $f$ is $(k+1)$ -times differentiable iff $f$ is $k$ -times differentiable and $f^{(k)}$ is differentiable, and in this case the $(k+1)$ th derivative is the function $f^{(k+1)}:= (f^{(k)})': X\to\mathbb{R}$ . A function is said to be infinitely differentiable iff it is $k$ -times differentiable for all $k$ . Seem fine so far? But then I thought, how would I say that a function $f$ is $k$ -times differentiable on some subset of its domain? Well, since we want "" $f$ is $1$ -times differentiable"" to mean the same as "" $f$ is differentiable"", we might want "" $f$ is $1$ -times differentiable on $S$ "" to mean the same as "" $f$ is differentiable on $S$ "". How would you define "" $f$ is $k$ -times differentiable on $S$ "" ?","['definition', 'derivatives', 'real-analysis']"
3901373,Generalized optimisation question about open topped boxes made from regular polygons,"This is based on the question about maximizing the volume of an open box being formed from a square with corners cut. Original question is here Optimisation question edit: added above picture for extra clarity about the case of a square base. I also solved this for the base being a triangle, pentagon and hexagon, the trig gets a little hectic. Spoiler alert - the answer in all cases ended up with $x$ being $1/6th$ the length of the original shape's side. My question is how to prove if is this the case for all regular polygons. In this low quality sketch, the cuts are along the green lines, making the rectangular flaps that fold up to make the triangular based box. Similar kite shapes need to be cut for the other shapes. triangle base","['trigonometry', 'calculus', 'geometry', 'polygons']"
3901397,Prime elements in $\mathbb{Z}[\sqrt{5}]$,"I need to determine which of the elements $3+2\sqrt{5}$ , $9+4\sqrt{5}$ and $4-\sqrt{5}$ are prime elements in $\mathbb{Z}[\sqrt{5}]$ , respectively which are associated. My ansatz is as follows: So let $x=3+2\sqrt{5}$ divide $ab$ for $a,b \in R$ . Thus, there are $u,v \in \mathbb{Z}$ , such that $$ab=(a_1+b_1\sqrt{5})(a_2+b_2\sqrt{5})=(a_1a_2+5b_1b_2)+(a_1b_2+a_2b_1\sqrt{5})=(u+v\sqrt{5})(3+2\sqrt{5})=(3u+10v)+(3v+2u)\sqrt{5}.$$ How do I go from here in order to check whether $x|a$ or $x|b$ ?","['number-theory', 'abstract-algebra', 'divisibility']"
3901510,Joint measure and absolute continuity wrt product of marginals,"I am trying to build intuition over the following matter: let $X,Y$ be two random variables with corresponding probability measures $P_X,P_Y$ . Assume also there exists a joint measure $P_{XY}$ such that for every measurable set $E$ $P_{XY}(\mathcal{X}\times E) = P_Y(E)$ and for every $F$ $P_{XY}(F\times \mathcal{Y}) = P_X(F)$ . My question is: is there a characterisation for when the joint is guaranteed to be absolutely continuous wrt to $P_XP_Y$ ?
All the counterexamples I could come up with are somehow dimension-related or ""pathological"", like: let $P_XP_Y$ be the Lebesgue measure over the unit square and $P_{XY}$ the joint induced by $X=Y$ . If we exclude this type of settings, is the constraint of $P_X,P_Y$ being the marginals enough to guarantee absolute continuity? Does anyone have intuition on this problem?","['measure-theory', 'lebesgue-measure', 'radon-nikodym', 'random-variables']"
3901636,Generalization of Radon-Nikodym Theorem,"I'm wondering if the following statement holds: Let $(X, \mathfrak{B})$ be a measurable space, and $\mu, \nu$ be complex measures. Assume $\nu << \mu$ . Is there a complex valued measurable function $f \colon X\to \mathbb{C}$ , which satisfies $$\forall B\in \mathfrak{B}; \int_B f d\mu = \nu(B)$$ If so, I want to call $f = \frac{d\nu}{d\mu}$ the Radon-Nikodym derivative. My textbook says it holds (only) when $\mu$ is $\sigma$ -finite positive measure. Any help will be appriciated. P.S. Here complex measure means the function $\mu \colon \mathfrak{A}\to \mathbb{C}$ which satisfies $\mu(\varnothing)=0$ and $\sigma$ -additivity in $\mathbb{C}$ (with ""Euclidean topological convergence"").","['measure-theory', 'radon-nikodym']"
3901672,"Point $M$ lies inside $\triangle ABC$, $\angle MAC = 10^\circ$ and $\angle MCA = 30^\circ$. Find $(180^\circ - \angle BMC)$","In $\Delta ABC, \angle CAB = 30^\circ$ and $\angle ABC = 80^\circ$ . Point $M$ lies inside the triangle such that $\angle MAC = 10^\circ$ and $\angle MCA = 30^\circ$ . Find $(180^\circ - \angle BMC)$ . What I Tried : Here is a picture to keep track of the angle-chasing . The red angle is $30^\circ$ , the green one is $10^\circ$ , the purple one is $20^\circ$ . Now I extend $MC$ to $AB$ such that it meets at $K$ . First I noticed that $AK = KC$ but that didn't seem to help. Also the $2$ yellow ones are equal to $40^\circ$ each, the brown one is equal to $80^\circ$ . The light green angle is $60^\circ$ . We have to find the blue angle. After all these information, it seems like I am still missing something, because from here you actually cannot deduce the value of the blue angle. Can anyone help? Thank You.","['contest-math', 'euclidean-geometry', 'geometry', 'triangles', 'problem-solving']"
3901699,About possible pathological solutions to the DE $y=y'+y''+y'''+\ldots$,"Is it possible to construct (or indirectly show the existence of) a function $y(x)\in\mathcal{C}^{\infty}(\mathbb{R})$ such that series $$ S(x) = \sum_{n=1}^{+\infty}\frac{d^n y}{dx^n} $$ is pointwise convergent for any $x\in\mathbb{R}$ , but $S(x)$ is not a differentiable function? A more-or-less equivalent and strictly related question is the following one: is it possible for the operator $$ T:\varphi(x) \mapsto e^x \int_{0}^{x} e^{-t}\varphi(t)\,dt $$ to produce $T(\varphi)\in \mathcal{C}^\infty$ without $\varphi$ being differentiable? Probably no, because... Assuming that the integral appearing above is the Riemann integral, $T$ acts on the space of almost-everywhere continuous functions, so we are free to assume that $\varphi$ is a.e. $\mathcal{C}^0$ , as well as $$\Phi(x)=\frac{d}{dx}\int_{0}^{x}e^{-t}\varphi(t)\,dt. $$ By the remark in the comments, $\int_{0}^{x}\left(\Phi(t)-e^{-t}\varphi(t)\right)\,dt$ equals zero almost everywhere, so the fundamental theorem of Calculus ""almost applies"". $T(\varphi)$ is smooth by assumption, so it is $T(\varphi)e^{-x}=\int_{0}^{x}e^{-t}\varphi(t)\,dt$ and its derivative $\Phi(x)$ . On the other hand $\Phi(x)=e^{-x}\varphi(x)$ almost everywhere, so $\varphi(x)$ is $\mathcal{C}^\infty$ almost everywhere.","['calculus', 'derivatives', 'ordinary-differential-equations', 'real-analysis']"
3901764,An example of a non-rectifiable curve,"How can I solve this problem? Let the curve in $\mathbb{R}^2$ of the equation $\alpha(t)=(t,g(t)), t\in [0,1]$ , where $$g(t)=\left\{ \begin{aligned} t \cos\left( \frac{\pi}{2t}\right), \quad t\not=0 \\ 0, \quad t=0\end{aligned}\right.$$ For $n \in \mathbb{N}$ , let the partition $P$ of $[0,1]$ given by $$P=\left\{ 0, \frac{1}{2n},\frac{1}{2n-1},\ldots, \frac{1}{3},\frac{1}{2},1\right\},$$ prove the length $\ell_n$ of the polygonal inscribed satisfies $$\ell_n>\sum_{k=1}^{2n}\frac{1}{k}$$ and and deduce that the curve $\alpha$ is not rectifiable. My approach: I know for example that if I prove that arc length of $\alpha$ is infinite, so I can say that $\alpha$ is not rectifiable, so I need to calculate $I_{n}$ . I know that the arc-length of a curve $\alpha$ is $$s(t)=\int_{t_0}^t \|\dot{\alpha}(t)\| \, dt$$ so I need to prove that $s(t)\to \infty \implies \alpha$ is not rectifiable. I know how calculate $\dot{\alpha}(t)$ , but how can I choose $t_0$ and $t$ ? Questions: Now, How can I prove the first part? It's to say, how can I prove that $\ell_n>\sum_{k=1}^{2n}\frac{1}{k}$ ? and how can I relate that result to the fact that alpha is not rectifiable?","['curves', 'differential-geometry']"
3901926,Why do each of these cylindrical triple integrals evaluate differently?,"The problem in question is thus:
Find the volume cut out of the sphere of a radius $a$ centered at the origin by the polar curve $r = a\cos\theta$ . I attempted to solve the problem using this cylindrical triple integral, taking advantage of $x$ -axis symmetry. $$
2\int_0^{\frac{\pi}{2}}\int_0^{a\cos\theta}\int_{-\sqrt{a^2 - r^2}}^{\sqrt{a^2 - r^2}}r\,dz\,dr\,d\theta = \frac{2\pi a^3}{3} - \frac{8a^3}{9}
$$ However, when this, similar integral is evaluated (which in my mind should yield the same results as the first one), the answer is different as the last term cancels out. $$
\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}} \int_0^{a\cos\theta}\int_{-\sqrt{a^2 - r^2}}^{\sqrt{a^2 - r^2}} r \, dz\,dr\,d\theta = \frac{2\pi a^3}{3}
$$ Which one of these answers is correct and why is the other one wrong? I understand that it has something to do with the theta integral, but I have no idea where my error is in assuming that these two triple integrals should produce identical results. Thanks! This is my first time on the Math Stack Exchange so let me know if there is anything else I can do to improve the nature of my question. THANK YOU ALL!!! Many wonderful explanations below, have a great day everyone!","['integration', 'cylindrical-coordinates', 'multivariable-calculus', 'spherical-coordinates', 'polar-coordinates']"
3901939,"Is there a single term for the *pair* (domain, codomain) of function $f$, or generally the (source, target) of morphism $f$?","This is just about finding concise terminology. So if $f:A \to B$ , is there a single generic name for the pair (A,B)? What about for the pair (domain, image)?","['elementary-set-theory', 'category-theory', 'terminology']"
3901962,When is 2 a quadratic residue mod p,"I'm trying to prove that 2 is a QR when $p = \pm \ 1 \text{ mod } 8.$ I know how to prove via a ""factorial"" proof, butI'm given the hint to use the fact that if $ \zeta $ is a primitive 8-root of unity, then $(\zeta + \bar{\zeta})^2 = 2$ , and would like to solve the exercise using the hint. This is where I got so far: $$ \text{ I can use the fact that } 2 \text{ is a quadratic residue if and only if } 2^{\frac{p-1}{2}}=1 \text{ mod p}$$ $$ 2^{\frac{p-1}{2}} = (\zeta + \bar{\zeta})^{p-1} = \sum_{k = 0} ^{p-1} {p-1 \choose k} \bar\zeta^k \zeta^{p-1-k} = \sum_{k = 0} ^{p-1} {p-1 \choose k}\zeta^{p-1+6k}$$ Now I guess I should use the fact that $ \zeta^4 = -1$ , and thus the symmetry of the sum cancels many factors, but I'm stuck at this point, and even computing the sum for small p didn't help much. Can someone help me out? Thanks!","['galois-theory', 'number-theory', 'algebraic-number-theory']"
3901995,Las Vergnas theorem,"I am supposed to prove that the graph satisfies the Las Vergnas
theorem. Here is the graph: Las Vergnas theorem is the following: $G$ is a graph and $V(G)=(v_1,...,v_n)$ . There is no such $i,j$ such that $i<j, i+j \geq n, v_iv_j \notin E(G),
deg(v_i) \leq i, deg(v_j) \leq j-1, deg(v_i) + deg(v_j) \leq n-1$ . Then $G$ is hamiltonian But if I label the vertices like this: I found such $i$ , $j$ : $i=2$ and $j=4$ , so the graph does not satisfy Las Vergnas theorem. Is there something, which I misunderstood or where is the mistake. Thanks","['graph-theory', 'discrete-mathematics', 'hamiltonian-path']"
3901999,Struggling to find solution: question 2,"$$xy'-2y=\frac{x^6}{(y+x^2)}$$ Can someone please help with this one? It looks like a bernoulli equation, I have tried to multiply both sides by denominator and gather them in parantheses to leave the $y'$ alone but to no avail.",['ordinary-differential-equations']
3902008,Benford's law and voting in Georgia,"With Benford's law ""the leading digits of the number found in real-world data sets"" should make sense.  But this video https://www.youtube.com/watch?v=DoF3WS42w3M&ab_channel=RobertA.Bonavito%2CCPA claims that by applying Benford's law they can prove electoral fraud. He claims that since Benford's law is ""broken"" in these graphs, it appears that there has been fraud committed, but I wonder if the artificiality of breaking things into counties and only having 156 of them violates the ""naturally occuring"" idea that is the foundation of Benford's Law. I am curious why Benford's law does not seem to apply here.",['probability']
3902010,"$\phi f \in L^p(\mu)$ whenever $f\in L^p(\mu)$, then $\phi \in L^\infty(\mu)$","Taken from Conway's A Course in Functional Analysis Chapter 3 Section 2 Problem 3 Problem statement: If $(X, \Omega, \mu)$ is a $\sigma$ -finite measure space, $\phi: X \rightarrow \mathbb{F}$ is an $\Omega$ -measureable function, $1\leq p \leq \infty$ , and $\phi f \in L^p(\mu)$ whenever $f\in L^p(\mu)$ , then show that $\phi \in L^\infty(\mu)$ . Below is my folioing attempt at a solution. Suppose that $\phi \notin L^\infty (\mu)$ . Then for any $N>0$ , there exists a subset $A_N\subset \Omega$ of nonzero measure such that $\phi(x) > N$ for $x \in A_N$ . We can define an $f \in L^p(\mu)$ so that $f|A_N = $ Id. Thus \begin{align}
        \left(\int_\Omega |\phi f|^p d\mu\right)^{1/p} &\geq  \left(\int_{A_n} |\phi f|^p d\mu\right)^{1/p} \\
        &= \left(\int_{A_n} |\phi|^p d\mu\right)^{1/p}\\
        &> \left(\int_{A_n} N^p d\mu\right)^{1/p}\\
        &= \mu(A_n)^{1/p}N
    \end{align} I hope to show that the last equation goes to infinity. I can let $N \rightarrow \infty$ but if I do then $\mu(A_n)$ could also tends towards $0$ even though it must be strictly greater than $0$ by assumption.","['functional-analysis', 'real-analysis']"
3902045,"Algorithmic simplification of expressions with $\arctan$, e.g. $2\arctan\frac{5-\sqrt2}{5+\sqrt2}=3\arctan\frac{1}{2\sqrt2}$.","This is a problem I encountered in another (unrelated) question . I got an expression which is a bit ugly, $2\arctan\frac{5-\sqrt2}{5+\sqrt2}$ , but when changing the method I got another prettier expression, $3\arctan\frac{1}{2\sqrt2}$ . Since they are both equal to the same integral, they must be equal as well (and we can easily prove that by doing some simple calculation). But what makes it interesting is, a lot of common mathematics software cannot do this kind of simplification (I tried SageMath and WolframAlpha, they both did nothing). So let's do it by hand to see what happened. \begin{align}
2\arctan\frac{5-\sqrt2}{5+\sqrt2} &= \arctan\frac{23}{10\sqrt2}\tag{automatic}\\
&= \arg\left(10\sqrt2+23i\right)\tag{automatic}\\
&= \arg\left(2\sqrt2+i\right)^3\tag{miracle}\\
&= 3\arctan\frac{1}{2\sqrt2}.\tag{automatic}
\end{align} (In the general case, we should also count how many $\pi$ should be added at the end.) As you can see, the key step here is the factorization $10\sqrt2+23i=\left(2\sqrt2+i\right)^3$ . This is a factorization in $\mathbb Z[\sqrt2,i]$ , which might be difficult even for computers (I'm not sure). What's more, normally there's not just $\sqrt2$ that appears, e.g. $$\arctan\frac{\sqrt5-77\sqrt2}{7+11\sqrt{10}}=3\arctan\sqrt5+5\arctan\sqrt2-3\pi.$$ So here is my question: Is there an algorithmic way to do this kind of simplification? Let me explain a little where this problem comes from. I got an integral, for $a>0$ : $$\int_0^\infty e^{-at}\left(\operatorname{erf}\sqrt t\right)^3\,dt=\frac{4}{\pi}\frac1{a\sqrt{a+1}}\arctan\frac{1-b}{1+b},$$ where $b=\frac{a}{a+4}\sqrt{\frac{a+3}{a+1}}$ . I wanted to simplify this expression but failed. The example mentioned before is just the case $a=1$ . I'd be glad if anyone can simplify this expression for any $a>0$ , and I'll take it as an acceptable answer since I think this expression is already complicated enough. I added my answer to the integral problem only. $$\int_0^\infty e^{-at}\left(\operatorname{erf}\sqrt t\right)^3\,dt=\frac{12}{\pi}\frac1{a\sqrt{a+1}}\left(\arctan\sqrt{\frac{a+3}{a+1}}-\frac\pi4\right).$$ And it didn't give us a general method to deal with all cases.","['trigonometry', 'factoring', 'complex-numbers', 'algorithms']"
3902049,"Spectral theorem, inner products and adjoints/transposes: what's the relation?","I only consider operators on finite-dimensional vector spaces. I'll write down my question and explain my motivation afterwards: The spectral theorem relates diagonalizability, which is a purely algebraic notion, with adjoints, which requires inner products. Why is that so? Is there a non-complex/real version of the spectral theorem? Consider the following version of the Spectral Theorem: Spectral Theorem : Every self-adjoint operator on a finite-dimensional (complex) inner product space admits a basis consisting of eigenvectors (""eigenbasis"") I am teaching Linear Algebra for the first time, so I am revisiting lots of concepts I studied several years ago, and just got to the Spectral Theorem. So this is a little funny, since ""diagonalizability""/existence of an eigenbasis does not relate directly to inner products in any obvious manner. Yes, I understand the point that self-adjointness of an operator $A$ takes in the proof: Every $A$ -invariant subspace has an $A$ -invariant complement. A similar statement holds for normal operators and eigenspaces. But the point seems to be that it just so happens that a condition related to inner product implies in a condition related to invariant subspaces, and it is not clear to me why that is so from a non-purely formal perspective. So that got me thinking: How would one be able to get a spectral theorem for non-complex/real spaces? The more I think the more I convince myself the condition needs to deal with some sort of transpose. But the problem is that transposes do not make much sense in general, and this is also another point: Inner products seem to just-so-happen allow us to define adjoints/transposes of operators. The point seems to be that, in general, transposes ""live"" on dual spaces, whereas Riesz representation allows us to go back to the level of the original spaces. So I would imagine the abstract version of the Spectral Theorem (if there is one) would need a condition relating an operator $T\colon V\to V$ and its formal transpose $T'\colon V'\to V'$ , perhaps passing to the bidual if necessary (as this is naturally isomorphic to the ground space), but I can't find any possible formula involving both $T$ and $T'$ .","['spectral-theory', 'linear-algebra']"
3902088,"Solving $a(a + 1) + b(b + 1) = 12, (a+ 1)(b+1) = 4$, is there any trick?",What are all (possibly complex) solutions to the following two equations? $a(a + 1) + b(b + 1) = 12$ $(a+ 1)(b+1) = 4$ Clearly one can substitute the second equation into the first then solve the quartic. But I am wondering if there is a shorter/sneakier way to solve this.,"['algebra-precalculus', 'systems-of-equations', 'quadratics']"
3902100,Minimum value of $\sqrt{x^4 + 3x^2 - 6x + 10} + \sqrt{x^4 - 5x^2 + 9}$ without using calculus?,"Hi mathematics stack exchange, what is the minimum value of $\sqrt{x^4 + 3x^2 - 6x + 10} + \sqrt{x^4 - 5x^2 + 9}$ ? I know how to solve this problem using calculus, you take a derivative, but I am wondering if there is an elementary method to find the minimum using precalculus methods.","['calculus', 'optimization', 'radicals', 'quadratics', 'algebra-precalculus']"
3902114,What is the remainder when $1^n + 2^n + 3^n + \ldots + 99^n$ is divided by $1 + 2 + 3 + \ldots + 99$?,"My first idea on how to approach was to create a polynomial expansion for the first few terms and then try to find a pattern for the rest but this became cumbersome and I don't think that this is a right approach as this would quickly become a problem that involves factorials and I have not covered modular arithmetic, is there a way to approach this problem in such a way that does not involve using factorials with mods.","['number-theory', 'modular-arithmetic', 'elementary-number-theory']"
3902135,"Convergence in probability and asymptotic distribution of MLE for Uniform $(-\theta, \theta)$","Problem. I am having difficulties with asymptotic results of the maximum likelihood estimator (MLE) of the parameter $\theta$ for the Uniform $(-\theta, \theta)$ distribution, given an IID sample $X_1, X_2,..., X_n$ .  Namely showing explicitly that: $$\widehat{\theta}_n \overset{p}{\rightarrow} \theta$$ and finding the limiting distribution $$n(\theta - \widehat{\theta}_n)$$ Where $\widehat{\theta}_n$ denotes the maximum-likelihood estimator. I am aware that if certain regularity conditions are satisfied, the MLE is a consistent estimator of a parameter of a distribution; so this question concerns showing it for a particular form of the Uniform distribution. I would have liked this question to be more concise, but it is not so because the issue lies in the fact that I am making reasoning errors which I am unable to discern. My attempt. Convergence in probability I first computed the likelihood function $L(\theta)$ , and found that the MLE of $\theta$ can be expressed as $\widehat{\theta}_n = \max \{ -X_{(1)}, X_{(n)} \}$ , where $X_{(1)}$ and $X_{(n)}$ are the 1st and nth order statistics. Now in order to show consistency, I began by considering the following, with a view to showing that it converges to 0 as $n \rightarrow \infty$ : $$P(|\widehat{\theta}_n - \theta | \geq \epsilon) = \underbrace{P(\widehat{\theta}_n \geq \theta + \epsilon)}_{=0} + P(\widehat{\theta}_n \leq \theta - \epsilon) = P(\widehat{\theta}_n \leq \theta - \epsilon)$$ I then proceeded by evaluating the RHS and this is where things start to become uncertain for me: $$\begin{align}
P(\widehat{\theta}_n \leq \theta - \epsilon) &= P(\max \{ -X_{(1)}, X_{(n)} \} \leq \theta - \epsilon) \\
&= P \left( \left\{ -X_{(1)} \leq \theta - \epsilon \right \} \cap \left \{ X_{(n)} \leq \theta - \epsilon \right \} \right) \\
&= P(-X_{(1)} \leq \theta - \epsilon) P(X_{(n)} \leq \theta - \epsilon)
\end{align}$$ In going from the 1st to the 2nd equality, I reasoned that in order for the maximum of $-X_{(1)}$ and $X_{(n)}$ to be less than $\theta - \epsilon$ , both $-X_{(1)}$ and $X_{(n)}$ must be less that $\theta - \epsilon$ . However, as it is a maximum of order statistics, the nesting is confusing me, and I'm starting to experience doubt as to whether this is a valid justification. In going from the 2nd to the 3rd equality, I am fairly certain that is appropriate as the $X_i$ are independent, and hence their order statistics are independent. I computed the CDF of the Uniform $(-\theta, \theta)$ to get: $$F_{X_i}(x_i) = 
\begin{cases}
0 \quad &x_i \leq -\theta \\
\frac{x_i + \theta}{2\theta} \quad &-\theta \leq x_i \leq \theta \\
1 \quad &x_i \geq \theta \\
\end{cases}$$ Evaluating the probability on the nth order statistic: $$P(X_{(n)} \leq \theta - \epsilon) = P \left(\bigcap^n_{i=1} X_i \leq \theta - \epsilon \right) = \prod^n_{i=1} P (X_i \leq \theta - \epsilon) = \left(1 - \frac{\epsilon}{2 \theta} \right)^n$$ Evaluating the probability on the 1st order statistic: $$P(-X_{(1)} \leq \theta - \epsilon) = P(X_{(1)} \geq - \theta + \epsilon) = 1 - P \left(X_{(1)} \leq -\theta + \epsilon \right) = 1 - P \left( \bigcap^n_{i=1} X_i \leq - \theta + \epsilon \right) = \left( 1 - \frac{\epsilon}{2\theta} \right)^n $$ Setting aside my reservation about how the above two calculations are combined , the above two individual probability calculations seem to accord with my intuitions, in that they are the same due to symmetry arguments. Which yields: $$P(|\widehat{\theta}_n - \theta | \geq \epsilon) = \left( 1 - \frac{\epsilon}{2\theta} \right)^{2n} = \left(\frac{2 \theta - \epsilon}{2\theta} \right)^{2n}$$ Which converges to 0 as $n \rightarrow \infty$ for all $\epsilon > 0$ . Convergence in distribution From a previous example in my notes, I know that for IID $X_1, ... ,X_n \sim \text{Uniform}(0,1)$ , the limiting distribution of the nth order statistic is an $\text{Exponential}(1)$ distribution; that is: $$X_{(n)} \overset{d}{\rightarrow} \text{Exponential}(1)$$ On this basis, I guessed that the solution would have a similar flavour. Invoking the arguments I made previously, I found that: $$\begin{align}
P(n(\theta - \widehat{\theta}_n) \leq t)  &= 1 - P \left(\widehat{\theta}_n \leq \theta - \frac{t}{n} \right) \\
&= 1 - P\left( \left\{-X_{(1)} \leq \theta - \frac{t}{n} \right \} \cap \left \{ X_{(n)} \leq \theta -\frac{t}{n} \right \} \right)  \\
&= 1 - \left( 1 - \frac{t}{2n \theta} \right)^{2n} \\
\end{align}$$ Now I am left with something that looks very similar to an exponential CDF, and as a guess I suspect it might be an $\text{Exponential}(1 / \theta)$ ; and that the appearance of the factor of 2 is erroneous (due to an issue in the previous arguments which I am unable to discern). Another attempt. After reading through some previous posts on here, I decided to try reformulating the MLE estimator to something equivalent, that is by setting $\widehat{\theta}_n = \max\{|X_i|\} \space \forall \space i = 1, ... , n$ . I found that this skirts around the issue of not being sure about point 1. Focusing on the convergence in distribution argument (as the convergence in probability argument is a stepping stone) I found that: $$\begin{align}
P(n(\theta - \widehat{\theta}_n) \leq t) = 1 - P \left(\widehat{\theta}_n \leq \theta - \frac{t}{n} \right) &= 1 - P \left( \max \{ |X_i | \} \leq \theta - \frac{t}{n} \space \forall \space i = 1, ... , n \right) \\
&= 1 - P \left( \bigcap^n_{i=1} |X_i| \leq \theta - \frac{t}{n} \right) \\
&= 1 - \prod^n_{i=1} \left( P(X_i \leq \theta - \frac{t}{n}) + P(-X_i \leq \theta - \frac{t}{n} ) \right) \\
&= 1 - \prod^n_{i=1} \left( P(X_i \leq \theta - \frac{t}{n}) + P(X_i \geq -\theta + \frac{t}{n}) \right) \\
&= 1 - \prod^n_{i=1} F_{X_i}\left( \theta - \frac{t}{n} \right) \left[1 - F_{X_i}\left(-\theta + \frac{t}{n} \right) \right] \\
&= 1 - \prod^n_{i=1} \left( \frac{2 \theta - t/n}{2 \theta} + 1 - \frac{t/n}{2 \theta} \right) \\
&= 1 - \left( 2 - \frac{t}{2n \theta} \right)^n
\end{align}$$ So I know that as I am not getting the same results, there are errors of reasoning being made. However, I am having trouble discerning where these lie. I would appreciate if members of this community were to assist me.","['statistical-inference', 'statistics', 'probability-limit-theorems', 'parameter-estimation', 'maximum-likelihood']"
3902225,nonexistence of a function satisfying $f(x)^{f(y)}=y^x$,"prove that there is no function $f:\mathbb{N}\mapsto \mathbb{N}$ such that $\forall x, y \in \mathbb{N}\ (f(x))^{f(y)}=y^x$ My attempt: If such function exists, so for $x,y,z \in \mathbb{N}$ $(f(x))^{f(y+z)}=(y+z)^x$ but in the other hand $(f(x+y))^{f(z)}=z^{(x+y)}=z^xz^y=(f(x))^{f(z)}(f(y))^{f(z)}$ and I need a Hint to accomplish this.","['functional-equations', 'functions']"
3902226,"How to show that if two Platonic solids have the same number of edges, vertices, and faces, then they are similar in $\mathbb{R}^{3}$?","Note: It appears that some of the terms here do not have standardized definitions, so some sources may give conflicting info. I was looking into the proof that there are only five Platonic solids in Basic Concepts of Algebraic Topology by F.H. Croom at page 29, Theorem 2.7. To clarify, We define a Platonic solid as a simple, regular polyhedron homeomorphic to $S^{2}$ . We define a simple polyhedron to be a polyhedron that does not self-intersect itself. We define a regular polyhedron to be a polyhedron whose faces are regular polygons all congruent to each other and whose local regions near the vertices are all congruent to each other. Using homology theory, one can prove that the Euler formula $V-E+F=2$ must hold for Platonic solids. Then by using Euler's formula and invoking a counting argument, we find that there are five possible tuples $(V, E, F)$ .
This is a beautiful proof, but I am unsatisfied with a question: How do we know there can't be two non-similar Platonic solids that have the same $(V, E, F)$ -tuple? Almost all sources I've looked at seem to assume it is obvious that two Platonic solids with the same $(V, E, F)$ -tuple are similar, and it is not obvious to me. Does anyone have any suggestions for how to prove this? Alternatively, does anyone know of a reference where this is proved rigorously? Edit 1: It's not completely clear, but it seems like the definition I used for ""regular polyhedra"" is different than the one commonly used. Note that I am not assuming any global symmetry, so if any global symmetry is to be invoked, it needs to be proven. Edit 2: I've been made aware of Cauchy's rigidity theorem, which is proven in, e.g., Proofs From the BOOK by Aigner & Zeigler. One can show that any two Platonic solids that have the same $(V, E, F)$ -tuple must be combinatorically equivalent. However, in order for the theorem to apply, we need to show that our Platonic solids are convex. I can't seem to think of any rigorous argument for why the Platonic solids have to be convex. And actually, you don't need to show that the entire polyhedron is convex. If I'm not mistaken, the proof for Cauchy's rigidity theorem only relies on the vertices of the polyhedron being locally convex. So really it suffices to show the vertices are convex.","['platonic-solids', 'geometry', 'reference-request']"
3902316,The Inverse of a Rational Section is a Rational Section of the Dual,"This question comes after reading the last paragraph of Vakil's FOAG, p. 400. We consider the set of $\{(\mathcal L, s) \}$ , where $\mathcal L$ is an invertible sheaf on a Noetherian, reduced, regular in codimension 1 (in case any of that matters) scheme $X$ , and $s$ is a nonzero rational section of $\mathcal L$ . The claim is that once you mod out by isomorphism, this set is an abelian group under $\otimes$ , with inverse $\{(\mathcal L^*, 1/s) \}$ . Thematically, this all works out well given that we know $\mathcal L \otimes \mathcal L^* \simeq \mathcal O_X$ , but why is $1/s$ a nonzero rational section of the dual? The best I can say is that both sheaves are locally isomorphic to $\mathcal O_X$ , so perhaps we mean to consider $s$ and $1/s$ as rational sections of $\mathcal O$ , and then we glue to produce $s$ and $1/s$ as rational sections of $\mathcal L$ ?",['algebraic-geometry']
3902337,Finding a tricky quotient set,"We will denote by $\mathbb{K}$ one of the fields $\mathbb{Q}, \mathbb{R}$ or $\mathbb{C}$ . On $\mathbb{K}\times \mathbb{K}$ we define the following equivalence relation: $$(a,b)\equiv (a', b') \iff \exists (q,\alpha)\in \mathbb{K}^{*}\times \mathbb{K} \text{ such that } \begin{cases} a=q^2a'+\alpha^2-b\alpha \\ b=qb'+2\alpha \end{cases}.$$ We wish to determine the quotient set $\mathbb{K} \times \mathbb{K}/\equiv$ $\space$ for all $\mathbb{K}$ s. This problem was an extra problem in my abstract algebra class (don't worry, this isn't an attempt to cheat, I am posting this a week after the solution was due to be sent) and I kind of got stuck when it comes to $\mathbb{K}=\mathbb{Q}$ . For $\mathbb{K}=\mathbb{C}$ , the things were nice and easy, because the original question asked me to prove that $\mathbb{C}\times \mathbb{C}/\equiv$ is equal to $\{\hat{(0,0)}, \hat{(0,1)}\}$ and this can be checked through (tedious) direct computations. For $\mathbb{K}=\mathbb{R}$ , a friend came up with the idea of expressing $\alpha$ from the second equation and then substituing it in the first one. This gives us the following equivalent characterisation of the equivalence relation: $$(a,b)\equiv (a', b') \iff \exists q\in \mathbb{K}^{*} \text{ such that } 4a+b^2=q^2(4a'+b'^2) \space (*).$$ (notice that this works for all $\mathbb{K}$ s, the case $\mathbb{K}=\mathbb{C}$ can be solved much easier by using this, but I didn't really need to think that much for that one since direct computations worked in my context) For real numbers, this rewrites as $(a,b)\equiv (a', b') \iff \operatorname{sgn}(4a+b^2)=\operatorname{sgn}(4a'+b'^2)$ . As a result, there will be three equivalence classes: the parabola $4x+y^2=0$ , its interior and its exterior. A representative for each of these are, respectively, $(0,0), (-1,0)$ and $(0,1)$ , so $\mathbb{R}\times \mathbb{R}/\equiv \space = \{\hat{(0,0)}, \hat{(-1,0)}, \hat{(0,1)}\}$ . For $\mathbb{K}=\mathbb{Q}$ , the things get pretty nasty by this approach. In this case, $(*)$ rewrites as $(a,b)\equiv (a',b') \iff \sqrt{\frac{4a+b^2}{4a'+b'^2}}\in \mathbb{Q}$ and I haven't been able to make any further progress.","['equivalence-relations', 'abstract-algebra']"
3902338,Sum of cube roots of two conjugate quadratic integers makes an integer.,"Consider the following expression: $$(20-\sqrt{392})^{1/3}+(20+\sqrt{392})^{1/3}.$$ This equals $4$ , but how can I show this? Note that I do not want to make use of the following line of reasoning: 4 is a solution to $x^3-6x-40=0$ , that this cubic has exactly one solution, and that all solutions to the cubic $x^3+px +q = 0$ are given by $$(-q/2-((q/2)^2+(p/3)^3)^{0.5})^{1/3}+(-q/2+((q/2)^2+(p/3)^3)^{0.5})^{1/3}.$$ I have noted the method set out here, How can I show that this complicated expression with square and cube roots reduces to the value 7? , but implementing it does not work very well. I attempted this and got more complicated expressions than I started off with! It doesn't result in a unique solution for $a$ and $b$ . I do think that this is the right approach, though, i.e. making use of the fact that if $\sqrt c$ is irrational and $a+b\sqrt c=d+e \sqrt f$ then $a=d$ and $b=e$ . (How do you prove this fact, by the way?)","['cubics', 'number-theory', 'nested-radicals']"
3902533,Understanding the difference between convergence in measure and convergence almost everywhere.,"My concern arises when I was going over the proof that if $f_n\rightarrow_\mu f$ , then for some subsequence $n_k$ we have $f_{n_k}\rightarrow_{a.e.}f$ , provided that $\{f_n\}$ are measurable and finite a.e. To me, I don't quite get the intuitive difference between convergence in measure and convergence almost everywhere very well. My characterization of convergence almost everywhere will be something like $$\mu(\{\lim f_n\neq f\})=0 \Leftrightarrow \mu(\lim |f_n-f|\neq0)=0$$ and convergence in measure would be $$\lim \mu(|f_n-f|\geq\epsilon)=0$$ In short, I distinguish the two concepts by thinking one has the limit operator inside the measure one is outside, so intuitively converge almost everywhere must be stronger. (I know it is only the case when the measure is finite, which I don't quite get yet.) However, when attempted to prove the statement, I confused myself when writing down the condition: $$\forall \epsilon>0, \mu(|f_n-f|\geq\epsilon)\rightarrow0,n\rightarrow\infty$$ It is clear that to attempt this question, I should construct a subsequence that is convergent almost everywhere. What I first think of is to write something like $$\mu(|f_n-f|\geq1/2^k)<1/2^k$$ but then I got stuck. I have no intuitive understanding of what kind of subsequence indexed by $k$ should I obtain from this condition. If I pick up $\{f_n\}$ such that the condition hold, find the $f_n$ with smallest $n$ and indext it by $k$ , let $k\rightarrow \infty$ , the choice of sequence seems more likely to follow the pattern of the original sequence. I looked at the textbook and here is the full step of the proof: for every positive integer $k$ , $\exists n_k,$ when $n\geq n_k$ $$\mu(|f_n-f|\geq1/2^k)<1/2^k$$ Suppose $n_1<n_2....<n_k<...$ , $f'_k\equiv f_{n_k},k=1,2,....$ is a subsequence of $\{f_n\}$ and it is convergent a.e. to $f$ . In fact, $\forall \epsilon>0$ \begin{align}
\mu(\bigcap_k\bigcup_v|f'_{k+v}-f|\geq\epsilon) & \leq \mu(\bigcup_v|f'_{k_0+v}-f|\geq\epsilon)\text{  for every $k_0$} \\
 & \leq \sum_v\mu(|f'_{k_0+v}-f|\geq\epsilon) \\ 
 & \leq \sum_v \mu(|f'_{k_0+v}-f|\geq\frac{1}{2^{k_0+v}})\ (\frac{1}{2^{k_0}}<\epsilon) \\
 & \leq \sum \frac{1}{2^{k_0+v}} \\ 
 & = \frac{1}{2^{k_0}}
\end{align} Since the inequality holds for any $k_0$ sufficiently large, $\mu(\bigcap_k\bigcup_v|f'_{k+v}-f|\geq\epsilon)=0.$ I quite enjoy the proof as it is clearly written and easy to follow, however, it doesn't very intuitive to me what has been done to the condition that allows the subsequence to have this nice property. What kind of constraint that I have made so that the subsequence convergent a.e. to $f$ ?Is there any intuitive way to understand the chosen subsequence?","['measure-theory', 'convergence-divergence']"
3902541,"A curve has the property that the normal line through any point on the curve passes through $(2,0)$. Find its equation.","A curve has the property that the normal line through any point on the curve passes through $(2,0)$ .  If the curve contains the point $(2,3)$ find its equation. My attempt: Assume $(a,b)$ is a point on the function. The normal line has a slope $m=\frac{b}{a-2}$ and hence the tangent line has a slope $m_T = \frac{2-a}{b}$ . Hence, this is the slope we can integrate to get the equation: $$F(x) = \int \frac{2-a}{b}x dx = \frac{2-a}{2b}x^2+C$$ Since we know that $(2,3)$ is a point we can isolate for $C$ to get: $$F(x) = \frac{2-a}{2b}x^2+\frac{3b-4+2a}{b}$$ I am getting stuck here because I am not sure how to get values for a $a$ and $b$ ...","['calculus', 'functions', 'tangent-line', 'differential-geometry']"
3902559,"$f: X \rightarrow \mathbb{R}$ is measurable iff $\{(x, t): t=f(x)\} \in \mathcal{F} \otimes \mathcal{B}(\mathbb{R})$.","A function $f: X \rightarrow \mathbb{R}$ is measurable if and only if its graph is measurable, $\{(x, t): t=f(x)\} \in \mathcal{F} \otimes \mathcal{B}(\mathbb{R})$ . This is easy when we consider the points under the graph, but not sure how to do the graph itself","['measure-theory', 'proof-writing', 'real-analysis', 'measurable-functions', 'product-measure']"
3902601,"$f$ is integrable $\iff$ for every sub-block $B$ we have that the function $f|_{B}$ is integrable, i.e. $\int_{A}f=\sum_{B}\int_{B}f|_{B}$","QUESTION: Let $f:A \rightarrow \mathbb{R}$ be a limited function and  let $P$ be a partition of the block $A$ ( $A$ is a block in $\mathbb{R}^m$ ). Then $f$ is integrable $\iff$ for every sub-block $B$ we have that the function $f|_{B}$ is integrable and in this case, $$\int_{A}f=\sum_{B}\int_{B}f|_{B}$$ . REMARK: The professor allowed us to use the following concepts: Proposition: Let $P_0$ be an arbitrary partition of the block $A$ . In order to consider the upper and lower integrals of the limited function $f:A \rightarrow \mathbb{R}$ , we just need to consider partition refinements of $P_0$ . That is, we have $$\underline\int_{A} f(x) dx= \underset{P\supset P_0}{sup} s(f; P)$$ and $$\overline\int_{A} f(x) dx= \underset{P\supset P_0}{inf} S(f; P)$$ Theorem: The limited function $f: A \rightarrow \mathbb{R}$ is integrable $\iff$ for every $\epsilon>0$ it is possible to find a partition $P$ of the block $A$ such that $$\displaystyle\sum_{B\in P} \omega_{B}\cdot vol B<\epsilon$$ Where $\omega_{B}$ is the set of the oscillations, i. e., $$\omega_{B}:= sup\{|f(x)-f(y)|; x, y \in B\}$$ MY ATTEMPTY: $(\Longrightarrow)$ Let $f: A \rightarrow \mathbb{R}$ be a limited function and let $P$ be a partition of the block $A$ . Suppose that $f$ is integrable then $\forall \epsilon >0$ it is possible to obtain an partition $P=P_1 \times \cdots \times P_n$ of $A$ such that $\displaystyle\sum_{B\in P} \omega_B \cdot \text{vol}B <\epsilon$ , where $B$ are blocks in $P$ . Once $B$ are sub-blocks of $A$ , let $P_0$ be an partition of $B$ . Therefore for every limited function $f|_{B}$ we just need to consider the refinement partitions of $P_0$ . Indeed, let $B=\displaystyle\Pi_{i=1}^{n}[b_i, c_i] \subset A$ then for every $i=1, \cdots, n$ lets define $Q_i= P_i\cap[b_i, c_i]$ from this we have a new partition $Q = Q_1 \times \cdots \times Q_n$ of $A$ that is a refinement of $P$ and, furthermore, the blocks of $Q$ are contained in $B$ makes a partition $P_0$ of $B$ . Thus $$\underbrace{\displaystyle\sum_{B'\in P_0}\omega_{B'}\cdot \text{vol} B'}_{(I)}\leq\displaystyle\underbrace{\sum_{B\in P}\omega_{B}\cdot \text{vol} B<\epsilon}_{(II)}$$ $(I) \subset (II)$ therefore $f|_{B}$ is intagrable. $(\Longleftarrow)$ We just need to consider $P=P_1 \times \cdots \times P_n$ as a partition of the block $A$ and we also need to consider that this partition is a composition of the block $A$ in sub-blocks like $B=I_1 \times \cdots \times I_n$ where every $I_j$ is an interval of the partition $P_j$ , where every sub-block $B$ is the block of partition $P$ , i.e., $B\in P$ . So, writting $A=\displaystyle\bigcup_{i=1}^{n}B_i$ and remembering that every $f|_{B}$ is integrable. Note that if $P_i$ is a partition of $B_i$ we can consider $Q=\displaystyle\sum_{i=1}^{n}P_i$ as an refinement partition of $P$ thus $f:A \rightarrow\mathbb{R}$ is integrable. Now we just need to show that: $$\int_{A} f \leq \displaystyle\sum_{B \in P} \int_{B} f|_{B}$$ . In $f:A \rightarrow \mathbb{R}$ considering the partition $P$ of the block $A$ we just need to consider refinement partitions of $P$ , let $Q$ be an arbitrary partition of the block $A$ we can consider, for instance $P_0= P+Q$ . It follows from upper integration definition that $$s(f, P)=\displaystyle\sum_{B \in P} m_B(f)\cdot \textbf{vol}B= \displaystyle\sum_{B \in P} m_{B}(f|_{B}) \cdot \textbf{vol} B$$ . Then, for every $B$ we consider $B' \subset B$ , the sub-blocks of $B$ resultants of the refinement of $P$ , and $B=\bigcup B'$ . Therefore, \begin{align*}
    \int_{A} f = \displaystyle sup_{P_0\supset P} s(f, P_0)& = sup \left(\displaystyle\sum_{B\in P}m_{B}(f|_{B}) \cdot \textbf{vol} B\right)\\
    & = sup \left(\displaystyle\sum_{B\in P}m_{B}(f|_{B}) \displaystyle\sum_{B'\subset B} \textbf{vol} B'\right)\\
    & = sup \left(\displaystyle\sum_{B\in P}\displaystyle\sum_{B'\subset B}m_{B}(f|_{B})  \textbf{vol} B'\right)\\
    & \leq sup \left(\displaystyle\sum_{B\in P}\displaystyle\sum_{B'\subset B}m_{B'}(f|_{B})  \textbf{vol} B'\right)\\
    & = \displaystyle\sum_{B\in P} sup \left(\displaystyle\sum_{B'\subset B}m_{B'}(f|_{B})  \textbf{vol} B'\right)\\
    & = \displaystyle\sum_{B\in P} \underline{\int_{B}} f|_{B}\\
    & = \displaystyle\sum_{B\in P} \int_{B} f|_{B}
\end{align*} Thus, $$\int_{A} f \leq \displaystyle\sum_{B\in P} \int_{B} f|_{B}$$ Similarly, we can show for upper sum, and obtain $$\int_{A} f \geq \displaystyle\sum_{B\in P} \int_{B} f|_{B}$$ And finally, conclude $$\int_{A} f = \displaystyle\sum_{B\in P} \int_{B} f|_{B}$$ MY DOUBT: Would you help me to improve my answer? Specialy in this $(\Longleftarrow)$ way.","['integration', 'riemann-integration', 'definite-integrals', 'analysis']"
3902621,Bounds for probability that two elements commute in a group?,"Let $G$ be a finite non-abelian group, and lets randomly choose two elements of $G.$ It seems quit well known that the probability that they commute is at most $\text{Pr}(G)\le\dfrac{5}{8}.$ Here is a nice reference if you like to know more about this cute result. Equality holds if and only if $G/Z(G)\cong\mathbb{Z}_2\times\mathbb{Z}_2$ ( Klein four-group). Further the equality implies that each non-central conjugacy class is of order $2$ and $8\big{|}|G|.$ Smallest non-ablalian groups with this divisibility property, namely $D_8$ (dihedral group of order 8) and $Q$ (quaternion group), achieve the equality. Next, I looked at the next possible case $G/Z(G)\cong S_3,$ and proved that here $$\dfrac{4}{9}\le\text{Pr}(G)\le\dfrac{7}{12}.$$ For example: we have $\text{Pr}(S_3)=\dfrac{1}{2}.$ But still wasn't able to find two groups that can achieve this upper and lower bound. Is it possible to reach these bound? If so, what are the examples? Otherwise, can we improve theses bounds? In general, how far can we recover a group using only inner automorphism group?","['finite-groups', 'group-theory', 'sylow-theory', 'p-groups', 'abelian-groups']"
3902681,Second Course on ODEs - Self Study - Book Recommendation,"I'm looking for a book that would serve as a a second course on Ordinary Differential Equations. The book should be theoretically-oriented and for self study. I just finished the book by George Simmons - Differential Equations with Historical notes and Applications and in all honestly didn't really like it, finished it for the sake of completeness but it isn't what I was looking for.
I would like the book to contain a decent number of exercises after each section/chapter and, I stress, to be a theoretical approach to the topic. I originally started with Birkhoff and Rota's ODEs but one of the requierements was basic knowledge with complex functions, which I haven't studied yet.
I have knowledge of Calculus I (Spivak and Apostol), Calculus II (Apostol), Linear Algebra (Friedberg), Abstract Algebra (Herstein). Also, I will be finishing Introduction to Analysis (Mattuck) this week and will be starting with Rudin's Principles of Mathematical Analysis afterwards.
I have added the authors of the books, all of which I've liked, I used to study the previous topics for reference.
I'm looking for a book that I will be able to complete with the current knowledge that I have. I don't know if learning complex functions first, which I beleive I can take after finishing Rudin's Analysis, and then going back to Birkhoff's would be an option, but if it is I'm open to it too. Thanks in advance","['self-learning', 'book-recommendation', 'ordinary-differential-equations', 'reference-request']"
3902705,Matrix representation of trace,"I got trace is $4$ for a $2\times2$ matrix. I know trace is a linear operation which transforms a matrix to a field. So the matrix representation of a trace should be like $1×n$ , where $n$ is the dimension of that square matrix. How I can represent number $4$ as a $1\times4$ matrix?
Thanks in advance!","['matrices', 'trace', 'linear-algebra']"
3902725,What is the Ramanujan summation for the series $\sqrt[n]{2}$,"A Ramanujan summation is a technique invented by the mathematician Srinivasa Ramanujan for assigning a value to divergent infinite series In my case, I'm interested in assigning a value to the divergent series $$\sum_{n=1}^\infty f(n) \ \ \ \ \ \ \ \text{where}\ \ \ \ f(n)=\sqrt[n]{2}$$ According to the Wikipedia page (and my understanding), the Ramanujan summation is $$\sum_{n=1}^\mathfrak{R} f(n)=\lim_{N\to\infty}\Bigg[\sum_{n=1}^N f(n)-\int_{1}^N f(t)dt\Bigg]$$ Thus $$\sum_{n=1}^\mathfrak{R} \sqrt[n]{2}=\lim_{N\to\infty}\Bigg[\sum_{n=1}^N \sqrt[n]{2}-\int_{1}^N \sqrt[t]{2}dt\Bigg]$$ Taking the antiderivative $$\sum_{n=1}^\mathfrak{R} \sqrt[n]{2}=\lim_{N\to\infty}\Bigg[\sum_{n=1}^N \sqrt[n]{2}-\Bigg(\ln2\Big(\text{li}\ 2-\text{Ei}\frac{\ln2}{N}\Big)+N\sqrt[N]{2}-2\Bigg)\Bigg]$$ Moving some constants outside the limit $$\sum_{n=1}^\mathfrak{R} \sqrt[n]{2}=2-\ln2\cdot\text{li}\ 2+\lim_{N\to\infty}\Bigg[\sum_{n=1}^N \sqrt[n]{2}-\Bigg(N\sqrt[N]{2}-\ln2\cdot\text{Ei}\frac{\ln2}{N}\Bigg)\Bigg]$$ It's at this point I'm unsure of how to proceed. I'm not terribly confident what the limit converges to. From my computational estimates up to $N=10^8$ , I find that $$\sum_{n=1}^\mathfrak{R} \sqrt[n]{2}\approx1.6$$ But due to floating point errors or slow convergence, it deviates substantially enough for me to not be confident about any more digits. I'd like to know if this converges at all, and if it does, is there a (reasonably) closed form / relation to other constants?","['limits', 'divergent-series', 'ramanujan-summation']"
3902733,"Question about Protter's proof that a Cadlag, locally square integrable local martingale is a semimartingale","This is Corollary $1$ in Chapter $2$ of Protter's Stochastic Integration and Differential Equations. Theorem 8 states that each $L^2$ martingale (martingales $X$ such that $X_0 = 0$ and $E[X_\infty^2]<\infty$ ) with cadlag paths is a total semimartingale. The Corollary to Theorem 6 states that : If $X$ is a process and there exists a sequence $T_n$ of stopping times increasing to $\infty$ a.s. such that $X^{T_n}$ ( or $X^{T_n} 1_{\{T_n>0\}}$ ) is a semimartingale for each $n$ then $X$ is a semimartingale. But I don't see how the proof is so straightforward here. First, let $X$ be a cadlag, locally square integrable local martingale. Then does this mean that $X$ is locally a square integrable martingale, i.e. we have a fundamental sequence $T_n$ such that $X^{T_n} 1_{\{T_n >0\}}$ is a square integrable martingale?  I have seen several questions about this on StackExchange, but no real answer to this. It seems like we would need this condition but then taking $X^n := X^{T_n} 1_{\{T_n >0\}}$ , square integrability just means that $E[(X^n_t)^2]<\infty$ for each $t$ . We don't get $L^2$ boundedness over all $t$ from this. So how do we use the corollary to Theorem 6 here? My attempt: $X$ being a semimartingale is by definition in the text, $X^t$ being a total semimartingale for each $t \ge 0$ . Assuming that being locally square integrable local martingale is the same as being locally a square integrable martingale, we can find a fundamental sequence $T_n$ such that $X^{T_n} 1_{\{T_n > 0\}}$ is a square integrable martingale. For convenience denote $M := X^{T_n} 1_{\{T_n > 0\}}$ . Then we have $E[(M_t^n)^2]<\infty$ for all $t \ge 0$ . Now in order to apply the Corollary to Theorem 6, we would need to show that $M$ is a semimartingale. And to do this we need to use Theorem 8. Thus, we are done if we show that for each $s \ge 0$ , $M^s$ is a $L^2$ - martingale. Now, for each $s \ge 0$ , $M^s_t = X_{T_n \wedge s \wedge t} 1_{\{T_n > 0\}}$ . Since a stopped martingale is a martingale, $M^s$ is still a martingale. Moreover, $s$ and $t$ are not random, so square integrability gives us that $M^s$ is a $L^2$ -bounded uniformly integrable martingale. Hence, by Theorem 8, $M^s$ is a total semimartingale, hence $M$ is a semimartingale. Finally, the Corollary to Theorem 6 applies. QED. This proof has been bothering me for a long time now. I think my final argument assuming a single fundamental sequence that makes $X$ , a locally square integrable local martingale, into a locally square integrable martingale, is correct but I don't know how to show this part. I would greatly appreciate any help.","['stochastic-processes', 'local-martingales', 'martingales', 'probability-theory', 'stochastic-calculus']"
3902745,Infinite sum of Wiener processes,"Context There are two different Wiener processes $W_t$ and $V_t$ . It's known that they are independent. Additionally, we are given with third Wiener process $B_t$ that is given by the formula $$B_t = aW_t+bV_t, \quad \quad a^2 + b^2= 1.$$ Problem Find the limit in $L^2$ of $$S_n = \sum_{i=1}^n\left[B_{it/n} - B_{(i-1)t/n}\right]\left[V_{it/n} - V_{(i-1)t/n}\right]$$ as $n$ tends to infinity. My ideas I assume that this is the type of task where we need to calculate the expected value and the variance. As the latter tends to $0$ (it should), we can say that the desired limit is the expected value. The issue is that it's very overextended work to calculate the $E(S_n)$ . Let $W_{it/n} = X_i$ and $V_{it/n} = Y_i$ . We have $$\Bbb E(S_n) = \Bbb E\sum_{i=1}^n [aX_i + bY_i - aX_{i-1} - bY_{i-1}][Y_{i} - Y_{i-1}]$$ which can be written as $$\sum \Bbb E\bigg( aX_iY_i + bY_i^2 - aX_{i-1}Y_i - bY_{i-1}Y_i   - aX_iY_{i-1} - bY_iY_{i-1} + aX_{i-1}Y_{i-1} + bY_{i-1}^2\bigg).$$ Next calculations confuse me (what is $\Bbb E(X_i Y_{i-1})$ ?) Is it zero? And the main question how to calculate the variance ? If I'm not mistaken, $\Bbb E(S_n) = nb \to \infty$ , so we don't need variance. Am I right?","['limits', 'stochastic-calculus', 'stochastic-processes']"
3902755,Is there a mathematical formula for the nearest-square function?,"Let $x$ be a positive integer. Is there a mathematical formula for $$f(x)=\text{nearest square to } x \text{ }(\text{in terms of } x)?$$ I tried searching for related questions in MSE and found this one , but my inquiry is not covered there. I also tried searching via Google and found this closely related question in StackOverflow . I used the search string "" nearest-square function "".","['number-theory', 'square-numbers', 'functions', 'reference-request']"
3902788,Probability of a random set of binary numbers XORing to $0$,"Question - Given a randomly generated set of binary numbers, what is the probability that all of them, when XORed with each other, yield $0$ ? Additional information - All of the binary numbers in the set are different from each other. As an example, $\{0001, 0010, 0011\}$ is a valid set, but not $\{0001, 0010, 0010\}$ or $\{0010, 0010, 0010\}$ . All of the binary numbers have equal length $n$ which is some given. For example, one could have a given length of 4, meaning numbers such as $00001$ and $010$ are not allowed. The length of the set $m$ is a given, its members are randomly generated. Each possible valid binary number has an equal likelihood of being generated. Context - For a school math project, I decided to analytically investigate the average percentage of errors an error correction code can correct given a data set and error rate. This question I am asking was a part of said project that I am unable to solve. If necessary, I can provide additional context. I already know how to do this problem if restriction 1 is lifted, but did not really know how to proceed. I visualized the randomly generated data set as being one number stacked on top of another, kind of how we write when adding large numbers on paper by hand, and then taking my answer to be the probability that each column of bits had an even number of $1$ s. Updated context - Looking at the answers already provided and doing some of the math myself, it is clear that using this in my project will take me well over the page count in all of the explanations. I have come up with a simpler path for my investigation, but am leaving this question up here as I am still interested in this problem and I see no reason to stop anyone interested in answering or seeing answers to this question from doing so. 1rst edit - I have assigned $m$ as the length of the set and $n$ as the length of the binary sequences 2nd edit - I have added additional context","['binary', 'probability']"
3902822,Does a contractible manifold admit a vector field whose flow is a contraction?,"The title might be slightly misleading (in that I don't need the flow to contract $M$ in finite time). What I'm asking is this: If $M$ is a contractible smooth $n$ -manifold and $p \in M$ , is there a smooth vector field $X$ on $M$ s.t. its flow $\Phi^X: [0, \infty) \times M \to M$ is defined for all positive times and for all neighborhoods $U$ of $p$ there is a $T$ s.t. $$
\Phi^X_t(M) \subset U \quad\text{for}\quad t > T \tag{1}
$$ This should be equivalent to $\Phi^X$ extending to a continuous map $[0, \infty] \times M \to M$ by $\Phi^X_\infty(x) = p$ . This is obviously true for $\mathbb{R}^n$ , where we can take $X(x) = -x$ . I don't really understand the Whitehead manifold (a contractible 3-fold that is not homoemorphic to $\mathbb{R}^3$ ), so I haven't checked that. Edit: I just noticed that picking $U$ to be a coordinate ball (1) shows that $M$ must embed into $\mathbb{R}^n$ (via $\Phi^X_T$ ). Is it possible that this gives a contradiction when $M$ is not $\mathbb{R}^n$ ? Background: It seems to me that this is (implicitly) used in Voisin's Hodge Theory and Complex Algebraic Geometry when proving Ehresmann's theorem (Thm 9.3). The last paragraph of the proof states: Here $U$ is a small neighborhood of an arbitrary point on the manifold $B$ .","['vector-fields', 'general-topology', 'differential-topology', 'ordinary-differential-equations']"
3902825,A problem related to $\int_{0}^{1} f(x)(x-f(x))dx=1/12$,"Consider  a differentiable  function satisfying $$\int_{0}^{1} f(x)(x-f(x))dx=1/12$$ Then find the nearest integer less than or equal to $\frac{1}{f'(1)}$ . Let $$F(x)=\int_{0}^{x}f(x)(x-f(x))$$ we notice $F(1)=1/12$ , $F(0)=0$ and $$F'(1)=f(1)-{f(1)}^2$$ What do i do next?
I have tried using LMVT and Rolle's but not getting anything nice.","['integration', 'calculus', 'definite-integrals', 'leibniz-integral-rule']"
3902905,"Logical translation of the verb "" to be "" in "" the cat is a mammal "". ( Inclusion or membership?)","Standardly, the sentence "" the cat is a mammal "" would be translated as for all x , if x is a cat, then x is a mammal, meaning that the set of cats is included in the set of mammals : $C\subseteq M$ . There is another interpretation: the species "" cat"" is a mammals species. Under this alternative interpretation ( admitting that a "" species"" is some sort of set or collection), we would have : $ C \in M $ * with C = the set of cats and M* = the set of species of mammals. The distinction I'm referring to is analogous to the medieval distinction between personal supposition and simple supposition of a term. In "" horses are animals"" the term "" horse"" supposes personnaly while in "" horse is a species"" , the term supposes simply. What are the drawbacks of the second interpretation ( where "" is"" is interpreted as membership relation holding between a species and a set of species)? Why has the first interpretation become standard?","['elementary-set-theory', 'logic']"
3902922,"Filling Riemannian manifolds, Gromov, Proposition 5.1.B","In Gromovs paper Filling Riemannian manifolds, https://projecteuclid.org/download/pdf_1/euclid.jdg/1214509283 it states on page 46: 5.1.B Proposition Let $v$ be a surface with complete Finsler metric, fix a point $v\in V$ and let $R\in[\frac{1}{2}h(v),\frac{1}{2}Sys(V,v)]$ . Then \begin{equation}
Area \ B(v,R) \ \geq \ \frac{1}{2}(2R-h(v)).
\end{equation} Here $Sys(V,v)$ denotes the length of a shortest noncontractible loop based at $v$ and \begin{equation}
h(v)=\inf\{tension(\gamma)|\gamma  \ \text{is either a noncontractible loop or an infinite path through  }v\},
\end{equation} where \begin{equation}
tension(\gamma)=\sup\{\delta>0|\text{there is an homotopy from }\gamma \ \text{to } \overline{\gamma}, \text{s.t. } length(\overline{\gamma})=length(\gamma)-\delta\}.
\end{equation} Now the proof works as follows: Suppose $R<\frac{1}{2}Sys(V,v)$ , then $B(v,R)$ is contractible. Thus for every connected component of its boundary $S_i\subset \partial B(v,R)$ , there is a surface $D_i$ homeomorphic to a disk, s.t. $\partial D_i=S_i$ . Denote by $B^+(v,R)$ the union of $B(v,r)$ with all the $D_i's$ . Let $\gamma$ be either a noncontractible loop or an infinite path through $v$ . Thus $\gamma$ intersects $\partial B^+(v,R)$ at least two times. Choose the first $x_1$ and the last $x_2$ of this interesections when walking along $\gamma$ . This divides $\partial B^+(v,R)$ in two parts $L_1,L_2$ with $l_1=length(L_1)\leq length(L_2)=l_2$ . Now $\gamma$ is homotopic to the loop $\gamma'$ obtained by the part of $\gamma$ between $x_1$ and $x_2$ union $L_1$ . Thus in particular \begin{equation}
tension(\gamma)\geq 2R-l_1,
\end{equation} Since $\gamma'$ is at least $2R-l_1$ shorter then $\gamma$ . Thus we get \begin{equation}
tension(\gamma)\geq 2R-\frac{1}{2}length(\partial B^+(v,R)),
\end{equation} which is equivalent to \begin{equation}
length(\partial B^+(v,R))\geq 4R-2tension(\gamma).
\end{equation} In the next line it says that we can thus say \begin{equation}
length(\partial B^+(v,R))\geq 4R-2h(v).
\end{equation} I don't see how this follows. Isn't by definition $h(v)\leq tension(\gamma)$ ? Thank you for any hint!","['riemannian-geometry', 'differential-geometry']"
3903042,Finding a rational number in between two rationals,Can anyone give me some hints or show me how to solve this problem? Problem Find a rational number in between $\frac{9}{10}$ and $\frac{10}{11}$ which may be written in the form $\frac{m}{2^n}$ where $m$ is an integer and $n$ is a non negative integer. I just do not know where to even start or what theorems I need to use to approach this. I would appreciate hints or guidance to solving these types of problems.,"['calculus', 'analysis']"
3903088,Derivation of geodesic on $m$-sphere $\gamma : \mathbb{R} \rightarrow S^m$ is given by $\gamma(t) = \cos(t |v|)p \ + \frac{\sin(t |v|)}{|v|} v$,"Given $p \in S^m$ and $0 \neq v \in T_pS^m = p^{\perp}$ , the geodesic $\gamma : \mathbb{R} \rightarrow S^m$ seems to take the form by $\gamma(t) = \cos(t |v|)p \ + \frac{\sin(t |v|)}{|v|} v$ , where $t \in \mathbb{R}$ . This is pretty much a given as fact in all differential geometry books I've seen so far. I assume this is some kind of supposedly trivial parameterization obtained by intersecting $S^m$ with the plane spanned by $\big\{p, \frac{v}{|v|} \big\}$ , but I haven't been able to connect that with the actual closed form of the geodesic $\gamma$ . I'd tremendously appreciate a derivation of this result, as I haven't been able to find this anywhere else.","['geodesic', 'riemannian-geometry', 'curves', 'parametrization', 'differential-geometry']"
3903116,Non-linear coupled differential equations,"I was trying to solve these coupled differential equations but can´t quite get to the solution. The differential equations are: $$ H^2= \frac{1}{3}\left[\frac{1}{2} \dot\phi^2+V(\phi)\right] \space\space\space\space\space(1)$$ $$  \ddot\phi+3H\dot\phi-\lambda V_0e^{-\lambda \phi}=0  \space\space\space\space\space\space (2)$$ where $ H(t)=\frac{\dot a(t)}{a(t)} $ , $\lambda$ and $V_0$ are constants and the dot notation represents $\space \dot a= \frac{da}{dt}$ . The respective solutions of $a(t)$ and $\phi(t)$ are the following: $$ a(t)=a_0 t^{P} \space \space \space, \space\space\space P=\frac{2}{\lambda^2}\space\space\space\space\space\space (3)$$ $$ \phi(t) =\phi_0 + \frac{2}{\lambda}\ln \left( \frac{t}{t_0}\right)\space\space\space\space\space\space\space\space (4)$$ I was told to do a power law solution for $a(t)$ and $\psi(t)$ , given that $\psi(t)=e^{- \lambda \phi (t)}$ , but making that substitution $ \psi(t)$ I got an equation that seems even worse: $$ -\ddot \psi \psi^2 \left(\frac{1}{\lambda} \right) + \left( \frac{\dot \psi}{\lambda}- \frac{3H \psi}{\lambda}  \right) \dot \psi = \lambda V_0 \psi^3$$ I tried using aproximations too but to no luck. Any help would be appreciated.","['systems-of-equations', 'ordinary-differential-equations']"
3903214,Mean value theorem for $\displaystyle f(x)=\frac{3-x^{2}}{x-2}$,"Given the following function $$f(x)=\frac{3-x^{2}}{x-2}$$ does the Mean value theorem applies to if in the interval $[1,3]$ ? Since $f$ is not defined for $x=2$ then $f$ is continuous and differentiable in $[1,3]-\{2\}$ . So, the theorem is valid. But I can't find a number $c\in(1,3)$ such that $$f'(c)=-2$$","['calculus', 'derivatives']"
3903248,"Let $O$ be the centre of the circumcircle of $\Delta ABC$, $P$ and $Q$ be the midpoint of $AO$ and $BC$, respectively.","Let $O$ be the centre of the circumcircle of $\Delta ABC$ , $P$ and $Q$ be the midpoint of $AO$ and $BC$ , respectively. Suppose $\angle CBA = 4\angle OPQ$ and $\angle ACB = 6\angle OPQ$ . FiNd $\angle OPQ$ . What I Tried : Here is a picture :- You can see what I did, I extended some lines and joined them and got a pair of congruent triangles there, and marked some of the angles, and this is my only progress. Now can anyone help? Thank You.","['congruences-geometry', 'triangles', 'problem-solving', 'geometry']"
3903283,"The function $\phi\colon t \mapsto \int_{0}^{1}f(x,t)\, dx$ is Borel","Suppose $f\colon[0,1]\times [0,1] \rightarrow \mathbb{R} $ . Functions $x \mapsto f(x,t)$ are integrable, functions $t \mapsto f(x,t)$ are continuous. I need to prove that the following function is Borel: $$\phi\colon t \mapsto \int_{0}^{1}f(x,t)\,dx.$$ I tried to use monotone class theorem for $E=\{B \in \mathcal{B}(\mathbb{R})\mid \phi^{-1}(B) \in \mathcal{B}([0,1]) \} \subset \mathcal{B}(\mathbb{R})$ . I need to show that $E=\mathcal{B}(\mathbb{R})$ . The only thing I managed to prove is that $E$ is an algebra. So, the idea seems wrong. Please give me any ideas how to solve the problem. Thank you for any hints!","['integration', 'measure-theory', 'lebesgue-measure', 'lebesgue-integral', 'functional-analysis']"
3903327,What's the bijection between scalar/inner products and (certain) almost complex structures (on $\mathbb R^2$)?,"Asked on maths overflow here . What's the bijection between (equivalence classes of) scalar products (I guess 'scalar product' is the same as 'inner product') and a.c.s. (almost complex structure/s) on $\mathbb R^2$ ? From Example 1.2.12 of Daniel Huybrechts - Complex Geometry An Introduction. Assumptions and notation: I just pretend $V = \mathbb R^2$ literally instead of just an isomorphism. Let $\Phi(V)$ be the set of real symmetric positive definite $2 \times 2$ matrices. This set is in bijection with inner products on $V$ , I believe. We have according to this , $$\Phi(V) = \{\begin{bmatrix}
h & f\\ 
f & g
\end{bmatrix} \ | \ h+g, hg-f^2 > 0 \}_{h,f,g \in \mathbb R}$$ Let $\Gamma(V)$ be the (matrix representations of) a.c.s. on $V$ . We have, according to this , $$\{\begin{bmatrix}
a & b\\ 
\frac{-1-a^2}{b} & -a
\end{bmatrix}\}_{a,b \in \mathbb R, b \ne 0}=: \Gamma(V) \subseteq Auto_{\mathbb R}(V) \subseteq End_{\mathbb R}(V)$$ I understand that the ' rotation ' matrices in $V$ are $SO(2) := \{R(\theta) := \begin{bmatrix}
\cos(\theta) & -\sin(\theta)\\ 
\sin(\theta) & \cos(\theta)
\end{bmatrix}\}_{\theta \in \mathbb R}$ , though I'm not sure that Huybrechts has the same usage of the term 'rotation'. (I ask about this later.) Questions : A. For injectivity (except for the equivalence class): Given (equivalence class of) scalar product ( $[M]$ of) $M$ , choose unique $I$ that assigns $v$ to the one described. I'll call this map $\gamma: \Phi(V) \to \Gamma(V)$ , $\gamma(M)=I$ . (Later, $\tilde \gamma: \frac{\Phi(V)}{\tilde{}} \to \Gamma(V)$ , $\tilde \gamma([M])=I$ .) It's 'rotation by $\pi/2$ ' or something. In what way ? For $M=I_2$ (2x2 identity), then $I$ is indeed 'rotation by $\pi/2$ ', in the sense that it's $\begin{bmatrix}
0 & 1\\ 
-1 & 0
\end{bmatrix} \in SO(2) \cap \gamma(V)$ , which is the ' $R(\theta)$ ' , for $\theta = \pi/2$ . What exactly is the formula for $I=\begin{bmatrix}
a & b\\ 
\frac{-1-a^2}{b} & -a
\end{bmatrix} \in \Gamma(V)$ given $M = \begin{bmatrix}
h & f\\ 
f & g
\end{bmatrix} \in \Phi(V)$ ? I'm asking because 2a - I would exceed wolfram computation time 2b - I notice for a different $M$ I tried, $I$ isn't a 'rotation matrix' in the sense of $SO(2)$ . In fact, I believe the only 'rotation' matrices that are also a.c.s. are $\pm \begin{bmatrix}
0 & 1\\ 
-1 & 0
\end{bmatrix}$ , i.e. $SO(2) \cap \gamma(V) = \{\pm \begin{bmatrix}
0 & 1\\ 
-1 & 0
\end{bmatrix}\}$ . However, I think $I$ kind of 'rotates by $\pi/2$ ' in some other sense. 2c - I think $SO(2) \cap \gamma(V)$ isn't meant to be the image of $\gamma$ B. For surjectivity : I'll call whatever map we would have as $\phi: \Gamma(V) \to \Phi(V)$ , $\phi(I)=M$ Given an a.c.s. $I$ , what are some possible scalar products $M$ ? There's a comment that goes choosing the unique $M_v$ such that for some $v \in V \setminus 0$ , we have $\{v,I(v)\}$ as an orthonormal basis. I tried this out (long to type!), and the only thing missing was the positively oriented. I guess either $\{v,I(v)\}$ or $\{v,-I(v)\}$ is positively oriented though. So I'll let $M_v$ / $N_v \in \Phi(V)$ correspond to $\{v,I(v)\}$ / $\{v,-I(v)\}$ . Then by fixing $v$ (I ask about non-fixing of $v$ later), we have $\phi(I)=M_v$ or $N_v$ , whichever corresponds to positively oriented basis. I'll just call this $\phi(I)=L_v$ Is this right? Is $\phi$ supposedly an inverse (or right inverse or left inverse or whatever) to $\gamma$ (or $\tilde \gamma$ or whatever), in the sense that $\gamma(\phi(I)) = I$ for all (a.c.s.) $I \in \Gamma(V)$ ? This whole thing about the $v$ makes me think there's another equivalence relation going on here. Is there? This seems like we can have maps parametrised by the nonzero $v$ , namely $\phi_v: \Gamma(V) \to \Phi(V)$ . In this case, we might investigate if $\phi_v(I)=L_v=L_w=\phi_w(I)$ or at least if $[L_v]=[L_w]$ under the old equivalence relation of positive scalar $\lambda$ , i.e. $L_v = \lambda L_w$ . If this investigation turns out negative, then I think there's some problem like if 2 inner products are equivalent if they are from the same a.c.s. $I$ under $\phi_{\cdot}$ , but for possibly different $v$ and $w$ , then I think the equivalence class of $L_v$ under this new relation, which is $\{L_w\}_{w \ne 0}$ , might not be the same as the equivalence class of $L_v$ under the old relation, which is $\{\lambda L_v\}_{\lambda > 0}$ . Ideas: Perhaps there's some matrix thing here about how scalar products are in bijection with positive definite symmetric matrices and then almost complex structures are rotation matrices or something that are square roots of $-I_2$ . Like given pos def symmetric $B$ , there exists unique a.c.s. $J$ such that (something something). Perhaps this is related, but I'd rather not further analyse the question or read through the answer given that I've spent over a month on almost complex structures BEFORE we even put inner products on vector spaces . Please consider spoon-feeding me here.","['complex-geometry', 'complex-analysis', 'abstract-algebra', 'linear-algebra', 'almost-complex']"
3903341,Visual proof for $[\triangle ABC] = rs$?,I'm teaching a high school geometry class and we just got to the part where we proved that the area of a triangle $[\triangle ABC]$ is equal to the inradius $r$ times the semiperimeter $s$ . A student pointed out to me that it's really the same thing as $[\triangle ABC] = \frac{1}{2}rp$ where $p$ is the perimeter. But this representation looks like the regular formula $\frac{1}{2}bh$ . So I was wondering if there's a way to take the original $\triangle ABC$ and construct a new triangle that will have a base of $p$ and a height of $r$ and show that the original and newly constructed triangle have the same area?,"['euclidean-geometry', 'geometric-construction', 'visualization', 'geometry', 'triangles']"
3903361,$6!\cdot 7!=10!$. Is there a natural bijection between $S_6\times S_7$ and $S_{10}$?,"Aside from $1!\cdot n!=n!$ and $(n!-1)!\cdot n! = (n!)!$ , the only product of factorials known is $6!\cdot 7!=10!$ . One might naturally associate these numbers with the permutations on $6, 7,$ and $10$ objects, respectively, and hope that this result has some kind of connection to a sporadic relation between such permutations - numerical ""coincidences"" often have deep math behind them, like how $1^2+2^2+\ldots+24^2=70^2$ can be viewed as an ingredient that makes the Leech lattice work. The most natural thing to hope for would be a product structure on the groups $S_6$ and $S_7$ mapping to $S_{10}$ , but as this MathOverflow thread shows, one cannot find disjoint copies of $S_6$ and $S_7$ living in $S_{10}$ , so a product structure seems unlikely. However, I'm holding out hope that some weaker kind of bijection can be found in a ""natural"" way. Obviously one can exhibit a bijection. For instance, identify the relative ordering of $1,2,\ldots 7$ in a permutation of size $10$ , and then biject $_{10}P_{3}=720$ with $S_6$ in some way. But I'd like to know if there is a way to define such a bijection which arises naturally from the permutation structures on these sets, and makes it clear why the construction does not extend to other orders. I tried doing something with orderings on polar axes of the dodecahedron ( $10!$ ) and orderings on polar axes of the icosahedron ( $6!$ ), in the hopes that the sporadic structure and symmetry of these Platonic solids would allow for interesting constructions that don't generalize, but ran into issues with the dodecahedron (sequences of dodecahedral axes aren't particularly nice objects) and the question of how to extract a permutation of length $7$ . I'm curious if someone can either devise a natural bijection between these sets or link to previous work on this question.",['combinatorics']
3903372,Prove this refinement of Nesbitt's inequality based on another,"Let $a,b,c\in[1,2]$ such that $a,b$ are constants then prove  : $$f(c)=\frac{a}{b+c}+\frac{b}{a+c}+\frac{c}{b+a}\geq h(c)=(c-1)\frac{g(2)-g(1)}{2-1}+g(1)\geq g(c)=\sqrt{\frac{9}{4}+\frac{3}{2}\frac{(a-b)^2}{ab+bc+ca}}$$ Yes it's a probable refinement of HN_NH's inequality/ Stronger than Nesbitt inequality My refinement is based on two observations : The function $g(c)$ is convex on $[1,2]$ 2.The chord of a convex function is greater than the convex function. To know if the LHS is good I have tried derivatives . It gives a quartic and it's very ugly so I can say that I have not the solution for the LHS but it seems to be true (numerical check). Update 12/11/2020: The function : $$p(c)=f(c)-h(c)$$ Is convex on $[1,2]$ so there is a possibility to use Jensen's inequality but now I don't see any good issue . Using Jensen's inequality we have : $$p(c)+p(1)\geq 2p\left(\frac{1+c}{2}\right)$$ And : $$p\left(\frac{1+c}{2}\right)+p(1)\geq 2p\left(\frac{3+c}{4}\right)$$ And : $$p\left(\frac{3+c}{4}\right)+p(1)\geq 2p\left(\frac{3+c}{8}+\frac{1}{2}\right)$$ And so on...Playing with these inequalities we got the result I think ! Have you an idea to show the LHS(or confirm my update)? Thanks in advance Max.","['jensen-inequality', 'functions', 'inequality', 'derivatives', 'convexity-inequality']"
3903400,"If a power of 2 divides a number, under what conditions does it divide a binomial coefficient involving the number that it divides?","We have had many questions here about the divisibility of $\binom{n}{k}$ , many of them dealing with divisibility by powers of primes, or expressions involving the $\textrm{gcd}(n,k)$ (I originally gave several more examples but took TheSimpliFire's advice to shorten the list, and many other examples can still be found by looking at this question's edit history): How can we prove that a binomial coefficient (n choose k) is divisible by the ratio of n and the gcd(n,m)? Proving prime $p$ divides $\binom{p}{k}$ for $k\in\{1,\ldots,p-1\}$ The topic has also lead to some interesting research papers recently: In 2014: Some divisibility properties of binomial and $q$ -binomial coefficients . In 2017: Divisibility of binomial coefficients and generation of alternating groups . In 2019: On the divisibility of binomial coefficients . There's also many associated theorems: Kummer's theorem Lucas's theorem Granville's theorem Désarménien's theorem However I've come across a related problem which is expressed completely in the title, and is remarkably not covered by the above extensive body of literature. In MathJax, if $s>0$ is an integer (let's also make it the largest one for which $2^s$ divides $n$ ), under what conditions of $k$ do we have the following: $$
2^s \mid n \implies 2^s \left\vert \binom{n}{k} \right. . \tag{1}
$$ Since $\binom{n}{k} = \frac{n}{k} \binom{n-1}{k-1}$ and $2^s \mid n$ we are left with determining when $\frac{q}{k} \binom{n-1}{k-1}$ is an integer ( $q$ being the result when $n$ is divided by $2^s$ ). The implication works for a remarkable number of cases, but not always (for example if $\binom{n}{k}$ is odd).","['divisibility', 'number-theory', 'gcd-and-lcm', 'binomial-coefficients', 'combinatorics']"
3903440,What's the result of $\lim_{x\to 0} \dfrac{e^{-\frac{1}{x^2}}}{\sin x}$?,"During a correction of an exercise in a differentiability class this limit showed up and the teacher said that it could be done with L'Hopital easily, but me and a couple of friends tried doing it and neither of us got it. How can it be solved? I know the limit is 0 thanks to Geogebra. $$ \lim_{x\to 0} \dfrac{e^{-\frac{1}{x^2}}}{\sin x}$$","['limits', 'calculus']"
3903461,$ \cos x\geq 1-\frac{x^2}{2} $ [duplicate],"This question already has answers here : Is $\cos(x) \geq 1 - \frac{x^2}{2}$ for all $x$ in $R$? (4 answers) Closed 3 years ago . Prove that for $x\in\mathbb{R}$ $$
\cos x\geq 1-\frac{x^2}{2}.
$$ My try: Consider $g(x)=\cos(x)-1+\frac{x^2}{2}.$ If I differentiate $g(x)$ then we get $g'(0)>0$ so locally we get $g(x)>g(0)=0$ and then we can see that the function is increasing for any $x$ the function is increasing and hence we have $g(x)\geq 0$ for any $x \geq 0$ . But I am getting that if $x<0$ then $g(x) \leq 0.$ So this inequality isn't true in general for all $x \in \Bbb R$ . But, if we use Taylor's theorem with Lagrange's remainder then also I am not sure what will be the point $\zeta\in [-x,0]$ where $\cos(x)=1-\frac{x^2}{2}+\frac{x^4}{4}\cos(\zeta).$","['real-analysis', 'maxima-minima', 'taylor-expansion', 'trigonometry', 'derivatives']"
3903482,How to find generators of translation subgroup of an abstract reflection (coxeter) group,"I have an infinite reflection group https://en.wikipedia.org/wiki/Coxeter_group Take for example the affine groups $[4,4],[4,3,4],[4,3,3,4]$ ... I'd like to get an explicit expression for generators that generate the translation subgroup: $$T=\langle t_1,\cdots,t_r \; :\; t_i t_j = t_j t_i\rangle $$ and each $t_i$ has infinite order. For example for [4,4] : $t_1=r_1r_2r_3; t_2=r_3r_2r_1$ ; [Edit : per Derek Holt's comment $t_1=r_2r_3r_2r_1; t_2=r_3r_2r_1r_2;$ is the appropriate example] What about others in general? I'm sure this is a known result but I can't find it.","['group-presentation', 'group-theory', 'coxeter-groups', 'reflection']"
3903490,Proving the solution of an ode converges as $t$ goes to infinity given its norm is bounded by a convergent function,"Question Let $f \in \mathcal C(\mathbb R_+ \times \mathbb R^n, \mathbb R^n)$ and $g \in \mathcal C(\mathbb R_+ \times \mathbb R, \mathbb R_+)$ . Assume that $g(t,u)$ is non-decreasing in $u$ for each fixed $t$ and $\vert f(t,x)\vert \lt g(t,\vert x \vert), (t,x)\in \mathbb R_+ \times \mathbb R^n$ Every solution $u(t) = u(t,t_0,u_0)$ of the IVP $u^\prime(t) = g(t,u),
u(t_0)=u_0$ is bounded. Show that every solution of the IVP $x^\prime(t) = f(t,x), x(t_0) = x_0$ exists on $[t_0,\infty)$ and converges to a constant vector as $t \to \infty$ . My Attempt I have completed the first part of the question (show every solution of the IVP $x' = f(t,x), x(t_0) = x_0$ exists on $[t_0,\infty)$ ) using following corollary. Corollary : Let $f \in \mathcal C(J \times \mathbb R^n, \mathbb R^n)$ where $J=[a,b)$ and $\vert f(t,x) \vert \lt g(t,\vert x \vert)$ for $(t,x)\in J \times R^n$ and $g \in \mathcal C(\mathbb R_+ \times \mathbb R, \mathbb R_+)$ . If $\gamma(t)$ is the maximal solution of $u^\prime = g(t,u), u(t_0)=u_0$ existing on $J$ , then the solution $x(t) = x(t,t_0,x_0)$ of $x^\prime = f(t,x), x(t_0) = x_0$ where $\vert x_0 \vert \leq u_0$ exists on $J$ and $\vert x(t) \vert \leq \gamma(t)$ for all $t\in J$ . Simply let $\gamma(t)$ be the maximal solution of $u^\prime = g(t,u), u(t_0)=\vert x_0 \vert$ . Since we know all solutions of this IVP are bounded (by (2)), we know that $\gamma(t)$ must exist on $J = [t_0,\infty)$ . By the corollary, $x(t)$ also exists on $[t_0,\infty)$ . Moreover, $\vert x(t) \vert<\gamma(t)\forall t\in[t_0,\infty)$ . Having said that, I do not know how to prove that $x(t)$ must converge. I have the following results: Since $g(t,u)$ is always positive, we know that $\gamma(t)$ is an increasing bounded continuous function of $t$ (by (2)). Thus, $\gamma(t)$ must converge. Hence, $\vert x(t) \vert$ is bounded from above by a convergent function. $g(t,u)$ is a non-decreasing function of $u$ for every fixed $t$ , thus $\vert x^\prime(t) \vert \leq \vert f(t,x) \vert \leq \vert g(t,\vert x(t) \vert) \vert \leq \vert (g(t,\gamma(t)) $ but I'm not sure what they imply about the convergence of $x(t)$ . Any help would be greatly appreciated.","['limits', 'derivatives', 'ordinary-differential-equations', 'real-analysis']"
3903511,What is the obvious mistake of this double integration?,"There is a definite double integral in Cartesian coordinates as follows: $$\int_{-2}^2\int_{-\sqrt{4-x^2}}^{\sqrt{4-x^2}} 2\sqrt{4-x^2}\ dydx\Rightarrow \int_{-2}^{2} 2y\sqrt{4-x^2}\rvert_{-\sqrt{4-x^2}}^{\sqrt{4-x^2}}  \ dx \Rightarrow 4\int_{-2}^{2} (4 - x^2)dx \Rightarrow 4(4x - \frac{x^3}{3})|_{-2}^{2} = \frac{128}{3} $$ and there is another double integral in polar coordinates which is supposed to be the exact same integral as the above, but the final answer is not 128/3. it took me several hours, but I cant figure out which step of this integration is wrong. $$ \int_{0}^{2\pi}\int_{0}^{2} 2r\sqrt{4-r^2{\cos^2\theta}} drd\theta
 \xrightarrow[]{u = 4-r^2{\cos^2\theta}}  \int_{0}^{2\pi} - \frac{2}{3} \frac{(4-r^2{\cos^2\theta})^\frac{3}{2}}{\cos^2\theta}\Big|_{0}^{2}  \ d\theta \Rightarrow \int_{0}^{2\pi} (-\frac{2}{3}\frac{(4-4\cos^2\theta)^\frac{3}{2}}{\cos^2\theta} + \frac{2}{3}\frac{4^\frac{3}{2}}{\cos^2\theta} ) d\theta \Rightarrow \frac{16}{3}\int_{0}^{2\pi} \frac{1-(1-\cos^2\theta)^\frac{3}{2}}{\cos^2\theta} d\theta \Rightarrow  \frac{16}{3} \int_{0}^{2\pi} \frac{1-{\sin^3\theta}}{\cos^2\theta} d\theta \Rightarrow \frac{16}{3}\int_{0}^{2\pi} \frac{1}{\cos^2\theta} - \frac{\sin^3\theta}{\cos^2\theta} d\theta \Rightarrow \frac{16}{3}(\int_{0}^{2\pi} \frac{1}{\cos^2\theta} d\theta - \int_{0}^{2\pi} \frac{\sin^3\theta}{\cos^2\theta} d\theta) \Rightarrow \frac{16}{3}(\tan\theta - \int_{0}^{2\pi} \frac{\sin^3\theta}{\cos^2\theta} d\theta) \xrightarrow[]{u=\cos\theta} \frac{16}{3}(\tan\theta - \int \frac{1-u^2}{u^2} du) \Rightarrow \frac{16}{3}(\tan\theta - (-\sec\theta - \cos\theta))\Big|_{0}^{2\pi}\Rightarrow \frac{16}{3} ((0+1+1) - (0+1+1)) = 0 $$ I know there is a huge problem with the second one and my apologies if it's an elementary problem. But I would really appreciate if you could help me find the problem.","['multivariable-calculus', 'multiple-integral', 'definite-integrals', 'polar-coordinates']"
3903565,Can $ y(t)=\left((t-1)^2+1\right)\cdot \cos(t) $ be a solution for this ODE,"I've been asked if $$
y(t)=\left((t-1)^2+1\right)\cdot \cos(t)
$$ can be a solution for the ODE: $$
y''+y+(y-\cos(t))P(t,y,y') = 0
$$ when $P$ is smooth. So I concluded that there exists a unique solution to the ODE, by Peano's theorem if: $$
y_{1}=y,\quad y_{2}=y'.
$$ Then the ODE is of the form: $$
y_{2}'=-y_{1}-P(t,y_{1},y_{2})(y_{1}-\cos(t)).
$$ $P$ is smooth and a function of $y_{2}$ and $y_{1}$ , therefore the ODE has a unique solution, but I don't know how to continue any further.  Any help/hints would be appreciated!",['ordinary-differential-equations']
3903626,Diffeomorphism from $n$-cone to $\mathbb{R}^{n+1}$ -- parametric equation of geodesic?,"Let $\mathbb{S}^n=\{\mathbf{x}\in\mathbb{R}^{n+1}\colon\lVert\mathbf{x}\rVert^2=1\}$ be the $n$ -sphere. We know that a bijection from $\mathbb{S}^n\backslash \{N\}$ to $\mathbb{R}^n\times\{0\}\subset\mathbb{R}^{n+1}$ , where $N$ is, for instance, the north pole of $\mathbb{S}^n$ , is given by stereographic projection. Moreover, we know that a great circle on $\mathbb{S}^n$ can be given analytically, parametrized for instance by an angle $\theta$ and with respect to some basis where this geodesic belongs. I am trying to figure out if any of the following can be defined in a similar manner: Can an $n$ -dimensional cone be defined similarly to the $n$ -sphere, i.e., as a subset of $n+1$ -dimensional Euclidean space, given a formula? If so, is there any injection (more specifically a diffeomorphism) from the $n$ -cone manifold to the Euclidean $\mathbb{R}^{n+1}$ ? Can a geodesic on $n$ -cone be parametrized in a similar way? For instance, if we define a $2$ -plane using a set of $n+1$ -dimensional vectors, can we parametrized the conic section in the general case?","['geodesic', 'diffeomorphism', 'differential-geometry']"
3903709,"If $f$ measurable and $f=g$ a.e. implies $g$ measurable, then $\mu$ is complete","This is the Proposition 2.11 of the book Real Analysis from Folland . The following implications are valid if and only if the measure is complete: (a) If $f$ is measurable and $f = g$ $\mu$ -a.e., then $g$ is measurable. (b) If $f_n$ is measurable for $n\in \mathbb{N}$ and $f_n\rightarrow f$ $\mu$ -a.e., then $f$ is measurable. The complete solution to this question can be found in this answer . But note that it was considered that $f,g:(X,\mathcal{M})\to(\mathbb{R},\mathcal{B}_{\mathbb{R}})$ , other solutions like [1] or others that I found on the internet also assume that the image is $\mathbb{R}$ or $\overline{\mathbb{R}}$ . I'm thinking about the general case, where, $f:(X,\mathcal{M})\to(Y,\mathcal{N})$ .
In this answer , the proposition $\mu$ is complete $\Rightarrow$ (a) is demonstrated in the general case. I tried to show the opposite direction but I couldn't, in the general case is this proposition true? How to demonstrate this?","['measure-theory', 'real-analysis']"
3903774,$30$ red balls and $20$ black balls are being distributed to $5$ kids. Each kid gets at least one red ball. In how many ways can we distribute balls?,"$30$ red balls and $20$ black balls are being distributed to $5$ kids, so that each kid gets at least one red ball. In how many ways can we distribute balls? Circle the correct answers: a) $\binom{29}{4}$ $\binom{24}{20}$ b) $\binom{29}{5}$ $\binom{24}{5}$ c) $|Sur(N_{30},N_{5})|S(20,5)$ Note: $|Sur(N_{30},N_{5})|$ is the number of surjections, and $S(20,5)$ is a Stirling number of the second kind d) None of $3$ previous answers are correct My approach: First I gave each of $5$ kids one red ball, which leaves me with $25$ red balls. Now I used the stars and bars method to distribute the balls I am left with. Red balls: $x_1+x_2+x_3+x_4+x_5=25,x_i\geq 0, i=1,..,5$ . This equation has $\binom{5+25-1}{25}=\binom{29}{25}=\binom{29}{4}$ . Black balls: $x_1+x_2+x_3+x_4+x_5=20,x_i\geq 0, i=1,..,5$ . This equation has $\binom{5+20-1}{20}=\binom{24}{20}=\binom{24}{4}$ . So I would say a) is the correct answer.. am I right?","['combinatorics', 'discrete-mathematics']"
3903784,Homeomorphic metric spaces where the identity map is not a homeomorphism,"In our Topology lectures, we were told that there are examples where two metric spaces $(M, d)$ and $(M, d')$ are homomeomorphic (i.e. there exists some homeomorphism between them, which is not necessarily the identity map) but the identity map $\operatorname{Id}\colon(M, d) \rightarrow (M, d')$ is not a homeomorphism. I was hoping someone could provide an example of this.","['general-topology', 'metric-spaces', 'examples-counterexamples']"
3903822,Basis of the space of vector fields on smooth manifolds,"Let $M$ be a smooth ( $C^\infty$ ) manifold. Let $\mathfrak{X}(M)$ be a set of all vector fields on $M$ and let $\mathfrak{F}(M)$ be a set of all real smooth functions on $M$ . $\mathfrak{X}(M)$ is a real vector space and it is also a module over $\mathfrak{F}(M)$ . We know that partial derivatives constitute a basis for tangent space at any point $p\in M$ .
Is there some sort of basis for $\mathfrak{X}(M)$ (as a vector space or as a module)? Do partial derivatives constitute a basis here as well? I think this question Basis of vector fields on manifold is similar to mine, but because of the way it's written, I'm not really sure I understand the question and I'm not sure we're asking about the same thing.","['vector-fields', 'tangent-spaces', 'smooth-manifolds', 'tangent-bundle', 'differential-geometry']"
3903840,Probability of receiving a job offer on 3rd application,"Suppose that when you submit a job application you have a probability of $0.1$ to receive an interview, and a job interview results in a job offer with probability $0.2$ . Also assume you only submit one application at a time (i.e. you wait to know if you have been rejected from the job, either at the application stage or the interview stage before applying to another job). What is the probability of getting your first job offer after submitting at most 3 job applications? Attempt: Either you get a job offer after your first, second, or third application. The probability of getting an offer after your 1st application is $(0.1)(0.2)=0.02$ . The probability of getting an offer after your 2nd application is $(1-0.02)(0.1)(0.2)=0.0196$ The probability of getting an offer after your 3rd application is $(1-0.02)(1-0.02)(0.1)(0.2)=0.0192$ Thus, $0.02+0.0196+0.0192=0.0588$ I don't think my approach is correct, it doesn't make sense to me that you have  a higher probability of getting an offer on your first application than you do on your 2nd or 3rd. I'm stumped as to what the mistake is that I am making.",['probability']
3903865,Summing a series having a geometric component,If $$\alpha=\frac {5}{2!3}+\frac {5\cdot 7}{3!3^2}+\frac {5\cdot 7\cdot 9}{4!3^3}+\dots $$  Find the value of $\alpha^2+4\alpha $. The possible options are: 21 23 25 27 I think $$\alpha=\sum \frac {(2n+4)!}{2^{n+2}(n+1)!(n+2)!3^{n+1}}$$ but cannot think of what else to do.,"['summation', 'sequences-and-series']"
3903882,Covering a polygon with an odd number of sides,"I have the following elementary problem/question that I do not know how to tackle. It comes with a ""math-olympiad-flavor"" but I suspect it may be much more difficult than an high-school olympiads problem. Let $\mathscr{P}$ be a simple polygon (not necessarily convex) in the plane, and such that the number of sides of $\mathscr{P}$ is an odd number $2k+1$ . For each vertex $v$ of $P$ there is an intuitive notion of opposite side (because the number of sides is odd), just label the sides counterclockwise at $v$ from $1$ to $2k+1$ , and it would be the one with the label $k+1$ . Now suppose that for each vertex and its corresponding opposite side I consider the triangle the determine in the plane. Let us call $\Delta_v$ this triangle and call it a central triangle . Is the following: $$ \mathscr{P} \subseteq \bigcup_{v\in \mathscr{P}} \Delta_v$$ always true? In other words, if one paints all central triangles is it true that the whole polygon is then painted?","['combinatorial-geometry', 'combinatorics', 'geometry', 'plane-geometry']"
3903883,The product of which 6 primes is 201-times larger than their sum?,"I have been able to find the answer via exhaustion. I defined $a\le b\le c\le d\le e\le f$ as the primes and solved an inequality for each variable. The answer is $2,2,3,3,7,67$ . However, the number of cases to solve got quite large and I was wondering if there is any neat solution to this problem.","['alternative-proof', 'number-theory', 'prime-factorization', 'prime-numbers']"
3903920,Examples of a K3 surface in a product of $\mathbb P^1$'s,"On the Wikipeida page for K3 surfaces,
there are several examples listed of how to produce a K3 surface
as a subvariety of
projective space
by taking polynomials of specified degrees.
Namely, a K3 surface is cut out by a degree 4 polynomial in $\mathbb P^3$ , a degree 2 and a degree 3 polynomial in $\mathbb P^4$ , and three degree 2 polynomials in $\mathbb P^5$ . Are there similar examples
of how to get a  K3 surface
as an explicit subvariety of $\mathbb P^1 \times \cdots \times \mathbb P^1$ ?
Or is there a reason this is not possible?","['algebraic-geometry', 'k3-surfaces']"
3903935,"Are there functions $f,g:\mathbb{R} \to \mathbb{R}$ such that they differentiate each other?","i.e. $$f'(x) = (g \circ f)(x)$$ $$g'(x) = (f\circ g)(x)$$ I came up with this question a few years ago. A friend found the only example I know: for $c\in\mathbb{R}$ $$f(x) = c$$ $$g(x) = cx - c^2 $$ After trying with some particular cases (with no success), I used the formula for the derivative of the inverse function and got that if $f$ and $g$ are bijective then, $$f^{-1} = \int{}{dt \over g(t)}  $$ $$g^{-1} = \int {dt \over f(t)}$$ Assuming all conditions neccesary for this to be possible. I tried using this fact to construct the functions, again with no luck. I would really appreciate any insight on how to tackle this problem.","['functions', 'derivatives', 'real-analysis']"
3903950,"Why the set of polynomials is not closed in $C[0,1]$","This question starts from the following theorem: ""Every finite dimensional subspace $Y$ of
a normed space $X$ is closed in $X$ "" (Kreyzig 2.4-3) So the given counterexample for an infinite dimension subspace, is that if we take $X=C[0,1]$ and $Y = span\{x_0,x_1,x_2...\}$ , where $x_j = t^j$ , then $Y$ is not closed. I guess this is the case because for example a sine function can be expressed as the limit point of a sequence of polynomials that converge uniformly (Taylor series). But why is the sine function not considered as a polynomial generated by an infinite basis? After all, we are explicitly saying $Y$ is infinite dimensional. For an infinite dimensional basis case, I would expect to have a mapping between every sequence to a polynomial in $C[0,1]$ . If this is not the case, what exacly does it mean for $span\{x_0,x_1,x_2...\}$ to be considered infinite dimensional? If I input an infinite basis, it seems like it is not longer a polynomial. I am confused.","['normed-spaces', 'polynomials', 'functional-analysis']"
3903987,Is $\sum_{n\ge0}(-1)^n\frac{\Gamma(\tfrac{n+1}{2})}{\Gamma(\tfrac{n}2+1)}=\frac{2}{\sqrt{\pi}}$ true?,"Prove/disprove $$\sum_{n\ge0}(-1)^n\frac{\Gamma(\tfrac{n+1}2)}{\Gamma(\tfrac{n}2+1)}=\frac{2}{\sqrt{\pi}}.\tag 1$$ As far as I can tell, this is true, although it seems to converge very slowly . I came up with a proof but I don't know if it's valid. Let $$J=\int_0^\pi \frac{xdx}{1+\sin x}.$$ On one hand, we have $$\frac1{1+\sin x}=\sum_{n\ge0}(-1)^n\sin(x)^n,$$ so that $$J=\sum_{n\ge0}(-1)^n p_n,\tag 2$$ where $$
\begin{align}
p_n&=\int_0^\pi x\sin(x)^ndx\\
&=\int_\pi^0 -(\pi-x)\sin(\pi-x)^ndx\\
&=\pi\int_0^\pi\sin(x)^ndx-p_n\\
\Rightarrow p_n&=\frac\pi2\int_0^\pi\sin(x)^ndx.
\end{align}
$$ And since $\sin(x)=\sin(\pi-x)$ , $$p_n=\pi\int_0^{\pi/2}\sin(x)^ndx=\frac{\pi^{3/2}}{2}\frac{\Gamma(\tfrac{n+1}2)}{\Gamma(\tfrac{n}2+1)}.\tag 3$$ On the other hand, we have $1+\sin x=2\sin^2(\tfrac{x}2-\tfrac\pi4)$ , so that $$\begin{align}
J&=\frac12\int_0^\pi\frac{xdx}{\sin^2(\tfrac{x}2-\tfrac\pi4)}\\
&=2\int_{\pi/4}^{3\pi/4}\frac{tdt}{\sin^2t}-\frac\pi2\int_{\pi/4}^{3\pi/4}\frac{dt}{\sin^2 t}\\
&=2\left(\ln\sin x-x\cot x\right)\bigg|_{\pi/4}^{3\pi/4}-\frac\pi2\left(-\cot x\right)\bigg|_{\pi/4}^{3\pi/4}\\
&=2\pi-\frac\pi2\cdot2=\pi.
\end{align}$$ Then from $(2)$ and $(3)$ , we have $$\frac{\pi^{3/2}}{2}\sum_{n\ge0}(-1)^n\frac{\Gamma(\tfrac{n+1}2)}{\Gamma(\tfrac{n}2+1)}=\pi,$$ which is equivalent to $(1)$ . $\square$ Can you come up with any other proofs to $(1)$ ? Thanks! Edit (11/12/2020): Here is a proof that the interchange of the sum and integral in $(2)$ is valid. The partial sums $$S_M(x)=\sum_{n=0}^M(-1)^n\sin(x)^n$$ form a uniformly convergent sequence of functions for $x$ in $[0,\pi/2)$ or $(\pi/2,\pi]$ , and they converge to the limit $$\lim_{M\to\infty}S_M(x)=\frac1{1+\sin x},\qquad x\in[0,\pi]\setminus\{\pi/2\}.$$ Choose $\varepsilon>0$ and notice that $$J=\int_{0}^{\pi}\frac{xdx}{1+\sin x}=\int_{\pi/2-\varepsilon}^{\pi/2+\varepsilon}\frac{xdx}{1+\sin x}+\int_0^{\pi/2-\varepsilon}\frac{xdx}{1+\sin x}+\int_{\pi/2+\varepsilon}^\pi\frac{xdx}{1+\sin x}.$$ The sums $S_M(x)$ converge uniformly as $M\to\infty$ when $x\in[0,\pi/2-\varepsilon]\cup[\pi/2+\varepsilon,\pi]$ , so we can interchange the sum and integral to get $$J=\int_{\pi/2-\varepsilon}^{\pi/2+\varepsilon}\frac{xdx}{1+\sin x}+\sum_{n\ge0}(-1)^n(a_n(\pi/2-\varepsilon)+b_n(\pi/2+\varepsilon)),$$ where $$\begin{align}
a_n(t)&=\int_0^t x\sin(x)^ndx\\
b_n(t)&=\int_t^\pi x\sin(x)^ndx.
\end{align}$$ We have $a_n(t)+b_n(t)=p_n$ for all $t\in[0,\pi]$ . As $\varepsilon$ approaches $0$ , $\int_{\pi/2-\varepsilon}^{\pi/2+\varepsilon}\frac{xdx}{1+\sin x}$ approaches $0$ . And since $a_n(t), b_n(t)$ are continuous, $a_n(\pi/2-\varepsilon)+b_n(\pi/2+\varepsilon)$ approaches $a_n(\pi/2)+b_n(\pi/2)=p_n$ , so that $$J=\sum_{n\ge0}(-1)^np_n$$ as desired. $\square$","['integration', 'special-functions', 'alternative-proof', 'calculus', 'sequences-and-series']"
3904064,Exercise about vector fields on smooth manifolds,"Let $V$ be a vector field on $\mathbb{R}^2$ . If $[\frac{\partial}{\partial x},V]=V=[V,\frac{\partial}{\partial y}]$ , determine $V$ . The first way to look at this is by observing vector fields primarily as functions from $\mathfrak{F}(\mathbb{R}^2)$ to $\mathfrak{F}(\mathbb{R}^2)$ . Obviously, if $D=\frac{\partial}{\partial x}+\frac{\partial}{\partial y}$ , then $[D,V]=0$ .
Writing it out this way: $$\frac{\partial}{\partial x}(V(f))-V(\frac{\partial}{\partial x}f)=V(f)=V(\frac{\partial}{\partial y}f)-\frac{\partial}{\partial y}(V(f))$$ , we can also see that $$\frac{\partial}{\partial x}(V(f))=V(f+\frac{\partial}{\partial x}f)$$ $$\frac{\partial}{\partial y}(V(f))=V(\frac{\partial}{\partial y}f-f)$$ . Frankly, I don't know what to do with this. Jacobi identity doesn't give me anything new. The second way to look at this is to observe these equations locally, at some point $p\in M$ . Then we'd be able to write $V$ as $V=\alpha \frac{\partial}{\partial x}+\beta \frac{\partial}{\partial y}$ (or at least I hope we can do this?). However, observed locally, given equations become $0=V=0$ , which looks like it's incorrect, because it would mean I have more data than I need. Other idea was to use the fact that we're working in $\mathbb{R}^2$ and that this is a manifold with one chart basically, but this comes down to the previous idea and the result is the same. I feel like $[D,V]=0$ is significant here, but I don't know what to do with it. Also, it seems that my attempt at observing things locally is completely flawed. Can we even use the fact that partial derivatives constitute a basis of tangent space the way I've used it?","['vector-fields', 'tangent-spaces', 'smooth-manifolds', 'tangent-bundle', 'differential-geometry']"
3904071,An example about a non-commutative division ring with finite characteristic,"After reading the proof of the theorem “For every central division $F$ -algebra $D$ with $D$ $\neq$ $F$ , $D$ contains a separable extension $K \supsetneqq F$ “, I have a question: dose there exist a non-commutative division ring $D$ with finite characteristic, which contains an $x\in D$ such that $x^n \not\in Z(D)$ for all positive integers n? First I tried quaternion algebra over  field $F_p(X)$ . Let $i^2= x$ , $j^2= y$ , $ij=k$ , $ji=-k$ where $x, y$ are any non-zero elements in $F_p(X)$ , the quaternion algebra $H(F_p(X))$ is a vector space over $F_p(X)$ with a basis $\{1,i ,j, k\}$ . Let $α =a+ib+jc+kd$ , $\bar{α}=a-ib-jc-kd$ , $α\bar{α}= \bar{α}α= a^2-xb^2-yc^2+xyd^2$ , then $H(F_p(X))$ is a division ring if and only if $α\bar{α}= 0$ implies $α= 0$ .
After trying some examples, I find that the consequent processes are hard. Does there exist some good example that satisfies the property in the question?","['abstract-algebra', 'noncommutative-algebra', 'modules']"
3904134,Is a single element a Lattice?,"Can a lattice be formed with just a single element? Because for a single element, both Greatest Lower Bound and Least Upper bound exists and is equal to the same element. So can this element be called as a lattice?","['lattice-orders', 'order-theory', 'discrete-mathematics']"
3904140,Details of Defining Free Group in Terms of Universal Property,"The only definition of the free group I have seen thus far (undergraduate) is an explicit construction
via reduced words and concatenation. What I am now trying to do is see if it is possible to define the free group abstractly in terms of the universal property. The definition I think should work is: A free group $F_S$ on a set $S$ is a group generated by $S$ such that for any group $G$ and any function $f: S \rightarrow G$ , $f$ extends to a unique homomorphism $\tilde{f}: F_S \rightarrow G$ . So what I want to do now is prove existence and uniqueness (up to isomorphism?) from this abstract definition, and I'm struggling to make progress, so I'm looking for tips, specifically: Is my definition correct, or is there something I'm missing? Any tips on how to make progress on existence? Any tips on how to make progress on uniqueness?","['universal-property', 'group-theory', 'free-groups']"
3904146,$0\leq a \leq b $ implies $ b^{-1} \leq a^{-1}$ in C*-algebra?,"I'm studying C*-algebra in Conway's Functional Analysis. Problem is this. $0\leq a \leq b $ , then $ b^{-1} \leq a^{-1}$ in C*-algebra? I tried this problem via this way. 1st trial) Since $  a^{-1} - b^{-1} = a^{-1}(b-a)b^{-1}$ so product of three positive element is positive. But this is false because product of positive element in C*-algebra is not positive. 2nd trial) How about using below Lemma? $a \geq 0$ is equivalent to $a= a^*, ||t-a||\leq t$ for some $t \geq ||a|| $ Then, since $||a^{-1}(b-a)b^{-1}||  \leq ||a^{-1}|| \cdot ||b-a|| \cdot ||b^{-1}||$ so set $t = ||a^{-1}|| \cdot ||b-a|| \cdot ||b^{-1}||$ and try to solve this problem. What I stuck is is $||a||^{-1} = ||a^{-1}||$ ? It is just kind of passing thought, but in generally $a$ is hermitian, then $||a^{2^n}|| = ||a||^{2^n}$ so i think this might be true. Can you help me?",['functional-analysis']
3904166,Characterization of a virtually nilpotent group,Let $G$ be a group that has an increasing sequence of subgroups $G_i \le G_{i+1}$ satisfying the following properties. (a) $G=\bigcup_{i\ge 1} G_i$ . (b) Each $G_i$ contains a nilpotent subgroup $N_i$ such that $[G_i:N_i] \le k$ for some constant $k$ independent of $i$ . Can we find a nilpotent subgroup $N \le G$ such that $[G:N]<\infty$ ?,"['nilpotent-groups', 'group-theory', 'abstract-algebra']"
3904217,Eigenvalues and Eigenvectors of $A=I-\alpha vv^{T}$,"Consider the matrix $A$ given by $A=I-\alpha vv^{T}$ with $v\neq0$ and $v\in\mathbb{R}^{n}$ and $\alpha\neq0$ . we want to show that there are two distinct eigenvalues $\lambda_{1},\lambda_{2}$ to be found with their corresponding eigenvectors $x_{\lambda_{1}}$ and $x_{\lambda_{2}}$ . My attempt : By definition, we have that $Ax=\lambda x$ thus : $$
(I-\alpha vv^{T})x=x-\alpha vv^{T}x=x-(\alpha v^{T}x)v
$$ One can easily notice that $v$ is nothing but a scalar multiple of $x$ that is to say $x=\beta v$ and thus we have that for $x=v$ we get : $$
Av=(1-\alpha v^{T}v)v
$$ Thus, an eigenvalue of $A$ is $\lambda_{1}=1-\alpha v^{T}v$ . I am unable to find the second eigenvalue nor the corresponding eigenvectors. I would truly appreciate help as I am lost in the process.","['linear-algebra', 'eigenvalues-eigenvectors']"
3904338,Differentials on hyperelliptic curves,"The following is an exercise of Vakil's Foundations of Algebraic Geometry . We consider a hyperelliptic curve $C$ of genus $g$ over an algebraically closed field $k$ of characteristic not $2$ . Associated to $C$ is a double cover $\pi \colon C \to \mathbb{P}^1$ of the projective line. This cover has $2g + 2$ ramification points $p_1,\ldots,p_{2g + 2}$ which we may assume to lie on an affine open cover $U = \operatorname{Spec} k[x]$ of $\mathbb{P}^1$ . I am asked to compute the zeros and poles of $\pi^* dx$ . I do not know how to proceed. From what I understand, we can think of $C \times_{\mathbb{P}^1} U$ as $\operatorname{Spec} k[x,y] / \big(y^2 - (x - p_1) \cdots (x - p_{2g+2})\big)$ , and so on this space we may understand $\Omega^1$ as being the module generated by $dx$ and $dy$ satisfying the relationship $$2y \,dy = \bigg(\frac{d}{dx}(x-p_1)\cdots(x-p_{2g+2})\bigg)\,dx,$$ and I presume that the $dx$ in this module corresponds to the $\pi^* dx$ we are interested in. But then what? I'm not sure I even know what it means for $\pi^* dx$ to have a zero or a pole anywhere, or how this can be seen from the formula above.","['algebraic-curves', 'curves', 'algebraic-geometry']"
3904350,What justifies the existence of two independent rvs $\overline{X}$ and $\overline{Y}$ that have the same distribution as some rvs $X$ and $Y$,"What justifies the existence of two independent rvs $\overline{X}$ and $\overline{Y}$ that have the same distribution as some rvs $X$ and $Y$ In the proof: $X,Y$ are independent random variables $\iff$ $E[\exp(i\langle (t,s),(X,Y)\rangle)]=E[\exp(i\langle t,X\rangle)]E[\exp(i\langle s,Y\rangle)](*)$ for the $\Leftarrow$ part, I have seen a proof that uses the fact there exist independent random variables $\overline{X}$ and $\overline{Y}$ with $X$ ~ $\overline{X}$ and $Y$ ~ $\overline{Y}$ . My question pertains to why it is always possible to find such independent random variables $\overline{X}$ and $\overline{Y}$ that follow such a distribution. To me this assertion seems very far-reaching and I am having trouble to believe that we are always able to construct such an $\overline{X}$ and $\overline{Y}$ . Any intuition or proofs?","['fourier-analysis', 'real-analysis', 'probability-theory', 'probability', 'random-variables']"
3904355,Does $\Vert f\Vert_{L^\infty}^2\leq \Vert f\Vert_{L^2}\Vert f'\Vert_{L^2}$ hold?,"I know that Sobolev's embedding implies that $H^1(\mathbb{R})\hookrightarrow C(\mathbb{R})\cap L^\infty(\mathbb{R})$ , which in particular implies that $$
\Vert f\Vert_{L^\infty}\leq \Vert f\Vert_{H^1}.
$$ Now, I was wondering about the following slightly more specific (?) inequality. More precisely, I was wondering if $f\in H^1(\mathbb{R})$ , then the following inequality holds: $$
\Vert f\Vert_{L^\infty}^2\leq \Vert f\Vert_{L^2}\Vert f'\Vert_{L^2}.
$$ The problem here is to be able to put a factor only with the $L^2$ norm of $f$ . Does this inequality hold?","['measure-theory', 'sobolev-spaces', 'partial-differential-equations']"
3904359,Non-supercuspidal representations are principal series,"I am reading Godement Jacquet's notes on Jacquet Langlands. I have an issue justifying a certain point. I want to prove If $\pi$ is an irreducible admissible representation of $GL_2(\mathbb{Q}_p)$ that is not supercuspidal, then it is isomorphic to a ""principal series"" $\rho_{\mu_1, \mu_2}$ for certain characters $\mu_1, \mu_2$ . The proof is pretty quick. We already know that, letting $K$ the Kirillov model of $\pi$ and $S(\mathbb{Q}_p^\times)$ the space of locally constant compactly supported functions on $\mathbb{Q}_p$ , we have $K/S(\mathbb{Q}_p^\times)$ is of finite dimension. Supposing $\pi$ non supercuspidal means that we suppose this dimension $\geq 2$ . Now here is the point where I am missing something: Godement claims that there is necessarily a non-zero linear form $B$ and characters $\mu_1, \mu_2$ of $\mathbb{Q}_p^\times$ such that $$B\left(\pi \pmatrix{a & \star \\ & b}\xi\right) = \mu_1(a) \mu_2(b) |a/b|^{1/2} B(\xi)$$ This is pretty enough to settle the proof since that already gives functions in the principal series. However, why can we conclude to the existence of this linear functional? Why specifically of this form?","['number-theory', 'automorphic-forms', 'representation-theory']"
3904392,What is the Real Prime?,"There seems to be an importance to the ring of adeles for the rational numbers (discussed here ), with valuations for every $\mathbb{Q}_p$ , but also one ""infinite"" valuation "" $\mathbb{Q}_∞$ "", seemingly equal to $\mathbb{R}$ . Why would something like $\mathbb{Q}_∞$ be used in the first place, and how is that equal to the reals? Is there something like a $∞$ -adic metric that works like the usual one? Moreover, it seems to suggest that $∞$ here is a sort of an infinite prime number, i.e. the real prime, having some occult-sounding books written about it. So, does it exist as some sort of a describable object here, or is it just notation?","['number-theory', 'p-adic-number-theory', 'adeles', 'prime-numbers']"
3904449,Is the function injective if the Jacobian has full column rank?,"Let $f:\mathbb{R}^m \to \mathbb{R}^n: x \to f(x)$ be a continuous and differentiable function with $m < n$ . If the Jacobian $J_f$ has full column rank (i.e., rank= $m$ ) $\forall x \in \mathbb{R}^m$ , does this imply that $f$ is an injective function? If yes, can I get a reference for this result?","['multivariable-calculus', 'linear-algebra']"
3904459,"In $\Delta ABC, AC > AB.$ The internal angle bisector of $\angle A$ meets $BC$ at $D,$ and $E$ is the foot of the perpendicular from $B$ onto $AD$.","In $\Delta ABC, AC > AB.$ The internal angle bisector of $\angle A$ meets $BC$ at $D,$ and $E$ is the foot of the perpendicular from $B$ onto $AD$ . Suppose $AB=5,BE=4$ and $AE=3$ . Find $\Big(\frac{AC + AB}{AC - AB}\Big)ED$ . What I Tried : Here is a picture :- The picture only shows it. I have no other idea to try it, first that angle-chasing has no use here. Second is that I only have the required information of one right triangle, so no Pythagoras Theorem here.
I got no similar triangles, or even if I got one pair, I cannot show they are similar and so on, and in the end I am weak at Trigonometry. Is there any way to solve this using basic techniques in Geometry? (Like angle-chasing , similarity , area , pythagorean theorem and so on) . Thank You. Edit :- After @Michal Adamaszek's hint, I was able to do a bit of progress. (everything is there in the picture). Some congruent and similar triangles came up, but now I don't know what to do next. I need to find the value of $AC$ and $DE$ , is there any way to do it?","['triangles', 'problem-solving', 'geometry']"
3904560,Prove $\lim\limits_{n \to \infty} na_n=0.$,"Suppose $\sum_{n=1}^{\infty} a_n$ be a convergent positive series, and
satisfy $\lim\limits_{n \to \infty}\frac{a_{n+1}}{a_n}=1$ . Prove $\lim\limits_{n \to \infty} na_n=0.$ First, we may consider applying Cauchy's condensation test. Since $\sum_{n=1}^{\infty}a_n$ is convergent, $\sum_{n=1}^{\infty} 2^n a_{2^n}$ is convergent as well, which implies $\lim\limits_{n \to \infty}2^n a_{2^n}=0$ . Hence, there already exists a subsequence of $\{na_n\}$ convergent to zero. But how to go on ?","['limits', 'calculus', 'sequences-and-series', 'real-analysis']"
3904578,Rank preservation of Hankel matrix by adding constrained sample,"Let some $x_i \in \mathbb{R}$ for every $i$ such that the Hankel matrix $$H_0=\begin{bmatrix} x_0 & x_1 & x_2 & x_3\\
x_1 & x_2 & x_3 & x_4\\
x_2 & x_3 & x_4 & x_5
\end{bmatrix} $$ is full rank, with the rank equal to 3. Show that there exists some $y\in [-1,1]$ such that the following Hankel matrix is full rank too $$H_1=\begin{bmatrix}  x_1 & x_2 & x_3 & x_4\\
 x_2 & x_3 & x_4 & x_5\\
 x_3 & x_4 & x_5 & y
\end{bmatrix} $$ The matrix $H_1$ is obtained by deleting the first column of $H_0$ and adding a new column as $[x_4 \ x_5 \ y]^\top$ . I tried to solve this by showing that there exists some $a_0,a_1,a_2,a_3\in \mathbb{R}$ with $a_0\neq 0$ and some $y_0\in [-1,1]$ such that \begin{equation}
\begin{bmatrix}
x_4\\ x_5\\ y_0
\end{bmatrix}= \begin{bmatrix} x_0 & x_1 & x_2 & x_3\\
x_1 & x_2 & x_3 & x_4\\
x_2 & x_3 & x_4 & x_5
\end{bmatrix} \begin{bmatrix}
a_0\\ a_1 \\ a_2 \\ a_3
\end{bmatrix}
\end{equation} But still couldn't come up with a formal proof for it. Do you have any suggestion?","['matrices', 'matrix-rank', 'linear-algebra', 'hankel-matrices']"
3904597,Degenerate Pi-notation,"can i say that a degenerate productory like $$
\prod_{t=1}^{0} (1+r_t)
$$ is equal to one? I cant seem to find a precise answer about this anywhere.",['discrete-mathematics']
3904617,"Finding Grad of G,","OK, so I'm out of ideas. The question is:
Let  f(x,y) be a function with continous partial derivatives that upholds: $$
\bigtriangledown f (0,-8) = (-3\widehat{i}, 5\widehat{j})
$$ let g(x,y) be the function : $$
g(x,y) = f(xy+x^2 , xy-y^2)
$$ Calculate: (a Vector) $$
\bigtriangledown g(-2,2) = ?
$$ My take on this: $$
\bigtriangledown g = \left(\frac{\partial \:g}{\partial \:x},\:\frac{\partial \:g}{\partial \:y}\right)
$$ let u,v be: $$
u\left(x,y\right)\:=\:xy+x^2\:\:,\:v\left(x,y\right)\:=\:xy-y^{2\:}
$$ and then By the chain Rule: $$
\frac{\partial g}{\partial x}\:=\:\frac{\partial g}{\partial u}\:\cdot \:\frac{\partial u}{\partial x}\:+\:\frac{\partial g}{\partial v}\cdot \frac{\partial v}{\partial x}
$$ $$
\frac{\partial g}{\partial y}\:=\:\frac{\partial g}{\partial u}\:\cdot \:\frac{\partial u}{\partial y}\:+\:\frac{\partial g}{\partial v}\cdot \frac{\partial v}{\partial y}
$$ Placing du/dx du/dy and dv/dx dv/dy , and evaluating at the point (-2,2) $$
\frac{\partial g}{\partial y}\:=\:+2\frac{\partial g}{\partial u}\:+6\:\frac{\partial g}{\partial v}
$$ $$
\frac{\partial g}{\partial x}\:=\:+2\frac{\partial g}{\partial u}\:-2\:\frac{\partial g}{\partial v}
$$ My question is how to procceed further?, What do i do with The partial derivatives of g with respect to u and v.","['scalar-fields', 'multivariable-calculus', 'calculus', 'vector-analysis', 'chain-rule']"
3904630,"If sum of triangle angles is $180$ degrees, how $\sin(270)$ is possible?","I'm not new to trigonometry, but this question always bothers me. As it is in Wolfram MathWorld - $$
\sin(\theta)=\frac{\text{opposite}}{\text{hypotenuse}}
$$ We know that the sum of the angles in a triangle is 180 degrees, which means each angle is less than 180 degrees, so how, for example, $\sin 270$ is possible? Edit After Joe posted his answer , I went back to MathWorld and read it more carefully. I noticed what Brian Drake mentioned in his answer . MathWorld has already explained both definitions. I kept the question because of the brilliant answers below.","['trigonometry', 'definition']"
3904705,Calculation on the Power of Means (Probability),"The problem is: Suppose $X\ge 0$ is a random variable, $p(x)$ is its probability density function. If $\operatorname EX$ exists, $r>0$ , prove: $$\operatorname EX^r = \int_0^\infty rx^{r-1}P(X>x) \, dx.$$ I see $\operatorname E X^r$ as $\int_{-\infty}^\infty x^r p(x) \, dx$ , while $$\text{RHS}=\int_0^\infty \left(\int_x^\infty p(t)\,dt \right) dx^r = \left.\left(x^r\int_x^\infty p(t) \, dt\right)\right|_0^\infty-\int_0^\infty x^r p(x) \, dx.$$ However, when calculating $x^r \int_x^\infty p(t) \, dt$ as $x\to \infty$ , it comes to a $0 \times \infty$ problem. How can I solve it? Thanks.","['integration', 'expected-value', 'probability-theory', 'probability']"
3904732,"What are ways to compute polynomials that converge from above and below to a continuous and bounded function on $[0,1]$?","Main Question Suppose $f:[0,1]\to [0,1]$ is continuous and belongs to a large class of functions (for example, the $k$ -th derivative, $k\ge 0$ , is continuous, Lipschitz continuous, concave, strictly increasing, bounded variation, and/or in the Zygmund class, or $f$ is real analytic). Then, compute the Bernstein coefficients of a sequence of polynomials ( $g_n$ ) of degree 2, 4, 8, ..., $2^i$ , ... that converge to $f$ from below and satisfy: $(g_{2n}-g_{n})$ is a polynomial with non-negative Bernstein coefficients once it's rewritten to a polynomial in Bernstein form of degree exactly $2n$ . Assume $0\lt f(\lambda)\lt 1$ or $f$ is polynomially bounded. The convergence rate must be $O(1/n^{r/2})$ if the class has only functions with Lipschitz-continuous $(r-1)$ -th derivative.  The method may not introduce transcendental or trigonometric functions (as with Chebyshev interpolants). See ""Strategies"", below, for different ways to answer this question. In this question, a polynomial $P(x)$ is written in Bernstein form of degree $n$ if it is written as— $$P(x)=\sum_{k=0}^n a_k {n \choose k} x^k (1-x)^{n-k},$$ where $a_0, ..., a_n$ are the polynomial's Bernstein coefficients . The degree- $n$ Bernstein polynomial of an arbitrary function $f(x)$ has Bernstein coefficients $a_k = f(k/n)$ .  In general, this Bernstein polynomial differs from $f$ even if $f$ is a polynomial. Background I asked this question in order to solve the so-called Bernoulli factory problem , described next. We're given a coin that shows heads with an unknown probability, $\lambda$ . The goal is to use that coin (and possibly also a fair coin) to build a ""new"" coin that shows heads with a probability that depends on $\lambda$ , call it $f(\lambda)$ . This is the Bernoulli factory problem , and it can be solved only if $f$ is continuous (Keane and O'Brien 1994). However, since I asked this question I have found a Bernoulli factory algorithm that I believe is general enough to cover all the cases that this question would help solve. Since this question may be of broader interest, though, I leave this question open.  See also my other open questions about the Bernoulli factory problem. Polynomials that approach a factory function An algorithm simulates a factory function $f(\lambda)$ via two sequences of polynomials that converge from above and below to that function.  To use the algorithm, however, the polynomial sequences must meet certain requirements; among them, the sequences must be of Bernstein-form polynomials that converge from above and below to a factory function.  Specifically: For $f(\lambda)$ there must be a sequence of polynomials ( $g_n$ ) in Bernstein form of degree 1, 2, 3, ... that converge to $f$ from below and satisfy: $(g_{n+1}-g_{n})$ is a polynomial with non-negative Bernstein coefficients once it's rewritten to a polynomial in Bernstein form of degree exactly $n+1$ ( see end notes ; Nacu and Peres 2005; Holtz et al. 2011).  For $f(\lambda)=1-f(\lambda)$ there must likewise be a sequence of this kind. A Matter of Efficiency However, ordinary Bernstein polynomials converge to a function at the rate $\Omega(1/n)$ in general, a result known since Voronovskaya (1932) and a rate that will lead to an infinite expected number of coin flips in general .  (See also my supplemental notes .) But Lorentz (1966) showed that if the function is positive and has a continuous $k$ -th derivative, there are polynomials with nonnegative Bernstein coefficients that converge at the rate $O(1/n^{k/2})$ (and thus can enable a finite expected number of coin flips if the function is ""smooth"" enough). Thus, people have developed alternatives, including linear combinations and iterated Boolean sums of Bernstein polynomials, to improve the convergence rate. These include Micchelli (1973), Guan (2009), Güntürk and Li (2021a, 2021b), the ""Lorentz operator"" in Holtz et al. (2011), Draganov (2014), and Tachev (2022). These alternative polynomials usually include results where the error bound is the desired $O(1/n^{k/2})$ , but nearly all those results (e.g., Theorem 4.4 in Micchelli; Theorem 5 in Güntürk and Li) have hidden constants with no upper bounds given, making them unimplementable (that is, it can't be known beforehand whether a given polynomial will come close to the target function within a user-specified error tolerance). (See end notes.) A Conjecture on Polynomial Approximation The following is a conjecture that could help reduce this problem to the problem of finding explicit error bounds when approximating a function by polynomials. Let $f(\lambda):[0,1]\to(0,1)$ have $r\ge 1$ continuous derivatives, let $M$ be the maximum of the absolute value of $f$ and its derivatives up to the $r$ -th derivative, and denote the Bernstein polynomial of degree $n$ of a function $g$ as $B_n(g)$ . Let $W_{2^0}(\lambda), W_{2^1}(\lambda), ..., W_{2^i}(\lambda),...$ be a sequence of functions on [0, 1] that converge uniformly to $f$ . For each integer $n\ge 1$ that's a power of 2, suppose that there is $D>0$ such that— $$|f(\lambda)-B_n(W_n(\lambda))| \le DM/n^{r/2},$$ whenever $0\le \lambda\le 1$ .  Then there is $C_0\ge D$ such that for every $C\ge C_0$ , the polynomials $(g_n)$ in Bernstein form of degree 2, 4, 8, ..., $2^i$ , ..., defined as $g_n=B_n(W_n(\lambda) - CM/n^{r/2})$ , converge from below to $f$ and satisfy: $(g_{2n}-g_{n})$ is a polynomial with non-negative Bernstein coefficients once it's rewritten to a polynomial in Bernstein form of degree exactly $2n$ . (See end notes.) Equivalently (see also Nacu and Peres 2005), there is $C_1>0$ such that the inequality $(PB)$ (see below) holds true for each integer $n\ge 1$ that's a power of 2 (see ""Strategies"", below). My goal is to see not just whether this conjecture is true, but also which value of $C_0$ (or $C_1$ ) suffices for the conjecture, especially for any combination of the special cases mentioned at the end of "" Main Question "", above. Strategies The following are some strategies for answering these questions: For iterated Boolean sums of Bernstein polynomials ( $U_{n,k}$ in Micchelli 1973 ; see also Güntürk and Li ), find an explicit bound, with no hidden constants, on the approximation error for functions with continuous $r$ -th derivative, or verify my proof of those error bounds . For linear combinations of Bernstein polynomials (Butzer 1953, Tachev 2022 ), verify my proof of those error bounds in my Proposition B10 . For the "" Lorentz operator "", find an explicit bound, with no hidden constants, on the approximation error for the operator $Q_{n,r}$ and for the polynomials $(f_n)$ and $(g_n)$ formed with it, and find the hidden constants $\theta_\alpha$ , $s$ , and $D$ as well as those in Lemmas 15, 17 to 22, and 24 in the paper.  Or verify my proof of the order-2 operator's error bounds in my Proposition B10A . Find other polynomial operators meeting the requirements of the main question (see ""Main Question"", above) and having explicit error bounds, with no hidden constants, especially operators that preserve polynomials of a higher degree than linear functions. Find a sequence of functions $(W_n(f))$ and an explicit and tight upper bound on $C_1>0$ such that, for each integer $n\ge 1$ that's a power of 2— $$\left|\left(\sum_{i=0}^k W_n\left(\frac{i}{n}\right)\sigma_{n,k,i}\right)-W_{2n}\left(\frac{k}{2n}\right)\right|=|\mathbb{E}[W_n(X_k/n)] - W_{2n}(\mathbb{E}[X_k/n])|\le \frac{C_1 M}{n^{r/2}},\tag{PB}$$ whenever $0\le k\le 2n$ , where $M = \max(L, \max|f^{(0)}|, ...,\max|f^{(r-1)}|)$ , $L$ is $\max|f^{(r)}|$ or the Lipschitz constant of $f^{(r-1)}$ , $X_k$ is a hypergeometric( $2n$ , $k$ , $n$ ) random variable, and $\sigma_{n,k,i} = {n\choose i}{n\choose {k-i}}/{2n \choose k}=\mathbb{P}(X_k=i)$ is the probability that $X_k$ equals $i$ . ( See end notes as well as "" Proofs for Polynomial-Building Schemes . ) References Łatuszyński, K., Kosmidis, I., Papaspiliopoulos, O., Roberts, G.O., ""Simulating events of unknown probabilities via reverse time martingales"", arXiv:0907.4018v2 [stat.CO], 2009/2011. Keane, M. S., and O'Brien, G. L., ""A Bernoulli factory"", ACM Transactions on Modeling and Computer Simulation 4(2), 1994. Holtz, O., Nazarov, F., Peres, Y., ""New Coins from Old, Smoothly"", Constructive Approximation 33 (2011). Nacu, Şerban, and Yuval Peres. ""Fast simulation of new coins from old"", The Annals of Applied Probability 15, no. 1A (2005): 93-115. Micchelli, C. (1973). The saturation class and iterates of the Bernstein polynomials. Journal of Approximation Theory, 8(1), 1-18. Guan, Zhong. "" Iterated Bernstein polynomial approximations ."" arXiv preprint arXiv:0909.0684 (2009). Güntürk, C. Sinan, and Weilin Li. "" Approximation with one-bit polynomials in Bernstein form "" arXiv preprint arXiv:2112.09183 (2021). C.S. Güntürk, W. Li, "" Approximation of functions with one-bit neural networks "", arXiv:2112.09181 [cs.LG], 2021. Draganov, Borislav R. ""On simultaneous approximation by iterated Boolean sums of Bernstein operators."" Results in Mathematics 66, no. 1 (2014): 21-41. Farouki, R.T., and Rajan, V.T., ""Algorithms for polynomials in Bernstein form"", Computer Aided Geometric Design 5(1), 1988. Tachev, Gancho. "" Linear combinations of two Bernstein polynomials "", Mathematical Foundations of Computing, 2022. Lee, Sang Kyu, Jae Ho Chang, and Hyoung-Moon Kim. ""Further sharpening of Jensen's inequality."" Statistics 55, no. 5 (2021): 1154-1168. Butzer, P.L., ""Linear combinations of Bernstein polynomials"", Canadian Journal of Mathematics 15 (1953). Note 5 : This condition is also known as a ""consistency requirement""; it ensures that not only the polynomials ""increase"" to $f(\lambda)$ , but also their Bernstein coefficients do as well.  This condition is equivalent in practice to the following statement (Nacu & Peres 2005). For every integer $n\ge 1$ that's a power of 2, $a(2n, k)\ge\mathbb{E}[a(n, X_{n,k})]= \left(\sum_{i=0}^k a(n,i) {n\choose i}{n\choose {k-i}}/{2n\choose k}\right)$ , where $a(n,k)$ is the degree- $n$ polynomial's $k$ -th Bernstein coefficient, where $0\le k\le 2n$ is an integer, and where $X_{n,k}$ is a hypergeometric( $2n$ , $k$ , $n$ ) random variable.  A hypergeometric( $2n$ , $k$ , $n$ ) random variable is the number of ""good"" balls out of $n$ balls taken uniformly at random, all at once, from a bag containing $2n$ balls, $k$ of which are ""good"".  See also my MathOverflow question on finding bounds for hypergeometric variables. Note 6 : If $W_n(0)=f(0)$ and $W_n(1)=f(1)$ for every $n$ , then the inequality $(PB)$ is automatically true when $k=0$ and $k=2n$ , so that the statement has to be checked only for $0\lt k\lt 2n$ .  If, in addition, $W_n$ is symmetric about 1/2, so that $W_n(\lambda)=W_n(1-\lambda)$ whenever $0\le \lambda\le 1$ , then the statement has to be checked only for $0\lt k\le n$ (since the values $\sigma_{n,k,i} = {n\choose i}{n\choose {k-i}}/{2n \choose k}$ are symmetric in that they satisfy $\sigma_{n,k,i}=\sigma_{n,k,k-i}$ ). This question is a problem of finding the Jensen gap of $W_n$ for certain kinds of hypergeometric random variables ( see Note 5 ).  Lee et al. (2021) deal with a problem very similar to this one and find results that take advantage of $f$ 's (here, $W_n$ 's) smoothness, but unfortunately assume the variable is supported on an open interval, rather than a closed one (namely $[0,1]$ ) as in this question.","['approximation-theory', 'real-analysis', 'polynomials', 'uniform-convergence', 'probability']"
3904775,Understanding proof that $C_c^{\infty}$ is dense in $C^{\infty}$ for the convergence in $C^{\infty}$,"Prior notations: $\mathcal{E} = C^{\infty}(\mathbb{R}^n)$ and $\mathcal{D}=C^{\infty}_c(\mathbb{R}^n)\subsetneq \mathcal{E}.$ Definition 'convergence in $\mathcal{E}$ ': $\psi_j\to \psi$ in $\mathcal{E}$ iff for each multi-index $\alpha$ we have that $D^{\alpha}\psi_j \to D^{\alpha}\psi$ uniformly on every compact subset of $\mathbb{R}^n$ (here: $D = \partial_{x_1}\dots \partial_{x_n}$ ). The subspace $\mathcal{D}$ is dense in $\mathcal{E}$ for convergence in $\mathcal{E}$ . Proof. Let $\phi\in\mathcal{E}$ . For each $j\in\mathbb{N}$ , consider $\psi_j\in\mathcal{D}$ such that $\psi_j=1$ on $B(0,j)$ . Then $\psi_j\phi \to \phi$ in $\mathcal{E}$ . Questions : why do such test functions $\psi_j$ exist? As for the convergence, is this reasoning correct: Let $K$ be a compact subset of $\mathbb{R}^n$ and suppose $K\subseteq B(0,j)$ for sufficiently large $j$ . Then $\psi_j\phi = \phi$ on $K$ , so it's clear that $D^{\alpha}(\psi_j\phi)\to D^{\alpha}\phi$ uniformly (since we will have $\forall k\ge j: \forall x\in K: 0=|(\psi_k\phi)(x)-\phi(x)|<\varepsilon, \forall \varepsilon >0$ and similarly for all possible derivatives $D^{\alpha}$ ). Thanks in advance.","['proof-explanation', 'functional-analysis']"
3904876,Limit of infinite product $ \lim_{x\to 1^-} \prod_{n=0}^{\infty } \left(\frac{1+x^{n+1}}{1+x^n}\right)^{x^n}$,How do we calculate this limit: $$ \lim_{x\to 1^-}  \prod_{n=0}^{\infty } \left(\frac{1+x^{n+1}}{1+x^n}\right)^{x^n}$$ I tried to take log and convert the resulting sum of logs into an integral but couldn't proceed after that. The answer is given as $\frac{2}{e}$ which probably means that taking log is the first step. Any help is appreciated. Edit: This is given as the solution but I couldn't understand where step 3(they express the limit between two inequalities) comes from.,"['infinite-product', 'limits']"
3904879,"Distribution of $ \tan \theta$ when $\theta$ is uniform on $[0,2\pi)$","Suppose you have $X$ and $Y$ , which are two random variables whose joint distribution is the standard Gaussian distribution. Let random variables $R \geq 0 $ and $\theta \in [0, 2 \pi)$ satisfy: $X=R\cos\theta$ and $Y=R\sin\theta$ . Let $Z=\frac{Y}{X}$ . Thus, to find the density of $Z$ , I did: $Z= \tan \theta$ . And so, $$F_Z(z)=P(Z\leq z) = P(\tan \theta \leq z) = P( \theta \leq \arctan z)$$ Since $ \theta\sim \text{Uniform} [0, 2\pi)$ due to the fact that $X$ and $Y$ are standard Gaussian and deducing the distribution of $\theta$ from the joint distribution of $ R$ and $ \theta $ , we get that this is equal to: $ \int_{0}^{\arctan z} \frac{1}{2𝜋} dt$ , which then works out to be equal to $\frac{\arctan(z)}{2𝜋} $ . After differentiating this to obtain the density function of $Z$ , I get: $$ f_Z(z)= \frac{1}{2\pi(1+{z^2})}$$ I am not sure about what the limits of $Z$ or $\theta$ should now be. Any feedback will be appreciated.","['statistics', 'probability-distributions', 'normal-distribution', 'probability', 'density-function']"
3904892,When will level sets be closed/Jordan curves?,"Are there any conditions on a $C^{1}$ function $f$ , for example from $\mathbb{R}^2$ to $\mathbb{R}$ , and constant $c$ which would guarantee that the level set $\left\{(x,y) \in \mathbb{R}^2: f(x,y)=c\right\}$ is a closed curve? By ""closed curve"" I mean a curve which is a closed loop (homeomorphic to the circle), rather than merely a curve which, seen as a set of points, is a closed set (as discussed in comments here: Do the curves of a level set of a continuous function have to be closed? ). Similarly, what about conditions for the level set to be a simple (non-self-intersecting) closed curve, i.e. a Jordan curve in the plane?","['multivariable-calculus', 'real-analysis']"
