question_id,title,body,tags
2193802,An Einstein manifold in dimension 3 has a constant sectional curvature.,"Let $M$ be an Einstein manifold, I know that if dim $M\ge 3$, $M$ has a constant scalar curvature $\rho$, and $Ric=\frac{\rho}{m}g$, but I can't see how can $M$ possess a constant sectional curvature in dimension 3. can someone provide a proof? thanks in advance!","['manifolds', 'riemannian-geometry', 'differential-geometry', 'curvature']"
2193810,What is the relation between convexity and positive semi-definite Hessian Matrix?,"While studying maximum likelihood estimation for linear regression, I followed all the contents and mathematical steps and understand the way steps were derived.
At the end of the context, the book says we take the second derivative to see if the Hessian Matrix of the result of the second derivative is the minimum, so we need the Hessian matrix to be positive semi-definite to guarantee a unique minimum. But I don't understand the meaning of positive semi-definite to be the minimum. And it also says convexity, but I am not really getting it. It seems like the reason I ask is that I cannot match the picture between the Hessian positive semi-definite matrix and the convexity. Hope to hear some intuitive explanations considering I am not a real math person but trying to learn more.","['hessian-matrix', 'optimization', 'calculus', 'algebra-precalculus', 'multivariable-calculus']"
2193852,Is the axiom of choice necessary to prove that closed points in the Zariski topology are maximal ideals?,"I would like to solve the beginner's standard exercise which claims that a point of $\mathrm{Spec} \ R$ is closed iff it is a maximal ideal. The reverse implication is easy. The direct one seems much subtler. If $P = V(I)$ then, since there exist a maximal ideal $M$ containing $P$, it would follow that $\{P, M\} \subseteq V(I) = P$, whence $P=M$. In order to show the existence of $M$ I applied Zorn's lemma to the set of ideals containing $P$. The question is the following: is there any other proof of the above implication that does not (indirectly) use the axiom of choice? My concern is that I may be using a cannon to shoot a fly. Also, it would be the first time that I see closedness of points to require the axiom of choice. If the axiom is indeed needed in general, are there ""nice"" classes of rings for which we could get away without it?","['algebraic-geometry', 'zariski-topology', 'affine-schemes', 'axiom-of-choice', 'ideals']"
2193873,"Is Wikipedia wrong when stating that $\emptyset$ has exactly one partition, namely $\emptyset$?","From Wikipedia: The empty set $\emptyset$ has exactly one partition, namely $\emptyset$. I believe this example to be wrong. A partition must have, by definition, nonempty cells, so $P = \{ \emptyset \}$ is not allowed. If I am reading correctly the definition of $P$, the empty set admits no partition.","['set-partition', 'elementary-set-theory']"
2193888,Relation between SVD and EVD,"Given SVD decomposition $ A = U \Sigma V^T $ (where $U$ and $V$ are orthonormal and $ \Sigma $ is a diagonal matrix), I wish to prove that $ AA^T=U\Sigma \Sigma ^TU^T $ is the EVD decomposition of $ AA^T  $ (same goes for $ A^TA=V^T \Sigma ^T \Sigma V $ ). It's easy to see that indeed $ AA^T=U\Sigma \Sigma ^TU^T $ . But I don't understand why the values on $ \Sigma \Sigma ^T $ 's diagonal are $ AA^T $ 's eigenvalues.","['eigenvalues-eigenvectors', 'matrices', 'matrix-decomposition', 'svd', 'linear-algebra']"
2193891,The spectral norm of a positive definite matrix is its largest eigenvalue,"Let $A$ be a symmetric positive definite matrix. Show that $$\lambda_n=\max \left\{\frac{\|Ax\|}{\|x\|}: x\ne 0 \right\}$$ is the largest eigenvalue of $A$ . My try is $\frac{\|Ax\|}{\|x\|}\le \lambda,\forall \lambda$ as a not eigenvalue of $A$ , and the equality occurs when $\lambda$ is an eigenvalue. So $\lambda$ is maximum? May be I am not even understanding the question.","['matrices', 'positive-definite', 'symmetric-matrices', 'spectral-norm', 'matrix-norms']"
2193906,Local inversion theorem problem,Let $f:$ $M_2(\mathbb{R})\rightarrow M_2(\mathbb{R})$ defined by $f(A)=A^2$. In the neighbourhoods of what diagonal matrix is f a local diffeomorphism?,"['derivatives', 'calculus']"
2193965,Contour Complex Integral,"I'm trying to solve the complex integral $\int \sin(z)dz$ over the contour shown below. The approach I took was to parametrize the semicircle and the line separately, obtaining $z(t)= 3e^{it}$ and $z(t)=3+3t+3it$ respectively. However, when I'm going to solve the integrals these two functions become the argument of the $\sin(z)$ function. I don't know how to proceed form here. Am I doing anything wrong? Is there a better way to solve it?",['complex-analysis']
2193975,Triangle Inequality on Quotient Norm,"Let $E/F$ be a quotient space where $E$ ,$F$ are normed spaces with $F \subset E$. 
I am trying to show that norm defined by $ || [x] || = \inf \{\lvert \lvert x-f\rvert\rvert : f \in F  \}  $  on $E/F$ satisfies the triangle inequality. (Note that $[x]   $ means equivalence class of x where equivalence relation is defined by $ y \in [x] \quad \Leftrightarrow \quad x-y \in F \quad $ ) I proceeded as follows: $$\lvert\lvert [x]+[y]   \rvert\rvert = \inf \{ \lvert\lvert [x]+[y]-f  \rvert\rvert: f \in F   \}   $$
$$   \leq \inf \{ \lvert\lvert[x]-f   \rvert\rvert + || [y] -f || + ||f|| : f\in F  \} $$ when I take the norm of f as zero in the above set , I make this extra term disappear, But this time I cannot assure the other norms will take the inf norm. How do I proceed from here?","['functional-analysis', 'normed-spaces', 'real-analysis', 'linear-algebra']"
2194042,"Prove $\lim_{n\to +\infty}\int_0^{\frac{\pi}{2}}\sin^n(x)\,dx=0$","Prove $$\lim_{n\to +\infty}\int_0^{\frac{\pi}{2}}\sin^n(x)\,dx=0$$
My attempt:
$$I_n=\int_0^{\frac{\pi}{2}}\sin^n(x)\,dx = \left.- \int_0^{\frac{\pi}{2}} \sin^{n-1} x \,d(\cos x) = -\sin^{n-1} x\cos x \right|_0^{\frac{\pi}{2}} + \int_0^{\frac{\pi}{2}} \cos x\,d(\sin^{n-1} x)$$
As $\left.-\sin^{n-1} x\cos x \vphantom{\dfrac11} \right|_0^{\frac{\pi}{2}} = 0$, hence
$$\int_0^{\frac{\pi}{2}}\sin^n(x)\,dx = (n-1) \int_0^{\frac{\pi}{2}}\sin^{n-1}(x) \, dx - (n-1)\int_0^{\frac{\pi}{2}}\sin^n(x) \, dx \Rightarrow I_n = \frac{(n-1)I_{n-2}}{n}$$ hence if $n=2k$: $$I_{2k}=\frac{(2k-1)!!}{(2k)!!}\cdot\frac{\pi}{2}$$ 
if $n=2k+1$:
$$I_{2k+1}=\frac{(2k)!!}{(2k+1)!!}$$
Got that by induction and because of $I_1=\int_0^{\frac{\pi}{2}}\sin x\,dx= \left.-\cos x \vphantom{\dfrac11} \right|_0^{\frac{\pi}{2}} = 1$.
So I have to prove that if $k \to +\infty \Rightarrow I_{2k} \to 0 \text{ and } I_{2k+1} \to 0$. I don't know how to do that. Please help","['integration', 'calculus', 'limits']"
2194046,How to prove that a collection of pairwise disjoint subsets of natural number has to be finite or countable,"I think if I could assume there exists an infinite and uncountable family of sets then prove its a contradiction, this question could be solved. But don't know how to do it. Many thanks.",['elementary-set-theory']
2194048,How to solve this exponential/logaritmical inequality,"I'm trying to find the interval where $e^x$ grows faster than $x^x$. So, i need to solve the inequality:
$$\frac{d}{dx}e^x > \frac{d}{dx}x^x$$
Taking the derivatives, i arrived to this inequality:
$$e^x > x^x(\operatorname{ln}(x) + 1)$$
And i don't know how to start here. I've tried going this way:
$$x^x = e^{x \operatorname{ln}(x)}$$
$$e^x > e^{x \operatorname{ln}(x)} (\operatorname{ln}(x) + 1)$$
$$e^{x-x\operatorname{ln}(x)} > \operatorname{ln}(x)+1$$
$${(e^{1-\operatorname{ln}(x)})}^x> \operatorname{ln}(x) + 1$$
$$\bigg(\frac{e}{x}\bigg)^x > \operatorname{ln}(x) + 1$$
And now i don't know where to go. Did i started correctly? If i did, where should i go now? If i didn't, what is the best way to approach this inequality? Thanks.","['algebra-precalculus', 'inequality', 'exponential-function']"
2194079,If $E\subseteq \mathbb{R}$ is measurable and $\delta>0$ then there exists open set $U$ s.t. $\delta \mu(U)<\mu(E)$,"In part of the proof of a problem I am trying to solve I need the following fact (assume that $\mu$ is the Lebesgue measure): If $E\subseteq \mathbb{R}$ is measurable and $\delta>0$ then there exists open set $U\subseteq \mathbb{R}$, such that $E\subseteq U$ and $\,$ $\delta \mu(U)<\mu(E)$. I know and have proven the following fact: Suppose $E \subseteq \mathbb{R}$. Then for each $\epsilon>0$ there exists an open set $U\subseteq \mathbb{R}$ such that $E\subseteq U$ and $\mu(U)< \mu(E)+\epsilon$. I am pretty sure I can use the second fact to prove the first fact, but I keep getting a value of $\epsilon$ that is in terms of $\mu(U)$, which isn't good because $U$ should depend on $\epsilon$, not the other way around. Some help?","['real-analysis', 'lebesgue-measure', 'measure-theory', 'analysis']"
2194107,Estimating joint probability distributions from marginal distributions,"I'm currently studying a problem where I have well determined marginal distributions for N random variables that conform to the beta distribution and very few samples of the joint distribution.  I want to leverage the marginal distributions to estimate the joint distribution is this feasible? For a concrete case, there may be 10k to 100k samples of the marginal distributions and fewer than 100 samples of any given joint distribution.","['statistics', 'statistical-inference']"
2194147,Definition of a probability kernel,"I don't quite understand the definition of a probability kernel (or Markov kernel). Is this correct, the reason to introduce this a transition kernel is, if we have a source $(X,\mathcal{A})$ and a target $(Y,\mathcal{B})$ , both measurable spaces, we want to have a new measurable space $(X,\mathcal{B})$ ? There is this example on Wikipedia for a random walk: Take $X=Y=\mathbb{Z}$ and $\mathcal A = \mathcal B = \mathcal P(\mathbb{Z})$ , then the Markov kernel $\kappa$ with $$\kappa(x,B)=\frac{1}{2}\mathbf{1}_{B}(x-1)+\frac{1}{2}\mathbf{1}_{B}(x+1), \quad \forall x \in \mathbb{Z}, \quad \forall B \in \mathcal P(\mathbb{Z})$$ describes the transition rule But I don't understand it. Why are we using here $Y$ and $\mathcal{A}$ ? Is the measurable space $(Y,\mathcal{B})$ the next position on the random walk with new event from $\mathcal{B}$ ?","['stochastic-processes', 'probability-theory', 'markov-chains', 'markov-process', 'probability']"
2194150,Using Maple to Approximate an Integral,"Use Maple to approximate the value of $b$ that solves the following equation: $$\int_1^b \frac1x\,dx= 1$$ I picked two values $b$ such that one over approximates the integral and one which that under approximates the integral. Then I try to interpolate to obtain a new value of $b$. I was gonna try to repeat until four or five decimals remain the same. It did not work. Please help. Thanks.","['integration', 'maple', 'calculus', 'limits']"
2194162,Real Matrices with Complex Eigenvalues and Eigenvectors: Clockwise or Counterclockwise?,"Say I am interested in a linear operator $A: \mathbb{R}^2 \to \mathbb{R}^2$ who's matrix in a standard orthonormal basis is $\begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}$. $A$ acts on the real plane as a rigid rotation by 90 degrees counter clockwise. The two pairs of eigenvalues and eigenvectors are $\left(i, \begin{bmatrix} 1 \\ -i \end{bmatrix} \right)$ and $\left(-i, \begin{bmatrix} 1 \\ i \end{bmatrix} \right)$. I can pick either eigenvector and take the real and imaginary parts to define a basis for $\mathbb{R}^2$. Now, the eigenvalue $i$ represents rotation by $90$ degrees counterclockwise, but $-i$ represents rotation by $90$ degrees clockwise. So how am I supposed to determine, given two eigenvalue-eigenvector pairs, what direction the rotation is in for a real operator on a real vector space? Thank you.","['complex-analysis', 'linear-algebra']"
2194172,"Find if $\sum\limits^{\infty}_{n=2} \frac{3}{n^2+3n}$ converges or not. If converges, find its sum.","Find if $$\sum^{\infty}_{n=2} \dfrac{3}{n^2+3n}$$ converges or not. If converges, find the sum. I used the integral test to determine whether the series is convergent or not. I found $$3\int^{\infty}_{2} \dfrac{1}{x^2+3x}\text{ dx} = \ln(5) - \ln(2)$$ and thus I can conclude that the series converges. But my question asks if it converges, find the sum. How? I evaluated the series at some points, and here is what I get: $$\dfrac{3}{10} + \dfrac{3}{18} + \dfrac{3}{28}+\dfrac{3}{40} \dots$$
I don't see a pattern or anything, so how can I find the sum?","['convergence-divergence', 'integration', 'sequences-and-series', 'calculus']"
2194198,Showing a semidirect product group is isomorphic to A4,"Let $V$ be the Klein four-group and $f : V \rightarrow V$ map the identity to itself, (12)(34) to (14)(23), (13)(24) to (12)(34), and (14)(23) to (13)(24). Let $C$ be a cyclic group of order 3 generated by $c$. Let $\phi : C \rightarrow Aut(V)$ be a group homomorphism such that $\phi_c=f$. Show that $V \rtimes_\phi C $ is isomorphic to $A_4$. I've shown that $f$ is a group homomorphism, $f \circ f \circ f$ is the identity mapping, and $f \circ f$ is a similar mapping to $f$, mapping the identity itself but mapping any other element to whatever $f$ didn't map to (but not itself). Since $\phi_c=f$ and $\phi_1=id =f \circ f \circ f$, and since $\psi$ is a group homomorphism, then I figured $\phi_{c^2}=f \circ f$ in order to maintain that homomorphism property. The order of $V$ is 4 and the order of $C$ is 3, and the order of $A_4$ is 12, so it makes sense that at least the orders between the twice groups are the same. I was going to simply draw a group table for $A_4$ and $V \rtimes_\phi C$ and then define a bijective mapping between each element based on the tables, but for 144 combinations this is tedious although it definitely would work. In any case, I'm almost certain it's not what I'm supposed to learn from this exercise. Is there a simpler method or an obvious group isomorphism between the groups that's I'm not seeing?",['group-theory']
2194401,Mean Value Theorem for Convex derivatives,"Let $f:\mathbb R^n\to\mathbb R^m$ be differentiable. Assume that the set of
  derivatives $$\{f'(x)\in L(\mathbb R^n,\mathbb R^m):x\in [a,b]\} \text{ is convex.}$$ Prove that there exist a $θ$ in $[a,b]$ such that $f(b)−f(a)=f'(θ)(b−a)$. This is problem number $17$ from chapter $5$ of "" Real mathematical analysis "" by Charles Chapman Pugh [Springer (February 19, 2010)]. If $f$ is $C^1$, there is a straightforward proof. Could anyone help to prove or disprove this statement without $C^1$ condition?","['real-analysis', 'examples-counterexamples', 'convex-analysis', 'multivariable-calculus', 'analysis']"
2194403,Evaluating $\sum_{n \geq 0} \frac{x^{8n}}{(8n)!}$ [duplicate],"This question already has answers here : Sum of $\sum \limits_{n=0}^{\infty} \frac{1}{(kn)!}$ (3 answers) Closed 7 years ago . $$\sum_{n \geq 0} \frac{x^{8n}}{(8n)!}$$ Here's my try: $$\sum_{n \geq 0, \text{even}} \frac{x^{4n}}{(4n)!}$$ $$=\sum_{n \geq 0} \frac{(-1)^n+1^n}{2} \frac{x^{4n}}{(4n)!}$$ By convergence I can split the sums. $$=\frac{1}{2} \sum_{n \geq 0} \frac{x^{4n}}{(4n)!}+\frac{1}{2} \sum_{n \geq 0} (-1)^n \frac{x^{4n}}{(4n)!}$$ Now consider, $$\sum_{n \geq 0} \frac{x^{4n}}{(4n)!}$$ $$=\sum_{n \geq 0, \text{even}} \frac{x^{2n}}{(2n)!}$$ $$=\sum_{n \geq 0} \frac{(-1)^n+1^n}{2} \frac{x^{2n}}{(2n)!}$$ $$=\frac{1}{2} \left( \cos(x)+\cosh (x) \right)$$ Now if I find, $$\sum_{n \geq 0} (-1)^n \frac{x^{4n}}{(4n)!}$$ I'll be done with the problem. How do I do that? Bonus question: Compute $$\sum_{n \geq 0} \frac{x^{3n}}{(3n)!}$$ For this one I'm out of ideas.","['algebra-precalculus', 'taylor-expansion', 'calculus']"
2194415,How is this a basis for the vector space of symmetric 2x2 matrices?,"*EDIT, I posted the incorrect set of matrices. Unsure how to indicate this,but I've correct the question now. 
\begin{bmatrix}0&1\\1&1\end{bmatrix}
\begin{bmatrix}1&1\\1&0\end{bmatrix}
\begin{bmatrix}1&2\\2&-3\end{bmatrix} This comes from a MC question that has 6 sets of three 2x2 matrices. 5 are basis for the vector space of symmetric 2x2 matrices and one is not. According to the answer, this one should be a basis. However, I'm having difficulty seeing how. My understanding is that if it's a basis for symmetric 2x2 matrices, then there must exist c1, c2, c3 such that: c1[M1] + c2[M2] +c2[M3]= any symmetric 2x matrix, ie. 
\begin{bmatrix}1&0\\0&1\end{bmatrix}
But, I can't find any such constants that would make this set of matrices equal to that matrix.","['matrices', 'symmetric-matrices']"
2194476,Differential form vs Hyperreal vs Dual number,"Differential forms, hyperreals, and dual numbers all seem to sort of do something similar: formalize the notion of the infinitesimal. How are they related to each other, and in what ways are they different? I know that the hyperreals sort of ""extend"" the real numbers via nonstandard analysis, but how is this different from what dual numbers do? Similarly, differential forms seem to be able to generalization to higher dimensions -- is this something that hyperreals cannot do?","['nonstandard-analysis', 'differential-geometry']"
2194480,To show that R is a division ring given some property on ring R.,"Let $R$ be a non-zero ring such that the equation $ax=b$ has a solution in $R$ for all $a, b\in R$ with $a\neq 0$. Show that $R$ must have unity and it is a division ring. Now to show this we need to show that every non zero element is invertible.
If we take '$a$' as non zero element with $ax=b$ then how can I use the given fact to show the ring is a division ring. Please help.","['abstract-algebra', 'ring-theory']"
2194490,Use method of direct proof to prove the statement [duplicate],"This question already has answers here : Prove that $n! + k$ is a composite number (2 answers) Closed 7 years ago . If $n$ is a natural number such that $ n \geq 2$, then the numbers $n! + 2, n! + 3, n! + 4... n! + n$ are all composite.
  (Thus, for any n greater than or equal to 2, one can find n consecutive composite numbers) I started with just plugging in numbers to see if they were composite and they were. But I don't know how to prove it for all natural numbers.","['proof-explanation', 'discrete-mathematics']"
2194520,Is there an accepted terminology/notation for the vector $\langle \theta \rangle := \begin{bmatrix} \sin \theta \\ \cos \theta \end{bmatrix}$?,"Suppose you start off at a point $A$, walk 10 units at a bearing of $\frac{\pi}{4}$, then walk 25 more at a bearing of $\frac{5\pi}{4}$. The point where you end up is, of course, $$A+10\left\langle \frac{\pi}{4}\right\rangle+25\left\langle \frac{5\pi}{4}\right\rangle,$$ where $$\langle \theta \rangle := \begin{bmatrix}
\sin \theta  \\
\cos \theta 
\end{bmatrix}$$ For pedagogical reasons, it would be nice to have standard notation and/or terminology for what I'm denoting here as $\langle \theta \rangle$. Question. Are there any standards here?","['plane-geometry', 'notation', 'terminology', 'geometry', 'vectors']"
2194522,Proving Average Fixed Points of a Permutation on n Letter is 1,"I'm in a combinatorics reading group and I was asked to prove that the average number of fixed points in a permutation on $n$ letters is $1$. I was also told to use the expansion for $log(1-x)$ and the equation $$\sum_{n \ge 0}\frac{t^{n}}{n!}A_{n}(x) = e^{tx + \sum_{n \ge 2}\frac{t^{k}}{k}}$$ where $A_{n}(x) = \sum_{w \in S_{n}}x^{fp(w)}$, $S_{n}$ is the set of all permutations on $n$ letters, and $fp(w)$ is the number of fixed points in a permutation. My Attempt: I decided to find the generating functions for the sum of all fixed points in $S_{n}$ and $|S_{n}|$. To my understanding these should be the same function so multiplying one by the other's reciprocal should yield $1$. The generating function for the sum of all fixed points in $S_{n}$ is $A_{n}(x)$ so I'm finished there. In my attempt to find a generating functions for $|S_{n}|$ I considered some letter $k$ in $w$. There are $f(n-1)$ ways to chose the order of the other letters and there are $n$ ways to chose the $k$ letter so $f(n) = nf(n-1)$. I then shifted the recurrence to $f(n+1) = (n+1)f(n)$ and used the generating function $F(x) = \sum_{n \ge 0}\frac{f(n)x^{n}}{n!}$ to obtain the differential equation $F' = xF' + F$. But with the convention $F(0) = 1$, $F = \frac{1}{1-x}$. This is where I could use some guidance because the coefficients of my recurrence shouldn't $1,1,\ldots$. P.S. I'm aware I haven't used any of the suggested equations yet either. This leads me to think my solution is moving in the wrong direction.","['fixed-points', 'permutations', 'combinatorics', 'group-theory', 'symmetric-groups']"
2194598,Definition of stalks of etale sheaves.,"Let $\bar{x} \longmapsto X$ be a geometric point of a scheme. Let $G$ be a sheaf on $X_{et}$. Then there are two definitions of the stalks:
$G_{\bar{x}} = i^{*}G(\bar{x}) $ and $\varinjlim_{U,i_{u}}G(U)$, where $(U,i_{u})$ is an etale neighborhood of $\bar{x}$. How do these two definitions coincide? The problem is that in case of $i^{*}G(\bar{x})$ we need to also sheafify.","['category-theory', 'etale-cohomology', 'algebraic-geometry']"
2194680,"Intuitive meaning of high order Fréchet derivative $D^k f_p(v_1, \cdots, v_l)$","Let $f:V \to W$ be a map between two Banach spaces $V$ and $W$. Let's denote the $k$-th Fréchet derivative of $f$ at $p$ as $D^kf_p$. Then $D^kf_p(v_1, v_2, \cdots, v_l)$ is a $(k-l)$-linear map from $l$ product of $V$ to $W$. Are there any intuitive meaning of the map? If $l = 1$, $D^kf_p(v)$ is a linear approximation of $D^{k-1}f_{p+v} - D^{k-1}f_{p}$ by definition. If $l = k$ and $v_1 = \cdots = v_k = v$, then it gives the order $k$ term of the Taylor's expansion of $f(p +v)$. However, for other cases, what meaning can we give to the quantity in general?","['derivatives', 'banach-spaces', 'frechet-derivative']"
2194732,Proof that $a^{\tan x} + a^{\cot x} \leq 2a$ where $\frac{1}{2} \leq a \leq 1$ and $0 \leq x \leq \frac{\pi}{4}$,"$a^{\tan x}+a^{\cot x} \leq 2a$ where $\frac{1}{2} \leq a \leq 1$ and $0 \leq x \leq \frac{\pi}{4}$ I would like to prove the inequality which is given above. I spent all day thinking about it - I tried some inequalities like relation between exponential and linear or Jensen, but it doesn't work. I tried also by calculus - but when we calculate the derivative and assume it is equal to 0 we have an equality which isn't easy to solve. Maybe anyone has an idea, maybe it's easy and I don't know why I have a problem... I would be grateful if you gave me a hint, I don't want a full solution :)","['inequality', 'exponential-function', 'trigonometry', 'calculus', 'contest-math']"
2194769,Why do all elementary functions have an elementary derivative?,"Considering many elementary functions have an antiderivative which is not elementary, why does this type of thing not also happen in differential calculus?","['derivatives', 'calculus', 'elementary-functions']"
2194772,How to quickly check if vectors are an orthonormal basis of a vector space?,"Let's say we got $3$ vectors given and we need to check if they are an orthonormal basis of some vector space. How would that be done quickly? I have read on several sites on the Internet and here is my summary, correct me if I'm wrong please: All vectors need to be linearly independent. Vectors are perpendicular aka orthogonal to each other ( if $3$ vectors given, I have to do it as pairs of $2$, right? ). Each vector has length $1$.","['orthogonality', 'linear-algebra', 'vector-spaces']"
2194854,Pulling back cycles to closed subschemes,"Let $X$ be a (reduced, affine) scheme and consider the closed embedding $i: X \cong X\times 0 \to X\times \mathbb A^1$. Consider the pullback map $i^*: CH^k(X\times \mathbb A^1) \to CH^k(X)$. I know that Chow groups are invariant under taking products with the affine line so this map should be an isomorphism and indeed I can show that it is a surjection since the composition $X \to X\times \mathbb A^1 \to X$ is an isomorphism. Can we show directly that $i^*$ is injective? I know the proof that proceeds by showing that $\pi^*: CH^k(X) \to CH^k(X\times \mathbb A^1)$ is a surjection (and hence isomorphism where $\pi: X\times \mathbb A^1 \to X$ is the projection. I am only interested in the case where $X$ is affine in which case I think the problem looks like the following in the case where $k=1$ (codimension one): Consider an ideal $I \subset A[t]$ and form the ideal $I_0 = \{f(0) : f(t) \in I\}$. Assume that the codimension of $I$ in $A[t]$ and $I_0$ in $A$ is both one (transversality + codimension one condition). We want to prove that if $I_0$ is principal, then so is $I$. Is this the right translation to commutative algebra? PS: While the above is the case of most interest, I would be interested in what one can say in general about pulling back cycles under closed subschemes.","['schemes', 'intersection-theory', 'algebraic-geometry', 'commutative-algebra']"
2194902,Approximation of an integrable function by a simple function,"Let $(X,\Sigma, \mu)$ be a measurable space and let $f: X \mapsto \mathbb{R}$ be an integrable function. I want to show that for $\epsilon > 0$, I can find a simple function g such that $\int_{X} |f - g| d\mu < \epsilon$. Since f is integrable, we know that $\int f = \text{sup}\{\int g :\text{g is a simple function and } g \leq_{a.e} f\}$ so if $g \leq_{a.e} f$ we have the inequality $\int f d\mu > \int g d\mu = \int \sum_\limits{i=1}^{n} a_{i} \chi(E_{i})d\mu$ where each $E_{i}$ are measurable sets. But now I'm not sure how to proceed.","['real-analysis', 'lebesgue-integral', 'measure-theory']"
2194927,Numerically approximating a dynamical system on a graph,"I am struggling with a project that originated in complex networks and graph theory and has since incorporated some dynamical system elements (I think?). Unfortunately I am not very well versed in dynamical systems, and hope to find some help on how to numerically approximate this system. Consider an undirected connected graph $G = (V, E)$, accompanied by a distance function $d: E \to \mathbb{R}$. In particular, every edge $e \in E$ has a weight $d(e)$ contained in the interval $(0, \infty)$. For each point in time $t$ (starting at zero) we find a value $\kappa_t(e)$, for every edge $e \in E$. This value is found as the result of a linear program and can in general only be calculated analytically for very simple graphs. The value $\kappa_t(e)$ ranges from $(-\infty, 1]$, and is then used to modify the distance $d_t(e)$ in the following manner: we say that
\begin{align*}
d_{t + \Delta t}(e) = d_{t}(e) - \kappa_t(e) \cdot \Delta t.
\end{align*}
This implies that the distance on edge $e$ can either increase, decrease, or remain constant. If I remember correctly, it is correct to say that with this definition, we can state
\begin{align*}
\frac{\partial d_t(e)}{\partial t} = - \kappa_t(e),
\end{align*} 
right? We do this for every edge in the graph, and the result is that some distances decrease to zero, and some distances increase to infinity (it might be possible that some distances remain constant, but this is unlikely, and might even be impossible). I am having trouble numerically approximating this system, in particular I struggle with choosing the time increments $\Delta t$. I started with using a simple Euler forward approximation, but this is not satisfactory in two ways: first, for most timesteps nothing 'interesting' happens. Second, by chosing the increments independently from the situation in the graph, we might 'jump over' interesting events, changing the geometry of the graph in the process! For my second attempt, I thought I might choose the time increments $\Delta t$ as a function of the current situation in the graph. I did this by first selecting those edges that have a positive value $\kappa_t$, which implies that the distances on those edges would decrease. Next, I took the maximum of those values, and calculated $\Delta t$ as the timeperiod it took for the edge with maximum value $\kappa_t$ to reach exactly distance zero. In that way, we are assured that after every jump, at least one edge has attained distance zero. This massively reduces the number of iterations in my approximation algorithm, but it gave a number of problems as well. For instance, it might be very possible that all values $\kappa_t$ are non-positive at time $t$, but more importantly, the geometry changes at every point in time, so that those edges with maximal positive value $\kappa_t$ are generally not the ones with distance zero on their edges at time $t + \Delta t$. Does anyone have a suggestion on how to simulate a dynamical system such as this where at any point in time the geometry can change (e.g. change from $\kappa(e) > 0$ to $\kappa(e) < 0$)? Thank you in advance for any help you can offer! EDIT: We calculate the value $\kappa_t(x, y)$ as follows:
\begin{align*}
\kappa_t(x, y) = 1 - \frac{\mathcal{W}_1(P_x, P_y)}{d_t(x, y)}, \qquad (x, y) \in E,
\end{align*}
where $P_x$ and $P_y$ are the probability measures of the standard random walk associated with the graph (i.e. $P_x(y)$ is the probability of jumping to vertex $y$, when you are at $x$. This probability is proportional to the inverse of the distance on $(x, y)$ and the neighbouring vertices) Next,
\begin{align*}
\mathcal{W}_1(P_x, P_y) = \inf_{\gamma \in \pi(P_x, P_y)} \left\{ \sum_{(a, b) \in V \times V} d_t(a, b) \gamma(a, b) \right\},
\end{align*}
where $\pi(P_x, P_y)$ is the collection of all couplings of $P_x$ and $P_y$. I hope this helps!","['numerical-methods', 'ordinary-differential-equations', 'dynamical-systems']"
2194929,How does one show that $\prod_{k=1}^{\infty}\left(\prod_{j=1}^{\infty}{(2j)^{2^k}-1\over (2j)^{2^k}+1}\right)^{1/2^k}={2\over \pi}?$,"Given this double infinite products $$\prod_{k=1}^{\infty}\left(\prod_{j=1}^{\infty}{(2j)^{2^k}-1\over (2j)^{2^k}+1}\right)^{1/2^k}={2\over \pi}\tag1$$ How can we prove $(1)$? An attempt: Take the log $$\sum_{k=1}^{\infty}{1\over 2^k}\sum_{j=1}^{\infty}\ln\left({(2j)^{2^k}-1\over (2j)^{2^k}+1}\right)=\log{2\over \pi}\tag2$$ Not sure how to handle $$\sum_{j=1}^{\infty}\ln\left({(2j)^{2^k}-1\over (2j)^{2^k}+1}\right)\tag3$$ Recall $(4)$, maybe it could be of some use for $(3)$ $$\ln{1+x\over 1-x}=\sum_{n=1}^{\infty}{2\over (2n-1)}x^{2n-1}\tag4$$","['infinite-product', 'sequences-and-series', 'pi']"
2194940,Does the second derivative of a convex function exist at all but countably many points?,"I know that the second derivative of a convex function exists almost everywhere ( Alexandrov's theorem ). And I know that the first derivative exists everywhere except countably many points ( This question ). So, does the second derivative exist everywhere except countably many points? I'd appreciate a source I can cite, or a counterexample.","['derivatives', 'real-analysis', 'convex-analysis']"
2194970,Are those properties sufficient for defining a field?,Consider the following properties: $a(x+y) = ax + ay$ $x + y = y + x$ $ax = xa$ $x + 0 = x$ $x \cdot 1 = x$ for every $x\ne 0$ there's a $y$ such that $xy=1$. Are those enough for defining a field?,"['abstract-algebra', 'field-theory']"
2194973,Is every finite list of integers coprime to $n$ congruent $\pmod n$ to a list of consecutive primes?,"For example the list $(2, 1, 2, 1)$ is congruent $\pmod 3$ to the consecutive primes $(5, 7, 11, 13)$. But how about the list $(1,1,1,1,1,1,1,1,2,3,4,3,2,3,1) \mod 5$? More generally, we are given some integer $n \geq 2$ and a finite list of integers that are coprime to and less than $n$. Is it always possible to produce the same list by consecutive primes $\pmod n$? Formally: given $n \geq 2$ and $(a_0,a_1,\cdots \,a_k)$ such that for all $i$, $GCD(a_i, n) = 1$, is there a list of consecutive primes such that each $p_i \equiv a_i \pmod  n$?","['number-theory', 'prime-gaps', 'prime-numbers', 'elementary-number-theory']"
2194975,Joint Distribution of n Poisson Random Variables,"Let $x_1,x_2,\dots,x_n$ be a set of independent and identically distributed random variables with distribution Poisson with parameter λ. Write the joint distribution of all those random variables. Simplify as much as possible your final answer and show work. So I think that the joint probability of independent random variables is the product of all individual probability distribution function, but I don't actually understand how to implement that in this case, since it's for $n$ variables.","['statistics', 'probability', 'poisson-distribution']"
2194988,Is every co-closed form co-exact?,"Let $M$ be a Riemannian manifold, $E$ a vector bundle over $M$, equipped with a flat connection connection $\nabla$ and a $\nabla$-compatible metric $\eta$. Denote by $\delta$ the adjoint of the covariant exterior derivative $d^{\nabla}$. $$\delta: \Omega^k(M,E) \to \Omega^{k-1}(M,E) $$ Let $\sigma \in \Gamma(T^*M \otimes E)$ be a one  $\,E$-valued form, and suppose that $\delta(\sigma)=0$. Does there exist an  $\,E$-valued form $\alpha \in \Omega^2(M,E)$ such that $\sigma=\delta(\alpha)$? Comment: The flatness of $\nabla$ implies $d^{\nabla} \circ d^{\nabla}=0$, hence $\delta^2=0$.  So, the condition $\delta(\sigma)=0$ is necessary for the existence of such an $\alpha$. The question is whether it is sufficient. If this is not true, is there some other representation theorem? 
What about the special case of $E=M \times \mathbb{R}$? (real-valued forms).","['vector-bundles', 'differential-forms', 'riemannian-geometry', 'differential-geometry']"
2195047,Solve the vector cross product equation,"Consider the vector equation $a \times x = b$ in $\Bbb R^3$ , where $a\ne0$. Show that $$x = \frac{b\times a}{|a|^2} + ka$$ is the solution to the equation for any scalar $k$ i really dont know where to start , i really appreciate any help","['cross-product', 'linear-algebra', 'vector-spaces']"
2195105,"Let $m,n \in \mathbb{Z}$. Assume $m < n$. Then $m \leq n-1$.","My question is concerning my proof's validity in the theorem written below. If someone could take a moment and see if there is a flaw in it, I'd appreciate it. Lemma. There are no natural numbers less than $1$. Proof. Assume to the contrary that there are natural numbers less than $1$ and let $S$ consist of these numbers. By assumption, $S \neq \varnothing$ and $S \subseteq \mathbb{N}$, so by the well ordering principle, the natural number $m = \min{S}$ exists such that $$ 0 < m < 1$$ If we multiply the above inequality by $m$, we get $$0 < m^2 < m < 1$$ We see that $m^2$ is a natural number and it is less than $1$, so it belongs to $S$. But $m^2 < m$, which is a contradiction. Hence, there are no natural numbers less than $1$. $\blacksquare$ Theorem. If $m$ and $n$ are integers with $m < n$, then $m \leq n-1$. Proof. Assume to the contrary that $n-1 < m$. Then $$n-1 < m < n$$ We observe that the length of this interval $(n-1, n)$ is $1$. If we let \begin{align} 
\mathcal{l_1} &= |m - n + 1| \\
\mathcal{l_2} &= |n-m| 
\end{align} which are the lengths of the subintervals $(n-1, m]$ and $(m,n)$. Then because $\mathbb{Z}$ is closed under addition and multiplication, and because $\mathcal{l_1},\mathcal{l_2} > 0$, we see that $\mathcal{l_1}, \mathcal{l_2} \in \mathbb{N}.$ However, $\mathcal{l_1} + \mathcal{l_2} = 1$, so we must have $$0<\mathcal{l_1} < 1 \thinspace \thinspace \thinspace \thinspace \text{and} \thinspace \thinspace \thinspace \thinspace 0<\mathcal{l_2} < 1$$ which contradict the above lemma that there are no natural numbers less than $1$. Therefore, we must have $m \leq n-1$ whenever $m<n$. $\blacksquare$ Corollary. There are no integers between two consecutive integers. Edit I included a revision of my proof for the lemma, thanks to the commentators below. Edit 2 I edited the proof to the above lemma to exclude the peano axioms, which is very important to me, since the textbook from which this problem came does not discuss them. The proof is now accessible to anyone beginning elementary number theory.","['peano-axioms', 'proof-verification', 'elementary-set-theory', 'elementary-number-theory']"
2195113,Find all solutions in positive integers to $a^b -b^a = 3$,"Find all solutions in positive integers to $a^b -b^a = 3$ It appears that the only solution is $a=4, b=1$. Modular arithmetic is not of much help (apart from deriving that $a,b$ must have opposite parity). We know that $a^x$ dominates the polynomial $x^a$ but this does not seem to yield the result. Tried hard to estimate a bound for $a^b - b^a$ but with no success. Any help is appreciated. This is a problem from a regional mathematics Olympiad. Added on 26 March, 2017: I was trying to find a solution that involves no Calculus (or a minimum of Calculus) since this is a problem from a Junior Olympiad in which the students do not have a knowledge of Calculus. The following argument seems to work. Please point out flaws/mistakes, if any. Lemma 1 For any $n \geq 4$, $$n^{n+1} - (n+1)^n > (n-1)^n - n^{n-1}$$ Proof \begin{align*}
&\qquad n^{n+1} - (n+1)^n > (n-1)^n - n^{n-1} \\
&\Leftrightarrow n^{n+1} + n^{n-1} > (n-1)^n + (n+1)^n \\
&\Leftrightarrow n+\frac{1}{n} > \left(1-\frac{1}{n}\right)^n + \left(1+\frac{1}{n}\right)^n
\end{align*}
Since $\left(1+\frac{1}{n}\right)^n < 3$ and $\left(1-\frac{1}{n}\right)^n < 1$, we have 
$$\left(1+\frac{1}{n}\right)^n+\left(1-\frac{1}{n}\right)^n < 4 < n +\frac{1}{n}$$ Lemma 2 For any $k$ and $n \geq 4$, 
$$n^{n+k} -(n+k)^n  > n^{n+1} - (n+1)^n$$ Proof \begin{align*}
&\qquad n^{n+k} -(n+k)^n  > n^{n+1} - (n+1)^n \\
&\Leftrightarrow n^{n+k} -n^{n+1} > (n+k)^n - (n+1)^n \\
&\Leftrightarrow n^k - n > \left(1+\frac{k}{n}\right)^n - \left(1+\frac{1}{n}\right)^n 
\end{align*}
Since $\left(1+\frac{k}{n}\right)^n < 3^k$ and $\left(1-\frac{1}{n}\right)^n < 1$, we have 
$$\left(1+\frac{k}{n}\right)^n+\left(1-\frac{1}{n}\right)^n < 3^k +1  < n^k - n $$
as $n > 4$. Now suppose that $a^b - b^a = 3$.  Let $a \geq 4$. If $b = a+k > a$, then we have 
\begin{align*}
a^{a+k} - (a+k)^a &> a^{a+1} - (a+1)^a \\
&> (a-1)^a - a^{a-1} \\
&> \cdots \\
&> 3^4 - 4^3 = 17
\end{align*}
Thus there are no solutions with $b > a \geq 4$. If $a > b \geq 4$, then 
$$a^b - b^a = -(b^a - a^b) \leq -17 $$
from what we have seen above. Thus there are no solutions if $a > b \geq 4$. Thus all solutions can be only in the range $1 \leq a,b \leq 4$. Clearly, in this range only $a=4, b=1$ satisfies the given equation.","['number-theory', 'diophantine-equations']"
2195185,How to solve this in an efficient way (without calculators) [duplicate],"This question already has answers here : Fast way to come up with solutions to $x(x-1)(x-2)(x-3)=1$? (2 answers) Closed 7 years ago . Solve for $x$, if
  $$(x+4)(x+7)(x+8)(x+11)+20=0.$$ Is there an easier way to solve this than trying to multiply all the values together? I've tried multiplying all of them together, and I get an equation of the fourth degree which I find very hard to factorise. I'm hoping there is an easier way to solve this question. Any help is appreciated.Thanks :). The equation I get after multiplying is
$$x^4+30x^3+325x^2+1500x+2484=0,$$
and its roots are 
$$-6, \quad -9, \quad \frac{-15\pm\sqrt{41}}2 .$$","['algebra-precalculus', 'polynomials']"
2195269,Simplification of $\binom{50}{0}\binom{50}{1} + \binom{50}{1}\binom{50}{2}+⋯+\binom{50}{49}\binom{50}{50}$,"There was a post on this web site an hour ago asking for the sum of \begin{equation*}
\binom{50}{0}\binom{50}{1} + \binom{50}{1}\binom{50}{2}+⋯+\binom{50}{49}\binom{50}{50}
\end{equation*} expressed as a single binomial coefficient. (Four choices were provided in the post.) The post seems to have been deleted.  I think it is worth keeping on this web site.",['combinatorics']
2195283,"If $\lim_{n\to\infty}\frac{a_{n+1}}{a_n}=1$, $\lim_{n\to\infty}\frac{a_{2n}}{a_n}=\frac{1}{2}$ then $\lim_{n\to\infty}\frac{a_{3n}}{a_n}=\frac{1}{3}$","Let $\{a_n\}$ be a decreasing sequence and $a_n>0$ for all $n$. 
If $\displaystyle\lim_{n\to\infty}\frac{a_{n+1}}{a_n}=1$ and $\displaystyle\lim_{n\to\infty}\frac{a_{2n}}{a_n}=\frac{1}{2}$, 
how to prove or disprove that
$\displaystyle\lim_{n\to\infty}\frac{a_{3n}}{a_n}=\frac{1}{3}$ ? Thank you.","['limits', 'sequences-and-series', 'calculus', 'analysis']"
2195289,Why does Brownian motion have drift on Riemannian Manifolds?,"Given a Riemannian manifold $(M,g)$, the paths of a Brownian motion on it can be written as the following stochastic differential equation in local coordinates:
$$
dX_t = \sqrt{g^{-1}} dB_t - \frac{1}{2} g^{ij}\Gamma^k_{ij} dt = \sigma(X_t)\, dB_t + \vec{b}(X_t) \,dt
$$
where $B_t$ is an $n$ dimensional Wiener process and $g_{ij}\sigma^i_k\sigma^j_\ell=\delta_{k\ell}$. My question is conceptual and geometric in nature: how can there be a drift term in this equation? Algebraically, I understand, roughly speaking, that it arises from the extra temporal terms in Ito's formula. However, in a general relativity sort of way, one can consider $g$ to be a ""warping of space"" (say for $M=\mathbb{R}^n$), and we note $g$ is always symmetric. The metric does not depend on directions , but only on locations (unlike say for Finsler manifolds).
In other words, acceleration in one direction due to the curvature also occurs in the opposite direction, meaning the effect of the curvature on the diffusion is also symmetric. So, geometrically, how can $\vec{b}$ exist, as it by definition favors some particular direction? This is even weirder to me when I think about Riemannian normal coordinates (say at $p$), where $g_{ij}=\delta_{ij}$ and thus $\Gamma^k_{ij}=0$ at $p$. Thus, $\vec{b}=0$ at $p$, in that system. One can do this at every point. I suppose the drift would not disappear off of $p$, but it still seems odd to me that the presence of drift would not somehow be an invariant. Undoubtedly, I am missing something here. I think something to do with the heat equation generating the SDE above, i.e. $\partial_t u = \Delta_g u/2$, may be useful. Edit: It's useful to note that the term ""drops out"" of the equation for the Laplace-Beltrami operator in local coordinates (see the cross-posted version of this question on MathOverflow ).","['stochastic-processes', 'riemannian-geometry', 'differential-geometry', 'brownian-motion', 'stochastic-calculus']"
2195291,Weights in group theory,"What are the meaning of weights in group theory? For example consider the $SU(3)$ group and the angular momentum problem. The spherical harmonics $lm$ induce weights which result irrep. If we add two spin-1 particles we will have: $$(2,0) \otimes (1,0) \otimes (0,0)$$","['spherical-harmonics', 'physics', 'group-theory']"
2195295,Find the flux out of a tetrahedron.,"The problem: Find the flux $\textbf{F} = 3x\hat{i} + z\hat{j}$ out of the tetrahedron closed in by the plane $5x + 3y + 3z = 4$ and the xy, xz and yz planes. My (wrong) solution: I calculated the divergence of $\textbf{F} = 4$. Then i find the volume of the tetrahedron $$\frac{\frac{4}{5} * \frac{4}{3} * \frac{4}{3}}{3} = \frac{64}{135}$$
Then i multiply the volume by the divergence and get $\frac{256}{135}$ Any help would me much appreciated. Update: The divergence is actually 3, and when calculating the volume I forgot to divide the base by two.","['multivariable-calculus', 'surface-integrals']"
2195297,"How is it ""easily checked"" that $[1-s(\cos\theta + i \sin \theta)] \sum_{n=0}^\infty s^n [\cos(n\theta)+i \sin(n\theta)] = 1$","This is from a derivation of de Moivre's theorem on p.148 of Grimmett & Stirzaker's Probability and Random Processes . The setup: The sequence $a_n = (\cos \theta + i \sin \theta)^n$ has generating function
  $$G_a(s) = \sum_{n=0}^\infty \left[ s(\cos\theta + i \sin\theta) \right]^n = \frac{1}{1 - s(\cos\theta + i \sin\theta)}$$
  if $|s| < 1$; here $i = \sqrt{-1}$. The part that I don't follow: It is easily checked by examining the coefficient of $s^n$ that $$\left[1-s(\cos\theta + i \sin \theta) \right] \sum_{n=0}^\infty s^n \left[\cos(n\theta)+i \sin(n\theta) \right] = 1 $$ when $|s| < 1$. The rest is clear to me, but in case you're interested: Thus $$\sum_{n=0}^\infty s^n \left[\cos(n\theta)+i \sin(n\theta) \right] = \frac{1}{1-s(\cos\theta + i \sin \theta)}$$ if $|s| < 1$. Equating the coefficients of $s_n$ we obtain the well-known fact that $\cos(n\theta)+i \sin(n\theta) = (\cos\theta + i \sin\theta)^n$. Thanks for any help!","['generating-functions', 'complex-analysis', 'probability-theory', 'calculus']"
2195309,N tuples of natural numbers,"I have a question about elements of $N^k$,k a positive integer. For all $1 < i < j$, no $i$ tuple is a $j$ tuple BUT 1-tuple may be $i$ tuple
  for some $i>1$. Why? Are there any nontrivial examples of this observation? How do I understand this statement inductively via the pairing operation? FOR SOURCE SEE THIS , page 4.","['elementary-set-theory', 'functions', 'elementary-number-theory']"
2195335,Modified Willmore energy and surfaces with infinitesimally narrow necks,"There is an open problem in theoretical biophysics that I think has a strong mathematical flavour, and I have always thought that mathematicians would be interested in it. I would like to hear your ideas on the problem. In the biophysics literature, cellular membranes are typically modelled using what is called the bending energy or Helfrich energy, 
$$
E = \int (H - H_0)^2 \mathrm{d}A
$$
where $H$ is the mean curvature of the surface. The constant $H_0$ is called the ""spontaneous curvature"", and represents the preferred curvature of the membrane, arising from an asymmetry between the two sides of the membrane. In the absence of asymmetries, $H_0=0$ and we recover the usual Willmore energy. I have also omitted a term related to the Gaussian curvature, because we will only deal with closed surfaces of fixed topology, and then the term becomes irrelevant through the Gauss-Bonnet theorem. Furthermore, the total area and enclosed volume of the surface are also constrained in practical situations, so that one has to minimize the bending energy of the surface for fixed area $A$ and volume $V$. Without loss of generality, one can fix $A=4 \pi$, and the minimum energy shapes will depend on two parameters, $H_0$ and $V$. For a surface of genus $0$, we have necessarily that $0 \leq V \leq 4 \pi /3$, with $V=4 \pi / 3$ corresponding to a sphere, and $V<4 \pi /3$ necessarily deviating from a sphere. Now here comes the mathematically interesting bit: among the many shapes that minimize the bending energy above for given $H_0$ and $V$, it was found from numerical minimization of axisymmetric shapes [1,2] that there exist minimum energy shapes corresponding to two separate shapes that are connected to each other via an infinitesimally narrow neck, in practice tangentially ""kissing"" at a single point. The infinitesimally narrow neck costs zero bending energy (its Gaussian curvature tends to infinity but its mean curvature stays finite), so the total energy is just the sum of the energies of each individual shape. These limit shapes were found to always satisfy the kissing condition $$
H_1 + H_2 = 2H_0
$$
where $H_1$ and $H_2$ are the mean curvatures of each of the two shapes at the kissing point. Two examples, corresponding to the particular case of kissing spheres, are illustrated in this image: In the example on the left, the shape would be a minimum energy solution for $1/R_1 + 1/R_2 = 2 H_0$. The shape would be a boundary minimum of the energy for $1/R_1 + 1/R_2 < 2 H_0$, and a similar shape with a small but finite neck would be a solution for $1/R_1 + 1/R_2 \gtrsim 2 H_0$. In the example on the right, the shape would be a minimum energy solution for $1/R_1 - 1/R_2 = 2 H_0$. The shape would be a boundary minimum of the energy for $1/R_1 - 1/R_2 > 2 H_0$, and a similar shape with a small but finite neck would be a solution for $1/R_1 - 1/R_2 \lesssim 2 H_0$. In both examples, the radii $R_1$ and $R_2$ are such that the constraints on the area and volume of the shape are satisfied. A particularly interesting case occurs for $H_0 = 0$, i.e. for the standard Willmore energy, where the kissing condition becomes $H_1 = -H_2$. In particular, the minimum bending energy shape for zero volume $V=0$ is composed of two nested spheres of equal radius connected to each other by an infinitesimal neck, with total energy $E=8\pi$. The kissing condition $H_1 + H_2 = 2 H_0$ was later analytically proven [3] for axisymmetric shapes (i.e. two axisymmetric shapes connected to each other by a neck at their axis of symmetry) of genus $0$, but it is still not known whether it holds for non-axisymmetric shapes or shapes of higher genus . Furthermore, the proof for axisymmetric shapes in [3] was quite complicated, and no simpler proofs have been given since. Two interesting observations are (i) that the kissing condition implies $(H_1 - H_0)^2 = (H_2 - H_0)^2$, that is, the bending energy density is equal on both sides of the neck, and (ii) that if we take the mean curvature at the neck to interpolate between the mean curvatures on both sides of the neck, i.e. $H_\mathrm{neck} \equiv (H_1+H_2)/2$, then the kissing condition implies that $H_\mathrm{neck} = H_0$ Furthermore, the neck condition has been extended [4] (heuristically, based on numerical results) to surfaces composed of two separate ""domains"" of fixed area, with a line energy that penalizes the length $L$ of the boundary between the two domains, $E_\lambda = \lambda L$. If the two domains have distinct spontaneous curvatures $H_{0,1}$ and $H_{0,2}$, one then finds the kissing condition $H_1 + H_2 = H_{0,1}+H_{0,2}  \pm \lambda$, with the plus and minus signs corresponding to the left and right situations in the image above. So, my questions to you are: Have you ever heard of this kissing condition, and the corresponding open problem of proving whether it holds for non-axisymmetric and higher genus shapes? Is there any mathematical literature on this problem that might have gone unnoticed to me and other physicists? Do you have any intuition on how it could be proven, or whether it will or won't hold in non-axisymmetric situations? Do you think mathematicians could/should get interested in it? [1] U. Seifert, K. Berndl, and R. Lipowsky, Phys. Rev. A 44, 1182 (1991).
""Shape transformations of vesicles: Phase diagram for spontaneous- curvature and bilayer-coupling models."" Journal link . [2] L. Miao, B. Fourcade, M. Rao, M. Wortis, and R. K. P. Zia, Phys. Rev. A 43, 6843 (1991). ""Equilibrium budding and vesiculation in the curvature model of fluid lipid vesicles."" Journal link . [3] B. Fourcade, L. Miao, M. Rao, M. Wortis, and R. Zia, Phys. Rev. E 49, 5276 (1994). ""Scaling analysis of narrow necks in curvature models of fluid lipid-bilayer vesicles."" Journal link . [4] F. Jülicher and R. Lipowsky, Phys. Rev. E 53, 2670 (1996). ""Shape transformations of vesicles with intramembrane domains"" Journal link .","['curvature', 'differential-geometry', 'surfaces']"
2195347,How can I find critical points of the multivariate polynomial?,"I need to find the critical points of the multivariate polynomial and types of critical points $f(x_1,x_2,x_3,x_4)=x_2x_3+x_3x_4+x_1x_2+x_1x_2x_3x_4$ What are soft to find the critical points and how I can classification the critical points. If taken: $$\frac{\partial f}{\partial x_1}=0,\frac{\partial f}{\partial x_2}=0,\frac{\partial f}{\partial x_3}=0,\frac{\partial f}{\partial x_4}=0,$$ then we get: $$x_2+x_2x_3x_4=0$$ $$x_3+x_1+x_1x_3x_4=0$$ $$x_2+x_4+x_1x_2x_4=0$$ $$x_3+x_1x_2x_3=0$$ Do you use a program to solve equations or is there a better way? If there is a set of solutions. How do you critical points ? Thanks for the help.","['multivariable-calculus', 'maxima-minima', 'optimization', 'systems-of-equations']"
2195356,Using the substitution method for a simple integral,"I have been playing with the substitution rule in order to test some ideas with computational graphs. One of the things I'm doing is applying the substitution to well known, and easy, integrals. For example, let's use that method to find the indefinite integral for $$f(x) = x^2$$ Using the rule $\int x^n dx = \frac{x^{n+1}}{n+1} + C$, we get $$F(x) = \frac{x^3}{3} + C$$ So, let's do the following substitution: $$u = x^2$$ $$\frac{du}{dx} = 2x \Leftrightarrow dx = \frac{1}{2x} du$$ So, performing the substitution in the integral of $f(x)$ gives us $$\int x^2 dx = \int u \frac{1}{2x} du = \frac{1}{2x} \frac{u^2}{2} + C = \frac{(x^2)^2}{4x} + C = \frac{x^3}{4} + C$$ Have I done anything wrong with the substitutions?? Thanks in advance!",['integration']
2195377,Reverse mode differentiation vs. forward mode differentiation - where are the benefits?,"According to Wikipedia forward mode differentiation is preferred when $f: \mathbb{R}^n \mapsto \mathbb{R}^m$ , m >> n. I cannot see any computational benefits. Let us take simple example: $f(x,y) = sin(xy)$ . We can visualize it
as graph with four nodes and 3 edges. Top node is $\sin(xy)$ , node one level below is $xy$ and two initial nodes are $x$ and $y$ . Derivatives on nodes are $\cos(xy)$ , $x$ , and $y$ . For both reverse and forward mode differentiation we have to compute these derivatives. How is reverse mode differentiation is computationally superior here?",['derivatives']
2195397,Exponential map and matrices: comparison of two pictures,"Let $M$ be a Riemannian manifold with the Levi-Civita connection. For $x \in M$ and $X \in T_x(M)$ we can consider geodesic $\gamma$ with the properties that $\gamma(0)=x$ and $\gamma'(0)=X$. Such geodesic exists and is unique: existence is guaranteed provided we take vectors $X$ in some suitable neighborhood of $0$ in $T_x(M)$. If we define $y:=\gamma(1)$ we arrive at the definition of exponential map $\exp_x:T_xM \to M$ (defined only on some small neighborhood of $0$ in $T_xM$). From the other hand, we can consider $GL(n,\mathbb{R})$ as an open set in $\mathbb{R}^{n^2}$: therefore it becomes manifold on its own right. I would like to understand why in this situation $\exp_{I}(A)=e^{A}$ where on the left hand side is the exponential map from riemannian manifold at identity matrix and the right hand side is just the exponential of the matrix. Here we identify $T_I(Gl(n))$ with $\mathbb{R}^{n^2}$.","['lie-groups', 'ordinary-differential-equations', 'differential-geometry', 'lie-algebras', 'exponentiation']"
2195422,Is the space of all bounded Hölder continuous functions a Banach space?,"Let $(\Omega,d)$ be a non complete metric space, and 
consider the space $C^{\gamma}_b(\Omega, \mathbb{R})$, $0<\gamma<1$
of all bounded Hölder continuous functions. Is $C^{\gamma}_b(\Omega, \mathbb{R})$  endowed with the norm
$$
\|f\|_{\gamma}=\|f\|_0+\sup_{x\neq y} \dfrac{|f(x)-f(y)|}{d(x,y)^{\gamma}}
$$ a Banach Space? Could someone provide me some reference?","['functional-analysis', 'reference-request', 'real-analysis', 'metric-spaces']"
2195432,Another martingale solution to the ABRACADABRA problem,"A monkey is typing out uniform and independent random letters. Let $T$ be the first time at which it's typed out ABRACADABRA. What is $E(T)$? You can read the standard solution here . Here's mine. Rather than having new gamblers join the game at every stage, just have one gambler. He starts with 1 dollar, and bets that dollar the next letter will be A. If it is, he wins 26 dollars and bets it all the next letter will be B, and so on - each time he bets it all, and each time, if we wins, he gets 26 times his wager. If he ever loses, he goes back to 1 dollar and keeps playing . Let $X_n$ be the amount of money after $n$ after $n$ games, so $X_0=1$. $$E(X_{n+1}|X_n)=\frac{1}{26} 26 X_n + \frac{25}{26} 1$$ So that $X_n - \frac{25}{26}n$ is a martingale with bounded increments ($X_{n+1} - X_n$ is bounded by something on the order of $26^{11}$) and it can be shown that $E(T)$ is finite. Thus $$E(X_T-\frac{25}{26}T)=1$$
$$E(T)=\frac{26}{25}(E(X_T) - 1)=\frac{26}{25}(26^{11} - 1)$$ This is not the correct answer. Where did I go wrong?","['probability-theory', 'martingales']"
2195440,Finding the flow of vector field with lie group (orthogonal group),"If $X_S (A) = AS$ defines a smooth vector field, where $A \in O(n)$ (matrices with $A^{-1} = A^{T}$) and $S$ belonging to the space of skew matrices (matrices with $S^{T} = -S$). How would I calculate the flow of $X_S$ here? I'm using the definition of flow as one whose flow domain is all of $\mathbb{R} \times O(n)$, where the flow is the map $\theta : O(n) \times \mathbb{R} \to O(n)$ such that for all $s,t \in \mathbb{R}$ and $A \in O(n)$ the following holds:
$$\theta(A,0) = A \space\ \space\ \text{and} \space\ \space\ \theta(\theta(A,t),s) = \theta(A, s+t)$$
I'm only familiar with finding flow via solving an ODE (or system of them). Need help understanding how this works with Lie Groups.","['differential-geometry', 'lie-groups']"
2195462,Method of calculating the last digit of $7^{49}$,"I would like to point out the difference between $7^{7^7}$ and $7^{49}$. typically the notation $a^{a^{a^a}}$ means $a^{(a^a)}$. Heuristically, start from the top and work your way down. e.g. $2^{2^{{2^2}}} = 2^{(2^{({2^2})})} = 2^{(2^{4})} = 2^{16}=65536$.","['number-theory', 'modular-arithmetic']"
2195485,proving a group is soluble (solvable if you are american),"(7 marks) (a) Show that every group $G$ of order 56 has a normal Sylow subgroup. (Hint: If $n_7 \neq 1$ then show that $n_7 = 8$ deduce from this that $G$ has 48 elements of order 7, leaving just enough elements for one Sylow 2-subgroup). If $n_7=1$, the the Sylow 7-subgroup of $G$ is normal. Suppose $n_7=8$. The eight distinct Sylow 7-subgroups are cyclic so that $G$ has at least $6\cdot8 = 48$ elements of order 7. Now $n_2$ divides 7 and is odd, so that $n_2\in \{1,7\}$. If $n_2=7$, then let $P_1$ and $P_2$ be distinct Sylow 2-subgroups of $G$. There must exist a non-identity element $x\in P_1$ and $x \notin P_1$, but then $G$ has at least $48+8+1=57$ elements, which is nonsense. Thus $n_2=1$, so that the unique Sylow 2-subgroup of G is normal. b) Explain why (a) means that every group of order 56 is soluble.","['finite-groups', 'group-theory']"
2195496,Prime number decomposition,"What is the fastest way to decompose the given number to prime numbers without using calculator? Example : $$3575$$ What I do is : $$3575 = 3 \times 10^3 + 5 \times 10^2 + 7 \times 10 + 5 = 3\times5^3\times2^3 + 5^2 \times 2^2 \times 5 + 7 \times 5 \times 2 + 5$$ But now I do not know how to effectively get rid of ""$+$"".","['number-theory', 'prime-factorization', 'prime-numbers']"
2195507,Parity of Ramanujan's Tau Function,"I have been working problems out of Ram Murty's ""Problems in the Theory of Modular Forms,"" which has been marvelous. His solutions are complete, but are way too slick for satisfaction (probably by his design). So, I'm taking the exercise of filling in the gaps of understanding. Exercise 1.5.1 has had me exploring for some time. The intent appears to be merely an application of Jacobi's Triple Product Formula. It just seems like overkill to me and I'm wondering if there's a more forward way using triangular numbers. Here's the problem. Define $\tau(n)$ by $$q\Pi_{n=1}^\infty(1-q^n)^{24}=\sum_{n=1}^\infty\tau(n)q^n$$ Show that $\tau(n)$ is odd iff $n=(2m+1)^2$ for some $m$ . The first thing I did was change the $-$ to a $+$ , since we're working $(\text{mod }2)$ . Then, it's natural to use Kummer's Theorem to weed out the even binomial coefficients. This can be done in two ways, $$\Pi_{n=1}^\infty(1+q^n)^{24}=\Pi_{n=1}^\infty(1+q^{8n})^3 (\text{mod }2)$$ or $$\Pi_{n=1}^\infty(1+q^n)^{24}=\Pi_{n=1}^\infty(1+q^{8n}+q^{16n}+q^{24n})(\text{mod }2)$$ The first way is the one Murty uses before he wraps up summarily with Jacobi's Triple Product Formula $$\Pi_{n=1}^\infty(1-q^n)^3=\sum_{k=0}^\infty (-1)^k(2k+1)q^{k(k+1)/2}$$ Both ways should end up using the nice touch that odd squares are one more than $8$ times triangular numbers (substitute $q^{8}$ for $q$ ). More Basic Approach If we instead want to use the top product, all that's left is to expand in another way. I'm hoping the intricacy of Jacobi's formula will unravel some, since we are working in $(\text{mod }2)$ . Particularly, my hope is to set $y=(1,2,3,4,\dots)$ and use $$\tau(8n+1) = \text{card}(X_n)$$ where $$X_n=\{x\in\{0,1,2,3\}^\infty: x\cdot y=n\}$$ Next, I think it is natural to look at sequences in $$S=\{x\in\mathbb{Z}^\infty: x\cdot y=0\}$$ which are generated by addition/subtraction of elements of the form $$(\dots, 0, 0, 1,-1, 0, 0, \dots, 0, 0, -1, 1, 0, 0, \dots)$$ Then, $\tau(8n+1)$ is something like the size of ""orbits"" of $S$ acting on $X_n$ (of course $S$ is not technically an action because it's not closed). Interest The reason I'm interested is that $X_n$ has an odd number of elements precisely when $n$ is a triangular number, i.e. when $(1,1,\dots,1,1,0,0,0,\dots)\in X_n$ . So, I'm hoping there is a nice involution of $X_n$ that only fixes the point beginning with $n$ 1's, then all zeros, $(1,1,\dots,1,1,0,0,0,\dots)$ . The parity problem would be solved immediately. Please let me know if you find such an involution, or if you have other comments or approaches regarding this problem. Thank you very much for reading!","['parity', 'infinite-product', 'q-series', 'combinatorics', 'elementary-number-theory']"
2195513,What exactly is a basis in linear algebra?,"I have a brief understanding of bases. But I don't know if it is right or not. So, I just need someone to correct me if it's not. When we look for the basis of the image of a matrix, we simply remove all the redundant vectors from the matrix, and keep the linearly independent column vectors. When we look for the basis of the kernel of a matrix, we remove all the redundant column vectors from the kernel, and keep the linearly independent column vectors. Therefore, a basis is just a combination of all the linearly independent vectors. By the way, is basis just the plural form of base? Let me know if I am right.","['matrices', 'linear-algebra', 'vectors']"
2195567,Exponentiating the Lie Derivative: Which direction does it rotate?,"I've been working through the book Functional Differential Geometry by Sussman and Wisdom and am having trouble with an example they give. We can exponentiate a Lie derivative
$$
e^{\mathsf{t\mathcal{L}_v}}\mathsf{y} = \mathsf{y + t \mathcal{L}_v y + \frac{t^2}{2!} \mathcal{L}^2_v y + \dots }
$$
which evolves $\mathsf{y}$ along the integral curves of $\mathsf{v}$.  As a concrete example they evolve the coordinate basis vector $\frac{\partial}{\partial \mathsf{y}}$ along $\mathsf{J}_z = x\frac{\partial}{\partial \mathsf{y}} - y\frac{\partial}{\partial \mathsf{x}}$  (a counter clockwise circular field or z-angular momentum generator) and give as an answer $$\exp(a \mathcal{L}_{\mathsf{J}_z})\tfrac{\partial}{\partial \mathsf{y}} = -\sin(a)\tfrac{\partial}{\partial \mathsf{x}} + \cos(a)\tfrac{\partial}{\partial \mathsf{y}}.$$
This agrees at $a=0$ and indicates that the evolution maintains the orientation of $\mathsf{v}$ and $\mathsf{y}$ along the flow.  $\tfrac{\partial}{\partial\mathsf{y}}$ just rotates along $\mathsf{J}_z$. If I try to calculate the expansion, for the first term I get
$$
a\mathcal{L}_{x\frac{\partial}{\partial \mathsf{y}} - y\frac{\partial}{\partial \mathsf{x}}}\tfrac{\partial}{\partial \mathsf{y}} = 
a\mathcal{L}_{- y\tfrac{\partial}{\partial \mathsf{x}}}\tfrac{\partial}{\partial \mathsf{y}} = -a[y\tfrac{\partial}{\partial \mathsf{x}}, \tfrac{\partial}{\partial \mathsf{y}}] = a\tfrac{\partial}{\partial \mathsf{x}} 
$$
which disagrees with the expansion of the answer by a sign, giving rotation in the opposite direction. The intuitive notions of the Lie Derivative I have also say it should be $a\tfrac{\partial}{\partial \mathsf{x}}$.  For example, beginning at $x=1, y=0$ we can travel along $\epsilon\mathsf{J}_z$ and then  $\epsilon \tfrac{\partial}{\partial \mathsf{y}} $, or begin along $\epsilon \tfrac{\partial}{\partial \mathsf{y}} $ and then along $\epsilon\mathsf{J}_z$ (points slightly to the left).  The difference between these paths will be $\epsilon^2 \tfrac{\partial}{\partial \mathsf{x}} $.  The same goes for beginning at $x=0,y=1$ or if I do it using a pushforward.  I'm tempted to say the book is mistaken, and that to get a rotation operator which coincides with the direction of $\mathsf{J}_z$ we should use $e^{-t\mathcal{L}_{\mathsf{J}_z}}$ but would appreciate some confirmation.","['lie-derivative', 'differential-geometry']"
2195568,Evaluating the limit: $\lim _{x\to \infty }\left(2^x\sin\left(\frac{b}{2^x}\right)\right)$,"I need to find the following limit :
$$\lim_{x\to \infty}\left(2^x\cdot \sin\left(\frac{b}{2^x}\right)\right)$$ I have tried it but I keep getting stuck, so any help would be helpful!
Thank you!",['limits']
2195631,"Induction, Well-Ordering, and my Failed Proof Attempt","I recognize there are posts already generally on the logical equivalence of induction and the well-ordering principle, however, I'd really appreciate some advice for finding the monster lurking underneath this marsh of poor reasoning. Thanks! Complete induction $\implies$ well-ordering principle Consider a statement $S$ where $S(n)$ states that a given subset of $\mathbb{N}$ with cardinality of $n$ has a least element. Let, for a set $X \subseteq \mathbb{N}$, $|X| = 1$. By the reflexive axiom, $\forall a \in X, a \le a$ so $S(1)$ holds. Now we assume $S(n)$ is true so $|X_{n}| = n$ and $\exists \ a\ \forall \ b\ : (a,b \in X) \implies a \le b$. $S(n+1)$ would state that the well-ordering principle holds for a set $X_n \cup {z}$ where $z$ is a new element. Since the well ordering principle held on $X$, there is a least element of $X$, call it $a$. Now $(z \le a)\vee (z > a) $. If the former is true than $z$ is now the least element. If the latter is true then $a$ is still the least element. Why is this seemingly much more obvious ""solution"" wrong? I have a gut feeling this has something to do with considering arbitrary elements of the power-set of $\mathbb{N}$ - which is uncountably infinite - and forming a bijection from $\mathbb{N}$ to these subsets of $P(\mathbb{N})$.","['induction', 'proof-writing', 'elementary-set-theory']"
2195642,"Spivak's calculus, chapter 2 question 3 (c)","The question is:
Give another proof that $nCk$ is a natural number by showing that $nCk$ is the number of sets of exactly $k$ integers each chosen from $1,.....,2$. I understand that I have to prove that $nCk$ is a natural number, but I do not understand the ""set"" part or what exactly the question asks of me. 
I can feel that the number will be a natural, but could anyone clarify both the question and the answer please? I am self-studying, thanks. The answer from the solution manual is:","['combinatorics', 'proof-writing', 'calculus', 'proof-verification']"
2195645,Exchange of Gradient and Expectation,"Let $f : \mathbb{R}^n \to \mathbb{R}^k$ and let $\nabla$ be the gradient operator. Under, what condition can we interchange \begin{align}  \nabla_u E[f(X+u)]= E[
 \nabla f(X+u)] \end{align} I am well aware of the condition for the one dimensional case. See for example https://en.wikipedia.org/wiki/Leibniz_integral_rule","['probability-theory', 'expectation']"
2195654,"If $x_1 = 3$, $x_{n+1} = \frac{1}{4-x_n}$ for $n \geq 1$, prove the sequence is bounded below by $0$, above by $4$.","If $x_1 = 3$, $x_{n+1} = \dfrac{1}{4-x_n}$ for $n \geq 1$, prove the sequence is decreasing and bounded below by $0$, above by $4$. Want to show: $0 < x_{n+1} < x_n < 4$ I decided to do the bounded part first. Base Case , $n=1$ $0 < 3 < 4$ [works] IH Suppose $0 < x_k < 4$, for some $k\in\mathbb{N}$ IS Show $0 < x_{k+1} < 4$ We know $x_{k+1} = \dfrac{1}{4-x_k}$ We also know that $0 < x_k < 4$, by IH . So we have: $$\dfrac{1}{4} < x_{k+1} < \infty$$ But this doesn't show the boundness. Where have I went wrong?","['recurrence-relations', 'calculus', 'induction', 'sequences-and-series', 'discrete-mathematics']"
2195655,"Calculate the path integral: $\int_{\lambda}\left[2z+\sinh\left(z\right)\right]\,\mathrm{d}z$","Calculate the path integral: $$\int_{\lambda}\left[2z + \sinh\left(z\right)\right]\,\mathrm{d}z$$ where $\displaystyle\lambda\left(t\right) =
\frac{t^{2}}{4} + \frac{\mathrm{i}t}{2}\,,\quad
\left(~0 ≤ t ≤ 4~\right)$ . Im not sure how to parameterize this and also how to answer the rest of the question so any help will be appreciated.","['complex-analysis', 'integration']"
2195656,What exactly is the geodesic flow?,"I understand what a geodesic is, but I'm struggling to understand the meaning of the geodesic flow (as defined e.g. by Do Carmo, Riemannian Geometry , page 63). I can state my confusion in two different ways: 1) Do Carmo writes: Why does a geodesic $\gamma$ uniquely define a vector field on an open subset ? In other words, why are the values of the vector fields uniquely defined on those points that are not on the geodesic $\gamma$? 2) In local coordinates, the geodesic flow is defined as the solution to the ordinary differential equation $$
\tag{1}\frac{d^2 x_k}{dt^2}+\sum_{i,j}\Gamma^k_{ij}\frac{dx_i}{dt}\frac{dx_j}{dt}=0
$$ For the solution to be unique on $TM$ (or on an open subset), we need some boundary condition. The only boundary condition I can see is a given geodesic $\gamma(t)$. What are the boundary conditions for this ODE?","['ordinary-differential-equations', 'geodesic']"
2195658,Is a fiber in algebra the same as a fiber in topology,"I am reading Dummit & Foote, and they describe a fiber in algebra as property of a homomorphism. So if $\phi$ is a homomorphism from a group $G$ to a group $H$, then the fibers of $\phi$ are the sets of elements in $G$ that map to a single element in $H$. Now I know that there is a notion of fibers in topology as well. I was just wondering if the definition of a fiber in topology is related to the definition of a fiber in algebra? Or is this just a case of inconvenient use of the same name?","['abstract-algebra', 'general-topology', 'fiber-bundles']"
2195661,Prime ideal implies irreducible affine variety,"Let $X$ be some affine variety. I am trying to understand why if $I(X)$ is prime, then $X$ is irreducible. In a proof here , the author states: Let $I(X)$ be a prime ideal, and suppose that $X=X_1\cup X_2$. Then $I(X)=I(X_1)\cap I(X_2)$. As $I(X)$ is prime, we may assume $I(X)=I(X_1)$, so $X=X_1$. My question is about this last sentence: ""As $I(X)$ is prime, we may assume $I(X)=I(X_1)\dots$"" What about the primeness of $I(X)$ implies this? For context, the reference the author provided for the last statement was If $X_1\subset X_2\subset\mathbb{A}^n$, then $I(X_1)\supset I(X_2)$.",['algebraic-geometry']
2195675,Bounding OR-type coordinates between $n$ Boolean vectors in a ball of radius $r$,"Say I have set of $m$ Boolean vectors 
$$B = \{x_1,\ldots, x_m\}$$ 
where $x_i  = [x_{i,1},\ldots, x_{i,n}] \in \{0,1\}^n$ for all $x_i \in B$. I know the following about the vectors $x_i \in B$: (i) $|x_i| \in [1,n-1]$ for all $x_i \in B$ ( at least 1 zero coordinate, and one non-zero coordinate) (ii) $d(x_i, x_j) \geq 1$ for all $x_i, x_j \in B$ ( all vectors are distinct ) (iii) $d(x_i, x_j) \leq r$ for all $x_i, x_j \in B$ (*all vectors differ by at most $r$ coordinates) Here, $d(x_i, x_j) = |x_i - x_j| = \sum_{k=1}^{k=n} 1[x_{i,k} \neq x_{j,k}]$. My question is the following: Given a vector $x_i \in B$, what is maximum number of coordinates where $x_i$ is zero, but at least one of the other vectors in $B$ is non-zero? That is, what is the largest possible size of the following set: $$Z(x_i) =  \left\{k =1,\ldots, n ~\Big|~ x_{i,k} = 0 ~\text{and}~ \sum_{j \neq i} x_{j,k} \geq 1 \right\} $$ Partial answer : I can derive a non-trivial bound using only (iii) as shown below. I am not sure if this bound is tight, or whether it can be improved. In particular, my bound does not account for (i) and (ii). To get the bound, first define: $$S_{01}(x_i,x_j) = \{k ~|~ x_{i,k} = 0, x_{i,j} = 1\}$$ and note that: $$|S_{01}(x_i,x_j)| = \frac{d(x_i,x_j) + |x_j| - |x_i|}{2}$$ Since $\sum_{j \neq i} x_{j,k} \geq 1$ requires that $x_{j,k} = 1$ for at least one $j \neq i$, we have the following upper bound: $$\begin{align}
|Z(x_i)| &\leq \max_{j \neq i} |S_{01}(x_i,x_j)| \\ 
&=\max_{j \neq i} \frac{d(x_i,x_j) + |x_j| - |x_i|}{2} \\ 
&\leq \frac{r - |x_i| + \max_{j \neq i}  |x_j|}{2} 
\end{align}$$","['boolean-algebra', 'combinatorics', 'real-analysis', 'discrete-mathematics']"
2195685,When is $\frac{x^2+xy+y^2}{49}$ an integer?,"Find the number of distinct ordered pairs $(x, y)$ of positive integers such that $ 1\leq x, y \leq 49$ and $\frac{x^2+xy+y^2}{49}$ is an integer. Multiplying the given equation by $(x-y)$ gives $x^3 \equiv y^3 \pmod{49}$. Thus we can't have $7 \mid x$ while $7 \nmid y$ or vice-versa. Rearranging the given equation gives $$(x+y)^2 \equiv xy \pmod{49}.$$ If $7 \mid x,y$ then there are $49$ solutions. Now suppose that $7 \nmid x,y$. Then we have $$(x+y)^2(xy)^{-1} \equiv xy^{-1}+2+xy^{-1} \equiv 1 \pmod{49}.$$ Thus, $xy^{-1}+yx^{-1} \equiv xy^{-1}(1+(yx^{-1})^2) \equiv -1 \pmod{49}$. I didn't see how to continue from here. The answer is $$2\varphi(49)+49,$$ and $\varphi(49)$ is the number of units modulo $49$, so maybe we can use that to solve this question.",['number-theory']
2195715,The famous prime race and generalizations,"So I was messing around with the famous prime race that comes down to this: We make a list of primes. The list has two rows; the top row is for primes $1\mod 4$ and the bottom row for primes $3\mod 4$. Our list, up to the 10th prime: 5  13  17  29
3   7  11  19  23  31 The idea is that the two rows are ""racing"" to be longest, and the bottom row always seems to be winning. An intuitive visual representation of this would be, we start at $0$ on the number line, and start going through the primes. Whenever we encounter a prime $1\mod 4$ we do a step right, and whenever the prime is $3\mod 4$, we do a step left. This is not quite such an interesting visualization however; it's just a line. However, this becomes more interesting when we generalize it. The generalization. We generalize this by picking a number, and do the prime race $\mod n$. Apart from a couple (certainly finite number of) exceptions, all primes fit in one of the $\varphi(n)$ categories. With this, I mean, let $U(n)=\{u_1,u_2,\cdots,u_{\varphi(n)}\}$ with $0\leq u_i<n$, and $i<j\iff u_i<u_j$, and $\gcd(u_i,n)=1$, then each $u_i$ represents a category; all primes $p$ (except for the ones that are divisors of $n$) must have $p\equiv u_i\mod n$ for exactly one $u_i$. Now we start going through the primes, and whenever we encounter a prime $p\equiv u_i\mod n$, we take a step length $1$ in the direction $\tfrac{2i\pi}{\varphi(n)}$. An example, $n=8$. This is a relatively easy example to try on grid paper, since $\phi(8)=4$, so we have $4$ directions to go in. For the first $10^3$, $10^4$ and $10^5$ primes, this looks like this (the red dot is the starting point, the blue dot is the end. Open the image in a new tab to see it at full resolution) Interestingly enough, the red dot, the start, is in all images the right-most point in the entire thing, even though it is seemingly random. So this is the case for $n=4$, and for $n=8$. Maybe it's the powers of $2$, let's try $n=10$ (which happens to have $\phi(10)=4$ as well). We get (again, for the first $10^3$, $10^4$ and $10^5$ primes): We see this time it's tending more to the bottom-left rather than just the left; but again, one side (the top-right) remains untouched. Let's try this again, but this time with an $n$ without $\phi(n)=4$. We try $n=11$ through $n=15$: $n=11$ $n=12$ $n=13$ $n=14$ and $n=15$ We see $n=12$ is very convincingly going to the left and not touching the right, and $n=15$ is leaning towards the top-left, and leaves the bottom-left almost untouched. $n=11$, $n=13$ and $n=14$ however there's no specific way the blob seems to lean. Question. Why do the blobs with $\varphi(n)=4$ lean so convincingly to one side, while not even touching the other? Do there exist similar patterns for other $n$ with different $\varphi$? Are they related to the fact that the $\varphi$ is a power of $2$ (since $n=15$ has similar behaviour)? I expected them to stay around the middle a lot more since the density of the primes $\mod n$ is evenly spaced, that is, picking any prime, it has chance $\frac{1}{\varphi(n)}$ to be $i\mod n$ (where $\gcd(i,n)=1$).","['distribution-of-primes', 'number-theory', 'visualization', 'prime-numbers', 'elementary-number-theory']"
2195726,"Regarding a sheaf of $\mathcal O_X$-modules as a sheaf of $\mathcal O_Z$-modules, where $Z$ is a closed subscheme","Let $(X,\mathcal O_X)$ be a scheme, and $(Z,\mathcal O_Z)$ a closed subscheme, with $i: Z \rightarrow X$ the defining closed immersion.  The underlying map of topological spaces is the inclusion. Since $i^{\#}: \mathcal O_X \rightarrow i_{\ast} \mathcal O_Z$ is a surjective morphism of sheaves of rings, the kernel $\mathscr I$  of $i^{\#}$ induces an isomorphism of sheaves of rings $\mathcal O_X/\mathscr I \cong i_{\ast} \mathcal O_Z$.  Since we are dealing with a sheaf of rings $\mathcal O_Z$ on a closed subset, we recover $\mathcal O_Z$ by taking the inverse image sheaf: $$\mathcal O_Z \cong i^{-1} (\mathcal O_X/\mathscr I)$$ Now, let $F$ be a sheaf of $\mathcal O_X$-modules.  Suppose that for each open $U$, $\mathscr I(U)$ annihilates $F(U)$.  Then $F$ is well defined as a sheaf of $\mathcal O_X/\mathscr I$-modules. This implies $i^{-1}F$ is a sheaf of $\mathcal O_Z = i^{-1}(\mathcal O_X/\mathscr I)$-modules. Suppose furthermore that for each $x \in X - Z$, the stalk $F_x = 0$.  Then $F$ can be recovered as $i_{\ast} i^{-1}F$. In this case, does it make sense to say that $F$ is a sheaf of $\mathcal O_Z$-modules ? (even though $F$ and $\mathcal O_Z$ are sheaves on different topological spaces)  If so, what is the meaning of ""$F$ is a sheaf of $\mathcal O_Z$-modules?"" I have encountered this language here (Mumford, ""Red Book of Varieties and Schemes""):",['algebraic-geometry']
2195738,convergence in distribution in $l_{2}$,"Consider the space $l^{2}$. Let $X_{n}$ be a sequence of random vectors in $l^{2}$, such that $X_{n} \stackrel{d}{\to} X$. Let $Z_{n}$ be another sequence of random vectors in $l^{2}$, such that $$
\mathbb{E}[\|X_{n} - Z{_n} \|_{2}^{2}] \to 0 \quad\text{as}\quad n\to\infty.
$$ The question: is it true that $Z_{n} \stackrel{d}{\to} X$, as $n\to \infty$?","['stochastic-processes', 'probability-theory', 'probability-distributions', 'statistics', 'probability']"
2195750,"Prove or disprove that if $\sum a_n$ and $\sum b_n$ are both divergent, then $\sum (a_n \pm b_n)$ necessarily diverges.","Prove or disprove that if $\sum a_n$ and $\sum b_n$ are both divergent, then $\sum (a_n \pm b_n)$ necessarily diverges. I decided to disprove this by offering the example: Let $a_n = n$. Let $b_n = -n$. Then $\sum (a_n + b_n) = \sum 0 = 0$, which converges. However, my question says $\pm$, and in the $-$ case, we have $\sum (a_n - b_n) = \sum 2n = \infty$, which diverges. So, does this still count as a valid counter example or not? I partly proved it wrong so I think it's still valid.","['sequences-and-series', 'proof-verification', 'calculus', 'convergence-divergence', 'discrete-mathematics']"
2195786,Find a and b such that $ P(\sigma_*^2 < a\sigma ^2) =0.025$ and $ P(\sigma_*^2 > b\sigma ^2) =0.025$,"Let $X_1, X_2 ... X_n \text{ ~ } N(\mu, \sigma^{2})$ be identically distributed and consider the estimate 
$$ \sigma_* ^{2} = \frac{1}{n} \sum\limits_{i=1}^n (X_i - \mu)^2 $$
for the variance. Find number $a$ and $b \in \mathbb{R} $  such that $ P(\sigma_*^2 < a\sigma ^2) =0.025$ and  $ P(\sigma_*^2 > b\sigma ^2) =0.025$ This is all that was given in the question, and I'm not sure how to approach it, I was thinking of using the equation for $\sigma ^2$ but its the same (or I think it is) as $ \sigma_* ^{2}$. This question is in relation to hypothesis testing and so far I've looked at z and t tests, but I'm not sure how i can standardise these two values. Any help is much appreciated.","['normal-distribution', 'variance', 'hypothesis-testing', 'statistics', 'probability']"
2195812,What can we say if the dot product of two vectors is equal to 1,The question really is in the title. I know what it means if the dot product equals 0 but I find it interesting thinking what it means when it equals exactly 1 and can't seem to find anything online to enlighten me. Thanks,['linear-algebra']
2195815,Dirichlet's theorem on primes in arithmetic progressions: how to arrive at the result naturally?,"I've seen the following theorem proved several times in several different ways: Let $a, b$ be relatively prime.  There are infinitely many prime numbers in the sequence $a,a+b,a+2b, ...$ The most difficult part of the argument comes down to showing that if $\chi$ is a nontrivial Dirichlet character, then $L(s,\chi)$ does not vanish at $s = 1$. I have understood the individual details of the arguments.  But I have never felt like I really understand why the result is true. Suppose you temporarily forgot what you knew about $L$-functions, Dirichlet density, Fourier analysis etc.  You know basic number theory and complex analysis.  How would you go about thinking about this problem in such a way as to naturally arrive at the result?  If necessary, you can rediscover L-functions as part of the process, but please justify your discovery with intuition.","['number-theory', 'analytic-number-theory', 'algebraic-geometry']"
2195902,"How to explain connections between bounded Radon-Nikodym derivatives, convex combinations of probabilities, and conditional probability","I have been thinking a lot about conditional probabilities recently and have noticed what to my mind are some surprising connections. I will sketch a few results and I'd just like someone to provide some deeper insight into why they hold. Let $(\Omega, \mathcal{F}, P)$ be a probability space. Let $P_E = P(\cdot \mid E)$ be the conditional probability given $E$. It's easy to verify that $P_E \ll P$ and that the Radon-Nikodym derivative $dP_E/dP$ is essentially bounded. Indeed, $\mathbf{1}_E/P(E)$ is a version of $dP_E/dP$ because for all $A \in \mathcal{F}$
$$\int_A \frac{\mathbf{1}_E}{P(E)}dP = \frac{P(A \cap E)}{P(E)}.$$ Somewhat surprisingly (to me), something like a converse holds as well. The following result is due to Diaconis and Zabell ( 1982 , Theorem 2.1). Assume $(\Omega, \mathcal{F})$ is countable (the result holds in general, and this is just for ease of exposition). Theorem. Suppose $Q$ is a probability measure on $(\Omega, \mathcal{F})$ such that for some $B \geq 1$ and all $\omega$
$$Q(\omega) \leq \beta P(\omega).$$
Then, there exists a probability space $(\Omega', \mathcal{F}', P')$ a sequence $\{ E_\omega : \omega \in \Omega \}$ of events in $\mathcal{F}'$ such that $P'(E_\omega) = P(\omega)$ and an event $E \in \mathcal{F}'$ such that $P'(E) > 0$ and $P'(E_\omega \mid E) = Q(\omega)$. In other words $(\Omega, \mathcal{F}, P)$ embeds into a richer space in which $P$ is a marginal probability and $Q$ is a conditional probability. We say that $Q$ can be obtained by conditioning $P$. Proof Sketch. It follows from the supposition that $P = (1/\beta)Q + (1 - 1/\beta)R$ for some probability measure $R$ on $(\Omega, \mathcal{F})$. Let $\Omega' := \Omega \times \{a,b\}$, $E_\omega := (\omega, a) \cup (\omega, b)$, and $E = \cup_\omega (\omega, a)$. Let $P'(\omega, a) = (1/\beta)Q(\omega)$ and $P'(\omega, b) = (1 - 1/\beta)R(\omega)$, and it's easy to verify that the result follows.QED Moreover, it's easy to check that $Q \leq \beta P$ is equivalent to $P = \alpha Q + (1 - \alpha)R$ for some probability $R$. We therefore have Corollary. The following are equivalent. (1) $Q \leq \beta P$, (2) $P = \alpha Q + (1 - \alpha)R$ for some probability $R$, (3) $Q$ can be obtained by conditioning $P$. The Corollary seems to relate three different properties that probabilities can have. (1) A measure-theoretic property: having a bounded density. (2) A vectorial property: being a convex combination of other probabilities. (3) A probabilistic property: being a probability that arises by conditioning. Is there anything deep that can be said about the above results? Why are (1), (2), and (3) equivalent? Can (1), (2), and (3) be subsumed nicely into some more general theory? Is there anything more precise that can be said to relate the three ""points of view"" (measure-theoretic, vectorial, probabilistic) that (1), (2), and (3) exhibit? I have recently begun reading about disintegrations and suspect that the material above is related to that topic somehow, but I cannot say anything precise at the moment.","['probability-theory', 'measure-theory']"
2195928,"Show $\pi: A \times B \to A$ defined by $ \pi (a,b)=a$ is a surjection, but not an injection.","Let $|A|>1$ and $|B|>1$. Show $\pi: A \times B \to A$ defined by $ \pi (a,b)=a$ is a surjection, but not an injection. My attempt: Surjection: For any $(a,b) \in A \times B, \exists  a \in A$ such that $\pi (a,b)=a$. Thus, $\pi$ is surjective.  Is this correct? Not injective: Suppose $(a,b)$ and $(a_1, b_1) \in A \times B$ and $ \pi (a,b)=\pi (a_1, b_1)$, then $a=a_1$. I know this is wrong, because I am suppose to show that it is NOT injective. Any help is appreciated.",['discrete-mathematics']
2195936,$H^1$ compactly embedded in weighted $L^2$ space,"I'm working on the following assignment: Define the weighted $L^2$-space $L^2_s$ on $\mathbb{R}^n$ to be the completion of $C_{\text{comp}}^{\infty}(\mathbb{R}^n)$ with the norm
  $$
\|f\|_{L^2_s}=\left(\int_{\mathbb{R}^n}(1+|x|^2)^s|f(x)|^2d^nx\right)^{1/2}.
$$
  Show that $H^1\subset L^2_s$ is a compact embedding when $s<0$. Here ""compact embedding"" means that every uniformly bounded sequence in $H^1$ has a subsequence which is Cauchy in $L^2_s$. But since $L^2_s$ is just $L^2(\mathbb{R}^n,\mu_s)$ with the measure $d\mu_s=(1+x^2)^sd\mathcal{L}^n$ (here $\mathcal{L}^n$ is Lebesgue measure), $L^2_s$ is complete and this is the same as saying every uniformly bounded sequence in $H^1$ has a convergent subsequence in $L^2_s$. Thoughts I've spent the last 2-3 days thinking about this problem, but to no avail. It's driving me insane. Here are a couple of my thoughts/observations: For small enough $n$ and $|s|$ large enough, $L^2_s=L^2(\mathbb{R}^n,\mu_s)$ is a finite measure space since $s<0$. The Fourier transform is a unitary isomorphism from $L^2_s=L^2(\mathbb{R}^n,\mu_s)$ to $H^s$. $\|f\|_{L^2_s}\leq\|f\|_{L^2}\leq\|f\|_{H^1}$, so given a uniformly bounded subsequence $\{f_k\}\subset H^1$, $\big\{\|f_k\|_{L^2_s}\big\}$ is uniformly bounded and hence we can find a subsequence $\{f_{k_j}\}$ such that $\big\{\|f_{k_j}\|_{L^2_s}\big\}_{j\in\mathbb{N}}$ converges, but the problem is this doesn't necessarily mean that $\{f_{k_j}\}$ converges in $L^2_s$ I wanted to use the Rellich compactness theorem, but this only holds when there exists a compact $K\subset\mathbb{R}^n$ such that $\text{supp }f_k\subset K$ for all $k$. I thought using the theorem on closed balls centered at zero whose radii go to infinity and then diagonalizing, but I don't believe this should work. For simplicity, I was thinking about the case where $n=1$. In this case I have the Sobolev embedding theorem which shows that $H^1(\mathbb{R}^n)\subset C_0(\mathbb{R}^n)$, where $C_0(\mathbb{R}^n)$ is the family of continuous functions on $\mathbb{R}^n$ which vanish at infinity. Morrey's inequality also shows here that the elements of $H^1$ are H$\ddot{\text{o}}$lder continuous, so a uniformly bounded sequence in $H^1$ would be equicontinuous, but I don't think necessarily bounded, so this ruins trying to use Arzela-Ascoli in any significant way. I feel like, as with most things in my life, I'm making this more complicated than it needs to be. I feel as though a push in the right direction would hugely beneficial. Any help is greatly appreciated. Please only hints as this is an assignment for class.","['real-analysis', 'lp-spaces', 'sobolev-spaces']"
2196002,Four scientists combinatorial problem.,Four scientists are working on a secret project.  They wish to lock up the documents in a cabinet such that the cabinet can be open if and only if 3 or more scientists are present. a. What is the smallest number of locks needed? b. What is the smallest number of keys that each scientist must carry? c. Design a scheme with the minimum number of locks needed and the minimum number of keys for each scientist that will actually accomplish the desired security.,['discrete-mathematics']
2196003,Universal typeface: how to take average of letters?,"In 2014, BIC launched the universal typeface experiment . It was a crowdsourcing attempt to make a universal typeface. Users could enter their written form of the letters and the experiment then took the average of all the letters of everyone who participated. You can define a letter as a union of finite bounded (closed) curves. How would one define the average of two letters? Remarks Let $\gamma_1:[0,1] \rightarrow \mathbb{R}^2$ and $\gamma_2:[0,1] \rightarrow \mathbb{R}^2$ be two curves. We could naively define the average as:
$$\gamma:[0,1]\rightarrow \mathbb{R}^2:x \mapsto \frac{\gamma_1(x)+\gamma_2(x)}{2}.$$
But taking a different parametrisation would give a different result. This means that this is not a good definition for an average. One would need the obvious property that the average of $n$ copies of the same letter should yield the same letter. Edit I included an image for how an average should look like. (I didn't know how to tag this, so feel free to add more tags.)","['curves', 'average', 'geometry']"
2196025,Motivation for test function topologies,"I'm a phisicist, who started looking just a little bit into distribution theory, so I can claim to know what I'm doing when throwing about dirac-deltas. Hence I only know two test function spaces: $\mathcal{D}=C^{\infty}_c(\Omega)$ (smooth functions with compact support) and $\mathcal{S}  (\Omega)$ (Schwartz-space) where $\Omega\subseteq\mathbb{R}^n$ open. Now I wonder what the motivation is for defining the topologies on these spaces as one does it. I'm reading ""Fundamental Solutions of Partial Differential Operators"" by Ortner and Wagner. They avoid actually defining the topologies on these spaces and only talk about convergence of sequences. I'm actually not sure what the exact relationship is between the sequence convergence and the topologies. For Schwarz space the question is irrelevant, since it its topology is metric. However $\mathcal{D}$ is not sequential. Question 1: Is there a way to characterize the topology of $\mathcal{D}$ with sequences, as in saying ""the coarsest topology having that convergence properties for sequences"" or something similar? What is the reason most people don't bother talking about the actual topology and seems satisfied with sequences, although the topology is not sequential? I've heared something about that being irrelevant for linear maps, but haven't seen a precise statement. As far as I know the definitions of sequence convergence are ""Uniform convergence of all derivatives on compact sets with supports contained in a compact set"" for $\mathcal{D}$ and ""uniform convergence of all derivatives"" for  $\mathcal{S}$ respectively. The rest of my question deals with motivating these definitions. For Schwarz space ""to some extent"" the motivation, as far as I know, is that almost everything one needs is continuous on this space and maps back into it. In particular all differential operators, and most particularly the Fourier transform. I'm fairly happy with this definition, although there is surely more to understand there. In particular I would like to know (Soft) Question 2: Is there a way to characterize Schwarz space as ""The subspace $X$ of $C^{\infty}(\Omega)$ where ??? can be defined $?:X\to X$"" and the topology (or sequential convergence) is motivated in some way by requiring all the ??? stuff to be continuous? In terms of the ??? stuff I'm thinking of usefull things like derivatives and fourier transforms, not artificial examples making it work out right. [I found a claim that one gets this starting from $L^1(\Omega)$ by taking differentiation and multiplication by polynomials as some kind of closure. Needs clarification and proof though] Let's turn to $\mathcal{D}$:
I'm aware of Why does a convergent sequence of test functions have to be supported in a single compact set? , where the motivation of the convergence criteria of $\mathcal{D}$ is discussed to some extent. In particular it seems to me, that the notion of distribution depends on the topology on $\mathcal{D}$. Hence an answer saying something like ""that part doesn't matter for compactly supported distributions"" makes no sense to me. I don't really understand the answers and would like more detail. Can something similar to question 2 be answered for $\mathcal{D}$? (Soft) Question 3: What is the motivation behind the topology for  $\mathcal{D}$? Why all that talk about compact sets?  Certainly I would be also happy with a motivation for what the topological dual (distribution space) should look like and then looking for what spaces have that as their dual. In particular, the quoted question confused me on the following: The space $\mathcal{D}$ being locally convex, the topology is given by a family of semi-norms. That would mean we need to reabsorb the criterion ""all supports of functions in the sequence (when testing for convergence) lie inside some compact set"" into just a set of seminorms. Can this be done? I haven't seen that.","['functional-analysis', 'distribution-theory']"
2196045,"How many ordered triples (a, b, c) exist if a, b, and c are positive digits and their product is divisible by 20?","Edit #1: Remember that a, b, and c are positive digits, so their values are between 1 & 9, inclusive I was thinking casework for this problem, but I got stuck on the last part. I can't figure out whether there are four cases or three. $20 = 2^2 * 5$, so you need two 2's and a 5. Case 1 : One five, either a four or an eight, and an odd number that is not five Case 2 : Two fives and either a four or an eight Here's where I get confused. I'm not sure whether it should be: Case 3 : One five and two even numbers or: Case 3 : One five and two even numbers that are different Case 4 : One five and two even numbers that are the same Here are the calculations that I have done so far: Case 1 : $2* 4 * 3!$, as the five is fixed, you can choose between the 4 & 8, and four choices for the odd number that is not five. The $3!$ accounts for the various permutations of the three that are chosen. Case 2 : $2 * 3$. The fives are fixed, you choose between the 4 & 8, and there are three different ways to arrange them after you choose your numbers. Edit #2: I also think that the answer is $102$. I 'cheated' (used Python and wrote a script) to figure that part out, but I want to figure out the answer with math, not programming. Thank you in advance!","['permutations', 'combinatorics', 'combinations']"
2196076,I believe I've found a third and fourth group of degree four. Tell me how wrong I am.,"According to every source I could find, the Cyclic group Z4 and the Klein four-group are the only two groups of degree 4.  As I'm new to group theory, I decided to figure out the groups by myself by creating multiplication tables through simple logical steps.  It was a trivial process for degrees 1,2,and 3, but when I got to degree 4 I found ""new groups"".  I've been trying to figure out where I went wrong, so I'm going to give you the multiplication tables of my ""new groups"" and hope that someone will tell me why they're not actually groups.  (I also found a couple groups of degree 5, and from what I know, there is a rule about groups of prime degree that say that I should be wrong, but I don't know why the extra ones that I found aren't groups either.  I'm hoping that these responses will help clear that up for me.) $$\begin{matrix}1&2&3&4\\
2&1&4&3\\
3&4&2&1\\
4&3&1&2\end{matrix}$$ and: $$\begin{matrix}1&2&3&4\\
2&4&1&3\\
3&1&4&2\\
4&3&2&1\end{matrix}$$ Thank you for any help.","['finite-groups', 'group-theory']"
2196118,Writing in Cartesian form and converting to polar form,"Question: Write $\frac{u+v}{w}$ in the form $re^{i\theta}$ when $u$=$1$, $v$=$\sqrt3i$, and $w$=$1+i$ I added $u$ and $v$ for the Cartesian form because it is easier to do. After adding $u$ and $v$, I get $\frac{1+\sqrt3i}{1+i}$. Need help converting to polar form please",['complex-analysis']
2196119,Introduction to Proof via Linear Algebra,"Many universities offer a transition course from computational courses like Calculus to proof-oriented courses like Abstract Algebra. Such courses often go by a name like ""Introduction to Proof"" or ""Transition to Higher Mathematics"". They typically contain an introduction to first-order logic (conditionals, conjunctions, negations, quantifiers, etc.) as well as various methods of proof (contradiction, induction, etc.). I'm hoping to find a text for a first course in linear algebra that fills the role of a ""transition course"" by deliberately incorporating first-order logic and proof techniques as part of the instruction. The text should be accessible to students with two semesters of Calculus (roughly the basics of single-variable differentiation, integration, and infinite series). In particular, the overwhelming majority of students will have never written a formal proof and will have extremely limited exposure to logic and set theory. Ideally, the author would discuss these topics just as they are needed in the treatment of linear algebra (as opposed to supposing the reader is familiar with them already). For example, the author might have a digression on proof by contradiction just prior to using it in some proof about linear independence. Less ideal (but still acceptable) would be a text that at the very least makes use of all the relevant ideas from first-order logic and proof techniques that one expects from a transition course. Hopefully, the progression of such a text would be such that the instructor could use a supplemental text to discuss, say, proof by contradiction just as it is about to make its first appearance in the text.","['reference-request', 'proof-writing', 'book-recommendation', 'education', 'linear-algebra']"
2196144,Find all functions $f:\Bbb{R}\to \Bbb{R}$ such that $f$ is continuous and $ f(f(x+y)) = f(x)+f(y)$,"Find all functions $f:\Bbb{R}\to \Bbb{R}$ such that $f$ is continuous
  and $ f(f(x+y)) = f(x)+f(y)$ My steps:
By inspection, $f(x)=x+c$ and $f(x)\equiv 0$ works, while $y=mx \,\,(m\neq1)$ and $y=mx+c\,\,(m\neq1)$ do not. Setting $x=y=0$ gives $f(f(0))=2f(0)$ Setting $y=-x$ leads to
$f(f(0)) = 2f(0) = f(x)+f(-x) \implies f(0) = \frac{f(x)+f(-x)}{2}$ However I am stuck here. Any suggestions?","['algebra-precalculus', 'contest-math', 'functions', 'functional-equations']"
2196178,Killing-form $\mathrm{Tr}(\mathrm{ad}_{X} \circ \mathrm{ad}_{Y})$ of $\mathfrak{so}(n)$,"I'm trying to understand how to write the Killing form $B: \mathfrak{g} \times \mathfrak{g} \to \mathbb{R}$ defined by
$$
B(X,Y) = \mathrm{Tr}(\mathrm{ad}_{X} \circ \mathrm{ad}_{Y})
$$ I'm trying to do this specifically for the Lie algebra $\mathfrak{g} = \mathfrak{so}(n)$, but I'm getting stuck right at the end. I know I $should$ be getting $B(X,Y) = (n-2)\mathrm{Tr}(XY)$ at the end of the day. In my solution I try to keep things in terms of a general Lie algebra, and then specify to $\mathfrak{so}(n)$ at the end. Assume $\mathfrak{g}$ is a matrix of size $n \times n$. Since $\mathrm{ad}_{X}(A) = [X,A]$, I'm thinking of the linear transformation $ad_{X}:\mathfrak{g} \to \mathfrak{g}$ as a matrix of size $n^{2} \times n^{2}$. Since
$$
\left( \mathrm{ad}_{X} \circ \mathrm{ad}_{Y}\right)(A) = [X,[Y,A]] = XYA - YAX - XAY + AYX
$$ If $M$ is my matrix (with components $M_{(j,k)(p,q)}$) corresponding to $\mathrm{ad}_{X} \circ \mathrm{ad}_{Y}$ in the sense that:
$$
\left( \mathrm{ad}_{X} \circ \mathrm{ad}_{Y}\right)(A)_{jk} = \sum_{p,q=1}^{n} M_{(j,k)(p,q)} A_{pq}
$$ I'm assuming then that $B(X,Y) = \mathrm{Tr}\left( \mathrm{ad}_{X} \circ \mathrm{ad}_{Y} \right) = \sum_{j,k=1}^{n} M_{(j,k),(j,k)}$. So after a little grinding, I find that:
$$
\left( \mathrm{ad}_{X} \circ \mathrm{ad}_{Y}\right)(A)_{jk} = \sum_{p,q=1}^{n} \left[ X_{jp}Y_{pq}A_{qk} - Y_{jp}A_{pq}X_{qk} - X_{jp}A_{pq}Y_{qk} + A_{jp}Y_{pq}X_{qk} \right] \\
= \sum_{p,q=1}^{n} \left[ \sum_{\ell=1}^{n} \left( X_{j\ell}Y_{\ell p} \delta_{kq} + Y_{q\ell} X_{\ell k} \delta_{j p} \right) - X_{jp} Y_{qk} - Y_{jp} X_{qk} \right] A_{pq} \\
$$ Where $\delta$ is the kronecker delta. So my matrix components are:
$$
M_{(j,k)(p,q)} = \sum_{\ell=1}^{n} \left( X_{j\ell}Y_{\ell p} \delta_{kq} + Y_{q\ell} X_{\ell k} \delta_{j p} \right) - X_{jp} Y_{qk} - Y_{jp} X_{qk}
$$ Finally, this tells me that the Killing-form ends up looking like:
$$
B(X,Y) = \sum_{j,k=1}^{n} M_{(j,k)(j,k)} \\
= \sum_{j,k=1}^{n} \left[ \sum_{\ell=1}^{n} \left( X_{j\ell}Y_{\ell j} \delta_{kk} + Y_{k\ell} X_{\ell k} \delta_{j j} \right) - X_{jj} Y_{kk} - Y_{jj} X_{kk} \right] \\
= 2n \mathrm{Tr}(XY) - 2 \mathrm{Tr}(X)\mathrm{Tr}(Y)
$$ So $B(X,Y) = 2n \mathrm{Tr}(XY) - 2 \mathrm{Tr}(X)\mathrm{Tr}(Y)$, and so far I'm still working with a general Lie algebra $\mathfrak{g}$. Looking at $\mathfrak{so}(n)$, I know this this is the set of skew-symmetric real matrices of size $n \times n$, which means they are all traceless. This simplifies my answer to $B(X,Y) = 2n \mathrm{Tr}(XY)$. However, $2n \neq n-2$! Where am I going wrong in my proof? Am I taking too large of a leap in assuming that I can write the matrix M corresponding to $\mathrm{ad}_{X} \circ \mathrm{ad}_{Y}$ so easily?","['matrices', 'bilinear-form', 'adjoint-operators', 'lie-algebras']"
2196236,"Multivariate Caluclus, harmonic, Divergence Theorem, Flux","Facts that would be helpful to the proof: I honestly have no idea how to begin/proceed since we rarely look at proofs or abstract idea ""near the origin"". We have only done a lot of parameterizations and flux integrals, etc. Any help or sketch of proof would be appreciated!! :)","['real-analysis', 'calculus', 'multivariable-calculus', 'multivalued-functions', 'divergence-operator']"
2196238,How many can the donuts be distributed among the staff?,"(1) A box of 13 donuts contains 5 catsup, 2 chocolate, 3 mayo, 3 plain. There are 12 members of the ics department each to have one donut. (a) How many ways can the donuts be distributed among the staff, if they all get one donut? (b) What is Prof. Duncan insists on having a chocolate donut? (2) (a) How many ways can 10 people be paired up for one on one basketball?
(b) How many ways can 10 people be divided into two teams to play each other? (1) (a) using the formula $\binom{n+k-1}{k-1}$ I get 16C11 * 13C11 * 14C11 * 14C11 (b) I'm not sure but I think it is 13C2 * 15C10 * 13C10 * 13C10 (2) (a) $\frac{10C2 \cdot 8C2 \cdot 6C2 \cdot 4C2 \cdot 2C2}{5!}$ (b) If the teams are equal then $\frac{10C5 \cdot 5C5}{2!}$ I'm not sure about the case then the teams are not balanced.","['combinations', 'combinatorics', 'discrete-mathematics']"
2196265,How can we evaluate $\int_{0}^{\infty}e^{-x}\ln (x) \sin\left({1\over x}\right){\mathrm dx\over x}?$,Given this integral $(1)$ $$\int_{0}^{\infty}e^{-x}\ln (x) \sin\left({1\over x}\right){\mathrm dx\over x}\tag1$$ How can we evaluate $(1)$? An attempt: $u=e^{-x}\implies \mathrm du=-e^x\mathrm dx$ $(1)$ becomes $$\int_{0}^{1}{\ln(-\ln u)\over \ln u}\cdot \sin\left({1\over \ln u}\right)\mathrm du \tag2$$ Using $$\sin x=\sum_{n=0}^{\infty}{(-1)^nx^{2n+1}\over (2n+1)!}$$ $(2)$ becomes $$\sum_{n=0}^{\infty}{(-1)^n\over (2n+1)!}\int_{0}^{1}{\ln(-\ln u)\over (\ln u)^{2n+1}}\mathrm du\tag3$$ $v=\ln(-\ln u)\implies \mathrm -e^{v-e^v}dv=\mathrm du$ $(3)$ becomes $$\sum_{n=0}^{\infty}{(-1)^n\over (2n+1)!}\int_{-\infty}^{\infty}{v\over e^{2nv+e^v}}\mathrm dv\tag4$$ This is where we got so far,"['integration', 'definite-integrals', 'sequences-and-series', 'calculus']"
2196266,Compute $\int_0^{\infty} \frac{1}{e^{x}+x}dx$,"I am trying to compute this integral: $$I = \int_0^{\infty} \frac{1}{e^x + x}dx$$ I don't see any obvious ways to integrate this using real methods. So, now I'm trying to integrate with using complex analysis, I tried to transform the equation with $z = e^{-x}$ and get $$I = \int_0^1 \frac{1}{1-z\log(z)}dz$$ I see that a pole of the above is $z = \frac{1}{W(1)}$, which is the Lambert W function When I evaluate the residue, I get the value 0. $$\text{Res}_{z \rightarrow \frac{1}{W(1)}}\left(\frac{1}{1-z \log (z)},f(z)\right) = 0$$ Assistance on next steps and a solution would be appreciated.","['lambert-w', 'complex-analysis', 'improper-integrals', 'integration', 'definite-integrals']"
