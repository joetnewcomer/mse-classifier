question_id,title,body,tags
2036881,Is there a closed-form for $\frac{d^ny}{dx^n}$?,"I am dealing with this... #Question# Given $y$ is a function of $x$ , $x^n+y^n=1$ , where $n$ is a positive integer. Find $\displaystyle\frac{d^ny}{dx^n}$ in terms of $x$ , $y$ and $n$ . ###Example 1###
For example, when $n=1$ , $x+y=1$ then $\displaystyle\frac{dy}{dx}=-1$ . ###Example 2###
When $n=3$ , $x^3+y^3=1$ then $\displaystyle\frac{d^3y}{dx^3}=-\left(\frac{10x^6}{y^8}+\frac{12x^3}{y^5}+\frac{2}{y^2}\right)$ . ###Example 3### When $n=6$ , $x^6+y^6=1$ then $\displaystyle\frac{d^6y}{dx^6}=-\left(\frac{623645x^{30}}{y^{35}}+\frac{1612875x^{24}}{y^{29}}+\frac{1425875x^{18}}{y^{23}}+\frac{482625x^{12}}{y^{17}}+\frac{46100x^6}{y^{11}}+\frac{120}{y^5} \right)$ . ##Table of values## Below is a triangular array of values for the coefficient: \begin{array}{|c|c|c|c|}
\hline
n \backslash k& 1 & 2 & 3 & 4 & 5 & 6 &.\\ \hline
1 & 1\\ \hline
2 & 1 & 1&\\ \hline
3 &  10& 12&2&\\ \hline
4 & 231&378&153&6 &\\ \hline
5 & 9576&20160&12960&2400 & 24&\\ \hline
6 & 623645&1612875 & 1425875&482625&46100&120&\\ \hline.
\end{array} Denote $a_{n,k}$ (where $k$ is a positive integer $\le n$ ) as the number in the $n$ -th row and the $k$ -th column. (e.g. $a_{3,2}=12$ ) Therefore, \begin{align} \displaystyle \frac{d^ny}{dx^n}=-\sum_{k=1}^n a_{n,k}\left(\frac{x^{n^2-kn}}{y^{n^2-kn+n-1}}\right).\end{align} I found that \begin{align}\boxed{\textbf{E1:} \quad \displaystyle \sum_{k \rm \ is \ odd} a_{n,k} =\sum_{k \rm \ is\  even} a_{n,k} \  \text{for}\  n>1\ \ \ },\end{align} (i.e. $a_{n,1}+a_{n,3}+a_{n,5}+...=a_{n,2}+a_{n,4}+a_{n,6}+...$ ) and \begin{align}\boxed{\textbf{E2:}\qquad \qquad \qquad  a_{n,n}=(n-1)! \qquad \quad \ }\end{align} related to the factorial. E1 and E2 are the two equalities I have discoverd. Moreover , \begin{align}\boxed{ \  a_{n,k}\  \text{is divisible by}\ (n-1) \text{.}\qquad \text{(i.e.}\ \  (n-1)|a_{n.k}\  \text{)}\  }\end{align} Someone has mentioned the generalized binomial theorem which can reduce $\displaystyle \frac{d^ny}{dx^n}$ to > \begin{align} \displaystyle \frac{d^ny}{dx^n}=\sum_{k=1}^{\infty} \binom{1/n}{k}  \frac{(kn)!}{\left(\left(k-1\right) n\right) !} x^{(k-1)n}\end{align} by rewriting $y=\left(1-x^n\right)^{1/n}$ for $|x|<1$ . It could be the answer, but now I'm more interested in finding the closed-form for $a_{n,k}$ . Is there a closed-form for $\displaystyle\frac{d^ny}{dx^n}$ (in terms of $a_{n,k}$ )? ###OR###
Is there a closed-form for $a_{n,k}$ ?","['derivatives', 'calculus', 'closed-form', 'functions', 'implicit-differentiation']"
2036911,SIR infection induced mortality,"I am reading the book Modelling Infectious Diseases in Humans and Animals by Matt J Keeling and Pejman Rohani. In it the SIR model is given as, $\frac{dS}{dt}  =\mu-\beta{S}I-\mu S$ $\frac{dI}{dt}= \beta SI - \gamma I-\mu I$ $\frac{dR}{dt}= \gamma I-\mu R$ where the rate at which individuals in any epidemiological class suffer natural mortality is given by $\mu$ and S,I,R are respectively the proportion of susceptible, infected and recovered individuals in the population. For this model when the infection induced mortality is introduced the following change is made. (page 34) $\frac{dI}{dt}= \beta SI - (\gamma+\mu) I-\frac {\rho}{1-\rho} (\gamma+\mu)  I$  where $\rho $ is the probability that an individual in I class dying from the infection before either recovering or dying from natural causes. In this I don't understand 1) How does $\frac {\rho}{1-\rho} (\gamma+\mu)  I$ capture the infection induced mortality. 2) Also it says rather than having the per capita disease-induced mortality rate for infected individuals it is preferable to think of $\rho$, probability that an individual in I class dying from the infection before either recovering or dying from natural causes. Why is this probability easy than mortality rate?","['biology', 'ordinary-differential-equations', 'mathematical-modeling']"
2036912,"A regression model for the discretization of $U \sim unif[0,1]$.","Let $U \sim unif[0,1]$ and $U_n = \frac{\lfloor nU\rfloor}{n}$. a) Determine the distribution of the difference variable $W_n = U - U_n$. b) Using part a), evaluate the correlation coefficient $\rho(U,U_n)$. c) For $Y=U$ and $X=U_n$, obtain the unique $\alpha,\beta\in\mathbb{R}$ and error variable $Z$ such that $Y = \alpha + \beta X + Z$ with $E(Z)=0$ and $\rho(X,Z)=0$.","['uniform-distribution', 'statistics', 'probability-distributions']"
2036932,Strong operator convergence of an uncountable series,"Let $X$ be a Banach space and $\{T_{\alpha}\}_{\alpha\in I}\subset B(X)$ be a (possible uncountable) family of bounded linear operators on $X$ such that $\sum T_{\alpha}$ converges to the identity operator in the strong operator topology. This means that $\sum T_{\alpha} x=x \ \forall x\in X$. We know that for any summable series, all but countably many of the terms must be $0$. Hence for each $x\in X$, $T_{\alpha}x=0$ for all but countably many terms. Can we then conclude that all but countably many of the $T_{\alpha}$  themselves are $0$? This should happen, for example if $X$ is a separable Hilbert space. What about a separable Banach space? Is there an example of a space where this need not happen? Thanks in advance.","['functional-analysis', 'banach-spaces', 'operator-theory', 'hilbert-spaces']"
2036955,Rendezvou course for moving boats,"There is a boat travelling along a straight line on $ \mathbb{R}^2 $ with its location at time t given by $ (t \cos(\theta_a), t \sin(\theta_a) ) $. I'm in a boat that starts at $(x_b, y_b)$ and travels a fixed speed of $v$.  My location at time $t$ is given by $ (vt \cos(\theta_b) + x_b, vt \sin(\theta_b) + y_b ) $. I can pick my angle $ \theta_b$.  I wish to pick an angle so that I meet up with the other vessel. In other words given $x_b$, $y_b$, $v$ and $ \theta_a$ find $\theta_b$ such that a some time $t_r$ the following equality holds $(t_r \cos(\theta_a), t_r \sin(\theta_a)) = (t_rv \cos(\theta_b) + x_b, t_rv \sin(\theta_b) + y_b) $. Now clearly there are some combinations of speed and location where there will be no solution.  However in the cases where there is a solution how do I solve it?","['trigonometry', 'calculus']"
2036961,surface integral hard question,"Compute $\int\int_\limits{\gamma}\vec{F}\cdot\bar{n} dS$, where $\vec{F}=[x^{2}yz+xe^{z}]\bar{i}+[x^{2}+y(1-e^{z})]\bar{j}+[2+x^{3}-xyz^{2}]\bar{k}$ $\gamma =x^{2}+y^{2}=(z-1)^{2}  ,0\leq  z\leq 1$ Am not interested only in solving this particular example but in understanding how to work these kind of problems in general so that I can do the rest of problems alone. I need detailed answer , the more detailed the better . Thanks in advance.","['multivariable-calculus', 'surface-integrals']"
2036994,How do I prove that: $\sum_{i=0}^{\infty} \frac{i^2}{i!}=2e$ [duplicate],This question already has answers here : What's the value of $\sum\limits_{k=1}^{\infty}\frac{k^2}{k!}$? (8 answers) Closed 7 years ago . I've seen in Wolfram Alpha that $$\sum_{i=0}^{\infty} \frac{i^2}{i!}=2e$$ but I have no idea how to prove that. Can anyone help me? Thanks.,"['sequences-and-series', 'calculus', 'limits']"
2037052,"How many different abelian groups exist, which include subgroup $ \mathbb Z/12 \mathbb Z$","How many different abelian groups exist which include subgroup $\mathbb Z/12 \mathbb Z$ and factor by this subgroup is also isomorphic to $\mathbb Z/12 \mathbb Z$? My suggestion is that we should have group of 144 elements (because of 12 elements in subgroup and 12 classes).
So, group of $144 $ elements can be decomposed into these 4 different direct sums: $\mathbb Z/144 \mathbb Z$ $\mathbb Z_{12} \times \mathbb Z_{12}$ $\mathbb Z_{3} \times \mathbb Z_{4} \times \mathbb Z_{12}$ $\mathbb Z_{3} \times \mathbb Z_{4} \times \mathbb Z_{3} \times \mathbb Z_{4}$ which includes subgroups isomorphic to  $\mathbb Z/12 \mathbb Z$ And it means that the answer is ""4"". Am I right?","['finite-groups', 'abstract-algebra', 'abelian-groups', 'group-theory']"
2037060,"On Reshetnikov's integral $\int_0^1\frac{dx}{\sqrt[3]x\ \sqrt[6]{1-x}\ \sqrt{1-x\,\alpha^2}}=\frac{1}{N}\,\frac{2\pi}{\sqrt{3}\,|\alpha|}$","V. Reshetnikov gave the remarkable integral ,
$$\int_0^1\frac{dx}{\sqrt[3]x\,\sqrt[6]{1-x}\,\sqrt{1-x\left(\sqrt{6}\sqrt{12+7\sqrt3}-3\sqrt3-6\right)^2}}=\frac\pi9(3+\sqrt2\sqrt[4]{27})\tag1$$
More generally, given some integer/rational $N$, we are to find an algebraic number $\alpha$ that solves, $$\int_0^1\frac{dx}{\sqrt[3]x\ \sqrt[6]{1-x}\ \sqrt{1-x\,\alpha^2}}=\frac{1}{N}\,\frac{2\pi}{\sqrt{3}\,|\alpha|}\tag2$$ and absolute value $|\alpha|$. ( Compare to the similar integral in this post .) Equivalently, to find $\alpha$ such that, $$\begin{aligned}
\frac{1}{N}
&=I\left(\alpha^2;\ \tfrac12,\tfrac13\right)\\[1.8mm]
&= \frac{B\left(\alpha^2;\ \tfrac12,\tfrac13\right)}{B\left(\tfrac12,\tfrac13\right)}\\
&=B\left(\alpha^2;\ \tfrac12,\tfrac13\right)\frac{\Gamma\left(\frac56\right)}{\sqrt{\pi}\,\Gamma\left(\frac13\right)}\end{aligned} \tag3$$ with beta function $\beta(a,b)$, incomplete beta $\beta(z;a,b)$ and regularized beta $I(z;a,b)$. 
Solutions $\alpha$ for $N=2,3,4,5,7$ are known. Let,
$$\alpha=\frac{-3^{1/2}+v^{1/2}}{3^{-1/2}+v^{1/2}}\tag4$$
Then,
$$ - 3 + 6 v + v^2 = 0, \quad N = 2\\ 
- 3 + 27 v - 33v^2 + v^3 = 0, \quad N = 3\\
3^2 - 150 v^2 + 120 v^3 + 5 v^4 = 0, \quad N = 5\\ 
- 3^3 - 54 v + 1719 v^2 - 3492v^3 - 957 v^4 + 186 v^5 + v^6 = 0, \quad N = 7$$ and ( added later ), $$3^4 - 648 v + 1836 v^2 + 1512 v^3 - 13770 v^4 + 12168 v^5 - 7476 v^6 + 408 v^7 + v^8 = 0,\quad N=4$$ using the largest positive root, respectively. The example was just $N=2$, while $N=4$ leads to, $$I\left(\tfrac{1-\alpha}{2};\tfrac{1}{3},\tfrac{1}{3}\right)=\tfrac{3}{8},\quad\quad I\left(\tfrac{1+\alpha}{2};\tfrac{1}{3},\tfrac{1}{3}\right)=\tfrac{5}{8}$$ I found these using Mathematica 's FindRoot command, and some hints from Reshetnikov's and other's works, but as much as I tried, I couldn't find prime $N=11$. Q: Is it true one can find algebraic number $\alpha$ for all $N$? What is it for $N=11$?","['calculus', 'closed-form', 'integration', 'definite-integrals', 'experimental-mathematics']"
2037077,"Find a conformal mapping from $U=\{z:|z|<1\}\smallsetminus[1/2,1)$ to upper half plane.","I need to find a conformal mapping from $U=\{z:|z|<1\}\smallsetminus[1/2,1)$ to upper half plane. My first thought is to send $z\mapsto \sqrt{z}$, which gives the upper half disk plus the real interval $[0,1/\sqrt2]$. Then taking $z\mapsto\frac{1+z}{1-z}$ gives us the first quadrant plus the real interval $[1,1/(1-1/\sqrt2)]$. Now squaring gives the upper half plane, plus the interval $[1,1/(1-1/\sqrt2)^2]$. This real interval is what's giving me trouble; I don't know how to get rid of it. Any suggestions?",['complex-analysis']
2037080,The average rate of job submissions in a computer center is 2 per minute.,"Full Question: The average rate of job submissions in a computer
  center is 2 per minute. If it can be assumed that the number of
  submissions per minute has a Poisson distribution, calculate the
  probability that (a) more than two jobs will arrive in a minute $P(X>2) = 1- P(X≤ 2) = 1- [P(X=0)+P(X=1)+P(X=2)]$ and I got 0.3233235 as my answer. Is that correct? Example: For $P(X=1),$ I did $((2^1)(e^-2))/1!$ (b) at least 30 sec will elapse between any two jobs This is where I'm confused. A similar problem was answered using Exponential distribution in the book even though the question said this was a Poisson distribution problem. I tried with Exponential and this is what I got: Since 30 seconds is $1/2$ of 1 minute, $(1/2) * λ$ or $(1/2)*2 = 1$ $P(X ≥ 1/2) = 1 - P(x < 1/2) = 1 - (1- e^-1) = 0.3678794$ Please tell me if I used the correct distribution functions for both a and b, and if I answered them correctly. Thank you.","['exponential-function', 'probability-distributions', 'statistics', 'exponential-distribution', 'poisson-distribution']"
2037109,Linear Transformation from Infinite dimensional to Finite dimensional Space,"Let $T:V\to V$ be a linear transformation, where $V$ is an infinite -dimensional vector space over a field $F$. Assume that $T(V)=\{T(v):v\in V\}$ is finite -dimensional. Show that $T$ satisfies a nonzero polynomial over $F$, that is, there exists $a_0,\dots, a_n\in F$, with $a_n\neq 0_F$ such that $$a_0v+a_1T(v)+\dots+a_nT^n(v)=0_V$$
for all $v\in V$. I am not very sure how to approach this question. Suppose the dimension of $T(V)$ is $n$.
I tried considering the set $\{T(v),T^2(v),\dots,T^{n+1}(v)\}$ which has to be linearly dependent thus there exists $a_i$ such that  $a_1T(v)+\dots+a_{n+1}T^{n+1}(v)=0$. This seems to be similar to what the question whats, except that the polynomial is dependent on $v$, while the question wants a polynomial that works for all $v\in V$. Thanks for any help.","['abstract-algebra', 'linear-algebra', 'linear-transformations']"
2037127,"For a compact set $K\subset \mathbb M_n(\mathbb R)$, the eigenvalues of matrices in $K$ form a bounded set","Let $K\subset \mathbb M_n(\mathbb R)$ be a compact subset. Then I have to show that : All the eigen values of the elements of $K$ form a bounded set. My work: consider the map $K \to det K$ which is continuous. Image set is compact in $\mathbb C$, hence closed bounded. If $\lambda_i (i=1\ldots,n)$ are the eigenvalues, then $\det K=\prod \lambda_i$ is bounded which in turn  gives $\lambda_i$ bounded. Is my approach correct? Is there any better way to do it?","['eigenvalues-eigenvectors', 'general-topology', 'linear-algebra']"
2037186,"$95$% confidence interval for $\theta_2-\theta_1$ from $\text{uniform}\left(\theta_1,\theta_2\right)$","I want to find a $95$ % confidence interval for $\theta_2-\theta_1$ where $X_1,...,X_n$ are the random sample from $U(\theta_1,\theta_2)$ , the uniform distribution with two parameters $\theta_1, \theta_2$ . I have the maximum likelihood estimators $\hat\theta_1=X_{(1)}$ , the minimum order statistic, and $\hat\theta_2=X_{(n)}$ , the maximum order statistic. But I don't know how to compute the confidence interval of $\theta_2-\theta_1$ . Is there anyone can help me? Thanks for your help","['statistical-inference', 'statistics', 'confidence-interval', 'uniform-distribution', 'parameter-estimation']"
2037265,The ring of integers of a number field is contained in any valuation ring.,"Let $K$ be a number field and let $v$ be a surjective discrete valuation on $K$.  Let $\mathcal O_v$ the valuation ring of $v$, namely 
$$\mathcal O_v=\{x\in K\,: v(x)\ge 0\}$$
Let's denote with $O_K$ the ring of integers of $K$. Why do we have the inclusion $O_K\subset\mathcal O_v$. I other word, why for any $y\in O_K$, then $v(y)\ge 0$? Many thanks in advance","['valuation-theory', 'abstract-algebra', 'algebraic-number-theory']"
2037277,"Ideal of $\mathbb{Z}[\sqrt{10}]$: principal, prime or maximal?","my task is following Let $I=\{a+b\sqrt{10}:13\mid2a-b\}$ be a subset of the ring $\mathbb{Z}[\sqrt{10}]$. Decide if $I$ is an ideal of $\mathbb{Z}[\sqrt{10}]$ and if so, decide if it is principal, prime or maximal. I've proved that $I$ is ideal indeed (addition is trivial and it is closed under multiplication because $13\mid2a-b\ \Leftrightarrow\ 13\mid a-20b$). But I have problem with the properties of $I$. I know that $I$ is prime/maximal iff $\mathbb{Z}[\sqrt{10}]/I$ is an integral domain / a field. I don't know, how to continue -- $I$ is kernel of some ring homomorphism, but I have no idea how to use it. I suppose we would like to use something like $f:\mathbb{Z}[\sqrt{10}]\to\mathbb{Z}_{13},a+b\sqrt{10}\mapsto[2a-b]_{13}$ but is this even a map?... Edit: oh, of course $f$ is a map, since the element $a+b\sqrt{10}$ represents only itself...","['abstract-algebra', 'ring-theory', 'maximal-and-prime-ideals', 'ideals']"
2037311,"Finding the roots, with multiplicity, of a polynomial with compound angle formula","My question is to find the roots, counted with multiplicity, of the polynomial equation $16x^5-20x^3+5x-1=0$ using the compound angle formula $\sin\left(5\theta\right)=16\sin^5\theta-20\sin^3\theta+5\sin\theta$ So after substituting $x=\sin\theta$, I get to the equation $\sin(5\theta)=1$ Then I get an infinitude of $\theta$ values, which when I find the sines of these values, all correspond to the distinct solutions $x=1,\sin\left(\frac{\pi}{10}\right),\sin\left(-\frac{3\pi}{10}\right)$. What I don't understand is how I can then find which roots are repeating, since the degree of the polynomial is 5 hence there must be 5 roots when counted with multiplicity. Also if possible, is there a way to solve this polynomial using the given compound angle formula without the need to find the distinct roots and then determine the ones which repeat? This is because the solutions to this question say $x=1,\sin\left(\frac{\pi}{10}\right),\sin\left(\frac{9\pi}{10}\right),\sin\left(\frac{13\pi}{10}\right),\sin\left(\frac{17\pi}{10}\right)$ without any reasoning, which makes me suspect I am unaware of some related theorem.","['polynomials', 'roots', 'trigonometry']"
2037314,"Find the number of triples $(x,y,z)$ of real numbers satisfying the equation $x^4+y^4+z^4+1=4xyz$","QUESTION Find the number of triples $$(x,y,z)$$ of real numbers satisfying the equation $$x^4+y^4+z^4+1=4xyz$$ I have tried to solve this problem but can't quite understand how to manipulate the given data to reach a clear result. Could someone please explain how I should approach and solve this problem.Thanks :)","['algebra-precalculus', 'inequality']"
2037328,relationship between topology and convergences,"In my textbook, the topology of $C_c^\infty$ and $C^\infty$ are defined by: how $f_n\to f$ in these topologies. Similarly, the weak-* topology are also defined by the same manner. I wonder why we can determine a topology only by convergent sequence since topology ought to be defined by open sets?","['functional-analysis', 'general-topology']"
2037334,Big Unions/Intersections,"I have a quick and simple question in regards to what a ""Big Union/Intersection"" even is. I saw this term in my text  book, but when I did some research there is nothing on it. Hopefully this article will serve as a resource for future researchers. Here is a screenshot of the questions http://prntscr.com/ddmvyd I am not looking for answers, I am looking at for the direction I need to research and learn this stuff so That I can solve those problems myself. These are the problems which yielded no results when I tried to research them, ordered by my lack of understanding. 1a,3,2","['notation', 'elementary-set-theory']"
2037412,"On the integral $\int_0^1\frac{dx}{\sqrt[4]x\ \sqrt{1-x}\ \sqrt[4]{1-x\,\gamma^2}}=\frac{1}{N}\,\frac{2\pi}{\sqrt{2\gamma}}$","V. Reshetnikov gave the interesting integral , $$\int_0^1\frac{\mathrm dx}{\sqrt[4]x\ \sqrt{1-x}\ \sqrt[4]{2-x\,\sqrt3}}=\frac{2\,\sqrt2}{3\,\sqrt[8]3}\pi\tag1$$ After some experimentation, it turns out that more generally, given some integer/rational $N$ , we are to find an algebraic number $\gamma$ that solves, $$\int_0^1\frac{dx}{\sqrt[4]x\ \sqrt{1-x}\ \sqrt[4]{1-x\,\gamma^2}}=\frac{1}{N}\,\frac{2\pi}{\sqrt{2\gamma}}\tag2$$ ( Compare to the similar integral in this post . ) Equivalently, to find $\gamma$ such that, $$\begin{aligned}
\frac{1}{N}
&=I\left(\gamma^2;\ \tfrac14,\tfrac14\right)\\[1.8mm]
&= \frac{B\left(\gamma^2;\ \tfrac14,\tfrac14\right)}{B\left(\tfrac14,\tfrac14\right)}
\end{aligned} \tag3$$ with beta function $B(a,b)$ , incomplete beta $B(z;a,b)$ and regularized beta $I(z;a,b)$ , and $B\left(\tfrac14,\tfrac14\right)=\frac{\sqrt\pi}{\Gamma^2\left(\frac14\right)}$ . Reshetnikov's example, after tweaking, was just the case $N=\frac{3}{2}$ and $\gamma=\frac{3^{1/4}}{\sqrt{2}}$ . Solutions for prime $N=2,3,5,7$ are known. Let $v=\gamma$ , then, $$-1 + 2 v^2 = 0\quad\quad  N=2\\ - 1 + 2 v + 2 v^2 = 0\quad\quad  N=3\\ - 1 + 8 v - 4 v^2 - 8 v^3 + 4 v^4 = 0\quad\quad  N=5$$ etc, with $N=7$ using a $12$ -deg equation. I found these using Mathematica 's FindRoot command but, unlike the other post , I couldn't find a nice common form for $\gamma$ . (The pattern of this family is also different. I had expected $N=7$ to also involve a sextic only.) Q: Is it true one can find algebraic number $\gamma$ for all prime $N$ ? What is it for $N=11$ ? Update, Aug 16, 2019 In this comment , Reshetnikov gave the explicit solution to, $$I\left(\gamma^2;\ \tfrac14,\tfrac14\right) = \tfrac17$$ as, $$\small\gamma = \frac16\left(5\cos x-\sqrt3\sin x-1-\sqrt3\sqrt{7+4\sqrt7-(11+2\sqrt7)\cos x+\sqrt3(5+2\sqrt7)\sin x}\right)$$ where $x = \tfrac13\arccos\big(\tfrac{13}{14}\big)$ . P.S. I forgot I also found $\gamma$ in this 2016 post as, $$\gamma = \tfrac12\left(2\cos\tfrac{2\pi}7-\sqrt{2\cos\tfrac{4\pi}7+\sqrt2\csc\tfrac{9\pi}{28}}\right)$$","['closed-form', 'integration', 'elliptic-curves', 'definite-integrals', 'experimental-mathematics']"
2037427,"Finding the minimum of $f(x,y)$ by simple inspection","Let $$f(x,y) = \sqrt{x^2 + y^2}+ \sqrt{x^2 + y^2-2x+1}+\sqrt{x^2 + y^2-2y+1}+\sqrt{x^2 + y^2-6x-8y +25}$$ Find minimum value of $f$. Now, We see that we can complete the squares, but how do we proceed after that? Is there any general technique for such problems?","['algebra-precalculus', 'calculus', 'functions']"
2037470,"Showing that $f_n:[0,1]\to\mathbb{R}$ when $f_n=nx(1-x)^n$ converges pointwise but not uniformly","Suppose $f_n:[0,1]\to\mathbb{R}$ such that $f_n=nx(1-x)^n$, then I need to show that $f_n$ converges pointwise but not uniformly. I can see that since $f$ is defined on $[0,1]$, the part $(1-x)^n$ is smaller than $1$ and therefore will converge, but what about $nx$? This will go to infinity... Therefore, I can't even see that it converges pointwise, but let's suppose it converges to some function, how do I show that it doesn't converge uniformly? I should find an $\epsilon$ such that it's not valid for all $n>n_0(\epsilon)$ that $|f_n-something|<\epsilon$, right?","['real-analysis', 'sequences-and-series', 'calculus', 'limits']"
2037558,Combinatorial puzzle reminiscent of knapsack problem. Is this classic?,"I have $n$ red integers $a_1,\ldots,a_n$ (not necessarily distinct), all with $1\leq a_i\leq n$. I also have $n$ blue integers $b_1,\ldots,b_n$ with same constraints. I want to show that there is a (red) subset of the $a_i$'s and a blue subset of the $b_j$'s that add up to the same value. I.e. that there exist two non-empty subsets $I,J\subseteq\{1,\ldots,n\}$ such that $\sum_{i\in I}a_i = \sum_{j\in J}b_j$. This is obvious if $a_1=a_2=\cdots=a_n$ and $b_1=b_2=\cdots=b_n$ and it seems that allowing the $a_i$'s and the $b_j$'s to be distinct only give me more opportunities for the existence of matching subsets but I did not manage to find a proof (or a counter-example?). At the moment the best I can prove is: if the $a_i$'s and $b_j$'s are all $\leq \frac{n}{2}$ then a solution exists and one can even find a solution where $I,J$ are convex subsets of $\{1,\ldots,n\}$ (i.e., they are subintervals). The solution is found by a greedy algorithm that runs in linear-time. This looks like a classic problem but I am outside my field ...","['additive-combinatorics', 'combinatorics', 'integer-partitions']"
2037570,Chance of flipping 50 heads over a span of 100 flips given more than 100 flips,"So I found that the chance of flipping 50 heads out of a string of 100 flips is $$0.5^{50} (1-0.5)^{50} \binom{100}{50},$$ My question is, how do the chances of having at least 1 string of 100 flips, with heads resulting 50 times, change if I am allowed to flip the coin 101 times?  In other words, I could get 50 out of 100 in flips 1-100 OR 50 out of 100 in flips 2-101 or both. What about if I were allowed to flip the coin 200 times, but needed to get at least one string of 100 flips resulting in 50 heads? My thinking is that there are 101 different 100 flip sequences in a 200 flip sequence, and each of those 101 sequences should have $$0.5^{50} (1-0.5)^{50} \binom{100}{50},$$ probability of yielding heads exactly 50 times, which would multiply the probability by 101 times, but since the 101 different 100 flip sequences are overlapping, rather than being independent of each other, does it change the odds?","['probability', 'sequences-and-series']"
2037602,What is range of a matrix?,"I am having some tough time understanding the basic concepts, like range of a matrix A. From what I basically understand, if a set columns in a matrix are linearly independent, i.e. one column in that set can not be derived from linear combination of others, than we can get a bunch of set of vectors by linear combination of the columns of matrix A. That set is called column space of the matrix A or its range. And those linear independent columns of matrix form basis for this range, or are called to ""span the column space"" of matrix A. Did I understand it correctly? In simplest terms can anyone explain it? Also what is Null space, rank and how they are related to a matrix?","['matrices', 'change-of-basis', 'linear-algebra']"
2037654,Understanding of Equality of Functions,"Let $f$ and $g$ be two functions such that $f:\mathbb{R} \rightarrow \mathbb{R}$, $g:\mathbb{R} \rightarrow \mathbb{R} ^{\geq 0}$ and $f\left( x\right) =g\left( x\right) =x^{2}$. My question is $f=g$ ? I think, yes. Recall that Let $f,g$ be two functions such that $f:X \rightarrow Y$ and $g:X' \rightarrow Y'$. If $f=g$ then $X=X'$ and $Y=Y'$ finally for all $x\in X$, $f\left( x\right) =g\left( x\right)$.",['elementary-set-theory']
2037701,Relationship Between the Albanese Variety and $\rm{Pic}^{0}(X)$,"I've been learning about the Albanese variety $\rm{Alb}(X)$ of a projective variety $X$.  As discussed very well in John Baez' blog ( https://golem.ph.utexas.edu/category/2016/08/the_magic_of_algebraic_geometr.html ) there is apparently an analogy between $\rm{Alb}(X)$ and the free abelian group on a pointed set.  To my knowledge the correspondence goes something like this: Let $\rm{Set}_{*}$ be the category of pointed sets.  One can define a functor from $\rm{Set}_{*}$ into the category of abelian groups $\rm{AbGrp}$, defined by taking a pointed set to the free abelian group it generates.  This construction satisfies the universal property you would expect. In the world of varieties, the Albanese map is given as a functor $$\rm{Var}_{*} \to \rm{AbVar}$$ where you send a pointed variety $X$ to its Albanese variety $\rm{Alb}(X)$.  The Albanese is classified by a universal property, so I guess this is what motivates people to think of the Albanese as the ""free abelian variety of a projective variety.""  This makes sense to me, as it seems like most of the literature defines the Albanese by this universal property, and its more concrete geometrical properties are proved as theorems.  So my questions are about its geometrical properties: I've heard the Albanese variety is dual to the degree-zero Picard group: $$\rm{Alb}(X) \simeq (\rm{Pic}^{0}(X))^{*}$$ I know that $\rm{Pic}^{0}(X)$ classifies degree-zero line bundles on $X$, but I was hoping someone could lay out the correspondence between its dual and the Albanese.  Perhaps my confusion is arising from that dual being the dual in the category of abelian varieties.  Secondly, what, if anything, does the analogy with the free abelian group say about this concrete realization of the Albanese as the dual of a variety which classifies line bundles?",['algebraic-geometry']
2037705,Evaluate the integral $ \int_{0}^{\infty}\frac{\sin(x^{2})x^{2}\ln(x)}{e^{x^2}-1}dx $,"Does the Following integral  admit a closed form? $$ \int_{0}^{\infty}\dfrac{\sin(x^{2})x^{2}\ln(x)}{e^{x^2}-1}dx $$ What I tried was: Define another integral $ I(a) $ as: $$ I(a)= \int_{0}^{\infty}\dfrac{\sin(x^{2})x^{a}}{e^{x^{2}}-1}dx $$ Write it as: $$ I(a) = \text{Im} \left[ \sum_{r=1}^{\infty} \int_{0}^{\infty} x^{a}e^{-x^{2}(r-\iota)}dx \right] $$ Clearly the required integral is $ I'(2) $. The above simplifies to: $$ \text{Im}\left[\frac{\Gamma(\frac{a+1}{2})}{2}\sum_{r=1}^{\infty}\frac{1}{(r-\iota)^{\frac{a+1}{2}}} \right] $$ which further simplifies to : $$ I(a) = \frac{\Gamma(\frac{a+1}{2})}{2}\sum_{r=1}^{\infty} \frac{\sin(\frac{a+1}{2}\tan^{-1}(\frac{1}{r}))}{(r^{2}+1)^{\frac{a+1}{4}}} $$ Let alone $I'(a) $ I could not evaluate even $I(a)$ in general form 
The only one which i could solve was $ a=1 $ SO that $$ I(1) = \int_{0}^{\infty}\dfrac{\sin(x^{2})x}{e^{x^{2}}-1}dx = \frac{1}{2}\left[\frac{e^{2\pi}(\pi -1)+(\pi +1)}{e^{2\pi}-1}\right]  $$ Any other approach or hints/suggestions are more than welcome!","['zeta-functions', 'improper-integrals', 'integration', 'definite-integrals', 'power-series']"
2037707,Deriving Yang-Mills equations,"On the same spirit of this unanswered question I am proposing this question which I have been trying for some time now. Here I'm working with dimension $n = 4$ (identifying $\mathbb H = \mathbb R^4$) considering the principal $SU(2)$-bundle with $\rho : SU(2) \to GL(2, \mathbb C)$ being the adjoint representation $ad_g (A) = g^{-1} A g$, for all $g \in SU(2)$ and $A \in \mathfrak {su}(2)$.  Moreover, the gauge potential is written (in local coordinates) as $\mathcal A = \mathcal A_{\alpha} dx^\alpha$, $\alpha = 1,2,3,4$ and its gauge field strength (curvature) is given by $$\mathcal F = d\mathcal A + \frac{1}{2}[\mathcal A, \mathcal A] = \frac{1}{2} \mathcal F_{\alpha\beta} dx^\alpha \wedge dx^\beta$$ where (after some lengthy calculation) $$\mathcal F_{\alpha\beta} = \partial_\alpha \mathcal A_\beta - \partial_\beta\mathcal A_\alpha + [\mathcal A_\alpha, \mathcal A_\beta]\ \  , \ \ \beta= 1,2,3,4$$
where $\partial_\alpha = \frac{\partial}{\partial x^\alpha}$. Question: Derive the Euler-Lagrange equations of the Yang-Mills functional given by $$\mathcal {YM} (\mathcal A) = \frac{1}{4}\int_{\mathbb R^4} \|\mathcal F\|^2 d(\bf vol_{\mathbb R^4})$$
  such equations are called in the physics literature Yang-Mills equations $$\ast d^{\mathcal A}(\ast \mathcal F) = \sum_{\alpha = 1}^4 (\partial_\alpha \mathcal F_{\alpha\beta} + [\mathcal A_\alpha, \mathcal F_{\alpha\beta}]) = 0$$ where $\ast$ is the Hodge dual operator, $d^\mathcal A$ is the covariant exterior derivative. Attempt: Since the space of gauge potentials is an affine space we may consider the variations $\mathcal A + t\mathcal B$ (family of 1-parameter of gauge potentials) thus $$\begin{aligned}\frac{d}{dt} \Big(\mathcal {YM}(\mathcal A + t \mathcal B)\Big)\Big|_{t= 0} &= \frac{1}{4}\int_{\mathbb R^4} \frac{d}{dt} \Big(\|\mathcal F\|^2\Big)\Big|_{t = 0} d (\bf vol_{\mathbb R^4})\\ &=\frac{1}{2} \int_{\mathbb R^4} \|\mathcal F\| \frac{\left\langle \mathcal F, \frac{d}{dt} (\mathcal F)\Big|_{t =0}\right\rangle}{\|\mathcal F\|} d(\bf vol_{\mathbb R^4})\\&=\frac{1}{2} \int_{\mathbb R^4} \left\langle \mathcal F, \frac{d}{dt} (\mathcal F)\Big|_{t=0}\right\rangle d(\bf vol_{\mathbb R^4})\end{aligned}$$ where $$\begin{aligned} \mathcal F &= \frac{1}{2} \mathcal F_{\alpha \beta} dx^\alpha \wedge dx^\beta \\&= \frac{1}{2}\Bigg( \partial_\alpha (\mathcal A_\beta + t \mathcal B_\beta) - \partial_\beta (\mathcal A_\alpha + t \mathcal B_\alpha) + [\mathcal A_\alpha, \mathcal A_\beta] \\&+ t [\mathcal A_\alpha, \mathcal B_\beta] + t [\mathcal B_\alpha, \mathcal A_\beta] + t^2 [\mathcal B_\alpha, \mathcal B_\beta]\Bigg) dx^\alpha \wedge dx^\beta\end{aligned}$$ by linearity we then obtain $$\frac{d}{dt} (\mathcal F)\Big|_{t = 0} = \Bigg(\partial_\alpha \mathcal B_\beta - \partial_\beta \mathcal B_\alpha + [\mathcal A _\alpha, \mathcal B_\beta] + [\mathcal B_\alpha, \mathcal A_\beta]\Bigg) dx^\alpha \wedge dx^\beta$$
Now $$\begin{aligned}\frac{d}{dt}\Big(\mathcal {YM}(\mathcal A + t \mathcal B)\Big)\Big|_{t = 0} &=\frac{1}{2} \int_{\mathbb R^4} \left\langle \mathcal F, \frac{d}{dt} (\mathcal F)\Big|_{t=0}\right\rangle d(\bf vol_{\mathbb R^4}) \\&= \frac{1}{2}\int_{\mathbb R^4} \mathcal F_{\alpha\beta} (\partial_\alpha \mathcal B_\beta - \partial_\beta \mathcal B_\alpha + [\mathcal A _\alpha, \mathcal B_\beta] + [\mathcal B_\alpha, \mathcal A_\beta])d(\bf vol_{\mathbb R^4}) \end{aligned}$$ 
Next step should be integration by parts. Then using the fact (Divergence Theorem) $$\int_{\mathbb R^4} \partial_i (f) g dV_g = - \int_{\mathbb R^4} f \partial_i (g) dV_g $$ and that $\mathcal F_{\beta\alpha} = - \mathcal F_{\alpha\beta}$ we have $$\begin{aligned}\delta\mathcal {YM}(A)&=\frac{1}{2} \int_{\mathbb R^4} \left\langle \mathcal F, \frac{d}{dt} (\mathcal F)\Big|_{t=0}\right\rangle d(\bf {vol}_{\mathbb R^4}) \\&= \frac{1}{2}\int_{\mathbb R^4} \mathcal F_{\alpha\beta} (\partial_\alpha \mathcal B_\beta - \partial_\beta \mathcal B_\alpha + [\mathcal A _\alpha, \mathcal B_\beta] + [\mathcal B_\alpha, \mathcal A_\beta])d(\bf {vol}_{\mathbb R^4})\\&= \frac{1}{2}\left(\int_{\mathbb R^4} \mathcal F_{\alpha\beta} (\partial_\alpha \mathcal B_\beta) d(\bf {vol}_{\mathbb R^4}) - \int_{\mathbb R^4}\mathcal F_{\alpha\beta}(\partial_\beta \mathcal B_\alpha) d(\bf {vol}_{\mathbb R^4})\right) \\&+ \frac{1}{2}\left(\int_{\mathbb R^4} \mathcal F_{\alpha\beta}[\mathcal A_\alpha, \mathcal B_\beta] + \mathcal F_{\alpha\beta}[\mathcal B_\alpha, \mathcal A_\beta])d(\bf {vol}_{\mathbb R^4}) \right)\\&= -\frac{1}{2}\left(\int_{\mathbb R^4} 2\mathcal B_\beta (\partial_\alpha \mathcal F_{\alpha\beta} d(\bf {vol}_{\mathbb R^4})\right) + \frac{1}{2}\left(\int_{\mathbb R^4} \mathcal F_{\alpha\beta}[\mathcal A_\alpha, \mathcal B_\beta] + \mathcal F_{\alpha\beta}[\mathcal B_\alpha, \mathcal A_\beta]d(\bf {vol}_{\mathbb R^4}) \right)\\&=-\int_{\mathbb R^4} \mathcal B_\beta (\partial_\alpha \mathcal F_{\alpha\beta} d(\bf {vol}_{\mathbb R^4}) \\&+ \frac{1}{2}\left(\int_{\mathbb R^4} \mathcal F_{\alpha\beta}[\mathcal A_\alpha, \mathcal B_\beta] + \mathcal F_{\alpha\beta}[\mathcal B_\alpha, \mathcal A_\beta])d(\bf {vol}_{\mathbb R^4}) \right)\\&=-\int_{\mathbb R^4} \mathcal B_\beta (\partial_\alpha \mathcal F_{\alpha\beta} d(\bf {vol}_{\mathbb R^4}) + \int_{\mathbb R^4} \mathcal F_{\alpha\beta}[\mathcal A_\alpha, \mathcal B_\beta] d(\bf {vol}_{\mathbb R^4})\end{aligned}$$ I need only show that $$\int_{\mathbb R^4} \mathcal F_{\alpha\beta}[\mathcal A_\alpha, \mathcal B_\beta] d(\bf {vol}_{\mathbb R^4}) = - \int_{\mathbb R^4} \mathcal B_\beta [\mathcal A_\alpha,\mathcal F_{\alpha\beta}]d (\bf {vol}_{\mathbb R^4}) $$ I could only think of using Jacobi's identity here to yield the change of $\mathcal F_{\alpha \beta}$ and $\mathcal B_\beta$ $$\begin{aligned} 0 &= [\mathcal F_{\alpha\beta}, [\mathcal A_\alpha, \mathcal B_\beta]] + [\mathcal A_\alpha, [\mathcal B_\beta, \mathcal F_{\alpha\beta}]] + [\mathcal B_\beta , [\mathcal F_{\alpha\beta}, \mathcal A_\alpha]] \\&= [\mathcal F_{\alpha\beta}, [\mathcal A_\alpha, \mathcal B_\beta]] + [\mathcal A_\alpha, [\mathcal B_\beta, \mathcal F_{\alpha\beta}]] - [\mathcal B_\beta , [\mathcal A_\alpha, \mathcal F_{\alpha\beta}]]\end{aligned}$$ But couldn't reach any further. Where do I want to get? $$\begin{aligned}\delta (\mathcal {YM}) (\mathcal A)  = - \int_{\mathbb R^4} \mathcal B_\alpha (\partial_\alpha \mathcal F_{\alpha\beta} + [\mathcal A_\alpha, \mathcal F_{\alpha \beta} ]) d(\bf vol_{\mathbb R^4})\end{aligned}$$ then the stationary points satisfy $$\sum_{\alpha = 1}^4 (\partial_\alpha \mathcal F_{\alpha\beta} + [\mathcal A_\alpha, \mathcal F_{\alpha\beta}]) = 0$$","['calculus-of-variations', 'integration-by-parts', 'differential-geometry']"
2037719,"Integral calculus sine functions: $\frac{1}{2\pi }\int_{-\pi }^{\pi }\frac{\sin\left((n+1/2)\,x\right)}{\sin\left(x/2\right)}\,dx = 1$","For an integer, $n$, how do I show the following? $$
\frac{1}{2\pi }\int_{-\pi }^{\pi }\frac{\sin\left((n+1/2)\,x\right)}{\sin\left(x/2\right)}\,dx = 1.
$$ Can I use induction?","['real-analysis', 'calculus', 'integration', 'definite-integrals', 'analysis']"
2037725,Can weak convergence be checked on an orthonormal basis?,"Let $H$ be a Hilbert space with orthnormal basis $(e_i)_{i\in I}$.
Let $(x_n)_{n\in\mathbb N}$ be a sequence in $H$ with $\langle x_n, e_i\rangle \to \langle x, e_i\rangle$ for some $x\in H$ and all $i\in I$. Can we conclude from this that that $(x_n)_{n}$ converges weakly? My intuition says no but I didn't find a counterexample.","['functional-analysis', 'weak-convergence', 'hilbert-spaces']"
2037764,Maximizing the line integral over a line segment,"Let $f(x,y) = 8x + 6y$. Take $C$ to be the line segment with length $10$ starting at the point $(2,2)$. At what point should the line segment end for $$\int_C \nabla f\cdot d{\bf r}$$ to be maximized? I think I have an idea of how to approach the problem, but I wanted to know if there are any special properties that are immediate from the given problem that make this problem ""nice"". Here's my attempt : We want ${\bf r}(t) = \langle 2, 2\rangle + t\langle a-2, b-2\rangle$ such that $\|{\bf r}\| = 10$, for some point $(a,b)$ and $t\in[0,1]$. We know that $${\bf r}(1) = (a,b),$$ so we have that $a^2 + b^2 = 100$ implies that $b = \pm\sqrt{100 - a^2}$. Well, using the fundamental theorem of line integrals, we have that $$\begin{align}\int_C\nabla f\cdot{\bf r} & = f(a,b) - f(2,2)\\& = 8a + 6b - 28 \\ &= 8a + 6\pm\sqrt{100-a^2} - 28\end{align},$$ which considering the positive root, we have a maximum of $10\sqrt{65} - 22 \approx 58.6$ at $a = 16\sqrt{5\over13}$, and the negative root has a maximum of $58$ at $a = 10$. Obviously $58.6 > 58$, so we only worry about the positive root. Therefore, when $a = 16\sqrt{5\over13}$, $b = 2\sqrt{5\over13}$. This seems to be correct (I haven't checked the answer and can't unfortunately), but I don't know if this is even the way you're expected to do this problem. Would anyone happen to know of a better way?","['multivariable-calculus', 'vectors', 'integration', 'proof-verification']"
2037818,Dimension of space of linear maps between vector spaces,"Let $F$ be a field, $V$ and $W$ are vector spaces over the field $F$. The dimension of $V$ is $n$ and the dimension of $W$ is $m$, where $m, n$ are natural numbers. Let $\mathcal{L}$ be a vector space of all linear maps from V to W. Determine the dimension of $\mathcal{L}$ depending on values $m,n$. I know there should be a solution using isomorphism between $\mathcal{L}$ an a vector space of $m \times n$ matrices, but I can't prove it.","['matrices', 'linear-algebra', 'vector-spaces']"
2037830,Determining the sections of a sheaf on any open via a sheaf on basic open sets,"This question is largely a follow on from my earlier question here . I'm not sure my understanding of the situation was good enough at that point to really frame the question properly to get the information I needed. Suppose we are working with an affine scheme, $X = \text{spec}A$. I am completely comfortable with the sections of the structure sheaf $\mathcal{O}_{X}(D(f))$ over the distinguished basic open sets (they are just elements of the localization $A_{f}$). What I am confused about is using that property to induce the sheaf structure on $\textit{all}$ the open sets. I think the best way is to break my question into a few parts. 1) I want to use the gluing property for sheaves. To do that, I need a sheaf on each of the distinguished open sets. Say we have a distinguished open set $D(f)$. This has a cover by the intersections of $D(f)$ with all the other $D(g_{i})$ (and by quasicompactness this can be a finite cover, say with $p$ elements). Then,
$$ D(f) = D(fg_{1}) \cup D(fg_{2}) \cup \cdots \cup D(fg_{p}).  $$ 
Is it true that the localizations are given by,
$$A_{fg_{i}} = \left\lbrace \frac{a}{f^{m}g_{i}^{n}}: a \in A \, \, n, m \in \mathbb{N} \right\rbrace ?$$
Surely if that is true, then the cocycle condition for gluing sheaves is satisfied in this case? 2) Say $U \subset X$ is an open set. By quasicompactness, we again have a finite cover by dinstinguished open sets
$$ U = D(f_{1}) \cup D(f_{2}) \cdots \cup D(f_{m}). $$
Is there some $\textit{explicit}$ description of the ring $\mathcal{O}_{X}(U)$? Is it true that every element of $\mathcal{O}_{X}(D(f_{i}))$ is realized as a restriction of an element of $\mathcal{O}_{X}(U)$? 3) My intuition tells me that if $V \subset U$, then the ring of regular functions $\mathcal{O}_{X}(V)$ should be in some sense ""bigger"" than the ring of regular functions $\mathcal{O}_{X}(U)$, since being regular on a subset is surely a weaker statement. Is there any usefulness to this intuition? More specifically to me previous question, are the $\mathcal{O}_{X}(D(f_{i}))$ some kind of ""extension"" in a ring theoretic sense of $\mathcal{O}_{X}(U)$? I apologize if this question is rather vague and/or long winded, I just feel like I am not understanding it well enough to word my exact problem concisely. I would appreciate any answers or advice, and if someone can somehow read between the lines and see what it is I'm missing regarding defining sheaves from local data that would be massively appreciated. Thanks","['localization', 'sheaf-theory', 'algebraic-geometry', 'commutative-algebra']"
2037863,Eigenvalues and elementary row operations,"We know that elementary row operations do not change the determinant of a matrix but may change the associated eigenvalues. Consider an example, say two $5 \times 5$ matrix are given: $$A = \begin{pmatrix}
0 & 1 & 0 & 0 & 0\\
a & b & 0 & 0 & 0\\
0 & 0 & p & q & r\\
0 & 0 & s & t & u\\
0 & 0 & v & w & x\\
\end{pmatrix}, \hspace{2cm}    B = \begin{pmatrix}
0 & 1 & 0 & 0 & 0\\
a & b & 0 & 0 & 0\\
0 & 0 & p & q & r\\
0 & 0 & s & t & u\\
ka & kb & v & w & x\\
\end{pmatrix} $$ 
Now $B$ can easily be reduced to $A$ by using the following operation on $B$
$$R_5 - kR_2$$
Now these two have the same eigenvalues. It is cumbersome to try to symbolically calculate the eigenvalues to show they are indeed same (I have tried tons of matrices with random numbers in Mathematica ). $A$ is a block diagonal matrix and $B$ is reduceable to one. If you talked about systems, $A$ shows two decoupled spaces (of dimensions $2$ and $3$) within the $5-D$ vector space of $A$. Can anyone prove that such a pair of matrices always have same eigenvalues? Is there any property that says so? Does $A$ being a block diagonal matrix have to do anything with the eigenvalues being the same? Any insight or discussion is welcome! Please correct me if I used any term loosely or wrongly.",['matrices']
2037874,Finite generated sheaf of graded $\mathcal{O}_X$ algebra,"Let $X$ be a projective normal variety. 
I am having trouble finding out what it means for a graded sheaf of $\mathcal{O}_X$ algebra $R$ to be finitely generated over $\mathcal{O}_X$. Does it mean that for each affine open $U=\operatorname{Spec} A\subset X$, one has $R|_U$ is a finitely generated graded algebra over $A$? Also how does one see that if $f:X\to Y$ is a binational morphism with $K_X$ $f$-ample, then $R=\bigoplus_{n\geq 0}f_*(\mathcal{O}_X(nK_X))$ is a finitely generated $\mathcal{O}_Y$-algebra? Thanks for the help!","['algebraic-geometry', 'birational-geometry']"
2037878,Show that $f_n(x) = \frac{\sin(nx)}{\sqrt{n}}$ converges uniformly to $0$,"I need to show that $f_n(x) = \frac{\sin(nx)}{\sqrt{n}}$ converges uniformly to $0$. My book has a theorem that says that if $|f_n(x)|<a_n$ and $\sum |a_n|$ converges, then $f_n$ converges uniformly. But $\sum \frac{1}{\sqrt{n}}$ doesn't converge, so I don't know how to do it. Should it be done by definition? $$\left|\frac{\sin(nx)}{\sqrt{n}}\right|\le \left|\frac{1}{\sqrt{n}}\right|$$ if I take $n_0 = \frac{1}{\epsilon^2}$, then $n>n_0\implies n>\frac{1}{\epsilon^2}\implies \sqrt{n}>\frac{1}{\epsilon}\implies \frac{1}{\sqrt{n}}<\epsilon$ therefore it converges uniformly. Now, I need to also show that $f'_n$ diverges in every point of the interval $[0,1]$. The derivative is: $$\frac{n\cos nx}{\sqrt{n}}$$ should I just say that this limit goes to infinity because $n$ grows faster than $\sqrt{n}$?","['real-analysis', 'sequences-and-series', 'calculus']"
2037946,Convergence of $\sum_{n=1}^{\infty}{\sqrt[n]{n}-1 \over n}$,"I am trying to conclude about the convergence of
$$\sum_{n=1}^{\infty}{\sqrt[n]{n}-1 \over n}$$
If I take ${\sqrt[n]{{\sqrt[n]{n}-1 \over n}}}= {\sqrt[n]{\sqrt[n]{n}-1} \over \sqrt[n]{n}}$ this looks like it converges to $0$ but I am suspicious about the numerator. At the same time, other techniques I know like the ratio test, $n(\sqrt[n]{n}-1)$ and comparisons don't yield meaningful results. Any hints?","['radicals', 'real-analysis', 'sequences-and-series']"
2037974,Surface integral over a sphere of inverse of distance,"Let $S$ be a sphere in $\mathbb{R}^3$ of radius $r$ centered at the origin and $x_0\not\in S$. Let $f:\mathbb{R}^3\to\mathbb{R}$ be given by $f(x)=\Vert x-x_0\Vert$. I'm asked to compute the (surface) integral
$$
\int_S fdS
$$
I think I have to separate this in the cases $\Vert x_0\Vert>r$ and $\Vert x_0\Vert<r$. For the former, we could use the divergence theorem and for the latter, try to show some kind of invariance and reduce the problem to the case where  we have to integrate over a small sphere around $x_0$. However, I haven't been able to develop this ideas as I can't find suitable vector fields $F$ to work with them. Any kind of help or suggestions are greatly appreciated.","['multivariable-calculus', 'surface-integrals']"
2038004,"Coupling showing that $\operatorname{Bin}(n,\frac{1}{n+1})$ stochastically dominates $\operatorname{Bin}(n-1,\frac{1}{n})$","The classical inequality
$$
\left(1-\frac{1}{n}\right)^{n-1} > \frac{1}{e}
$$
has a probabilistic generalization: the binomial distribution $\operatorname{Bin}(n-1,\frac{1}{n})$ is stochastically dominated by the Poisson distribution $\operatorname{Po}(1)$. A simple coupling proof of this can be found in Klenke and Mattner, Stochastic ordering of classical discrete distributions . They also prove the stronger result that $\operatorname{Bin}(n,\frac{1}{n+1})$ stochastically dominates $\operatorname{Bin}(n-1,\frac{1}{n})$, which generalizes the fact that the sequence $(1-\frac1n)^{n-1}$ is decreasing. Is there a natural coupling that shows that $\operatorname{Bin}(n,\frac{1}{n+1})$ stochastically dominates $\operatorname{Bin}(n-1,\frac{1}{n})$? What I have in mind is an elementary argument that involves a balls-into-bins scenario, but any other simple coupling would be great. (Just to be clear: I already know that there exists some coupling, this follows from the stochastic domination.)","['binomial-distribution', 'probability', 'coupling']"
2038035,Analytic function bounded away from two simple poles,"I am having trouble hunting down a solution to an old exam problem.  This is a type of problem I seem to have trouble with, so maybe someone can help me rewire my brain for these. Suppose $f$ is analytic on $\mathbb{C} \setminus \{ \pm 1 \}$ and there exist $a$, $b>0$ such that $$|f(z)| \leq \frac{a}{|z-1|} + \frac{b}{|z+1|}$$ for all $z \in \mathbb{C} \setminus \{ \pm 1 \}$.  Prove that $$f(z) = \frac{p(z)}{z^2 - 1}$$ where $p$ is a polynomial of degree $\leq 1$. I sort of see this by noting that $$\lim_{z \to \infty}|f(z)| \leq \lim_{z \to \infty} \frac{a}{|z-1|} + \frac{b}{|z+1|} = 0$$ so that certainly if $f$ has the form indicated, then the numerator must have degree less than 2.  But why this form in the first place?",['complex-analysis']
2038099,Finding a derivative using multiple chain rules,"Find the derivative of the function.
$y = [x + (x + \sin^2 x)^3]^4$ I know how to use the chain rule and I found the derivative to be: $$4[x+(x+\sin^2(x))^3]^3 \cdot (1 + 3(x + \sin^2(x))^2) \cdot (1+\sin (2x))$$ but my online homework says that this is wrong. I can't figure what what I've done wrong and I've tried it several times now. Can somebody help? Note: In the last term, I simplified $2\sin x\cos x$ to be $\sin(2x)$. I tried inputting both versions into my homework but it was wrong both ways.","['derivatives', 'chain-rule', 'calculus']"
2038140,On the Whitney-Graustein theorem and the $h$-principle.,"If you are in a hurry and this question still has caught your interest, please jump directly to the last proposition, where my question lies. Throughout this question I am going to identity $\mathbb{S}^1$ and $[0,1]/\partial[0,1]$. Therefore, when I will talk about mappings from $\mathbb{S}^1$ to $\mathbb{R}^2$, I will consider maps $f\colon[0,1]\rightarrow\mathbb{R}^2$ such that $f(0)=f(1)$. Let $I(\mathbb{S}^1,\mathbb{R}^2)$ be the set of immersions of $\mathbb{S}^1$ into $\mathbb{R}^2$, that is the set of $C^1$-mappings from $\mathbb{S}^1$ to $\mathbb{R}^2$ such that their derivatives do not vanish. My goal is to prove the well-known: Theorem. (Whitney-Graustein) The turning number gives a bijection from $\pi_0(I(\mathbb{S}^1,\mathbb{R}^2))$ to $\mathbb{Z}$. For sake of clarity, by now let $X:=C^0(\mathbb{S}^1,\mathbb{R}^2\setminus\{(0,0)\})$. Inspired by the Gromov's $h$-principle , I introduced the following map: 
$$J\colon\left\{\begin{array}{ccc}I(\mathbb{S}^1,\mathbb{R}^2)&\rightarrow&X\\f&\mapsto&f'\end{array}\right..$$ I claim that one has the following: Theorem. The map $J$ induces a well-defined bijection $$\pi_0(J)\colon\left\{\begin{array}{ccc}\pi_0(I(\mathbb{S}^1,\mathbb{R}^2))\rightarrow\pi_0(X)\\ [f]_0\mapsto [f']_0\end{array}\right..$$ I have already prove the well-definedness and the following: Proposition. Let $f\in X$, there exists $g\in I(\mathbb{S}^1,\mathbb{R}^2)$ and $H\colon\mathbb{S}^1\times[0,1]\overset{C^0}{\rightarrow}\mathbb{R}^2\setminus\{(0,0)\}$ such that $H(\cdot,0)=f$ and $H(\cdot,1)=g'$. Proof. On request. $\Box$ Which has direct corollary $\pi_0(J)$ being surjective. Hence, I am left to establish the following: Proposition. Let $g_1,g_2\in I(\mathbb{S}^1,\mathbb{R}^2)$ such that there exists $H\colon\mathbb{S}^1\times [0,1]\overset{C^0}\rightarrow\mathbb{R}^2\setminus\{(0,0)\}$ such that $H(\cdot,0)={g_1}'$ and $H(\cdot,1)={g_2}'$. Then, there exists $F\colon\mathbb{S}^1\times [0,1]\overset{C^1}{\rightarrow}\mathbb{R}^2$ such that $F(\cdot,0)=g_1$, $F(\cdot,1)=g_2$ and for all $t\in [0,1],F(\cdot,t)\in I(\mathbb{S}^1,\mathbb{R}^2)$. Proof. My idea is to integrate the homotopy $H$, that is introducing: $$F(x,t):=\int_0^xH(u,t)\,\mathrm{d}u-x\int_0^1H(u,t)\,\mathrm{d}u.$$
The removed corrective term is here to ensure that for all $t\in [0,1]$, $F(0,t)=F(1,t)$, that is for the well-definedness of $F(\cdot,t)$ on $\mathbb{S}^1$. Notice that one has $F(\cdot,0)=g_1-g_1(0)$ and $F(\cdot,1)=g_2-g_2(0)$. Therefore, if for all $t\in [0,1]$, $F(\cdot,t)\in I(\mathbb{S}^1,\mathbb{R}^2)$, I am almost done. However, it is not clear and wrong in all generality, that the following quantity is nonzero: $$\frac{\mathrm{d}}{\mathrm{d}x}F(x,t)=H(x,t)-\int_{0}^1H(u,t)\mathrm{d}u.$$
That is where I am stuck. $\Box$ Question. If $H(x,\cdot)$ is non-constant and belongs to $\mathbb{S}^1$, I am done. Indeed, $\displaystyle\int_0^1H(u,t)\mathrm{d}u$ will lie in the interior of the unit disk. However, it is not clear to me that I can boil down my problem to this case and doing a naive radial on $H(x,\cdot)$ homotopy does not seem to help in anything. If the latter proposition is true, I am done with the injectivity of $\pi_0(J)$ and with the Whitney-Graustein theorem. Indeed, I will have the following commutative diagramm, where all arrows are bijections: $$\require{AMScd}\begin{CD}
\pi_0(I(\mathbb{S}^1,\mathbb{R})) @>\textrm{turning number}>> \mathbb{Z}\\
@VV\pi_0(J)V @AA\deg A\\
\pi_0(X) @>\textrm{str. def. retract}>> \pi_1(\mathbb{S}^1)\end{CD}$$ Any enlightenment will be greatly appreciated. If my approach proving the last proposition is plain wrong, could you provide me some other thoughts to manage my way toward the proof?","['h-principle', 'differential-geometry', 'homotopy-theory']"
2038178,"Proving that if $(ab)^{p}=a^{p}\,b^{p}$, then the p-sylow subgroup is normal","So, while studying Abstract Algebra, i ran into this problem (i.n. Herstein, second edition, chapter 2.12) and have been stuck since: Given a group of finite order, and a prime $p$ that divides $o(G)$, and suppose that $\forall\,a,b\in G$, $(ab)^{p}=a^{p}\,b^{p}$. Prove that the $p$-sylow subgroup is normal in $G$. What I've tried: I defined a mapping $\varphi:G\to H=\lbrace x^{p}:\,x\in G\rbrace;\,\,\varphi(x)=x^{p}$, wich would be a surjective homomorphism. Then, I proved that $\ker(\varphi)\subseteq P$, where $P$ is a $p$-sylow subgroup. If I could prove either that $P\subseteq\ker(\varphi)$ or that $o(\ker(\varphi))=o(P)$, that would end it, because that would imply that $P=\ker(\varphi)$, and I know that $\ker(\varphi)\unlhd G$. One ideia to follow up on those would be to use the firs isomorphism theorem ($G/\ker(\varphi)\simeq Im(\varphi)$) to get $o(G)=o(\ker(\varphi))o(Im(\varphi))$ and from there work something out about the orders, but I cannot think of how to do that. I've also tried proving that $G$ only has one $p$-sylow subgroup, using Sylow's third theorem, but I believe it's a dead end. Any ideas?","['finite-groups', 'abstract-algebra', 'sylow-theory']"
2038212,question about line bundle on projective scheme,"Let $X$ be a projective variety. Suppose $\mathcal{L}$ is a basepoint free globally generated line bundle so we get a map $\pi:X\to \mathbb{P}^N$ induced by $\mathcal{L}$. Let $R_n=H^0(X,\mathcal{L}^{\otimes{n}})$ and $R=\oplus_{n=0}^{\infty}R_n$. I vaguely remember that $\overline{\pi(X)}\cong \text{Proj} (R)$ (the grading is by natrual number) Is it true? In particular, when $\mathcal{L}$ is very ample,do we have $X\cong Proj(R)$?
Thank you!","['sheaf-cohomology', 'algebraic-geometry']"
2038218,"What is the second derivative?, Part I","I've been interested in learning about higher order derivatives of vector functions recently and inspired by this answer by @Bye_World , I have some questions. So first, how exactly do we define higher order derivatives of functions $f:\Bbb R^m \to \Bbb R^n$?  I know that the first derivative is the linear function $L$ such that $$\lim_{h\to 0}\frac{\|f(x+h)-f(x)-L(h)\|_{\Bbb R^n}}{\|h\|_{\Bbb R^m}}=0$$ I asked a question earlier today about a possible limit definition of the second derivative but found that my guess at one didn't work.  Is there no better way than to define it recursively by $$\lim_{h\to 0}\frac{\|D^{n-1}f(x+h)-D^{n-1}f(x)-L(h)\|_{\Bbb R^n}}{\|h\|_{\Bbb R^m}}=0\, ?$$ And even in that formula, I don't know how you take account of the fact that the $n$th derivative is evaluated at $n$ different points (or vectors?). Also @Bye_World mentions ""the isomorphism $\mathcal L(X,\mathcal L(X,Y)) \simeq \operatorname{Bil}(X\times X\to Y)$"".  What does that mean and how is it related?","['multivariable-calculus', 'real-analysis', 'derivatives']"
2038229,How tall should my Christmas tree be?,"This question has vexed me for the 20 years we've lived at my current house.  There is a fir tree in the front that I dress every Christmas with lights.  It grows.  I prune it.  This is what it looks like with the lights on... The bulbs (purple dots) are all on a single string that I start at the top and helically wrap down to the bottom.  There are 100 bulbs spaced 300mm apart.  I have decided that the tree looks best if the height is twice the width at the base. Q.  What height should I maintain the tree at so that all the bulbs are equispaced from each other?  I take this to mean that the next wrap around the tree is 300mm in Z below the previous wrap.  Not perfect equidistance, but it will do for the neighbours and me. (There are similar questions, but I believe non so specific.)",['geometry']
2038230,"Prove or disprove:If $m^*(E)+m^*([0,1]-E)=1 $ then $E$ is a measurable set.","Prove or disprove the following statement. Let $E\subset [0,1]$, if $ m^*(E)+m^*([0,1]\setminus E)=1$ ,then $E$ is a measurable set, where $m^*$ is Lebesgue outer measure. At first glance, I thought it is triavially true. That is, the set $E$ is measurable provided for any set $A$, $$m^*(A \cap E)+m^*(A\cap E^c)=m^*(A)..............(*),$$ and thus $$m^*(E)+m^*([0,1]\setminus E) =  m^*([0,1]\cap E)+m^*([0,1]\setminus E)=m([0,1])=1.$$ Hence $E$ is measurable. But I realized that using the whole set $[0,1]$ as $A$, i.e., $A=[0,1]$ is just a particular case and that I need to show  that $(*)$ is true for any set $ A$. Can someone give me an idea how to go by this?","['outer-measure', 'real-analysis', 'lebesgue-measure', 'measure-theory']"
2038248,Proving that the function $f(x)=\sum_i|x-q_i|2^{-i}$ is not differentiable at any rational point,"Here,$\{q_i\}$ an enumeration of the rationals in the closed unit interval.Take $y=q_j\in \mathbb{Q}\cap [0,1]$. I tried to gleen something from the following manipulation \begin{align*}
&\ \ \ \lim_{x\rightarrow q_j}\frac{1}{x-q_j}\sum_i 2^{-i}(|x-q_i|-|q_j-q_i|)\\
&=\lim_{x\rightarrow q_j}\frac{1}{x-q_j}(2^{-j}|x-q_{j}|+\sum_{i\ne j} 2^{-i}(|x-q_i|-|q_j-q_i|))\\
&=2^{-j}+\lim_{x\rightarrow q_j}\frac{1}{x-q_j}\sum_{i\ne j} 2^{-i}(|x-q_i|-|q_j-q_i|)
\end{align*}
Which I think will blow up, since the sum is some constant and the denominator blows up, with no chance of the differences in the numerator vanishing. Is my work correct? How can I show this a little more rigorously than just saying some words? For example, am I justified in commuting the limit and the sum?","['derivatives', 'real-analysis', 'geometric-measure-theory', 'measure-theory', 'lipschitz-functions']"
2038261,"What is wrong with this ""proof"" that $a^q \equiv a \pmod n$ for any $a, q, n$?","I came across this while trying to prove this for a specific $n$ and $q$, but I knew it was wrong once it generalized to any $n$, it's simply too good to be true: Let $\gcd (a, n) = d$. Then $d | a \implies d|a^q$, so $$d|(a^q-a)$$ Also $d|n$, so there exist integers $k_1, k_2$ such that $$k_1 \cdot d = a^q-a \tag{1}$$ $$k_2 \cdot d = n \tag{2}$$ So $(2)$ is equivalent to $d = \dfrac {n}{k_2}$. Substituting this in $(1)$, $$k_1 \cdot \dfrac {n}{k_2} = a^q-a$$ $$ \dfrac {k_1}{k_2} \cdot n = a^q-a$$ Because the RHS is an integer and $n$ is an integer, $\dfrac {k_1}{k_2}$ is an integer, so $$a^q \equiv a \pmod n$$","['discrete-mathematics', 'modular-arithmetic', 'elementary-number-theory']"
2038286,Accurate identities related to $\sum\limits_{n=0}^{\infty}\frac{(2n)!}{(n!)^3}x^n$ and $\sum\limits_{n=0}^{\infty}\frac{(2n)!}{(n!)^4}x^n$,"Some time ago while playing around with maths, I derived the following pair of formulae: $$\sum_{n=0}^{\infty}\frac{(2n)!}{(n!)^3}x^n=\frac{2}{\pi}\int_0^{\frac{\pi}{2}}e^{4x\cos^2\theta}\;d\theta\tag{1}$$ $$\sum_{n=0}^{\infty}\frac{(2n)!}{(n!)^4}x^n=\frac{2}{\pi}\int_0^{\frac{\pi}{2}}I_{0}\left(4\sqrt{x}\cos\theta\right)\;d\theta\tag{2}$$ as well as the following generalizations: $$\sum_{n=0}^{\infty}\frac{(2n)!^m}{(n!)^{2m+1}}x^n=\left(\frac{2}{\pi}\right)^m\int_0^{\frac{\pi}{2}}\ldots\int_0^{\frac{\pi}{2}}e^{4^{m}x\prod_{k=1}^{m}\cos^2\theta_k}\;d\theta_1\ldots\;d\theta_m\tag{3}$$ $$\sum_{n=0}^{\infty}\frac{(2n)!^m}{(n!)^{2m+2}}x^n=\left(\frac{2}{\pi}\right)^m\int_0^{\frac{\pi}{2}}\ldots\int_0^{\frac{\pi}{2}}I_0\left(2^{m+1}\sqrt{x}\prod_{k=1}^{m}\cos\theta_k\right)\;d\theta_1\ldots\;d\theta_m\tag{4}$$ (In what follows $J_0$ and $I_0$ and Bessel are modified Bessel functions respectively of the first kind). The method I used to find these was extremely formal (in the second sense of this answer) and I know that such derivations can give spurious results, so my question is: are these formulae correct? (Note that I have asked them in the same question because my derivation below proved them together, requiring one to prove the other. These series could be written in terms of binomial coefficients but this page does not seem to contain any formulae like these involving powers of binomial coefficients in the numerator of the summand.) [ Edit : I have just noticed this question which appears to mention a special case of $(2)$ (modified by the formal identity $J_0(ix)=I_0(x)$ ).] Outline of formal derivation : I started with the Laplace transform formula $L\left[t^{\frac{n}{2}}\int_{0}^{\infty}u^{-\frac{n}{2}}J_n(2\sqrt{ut})f(u)\;du\right](s)=\frac{1}{s^{n+1}}L[f(t)]\left(\frac{1}{s}\right)$ from Borelli and Coleman's Differential Equations: a modelling perspective (the proof is not too difficult), which formally implies that if $f(t)=\sum\limits_{n=0}^{\infty}a_{n}t^n$ then: $$\sum_{n=0}^{\infty}\frac{a_{n}t^n}{n!}=L^{-1}\left[\frac{f(\frac{1}{s})}{s}\right](t)=\int_0^\infty J_0(2\sqrt{ut})L^{-1}[f(s)](u)\;du\tag{5}$$ Taking $f(t)=\sqrt{\frac{1}{1-t}}=\sum\limits_{n=0}^{\infty}\frac{(2n)!t^n}{(n!)^{2}4^n}$ (a binomial series ) in $(5)$ , by convolution and shifting we get $\sum\limits_{n=0}^{\infty}\frac{(2n)!t^n}{(n!)^{3}4^n}=L^{-1}\left[\frac{1}{s}\sqrt{\frac{1}{1-\frac{1}{s}}}\right](t)=\frac{1}{\pi}\int_0^t \frac{e^u}{\sqrt{u(t-u)}}\;du$ , and $\color{#ff0000}{(1)}$ follows by setting $u=\frac{t\left(\cos(2\theta)+1\right)}{2}$ . Taking $f(t)=\sum\limits_{n=0}^{\infty}\frac{(-1)^n(2n)!}{(n!)^3}t^n$ in $(5)$ and using $(1)$ and elementary trigonometric identities gives $\sum\limits_{n=0}^{\infty}\frac{(-1)^n(2n)!}{(n!)^4}t^n=\frac{1}{\pi}\int_0^\infty J_0(2\sqrt{ut})L^{-1}\left[e^{-2s}\int_0^\pi e^{-2s\cos\theta}\;d\theta\right](u)\;du$ . But making the transformation $\theta=\cos^{-1}\left(\frac{t-2}{2}\right)$ , the argument of the inverse Laplace transform becomes $\int_0^4\frac{e^{-st}}{\sqrt{4t-t^2}}\;dt$ which is the Laplace transform of $\frac{1-H(t-4)}{\sqrt{4t-t^2}}$ ( $H(t)$ is the Heaviside function ) so our formula reduces to $\frac{1}{\pi}\int_0^\infty J_0(2\sqrt{ut})\frac{1-H(u-4)}{\sqrt{4u-u^2}}\;du$ which by taking $u=2(1+\cos2\theta)$ and formally setting $J_0(ix)=I_0(x)$ gives $\color{#ff0000}{(2)}$ . Using the resulting formula $\sum\limits_{n=0}^{\infty}\frac{(2n)!}{(n!)^4}(-1)^n t^{2n}=\frac{2}{\pi}\int_0^{\frac{\pi}{2}}J_{0}\left(4t\cos\theta\right)\;d\theta$ and taking the Laplace transform of both sides and interchanging integral and Laplace transform gives $\sum\limits_{n=0}^{\infty}\frac{(2n)!^2}{(n!)^4}\frac{(-1)^n}{ s^{2n+1}}=\frac{2}{\pi}\int_0^{\frac{\pi}{2}}\frac{1}{\sqrt{s^2+16\cos^2\theta}}\;d\theta$ , which gives: $$\sum_{n=0}^{\infty}\frac{(2n)!^2}{(n!)^4}(-1)^n t^n=\frac{2}{\pi}\int_0^{\frac{\pi}{2}}\frac{1}{\sqrt{1+16t\cos^2\theta}}\;d\theta\tag{6}$$ This is an elliptic integral I believe. Applying $(5)$ to this, interchanging integral and Laplace transform, applying convolution and shifting, and taking $u=\frac{t}{2}(1+\cos2\varphi)$ lets us arrive at: $$\sum_{n=0}^\infty \frac{(2n)!^2}{(n!)^5}x^n=\frac{4}{\pi^2}\int_0^{\frac{\pi}{2}}\int_0^{\frac{\pi}{2}}e^{16x\cos^2\theta\cos^2\varphi}\;d\theta\;d\varphi\tag{7}$$ The methods of deriving $(2)$ from $(1)$ and of deriving $(7)$ from $(2)$ may be easily extended to an induction deriving $\color{#ff0000}{(3)}$ and $\color{#ff0000}{(4)}$ . Accuracy : The error of $(1)$ for $x=0.02$ according to Wolfram|Alpha is on the order of $10^{-16}$ which seems extremely small although I am not sure about the sizes of errors in Wolfram|Alpha (and there are very accurate near identities ; I have also derived elsewhere identities with errors of $10^{-16}$ which appear to be mere approximations). I am not sure about the accuracy of $(2)$ , $(3)$ , or $(4)$ but noting the relation of $(6)$ to elliptic integrals we get the following $AGM$ formula: $$\sum_{n=0}^{\infty}\frac{(2n)!^2}{(n!)^4}x^n=\frac{1}{AGM(\sqrt{1-16x},1)}\tag{8}$$ whose error at $x=0.02$ appears to be also on the order of $10^{-16}$ . This seems to imply that the derivation may still be on the right track when $(6)$ is shown. So my question is: Are some or all of the above (highlighted) identities correct?","['bessel-functions', 'laplace-transform', 'calculus', 'elliptic-integrals', 'sequences-and-series']"
2038290,Proof that the series $\sum\limits_n P\left(\max\limits_{1\leq k\leq n}|X_{nk}|>\theta\sqrt{n\log n}\right)$ converges,"Let $\{X_{nk}\}$ be an array of i.i.d. random variables such that$$E|X_{11}|^4(\log^+|X_{11}|)^{-2}<\infty,\qquad EX_{11}^2=1,\qquad EX_{11}=0.$$ Then the series $$S=\sum_{n=1}^\infty P \left(\max_{1\leq k\leq n}|X_{nk}|>\theta\sqrt{n\log n} \right)$$ converges. This is what I saw in a paper and I failed to prove it. By Chebyshev inequality, 
\begin{equation}\nonumber
\begin{aligned}
S \leq&\sum_{n=1}^\infty E\left[\max_{1\leq k\leq n}|X_{nk}|^4(\log^+|X_{nk}|)^{-2}\right]
\frac{\left(\log\theta\sqrt{n\log n}\right)^2}{\theta^4n^2\log^2n}\\
\le&\sum_{n=1}^\infty E\left[|X_{11}|^4(\log^+|X_{11}|)^{-2}\right]
\frac{\left(\log\theta+\frac{1}{2}\log n+\frac{1}{2}\log\log n\right)^2}{\theta^4n\log^2n}.\\
\end{aligned}
\end{equation}
However, the above series is divergent. How to prove that $S$ converges?","['probability-theory', 'sequences-and-series', 'random-variables']"
2038350,Must a Hermitian/Kähler Manifold have a complex structure?,"Recall the following definitions: A Hermitian manifold is a smooth manifold $M$ endowed with a tensor field $J\in\mathcal{T}^1_1(M)$ and a Riemannian metric $g$ such that $$\forall x\in M,\ (J_x)^2 = -\mathrm{id}_{\mathrm{T}_x}$$ and such that the tensor field $\omega\in\mathcal{T}^2_0(M)$ defined by $\omega(-,-):=g(-,J(-))$ is a nondegenerate differential 2-form. Additionally, a Hermitian manifold $(M,J,g,\omega)$ is said to be Kähler if $\omega$ is closed – i.e. if $(M,\omega)$ is a symplectic manifold. Now, the tensor field $J$ allows us to consider $\mathrm{T}_x$ as a complex vector space for each $x\in M$, by defining, for $v\in\mathrm{T}_x$ and for $\lambda=\lambda_1 + i\lambda_2\in\mathbb{C}$, $$\lambda\, v = \lambda_1\, v + \lambda_2\, J(v).$$ My aim is to use this to show that in fact, for any smooth chart $(U,\varphi)$ of $M$ with $\varphi:U\hookrightarrow\mathbb{R}^{2n}$, we can identify $\mathbb{R}^{2n}$ with $\mathbb{C}^n$, and obtain a holomorphic atlas. Is anything like this possible? Or does there exist a Kähler manifold that cannot be endowed with (holomorphic) complex structure?","['complex-geometry', 'complex-manifolds', 'kahler-manifolds', 'manifolds', 'differential-geometry']"
2038383,Applications of Euclidean harmonic analysis to geometry?,"The term ""Euclidean harmonic analysis"" means the studying of the classical Fourier transform for functions on $\mathbb{R}^n$ or $\mathbb{T}^n$. This include the basic properties of the Fourier transform such as Plancherel's theorem for $L^2$ functions as well as other more sophisticated techniques and results, for example, the Fourier transform for $L^p$ functions, Hardy-Littlewood maximal function, Littlewood-Paley functionals, etc. My question is if there is any application for these theories to geometry? What I know about applications of the Fourier transform to geometry is that it can be used to define Sobolev spaces for $L^2$ functions on manifolds and then pseudodifferential operators. Combining these with the topological $K$-theory, one can prove the Atiyah-Singer index theorem which gives a lot of interesting results for compact manifolds. A standard reference for this is Lawson's book, Spin Geometry. But to do this we don't need anything that is essentially more delicate then the Plancherel's theorem or formulae such as $\widehat{\partial_x^\alpha u}(\xi)=\xi^\alpha \widehat{u}(\xi)$. So do all those $L^p$ results or Hardy-Littlewood maximal functions, etc. help us understand more about geometry? If so, does anyone has some reference for this kind of results?","['riemannian-geometry', 'fourier-analysis', 'harmonic-analysis', 'geometry', 'differential-geometry']"
2038428,"Why is $\infty \cdot 0$ an indeterminate form, if $\infty$ can be treated as a very large positive number?","Why is $\infty \cdot 0$ indeterminate? Although $\infty$ is not a real number, it can be treated as a very large positive number, and any number multiplied by $0$ is $0$. Why not in this case?","['indeterminate-forms', 'infinity', 'calculus']"
2038456,Intersection points of two polar curves,"I was asked to find the area inside $r=3cos\theta$ and outside $r=1+cos\theta$ (see figure) My question is, how do i find the intersection points, I was taught to make $1+cos\theta = 3cos\theta$ and solving it we get $\theta=\pi/3$ and $\theta =5\pi/3$, but as you can see the curves meet at the pole as well, how do I find this point? (I do not need this for the área but I'm just curious)","['multivariable-calculus', 'polar-coordinates']"
2038467,Rouché's Theorem Application,"I'm trying to use Rouché's theorem to find the number of roots of the function $f(z)=z^5+3z^4+9z^3+10$ on $\vert z\vert<2$. I know that the answer is $3$, but I have been unsuccessful in proving it. I've tried nearly every combination of the components of $f$ which also have $3$ roots, applying the theorem up to $3$ times, but to no avail. I've even resorted to trying unrelated polynomials, but there's a lot of those! A hint would surely benefit my sanity.",['complex-analysis']
2038469,Show that $\sum \pm X_n<\infty$ a.s. implies $\sum X_n^2<\infty$ a.s.,"Suppose $\{X_n\}$ are arbitrary random variables such that $\sum \pm X_n$ converges almost surely for all choices $\pm1$. Show that $\sum X_n^2$ converges almost surely. Denote $\{B_n\}$ Bernoulli random variables, the statement above is to say $\sum B_n X_n$ converges a.s. implies $\sum X_n^2$ converges a.s. Can anybody give a solution to this problem? This problem comes from Kai Lai Chung's A Course in Probability Theory , Third Edition, pp.129","['probability-limit-theorems', 'probability-theory', 'sequences-and-series']"
2038476,Probability that cutting a stick at two points forms a triangle?,"Math overflow has given many creative answers to this topic at link: https://mathoverflow.net/questions/2014/if-you-break-a-stick-at-two-points-chosen-uniformly-the-probability-the-three-r I mostly understand but just need one clarification to put me at ease. The solution I am most comfortable with in the context of my homework relates to uniform joint distributions. I refer to the answer given by ""Bill the Lizard"". ""The three pieces form a triangle if none of the pieces is greater than half the length of the stick . In other words..."" This conclusion leads to: $(y > 1/2) AND (x < 1/2) AND (y - x) < 1/2$ $(x > 1/2) AND (y < 1/2) AND (x - y) < 1/2$ Now, my confusion. If none of the pieces is greater than half the length of the stick , then why is $y > 1/2 \;\;and\;\; x > 1/2\;\;$ in the above inequalities?? Thank you.","['uniform-distribution', 'probability']"
2038495,Is sum of non-negative closed sets closed?,"Sum of two closed sets $A,B$ is closed is not necessarily closed. Every answer I could find constructs a counterexample exploiting subtraction in some way, the easiest one being
$$
A=\mathbb Z_{\ge1},\;B=\left\{-n+\frac{1}{n}:n\in\mathbb Z_{\ge2}\right\}\\
0\notin A+B
$$
A hard one is
$$
A = \{(x, y) \in \mathbb R^2 : y > 1/x,x > 0\},\; B = \{(x, y) \in\mathbb R^2 : y > -1/x,x < 0\}\\
\mathbb R\times\{0\}\not\subset  A+B = \{(x, y) \in \mathbb R^2 : y > 0\}
$$
Another one uses a density argument
$$
A=\mathbb Z,\;B=\alpha\mathbb Z\quad(\alpha\in\mathbb{ R\setminus Q})\\
\mathbb R\not\subseteq A+B
$$
Even the last one uses subtraction to achieve density. Replacing $\mathbb Z$ with $\mathbb N$ in the last counterexample does not work. Q: $A,B$ are closed subsets of $[0,\infty)$. Must $A+B$ be closed? Restriction to non-negative numbers takes away the possibility of exploiting subtraction in some sense so I think the answer is yes . Picking a Cauchy sequence $\{c_n\}$ in $A+B$ we see $$\exists (a_n,b_n)\in A\times B: a_n+b_n=c_n$$ Assuming $c_n\to c>0$ we note that $\exists N:$ if $n> N$,
$$ a_n+b_n\in B\left(c,\frac{c}{2}\right)\implies \frac{c}{2}<a_n+b_n<\frac{3c}{2}\implies \frac{c}{2}<a_n,b_n<\frac{3c}{2}$$
$\{a_n\},\{b_n\}$ are bounded sequences and by Bolzano Weierstrass each has at least one limit point but I can't continue from here on. If however $c=0$, it is clear that $a_n,b_n\to 0$ and therefore $0\in A+B$.","['general-topology', 'real-analysis']"
2038507,When is an outer product $ab^\top$ diagonalisable? [duplicate],"This question already has answers here : Eigenvalues of the rank one matrix $uv^T$ (3 answers) Closed 7 years ago . If $a$ and $b$ are $n\times 1$ vectors and $A=ab^\top$, why is A diagonalizable if and only if $a^\top b \ne 0$ ? From my understanding, $a^\top b$ is the sum of the diagonal of A. How does this relate to diagonalizability?","['matrices', 'diagonalization', 'linear-algebra']"
2038540,Permutations with men and women alternating,"There are $3$ men and $3$ women.They should be arranged in such a way that no man stands next to another man how many ways is it possible ? I got as far as - there can either be a man standing first or a woman. so it would be either M1 W1 M2 W2 M3 W3 or W1 M1 W2 M2 W3 M3 
for the first possibility,men can be arranged in 3! ways and women can also be arranged in $3!$ ways so they can be arranged in $3!*3!$ ways which is $36$ ways. This is also the case for the second possibility
so there are $36+36=72$ ways of arranging them but the answer is $144$ ways $(72\times2)$ how???","['permutations', 'combinatorics']"
2038575,Vector space as direct sum of kernel and image,"Let $T: V\to V$ be a linear map on an infinite-dimensional vector space $V$ over a field $F$. Suppose that $T(V)$ is finite dimensional, and $T^2(V)=T(V)$. Show that $V=\ker T\oplus T(V)$. (Note that this is not a duplicate of Show that the direct sum of a kernel of a projection and its image create the originating vector space. , it seems to be more difficult since we only have $T^2(V)=T(V)$ rather than $T^2=T$.) What I tried: I can prove $V=\ker T+T(V)$, what is difficult seems to be showing the direct sum, i.e. the intersection is 0. Let $v\in V$. Since $T(v)\in T(V)=T^2(V)$, $T(v)=T^2(x)$ for some $x\in V$. Then $T(v-T(x))=0$, so that $v-T(x)=y\in\ker T$. Thus $v=y+T(x)\in\ker T+T(V)$. Hence $V\subseteq\ker T+T(V)$. $\ker T+T(V)\subseteq V$ is clear since $V$ is a vector space, so $V=\ker T+T(V)$. I also have a previous result Linear Transformation from Infinite dimensional to Finite dimensional Space which may or may not be useful. Thanks for any help!","['abstract-algebra', 'linear-algebra', 'linear-transformations']"
2038617,The sum $\sum_{k=0}^{3n}(-3)^k \binom{6n}{2k}=2^{6n}$,"The question asks to show that $$\sum_{k=0}^{3n}(-3)^k \binom{6n}{2k}=2^{6n}$$ by considering the binomial expansion I thought about the use of $$(1+z)^n=\sum_{k=0}^{n}\binom{n}{k}z^k$$
with suitable complex number $z$, as the formula shows the $(-3)^k$ term might suggest the use of complex number, which take the imaginary part of the expansion However, I cannot find such $z$ that makes the sum to $2^{6n}$ Any hints are appreciated!","['combinatorics', 'binomial-coefficients']"
2038682,Dominated Theorem/ Monotone Convergence problem,"I'm trying to prove the following: Let $f_{n}$ be a sequence defined by $f_{n}(x)=(1-(x/n))^n\ln(x)1_{[1,n]}(x)$ for every $x\in\mathbb{R}$ and for every $n\geq 1.$ Show that $$\displaystyle\lim_{n\rightarrow\infty}\int_{-\infty}^{\infty}f_{n}(x)dx=\int_{-\infty}^{\infty}e^{-x}\ln(x)1_{[0,\infty]}(x)dx.$$ My attempt is based in using dominated convergence because the limit of the sequence $f_{n}$ convergences without problem to $e^{-x}\ln(x)1_{[0,\infty]}(x).$ I'm stuck finding a function such as dominates the sequence $f_{n}$ and of course, it be integrable. I tried bounded $\ln(x)$ with identity function and $e^-x$; the problem here is that the sequence $(1-\frac{x}{n})^n$ is monotone decrecient. Any kind of help is thanked in advance.","['real-analysis', 'lebesgue-measure', 'measure-theory', 'probability-theory']"
2038688,vector space dimension of linear varieties,"Suppose $Y=Z(f_1,\cdots ,f_r)\subseteq \mathbb{A}^n_k$ where each $f_i$'s are linear homogeneous polynomials which are $k$-linear independent. Then $Y$ is also a vector space over $k$. My question: Is the vector space dimension of $Y$ is same as the dimension of $Y$ as an affine variety? If yes how? Thank you.","['affine-geometry', 'algebraic-geometry']"
2038715,Spectrum of the inverse operator?,How can you prove that the spectrum of the inverse of an operator $A^{-1}$ is given by all $\frac{1}{\lambda}$ for all $\lambda \in \sigma(A)\backslash \{0\}$?,"['real-analysis', 'analysis', 'proof-explanation']"
2038752,How many $3 \times 3$ integer matrices are orthogonal?,"Let $S$ be the set of $3 \times 3$ matrices $\rm A$ with integer entries such that $$\rm AA^{\top} = I_3$$ What is $|S|$ (cardinality of $S$)? The answer is supposed to be 48. Here is my proof and I wish to know if it is correct. So, I am going to exploit the fact that the matrix A in a set will be orthognal, so if the matrix is of the form \begin{bmatrix}
        a_{11} & a_{12} & a_{13} \\
        a_{21} & a_{22} & a_{23} \\
        a_{31} & a_{32} & a_{33} \\
        \end{bmatrix} Then each column and row will have exactly one non-zero element which will be +1 or -1. Thus, I have split possibilities for the first column into three cases and counted the possibilities in each case as follows :- $$a_{11} \neq 0$$ or $$ a_{21} \neq 0$$ or $$ a_{31} \neq 0$$ In case 1), we obviously have two possibilities(+1 or -1) so we consider the one where the entry is +1. Now, notice that the moment we choose the next non-zero entry, all the places for non-zero entries will be decided because of the rule 'each column and row will have exactly one non-zero element'. Meaning, if b and c are remaining two non-zero entries, we only have two possibilities left 
\begin{bmatrix}
        1 & 0 & 0 \\
        0 & b & 0 \\
        0 & 0 & c \\
        \end{bmatrix} or \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 0 & c \\
        0 & b & 0 \\
        \end{bmatrix} Using the fact that b and c are simply
$$\pm1$$ In each of the above matrices, we get 4 possibilities for each of the matricies. Thus, 8 possibilities in totality. Basically, we are getting 8 possibilities on the assumption that $$a_{11} = 1$$ Thus, we get 16 possibilities on the case that $$a_{11} \neq 0$$ Following, the second and third cases analogously, we get a total of 16 possibilities in each of them and 48 possibilities in total. Source :- Tata Institute of Fundamental Research Graduate School Admissions 2016","['matrices', 'orthogonal-matrices', 'proof-verification', 'combinatorics', 'linear-algebra']"
2038759,Why is it necessary to have convexity outside the feasible region?,"In the book ""convex optimization"" by Prof. Boyd and Prof. Vandenberghe, a convex problem is defined as, $$
\begin{equation}\label{op_prob_gen_equivalent}
\begin{aligned}
& {\text{minimise}} & &  \  f_0(x)
\\
& \text{subject to} & & f_i(x) \leqslant 0 \ , \ \quad \forall i \in \{1,2,...,m\}\\
&&& a_i^Tx=b_i \ , \ \quad \forall i \in \{1,2,...,p\}\\
\end{aligned} \quad ,
\end{equation}
$$
where $f_0(x) \ , \ f_1(x) \ , \ ... \ , \ f_m(x)$ are convex. Let $X$ be the feasible set defined as $X=\{x| x \in \mathbb{R}^n, \ f_i(x) \leqslant 0 \  \forall i =1,2,...,m , \ a_i^Tx=b_i \ , \ \forall i =1,2,...,p \}$. My question is, why is it necessary for $f_0(x) \ , \ f_1(x) \ , \ ... \ , \ f_m(x)$ to be convex outside $X$?","['real-analysis', 'convex-optimization']"
2038788,Does this propetry $A\subseteq f(A)$ have a name?,"Let $f:X\to X$. If $S\subset X$ is such that $f(S)\subseteq S$, the set $S$ is called $f$-invariant. But what if $$A\subseteq f(A)$$ holds for some $A\subset X$? Does this property of the set $A$ have a name?","['elementary-set-theory', 'notation', 'functions']"
2038799,Prove a geometry question about angles and radii in five collinear circles?,Our teacher give us this question and I worked on it but I couldn't find a way to prove that. Is it possible to help me to prove that? Thanks.,"['circles', 'geometry']"
2038861,How do algebraists intuitively picture normal subgroups and ideals?,"I have developed some intuitive picture to homomorphisms and kernels, but have troubles developing useful insight to normal subgroups and ideals.","['abstract-algebra', 'normal-subgroups', 'ring-theory', 'group-theory', 'ideals']"
2038969,Show the differential operator is bounded,"I need to show the differential operator is bounded. I believe it is bounded on $C[0,1]$ with the sup norm by 1, but I am struggling to show this. What I have so far:
NTS: $\|Tf\|\leq\|T\|\|f\|$ Where: \begin{equation} \|Tf\|=\sup_{\|f\|=1}\|Tf\|=\sup_{\|f\|=1} \sup_{x\in [0,1]} |f'(x)|=\sup_{\|f\|=1} \sup_{x\in [0,1]} \left|\frac{\lim_{h\rightarrow 0} f(a+h)-f(a)}{h}\right|
\end{equation} Not entirely sure if I should have written the derivative like that in the last equality above. I need a hint to see where to go next. Thanks in advance!","['functional-analysis', 'normed-spaces', 'differential-operators']"
2039001,$\mathbb F$-Martingale indexed on $\mathbb Q$ and left/right limit on $\mathbb R$ along $\mathbb Q$,"I have a question about an article (reference below). Let $(X_t)_{t\in\mathbb Q_+}$ be an $\mathbb F$-martingale (*) (Assume $\mathbb F$ is right-continuous). Let $C_t$ be the set of all $\omega$  s.t. $X_.(\omega)$ has a left and right limit in $\mathbb R$ along $\mathbb Q$ for all $s\in[0,t]$. I have no more assumptions (I think I have carefully read the reference below). Questions : (*) involve $\mathbb P(C_t) = 1$. My question is why ? Furthermore do I get : $\mathbb E [X_t\mid \mathcal F_s]=X_s$ for $t\in\mathbb Q$ and $s\in\mathbb R$ with $t>s$ ? I think no because $X_s$ is not well defined. Reference : Jacod - 1985 - Grossissement initial, hypothese H' et theoreme de girsanov - pages 18/19 - proof of lemma 1.8 - take a glance at (1.9) page 19.","['stochastic-processes', 'martingales', 'general-topology', 'measure-theory']"
2039015,Are vector field flows and Feller processes related?,"This is a follow-up to a previous question of mine . In particular I know now that the terminology used for Lie groups which I referenced is a special case of the subject of flows of smooth vector fields on smooth manifolds, like that discussed in Lee's Introduction to Smooth Manifolds . Question: Is there a general theory encompassing both the one-parameter semigroups of Feller processes and the one-parameter groups of flows of smooth vector fields? Obviously the spaces involved are in general quite different, but there seems to be many analogies, in particular both involve infinitesimal generators and exponential maps. Perhaps an answer will be related to the theory of stochastic processes on Riemannian manifolds, I don't know. A reference will suffice for an answer.","['stochastic-processes', 'reference-request', 'differential-geometry']"
2039039,What is the Fisher information matrix of the Wishart distribution?,"I have been struggling computing the Fisher's information of the Wishart distribution. I'll write what I have gone through. Let's $\Omega$ denote a $p\times p$ Wishart random variate denoted by $\mathcal{W}(k,V)$ where $k$ is the degrees of freedom and $V$ a positive definite scale matrix. If we write $\mathcal{W}(\Omega\,|\,k,V)$ for the density function,
$$
\begin{align}
\nabla_{\operatorname{vech}(V)}\log\mathcal{W}(\Omega\,|\,k,V) &= \dfrac{1}{2}D_{p}'\left(V^{-1}\otimes V^{-1}\right)D_{p}\operatorname{vech}(\Omega)-\dfrac{k}{2}D_{p}\operatorname{vech}\left(V^{-1}\right)\\
\nabla_{k}\log\mathcal{W}(\Omega\,|\,k,V) &= \dfrac{1}{2}\log|\Omega|-\dfrac{1}{2}\log|V|-\dfrac{p}{2}\log 2-\dfrac{1}{2}\sum_{i=1}^{p}\psi\left(\dfrac{k+1-i}{2}\right)
\end{align}
$$
where $D_{p}$ is the unique duplication matrix such that $D_{p}\operatorname{vech}(A)=\operatorname{vec}(A)$, $\otimes$ Kronecker product, and $\psi$ digamma function. What is the Fisher information matrix of the Wishart distribution?","['matrix-calculus', 'statistics', 'random-matrices', 'random-variables']"
2039063,Is a function whose derivative vanishes at rationals constant? [duplicate],"This question already has an answer here : Let $f:\mathbb{R}\longrightarrow \mathbb{R}$ a differentiable function such that $f'(x)=0$ for all $x\in\mathbb{Q}$ [closed] (1 answer) Closed 7 years ago . I'm trying to make a problem for my advanced calculus students.  I was thinking, if we have a differentiable function $f:\mathbb{R}\to\mathbb{R}$ such that $f'(q)=0$ for all $q\in\mathbb{Q}$, can we say that $f$ is constant?","['derivatives', 'real-analysis', 'calculus']"
2039071,Properties of The Number 137,"The number $137$ is a prime number.One of the permutation of $137$ is $173$, which is a prime number. The summation of $137$ digits is $11$ which is again a prime number! My question: Is there a name for this type of numbers? There are some other Properties for $137$ in Wikipedia . I found another Properties such that
$137=2^7+2^3+1$ or the only way to write the number $137$ as a summation of  two square numbers is $137=4^2+11^2$. thanks for your advice and suggestions. Edit: After reading comments and answers, I want to suggest a definition for such numbers like $137$. At first, I want to excuse me for this Venture that I want to make a definition. In the following definition, we suppose that the number one is a prime number. Definition: We call a prime number like $p$,  $\color{red}{\text{Sum and Permutation Prime}}$ of order $i\geq 1$ or in abbreviation SPP of order $i\geq 1$ if and only if The summation of digits of $p$ be prime. There is a number like $i\geq 1$, where all permutation of  the $p$ of length $i$ be prime numbers. If the last condition holds for $1\leq i \leq L(p)$, where $L(p)$ is the length of prime number $p$, we call the prime number $p$, SPP with full order.
The last condition is because of one of the answers that said all permutation of $137$ of length two are prime numbers. The number $113$ is a  SPP numbers with full order but $137$ is not SPP with full order because $371$,$713$ and $731$ are not prime numbers. In fact $137$ is a SPP number of orders $1$ and $2$. The SPP numbers of order ($\geq2$), do not have prime numbers $2$ and $5$ in their digits and just are consist of numbers $1$, $3$, $7$ and $9$. Thanks again for all nice comments and beautiful answers.","['number-theory', 'prime-numbers']"
2039091,Why do these two integrals use roots of reciprocal polynomials?,"There is the nice integral by V. Reshetnikov, $$\int_0^1\frac{dx}{\sqrt[3]x\ \sqrt[6]{1-x}\ \sqrt{1-x\,\alpha^2}}=\frac{1}{N}\,\frac{2\pi}{\sqrt{3}\;\alpha}\tag1$$ also discussed in this post . By direct analogy, we can consider its cousin, $$\int_0^1\frac{dx}{\sqrt[4]x\ \sqrt[4]{1-x}\ \sqrt{1-x\,\beta^2}}=\frac{1}{N}\,\frac{2\pi}{\sqrt{2}\;\beta}\tag{2a}$$ and the version investigated also by Reshetnikov, $$\int_0^1\frac{dx}{\sqrt[4]x\ \sqrt{1-x}\ \sqrt[4]{1-x\,\gamma^2}}=\frac{1}{N}\,\frac{2\pi}{\sqrt{2\gamma}}\tag{2b}$$ Given some integer/rational $N$ , it entails finding algebraic numbers $\alpha, \beta, \gamma$ such that, $$\begin{aligned}
\frac{1}{N}=I\left(\alpha^2;\ \tfrac12,\tfrac13\right)= \frac{B\left(\alpha^2;\ \tfrac12,\tfrac13\right)}{B\left(\tfrac12,\tfrac13\right)}
\end{aligned} $$ $$\begin{aligned}
\frac{1}{N}=I\left(\beta^2;\ \tfrac12,\tfrac14\right)= \frac{B\left(\beta^2;\ \tfrac12,\tfrac14\right)}{B\left(\tfrac12,\tfrac14\right)}\\
\end{aligned}$$ $$\begin{aligned}
\frac{1}{N}=I\left(\gamma^2;\ \tfrac14,\tfrac14\right)= \frac{B\left(\gamma^2;\ \tfrac14,\tfrac14\right)}{B\left(\tfrac14,\tfrac14\right)}\\
\end{aligned}$$ with regularized beta $I(z;a,b)$ , incomplete beta $B(z;a,b)$ , and beta function $B(a,b)$ . Solutions for $\alpha, \beta, \gamma$ for $N=2,3,5,7,11$ are known, for the latter two as, $$\begin{array}{|c|c|c|}
\hline
N & P(\beta)=0 &P(\gamma)=0 \\
\hline
2 & -4 + 4 \beta^2 + \beta^4 & -1 + 2 \gamma^2\\
3 & -2 - 2 \beta + \beta^2 & - 1 + 2 \gamma + 2 \gamma^2\\
5 & -4 + 8 \beta + 4 \beta^2 - 8 \beta^3 + \beta^4 & - 1 + 8 \gamma - 4 \gamma^2 - 8 \gamma^3 + 4 \gamma^4\\
\hline
\end{array}$$ with $N=7$ a $12$ -deg, and $N=11$ a $30$ -deg , both reciprocals. (Polynomials for $\alpha$ are given here .) Q: For $\beta$ and $\gamma$ , why are they roots of reciprocal polynomials when $N=3,5,7,11$ (and presumably others), but different polynomials when $N=2$ ? Ex. For $N=5$ and using $(2a)$ yields the succinct $\beta = 1-\tan\tfrac{3\pi}{20}$ hence, $$\int_0^1\frac{dx}{\sqrt[4]x\ \sqrt[4]{1-x}\ \sqrt{1-x\big(1-\tan\tfrac{3\pi}{20}\big)^2}}=\frac{\sqrt{2}\,\pi}{5\big(1-\tan\tfrac{3\pi}{20}\big)}$$","['polynomials', 'calculus', 'closed-form', 'integration', 'beta-function']"
2039143,Is the complex symmetric part of a $2\times2$ normal matrix always normal?,"I want to prove (or disprove) that, for a normal matrix $A\in\mathbb{C}^{2x2}$ (so that $AA^\ast=A^\ast A$), the sum $A+A^T$ is (or isn't) normal. I know that for $3\times3$ normal matrices, $A+A^T$ isn't normal, but for the $2\times2$ case, when I try to come with a counterexample I do not manage to make it.","['matrices', 'linear-algebra']"
2039204,Geometric solution to find a Set of totally ordered Subsets of $ \mathbb{N} $ is uncountable,"My question is, does there exist a set of subsets of the naturals, $S$, such that $S$ is uncountable, where for any two subsets $A,B \in S$ we have either $A \subset B $ or $B \subset A $? Initially my thoughts were that we could construct a chain $A_{1} \subset A_{2} \subset \cdots $ and find a bijection by picking out one element which is 'new' to each set by axiom of choice and so we have bijected to a subset of naturals, however I'm not convinced this fully works. Furthermore, I was thinking there may be a nice elegant solution by looking for such a set in either $ \mathbb{Q}^2 $ or $ \mathbb{Z}^2 $ for example, but any attempt seems to fail due to the 'ordering' part of the question.","['elementary-set-theory', 'elementary-number-theory']"
2039207,Convergence of $x_{n+2} = \sqrt{x_{n+1}} + \sqrt{x_{n}}$,"Let $x_0,x_1> 0$. Prove that the sequence defined by $x_{n+2} = \sqrt{x_{n+1}} + \sqrt{x_{n}}$ converges. Here's my solution: it's easy to prove by induction that $$\forall n, 0<\min(4,u_0,u_1)\leq u_n\leq \max(4,u_0,u_1)$$ The sequence is therefore bounded with $0<\liminf u_n \leq \limsup u_n<\infty$. By considering a subsequence that converges to $\limsup u_n$, you get two  limit points $l_1$ and $l_2$ such that $\limsup u_n = \sqrt{l_1}+\sqrt{l_2}$. Since $\limsup u_n$ is the greatest limit point, this yields $\limsup u_n\leq 2 \sqrt{\limsup u_n}$ which in turn implies $\limsup u_n \leq 4$. A similar reasoning proves $\liminf u_n \geq 4$, hence $$\liminf u_n = \limsup u_n = 4$$ and the sequence converges to $4$. Do you know any less advanced proof that an undergraduate might think of ? Preferably something that relies on monotony or an auxillary sequence.","['recurrence-relations', 'real-analysis', 'alternative-proof', 'convergence-divergence', 'sequences-and-series']"
2039233,Expectation of log of linear function of the Dirichlet distribution [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Given $\mathbf{X}\sim\mathsf{Dir}(\alpha_1,\cdots,\alpha_k)$, is there an expression for the expectation
$$ \mathbb{E}\left[ \log \left(\mathbf{c}^\top \mathbf{X} \right)\right] $$
where $\mathbf{c}\in\mathbb{R}_+^k$ is a vector of positive constants?","['statistics', 'integration', 'probability', 'expectation']"
2039262,Equivalent norms and rate of convergence,"Suppose that in a finite-dimensional normed space we have defined two (equivalent) norms $\|\cdot\|_1$ and $\|\cdot\|_2$. Is it true that if  a sequence $(x_n)_{n \geq 1}$ converges to some point $x$ at a rate $n$ in the norm $\|\cdot\|_1$ (in the sense that $\|x_n-x\|_1 \sim \frac{1}{n}$), then we also have the same rate of convergence for the second norm, i.e. $\|x_n-x\|_2 \sim \frac{C}{n}$ for some constant $C$ ? So far, I've been only able to deduce that
$$C_1 \leq \lim_{n \to \infty} n\|x_n-x\|_2 \leq C_2$$
for some positive constants $C_1$ and $C_2$ by using the definition of equivalent norms. How can I prove that the limit actually exists and equals some constant $C$ ? Perhaps it is obvious, but I don't see it !","['normed-spaces', 'convergence-divergence', 'limits']"
2039286,If $x+\frac{1}{x}=\frac{1+\sqrt{5}}{2}$ then $x^{2000}+\frac{1}{x^{2000}}= $?,If $x+\frac{1}{x}=\frac{1+\sqrt{5}}{2}$ then $$x^{2000}+\frac{1}{x^{2000}}=?$$ My try: $$\left(x^{1000}\right)^2+\left(\frac{1}{x^{1000}}\right)^2=\left(x^{1000}+\frac{1}{x^{1000}}\right)^2-2$$ Continuation ?,['algebra-precalculus']
2039356,Continuous function that equals 0 when 4 points form a square,"Given the distances between four points that lie in $\mathbb{R^2}: a,b,c,d,e,f$ Is there a continuous function $g(a,b,c,d,e,f) \Rightarrow \mathbb{R}$ such that $g = 0$ if and only if the four points form a square? I know that it is true that when they form a square, some set of four of the distances will be equal and the other two distances will be equal to each other. And I know that this formula: $$\frac{d_1 + d_2 + d_3 + d_4}{d_5 + d_6}\cdot \left( \frac{\sqrt 2}2 \right) - 1$$ Will equal zero when the four points form a square, but I don't think that formula being zero implies the other direction nor am I convinced that it is continuous (or at least able to be analytically continued into a continuous function.","['continuity', 'linear-algebra', 'geometry']"
2039374,Show given X and Y uncorrelated but not independent,"Let $(X,Y)$ with joint density
  $$p(x,y)=\begin{cases}
\pi^{-1}e^{-2x^{2}-y^{2}/2},  & xy \le 0 \\[2ex]
\pi^{-1}e^{-x^{2}/2-2y^{2}}, & xy \ge 0
\end{cases}$$
  Show that $X$ and $Y$ are uncorrelated but not independent. I have already seen easier counterexamples for this und wanted to do a similar approach, thus showing $\Bbb{E}[XY]=\Bbb{E}[X]\Bbb{E}[Y]$ and than $\Bbb{E}[X^{2}Y^{2}]\neq\Bbb{E}[X^{2}]\Bbb{E}[Y^{2}]$ given the structure of the distribution.
But I can't see how this should be possible and how I can derive $\Bbb{E}[X]$ in general from the given information. So I don't really know what I am supposed to show. Any help is appreciated.","['independence', 'probability-theory', 'probability-distributions']"
2039375,"Prove that if $f$ is uniformly continuous on $[a,b]$ and $f$ is uniformly continuous on $[b,c]$, then $f$ is uniformly continuous on $[a,c]$.","Prove that if $f$ is uniformly continuous on $[a,b]$ and $f$ is uniformly continuous on $[b,c]$, then $f$ is uniformly continuous on $[a,c]$. My attempt: Let $\epsilon >0$ be given. Since $f$ is uniformly continuous on $[a,b]$ there $\exists \delta_1 >0$ such that if $x,y \in [a,b]$ and $|x-y|<\delta_1$, then $|f(x)-f(y)|<\epsilon$. Since  $f$ is uniformly continuous on $[b,c]$ there $\exists \delta_2 >0$ such that if $x,y \in b,c]$ and $|x-y|<\delta_2$, then $|f(x)-f(y)|<\epsilon$. Now to show $f$ is continuous on $[a,c]$ how would i show this. Do i sort of add the two above relations?","['real-analysis', 'calculus', 'continuity', 'epsilon-delta', 'uniform-continuity']"
2039393,What is the value of $\prod_{n=1}^\infty (1+\frac{1}{n^2})$?,"How would I go about finding $\prod_{n=1}^\infty (1+\frac{1}{n^2})$ ? Setting $a_k = \prod_{n=1}^k (1+\frac{1}{n^2})$, the first few partial products are $a_1 = 2$, $a_1 = 2.5$, $a_3 \approx 2.78$, $a_4 \approx 2.591$, $a_{20} \approx 3.50$, and they eventually appear to converge at around $3.639$. (These values were found with a quick python program. So much for my initial conjecture that the product converged to $e$.) I tried writing $1+\frac{1}{n^2}$ as $\frac{n^2 + 1}{n^2}$, but it wasn't immediately obvious how to prove convergence / find the limit because the product doesn't really factor or telescope. Is there a way to calculate the limit?","['infinite-product', 'calculus', 'limits']"
2039411,identifying non isomorphic graphs with no comparison graph,"hi one of my homework problems is ""Draw all nonisomorphic simple graphs with four vertices"" my issue with this is that I thought isomorphism was used as a comparison, so how would you start to draw a graph that is non isomorphic?","['graph-theory', 'algebraic-graph-theory', 'discrete-mathematics']"
2039432,Reflexive fractional Sobolev spaces,"I have the following question: Let $(u_n)_n \in W^{s,1}(0,\pi)$, $s\in (0,1)$, be a uniformly bounded sequence with the $||.||_{W^{s,1}(0,\pi)}$ norm (with Gagliardo semi-norm). From the compact embedding $W^{s,1}(0,\pi)\subset\subset L^1(0,\pi)$ there exists a subsequence $(u_{n_k})$ that strongly converges to $u\in L^1(0,\pi)$. Is the limit function $u$ in $W^{s,1}(0,\pi)$? I can't find any reference that proves or disproves the reflexivity of this space (although I think it is not). Any help will be appreciated! Thank you in advance. EDIT: Taking a look at the book ""Theory of functions spaces"" (Hans Triebel, 1983), page 178, the Besov space $B^{s,1}$ is not reflexive, and since the space $B^{s,1}$ coincides with the space $W^{s,1}$, I cannot use reflexivity to prove the above problem (am I saying it wrong...? I know very little to nothing about Besov spaces).","['banach-spaces', 'functional-analysis', 'weak-convergence', 'calculus-of-variations', 'sobolev-spaces']"
2039440,A Differential Equation with Dirac Delta as the Non-homogeneity Term,"I'm currently studying Computational Mechanics, and an important step to solve problems is knowing how to express physical interactions in terms of mathematical equations. Consider for example, the problem of achieving the nodal displacement of a bar that is clamped on a wall, and there's a punctual force $P$ applied in the middle of its length. $\hspace{75pt}$ The differential equation that rules the system is: $$\frac{d^2u}{dx^2}=-\frac{P}{EA}\delta\left(x-L\right)$$ where: $u$ is the displacement; $E$ is the Young's modulus; $A$ is the sectional area of the bar; $L$ is half of the length of the bar; $\delta$ is the Dirac delta function. I intend to know if my solution of this problem is mathematically right: I just integrated two times the differential equation and got:
  $$
u(x)=
\begin{cases}
\hphantom{-}C_1x+D_1 &\text{if x}<L,\\[2ex]
\left(-\frac{P}{EA}+C_1\right)(x-L)+D_2&\text{if x}>L.
\end{cases}
$$
  where $C_1$, $D_1$ and $D_2$ are constants of integration.
  Now we just need to look for boundary conditions.
  The clamping condition gives: 
  $$u(0)=0$$
  The continuity of the bar gives: 
  $$u(L^-)=u(L^+)$$
  The clamping is on the left side of the bar, so the force $P$ will only be applied on the immediate left surroundings of $L$, i.e. $L^-$ ($P$ it's the complement action of the reaction on the wall). So,
  $$P=EA\frac{du}{dx}\bigg{|}_{(x=L^-)}$$
  with these conditions I got:
  $$
u(x)=
\begin{cases}
\hphantom{-}\frac{P}{EA}x &\text{if x}<L,\\[2ex]
\frac{P}{EA}L &\text{if x}>L.
\end{cases}
$$ My main difficulty in these kind of problems is how to deal with the Dirac delta functions, and how to deal with the boundary conditions in respect to the concentrated applied forces. This requires much more than pure mathematical analysis. I'm not 100% confident of what I did because I didn't compare the results with known literature. The bibliography of my course doesn't talk about the solution of differential equations of this kind. Do you know any good books that treat this type of problems?","['dirac-delta', 'distribution-theory', 'ordinary-differential-equations', 'finite-element-method']"
2039477,Cholesky decompostion: upper triangular or lower triangular?,"Can I find and use $U$ such that $$A = U U^{T}$$ where $U$ is an upper triangular matrix, to find a solution instead of finding $L$ such that $ A = L L^{T}$ (where $L$ is a lower triangular matrix) to solve $Ax=b$ using Cholesky factorization? If not, what is the correct way of using Cholesky factorization?","['matrices', 'numerical-linear-algebra', 'matrix-decomposition']"
2039484,Calculate: $\lim\limits_{n\to\infty} \sum_{k=0}^{n} \frac{2n+k}{n^2+(2n+k)^2}$,"Calculate: $$\lim\limits_{n\to\infty} \sum_{k=0}^{n} \frac{2n+k}{n^2+(2n+k)^2}$$ I thought a Riemann sum could lead to something, but couldn't find a suitable partition.
Hint, please?","['summation', 'limits']"
2039513,Let $W$ be a Wiener process. Which of the following define a Wiener process?,"From what I know so far, the process $W = \{W(t): t\geq 0\}$ is a Wiener process provided. (a) $W$ is time-homogeneous, i.e., $\{W(t+h)-W(h)\}$ has the same distribution as $W(t)$. (b) $W$ has independent increments, i.e., the increments $W(t_i) - W(t_i^*)$, $i \geq 1$ are independent whenever the intervals $(t_i,t_i^*]$ are disjoint. I found another definition that tell us that for any $t_0 \leq t_1 \leq \cdots \leq t_n$, the increments 
$$W(t_1)-W(t_0), \ldots, W(t_n)-W(t_{n-1}),$$
are independent. (c) $W(t)$ has normal distribution mean zero and variance $\sigma^2 t$ for some constant $\sigma^2$. Are the following Wiener Process:
  $$(1) -W(t), \quad (2)\sqrt{t}W(1), \quad (3)W(2t)-W(t).$$ I was given a hint that only (a) defines a Wiener process, but (b) and (c) doesn't have independent normally distributed increments. (1) Set $K = \{-W(t): t \geq 0\}$, then we have that $K$ is time-homogeneous. For instance,
$$K(t+h) - k(h) = -W(t+h) - (-W(h)) = - (W(t+h) - W(h)) = - W(t) = k(t),$$
so they have the same distribution. Now, $E(K(t)) = -E(W(t)) = 0$ and 
$$Var(K(t)) = Var(-W(t)) = Var(W(t)) = \sigma^2t,$$
as required. We proceed to show that $K$ has independent increment. We have that since $W(t)$ is a Wiener, for any disjoint interval $(t_i, t_i^*]$ ($i \geq 1$), the increments $W(t_i) - W(t_i^*)$ are independent. Thus, 
$$K(t_i) - K(t_i^*) = -W(t_i) - (-W(t_i^*)) = -(W(t_i) - W(t_i^*)),$$
so the increments $K(t_i) - K(t_i^*)$ are independent as required. Do I need to show that $K$ is normally distributed in order for $K$ to be a Weiner or this three conditions are sufficient? We have
$$F_K(x) = P(-W(t) \leq x) = P(W(t) \geq -x) = 1 - F_W(-x),$$
where $F_W$ represents the distribution function of $W$. Now, for part(2) and (3) I am not understanding how to show that the increments are not independent. (2) the only argument I can think is the following: let $0 \leq t_1 < t_2 \leq t_3 < t_4$, then for $\mathcal{O} = \{\sqrt{t}W(1): t \geq 0\},$ we have 
$$\mathcal{O}(t_2) - \mathcal{O}(t_1) = W(1)(\sqrt{t_2} - \sqrt{t_1}),$$ 
$$\mathcal{O}(t_4) - \mathcal{O}(t_3) = W(1)(\sqrt{t_4} - \sqrt{t_3}),$$
and it follows that 
$$[\mathcal{O}(t_4) - \mathcal{O}(t_3)](\sqrt{t_2} - \sqrt{t_1})= W(1)[(\sqrt{t_2} - \sqrt{t_1})](\sqrt{t_4} - \sqrt{t_3}),$$
which gives
$$[\mathcal{O}(t_4) - \mathcal{O}(t_3)](\sqrt{t_2} - \sqrt{t_1}) = [\mathcal{O}(t_2) - \mathcal{O}(t_1)](\sqrt{t_4} - \sqrt{t_3}).$$
Therefore, $\mathcal{O} = \{\sqrt{t}W(1): t \geq 0\}$ is not a Wiener. (3)Set $X = \{W(2t) - W(t): t \geq 0\}$ and consider $0 \leq t_1 < t_2 \leq t_3 < t_4$. Then, 
$$X(t_2) - X(t_2) = W(2t_2) - W(t_2) - (W(2t_1) - W(t_1)) = W(2t_2)-W(2t_1) - (W(t_2) -W(t_1)),$$
$$X(t_4) - X(t_3) = W(2t_4) - W(t_4) - (W(2t_3) - W(t_3)) = W(2t_4)-W(2t_3) - (W(t_4) -W(t_3)).$$
What I know is that $W$ is a Wiener, which means that $W(t_4) -W(t_3)$ and $W(t_2) -W(t_1)$ are independent. I am not sure how to proceed from the previous argument.","['proof-explanation', 'probability-theory', 'brownian-motion', 'probability-distributions']"
2039554,Birthday problem variation with non-uniform distribution?,"Consider a birthday problem : $N$ people have birthdays within year. The distribution of the birthdays is independent among people, not uniform (e.g., could be poisson), with some known CDF $G(x)$. Consider $q_i$ - a random variable indicating the number of people , who had birthdays before and on the date $i$ (where $i\in(1, 365)$). How to compute  the next day $q_{i+1}$ distribution (number of people who had birthdays on and before $i+1$), knowing the distribution of $q_i$ and the distribution $G(x)$?","['probability-theory', 'probability', 'probability-distributions']"
2039571,What does a colon (:) mean as a mathematical symbol in the context of mapping?,"I am trying to understand the principle of recursion: Let  ℕ  be the natural numbers. Let T be a set. Let a ∈ T. Let g: T → T be a mapping. $$\forall x \in N: f \left({x}\right) = \begin{cases} a & : x = 0 \\ g
> \left({f \left({n}\right)}\right) & : x = n + 1 \end{cases}$$ Wiki Could someone elaborate on what the ':' symbol implies? Judging from the reference at Wolfram I believe my concept of what exactly a mapping is may be faulty, especially in the latter half of the equality.","['recursion', 'elementary-set-theory', 'arithmetic']"
2039592,Showing n! is greater than n to the tenth power,"I'd like to show $n!>n^{10} $ for large enough n ( namely $ n \geq 15 $). By induction, I do not know how to proceed at this step: $$ (n+1)\cdot n!>(n+1)^{10}  $$ As I can't see how to simplify $(n+1)^{10} $. This seems like such a trivial thing (and it probably is), yet I can't do it. Isn't there an easier way to show this? (P.S. I need to refrain from the use of derivatives, integrals etc., I suppose, then you could work something out with the slope of the respective functions)","['factorial', 'real-analysis', 'inequality']"
2039654,Rigorous Understanding of Behavior around Equilibrium Points of this Dynamical System,"I'm starting Strogatz's Dynamical Systems and Chaos . At the beginning he wants to develop intuition towards the concepts in dynamical systems before jumping into rigorous theory. As such, he emphases graphical analysis as opposed to analysis. One of the first examples he gives is the simple one dimensional system $$\dot{x} = \sin x$$ The following question is asked: For an arbitrary initial condition $x_0$ what is the behavior of $x(t)$ as $t \to \infty$ ? First, lets turn to the graph of $\dot{x}$ . Strogatz's Answer Strogatz answers that the particle will asymptotically approach the nearest stable point on the graph; here's an example trajectory where $x_0=\frac{\pi}{2}$ While I intuitively understand why this would be so, being newly introduced to rigorous mathematics, I'm intellectually skeptical of myself if I think I understand an answer without first seeing some rigorous reasoning as to why it is so. I'm hoping someone can confirm my more rigorous solution or suggest what reasoning allows him to make his argument so confidently. My answer From the graph, we see that a particle dropped at any point $x_0$ will continue moving in a direction determined by $\text{sgn}(\dot{x}(x_0))$ . By the graph, the magnitude of the particle's velocity decreases as it nears a stable point of $\dot{x}(x)$ . Consider that after some time, the particle is sufficiently close to a point $\tilde{x}$ s.t. $\dot{x}(\tilde{x})=0$ and we can make a small angle approximation for velocity, $\dot{x}=\sin x\approx x'$ , where $x'$ shifts the function such that the origin is at $\tilde{x}$ . Now the derivative of velocity linearized at $\tilde{x}$ is $\dot{x}'=sgn{x'}x$ and solving this yields $x(t)=x'e^{-t}$ . It can trivially be shown that $x(t)$ converges to zero as $t \to \infty$ . $\square$ This answer is very heuristic – it only describes the magnitude of distance, not direction or sign and helps understand the qualitative nature of Fig 2.1.2 , but not rigorously. I'm looking for guidance as to make my argument more precise. Assume my formal background ends at Calc I and that I won't appreciate any advanced real analysis concepts. EDIT: I had originally said that the particle approaches the nearest zero of the graph, but I edited that to say stable point which is the correct limit of the system.","['ordinary-differential-equations', 'dynamical-systems', 'calculus']"
