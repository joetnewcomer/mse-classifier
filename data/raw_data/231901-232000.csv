question_id,title,body,tags
4823435,"In ODE $\frac{dy}{dx}$ is a derivative, what is $\ dx$ and $\ dy$ when separated?","In Schaum's Outline Of Theory And Problems Of Partial Differential Equations, equations like these are quite common: \begin{align}
a\,dy^2-2b \,dx\cdot dy+c\,dx^2=0\\
a^2 dt^2-dx^2=0
\end{align} I have not seen this way of specifying ODE's in the past and the syntax doesn't make a lot of sense to me. Multiplying and dividing by differentials is always a neat trick for the inverse function, but what does it actually mean to not divide afterwards? What does it mean to have an ODE in this differential form rather than the derivative $\frac{dy}{dx}$ form? Is there an advantage to this syntax? Can one solve ODEs directly in this differential form without first going through the derivative form?","['calculus', 'ordinary-differential-equations', 'partial-differential-equations']"
4823629,"Where does ""A family of compact sets with the finite intersection property has nonempty intersection"" lie between $T_1$ and $T_2$?","This question is a follow-up of sorts to a recent question by Steven Clontz about the property that (arbitrary) intersections of compact sets are compact (abbreviated "" $IKK$ "" in that question). I am interested in a stronger property ("" $KIP$ "" defined below), and where $KIP+T_1$ lies between $T_1$ and $T_2$ . I'll give definitions and some results, then conclude with my question and attempts. Definition. Throughout, we say a family of sets $\mathcal F$ has the finite intersection property ( $FIP$ ) if every finite intersection $F_1\cap\dots\cap F_n$ , with each $F_k\in \mathcal F$ , is nonempty. Let $X$ be a topological space.  We say $X$ has the compact intersection property (abbreviated "" $KIP$ "") if every family of compact subsets of $X$ with the $FIP$ has nonempty intersection. Below, we shall often abuse grammar and say things like "" $X$ is $KIP$ "" do indicate $X$ has the compact intersection property. It is immediate that any space in which compact subsets are closed ("" $KC$ "") must be $KIP$ .  In particular, this holds for all Hausdorff spaces. Consequences. Lemma 1. $KIP\implies IKK$ Proof. Let $X$ be $KIP$ , and suppose $\mathcal K$ is a family of compact sets.  We wish to show $\bigcap \mathcal K$ is compact. To this end, suppose $\mathcal C$ is a family of (relatively) closed subsets of $\bigcap \mathcal K$ , having the finite intersection property.  Let $\mathcal S$ be the family of all closed subsets $S\subseteq X$ such that $S\cap \bigcap\mathcal K\in \mathcal C$ .  Then for any finite collections $S_1,\dots,S_n\in \mathcal S$ and $K_1,\dots,K_n\in \mathcal K$ , we have $$\bigcap_{i=1}^n(K_i\cap S_i)\supseteq (S_1\cap\dots\cap S_n)\cap \bigcap \mathcal K\neq \emptyset,$$ since the latter intersection is a finite intersection of members of $\mathcal C$ .  Therefore the family $\mathcal K':=\{K\cap S\mid K\in\mathcal K,S\in\mathcal S\}$ is a family of compact sets with the $FIP$ , and thus by $KIP$ has nonempty intersection.  But then $$\emptyset\neq \bigcap \mathcal K'=\left(\bigcap \mathcal K\right)\cap \left(\bigcap \mathcal S\right)=\bigcap \mathcal C.$$ Thus every family of closed subsets of $\bigcap \mathcal K$ with the $FIP$ has nonempty intersection, hence $\bigcap \mathcal K$ is compact. Lemma 2. $KIP+T_1\implies US$ (""Convergent sequences have unique limits."") Proof. Suppose $X$ is $T_1$ , and not $US$ .  Let $x_n\in X$ be a sequence with $x_n\to x$ and $x_n\to y$ , $x\neq y$ .  Since $X$ is $T_1$ , $x_n$ must contain infinitely many distinct members, and so we may suppose with no loss of generality that the sequence $x_n$ consists of distinct elements not equal to $x$ or $y$ .  Letting $S_N=\{x_n\mid n\geq N\}\cup \{x\}$ and $T_N=\{x_n\mid n\geq N\}\cup\{y\}$ , we see that $\mathcal K:=\{S_N\mid N\in \mathbb N\}\cup\{T_N\mid N\in \mathbb N\}$ is a family of compact sets with the $FIP$ , yet $\bigcap \mathcal K=\emptyset$ , so $X$ is not $KIP$ . Equivalent formulation. A perhaps more tangible formulation of $KIP$ is the following: Lemma 3. $X$ is $KIP$ if and only if the following two conditions are satisfied: Pairwise intersections of compact subsets are compact ("" $IKK_2$ ""). Every family of nonempty compact subsets of $X$ , directed under reverse inclusion, has nonempty intersection (""Nonempty nested compact intersections‚Äù, or "" $NKI$ ""). Proof. If $X$ is $KIP$ then the first property follows from Lemma 1, and the second is immediate from the definition, since any family of nonempty subsets directed under reverse inclusion must have the $FIP$ . Conversely, if $X$ satisfies the preceding two properties, and $\mathcal K$ is a family of compact subsets of $X$ with the $FIP$ , then the family $\mathcal K'$ of finite intersections of members of $\mathcal K$ is a directed system of nonempty compact sets, and so $\bigcap \mathcal K=\bigcap\mathcal K'\neq \emptyset$ . In $ZFC$ . So far, everything we have said is valid in $ZF$ .  Now, even in $ZFC$ , a family of nonempty sets directed under reverse inclusion need not have a cofinal chain (consider, as a counter-example, the family of cofinite subsets of any uncountable set).  Nonetheless, in $ZFC$ we may replace the directed families in Lemma 3 with well-ordered chains: Lemma 4. In $ZFC$ , $X$ is $KIP$ if and only if the following conditions are satisfied: Pairwise intersections of compact subsets are compact. Every chain of nonempty compact subsets of $X$ , well-ordered under reverse inclusion, has nonempty intersection. Proof. From Lemma 3, $KIP$ spaces certainly satisfy the preceding properties.  Conversely, we suppose the preceding properties are satisfied, and argue by transfinite induction that for every cardinal $\kappa$ , and every family $\mathcal K$ of compact subsets of $X$ , having the $FIP$ , and cardinality at most $\kappa$ , $\bigcap \mathcal K$ is nonempty and compact. To see this, note the claim is true for finite cardinals.  If it is true for all cardinals $\kappa'<\kappa$ , and $\mathcal K$ has cardinality $\kappa$ , then let $\lambda\mapsto K_\lambda$ be a bijection $\kappa\to \mathcal K$ , and for each $\lambda<\kappa$ define $J_\lambda:=\bigcap_{\alpha\leq \lambda}K_\alpha$ .  Then by inductive assumption $\{J_\lambda\}$ is a well-ordered chain of nonempty compact subsets, so by the second property $\bigcap \mathcal K=\bigcap_{\lambda<\kappa} J_\lambda\neq \emptyset$ . Moreover, if $\mathcal C$ is a family of closed subsets of $\bigcap \mathcal K$ with the $FIP$ , then as in the proof of Lemma 1, we let $\mathcal S$ be the family of all closed subsets $S\subseteq X$ such that $S\cap \bigcap \mathcal K\in \mathcal C$ .  Then for each $\lambda<\kappa$ , and any finite collection $S_1,\dots,S_n\in\mathcal S$ , we have $$S_1\cap\dots\cap S_n\cap J_\lambda\supseteq S_1\cap\dots \cap S_n\cap \bigcap \mathcal K\neq \emptyset,$$ so the family $\mathcal S_\lambda:=\{S\cap J_\lambda\mid S\in \mathcal S\}$ has the finite intersection property.  Since $J_\lambda$ is compact, we have $J_\lambda\cap \bigcap \mathcal S=\bigcap \mathcal S_\lambda\neq \emptyset$ .  Therefore $\{J_\lambda\cap \bigcap \mathcal S\}$ is a well ordered chain of nonempty compact sets under reverse inclusion, and so $$\bigcap \mathcal C=\left(\bigcap \mathcal S\right)\cap \left(\bigcap \mathcal K\right)= \left(\bigcap \mathcal S\right)\cap \left(\bigcap_{\lambda<\kappa} J_\lambda \right)\neq \emptyset.$$ Since this holds for any family $\mathcal C$ of closed subsets of $\bigcap \mathcal K$ satisfying the $FIP$ , $\bigcap \mathcal K$ is compact. Question. In general, $KIP$ spaces need not be $T_1$ , and in fact it is easy to see that every finite topological space is $KIP$ .  Moreover, even in a $T_1$ space, the properties $IKK_2$ and $NKI$ from Lemma 3 are insufficient, separately, to guarantee $US$ :  If only $T_1$ and $IKK_2$ are assumed (or even $IKK$ itself), then the cofinite topology on an infinite set will give an example that is not $US$ .    If only $T_1$ and $NKI$ are assumed, $[0,\omega]$ with a doubled endpoint gives an example that fails $US$ . Therefore it seems $KIP+T_1$ spaces may be reasonable objects of study.  Where does this property fall among other properties between $T_1$ and $T_2$ ?  Particularly, those discussed in this answer ? Attempts. We know that $KC\implies KIP+T_1\implies US$ .  Moreover, the second implication is strict, as can be seen by the example of $[0,\omega_1]$ with a doubled endpoint , which is $US$ , but fails $IKK_2$ , hence fails $KI$ . I suspect the first implication is strict as well (it in fact is, see update), with a possible example in $X=\mathbb Q^*\times\mathbb Q^*$ , the square of the one-point compactification of the rationals . $X$ is not $KC$ (the diagonal being compact and not closed), but $X$ is $NKI$ (this follows from the fact that $\mathbb Q^*$ is $KC$ , hence $KIP$ , hence $NKI$ , and it is relatively straightforward to show that a product of two $T_1$ $NKI$ spaces is again $NKI$ ). However, I ran into trouble trying to determine whether $X$ is $IKK_2$ .  I expect there should be either a straightforward proof of this, or a straightforward counter-example, but both have so far eluded me. Update. I neglected to consider the one point compactification of the Arens-Fort space , discussed here .  Subsets there are compact if and only if they are either finite, or contain the point at infinity.  From this, $KIP$ is immediate, yet this space is not $KC$ or even weak Hausdorff , though it is $k_2$ -Hausdorff . Given that the implication $KC\implies KIP+T_1$ is indeed strict, it remains to show how $KIP+T_1$ relates to the aforementioned properties (i.e., does weak Hausdorff imply $KIP$ , and is there an implication in either direction for $k_2$ -Hausdorff?) or any other weak separation axioms. Addendum. Let me add one additional set of observations that I didn't mention originally.  They weren't particularly fruitful for me, but might be of some use that I didn't see: If $X=(X,\tau)$ is a topological space, let $\tau_{cc}\subseteq \mathcal P(X)$ consist of the co-compact subsets of $X$ , together with the empty set.  Then $\tau_{cc}$ is a topology if and only if $X$ is $IKK$ .  In this case $\tau_{cc}$ is always $T_1$ . $\tau_{cc}$ is a compact topology if and only if $X$ is $KIP$ . $\tau_{cc}\subseteq \tau$ if and only if $X$ is $KC$ . $\tau_{cc}\supseteq \tau$ if and only if $X$ is compact.","['separation-axioms', 'general-topology', 'compactness']"
4823753,Magical relationship between Exponential distribution and Poisson process,"Consider i.i.d. random variables $X_1,X_2,\ldots,X_n$ satisfying exponential distribution $\operatorname{Exp}(1)$ . Let $Y=X_1+X_2+\ldots+X_n$ . We know that the p.d.f. of $Y$ is the Gamma distribution $$
f_Y(t)=\frac{t^{n-1}}{(n-1)!}e^{-t}, (t\geq 0).
$$ What surprised me is that this is exactly the p.m.f. of a Poisson distribution with parameter $t$ . Define a Poisson process with rate $1$ , $N_t$ , then $\frac{t^{n-1}}{(n-1)!}e^{-t}$ equals $P(N_t=n-1)$ . Since we can define Poisson process with i.i.d. exponentials, we actually can write $$
P\left(\sum_{i=1}^{n-1}X_i\leq t, \sum_{i=1}^n X_i>t\right)= P(N_t=n-1)=\frac{t^{n-1}}{(n-1)!}e^{-t}.
$$ I believe there must exist a intuitive explanation of this relationship. But I couldn‚Äôt find it.","['statistics', 'equivalence-relations', 'stochastic-processes', 'gamma-distribution', 'probability']"
4823756,"Show that $f(a,b,c)=(a+b+c)^3+(a+b)^2+a$ is injective.","For a function $f : \mathbb{N} \times \mathbb{N} \times \mathbb{N} \rightarrow \mathbb{N}$ defined by $f(a,b,c)=(a+b+c)^3+(a+b)^2+a$ I want to show that $f$ is injective. How can I show this? I started by assuming $(a,b,c) \neq (d,e,f)$ and $f(a,b,c)=f(d,e,f)$ and seeking a contradiction. However, I am uncertain about the subsequent steps. Thank you.","['elementary-set-theory', 'functions', 'natural-numbers', 'analysis']"
4823846,Less than or equal on random variables,"Casella and Berger exercise 2.15 asks to prove ( $Y$ and $X$ are any random variables): $E[\max(X,Y)] = E[X] + E[Y] - E[\min(X,Y)]$ , in the solution it gives: Assume without loss of generality that $X \le Y$ . Then $\max(X, Y) = Y$ and $\min(X, Y) = X$ . Thus $X + Y = \max(X, Y) + \min(X, Y)$ . Taking expectations $E[X + Y] = E[\max(X,Y) + \min(X,Y)] = E[\max(X,Y)] + E[\min(X,Y)]$ .
Therefore $E[\max(X,Y)] = E[X] + E[Y] ‚àí E[\min(X, Y)]$ . How shall I think of $X \le Y$ ""without loss of generality"". Is this stochastic dominance (but then I would not think we can assume it without loss of generality)? Or does it mean something else?","['expected-value', 'statistics']"
4823865,Maximum value of linear combination of sinusoids,"Given a sequence of integers $a_1, a_2, \dots, a_n$ , is there a way to calculate the maximum value of the following function? $$f(\theta) = \sum_{j=1}^n a_j \cos(j \theta)$$ More specifically, I need to know whether $f(\theta)$ ever exceeds a given value $N$ . I expect this to be true for most of the sequences being tested. My approach so far has been to simply evaluate the function at 101 different points $\theta = 0, \frac\pi{100}, \dots, \frac{99\pi}{100}, \pi$ . The maximum result, $M$ , is a lower bound on the maximum value of $f$ , and I simply test whether $M \gt N$ . The benefit of this approach is that it can't give a false positive (if $M > N$ , then $f$ certainly exceeds $N$ at some point). But is there an alternative (with similar computational complexity) that could give a tighter lower-bound - and hence reduce the likelihood of a false negative ( $M <= N$ but $f$ still exceeds $N$ somewhere)? Edit: Newton's Method, as described below, is a definite improvement over the above approach. I tested using a set of 100,000 sequences of length 20, for all of which the function $f$ exceeds $N$ at some point. Evaluating $f$ at 100 points, as above, was sufficient to confirm that $N$ is exceeded in 99.6% of cases. Reducing the number of evaluations to 25 took only 30% as much time, but could only confirm 92.6%. Using 25 evaluations and Newton's Method, however, confirmed 99.7% in 50% of the time. Are there other methods, similar to Newton's or otherwise, that might improve things further? Perhaps something specific to sinusoidal functions?","['optimization', 'trigonometry', 'numerical-methods']"
4823901,Probability measures on the hypersphere $\Bbb S^{n-1}$ and the $O(n)$ isomorphisms,"Let $\Bbb S^{n-1}$ denotes the unit sphere in $\Bbb R^n$ and let $O(n)$ be the group of all orthogonal linear transformations on $\Bbb R^n$ . For any (outer) measure $\mu$ on $\Bbb R^n$ , the pushforward of $\mu$ by $T\in O(n)$ is defined as $T_{\#}\mu(E) := \mu(T^{-1}(E))$ . It is clear that the pushforward under $T\in O(n)$ maps a probability measure on $\Bbb S^{n-1}$ to another probability measure on $\Bbb S^{n-1}$ . Suppose that $f\in C(\Bbb S^{n-1})$ satisfies the condition $$
\int_{\Bbb S^{n-1}} f \,d\mu = \int_{\Bbb S^{n-1}} f \,dT_{\#}\mu \quad \text{for all } \ T\in O(n), \tag{*}\label{*}
$$ is it true that $$
\int_{\Bbb S^{n-1}} f \,d\mu = \frac{1}{\mathcal H^{n-1}(\Bbb S^{n-1})} \int_{\Bbb S^{n-1}} f \,d\mathcal H^{n-1} =: \bar f, \tag{**}\label{**}
$$ where $\mathcal H^{n-1}$ is the $n-1$ dimensional Hausdorff measure? Of course, the case where $\mu$ is a Dirac mass $\delta_x$ for some $x\in\Bbb S^{n-1}$ is clear, and, in fact, we can trivially conclude that $f$ must be a constant function. One might be tempted to conjecture that the condition $\eqref{*}$ implies that $f$ must be a constant function, but that is false . We can already find a counterexample to the stronger conjecture by considering $\mu$ to be a sum of Dirac masses: $\mu = \frac1n (\delta_{e_1} + \cdots + \delta_{e_n})$ and $f(x) = x\cdot Ax$ for any $n\times n$ matrix $A$ . Indeed, in this case we have $$
\int_{\Bbb S^{n-1}} f \,d\mu = \frac 1n \sum_{i=1}^n e_i\cdot Ae_i = \frac{\text{Tr}(A)}{n},
$$ and the statement that $\eqref{*}$ holds follows from the well-known fact that the trace of a matrix $A$ is invariant under a change of an orthogonal basis. Clearly, $f$ is not a constant function, but it is not hard to show that $$
\frac{\text{Tr}(A)}{n} = \frac{1}{\mathcal H^{n-1}(\Bbb S^{n-1})} \int_{\Bbb S^{n-1}} x\cdot Ax \,d\mathcal H^{n-1}(x)
$$ using the divergence theorem and that fact that $\text{Tr}(A) = \text{div}\, Ax$ . This question is largely inspired by the example regarding the trace that I mentioned above. I restrict $f$ in this question to be a continuous function since I feel like there might be a beautiful proof using the duality between continuous functions and Radon measures on $\Bbb S^{n-1}$ . I am aware of the existence of the things called measures on homogeneous spaces that generalize Haar measures, but aside from that, I know next to nothing about it. I welcome a proof that uses ideas from that theory, but would like to read a proof that doesn't use it as well, even if it might be longer and less elegant.","['measure-theory', 'fourier-analysis', 'lie-algebras', 'real-analysis', 'probability']"
4823986,"Does there exist a function $\mathbb{R}^2 \rightarrow \mathbb{R}$ whose directional derivatives are infinity in almost every direction, at all points?","For $\mathbf{x}\in \mathbb{R}^2$ and $\mathbf{r}\in S^1$ (here $S^1=\{\mathbf{r}\in \mathbb{R}^2: \|\mathbf{r}\|=1\}$ ), let us write $f'(\mathbf{x};\mathbf{r})=\infty$ if $\liminf\limits_{t\rightarrow 0}\frac{f(\mathbf{x}+t\mathbf{r})-f(\mathbf{x})}{t}=\infty$ . My question is the following: does there exist a (not necessarily measurable) function $f:\mathbb{R}^2\rightarrow \mathbb{R}$ , such that for all $\mathbf{x}\in \mathbb{R}^2$ , the set $$\{\mathbf{r}\in S^1: f'(\mathbf{x};\mathbf{r})=\infty\text{ or }f'(\mathbf{x};-\mathbf{r})=\infty\}$$ has full measure as a subset of $S^1$ ? Here $S^1$ is equipped with the ""arc length"" measure. Can a function $f:\mathbb{R} \rightarrow \mathbb{R}$ be ""infinitely steep"" on a set with non-zero Lebesgue outer measure? tells us that on any line parallell to $\mathbf{r}$ , the set of points $\mathbf{x}$ for which $f'(\mathbf{x};\mathbf{r})=\infty\text{ or }f'(\mathbf{x};-\mathbf{r})=\infty$ , is a null set. One is then tempted to argue that by Tonelli's, the set $$A=\{(\mathbf{r}, \mathbf{x})\in S^1\times \mathbb{R}^2: f'(\mathbf{x};\mathbf{r})=\infty\text{ or }f'(\mathbf{x};-\mathbf{r})=\infty\}$$ is a null set. On the other hand, Tonelli's seems to imply that when $\{\mathbf{r}\in S^1: f'(\mathbf{x};\mathbf{r})=\infty\text{ or }f'(\mathbf{x};-\mathbf{r})=\infty\}$ has full measure for all $\mathbf{x}\in \mathbb{R}^2$ , then $A$ has full measure, so there appears to be a contradiction. However, none of these applications of Tonelli are valid, since Tonelli only works when one integrates over a measurable set and a set may be non-measurable even if all of its sections are null sets. Indeed, there are non-measurable subsets of $\mathbb{R}^2$ for which all sections are singletons (see https://mathoverflow.net/questions/89375/sections-measure-zero-imply-set-is-measure-zero ). All of this is to say, I cannot straightforwardly deduce from the "" $1$ -dimensional case"", i.e. the first link, that that there cannot exist a function with the properties as in my question. My hope is that I have not overlooked some obvious argument here and that either the answer to my question is negative for some ""deeper"" reason, or, even better, that the answer is affirmative!","['measure-theory', 'derivatives', 'real-analysis']"
4823997,Unknown numbers of groups up to isomorphism of various prime powers,"Let $\mathrm{gnu}(n)$ denote the number of groups of order $n$ . I'm aware that we know $\mathrm{gnu}(2^{10})$ , but not $\mathrm{gnu}(2^{11})$ . Also, we know functions describing $\mathrm{gnu}(p^{1}), \dots, \mathrm{gnu}(p^{7})$ for $p$ prime in terms of PORC functions. This paper states $\mathrm{gnu}(3^{9})$ is known. I can't find anything on $\mathrm{gnu}(3^{10})$ . Presumably, this is unknown as of December 2023, is that correct? I can't find any sources for $\mathrm{gnu}(5^{8})$ or higher prime bases to the power of $8$ . Are these, as of December 2023, also unknown?","['group-theory', 'finite-groups', 'groups-enumeration']"
4824061,The sum $\sum \frac{1}{(i!)^2}$,"We know the sum: $$\sum\limits_{i=0}^\infty \frac{1}{(i!)} = e.$$ This makes me wonder about the sum: $$S = \sum\limits_{i=0}^\infty \frac{1}{(i!)^2}.$$ It's clear that it converges and also that it's less than $e$ since all the terms are less than the first summation. But is there a closed form for it? Numerically, it seems to be around $2.279$ . In general, one can define a function similar to the Riemann Zeta: $$S(u) = \sum\limits_{i=0}^\infty \frac{1}{(i!)^u}$$ This will converge for all $u>0$ . We know the closed form for $u=1$ . For large $u$ , it'll just become $2$ . But I wonder if any other $u$ has a closed form. And if we wanted to extend it to the complex numbers and analytically continue it, what would we get for: $$S(-1) = \sum\limits_{i=0}^\infty (i!)$$","['riemann-zeta', 'summation', 'bessel-functions', 'sequences-and-series']"
4824108,"The number of groups of order $2^n p$, $p$ an odd prime depends only on $p$ modulo $2^n$","Let $\mathrm{gnu}(n)$ denote the number of groups of order $n$ , and $p$ denote an odd prime. It's well known that $$\mathrm{gnu}(2p)=2,$$ and that $$\mathrm{gnu}(4p)= \begin{cases}5 & p \equiv 1 \mod 4 \\4 & p \equiv 3 \mod 4\end{cases}$$ with the one exception that $\mathrm{gnu}(4 \cdot 3) = 5$ . Looking through the data of the number of groups of order $8p$ , $16p$ , it looks to me that $$\mathrm{gnu}(8p)= \begin{cases}15 & p \equiv 1 \mod 8 \\12 & p \equiv 3,7 \mod 8\\14 & p \equiv 5 \mod 8\end{cases}$$ with the exceptions $\mathrm{gnu}(8 \cdot 3) = 15$ , $\mathrm{gnu}(8 \cdot 7) = 13$ , and $$\mathrm{gnu}(16p)= \begin{cases}54 & p \equiv 1 \mod 16 \\42 & p \equiv 3,7,11,15 \mod 16\\51 & p \equiv 5,13 \mod 16\\53 & p \equiv 9 \mod 16\end{cases}$$ with the exceptions $\mathrm{gnu}(16 \cdot 3) = 52$ , $\mathrm{gnu}(16 \cdot 5) = 52$ , $\mathrm{gnu}(16 \cdot 7) = 43$ , although I haven't attempted to prove either of these rigorously. Altogether, it looks to me like $\mathrm{gnu}(2^n \cdot p)$ depends on $p$ modulo $2^n$ , and takes on a constant value, other than a few small exceptions. Is this known to be true? If not, is a counterexample known? (Also, are my results for $\mathrm{gnu}(8 p)$ and $\mathrm{gnu}(16 p)$ actually correct?) Edit: If you're curious, I've just written a computer program to check through data on groups of order up to $50,000$ (see this ) to attempt to find patterns for $32p$ and $64p$ , and indeed it did. Here's a link to a pastebin , I won't write it all out here, to save making the question even longer. It couldn't really do $128p$ with confidence, since there's only one group of the relevant orders in each of the classes at that point.","['group-theory', 'finite-groups', 'groups-enumeration']"
4824174,What is the sum of an infinite resistor ladder with geometric progression?,"I am trying to solve for the equivalent resistance $R_{\infty}$ of an infinite resistor ladder network with geometric progression as in the image below, with the size of the resistors in each section double the size of the previous section. For those not familiar with the calculations for summing resistors, here is a quick recap: There are standard solutions in the literature for infinite resistor ladder networks without geometric progression such as the one below (where $R_x$ is simply R multiplied by a factor of x). The trick here is to replace all of the resistors to the right of the first two with a single resistor of value $R_y$ ... ... and realise that because the network is infinite, the first two resistors make no difference and so $R_{\infty} = R_y$ . The problem becomes finding the combined resistance of the two resistors in parallel $R_x$ and $R_y$ which by the rules of adding resistors is $\frac{R_y R_x}{R_y +R_x}$ and simply adding them to the resistor $R$ that is in series with the rest. We then get the equation: $R_{\infty} = \frac{R_y R_x}{R_y +R_x} + R \rightarrow \frac{R_{\infty} R_x}{R_{\infty} +R_x} + R$ and solve for $R_{\infty}$ using the quadratic solution and the result is: $R_{\infty}= \frac R 2 (1+\sqrt{4x +1} )$ For R=1 and x=1, the result is $(1+\sqrt{5})/2$ which interestingly enough is the golden ratio. For the original problem with the geometric progression, all the rungs are not the same and we can not use the standard trick. Doing it the hard way the first few steps of a small finite geometric ladder are: First rung = 2, First 2 rungs = 9/5 = 1.8, First 3 rungs = 41/23 $\approx 1.78260$ , First 4 rungs = 187/105 $\approx 1.78095$ , First 5 rungs = 853/479 $\approx 1.78079$ , which appears to be converging rapidly toward somewhere around 1.7. It is known by the properties of resistors that the result must be between $1<R_{\infty} <2$ After investigating the pattern for a while it emerges that there is a recursive relationship. If we represent the irreducible fraction for the resistance of each step as A'/B' and the previous step as A/B then the recursive relationship is $$\frac {A'}{B'} = \frac {4A+B}{2A+B}$$ . For example, the first step has a resistance of 2/1. The next step has a total resistance of $\frac{4 * 2 +1}{2 * 2 +1}  = \frac 9 5$ . The result always appears to be an irreducible fraction without requiring a reduction step. This is where I get stuck. It does not seem to be possible to find the limit as n goes to infinity for a recursive formula so I was wondering how to convert the recursive formula to a closed form? P.S. Should I remove the preamble where I recap the formulas for finding the equivalent resistance of resistor circuits? I added it because I assumed the mathematic experts here are not necessarily physics experts. P.P.S. Just for fun I wrote this short Python code to carry out the iteration of the recursive function and compare the result to the theoretical limit (given in the answers) of $(3+\sqrt{17})/4$ : import math

def iterate_formula(n):
    A, B = 2, 1
    for _ in range(n):
        A_prime = 4*A + B
        B_prime = 2*A + B
        A, B = A_prime, B_prime
        print(f""After iteration {_+1}, A' = {A}, B' = {B}, and A'/B' = {A/B:.80f}"")

# Call the function with the number of iterations you want
iterate_formula(16)
x=(3+math.sqrt(17))/4
print(""Compare:"")
print(f""Infinite Sum = {x:.80f}"") After just 15 iterations, Python (without special import modules) can not distinguish between the iterated result and the limit at infinity.","['recursive-algorithms', 'electromagnetism', 'physics', 'sequences-and-series', 'limits']"
4824183,solutions of two quadratic equations,"Suppose that $\alpha$ and $\alpha'$ are two negative real numbers.
Consider the following two equations (1) $b^2=b\alpha'+\alpha$ (2) $b^2\alpha'+b\alpha=1$ Find the number the solutions of the two equations with $b\in \mathbb C$ . The equation (2) implies that $b(b\alpha'+\alpha)=1$ .
And replacing  the equation (1) we have that $$b^3=1.$$ This implies that  the cube roots of unity are the only possible solutions. Note that $b=1$ is impossible, because $\alpha,\alpha'$ are negative. The cube roots of unity are they possible solutions or are there more possible solutions? How are these types of equations solved? Thanks you all.","['algebra-precalculus', 'systems-of-equations', 'quadratics']"
4824198,Recommendations for a Comprehensive Problem Book(s) in Calculus $3$.,"I am on the lookout for a comprehensive problem book specifically tailored to Multivariable Calculus and Vector Calculus (Calculus $3$ ), covering topics such as multiple integrals, partial differentiation, and other advanced concepts of Calculus $3$ . While I have found numerous resources for Calculus $1$ and $2$ , there seems to be a scarcity when it comes to Multivariable Calculus and Vector Calculus (Calculus $3$ ) Calculus $3$ . Could anyone recommend a problem book that provides a wide array of problems with varying difficulty levels in Multivariable Calculus and Vector Calculus (Calculus $3$ )? For context I have read:  Stewart's calculus, Thomas' calculus, Larson's Calculus, Baby Rudin and linear algebra by Hoffman and Kunze. EDIT: There are some good recommendations in the comments, but I am specifically looking for problem books that come with personal endorsements from individuals who have read and worked through them. Thank you in advance for your suggestions!","['book-recommendation', 'multivariable-calculus', 'calculus', 'vector-analysis', 'problem-solving']"
4824215,"Why is an (algebraic) loop group called a ""loop group""?","Let $G$ be an algebraic group over a field $k$ . Then we can define the loop group $LG$ to be the sheaf which takes a $k$ -algebra $R$ and spits out $G(R((t)))$ . My question is, why is this called the loop group? If one takes $k = \mathbb{C}$ , then is there a relation between this group and the ""topological"" loop group $\mathrm{Hom}(S^1, G(\mathbb{C}))$ ?","['loop-spaces', 'algebraic-geometry', 'representation-theory', 'algebraic-groups']"
4824226,Finding two extensions of this linear functional,"Let X := $\mathbb{C}^{3}$ equipped with the norm $|(x, y, z)|_{1} := |x| + |y| + |z|$ and $Y := \{(x, y, z) ‚àà X|x + y = 0, z = 0\}$ . Find at least two extensions of $‚Ñì(x, y, z) := x$ from $Y$ to $X$ which preserve the norm. What if we take $Y := \{(x, y, z) ‚àà X|x + y = 0\}$ ? My first approach (I am pretty unfamiliar with extensions of linear functionals, so I try to be as precise as possible): clearly our linear functional $l(x,y,z)$ is bounded since $|l(x,y,z)| = |x| \leq |x| + |y| + |z| = |(x,y,z)|_{1}$ . At this point, we can say that Hahn-Banach tells us that the existence of such an extension (which preserves the norm!) is guaranteed. So, we have $||l|| \leq 1$ . I am a little bit unsure about this point but by taking $y = z = 0$ , we see that we get equality and I think therefore we can conclude that $||l|| = 1$ (even though I am open to more elaborate suggestions about that). Now, my guess would be that $l(x,y,z) = y$ and $l(x,y,z) = z$ are two extensions of $l$ which also should preserve the norm. However, I am not really able to show that. Can anybody help me?",['functional-analysis']
4824243,Verifying change of variable in published work,"In the proof of Proposition A.11 on page 32 of this paper , the author takes the following step: $$
\int_U \exp(-\|D_Gu\|^2_2/2 - \omega \|D u\|^2/2)du
=
\text{det}(I+\omega D_G^{-1} D^2 D_G^{-1})^{-1/2}
\int_U 
\exp(-\|D_G w\|^2_2/2 )dw
$$ which they explain is by substituting $(I+\omega D_G^{-1} D^2 D_G^{-1})^{1/2}u$ for $w$ . To me this seems incorrect but given that it is central to the paper I am seeking a sanity check and likely missing something obvious. Note that the definition of $U$ is irrelevant to the question, $D_G, D$ are symmetric and positive definite matrices, and $\omega$ is some positive constant. Note that the exponent in the LHS can be written as $$
-\|D_G u\|^2_2/2 - \omega \|D u\|^2/2 
= -\frac{1}{2}u^T (D_G^2+ \omega D^2)u
= -\frac{1}{2}\|(D_G^2 + \omega D^2)^{1/2} u\|^2_2,
$$ and plugging in the proposed subsitution: $u = (I+\omega D_G^{-1} D^2 D_G^{-1})^{-1/2}w$ gives \begin{align*}
-\frac{1}{2}\|(D_G^2 + \omega D^2)^{1/2} u\|^2_2
&=
-\frac{1}{2}\|(D_G^2 + \omega D^2)^{1/2} (I+\omega D_G^{-1} D^2 D_G^{-1})^{-1/2} w\|^2_2\\
&=
-\frac{1}{2}\|D_G^{1/2}(I + \omega D_G^{-1}D^2D_G^{-1})^{1/2}D_G^{1/2} (I+\omega D_G^{-1} D^2 D_G^{-1})^{-1/2} w\|^2_2\\
&\neq 
-\frac{1}{2}\|D_G w\|^2_2.
\end{align*} It seems to me that the correct substitution ought to have been: $D_G^{1/2}(I+\omega D_G^{-1} D^2 D_G^{-1})^{1/2} u=w$ .","['integration', 'statistical-inference', 'statistics', 'change-of-variable', 'probability']"
4824296,"Given unimodular matrices $A, B$, solve the matrix equation $T^\top A T = B$","Given two symmetric integer unimodular matrices $A$ and $B$ with $\det A = \det B = \pm 1$ . How do we find any integer unimodular matrices $T$ such that $$
T^\top A T = B?
$$ Here $T^\top$ , denotes the transpose of $T$ . As an example, here are the data from the Mathematica Table: A = ({{2, -1, 0, 0, 0, 0, 0, 0, 0, 0},
      {-1, 2, -1, 0, 0, 0, 0, 0, 0, 0},
      {0, -1, 2, -1, 0, 0, 0, -1, 0, 0},
      {0, 0, -1, 2, -1, 0, 0, 0, 0, 0},
      {0, 0, 0, -1, 2, -1, 0, 0, 0, 0},
      {0, 0, 0, 0, -1, 2, -1, 0, 0, 0},
      {0, 0, 0, 0, 0, -1, 2, 0, 0, 0},
      {0, 0, -1, 0, 0, 0, 0, 2, 0, 0},
      {0, 0, 0, 0, 0, 0, 0, 0, 1, 0},
      {0, 0, 0, 0, 0, 0, 0, 0, 0, -1}    });
 
B = ({{1, 0, 0, 0, 0, 0, 0, 0, 0, 0},
      {0, 1, 0, 0, 0, 0, 0, 0, 0, 0},
      {0, 0, 1, 0, 0, 0, 0, 0, 0, 0},
      {0, 0, 0, 1, 0, 0, 0, 0, 0, 0},
      {0, 0, 0, 0, 1, 0, 0, 0, 0, 0},
      {0, 0, 0, 0, 0, 1, 0, 0, 0, 0},
      {0, 0, 0, 0, 0, 0, 1, 0, 0, 0},
      {0, 0, 0, 0, 0, 0, 0, 1, 0, 0},
      {0, 0, 0, 0, 0, 0, 0, 0, 1, 0},
      {0, 0, 0, 0, 0, 0, 0, 0, 0, -1}    }); In matrix form, $$A=
\left(
\begin{array}{cccccccccc}
 2 & -1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
 -1 & 2 & -1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
 0 & -1 & 2 & -1 & 0 & 0 & 0 & -1 & 0 & 0 \\
 0 & 0 & -1 & 2 & -1 & 0 & 0 & 0 & 0 & 0 \\
 0 & 0 & 0 & -1 & 2 & -1 & 0 & 0 & 0 & 0 \\
 0 & 0 & 0 & 0 & -1 & 2 & -1 & 0 & 0 & 0 \\
 0 & 0 & 0 & 0 & 0 & -1 & 2 & 0 & 0 & 0 \\
 0 & 0 & -1 & 0 & 0 & 0 & 0 & 2 & 0 & 0 \\
 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & -1 \\
\end{array}
\right)
$$ $$
B=\left(
\begin{array}{cccccccccc}
 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & -1 \\
\end{array}
\right)
$$ such that $\det A=\det B=-1$ here. Can you solve integer matrix $T$ with $T^\top A T = B$ ?","['unimodular-matrices', 'integer-lattices', 'matrices', 'matrix-equations', 'quadratics']"
4824313,Expected difference between largest and second largest of i.i.d. random variables,"Let $(X_i)_{i\geq 0}$ be i.i.d. nonnegative random variables with continuous density function $f$ . Let \begin{align}
\mu_n = \mathbb{E}[X_{(n)}-X_{(n-1)}]
\end{align} be the expected difference between the largest and second largest of the first $n$ random variables. Question : Can we show that $\mu_n$ is decreasing in $n$ ? We can assume the density $f$ to be decreasing for $x>0$ and the mean of the order statistics to be well-defined; in particular, assume $\mathbb{E}[X_i]<\infty$ . Edit: My intuition tells me that $\mu_n$ is generally decreasing in $n$ , but I have been unable to prove it. I am more interested in simple conditions under which the result is true, rather than a specific counterexample where it is not.","['stochastic-processes', 'order-statistics', 'probability-theory', 'probability']"
4824344,Prove that a particle moves in a plane,"Let's say $r \times v = const$ and $r \times a = 0$ . We should prove that the particle moves in a plane. I thought to approach it from a differential equation perspective, where we know from the second equation that $a=  r''  = kr$ , which means that $r(t) = c_1e^{kt}+c_2e^{-kt}$ . If $k$ would not be a scalar, we would get that $v$ is a scalar, so it seems wrong, and the only solution is of the form $r(t) = (t, c_1e^{kt}+c_2e^{-kt})$ . Is there a way to prove that with differential equations, or should I stick instead to a different method (e.g. like here )?","['solution-verification', 'linear-algebra', 'ordinary-differential-equations']"
4824356,Strong continuity of an automorphism group on a $C^\ast$-algebra (quantum dynamics),"Let $H$ be a Hilbert space and $T : H \supseteq \mathrm{dom}(T) \to H$ be a self-adjoint linear operator on $H$ . Define a one-parameter group $(\tau_t)_{t \in \mathbb{R}}$ of $\ast$ -automorphisms on the $C^\ast$ -algebra $\mathscr{B}(H)$ of bounded operators on $H$ by setting $$
\tau_t(A) = e^{i t T} A \, e^{- i t T}
$$ for all $A \in \mathscr{B}(H)$ , $t \in \mathbb{R}$ . I came across the following statement in some book: the group $(\tau_t)$ is strongly continuous on $\mathscr{B}(H)$ if and only if the operator $T$ is bounded on $H$ . Question: How can one prove this equivalence, especially the implication that strong continuity implies boundedness of $T$ ? Since $T$ is self-adjoint, it would suffice to show that $\mathrm{dom}(T) = H$ . I thought about mimicking the well-known proof of the fact that a strongly continuous one-parameter group of operators is uniformly continuous iff its generator is defined everywhere, but this did not work out as I hoped. Context of the question: If $T$ is interpreted as the Hamiltonian of a quantum system, then the group $(\tau_t)$ describes the time evolution of (bounded) observables of the system. Hence, the above statement implies that the quantum dynamical system $(\mathscr{B}(H), \tau_t)$ is a $C^\ast$ -dynamical system iff $T$ is bounded.","['operator-theory', 'functional-analysis', 'mathematical-physics', 'operator-algebras']"
4824467,Extracting the sequence generated by $\frac{1}{(1-x)(1-x^2)(1-x^3)}$ [duplicate],"This question already has answers here : The Number $p(n)$ of triplets $(x,y,z)$ : $x+2y+3z=n$ (2 answers) Closed 7 months ago . So I need the general formula for the sequence generated by the generating function $$
\frac{1}{(1-x)(1-x^2)(1-x^3)}.
$$ My idea was the decompose this into partial fractions and thus easily deduce the sequence, since the terms would be of the form $\frac{C}{(1-x)^n}$ , but I'm left with a term of the form $\frac{x+2}{9(x^2+x+1)}$ ... How should I proceed?","['generating-functions', 'combinatorics', 'partial-fractions', 'sequences-and-series']"
4824472,"Given collinear points $A$, $B$, $C$, a variable line through $C$ meets a conic at $P$ and $Q$. Why does $AP\cap BQ$ trace another conic?","At first, there are a fixed conic curve $\Gamma$ and three fixed collinear points $A,B,C$ . Draw a moving straight line $l$ through point C, intersecting $\Gamma$ at two points $P,Q$ . Find the two intersections $S=AP\cap BQ$ and $T=AQ\cap BP$ . As line $l$ move, $S$ and $T$ generate a locus $\Delta$ . Due to symmetry, $S$ and $T$ must have a common locus. I found that $\Delta$ will be a new conic curve. So the question is: why this is true? Use knowledge harmonic point sequences, the intersection of $ST$ and $AB$ is a fixed point $D$ that form a harmonic point sequences $[A,B;C,D]$ together with points $A,B,C$ . Suppose that $C$ is outside of $\Gamma$ . Make two tangent lines $CM,CN$ from $C$ to $\Gamma$ . Then the line $MN$ is the polar of $C$ in $\Gamma$ . Let $E$ be the intersection of $MN$ and $l$ . Through the image we can guess these two are right: $E$ is also the intersection of $MN$ and $ST$ . $DM,DN$ also tangent to $\Delta$ . The proof of the first guess is not too hard. Let $E'$ be the intersection of $ST$ and $l$ , then all that need to do is to prove $E=E'$ . According to the property of pole $C$ and polar $MN$ , $[C,E;P,Q]$ is a harmonic point sequences; according to the property of complete quadrangle $[A,B;P,Q;S,T]$ , $[C,E';P,Q]$ is also a harmonic point sequences; these two facts indecate $E=E'$ . The second guess suggest that $[C,\Gamma,P,Q]$ and $[D,\Delta,S,T]$ may be symmetric. This inspired me to turn to find that what's the relation between $\Gamma$ and $C,M,N$ . Or, if three points $C,M,N$ have been given, what conditions should a point $P$ meets in order to the locus of $P$ is a conic curve tangenting $CM,CN$ at $M,N$ . If this is finded, we can transfer the condition from $P$ or $Q$ to $S$ or $T$ , then $S$ or $T$ must meet the same condition and the locus of $S,T$ must be a conic curve tangenting $DM,DN$ at $M,N$ . The ratio $\dfrac{\overline{ME}}{\overline{EN}}$ uniquely determines the position of $E$ on $MN$ ; the position of $E$ determine the line $l$ ; and the other ratio $\dfrac{\overline{CP}}{\overline{PE}}$ also uniquely determines the position of $P$ on $l$ . So these two ratio must have a relation when $P$ moving on a fixed conic curve $\Gamma$ . By attempting on some special curve, I find this theorem: Theorem: Let $E$ be a moving point on $MN$ , and $P$ a point on $CE$ , then the locus of $P$ is a conic curve if and only if this equation holds: $$ \left(\frac{\overline{CP}}{\overline{PE}}\right)^2\frac{\overline{ME}\overline{EN}}{\overline{MN}^2}=\text{Const}$$ If the Theorem is true, by Menelaus's Theorem on triangle $\triangle CED$ and transversal line $PSA$ we know $\dfrac{\overline{CP}}{\overline{PE}}\times
\dfrac{\overline{ES}}{\overline{SD}}\times
\dfrac{\overline{DA}}{\overline{AC}}=-1$ .
So there is $\dfrac{\overline{DS}}{\overline{SE}}=
-\dfrac{\overline{DA}}{\overline{AC}}
\times\dfrac{\overline{CP}}{\overline{PE}}$ in which $-\dfrac{\overline{DA}}{\overline{AC}}$ is contant.Then $$
\left(\frac{\overline{DS}}{\overline{SE}}\right)^2
\frac{\overline{ME}\overline{EN}}{\overline{MN}^2}=
\left(\frac{\overline{DA}}{\overline{AC}}\right)^2
\left(\frac{\overline{CP}}{\overline{PE}}\right)^2
\frac{\overline{ME}\overline{EN}}{\overline{MN}^2}
=\text{Const}$$ and give what I want. Now I have these questions: How to prove the above theorem? How to explain the above theorem? How to prove the same problem when $C$ is inside of $\Gamma$ ?, excluding use the space $\mathbb{CP}^2$ . Is there another way to prove the problem Do I find this fact that collinear $A,B,C$ transform $\Gamma$ to a new conic curve $\Delta$ first? Is it been found by someone before?","['projective-geometry', 'conic-sections', 'geometry']"
4824490,Characterising groups of squarefree order.,"Is there any interesting characterisation for finite groups of order $p_1p_2 \cdots p_n$ , where $p_1,p_2, \cdots, p_n$ are distinct primes ? For the simplest possible case of one prime, it is clear that any such group is cyclic. For groups of order $pq$ where $p$ and $q$ are distinct primes with $p<q$ , we have two cases that classify all groups of this order using Sylow theorem. This involves a congruence relation namely $p \mid q-1$ or not. For groups or order $pqr$ , we have using Sylow theorem that, such groups cannot be simple. I would like to know more on the cases involving product of 4 or more primes. Does any congruence relation come into the picture while thinking about this idea in the general case? One trivial observation I can see is that by fundamental theorem, if a group of order $p_1p_2 \cdots p_n$ is abelian, then it is cyclic.","['group-theory', 'finite-groups']"
4824496,Calculate the angle between the lines,"I need to calculate the angle x. I feel like I'm missing something basic since this is supposed to be an easy exercise but I can't seem to get it. It's under the chapter trigonometry. AB is 8 units long, the radius is 5. I was looking for right angle triangles, but didn't find a way to make use of the info.",['trigonometry']
4824549,When does the concept of random variable first appear?,"I am writing an essay on the history of Probability Theory for a class project. While writing it, I wondered where the concept of random variable first appeared. Obviously not in its current form as a measurable function, which did not arrive until Kolmogorov's Grundbegriffe , in 1933; but rather the intuition behind it and its name. Reading the original paper from Bayes ( Essay Towards Solving a Problem in the Doctrine
of Chances , 1763), he does not seem to use the expression ""random variable"". However, when reading about the works of Laplace (which suceed those of Bayes by little time) from secondary sources (Todhunter's History of the Theory of Probability , Hald's A History of Mathematical Statistics From 1750 to 1930 , David's Games, Gods and Gambling ...), all of them seem to talk about variables or random variables , and I haven't found a place in those texts that dates the origin of the concept. In some texts the expression even appears when talking about Bayes, who did not use it in the first place. I would like to know when and in which text the expression ""random variable"" was introduced. Perhaps I'm in the wrong with respect to Bayes or the secondary sources? I have not read the texts with full detail, and I might have overlooked something.","['math-history', 'probability-theory', 'probability', 'random-variables']"
4824571,Generalisation of abelianisation using representation theory?,"Let $G$ be a finite group and let $k$ be an algebraically closed field of characteristic zero. Every $1$ -dimensional representation of $G$ over $k$ factors through $G^{\mathrm{ab}} = G/[G, G]$ , and every finite abelian group has a faithful representation over $k$ .
Taken together, these simple facts show that one can characterise the abelianisation $G^{\mathrm{ab}} = G/[G, G]$ of $G$ as the smallest quotient of $G$ such that every $1$ -dimensional representation of $G$ factors through it. Let $G_n$ be the smallest quotient of $G$ such that every $n$ -dimensional representation of $G$ over $k$ factors through $G_n$ . Clearly $G_1 = G^{\mathrm{ab}}$ and I've read that if $G$ has a generating set of size $m$ then $G_m = G$ , i.e. $G$ has an $m$ -dimensional faithful representation.
What does $G_2$ , or what do the $G_n$ look like in general? Can they be characterised in a different way? What do they tell us about $G$ ?","['group-theory', 'abstract-algebra', 'representation-theory']"
4824673,"Correctly Calculate $\sin(\mathrm{arc\,cot}(x)) $","I'm attempting to calculate $\sin(\mathrm{arc\,cot}(x))$ . I know that: $$\
\theta = \mathrm{arc\,cot}({\frac x1})
$$ where $\ x$ is adjacent to $\theta$ and $\ 1$ is opposite to $\theta$ . $$\
a^2 + b^2 = c^2
\to c = \sqrt{x^2 + 1^2} = \sqrt{x^2 + 1}
$$ Now plug into $\sin$ : $$\
\sin\theta={\frac{\mathrm{opp}}{\mathrm{hyp}}}
= {\frac {1}{\sqrt{x^2 + 1}}}
= {\frac {\sqrt{x^2+1}}{x^2+1}}
$$ $$\
\to \sin(\mathrm{arc\,cot}(x))={\frac {\sqrt{x^2+1}}{x^2+1}}
$$ However I have a table that lists the solution as: $$\
\sin(\mathrm{arc\,cot}(x))={\frac {1}{x \sqrt{{\frac{1}{x^2}}+1}}}
$$ My question is: how does one go from: $$\
{\frac {\sqrt{x^2+1}}{x^2+1}}
\to
{\frac {1}{x \sqrt{{\frac{1}{x^2}}+1}}}
$$ I'm quite sure the two expressions are equal, but I'm lacking the algebra skills to simplify. Wolfram provides the correct result, however symbolab does not simplify.","['algebra-precalculus', 'trigonometry']"
4824674,How to prove that $\operatorname{Card}(J) \leq \aleph_0=\operatorname{Card}(\mathbb{N})$.,"I have this problem Let the set $H=\left\{C_{(x, r)}=C((x, r),|r|)\right\}_{(x, r) \in \mathbb{R} \times \mathbb{R}^*}$ where $C(\mathbf{M}, r)$ denotes the circle with center M and radius r. Let the subset $\left\{C_j\right\}_{j \in J \subset \mathbb{R} \times \mathbb{R}^*}$ be such that $(j, i) \in J^2, j \neq i \Rightarrow C_j \cap C_i=\varnothing$ . Prove that $\operatorname{Card}(J) \leq \aleph_0=\operatorname{Card}(\mathbb{N})$ . .................. I suggest: The set H contains the circles $C(x,r)$ such that the center belongs to $\mathbb{R}$ and the the radius belongs to $\mathbb{R}^*$ . The subset $\left\{C_j\right\}_{j \in J \subset \mathbb{R} \times \mathbb{R}^*}$ of H, which is a set of circles that do not intersect with each other, i.e. $(j, i) \in J^2, j \neq i \Rightarrow C_j \cap C_i=\varnothing$ $\aleph_0$ is the set of infinite and well-ordered numbers. $\mathbb{N}$ is the set of countable natural numbers. Thus, we have to prove that the number of circles (that do not intersect each other) in the subset J must be ordered like the set $\aleph_0$ and countable like the set $\mathbb{N}$ . We can get the most posible number of circles ordered and countable (and do not intersect each other) when the centers of all of them are located in the center coordinates (0,0) and the radiuses of all of them  are graduated from lower value to the biggest value. is my new answer correct, please?",['elementary-set-theory']
4824678,Analog of Mahler's theorem over other non archimedean fields,"For any continuous function $f:\mathbb{Z}_p \to \mathbb{Q}_p$ , Mahler's theorem provides us with a relatively explicit series of polynomials converging uniformly to $f$ . Is there any analogue for other non archimedean fields? In particular, what about the completion $\mathbb{C}_p$ of the algebraic closure of $\mathbb{Q}_p$ ?","['number-theory', 'p-adic-number-theory', 'analysis', 'abstract-algebra', 'nonarchimedian-analysis']"
4824728,Probability of following the right person,"The following question was found in my textbook on probability. It is a question from the math olympiade final in Belgium, edition 1992 (actually the Flemish part of Belgium, the contest is called 'De Vlaamse Wiskunde Olympiade). I gave it a shot. Below is the question and my attempt at a solution. Through an informant in the underworld, the police have learned where a gang gathers. The identity of the gang members is unknown. An agent is tasked with shadowing the leader of the gang. He only knows that the gang leader is the tallest of five individuals, all of different heights. After the meeting, the gangsters leave the building one by one, leaving a quarter of an hour between each departure. The agent cannot see who the tallest person is and decides to let the first two go, then follow the first one who is taller than those who left before him. What is the chance that the agent shadows the right man? According to my book, the correct answer should be $13/30$ . Attempt at a solution: There are a total of $5! = 120$ possibilities. We will count the favorable possibilities. Note that if the leader leaves the building first or second, the agent can not follow the leader. Therefore, we need to consider the leader leaving third, fourth or fifth. If the leader is the third person to leave : the agent will follow him, no matter the order the other gangsters appear in. Number of possibilities: $4! = 24$ If the leader is the fifth person to leave : the second tallest gangster had to leave first or second in order for
the agent to follow the correct person. There are 3 remaining
possibilities for the other gangster leaving first or second. The
order of the remaining two gangsters is fixed: from largest to
smallest. Therefore, there are a total of $2\cdot 3\cdot 1 \cdot 1 =
   6$ possibilities to follow the leader if he appears last. If the leader is the fourth person to leave : We distinguish two cases: If the second tallest gangster appeared first or second, there is no problem. Number of possibilities: $2 \cdot 3! = 12$ . If the second talles gangster did not appear first or second, he needs to appear last. The third tallest gangster should then appear first or second, otherwise the agent will follow him. Therefore, there are $2\cdot 2\cdot 1 \cdot 1 = 4$ possibilities. This totals $24 + 6 + 12 + 4 = 46$ cases. Unfortunatly $46/120$ does not simplify to the desired answer. Question: What cases am I missing? I'm also interested to know other approaches to this problem. EDIT: as pointed out by Heropup and Bram28, I made a mistake in the part where the leader emerges last: the police only follows if a gangster is taller than all previous gangsters. Therefore, if the second tallest gangster left first or second, the police will follow the leader, no matter what order the other three gangster emerge. There are $2 \cdot 3! = 12$ possibilities instead of $6$ .}","['contest-math', 'recreational-mathematics', 'probability']"
4824755,Color regions generated by circles with one chord (Induction),"I am trying to learn Algorithms by reading the book Introduction to Algorithms: A Creative Approach by Udi Manber. I just want to solve this question: Prove (by induction) that regions formed by $n$ circles in the plane, each with one chord,can be colored with three colors such that any neighboring regions are colored differently. My guess is that when adding the new circle with its chord, we should start coloring the regions that have a neighbor colored. In other words, we should first color the regions with the lowest number of available colors. However, I cannot prove that in this way, we can color all regions.","['coloring', 'graph-theory', 'discrete-mathematics', 'algorithms', 'induction']"
4824806,Prove that there is a zero of a derivative $x_0$ such that $|p(x_0)| < \frac{2}{3} |x_0|$,"Let $p(x)$ be $a_5 x^5 + a_4 x^4 + a_3 x^3 + a_2 x^2 + x, a_5 \neq 0$ , all zeros of $p'$ are from $\mathbb{R}$ . I need to prove that there is $x_0$ a zero of a derivative such that $|p(x_0)| < \frac{2}{3} |x_0|$ . $$p'(x) = 5 a_5 x^4 + 4 a_4 x^3 + 3 a_3 x^2 + 2 a_2 x + 1$$ From here I don't know what to do. I've tried assuming the opposite, but it got me nowhere. It's the first time I've come across this type of exercise, how should I approach this?","['derivatives', 'polynomials']"
4824818,When do $\cos \theta$ and $\sin \theta$ exist over $\mathbb{F}_p$,"Let $p$ be a prime and $\theta \in \left[0, \frac{\pi}{2}\right]$ be a real number. Suppose $\cos \theta$ and $\sin \theta$ are algebraic over $\mathbb{Q}$ . When do they also exist over $\mathbb{F}_p$ , i.e. when do their $\mathbb{Q}$ -minimal polynomials have a root mod $p$ ? For instance, if $\theta = \frac{\pi}{3}$ , then these polynomials are $2x-1$ and $4x^2 - 3$ and both simultaneously have a root if and only if $p\geq 3$ and $3$ is a quadratic residue mod $p$ , i.e. if and only if $p\equiv \pm 1 \pmod {12}$ . Can this be nicely generalized?","['number-theory', 'finite-fields', 'modular-arithmetic', 'minimal-polynomials']"
4824840,"What is the ""extension class of a nilpotent group""?","My question is: what is the extension class of a nilpotent group? I'm trying to understand the proof sketch of Theorem 6.2 of Morita's paper ""A Linear Representation of the Mapping Class Group."" I don't have a link to the paper, but hopefully I give enough context below. I have a representation $$f: G \rightarrow (A \times B) \rtimes C$$ This representation restricts to $$f|_{H} : H \rightarrow A \times B$$ and projects to $$g: H \rightarrow A$$ Ultimately, the goal is to find $\text{ker}(g^*: H^2(H) \rightarrow H^2(A))$ . As a first step, Morita claims the extension class of the nilpotent group $A \times B$ can be expressed as a homomorphism $\wedge^2 B \rightarrow \wedge^2 A$ and that this is nontrivial. The rest of the proof relies on this fact. Can you help me understand what this means and maybe share some simple examples of what this tells us?","['representation-theory', 'abstract-algebra', 'group-cohomology', 'group-theory', 'homology-cohomology']"
4824850,Tricky Integral with parameter with difficult limits,"I had to calculate: $$\int_0^\infty\textrm{d}x\,\frac{1-\exp(-t)}{t}\cos(t)$$ My approach was to define a function with parameter: $$\mathcal{F}(a)=\int_0^\infty\textrm{d}x\,\frac{1-\exp(-a t)}{t}\cos(t):=\int_0^\infty\textrm{d}x\,f(x,a)$$ Then we can consider: $$\partial_af(x,a)=\exp(-a t) \cos(t)$$ Which is clearly dominated for $a\in[a_0,\infty[$ (for $a_0>0$ ) by a function: $\exp(-a_0t)$ . Then there is a theorem that states that if there exists some $a_1$ for which: $$\mathcal{F}(a_1)\; \textrm{is well defined} \quad \textrm{and}\quad \partial_af(x,a)\;\textrm{is dominated}$$ Then $\mathcal{F}(a)$ can be redefined to: $$\mathcal{F}(a)=\mathcal{F}(a_1)+\int_{a_1}^{a}\textrm{d}y\int_0^\infty\textrm{d}x\;\partial_yf(x,y)$$ Which is then well defined on $a\in[a_0,\infty[$ . Then by exact calculation: $$\int_0^\infty\textrm{d}x\;\exp(-a t) \cos(t)=\frac{a}{1+a^2}$$ And now integrating I finally got expression: $$\mathcal{F}(a)=\frac{1}{2}\log(1+a^2)+C$$ And here emerges the problem to find the value of $C$ . Then the final integral would be for $\mathcal{F}(1)$ . With Mathematica I got that $\mathcal{F}(1)=\frac{1}{2}\log(2)$ therefore $C=0$ and if someone could prove that: $$\lim_{a\to0}\int_0^\infty\textrm{d}x\;f(x,a)=\int_0^\infty\textrm{d}x\;\lim_{a\to0}f(x,a)$$ I would be very grateful, because then $\lim_{a\to0}\;f(x,a)=0$ and $C=0$ .","['limits', 'convergence-divergence', 'improper-integrals']"
4824892,How many numbers between 10 and 100 are divisible by 3 but not 2 nor 7?,"My working: Let $|M_3|$ denote the number of integers between 10 and 100 that divides 3. So we have $$|M_3|= \lfloor \frac{100}{3} \rfloor-3=30$$ , and similarly, $$|M_2|=\lfloor \frac{100}{2}\rfloor-4=46$$ , $$|M_7|=\lfloor \frac{100}{7}\rfloor-1=13.$$ Thus, we want to find $|M_3 \cap (M_2 \cup M_7)'|=$ . From a venn diagram, we deduce that this is equivalent to finding $$|M_3|-|M_3 \cap M_7| - |M_3 \cap M_2| + |M_3 \cap M_7 \cap M_2|=30-4-15+2=13.$$ But the tecaher said that this is incorrect, but close. After reviewing my solution, I am confused to as what I am getting wrong? Is it the interpretation of ""between"" 10 and 100? i.e., I minused away all integers $1‚â§n<10$ for each of the $M_i$ , i.e., the four integers $2,3,6,8$ are not counted in $|M_2|$ , but $10$ is.","['inclusion-exclusion', 'combinatorics', 'problem-solving', 'discrete-mathematics']"
4824900,Probability of at least one ace in a hand without using complementary event,"While going through the textbook, Discrete Math and its Applications , I found a problem, ""what is the probability that a 5-card poker hand contains at least one ace?"" It uses a standard deck of 52 cards with 4 aces. The solution given and that I have found on here uses a complement, which I understand. But my initial answer, which leads to a wrong value, was $$\frac{\binom{4}{1}\cdot\binom{51}{4}}{\binom{52}{5}}$$ I still can't figure out why this is wrong, which suggests I have a misunderstanding of combinatorics. I'll write my reasoning and would appreciate if someone can point out how it's flawed. I thought the size of the event in which our hand has at least one ace would be equal to the number of ways we can choose one ace, which is $\binom{4}{1}$ times the amount of ways we can choose 4 other cards from the remaining cards in the deck, which is $\binom{51}{4}$ . And then this goes over the sample space of $\binom{52}{5}$ . I would greatly appreciate an explanation of why this reasoning is flawed or resources that might help me understand this better.","['discrete-mathematics', 'combinatorics', 'card-games', 'probability']"
4824925,Why do cyclic inequality questions usually have three variables?,"This is probably more of a meta- or soft- question , but I thought I'd ask anyway. The question is the title: Why do nearly all posts involving cyclic sums involve exactly three variables? With only one or two variables, especially if the two variables have a constraint, perhaps the problem could be reduced to a single-variable calculus question. I guess I'm wondering if with four or more, aside from the obvious chore of writing out all the terms, if there is some inherent lack of symmetry or 'niceness' present in the three-variable case that often rules them out. As an analogy, which is somewhat of a stretch, the cross-product is really only used in three dimensions. It exists in $\mathbb{R}^7$ , and is rotationally invariant, but doesn't satisfy the Jacobi identity : there's something special about $\mathbb{R}^3$ specifically. I was wondering if there was a similar symmetry or property involving cyclic sums in $n$ variables that is only present when $n=3$ .","['contest-math', 'multivariable-calculus', 'optimization', 'inequality', 'soft-question']"
4824944,What are the eigenvalues of a tridiagonal Toeplitz matrix?,"Consider a matrix $M \in \mathbb{R}^{n \times n}$ in the form: $$ M = \begin{bmatrix} 
       \alpha & \beta  &  0     & \cdots & 0                 \\
       \gamma & \alpha & \beta  & \cdots & 0                 \\
        0     & \gamma & \alpha & \cdots & \vdots            \\
       \vdots & \vdots & \ddots & \ddots & \beta             \\
        0     &  0     & \cdots & \gamma & \operatorname{\alpha}
       \end{bmatrix} $$ Then a closed form expression for the eigenvalues is $$\mu_k = \alpha + 2 \beta \sqrt{\frac{\gamma}{\beta}} \cos \left( \frac{k \pi}{n+1} \right)$$ However, this also requires that $\beta \neq 0$ , since the corresponding eigenvectors contain a division by $\beta$ . Therefore, I am wondering if there exists a separate general formula for the case where $\beta = 0$ . This simplifies the above matrix $M$ to: $$
\begin{bmatrix} 
\alpha & 0 & 0 & \cdots & 0 \\
   \gamma & \alpha & 0 & \cdots & 0 \\
   0 & \gamma & \alpha & \cdots & \vdots \\
   \vdots  & \vdots  & \ddots & \ddots & 0 \\
   0 & 0 & \cdots & \gamma & \operatorname{\alpha}
 \end{bmatrix}$$ Of course, it is simple enough to compute this manually for smaller matrices, but I would be interested in knowing what the general solution is in this case. Note: the corresponding eigenvectors in the case where $\beta \neq 0$ are: $$v_k = \Big{(} \sqrt{ \frac{\gamma}{\beta}} \sin \big{(} \frac{ k \pi}{n+1} \big{)}, \big{(} \sqrt{ \frac{\gamma}{\beta}} \space \big{)}^2 \sin \big{(} \frac{ 2k \pi}{n+1} \big{)}, \cdots, \big{(} \sqrt{\frac{\gamma}{\beta}} \space \big{)}^n \sin \big{(} \frac{ nk \pi}{n+1} \big{)} \Big{)}^T$$","['eigenvalues-eigenvectors', 'toeplitz-matrices', 'matrices', 'linear-algebra', 'tridiagonal-matrices']"
4824953,"Coherent sheaves, Serre‚Äôs theorem and ext groups","Let $X$ be a smooth projective variety over an algebraically closed field $k$ (if necessary we assume that $\operatorname{ch}(k)=0$ ).
Let $O_X(1)$ be a very ample invertible sheaf on $X$ . Then, the category $\operatorname{Coh}(X)$ of coherent sheaves on $X$ is equivalent to the category $\operatorname{qgr}(A):= \operatorname{gr}(A)/{\operatorname{tor}(A)}$ , where $A:= \bigoplus_{i \geq 0} H^0(X,O_X(1))$ and $\operatorname{qgr}(A)$ is the Serre quotient category of the category $\operatorname{gr}(A)$ of finitely generated graded $A$ -modules by the category $\operatorname{tor}(A)$ of finitely generated torsion $A$ -modules. Then, we have an isomorphism \begin{align}
\operatorname{Hom}_{O_X}(\mathcal{F},\mathcal{G}) &= \lim_{n \to \infty} \operatorname{Hom}_{\operatorname{gr}(A)}(\Gamma_{*}(\mathcal{F})_{\geq n}, \Gamma_{*}(\mathcal{G})_{\geq n})\\
 &(= \operatorname{Hom}_{\operatorname{qgr}(A)}(\Gamma_{*}(\mathcal{F}),\Gamma_{*}(\mathcal{G}))),
\end{align} where $\mathcal{F},\mathcal{G} \in \operatorname{Coh}(X)$ and $\Gamma_{*}(\mathcal{F}):=\bigoplus_{i} H^0(X,\mathcal{F}(i))$ , $\Gamma_{*}(\mathcal{G}):=\bigoplus_{i} H^0(X,\mathcal{G}(i))$ . $\Gamma_{*}(\mathcal{F})_{\geq n}$ also means the truncation of $M$ at the degree $n$ .
Moreover, we have $$
\operatorname{Ext}^i_{O_X}(\mathcal{F},\mathcal{G}) = \lim_{n \to \infty} \operatorname{Ext}^i_{\operatorname{gr}(A)}(\Gamma_{*}(\mathcal{F})_{\geq n}, \Gamma_{*}(\mathcal{G})_{\geq n})
$$ according to Proposition 4.3.3 (a) in Ciocan-Fontanine and Kapranov - Derived Quot schemes . On the other hand, the functor $$
\Gamma_{*} : \operatorname{Coh}(X) \rightarrow  \operatorname{gr}(A)
$$ has a right adjoint $$
\tilde{-}:\operatorname{gr}(A) \rightarrow  \operatorname{Coh}(X),\  M \mapsto \tilde{M}.
$$ Then, for any $M \in \operatorname{gr}(A)$ , there exists $m \in \mathbb{N}$ , such that $\Gamma_{*}(\tilde{M})_{\geq m} \simeq M_{\geq m}$ in $\operatorname{gr}(A)$ . (Perhaps, the following relative theory is not necessarily in my questions.)
We also have relative statement (Section 3.1 of The derived moduli space of stable sheaves ) :
Let $S$ be a noetherian scheme. Then, we have a functor \begin{align*}
\Gamma_{*} : \operatorname{Coh}(X \times_k S)& \rightarrow  \{\text{the category of graded  coherent sheaves of $A \otimes_k \mathcal{O}_S$-modules} \} \\
&\hat{\mathcal{F}} \mapsto \oplus_i {\pi_{S}}_*(\hat{\mathcal{F}} \otimes \pi_{X}^*\mathcal{O}_X(i)).
\end{align*} There exists the right adjoint functor $$
\tilde{-} :  \{\text{the category of graded  coherent sheaves of $A \otimes_k \mathcal{O}_S$-modules} \} \rightarrow  \operatorname{Coh}(X \times_k S) .
$$ Questions :
Are the following true ? For each $\mathcal{F},\mathcal{G} \in \operatorname{Coh}(X)$ , and each $i \geq 0$ , there exists $m$ such that $$
\operatorname{Ext}^i_{O_X}(\mathcal{F},\mathcal{G}) = \operatorname{Ext}^i(\Gamma_{*}(\mathcal{F})_{\geq m}, \Gamma_{*}(\mathcal{G})_{\geq m}). 
$$ Let $S$ be a noetherian scheme over $k$ .
Let $\hat{\mathcal{F}},\hat{\mathcal{G}} \in \operatorname{Coh}(X \times_k S)$ be flat families over $S$ .
Then, there exists $m \geq 0$ such that for any $i \geq 0$ and any $s \in S$ , $$
\operatorname{Ext}^i_{O_X}(\hat{\mathcal{F}}_s,\hat{\mathcal{G}}_s) = \operatorname{Ext}^i_{\operatorname{gr}(A)}(\Gamma_{*}(\hat{\mathcal{F}}_s)_{\geq m}, \Gamma_{*}(\hat{\mathcal{G}}_s)_{\geq m}),
$$ where $\hat{\mathcal{F}}_s$ is the restriction of $\hat{\mathcal{F}}$ to the fiber $\{s\} \times_S (X \times_k S) \simeq X$ .
(In Prop 4.3.3 (b) of the above reference, a similar result appears, but I am not sure that it is true. At least, I think $m$ is dependent on $i$ in the question.) Let $S$ be a noetherian scheme.
Let $\mathcal{M}$ be a flat family of finitely generated graded $A$ -modules over $S$ (so that $\mathcal{M}$ is vector bundle on $S$ ).
Then, there exists $m \geq 0$ s.t. for any $s \in S$ , $$
\Gamma_{*}(\tilde{\mathcal{M}_s})_{\geq m} \simeq (\mathcal{M}_s)_{\geq m} \text{ as graded modules},
$$ where $\mathcal{M}_s$ is the  restriction of $\mathcal{M}$ to the fiber $\{s \} \times_S S \simeq \operatorname{Spec}(k)$ (so that $\mathcal{M}_s$ is a graded $A$ -module). Any  comment is welcome. (Please let me know if any conditions are missing.) The same question is in MO . Edit(2023,12/15) :
Some notations are changed. Question 3 is added.
I also added more information about the adjoint functor of $\Gamma_*$ , a relative theory of the Serre's theorem and question 2. Edit(2024,1/29) In Ciocan-Fontanine and Kapranov‚Äôs paper, they tried to construct derived quot schemes.
However, their construction is not correct.
Actually, they point out a problem(cf. derived quot stacks I , derived quot stacks II ).
In addition, the authors in ‚Äúderived quot stacks‚Äù gave the answer to the problem.
On the other hand, they did not mention our questions.","['coherent-sheaves', 'category-theory', 'algebraic-geometry', 'abstract-algebra', 'graded-modules']"
4824978,Given $A\in M_{2}(\mathbb Z)$ be such that $|A_{ij}(n)|\leq 50$ . prove $|A_{ij}(n)|\leq 50$ for all $n$.,"Let $A\in M_{2}(\mathbb Z)$ be such that $|A_{ij}(n)|\leq 50$ for all $1 \leq n \leq 10^{50}$ .
and all $1\leq i ,j\leq 2$ .where $A_{ij}(n)$ denotes the $(i,j)$ -th entry of the $2\times 2$ matrix $A^n$ .then prove that $|A_{ij}(n)|\leq 50$ for all positive integers n. I am looking for some hints on this question.","['matrices', 'linear-algebra']"
4825098,"If n independent trials are made from a discrete uniform distr. with parameter n, what is the probability that $(n+1)^\text{th}$ is the least of all?","My solution is that since the expected value of the discrete uniform distribution is $\frac{n+1}{2}$ , the probability that the outcome of the second trial is the least of any two trials is $\frac{1}{2}$ . Then the probability that the outcome of the $(n+1)^\text{th}$ trial being less than each of the outcomes of all the previous $n$ trials is $(\frac{1}{2})^n$ . But simulations of the experiment (in python) suggest that the correct answer is different. I have simulated the experiment for $n = 2 \text{ to } 100$ , $10000$ times each, and summarized the final probabilities for various $n's$ in this graph (with $n$ on the x-axis and $p$ on the y-axis). The closest formula I could find for these results is $\frac{1}{2n}$ , whose graph looks like this . This is the python code that I ran the simulations with: import numpy as np

def experiment(trials, n):
    count_min = 0
    
    for _ in range(trials):
        # Generating n random observations from a discrete uniform distribution
        observations = np.random.randint(1, n+1, size=n)  # Discrete uniform distribution from 1 to n
        
        # Finding the minimum value among the observations
        min_observation = np.min(observations)
        
        # Generating the next observation from the same discrete uniform distribution
        next_observation = np.random.randint(1, n+1)
        
        # Checking if the next observation is the minimum among the previous ones
        if next_observation < min_observation:
            count_min += 1
    
    probability = count_min / trials
    return probability

# Setting the number of trials
trials = 10000  # Number of trials
results = []
for n in range(3,100):
    results.append(experiment(trials, n))
results = np.array(results)
print(results) But how do you arrive at the right answer?","['statistics', 'probability-distributions', 'probability']"
4825143,Intuition behind Picard's Thorem requiring f to be Lipschitz in the *second* variable (rather than the first variable),"These are the statements of Picard's Theorem I've been taught Let $f:R\to\mathbb R$ be a function defined on the rectangle $R:= \{(x,y):|x-a|\leq h, |y-b|\leq k\}$ which satisfies f is continuous in R with bound M and $Mh\leq k$ f satisfies a Lipschitz condition in R
ie. $\exists L>0:|f(x,y_1)-f(x,y_2)| \leq L|y_1-y_2| \  \forall x\in [a-h,a+h], \forall y_1,y_2 \in [b-k,b+k]$ Then the IVP $y'(x)=f(x,y(x))$ with $y(a)=b$ has a unique solution y on the interval [a-h,a+h] Suppose that $f:R\to\mathbb R$ is a continuous function on an unbounded rectangle $R = [c,d] \times \mathbb{R}$ which satisfies a global Lipschitz condition on R ie. $\exists L>0:|f(x,y_1)-f(x,y_2)| \leq L|y_1-y_2| \  \forall x\in [a-h,a+h], \forall y_1,y_2 \in \mathbb R$ . Then for any $a\in[c,d]$ and any $b\in\mathbb R$ the IVP has a unique solution that is defined on all of [c,d] As in the subject, I don't think I understand the intuition about why the Lipschitz condition is placed on the second variable rather than the first. I can follow the algebra but don't fully understand why it works. Further to this, in the global existence version of Picard's theorem, why do we look for a global Lipschitz condition in the second variable when we want a global solution for all x","['derivatives', 'ordinary-differential-equations']"
4825157,A doubt on invertibility of a bounded linear operator,"Let $C[0,1]$ be the Banach space of real valued continuous functions on $[0,1]$ equipped with the supremum norm. Define $T: C[0,1] \to C[0,1] $ by $$T(f(x))=\int_{0}^{x}xf(t)dt.$$ Let $R(T)$ denote the range of $T.$ Consider the following statements. P: $T$ is a bounded linear operator. Q: $T^{-1}:R(T) \to C[0,1]$ exists and is bounded. My attempt: For any $f \in C[0,1],$ we have that $$ || T(f(x))||=\sup_{x \in [0,1]} \bigg| \int_{0}^{x}xf(t) dt\bigg| \leq \sup_{x \in [0,1]} |f(t)| = ||f||.$$ The inequality in the above equation is because we are dealing with $x \in [0,1].$ So, $T$ is a bounded linear operator. So, statement P is TRUE. As for Q, I shall quote a result. Let $T:X \to Y$ be a linear operator between two normed spaces $X,Y,$ and let $T$ be onto. Then, $T^{-1}$ exits and is continuous if and only if there exists $c>0$ such that $c||x|| \leq ||T(x)||$ for each $x \in X.$ One can easily see that $||T|| \leq 1,$ with equality achieved for $f(x)=1.$ Now, set $X=C[0,1],Y=R(T).$ For, $c=1,$ we have satisfied all properties of the above quoted result. So, $T^{-1}$ exits and it is continuous and consequently bounded.
So, statement Q is also TRUE. But the answer key says that P is TRUE and Q is FALSE. I am confused as I think I have applied things correctly.
Please help me.",['functional-analysis']
4825174,Compute $\int_{0}^{e}\frac{\ln(1-W(x))}{x}dx$,"So I made this integral for funzies, and I'm having trouble solving it. $$\int_0^e\frac{\ln(1-W(x))}x dx$$ I may have made a mistake. I let $u=W(x)$ , then the integral becomes $$\int_{0}^{1}\frac{(u+1)\ln(1-u)}{u}du$$ I used Taylor Series after, but then I got a weird sum.
Anyone else wanna try?","['integration', 'definite-integrals', 'lambert-w']"
4825182,"Integration by substitution, but from sphere to sphere","Suppose we have a smooth injection $\Phi(x,y)$ from an open set $\Omega \subset \mathbb{R}^2$ to $S = \Phi(\Omega) \subset \mathbb{R}^3$ . It is well-known that the integration by substitution formula holds: $$
\int_{S} f(x,y,z) \, \text{d}\sigma(x,y,z) = \int_{\Omega} f(\Phi(x,y)) \, J(x,y) \, \text{d}x \, \text{d}y
$$ where $J(x,y)$ denotes the Jacobian. Recently, I have been working on projects related to integration on a sphere. For example, consider a smooth bijection $\Phi(\omega)$ from $S^2$ to $S^2$ , the sphere in 3-dimensional space. I am curious if there exists a formula like: $$
\int_{S^2} f(\omega) \, \text{d}\sigma(\omega) = \int_{S^2} f(\Phi(\omega)) \, T(\omega) \, \text{d}\omega
$$ where $T(\omega)$ is only determined by the transition function $\Phi$ . I tried to use the Jacobian, but it is evidently incorrect because $T$ is not even differentiable on the sphere in 3-dimensional space (it is only defined on the sphere!). I read some papers and found something like: $$
\int_{S^2} f(\omega) \, \text{d}\sigma(\omega) = \int_{S^2} f(\Phi(\omega)) \, ||DT_\omega(\vec s)\times DT_\omega(\vec t)|| \, \text{d}\omega
$$ where(copied from paper): $\vec s$ and $\vec t$ are orthonormal tangent vectors of the unit
sphere at $\omega$ and the $DT_{\omega}$ is the 'differential' of $T$ with respect to the vector $\omega$ , The norm of the cross product of
transformed tangent vectors accounts for the distortion in the
integration domain, similar to the Jacobian determinant for a change
of variables in ambient space. But it doesn't provide any precise definition. I don't know how to define the 'differential' of $T$ w.r.t. $\omega$ . I would be more than grateful if you could provide any clues, thank you! Update: I found something like: $$\int_\mathcal{S^2}{f(\omega)}\text d\sigma(\omega)=\int_\mathcal{S^2}{f(\Phi(\omega))\frac{|\omega^TJ^*(\omega)F(\omega)|}{|F(\omega)|^3}}\text d\sigma(\omega)$$ where $\Phi((x,y,z)^T)=\frac{F((x,y,z)^T)}{|F((x,y,z)^T)|}$ , $\Phi$ is bijection from sphere to sphere, $F$ is smooth and non-zero on the sphere. $J$ denotes the Jacobian of $F$ and $J^*$ is the adjugate matrix.","['field-theory', 'calculus', 'analysis', 'differential-geometry']"
4825188,Understanding Arzela-Ascoli theorem edition 2.,"This is a question related to the post How to understand Arzela-Ascoli theorem? In that post the notion of bounded as a set is explained in a bit more detail and the requirements of Arzela-Ascoli theorem delved into as well. Recalling the definition of uniformly bounded, we have A set $M \subseteq C(X, \mathbb{R})$ is uniformly bounded if there exists $L > 0$ such that for all $f \in M$ if we have $sum_{x \in X} |f(x)| \le L$ . In the answer to the post the author @levap prefaces it saying, ""The conditions of Arzela-Ascoli are ""uniformly bounded"" and ""equicontinuous"" and not ""each function is bounded"" and ""each function is uniformly continuous"". The prefixes ""uniformly"" and ""equi"" suggest that those are conditions that are relevant for a set $M$ of functions and cannot be checked for each function $f \in M$ separately."" I thought that the set $M$ has to satisfy this fact that there exists a single constant $L$ that bounds all the functions in the set. In the comments to clarify @levap gives a wonderful example $M = \{nx | n \in \mathbb{N}\}$ . He again reiterates his preface of not being able to check for each individual function. This is contrarian to my understanding. I was under the impression that there has to be a single constant that bounds all these functions and does so individually. So in my opinion the example that the author of the answer gave did not and does not satisfy the requirement of uniform boundedness. On the other hand, is the author saying we can do with a weaker condition than the stronger uniform boundedness condition that I have in mind?","['complex-analysis', 'functional-analysis', 'real-analysis']"
4825195,An idea for this difficult integral: $\int_{-\tfrac{1}{2}}^{\tfrac{1}{2}}\dfrac{\arccos x}{\sqrt{1-x^2}(1+e^{-x})}dx$.,"I am being stuck in caculating this integral: $$J=\int_{-\tfrac{1}{2}}^{\tfrac{1}{2}}\dfrac{\arccos x}{\sqrt{1-x^2}(1+e^{-x})}dx$$ I tried to change to another variable: $x = - t$ then $dx = - dt$ , also I got: $$J=\int_{-\tfrac{1}{2}}^{\tfrac{1}{2}}\dfrac{\arccos(-t)}{\sqrt{1-t^2}(1+e^t)}dt=\int_{-\tfrac{1}{2}}^{\tfrac{1}{2}}\dfrac{\arccos(-x)}{\sqrt{1-x^2}(1+e^x)}dx.$$ Therefore $$2J=\int_{-\tfrac{1}{2}}^{\tfrac{1}{2}}\left[\dfrac{\arccos x}{\sqrt{1-x^2}(1+e^{-x})}+\dfrac{\arccos(-x)}{\sqrt{1-x^2}(1+e^x)}\right]dx=\int_{-\tfrac{1}{2}}^{\tfrac{1}{2}}\dfrac{e^x\arccos x+\arccos(-x)}{\sqrt{1-x^2}(1+e^x)}dx$$ The numerator looks very complex and I really do not know how to do next. Can you guys give me some ideas ?","['integration', 'inverse-function', 'real-analysis', 'calculus', 'trigonometric-integrals']"
4825212,How to solve this indeterminate form?,"Let $a_1(n)=n$ , $a_k(n)=2+\sqrt{a_{k-1}(n)},k\ge2$ , then can we determine the limit of the following indeterminate form $$
\lim_{k\to+\infty}\frac{\sqrt{2-\sqrt{a_k(2)}}}{\sqrt{2-\sqrt{a_k(3)}}}.
$$ I have no idea to solve it, and I tried to calculate it in Matlab, and the answer maybe is $\frac{3}{2}$ .","['limits', 'analysis']"
4825222,Using L'hospitals rule when right hand limit and left hand limit are different,"Is it possible to use L'hospital rule for a function whose left hand limit and right hand limit are different. For example in the question $\lim_{x\rightarrow0} \frac{e^{1/x}-1}{e^{1/x}+1}$ The Left hand limit is equal to -1 while the right hand limit is equal to 1.
However using l'hospitals rule gives $$\lim_{x\rightarrow0} \frac{e^{1/x}-1}{e^{1/x}+1}=\lim_{x\rightarrow0} \frac{-e^{1/x}/x¬≤}{-e^{1/x}/x¬≤}=1$$ Why does the rule gives the value of the right hand limit and not of the left hand limit. Is there any utility in using the l'hospitals rule for a evaluating a limit that does not exist (left hand limit and right hand limit are different)","['limits', 'calculus', 'derivatives']"
4825276,"If two pairs of numbers have equal sums and products, what can we conclude about the pairs?","If two pairs of non-zero real numbers $\{a,b\},\{c,d\}$ have the property that the sum of the first pair equals the sum of the second pair, and the product of the first pair equals the product of the second pair, what can we conclude about the numbers? I believe both pairs must be identical, but am struggling to prove this. I've deleted my failed attempts to prove this from the OP; if you'd like to see them, they're in the history.  Below are the successful proofs, along with how we reached them Based on ""ancient mathematician's"" solution, I asked myself ""How could I come up with that?"" and worked out the following.  I request feedback on how someone could come up with any of these solutions given that the direct approach above did not work. Inspiration, Approach, Solution Intuitively, we suspect that the pairs are equal, since if $c = ar$ , then $b = a/r$ , implying that the $\Delta a$ by multiplying $a$ by $r$ is negative the $\Delta b$ by dividing by $r$ , which suggests $r = 1$ .  However, attempting to formalize this  proves difficult.  Why? Because in fact it's not correct to conclude that $a = c$ , only that $\{a,b\}  =\{c,d\}$ . It is the pairs that are equal, not the numbers .  And equations are about numbers, not pairs! How can we overcome this? One way is to ask ""What object does respect pairs?""  And the obvious answer is: A polynomial of degree 2.  That is, if $f$ and $g$ are polynomials of degree 2 with  identical pairs of roots, then $f = g$ . So we need to form a polynomial with roots $a,b$ .   And remembering ""Math has a tendency to reward you if you respect its symmetries"" , we form a second polynomial $g$ , aiming to show equality: $$f(x) = (x-a)(x-b)\\
g(x) = (x-c)(x-d).$$ Now multiply out $$f(x) = x^2 - (a+b)x +ab \\
g(x) = x^2 - (c +d)x + cd$$ and the result is trivial. Alternately... A similar path, based on ""user's"" solution: We need to prove $a = c \lor a = d$ , which can only be encoded via a 2nd degree polynomial: $(a-c)(a-d) = 0$ .  Working backwards temporarily: $$(a-c)(a-d) = 0\\
a^2 -a(c+d) +cd = 0\\
a^2 - a(a+b) + ab = 0\\
0 = 0.$$ Now let's work forwards: $$a^2 - a(a+b) + ab = 0\\
a^2 -a(c+d) +cd = 0\\
(a-c)(a-d) = 0\\
a = c \lor a = d$$ as desired.","['contest-math', 'algebra-precalculus']"
4825280,"Is $A \vert B$ an event, or is $P(A \vert B)$ an abuse of notation?","Suppose we have a probability space $(\Omega, \mathcal{F}, P)$ .
Thus, when we write $P(A \vert B)$ , either $A \vert B$ is an element of the event space $\mathcal{F}$ (i.e. the domain of $P$ ), or this an abuse of notation.
I suspect that this is simply an abuse of notation, but I'm looking for two things here: Am I interpreting (that is ""unabusing"") this notation correctly? Is it possible to view $A \vert B$ as an event, even if this isn't the standard way of thinking about it? Notational Abuse? Entertaining the second possibility (notational ""abuse"") I can easily imagine how to ""desugar"" this notation into something more formal.
For instance, we could read $P(A \vert B)$ as $P_B(A)$ , where $P_B$ is a probability measure on a new/derived probability space $(B, \mathcal{F}_B, P_B)$ .
That is, this new space has $B$ as its sample space and a different event space derived from our original event space.
We can then define $P_B(A)$ in terms of our original probability measure $P$ as: $P_B(A) = \frac{P(A \cap B)}{P(B)}$ .
I haven't yet worked out the details (e.g. What is $\mathcal{F}_B$ ? , Is $P_B$ a probability measure? , etc.), but I suspect they're not too difficult. Conditional Events? But, is it possible to view $A \mid B$ as an event (that is, some element of $\mathcal{F}$ )? This section of the Wikipedia article on Conditional Probability states that Conditional probability can be defined as the probability of a conditional event $A_B$ . it then proceeds to define the ""Goodman‚ÄìNguyen‚ÄìVan Fraassen conditional event"", but I haven't been able to decipher this yet. Possible Counterexample However, there appear to be simple examples in which $A \mid B$ just can't be an event.
For instance, suppose we have a sample space $\Omega = \{ TT, TH, HT, HH \}$ and we define a probability measure using the mass function $p : \Omega \to [0, 1]$ like so: $$
\begin{cases}
p(TT) &= \frac{1}{6} \\
p(TH) &= \frac{1}{6} \\
p(HT) &= \frac{1}{6} \\
p(HH) &= \frac{1}{2}
\end{cases}
$$ We can think of this as some experiment involving flipping two coins that are biased in some strange way that makes double-heads more likely than other outcomes.
Then the probability that ( $A$ ) both coins land heads up given ( $B$ ) the first coin lands heads up is $$
\begin{align*}
P(A \mid B) &= \frac{P(A \cap B)}{P(B)} \\
            &= \frac{P(\{ HH \} \cap \{ HT, HH \})}{P(\{ HT, HH \})} \\
            &= \frac{P(\{ HH \})}{P(\{ HT, HH \})} \\
            &= \frac{p(HH)}{p(HT) + p(HH)} \\
            &= \frac{\frac{1}{2}}{\frac{1}{6} + \frac{1}{2}} \\
            &= \frac{3}{4}
\end{align*}
$$ However, no subset of the sample space has probability $\frac{3}{4}$ , so $A \mid B$ can't be an event in the original probability space. Have I made a mistake in this example, or am I misunderstanding the basic idea of ""conditional events""?",['probability']
4825308,Category of group actions,"Commonly, we consider for a fixed group $G$ the category $\mathcal{A}_G$ where the objects are group actions $G \times X \to X$ and a morphism between two group actions $\alpha: G \times X \to X$ , $\beta: G \times Y \to Y$ is a set function $f: X \to Y$ such that we have $\forall x \in X: f(g \cdot_{\alpha} x) = g \cdot_{\beta} f(x)$ . Now, what about the category $\mathcal{B}$ where the objects are group actions $G \times X \to X$ and a morphism between two group actions $\alpha: G \times X \to X$ , $\beta: H \times Y \to Y$ is a pair $(\varphi, f)$ such that $\varphi: G \to H$ is a group homomorphism, $f: X \to Y$ is a set function and such that we have $\forall x \in X: f(g \cdot_{\alpha} x) = \varphi(g) \cdot_{\beta} f(x)$ ? Does this category have a common name? Where can I read more about this category? Additionally, how do common categorical constructions look like in these categories? For example, what are the initial and terminal objects and how do products and coproducts look like? My guess would be that in $\mathcal{A}_G$ , the initial object is the trivial action $G \times \emptyset \to \emptyset$ , the terminal object is the trivial action $G \times \{ * \} \to \{ * \}$ , the product of $\alpha$ and $\beta$ is the natural action $G \times (X \times Y) \to X \times Y$ and the coproduct of $\alpha$ and $\beta$ is the natural action $G \times (X \sqcup Y) \to X \sqcup Y$ . In $\mathcal{B}$ , my guess would be that the initial object is the trivial action $\{ e \} \times \emptyset \to \emptyset$ and the terminal object is the trivial action $\{ e \} \times \{ * \} \to \{ * \}$ . However, I'm very unsure about the product and coproduct in $\mathcal{B}$ . Is the construction of $\mathcal{A}_G$ related to the construction of the coslice category? Is the construction of $\mathcal{B}$ related to the construction of the comma category?","['group-theory', 'group-actions', 'category-theory']"
4825316,Why is the distance in this metric space defined as it is?,"From the book: Sidney I.Resnick, A Probability Path, 2005 Birh√§user Boston, 5th printing, page 79. ''
Define the metric $d$ of $\mathbb{R}^{\infty}$ by $$d(\boldsymbol{x},\boldsymbol{y})=\sum_{k=1}^{\infty}2^{-k}\dfrac{\displaystyle{\sum_{i=1}^{k}|x_i-y_i|}}{\displaystyle{1+\sum_{i=1}^{k}|x_i-y_i|}}.$$ where $x_i$ and $y_i$ are the $i$ th components of the vectors $\boldsymbol{x},\boldsymbol{y}\in\mathbb{R}^{\infty}$ respectively.
'' I'm wondering why this metric is defined the way it is, it doesn't seem like a natural extension of finite dimensional Euclidean space. Is it defined this way to relate to something in statistics? Thanks in advance for your help!",['probability-theory']
4825373,Brezis' exercise 8.30.8: how to find the eigenvalues of this self-adjoint compact operator?,"Let $I$ be the open interval $(0, 1)$ . Let $k \in \mathbb R \setminus \{1\}$ . We consider the space $$
V := \{v \in H^1 (I) : v(0) = kv(1)\},
$$ and the symmetric bilinear form $a$ defined on $V$ by $$
a(u, v) = \int_I [ u'v' + uv ]  - \left ( \int_I u \right) \left ( \int_I v \right).
$$ I am trying to solve a problem in Brezis' Functional Analysis Exercise 8.30 Check that $V$ is a closed subspace of $H^1 (I)$ . In what follows, $V$ is equipped with the Hilbert structure induced by the $H^1$ inner product. Prove that $a$ is a continuous and coercive bilinear form on $V$ . Deduce that for every $f \in L^2 (I)$ there exists a unique solution of the problem $$
(1) \quad
u \in V
\quad \text{and} \quad
a(u, v)=\int_I f v
\quad \forall v \in V.
$$ Show that the solution $u$ of $(1)$ belongs to $H^2 (I)$ and satisfies $$
(2) \quad
\begin{cases}
-u'' + u-\int_I u = f \quad \text {on} \quad I, \\
u(0)=k u(1) \text { and } u'(1)=k u'(0).
\end{cases}
$$ Conversely, prove that any function $u \in H^2(I)$ satisfying $(2)$ is a solution of $(1)$ . Let $k_n \in \mathbb{R} \setminus \{1\}$ for all $n$ . Assume $k_n \xrightarrow{n \to \infty} k \neq 1$ . Set $$
V_n = \{v \in H^1(I) : v(0)=k_n v(1)\} .
$$ Given $f \in L^2 (I)$ , let $u_n$ be the solution of $$
(1_n) \quad
u_n \in V_n
\quad \text { and } \quad
a(u_n, v) = \int_I f v
\quad \forall v \in V_n .
$$ Prove that $u_n \to u$ in $H^1 (I)$ , where $u$ is the solution of $(1)$ . Deduce that $u_n \to u$ in $H^2 (I)$ . What happens to the sequence $\left(u_n\right)$ if $k_n$ converges to $1$ ? Consider the operator $T: L^2 (I) \rightarrow L^2 (I)$ defined by $T f=u$ , where $u$ is the solution of $(1)$ . Show that $T$ is self-adjoint and compact. Study the set $E V(T)$ of eigenvalues of $T$ . I am trying to solve (8.) In below attempt, I come up with a system of $4$ linear equations to solve for $(A, B, C, \beta)$ . Unfortunately, I don't know how to tackle this system. Could you elaborate on how to solve (8.) efficiently? Clearly, $T$ is injective, so $0 \notin EV (T)$ . Let $\frac{1}{\lambda}$ be an eigenvalue and $u$ the corresponding eigenfunction. Then $$
(3) \quad
\begin{cases}
-u'' - \alpha u = \beta \quad \text {on} \quad I, \\
u(0)=k u(1) \\
u'(1)=k u'(0), \\
\alpha = \lambda-1, \\
\beta = \int_I u.
\end{cases}
$$ We consider three cases. $\alpha < 0$ . Then $u$ has the form $$
u(x) =  A e^{\sqrt{-\alpha} x} + B e^{-\sqrt{-\alpha} x} +C,
$$ for some $A,B, C \in \mathbb R$ to be chosen. We have $$
\begin{align*}
u'(x) &= A \sqrt{-\alpha} e^{\sqrt{-\alpha} x} - B \sqrt{-\alpha} e^{-\sqrt{-\alpha} x}, \\
u''(x) &= -A \alpha e^{\sqrt{-\alpha} x} - B \alpha e^{-\sqrt{-\alpha} x}.
\end{align*}
$$ Then $$
\begin{align*}
u (0) &= A+B+C, \\
u (1) &= A e^{\sqrt{-\alpha}} + B e^{-\sqrt{-\alpha}} +C, \\
u' (0) &= A \sqrt{-\alpha} - B \sqrt{-\alpha} , \\
u' (1) &= A \sqrt{-\alpha} e^{\sqrt{-\alpha}} - B \sqrt{-\alpha} e^{-\sqrt{-\alpha}}.
\end{align*}
$$ We have $-u'' - \alpha u = \beta$ implies $-C\alpha = \beta$ . We have $u'(1)=k u'(0)$ implies $A e^{\sqrt{-\alpha}} - B e^{-\sqrt{-\alpha}} = k(A-B)$ . We have $u(0)=k u(1)$ implies $A+B+C = k (A e^{\sqrt{-\alpha}} + B e^{-\sqrt{-\alpha}} +C)$ . We have $\beta = \int_I u$ implies $\frac{A e^{\sqrt{-\alpha}} - B e^{-\sqrt{-\alpha}}}{\sqrt{-\alpha}} + \frac{B-A}{\sqrt{-\alpha}} = \beta -C$ . So we have the system $$
(4) \quad
\begin{cases}
\alpha C + \beta &=0 , \\
( e^{\sqrt{-\alpha}} - k ) A + ( k - e^{-\sqrt{-\alpha}} ) B &=0, \\
( e^{\sqrt{-\alpha}} - 1 ) A + ( 1 - e^{-\sqrt{-\alpha}} ) B + \sqrt{-\alpha} C - \sqrt{-\alpha} \beta &=0, \\
( ke^{\sqrt{-\alpha}} - 1 ) A + ( ke^{-\sqrt{-\alpha}} -1 ) B + (k-1) C &=0,
\end{cases}
$$ to solve for $(A, B, C, \beta)$ .","['ordinary-differential-equations', 'sobolev-spaces', 'functional-analysis', 'eigenfunctions', 'spectral-theory']"
4825446,How many ways to fill $n\times n$ grid with $2\times 2$ and $1\times1$ squares,"How many ways are there to fill a $n\times n$ grid with $2\times 2$ and $1\times 1$ squares? Squares cannot overlap or exceed the boundary. I don't have much opinion about the question. Well, we don't need to calculate the $1\times 1$ squares as they are the squares that remain after $2\times 2$ squares. I think we might approach with functions. Is there a formula for the solution? I calculated for really small numbers. Calling $f(n)$ for $n\times n$ square: $f(0)$ (I think it would be $1$ ) $f(1) = 1$ $f(2) = 2$ $f(3) = 5$ $f(4)$ (I guess it is 35 but might be wrong.)","['functions', 'combinatorics']"
4825449,"Show that the following system: $x= \frac{\sin(x+y)}{2}, y= \frac{\cos(x-y)}{2} $ admits at most one solution","Question: Show that the following system: $x= \frac{\sin(x+y)}{2},  y= \frac{\cos(x-y)}{2} $ admits at most one solution. I have tried to solve this question by bounding the norm of the differential of $$f(x;y)=\frac{1}{2} (\sin(x+y);\cos(x-y)),$$ but I am stuck. I will be really happy if someone can help me please. Thank you.","['systems-of-equations', 'functions', 'derivatives']"
4825514,About the constant from the Marcinkiewicz Interpolation Theorem,"The Marcinkiewicz Interpolation Theorem states as following: Let $T$ be a linear operator of weak type $(p_0,q_0)$ with constant $C_0$ and of weak type $(p_1,q_1)$ with constant $C_1$ where $q_0\neq q_1$ . Now for any $t\in (0,1)$ , define $p_t,q_t$ by $$
\frac{1}{p_t} = \frac{1-t}{p_0}+\frac{t}{p_1},\quad
\frac{1}{q_t} = \frac{1-t}{q_0} + \frac{t}{q_1}.
$$ Then $T$ is strong type $(p_t,q_t)$ with constant $C_t$ . Moreover, this constant $C_t$ satisfies $$
C_t\leq C C_0^{1-t} C_1^t,
$$ where $C=C(p_0, p_1, q_0, q_1, t)$ is bounded for $0<\epsilon \leq t\leq 1-\epsilon<1$ but tends to infinity as $t\to 0$ or $t\to 1$ . When we say an operator $T$ is of strong type $(p,q)$ with constant $C$ , it means $|| Tf||_q \leq C||f||_p$ . When we say an operator $T$ is of weak type $(p,q)$ with constant $C$ , it means $|| T f ||_{q,w}\leq C||f||_p $ . Every proof of this Theorem (for instance in Folland's ""Real Analysis"" or more accessibly in this writeup ) that I have found has worked by obtaining an inequality involving the distribution function of $Tf$ of the form $$
\lambda_{Tf}(2\alpha)\leq \lambda_{Tg_A}(\alpha)+\lambda_{Th_A}(\alpha).
$$ Then by applying the weak type estimates of $T$ and some clever integral manipulation, one obtains a final inequality like: $$
||Tf||_{q_t}\leq 2 q_t^{\frac{1}{q_t}}\left[C_0^{q_0}(p_0/p_t)^{\frac{q_0}{p_0}}|q_t-q_0|^{-1}+C_1^{q_1}(p_1/p_t)^{\frac{q_1}{p_1}}|q_t-q_1|^{-1}\right]^{\frac{1}{q_t}}=:C_t,
$$ for all $f$ with $||f||_p=1$ . Then the general inequality follows from the linearity of $T$ because $|T(cf)|=c|T(f)|$ for $c>0$ . My question is as follows: I don't see how we can obtain from this that $C_t\leq C C_0^{1-t} C_1^t$ . For example, if we take $q_0=3$ , $q_1=6$ , $t=\frac{1}{2}$ , and $q_t=4$ which satisfies the hypothesis $$
\frac{1}{q_t} = \frac{1-t}{q_0} + \frac{t}{q_1},
$$ then (for some given choice of $p_0, p_1, p_t$ ) the quantity $$
\frac{\left[C_0^3 +C_1^6 \right]^{\frac{1}{4}}}{C_0^\frac{1}{2}C_1^\frac{1}{2}}
$$ should remain bounded with respect to $C_0,C_1$ . But it clearly doesn't (Just take $C_0=1$ or something and ramp $C_1\to \infty$ ). So how do we obtain the inequality $C_t\leq C C_0^{1-t} C_1^t$ ? Is there an alternative proof of this?","['interpolation', 'analysis', 'real-analysis', 'lp-spaces', 'functional-analysis']"
4825585,Defining set of all real numbers [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 months ago . Improve this question Before you read, Yes I have seen the other posts on stack exchange about this topic, but I have something new. I'm an elementary student that just started sets so please bear with me :) I've seen online that the set of all real numbers is simply defined as: $$
\Bbb R = \{x \mid x \in\Bbb R \},
$$ which doesn't seem right. I randomly came up with this notation while studying sets $$
\Bbb R = \{x \mid a \le x \le b \text{ and } a, b \in \Bbb{Q} \},
$$ where $\mathbb{Q}$ is the set of all rational numbers. So a number like $\pi$ would be greater than $3$ and less than $3.15$ but you couldn't say an imaginary number like $i$ is greater or less than any rational or irrational number. What is wrong with this notation? (if there is anything wrong?)",['elementary-set-theory']
4825622,Blow-ups and special fibers of schemes over DVR,"Let $S$ be the spectrum of a DVR with generic point $\eta$ and closed point $s$ . Let $X$ be a flat, quasi-projective scheme over $S$ . Let $X_s$ denote the special fiber, and let $Z \subset X_s$ be a closed subvariety. On the one hand, I can consider $\mathrm{Bl}_Z(X_s)$ , the blow-up of the special fiber $X_s$ at $Z$ . On the other hand, I can also see $Z$ as a closed subscheme of $X$ via the closed embedding $X_s \hookrightarrow X$ , and thus form the blow-up $\mathrm{Bl}_Z(X)$ . The latter is a scheme over $S$ , thus it makes sense to consider its special fiber $\mathrm{Bl}_Z(X)_s$ . What is the relation between $\mathrm{Bl}_Z(X_s)$ and $\mathrm{Bl}_Z(X)_s$ ? I think that Proposition 3.12 of these notes by Toni Annala gives a natural commutative square $$\begin{array}[ccc]
\mathrm{Bl}_Z(X_s) & \rightarrow & \mathrm{Bl}_Z(X) \\
\downarrow & & \downarrow \\
X_s & \rightarrow & X
\end{array}$$ and the horizontal maps are closed immersions. The commutativity of the diagram implies that the closed immersion $\mathrm{Bl}_Z(X_s)  \rightarrow  \mathrm{Bl}_Z(X)$ factors through the special fiber, thus $\mathrm{Bl}_Z(X_s)$ is a closed subvariety of $\mathrm{Bl}_Z(X)_s$ . Is this actually an equality?","['algebraic-geometry', 'blowup']"
4825652,What does $G = G_1 G_2$ mean in group theory?,"My course notes contain the proposition, Let $G$ be a group and $G_1 , G_2$ normal subgroups of $G$ such that $G = G_1 G_2$ and $G_1 \cap G_2 = \{e\}$ . Then $G \cong G_1 \times G_2$ . An example is given using $\mathbb{Z}_6 \cong \langle 2 \rangle \times \langle 3 \rangle$ . I think I can see that $\mathbb{Z}_6 \cong \langle 2 \rangle \times \langle 3 \rangle$ but I don't recognise the notation $G_1 G_2$ and I can't think what $\mathbb{Z}_6 = \langle 2 \rangle \langle 3 \rangle$ should mean. Could someone explain that ? Conjecture : $G_1G_2:=\{g_1G_2:g_1\in G_1\}$ .","['notation', 'group-theory', 'group-isomorphism']"
4826787,Does a unique Hamiltonian graph have other long cycles?,"Let $G$ be a graph of order $n$ and let $C$ be a Hamiltonian cycle of $G$ .
Call an edge $e$ of the graph $G$ a chord if $e\not\in C$ .
Let each edge of $C$ have a weight of $1$ and each chord have a weight of $2$ .
The weight of a path or cycle of this graph is the sum of the weights of its edges.
For example, the weight of the cycle $C$ is $n$ . Is the following statement true under the above conditions on the graph $G$ ? Or maybe there is a counterexample after all? If at least one chord leaves every vertex of the graph $G$ , then there
exists a cycle in $G$ (not necessarily Hamiltonian) whose weight is
greater than $n$ . What I know is that if the number of chords for each vertex is odd,
then it follows from Thomason's theorem $G$ has a Hamiltonian cycle $C'$ different from $C$ ,
hence $C'$ contains chords.
It follows that the weight of $C'$ is greater than $n$ .
By the way, a good statement of Thomason's theorem can be found in Brian Scott's answer . However, in general a graph can have one single Hamiltonian cycle
(such graphs are called uniquely hamiltonian) even if the degree of each vertex is at least three.
The smallest uniquely hamiltonian graph
with minimum vertex degree at least $3$ contains $18$ vertices and is shown in the figure.
This graph was constructed by Gordon Royle .
The cycle $$
C'=\{1,5,6,\ldots,17,4,3,2,1\}
$$ has weight $19$ .
This graph also has a cycle of weight $25$ (apparently the maximum possible). Addition. It's worth noting that exactly this problem is formulated in Tony Huynh's answer to a question from MathOverflow .
Here is the very last sentence of his answer ""I think it should be easy to show that such a graph has a cycle longer than $C$ "". The cycle $C$ is the Hamiltonian cycle of the graph and the length is the weight of the cycle in our terminology. Addition 2. @caduk drew my attention to the following problem posed by Thomassen in the paper On the Number of Hamiltonian Cycles in Bipartite Graphs, page 440: Problem 1. Does every Hamiltonian graph $G$ of minimum degree at least $3$ contain an edge $e$ such that $G-e$ and $G/e$ are both Hamiltonian? Thomassen proved that Problem 1 has an affirmative answer for bipartite graphs. From the affirmative answer to Problem 1, it is easy to get an affirmative answer to my question.
In fact, let there exist an edge $e$ in our graph $G$ such that $G-e$ and $G/e$ are both Hamiltonian.
If $e\in C$ , then we exploit the fact that $G-e$ is Hamiltonian.
If $e\not\in C$ , then we will take advantage of the fact that $G/e$ is Hamiltonian. However, it seems to me that my question is much simpler than Thomassen's problem. Remark. I posted this question –Ω–∞ MathOveflow","['graph-theory', 'combinatorics', 'discrete-mathematics']"
4826901,Turning over blue and pink cups until the gender of the baby is known.,"I'm trying to do the following problem, and want to know why my approach does not work. A ‚Äúgender reveal‚Äù party is held to announce the gender of an expected newborn. 15 cups are filled in advance with colored beads and covered: if the baby is a boy, 8 are filled with blue beads and 7 with pink. If the baby is a girl, 7 are filled with blue and 8 with pink. When the audience arrives the cups are knocked over (revealing bead colors) in a uniformly random order until the audience has seen 8 cups of the same color (and thereby knows the gender). Let N be the number of cups turned over before the gender is known. Compute the probability that N = k for k ‚àà {8, 9, 10, . . . , 15}. Is the probability that N = 15 (and one has to wait to the very end) more or less than 1/2? Let $E_i$ be the event that the gender of the baby is known after the $i^{\text{th}}$ cup is turned over, for $i \in \{ 8,9,10,...,15 \}$ . Let Boy be the event that the baby is a boy, and Girl the event that it is a girl. Then the desired probability is: $$P(E_i) = P(E_i| \text{Boy})P(\text{Boy}) + P(E_i| \text{Girl})P(\text{Girl}) $$ For a particular $i$ , we try to compute $P(E_i| \text{Boy})$ . Given that the baby is a boy, there would be 8 cups filled with blue beads and 7 cups filled with pink beads. If we place the cups in a line, then the outcomes we are interested in, i.e., the ones where we have to turn over the $i^{\text{th}}$ cup (from left to right) before the gender of the baby is known, are the cases where the $i^{\text{th}}$ cup is a cup filed with blue beads (denoted $B$ ), and all the cups to the right of it are filled with pink beads (denoted $P$ ), i.e., the situation in the following diagram: $${\underbrace{\_ \phantom{.} \_ \phantom{.} \_ \dots \underline{B} }_\text{i}} \phantom{.} {\underbrace{\underline{P} \phantom{.} \underline{P} \phantom{.} \underline{P} \dots \underline{P} } _\text{15-i}}$$ In the above diagram, there are $7 \choose {15-i}$ ways to choose the rightmost pink-beads cups, with $(15-i)!$ ways to permute each selection, 8 ways to choose the blue-beads cup at position $i$ , and $(i-1)!$ ways to arrange the remaining cups. So we'd have: $$P(E_i| \text{Boy}) = \frac{{7 \choose {15-i}} \cdot (15-i)! \cdot 8 \cdot (i-1)!}{15!}$$ Given that the situation is similar when the baby is a girl, and that $P(\text{Boy}) = P(\text{Girl}) = \frac{1}{2}$ , we'd have: $$P(E_i) = \frac{{7 \choose {15-i}} \cdot (15-i)! \cdot 8 \cdot (i-1)!}{15!}$$ But this is wrong, since, with $i = 15$ , for example, we'd have $P(E_{15}) \approx 3.73$ . Where did I go wrong with my analysis?","['conditional-probability', 'bayes-theorem', 'probability']"
4826959,Statistics Question of the Day,"Motivation: I am a graduate student in the Department of Statistics at Kansas State University. Everyday I create a ""question of the day"" for myself, and it has been going well for the past year. These are often little distribution puzzles (sum of iid r.v. from Chi-Square is also Chi-Square, etc). Today, I came up with a question that seems simple (and perhaps is); however, I am quite stumped. The question is as follows: Question: Let $\{X_n\} \stackrel{iid}{\sim}$ Exp( $\theta$ ), where $\theta > 0$ . What is the distribution of $\{X_{(i)} - X_{(i-1)}\}$ , $i = 2, \dots, n$ ? (Note: $X_{(i)}$ denotes the i-th order statistic) Thought Process: I began by looking at the pdf for order statistics, both marginally and jointly; however, this did not lead anywhere because I am interested in the difference between two order statistics, and not the joint pdf of two order statistics. My other thought was to examine the mgf of each, but I cannot find an explicit method for determining the mgf of an order statistic.","['statistics', 'probability-distributions', 'exponential-distribution', 'order-statistics']"
4826995,"On average, how many uniformly random real numbers from $0$ to $1$ are required for the sum of their squares to exceed $1$?","A well-known question is: On average, how many uniformly random real numbers are needed for their sum to exceed $1$ ? The answer is $e$ . Let's tweak the question: On average, how many uniformly random real numbers are needed for the sum of their squares to exceed $1$ ? My attempt Let $f(n)=$ probability that the sum of squares of $n$ uniformly random real numbers is less than $1$ . The probability that the sum of squares exceeds $1$ for the first time with the $n$ th random number, is $f(n-1)-f(n)$ . Then the expectation is $\sum\limits_{n=2}^\infty n(f(n-1)-f(n))$ . I have worked out that: $f(1)=1$ $f(2)=\int_0^1\sqrt{1-{x_1}^2}dx_1$ $f(3)=\int_0^1\int_0^{\sqrt{1-{x_1}^2}}\sqrt{1-{x_1}^2-{x_2}^2}d{x_2}d{x_1}$ $f(4)=\int_0^1\int_0^{\sqrt{1-{x_1}^2}}\int_0^{\sqrt{1-{x_1}^2-{x_2}^2}}\sqrt{1-{x_1}^2-{x_2}^2-{x_3}^2}d{x_3}d{x_2}d{x_1}$ And so on. With help from Wolfram, the above expressions are: $f(2)=\frac{\pi}{4}$ $f(3)=\frac{\pi}{6}$ $f(4)=\frac{\pi^2}{32}$ A087299 suggests that $f(n)$ equals the ratio of the volume of an $n$ -dimensional ball to the circumscribed $n$ -cube, but I don't understand why. Assuming this is true, the expectation is  approximately $3.9257708130843$ . I don't know if this expectation has a closed form.","['integration', 'volume', 'expected-value', 'closed-form', 'probability']"
4827018,Nonlinear ODE $a^2u''+bu'+b^*(u')^2/u+cu=-1$,"Consider the ODE $$[a(x)]^2u''+b(x)u'+b^*(x)(u')^2/u+c(x)u=-1$$ on $\mathbb{R}$ . I am trying to make it looks better by writing it as a variational problem ( $u$ is some minimizer of some energy functionals). Do anyone have ideas about that? Or any good change of variable to reduce the nonlinearity of the equation? My only thought is to write $u=e^v$ and $$[a(x)]^2v''+b(x)v'+\{b^*(x)+[a(x)]^2\}(v')^2+c(x)=-e^{-v},$$ but I do not know if it is useful.","['ordinary-differential-equations', 'calculus-of-variations']"
4827050,What is the order of the following system $\frac{dx}{dt} = \sqrt{1-x^2}$,"$\frac{dx}{dt} = \sqrt{1-x^2}$ I‚Äôm referring ‚ÄúNon linear dynamics‚Äù by Steven Strogatz.
Does the above mentioned system belongs to first order?
One particular solution of the system is $x(t) = \sin(t)$ .
Then there are no fixed point in the system, rather the system exhibits limit cycle. Still I‚Äôm able to represent the system in first order format. As per Strogatz, oscillations are prohibited in first order system. What am I missing here?","['nonlinear-system', 'ordinary-differential-equations', 'dynamical-systems']"
4827054,Calculate the transitive interior and the transitive closure.,"We say that a set $X$ is transitive whether for any $x$ in $X$ the implication $$
(x\in X)\to(x\subseteq X)
$$ holds; moreover, we say that the set $S(y)$ is the successive of any set $y$ when the equality $$
S(y)=y\cup\{y\}
$$ holds. Now if $\mathfrak X$ is a collection of transitive sets then it is not complicated to show that even $\bigcup\mathfrak X$ is transitive so that if $\emptyset$ is transitive then for any $U$ in $\mathscr P(X)$ we can find the biggest transitive set $A_U$ (I call it transitive interior) there contained taking the union of all that transitive sets. So I am trying to calculate (provided it is possibile) explicitly $A_U$ so that I initially proved that the set $$
V:=\big\{x\in U\mid \text{ $y$ is transitive and $y\subseteq U$ for any $y \in S(x)$}\big\}
$$ is transitive but unfortunately it seems to me that it is note equal to $A_U$ . Anyway, it is not complicated to show that $\bigcap\mathfrak X$ is transitive -provided any $X$ in $\mathfrak X$ is it: so if $X$ is a transitive set for any $U$ in $\mathscr P(X)$ there exists the littlest transitive set $B_U$ (I call it transitive clousure) of $\mathscr P(X)$ containing $U$ taking the intersection of all that sets. So I tried to calculate (provided it is possibile) explicitly $B_U$ but unfortunately I have not idea how procede here: to be honestly I tried to put $$
B_U:=U\cup\left(\bigcup_{x\in U}x\right)
$$ but then $B_U$ it would not be transitive apparently. Finally, I observed (I think it could be useful) that if $X$ is transitive then the collection $$
\mathcal T_X:=\{U\in\mathscr  P(X)\mid \text{ $U$ is transitive}\}
$$ is a topology of open or closed set so that if we consider it as ""open"" topology then $A_U$ would be (apparently) the (topological) interior of $U$ whereas if we consider it as ""closed"" topology then $B_U$ would be (apparently) the (topological) closure of $U$ . So I ask to calculate $A_U$ and $B_U$ , hoping it is possible: could someone help me, please?","['elementary-set-theory', 'general-topology', 'alternative-proof', 'relations']"
4827073,Spatial Poisson process of discs and the $r \rightarrow 0$ limit,"Consider a (homogeneous) spatial Poisson process $\Pi$ in $[0,1]^2$ with constant rate $\lambda^2$ . For each $x \in \Pi$ , let $D_x$ be the disc centered at $x$ with radius $r$ , and let $D = \bigcup_{x \in \Pi} D_x$ be the union of all disks. We couple $r$ and $\lambda$ such that $\pi r^2 \lambda^2 = \varepsilon$ for some $\varepsilon \in (0,1)$ fixed, so that the (approximate) expected area of $D \cap [0,1]^2$ is constant ( $\pi r^2$ is the area of one disk and $\lambda^2$ is the expected number of points in $\Pi$ ). I am interested in the $r \rightarrow 0$ (equivalently, $\lambda \rightarrow \infty$ ) limit of this process. In particular, I suspect that the indicator function $1_{D \cap [0,1]^2}$ of the region $D$ in $[0,1]^2$ tends to the constant function $\varepsilon$ in the limit, in the weak sense. That is, in the limit the collection of disks $D$ end up 'blanketing' the space $[0,1]^2$ with density $\varepsilon$ . I am unsure if this is an established property in the literature and do not know how to go about proving this. We can upper-bound $\text{Area}(D)$ by $|\Pi|\cdot \pi r^2$ , which tends to $\varepsilon$ in the limit, but I cannot get anything more.","['probability-theory', 'functional-analysis', 'poisson-process']"
4827109,Questions concering the integral $F(\lambda)= \int_{0}^{\infty}\frac{x^2}{\sqrt{1+x^4}e^{\lambda x}}dl(x)$,"I have 4 questions concerning the following integral: $\forall \lambda >0$ we have: $$F(\lambda)= \int_{0}^{\infty}\frac{x^2}{\sqrt{1+x^4}e^{\lambda x}}dx $$ I would like to have some help to find an answer concerning question 1 & 4 and to know if my solution at question 2 & 3 is correct for all the cases. ->Any help on just one of this question will be very nice<- Please don't delete/erase this question, since it took me quite some time to write it. :-) Prove that $F(\lambda)$ is continuous on $]0; \infty[$ . -Did not succeed to find a convincing proof, indeed it is easy to prove that $F(\lambda)$ is defined on $]0; \infty[$ as it is finite but how to prove the continuity? (in the simplest way possible). Let $(u_n) \in ]0; \infty[$ an increasing function s.t. $u_n \underset{n \to \infty }{\rightarrow} \infty $ . Compute $\lim_{n \to \infty} F(u_n)$ -It is easy to show that $F(\lambda)$ is positive decreasing function. Moreover we we have that $f_{\lambda}(x) = \frac{x^2}{\sqrt{1+x^4}e^{\lambda x}} = \frac{\sqrt{x^4}}{\sqrt{1+x^4}e^{\lambda x}} \leq \frac{\sqrt{x^4+1}}{\sqrt{1+x^4}e^{\lambda x}}=\frac{1}{e^{\lambda x}}=g(x)$ Hence $f_{u_n}(x)$ is bounded by $g(x)$ and because $f_{u_n}(x)$ is a continuous mapping we can writte $\lim_{n \to \infty} f_{u_n}(x) = 0$ . So by the DCT we get that: $$\lim_{n \to \infty} \int_{0}^{\infty}\frac{x^2}{\sqrt{1+x^4}e^{u_n x}}dl(x)= \int_{0}^{\infty} \lim_{n \to \infty} \frac{x^2}{\sqrt{1+x^4}e^{u_n x}}dl(x) =0 $$ Let $(v_n) \in ]0; \infty[$ be a decreasing function s.t. $v_n \underset{n \to \infty }{\rightarrow} 0 $ . Compute $\lim_{n \to \infty} F(v_n)$ -As $ v_{n+1} \leq v_n \Rightarrow e^{v_{n+1}} \leq e^{v_n} \Rightarrow \frac{1}{e^{v_n}} \leq \frac{1}{e^{v_{n+1}}} \Rightarrow f_{v_n}(x) = \frac{x^2}{\sqrt{1+x^4}e^{v_n x}} \leq f_{v_{n+1}}(x) = \frac{x^2}{\sqrt{1+x^4}e^{v_{n+1} x}} $ Now because $f_{v_n}(x)$ is continuous $\lim_{n \to \infty} f_{v_n}(x)= f_{\lim_{n \to \infty} v_n}(x) = \frac{x^2}{\sqrt{1+x^4}} $ . Hence by the MCT we have: $$ \lim_{n \to \infty} \int_{0}^{\infty}\frac{x^2}{\sqrt{1+x^4}e^{v_n x}}dl(x)= \int_{0}^{\infty} \lim_{n \to \infty} \frac{x^2}{\sqrt{1+x^4}e^{v_n x}}dl(x)=\int_{0}^{\infty}\frac{x^2}{\sqrt{1+x^4}}dl(x) = \infty $$ Prove that the limit $\lim_{\lambda \to 0^+} \lambda F(\lambda)$ exists and find it. -Did not succeed to find a convincing answer. Thank you","['integration', 'continuity', 'calculus', 'solution-verification', 'limits']"
4827117,what is the second approach?,"Question : There are 9 vans , 5 are long wheel and 4 short. If you want 4 vans where at least 2 vans should be long wheels , how many combination can you have? My Approach explanation: Combinations with 2 long & 2 short vans: Choose 2 long vans from 5: 5C2 = 10 ways
Choose 2 short vans from 4: 4C2 = 6 ways
Total combinations for this case: 10 * 6 = 60 Combinations with 3 long & 1 short van: Choose 3 long vans from 5: 5C3 = 10 ways
Choose 1 short van from 4: 4C1 = 4 ways
Total combinations for this case: 10 * 4 = 40 All 4 vans are long: Choose 4 long vans from 5: 5C4 = 5 ways Total combinations: Add the combinations from all cases: Total combinations = Case 1 + Case 2 + Case 3 = 60 + 40 + 5 = 105 My Concern Can we do it with another approach where you calculate the total combinations first and then subtract the unwanted combinations?","['combinations', 'combinatorics']"
4827221,Probability of tournament never ending,"Three players $A,B,C$ play tennis matches. There is always one player waiting to face the winner of the match between the other two. For a given match both players have the same probability of winning. The tournament ends whenever a player wins two consecutive matches. What is the probability of the tournament never ending? Do all players have same chance of winning? Intuitively I would say that the probability of the match going on forever is $0$ . How can I show this formally. For the tournament to keep on going we need that player that wins first match loses the second one, the player that wins second match loses the third and so on. How can I show that the probability of the intersection of these events is $0$ . For the second question the players that play the first game clearly have the same probability of winning the tournament, however I don't know whether the one that doesn't play the first match has the same probability of winning the tournament than the other two.",['probability']
4827230,Bernstein's theorem for non-finite measures,"Bernstein's theorem states the following: Let $g: [0, \infty) \to [0, \infty)$ be a smooth function. Then the following are equivalent: (a) $g$ is a completely monotone function, i.e. $(-1)^{n}g^{(n)}(x) \geq 0$ for all $n \geq 0$ and $x \geq 0$ . (b) $g(x) = \int_{0}^{\infty} e^{-tx} d\mu(t)$ for some (unique) finite Borel measure $\mu$ on $[0, \infty)$ . I wonder if we can modify this theorem slightly to the functions on $(0, \infty)$ (exclude $0$ ) and allow non-finite measures. For example, we have $\frac{1}{x} = \int_{0}^{\infty} e^{-tx} dt$ , and the function $g(x) = \frac{1}{x}$ is completely monotone on $(0, \infty)$ , and the corresponding measure is $dt$ , which is not finite. Another example is $g(x) = 1 + e^{-x} + e^{-2x} + \cdots = \frac{1}{1 - e^{-x}}$ , which is completely monotone on $(0, \infty)$ and a Laplace transform of the measure $\mu = \sum_{n \geq 0} \delta_{n}$ , which is again not finite.","['measure-theory', 'real-analysis']"
4827267,"Let $f(x)\in C^2[0,2]$, prove that there exists a $\xi\in(0,2)$ such that $\int_0^2f(x)dx=2f(1)+\frac{f''(\xi)}{3}$.","Let $f(x)$ be a twice continuously differentiable function over $[0,2]$ , i.e. $f(x)\in C^2[0,2]$ , prove that there exists a $\xi\in(0,2)$ such that $$\int_0^2f(x)dx=2f(1)+\frac{f''(\xi)}{3}.$$ First, my naive idea is to use Taylor expansion with Lagrange remainder $$f(x)=f(1)+f'(1)(x-1)+\frac{f''(\xi)}{2}(x-1)^2.$$ Then integrate both sides over $[0,2]$ we have that $$\int_0^2f(x)dx=\int_0^2f(1)dx+\int_0^2f'(1)(x-1)dx+\int_0^2\frac{f''(\xi)}{2}(x-1)^2dx=2f(1)+\frac{f''(\xi)}{3}.$$ When I finished writing this, I realized that I was wrong, because in Lagrange remainder, $\xi$ is depend on $x$ and it is not a constant, for different $x$ we get different $\xi$ , so we can not deduce that $$\int_0^2\frac{f''(\xi)}{2}(x-1)^2dx=\frac{f''(\xi)}{3}.$$ Then I tried many different ideas and failed until I came up with taylor's theorem with Cauchy remainder $$f(x)=f(1)+f'(1)(x-1)+\frac{1}{2!}\int_1^x(x-t)^2f''(t)dt.$$ Integrate both sides over $[0,2]$ we have that $$\int_0^2f(x)dx=2f(1)+\int_0^2\left(\int_1^x(x-t)^2f''(t)dt\right)dx.$$ And $$\int_0^2\left(\int_1^x(x-t)^2f''(t)dt\right)dx
=\int_0^1\left(\int_1^x(x-t)^2f''(t)dt\right)dx+\int_1^2\left(\int_1^x(x-t)^2f''(t)dt\right)dx.$$ Since $$\int_0^1\left(\int_1^x(x-t)^2f''(t)dt\right)dx=
-\int_0^1\left(\int_x^1(x-t)^2f''(t)dt\right)dx=-\int_0^1\left(\int_0^t(x-t)f''(t)dx\right)dt=\int_0^1f''(t)\frac{t^2}{2}dt=\int_0^1f''(x)\frac{x^2}{2}dx.$$ Similarly, we have that $$\int_1^2\left(\int_1^x(x-t)^2f''(t)dt\right)dx=\int_1^2f''(x)\frac{(2-x)^2}{2}dx.$$ Thus we have the following formula $$\int_0^2f(x)dx=2f(1)+\int_0^1f''(x)\frac{x^2}{2}dx+\int_1^2f''(x)\frac{(2-x)^2}{2}dx.$$ Use this and the first mean value theorem of definite integral, there exists $\xi_1\in(0,1)$ and $\xi_2\in(1,2)$ such that $$\int_0^1f''(x)\frac{x^2}{2}dx=f''(\xi_1)\int_0^1\frac{x^2}{2}dx=\frac{f''(\xi_1)}{6}.$$ $$\int_1^2f''(x)\frac{(2-x)^2}{2}dx=f''(\xi_2)\int_1^2\frac{(2-x)^2}{2}dx=\frac{f''(\xi_2)}{6}.$$ Since $f''(x)$ is continuous, by the intermediate value theorem there exists $\xi\in(0,2)$ such that $$\frac{f''(\xi_1)}{6}+\frac{f''(\xi_2)}{6}=\frac{f''(\xi)}{3}.$$ Thus we finish our proof. My QuestionÔºö (1) Is there a simple way to prove this exercise? (2) Whether the formula $$\int_0^2f(x)dx=2f(1)+\int_0^1f''(x)\frac{x^2}{2}dx+\int_1^2f''(x)\frac{(2-x)^2}{2}dx$$ is a special case of a general integral formula, like the Euler-Maclaurin formula or Trapezoidal rule?","['complex-analysis', 'multivariable-calculus', 'calculus', 'real-analysis']"
4827329,"How to interpret the ""independent probability"" of an event which may only happen once at most?","Good evening, everyone. I'm currently reading Davy Paindaveine & Philippe Spindel (2023) Revisiting the Name Variant of the Two-Children Problem, The American Statistician, 77:4, 401-405, DOI:
10.1080/00031305.2023.2173293 , and, working through some of the preliminary probabilities, I got stuck on something I can't resolve: The second variant rather asks: for a two-children family having at
least a girl whose name is Florida, what is the probability that the
other child is a boy? See, for example, Mlodinow (2008) or Marks and
Smith (2011). If two sisters may be given the same name, then this
variant is strictly equivalent to the previous one: more precisely, if
it is assumed that girls are independently named Florida with
probability r, then the probability that the other child is a boy is
2/(4 ‚àí r). To make the second variant of interest, one therefore needs
to assume that two sisters may not be given the same name, in which
case, under the assumptions associated with what we will call Model A
below, the probability that the other child is a boy is 1/2,
irrespective of r I'm trying to reason through the italicized part of the second variant as follows. The siblings are listed in order where $B$ is a boy, $G_F$ is a girl named Florida, and $G_F^C$ is a girl not named Florida: $$\frac{P\{(B,G_F)\cup(G_F,B)\}}{P\{(B,G_F)\cup(G_F,B)\cup(G_F^C,G_F)\cup(G_F,G_F^C)\}}=\frac{2\times.5^2r}{2\times .5^2r+P\{(G_F^C,G_F)\cup(G_F,G_F^C)\}}$$ Now, where I'm getting stuck is resolving $P\{(G_F^C,G_F)\cup(G_F,G_F^C)\}$ . If I reason as follows, I can get the right answer: The probability neither sister is named Florida is $(1-r)^2$ So the probability at least one sister is named Florida is $1-(1-r)^2$ But I don't want to count the probability both are named Florida So I subtract $r^2$ , leaving $1-(1-r)^2-r^2=2r$ So the probability of two girls, one named Florida, is $.5^2\times 2r$ . What's confusing me is that I also feel like I should be able to do it this way: $$P\{(G_F^C,G_F)\cup(G_F,G_F^C)\}=P(G_F^C,G_F)+P(G_F,G_F^C)=P(G_F^C|G_F)P(G_F)+P(G_F|G_F^C)P(G_F^C)$$ $$=.5^2[1r+r(1-r)]=.5^2[2r-r^2]$$ Where is this $-r^2$ term coming from? What's weird is that I was able to confirm that, reasoning this way, $P(G_F^C,G_F)+P(G_F,G_F^C)+P(G_F^C,G_F^C)=.5^2(1)$ . Somehow the $(G_F,G_F)$ event has probability 0 like it's supposed to, but the probability of ""exactly one sister named Florida"" is off by exactly what $P(G_F,G_F)$ would've been if possible. I feel like I'm missing something fundamental here. The only explanation I have is that, as soon as we defined an independent probability $r$ of a girl being named Florida, we lost the right to treat ""two sisters named Florida"" as an impossibility excluded from the overall sample space. That would necessitate the first strategy I used, while invalidating my $P(G_F|G_F^C)=.5r$ and $P(G_F^C|G_F)=.5(1)$ conditionals in the second strategy. ""May not be given the same name"" appears to mean ""exclude the $(G_F,G_F)$ event from consideration"", and not "" $P(G_F,G_F)=0$ "". However, even if this is correct, I still can't explain the fact that the second strategy gave a complete probability distribution. Can anyone shed some light on this?","['conditional-probability', 'probability']"
4827352,Fake proof that $\lim \limits_{x \to 0} \frac{\sin(3x)}{x^3}=\frac{-9}{2}$,"In the book Mathematical Fallacies, Flaws, and Flimflam, the following fake proof is provided: The question starts with using trigonometric identities to turn $\sin(3x)\to3\sin(x)-4\sin^3(x)$ . Then divides the limit into two where the limit of $({\frac{\sin(x)}{x}})^3$ is obviously $1$ , and variable substitutes $x=3t$ making $\frac{\sin(3t)}{(3t)^3}$ which can then be manipulated to become the original equation of $\frac{\sin(3x)}{x^3}$ , giving a final result of $-9/2$ . I fail to see where the error is in the calculations, everything seems sound, but I know through the use of L'H√¥pital's rule and Desmos that the answer should be $\infty$ at $x=0$ .","['limits', 'limits-without-lhopital', 'fake-proofs']"
4827393,"Prove that $\int_0^{\infty} \frac{{\rm Li}_2(-x)}{1+x^2}\, {\rm d}x = - \frac{7 \pi^3}{96}$","Here is something interesting I came up while playing with dilogs. Prove that $$\int_0^{\infty} \frac{{\rm Li}_2(-x)}{1+x^2}\, {\rm d}x = - \frac{7 \pi^3}{96}$$ maybe we can derive a general form. For instance $$\int_0^{\infty} \frac{{\rm Li}_2(-x)}{1+x^\alpha} \, {\rm d}x , \; \; \alpha \in \mathbb{N} \setminus \{1\}$$ Even more interesting is that if we replace $\text{Li}_2(-x)$ with $\text{Li}_2(x)$ , the result is $\frac{5\pi^3}{96} - i\pi G$ $G$ being Catalan's Constant. It would not be too much of a leap to imagine the results with $-x$ and $x$ are somehow connected to each other.","['integration', 'calculus', 'definite-integrals', 'logarithms']"
4827399,What is the name of this function space?,"Say $(S,\Sigma,\mu)$ and $(\Omega,\mathcal F,P)$ are measure spaces and $X : S \times \Omega \rightarrow \mathbb R$ a measurable function. The Wikipedia article on the Minkowski inequality ( https://en.wikipedia.org/wiki/Minkowski_inequality#Minkowski's_integral_inequality ) defines the quantity $$
   || X ||_{p,q} = \big( \int_S \big(\int_\Omega |X|^q dP\big)^{\frac p q} d\mu \big)^{\frac 1 p}   
$$ and the function space $\mathcal L_{p,q}$ , in which it is finite. The article states that these function spaces have been studied, but gives no source. I also wasn't able to find any other resources. I'm interested because I'm investigating the setting where $(\Omega,\mathcal F,P)$ is a probability triple and $X$ a stochastic process, in which the pointwise expectation $$
\mathbb E[X] : S \rightarrow \overline{\mathbb R}, s \mapsto \mathbb E[X(s)]
$$ satisfies $\mathbb E[|X|^q]^{1/q} \in \mathcal L_p$ , i.e. $$
    || X ||_{p,q} =\big( \int_S \big( \int_\Omega |X|^q dP \big)^{\frac p q} d\mu \big)^{\frac 1 p} = ||\mathbb E[|X|^q]^{\frac 1 q} ||_p < \infty
$$ and hence $X \in \mathcal L_{p,q}$ .","['stochastic-processes', 'measure-theory', 'lp-spaces']"
4827421,"Show $\sum_{n=0}^\infty x^{2^n}\geq -\frac{\ln(-\ln(x))}{e\ln(2)}$ for all $x\in(\frac{1}{e},1)$","I am trying to show the inequality $$\sum_{n=0}^\infty x^{2^n}\geq -\frac{\ln(-\ln(x))}{e\ln(2)}$$ for $x\in(\frac{1}{e},1)$ . I have observed that the inequality $$\sum_{n=0}^\infty x^{2^n}\geq\sum_{n=0}^\infty e^{-1-n\ln(2)}(-\ln(-\ln(x)))$$ appears to hold, and from this right hand side I believe it is possible to use integral comparisons to obtain the desired inequality. However, I have been unsuccessful in showing this inequality since it appears that it does not hold term-wise (i.e. it appears that we don't have $x^{2^n}\geq e^{-1-n\ln(2)}(-\ln(-\ln(x)))$ for all relevant $x$ and $n$ ). Maybe there is an easier approach to the original problem?","['inequality', 'sequences-and-series']"
4827422,Explicit inverse of $x\mapsto\frac{1}{\alpha} (x+1)^{\frac{\alpha - 1}{\alpha}} | x - 1 |^{\frac{1}{\alpha}} \frac{\alpha(x - 1) + 2}{(x- 1)(x + 1)}$,"In the very same spirit as my previous question I am trying to find an explicit representation of the convex conjugate $f^*$ of the function $$
f \colon \mathbb R \to [0, \infty], \qquad x \mapsto | 1 - x |^{\frac{1}{a}} (1 + x)^{\frac{\alpha - 1}{\alpha}} + \iota_{[0, \infty)}(x),
$$ where for any set $A$ , the convex indicator function $\iota_{A}(x)$ is equal to $0$ for $x \in A$ and equal to $\infty$ else and $\alpha \in (0, 1)$ . Up to edge cases, this first boils down to finding the inverse of $$
f'(x)
= \frac{1}{\alpha} (x+1)^{\frac{\alpha - 1}{\alpha}} | x - 1 |^{\frac{1}{\alpha}} \frac{\alpha(x - 1) + 2}{(x- 1)(x + 1)}
= \frac{1}{\alpha} \left| \frac{x-1}{x+1}\right|^{\frac{1}{\alpha}} \left(\alpha + \frac{2}{(x- 1)}\right)
$$ (to obtain $f^*(y) = y (f')^{-1}(y) - f((f')^{-1}(y))$ .) Due to the last term in parenthesis, I do not know how to invert $f'$ , like it was done in this answer . We have that $f'$ is strictly monotonically increasing and continuous, so that it is bijective onto its range and hence invertible. For $\alpha = \frac{2}{3}$ , WolframAlpha gives an explicit inverse, which as a quite complicated expression , whereas for $\alpha = \frac{1}{\sqrt{2}}$ , it can't find an answer in terms of standard mathematical functions.","['special-functions', 'inverse-function', 'real-analysis', 'closed-form', 'derivatives']"
4827430,Geometric generalization of Fermat Last Theorem,"I apologized if the post is a bit long. I am not a mathematician, but I love playing with math. I was looking at Fermat last Theorem the other day, and consider this situation. In the original formulation, it was positive integer solution to $ a^n + b^n = c^n $ , for $n>2,n \epsilon \mathbb{N}$ . I consider it to be equivalent to finding non zero rational points on this closed curve $$|x|^n + |y|^n = 1$$ I then plot that in 3D just to make it easier ( $z=n$ ), and got a weird tube (I called Lz tube). I then realized that the closed curve $|x|^n + |y|^n = 1$ , is just the intersection between the Lz tube with horizontal plane (perpendicular to z-axis). Now, here is my question, what happened if you tilt the plane? When plane $ax+by+cz+d=0$ that intersect the tube $|x|^z + |y|^z = 1$ , it will produce a curve. What $(a,b,c,d)$ will produced infinitely many rational points on the closed loop that curve? Caveats: Exclude points that contain zero There are closed part and open parts of the resulting curve. I only want to know the solution on the closed loop part.","['number-theory', 'elementary-number-theory', 'rational-numbers']"
4827463,Interesting contexts in which the first few cohomology groups ($H^1$ and $H^2$) show up?,"I am learning about $H^n(G,A)$ and as part of it I'm cataloguing interesting examples of these groups showing up ""in the wild"", mostly for the case $n=1,2$ . I'm looking for more isolated examples, rather than entire theories built around cohomology (e.g. class field theory). What I have so far: $H^1(G,L)$ and $H^1(G,L^\times)$ being trivial for $L/K$ Galois (and its connection with Hilbert $90$ and Kummer theory). $H^2(G,A)$ and group extensions, $H^1(G,A)$ and split extensions. $H^2(G,L^\times)$ and Brauer groups.","['examples-counterexamples', 'abstract-algebra', 'group-cohomology', 'group-theory', 'homology-cohomology']"
4827496,(Real) Vector space axioms: Simple example for necessity of axiom of associativity of multiplication,"I'm writing some notes on Linear Algebra and am trying to show the independence of an ""optimal list"" for vector space axioms. I am using the following definition: A vector space is a structure $(V,0,+,\cdot)$ , where $V$ is a set, $0$ is an element of $V$ and $+\colon V\times V\to V$ and $\cdot\colon\mathbb{R}\times V\to\mathbb{V}$ are binary operations such that $v+(u+w)=(v+u)+w$ $v+0=v$ For all $v$ there exists $w$ such that $v+w=0$ . $\alpha(\beta v)=(\alpha\beta)v$ $\alpha(v+w)=\alpha v+\alpha w$ . $(\alpha+\beta)v=\alpha v+\beta v$ $1v=v$ Question : Is there a simple example of a structure which satisfies all axioms except only 4? A few remarks: I used just the ""right-identity/right-inverses"" version of the group axioms in 1.-3., which is well-known to be sufficient for having a group ( see here ) I added "" $0$ "" as a constant because axiom 3. depends on the given $0$ which satisfies 2. This is a way to avoid the usual ambiguities with the way axiom 3. is commonly stated ( see this question and the accepted answer ) It is well known that commutativity is implied by the other axioms (ignoring 4., even), so I ommited it. I am considering only real vector spaces. (So the example in this answer , for example, does not apply) For each axiom in the list except 5. , I have already managed to find a structure which satisfies all other axioms except it. However, my example for axiom 4. is quite complicated: Let $V=\mathbb{R}$ as a $\mathbb{Q}$ -vector space, and $L_{\mathbb{Q}}(\mathbb{R})$ be the $\mathbb{Q}$ -vector space of $\mathbb{Q}$ -endomorphisms of $\mathbb{R}$ . Take any $\mathbb{Q}$ -linear map $\Phi\colon\mathbb{R}\to L_\mathbb{Q}(\mathbb{R})$ which takes $1$ to the identity and $\sqrt{2}$ to the zero map (which can be done by difining $\Phi$ on a basis of $\mathbb{R}$ over $\mathbb{Q}$ which contains $1$ and $\sqrt{2}$ ), and let $\lambda x = \Phi(\lambda)(x)$ for $\lambda\in\mathbb{R}$ and $x\in V$ . Then axioms 1.-3. are immediate for $V$ ; axiom 5. follows from $\Phi(\alpha)$ being $\mathbb{Q}$ -linear (additive) for each $\alpha$ , 6. follows from $\Phi$ itself being $\mathbb{Q}$ -linear, and 7. from $\Phi(1)$ being the identity. Axiom 4. is not true: Taking $\alpha=\beta=\sqrt{2}$ and $v=1$ , we have $\alpha(\beta v)=0$ but $(\alpha\beta)v=2v=2$ . However, this example depends on the axiom of choice (for having a basis of $\mathbb{R}$ as a $\mathbb{Q}$ -vector space) and some linear algebraic facts as well. If we allowed for vector spaces over other fields, then this would give us an example which does not depend on choice simply by taking $\mathbb{Q}[\sqrt{2}]$ instead of $\mathbb{R}$ , but this is not what I want. Is there any other simpler example of a structure which satisfies all axioms except 4.? The distributivity axioms actually allow us to prove that such a given structure will be a divisible group , and that in fact the product will be associative for integers (although not necessarily for rationals). It's also not clear to me whether the subjacent group will be torsion-free.","['linear-algebra', 'axioms']"
4827512,I don‚Äôt know what a natural number actually is,"For some context I did a course on set theory where I was taught about ZFC, and the construction of the natural numbers, integers etc. I think I was far too young to take the course because it‚Äôs left me really confused and questioning about everything. You guys might not be qualified for this kind of thing but I don‚Äôt feel real anymore (as strange as it sounds) and I feel dissociated from everything. The construction of the integers and rationals felt easy and intuitive after the natural numbers were defined. However, when I think about what a natural number is, I just don‚Äôt know anymore. I know in set theory they are symbols defined recursively in terms of empty sets, but I just don‚Äôt understand how they are used in such broad contexts and take so many forms when they are just sets. The bijection (counting) elements of a set feels strange to me, since I just feel uncomfortable now saying a set has 3 elements.","['logic', 'natural-numbers', 'discrete-mathematics', 'philosophy', 'set-theory']"
4827589,What is wrong with this proof that a polynomial is solvable by radicals?,"We consider $$x^5 - x + 5 \in \mathbb{Q}[x].$$ I'm pretty sure that this polynomial is NOT solvable by radicals since its Galois group is isomorphic to $S_5$ (see my question from yesterday ). However, my teacher is pretty sure that it IS solvable by radicals, and he proved it by seeing that its Galois group is isomorphic to $D_5$ (and I think it's not). He considers the roots of the polynomial $\alpha_1, \alpha_2, \overline{\alpha_2}, \alpha_3, \overline{\alpha_3}$ , with $\alpha_1 \in \mathbb{R}, \alpha_2, \alpha_3 \in \mathbb{C}\setminus\mathbb{R}$ .
These roots form a regular pentagon, and since two pairs of roots are conjugate, we can consider the rotation $a = (1, 2, 3, 4, 5)$ and the permutation $b = (2, 5)(3, 4)$ , which generate $$D_5 = \left\langle a, b \mid a^5 = 1 = b^2, ba = a^{-1}b \right\rangle.$$ So then he saw that $D_5$ is solvable by radicals and deduced that the polynomial is solvable by radicals too. I don't really understand at all what he does, so I am not able to say what is wrong with it‚Ä¶ Could you help me? Thanks!","['galois-theory', 'group-theory', 'solvable-groups']"
4827620,Winning strategy for game guessing if next number is prime,"I've come across an intriguing card game where a deck is used with cards numbered from $1$ to $N$ . The game proceeds as follows: the dealer draws cards one by one from the deck and shows the number on each card. At any point in the game, even before the first card is shown, a player can say ""stop"". If the number on the next card drawn is a prime number, the player wins; if it's not, the player loses. The cards in the deck are shuffled randomly, so there's no way to predict the order of the cards. I'm trying to determine the best strategy for this game to maximise the chances of winning. Also, how would you calculate the probability of winning with this strategy for a given number of cards, $N$ , in the deck? Some clarifications: The number 1 is not considered a prime. You can see and remember the cards that have been drawn out, which can inform your strategy. Knowing the number $N$ , you can identify all the prime numbers within that range beforehand. If all cards are drawn and you haven't said ""stop"" before the last one (even if it's not), you lose. Additionally, the author of the problem mentioned that there should be a strategy that optimizes the probability of winning.","['combinatorics', 'card-games', 'probability']"
4827621,Approximation for $\sum_{i=1}^\infty \frac{\sqrt i}{\sqrt{2\pi \epsilon}} e^{\frac{-i}{2\epsilon}}$?,"I am trying to get some nice number (not the polylogarithmic) with the series on the title. I know the exact result is $\dfrac{1}{\sqrt{2\pi \epsilon}}Li_{-\frac{1}{2}}(\dfrac{1}{^{\epsilon}\sqrt{e}})$ . However, putting some values into Wolfram Alpha (I'm sorry I'm an engineering student, it's stronger than me) I came to notice that the result seems to be pretty close to $\epsilon$ . In particular, it gets closer to $\epsilon$ as $\epsilon$ grows. Therefore, is there a way to approximate the remainder between the real result and $\epsilon$ ? If it's of any use, I know that $\int_0^{\infty} \dfrac{\sqrt x}{\sqrt{2\pi \epsilon}} e^{\dfrac{-x}{2\epsilon}} dx = \epsilon $ . It's kinda important because I'm trying to find a discrete-analogue of the gamma distribution $\dfrac{1}{\sqrt{2\pi \epsilon x}} e^{\dfrac{-x}{2\epsilon}} $ and this would be the mean. If anyone has another discrete gamma distribution similar to this one it would also be appreciated. Edit: well, it seems that it was not as easy as my other questions. For the sake of not going crazy, it would be more than enough to get some result like: $\epsilon + C > \dfrac{1}{\sqrt{2\pi \epsilon}}Li_{-\frac{1}{2}}(\dfrac{1}{^{\epsilon}\sqrt{e}})$ , where there is C is a function of the other variables or a constant","['polylogarithm', 'discrete-mathematics', 'sequences-and-series']"
4827633,I have multiple questions regarding one problem: For a positive real $a$ what's the value of $\sqrt{a\sqrt{a\sqrt{a...}}}$,"I'm reading a book, the book is aimed at high schoolers so it is perhaps not the most rigorous book. The intended solutions the book provides for the problem are the following: First assume the expression has a value and call it $x$ , so $x = \sqrt{a\sqrt{a\sqrt{a...}}}$ Then notice that $x = \sqrt{ax}$ , then $x^2 = ax$ , then $x(x-a) = 0$ , then since $a$ is positive the solution is $x = a$ and that's the value of the whole expression. They also offer a second solution that goes like this: If $x = \sqrt{a\sqrt{a\sqrt{a...}}}$ then we can also express it as: $x = a^{\frac{1}{2}}a^{\frac{1}{4}}a^{\frac{1}{8}}...$ , then $x = a^{\frac{1}{2}+\frac{1}{4}+\frac{1}{8}+...}$ , and since $\frac{1}{2}+\frac{1}{4}+\frac{1}{8}+... = 1$ , then $x = a$ once again. My first question is, how valid are any of these solutions at the university level? Say if this was a real analysis class, would any of those two answers be considered valid? Also, although I haven't taken a real analysis class yet, I have heard of the $\epsilon-N$ for sequences and series, I assume if we want to do the same for this particular problem we would have to look at the sequence $\sqrt{a}, \sqrt{a\sqrt{a}}, \sqrt{a\sqrt{a\sqrt{a}}}, ...$ , then check if it converges using $\epsilon-N$ and if it does then that's the value of he expression $\sqrt{a\sqrt{a\sqrt{a...}}}$ Second question is, is my speculation correct? Would this way of doing it with $\epsilon-N$ constitute a valid way of solving the problem if this was a real analysis class? Lastly, my last question is, how would you solve the problem in a way that is valid for a real analysis class? Either using $\epsilon-N$ or whatever else, please show me in a way someone who hasn't taken a real analysis class before can understand.","['infinite-product', 'real-analysis']"
4827654,How to solve $4$th degree polynomial if only $3$ points are known,"I'm currently revising my notes for upcoming exam in derivative calculus and I have stumbled upon one question I can not solve. Maybe someone can guide me/help me to understand how to solve this: It is known that the fourth degree polynomial $P(x)$ is greater than or equal to $x$ for all real $x$ . It is also known that $P(1)=1$ , $P(2)=4$ , and $P(3)=3$ . Determine the value of $P(4)$ . I have gotten this: Fourth degree polynomial formula: $P(x)\ge x,\quad P(x)=ax^4+bx^3+cx^2+dx+e$ $P(1)= a(1)^4+b(1)^3+c(1)^2+d(1)+e =1 \iff 1a+1b+1c+1d+e=1$ $P(2)= a(2)^4+b(2)^3+c(2)^2+d(2)+e =4 \iff 16a+8b+4c+2d+e=4$ $P(3) = a(3)^4+b(3)^3+c(3)^2+d(3)+e=3 \iff 81a+27b+9c+3d+e=3$ And here I'm stuck because I need two more equations to solve this problem.","['calculus', 'derivatives', 'polynomials']"
4827691,Winding a wire of definite width around a right cylinder,"The original question is presented like this: A copper wire, $3~mm$ in diameter is wound about a cylinder whose length is $12~cm$ and diameter $10~cm$ , so as to cover the curved surface area of the cylinder. Find the length  of the wire. Answer: $400~\pi$ I am a highschool student and with my teacher's help and my own start I came to the bookish answer. The process was like you count the no. of loops to be placed around the cylinder and then multipy the length of wire in 1 loop and no. of loops to get the final length of wire. Everthing was good, it came out to be 40 loops and now the problem starts. According to my teacher and my own second start (At first I thought the same that I am thinking right now but we need to follow our teacher so that's why I stuck with his reasoning.), the length of wire in 1 loop is 10 $\pi$ cm , which is the circumference of the cylinder. My reasoning: I feel that the length of the circular loop of wire will be $2\pi (R+r)$ where R is the radius of cylinder and r is the radius of the wire but according to my teacher it is just the circumference of the circle. To try and to justify this I tried using some items I found at my home and measuring the length of 2 different wires around a cylindrical pencil holder. Although I am getting a different length each time when I change the thickness but I cannot justify this to my teacher. I tried explaining him but he says that it will be same no matter what is the radius of the wire and I cannot explain him how. He is quite understanding btw, just the thing is I don't have any formal explanation to give him and whenever I try I come back with one explanation from him in hand. He says that the wire is touching the circumference so the length will be same as that. I saw that Veritasium's recent video on youtube about the SAT circle problem and can relate this with it  which is how I came us to my reasoning part, but I still don't know if I am correct or not or the length difference is just due to rough measuring? Any kind of 3D graphic visualizations would be appreciated very much and feel free to explain a little deeply. Who is correct? Me or my teacher? How? Sorry for bad English and the way I have put up the matter. Related Video: https://youtu.be/FUHkTs-Ipfg?si=3rTKZjwRwtgfRehg","['analytic-geometry', 'solid-geometry', 'algebra-precalculus', 'geometry']"
4827704,Multi-variate cross-partial derivative of a inverse function,"I have an invertible mapping $y=f(x,\theta)$ where $x,y\in\mathbb{R}^K$ with a scalar parameter $\theta\in\mathbb R$ . Consider its inverse $x=g(y,\theta)$ . I'm interested in the matrix $\partial^2g/\partial y'\partial\theta$ . Is it possible to express it in terms of the derivatives of $f$ ? For $K=1$ , the answer (in the more general case of an implicit $f$ ) is given here but is there a generalization for $K>1$ ?","['multivariable-calculus', 'implicit-differentiation', 'inverse-function-theorem', 'implicit-function-theorem']"
4827793,Local minimum implies global minimum.,"Let $f \in C^1(\mathbb{R}^n, \mathbb{R})$ (or even smoother if that helps) with $n > 1$ . Assume that $f$ has a local minimum in $x_0 \in \mathbb{R}^n$ and no other stationary points. Is $x_0$ then automatically a global minimum? For $n=1$ this is true and easy to show (e.g. with Rolle's theorem)... My intuition says the answer should be yes but I can't prove it.","['optimization', 'analysis', 'real-analysis']"
4827833,Does $\int (x^{dx}-1)$ make sense?,"I came across the YouTube video ""I Computed An Integral That Breaks Math"" by BriTheMathGuy, where the problem is computing $$\label{eq_1}\tag{1}\int (x^{dx}-1)$$ And basically to solve this integral we use a little trick writing the argument of $\eqref{eq_1}$ like $$\label{eq_2}\tag{2}\color{red}{\frac{x^{dx}-1}{dx}}\cdot dx$$ The red part of $\eqref{eq_2}$ could be interpreted as $$\label{eq_3}\tag{3}\lim_{h\to 0} \frac{x^h-1}{h} = \log (x)$$ And then we can compute the integral of $$\eqref{eq_1} = \int \log(x)dx = x\log(x) - x + c$$ Is this possible, does $\eqref{eq_1}$ even makes sense to write an integral with $dx$ in that unusual position, or is a glamour social media trick? What disturbs me about $\eqref{eq_2}$ is that even tough we can think of $dx$ as an infinitesimal quantity, deeply it isn't, it is a differential. Has someone some thoughts on this or is just a waste of time? Thank you so much.","['integration', 'indefinite-integrals', 'calculus', 'recreational-mathematics']"
4827885,Understanding a proof of the covariance matrix of a random process in SDE,"Considering the following SDE with multiplicative as well as additive stochastic uncertainty, $$
\dot{x}=A x+\sum_{\ell=1}^m \sigma_{\ell} B_{\ell} x \xi_{\ell}+H \eta
$$ where $x \in \mathbb{R}^n, H \in \mathbb{R}^n$ ; for $\ell=1, \cdots, m$ , $\xi_{\ell}=\frac{d \Delta_{\ell}}{\iota}, \eta=\frac{d \zeta}{u}$ with $\Delta_1, \cdots , \Delta_m$ and $\zeta$ being the standard independent Wiener process. How to prove the following Lemma: Let the covariance matrix, $\bar{Q}(t)=E\left[x(t) x(t)^{\top} \mid \psi\right]:=\int_{\mathbb{R}^n} x x^{\top} \psi(x, t) d x$ , and $\bar{Q}(0):=\bar{Q}_0<$ $\infty$ , then $\bar{Q}(t)$ satisfies the following matrix differential equation (MDE) $$
\dot{\bar{Q}}=\bar{Q} A^{\top}+A \bar{Q}+\sum_{\ell=1}^m \sigma_{\ell}^2 B_{\ell} \bar{Q} B_{\ell}^{\top}+H H^{\top}
$$ Here is the detailed proof: Taking the time derivative on both sides and after simplification, we obtain [1, Theorem 11.9.1] $$
\frac{d E[V \mid \psi]}{d t}=\int_{\mathbb{R}^n}\left\{\frac{1}{2} \sum_{i, j=1}^n\left[\sum_{\ell=1}^m \sigma_{\ell}^2\left(b_{\ell}^i x\right)\left(b_{\ell}^j x\right)+h_i h_j\right] \frac{\partial^2 V}{\partial x_i \partial x_j}+\sum_{i=1}^n\left(a_i x\right) \frac{\partial V}{\partial x_i}\right\} \psi(x, t) d x=E[\mathcal{L} V \mid \psi] .
$$ where the term $\mathcal{L} V$ is given by $$
\mathcal{L} V=x^{\top}\left(A^{\top} P+P A+\sum_{\ell=1}^m \sigma_{\ell}^2 B_{\ell}^{\top} P B_{\ell}\right) x+H^{\top} P H .
$$ The time derivative of $E[V \mid \psi]$ is obtained by doing integration by parts where we make use of Remark 4 .
In particular, we make use of the fact that the products, $\psi V, \frac{\partial \psi}{\partial x_i} V, \psi \frac{\partial V}{\partial x_i}$ vanish exponentially as $|x| \rightarrow \infty$ and hence, the higher order moments become zero. By substituting $\mathcal{L} V$ in the above euqation, and using the linearity of trace, expectation and commutativity inside trace, we obtain, $$
d\left(\operatorname{tr}\left(E\left[x x^{\top} \mid \psi\right] P\right)\right) / d t=\operatorname{tr}\left(\left(A^{\top} P+P A+\sum_{\ell=1}^m \sigma_{\ell}^2 B_{\ell}^{\top} P B_{\ell}\right) E\left[x x^{\top} \mid \psi\right]+H H^{\top} P\right) .
$$ By definition of expectation, $E\left[x x^{\top} \mid \psi\right]=\bar{Q}$ , we have, $$
\operatorname{tr}(\dot{\bar{Q}} P)=\operatorname{tr}\left(\left(\bar{Q} A^{\top}+A \bar{Q}+\sum_{\ell=1}^m \sigma_{\ell}^2 B_{\ell} \bar{Q} B_{\ell}^{\top}+H H^{\top}\right) P\right) .
$$ This can be rewritten in terms of an inner product as $$
\left\langle\dot{\bar{Q}}-\left(\bar{Q} A^{\top}+A \bar{Q}+\sum_{\ell=1}^m \sigma_{\ell}^2 B_{\ell} \bar{Q} B_{\ell}^{\top}+H H^{\top}\right), P\right\rangle=0
$$ since $P>0$ , $\dot{\bar{Q}}=\bar{Q} A^{\top}+A \bar{Q}+\sum_{\ell=1}^m \sigma_{\ell}^2 B_{\ell} \bar{Q} B_{\ell}^{\top}+H H^{\top}$ [1] A. Lasota and M. C. Mackey, Chaos, Fractals, and Noise: Stochastic Aspects of Dynamics. New York: Springer-Verlag, 1994 Since I have no access to the context of ref [1], I'm just confused of the above derivation of $\frac{d E[V \mid \psi]}{d t}$ , could someone offer me a more detailed calculation, thanks.","['stochastic-calculus', 'stochastic-processes', 'stochastic-differential-equations', 'probability-theory', 'probability']"
4827972,Understanding Gateaux derivatives,"Background I'm studying Gateaux derivatives and I find some difficulties to underestand the general case where the order of the derivative is $n>1$ . First, I'm considering the following definition for the first-order derivative at $h(x):\mathbb{R}^d\mapsto \mathbb{R}$ and in direction $g(x):\mathbb{R}^d\mapsto\mathbb{R}$ of the generic functional $F[h]$ , \begin{equation*}
F'[h;g]\triangleq \left[\frac{\text{d}}{\text{d}\varepsilon}F[h+\varepsilon g]\right]_{\varepsilon=0}
\end{equation*} we can simplfy this expression by removing the variable $\varepsilon$ (since in the end it will be fixed to the value $\varepsilon=0$ ). Since the internal derivative is a ""conventional"" derivative in $\varepsilon$ , we have \begin{equation*}
F'[h;g]=\left[\lim_{\tau\to 0} \frac{F[h+(\varepsilon+\tau)g]-F[h+\varepsilon g]}{\tau}\right]_{\varepsilon=0}
\end{equation*} then setting $\varepsilon=0$ gives \begin{equation*}
F'[h;g]=\lim_{\tau\to 0} \frac{F[h+\tau g]-F[h]}{\tau}
\end{equation*} Questions The $n$ -th order derivative is defined as \begin{equation*}
F^{(n)}[h;g]\triangleq \left[\frac{\text{d}^n}{\text{d}\varepsilon^n}F[h+\varepsilon g]\right]_{\varepsilon=0}
\end{equation*} Currently I have two questions about this expression. I'm wondering if we can do, like in the previous case, a simplification where we eliminate $\varepsilon$ I'm wondering if, as it happens with conventional derivatives, we can write a recursion of the type \begin{equation*}
F^{(k+1)}[h;g]=F^{(k)}[h;g]
\end{equation*} If the answer of the previous questions is yes, how do you prove it? It will be enough the simple case with $n=2$ .","['gateaux-derivative', 'calculus', 'derivatives']"
4828000,Conjecture: $\sum\limits_{k=1}^nk^m=S_3(n)\times\frac{P_{m-3}(n)}{N_m}$ for odd $m>1 \ ;\ =S_2(n)\times\frac{P_{m-2}'(n)}{N_m}$ for even $m$.,"When I was in high school, I was fascinated by $\displaystyle\sum\limits_{k=1}^n k= \frac{n(n+1)}{2}$ so I tried to find the general solution for $\displaystyle\sum\limits_{k=1}^n k^m$ s.t $m \in \mathbb{N}$ . I used a similar approach to
my answer here My approach was turning $n^m$ to $n(n-1)(n-2)\dots (n-m+1)+ f(n)$ and to use the fact that $$\displaystyle\dbinom{n}{k}+\dbinom{n}{k+1}= \dbinom{n+1}{k+1}$$ $f(n) $ will be combination of sums $\displaystyle\sum_{k=1}^n k^i$ st $i<m$ which I evaluated before I was able to to find the sum up to $m=6$ .  Here, I tried to search for a pattern to find the general solution of $\sum\limits_{k=1}^n k^m $ s.t $m \in \mathbb{N}$ which I failed to do, but I noticed this pattern: $\\[3pt]$ $$S_1(n):={\sum\limits_{k=1}^n k= \frac{n(n+1)}{2}}$$ $$S_2(n):={\sum\limits_{k=1}^n k^2= \color{blue}{\frac{n(2n+1)(n+1)}{6}}}$$ $$S_3(n):={\sum\limits_{k=1}^n k^3= \color{red}{\frac{\color{red}{n^2(n+1)^2}}{\color{red}{4}}}}$$ $$S_4(n):={\sum\limits_{k=1}^n k^4=\color{blue}{\frac{n(2n+1)(n+1)}{6}} \times \frac{3n^2+3n-1}{  5}}$$ $$S_5(n):={\sum\limits_{k=1}^n k^5= \color{red}{\frac{\color{red}{n^2(n+1)^2}}{\color{red}{4}}} \times\frac{2n^2+2n-1}{ 3}}$$ $$S_6(n):={\sum\limits_{k=1}^n k^6= \color{blue}{\frac{n(2n+1)(n+1)}{6}} \times \frac{3n^4+6n^3-3n+1}{ 7 }}$$ $$S_7(n):={\sum\limits_{k=1}^n k^7= \color{red}{\frac{\color{red}{n^2(n+1)^2}}{\color{red}{4}} }\times\frac{3n^4+6n^3-n^2-4n+2}{ 6}}$$ $$S_8(n):={\sum\limits_{k=1}^n k^8= \color{blue}{\frac{n(2n+1)(n+1)}{6}} \times \frac{5n^6+15n^5+5n^4-15n^3-n^2+9n+3}{ 15}}$$ $$S_9(n):={\sum\limits_{k=1}^n k^9= \color{red}{\frac{\color{red}{n^2(n+1)^2}}{\color{red}{4}}} \times\frac{(n^2+n-1)(2n^4 +4n^3-n^3-3n^2+3)}{ 5}}$$ $$S_{10}(n):={\sum\limits_{k=1}^n k^{10}= \color{blue}{\frac{n(2n+1)(n+1)}{6}} \times \frac{ 3 n^8+ 12 n^7+ 8 n^6 - 18 n^5- 10 n^4+ 24 n^3   + 2 n^2 - 15 n +5}{ 11}}$$ $$S_{11}(n):={\sum\limits_{k=1}^n k^{11}= \color{red}{\frac{\color{red}{n^2(n+1)^2}}{\color{red}{4}}} \times\frac{2n^8 +8n^7+4n^6-16n^5-5n^4+26n^3-3n^2-20n+10}{ 6}}$$ $\\[6pt]$ I noticed that: For odd $m>1$ , $\displaystyle\sum\limits_{k=1}^n k^m= \color{red}{\frac{{n^2(n+1)^2}}{{4}}} \times \frac{P_{m-3}(n)}{N_m}$ s.t $P_{m-3}(n)$ is an $m-3$ polynomial with integer coefficients $ \{a_{m-3},\dots a_1,a_0 \}$ such that $\gcd \{ a_{m-3},\dots a_1,a_0 \}=1$ , $N_m\in \mathbb {N}$ . For even $m$ , $\displaystyle\sum\limits_{k=1}^n k^m= \color{blue}{\frac{n(2n+1)(n+1)}{6}}\times \frac{P_{m-2}'(n)}{N_m}$ s.t $P_{m-2}'(n)$ is an $m-2$ polynomial with integer coefficients $\{ a_{m-2},\dots a_1,a_0 \}$ such that $\gcd \{ a_{m-2},\dots a_1,a_0 \}=1$ , $N_m\in \mathbb {N}$ . When I was in high school I couldn't prove this pattern, and this question reminded me of this observation that I had totally forgotten about. Now, after two years from my first attempt, I tried to prove this pattern again, but I couldn't. EDIT This question has been partly answered in this MathOverflow answer (The answer shows that $\displaystyle\sum\limits_{k=1}^n k^m$ is divisible by ${n^2(n+1)^2}$ for odd $m>1$ and $\displaystyle\sum\limits_{k=1}^n k^m$ is divisible by ${n(2n+1)(n+1)}$ for even $m$ ) the only missing part is to show that the denominator is a multiple of $4$ if $m\in 2\mathbb{N}+1$ , and the denominator is a multiple of $6$ if $m \in 2\mathbb{N}$ . Although this question has been almost solved before, I still would like to see if there is any other proofs or methods. Update: I asked this question on MO here","['conjectures', 'summation', 'number-theory', 'calculus', 'sequences-and-series']"
4828059,How do I know whether a local minima is also global?,"In the region $0\leq x\leq 1$ and $0\leq y\leq 1$ I have 1 radio broadcaster placed in $(x_b,y_b)$ and N receivers. The position of the broadcaster is not known.
The position of each receiver however is known: $(x_i,y_i)$ as is the distance from the receiver to the broadcaster: $d_i$ .
There is some noise in the distance measurements that I assume to be normal distributed with mean=0 and the same variance for all receivers. My goal is to estimate the most likely position of the radio broadcaster: $(x_b,y_b)$ . I do this by minimizing: $$
SE = \sum_i \left[\sqrt{(x_b - x_i)^2+(y_b - y_i)^2} - d_i\right]^2
$$ using scipy.optimize.minimize() . This approach seems to work fine. However how do I know that the local minima I find is also the global minima?
In this example I can plot $SE(x_b,y_b)$ : However my real problem is estimating the position: $(x,y,z)$ and the orientation of a camera: $(roll,pitch,yaw)$ .
There is no way I can visualize 6D space.
Therefore I am looking for some other way to check whether a found minima is global. As far as I understand there are only two ways to show that there is only 1 local minima? Method I Show that $$
\frac{\partial SE}{\partial x_b} = \frac{\partial SE}{\partial y_b} = 0
$$ only has 1 solution and that all the eigenvalues of the Hessian of SE in this point are positive. Method II Show that SE is convex by showing that all eigenvalues of the Hessian of SE are positive at all points. Let us try method I. I find that: $$
\frac{\partial SE}{\partial x_b} = \Sigma_i 2 (x_b - x_i) 
\frac{\sqrt{(x_b - x_i)^2 + (y_b - y_i)^2} - d_i}{\sqrt{(x_b - x_i)^2 + (y_b - y_i)^2}}
$$ $$
\frac{\partial SE}{\partial y_b} = \Sigma_i 2 (y_b - y_i) 
\frac{\sqrt{(x_b - x_i)^2 + (y_b - y_i)^2} - d_i}{\sqrt{(x_b - x_i)^2 + (y_b - y_i)^2}}
$$ How many solutions do $$
\frac{\partial SE}{\partial x_b} = \frac{\partial SE}{\partial y_b} = 0
$$ have?
Difficult to tell, I guess it dependens on the $\{(x_i,y_i,d_i)\}$ values? Method II seems even harder.","['optimization', 'multivariable-calculus', 'nonlinear-optimization', 'numerical-methods']"
4828130,"Constructing ""Double"" circumscribed pentagons.","I want to construct the following thing : A non regular pentagon $ABCDE$ circumscribed about a circle such that if $$A_1 = BC\cap ED, B_1=CD\cap AE...$$ then $A_1B_1C_1D_1E_1$ is also circumscribed about a circle. I tried to do that in Geogebra for a bit but didn't succeed.","['geometry', 'geometric-construction']"
4828168,Is it possible to fit cosine (or sine) graph into an algebraic curve?,"A set of the form $\{(x,y)\in \mathbb{R}^2\,:\,P(x,y)=0\}$ for a nonzero polynomial $P\in \mathbb{R}[x,y]$ is said to be an algebraic curve. This set can be denoted simply by "" $P(x,y)=0$ "". Question: Can we fit the graph of cosine, namely, $\{(t,\cos t)\in \mathbb{R}^2\,:\,t\in\mathbb{R}\}$ , into an algebraic curve? Intuition says ""no"" and that's correct. Indeed, suppose there is some nonzero polynomial $P$ such that $P(t,\cos t)\equiv 0$ . Consider the polynomial $Q_\lambda(x,y):=y-\lambda$ , for $\lambda\in [-1,1]$ . Due to cosine's periodicity, the algebraic curve $Q_\lambda(x,y)=0$ intersects the graph of cosine infinitely many times. This tells us that the algebraic curves $P(x,y)=0$ and $Q_\lambda(x,y)=0$ have an infinite intersection and, by an Algebraic Geometry theorem, $P$ and $Q_\lambda$ must have a nonconstant common factor. Since $Q_\lambda$ is irreducible, this factor must be $Q_\lambda$ itself. So we are saying that $y-\lambda$ is a factor of $P$ for every $\lambda \in [-1,1]$ ... Since $P$ is a polynomial, this is impossible, unless $P\equiv 0$ , which we discarded by hypothesis. Very good. But what if we wanted to answer the ""local"" version of the question? Question (local): Can we fit a piece of the graph of cosine, say, $\{(t,\cos t)\in\mathbb{R}^2\,:\,t\in (a,b)\}$ , into an algebraic curve? Intuition still says that we can't, but I don't know how to prove this. Since we're limited to a piece of the graph, we lose the argument of that infinite intersection...","['algebraic-curves', 'algebraic-geometry']"
