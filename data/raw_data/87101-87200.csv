question_id,title,body,tags
1160644,Find the sum of the sides in a spherical right triangle,"In a spherical triangle the angles at α, β and γ are π/5, π/3, π/2.
  Find the sum of the sides, we shall call the sides a,b,c So I'm looking at the formulas and I see one of Napier's rule which might work here. Just to make things easier: $$A=α=π/5$$
$$B=β=π/3$$
$$C=γ=π/2$$ So I want to use Napier's circle So we need to find a,b,c and here is my work: -> To find c:
$$\sin(\pi-c)=\tan(\pi-B)\tan(\pi-A)$$
$$\cos(c)=\cot(B)\cot(A)$$
$$\cos(c)=\frac{\cos(\pi/3)}{\sin(\pi/3)}\frac{\cos(\pi/5)}{\sin(\pi/5)}$$
$$ c=37.38° $$ -> To find b:
$$\sin(\pi-A)=\tan(b)\cos(\pi-B)$$
$$\cos(A)=\tan(b)\sin(B)$$
$$\tan(b)=\frac{\cos(\pi/5)}{\sin(\pi/3)}$$
$$ b=43.05° $$ -> To find a:
$$\sin(\pi-A)=\cos(a)\cos(\pi-B)$$
$$\cos(A)=\cos(a)\sin(B)$$
$$\cos(\pi/5)=\cos(a)\sin(\pi/3)$$
$$ a=20.9° $$ -> sum of sides:
$$a+b+c=20.9+43.05+37.38=101.3°$$ If anyone can verify if I did the whole process right that would be great. Also, note that the sum of the sides is 101.3°. I know spherical triangles have to be between 180° and 540° but that is for the sum of the angles, are there any rules like that for the sum of spherical triangle sides?","['geometry', 'trigonometry', 'spherical-trigonometry', 'spherical-geometry']"
1160645,The support and the non-vanishing set of a function on a scheme,"I have some confusion regarding the two concepts: Let $(X,\mathscr{O}_X)$ be a scheme, let $f\in \Gamma(\mathscr{O}_X,X)$ and define the support of $f$ to be
$$\operatorname{Supp}(f) : = \{p\in X: 0\ne f\in\mathscr{O}_{X,p}\}.$$ But we can also define the non-vanishing set of $f$ as
$$NV(f): = \{p\in X: f\not\in \mathfrak{m}_p \text{ the unique maximal ideal of }\mathscr{O}_{X,p}\}.$$ We clearly have $NV(f)\subset \operatorname{Supp}(f)$. Now $NV(f)$ is open and $\operatorname{Supp}(f)$ is closed. How do I understand these two concepts intuitively?","['algebraic-geometry', 'schemes', 'intuition']"
1160692,"Show that for all $ t \in [a,b] $ it holds that $ |y(t)-z(t)| \leq |y_0-z_0|$","Let $f: [a,b] \times \mathbb{R} \to \mathbb{R}$ be a continuous function so that: $$ \forall t \in [a,b] \ \forall y_1, y_2 \in \mathbb{R} (f(t,y_1)-f(t,y_2))(y_1-y_2) \leq 0 $$ (So, for each constant value $t$ of the first variable, the function  $ f(t, \cdot)$ is decreasing.) Let $y$ and $z$ be solutions of the initial value problems $$\left\{\begin{matrix}
y'=f(t,y) &, t\in [a,b] \\ 
y(a)=y_0 & 
\end{matrix}\right. $$ and $$\left\{\begin{matrix}
z'=f(t,z) &, t\in [a,b] \\ 
z(a)=z_0 & 
\end{matrix}\right.$$ respectively. Show that for all $ t \in [a,b] $ the following holds: $$ |y(t)-z(t)| \leq |y_0-z_0|$$ Could you give me a hint what we could do?","['ordinary-differential-equations', 'numerical-methods']"
1160699,Is it true that $2^n-1$ is prime whenever $n$ is prime?,"In my discrete math book, I was tasked with finding a counterexample for this: If $n$ is prime, then $2^n-1$ is prime. Does there exist a counterexample for such a statement? Also, am I wrong in thinking that when something asks for a counterexample, is it looking for some logic that proves the original statement to be false? Any help is appreciated, as I've got a test on subjects like this tomorrow.","['prime-numbers', 'elementary-number-theory', 'examples-counterexamples', 'discrete-mathematics']"
1160713,Infinitely many primes $5n+1$,"Prove without quadratic residues or Dirichlet' theorem that there are infinitely many primes in the form 
$$5n+1, \ \ \ \ \ n\in \mathbb{N}$$ My try: Obviously $n$ has to be even, or $5n+1$ is even. We can use $n':=\frac{n}{2}$, then
$$5n+1=10n'+1$$
This should increase the odds of the number being prime. Since $5n+1$ the number of primes is about $\frac{li(n)}4$. For $10n'+1$, it should be $\frac{li(n')}2$. Does this help at all? or should I stay with $5n+1$?",['number-theory']
1160731,Each of the following functions f is bijective. Describe its inverse.,"QA,B: Each of the following functions f is bijective. Describe its inverse. A:
$$f:\mathbb{R} \rightarrow (0,\infty); \text{ defined by } f(x)=e^x $$ For this function, I said the inverse is:
$$f^{-1}:(0,\infty) \rightarrow \mathbb{R}; \text{ defined by } f^{-1}(x)=\ln(x) $$ B: $$f:\mathbb{R} \rightarrow \mathbb{R}; \text{ defined by } f(x)=x^3+1 $$ At first, for the inverse, I said: $$f^{-1}:\mathbb{R} \rightarrow \mathbb{R}; \text{ defined by } f^{-1}(x)=\sqrt[3]{x-1}$$ but then I notice if x=-1 then the value inside the root will be negative which is illegal, therefore I adjusted by domain and codomain and obtained: $$f^{-1}:\mathbb{R^{\geq 1}} \rightarrow \mathbb{R^{\geq 0}}; \text{ defined by } f^{-1}(x)=\sqrt[3]{x-1}$$ Can anyone verify if these answers are correct?","['inverse', 'functions']"
1160763,Bounded integral operator,"So if $k:\mathbb{R}^2\rightarrow\mathbb{R}$ such that $$\sup_{x}\int |k(x,y)|dy<\infty \ \  \ \sup_{y}\int|k(x,y)|dx<\infty$$
How do you show in this case, $Kf(x)=\int k(x,y)f(y)dy$ is a bounded from $L^2$ to $L^2$. 
Any hints? Clearly $k$ cannot be $L^2$.","['multivariable-calculus', 'functional-analysis', 'real-analysis']"
1160776,Set Theoretic Definition of Complex Numbers: How to Distinguish $\mathbb{C}$ from $\mathbb{R}^2$?,"I have spent some time looking for a rigorous, set-theoretic definition of the complex numbers. I have read the book Elements of Set Theory by Herbert Enderton (1977) which does an excellent job of constructing numbers from sets including the natural numbers, integers, and rational numbers, but stops at the real numbers. So far, I have only found two comparable constructions of complex numbers The set of all $2 \times 2$ matrices taking real-valued components The set of all ordered pairs taking real-valued components I favor the second construction better, because I feel it has a stronger geometric interpretation because of its similarities to Euclidean vector spaces. That is, define
\begin{equation*}
\mathbb{C}=\{(x,y):x,y \in \mathbb{R}\},
\end{equation*}
which also is exactly how the Euclidean plane, $\mathbb{R}^2$, is defined. This leads me to my question. With $\mathbb{C}$ defined exactly the same as how one defines $\mathbb{R}^2$, how does one distinguish the elements of these two sets? For example, how does one distinguish the ordinary vector $(0,1) \in \mathbb{R}^2$ from what we define to be $i$, namely the number $i=(0,1) \in \mathbb{C}$, when they are set-theoretically identical? In set theory, these two very different ""numbers"" -- the vector $(0,1)$ and the number $i$ -- are exactly the same set! Thanks for your thoughts!","['complex-numbers', 'elementary-set-theory', 'analysis']"
1160815,Measurable functions on complete space.,"$(\Omega, \mathcal{E}, \mathbb{P})$ a probability space,  $\mathcal{F}$ is $\mathcal{E}$ completed by the $\mathbb{P}-$null sets. Which conditions should meet a topological space $X$ (taken with its Borel $\sigma$ algebra) so that for any map $f$ from $\Omega$ to $X$ conditions (1) and (2) below are equivalent ? (1) $f$ is $\mathcal{F}-$measurable (2) There exists a $\mathcal{E}-$measurable $g$ (from $\Omega$ to $X$) so that $f=g$ holds everywhere, or only fails on some null set In particular does it work for any separable normed vector space ? I don't find the proof when $X$ is not Hilbert. Any classical reference is welcome. Thank you.","['probability-theory', 'measure-theory']"
1160824,Prove an infinite set A is countable,"In one of my classes I was asked to prove the following theorem. Given a set $A$, which is a infinite set, another set $B$, which is a subset of $\mathbb{N}$. If there exists a function/map $f$ s.t. $f: B\to A$, the function $f$ is onto. Then the set $A$ is countable. Our definition of countable is that a set $X$ is countable if and only if it is numerically equivalent to any subset of $\mathbb{N}$. Our definition of numerical equivalence is that the set $X$ is numerically equivalent to the set $Y$ if there exists a map $g$ s.t. $g: X\to Y$, which is one-one(injective) and onto(surjective). Here is my proof: Consider any element $a$ from $A$. Since $f$ is onto, then there exists a set of inverse image of $a$, call this set $F(a)$. If $F(a)$ has only one element, then we proceed to the another element from $A$, and consider its inverse image. If $F(a)$ has more than one elements, then take the least element from $F(a)$, then processed to another element from $A$. Iterating this process for all the elements of $A$. We have a set $F(A)$ which is the set of all the inverse image of $A$ satisfying the above conditions. Define a map $F: A\to F(A)$, then $F$ is both one-one and onto. Thus $A$ is numerically equivalence to $F(A)$ which is a subset of $\mathbb{N}$. Thus the set $A$ is countable. Is my proof rigorous? If not, can anyone kindly show me how to do this rigorously? Our professor also mentions we can prove this through Axiom of Choice, but I have no idea how to do that since this is a real-analysis course, and we are not required to do it.",['elementary-set-theory']
1160835,"$f$ has simple pole at origin. Compare $\rm Res (f,0)$ with $\rm Res (f(1/f), 0 )$","I'm preparing for a qualifying exam (instead of working assignments I have due now) and I have a small question about a problem that was on a past qualifying exam: Suppose $f$ has a simple pole at the origin, and $g$ denotes $1/f$ (the reciprocal function). How is the residue at the origin of the composite function $f\circ g$ related to the residue at the origin of $f$? I have a possible solution. I'm just just unsure of it: Since $f$ is holomorphic in a small enough annulus of the origin it has a convergent Laurent series centered at $z=0$, i.e.,
$$
\frac{c_{-1}}{z}+c_0+c_1z+c_2z^2+\cdots
$$
for $c_{-1},c_0,\ldots\in\mathbb{C}$. However, in a small enough annulus, $f\simeq c_{-1}/z$. Therefore
$$
f\circ g=f\left(\frac{1}{f}\right)\simeq f\left(\frac{1}{\frac{c_1}{z}}\right)=f\left(\frac{z}{c_1}\right)=\frac{c_{-1}}{\left(\frac{z}{c_{-1}}\right)}=\frac{(c_{-1})^2}{z}
$$
and $($Res$(f,0))^2=$Res$(f\circ g,0)$. Is there something wrong in working with $c_{-1}/z$ instead of the full Laurent series? Would I at least have to work with
$$
f(z)=\frac{c_{-1}}{z}+c_0+\mathcal{O}(z)?
$$
I tend to be afraid of working with approximations but if we're in small enough annulus it shouldn't make a difference (right?). Any help would be appreciated. Thanks in advance!",['complex-analysis']
1160840,Explicit solution to linear stochastic differential equation (in several dimensions),"I have found many references where they provide with a ""explicit"" solution of the following SDE:
$$dX_t = (a_1(t) X_t + a_2(t) )dt + (b_1(t) X_t + b_2(t))dB_t, \quad X_0=x, \quad (1)$$
where $B$ is a standard Brownian motion. It is namely given by
$$X_t = \Phi_t \left(x + \int_0^t (a_2(s) - b_1(s)b_2(s)) \Phi_s^{-1} ds+ \int_0^t b_2(s)\Phi_s^{-1} dB_s \right)$$
where
\begin{align}
\Phi_t = \exp \left\{\int_0^t \left(a_1(s)-\frac{1}{2}b_1(s)^2 \right)ds + \int_0^t b_1(s)dB_s\right\}. \quad (2)
\end{align} Nevertheless, in several dimensions it might not be possible to find a close explicit solution of the homogeneous SDE and hence expression (2) does not make sense. Can we solve Equation (1) when $a_1,a_2,b_1$ and $b_2$ are square matrices? In exactly the same way by just defining $\Phi_t$ the solution to $d\Phi_t = a_1(t)\Phi_1 dt + b_1(t) \Phi_t dB_t$, $\Phi_0=Id$? I found this link http://math.uni-heidelberg.de/studinfo/reiss/sode-lecture.pdf (page 26) for the case $b_1(t)=0$. Does anyone know any references on this?
Thanks!","['stochastic-processes', 'ordinary-differential-equations', 'calculus', 'probability', 'functional-analysis']"
1160862,Definition of an induced matrix norm.,"Could someone explain the second equality in the definition of a induced matrix norm to me? Let $\| \cdot \|$ be a norm for $\cdot \in \mathbb{R}^n$, 
  then the induced matrixnorm for $A\in \mathbb{R}^{n\times n}$ is given by: $$\|A\| = \sup_{x\not = 0} \frac{\|Ax\|}{\|x\|} \color{red}{\stackrel{?}{=}} \max_{\|x\|= 1} \|Ax\|$$ Problem: Why does $\color{red}{=}$ hold? I know $\|Ax\|$ is continous for all $x\in \mathbb{R}^n$ and
 $\{x \in \mathbb{R}^n :\|x\| = 1 \}$ is compact which implies $\dfrac{\|Ax\|}{\|x\|}$ 
 to reach a maximum according to Weierstrass. But why is this maximum also the maximum for all $x$? (even those with norm bigger or less then 1?)","['matrix-norms', 'definition', 'supremum-and-infimum', 'matrices', 'normed-spaces']"
1160900,Showing $f=0$ almost everywhere,"Let $\psi_n(x)=e^{-x^2/2}P_n(x)$ where $P_n$ is a degree $n$ polynomial with real coefficients. Assume that $$\int_{\mathbb{R}}e^{-x^2/2}P_n=0.$$ Suppose that for any $f\in L^2$, such that $$\int_\mathbb{R} e^{-x^2/2}P_n(x)f(x)=0,$$ then how would you show that $f=0$ a.e? Any ideas/hints? Thanks","['multivariable-calculus', 'complex-analysis', 'functional-analysis', 'real-analysis']"
1160911,What's so special about the group axioms?,"I've only just begun studying group theory (up to Lagrange) following on from vector spaces and I am still finding them almost frustratingly  arbitrary. I'm not sure what exactly it is about the axioms that motivated them defining groups. My textbook asks you to list 'common features' of vector spaces and then later defines a set of axioms for vector spaces under addition, scalar multiplication and both, noting that the axioms under addition form an Abelian group. So are groups just a generalisation of vector spaces under any binary operation? My main problem is that the book notes that 'the axioms may seem rather arbitrary' and links groups with vector spaces but doesn't elaborate. When introducing groups, you are tasked to complete Cayley tables for the symmetries of an equilateral triangle and square.  Then, similarly to the delivery of vector spaces, notes that the tables have common properties (Closure, Identity, inverse and association) and defines a group as a set of elements under a binary operation that has these features. So what's so important about these 4 properties? For example, if 1 or 2 the properties were excluded form the  axioms, or we added an extra few properties as axioms how would that cripple the effectiveness of groups? Are the group axioms ever difficult to work with or do they always work, forgive the crude Littlewood analogy, like a mathematical skeleton key? What is it about these 4 properties that make groups such a powerful tool in mathematics and physics? My best guess is that a group is the best way to express our sense of  symmetry and what is symmetric mathematically, but I would prefer some elaboration.","['big-picture', 'axioms', 'soft-question', 'symmetry', 'group-theory']"
1160943,"Show a closed, convex, absorbing set in a Topological space nonmeger in its self contains a neighborhood of $0$.","Been sitting on this one for a few days and would really appreciate some help. I have included a definition and theorem that seemed useful. If anyone would be willing to critique or confirm my proof it would really make my day. Thanks in advance. Suppose $X$ is a topological vector space which is of the second category in itself. Let $K$ be a closed, convex, absorbing subset of $X$. Prove that $K$ contains a neighborhood of $0$. Suggestion: Show first that $H = K \cap (-K)$ is absorbing. By a catagory argument, $H$ has interior. Then use
$$
2H = H + H = H - H.
$$
Show that the result is false without convexity of $K$, even if $X = \mathbb{R}^2$. Show that the result is false if $X$ is $L^2$ topologized by the $L^1$-norm Definition: A convex set $K\subset X$ is called absorbing, if given $x\in X$ there exists $\lambda>0$ such that $\lambda x\in K$. Theorem: Suppose $V$ is a neighborhood of $0$ in a topological vector space $X$. If 
$$
0 < r_1 < r_2 < \dots < r_n \rightarrow \infty, then
$$ 
$$
X = \bigcup_n r_n V.
$$ Proof: Let $X$ be a topoligical vector space which is of second category in itself. Let $K$ be a closed, convex absorbing subset of $X$. Since $K$ is absorbing, we observe that $X = \bigcup_n nK$. Consider $H = K \cap (-K)$. We observe that $H$ is also a closed convex set since the intersection of convex sets are convex, and finite intersections of closed sets are closed. Since $K\subset X$ is convex  we note that $K$ is second category in its self since $K$ is everywhere dense in its self. We observe that $\overline{H}^\circ \not = \emptyset$ since the intersection of second category sets is second category. Hence H has non empty interior. Since $2H = H+H = H-H$; $H$ must be a neighborhood of $0$. By the theorem stated above, we may say that $H$ is absorbing since $H$ is a neighborhood of $0$. $H \subset K \Rightarrow K$ contains a neighborhood of $0$. To emphasize the requirement of $K$ convex suppose that $X = \mathbb{R}^2$. Suppose $K$ is the set consisting of the whole space with the nonzero points of the parabola $y=x^2$. Then obviously $K$ is absorbing but does not contain a neighborhood of $0$. If $X$ is $L^2$ topologized by the $L^1$ norm consider a set $K = \{f: \|f\|_2 \leq 1\}$ then we notice that $K$ is convex and absorbing, but there exists a sequence of functions $f_n \in X$ with $\| f_n\|_1 = 1/n \rightarrow 0$ and $\| f_n\|_{2} = 2$ for all $n$. So no ball around $0$ can be contained in $K$.","['general-topology', 'functional-analysis']"
1160944,Prove that $(A∪B)-B=A$ iff sets $A$ and $B$ are disjoint.,"I was working on this assignment and was wondering if an assumption I made was correct and if my proofs as a whole were correct. The first part of the question was: Use the algebraic method to prove the following set equality: $A∪B-B=A-B$ I've done this part already and use the result of said proof to formulate the start of the next proof. Let $A$ and $B$ be two sets. Prove that $(A\cup B)-B=A$ iff $A$ and $B$ are disjoint. Based on the previous proof: $Let:(A∪B)-B=A-B=A$ $1)\ A-B=A\cap\bar B\text{; by Set Difference}$ $2)\ A\cap\bar B=A\cap(\bar B\cup\emptyset) \text{; by Identity}$ $3)\ A\cap(\bar B\cup\emptyset)=A\cup\overline{(B\cap\bar\emptyset)}\text{; by DeMorgan’s 
Law}$ $4)\ A\cup\overline{(B\cap\bar\emptyset)}=A\cup\overline{(B\cap U)}\text{; by 0/1 Law}$ $5)\ A\cup\overline{(B\cap U)}=A\cup\bar B\text{; by Identity}$ $6)\text{ Then, } A\cap\bar B=A\cup\bar B=A,\text{ which can only be true if }\bar B=A\text{; by Repetition Law}$ $7)\ \bar B=A\text{ implies }B\subseteq\bar A \therefore A\text{ and }B\text{ must be disjoint.}$ I'm wondering if this proof is acceptable (especially the conclusion I draw from points 6 and 7) or if I'm missing out on something (first time taking a course that wants rigorous proofs). Thanks!","['discrete-mathematics', 'elementary-set-theory', 'proof-verification']"
1160962,Does the sum of two random variables that converge in distribution also converge in distribution?,"Suppose $X_n$ and $Y_n$ are defined on the same probability space, that $X_n$ converge in distribution to $X$ and that $Y_n$ converge in distribution to $Y$. I'm trying to find an illustrative example showing that $X_n+Y_n$ need not converge in distribution to $X+Y$. Any suggestions would be appreciated! Certainly $X_n$ and $Y_n$ cannot be independent...","['probability-theory', 'probability-distributions']"
1160986,What is the Spivak of Probability? [duplicate],"This question already has answers here : Good books on ""advanced"" probabilities (12 answers) Closed 3 years ago . I'm looking for a rigorous introduction to probability to help prepare me for a future course I plan on taking, ""Advanced Introduction to Probability"". Something possibly like Spivak's, where proofs are key. Any help is appreciated.","['probability-theory', 'reference-request']"
1160988,Blowing up the Grassmannian at a point,"I want to know what the blow-up of the Grassmannian at a point looks like. Consider $G=Gr(r,n)$ and $V\in G$. I want to understand more explicitly what $Bl_V(G)$ should mean. Of course for affine space $\mathbb{A}^n$, the blow-up at the origin is a subset of $\mathbb{A}^n\times \mathbb{P}^{n-1}$ defined by $B=\{(x,L)\in \mathbb{A}^n\times \mathbb{P}^{n-1}| x\in L\}$. Similarly we can view the blow-up of a point in projective space $\mathbb{P}^n$ as a subvariety of $\mathbb{P}^n\times \mathbb{P}^{n-1}$. If we take the viewpoint that the blow-up at a point should be the projectivization of the tangent space, and use the fact that $T_VG = Hom(V,\mathbb{C}^n/V)$, we probably want to describe $Bl_V(G)$ as a subset of $\mathbb{P}(T_VG )\times G$ given by some convenient set of equations. Of course we can always look locally and describe the blow-up that way, but I want something more global. In particular, the map from $Bl_V(G)$ to $G$ should be an isomorphism away from $V$, and only blow-up $V$. Any references would also be appreciated.","['algebraic-geometry', 'reference-request']"
1160996,How to find the inverse cosine without a calculator,"How to find the inverse of: $$\cos(c)=\frac{1}{3}$$ In other words, i'm trying to solve for c and without a calculator. If it's hard or not possible, then how would you go about solving inverses in general. For example, lets say: $$\cos(c)=\frac{3+1}{2\sqrt{2}}$$ how would we solve for c, which in this case is 105","['trigonometry', 'inverse']"
1161029,conformal map of unit disk slit,"Map the unit disk slit along $(-1,-r ]$, $r \in (0, 1)$, onto the unit disk. Can anyone explain how to do the conformal map thoroughly since I have difficulty understanding it. Thanks","['conformal-geometry', 'complex-analysis']"
1161049,Evaluating $\int_{0}^{\infty}\frac{e^{-x^2}-e^{-x}}{x}dx $ [duplicate],"This question already has answers here : Unusual integral (2 answers) Closed 9 years ago . I am working on this improper integral, $$\int_{0}^{\infty}\frac{e^{-x^2}-e^{-x}}{x}dx$$ First, I separate the integral into two pieces, I have $$\int_{0}^{\infty}\frac{e^{-x^2}}{x}dx -\int_{0}^{\infty}\frac{e^{-x}}{x}dx=I_1+I_2$$ I know that $I_2$ can use double integral, $$ I_2=\int_0^\infty  \int _1^\infty e^{-tx}  dt dx = \int _1^\infty \frac{1}{t}dt $$ $$ \int_0^\infty   (\frac{e^{-tx}}{-x}) \Bigg|_{1}^\infty   dx = \int_1^\infty \frac{1}{t}dt =\ln t \Bigg|_{1}^{\infty}=\infty$$ But I don't know how to do $I_1$, can someone give me a hit or suggestion? Thanks!","['calculus', 'integration', 'analysis']"
1161082,"""Simplify your negation so that no quantifier lies within the scope of a negation."" What does this mean?","I'm getting an alert from the script that says my question appears to be subjective, but I don't think it is. What I seek is an explanation as to what the statement in the title in quotes means. It asked me to do this for a specific statement in predicate logic, but I couldn't find a way to type out some of the symbols, which i am researching on my own right now. Any help is appreciated, as I have a test on material like this tomorrow morning.","['predicate-logic', 'discrete-mathematics']"
1161096,show $\lim_{x\to 0}\frac{e^x-1}{x}=1$ without L'Hopital,"how would you show that $$\lim_{x\to 0}\frac{e^x-1}{x}=1$$ without using derivatives or l'hopital but using basic ideas that are generally introduced just before derivatives in a typical introductory calculus course. my attempt.. $$\lim_{x\to 0}\frac{e^x-1}{x}=\lim_{x\to 0}\lim_{n\to \infty}\frac{\left(1+\frac{x}{n}\right)^n-1}{x}$$ then.. this is $$ =\lim_{x\to 0}\lim_{n\to \infty}\frac{1+{n \choose 1}\frac{x}{n} + {n \choose 2}\left(\frac{x}{n}\right)^2 +\cdots+ {n \choose n}\left(\frac{x}{n}\right)^n -1}{x}$$ $$ =\lim_{x\to 0}\lim_{n\to \infty} \left[ 1+ \frac{   {n \choose 2}\left(\frac{x}{n}\right)^2 +\cdots+ {n \choose n}\left(\frac{x}{n}\right)^n }{x}\right]$$ then we could switch the limit order but the justification for this seems to be not very basic and I would like to provide a basic explanation, as basic as possible. by the way I am doing this to provide a pre-derivatives proof of $$\lim_{x\to c}\frac{e^x-e^c}{x-c}=e^c$$ thank you.","['limits-without-lhopital', 'calculus', 'limits']"
1161112,Why $\|f-g\|=0$ if and only if $f=g$?,"I'm learning Fourier Transformation lately, and in the Course Reader page 23, it defines
$\|f\|=\left(\int_0^1 \left|f(t)\right|^2 dt\right)^{1/2}$. And then $\|f-g\|=0$ if and only if $f=g$. My question is what does it mean exactly by $f=g$? Is it a new definition of $f=g$?
Since, for example $f=0$ and $g(x)=1$ for $x=0.5$ and $0$ otherwise, will result $f=g$.","['functional-analysis', 'lp-spaces']"
1161115,A Representation Theory Problem in Putnam Competition,"The following was the B6 problem of 1985 Putnam Competition: Suppose $G$ is a finite group (under matrix multiplication) of real $n\times n$ matrices $\{M_i\}, 1\leq i\leq r$. Suppose that $$\sum_{i=1}^r tr (M_i)=0$$, prove that $$\sum_{i=1}^rM_i=0.$$ Here is an official proof from the committee which I didn't understand: Lemma: Let $G$ be a finite group of order $r$. Let $\rho: G\rightarrow GL(V)$ be a representation of $G$ on some finite dimensional vector space $V$. Then $$\sum_{g\in G}tr \rho_g$$ is a non-negative integer divisible by $r$, and is zero iff $$\sum_{g\in G}\rho_g=0$$. Proof: Let $\chi_1,\cdots, \chi_s$ be the irreducible characters of $G$ and $\chi= \sum_{i=1}^s a_i\chi_i$ and $\psi=\sum_{i=1}^sb_i\chi_i$ be arbitrary characters. Then by the orthogonality relations of characters, we have
$$\frac{1}{|G|}\sum_{g\in G}\chi(g)\overline{\psi (g)}=\sum_{i=1}^sa_ib_i$$.
Applying this to the character of $\rho$ and the trivial character $\mathbb{1}$ shows that $\frac{1}{|G|} \sum_{g\in G}tr \rho_g$ equals the multiplicity of $\mathbb{1}$ in $\rho$, which is a non-negative integer.
Now suppose that the matrix $S=\sum_{g\in G}\rho_g$ is non-zero. Choose $v\in V$ with $Sv\not=0$. The relation $\rho_hS=S$ shows that $Sv$ is fixed by $\rho_h$ for all $h\in G$. In other words, $Sv$ spans a trivial subrepresentation of $\rho$, so the non-negative integer of the previous paragraph is positive.  QED We now return to the problem at hand. ""Unfortunately the $M_i$ do not necessarily define a representation of $G$, since the $M_i$ need not be invertible."" Instead we need to apply the lemma to the action of $G$ on $\mathbb{C}^n/K$ for some subspace $K$ ... I do not understand wthe sentence in """". Isn't the set of $M_i$'s form  a group under multiplication? why they need not to be invertible? The above proof is copied from Kedlaya, Poonen and Vakil's Putnam competition 1985-2000. Thanks for helping","['linear-algebra', 'representation-theory', 'contest-math']"
1161116,Suppose that the Wronskian of any 2 solutions of $\frac{d^2y}{dt^2}+p(t)\frac{dy}{dt}+q(t)y=0$ Prove that P(t)=0.,"Suppose that the Wronskian of any 2 solutions is constant of $\frac{d^2y}{dt^2}+p(t)\frac{dy}{dt}+q(t)y=0$ Prove that P(t)=0. So my attempt: $$W(t)=y_1y_2'-y_1'y_2$$ So what I thought I would do is set W(t) = C, some constant: $$C=y_1y_2'-y_1'y_2$$
$\frac{C+y_1'y_2}{y_2'}=y_1$ and ${y_1y_2'-C}{y_2}=y_1'$
Then I thought I can substitute these into the original equation and somehow prove p(t)=0 $\frac{d^2y}{dt^2}+p(t)[{y_1y_2'-C}{y_2}]+q(t)\frac{C+y_1'y_2}{y_2'}=0$ I don't think this is quite right though...any thoughts?","['ordinary-differential-equations', 'wronskian']"
1161128,Integrals of orthogonal polynomials and combinatorics,"A beautiful result due to Evans and Gillis is that the function $$A(n_1,n_2,\cdots,n_r)=\int_0^\infty L_{n_1}(x)L_{n_2}(x)\cdots L_{n_r}(x)e^{-x}dx$$ counts the number of generalized derangements (up to a sign, I believe). Here, $L_{n}(x)$ is a Laguerre polynomial There's a nice application of it here . For example the usual derangements are counted by: $$(-1)^n\int_0^\infty L_1^n(x)e^{-x}dx.$$ Messing around with Mathematica, it seems like one can try using other polynomials with their respective weights to produce integer valued $A(n_1,n_2,\cdots,n_r)$. For example, Hermite Polynomials: $$\int_{-\infty}^\infty H_{n_1}(x)\cdots H_{n_r}(x)e^{-x^2}dx$$ which tends to give either an integer, or an integer multiple of $\sqrt{\pi}$. For example: $$\int_{-\infty}^\infty H_{1}(x)H_{2}(x)H_{3}(x)H_{4}(x)e^{-x^2}dx=4224\sqrt{\pi},$$
$$\int_{-\infty}^\infty H_{1}(x)H_{2}(x)H_{3}(x)H_{5}(x)e^{-x^2}dx=19776.$$ $H_1(x)=2x$ is the simplest case and $$\frac{(2n)!}{n!}=\frac{1}{\sqrt{\pi}}\int_{-\infty}^\infty H_1(x)^{2n}e^{-x^2}dx,$$ which corresponds to this OEIS sequence. The only permutation related thing on that sequence is a ""downgrade permutation"" which is a permutation that sends another permutation to a descending sequence. Then this integral counts the number of self-downgrade permutations. This seems semi-promising as it's some kind of order-derangement condition. I didn't have much luck with Jacobi polynomials, which gave rational numbers. So here's a general question. Has this kind of phenomena, as in the case of Laguerre polynomials, been studied for other families? In other words, for a family of orthogonal polynomials $P_n(x)$, with orthogonality weight $w(x)$ on a space $A$, is there a combinatorial interpretation for: $$\int_A P_{n_1}(x)\cdots P_{n_r}(x)w(x)dx?$$ Are there any references on this? As far as I'm aware, Askey was able to reprove Evan's and Gillis result using analytic techniques (Evan's proof was actually combinatorial). Naively, all orthogonal polynomials fall into some category of the Askey Scheme, meaning that really these identities are somehow connected to the right set of hypergeometric functions and their weights. Unfortunately, that's not exactly illuminating.","['orthogonal-polynomials', 'real-analysis', 'combinatorics']"
1161147,Why does a fourier series have a 1/2 in front of the a_0 coefficient [duplicate],"This question already has answers here : Why is the zeroth coefficient in a Fourier series divided by 2? (3 answers) Closed 5 months ago . I am reading up on the fourier series, and I keep seeing it as being defined as: $$
f(\theta)= \frac{1}{2}a_0 + \sum_{n=1}^{\infty}(a_n \cos(n\theta) + b_n \sin(n\theta))
$$ where $$
a_n = \frac{1}{\pi}\int_{0}^{2\pi}\cos(n\theta)f(\theta)d\theta
$$ and $$
b_n = \frac{1}{\pi}\int_{0}^{2\pi}\sin(n\theta)f(\theta)d\theta
$$ I understand the derivation of the coefficients using trig integral identities, but I can't find a clear explanation of why $\frac{1}{2}$ is in front of $a_0$. Can anyone help show my why this is the case? Why can't we just have $a_0$ with no number in front of it. Thanks! edit: corrected summation term","['fourier-series', 'fourier-analysis', 'trigonometry', 'analysis', 'trigonometric-series']"
1161153,Does $\bigcap_{i=1}^\infty \bigcup_{n=i}^\infty A_n = \bigcup_{i=1}^\infty \bigcap_{n=i}^\infty A_n$?,"Are
$$B=\bigcap_{i=1}^\infty \bigcup_{n=i}^\infty A_n \quad C=\bigcup_{i=1}^\infty \bigcap_{n=i}^\infty A_n$$
equivalent sets (i.e. does $B=C$)? I realize that $B$ can be described as all elements that are in an infinite number of sets and $C$ can be described as all elements that are not in a finite number of sets. Can someone provide more characteristics of these sets?",['elementary-set-theory']
1161166,Uniform bound of linear function of sub-Gaussian random variable over a compact set,"Problem : Let $K\subset \mathbb{R}^p$ be a compact set that admits an $\epsilon$-net $\mathcal{N}_\epsilon$ with respect to the Euclidean distance of $\mathbb{R}^p$, and $|\mathcal{N}_\epsilon|\leq (C/\epsilon)^d$ for all $\epsilon\in (0,1)$. Here $C\geq 1$ and $d\leq p$ are positive constants. Let $X$ be a centered sub-Gaussian random variable with variance proxy $\sigma^2$. That is,
$$
\mathbb{E}_X\left[ e^{su^TX} \right] \leq \exp\left( \frac{s^2\sigma^2}{2} \right), \quad \forall u\in\mathbb{R}^p,\|u\|=1, \;\forall s\in\mathbb{R}
$$ Show that there exists positive constants $c_1$ and $c_2$ (to be made explicit) such that for any $\delta\in (0,1)$, it holds $$
\max_{\theta\in K}\theta^TX \leq c_1\sigma\sqrt{d\log(2p/d)} + c_2\sigma\sqrt{\log(1/\delta)}
$$ with probability at least $1-\delta$. My attempt : Given an $\epsilon$-net, $\forall \theta\in K$, $\exists x\in \mathcal{N}_\epsilon$ and $y\in \epsilon \mathcal{B}_2$, such that $\theta = x+y$. So $$
\mathbb{P}\left(\max_{\theta\in K}\theta^TX > t\right) \leq \mathbb{P}\left( \left\{\max_{x\in \mathcal{N}_\epsilon}x^TX > t/2\right\}\cup \left\{\max_{y\in \epsilon\mathcal{B}_2}y^TX > t/2\right\}\right)
$$ And then use union bound to bound the two probabilities on the right hand side separately. For the first term, I can use union bound on finite set to get
$$
\mathbb{P}\left( \max_{x\in \mathcal{N}_\epsilon}x^TX > t/2 \right) \leq |\mathcal{N}_\epsilon| e^{-t^2/8\tilde{\sigma}^2}
$$
here $\tilde{\sigma}^2 = \max_{x\in \mathcal{N}_\epsilon}\|x\|^2\sigma^2$ since the vectors in the $\epsilon$-net are not necessarily of unit length. For the second part, we can bound directly since we already have conclusion for a unit $L_2$ ball $\mathcal{B}_2$
$$
\mathbb{P}\left(\max_{\theta\in\mathcal{B}_2}\theta^TX > t\right) \leq 6^pe^{-t^2/8\sigma^2}
$$ However, the second part is creating $p$ in the exponential, this will leads to order of $\sqrt{p}$ in the final bound, while in the problem we have only order of $\sqrt{\log(2p/d)}$. My intuition : since the size of the $\epsilon$-net for $K$ grows only at exponential to the $d$-th, I'm thinking maybe we could make use of this by creating a very large $\epsilon$-net (making $\epsilon$ very small) so that the bounds over each $\epsilon$-balls becomes small, somewhat compensating the $p$-th order growth there. But I have not made this work. Any suggestion would be welcome. Thank you very much!","['statistics', 'distribution-tails', 'probability-theory']"
1161196,How to find all possible polynomials over a given finite field?,How would I find the possible polynomials over GF(p)? I'm trying to figure out which polynomials of a specific given finite field have no roots.,"['finite-fields', 'discrete-mathematics']"
1161248,What is the correct integral of $\frac{1}{x}$? [duplicate],"This question already has answers here : Is the integral of $\frac{1}{x}$ equal to $\ln(x)$ or $\ln(|x|)$? (7 answers) Closed 7 years ago . I understand that the graphs of $\log(x)$ and $\ln(x)$ both have derivatives (changes in slope) that follow the pattern of: $$\frac{d}{dx}\log_{b}x= \frac{1}{(x\ln(b))}$$ However, depending on the source, I have seen different definitions for the integral of $\frac{1}{x}$: $$\int\frac{1}{x}dx=\ln| x |+C$$ $$\int\frac{1}{x}dx=\log| x |+C$$ $$\int\frac{1}{x}dx=\ln x+C$$ $$\int\frac{1}{x}dx=\log x +C$$ I believe that only the top two definitions are close to being valid, and I also think that $\ln| x |+C$ is the only correct answer, based on the formula for the derivative given above, and the fact that $ln(e) = 1$. Is that incorrect? Can the answer to this be shown graphically as well as algebraically?","['logarithms', 'calculus', 'integration', 'indefinite-integrals']"
1161256,Lebesgue Differentiation Type Result,"Let $\mathcal{B}$ be a collection of bounded, measurable subsets of $\mathbb{R}^{n}$, such that for every $x$ there exists a sequence $\left\{R_{k}(x)\right\}\subset\mathcal{B}$ containing $x$ and with diameter tending to zero. Given a measurable set $E$, we say that $\mathcal{V}\subset\mathcal{B}$ is a Vitali covering of $E$ if for every $x$ there exists a sequence $\left\{R_{k}(x)\right\}\subset\mathcal{V}$ containing $x$ and with diameter tending to zero. Suppose that for every measurable set $A$ with $\left|A\right|<\infty$, every Vitali covering $\mathcal{V}$ of $A$, and every $\varepsilon>0$, there exists a countable subcollection $\left\{R_{k}\right\}\subset\mathcal{V}$ satisfying the following properties: $\left|A\setminus\bigcup R_{k}\right|=0$ $\left|\bigcup R_{k}\setminus A\right|\leq\varepsilon$ $\left\|\sum_{k}\chi_{R_{k}}\right\|_{L^{1}}\leq C\left|A\right|$ where $C$ is a constant independent of $A$, $\mathcal{V}$, and $\varepsilon$.
Does it follow that $\mathcal{B}$ satisfies the density property
$$\lim_{k\rightarrow\infty}\dfrac{\left|A\cap R_{k}\right|}{\left|R_{k}\right|}=\chi_{A}(x) \ \text{ a.e.    (*)}$$
where $x\in R_{k}\in\mathcal{B}$ for all $k$ and the diameter of $R_{k}$ tends to zero? My motivation for asking this question is that when $q>1$ and $1/q+1/p=1$, a variant of this theorem holds when we replace condition 3 with
$$\left\|\sum_{k}\chi_{R_{k}}\right\|_{L^{q}}\leq C\left|A\right|^{1/q}$$
and (*) by
$$\lim_{k\rightarrow\infty}\dfrac{1}{\left|R_{k}\right|}\int_{R_{k}}f=f(x) \ \text{ a.e.}$$
for every sequence $\left\{R_{k}\right\}$ as above and $f\in L_{loc}^{p}$.","['measure-theory', 'lebesgue-measure']"
1161296,Problem book on linear algebra,"Please refer a problem book on linear algebra containing the following topics: Vector spaces, linear dependence of vectors, basis, dimension , linear transformations, matrix representation with respect to an ordered basis, range space and null space, rank-nullity theorem; eigenvalues and eigenvectors, Cayley-Hamilton theorem; symmetric, skew-symmetric, hermitian, skew-hermitian, orthogonal and unitary matrices. I have already done Schaum's 3000 solved problems on linear algebra , but I need one more problem book to solve in order to be confident to sit for my exam. I don't need a proof oriented problem book; my focus is to solve problems which are applications of theorems Please provide your suggestions. Thanks.","['book-recommendation', 'abstract-algebra', 'linear-algebra', 'reference-request', 'soft-question']"
1161328,problem book on Group theory after doing Fraleigh. [duplicate],"This question already has an answer here : Group theory problems manual (1 answer) Closed last year . Please refer a problem book on Group Theory :TOPICS: Groups, subgroups, Abelian groups, non-abelian groups, cyclic groups, permutation groups; Normal subgroups, Lagrange's Theorem for finite groups, group homomorphisms and basic concepts of quotient groups (only group theory). I have already done A first course in abstract algebra by John b. Fraleigh . But I need one  problem book to solve in order to be confident. I don't need a proof oriented problem book,my focus is to solve problems which are applications of theorems . I had Gallian on my mind, but it lacked solutions. please provide your suggestions. thanks","['reference-request', 'book-recommendation', 'group-theory', 'abstract-algebra']"
1161350,measure-theoretic definition of expectation,"Consider a random variable $X \colon\Omega \rightarrow \mathbb{R}$ for a probability space $(\Omega, \mathcal{F}, P)$. We had the following definition for the expectation: $$\mathbb{E}[X]= \int_{\Omega} X(\omega) d P(\omega). \quad (1)$$ For the discrete case we would probably write $$\mathbb{E}[X]= \sum_{ \omega \in \Omega} X(\omega) P(\omega). \quad (2)$$ Now as non-mathematician (i.e. not having much knowledge about integrals and measure theory), I am  wondering why converting the sum in (2) to an integral does not result in $$\mathbb{E}[X]=\int_{\Omega} X(\omega) P(\omega) d \omega\quad (3)$$ instead of (1). (3) is what I would expect, because instead of the sum we just integrate, hence informally ""replace sum in (2) by integral sign and add a $d \omega$ at the end"". What is the difference between (1) and (3)? Is it valid to write (3)? Is it just ""notation"" for measures $P$ or is there a reason for this? Thank you very much and please have patient with a non-mathematician that never had a lecture in mass theory or integrals.","['integration', 'measure-theory', 'expectation', 'probability-theory', 'probability']"
1161370,What does a distributed lattice have to do with GCD and LCM?,"$\newcommand{\lcm}{\operatorname{lcm}}$I am lost while following this explanation: Let $$A(g, i) = \gcd(F_{g}, \lcm(F_{a_1}, F_{a_2}, \ldots , F_{a_i}))$$ and $$X = \lcm(F_{a_1}, F_{a_2}, \ldots , F_{a_{i - 1}})$$ Then $A(g, i) = \gcd(F_g, \lcm(X , F_{a_i}))$ Because GCD distributes over LCM, and vice versa (distributive lattice), we can write: $$A(g, i) = \lcm(\gcd(F_{g}, F_{a_i}), \gcd(F_g, X)))$$ When I looked what distributed lattice mean, I was not able to find any connection to what I see here. Can anyone explain me what is going on here?","['lattice-orders', 'discrete-mathematics', 'divisibility', 'least-common-multiple']"
1161375,Entire function vanishing at $n+\frac{1}{n}$ for $n\geq 1$.,"It was a problem: does there exists an entire function which vanishes at $n+\frac{1}{n}$ for all $n\in\mathbb{N}$? Since the set $\left\{n+\frac{1}{n}\right\}_{n\geq 1}$ has no limit point in $\mathbb{C}$, by Weierstrass theorem, there exist such function. Question: Can we produce an entire function (non-zero) which vanishes on above set using the well-known functions $\sin z, \cos z, e^z$, polynomial etc.?
 [In other words, can we write our required function as a composition of $\sin, \cos, e$, polynomials etc.?]",['complex-analysis']
1161410,Floating point arithmetic operations when row reducing matrices,"A numerical note in my linear algebra text states the following: ""In general, the forward phase of row reduction takes much longer than the backward phase. An algorithm for solving a system is usually measured in flops (or floating point operations). A flop is one arithmetic operation $(+,-,*,/)$ on two real floating point numbers. For an $n\times(n+1)$ matrix, the reduction to echelon form can take
$$
\frac{2}{3}n^3+\frac{1}{2}n^2-\frac{7}{6}n\tag{1}
$$
flops (which is approximately $2n^3/3$ flops when $n$ is moderately large--say, $n\geq 30$). In contrast, further reduction to reduced echelon form needs at most $n^2$ flops."" There is no explanation at all how the author came up with the expression in $(1)$. There is obviously a reason for that expression and the $n^2$ mentioned at the end of the quote. Can someone with knowledge of linear algebra or computer architecture, etc., explain why these expressions are the way there? It seems like the author just pulled them out of nowhere.","['numerical-linear-algebra', 'linear-algebra', 'numerical-methods']"
1161424,The sequence $(\frac 1n )$ of inverses of natural numbers converges to a limit other than $0$,Finding difficulty to construct a metric on $\Bbb R$ in which the sequence $(\frac 1n )$ of inverses of natural numbers that converges to a limit other than $0$.,"['general-topology', 'metric-spaces']"
1161427,Application Fundamental Theorem of Calculus,"Let $k:[a,b]\times[a,b]\to \mathbb{C}$, $f:[a,b]\to\mathbb{C}$. 
$$
f(x)=\int\limits_a^x k(x,y)\phi(y)\mathrm{d}y \qquad a\le x \le b
$$
I have to compute $f'(x)$. I don't know how to apply the Fundamental Theorem of Calculus in this case.","['derivatives', 'integration', 'real-analysis', 'analysis']"
1161454,Proving integral of zeroth-order Bessel function multiplied by cosine with complicated arguments,"How could it be proved that
$$ \int_0^\infty J_0\left(\alpha\sqrt{x^2+z^2}\right)\ \cos{\beta x}\ \mathrm{d}x = \frac{\cos\left(z\sqrt{\alpha^2-\beta^2}\right)}{\sqrt{\alpha^2-\beta^2}} $$
for $0 < \beta < \alpha$ and $z > 0$ ?
$J_0(x)$ is the zeroth order of Bessel function of the first kind. I found this integral in Gradshteyn and Ryzhik's book 7th edition, section 6.677, the equation number 3. Any helps and hints will be appreciated!","['definite-integrals', 'bessel-functions', 'special-functions', 'integration']"
1161471,problem book on Functions of Two Real Variables,"please refer a problem book on Functions of Two Real Variables : limit, continuity, partial derivatives, differentiability , maxima and minima. Method of Lagrange multipliers, Homogeneous functions including Euler’s theorem. i have already done this text from Calculus by Thomas Finney .But i need one problem book to solve in order to be confident to sit for my exam. I dont need a proof oriented problem book,my focus is to solve problems which are applications of theorems. please provide your suggestions.thank you",['multivariable-calculus']
1161497,Definition of the norm of a bounded linear operator.,"I have a somewhat basic but confusing question regarding the definition of the norm for bounded linear operator. Suppose $f$ is a bounded linear operator, that is, there exists $M>0$ such that $\|f(x)\| \leq M\|x\|$. We define $\mathcal{M}=\{M\geq 0:\|f(x)\|\leq M\|x\|, \forall x\}$
$$\|f\|:=\inf\{M\geq 0:\|f(x)\|\leq M\|x\|, \forall x\}=\inf\{\mathcal{M}\}.$$ Now, the question is: how do we know that $\|f\|\in \mathcal{M}$, i.e. that $\|f(x)\|\leq \|f\|\|x\|$ for all $x$? This might seem a rather dumb question, but I've noticed a lot of books skirt around the technicalities of this, and even when they don't, the arguments don't strike me as convincing. The argument I've seen a lot goes like this: Since $\|f\| =\inf\{\mathcal{M}\}$, for all $n\in\mathbb{N}$, $$\|f\|+\frac1n \in \mathcal{M}.$$ Otherwise $\|f\|$ would not be the infimum. Therefore
$$\|f(x)\| \leq \left( \|f\|+\frac1n \right)\|x\|,$$
for all $x$ and for all $n\in \mathbb{N}$. Letting $n\rightarrow \infty$ while keeping everything else fixed we obtain:
$$\|f(x)\| \leq \|f\|\|x\|.$$ 
How exactly is $\|f(x)\| \leq \|f\|\|x\|$ a consequence of $\|f(x)\| \leq \left( \|f\|+\frac1n \right)\|x\|$? 
I tried obtaining a proof by contradiction: let us replace $\frac1n$ by $\epsilon>0$ and let's assume that the latter is true but the conclusion is not. This would imply the existence of $x_0$ such that 
$$\|f(x_0)\| > \|f\|\|x_0\|$$ while simultaneusly having
$$\left( \|f\|+\epsilon \right)\|x_0\|\geq \|f(x_0)\|.$$
Without loss of generality we can assume $x_0\neq 0$, and putting this all together yields:
$$\|f\|+\epsilon> \|f\|,$$
which is not a contradiction. More generally, the fact that $\|f\|+\epsilon \in \mathcal{M}$ amounts to saying that $\|f\|$ is a limit (or accumulation, depending on your definition) point of $\mathcal{M}$. It does not follow that the limit point must belong to the set in question (that would be saying $\mathcal{M}$ is closed (which is what we're trying to prove). Thanks in advance for any insights into this.","['normed-spaces', 'functional-analysis']"
1161532,Convergence of $\int_0 ^{\infty}\frac{\cos t}{t^{\alpha}} dt$ related to $\Gamma$ function,"I would like to show that the integral $$\int_0 ^\infty \frac{\cos t}{t^{\alpha}} dt$$ converges for $0<\alpha <1$. I already showed that it does not converge for $\alpha\leq 0$ or $\alpha \geq 1$. Do you have any hints for me? I know that I can use the gamme function (using $\cos t=\frac{e^{it}+e^{-it}}{2}$) but I don't see why I can use the gamma function just for $\alpha \in (0,1)$.","['calculus', 'integration', 'functions', 'analysis', 'gamma-function']"
1161551,Two isometries that have same value and differential at some point are the same.,"I also have trouble in this problem: Let $f, g$ be two isometries of a connected Riemannian manifold $(M, g)$. If $f(p)=g(p)$, $df_p = dg_p$, show that $f=g$. Any comment is expected. I know it is a well-known rigidity result of Riemannian manifold. Could anyone give some advice?","['geometry', 'riemannian-geometry', 'differential-geometry']"
1161561,Do absolutely continuous functions have bounded derivative?,"I am an outsider for this field of mathematical analysis. But to analyse a problem of Control Systems, which is my area of interest, I need to know this. I learned that absolutely continuous functions are also differentiable almost everywhere . On the other hand, Lipschitz continuity ensures bounded derivative of the function a.e. I am wondering whether there is any link between the derivative being bounded and the function being absolutely continuous. Or, is there any sufficient condition to be imposed over the absolute continuity to ascertain that the derivative of the function will be bounded?","['real-analysis', 'analysis']"
1161589,Riemannian metric induced by metric,"This seems a very basic and useful construction, and yet I cannot find any reference for it. So my questions are, 
1) Is the following definition correct?
2) Is there a simpler construction?
3) Do you know any references where this definition is used/found? Definition: Let $\mathcal{M}$ be a differentiable manifold on which we have a metric $d:\mathcal{M}\times \mathcal{M}\rightarrow \mathbb{R} $ whose square is twice differentiable on the diagonal $\{(p,q)\in \mathcal{M}\times \mathcal{M}\;|\; p=q\}$. We define a Riemannian metric $g$ as follows. Let $p\in\mathcal{M}$ and $X,Y\in T_p\mathcal{M}$. Let $\epsilon>0$ and pick two curves $\gamma_1,\gamma_2: (-\epsilon,\epsilon)\rightarrow \mathcal{M}$ with 
$$ \gamma_1(0)=\gamma_2(0)=p, $$
$$ \gamma_1'(0)=X,\quad \gamma_2'(0)=Y. $$
Then,
$$g_p(X,Y)=-\frac{1}{2}\frac{\partial^2}{\partial t_1\partial t_2}d^2(\gamma_1(t_1),\gamma_2(t_2)) \Bigg|_{t_1=t_2=0}$$ This works for $X\neq Y$. For $X=Y$ take 
$$g_p(X,X)=\frac{\partial^2}{\partial t_1^2}d^2(\gamma_1(t_1),p) \Bigg|_{t_1=0}$$ Remark These formulae work in Euclidean space, i.e. $$\| \gamma_1(t_1)-\gamma_2(t_2)\|^2=t_1^2\|X\|^2+t_2^2\|Y\|^2-2t_1t_2X\cdot Y+...,$$ so taking the 2nd derivatives as specified above will produce the correct scalar products. This leads me to believe that they are correct.
However, my first guess was to take the pushforward of $d$, which I couldn't get to work out. What is the interpretation of the above construction in terms of pushforwards?","['riemannian-geometry', 'metric-spaces', 'differential-geometry']"
1161594,Show that SO(2) is isomorphic with the complex circle group,"For my math study I have to prove that $SO(2)$ is isomorphic with the complex circle group. Some steps in this prove are a bit difficult to me, so I hope you could help me. With $SO(2)$ I mean the group of all rotations $\rho_{x}$ in $\mathbb{R^{2}}$ with $x$ the angle of rotation around the origin. With $U_{1}$ I mean the complex circle group, so $U_{1}$ = {$z \in \mathbb{C}: |z| = 1$}. This is what I've done so far: I defined $f: SO(2) \to U_{1}$ with $f(\rho_{x}) = e^{2\pi ix}$ and proved that it is an homomorphism and that it is surjective. So the only things I have to do is proving that $f$ is well-defined and injective.For injectivity, I have come so far: Assume $f(\rho_{x}) = f(\rho_{y})$ $\Rightarrow$ $e^{2\pi ix} = e^{2\pi iy}$ $\Rightarrow$ $x - y \in \mathbb{Z}$ How do I have to complete the injectivity prove and prove that $f$ is well-defined? Thanks in advance!","['complex-numbers', 'group-theory', 'functions', 'rotations']"
1161621,Lifting an $\mathbb{RP}^{n-1}$-valued map to an $S^{n-1}$-valued map.,"Let $M$ be a smooth manifold and assume that for each $v\in M$, I have a vector $v(m)\in\mathbb{R}\setminus\{0\}$ with some property and this vector is unique up to multiple : further, it depends smoothly on $m$. I would like to define a smooth map $F: M\to S^{n-1}$ that sends $m$ to a representative unit vector with the required property. Under which conditions can this be done? I understand that it is a lifting problem: I want to lift a smooth map $f: M\to\mathbb{RP}^{n-1}$ to a map $F: M\to S^{n-1}$. If necessary, I can assume that $\mathrm{dim}\, M<n-1$ but I'm not sure if it is sufficient. Then $f(M)$ has ""lower dimension"" then $n-1$ and I can assume that it aviods a point, but I can hardly assume that it aviods the whole $\mathbb{RP}^{n-2}$ (in which case I could define a global section on $\mathbb{RP}^{n-1}\setminus\mathbb{RP}^{n-2}\to S^{n-1}$).. Any hint please?","['differential-topology', 'algebraic-topology', 'differential-geometry']"
1161638,"Proof that a sequence of set has a set dense somewhere in $[a,b]$","Is the following proof correct? Proposition : if we have a sequence of set $U_i$ such as $\bigcup_{i\in \mathbb{N}} U_i=[a,b]$ then there exist a $i$ such as $U_i$ is dense somewhere in $[a,b]$ Proof : If for all $i, U_i$ is nowhere dense, we have $\forall ]u,v[$ $$\forall i, \exists c \in ]u,v[, \exists h > 0,\quad U_i \cap ]c-h,c+h[ =\emptyset\text{ and } ]c-h,c+h[ \subset ]u,v[$$ Let's define $$\left\lbrace \begin{array} .E_{c,h}^i= ]c-h,c+h[ & \text{ if } & U_i \cap ]c-h,c+h[ =\emptyset \\ E_{c,h}^i= \emptyset &\text{ if } & U_i \cap ]c-h,c+h[ \neq \emptyset  \end{array}\right.$$
and
$$E_i = \bigcup_{h>0} \bigcup_{c\in ]u,v[} E^i_{c,h}$$ $E_i$ is open (as a reunion of open set), and dense in $]u,v[$. Indeed, we have as $$\forall \epsilon >0 \forall x\in ]u,v[, \exists c \in ]x-\epsilon,x+\epsilon[,\exists h >0,\quad U_i \cap ]c-h,c+h[ = \emptyset$$ $$\forall \epsilon >0 \forall x\in ]u,v[, \exists ]c-h,c+h[ \subset U_i^C,\quad]c-h,c+h[\subset ]x-\epsilon,x+\epsilon[$$ $$\forall \epsilon >0 \forall x\in ]u,v[, \exists c\in E_i,\quad c\in ]x-\epsilon,x+\epsilon[$$ So $E_i$ is an open dense set for all i. By Baire theorem, we get that $\bigcap_{i\in\mathbb{N}} E_i$ is dense in ]u,v[. As we have $\forall i, E_i\subset U_i^C$, we get that  $\bigcap_{i\in\mathbb{N}} E_i \subset \bigcap_{i\in\mathbb{N}} B^C_i$ and  $\bigcap_{i\in\mathbb{N}} B_i^C$ is dense in $]u,v[$. But we also have $$\bigcap_{i\in\mathbb{N}} \left(B_i^C \cap ]u,v[\right) =  ]u,v[ \cap \left( \bigcup_{i\in\mathbb{N}} B_i \right)^C = ]u,v[^C \cap ]u,v[= \emptyset$$ Contradiction.","['elementary-set-theory', 'real-analysis']"
1161664,Computing the sum $\sum_{k=0}^{n} \binom{n}{k}(-1)^k(n-k)^n$,"I need to compute $S_{n} = \sum_{k=0}^{n} \binom{n}{k}(-1)^k(n-k)^n$.
$S_{2} = 2, S_{3} = 6,S_{4}=24$ therefore i think answer is $S_{n} = n!$.
And i have got $S_{n}=(\Delta^n(n-x)^n)(0)$.
But it gives no benefit to me..","['discrete-mathematics', 'combinatorics']"
1161704,"Smooth scheme over a field is reduced, why?","Could someone tell mey why this is true: A smooth scheme over a field is regular and hence normal. In particular, a smooth scheme over a field is reduced. [ http://en.wikipedia.org/wiki/Smooth_scheme#Properties] I suppose that this comes from the property normal but I don't see the connection.","['algebraic-geometry', 'abstract-algebra']"
1161710,Dudley’s exercise on finitely additive probabilities,"Yesterday I found the following exercise in Dudley’s “Real Analysis and Probability”. In a game, two players, Sam and Joe, each pick a nonnegative integer at random. For each, the probability that the number is in any set $A$ is $\mu(A)$, where $\mu$ is a finitely additive function defined on $2^X$ with $\mu(A) = 0$ for every finite set $A$ and $\mu(X) = 1$, and $X = \mathbb{N}$. The one who gets the larger number wins. A coin is tossed to determine whose number you find out first. It’s heads, so you find out Sam’s and still don’t know Joe’s. Now, who do you think will win? I was wondering if the following line of reasoning is correct. The one that will win with probability $1$ is Joe. To prove this, let $m \in \mathbb{N}$ be the number picked by Sam. Then, according to the way in which $\mu$ is defined, $\mu (x > m) = 1$ and $\mu (x \leq m) = 0$. This is so, because the set $\{ x \ | \ x > m \}$ is an infinite subset of $\mathbb{N}$, while $\{ x \ | \ x \leq m \}$ is clearly finite. QED Is this correct? Any feedback is more than welcome as always (even if this exercise looks completely trivial to you), in particular because I feel really uncomfortable with measure theory. Thank you for your time and assistance.","['probability-theory', 'measure-theory']"
1161733,Can I recover a group by its homomorphisms?,"There is finitely generated group $G$ which I don't know.
For every finite group $H$ I can think of, I know the number of homomorphisms $G \to H$ up to conjugation.
(By this I mean that two homomorphisms $\phi_1$ and $\phi_2$ are being considered equivalent if there is a $h \in H$ such that $\phi_1(g)h = h\phi_2(g)$ for all $g \in G$.) Given these numbers, do I have enough information to recover $G$? Edit: The question is motivated from physics. A flat $H$-connection on a manifold $M$ is a homomorphism $\pi_1(M) \to H$ and a gauge transformation is a conjugation. So I'm interested whether I can recover the fundamental group by counting equivalence classes of connections, for arbitrary finite gauge groups. Edit 2: It would be interesting as well if we count the number of homomorphisms without taking the equivalence by conjugation into account.","['group-homomorphism', 'finitely-generated', 'finite-groups', 'group-theory']"
1161746,How come $\left(\frac{n+1}{n-1}\right)^n = \left(1+\frac{2}{n-1}\right)^n$?,"I'm looking at one of my professor's calculus slides and in one of his proofs he uses the identity: $\left(\frac{n+1}{n-1}\right)^n = \left(1+\frac{2}{n-1}\right)^n$ Except I don't see why that's the case.
I tried different algebraic tricks and couldn't get it to that form. What am I missing? Thanks. Edit: Thanks to everyone who answered. Is there an ""I feel stupid"" badge? I really should have seen this a mile a way.","['sequences-and-series', 'calculus', 'algebra-precalculus', 'fractions', 'arithmetic']"
1161785,Proof: The inverse of the inverse matrix is the matrix. [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved Improve this question If $A$ is a square matrix such that it is not singular, then  $(A^{-1})^{-1} = A$
How can I prove this property? I would appreciate it if somebody can help me.","['matrices', 'linear-algebra', 'inverse']"
1161797,How can I prove the $\sum_{n=2}^{\infty }\frac{\log(n)^\frac{1}{k}}{n!}< (e-2)$,"How can I prove the $$\sum_{n=2}^{\infty }\frac{\log(n)^\frac{1}{k}}{n!}< (e-2)$$ 
If $k>1$","['inequality', 'sequences-and-series']"
1161829,Exotic bijection from $\mathbb R$ to $\mathbb R$,"Clearly there is no continuous bijections $f,g~:~\mathbb R \to \mathbb R$ such that $fg$ is a bijection from $\mathbb R$ to $\mathbb R$. If we omit the continuity assumption, is there such an example ? Notes: to follow from Dustan's comments:
Notes:  By definition $fg~:~x \mapsto f(x)\times g(x)$ and not $f\circ g$.
If there were continuous bijections just look at the limits of $f$ and $g$ at $+\infty$ and $-\infty$ to conclude that $fg$ can't be a bijection",['real-analysis']
1161857,Compact set and continuous function [duplicate],"This question already has an answer here : From injective map to continuous map (1 answer) Closed 9 years ago . Let $(E,d), (E',d')$ be two metric space, and $f:E\rightarrow E'$ an injective function such that the image of any compact set from $E$ is compact in $E'$. How can I prove that $f$ is continuous? Thank you.","['general-topology', 'metric-spaces', 'compactness', 'analysis']"
1161862,How to bound error when approximating ODE,"I have a question regarding how to bound the error, if one changes the ""right hand side"" of an ODE. For example, the equation of a simple pendulum in polar coordinates is something like $$\ddot{\theta}= k\sin\theta$$ The common adjustment is to let $$\sin\theta \sim\theta$$
for small enough $\theta$. I'll try to state my question in general terms now. Take the IVP: $$\left\{\begin{align}\dot x  = f(x) \\ x(0) = x_0\end{align}\right.$$
with $$f:B_{r}(0)\subset\Bbb R^n\to\Bbb R^n$$ Now, suppose that $\tilde f:B_{\tilde r}(0)\to\Bbb R^n$ is such that $\|f(x)-\tilde f(x)\| \lt \varepsilon$ for $x\in B_{\tilde r}(0)$ if $\tilde r < \delta \leq r$. Under which conditions, and then how, can we place a bound on $$\|\varphi(t) - \tilde\varphi(t)\|$$
where $\varphi$ solves the original IVP, and $\tilde\varphi$ solves the new IVP formed by replacing $f$ with $\tilde f$? Obviously, the conditions must at least guarantee existence and uniqueness for the question to make sense, but what else (if anything)? I'm not sure if this has to do with perturbation theory proper, so let me know if it's wrongly tagged.","['ordinary-differential-equations', 'continuity', 'perturbation-theory']"
1161897,Finding solution of non linear DE $x\ddot{x}-\dot{x}^2=1$,"I am looking for help on how to find the solution of the following differential equation, $$x\ddot{x}-\dot{x}^2=1,$$ which comes from solving the Euler-Lagrange equations for the lagrangian $L=x\sqrt{\dot{x}^2+1}$. It is from an exercise in my field theory course. My professor provided me with the solution $x(t)=c_1\cosh{\left(\frac{t-c_2}{c_1}\right)}$, which is a solution when plugged into the equation, but does not give any derivation. Is there a way to solve this DE other than an educated guess? If so how?","['nonlinear-system', 'ordinary-differential-equations']"
1161907,Cayley transformation of a skew-symmetric matrix is orthogonal?,"If $S$ is skew-symmetric ($S^{T} = -S$), how do I show that $Q$ is orthogonal where $$Q = (I + S)(I - S)^{-1}$$ which is the Cayley transformation of $S$.","['matrices', 'linear-algebra', 'transformation']"
1161940,Proving $y=\lfloor x\rfloor$ doesn't have a primitive function,"Prove that $y=\lfloor x\rfloor$ doesn't have a primitive function. I have the proof from a book here but I don't understand it: Suppose that there is $F(x)$ such that $F'(x)=\lfloor x\rfloor$. We
  have that $y=\lfloor x\rfloor$ has a jump discontinuity (or type 1)
  but $F'(x)$ has only an essential discontinuity (or type 2), a
  contradiction. I don't understand, why does $F'(x)$ has an essential discontinuity?","['calculus', 'integration']"
1161955,Proof: The identity matrix is invertible and the inverse of the identity is the identity,"How can i show that: $II^{-1} = I = I^{-1}I$ (the identity matrix is invertible) for all cases. And then proof that: $I^{-1} = I$ (The inverse of the identity is the identity). I don't know how start both proof, any suggestion?","['matrices', 'linear-algebra', 'inverse']"
1161957,Example of disjoint union of sets which does not have additive measure,"I had a question about the additivity property of the outer measure. Can someone provide an example of a disjoint union of sets which doesn't have an outer measure equal to the sum of the outer measure of each set (in $\mathbb{R}^n$)? That is: $m_*(E_1\bigcup E_2)\neq m_*(E_1) + m_*(E_2)$, where $E_1\bigcap E_2=\emptyset$ and $d(E_1,E_2)$ possibly equal to zero. The theorem states this holds in general if $d(E_1,E_2)> 0$, but I can't find a counterexample where it doesn't if $d(E_1,E_2)=0$. Thanks!","['measure-theory', 'lebesgue-measure', 'real-analysis', 'analysis']"
1161981,General linear group is an affine variety,"I am trying to prove that the general linear group $GL(n)$ is an $\underline{\text{affine}}$ variety. Unfortunately, I am having trouble with showing that $GL(n)$ is indeed affine. Before I show my progress I present the definitions as given to me during the lecture: A quasiprojective variety is a locally closed set of affine or projective space. An affine variety is a quasiprojective variety isomorphic to a closed subset of affine space. Thank you in advance! Let $k$ be the underlying field and let $\mathbb{A}^n$ represent affine $n$-space. The first step is that $GL(n)\subset M_n(k)\cong\mathbb{A}^{n^2}$ is given by all matrices with determinant unequal to zero. Since the determinant is a polynomial in the coefficients of a matrix we conclude that $GL(n)$ is open in $\mathbb{A}^{n^2}$. Therefore, it is locally closed in $\mathbb{A}^{n^2}$ and thus a (quasiprojective) variety. However, to show it is affine I have to show that it is isomorphic to a closed subset of some affine space, which I don't know how to tackle.","['general-topology', 'algebraic-geometry', 'zariski-topology']"
1161987,Help with understanding the notation $\mathbb{C}\{f\}$,"I am reading an article ""Relative Cohomology and volume forms"" of J. P. Francoise. 
Here the author considers the germ of a function $f\colon(\mathbb{C}^n,0) \to (\mathbb{C},0)$, and he speaks of the ring $\mathbb{C}\{f\}$, but he doesn't define this ring. Does anyone know the definition of this ring $\mathbb{C}\{f\}$? Can someone recommend me a book or something similar? Thank you.","['ring-theory', 'singularity-theory', 'sheaf-cohomology', 'germs', 'complex-analysis']"
1161995,Find the functions,"Find all the functions $ f : \mathbb{Q} \rightarrow \mathbb{Q} $ with the following property: $$ f(x + 3f(y)) = f(x) + f(y) + 2y, \: \forall x, y \in \mathbb{Q} $$","['rational-numbers', 'functions', 'functional-equations']"
1162013,Find integral $\int\frac{\sin\sqrt{x}}{\sqrt{x}}dx$,"Please help me to find indefinite integral: $$\int\frac{\sin\sqrt{x}}{\sqrt{x}}dx$$ Please suggest to me a way to do it. I tried the substitution $t = \dfrac{1}{\sqrt{x}},\,$ so that $\,dt = -\dfrac{1}{2x^{3/2}}\,dx$. But I don't know what to do next...","['integration', 'indefinite-integrals']"
1162032,Finding the inverse of a function.,"Let $f:\mathbb{R}\to \mathbb{R}_+$ with $f\geq\epsilon>0$ be smooth and define $G:\mathbb{R}\to\mathbb{R}$ thus $$G(x):=\int_0^x\frac{1}{f(u)}\mathrm{d}u$$ Then it is clear that $G$ is well-defined, continuous and strictly increasing (so bijective). Therefore $G$ must have a continuous inverse. What I would like is a formula for the inverse, but I can't seem to come up with one. Suppose the inverse is given by  $P$, then $G(P(x))=x$ differentiating both sides $$G'(P(x))P'(x)=1$$ hence $$P'(x)=f(P(x))$$ using the fact that $G'=\frac{1}{f}$. So I have an ODE that $P$ would solve ($P$ is continuously differentiable by the inverse function theorem), but it doesn't seem to help! EDIT in light of the comment: This problem arose in an approximation scheme for stochastic differential equations on manifolds; I don't really know how to explain the background succinctly, but I have imposed no conditions on f beyond what's mentioned above. If anyone knows how to solve the problem in less generality, I would also be very happy.","['ordinary-differential-equations', 'real-analysis', 'analysis']"
1162047,Show that distributions of linear combinations of two variables determine couple law,"Let $X=(X_{1},X_{2})$ and $Y=(Y_{1},Y_{2})$ be two $\mathbb{R}^{2}$-valued variables. Show that if $aX_{1}+bX_{2}$ has same distribution as $aY_{1}+bY_{2}$ for all real numbers $a,b$ then $X$ has same distribution as $Y$. I know this can be made to follow by the fact that knowledge of  the characteristic function $v\mapsto\mathbb{E}(\exp(i\langle v, X \rangle))$ determines the distribution of $X$. However I don't know how to prove that (and I know it is tedious), so I was wondering if it could be made to follow from the fact that $t\mapsto \mathbb{E}(\exp(itX_{1}))$ determines $X_{1}$, or even better if it is possible to prove this fact without characteristic functions.","['probability-theory', 'probability-distributions', 'probability']"
1162114,Existence of idempotents versus existence of projections in a C*-algebra,"Let $\mathcal{A}$ be any C*-algebra.
Suppose $x\in\mathcal{A}$ is idempotent, with $x\neq 0$ and $x\neq 1$. Does it follow that $\mathcal{A}$ admits nontrivial projections? Clearly, when $x$ is normal, $x^*x$ is self-adjoint and idempotent, so that settles the matter. I didn't manage to prove the claim in the general case, though. (for good measure: a projection is a self-adjoint idempotent element of $\mathcal{A}$)","['c-star-algebras', 'operator-algebras', 'spectral-theory', 'functional-analysis']"
1162122,Numerical differentiation: 2-point vs 5-point method,"I want to compare the following two numerical differentiation schemes: 2-point numerical differentiation:
\begin{equation}
\dot{\omega}_t = \frac{1}{dt} \left [ \omega_{t} - \omega_{t-dt} \right ] + \mathcal{O}(h)
\end{equation} 5-point numerical differentiation:
\begin{equation}
\dot{\omega}_t = -\frac{1}{12dt} \left [ -25\omega_{t} + 48\omega_{t-dt} -36\omega_{t-2dt} + 16\omega_{t-3dt} - 3\omega_{t-4dt} \right ] + \mathcal{O}(h^4)
\end{equation} The data that I use can be downloaded here , where $p$ is the signal without noise, $pnoisy$ is signal $p$ plus independent Gaussian noise, $pdot$ is the exact derivative of $p$ which needs to be estimated by using $pnoisy$. At last, $time$ is simply the time vector. I use the following Matlab script: clear all; clc; close all

load tempdat

%   Two-point method
time_step = time(2) - time(1);

pdot_twopoint = (1/time_step)*diff(pnoisy);

%   Five-point method
j = 1;
for i = 5:length(time)
    pdot_fivepoint(j,1) = -(1/(12*time_step))*(-25*pnoisy(i) + 48*pnoisy(i-1) - 36*pnoisy(i-2) + 16*pnoisy(i-3) - 3*pnoisy(i-4));
    j = j+1;
end

plot(time, pdot)
hold all
plot(time(2:end), pdot_twopoint)
plot(time(5:end), pdot_fivepoint)
hold off
legend('real','2-point','5-point')
xlabel('time (s)')
ylabel('pdot')

sum(abs(pdot(2:end) - pdot_twopoint).^2)
sum(abs(pdot(5:end) - pdot_fivepoint).^2) The RMSE (w.r.t. to $pdot$ and its estimate) of the 2-point method is 3.2038, while the RMSE of the 5-point method is 47.4570. I have also added a graph below where it can be seen that the 2-point method results in a more accurate estimate of $pdot$ compared to the 5-point method. I wonder why the 5-point method performs better than the 2-point method, because the accuracy of the 5-point method is supposed to be $\mathcal{O}(h^4)$ while that of the 2-point method is $\mathcal{O}(h)$.","['derivatives', 'numerical-methods']"
1162190,Proof alternating sum of squares is alternating sign of sum,"I'm trying to prove by induction that $1-4+9-...\pm n^2 = \pm(1+2+...+n)$.  The base-case is obvious, and the formula that I write this as is $$\sum_{i=1}^{n}(-1)^{i+1}i^{2} = (-1)^{n+1}\sum_{i=1}^{n}i$$ If we assume this holds up to $n$ then I try to prove the equation true for $n+1$ but get stuck.  I have $$\sum_{i=1}^{n+1}(-1)^{i+1}i^{2} = (-1)^{n+1}\sum_{i=1}^{n}i + (-1)^{n+2}(n+1)^{2}$$ $$ = (-1)^{n+2}\Big( (n+1)^{2}-\sum_{i=1}^{n}i\Big)$$ At this point I can't think of anyth
ing that sounds like a good idea.  I suppose I could try to use what I have to get a formula for $n+1$ like moving everything away from this in the inductive hypothesis, $n+1=\pm(1-4+...\pm n^{2})-2-3-...-(n-1)$ but that sounds like I'll just be heading in a circle. For guidance I can try setting this equal to my goal and reduce it to a known equation, but that doesn't seem to help. $$(-1)^{n+2}\Big((n+1)^{2}-\sum_{i=1}^{n}i\Big) = (-1)^{n+2}\sum_{i=1}^{n+1}i$$ iff $$n^{2}+2n+1 = n+1+2\sum_{i=1}^{n}i$$ iff $$n^{2} = n+ 2\sum_{i=1}^{n-1}i$$ but this is not obviously true and I can't see where to go from here either.","['induction', 'discrete-mathematics']"
1162257,How to calculate clock-wise and anti-clockwise arc lengths between two points on a circle,I have two points with known coordinates on a circle of known position and radius. I need to calculate two things: The clockwise arc length between the two points on the circle The anti-clockwise arc length between the two points on the circle How would I go about doing this? Note this is 2D.,"['trigonometry', 'circles']"
1162264,Closest matrix with specific eigenvector,"Consider a vector ${\bf x}$ and a matrix $A_0$ with $A_0(i,j)\ge 0$. What is the best way of getting matrix $A$ s.t. $$A = \arg \min \|A-A_0\|_{\text F}$$ subject to $$A{\bf x} = \lambda {\bf x} \hspace{2mm} \mbox{and} \hspace{2mm} A(i,j)\ge0$$ where $\|\cdot\|_{\text F}$ denotes the Frobenius norm?","['optimization', 'matrices', 'convex-optimization']"
1162359,Distributing n identical balls in k distinct boxes,"In how many ways can $20$ identical balls be distributed in $4$ distinct boxes, subject to the following conditions: Each box has at least $2$ balls, Each box has an even number of balls? The distribution of $20$ identical balls in $4$ distinct boxes is equal to the sequence of $20$ $0$'s and $3$ $1$'s. 
So first we put $2$ balls in each box so no box is left empty( We have $1$ way of doing so),
then we distribute the remaining $12$ balls in $4$ boxes(The formula is $C(k + n - 1, k),$ from the theorem: The number of $k$-combinations with unlimited repetition of a set of
$n$ distinct objects is $C(k +n−1, k)$) Which then implies $C( 12 + 4 - 1, 12)$, so the total number of sequences of $0$'s and $1$'s is $C(15,12) = 15! / (12! * 3!) I am not sure if the above written is correct and I do not know how to find the answer for the $2^{nd}$ condition.",['discrete-mathematics']
1162366,Why does an elliptic curve have genus one?,"I read that one definition of an elliptic curve goes as follows: Let $k$ be a field. We define the elliptic curve over $k$ be a smooth projective curve $E$ over $k$, isomorphic to a closed subvariety of $\mathbb P_k^2$ defined by a homogeneous polynomial $F(u,v,w)$ of the form
$$F(u,v,w)=v^2w+(a_1u+a_3w)vw-(u^3+a_2u^2w+a_4uw^2+a_6w^3),$$
where $a_i\in k$ for $k\in\{1,2,3,4,6\}$, with privileged rational point $o=(0,1,0)$. How can I prove from this definition that the arithmetic and geometric genus of any elliptic curve over a given field $k$ is 1 if $\operatorname{char}(k)\ne 2$? This problem was popped on my mind when I was reading Takeshi Saito's book Fermat's last theorem: Basic tools lemma 1.10. (2) as well as definition of an elliptic curve from Qing Liu's book Algebraic Geometry and Arithmetic curves .","['algebraic-geometry', 'elliptic-curves']"
1162373,"If prime $p \mid ab$, then $p \mid a$ or $p \mid b\ $ [Euclid's Lemma]","The proof is already given in the textbook but I tried other way around. Proof by contradiction:
Let's assume that $p$ doesn't divide $a$ and $p$ doesn't divide $b$, but $p$ divides $ab$. So $\gcd(p,a)=1$ and $\gcd(p,b)=1$. Given that we can construct linear combinations $sp+ta=1$ together with $up+wb=1$. Multiplying the left and the right sides of the equations we get $spup+spwb+taup+tawb=1$ or $p(sup+swb+tau)+ab(tw)=1 \implies \gcd(p,ab)=1$. This is a contradiction. Is it rigorous and sound proof?","['prime-numbers', 'solution-verification', 'number-theory']"
1162379,Non-negative integer solutions given restrictions on $x_i$ (check work),"Use Inclusion-Exclusion to determine the number of integer solutions to the equation $$x_1+x_2+x_3+x_4=14$$ Where $0{\leq}x_1{\leq}8; 0{\leq}x_2{\leq}5; x_3, x_4{\geq}0$. My thought process: I can disregard the last two restrictions because those two are given since the combination gives the number of nonnegative integer solutions. Have A= # solutions given $0{\leq}x_1{\leq}8$ B= # solutions given $0{\leq}x_2{\leq}5$ Which means $A^c$ is the number of solutions for $9{\leq}x_1{\leq}14$. *Would the possible solutions for $A^c$ then be partitioning 9+1+1+1+1+1=14? Or rather, give us ${6+4-1\choose 4-1}$? *And $B^c$ is the number of solutions for $6{\leq}x_2{\leq}14$, so we would have $6+1+1+1+1+1+1+1+1=14$ or ${9+4-1\choose 4-1}$? If $x_1{\gt8}$ and $x_2{\gt}5$ then for $A^c{\cap}B^c$ we have
$$9+6+0+0=15$$ or similar and we see there are no possible solutions for $x_1+x_2+x_3+x_4=14$. Then $$A{\cap}B = A{\cup}B-A^c-B^c+A^c\cap B^c$$
$$={14+4-1\choose 4-1} - {6+4-1\choose 4-1}- {9+4-1\choose 4-1} +0$$
$$=376$$ *I'm a little nervous about my reasoning for $A^c$ and $B^c$. I always end up missing some crucial step in these kinds of problems. - resolved in comments","['inclusion-exclusion', 'diophantine-equations', 'combinatorics']"
1162419,Can we predict the past?,"Can we use probability rules to predict the occurrence of an event which has already happened in the past or already formed? For example, hemoglobin is a protein formed of $141$ amino acids connected like a chain with specific order, the first amino acid is Leucine (we have only $20$ types of amino acids forming any protein), is it valid to say that probability of leucine being first in this chain is $\frac1{20}$ so the probability to get the $141$ a.a hemoglobin with such order by chance is $\left(\frac{1}{20}\right)^{141}?$ or this prediction makes no sense as we already have hemoglobin formed with such order in the nature?","['statistics', 'probability']"
1162480,Integral of sine multiplied by Bessel function with complicated argument,"I need a help with integral below,
$$ \int_0^\infty \sin(ax)\ J_0\left(b\sqrt{1+x^2}\right)\ \mathrm{d}x, $$
where $a,b > 0 $ and real, $J_0(x)$ is the zeroth-order of Bessel function of the first kind. I found some integrals similar to the integral above, but I don't have any idea on how to apply it. Here are some integrals that might help.
$$ \int_0^\infty \cos(ax)\ J_0\left(b\sqrt{1+x^2}\right)\ \mathrm{d}x = \frac{\cos\sqrt{b^2-a^2}}{\sqrt{b^2-a^2}}; \mathrm{~~for~0 < a < b} $$ $$ \int_0^\infty \sin(ax)\ J_0(bx)\ \mathrm{d}x = \frac{1}{\sqrt{a^2-b^2}}; \mathrm{~~for~0 < b < a} $$ The proof of the first integral can be seen here .","['definite-integrals', 'trigonometry', 'bessel-functions', 'integration']"
1162514,Question about cusp cubic example in Hartshorne,"In Hartshorne's Algebraic Geometry, in Chapter II.6 on Divisors he computes the Cartier class group (denoted $\operatorname{CaCl}$) of the cuspidal cubic cut out by $y^2z=x^3$ in $\mathbb{P}^2$. He claims that there is a surjective degree homomorphism $\operatorname{CaCl}\rightarrow \mathbb{Z}$, and that there is a one to one correspondence between non-singular closed points of the cuspidal cubic and the kernel of the degree map. I am confused about his proof of this statement. In particular, he begins with ""note that any Cartier divisor is linearly equivalent to one whose local function in some neighborhood of the singular point $Z=(0,0,1)$"". How can he do that? Could you do that for any curve and argument that the points in the kernel of the degree map are in one to one correspondence with the all closed points but that one (but the same correspondence would not work). I guess part of my question is where in the proof does he actually use the cuspness? Any clarification would be appreciated. 
Thank you.","['algebraic-geometry', 'algebraic-curves']"
1162567,Union of holomorphic atlases is holomorphic atlas.,Let $S$ be a surface with open subsets $V$ and $W$ such that $s = V \cup W$. Suppose that $V$ and $W$ have holomorphic atlases $\Phi$ and $\Psi$ such that the holomorphic atlases $\Phi|_{V \cap W}$ and $\Psi|_{V \cap W}$ on $V \cap W$ are compatible. Show that $\Phi \cup \Psi$ is a holomorphic atlas on $S$.,"['algebraic-geometry', 'algebraic-curves', 'projective-geometry', 'riemann-surfaces', 'complex-analysis']"
1162576,The Matrix in ZFC: A Set-Theoretic Foundation of Matrices?,"Today, we can use ZFC to found all of mathematics. We show from ten or so axioms the existence of everything from ordered pairs and Cartesian products to relations and functions (whose existence follows from the existence of Cartesian products). I have learned, from reading books like Enderton's 1977 Elements of Set Theory , that it is not enough simply to define something (for example, we define the ordered pair of $a$ and $b$ as the set $(a,b)=\{\{a\},\{a,b\}\}$), but we must also prove that what we have defined exists within ZFC. I have done some searching, but have not found a set-theoretic construction of matrices. Wikipedia's Matrix page states the definition of an $m \times n$ matrix as simply a ""rectangular array"", but this definition does not show that a matrix is a set which actually exists in ZFC. Does anyone know how to construct matrices in ZFC? Please feel free to point me in the direction of books in set theory which tackle this. Thanks for reading my question!","['matrices', 'elementary-set-theory']"
1162581,Is there any elegant formalization of fractional numbers?,"The question is just what is on the title, but I'll describe the context for completion: Natural numbers can be encoded quite elegantly on the Lambda Calculus as church numbers, that is, a function applied to an argument N times: 0 = (λ f x → x)
1 = (λ f x → f x)
2 = (λ f x → f (f x)))
3 = (λ f x → f (f (f x)))) That representation gives us a very simple formulas to addition, multiplication and exponentiation: add = λ n f x → f (n f x)
mul = λ m n f → m (n f)
exp = λ m n → n m The representation of their inverses is not as straightforward, though. Division implementations either require recursion through the Y-combinator, or are huge formulas. This makes me suspect that church numbers are not an ideal representation of fractionals, so I've been looking for a better alternative. I guess the problem is fundamental: there is something about the nature of numbers I am missing. This is why I ask the following question: is there any kind of system/encoding in which fractional numbers can be represented elegantly? One for which algorithms such as division, logarithm and sine are as simple as the add , mul and exp above? Put short, what is the most elegant representation of fractional numbers you know?","['foundations', 'continued-fractions', 'number-theory', 'fractions', 'lambda-calculus']"
1162615,What is an ordinary differential equation equation that is yet to be solved?,"In another word, the ODE i am talking about is very special that an special method must be developed in order to solve solely that ODE approximately in infinite series. An standard method mean it could solve an class of general ODEs. I am wondering are there any, You could just give one example and its reference if you want. ""Yet to be solved"" literally mean mathematician can't figure out its solution(I don't meant that the equation that have no solution at all)","['ordinary-differential-equations', 'recreational-mathematics', 'reference-request', 'math-history']"
1162623,Measure and limsup and liminf,"Given a ring of set $R$ , $\mu$ a measure on $R$ and if $\{E_n\}$ is a sequence of sets in $R$ for which $\bigcap_{i=n}^{\infty}E_i$ for $n=1,2,...$ and $\lim \inf E_n \in R$ then $\mu(\lim \inf E_n)\leq \lim \inf \mu(E_n)$ $\bigcup_{i=n}^{\infty} E_i$ for $n=1,2,...$ and $\lim \sup E_n \in R$ then $\mu(\lim \sup E_n)\geq \lim\sup  \mu(E_n)$ How can I prove these results? I don't know how to begin. I know that it is a direct use of the definition but I only know how to work with $\lim\sup$ for set point by point, not as a whole. When I posted this question I had no idea about how to begin the problem. Now, I have resolved my doubts (someone in this forum give me an advice), but I can not post what I have done if nothing have been done, not because I was not thinking in the problem, but because nothing come to my mind to solve it. In this kind of cases (in which you really don't have any idea about how to attack the problem) I am not sure how to post here. Since I need a hint, but could happen someone thinks that I want that others solve my homework (which is not my case). Thanks!",['measure-theory']
1162633,Why does an infinite Neumann boundary condition become a Dirichlet condition?,"Often when I read a paper I see a statement of the type: Our boundary condition at the surface is $\frac{\partial f}{\partial x} = \alpha$.  In the limit of $\alpha \to \infty$ this is equivalent to the previously studied case of $f = \beta$ If it matters, I've seen this on papers dealing with chemical reactions, when the Neumann condition would represent a reaction on the surface and the Dirichlet would represent constant concentration at the surface. Why is this true? I'm honestly more interested in an intuitive or physical explanation, but I'd also be happy with a formal proof.","['chemistry', 'ordinary-differential-equations', 'partial-differential-equations']"
1162663,"Stuck on proving $\int_{-\infty}^\infty \cos(\frac{\pi}{a}x)\cos(\frac{3\pi}{a} x) \, \mathrm{d}x$ = 0","Can someone please help me to show how $$\int_{-\infty}^\infty \cos(\frac{\pi}{a}x)\cos(\frac{3\pi}{a} x) \, \mathrm{d}x = 0$$ Attempt: Trig Identity yields $$= \frac{1}{2} \int_{-\infty}^\infty \cos(\frac{4\pi}{a}x) + \cos(\frac{2\pi}{a} x) \, \mathrm{d}x$$ $$= \frac{a}{2} (\frac{\sin(\frac{4\pi}{a}x)}{4\pi} + \frac{\sin(\frac{2\pi}{a}x)}{2\pi}) $$ evaluated from $-\infty$ to $\infty$ What is a nontrivial way to show that the last expression is zero? My course notes says something about stretching of the sine function, not good enough for me.","['improper-integrals', 'calculus', 'integration', 'cauchy-principal-value', 'trigonometry']"
1162723,When does $\lim_{z\to a}f(z)$ exist when $\lim_{z\to a}|f(z)|=L'$?,"The following is an excerpt from Silverman's Complex variables with applications discussing the question in the title. However, I don't understand the bolded parts. If $\lim_{z \to a}f(z)=L$, then for a given $\epsilon \gt 0$ there exists $\delta \gt 0$ such that $$||f(z)|-|L||\le |f(z)-L| \lt \epsilon \, \text{whenever}\, 0\lt|z-a|\lt \delta$$
and therefore, $$\lim_{z\to a}|f(z)|=|L|$$
Clearly, if $L=0$, $\lim_{z\to a}|f(z)|=|L|$ iff $\lim_{z\to a}f(z)=L$. What happens if $L \neq 0?$ More precisely, if $\lim_{z\to a}|f(z)|=L'$, then is it always the case that $\lim_{z\to a}f(z)$ exists? Remember that if $\lim_{z\to a}f(z)=L$, then $|L|=L'$ and therefore, we have to examine when equality holds in $$||f(z)|-L'|=||f(z)|-|L||\le |f(z)-L|.$$ Equality would imply that $$Re(f(z)\bar{L})=|f(z)||L| \, \text{or} \, |f(z)|=Re(f(z)\frac{\bar{L}}{|L|})=Re(e^{i\theta}f(z))$$ where $\theta=Arg(\bar{L}/|L|)$, or equivalently, 
$$|e^{i\theta}f(z)|=Re(e^{i\theta}f(z))$$ so that $e^{i\theta}f(z)$ is real and nonnegative which is impossible for a general complex-valued function $f(z)$. However, this is possible when $f(z)=L'$ or $f(z)$ is a real-valued function with constant sign. Firstly, I don't see why we have to examine when equality holds in $||f(z)|-L'|=||f(z)|-|L||\le |f(z)-L|.$ What does this have to do with the existence of the limit of $f(z)$ at $a$? Next, in the second bolded part, why does the equality imply that equation? Finally, I don't understand the final statement. I see that $e^{i\theta}f(z)$ must be real because the modulus of the value depends solely on the real part, i.e. the modulus of the imaginary part is zero. And since the real part is nonnegative, the value must be real and nonnegative. But why is this only possible in the mentioned case? How can I geometrically interpret this result? I would greatly appreciate it if anyone could explain the above questions to me.","['continuity', 'complex-analysis', 'analysis']"
1162747,Probability that less dice thrown will beat more dice thrown?,"I'm trying to figure out a general equation to determine the probability that the sum of $m$ dice will be greater than the sum of $n$ dice, where $m < n$. For example, if I roll $4$ dice, what's the probability that I will beat a roll of $6$ dice? Assume standard $6$ sided dice. I've been trying to adapt the answer I found here , but I think it's really a completely separate problem.","['statistics', 'dice', 'probability']"
1162751,How do I differentiate $\sin\left(\frac1x\right)$,"How can I differentiate $\sin\left(\dfrac{1}{x}\right)$ . Do I take $\sin^{-1}(x)$ or what? 
If I let $u = \dfrac{1}{x}$ , then $\sin(u)'$ equals $\cos(u)$ , then replace $u = \dfrac{1}{x}$ , $\cos\left(\dfrac{1}{x}\right)$ .","['trigonometry', 'calculus', 'derivatives']"
1162754,Conformal Mappings dealing with Slits,"The goal is to find a conformal mapping of the domain $$U=\lbrace z: \vert z \vert <1, z\not\in [1/2,1)\rbrace$$ to the unit disc $D$. I would like to learn how to deal with the slit; I imagine in one direction the slit has to be introduced while in the other direction it has to be erased (and those are both techniques I'd like to be able to use). I have no experience with slits and examples that I have seen haven't included enough detail or images, so I don't have the right intuition when dealing with these yet. Can I get a detailed explanation (preferably with some diagrams) of how to deal with the slit in one of the directions? In going from $D$ to $U$, I would probably start by mapping onto a half-plane, but usually the slit is introduced shortly after and that's where I'm lost. I'm not sure what it means to introduce a slit.","['conformal-geometry', 'complex-analysis']"
1162764,Evaluating $\int e^{\Gamma(x)} dx $ and $\int \pi^{\Gamma(x)} dx $,"I don't know how to solve these integrals:
$$I_1 =\int e^{\Gamma(x)} dx $$
$$I_2 =\int \pi^{\Gamma(x)} dx $$ As a tenth grader I have no idea what the solutions could be. How would one go about evaluating this without computational engines? I'm asking this here because many complex problems have been tackled here...(eg: Integral $\int_{-1}^1\frac1x\sqrt{\frac{1+x}{1-x}}\ln\left(\frac{2\,x^2+2\,x+1}{2\,x^2-2\,x+1}\right) \ \mathrm dx$ ). Any hints or solutions to these integrals would be greatly appreciated. Note: I don't necessarily want closed forms; special functions are okay. [ http://en.wikipedia.org/wiki/List_of_mathematical_functions and http://en.wikipedia.org/wiki/Closed-form_expression] [[ PS: The graphs for the functions inside the aforementioned integrals are amazing! ]] Background : I was recently in the process of understanding the wonders of the gamma function. It is really fun to attend to derivatives involving subfactorials, factorials and the gamma function.
[In case someone is interested, here are some examples of expressions I was solving]  :- $$ \frac{d}{dx} [x!^{!x}!x^{x!}]^{(x!)(!x)} $$
 $$ \frac{d}{dx} \frac{\sqrt{1+\arctan(x)}}{\Gamma(x)} $$ The problem arose when I thought of the two aforementioned integrals I have no answer to.","['calculus', 'integration', 'real-analysis', 'factorial', 'complex-analysis']"
1162774,Weak convergence of Dirac measures converges to a Dirac measure?,Let $X$ be a metrizable space and $\{x_n\}$ be a sequence in $X$. Suppose the sequence of Dirac measures $\delta_{x_n} \xrightarrow{w} P$ where $P$ is some probability measure. Prove that $P = \delta_x$ for some $x \in X$. I am not sure how to go about proving this exactly... The definition of weak convergence I am using is the standard one: so $\delta_{x_n} \xrightarrow{w} P$ happens when for every continuous and bounded $f: X \xrightarrow{} \mathbb{R}$ we have $\int f  d\delta_{x_n} \xrightarrow{} \int f dP$.,"['probability-theory', 'weak-convergence', 'metric-spaces', 'functional-analysis']"
1162816,Find all continuous functions $f(x)^2=x^2$,"Find all functions $f$ which are continuous on $\mathbb R$ and which satisfy the equation $f(x)^2=x^2$ for all $x \in \mathbb R$. Clearly $f(x)=x, -x, |x|, -|x|$ all satisfy the condition. However, how can I show that these must be the only possible choices? The condition guarantees that $|f(x)|=|x|$, for all $x$ so I think it's quite obvious that these four choices are the only possibilities. But I don't see why continuity is necessary. If $f$ does not need to be continuous, are there other possibilities? Then how can I use continuity to guarantee that these are the only choices?","['calculus', 'continuity', 'real-analysis', 'analysis']"
