question_id,title,body,tags
4357747,Problem with inequality: $ \left| \sqrt{2}-\frac{p}{q} \right| > \frac{1}{3q^2}$,"Prove that for for all $p,q\in \mathbb{Z}$, $q>0$ we have:
$$
\left| \sqrt{2}-\frac{p}{q} \right| > \frac{1}{3q^2}.
$$ To be honest, I do not know where to start - any help would be appreciated.","['inequality', 'absolute-value', 'diophantine-approximation']"
4357749,Relation Between Involutions on an Elliptic Curve and the Corresponding Complex Torus,"This is a question from the book Elliptic Curves by Henry McKean and Victor Moll: Consider the cubic $X_1: y^2=x^3-x$ . It admits the involution $(x,y) \mapsto (\frac{-1}{x}, \frac{y}{x^2})$ . It is not the involution $j: (x,y) \mapsto (x,-y)$ so it comes from addition of some half-period. What is that half period? We know that every automorphism of $\mathbb{C}/\Lambda$ where $\Lambda=\{n_1\omega_1+n_2\omega_2 \mid n_1,n_2\in\mathbb{Z}\}$ for two fixed vectors $\omega_1, \omega_2 \in \mathbb{C}$ is either a translation $f(z)=z+c$ or a glide reflection $f(z)=-z+c$ . This can be shown easily by considering $\mathbb{C}$ as the universal cover of $\mathbb{C}/\Lambda$ . I know that I can find the corresponding point of a lattice on an elliptic curve using the $\wp$ function but I can not solve this problem. Any help is appreciated.","['complex-analysis', 'riemann-surfaces', 'elliptic-functions', 'elliptic-curves']"
4357754,Proving that the quotient $\ell^\infty/c_0$ is not reflexive,"Let $\ell^\infty$ be the space of bounded sequences with the maximum norm and $c_0$ the space of sequences that have limit $0$ with the same norm. I use the notation $E^*$ for the dual of a normed space $E$ . I know that $c_0$ is a closed subspace of $\ell^\infty$ , that $c_0^{**}=\ell^\infty$ , and that both $c_0$ and $\ell^\infty$ are not reflexive. How can I prove that $\ell^\infty/c_0$ is not reflexive? My best guess has been using that a space is reflexive iff its dual is reflexive. The dual of the quotient is $(\ell^\infty/c_0)^*=c_0^\perp$ , the orthogonal of $c_0$ in the dual of $\ell^\infty$ (thanks to David for the correction). This is a closed subspace of $(\ell^\infty)^*$ and $(\ell^\infty)^*$ is also not reflexive, but I don't think this implies that $c_0^\perp$ is not reflexive. I also tried using the characterization ""A Banach space is reflexive iff the unit ball is weakly compact"" but I haven't been able to prove that the ball isn't weakly compact.","['functional-analysis', 'reflexive-space']"
4357755,"If $f(s) =\frac{s}{\sqrt{1-s^2}}$, how to evaluate $\lim_{s\to \pm\infty}f^{-1}(s)$?","Let $f:[0,1]\to\mathbb{R}$ be such that $$f(s) =\frac{s}{\sqrt{1-s^2}}.$$ I wrote on my notes during the calculus class that $$\lim_{s\to +\infty}f^{-1}(s)=1\quad\mbox{ and }\quad \lim_{s\to -\infty}f^{-1}(s)=-1.$$ Looking back, how it is possible to evaluate these limits? Actually, I’m not even able to explicitly compute $f^{-1}(s)$ . Could someone please help me to understand it? Thank you in advance!","['limits', 'calculus', 'inverse-function', 'real-analysis']"
4357801,Failing condition on Lebesgue's Dominated Convergence Theorem,"I've been given the following conditions on a sequence of functions $f_k:\mathbb N\to [0,+\infty)$ $f_k(n)\to0$ as $k\to \infty$ for every $n\in\mathbb N$ . $\int f_k\text{d}\mu=1$ for every $k$ . Obviously we can't interchange limit and integral, so Lebesgue's Dominated Convergence Theorem doesn't apply here; but, what condition is failing? I've been trying to prove that if there is some $g:\mathbb N\to [0,+\infty)$ such that $f_k(n)\leq g(n)$ for every $n\in\mathbb N$ then such $g$ cannot be $L^1$ , meaning that we'd have $\int g\text{d}\mu=+\infty$ . How can I proceed? Any help will be appreciated. Thanks in advance.","['measure-theory', 'lebesgue-integral']"
4357820,Why do I get a contradiction from $\frac{\sum_{i=1}^n (X^2-\mu^2) }{N}=\frac{\sum_{i=1}^n (X-\mu)^2}{N}$?,"I keep ending up in a contradiction with sigma notation, my confusion comes from the two standard deviation squared formulas. ] 1 But going upwards $$\sigma^2=\frac{\sum_{i=1}^n X^2 }{N} -\frac{N\mu^2}{N}=\frac{\sum_{i=1}^n X^2 -N\mu^2 }{N}=\frac{\sum_{i=1}^n X^2 - \sum_{i=1}^n \mu^2}{N}=\frac{\sum_{i=1}^n (X^2-\mu^2) }{N}$$ But of course but (1) which is my most trusted statement $$\frac{\sum_{i=1}^n (X^2-\mu^2) }{N}=\frac{\sum_{i=1}^n (X-\mu)^2}{N}$$ $\implies$ $$X^2-\mu^2=(X+\mu)(X-\mu)=(X-\mu)(X-\mu)$$ which is a trival contradiction so my problem is simply to see where I made my mistake","['algebra-precalculus', 'statistics']"
4357876,Condition for a Lagrangian submanifold to be totally geodesic,"Consider $(M,\omega)$ a symplectic manifold and $L\subset M$ a Lagrangian submanifold. Let $J$ be an almost complex structure and $g:=\omega(.,J.)$ a compatible riemannian metric . Are there conditions that we can put on the lagrangian submanifold $L$ so that this becomes totally geodesic ? I know that given a totally real submanifold of an almost complex manifold one can always find a metric for which he is totally geodesic, but I was wondering if with the existence of the symplectic form $\omega$ something more could be said. Any insight is appreciated, thanks in advance.","['symplectic-geometry', 'differential-geometry']"
4357883,"How many ways are there to choose subsets $S$ and $T$ of $A=\{1,2,3,,.....,n\}$ so that $S$ contains $T$?","How many ways are there to choose subsets $S$ and $T$ of $A=\{1,2,3,,.....,n\}$ so that $S$ contains $T$ ? My attempt : The number of all subsets of $A$ is $2^n$ . Let's denote this subsets by $S_{\alpha}$ ,where $\alpha$ is the size of each subset so $S=[S_0 ,S_1 ,S_2 ,...,S_n ]$ Suppose we choose $S_3$ so we must to choose a subset T of this $S_3$ for having that $S_3$ contain T, and the number of T in this case is $2^3$ , but we must to note that there are ${ n \choose 3}$ of this $S_3$ , so the number of ways to choose a $S_3$ and T such that $S_3$ contain T is ${n \choose 3}\cdot2^3$ and we will the same thing for each $S_{\alpha}$ for example for $S_{12}$ , the number of ways to choose a $S_{12}$ and T such that $S_{12}$ contain T is ${n \choose 12}\cdot2^{12}$ and so on so the answer is that: ${n \choose 0}\cdot2^{0}+{n \choose 1}\cdot2^{1}+{n \choose 2}\cdot2^{2}+....{n \choose n}\cdot2^{n}$ and that's equal: $\sum_{k=0}^{n}{n \choose k}\cdot2^{k}=\sum_{k=0}^{n}{n \choose k}\cdot2^{k}\cdot1^{n-k}=(1+2)^n=3^n$ What do you think about my answer?","['combinations', 'analysis', 'solution-verification', 'combinatorics', 'algebra-precalculus']"
4357954,Is calculus still being developed?,"I have heard a podcast episode about calculus, with a mathematician and a journalist, on which he asked the mathematician if there still were developments being made in calculus. He answered that no, but that there were in (real) analysis. Which made me even more sense since that advanced calculus refers to (real) analysis, that real analysis is a famous next-step after calculus, and that some advanced cool stuff I have seen were said to be of analysis. So, is calculus basically (emphasis on basically ) what is learnt in calc1–calc3, being more of it real analysis? Could you, like, say that you learn t calculus? P.S.: I am not a mathematician, nor am I in any college. I also do not know if this type of question is allowed — pardon me if it be not.","['calculus', 'soft-question', 'analysis']"
4357955,Curvature as a rate of change in slope,"While working with some  formulas on cantilever in mechanical engineering , in some places it assumes that $\frac{1}{R} =\kappa = \frac{d^{2} y}{d x^{2}}$ But I know That $k=\frac{y^{\prime \prime}}{(1+y^{\prime 2})^{\frac{3}{2}}}$ So It Is Very Counterintuitive to me , So Someone please help me","['classical-mechanics', 'curvature', 'calculus', 'physics', 'derivatives']"
4357966,Computation of a left-invariant vector field,"Consider $\mathbb{R}^3\times\mathbb{R}^3$ as a Lie group endowed with the group operation $(x,y).(u,v)=(x+u,y+v+x\times u) $ where ( $x\times u $ is the cross product of the $\mathbb{R}^3$ vectors $x$ and $u$ ). I need to explicitly calculate the unique left-invariant vector field $X$ such that $X_{(0,0)}=(u,v)\in T_{(0,0)}(\mathbb{R}^3\times\mathbb{R}^3)$ Now using $X_{(a,b)}=DL_{(a,b)}(X_{(0,0)})$ after some computations I'm getting $X_{(a,b)}=(u,v+a\times u)$ . But the answer seems to be $X_{(a,b)}=(a+u,b+v+a\times u)$ . So can you please give me an explicit computation for $X_{(a,b)}$ so that I know which one of the above is correct.","['vector-fields', 'lie-groups', 'differential-geometry']"
4357987,Examining the injectivity and surjectivity of $f(x) = x/(x^2+1)$,"I'm beginning with my self study in math from the $11$ th grade, so bear with the simplicity. I've got to test the objectivity of $x/(x^2+1)$ , function is from $\Bbb R$ to $\Bbb R$ . First issue is, I think it is injective, because my algebra after equating the output of two inputs leads to both inputs coming equal to each other. The book answer says I'm wrong (without explanation). With surjectivity, I can't find a simple expression for the inverse of the function, so I thought of this: Let $g(x) = x, t(x) = x^2+1$ Then $f(x) = [g(x)/t(x)]$ . Now $g(x)$ is surjective and $t(x)$ isn't. Can I argue that the ratio however, will be surjective because the numerator is surjective, and thus even if it is being divided only by numbers greater than or equal to $1$ , the numerator's surjectivity will make the whole function surjective? If I can, is there a rigorous way of arguing this? If I can't, a counterexample or counter-proof would help.",['functions']
4357998,Reduction of independence to uncorrelatedness,"Consider two random variables $Y, Z \in \mathbb R$ . Someone claims that $Y$ is independent of $Z$ if and only if $1\{Y \ge t\}$ is uncorrelated with $Z$ for all $t \in \mathbb R$ . Is this true? The statement seems suspect to me. I can see that they are independent iff $1\{Y \ge t\}$ is uncorrelated with $1\{Z \ge s\}$ for all $s,t \in \mathbb R$ . But not the above.",['probability-theory']
4358007,Logical conclusion from a function being neither increasing nor decreasing.,"A function $g:[a,b]→\mathbb{R}$ is increasing if $∀x_1{<}x_2$ in $[a,b]$ , $g(x_1)≤g(x_2).\quad$ Similar is the definition of a decreasing function. So, if a function $f:[a,b]→\mathbb{R}$ satisfies property $p:\quad f$ is neither increasing nor decreasing, then: My conclusion ( $c_1$ ): $∃x_1{<}x_2$ and $x_3{<}x_4$ in $[a,b]$ which satisfy $f(x_1)<f(x_2)$ and f $(x_3)>f(x_4)$ (or $f(x_1)>f(x_2)$ and $f(x_3)<f(x_4)$ ). That is, $f$ 'increases' for some $x_1$ , $x_2$ and 'decreases' for some $x_3,x_4$ (or vice versa). My textbook's and instructor's conclusion ( $c_2$ ): $∃x_1{<}x_2{<}x_3$ in $[a,b]$ s.t. $f(x_1)<f(x_2)$ and f $(x_2)>f(x_3)$ (or vice-versa). My questions are: I think that $c_1$ can be logically concluded from $p$ , but not $c_2.$ Which conclusion can be derived immediately from $p$ by logic? Are the $c_1$ and $c_2$ identical? If yes, how can we prove it? There are a few theorems and proofs that can be easily proved from $c_2$ , not $c_1$ . But I think that the proofs which use $c_2$ are not complete, but neither I can prove those theorems using $c_1$ , nor prove $c_2$ from $c_1$ , so I need some help.","['logic', 'monotone-functions', 'calculus', 'functions', 'inequality']"
4358050,Is there a function whose graph is connected but contains no arcs?,"Is there $f:\mathbb{R}\to\mathbb{R}$ such that the graph is connected but for any two points in the graph, there isn´t any path in the graph between them? I´m not sure what the answer is going to be. While trying to prove that there is such a function, it only occurred to me using functions such that the image of any open set is all the real line, but that seems not to be enough to ensure that the graph is connected. Edit: The function exists, as shown in the comments of Brandon du Preez and HallaSurvivor. In fact, there are solutions $f$ with $f(x)+f(y)=f(x+y)\forall x,y$ .","['continuity', 'descriptive-set-theory', 'functions', 'examples-counterexamples']"
4358080,Will a circle projected onto a cylinder be an ellipse?,"I'm making a pump and I need to make a circular (from the front perspective) hole in a side of a pipe. I can't use a drill and I have to print out a shape that I will stick onto it and cut and file away. Will this circle projected off center onto a cylinder be an ellipse, or is it not an exact ellipse and I have to use a different shape? To further clarify, it will look something like this picture if you imagine the orange pipe is a boring bit and is not reaching beyond the centerline of the green pipe.",['geometry']
4358081,Condition on local flows that imply completeness of vector field,"Given any smooth vector field $X$ on a manifold $M$ we can cover $M$ by open sets $\{U_\alpha\}_{\alpha\in A}$ such that for each $\alpha$ there is an $\epsilon_\alpha>0$ such that if $p \in U_\alpha$ the local flow for $X$ at $p$ , say $\phi^\alpha_t$ is defined for $t\in (-\epsilon_\alpha,\epsilon_\alpha)$ . The $\phi^\alpha_t$ being local diffeomeorphisms we get a local family of local diffeomorphisms out of this i.e. $\{U_\alpha, \phi^\alpha_t,\epsilon_\alpha\}_{\alpha\in A}$ Now there is a result that if $\inf_{\alpha\in A}\epsilon_\alpha>0$ then the local family can be extended to a global one-parameter family of diffeomorphisms as follows: Say $\inf_{\alpha\in A}\epsilon_\alpha=\epsilon$ . Then define $\phi_t(p)=\phi^\alpha_t(p)$ where $p\in U_\alpha$ and $|t|<\epsilon$ else if $n$ is such that $|\frac{t}{n}|<\epsilon$ then $\phi_t(p)=\underbrace{\phi^\alpha_{\frac{t}{n}}\circ\phi^\alpha_{\frac{t}{n}}\circ...\phi^\alpha_{\frac{t}{n}}}_{n\ \ \text{times}}(p)$ The well-defined ness of the above definition etc needs to be checked here. Now my problem here is that I don't see what problem arises when $\inf_{\alpha\in A}\epsilon_\alpha=0$ .Given $p$ in a particular $U_\alpha$ the $n$ that we took above has to be adjusted so that $|\frac{t}{n}|<\epsilon_\alpha$ so we'd have to work with a variable $n$ in this case. But I don't see what harm this variable n causes to the well-definedness etc of $\phi$ (which has lead me to suspect that I have missed something vital that led to well-definedness in the result mentioned). Please help see the necessity of $\inf_{\alpha\in A}\epsilon_\alpha>0$ in the result above.","['vector-fields', 'diffeomorphism', 'differential-geometry']"
4358198,Some set theory proof,"While reading ""Understanding Analysis"" by Stephen Abbott one of the exercises was to check whether the following claim was correct, and then if so, to prove it. If $A_1 \supseteq A_2 \supseteq A_3 \supseteq A_4 · · ·$ are all sets containing an infinite number of
elements, then the intersection $\bigcap\limits_{n \in \mathbb{N}} A_n$ is infinite as well. It would be a great help if anyone could read through my proof and tell me whether there are any mistakes. Also I want to know if it is coherent and whether there is too little/too much detail. Lemma: $\bigcap\limits_{n=1}^{k} A_n=\bigcap\limits_{n=1}^{k-1} A_n \cap A_k$ Let $x\in \bigcap\limits_{n=1}^{k} A_n$ , then by definition of intersection, $x\in A_1, x \in A_2...x\in A_{k}$ . Hence $x\in A_k$ , and $x\in A_1, x\in A_2...x\in A_{k-1}$ .It follows that $\bigcap\limits_{n=1}^{k} A_n \subseteq\bigcap\limits_{n=1}^{k-1} A_n \cap A_k$ . Let $x\in\bigcap\limits_{n=1}^{k-1} A_n \cap A_k$ , then by definition of intersection, $x\in A_k$ , and $x\in A_1,x\in A_2...x\in A_{k-1}$ . Hence $x\in A_1,...,x\in A_k$ . Thus $x\in \bigcap\limits_{n=1}^{k} A_n$ . Thus $\bigcap\limits_{n=1}^{k} A_n \supseteq\bigcap\limits_{n=1}^{k-1} A_n \cap A_k$ . We can conclude that $\bigcap\limits_{n=1}^{k} A_n = \bigcap\limits_{n=1}^{k-1} A_n \cap A_k$ . To prove 1) we use mathematical induction. Let $S$ be a subset of $\mathbb{N}$ so that if $k \in S$ , if $A_1 \supseteq A_2  · · ·\supseteq A_k$ are all sets containing an infinite number of
elements, then the intersection $\bigcap\limits_{n=1}^{k} A_n$ is infinite as well. It is trivial that this is the case when $k=1$ . Let the property be fulfilled for some $k \in \mathbb{N}$ , then we shall prove that $k+1$ also fulfills that property(and thus is also in $S$ ). Let $A_1 \supseteq A_2  · · ·\supseteq A_{k+1}$ be all sets containing an infinite number of elements. From the lemma, we know that $\bigcap\limits_{n=1}^{k+1} A_n=\bigcap\limits_{n=1}^{k} A_n \cap A_{k+1}$ . Furthermore, $\bigcap\limits_{n=1}^{k} A_n \cap A_{k+1}=A_{k+1}$ , since $A_1 \supseteq A_2  · · ·\supseteq A_{k+1}$ . Hence $\bigcap\limits_{n=1}^{k+1} A_n=A_{k+1}$ , and since $A_{k+1}$ is a set with an infinite number of elements, so is $\bigcap\limits_{n=1}^{k+1} A_n$ . Hence $k+1 \in S$ . By induction, $S = \mathbb{N}$ . Therefore, if $A_1 \supseteq A_2 \supseteq A_3 \supseteq A_4 · · ·$ are all sets containing an infinite number of
elements, then the intersection $\bigcap\limits_{n \in \mathbb{N}} A_n$ is infinite as well.","['elementary-set-theory', 'proof-writing', 'solution-verification']"
4358236,Bivariate negative binomial distribution for 2d count data,"Bivariate negative binomial distribution The probability mass function (PMF) of a bivariate negative binomial distribution ( $\mathsf{BNBin}$ ) is given by [1]: $$P(X=x, Y=y) = \frac{(a + x + y - 1)!}{(a-1)! x! y!} p_0^a p_1^x p_2^y $$ where $a, p_0, p_1, p_2 > 0$ and $p_0 + p_1 + p_2 = 1$ . It can be shown that the marginal $P(X)$ of this distribution follows a negative binomial ( $\mathsf{NBin}$ ) with parameters $r=a$ and $p=\frac{p_1}{1 - p_2}$ (and similarly $r=a$ and $p=\frac{p_2}{1 - p_1}$ for $P(Y)$ ). Small motivating example Consider the following 2d count dataset: $X$ 0 0 1 1 1 1 2 2 3 3 4 4 4 4 4 5 5 7 8 9 $Y$ 7 6 5 5 4 3 3 3 3 3 3 2 2 2 2 1 0 0 0 0 The marginal distributions are both $\mathsf{NBin}$ : However, the joint distribution $P(X, Y)$ cannot effectively be modeled as a $\mathsf{BNBin}$ . This is because the covariance of $\mathsf{BNBin}$ , $\text{Cov}(X, Y) = \mathbb{E}[XY] - \mathbb{E}[X] \mathbb{E}[Y] = \frac{ap_1p_2}{p_0}$ , is always positive, but the variables $X$ and $Y$ are negatively correlated (the sample covariance is approximately -0.77). Questions Under what conditions is the $\mathsf{BNBin}$ distribution suitable for 2d count data? Is there any other 2d distribution for count data that generalises $\mathsf{BNBin}$ ? If not, how could $\mathsf{BNBin}$ be extended to deal with these failure cases? (e.g. negative correlation between both variables) Any comment or suggestion will be greatly appreciated. References: [1] Dunn 1967, Characterization of the Bivariate Negative Binomial Distribution ( pdf )","['statistics', 'probability-distributions', 'discrete-mathematics', 'probability']"
4358238,Using the universal characterisation to derive properties of the quotient group and projection,Let $N$ be a normal subgroup of some group $G$ . If I understand it correctly a group $Q$ and a homomorphism $\pi \colon G \to Q$ are a quotient group of $G$ by $N$ if for any group $H$ and homomorphism $\phi \colon G \to H$ with $N \subseteq \ker(\phi)$ there is a a unique homomorphism $\psi \colon Q \to H$ with $\phi = \psi \circ \pi$ . I was wondering if this characterization is enough to prove that $\ker(\pi)=N$ and $\pi(G)=Q$ or if details of the standard quotient construction in terms of cosets is required to prove these facts.,"['group-theory', 'abstract-algebra', 'category-theory']"
4358249,Is being directly negatively proportional same as being inversely proportional?,"If I have the proportionality $$x\propto-y$$ It suggests that x increases with $-y$ and since there's a negative symbol present it would mean that the lower the value of $y$ the higher the value of $x$ ...so does this still qualify as directly proportional? It does seem to be different from $x\propto\frac{1}y$ so I guess it's not inversely proportional? The direct relation of $y$ with $x$ sounds wrong to be called directly proportional, so what exactly is a negative proportionality called if not being the same as inversely proportional? Thanks.","['algebra-precalculus', 'terminology']"
4358267,Understanding proof of discrete optimal sampling theorem,"Let $X = \{X_n\}_{n=0}^{\infty}$ be a closable submartingale. Then, for any
stopping time $τ, X_τ$ is integrable and, for another stopping time $σ$ , $E[X_\tau |\mathcal{F}_\sigma ]\ge X_{\sigma\wedge \tau}$ , P-a.s. The proof start by showing $E\left[X_{\infty}|\mathcal{F}_{\sigma}\right]\ge X_{\sigma}$ where $X_{\infty}$ is the closing element, and then says it suffices to show that the stopped process $X^{\tau}=\{X_{n\wedge\tau}\}_{n=0}^{\infty}$ is closable. But how does this help us to conclude?","['conditional-expectation', 'martingales', 'stopping-times', 'probability-theory']"
4358314,An inequality involving the Laplacian and the norm of the gradient,"For any given probability density function $p$ (with finite second moments) on $\mathbb{R}^d$ I want to show that the following integral is bigger than $\textit{(or equal to)}$ zero $$ \int_{\mathbb{R}^d} \Big(\Delta\Psi(x)-\|\nabla \Psi(x)\|^2\Big) p(x)dx. $$ Obviously here I need to impose some assumptions on the function $\Psi$ . Since this needs to hold for any probability density $p$ , I think the easiest thing to ask is : Are there any conditions I can impose on a function $\Psi :\mathbb{R}^d \to \mathbb{R}$ such that $$ \Delta\Psi(x)-\|\nabla \Psi(x)\|^2 \geq 0, ~~\text{for}~a.e~x\in\mathbb{R}^d. $$","['functional-inequalities', 'harmonic-functions', 'laplacian', 'functions', 'inequality']"
4358323,How to prove that the following metric induces the subspace topology?,"I am trying to follow Theorem (3.11) of Kechris's Classical Descriptive Set Theory. In this part of the proof he shows that a $G_{\delta}$ -subspace Y of a completely metrizable space $(X,d)$ is completely metrizable. For this, he defines the following metric. Sadly, he skips over why this new metric induces the subspace topology. Here is the relevant part of the proof: (screenshot) $\quad$ For the second assertion, let $Y=\bigcap_{n} U_{n}$ , with $U_{n}$ open in $X$ . Let $F_{n}=X \backslash U_{n}$ . Let $d$ be a complete compatible metric for $X$ . Define a new metric on $Y$ , by letting $$
d^{\prime}(x, y)=d(x, y)+\sum_{n=0}^{\infty} \min \left\{2^{-n-1},\left|\frac{1}{d\left(x, F_{n}\right)}-\frac{1}{d\left(y, F_{n}\right)}\right|\right\} .
$$ It is easy to check that this is a metric compatible with the topology of $Y$ . Because of $d'(x,y)\geq d(x,y)$ it follows that any for $\epsilon > 0$ and any $x\in X$ , $B_{d'}(x, \epsilon) \subset B_d(x, \epsilon)$ . So any set open in $(X, d)$ is also open in $(Y, d')$ . But I can't figure out the other direction. Any help is appreciated. Kechris, Alexander S. , Classical descriptive set theory, Graduate Texts in Mathematics. 156. Berlin: Springer-Verlag. xx, 402 p. (1995). ZBL0819.04002 .","['metrizability', 'general-topology', 'descriptive-set-theory', 'polish-spaces']"
4358351,Each extra-special group of order $2^{2n+1}$ is a central product of $D_8$s or of $D_8$s and a single $Q_8$.,"This is Exercise 5.3.7(i) of Robinson's, ""A Course in the Theory of Groups (Second Edition)"" . According to this search , it is new to MSE. This is a classification problem. This Wikipedia entry describes the result. The Details: Let $p$ be prime. A $p$ -group is a group all of whose elements have order $p$ . Let $G$ be a finite $p$ -group. Then $G$ is extra-special if $G'$ (the derived subgroup of $G$ ) and $Z(G)$ (the centre of $G$ ) coincide and have order $p$ . The quaternion group $Q_8$ is defined to be the group given by the presentation $$\left\langle x,y\, \middle|\, x^{2^2}=1, y^2=x^2, y^{-1}xy=x^{-1}\right\rangle.$$ The dihedral group $D_8$ of order eight is the group given by the presentation $$\left\langle r,s\,\middle|\,r^{2^2}, s^2, srs=r^{-1}\right\rangle.$$ Let $G$ be a group with normal subgroup $G_1,\dots, G_n$ . Then $G$ is the central product $G_1\circ\dots\circ G_n$ of those normal subgroups if: $G=G_1G_2\dots G_n$ , $[G_i, G_j]=1$ for $i\neq j$ , and for all $i$ , $$G_i\cap\prod_{j\neq i}G_j=Z(G).$$ Robinson claims that $Z(G_i)=Z(G)$ . The Question: Paraphrased: Consider an extra-special group $G$ of  order $2^{2n+1}$ . Prove $G$ is a central product of the $D_8$ s or a central product of $D_8$ s and a single $Q_8$ . There is a hint (which I have paraphrased): Prove that a central product of two $D_8$ s is a central product of two $Q_8$ s. Thoughts: I thought I'd rewrite the question in terms of group presentations, so I asked the following question: Given $H=\langle X_H\mid R_H\rangle$ and $K=\langle X_K\mid R_K\rangle$, find a presentation for Robinson's $H\circ K$ It turns out that it is insufficient to know presentations of groups $H,K$ in order to find a presentation of $H\circ K$ . I don't know how to use the hint, let alone prove what it suggests I prove. My guess, though, is that the parity of $n$ determines whether there is a $Q_8$ term in the central product (since, if I'm right, we can replace an even number of $Q_8$ s by the same number of $D_8$ s in the central product). According to the Wikipedia entry (cited above), the number of $D_8$ s in the central product does not depend on the parity of $n$ . If $G$ is an extra-special $2$ -group of order $2^{2n+1}$ , then $G'=Z(G)\cong \Bbb Z_2$ . Previous, relevant questions of mine include: Find the upper central series of $Q_{2^n}$. Showing ${\rm Aut}(D_{2^n})\cong{\rm Aut}(Q_{2^n})$ for $n\ge 4$. Showing ${\rm Aut}(Q_{2^n})\cong{\rm Hol}(\Bbb Z_{2^{n-1}})$ for $n>3$ Please help  :)","['dihedral-groups', 'finite-groups', 'p-groups', 'group-theory', 'quaternions']"
4358392,Conditions on sets $A \subset B \subset X$ for the equality $\mu[B\setminus A] = \mu[B] - \mu[A]$ to hold,"Let $\mu$ be an outer measure for the set $X$ and $A \subset B \subset X$ . I am wondering what are the conditions on the sets $A$ and $B$ for the equality $\mu[B\setminus A] = \mu[B] - \mu[A]$ to hold. If both $A$ and $B$ are $\mu$ -measurable and $\mu[A] < \infty$ , then the proof is equality is quite evident as $\mu[B] = \mu[B \cap A] + \mu[B\setminus A] \Longleftrightarrow \mu[B \setminus A] = \mu[B] - \mu[A]$ Since the measurability of $B$ was not really used, it seems that $B$ does not need to be $\mu$ -measurable for the equality to hold. Also if $B$ were to be $\mu$ -measurable, but $A$ wasn't event with $\mu[A] < \infty$ , then if $X = \{a, b\}, \mu[\{a, b\}]\{a\}] = \mu[\{b\}] = 1, \mu[\varnothing] = 0$ , shows that the equality does not hold if $B$ is $\mu$ -measurable, but $A$ was not, as if $B = X, A = \{a\}$ we'd have $\mu[X \cap A] + \mu[X \setminus A] = 1 + 1 \neq 1 = \mu[A]$ . In addition the equality can hold only if $\mu[A]$ is finite, as otherwise if $\mu[A] = \infty$ , then also $\mu[B] = \infty$ and we'd have $\infty - \infty$ in the rhs of the equation, which is not defined. Is there anything other to consider?",['measure-theory']
4358397,Why $F^S$ represents set of functions from S to F?,"This is a definition from the book ""Linear Algebra Done Right"" $F^S$ Notation If $S$ is a set, then $F^S$ denotes the set of functions from $S$ to $F$ . And $F^S$ is a vector space with following addition and scalar multiplication. $\forall f, g \in F^S, \ \exists f + g \in F^S$ is a function defined by $$\forall x \in S, \ (f + g)(x) = f(x) + g(x)$$ $\forall \lambda \in F, \ \forall f \in F^S, \ \exists \lambda f \in F^S$ is the function defined by $$\forall x \in S, \ (\lambda f )(x) = \lambda f(x)$$ Here $F$ denotes a field. My Question is why $F^S$ , I don't understand the intuition behind this notation. Everything work perfectly fine even if we say Let $X$ be set of all function from $S$ to $F$ . It feels like an arbitrary choice, why $F^S$ ? Is there a deeper meaning or some historical reason? Sorry if the question is stupid.","['elementary-set-theory', 'notation']"
4358431,"$ \int_{[0,1]^n} \min(x_1,\ldots,x_n) \, dx_1\cdots dx_n $","I need to compute: $$ \int_{[0,1]^n} \min(x_1,\ldots,x_n) \, dx_1\cdots dx_n $$ I have shown that: $$\int _{0}^{1} \min( x_{k} ,\dots,x_{n})^{m} dx_{k} = \min( x_{k+1} ,\dots,x_{n})^{m} -\frac{m}{m+1} \min( x_{k+1} ,\dots,x_{n})^{m+1}$$ and tried to do it recursively: $ \begin{array}{l}
\int _{0}^{1} ...\int _{0}^{1} \min( x_{1} ,...,x_{n}) dx_{1} ...dx_{n} =\int _{0}^{1} ...\int _{0}^{1} \min( x_{2} ,...,x_{n}) -\frac{1}{2} \min( x_{2} ,...,x_{n})^{2} dx_{2} ...dx_{n} =\\
=\int _{0}^{1} ...\int _{0}^{1} \min( x_{3} ,...,x_{n}) -\frac{2}{2}  \min( x_{3} ,...,x_{n})^{2} +\frac{1}{3} \min( x_{3} ,...,x_{n})^{3} dx_{3} ...dx_{n} =
\end{array}$ but I don't see how I can get something from this.
Can anyone help?","['multivariable-calculus', 'multiple-integral']"
4358461,"Interpretation of ""Noise"" in Function Optimization","I am trying to better understand the meaning of ""noise"" with regards to function optimization - specifically, why ""Noisy"" functions are more difficult to optimize compared to ""Non-Noisy"" functions. Up until now, I always thought of ""noise"" from a signal processing standpoint: for example - how to remove and filter out the noise component from some signal: I also generally think of this in the context of Time Series Analysis, where a time series is separated into non-random components (e.g. seasonal) and random components (e.g. noise): In both of these above cases, ""Noise"" is viewed as something with inherent ""negative connotations"", as something undesirable which is either hindering or further complicating the end goal of (usually) a forecasting or engineering project. However, I am interested in ""noise"" from more of a Machine Learning and Optimization perspective. For instance, (I am not sure if this is correct) I have heard that since the ""loss functions"" of Machine Learning algorithms are always modelling a random variable - thus, any ""loss function"" of a Machine Learning algorithm is always considered to be a ""noisy function"": My Question: Why are ""Noisy"" Functions difficult to optimize compared to ""Non-Noisy"" Functions? I can understand that ""Noisy"" Functions contain ""random noise"" (as the name implies) which alters their ""fidelity"" with regards to the concept they are attempting to represent (i.e. an additional source of ""difficulty"" when attempting to use them for some applied purpose) - but are ""Noisy"" Functions inherent more ""computationally expensive"" to evaluate (e.g. their derivatives) compared to ""Non-Noisy"" and ""Lesser-Noisy"" Functions of similar complexity? How exactly does the ""Noisiness"" of a function contribute to its computational complexity (to the extent that gradient-free methods are often used on ""Noisy"" Functions in order to reduce their ""computational costs"")? I have heard the following argument being made on an informal level : Given that ""Noisy"" Functions are often more ""computationally expensive"" to optimize, and that no major theoretical results have been established on the convergence properties of gradient-based optimization algorithms on ""Noisy"" Functions - using gradient-free optimization algorithms (e.g. evolutionary algorithms, genetic algorithm, metaheuristics) might have certain advantages in optimizing such ""Noisy"" Functions. Have any significant theoretical results been established regarding the convergence properties of common optimization algorithms (e.g. gradient descent, stochastic gradient descent) on ""Noisy"" Functions? Thanks! References: http://pages.cs.wisc.edu/~ferris/talks/Informs-washington.pdf https://noisyopt.readthedocs.io/en/latest/ https://hal.archives-ouvertes.fr/hal-01306636v2/document","['noise', 'machine-learning', 'functions', 'optimization', 'convergence-divergence']"
4358469,Laurent series of $f(z) = \frac{z - 4}{(z+1)^2(z - 2)}$,I want to expand function $$f(z) = \frac{z - 4}{(z+1)^2(z - 2)}$$ for $1 < |z| < 2$ . My work so far By partial fraction decomposition we can get that: $$\frac{z - 4}{(z + 1)^2(z - 2)} = \frac{-3(z - 4)}{(z + 1)^2} + \frac{\frac 1 9 (z - 4)}{z - 2}$$ Now deriving Laurent expansion for second term is not so hard: $$\frac{1}{z - 2} = -\frac{1}{2} \frac{1}{1 - \frac z 2} = - \frac 1 2 \sum_{n = 0}^\infty (\frac z 2)^n = - \sum_{n = 0}^\infty \frac{z^2}{2^{n + 1}}$$ So we have that: $$ \frac{\frac 1 9 (z - 4)}{z - 2} = \sum_{n = 0}^\infty - \frac 1 9 \cdot \frac{z^n}{2^{n + 1}}(z - 4)$$ But I'm quite struggling with expanding $\frac{1}{(z + 1)^2}$ . I tried several possibilities but those all led me nowhere. Can I ask you for a hand in expanding form $\frac{1}{(z + 1)^2}$ ? EDIT We know that $\frac{1}{1 + z} = \sum_{n = 0}^\infty \frac{(-1)^n}{z^{n + 1}}$ As I understood the idea with derivative is to: $$\frac{d(\frac{1}{1 + z})}{dz} = \sum_{n = 0}^\infty \frac{d(\frac{(-1)^n}{z^{n + 1}})}{dz}$$ $$- \frac{1}{(1 + z)^2} = \sum_{n = 0}^\infty - (n + 1)z^{-n-2}(-1)^n$$ $$\frac{1}{(1 + z)^2} = \sum_{n = 0}^\infty (n + 1)z^{-(n + 2)}(-1)^n$$ Does it make any sense to you?,"['complex-analysis', 'laurent-series', 'sequences-and-series']"
4358504,Confidence interval for the variance of exponential distribution?,"I know that if $X \sim \operatorname{Exp}(\theta)$ , and $Y=\theta X \implies Y \sim \operatorname{Exp}(1)$ . I want a confidence interval for $E(X)= \frac{1}{\theta}$ and $\operatorname{Var}(X)= \frac{1}{\theta^2}$ The confidence interval for the mean is: $P(a<Y<b)=1-\alpha \implies P(a<\theta X<b)=1-\alpha $ And then $P(\frac{X}{b}<\frac{1}{\theta}<\frac{X}{a})=1-\alpha$ . Can I do this? : $P((\frac{X}{b})^2<(\frac{1}{\theta})^2<(\frac{X}{a})^2)=1-\alpha \ $ is a confidence interval for the variance. I'm not sure if this is correct.","['statistical-inference', 'statistics', 'confidence-interval', 'probability-distributions', 'probability']"
4358505,Is the characterization of Hausdorff spaces in terms of ultrafilter convergence equivalent to the ultrafilter lemma?,"It can be easily proven using the ultrafilter lemma that if every ultrafilter on a topological space converges to at most one point, then the space is Hausdorff. My question is is whether this statement implies the ultrafilter lemma. It certainly implies some fragment of it, for instance it implies that every infinite set has a nonprincipal ultrafilter (the cofinite topology on an infinite set is not Hausdorff and also has the property that every principal ultrafilter converges to a unique point). Apologies if this is in the consequences of AC book… I’ve temporarily lost access to my copy.","['separation-axioms', 'filters', 'axiom-of-choice', 'general-topology', 'set-theory']"
4358514,Intuition behind conditioning to events with probability zero,"What's the intuition behind why the conditional expectation w.r.t. a $\sigma$ -algebra allows us to condition to events with zero probability? For example, let’s say we have two continuous random variables $X$ and $Y$ defined on $(\Omega,\mathcal{F},\mathbb{P})$ . We define the conditional probability density function of $Y$ given $X=x_0$ , as $$f_{Y|X}(y,x_0)=\frac{f_{XY}(x_0,y)}{f_X(x_0)}.$$ Even though in this definition $f_X$ has to be strictly greater than $0$ , we are still conditioning to an event with probability zero $\mathbb{P}(X=x_0)=0$ , as $X$ is a continuous r.v. I understand that the concept of conditional expectation of a random variable (and conditional probability of an event) to a $\sigma$ -algebra was introduced in order to deal with conditioning to events with probability zero, but I haven’t quite grasped the intuition behind this.","['measure-theory', 'conditional-probability', 'conditional-expectation', 'probability-theory', 'probability']"
4358532,Differential formula for polynomial,Let $g(x)$ be a polynomial whose degree  is $m$ . I want to show that $$\lim_{h\rightarrow 0}\frac{\sum_{j=0}^m\binom{m}{j}(-1)^{m-j}g(x+jh)}{h^m}=\frac{d^m}{dx}g(x).$$ My idea is to proceed by induction on $m$ . The formula holds trivially for $m=1$ . Assume we have the equation for $m$ . Let $g(x)$ be a polynomial with degree $m+1$ . We have to show $$\lim_{h\rightarrow 0}\frac{\sum_{j=0}^{m+1}\binom{m+1}{j}(-1)^{m+1-j}g(x+jh)}{h^{m+1}}=\frac{d^{m+1}}{dx}g(x).$$ I tried to expand the numerator using the identity $\binom{m+1}{j}=\binom{m}{j}+\binom{m}{j-1}$ but that got me nowhere. It's not obvious to me how I can utilize the inductive hypothesis. Maybe a proof by induction is not the right approach? Any hints/suggestions?,"['derivatives', 'polynomials', 'real-analysis']"
4358548,Question on credible intervals for the parameter of an exponential distribution with Gamma prior,"Suppose we have a random sample $X_1, ..., X_n$ where $X \sim$ Expo $(\theta)$ (here $\theta$ is the rate). We're studying the Bayes estimator of $\theta$ and its properties. For a Gamma $(\alpha, \beta)$ ( $\beta$ as scale) prior, I found the posterior distribution to be $$\text{Gamma}(\alpha + n, \left( \sum X_i + 1 / \beta \right)^{-1})$$ and consequently the Bayes estimator as $$T^{B}(X_1, ..., X_n) = \frac{\alpha + n}{\sum X_i + 1 / \beta}. $$ Now I'm tasked to find a credible region for $\theta$ , but I'm quite confused on how to find this. Initially I thought this could just be the quantiles of our posterior distribution, but looking a bit around online, this doesn't seem to be the case...","['statistical-inference', 'statistics', 'bayesian']"
4358568,Markov chain returning to its starting position,"Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space and let $(X_n)_{n \in \mathbb{N_0}}$ be an $I$ -valued (homogeneous) Markov chain, where $I \subset \mathbb{R}$ is countable. For $i \in I$ , set $$
T_i := \inf\{ n \geq 1 : X_n = i \} \quad (\inf \emptyset = + \infty),
$$ which represents the first time $X$ takes the value $i$ starting from time $1$ . Furthermore, set $T_i^{(1)} := T_i$ and $$
T_i^{(k)} := \inf\{ n \geq T^{(k-1)}_i : X_n = i \}, \quad k \geq 2.
$$ Now let $i, j \in \mathbb{N}$ be such that $i \neq j$ and $$
\mathbb{P}( T_j < T_i \mid X_0 = i ) = \mathbb{P}( T_i < T_j \mid X_0 = j ). \tag{1}
$$ Denote by $\mathbb{P}_i (B) := \mathbb{P} ( B \mid X_0 = i )$ , $\mathbb{P}_j (B) := \mathbb{P} ( B \mid X_0 = j )$ , $B \in \mathscr{B}(\mathbb{R})$ . Task: Given $X_0=i$ , determine the expected number of visits to $j$ before the chain returns to $i$ . Hint: Let $N$ denote the number of visits to $j$ before it returns $i$ . Show that $$
\mathbb{P}_{i} (N \geq k) = \mathbb{P}_i (T_i > T_j) \left( \mathbb{P}_j (T_i > T_j) \right)^{k-1}. \tag{2}
$$ My approach:
One can first observe that $$
N = \sum_{ k = 1 }^{ \infty } \mathbb{1}_{\{ X_k = j \}} \mathbb{1}_{\{ T_i > k \}},
$$ which is well-defined as a countable sum of non-negative random variables.
If one is able to show $(2)$ , then due to $(1)$ , it follows that $$
\mathbb{P}_{i} (N \geq k) = \mathbb{P}_i (T_i > T_j) \left( \mathbb{P}_j (T_i > T_j) \right)^{k-1} = \mathbb{P}_j (T_j > T_i) \left( \mathbb{P}_j (T_i > T_j) \right)^{k-1} = \alpha ( 1 - \alpha)^{k-1},
$$ where $\alpha : =  \mathbb{P}_j (T_j > T_i)$ .
Since $N$ is non-negative, we can further use the formula $$
\mathbb{E}_i(N) = \sum_{ k = 1 }^{\infty} \mathbb{P}_i ( N \geq k ) = \sum_{ k = 1 }^{\infty} \alpha ( 1 - \alpha)^{k-1} = \sum_{ k = 0 }^{\infty} \alpha ( 1 - \alpha)^k = \frac{\alpha}{1 - (1 -\alpha )} = 1,
$$ if $0< \alpha < 1$ ; otherwise $\mathbb{E}_i(N) = 0$ . But how can one show $(2)$ ? It is possible to observe that for $k \geq 1$ , we have $N \geq k$ if and only if the first time $X$ visits $i$ is strictly greater than the $k$ 'th time $X$ vsits $j$ , i.e., $$
\mathbb{P}_{ i }  ( N \geq k ) = \mathbb{P}_i ( T_i >  T^{(k)}_j ) = \ldots
$$ Can this be transformed to $(2)$ ? If the answer includes the application of a certain property of Markov chains, I would kindly ask to state this property explicitly as well.","['stochastic-processes', 'markov-chains', 'probability-theory', 'probability']"
4358606,$u_n$ converges if and only if $\frac1{\sum_{k=0}^n a_k} \sum_{k=0}^n a_ku_k $ converges.,"Let $(a_n)_{n \in \mathbb{N}} $ be a positive sequence with $a_0\neq0$ . Find a necessary and sufficient condition on $(a_n) $ in order that: for any real sequence $(u_n)_{n \in \mathbb{N}}$ , $$\boxed{(u_n)_{n \in \mathbb{N}}\quad \text{converges}\quad \iff\quad \left(\frac1{\sum_{k=0}^n a_k} \sum_{k=0}^n a_ku_k  \right)_{n \in \mathbb{N}}\quad  \text{converges }}$$ Add 1: Thank you very much @SeverinSchraven , I see now that the condition sought is $\liminf_{n\rightarrow \infty} \frac{a_n}{ \sum_{k=0}^n a_k} >0.$","['sequences-and-series', 'functional-analysis', 'cesaro-summable', 'real-analysis']"
4358618,Do cyclic groups appear as symmetry groups of exponentiation-only terms?,"Let $\mathfrak{E}=(\mathbb{N};\mathsf{exp})$ be the structure consisting of the natural numbers together with exponentiation. Given a term $t$ in the language of $\mathfrak{E}$ - that is, an expression built only out of variables and the exponentiation function - we can associate to $t$ the group $E_t$ consisting of all permutations of the variables occurring in $t$ which result in the same function when interpreted in $\mathfrak{E}$ . So for example, abusing notation in the obvious way, if $t=((x^y)^z)^w$ then $E_t\cong S_3$ since the three variables $y,z,w$ can be freely permuted. I've asked earlier exactly which groups appear as symmetry groups of terms in $\mathfrak{E}$ . However, that seems harder than I originally expected. I think the following special case is more likely to have a quick answer: Is there a term $t$ , using exponentiation alone, with $E_t\cong C_3$ (or indeed any cyclic group $C_n$ for $n>2$ )? I suspect the answer is negative , but I don't immediately see how to prove this. Note that the equational theory of this structure is generated by the single nontrivial equation $(x^y)^z=(x^z)^y$ ; see here .","['universal-algebra', 'exponentiation', 'finite-groups', 'logic', 'group-theory']"
4358643,"Does my operator $C([0,1]) → C([0,1])$ have an inverse?","Consider the Banach space $X := C_\mathbb{C}([0,1])$ with the supremum norm. Consider the bounded linear operator $T : X → X$ given by $$ (Tf)(t) = \int_0^t 2sf(s)ds + f(1).$$ I want to know whether $4I - T$ (with $I$ the identity operator) has an inverse in $B(X)$ . I know this is the case whenever $\lVert(4I-T) - I\rVert = \lVert3I-T\rVert < 1$ . Indeed, the Neumann series $\sum_{n=0}^∞ (T - 3I)^n$ would then be a candidate, and by uniqueness, the inverse of $4I-T$ . Now if I perform the calculation, I end up with, if I have interpreted all the norms in the right ways, $$\lVert 3I - T\rVert = \sup_{\lVert f \rVert ≤ 1} \sup_{t ∈ [0,1]} \left\{\left|3f(t) -2\int_0^t 2sf(s)ds + f(1) \right|\right\} ≤ \left|3 - \int_0^1 2sds + 1\right|≤ 5.
$$ Now, although this is not less than $1$ , why could I not simply multiply by, say, ⅙, obtain an operator with norm ⅚ $< 1$ , and later scale up by $6$ again, which should not matter much in the vector space setting we are in. Perhaps I'm approaching this the complete wrong way, though. EDIT: For a further question (whether $T$ is compact), I know that, since $[0, 1]$ is compact, $\overline{\mathcal{B}_{00}(C_{\mathbb{C}}([0,1]))} = \mathcal{B}_0(C_{\mathbb{C}}([0,1]))$ . That is, if $T$ is compact, we can approximate $T$ by a sequence of finite-rank operators. One easy way to define a sequence that converges to $T$ is to define, for every $n ∈ ℕ$ , the operators $T_n$ given by $$ (T_nf)(t) = \int_0^{t(1-\frac{1}{n})}2sf(s)ds + f(1),$$ but I don't know whether these are of finite rank, and I generally have trouble with such arguments in infinite-dimensional vector spaces..","['banach-spaces', 'compact-operators', 'functional-analysis', 'analysis']"
4358654,"An ""Easy"" Truncation Procedure","Let $X$ be a real random variable on a probability space $(\Omega,F,P)$ of mean $0$ and variance $1$ . I am asked to prove that that for every $\epsilon>0$ there is a function $\phi: \mathbb{R} \to \mathbb{R}$ that satisfies all of the following: $\phi$ is bounded and continuous in $\mathbb{R}$ . If $Y=\phi \circ X$ , then $Y$ still has mean $0$ and variance $1$ . The random variable $Z=X-Y$ has variance at most $\epsilon^2$ . Here is my attempt: Although I am asked to prove the result, it seems to me that this is not doable. Let $\mu$ be the law of $X$ . If such a $\phi$ exists, then we have: \begin{equation*}
0=\mathbb{E}[X]=\int_{\mathbb{R}}xd\mu(x) = \int_{\mathbb{R}}\phi(x)d\mu(x)=\mathbb{E}[\phi \circ X]
\end{equation*} Then $\phi(x)=x$ almost everywhere with respect to $\mu$ . But $x$ is obviously not bounded so I am stuck here. Hints would be really appreciated and a specific example of $\phi$ is even better. I think whoever designs the question has a specific example of $\phi$ in mind but I was not able to find it. Edit: as it is pointed out in comments, I somehow thought that $\int_{\mathbb{R}}|x-\phi(x)|d\mu(x)=0$ and concluded $\phi=x$ in my attempt, which is simply not right. So the problem is more likely to be solvable. However, I am still stuck on approaching the problem.",['probability-theory']
4358684,How to Use Math Induction to prove my formula?,"On day one, I was fined 2 dollars. Every subsequent day the fine is squared (i.e., day 1: 2, day 2: 4, day 3: 16, day 4: 256, day 5: 65,536). After some analysis, I came up with the formula to determine the fine amount D given the day N: D = $2^{2^{(N-1)}}$ . I am now trying to prove this formula is true using mathematical induction, but I do not know what to show for the k + 1 step.
Assume N=k is true: D = $2^{2^{(k-1)}}$ Show k + 1 is true: ???","['induction', 'discrete-mathematics', 'algorithms']"
4358751,Alternatives For Numerical Differentiation,"For a long time, the following point always confused me: In High School Calculus, we are told that almost all functions we encounter are are analytically differentiable  (i.e. have derivatives) but not all functions are analytically integrable (i.e. have integrals) - if this is true, then why do we need numerical differentiation? Then, I found out that numerical differentiation is used in instances when we do not have the actual equation of the ""function"", but instead only have observed data points from this ""function"" instead: Thus, it seems as Numerical Differentiation is providing us with a way to ""interpolate derivatives"" at arbitrary points over an observed range. This leads me to my next point: it seems that there are many instances in statistics where we are faced with a very similar problem : for example, we are often given a set of data points and are sometimes interested in interpolation, or fitting a smoothed function through these points: As far as I know, this can be done in many different ways, such as: Polynomial Regression (Smoothed) Splines (Smoothed) Kernel Density Estimation By fitting a smoothed function to the data (as opposed to a piecewise function), this allows us to obtain an ""analytical and closed form"" function that passes through the data (we can also measure the quality of how well this function fits the data) - and then we should be able to differentiate (i.e. evaluate derivatives) this function at any point that we desire. For instance, provided if we have sufficient reasons to believe that the regression models below fit the data well - could we not just evaluate derivatives (e.g. first derivative, second derivative) of these regression models and use them as surrogates for numerical differentiation? My Question: Is fitting a statistical model a mathematically valid alternative to numerical differentiation? The only downside to this I can think of is that this approach requires you to first fit a statistical model to the observed data (a source of error and uncertainty) and then differentiate this fitted statistical model (another source of error and uncertainty) - effectively compounding the uncertainties through error propagation. I am not sure, but perhaps standard numerical differentiation techniques (e.g. Backwards Differencing, Forward Differencing ) have less of a chance of invoking such error prorogation. Can someone please comment on this? Thanks!","['calculus', 'derivatives', 'numerical-methods']"
4358766,Why isn't the Weierstrass function $\sum_{n=0}^\infty a^n \cos(b^n\pi x)$ differentiable?,"There is a famous example of a function that has no derivative: the Weierstrass function: But just by looking at this equation - I can't seem to understand why exactly the Weierstrass Function does not have a derivative? I tried looking at a few articles online (e.g. https://www.quora.com/Why-isnt-the-Weierstrass-function-differentiable ), but I still can't seem to understand what prevents this function from having a derivative? For example, if you expand the summation term for some very large (finite) value of $n$ : $$
f(x) = a \cos(b\pi x) + a^2\cos(b^2\pi x) + a^3\cos(b^3\pi x) + ... + a^{100}\cos(b^{100}\pi x)
$$ What is preventing us from taking the derivative of $f(x)$ ? Is the Weierstrass function non-differentiable only because it has ""infinite terms"" - and no function with infinite terms can be differentiated? For a finite value of $n$ , is the Weierstrass function differentiable? Thank you!","['calculus', 'functions', 'derivatives']"
4358831,Easy question on probability,"I know this is a trivial question but I want to make sure I'm not missing anything:
We have a biased 6-sided die, which brings any of the 6 numbers with equal probability in the first roll, but in the second and all subsequent rolls, brings the previous result with probability $\frac {1}{2}$ and all others with probability $\frac {1}{10}$ . The question is: Suppose we get a 4 in the first roll; what is the probability we also get a 4 in the 3rd roll? In the 4th? And so on. If we get a 4 in the 1st roll, then for the 2nd roll we have $\frac {1}{2}$ probability to get a 4 and $\frac {1}{10}$ for all other numbers in the 2nd roll. So in the 3rd roll, we already have the results of the previous roll of getting a 4 with probability $\frac {1}{2}$ , so now the probability is $\frac {1}{4}$ ?",['probability']
4358838,Prove that this function defined over $ \mathbb{R^2} $ is continuous.,"Recently, I've encountered with the following problem in an elementary analysis course: Let $ f: \mathbb{R^2} \rightarrow \mathbb{R^2} $ be a function that maps every connected set to connected set and also, every compact set to compact set. Prove that $ f $ is continuous. My attempt was to consider a convergent sequence $ (x_n, y_n) $ with its limit point, say $ (x, y) $ , which together form a compact set, and I wanted to show that $ \lim_{n \rightarrow \infty} f((x_n, y_n)) = f((x, y)) $ . But I couldn't continue. I wonder if there is anything special with $ \mathbb{R^2} $ here? Any help would be appreciated.","['continuity', 'connectedness', 'compactness', 'real-analysis']"
4358872,Choose signs such that $\pm\sqrt{1}\pm\sqrt{2}\pm\dots\pm\sqrt{2022}$ is as close as possible to $0$.,"Choose signs such that $\pm\sqrt{1}\pm\sqrt{2}\pm\dots\pm\sqrt{2022}$ is as close as possible to $0$ . I tried looking at examples for small $n$ (up to $8$ ) for inspiration: $$\begin{align}
&1: -\sqrt{1} = -1 &(0, 0) \\
&2: +\sqrt{1}-\sqrt{2} = -0.414214 &(10, 2) \\
&3: +\sqrt{1}+\sqrt{2}-\sqrt{3} = 0.682163 &(110, 6) \\
&4: -\sqrt{1}+\sqrt{2}+\sqrt{3}-\sqrt{4} = 0.146264 &(0110, 6) \\
&5: +\sqrt{1}+\sqrt{2}+\sqrt{3}-\sqrt{4}-\sqrt{5} = -0.0898034 &(11100, 28) \\
&6: -\sqrt{1}+\sqrt{2}+\sqrt{3}-\sqrt{4}+\sqrt{5}-\sqrt{6} = -0.0671574 &(011010, 26) \\
&7: -\sqrt{1}-\sqrt{2}-\sqrt{3}+\sqrt{4}+\sqrt{5}+\sqrt{6}-\sqrt{7} = -0.106458 &(0001110, 14) \\
&8: -\sqrt{1}+\sqrt{2}-\sqrt{3}+\sqrt{4}+\sqrt{5}+\sqrt{6}-\sqrt{7}-\sqrt{8} = -0.106458 &(01011100, 92)
\end{align}$$ The right side $(x, y)$ is interpreted as $$
\begin{align}
x &= \text{binary for + and - where + is 1 and - is 0} \\
y &= x \text{ to base 10} \\
\end{align}
$$ Doing this, I had hoped for a pattern that emerges from say powers of two but nothing seems useful here. By flipping each sign of a correct solution, we can trivially get another solution for each $n$ . If it was integers, we could use the parity argument but I don't see a way forward for square roots. This question was given in an interview for a trading firm. The interviewer wanted to hear how I would approach and analyze this problem under time-constrained circumstances. I asked this question in math.stackexchange because the structure of this question reminds me of typical Olympiad question where there's a ""trick"" to solving it.","['discrete-optimization', 'numerical-methods', 'combinatorics', 'real-analysis']"
4358875,Do any terms in mathematics exist to describe these concepts?,"If you have already waited 20 minutes for the bus, the bus can not arrive in less than 20 minutes because you can not ""un-wait"" the time you have already waited. If you have 5 donuts and you eat some donuts - it is impossible to have more than 5 donuts when you are finished eating. This is because once you remove items from a set, the set can not have more items than the original number of items in the set. If you walked 345 meters by 6 PM, the total number of meters you will have walked today can not be less than 345 meters. This is because you can not ""un-walk"" the meters you have already walked. Do these concepts have proper names in math? E.g
Commutative, reflexive, symmetric, etc?  How would you describe this property using mathematical terms - a set of objects that have certain properties, such that once a certain type of opperation is performed on objects in the set, the ""cardinality"" of set achieves a new infimum and supremum? Do such terms in mathematics exist that can correspond to the examples I laid out? Thanks!","['supremum-and-infimum', 'discrete-mathematics']"
4358878,"Solve the equation $\sin x+\cos x=k \sin x \cos x$ for real $x$, where $k$ is a real constant.","As I had solved the equation when $k=1$ in Quora and MSE by two methods, I started to investigate the equation for any real constant $k$ : $$
\sin x+\cos x=k \sin x \cos x,
$$ I first rewrite the equation as $$
\sqrt{2} \cos \left(x-\frac{\pi}{4}\right)=\frac{k}{2}  \sin (2 x)
$$ Letting $ \displaystyle y=x-\frac{\pi}{4}$ yields $$
\begin{array}{l}
\sqrt{2} \cos y=\frac{k}{2}\left(2 \cos ^{2} y-1\right) \\
2 k \cos ^{2} y-2 \sqrt{2} \cos y-k=0
\end{array}
$$ When $k\neq 0$ , using quadratic formula gives $$
\cos y=\frac{1 \pm \sqrt{1+k^{2}}}{\sqrt{2} k}
$$ For real $y$ , we have to restrict $\displaystyle \frac{1 \pm \sqrt{1+k^{2}}}{\sqrt{2} k}$ in $[-1,1]$ . Then I found that $$
-1 \leqslant \frac{1+\sqrt{1+k^{2}}}{\sqrt{2} k} \leqslant 1 \Leftrightarrow \quad|k| \geqslant 2 \sqrt{2}
$$ and $$
-1 \leqslant \frac{1-\sqrt{1+k^{2}}}{\sqrt{2} k} \leqslant 1 \Leftrightarrow \quad k<0 \text { or } k>0
$$ Now we can conclude that A. When $k\neq0$ $$x=n \pi-\frac{\pi}{4}$$ B. When $0\neq|k| \geqslant 2 \sqrt{2}, $ $$x=\frac{(8 n+1) \pi}{4} \pm \arccos \left(\frac{1\pm \sqrt{1+k^{2}}}{\sqrt{2} k}\right)$$ C. When $ 0 \neq|k|<2 \sqrt{2},$ $$ x= \frac{(8 n+1) \pi}{4}  \pm \arccos \left(\frac{1-\sqrt{1+k^{2}}}{\sqrt{2} k}\right)
$$ where $n\in Z.$ I am looking forward to seeing other methods to solve the equation. Furthermore, how about $$a\sin x+b\cos x+c\sin x\cos x=0?$$","['algebra-precalculus', 'quadratics', 'trigonometry', 'inequality']"
4358900,"Complete statistic for Normal Distribution $\mathcal{N}(\mu, \mu^2)$","We call a ""curved"" normal if its distribution is $\mathcal{N}(\mu, \mu^2), \mu > 0$ . Then a ""curved"" normal has pdf $$
      \left(\dfrac{1}{2\pi \mu^2}\right)^{\frac{1}{2}}e^{\frac{-1}{2\mu^2}(x - \mu)^2}
$$ Here we can rewrite this pdf as $e^{t(x)^T \eta(\mu) - \epsilon(\mu)}h(x)$ where $t(x) = (x, x^2), \eta(\mu) = \left(\dfrac{1}{\mu}, \dfrac{-1}{2\mu^2}\right), \epsilon(\mu) = \dfrac{1}{2}[1 + \ln(2\pi \mu^2)]$ and $h(x) = 1$ . Hence, ""curved"" normal belongs to exponential distribution. This leads me to the conclusion that statistic $$
       T(\mathbf{X}) = \left(\displaystyle\sum_{i = 1}^{n} X_i, \displaystyle\sum_{i = 1}^{n} X_i^2\right)
$$ is complete sufficient statistic for parameter $\mu$ , given $\mathbf{X} = (X_1, X_2, \cdots, X_n)$ is a random sample of size $n$ draw from this distribution However , we have that $$
       \mathbb{E}\left[\dfrac{1}{n}\displaystyle\sum_{i = 1}^{n} X_i^2 - 2S_n^2\right] = (\mu^2 + \mu^2) - 2\mu^2 = 0
$$ where $S_n^2$ is sample variance. Hence, $T(\mathbf{X})$ cannot be complete statistic (contradict to previous statement) So my question is what is wrong with my logic ? Any help is appreciated, thanks!","['statistics', 'probability-distributions', 'normal-distribution']"
4358926,"$\mathbb{C}[x,y,z]/(x^2+y^2+z^2-1)$ is not a UFD","Wiki says that the coordinate ring $\mathbb{C}[x,y,z]/(x^2+y^2+z^2−1)$ of the complex sphere is not a unique factorization domain. I want to know why it is not a UFD. We denote $X,Y,Z$ the residue class of $x,y,z$ . Obviously, we have $(X+iY)(X-iY)=(1+Z)(1-Z)$ in $\mathbb{C}[x,y,z]/(x^2+y^2+z^2−1)$ . But how to prove the irreducibility? Maybe there are some deep techniques of commutative algebra or algebraic geometry should be used in this question.","['unique-factorization-domains', 'algebraic-geometry', 'abstract-algebra', 'polynomial-rings', 'commutative-algebra']"
4358961,"If $p$ is the largest prime less than $2n$, what is the probability that $2n-p$ is a prime?","Let $p$ be the largest prime not exceeding an even number $2n$ . I observed that $2n - p$ has a high density of primes and it decrease very slowly. More specifically, for $2n \le 10^6$ the density is $0.6212$ and it drop down to $0.58427$ for $2n \le 3.65 \times 10^{10}$ . Question 1 : What is the density of primes in the sequence $2n-p$ ? Some observation : The divisors of $n$ do not divide $2n-p$ for $n \ge 4$ . Thus, the  more divisors $n$ has, the more likely it is for $2n-p$ to be a prime. This was observed in the data. Let $a_{n,d}$ be the sequence of positive integers which have $d$ or more divisors. I observed that as $d$ increases, the density of primes in the sequence $2a_{n,d} - p$ increase. Given below are the densities of primes in the first $10^6$ terms of $2a_{n,d} - p$ for different values of $d$ . (d, density)
(2, 0.621)
(3, 0.625)
(5, 0.635)
(10, 0.647)
(20, 0.662)
(50, 0.695)
(100, 0.720)
(150, 0.723)
(200, 0.744)
(230, 0.747)","['number-theory', 'elementary-number-theory', 'asymptotics', 'analytic-number-theory', 'prime-numbers']"
4359009,Laurent series of $\frac{z^2 + iz - 1}{(z - 2)(z + 3)}$,I want to expand the function following into Laurent series: $$\frac{z^2 + iz - 1}{(z - 2)(z + 3)}$$ for $2 < |z| < 3$ My solution First thing to apply is partial function decomposition: $$\frac{1}{(z - 2)(z  +3)} = \frac{\frac 1 5}{z - 2} + \frac{-\frac 1 5}{z + 3}$$ From which we have that: $$\frac{z^2 + iz - 1}{(z - 2)(z  +3)} = \frac{\frac 1 5 (z^2 + iz - 1)}{z - 2} + \frac{-\frac 1 5 (z^2 + iz - 1)}{z + 3}$$ Now we expand those two series: $$\frac{1}{z - 2} = \frac{1}{z}\frac{1}{1 - \frac 2 z} = \sum_{n = 0}^\infty \frac{2^n}{z^{n + 1}}$$ $$\frac{1}{z + 3} = \frac{1}{3(\frac z 3 + 1)} = \frac 1 3 \frac{1}{1 - ( - \frac z 3)} = \sum_{n = 0}^\infty (-1)^n \frac{z^n}{3^{n + 1}}$$ So final result is: $$\frac{z^2 + iz - 1}{(z - 2)(z + 3)} = \sum_{n = 0}^\infty \frac{z^2 + iz - 1}{5}[\frac{2^n}{z^{n + 1}} - \frac{(-1)^nz^n}{3^{n + 1}}]$$ Does it make any sense to you?,"['complex-analysis', 'sequences-and-series']"
4359014,$F'$ and $E'$ are isomorphic isometrically.,"Let $F$ be a dense subspace of the normed space $E$ .
Prove that $F'$ and $E'$ are isomorphic isometrically. At first I was trying to define a function to check the isomorphism and then show that it preserves the norm. However, I couldn't think of a function that is an isometric isomorphism and preserves the norm at the same time. So, I found some sources and I saw that this function defined as $$\varphi:F'\to E' \text{ as }\varphi(f\mid_F)=f.$$ But I don't understand why this function is well defined, is an isometric isomorphism and why it's defined like that. Can someone explain it to me? I don't get why the density property is important as well. Note: $E'$ and $F'$ are the spaces of continuous linear functionals. The topological dual of $E$ and $F$ , respectively.","['normed-spaces', 'topological-vector-spaces', 'vector-space-isomorphism', 'functional-analysis', 'general-topology']"
4359023,Why is it called an 'integral' curve?,"The concept of a integral curve is relatively easy to understand as path through a vector field which is tangent to the field at each point. But why is it called an ""integral"" curve? It appears to have little to do with integers, or integration.","['curves', 'differential-geometry']"
4359228,Factorization of a two variable polynomial over $\mathbb{C}$,"I was wondering how would I factor the polynomial $x^{2}-3xy+3y^2$ for variables $x$ and $y$ over $\mathbb{C}$ . I am aware that if you solve, say for $y$ you get $y=\frac{1}{6}(3x\pm i\sqrt{3}x)$ . Thus it seems like the factorization will be of the form $(x-a)(x-b)$ for some $a,b\in \mathbb{C}$ (involving $y$ somewhere of course), but I am unsure how to proceed, since I cannot find a proper formula to use for this situation.","['calculus', 'abstract-algebra', 'polynomials', 'discrete-mathematics']"
4359233,Suggestions for proving $ \displaystyle \sum^n_{k=0} \binom{n}{k}(-1)^{n-k}p(k)=p^{(n)}(0)$,"The polynomial $p$ has degrees less or equal to n
and I'm trying to prove \begin{equation}
 \displaystyle \sum^n_{k=0} \binom{n}{k}(-1)^{n-k}p(k)=p^{(n)}(0)
\end{equation} $p^{(n)}(0)=a_n\cdot n!$ The followings are my attempts First is just to expand $\displaystyle \sum^n_{k=0} \binom{n}{k}(-1)^{n-k}p(k)$ , but I didn't find any way of solving this. Second I started to consider about using induction. Base case degree p=1 is true and suppose $\displaystyle \sum^n_{k=0} \binom{n}{k}(-1)^{n-k}p(k)=p^{(n)}(0)$ Prove \begin{equation}
\displaystyle \sum^{n+1}_{k=0} \binom{n+1}{k}(-1)^{n+1-k}p(k)=p^{(n+1)}(0)
\end{equation} Then I tried to just directly expand this formula, but it seemingly doesn't work and it requires a lot of computation (as so far, the most hopeful way)
Then I use the fact $\binom{n+1}{k}=\binom{n}{k}+\binom{n}{k-1}$ to split the formula My attempt was: \begin{equation*}
\displaystyle  \sum^{n+1}_{k=0}[\binom{n}{k}+\binom{n}{k-1}](-1)^{n+1-k}p(k) 
\end{equation*} and get \begin{equation*}
\displaystyle  \sum^{n+1}_{k=0}\binom{n}{k}(-1)^{n+1-k}p(k) + \sum^{n+1}_{k=0}\binom{n}{k-1}(-1)^{n+1-k}p(k) 
\end{equation*} For the left part, since when k=n+1, $\binom{n}{n+1}=0$ , \begin{equation*}
\displaystyle  \sum^{n+1}_{k=0}\binom{n}{k}(-1)^{n+1-k}p(k)=\displaystyle  \sum^{n}_{k=0}\binom{n}{k}(-1)^{n+1-k}p(k)=(-1)\sum^{n}_{k=0}\binom{n}{k}(-1)^{n-k}p(k)=-p^{(n)}(0)
\end{equation*} However, For the right part:when k=0, $\binom{n}{−1}=0$ . Also, let k=j+1 thus it can be \begin{equation}
\sum^{n+1}_{k=0}\binom{n}{k-1}(-1)^{n+1-k}p(k)= \sum^{n+1}_{k=1}\binom{n}{k-1}(-1)^{n+1-k}p(k)=\sum^{n}_{j=0}\binom{n}{j}(-1)^{n-j}p(j+1)
\end{equation} Then I totally get stuck, since I don't know how to deal with this $p(j+1)$ , and I can't come up with more ideas. Any help, comments, or suggestions are appreciated! Based on the idea from pirahahindu1999, my trial: regard the formula $\displaystyle \sum^n_{k=0} \binom{n}{k}(-1)^{n-k}p(k)$ as a map, and all polynomials are vectors. Thus we have $F(p(x))=\displaystyle \sum^n_{k=0} \binom{n}{k}(-1)^{n-k}p(k)$ It's true that the map $F$ is a well-defined and linear map. Then $F$ : $ P_n(x) \to \mathbb R$ It's clear that $P_n(x)$ has a basis $\{1,x,x(x-1),x(x-1)(x-2),...,x(x-1)...1\}$ and suppose $1=e_0,x=e_1,x(x-1)=e_2...$ Then it's true that $F$ depends on the degree of the basis vectors, and the fact is that $F_3(e_3)=3!,F_3(e_2)=F_3(e_1)=F_3(e_0)=0$ Similarly for $F_n$ Since any polynomial can be expressed as the linear combination of basis vectors, it's true to have \begin{equation}
p(x)=a_0e_0+...+a_me_m
\end{equation} When m=n, $F_n(p(x))=F(a_ne_n)=a_n·n!=p^{(n)}(0)$ When m<n, $F_n(p(x))=0$ qed","['solution-verification', 'analysis']"
4359271,What there is to be learned from Dynkin's identification theorem?,"By Dynkin's identification theorem I mean the following: Let $\mu_1, \mu_2$ be two probability measures on a measurable space $(\Omega, F)$ . Suppose that G is a $\pi$ -system on $\Omega$ such that $\sigma(G) = F$ , where $\sigma(G)$ is the smallest $\sigma$ -algebra on $\Omega$ which contains $G$ . Then the following are equivalent:
1.) $\mu_1 = \mu_2$ , 2.) $\forall E \in F: \mu_1[E] = \mu_2[E]$ I am wondering what there is to learn from that theorem. I mean that is the theorem just a mathematical way to state the (quite obvious) property of functions that if $f:A \to B, g:A \to B$ are two mappings and $f(x) = g(x) \forall x \in A$ , then $f = g$ , to probability measures? I am wondering this because a probability measure is to my knowledge a ""relative"" function in the sense that it is a measure with finite total mass on a space $(S, F)$ , where $F$ is a sigma-algebra on $S$ , i.e. relative to the sigma-algebra of $F$ . Is it then true that if two probability measures $\mu_1, \mu_2$ are equal on sigma-algebra $F_1$ on $\Omega$ , then they might not be equal on a different sigma-algebra $F_2$ on $\Omega$ ? And if this is the case, then is the identification theorem any different from the property of functions I stated earlier?","['measure-theory', 'probability-theory']"
4359302,Calculating the asymptotic normality result of a MLE from a skew-logistic distribution,"Suppose we have $X$ with cumulative distribution function $F_X(x) = (1-e^{-x})^\frac{1}{\theta}$ where $x \geq 0, \theta > 0$ . How can one find a MLE for $\theta$ from this and the asymptotic normality result? We get the density function as $f_X(x;\theta) = \frac{1}{\theta}(1-e^{-x})^{\frac{1}{\theta}-1}e^{-x}$ . $\textbf{EDIT : } $ I found the MLE to be $$\hat{\theta} = -\frac{1}{n}\sum^n_{i=1} \ln(1-e^{-x_i}).$$ Now how do we get the asymptotic normality result? I tried calculating the Fisher information number but I'm stuck at $$I(\theta) = -E\left[\frac{\partial^2}{\partial \theta^2} \ln f(x,\theta) \right] = \frac{-1}{\theta^2}+ \frac{2}{\theta^3}E[\ln(1-e^{-x})]$$","['statistics', 'parameter-estimation', 'maximum-likelihood']"
4359305,Measurable invariance of domain,"Invariance of domain theorem tells us that if a subset $V$ of $\mathbb{R}^n$ is homeomorphic to an open subset of $\mathbb{R}^n$ , then $V$ must be open itself. Question: If a subset $V$ of $\mathbb{R}^n$ is homeomorphic to a Borel subset of $\mathbb{R}^n$ , must $V$ be Borel ? Recall $Borel(\mathbb{R}^n)$ is defined to be the $\sigma$ -algebra generated by the topology of $\mathbb{R}^n$ .","['general-topology', 'descriptive-set-theory', 'measure-theory']"
4359316,Support of homology in quasi-projective varieties.,"Given a quasi-projective complex variety $X$ and a positive integer $i<\text{dim}(X)-1$ . Consider the homology group $H_i(X(\mathbb{C}))$ . Is it possible to find a subvariety of codimension at least $1$ inside $X$ , denoted by $Y$ such that $H_i(Y)\cong H_i(X)$ ? Note that this is true if replace quasi-projective by projective (Lefschetz hyperplane), or the homology with Borel-Moore homology.","['algebraic-geometry', 'homology-cohomology', 'algebraic-topology', 'projective-varieties']"
4359350,Quick inequality verification,"The assertion is: Let $c>1$ . Then it holds that $$\forall x,y,z \in \mathbb{R}^n,\;
\left|z-x-y\right|^2 \ge \dfrac{\left|z-x\right|^2}{c}
 - \dfrac{\left|y\right|^2}{c-1}$$ I can not think of anything more than maybe using the reverse triangle inequality, although it is not leading me anywhere until now. Thanks in advance.",['algebra-precalculus']
4359373,Find the satisfying functions such that $f(x^5)=5f(x)$,Question is same as above. Let me write again. Find the satisfying functions such that $f(x^5)=5f(x)$ Clearly seen that $c\ln x$ satisfy the condition. Question is what else?,['functions']
4359427,Stuck on proving a variation of the Central Limit Theorem.,"I'm trying to prove the following version of the central limit theorem. Let $$L^n = (L_1^n,...L_n^n) $$ such that the $L_i$ are i.i.d., there exists a sequence of constants such that $|L_i^n|\leq K^n$ for all $i$ and $K^n \rightarrow{0}$ , and it holds that for $Z_n= \sum_{i=1}^nL_i^n$ we have $\mathbb{E}[Z_n] \rightarrow{\mu}$ and $\operatorname{Var}(Z_n) \rightarrow \sigma^2$ . Then we have that $Z_n$ converges in distribution to $Z$ where $Z$ is normal with mean $\mu$ and variance $\sigma^2$ . I think it should be done with Characteristic functions + Levy's continuity theorem. The characterstic function will factor by the first hypothesis but I don't see how to incorporate the other two to arrive at the desired result.","['probability-limit-theorems', 'central-limit-theorem', 'probability-theory', 'probability']"
4359445,Are all isomorphisms between the Fano plane and its dual of order two?,"Famously from every finite projective plane $P$ we can create a dual projective plane $P'$ by taking the points of $P'$ to be the lines of $P$ and the lines of $P'$ to be the points of $P$ . It is also well known that the smallest projective plane, the Fano plane, is isomorphic to its own dual. I am interested in the set of all isomorphisms between the Fano plane and its dual. Perhaps an easier way to think about these maps is to take the Heawood graph: the bipartite graph whose 14 vertices represent the 7 points and 7 lines of the Fano plane with an edge connecting a point-vertex and a line-vertex whenever in the Fanoplane the point lies on the line. This graph has 336 graph-automorphisms: 168 mapping point-vertices to point-vertices and line-vertices to line-vertices - a normal subgroup of the automorphsim group isomorphic to the symmetry group of the fano plane. And it has 168 automorphisms mapping point-vertices to line-vertices and vice-versa: the unique non-group coset for this normal subgroup. It are these 168 symmetries in the non-trivial coset that I'm interested in. My question is: Are these all of order 2? Obviously they are all of even order, but all being of order 2 is a much stronger claim. I don't see a clear reason for this when merely staring at the Fano plane, but in the mean time I wasn't able to construct an example of higher order. I hope someone can clarify this!","['graph-theory', 'exceptional-isomorphisms', 'group-theory', 'duality-theorems', 'projective-space']"
4359494,Stabilizer of probability measure on projective space,"I'm reading notes on Lattices on Lie groups (in French: Réseaux des groupes de Lie ) by Yves Benoist and I have some
troubles to understand some aspects on the setting & proof of the Lemma 7.3 (due to Furstenberg) on page 60. Let $k= \mathbb{R}, \mathbb{C}$ or a finite
extension of $p$ -adic field $\mathbb{Q}_p$ , $p$ prime. Let $E:=k^d$ be
a vector space and $\mathbb{P}(E)$ the associated projective space, $\operatorname{PGL}(E)$ the group of projective transformations
on $\mathbb{P}(E)$ and $\nu$ a probability measure on $\mathbb{P}(E)$ . Let $S:= \{ g \in \operatorname{PGL}(E) \ \vert \ g_* \nu = \nu \} $ . (recall $ g_* \nu $ is the push-forward or image measure with respect
automorphism $g: \mathbb{P}(E) \to \mathbb{P}(E)$ ) Questions: 1) In the notes there is not a word said about the $\sigma$ -algebra
with which the projective space $\mathbb{P}(E)$ is endowed which allows
to consider a (probability) $\nu$ .
In most cases when dealing with measures on affine space $k^d$ without making any comment on the underlying $\sigma$ -algebra,
one uses in silence the Borel- $\sigma$ -algebra, a kind of canonical $\sigma$ -algebra on affine spaces. Is there any type of ""canonical"" $\sigma$ -algebra for
projective spaces which is in most cases used when one is
discussing measures on projective spaces without mentioning
by name the underlying $\sigma$ -algebra? So is there a kind of
standard $\sigma$ -algebra for projective spaces known as a sort of
pendant to the Borel- $\sigma$ -algebra for affine spaces? 2) At the beginning of the proof of the Lemma there is remarked that it's rather
obvious that the stabilizer $S:= \{ g \in \operatorname{PGL}(E) \ \vert \ g_* \nu = \nu \} $ of probability measure $\nu$ is a closed subspace in $\operatorname{PGL}(E)$ . Why that's the case? I assume that the topology on $\operatorname{PGL}(E)$ is the quotient topology induced
from $\operatorname{GL}(E) \subset \operatorname{End}(E) \cong k^{d^2}$ . It is known that if in general a topological group $G$ acts continuously
on a topological space $S$ by continuous map $a: G \times S \to S$ , then any stabilizer is closed.
But here we consider the ""stabilization"" of a measure und
there is no standard way I know to endow the ""space of meausures""
on $\mathbb{P}(E)$ with continuous $\operatorname{PGL}(E)$ -action. How the closedness of $S= \{ g \in \operatorname{PGL}(E) \ \vert \ g_* \nu = \nu \} $ can be deduced? I not see the exact reason why that's really clair at all.","['ergodic-theory', 'lattices-in-lie-groups', 'probability-theory', 'lie-groups', 'dynamical-systems']"
4359498,Almost-sure convergence of a series of log-normal random variables,"Let $(X_n)_{n\geq 1}$ be a sequence of independent and identically
distributed standard normal random variables and define for $n\geq 1$ , $$ W_n = \exp\left\{\sum_{i=1}^nX_i-\frac{n}{2}\right\} $$ Show that $W_n$ converges almost-surely and check if the sequence is a uniformly
integrable collection. $W_n$ is clearly a log-normal random variable with the following moments, \begin{align}
\mathbb{E}[W_n] &= 1\\
\operatorname{\mathbb{V}ar}[W_n] &= \exp(n)-1
\end{align} that are easily obtainable applying the fact that the moment generating function for the normal distribution is $$
M_X(t)=\exp\left\{\mu t + \frac{t^2\sigma^2}{2} \right\}.
$$ I tried to apply the Kolmogorov 3-series theorem and the Levy equivalence, but failed as the variance explodes. I also tried working with $S_n -\frac{n}{2}$ and applying the continuous mapping theorem, but it also didn't work, I would appreciate some guidance.","['probability-limit-theorems', 'uniform-integrability', 'stochastic-processes', 'martingales', 'probability-theory']"
4359504,Minimal set definition,"I'm very confused about minimal sets, particularly with graphs. According to Wolfram Mathworld, a minimal set is ""a member set that is not a proper subset of another member set is called a minimal set."" However I'm confused because doesn't that mean that it can contain proper subsets? And in that case, how can a minimum set also be minimal? For example, Wolfram also says here that every minimum vertex cover is minimal. But some minimum vertex cover is a subset of the vertex set, which is also a vertex cover, so how does the original definition of minimal set make sense here? I'm confused on the meaning of minimal in general, because I thought that minimal means it cannot get any smaller, or that it has no proper subsets. But Wolfram's definition makes it seem like it can't get any bigger. Examples to explain this would be very helpful.","['elementary-set-theory', 'graph-theory']"
4359518,Unbounded Bloch function,I'm studying the Bloch space $\mathcal{B}$ ; this is the set of all analytic fuctions $f$ defined on the open unit disc $\mathbb{D}$ such that $$\sup_{|z|<1}(1-|z|^2)|f'(z)|<\infty$$ My question is more curious: can you give an example of an unbounded Bloch function?,"['complex-analysis', 'functional-analysis', 'analysis']"
4359525,Is it possible to realize the Moebius strip as a linear group orbit?,"Is the Moebius strip a linear group orbit? In other words: Does there exists a Lie group $ G $ , a representation $ \pi: G \to GL(V) $ , and a vector $ v \in V $ such that the orbit $$
\mathcal{O}_v=\{ \pi(g)v: g\in G  \} 
$$ is diffeomorphic to the Moebius strip? My thoughts so far: The only two obstructions I know for being a linear group orbit is that the manifold (1) must be smooth homogeneous (shown below for the the group $ SE_2 $ ) and (2) must be a vector bundle over a compact Riemannian homogeneous manifold (here the base is the circle $ S^1 $ ). The Moebius strip is homogeneous for the special Euclidean group of the plane $$
SE_2= \left \{ \
\begin{bmatrix}
a & b & x \\
-b & a & y \\
0 & 0 & 1 
\end{bmatrix} : a^2+b^2=1 \right \} 
$$ there is a connected group $ V $ of translations up each vertical line $$
V= \left \{ \ 
\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & y \\
0 & 0 & 1 
\end{bmatrix} : y \in \mathbb{R} \right \} 
$$ Now if we include the rotation by 180 degrees $$
\tau:=\begin{bmatrix}
-1 & 0 & 0 \\
0 & -1 & 0 \\
0 & 0 & 1 
\end{bmatrix}
$$ Then $ \langle V, \tau \rangle$ has two connected components and $$
SE_2 \mathbin{/} \langle V, \tau \rangle
$$ is the Moebius strip. Indeed if we consider the model of the Moebius strip as the manifold of affine lines in the plane then the subgroup $ <V,\tau> $ is exactly the stabilizer of the $ y $ -axis.","['algebraic-groups', 'representation-theory', 'geometric-topology', 'lie-groups', 'differential-geometry']"
4359528,"Prove that if $G$ is a graph of order $101$ and $δ(G) = 51$, then every vertex of $G$ lies on a cycle of length $27$","Prove that if $G$ is a graph of order $101$ and $δ(G) = 51$ , then every vertex
of $G$ lies on a cycle of length $27$ (Chapter 3 Exercise 16.a  Chromatic Graph Theory,Gary Chartrand, Ping Zhang) Attempt: First I observed that the hypotheses of the dirac theorem are fulfilled. A simple graph with $n$ vertices ( $n\geq 3$ ) is Hamiltonian if every vertex has degree $\frac {n}{2}$ or greater. So the graph $G$ is hamiltonian. And by definition a hamiltonian graph is a graph that contains a hamiltonian cycle (a cycle in G that contains every vertex of G). So now I have a cycle but I don't know how to get what I need.","['graph-theory', 'discrete-mathematics', 'hamiltonian-path']"
4359533,Does the operation $(A\times B) \cup (B\times A)$ have its own name?,"I use such an operation a lot in a computer program I write and I was wondering if it has its own mathematical name. That's all. I was looking for ""commutative cartesian product"" and for some other similar things in Google but I haven't find anything of that sort so I suspect that this operation remains unnamed. Thank you in advance for your response.","['elementary-set-theory', 'terminology']"
4359535,Convert ODE to a form of Bessel differential equation,"I'm working on the solution of the equation $$\tan^2u\partial^2_u y_2 + (2+\tan^2u)\tan u \partial_u y_2 -a^2\lambda_2y_2 - n^2(1+\cot^2u)y_2 = 0.$$ It is possible to write the above equation in terms of the Sturm-Liouville operator by just multiplying the equation by $\cos u$ : \begin{align*}
\sin u \tan u \partial^2_u y_2 + (\cos u \tan u + \sec^2 u \sin u)\partial_u y_2 - \cos u(a^2\lambda_2 + n^2(1+\cot^2u))y_2 = 0\\
\implies \partial_u (\sin u \tan u \partial_u y_2) - \cos u (a^2\lambda_2 + n^2(1+\cot^2u))y_2 = 0.
\end{align*} Now, let $\eta = \sin u$ , hence, $$
\frac{\partial}{\partial u} = \frac{\partial\eta}{\partial u}\frac{\partial}{\partial\eta} = \cos u \frac{\partial}{\partial\eta}.
$$ The last equation becomes \begin{align*}
\cos u \partial_\eta \left(\eta^2\partial_\eta y_2\right) - \cos u (a^2\lambda_2y_2 + n^2(1+\cot^2u))y_2 &= 0\\
\implies \left(\eta^2\partial_\eta y_2\right) -\left(a^2\lambda_2 + n^2\left(1+\frac{\cos^2 u}{\sin^2 u}\right)\right)y_2 &= 0,
\end{align*} but, since $\sin u = \eta,$ it implies that $\cos u = \sqrt{1-\eta^2}$ by trigonometric relations. Thus, \begin{align*}
\partial_\eta \left(\eta^2\partial_\eta y_2\right) -\left(a^2\lambda_2 + n^2\left(1+\frac{1-\eta^2}{\eta^2}\right)\right)y_2 &= 0\\
\implies \partial_\eta \left(\eta^2\partial_\eta y_2\right) -\left(a^2\lambda_2 +\frac{n^2}{\eta^2}\right)y_2 &= 0
\end{align*} Now, the problem is reduced to solve this last ODE, which I can see something similar to the associated Legendre ODE or Bessel ODE. The last one seems to be more plausible when we apply the change of variables $x=n/\eta$ . By making this change, the equation becomes $$ x^2\partial^2_x y - (a^2\lambda_2 + x^2)y = 0$$ I want to reduce the equation to a known one to avoid the Frobenius method, but if it is not possible, I'm ok with that. If possible, how can I write this last equation as a Bessel-type ODE? Any help/idea of substitution will be very appreciated. Thanks in advance","['mathematical-physics', 'legendre-polynomials', 'ordinary-differential-equations', 'bessel-functions']"
4359552,Will this determinant of the matrix of determinants of the transformations of the group applied to a square matrix always be zero?,"Background Let $A = \left[ \begin{matrix} a & b \\ c & d  \end{matrix} \right]$ be a matrix in $\mathbb{R}^{2 \times 2}$ . While matrices are often used to represent a variety of linear transformations, including rotations, here I am transforming the matrix itself. A classic way to introduce group transformations is via the following 8 transformations of a square with distinct corners: Rotation of 0 degrees: $R_0(A)=\left[\begin{matrix}a & b\\c & d\end{matrix}\right]$ Rotation of 90 degrees: $R_{90}(A)=\left[\begin{matrix}b & d\\a & c\end{matrix}\right]$ Rotation of 180 degrees: $R_{180}(A)=\left[\begin{matrix}d & c\\b & a\end{matrix}\right]$ Rotation of 270 degrees: $R_{270}(A)=\left[\begin{matrix}c & a\\d & b\end{matrix}\right]$ Flipping about a horizontal axis: $H(A)=\left[\begin{matrix}c & d\\a & b\end{matrix}\right]$ Flipping about a vertical axis: $V(A)=\left[\begin{matrix}b & a\\d & c\end{matrix}\right]$ Flipping about the main diagonal: $D(A)=\left[\begin{matrix}a & c\\b & d\end{matrix}\right]$ Flipping about the other diagonal: $D^\prime(A)= \left[\begin{matrix}d & b\\c & a\end{matrix}\right]$ Considering the compositions of all pairs of operations yields an 8 by 8 Cayley table. Thinking of my matrix $A$ as the square to be rotated, I created a similar table except the entries are the determinants of the resulting matrices after applying a composition of two of the above operations which I have represented with matrix $B$ below. The order of the rows and columns follow the order of the transformations as they are listed above. $$B = \left[\begin{matrix}a d - b c & - a d + b c & a d - b c & - a d + b c & - a d + b c & - a d + b c & a d - b c & a d - b c\\- a d + b c & a d - b c & - a d + b c & a d - b c & a d - b c & a d - b c & - a d + b c & a d - b c\\a d - b c & - a d + b c & a d - b c & - a d + b c & - a d + b c & - a d + b c & a d - b c & a d - b c\\- a d + b c & a d - b c & - a d + b c & a d - b c & a d - b c & a d - b c & - a d + b c & a d - b c\\- a d + b c & a d - b c & - a d + b c & a d - b c & a d - b c & a d - b c & - a d + b c & a d - b c\\- a d + b c & a d - b c & - a d + b c & a d - b c & a d - b c & a d - b c & - a d + b c & a d - b c\\a d - b c & - a d + b c & a d - b c & - a d + b c & - a d + b c & - a d + b c & a d - b c & a d - b c\\a d - b c & - a d + b c & a d - b c & - a d + b c & - a d + b c & - a d + b c & a d - b c & a d - b c\end{matrix}\right]$$ Finally I computed the determinant of $B$ to be $\det (B) = 0$ . Question Just as I started with 2 by 2 matrices and got a final ""determinant of the matrix of determinants of the transformations of the group applied to the original matrix"", will I also such a determinant of zero for any n by n matrix? Edits 1 Starting with $A = \left[\begin{matrix}x_{0} & x_{1} & x_{2}\\x_{3} & x_{4} & x_{5}\\x_{6} & x_{7} & x_{8}\end{matrix}\right]$ , I found that the matrix of determinants was $$B = \left[\begin{matrix}x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6}\\- x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6}\\x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6}\\- x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6}\\- x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6}\\- x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6}\\x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6}\\x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6}\end{matrix}\right]$$ And I similarly found $\det(B) = 0$ as was found in the 2 by 2 case. 2 I found the resulting determinant be zero in the 4 by 4 and 5 by 5 cases. Note that the transformations above are rotating/reflecting the matrix entries themselves, rather than vectors in a vector space.","['determinant', 'cayley-table', 'matrices', 'linear-algebra', 'group-theory']"
4359586,Can a property hold almost everywhere and also nowhere?,"Let $X$ = {0}. Use the trivial measure on $X$ such that $\mu(A)$ = 0 for all measurable sets in $X$ . Let the property $P$ be such that $P$ does not hold at 0. But then there exists a set $N = X$ with $\mu(N) = 0$ and all $x \in X - N = \emptyset$ have the property $P$ (this is true since there is no such $x$ ). Thus, it appears that $P$ holds almost everywhere on $X$ . But, simultaneously, there is no $x \in X$ such that $P$ holds, so we might say $P$ holds nowhere. Is then correct that a property can hold almost everywhere and also nowhere as in the above example, or did I misunderstand something?","['measure-theory', 'almost-everywhere']"
4359608,Weak * Compactness and Local Compactness,"I'm reading through Murphy's C* Algebras, and the following theorem is presented: If $A$ is an abelian Banach algebra, then the set of characters $\Omega(A)$ is a locally compact Hausdorff space.  If $A$ is unital, then $\Omega(A)$ is compact. Proof: It's easily checked that $\Omega(A) \cup \{0\}$ is weak* closed in the weak * compact closed unit ball $S$ of $A^*$ .  Thus, $\Omega(A) \cup \{0\}$ is weak * compact implying that $\Omega(A)$ is locally compact.  If $A$ is unital, then $\Omega(A)$ is weak * closed in $S$ , and hence $\Omega(A)$ is compact. So I have two main questions here. Why does weak * compactness of $\Omega(A) \cup \{0\}$ imply local compactness of $\Omega(A)$ ? Does the term ""compact"" is the last sentence of the proof mean weak* compact?  If not, what does it mean simply by compact? I'm brand new to this subject, so any help would be really appreciated.","['operator-algebras', 'operator-theory', 'banach-algebras', 'functional-analysis', 'general-topology']"
4359627,Does there exist bounded function with a bounded first derivative but unbounded second derivative?,I know that a bounded function with a bounded second derivative also has a bounded first derivative. But consider a bounded function $f(x)$ such that $f'(x)$ is also bounded. Can we claim that $f''(x)$ will also be bounded. Intuition says NO ! But I am unable to find an example. Please help.,"['derivatives', 'real-analysis']"
4359713,"Is $F(t)=\int_0^{\infty} \frac{e^{-tx^3}}{1+x^4}dx$ well defined on $(0,\infty)$?","I've $F(t)=\int_0^{\infty} \frac{e^{-tx^3}}{1+x^4}dx$ and I have to see that it is well defined on the interval $(0,\infty)$ . For that, I have defined $f(x,t)=\frac{e^{-tx^3}}{1+x^4}, x,t\in(0,\infty) $ so I have to see if $f$ is integrable in $(0,\infty)$ . We know that $f$ is integrable on $(0,\infty)$ $\leftrightarrow$ $\int_{(0,\infty)}|f|d\mu<\infty$ $f(x,t)=|\frac{e^{-tx^3}}{1+x^4}|=\frac{e^{-tx^3}}{1+x^4}\le e^{-tx^3}$ But hoe can I bound this? I have to bound it with an integrable function... but I don't know how to calculate the integral of $e^{-tx^3}$ ... Is there any other easier way to bound that? Or how can I solve my problem?","['integration', 'measure-theory', 'functions', 'lebesgue-integral']"
4359811,What assumptions are needed for compactness and self-adjointness?,"I am studying functional analysis and got stuck with a textbook exercise. I greatly appreciate some hints/help or a push in the right direction! Define for $g\in C^0[-1,1]$ the integral operator $T_g:L^2\to L^2$ , by $$T_g(f)(s):=\int_0^1g(s-t)f(t)dt$$ Now I need to oppose conditions on $g$ such that $T_g$ becomes a compact self-adjoint operator. My progress: First for we look at when $T_g$ is self-adjoint. So we want some conditions such that: $\langle T_g(f)(s),h(s)\rangle=\langle f(s), T_g(h)(s)\rangle$ . For the left hand side we get the following: $$\langle T_g(f)(s),h(s)\rangle=\int_0^1T_g(f)(s)\overline{h(s)}ds=\int_0^1\int_0^1g(s-t)f(t)\overline{h(s)}dtds$$ And for the right hand side we get the following: $$\langle f(s), T_g(h)(s)\rangle=\int_0^1f(s)\overline{T_g(h)(s)}ds=\int_0^1 \int_0^1f(s)\overline{g(s-t)} \overline{h(t)}dsdt$$ Well now I am not to sure I thought that $\overline g=g$ is the only condition for $T_g$ to be self-adjoint.
Now for compactness: I wanted to pick a bounded subset $M \subset L^2$ , and show that $T_g(M) $ is compact.
For that I want to show that $T_g(M)$ is closed, bounded and equicontiuous.
I have no good idea/intuition on how to show these properties with the given information I have. Can someone help me solve this exercise I think it is a exercise from which I can learn a lot and get a better grasp! :)","['operator-theory', 'compact-operators', 'functional-analysis', 'self-adjoint-operators']"
4359847,Boundary of manifold with boundary has empty boundary: What about corners in the topological category?,"It is a general fact that the boundary $\partial\mathcal{M}$ of any topological manifold with boundary $\mathcal{M}$ is itself a topological manifold and has empty boundary, i.e. $\partial (\partial\mathcal{M})=0$ . Now, I am a little bit confused about the following: There is also the notion of manifolds with corners , i.e. second countable Hausdorff spaces, which are locally homeomorphic to $\mathbb{R}_{\geq 0}^{d}=[0,\infty)^{d}=\{x\in\mathbb{R}^{d}\mid x_{i}\geq 0\}$ . I am aware of the fact that topologically, manifolds with corners are one and the same as topological manifolds with boundary , since the space $\mathbb{R}_{\geq 0}^{d}$ is homeomorphic to the model space of manifolds with boundary, i.e. the half-plane $\mathbb{H}^{d}=[0,\infty)\times\mathbb{R}^{d-1}$ . This is no longer true in the differentiable setting, since these two sets are not diffeomorphic. However, in my question I do not care about differentiability, but I am only thinking about the topological category. So, in other words, every manifold with corners is in particular a (topological) manifold with boundary. So, it should also be true that the boundary of every such manifold is by itself a manifold with empty boundary, right? I mean, the Theorem mentioned at the beginning is for arbitrary manifolds with boundary and the notion of boundary is a purely topological object and has nothing to do with a differentiable structure. Now, when I think for example about the solid cylinder, i.e. the manifold with corner defined by $C:=D^{2}\times [0,1]$ , where $D^{2}$ is the disk (=closed 2-ball), then its boundary is clearly $$\partial C=(\partial(D^{2})\times [0,1])\cup (D^{2}\times\partial [0,1])=(S^{1}\times [0,1])\cup (D^{2}\times\{0,1\}).$$ But this is not a topological manifold without boundary right? (Or maybe it is, and I just have an thinking error. But then it has to be homeomorphic to one of the compact surfaces, by the classification theorem and since it has no holes, it has to be homeomorphic to $S^{2}$ , but I can't see why this should be the case. I already tried to choose triangulations of the boundary cylinder $\partial C$ , but I do not find a Euler-characteristic of $2$ . For example, take a single prism as a cellular decomposition of the solid cylinder. Then its boundary has $5$ faces, $7$ edges and $6$ vertices and hence, I would get $\chi=4$ ). EDIT: I know that there is a related question here on this side , however, this question seems to ask about the differentiable category and does not answer my question and confusion about the solid cylinder.","['manifolds', 'general-topology', 'manifolds-with-boundary']"
4359863,Restriction of bijection is bijection,"I have proven the following property of bijective functions. Could you verify the argumentation and suggest improvements if you have any? Lemma. Let $f: A \rightarrow B$ be a bijection and $C \subseteq A$ . Then the restriction $ f\mid_C : C\rightarrow f(C) $ is also bijection. Proof. By assumption, $f$ is bijection, i.e. $ \forall a, b\in A: \ a\ne b \Rightarrow f(a)\ne f(b) $ , and $ \forall c \in B: \exists x\in A: f(x)=c. $ To show that $f\mid_C $ is bijective, we need to show that $ \forall a, b\in C: \ a\ne b \Rightarrow f(a)\ne f(b) $ , and $ \forall c \in f(C): \exists x\in C: f(x)=c. $ Let $a, b\in C\subseteq A $ so that $a\ne b $ . Since $a, b \in A$ and $a \ne b$ , by 1) we have $f(a)\ne f(b) $ . This shows 3). Let $ c\in f(C) $ . $$f(C) = \left\{d\in
B \mid \exists x\in C: f(x)=d \right\} $$ By the definition of $f(C)$ , there exists $x' \in C$ such that $f(x')=c$ . This shows 4). Therefore $f\mid_C : C \rightarrow f(C)$ is bijective. $\square$ As a corollary we get that $ \left|C\right|=\left| f(C)\right|. $","['functions', 'solution-verification']"
4359872,"Showing that continuous and differentiable $s(t)$, with $s(0)=0$ and $s'(t)\leq 2ts(t)+\sqrt{s(t)}$ for $t>0$, is identically zero for $t\geq 0$","Let $s:[0,+\infty)\to [0,+\infty)$ be a continuous function that is differentiable on $(0,+\infty)$ . It also holds that $$s'(t)\leq 2ts(t)+\sqrt{s(t)}, \ \ t>0 \\ s(0)=0$$ Show that $s(t)=0, t\ge 0$ . Do we us some theorem, maybe Rolle or the mean value theorem? $$$$ Do we have to check maybe the sign of $2ts(t)+\sqrt{s(t)}$ ? This depends on the value of $s(t)$ , or not?","['calculus', 'functions', 'derivatives', 'ordinary-differential-equations']"
4359904,Can the field of Laurent series be made into an Archimedean ordered field?,"In today's analysis class, my professor introduced the field of formal Laurent series $\Bbb R((x))$ . He also talked about the dictionary order on $\Bbb R((x))$ and why it is not an Archimedean ordered field. One natural question arises: Can $\Bbb R((x))$ be made into an Archimedean ordered field? My professor answered that there are uncountably orders on $\Bbb R((x))$ , but none that he knows of make it an Archimedean field. Since we know that every Archimedean field can be embedded into the real number and conversely, every subfield of $\Bbb R$ is Archimedean, the question transforms into: Can $\Bbb R((x))$ (as a field) be embedded into $\Bbb R$ ? The new formulation doesn't seem easier, but it makes the problem a purely algebraic one. Thanks for nombre's answer, $\Bbb R((x))$ cannot be embedded into $\Bbb R$ for the simple reason that it contains a copy of $\Bbb R$ . Now I wonder if $\Bbb Q((x))$ embeds into $\Bbb R$ .","['formal-power-series', 'abstract-algebra', 'laurent-series', 'ordered-fields']"
4359948,A closed form of $\int_0^1\frac{\arctan( x^3)}{1+x}\text dx$? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I've tried series summation and contour integral, but neither works. Perhaps there's something related to number theory here.
The result should be $$\frac{3\pi}{8}\log2-\frac\pi6\log(2+\sqrt{3})$$ but I have no idea how to obtain it.","['integration', 'definite-integrals', 'sequences-and-series']"
4359953,Cramér-Rao lower bound - Estimator is independent of the parameter,"I have followed the Cramér-Rao lower bound (CRLB) derivation, and I couldn't figure out why - If $f(x; \theta)$ be a probability density with continuous parameter $\theta$ , and $X_1, \dots, X_n$ be independent random variables with density $f(x; \theta)$ , and $\Theta(X_1, \dots ,X_n)$ be an unbiased estimator of $\theta$ . Why does the estimator, $\Theta$ , is independent of $\theta$ (the param to be estimated)? $\Theta$ is a function of $X_1, \dots, X_n$ , and in the pdf of each one of them $\theta$ appears as a param. Doesn't it imply that $\Theta$ is also dependent on $\theta$ ?","['statistical-inference', 'statistics', 'independence', 'estimation']"
4360039,Is it possible to have an equivalence relation with exactly $n + 1$ elements on $A$?,"$A = \{1,2,\dots,n\}$ , and $R$ be a relation on $A$ (so that $R \subset A \times A$ ). i) Is it possible to have an equivalence relation with exactly $n+1$ elements on $A$ ? If yes, give an example, if no, explain why? For this I considered what adding an extra element from the possible relations would do. For example from $R = \{(1,1),(2,2),(3,3)\}$ adding an extra element $(1,2)$ would stop the relation from being symmetric, meaning adding in just one element would prevent this from being an equivalence relation. So you cannot have an equivalence relation with exactly $n+1$ elements in. ii) Is it possible to have an equivalence relation with exactly $n+2$ elements on $A$ ? If yes, give an example, if no, explain why? My reasoning for this question follows a similar patter to I, two elements could be added that allow it maintain being an equivalence relation. Adding in $(1,2)$ and $(2,1)$ would ensure it was still symmetric, and so still an equivalence relation. iii) When $n = 3$ , give all possible equivalence relations on $A$ . For this question I considered what adding in specific extra elements would require to be included in the final set. $(1,1), (2,2), (3,3)$ $(1,1), (1,2), (2,1), (2,2), (3,3)$ $(1,1), (1,3), (2,2), (3,1), (3,3)$ $(1,1), (2,2), (2,3), (3,2), (3,3)$ $(1,1), (1,2), (1,3), (2,1), (2,2), (2,3), (3,1), (3,2), (3,3)$ Are my answers and reasoning correct? Also, should $\emptyset$ be included as an equivalency relation for III?","['elementary-set-theory', 'equivalence-relations', 'discrete-mathematics']"
4360064,Asymptotic Notations: $\sim$ vs $\asymp$ and PNT,"Is the only difference between $f(x) \sim g(x)$ and $f(x)\asymp g(x)$ the constant which their ratio approaches? My understanding is that $f(x)\asymp g(x)$ means that $f(x)=O(g(x))$ and $g(x)=O(f(x))$ , and that this is equvialent to saying that $$\lim\frac{f(x)}{g(x)}$$ exists (or at least, $\limsup$ ) and is some fixed number. On the other hand, $f(x)\sim g(x)$ means that this limit is specifically $1$ . I ask this question because it doesn't feel like one is that much stronger than the other, they both establish that $f(x)$ and $g(x)$ are the same ""kind"" of function with respect to their growth (they feel more or less the same qualitatively), $\asymp$ is more akin to ""proportional to"" in the limit, rather than $\sim$ which is sort of like ""equal to"" in the limit. In particular, I'm surprised how easy the proof of Chebychev's theorem $\pi(x)\asymp x/\log x$ is when compared with the PNT $\pi(x)\sim x/\log x$ .","['complex-analysis', 'number-theory', 'asymptotics', 'real-analysis']"
4360086,Measurability of the Wasserstein distance of a Markov kernel,"Consider two Polish metric spaces $(\mathcal{A}, \Sigma_\mathcal{A})$ and $(\mathcal{B}, \Sigma_\mathcal{B})$ endowed with their Borel $\sigma$ -algebras. Let $$a\mapsto \mu_a$$ be a Markov kernel, that is $\mu_a$ is a probability measure on $(\mathcal{B}, \Sigma_\mathcal{B})$ for all $a\in A$ , and for any measurable set $B\in\Sigma_\mathcal{B}$ we have that $a\mapsto \mu_A(B)$ is a measurable function from $\mathcal A$ to $[0, 1]$ . Let $\mathcal{P}_\mathcal{B}$ be the space of probability measures on $(\mathcal{B}, \Sigma_\mathcal{B})$ and fix $\nu\in\mathcal{P}_\mathcal{B}$ . Let $\mathcal{W}$ denote the $1$ -Wasserstein distance (wrt to the metric on $\mathcal{B}$ ) between probability measures in $\mathcal{P}_\mathcal{B}$ . Is the mapping $$a\mapsto \mathcal{W}(\mu_a, \nu)$$ a measurable map $\mathcal A\to \mathbb R$ ? I think that one way to prove the measurability would be to exploit Corollary 5.22 in [1], which essentially tells you that if $a\mapsto\mu_a$ is measurable wrt the Borel $\sigma$ -algebra induced on $\mathcal{P}_\mathcal{B}$ by the weak measure convergence, then $a\mapsto\pi_a$ is measurable, where $\pi_a$ is the optimal coupling between $\mu_a$ and $\nu$ . It would then follow that $a\mapsto \mathcal{W}(\mu_a, \nu) = \mathbb E_{(A,A')\sim\pi_a}[d(A, A')]$ is measurable. But is it actually true that $a\mapsto\mu_a$ is measurable? The main reason for the question is that I have often encounter expressions like $$\int_\mathcal A \mathcal{W}(\mathbb P_B, \mathbb P_{B|A=a})\,\mathrm d\mathbb P_A(a)$$ (where $A, B$ are coupled random variables, with marginals $\mathbb P_A$ and $\mathbb P_B$ , and $\mathbb P_{B|A=a}$ is a regular conditional probability) without any formal justification, see for instance [2] and the papers it builds on. But does this expression actually make sense? Is the integrand always measurable? [1] Villani, Optimal transport, old and new, 2008. [2] Rodríguez-Gálvez, Tighter expected generalization error bounds via Wasserstein Distance, 2021.","['measurable-functions', 'measure-theory', 'probability-theory', 'probability']"
4360091,The composition of soft curves is not always soft.,"An exercise I found in one of my text books: (In my book a curve that is ""soft by pieces"" is a curve $\gamma :[a,b]\subset \mathbb{R}\to\mathbb{R}^n$ such that there is a partition $P=\{a=t_0<t_1<...t_k=b\}$ of $[a,b]$ such that $\gamma$ has a continuous derivative on each $[t_{i-1},t_i]$ for $i \in \{1,2,...k\}$ ; I believe ""soft by pieces"" may not be a universal term to refer to such curves, this is a direct translation of the term used in the book, which is in Spanish) ''If $\gamma :[a,b]\subset \mathbb{R}\to\mathbb{R}^n$ is $\textit{soft by pieces}$ and $\alpha:[c,d]\subset\mathbb{R}\to[a,b]\subset\mathbb{R}$ , surjective on $[a,b]$ and $C^1$ on $[c,d]$ . Show, with an example, that $\gamma \circ \alpha:[c,d]\subset\mathbb{R}\to \mathbb{R}^2$ is not always soft by pieces"" I took $\gamma:[-1,1]\to\mathbb{R}^2$ , $\gamma(t)=(t,|t|)$ , I know that this function is soft by pieces (I need only to take $P=\{-1,0,1\}$ , $\gamma$ 's derivative is definitely continuous in $[-1,0]$ and $[0,1]$ ).
The problem I'm faced with lies in finding the adequate $\alpha$ , at first I thought about proposing a function similar to $sin(\frac{1}{x})$ such that this function is surjective, but has problems on 0, the problem with this function is that is is not differentiable on zero. Then I thought about $x^2\sin\left(\frac{1}{x}\right)$ and $x^3\sin\left(\frac{1}{x}\right)$ , however these don't work either because even though I could make them continuous on zero (by defining a function that is 0 if $x=0$ and $x^2\sin\left(\frac{1}{x}\right)$ or $x^3\sin\left(\frac{1}{x}\right)$ ) everywhere else) and their derivative is continuous, they are not surjectve on $[-1,1]$ I believe that my intuition about my choice if $\gamma$ is sound and the idea of choosing an $\alpha$ that ""has problems on zero"" could work. Still, I'm stuck and unable to find an $\alpha$ that ""has problems on zero"", but is surjective and has a continuous derivative.","['multivariable-calculus', 'calculus']"
4360118,Definition of optional sigma algebra require the left hand limit or just right continuous adapted processes?,"While reading the following post : https://almostsuremath.com/2009/11/08/filtrations-and-adapted-processes/ I have come across a question on the definition of optional and predictable processes. In most books I have read before, optional sigma algebra is defined as the sigma algebra generated by cadlag adapted processes, and predictable sigma algebra is defined as the sigma algebra generated by caglad adapted processes or in some books just left continuous adapted processes. In George's post he defines both as just right-continuous  or left-continuous adapted processes. In the predictable case, I can see that both are in fact the same as it is generated by continuous adapted processes. However, in the case of optional sigma algebra I am not sure if they are the same. Does the omission of left-limit here matter?","['stochastic-processes', 'measure-theory', 'probability-theory', 'stochastic-calculus']"
4360124,Expressing $\int_0^{+ \infty}\frac{\mathrm{e}^{- t^2} \mathrm{d} t}{z^2 - t^2}$ with the error function (complex analysis or Fubini theorem ?),"I would like to show the following (Abrahomitz et Stegun) for any $z\in\mathbb{C}, \Im(z)>0$ , $$
     \mathrm{e}^{- z^2} \left( 1 + \frac{2 i}{\sqrt{\pi}} \int_0^z
     \mathrm{e}^{t^2} \mathrm{d} t \right) = \frac{2 iz}{\pi} \int_0^{+ \infty}
     \frac{\mathrm{e}^{- t^2} \mathrm{d} t}{z^2 - t^2} \quad, \Im (z) > 0\\
$$ I tried two methods so far: Evaluating the integral on the right with the help of complex analysis, I didn't manage to make this work as I couldn't use Jordan Lemma because of divergence of $\mathrm{e}^{-R^2(\cos(2\theta)+i\sin(2\theta))}$ for large $R$ Expressing the integrand in RHS $\times \mathrm{e}^{z^2}$ as an integral and applying Fubuni theorem: \begin{align} \int_0^{+ \infty} \int_{- \infty}^1 \mathrm{e}^{- a (t^2 - z^2)} \mathrm{d} a
     \mathrm{d} t = \int_{- \infty}^1 \mathrm{e}^{az^2} \int_0^{+ \infty} \mathrm{e}^{-
     at^2} \mathrm{d} t \mathrm{d} a \end{align} Then using expression for Gaussian integral and splitting the $]-\infty;1]$ into $]-\infty;0]$ and $]0;1]$ : \begin{align} \frac{1}{2} \int_{- \infty}^1 \mathrm{e}^{az^2} \sqrt{\frac{\pi}{a}} \mathrm{d} a
     = \frac{\sqrt{\pi}}{2} \left( \int_{- \infty}^0
     \frac{\mathrm{e}^{az^2}}{\sqrt{a}} \mathrm{d} a + \int_0^1
     \frac{\mathrm{e}^{az^2}}{\sqrt{a}} \mathrm{d} a \right) \end{align} With substitution and using Gaussian formula again I get the good result, but then I realized ... there is a problem in writing: $$   \frac{\mathrm{e}^{- (t^2 - z^2)}}{z^2 - t^2} = \int_{- \infty}^1 \mathrm{e}^{- a
     (t^2 - z^2)} \mathrm{d} a $$ as we don't have: $$   \lim_{a \rightarrow - \infty} \mathrm{e}^{- a (t^2 - z^2)} = 0 $$ How can I conclude ? Thanks.","['complex-analysis', 'error-function']"
4360148,Uniform convergence of a sequence of functions to a function (g) but their derivatives converge pointwise to a function which is not (g'),"Question is that find a sequence ( $f_n$ ) of continuously differentiable real functions defined on $[0,1]$ converges uniformly to a differentiable function ( $g$ ) and ( $f_n'$ ) converge pointwise to a function that is not ( $g'$ ). I am trying to find this function. I think that $\frac{x}{1+x^2n^2}$ converges uniformly to zero on [0,1] and derivate $\frac{1-n^2x^2}{(1+x^2n^2)^2}$ converges pointwise to 1 at zero and 0 for all other points. Is my logic correct? Any help will be appreciated.Thanks",['real-analysis']
4360218,Finding $\theta$ given that $\sin(2\theta) = 0.99810$,Question: If $\sin(2\theta) = 0.99810$ then what is $\theta$ ? My try: Can I just do inverse $\sin()$ of $2\theta$ then divide by $2$ to get $\theta$ ? $\sin^{-1}(2\theta)=0.99810$ $2\theta= 86.47^\circ$ $\theta=86.47^\circ/2=43.23^\circ$ ?,['trigonometry']
4360242,Solving $y^{(2)}-5y^{(1)}+4y=\frac{1}{e^x+1}.$,"The equation is the following $$ y^{(2)} - 5y^{(1)} + 4y =  \frac{1}{e^x + 1} $$ I’ve just started studying these kind of equations and I know of only two method to solve it, they both involve the following first step: Find 2 independent solutions of the associated homogeneous equation. I have done it through the characteristic polynomial and obtained the following 2 solutions: $$ y_1(x) = e^x \quad y_2(x)= e^{4x} $$ Now, 2 different possibilities I’m aware of are: A) If the $f(x)$ on the right is of the kind $e^{kx}P_m(x)$ , with $P_m$ polynomial of degree m, or of the kind $e^{kx}(P_m(x)\sin(\omega x)+R_k(x)\cos(\omega x))$ , then we know that the equation admits certain solutions similar in form to the $f(x)$ , that we need to specify finding the constants of the polynomials.
The $f(x)$ in this case doesn’t belong to either of the types, still I tried this method imposing $e^z = \frac{1}{e^x+1}$ but I couldn’t make it work. B)The variation of parameters method. I solved the system to find $\gamma_1^{(1)}(x)$ and $\gamma_2^{(1)}(x)$ obtaining $$ \gamma_1^{(1)}(x) = -\frac{1}{3(e^x+1)}$$ $$ \gamma_2^{(1)}(x) = \frac{1}{3e^{3x}(e^x+1)}$$ Still, I wasn’t able to integrate the second function. I searched online for a solution and it came out very complicated which tells me that this isn’t the way is was intended to be solved (cause it was part of an exam and it shouldn’t take more than half an hour). Can you show me how it’s done? Thanks","['calculus', 'ordinary-differential-equations']"
4360301,The length arc of parabola,"Find the length of the arc of the parabola $y^2=4x$ from $x=0$ to $x=4$ . In the manual solution is $2\sqrt{5}+\ln(2+\sqrt{5}).$ My answer is $\displaystyle 2\int_0^4 \! \sqrt{1+\frac{\mathrm{d}x}{\mathrm{d}y}} \, \mathrm{d}x$ $\displaystyle 2\int_0^4 \! \sqrt{1+\frac{y^2}{4}} \, \mathrm{d}x$ $\displaystyle 2\int_0^4 \! \sqrt{4+y^2} \, \mathrm{d}x= 4 \sqrt{5}+2\ln(2+\sqrt{5})$ But my answer is $4 \sqrt{5}+2\ln(2+\sqrt{5})$ As I doubled the length, Do we need to double the arc length?","['integration', 'calculus', 'conic-sections']"
4360313,Difficulty in reading Introduction to Set Theory by Hrbacek,"Background: I was reading Karel Hrbacek's Introduction to Set Theory and need help with understanding this snippet: What I don't understand/need to clarify: In order to be allowed to use the notion $\{x| P(x)\}$ I need to prove that: $\exists A:\forall x:\big(P(x)\implies x\in A\big)\implies \{x\in A|P(x)\}\textit{ exists and it's unique}$ , right? I've noticed that sometimes it's not mandatory to prove the existence and uniqueness of an object to define it. For instance: When defined what $A\subseteq B$ mean for two given sets $A$ and $B$ . So, when is it mandatory to prove the existence and uniqueness of an object in order to define it? Update: Why is it even mandatory to prove the existence and uniqueness of an object to define it?",['elementary-set-theory']
4360324,Hyperbolic functions as polynomials,"I have recently found that the change of variable $t\to 2 \arctan (t)$ makes $\cos(2 \arctan (t)) = \dfrac{1-t^2}{1+t^2}$ and $\sin (2\arctan (t) ) = \dfrac{2t}{1+t^2}$ for certain values of $t$ . I was wondering, is there any change of variables that transforms $\cosh(t)$ and $\sinh(t)$ into polynomials or a quotient of polynomials locally? I have done some research but all I have found is about hyperbolic polynomials and I don't see how that would help me. Thanks everyone!","['trigonometry', 'polynomials', 'hyperbolic-functions']"
4360325,Proving that $\limsup_{h \to 0^{+}} \frac{F(x + h) - F(x)}{h}$ is measurable if $F$ is continuous,"I am trying to write up a detailed proof for the following statement: Let $F: \mathbb{R} \rightarrow \mathbb{R}$ be continuous. Then $$D^{+}(F)(x) := \limsup_{h \to 0^{+}} \frac{F(x+h) - F(x)}{h}$$ is measurable. (Note: This question has been asked a few years ago here , but I would like to solicit feedback on my particular proof. I also have specific questions/concerns about this proof that were not addressed in the linked post. For these reasons, I feel this merited a separate post...) My attempt : To ease notation, let $\Delta_h F(x) := \frac{F(x + h) - F(x)}{h}$ for $h \neq 0$ . By unravelling the definition of $\limsup\limits_{h \to 0^{+}}$ , we have \begin{align*}
   D^{+}(F)(x) &= \limsup_{h \to 0^{+}} \Delta_h F(x) \\[5pt]
               &= \lim_{\delta \to 0^{+}} \left( \sup\left\{\Delta_h F(x): h \in B_{\delta}(0) \cap (0,\infty) \setminus \{0\} \right\} \right) \\[5pt]
               &= \lim_{\delta \to 0^{+}} \sup_{h \in (0,\delta)} \Delta_h F(x).
\end{align*} I now claim (with some wariness--see my question below) that \begin{align*}
    \hspace{2cm} \lim_{\delta \to 0^{+}} \sup_{h \in (0,\delta)} \Delta_h F(x) = \lim_{\delta \to 0^{+}} \sup_{h \in (0,\delta) \cap \mathbb{Q}} \Delta_h F(x) \hspace{2cm} (\star)
\end{align*} Now for each $\delta > 0$ , let $\{h_n^{\delta}\}_{n=1}^{\infty}$ be an enumeration of $(0,\delta) \cap \mathbb{Q}$ . Then we have $$ \sup_{h \in (0,\delta) \cap \mathbb{Q}} \Delta_h F(x) = \sup_{n \in \mathbb{N}} \Delta_{h_n^{\delta}} F(x)$$ for all $x \in \mathbb{R}$ and all $\delta > 0$ . And since $F$ is continuous, the function $x \mapsto \Delta_{h_{n}^{\delta}} F(x) = \frac{F(x + h_n^{\delta}) - F(x)}{h_n^{\delta}}$ is clearly continuous for all $h_n^{\delta}$ , and hence measurable. It follows that $G_{\delta}(x) := \sup_{n \in \mathbb{N}} {\Delta_{h_n^{\delta}}} F(x)$ is measurable because the supremum of a sequence of measurable functions is measurable . Now let $\{\delta_n\}_{n=1}^{\infty}$ be a sequence in $(0,\infty)$ such that $\delta_n \to 0$ . We then have $$ D^{+}(F)(x) = \lim_{\delta \to 0^{+}} G_{\delta}(x) = \lim_{n \to \infty} G_{\delta_n}(x). $$ And so finally, $D^{+}(F)(x)$ is measurable because the pointwise limit of a sequence of measurable functions is measurable. $\quad \square$ My questions : 1.) Is my proof sound? Any issues, major or minor? 2.) Main question: Is $(\star)$ actually true? My concern with regards to $(\star)$ is: Do we know a priori if $\sup_{h \in (0,\delta)} \Delta_h F(x)$ is finite a.e.? If yes, how would we prove it? (More generally, does the definition of measurability even apply to functions which are $\pm \infty$ on a set of positive measure?) In the case where $\sup_{h \in (0,\delta)} \Delta_h F(x)$ is assumed to be bounded on $(0,\delta)$ (for some $\delta > 0$ ), my argument for $(\star)$ is based on the following (more generalized) claim: Claim : Let $(S,d)$ be a metric space and let $E$ be a dense subset of $S$ . If $f: S \rightarrow \mathbb{R}$ is continuous, then $$ \sup_{x \in S} f(x) = \sup_{x \in E} f(x).$$ Proof : Let $\alpha := \sup_{x \in S} f(x)$ and $\alpha' := \sup_{x \in E} f(x)$ . Clearly, $\alpha' \leq \alpha$ , so we just need to show the reverse inequality. Fix $\epsilon > 0$ . Then there is some $x_0 \in S$ such that $f(x_0) > \alpha - \frac{\epsilon}{2}$ . Then since $f$ is continuous at $x_0$ , there exists some $\delta > 0$ such that \begin{align*}
   x \in N_{\delta}(x_0) &\implies |f(x_0) - f(x)| < \frac{\epsilon}{2}  \\[3pt]
                         &\implies -\frac{\epsilon}{2} < f(x_0) - f(x) <  \frac{\epsilon}{2} \\[3pt]
&\implies f(x) > f(x_0) - \frac{\epsilon}{2} > \left(\alpha - \frac{\epsilon}{2}\right) - \frac{\epsilon}{2} = \alpha - \epsilon.
\end{align*} Then since $E$ is a dense subset of $S$ , there exists some $x_1 \in E \cap N_{\delta}(x_0)$ . Hence, $\alpha' \geq f(x_1) > \alpha - \epsilon$ , i.e. $\alpha' > \alpha - \epsilon$ .  And since $\epsilon > 0$ was arbitrary, it follows that $\alpha' \geq \alpha. \quad \square$ Finally: Does this claim & proof look sound?","['measure-theory', 'limsup-and-liminf', 'analysis', 'real-analysis', 'derivatives']"
4360351,"Martingale representation theorem with respect to different Brownian motion, than the one generating filtration?","Say I have a $\mathcal{F}_t$ -adapted stochastic process $Y$ with Itô-representation $$Y_t=\int_0^t\alpha_sds+W_t$$ where both the Brownian motion and the integrable process $\alpha$ are $\{\sigma(Y_t)\}_{t}$ -adapted. Moreover we know, that $\alpha$ is bounded by a constant $g\in \mathbb{R}$ and càdlàg, if that helps.
Now I have a square-integrable martingale $M$ with respect to $\{\sigma(Y_t)\}_{t}$ and I want to use martingale representation theorem to get something like this $$M_t=M_0+\int_0^t\gamma_sdW_s.$$ Issue is I don't know if $M$ is $\{\sigma(W_t)\}_{t}$ -adapted.
Now obviously I can use Girsanov to show that $Y$ is a Brownian motion with respect to some other measure $\widetilde{P}$ on the same probability space, but I don't know if this enough. To be precise this question is motivated by this paper https://ieeexplore.ieee.org/document/179372 Theorem 1 step 3. EDIT: If this was actually true, it would answer my question affirmatively -or would it?","['martingales', 'brownian-motion', 'probability-theory']"
4360364,"How to show that $\frac{1}{X_n}\to 1/c \, \mbox{ in probability} $?","If a random variable $X_n$ converges to a non-zero constant $c$ in probability, then $
\dfrac{1}{X_n}\to \dfrac{1}{c}$ in probability.  I try to prove this statement by definition. Here we want to prove that for every $\epsilon>0$ , as $n\to\infty$ , $P \left( \bigg \lvert \dfrac{1}{X_n} - \dfrac{1}{c} \bigg \rvert \ge \epsilon \right) \to 0$ .
Note that $\bigg \lvert \dfrac{1}{X_n} - \dfrac{1}{c} \bigg \rvert = \dfrac{\lvert c - X_n \rvert}{\lvert c X_n \rvert}$ . Then $\displaystyle P \left( \bigg \lvert \frac{1}{X_n} - \frac{1}{c} \bigg \rvert \ge \epsilon \right) = P \left( \frac{\lvert c - X_n \rvert}{\lvert c X_n \rvert} \ge \epsilon \right) = P( \lvert c - X_n \rvert \ge \lvert c X_n \rvert \epsilon)$ . But it seems that we need $E \lvert X \rvert < \infty$ ?","['probability-limit-theorems', 'probability']"
4360371,How do I prove a sequence diverges to infinity?,"I have been scratching my head for a couple of days on how to determine convergence/divergence of sequences. I made it to understand how to prove that a sequence converges, but still have numerous doubts about proof of divergence. Say I have $\lim_{n \to +\infty } \sqrt{n+1} = +\infty$ and I have to prove the sequence diverges. What I did is using the definition of converging sequence $| a_n - L | < \ \varepsilon$ Where L is a theoretical limit (Fixed, real number) and $\varepsilon$ is also a theoretical, real number bounding the sequence (am I understanding this correctly?) Then, I tried the proof by contradiction by doing $-\varepsilon \ <  \sqrt{n+1} - L < \varepsilon$ $ -\varepsilon + L <  \sqrt{n+1} < \varepsilon + L$ $(-\varepsilon + L)^2 - 1 < n < (\varepsilon + L)^2 - 1$ Assuming that $\varepsilon$ and L are fixed, real numbers, we can always come up with an n greater than any operation done between those numbers, thus contradicting the fact that a bound exists. Is the proof I've come up with valid and sufficient ? I want to apologize in advance to people familiar with this, in case I made a horrible mess.","['analysis', 'calculus', 'solution-verification', 'sequences-and-series', 'limits']"
