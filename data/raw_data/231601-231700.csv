question_id,title,body,tags
4815423,Convergence of series where $\alpha$ gets close to 1 periodically?,Its known that $$ \sum_{n=1}^{\infty}\frac{1}{n^\alpha}$$ converges with $\alpha > 1$ . But how does something like $$\sum_{k=1}^\infty\frac{1}{k^{2-sin(k)}}$$ behave that gets close to 1 periodically? The usual tests for convergence failed. Interestlingly I tried this logarithmic convergence criteria I found on german wikipedia. I states that if the series $$b_k=\frac{ln(k^{sin(k)-1})}{ln(ln(k))}$$ converges to something stricly smaller that $-1$ the series converges. Now i pluged this into wolframalpha and it outputs $-\infty$ . But looking at the step-by-step soltion it seems to make a mistake. It applies the product rule for limits wrong IMO.,"['calculus', 'analysis', 'sequences-and-series']"
4815470,Proof of Kolmogorov's $0$-$1$ Law in Shiryaev,"I know this theorem has been discussed in a lot of posts on this site however I was unable to find an answer that deals with the particular proof I am trying to understand. I'm interested in the proof of Kolmogorov's $0$ - $1$ law given in the book "" Probability by Albert Shiryaev "". The statement is as follows, Theorem: Let $\xi_1,\xi_2,\ldots$ be a sequence of independent random variables and let $A \in \cap_{n=1}^\infty \mathcal{F}_n^{\infty}$ , i.e an element of the tail $\sigma$ -algebra. Then $\mathbb{P}(A)=0$ or $1$ . We define $\mathcal{F}_n^k = \sigma(\xi_n,\xi_{n+1},\ldots,\xi_{k})$ with the convention $\mathcal{F}_n^{\infty}=\sigma(\xi_n,\xi_{n+1},\ldots)$ . Proof: Clearly we have $A \in \mathcal{F}_1^\infty=\sigma(\xi_1,\xi_{2},\ldots)=\sigma(\cup_n \mathcal{F}_1^n)$ . We can find sets $A_n \in \mathcal{F}_1^n$ , $m\geq1$ , such that $\mathbb{P}(A \Delta A_n) \to 0$ as $n \to \infty$ . Hence, $$\mathbb{P}(A_n) \to \mathbb{P}(A), \quad \mathbb{P}(A_n \cap A) \to \mathbb{P}(A) .$$ But as $A \in \cap_{n=1}^\infty \mathcal{F}_n^{\infty}$ , the events $A_n$ and $A$ are independent for every $n \geq 1$ . Hence it follows that $\mathbb{P}(A)=\mathbb{P}(A)^2$ , which completes the proof. Now first of all I presume the $m$ is a typo and this should be an $n$ . The only issue I'm having with this proof is the claim that you can approximate the event $A$ by an event in $\mathcal{F}_1^n$ , and by approximate I mean in the sense of the pseudo metric defined by the symmetric difference operation. Shiryaev has an exercise in a previous chapter which he refers to in this proof that says given a probability space $(\Omega,\mathcal{F},\mathbb{P})$ and an algebra $\mathcal{A}$ such that $\sigma(\mathcal{A})=\mathcal{F}$ , for any $B \in \mathcal{F}$ and $\epsilon>0$ there exists a set $A \in \mathcal{A}$ such that, $$\mathbb{P}(B \Delta A) \leq \epsilon. $$ In the proof above the set in which we are taking the $A_n$ 's are one, not necessarily an algebra as unions of $\sigma$ -algebras need not be an algebra and secondly they do not even generate the $\sigma$ -algebra that $A$ belongs to, i.e $\mathcal{F}_1^\infty$ . So my question is why we can apply this result ? Just to be 100% clear the proof is transcribed correctly, i.e the possible typo of $m\geq1$ does appear in the book.","['probability-theory', 'probability']"
4815477,Cohomological implications of Twisting by Effective Nef Divisors,"It is a standard result due to Serre that for a projective scheme $X$ , $D$ an ample divisor on $X$ , and $F$ a coherent sheaf, there exists an $N$ such that for all $n\geq N$ , one has $H^0(X,F\otimes O_X(nD))\neq 0$ (or better yet, $F\otimes O_X(nD)$ is generated by global sections). In particular, Serre gave a cohomological criterion for ampleness. I ask a similar question for nef line bundles $O_X(D)$ , but with some added conditions to study how far off $D$ is from being ample. I require $D>0$ i.e. it is effective so as to rule out the trivial bundle. Under this hypothesis, is the following property true? Let $D$ be a nef and (numerically?) effective divisor. Then $H^0(X,F\otimes O_X(nD))\neq 0$ for $n\gg 0$ and any coherent sheaf $F$ . I've looked at Lazarsfeld's books and nothing came up. Certainly, there are consequences for cohomology for big and nef divisors such as Kawamata-Viehweg Vanishing and Lazarsfeld's book is riddled with such consequences. These are not what I am searching for. As another example, big divisors themselves have the property that $h^0(X,F\otimes O_X(nD))\geq Cn^{\dim X}$ for $n\gg 0$ and for any coherent sheaf supported on all of $X$ and where $C>0$ is a constant. So my question is a weaker form of this.",['algebraic-geometry']
4815479,Simple bisection geometry,"Let $\triangle ABC$ have incenter $D$ and let the incircle intersect sides $BC,AB,AC$ at $E,F,G$ respectively. Extend $AB$ and $AC$ to meet the circumcircle of $\triangle ADE$ at $K$ and $I$ respectively. Prove that $FG$ bisects $KI$ . I have solved this with barycentric coordinates, but I would like to see a synthetic solution as it may be more elegant. We employ a barycentric coordinate systÂ¹em. We list the coordinates of the points that we know. Here $s$ denotes the semiperimeter of $\triangle{ABC}$ , so $s=\frac{a+b+c}{2}$ . \begin{align*}
E = (0 : s-c : s-b) \\
F = (s-c : 0 : s-a) \\
G = (s-b : s-a : 0) \\
D = (a : b : c) \\
A = (1 : 0 : 0) \\
B = (0 : 1 : 0) \\
C = (0 : 0 : 1).
\end{align*} We also know that the equation of $AB$ is $z=0$ and the equation of $AC$ is $y=0$ . Thus we simply have to find the equation of the circumcircle of $\triangle{ADE}$ and the equation of line $KI$ and $FG$ and show that it intersects at the desired points.","['analytic-geometry', 'circles', 'geometry', 'triangles', 'bisection']"
4815481,Trouble finding most powerful test given hypothesis test and density $\theta e^{x - \theta(e^{x} -1)}$,"Let $X_{1}, \dots X_{n}$ be a sample from the distribution with density given by: \begin{equation}p_{\theta}(x) = \theta e^{x - \theta(e^{x} - 1)},\end{equation} where $x > 0$ and $0$ otherwise. Let $\theta$ be an unknown parameter. Determine a most powerful test $H_{0} \colon \theta = \theta_{0} = 1$ against $H_{1}\colon \theta = \theta_{1} = 2$ at level $\alpha = 0.05$ . I intend to solve for $k$ in $\mathbb{P}(\frac{L_{1}}{L_{0}} \geq k \mid \theta = 1) = 0.05$ . I've determined the likelihood ratio, for $\theta_{0}$ and $\theta_{1}$ . This gave me: \begin{equation}
\left(\frac{\theta_{1}}{\theta_{0}}\right)^{n} \cdot e^{\sum_{i=1}^{n} (-\theta_{1} + \theta_{0})e^{x_{i}} + (\theta_{1} - \theta_{0})}
\end{equation} This function must be greater than or equal to some $k$ . Simplifying this as the function of a sufficient statistic and inserting the values of $\theta_{0}$ and $\theta_{1}$ , means that $\sum_{i=1}^{n} -e^{x_{i}} + 1 = n - \sum_{i=1}^{n} e^{x_{i}}$ must be greater than or equal to some constant $k_{1}$ . I assume I should arrive at a value from the Gamma distribution for $k$ / $k_{1}$ but I'm not sure how, or whether I've made a mistake on the way. Would someone be so kind as to help me. Thanks in advance.","['statistics', 'hypothesis-testing']"
4815488,"Propositions 10.12 and 10.15 from Lee's book ""Introduction to Smooth Manifolds""","Let $\pi:E\to M$ be a (smooth) vector bundle. 10.12. Let $C$ be a closed subset of $M$ and $s:C\to E$ a (smooth) section of $\pi_C:E|_C\to C$ . For each open subset $U \subseteq M$ containing $C$ , there exists a (smooth) section $\bar{s}$ of $U$ such that $\bar{s}|_C=s$ and $\operatorname{supp}\bar{s}\subseteq U$ . 10.15. If $C\subseteq M$ is a closed subset and $\{s_1,\dots,s_k\}$ are linearly independent (smooth) sections of $\pi_C:E|_C\to C$ , there exists a (smooth) reference frame $\{\bar{s}_1,\dots,\bar{s}_k\}$ of some open subset $U$ such that $C\subseteq U$ and $\bar{s}_i|_C=s_i$ for all $i\in\{1,\dots,k\}$ . I've been able to prove the first, but I'm stuck in the other.","['vector-bundles', 'smooth-manifolds', 'differential-geometry']"
4815545,Summation inside tan(x),"Given $ a_1 +a_2 + a_3+...+a_n= \theta$ degrees. Where $tan(a_k) = \frac{n}{n^2 + k(k-1)}$ . Find $tan(\theta)$ in terms of ""n"". I tried using the formula tan(a+b+c+d+...) = $\frac{S_1-S_3-S_5-...}{1-S_2-S_4-...}$ , where $S_n$ denotes summation of tan(x), taken 'n' at a time. But it was proving to be quite difficult. Can anyone help me with this problem? Is there some sort of visual solution to this?","['trigonometric-series', 'algebra-precalculus', 'trigonometry']"
4815567,"Is there a ""hidden"" product-rule in every derivative?","This is a very quick question, and it might be pretty basic. But as a preface, I plan to dive into the actual proofs behind the derivative rules after this post, but I would like to see if my intuition is correct here first! I was toying around with derivative rules, and I had the thought that, no matter what form your function is in, you will be performing the product rule ! For example, if you want to take a derivative of the function $f(x) = x^2\times sin(x)$ , you would apply the product rule and get $\frac{d}{dx} f(x) = 2x\times sin(x) + x^2\times cos(x)$ This is an obvious use of the product rule , but I think that there is a less obvious use of the product rule when taking the derivative of $f(x) = x^2$ . The derivative is simple, if we use the power rule we get: $$\frac{d}{dx} f(x) = 2x$$ And this is done with the formula $\frac{d}{dx} [x^n] = nx^{n-1}$ . But I am positing that there is a ""hidden"" product rule being performed as well! Because if we think about it, there is still multiplication taking place with the $x$ in $f(x) = x^2$ , this is shown in the ""expanded"" version of: $$f(x) = 1\times x^2$$ And you cannot just skip over the product rule ! So in taking the derivative of this function we are actually performing a combination of the power rule and product rule : $$\frac{d}{dx} f(x) = 0\times x^2 + 1\times 2x$$ which is implicitly done with the output of the power rule alone, so this is why we do not show the product rule in action here. This is my own intuition based on what I have learned so far, and I am curious if I am correct, or making dangerous assumptions!",['derivatives']
4815615,What's the hypersolid angle of a 5-cell (4d tetrahedron)?,"It's known that the solid angle of the vertex of a regular tetrahedron is $\arccos(\frac{23}{27})$ , or equivalently, $\frac\pi2-3\arcsin(\frac13)$ or $3\arccos(\frac13)-\pi$ . (Trig identities are weird.) This is around $0.551$ steradians, or around $0.044$ times the whole sphere. What about the same question but one dimension higher? The 4d equivalent of the tetrahedron is known as the 5-cell, also called the 4-simplex. What is the hypersolid angle of the regular 5-cell? I managed to write some Python code that approximates the solid angle, shown below. It does so by choosing a random point in $\Bbb R^5$ using a spherically symmetric distribution (the normal distribution was easiest) and checking to see if the all coordinates were above average but the last one. The odds of success was then multiplied by the surface volume of the hypersphere (which is $2\pi^2$ ) to convert to cubic radians. (You can check that the equivalent code one dimension down gives the correct answer for the tetrahedron.) This code is very slow (it takes a minute or two to run) and not very accurate, but it was the best I could think of. The final verdict is that it's around $0.192$ or $0.193$ cubic radians. import numpy as np

many = 10_000_000
count = 0
for i in range(many):
  vec = np.random.normal(size=5)
  avg = np.mean(vec)
  count += (vec[0]>=avg and vec[1]>=avg and vec[2]>=avg and vec[3]>=avg)
result = count/many
print(f""The solid angle of a 5-cell is {round(result,3)} of the full""
  f"" hypersphere, or {round(2*np.pi**2*result,3)} radians^3."") The solid angle of a 5-cell is 0.01 of the full hypersphere, or 0.193 radians^3. I tried Googling for the answer but couldn't find anything. Since it's not readily available online, I'm guessing that it might not even have a closed form. If that's the case, I would be satisfied with a numerical answer, to perhaps 10 digits or so. (My Monte Carlo method can't get anywhere close.)","['trigonometry', 'solid-angle', 'geometry', 'simplex']"
4815624,Test if a finite group is a symmetric group algorithmically,"Suppose $G$ is a finite group with $|G|=n!$ for some $n \in \mathbb{N}$ . Suppose that we have the entire Cayley table of the group stored and we can perfom multiplication in the group as fast as it takes us to perform a lookup in the table. What's an efficient algorithm to test whether $G \cong S_n$ ? I.e. it returns true if $G \cong S_n$ , false otherwise. Any group theoretic object or property we may wish to find (e.g. abelianness, the centre of the group, the orders of elements, the conjugacy classes...) are all programmed in. If desired, suppose we also have a group known as $S_n$ also within the program that we can compare to the group somehow. Here's what I've thought of so far. Iterating through all maps $G \to S_n$ to check if it is an isomorphism would be awfully slow, since there's $(n!)^{n!}$ such maps which obviously grows incredibly fast. We may be able to reduce this if we have nice methods to find all homomorphisms, or all bijections, or whatever else, but it'd still be incredibly large. We can rule out some groups quickly by checking certain things that are fast to compute and whether they match $S_n$ such as The abelianness of $G$ matches $S_n$ (i.e. $G$ non-abelian, unless $n = 2$ ) The sizes of conjugacy classes The orders of elements of the group The size of the center Etc. But this would be a necessary but not sufficient condition. So how can we improve on this?","['symmetric-groups', 'group-theory', 'finite-groups', 'algorithms']"
4815644,Confusion regarding the definition of the state of a physical system,"I'm currently covering Jan de Vries' Elements of Topological Dynamics and in it he gives a brief introduction to the field through the lens of classical mechanics. He defines the state of a mechanical system in the following way: In order to generalize his definition of state, as used in this paragraph, say to a physical system consisting of $n$ objects $p_1, \ldots, p_n$ , is it right to say that a system's state, at a particular time $t$ , can be represented by a tuple $$(x_1(t), \ldots, x_n(t), \dot x_1(t), \ldots, \dot x_n(t))$$ where $x_i(t)$ and $\dot x_i(t)$ are the position and velocity vectors, respectively, of object $p_i$ at time $t$ ? Here, if I've understood things correctly, the first $n$ components give information on the systems configuration while the latter $n$ components give information on the system's velocity (by giving the velocity of each of its constituents). If I've understood the above notion of state correctly (and please tell me if I haven't), I'm a little confused at the way he introduces the notion of a law of motion. He briefly introduced it in the former picture, but he finishes here in this image: Since he uses the notation $x(t) = (x_1(t), \ldots, x_n(t))$ to denote the state of a system, I'm assuming some of the components of this tuple are velocities and not everything is simply configuration. So, if $n = 2k$ (for $k$ the number of objects in our system), the family of autonomous ordinary differential equations he gives in $(5)$ can be written as $$\dot x_i(t) = F(x_1(t), \ldots, x_k(t), \dot x_1(t), \ldots, \dot x_i(t), \ldots, \dot x_k(t))$$ So, $\dot x_i$ is the output of a function with $\dot x_i$ itself as one of the input variables. Is this correct or have I mixed up the notation?","['physics', 'classical-mechanics', 'ordinary-differential-equations', 'dynamical-systems']"
4815655,Does this prove that the factorial grows faster than the exponential?,"I want to prove that the factorial grows faster than the exponential function.
First, I introduce the ratio $$L = \frac{n!}{e^n}.$$ Then, I introduce another ratio : $$\frac{(n+1)!}{e^{n+1}} = \frac{(n+1) \cdot n!}{e \cdot e^n}
    = \frac{(n+1)}{e} L.$$ When the value of $n$ take values bigger and bigger and $L$ gets bigger and bigger.
In other words, $$\lim_{n \to \infty} \frac{(n+1)}{e} = \infty,$$ meaning that $L$ is divergent. Thus $n!$ grows faster than the exponential.","['factorial', 'proof-writing', 'solution-verification', 'limits', 'exponential-function']"
4815682,Prove $\frac{1}{\sqrt{a+b+7c}}+\frac{1}{\sqrt{c+b+7a}}+\frac{1}{\sqrt{a+c+7b}}\ge 1.$,"Problem. Given non-negative real numbers $a,b,c$ satisfying $a+b+c+abc=4.$ Prove that $$\color{black}{\frac{1}{\sqrt{a+b+7c}}+\frac{1}{\sqrt{c+b+7a}}+\frac{1}{\sqrt{a+c+7b}}\ge 1.}$$ I found the inequality accidentally and there is no original proof. In case it's old problem, I hope there is nice proof like AM-GM, Cauchy-Schwarz,... Equality holds at $(1,1,1);(0,2,2)$ and that makes some troubles when I tried to use classical inequalities application. You're welcome to share any ideas and comment here. Thank you for your interest. Here is what I tried so far. I thought of Holder inequality but my try is not good enough. $\bullet$ Holder using 1 $$\left(\sum_{cyc}\frac{1}{b+c+7a}\right)^2\cdot \sum_{cyc}(b+c+7a)(b+c+xa)^3\ge (x+2)^3(a+b+c)^3.$$ Choose $x$ such that equality holds at $a=b=2;c=0.$ Thus, we solve the equation $$256+2\cdot16\cdot(2x+2)^3=64(x+2)^3 \iff x=-2;x=0.$$ Notice that $x=0$ is satisfied. We consider $$\left(\sum_{cyc}\frac{1}{b+c+7a}\right)^2\cdot \sum_{cyc}(b+c+7a)(b+c)^3\ge 8(a+b+c)^3.$$ But $$ 8(a+b+c)^3\ge \sum_{cyc}(b+c+7a)(b+c)^3$$ is already wrong at $a=b=\dfrac{9}{10}.$ $\bullet$ Holder using 2 $$\left(\sum_{cyc}\frac{1}{b+c+7a}\right)^2\cdot \sum_{cyc}(b+c+7a)(x+a)^3\ge (a+b+c+3x)^3.$$ Choose $x$ such that equality holds at $a=b=2;c=0.$ Thus, we solve the equation $$4x^3+32(x+2)^3=(4+3x)^3\iff x=-4;x=-\frac{4}{3}. $$ Thus, both Holder using ways are failed. Maybe there is exist a better way.","['multivariable-calculus', 'lagrange-multiplier', 'inequality']"
4815683,Does every finitely presented group have a finite index subgroup with free abelianisation?,"Let $G$ be a finitely presented group. Does there exist a finite index subgroup $H$ such that its abelianisation $H^{\text{ab}} = H/[H, H]$ is free abelian? Note, if $G^{\text{ab}}$ is not already free abelian, then it is non-zero and surjects onto a non-trivial finite group, so $G$ admits a finite index subgroup. My motivation comes from compact manifolds. If $M$ is a compact manifold, then $\pi_1(M)$ is finitely presented and its abelianisation is $H_1(M; \mathbb{Z})$ which is a finitely generated abelian group which may have torsion. A finite index subgroup of $\pi_1(M)$ corresponds to a finite cover $N \to M$ . So my question above is asking whether I can take a finite cover $N$ of $M$ such that $H_1(N; \mathbb{Z})$ is torsion-free. If $M = k\mathbb{RP}^2$ is a non-orientable surface, then $H_1(M; \mathbb{Z}) \cong \mathbb{Z}^{k-1}\oplus\mathbb{Z}_2$ . Its orientable double cover $N = \Sigma_{k-1}$ has $H_1(N; \mathbb{Z}) \cong \mathbb{Z}^{2k-2}$ which is torsion-free. This example shows that the rank of $H_1$ can change by passing to covers (in terms of groups, the rank of the abelianisation can change by passing to finite index subgroups). If $G$ is virtually free, then of course the answer is yes. Nilpotent groups also have the desired finite index subgroups, see this MO question . In this case, more is true. Namely, the finite index subgroup $H$ can be chosen so that $H^{\text{ab}}$ has the same rank as $G^{\text{ab}}$ .","['free-abelian-group', 'compact-manifolds', 'group-theory', 'covering-spaces']"
4815685,Proof of global Peano existence theorem in ZF without mathematical logic,"There is a proof of Peano existence theorem in ZF. Peano existence theorem: For any open $D \subseteq \mathbb{R}^2$ , continuous $f:D \to \mathbb{R}$ and initial condition $\langle t_0,x_0\rangle \in D$ , there is an open interval $I \subseteq \mathbb{R}$ with $t_0 \in I$ and a differentiable function $X: I \to \mathbb{R}$ such that $X(t_0)=x_0$ and $X'(t) = f(X(t),t)$ for all $t \in I$ , and no strictly larger $I_\ast \supset I$ has such a function extending $f$ . reference: https://mathoverflow.net/a/455875 The proof in the link uses a theorem in mathematical logic called Shoenfield absoluteness to automatically translate ZFC proofs of low quantifier complexity statements about countably coded objects to ZF proofs. I am not familiar with mathematical logic and advanced set theory, nor do I understand Shoenfield absoluteness. Even if I understand it, I expect the proof generated by automatic translation to be not very human readable. Is there a proof of this theorem in ZF that can be understood by an undergraduate student who is not a set theory major?","['ordinary-differential-equations', 'logic', 'reference-request', 'axiom-of-choice', 'set-theory']"
4815733,Convergence of $\int_0^1 \int_0^1 (1-xy)^{-a}dxdy$,"How can I investigate the convergence of $\int_0^1 \int_0^1 (1-xy)^{-a}dxdy$ where $a \in (0, \infty)$ ? I know that one approach is to explicitly find the antiderivative but that seemed too laborious. Also, I believe that the integral converges for $a \in (0,2)$ by numerical solutions provided by WolframAlpha. Any help is greatly appreciated :)","['improper-integrals', 'multivariable-calculus', 'convergence-divergence']"
4815746,Big O Notation and Derivative for Proof,"Show that $\frac{f(a+h)-f(a-h)}{2h}-f'(a)=\mathcal O(h^2)$ if, on open interval containing $a$ , $f$ has continuous derivative up to third derivative By using L'Hospital rule, $\lim_{h\to 0} \frac{f(a+h)-f(a-h)}{2h}=\lim_{h \to 0} \frac{f'(a+h)+f'(a-h)}{2}=\frac{2f'(a)}{2}=f'(a)$ But the question specifically states derivatives up to the third one. How to incorporate the second and third derivative in the proving?","['calculus', 'derivatives']"
4815767,Monochromatic square in colored plane,"Square Theorem. Color every point in the real plane using a finite amount of colors. Show there exists a square whose vertices are monochromatic. I am aware this question is a duplicate , however, the answers given there don't really prove the statement. Despite there being references quoted, they all seem to be lacking a proof as well. I am also aware this is a consequence of Gallai's Theorem, which states Gallai's Theorem. Color every point in $\mathbb R^n$ using a finite amount of colors. Let $A\subset\mathbb R^n$ be a finite set. Then there is a monochromatic subset $A'\subset\mathbb R^n$ that is homothetic (i.e., similar and parallel) to $A$ . This theorem is so unpopular there is not even a Wikipedia page about it, but one may find a proof in the great The Mathematical Coloring Book (which do have its own Wikipedia page ). Still, the book mentions two existing direct proofs for the Square Theorem, one through Van der Waerden's Theorem and the other through a strategy similar to Van der Waerden's Theorem's proof, none of which it presents nor I've found elsewhere. It seems I'm not the only one: this presentation cites two supposed direct proofs for the Square Theorem as ""folklore"". Anyway, is there a direct proof for the Square Theorem? Does anyone knows it? I have not even been able to find a proof for the three color case and its preventing me to sleep for so long now.","['graph-theory', 'ramsey-theory', 'combinatorics', 'coloring']"
4815777,Is there a shape which maintains a constant surface area as it dissolves?,"Whether for hard candies, medicines, chlorine tablets, etc it seems that many applications would benefit from a 3D shape that maintains its surface area as it dissolves. Anton Petrunin's answer to the 2017 MathOverflow question ""Solids with constant surface area during 'erosion'"" suggested a sphere with a small hole drilled through the center. This is a pretty good answer as it transitions from an almost-sphere to a toroid by the end of its life (which maintains a high surface area even as its volume approaches zero). For a shape that remains a single, unbroken solid throughout, this is probably as good as it gets? But what if we relax that constraint and allow the shape to break into pieces as it dissolves? Is there some kind of fractal shape perhaps that might allow an even more consistent surface area as the shape dissolves? EDIT: We're still assuming uniform dissolving, so any material within epsilon of the outside will dissolve within delta_t, and the total time for the shape to dissolve shouldn't approach zero. EDIT2: Here's the sketch of an idea in 2D: The idea here is that the card will repeatedly split in half as it dissolves, due to the specially shaped holes that will fill with solvent. But I guess along with such ""hole filling"" and ""splitting"" there will always be discontinuities in the surface area over time (?), so it's impossible to hold the surface area truly ""constant""... Is that true?",['geometry']
4815786,Probability that $n$ white balls are drawn before $m$ black balls,"Question: Balls are randomly withdrawn, one at a time without
replacement, from an urn that initially has
N white and M black balls. Find the probability
that n white balls are drawn before m black balls, $n\leq N ,m \leq M$ . The answer: Now I have a a lot of issues with this answer and I will try to summarize. First: I don't understand what the $m+n-1$ should represent. Second: The answer says $m$ or less; shouldn't it be just less than $m$ ? since if were to have at least $n$ balls then the largest number of black balls we are to have is simply $m-1$ . Third: Overall as you've probably already noticed, I simply do not understand this solution whatsoever, for instance, why did we have to consider at least $n$ balls; shouldn't we just care about getting exactly $n$ balls before $m$ ? Fourth: Following up my third point I propose another solution where the experiment ends once we withdraw our $n^{th}$ ball before having withdrawn our $m^{th}$ ball; this can be done in a number of ways: $n$ white; $0$ black $n$ white; $1$ black; and so on until: $n$ white; $m-1$ black so the solution in this case should be: $\sum\limits_{i=0}^{m-1} \frac{ \binom{N}{n} \binom{M}{i}}{\binom{M+N}{n+i}}$ Now obviously if you were to substitute with values for $n, m, N, M$ ; the results will be different. What I want is to understand the author's answer first; then understand why my answer is incorrect. Thanks in advance!","['combinatorics', 'probability']"
4815819,"How to prove $\int_{\mathbb R} \nabla_u F (x, u_k(x))\cdot\varphi dx \to \int_{\mathbb R} \nabla_u F (x, u(x))\cdot\varphi dx$ for all test function?","Let $F(x, u)\in C^1(\mathbb R\times\mathbb R^n, \mathbb R)$ and let $(u_k)$ be a sequence such that $$ u_k\to u \text{ in } L^\infty_{loc}(\mathbb R, \mathbb R^n)$$ and $$ u_k\to u \text{ a.e. in } \mathbb R.$$ I want to use these information to get that $$\int_{\mathbb R} \nabla_u F (x, u_k(x))\cdot\varphi dx \to \int_{\mathbb R} \nabla_u F (x, u(x))\cdot\varphi dx \quad\forall \varphi\in C_c^\infty(\mathbb R, \mathbb R^n)$$ as $k\to +\infty$ . Anyone can please provide some hints?
I can not use Fatou's Lemma (since I don not know if the sequence is nonnegative) and I can not use the dominated convergence theorem (since I can not find a function of $L^1$ to dominate that quantity.
The only idea left is to observe that $$\int_{\mathbb R} \nabla_u F (x, u_k(x))\cdot\varphi dx = \int_{supp(\varphi)} \nabla_u F (x, u_k(x))\cdot\varphi dx,$$ but I am not sure how to use this information. Anyone could please help?","['multivariable-calculus', 'limits', 'calculus', 'real-analysis']"
4815822,"The two envelopes problem, only we know which envelope has the ""original"" amount","This problem is from Sheldon Ross's book, a first course in probability: Problem Body: A philanthropist writes a positive number $x$ on a piece of red paper, shows the paper to an impartial observer, and then turns it face down on the table. The observer then flips a fair coin. If it shows heads, she writes the value $2x$ and, if tails, the value $x/2$ , on a piece of blue paper, which she then turns face down on the table. Without knowing either the value $x$ or the result of the coin flip, you have the option of turning over either the red or the blue piece of paper. After doing so and observing the number written on that paper, you may elect to receive as a reward either that amount or the (unknown) amount written on the other piece of paper. For instance, if you elect to turn over the blue paper and observe the value $100$ , then you can elect either to accept $100$ as your reward or to take the amount (either $200$ or $50$ ) on the red paper. Suppose that you would like your expected reward to be large. Let $y$ be a fixed nonnegative value, and consider the following strategy: Turn over the blue paper, and if its value is at least $y$ , then accept that amount. If it is less than $y$ , then switch to the red paper. Let $R_y(x)$ denote the reward obtained if the philanthropist writes the amount $x$ and you employ this strategy. Find $E[R_y(x)]$ . Note that $E[R_0(x)]$ is the expected reward if the philanthropist writes the amount $x$ when you employ the strategy of always choosing the blue paper. My doubts: Now this is different from the usual envelope problem because we know which paper ""envelop"" has the original i.e. $x$ amount; so it's always beneficial to pick the blue paper. Concerning the $E[R_y(x)]$ ; I don't know how to calculate it for any $y, x$ ; that's because $x$ can be any positive number for example let's call the amount on the blue paper $b$ , if we have $y=1$ ; then for any $b$ if $b \geq y$ then we stay with the blue paper and if $b < y$ then we switch to the red paper; for each of those cases we can easily calculate the expected gain, the issue arises when I want to calculate the overall expected gain; that's because $0 < b \leq +\infty$ ; so it doesn't make sense to calculate the probability for $b \geq y$ and $b < y$ . I'm very confused so any help will be much appreciated. Thanks in advance!","['probability', 'random-variables']"
4815845,"Would We have the preimage of at least one interval taken out from $[0;1]$, inside the any ball taken out from $X$?","This is the theorem: Let $X$ be separable metric space endowed with non-atomic Borel measure such that $\mu X = 1$ . Using this theorem We can establish isomorphism between $X$ and $[0;1]$ . Denote this mapping by $f$ . I want to show that for at least one positive-measured interval $I \subset [0;1], \: \:$$f^{-1}(I\setminus Iâ) \subset B$ , where $Iâ$ is the subset of $I$ , of which measure is equal to $0$ , i.e. $m(Iâ) = 0$ , where $B$ is any ball in $X$ with positive measure. To sum it up I have to show that Inside every positive-measured $B \subset X$ , We  would have the preimage of at least one subset of positive-measured interval taken out from $[0;1]$ , of which Lebesgue measure would be equal to the measure of taken interval. Is it possible to construct such isomorphic mapping or at least a measure-preserving mapping which would satisfy these conditions?
Any help would be appreciated.","['measure-theory', 'lebesgue-measure', 'analysis', 'real-analysis', 'functional-analysis']"
4815847,Can this be done? Split Pascal's triangle (without the $1$s) with a straight line into two regions of equal sums.,"Consider Pascal's triangle with $n$ rows, without the $1$ s, with each number corresponding to a vertex on a pyramid of equilateral triangles, as shown below with example $n=5$ . Can the triangle be divided by a straight line (that does not pass through any vertex) into two regions of equal sums? (That is, does there exist an $n$ such that Pascal's triangle with $n$ rows, without the $1$ s, can be divided by a straight line into two regions of equal sums?) For example, with $n=5$ , the red line below fails to divide the triangle into two regions of equal sums: the upper-left region has a sum of $58$ , and the bottom-right region has a sum of $56$ . I have tried to do this with different size triangles, without success. It seems that it cannot be done, but I don't know how to prove it. Remarks We exclude the $1$ s because the triangle with the $1$ s has a total sum of $2^n-1$ , an odd number, making an equal split obviously impossible. The sums of the all the terms in the first $n$ rows (not just one row) are given by A145654 (ignoring the initial $0$ ): $2,8,22,52,114,240,494\dots$ I considered the partial sums of a row of Pascal's triangle, but such expressions are not easy . I also tried to start with Pascal's triangle without the $1$ s and without the terms $\binom{n}{1}$ and $\binom{n}{n-1}$ , and I still couldn't split it. However, it is easy to split a triangle with terms $1*1\ 1*1\ 2\ 1*1\ 2\ 2\ 1*1\ 2\ 3\ 2\ 1$ (rows are separated by "" $*$ ""; in each row, numbers increase by $1$ up to a max, then decrease by $1$ .). I'm more interested in the method used to answer this question, than the answer itself. I've been trying to demystify Pascal's triangle, but it still mystifies me. EDIT: Taking @Jean Marie's suggestion, I've added a left-justified version of the triangle, which is easier to use. EDIT2: â","['conjectures', 'geometry', 'examples-counterexamples', 'binomial-coefficients', 'sequences-and-series']"
4815861,Operator norm of integration operator to the power n,"I'm working on an assignment in 'Functional Analysis' by Haase. It is as follows: We define the integration operator $J: \mathrm{L}^2(a,b) \to \mathrm{C}[a, b]$ as $$
(Jf)(t) := \int_a^t f(x) \mathrm{d}x = \langle f, \mathrm{1}_{(a, t)}\rangle_{\mathrm{L}^2} \quad \quad (t\in[a,b],\,f\in\mathrm{L}^2(a,b))
$$ Determine the operator norm $\left\lVert J^n \right\rVert$ of $J^n$ , for $n\in \mathbb{N}$ acting on $\mathrm{C}[a,b]$ with the supremum norm. My attempted solution In the book it is already stated that the operator norm is equal to $\frac{1}{n!}$ . The definition of the operator norm is $$
     \left\lVert J^n\right\rVert:= \sup_{\substack{f\in\mathrm{L}^2(a,b) \\ \left\lVert f\right\rVert\leq 1}} \left\lVert J^nf\right\rVert_\infty = \sup_{t\in[a,b]} \left| \frac{1}{(n-1)!} \int_a^t (t - s)^{n-1}f(s)\mathrm{d} s  \right|
     \leq \frac{1}{(n-1)!} \sup_{t\in[a,b]} \int_a^t (t - s)^{n-1}|f(s)| \mathrm{d} s.
$$ Lower bound Assuming that $b - a > 1$ , then certainly $f(s) = \frac{1}{(b - a)^n}$ has norm less than or equal to one. Then we can get $\left\lVert J^n\right\rVert \geq \frac{1}{n!}$ . But what about the case $b - a < 1$ ? Upper bound I can pull the absolute values inside the integral, but then I do not see how to proceed. I can use Cauchy-Schwarz, but then I get the Hilbert-Schmidt norm of this (integration) operator. This norm is too big I believe (and cannot involve a 'simple' $n!$ , as we square the kernel function) Any help/tips are appreciated! EDIT The book stated that the solution is $\frac{1}{n!}$ while it should have been $\frac{(b-a)^n}{n!}$ . Thanks to @geetha290krm for pointing this out! The big mistake is that I did not realize that the domain and the target of $J$ , is $\mathrm{C}[a,b]$ . This makes things a lot easier. If someone has a solution if we consider $J: \mathrm{L}^2(a,b) \to \mathrm{C}[a,b]$ feel free to post it!","['normed-spaces', 'functional-analysis', 'analysis']"
4815892,Uniform Taylor expansion,"$f \colon \mathbb R^n \to \mathbb R$ is  differentiable in $x_0$ if there exists a functional $L$ $$f(x_0+h)-f(x_0)-Lh=o(|h|),$$ as $|h|\to 0.$ Here $o(|h|)$ denotes a function going to $0$ faster than $|h|$ depends on $x_0.$ Denote $Df(x_0)=L$ . $f$ is continuously differentiable in $\mathbb R^n$ if it  is differentiable in $x_0$ for every $x_0$ and $Df(x)$ is continuous on $\mathbb R^n.$ What are the conditions guaranteing that $o$ is independent of $x_0$ ? Intuitively it should be that $Df(x)$ is uniformly continuous, is that the case?","['frechet-derivative', 'derivatives', 'analysis', 'real-analysis']"
4815918,Diagonalization and Homomorphism,"Maybe this question could sound silly, but after carefully re-reading my linear algebra notes one particular detail catch my sight. Let $V$ a $\Bbb K$ -vector space, $\dim(V) = n$ , $f$ an $\textbf{endomorphism}$ is said to be diagonalizable if exist a base $\mathcal{B}$ of $V$ s.t. $f$ representative matrix with respect to $\mathcal{B}$ is a diagonal matrix. Till there nothing strange is the definition of diagonalization, but as far as I'm concerned there are multiple ways to define some application $g: V \to W$ with $\dim(V) = \dim(W)$ , and the representative matrix of $g$ is a square matrix. The question is: we want $f$ to be an endomorphism because otherwise there isn't nothing relevant to say even tough algorithmically speaking we could make the same eigenvalue/eigenvector calculation over any square matrix, or since $V,W$ have the same dimension are isomorphic as vector spaces and we are basically working with endomorphism? My guess is that the second reasoning could makes sense since (using the little bit of understanding I have of category theory) let $F$ be a $W,V$ isomorphism and $g:V\to W$ $$\require{AMScd}
\begin{CD}
V @>{g}>> W \\
@V{id_{V}}VV @V{F}VV \\
V @>{Fg}>> V 
\end{CD}$$ This is a commutative diagram ad $Fg$ is an endomorphism. Hope for some clarification, thank you. Edit After some responses, the possibility to choose different basis for $V,W$ lead to every homomorphism to have a diagonal form (this isn't the case considering only endomorphism). In the situation descibed above can we deduce some information (eigenvalues/eigenvectors,...) from $g$ that can be transported to $Fg$ ?","['diagonalization', 'linear-algebra', 'linear-transformations']"
4815923,Can't understand how my solution is leading to double counting.,"We want to count all possible 8 coin tosses with at least 3 heads. I know two correct ways to solve this. One is to simply add up all individual cases of 3 heads, 4 heads and so on upto 8 heads. This results in $$\sum_{i=3}^n \binom{8}{i} = 219$$ . Another is to subtract the unwanted cases from all possible cases: $$2^8 - \binom{8}{0} - \binom{8}{1} - \binom{8}{2} = 219$$ . My (incorrect) reasoning is:
Choose 3 tosses out of eight to be heads, and assign heads or tails to the remaining 5 tosses. Leading to the expression: $$\binom{8}{3}\times 2^5 = 1792$$ This is absurd because it is bigger than the possible types of eight coin tosses namely $2^8=256$ . Clearly there is double counting. But I can't seem to understand how.","['combinatorics', 'discrete-mathematics']"
4815958,Question about proof of Kolmogorov inequality for Bernoulli random variables,"The question is about one inequality which shows in Kolmogorov's paper ( inequality (3.1) ) but is not proved. The inequality says that, if we assume $Y_1,Y_2,\ldots$ are i.i.d. Bernoulli random variables with expectation $p$ , then the following inequality holds $$\mathbb{P} \left( \sup_{k\ge n} |\overline{Y}_k - p|\ge \varepsilon \right) \le 2 e^{-2 n \varepsilon^2 (1 - \varepsilon)} \qquad \text{for any $\varepsilon>0$,}$$ where $\overline{Y}_k = \sum_{j=1}^k Y_j / k = S_k / k$ .
To prove and improve this inequality, several people made contributions. However, I found I got lost when reading several steps in the proofs of these literature. Paper 1 discusses a general case where Bernoulli random variables are independent but not identical. However, in the 9th line of the proof of the Lemma 1 (key lemma), it seems that the inequality is contrary to the assumption ( $\phi_i^\varepsilon (t) \ge 1$ ). Without Lemma 1, we cannot get the inequality we want. Paper 2 improves the inequality proposed by Kolmogorov. However, Lemma 1 is also weird. First it cites Lemma 3 in paper 3 , but the definition of $\phi_\varepsilon (\lambda)$ (paper 2) and $\varphi_\varepsilon (\lambda)$ (paper 3) are different, with opposite sign before $\lambda$ . Despite this fact, both these two lemmas are strange. They say that: let $\varepsilon\ge 0$ , if there exists a $\lambda>0$ s.t. $\phi_\varepsilon (\lambda) = \mathbb{E} (e^{\lambda (X_i - \varepsilon)}) \le 1$ , then $$ \mathbb{P} \left( \sup_{k\ge n} \frac{\sum_{i=1}^k Y_i}{k} \ge \varepsilon \right) \le \left[ \phi_\varepsilon(\lambda) \right]^n. $$ It is strange that the inequality is also true (trivial) if $\phi_\varepsilon (\lambda) > 1$ . We only needs to find that minimum of $\phi_\varepsilon(\lambda)$ when we fix an $\varepsilon$ . Then I checked the proof in paper 3 , especially the proof of Lemma 2. I did not find mistakes except one citation in the first line of the proof. It is a Russian book and we need theorem in page 183. In the proof, it defines a random variable $\eta (x) = \inf \{ i: 0\le i\le n, S_i \ge x \}$ . However, I did not understand the following inequality. $$ \mathbb{E} (e^{t S_n} \mid \eta(x) = k) \ge e^{tx} \mathbb{E}(e^{t S_{n-k}}). $$ I guess it uses the independence of $Y_i$ , but I think here we need conditional independence, and it is not true since $\eta(x)$ is defined by $Y_1,\ldots,Y_n$ . I truly wish you can give me some hints, no matter on whether this inequality is true, or on where I can find the correct proof, or on whether the ''mistakes'' I found is right. Thank you. Any comments help.","['inequality', 'probability-theory']"
4815985,Is $\mathbb{C}^{n+1}$ \ $\{0\}$ isomorphic to $\mathbb{P}^n\times \mathbb{C}^*$?,"Is $\mathbb{C}^{n+1}$ \ $\{0\}$ isomorphic to $\mathbb{P}^n\times \mathbb{C}^*$ ? Where $\mathbb{P}^n$ is $n$ -dimensional complex projective space, $\mathbb{C}^*=\mathbb{C}$ \ $\{0\}$ . My idea: Under the classical topology, the $\pi_1(\mathbb{C}^{n+1})=\{e\}$ but $\pi_1(\mathbb{P}^n\times \mathbb{C}^*)=\mathbb{Z}$ . hence their classical topologies are not homeomorphic. The regular map is clearly continuous, hence if they are isomorphic(as two varieties), they will be homeomorphic, this can't be true. The above idea is effective but not algebraic geometrical. Is there some algebraic geometrical way to prove $\mathbb{C}^{n+1}$ \ $\{0\}$ is not isomorphic to $\mathbb{P}^n\times \mathbb{C}^*$ ? (I haven't learned about ""scheme"", can somebody tell me in basic language?)","['algebraic-geometry', 'abstract-algebra', 'classical-algebraic-geometry']"
4815996,Why is the Spectrum of an Operator Used as the Domain in Continuous Functional Calculus?,"I'm currently working to grasp the concepts of (continuous) functional calculus, aiming to prove the spectral theorem for bounded self-adjoint operators as outlined in ""Introduction to Hilbert space and the theory of spectral multiplicity"" by Paul Halmos and ""Quantum Theory for Mathematicians"" by Brian Hall. The specific definition of continuous functional calculus I'm exploring is as follows: Theorem 8.3 ( ""Quantum Theory for Mathematicians"" - Brian Hall): Let $A \in \mathcal{B}(H)$ be a self-adjoint operator. Then there exists a unique bounded linear map from $\mathcal{C}(\sigma(A); \mathbb{R})$ to $\mathcal{B}(H)$ , denoted as $f \mapsto f(A)$ , such that when $f(\lambda)=\lambda^{m}$ , we have $f(A)=A^{m}$ . This mapping, denoted as $f \mapsto f(A)$ , where $f\in \mathcal{C}(\sigma(A);\mathbb{R})$ , is commonly referred to as the (real-valued) functional calculus for the operator $A$ ."" My questions are mainly concerned with the choice of domain of $f$ : Why is the domain of $f$ restricted to $\sigma(A)$ (the spectrum of $A$ )? Why not simply use $\mathbb{R}$ as the domain, given that $A$ is self-adjoint and $\sigma(A) \subset \mathbb{R}$ ? In Halmos' book, for a polynomial $p(\lambda)=\sum_{j=0}^{n}\alpha_{j}\lambda^{j}$ and an operator $A$ on a Hilbert space, he defines $p(A)$ to be $p(A)= \sum_{j=0}^{n}\alpha_{j} A^{j}$ . To me, this seems acceptable, assuming that the domain and codomain of $p(A)$ and $A$ are the same. However, am I correct that when considering more complex functions like Borel or holomorphic functions, we need to be cautious about our choice of domain? Unfortunately, this aspect is not generally explained. Update :
The proof makes use of the Stone-Weierstrass theorem, i.e. that polynomials are dense in $C(K;\mathbb{R})$ , where $K$ is a compact set. Thus we could use any compact $K \supset \sigma(A)$ . However, since $\sigma(A)$ is compact, we wouldn't gain anything as pointed out in the answers below.","['spectral-theory', 'functional-analysis', 'functional-calculus']"
4816000,Why is this integral zero?,"I was solving a problem on Fourier Analysis ( namely, I was proving that $\int|f|^2dx=\sum_{n\in \mathbb{Z}} |f(n)|^2$ if $f$ is of moderate decrease and the fourier transform of $f$ is supported on $[-1/2,1/2]$ ) and the problem boils down to proving that this integral is zero for $n\not=m$ where $n$ and $m$ are both integers: $$\int_{-\infty}^\infty \frac{\sin(\pi x)}{(x-n)}\frac{\sin(\pi x)}{(x-m)}dx=0$$ I am not really sure how to compute this integral. I know $\sin(\pi x)^2/x^2$ can be integrated using the fourier transform of a characteristic function. I tried to adapt this here but to no avail.","['integration', 'calculus', 'fourier-analysis']"
4816004,"A fair 6-sided die is thrown 10 times. What is the probability of rolling a six three times in a row, and the other rolls not being a 6?","In 10 rolls, there are 8 positions where the chain of three can start, so there are 8 permutations since dice rolls are interchangeable (their order doesn't matter). Therefore, I believe that the answer should be $8\times \left(\frac 16\right)^3\times \left(\frac 56\right)^7$ . However, I am not $100\%$ sure. Any confirmation is appreciated.","['binomial-theorem', 'probability']"
4816015,doubt regarding a step proof of Cauchy-Schwarz inequality. Is it valid?,"I'm quite new to math proofs,I can't understand why the part where we set $\alpha=\|\boldsymbol{v}\|^2$ e $\beta=-\boldsymbol{u} \cdot \boldsymbol{v}$ works
, why are proofs like those possible and valid? By setting alpha and beta to these values aren't we proving the theorem just for the case where $\alpha=\|\boldsymbol{v}\|^2$ e $\beta=-\boldsymbol{u} \cdot \boldsymbol{v}$ , the proof isn't generalizing enough or am I missing something? Theorem : If $(V, \cdot)$ is an euclidean vector space (real), then $\forall \boldsymbol{u}, \boldsymbol{v} \in V$ , we have: $|\boldsymbol{u} \cdot \boldsymbol{v}| \leq\|\boldsymbol{u}\|\|\boldsymbol{v}\|$ ,  Cauchy-Schwarz inequality. Proof. Let us first prove the Cauchy-Schwarz inequality. It is clear that the inequality is verified if at least one of the two vectors is null. We therefore assume that they are both nonzero. Let us consider $\boldsymbol{w}=\alpha \boldsymbol{u}+\beta \boldsymbol{v}, \operatorname{with} \alpha, \beta \in \mathbb{R}$ , $$
\boldsymbol{w} \cdot \boldsymbol{w}=(\alpha \boldsymbol{u}+\beta \boldsymbol{v}) \cdot(\alpha \boldsymbol{u}+\beta \boldsymbol{v})=\alpha^2\|\boldsymbol{u}\|^2+\beta^2\|\boldsymbol{v}\|^2+2 \alpha \beta \boldsymbol{u} \cdot \boldsymbol{v} \geq 0 .
$$ *** then taking $\alpha=\|\boldsymbol{v}\|^2$ e $\beta=-\boldsymbol{u} \cdot \boldsymbol{v}$ , we get $$
\|v\|^4\|\boldsymbol{u}\|^2+(\boldsymbol{u} \cdot \boldsymbol{v})^2\|v\|^2-2\|v\|^2(\boldsymbol{u} \cdot \boldsymbol{v})^2=\|\boldsymbol{v}\|^2\left(\|\boldsymbol{v}\|^2\|\boldsymbol{u}\|^2-(\boldsymbol{u} \cdot \boldsymbol{v})^2\right) \geq 0 .
$$ Since $v \neq 0$ , we can divide by $\|v\|^2$ , and get the inequality $$
\|\boldsymbol{v}\|^2\|\boldsymbol{u}\|^2 \geq(\boldsymbol{u} \cdot \boldsymbol{v})^2, \Longrightarrow|\boldsymbol{u} \cdot \boldsymbol{v}| \leq\|\boldsymbol{u}\|\|\boldsymbol{v}\| .
$$","['proof-explanation', 'proof-writing', 'linear-algebra']"
4816045,Taylor series of $\sum_{n=0}^{\infty} \frac{\cos(n^2 x)}{2^n}$,"I have the following problem. I must show that the following function $f$ is infinitely differentiable, then find its Taylor series centered at $0$ , and the corresponding radius of convergence: $$f(x) = \sum_{n=0}^{\infty} \frac{\cos(n^2 x)}{2^n}.$$ I think that, since the series is absolutely convergent, it follows that it is infinitely differentiable, but I am not sure about that. Furthermore, I have no clue as to how to find the Taylor series. I was thinking maybe I could use the series expansion of the cosine, but I am not sure about that either.","['taylor-expansion', 'analysis', 'sequences-and-series']"
4816069,Why is the set of probability measures not weak*-compact?,"Let $M(X)$ be the set of probability measures on a Polish space $X$ with Borel $\sigma$ -field. Further consider the properties of $M(X)$ when considered as members of the dual space of $Y:=C_b(X)$ - the supremum-normed space of bounded continuous functions on $X$ . In this context, I struggle to see the reason, wyh the following statement is false: $M(X)$ is weak*-compact. My reasoning:
According to Banach-Alaoglu the following set is weak*-compact: $$K:=\{\Lambda \in Y^* | |\Lambda(V)|\leq 1 \},$$ wherein $V$ denotes any neighborhood of zero. Choose the open unit-ball as $V$ then, clearly, every probability measure $\mu \in M(X)$ lies in $K$ . Hence, the set of probability measures is at least a subset of a weak*-compact set.
Further, if I have a sequence of probability measures which converge weakly to another measure $\delta$ , then I think one can directly show that $\delta$ itself is also a probability measure (using $\int f\mathrm{d}\mu_n \to \int f\mathrm{d}\delta$ for any $f \in Y$ ).
Therefore, I have almost proven that M(X) is a closed subset of a compact set - and therefore compact. The only way this can be wrong is if a) someone can point out an error in the above derivation or b) there are members of the dual-space of $Y$ which aren't representable as measures $\delta$ , in that case I would appreciate it, if someone could give an example of such a member. In any case: Many thanks for your help!","['general-topology', 'probability-theory', 'functional-analysis', 'polish-spaces']"
4816125,Krull dimension and Lebesgue covering dimension,"I'm reading the book Algebraic Geometry, an Introduction (Daniel Perrin) and it introduces a notion of dimension of a topological space as the maximal length of ascending chains of irreducible closed subsets. It looks like the same notion of topological dimension is introduced in Hartshorne and called the combinatorial dimension (see this question ). Using this definition on an algebraic variety equipped with the Zarizki topology, the dimension can be defined algebraically (e.g. on an affine variety it corresponds to the Krull dimension of the coordinate ring). My question is: how does the combinatorial dimension relate to the Lebesgue covering dimension , which is what is usually used on a generic topological space? Also, how do we see in the case $K = \mathbb{C}$ and when the variety $X$ is non-singular, that it corresponds to the dimension of $X$ as a manifold?","['krull-dimension', 'general-topology', 'algebraic-geometry']"
4816148,"prove that $fâ(x)e^{\lambda x}$ is increasing if and only if $fâ(x)+\lambda f(x)$ is increasing. Where $f\in C^1(0,\infty)$.","prove that $fâ(x)e^{\lambda x}$ is increasing if and only if $fâ(x)+\lambda f(x)$ is increasing. Where $f\in C^1(0,\infty)$ , and $\lambda$ is a real number. I have tried to prove it by taking $0<x_1<x_2$ to make difference and to control each other. But it seems not that easy, I also have tried to prove it by contradiction, it failed as I need to find a small interval that $fâ$ is monotone. But it isnât always satisfied. Can anyone help me to figure it out or just give me some intention? Very appreciate it!","['inequality', 'derivatives', 'monotone-functions']"
4816155,integral evaluating to a quantity 'almost everywhere' in $\mathbb{R}^2$,"Let $g:[0,1]^2 \to [0,1]$ be a measurable function. Suppose $\int\limits_0^1 g(x,t)g(y,t)dt = A$ holds for almost every $(x,y)\in [0,1]^2$ then prove that $\int\limits_0^1 g^2(x,t)dt = A$ for almost every $x \in [0,1]$ I found the question in an assignment so I am not sure if the question has a typo or not but I am led to believe so. Suppose we have that the condition in the hypothesis does not hold over the line segment $x=y$ lying inside the $2$ dimensional unit square (which of course has measure zero in $[0,1]^2$ ). Then clearly the conclusion will not follow, right? I need help finding a counter example OR if my intuition is wrong, I would like to see some ideas towards a proof","['integration', 'functional-analysis', 'almost-everywhere']"
4816163,Nonincreasing cdf? Potential error in my textbook..,"Image from Introduction to Mathematical Statistics (7th edition) by Hogg, McKean & Craig: I hope I'm not missing something obvious here, but isn't a cdf supposed to be non decreasing ? If $F$ is a cdf of a random variable $X$ and $x \leq y$ then it must be that $F(x) = P(X \leq x) = P(X \leq y) - P(x < X \leq y) \leq P(X \leq y) = F(y)$ .","['statistics', 'probability-theory', 'probability']"
4816169,"simplify degree 2 polynomials over $\mathbb{R}[X,Y]$","I am currently attending a class on scheme theory and we were given the following problem: Let $f(x,y)=a_1x^2+a_2xy+a_3y^2+a_4x+a_5y+a_6\in\mathbb{R}[x,y]$ be a non-zero polynomial of degree 2. Show that the vanishing locus of $f$ is isomorphic to one of the followig: $V(x^2+y^2-1)$ $V(y-x^2)$ $V(yx-1)$ $V(x^2)$ $V(x(x-1))$ $V(xy)$ $V(x^2+y^2+1)$ We were given the hint that wlog $a_1x^2+a_2xy+a_3y^2\in \{x^2,xy,x^2+y^2\}$ after suitable linear trasnformations. Now for my question: I understand how you can proof the exercise from the hint (essentially huge case distinction and work everything out by hand if there is a more elegant way please give me a hint). However I am unsure how I can obtain the hint. I know how to change variables to elimante, say $y^2$ but I don't understand how I can then kill $xy$ without creating new potential $y^2$ terms. I feel like there is a trick am missing but I can't seem to make any progress. Any help is appreciated","['algebraic-geometry', 'abstract-algebra']"
4816183,Radius of circle given horizontal distances of two points and angle between them,"Problem I'm not certain this is possible to solve (and indeed, if the answer is ""don't be silly, that's not possible"" then at the very least it'll save me some time!). I am attempting to find the radius $r$ of a circle on which I know the horizontal position of two points (relative to the center), $x_1$ and $x_2$ , and the angle between them, $\alpha$ . I realize this certainly won't be possible in all cases, e.g. when $\alpha$ is zero or any multiple of $2\pi$ then there's a single point and no possible solution, but I'm not sure it's possible even if $\alpha$ is not these things. Put simply: I know $x_1$ , $x_2$ , and $\alpha$ . I need to find $r$ . Things I've tried so far: using the Cosine rule on the isosceles $\triangle{{x_1}{x_2}O}$ to produce an equation for the base ( ${x_1}\to{x_2}$ ) in terms of $r$ and $\alpha$ , but of course while I know $\alpha$ I don't know the length of the base because I've only got the horizontal coordinates. Combining that with right-angle triangles dropped from $O$ proved fruitless, but perhaps I'm missing something here? Some searching on this site turned up numerous problems involving two points and a circle, but the closest I could find to my particular situation was ""calculating the radius of a circle if the distance between two points and the angle from the center are known"" , but of course in this case I don't know the distance between these points, precisely. Context A couple of years ago, I was watching Matt Parker put a ludicrous number of RGB LEDs on his Christmas tree . Being in possession of a similarly ludicrous number of RGB LEDs, and a micro-controller needed to drive them (plus some small skill in coding on Raspberry Pis, and their cameras), I decided I should attempt something similar with ours. I got the micro-controller, and the LEDs, and even a nice web interface going. However, the calibration of the 3D positions of the LEDs on the tree proved too much to do in my spare time that year. This year, I'm revisiting the problem. I've got to the point of a nice chunk of code (with web interface) that'll capture an image of the base tree (unlit), and an image of each LED lit in turn, at a series of arbitrary angles (specified by the user), calculating the positions of each LED as seen by the camera by diff'ing each capture to the (unlit) base capture. Hence, in the diagram above, $O$ is looking down the trunk of the tree. I can estimate (pretty accurately) its location based on an average of the horizontal positions of all LEDs found in the captured images. $x_1$ and $x_2$ are the horizontal positions of a single LED captured at two different tree angles, and we know $\alpha$ because it's literally input from the user (""how far did you turn the tree?""). Calculating the radius $r$ will give me the distance of the LED from the trunk. We only have the horizontal positions because the diagram above is us looking at the tree down its trunk. The camera in this case is placed in ""front"" of the tree, hence why I've extended the dotted lines down below the diagram towards the ""camera"" (and yes, I realize they're not technically parallel in practice, but ""good enough"" is fine here!). I did note that, though Matt doesn't go through his method of calibrating the positions of the LEDs (only the corrections post-calibration), he does mention that he rotated the tree through 90 degrees each time. This makes the problem much simpler: If the tree is always rotated through a right-angle, we can take advantage of the congruence of two right-angled triangles and note that: $$x_1 = r\cos{\alpha}\\
x_2 = r\sin{\alpha}\\
\frac{x_2}{x_1} = \frac{r\sin{\alpha}}{r\cos{\alpha}} = \tan{\alpha}\\
\alpha = \arctan{\frac{x_2}{x_1}}\\
r = \frac{x_1}{\cos{\alpha}}
$$ Job done! But that gives you a maximum of four possible positions of the tree, and it can be quite fiddly to reach one of those positions in our diminutive living room with all the LEDs hooked up. Hence, I'd really like to be able to have the user specify arbitrary rotations of the tree and capture whatever happens to be convenient / necessary for reasonable calibration of the points. Unfortunately, my trigonometric knowledge has been steadily gathering dust for 30 years, and something in my gut is mumbling that there's not enough data to solve this; that a hack like forcing the rotation to be right-angles is required. Am I barking up the wrong (Christmas) tree? Or just plain barking? I look forward to any (LED) light that you can shed! Alright, that's enough puns ...","['trigonometry', 'circles']"
4816218,Continuous and increasing in every variable does not imply continuous?,"Let $f:\mathbb R^2\to \mathbb R$ $z=f(x,y)$ $f$ is continuous and strictly increasing in both $x,y$ . It is known that continuous in each linear directions does not imply continuity. But the examples that I found all involve non-monotonic functions. $$f(x,y)=\begin{cases}\frac{xy}{x^2+y^2},&(x,y)\neq(0,0)\\
0,&(x,y)=(0,0)\\
\end{cases}$$ I wonder if continuous and increasing in every variable implies continuous? I think the answer is no","['examples-counterexamples', 'real-analysis', 'continuity', 'multivariable-calculus', 'functions']"
4816225,Trouble understanding order statistics,"Order statistics were introduced in my text as follows: I am trying to understand what this means. $X_1 , \dots , X_n$ is a random sample, i.e. an independent and identically distributed sequence of random variables. $Y_1$ is the ""smallest"" of these $X_i$ . Do I interpret this as $Y_1 = \min \{ X_1 , \dots , X_n \}$ ? Then $Y_2$ is the second smallest of the $X_i$ when realized, $Y_3$ the third smallest, and so on until $Y_n = \max \{ X_1 , \dots , X_n \}$ ? An example to illustrate my thinking: Suppose we perform an experiment and the random variables $X_i$ are realized as $x_i = i$ (so $x_1 = 1, x_2 = 2$ etc..). Then the random variables $Y_i$ are realized as $y_i = i$ as well because the $x_i$ are already in order. If we perform another experiment and the $X_i$ are realized as $x_i = -i$ (so $x_1 = -1, x_2 = -2$ etc..), then this time the variables $Y_i$ were realized as $y_i = -n + i - 1$ (so $y_1 = -n, y_2 = -n + 1, \dots , y_n = -1$ ). Is this the correction interpretation? My confusion/hesitation comes from the fact that these $X_i, Y_i$ are random variables and therefore functions instead of values and it's not really clear from the text what is meant by ordering them.","['statistics', 'probability-distributions', 'order-statistics', 'probability-theory', 'probability']"
4816232,Is there a topology on the reals such that addition is continuous but not multiplication? What about conversely?,"Is there a topology $T$ on the set of real numbers $\mathbb{R}$ such that the binary operation of addition is continuous under $T$ , but the binary operation of multiplication is not continuous under $T$ ? And, what about conversely? That is, is there a topology on the set of real numbers such that multiplication is continuous but addition is not continuous?","['real-numbers', 'continuity', 'general-topology']"
4816261,IMO 1987/P1 - Combinatoric approach,"I was was solving IMO 1987, Problem 1 and also found the first solution. However, I also tried a combinatorics approach but couldn't find any valid argument. My argument was as follows: Consider each $p_n(k)$ seperately. There are $n \choose k$ possibilities to choose $k$ fixed points and $n-k$ positions that should not contain fixed points. Now filling the gaps from begin to end, the first non-fix point position has $n-k-1$ possibilities ( $k$ taken + 1 is not allowed). The second position has $n - k - 2$ by the same logic, etc. In total we get: $$
p_n(k) = {n \choose k} (n-k-1)! 
$$ Now, for $k = n-1$ we automatically get $n$ fixed points by the pigeonhole principle. Therefore, we can exclude $k-1$ from our answer: $$
\sum_{k=0}^n k p_n(k) = \sum_{k=1}^{n} k p_n(k) = \left( \sum_{k=1}^{n-2} kp_n(k) \right) + n = n + \sum_{k=1}^{n-2} k{n \choose k} (n-k-1)!
$$ I then tried to show that $$
\sum_{k=1}^{n-2} k {n \choose k} (n-k-1)! = n! - n
$$ via induction but it doesn't work. For $n = 3,4$ the formula gives the correct result. Could anyone point out where the flaw in my logic is and maybe share a valid solution based on combinatorics.","['permutations', 'contest-math', 'fixed-points', 'combinatorics', 'problem-solving']"
4816282,There exists a function that satisfies $\sum_{n=1}^\infty |f^{[n]}(x) - f^{[n]}(y)| < \infty$ but is not a contraction?,"There exists a function $f: \mathbb{R} \to \mathbb{R}$ that satisfies $$
\sum_{n=1}^{\infty} |f^{[n]}(x) - f^{[n]}(y)| < \infty \quad \forall x,y \in \mathbb{R}
$$ where $f^{[n]}(x) = f(f(f...(f(x)))$ iterated $n$ times, but there is no $\lambda < 1$ such that $|f(x) - f(y)| < \lambda |x - y|$ .
I think that a function that is not a contraction but $f(f(x))$ is, satisties my problem, but how to prove it? And how I find such function? OBS: I prove that a function that satisfies this property has a unique fixed point","['dynamical-systems', 'functions', 'fixed-point-theorems', 'real-analysis']"
4816298,"$f,g$ are real-valued, $f(x,y)=g(h_1(x),h_2(y))$, $f,h_i$ are continuous, $g$ is increasing, does $g$ must be continuous?","Assumptions: $f,g$ are real-valued. $g:[0,1]^2\to\mathbb R$ . Functions $h_1:X\to\mathbb [0,1]$ and $h_2:Y\to\mathbb [0,1]$ are surjective continuous. $X,Y$ are connected separable. $f$ is continuous, $f(x,y)=g(h_1(x),h_2(y))$ . Questions: Does the strict increasingness of $g$ in all variables imply the continuity of $g$ ? Backgrounds : It is known the composite of continuous functions is continuous. What about the other way around? What we know: even without increasingness, $g$ is continuous if one of the followings hold: X, Y are path-connected X,Y are compact Hausdorff $h_i$ are homomorphisms $h_i$ are quotient maps To prove the claim, we only need to show that if $g$ is continuous restricted to one direction, then $g$ is continuous. Also, since $g$ is increasing, the restricted function of $g$ can only contain a jump discontinuity. (If we allow essential discontinuity, a simple counterexample exists , this is why the increasingness is powerful here). I've been trying to construct a counterexample with jump discontinuity for a while, with no luck.","['multivariable-calculus', 'general-topology', 'differential-topology', 'real-analysis']"
4816309,Ricci Flow: The existence of potential of Curvature,"For a compact Riemannian Manifold $(M,g)$ without boundary. $R$ as the scalar curvature. And $d\mu$ is the Riemannian volume form. So we can define the average of the scalar curvature $r:=  \frac{\int_{M} R d\mu}{\int_{M}  d\mu}$ When consider the equation $$\Delta f = R-r$$ I was told that $f$ must exist due to the fact that $$\int_M R-r d\mu =0  \quad (1)$$ But I cant prove this as true. I current idea is try to use the Elliptic PDE theory, but of what from the theory? I think Fredholm Theory is a choice but I don't know how to connect the fact (1) to there. Any help will be appreciated .","['riemannian-geometry', 'ricci-flow', 'elliptic-operators', 'partial-differential-equations', 'differential-geometry']"
4816315,"Define $f(0,0)$ so that $f'_x(0,0)$ exists","I am stuck with this problem and do not know how to proceed. I have been dealing with Discrete Math for years and forgot many things about Calculus. Consider the function $f:\mathbb{R}\setminus \{(0,0)\}\to \mathbb{R}$ such that $$
f(x,y) = \frac{e^{x+y}-x-y-1}{x^2+y^2}. 
$$ Determine $f(0,0)$ such that $f'_x(0,0)$ exists and also determine $f'_x(0,0)$ . What I have tried: We know that $$
f'_x(0,0) = \lim_{h\to 0} \frac{f(0+h,0)-f(0,0)}{h}.
$$ Firstly I tried to calculate: $$
\lim_{h\to 0} \frac{f(0+h,0)}{h} 
= \lim_{h\to 0} \frac{e^h-h-1}{h^3}
$$ but the limit does not exist. So I do not think that it is possible to give any value to $f(0,0)$ such that $f'_x(0,0)$ exists. Am I missing something? Any help is appreciated.","['multivariable-calculus', 'calculus', 'partial-derivative', 'limits', 'derivatives']"
4816336,How many connected nonisomorphic graphs of N vertices given certain edge constraints?,"Background: Iâm helping a colleague with a theoretical problem in ecology, and I havenât quite the background to solve this myself. However, I can state the problem clearly, I think: Problem statement: Weâd like to enumerate and count (somehow) the number of connected, nonisomorphic graphs of $N$ vertices, where edges are categorized into three types, each representing a specific ecological interaction: Green represents a bidirectional Mutualism , where any pair of vertices may be connected and the order of vertices does not matter. Thus, $A \leftrightarrow B$ is always equivalent to $B \leftrightarrow A$ . Blue represents a directional Competition , with edges capable of pointing either way between any pair of vertices. Here, the order of vertices matters, and $A \rightarrow B$ is always equivalent to $B \leftarrow A$ , but sometimes $A \rightarrow B$ is equivalent to $A \leftarrow B$ under an isomorphism between two graphs. Red represents a directional Predation , with edges capable of pointing either way between any pair of vertices, exactly similar to Blue. Examples of isomorphisms for $N=3$ : $A \mathop{\longrightarrow}\limits^{\text{Red}} B \mathop{\longrightarrow}\limits^{\text{Red}} C \enspace\equiv\enspace A \mathop{\longleftarrow}\limits^{\text{Red}} B \mathop{\longleftarrow}\limits^{\text{Red}} C$ $A \mathop{\longrightarrow}\limits^{\text{Blue}} B \mathop{\longrightarrow}\limits^{\text{Blue}} C \mathop{\longrightarrow}\limits^{\text{Blue}} A \enspace\equiv\enspace A \mathop{\longleftarrow}\limits^{\text{Blue}} B \mathop{\longleftarrow}\limits^{\text{Blue}} C \mathop{\longleftarrow}\limits^{\text{Blue}} A$ $A \mathop{\longrightarrow}\limits^{\text{Blue}} B \mathop{\longleftarrow}\limits^{\text{Blue}} C \mathop{\longleftarrow}\limits^{\text{Blue}} A \enspace\equiv\enspace A \mathop{\longleftarrow}\limits^{\text{Blue}} B \mathop{\longrightarrow}\limits^{\text{Blue}} C \mathop{\longrightarrow}\limits^{\text{Blue}} A$ $A \mathop{\longrightarrow}\limits^{\text{Blue}} B \mathop{\longrightarrow}\limits^{\text{Red}} C \mathop{\longleftrightarrow}\limits^{\text{Green}} A \enspace\equiv\enspace A \mathop{\longleftarrow}\limits^{\text{Red}} B \mathop{\longleftarrow}\limits^{\text{Blue}} C \mathop{\longleftrightarrow}\limits^{\text{Green}} A$ Known answers for trivial values of N: For $N=1$ , there are 0 graphs satisfying the criteria, because there are no interspecies interactions, and the only graph is also unconnected. For $N=2$ , there are 3 graphs satisfying the criteria, one for each of the colored edges connecting the two vertices. Note that there arenât 5, because 2 pairs of graphs are isomorphisms. ( $A \rightarrow B$ is an isomorphism of $A \leftarrow B$ for each color Red and Blue.) For $N=3$ , it gets a complicated fast, but it is known that there are 40 graphs satisfying the criteria. This has been verified both by hand enumeration with clear plastic cutouts and by a computer program to examine all candidates and exclude isomorphisms uncovered by simple flips and rotations. Beyond that, the problem becomes difficult to think about visually in a way that guarantees that all isomorphisms are found and eliminated. Even identifying all possible flips, twists, exchanges, and rotations of the graphs with $N=4$ is a bit of a challenge to get right in a hand-written program. I suspect a more general approach using graph theory, group theory, and/or combinatorics will lead to answers much more readily. Objectives: Primary Goal: Weâd like to establish some formula (not necessarily in closed form) for computing the count, and be able to evaluate the formula for values of $N$ up to, say, 10, if possible. This might be accomplished with a computer program written in Mathematica or Haskell. Secondary Goal: Weâd like to enumerate all connected non-isomorphic graphs with $N$ vertices (under the Green/Blue/Red edge conditions). Ideally, this would be an oriented incidence matrix that we could transform into input for Graphviz. This mathematical exploration aims to contribute insights into the dynamics of multi-species interactions in ecological systems. Advice from experts in graph theory, combinatorics, and group theory on how to approach this problem would be very helpful. Your guidance on tackling this mathematical challenge, along with strategies for creating a software program to achieve the primary or secondary goal, would provide valuable perspectives on ecological interaction modeling. Upper bound: We know trivially that the total count is bounded by $6^{T(N)}$ , where $T(N)={N(N-1)\over 2}$ , i.e., the $N$ th triangular number or the number of edges in a graph of $N$ vertices. The base of 6 here is due to there being six possible edge states: None, Green, Blue forward, Blue backward, Red forward, and Red backward. That part is trivial and easy; the hard part is excluding isomorphic redundancies, of which there are many. For example, for $N=3$ , the set of candidates is initially 216, but is quickly whittled down to 40 after removing non-connected graphs and coalescing isomorphisms.","['graph-isomorphism', 'graph-theory', 'number-theory', 'combinatorics', 'group-theory']"
4816354,Is a hollow sphere topologically equivalent to a torus?,"I'm not very well-versed in topology, but I know the basic concept of topological equivalence can be approximated by ""counting the holes"" in objects, in the sense that a sphere is different from a torus, but a torus is the same as a coffee mug because they have the same number of holes. I know this roughly approximates being able to smoothly deform one object into another. Can this be applied to interior ""holes"" as well, such as a hollowed-out sphere? Does the hollow center count as a ""hole""? Is there a way to smoothly deform a hollow sphere into a torus or a sphere? If not, is a hollow sphere in a different topological equivalence class from both a sphere and a torus?",['general-topology']
4816388,Simulating Random Multiples,"I was running a statistical simulation, where I'd generate random numbers from 1 to $N$ , where $N$ varies from 1 to $10^6$ . I would keep generating random numbers until the current random number was a multiple of one of the previous numbers. For example: If $N = 10$ $5, 8, 6, 7, 10$ -> We stop at 10 because it's a multiple of 5. Total count is 5 $10, 2, 4$ -> We stop at 4 because it's a multiple of 2. Total count is 3 For each $N$ , I ran 1000 trials and averaged the total count for each range. I came up with the following graph, which is almost like a square root equation: Why does it start to converge to what seems like some constant * $\sqrt{N}$ ? Does anyone have a mathematical explanation for this trend occurring? Upon using a fit line of y = Ax^0.5, I have A to be 0.4486 and a correlation of 0.9970. Edit: I coded this in C++ here for any coders out there.","['number-theory', 'statistics', 'simulation']"
4816410,Convergence of the sum of exponential random variables,"Let $\{X_n\}$ be a sequence of nonnegative random variables such that for each n, $X_n$ has density $\lambda_n e^{-\lambda_n x}$ for $x \geq 0$ and $\lambda_n > 0$ i) Show that if $\sum_{n=1}^\infty 1 /\lambda_n < \infty$ then $\sum_{n=1}^\infty X_n < \infty$ almost surely ii) If the $\{X_n\}$ are independent, show that $\sum_{n=1}^\infty 1/\lambda_n < \infty$ if and only if $\sum_{n=1}^\infty X_n < \infty$ almost surely I proved the first implication ( $\Rightarrow$ ) of ii) using the three series of Kolmogorov, but I can't figure out the converse. Also, I have no idea how to solve i) without independence (since my only idea was to use the three series of Kolmogorov). This is exercise 11.29 on Leadbetter, Cambanis a basic course in Measure and Probability . I would appreciate any hint or help EDIT: I think I figured the solution out, this is my attempt: $(\Leftarrow$ Let $X_n'$ be equal to $X_n$ when $X_n \leq 1$ and $0$ otherwise.
Then we have that $P(X_n > 1) = \int_1^\infty \lambda_n e^{-\lambda_nx}dx = e^{-\lambda_n}$ But by Three series of Kolmogorov (since $\sum X_n$ converges a.s.), it results that $\sum e^{-\lambda_n} < \infty$ which implies that $\lambda_n \to \infty$ , hence $1/\lambda_n \to 0$ Now, considering the expectation of $X_n'$ we get that \begin{equation*}
    EX_n' = - e^{-\lambda_n} - \frac{1}{\lambda_n}e^{-\lambda_n} + \frac{1} 
    {\lambda_n}
\end{equation*} Again, by Three series of Kolmogorov, \begin{equation*}
   -\sum e^{-\lambda_n} + \sum \frac{1}{\lambda_n} \left( 1 - e^{-\lambda_n} 
   \right) < \infty
\end{equation*} But since $\sum e^{-\lambda_n} < \infty$ and $1/\lambda_n \to 0$ we conclude that \begin{equation*}
   \sum \frac{1}{\lambda_n} < \infty
\end{equation*} Is this correct?","['probability-theory', 'probability', 'sequences-and-series']"
4816419,Proving Euler's formula,"I'm considering a new proof of Euler's formula, but I'm not confident if my method works. If $f(x+iy)= \cos(x)+i \sin(x)$ , then we have $f_x/f=1$ . Does it follow that $f(x+iy)=C \exp(x+ix)$ ? Since $f\overline{f}=1$ , we could infer that $f(x+iy)= \exp(ix)$ . EDIT: It was a typo to write $f_x/f=1$ . Of course, $f_x/f=i$ and thus we just consider a function of $x$ .","['complex-analysis', 'solution-verification', 'complex-numbers']"
4816458,How to show that is unique asymptotic stable,"Based on this question: PoincarÃ©-Bendixon show periodic solutions. Show that the system $x^{'}=x-y-x^{3}$ , $y^{'}=x+y-y^{3}$ has a unique periodic orbit on annulus $A:=\{(x,y): 1\le x^2+y^2\le 2\}$ and this periodic solution is asymptotic stable. I consider function $V(x,y)=(x^2+y^2)/2$ and show that $\dot V(x,y)>0$ on the circle $x^2+y^2=1$ and $\dot V\le 0$ on $x^2+y^2=2$ . So $A$ is positively invariant. By Poincare-Bendixson theorem, there is at least one periodic orbit in $A$ . Question: But how to apply one theorem to show that is unique asymptotic stable? I have the following theorem: Let $p(t)$ be a $T$ -periodic orbit of our system. If $\int_0^T div(f(p(t))dt<0$ , then $p(t)$ is orbitally asymptotic stable. I am confused how to find our $p(t)$ ? Use the polar coordinate transform $$
%
\begin{align}
%
 x &= r \cos \theta \\
%
 y &= r \sin \theta \\
%
\end{align}
%
$$ which implies $$
  r^{2} = x^{2} + y^{2}
\tag{2}
$$ Compute the derivative with respect to time for $(2)$ and use the definitions in $(1)$ . This leads to $$
 \dot{r} = r - r^{3} \left( \cos^{4} \theta + \sin^{4} \theta \right) 
= r 
\left( 
 1 - \frac{1}{4} \left( 3 + \cos 4 \theta \right) r^{2} 
\right)
\tag{3}
$$","['ordinary-differential-equations', 'dynamical-systems']"
4816464,Algebraic vector bundles which are analytically but not algebraically isomorphic,"I am looking for an example of two algebraic vector bundles on an algebraic complex manifold / smooth complex algebraic variety which are analytically isomorphic, but not algebraically isomorphic. By Serre's GAGA theorem such examples can only exist if the manifold is not projective. I would expect that such examples exist, but I haven't been able to find a reference.","['complex-geometry', 'vector-bundles', 'algebraic-geometry', 'holomorphic-bundles']"
4816476,Asymptotic of $\sum_{k=0}^\infty\frac{\Gamma (k+n+1) \Gamma (3 k+n+1)}{\Gamma (k+2) \Gamma (3 k+2 n+2)}$ as $n\to \infty$,"Let $a,b,c,d$ be fixed and in a small neighborhood of $0$ , so the series $$f(n) = \sum_{k=0}^\infty\frac{\Gamma (a+k+n+1) \Gamma (b+3 k+n+1)}{\Gamma (c+k+2) \Gamma (d+3 k+2 n+2)}$$ converges, the title is special case $a=b=c=d=0$ . Question: What is the leading asymptotic of $f(n)$ as $n\to \infty$ ,
with general $a,b,c,d$ near $0$ . The expression of $f(n)$ is a hypergeometric series and can be converted into an integral form, then a similar analysis like here should be feasible. But this approach is quite complicated, I'm looking for alternative solutions. Thank you very much.","['asymptotics', 'real-analysis', 'sequences-and-series', 'limits', 'hypergeometric-function']"
4816477,Can we say that $x=0$ is a double root of $f(x) = (e^x-1)(\ln( x+1))$?,"Let $$f(x) = (e^x-1)(\ln( x+1))$$ So far , I've only seen examples of textbooks referencing repeated roots if we can write the function with linear factors raised to some natural exponent ( like if $a$ is a repeated root , then we can write the function as $(x-a)^{m+1}g(x), m\in\mathbb N$ ). But in above example, the factors are not linear polynomials. So ,
Is it correct to say that $x=0$ is a repeated root of $f$ or not? Context: I'm confirming the terminology because currently I'm studying derivatives. My teacher said that functions having repeated roots are always differentiable at that point with derivative equal to $0$ . That's why I asked question whether above type of functions come into that category so that the property of differentiability could be extended.","['roots', 'calculus', 'derivatives', 'soft-question', 'terminology']"
4816503,Yule process intuitive question,"I have a Yule process with $n$ individuals. There is no death, so the death rate is $\mu_n$ $=$ $0$ for all $n$ . Each individual gives birth to a new individual independently after waiting for $\text{Exponential}(\lambda)$ amount of time. So the birth rate is $\lambda_n = n\lambda$ for all $n \ge 1$ . I get this by considering the minimum waiting time, as there are $n$ waiting times here, each distributed as $\text{Exponential}(\lambda)$ . Now, my question is, in this process, does the population become infinite in a finite amount of time?  Is that possible, and if so, how should I think about it? Any hints or advice will be very helpful. Thank you so much!","['exponential-distribution', 'stochastic-processes', 'probability-theory', 'random-variables']"
4816564,Different dot product values,"I came across the following problem while studying geometry. Given $v_1, ..., v_7 \in \mathbb{R}^3$ with no three vectors in the same plane. Show that among the dot products $v_i \cdot v_j, i \neq j$ there are at least three different values. I managed to find a counterexample for six vectors but I'm not sure how to prove it for 7 vectors. Any help is appreciated.","['inner-products', 'euclidean-geometry', 'geometry']"
4816573,Strong Law of Large Numbers for increasing index sets,"Let $(X_n)_{n \in \mathbb{N}}$ be a sequence of integrable i.i.d random variables. Let $I_n \subset \mathbb{N}$ satisfy $\lim_{n \to \infty} |I_n| = \infty$ , where $|I_n|$ is the cardinality of $I_n$ . Is it true without further hypotheses that $$ \lim_{n \to \infty} \frac{1}{|I_n|} \sum_{k \in I_n} X_k = \mathbb{E}[X_1] \quad \text{a.s.}$$ You can assume that the $I_n$ are disjoint and the cardinalities are strictly increasing for simplicity.","['law-of-large-numbers', 'probability-theory']"
4816620,"Given $\log2=0.3010300$, $\log3=0.4771213$, $\log7=0.8450980$, find $\log0.3048$","Given $\log2=0.3010300$ , $\log3=0.4771213$ , $\log7=0.8450980$ , find $\log0.3048$ $\log0.3048=\log\left(\dfrac{3048}{10000}\right)=\log\left(\dfrac{2^3\cdot3\cdot127}{10^4}\right)$ $\Rightarrow 3\log2+\log3+\log127-4\log10$ Problem is I don't know what $\log127$ is. So there must be another approach but I don't see how else to factorize $0.3048$ .",['algebra-precalculus']
4816641,Functional equation: $f(z) = \sum_{n=0}^{\infty} f(n)^2 z^n$?,"Im looking for functions $f(z)$ such that $f(z) = \sum_{n=0}^{\infty} f(n)^2 z^n = f(0)^2 + f(1)^2 z + f(2)^2 z^2 + f(3)^2 z^3 + ...$ and $f(n)$ are all real. And I wonder how fast $f(n)$ grows. I had this question in my head for a long time but I got reminded by these Function $f(x)$, such that $\sum_{n=0}^{\infty} f(n) x^n = f(x)$ Does there exist a function that generates itself? and I am grateful to them. edit PROOF BY CONTRADICTION : infinite radius is not possible : Note $f(z)$ can not be a nonconstant polynomial.
This also implies that : $f(n) = 0 $ for all $n > N$ for some $N$ , cannot be true unless $f(z)=0$ . Also notice that the radius must be $\infty$ since $f(n)$ goes to infinity. So apart from the trivial constant functions, we are dealing with real entire transcendental functions. That implies that $f(n)^2$ converges to $0$ faster than exponential.
So it might make more sense to speak about how fast $\frac{1}{f(n)}$ grows. EVEN STRONGER : One can show that most $f(n)$ are not $0$ ; the tail of $f(n)$ must have no zero values, since all taylor coefficients are positive. And finally realize that therefore $f(1),f(2)$ must be nonzero, we can show that the start also can contain no zero's. So we end up with concluding that only $f(0)$ can be zero, what makes perfect sense since the function is strictly increasing on the positive reals. But now I run into a problem. Since $f(z)$ is strictly increasing , so is $f(n)$ . But strictly increasing $f(n)$ implies $0 < C < f(n)^2$ for some constant $C$ . Hence the radius is not infinite unless $f(z)$ is a constant !!! So we end up studying functions with finite radius, than can somehow get the real values $f(n)$ .
Analytic continuation around a pole or such might work.
Even lacunary series than can be extended might work. So the situation is not so clear and simple.","['functional-equations', 'complex-analysis', 'taylor-expansion', 'generating-functions', 'sequences-and-series']"
4816663,Find $\lim_{n\to\infty}\left(\int _{0}^{n}\frac{\mathrm dx}{x^{x^n}}\right)^n$,"I had to solve the following limit : $$\lim_{n\to\infty}\left(\int _{0}^{n} \frac{\mathrm dx}{x^{x^n}}\right)^n$$ So far I've made no progress and the integral looks unsolvable. With online calculators, it seems like the inner integral approaches to $1$ , yielding the indeterminate form of $1^{\infty}$ . How can I approach to solve it?","['integration', 'limits', 'calculus', 'real-analysis']"
4816672,Prove that the 3 lines are concurrent,"Let triangle $ABC$ be such that $AB<AC$ , with circumcenter $O$ , incenter $I$ ; let $M$ be the midpoint of $BC$ , $N$ the midpoint of arc $BC$ of the circumcirle of $ABC$ containing $A$ . The circumcircles of triangles $IAN$ and $IBM$ intersect at a point $K$ other than $I$ . $BK$ intersects $AC$ at $X$ , $NK$ intersects $AI$ at $Y$ . Prove that $BI$ , $AN$ , $XY$ are concurrent. (Diagram in Geogebra) I tried but couldn't find a bright direction for this problem, someone help me, give me a suggestion, thank you very much. I think that if we call D the intersection of AN and BI we can prove that X,Y,D are collinear. Let F be the intersection of AC and BD, then (AF,ID)=-1. We will try Menelaus proof for triangle AIF.","['euclidean-geometry', 'geometry']"
4816817,"Summing a nonstandard sequence, closed form of $S_n(x) = \sum_{i=1}^n x^{c^{i-1}}$","Arithmetic sequences have a common difference, where you add a constant to each term to get the next. Geometric sequences have a common ratio, where you multiply a constant to each term to get the next. Each one of these sort of sequences have a summation formula where you can sum the first $n$ terms without generating them. For an arithmetic sequence $(a_i)_{i=1}^n$ , we have $$\begin{align}
a+(a+d)+(a+2d)+\cdots+(a+(n-1)d) &= na+(1+2+\cdots+(n-1))d \\
&=na+\frac{(n-1)(n)}{2}d.
\end{align}$$ For a geometric sequence $(a_i)_{i=1}^n$ , we have $$\begin{align}
a+ra+r^2a+\cdots+r^{n-1}a &= s \\
ra+r^2a+r^3a+\cdots+r^{n}a &= rs\\
\phantom{d}&\\
\mbox{subtracting line 2 from line 1 gives us:}&\\
\\
r^na -a&= rs-s\\
(r^n-1)a &= (r-1)s\\
\\
\mbox{so, we get:}\\
\\
s &= \frac{r^n-1}{r-1}a
\end{align}$$ What if we take this to the next natural step. Instead of adding a constant to get the next term, as in an arithmetic sequence, or multiplying a constant to get the next term, as in a geometric sequence, you instead raise each term to a constant power to get the next term. So, you would have $$a,a^e,(a^e)^e,((a^e)^e)^e,\dots$$ or, more elegantly, $$a,a^e,a^{e^2},a^{e^3},\dots,a^{e^n}$$ for some constant $e$ . I feel like this would be called an exponential sequence, but that seems to be another name for a geometric sequence from google searches. I was playing with trying to make a summation formula for the first $n$ terms of such a sequence, and quickly realized I was stuck. I am sure others have played with such a sequence before, but I couldn't find anything online for similar sequences. Does anyone have any ideas for how to proceed with generating a summation formula? Note that I am not asking what the formula is - I want to derive it myself (this is just for fun), but I cannot find a good starting point. Thanks for any thoughts you all may have! -Andrew Edit: A user named Doug supplied a useful starting point on a comment that has since been deleted, suggesting an approach using partial sums. I wasn't able to look at it until late last night, and I spent a few hours working on it before I had to go to bed. Unfortunately, I got stuck in the weeds of the calculus on their approach, and since it is now gone I can't continue working on it. But, the starting point is sound I think. Note that I will be using $c$ as the constant power, instead of $e$ , because as users have pointed out, $e$ was a bad choice (Euler's constant). If we think of the sum as a finite power series $S_n(x) = \sum_{i=1}^n x^{c^{i-1}}$ , the question becomes finding a closed form of that finite power series. We notice that $$S_n(x^c)-S_n(x)=x^{c^n}-x$$ This gives us a starting point similar to that in finding the summation formula of a geometric series. Unfortunately, the left-hand side doesn't lend itself well to being written as a multiple of $S_n(x)$ . I have tried some tricks, seeing if I could take derivatives and end up with a solvable ODE, but I haven't had much luck. I am going to keep thinking about this, but if anyone browsing the forum has any ideas, please feel free to share!","['power-series', 'summation', 'sequences-and-series']"
4816860,Gaussian approximation of collision time,"In this answer there is a claim that $$\frac{n!}{(n-k)! n^k} \approx e^{-\frac{k^2}{2n}} \tag{1}$$ which is then used to approximate the sum over $k=1,\ldots, n$ via $$\sum_{k=1}^n \frac{n!}{(n-k)! n^k} \approx \sum_{k=1}^n e^{-\frac{k^2}{2n}} \approx \int_0^n e^{-\frac{t^2}{2n}} \, dt \approx \sqrt{\pi n /2}.$$ How is the approximation (1) obtained? I attempted to use Stirling's inequality and some hand-wavey approximations: $$\frac{n!}{(n-k)! n^k} \sim \frac{n^{n+1/2}/e^n}{((n-k)^{n-k+1/2}/e^{n-k}) \cdot n^k} = e^{-k}\left(1-\frac{k}{n}\right)^{-(n-k+1/2)}
\sim e^{-k} e^{\frac{n-k+1/2}{n/k}}
= e^{-\frac{k^2}{n}+\frac{k}{2n}}.$$ Besides getting the wrong asymptotics, I also recognize that this approximation only works well when $k$ is small compared to $n$ , but in the above application we are considering $k$ close to $n$ as well.","['binomial-coefficients', 'combinatorics', 'stirling-numbers', 'probability']"
4816868,Is the tensor product of W*-algebras commutative?,"According to definition 1.22.10 in Sakai's book if $\mathscr{N}$ and $\mathscr{M}$ are two W $^*$ -algebras its tensor product is defined by \begin{equation}
\mathscr{N} \overline{\otimes}\mathscr{M} := (\mathscr{N}_* \otimes_{min^*} \mathscr{M}_*)^*,
\end{equation} where $\text{min}^*$ is the dual projective norm and $\mathscr{N}_*$ , $\mathscr{M}_*$ are the corresponding predual spaces of $\mathscr{N}$ and $\mathscr{M}$ . What I would like to know if it is true that $\mathscr{N} \overline{\otimes}\mathscr{M} \cong \mathscr{M} \overline{\otimes}\mathscr{N}$ for this particular norm, that I will call W $^*$ -norm, . According to proposition II.9.2.6 in Blackadar's book when $\mathscr{N}$ and $\mathscr{M}$ are C $^*$ -algebras it is true that \begin{equation}
\mathscr{N} {\otimes}_{min \\ max} \mathscr{M} \cong \mathscr{M} {\otimes}_{min \\ max} \mathscr{N},
\end{equation} however, he does not prove this result and a similar property for the case of W $^*$ -algebras is not discussed. In order to prove this isomorphism of W $^*$ -algebras I started by defining the map \begin{equation}
s: \mathscr{N} \odot \mathscr{M} \to \mathscr{M} \odot \mathscr{N},  \sum_i n_i \odot m_i  \mapsto  \sum_i m_i \odot n_i, 
\end{equation} where $\odot$ denotes the algebraic tensor product; this map has an inverse given by \begin{equation}
s': \mathscr{M} \odot \mathscr{N} \to \mathscr{N} \odot \mathscr{M},  \sum_i m_i \odot n_i  \mapsto  \sum_i n_i \odot m_i. 
\end{equation} Now, the idea will be to prove that the map $s$ can be extended to an unique norm-preserving map in $\mathcal{B}(\mathscr{N} \overline{\otimes}\mathscr{M}, \mathscr{M} \overline{\otimes}\mathscr{N})$ ; for that I should prove that $s: \mathscr{N} \otimes_\alpha \mathscr{M} \to \mathscr{M} \overline{\otimes} \mathscr{N}$ is bounded where, $\otimes_\alpha$ is the W $^*$ -norm. Then, if the same is done for the map $s': \mathscr{M} \otimes_\alpha \mathscr{N} \to \mathscr{N} \overline{\otimes} \mathscr{M}$ then, I am almost done. However, I am failing to prove that the map $s$ is bounded. One thing I do know is that since $\alpha$ is a cross norm then $||s(m\otimes n)|| = ||m \otimes n||$ but this only works for simple tensors. So, I was wondering if there is any way I can bound these maps so they can be extended? Or perhaps, if there is another way to proceed to prove the isomorphism by using Sakai's language. Thanks in advance.","['von-neumann-algebras', 'banach-spaces', 'operator-algebras', 'tensor-products', 'functional-analysis']"
4816958,Show that $ I = \int_0^1 \frac{dx}{\sqrt{\sin(2x) + \sin(x) + 3}} < \frac{1}{2} $?,"How to efficiently show that $$ I = \int_0^1 \frac{dx}{\sqrt{\sin(2x) + \sin(x) + 3}} < \frac{1}{2} $$ ? What I tried : $$\sin(2x) + \sin(x) + 3 =
2 \cos(x)\sin(x) + \sin(x) + 3 =(2 \cos(x) + 1)\sin(x) + 3$$ and that is about $$(2(1 - x^2/2 + x^4/24 - O(x^6) ) + 1)(x - x^3/6 + x^5/120 - O(x^7)) + 3$$ and more importantly, it is above $$(2(1 - x^2/2 + x^4/24 -1/720 ) + 1)(x - x^3/6 + x^5/120 -1/5040) + 3$$ and therefore we compute (in closed form or numerical method) $$ \int_0^1 \frac{dx}{((2(1 - x^2/2 + x^4/24 -1/720 ) + 1)(x - x^3/6 + x^5/120 -1/5040) + 3} $$ what is about $0.24..$ Now including the $\sqrt *$ should be about twice that, what makes about $0.49..$ . (edit : to be more precise : Jensen's inequality gives here $\int \sqrt{f(x)} dx < \sqrt \int f(x) dx$ , so $\sqrt 0.24.. = 0.49... < \frac{1}{2}$ see : https://en.wikipedia.org/wiki/Jensen%27s_inequality ) But this gets complicated, I want an efficient and convincing method. But lets continue by truncating further : $$ \int_0^1 \frac{dx}{\sqrt{3x + 3}} = \frac{2(\sqrt 2 - 1)}{\sqrt 3}$$ this is about $0.478$ , however this requires $\sqrt 2,\sqrt 3$ and is in fact a lower bound, not an upper bound. I tried a truncated product as well, but also without any simple ways to success. Then I gave up. Edit : I explained how I used https://en.wikipedia.org/wiki/Jensen%27s_inequality and thus the answer of Zarrax is most appealing to me for now.","['inequality', 'definite-integrals', 'real-analysis', 'calculus', 'numerical-methods']"
4816992,Taylor series at singular points in algebraic geometry,"There is a notion of a Taylor Series for regular functions on an affine variety X, given a set of generators $\{u_1,\cdots,u_n\} $ of the maximal ideal $\mathfrak{m}$ at a point $P$ that form a basis for the cotangent space $\mathfrak{m}/\mathfrak{m}^2$ we define the Taylor series of a function f as a formal series $F\in k[[T_1,\cdots,T_n]]$ such that the difference between the function and the truncated series lies in powers of the maximal ideal. Shafarevich in Basic Algebraic Geometry I shows that any function in the local ring has a Taylor series, and if the point is non-singular then this Taylor series is unique. He then asks as a exercise in the same chapter to show that, if the point is singular, then the Taylor series is never unique, i.e, there is an infinite number of Taylor series for any given function $f$ . I've solved the problem in the case that X is a hiper-surface simply by considering the generating polynomial as a power series, then tried to prove the result by induction on the codimension with no success. How would one begin to show this result for arbitrary affine varieties?",['algebraic-geometry']
4816994,How many 7-digit numbers with distinct digits can be made that are divisible by 3?,"How many 7-digit numbers with distinct digits can be made that are divisible by 3?
First of all, I counted all the ways to insert 7 of 10 digits in a number making the number divisible by 3.
Digits that can be left:
a)012, 015, 018, 024, 027, 036, 039, 045, 048, 057, 069, 078 12 ways ;
b)123, 126, 129, 135, 138, 147, 156, 159, 168, 189 10 ways ;
c)234, 237, 246, 249, 258, 267, 279 7 ways ;
345, 348, 357, 369, 378 5 ways ;
456, 459, 468, 489 4 ways ;
567, 579 2 ways ;
678 1 way ;
789 1 way . In case a), all the remaining 7 digits are no 0, so I counted: 7! x 12 = 60480 numbers
In all the other cases (30), from 7! I subtracted the cases in which the number begin with 0 (6! cases).
So, 30 x (7! - 6!) = 30 x (5040 - 720) = 4320 x 30 = 129600 numbers.
In total I counted 190080 numbers with distinct digits that are divisible by 3 .
Is this process correct? This problem is the number 23 of ""Guts February 2000 HMMT"". It doesn't specify if the leftmost digit is permitted to be 0; I supposed that a number can't begin with a ""0"". The reason I wrote this problem on this forum is that the answer posted in the ""answer sheet "" of that competition is 224640. I tried hard, also considering ""0"" as the leftmost digit, but I hadn't been able to find this number. I wanted to verify with other people if it could be a mistake in the answer sheet. I thank in advance those who want to solve this problem.","['combinatorics', 'divisibility']"
4817011,Can a birational morphism contract a positive ray?,"This question says really screams ""I don't know how to compute nef cones and find interesting birational morphisms"". If $f:X \to Y$ is a birational morphism between smooth $X,Y$ and an algebraic fiber space, is it possible there is a ray $R \in NE(X)$ s.t $K_X \cdot R \geq 0$ ? I'm asking whether Mori theorem tells us ""everything"" there is to know about morphisms.","['algebraic-geometry', 'birational-geometry']"
4817059,Calculating Mehta integral using induction.,"I was wondering if there is a way to compute the Mehta integral for real numbers by using induction or some nice reverse engineering with Laplace transform, or if this is vain. Here $\gamma$ is real $$
\frac{1}{(2 \pi)^{n / 2}} \int\limits_{-\infty}^{+\infty} \cdots \int\limits_{-\infty}^{+\infty} \prod_{i=1}^n e^{-t_i^2 / 2} \prod_{1 \leq i<j \leq n}\left|t_i-t_j\right|^{2 \gamma} d t_1 \cdots d t_n=\prod_{j=1}^n \frac{\Gamma(1+j \gamma)}{\Gamma(1+\gamma)}
$$ Update : I want to inform that instead of requiring $\gamma$ to be real, now I want $\gamma$ to be a positive integer. Now any idea? Thank you.","['complex-analysis', 'definite-integrals', 'real-analysis']"
4817065,"$y''=a\frac{y}{(y')^2+b},y(0)=0,y'(0)=c,\forall c\in\mathbb{R}$ [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 7 months ago . Improve this question Can some solve the following equation or at least tell me how to approximate the result? $$y''=a\frac{y}{(y')^2+b},y(0)=0,y'(0)=c,\forall c\in\mathbb{R}$$ I have no idea if it is an easy task, but for my level of math, I can't solve it and I need it for a project. Wolfram cannot give me an answer, at least without pro computation power. It tells me it is a second order non linear ODE. Thanks in advance",['ordinary-differential-equations']
4817144,"counting sequences of elements of the set {1,2,3,4} with given property","I found this problem in some internet olympiad-prep materials (don't remember the source of this file): Find the number of $n$ -element sequences $(a_1,a_2,\dots,a_n)$ such
that $a_1=1$ , $a_n=4$ and $a_i\in\{1,2,3,4\}$ and $|a_{i+1}-a_i|=1$ for all $i=1,\dots,n-1$ . Let $X_n$ be the number we want. Obvious values: $X_1=0$ , $X_2=0$ , $X_3=0$ , generally $X_{2k-1}=0$ since we need to go from 1 to 4 and every step is $\pm 1$ , so it changes the parity of our term.
For the even $n$ , I computed $X_4=1$ (as $(1,2,3,4)$ is the only one), $X_6=3$ (as $(1,2,1,2,3,4)$ , $(1,2,3,2,3,4)$ , $(1,2,3,4,3,4)$ are the only ones) and if I'm not mistaken $X_8=8$ , $X_{10}=18$ , $X_{12}=50$ .
Probably there is a recurrence relation between $X_{2k+2}$ and $X_{2k}$ or also previous terms, but I couldn't find it.","['contest-math', 'combinatorics', 'recurrence-relations']"
4817199,The fabulous Wenger's Summation,"The user Avery Wenger said he came up with this problem randomly and I think it's very interesting! Let $s:\mathbb N\to\mathbb N$ be the sum of digits function. Then, Wenger defines $w:\mathbb N\to\mathbb N$ given by $$w(n):=\min\{k\in\mathbb N: s^k(n)\;\mbox{has a single digit}\}\;\forall n\in\mathbb N$$ Now comes the really interesting part: the Wenger's Summation . $$W_m:=\sum_{n=1}^m (-1)^{n+1}w(n)$$ What is so interesting about it? Its amazingly weird graph! As you can see, it behaves very poorly at first, looking kinda bumpy (jumps seem to occur on multiples of powers of ten). Soon, however, the graph starts to look very much like a line! Is this really Wenger's Summation asymptotic behavior? Is there a real constant $\omega$ (the Wenger's Constant) such that $$\lim_{m\to\infty} \frac{W_m}{m} = \omega\;?$$ If not, then what is Wenger's Summation asymptotic behavior? Can we at least show $(W_m)_m$ to be unbounded? But of course, this is only the decimal Wenger's Summation , since we are taking number in decimal notation! What is so special about $10$ , anyway? What about the binary Wenger's Summation ? Does it behave just as oddly? It actually does worse! So, for every possible basis $b$ we have a different Wenger's Summation $(W_m^b)_m$ and a different Wenger's Constant $\omega^b$ . Ain't that wonderful? Is it true that, if $a<b$ , then $\omega^a>\omega^b$ ? The alternating signals makes proving all this stuff so awkward. I'm just amazed about this very innocent looking procedure generating such a beautiful cacophony. It all looks so useless and disconnected to... anything else. Do mathematics even has the tools to answer these questions? If you can find anything interesting about Wenger's Summations, please let me know it! Attention: Be aware Avery Wenger did not call all these mathematical objects by his own name in the original post. I'm the only one responsible for that (Avery, if you are seeing this, thank you for sharing the formidable problem that randomly came to you). Values for the Wenger's Summation were calculated through this rudimentary python script. import matplotlib.pyplot as plt
import numpy as np

b = 4 # basis

def w(n):
    ap = 0 # additive persistence
    while n>=b:
        aux = n
        sum = 0
        while aux>0:
            sum+=(aux%b)
            aux//=b
        n = sum
        ap+=1
    return ap

S = 0  # Wenger's summation
W = []
for n in range(100000000):
    S+=w(n)*(-1)**(n+1)
    W.append(S) Update: A comment pointed out the value $w(n)$ is called the additive persistence of $n$ . The sequence $(w(n))_n$ (OEIS A031286 for base $10$ ) has some interesting properties. For example, every positive integer appears in it infinitely many times. Furthermore, the first occurrence of $N$ in it (OEIS A006050 for base $10$ ) is given by the recurrence formula $a(N)=2\cdot b^{\frac{a(N-1)-1}{b-1}}-1$ , so its always odd , i.e., $N$ appears for the first time in the Wenger's Summation with a positive sign! This might be a first step in proving its boundlessness. Also, inspired by the same comment, I decided to add the graph for the Quartenary Wenger's Summation , which twists our expectations by trending downwards (until it doesn't anymore)!","['number-theory', 'asymptotics', 'real-analysis', 'sequences-and-series', 'recreational-mathematics']"
4817219,"Matrix with entries $A_{ij} = u^{|i-j|} - u^{i+j}$ is positive semidefinite when $u \in (0, 1)$?","Consider the square $n \times n$ matrix $A_n$ with entries $u^{|i-j|} - u^{i+j}$ where $u \in (0, 1)$ . Is it true that $A$ is positive semidefinite? In the case $n = 1$ , we have $A_1 = 1-u^2 \geq 0$ . For the case $n = 2$ , we have $A_2 = (1 - u^2)\begin{pmatrix} 1 & u \\ u & 1+u^2 \end{pmatrix}$ . We can look at a quadratic form to see it is again positive semidefinite: $$\frac{(x, y)^T A_2 (x, y)}{1-u^2} = x^2 + (1 +u^2)y^2 + 2xyu \geq x^2 + u^2 y^2 + 2xyu = (x + uy)^2 \geq 0.
$$ I can't tell how to generalize this to $n \geq 3$ . Perhaps there is a better way establish the claim? For instance, it suffices (by induction) to prove that $A_n$ has positive determinant, but this also seems a little tricky.","['inequality', 'linear-algebra', 'positive-semidefinite', 'positive-definite']"
4817238,Weak convergence implies convergence in probability?,"I'm striving to understand a proof from a paper. Denote the symbol $\rightsquigarrow$ as a weak convergence, and $l^{\infty}(\Omega)$ as the space of bounded functions on a metric space $(\Omega, d)$ . Let $M_n$ be a sequence of random objects of $l^{\infty}(\Omega)$ , which converges weakly to $M$ , i.e. $M_n \rightsquigarrow M$ . Is it true that $\lVert M_n - M \rVert _{l^{\infty} (\Omega)}$ converges to zero in probability? I know that convergence in probability implies convergence in distribution, but the converse is not always true. If you need a reference, please read the last paragraph of https://arxiv.org/pdf/1608.03012.pdf . (Alexander Petersen and Hans-Georg MÃ¼ller, FrÃ©chet Regression for Random Objects with Euclidean Predictors) Thank you very much. Caution : $\Omega$ is not a probability space in this question, instead it is a metric space. P.S. I just highlighted the part. Thank you. Theorem 1.3.6 in van der Vaart(1996) is about the continuous mapping theorem.","['probability-theory', 'functional-analysis', 'weak-convergence']"
4817267,When are all normal subgroups of a direct product of finite groups a direct product of normal subgroups?,"Let ${G_1}$ and ${G_2}$ be finite groups. When are all normal subgroups of their direct product ${{G_1}\times{G_2}}$ the direct product of normal subgroups in ${G_1}$ and ${G_2}$ ? So when is it true that every normal subgroup of their direct product can be represented as ${{N_1}\times{N_2}}$ where ${{N_1}\lhd{G_1}}$ and ${{N_2}\lhd{G_2}}$ . I know this is true when ${G_1}$ and ${G_2}$ are finite groups of coprime order. But generally this isn't the case. For example $\langle (1,1) \rangle$ is a normal subgroup of $\mathbb{Z_2}\times\mathbb{Z_2}$ . However it cannot be represented in such a way. But for some reason all normal subgroups of, for example, ${{A_4}\times\mathbb{Z_3}}$ can be represented like that. So I'm interested whether there is some sort of a criterion.","['direct-product', 'finite-groups', 'normal-subgroups', 'abstract-algebra', 'group-theory']"
4817296,Non-zero measure sets that are stable under composition,"Throughout this question when I talk about relations, I am referring to relations on $\mathbb{R}$ , i.e. subsets of $\mathbb{R}^2$ One of the first examples given when the topic of composition of relations is discussed, is that of the composition of a circle with itself. This is a circle: And this is what you get when you compose the circle with itself (if the circle were represented by the relation $C=\{(x,y):x^2+y^2=1\}$ , then this is $C\circ C$ ) So the circle is not stable under composition. And neither is the unit disc. The unit square however, is. Below is its graph: Another relation which is stable under composition is the diagonal, i.e. the relation $$D = \{(x,x)|x\in \mathbb{R}\}$$ And of course the entire $\mathbb{R}^2$ . Now we have already found one relation whose graph has non-zero finite measure, and which is stable under composition, namely the unit square. Are all such relations simply the unions of squares with their diagonals lying on the main diagonal? or is there some other non-trivial non-zero measure region which has this property?","['measure-theory', 'relations', 'function-and-relation-composition']"
4817311,On kernels and stalks of sheaves.,"Suppose we're given sheaves $F,G$ on a space $X$ and a morphism of sheaves (of abelian groups) $\phi:F\to G$ . I want to prove two things : the presheaf $\ker \phi$ , defined by $(\ker\phi)(U):=\ker(\phi_U:FU\to GU)$ is also a sheaf. $(\ker\phi)_p \simeq \ker(\phi_p)$ I can do 1. by actually procing the axioms for a sheaf with the presheaf $\ker \phi$ . But I know this should follow formally from the fact sheafification $\tilde{\,}:Psh(X)\to Sh(X)$ and the forgetful functor or inclusion $i:Sh(X)\to Psh(X)$ form a pair of adjoint functors, $\tilde{}$ being left adjoint to $i$ . Because then $i$ preserves limits, and kernels are limits but the precise way to conclude here isn't clear, writing it down is a bit awkward. So $\ker \phi$ is the limit of $F\rightrightarrows G$ (taken in the category of presheaves?) where the arrows are given by $\phi$ and $0$ . Saying $i$ commutes with limits, would mean $i (\lim F\rightrightarrows G )\simeq \lim iF\rightrightarrows iG$ but I don't know Ã  priori that this limit is a sheaf, in fact this is what I want to prove so writing $i(\lim...)$ just doesn't make sense, this is I what I mean when I say the setup feels awkard. I have a similar issue with 2. : writing down an actual map giving the isomorphism is ok, but I feel like this should formally follow from the fact that the functor $_p$ ""stalk at $p$ "" is a filtered colimit and $\ker$ is a limit, these should commute. But I ran into similar issues, when writing it down. Any help would be deeply appreciated.","['limits', 'limits-colimits', 'category-theory', 'sheaf-theory']"
4817327,Concerns about the definition of Hawkes process,"In the lecture notes I am reading about Point process, when we introduced the Hawkes process several expressions are given and I have some difficulty to understand properly what is the $Z_t$ (defined below). I tried to make my doubts clear by precising two points w of concerns. Before I introduce the framework in order to fix notations. We consider $T_1<â¦<T_n<..$ the sequence of jump event with values in $\mathbb{R}^{2}_{+}$ and $\lambda(t)$ the intensity function at time $t$ . We define the poisson random measure (PRM) associated to the sequence of jump event by $$
M(\omega)(A)= \sum_{i\geq 0}\delta_{T_i(\omega)}(A),\quad A\in\mathcal{B}(\mathbb{R}^{2})
$$ I will skip the $\omega$ in the sequel for the PRM. Now we define the set $$
A_t = \{ (x_1,x_2) : 0\leq x_1\leq t, 0\leq x_2\leq\lambda(x_1)\}
$$ By precising that $\lambda(t) = \mu + \sum_{n : T_n <t}h(t-T_n)$ with $\mu>0$ and $h :\mathbb{R}_{+}\to\mathbb{R}_{+}$ , we define the linear Hawkes process by $$
Z_t = M(A_t) 
$$ Then, I have two concerns : The justification of such equality $Z_t = \int_{[0,t]}\int_{0}^{\infty}1_{z\leq\lambda(s)} M(ds,dz)$ Indeed,I would like to start with the very definition of $Z_t$ that is with an integral over $\mathbb{R}^{2}$ since the PRM is a $2$ dimensional random measure : $$
Z_t = \int_{[0,t]\times[0,\lambda(s)]}M(ds,dz)
$$ But then I donât know how to separate the integral in two since I cannot use Fubini in this context. How can I justify the passage from the integral above to $$
Z_t = \int_{[0,t]}\int_{0}^{\infty}1_{z\leq\lambda(s)} M(ds,dz)
$$ ? This makes me doubt about my comprehension of this $Z_t$ . The second concern is about the function $\lambda(t)$ . Indeed $h$ is defined on $\mathbb{R}_{+}$ so the expression $h(t-T_n)$ does not make sense since $T_n(\omega)\in\mathbb{R}^{2}_{+}$ ? This concerns me all the more as the following expression is often used $$
\int_{(0,s)}h(s-u)dZ_u = \sum_{ k : T_k < s} h(s-T_k)
$$ Which, for me until now, does not make sense given the definition of $T_n$ . If you have any answers that might help me, please don't hesitate! Thank you very much. $\mathbb{R}_{+}^{2} = \{ (x,y)\in\mathbb{R}^{2} : x\geq 0, y\geq 0\}$ After some thoughts concerning the second point, it may be possible to consider as the initial point process $X_k$ with values in $\mathbb{R}^{2}_{+}$ (for example a $2$ dimensional uniform law) and then define $$
T_k(\omega)= \pi_{1}(X_k(\omega))= x_1
$$ Where $X_k(\omega)=(x_1,x_2)$ . In order to have $$
\lambda(t) = \mu + \sum_{n : T_n <t}h(t-T_n)
$$ that is well defined. I donât know if it is correct in that way but I think it avoids the confusion I highlighted above. Let me know what is your thoughts on this please.","['stochastic-processes', 'measure-theory', 'random-measures', 'point-processes']"
4817344,Depending on a Probability Measure.,"In Jun Shao's Mathematical Statistics, the author tries to frame statistics in a more general setting than I'm used to. For example, by his definition, the Population is a probability measure $P$ . So far so good...
Then in some definitions, he talks about ""depending on $P$ "".
Here are some examples: In factorization theorem for a sufficient statistics $T(X)$ , he states $$ \frac{dP}{d\nu}=g_P(T(x)) h(x)$$ that $g_P$ depends on $P$ , but $h(x)$ does not. In the definition for ancillary statistic, he states that the distribution of that statistic does not depend on the population $P$ . I'm used to these definitions being given w.r.t. parameters. I'm assuming that given this level of generality, these definitions may also work for non-parametric distribution families. What does it mean to depend on the distribution, on the previous definitions?","['statistics', 'probability-theory']"
4817376,Associative binary operation on the positive reals,"Let $*$ be an associative binary law on the interval $(0,\infty)$ of the positive reals. Suppose that for every $a,b,c \in (0, \infty)$ we know $$ a * b * c = \frac{abc}{ab+bc+ca}.$$ Can we show that for all $a,b \in (0, \infty)$ we have $a * b = \frac{ab}{a+b}$ ? I suspect that the answer to this question is yes, but was not able to make any meaningful progress towards a full proof in the last few days. Any ideas would be appreciated.","['binary-operations', 'abstract-algebra']"
4817416,"True or false: For every $n\in\mathbb{Z^+}$, there exist $a,b,c$ such that $y=(x-a)^2+b$ and $y=c$ enclose exactly $n$ lattice points.","It is easy to show that, for every $n\in\mathbb{Z^+}$ , there exists a circle that encloses exactly $n$ lattice points (points with integer coordinates). Can we say the same thing about a parabola and a horizontal ""cap"" at the top? True or false: For every $n\in\mathbb{Z^+}$ , there exist $a,b,c$ such that $y=(x-a)^2+b$ and $y=c$ enclose exactly $n$ lattice points. If a point lies on the curve or the horizontal line, then the point is considered to be enclosed. For example, $y=(x-0)^2-0.5$ and $y=2.5$ enclose exactly $7$ lattice points. $y=(x-0.5)^2-0.8$ and $y=2.5$ enclose exactly $8$ lattice points. But as $n$ gets larger, more fine tuning is needed to enclose exactly one more lattice point than before, and I don't know if it will always be possible to do so. Context: I was trying to draw a parabola using basic equipment , and this conjecture came to mind.","['coordinate-systems', 'conic-sections', 'geometry']"
4817444,Doubts: continuity VS existence of the limit,"I'm having some doubts that I cannot solve about this ""conundrum"", if I can call it like so. Consider $f(x) = \sqrt{x}$ . Now we all know that $\sqrt{0} = 0$ , hence the square root is well defined at $x = 0$ . $f$ is continuous at $x = 0$ because $\lim_{x\to 0} \sqrt{x} \equiv f(0) = 0$ .
But now I was thinking: the limit at $x = x_0$ exists if and only if $$\lim_{x\to x_0^+} f(x) = \lim_{x\to x_0^-} f(x)$$ Yet here $\lim_{x\to 0^-} \sqrt{x}$ doesn't exist. How do we solve this ""conundrum""? How can we talk about continuity at $x = 0$ in this case?","['limits', 'calculus', 'continuity', 'analysis']"
4817469,"When $\mathbb {P} (X \ge Y)\ge \frac{1}{2}$, $\mathbb {P} (Y \ge Z)\ge \frac{1}{2}$ $\Rightarrow $ $\mathbb {P} (X \ge Z)\ge \frac{1}{2}$?","Let $X, Y, Z$ be random variables. When does the following hold? $$\mathbb {P} (X \ge Y)\ge  \frac{1}{2} \text{ and } \mathbb {P} (Y \ge Z)\ge  \frac{1}{2} \Rightarrow \mathbb {P} (X \ge Z)\ge  \frac{1}{2}.$$ Could you specify some necessary or sufficient conditions under which the above holds when $X, Y, Z$ are independent? Updates 1: Considering the counterexample in the answer given by @RyszardSzwarc, I updated my findings as follows: A- When $X, Y, Z$ are independent and have symmetric continuous distributions with unique medians , the above holds (still not sure about that this holds when $X, Y, Z$ are dependent). B- When the distributions of $X, Y, Z$ do not have unique medians, even if they are symmetric, the above may not hold. Specific questions: 1- If $X, Y, Z$ are independent and have distributions with unique medians , does the above hold? This can be written as follows: $$\int_{-\infty}^{+\infty} F_Y(x)dF_X(x) \ge  \frac{1}{2}, \int_{-\infty}^{+\infty} F_Z(y)dF_Y(y) \ge  \frac{1}{2} \Rightarrow \int_{-\infty}^{+\infty} F_Z(x)dF_X(x) \ge  \frac{1}{2}.$$ Please provide a proof or counterexample. 2- What happens if the analysis of the above question is restricted to the class of distributions whose cdfs are continuous and strictly monotone over their supports (implying that the median is unique)? For a subclass of this class including those distributions with densities, the claim is equivalent to the following $$\int_{-\infty}^{+\infty} F_Y(x)F'_X(x)dx \ge  \frac{1}{2}, \int_{-\infty}^{+\infty} F_Z(y)F'_Y(y)dy \ge  \frac{1}{2} \Rightarrow \int_{-\infty}^{+\infty} F_Z(x)F'_X(x)dx \ge  \frac{1}{2}.$$ Here, $F_X, F_Y, F_Z$ denote the cdfs of $X, Y, Z$ , respectively. $X, Y, Z$ have unique medians when $F_X(m_X)=F_X(m_Y)=F_X(m_Z)=0.5$ have unique solutions $m_X, m_Y, m_Z$ . Updates 2: Based on the counterexample provided by @stochasticboy321, I realized that the set of independent RVs with symmetric continuous distributions whose medians are unique is the only set of independent RVs over which the proposed probabilistic order is transitive. Note that based on the counterexample provided by @RyszardSzwarc we can design symmetric continuous distributions with non-unique medians for which the implication does not hold (the continuous version of $Y$ has a symmetric bimodal continuous distribution   with multiple medians). $\color{red}{\text{Prove, refine, or provide a counterexample:}}$ I guess this can be generalized to the set of all RVs with symmetric distributions whose medians are unique (including constants and symmetric distributions with unique medians) as The union of the set of real numbers and the set of RVs with symmetric continuous distributions whose medians are unique is the largest set of RVs over which the proposed probabilistic order is transitive. Here, $(X,Y, Z)$ is called to have a symmetric distribution if for some $m_1, m_2, m_3$ : $$(X-m_1,Y-m_2, Z-m_3) \sim - (X-m_1,Y-m_2, Z-m_3).$$ Updates 3: By the Bonferroni's inequality and $$\mathbb {P} (X \ge Z)\ge \mathbb {P} (X \ge Y , Y \ge Z)$$ we have $$\mathbb {P} (X \ge Y)\ge  p, \mathbb {P} (Y \ge Z)\ge  q \Rightarrow \mathbb {P} (X \ge Z)\ge  p+q-1.$$ Hence, to have a probabilistic order over the set of all RVs , $p=q=1$ seems to be the only choice for which $p=q=p+q-1$ .","['integration', 'statistics', 'real-analysis', 'order-theory', 'probability']"
4817472,Doubt regarding shortest distance between exponential and logarithmic curve?,"Consider the two functions functions : $e^x$ and $\ln x$ . I know that the shortest distance is along the common normal. But my teacher said that ""both the curves are inverse of each other and symmetrical about the line $y=x$ . Hence , the common normal must be perpendicular to the line $y=x$ ."" My problem is only regarding the last line. My teacher said it in a way that it seems obvious. But I cannot prove that why the normal must be perpendicular to the line $y=x$ also. I'm especially looking for a geometric proof.","['calculus', 'geometry', 'tangent-line']"
4817485,Is $f'(x)=f(1/x)$ solvable?,"So recently I have been scrolling through Youtube (mainly to find math videos for entertainment, I'll attempt a question on my own every now and then) when I came across this video by Michael Penn solving the differential and functional equation $$f'(x)=f(1/x)$$ which I thought might be a nice challenge for me. Here is my attempt: Right away we can see that $f'(1)=f(1)$ from plugging in $x=1$ . But, this didn't really tell me anything, however I decided to keep this in mind for later. To make some actual progress we can see that $$\begin{align}f''(x)=\dfrac d{dx}f'(x)=\dfrac d{dx}f(1/x)\\=f'(1/x)\cdot\dfrac d{dx}\dfrac1x=-\dfrac{f(x)}{x^2}\end{align}$$ Therefore $$f''(x)=-\dfrac{f(x)}{x^2}$$ Now, we can set up 2 differential equations that our function must satisfy: $$x^2y''+y=0$$ and $$y'(1)=y(1)$$ I'll try solving our first one first: Plugging in $y=x^r$ ,we have $$x^2x^{r-2}(r^2-r)+x^r=0\\\implies x^r(r^2-r)+x^r=0\\x^r(r^2-r+1)=0\\\implies r^2-r+1=0$$ So solving our quadratic equation gets us $$r=\dfrac{1\pm i\sqrt3}2$$ We now let $$r_1=\dfrac{1+i\sqrt3}2,r_2=\dfrac{1-i\sqrt3}2\implies y=c_1x^{r_1}+c_2x^{r_2}$$ which we can write as [1] $$x^{r_1}=\sqrt x\left(\cos\left(\dfrac{\sqrt3}2\ln x\right)+i\sin\left(\dfrac{\sqrt3}2\ln x\right)\right)=z\sqrt x\\x^{r_2}=\sqrt x\left(\cos\left(\dfrac{\sqrt3}2\ln x\right)-i\sin\left(\dfrac{\sqrt3}2\ln x\right)\right)=\overline z\sqrt x$$ which is where I am currently stuck because I am unsure about what to do from here. So, my question is: Is $f'(x)=f(1/x)$ solvable, or am I doing math manipulations with no real meaning? [1] I decided to write everything that was being multiplied by $\sqrt x$ as $z$ and $\overline z$ for $r_1$ and $r_2$ respectively just so I didn't have to deal with the huge mess that the expression was. Also note that I am leaving how I expanded $x^{r_1}$ and $x^{r_2}$ as an exercise for the reader.","['functional-equations', 'ordinary-differential-equations']"
4817587,Expected number of rolls before rolling a 6 given that the first two rolls are not a 6,"You roll a 6-sided repeatedly until you see a 6. Let X= the number of rolls until you see a 6. Let A be the event that you do not see a 6 on the first two rolls. What is $E[X|A]$ ? I know that $E[X|A] = \sum_{k=1}^\infty k\cdot Pr(X=k|A)$ . I also think that $Pr(A)=\frac{5}{6}\cdot\frac{5}{6}=\left(\frac{5}{6}\right)^2$ since you can roll anything other than a 6 on the first two rolls. I'm just struggling a little because technically shouldn't $Pr(X=1|A)$ and $Pr(X=2|A)$ be both 0? Since you can't a roll a 6 on the first or second rolls and also not roll a 6 on the first two rolls at the same time. For $Pr(X=3|A)$ I figured that it's calculated like this: $$Pr(X=3|A) = \frac{Pr(X=3 \cap A)} {Pr(A)}=\frac{\frac{5}{6} \cdot \frac{5}{6} \cdot \frac{1}{6}} {\left(\frac{5}{6}\right)^2}=\frac{1}{6}$$ I've deduced that this means that: $$Pr(X=k|A)=\left(\frac{5}{6}\right)^{k-3}\cdot\frac{1}{6}, k\ge3$$ But I'm not sure if this is correct. I'm also not quite sure how to calculate the sum if this is the case. Any help would be much appreciated.",['probability-theory']
4817610,"Intuition for how the Fourier transform ""preserves information"" and ""changes basis""?","I am an undergrad that has taken a few courses in real and complex analysis. I am trying to understand the Fourier transform better at a level of abstraction somewhere between ""it moves from physical to frequency space"" and a graduate level that talks about things like ""Pointryagin dual spaces."" 1) (Preserving information) My current intuition for integral transforms comes from measure-theoretic probability. My thinking from scratch: I know the probability distribution of a random variable is essentially the ""measure assigned to all Borel sets,"" so it makes sense that knowing $E[f \circ X]$ for any measurable $f$ suffices to characterize the distribution of $X$ . This means knowing $E[X^k]$ for all integers $k$ suffices by polynomial approximation, and so moments characterize a distribution. Then since the exponential function is defined as a power series, the object $E[\exp( tX)]$ tells us the moments, and thus the distribution, of the rv $X$ , so a MGF characterizes a distribution. I later learned the MGF is sometimes called a ""Laplace Transform,"" so it makes sense that the Laplace transform preserves information. I also know the Fourier transform is the ""characteristic function"" of a random variable, but I'm not sure what the analog of the above explanation is for the characteristic function, and in particular the need for complex numbers here and the intuition for what their resulting interpretation (in the above sense) puzzles me. Of course, if you think of the naÃ¯ve idea that the Fourier transform goes from ""physical to frequency space"" then it's obvious that it preserves information. But I am looking for an explanation in the sense given above. 2) (Change of basis) I have read the Fourier transform is a ""change of basis"" and I have also heard the fact that the exponential function is an eigenfunction of the Laplace operator, but I'm not sure how these two operators are related. In particular, I know the Fourier transform sends $\mathcal{F}: L_2 \to L_2$ , where that function space has a countable basis. So I would expect a ""change of basis"" to be something like $\hat{f} = \sum_i a_i \phi_i$ where $\phi_i$ are a basis set of functions for the space. But the Fourier transform involves an integral , not a sum, somehow implying an ""uncountable infinite basis"" if we view $\hat{f}(x) = \int f(t) \exp(-2\pi i x t) dt$ the values $f(t)$ as the co-efficients $a_i$ and the $\exp(-2\pi i x t)$ as the eigenfunctions. What's going on here? In what precise sense is it a change of basis?","['fourier-analysis', 'operator-theory', 'fourier-transform', 'probability-theory', 'integral-transforms']"
4817648,Find the value ${a_{10}}^2+4a_{9}-a_9a_{11}$,"Let $a_0=3$ and $a_1=7$ and $a_{n+2}-5a_{n+1}+6a_n=2$ for all $n$ . What's the value of $${(a_{10})}^2+4a_{9}-a_9a_{11}$$ I have successfully solved the recursion and obtained the explicit formula $a_n=2\times 3^n+1$ . However, I am currently facing challenges in progressing further.","['algebra-precalculus', 'recurrence-relations', 'recursion']"
4817695,"In a simple graph with $2m$ vertices and a unique perfect matching, prove that $|E(G)|$ is bounded by $m^2$.","I have been trying to solve this question , it was already asked but the response seems to have some issues. The accepted answer implies that if a graph has a cycle of length 4, this implies that it has more than one perfect matching, but this is not true. Iâm including a counter-example below, a graph with a cycle of length 4 but still with only one perfect matching:","['graph-theory', 'matching-theory', 'combinatorics', 'discrete-mathematics', 'extremal-graph-theory']"
4817697,Metric on the set of non-empty finite subsets (Ex. 2.4 MTH 427/527),"I am taking a course titled ""Introduction to Topology I. General Topology"" and stuck on the following exercise from the course notes: Let $S$ be a set and $\mathcal F(S)$ denote the set of all non-empty finite subsets of $S$ . For $A, B \in \mathcal F(S)$ define \begin{equation*}
\rho(A, B) = 1 - \frac{|A \cap B|}{|A \cup B|},
\end{equation*} where $|A|$ denotes the number of elements of the set $A$ . Show that $\rho$ is a metric on $\mathcal F(S)$ . I can prove the positivity and symmetry properties, but cannot verify the triangle inequality. That is, if $A, B, C \in \mathcal F(S)$ , how can we show that $\rho(A, B) + \rho(B, C) \geq \rho(A, C)$ ? I tried to substitute the definition given above and manipulate the obtained expressions by rewriting the intersections and unions of sets, but could not get the desired result. Perhaps De Morgan's laws need to be used here, but I have no idea how yet.","['elementary-set-theory', 'general-topology', 'metric-spaces']"
4817710,A positive operator $P: H \to H$ is bounded below iff $P \ge cI$ for some $c>0$,"Let $H$ be a Hilbert Space. A positive operator $P: H \to H$ is bounded below iff $P \ge cI$ for some $c>0$ . I have been trying to prove this for a while but perhaps I am missing something: First suppose that $P$ is bounded below, that is, there is some $c>0$ we have that $\lVert P x \rVert \ge c \lVert x \rVert$ for each $x\in H$ . We need to show that $\langle Px-cx,x \rangle \ge 0$ for each $x\in H$ . To this end, let $x\in H$ . From $\lVert Px \rVert \ge c \lVert x \rVert$ , by squaring both sides, we can conclude that $P^2-c^2I \ge 0$ . I am expecting that $P \ge cI$ by taking square root but I cannot justify this argument. In fact, it is rather easy to see that $P: H \to H$ is bounded below iff $P^2 \ge c^2I$ for some $c>0$ . Hints on proving this will be appreciated! Here's one direction of the proof that I was able to check: $(\Longleftarrow)$ Suppose that $P-cI \ge 0$ . We claim that $\lVert Px \rVert \ge c \lVert x \rVert$ for each $x\in H$ . If $x=0$ , then the inequality is trivial, so, let $x\in H$ be nonzero. \begin{align*}
\lVert Px \rVert \rVert x\rVert &\ge  \langle Px, x \rangle \ge \langle cx ,x\rangle \ge c \lVert x \rVert^2.
\end{align*} Cancelling $\lVert x \rVert$ , we get what we wanted.","['operator-theory', 'functional-analysis']"
4817734,How do I find the arctan of a complex number?,"I am trying to simplify $$\tan^{-1}\left(e^{ix}\right), x\in\mathbb{R}$$ $$=\\tan^{-1}\left(\cos x+i\sin x\right)$$ Using $$\tan^{-1}\left(a+b\right)=\frac12\tan^{-1}\left(\frac{2a}{1-a^2+b^2}\right)+\frac12\tan^{-1}\left(\frac{2b}{1+a^2-b^2}\right)$$ Gives $$\frac12\tan^{-1}\left(\frac{2\cos x}{1-\cos^2x-\sin^2x}\right)+\frac12\tan^{-1}\left(\frac{2\sin x}{1+\cos^2x+\sin^2x}\right)$$ $$\frac12\tan^{-1}\left(\frac{2\cos x}{0}\right)+\frac12\tan^{-1}\left(\sin x\right)$$ Which is undefined (?), however if I enter the expression into my HP Prime with a value of x, say 0.23 it gives 0.785+0.116i So which of these is correct, thanks for the help.",['complex-analysis']
4817741,non-negative $\{f_n\}$ such that $\int f_{n}d\mu=1$ but not $f_{n}/n \to 0$ a.e.,"Let $(\mathbb{X},\mathcal{M},\mu)$ be a $\sigma$ -finite measure space. Let $f_n \in L^{1}(\mu)$ be non-negative functions satisfying $$\int_{\mathbb{X}} f_{n}d\mu=1$$ (a) Show that it is not necessarily true that $f_{n}/n \to 0$ a.e. (b) Show that $f_{n}/n^{2} \to 0$ a.e. I solved (b) using the fact that $\sum_{n=1}^{\infty} \frac{1}{n^2}<\infty$ . So naturally in order to solve (a) I tried to find some example that leads to the fact that $\sum_{n=1}^{\infty} \frac{1}{n}=\infty$ . But any example I've tried failed (for instance sequences of triangles with the area of 1). Thanks!","['measure-theory', 'lebesgue-integral']"
4817752,Proving an algebraic problem categorically,"I am trying to prove that $\text{Ab}(\mathop{\Large {*}}_\alpha G_\alpha) \simeq \bigoplus_\alpha \text{Ab}(G_\alpha)$ , where $\text{Ab}(G)=G/[G,G]$ . I attempted to prove it categorically and am wondering if the following justification is correct. Define the covariant functor $\text{Ab}:\text{Grp}\to \text{AbGrp}$ by sending objects $G\in \text{Ob}(\text{Grp})$ to $G/[G,G]\in \text{Ob}(\text{Ab})$ and morphisms $f \in \text{Hom}(G,H)$ to $f' \in \text{Hom}(\text{Ab}(G),\text{Ab}(H))$ by $f'([g]_G)=[f(g)]_H$ . The well-definedness of $f'$ can be easily checked. Since the free product is the coproduct in the category $\text{Grp}$ , it follows that for all groups $H$ and morphisms $f_\alpha:G_\alpha\to H$ , there exists a unique $f:\mathop{\Large *}_\alpha G_\alpha\to H$ such that $f \circ \iota_\alpha=f_\alpha$ for all $\alpha$ . Now, suppose $H$ is abelian. Then, Ab $(H)\simeq H$ . Now, take a family of morphisms $\{g_\alpha:\text{Ab}(G_\alpha)\to H\}$ , and define $f_\alpha:G_\alpha \to H$ by $f_\alpha(k)=g_\alpha([k]_G)$ for all $k \in G_\alpha$ . It can easily be checked that $\text{Ab}(f_\alpha)=g_\alpha$ . Using this family of functions $\{f_\alpha\}$ and abelian group $H$ , one can take the above coproduct setup and apply the Ab functor to everything. This yields the following: For all abelian groups $H$ and morphisms $\{g_\alpha:\text{Ab}(G_\alpha)\to H\}$ , there exists a unique function $\text{Ab}(f):\text{Ab}(\mathop{\Large *}_\alpha G_\alpha)\to H$ such that $\text{Ab}(f)\circ \text{Ab}(\iota_\alpha)=g_\alpha$ for all $\alpha$ . However, this is exactly the definition of the coproduct of $\text{Ab}(G_\alpha)$ in the category AbGrp, and since the direct sum is the unique coproduct in the category AbGrp, it follows that $\text{Ab}(\mathop{\Large *}_\alpha G_\alpha)\simeq \bigoplus_\alpha \text{Ab}(G_\alpha)$ . Is this line of reasoning correct?","['category-theory', 'group-theory', 'abstract-algebra', 'solution-verification', 'abelian-groups']"
4817802,Please criticize my attempted proof of a simple set theory lemma.,"I am reading Terrence Tao's analysis 1 and am new to writing proofs. I'm currently financially unstable and have no one to talk about math's with, so any criticm would help me so much. In his book, I was asked to prove the following statement: Let X, Y be sets. Define a partial function from X to Y to
be any function f : X' â Y' whose domain X' is a subset of X, and whose
range Y' is a subset of Y. Show that the collection of all partial functions
from X to Y is itself a set. (Hint: use Exercise 3.4.6, the power set axiom, the
replacement axiom, and the union axiom.) All of the ZFC axioms of set theory except the axiom of choice are allowed to be assumed to prove this statement. A set is considered by terrence tao at that stage of the book, and collection of mathematical object that satisfies all of the set theory axioms except the axiom of choice. (Power set axiom). Let X and Y be sets. Then there exists a set, denoted Y X, which consists of all the functions from X to Y, thus
f â Y X ââ (f is a function with domain X and range Y). Axiom 3.11 (Union). Let A be a set, all of whose elements are themselves sets. Then there exists a set UA whose elements are precisely
those objects which are elements of the elements of A, thus for all objects x
x â UA ââ (x â S for some S â A). (Replacement). Let A be a set. For any object x â A, and
any object y, suppose we have a statement P(x, y) pertaining to x and
y, such that for each x â A there is at most one y for which P(x, y) is
true. Then there exists a set {y : P(x, y) is true for some x â A}, such that for any object z, z â{y : P(x, y) is true for some x â A}
ââ P(x, z) is true for some x â A. Ex 3.4.6 :
Lemma 3.4.9:
Let X be a set. Then the set
{Y : Y is a subset of X}
is a set.
Proof. See Exercise 3.4.6. My attempted proof: Let X and Y be sets. {A | A is a subset of X} is a set by lemma 3.4.10, let P(X) = {A | A is a subset of X} {A | A is a subset of Y} is a set by lemma 3.4.10, let P(Y) = {A | A is a subset of Y} Let y be any subset of Y, i.e let y â P (Y). For each x â P (X), there exists a unique set denoted by yx , which consists of all functions f, f:x -> y. Also, for any object z, let P(x,z) be the property pertaining to x and z := â z = yx â. By the replacement axiom, we have that {z | z = yx for any x â P (X)} is a set, for each subset y of Y. For each y â P (Y), there exists one set {z | z = yx for any x â P (X)} as previously established. Also, for any object z, let P(y,z) be the property pertaining to y and z := â z = {z | z = yx for any x â P (X)}â. By the replacement axiom, we have that {z | z = {z | z = yx for any x â P (X)} for each subset y of Y} is a set. Let A = {z | z = {z | z = yx for any x â P (X)} for each subset y of Y}. By the axiom of union, we have â A is a set, such that x â â A ââ (x â S for some S â A), which is equivalent to x â â A ââ x is some function from a subset of X to a subset of Y. Any help is appreciated.Also if you want to rewrite it to be proper that would help me learn also. Thank you so much for your time.","['elementary-set-theory', 'logic']"
