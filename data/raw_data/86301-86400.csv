question_id,title,body,tags
1142943,Conditional expectation acting on square of a random variable,"Suppose that $X$ and $Y$ are random variables such that $E(Y¦X) =X$ and $E(Y^2¦X)=X^2$; also, $Y$ is in $L^2(\Omega,\mathcal{A},\mathbb{P})$. I need to show that $Y=X$ almost surely. I know the definition of conditional expectation as a projection, I. E. $E(X¦Y) =W$ is the unique $W$ in $L^2(\Omega,\sigma(X),\mathbb{P})$ satisfying $E(WZ) =E(YZ) $ for all $Z$ in that same $L^2$. My intuition is that we get the result by applying this definition and making appropriate choices for $Z$. I've tried,  but I got stuck. I am also worried that I don't know where the $L^2$ should come in. Why not $L^3$ or $L^1$, instead? Whats this have to do with it?","['probability-theory', 'conditional-expectation', 'probability']"
1142946,Good books about differentiation in normed spaces?,"Typical functional analysis books don't seem to cover this subject at all, so I'm looking for some good books that deal with differentiation in normed spaces(Gateaux/Frechet derivatives etc.). Preferably moving also into other calculus-like topics in infinite dimensional spaces, like integration on Banach spaces, perhaps Banach manifolds. Any recommendations?","['reference-request', 'normed-spaces', 'functional-analysis', 'analysis']"
1142958,"Confused about subspaces, how do I picture them geometrically?","I'm a visual learner and I'm having trouble intuitively understanding subspaces. Our professor defined a subspace as a non-empty set closed under linear combinations. However, I'm having trouble picturing this. Are subspaces just lines, planes, and n-dimensional spaces? Can subspaces be finite? I.e. can a subspace in $R^3$ be a cube with dimensions 10x10x10?","['vector-spaces', 'geometry', 'linear-algebra']"
1142962,Show that any element of a sigma algebra is the union of disjoint sets,"Let $\mathscr{M}$ be a $\sigma$-algebra on $X$ generated by a finite family of sets. Prove that there exists a partition of $X$ into disjoint sets $E_1, E_2, \ldots, E_n$ such that $A$ is an element of $\mathscr{M}$ if and only if $A$ is the union of some sets $E_1, E_2,\ldots, E_n$. My work so far is Suppose $\mathscr{M}$ is generated by a finite collection say $\{B_i\}$ $i = 1,\ldots,n$. Then I define a partition $\mathscr{P}$ by 
$$\mathscr{P} = \left\{\bigcap C_i: C_i = B_i  \text{ or } B_i^c, i = 1,\ldots,n\right\}$$ Let $\mathscr{L}$ be a collection of the arbitrary union of the sets in $\mathscr{P}$.  I claim that $\mathscr{L}$ is a $\sigma$-algebra.  If I can show that $\mathscr{L} = \mathscr{M}$, is this enough to prove the proposition? Thank you. Sorry about the typsetting.  I don't know how to type in the symbols.","['measure-theory', 'real-analysis']"
1142995,"""Interesting"" subset of $\mathbb{R}$ with exactly three limit points","Baby Rudin Exercise 2.5: Find a bounded subset of $\mathbb{R}$ with exactly three limit points. The easy answer is the union of three subsets which each have one limit point, i.e. $\{\frac{1}{n}, \frac{1}{n} + 1, \frac{1}{n} + 2\}(n=1, 2, 3, \dots)$ which have limit points $0, 1, 2$. My question is that is there a more ""interesting"" example? As in, not just a set that is the union of three sets which each have exactly one limit point? Of course ""interesting"" can be quite subjective, but I trust you get generally what I mean.",['general-topology']
1143063,Find a countable set with the given property.,"Given any extended-valued $f$ on $(-\infty,+\infty)$, prove that there exists a countable set $D$ with the following property. For each $t\in\mathbb R$, there exist $t_n\in D$, $t_n\to t$ such that $f(t)=\lim_{n\to\infty}{f(t_n)}$. The assertion remains true if ""$t_n\to t$"" is replaced by ""$t_n\downarrow t$"" or ""$t_n\uparrow t$"". This is an exercise in Kai Lai Chung's A course in Probability Theory , and a hint is given:""Consider the graph $(t,f(t))$ and introduce a metric."" I think it is similar to the fact that $\mathbb R^n$ is separable, but what metric should we define on the graph?","['stochastic-processes', 'metric-spaces', 'real-analysis']"
1143065,Divergent sequence with decreasing function,"Let $f: \mathbb{R} \to (0,\infty)$ be a decreasing function. Define a sequence $(a_n)$ by $a_1=1   $ and $a_{n+1}=a_n+f(a_n)$ for every $n\ge 1$. Prove that $(a_n) \to \infty$. I have tried by contradiction to assume it is bounded and therefore converges. clearly the $a_n$ are strictly increasing and I tried to use Cauchy definition to arrive at a contradiction that $f$ is not decreasing but could not process when having to choose epsilons and which $N$. What can be fixed etc. Any help?","['cauchy-sequences', 'sequences-and-series', 'real-analysis', 'analysis']"
1143075,Finding number of integer solutions using Generating Functions,"This is a problem for a practice test my professor gave me. $$\text{How many integer solutions are there to } x_1+x_2+x_3+x_4 \leq 50 \\ \text{with } x_i \geq 2 \text{ for all } i = 1,2,3,4 \text{ and } x_1,x_2 \leq 7 \text{?}$$ This is how I approached the problem, using generating functions: $\text{Same as}$
$$x_1+x_2+x_3+x_4+x_5 = 50, \space x_5 \geq 0 $$
$\text{Find the coefficient of } x^{50} \text{ in}$
$$(x^2 + x^3 + x^4 + \dotso)^2 (x^2 + x^3 + x^4 + \dotso + x^7)^2 (1 + x + x^2 + x^3 + \dotso)$$ After some factoring, we'll have: $$x^8(1+x^2+x^3+ \dotso + x^5)^2 (1+x+x^2+\dotso)^3$$ This is the same as: $\text{Find the coefficient of } x^{42} \text{ in}$
$$(1+x^2+x^3+ \dotso + x^5)^2 (1+x+x^2+\dotso)^3$$ To simplify further: $$(1-x^6)^2 \frac{1}{(1-x)^3}$$ So, this is where I'm confused. I was using the formula which is based off this answer on Math.SE , but I don't get the correct answer. According to my professor, the correct answer is: $$\dbinom{30+5-1}{30} - 2\dbinom{36+5-1}{36} + \dbinom{42+5-1}{42} = 26,781$$ What I end up doing mirrors that of the linked question on Math.SE: $$(1-x^6)^2 = 1-2x^6+x^{12} \\
\\
(1-2x^6+x^{12}) \frac{1}{(1-x)^3}$$
Using the formula from the linked question: $\text{We do this three times, for } k=0, k=6, \text{ and } k=12$.  The result is 
$$(1-2x^6+x^{12})\frac{1}{(1-x)^3}={m-0+2 \choose 2}- 2{m-6+2 \choose 2} + {m-12+2 \choose 2}
\\
\
\\m = 42 \\
\
\\={42-0+2 \choose 2}- 2{42-6+2 \choose 2} + {42-12+2 \choose 2} = 36$$ As you can see, my answer differs greatly from what my professor said was correct. I don't understand why this formula I used doesn't work; I've used it for lots of other problems of this same type, and I calculated the correct number; for this one though, it doesn't seem to be working. What am I doing wrong?","['generating-functions', 'combinatorics']"
1143086,"""Pulling back a function from a neighborhood of 0 in $\mathbb{C}^2$ to $\mathbb{U}$ is analogous to computing the derivative of that function.""","I'm reading on Riemann Surfaces and after defining 
$$\mathbb{U} \equiv \{(z,\ell)\in\mathbb{C}^2\times\mathbb{P}_1;z\in\ell\},$$ and 
$B\ell_0:\mathbb{U}\to\mathbb{C}^2$, the author mentions that, ""Pulling back a function from a neighborhood of 0 in $\mathbb{C}^2$ to $\mathbb{U}$ is analogous to computing the derivative of that function."" I understand what $\mathbb{P}_1$ is but I'm not 100% on the analogy. Is there an intuitive way to understand this?","['riemann-surfaces', 'algebraic-geometry', 'complex-geometry']"
1143132,Is there a size of rectangle that retains its ratio when it's folded in half?,"A hypothetical (and maybe practical) question has been nagging at me. If you had a piece of paper with dimensions 4 and 3 (4:3), folding it in half along the long side ( once ) would result in 2 inches and 3 inches (2:3), which wouldn't retain its ratio. For example, here is a piece of paper that doesn't retain its ratio when folded: Is retaining the ratio technically possible? If so, what is the side length and ratio that fulfills this requirement? Any help would be appreciated. Update: I added "" once "" because I got an answer saying that any recectangle would work, as any rectangle folded twice has the original ratio. Nice answer, but not quite what I was looking for. As for the other answers, I got 3x as much information as I needed! Thanks!","['geometry', 'ratio', 'sequences-and-series']"
1143149,How did author reach the conclusion tm $\equiv$ 0($\bmod$ m)?,"This is a proof of a theorem from my book, Discrete Mathematics and its Applications Here is theorem 6 of Section 4.3 The first part of the proof, ""because gcd(a, m) = 1"" makes sense because the conditional statement includes the statement that ""a and m are relatively prime integers"", meaning 
that their gcd is 1. The next step from sa + tm = 1 to sa + tm $\equiv$ 1(mod m) made sense thanks to BRIC-Fan's answer to my last question How does author reach step of $sa + tm \equiv 1 \pmod m$? Can someone explain how the author go to next conclusion though? ""Because tm $\equiv$ 0($\bmod$ m)""? To me, that came out of nowhere.","['modular-arithmetic', 'computer-science', 'discrete-mathematics']"
1143175,Doing algebra with differential operators.,"I was thinking about, of the derivative as an operator, like $\frac{dy}{dx}$, and i am having trouble thinking on the things you do in courses of differential equations, with the $dx, dy$, like passing them around from one side of the equation, and to the other, with apparently no problem, i don't worry much about it, because in a lot of physics texts they do that, and it works, But when i ask about WHY?, they say, don't worry for now, if it works, don't worry. Once i ask a mathematician, why and he refereed to ""high level math"" subjects and books, and i couldn't get all he said, so i am looking for a ""formal"" simplest answer to 
""Why can we do Algebra with the Differential operators, why does it work, and until what point does it works""",['ordinary-differential-equations']
1143190,Prove $n^2+4n+3$ is not prime for $n \in \mathbb{Z}^{+}$.,"I am trying to write a proof for this theorem: For every positive integer $n$, $n^2+4n+3$ is not a prime. Proof : Let $n \in \mathbb{Z}^{+}$. Note that $$n^2+4n+3=(n+1)(n+3)>1\text{,}$$ 
and $n+1 >1$ and $n+3 >1$. Let $a = n+1$ and $b = n+3$. Then we have $$\dfrac{(n+1)(n+3)}{a}>\dfrac{1}{a}$$ and $$\dfrac{(n+1)(n+3)}{b}>\dfrac{1}{b}\text{.}$$ 
Therefore, $n^2+4n+3$ is not prime. $\square$ I don't think my proof is right and miss many things. Can anyone give me a hit or show me how to write a better proof for this question?","['elementary-number-theory', 'proof-verification', 'discrete-mathematics']"
1143211,Intersection of two open dense sets is dense,"Let $X$ be a topological space and suppose that $H$ and $G$ are open dense subsets of $X$.Then show that $G \bigcap H$ is also an open dense subset of $X$. My attempt : Well since the finite intersection of open sets is open therefore, $G \bigcap H$ is also open. I was trying to prove the other part by the method of contradiction : Suppose on the contrary that $G \bigcap H$ is not dense. Then that implies that $Int(Cl(G \bigcap H)) = \phi$ . Now, $Cl(G \bigcap H) \subset Cl(G) \bigcap Cl(H)$ so, $Int(Cl(G \bigcap H)) \subset Int(Cl(G) \bigcap Cl(H)) = Int(Cl(G)) \bigcap Int(Cl(H)) = X $. How do i proceed further to get a contradiction ?","['general-topology', 'proof-writing']"
1143251,Prove that a function is subharmonic,"I am trying to prove the following conjecture (or find a counterexample): Claim : Let $f(z)$ and $g(z)$ be holomorphic functions defined on a simply connected bounded domain  $\Omega \subset \mathbb C$ in the complex plane. We know that at any point in the domain the following inequality holds $$ |f(z)| > |g(z)|, $$ then the function  $\zeta(z) = \frac{1}{|f(z)|-|g(z)|}$ is subharmonic in $\Omega$. Some properties that (might) be useful: 1) From the above inequality we also know that $f$ does not vanish in the domain, i.e. $\forall z\in \Omega$,  $f(z)\neq 0$. 2) $log|f(z)|$ and $log|g(z)|$ are subharmonic and since $f$ does not vanish in the domain $log|f(z)|$ is also harmonic. 3) The function $\zeta(z)$ is strictly positive. 4) The modulus of a holomorphic function is subharmonic. In particular, $|f(z)|$ and $|g(z)|$ are subharmonic. 5) Linear combinations of subharmonic functions with positive weights is subharmonic. In particular the sum of two subharmonic functions is subharmonic (but the difference of two subharmonic functions is not in general subharmonic).",['complex-analysis']
1143256,What is an example of pairwise independent random variables which are not independent?,"I've just read in a stochastics textbook: Let $(\Omega, P)$ be a discrete probability space. (a) The events $A_i \subseteq \Omega, i=1,2, \dots$ are called independent , if
  $$P(A_{i_1} \cap A_{i_2} \cap \dots \cap A_{i_k}) = P(A_{i_1}) \cdot \dots \cdot A_{i_k}$$
  for all finite sets $\{i_1, \dots, i_k\} \subsetneq \{1,2, \dots\}$ and all $k \geq 2$. (b) The events $A_i \subseteq \Omega, i=1,2,\dots$ are called pairwise independent , if
  $$P(A_{i_1} \cap A_{i_2}) = P(A_{i_1}) \cdot P(A_{i_2})$$
  for all pairs $\{i_1, i_2\} \subsetneq \{1,2,\dots\}$. After these two definitions it states that pairwise independent events are not always independent. Do you have an example?","['probability-theory', 'discrete-mathematics']"
1143277,Lebesgue measure of non-measurable set times the null set.,"The title might not be accurate, but here is my question: Let $\lambda$ be the Lebesgue measure on some measurable space $(\mathbb{R}^2, \mathcal{A})$. Assume $A\in \mathcal{A}$ is such that $A = X \times Y$, where $X$ is a non-measurable subset of $\mathbb{R}$ with respect to the Lebesgue measure on some $\sigma$-algebra on $\mathbb{R}$, and $Y = \emptyset$ or $Y = \{y\}$,  ${y}\in \mathbb{R}$. Is the Lebesgue measure of A identically equal to zero, or is it non-measurable? I know that generally $\lambda (A) \neq \lambda(X) \, \lambda(Y)$, still I would like to say that $\lambda(A) = 0$, since $A$ either has no extension in the space $\mathbb{R}^2$ or consists of line segments, which are null sets. Thank you.","['measure-theory', 'lebesgue-measure']"
1143284,What is the meaning of $A^2$ if $A$ is a set?,"Like I already wrote in the title, I would like to know what the meaning of $A^2$ is if $A$ is a set. I think that it might have the same meaning as the power set of $A$.",['elementary-set-theory']
1143307,how to extend a vector at $e$ of a Lie group to a left invariant vector field?,"I am reading some books about Lie group and Lie algebra. Denote the set of all the left invariant vector fields as $\mathfrak{X}_L$, and the tangent space at $e$ of $G$ as $T_eG$. They say that the $\mathfrak{X}_L$ and $T_eG$ are isomorphics. So we can extend a vector $\xi\in T_eG$ to the vector field on $G$ by this way:
$$X(g)=dL_g(\xi),\mbox{for any }g\in G$$ where $X(g)$ is a vecotr at $g$, $L_g:G\rightarrow G$ is the left translation, and $dL_g:T_eG\rightarrow T_gG$ is the pushforward. My question is what is $dL_g(\xi)$ exactly? Thank you.","['lie-algebras', 'lie-groups', 'differential-geometry', 'smooth-manifolds']"
1143332,Show that boundary of a closed set is nowhere dense,"Let $H$ be a closed set then, $Cl(H) =H$ and hence the $\partial H \subset H$. Now to show that the boundary is nowhere dense, it would suffice to show that $Int(Cl(\partial H)) =\emptyset$,
i.e., $Int(\partial H) = \emptyset$, but how do I proceed further in order to show this?","['general-topology', 'proof-writing']"
1143341,filling up numbers in a matrix,"Suppose you have a $k.n \times 2$ matrix. You have to fill up the numbers $1,2,3, \cdots, n$ as entries in such a way that in each column it is non-decreasing, in each row it is strictly increasing and each number should appear exactly $2k$ times. How many ways one can fill up the matrix ?","['permutations', 'discrete-mathematics', 'combinatorics']"
1143382,Convergence of integers by transformations,"Let $x=(a,b)$, where $a,b$ are in $N$ Now we have the transformations:
$$T_1(x) = (ka, b+1)$$ 
$$T_2(x) = (b,a)$$ where $k$ is in $N$.
Where the order of choosing a transformation is not fixed. 
(E.g. you can first apply 3 times $$T_1(x)$$, then $$T_2(x)$$ Will we for all $(a,b)$ produce $a=b$ by only being allowed to use these two transformations? If so, is this true for every $k$? I'm specifically interested in the case $k=2$ though.","['arithmetic-combinatorics', 'linear-transformations', 'linear-algebra', 'analysis', 'combinatorics']"
1143400,Improper integral - equivalent definition?,"Intuitively, it is rather obvious that $$\lim_{l\to\infty}\sum_{n=-\infty}^{\infty}f(n\Delta x)\Delta x = \int_{-\infty}^{\infty}f(x)dx \tag{1}$$ where $\Delta x = \frac{1}{l}$, assuming $f$ is integrable and the limit exists. The fact that this equality is true is the core part of deriving Fourier transform from Fourier series, see page 4, eq. 4.7 in this document . Or maybe we cannot consider this derivation as formal, as it was never intended to be formal, but I thought n mathematics there's no place for informal thinking. My question is how can we prove it's true from the definitions and properties of improper integral, definite integral and limits? I've listed the important definitions below in case you would like to refer to some of these in your answers. Oh, and please ignore mrf's answer - it doesn't refer to my question anymore, I've reformulated it. If function $f$ is integrable on $[a,b]$, then:
$$\int_{a}^{b}f(x)dx=\lim_{n\to\infty}\sum_{i=1}^{n}f(x_i)\Delta x \tag{2}$$
where $\Delta x = \frac{b-a}{n}$ and $x_i = a+i\Delta x$. Improper integral definitions $$\int_{a}^{\infty}f(x)dx=\lim_{t\to\infty}\int_{a}^{t}f(x)dx \tag{3}$$ $$\int_{-\infty}^{b}f(x)dx=\lim_{t\to-\infty}\int_{t}^{b}f(x)dx \tag{4}$$ $$\int_{-\infty}^{\infty}f(x)dx=\int_{a}^{\infty}f(x)dx + \int_{-\infty}^{a}f(x)dx \tag{5}$$","['improper-integrals', 'fourier-analysis', 'calculus', 'integration', 'real-analysis']"
1143405,Product of all elements in finite abelian group equals its own inverse,"Let $G = \{e, a_1, a_2, \dots, a_n\}$ be a finite abelian group and let $S$ be the set of all the elements of $G$ which are not equal to their own inverse. The set $S$ can be divided up into pairs so that each element is paired off with its own inverse. Prove that $(a_1 a_2 \dots a_n)^2 = e$. Every $x$ in $G$, outside $S$ abides by $x^2 = e.$ Let $a_i$ be in $S$. Since $G$ is abelian, we have $a_ia_i^{-1} = a_i^{-1}a_i$ $e = a_i^{-1}a_i$ Since $a_i^{-1} = a_i$, $a_i^2 = e.$ Thus, $\dots a_i^2a_j^2\ldots a_n^2 = e.$ Does that make sense? edit: We need to prove $a_1a_2...a_na_1a_2...a_n = e$, right? Let $a_i$ be in $G \setminus S$. Then $(a_i)^2$ is $e$, is it? If we rearrange the elements on the left side of the equation above according to what belongs to $S$ and what belongs to $G \setminus S$ and take the product of all the $(a_i)^2$, then every element belonging to $G \setminus S$ will  be cancelled in the right side of $a_1a_2...a_na_1a_2...a_n = e$. So, we only need to consider the elements in $S$. Suppose $a_j \in S$. Let $a_k$ be the inverse of $a_j$. We need to prove $(a_j)^2 \cdot (a_k)^2 = a_j \cdot a_j \cdot a_k \cdot a_k = e$. Since $S$ is abelian , $a_j \cdot a_j \cdot a_k \cdot a_k$ can be rearranged as $a_j \cdot a_k \cdot a_j \cdot a_k$ which is $e$.","['group-theory', 'abstract-algebra']"
1143412,Numbers interpreted as sets and functions,"In set theory numbers are defined as sets $$\emptyset,\{\emptyset\},\{\emptyset,\{\emptyset\}\},\{\emptyset,\{\emptyset\},\{\emptyset,\{\emptyset\}\}\},\dots$$
where $n+1=n\cup\{n\}$ and $n-1=\bigcup_{k\in n}k, \; n\ne\emptyset$. As I remember there are some complicated formulas for $m+n$ and $m\cdot n$ but I don't know how to get them and would like some hints. Also, is there a canonical way to interpret a number $k\in\{0,\dots, n^m-1\}$ as a function 
$m\to n$? I'll guess $\emptyset$ and $\{\emptyset\}$ are trivial, but the rest?","['puzzle', 'recreational-mathematics', 'elementary-set-theory', 'functions']"
1143432,How to figure out the limit of this question,"I am  trying to solve this question: Suppose $X$ and $Y$ are random variables with joint density
  $$ f_{X,Y}(x,y) = \left\{ \begin{array} {cc} 
   2 &\text{ for } 0<x<y<1   \\
   0 & \text{ otherwise.}\end{array} \right.  $$
  Find the density function of $Z$ where $Z=X+Y$. The given solution is: if $Z=X+Y$ then (from a given theorem earlier), $$f_Z(z)=\int\limits_{-\infty}^{\infty} f_{X,Y}(u,z-u) \,\mathrm{d}x \tag{1}$$ . Consider the support of $f_{X,Y}$ that is, $\{(x,y):0<x<y<1\}$. In order for the integrand in (1) to take non-zero values we need $0<u<z-u<1$. This implies: $$\begin{array} {cc} 0<u<\frac{z}{2} & \text{ if } 0<z<1 \\
z-1<u<\frac{z}{2} & \text{ if } 1<z<2 \tag{2}\\
\end{array}$$
  Thus the marginal density for the sum if:
  $$ f_Z(z) = \left\{ \begin{array} {cc} 
   \int\limits_0^\frac{z}{2} 2 \, \mathrm{d}u=z & 0<z<1   \\
   \int\limits_{z-1}^\frac{z}{2} 2 \, \mathrm{d}u= 2-z & 1<z<2 \\
    0 & \text{ otherwise} \end{array} \right.  $$ There two points I need help on: Question 1 I can understand why we need $0<u<z-u<1$. However, it is not clear to me why this must imply we must divide the integrand into two regions, namely $0<z<1$ and $1<z<2$ and not some other region. In fact, it did not occur to me that I must split the integrand at all. Please help me see why I must perform the integrand in the way it is described in the solution. Question 2 Having looked at the solution, I tried to redo the question (just accepted that I need to split the integration into two) and tried to obtain the inequalities in (2) but was not successful in getting the second inequality i.e. $z-1<u<\frac{z}{2}$. Instead I got the following: $$\begin{array}  {cc} 0<z<2 & \Rightarrow& 1<x+y<2 \\
& \Rightarrow & 0<z-1<1 \\ 
& \Rightarrow & 0<u+y-1<u<1 \\
& \Rightarrow & 0<x+y-1<y<1 \\
& \Rightarrow & 0<z-1<u<1 \\
& \Rightarrow & z-1 <u<1
\end{array} $$ Did I make and error somewhere? I know this can't be the answer because then $f_Z(z)$ won't integrate to $1$ from $0<z<2$. But I am not able to find my mistake.","['statistics', 'inequality', 'integration']"
1143437,Proof: $n^p < \frac{(n+1)^{p+1}-n^{p+1}}{p+1} < (n+1)^p$,"I've edited the post in order to add at the end what, I think, is the complete proof of these inequalities. I want to apologize by not having given a reply as early as those given by the users who gave me hints about this exercise. It took me a little of time but your answers were very helpful (:. This is actually part (b) of the exercise. I was able to prove these inequalities by applying the Binomial Theorem, but I have no idea about how to do it using part (a), which I'll post here too. (a) Let $b$ be a positive integer. Prove that:
$$b^p - a^p = (b-a)(b^{p-1}+ b^{p-2}a+b^{p-3}a^{2}+...+ba^{p-2}+a^{p-1})$$ (b) Let $p$ and $n$ denote positive integers. Use part (a) to show that $$n^p < \frac{(n+1)^{p+1}-n^{p+1}}{p+1} < (n+1)^p$$ (a) Proof: $$\begin{split}b^p - a^p &= (b-a)(b^{p-1}+ b^{p-2}a+b^{p-3}a^{2}+...+ba^{p-2}+a^{p-1})\\
&= (b-a)\sum_{k=0}^{p-1}\big[b^{p-(k+1)}a^k\big]\\
&=\sum_{k=0}^{p-1}\big[b^{p-(k+1)+1}a^k- b^{p-(k+1)}a^{k+1}\big]\quad \Leftarrow \small{\text{Distribute }(b-a).}\\  
&=\sum_{k=0}^{p-1}\big[b^{p-k}a^k- b^{p-(k+1)}a^{k+1}\big]\\ 
&=b^{p-0}a^0- b^{p-[(p-1)+1]}a^{(p-1)+1}\quad\Leftarrow \small{\text{Apply the telescoping property for sums.}}\\
&= b^p - a^p\end{split}$$ (b) This is my attempt: $$n^p < \frac{(n+1)^{p+1}-n^{p+1}}{p+1} < (n+1)^p$$ By multiplying by $p+1$ we have $$(p+1)n^p < (n+1)^{p+1}-n^{p+1} < (p+1)(n+1)^p$$ which can also be written as $$(p+1)n^p < [(n+1)-n]\sum_{k=0}^{p}(n+1)^{p-(k+1)}n^{k} < (p+1)(n+1)^p\\
(p+1)n^p < \sum_{k=0}^{p}(n+1)^{p-(k+1)}n^{k} < (p+1)(n+1)^p$$ By dividing the inequalities by $(n+1)^p$ we get $$(p+1)\left(\frac{n}{n+1}\right)^p < \sum_{k=0}^{p}\frac{n^k}{(n+1)^{k+1}} < (p+1)$$ Here I'm stuck. I guess the last step wasn't necessary. Edit: This is my last attempt. Hopefully it's not flawed. We will prove each bound separately. To prove: $(p+1)n^p < (n+1)^{p+1}-n^{p+1}$ Proof (direct): Let the numbers $a$ and $b$ be defined as $$a = n \quad \text{and}\quad b = n+1\qquad \text{For }n \in \mathbb{N}.$$ Then we have $$a < b$$ And it follows $$\frac{a^p}{a^k} < \frac{b^p}{b^k}\quad \text{For } p\ \text{and }k\in\mathbb{N}, p \neq k. \qquad (1)$$ By multiplying both sides by $a^k$ we get $$a^p < b^{p-k}a^k$$ By taking the sum of both sides we have $$\sum_{k=0}^pa^p < \sum_{k=0}^pb^{p-k}a^k$$ On the LHS $a^p$ is summed $p+1$ times (from $0$ to $p$). So it can also be written as $$(p+1)a^p < \sum_{k=0}^pb^{p-k}a^k$$ Since $b-a = 1$, let multiply the RHS by $b-a$ $$(p+1)a^p < (b-a)\sum_{k=0}^pb^{p-k}a^k$$ By distributing $b-a$ inside the sum we have $$(p+1)a^p <\sum_{k=0}^p[b^{p-(k-1)}a^k - b^{p-k}a^{k+1}]$$ And by applying the telescoping property for sums we get $$\begin{align*}(p+1)a^p &< b^{p-(0-1)}a^0 - b^{p-p}a^{p+1}\\
(p+1)a^p &< b^{p+1} - a^{p+1}\\
(p+1)n^p &< (n+1)^{p+1} - n^{p+1}\end{align*}$$ which completes the proof. To prove: $(n+1)^{p+1}-n^{p+1} < (n+1)^p$ Proof (direct): By the definition of $a$ and $b$ previously given and from inequality $(1)$ we have $$\frac{a^p}{a^k} < \frac{b^p}{b^k}$$ Let multiplying both sides by $b^k$ $$a^{p-k}b^k < b^p$$ Let take the sum of both sides $$\sum_{k=0}^pa^{p-k}b^k < \sum_{k=0}^pb^p$$ On the RHS $b^p$ is summed $p+1$ times. Then $$\sum_{k=0}^pa^{p-k}b^k < (p+1)b^p$$ Let multiply the LHS by $1 = b-a$ $$\begin{align*}(b-a)\sum_{k=0}^pa^{p-k}b^k < (p+1)b^p\\
\sum_{k=0}^p[a^{p-k}b^{k+1}-a^{p-(k-1)}b^k] < (p+1)b^p\end{align*}$$ And by applying the telescoping property for sums we get $$\begin{align*}a^{p-p}b^{p+1}-a^{p-(0-1)}b^0 &< (p+1)b^p\\
b^{p+1}-a^{p+1} &< (p+1)b^p\\
(n+1)^{p+1}-n^{p+1} &< (p+1)(n+1)^p\end{align*}$$ which completes the proof. It looks like I can go to sleep without remorse, doesn't it? (:","['inequality', 'algebra-precalculus', 'proof-verification']"
1143440,Does convergence in probability imply a.s. convergence in a countable space?,"Let $(\Omega, \mathcal F,\mathbb P)$ be such that $\Omega$ is countable. I'm trying to find a simple example of random variables $X_n$ which converge to $0$ in probability but not a.s. If $\mathcal F = 2^{\Omega}$ (i.e., $\{\omega\} \in \mathcal F$ for each $\omega$ since $\Omega$ is countable), it is shown here that such random variables do not exist. But now we only assume (of course) that the $X_n$ are measurable. This question is essentially the same (but the measure there need not be finite).","['probability-theory', 'measure-theory', 'convergence-divergence', 'real-analysis']"
1143459,The idea behind the notion of dualizing sheaf,"Well, studying sheaf cohomology, I've faced the notion of dualizing sheaf on a projective scheme over a field $k$ . Recall that a dualizing sheaf on $X$ (according to Hartshorne) is a coherent sheaf $\omega_X^\circ$ , such that the composition $
Hom(\mathscr{F},\omega_X^\circ)\times H^n(X,\mathscr{F})\to H^n(X,\omega_X^\circ)\overset{t}{\longrightarrow}k
$ of the natural pairing with the trace homomorphism $t$ induces an isomorphism $
Hom(\mathscr{F},\omega_X^\circ)\cong H^n(X,\mathscr{F})^*.
$ Although I formally understand the definition (and the proof of its existence), it seems quite mysterious to me (trace homomorphism, in particular). As I can guess, we want to define some analog of canonical sheaf for singular schemes. Anyway, what is a motivation for introducing such a definition? Are there simple examples (maybe explicit calculations) when we have to deal with the dualizing sheaf instead of the canonical one? P.S. By the way, I know that in complex-analytical settings there is a way to define the canonical sheaf on a normal variety starting from defining it on a nonsingular part. Maybe there is some connection between these two approaches?","['sheaf-theory', 'algebraic-geometry', 'sheaf-cohomology']"
1143462,one-sided differentiability,"Well known theorem: If $f\colon\mathbb{R}\to\mathbb{R}$ is differentiable and $f'(x)=0$ for all $x$, then $f$ is constant. The assumption of differentiability can be weakened to continuity and one-sided differentiability: If $f\colon\mathbb{R}\to\mathbb{R}$ is continuous and for every $x$ function $f$ is right differentiable at $x$ and the right derivative equals 0, then $f$ is constant. This is also a known fact, and the same holds when ""right"" is replaced with ""left"". I wonder if we could make it even stronger: If $f\colon\mathbb{R}\to\mathbb{R}$ is continuous and for every $x$ left or right derivative exists and equals 0, then $f$ is constant? (in my version the sides can be different for different points -- this is the difference between my conjecture and the theorem)",['real-analysis']
1143507,How to prove the 2nd & 3rd conditions of outer measure?,"I have this question on outer measure from Richard Bass' book: Prove that $\mu^*$ is an outer measure, given a measure space $(X, \mathcal A, \mu)$ and define $$\mu^*(A) = \inf \{\mu(B) \mid A \subset B, B \in \mathcal A\}$$ for all subsets $A$ of $X$. Here are what I have gone so far: (1) The first condition is the easiest one: 
$$\begin{align}
\mu^*(\emptyset) &= \inf \{\mu(B) \mid \emptyset \subset B, B \in \mathcal A\}\\
&= \mu (\emptyset) \\
&= 0
\end{align}$$ (2) Now the second condition. Let $D, E \in X$ and $D \subset E$,
$$\begin{align}
\mu^*(D) &= \inf \{\mu(D') \mid D \subset D', D' \in \mathcal A\}\\
\mu^*(E) &= \inf \{\mu(E') \mid E \subset E', E' \in \mathcal A\}\\
\end{align}$$
Here I need to prove $\mu^* (D) \leq \mu^*(E)$. It looks to me so intuitive especially if I draw Venn diagrams of $D, E, D'$ and $E'$, but I don't know how to say it in math-speak. I would appreciate helps on this 2nd. condition. (3) And this 3rd. condition is my major stumbling block: Given $(A_i)_{i \in \mathbb N} \subset X$, I need to arrive at $$\mu^* (\bigcup _{i=1}^{\infty} A_i)\leq \sum_{i=1}^{\infty} \mu^* (A_i).$$ Here, I know for sure I need to  state this first: $\forall A_i, \exists B_i $ such that $ A_i \subset B_i, B_i \in \mathcal A$, but I don't think the next step is right: $$\begin{align}
\mu^*(\bigcup_{i=1}^{\infty}A_i) &= \inf \{\bigcup_{i=1}^{\infty}\mu(B_i) \mid A_i \subset B_i, B_i \in \mathcal A\}\\
&= \ldots\\
\end{align}$$ I would appreciate any help on this 3rd. condition in addition to the 2nd. above. Thank you for your time and effort. POST SCRIPT: After I posted this question, I found this proposition on the same text, perhaps this proposition holds key to the solution, in that I don't have to prove the 2nd and 3rd conditions? Thanks again. Proposition : Suppose $\mathcal C$ is a collection of subsets of $X$ such that $\emptyset$ and $X$ are both in $\mathcal C$. Suppose $\mathscr l : \mathcal C \to [0, \infty]$ with $\mathscr l (\emptyset) = 0$. Define $$\mu^* (E) = \inf \{ \sum_{i=1}^{\infty} \mathscr l (A_i) \mid A_i \in \mathcal C \text{ for each } i, \text{and }  E \subset \cup_{i=1}^{\infty} A_i \}.$$
  Then $\mu^*$ is an outer measure.","['measure-theory', 'real-analysis', 'analysis']"
1143522,Is there a fiber bundle approach to nonlinear oscillations?,"I've recently been learning about nonlinear oscillations, and I noticed a seemingly strong connection between how the equations of motion are solved/approximated, and fiber bundles (or vector bundles considering the systems use vectors). For instance: in general when looking at a nonlinear system, you usually only consider a small localization around a minimum that allows the equations of motion to be approximated as linear differential equations; however, the global system is in general nonlinear, so I figured it's like dealing with a local trivialization. But looking online I couldn't seem to find anything relating the two or giving a fiber bundle approach. I thought it might be interesting, or maybe shed light on understanding nonlinear oscillations. Though I've been told that almost all of these systems have no mathematical solutions (only numerical approximations), so is it maybe that this approach has been tried, but it didn't give anything new?
Or is there a fiber/vector bundle approach, but I'm just not looking in the right places?","['dynamical-systems', 'nonlinear-system', 'ordinary-differential-equations', 'vector-bundles', 'fiber-bundles']"
1143538,limit of a sequence $\ln n\cdot(\sin n)^2$,"It's easy to see that $\lim\limits_{x\to \infty}\ln x\cdot(\sin x)^2$ does not exist (sin can take 0 and 1 as values, so liminf is $0$, limsup is $\infty$). how about the limit of a sequence: $\lim\limits_{n\to \infty}\ln n\cdot(\sin n)^2$ ?","['real-analysis', 'limits']"
1143545,Uniformly Random Tuples,"Consider a multiset of natural numbers. As an example take $$ M = \{1, 2, 2, 3, 3, 3\} $$ If we treat copies of the same number as indistinguishable, there are 8 distinct 2-tuples we can form from this, by not using any number more often than it appears in $M$: $$ \{ (1,2), (2,1), (1,3), (3,1), (2,3), (3,2), (2,2), (3,3) \} $$ Is there an algorithm which generates one of these tuples with uniform probability, without having to generate all the tuples up front and selecting one at random? Note that the uniformity is in reference to the list of tuples, so $(3,3)$ should appear with the same probability as $(2,2)$ or $(1,2)$, respectively. The algorithm should work for arbitrary non-empty $M$ and $n \leq |M|$, where the goal is to generate $n$-tuples. Ideally, I'm looking for an algorithm that is not based on rejection, but if that is not possible, I'd also be interested in how I can minimise the number of necessary rejections to generate the tuples efficiently. If special cases (i.e. small, fixed $n$) yield good algorithms, I'd still be interested in those, even if they don't generalise to larger $n$.","['multisets', 'random', 'probability', 'algorithms']"
1143546,Are $L^\infty$ bounded functions closed in $L^2$?,"Is the set $\{ m \in L^2(0,1) : |m|_{L^\infty}\leq A \}$, (i.e. the set of $L^2$ functions with bounded $L^\infty$ norm) a closed subset of $L^2$? (Closed in the topology induced by the $L^2$-norm)","['general-topology', 'lp-spaces', 'functional-analysis', 'functions']"
1143562,"Let $a,b$ be positive integers such that $a\mid b^2 , b^2\mid a^3 , a^3\mid b^4 \ldots$ so on , then $a=b$? [duplicate]","This question already has answers here : Proving $\,a\mid b^2\mid a^3\mid b^4\mid a^5\ldots\implies a=b$. (2 answers) Closed last year . Let $a,b$ be positive integers such that $a\mid b^2 , b^2\mid a^3 , a^3\mid b^4 \ldots$ that is $a^{2n-1}\mid b^{2n} ; b^{2n}\mid a^{2n+1} , \forall n \in \mathbb Z^+$ , then is it true that $a=b$ ?","['elementary-number-theory', 'divisibility', 'number-theory']"
1143570,"Given two diagonally opposite points of a rectangle, how to calculate the other two points","If point A($x_1,y_1$) and C($x_3,y_3$) are given i have to find points B($x_2,y_2$) and D($x_4,y_4$),if points B and D are given i need to find point A and C. Edges of rectangle may not be parallel to axes",['geometry']
1143575,"Prove that $f$ analytic, $f(x) \in \mathbb{R}$ for all $x \in \mathbb{R}$ implies $f(\overline{z})=\overline{f(z)}$","Let $U\subset \mathbb{C}$ be a nonempty connected open set such that for every $z\in U$, $\overline z\in U$. Let $f$ be analytic on $U$. Suppose $f(x)\in\mathbb R$ for every $x\in U\cap\mathbb R$. Prove that $f(\overline{z})=\overline{f(z)}$ for any $z \in U$. By definition, I know that $f$ analytic on $U$ means that for every $z_0 \in U$, there exists $r>0$ and a sequence of complex numbers $\left(a_n\right)_{n=0}^\infty$ such that $f(z)=\sum_{n=0}^\infty a_n\left(z-z_0\right)^n$ on the disc $D(z_0,r)\subseteq U$. I see that $f$ takes points without imaginary components to other points without imaginary components. But I don't see how this implies a symmetry that $f(\overline{z})=\overline{f(z)}$ for $z \in \mathbb C \setminus \mathbb R$. It seems like a very strong conclusion and I'm not sure how I would prove it. I have also shown in the preceding question that $U\cap\mathbb R$ contains an open interval, however I am not sure if that detail is meant to be helpful to this question.","['analyticity', 'complex-analysis']"
1143597,Derive a Recurrence,"Could really use some help with this. For an integer $m \geq 1$ and $n \geq 1$, consider $m$ horizontal lines and $n$ non-horizontal lines, such that no two of the non-horizontal lines are parallel and no three of the $m+n$ lines intersect in one single point. These lines divide the plane into regions (some of which are bounded and some of which are unbounded). Denote the number of these regions by $R_{m,n}$. For example, $R_{4,3} = 23$. Derive a recurrence for the numbers $R_{m,n}$ and use it to prove that $$R_{m,n} = 1 + m(n+1) + \binom{n+1}{2}$$","['recurrence-relations', 'discrete-mathematics']"
1143614,Is matrix transpose a linear transformation?,"This was the question posed to me. Does there exist a matrix $A$ for which $AM$ = $M^T$ for every $M$ . The answer to this is obviously no as I can vary the dimension of $M$ . But now this lead me to think , if I take , lets say only $2\times2$ matrix  into consideration. Now for a matrix $M$ , $A=M^TM^{-1}$ so $A$ is not fixed and depends on $M$ , but the operation follows all conditions of a linear transformation and I had read that any linear transformation can be represented as a matrix. So is the last statement wrong or my argument flawed?","['linear-transformations', 'matrices', 'linear-algebra']"
1143649,Graph with Cycle and Two-Colorable,"i think if the graph G has an odd cycle, it's not two-colorable, otherwise it can be two colorable. i read in one notes that the following is True: we couldent two-colorable any graph G that has cycle. anyone could clarify me ?","['graph-theory', 'computer-science', 'discrete-mathematics', 'coloring']"
1143738,"If $B$ is an algebra, $X \subseteq B$ and $A$ the smallest subset that extends $X$, then $A= \bigcup_{n < \omega} A_n$","I need to prove the next thing: Let $\textbf{B} = \langle B, (b_i)_{i \in I}, (g_j)_{j\in J} \rangle$. Let $X \subseteq B$ and let $A$ be the smallest subset of $B$ which extends $X$ and is closed in $\textbf{B}$. Then $A= \bigcup_{n < \omega} A_n$ where $A_0 = X \cup \{ b_i : i \in I\}$ and $A_{n+1}= A_n \cup \bigcup_{j \in J}g_j[A^{n_j}_n]$. So, I know I need to prove both inclusions. For the first one, $ \bigcup_{n < \omega} A_n \subseteq A$. I need to do it by induction, $A_n \subseteq A$. Let $A_0 = X \cup \{ b_i : i \in I\}$, but I don't know how to justify that it is. I mean, it ""clearly"" is a subset, because of the way it is constructed, but I don't know how to formally explain that, I would really appreciate the help. 
Now, As inductive hypothesis, assume $A_n \subseteq A$. As $A_\{n+1\}= A_n \cup \bigcup_{j \in J}g_j[A^{n_j}_n]$. Again, I have the notion: I know that $A_n$ is a subset by hypothesis, and I know that the other elements are also part of the subset because of the form it has, but again, I am stuck when I try to write it in a formal way, and I feel like I might be missing details. For the other part of the inclusion, I have that $A \subseteq \bigcup_{n < \omega} A_n$. As $\bigcup_{n< \omega} A_n$ is a closed subset of $B$, and it contains $X$, it implies that $A \subseteq \bigcup_{n< \omega} A_n$, but in this case I know I'm missing on the details, but I have issues figuring them out. I would really appreciate your help in working this proof out. I'm working on improving my writing abilities so I really appreciate input on how to express things the best way possible. Thank you very much.","['elementary-set-theory', 'universal-algebra']"
1143769,Find the three digit number whose sum of 5 Permutations is 3194,Let $N={abc}$ be Three Digit Number such that $$abc+bca+bac+cab+cba=3194$$ Find the Number My Try: I added both sides the left over number $acb$ both sides Then we get $$222(a+b+c)-3194=b+10c+100a$$ Help needed from here,"['elementary-number-theory', 'number-theory']"
1143821,Should I have a repeated digit in a $4$-digit pin code?,"A small argument my colleague and I were having... Say I've got a $4$-digit security code on my burglar alarm, and I'm too lazy to ever change it.  Over time, the digits involved will become worn. If I've used $4$ different digits (as is common, eg $1357$), then an attacker, knowing those are the digits in the code, needs (if I'm right!) $24$ goes to try all combinations. If I've repeated a digit (eg, $1317$), the attacker just knows $1$, $3$ and $7$ are used (assume that all wearing is indistinguishable).  How many different possible $4$-digit codes are possible, using each digit at least once?  I say it's more than $24$, my colleague is convinced otherwise. Bonus kudos for generalizing the problem to knowing $k$ digits in an $n$-digit pin. ;)",['combinatorics']
1143845,How to find feasible set of $a_1 \cos x_1 + a_2 \cos x_2 + \cdots + a_n \cos x_n \ge 0$,"I'm looking to describe the feasible set of $x_1, \ldots, x_n$ for the following inequality: $$a_1\cos x_1 + a_2\cos x_2 + \cdots+ a_n\cos x_n \ge 0$$ For variables $x_1, \ldots, x_n$ restricted to domain $-k\pi \le x_i \le k\pi$ , $k \ge 1$ an integer, and $a_i$ are real numbers. Is this even possible to describe analytically? EDIT - I should mention that numerical methods would be very welcome as well. For example, if the problem could somehow be split up into finding $x$ satisfying the intersection of a bunch of convex sets that would be fantastic.","['trigonometry', 'inequality']"
1143878,Find polygon with smallest perimeter that encompasses all points,Given a random set of points in 2D space such as: How would one go about finding the smallest perimeter polygon that encompasses all points and has a point as each one of its vertices? For the above diagram the polygon would be:,"['general-topology', 'geometry', 'computational-geometry']"
1143881,Difference between Real Analysis and Probability Theory?,"I do not really see a big difference between the two subjects. I was wondering if somebody can explain what the big difference between them is. Let us compare the superficial differences: In real analysis our subsets are called ""measurable sets"", in probability our subsets are called ""events"". The measure of a set in analysis is called the ""measure"", while in probability it is called ""probability"". In real analysis we deal with ""measurable functions"", in probability theory we deal with ""random variables"". In probability theory random variables induce ""distributions"", while in real analysis they are more naturally called ""push-forwards"". In analysis we ""integrate"" with respect to the measure, in probability we compute the ""expected value"". In analysis we say ""almost everywhere"" in almost every theorem, and in probability we say ""almost surely"" in almost every theorem. There is one major difference: Probability theory assumes that we have a finite measure normalized to be equal to 1. Other than that last part everything else seems to be essentially the same. It is the ""finite measure assumption"" which makes probability theory ""work"". The only difference that I see is that, analysis is more general than probability theory. In mathematics we often require more generality with a compromise of some of its theorems. Is there something more?",['probability']
1143925,Precise limit definition,"Prove: $$\displaystyle \lim_{x \to 2} \frac 1x = \frac 12.$$ We need to find a $\delta$ in terms of $\epsilon$. Here is what I did so far:
\begin{align}
\left|\frac 1x-\frac 12 \right| &< \epsilon \\
-\epsilon < \frac 1x-\frac 12 &< \epsilon \\
-\epsilon + \frac 12< \frac 1x&< \epsilon + \frac 12 \\
\frac 2{1+2\epsilon} < x &< \frac 1{1-2\epsilon}
\end{align} I am having trouble with this last step. Did I do this correctly?","['epsilon-delta', 'calculus', 'limits']"
1143951,What do these sets mean? Relational Algebra,"I'm in a databases class and this homework is due next week. I have been home sick for a couple days so I can't go to class to ask this question right now and I'd hate to waste time, so hopefully someone can help me out on here. I'm a little confused, this is problem #1: Problem1 [15′ = 5′ ∗ 3] Given the following relations $R$ and $S$, where $R(A,B,C) =((x,y,z); (j,g,s);(y,x,g); (q,w,e))$ and $S(B,C,D) = ((g,s,r); (y,x,g); (r,q,e))$ compute the following operations: Cartesian product: $R\times $S; Natural join: $R \bowtie S$; Equal join: $R \bowtie R.B=S.B S$ First: Is there a significance to the [15' = 5' * 3] next to the problem number? each problem has something like that and I'm not sure what's going on or if my teach is being silly. Second: What is with the $(A,B,C)$ behind $R$? does it relate to the sets on the right side of the equal sign? I took discrete math and I feel confident I can do this work easily enough, but I have a feeling I'm missing out on some key concepts. If I'm totally wasting time and it'd be too much to try to explain, could someone point me in the direction of a solid tutorial that can explain how this format works? I've been reading the textbook diligently, but the lectures haven't been following the book content very closely it seems.","['relation-algebra', 'elementary-set-theory']"
1143955,Limit of $\frac{1}{\sqrt[n]{n!}}$ as $n$ approaches infinity [duplicate],"This question already has answers here : $\lim\limits_{n \to{+}\infty}{\sqrt[n]{n!}}$ is infinite (12 answers) Closed 9 years ago . So i was trying to evalue this limit:
$$\lim_{n \to \infty}\frac{1}{\sqrt[n]{n!}}, n \in \mathbb{N}$$ This, of course, by common sense is equal to zero (since factorial grows a lot faster). Is there a way to prove this limit without having to tackle with proving function growth rate. I;m not sure how that would be done, but I believe i would have to expand the factorial function to set $\mathbb{R}$ in order to compare, and that's still beyond my abilities. Thanks.",['limits']
1143964,"Computing $\lim_{n \to \infty} \int_0^{n^2} e^{-x^2}n\sin\frac{x}{n}\,dx$?","I am trying to compute this integral/limit, I don't feel like I have any good insight... $$\lim_{n \to \infty} \int_0^{n^2} e^{-x^2}n\sin\frac{x}{n} \, dx.$$ I have tried to make a change of variable to get rid of the $n^2$, I changed to $X=\frac{x}{n^2}$ but got something even worse, I've tried to reach a situation where I could use a convergence theorem for Lebesgue Integrals,... I'm not sure I'm even on the right track! Could you give me a hint on how to start this? Thank you very much!","['lebesgue-integral', 'integration', 'limits']"
1143965,When does a generic point map to a generic point?,"This question may be too vague, so feel free to specialize to particular examples. Given a morphism of schemes $f:X\to Y$, I want to know what conditions one can impose on $f,X$ or $Y$ such that a generic point of $X$ will map to a generic point of $Y$. For example, if $X$ and $Y$ are irreducible etc.",['algebraic-geometry']
1144040,Intuitive interpretation of the alternative definition of sub modular functions,"I know that the the following definition is the ""usual"" definition of sub modularity: $$ \forall A \subseteq B , s \notin B, F(A \cup \{s\}) - F(A) \geq F(B \cup \{ s \}) -F(B)$$ Which for me has a very clear intuitive interpretation. Adding an additional element to the larger set in question, does not provide as much marginal returns as it does adding it to the smaller set. i.e. ""there are diminishing returns when making a set bigger and bigger"". Thats, kind of what it means intuitively. However, there is an alternative definition: $$\forall A,B \subseteq V : F(A) + F(B) \geq F(A \cup B) + F(A \cap B)$$ however, I was not sure what to make of that definition. Why is that alternative definition ""obviously"" the same as the original? Is there an intuitive way to express the second definition such that its obvious that its equivalent to the first? How would some one interpret this second definition (and if you can, how would you link it with the interpretation of ""diminishing returns"")? Notice that this question is not asking for a rigorous proof of equivalence for the two definitions. There is already a question about that. Proving equivalence of different definitions of sub modular functions","['machine-learning', 'functions', 'combinatorics']"
1144064,Understanding Hartshorne's proof that every projective morphism is proper.,"In chapter $II$ of Hartshorne, theorem $4.9$ shows that every projective morphism is proper, using the valuative criterion for properness. I understand how the required morphism is constructed, but I don't understand why it is unique. I would greatly appreciate an explanation explaining why ""The uniqueness of this morphism follows from the construction and the way the $V_i$ patch together."" I can see why any morphism with image lying inside $V_k$, the affine open piece $D_+(X_k)$, is unique but I don't see why it should be the case that the image of any morphism of the correct form should lie inside $V_k$.","['algebraic-geometry', 'schemes', 'projective-schemes']"
1144072,From $\frac{1-\cos x}{\sin x}$ to $\tan\frac{x}{2}$,How can I write $\frac{1-\cos x}{\sin x}$ as $\tan\frac{x}{2}$? I wrote $\sin x$ as $2\sin\frac{x}{2} \cos\frac{x}{2}$ also used the double angle identity for $\cos$ but wasn't able to make much progress,['trigonometry']
1144088,Finding angles in spherical triangle using law of cosines,"Problem: Assume that the earth is a sphere of radius $5280$ miles, find the length of the sides, the measure of the angles and the area of the spherical triangle with vertices $A(70°N,10°E)$,$B(10°S,100°E)$ and $C(50°S,80°W)$. The earth's radius will be used as the unit of length. The spherical coordinates $(r,v,u)$ of the three vertices are $(1,10,20)$, $(1,100,100)$ and $(1,−80,140).$ Their Cartesian coordinates are: $$(\sin(20°)\cos(10°),\sin(20°)\sin(10°),\cos(20°))=(0.3368,0.0594,0.9397)$$
$$(\sin(100°)\cos(100°),\sin(100°)\sin(100°),\cos(100°))=(−0.1710,0.9698,−.1736)$$
$$(\sin(140°)\cos(−80°),\sin(140°)\sin(−80°),\cos(140°))=(0.1116,−0.6330,−.7660)$$ The cosines of the angles between radii $OA,$ $OB$ and $OC$ are equal to the dot product of the corresponding position vectors. Thus: $$∠AOB=\cos^{−1}(-0.1631)=1.7347 \ \mathrm{radians}$$ $$∠BOC=\cos^{−1}(-0.5000)=2.0944 \ \mathrm{radians}$$ $$∠COA=\cos^{−1}(-0.7198)=2.3743 \ \mathrm{radians}$$ Since the length of the arc of a circle is the product of its radius by the radian measure of its central angle, it follows that the length of the sides of the spherical triangle $ABC$ are: $9,159$ miles, $11,058$ miles, and $12,536$ miles. The angle of this triangle are derived by means of the appropriate Law of cosines. Thus: !This is where I begin to have problem understanding the question!! $$\cos A = \frac{\cos(2.0944) - \cos(1.7346) \cos(2.3743)}{\sin(1.7346) \sin(2.3743)}
 = -0.9013$$ So $A=154°=2.6941$ radians Similarly, $B=160°=2.7874$ radians $C=150°=2.6261$ radians So, my question is how did they calculate $B$ and $C$ to be $160°$ and $150°$? The book says: $$\cos A = \frac{\cos(a)-\cos(b) \cos(c)}{\sin(b) \sin(c)}.$$ Therefore, I know how they calculated A. So i'm guessing they did $B$ and $C$ the same way but I dont get the same answer. SO 
$$a=2.0944$$ $$b=1.7346$$ $$c=2.3743$$ So if I wanted to calculate $B$: $$\cos B = \frac{\cos(b)-\cos(a) \cos(c)}{\sin(a) \sin(c)}.$$ $$\cos B = \frac{\cos(1.7346)-\cos(2.0944) \cos(2.3743)}{\sin(2.0944)\sin(2.3743)}.$$ and I get $B=45°$ not $160°$ I'm not sure where I went wrong.","['trigonometry', 'spherical-trigonometry', 'spherical-geometry']"
1144096,Solving a partial differential equation by transformation of variables?,"I found an exercise in a book, where one was supposed to transform the differential equation $$y\frac{\partial f}{\partial x}-x\frac{\partial f}{\partial y}=xyf(x,y)$$ by using the substitutions $x^2+y^2=u$ and $e^{-x^2/2}=v$, and then expressing $\frac{\partial f}{\partial x}$ and $\frac{\partial f}{\partial y}$ in terms of $\frac{\partial f}{\partial u}$ and $\frac{\partial f}{\partial v}$. By using the chain rule and simplifying things, I get that $$-xyv\frac{\partial f}{\partial v}=-2xyf(x,y).$$ Now the ""solutions manual"" for this book just divides this with $xy$ on both sides. I do not really know why one can do that. By doing that, we can only say things about the function $f$ off the cordinate axes. Does anyone see anything obvious that I'm missing?","['multivariable-calculus', 'partial-differential-equations']"
1144102,Calculating a rate with a single summation,"I have a $n$ events, each with some value $x$ and duration $t$. I can calculate the global rate as follows:
$$\text{rate} = \frac{\sum_{i=1}^nx_i}{\sum_{i=1}^nt_i}$$ Is it possible (or provably impossible) to calculate a global rate with a single summation over the events:
$$\text{rate} = \sum_{i=1}^nf(x_i, t_i)\text{ ?}$$","['summation', 'algebra-precalculus']"
1144165,Solve $x^2 y''+(-2x-x^3)y'+5y=0$,"Ok so for me I am having trouble solving this equation to get $y=C_1y_1+C_2y_2$, but I'm having trouble dealing with the (-2x-x^3) part.  Usually I would isolate $y''$, then make $y=x^m$, then go from there, but I'm having trouble dealing with the $(-2x-x^3)$ part.  Thanks! When I tried that approach, I got $[x^{3/2}x^{x^2/2}x^{(\sqrt{x^2+6x-11})/2}, x^{3/2}x^{x^2/2}x^{-(\sqrt{x^2+6x-11})/2}]$ form a basis, but it just seems too complicated when I need to solve $y_1(1)=-4, y'_1(1)=5, y_2(1)=-5,y'_2(1)=5$ The point is to find the Wronskian, $w(x)=y_1(x)y'_2(x)-y_2(x)y'_1(x)$ for x>0","['ordinary-differential-equations', 'calculus']"
1144180,Number of ways to play $n$ games without losing three in a row,"So I'm trying to solve this problem but I'm having a few difficulties. A gambler decides to play successive games of blackjack until he loses three times in a row. (Thus the gambler could play five games by losing the first, winning the second, and losing the final three or by winning the first two and losing the final three.) These possibilities can be symbolized as LWLLL and WWLLL. Let $g_n$ be the number of ways the gambler can play $n$ games. Explain your answer as well. a. Find $g_3$ , $g_4$ , and $g_5$ . b. Find $g_{10}$ . When it's asking for $g_3$ for example, Am I supposed to come up with $3$ scenarios in which the win/lose events could happen?","['discrete-mathematics', 'combinatorics']"
1144206,Is $A$ nilpotent matrix?,"Let $A$ be a $n \times n$ matrix over $\mathbb{C}$ and $I+At$ is invertible for all t, does that imply $A$ is nilpotent? I know the converse is true, but is it also true?","['matrices', 'nilpotence']"
1144211,Prove: f is surjective -->$ f(f^{-1}(S)) = S$,"I have to prove this exercise for my math-study: Let $f: X \rightarrow Y$ be a function and $S \subseteq Y$ Prove: $f$ is surjective $\Rightarrow$ $f(f^{-1}(S)) = S$ I divided this exercise in two parts,
first proving that $S \subseteq f(f^{-1}(S))$.
This is what I did: Assume $f$ is surjective $\Rightarrow$ $\forall s$  $\in S$ $\exists x \in$ $f^{-1}(S)$ such that $f(x) = s \Rightarrow s$ $\in$ $f(f^{-1}(S))$ $\Rightarrow$ $$S \subseteq f(f^{-1}(S))$$ Is this part right, or did I make any mistakes? For the second part, I have to prove that $f(f^{-1}(S)) \subseteq S$ I began with this: Assume $x$ $\in$ $f(f^{-1}(S))$. $f^{-1}(S)$ = {$x$ $\in$ X | $f(x)$ $\in$ S} But I don't know how to prove from that that $x \in S$. Could you please help me with these two questions? Thanks in advance!","['elementary-set-theory', 'analysis']"
1144281,"Why is the probability of a dice being larger then the 2nd roll the same 2nd,3rd?","I understand that the probability of the first die roll being larger then the second is 5/12. I'm having a harder time understanding why, according to a small problem I wrote, the probability of the first die being larger on the first roll then the next two rolls is still 5/12. It would seem to me that these would be two events. p(a) = 5/12 and p(b) = 5/12. p(a and b) would then be p(a) * p(b) I would think, considering these are independent events?","['discrete-mathematics', 'statistics', 'dice', 'probability-distributions', 'probability']"
1144301,Is information entropy $H(X)$ a sub modular function?,"I was trying to learn more about sub modular functions and wanted to see an example of proving that some function is sub modular. Wikipedia said that Entropy was an example so I decided to try it out to see if it really was sub modular (since they didn't provide a proof). I will provide the proof I have however, I am not entirely satisfied with my proof and was wondering if it was correct or if someone had a better proof for it. Lets say we have $n$ discrete random variables $X_1, ... ,X_n$ and let $\mathcal{X}$ be the alphabet of values that $X_i$ can take. Let me denote $X_A$ as the set of random variables such that $A \subseteq \{1,...,n \}$. Recall information entropy of a set of random variables to be: $$ H(X_A) = \sum_{a \in A} \sum_{x_a \in \mathcal{X}} p_x(x_a) \log\big( \frac{1}{p_x(x_a)} \big)$$ Now lets define our function $F(A) = H(X_A)$. If F is sub modular then it should satisfy: $$ \forall A \subseteq B , s \notin B, F(A \cup \{s\}) - F(A) \geq F(B \cup \{ s \}) -F(B)$$ Lets check that property. Hence, consider $A \subseteq B$ and $s \notin B$. Thus we have: $$F(A) = H(X_A)$$ $$F(B) = H(X_B)$$ $$ F(A \cup \{s\}) = H(X_{A \cup \{s\} }) = H(X_A) + H(X_s)  $$ $$ F(B \cup \{s\}) = H(X_{B \cup \{s\} }) = H(X_B) + H(X_s)  $$ Now lets compute the marginal returns: $$ F(A \cup \{s\}) - F(A) = H(X_A) + H(X_s) - H(X_A) = H(X_s) $$ Similarly: $$ F(B \cup \{s\}) - F(B) = H(X_B) + H(X_s) - H(X_B) = H(X_s) $$ since the RHS is equal to the LHS, always, then, the property of sub modularity is satisfied. Is that correct? It seemed to easy and being the case that they are always equal seemed pretty dissatisfying. It might be correct, but I am not sure.","['alternative-proof', 'proof-verification', 'machine-learning', 'combinatorics']"
1144304,Why doesn't $E[E[X|\mathcal{F_n}]]=E[X]$ work for $X=(Z_{n+1}-Z_n)^2$?,"Let $(\Omega,\mathcal{A},P)$ be a probability space. Let $\Bbb F=(\mathcal{F}_n)_n$ be a filtration wrt this space and $(Z_n)_n$ an $\Bbb F$-martingale. Now, basic properties of the conditional expectation tell us that
$$
X\in\mathcal{L}^1(\Omega,\mathcal{A},P)\Rightarrow E[E[X|\mathcal{F_n}]]=E[X]
$$
right? Why doesn't this work for $X=(Z_{n+1}-Z_n)^2$? I wrote
$$
E[E[(Z_{n+1}-Z_n)^2|\mathcal{F_n}]]=E[(Z_{n+1}-Z_n)^2]\;\;\;\;\;\;\;\;(*)
$$
but when I checked I saw it's not correct and I can't see where the mistake is. LHS is
\begin{align*}
E[E[Z_{n+1}^2-2Z_nZ_{n+1}+Z_n^2|\mathcal{F_n}]]
&=E\{E[Z_{n+1}^2|\mathcal{F_n}]-2E[Z_nZ_{n+1}|\mathcal{F_n}]+
E[Z_n^2|\mathcal{F_n}]\}\\
&=E\{E[Z_{n+1}^2|\mathcal{F_n}]-2Z_nE[Z_{n+1}|\mathcal{F_n}]+
Z_n^2\}\\
&=E\{E[Z_{n+1}^2|\mathcal{F_n}]-2Z_n^2+
Z_n^2\}\\
&=E\{E[Z_{n+1}^2|\mathcal{F_n}]-Z_n^2\}\\
&=E\{E[Z_{n+1}^2|\mathcal{F_n}]\}-E[Z_n^2]\\
&=E[Z_{n+1}^2]-E[Z_n^2]\\
\end{align*} which is different from RHS (in fact it's $E[Z_{n+1}^2]-2E[Z_{n+1}Z_n]+E[Z_n^2]$).
Unless we have
$$
E[Z_{n+1}^2]-E[Z_n^2]=
E[Z_{n+1}^2]-2E[Z_{n+1}Z_n]+E[Z_n^2]
$$
that is
$$
E[Z_n^2]=
E[Z_{n+1}Z_n]
$$
and I can't see why it should be true.
Can someone help me please? Thanks a lot! EDIT: all right! I think I understood: the last one must be true precisely because the rule for the conditional expectation I wrote at the beginning is true. It's a consequence. My problem was that I wanted to see the equality directly.
I bump into this problem checking another equality (I didn't wrote it here), and 
when I came to LHS of (*), first I wrote it as RHS, but precisely because I didn't know how to handle $ E[Z_{n+1}Z_n]$ I got stuck. Then I come back to LHS, I developed it as I wrote above and the equality from which all began (the one I didn't wrote) got fixed. But the doubt remained: why did in the second way it worked and in the first one not? Hence I wrote this post and I understood where the problem was. Many thanks guys!","['probability-theory', 'martingales']"
1144357,How can I prove that two vectors in $ℝ^3$ are linearly independent iff their cross product is nonzero?,"Here's my attempt: Let $𝒙 = (x_1, x_2, x_3)$ and $𝒚 = (y_1, y_2, y_3)$ The cross product of $𝒙, 𝒚$ is $𝒙⨯𝒚=(x_2y_3-x_3y_2, x_3y_1 - x_1y_3, x_1y_2 - x_2y_1)$ And linear independence of $𝒙, 𝒚$ means that if $a𝒙 + b𝒚 = 0$, then $a = b = 0$,
i.e.
$a(x_1, x_2, x_3) + b(x_1, x_2, x_3) = 0 ⇒ a = b = 0$ So if cross product is nonzero, then $ x_2y_3-x_3y_2 + x_3y_1 - x_1y_3 + x_1y_2 - x_2y_1 ≠ 0, i.e. x_1(y_2-y_3)+x_2(y_3-y_1) + x_3(y_1 - y_2) ≠ 0$ And then I'm just getting confused. I can't seem to connect the two in a formal proof.","['cross-product', 'linear-algebra']"
1144381,Cummulative Probability 5% chance per iteration,"Let's say you eat at some very cheap restaurant and every time you do there's a 5% chance you'll get food poisoning. How do you calculate the probability of getting food poisoning if you eat there x days in a row? 
Obviously eating there 20 days in a row doesn't give you a 100% chance, but I'm really not sure how that kind of probability works.",['probability']
1144386,How to show $\mathbb{Z}[\sqrt{-5}]_2$ is a UFD?,"I would like to know how to show $\mathbb{Z}[\sqrt{-5}]_2$ is a UFD. I am actually given hints that $\mathbb{Z}[\sqrt{-5}]$ has class group $\mathbb{Z}/2$ and
that $(1 + \sqrt{-5},2)$ is not principal, but I can't quite figure how to use these hints.
Thank you!","['commutative-algebra', 'unique-factorization-domains', 'abstract-algebra', 'number-theory']"
1144392,Evaluating Logarithmic Expressions,Evaluate: $$\log_4 \left(\dfrac{1}{256}\right)$$ I am not sure how to approach this since there is nothing set equal to it.,"['logarithms', 'functions']"
1144400,Stochastic order of Max,"Consider an array $\{\{X_{ni}\}_{i=1}^n\}_{n=1}^\infty$ s.t. for each $i$, $X_{ni}=o_p(n^\alpha)$. What is the order of $M_n=\max_{1\le i\le n}X_{ni}$? I got the following $$P\{|M_n|>n^\alpha\epsilon\}\le \sum_{i=1}^n P\{|X_{ni}|>n^\alpha\epsilon\}$$ So, if $P\{|X_{ni}|>n^\alpha\epsilon\}\in o(n^{-(1+\delta)})$ for some $\delta>0$ then $M_n=o_p(n^\alpha)$ as well. However, can we say anything else? Thank's!","['probability-theory', 'convergence-divergence']"
1144404,"Prove that if an average of a thousand numbers is less than 7, then at least one of the numbers being averaged is less than 7 [closed]","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question I tried proving this by contraposition, by saying, ""If every number that is being averaged is greater than 7, then the average of a thousand numbers is less than 7.""  This seems easier to prove, but I don't know where to go from there.","['logic', 'discrete-mathematics']"
1144470,Finding an unbiased estimator for $\mu^2$,"I'm currently studying my first course in statistics. I am going through some question and have come across one that has stumped me. The question is as follows: Suppose $\bar{X}_1, \ldots, \bar{X}_n$ are $n$ identically distributed independent random variables each with mean $\mu$ and variance $1$. Find an unbiased estimator for $\mu^2$. To my understanding an unbiased estimator for $\mu^2$ is one such that $E[\text{?}] = \ldots = \mu^2$. However, I don't know how to go about solving for $?$ Any help or hints will be greatly appreciated. Thank you for your time.",['statistics']
1144478,Tetrahedron volume relation to parallelepiped and pyramid,"Reading at Mathworld , I came across the subject of tetrahedrons. Particularly calculating the volume with four known vertices. There's a formula which uses the triple product to calculate the volume of a parallelepiped. I'm very aware of why the triple product represents the volume of a parallelepiped. However, I don't understand how to derive the relation that the volume of a tetrahedrons is one-sixth of a parallelepiped. I can accept it, sure. And graphically, I'm sure that it is true. Is there any way to prove it using linear algebra? Wikipedia provides me with one proof. However, I don't understand how they went from pyramid volume being $1/3\,A_0h$ to tetrahedrons being $1/6\,A_0h$. It must mean that the volume of a tetrahedron is half that of a pyramid, must it not?","['vector-spaces', 'geometry', 'linear-algebra']"
1144488,Apparent counterexample to the Picard-Lindelöf theorem,"I was assigned the following differential equation with initial value condition: $$dy/dx=xy^{1/2}, y(2)=1.$$ Then I found out that the following functions, $y(x)=x^4/16$ and $(x^2-8)^2/16$, are solutions to this equation. Why does not this contradict the uniqueness of the Picard-Lindelöf theorem? The statement of the theorem which I'm using is the following: http://www.math.byu.edu/~grant/courses/m634/f99/lec4.pdf Is this formulation wrong?",['ordinary-differential-equations']
1144490,Probability of $\limsup X_n/n$ being zero or infinite,"This is Exercise 5.1.3 from Achim Klenke: »Probability Theory — A Comprehensive Course«. Exercise: Let $X_1, X_2, \ldots$ be i.i.d. nonnegative random variables. By virtue of the Borel–Cantelli lemma, show that
  \begin{equation*}
  \limsup_{n\rightarrow\infty} \frac{1}{n} X_n =
  \begin{cases}
    0 \text{ a.s.}, & \text{ if } \mathbf{E}[X_1] < \infty,\\
    \infty \text{ a.s.}, & \text{ if } \mathbf{E}[X_1] = \infty.
  \end{cases}
\end{equation*} Solution: We show the first row. Let $\varepsilon > 0$. Using that the $X_1, X_2, \ldots$ are nonnegative, we get
\begin{equation*}
  \mathbf{P}\bigl[\limsup_{n\rightarrow\infty} X_n/n < \varepsilon\bigr] = 1 - \mathbf{P}\bigl[\{X_n/n \geq \varepsilon \; \text{ i.o.}\}\bigr] = 1 - \mathbf{P}\Bigl[\limsup_{n\rightarrow\infty} \, \{X_n/n \geq \varepsilon\}\Bigr] \, .
\end{equation*}
Using that $\mathbf{E}[X_1] <\infty$ and $X_1, X_2, \ldots$ are identically distributed, we see that:
\begin{equation*}
  \sum_{n=1}^\infty \mathbf{P}\bigl[\{X_n/n \geq \varepsilon\} \bigr] = \frac{1}{\varepsilon} \sum_{n=1}^\infty \mathbf{P}\bigl[\{X_1 \geq n  \varepsilon\}\bigr] \cdot \varepsilon \leq \frac{1}{\varepsilon}\cdot \underbrace{\mathbf{E}[X_1]}_{<\infty} < \infty\, .
\end{equation*} The Borel-Cantelli lemma now gives us
$\mathbf{P}\Bigl[\limsup_{n\rightarrow\infty} \, \{X_n/n \geq \varepsilon\}\Bigr] = 0$. So for every $\varepsilon > 0$ we have $\mathbf{P}\bigl[\limsup_{n\rightarrow\infty} X_n/n < \varepsilon\bigr] = 1$ and since $\mathbf{P}$ is upper semicontinuous $\mathbf{P}\bigl[\limsup_{n\rightarrow\infty} X_n/n = 0 \bigr] = 1$ Now we show the second row. Let $M \geq 1$, then
\begin{equation*}
  \mathbf{P}\bigl[\limsup_{n\rightarrow\infty} X_n/n \geq M \}\bigr] = \mathbf{P}\bigl[\{X_n/n \geq M \; \text{ i.o.}\}\bigr] = \mathbf{P}\Bigl[\limsup_{n\rightarrow\infty} \, \{X_n/n \geq M \}\Bigr] \, .
\end{equation*}
Using that $\mathbf{E}[X_1] = \infty$ and $X_1, X_2, \ldots$ are identically distributed and $\mathbf{P}[X_n/n \geq 0] = 1$ we see that:
\begin{equation*}
  \sum_{n=1}^\infty \mathbf{P}\bigl[\{X_n/n \geq M\} \bigr] = -1 + \frac{1}{M} \sum_{n=0}^\infty \mathbf{P}\bigl[\{X_1 \geq n  M\}\bigr] \cdot M \geq -1 + \frac{1}{M}\cdot \underbrace{\mathbf{E}[X_1]}_{=\infty} = \infty\, .
\end{equation*} $X_1, X_2, \ldots$ are independent, so we can use the Borel-Cantelli lemma and we obtain that 
$\mathbf{P}\Bigl[\limsup_{n\rightarrow\infty} \, \{X_n/n \geq M\}\Bigr] = 1$. So for all $M \geq 1$ we have $\mathbf{P}\bigl[\limsup_{n\rightarrow\infty} X_n/n \geq M \bigr] = 1$ and since $\mathbf{P}$ is upper semicontinuous $\mathbf{P}\bigl[\limsup_{n\rightarrow\infty} X_n/n = \infty \bigr] = 1$. $\square$ Could you please check my proof? Thank you!","['probability-theory', 'proof-verification']"
1144526,Example of maximal monotone operators in non-reflexive Banach spaces with applications in PDE,"My question is about examples of maximal monotone operators that are defined in non-reflexive Banach spaces and have applications in PDEs, variational inequalities, etc (any application actually)? If possible I would like to exclude the convex subdifferential since it is a well-known example.","['convex-analysis', 'nonlinear-optimization', 'monotone-operator-theory', 'partial-differential-equations', 'functional-analysis']"
1144539,Almost sure convergences of series of Poisson random variables,"Let $X_i$ be independent Poisson with mean $\lambda_i$. Show that i)$\sum_1^\infty\lambda_i<\infty$ implies $\sum_1^\infty X_i$ converges almost surely to a finite limit. ii)$\sum_1^\infty\lambda_i=\infty$ implies $\sum_1^\infty X_i=\infty$ almost surely I could prove (i) by using Kolmogorov one series lemma just setting $Y_i=X_i-\lambda_i$, but I could not prove the second part. Any help/hint/solution for (ii) is appreciated. Thanks in advance","['probability-theory', 'probability-distributions', 'probability']"
1144564,Time derivative of Jacobian,"Say I have a function $f(p) : \mathbb{R}^3 \to \mathbb{R}$, where $p = (x,y,z)^T$. I know that the Jacobian $J$ is $f_p = (f_x, f_y, f_z)$. I know that the time derivative of the Jacobian, $J'$, is $\dfrac{\partial J}{\partial p} \cdot\dfrac{dp}{dt}$. The quantity $\dfrac{dp}{dt}$ is simply the velocity $v$ at $p$. But what is $\dfrac{\partial J}{\partial p}$? Is it the Jacobian of the Jacobian? How can I express this as a matrix (and not a tensor)? Context: I am trying to apply equation (11) of Constrained Dynamics .",['multivariable-calculus']
1144569,How is induction related to a well founded set?,"I read a proof that went along these lines: ""Let $(X, \le)$ be a woset. Let $E$ be a subset of $X$ such that: the minimal element of $X$ is a member of $E$ for any $x \in X$, if $\forall_y[y \lt x \rightarrow y \in E]$, then $x \in E$ Then, $E = X.$ Proof: Suppose that $E \ne X$. Then, let $x$ be the minimal element of $X - E$. Since, it is an element of $X - E$, it can not be an element of $E$. But, any $y \lt x$ would imply $y \in E$, because $x$ is the minimal element of $X - E$, so any element $y$ such that, $y \lt x$ would have to be in $E$. If it wasn't, $x$ wouldn't be the minimal element of $X -E$. Since $y \lt x$ implies $y \in E$, then by the second condition, $x \in E$ and a contradiction arises, so $X = E$."" But, then the claim was made that this being true allows proof by induction on any well founded set. I understand the proof, but I don't see how this theorem and induction are related. Could someone explain?","['elementary-set-theory', 'order-theory']"
1144593,How to determine the radius of convergence if the Taylor series cannot be written in a neat way?,"I am trying to evaluate the radius of convergence of Taylor series centered at zero of function
$$f(z)=\frac{\sin(3z)}{\sin(z+\pi/6)}$$
I guess the answer should be $\pi/6$ because the function will not be bounded if $x$ approaches $\pi/6$. And it is easy to show that $f$ is convergent for all $z$ with norm less than $\pi/6$ because the denominator will not reach zero. However, outside the circle with radius $\pi/6$ there do exist points that makes $f$ converge. So I am confused whether I get the right answer. Usually for a simple power series, if we determine the radius of convergence we will have the series diverge for all $z$ outside the circle. So I am really not sure what is the situation here.","['convergence-divergence', 'complex-analysis', 'taylor-expansion']"
1144600,"If $A$ is nilpotent, why is $A$ non-invertible?",Here $A$ is a square matrix. I'm confused as to why $A$ would be non-invertible as well.,"['matrices', 'nilpotence', 'linear-algebra']"
1144667,Is there always a solution for this packing problem with collision constraints,"I have six boxes with different sizes. Two boxes are red, two boxes are blue and two boxes are green. There is only one dimension that matters. Let $r$ and $r'$ be the size of red boxes. Similarly for $b$ and $b'$, and $g$ and $g'$. I know that: $\begin{align*}
r + b + g    &= 1\\
r' + b' + g' &= 1\\
r + r'       &\le 1\\
b + b'       &\le 1\\
g + g'       &\le 1
\end{align*}$ I want to put boxes of size r, g and b in ""line"", one after the other. I also want to put r', b' and g' in line, one after the other. These two lines formed by the two sets of boxes are to be put in parallel, side by side, touching each other. Example: _____________________________________________
|            |                       |        |
|     r      |           g           |   b    |
|____________|_______________________|________|
|       |             |                       |
|   g'  |      b'     |          r'           |
|_______|_____________|_______________________| Can I always (for all values of $r,r',g,g',b,b'$ respecting the conditions above) find an arrangement that does not have boxes of the same color touching each other (such as the one in the example above) ? Thanks, A","['recreational-mathematics', 'combinatorics']"
1144695,"Assume $P$ is a prime ideal s.t. $K \subset P$, show $f(P)$ is a prime ideal","I'm currently trying to solve this problem. Let $f: R \rightarrow S$ be a surjective ring homomorphism. Let $K = \ker(f)$. Assume $P$ is a prime ideal s.t. $K \subset P$. Show $f(P)$ is a prime ideal in $S$. I solved the ideal part. Let $y \in f(P)$, by definition then $y = f(x)$ for some $x \in P$. Now let $s \in S$. Then since $f$ is surjective $\exists r \in R$ s.t. $f(r) = s$. So then since $f$ is a homomorphism we have $f(x)\cdot s = f(x) \cdot f(r) = f(x\cdot r)$. Then since $P$ is ideal and $x \in P$, $x\cdot r \in P$, so $f(x \cdot r) = f(x)\cdot s \in f(P)$ so $f(P)$ is closed under multiplication by an element in $S$. Now let $y, y' \in f(P)$, again by definition we have $x, x' \in R$ s.t. $f(x) = y, f(x') = y'$. By $f$ homomorphism we have $y - y' = f(x) - f(x') = f(x-x')$. Then by $P$ ideal we have $x-x' \in P$, thus $y-y' \in f(P)$. Therefore $f(P)$ is an ideal. The part I'm having trouble with is showing that $P$ prime implies $f(P)$ prime. I started off let $AB \subset f(P)$, by definition we know we have $C \subset P$ s.t. $f(C) = AB$. From here I need to arrive at showing that either $A \subset f(P)$ or $B \subset f(P)$. I'm not really sure how to move forward from here but I know I somehow need to involve the fact that $K \subset P$ since I haven't used that yet. Any help would be appreciated. Note: I'm not assuming that $R$ or $S$ is commutative, I already know how to solve it if they are using the commutative definition of prime.","['maximal-and-prime-ideals', 'abstract-algebra']"
1144719,Integral $\int_{0}^{n^{2}} \lfloor \sqrt{t} \rfloor \rm dt $,"To find the integral $\int_{0}^{n^{2}} \lfloor \sqrt{t} \rfloor \rm dt$ if $n \geq 1$ is an integer and $\lfloor \cdot \rfloor$ is the floor function, I began with the observation that $$\int_{0}^{n^{2}} \lfloor \sqrt{t} \rfloor \rm dt = (2^{2}-1^{2})\cdot 1 + \cdots + \big[ n^{2} - (n-1)^{2} \big]\cdot (n-1).$$ However it seems that this observation may not be sufficiently useful?","['definite-integrals', 'integration', 'ceiling-and-floor-functions']"
1144729,How to show that $p$th moment being finite is equivalent to a limit existing,"Let $p \in (0,2)$ and let $\xi_n, n \geq 1$, be iid random variables. Show that the following two conditions are equivalent: With probability one, the limit
$$ \lim_{n \rightarrow \infty} \frac{1}{n^{1/p}} \sum_{k=1}^n \xi_k$$
exists and is finite. $\mathbb{E}\lvert \xi_i \rvert^p < \infty$ AND either $\mathbb{E} \xi = 0$ or $p \leq 1$. I have tried using Holder's inequality but didn't really get anywhere. I don't really have any idea how to approach the problem for either direction...","['probability-theory', 'probability']"
1144744,Total Variation Distance,"Grimmett and Stirzaker in Probability and Random Processes , 3rd ed. have on pg. 44: Let $X$ and $Y$ be integer-valued random variables, and let $$d_{TV}(X,Y) = \sum_{k}{\vert P(X=k) - P(Y=k)\vert }.$$ They ask the reader to prove that $d_{TV}(X,Y) = 0$ iff $P(X=Y) = 1$. I have no problem with $P(X=Y) = 1 \implies d_{TV}(X,Y) = 0$ but the other direction, $d_{TV}(X,Y) = 0 \implies P(X=Y) = 1$, doesn't look valid to me. Simple counter-example: One toss of a fair coin. Let $X$ be $1$ for $H$ and $0$ for $T$. Let $Y$ be $0$ for $H$ and $1$ for $T$. Then $d_{TV}(X,Y)=0$ but $P(X=Y)=0$ by my calculation. It seems to me that $d_{TV}(X,Y)$ is more a measure of the ""distance"" between the distribution functions of $X$ and $Y$ rather than between $X$ and $Y$ themselves. In the example, $X$ and $Y$ have the same distribution function. I might well be missing something here and the book is correct, but is $d_{TV}(X,Y) = 0 \implies P(X=Y) = 1$ true? If so, could you provide a proof to confirm it? Thanks.","['probability-theory', 'probability']"
1144750,What's the difference between interior and relative interior?,"As defined in Convex Optimization written by Stephen Boyd & Lieven Vandenberghe, both interior and relative interior seems to describe a same thing: a set that peels away its boundary points. So, what on earth is the difference between these two concepts ? Here are the definitions of these two concepts from Convex Optimization : interior : The set of all points interior to $C$ is called the interior of $C$ relative interior : its interior relative to the affine hull of $C$","['general-topology', 'convex-analysis']"
1144805,What's With The Diagonal Morphism?,"Given a morphism $X \to Y$ of schemes, we can construct a diagonal morphism $\delta: X \to X \times_Y X$ via the universal property of the fiber product applied to the identity map $X \to X$. Recently, I've been puzzled (shocked even) by how many different properties or results about schemes can be phrased in terms of the diagonal morphism. Does anyone have intuition as to why this map is so important and why this should be the case? In particular, I've been thinking about the following collection of statements: 1) Separatedness: We say a morphism $\pi: X \to Y$ is separated if the diagonal morphism is a closed embedding. 2) Quasi-separatedness: $\pi: X \to Y$ is quasi-separated if the diagonal morphism is quasi-compact. 3) Cancellation Property: If $P$ is a class of morphisms preserved by base change and composition and we have the maps $\pi: X \to Y, \rho: Y \to Z, \tau: X \to Z$, then if the diagonal $Y \to Y \times_Z Y$ and $\tau$ are in $P$, so is $\pi$. 4) $\pi: X \to Y$ is universally injective if it is injective after any base change. $\pi$ is universally injective iff the diagonal morphism is surjective. This is apparently the analog for schemes of a purely inseparable extension of fields. 5) Given $\pi: X \to Y$ the relative cotangent sheaf $\Omega_{X/Y}$ is the conormal sheaf of the diagonal morphism. 6) $B$ is a separable $A$-algebra if $B$ is projective as a $B \otimes_A B$-module. 7) If $\pi: X \to Y$ is locally finite type, then $\pi$ is unramified iff the diagonal is an open immersion.","['soft-question', 'algebraic-geometry', 'schemes', 'abstract-algebra']"
1144808,A Qual Problem:Convergence of sum of Poisson random variable over sum of their mean,"Assume that $X_i$'s are independent  Poisson$(\lambda_i)$, and $\sum_i^\infty\lambda_i=\infty$. Show that $\frac{\sum_i^n X_i}{\sum_i^n\lambda_i}\rightarrow1$ almost surely. I also tried to show weaker version: convergence in probability, yet I could not succeed. We know that $\sum_i^\infty\lambda_i=\infty$ implies $\sum_i^\infty X_i=\infty$ almost surely but i could not connect their rate of convergence to solve the problem. For any help/hint/solution, thanks in advance.","['probability-theory', 'probability-distributions', 'probability']"
1144820,A Tricky Quetiones on Creative Algorithm in Graph,"an agent is works between n producer and m consumers. i'th producer, generate $s_i$ candy and j'th consumer, consumes $b_j$ candy, in this year. for each candy that sales, agent get 1 dollar payoff. for some problems, one strict rule was defined that a producer should be sales candy to any producer that the distance between them is not greater than 100 KM (kilometers). if we have list of all pairs of producer-consumer that the distance  between them is lower than 100 KM, which of the following algorithm is goof for finding maximum payoffs? (suppose $s_i$ and $b_j$ may be becomes very large). 1) Maximal Matching

2) Dynamic Programming

3) Maximum Flow 

4) 1 , 3 this is a last question on 2013-Final Exam on DS course.  any hint or idea?","['discrete-mathematics', 'contest-math', 'algorithms', 'graph-theory', 'computer-science']"
1144821,Find general solution $y''y^3=1$,"I'm having this question in my homework assignment in Linear Algebra and diffrential equation class, and trying to find the general solution for this second ODE. $$y''y^3 = 1$$ Using substitution I said $p = y'$ and $p' = y''  \rightarrow  \frac{dp}{dx}= \frac{dp}{dy} \times \frac{dy}{dx}$ Then we get $y^3 \frac{dp}{dx}$ now what's the next step should I integrate or differentiate the equation? What's the trick on these type of ODEs? Thanks in advance!",['ordinary-differential-equations']
1144825,Limit of Gaussian random variables is Gaussian?,"Consider a sequence $X_n$ of Gaussian random variables with mean $\mu_n$ and variance $\sigma_n^2$, which converges in distribution (to some limiting distribution). Can I then conclude that $\mu_n$ converges to some $\mu$ and $\sigma_n^2$ converges to some $\sigma^2$, with the limiting distribution being $N(\mu, \sigma^2)$? Here, I think I'd allow the degenerate Gaussian distribution with zero variance. I've tried looking this up elsewhere on this site, but all those I've found assume almost sure/L2 convergence. I was thinking this should be true, and was trying to do this by characteristic functions but without assuming the convergence of the two parameters, I can't seem to conclude anything.","['normal-distribution', 'convergence-divergence', 'probability']"
1144842,Is there a difference between $y=\frac{\sqrt{1-x}}{\sqrt{1+x}}$ and $y=\sqrt{\frac{1-x}{1+x}}$,"Is there a difference between $$y=\frac{\sqrt{1-x}}{\sqrt{1+x}}$$ and $$y=\sqrt{\frac{1-x}{1+x}}$$
If there is a difference, why when I give the square for both equations, they will be equal.",['calculus']
1144868,General solution to ODE $ y''-Ay^5=0 $,"What is the solution of $$ y''-Ay^5=0  $$
I got the solution $ y = {(3/4A)}^{1/4} x^{-1/2}$ using trial and error but how to solve this type of problem in general?","['ordinary-differential-equations', 'calculus', 'derivatives']"
1144885,Banach Spaces which are not $L^p$,"Most of the times, when I think of Banach Spaces I think of $L^p$ spaces. I would like to know if there is any Banach space which can not be written as $L^p$ space. Please also indicate any applications of such spaces. Thanks","['reference-request', 'functional-analysis', 'soft-question', 'analysis']"
1144895,Calculating of least significant digit of an expression,I want to calculate te least significant digit (1s place) of following: $ 1+2^{1393} + 3^{1393}+4^{1393} $ How we can calculate this? It's very hard for me!,"['numerical-methods', 'discrete-mathematics', 'algebraic-number-theory', 'number-theory']"
1144908,Stupid factorial question.,If i have $(3(n+1))!$ can I say: $(3(n+1))! = 3(n+1) \times (3n)!$ but if I expand by first multiplying the expression in the parenthesis; $(3(n+1))! = (3n+3) \times (3n+2) \times (3n+1) \times (3n)!$ Which one is it? I think the second one.,"['factorial', 'algebra-precalculus']"
1144914,Has a $L^1$ bounded sequence a weak converging subsequence in $L^2$?,"Let $f_n \in L^2(0,1)$ with the property that
$\sup_n || f_n ||_{L^1}<A< \infty$, i.e. $f_n$ is a sequence in $L^2$ that is uniformly bounded in the $L^1$-Norm. Does $f_n$ then have a weak converging subsequence in $L^2$? i.e. is there a $f \in L^2$ and $n_k$, such that $ \int gf_{n_k} \rightarrow  \int gf$ for all $g \in L^2(0,1)$?","['convergence-divergence', 'integration', 'lp-spaces', 'functions', 'functional-analysis']"
1144920,Two linear subspaces are isomorphic as algebraic sets if and only if they have the same dimensions,"This is an example from Karen Smith's notes. Let $V_1, V_2 \subset k^n$ be linear subspaces (defined by some collection of linear polynomials). Then $V_1 \cong V_2$ as algebraic sets if and only if $\dim(V_1)=\dim(V_2)$. I am not sure how to start. I think they are defined by linear polynomials does not mean there exist a linear map between them. And they might not be able to be defined by finitely many linear polynomials, right? Thanks for your help!",['algebraic-geometry']
1144921,Interior of a compact 3-manifold,"I have an orientable 3-manifold $X$, such that
 $$X=\lbrace(x,y,z)\mid x\neq y \neq z \neq x \rbrace\subseteq S^1\times S^1 \times S^1 $$
How to find a compact 3-manifold $M$ such that $X= interior(M)$ any help please???","['general-topology', 'manifolds', 'compactness']"
