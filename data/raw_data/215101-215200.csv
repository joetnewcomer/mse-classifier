question_id,title,body,tags
4363105,Two quadratics with one root opposite to each other,"Say we have two quadratic equations as follows, $$x^2-2mx-1=0$$ $$x^2+(m+3)x-4=0$$ where $m$ is real, such that the two equations have one root that is the additive inverse of each other. (For example, one may have root $x=3$ and the other would have $x=-3$ as one of its roots.) Find m. My only thought is using the quadratic formula and compare the two equation's roots, it'll get the job done, but takes a lot of work. I suppose there's a method to modify one of the equations so that the new equation from modification has one same root as the equation we didn't modify. That way, we can just subtract them and solve for a much simpler equation.","['algebra-precalculus', 'quadratics']"
4363136,How to test $\mu_1 = \mu_2 = \mu_3$ when the covariance matrix is known/unknown?,"I need to verify the hypothesis H0: $\mu_1 = \mu_2 = \mu_3$ vs the alternative(at least one mean doesn't equal to the others). Where $N=20$ and the estimated mean $\overline{x} = (1, 0, -1)^T$ . Popluation comes from normal distribution $N_3(μ,Σ)$ . a) with known covariance matrix $ \begin{bmatrix}
3 & 2 & 1 \\
2 & 3 & 1 \\
1 & 1 & 4 
\end{bmatrix}  $ . I wrote the hypothesis in the form of $C\mu$ where $ C = \begin{bmatrix}
1 & -1 & 0 \\
1 & 0 & -1 \\
\end{bmatrix}  $ . Then I calculated the statistics (I found the formula in the book, is it ok?) in such a way $$ T^2 = N(C\overline{x})^T(CSC^T)^{-1}(C\overline{x})$$ but then I cannot find one good critical value to compare it to (I found this formula, but again could you please confirm/deny is it correct): $$ T^2_{n-1,N-1,\alpha} = \frac{(N-1)(n-1)}{N-n+1}, \quad \text{where $n$ is the dimension of the mean distribution, here it's 3.}$$ Is this formula correct? (I saw other formulas as well and I am confused.) B) What will be the difference in the calculations if we assume that the covariance matrix is unknown ? Note: I don't need any coding solutions (R, python, etc.); I want to understand it on the theoretical level.","['statistics', 'means', 'hypothesis-testing']"
4363180,"Why do $SO(n,\mathbb{R})$ and $O(n,\mathbb{R})$ have the same Lie algebra?","If we define $O(n,\mathbb{R})=\{A\in GL(n,\mathbb{R}):A^tA=I\}$ and $SL(n,\mathbb{R})=\{A\in GL(n,\mathbb{R}):\det(A)=1\}$ then $SO(n,\mathbb{R})=SL(n,\mathbb{R})\cap O(n,\mathbb{R}).$ I know that the Lie Algebra of $O(n,\mathbb{R})$ comprises of skew-symmetric matrices and that the Lie Algebra of $SL(n,\mathbb{R})$ comprises of matrices with zero trace. I was guessing that the Lie Algebra of $SO(n,\mathbb{R})$ then should comprise of skew-symmetric matrices with zero trace. However, this is not the case. Question 1 : How do I show that the Lie algebra of $SO(n,\mathbb{R})$ is the same as the Lie Algebra of $O(n,\mathbb{R})?$ I also noticed that on the other hand the Lie Algebra of $SU(n,\mathbb{C})$ comprises exactly of the skew-Hermitian matrices with zero trace. I am guessing an answer to the first question should explain why this is true. Still I wonder Question 2 : Under what conditions do we have that $N=H\cap G$ implies $\mathfrak{n}=\mathfrak{h}\cap \mathfrak{g}$ where $N,H,G$ are Lie Groups and $\mathfrak{n},\mathfrak{h}$ and $\mathfrak{g}$ are their associated Lie Algebras?","['lie-algebras', 'lie-groups', 'differential-geometry']"
4363268,"Where is the ""covariance"" in a covariant derivative","I have the following definition of a covariant derivative. Consider a general fibre bundle $E \rightarrow M$ with a connection given by a parallel transport, i.e. along a path $\gamma$ in $M$ we have a transport $\Gamma(\gamma)^t_s : E_{\gamma(s)} \rightarrow E_{\gamma(t)}$ with a covariant derivative $\nabla_{\dot{\gamma}(0)} \sigma(x) := \frac{d}{d t}\mid_{t=0}(\Gamma(\gamma)^t_0)^{-1} \circ \sigma \circ \gamma(t)$ . My question is, what does ""covariant"" refer to in the name covariant derivatrive? I have two main guesses: It reflects the fact that local forms of the covariant derivative ""commute"" with the transition maps of the bundle - but it is pretty obvious as the derivative is defined globally and its local expressions are defined so that it makes sens. It is purely historical and stems from the fact that the above covariant derivative is a generalisation of the covariant derivative of a metric connection which ""vector field component"" changes covariantly.","['connections', 'differential-geometry']"
4363308,"Let $R,S,T$ be positive operators, $S,T\leq 1$. Is $\left|\left|SRS\right|\right|\leq \left|\left|(S+T)R(S+T)\right|\right|$?","Given three positive operators $R,S,T\in B(H)$ , with $\left|\left|T\right|\right|\leq 1$ and $\left|\left|S\right|\right|\leq 1$ , I would like to know whether the inequality $\left|\left|SRS\right|\right|\leq \left|\left|(S+T)R(S+T)\right|\right|$ holds. I would be equally happy if the right hand side is replaced by a polynomial of the original right hand side. I could achieve $\left|\left|SRS\right|\right|\leq \left|\left|(S+T)^{1/2}R(S+T)^{1/2}\right|\right|$ , mainly by using the identity $||x^*x||=||x||^2$ for bounded operators on a Hilbert space, but it's not good enough for me. Thanks for any help!","['hilbert-spaces', 'operator-theory', 'functional-analysis', 'operator-algebras']"
4363320,Show that $\int\limits_a^bf(x)g(x)dx\geq 0 \implies f(x)\geq0$,"Let be $f:[a,b]\to\mathbb{R}$ a continuous function. Show that if $\int\limits_a^bf(x)g(x)dx\geq 0$ holds for all continuous functions $g:[a,b]\to\mathbb{R}$ with $g(a)=g(b)=0$ and $g(x)\geq 0$ for all $x\in]a,b[$ , then $f(x)\geq 0$ for all $x\in[a,b]$ . My approach: Let's assume there exists a $x_0\in]a,b[$ such that $f(x_0)<0$ . If we set $\epsilon:=-\frac{f(x_0)}{2}>0$ , then by continuity of $f$ there exists a $\delta>0$ such that for all $x\in[a,b]$ with $|x-x_0|<\delta\implies |f(x)-f(x_0)|<-\frac{f(x_0)}{2}$ . This means that $f(x)<0$ for all $x\in]x_0-\delta,x_0+\delta[$ . Next, we define the function $g:[a,b]\to\mathbb{R}$ where $$
g(x):=\begin{cases}0,&x\in[a,x_0-\delta]\\x-(x_0-\delta),&x\in]x_0-\delta,x_0]\\ (x_0+\delta)-x,&x\in]x_0,x_0+\delta[\\0,&x\in[x_0+\delta,b].\end{cases}
$$ If we check the limits at the ""critical"" three points $x_0-\delta,x_0$ and $x_0+\delta$ we see immediately that $g$ is continuous. Further, we see that \begin{align*}
&f(x)g(x)=0,  x\in[a,x_0-\delta]\\
&f(x)g(x)\leq\frac{f(x_0)}{2}(x-x_0+\delta)\leq0,  x\in[x_0-\delta,x_0]\\
&f(x)g(x)\leq\frac{f(x_0)}{2}(x_0+\delta-x)\leq0,  x\in[x_0,x_0+\delta]\\
&f(x)g(x)=0,  x\in[x_0+\delta,b].
\end{align*} Using the well known operations of Riemann integrals we get: \begin{align*}
&\int\limits_a^bf(x)g(x)dx\\
&=\underset{=0}{\underbrace{\int\limits_a^{x_0-\delta}f(x)g(x)dx}}+\underset{\leq0}{\underbrace{\int\limits_{x_0-\delta}^{x_0}f(x)g(x)dx}}+\underset{\leq0}{\underbrace{\int\limits_{x_0}^{x_0+\delta}f(x)g(x)dx}}+\underset{=0}{\underbrace{\int\limits_{x_0+\delta}^bf(x)g(x)dx}}\leq0.
\end{align*} We know that if $h$ is a continuous function, then $\int\limits_a^bh(x)dx=0\implies h(x)=0$ for all $x\in[a,b]$ . As $f(x)g(x)\neq 0$ for particular $x\in[a,b]$ it follows that $\int\limits_a^bf(x)g(x)dx\neq 0$ . Hence, $\int\limits_a^bf(x)g(x)dx<0$ . This is a contradiction so there mustn't be a $x_0$ such that $f(x_0)<0$ . Note that if we had assumed $x_0=a$ or $x_0=b$ then we could adjust $g$ accordingly and get the same result. Is this correct? Do you have any suggestions or maybe a shorter proof? Also: The function $g$ I have constructed satisfies $g(a)=g(b)=0$ but I don't see why it should be necessary?! Maybe there are other functions which we can use to achieve a contradiction which doesn't satisfy $g(a)=g(b)=0$ ? EDIT We know that if $h$ is a continuous function with $h(x)\geq0$ for all $x\in[a,b]$ , then $\int\limits_a^bh(x)dx=0\implies h(x)=0$ for all $x\in[a,b]$ . As $(-f(x)g(x))\geq 0$ for all $x\in[x_0-\delta,x_0+\delta]$ and $(-f(x)g(x))\neq 0$ for particular $x\in[x_0-\delta,x_0+\delta]$ it follows that $\int\limits_a^b-f(x)g(x)dx\neq 0$ . Hence, $\int\limits_a^bf(x)g(x)dx\neq0$ which shows that $\int\limits_a^bf(x)g(x)dx<0$ . This is a contradiction so there mustn't be a $x_0$ such that $f(x_0)<0$ .","['integration', 'real-analysis', 'continuity', 'solution-verification', 'riemann-integration']"
4363321,"General solution of a second order, non-homogeneous ODE [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question How might I go about computing the general solution (i.e. with an arbitrary initial condition) of $$tu''−(t+1)u'+u=t^2$$ where $u'= \frac{du}{dt}$ and $u''= \frac{d^2u}{dt^2}$ I am a bit confused. Can anyone give me the exact solution?",['ordinary-differential-equations']
4363322,Theorem 19.5 of Munkres’ Topology,"Let $\{X_\alpha\}$ be an indexed family of spaces; let $A_\alpha  \subset X_ \alpha$ for each $\alpha$ . If $\prod X_{\alpha}$ is given either the product or the box topology, then $\prod \bar{A}_{\alpha} = \overline{\prod A_{\alpha}}$ It’s natural to prove $\supseteq$ inclusion by showing $\prod \bar{A}_{\alpha}$ is closed and $\prod \bar{A}_{\alpha} \supseteq {\prod A_{\alpha}}$ . How do I show $\prod \bar{A}_{\alpha}$ is closed? I have tryed $\prod (X_\alpha \setminus \overline{A_\alpha})$ , don’t known how to simplify further.","['elementary-set-theory', 'general-topology']"
4363323,What is the direction of vector $\left[\begin{smallmatrix}d x \\ d y\end{smallmatrix}\right]$,"Recently I came across a topic "" total differential "" which comes with a result $d f=\frac{\partial f}{\partial x} d x+\frac{\partial f}{\partial y} d y$ As much I learned in multivariable calculus this can be simplified as $\nabla f \cdot\left[\begin{array}{l}d x \\ d y\end{array}\right]$ which graphically means taking directional Derivative in diraction of $\left[\begin{array}{l}d x \\ d y\end{array}\right]$ But is it  Makes any sense ? someone plese explain","['multivariable-calculus', 'calculus', 'derivatives', 'vectors']"
4363335,Why is this probability equation wrong?,"I have a problem stating ""There are 13 apples and 17 oranges. What is the probability of making a group of 4 with at least two apples?"" Using my logic (left side of the following equation), I first multiply a group of 2 that consists of 2 apples, and then another group of 2, which consists of the other 28 elements. However, this is wrong; the right way to do it is to add all possible outcomes (AAOO, AAAO and AAAA) as shown on the right side of the equation $$\frac{\binom{13}{2}\binom{28}{2}}{\binom{30}{4}} \neq \frac{\binom{13}{4}+17\binom{13}{3}+\binom{13}{2}\binom{17}{2}}{\binom{30}{4}}$$ Why is my way incorrect? Is there a smarter way to do this? p.s.: the right answer is $\frac{1079}{1827}$ if it helps...","['combinations', 'combinatorics', 'probability']"
4363359,Why is Borel measurability needed here?,"Let $X,Y$ be random variables on the probability space $(\Omega,\mathcal{F})$ . Since $P(X|Y)$ is $\sigma(Y)$ -measurable, we have by the Doob-Dynkin lemma that $P(X|Y)=h(Y)$ for some Borel-measurable function $h$ . Now we define $P(X|Y=y):=h(y)$ . Suppose that there is some other $\alpha$ , such that $P(X|Y=y)=\alpha(y)$ , $PX^{-1}$ -a.e $y\in\mathbb{R}$ . Then if $\alpha$ is Borel measurable, $P(X|Y)=\alpha(Y)$ $P$ -a.e. $\omega\in\Omega$ . Why is the condition $\alpha$ is borel-measurable needed?","['conditional-probability', 'conditional-expectation', 'probability-theory', 'probability']"
4363364,Proof that limit points are unique,"I ams having problems to prove that the limits points of a sequence are unique. For example given the following sequence \begin{equation}
x_n=(-1)^n+\frac{1}{n}
\end{equation} To find the limit points, I establish these subsequences \begin{equation}
y_n=x_{2n}=(-1)^{2n}+\frac{1}{2n}=1+\frac{1}{2n}
\end{equation} \begin{equation}
z_n=x_{2n+1}=(-1)^{2n+1}+\frac{1}{2n+1}=-1+\frac{1}{2n+1}
\end{equation} Where it can be seen that \begin{equation}
y_n\rightarrow 1
\end{equation} \begin{equation}
z_n\rightarrow -1
\end{equation} My question is how can I prove that these are the only two limit points of the sequence.
I suppose that it is related to the fact that \begin{equation}
Image(f(n))+Image(g(n))=\mathbb{N}
\end{equation} Being \begin{equation}
f(n)=2n\; \; \; \;  
g(n)=2n+1
\end{equation}","['limits', 'proof-explanation', 'sequences-and-series', 'real-analysis']"
4363421,"If $A$ is a $W^*$-algebra, how to construct a nice predual for $M_n(A)$?","Let $A$ be a $W^*$ -algebra. By concretely representing $A \subseteq B(H)$ as a von Neumann algebra and using that $M_n(A) \subseteq B(H^{n})$ , one can check that $M_n(A)$ is again a $W^*$ -algebra such that $\sigma$ -weak convergence in $M_n(A)$ is given by entrywise $\sigma$ -weak convergence of all entries. However, I'm wondering if it is also possible to deduce this fact from Sakai's predual theorem. I.e., given a $W^*$ -algebra $A$ , it it possible to construct a Banach space $F$ together with an isometric isomorphism $$\Phi: M_n(A) \to F^*$$ so that it becomes clear that $\sigma$ -weak convergence in $M_n(A)$ is given entrywise? Very naively, we can fix a predual $A_*$ for $A$ and then we want to do something like $$M_n(A) \cong M_n((A_*)^*) \cong M_n(A_*)^*$$ but it is not clear to me how to turn $M_n(A_*)$ in a Banach space such that we obtain an isometry.","['von-neumann-algebras', 'c-star-algebras', 'banach-spaces', 'operator-algebras', 'functional-analysis']"
4363451,"Determine the greatest of the numbers $\sqrt2,\sqrt[3]3,\sqrt[4]4,\sqrt[5]5,\sqrt[6]6$","Determine the greatest of the numbers $$\sqrt2,\sqrt[3]3,\sqrt[4]4,\sqrt[5]5,\sqrt[6]6$$ The least common multiple of $2,3,4,5$ and $6$ is $LCM(2,3,4,5,6)=60$ , so $$\sqrt2=\sqrt[60]{2^{30}}\\\sqrt[3]3=\sqrt[60]{3^{20}}\\\sqrt[4]4=\sqrt[60]{4^{15}}=\sqrt[60]{2^{30}}\\\sqrt[5]{5}=\sqrt[60]{5^{12}}\\\sqrt[6]{6}=\sqrt[60]{6^{10}}=\sqrt[60]{2^{10}\cdot3^{10}}$$ Now how do we compare $2^{30},3^{20},4^{15},5^{12}$ and $6^{10}$ ? I can't come up with another approach.","['algebra-precalculus', 'number-comparison', 'radicals']"
4363457,Counting possible number of food orders,"I am trying to calculate the total number of possible orders from a group of $4$ friends at a restaurant and there are $12$ different dishes to choose from. Each friends has to have a different dish from each other. So far, I have this calculation: $12 * 11 * 10 * 9 = 11,880$ Does this look right?","['combinatorics', 'discrete-mathematics']"
4363494,Counting number of possible food orders with repetition allowed,"A group of four friends goes to a restaurant for dinner. The restaurant offers 12 different main dishes.
Suppose that each individual orders a main course. The waiter must re- member who ordered which dish as part of the order. It's possible for more than one person to order the same dish. How many different possible orders are there for the group? I have this calculation so far: $12^4 = 20,736$ possible orders. Does this look right?","['combinatorics', 'discrete-mathematics']"
4363507,Weak limit of the divergence of a piecewise constant approximation in $L^1$,"The original problem is the following:
let $\Omega\subset\mathbb{R}^n$ be regular and bounded, $f\in L^1(\Omega)\cap C^\infty(\Omega)$ with $\operatorname{div} f=0$ . Let $f_k \rightarrow f$ in $L^1(\Omega)$ where $f_k=\sum_{i=1}^{M(k)}\chi_{T_i^k}(x)f(x^k_i)$ where $\{T_i^k\}_{1\leq I\leq M(k)}$ is a family of $n$ -simplex with $x^k_i \in T_i^k$ . Hence $f_k$ is a piecewise constant approximation of $f$ . $(\star)$ Is it true that $\operatorname{div}f_k \rightharpoonup^\star \operatorname{div} f$ in $\mathcal{M}(\Omega)$ , i.e. weak star in the sense of measure? Side questions involved are: $(1)$ Is there a sequence of piecewise constant functions (possibly different form the one defined above) such that $(\star)$ is true? $(2)$ If $(\star)$ holds what about $\lim_k\|\operatorname{div}f_k\|(\Omega)$ ? My efforts are all blocked by the boundary: neither $f$ nor $\nabla f$ are bounded, hence we do not have a trace on $\partial \Omega$ , and all the estimates I came up with may diverge near $\partial \Omega$ . I cannot even prove that $
\sup_k\|\operatorname{div}f_k\|(\Omega)\leq C$ which would be enough to check the convergence on a dense set like $\psi\in C_c^1(\Omega)$ for which I have diverge theorem and so $$\lim_k \langle div f_k,\psi\rangle=\lim_k-\langle f_k,\nabla \psi \rangle  =\langle f ,\nabla\psi\rangle= \langle div f ,\psi\rangle,
$$ where in the last passage I used the compact support of $\psi $ (no trace of $f$ ). I tried to work out some counterexamples, using $f(x)=\frac{x}{|x|^2}$ on $[0,1]^2$ .
For simplicity in the calculations I tried with $f_k=\sum_{i,j=0}^{k-1}\chi_{Q_{i,j}^k}(x)f(\frac{2i+1}{2k},\frac{2j+1}{2k})$ where $Q_{i,j}^k=Q((\frac{2i+1}{2k},\frac{2j+1}{2k}),\frac{1}{k})$ . I am currently lost in the calculations.","['measure-theory', 'weak-convergence']"
4363508,On spherical distributions in abstract harmonic analysis,"I need a detailed and comprehensive guidance or possibly relevant materials that can help me in my research on spherical distributions on a locally compact group. Jacques Faraut wrote on spherical distributions on hyperbolic spaces in a famous paper in 1978, "" Distributions sphériques sur les espaces hyperboliques..."" Helminck A.G.  and Helminck G.F. did the same in 2002 and 2005. However I am still confused as per the true and explicit picture of a spherical distribution in abstract harmonic analysis.","['book-recommendation', 'reference-request', 'functions', 'online-resources', 'group-theory']"
4363509,"If $\int_{0}^{\infty}\frac{dx}{1+x^2+x^4}=\frac{\pi \sqrt{n}}{2n}$, then $n=$","$$\text{If }\int_{0}^{\infty}\frac{dx}{1+x^2+x^4}=\frac{\pi \sqrt{n}}{2n},\text{then } n=$$ $$$$ $$\text{A) }1 \space \space \space \space \space\text{B) }2 \space \space \space \space \space\text{C) }3 \space \space \space \space \space\text{D) }4 \space \space \space \space \space\text{E) }\text{None of the given options}$$ Using partial fraction decomposition, I ended with $n=3$ , that is option $\text{C}$ . However, it took me about $8$ minutes, which is not feasible for an MCQ test, where the average time to solve a problem is $3$ minutes, and no calculator is allowed. I believe that there is something obvious for some of you. If there is nothing obvious, then there must be a simpler and a quicker way than partial fraction decomposition. This post is voted to be closed and considered as [duplicate] but it is not. To prove is different than evaluating $n$ with possibly a cliver and a fast way. To prove, I can use partial fraction decomposition. Some users DO NOT really read the post properly! Please share your thoughts. THANKS!","['integration', 'improper-integrals', 'calculus', 'completing-the-square', 'partial-fractions']"
4363530,Is set equality a primitive notion or a relation whose definition is given by the axiom of extensionality?,"I saw some sources claiming that the equality of sets is a primitive notion (i.e. according to Wikipedia, a concept which is not defined in terms of previously defined concepts). However, the axiom of extensionality gives that two sets are said to be equal iff $\forall x(x \in A \iff x \in B)$ . Isn't set equality then a concept which can be defined in terms of the the primitive notion of set equality together with the concepts from first order logic? (i.e. isn't set equality not a primitive notion?)","['elementary-set-theory', 'set-theory']"
4363539,Stuck while using Laplace transform to solve delayed DE.,"I am studying delayed DE's, and while I've solved the following equation $$y'(t) = y(t-1)$$ $$y[-1,0] = y_0$$ where $y_0$ is a constant by using the method of steps to get $$y(t) = \sum y_0\frac{t^n}{n!} + c$$ on $[n,n+1]$ , I'm trying to now solve this using a Laplace transform. (Also, could someone confirm if the above is the correct answer? I remember following online notes, but it was a while ago, and I don't remember if this was supposed to be the final solution, since it's only defined on a specific interval?) Anyway, until now, I've worked out the following by applying a Laplace transform to both sides of the equation. (I'll be skipping some more simple steps while writing this down though) $$\int_0^\infty e^{-st}y'(t)dt = \int_0^\infty e^{-st} y(t-1)dt$$ $$sY(s) - Y(0) = \int_{-1}^\infty e^{-s(\tau+1)} y(\tau)d\tau$$ where $Y(s) = \mathcal{L}\{y(t)\}$ , and $t-1 = \tau$ . I then continue and get $$sY(s) - y_0 = e^{-s}\bigg[\int_{-1}^0 e^{-s\tau} y(\tau)d\tau +
\int_{0}^\infty e^{-s\tau} y(\tau)d\tau \bigg]$$ $$sY(s) - y_0 = e^{-s}\bigg[y(\tau)\frac{e^{-s\tau}}{-s}\bigg|^0_{-1} + 
\frac{1}{s}\int_{-1}^{0} y'(\tau)e^{-s\tau}d\tau +Y(s)\bigg]$$ where I used integration by parts to evaluate the first integral in the upper equation that's from $-1$ to $0$ . This is where I'm stuck, because in the last equation, while I can evaluate the first part, and obviously simplify the rest of the equation, I'm not sure how to solve the middle integral. I tried letting it be equal to a random integral $I$ to use integration by parts again and get a value, but I ended up with a $0=0$ equation lol. I might be fuzzy on my calculus skills, so can someone help me evaluate that center integral? Or basically just this following integral overall $$\int_{-1}^0 e^{-s\tau} y(\tau)d\tau$$ I feel like it's probably something really simple I'm missing, but I would be super helpful if someone could help point it out. Thank you!!","['integration', 'delay-differential-equations', 'laplace-transform', 'ordinary-differential-equations']"
4363558,Show that the bissectors of $\angle BHC$ and $\angle BFC$ meet on $BC$,"Given an acute triangle $\triangle ABC$ with orthocenter $H$ . Let $D = BH \cap AC, E = CH \cap AB$ and $F = (AEDH) \cap (ABC) \neq A$ . Show that the inner angle bisectors of $\angle BFC$ and $\angle BHC$ meet on the side $BC$ . It is easy to prove that the dotted ray passes through the midpoint of $BC$ and the antipode of $A$ in the $(ABC)$ . My first idea was to find $\angle CBF$ but that is a bit tough. So maybe drawing the perpendiculars from the meeting of the bissector of $\angle CHB$ with $BC$ to $FB$ and $FC$ may lead to something but it seems I need the forementioned angles.","['euclidean-geometry', 'geometry']"
4363591,Apply Dominated Convergence Theorem: $\lim_{n\rightarrow\infty}\int_0^\infty \dfrac{1+nx^2+n^2x^4}{(1+x^2)^n}d\mu.$,"I am working on a problem which I assume is an application of Dominated Convergence Theorem. I am supposed to find $$\lim_{n\rightarrow\infty}\int_0^\infty \dfrac{1+nx^2+n^2x^4}{(1+x^2)^n}d\mu.$$ I guess I need to find a bound on $$\frac{1+nx^2+n^2x^4}{(1+x^2)^n}=\frac{1+nx^2}{(1+x^2)^n}+\frac{n^2x^4}{(1+x^2)^n}.$$ I can bound the first part, $ \dfrac{1+nx^2}{(1+x^2)^n}$ , by using the binomial expression of ${(1+x^2)^n}$ . But I am stuck with dealing with the second part, $\dfrac{n^2x^4}{(1+x^2)^n}$ . Thank you in advance for any suggestions and help.","['integration', 'measure-theory', 'improper-integrals', 'real-analysis']"
4363608,How do we prove that $|\mathbb{P}(X = m) - \mathbb{P}(Y = m)| \leq \mathbb{P}(X \neq Y )$?,"While preparing for an upcoming test in probability, I ran into the following question. Let $(\Omega,\mathbb{P})$ be a probability space and let $X,Y :\Omega\to\mathbb{R}$ be random variables. Prove that for every $m\in\mathbb{R}$ , it holds that $$|\mathbb{P}(X = m) - \mathbb{P}(Y = m)| \leq \mathbb{P}(X \neq Y )$$ I'm not sure about the meaning of $X \neq Y$ . My intuition tells me that its something like $\forall \omega \in \Omega \;, Y(\omega) \ne X(\omega)$ . Is this right?","['probability-theory', 'probability', 'random-variables']"
4363690,What is the Double and Triple Dual of $\ell^\infty$,"Suppose that we start with $c^0$ and keep taking duals, $$c^0 \to \ell^1 \to \ell^\infty \to \ell^{\infty*} \to \ell^{\infty **} \to \ell^{\infty***} \to \ell^{\infty****}\to \dots$$ Is this an infinite sequence of distinct spaces?  If not, when does it ""stabilise"", i.e. stop generating new spaces?  What about other Lebesgue spaces $$L^1(\Omega,\mu) \to L^\infty(\Omega,\mu) \to L^{\infty*}(\Omega,\mu) \to L^{\infty **}(\Omega,\mu) \to L^{\infty***}(\Omega,\mu) \to L^{\infty****}(\Omega,\mu)\to \dots?$$ I know that a Banach space is non-reflexive iff its dual is non-reflexive, but do not know anything beyond that.  Thanks! Edit: Thanks Jose for posting the mathoverflow link.  Follow-up questions: What is the characterisation of $(L^{\infty})^{n*}, n\in\mathbb{N}$ ? What uses do these spaces have? Thanks!","['lp-spaces', 'functional-analysis', 'dual-spaces']"
4363726,Why do we study Cameron-Martin Space and what is the motivation behind it?,"I'm reading about Gaussian Measures and the chapters always define the Cameron-Martin space shortly after. Typically they'll define a covariance operator first.  Let $U$ be a separable Banach space, and $\mu$ be a centered Gaussian measure on $U$ . $U^*$ is the dual.  The Covariance operator $C:U^*\times U^*\to\mathbb{R}$ is first given by $$C_{\mu}(f)(g):= \int_Uf(x)g(x)\mu(dx)$$ First define $|x|_{H(\mu)} = \sup_{l\in U^*}\{ l(x) : C_{\mu}(l)(l)\leq 1\}.$ The Cameron-Martin space is then defined as $$H(\mu) := \{x \in U : |x|_{H(\mu)} < \infty \}.$$ (At least that's one of a couple definitions) Intuitively, I've also heard that the Cameron-Martin space is the set of all elements that make the null sets of $\mu$ translation-invariant (ie you can shift a null-set by that element and it will still be a null set). But I still feel like I'm missing a broader perspective.","['stochastic-processes', 'stochastic-differential-equations', 'functional-analysis', 'partial-differential-equations', 'probability-theory']"
4363750,Is an order necessary in a finite or infinite cartesian product?,"The elements of $\displaystyle \prod_{i\in I}X_i$ are usually represented by tuples. For this cartesian product to be defined, is it necessary for $I$ to be an ordered set?",['elementary-set-theory']
4363798,"Example of a non complete, connected, non extendable Riemannian manifold with minimizing geodesics connecting any two of its points","A friend asked this question and I couldn't give a concrete answer. If we take out ""extendable"", then it's clear an open ball solves the problem. If we take out ""non complete"", then a cap of a sphere solves the problem. If we mantain both, however... I can't think of an example! The cone without its vertex is a non complete, non extendable manifold, but I think its geodesics don't satisfy the requirements. I'd be grateful if anyone could shed some light on this! Maybe there are no such examples, but I really wanna know one way or the other.","['surfaces', 'riemannian-geometry', 'geometry', 'manifolds', 'differential-geometry']"
4363846,What does the derivative really mean?,"I was introduced to calculus a few weeks ago, and while I can ""solve"" problems consisting of derivatives and integrals, I still do not truly understand what the derivative means.
Here are some of the arguments/explanations I have heard : 1. Derivative is the instantaneous rate of change. However, this to me never makes sense, for a change to occur there needs to be an interval, but if there is an interval, then it is not instantaneous? 2. Derivative is the slope of the tangent line. This is simple and easy to understand, but it I do not understand how the ""slope"" of the tangent line tells us how ""fast"" the function is changing at a point. 3. Derivative is the sensitivity of the function at the point. This to me, is the most appealing definition, at a point, the derivative measures how much my function will change around that point if i make a tiny change in my input variable. However, this still causes a bit of confusion to me. How does this ""sensitivity"" intuition lead to the limit definition of the derivative? I am sorry, if I made any conceptual errors in my interpretations of above definitions. It would be a great help if someone could help me understand derivatives better.","['calculus', 'soft-question', 'derivatives']"
4363852,"A generalized ""Rare"" integral involving $\operatorname{Li}_3$","In my previous post , it can be shown that $$\int_{0}^{1} 
\frac{\operatorname{Li}_2(-x)-
\operatorname{Li}_2(1-x)+\ln(x)\ln(1+x)+\pi x\ln(1+x)
-\pi x\ln(x)}{1+x^2}\frac{\text{d}x}{\sqrt{1-x^2} } 
=\frac{\pi^3}{48\sqrt{2} }.$$ But how we verify this? $$\int_{0}^{1} 
\frac{\operatorname{Li}_3(1-z)+\operatorname{Li}_3
\left ( \frac{1}{1+z}  \right ) +\frac{\pi^2}{3}\ln(1+z) 
-\frac{\pi z}{2}\ln(1+z)^2-\frac{1}{6}\ln(1+z)^3
- \frac{\pi z}{2}\ln(z)^2+\pi z\ln(z)\ln(1+z) }{1+z^2} 
\frac{\text{d}z}{\sqrt{1-z^2} } 
=\frac{35\pi\zeta(3)}{64\sqrt{2} }+\frac{\pi^3}{32\sqrt{2} }\ln(2).$$ Where $\operatorname{Li}_3$ is trilogarithm and $\zeta(3)=\operatorname{Li}_3(1)$ in the principal branch.
The same method seems not quite powerful. Any suggestion will be appreciated.","['integration', 'definite-integrals', 'real-analysis', 'polylogarithm', 'closed-form']"
4363861,Why should the sum of squares of two Independent normals be memory-less,"In section 11.3.1 of Introduction to probability models by Ross (10th edition), a very strange phenomenon is described. If you take two independent standard normal distributions and sum their squares, you get an exponential distribution with rate $\frac{1}{2}$ . This is proven mechanically. I'm looking into some intuitive insight into this. The exponential distribution is known to be memory-less. And it seems this sum of squares of two i.i.d. Gaussians is also memory-less. For the exponential distribution ( $X$ ), this means (from some $t>s$ ): $$P(s <X\; \& \; X<t) = P(X<t | s<X)P(s<X) = P(X>s)P(X<t-s)$$ Now for two Gaussians, $Y$ and $Z$ we get: $$P(Y^2+Z^2 < r^2+t | Y^2+Z^2>r^2) = P(Y^2+Z^2<t)$$ Is there some connection to a fundamental property of the Gaussian that helps get an intuitive explanation for this memoryless behavior?","['chi-squared', 'gaussian', 'exponential-distribution', 'probability']"
4363909,"Are there some solutions in $\mathbb{F}_{2^k}\times \mathbb{F}_{2^k}$ to the equation in $x,y$: $x^{2^t-1}=y+y^{2^t}$?","Let $k$ be an odd positive integer and $t$ a positive integer with $t<k$ and $t|k$ . Denote $\mathbb{F}_{2^k}$ the finite field with $2^k$ elements. I have a quenstion about the solutions in $\mathbb{F}_{2^k}\times \mathbb{F}_{2^k}$ to the following equation in $x,y$ : $$x^{2^t-1}=y+y^{2^t}$$ According to some expriements with Magma for $k=9,15,21$ , the above equation in $x, y$ has some solutions $(x,y)$ with $x\neq 0$ in $\mathbb{F}_{2^k}\times \mathbb{F}_{2^k}$ . Thus, I guess that the equation always has some solution $(x,y)$ in $\mathbb{F}_{2^k}\times \mathbb{F}_{2^k}$ with $x\neq 0$ . Can you help me to give an answer to this quenstion?","['algebraic-number-theory', 'finite-fields', 'galois-theory', 'algebraic-geometry', 'abstract-algebra']"
4363913,Why mean is the balancing point?,"4,5,7,12,34 are the data points. Median is 7. But, why mean would be the balancing point here? I know the mean distribute the data points evenly among individuals, which means if any individual would’ve scored any point, he’d have scored the mean . But, I don’t understand this balancing point thing.",['statistics']
4363927,"Proof verification: Given $\lim_{x\to 0}\frac{f(2x)-f(x)}{x}=0$ and $\lim_{x\to 0} f(x)=0$, show that $\lim _{x\to 0}\frac {f(x)}{x}=0$.","Given that $\lim_{x\to 0}\frac{f(2x)-f(x)}{x}=0$ and $\lim_{x\to 0} f(x)=0$ , it is to be proven that $\lim _{x\to 0}\frac {f(x)}{x}=0$ . Proof: Let $\epsilon\gt 0$ be fixed. \begin{align*}
\frac {f(x)}x&=\sum_{k=0}^n\color{blue}{\frac{f(x/2^k)-f(x/2^{k+1})}{x/2^{k+1}}}(1/2^{k+1})+\frac{\color{green}{f(x/2^{n+1})}}{x}\\
&=\sum_{k=0}^n\color{blue}{g_k(x)}(1/2^{k+1})+\frac{\color{green}{h_n(x)}}{x}\\
\text{There exists $N$ such that }\\ \left|\frac {f(x)}x\right|&\le \sum_{k=0}^N|g_k(x)|\frac 1{2^{k+1}}+\epsilon\sum_{k=N+1}^\infty\frac 1{2^{k+1}}+\frac 1{|x|}\epsilon|x|\\
\text{It follows that } \\&0\le\liminf \left|\frac {f(x)}x\right|\le \limsup\left|\frac {f(x)}x\right|\le \epsilon\sum_{k=N+1}^\infty\frac 1{2^{k+1}}+\epsilon\\
\text{Taking $N\to \infty$,it follows that}\\&0\le\liminf \left|\frac {f(x)}x\right|\le \limsup\left|\frac {f(x)}x\right|\le \epsilon\tag 1
\end{align*} Since $\epsilon\gt 0$ is arbitrary, it follows by $(1)$ that $\liminf \left|\frac {f(x)}x\right|= \limsup\left|\frac {f(x)}x\right|=0=\lim_{x\to 0} \frac{f(x)}x. \;\;\; \blacksquare$ Is my proof correct? Thanks.","['limits', 'calculus', 'solution-verification', 'real-analysis']"
4364009,finding derivatives of variables in multivariable taylor polynomial,"Given: $$F(x,y) = -6 -4(x-4) +6(y-6) +8(x-4)^2 +9(x-4)(y-6) -4(y-6)^2 + R_2$$ and that $F(4,6)=-6$ find y'(4), y''(4), make sure that the function is applicable for the implicit function theorem. my attempt: so I've basically made sure that the conditions applied, implicitly derived F and found out y'. It was a lot of hard work and while I got a result my method didn't at all utilize the fact it's taylor -I think there must be an easier way but I just can't put my finger on it. I think it'll utilize the fact that $-4 = F_x(x,y)$ and that $6 = F_y(x,y)$ - and maybe the chain rule?
since $$-4 = \frac{\partial F(4,6)}{\partial{x}}=\frac{\partial{y}}{\partial{x}}* \frac{\partial F(4,6)}{\partial{y}} = 6*y'(x) \Rightarrow y'(x) = \frac{-2}3$$ but both my calculation and this calculator https://www.emathhelp.net/en/calculators/calculus-1/implicit-differentiation-calculator/?f=-4%28x-4%29%2B6%28y-6%29%2B8%28x-4%29%5E2+%2B9%28x-4%29%28y-6%29+-4%28y-6%29%5E2%3D0&type=x&px=4&py=6 output $2/3$ To sum up - where is my logic faulty? is there a similar easier way? how do I extend this to y''(4)?","['multivariable-calculus', 'implicit-differentiation', 'taylor-expansion', 'implicit-function-theorem']"
4364045,A certain free product of groups is virtually torsion-free,"Suppose that $G_1,\ldots,G_n$ are finite groups, and $m\geqslant 0$ is some integer. Set $$G=G_1\ast\cdots\ast G_n*F_m,$$ (where $F_m$ is the free group on $m$ generators). Then, is $G$ virtually torsion-free (i.e. has a finite index torsion-free subgroup)? It seems conceivable that the method given here just works in this context, but I am not familiar enough with geometric group theory.","['combinatorial-group-theory', 'free-groups', 'free-product', 'geometric-group-theory', 'group-theory']"
4364070,How to understand the explanation of conjugate point in Petersen's Riemannian geometry,"I am reading Petersen's Riemannian geometry.He uses distance function to establish three equations. 1. $L_{\partial_{r}} g=2 \operatorname{Hess} r$ 2. $\left(\nabla_{\partial_{r}} \operatorname{Hess} r\right)(X, Y)+\operatorname{Hess}^{2} r(X, Y)=-R\left(X, \partial_{r}, \partial_{r}, Y\right)$ 3. $\left(L_{\partial_{r}} \operatorname{Hess} r\right)(X, Y)-\operatorname{Hess}^{2} r(X, Y)=-R\left(X, \partial_{r}, \partial_{r}, Y\right)$ Then he says if we assume that the curvature is bounded,
then equation (2) tells us that, if the Hessian blows up, then it must be decreasing
as r increases, hence it can only go to $-\infty$ : I'm not sure what 'blow up'here means here, and I cannot understand why this implies that Hessian is decreasing. He also says A conjugate point occurs when the Hessian of r
becomes undefined as we solve the differential equation for it. I cannot imagine when and why will the Hessian of r becomes undefined. Other places in this section are difficult to understand too since there's only literal explanation without examples or calculations. Any help will be thanked.","['riemannian-geometry', 'differential-geometry']"
4364074,Calculating a limit with exponents,"There is a limit I want to calculate, and I've calculated it as follows: $$\lim_{x \to \infty} \frac{7^{-x+1}-2\cdot 5^{-x}}{3^{-x}-7^{-x}} = \lim_{x \to \infty} \frac{\frac{7}{7^x}-\frac{2}{5^x}}{\frac{1}{3^x}-\frac{1}{7^x}}$$ If I now multiply the denominator and numerator by $3^x$ , I get the following: $$\lim_{x \to \infty} \frac{7\cdot\frac{3^x}{7^x}-2\cdot\frac{3^x}{5^x}}{1-\frac{3^x}{7^x}} = \frac{0}{1}.$$ Is it correct to assume that the fractions in the numerator will limit to $0$ and not to infinity (because they are multiplied by $7$ and $2$ respectively)? Does this way of thinking apply to all limits of such format (except for those that limit to $e$ ) or are there exceptions?","['limits', 'exponentiation']"
4364147,"$( 1 - \sin x ) ( 1 + \csc x ) = \cos x \; , \; 0 \leq x \leq 2 \pi$","I found some solutions to this problem however I appear to be missing some solutions. Could anyone help me find the whole solution set? $$( 1 - \sin x ) ( 1 + \csc x ) = \cos x \; , \; 0 \leq x \leq 2 \pi$$ $$1 + \csc x - \sin x - \sin x \csc x = \cos x $$ $$ 1 + \frac {1}{\sin x} - \sin x - 1 = \cos x $$ $$ \frac {1}{\sin x} - \sin x = \cos x $$ $$ \frac {1}{\sin x} = \cos x + \sin x $$ $$ 1 = \sin x (\cos x + \sin x) $$ $$ \sin^2 x + \cos^2 x = \sin x \cos x + \sin^2 x $$ $$ \cos^2 x = \sin x \cos x $$ $$ \cos x = \sin x $$ $$ \frac {\sin x}{\cos x} = 1 $$ $$\tan x = 1$$ $$ x = \frac \pi 4, \, \frac {5 \pi}{4}$$",['trigonometry']
4364186,Show that the law of $P(X_n=0)=\frac{1}{n^\alpha}=1-P(X_n=1)$ converges weakly?,"The question is suppose that $\mu_n$ is the law of independent random variables $\{X_n\}$ , where \begin{equation*}
P(X_n=0)=\frac{1}{n^\alpha}=1-P(X_n=1)
\end{equation*} where $\alpha \in (0,\infty)$ . For what value of $\alpha$ will weak convergence, almost sure convergence and convergence in probability holds true? Convergence in Probability: Let $\{X_n\}$ be a sequence of independent random variables defined as \begin{align*}
\begin{cases}
X_n = 0 & w.p~~\frac{1}{n^\alpha}\\
X_n=1 & w.p~1-\frac{1}{n^\alpha}
\end{cases}
\end{align*} Then, $\lim_{n\rightarrow \infty}P(|X_n-1|>\epsilon)=\lim_{n\rightarrow \infty}P(X_n=0)=\lim_{n \rightarrow \infty}\frac{1}{n^\alpha}=0$ . This implies that $X_n \rightarrow 1$ in probability. Almost sure convergence: Let $A_n$ be the event that $\{X_n=0\}$ . Then, $A_n's$ are independent and $\sum_{n=1}^{\infty} P(A_n)=\infty$ for $\alpha \in(0,1]$ . Then by Borel-Cantelli lemma 2w.p. 1 infinitely many $A_n's $ will occur. I am not sure what to say about when $\alpha >1$ and weak convergence? Can anyone give some idea about it?","['measure-theory', 'probability-theory']"
4364214,"Highest weight of standard exterior power representation of $\mathfrak sl(n,\mathbb C)$","Consider the standard representation of $\mathfrak sl(n,\mathbb C)$ on $\mathbb{C}^n$ . Let $1 \leq i < n$ . Consider the quotient representation of $\mathfrak sl(n,\mathbb C)$ on the exterior power $\bigwedge^i(\mathbb{C}^n)$ . Consider the Cartan subalgebra $\mathfrak h$ of $\mathfrak sl(n,\mathbb C)$ consisting of diagonal, traceless matrices. Denote by $L_j$ the linear form $\mathfrak h \rightarrow \mathbb{C}$ that picks out the $j$ -th diagonal entry. Fulton and Harris claim that $\bigwedge^i(\mathbb{C}^n)$ has heighest weight $L_1 + ...+ L_i$ . I am struggling to verify that claim. I am even unable to show that $L_1 + ...+ L_i$ is a weight. Any suggestions?","['lie-algebras', 'representation-theory', 'abstract-algebra', 'linear-algebra', 'exterior-algebra']"
4364226,Correlation of $X^3$ and $Y$,Suppose that $X$ and $Y$ are two uncorrelated random variables and the PDF of them are both even. Can we expect that $X^3$ and $Y$ are also uncorrelated? Thanks.,"['correlation', 'statistics', 'probability']"
4364239,What is the operator norm of the symmetric difference operator on a finite periodic lattice?,"Let \begin{equation}
\Lambda_n:=\{ -\frac{1}{2}, -\frac{1}{2}+\frac{1}{2^n}, \cdots, 0 ,\cdots ,\frac{1}{2}-\frac{1}{2^n} , \frac{1}{2} \}
\end{equation} be a finite lattice and think of the vector space over $\mathbb{C}$ defined as \begin{equation}
V_n:=\{ \text{all }f: \Lambda_n \to \mathbb{C} \mid f\text{ is periodic } \}
\end{equation} Then, since $\Lambda_n$ is a finite set, $V_n$ is clearly a finite dimensional vector space. We can give the inner product on $V_n$ in an obvious way as well. However, I am quite confused about the following linear operator: \begin{equation}
D : V_n \to V_n 
\end{equation} defined as \begin{equation}
(Df)(x):=2^{n+1}[f(x+\frac{1}{2^n})-f(x-\frac{1}{2^n})]
\end{equation} This is clearly a well-defined linear operator on $V_n$ and thus must have a matrix representation. I can see that it has no diagonal components so that its trace is always zero, regardless of $n$ . However, what about its operator norm? I am stuck at calculating it. Does the norm depend on $n$ ? Could anyone please help me?","['calculus', 'linear-algebra', 'functional-analysis']"
4364245,Find $f$ such that $xf(x)+x^2f(x-1)=f(x^2)$,I found this functional equation: $$xf(x)+x^2f(x-1)=f(x^2)\tag1$$ for all $x\in\mathbb{R}$ . I tried to solve it (that is: find all functions $f:\mathbb{R}\to\mathbb{R}$ such that (1) is true); but I only found that $f(0)=0$ and classical substitution $x-1\mapsto x$ doesn't give me any information. What do you think that can be a good approach?,"['functional-equations', 'functions']"
4364262,Intuitive way of describing a conformal distribution,"I was wondering if anyone knows a way to describe a conformal distribution in an intuitive way, preferably to someone who doesn't have much previous knowledge of differential geometry. I know that a distribution is a subbundle to the tangent bundle. Given a distribution $\mathcal{S}$ and its complementary distribution $\mathcal{T}$ , we can define its second fundamental form as $$B(X,Y)=\frac{1}{2}\mathcal{T}(\nabla_XY+\nabla_YX),$$ where $X,Y$ are smooth sections of $\mathcal{S}$ . The distribution $\mathcal{S}$ is conformal if there exists a vector field $V$ is $\mathcal{S}$ such that $$B(X,Y)=g(X,Y)\,\otimes\,V,$$ where $g$ is the Riemannian metric. What does this mean, in practice?","['conformal-geometry', 'tangent-bundle', 'riemannian-geometry', 'differential-geometry']"
4364298,Rigorous treatment of integration by parts in a Calculus 1 course,"I will be teaching Calculus 1 soon and I am trying to find some justifications for fishy arguments that are widespread out there. In a standard Calculus 1 course, the following concepts are presented to students. Antiderivative: A function $F$ is called an antiderivative of a function $f$ in an interval if $F'=f$ in that interval. Indefinite integral: the family of all the antiderivatives of a function $f$ is called indefinite integral of $f$ and is denoted by $\int f(x)dx$ . Having shown that the difference of any two antiderivatives of the same function is constant, if $F$ is an antiderivative of $f$ , then
we write $\int f(x)dx=F(x)+C$ , where $C$ is a constant. The problem I see is that some textbooks define the differential in a very vague manner and then foster the use of the equality $dy=y'dx$ without justification. For example, when presenting the integration by parts all starts fine with the product rule of two differentiable functions $u$ and $v$ : $$(uv)'=u'v+uv'\implies uv'=(uv)'-u'v$$ which implies that $$\int u(x)v'(x)dx=u(x)v(x)-\int u'(x)v(x)dx\quad\quad\quad (A)$$ The problem starts with the manipulation of the dummy symbols in the notation of the indefinite integral by the substitutions $dv=v'(x)dx$ and $du=u'(x)dx$ resulting in the popular formula: $$\int udv=uv-\int vdu\quad\quad\quad (B)$$ When I look at the definition of indefinite integral, equality (A) is well-defined but (B) is not. Into practice: Calculate $\int 2x\cos(x)dx$ . A student using (A) will write: let $u(x)=2x$ and $v'(x)=\cos(x)$ . Then $u'(x)=2$ and $v(x)=\int cos(x)dx=\sin(x)$ (here undertanding that we just need 1 (any) antiderivative) Then by (A) we have: $\int 2x\cos(x)dx=2x\sin(x)-\int 2\sin(x)dx=2x\sin(x)+2\cos(x)+C$ . When using (B) students use $u=2x$ and $dv=\cos(x)dx$ . Then compute $du=2dx$ and $v=\sin(x)$ , and finally replace the pieces into (B) as if they were TeX processors. I mean, the method relies on the syntax of (B), not in the definition of indefinite integral. Question: what is the mathematical justification to accept the use of (B)? The justification should be at the level of students taking Calculus 1. Remark: Note that substitutions of the type $dy=y'dx$ are not necessary for the substitution techniques of integration in a Calculus 1 course. Indeed, if $F'=f$ , then the chain rule shows: $$(F\circ g)'(x)=f(g(x))g'(x)$$ so by the definition of indefinite integral $$\int f(g(x))g'(x)dx=F(g(x))+C,$$ or equivalently, $$\int f(g(x))g'(x)dx=\left.\int f(u)du\right|_{u=g(x)}.$$ Update: Thanks to the answers posted, I realized that my concern was justified: (B) is (apparently) only justified after considering contents that are not part of a calculus 1 course, say, through Stieltjes integrals or differentials. Thank you for the well-presented answers and for the comments and resources presented in the comment sections. I am well aware that it would not be good to hide (B) from my students since as it was pointed out in the comments, students will face it sooner or later and they should be prepared for it. That is why I posted this question. I think I will present and mostly use (A) during the course. I will mention (B) stating that is true but we do not have the tools to prove it and that for now it can be used as a notation-wise shortcut for (A), so they have a way to justify steps that appear in many calculus textbooks, steps that are layout without a proper justification (and you wonder why people do not understand mathematics).","['integration', 'indefinite-integrals', 'education']"
4364308,In what situations are Path Integrals well-defined?,"In physics I have come across contexts where apparently path integrals are well-defined, and others where they are not. However I have no clear understanding of when and why they succeed or fail to be well-defined. Question 1. What are the obstructions to path integrals being well-defined? When path integrals are well-defined, what goes ‘right’ which otherwise typically goes wrong? I have seen other questions on here that are similar, but I have never really understood the answers. For instance often approaches involving the limit of a discretization lead to divergences. Can a regularization scheme not be included in the path integral definition, to avoid this? Even if this does not work, are there no other integration measures over path or configuartion spaces which give sensible answers? Or do appropriate measures only exist in special cases? Question 2. Is there a simple summary of situations where we currently know path integrals are well-defined? For instance, this question appears to indicate that path integrals with Euclidean action over paths in $\mathbb{R}^n$ are always well defined. Question 3. I have heard path integrals are always well-defined for TQFTs. If this is true, then why?","['functional-calculus', 'topological-quantum-field-theory', 'functional-analysis', 'quantum-field-theory', 'quantum-mechanics']"
4364317,Double integral of a logarithm multiplied by an exponential,"I would like to know the value of the following integral: $$
f(\alpha,\beta,\gamma)=\int_{0}^{1}\int_{0}^{1}\ln{(t+i\alpha t')}~e^{i(\beta t+\gamma t')}\,\mathrm{d}t\,\mathrm{d}t'
$$ where $\alpha$ , $\beta$ and $\gamma$ are real-valued constants. The integral arises when considering surface currents on electrical conductors. Mathematica will calculate the integral analytically for individual integer values of the constants, in terms of the exponential integral, so I know an expression should be possible, but I'm not sure how to do this more generally. It is possible that the method in How to double integrate the product of a logarithm and an exponential may be helpful, but I don't see how to modify it to cover the difference in the argument of the logarithm here. Thanks in advance for any help.","['integration', 'definite-integrals', 'logarithms', 'multiple-integral', 'exponential-function']"
4364326,Prove non-existence of UMP test for Cauchy distribution,"I wish to prove for $X_1,\dots,X_n$ i.i.d. Cauchy( $\theta,1$ ) random variables that there doesn't exist a UMP for $H_0: \theta = 0$ against $H_1: \theta>0$ . For $\theta_1<\theta_2$ , NP tests $(\varphi_1)$ for $H_0: \theta = 0$ against $H_1: \theta=\theta_1$ and $(\varphi_2)$ $H_0: \theta = 0$ against $H_1: \theta=\theta_2$ should produce the respective critical regions $$\prod_{i=1}^n \frac{1+(x_i-\theta_1)^2}{1+x_i^2} < k$$ and $$\prod_{i=1}^n \frac{1+(x_i-\theta_2)^2}{1+x_i^2} < k$$ I'm at a bit of a loss how to proceed from here. How do I show with rigor that these regions are not equal?","['statistics', 'hypothesis-testing']"
4364345,Stuck proving that $\mid z\mid^2$ is not analytic in $z$,"I am proving that $f(z) = \mid z\mid^2$ is not an analytic function. So i didn't want to use the Cauchy-Riemann condition or anything but i know that this particular function is diffentiable at only $z=0$ and nowhere else. So I check the differentiability at $z=0$ without any difficulty , just by using the definition of diffentiable complex valued function as in pic : Now I let a another arbitrary point $z_0 \neq 0 $ and check the differentiability at $z_0$ just by using the existence of this limit $$\lim_{z \to z_o} \frac{f(z)-f(z_0)}{ z - z_0}$$ $\implies$ $\lim_{z \to z0o} \frac{\mid{z}\mid^2-\mid{z_0}\mid^2}{ z - z_0}$ $\implies$ $\lim_{z \to z_0} \frac{(X^2-X_0^2) + ( Y^2 -Y_0^2 )}{ (X-X_0) + (Y-Y_0)\iota}$ But now I am stuck that how can I provde the non-existence of Limit  . If I will rationalise then also not getting any satisfactory results. Or choosing two different path is looking impossible because $z_0$ is an unknown point.","['complex-analysis', 'limits', 'calculus', 'analyticity']"
4364346,Geodesics in hyperboloid model of hyperbolic geometry,"Let $X$ be the north sheet of the hyperboloid $ z^2=x^2+y^2+1$ and $g=dx^2+dy^2-dz^2$ a scalar product on the tangent space at every point. I have showed that $(X,g)$ , is a Riemannian surface , both diffeomorphic and isometric to the Poincaré disk $\mathbb{D}=\{(x,y,0)\in \mathbb{R^3},x^2+y^2<1\}$ with the hyperbolic metric, via the stereographic projection from the point $q=(0,0,-1)$ . My question is this, I know that the geodesics on $\mathbb{D}$ (with the hyperbolic metric) are its diameters and its intersections with euclidian circles orthogonal to $S^1$ ,  and since stereographic projection is both bijective and an isometry, the geodesics on $X$ should be the preimages of geodesics on $\mathbb{D}$ under stereographic projection, i.e. intersections of the Hyperboloid with cones having a vertex at the point $q$ and whose cross section on $\mathbb{D}$ is a geodesic. However I have read that geodesics on the hyperboloid model are its intersections with planes in $\mathbb{R^3}$ passing through the origin. How can you see that these two descriptions are equivalent?","['riemannian-geometry', 'differential-geometry']"
4364372,How is the t-distribution a statistic considering it is dependent on parameter mu?,"Casella and Berger textbook teaches that a statistic is only the function of a random sample and never its unknown population parameter(s). In the case of Student's t-distribution, while it utilizes the sample variance in place of the unknown variance population parameter, its distribution still relies on the mean mu population parameter. Shouldn't this disqualify Student's t-distribution as being a statistic? Thanks for any information","['statistical-inference', 'statistics', 'normal-distribution', 'probability-theory', 'probability']"
4364438,Are nonmeasurable sets whose sections are null always contained in a null measurable set?,"Equip $\mathbb R_{\geq 0} \times \Omega$ with the product measure structure of Borel measure for $\mathbb R_{\geq 0}$ and a probability measure for $\Omega$ . Suppose we have a set $S=\cup_{t \geq 0}(t,E_t)$ all of whose sections $E_t$ are measurable and have probability zero. Unfortunately, $S$ is potentially nonmeasurable. Can we at least prove that it is contained in a measurable set of zero product measure? The usual example of a nonmeasurable set on the diagonal of the Borel unit-square fails to form a counterexample since the diagonal is measurable and null.","['measurable-sets', 'measure-theory', 'lebesgue-measure', 'probability-theory']"
4364469,Is a differentiable multivariable function with continuous derivatives on analytic paths continuously differentiable?,"Let $f: \mathbb R^n \rightarrow \mathbb R$ be an everywhere differentiable function; and continuously differentiable when restricted to any analytic path. Is then $f$ continuously differentiable?
I would bet on no, but I failed to construct a counter-example in a reasonable time.
But maybe I am overthinking, and there should be a way to show the continuous differentiability? This problem came to me from my research: I found myself in a company of a functional, which is smooth outside of some codimension 1 piecewise-analytic set. I was able to show that over this set the functional is differentiable. It was also clear that its restriction to any analytic path was nice. So I started wondering whether I can rely on that my functional friend is $C^1$ .","['multivariable-calculus', 'derivatives', 'real-analysis']"
4364471,Use Schrödinger's equation in order to derive a differential equations,"We will study the time-evolution of a finite dimensional quantum system. To this end, let us consider a quantum mechanical system with the Hilbert space $\mathbb{C}^2$ . We denote by $\left . \left |   0  \right \rangle\right .$ and $\left . \left |   1  \right \rangle\right .$ the standard basis elements $(1,0)^T$ and $(0,1)^T$ . Let the Hamiltonian of the system in this basis be given by $$
H=\omega\begin{pmatrix}
0 &1 \\ 
1 &0 
\end{pmatrix}=\begin{pmatrix}
0 &-i \\ 
-i &0 
\end{pmatrix}
$$ and assume that for $t=0$ the state of the system is just given by $\psi(t=0)=\left . \left |   0  \right \rangle\right .$ . In the following, we also assume natural units in which $\hbar=1$ . We expand the state at time t in the basis $|   0   \rangle$ , $|   1   \rangle$ so: $$|\Psi(t)\rangle=\alpha_0(t)|0\rangle+\alpha_1(t)|1\rangle$$ Problems :
Use Schrödinger's equation in order to derive a differential equations for $\alpha_0$ and $\alpha_1$ : (i) Find a solution given the initial conditions. (ii) What is the probability that the system can be measured in $\left . \left |   1  \right \rangle\right .$ at some time $t$ ? I hope anyone can help me with some hints how to show that derive a differential equations? I'm not sure about the notation $|\Psi(t)\rangle=\alpha_0(t)|0\rangle+\alpha_1(t)|1\rangle$ . Can anyone help me? I have that the  Schrödinger equations is $ih \frac{d\phi(t)}{dt}=H \phi(t)$ . But I'm not sure how to deal with the calculation and how to solve the equation, by the condition $|\Psi(t)\rangle=\alpha_0(t)|0\rangle+\alpha_1(t)|1\rangle$ . Is it one condition or two and how can I use it to solved the equation, I have not seen conditions on that form before. Can anyone help me?
I can see many have seen the problem. No one can give me some hints/equations?","['ordinary-differential-equations', 'analysis', 'complex-analysis', 'hilbert-spaces', 'quantum-mechanics']"
4364494,Discontinuous function with continuous restriction to any smooth curve,"The famous function $$f(x,y) = \cases {0,&if $(x,y)=(0,0)$\\
\frac{xy^2}{x^2+y^4} &otherwise}$$ is continuous when restricted to any smooth curve of the form $(x, g(x))$ , but is discontinuous at $(0, 0)$ over the curve $(y^3, y)$ . But if a function $f: \mathbb R^2 \rightarrow \mathbb R$ is continuous when restricted to any smooth curve in $\mathbb R^2$ , does it need to be continuous?
I would bet on no, but I can not come up with a sound construction at the moment. What about restrictions only to analytic curves?","['continuity', 'multivariable-calculus', 'real-analysis']"
4364495,Optimal control of posterior belief over a finite horizon,"$\large \textbf{Preface:}\ $ Below I describe a dynamic programming problem I am not sure how to formalize. In short: a (Bayesian-updating) agent sequentially runs costly experiments over a finite number of periods before predicting an unknown. The agent essentially wants to ""control"" the posterior belief they will have at the time they have to make the prediction. (It is sort of like a finite-horizon bandit problem but with $\textit{only}$ a terminal payoff). The problem is described in detail below; this is followed by my questions and my progress thus far. I employ footnotes (denoted $^{[n]}$ , $n\in\mathbb{N}$ ) to ""declutter'' while preserving details you may find useful. Thank you very much to anyone who takes the time to consider my question. Please let me know if any clarification/further detail is needed. $\large \textbf{Problem Description} $ This problem essentially considers an $\textit{agent}$ who learns about a random variable (""state"") $Y$ over $T\in\mathbb{N}$ time periods. First, $Y\sim\text{Bernoulli}(\mu)$ is drawn; the agent does not observe the realization $y\in\{0,1\}$ of $Y$ but has a ""correct"" prior belief. $^{[1]}$ Each $\textit{period}$ $t\in\{1,2,...,T\}$ has the following timing: First, the agent chooses $\alpha_t\in[0,1]$ at cost $c(\alpha_t)$ . $^{[2]}$ Agent observes realization $z_{t}\in\{0,1\}$ of random variable (""signal"") $Z_t$ with conditional pmf $\psi(\cdot|y,\alpha_t)$ . Assume that \begin{equation}
\psi(1|y,\alpha_t) = \text{Prob}\{Z_t=1|Y=y,\alpha_t\} =
\begin{cases} 
\frac{1}{2}-\left(\beta-\frac{1}{2}\right)\alpha_t , & \text{if } y = 0\\
\frac{1}{2}+\left(\beta-\frac{1}{2}\right)\alpha_t , & \text{if } y = 1
\end{cases}, \qquad (1)
\end{equation} where $\beta\in (.5,1)$ ; of course, $\psi(0|y,\alpha_t)=1-\psi(1|y,\alpha_t)$ . $^{[3]}$ Agent forms posterior ""belief"" (distribution) about $Y$ using Bayes' rule. Denote this by $b_t(\cdot|\boldsymbol{h}_t)$ , where $\boldsymbol{h}_t \equiv (\alpha_1,z_1,...,\alpha_t,z_t)$ is the ""history"" of past signal realizations and $\alpha_{\bullet}$ choices. (In an abuse of notation let $h_0\equiv\emptyset$ .) Finally, in period $T+1$ , the agent chooses an $\textit{action}$ $a\in\{0,1\}$ and earns payoff $u_{a y}$ . $^{[4]}$ Assume that $\min\{u_{11},u_{00}\}>\max\{u_{01},u_{10}\}$ (i.e. wants to ""match the state""). Given belief $b_T(\cdot|\boldsymbol{h}_T)$ , their expected payoff is \begin{equation}
W(a;b_T(\cdot|\boldsymbol{h}_T)) := \mathbb{E}_{_{Y\sim b_T(\cdot|\boldsymbol{h}_T)}} u_{aY} =  b_T(0|\boldsymbol{h}_T)u_{a0}+b_T(1|\boldsymbol{h}_T) u_{a1}. \qquad  (2)
\end{equation} The agent does not discount time. Their objective it to maximize their expected terminal payoff. What this means for their choice of $a$ at time T is easy (choose $a$ to maximize (2) given belief; solution given in (3)). What is giving me trouble is formulating the part of the problem before time $t$ . $\large \textbf{My Questions:} $ how does one formulate the above problem as a dynamic optimization problem?  Is a (closed-form) solution known? $^{[5] \ \leftarrow\ \text{(possible “quality of life"" improvements)} }$ My attempt at answering the first question is in equation (5), below. An extra big thank you to anyone who made it this far! $\ $ $\large \textbf{My Attempt} $ First off, the optimal choice of $a$ at time $T+1$ is very easy to characterize given belief $b_T\equiv b_T(\boldsymbol{h}_T)$ . Recall that the probability of event "" $Y=1$ '' assigned by belief $b_T$ is denoted by $\mu_T \equiv \mu_T(\boldsymbol{h}_T)$ . Then optimal choice $a^*(\mu_T)$ is given by \begin{equation}
a^*(\mu_T) = \begin{cases} 0, & \text{ if } \mu_T < \frac{u_{00}-u_{10}}{(u_{11}-u_{01})+(u_{00}-u_{10})}\\ 1, & \text{otherwise}^{[6]}  \end{cases} \qquad (3)
\end{equation} which yields expected payoff \begin{equation}
W^*(\mu_T) = \begin{cases} \left[1-\mu_T\right]u_{00}+\mu_T u_{01}, & \text{ if } \mu_T < \frac{u_{00}-u_{10}}{(u_{11}-u_{01})+(u_{00}-u_{10})}\\ \left[1-\mu_T\right]u_{10}+\mu_T u_{11}, & \text{otherwise}  \end{cases} \qquad (4)
\end{equation} $\boldsymbol{Now}\ \boldsymbol{for}\ \boldsymbol{the}\ \boldsymbol{hard}\ \boldsymbol{part}.$ Let $\boldsymbol{\alpha}_{r:s} := (\alpha_r,...,\alpha_s)\in[0,1]^{r-s}$ for $1\leq r < s \leq T$ . Similarly define $\boldsymbol{Z}_{r:s}$ and $\boldsymbol{z}_{r:s}$ , the vector of signals and realizations from time $r$ to $s$ , respectively. Given choices $\boldsymbol{\alpha}_{1:t}$ and observations $\boldsymbol{Z}_{1:t}=\boldsymbol{z}_{1:t}$ , the agent believes "" $Y=1$ '' with probability $$ \mu_t(\alpha_1,Z_1,...,\alpha_t,Z_t) =  \frac{\mu \prod_{\tau=1}^{t} \psi(Z_{\tau}|1,\alpha_\tau)}{\mu \prod_{\tau=1}^{t} \psi(Z_{\tau}|1,\alpha_\tau)+(1-\mu) \prod\limits_{\tau=1}^{t} \psi(Z_{\tau}|0,\alpha_\tau)} =\frac{1}{1+\frac{1-\mu}{\mu} \prod_{\tau=1}^{t} \frac{\psi(Z_{\tau}|0,\alpha_\tau)}{\psi(Z_{\tau}|1,\alpha_\tau)} }$$ I $\textit{think}$ the above equation is correct, and that it can be rewritten recursively as $$ \mu_t(\boldsymbol{h}_{t}) = \frac{\mu_{t-1}(\boldsymbol{h}_{t-1}) \cdot \psi(z_{t}|\alpha_t,1) }{\mu_{t-1}(\boldsymbol{h}_{t-1})\cdot\psi(z_{t}|\alpha_t,1) + [1-\mu_{t-1}(\boldsymbol{h}_{t-1})] \psi(z_{t}|\alpha_t,0) } = \frac{1}{1+\frac{1-\mu_{t-1}(\boldsymbol{h}_{t-1})}{\mu_{t-1}(\boldsymbol{h}_{t-1})} \frac{\psi(z_{t}|0,\alpha_\tau)}{\psi(z_{t}|1,\alpha_\tau)} }  .$$ In addition, the following also seems to be true. $$ \mu_t(\boldsymbol{h}_{t}) = \frac{\mu_s(\boldsymbol{h}_s) \prod_{\tau = s+1}^{t} \psi(Z_\tau|1,\alpha_\tau)}{\mu_s(\boldsymbol{h}_s) \prod_{\tau = s+1}^{t} \psi(Z_\tau|1,\alpha_\tau)+ [1-\mu_s(\boldsymbol{h}_s)] \prod_{\tau = s+1}^{t} \psi(Z_\tau|0,\alpha_\tau)} = \frac{1}{1+\frac{1-\mu_s(\boldsymbol{h}_s)}{\mu_s(\boldsymbol{h}_s)}\prod_{\tau=s+1}^t \frac{\psi(Z_\tau|0,\alpha_\tau)}{\psi(Z_\tau|1,\alpha_\tau)} } $$ where $1\leq s<t\leq T$ . This is the point at which I have difficulty making progress. I am not sure how to write the belief over $\mu_T$ from the agent's perspective at the beginning $^{[7]}$ of time $t$ ; denote this by $\eta_t(\cdot|{\color{red} {\textbf{unsure}} })$ . Moreover, I am simply unsure how to write the whole dynamic optimization problem, or make it recursive. My best attempt so far is as follows: For each $t\in\{1,...,T\}$ , let \begin{equation}
    \begin{array}{rcl}
        V_t({\color{red} {\textbf{unsure}} }) & = & \max\limits_{\alpha_t,...,\alpha_T} \mathbb{E}_{\mu_T \sim \eta_{t}(\cdot|\mu_{t-1}, \alpha_t,...,\alpha_T)} \left[ W^*(\mu_T) \right] - \sum_{\tau = t}^{T} c(\alpha_\tau)   \\
         & s.t. & \ \alpha_\tau \in [0,1]\ \  \forall \tau\in\{t,...,T\} \\ 
         & & \\
    \end{array} \qquad (5)
\end{equation} $\ $ $\large \textbf{Footnotes} $ $\quad$ [𝟏] I.e. the agent's prior (regarding $Y$ ) is $\text{Prob}\{Y=y\}\begin{cases} 1-\mu, & \text{if } y=0\\ \hfill \mu, & \text{if } y=1 \end{cases}$ . $\quad$ [2] Assume $c:[0,1]\to[0,\infty)$ is $\mathscr{C}^2$ , strictly monotonically increasing and strictly convex $\quad$ $\quad $ $\big(c'(\alpha_{\cdot}),c^{\prime \prime}(\alpha_{\cdot})>0\ \forall \alpha_{\cdot}\in [0,1]\big)$ , and $c(0)=0$ . $\quad$ [3] $Z_\tau$ is assumed to be conditionally independent of $Z_{\tau'}$ ( $\tau\neq \tau'$ ) given $\alpha_\tau$ and $Y$ . $\quad \quad \ \ $ Assume $\psi(\cdot|\cdot,\cdot)$ is known by the agent. $\quad$ [4] $\textbf{Alternative Payoff Function:}$ If the ""binary choice"" setup makes things difficult, $\quad \quad \ \ $ assume instead that the agent chooses $a\in[0,1]$ and earns payoff $u(a,y):=-(a-y)^2$ ; $\quad \quad \ \ $ expected payoff has the analogous form to (2). $\quad$ [5] If the answer to the second question is ""no"", would life be a lot easier if $\qquad \quad$ $\text{i}.\ $ terminal time $T$ was $\textit{random}$ ? (E.g. at the end of each period, the agent is forced to stop $\qquad \quad \quad\ $ learning and choose $a$ with probability $\delta\in(0,1)$ .) $\qquad \quad$ $\text{ii}.\ $ Signals $Z_\bullet$ were $\textit{continuous}$ r.v.'s? $\qquad \quad$ $\text{iii}.\ $ time was $\textit{continuous}$ ? $\qquad \quad$ $\text{iv}.\ $ $T$ is a $\textit{stopping time}$ optimally chosen by the agent? $\quad$ [6] Technically indifferent if equality holds; assume for simplicity that $a^*(\cdot)=1$ in this case. $\quad$ [7] I.e. before $\alpha_t$ is chosen.","['optimal-control', 'bayesian', 'dynamic-programming', 'recreational-mathematics', 'probability-theory']"
4364523,How do I solve this trigonometry word problem concerning sinusoidal functions? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question To my understanding, amplitude $= 70$ and angular frequency = $10\pi$ . However, I don't know if it is a cosine/sine graph, and I'm not sure about the midline. Can someone help me solve this math problem? Please help.","['trigonometry', 'functions']"
4364546,An identity with vector products,"Consider $a$ , $b$ , $c$ , $a'$ , $b'$ , $c'$ points in $\mathbb{R}^3$ . Denote by $[ab]$ the cross product of the vectors $a$ , $b$ , $(a b)$ their dot product , and $[abc] = ([ab]c)$ their mixed product. Let $x=[bc]$ , $x'=[b'c']$ , $y =[ca]$ , $y'=[c'a']$ , $z=[ab]$ , $z'=[a'b']$ . Show that $$[[xx'][yy'][zz']]=[abc][a'b'c'][[aa'][bb'][cc']]$$ Notes: This is from Gurevich, Foundations of the Theory of Algebraic Invariants, p. 138. Why is the identity important?  If $a$ , $b$ , $c$ , $a'$ , $b'$ , $c'$ are viewed as points in $\mathbb{P}^2(\mathbb{R})$ then $[[bc][b'c']]$ is the point of intersection of the lines $bc$ , $b'c'$ . So the LHS gives the condition that the points $bc\cap b'c'$ , $ca\cap c'a'$ , $ab\cap a'b'$ are collinear, while the third factor on RHS is the condition that the lines $aa'$ , $bb'$ , $cc'$ is concurrent. So this would give a proof of Desargues' theorem , and its converse. Here are identities that are listed as helping in the proof: $$[[ab]x]= (ax)b-(bx)a \\
[[ab][cd]] = [acd]b-[bcd]a = [abc]d-[abd]c$$ (another identity $([ab][cd]) = (ac)(bd)-(ad)(bc)$ might be useful here) From the second identity we get $$[[ab][cd][ef]]=[acd][bef]-[bcd][aef]=[abc][def]-[abd][cef]$$ This would allow us to tackle the LHS of the required identity, but I am a bit stuck at the moment.  Maybe both sides can be expressed in terms of mixed products, and then we can deduce all from the Plucker relations . Any ideas/hints would be very helpful!","['projective-geometry', 'linear-algebra', 'geometry', 'invariant-theory']"
4364549,How wild are weakly continuous curves in a Banach space?,"For one reason or the other I work in a reflexive infinite dimensional Banach space $X$ and I am interested in curves $$
\gamma:[0,1]\to X
$$ that are only weakly continuous. I have no idea how to visualise their behaviour. My best understanding of how bad things can go is that we have sequences on the unit sphere that converge to any point inside the unit ball.
From this observation it is clear that I can always build this behaviour on one point, but how much can I push it? Even a countable set seems doable, but I would still get a locally strongly continuous curve. In general that means that I could expect curves to wildly jump inside and outside of some fixed ball. I would be interested in one such example if there is one.
In such constructions one things tends to happen, and it seems that the derivative ought to explode. We know that the weak topology in a reflexive Banach space $X$ can be metrized locally when the space $X$ is separable, and thus we obtain a notion of Lipschitz continuous curve and absolutely continuous curve when considered with respect this metric. So my question are: Can you show an example of a weakly continuous curve that is not strongly continuous in at least a dense set if not more, and/or Are there examples of weakly continuous curves in separable reflexive Banach spaces that are not strongly continuous but are absolutely continuous or Lipschitz with respect to the metrization of some ball?","['weak-convergence', 'banach-spaces', 'weak-topology', 'analysis']"
4364561,Is “defective eigenvalue“ a common term?,"Is defective eigenvalue an acceptable term that is used often? By a defective eigenvalue, I mean an eigenvalue whose geometric multiplicity is strictly less than its algebraic multiplicity. I have seen it used occasionally but not enough to be sure that it is an “official” terminology in the sense that defective matrix is definitely an official terminology.","['linear-algebra', 'terminology']"
4364615,Why can we use MCT in the proof of Fubini's theorem (in Stein's book)?,"In Stein's Real Analysis , he uses six steps to prove Fubini's theorem. First he uses $\mathcal F$ to denote the set of integrable funcyions satisfying the conclusions in the theorem: $f\in L^1(\mathbb R^{d_1}\times \mathbb R^{d_2})$ and $f^y\in L^1(\mathbb R^{d_1})$ a.e. $y\in \mathbb R^{d_2}$ . $\int_{\mathbb R^{d_1}}f^y(x)\mathrm d x\in L^1(\mathbb R^{d_2})$ . $\int_{\mathbb R^{d_2}}\int_{\mathbb R^{d_1}}f(x,y)\mathrm dx\mathrm dy=\int_{\mathbb R^d}f.$ However, it confuses me that the definition of $\mathcal F$ is unclear, because it's not trivial that if $f\in\mathcal F$ and $f=g$ a.e., then we have $g\in \mathcal F$ . Besides, when proving that $\mathcal F$ is closed under operation of limits, he assumes that $\mathcal F\ni f_k\nearrow f$ (the $\nearrow$ sign implies "" $f_k\to f$ increasingly a.e."" in the book) Then he applies MCT on $f_k^y$ , but I can't see why $f_k^y\nearrow f^y$ is true? (If we have $\lim f_k=f$ for every point in $\mathbb R^d$ , then we can concludes that $f^y_k$ is integrable except a set $A(m(A)=0)$ , then for all $y\in A^c$ , $f^y_k\nearrow f^y$ is true. But here we must prove when $f_k\nearrow f$ , we also have $f^y_k\nearrow f^y$ ?) I think the two questions above can both be solved if we can prove that any function supported on a set of measure zero is in $\mathcal F$ , so we can drop the condition ""a.e."" to prove $f^y_k\to f^y$ increaingly everwhere. Hence it's sufficient to prove that if $m(E)=0$ , then $m(E^y)=0$ a.e. $y\in \mathbb R^{d_2}$ ?
(But Stein proves this fact on step 4, which relies on the result we just want to prove.) I don't know if I have missed something important or misunderstood the author, please point out any mistakes or give any hints.","['measure-theory', 'lebesgue-measure', 'real-analysis', 'measurable-functions', 'fubini-tonelli-theorems']"
4364667,Why is the Kolmogorov-Smirnov statistic distribution free?,"Context Let $X_1,..X_n$ iid from X with an unknown cumulative distribution function F. Let $F_0$ be the hypothetized cdf of X. One may use the Kolmogorov-Smirnov (KS) statistic to test for the hypothesis test : \begin{align}
H_0 : F = F_0 \\
H_1 : F \neq F_0 
\end{align} The Kolmogorov-Smirnov (KS) statistic is : \begin{align}
    D_n = sup_x |F_n(x)-F_0(x)|
\end{align} The empirical cdf F_n(x) is : \begin{align}
    F_n(x) = \frac{i}{n} \ \text{for} \ X_{(i)}\leq x < X_{(i+1)} i = 0,1..,n
\end{align} Question I fail to understand the distribution-free property of the KS statistic. If by $D^+_n$ I denote : \begin{align}
    D^+_n = sup_x(F_n(x)-F_0(x))
\end{align} Then I get the following : \begin{align}
    D^+_n = \sup_x(F_n(x)-F_0(x)) &= \max_{0<i\leq n} \sup_{X_{(i)}\leq x < X_{(i+1)}} (F_n(x) - F_0(x)) \\
    &= \max_{0<i\leq n} \sup_{X_{(i)}\leq x < X_{(i+1)}} (F_0(x) - \frac{i}{n}) \\
    &= \max_{1<j\leq n+1} (F_0(X_{(j)}) - \frac{j-1}{n})
\end{align} Which according to one of my textbook shows the distribution-free property of the KS statistic, because $(F_0(X_{(j)})$ would be the order statistic from a uniform distribution (0,1). I fail to understand why $(F_0(X_{(j)})$ are the order statistics from the uniform distribution.","['statistics', 'probability-distributions', 'probability-theory']"
4364672,Connectedness and Compactness of $\mathbb{N}$ with basic sets of the form $\mathbb{N}_{\ge n}$?,"We have $X = \mathbb N$ . The topology is generated by the basic sets $A_n = \{n,n+1,n+2,...\} , n\in \mathbb N$ . Is $X$ connected and compact? My guess is that if we take any two open sets, one of them has to be contained in the other and hence, $X$ is both connected and compact.","['connectedness', 'general-topology', 'compactness']"
4364718,Finding $\mathbb{E}[h(X)\mid X^2]$,"Suppose that $h: \mathbb{Z}\rightarrow \mathbb{R}$ is such that $\mathbb{E}[|h(Z)|]< \infty$ . I have to prove that $$\mathbb{E}[h(X)\mid X^2]=h(|X|)\dfrac{p(|X|)}{p(|X|)+p(-|X|)}+h(-|X|)\dfrac{p(-|X|)}{p(|X|)+p(-|X|)}.$$ I know that $$\psi(y)=\sum_x h(x)P(X=x\mid X^2=y)=\dfrac{\sum_x h(x)P(X=x \land X^2=y)}{P(X^2=y)}$$ if $y > 0$ , $\sum_x h(x)P(X=x \land X^2=y)$ has two expressions, because $x=\pm\sqrt y$ . How to write the solution correctly?","['conditional-expectation', 'probability-theory', 'probability']"
4364776,How many ways to reach an integer by addition and multiplication,"Inspired by the title of a recent question, which turned out to be about something else. Start with any number.  Then repeatedly add any positive integer or multiply by any integer greater than $1$ . Let $f(n)$ be the number of ways to reach $n$ . For example, $f(3)=6$ because $$3=2+1=1+2=(1+1)+1=(1×2)+1=1×3$$ The first few values are https://oeis.org/A348378 $$1,3,6,15,27,63,117,...$$ On one hand, it always seems to be a multiple of $3$ .  On the other, the first twenty terms are close to $0.988×2^n$ with an oscillation of size around $1.4^n$ . How would one prove either of those things?","['combinatorics', 'sequences-and-series']"
4364777,How to check for roots of a polynomial from a certain number set in certain range?,"It's known that it's possible to determine whether some roots of a polynomial are rational by using rational root theorem . It's also known that Sturm's theorem can easily answer the question about existence of roots of a polynomial in a certain domain. With these two facts in mind, there is this pack of questions, breaking down the one in title: Is it possible to determine whether a polynomial has natural roots (such $x$ that $x\in\mathbb{N}$ )? Is it possible to determine whether a polynomial has integer roots (such $x$ that $x\in\mathbb{Z}$ )? Is it possible to determine whether a polynomial has irrational roots (such $x$ that $x\in\mathbb{I}$ )? Is it possible to determine whether a polynomial has real roots (such $x$ that $x\in\mathbb{R}$ )? Is it always true that any polynomial has complex roots (such $x$ that $x\in\mathbb{C}$ )? Can the questions above be answered if it's necessary to test for such roots in a certain domain (for example, integer roots inside $x\in(0,3)$ or rational roots inside $x\in(-5,8)$ , in general, test for existence of roots $x\in A$ inside $x\in(x_1,x_2)$ ), and if yes, which of these can be and how? Many thanks.","['functions', 'roots', 'polynomials']"
4364805,Limit of largest term in a recursively defined sequence: Why is it $\frac{9}{7}$?,"Sequence $a_n$ is defined as follows. $a_1=a_2=a_3=1$ For $n>3,\text{ } a_n=\frac{1}{2}(a_{n-1}+a_{n-2})-a_{n-3}$ Using excel, plotting $a_n$ against $n$ produces a sine-like curve, with the local maximums seeming to be unique and approaching (but not sequentially) some limit; same for the local minimums. My question is: $$\text{What is}\lim_{n\to\infty}\max{(a_1, a_2, a_3, ..., a_n)}\text{ }?$$ Using excel, the limit seems to be $\frac{9}{7}$ (e.g. the largest of the first 10,000 terms is approximately 1.2857142854), but I do not know how to prove this. (Context: I was looking at the sequence in this question and then started tweaking the sequence and seeing what happens.)","['limits', 'recurrence-relations', 'sequences-and-series']"
4364814,no. of minimum generating sets of $M_{n}(\mathbb{C})$,"Problem: How many $n \times n$ pairs $(A, B)$ of (0 , 1) matrices are there which generate the whole algebra $M_{n \times n}(\mathbb{\mathbb{C}})$ ? I don't know if this question is open or hopelessly hard. I would like to know related results and such.
To give some background each such pair will lead to an irreducible representation for a quiver with one vertex and two loops or in more common language irreducible $\mathbb{C}<x, y>$ modules of dimension $n$ I could find one as follows: From what we know it is enough that $A$ and $B$ generate All the elementary matrices $e_{i, j}$ . The idea is to choose $A$ such that $A$ by its own is able to create shifts of rows (on left multiplication by $A$ ) and $B$ does analgously for columns!. This way we can use one $e_{i ,j}$ to generate the rest! What is the matrix that shifts the $1st$ row to the $2nd$ row? \begin{bmatrix}
0 & 0 & 0 \\
1 & 0 & 0 \\
0 & 0 & 1
\end{bmatrix} But we need one matrix that does all the row shifts, i.e., that is shifts the 1st row to 2nd, 2nd to 3rd and so on! so we take ( 4 by 4 case): \begin{bmatrix}
0 & 0 & 0 & 0\\
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0\\
\end{bmatrix} We let this be our $A$ . So, when we multiply $A.C$ then the rows of $C$ get shifted. So, $A.e_{1, 1} = e_{2, 1}$ so on! Wonderful!
We can do analgously for columns by choosing a $B$ that does similar thing to columns! Easy to see that $B$ will just be the transpose of $A$ . So, from $e_{1, 1}$ we can get all $e_{i, j}$ . Now $A^{n-1} =e_{n ,1} $ and $B^{n-1} = e_{1, n}$ . and $e_{1, n}e_{n, 1} = e_{1, 1}$ I can find more by conjugating with permutation matrices. But not sure how to count since i dont know the stabilizers and the main question is are there others?","['quiver', 'representation-theory', 'matrices', 'linear-algebra', 'combinatorics']"
4364822,Are there incomplete normed spaces of arbitrary size?,"For every infinite cardinal, there is a vector space whose dimension is that cardinal. Is the situation the same for incomplete normed spaces? Can we have algebraic dimension of arbitrary size?","['elementary-set-theory', 'linear-algebra', 'functional-analysis']"
4364844,Orthogonal basis for $L^2$-martingales (martingale-Hardy space),"Let $(X,\mathcal{B}(X),(\mathcal{A}_n)_{n=1}^N,\mu)$ be a filtered probability space with $\mathcal{A}_N=\mathcal{B}(X)$ and let $H$ be the space of $\mathcal{A}_{\cdot}$ -adapted martingales $m_{\cdot}$ satisfying: $$
\mathbb{E}\left[\sum_{n=1}^N (m_n)^2dt\right]<\infty.
$$ The space $H$ is a separable Hilbert space with inner product $$
\langle m_{\cdot},m'_{\cdot}\rangle\triangleq \mathbb{E}\left[\sum_{n=1}^N|m_nm'_n|dt\right].
$$ Where can I find an example of an explicit orthonormal basis for this space?  I expect that, given an orthogonal basis $\{h^k\}_{k=1}^{\infty}$ of $L^2(\mathcal{B}(X))=L^2(\mathcal{A}_N)$ one can construct an orthogonal basis of $H$ using Doob's construction $$
h^{k}_n:=\mathbb{E}_{\mathbb{P}}\left[h_k\mid \mathcal{A}_n\right].
$$ However, is this true and, if so, can someone provide a reference to a body of literature for which this is used?","['stochastic-processes', 'hardy-spaces', 'probability-theory', 'martingales']"
4364863,Find Heisenberg evolution (matrix products with complex numbers),"We will study the time-evolution of a finite dimensional quantum system. To this end, let us consider a quantum mechanical system with the Hilbert space $\mathbb{C}^2$ . We denote by $\left . \left |   0  \right \rangle\right .$ and $\left . \left |   1  \right \rangle\right .$ the standard basis elements $(1,0)^T$ and $(0,1)^T$ . Let the Hamiltonian of the system in this basis be given by $$
H=\omega\begin{pmatrix}
0 &1 \\ 
1 &0 
\end{pmatrix}=\begin{pmatrix}
0 &-i \\ 
-i &0 
\end{pmatrix}
$$ and assume that for $t=0$ the state of the system is just given by $\psi(t=0)=\left . \left |   0  \right \rangle\right .$ . In the following, we also assume natural units in which $\hbar=1$ . I have found that $\psi(t)=\begin{pmatrix}
\frac{1}{2}e^{t}+\frac{1}{2}e^{-t}\\ 
-\frac{1}{2}e^{t}+\frac{1}{2}e^{-t}
\end{pmatrix}.$ And I have computed  the expectation value of $\left \langle \sigma_z \right \rangle_{\psi(t)}=\left \langle \psi(t)\mid \sigma_z\psi(t) \right \rangle$ where $
\sigma_z=\begin{pmatrix}
1 &0 \\ 
0 &-1 
\end{pmatrix}$ to 1. Now I have to determine the time-evolved observable $\sigma_z=e^{iHt}\sigma_ze^{-iHt}$ (the Heisenberg evolution of $\sigma_z$ ) and I have to determine the resulting expectation value $\langle \sigma_z(t)\rangle_{\psi_0}=\langle\psi_0|\sigma_z(t)\psi_0\rangle$ . But I'm not sure how to deal with it? Is the first one just a matrix product I have to solve? But what is $e^{iHt}$ then? I hope anyone can help me?","['analysis', 'hilbert-spaces', 'linear-algebra', 'quantum-mechanics', 'mathematical-physics']"
4364920,$\sqrt{x^2}=|x|$ or $\sqrt{x^2}=x$ in an indefinite integral,"Question: Find the following integral: $\int{\sqrt{1+\cos(x)}}dx$ My attempt: $$\int{\sqrt{1+\cos(x)}}dx$$ $$=\int{\sqrt{2\cos^{2}\frac{x}{2}}}dx$$ $$=\int{\sqrt{2}\ \left|\cos\frac{x}{2}\right|}dx$$ $$=\sqrt{2}\int{\left|\cos\frac{x}{2}\right|}dx\tag4$$ $$=\sqrt{2}\int{\pm\cos\frac{x}{2}}dx$$ $$=\pm\sqrt{2}\int{\cos\frac{x}{2}}dx$$ $$=\pm2\sqrt{2}\sin\frac{x}{2}+C$$ My book's attempt: $$\int{\sqrt{1+\cos(x)}}dx$$ $$=\int{\sqrt{2\cos^{2}\frac{x}{2}}}dx$$ $$=\int{\sqrt{2}\cos\frac{x}{2}}dx$$ $$=\sqrt{2}\int{\cos\frac{x}{2}}dx$$ $$=2\sqrt{2}\sin\frac{x}{2}+C$$ Basically, my book didn't put $\pm$ sign, while I did. My book did this essentially: $\sqrt{x^2}=x$ , while I did this: $\sqrt{x^2}=|x|$ . Is my process more correct?","['integration', 'calculus', 'trigonometry']"
4364948,Probabilty and Permutations,"A nine digit number is formed using 1 to 9 without repetition, find the probability of forming a number such that the product of any of its five  consecutive digits is divisible by 3 or 5. Now my approach for solving this problem was to first find out the cases where this would not be possible. If the product of five consecutive digits is divisible by 3 or 5, then the digits  must have atleast a multiple of 3 i.e. 3,6 or 9 or 5 as one of the 5 digits. The only case where this would not be possible would be when the special digits {3,6,9,5} are all huddled at one end of the number taking 4 of the 9 available spaces. Now we have 4!=24 ways of arranging the four special digits and 5!=120 ways of arranging the remaining digits. Since the special digits can be huddled either at the end or at the start of the number, there would be twice the numbers not satisfying the condition specified in the question. That results in 2.4!.5!=5760 rebellious numbers. Since the total possible 9 digit numbers will be 9!=362880 , that leaves us with 362880-5760=357120 favourable outcomes. Therefore the required probability should be 357120/362880=0.984 However that's not the answer, the answer specified is 0.96 Can anybody please tell me where I went wrong?
(Pardon my formatting skills :)","['permutations', 'divisibility', 'probability']"
4364958,Continuous dependence on the right-hand side of a nonlinear PDE,"Let $x \in \Omega$ ( $\Omega \subset \mathbb{R}^2$ is a bounded set with a sufficiently smooth boundary) and let us suppose that we have a PDE defined as $$\begin{cases}
\dfrac{\partial u(t,x)}{\partial t} &= A(u(t,x)) + F(u(t,x)),\\[10pt]
u(t,x)|_{\partial \Omega} &= 0,
\end{cases}$$ in which $A$ is a linear operator (which does not depend on either $t$ or $x$ so it is basically a matrix) and $F$ is a nonlinear operator with some nice properties (bounded, Lipschitz property, etc.) which describes the spatial dependence. We know that this equation has a unique solution. What are the usual techniques to show that the solution of this problem depends continuously on the right-hand side, meaning that if we consider the modified problem $$\begin{cases}
\dfrac{\partial u_{\varepsilon}(t,x)}{\partial t} & = A(u_{\varepsilon}(t,x)) + F(u_{\varepsilon}t,x)) + \varepsilon,\\[10pt]
u_{\varepsilon}(t,x)|_{\partial \Omega} &= 0,
\end{cases}$$ then $\Vert u - u_{\varepsilon} \Vert \rightarrow 0$ as $\varepsilon \rightarrow 0$ ? I have tried to use the variation of parameters formula (the operator semigroup version) but to no avail. I would be grateful even for some references since usually ""continuous dependence"" means the one considering the initial value, and not the right-hand side one.","['semigroup-of-operators', 'functional-analysis', 'partial-differential-equations']"
4364983,Compute the derivative of $E[1_{\{X\leq x\}}Y]$ w.r.t. $x$,"Let $X,Y$ be real random variable on the probability space $(\Omega,\mathcal F,P)$ , and suppose that $Y$ is $P$ -integrable and that $X$ has probability density $f$ with respect to the Lebesgue measure $\lambda$ . Am trying to show that $$\frac{d}{dx}E[1_{\{X\leq x\}}Y]=E[Y|X=x]f(x)$$ for $\lambda$ -almost every $x\in\mathbb R$ . Attempt: First lets recall the definition of $E[Y|X=x]$ . The Doob-Dynkin lemma implies the existence of a real-valued Borel measurable map $g:\mathbb R\mathbb \to \mathbb R$ such that $E[Y|\sigma(X)]=g\circ X$ $P$ -almost surely. Such map $g$ is $P_X$ -integrable and unique $P_X$ -almost surely. We let $E[Y|X=x]$ denote any version of $g$ . Now fix $x\in \mathbb R $ and compute using the properties of conditional expectation: $$E[1_{\{X\leq x\}}Y]=E[1_{\{X\leq x\}}(g\circ X)]=E[(1_{(-\infty,x]}g) \circ X]=\int 1_{(-\infty,x]}g \,dP_X=\int 1_{(-\infty,x]}g f \, d\lambda$$ Moreover the same calculation without $1_{\{X\leq x\}}$ shows that $g f$ is $\lambda$ -integrable. Hence we can invoke the Lebesgue differentiation theorem to obtain the result. Am I missing something? Thanks a lot for your help.","['lebesgue-measure', 'lebesgue-integral', 'conditional-expectation', 'probability-theory', 'probability']"
4365010,"Find linear functional $\ell$ s.t. $\frac{1}{2n}\int_{-n}^{n}g(x)\, dx \to \ell(g)$","(Update below) I'm studying for my Functional Analysis final, and I found an interesting exercise in one of the old exams. There is one part where I am unsure, maybe someone could point me in the right direction. Consider the Banach space $V:= L^\infty(\Bbb{R})$ with the
supremum norm. For each $n \in \Bbb{N}$ , define $$ l_n(g) = \frac{1}{2n}\int_{-n}^ng(x)\, dx, \,\,\, \forall g\in V. $$ Prove that there exists $l \in V^*$ and a subsequence $(n_k)_{k \in \Bbb{N}}$ such that $(l_{n_k})$ converges weak- $*$ to $l$ as $k \to \infty$ . If I understood the convergence in the weak- $*$ sense correctly, I need to find a linear functional $l \in V^*$ such that $l_n(g) \to l(g)$ for all $g \in V$ (or rather, we have convergence along a subsequence). My initial thought was just defining $l(g) = \lim_{n\to \infty} l_n(g)$ , but I see no reason why this limit should exist for all $g$ . I then proceeded to define $l$ as above on the subspace of $V$ given by $\{g \in V \mid lim_{x\to \infty}g(x)\text{ and }\lim_{x \to -\infty}g(x) \text{ exist.}\}$ , where $l_n(g)$ should converge. Using Hahn-Banach, I can then extend this functional to all of $V$ . Now, since $l_n(g)$ is a bounded sequence, by Bolzano-Weierstrass we can extract a converging subsequence, which should give exactly the subsequence $(l_{n_k})_{k\in \Bbb{N}}$ we are looking for. Am I on the right track here? I'm particularly unsure about the characterization of weak- $*$ convergence, which is a subject I am still struggling a bit. Any help is greatly appreciated. Update As mentioned by @Ofek Arian in his answer, my argument does not work, because just finding for all $g \in V$ a subsequence s.t. $l_{n_k}(g)$ converges is not enough to conclude there is a subsequence s.t. $(l_{n_k})$ converges for all $g \in V$ . Based on @David Mitra's comment, the statement that the exercise wants me to prove might actually not even be true. I will try to flesh out his argument here, I hope I understood correctly. If we look at $l_n$ as an element of $L^1$ , say $l_n(x) = \frac{1}{2n}\chi_{[-n,n]}(x)$ , then the weak- $*$ convergence in the exercise is the same as weak convergence of $l_n$ in $L^1$ . We have that $l_n(x) \to 0$ pointwise and if a sequence of elements in $L^1$ converges pointwise (a.e.), as well as weakly in $L^1$ , then the two limits must agree. (See e.g., Pointwise a.e. convergence and weak convergence in Lp ). So the weak limit along any subsequence would need to be the zero function. But for $g \equiv 1$ , the constant $1$ function, we have $l_n(g) = 1 \, \forall n \in \Bbb{N}$ , showing that $l_n$ can't possibly converge weakly to the zero function along any subsequence. Since the first part of the exercise was 'state the Banach-Alaoglu Theorem', I am led to believe that the intention might have been what @Ofek Arian mentioned in his comment, namely to invoke Banach-Alaoglu to deduce that the closed unit ball of $V^*$ is weak- $*$ compact, and use this fact to extract a converging subsequence. But compactness and sequential compactness need not agree in the weak- $*$ topology, and it is in fact different in the case of $(L^\infty)^*$ . It's possible that this is just a mistake in the exercise... Update 2 As @SeverinSchraven noticed, one needs to be careful when arguing that such subsequence cannot exist. It is a priori not clear, that the weak- $*$ limit of $l_n$ can be represented by a function in $L^1$ , so the linked result cannot be directly applied. Nonetheless, this can be fixed, as @DavidMitra argued: If we had weak- $*$ convergence in $(L^\infty)^*$ , we would get a weak Cauchy sequence (in $L^1$ ), and in $L^1$ , weak Cauchy implies weakly convergent. See e.g., Weak limit of an $L^1$ sequence . Thus, we may actually apply the linked result as done above to argue that no such subsequence can exist. (I hope I have written down your argument faithfully).","['functional-analysis', 'weak-convergence']"
4365024,"$e^{|\sin x|}+e^{-|\sin x|}+4a=0$ will have exactly four different solution in $[0,2\pi]$","$e^{|\sin x|}+e^{-|\sin x|}+4a=0$ will have exactly four different solution in $[0,2\pi]$ if (A) $a\in\bigg[\frac{-e}{4},\frac{-1}{4}\bigg]$ (B) $a\in\bigg[\frac{-1-e^2}{4e},\infty\bigg)$ (C) $a\in \mathbb R$ (D) None of these My Approach: Let $e^{|\sin x|}=t \in[1,e]$ So equation transformed into $t^2+4at+1=0$ Above equation must have two distinct solution in $[1,e]$ For two distinct solution we must have $f(1)\geq0$ , $f(e) \geq0$ , $4a^2-4 \gt0$ and $1\lt -2a \lt e$ where $f(t) = t^2+4at+1$ Is my Approach Correct?","['trigonometry', 'quadratics', 'roots']"
4365117,Solving a second order differential equation to obtain trajectories,"I don't know if this is appropriate to ask so please let me know if I should keep this post or not. While calculating some accelerated trajectories, i encountered a rather tedious differential equation of the form, $$  \frac{da^0}{d\tau} + \frac{\Big(\mu - \frac{GQ^2}{r}  \Big) }{GQ^2 + r^2 - 2r\mu} (u^0 a^1 + u^1 a^0) = \vert a \vert^2 u^0$$ where, $u^i$ is a 4-vector of velocity (dependent on $\tau$ ), $G$ , $Q$ can be treated as constants along with $\mu$ and $r$ is a variable. Here, $a^i$ is the acceleration 4-vector and the expression for which is, $$a^0 = \frac{du^0}{d\tau} + \frac{2\Big(\mu - \frac{GQ^2}{r}  \Big) }{GQ^2 + r^2 - 2r\mu} (u^1 u^0)\\
a^1 = \frac{du^1}{d\tau} - \frac {(Mr-Q^{2})}{r^{2}-2Mr+Q^{2}}(u^0)^2 + \frac {Q^{2}-Mr}{Q^{2}r-2Mr^{2}+r^{3}} (u^1)^2$$ So, after combining these equations (where $\vert a \vert^2$ is also a constant ) and substituting the later expressions into the former equation,we get a long differential equation like, $$\frac{d}{d\tau} \Big(\frac{du^0}{d\tau} + \frac{2\Big(\mu - \frac{GQ^2}{r}  \Big) }{GQ^2 + r^2 - 2r\mu} (u^1 u^0)\Big) +  \frac{\Big(\mu - \frac{GQ^2}{r}  \Big) }{GQ^2 + r^2 - 2r\mu} \Bigg(u^0 \Big( \frac{du^1}{d\tau} - \frac {(Mr-Q^{2})}{r^{2}-2Mr+Q^{2}}(u^0)^2 + \frac {Q^{2}-Mr}{Q^{2}r-2Mr^{2}+r^{3}} (u^1)^2\Big) + u^1 \Big(\frac{du^0}{d\tau} + \frac{2\Big(\mu - \frac{GQ^2}{r}  \Big) }{GQ^2 + r^2 - 2r\mu} (u^1 u^0)\Big)\Bigg) = \vert a \vert^2 u^0 $$ which I am not getting anywhere with. Any help/clue would be greatly appreciated. Thanks! EDIT: I think the additional information about the $a^1$ equation could be needed. $$  \frac{da^1}{d\tau} =  \vert a \vert^2 u^1$$","['physics', 'ordinary-differential-equations']"
4365141,$(1-x^2)y''-xy'+\lambda y=0$ -Sturm–Liouville,"I try to get this equation in Sturm–Liouville and I get stuck. $(1-x^2)y''-xy'+\lambda y=0$ My solution: $\mu(1-x^2)y''-\mu xy'+\mu \lambda y=(py')'+(\lambda r-q)y$ $\mu(1-x^2)y''-\mu xy'+\mu \lambda y=p'y'+py''+(\lambda r-q)y$ $p'=\mu x, p=\mu(1-x^2) \implies \frac{p'}{p}=\frac{x}{1-x^2}\implies ln(p)=-\frac{1}{2}ln|1-x^2|\implies p=\frac{1}{\sqrt{(|1-x^2|)}}$ Then $\mu=\frac{1}{(1-x^2)^3}$ $\frac{1}{(1-x^2)^2}y''-\frac{1}{(1-x^2)^3}xy'+\frac{1}{(1-x^2)^3}\lambda y=p'y'+py''+(\lambda r-q)y$ Here I get stuck ,please help Thanks !",['ordinary-differential-equations']
4365150,Hoeffding's inequality for conditional probability,"I am currently reading the paper Functional Classification in Hilbert Spaces by Biau, Bunea and Wegkamp, and there is one step in the proof of Theorem 1 that is not clear to me. I give below a simplified version of the steps of the proof that I do not understand, but the original calculations can be found in Section V-B, page 7 of the original paper. To briefly introduce the notations, we are dealing with a binary classification problem. The dataset is made of $n$ i.i.d. pairs $(X_i,Y_i) \in \mathcal X \times \left\{0,1\right\}$ and we denote by $\hat\phi_n$ the classifier which belongs to a certain hypothesis class and is a function of the data . Lastly, $L$ and $\hat L$ are respectively the true and empirical losses of the classifier $\hat\phi_n$ , i.e. $$L:= \mathbb P\left(\hat\phi_n(X)\ne Y\ |\ (X_i,Y_i),1\le i\le n\right) \quad \text{ and }\quad \hat L :=\frac{1}{n}\sum_{i=1}^n\mathbf 1\left\{\hat\phi_n(X_i)\ne Y_i\right\} $$ The inequality I'm having trouble with is the following : $$\begin{align}
\mathbb P\left(L-\hat L\ge \varepsilon\right) &= \mathbb E \mathbb P\left(L-\hat L\ge \varepsilon\ |\ (X_i,Y_i),1\le i\le n\right) \\
&\le \exp\left(-2n\varepsilon^2\right)
\end{align}$$ The first line is clearly true by the law of total expectation, and I understand that the second line is a direct application of Hoeffding's inequality since, conditional on the data, $n\hat L$ is a sum of i.i.d Bernoulli variables of parameter $L$ . My issue is that I don't know how to formalize Hoeffding's inequality in this case where the probability is conditional on a sigma algebra , since it then becomes a random variable. Should I maybe show that Hoeffding's inequality holds for the conditional probability conditional on any event in $\sigma\left((X_i,Y_i),1\le i \le n\right) $ ? In summary, my question is the following : How to formally derive Hoeffding's inequality for conditional probability with respect to a sigma algebra ? Many thanks in advance for the help. Update : First off, I noticed I got confused with the notations and $\hat\phi_n$ is, in fact, dependent on the data (in the paper at least), I fixed the mistake. Regarding my question, I attempted to prove Hoeffding's inequality for conditional probability by following the proof of the original inequality and adapting it. So let $X_i$ be independent r.v.'s respectively bounded in $[a_i,b_i]$ , $S_n = \sum_{i=1}^n X_i$ and $\mathcal F$ be a sigma-algebra : By conditional Markov inequality , we have $$\begin{align} \mathbb P\left(S_n-\mathbb E S_n \ge \varepsilon \ \vert\ \mathcal F\right)&\le e^{-s\varepsilon}\mathbb E \left[e^{s(S_n-\mathbb E S_n)} \ \vert\ \mathcal F\right]\\
&=e^{-s\varepsilon}\mathbb E \left[\exp\left(s\left(\sum_{i=1}^n X_i - \mathbb E X_i\right)\right) \ \vert\ \mathcal F\right] \\
&= e^{-s\varepsilon} \prod_{i=1}^n\mathbb E \left[e^{s\left( X_i - \mathbb E X_i\right)} \ \vert\ \mathcal F\right] \text{ (by independence)}\\
\end{align}$$ To conclude, I then have to prove/apply a conditional version of Hoeffding's lemma , but sadly that doesn't seem to work since $\mathbb E\left[X_i - \mathbb E[X_i]\ \vert \ \mathcal F\right]\ne 0$ in general. I however believe it's still possible to upper bound this conditional expectation and get a similar, but weaker version of the lemma for conditional probability, but that would yield a different result (I haven't done the calculation yet). Am I on the right track ?","['conditional-expectation', 'statistics', 'probability-theory', 'inequality']"
4365160,How do I determine further solutions of the equation using Rolle's theorem?,"I gave this equation $2^x=1+x^2$ with the $1$ st zero is $x_1=0$ and the $2$ nd zero is $x_2=1$ . (easy reading) Now I want to calculate more zeros using Rolle's theorem, and I've rearranged the function for this: $$f(x)=1+x^2-2^x$$ and formed the first two derivatives: $f'(x)=2x-\ln(2)*2^x$ and $f''(x)=2-\ln(2)^2*2^x$ So and from here it fails now. I know what Rolle's theorem says. Between two zeros of the function there is a zero of the derivative, and if $f$ is twice differentiable, then between three zeros of the function there are two zeros of the first derivative and one zero of the second derivative. Also, I saw that $f(4) > 0$ and $f(5) < 0$ . Therefore, in the interval (4,5) there is at least one more zero of $f$ according to the intermediate value theorem. But how do I determine furthermore precise real solutions of the equation?","['rolles-theorem', 'derivatives', 'analysis', 'roots']"
4365245,"If $T$ is compact, then $\|T e_n \| \to 0$ for any othonormal sequence $e_n$.","The following question is a part of question 2.4.6 of Conway's functional analysis: Let $\mathcal{H}$ and $\mathcal{K}$ be Hilbert spaces, and let $T: \mathcal{H} \rightarrow \mathcal{K}$ be a bounded linear operator. Show that if $T$ is a compact operator, then $$
\lim _{n \rightarrow \infty}\left\|T\left(e_{n}\right)\right\| =0.
$$ This question been extensively answered on the stack exchange (for example here) : A Question on Compact Operators . One thing that is unappealing about the answers that I have seen is that they use  what seem like ""big-gun"" results about weakly convergent sequences and compact operators, which a reader of Conway's book up to this point would not have seen yet! I've devised an alternate proof that uses only basic principles below. Is it correct? If so, could any light be shed as to how it relates to the weak-convergence argument? Proof: $T$ compact implies that $\sup _{\|x\| = 1}\left\|T{x}\right\| \leq M<\infty$ for some $M$ , and that $Te_n$ has a convergent subsequence $ T e_{n_k} $ . Suppose $\left\|T e_{n_k}-h\right\| \rightarrow 0$ , but $h \neq 0$ . By restricting to a further subsequence if necessary, we may assume $\left\|T_{ {e_{n_k} }}-h\right\| \leq 2^{-k}$ . Define $$
x_{m}=\sum_{j=1}^{m} \frac{c}{j} e_{n_{j}} \text {, where } \frac{1}{c}=\left(\sum_{j=1}^{\infty} \frac{1}{j^{2}}\right)^{1/2} \text {. }
$$ clearly $\left\|x_{m}\right\| \leq 1$ For all $m$ . But $$ \begin{aligned} M & \geqslant \| T_{x_m}\|=\| \sum_{j=1}^{m} \frac{c}{j} (T_{e n j}-h+h) \| \\ &=\left\|\sum_{j=1}^{m} \frac{c}{j}\left(T_{e_{n_j}}-h\right)+h \sum_{j=1}^{m} \frac{c}{j}\right\| =:\left\|A_{m}+B_{m}\right\| \end{aligned} $$ since $\|h\| \neq 0, \quad\left\|B_{m}\right\| \rightarrow \infty$ as $m \rightarrow \infty$ . By the triangle inequality $\sup_m\left\|A_{m}\right\| \leqslant \sum_{j=1}^{\infty} \frac{c}{j} 2^{-j}<\infty$ .
This implies $\left\|A_{m}+B_{m}\right\| \rightarrow \infty$ as $m \rightarrow \infty$ , a contradiction. Hence $h=0$ , and every subsequence of $Te_n$ must have a further subsequence converging to 0, giving $$
\|  Te_n  \| \rightarrow 0 \quad \text { as } n \rightarrow \infty \text {. }
$$","['operator-theory', 'solution-verification', 'functional-analysis']"
4365261,"In a certain sense, can ""Piecewise Linear"" be Interpreted as ""Non-Linear""?","I was looking at the following function (called ""ReLU"") : I am trying to understand why this function (""ReLU"") is considered to be non-linear , when it appears to look ""piecewise linear"" (and even contains the term ""linear"" in its name): Can someone please explain why the ""ReLU"" function is described as non-linear, when it seems to be linear in appearance? Is it possible that the individual ""pieces"" of the ReLU function are linear, but the entire function itself is somehow non-linear? When functions are defined in ""pieces"" - can we still determine if the entire function is ""convex"" or ""non-convex"" - or are we forced to only label the individual pieces of the function as convex and non-convex? Thanks! References: https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html","['machine-learning', 'convex-analysis', 'functions', 'derivatives']"
4365263,Zero locus of a general section,"I just need one precision about the definition of zero locus of a general section of a vector bundle. I know that if the $s$ is a section of a vector bundle $E$ on a scheme $X$ , the zero locus of $s$ is the set $Z(s)$ which ideal sheaf is the image of the morphism $\mathcal E^{\vee}\rightarrow \mathcal O_X$ , where $\mathcal E$ is the corresponding locally free sheaf to $E$ . So, I think that the zero locus of a general section is the zero locus of a section in a Zariski open subset in the space of global sections of $E$ . Right?
Thanks for your answers in advance.","['locus', 'vector-bundles', 'algebraic-geometry']"
4365279,Formula for $a\sin(\theta)+b\cos(\theta)$ confusion,"I've been doing some trig lately and in a reference sheet I was given the following formula that I tried to prove $$
a\sin(\theta)+b\cos(\theta)=\frac{a}{|a|}\sqrt{a^2+b^2}\sin(\theta+\alpha)
$$ with $\alpha=\arctan\left(\frac{b}{a}\right)$ and $a\neq 0$ . From the standard approach with the sum formula, I was able to get $\sqrt{a^2+b^2}\sin(\theta+\alpha)$ by setting $$A\sin(\theta+\alpha)\equiv A\sin(\theta)\cos(\alpha)+A\cos(\theta)\sin(\alpha)=a\sin(\theta)+b\cos(\theta)$$ and solving $$A\cos(\alpha)=a\\A\sin(\alpha)=b$$ which gave $A=\sqrt{a^2+b^2}$ and $\alpha=\arctan\left(\frac{b}{a}\right)$ , but I'm unsure where the $\frac{|a|}{a}$ terms comes from nor was I able to determine the reason why the expression depends on the sign of $a$ anywhere during the solution.",['trigonometry']
4365284,"Show that $(0,1)$ is uncountable if and only if $\mathbb{R}$ is uncountable.","I am trying to write a proof for this problem, and have been having difficulty completing the other direction of the iff statement. Below is what I have. ( $\Rightarrow$ ) Let $(0,1)$ be uncountable. Assume towards a contradiction that $\mathbb{R}$ is countable. Any subset of a countable set is also a countable set.  We know that because $(0,1) \subset \mathbb{R}$ , $(0,1)$ must be countable. However, this contradicts our claim that $(0,1)$ is uncountable. Therefore, in order for $(0,1)$ to be a subset of $\mathbb{R}$ , $\mathbb{R}$ must be uncountable. This contradicts our original assumption that $\mathbb{R}$ is countable. ( $\Leftarrow$ ) Let $0.d_{1}d_{2}d_{3}...d_{k}...$ denote the decimal
representation of a number in $(0,1)$ . Assume towards a contradiction
that $(0,1)$ is countable. This means that there exists a function $f$ such that $f: \mathbb{N} \to (0,1)$ that is one-to-one and onto. For each $m
> \in \mathbb{N}$ , $f(m)$ is a real number between 0 and 1. We represent this as $$0.d_{m1}d_{m2}d_{m3}...d_{mn}$$ This means that for each $m,n \in \mathbb{N}$ $d_{mn}$ is the digit from the
set $\{0,1,2,...,9\}$ that represents the $n$ th digit in the decimal
expansion of $f(m)$ . The key assumption about this one-to-one
correspondence is that every real number in (0,1) is assumed to appear
somewhere on the list. Define a real number $x \in (0,1)$ with the decimal expansion $x =
> 0.b_{1}b_{2}b_{3}b_{4}...$ using the rule $$b_{n}= \begin{cases} 
       2 & \text{if}\ d_{nn} \neq 2\\
       3 & \text{if}\ d_{nn} =2. 
         \end{cases}$$ There are other similar posts (such as this post from 2016 and this post from 2017 ), but they have not been helpful to me. I am looking for guidance regarding my specific proof about what can be improved/changed, and perhaps a hint to guide me in the right direction so that I may complete the other direction of the proof. Thank you for your help!","['elementary-set-theory', 'proof-writing', 'solution-verification', 'real-analysis']"
4365319,A sum that's possibly equal to the Euler-Mascheroni Constant $\sum_{n=1}^\infty \frac{\ln n!}{n^3}$,"The following interesting sum seems to approach the Euler-Mascheroni constant $\gamma$ . $$\sum_{n=1}^\infty \frac{\ln n!}{n^3} \overset{?}{=} \gamma$$ I've looked at the different ways to express the Euler-Mascheroni constant and tried to apply those methodes to this sum. I also tried using the series representation of $\displaystyle \frac{\ln n!}{n^3}$ for $\displaystyle n=0,1,2$ and using the log gamma function $$\displaystyle\ln \Gamma(n+1)=-\gamma-\gamma n-\ln (n+1)+\sum\limits_{k=1}^\infty\frac{n+1}{k}-\ln\left(1+\frac{n+1}{k}\right)$$ This problem might be quite well out of my mathmatical reach but I still would love to know the answer. It would be awesome if anyone could prove or disprove that the sum equals the Euler-Mascheroni constant and show their method.","['gamma-function', 'euler-mascheroni-constant', 'factorial', 'sequences-and-series']"
4365321,Finding integer solutions to $(x-y)^2+(y-z)^2+(z-x)^2=2022$,"This was from round A of the Awesomemath Summer Program application. The deadline was yesterday and we can discuss the problems now. Find all integer triples $(x,y,z)$ which satisfy $$(x-y)^2+(y-z)^2+(z-x)^2=2022$$ I tried a couple of things. First I noticed $x-y+y-z+z-x=0,$ so if we let $a=x-y, b=y-z,c=z-x$ than we have $$\begin{align}
a^2+b^2+c^2&=2022 \tag1\\
a+b+c &=0 \tag2\\ 
(a+b+c)^2-2ab-2bc-2ca &=2022 \tag3
\end{align}$$ So, $$-2ab-2bc-2ca=2022 \tag4$$ so $$ab+bc+ca=-1011 \tag5$$ However, it is tricky to proceed from here. I also recalled the factorization $$x^2+y^2+z^2-xy-yz-zx=\frac{1}{2}((x-y)^2+(y-z)^2+(z-x)^2)=1011 \tag6$$ This also equals $$x(x-y)+y(y-z)+z(z-x) \tag7$$ but this doesn't seem promising. I also tried to factorize $2022=2 \cdot 3 \cdot 337$ , but I am not sure what to do after prime factorizing. So far, I feel like my first approach is the most promising, but I am not sure how to finish with it.","['integers', 'roots', 'diophantine-equations', 'factoring', 'algebra-precalculus']"
4365346,Can two distinct elementary functions be equal over an interval of nonzero width?,"The wikipedia entry on elementary functions describes them to be ""of a single variable (typically real or complex) that [are] defined as taking sums, products, and compositions of finitely many polynomial, rational, trigonometric, hyperbolic, and exponential functions, including possibly their inverse functions"". Piecewise functions do not fit this description, and I believe these functions are continuous for the regions for which they are defined, as well as their derivatives for the regions along which those derivatives are defined. Taking two functions which are composed of elementary functions and are unequal for the majority of the interval along which they can be defined for the independent variable, can these functions be equal (meaning they contain all the same points) over an interval with nonzero width, such as being defined and equal for (2,3) or (0, inf)? If this is impossible, why is it impossible? Thanks!",['functions']
4365387,Taxicab metric *with stoplights*; does it ever give the Euclidean metric?,"This question is not terribly formal in nature, but please bear with me: what I’m looking for is a model of traveling via taxi on a grid of streets where the “metric” in some sense becomes the standard Euclidean metric in a limit, for non-trivial reasons. I’ll elaborate below: Recall the taxicab metric in the plane, where $d(x, y) = |x_1 - x_2| + |y_2 - y_2|$ . I'm interested in looking at restricting this to $\mathbb{Z}^2$ , and using that as a basis for a model in which we imagine taxis traveling on actual streets, where we then add complexity to the model (in ways that plausibly model car traffic, though it need not be actually realistic—extreme idealization is okay). For example, one could imagine that on each lattice point, for each direction there is a $50\%$ chance it is blocked by a red light for $1$ unit of time; perhaps we also let traveling cost $1$ unit of time between lattice points when not stopped by a light. In this model, the expected time to get to two points depends on more than just the taxicab metric; you’d rather be $10$ units north and $10$ units west of your destination than $20$ units north, since in the former case when one of your ways is stopped, you have the choice of making progress in the other direction rather than having to stop and wait. (In fact, even if both ways are open, you want to move in the way that keeps how far you have to move south vs east as “balanced” as possible—I’ve made choices in real life based on this). As a result of this, the “balls” around where you start are no longer diamond-shaped as in the taxicab metric (of course, we don't really have a metric, but a probabilistic model, where one can think in terms of expected times, and take approximate level sets of this and see the limiting shape: I’m intentionally informal here as I’m open to answers that take this in different directions). In fact, if I recall, I believe this forms an octagon, in some sense! What I would like is model in the spirit of the above, devised so that the “balls” in some expected sense form a Euclidean circle. That is, for larger $r$ , the expected time to move approximately $r$ in any given direction (not just north and south; just look at lattice points near $r$ in some direction) is the same as $r$ goes to infinity. One reason one might be hopeful such a model exists is because there are examples of things defined on a grid where in the limit, Euclidean symmetry is recovered (see Brownian motion defined as a limit of random walks on a grid as an example of this). I’m open to various creative things leading to this behavior, such as other cars, weird stoplight systems, etc, though ideally there is some degree of elegance to the model and it doesn’t immediately feel cooked up to give a Euclidean result.","['euclidean-geometry', 'mathematical-modeling', 'random-walk', 'metric-spaces', 'probability']"
4365463,Using residue theorem to calculate $\int_0^\infty \frac{dx}{x^\frac12 (1+x^2)}$ get a different value from the value obtained by classical method,"I'm trying to calculate the next integral by using residue theorem: $$
\int_0^\infty \frac{dx}{x^\frac12 (1+x^2)}.
$$ Let $f(z)=1/(z^\frac12(1+z^2))$ and $\varepsilon,R$ be any real number such that $0 < \varepsilon < 1 < R$ . Choose the contour \begin{align}
C_1&:z(t)=Re^{it}\quad(\theta \le t \le 2\pi-\theta)\\
C_2&:z(t)=t-i\varepsilon\quad(0\le t\le R\cos\theta) \\
C_3&:z(t)=\varepsilon e^{it}\quad(\frac\pi2\le t\le \frac{3\pi}2) \\
C_4&:z(t)=t+i\varepsilon\quad(0\le t\le R\cos\theta)
\end{align} where $\theta= \arcsin\frac{\varepsilon}{R}$ . By residue theorem $$
\oint_{C_1-C_2-C_3+C_4}f(z)dz = 2\pi i\left( \operatorname{Res}(f,i) + \operatorname{Res}(f,-i)\right).
$$ Here \begin{align}
\operatorname{Res}(f,i) &= \lim_{z\to i}\frac{z-i}{z^\frac12 (1+z^2)} = \lim_{z\to i}\frac1{z^\frac12(z+i)} = \frac{1}{2i\cdot i^\frac12} = \frac{-1-i}{2\sqrt 2}, \\
\operatorname{Res}(f,-i)&= \lim_{z\to -i}\frac{z+i}{z^\frac12 (1+z^2)} = \lim_{z\to i}\frac1{z^\frac12(z-i)} = \frac1{-2i\cdot(-i)^\frac12} = \frac{-1+i}{2\sqrt2}.
\end{align} Thus $$
\oint_{C_1-C_2-C_3+C_4}f(z)dz = -\frac{2\pi i}{\sqrt2}.
$$ It is easy to check $\int_{C_1}f(z)dz,\int_{C_3}f(z)dz\to 0\ (\varepsilon\to0,R\to\infty)$ . And since $\exp(-\frac12(\ln|z|+i(\arg z+2\pi))) = -1/\sqrt z$ it obtains $$
\lim_{\substack{\varepsilon\to0\\R\to\infty}}\int_{C_4}f(z)dz = -\lim_{\substack{\varepsilon\to0\\R\to\infty}}\int_{C_2}f(z)dz = \int_0^\infty \frac{dx}{x^\frac12 (1+x^2)}.
$$ Therefore $$
\int_0^\infty \frac{dx}{x^\frac12 (1+x^2)} = -\frac{\pi i}{\sqrt2}.
$$ It is very curious that the integration of a real positive-valued function become a imaginary number. In fact, by using classical method, the integration will be $\pi/\sqrt2$ , which seems to be the true answer. Where is the mistake?","['complex-analysis', 'contour-integration', 'residue-calculus']"
4365477,Maximal cyclic subgroups,"Can we classify the groups for which every maximal cyclic subgroup is of same order and intersection of any two maximal cyclic subgroups is identity? For example in case of abelian groups $$G=\mathbb{Z}_p\times \mathbb{Z}_p \times \cdots \times \mathbb{Z}_p$$ Can we do same kind of classification for non abelian groups? Is there any example or some contradictions such that no such non abelian group exists which satisfy both these properties.
Any suggestion will be helpful.","['maximal-subgroup', 'group-theory']"
4365489,Showing a function is onto,"Define $f:[0,\infty)\rightarrow \mathbb R$ by $f(x)=x^2\sin x$ . Show that $f$ is onto. My attempt:
For $n\in \mathbb N$ , $f((4n+1)\frac{\pi}{2})=(4n+1)^2$ And. $f((4n+3)\frac{\pi}{2})=-(4n+3)^2$ . Now for any $y\in\mathbb R$ we can find $n\in\mathbb N$ such that $y\in(-(4n+3)^2,(4n+1)^2)$ . Then by intermediate value theorem there is some $x$ such that $f(x)=y$ . Hence $f$ is onto. Is this correct?","['continuity', 'functions', 'real-analysis']"
4365522,Why $\frac{d}{dx}(f(g)) = \frac{d(g^T)}{dx}\cdot\frac{\partial f}{\partial g}$ for $f:\mathbb{R}^n\to\mathbb{R}$ and $g:\mathbb{R}^m\to\mathbb{R}^n$?,"Reading about differentiation of matrices seems to state that, given $f:\mathbb{R}^n\rightarrow \mathbb{R}$ and $g:\mathbb{R}^m\rightarrow \mathbb{R}^n$ $$\frac{d}{dx}(f(g)) = \frac{d(g^T)}{dx} \cdot \frac{\partial f}{\partial g}$$ (I think I might be wrong) Why is the transposition needed? Moreover, does this mean that: $\nabla f = \frac{d(f^T)}{dx} = (\frac{d(f)}{dx})^T$ ?","['matrices', 'multivariable-calculus', 'linear-algebra', 'linear-transformations', 'partial-derivative']"
4365523,A Combinatorial Problem from HMMT-2009,"I've been stuck on a problem from HMMT-2009. I understand the main part of the problem and the answer. My problem with the solution is the last part of it: Given a rearrangement of the numbers from 1 to n, each pair of consecutive elements a and b of the sequence can be either increasing or decreasing. How many rearrangements have of the numbers from 1 to n have exactly two increasing pairs? My answer:
Well, obviously we can split these numbers into three partitions and subtract off the undesirable rearrangements. To do so, the number of total counts is $3^n$ Now, we don't wanna count those rearrangements that have 1 or 0 increasing pairs. The partitions whose associated permutation has exactly one increasing pair can be counted by splitting the numbers into two subsets with 1 increasing pair first, which gives us $2^n-(n+1)$ , and after that we split these rearrangements into another subset, and we'll have three subsets. There are $n+1$ ways to do that. So far we have $3^n-(n+1)(2^n-(n+1))$ according to the multiplication principle.
Now, my problem begins when we start trying to find the number of rearrangements with no increasing pair. The solution says There are $\frac{n+2}{2}$ ways of placing two barriers between these elements to split the numbers into three subsets, and so there are $\frac{n+2}{2}$ such partitions of {1,2,..,n} into three subsets. Thus, the answer is $$
3^n-(n+1)(2^n-(n+1))-{n+2 \choose 2}=3^n-(n+1)2^n+\frac{n(n+1)}{2}
$$ How did we get that last part ${n+2 \choose 2}$","['permutations', 'combinations', 'combinatorics', 'discrete-mathematics']"
4365592,What is the probability one man is chosen for treasurer and one woman for secretary?,"So everyone, I have a simple probability question that I cannot wrap my head around. It go like this: There are $6$ men $9$ women on committee. $2$ people are chosen for treasurer and secretary. What is the probability one man is chosen for treasurer and one woman for secretary? My solution goes like this: $6$ men/ $15$ people $\cdot$ $9$ women/ $14$ people = $9/35$ but I was thinking, if we think that the denominator will be $15$ choose $2 = 105$ , then we get different answer of $6 \cdot 9/105 = 18/35$ ??? Now I'm confused on which one is the correct answer. Someone help clear the confusion for me.  Thanks.",['probability']
4365606,Let $h:S\rightarrow \mathbb{R}$ be a differentiable function. Show there exists a unique tangent field $F$ with $D_qh(v)=F\cdot v$.,"The following is an exercise from a set of notes on Differential Geometry asking us to prove a result. I have a few questions about the proof. Theorem: Let $S$ be a regular surface and let $h:S\rightarrow \mathbb{R}$ be a differentiable function. Show there exists a unique tangent field $F:S\rightarrow \mathbb{R}^3$ (that is, a unique differentiable function $F$ tangent to the surface at every point) such that $D_qh(v)=F\cdot v$ (where $\cdot$ denotes the dot producut on $\mathbb{R}^3$ ). Proof: Since the restriction of the dot product to $T_qS$ (the tangent plane to $S$ at $q$ ) is non-degenerate, there exists a unique vector $F(q)\in T_qS$ such that $D_qh(v)=F(q)\cdot v$ for every $v\in T_qS$ . $ \ \ \ $ Q1 . Why is there such a unique vector? To show that $F$ is a tangent field, it remains to be shown that $F$ is differentiable. Let $\phi : \Omega \rightarrow \mathbb{R}^3$ be a parametrization of $S$ .  We'll show the function $\textbf{x}:=F\circ \phi$ is differentiable. Since the partial derivatives $\phi _u, \phi _v$ define a basis for each tangent plane to the surface $S$ , there exist functions $\lambda, \mu:\Omega \rightarrow \mathbb{R}$ such that $\textbf{x}=\lambda \phi_u+\mu\phi_v$ . Therefore to show that $F$ is differentiable it is sufficient to show that (for every parametrization $\phi:\Omega \rightarrow \mathbb{R}^3)$ the functions $\lambda, \mu$ are differentiable. $ \ \ \ $ Q2 . Why does the differentiability of $\lambda$ and $\mu$ imply the differentiability of $F$ ? Let $E, F, G$ be the coefficients of the first fundamental form of $\phi$ . We have that $\textbf{x}\cdot \phi_u=\lambda E+\mu F$ and $\textbf{x}\cdot \phi_v = \lambda F + \mu G$ . On the other hand, we have that $\textbf{x}\cdot \phi _u = (h\circ \phi)_u$ and $\textbf{x}\cdot \phi _v = (h\circ \phi)_v$ . $ \ \ \ $ Q3 . Where do these last two equalities come from? Since $h:S\rightarrow \mathbb{R}$ is differentiable, both functions $(h\circ \phi)_u$ , $(h \circ \phi)_v$ are differentiable. Thus we have $$
\begin{pmatrix}
\lambda \\
\mu
\end{pmatrix}
=
\frac{1}{\Delta ^2}
\begin{pmatrix}
G & -F \\
-F & E
\end{pmatrix}
\begin{pmatrix}
\lambda E + \mu F \\
\lambda F + \mu G
\end{pmatrix}
= 
\frac{1}{\Delta ^2}
\begin{pmatrix}
G & -F \\
-F & E
\end{pmatrix}
\begin{pmatrix}
(h\circ \phi)_u \\
(h\circ \phi)_v
\end{pmatrix}
$$ where $\Delta ^2 = EG-F^2$ . Therefore both functions $\lambda, \mu$ are differentiable.","['proof-explanation', 'multivariable-calculus', 'geometry', 'differential-geometry']"
4365625,How to solve this solvable 8th-degree algebraic equation by radicals?,"Solve the following equation in radicals. $$x^8-8x^7+8x^6+40x^5-14x^4-232x^3+488x^2-568x+1=0$$ I use Magma to verify that its Galois group is a solvable group. R := RationalField(); 
R < x > := PolynomialRing(R); 
f := x^8-8*x^7+8*x^6+40*x^5-14*x^4-232*x^3+488*x^2-568*x+1; 
G := GaloisGroup(f); 
print G;
GroupName(G: TeX:=true);
IsSolvable(G); The output of Magma(Online) is: Permutation group G acting on a set of cardinality 8
Order = 16 = 2^4
    (2, 4)(6, 8)
    (1, 2, 3, 4)(5, 6, 7, 8)
    (1, 5)(2, 8)(3, 7)(4, 6)
C_2\times D_4
true I also tried to calculate with PARI/GP(64-bit)v_2.13.3+GAP(64-bit)v_4.11.1, but failed. gap> LoadPackage(""radiroot"");
true
gap> x := Indeterminate(Rationals,""x"");;
gap> g := UnivariatePolynomial( Rationals, [1,-8,8,40,-14,-232,488,-568,1]);
x^8-8x^7+8x^6+40x^5-14x^4-232x^3+488x^2-568x+1
gap>  RootsOfPolynomialAsRadicals(g, ""latex"");
""/tmp/tmp.sfoZ6C/Nst.tex""
Error，AL_EXECUTABLE，the executable for PARI/GP，has to be set at /proc/
cygdrive/C/gap-4.11.1/pkg/aInuth-3.1.2/gap/kantin.gi : 205 called from","['gap', 'abstract-algebra', 'polynomials']"
4365642,"Show that, there do not exist two consecutive perfect numbers.","Show that, there do not exist two consecutive perfect numbers I know that it is unknown whether there exist any odd perfect number(s) or not. Also, I know that all the even perfect numbers are determined by Euler's theorem. Which states that: If $N$ is an even perfect number, then N can be written in the form $N =
 2^{n−1} (2^n − 1)$ , where $2^n − 1$ is prime But, still, I am unable to crack the above-mentioned problem asked in some olympiad, I guess. Any progress is appreciated. Please help with the ideas. Thanks in advance.","['number-theory', 'perfect-numbers', 'elementary-number-theory']"
