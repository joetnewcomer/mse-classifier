question_id,title,body,tags
1508035,If A is invertible and $||B-A|| < ||A^{-1}||^{-1}$ prove $B$ is invertible.,Just having really hard time trying to proof : If $A$ is invertible and $||B-A|| < ||A^{-1}||^{-1}$ prove $B$ is invertible. It is related to Neumann Series but i don't understand how to proof with math. Thanks for your help and time. Brian Ignacio,"['normed-spaces', 'linear-algebra', 'matrices']"
1508070,Introduction a good text on principal bundle,"Is there a good text that teaches principal bundle or frame bundle? I am looking for a textbook that might serve as an introduction to topology of principal bundles or frame bundles, specially the global frame field and  the connection on them. Currently, the only book I know of in this regard is: ""Lectures on Differential Geometry"" by S. S. Chern & W. H. Chen & K. S. Lam I have been reading it, but I am occasionally frustrated by the lack of more details on the global section on frame bundles. Thanks in advance!","['principal-bundles', 'differential-geometry', 'reference-request']"
1508125,Uncountable random graphs,"There is a theorem from Erdos and Renyi that says that a random graph on $\aleph_0$ vertices (where each pair of vertices is connected with probability equal to $\frac{1}{2}$) will be isomorphic to the Rado graph with probability 1.  Does an analogous result hold for random graphs on $\kappa$ vertices, for $\kappa > \aleph_0$?  The proof I have read of the Erdos-Renyi theorem does not appear to generalize to larger cardinals. More generally, how does one compute probabilities in cases like this (where, in general, the sample space and the event are both sets of the same cardinality $\lambda > 2^{\aleph_0}$.  What sorts of arguments are used to show that such an event has probability $0$ or $1$.  If anyone has an example of such an problem and solution, I would appreciate it!","['graph-theory', 'probability', 'infinitary-combinatorics']"
1508147,"Orbits of $SL(3, \mathbb{C})/B$","Let $B=  \Bigg\{\begin{bmatrix}
* & *&* \\
0 & *&*\\
0&0&*
\end{bmatrix}
\Bigg\}< SL(3,\mathbb C)$. What is $SL(3,\mathbb C)/B$? Do we use these facts: Borel fixed point theorem. Algebraic actions of unipotent groups are cells $U_-\cdot x_0=U_-/H$? I actually need a detailed solution because I don't have enough background in this subject and I have to understand it! Any solutions or comments are highly appreciated as well. I also need to know the basic definitions and examples in flag manifolds which are essential to the solution. Particularity, in matrices case.","['matrices', 'lie-groups', 'algebraic-groups', 'algebraic-geometry', 'homogeneous-spaces']"
1508160,Prove that a graph is a maximal planar graph if and only if $e = 3v âˆ’ 6$,"Definition: A planar graph with no multi-edges $G$ is called a maximal planar graph if the graph formed by addition of any edge (not already in the $G$) is not planar or the graph is $K_3 $ or $K_4$ (in these cases we can't add any more edges!) I am asked to prove that a graph is maximally planar if and only if $e = 3v - 6$. To prove the first direction (If the graph is maximal then $e = 3v-6$) $\longrightarrow$ I use the fact that every face in a maximal planar graph is a triangle and that every edge is incident to exactly two faces, and so we have $$ 3f = 2e \to f = \frac{2e}{3}$$ Now I substitue this value of $f$ Euler's formula $$v + f = e + 2$$ to get $$v + \frac{2e}{3} = e  + 2$$ and so we have $$e - \frac{2e}{3} + 2 = v$$ and so we have $$\frac{e}{3} = v - 2$$
and so we finally get $$e = 3v - 6$$ $ \longleftarrow$ Now I have some troubles in proving that other direction, that is if $e = 3v-6$ then the graph if maximal planar. I was thinking by mathematical induction on the number of vertices. But I am lost on how to even begin, what will be the least pair of $(e,v)$ for this to work, I assume $v=1,2$ both doesn't make any sense and so $v=3$ will be the least value, and if we have $v = 3$ then we would have $e= 3 \times 3 - 6 = 3$ so we would have three edges for $3$ vertices which is basically $k_3$ and so it works as a base case.","['discrete-mathematics', 'graph-theory', 'combinatorics', 'planar-graphs']"
1508168,Is Max (R) a Hausdorff space?,"Recall a space is totally disconnected if the only connected subsets are singletons (one-point subsets). Now let $R $ be a commutative ring with identity such that $\operatorname{Max}(R)$ is a totally disconnected space, in the sense of the Zariski topology. I want to know if $\operatorname{Max}(R)$ Hausdorff in this case?","['algebraic-geometry', 'general-topology', 'commutative-algebra']"
1508178,Find : $\sqrt[6]{\frac{\sqrt{2}+(-\frac{\sqrt{2}}{2}+\frac{\sqrt{2}}{2}i)^7}{(\frac{\sqrt{2}}{2}+\frac{\sqrt{2}}{2}i)^{11}}}$ in its algebraic form.,"Find : $$\sqrt[6]{\frac{\sqrt{2}+(-\frac{\sqrt{2}}{2}+\frac{\sqrt{2}}{2}i)^7}{(\frac{\sqrt{2}}{2}+\frac{\sqrt{2}}{2}i)^{11}}}$$ in its algebraic form. Now, I kinda think it would not be wise to try to expand this, but rather apply de Moivre formula on the complex number in the numerator and denominator, then simplify that complex number within the root, and once again apply moivres formula. I have tried but then I get the expression:$\sqrt[6]{\frac{\sqrt{2}+\cos{\frac{21 \pi}{4}+i\sin{\frac{21 \pi }{4}}}}{\cos \frac{11\pi}{4}+i\sin\frac{11 \pi}{4}}}$ and don't know what to do with it.","['calculus', 'complex-analysis', 'complex-numbers']"
1508187,Prove that $f$ is injective iff $f(A\cap B)=f(A)\cap f(B)$.,"Let $f$ be a function. Prove that $f$ is injective iff $f(A\cap B)=f(A)\cap f(B)$ for all subsets $A$ and $B$ of the domain of $f$. I realize that this is a duplicate question, however the past post on this question doesn't make sense to me so I would like more clarification.  Here is the post that I have gone over: $f(A\cap B)=f(A)\cap f(B)$ $\iff$ $f$ is injective. What I am not understanding is the general process of how to approach this proof.  I would think that the steps to proving this would go something like this: Assume $f$ is injective. Show that this implies that $f(A\cap B)\subset f(A)\cap f(B)$ and then show that it also implies that $f(A\cap B)\supset f(A)\cap f(B)$. Then double containment has been shown, so $f$ being injective implies that $f(A\cap B)=f(A)\cap f(B)$. Now assume that $f(A\cap B)=f(A)\cap f(B)$ is true. Show that this implies that $f$ is injective. I wanted to approach this in two steps because of the ""if and only if"" statement. However, in the other example, only part number 1 is addressed.  Whynot number 2?",['elementary-set-theory']
1508214,"Over $\mathbb{R}$, is it still the case that the singular locus of a nonempty variety is a proper subvariety?","I thought through the argument in Hartshorne for algebraically closed fields  and I don't see any issues, but I would appreciate it if someone checked their intuition against this, since I don't know anything about real algebraic geometry. I imagine that something weird could happen because points are missing. Roughly the argument would go like this: Given a real affine variety $X$... Because $R$ is a perfect field, there is a separable trascendence basis for the function field. Hence $X$ is birational to a hypersurface. The singular locus of a nonempty irreducible hypersurface $Z(f)$ is a proper closed subvariety, since otherwise $f$ divides all of its derivatives, which implies that all of its derivatives are zero, hence it is the constant function. (I just want to conclude that the Grassmanian is a manifold by identifying it with the locally closed space of orthogonal projections inside $M_n(\mathbb{R})$, and observing that it is homogeneous because $GLn(R)$ acts transitively on it, hence the fact that it is generically smooth implies that it is smooth. (I know how to describe the Grassmanian as a manifold without this argument.))","['algebraic-geometry', 'real-algebraic-geometry']"
1508229,Winning strategy - nim variation,"I was reading about different variations of nim game and I'm trying to find the winning strategy in one of them: there are n empty places on the circle. Two players are placing their coins on empty places. They can put their coin only on empty places which aren't next to a coin of the opponent. In one turn each player can place only one coin. The player who can't move loses. Who's got the winning strategy for each n? I think that for even n the second player will win, because he can always place his coins symmetrically to the first player's. But what about for odd n?","['combinatorial-game-theory', 'game-theory', 'combinatorics']"
1508249,"A plane is colored with three colors. Prove that there exists a right triangle on this plane, having vertices of the same color.","I got stuck with this idea in mind that I could find a shape with all of the vertices connected to each other and all of its angles being 90 degrees. One of such shapes which is not correct is as follows. But, as you can see, it does not work. What is your suggested manner... shape?","['coloring', 'combinatorics']"
1508257,"Procedure for ""rounding the corner"" of a piecewise smooth function","Let $f : \mathbb{R} \to \mathbb{C}$ be a piecewise smooth function which is continuous everywhere and smooth at all but one point $a \in \mathbb{R}$. I would like to know if there is a procedure for taking $f$ and modifying it slightly to get $F \in C^\infty(\mathbb{R})$ such that $F = f$ everywhere except for a small interval around $a$. Intuitively, I think the idea is to replace $f$ on some small interval $(a + \epsilon, a - \epsilon)$ by a smooth function, and then connect that function up to $f$ on either end (in a smooth way). But I'm not exactly sure if or how this can done (for instance, can we only get $F \in C^N(\mathbb{R})$?). I imagine that a convolution may need to be used. Is there a standard analysis book that gives a proof of this procedure, if in fact it can be done? Solutions or reference suggestions are greatly appreciated.","['reference-request', 'real-analysis']"
1508292,To prove the integral inequality $\int_\overline{\theta}^{\pi}\frac{d\theta}{\sqrt{1-\lambda \cos{\theta}}}\gt\pi$,"The following inequality comes up in connection with motion in a dipole field. One has to show that 
$$\int_\overline{\theta}^{\pi}\frac{d\theta}{\sqrt{1-\lambda \cos{\theta}}}\gt\pi$$ where $$\overline{\theta}= \begin{cases}0, & 0\lt\lambda \lt1\\ \arccos(\frac{1}{\lambda}), & \lambda \gt 1\end{cases}.$$
The case $0\lt\lambda\lt 1$ is easily verified. For then $$\int_{0}^{\pi}\frac{d\theta}{\sqrt{1-\lambda \cos{\theta}}}=\int_{0}^{\pi/2}\frac{d\theta}{\sqrt{1-\lambda \cos{\theta}}}+\int_{\pi/2}^{\pi}\frac{d\theta}{\sqrt{1-\lambda \cos{\theta}}}$$
$$\implies\int_{0}^{\pi}\frac{d\theta}{\sqrt{1-\lambda \cos{\theta}}}=\int_{0}^{\pi/2}\frac{d\theta}{\sqrt{1-\lambda \cos{\theta}}}+\int_{0}^{\pi/2}\frac{d\theta}{\sqrt{1-\lambda \sin{\theta}}}\gt2\int_{0}^{\pi/2}d\theta=\pi.$$
However I am finding it difficult to prove the second case. Any pointers would be helpful. Thanks. NOTE: The inequality does not hold for all values of $\lambda > 1$, as has been noted in the following responses.","['integral-inequality', 'integration']"
1508304,Interior product and exterior product,"I have seen around the internet that this should hold: $$\iota_X(\alpha\wedge\beta)=\iota_X\alpha\wedge\beta+(-1)^k\alpha\wedge\iota_X\beta,$$ where $X$ is a vector field, $\alpha$ a $k$-form, $\beta$ an $\ell$-form, $\iota_X$ is the interior product (i.e. $\iota_X\alpha(v_1,\dotsc,v_{k-1})=\alpha(X,v_1,\dotsc,v_{k-1})$), and $\wedge$ is the exterior product. Now I define the exterior product as: $$\alpha\wedge\beta(v_1,\dotsc,v_k,v_{k+1},\dotsc,v_{k+\ell})=\sum_{\sigma\in S_{k+\ell}}\operatorname{sgn}\sigma\alpha(v_{\sigma(1)},\dotsc,v_{\sigma(k)})\beta(v_{\sigma(k+1)},\dotsc,v_{\sigma(k+\ell)}),$$ where some others define it with a coefficient in front of it involving factorials of $\ell$ and $k$. I tried all I could to prove the above identity. I reduced it to proving the case $\alpha=df$. And I'm simply stuck on that case. No matter what, there are sign problems. So could you help me figure this out? Are there coefficients missing with my definition of wedge btw?","['differential-geometry', 'differential-forms', 'exterior-algebra']"
1508355,A curious property of orthogonal matrices,"Let $A$ be an $n\times n$ orthogonal matrix, i.e. $AA^T=A^TA=I$. I noticed experimentally that if we take any increasing set of indices $i_1<\cdots<i_p$, then the sum of the squares of all possible minors with rows corresponding to $\{i_1,\ldots,i_p\}$ is equal to 1. This is a bit hard to formulate, so let me try to say what I mean more explicitly. Let
$$A=\begin{pmatrix}a_1^1&\ldots&a_1^n\\\vdots&\ddots&\vdots\\a_n^1&\cdots&a_n^n\end{pmatrix}$$
and let
$$A_{i_1\ldots i_p}^{j_1\ldots j_p}=\begin{pmatrix}a_{i_1}^{j_1}&\ldots&a_{i_1}^{j_p}\\\vdots&\ddots&\vdots\\a_{i_p}^{j_1}&\cdots&a_{i_p}^{j_p}\end{pmatrix}$$
for $i_1<\cdots<i_p$, $j_1<\cdots<j_p$ and $1\leq p\leq n$. Conjecture: If $A$ is orthogonal, then, for any $i_1<\cdots<i_p$ we have
  $$\sum_{j_1<\cdots<j_p}\left(\det A_{i_1\ldots i_p}^{j_1\ldots j_p}\right)^2=1,$$
  where the sum is over all increasing sets of $p$ indices. How do we prove such a thing? Is that even true? (The answer is very likely yes, as I have checked it for many random examples.) Note: The case $p=1$ is the statement that the rows of $A$ have unit norms, and the case $p=n$ is the statement that $(\det A)^2=1$. The conjecture generalizes these two statements. In particular, the conjecture is clearly true for $n=2$. Now, here is a slightly less trivial example: Example: Take
$$
A=
\begin{pmatrix}
1/\sqrt{2}&0&1/\sqrt{2}\\
-1/\sqrt{6}&\sqrt{2/3}&1/\sqrt{6}\\
1/\sqrt{3}&1/\sqrt{3}&-1/\sqrt{3}
\end{pmatrix}\in O(3).
$$
Then, with $p=2$ and $i_1=1$, $i_2=2$ we have
$$
\begin{vmatrix}
1/\sqrt{2}&0\\
-1/\sqrt{6}&\sqrt{2/3}
\end{vmatrix}^2
+
\begin{vmatrix}
1/\sqrt{2}&1/\sqrt{2}\\
-1/\sqrt{6}&1/\sqrt{6}
\end{vmatrix}^2
+
\begin{vmatrix}
0&1/\sqrt{2}\\
\sqrt{2/3}&1/\sqrt{6}
\end{vmatrix}^2
=\frac{1}{3}+\frac{1}{3}+\frac{1}{3}=1.
$$","['linear-algebra', 'matrices']"
1508440,Multiplying permutation matrix by itself to get identity,"What is rigorous proof that any permutation matrix $P$ raised to some power $k$ equals identity matrix i.e. $P^k= I_n$? Is there any way to find the smallest $k$ other than looking at the matrix, finding all its cycles and then calculating least common multiple? Thank You!","['linear-algebra', 'permutation-matrices', 'permutations', 'matrices']"
1508478,Characteristic function of symmetric distribution,"I know that if $X$ is a symmetric random variable, then the characteristic function is real, since $\overline{E[e^{itX}]} = E[e^{-itX}]=E[e^{itX}]$. However, is there a simple way (without appealing to some sort of inversion) to show the converse? (If the characteristic function of a distribution is real, then the distribution is symmetric.)","['probability-theory', 'characteristic-functions']"
1508522,Why is the number of ways to arrange n people in distinct ways around a circle $\frac{n!}{n}$,"First off, this sounds like permutations, and I'm unable to squeeze  $\frac{n!}{n}$ out of the permutations formula. Could someone break down how to get to that solution from either the Combinations or Permutations standard formulas?",['probability']
1508560,When are these two definitions equivalent?,"I can't remember how to show this but I feel like it must be true: if I have a continuous function $f$, then how do I show that $$\lim_{n\rightarrow \infty}\frac{f(x+\frac{1}{n})-f(x)}{\frac{1}{n}}= c \quad\text{implies}\quad \lim_{\delta\rightarrow 0}\frac{f(x+\delta)-f(x)}{\delta}=c$$ EDIT: Sorry for the confusion, I did indeed mean $n\in \mathbb{N}$.","['calculus', 'real-analysis']"
1508571,Intuition for expectation of random variable as an integral of probability,"Let $X$ be a non-negative random variable on some probability space. Assume $X\in L^1$.
Then, a theorem says: $$E[X] = \int_0^\infty P(X>t) \, dt$$ What would be an intuitive explanation for this theorem? 
I would like to get an intuitive feel for why the theorem should be true. I mean, why should the ""infinite Riemann sum"" $\sum  P(x>t) \, \Delta t$ be equal to the expectation of $X$?",['probability-theory']
1508604,Is my proof that $\sum_{n=1}^{\infty} \frac{n^2}{3^n-2^n}$ converge correct?,"Is the series $\sum_{n=1}^{\infty} \frac{n^2}{3^n-2^n}$ convergent? My Approach: $$\sum_{n=1}^{\infty} \frac{n^2}{3^n-2^n}$$ $$\lim_{n\to \infty}\left|\frac{a_{n+1}}{a_n}\right|$$ So, I did this, and I will write down the simplified form because its hard to type big fractions: $$\lim_{n\to \infty} \frac{(3^n-2^n)(n+1)^2}{(3^{n+1}-2^{n+1})n^2}=\lim_{n\to \infty} \frac{(3^n-2^n)(n+1)^2}{(3\cdot3^n-2\cdot2^n)n^2}$$ Now all I have to do is prove that the denominator gets larger faster than the numerator. But the problem is unlike other questions both the numerator and the denominator have the same highest power. So: $$\lim_{n\to \infty} \frac{(3^n-2^n)(n^2+\cdots)}{(3\cdot3^n-2\cdot2^n)n^2}$$ Divide by $n^2$: $$\lim_{n\to \infty} \frac{(3^n-2^n)}{(3\cdot3^n-2\cdot2^n)}$$ Other terms in the form $\frac{a}{n}$ and $\frac{b}{n^2}$ in the numerator go to zero, so I omitted writing that in latex. Now is this clear to show that the denominator is larger than the numerator? Because 3, and 2 seems small numbers, nothing compared to powers that we deal with in other questions. Also I know that this converges because I confirmed it on http://www.wolframalpha.com/input/?i=%5Csum_%7Bn%3D1%7D%5E%7B%5Cinfty%7D+%5Cfrac%7Bn%5E2%7D%7B3%5En-2%5En%7D","['summation', 'sequences-and-series', 'calculus']"
1508609,Isoclinic rotations in four dimensions,"Given any collection of complementary, oriented (2D) planes in n-dimensional space, and an angle associated with each one, there is a unique rotation of the whole space which restricts to rotations in each plane by their given angles. Conversely, it's not impossible to prove all rotations of space arise in this way. Which leads to the question: can a single rotation arise in this way but from two different sets of complementary planes? If the attached angles are all distinct, I think the answer is no, which means the collection of stable planes is an invariant of ""almost all"" rotations (in the sense that the set of exceptions are of positive codimension in the group of all rotations). If any two of the attached angles are the same, though, then there are an infinite number of planes which are stable under the rotation. Indeed, it is easy to verify that such a rotation moves every ray from the origin by the same angle. (Compare with rotation around an axis in 3D: if a ray is perpendicular to the axis then it is rotated by the full angle, but otherwise if the ray is closer to the axis, the before and after rays will be less than the full angle apart from each other.) For simplicity, lets just take n=4 so our isoclinic rotations are rotations in two orthogonal planes by the same angle. Depending on if combining the orientations of the planes agrees with the orientation of the whole space or not, we can call them left isoclinic or right isoclinic rotations, because they correspond to multiplying quaternions (which form a 4-dimensional space) either on the left or on the right by unit quaternions. As such, the subgroups of left and right isoclinic rotations are both copies of S^3 within SO(4). Now my curiosity is in which planes are stable under an isoclinic rotation, or without loss of generality a one-parameter subgroup of them. Any 2D plane has a unique orthogonal complement, and to the two resulting planes we get a one-parameter subgroup, which can be identified with a line in the corresponding lie algbera so(3) of dimension 3. Therefore we have some map from the Grassmanian manifold of 2D planes to the projective plane. Characterizing stable planes of a given one-parameter group of isoclinic rotations is then equivalent to finding the fibers of this map, which should all be identical by symmetery. So we have some fiber bundle $$F\to \mathrm{Gr}(2,4)\to\mathbb{P}^2 $$ Is there a good explicit description of the fibers? Or just its isomorphism type? My initial thoughts: Given the fiber $F\subseteq\mathrm{Gr}(2,4)$ of a point in $\mathbb{P}^2$ corresponding to some one-parameter subgroup $H$ of isoclinic rotations, if we have any point $x\in\mathbb{S}^3\subset\mathbb{R}^4$ the orbit $Hx$ is a circle in $\mathbb{S}^3$, which determines a plane $\pi$ in $\mathbb{R}^4$, and indeed $\pi\in F$. So we get a map $\mathbb{S}^3\to F$, and the fibers of this map should be the circles in $\mathbb{S}^3$ which intersect planes $\pi\in F$. Thus we have a fiber bundle $\mathbb{S}^1\to\mathbb{S}^3\to F$, which naively suggests to me $F\cong\mathbb{S}^2$ based on the Hopf fibration. Some folks in chat though suggested $F$ should be $\mathbb{P}^2$ or a couple disjoint copies of it. Also mentioned was the fact that $\widetilde{\mathrm{Gr}}(2,4)$, the oriented Grassmanian which is a double cover of the usual $\mathrm{Gr}(2,4)$, is $\cong \mathbb{S}^2\times\mathbb{S}^2$, although I don't know why or for sure how to use this. Ideas?","['lie-groups', 'differential-geometry', 'fiber-bundles', 'grassmannian']"
1508635,Finding dual forms of a frame field on a sphere,"I'm attempting to calculate the Gaussian curvature of the sphere of radius $r$, but I'm not sure how to find the dual forms of the frame field. I start with the parametrization $X(\phi, \theta) = (r \sin \phi \cos \theta, r \sin \phi \sin \theta, r \cos \phi)$. Then this gives me the tangent frame field $E_1 = r \cos \phi \cos \theta \frac{\partial}{\partial x} + r \cos \phi \sin \theta \frac{\partial}{\partial y} - r \sin \phi \frac{\partial}{\partial z}$ $E_2 = -r \sin \phi \sin \theta \frac{\partial}{\partial x} + r \sin \phi \cos \theta \frac{\partial}{\partial y}$ I also compute $dx = r \cos \phi \cos \theta d\phi - r \sin \phi \sin \theta d\theta$ $dy = r \cos \phi \sin \theta d\phi + r \sin \phi \cos \theta d\theta$ $dz = -r \sin \phi\ d\phi$ Now we must find the dual forms of the frame field ${E_1, E_2}$, but how do we do that? I thought we would just substitute $\frac{\partial}{\partial x}, \frac{\partial}{\partial y}, \frac{\partial}{\partial z}$ with $dx, dy, dz$ in their expressions, but this doesn't get me the right answer. I have searched for an explanation of this but couldn't find anything clear. Any help is appreciated.","['differential-geometry', 'curvature']"
1508704,an exercise involving open mapping theorem,"I am stuck with an exercise in complex analysis that reads as follows... Suppose $f:U\longrightarrow\mathbb{C}$ is a holomorphic function from a connected open set $U$ such that for every $z\in U$ there are integers $m,n$ possibly depending on $z$ such that $f(z)^m=\overline{f(z)}^n$. Show that $f$ is constant. I think I should use the open mapping theorem to define a logarithm or a $m$th root of $f$ locally, but since $m,n$ are a priori dependent on $z$ I don't see where to go from there... any help would be appreciated.
Thanks!",['complex-analysis']
1508708,Difference between homology and integral homology?,"What is integral homology? And how does it relate to homology? I can't find a good answer anywhere, so I thought I would ask here.","['abstract-algebra', 'homology-cohomology', 'algebraic-topology', 'homological-algebra']"
1508769,Holomorphic Functional Calculus for the Square Root,"I'm working on a problem set, so I'm not looking for a solution, but just maybe a pointer on where I'm going wrong. I want to use the holomorphic functional calculus to determine the square root of the matrix $$T=\begin{bmatrix}1&1\\0&1\end{bmatrix}.$$ The spectrum of this matrix is just $\{1\}$, so a convenient curve enclosing the spectrum is $\gamma(t)=2e^{it}$ for $t\in\left[-\pi,\pi\right)$. Now I know that for $f(z)$ holomorphic (at least on a region containing both the spectrum and $\gamma$), the operator $\hspace{20pt}f(T)=\frac{1}{2\pi i}\int_\gamma f(\xi)(\xi I-T)^{-1}d\xi=\frac{1}{2\pi i}\int_{-\pi}^\pi f(2e^{it})(\xi I-T)^{-1}\cdot 2ie^{it}dt$ is well-defined. When I then plug in a polynomial for $f$, I get the right answer. However, when I plug in the square root, which I'm taking to be $\sqrt{\gamma(t)}=\sqrt{2}e^{it/2}$ so that the branch cut is along the negative real axis, I get a nonsensical answer: $\hspace{20pt}\left(\frac{20+\sqrt{10}\pi-2\sqrt{10}\arctan\sqrt{10}}{5\sqrt{2}\pi},\frac{10\sqrt{2}+11\sqrt{5}\pi-22\sqrt{5}\arctan\sqrt{10}}{110\pi};0,\frac{20+\sqrt{10}\pi-2\sqrt{10}\arctan\sqrt{10}}{5\sqrt{2}\pi}\right).$ Of course I expect $\sqrt{T}=\begin{bmatrix}1&1/2\\0&1\end{bmatrix}$. The only thing that seems different is that the square root is a multi-valued function whereas polynomials are single-valued, so I'm assuming my error is failing to take something along these lines into account. (I should also not that I'm doing the integral in Mathematica, so maybe there's some problem with doing that.)","['functional-calculus', 'operator-algebras', 'operator-theory', 'functional-analysis']"
1508821,Is $\lfloor n!/e\rfloor$ always even for $n\in\mathbb N$?,I checked several thousand natural numbers and observed that $\lfloor n!/e\rfloor$ seems to always be an even number. Is it indeed true for all $n\in\mathbb N$? How can we prove it? Are there any positive irrational numbers $a\ne e$ such that $\lfloor n!/a\rfloor$ is even for all $n\in\mathbb N$?,"['calculus', 'exponential-function', 'ceiling-and-floor-functions', 'combinatorics', 'factorial']"
1508866,Infinite sequence of digits without consecutive repeating subsequenes,"Problem description: Suppose we use a set of digits $\{0,1,2\}$ to form a sequence, for example
\begin{equation}
120210120102012102010210120212010\cdots 
\end{equation}
The length can be finite or infinite. The sequence is invalid if there are two finite subsequences appears consecutively. For example, the following sequences are invalid 
\begin{equation}
0{\bf 11}\cdots, \quad 010{\bf 201\, 201 }\cdots 
\end{equation}
where the consecutive subsequences are highlighted. Question : Does there exist a valid sequence of infinite length ?
(a non-constructive proof is also acceptable; but a constructive one would be better.) One way to think about it: We can transform the sequence to a (base 3) real number, 
\begin{equation}
120\cdots \rightarrow 0.120\cdots 
\end{equation}
Then the only possible infinite sequence should be a irrational number. Irrational number has no period, which means the pattern will not repeat infinite many times, but may repeat finite many times. A famous example is the Feynman point , The Feynman point is a sequence of six 9s that begins at the 762nd
  decimal place of the decimal representation of $\pi$. So not all irrational number($\le 1$) are valid sequences. I use a short program to do enumeration, but it quickly falls in exponential growth of candidates. I failed to come up with a contraction. Generalization: Can we take a larger set like $\{0,1,2,3, \cdots, n\}$? Can we relax the constraint to be three consecutive repetition or more? Motivation This problem origins from threefold repetition rule in chess, In chess and some other abstract strategy games, the threefold
  repetition rule (also known as repetition of position) states that a
  player can claim a draw if the same position occurs three times, or
  will occur after their next move, with the same player to move. The
  repeated positions do not need to occur in succession. The idea behind
  the rule is that if the position occurs three times, no progress is
  being made. It reduces to the proposed problem if we modify the this rule to be ""the same position occurs two times with the same sequences of moves "". Thanks for your help. Edit: I find a related question: Largest binary sequence with no more than two repeated subsequences","['real-numbers', 'sequences-and-series', 'irrational-numbers']"
1508902,"Given a number, how to find the length of its binary representation?","I think of $\text{log}_2$. But it does not work. For $8 = 2^3$, but the binary representation of 8 is $1000$. The length of it is 4. Any suggestion or help? Thanks.","['number-theory', 'elementary-number-theory']"
1508914,Prove that $\frac{x^n}{n!} < 1$ for $n$ sufficiently large,"Is this statement always true? For any real number $x$, there exists a natural number $n$, such that 
  $\frac{x^n}{n!} < 1$. I reach this conclusion observing the series form of '$e^x$'. I observe that for larger value of $n$, $\frac{x^n}{n!}$ changes the numeric value of $e^x$ right side of decimal point. Please provide the proof or link of the proof.","['real-analysis', 'inequality']"
1508925,How to calculate the expected value of a discrete random variable?,"A private investor has capital of Â£16,000. He divides this into eight
  units of Â£2,000, each of which he invests in a separate one-year
  investment. Each of these investments has three possible outcomes at
  the end of the year: total loss of capital probability 0.1 capital payment of Â£2,000 probability 0.7 capital payment of Â£5,000 probability 0.2 The investments behave independently of one another, and there is no
    other return from them. Calculate the expected payment received by the investor at the end of
    the year. I am unable to figure out how to create a probability distribution from the given data. If I could create it,then I think I can calculate it just applying the formula.","['probability', 'statistics', 'random-variables', 'expectation']"
1508971,How to show that the closed unit ball is a measurable set for outer measure $m^*$?,"How to show that $\{x \in \mathbb R^{n} : \|x\| \le 1 \}$ is an $m^* $-measurable set? I know that a set $E$ is said to be $m^*$-measurable (or Lebesgue measurable) if for any set $A \subset \Bbb R^{n}$ we have, $$m^*(A)=m^*(A \cap E)+m^*(A \cap E^C)$$ How should I use this definition? Is there any other method to show this?","['lebesgue-measure', 'measure-theory']"
1508981,"If $A=\left(\begin{smallmatrix} 1 & \tan x\\ -\tan x & 1 \end{smallmatrix}\right)$ and $f(x)=\det(A^TA^{-1})$, then what is $f(f(f(f \cdots f(x))))$?",Let $$A := \begin{bmatrix} 1 & \tan x\\ -\tan x & 1\end{bmatrix}$$ and $$f(x) := \det(A^TA^{-1})$$ Which of the following can not be the value of $f(f(f(f \cdots f(x))))$ ? $f^n(x)$ $1$ $f^{n-1}(x)$ $nf(x)$ where $n \geq 2$ . I found $$f(x)=\frac{\det(A^T)}{\det(A)}=\frac{\det(A)}{\det(A)}=1.$$ But I could not figure out the answer. Please help me.,"['functions', 'matrices']"
1508996,Using Cauchy's integral theorem to prove a inequality,"I am trying to solve this question: Let $f(z) = c_0 + c_1z + \ldots + c_nz^n$ be a polynomial. If the $c_k$'s are real, show that   $$\int_{-1}^1 f(x)^2 dx \le \pi \int_0^{2\pi} \left | f(e^{i\theta})\right |^2 \frac{d\theta}{2\pi} = \pi\sum_{k=0}^n\left | c_k\right |^2\ .$$ $Hint.$ For the first inequality, apply Cauchy's theorem to the function $f(z)^2$ separately on the top half and the bottom half of the unit disk. I have taken the top half of the unit disk to be the domain $D$ as given in the hint. Since $f(z)^2$ is analytic in $D$, by Cauchy's integral theorem, $\int_{\partial D}f(z)dz = 0$. I can't understand how to proceed from here. Can I break the integral into the sum of the integrals along the semicircle, $|Z| = 1 $ and $Im(Z) \gt 0$, and along the straight line, $-1 \lt Re(Z) \lt 1$ and $Im(Z) = 0$?","['complex-analysis', 'integration']"
1509033,Differential operators acting on the Schwartz space,"I was asking me the following question and cannot find any answer to it. Any help/suggestion is most than welcome!
Let $D$ be a linear differential operator with polynomial coefficients on $\mathbb{R}^n$. Thus $$\displaystyle D=\sum_{\lvert \alpha\rvert\leqslant k} P_\alpha(x) \frac{\partial^{\lvert \alpha\rvert}}{\partial x^\alpha}$$ where the $P_\alpha$'s are polynomial functions on $\mathbb{R}^n$. Consider $D$ as an endomorphism of the space of Schwartz functions $\mathcal{S}(\mathbb{R}^n)$. My question is now the following: Is the image of $D$ a closed subspace of $\mathcal{S}(\mathbb{R}^n)$ (for its natural Frechet topology)?","['schwartz-space', 'ordinary-differential-equations', 'functional-analysis', 'topological-vector-spaces']"
1509044,probability that a sum of 3 is rolled before a sum of 5 is rolled in a sequence of rolls of the dice from a four sided die.,"I am stuck at this problem- A pair of four-sided dice is rolled and the sum is determined.
  What is the probability that a sum of 3 is rolled before a sum of 5 is rolled
  in a sequence of rolls of the dice? What I tried- Let $A=$ sum of $3$; Let $B=$ sum of 5; then $P(A \mid \text{not}~B)=$ i.e probability of A given that B has not happened. $P(A \mid not B)= \dfrac{P(A~\text{and not}~B)}{P(\text{not}~B)}$; But using this approach I am not getting answer.","['bayes-theorem', 'probability', 'permutations']"
1509078,limit of composite functions- switching order of limit,"When is it correct to do stuff like this: 
$$\lim\limits_{x\to a}f(g(x))=f(\lim\limits_{x\to a}g(x)) \quad (*)$$ I know (*) is true when $g$ is continuous at $a$ and $f$ is continuous at $g(a)$. However, what about the other cases ? For instance,  what can we say about $\lim\limits_{x\to a}f(g(x))$ when $\lim\limits_{x\to a}g(x)$ equals $\pm \infty$ or does not exist ? A Similar question in evaluating limits in the indeterminate forms $1^\infty, 0^0,\infty^\infty$. The technique used here is using $e^x$ and $\ln x$ as following $$\lim\limits_{x\to a}f(x)^{g(x)}=
\lim\limits_{x\to a} e^{g(x)\ln f(x)}=e^{\lim\limits_{x\to a}g(x)\ln f(x)},\quad \text{provided the limit exists}$$
Here, the limit is switched from the outside to the inside of the exponential fuction. 
Can I conclude the following statements ? why and why not ? If $\lim\limits_{x\to a}g(x)\ln f(x)=\infty$ then $\lim\limits_{x\to
    a}f(x)^{g(x)}=\infty$ If $\lim\limits_{x\to a}g(x)\ln f(x)=-\infty$ then
$\lim\limits_{x\to a}f(x)^{g(x)}=0$ If $\lim\limits_{x\to a}g(x)\ln f(x)$ does not exist, then
$\lim\limits_{x\to a}f(x)^{g(x)}$ does not exist.","['function-and-relation-composition', 'calculus', 'limits']"
1509100,Poisson Process - non-zero probability of more than one arrival,"Quoting Bertsekas' Introduction to Probability : An arrival process is called a Poisson process with rate $\lambda$ if
  it has the following properties: a) Time homogenity - the probability $P(k,\tau)$ of $k$ arrivals is
  the same for all intervals of the same length $\tau$ b) The number of arrivals during a particular interval is independent
  of the history of arrivals outside this interval. c) Small interval probabilities - The probabilities $P(k,\tau)$
  satisfy: $P(0,\tau)=1-\lambda\tau + o(\tau)$ $P(1,\tau)=\lambda\tau + o_1(\tau)$ $P(k,\tau)=o_k(\tau)$ for $k=2,3,...$ Here, $o(\tau)$ and $o_k(\tau)$ are functions of $\tau$ that satisfy $\mathbb{lim}_{r\to0}\frac{o(\tau)}{\tau}=0$,
  $\mathbb{lim}_{r\to0}\frac{o_k(\tau)}{\tau}=0$ Then we are given the formula: $$P(k,\tau)=e^{-\lambda\tau}\frac{(\lambda\tau)^k}{k!}$$ Note that a Taylor series expansion of $e^{-\lambda\tau}$ yields: $P(0,\tau)=e^{-\lambda\tau}=1-\lambda\tau+o(\tau)$ $P(1,\tau)=\lambda\tau
 e^{-\lambda\tau}=\lambda\tau-\lambda^2\tau^2+O(\tau^3)=\lambda\tau+o_1(\tau)$. First of all, what are $o(\tau)$, $o_1(\tau)$ and $O(\tau)$ in the Taylor expansion? Does it have anything to do with Taylor expansion per se? I thought that $o$ is the little-o notation, but its definition is quite different - $ f(n) = o(g(n))$ if $g(n)$ grows much faster than $f(n)$. In this case, it's quite different. Then what is it? Secondly, the author doesn't prove that the $o$ terms above satisfy $\mathbb{lim}_{r\to0}\frac{o(\tau)}{\tau}=0$,
  $\mathbb{lim}_{r\to0}\frac{o_k(\tau)}{\tau}=0$ as stated in the definition of Poisson process. How can we prove it? Most importantly - why do we want it to satisfy the properties described in 'c) Small interval probabilities'? These 3 formulas are not arbitrary, there has to be a good reason for them. Ideally, if we let $\lambda \to 0$ and it's natural to expect that the probability $P(k,\tau)$ to equal exactly $0$ in the limit, but apparently it's not possible (there will always be that tiny number, $o_k(\tau)$). Or does it equal $0$ in the limit?","['taylor-expansion', 'poisson-distribution', 'probability', 'calculus']"
1509132,If a matrix is row equivalent to some invertible matrix then it is invertible,"I'm not really sure where to go with this question, any help would be appreciated. Question Let $A$ be an $n\times n$ matrix. Prove that if $A$ is row equivalent to some invertible $n\times n$ matrix $B$ then A is invertible. Attempt I'm not sure where a starting point would be. I know that an $n\times n$ matrix B is invertible if there is a matrix A such that B is both the left and the right inverse of A: AB = In and BA = In, but I'm not sure if this would be useful.","['linear-algebra', 'matrices']"
1509136,Independence of complementary events,"Suppose $(\Omega,\mathcal{F},\mathbb{P})$ is a probability space, $I$ is an arbitrary index set and $\{A_i\}_{i \in I} \in \mathcal{F}^{I}$. For $i \in I$ we define $B_i^{(0)} := A_i$ and $B_i^{(1)} := A_i^{\mathsf{c}}$. I want to show the implication
  $$
\exists \, \alpha \in \{0,1\}^{I} \colon \ \{B_i^{(\alpha_i)}\}_{i \in I} \text{ is independent } \Longrightarrow \{A_i\}_{i \in I} \text{ is independent }.
$$ Let $\alpha$ be the fixed sequence and let $J \subseteq I$ with $|J|< \infty$. We have to show that
$$
\mathbb{P}\left( \bigcap_{j \in J} A_j \right) = \prod_{j \in J} \mathbb{P}(A_j).
$$
Let us write $J=J_0 \uplus J_1$, where $J_0 = \{ j \in J \colon \alpha_j =0\}$ and $J_1 = \{ j \in J \colon \alpha_j =1\}$. Then we have
$$
\mathbb{P}\left( \bigcap_{j \in J} A_j \right) =\mathbb{P}\left( \bigcap_{j \in J_0} A_j  \cap  \bigcap_{j \in J_1} A_j \right) = \mathbb{P}\left( \bigcap_{j \in J_0} B_j^{(\alpha_j)}  \cap  \bigcap_{j \in J_1} A_j \right)
$$
To apply the assumption I need something like $\bigcap_{j \in J_1} B_j^{(\alpha_j)}$. But using De Morgan's laws I only get that
$$ 
\bigcap_{j \in J_1} A_j = \left( \left(\bigcap_{j \in J_1} A_j \right)^{\mathsf{c}} \right)^{\mathsf{c}} =  \left(\bigcup_{j \in J_1} A_j^{\mathsf{c}}  \right)^{\mathsf{c}}  = \left(\bigcup_{j \in J_1} B_j^{(\alpha_j)}  \right)^{\mathsf{c}}.
$$
What is the correct way to continue the proof?","['probability-theory', 'independence', 'probability']"
1509153,Explicit Runge-Kutta with Butcher Tableau not stricty positive,"I have a hard time figuring out how explicit Runge-Kutta methods work, so this is a ""what do I misunderstand?""
kind of question. Let $S$ be a system, $Y$ its variables vector and $f(t, Y)$ the function returning the derivative of each variables at a given time and a given state of the system. All explicit Runge-Kutta methods imply to compute the value of $f$ for multiple $(t, Y)$ conditions, and then to perform
a linear combination of all these derivatives in order to obtain a ""better"" derivative. The linear combination to perform is defined by a vector which can be found at the bottom of the Butcher tableau of the method. Lets call
this vector $B$. My question is : as negative values can be found in some $B$ vectors (like those used by the Runge-Kutta-Fehlberg method )
it is possible that the final derivative will be negative whereas all values produced by f are positive, and vice versa . It seems that in some circumstances, the explicit Runge-Kutta methods including negative coefficients in their $B$ vector
can produce irrelevant results by changing the sign of the final derivative. So how comes those methods can be used to
produce meaningful results?","['numerical-methods', 'ordinary-differential-equations']"
1509214,Express an abelian group given as finite generators and their relations as a direct sum of cyclic groups and find corresponding generators.,"According to page 158 of Dummit and Foote's Abstract Algebra (3rd edition): Theorem. ( Fundamental Theorem of Finitely Generated Abelian Groups ) Let $G$ be a finitely generated abelian group. Then (1) $G \cong \mathbb{Z}^r \times Z_{n_1} \times Z_{n_2} \times ... \times Z_{n_s}$ , for some integers $r$ , $n_1$ , $n_2$ , ... , $n_s$ satisfying the following conditions: (a) $r \ge 0$ and $n_j \ge 2$ for all $j$ , and (b) $n_{i+1} \mid n_i$ for $1 \le i \le s-1$ (2) the expression in (1) is unique: if $G\cong \mathbb{Z}^t \times Z_{m_1} \times Z_{m_2} \times ... \times Z_{m_u}$ , where $t$ and $m_1$ , $m_2$ , ... , $m_u$ satisfy (a) and (b) (i.e., $t \ge 0$ , $m_j \ge 2$ for all $j$ and $m_{i+1} \mid m_i$ for $1 \le i \le u-1$ ), then $t = r$ , $u = s$ and $m_i = n_i$ for all $i$ . Question: If an abelian group is expressed as finite generators and their defining relations (for example $G = \langle s,t,u,v \mid s^{4}t^{2}u^{10}v^{6} = s^{8}t^{4}u^{8}v^{10} = s^{6}t^{2}u^{9}v^{8} = e_G\rangle$ ), how to compute and express the group as a direct sum of cyclic groups and find the corresponding generators satisfying the relations ? Could someone explain the computing procedure and show some worked examples? Or could someone point out which textbook taught such computing procedure with worked examples and at which pages/sections? P.S.1 Although I can't find which textbook taught such computing procedure, Derek Holt gave a link to a PDF document teaching such computing procedure in his comment. P.S.2. Could someone reply my 2nd comment of lhf's answer and explain how to find that 4 generators in $C_2 \times C_2 \times C_{\infty}$ satisfying the relations?","['abstract-algebra', 'group-theory', 'abelian-groups', 'finitely-generated']"
1509233,Characterization Theorem of the Lebesgue Stieltjes Measure on the Real Line (Folland Theorem 1.18),"This is a theorem from Folland's Real Analysis, and I have difficulty understanding the proof. My problem is the last two sentences. $H_n$ as defined is clearly compact, and I follow the inequality obtained, which comes from additivity of the measure. Also, taking the limit as $n\to \infty$, we clearly get the last inequality too, but the limit should also affect $H_n$ in the above inequality. However,$\lim_{n\to \infty} H_n=\bigcup_{-\infty}^{\infty}H_n$ need not be a compact set. So how is the result proven here? I would greatly appreciate some help to complete my understanding of this proof.","['analysis', 'real-analysis', 'lebesgue-measure', 'measure-theory']"
1509235,What is the difference between a Summation and an Integration?,"What is the difference between a Summation and an Integration? Both of them add some values. Right? Then what is the difference? Please, explain in layman's terms.","['summation', 'integration']"
1509237,Show that convex hull of a finite set is compact,"I refer to the answer here by Mariano SuÃ¡rez-Alvarez. Question: $(1)$ Why the set $S = \{ (t_1,t_2,...,t_n) \in \mathbb{R}^n  | t_1,t_2,...,t_n \geq 0, t_1 + t_2 + ... + t_n = 1\}$ closed? $(2)$ Why the map $f:S \rightarrow X$ given by $f(t_1,t_2,...,t_n)=t_1x_1 + t_2x_2 + ... + t_nx_n$ continuous? For question $(1)$, I understand the set is bounded as in finite dimensional space, all norms are equivalent. So we can take the norm $\| \cdot \|_1$ to show that $\| t \|_1 = \sum_{k=1}^n{t_k} = 1 \leq M$ for all $t \in S$. Hence, $S$ is bounded.","['continuity', 'simplex', 'functional-analysis']"
1509261,"If Aâ†’(Bâˆ§C), Prove (Dâ†’A) â†’ (Dâ†’C) without using conditional proof","The conditional proof version of this is pretty easy. However, solving this without conditional proof seems to be quite difficult. I tried to turn the premise into: ~A v (Bâˆ§C) (~AvB) âˆ§ (~AvC) I tried to use disjunction introduction to transform (~AvC) into (Dâ†’A) â†’ (Dâ†’C) but failed. Any ideas on how should I proceed? Maybe disjunction introduction is not the right way to deal with this problem?","['logic', 'propositional-calculus', 'discrete-mathematics']"
1509267,About centralizers of involutions in finite simple groups,"I read that for the classification of the finite simple groups the centralizers of involutions plays a crucial role. This has to to with the Brauer-Fowler-Theorem , which in one formulation could be read in this paper by R. Solomon: Let $G$ be a finite simple group of even order containing an involution $t$. If $|C_G(t)| = c$, then $|G| \le (c!)^2$. So I am asking could anyone give a lightweight introduction in what sense these centralizers ""control the structure of the group"", maybe point to simple properties, or even give some accessable, preferable short proofs, about these centralizers or where they were used? So someone not having read all these heavy papers can gain some intuition about them and why they are important?","['abstract-algebra', 'group-theory', 'finite-groups']"
1509276,Evaluation of $\lim_{m \to \infty} \sum_{k = 0}^{m} \frac{m! (2m-k)!}{(m-k)!(2m)!}\frac{x^k}{k!}$,"I have problems evaluating the following limit: $$\lim_{m \to \infty} \sum_{k = 0}^{m} \frac{m! (2m-k)!}{(m-k)!(2m)!}\frac{x^k}{k!}$$ What causes problems in particular is that I am unsure how to behave when there is a sum which becomes a series. I am aware that I need to evaluate all limits at the same time. So, I am not sure if I can use any knowledge about series (e.g. radius of convergence, which should give me the ratio of the coefficents of the series so that I can probably write down the limit) to get more information because it will only be a series when taking the limit. Using Sterling is in question was not that helpful either, because I didn't figure out any way of properly arranging the terms. I need to show that it is only $\approx \exp(x/2)$ which would be enough. Any help would be appreciated!","['factorial', 'sequences-and-series', 'limits']"
1509282,What is the meaning of the subscript sometimes seen after Cartesian products in geometry?,"I often seen a subscript after a Cartesian product, e.g. $A \times_\kappa B$, for example in this Wikipedia article about the spin bundle. What is the meaning of this generalised Cartesian product? Thanks.","['notation', 'general-topology']"
1509314,Does pointwise convergence imply that a subsequence converges in measure?,"I am working to understand the relationships between the many modes of convergence.  One of the true/false problems I am looking at is If $f_n\rightarrow f$ pointwise, then a subsequence $f_{n_k}\rightarrow f$ in measure. My initial thought is true if $\mu(X)<\infty$, since in that case $f_n\rightarrow f$ a.e. implies $f_n\rightarrow f$ in measure.  Pointwise convergence is even better than almost everywhere convergence (right?), so the result should hold.  If $\mu(X)=\infty$ then it is false?","['convergence-divergence', 'measure-theory']"
1509356,Probability of guessing a list,"The question is ""A history quiz has one question where the students are asked to arranged the first ten presidents in correct chronological order. If a student is totally unprepared and makes a random list, what is the probability of getting the incorrect order? The correct order?"" I tried to just do 10! (10 factorial) and got 3,628,800 So I figured probability of guessing correctly is 1/3,628,800 = 2.76 E^-7
And incorrectly is 3,628,799/3,628,800 = .9999997244 Did I do this correctly or is this wrong? Thank you!","['probability', 'statistics']"
1509373,Problem proving Cartan's identity,"There is a famous identity stating that, if $X$ is a field and $\omega$ a form, then: $$\mathcal{L}_X\omega=\iota_Xd\omega+d\iota_X\omega.$$ I'm trying to prove it. Thanks to Anthony Carapetis , I know that: $$\iota_X(\alpha\wedge\beta)=(k+\ell)(\iota_X\alpha\wedge\beta+(-1)^k\alpha\wedge\iota_X\beta),$$ where $\alpha$ is a $k$-form and $\beta$ an $\ell$-form. Now I start on Cartan. I first suppose $\omega=d\alpha$ for $\alpha$ a $(k-1)$-form. Then: $$\mathcal{L}_Xd\alpha=d\mathcal{L}_X\alpha=d(d\iota_X\alpha+\iota_Xd\alpha)=d\iota_Xd\alpha=d\iota_X\omega,$$ and being $\omega$ exact, it is closed, so $\iota_Xd\omega=\iota_X0=0$. Now consider $f\omega$ for any smooth function $f$. $f$ will then be a 0-form. If I try induction, I get stuck with: $$\mathcal{L}_X(f\omega)=f\mathcal{L}_X\omega+\mathcal{L}_Xf\cdot\omega=fd\iota_X\omega+\iota_Xdf\cdot\omega,$$ whereas the RHS of Cartan would be: \begin{align*}
d\iota_X(f\omega)+\iota_Xd(f\omega)={}&d(kf\iota_X\omega)+\iota_X(df\wedge\omega)={} \\
{}={}&kdf\wedge\iota_X\omega+kfd\iota_X\omega+(k+1)\iota_Xdf\wedge\omega-(k+1)df\wedge\iota_X\omega,
\end{align*} and those coefficients get in the way, because the two sides only equate for $k=1$. Am I doing something wrong? Have I used the linked question too hastily to deduce a formula for the normalized wedge? Details Anthony proved, in his answer, that, if $\overline\wedge$ denotes the unnormalized antisimmetrization, then: $$\iota_X(\alpha\overline\wedge\beta)=k\iota_X\alpha\overline\wedge\beta+(-1)^k\ell\beta\overline\wedge\iota_X\beta.$$ The relationship between $\wedge$ and $\overline\wedge$ is that, if $\alpha$ is a $k$-form and $\beta$ an $\ell$-form: $$\alpha\wedge\beta=\frac{(k+\ell)!}{k!\ell!}\alpha\overline\wedge\beta.$$ The above identity then becomes: $$\iota_X\left(\frac{k!\ell!}{(k+\ell)!}\alpha\wedge\beta\right)=\frac{(k-1)!\ell!}{(k+\ell-1)!}k\iota_X\alpha\wedge\beta+(-1)^k\frac{k!(\ell-1)!}{(k+\ell-1)!}\ell\alpha\wedge\iota_X\beta,$$ which with due simplifications done after extracting the fraction from the parenthesis on the left yields: $$\frac{1}{k+\ell}\iota_X(\alpha\wedge\beta)=\iota_X\alpha\wedge\beta+(-1)^k\alpha\wedge\iota_X\beta,$$ which is almost identical to what I said at the start of the question after the link to my previous question. Am I doing something wrong here?","['differential-geometry', 'differential-forms', 'exterior-algebra']"
1509446,Closed-form formula to evaluate $\sum_{k = 0}^{m} \binom{2m-k}{m}\cdot 2^k$,"Inspired by this question I'm trying to prove that $$\lim_{m \to \infty} \sum_{k = 0}^{m} \frac{m! (2m-k)!}{(m-k)!(2m)!}\frac{x^k}{k!} \approx e^{\frac{x}{2}}$$
So I needed to find the value of
$$\frac{\lim_{m \to \infty} \sum_{k = 0}^{m} \frac{m! (2m-k)!}{(m-k)!(2m)!}\frac{x^k}{k!}}{e^{\frac{x}{2}}} = \frac{\lim_{m \to \infty} \sum_{k = 0}^{m} \frac{m! (2m-k)!}{(m-k)!(2m)!}\frac{x^k}{k!}}{\lim_{m \to \infty} \sum_{k = 0}^{m} \frac{\frac{x}{2}^k}{k!}} \\
= \lim_{m \to \infty} \frac{\sum_{k = 0}^{m} \frac{m! (2m-k)!}{(m-k)!(2m)!}\frac{x^k}{k!}}{\sum_{k = 0}^{m} \frac{\frac{x}{2}^k}{k!}} = \lim_{m \to \infty} \sum_{k = 0}^{m} \frac{\frac{m! (2m-k)!}{(m-k)!(2m)!}\frac{x^k}{k!}}{\frac{\frac{x}{2}^k}{k!}} = \lim_{m \to \infty} \sum_{k = 0}^{m} \frac{m! (2m-k)!}{(m-k)!(2m)!}\frac{x^k}{k!}\cdot\frac{k!}{\frac{x}{2}^k} \\
= \lim_{m \to \infty} \sum_{k = 0}^{m} \frac{m! (2m-k)!}{(m-k)!(2m)!}\cdot 2^k $$
Now, since $$\sum_{k = 0}^{m} \frac{m! (2m-k)!}{(m-k)!(2m)!}\frac{x^k}{k!}\cdot 2^k=\sum_{k = 0}^{m} \frac{m! (2m-k)!}{(m-k)!(2m)!}\frac{x^k}{k!} \frac{m!}{m!}\cdot 2^k=\sum_{k = 0}^{m} \binom{m}{2m} \cdot \binom{2m-k}{m}\cdot 2^k\\=\binom{m}{2m} \cdot \sum_{k = 0}^{m} \binom{2m-k}{m}\cdot 2^k=\frac{\sum_{k = 0}^{m} \binom{2m-k}{m}\cdot 2^k}{\binom{2m}{m}}$$
I only need to prove that
$$\lim_{m \to \infty} \sum_{k = 0}^{m} \frac{m! (2m-k)!}{(m-k)!(2m)!}\frac{x^k}{k!} = \lim_{m \to \infty} \frac{\sum_{k = 0}^{m} \binom{2m-k}{m}\cdot 2^k}{\binom{2m}{m}} \approx e^{\frac{x}{2}}$$
so the question: is there a closed-form formula to evaluate $\sum_{k = 0}^{m} \binom{2m-k}{m}\cdot 2^k$ ( Bessel functions are not allowed)?","['limits', 'summation', 'bessel-functions', 'sequences-and-series', 'binomial-coefficients']"
1509523,Random sphere-valued fields,"I would like to generate random functions from an $m$-sphere $S^m$ to an $n$-sphere $S^n$ that are not too wild, some kind of generalization of random Gaussian fields . More precisely, I want $f(x)$, for any $x\in S^m$, to be a random point on $S^n$ with uniform probability distribution, and a property that $f(x)$ and $f(y)$ are probably close to each other, whenever $x$ and $y$ are close. (The analogy in Gaussian processes is that the covariance decays via some given function) One question I have in mind is to see, how often a random function is homotopically trivial. Is there a way to define such maps algorithmically and/or a reference to a rigorous definition of such sphere-valued fields?","['statistics', 'random-variables', 'homotopy-theory']"
1509545,If $X$ and $Y$ are independent random variables and $X$ is absolutely continuous then $X+Y$ is absolutely continuous,"I'm stuck in the following problem. Let $X$ and $Y$ independent random variable where $X$ is absolutely continuous. Then $X+Y$ is absolutely continuous. This is what I have so far. Let $(\Omega,\mathscr{F}, P)$ the probability space where $X$ and $Y$ are random variables. Let $Z=(X,Y)$ a random vector and let $g$ the sum function from $\mathbb{R}^2$ to $\mathbb{R}$, that is, $(x,y) \longmapsto x+y$, clearly $g$ is Borel measurable since it's continuous. Then $g\circ Z =X+Y$ is a random variable. Now since $X$ and $Y$ are independent random variables, the law of $Z$ is the product measure of the law of $X$ and the law $Y$, that is, $P_Z =(P_X \times \,P_Y)$, where $P_Z=PZ^{-1}$ and $(P_X \times \,P_Y)$ is the product measure. Then for $A$ in $\mathscr{B} (\mathbb{R})$ we have \begin{align*}P_{X+Y}(A)=P_Z (g^{-1 }(A))&=(P_X \times\,P_Y)(g^{-1 }(A))\\
&=\int_{\mathbb{R}} P_Y(g^{-1}(A)_x) dP_X \tag{definition of prod. measure}\end{align*} Where $g^{-1}(A)_x= \{y\in \mathbb{R}:(x,y)\in g^{-1}(A)\}=\{y\in \mathbb{R}:x+y\in A\}$, that is, $g^{-1}(A)_x =A-x$. Now we know that $P_X<<\lambda$, since $P_X$ is finite, the Radon-Nikodym Thm (which we abbreviate as R-N), implies that there is a non-negative measurable funcion $f$, such that $P_X(B)= \int_B f d\lambda$ for any Borel set $B$. Then, by the chain rule for R-N we have \begin{align*} \int_{\mathbb{R}} P_Y(g^{-1}(A)_x) dP_X &= \int_{\mathbb{R}} P_Y(A-x) f (x)\, \lambda(dx)
\end{align*} and from here I'm stuck, I'd appreciate a hint to conclude. Thank you. Edit: Thus, \begin{align*} \int_{\mathbb{R}} P_Y(g^{-1}(A)_x) dP_X
&=\int_{\mathbb{R}} \left(\int_{\mathbb{R}} f(x)\chi _{A} (x+y) P_Y(dy) \right) \lambda(dx)\\
&=\int_{\mathbb{R}} \left(\int_{\mathbb{R}} f(x)\chi _{A} (x+y) \lambda(dx)  \right) P_Y(dx) \tag{Fubini-Tonelli}\\
&= \int_{\mathbb{R}} \left(\int_{\mathbb{R}} f(x-y)\chi _{A} (x) \lambda(dx)  \right) P_Y(dx) \tag{$\lambda$ is invariant}\\
&=\int_{\mathbb{R}} \left(\int_{\mathbb{R}} f(x-y)\chi _{A} (x) P_Y(dy)  \right) \lambda(dx) \tag{Fubini-Tonelli}\\
&=\int_{A} \left(\int_{\mathbb{R}} f(x-y) P_Y(dy)  \right) \lambda(dx) \end{align*} Am I right?","['probability-theory', 'probability-distributions', 'measure-theory']"
1509567,How many ways can you form a string with these constraints?,"Suppose we have a string composed of $a$ $1$'s, $b$ $0$'s, and $c$ $-1$'s. Also suppose that $-1$ cannot be in the first, or last two positions of the string. Then we know that the number of unique strings we can have with this constraint is: $$S = \dbinom{a+b+c-3}{c}\dfrac{(a+b)!}{a!b!}$$ Now I'm interested to know how many strings there will be if $-1$ cannot be in the first or last two positions as well as the following two additional constraints: A $1$ must come sometime before a $-1$. There must be a $1$ between two $-1$'s. So I know we need to subtract constraints 1. and 2. from $S$, but I'm not sure how to actually generally calculate them. It will always be the case that the number of $1$'s is greater than the number of $-1$'s and there are an arbitrary number of $0$'s.","['discrete-mathematics', 'probability', 'algebra-precalculus', 'combinatorics']"
1509591,Is the product of a CesÃ ro summable sequence of $0$s and $1$s CesÃ ro summable?,"Suppose $a_n$ and $b_n$ to be CesÃ ro summable sequences of zeros and ones, $a_n\in\{0,1\}$ and $b_n\in\{0,1\}$, i.e. the limits
$$
\lim_{N\rightarrow\infty}\frac{1}{N}\sum_{n=1}^{N}a_n, 
$$
and 
$$
\lim_{N\rightarrow\infty}\frac{1}{N}\sum_{n=1}^{N}b_n, 
$$
do exist. Is the product sequence $c_n=a_nb_n$ always CesÃ ro summable?","['cesaro-summable', 'real-analysis']"
1509632,Attempting to Prove a lower bound for the Binomial Coefficient [duplicate],"This question already has answers here : Proving the inequality $\frac{n^k}{k^k}\leq\binom{n}{k}\leq\frac{n^k}{k!}$ involving binomial coefficient. (3 answers) Closed 8 years ago . The inequality I am having a problem with is as follows: Let $1\leq k\leq n$, where $n,k$ are natural numbers. Prove that $\binom{n}{k}\geq(\frac{n}{k})^k$ I have proven already the inequality $\binom{n}{k}\leq\frac{n^k}{k!}\leq(\frac{en}{k})^k$, but cannot seem to prove my lower bound. I have been told that differentiating may be of use, though I am unsure how to approach this. Any help is greatly appreciated.","['binomial-coefficients', 'combinatorics', 'inequality']"
1509666,"Is every closed set , the set of zeroes (resp.critical points) of some smooth real valued function? [duplicate]","This question already has answers here : Infinitely differentiable function with given zero set? (3 answers) Closed 8 years ago . Let $A$ be a closed subset of $\mathbb R^n$ : 1) Is it true that for some smooth function $f: \mathbb R^n \to \mathbb R$ , $A=f^{-1}(\{0\})$ 2)Is it true that for some smooth function $f: \mathbb R^n \to \mathbb R$ , $A$ is the set of all critical points of $f$ ?","['metric-spaces', 'multivariable-calculus', 'normed-spaces']"
1509671,Pullback of meromorphic differential forms to blowup,"Consider $Bl_{(0,0)}\mathbb{A}^2 \stackrel{f}{\to} \mathbb{A}^2$ where $f$ is the natural morphism contracting the exceptional curve to a point. For the pullback of a regular 1-form from $\mathbb{A}^2$ along $f$, one can see in a few different ways that this differential form must be 0 along the exceptional $\mathbb{P}^1$ in the blowup. Either by Hartshorne II.8.11, or by the argument that all tangent vectors that are purely in the  $\mathbb{P}^1$ direction are in the kernel of the differential of $f$. On the other hand, what happens if I have a meromorphic differential form? Say, one with a denominator that vanishes along some proper subvariety of $\mathbb{A}^2$ containing $(0,0)$. I think there ought to be some sort of calculus argument here which says something different will happen, but I'm fairly confused about this right now. Any advice or answers would be appreciated.",['algebraic-geometry']
1509681,How to have idea to prove trigonometric identities,"Hi to explain this better I'll take an example.
I have this identity that's giving me a hard time. $$\frac{\cos^2(a)-\sin^2(b)}{\sin^2(a)\sin^2(b)} = \cot^2(a)\cot^2(b)-1$$ This is what i would do $$\cos^2(a)/(\sin^2(a)Â·\sin^2(b))-\sin^2(b)/(\sin^2(a)Â·\sin^2(b)) \\ 
\cot^2(a)Â·1/\sin^2(b)-1/\sin^2(a)$$ then, we know that $$1=\cos^2(b)+\sin^2(b) \\
\vdots \\
\cot^2(a)Â·\cot^2(b)+\cot^2(a)-1/\sin^2(a)$$ Which of course is wrong but just wanted to show you guys how my mind thinks.
Is there any right way of solving this or do I just  have to keep trying.
THANKS.",['trigonometry']
1509778,Chevalley Decomposition of $x+y$ in terms of Chevalley decomposition of $x$ and $y$.,"Given $V$ finite dimensional vector space on an algebraically closed field $k$ write for $x\in \text{End}_k(V)$
  \begin{gather}
x=x_s+x_n
\end{gather}
  the Chevalley decomposition of $x$ with $x_s$ diagonalizable and $x_n$ nilpotent.
  Let $x,y \in \text{End}_k(V)$ be such that $xy=yx$, then
  \begin{gather}
(x+y)_s=x_s+y_s \\ (x+y)_n=x_n+y_n
\end{gather} How can I prove it?
If I show that $x_sy_s=y_sx_s$ then $x_s$ and $y_s$ admit a simultaneous basis of eigenvectors, then $x_s+y_s$ is diagonalizable and $x+y-(x_s-y_s)=x_n+y_n$ is nilpotent since sum of nilpotent matrices, and we get the result.","['abstract-algebra', 'linear-transformations', 'linear-algebra', 'matrices']"
1509864,"For which $\mathcal{F} \subset C[0,1]$ does there exist a sequence converging pointwise to the supremum?","This question is following this one . Take a subset $\mathcal{F} \subset C[0,1]$ and consider 
$$g(x)= \sup{ \{f(x)\mid f\in \mathcal{F}} \}.$$ Added : $g$ may take infinite values. What property(ies) should have $\mathcal{F}$ in order that it exists a sequence $(f_n)$ of elements of $\mathcal{F}$ that converges pointwise to $g$? I know that the question is open. Answers can be in the form of examples. Please don't shoot to quick to close it because it is an open question!","['convergence-divergence', 'real-analysis', 'general-topology']"
1509872,Need help to prove $(R\circ S \circ R)^n \subseteq (R \circ S)^n \circ R$,"There is a relation $R$ and $S$ on set $U$. Given $R$ is transitive. I need to prove  $(R\circ S \circ R)^n \subseteq (R \circ S)^n \circ R$ for all $n \geq 1 $ My Attempt:-
Tried doing this by induction for base case $n=1$  its trivial as $(R\circ S \circ R)^1 \subseteq (R \circ S)^1 \circ R = (R\circ S \circ R) \subseteq (R \circ S) \circ R$ Took Induction Step as : $(R\circ S \circ R)^n \subseteq (R \circ S)^n \circ R$ now for $n=n+1$ $(R\circ S \circ R)^{n+1} $ $R\circ S \circ R \circ (R\circ S \circ R)^n $  $\qquad$ { Def. of exponents } $\subseteq R\circ S \circ R \circ (R\circ S)^n \circ R$  $\qquad$ { Using $I.H$ and monotonicity } I am stuck here and do not know how to proceed further. Can anybody try to help me by providing a hint.","['relations', 'discrete-mathematics']"
1509873,Inequality involving homogenous function of degree -1,"Let $p\in[1,\infty]$ . For $f\in L^p(0,\infty)$ we define $Tf:x\mapsto \int_0^\infty K(x,y)f(y)\,dy$ where $K$ is homogenous of degree $-1$ , i.e. $K(\lambda x,\lambda y) = \lambda^{-1} K(x,y)$ for $\lambda>0$ . Suppose that $$A_K=\int_0^\infty |K(1,y)|y^{-1/p}\,dy<\infty.$$ Then $\|Tf\|_p\leq A_K\|f\|_p$ . My attempts included applying the generalised Minkowski-inequality and dualizing $L^p=(L^q)^*$ but the farthest I've gotten is showing that $$\|Tf\|_p\leq \int_0^\infty |f(y)|\left( \int_0^\infty |K(x,y)|^pdx\right)^{1/p}dy$$ but it doesn't exactly seem promising. I'm extremely thankful for any help!","['integral-inequality', 'functional-analysis', 'measure-theory']"
1509887,Is the function characterized by $f(\alpha x+(1-\alpha) y) \le f^{\alpha}(x/\alpha)f^{1-\alpha}(y)$ convex?,"[Edit. The question lacks certain important conditions, as kindly pointed out by NeutralElement .  Below is the amended version.  I apologize for the omissions and many thanks to NeutralElement and user254665 for helpful comments. ] This is related to the question raised by Boby .  For a variant, consider the nonnegative function $f(x)$ satisfied by
$$
f(\alpha\, x + (1 - \alpha) \, y) \le f^{\alpha}(x/\alpha) \, f^{1-\alpha}(y),
\qquad (1)
$$
for $\alpha \in (0, 1]$ and real $x$ and $y$. What can we say about his function?  Is it always convex? [Edit2.  In the original question by Boby, there is one additional condition that requires that $x \ge y$.  But unfortunately I missed this condition in the previous edit.  So I'll settle for the result for the above question.  I apologize for the many omissions.  However, if anyone can comment how this condition changes the result, it would be much appreciated.  Thank you.] Here are some observations that may or may not help. Observation 1 With $y = x$, we have
$$
f(x) \le f(x/\alpha).  \qquad (2)
$$
This means that $f(x)$ is increasing for $x \ge 0$, and decreasing for $x \le 0$.  A corollary is that $f(0)$ is the global minimum, i.e,
$$
f(0) \le f(x). \qquad (3)
$$ Observation 2 By Young's inequality or the weighted AM-GM inequality , we have
$$
f^\alpha(x/\alpha) f^{1-\alpha}(y)
\le \alpha f(x/\alpha) + (1 - \alpha) f(y). \qquad (4)
$$
Thus, (4) and (1) require
$$
f(y + \alpha (x - y))
\le
\alpha \, f(x/\alpha) + (1 - \alpha) \, f(y).
$$","['convex-analysis', 'functional-analysis', 'functional-inequalities']"
1509912,A time reversible Markov chain problem on urns,"Question: (Ross Probability Models , Ch. 4, Ex. 70) A total of $m$ white balls and $m$ black balls are distributed into two urns such that each urn contains $m$ balls. At each stage, a ball is selected from either urn, and the selected balls are switched. Let $X_n$ denote the number of black balls in the first urn after the $n$th switch. Give (a) the transition probabilities, (b) find/guess the limiting probabilities of the chain without computation, and (c) actually compute the limiting probabilities and show that the chain is time reversible. Problem: I don't know how to reason through (b), and I can't find a simple form solution for (c). Attempt: I will first do part (a). Let $P$ be the transition matrix and $\pi$ the limiting probabilities vector. Then $$P_{ii}=2\left(\frac{i}{m}\right)\left(\frac{m-i}{m}\right)$$ Since we choose $i$ blacks from the first urn and $m-i$ blacks from the second, or vice versa regarding whites. $$P_{i,i+1}=\left(\frac{m-i}{m}\right)^2$$ Since we choose from the $m-i$ whites from the first urn and $m-i$ blacks from the second. Similarly, then, $$P_{i,i-1}=\left(\frac{i}{m}\right)^2$$ Noting that $P_{i,i-1}+P_{ii}+P_{i,i+1}=1$, all other entries in $P$ for each $i$ is $0$. Now I will solve (c). The chain is time reversible because for any states $i,j$, it's not possible to witness $i$ followed by $j$ twice without witnessing $j$ followed by $i$ someplace in between. Thus, we have $$\pi_iP_{i,i-1}=\pi_{i-1}P_{i-1,i}\Longrightarrow\pi_i=\pi_{i-1}\frac{P_{i-1,i}}{P_{i,i-1}}=\pi_{i-1}\left(\frac{m-i+1}{i}\right)^2$$ Then $$\pi_i=\pi_0\prod_{j=1}^i\left(\frac{m-j+1}{j}\right)^2$$ And $$\sum_{k=0}^m\pi_k=\pi_0\sum_{k=0}^m\prod_{j=1}^k\left(\frac{m-j+1}{j}\right)^2=1$$ Which gives me a very messy solution for $\pi_i$ for all $i$. Unfortunately, I can't think of any way to simplify this with the squared term, but for various reasons I suspect that the solution has a nice representation. Update: We have that $$\pi_0\sum_{k=0}^m\prod_{j=1}^k\left(\frac{m-j+1}{j}\right)^2=\pi_0\sum_{k=0}^m{m\choose k}^2=\pi_0\sum_{k=0}^m{m\choose k}{m\choose{m-k}}=\pi_0{{2m}\choose m}=1$$ Or $$\pi_0=\frac1{{2m}\choose m}$$ And $$\pi_i=\pi_0\prod_{j=1}^i\left(\frac{m-j+1}{j}\right)^2=\pi_0{m\choose i}^2=\pi_0{m\choose i}{m\choose{m-i}}=\frac{{m\choose i}{m\choose{m-i}}}{{2m}\choose m}$$","['probability', 'markov-chains', 'stochastic-processes']"
1509927,"Probability that a random number belongs to $[a,b]\subseteq[0,1]$","Let $(a_k)$ be an infinite sequence of $0$s and $1$s chosen at random so that the probability that any $a_k=0$ is $1/2$. Show that the probability that the random number $\sum_{k=1}^\infty a_k/2^k$ belongs to $[a,b]\subseteq[0,1]$ is $b-a$. Hint: Consider dyadic intervals (i.e. intervals of the form $\left[j/2^k,(j+1)/2^k\right]$) first. Now this is in the context of a measure theory course. The only thing we have said about probabilities is that a probability is a measure $\mu$ on a space $(X,\mathcal A)$ that satisfies $\mu(X)=1$. Nothing else. Initially, I thought that the Lebesgue measure on $[0,1]$ is a probability measure and so we must have $\mu([a,b])=b-a$. So the probability that $x\in[a,b]$ is $b-a$ for any $x$. Clearly something is wrong with this. So, I tried to use the hint. Well, I know that $[0,1]=\bigcup_{j=0}^{2^k-1}\big[j/2^k,(j+1)/2^k\big)$ for any $k$ and so the probability that any random $x$ is in one of the dyadic intervals is $1/2^k$. I wish to find a similar decomposition for $[a,b]$ but I can't think of any; and even if I could, I don't see how this is useful for the problem. As you see I am very confused here. Any explanation will be appreciated.","['probability-theory', 'lebesgue-measure', 'measure-theory', 'analysis', 'probability']"
1509944,An example of a curve with all indexes,"Give an example of a closed rectifiable curve $\gamma$ in $\mathbb{C}$ such that for every $k\in\mathbb{Z}$ there is some $a$ out of the curve such that $n(\gamma;a)=k$. Here, $n(\gamma;a)$ is the index of a curve $\gamma$ around $a$ defined by $\dfrac{1}{2\pi i}\displaystyle\int_{\gamma}\dfrac{dz}{z-a}$. If we only care for $k\ge 0$, I have thought something like a ""spiral"" converging to $0$. Would this work?: $\gamma(t)=te^{i/t}$ if $0<t\le 1$, and $\gamma(0)=0$ My idea is you can always choose a point between the arcs, so that the curve travels a fixed $k$ times counterclockwise around the point. But I don't know how to choose these points formally. This is a fine example or is there something easier?",['complex-analysis']
1509945,"Sequences $(\lambda_n)$ such that for every summable sequence $(a_n)$, $(\lambda_na_n)$ is also summable.","An oral examination exercise : Find all the sequences $(\lambda _n)_n$ of real numbers such that :
  $$\sum a_n \; \text{is convergent} \Longrightarrow \sum \lambda_n a_n \; \text{is convergent}$$ I think the only working sequences are the ones which are bounded and ultimately of constant sign. I've not seen it on the site so if you have seen it please tell me how you would solve it. Thanks.","['analysis', 'sequences-and-series', 'real-analysis']"
1510020,How to calculate the radius of convergence of this power series?,"Let $f(z)=\sum_{n=0}^{\infty}a_nz^n$ be a formal pwer series with radius of convergence $R(f)=1$. Set $s_n=a_0+a_1+....a_n$. Let $g(z)=\sum_{n=0}^{\infty}s_nz^n$. Show that $R(g)=1$. I noticed that $g(z)-zg(z)=f(z)$, if we are allowed to rearrange the summation of those power series $g(z)$ and $zg(z)$. But we don't know the radius of convergence yet so we can't say so. We can at most say that within $|z|<\min(1,R(g))$, the equation holds. So it means that radius of convergence of $g$ is greater or equal to $1$, but if $|z|>1$, the RHS $f(z)$ diverges, so the radius of convergence can only be $1$?  I forgot to mention that the coefficients and $z$ take on $\mathbb{C}$","['power-series', 'analysis', 'complex-analysis']"
1510040,"In an acute-angle triangle $ABC$, find the minimum value of $5\tan A+2\tan B+ \tan C$.","Question. In an acute-angle triangle $ABC$, what is the minimum value of $5\tan A+2\tan B+ \tan C$? I have a form : Given $5\tan A+2\tan B+\tan C$ in a $\triangle ABC$ and $A<90,B<90,C<90$.
The function $f(A,B)=5\tan A+2\tan B-\tan(A+B)$ has an extremum if 
$$
\begin{align}
\frac{\partial f(A,B)}{\partial A} &= 0 \\
\frac{\partial f(A,B)}{\partial B} &= 0.
\end{align}
$$
This gives us
$$
\begin{align}
\frac{5}{\cos^{2}A} &=\frac{1}{\cos^{2}(A+B)} \\
\frac{2}{\cos^{2}B} &=\frac{1}{\cos^{2}(A+B)},
\end{align}
$$
or
$$
\begin{align}
2\cos^{2}A &= 5\cos^{2}B \\
2\cos^{2}(A+B) &=\cos^{2}B.
\end{align}
$$
Since $A<90,B<90,C<90$, we write
$$
\sqrt{2}\cos A=\sqrt{5}\cos B \quad (1)
$$
and
$$
-\sqrt 2 \cos C=\sqrt 2 \cos(A+B)=-\cos B \quad (2).
$$
From (2): $\sqrt{2}\cos A\cos B-\sqrt{2}\sin A\sin B=\cos B$.
Using (1): 
$$
\sqrt{5}\cos^{2}B-\sqrt{2}\sin A\sin B=-\cos B,
$$
$$
\sqrt{5}\cos^{2}B+\cos B=\sqrt{2}\sin A\sin B.
$$
$$
(\sqrt{5}\cos^{2}B+\cos B)^{2}=2\sin^{2}A\sin^{2}B.
$$
$$
5\cos^{4}B+2\sqrt{5}\cos^{3}B+\cos^{2}B=(2-5\cos^{2}B)(1-\cos^{2}B).
$$
$$
\sqrt{5}\cos^{3}B+4\cos^{2}B-1=0.
$$
$$
\begin{align}
\cos B &= \frac{1}{\sqrt{5}} \\
\cos A &= \frac{1}{\sqrt{2}}.
\end{align}
$$ Conclusion: $\tan A = 1$ and $\tan B = 2$ and $\tan C = -\tan(A+B)=3$. There is another way to solve without calculation?",['trigonometry']
1510074,SVM : Why can we set 1 in the hyperplane equation?,"I am reading the Wikipedia article about SVM and there is something I don't understand. When they say: These hyperplanes can be described by the equations $$ wx - b=1 $$  and $$wx - b=-1$$ I was wondering where does the +1 and -1 come from? I found two papers which explain that this is an arbitrary choice: We can write the following equations for the support hyperplanes: $$w^T x  = b + \delta    $$ 
   $$w^T x = b âˆ’ \delta   $$ 
  We now note that we have over-parameterized the problem: 
  if we scale w, b and $\delta$ by a constant factor $\alpha$, 
  the equations for x are still satisfied. To remove this  ambiguity we will require that $\delta$ = 1, this sets the scale of the  problem , i.e. if we measure distance in millimeters or meters Source but I don't understand what he means when he says ""this sets the scale of the problem"" and Note that if the equation
  $f(x) = wx + b$ defines a discriminant function (so that the output is > $sgn(f(x))$), then
  the hyperplane $cwx + cb$ defines the same discriminant function for any $c > 0$. Thus we have the freedom to choose the scaling of $w$ so that $min_{x_i}
|wx_i + b| = 1$. Source but I don't understand why he introduces  $min_{x_i}
|wx_i + b| = 1$. My understanding is that we can do it because variables $w$, $b$ and $\delta$ are kind of linked together. Can we change the Wikipedia definition and say These hyperplanes can be described by the equations  $$ wx - b= 2 $$ 
  and   $$wx - b=- 2$$ or is this incorrect and so we must say that : These hyperplanes can be described by the equations  $$ 2wx - 2b= 2 $$ 
  and   $$2wx - 2b=- 2$$ Could you clarify this for me?","['geometry', 'linear-algebra', 'vectors']"
1510076,Can all differential equations be solved using power series?,"To elaborate, does the differential equation have to be of some form to be solvable by power series.  More specifically, if I wanted to solve this equation by power series would I be able to?
$$
(1-x^2)y''(x)-3xy'(x)+n(n+2)y(x)=0
$$
Also, $n$ is a constant in the stated equation.","['power-series', 'ordinary-differential-equations']"
1510077,how to find the derivate of a function g(x),It's $g(x)={{x^{2}-1}\over{x^{2}+2}}$ and i have to calculate $g^{13}(0)$. I can't calculate all the derivates so i think to use power series. $g(x)={{x^2\over{x^{2}+2}}-{1\over{x^2+2}}}$ Can i use the geometric series?,"['power-series', 'calculus', 'real-analysis', 'derivatives']"
1510092,Inversion of a block matrix,"Let $S$ to be a symmetric and positive semi-definite matrix of size $n$ . What is the inverse of the following block matrix $$
M_{2n\times 2n}=
\begin{bmatrix}
aI+S & -I\\ -I & aI+S
\end{bmatrix}
$$ where $a$ is an arbitrary real scalar?","['inverse', 'linear-algebra', 'block-matrices', 'matrices']"
1510125,Explain unoriented $S^2$ bundle over $S^1$.,"In Hamilton's classification of closed 3-d nonnegative Ricci curvature manifold, unoriented $S^2$ bundle over $S^1$ is one of the possible type.
Who can give me a description of it? Many thanks!","['differential-geometry', 'algebraic-topology', 'geometric-topology']"
1510188,Spin^c structure induced by a spin structure,"I wondered how it works exactly to induce a $\mathrm{spin^c}$-structure if a spin structure is given. I wanted to use the following definitions as used in Friedrich`s ""Dirac operators in Riemannian Geometry"" Let $(Q,\pi_Q,M;\operatorname{SO}(n))$ be a $\operatorname{SO}(n)$-principle bundle on a manifold $M$. The maps $\lambda:\operatorname{Spin}(n)\to \operatorname{SO}(n)$ and $\lambda_\mathrm{c}:\mathrm{Spin^c}(n)\to \operatorname{SO}(n)$ below are the respective standard covering maps. The  horizontal arrows are the actions of the groups on the fibers. Definition A spin structure on $Q$ is a pair $(P,\varLambda)$ consisting of a $\operatorname{Spin}(n)$-principal bundle $(P,\pi_P,M;\operatorname{Spin}(n))$ such that $\varLambda:P\to Q$ is a $2$-fold covering bundle map for which the following diagram commutes Definition A $spin^c$ structure on $M$ is a pair $ (P^{\mathrm{c}},\varLambda_{\mathrm{c}})$ consisting of a $\mathrm{Spin^c}(n)$-principle bundle $(P^{\mathrm{c}},\pi_P,M;\mathrm{Spin^c}(n))$ over $M$ and a fiber map $\varLambda_{\mathrm{c}}:P\to Q$ such that the following diagram commutes: Now a typical assertion is that each spin structure induces a canonical $\mathrm{spin^c}$ structure. I tried to show that via connecting the commutative diagrams, and using the fact that there is a natural inclusion of the spin group in the $\mathrm{spin^c}$ group. That means I get a diagram, where the ""upper"" and ""big"" part commutes, but I do not know how to define $\varLambda_{\mathrm{c}}$:","['principal-bundles', 'differential-geometry', 'spin-geometry']"
1510245,Proving a set of Lipschitz Continuous functions is closed,"Let $X:=C[0,1]$ denote the set of all continuous functions $f:[0,1]\to\mathbb{R}$. For $f,g\in C[0,1]$, define $$d(f,g):=\max_{x\in[0,1]}|f(x)-f(y)|.$$ Consider the subset of X containing all Lipschitz continuous functions with Lipschitz constant $l$: $$\Omega:=\{f\in C[0,1]\mid |f(x)-f(y)|\leq l|x-y|\}.$$ Prove that $\Omega$ is a closed set.","['analysis', 'real-analysis']"
1510275,"Differentiability of $f(x) = \exp(-1/x^2), f(0) = 0$ [duplicate]","This question already has answers here : How do you show that $e^{-1/x^2}$ is differentiable at $0$? (4 answers) Closed 8 years ago . Let $f: \mathbb{R}\to \mathbb{R}$ be defined by $f(x) = \exp(-1/x^2)$ if $x\neq 0$ and $f(0)=0$. I'm trying to show that $f$ is differentiable with continuous derivative in all points $x\in \mathbb{R}$. Now, if $x\neq 0$ then $f(x) = \exp \circ g(x)$ being $g(x)=-1/x^2$. In that case, the chain rule can be used to see that $f$ is differentiable with derivative $$f'(x) = \exp(g(x))g'(x) =\exp \left(\dfrac{-1}{x^2}\right)\dfrac{2}{x^3},$$ which clearly is continuous, since $\exp$ is continuous, $g$ is continuous and $g'$ is continuous on these points. Now, when $x=0$ I don't really know what to do. By the definition I think it is not a good way, since we have $$f'(0)=\lim_{x\to 0}\dfrac{\exp\left(\frac{-1}{x^2}\right)}{x}$$ and I'm unsure on how to proceed. Of course I could write $\exp$ as a series, but I don't know how to rigorously justify the manipulations that follows. So how to show that this function is differentiable with continuous derivative? EDIT: As pointed out in comments, if we set $h(t)=te^{-t^2}$ and $q(x)=\exp\left(\frac{-1}{x^2}\right)/x$ then if $s(x)=1/x$ we have $$q(x)=h(s(x)).$$ Then it is intuitively clear that $$\lim_{x\to 0^+}q(x)=\lim_{t\to \infty}h(t).$$ Now how to prove this rigorously? This change of variables involves one limit at infinity, and I'm unsure how to justify this.","['calculus', 'real-analysis', 'limits', 'derivatives']"
1510300,What is the minimum number of points a sample space must contain that there exist n independent events that wouldnt have probability of zero or one,"This is classic problem from most of the probability theory textbooks. What is the minimum number of points a sample space must contain in order that there exist $n$ independent events $A_1,\ldots,A_n$ none of which has probability zero or one? The answer I obtained by drawing Venn diagrams for cases $n=2,3$ was $2^n$. Cannot prove it for general case, meaning for any $n$.",['measure-theory']
1510304,Likelihood of a correct diagnosis of a disease,"A certain cancer is found in one person in 5000. If a person does have the disease, in
92% of the cases the diagnostic procedure will show that he or she actually has it. If
a person does not have the disease, the diagnostic procedure in one out of 500 cases
gives a false positive result. Determine the probability that a person with a positive
test has the cancer. Very lost on how to go about formulating an equation for this problem. I know it has something to do with conditional probabilties but I'm unsure where to begin. My best guess would be $((1-0.92)*(1/500))/(1/5000)=0.8$ but I am unsure if this is even close to right. Any advice would be appreciated.","['probability', 'statistics']"
1510319,Spanning trees of ladder graphs...,"a. Draw the 1-ladder, 2-ladder, and 3-ladder graphs, and calculate the number of spanning
trees for each.
- I have completed this part and wanted to confirm that these numbers look accurate, I feel like i was able to get all of them. b. A spanning tree of a connected graph $G$ is a a tree which contains all the vertices
of $G$ and which is a subgraph of $G$. If $T(n)$ is the number of spanning trees for the $n$-ladder, then
$T(n)=4 Â· T(n - 1) - T(n - 2)$. Solve this recurrence relation to obtain a closed formula the number of spanning trees for a $n$-ladder.
I don't really know how to do this part of the problem and would appreciate any help.","['graph-theory', 'discrete-mathematics', 'trees']"
1510360,"$g'(x) = cg(x)$. Are there any other functions, aside from ${e^{cx}}$, that satisfy the condition?","Assume that $g:\mathbb R \longrightarrow  \mathbb R$ and $g'(x) = cg(x)$, where $c\in\mathbb R$ and $\forall\ x\in\mathbb R$. Are there any other functions, aside from ${e^{cx}}$, that satisfy the condition? P.S If there are not, how can it be proven?","['derivatives', 'calculus', 'integration']"
1510384,Show that a developable surface has zero Gaussian curvature.,"A developable surface is a ruled surface $x(s,t)=\alpha(s)+t\beta(s)$, where $\alpha(s)$ is a unit-speed curve and $|\beta(s)|=1$, such that the tangent planes along each ruling are parallel. Show that the Gaussian curvature of such surface is $K=0$. I am trying to show this using the equations of $\kappa_1$ and $\kappa_2$ which are the eigenvalues of the matrix $L$, which is the Weingarten map matrix. In computing the coefficients of the second fundamental form $L_{ij}:=\langle x_{ij},n \rangle$ with $i,j\in \{s,t\}$, I only find that $L_{tt}=0$, and I don't see how the calculations will simplify.","['differential-geometry', 'curvature']"
1510389,On the form of injective function $f:\mathbb R^+ \to \mathbb R^+$ such that $\|u\|:=f^{-1}\Big(\sum_{i=1}^nf(|u_i|)\Big)$ is a norm,"Let $f:\mathbb R^+ \to \mathbb R^+$ be injective such that on $\mathbb R^n , n>1)$ , $\|u\|:=f^{-1}\Big(\sum_{i=1}^nf(|u_i|)\Big)$ , where $u=(u_1,u_,\ldots,u_n)$ , defines a norm ; then is it true that for some $p\ge1$ , $f(x)=cx^p , \forall x \in \mathbb R^+$ ?","['analysis', 'multivariable-calculus', 'normed-spaces']"
1510439,To prove $(a âˆ— b^{âˆ’1} âˆ— a)^3 = I$.,"If $(G, âˆ—, I)$ is a group and $a, b âˆˆ G.$ satisfy $a^2=b^3=I$. 
Then I need to prove $(a âˆ— b^{âˆ’1} âˆ— a)^3 = I$. My Work:- $(a âˆ— b^{âˆ’1} âˆ— a)^3$ $a âˆ— b^{âˆ’1} âˆ— a * a âˆ— b^{âˆ’1} âˆ— a * a âˆ— b^{âˆ’1} âˆ— a$ $a âˆ— b^{âˆ’1} âˆ— b^{âˆ’1} âˆ— b^{âˆ’1} âˆ— a$     $\qquad$  { as $a^2=I$ given } but from here onwards I am not getting exact steps to solve it. Does anyone have an idea?","['group-theory', 'discrete-mathematics']"
1510457,Partial derivatives bounded implies continuity,"Suppose that $f$ is a real-valued function defined in an open set $E \subset \Bbb R^n$, and that the partial derivatives $D_1f, \ldots D_nf$ are bounded in $E$. Prove that $f$ is continuous in $E$. So if $\textbf{x} = (x_1, x_2, x_3, ... , x_n)$ and $\textbf{y} = (y_1, y_2, y_3, ... , y_n)$, then we have to show that for all $\epsilon >0$, there exists a $\delta>0$ such that $d(\textbf{x}, \textbf{y}) < \delta \implies d(f(\textbf{x}), f(\textbf{y})) < \epsilon$. I am reading a solution here and it says we can write $f(x_1 + h_1, x_2 + h_2, x_3 + h_3, ... , x_n + h_n) - f(x_1, x_2, x_3, ... , x_n)$ as: $f(x_1 + h_1, x_2 + h_2, x_3 + h_3, ... , x_n + h_n) - f(x_1, x_2 + h_2, x_3 + h_3, ... , x_n + h_n) + f(x_1, x_2 + h_2, x_3 + h_3, ... , x_n + h_n) - f(x_1, x_2, x_3 + h_3, ... , x_n + h_n) + \ldots + f(x_1, x_2, x_3, ... ,x_n + h_n) - f(x_1, x_2, x_3, ... , x_n)$ and then use the mean value theorem to get: $D_1(x_1 + h_1t_1, x_2 + h_2, x_3 + h_3, ... , x_n + h_n)h_1$ + $D_2(x_1, x_2 + h_2t_2, x_3 + h_3, ... , x_n + h_n)h_2 + \ldots D_n(x_1, x_2, x_3, ... ,x_n + h_nt_n)h_n$ Since each $D_n$ is bounded, take the maximum of these bounds, call it $M$. Then we have that the expression directly above this sentence is $\leq M(|h_1| + |h_2| + \ldots + |h_n|)$, so: $f(x_1 + h_1, x_2 + h_2, x_3 + h_3, ... , x_n + h_n) - f(x_1, x_2, x_3, ... , x_n) \leq M(|h_1| + |h_2| + \ldots + |h_n|)$ Then the proof just stops there and doesn't continue. I don't understand what the $h_n$'s are supposed to represent. Are they real numbers? If so, why are we adding an arbitrary vector $\textbf{h}$ to $\textbf{x}$? After we get $f(x_1 + h_1, x_2 + h_2, x_3 + h_3, ... , x_n + h_n) - f(x_1, x_2, x_3, ... , x_n) \leq M(|h_1| + |h_2| + \ldots + |h_n|)$ , from this how do we show that for all $\epsilon >0$, there exists a $\delta>0$ such that $d(\textbf{x}, \textbf{y}) < \delta \implies d(f(\textbf{x}), f(\textbf{y})) < \epsilon$","['partial-derivative', 'continuity', 'real-analysis', 'lipschitz-functions', 'analysis']"
1510467,Select cards from the Card Deck,"How many ways are there to pick 2 different cards from a standard 52 cards deck such that the first card is a spade and the second card is not a Queen? Answer: Case I) First card is not Queen of spade and the other card is any card but not queen.
13-1 * (52-1-4) = 12 *47 Case II) First card is Queen of spade  and the other card is any card but not queen.
1 * (52-4) = 1 * 48 Total ways are: 12*47 + 48 ?? Is this correct?","['combinations', 'discrete-mathematics', 'combinatorics']"
1510480,MLE of Î² in the gamma distribution?,"So I have the pdf for the gamma distribution, $$f(x) = \frac{1}{\Gamma(\alpha)} \beta^\alpha x^{\alpha - 1} e^{-\beta x} $$ and I'm having trouble getting to the MLE of $\beta$, which should be $\frac{\alpha}{\overline{x}}$.  I keep messing up when it comes to taking the log but I'm not sure where.  Thanks!","['estimation', 'probability', 'statistics']"
1510491,Volumes + Surface Areas of Sphere with Calculus: Paradox?,"I have an interesting observation, what appears to be an apparant contradiction. We calculate volume of sphere, we get it as $$\int_{-R}^R \pi (\sqrt{R^2-y^2})^2 dy$$ because we see it as, circular cylinder with height dy. We calculate surface area of sphere, we get it as $$\int_{-R}^R 2*\pi \sqrt{R^2-y^2} dy$$ because we see it as, circular cylinder outside with height dy. First integral, well I guess it is right. $\frac{4}{3} \pi R^3$. No problem there. But then, you look the second integral, you get, $\pi^2 R^2$! What is discrepancy here, and what is wrong.","['volume', 'calculus']"
1510535,Concentration inequalities for $P(\sum_{i=1}^n \epsilon_i X_i > t)$,"Let $\epsilon_i \sim \text{Bernoulli}(p)$ and $X_i \sim \text{Normal}(0, \sigma^2 / n)$ for $i=1,\ldots,n$. I am interested in getting a sub-Gaussian type upper bound for
$$
P\left(\sum_i \epsilon_i X_i > t\right). 
$$
Ideally, it would look something like 
\begin{align}
P\left(\sum_i \epsilon_i X_i > t\right) \le K\exp\left(C\frac{-t^2}{2\sigma^2 p}\right) \tag{1}
\end{align}
for some constants $K$ and $C$, although I'm thinking this isn't true; I'm also not sure how to incorporate $n$ here into $K$ or $C$. It's important that the bound quantifies the deviation in terms of $p$, so that I have good control over what is going on as $p \to 0$. The furthest I've gotten is that, by iterated expectation and the usual Gaussian concentration stuff,
\begin{align}
P\left(\sum_i \epsilon_i X_i > t\right)
\le E\left\{\exp\left(\frac{-t^2 n}{2\sigma^2 \sum_i \epsilon_i}\right)\right\} \tag{2}
\end{align}
which, for fixed $p$, gives 
$$
\limsup_n P\left(\sum_i \epsilon_i X_i > t\right)
\le \exp\left(\frac{-t^2}{2\sigma^2 p}\right)
$$
by the law of large numbers. But this approach doesn't seem strong enough, since a little bit of numeric investigation shows that (2) is quite a bit worse than (1) as $p \to 0$. Some quantification of how $p$ and $n$ play into the bound (2) would also be good.","['probability-theory', 'concentration-of-measure', 'probability']"
1510549,What makes us say that Pythagoras theorem can be used in higher dimensions too?,"Pythagoras theorem seems to be a geometric property of our Universe. It's a property that helps us find the distances between two points in coordinate geometry in one dimension, two dimensions and three dimensions. 
But what makes us comment that this geometrical property can too be used in higher dimensions too.",['geometry']
1510569,$C(K)$ is reflexive if and only if $K$ is finite,"Let $C(K)$ be the set of all continuous complex valued functions on a compact Hausdorff space $K$ . Is it true that $K$ must be finite if $C(K)$ is reflexive?
To me it seems true, but I don't know how to prove it. As $C(K)$ is reflexive then we have canonical isometry onto $C(K)^{**}$ . How does that help?","['reflexive-space', 'continuity', 'real-analysis', 'functional-analysis', 'banach-spaces']"
1510578,"Find polynomials $f(x), g(x)$, and $h(x)$","Find polynomials $f(x), g(x)$, and $h(x)$, if they exist, such that
  for all $x$,   $$\mid f(x)\mid-\mid g(x) \mid+h(x)=   \begin{cases}  
 -1, & \text{if}~x<-1 \\   3x+2, & \text{if}~-1\leq x\leq 0 \\  
-2x+2, & \text{if}~x>0\\ \end{cases}$$ The solution are $f(x) := 3(x+1)/2$ , $g(x) := 5x/2$ and $h(x) := (1 â€“ 2x)/2$ meet the requirements. Is anyone can explain to me why a particular solution can be determined most easily by solving three linear equations:
$â€“f + g + h = â€“1 $, $f + g + h = 3x + 2$ , $f â€“ g + h = â€“2x + 2$ ? Namely it is a question of PUTNAM 1999.","['contest-math', 'discrete-mathematics']"
1510629,Polar coordinates for double integral for $\theta$,"To evaluate the integral $$\iint_D \sqrt{x^2+y^2}dA$$
$$D=\{(x,y)\mid0\leq(x-1)^2+y^2\leq1 \}$$
it should be best to change variables into polar coordinates to get $$\int_{\frac{-\pi}{2}}^{\frac{\pi}{2}}\int_0^{2cos\theta} r^2drd\theta$$ Every problem like the above I have seen are done by integrating with respect to $r$ then $\theta$. Most often these integrals are done over a circle centered at the origin so the limits are constants and the order of integration is changeable through Fubini's theorem. I have not yet seen one where the limits are functions of r and the integral is first done with respect to $\theta$. If I decided to switch the the order of integration of the previous integral would I end up with the following integral? $$\int_{0}^{2}\int_{-cos^{-1}\frac{r}{2}}^{cos^{-1}\frac{r}{2}} r^2 d\theta dr$$ This integral seems far worse than the previous, so I also ask how often would switching the polar bounds be beneficial? I would assume this should only be done if $\theta=f(r)$. Is that correct? Any other information or resources on integrating with respect to $\theta$ would also be appreciated.","['polar-coordinates', 'multivariable-calculus', 'definite-integrals', 'integration']"
1510651,On the fourth power $2^4 + 15045^4 + 26870^4 + 34090^4 = 37239^4$,"The Diophantine equation, $$x_1^4+x_2^4+x_3^4+x_4^4 = z^4\tag1$$ can be solved with $x_1 = 0$ (Elkies), or as the title shows, with $x_1 = 2$ (Wroblewski). See link here . Q: Can $(1)$ be solved in the integers with $x_1 = 1$ , $$1+x_2^4+x_3^4+x_4^4 = z^4\tag2$$ or is there a congruential constraint that prevents this? P.S. Anyone has the computing power to extend Wroblewski's table? He created that back in 2009 (with slower computers). One helpful fact is that all primitive solutions have the form, $$x_0^4+5^4(x_1^4+x_2^4+x_3^4) = y_1^4$$ or three terms are always multiples of $5$ .","['number-theory', 'diophantine-equations']"
1510682,Show sets are not homeomorphic,"Consider the following sets: 
$$A = \big \{ (x, 0) : -1 \leq x \leq 1 \big \} \cup \big \{ (0,y): 0 \leq y \leq 1 \big \}$$
$$B = \big \{(x, y) : x^2 + y^2 = 1 \} \cup \big \{ (x, 0) : 1 \leq x \leq 2 \big \}$$
$C = [0, 1]$ (line segment), $D = \mathbb{S}^1$ (circle). Show that these sets are not homeomorphic to each other. I have already shown that $C$ and $D$ are not homeomorphic. I am stuck on showing that $A$ is not homeomorphic to $B$ and that $B$ is not homeomorphic to $C.$ I am trying to apply a proposition to prove these spaces are not homeomorphic: Let $X$ and $Y$ be topological spaces. Suppose there is $x \in X$ such that $X\setminus\{x\}$ has $m$ connected components. If there is no $y \in Y$ such that $Y\setminus\{y\}$ has $m$ connected components, then $X$ and $Y$ are not homeomorphic. but I am having so much trouble. Any help gladly accepted!","['elementary-set-theory', 'general-topology']"
1510698,How do we find Associated primes Geometrically?,"I'm reading about primary decomposition from Atiyah and Macdonald's book. One of my friend told me that it is possible to find Associated primes and Primary decomposition geometrically. I tried to found the geometric way in Atiyah and Macdonald's book, but it is not explained in the book. Can someone explain for the following example: Let $I=(x^2+xy,y^2-1) \subset k[x,y]$. Find Associated primes and Primary decomposition geometrically.","['algebraic-geometry', 'maximal-and-prime-ideals', 'commutative-algebra']"
1510756,Solve for $x$: $x\lfloor x + 2 \rfloor +\lfloor 2x - 2 \rfloor +3x =12$,Solve for $x$: $$x\lfloor x + 2 \rfloor +\lfloor 2x - 2 \rfloor +3x =12$$ My attempt: I have changed this equation into the fractional part function. so that we know $0 \leq \{x\}<1$. I have final equation $x^2 +7x-x\{x+2\}-\{2x-2\}=14$. How to proceed now?,"['ceiling-and-floor-functions', 'algebra-precalculus']"
