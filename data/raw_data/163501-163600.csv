question_id,title,body,tags
2843178,Proof of theorem egregium with moving frames,"I'm learning the moving frame approach (with differential form) in surface theory from different books and papers (Cartan, O'Neill, Shifrin, Flanders and others). Briefly: you define an adapted moving frame on the surface $(P, e_1, e_2, e_3)$, then you define the dual forms $\omega_i = dP\cdot e_i$ and the connection forms $\omega_{ij} = de_i \cdot e_j$. From Cartan's structure equations you find Gauss and Codazzi equations and other things. Next you prove the fundamental formula $d\omega_{12} = -Kd\sigma$ (where $K = detS$ is the gaussian curvature). Here in some texts follows two steps before arrive at theorem egregium: 1) You prove that if you consider another frame $(P, \bar e_1, \bar e_2, \bar e_3$) than $d\omega_{12} = d \bar \omega_{12}$ 2) You prove that $\omega_{12}$ is the only 1-form which satisfy the two equations $d\omega_1 = -\omega_2 \wedge \omega_{12}$ and $d\omega_2 = \omega_1 \wedge \omega_{12}$ and so it is intrinsic Are these steps necessary? I mean 1) It is not obvious that $d\omega_{12}$ does not depend on the frame since neither $K$ nor $d\sigma$ depend on the frame? 2) It is not obvious that $\omega_{12}$ is intrinsic since it's defined as $de_1\cdot e_2$? Thanks in advance.","['differential-forms', 'differential-geometry']"
2843185,Can directional derivatives be written as linear combination of partial derivatives even if f is not differentiable?,"Given $f:\mathbb{R}^n\to\mathbb{R}$ and $\{v_1,...,v_n\}$ linearly independent vectors such that $\frac{\partial f}{\partial v_i}$ exists. I know if f is differentiable then $\frac{\partial f}{\partial v_i}$=$\nabla f\cdot v$ so the directional derivative of f can be expressed as a linear combination of the partial derivatives. If f is not differentiable: 1) $\nabla f$ exists ? 2) If $\nabla f$ exists ,can we express the directional derivatives as a linear combination of the partial derivatives?","['multivariable-calculus', 'partial-derivative', 'calculus']"
2843198,When is the index of minimal submanifold finite?,"Let $(M^n,g)$ be a Riemannian manifold and $f:\Sigma^k\to M^n$ a minimal submanifold. The Morse index of $\Sigma$ is the number of negative eigenvalues of the stability operator $L:\Gamma(N\Sigma)\to \Gamma(N\Sigma)$ acting on (smooth) sections of the normal bundle $N\Sigma$ of $\Sigma$ in $M$. The Morse index of $\Sigma$ need not be finite in general, however, under certain conditions it is. For example: the classical Morse index theorem says that if $\gamma:[0,1]\to (M^n,g)$ is a geodesic, then the Morse index of $\gamma$ is finite and equal to the number of points on $\gamma$ which are conjugate to $\gamma(0)$ (counted with multiplicity). Another example: a complete oriented minimal surface in $(\mathbb{R}^n,\delta)$ with finite total curvature has finite morse index . Question: Suppose $\Sigma^{n-1}\subset (M^n,g)$ is a closed (compact without boundary) minimal hypersurface. Is the Morse index of $\Sigma$ finite? If this is not true in general, which extra assumptions are sufficient to ensure the Morse index is finite?","['calculus-of-variations', 'minimal-surfaces', 'differential-geometry', 'partial-differential-equations']"
2843201,Integral of impulse-train,"Can someone check if I have this right please? (Or point out where I'm wrong!) I have a (real) impulse-train with maximum amplitude $1$ and period $R+1$, given by $$s(x)=\frac {1}{R+1}+\sum_{k=1}^R\frac {\cos \bigl(\frac {2k\pi x}{R+1}\bigr)}{R+1}$$ The indefinite integral is (I think) $$\int s(x) dx=\frac {x}{R+1}+\sum_{k=1}^R\frac {\sin \bigl(\frac {2k\pi x}{R+1}\bigr)}{2k\pi}+C$$ I am looking for the definite integral over the range $0$ to $R+1$, given by $$\int_0^{R+1} s(x) dx=\int s(R+1) dx-\int s(0) dx$$ $$=\frac {R+1}{R+1}+\sum_{k=1}^R\frac {\sin \bigl(\frac {2k\pi (R+1)}{R+1}\bigr)}{2k\pi}-\frac {0}{R+1}-\sum_{k=1}^R\frac {\sin \bigl(\frac {0}{R+1}\bigr)}{2k\pi}$$ $$=1+\sum_{k=1}^R\frac {\sin 2k\pi}{2k\pi}$$ $$=1+\sum_{k=1}^R\operatorname{sinc}2k\pi$$ $$=1$$ (Because $\operatorname{sinc}2k\pi=0$ for all integer values of $k$.) Could someone please confirm if I have this right, or point out where the error is?","['fourier-series', 'trigonometry', 'integration', 'definite-integrals', 'sequences-and-series']"
2843214,Are omega limit points always in the domain of definition of the flow?,"Definition of an $\omega$-limit point: Let $\phi_t(p)$ be the orbit of the solution of the ODE $\dot{x}=f(x)$ which passes through the point $p$. We know that a point $x$ in $\mathbb{ R}^n$ is called an $\omega$-limit point of the orbit through the point $p$ if there is a sequence of numbers $t_1 \le t_2 \le t_3 \le · · ·$ such that $\lim_{i \to \infty } t_i = \infty $ and $\lim_{i\to \infty} \phi_{t_i}(p) = x$. From this definition, first and foremost, we understand that an $\omega$-limit point is ""the limit point of a subsequence"" constructed on the orbit. Obviously, this limit point may not be on the same orbit. I appreciate it if someone can answer the following questions: Q1- Is that possible that an $\omega$-limit point is not on any orbit (i.e., it is not in the domain of definition of the solution (flow))? I guess that if the solution is forward complete (i.e., defined for all time $t\in(0,+\infty)$), then all the omega limit points are always in the domain of definition of the solution. Q2- How should we define the $\omega$-limit point for the case when $t$ cannot go to $\infty$, i.e., the maximal interval of existence of the solution (i.e., domain of $t$) is $(-\infty, a)$ where $a<\infty$?","['control-theory', 'manifolds', 'ordinary-differential-equations', 'differential-geometry', 'analysis']"
2843219,Triple integral with multiple y boundaries.,"Problem : I am trying to calculate the mass for the solid bounded by $z = y^5 + 1, \ z = 0, \ y = x, \ y = x^2, \ y = 1.$ The density at each point is directly proportional to the square of the distance from the yz-plane. I split the integral to simplify the calculation: Assuming $\rho=kx^2$, $k \cdot \int_{-1}^0 \int_{x^2}^1 \int_0^{y^5 + 1} x^2 dz \ dy \ dx + k \cdot \int_{0}^1 \int_x^{1} \int_0^{y^5 + 1} x^2 dz \ dy \ dx$ I am wondering if there is an easier way to set up the triple integral and if my limits of integration are correct?","['multivariable-calculus', 'multiple-integral', 'integration']"
2843242,Justify why there are no non-constant periodic solutions,"I'm probably missing something stupid here, regarding periodic solutions to ODE's. The ode is $$\dot{x}=-\nabla f$$ I can see that a solution will require $\frac{df}{dt}=-\|\nabla f\|^2$. Buy why does this prevent existence of such solutions? More generally, why are we interested in the behavior of $f$ rather than $x$?",['ordinary-differential-equations']
2843269,All relationships between angles formed by drawing diagonals of a quadrilateral,"Draw a convex nondegenerate quadrilateral $ABCD$ and draw in the diagonals $AC,BD$. Write $O$ for the meeting point of the diagonals. There are twelve angles you could label in the picture using $A,B,C,D,O$ that aren't identically $\pi$. Four relationships such as $\angle OAB +\angle ABO +\angle BOA = \pi $ hold by setting $A,B$ to be any two adjacent vertices. Two relationships of the form $\angle AOB = \angle COD$ hold by cyclically permuting $A,B,C,D$. The relationship $\angle AOB + \angle BOC = \pi$ also holds. This gives 5 degrees of freedom among choosing these angles when there should be 4 (the space of all convex quadrilaterals up to similarity is 4-dimensional, and certainly all of these angles together determine a quadrilateral up to similarity). Where's the missing relationship? Is it easy to see what it ought to be? Is it a linear relationship, like the others?","['recreational-mathematics', 'euclidean-geometry', 'quadrilateral', 'geometry']"
2843280,"Topology of ""degenerate spectrum submanifold"" in the space of hermitian matrices","Consider the space $H_n$ of hermitian matrices acting on $\mathbb C^n$ . It contains a subset $LC_n$ of matrices with degenerate spectrum. I want to know as much as possible about topology and geometry of this set and its complement. In particular is $LC_n$ a submanifold? I suspect that it could, and its codimension is 3.  Can we calculate the cohomology ring and some homotopy groups of $LC_n$ and $H_n \setminus LC_n$ ? $H_n \setminus LC_n$ is an open subset of $H_n$ , hence submanifold. Moreover it is dense and connected. All these properties are easily seen by considering decomposition $T=U^{\dagger}DU$ with $D$ - diagonal and $U$ unitary. Let $\mathcal E_n$ be the space of increasing $n$ -tuples of real numbers and let $\mathcal F_n$ be the complete flag variety, $\mathcal F_n = \frac{U(n)}{U(1)^n}$ . Then $H_n \setminus LC_n$ is diffeomorphic to the cartesian product $\mathcal E_n \times \mathcal F_n$ . Once again, this is is easy to see using the $U^{\dagger}DU$ decomposition. Elements of $\mathcal E_n$ are the eigenvalues, while elements of $\mathcal F_n$ are the eigenspaces. Since $\mathcal E_n \cong \mathbb R^n$ , we see that $H^{\bullet}(H_n \setminus LC_n) \cong H^{\bullet} (\mathcal F_n)$ . This cohomology group is computed on Wikipedia in the article on generalized flag varieties. I think this description of the topology of $H_n \setminus LC_n$ is pretty complete and satisfying. However I am also interested in $LC_n$ itself. Let $\chi_T$ be the characteristic polynomial of $T$ . Operator $T$ is in $LC_n$ if and only if $\chi_T$ has a double zero. This is equivalent to vanishing of the discriminant $\Delta$ of $\chi_T$ , which is easily seen to be a polynomial in the matrix elements of $T$ . Thus $LC_n$ is an algebraic variety in $H_n \cong \mathbb R^{n^2}$ . For any $n$ there exist points of $LC_n$ on which the first derivative of $\Delta$ vanishes. Thus it's impossible to conclude that $LC_n$ is a submanifold using implicit function theorem. My conjecture about codimension $3$ is based on the analysis of oribts of $U(n)$ acting on $H_n$ . Namely we need to tune one real number parametrizing $T$ to make it degenerate, but then dimension of the stabilizer of $T$ in $U(n)$ (acting by conjugation) becomes larger at lest by $2$ . More precisely, if $T$ has $k$ distinct eigenvalues with dimensions of eigenspaces $g_1,...,g_k$ , then $\mathrm{Stab}(T) \cong U(g_1) \times ... \times U(g_k)$ . $0$ is in $LC_n$ and $\lambda T \in LC_n$ whenever $T \in LC_n$ and $\lambda \in \mathbb R$ . In particular $LC_n$ is contractible to a point. Clearly the ""correct"" way of studying the geometry of $LC_n$ would be to consider it as a projective variety in $\mathbb P \mathbb R^{n^2-1}$ . In fact $\Delta$ is a homogeneous polynomial of degree $n(n-1)$ .","['algebraic-geometry', 'algebraic-topology', 'algebraic-groups', 'differential-geometry', 'linear-algebra']"
2843297,How to solve this first-order nonlinear differential equation?,"1) I'm wondering how one might solve this differential equation: (here $c$ is a real constant) $$ c = y - \frac{x}{2} \left( y' - \frac{1}{y'}\right).$$ WolframAlpha says it is $y = \frac{1}{2}\left( 2c \pm e^{-k}x^2 \mp e^{k}\right)$, and I expected to find an $x^2$ in there based off of where the equation comes from, but I'm not super experienced with nonlinear DE's. ( For those that are curious: lets say that a ray comes from above $x_0$ parallel to the $y$-axis, hits the graph of $y = f(x)$, and the angle between the ray and the tangent line at $x_0$ was equal to the angle between the reflected line and the tangent line---as if the ray were a pool ball and the tangent line were the side of the table. Then the reflected line has a $y$-intercept of $c$. ) 2) I'm curious if anyone might have some advice for solving differential equations of this form? $$ g(x) = y - \frac{x}{2} \left( y' - \frac{1}{y'}\right).$$ Thank you!",['ordinary-differential-equations']
2843301,Rational rotational algebra (noncommutative torus) is not simple,"I would like to show that the rational rotational algebra $A_\theta$ is not simple where $A_\theta= C^{*}(u,v : u,v$ are unitaries and $uv=e^{i2\pi\theta}vu$) and $\theta$ is a rational number. The hint is given that I need to show the existence of unital $*$-homomorphisms $\phi : A_\theta → B $ and $\psi : A_\theta  → D $ such that $\phi (v^{q})=1$ and $\psi(v^{q})\neq 1$. And I do not know how to show the existences of such homomorphisms. Any help would be appreciated.","['functional-analysis', 'c-star-algebras', 'operator-algebras', 'universal-algebra']"
2843307,Getting consistent normals along a 3D (Bezier) curve,"I'm trying to get consistent normals along a 3D Bezier curve $B(t)$ , where for any point I compute the normal as: $$
\begin{align}
\vec{a} &= B'(t) \\
\vec{b} &= B''(t) \\
\vec{c} &= \vec{a} + \vec{b} \\
\vec{r} &= \vec{c} × \vec{a} \\
\vec{n} &= \vec{r} × \vec{a} \\
\end{align}
$$ So, get the derivative at a point for time value $t$ , and implicitly get the plane of curvature at the point by computing the cross product of the derivative vector at the point, and the ""next"" derivative vector we get from moving the derivative by the amount dictated by the second derivative. The cross product yields the axis of rotation, so to then form the normal at the point for time value $t$ I take the cross product of the axis of rotation, and the original derivative vector, since these three vectors are by definition perpendicular. The problem is that normals computed this way are not consistent: they will ""flip"" around inflections, and I'm not sure what the right way is to go about making sure that does not happen. As visual illustration, consider the following 3D cubic Bezier curve: $$
B(t) =
\left[\begin{matrix}1&t&t^2&t^3\end{matrix}\right]
\left[\begin{matrix}1&0&0&0\\-3&3&0&0\\3&-6&3&0\\-1&3&-3&1\end{matrix}\right]
\left[\begin{matrix}
0 & 0 & 0\\
-0.38 & 2.68 & 0\\
-0.25 & 5.41 & 0\\
-0.15 & 8.21 & 0
\end{matrix}\right]
$$ Now, this happens to be a 3D curve that lies entirely on the x/y plane, but it illustrates the problem rather well. The above procedure yields the following normals: However, this is rather different from the 2D normals we get when taking advantage of the 2D plane, where a normal can be constructed by simply rotating the (normalised) derivative vector a quarter turn clockwise, setting $(x,y)$ as $(-y,x)$ : I'd like to get something similar to the 2D case for the 3D case, but I don't know how to ensure that the cross products are unaffected by ""which direction"" the second derivative moves the derivative across its plane of curvature (Effectively, how do I ensure that, when considering the triplet {normal,derivative,axis of rotation} that these always map to the local {x,y,z} axes, rather than sometimes mapping to {x,y,z} and somethings mapping to {y,x,z} axes) Edit While more ""algorithmic"" than I'd like, the only workable solution I've found so far is to compute the normals for two points $B(t)$ and $B(t+\varepsilon )$ , then computing the angular difference in the plane for those two normals, $$
\theta = \textit{acos} \left ( \frac{n_1 \cdot n_2 }{||n_1|| ||n_2||} \right )
$$ and then check whether that value is close to $\pi$ or not. Even in fast-changing curves, the angle between two ""reasonable"" normals is a relatively small value, so if the angle suddenly flips to ""nearly $\pi$ "" then as of that time value the ""desired normals"" are negative actual normal. While that works, it feels kind of hacky. Without algorithmic flipping: With algorithmic flipping: Note this does not affect cuves with ""reasonable twisting"", e.g. when we set the $z$ values to $\{0,200,-200,600\}$ for the first, second, third and fourth control point respectively:","['bezier-curve', 'linear-algebra', 'geometry']"
2843337,"Trig function bounded on interval (without calculus), prove that $x^{3/2}\sin x + \sqrt{9-x^3}\cos x \leq 3$.","If $0 \lt x \lt \dfrac{\pi}{2}$, prove that 
  $$x^{3/2}\sin x + \sqrt{9-x^3}\cos x \leq 3$$ This question must be done without calculus. First, I tried splitting it into the intervals $(0,\pi/4)$ and $(\pi/4, \pi/2)$, hoping that, $\sin x$ was bound tightly enough on the interval that it'd be less than 3 even if $\cos x = 1$ (which doesn't work -- letting $\sin x = \dfrac{1}{\sqrt{2}}$ and $\cos x = 1$ produces a result greater than 3). The other thing I noticed was that inside the square root sign, we have $\sqrt{9-x^3} = \sqrt{(3-x^{3/2})(3+x^{3/2})}$, and an $x^{3/2}$ appears in the first term, but I'm not sure how useful the similarity there is. Advice on how to proceed?","['inequality', 'trigonometry', 'functional-inequalities']"
2843350,Kata's for Statistics,"In programming one way at getting better at common problems is solving them in different ways with exercises known as 'kata'. I was wondering, since I'm long out of school, are there available exercises for statistics to increase ones math ability outside of just doing the same problems over and over again out of a textbook?",['statistics']
2843386,Upper triangular matrices $B$ that commute with every upper triangular matrix commuting with $A$,"I remember being told that this was true by a professor, but I haven't been able to find a source for it yet. In the theorem as stated, $\mathbb{F}$ is any field and $T_n(\mathbb{F})$ denotes the algebra of upper triangular $n\times n$ matrices over $\mathbb{F}$. Theorem: Let $A,B\in T_n(\mathbb{F})$ be such that for all $X\in T_n(\mathbb{F})$, $$AX=XA\implies BX=XB$$ Then $B=p(A)$ for some $p\in \mathbb{F}[t]$. If we replace $T_n(\mathbb{F})$ by $M_n(\mathbb{F})$, the question is answered in this paper . Unfortunately, the argument doesn't seem to translate directly, as I can't find a way to force the $M_i$ maps to be upper-triangular. Update: I have re-asked the question here on MO. Thanks to David E Speyer, we now know that this theorem is false. In particular, if 
$$A=\left[\begin{array}{cccc}0&0&0&1\\ &0&1&0\\ & &0&0\\ & & & 0\end{array}\right]$$
then the matrices commuting with $A$ are those of the form
$$\left[\begin{array}{cccc}a&0&*&*\\ &b&*&*\\ & &b&0\\ & & & a\end{array}\right]$$
The matrix
$$B=\left[\begin{array}{cccc}0&0&0&1\\ &0&0&0\\ & &0&0\\ & & & 0\end{array}\right]$$
commutes with all matrices of this form, but is clearly not a polynomial in $A$.","['reference-request', 'linear-algebra']"
2843463,Evaluate: $\frac{1}{(1+1)!} + \frac{2}{(2+1)!}+...+\frac{n}{(n+1)!}$ using combinatorics.,Evaluate    $\frac{1}{(1+1)!} + \frac{2}{(2+1)!}+...+\frac{n}{(n+1)!}$.  This is from a combinatorics textbook so I'd like a combinatorial proof.  I find doing this kind of problem difficult especially when you have to sum - I don't know how to construct a sensible analogy using the addition principle. Similar question that appears just before this question in the text: Combinatorics problem involving series summation,['combinatorics']
2843465,Please help me with this trigonometric limit without using L'Hopital's rule,"I need to solve the following limit without using L'Hopital's rule: $$\lim _{x\to 0}\left(1+\sin\left(x\right)\right)^{\frac{1}{x}}$$ The thing is that I can not figure out what to do. One of my ideas was to apply this rule: $a^x=e^{\ln \left(a^x\right)}=e^{x\cdot \ln \left(a\right)}$, getting this: $$\lim _{x\to 0}e^{\frac{1}{x}\ln \left(1+\sin \left(x\right)\right)}$$ I already know that the answer is $e$, so the exponent is definitely 1. However, I tried everything I could but have no idea how to solve $\lim _{x\to 0}\left(\frac{1}{x}\ln \left(1+\sin \left(x\right)\right)\right)$ which needs to be 1. I would  really appreciate your help, and if you find a totally different way to solve the limit without using L'Hospital's rule it will be good as well.","['trigonometry', 'limits-without-lhopital', 'limits']"
2843505,Derivative of Softmax without cross entropy,"There are several resources that show how to find the derivatives of the softmax + cross_entropy loss together. However, I want to derive the derivatives separately. For the purposes of this question, I will use a fixed input vector containing 4 values. Input vector $$\left [ x_{0}, \quad x_{1}, \quad x_{2}, \quad x_{3}\right ]$$ Softmax Function and Derivative My softmax function is defined as : $$\left [ \frac{e^{x_{0}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}}, \quad \frac{e^{x_{1}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}}, \quad \frac{e^{x_{2}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}}, \quad \frac{e^{x_{3}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}}\right ] $$ Since each element in the vector depends on all the values of the input vector, it makes sense that the gradients for each output element will contain some expression that contains all the input values. My jacobian is this: $$
\left[\begin{matrix}\frac{e^{x_{0}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}} - \frac{e^{2 x_{0}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}} & - \frac{e^{x_{0}} e^{x_{1}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}} & - \frac{e^{x_{0}} e^{x_{2}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}} & - \frac{e^{x_{0}} e^{x_{3}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}}\\- \frac{e^{x_{0}} e^{x_{1}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}} & \frac{e^{x_{1}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}} - \frac{e^{2 x_{1}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}} & - \frac{e^{x_{1}} e^{x_{2}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}} & - \frac{e^{x_{1}} e^{x_{3}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}}\\- \frac{e^{x_{0}} e^{x_{2}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}} & - \frac{e^{x_{1}} e^{x_{2}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}} & \frac{e^{x_{2}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}} - \frac{e^{2 x_{2}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}} & - \frac{e^{x_{2}} e^{x_{3}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}}\\- \frac{e^{x_{0}} e^{x_{3}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}} & - \frac{e^{x_{1}} e^{x_{3}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}} & - \frac{e^{x_{2}} e^{x_{3}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}} & \frac{e^{x_{3}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}} - \frac{e^{2 x_{3}}}{\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}\right)^{2}}\end{matrix}\right]
$$ Each row contains the contribution from each output element. To calculate the 'final' derivative of each node , I sum up all the elements in each row, to get a vector which is the same size as my input vector. Due to numerical stability issues, summing up the values gives unstable results. 
However, it is quite easy to reduce the sum of each row to this expression: Notice that except the first term (the only term that is positive) in each row, summing all the negative terms is equivalent to doing: $$\sum_{i}{} softmax_{x_0} * softmax_{x_i} $$ and the first term is just $$ softmax_{x_0} $$ Which means the derivative of softmax is : $$softmax - softmax^2$$ or $$softmax(1-softmax)$$ This seems correct, and Geoff Hinton's video (at time 4:07) has this same solution. This answer also seems to get to the same equation as me. Cross Entropy Loss and its derivative The cross entropy takes in as input the softmax vector and a 'target' probability distribution. $$\left [ t_{0}, \quad t_{1}, \quad t_{2}, \quad t_{3}\right ]$$ Let the softmax index at i be denoted as $s_i$ 
So the full softmax vector is : $$\left [ s_{0}, \quad s_{1}, \quad s_{2}, \quad s_{3}\right ]$$ Cross entropy function $$
- \sum_{i}^{classes} t_i log(s_i)
$$ For our case it is $$
- t_{0} \log{\left (\frac{e^{x_{0}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}} \right )} - t_{1} \log{\left (\frac{e^{x_{1}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}} \right )} - t_{2} \log{\left (\frac{e^{x_{2}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}} \right )} - t_{3} \log{\left (\frac{e^{x_{3}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}}} \right )}
$$ Derivative of cross entropy Using the simple multiplication rule along with the log rule, the derivative of cross entropy is: $$
-\frac{t_i}{s_i}
$$ Using chain rule to get derivative of softmax with cross entropy We can just multiply the cross entropy derivative (which calculates Loss with respect to softmax output) with the softmax derivative (which calculates Softmax with respect to input) to get: $$
-\frac{t_i}{s_i} * s_i(1-s_i)
$$ Simplifying , it gives $$
-t_i *(1-s_i)
$$ Analytically computing derivative of softmax with cross entropy This document derives the derivative of softmax with cross entropy and it gets: $$
s_i - t_i
$$ Which is different from the one derived using chain rule. Implementation using numpy I thought perhaps both the derivatives would evaluate to the same result, and I had missed some simplification that could be applied using assumptions (e.g. probability distributions sum up to 1) This is the code to evaluate: x = np.array([-1.0, -1.0, 1.0])                 # unscaled logits, my x vector
t = np.array([0.0,1.0,0.0])                     # target probability distribution


## Function definitions

def softmax(v):
    exps = np.exp(v)
    sum  = np.sum(exps)
    return exps/sum

def cross_entropy(inps,targets):
    return np.sum(-targets*np.log(inps))

def cross_entropy_derivatives(inps,targets):
    return -targets/inps

def softmax_derivatives(softmax):
    return softmax  * (1-softmax)


soft = softmax(v)                               # [0.10650698, 0.10650698, 0.78698604]

cross_entropy(soft,t)                           # 2.2395447662218846

cross_der = cross_entropy_derivatives(soft,t)   # [-0.       , -9.3890561, -0.       ]

soft_der = softmax_derivatives(soft)            # [0.09516324, 0.09516324, 0.16763901]

## Derivative using chain rule  
cross_der * soft_der                            # [-0.        , -0.89349302, -0.        ]


## Derivative using analytical derivation 

soft - t                                        # [ 0.10650698, -0.89349302,  0.78698604] Notice the difference in values. My question, to clarify, is, what is the mistake that I am making. These two values should be quite similar.","['multivariable-calculus', 'neural-networks', 'calculus']"
2843514,Spivak Calculus on Manifolds - Problem 3-21 Help with details of proof,"3-21) ""Let $A \subseteq \mathbb{R}^n$ be a closed rectangle, and $C \subseteq A$. Show that $C$ is Jordan measurable if and only if for every $\varepsilon > 0$, there is a partition $P$ of $A$ such that $$\sum_{S \in \sigma_1}v(S) - \sum_{S \in \sigma_2}v(S) < \varepsilon$$ where $\sigma_1$ is the collection of subrectangles $S$ determined by the partition $P$ such that they intersect $C$, and $\sigma_2$ consists of those which are contained in $C$."" So I think I have an outline of a solution but there are a few steps which I can't justify even though they seem to be true intuitively. proof of $\implies$ direction: Suppose $C$ is Jordan measurable and let $\varepsilon > 0$ be given. Then we can choose a collection of closed rectangles $\{U_i\}_{i \in \mathbb{N}}$ to cover the boundary of $C$ such that $\sum_{i=1}^{\infty} v(U_i) < \varepsilon$. Since bd($C$) is closed and bounded, it's compact thus it has a finite subcover $\{U_{\omega}\}_{\omega \in \Omega}$. Next, we construct a partition $P$ of $A$ using the endpoints of the closed intervals which make up the closed rectangles $U_{\omega}$; so that each $U_{\omega}$ is a union of subrectangles determined by $P$. Now, because $\sigma_2 $ is contained in $\sigma_1$, we have that $$\sum_{S \in \sigma_1}v(S) - \sum_{S \in \sigma_2}v(S) = \sum_{S \in \sigma_1 - \sigma_2}v(S) $$ Now here is where I can't properly justify my next step: I want to claim that the RHS is $\leq \sum_{\omega \in \Omega} v(U_{\omega})$, because from the pictures I drew that's what it seems like... Assuming that step is true, we have that 
\begin{align}
\sum_{\omega \in \Omega} v(U_{\omega}) &\leq \sum_{i=1}^{\infty} v(U_i) \\
&< \varepsilon
\end{align} Since $\varepsilon > 0$ was arbitrary, this completes the ""proof"" of this direction. For the other direction, for any given $\varepsilon > 0$, we can choose a partition $P$ such that $$ \sum_{S \in \sigma_1}v(S) - \sum_{S \in \sigma_2}v(S) < \varepsilon$$
Once again since $\sigma_2 \subseteq \sigma_1$, we have that $$\sum_{S \in \sigma_1 - \sigma_2}v(S) < \varepsilon$$ Here, I would like to claim that $\sigma_1 - \sigma_2$ covers bd($C$), which once again seems like it should be true from the pictures I've drawn, but I can't prove it. If this is indeed true then we've just shown that bd($C$)
has content $0$, which implies it has measure $0$; thus completing the proof. I'd appreciate any help in justifying those two steps of my proof, and also any comments on any other mistakes which are present. Thanks!","['multivariable-calculus', 'integration']"
2843516,Algebra with exponential functions,"If $f(x) = 4^x$ then show the value of $$f(x+1) - f(x)$$ in terms of $f(x)$. I know the answer is $3f(x)$ because 
$f(x+1)$ means that it is $4^x$ multiplied by 4 once more, which minus one is 3. The question: How do I show this process algebraically? (Hints only please) I have tried using ln() functions to remove the powers to no avail. $$\ln(f(x)) = x\ln(4)$$ 
$$\ln(f(x+1)) = (x+1)\ln(4) = x\ln(4) + \ln(4)$$
$$\ln(f(x+1)) = \ln(f(x)) + \ln(4)$$ from here I don't know how to remove the natural logs to replace $f(x+1)$. What is a different approach I should use?",['algebra-precalculus']
2843553,Derivative of $A^\top A$,"Let $f(A):= A^\top A$ where $A$ is an $m \times n$ matrix.  We want to
find the derivative of $f$ with respect to $A$. By derivative we mean
to find the Jacobian of all partial derivatives of $f(A)$ with respect
to $A$. Here is how I proceed. The Derivative of $f$ is the linear map $D f(A): X \to A^\top X +
X^\top A$. Let $K$ be the commutation matrix such that
$K\operatorname{vec}(X^\top A) = \operatorname{vec}(A^\top X)$.
Then, \begin{align}
  \operatorname{vec}(A^\top X + X^\top A)
  & = \operatorname{vec}(A^\top X) + \operatorname{vec}(X^\top A) \\
  & = (I_n\otimes A^\top) \operatorname{vec}(X) +
    \operatorname{vec}(X^\top A) \\
  & = I_n (\otimes A^\top) \operatorname{vec}(X) +
    K_{n,n} \operatorname{vec}(A^\top X) \\
  & = (I_n \otimes A^\top) \operatorname{vec}(X) +
    K_{n, n} (I_n \otimes A^\top) \operatorname{vec}(X)
\end{align} It now follows that
\begin{align}
  \frac{\partial f}{\partial A} & =  (I_n \otimes A^\top)
                                  + K_{n, n} (I_n \otimes A^\top)
\end{align} In here I am using the fact that $\operatorname{vec}(AXB) = (B^\top \otimes A)\operatorname{vec}(X)$ where $\operatorname{vec}$ is the vectorization operator. I was inspired by this answer and the corresponding equation under the section Differentials of Quadratic Products on this webpage My Questions: Is this approach correct?. If not how do I go about finding the desired derivative? Where can I find references regarding this type of manipulation?. (I don't mean this particular manipulation, but a reference for derivatives of matrices in general). I looked on Horn and Johnson Matrix Analysis , but a 'commutation matrix' is nowhere to be found. When I say reference, I mean a rigorous linear algebraic exposition.","['matrices', 'reference-request', 'matrix-calculus', 'jacobian', 'linear-algebra']"
2843554,"If $\int^{x}_{0}2x(f(t))^2dt = \bigg(\int^{x}_{0}2f(x-t)dt\bigg)^2$ and $f(1) = 1,$ Then $f(x)$ is","If  $\displaystyle \int^{x}_{0}2x(f(t))^2dt = \bigg(\int^{x}_{0}2f(x-t)dt\bigg)^2$ and $f(1) = 1,$ Then $f(x)$ is Try: Using $\displaystyle  \int^{a}_{0}f(x)dx = \int^{a}_{0}f(a-x)dx$ We can write it as $$\displaystyle x\int^{x}_{0}f^2(t)dt = \bigg(\int^{x}_{0}2f(t)dt\bigg)^2 = 4 \bigg(\int^{x}_{0}f(t)dt\bigg)^2\;\;\;(*)$$ Using Leibnitz Rule of Differentiation $$xf^2(x)+\int^{x}_{0}f^2(t)dt = 8\int^{x}_{0}f(t)dt\cdot f(x)$$ Again Differentiate w r to $x$ $$x\cdot 2f(x)\cdot f'(x)+f^2(x)+f^2(x)=8f^2(x)+8\int^{x}_{0}f(t)dt$$ Could some help me how to solve it, Thanks","['derivatives', 'real-analysis', 'functional-equations', 'calculus', 'ordinary-differential-equations']"
2843560,"If $\sin x +\sin 2x + \sin 3x = \sin y\:$ and $\:\cos x + \cos 2x + \cos 3x =\cos y$,","If  $\sin x +\sin 2x + \sin 3x = \sin y\:$ and $\:\cos x + \cos 2x + \cos 3x =\cos y$, then $x$ is equal to (a) $y$ (b) $y/2$ (c) $2y$ (d) $y/6$ I expanded the first equation to reach $2\sin x(2+\cos x-2\sin x)= \sin y$, but I doubt it leads me anywhere. A little hint would be appreciated. Thanks!",['trigonometry']
2843564,Hausdorff covering space of a quotient,"I'm in trouble with this (maybe simple) exercise: Let $X$ be an Hausdorff topological space and let $G$ a finite group of automorphism of $X$. Suppose that $G$ acts on $X$ without fixed points (i.e. $\forall \,\psi \in G \setminus \{id\}$ then $\psi(x) \neq x \quad \forall \, x \in X$). Then $p \colon X \to X/G$ together with $X$ is a cover space of $X/G$ where $p$ is the canonical projection. My first idea is to prove that $G$ is properly discontinuous.
Indeed, $G$ is a finite group, so we suppose $G= \{g_0,\dots,g_n\}$. Let $x_0 \in X$ and now we consider its orbit under the action of $G$, say $O_{x_0} =\{x_0,x_1, \dots, x_n\}$; $X$ is Hausdorff then there exist $V_i$ disjoint neighborhoods of $x_i$ respectively ($V_i \cap V_j= \emptyset \, \, \forall i \neq j$). Now, for example we take $g_i$ such that $g_i(x_0)=x_i$; we have to show that exits a neighborhood of $x_0$, say $U_i$, such that $g_i(U_i) \subset V_i.$ I think $U_i = V_0 \cap g_{i}^{-1}(V_i)$ could work, but I don't know exactly why (any ideas?). Then we call $U= \bigcap_{i=0}^{n} U_i$. Thus, theoretically, $U \subset V_0$ and $g_i(U) \subset V_i$ and so $U \cap g_i(U) = \emptyset \, \, \forall \,i$. Now, if I knew $X$ is connected and locally arcwise-connected I would conclude the proof. So, how to prove the statement without knowing $X$ is connected and locally arcwise-connected? I think $p$ is an open map, 
is this information useful?","['algebraic-topology', 'general-topology', 'covering-spaces']"
2843582,Using complex method to evaluate $\int_0^\infty\frac{\ln(1+x)}{1+x^2}dx$,"Evaluate$$I=\int_0^\infty\frac{\ln(1+x)}{1+x^2}dx$$
I know the answer and other methods to solve this integral. I want to know what is wrong with my attempt. Let $f(z)=\frac{\ln(1+z)\ln z}{1+z^2}$, where $\Im\ln(1+z)\in(-\pi,\pi]$ and $\Im \ln z\in[0,2\pi)$. Take the ""keyhole contour"" $C=C_+\cup C_R\cup C_-\cup C_r$. Edit: $C_+$: the upside of the positive real axis, $C_-$: the downside of the positive real axis, $C_R$: big circle around the origin, $C_r$: small circle around the origin. $$\int_Cf(z)dz=2\pi i(\operatorname{Res}_{z=i}f(z)+\operatorname{Res}_{z=-i}f(z))=\frac12\pi^2 i(\ln(1+i)-3\ln(1-i))$$
As $r\to 0$ and $R\to\infty$, $\int_{C_R}f(z)dz$ and $\int_{C_r}f(z)dz$ vanish. $$(\int_{C_+}+\int_{C_-})f(z)dz=\int_0^\infty\frac{\ln(1+x)}{1+x^2}(\ln x+2\pi i-\ln x)dx$$
Hence I get $$I=\frac{1}{2\pi i}(\int_{C_+}+\int_{C_-})f(z)dz=\frac14\pi(\ln(1+i)-3\ln(1-i))$$
which is wrong.","['complex-analysis', 'integration', 'definite-integrals', 'calculus']"
2843586,Bronze and silver beads,"We have 13 beads which look identical, but 6 of them are made of bronze, while the others are made of silver. All the bronze are of the same weight and all the silver also are of the same weight but the weight of each bronze is by 1 gram different from the weight of each silver (we don't know whether it is heavier or lighter). We randomly choose one of the 13 beads and by using a balance scale we want to determine whether it is bronze or silver. The scale shows the weight difference in grams between her two sides. What is the maximum number of weighings we will need? I was told this is a variation of an old IMO problem but I can't figure out any idea. I know the basic concept of the weightings using a balance scale, where  we need $3^n$ weighings (taking into account the 3 different possible outcomes from each use of the scale: Balance, one side is heavier, one side is lighter), but this is different!",['combinatorics']
2843613,Solving $y''-k^2y=0$ without substituting $e^{kx}$,As the title says I am trying to solve $y''-k^2y=0$. The method that I want to use is to assume $y'=p$ which gives us $y''=p\frac{dp}{dy}$. Substituting above values in original equation gives me $p\frac{dp}{dy}-k^2y=0$ which further reduces to $\frac{dy}{dx}=\sqrt{k^2y^2+c}$. On trying to solve this differential equation I am not reaching any close to the expected answer which should be summation of two exponential terms.,"['ordinary-differential-equations', 'calculus']"
2843675,The set of integers $n$ expressible as $n=x^2+xy+y^2$,"Let $S$ be the set of integers $n$ , such there exist integers $x,y$ with $$n=x^2+xy+y^2$$ Is the implication $$a,b\in S\implies ab\in S$$ true? If yes, how can I prove it? I worked out $$n\in S\iff 4n\in S$$ and $$n\in S\iff 3n\in S$$ I tried two approaches. The first is to express $$(a^2+ab+b^2)(c^2+cd+d^2)$$ in the form $$f^2+fg+g^2$$ with polynomials $f,g$ with integer coefficients. I however could not find suitable $f$ and $g$ . The second approach is based on $$x^2+xy+y^2=\frac{(2x+y)^2+3y^2}{4}$$ If we have $n=x^2+xy+y^2$ , we have $u^2+3v^2=4n$ for some integers $u,v$ with equal parity. The main problem of this approach is to consider the equal parity. Any ideas ?","['eisenstein-integers', 'algebraic-number-theory', 'number-theory', 'quadratic-forms', 'elementary-number-theory']"
2843759,Compute $\int_0^{2\pi} \frac 1{\sin^4x+\cos^4x}dx$,Evaluate $\int_0^{2\pi} \frac 1{\sin^4x+\cos^4x}dx$ My attempt: $I=\int_0^{2\pi}\frac 1{\sin^4x+\cos^4x}dx=\int_0^{2\pi}\frac 1{(\sin^2x+\cos^2x)^2-2\sin^2(2x)}dx=\int_0^{2\pi}\frac {1}{1-2\sin^2(2x)}dx=\frac 12\int_0^{4\pi}\frac 1{1-2\sin^2(x)}dx=\frac 12 \int_0^{4\pi}\frac {1}{\cos(\frac{x}2)}dx=\int_0^{2\pi}\frac 1{\cos x}dx=0$ So it actually is: $$I=2\int_0^{2\pi}\frac {1}{2-\sin^2(2x)}dx=\int_0^{4\pi}\frac{1}{2-\sin^2(x)}dx=\int_0^{4\pi}\frac 1{1+\cos^2x}dx$$ Now if I try to make the substituion $u=\tan(\frac x2)$ I get integral from $0$ to $0$...Why? What I am doing wrong?,"['integration', 'trigonometry', 'calculus']"
2843760,Find a close formula using generating functions,"I want to find the sum:
$$\sum \limits_{k=0}^n {n\choose k}2^{k-n}$$ I did it by using the binomial theorem and I got $$\sum \limits_{k=0}^n {n\choose k}2^{k-n}
= 2^{-n} \sum \limits_{k=0}^n {n \choose k}2^k
= \left(\frac{1}{2}\right)^n3^n$$ I would like to know how to do it using generating functions. I have tried some calculations but I am stuck. This is what I have so far. $$\sum \limits_{k=0}^n {n\choose k}2^{k-n}
=\sum \limits_{n\ge0} \sum \limits_{k\ge0} {n \choose k} 2^{k-n} x^n
=\sum \limits_{k\ge0} 2^k \sum \limits_{n\ge0} {n \choose k}\left(\frac{x}{2}\right)^n$$","['generating-functions', 'discrete-mathematics']"
2843785,The Z transform: why $z^{-n}$ and not $z^n$?,"The Z-transform for a discrete signal is defined as $X(z) = \sum_{n = -\infty}^{\infty} x[n]z^{-n}$. I was wondering why we invert the exponent of $z$, rather than define it as $X(z) = \sum_{n = -\infty}^{\infty} x[n]z^{n}$, which to my mind seems like a more natural definition. I've not been able to find an answer for this anywhere. Both definitions seem to give pretty much the same theory (but different domains of convergence in applications).","['signal-processing', 'recurrence-relations', 'ordinary-differential-equations']"
2843797,A possible paradox about topology and the relation between zero and infinity? [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 5 years ago . Improve this question We know that the volume of higher dimensional sphere is inversely proportional to the number of dimensions. Hence, as the we increase the number of dimensions keeping the radius fixed, the volume of the sphere tends to zero. On the other hand, if we increase the number of dimension in case of cubes or polyhedra (to be specific), the volume increases. Hence, the volume of an infinite dimensional hypercube is infinite. Now, in the context of topology, a sphere is homeomorphic to a cube or parallelepiped, in the case of infinite dimensions... where the volume of the sphere is infinite and the volume of the hypercube is infinite... Does that imply that an object of volume of the magnitude zero is homeomorphic (that it can be continuously transformed) to something which has the volume of the magnitude Infinity? Perhaps this is not a new problem to the topologists, or maybe it is not a problem at all to them... But it is confusing me.","['general-topology', 'lebesgue-measure', 'measure-theory']"
2843802,What is the solution to this inequality: $| 2x-3| > - | x+3|?$,"By using graphical method, I am getting all real numbers..
Where am I wrong in graphical method? How to solve this using calculation?","['inequality', 'functions']"
2843816,Cantor set of constant dissection,"Let $\alpha$ be a fixed real number s.t. $0<\alpha<1$.
In stage one of the construction, remove the centrally situated open interval in $[0,1]$ of length $\alpha$. In stage 2, remove two central intervals each of relative length $\alpha$, one in each of the remaining intervals after stage 1, and so on. Let $C_\alpha$ denote the set which remains after applying the above procedure indefinitely. So that's the set up. I am now trying to prove that the complement of $C_\alpha$ in $[0,1]$ is the union of open intervals of total length equal to 1. I'm having trouble understanding how there can be an open interval at all in the complement, because to me it seems like any open interval would have to contain a element of the cantor set. But then it's common knowledge that cantor sets are nowhere dense, and therefore every open interval on $[0,1]$ must be contained in it's complement? If anyone could shed some light on this for me I'd really appreciate it! I believe I have proven that $m_*(C_\alpha)$=0. To see this, I will prove that after the k'th stage, the remaining set has total length $(1-\alpha)^k$. The base case is trvial, assume by induction that this holds for the case $n=k$. To get to the kth step, we removed $2^{k-1}$ intervals, and so at this step we have $2^k$ intervals total. I want to calculate $\alpha_k$. $((1-\alpha)^k/2^k)$/$(\alpha_k)$ = $1/\alpha$ Because on step k the total length is $(1-\alpha)^k$, and this total length is divided amongst $2^k$ intervals, and so this equation makes sense because $\alpha_k$ is defind as being relative to $\alpha$. So $\alpha_k=\alpha(1-\alpha)^k/(2^k)$ The length at the k+1 stage will be the length of the kth stage minus the total length of the $2^k$ segments we remove, i.e. $(1-\alpha)^k-2^k\alpha_k=(1-\alpha)^k-2^k\alpha(1-\alpha)^k/(2^k)=(1-\alpha)^k-\alpha(1-\alpha)^k=(1-\alpha)^k(1-\alpha)=(1-\alpha)^{k+1}$ The length of this interval will go to zero as $k \to \infty$","['real-analysis', 'lebesgue-measure', 'measure-theory', 'cantor-set']"
2843817,Properties of the solution of the Ordinary Differential Equation $y' = y(y-1)(y-2)$ as per the Initial conditions?,"Consider the Ordinary Differential equation $y' = y(y-1)(y-2)$ . Then from the different initial conditions, can we derive properties of the function $y$ ? I thought of finding the solution to this differential equation, which I tried to use Partial fractions!, (any other easier or efficient method to solve this ODE?). After doing Partial fractions  I got $0.5 \ln|y| - \ln|y-1| +0.5 \ln|y-2| = x + c$ . Now if $y(0) = 0.5$ , is the function $y$ decreasing?, well I thought of substituting value of $y(0)$ into the ode to get $y' = 0.5(0.5-1)(0.5-2)>0$ , implying $y$ is decreasing (is this the correct way?). Also if $y(0) =1.2$ then using the above criteria I think $y$ is increasing? Also if $y(0) = 2.5$ then can we say anything about the boundedness of $y$ ? If $y(0)<0$ , can we say $y$ is bounded below?.",['ordinary-differential-equations']
2843848,Prove a.s. convergence of $(X_n)_n$ satisfying $E(X_{n+1} \mid F_n) \leq X_n+Y_n$ for $\sum_n Y_n<\infty$,"I am have a problem with proving a convergence of a sequence of random variables in the given context: Let $F_n$ be a filtration. Assume that $X_n$ and $Y_n$ are non-negative and integrable $F_n$ -adapted random variables for $n\geq 0$ . Furthermore, I assume that: $\mathbb{E}[X_{n+1} \mid F_n]\leq X_n+Y_n$ and that $\sum_{n}Y_n < \infty$ . Under the above conditions, I should show that $X_n$ converges almost surely for $n\rightarrow \infty$ . My approach has been to define a variable $Z_n=X_n-\sum_{i=0}^{n-1} Y_i$ , and to set this into the expression for the conditional mean of $X_{n+1}$ , but this did not gave any result. Anybody has an idea to this?","['martingales', 'probability-theory', 'convergence-divergence']"
2843850,What is the motivation of creating of $T^*_p(\mathbb R^n)?$ How can we visualize covectors?,"Question 1 In calculus, we visualize the tangent space $T_p(\mathbb R^n)$ at $p$ in $\mathbb R^n$ as the vector space of all arrows emanating from $p$. What is the motivation of creating of $T^*_p(\mathbb R^n)?$ How can we visualize covectors? Question 2 How do I prove the sets $T^*_p(\mathbb R^n)$ are all disjoint? where $p\in U$ Considering $p,q\in U$ and $p\neq q$, suppose f is a non zero linear functional and $f \in T^*_p(\mathbb R^n)\cap T^*_q(\mathbb R^n)\implies f\in T^*_p(\mathbb R^n)$ and $f\in T^*_p(\mathbb R^n)$.How do I proceed further?. Can you please help me?","['differential-geometry', 'linear-algebra', 'tangent-spaces']"
2843863,How to solve Fokker-Planck PDE for Brownian particle in square potential driven by periodic time-dependent force,"Problem statement: We have a Brownian particle in harmonic potential with additional time-dependent force. 
Langevin equation(mass taken to be unit): $\ddot{x} + \gamma \dot{x} + \omega_0^2 x = \epsilon \cos(\omega t) + \sqrt{2D} \xi(t)$ Corresponding Fokker-Planck equation for $P = P(x,v,t)$, with $v = \dot{x}$: $\partial_t P = - v\partial_x P + \partial_v(\gamma v + \omega_0^2 x - \epsilon \cos(\omega t)) + D\partial_v^2 P$ My attempt at solving: Perform bilateral Laplace transform on $x$ and $v$. This would lead to 
a differential equation only involving derivatives in $t$. We would solve that and then preform inverse Laplace transform to get back dependency on $x$ and $v$. The problem is that Laplace transform doesn't get rid entirely of derivatives in $v$ and $x$ since for example $v\partial_x P \Rightarrow  \hat{x}\partial_{\hat{v}}\hat{P}$ Because of that, we would first have to make some sort of variable substitution: $y = y(x,v)$ $z = z(x,v)$ and possibly $F = u(x,v) P (x,v,t)$ which would than lead to PDE without variables as prefactors, sth like: $\partial_t F = c_1 \partial_x F + c_2(t) \partial_v F+ c_3 F + c_4 \partial_v^2 F$ and then continue as described.","['stochastic-processes', 'stochastic-pde', 'partial-differential-equations', 'brownian-motion', 'ordinary-differential-equations']"
2843879,Functions satisfying $f(xy)=f(x)+f(y)$,"Are there any other functions (not necessarily continuous) satisfying $f(xy)=f(x)+f(y)$ other than $f(x)=A \ln x$ and $f(x)=0$ ? After a little thought I came to identify a function $$f : \mathbb{C} \to \mathbb{R}: z \mapsto \arg(z),$$ since $$\arg(z_1z_2)=\arg(z_1)+\arg(z_2).$$ Also $$f: A-\left\{0\right\} \to \mathbb{Z_0^+}$$ where $A$ is set of non-zero polynomials such that $$f(x)=\operatorname{Deg}(\text{polynomial}),$$ since $$\operatorname{Deg}(h(x)g(x))=\operatorname{Deg}(h(x))+\operatorname{Deg}(g(x)).$$ Are there other functions with this property?","['algebra-precalculus', 'functional-analysis', 'complex-numbers', 'functions']"
2843910,Square matrix with rational coefficients having $k$-th root,"Let $A\in M_{n}(\mathbb{Q})$, meaning that $A$ is $n\times n$ matrix with entries in the rational numbers $\mathbb{Q}$. Suppose that $A$ satisfies two conditions: $\det(A)\neq 0$ For every integer $k$, there exists a $B\in M_{n}(\mathbb{Q})$ such that $B^k = A$. Does it follow that $A = I_n$? Here, $I_n$ is the $n\times n$ identity matrix. Motivation. The analogous problem where $\mathbb{Q}$ everywhere is replaced by $\mathbb{Z}$ has an affirmative answer. More precisely, let's prove the following statement: Claim. Let $A\in M_{n}(\mathbb{Z})$, meaning that $A$ is $n\times n$ matrix with integer entries. Suppose that $\det(A)\neq 0$ and that for every integer $k$, there exists $B\in M_{n}(\mathbb{Z})$ such that $B^k = A$. Then $A = I_n$. Proof. Since $\det(A)\neq 0$, there are only finitely many primes dividing $\det(A)$. Let $p$ be any prime which does not divide $\det(A)$. After reducing mod $p$, we still get a non-singular matrix $\overline{A}$, so $\overline{A}\in\operatorname{GL}_{n}(\mathbb{Z}/p\mathbb{Z})$. Let $k=\# \operatorname{GL}_{n}(\mathbb{Z}/p\mathbb{Z})$ be the cardinality of the group of invertible matrices with entries in $\mathbb{Z}/p\mathbb{Z}$. By hypothesis, there exists $B\in M_{n}(\mathbb{Z})$ such that $B^k=A$. After reducing mod $p$, we get $(\overline{B})^k = \overline{A}$. By Lagrange's theorem, $(\overline{B})^k = \overline{I_{n}}$. Therefore, $\overline{A} = \overline{I_{n}}$, so that each entry of $A-I_{n}$ is divisible by $p$. Since we have infinitely many choices for the prime $p$, it follows that each entry of $A-I_{n}$ must in fact be zero, yielding $A=I_n$. Remark 1. The hypothesis $\det(A)\neq 0$ is necessary. Otherwise, we can just take any diagonal matrix $A= \operatorname{diag}(1, 1, .. 1, 0, 0 .., 0)$ with any number of $1$s and $0$s. Certainly $A^k = A$ for every $k$ but $A\neq I_{n}$ if there is at least one zero on the diagonal. Remark 2. It is tempting to modify the above argument for the case of $\mathbb{Q}$, but it doesn't seem to work immediately. Again using the fact that $\det(A)\neq 0$, there are only finitely many primes appearing in the numerator and denominator of $\det(A)$, so we can pick a prime $p$ which doesn't appear there. Reducing the original matrix $A$ mod $p$ still works fine. But the problem arises at the step when we use hypothesis to get a matrix $B$ such that $B^k = A$ (where $k$ clearly depends on p -- this is important). That matrix $B$ might have the prime $p$ show up in the denominator of some of its entries, so reduction mod $p$ doesn't make sense. Reference. I learnt the proof above from ""Mathematical Bridges"" by Andreescu, Mortici and Tetiva. The problem appears as Exercise 18 in Chapter 6, and the solution is presented few pages later. So I am also adding the (contest-math) tag, since that is the theme of the aforementioned book.","['matrices', 'abstract-algebra', 'contest-math', 'group-theory', 'linear-algebra']"
2843918,The second differential versus the differential of a differential form,"I'm curious to know if the definition given below of the second
differential is actually used in mathematics. It is based on the following definition of the differential of a function : Let $dx\in\mathbb{R}$ be an independent variable. Let $y:\mathbb{R}\to\mathbb{R}$
be a differentiable function of $x$. The differential $dy$ of $y=y\left[x\right]$
is defined as $$
dy\equiv\frac{dy}{dx}dx,
$$ where $\frac{dy}{dx}$ is the derivative of $y$ with respect to $x$
by the standard definition. The second differential $d^{2}y$ of $y$ is defined as $$
d^{2}y\equiv\frac{d^{2}y}{dx^{2}}dx^{2}.
$$ I don't recall any case where this construct has been useful to me.
Nonetheless, I have it in my notes, and am confident that it reflects
a reputable, though dated, source. Edit to provide source: 
Link: Calculus and Analytic Geometry: With Supplementary Problems, Classic Edition, by George B. Thomas $\S$ 2-7, Problem 22. Is the above definition of the second differential used today
in mathematics? This is the question for which I will accept an answer. Can the above definition be brought into consonance with the definition of the differential of a differential form? Which, as I understand
it goes as follows: Let $\mathfrak{r}\equiv\left\{ x,y\right\} $ be a position variable
in $\mathbb{R}^{2},$ the functions $P,Q:\mathbb{\mathbb{R}}^{2}\to\mathbb{R}$
be continuously differentiable over the neighborhood of interest,
and $dx,dy:\mathbb{\mathbb{R}}^{2}\to\mathbb{R}$ be the coordinate
projection mappings. That is, given a vector $\mathfrak{v}=\left\{ v^{x},v^{y}\right\} ,$
$dx\left[\mathfrak{v}\right]=v^{x}$ and $dy\left[\mathfrak{v}\right]=v^{y}.$
The the following product is defined 
$$
dxdy\equiv-dydx,
$$ $$
dxdx\equiv dydy\equiv0.
$$
A mapping which associates with every point $\mathfrak{r}$ a linear
mapping of the form $$
\omega_{\mathfrak{r}}=P\left[\mathfrak{r}\right]dx+Q\left[\mathfrak{r}\right]dy
$$ is a differential 1-form. The differential of this differential form
is defined to be
$$
d\omega=\left(\frac{\partial Q}{\partial x}-\frac{\partial P}{\partial y}\right)dxdy.
$$ Given a continuously differentiable function $f:\mathbb{\mathbb{R}}^{2}\to\mathbb{R},$
its differential (form) is defined to be 
$$
df_{\mathfrak{r}}\equiv\left(\frac{\partial f}{\partial x}dx+\frac{\partial f}{\partial y}dy\right)_{\mathfrak{r}}.
$$ The differential (exterior derivative) of $df$ is therefore $$
d^{2}f\equiv ddf=\left(\frac{\partial^{2}f}{\partial x\partial y}-\frac{\partial^{2}f}{\partial y\partial x}\right)dxdy=0.
$$","['differential-forms', 'calculus']"
2843939,A countable dense set of irrational numbers.,"I think the following is a countable dense subset of irrational numbers. I request you to verify this. Since $\Bbb R -\Bbb Q$ is dense in $\Bbb R$, for every rational number $r$ there is a sequence of irrational numbers, which I will denote by $(\alpha_k^r),k\in \Bbb N$ such that $lim(\alpha_k^r) = r$. For a fixed $r\in\Bbb Q$, the set of points 
$ \{ (\alpha_k^r)|k\in\Bbb N \} $ must be infinite, otherwise some irrational number would be equal to $r$. Also $r\in\overline {\{ (\alpha_k^r)|k\in\Bbb N \}} $. Now consider the set, $$\Gamma = \bigcup_{r\in\Bbb Q} \{ (\alpha_k^r)|k\in\Bbb N \}\subset \Bbb R-\Bbb Q$$ which is a countable union of countable sets and hence countable. And because of the previous remarks,  $\overline \Gamma = \Bbb Q$ . But $\overline{ \Bbb Q} = \Bbb R$ and $\overline{\overline \Gamma} = \overline\Gamma .$ So, $\overline\Gamma = \Bbb R. $","['general-topology', 'real-analysis', 'metric-spaces']"
2843940,Galois Descent for representations of finite groups OR a question about block matrices where the blocks are Galois conjugates,"The question is quite long since I give some background but really I am interested in some very concrete fact about matrix representations. Scroll all the way to the bottom for a self contained question, feel free to ignore the rest. Let $L/K$ be a Galois field extension with Galois group $\Gamma$ . Let $G$ be a finite group and $V$ a representation of $G$ over $L$ (aka, a module over $L[G])$ . Suppose there is a semi linear action of $\Gamma$ on $V$ with respect to $L(G)$ . That is, for all $a \in L[G]$ , $\gamma \in \Gamma$ and $v \in V$ , we have $(av)^\gamma = a^\gamma v^\gamma$ . Does there necessarily exist a $k[G]$ submodule $W \subset L$ such that the natural map $W\otimes_{k[G]} L[G] \to V$ is an isomorphism? I believe I can prove this using general ideas from faithfully flat descent (extend to a separably closed base field...) but I am looking for a more concrete way of proving this along the following lines: Let us decompose $V = \bigoplus_k V_k$ as a sum of irreducible representations. Now, note that we can define conjugate representations $g^\gamma$ on $V$ since $g^\gamma (v) = (g(v^{\gamma^{-1}}))^\gamma \in V.$ Moreover, this conjugate action preserves irreducibility and so the representation $(G^\gamma,V_k)$ corresponds to some $(G,V_{\gamma(k)})$ . In concrete terms, if we fix a basis of vectors fixed under the semilinear action of $\Gamma$ and $g$ is given by a matrix $[a_{ij}]$ on $V_k$ , then $g^\gamma$ is given by the matrix $[\gamma(a_{ij})]$ . Therefore, we can group the conjugate representations together and now suppose $W_k = \bigoplus_{\gamma \in \Gamma}V_k^\gamma$ . (In general, there might be repeated terms in this sum but let me ignore that for now). This is closed under both the $G$ action and the $\Gamma$ action. Concretely, for any $g$ with matrix $[a_{ij}]$ for $W_k$ , the matrix for $W$ is given by a block matrix with $|\Gamma|$ blocks and the block corresponding to $\gamma$ is equal to $[a_{ij}^\gamma]$ . In this case, I want to show that the matrix for each $g$ is similar to a matrix over $k$ and moreover that this similarity is realized by the same matrix (over $L$ ) for all $g \in G$ . The second condition is what is tripping me up. Since the characteristic polynomial for any $g$ is a polynomial over $k$ , the rational canonical form realizes an element in the conjugacy class over the base field. But how do I make sure that I can do just one base change to get all the matrices corresponding to all the $g \in G$ into rational form? Concrete question: Let $L/K$ be a Galois extension with Galois group $\Gamma$ . Let $G$ be a finite group. Let $\rho: G \in M_n(L)$ be a representation and suppose that for each $g, \rho(g)$ is a block matrix with the blocks indexed by $\gamma \in \Gamma$ such that if the block corresponding to the identity in $\Gamma$ is $[a_{ij}]$ , then the block corresponding to $\gamma$ is $[\gamma(a_{ij})]$ . Can I find an invertible matrix $P$ over $L$ such that for all $g \in G$ , $P\rho(g)P^{-1}$ is a matrix over $K$ ? Note that $\rho(g)$ always has characteristic polynomial defined over $K$ so that the rational canonical form gives us some $P_g$ for each $g \in G$ with the above property. The question is eliminate the dependence of $P_g$ on $g$ .","['galois-theory', 'descent', 'finite-groups', 'representation-theory', 'linear-algebra']"
2843967,Ring homomorphism from $Z_m$ to $Z_n$ where $a^2=a$ but $a\neq\phi(1)$.,"Suppose $\phi$ is a ring homomorphism from $Z_m$ to $Z_n$. Prove if $\phi(1)=a$ then $a^2=a$. Give an example to show the converse is false. The first part I found easy enough.
$$a^2=\phi(1)^2=\phi(1^2)=a$$
Now I have trouble to negate the converse. Here is the initial statement:
$$\forall\phi:Z_m\rightarrow Z_n,\quad\forall a\in Z_n,\quad\phi(1)=a\implies a^2=a\tag{1}$$
Converse of $(1)$:
$$\forall\phi:Z_m\rightarrow Z_n,\quad\forall a\in Z_n,\quad a^2=a\implies\phi(1)=a\tag{2}$$
Negation of $(2)$:
$$\exists\phi:Z_m\rightarrow Z_n,\quad\exists a\in Z_n,\quad a^2=a\land\phi(1)\neq a\tag{3}$$
which is what we prove. Choose the homomorphism $\phi:x\mapsto x\cdot1$. $\phi(1)=1$. Choose $a=0=0^2$. We have negated $(3)$. Yeah?",['abstract-algebra']
2844020,Example of Non-Equal Random Variables that are Identically Distributed?,"What is a simple example of two random variables $X$ and $Y$ that are identically distributed s.t. $X \ne Y$? Is the only way to achieve this by changing the sample space underneath $X$ and $Y$ (i.e., $\Omega_X \ne \Omega_Y$)?","['probability-theory', 'statistics', 'probability-distributions']"
2844021,What is the name of this operation between sets?,"Suppose we have a collection of sets $\{S_i\}_{i\in I}$.
(To make it simple, we can assume these sets lie in some ""universe set"" $S$ and the index set $I$ is possibly finite.) The union of these sets is defined as $\bigcup_{i\in I} S_i = \{a\in S | \exists i \colon a\in S_i\}$. The intersection of these sets is defined as $\bigcap_{i\in I} S_i = \{a\in S | \forall i \colon a\in S_i\}$. Similarly we can define the set operation $\mathbf{\{a\in S | \exists! i \colon a\in S_i\}}$. I've seen it a lot of times, but does this have an actual name? It generalizes the symmetric difference between two sets, but the similarity stops there.","['terminology', 'elementary-set-theory']"
2844029,First Year ODE Exam Question,"What are the eigenfunctions that satisfy
  $$xy''(x) + (1-x)y'(x) + y(x) = -\lambda y(x)$$
  on the interval $I = [0, \infty)$ given that the eigenvalues are natural numbers (positive integers) such that $y(0) = 1$? This question was on my first-year university ODE final exam and not a single student was able to solve it. We are trying to have it curved from the final exam grade, do we have a strong case or is this fairly ""ordinary"" material?",['ordinary-differential-equations']
2844037,Nested summation - intuition,"To begin with, I noted that $$ \begin{aligned} \displaystyle \sum_{r_1 = 1}^{r} r_1 &= \dfrac{1}{2} r (r+1) \quad &(1)\\ \displaystyle \sum_{r_2 = 1}^{r} \displaystyle \sum_{r_1 = 1}^{r_2} r_1 &= \dfrac{1}{6} r (r+1) (r+2). & \qquad(2) \end{aligned}$$ This led me to suggest the more general conjecture that $$ \begin{aligned} \displaystyle \sum_{r_n = 1}^{r} \displaystyle \sum_{r_{n-1} = 1}^{r_n} \cdots \displaystyle \sum_{r_2 = 1}^{r_3} \displaystyle \sum_{r_1 = 1}^{r_2} r_1 &= \dfrac{1}{(n+1)!} \prod_{k=0}^{n} (r+k) \\ &= \dfrac{1}{(n+1)!} \dfrac{(r+n)!}{(r-1)!} \qquad(\star)  \end{aligned} $$ I believe that I've managed to successfully prove this using induction, but on the whole the process isn't very enlightening and given how ""nice"" the result is I'm led to believe that there's some more general insight here that I'm missing. I've seen a link to the geometric interpretation of $ (1) $ by ""pasting together"" two copies of the sum to form a rectangle and I imagine the proof carries through analogously for $ (2) $ by forming a cuboid using 6 copies of the summation, but I'm not sure how to formalise this method of thinking (or indeed how to generalise it to higher dimensions). Of course this is just one particular thought I've had so any alternative proofs would also be welcome!","['combinatorics', 'summation', 'alternative-proof', 'geometry']"
2844053,Conditions for an inequation to hold,"I am trying to find the domain of a real number n so that $$nx^2+(n-1)x+(n-1)< 0$$ for any $x\ge 0$. I am thinking to solve as:  $$n<0$$ and $$\Delta<0$$ Is this enough? ""For any $x\ge 0$ ""makes me unsure. Is there perhaps some intuition behind this?","['functions', 'quadratics']"
2844059,Limit of a function at a point in the domain of the function and/or the closure of the domain.,"I'm working through Vector Calculus, Linear Algebra and Differential Forms: A Unified Approach (5th ed, Hubbard) and on page 91 the author writes ""Limits like $\lim_{x\rightarrow x_0}f(x)$ can only be defined if you can approach $x_0$ by points where $f$ can be evaluated. Thus when $x_0$ is in the closure of the domain of $f$, it makes sense to ask whether the limit $\lim_{x\rightarrow x_0}f(x)$ exists. Of course, this includes the case when $x_0$ is in the domain of $f$; in that case for the limit to exist we must have
$$\lim_{x\rightarrow x_0}f(x)=f(x_0).\hspace{5em}(1)$$ But the interesting case is when $x_0$ is in the closure of the domain but not in the domain. For example, does it make sense to speak of $$\lim_{x\rightarrow 0}(1+x)^{1/x}?\hspace{7em}(2)""$$ And on the next page where he defines the limit of a function, he gives the usual definition except that he allows $|x-x_0|=0$ (in his words this makes limits better behaved under composition). are my statements correct? If $x_0$ is in the closure of the domain and the domain, then since the author allows $|x-x_0|=0$ in the definition of the limit (and clearly $x_0-x_0=0$), we get the condition (1). If $x_0$ is in the closure of the domain but not the domain, it is in the boundary of the domain and $|x-x_0|>0$ for all $x$ , so for (2) it does make sense to speak of the limit since we can still approach $x_0$ from points in the domain and since $|x-x_0|>0$  we don't require (1) (or even for $f(x_0)$ to exist, which it cannot since $x_0$ is not in the domain).","['multivariable-calculus', 'calculus']"
2844106,$2021^{\text{st}}$ term of a Sequence,"Q. Let sequence ${a_{n}}$ satisfy $$a_{1} = 1, a_{2}=4, a_{3}=5 $$ and $$ a_{n}+a_{n-1}+a_{n-2}+a_{n-3}=n^2$$ $$\forall  n \geq 4 $$ Then find the sum of the digits of $ a_{2021}$ . My attempt : The given sequence isn't making any progression . So I tried to calculate the furthur terms of the series and tried to get a possible sequence for the terms. However I was unsuccessful in that! Then I attempt to make a possible sequence for the sum of the digits of the further terms in the series, which got ruined too! Now I have no clue how to get on with that problem! I think forming a function of n as a difference of two terms would help but please could you suggest how to make it? Please help.","['sequences-and-series', 'functions']"
2844143,Understand closure and limit points,"I study Rudin's book on analysis. I want to check with you that I understand the definition of closure. Therefore I must check with you that I understand the definition of limit points. A neighborhood is for example an interval, a circle or a sphere around a point or a set. A limit point is any point that ""can be approached as a limit"" in contrast to an isolated point for example the set $\{\{1\}, [2,3]\}$ i.e. the union of the point $1$ and the interval between $2$ and $3$ has an isolated point $1$ which is not a limit point but any points between $2$ and $3$ are limit points. Do you agree? What I don't understand about closure is: How can there be limit points of a set $E$ which are outside of $E$? I think that the example is an open set but I don't get the picture. I think that the closure by definition always is a closed set and therefore the union of an open set and its limit points is not closed, but I still don't understand how a set can have a limit point not in the set. I must have misunderstood.","['general-topology', 'real-analysis', 'elementary-set-theory']"
2844156,an inequality about the maximum,"Let $f \colon [0, \infty] \to [0, \infty]$ be a monotonically decreasing function. Given any two continuous non-negative real-valued functions $g$ and $h$ defined on a metric compact space $X$ , I am curious that could we get the following result $$ \left|\,  f \left[ \max_{x \in X} g(x) \right] - f \left[ \max_{x \in X} h(x) \right] \,\right| \leq \max_{x \in X}\, \bigg| \,  f \left[ g (x) \right] -  f \left[ h (x) \right] \, \bigg| \quad?$$ In fact, I tried to verify the above inequality with some concrete examples and the result indicates that this inequality does hold so far. However, I have not figured out yet how to proof it. Thus, does this inequality really hold true? Could anyone give me some hint or advice please? Any idea or suggestions are really appreciated! Thank you so much in advance!","['real-analysis', 'inequality', 'functions']"
2844165,How to see $x^3-17$ has a degree 2 irreducible factor in $Q_{3}[x]$?,"Consider $Q_3$ field which is rational number $Q$ completed at $p=3$.(In other words, $Q_3$ is $3-$adic rational numbers.) Let $f=x^3-17\in Q_3[x]$. Let $O$ be the complete DVR associated to $Q_3$. Clearly $f\in O[x]$. From Gauss lemma by $O$ PID, I see that if $f=gh,g,h\in Q_3[x]$,then $g,h\in O[x]$. Therefore, I can perform $3-$reduction by considering $\frac{O}{3}[x]$. Now $\bar{f}\in Z_3[x]$ has $x^3+1=(x+1)^3\in Z_3[x]$. So I cannot apply hensel lemma here to determine factor of $f$ in $Q_3[x]$. The book says it has a degree 2 irreducible factor in $Q_3[x]$. The other version of Hensel uses $f'(x)=3x^2$. And $x=-1$ yields $3^2\vert f(-1)$ and $3^2\not\vert f'(x)$ but $2=1+1$. The absolute value requires $|f(-1)|<|f'(-1)|^2$ where $|x|$ is the $\frac{1}{3^{v_3(x)}}$ and $v_3(x)$ is $3-$adic valuation of $x\in Q_3$. Both sides yields $\frac{1}{9}$ for absolute value. So I cannot apply this hensel lifting. $\textbf{Q:}$ Can someone kindly provide hints to methods to determine reducibility for local fields? If I am lucky, I can proceed by hensel lemma but it requires factors after prime reduction being coprime. Have I done something wrong above?","['number-theory', 'abstract-algebra', 'p-adic-number-theory']"
2844201,Is there any way to systematically do all epsilon delta proofs?,"If you want to prove that the limit of $f(x)$ as $x$ to $a$ is equal to $L$ using the epsilon-delta definition of the limit, you need to solve the inequality $$|f(x)-L|<\epsilon$$ for $x$, getting it into the form $$|x-a|<\delta$$ for some $\delta$, which will in general be a function of $\epsilon$. My question is, is there some way to calculate the function $\delta(\epsilon)$, short of solving the inequality above using the function $f$ you have? Is it at least possible if $f$ is sufficiently well behaved?  Like if $f$ is differentiable, can you calculate $\delta(\epsilon)$ using the derivative of $f$? EDIT: This journal paper shows a formula for polynomials.  If $f(x) = \sum_{n=0}^{k} a_n (x-a)^n$, then to prove that the limit of $f(x)$ as $x$ goes to $a$ equals $f(a)$, we can let $\delta = min(1,\frac{\epsilon}{ \sum_{n=1}^{k} |a_n|})$. Can this be generalized to Taylor series?  If $f(x) = \sum_{n=0}^{\infty} a_n (x-a)^n$, then can we prove that the limit of $f(x)$ as $x$ goes to $a$ equals $f(a)$ by letting $\delta = min(1,\frac{\epsilon}{ \sum_{n=1}^{\infty} |a_n|})$ ?","['real-analysis', 'limits', 'calculus', 'epsilon-delta', 'proof-writing']"
2844214,Why does the following construction describe the Serre functor?,"In the book ""Spectral Agebraic Geometry"" that Jacob Lurie is currently writing, he gives a construction (11.1.5.1), which describes the Serre functor: it has already been shown that any proper $A$-linear category $C$ over a ring spectrum (or just ring, I don't think anything about the construction is inherently $\infty$-categorical) $A$ is compactly generated and therefore dualizable. Let $e:C\otimes C^\vee\rightarrow Mod_A$ be the duality datum. Then this functor has a right adjoint $e^R:Mod_A\rightarrow C\otimes C^\vee$ which is also $A$-linear and can be identified with an $A$-linear functor $S_C:C\rightarrow C$, that can be more explicitly described as the composition $C\simeq Mod_A\otimes C\overset{e^R\otimes id}{\rightarrow} C\otimes C^\vee \otimes C\overset{id\otimes e}\rightarrow C\otimes Mod_A\simeq C$. Furthermore he describes in the appendices (D.7.2) that $C^\vee$ can be identified with $Ind(C_c^{op})$, where $C_c$ denotes the full subcategory of $C$ on the compact objects and that the duality datum $e$, restricted to the compact objects can be identified with the enriched $hom$ functor $\underline{hom}:C_c\otimes C_c\rightarrow Mod_A$. This is certainly a very beautiful description of the Serre functor of such a category, but sadly I fail to see why the described thing actually is this functor. Given that he only only offhandedly mentions that, for $F,G\in C_c$ it induces an equivalence $\underline{hom}(F,S_C(G))\simeq \underline{hom}(G,F)^\vee$ and therefore those two coincide, I am probably missing something basic, maybe some well-known fact about the interaction of duals of categories and adjoints, but no matter how long I stare at it, I just cannot see it. Anything that might help me understand why those this abstract category-theoretic construction gives us indeed the Serre functor would be greatly appreciated.","['category-theory', 'algebraic-geometry']"
2844254,"The proof of $c_1(H^1(X,\mathcal{O}^*))=\tilde{H}^2_{1,1}(X,\mathbb{Z})$","Here $X$ is a complex manifold. The proof below is from Wells'book p.106. I have a question about the claim that to see $\delta(H^1(X,\mathcal{O}^*))=H^2_{1,1}(X,\mathbb{Z})$, it suffices to show that the image of $H^2_{1,1}(X,\mathbb{Z})$ in $H^1(X,\mathcal{O})$ is zero. By the exactness of (4.7) (attached at last), we know $\delta(H^1(X,\mathcal{O}^*))=ker(H^2(X,\mathbb{Z})\to H^1(X,\mathcal{O}))$, but the image of $H^2_{1,1}(X,\mathbb{Z})$ in $H^1(X,\mathcal{O})$ is zero only shows $H^2_{1,1}(X,\mathbb{Z})\subset ker(H^2(X,\mathbb{Z})\to H^1(X,\mathcal{O}))$, I wonder why it proves that $\delta(H^1(X,\mathcal{O}^*))=H^2_{1,1}(X,\mathbb{Z})$?","['complex-geometry', 'sheaf-cohomology', 'differential-geometry']"
2844296,Evaluating $\operatorname{arccos} \frac{2}{\sqrt5} + \operatorname{arccos} \frac{3}{\sqrt{10}}$,"Evaluate: 
$$\operatorname{arccos} \frac{2}{\sqrt5} + \operatorname{arccos} \frac{3}{\sqrt{10}}$$ We let 
$$\alpha = \operatorname{arccos} \frac{2}{\sqrt5} \qquad  \beta = \operatorname{arccos}\frac{3}{\sqrt{10}}$$ Then we have: $$\begin{align}
\cos(\alpha) = \frac{2}{\sqrt5} &\qquad \cos(\beta) = \frac{3}{\sqrt{10}} \\[4pt]
\sin(\alpha) = \frac{1}{\sqrt5} &\qquad \sin(\beta) = \frac{1}{\sqrt{10}}
\end{align}$$ In order to evaluate, we are told, we first determine $\sin(\alpha + \beta)$; we wind up with $1/\sqrt2$, thus we have $\pi/4$. What I am confused about is why we have to use sin($\alpha + \beta$). For example, if I were to use $\cos(\alpha + \beta)$, I would get the answer $7/(\sqrt{10}\sqrt5)$, which I do not know what to do with. I am having trouble finding out whether there is some kind of pattern to this kind of thing, or did the author just know to use $\sin(\alpha + \beta)$ since he/she checked cos and saw nothing comes out of this? Any help is much appreciated, thank you","['summation', 'trigonometry', 'inverse']"
2844327,$f(x)$ differentiable at $x_0$ with full rank derivatives implies $|f(x_0) - f(x)| \ge C |x_0 - x|$?!,"I found a claim in this highly cited paper: Pakes, A., & Pollard, D. (1989). Simulation and the asymptotics of optimization estimators. Econometrica: Journal of the Econometric Society, 1027-1057. The claim appears in the proof of theorem 3.3 and is essentially the following: (1?) If $f(x)$ is differentiable at $x_0$ with a derivative matrix $\Gamma$ of full rank, then there exists a $C > 0$ and a neighborhood $\mathcal{B}$ of $x_0$ such that, for every $x$ in $\mathcal{B}$ , $$ \| f(x) - f(x_0) \| \ge C \| x - x_0 \|$$ How do I prove this statement? Any help would be very appreciated.","['derivatives', 'real-analysis', 'lipschitz-functions']"
2844438,How can one go about counting the amount of abelian groups of a fixed order?,"Let $n \in \mathbb{N}$ and let $$
n = p_1^{a_1} \cdots p_r^{a_r} 
$$ be its factorization into primes. My intention is to count how many abelian groups $G$ of order $n$ are there. What follows are my thoughts about this question, which surely are quite rudimentary. By the structure theorem, we have that:
$$
G \simeq \bigoplus_{i = 1}^r \bigoplus_{j = 1}^{m_i} \mathbb{Z}/(p_i^{s_{i,j}}) 
$$ with $s_{i,1} \leq \dots \leq s_{i,m_i}$ for each $i$ and thus, the cardinals of these two groups must coincide: $$
n = \prod_{i = 1}^r \prod_{j = 1}^{m_i} p_i^{s_{i,j}} = \prod_{i = 1}^r p_i^{\sum_{j = 1}^{m_i}s_{i,j}}
$$ Therefore, by the uniqueness of prime factorization, $$
s_{i,1} + \dots + s_{i,m_i} = a_i \quad (\forall i\in [r]) 
$$ The question then turns into a combinatorial one, that is, how many combinations of non-negative, non-decreasing integers are there such that: $$
s_{i,1} + \dots + s_{i,m_i} = a_i \quad (\forall i\in [r]) 
$$ Since the problem is independent for each prime, we can tackle each of these independently. Moreover, we can count the amount of tuples of non negative numbers $(s_1, \dots, s_{a_i})$ in increasing order such that $\sum_{j = 1}^{a_i}s_j = a_i$; that is, we can fix a size for the amount of numbers, since the smallest possible case is to have exactly $a_i$ ones. Having done this, we have as many combinations as the $a_i$-th coefficient of the following generating function $$
f_i = \prod_{j = 1}^{a_i}(1-X^j)^{-1} = \left(\sum_{k \geq 0}X^k\right) \dots \left(\sum_{k \geq 0}X^{ka_i}\right)
$$ by corresponding solutions to $x_1 + 2x_2 + \dots + a_ix_{a_i} = a_i$ with tuples by taking: $$
s_{a_i-q} = \sum_{k = q}^{a_i}x_k
$$ that is, we would have $[f_i]_{a_i}$ possible solutions, giving a total of $$
[f_1]_{a_1} \cdots [f_r]_{a_r}
$$ possible abelian groups of order $n$. I'd really appreciate if you could comment on whether this approach is correct or not and if so, how could one get a more explicit answer, since this one is rather unsatisfactory.","['abelian-groups', 'abstract-algebra', 'combinatorics']"
2844441,What is the $n^\text{th}$ derivative of $f(x)=\frac{1}{1+x^2}$,"I want the taylor series expansion around some value $a$ of the function $f(x)=\frac{1}{1+x^2}$. I used the general formula
\begin{eqnarray}
f(x) = f(a) + \sum_{n=1}^\infty \frac{f^{(n)}(a)}{n!}(x-a)^n
\end{eqnarray}
But unfortunately, I cannot compute any general formula for $f^{(n)}(a)$. The first derivative is 
$$ f^{(1)}(x)= -\frac{2x}{(1+x^2)^2}.$$The second derivative is 
$$ f^{(2)}(x)= \frac{6x^2-2}{(1+x^2)^3}.$$The third derivative is 
$$ f^{(3)}(x)= \frac{24x(x^2-1)}{(1+x^2)^4}.$$The fourth derivative is 
$$ f^{(4)}(x)= -\frac{24(5x^4-10x^2+12)}{(1+x^2)^5}$$. The fifth derivative is 
$$ f^{(5)}(x)= \frac{240x(3x^4-10x^2+3)}{(1+x^2)^5}$$. What is the $n$-th derivative of the function for working with the above taylor series which I want to use to prove something?","['derivatives', 'taylor-expansion', 'calculus']"
2844457,Prove that the matrix $[\Gamma(\lambda_{i}+\mu_{j})]$ is nonsingular.,Let $A$ be an $n\times n$ matrix whose entries are \begin{align*} a_{ij} = [\Gamma(\lambda_{i}+\mu_{j})] \end{align*} where $0 < \lambda_{1} < \ldots < \lambda_{n}$ and $0 < \mu_{1} < \ldots < \mu_{n}$ are real positive numbers and $\Gamma$ denotes the Gamma function given by $\Gamma(z) = \int_0^\infty t^{z-1}e^{-t}dt$ for $\operatorname{Re}(z)>0$. We need to show that matrix $A$ is non-singular. I have no idea how to start. Any hint or solution will be appreciated.,"['matrices', 'linear-algebra', 'gamma-function', 'determinant']"
2844479,"Are there $n$ disjoint, connected sets whose union is $\mathbb{R^3}$ which translate to each other?","When thinking the other day about grids, it occurred to me that if you take each point $p$ in $\mathbb{Z^3}$ and place a cube of sidelength 0.5 with $p$ as its center and then connect the cubes by extruding their faces to meet all their ""nearest neighbours"" (i.e all points with one of their coordinates $+$ or $-$ $p$'s value), you get a 3D grid with the nice property that its complement in $\mathbb{R^3}$ is connected. It then occurred to me that its complement was in fact a copy of itself translated by a specific vector. My question : Is there a way of splitting $\mathbb{R^3}$ into 3 disjoint connected sets, so that you can make with one all the others through translations? If this is possible : Can we do this with $n$ sets? If this isn't possible : Can we make a construction that works for $n$ sets if we further allow the operations of rotation and scaling? REMARK: It also occurs to me that if it isn't possible to make it work in 3-space, then it might work in higher dimensions... This intuition comes from the fact that in 2-space, the complement of a grid is a bunch of disjoint squares (i.e. the complement is not a connected set), while in 3-space there is ""enough room"" for its complement to still be connected.","['general-topology', 'metric-spaces', 'euclidean-geometry']"
2844598,Speaking conceptually about the tail $\sigma$-algebra,"I have trouble giving negative answers when asked whether some event belongs to the tail $\sigma$-algebra or not. Solutions often argue conceptually, that is answer something along the lines: ""This event depends on the first outcomes, so it is not part of the tail sigma algebra"". Can we make this precise? Let $(X_n)$ be a sequence of real random variables. The tail $\sigma$-algebra is $\mathcal{T}=\bigcap_{k=1}^\infty \sigma(X_k,X_{k+1},\dots)$.
I'd like some Lemma that goes something like this: If $(\sigma(X_n))_{n\in\mathbb{N}}$ suffices some necessary conditions (independency, non-inclusion,... ) and there are outcomes $\omega_1,\omega_2$ such that $\omega_1\in A$, $\omega_2\notin A$, and $(X_n(\omega_2))_{n\in\mathbb{N}}=(X_n(\omega))_{n\geq k}$ for some $k\geq 2$, then $A\notin\mathcal{T}$. Can someone give a such statement and proof? Example: If $(X_n)$ i.i.d. and $P([X_1=1])=p$, $P([X_1=-1])=1-p$, $S_n=\sum_{k=1}^n X_k$, determine whether $\limsup_n[S_n=0]\in\mathcal{T}$. We could use $(X_n(\omega_1))=(1,-1,-1,1,-1,1,-1,\dots)$ and $k=2$ in the eventual lemma above to conclude that $S_n\notin\mathcal{T}$. (Bonus question: How can we argue existence of suitable $\omega_1,\omega_2$ in this case?)","['probability-theory', 'measure-theory', 'random-variables']"
2844619,Mean and root mean square of a random variable,"Given a positive random variable $x$ with continuous probability density $f(x)$. What is the main difference between the ordinary mean value $$\bar{x} = \int_0^\infty\,x\,f(x)\ dx$$ and the root mean squared expression $$\tilde{x}= \sqrt{\int_0^\infty\,x^2\,f(x)\ dx}.$$ Are there any relations or inequalities between both of them?","['real-analysis', 'integral-inequality', 'statistics']"
2844650,Evaluate $\int \sqrt {3 \tan^2 \theta - 1} d \theta$,"Evaluate $I=\int \sqrt {3  \tan^2 \theta - 1} d \theta$ My attempt
$\tan \theta = t, $ then $I = \int \frac{\sqrt {3t^2-1}}{1+t^2} dt   $ Now integrating by parts, I = $\sqrt {3t^2-1} \tan^{-1} t- \int( \frac{6t}{2\sqrt {(3t^2-1)}} \tan^{-1} t) dt$ Now i am struck... How to proceed.","['indefinite-integrals', 'integration', 'calculus']"
2844695,Free amalgated product and injectivity (using universal property),"Consider the following commutative diagram of group homomorphisms ( I know it looks ugly):
$$
\begin{array}{ccccccccc}
     &  & G_1 & \\
    & \nearrow{i_1} &  & \searrow{f_1} & \\
   A &  &  &  & G 
\\
    & \searrow{i_2} &  & \nearrow{f_2} & \\
    &  & G_2 & 
    \end{array}
$$ here $i_1,i_2$ are injective. We say that $G$ is the amlagated product $G_1*_A G_2$ if for any pair of homomorphisms $\phi_i:G_i\to H$ with $\phi_1 \circ i_1=\phi_2 \circ i_2$ there exists a unique homomorphism $\phi:G\to H$ such that $\phi \circ f_i=\phi_i$. I want to show that then the composition $f_1 \circ i_1$ is injecetive, using the universal property above. I already know how to show it using the explicit construction of $G$ as a quotient of $G_1 *G_2$, but I was not able to mimic the proof.","['abstract-algebra', 'group-theory']"
2844711,Independent solutions to $\sin(\theta) = -\sin(\phi)$ and $\cos(\theta) = -\cos(\phi)$,"The equation
$$
\sin(\theta) = \sin(\phi)
$$
has the set of solutions
$$
\big\{\, (\theta, \phi) \in \mathbb{R}^{2} : \theta - \phi \equiv 0 \,\,\mathrm{(mod}\,\,2\pi) \hspace{10pt}\text{or}\hspace{10pt} \theta + \phi \equiv \pi \,\,\mathrm{(mod}\,\,2\pi) \,\big\},\tag{$1$}
$$
and similarly the equation
$$
\cos(\theta) = \cos(\phi)
$$
has the set of solutions
$$
\big\{\, (\theta, \phi) \in \mathbb{R}^{2} : \theta \pm \phi \equiv 0 \,\,\mathrm{(mod}\,\,2\pi) \,\big\}.\tag{$2$}
$$ The solution set $(1)$ can be written equivalently as
$$
\theta = n\pi + (−1)^{n}\phi
$$
for some integer $n \in \mathbb{Z}$. I am similarly trying to find the solutions to the (separate) equations
  $$
\sin(\theta) = -\sin(\phi) \quad\text{and}\quad \cos(\theta) = -\cos(\phi)
$$
  in terms of $\theta$ and $\phi$. Workings Since $\sin$ is an odd function,
$$
\sin(\theta) = -\sin(\phi) = \sin(-\phi)
$$
has the set of solutions
$$
\big\{\, (\theta, \phi) \in \mathbb{R}^{2} : \theta + \phi \equiv 0 \,\,\mathrm{(mod}\,\,2\pi) \hspace{10pt}\text{or}\hspace{10pt} \theta - \phi \equiv \pi \,\,\mathrm{(mod}\,\,2\pi) \,\big\}\tag{3}
$$
by substituting $-\phi$ in place of $\phi$ in $(1)$. By drawing key points on the cosine graph, the set of solutions to the cosine equation is
$$
\big\{\, (\theta, \phi) \in \mathbb{R}^{2} : \theta \pm \phi \equiv \pi \,\,\mathrm{(mod}\,\,2\pi) \,\big\}.\tag{4}
$$
I would like some clarification that this is correct. I have checked them numerically for a range of values, and it is sensible that these should be the answers.","['trigonometry', 'proof-verification']"
2844749,Minimum area contained between measurable set and translate by $\lambda$: A strengthening of 2018 USA TSTST #9,"Question Given $\lambda\in\mathbb{R}^+$, what is the smallest possible $c$ for which, given any measurable region $\mathcal{P}$ in the plane with measure $1$, there always exists a vector $\mathbf{v}$ with magnitude $\lambda$ so that the area shared between $\mathcal{P}$ and its translate by $\mathbf{v}$ is at most $c$? Background On this year's USA TSTST (a test that determines a group of about 30 people to take selection tests for the following year's USA team to the International Math Olympiad), there was an algebra/geometry/combinatorics hybrid problem that I found interesting: Show that there is an absolute constant $c < 1$ with the following property: whenever $\mathcal P$ is a polygon with area $1$ in the plane, one can translate it by a distance of $\frac{1}{100}$ in some direction to obtain a polygon $\mathcal Q$, for which the intersection of the interiors of $\mathcal P$ and $\mathcal Q$  has total area at most $c$. Essentially every solution can be boiled down to the following: Step 1. For a given vector $\mathbf{v}\in \mathbb{R}^2$, define $f(\mathbf{v})=\mu\big(\mathcal{P}\cap(\mathcal{P}+\mathbf{v})\big)$, where $\mu(\cdot)$ is the area of a region, and $\mathcal{P}+\mathbf{v}$ consists of the points in $\mathcal{P}$ translated by $\mathbf{v}$. Step 2. Prove $f(\mathbf{u}+\mathbf{v})\geq f(\mathbf{u})+f(\mathbf{v})-1$ and generalize it to $$1-f\left(\sum_{i=1}^N \mathbf{v}_i\right)\leq \sum_{i=1}^N \left(1-f\left(\mathbf{v}_i\right)\right).$$ Step 3. Define, for real $t$, $$I_t=\int_{0\leq ||\mathbf{v}||\leq t} f(\mathbf{v})\ d^2\mathbf{v} = \int_{0\leq ||\mathbf{v}||\leq t}\int_{\mathbf{x}\in \mathcal{P}} \mathbf{1}_{\mathcal{P}}(\mathbf{x}+\mathbf{v})\ d^2\mathbf{x}\ d^2\mathbf{v}.$$ We have \begin{align}
I_t&=\int_{\mathbf{x}\in \mathcal{P}} \int_{0\leq ||\mathbf{v}||\leq t}\mathbf{1}_{\mathcal{P}}(\mathbf{x}+\mathbf{v})\ d^2\mathbf{v}\ d^2\mathbf{x}\\
&=\int_{\mathbf{x}\in \mathcal{P}} \mu\left(\mathcal{P}\cap\big\{\mathbf{x}+\mathbf{v}\big|0\leq ||\mathbf{v}||\leq t\big\}\right)\ d^2\mathbf{x}\\
&\leq\int_{\mathbf{x}\in \mathcal{P}} 1\ d^2\mathbf{x} = 1.
\end{align} So, there must exist some $\mathbf{v}$ with $0\leq ||\mathbf{v}||\leq t$ that satisfies $$f(\mathbf{v})\leq \frac{1}{\pi t^2}.$$ Step 4. Write this vector as $$\mathbf{v}=\sum_{i=1}^{\lceil 100t\rceil}\mathbf{u}_i$$ where $||\mathbf{u}_i||=1/100$. Step 5. We now have $$1-\frac{1}{\pi t^2}\leq 1-f(\mathbf{v})\leq \sum_{i=1}^N (1-f(\mathbf{u}_i)),$$ so for some $i$ we must have $$1-f(\mathbf{u}_i) \geq \frac{1}{\lceil 100t\rceil}\left(1-\frac{1}{\pi t^2}\right)$$ $$f(\mathbf{u}_i) \leq 1-\frac{1}{\lceil 100t\rceil}\left(1-\frac{1}{\pi t^2}\right).$$ As long as $t^2>1/\pi$, this gives us a working value of $c<1$. It is minimized at $t=0.98$, which gives $$c\approx 0.99318.$$ More generally, if $\lambda$ is the length of our translate, this is minimized at the nearest integer multiple of $\lambda$ to $\sqrt{3/\pi}$, which gives $$c\approx 1-2\lambda\sqrt{\frac{\pi}{27}}.$$ In particular, for large enough $\lambda$, this bound does nothing. Progress I've been able to improve the bound on $c$ given slightly in the following manner: Assume, for all $||\mathbf{u}||=\lambda$, that $f(\mathbf{u})>1-\epsilon$. Then, for all vectors $\mathbf{v}$ of length $\leq n\lambda$, by writing $\mathbf{v}=\sum_{i=1}^n \mathbf{u}_i$ with $||\mathbf{u}_i||=\lambda$, we must have $$1-f(\mathbf{v})\leq n\epsilon\implies f(\mathbf{v})\geq 1-n\epsilon.$$ Using (almost) our same integral as earlier and setting $t=N\lambda$, we have $$\int_{\lambda< ||\mathbf{v}||\leq t} f(\mathbf{v})\ d^2\mathbf{v}\leq 1.$$ (we cannot start the integral at $0$ as vectors with magnitude $<\lambda$ cannot be represented as the sum of one vector with magnitude $\lambda$). However, \begin{align}
\int_{\lambda \leq ||\mathbf{v}||\leq N\lambda} f(\mathbf{v})\ d^2\mathbf{v}
&=
\sum_{k=2}^N \int_{(k-1)\lambda< ||\mathbf{v}|| \leq k\lambda} f(\mathbf{v})\ d^2\mathbf{v}\\
&\geq 
\sum_{k=2}^N \int_{(k-1)\lambda< ||\mathbf{v}|| \leq k\lambda} 1-k\epsilon\ d^2\mathbf{v}\\
&=
\pi\lambda^2\sum_{k=2}^N (2k-1)(1-k\epsilon)\\
&=
\pi\lambda^2\frac{N-1}{6}\left(6N+6-\epsilon\left(4N^2 + 7N  + 6\right)\right),
\end{align} so $$\pi\lambda^2\frac{N-1}{6}\left(6N+6-\epsilon\left(4N^2 + 7N  + 6\right)\right)\leq 1$$ $$N+1-\epsilon\left(\frac{4N^2 + 7N  + 6}{6}\right)\leq \frac{1}{\pi(N-1)\lambda^2}$$ $$N+1- \frac{1}{\pi(N-1)\lambda^2}\leq \epsilon\left(\frac{4N^2 + 7N  + 6}{6}\right)$$ $$\frac{6\left(\pi\left(N^2-1\right)\lambda^2- 1\right)}{\left(4N^2 + 7N  + 6\right)\left(\pi(N-1)\lambda^2\right)}\leq \epsilon$$ $$1-\frac{6\left(\pi\left(N^2-1\right)\lambda^2- 1\right)}{\left(4N^2 + 7N  + 6\right)\left(\pi(N-1)\lambda^2\right)}\geq 1-\epsilon.$$ Minimizing $N$ gives you about $0.9898$ for $\lambda=1/100$. On the other hand, I've also been trying to find constructions that give a large value of $c$. I haven't come up with any great ones - the best I have is a circle of radius $\sqrt{\frac{1}{\pi}}$ which gives you a $c$ of about 0.9887 . For larger $\lambda$ the best I can think of is something like a ""star"" with large radius and very many thin ""prongs,"" but I haven't calculated the asymptotics on it yet. I believe the bound given in step 2 of the solution is sharp iff $$(\mathcal{P}+\mathbf{u})\cap(\mathcal{P}+\mathbf{v})\subseteq\mathcal{P}\subseteq(\mathcal{P}+\mathbf{u})\cup(\mathcal{P}+\mathbf{v}),$$ but all attempts I've made to find measurable sets for which this is true or nearly true for small vectors have failed. So I'm stuck. Anyone have any ideas, either on sharper upper bounds on $c$ or a sharper $\mathcal{P}$?","['geometric-measure-theory', 'lebesgue-measure', 'measure-theory']"
2844755,Find a solution to the differential equation: Which is NOT a constant multiple of the solution given.,"Find a solution to the equation:
$$
ty'' - (t+1)y' + y =0
$$
Which is not a constant multiple of the solution: $y(t) = e^t$. Hi, I have worked this problem and have solved it, using the particular solution of $e^t$. Why is this problem asking for a solution that is not a constant multiple of the only given solution? I do not see another way of solving it Thank you! ( solution from comment ) Using reduction of order we aim for finding $y_2$ based on the solution given. In this case $y(t) = e^t$ is a solution. Following that we have $y_2 = g(t)  y_1$. $G$ is an unknown and will eventually be substituted for $u = g'$. Following this we differentiate $y_2$ to get $$y_2' = g_1'  e^t + g_1  e^t.$$ We differentiate again for $$y_2'' = g_1''  e^t + 2g_2'  e^t + g_2  e^t.$$ Next, we substitute $y_2$, $y_2'$ and $y_2''$ into the given equation and also substitute $u=g'$ to get $$tu' +2ut -tu -u =0.$$ Solve this equation by separation and then integrate and get the solution of y_2. We find the last $y_2 = t+1$.","['reduction-of-order-ode', 'ordinary-differential-equations']"
2844791,What does it mean for an integral to be stationary?,I may have the wrong group.  I could not find calculus of variations and had to start somewhere. In the calculus of variations we start by finding the 0 points where the functions are at minimum or maximum. Is this the same as stationary that is referred to in standard textbooks ? i.e  where the integral = I[f] is stationary.   Can stationary be replaced with 0?   I get confused  when a new word is added to describe something that doesn't need a new word.  Or perhaps means something else.  If so can someone explain?,['multivariable-calculus']
2844811,Mean value theorem for a derivative being equal to the function,"I have the following task in my homework: Let $a,b \in \mathbb{R}, a < b $ and $f:[a,b] \to \mathbb{R} $ be a continuous function that is differentiable on $(a,b)$. Show that if $f'(x)=f(x)$ for all $x \in (a,b)$, then there is a $c \in \mathbb{R}$ with $f(x)=ce^x$ for all $x \in [a,b]$. Hint: Consider the function $x\mapsto f(x)e^{-x} $ I know that the two solutions here would be $c=1$ and $c=0$, but apparently, that has to be shown with the mean value theorem, and that hint just confuses me more. Any help would be appreciated!","['derivatives', 'real-analysis', 'ordinary-differential-equations']"
2844816,Find $\operatorname{Tr}(A^{2018})$ if $\det(A^2-2018I_2)=0$,"Find $\operatorname{Tr}(A^{2018})$ if $\det(A^2-2018I_2)=0, A\in M_2(\mathbb{Q})$ My attempt: Let $A^2=\begin{bmatrix}a&b\\c&d\end{bmatrix}$ then $B=A^2-2018I_2=\begin{bmatrix}a-2018&b\\c&d-2018\end{bmatrix}$ then $\det(B)=(a-2018)(d-2018)-bc$ then we have: $a = 2018$ or $d=2018$ AND $b=0$ or $c=0$. And I took the possible candidate for $A^2$ when all of these happen at the same time so: $a=2018,d=2018,b=0,c=0\implies \operatorname{Tr}(A^{2018})=2\times2018^{1009}$ by induction. Did I do everything right? I feel like my solution is not really that good. Also can you give me some advice that might help me in these kind of situation with problems like this?","['matrices', 'linear-algebra']"
2844854,Maxima and minima of $\operatorname{sinc}$ function,"The function $\operatorname{sinc}{\pi x}$ has maxima and minima given by the function's intersections with $\cos \pi x$, or alternatively by $\frac {d}{dx}\operatorname{sinc}{\pi x}=0$. Mathematica tells me that $$\frac {d}{dx}\operatorname{sinc}{\pi x}=\pi \Bigl(\frac {\cos \pi x}{\pi x}-\frac {\sin \pi x}{\pi^2 x^2}\Bigr)$$ So question 1, how do I prove this? And question 2, how do I derive an equation for all maxima and minima?","['derivatives', 'trigonometry']"
2844881,Is $\underset{i}\varprojlim R/(I+J)^i \cong \underset{i}\varprojlim \left( \underset{j}\varprojlim R/(I^i + J^j)\right)$?,"I am studying $I$-adic completions of a ring and I was wondering if the following holds for any commutative ring $R$ and ideals $I, J \subset R$ $\underset{i}\varprojlim R/(I+J)^i \cong \underset{i}\varprojlim \left( \underset{j}\varprojlim R/(I^i + J^j)\right)$ I don't know whether this is true or not, but I can't find a proof nor a counterexample of the above. Thank you!","['abstract-algebra', 'ring-theory', 'limits-colimits', 'commutative-algebra']"
2844926,How to obtain Asymptotic Expansion of a given function,"Today I was playing with this function
$$
\sqrt{1-x}
$$
and I found that if I try to approximate (near $x=0$) such a function with a polynomial of some order then I can find the same coefficients that WolframAlpha find. Starting with
$$
\sqrt{1-x}\sim a_0+a_1x+a_2x^2+a_3x^3+\cdots
$$
where $\cdots$ are terms of higher power in $x$, I squared both sides obtaining
$$
1-x\sim a_0^2+a_1^2x^2+a_2^2x^4+a_3^2x^6+2a_0a_1x+2a_0a_2x^2+2a_0a_3x^3+2a_1a_2x^3+2a_1a_3x^4+2a_2a_3x^5+\dots
$$
or
$$
1-x\sim a_0^2+2a_0a_1x+(a_1^2+2a_0a_2)x^2+(2a_0a_3+2a_1a_2)x^3+\dots
$$
where I choose to ignore terms $x^n$ with $n>3$. In this way I obtain an ""equivalence"" between two polynomials and imposing that each term of those polynomials are equal one find
$$
a_0^2=1,\,2a_0a_1=-1,\,a_1^2+2a_0a_2=0,\,2a_0a_3+2a_1a_2=0.
$$
Choosing $a_0=1$ one get
$$
\sqrt{1-x}\sim1-\frac{1}{2}x-\frac{1}{8}x^2-\frac{1}{16}x^3+\cdots.
$$
Is this method correct? I tried and it works also for $\frac{1}{1-x}$ or others irrational and rational functions!
I also would love if someone give some references.","['taylor-expansion', 'asymptotics', 'functions']"
2844950,$c_{00}$ is not contained in maximal ideal,"As the title says the problem is to show that $c_{00}$ is not contained in any maximal ideal ( $c_{00}$ is considered lying in $c_0$ ). I am not used nonunital algebras (rings) so I don't know how to approach to questions like this. Edit: $c_0 = \{ x_n \in \mathbb{C}, n \in \mathbb{N} : \lim x_n = 0 \}$ and $c_{00}$ is subspace (subalgebra) in $c_0$ with $x_n$ nonzero for only finitely  many indexes. I am sorry, I thought the notation was standard.","['functional-analysis', 'maximal-and-prime-ideals', 'banach-algebras', 'ideals']"
2844955,Analogue of complex infinity for quaternions,"The complex infinity is defined as a pole on the Riemann sphere, which is the result of the 1-point compactification of the complex plane. Considering that quaternions are an extension of complex numbers, and that unit quaternions are points on the 4D unit sphere, is there any analogue of this for quaternions, like quaternion infinity?","['riemann-surfaces', 'complex-numbers', 'quaternions', 'number-theory', 'infinity']"
2844961,Real content of Chebyshev sum inequality,"There's well-known Chebyshev's sum inequality . I'm looking for possible generalization of this statement; problem is that I'm not sure what is the right direction, but here's one which I'd prefer. Let $A$ be some $\Bbb R$ - or $\Bbb C$ -algebra with seminorm $|\cdot|$ and ""positive cone"" $P$ . $T$ is some operator (or parametric family $T_{\alpha}$ ) on $A$ satisfying several unknown properties ; maybe, it looks like a shift operator on $l^1(\Bbb Z)$ or parametric shift on $l^1(R)$ . Template of theorem (Generalized Chebyshev's sum inequality) If $a, b \in A$ are elements such that $(T-1)a$ and $(T-1)b$ are positive, then $|ab| \geq |a||b|$ If $x, y \in A$ are elements such that $(T-1)x$ and $(1-T)y$ are positive, then $|xy| \leq |x||y|$ Original inequality is obtained by taking $A = l^1(\Bbb Z), T$ being equal to shift, $P$ usual positive cone in $C^*$ sense and seminorm being stadard or $|x|_n := |x \cdot \chi_{[0, n]}|$ . With obvious tweaks one can adjust this to integral version on something like $L^1(\Bbb R)$ with limiting positivity condition for small forward difference operators. Question 1 . What's so special about shift operator? Question 2 . Is there some statement fitting the template and where $T$ does not look like a shift at all?","['inequality', 'c-star-algebras', 'normed-spaces', 'operator-theory', 'functional-analysis']"
2844962,"Is there a 3D shape with a flat face throughout which one would experience constant ""downward"" acceleration?","A spaceman restricted to the center of his platform A person standing on a thin disk in space will experience gravitational acceleration exactly normal to the surface only when he is situated exactly in the center. Is there a finitely large ""underlayment"" we can place underneath this platform such that if one were to walk on it one would experience a gravitational acceleration of constant magnitude and normal to the surface no matter where one is standing on it? What shape must this underlayment take, and can it be achieved using only material of uniform density? A spaceman and his companions experiencing the same acceleration at various points on the platform edit: The cross sectional area of the underlayment can be larger than the platform. edit: One way to approach this problem might be to first consider a ""2D"" version of the problem: a thin wafer with a flat edge may take any finite shape, with the goal being to have the gravitational acceleration perpendicular to the flat edge and of constant magnitude at all points along the edge. It may be the case that a solution to the 3D platform problem is simply a solid of revolution of the 2D solution.","['physics', 'mathematical-physics', 'calculus', 'geometry', 'classical-mechanics']"
2844987,Differentiability of $f$ if $f = \sum f_k$ and $f_k$ are differentiable at one point,"Let $f:U\rightarrow \mathbb{R}^n$ be a function, where $U\subset \mathbb{R}^m$ is open. Suppose there exist functions $f_k:U\rightarrow \mathbb{R}^n,$ $k\in\mathbb{N}$, such that for all $x\in U$:
$$f(x)=\sum_{k=1}^{\infty}f_k(x).$$ Moreover, suppose that each $f_k$ is differentiable at some point $x_0\in U$. Does this imply that $f$ is differentiable at $x_0$? Unfortunately, I do not know the answer. I checked some examples, and it seems to be correct. I would be thankful, if someone can help me.","['multivariable-calculus', 'real-analysis']"
2844990,"Show that $\left(\left\{x, -x, \frac{1}{x}, -\frac{1}{x}\right\}, \circ\right)$ is a group.","Given the following problem: Given the functions $g_1, g_2, g_3, g_4$ of $\mathbb{R}^*$ in $\mathbb{R}^*$ defined in the following way: $g_1(x) = -x$, $g_2(x) = -\frac{1}{x}$, $g_3(x) = x$ and $g_4(x) = \frac{1}{x}$. If $G = \{g_1, g_2, g_3, g_4\}$: Show that $(G, \circ)$ is a group where $\circ$ is the composition of functions. Write the table. Identify a generator set of $(G, \circ)$ that has the least number of elements possible. Extract all the normal subgroups of $(G, \circ)$. If $H$ is one of them, describe $G$ \ $H$. I am having lots of problems figuring out how proceed with such a set. If I understand correctly, the composition of functions is for example: $$
(\forall x\in\mathbb{R}^*):\enspace(g_1 \circ g_2)(x) = g_1(g_2(x)) = g_1(-\frac{1}{x}) = -(-\frac{1}{x}) = \frac{1}{x}
$$ Is that so? Then I know that to prove that $(G, \circ)$ is a group, I have to prove the following: The internal law: $g_i \circ g_j \in G$. Associativity: $g_i \circ (g_j \circ g_k) = (g_i \circ g_j) \circ g_k$. Existence of the neutral element $g_e$ such that: $g_i \circ g_e = g_e \circ g_i = g_i$. Existence of an inverse element $g_i'$ for each $g_i \in G$, we have that $g_i \circ g_i' = g_i' \circ g_i = g_e$ But how do I proceed with such a set?","['group-theory', 'linear-algebra', 'functions']"
2845002,Comparing the Kullback-Leibler divergence to the total variation distance on discrete probability densities.,"I am trying to get a clearer understanding on how the Kullback_Leibler divergence ranks distributions with respect to the total variation in the discrete setting. let $P,Q$ be two probability measures on $(\Omega, \mathscr {F})$, and let $\nu$ be a $\sigma$-finite measure on the same event space such that $P \ll v, Q \ll v$. Define $\frac{dP}{dv}=p$, $\frac{dQ}{dv}=q$. The total variation distance between P and Q is then: $$
V(P,Q) = \frac{1}{2} \int |p-q|d\nu
$$ (in the discrete case we replace the integral with a summation). It is very obvious geometrically what the total variation is measuring since it's fundamentally the $L^1$ distance and no ""special treatment"" is given for different values of $p(x)$ or $q(x)$. The Kullback-Leibler divergence is defined as: $$KL(P,Q) = -\int p \log{\frac{q}{p}} d\nu$$ I understand the information theoretic nature of this divergence (and know it is not symmetric or that the triangle inequality does not hold). What I am missing is how actually does this divergence rate distributions against one another. To get my point across I give an example, say I have three probability distributions $P_1,P_2,P_3$ s.t. $P_1( X = 0) = 1/4 , P_1( X = 1) = 1/2, P_1( X = 2) = 1/4 $ blue. $P_2( X = 0) = 1/3 , P_2( X = 1) = 1/3, P_2( X = 2) = 1/3 $ green. $P_3( X = 0) = 1/4 , P_3( X = 1) = 1/3, P_3( X = 2) = 5/12 $ light blue. The total variation distance between $P_1$ and $P_2$ is the same as the one between $P_1$ and $P_3$ this is geometrically intuitive since the sum of distances between the top of the charts in the two cases is the same. I would like to find a similar way to inspect the chart to quickly determine what should be the rankings for the Kullback-Leibler divergence. For example $KL(P_1,P_2) \approx 0,06$ and $KL(P_1,P_3) \approx 0,07$ but what is the explanation behind this ranking. Moreover when a discrete density assigns probability zero to a value the K-L divergence can completely miss the difference in the distributions since the convention is this case is that $x \log \frac{y}{x}|_{x = 0}= 0$. To cut it short I can't find a (geometric) way to compare the K-L divergence to a symmetric distance like the total variation and I am having some doubts on the validity of considering the K-L divergence a good measure of distance between distributions.","['real-analysis', 'probability', 'statistics', 'probability-theory']"
2845039,A standard deck of cards is shuffled and dealt. Find the probability that the last king appears on the 48th card?,A standard deck of cards is shuffled and dealt. Find the probability that the last king appears on the 48th card? I would like to verify if my reasoning for the solution is correct: I attempted to use hypergeometric distribution to solve this problem. The first half of the setup deals with drawing 47 cards of which exactly 3 are Kings and 44 are non-kings. The second half of setup completes the missing conditional probability of drawing the last king on the 48th card which is $\frac {1}{5}$ $\frac {\binom {4}{3}* \binom{48}{44}}{\binom {52}{47}}* \frac {1}{5}$ Is my reasoning correct? Thank you!,"['combinatorics', 'probability']"
2845048,Prove that two operators have a common eigenvector,"Suppose $S,T:\mathbb C^3\to\mathbb C^3$ are linear operators. The
  degree of the minimal polynomial of each of the operators is at most
  2. Show that they share a common eigenvector. I tried to exploit the condition on the degree. I obtained that there are two possible Jordan canonical forms for $S,T$. The first possibility is that the JCF is diagonal of the form $(a,a,b)$. The second possibility is that the JCF has one block of size 2 with eigenvalue $a$ and 1 block of size 1 with eigenvalue $a$. For the other operator the possibilities for the JCF are the same except that the eigenvalues may be different. But I don't know how to proceed from this point.","['eigenvalues-eigenvectors', 'jordan-normal-form', 'linear-algebra', 'linear-transformations']"
2845049,Convergence of decreasing decrements of sequences,"Let $\beta_m\searrow 0$ such that $\alpha_m:=\beta_m-\beta_{m+1}\searrow 0$. Define $b_n:=\inf\{m:\alpha_m<2^{-n}\}$. Is it true that
$$
\sum_{n=1}^\infty \frac{b_n}{2^n}<\infty?
$$ For example, if $\beta_m=\frac 1 m$, then $b_n\sim 2^{n/2}$, so that the above series converges. A critical case is when $\beta_m=1/\log m$, whence $\alpha_m\sim 1/m(\log m)^2$, and $b_n\sim 2^n/n^2$, so the series converges. Edited: I am sorry I had a typo: I meant $\beta_m:=1/\log m$, not $\alpha_m:=1/\log m$. In the latter case, this is a simple question. However, it is not in the former case.","['convergence-divergence', 'sequences-and-series', 'analysis']"
2845132,Find the Ricci tensor from given product metric quickly,"I am having trouble calculating the Ricci tensor. Suppose the product $I\times S^n$ has metric
$$g=dt^2+f^2(t)g_{S^{n}}$$
where $g_{S^{n}}$ is the standard metric on $S^{n}$, what is the Ricci tensor of this metric?
I have tried to do it for $n=2$ by writing the metric as
$$g=\begin{pmatrix}1&0&0\\0&f^2&0\\0&0&f^2\sin^2\theta\end{pmatrix}$$
In a standard way, I calculated the Christoffel symbols and using the definition of Riemann tensor, of course, It is not wise to do similar thing in higher dimension. I am thinking that since the hypershpere is a manifold of constant sectional curvature 1 and the Ricci tensor is $$Ric=(n-1)g_{S^{n}}$$ Can we write the Ric on $I\times S^n$ from the Ric on $S^n$ and the product factor $f(t)$ quickly? But I don't know how to proceed. Any hint will be appreciated.","['riemannian-geometry', 'differential-geometry']"
2845143,Morphism between thick fibers of schemes extends to a neighbourhood,"Let $ S $ be a locally Noetherian scheme, and $ X $, $ Y $ finite type $ S $-schemes. Let us fix $ s \in S $. Let $ \varphi : X \times _ { S }   \mathcal{O}_{S,s}   \to   Y   \times _ { S }   \mathcal{O}_{S,s} $ be a morphism of $ S $-schemes.   Show that there exists  an  open subset $ W \ni s $ of $ S $ and a morphism $ f : X \times _ { S }  W  \to   Y  \times _ { S } W $  such that $ \varphi $ is obtained   from $ f $ via   base  change  $ \text{Spec} \mathcal{O}_{S,s}   \to   W   $. If $ \varphi $ is an isomorphism, show that   there exists such an  $ f  $ which is moreover an isomorphism. 
$ \quad  $ P.S. This question is Exercise 2.3.5 from Qing Liu's book and is related to "" Extending a morphism of schemes "", "" Extending a morphism from Spec $\mathcal{O}_{X,x}$ "".    I am writing the solution below in order to record some of the details that initially trumped me.","['schemes', 'algebraic-geometry']"
2845177,Can we solve this recurrence relation?,"Consider the following recurrence relation
$$x_1=1\\ x_2=a\\ x_{n+2}=ad^nx_{n+1}-dx_n$$
where $a,x_n\in\mathbb{C}$ and $d\in\mathbb{R},d>1$. Is there any $a\in\mathbb{C}$ such that $\lim x_n=0$ ? or at least can
  we find a formula for $x_n$ depending only on $a$ and $d$?","['recurrence-relations', 'sequences-and-series', 'convergence-divergence', 'stability-theory']"
2845183,Recovering a quotient map of groups from the outer action of the quotient on the kernel,"Given any exact sequence of groups $$1\rightarrow A\rightarrow B\rightarrow C\rightarrow 1$$ one can define a natural outer action of $C$ on $A$, ie a homomorphism $\rho : C\rightarrow\text{Out}(A)$ given by lifting elements of $C$ to $B$ and restricting the conjugation action to the normal subgroup $A$. Now forget the above, and suppose we are given groups $A,C$ and a representation $\rho : C\rightarrow\text{Out}(A)$, and further suppose $A$ has trivial center. Does this data determine the group $B$ (up to isomorphism as an extension of $C$ by $A$)?",['group-theory']
2845192,a locally confluent and terminatting rewrite system is complete,"I want to prove that every locally confluent rewrite system is confluent. Since I know very little about rewrite systems and logic, I tried looking at it as a digraph with no external infinite paths and with the locally confluent property but I could only prove it in very specific cases such as when every path in the digraph has length two. 
Any hints to proving this would be very helpful.","['rewriting-systems', 'directed-graphs', 'logic', 'discrete-mathematics']"
2845247,"Finding a general formula for the sequence {$0, 1, 3, 5, 6, 7, 9, 15$}.","I'm working on a mathematical model which requires me to generalize the elements of an infinite set. The element n is the nth finite sequence of the set. For: n = 1: {0}

n = 2: {0, 1, 3, 5, 6, 7, 9, 15}

n = 3: {0, 1, 2, 5, 26, 29, 32, 81, 83, 87, 107, 112, 113, 116, 135, 140, 141, 142, 143, 161, 162, 194, 224, 351, 353, 356, 364, 365, 377, 647, 728, 1514, 1536, 1538, 1595, 1601, 1617, 1619, 1862, 2271, 2273, 2300, 2460, 2462, 2541, 2543, 2561, 3028, 3029, 3168, 3280, 3281, 3289, 3293, 3785, 3806, 3968, 4001, 4009, 4017, 4018, 4019, 4022, 4037, 4046, 4048, 4049, 4069, 4100, 4130, 4289, 4373, 5975, 6056, 6209, 6317, 6336, 6479, 6560, 6673, 6674, 7381, 8180, 9104, 9113, 9833, 9840, 9841, 9842, 9854, 10598, 10609, 10610, 11453, 12301, 13121, 13265, 13346, 14001, 14741, 15227, 15665, 16402, 17141, 17222, 17411, 18146, 18914, 19115, 19331, 19520, 19601, 19682} OEIS doesn't have the sequence n = 2 or n = 3 , let alone a master sequence, so I'm kinda stuck. Does anyone know any math magic to help me generalize the set of sequences. I can provide sequences for n > 3 if necessary. I can also provide the algorithm (c-code) and/or system of equations I'm using to generate these finite sequences. Weird, right? I have an indirect method for generating these sequences, but no direct function. If you don't have an answer, that's understandable, but any advice for a plan of attack would surely help. Thanks for your time, assuming you take the time to read this. Update as requested: Origin of the family of sequences: Generate an ordered, finite set $T_n:  T_n = \{A_1,...,A_\mu...,A_n\} $ , and an operator $\circ: A_p \circ A_q ∈ T_n$ . For $A_\mu ∈ T_n,\ P(A_\mu)$ is the position of $A_\mu$ in set $T_n$ . Further, let: $1. \ \ \ Q_k := (A_X \circ A_Y)_k$ $2.\ \ \ P(Q_k) = \left \lfloor{k\cdot n^{n*P(A_X)+P(A_Y)+1-{n^2}}} \right \rfloor\mod n:0\le k\lt n^{n^2}$ Then associativity of $\circ$ on the set $T_n$ requires that: $ \bigl((A_X \circ A_Y)_k \circ A_Z\bigr)_k = \bigl(A_X \circ (A_Y \circ A_Z)_k\bigr)_k$ . Once you select a given $n$ , we seek the instances $k$ for which the set $T_n$ is associative. There is typically more than one solution, $k$ . The solutions of k for a given n generate the sequences written above. Hence my dilemma: I have an algorithm for generating the numbers, but can't see through the math in order to find a general function that generates all of the finite sequences, and I'm hoping for a fresh perspective, maybe a technique or something, to hopefully find said function. Again, thanks for your input. I can do the work, but I'll appreciate a nudge in the right direction. :)","['finite-groups', 'group-theory', 'sequences-and-series']"
2845252,integrate $2\sqrt{1-x^2/4-y^2/9}$,"I am trying to find the volume of the ellipsoid $E$ given by $x^2/4 + y^2/9 + z^2 \leq 1$ by computing $\iiint_E dV$ I ended up with $$\int_{-2}^{2}\int_{-3\sqrt{1-x^2/4}}^{3\sqrt{1-x^2/4}}\int_{-\sqrt{1-x^2/4-y^2/9}}^{\sqrt{1-x^2/4-y^2/9}}1dzdydx$$ After doing the inner integral I get $$\int_{-2}^{2}\int_{-3\sqrt{1-x^2/4}}^{3\sqrt{1-x^2/4}}2\sqrt{1-x^2/4-y^2/9}\space dydx$$ I am not sure about how to figure out the next one The hint is to use trig substitution, though I am not sure if it is talking about this step or the next one","['multivariable-calculus', 'volume']"
2845262,Advantages/disadvantages of Morita over Tu,"I plan on studying manifolds and differential geometry. I have heard good things about Morita's Geometry of Differential Forms/Characteristic Classes and Tu's Introduction to Manifolds/Differential Geometry . However, I do wonder about the pedagogical/topic coverage advantages/disadvantages between both pairs of books. Thus I am looking for an honest comprasion between the two pairs. For example, would it be better if I choose one pair to study and find more advanced references for topics not present in the chosen pair but in the other pair(example, Hodge theory in Morita, but not in Tu, etc.)? Perhaps one may also mention which pair of books have their errors/typos corrected/have less errors and typos(Tu's Manifolds 2nd edition has a 3 page errata PDF on Google, whereas Morita's errata PDF is nowhere to be found, if ti exists). My background is linear and abstract algebra, point set and algebraic topology(Hatcher), analysis from Robert Gunnig's MAT 218 notes, and classical differential geometry from O'Neill. Suggestions for books not mentioned here based off of my background/other consideratoins are welcome!","['reference-request', 'characteristic-classes', 'book-recommendation', 'manifolds', 'differential-geometry']"
2845288,Existence of elementary integral because of simple product rule?,"Let's say I wish to evaluate $$\int\frac{\sin(x)}{x}+\log(x)\cos(x)\ dx$$and$$\int\frac{xe^{2x}}{(1+2x)^2}\ dx$$ I can recognize at once the integrals as antiderivatives, or results of the product rule, quotient rule, power rule, etc. For example in the first integral, $$\frac{d(\sin(x)\log(x)}{\ dx}=\frac{\sin(x)}{x}+\log(x)\cos(x)$$ thus $$\int\frac{\sin(x)}{x}+\log(x)\cos(x)\ dx= \sin(x)\log(x) +C$$ The same is true for the second integral by a multiplicative constant, $$\frac{\ d\big(\frac{e^{2x}}{4(1+2x)}\big)}{dx}=\frac{xe^{2x}}{(1+2x)^2}$$ thus $$\int\frac{xe^{2x}}{(1+2x)^2}\ dx=\frac{e^{2x}}{4(1+2x)}+C$$ My question is, does there exist an elementary path for these integrals, by that I mean can these integrals be  explicitly evaluated in an easy way, if there exists a simple product rule, quotient rule, power rule, etc  derivation such as the ones above? I tried the first one for a long time but couldn't find a way to  combine the fractions and make a substitution. Thanks in advance!","['derivatives', 'indefinite-integrals', 'calculus']"
2845308,"How to prove that $x\ln\left(\frac{e^x+1}{e^x-1}\right)$ tends to $0$ as $x\to0,\infty$","How could it be shown that $$\lim_{x\to0}\left[x\ln\left(\frac{e^x+1}{e^x-1}\right)\right]=\lim_{x\to\infty}\left[x\ln\left(\frac{e^x+1}{e^x-1}\right)\right]=0\quad?$$ Note that when $x=0$, we have $x=0$ (obviously) and $\ln\left(\frac{e^x+1}{e^x-1}\right)=\ln\left(\frac20\right)$ which is indeterminate and when $x\to\infty$, we have $x\to\infty$ (obviously) and $\ln\left(\frac{e^x+1}{e^x-1}\right)=\ln\left(1+\frac2{e^x-1}\right)\to\ln1=0$. So it is not possible to just multiply both. I can't see L'Hopital working as the fraction in $\ln$ expands to a sum, not a fraction. Here's a plot of the function in Desmos. Any approaches?",['limits']
2845342,"Even $p \in \mathbb{R}[x,y]$ satisfying $p(\mathbb{R},\mathbb{R}) \geq 0$","Let $p=p(x,y) \in \mathbb{R}[x,y]$ be a two variable polynomial over $\mathbb{R}$.
Assume that: (1) All monomials in $p$ are of (total) even degrees, namely, $p$ is of the form: $p=p_{00}+p_{20}x^2+p_{11}xy+p_{02}y^2+\cdots$, where $p_{ij} \in \mathbb{R}$. (2) For all $x,y \in \mathbb{R}$, $p(x,y) \geq 0$. Is it possible to say something interesting about the coefficients of such $p$? I guess that there should be some relation between the coefficients $p_{ij}$ and the degrees of the monomials, though it seems quite complicated to find it. Remark: This question is relevant. Edit: What if the $p_{ij}$'s belong to one of the following four sets: $\{1,-1,0\}$; $\mathbb{Z}$; $\mathbb{N}$; $\mathbb{R}^{+}$? For example: In the special case $p=a+bx^2+cxy+dy^2$, with $a,b,c,d \in \mathbb{R}^{+}$, if I am not wrong, such $p$ will be positive if $c< min \{b,d\}$ and $b-c+d \geq 0$ (the second condition can be replaced with $a>|b-c+d|$). Perhaps (if I am not missing something) the situation is similar in higher degrees, for example:
$p=a+bx^2+cxy+dy^2+ex^4+fx^3y+gx^2y^2+hxy^3+iy^4$, with all the coefficients in $\mathbb{R}^{+}$ will be positive if $c < \min \{e,g,i\}$, $\max \{f,h \} < \min \{e,g,i\}$, $b-c+d \geq 0$ and $e-f+g-h+i \geq 0$ (the third and fourth conditions can be replaced by $a > 2 \max\{|b-c+d|, |e-f+g-h+i|\}$). It seems that similar arguments will be valid for a more general case, in which only the highest degree $m$ is required to be even, without restriction on the parity of lower degrees (but still under the assumption that all the coefficients belong to $\mathbb{R}^{+}$, of course). Any hints and comments are welcome! Edit: This paper more or less answers my question, by applying Proposition 1.15 to each homogeneous component of my given polynomial $p$. If there are $r$ homogeneous components in $p$, then we have $r$ conditions, if all satisfied, then $p$ is positive. (Hankel quadratic form, mentioned in its last section, also sounds interesting). However, the ideas in that paper are new to me, so any help in showing how to apply them in practice is welcome.","['real-analysis', 'polynomials', 'algebraic-geometry']"
2845375,Why do we want Dedekind rings to be integral closed?,"As I understand, the idea of Dedekind domains is motivated by the wish to factorize ideals into prime ideals.
Dedekind rings are supposed to: be noetherian, which makes sense because that ensures that the factorization is finite; have every prime ideal to be a maximal ideal, which makes sense because we want to factorize into prime ideals, so they need to be ""very"" big. integral closed. Can anybody give me an intuitive idea about why we need that property?","['abstract-algebra', 'dedekind-domain', 'commutative-algebra']"
2845458,"Reference for ""Discrete differential Geometry""","Can you suggest some references for ""Discrete differential geometry"" for beginners? My background is Elementary Differential Geometry (Curves and Surfaces)/ Computational Geometry / Numerical Methods, what specifically I'm looking for is a good reference that explaines how to discretize operators tipically defined in differential geometry, when we model our geometry as a mesh.","['reference-request', 'numerical-methods', 'computational-geometry', 'differential-geometry']"
2845464,"If $a_n\sim b_n$ decrease to $0$ and $a_{n+1}b_n-a_nb_{n+1}$ changes sign i.o. , does it imply the same for $(1-a_{n+1})(1-b_n)-(1-a_n)(1-b_{n+1})$?","Suppose $a_n<b_n$ for all $n$, both are strictly decreasing to $0$ and $a_n\sim b_n$. If $A_n=a_{n+1}b_n-a_nb_{n+1}$ changes sign infinitely often, does it follow that $B_n=(1-a_{n+1})(1-b_n)-(1-a_n)(1-b_{n+1})$ also does? In some sense, the implication should not be there, because while $A_n<0$ does not seem to be crucial for $B_n$, I have found that whenever $A_n>0$ we have $B_n<0$: the former is equivalent to $$a_{n+1}>a_n\frac{b_{n+1}}{b_n}$$ and since for large enough $n$, $0<a_n<1, a_n<b_n<1, 0<b_{n+1}<b_n$ this is stronger than $$a_{n+1}>1-\frac{1-a_n}{1-b_n}(1-b_{n+1}),$$ equivalent to $B_n<0$. $A_n<0$ does not seem to be crucial because it should be possible to have this happen without making $a_{n+1}$ so small that $B_n$ becomes positive. On the other hand, maybe the fact that $A_n$ changes sign infinitely often gives more weight to the instances of $A_n<0$. Not for too long, but I have tried to construct some simple $A_n,B_n$ contradicting the claim and I have not found them yet. Are there any, with $B_n<0$? Besides, how does the situation change if $a_n-b_n$ is also assumed to change sign infinitely often?","['real-analysis', 'examples-counterexamples', 'sequences-and-series']"
