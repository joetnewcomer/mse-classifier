question_id,title,body,tags
945788,Exponential growth's power,"If the number of Ebola cases are doubling every three weeks, how long before everyone in the world ($7$ billion population) will be infected? Sorry, I forgot to add that currently we have $3000$ cases. I'm just an English major but math layman. I think this may be too scary for many of us to consider. My friend offered this: $2^n=7000000000=7*10^9$
Solve for $n$
Then
$3n=$number of weeks",['statistics']
945805,"Equation simplification, can't get it right","$$\frac{1}{\frac{x-1}{x+2}}-\frac{2}{x^2-1}$$ should be simplified into $$\frac{x^2+3x}{x^2-1} \quad .$$ However, when I try to do it (tried several times), I fail to get it done right: $$\frac{1}{\frac{x-1}{x+2}}-\frac{2}{x^2-1} = 1 * \frac{x+2}{x-1} - \frac{2}{x^2-1} $$  $$=\frac{x(x+2)-2}{x^2-1} = \frac{x^2+2x-2}{x^2-1}$$ What am I doing wrong? Thanks a lot for the help","['fractions', 'algebra-precalculus']"
945815,"How do I show $[\mathbb{Q}(\sqrt{2},\sqrt{3}):\mathbb{Q}]\geq[\mathbb{Q}(\sqrt2):\mathbb{Q}][\mathbb{Q}(\sqrt3):\mathbb{Q}]$?","I know how to show $[\mathbb{Q}(\sqrt{2},\sqrt{3}):\mathbb{Q}]\leq[\mathbb{Q}(\sqrt2):\mathbb{Q}][\mathbb{Q}(\sqrt3):\mathbb{Q}]$, but don't know how to show the converse inequality. $[\mathbb{Q}(\sqrt2):\mathbb{Q}]$ and $[\mathbb{Q}(\sqrt3):\mathbb{Q}] $ both divides $[\mathbb{Q}(\sqrt{2},\sqrt{3}):\mathbb{Q}]$, but they are not relatively prime, so I guess I can't the converse inequality this way. I'm considering using the fact that the intersection of $\mathbb{Q}(\sqrt2)$ and $\mathbb{Q}(\sqrt 3)$ is $\mathbb{Q}$, but I don't know how to proceed...",['abstract-algebra']
945847,"Is every Lipschitz continuous function is holder continuous with exponent $\in (0,1)$?","Is every Lipschitz continuous function is holder continuous with exponent $\in (0,1)$? This seems to be true,but I haven't found such a conclusion in any textbook.","['real-analysis', 'analysis']"
945848,"If $x * x = e$ for all $x$ and $(x_i * x_j) * (x_j*x_k) = x_i*x_k$, then $*$ is associative","This should be simple, but for some reason I get stuck on this. Let $G = \{x_1, \ldots x_n\}$ be a set equipped with operation $*$ satisfying the following : 1) $G$ has an identity element $e$ with $e *x = x = x * e$ for all $x \in G$. 2) All elements are self-inverse (i.e. $x * x = e$ for all $x \in G$). Prove that $*$ is associative if and only if for all $i,j,k$, $(x_i * x_j) * (x_j * x_k) = x_i * x_k$. It's the $\Leftarrow$ part that's bugging me.
From 2) I can show that the multiplication matrix of $G$ is a Latin Square.
And I think $*$ is commutative since $(x_i * x_j) * (x_j * x_i) = x_i * x_i = e \Rightarrow x_i * x_j = x_j * x_i$. But I can't see how this leads to associativity.","['associativity', 'group-theory']"
945871,Derivative of Softmax loss function,"I am trying to wrap my head around back-propagation in a neural network with a Softmax classifier, which uses the Softmax function: \begin{equation}
p_j = \frac{e^{o_j}}{\sum_k e^{o_k}}
\end{equation} This is used in a loss function of the form \begin{equation}L = -\sum_j y_j \log p_j,\end{equation} where $o$ is a vector. I need the derivative of $L$ with respect to $o$. Now if my derivatives are right, \begin{equation}
\frac{\partial p_j}{\partial o_i} = p_i(1 - p_i),\quad i = j
\end{equation} and \begin{equation}
\frac{\partial p_j}{\partial o_i} = -p_i p_j,\quad i \neq j.
\end{equation} Using this result we obtain \begin{eqnarray}
\frac{\partial L}{\partial o_i} &=& - \left (y_i (1 - p_i) + \sum_{k\neq i}-p_k y_k \right )\\
&=&p_i y_i - y_i + \sum_{k\neq i} p_k y_k\\
&=& \left (\sum_i p_i y_i \right ) - y_i
\end{eqnarray} According to slides I'm using, however, the result should be \begin{equation}
\frac{\partial L}{\partial o_i} = p_i - y_i.
\end{equation} Can someone please tell me where I'm going wrong?","['linear-algebra', 'machine-learning', 'derivatives']"
945905,I got stucked in middle of the problem. How to find the value of radius 'x' cm from the given figure?,"Firstly, I calculated the area of sector $AOB$ by applying $\frac{1}{2}\times (1.2\ \text{radians})\times 20^{2}$ (formula for area of sector of circle) and calculated area of sector $AOB= 240\ cm^2$ , tried bit of construction by joining $PC$ and $CR$ thus reaching $CP \perp AO$ and $CR \perp CR$ . Also, tried Joining $PR$ to form $\triangle POR$ but couldn't have the idea to find the value of ' $x$ '. Give it a try.","['geometry', 'circles']"
945928,Generalized inverse/Pseudo Inverse,"Let $A_{m. n}$ be a matrix with rank $p$ where $p\leq m$ and $p\leq n$. First Question: We need to show that $A$ can be decomposed as a product of two matrices $A=BC$ where $B$ is an $m$ by $p$ and $C$ is an $p$ by $n$ and both $B$ and $C$ have rank $p$. Second question: We define the generalized inverse of $A$ by $A^{+}=C^{T}(CC^{T})^{-1}(B^{T}B)^{-1}B^{T}$. Show that the solution to the equations: $Ax=b$ (in case it exists) is given by: $x=A^{+}b+e$ where $Ae=0$ and that the solution that has the smallest norm is given by: $x=A^{+}b$. For the first question, I tried to decompose $A$ using the generalized QR decomposition where $Q$ has $p$ linearly independent columns and $R$ has all entries below the main diagonal equal to $0$. The issue is I don't know how to prove such decomposition in the case of this problem because not all columns of $A$ are linearly independent. If anyone has another way to decompose $A$ like required by the statement of the problem, and that makes solving part 2 easy, please let me know. For part 2, I have no clue how to do it, so any help is appreciated. Thanks!","['numerical-linear-algebra', 'matrices', 'linear-algebra', 'least-squares']"
945929,Are these graphs isomorphic?,Are these 2 graphs isomorphic? (sorry for the bad picture quality) For the solution: 1) They both have 8 vertices 2) They both have 12 edges 3) They both have 8 vertices of degree 3 4) Is this enough to prove isomorphism?,"['discrete-mathematics', 'graph-isomorphism']"
945952,On why we have $dy = f'(x)dx$?,"I am following Ordinary differential equations by Tenenbaum. Page 48 The differential is defined as: $$dy(x,\Delta x) = f'(x) \Delta x$$ Note: we will want to apply this definition to the function defined by $y = x$. Therefore, in order to distinguish between the function defined by $y = x$ and the variable x, we place the symbol $\hat{}$ over the x so that:   $y = \hat{x}$ will define the function that assigns to each value of the independent variable x the same unique value to the dependent variable y. (in a Cartesian plane a horizontal line?) Theorem 6.2 If, $$y = \hat{x}$$ then $$dy(x,\Delta x)= (d\hat{x})(x,\Delta x) = \Delta x$$ Straight after this comes the thing I do not clearly understand, the book says: Comment 6.3: Replace the value found for $\Delta x$ (from theorem 6.2) in the definition of differential, we obtain: $$dy(x,\Delta x)= f'(x)(d\hat{x})(x,\Delta x) $$ ...
this relation is the correct one, but in the course of time, it became customary to write it down in the familiar form: $$dy = f'(x)dx$$ The book then proceeds to use this formula (that I know is correct) in any case. But wouldn’t this formula be only relevant in the case $y = \hat{x}$ since it was found relying on theorem 6.2 that is only valid if $y = \hat{x}$?","['notation', 'ordinary-differential-equations']"
945972,Relations connecting values of the polylogarithm $\operatorname{Li}_n$ at rational points,"The polylogarithm is defined by the series
$$\operatorname{Li}_n(x)=\sum_{k=1}^\infty\frac{x^k}{k^n}.$$
There are relations connecting values of the polylogarithm at certain rational points in the interval
$(0,1)$ using combinations of logarithms and values of
$\zeta$-function of integer arguments. Here are some examples for small integer
orders: $$\pi^2-12\ln^22+12\ln2\cdot\ln3-6\ln^23-12\operatorname{Li}_2\!\left(\tfrac13\right)-6\operatorname{Li}_2\!\left(\tfrac14\right)=0$$ $$2\!\;\pi^2\ln2-4\!\;\pi^2\ln3+8\ln^32-12\ln2\cdot\ln^23+8\ln^33\\+45\,\zeta(3)-24\operatorname{Li}_3\!\left(\tfrac13\right)-24\operatorname{Li}_3\!\left(\tfrac23\right)-6\operatorname{Li}_3\!\left(\tfrac14\right)=0$$ $$8\,\pi^4\ln2-12\,\pi^2\ln^32+18\ln^52-1209\,\zeta(5)\\+1728\operatorname{Li}_5\!\left(\tfrac12\right)-486\operatorname{Li}_5\!\left(\tfrac14\right)-48\operatorname{Li}_5\!\left(\tfrac18\right)+3\operatorname{Li}_5\!\left(\tfrac1{64}\right)=0$$ Are there any known similar relations for $\operatorname{Li}_6$ and
higher orders? I know there are some relations (""ladders"") for non-rational algebraic arguments, but now I'm interested in rational arguments only. MathWorld has formula $(19)$ attributed to Bailey et al. that is apparently supposed to hold for any positive integer order, but it does not check numerically for $m>5$. Perhaps, there is a missing term or condition. ( Update: Indeed, the original paper has this identity as formula $(2.16)$ saying it only holds for $1\le m\le5$, and attributes it to Lewin)","['special-functions', 'sequences-and-series', 'calculus', 'logarithms', 'polylogarithm']"
945999,What are {} and [] functions?,"I see these functions in Bondy-Murty book about graph theory... In book written: ""The complete m-partite graph on n vertices in which each part has either [n/m] or {n/m} vertices is donated by T m,n. Show that..."" Can someone explain me what are these functions?","['notation', 'functions']"
946001,Suppose that $f(x) \ge 0$ and $\lim_{x \to c} f(x) = L$. Prove $\lim_{x \to c} \sqrt{f(x)} = \sqrt{L}$,"Suppose that $f(x) \ge 0$ in some deleted neighborhood of $c$, and that $\lim_{x \to c} f(x) = L$. Prove that $\lim_{x \to c} \sqrt{f(x)} = \sqrt{L}$ under the two different assumptions on $L$: $L=0$ and $L>0$","['convergence-divergence', 'real-analysis', 'limits']"
946040,Properties of the solutions to $x'=t-x^2$,"Let $f_c$ be the solution to 
$$
\left\{ 
\begin{array}{c}
x'=t-x^2 \\ 
x(0) =c 
\end{array}
\right. 
$$ I'm trying to prove: If $c \geq 0$ then $f_c(t)$ is defined for all $t>0$ There is a unique $c_0$ such that $f_{c_0}(t)$ is defined for all $t>0$ and $\lim_{t\to +\infty} (f_{c_0}(t)+\sqrt t) =0$.",['ordinary-differential-equations']
946063,Tough trigonometric identity: $\cot 13^\circ\cot 23^\circ \tan 31^\circ\tan35^\circ\tan41^\circ = \tan 75^\circ$,"Prove that $$\cot 13^o\cot 23^o \tan 31^o\tan35^o\tan41^o = \tan 75^o$$ I managed to rearrange it to the form $$\tan 31^o\tan 35^o\cot 49^o = \cot 15^o\tan 23^o\cot 77^o$$
and in this form we have the interesting property that the sum of arguments on both sides is equal, i.e. 31+35+49=15+23+77. I couldn't get past this stage, so I would appreciate any help. EDIT: I found that $\tan x\tan(60^o-x)\tan (60^o+x)=\tan 3x$. Perhaps someone can use this to solve the problem. I myself haven't been able to. Thanks.","['trigonometry', 'algebra-precalculus']"
946065,Is the number of subsequential limits of a sequence always countable,I know that a sequence can have many different subsequential limits but is the number of subsequential limits always countable? How do we know?,['analysis']
946071,A problem of Ramanujan's interest: closed form of $1 + 2\sum_{n=1}^{\infty} \frac{\cosh(n\theta)}{\cosh(n\pi)} $,"I am Brian Diaz, and I am new to the math.stackexchange community. I have been struggling with attempting to find a closed form of the following series: $$ \varphi(\theta) = 1 + 2\sum_{n=1}^{\infty} \frac{\cosh(n\theta)}{\cosh(n\pi)} $$ Admittedly, I attempted to convert it to a ""workable integral"", but to no avail. Heck, in the process of converting it to an integral, I am not even sure interchanging the sum and the integral was valid. Nevertheless, this was my result.
$$\frac{1}{\pi}\int_{-\infty}^{\infty} \frac{\sin(x)}{\cosh(\theta) - \cos(x)} \frac{1}{\cosh(x)}dx $$ This was derived from a problem Ramanujan was working. For those who are interested in the source, you can visit http://mathworld.wolfram.com/RamanujanCosCoshIdentity.html . Note: Even if it does not have a closed form, I am still interested in valuable insight to the problem. In addition, I have been reported by my professor to consider applying residue theory, though he his not so sure what the result would be. Thank you so much for your support, and I hope you do have a blessed day!","['improper-integrals', 'closed-form', 'sequences-and-series', 'hyperbolic-functions', 'trigonometric-series']"
946101,When does a variety have a point over a finite field for sufficiently large primes p?,"Let $X$ be an algebraic variety over the rational numbers. Suppose that $X$ has positive dimension. I would like to say that $X(\mathbb{F}_p)$ is non-empty for sufficiently large primes $p$. One idea is to use the Weil conjectures, but that seems like overkill. So my two questions are: When can one deduce that a system of polynomial equations has a solution over $\mathbb{F}_p$ for sufficiently large primes $p$? I might have guessed that $\dim(X) > 0 $ is sufficient, but it seems like the affine variety $X^2+Y^2 = 0$ minus the point $(0,0)$ (which has no solutions for $p = -1 \mod 4$) suggests that extra conditions are necessary. Is there an elementary proof of the (correct) version of part 1?","['arithmetic-geometry', 'algebraic-geometry', 'number-theory']"
946142,Subject GRE question - set of points of discontinuity,"I was just working on a Math Subject GRE practice test, and I got the following problem wrong: Let $f$ be the function defined on the real line by $\displaystyle f(x) = \begin{cases} \displaystyle \frac{x}{2} &\mbox{if } x \text{ is rational}, \\ 
\displaystyle \frac{x}{3} & \mbox{if } x \text{ is irrational}. \end{cases}$ If $D$ is the set of points of discontinuity of $f$, then $D$ is the $(A)\, \text{Empty set} \\
(B)\, \text{set of rational numbers}\\
(C)\, \text{set of irrational numbers}\\
(D)\, \text{set of nonzero real numbers}\\
(E)\, \text{set of real numbers}\\ $ I chose $(E)$, the set of real numbers, since this set is clearly discontinuous at both every rational number and every irrational number.  However, the answer key told me the answer is $(D)$, the set of nonzero real numbers. I find this very confusing because, as far as I know, $0$ is a rational number, since it can be written in the form $\displaystyle \frac{0}{q}$, for any integer $q$. Can $0$ also be irrational?  I've been looking online to figure that out, and everywhere I've looked has told me that $0$ is only rational.  Are they all wrong, or is there some other reason why $0$ is not a point of discontinuity? I'm very confused :(","['gre-exam', 'calculus', 'real-analysis', 'analysis']"
946148,"Is there a group between $SO(2,\mathbb{R})$ and $SL(2,\mathbb{R})$?","Is there a non-trivial subgroup $H \subset SL(2,\mathbb{R})$ such that $H \supset SO(2,\mathbb{R})$ ? My intuition is that, since $\dim SO(2)=1$ and $\dim SL(2)=3$, there should be some group between, but I can't point out one. Note : in the complex case, $H:= SO(2,\mathbb{C}) \cup \left\{ \left( \matrix{ a&b \\ b&-a} \right) ~|~ a^2+b^2=-1 \right\}$ is an example.","['lie-groups', 'algebraic-groups', 'group-theory']"
946156,Proving convexity of a function whose Hessian is positive semidefinite over a convex set,"Let $C$ be a convex set in $\mathbb{R}^n$ and let $f:{\mathbb{R}}^n \rightarrow \mathbb{R}$ be twice continuously differentiable over $C$ . The Hessian of $f$ is positive semidefinite over $C$ , and I want to show that $f$ is therefore a convex function. I am currently trying to apply Taylor's Theorem to replace $f(x)$ with an expression that includes its Hessian.","['optimization', 'convex-optimization', 'continuity', 'derivatives']"
946164,$n$ linearly independent rows of Vandermonde matrix,"Consider the ""infinite"" Vandermonde matrix 
$$
V (x_1, x_2, \ldots , x_n) =
\begin{pmatrix}
  1      & x_1    & x_1^2  & \cdots & x_1^{n-1} & x_1^n & x_1^{n+1} & \cdots \\
  1      & x_2    & x_2^2  & \cdots & x_2^{n-1} & x_2^n & x_1^{n+1} & \cdots \\
  1      & x_3    & x_3^2  & \cdots & x_3^{n-1} & x_3^n & x_1^{n+1} & \cdots \\
  \vdots & \vdots & \vdots & \ddots & \vdots    & \vdots & \vdots & \cdots \\
  1      & x_n    & x_n^2  & \cdots & x_n^{n-1} & x_n^n & x_n^{n+1} & \cdots
\end{pmatrix}
$$
with distinct $x_1, \dots, x_n$. It is well-known that if we pick the first $n$ columns, then they span the whole space. I was wondering, if the same holds true if one picks arbitrary (not necessary consecutive) $n$ columns in the above ""infinite"" Vandermonde matrix. I looked at simple examples and they suggest that this is true. Is there a nice way of proving this?","['linear-algebra', 'real-analysis']"
946228,How to prove a tempered distribution is in $L^p(\mathbb{R}^n)$,"Given $g \in L^p(\mathbb{R}^n)$, how can I to prove that the tempered distribution  $$f=\mathcal{F}^{-1}[(z-4\pi^2|x|^2)^{-1}\mathcal{F}g]$$ is in $L^{p}(\mathbb{R}^n)$ where $z \in \{u \in \mathbb{C}-\mathbb{R}_{+};Re(u) \geq 0\}, x \in \mathbb{R}^n$ and $\mathcal{F}$ and $\mathcal{F}^{-1}$ denotes the Fourier Transforms and Inverse Fouries Tranformes respectively? This problems appears in a proof line of theorem 2.3.3, pg 40, from book ""The Theory of Fractional Powers of Operators"", by C. Martínez Carracedo and M. Sanz Alix.","['distribution-theory', 'ordinary-differential-equations', 'functional-analysis', 'lp-spaces']"
946247,Collection of all bounded linear operators from $X$ into $Y$ is a normed linear space and is a Banach space if $Y$ is a Banach space,"Let $X$ and $Y$ be normed linear spaces, and let $B(X,Y)$ be the collection of all bounded linear operators from $X$ into $Y$ with the operator norm. Show that $B(X,Y)$ is a normed linear space, and $B(X,Y)$ is a Banach space if $Y$ is a Banach space. The vector operations in $B(X,Y)$ are to be defined pointwise: $(A+B)(x) = Ax + Bx$ and $(\alpha A)(x)=\alpha (Ax)$. Here is what I got so far Let $\{B_k\}^\infty_{k=1}$ be a Cauchy sequence in $B(X,Y)$. Then for each $x\in X$, $\|B_nx-B_mx\|\leq\|B_n-B_m\|\|x\|\to 0$ as $m,n\to\infty$. So $\{B_k\}^\infty_{k=1}$ is Cauchy in $Y$. As $Y$ is complete there exists $Bx\in Y$ such that $lim_{k\to\infty}\|B_kx-Bx\|=0$. Using the above, let $x_1,x_2\in X$ and $\alpha,\beta\in \mathbb{R}$. Then: $B(\alpha x_1+\beta x_2)=lim_{k\to\infty}B_k(\alpha x_1+\beta x_2)=lim_{k\to\infty}(\alpha B_k(x_1)+\beta B_k(x_2))=\alpha B(x_1)+\beta B(x_2)$ ...? Not really sure how to go from there, to showing that $B(X,Y)$ is a normed linear space, or a Banach space. Any suggestions? Thanks!",['functional-analysis']
946280,nth derivative of $e^{-x}\sin(x)$,"I'm trying but no luck. Can't find a pattern yet.
The exercise is to find the nth derivative of $e^{-x}\sin(x)$ probably by induction.","['calculus', 'derivatives']"
946288,An open set in $l^{\infty}$,"Let $X$ denote the set of all bounded real sequences, equipped with the norm $\| (x_n)\|_\infty:= \sup\{|x_1|,|x_2|,|x_3|,\ldots\}$; Let $X_{++}$ denote the set of all bounded positive real sequences (“sup” is supremum).
Is $X_{++}$ an open subset of $X$? Would anyone please tell me the answer to this question and provide suggestions or hints on how to prove it?","['general-topology', 'normed-spaces', 'functional-analysis', 'lp-spaces']"
946311,A convex function is differentiable at all but countably many points,"Let $f:\Bbb R\to\Bbb R$ be a convex function. Then $f$ is differentiable at all but countably many points. It is clear that a convex function can be non-differentiable at countably many points, for example $f(x)=\int\lfloor x\rfloor\,dx$. I just made this theorem up, but my heuristic for why it should be true is that the only possible non-differentiable singularities I can imagine in a convex function are corners, and these involve a jump discontinuity in the derivative, so since the derivative is increasing (where it is defined), you get an inequality like $f'(y)-f'(x)\ge \sum_{t\in(x,y)}\omega_t(f')$, where $\omega_t(f')$ is the oscillation at $t$ (limit from right minus limit from left) and the sum is over all real numbers between $x$ and $y$. Since the sum is convergent (assuming that $x\le y$ are points such that $f$ is differentiable at $x$ and $y$ so that this makes sense), there can only be countably many values in the sum which are non-zero, and at all other points the oscillation is zero and so the derivative exists. Thus there are only countably many non-differentiable points in the interval $(x,y)$, so as long as there is a suitable sequence $(x_n)\to-\infty$, $(y_n)\to\infty$ of differentiable points, the total number of non-differentiable points is a countable union of countable sets, which is countable. Furthermore, I would conjecture that the set of non-differentiable points has empty interior-of-closure, i.e. you can't make a function that is non-differentiable at the rational numbers, but as the above discussion shows there are still a lot of holes in the proof (and I'm making a lot of unjustified assumptions regarding the derivative already being somewhat well-defined). Does anyone know how to approach such a statement?","['convex-analysis', 'derivatives', 'real-analysis']"
946339,Constructing a countable dense subset of a totally bounded set,"Given a metric space $(X,d)$, and (non-empty) totally bounded set $E$ in $X$, is it possible to construct $D \subseteq E$ which is countable and dense? I feel that this should definitely be possible. Since $E$ is totally bounded, I suspect we need to pick the $\epsilon$-balls that cover $E$ in such a way that the centres of them are dense (and assumed to be in $E$). I've thought about picking $\epsilon>0$ so that every $\epsilon$-ball contains the centre of another $\epsilon$-ball, but this does not guarantee the density we require. I'm not sure which other $\epsilon$ to try.","['general-topology', 'metric-spaces', 'analysis']"
946381,"If all directional derivatives are $0$, the function is constant.","Let $f:\mathbb{R}^{n} \rightarrow \mathbb{R}$ be differentiable in every point of the disc $
B_{r}(\vec{a})$. If $D_{\vec{y}}f(\vec{x})=0$ for $n$ linearly independent vectors $\vec{y}_{1}, \vec{y}_{2},...,\vec{y}_{n}$ and $\forall \vec{x} \in B_{r}(\vec{a})$, show that $f$ is constant. I've proven the following statement: Let $f$ be a function of $n$ variables such that the directional derivatives $D_{\vec{y}}f(\vec{a}+t\vec{y})$ exist $\forall t \in [0,1]$. Then, there exists a real number $\theta \in ]0,1[$ such that: \begin{equation}
f(\vec{a}+\vec{y})-f(\vec{a}) =  D_{\vec{y}}f(\vec{a}+\theta \vec{y}) 
\end{equation} I believe that this question may be answered by means of a corollary of this last statement, or by using a similar version of the Mean Value Theorem . I would appreciate any help.","['derivatives', 'real-analysis']"
946383,Show that $\log \left| z \right|$ is harmonic and find its the conjugate harmonic function.,"Is the form correct for the conjugate harmonic? Attempt: First, we are given
\begin{align*}
\log \left| z \right| &= u(x,y) + iv(x,y) = \log \sqrt{x^2 + y^2} + i \cdot 0 \\
u(x,y) &= \log \sqrt{x^2 + y^2} = \frac{1}{2} \log (x^2 + y^2).
\end{align*}
Then we differential to get $u_{xx}$ and $u_{yy}$,
\begin{align*}
u_{xx} = \frac{\partial u}{\partial x} \frac{x}{x^2 + y^2} = \frac{y^2 - x^2}{(x^2 + y^2)^2} \\
u_{yy} =  \frac{\partial u}{\partial y} \frac{y}{x^2 + y^2} = \frac{x^2 - y^2}{(x^2 + y^2)^2}
\end{align*}
From here, we can see that $u_{xx} + u_{yy} = 0$. Thus, we have shown that $\log \left| z \right|$ is harmonic.
Using the Cauchy-Riemann equations, we can find the conjugate harmonic function $v$. This gives us the relationships
\begin{align*}
u_x &= v_y = \frac{x}{x^2 + y^2} \\
u_y &= -v_x = \frac{y}{x^2 + y^2}.
\end{align*}
Integrate with respect to $y$ to get $v(x,y) = \tan^{-1}(\frac{y}{x}) + C$, the conjugate harmonic function.","['harmonic-functions', 'proof-verification', 'complex-analysis']"
946387,I don't understand the calculus problem from my note?,I don't understand the highlight part in my note from class. Why we have to multiply this? Can someone explain it for me? Why $n-1$ power? Thanks!,"['multivariable-calculus', 'calculus']"
946411,Multivariable optimization - how to parametrize a boundary?,"A metal plate has the shape of the region $x^2 + y^2 \leq  1$ . The plate is heated so that the temperature at any point $(x,y)$ on it is indicated by $$T(x,y) = 2x^2 + y^2 - y + 3$$ Find the hottest and coldest points on the plate and the temperature at each of these points. (Hint: Parametrize the boundary of the plate in order to find any critical points there.) I know how to do the actual optimization part of this problem. I already found a critical point at (0,0.5) by setting the first partial derivatives equal to 0. My problem is, how do I parametrize the boundary to find the other ones? I've seen solutions where they used cos(t) and sin(t) - where and how did they know how to do that?","['optimization', 'multivariable-calculus', 'parametric']"
946466,Is it true that every bounded sequence with the following property converges?,Is it true that every bounded sequence $\{a_n\}$ of real numbers such that $|{a_n - a_{n-1}}|<1/n$ for all $\ge2$ is convergent?,['analysis']
946477,Lists versus sets in linear algebra,"I’m currently learning linear algebra from “Linear Algebra Done Right” by Sheldon Axler. The author, in his proofs, makes use of lists of vectors, as opposed to the more conventional usage of sets of vectors. I have some questions concerning this:- Are lists standard in linear algebra? I mean, I have referred to many other books, and all of them seem to use sets rather than lists. If I study linear algebra at a higher level, will my current study using lists prove to be a hindrance? In essence, I mean to ask whether results in higher linear algebra texts make use of lists or rather sets, in their proofs? Aren’t certain results more cumbersome to prove using lists? As an example, consider this statement: If $S$ is a linearly independent set in $V$, and $x \notin span(S)$, then prove that $S \cup \{x\}$ is linearly independent. This is fairly easy to prove considering $S$ as a set. But when it comes to an analogous result for lists, since the order matters, wouldn’t there be many possibilities of adjoining $x$ to the list? And for each of these possibilities, wouldn’t I have to prove that the list is linearly independent? If $x$ is adjoined at the very end of the list, this follows easily from the linear dependence lemma, but what if $x$ is adjoined to the list at some arbitrary position? Wouldn’t statements like this, which involve adjoining (or equally, removal) of vectors be more cumbersome to prove, in the case of lists? Are there other disadvantages of using lists over sets? Frankly, I am in love with Axler’s book and his simple, clean proofs using lists, over other linear algebra texts at this level. But I’m worried that this very simplicity is going to prove troublesome when I decide to study linear algebra at the graduate level. I apologize if this isn’t the right place to post this, but I haven’t seen any discussion regarding this anywhere else, and I thought someone here might be able to give insights.","['linear-algebra', 'soft-question']"
946505,differentiating a function of a function $w=\sqrt{u^2+v^2}$,"I want to find the total differential of $w$ from a given function $w=\sqrt{u^2+v^2}\:with\:u\:=\:cos\left(ln\left(tan\left(p+\frac{1}{2}\pi \right)\right)\right)\:and\:v\:=\:sin\left(ln\left(tan\left(p+\frac{1}{2}\pi \:\right)\right)\right)$ to solving this problem I'm using $\left(a\right)\:\:\:\:\frac{dw}{dp}=\frac{∂w}{∂u}\cdot \frac{du}{dp}+\frac{∂w}{∂v}\cdot \frac{dv}{dp}$ first I'm solving for the partial derivatives of $w$ with respect to $u$ and $v$ on RHS $\left(b\right)\:\:\:\:\frac{∂w}{∂u}=\frac{u}{\sqrt{u^2+v^2}}\:\:\:\:and\:\:\:\:\frac{∂w}{∂v}=\frac{v}{\sqrt{u^2+v^2}}$ and then I'm solving for the total derivatives of $u$ and $v$ with respect to $p$ on RHS using chain rule. $\frac{du}{dp}=\frac{d}{dx}cos\left(ln\left(tan\left(p+\frac{1}{2}\pi \right)\right)\right)=\frac{du}{dx}\cdot \frac{dx}{dy}\cdot \frac{dy}{dp}$ let $x=ln\left(tan\left(p+\frac{1}{2}\pi \:\right)\right)\:$ and $y\:=tan\left(p+\frac{1}{2}\pi \right)$ $\left(1\right)\:\frac{du}{dx}=\frac{dcos\left(x\right)}{dx}=-sin\left(x\right)=-sin\left(ln\left(tan\left(p+\frac{1}{2}\pi \right)\right)\right)$ $\left(2\right)\:\frac{dx}{dy}=\frac{dln\left(y\right)}{dy}=\frac{1}{y}=\frac{1}{tan\left(p+\frac{1}{2}\pi \right)}$ $\left(3\right)\:\frac{dy}{dp}=\frac{dtan\left(p+\frac{1}{2}\pi \right)}{dp}=sec^2\left(p+\frac{1}{2}\pi \right)$ substitute back $(1)$ , $(2)$ , and $(3)$ into $\frac{du}{dp}$ i get $\left(c\right)\:\:\:\:\frac{du}{dp}=-\frac{sin\left(ln\left(tan\left(p+\frac{1}{2}\pi \right)\right)\right)}{tan\left(p+\frac{1}{2}\pi \:\right)cos^2\left(p+\frac{1}{2}\pi \:\right)}$ now i want to find $\frac{dv}{dp}$ $\frac{dv}{dp}=\frac{dsin\left(ln\left(tan\left(p+\frac{1}{2}\pi \right)\right)\right)}{dp}=\frac{dv}{dx}\cdot \frac{dx}{dy}\cdot \frac{dy}{dp}$ since $\frac{dx}{dy}$ and $\frac{dy}{dp}$ have the same result as $(2)$ and $(3)$ , now i only have to find $\frac{dv}{dx}$ with $v\:=\:sin\left(x\right)$ i get $\frac{dv}{dx}=cos\left(x\right)=cos\left(ln\left(tan\left(p+\frac{1}{2}\pi \right)\right)\right)$ and then substitute them back into $\frac{dv}{dp}$ yields $\left(d\right)\:\:\:\:\frac{dv}{dp}=\frac{cos\left(ln\left(tan\left(p+\frac{1}{2}\pi \right)\right)\right)}{tan\left(p+\frac{1}{2}\pi \:\right)cos^2\left(p+\frac{1}{2}\pi \:\right)}$ finally, substitute $(b)$ , $(c)$ , and $(d)$ into $(a)$ i get $\frac{dw}{dp}=\frac{cot\left(p+\frac{1}{2}\pi \:\right)\cdot sec^2\left(p+\frac{1}{2}\pi \:\right)}{\sqrt{u^2+v^2}}\left(v\cdot cos\left(ln\left(tan\left(p+\frac{1}{2}\pi \:\right)\right)\right)-u\cdot sin\left(ln\left(tan\left(p+\frac{1}{2}\pi \:\:\right)\right)\right)\right)$ this is my answer, but i'm not sure is there an error in my calculation? and how to know if my answer is correct? thank you for your time and advice, it has nothing to do with homework or school, i just have no one to ask to correct my answer.","['calculus', 'chain-rule', 'derivatives']"
946512,Ring with convolution product,"Let $M$ be a monoid and $R$ a ring, $f,g \in R^{(M)}$ (the functions from $M$ to $R$ with finite support), we define the convolution product as $$x\in M \implies (f*g)(x)=\sum_{yz=x} f(y)g(z).$$ Show that $R[M]:=(R^{(M)},+,*)$ is a ring (the operation $+$ is defined by $(f+g)(x)=f(x)+_Rg(x)$. I could prove that $R[M]$ is an abelian group with $+$ and the distribution property. I had problems proving that $R[M]$ is a monoid with $*$. I've tried to show that $f$ defined as $f(x)=1_R$ for all $x\in M$ is the identity for $*$, so if I take $g$, then $$(f*g)(x)=\sum_{yz=x}f(y)g(z)=\sum_{yz=x}1_R.g(z)=\sum_{yz=x}g(z).$$ I would like to conclude that $\sum_{yz=x}g(z)=g(x)$. I suppose that this is true since this sum doesn't depend on $y$ but only on $z$, so instead of $yz=x$, it is $z=x$ Would this be correct? My major doubt is about the associativity: let $f,g,h$, then $$((f*g)*h)(x)=\sum_{yz=x}((f*g)(y))h(z)=\sum_{yz=x}(\sum_{wt=y}f(w)g(t))h(z).$$ I got stuck at that point. I've already verified closure under $*$, I would appreciate if someone could tell me how to prove associativity and to check if what I've done for existence of identity element under $*$ is correct.","['ring-theory', 'abstract-algebra']"
946518,Vector Calculus intuition: Why is the magnitude of a velocity vector the speed?,"From my understanding of basic Calculus (which could very well be completely flawed), the derivative of position with respect to time would give us the slope at every point of that function, which would be the speed at that point. In that case, there is no notion of magnitude of vector since the slope is just a measure of inclination of the tangent line at that point. I don't understand how the magnitude of a velocity vector in a vector-valued function gives us the speed at a point, the equivalent of the slope of a tangent line in a ""normal"" non-vector-valued function. Why does that make sense? Sorry if the functional-analysis tag is wrong, I wasn't sure but it seemed to fit the subject.","['multivariable-calculus', 'vector-analysis']"
946524,A triangle determinant that is always zero,"How do we prove, without actually expanding, that $$\begin{vmatrix} 
\sin {2A}& \sin {C}& \sin {B}\\
\sin{C}& \sin{2B}& \sin {A}\\
\sin{B}& \sin{A}& \sin{2C}
\end{vmatrix}=0$$ where $A,B,C$ are angles of a triangle? I tried adding and subtracting from the rows and columns and I even tried using the sine rule, but to no avail.","['geometry', 'contest-math', 'triangles', 'trigonometry', 'determinant']"
946527,"Can the partial derivative of f(x,y) at (a,b) exist if f(x,y) is not continuous at (a,b)?","Suppose f(x,y) is continuous for all $(x,y) \neq (a,b)$, (not continuous at (a,b)), can the partial derivative with respect to x (or y) at (a,b) still exist?",['multivariable-calculus']
946530,Satisfying Cauchy Riemann equations at discontinuity,"What does it mean for a function to satisfy Cauchy-Riemann equations at a point discontinuity? The Cauchy-Riemann equations are about the partial derivatives of a function satisfying particular condition(s), but how would one calculate the partial derivatives at all at that point? Do you just take the limit of the partial derivative as one approaches that point?","['cauchy-riemann-equations', 'continuity', 'complex-analysis']"
946536,General form for finding tangent that intersects a point not on the curve,"Particular cases of this problem have previously been addressed here and here , but I'm interested in the general case of the following problem: Given a function $f(x)$ and a point $P = (x_0, y_0)$, find all tangents to $f(x)$ that pass through $P$. I tried to work on this on my own, and using point-slope form of lines I found: $$f(x) - y_0 = f'(x)(x - x_0)$$ Replacing $f(x)$ with $y$ and $f'(x)$ with $y'$: $$y - y_0 = y' (x - x_0)$$ $$y' = \frac{y - y_0}{x - x_0}$$ Trying to get it into the friendly form of a first-order linear differential equation: $$y' - \frac{y}{x - x_0} = -\frac{y_0}{x - x_0}$$ $$y' + \frac{y}{x_0 - x} = \frac{y_0}{x_0 - x}$$ But when I worked this out, I found $y = y_0$, which doesn't seem right, but my differential equation skills are admittedly very rusty. How do I find the tangents through $P$? What properties of $f(x)$ and $P$ make a solution unique or nonexistent? Where could I learn more about this problem?","['ordinary-differential-equations', 'calculus']"
946550,Proving maximum of dot product using derivatives,"I am curious to know whether there is a way to prove that the maximum of the dot product occurs when two vectors are parallel to each other using derivatives. In particular, given: $c = \textbf{a}\cdot\textbf{b}$ with $\textbf{a},\textbf{b}\in\mathbb{R}^3$ Assuming that $\textbf{b}$ is fixed, and I can only change $\textbf{a} = (x,y,z)^T$, how would one go about it? I would not even know how to properly take the derivatives in this case. This is not homework and because of that the question as I put it might be ill-posed, I apologise in advance.",['multivariable-calculus']
946573,Simple example which uses Borel-Cantelli lemma,"I read the Borel-Cantelli lemma, but I am not that familiar with probability. I would like to see this lemma with an illustration of a simple example. I am not too interested to see the proof of this lemma, but for my understanding, I would like to see this through examples. Whilst searching online for applications of the lemma, I couldn't find any article in Amer. Math. Monthly, of Mathematics Gazette, or other, but only research papers, which are hard to read for me 'now'. 
Also, while studying limit superior and limit inferior defined for sequence of sets, the examples usually involved are coming from the Borel-Cantelli lemma.
I will be happy if one explains this lemma by a simple example.","['measure-theory', 'limsup-and-liminf', 'probability-theory', 'analysis', 'borel-cantelli-lemmas']"
946580,Lattice which is not bounded lattice,I want to find an example of a lattice which is not a bounded lattice . Diagrams would be good with an explanation .,"['lattice-orders', 'discrete-mathematics']"
946623,Equality of sigma-algebras,"Let $X$ be a set, $\mathcal{C} \subseteq X$ and $\mathcal{A}$ a $\sigma$-algebra on $X$.
Is it true or false that $\sigma(\mathcal{C}) \cap \mathcal{A} = \sigma(\mathcal{C} \cap \mathcal{A})$? Of course, $\supseteq$ is true. I found a trivial counter-example for the other direction: Choose $\mathcal{C} = \{ A, B \}$ and $\mathcal{A} = \sigma(A \cap B)$, where $A \cap B \neq \emptyset$. Then $\mathcal{C} \cap \mathcal{A} = \emptyset$, $\sigma( \mathcal{C} \cap \mathcal{A} ) = \left\{ \emptyset , X \right\}$ but $\sigma(\mathcal{C}) \cap \mathcal{A} = \mathcal{A}$. The problem with the counter-example is that the generator $\mathcal{C} \cap \mathcal{A}$ is an empty set.
If we forbid an empty set as a generator, is the statement then true?
Or more generally, under what typical circumstances does the equality of the $\sigma$-algebras hold? I need to check, whether the following statement is true:
Let $\mathcal{E} \subseteq \mathcal{C}$ be two generators. Then
$\mathcal{C} \cap \mathcal{A} \subseteq \mathcal{E} \Rightarrow \sigma(\mathcal{C}) \cap \mathcal{A} \subseteq \sigma(\mathcal{E})$.
I tried to prove with a monotone class argument, but had then problems to show the union property of the Dynkin class $\left\{ E \subseteq X \ | \ E \in \mathcal{A} \Leftrightarrow E \in \sigma(\mathcal{E}) \right\}$.
The first statement above is just necessary in order to prove the second one.","['probability-theory', 'measure-theory']"
946628,Polynomial satisfying $ P \big(P (x)\big)=P (x)+ P\big(x^2\big)$,"If $P(x)$ is a polynomial with integer coefficients such that for all integer $x$, $$P (P (x)) = P (x)+P (x^2).$$ 
I've tried solving it putting it as a function instead. Not much though. How do you find such polynomials?","['algebra-precalculus', 'contest-math', 'functions', 'functional-equations', 'polynomials']"
946686,"Two sequences $a_{2n}=a_n+1, a_{2n+1}=a_{n}+2$ and $b_{3n}=b_n+1, b_{3n+1}=b_n+2, b_{3n+2}=b_n+3$","Let us consider two  sequences $$a_{2n}=a_n+1, a_{2n+1}=a_{n}+2, a_1=1,a_2=2$$ and  $$b_{3n}=b_n+1, b_{3n+1}=b_n+2, b_{3n+2}=b_n+3, b_1=1,b_2=2,b_3=2.$$ Prove that $a_{2^n} < b_{2^n}$ for $n>3.$ My attempts to prove it by mathematical induction were without success.","['algebra-precalculus', 'combinatorics']"
946704,The intersection of a subgroup $H$ and one of its conjugates is always a subgroup of finite index in $H$,$G$ is a group in which every subgroup has a finite number of conjugates. Let $H$ be a subgroup of $G$. Then I have to prove that the intersection of $H$ and one of its conjugates is always a subgroup of finite index in $H$. I get no clue how to prove it. Plz help me. Thank you.,"['group-theory', 'abstract-algebra']"
946712,Proving the summation formula using induction: $\sum_{k=1}^n \frac{1}{k(k+1)} = 1-\frac{1}{n+1}$,"I am trying to prove the summation formula using induction: $$\sum_{k=1}^n \frac{1}{k(k+1)} = 1-\frac{1}{n+1}$$ So far I have... Base case: Let n=1 and test $\frac{1}{k(k+1)} = 1-\frac{1}{n+1}$ $\frac{1}{1(1+1)} = 1-\frac{1}{1+1}$ $\frac{1}{2} = \frac{1}{2}$ True for n=1 Induction Hypothesis: Assume the statement is true for the n-th case $\sum_{k=1}^n  \frac{1}{k(k+1)} = 1-\frac{1}{n+1}$ Inductive Step: Prove, using the Inductive Hypothesis as a premise, that $$\sum_{k=1}^{n+1}  \frac{1}{k(k+1)} = \sum_{k=1}^{n}  \frac{1}{k(k+1)} + \frac{1}{(n+1)(n+2)} = 1-\frac1{n+1} + \frac{1}{(n+1)(n+2)} = \frac{(n+1)(n+2)}{(n+1)(n+2)}+\frac{-2-n}{(n+1)(n+2)}+\frac{1}{(n+1)(n+2)} = \frac{(n+1)(n+2)-2-n+1)}{(n+1)(n+2)} = \frac{(n+1)(n+2)-n-1}{(n+1)(n+2)} = \frac{n^2+2n+1}{(n+1)(n+2)} = \frac{(n+1)(n+1)}{(n+1)(n+2)} = \frac{n+1}{n+2}$$ To prove
$$ 1-\frac{1}{n+2} = \frac{n+1}{n+2} $$ Multiply both sides by $n+2$ to get an equivalent expression. $$ (1-\frac{1}{n+2}) * (n+2) = (\frac{n+1}{n+2}) * (n+2)  $$
$$ n+1=n+2−1 $$ Does this all make sense? How can this be improved upon?","['induction', 'summation', 'discrete-mathematics']"
946738,Nice Question in Mathmatics about Times,"I ran into a nice question from one book in Discrete Mathematics. I want to someone lean me how solve such a problem, because I prepare for entrance exam. if the time is ""Wednesday 4 afternoon"", after $47^{74}$ hours, we are in what hours? and what day? Thanks to all.","['discrete-mathematics', 'contest-math', 'number-theory', 'modular-arithmetic', 'computer-science']"
946752,Can you somehow combine the probabilities of two events?,"I know P(A|X) and P(A|Y), and I'm interested to compute/estimate P(A|X,Y). I suspect it cannot be done directly: if I know that 30% of women have blue eyes and 20% of english people have blue eyes, I can't directly estimate how many english women have blue eyes because I'd find out the percentage of women in England; OTOH, if I know the percentage of women in England then I don't care how many english people have blue eyes. I guess my question is - can I combine these two informations somehow? What is the minimum (additional) information that I need to find out (or estimate) in order to compute P(A|X,Y) ?","['statistics', 'probability']"
946776,Show that sum of elements of rows / columns of a matrix is equal to reciprocal of sum of elements of rows/colums of its inverse matrix,"Suppose $A=(a_{ij})_{n\times n}$ be a non singular matrix. Suppose sum of elements of each row is $k\neq 0$, then the sum of elements of rows of $A^{-1}$ is $\frac{1}{k}$. Let    $\,A^{-1}=(b_{ij})_{n\times n}$. Then \begin{align*}
&A\left[ \begin{array}{c} 1\\ 1\\ 1\\\vdots \\1 \end{array}\right]=k\left[ \begin{array}{c} 1\\ 1\\ 1\\\vdots \\1 \end{array}\right]\\
\implies &\frac{1}{k}A^{-1}A\left[ \begin{array}{c} 1\\ 1\\ 1\\\vdots \\1 \end{array}\right]=\frac{1}{k}A^{-1}k\left[ \begin{array}{c} 1\\ 1\\ 1\\\vdots \\1 \end{array}\right]\\
\implies &\frac{1}{k}I_n\left[ \begin{array}{c} 1\\ 1\\ 1\\\vdots \\1 \end{array}\right]=A^{-1}\left[ \begin{array}{c} 1\\ 1\\ 1\\\vdots \\1 \end{array}\right]\\
\implies &\frac{1}{k}I_n\left[ \begin{array}{c} 1\\ 1\\ 1\\\vdots \\1 \end{array}\right]=(b_{ij})\left[ \begin{array}{c} 1\\ 1\\ 1\\\vdots \\1 \end{array}\right]\\
\implies& (b_{ij})\left[ \begin{array}{c} 1\\ 1\\ 1\\\vdots \\1 \end{array}\right]=\frac{1}{k}I_n\left[ \begin{array}{c} 1\\ 1\\ 1\\\vdots \\1 \end{array}\right]\\
\implies &(\displaystyle\sum_{r=1}^n b_{1r}, \displaystyle\sum_{r=1}^n b_{2r},\cdots, \displaystyle\sum_{r=1}^n b_{nr})=(\frac{1}{k}, \frac{1}{k}\cdots, \frac{1}{k})\\
\implies & \displaystyle\sum_{r=1}^n b_{ir}=\frac{1}{k}, \forall i=1,2,\cdots,n 
 \end{align*}","['matrices', 'inverse']"
946785,Is this a valid use of l'Hospital's Rule? Can it be used recursively?,"$$\lim_{x \to 0} \frac{1}{x} + \frac{1}{x}$$
$$\lim_{x \to 0} \frac{2x}{x^2}$$ Since this evaluates to an indeterminate form $\frac{0}{0}$ we use l'Hospital's Rule: $$\lim_{x \to 0} \frac{2}{2x}$$ Since this also evaluates to an indeterminate form $\frac{2}{0}$ we use l'Hospital's Rule again: $$\lim_{x \to 0} \frac{0}{2}=0$$ I know that I could have simply divided both numerator and denominator by $x$ to get the same result. This is just an example to ask the question: Can l'Hospital's Rule be used recursively? EDIT: Sorry, I messed up my example. I cannot think of a good example right now but the question still stands. Can one use l'Hospital's Rule recursively?","['calculus', 'limits']"
946796,"propositional logic , writing the statement in terms of propositional variables","I came across this question while practicing  propositional logic Consider the argument: “If Anna can cancan or Kant can’t
cant, then Greville will cavil vilely. If Greville will cavil vilely, Will will want.
But Will won’t want. Therefore Kant can cant.” By writing the statement in
quotes as a proposition in terms of four propositional variables and simplifying,
show that it is a tautology and hence that the argument holds I am not sure how to write the the proposition for both the sentences as one proposition.
please help me with this problem.","['logic', 'discrete-mathematics']"
946825,"Integrals $\int \frac{1}{\operatorname{arctanh}(x)} \, dx$ and $\int \frac{1}{\operatorname{arccoth}(x)} \, dx$","Do we know anything about this integrals? $$
\begin{align}
I_1(x) = \int \frac{1}{\operatorname{artanh}(x)} \, dx \\
I_2(x) = \int \frac{1}{\operatorname{arcoth}(x)} \, dx
\end{align}$$ Similar integrals. $$
\begin{align}
\int \operatorname{artanh}(x) \, dx & = x \operatorname{artanh}(x) + \frac12 \ln(1-x^2) + C, \\
\int \operatorname{arcoth}(x) \, dx & = x \operatorname{arcoth}(x) + \frac12 \ln(1-x^2) + C, \\
\int \frac{1}{\operatorname{arsinh}(x)} \, dx & = \operatorname{Chi}(\operatorname{arsinh}(x)) + C, \\
\int \frac{1}{\operatorname{arcosh}(x)} \, dx & = \operatorname{Shi}(\operatorname{arcosh}(x)) + C, \\
\end{align}$$ where $\operatorname{Chi}$ is the hyperbolic cosine integral , and $\operatorname{Shi}$ is the hyperbolic sine integral . I found nothing with Maple or Mathematica . As I see some kind of ""hyperbolic tangent integral"" is not defined.","['closed-form', 'calculus', 'integration', 'indefinite-integrals']"
946832,"If $f$ is differentiable everywhere, is $f'$ the weak derivative?","Let $f \in C^0([0,T])$ be such that $f'$ exists in the classical sense everywhere, but $f'$ may not be continuous. Is it true that $f'$ is the weak derivative of $f$ too, if it exists? I know this is true if $f \in C^1$, but I don't have that..","['distribution-theory', 'weak-derivatives', 'real-analysis', 'analysis']"
946840,Number of Elements Not divisble by 3 or 5 or 7 [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question if A={1,2,...,600} includes all natural numbers between 1 to 600. I want to find number of elements of A that not divisible by 3 or 5 or 7? any hint or idea?",['discrete-mathematics']
946849,"unirational complex variety has $H^i(X,O_X) = 0$ for i > 0","Let $X/\mathbf{C}$ be a smooth projective connected unirational variety. Why do we then have $H^i(X,O_X) = 0$ for $i > 0$?","['algebraic-geometry', 'hodge-theory']"
946877,If $y = a\sin{x} + b\cos{x} +C$ then find maxima and minima for $y$.,I was able to solve it till $$y = \sqrt{(a^2 + b^2)}\sin(\alpha + x)  +  C.$$ But I don't know how to find maxima and minima from here. If $C = 0$ then maxima & minima equals the amplitude of the sine curve but when $C$ is non-zero then? I need help from here onwards.,['trigonometry']
946911,Minimize the Frobenius norm of the difference of two matrices with respect to matrix: $\underset{B} {\mathrm{argmin}} \left\| A- B \right\|_F$,"The following question is similar to this one, but I think that it is not straightforward to move from one to the other, so please take a look. Otherwise, please let me know and I will delete it. Let $A,B\in\Bbb{R}^{n\times n}$ two square real $n\times n$ matrices with the additional properties that $A$ is also symmetric, and $B$ is diagonal with entries $\{b_i\colon b_i\in\Bbb{R}, i=1,\ldots,n\}$, that is, $B=\operatorname{diag}(b_1,\ldots,b_n)$. We want to minimize the Frobenius norm of the difference of $A$, $B$ with respect to the matrix $B$. Let $B^{\star}$ denote the minimizer of the aforementioned norm, that is
$$
B^{\star} = \underset{B} {\mathrm{argmin}} \left\| A- B \right\|_F,
$$
where $\lVert A\rVert_F$ denotes the Frobenius norm of an $n\times n$ real symmetric matrix $A=\big(a_{ij}\big)_{i,j=1}^n$, and is given by
$$
\lVert A\rVert_F
=
\left(\sum_{i,j=1}^n \left\| a_{ij} \right\|^2\right)^{\frac{1}{2}}
=
\sqrt{\operatorname{tr}(A^\top A)}
=
\sqrt{\operatorname{tr}(AA^\top)}.
$$","['optimization', 'matrices', 'linear-algebra']"
946913,Trigonometric Identity for tan?,"Can somebody please help with this (probably simple) query. Given 
$$dx = R\,d(\tan\theta)$$ this can be expressed as
$$dx = R\,\sec^2\theta\,d\theta$$ I can't determine where the $\sec^2\theta\,d\theta$ terms are coming from.  I know there is the following Trigonometric Identity for $\tan\theta$ : $$\tan^2\theta = \sec^2\theta - 1$$ But for some reason I'm struggling! Not necessarily after the answer but some pointers would be appreciated. Thankyou.","['geometry', 'derivatives']"
946933,The role of valuation rings in algebraic geometry,"I am familiar with basic algebraic geometry in the tradition of Hartshorne's book. Discrete valuation rings appear there in the criteria for separatedness/properness, and are used to define the order of a pole/zero of a meromorphic function on a nonsingular variety. Furthermore, points of a nonsingular projective curve are in bijection with valuation rings on of the function field. In Ravi Vakil's notes on algebraic geometry there is a quote of a letter from Serre to Grothendieck. It appears that Grothendieck disliked valuation rings, whereas Weil thought that valuation rings should play a central role. I have heard from other sources that valued fields have applications in geometry. My questions are: 1) What is the importance of valuation rings in algebraic geometry other than what I mentioned in the first paragraph? For example, does the topology on a valuation ring play a role? Are valuation rings which are non-discrete important? Can they be used in intersection theory? 2) Is there any old paper, accessible as much as possible to someone who is familiar with the language of schemes, where valuation rings / valued fields are used?","['valuation-theory', 'algebraic-geometry']"
946956,How to prove whether this function is continuous or not?,"$$f(x)=
\begin{cases}
2, \mbox{  } (\forall) \mbox{ } x\in \mathbb{Q}\\
-2, \mbox{  } (\forall) \mbox{ } x\in \mathbb{R}\setminus \mathbb{Q}
\end{cases}
$$ Usually piece wise functions can be checked for continuity by taking limit at the end points of the intervals. But, how to do this problem? Please give me some hints.","['continuity', 'functions']"
947007,Book with novel approaches to analysis,"Now I'm studying Rudin's Principles of mathematical analysis , but I'm searching for a book that offers geometric, physical or otherwise non-standard approaches to topics in analysis. Also, I'm looking for some book (like Bell's), that describe calculus techniques from a novel perspective (possibly emphasizing their applications). Note : I'm not searching only for books that emphasize the application of analysisto physics, but also the other way round: a book that emphasizes the applications of physical arguments or geometry or any other non-standard approach to solve problems that should require a ""standard"" technique. For example, something like New Horizons in Geometry .","['geometry', 'book-recommendation', 'calculus', 'reference-request', 'soft-question']"
947029,To prove that the sum of the roots of the characteristic polynomial of a square matrix is equal to the trace of the matrix,"How do we prove that the sum of the roots of the characteristic polynomial of a square matrix is equal to the trace of the matrix ? I want a proof which does not use much computation or determinants ; please help , Thanks in Advance .","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
947035,How to show $-\sup(-A)=\inf(A)$?,"Let $\emptyset\neq A\subseteq\mathbb{R}$ a bounded set. Consider $-A=\{-a:a\in A\}$. I want to prove that $-\sup(-A)=\inf(A)$. It is easy to see that $-\inf(A)$ is an upper bound of $-A$, so $\sup(-A)\le -\inf(A)$, then $-\sup(-A)\ge \inf(A)$. How can we prove that $-\sup(-A)\le \inf(A)$? Thanks.","['supremum-and-infimum', 'real-analysis']"
947052,If $Y$ is measurable with respect to $\sigma (X)$ then there is a measurable function $f$ so $f(X)=Y$ - Stuck in Proof.,"I have $X$ an $\mathbb{R}^n$ random variable, and $Y$ is $\mathbb{R}$ valued that is measurable with respect to $\sigma (X)$.  I'm trying to follow a proof showing that there is a Borel measurable function $f$ on $\mathbb{R}^n$ such that $Y=f(X)$. So where I am at is a sequence of simple random variables $Y_n$ converging up to $Y$, and each $Y_n$ has the function $f_n$ for the above result, and the part I'm stuck at is actually on the convergence of the functions. The book States: Set $f(x) = \lim \sup_{n\rightarrow \infty} f_n(x)$ and then $Y$ = $\displaystyle \lim_{n\rightarrow \infty} Y_n = \lim_n f_n(X)$. But $(\displaystyle \lim \sup_{n \rightarrow \infty} f_n)(X) = \lim \sup_n (f_n(X)$, and since $\lim \sup_{n \rightarrow \infty} f_n(X)$ is Borel measurable, we are done. I'm very confused by the notation at each step and what exactly is being said.  For instance, I would have began this part of the proof by defining $f(x)=\lim_{n \rightarrow \infty} f_n(x)$, and then say $Y$ = $\displaystyle \lim_{n\rightarrow \infty} Y_n = \lim_n f_n(X)=f(X)$. Thanks.",['probability-theory']
947071,Identities for differential forms and vectorfields (reference request),"Recently I found the slides of a talk of J. E. Marsden, ( Differential Forms and Stokes' Theorem ). These slides introduce the required objects and summarize the basics of the corresponding theory. In the last part some very useful formulas (identities) are given, which establish the relation between objects (vector fields and differential forms) and operators (Lie derivatives, Lie brackets, exterior derivative, wedge product, innterior product (= contraction)). E.g. two of them are: $$\iota_{[X,Y]} \alpha = \mathcal{L}_X \iota_Y \alpha - \iota_Y \mathcal{L}_X \alpha \qquad (1)$$ $$\mathcal{L}_{[X,Y]} \alpha = \mathcal{L}_X \mathcal{L}_Y \alpha - \mathcal{L}_Y \mathcal{L}_X \alpha \qquad (2)$$ for vector fields $X,Y$, a $k$-form $\alpha$ and $\mathcal{L}, \iota$ denoting the Lie-derivative and the interior product, respectively. Probably, such formulas can be derived self-sufficiently from basic properties of the related objects. Q1: Must this be done by choosing coordinates or is it always possible to argument without coordinates (like in the proof of Cartans ""magic formula"")? On the other hand it would be nice to know a reference where these (and other) identities are published (including their proofs). Q2: Where to look for the derivation of such identities?","['exterior-algebra', 'differential-forms', 'reference-request', 'differential-geometry']"
947077,Definition of multivariate random variable,"Let $(\Omega,\mathcal E,P)$ be  a probability space and $X_1,\dots,X_n$ random variables on $(\Omega,\mathcal E, P)$.
Then I can define the vector $X=(X_1,\dots,X_n)\colon \Omega \to \mathbb R ^n$ and call it multivariate random variable (and this comes from wikipedia). Now, let us denote as $\mathcal B^1$ the borel sets of $\mathbb R$, so that $X_i^{-1}(B)\in \mathcal E$ for each $B\in \mathcal B^1$ for each $1\leq i \leq n$, since $X_i$ is a RV for each $i$. My question is: does something similar happen for $X$ defined as above? I mean, is it true that if I take a borel set $I\in \mathcal B^n$ then $X^{-1}(I)\in \mathcal E$, provided that $X$ is simply defined as a vector of one-dimensional random variables? I guess it is, but I can't say exactly why. Can someone help figuring out this?","['probability-theory', 'measure-theory', 'random-variables', 'real-analysis']"
947085,Integration of a function containing inverse trigonometric functions,"Q. $$\int \sin\left\{2\tan ^{-1}\left(\sqrt{\frac{3-x}{3+x}}\right)\right\}dx$$ $\implies$ $$\int \sin\left\{\sin ^{-1}\left(\frac{2\left(\sqrt{\frac{3-x}{3+x}}\right)}{1+\left(\sqrt{\frac{3-x}{3+x}}\right)^2}\right)\right\}dx$$ $\implies$ $$\int \:\frac{\left(\sqrt{3^2-x^2}\right)}{3}dx$$
$\implies$ANSWER $=$$$\frac{1}{3}\left(\left[\frac{x}{2}\sqrt{3^2-x^2}\right]+\left[\frac{9}{2}\sin ^{-1}\left(\frac{x}{3}\right)\right]\right)=\left(\left[\frac{x}{6}\sqrt{3^2-x^2}\right]+\left[\frac{3}{2}\sin ^{-1}\left(\frac{x}{3}\right)\right]\right)$$ Am I right?","['trigonometry', 'inverse', 'integration', 'indefinite-integrals']"
947172,"If $f=u+iv$ is an entire function such that $u^2\geq v^2,$ then $f$ is constant","Let $f=u+iv$ be an entire function such that $u^2(z) \geq v^2(z), \forall z \in  \mathbb{C}.$ Could anyone advise me how to prove $f \equiv$ constant $?$ Hints will suffice. Thank you.","['analyticity', 'complex-analysis', 'analysis']"
947187,Closed form of $\sum_{n=1}^{\infty}(-1)^{n+1} n (\log(n^2+1)-\log(n^2))$,"How would you start computing this series? $$\sum_{n=1}^{\infty}(-1)^{n+1} n (\log(n^2+1)-\log(n^2))$$ One of the ways to think of would be Frullani integral with the exponential function , but it's troublesome due to the power of  $n$ under logarithm. What else might I try?","['sequences-and-series', 'calculus', 'integration', 'real-analysis']"
947189,"Simplifying a summation involving ""cos"".","$\sum_{r=1}^{n-1}\cos ^{2}\left ( \frac{r\pi }{n} \right )$ How can I simplify this summation when I do not know whether ""n"" is odd or even?",['trigonometry']
947198,Cardinality of a line and a half plane,"intuitively it seems like the cardinality of the set of points that make up a line should be different than the cardinality of the set of points that make up a half plane but I couldn't come up with a proof, does a simple proof exist?","['elementary-set-theory', 'real-analysis']"
947201,Novel approaches to linear algebra and geometry,"I'll be studying Brannan's Geometry and Lang's Introduction to Linear Algebra for one university course. I would like to know if you can you suggest some books that offer a unique perspective on the material covered by such books and that can be therefore used as effective companions. This unique perspective may include, for example, the emphasis of the geometric side of linear algebra","['geometry', 'book-recommendation', 'linear-algebra', 'reference-request', 'soft-question']"
947227,Question regarding KKT conditions in optimization,"Following is Proposition 3.3.7 in Bersekas' Nonlinear Programming. Let $x^*$ be the local minimum of the problem:
  $$\text{Minimize }\; f(x)  $$ $$
     \text{subject to: }\ 
         h_j(x) = 0, j=1,\dots, m, g_i(x) \le 0, i=1, \dots, r, $$
  where $f,h_j,g_i$ are continuously differentiable functions from $\mathbb{R}^n\to \mathbb{R}$. $h_j$ are linear, $g_i$ are concave. Then there exists $\lambda_1^*,\dots,\lambda_m^*$ and $\mu_1^*,\dots, \mu_r^*$ such that 
  (i)$$   \nabla f(x^*) + \sum_{i=1}^r \mu_i \nabla g_i(x^*)
 + \sum_{j=1}^m \lambda_j \nabla h_j(x^*) = 0, $$
  (ii) $\mu_j^*\ge 0$
  (iii) In every neighborhood $N$ of $x^*$, there is an $x$ such that $\lambda_i^*h_i(x) > 0$ for all $i$ with $\lambda_i^* \neq 0$ and $\mu_j^*g_j(x) > 0$ for all $j$ with $\mu_j^* \neq 0$. My question is, is this theorem applicable even in the infinite dimensional case? Specifically, say, is this applicable in the following setting? Let  $\alpha >1$. Minimize $$\int_{-\infty}^{\infty} p(x)^{\alpha}\, dx,$$ subject to
  $$\int_{-\infty}^{\infty} p(x)\, dx = 1,$$
  $$\int_{-\infty}^{\infty} x\, p(x)\, dx = a_1,$$
  $$\int_{-\infty}^{\infty} x^2\, p(x)\, dx = a_2,$$ and
  $$p(x)\ge 0.\, \forall x$$ Note that the optimizing variable is $p(x)$, not $x$. Unlike the first one, this has a continuum of inequality constraints, namely, $p(x)\ge 0, \, \forall x$ and it is no more an optimization problem in $\mathbb{R}^n$. Is there any theorem which allows us to apply that the same Prop. 3.3.7 even to this setting?","['optimization', 'functional-analysis', 'nonlinear-optimization']"
947235,Why does being holomorphic imply so much about a function?,"I haven't yet started my complex analysis course (soon!), but recently (inspired by you guys) I've been looking into holomorphic functions. And wow, they're cool! There's so much stuff that's true about them... But my question is: why is being holomorphic such a strong condition? Is there some intuitive reason why this seemingly simple condition implies so much about a function? edit: may I add that I am thinking in particular about entire functions. e.g. it is not all obvious to me why being differentiable on the complex plane would imply Picard's theorem, that the function takes every value except at most one.","['soft-question', 'complex-analysis']"
947239,Novel approaches to elementary number theory and abstract algebra,"As a part of a university course, I'll have to study Herstein's Topics in algebra and Hardy&Wright's Introduction to the theory of numbers . Can you suggest some books (to be used as companions) that offer a unique approach to abstract algebra and number theory (for example, some geometric approaches to otherwise number-theoretical topics and problems, physical intuitions, or some intuitive but rigorous explanations of algebraic topics)?","['book-recommendation', 'physics', 'elementary-number-theory', 'abstract-algebra', 'intuition']"
947274,"Given $f:[0,1]\to[0,1]$ and $q:[0,1]\to\mathbb{R}$, is there a $g$ such that $q(f(x))\equiv g(q(x))$?","Let $f$ is a bijection and $q$ a given function, is there some function $g:\mathbb{R}\to\mathbb{R}$?  It is no need to be continuous or differentiable function. $\matrix{ [0,1] & \to^{f} &  [0,1] \\ q\downarrow & & q\downarrow  \\ \mathbb{R} & \cdots^{g} & \mathbb{R} }$",['functions']
947306,Prove that $2^n +1$ in never a perfect cube,"Prove that $2^n +1$  in never a perfect cube I've been thinking about this problem, but I don't know how to do it. I know that if $m^3=2^n+1$, then  $m$ should be an odd number, but I 'm not able to get to a contradiction.","['elementary-number-theory', 'algebra-precalculus']"
947310,Give example where an outer measure is strictly less than the set function from which it is defined.,"Let $K $ be a class of subsets of $X $ where for every subset $A $ of $X $ there is a sequence $\{E _n \}  $ of sets  in $K $ such that $A \subset \bigcup _{n=1 }^{\infty } E _n $. Let $\lambda$ be a extended-real valued, nonnegative  set function, with $\lambda ( \emptyset )=0$ Define $$\mu(E)= \inf \left\{\sum _{n=1 } ^{\infty } \lambda(E _n): E _n \in K, A \subset \bigcup _{n=1 }^{\infty } E _n \right\}$$ Show that if $E \in K $ then $\mu(E) \le \lambda (E) $  and give an example of where strict inequality holds. I think that if $E \in K $ , then $ \lambda (E ) $ is in the set from which we take infinum of in the definition of $ \mu $ and thus $\mu (E) \le \lambda (E) $. But how can you give an example of a set function $\lambda $ where strict inequality holds?","['measure-theory', 'analysis']"
947359,What intuition stands behind implicit differentiation,"I'm trying to undestand implicit differentation Let's take as a an example equation y^2 + x^2 = 1 1. How i think about how the equation works I think the function as : if x changes then the y term have to hold value of ""y^2 + x^2"" equal 1. Therefore the equation defines some set of numbers at x cordinates and y cordinates. 2. How i think about how differentate the equation If i want to know how the equation changes as x changes, i'm taking
derivative with respect to x $\frac{d}{dx}y^2+\frac{d}{dx}x^2=\frac{d}{dx}1$ We can consider $y$ as a function, $y = f(x)$ Therefore: $\frac{d}{dx}(f(x))^2+\frac{d}{dx}x^2=\frac{d}{dx}1$ We can calculate how (f(x))^2 changes as f(x) changes, using chain rule. $\frac{df(x)}{dx}\frac{d}{df(x)}(f(x))^2+\frac{d}{dx}x^2=\frac{d}{dx}1$ This is equal: $2f(x)\frac{df(x)}{dx}f(x)+\frac{d}{dx}x^2=\frac{d}{dx}1$ As $x$ changes, $x^2$ changes as $2x$, therefore $2f(x)\frac{df(x)}{dx}f(x)+2x=\frac{d}{dx}1$ As x changes, 1 doesn't changes, therefore it is 0.  $2f(x)\frac{df(x)}{dx}f(x)+2x=0$ We don't know derivative of $f(x)$ but we can solve it If we solve the derivative, we get $f'(x) = -\frac xy$ 3. Questions My way of thinking is right? What does mean the final answer? It looks strange, it doesn't tell me nothing comparing to norma, explicit derivative of a function. There is a difference between $\frac{dy}{dx}$ and $\frac{d}{dx}y$ ? Why i want to know ? Because i want to know how to interpretate steps and solution, not only algorithmically solve some book's problems. PS. I'm barely after highschool - Therefore i don't know yet set theorem and other high level math things. I'm learning calculus on my own.","['implicit-differentiation', 'education', 'calculus', 'derivatives']"
947377,Is $\mathbb R^n$ added by one point diffeomorphic to $S^n$?,"Let $M$ be a closed smooth manifold. If for some point $p$ on $M$ we can find a diffeomorphism between $M-\{p\}$ and $\mathbb R^n$, then is $M$ diffeomorphic to $S^n$(with the standard differential structure)?",['differential-geometry']
947380,"With matrices, why does the Lie bracket agree with the matrix commutator?","I gave an answer to Is there a group between $SO(2,\mathbb{R})$ and $SL(2,\mathbb{R})$? which was not popular. Meanwhile, I found myself at a loss when wishing to explain why a matrix Lie group had, as a tangent space at $I,$ not only a vector space but a Lie algebra, with bracket agreeing with $[A,B] = AB-BA$ from matrix multiplication, that the matrix exponential of a bracket puts us back in the same group, and so on. I got what I think is a proof in one direction using this: http://en.wikipedia.org/wiki/Baker%E2%80%93Campbell%E2%80%93Hausdorff_formula#Matrix_Lie_group_illustration http://en.wikipedia.org/wiki/Matrix_group I do have Matrix Groups by Curtis, he does not go this far. I also have Foundations of Differentiable Manifolds and Lie Groups by Warner, which seems to do large portions of this. Still, I am not entirely satisfied: with matrices, why does the Lie bracket agree with the matrix bracket, given a matrix group why is the identity tangent space both a vector space and a Lie algebra, given a tangent space at the identity that is a Lie algebra under matrix bracket, why do we get a group under matrix exponential?","['matrices', 'lie-algebras', 'reference-request', 'lie-groups']"
947431,What is contracting a tensor actually doing?,"I'm learning about tensors, and have a vague idea regarding what contracting a tensor means—but I'm still not sure of exactly what it's doing. Maybe someone here can put it in more intuitive terms. Say we have a $(k,l)$ tensor, $T^k_l$. This is some abstract mathematical structure which takes $k$ dual-vectors / covectors and $l$ vectors as ""arguments"" and spits out a number. Contracting this tensor should give me a $T^{k-1}_{l-1}$ tensor, right? Where (using summation convention) we have something like: $$T^{k_1}_{l_1} + T^{k_2}_{l_2} + ... + T^{k_3}_{l_3}$$ This is the $k_1$ covector acting on the $l_1$ vector. I guess the more I think about it, the less I really know what contracting the indices of a tensor really means. Could anyone give me a push in the right direction?","['general-relativity', 'tensors', 'differential-geometry']"
947433,Closure and subbasis,"Let $X$ be a topological space and $A \subset X$ with a subbasis $S$. Does it then hold that $x \in \overline{A}: \Leftrightarrow \forall s \in S: (x \in s \Rightarrow s \cap A \neq  \emptyset).$ This is certainly true if $S$ is a basis, but I suspect it is wrong for a subbasis, am I right?","['general-topology', 'real-analysis', 'analysis']"
947503,laurent series of $1/(z-5)$ about $z_o=2$,"I'm calculating the Laurent Series of $f=1/(z-5)$ about the point $z_{0}=2$.
The expansion I get has no principal part and is analytic within the circle $|z-2|<3$ since there are no singularities within that disk. I write $f=1/(z-5) = 1/(z-2 -3)= 1/3[(z-2)/3-1)]$, and since $(z-2)/3<1$, we can expand it into a convergent series without containing negative powers of $z-2$. However, I saw a book say that the answer does have a principal principal part, and in their derivation they introduce a singularity at $z_{0}=2$ . I don't see why they did that.","['laurent-series', 'sequences-and-series', 'complex-analysis']"
947531,Gradient and Laplacian in $S^1$,"I'm trying to solve the particle in a ring problem without embedding the circle in $\Bbb R^3$, by instead taking the entire space to be $S^1$.  Unfortunately, I haven't taken differential geometry yet and am unsure of how the gradient (or really the laplacian) is found in this space.  I assume it should still come out to being $\nabla^2 = \frac 1{r^2} \frac {\partial^2}{\partial \theta^2}$, but I don't how to derive that.  Is this something simple enough that you guys could explain it in a post, or do I just need to study differential geometry before attempting to solve a problem like this? Edit: What I've got so far is $$\frac {\partial^2 \psi(x)}{\partial x^2}=\frac{-2mE}{\hbar^2}\psi(x)$$ where $\psi(x)=\psi(x+2\pi Rn),\ n \in \Bbb Z$ and $\int_{-\infty}^\infty |\psi(x)|^2 dx = 1$.
$$\implies \psi(x)=Ae^{ikx}+Be^{-ikx}=Ae^{ik(x+2\pi Rn)}+Be^{-ik(x+2\pi Rn)}$$
$$\psi'(x)=ikAe^{ikx}-ikBe^{-ikx}=ikAe^{ik(x+2\pi Rn)}-ikBe^{-ik(x+2\pi Rn)}$$
$$\psi''(x)=-k^2Ae^{ikx}-k^2Be^{-ikx}=-k^2Ae^{ik(x+2\pi Rn)}-k^2Be^{-ik(x+2\pi Rn)}$$
$$\implies -k^2(Ae^{ikx}+Be^{-ikx})=\frac{-2mE}{\hbar^2}(Ae^{ikx}+Be^{-ikx})$$
$$\implies k=\sqrt{\frac {2mE}{\hbar^2}}$$ but I don't know how to find $A$ and $B$. Edit 2:
@CameronWilliams, is the periodic boundary condition, $\psi(x)=\psi(x+2\pi Rn),\ n \in \Bbb Z$, not enough to specify $A$ and $B$ then?  Because I haven't really found a way to use that condition, yet.","['laplacian', 'physics', 'quantum-mechanics', 'differential-geometry']"
947555,How to determine if 3 points on a 3-D graph are collinear?,"Let the points $A, B$ and $C$ be $(x_1, y_1, z_1), (x_2, y_2, z_2)$ and $(x_3, y_3, z_3)$ respectively. How do I prove that the 3 points are collinear? What is the formula?","['calculus', 'functions']"
947579,Question concerning Preimage,"Let $f$ be the map from $\mathbb{R} \to \{a,b,c\}$ defined by
  \begin{equation}
f(x)=\begin{cases} a &\text{if} \quad x>0 \\ b & \text{if} \quad x<0 \\ c &\text{if} \quad x=0 \end{cases}
\end{equation}
  Determine pre-images $f^{-1}(B)$ of all subsets of $B$ and justify if they are closed, open or neither. My reasoning Let $f$ be a function from $X$ to $Y$. The pre-image of the set $B \subseteq Y$ under $f$ is the subset of $X$ defined by
\begin{equation}
f^{-1}[B]=\{x \in X:f(x) \in B\}
\end{equation} As such, I would answer $f^{-1}(a)=\mathbb{R}^{+}\backslash \{0\}, f^{-1}(b)=\mathbb{R}^{-1}\backslash \{0\},f^{-1}(c)=0$. As for the second part, I would argue the two first cases are open, as every point in these two subsets are interior point in their respective subset. As for the last case, I interpret the corollary to theorem 2.20 (Rudin, p.33) - A finite point set has no limit points - that it is neither open nor closed. Is any of this correct?","['functions', 'inverse', 'real-analysis']"
947593,Solve $ y'=y^2e^{-x}+4y+2e^x $,Not a bernouli equation because the y^2 cannot be factored from right hand side. Any ideas on how to approach such an equation ? wolfram gives a nice answer,['ordinary-differential-equations']
947604,"Gradient, tangents, planes, and steepest direction","I know this is a topic covered on the internet frequently, but I still have further questions regarding visualization. I last took calculus some time ago, but am struggling with visualizations. Assuming $f(x,y)$ is some surface in 3D space. Now, let's say we take the gradient at point $(x_1, y_1)$. $\nabla f$ is then some two-dimensional vector $\langle \frac {\partial f}{\partial x},\frac{\partial f}{\partial y}\rangle$ and evaluated at $x_1$ and $y_1$. This two-dimensional vector symbolizes both how much change we have with respect to the x-axis and how much change we have with respect to the y-axis. I now have a couple points that I'm struggling (for whatever reason, to figure out) 1) Is it safe to say that the magnitude of the gradient vector at $(x_1, y_1)$ is the slope of the tangent plane to the surface at $f(x_1, y_1)$? 2) How is it that I read the gradient both is normal to the surface, but also points in the direction of maximum increase of $f$? I guess this is the problem I'm having, visualizing what the gradient vector looks like.",['multivariable-calculus']
947613,right/left inverse mappings,"Need some hints to solve Ex6a from V. Zorich course of Analysis vol.1 chap.1 §3. If mappings $f:X\to Y$ and $g:Y\to X$ are such that $g \circ f=id_X$ where $id_X$ - identity map X, then $g$ is called left inverse for $f$ and $f$ is called right inverse for g . Show that unlike the unique inverse map, there can exist many one-sided inverse maps. Author's hint to regard set $X$, for example consisting of one element, set $Y$ of two elements. My thoughts:
using the fact that $g(f(x))=x$ and $f(g(y))=y$, then (may be) to use lemma: $g \circ f=id_X => (g \quad surjective)\wedge(f\quad injective)$",['functions']
947614,Computing summation,"I'm trying to evaluate the series: $$\sum_{n=1}^\infty \frac{1}{3^{2^n}-3^{-2^n}}$$ I have tried to put it into partial fractions but it doesn't seem to telescope. Does anyone have any ideas? According to Wolfram, the answer is $\frac{1}{8}$.",['sequences-and-series']
947622,Finding variance from bell curve,Find variance from the graph given. I know the mean is 6 but I have no idea how to find the variance using this graph,"['statistics', 'standard-deviation', 'means']"
947677,the need for the formal definition of a limit [duplicate],"This question already has answers here : Understanding of the formal and intuitive definition of a limit (4 answers) Closed 9 years ago . The intuitive definition for $\lim\limits_{x \to a} f(x) = L$ is the value of $f ( x )$ can be made arbitrarily close to $L$ by making $x$ sufficiently close,  but not equal to, $a$. the formal definition of limit:For every real ε > 0, there exists a real δ > 0 such that for all real x, 0 < | x − a | < δ implies | f(x) − L | < ε. Why we need the formal definition of a limit? Does the intuitive definition has some flaw ? I understand the intuitive definition quite well, but I don't understand much about the formal one. P.S.I must declare I only have some basic knowledge of limit ,I started to learn calculus a few days ago . Understanding of the formal and intuitive definition of a limit ,
This post is not good ,because it covered more than one question , I picked one question from it ,so we can discuss it more  targeted .","['calculus', 'limits']"
947680,3 Variable Diophantine Equation,"Find all integer solutions to $$x^4 + y^4 + z^3 = 5$$ I don't know how to proceed, since it has a p-adic and real solution for all $p$.
I think that the only one is (2, 2, -3) and the trivial ones that come from this, but I can't confirm it.","['diophantine-equations', 'number-theory']"
947693,What are the real-valued functions?,"Is $f(x)=\frac{1}{x}$ a real-valued function? I think the definition of a real-valued function is that the range is in the real numbers. Is that right? Hence, I think $f(x)=\frac{1}{x}$ is not a real-valued function. If then, what are the other examples of real-valued functions? Are polynomial functions the only real-valued functions?",['functions']
947702,Showing that no homomorphism $ϕ:∏ _{i∈N} \Bbb Z→\Bbb Z$ can send all $e_i$ to 1,"why is that no homomorphism $\phi:\prod_{i\in\mathbb{N}}\mathbb{Z}\to\mathbb{Z}$ can send all $e_i$ to 1? In fact I saw a proof in MO using 2-adic integers...but I know very little about those topics. Could anyone prove it by other (possibly more elementary) means? I think I must show some contradiction about the value of $\phi(1,1,..,1,..)$ but can't formulate an argument.",['group-theory']
947710,"Show that an entire function that is real only on the real axis has at most one zero, without the argument principle","Could someone advise me on how to approach this problem: Suppose an entire function $f$ is real if and only if $z$ is real. Prove that $f$ has at most $1$ zero. without the use of argument principle ? Here is my attempt: Suppose $f(z)$ has two zeroes at $z=a.$ Let $f(z) = \sum^{\infty}_{n=0}a_n(z-a)^n, \forall z. \ $ Then $f(z)=(z-a)^2 \left(\dfrac{a_{0}}{(z-a)^2} +\dfrac{a_1}{z-a}+a_2+a_3(z-a)+...\right)$ $\implies a_0=a_1=0.$ $\implies f(z)= (z-a)^2\left(a_2+a_3(z-a)+a_4(z-a)^2+...\right)$ $\implies .... ?$ Thank you.",['complex-analysis']
