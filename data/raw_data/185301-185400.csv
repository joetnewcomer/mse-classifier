question_id,title,body,tags
3416844,Evaluating Convergence (Uniform),"Hi Guys was attempting this question and was wondering if I was doing the question correctly? Determine whether or not the sequence of functions is uniformly convergent:- $$g_n:(0,1)\to \mathbb{R}$$ $$g_n(x) = \frac{n^3+1}{n^3x^2+1}, x\in(0,1)$$ Checking point wise convergence first $$\lim_{n\to \infty}g_n(x) =  \lim_{n\to \infty}\frac{n^3+1}{n^3x^2+1}$$ Dividing by $n^3$ gives the following :- $$\lim_{n\to \infty}g_n(x) =  \lim_{n\to \infty}\frac{1+\frac{1}{n^3}}{x^2+\frac{1}{n^3}}$$ Taking the Limit as n $\to \infty$ gives the following $$\lim_{n\to \infty}g_n(x) =  \frac{1+\frac{1}{\infty^3}}{x^2+\frac{1}{\infty^3}}$$ $$\lim_{n\to \infty}g_n(x) =  \frac{1+0}{x^2+1} = \frac{1}{x^2}$$ Therefore by point wise convergence the sequence of functions converges to the previous function. In order to determine the uniform convergence we must analyze the follwing $$M_n =  sup|f_n(x)-f(x)|,x\in \mathbb{R}$$ $$|f_n(x)-f(x)|$$ $$|\frac{n^3+1}{n^3x^2+1} - \frac{1}{x^2}|$$ $$\frac{(n^3x^2+x^2)-(n^3x^2+1)}{(n^3x^2+1)(x^2)}$$ $$|\frac{x^2-1}{(n^3x^2+1)(x^2)}|$$ The mod gives $$\frac{x^2+1}{(n^3x^2+1)(x^2)}$$ is it accurate to say the following when checking to see uniform convergence $$\frac{x^2+1}{(n^3x^2+1)(x^2)} < \frac{1}{n^3}$$ $$\lim{n \to \infty} $$ Therefore I can conlclude that $$sup|f_n(x)-f(x)|\to 0$$ Therefore the function is uniformly convergent? Oh am i wrong in my evaluation?","['sequence-of-function', 'functions', 'uniform-convergence', 'limits', 'convergence-divergence']"
3416937,In how many ways can we arrange the English alphabet so that exactly $10$ of the letters lie between $A$ and $Z$?,"In how many ways can we arrange the English alphabet (of 26 letters) so that exactly $10$ of them lie between $A$ and $Z$ ? Attempt: First we select $10$ letters to put between $A$ and $Z$ in $C(24,10)$ . Now the letters that lie outside get selected automatically. We consider $[A(10 letters)Z]$ as a single unit and permute this with the rest of alphabets in $15!$ ways. Letters between $A$ and $Z$ can be permuted in $10!$ ways. Finally we can also permute $A$ and $Z$ in 2 ways. So using the rule of product, the required answer would be $C(24,10)*(15!)*(10!)*2$ . Is this correct?","['permutations', 'combinatorics']"
3416993,"Cardinality of Cartesian Product where (a,b) are elements of A x B","I'm having issues with a question regarding the cardinality of a cartesian product. Question:
Let $A=\{0, 1, 2, 3, 4, 5, 7\}$ and $B=\{0, 2, 4, −1, 12\}.$ How many elements are in $\{(a, b) ∈ A × B \; |  \; a < 7 \text{ and } b < 4\}$ ? I want this to be $6 \cdot 3 = 18$ . but that seems to be wrong.",['discrete-mathematics']
3417020,"Efficient homework grading: 3 graders, each HW needs to be graded by 2 people","No idea if this is a math problem, but it sounds like a problem that would have been covered in my discrete math class 4 years ago, which I remember very little about. This is a real life situation, not an assignment question: 3 people in a group. ~50 homework papers need to be graded. Each homework assignment needs to be graded by at least 2 people. What is the most efficient way of doing this so each person does an equal amount of grading?",['discrete-mathematics']
3417031,How to change basis for normal vector fields on regular surfaces?,"I have two normal vector fields over a surface $S$ , to two parameterizations $\psi,\varphi$ : $$X_{\psi}:p\mapsto X_{\psi}(p)=\dfrac{\partial \psi}{\partial u}(\psi^{-1}(p))\wedge \dfrac{\partial \psi}{\partial v}(\psi^{-1}(p))$$ $$X_{\varphi}:p\mapsto X_{\varphi}(p)=\dfrac{\partial \varphi}{\partial u}(\varphi^{-1}(p))\wedge \dfrac{\partial \varphi}{\partial v}(\varphi^{-1}(p))$$ At the intersection of the neighborhoods, $X_\psi,X_\varphi$ are related by matrix of coordinate changing, $g=\varphi^{-1}\circ\psi$ . A professor wrote: $$X_{\psi}(q)=\det(Jg(\psi^{-1}(q)))X_{\varphi}(q).$$ I am confused with the transformations: I thought $g$ leads $Dom(\psi)$ to $Dom(\varphi)$ , so why is not $$X_{\fbox{$\varphi$}}(q)=\det(Jg(\psi^{-1}(q)))X_{\fbox{$\psi$}}(q)?$$ Many thanks in advance for a light. If possible, I'd like not a yes/no answer, but some light to deal with this kind of transformation. Please take some look at the comments bellow. Thank you so much!","['surfaces', 'vector-fields', 'change-of-basis', 'parametrization', 'differential-geometry']"
3417148,A triangulation of $\Delta_p\times I$,"While reading the proof of the homotopy axiom for singular homology from Lee's Introduction to Topological Manifolds and Hatcher's Algebraic Topology , I found out that there is a little technical detail that is not further commented. Adopting Lee's notation, let's recall that: $$\Delta_p=[e_0,\dots,e_p]$$ Denotes the standard $p-$ simplex. Then, in these references, it is taken for granted that the union of the $(p+1)$ -simplices $$[E_0,E_0',\dots,E_p'],[E_0,E_1,E_1',\dots,E_p'],\dots,[E_0,\dots,E_p,E_p']$$ is precisely $\Delta_p\times I$ , where $E_i=(e_i,0)$ and $E_i'=(e_i,1)$ . Moreover, in Hatcher it is implicitly asserted that the intersection of any two consecutive $(p+1)$ -simplices $$[E_0,\dots,E_i,E_i',\dots,E_p']\text{ and }\;[E_0,\dots,E_{i+1},E_{i+1}',\dots,E_p']$$ is precisely $$[E_0,\dots,E_i,E_{i+1}',\dots,E_p'].$$ So my questions are the following: Why do the given simplices form a cover of the whole space $\Delta_p\times I$ ? Why each of those is a subset of the aforementioned space? Why is the intersection of any of two consecutive simplices just the face they have in common? In general, why is this decomposition a triangulation of the whole space? I think it is obvious how to check that these assertions are true when we consider the cases $p=1,2$ . However, I would like to prove this in its most general form, for any positive integer $p$ . I thought about using induction, but the induction step is difficult to apply. Indeed, I still don't know how it should be applied from $p=1$ to prove $p=2$ . Thanks in advance for your time.","['triangulation', 'geometry', 'simplex', 'general-topology', 'algebraic-topology']"
3417168,Integrals are equal? How?,"Why are these integrals the same? $$ \int_0^1\bigg(\frac{-1}{\ln^3(x)}\bigg)\exp\bigg(\frac{1}{\ln(x)}\bigg)~dx=\int_0^1 \bigg(-\ln(x)\bigg)\exp\bigg(\frac{1}{\ln(x)}\bigg)~dx=2K_2(2) $$ Where the $K_2$ is a modified Bessel function. Is there some property that describes when this happens? I think it has to do with the functions being in the same general form, but I can't pinpoint the specifics. Thanks.","['integration', 'calculus', 'definite-integrals']"
3417228,Justifying $\lim \limits_{x \to 0} f(x) = \lim \limits_{bx \to 0} f(bx)= \lim \limits_{(x - 7) \to 0} f(x-7)$,"How do you justify such limit equalities as these? $\lim \limits_{x \to 0} f(x) = \lim \limits_{bx \to 0} f(bx)= \lim \limits_{(x - 7) \to 0} f(x-7)$ I am familiar with the $\epsilon-\delta$ definition of the limit, I just don't know what it is that makes these statements equivalent: $$ \forall \epsilon \ \exists \delta> 0 \ \forall x: |x| < \delta \implies |f(x) - l| < \epsilon$$ $$ \forall \epsilon \ \exists \delta> 0 \ \forall x: |bx| < \delta \implies |f(bx) - l| < \epsilon$$ $$ \forall \epsilon \ \exists \delta> 0 \ \forall x: |x - 7| < \delta \implies |f(x - 7) - l| < \epsilon$$","['limits', 'calculus', 'real-analysis']"
3417269,Number of invertible elements in quotient ring,"I want to find an analog of Euler function $\varphi_{R}(a)$ that determines the number of invertible elements in the quotient ring $$R = \mathbb{F}_p[x]/(a), \text{ for } a \in \mathbb{F}_p[x]$$ where $\mathbb{F}_p[x]$ is a Euclidian domain with standard norm $N(f(x)) := \deg f(x)$ . UPD: I tried the following: Case 1: $a$ is prime $\Rightarrow \mathbb{F}_p[x]/(a)$ is field then $$\varphi_R(a) = |\mathbb{F}_p[x]/(a) \setminus \{0\}| = p^{deg(a)} - 1$$ Case 2 : take factorization $a = a_1^{e_1} \dots a_n^{e_n}$ then $$R \simeq \mathbb{F}_p[x]/(a_1^{e_1}) \oplus \mathbb{F}_p[x]/(a_2^{e_2}) \oplus \cdots\oplus\mathbb{F}_p[x]/(a_n^{e_n})$$ R is finite ring then $$\forall g \in R((g \text{ is zero divisor}) \vee (g \text{ is invertible}))$$ $$(g \text{ is zero divisor}) \iff \vee_{i = 1}^n(a_i | g)$$ Every element $g \in R$ now can be represented as following: $$g = (g (\text{mod } a_1^{e_1}),\dots,g(\text{mod } a_n^{e_n}))$$ $$a_i | g \iff \exists i =1\dots n : g (\text{mod }a_i^{e_i}) = 0$$ So we can conclude $$\varphi_R(a) = p^{deg(a)} - \prod_{i = 1}^n(p^{c_i} - 1), \text{ where } c_i = \deg(a_i^{e_i})$$ Is this answer correct?","['euclidean-domain', 'ring-theory', 'finite-fields', 'abstract-algebra']"
3417287,Can we take out an integrand random variable which does not depend on time from the stochastic integral?,"Let $Y$ is a ${\mathbb R}$ -valued random variable, $(X_{t})_{t \geq 0}$ is one-dimensional stochastic process and $(W_{t})_{t \geq 0}$ is a one-dimensional Brownian motion.
Then is the following formula correct? $$
Y \int_{0}^{t}X_{s}{\rm d}W_{s}=\int_{0}^{t}YX_{s}{\rm d}W_{s}, \quad t \geq 0.
$$ Here, these integrands $(X_{t})_{t \geq 0}$ and $(YX_{t})_{t \geq 0}$ have some conditions that allow the stochastic integrals to be defined.","['stochastic-integrals', 'stochastic-calculus', 'probability']"
3417309,Why does exponentiating group elements by integers always make sense?,"Let $G$ be a group.
For any $g\in G$ , we can define a mapping $\mathbb Z\to G$ via $n\mapsto g^n$ .
This map is a homomorphism between the additive group $\mathbb Z$ and $G$ . On the one hand, it's obvious why we can always do this. We just define $g^n$ as ""multiply $g$ with itself $n$ times"" (for $n>0$ , and then straightforwardly generalise to all $n\in\mathbb Z$ ).
Thus, $\mathbb Z$ comes into play because we are ""counting"" the number of times $g$ is operating on itself. On the other hand, this means that for any group $g\in G$ , there is a homomorphism $\mathbb Z\to G$ sending $n$ to $g^n$ .
Is there any reason why this group in particular, $\mathbb Z\equiv(\mathbb Z,+)$ , ends up being homomorphic to subgroups of every group? Are there groups other than $\mathbb Z$ sharing this property?","['group-theory', 'integers']"
3417338,Index of the closed trajectory,"Consider the system $$\begin{cases}
\frac{dx}{dt}=x(1-x-y) \\
\frac{dy}{dt}=y(-1+2x)
\end{cases}
$$ Show that closed orbits are impossible for this system using index theory and properties of the solution curves. The fixed points of this system are: $(0,0),(1,0),(\frac{1}{2},\frac{1}{2}).$ I've drawn a global phase portrait, which clearly shows that there is no closed trajectory. But how do I show that using index theory? Note: Index of curve $𝐶$ with respect to vector ﬁeld $(𝑓,𝑔)$ is $𝐼(𝑓,𝑔,𝐶)=$ number of times the vector turns counter-clockwise",['ordinary-differential-equations']
3417373,Please explain the proof of the Mutilated Chessboard Problem,"Can someone explain the proof behind why the mutilated chessboard problem is unsolveable? The problem asks, given an 8x8 chessboard with two diagonally opposite corners removed, is it possible to fill the entire board with 31 dominos (assuming that each domino covers 2 adjacent squares)? The classic solution to this problems states that when you cut two diagonal corners, you always cut away two squares of the same color, meaning that your board has an unequal number of black and white squares. Thus it is impossible to cover the board completely with dominos (since each domino must cover a black and white square). However I'm not convinced this is an adequate proof on its own. Imagine a different 8x8 board where each row alternates between black and white (so the first row is 8 black squares, then the second row is 8 white squares, ect). If you remove two opposite corners of this board then you'll remove one white and one black square, meaning you'll be left with the same number of white and black squares. However aside from the color arrangement this is still clearly the same board. Is there something about the above proof that I'm not getting? Or is this one of those proofs that sounds neat but doesn't actually hold up?","['chessboard', 'combinatorics']"
3417417,Proof for intersection.,"Really stuck on this one, seemed simple but I just couldn't get it for some reason. Suppose that $\mathcal U$ is the universal set, and that $A$ , $B$ and $C$ are three arbitrary sets
of elements of $\mathcal U$ . Prove that if $C \setminus A = B$ , then the intersection of $A$ and $B$ is empty. Hint: use an indirect proof.","['elementary-set-theory', 'predicate-logic', 'logic']"
3417471,"Non-negative, continuous function $f \in L^1(\mathbb{R})$ that is unbounded?","I'm trying to construct a non-negative, continuous function $f \in L^1(\mathbb{R})$ such that $\limsup\limits_{x\to \infty} f(x) = \infty.$ If someone could look over my attempt below, I would appreciate it! Consider $$g_n(x) = \begin{cases}
  2n^4x-2n^5+n & \text{if $n-\frac{1}{2n^3}<x<n$}   \\
 -2n^4x+2n^5+n & \text{if $n \leq x<n+\frac{1}{2n^3}$} \\
0 & \text{otherwise}
\end{cases}$$ My idea here was to construct a function with triangular bumps at each $n=1,2,3,...$ with height $n$ and base width $\frac{1}{n^3}$ . We note that the $g_n$ are continuous, non-negative and have support $[n-\frac{1}{2n^3}, n+\frac{1}{2n^3}]$ . The supremum for each $g_n$ is $n$ . Moreover, its integral is finite and given by $\frac{1}{2n^2}.$ Now, we consider the sum $$G(x) :=\sum_{n=1}^{\infty}g_n(x)=\sum_{n=1}^{\infty}(2n^4x-2n^5+n)\chi_{(n-1/2n^3, n)}(x)+(-2n^4x+2n^5+n)\chi_{[n, n+1/2n^3)}(x)$$ We note that $G(x)$ is also continuous and non-negative. Its $L^1$ norm is given by $$\sum_{n=1}^{\infty}\frac{1}{2n^2} = \frac{\pi^2}{12}<\infty$$ so $G \in L^1.$ However, the heights of the function increase without bound, so we have that $\limsup\limits_{x\to \infty} f(x) = \infty.$ G(x) should look like this:","['measure-theory', 'proof-verification', 'functional-analysis', 'real-analysis']"
3417512,Isn't it always possible that one can perturb the metric such that it has positive sectional curvature at a point?,"This is the famous theorem of Gromoll and Meyer: Theorem (Gromoll-Meyer, 1974) There is an exotic 7-sphere with nonnegative
  sectional curvature and positive sectional curvature at a point. I don't understand the second part of theorem "" positive sectional curvature at a point "". Isn't it always possible that one cane perturb the metric such that it has positive sectional curvature at a point and we have $\sec_{\min}(M)\leq \sec_q\leq \sec_p$ for all $q$ in a small neighborhoods of $p$ ?","['curvature', 'riemannian-geometry', 'differential-geometry']"
3417558,Suppose $f: A \rightarrow B$ and $g: B \rightarrow C$. Prove that $g \circ f: A \rightarrow C$,"Suppose $f: A \rightarrow B$ and $g: B \rightarrow C$ . Prove that $g
 \circ f: A \rightarrow C$ My attempt: Definition of the composition (according to the book I'm reading): $$g \circ f = \{(a,c) \in A \times C \mid \exists d \in B((a,d) \in f \land
 (d,c) \in g)\}$$ Take $a \in A$ . There exists some $b \in B$ such that $(a,b) \in f$ . Since $g: B \rightarrow C$ and $b \in B$ , there also exists some $c \in C$ such that $(b,c) \in g$ . By definition of the composition of the relations, we have $(a,c) \in g \circ f$ . Hence for all $a \in A$ , there is at least one $c \in C$ such that $g \circ f$ . Now we prove uniqueness. Suppose $(a,x) \in g \circ f $ and $(a,y) \in g \circ f$ . Then there exists some $d$ such that $(a,d) \in f$ and $(d,x) \in g$ . And there also exists some $v$ such that $(a,v) \in f$ and $(v,y) \in g$ . We have $(a,d) \in f$ and $(a,v) \in f$ . By definition of the functions, we conclude that $d = v$ . Hence for all $a \in A$ , there exists unique $c \in C$ such that $(a,c) \in g \circ f$ . Therefore, $g \circ f$ is a function from $A$ to $C$ . $\Box$ Is it correct?","['elementary-set-theory', 'functions', 'proof-verification']"
3417599,"$\text{Aut}(A_6)$ is not split extension of $A_6\cong \text{Inn} A_6$, i.e. $\text{Aut}(A_6) \not \cong A_6 \rtimes ( \mathbb Z_2 \times \mathbb Z_2)$","Claim: Short exact sequence $1 \longrightarrow \operatorname{Inn}\left(A_{6}\right) \longrightarrow \operatorname{Aut}\left(A_{6}\right) \longrightarrow \operatorname{Out}\left(A_{6}\right) \longrightarrow 1$ is not right split, where $\operatorname{Inn}\left(A_{6}\right)\cong A_6$ , $\operatorname{Aut}\left(A_{6}\right)\cong\operatorname{Aut}\left(S_{6}\right)\cong S_6\rtimes \mathbb Z_2$ and $\operatorname{Out}\left(A_{6}\right)\cong\mathbb Z_2\times \mathbb Z_2$ . To move this question out of the unanswered list, I put my proof in the answer. Thanks for your time and patience :)","['automorphism-group', 'group-theory', 'abstract-algebra', 'finite-groups']"
3417613,"How could the ""Riemann mapping group"" be a group?","Here is the definition of Riemann mapping group:
A set of injective holomorphic functions from a closed unit disk $f:D\rightarrow\mathbb{C}$ which is holomorphic on the interior and smooth on boundary, normalized by the condition $f(0)=0$ and $f'(0)=1$ . How to define the group multiplication? There's another story that this group is isomorphic to the group $\mathrm{Diff}_+(S^1)/\mathrm{Rot}(S^1)$ , which is a conformal  transformation group in physics. For $f:D\rightarrow D$ , by Schwarz lemma, $f(z)=az$ for some $a\in\mathbb{C}$ with $|a|=1$ . We can then write $f=e^{i\theta}$ which is isomorphic to $\mathrm{Rot}(S^1)$ . But I don't know the detail about the general $f$ . I saw it at Bartlett's comment .
And I guess the Kirillov's paper he mentioned is the origin.","['complex-analysis', 'group-theory']"
3417619,initial value problem with differential,"Does this initial value problem have a solution which is valid on the domain $\mathbb{R}$ ? $$y'=\sqrt{x^2-y^2}\\ y(1) = 1$$ If not, does it have a solution which is valid on the domain $(1-\epsilon, 1+\epsilon)$ ? I can't use the Picard Lindelöf theorem here since $\sqrt{x^2-y^2}$ is not Lipschitz continuous with respect to $y$ when $x=y=1$ .","['initial-value-problems', 'ordinary-differential-equations']"
3417655,Stone's theorem and the spectral theorem,"I am struggling to formally derive the expression found in the Stone's theorem for one-parameter unitary groups. I am aware that this can be done by using the spectral theorem. I am mostly interested in the discrete 'version' of the spectral theorem. Here's a short statement of Stone's theorem: If ${\cal H}$ is a Hilbert space and $U(t)$ is a strongly-continuous,
one-parameter unitary group, then $U(t)=\exp\bigl(-itH\bigr)$ , where $H$ is self-adjoint. Can anyone help me out or point me to some reference where this is explicitly done?","['operator-theory', 'hilbert-spaces', 'functional-analysis', 'quantum-mechanics', 'spectral-theory']"
3417663,How to derive the variance of this MLE estimator,"Let $(x_i, Y_i)\in\mathbb{R}^2$ be independent observations on $n$ subjects, such that $$Y_i|x_i\sim N(x_i\beta, \sigma^2)$$ where $(\beta, \sigma^2)\in\mathbb{R}^2$ are unknown coefficients. I computed the maximum likelihood estimate $\hat\beta$ of $\beta$ , which is $\hat\beta = \frac{\sum_{i=1}^n y_{i}x_i}{\sum_{i=1}^n x_i^2}$ , and we want to compute the variance of this estimator $\hat\beta$ . Using that $Var(\hat\beta)= E[\hat\beta^2]-E[\hat\beta]^2$ , I would only need $E[\hat\beta^2]$ to get the variance, as I already showed $E[\hat\beta]=\beta$ , but I'm struggling with it. $$E[\hat\beta^2]=E[(\frac{\sum_{i=1}^n y_{i}x_i}{\sum_{i=1}^n x_i^2})^2]=\frac{1}{(\sum_{i=1}^n x_i^2)^2}E[(\sum_{i=1}^n y_{i}x_i)^2]$$ I do not really know how to compute this expectation. Any help would be appreaciated.","['statistics', 'parameter-estimation', 'maximum-likelihood', 'estimation']"
3417685,"Flip a coin, but you lose when tails appears","I have the following game: you flip a coin (heads with probability $p$ ), and if you get heads you earn $1000$ dollars, and you can decide if you want to flip again. If not, you keep the money. However, if you get tails, the game is over and you go home with nothing. I am having trouble calculating an optimal strategy for this game: I would like to maximize the winning amount by stopping at turn $f(p)$ , but... what is $f(p)$ ? If you did not lose all the money when tails appears, the expected winning value would just be a standard geometric distribution $\sum_{i=0}^\infty i p^{i-1} (1-p) = \frac{1}{1-p}$ , but I am having trouble evaluating the risk at each turn. Clearly never stopping is a good strategy only when $p=1$ , but intuitively when $p$ is very close to $1$ then stopping after the first win is not the best strategy. I am confused on how to model this phenomenon.",['probability']
3417704,What is $\sum_{n=1}^{\infty}a_1 a_2...a_n$?,"Let $\{a_n\}$ be defined as follows: $a_1>0, a_{n+1}=\ln \dfrac{e^{a_n}-1}{a_n}$ for $n \geq 1.$ Then find the sum $\displaystyle \sum_{n=1}^{\infty}a_1a_2...a_n.$ Please give some hints to solve this. At the question it was also given that $\{a_n\}$ is a decreasing sequence of positive terms converging to 0.","['analysis', 'sequences-and-series']"
3417706,The Hessian of a Radial Basis Function,"In this paper the Hessian of a RBF is shown as $$
H_j = \frac{\beta_j}{r_j(x)} \bigg(
\phi'(r_j) I +
\bigg[
\phi''(r_j) - \frac{\phi'(r_j)}{r_j(x)} 
\bigg]
\bigg) (x-x_j) \frac{\partial r_j}{\partial x}
$$ where $$
r_j = || x-x_j|| = \sqrt{(x-x_j) ^T (x-x_j)}
$$ and $\phi(r)$ could be one of the many RBF functions, such as $\phi(r) = \exp(-cr^2)$ . An RBF is of course $$
f(x) = \beta^T g(x) = \sum_i \beta_i \phi(r_i)
$$ I understand how the Jacobian is obtained, but was not able to derive the Hessian. Where did the identity matrix come from? Why $H_j$ and not $H_{ij}$ ? Any help would be appreciated.","['partial-derivative', 'multivariable-calculus', 'matrix-calculus', 'hessian-matrix']"
3417730,Liouville Theorem for Harmonic Functions,"If $u$ is bounded and harmonic in $\mathbb{R}^n$ , then $u$ is constant For any twice differentiable function $u$ defined on an open subset $\Omega$ we have $$u(x)=\int\limits_\Omega G(x,y)\Delta u(y)dy+\int\limits_{\partial\Omega}\frac{\partial G}{\partial n}(x,y)u(y)dS_y$$ where $G$ is the corresponding Green's function. We take the region to be $B_a(0)$ and take $G(x,y)=\Phi(x-y)-\Phi(\frac{|x|}{a}|x^*-y|)$ where $\Phi$ is the fundamental solution to the Laplace equation $\Delta u=0$ and $x^*$ is the inverse of the point $x$ w.r.t. the ball $B_a(0)$ .It turns out then $$u(x)=\int\limits_{\partial\Omega}H(x,y)u(y)dS_y
$$ where $$H(x,y)=\frac{a^2-|x|^2}{aw_n}\frac{1}{|x-y|^n}$$ $w_n$ being the surfacae integral of the unit sphere in $\mathbb{R}^n$ .
Partially differentiating $H$ we get $$|H_i(0,y)|\leq \frac{n}{w_na^n}$$ which gives $$|\frac{\partial u}{\partial x_i}(0)|\leq \frac{n}{a}\|u\|_\infty$$ Now I want something like $$|\frac{\partial u}{\partial x_i}(x)|\leq \frac{n}{a}\|u\|_\infty \forall x$$ to conclude $u$ is constant. I am not sure how to do so. Any help is appreciated.","['partial-differential-equations', 'harmonic-functions', 'analysis', 'real-analysis']"
3417765,Find time of the shortest distance between two accelerating points in a plane,"Given two points $A$ and $B$ in a plane, with initial positions $P$ and velocities $V$ , and constant accelerations $a$ , find solutions for what value of $t$ (in seconds) they will be the closest? I would imagine if I started with a set of values for the 6 variables it would be easier as I could simplify a lot, but in my case I need to find $t$ for any $P_A$ , $V_A$ , $a_A$ , $P_B$ , $V_B$ , $a_B$ . I have solved this problem for when there is no acceleration, and my approach there was to create a function for their positions, and then a distance function $d(t)$ using the Pythagorean theorem, and then minimizing $d(t)$ by solving for $d'(t) = 0$ . I tried this same approach for this case with acceleration, and solving it in Wolfram Cloud gave me a few solutions for $t$ , however, each solution was more than a 1000 symbols long, with many complex parts, so not very practical to use for my case (a simulation). Is there a simpler solution to this problem?","['optimization', 'calculus', 'derivatives']"
3417766,Existence of an open interval on which $f'$ is bounded,"Recently we've just begun to learn derivative, and our teacher left the following problem. Let $f:\mathbb{R}\to \mathbb{R}$ be differentiable on $\mathbb{R}$ . Show that there is an open interval $(a,b)$ on which $f'$ is bounded. Since we've only come to the definition of derivative, the problem seems remote. Yet our teacher encouraged us to try. I've tried to disprove the opposite, but it didn't work. Please help. A mere hint will do.","['derivatives', 'analysis']"
3417828,Number of factors of $2^{p_1\cdots p_n}+1$,"Let $p_1 <p_2 \cdots <p_n$ be the ordered prime numbers starting with $p_1=5$ . Let $a_n = 2^{p_1\cdots p_n}+1$ . It is elementary to show that $a_n$ has at least $4^n$ factors but this bound ( $\tau(a_n)\geq 4^n$ ) does not seem to be sharp, hence my questions: Do we know better bounds than $4^n$ ? Could we estimate $\tau(a_n)$ when $n\to \infty$ ? EDIT Nov 9, 2019
With the link I mentioned in comment I was able to show that $\tau(a_n)\geq 2^{2^{n-1}}$ .","['number-theory', 'asymptotics', 'elementary-number-theory', 'upper-lower-bounds']"
3417898,Nonisomorphic Groups of order $100$,"This is from Dummit and Foote, exercise 3.4. Give the number of nonisomophic abelian groups of order 100. What I did was factor $100$ into invariant factors $$ 1, \; 100$$ $$ 1 ,\; 2 ,\; 50 , \;100$$ $$ 1 ,\; 4 ,\; 25 , \;100$$ $$ 1 ,\; 5 ,\; 20 , \;100$$ $$ 1 ,\; 10 ,\; 10 , \;100$$ This gives me 5 options products of cyclic groups of order 100. So I get $$ \mathbb{Z}_{100} $$ $$ \mathbb{Z}_{2} \times \mathbb{Z}_{50}$$ $$ \mathbb{Z}_{4} \times \mathbb{Z}_{25}$$ $$ \mathbb{Z}_{5} \times \mathbb{Z}_{20}$$ $$ \mathbb{Z}_{10} \times \mathbb{Z}_{10}.$$ But the answer should be 4, and I have 5 groups here. Comparing with the examples in the book, I think that $ \mathbb{Z}_{100} $ is not an option. Why is $ \mathbb{Z}_{100} $ not an option? It's abelian, and it has order $100$ . I was thinking maybe cyclic groups only have prime order, but $ \mathbb{Z}_{4} $ is cyclic, abelian, and not of prime order, so that's not true.","['group-theory', 'abstract-algebra', 'abelian-groups', 'cyclic-groups']"
3417916,Proving there exist three different vectors that sum to zero,"I'm trying to prove the following statement: Prove that for each $S \subset {\mathbb{Z}_3}^3$ satisfaying $|S|=9$ , there exist three different vectors $a, b, c \in S$ such that $a+b+c=\overrightarrow{0}$ . I know that a direct verification (which may be done by a computer) is an ""immediate"" proof. However, I'm interested in a simpler and more sophisticated proof. I've tried several approaches; The first one is linear algebra - I've placed the elements of $S$ in a $9 \times 3$ matrix and tried to show that there exist three rows which sum to zero using the fact that the rank of this matrix $\leq 3$ . It didn't work. The second approach I've tried is the pigeonhole principle, and it also didn't work. The third approach I've tried is the probabilistic method by randomly selecting three different vectors $a,b,c \in S$ and examining the three-dimensional random variable $a+b+c$ , using the fact that it equals the zero vector iff each coordinate is zero. I've also tried considering two cases: $\overrightarrow{0} \in S$ and $\overrightarrow{0} \not \in S$ . I've proved that there exists some coordinate $1 \leq i \leq 3$ such that there exist three vectors $a,b,c \in S$ such that their $i$ th coordinates are $-1, 0, 1$ . It might be useful. Thanks in advance.","['pigeonhole-principle', 'linear-algebra', 'combinatorics']"
3417948,"Find positive integers $a, b$ such that $491! = 11^a \cdot 7^b \cdot c $","The question is to find positive integers $a, b$ such that $491! = 11^a \cdot 7^b \cdot c $ where $c$ is a natural that is not divisible by either $11$ or $7$ . Giving it some thought I believe to have reached something tangible, but not too sure on whether or not it suffices: Of course $491! = (491)(490)...(2)(1)$ so any multiple of 11 which is $<491$ will be here. That is, every $11 \cdot k $ for $k\le 45$ $\space$ (for $11*46>491$ ). So the exponent of $11$ in the prime decomposition of $491!$ will be at least $45$ . But we have to account for the numbers which are divisible by powers of $11$ . How to do that? Well, we've already established $k \le45$ so there is only one number with such property: $11^2$ . Now we just have to sum $44$ (for the numbers divisible only ""one time"" by $11$ ) and $2$ . That is $46$ . Now, for the exponent of $7$ the work to be done is analogous. It might seem a bit trivial to some of you, but it's bugging not to know whether my reasoning is correct. Thank you in advance for your time and help.","['number-theory', 'elementary-number-theory']"
3417963,Multivariate limit,"I have troubles with such limit $$ \lim_{(x,y)\rightarrow(0,0)} \frac{x^3}{y^4+2\sin^2{x}}$$ Nothing works, as I approach on any line or curve I get limit equal to $0$ . I try polar coordinates - also nothing, still $0$ .
I try to bound it somehow - I get infinity.
However, wolfram tells that there is no limit for the function in $(0,0)$ .
If I can get any hint, that would be great.","['limits', 'multivariable-calculus', 'trigonometry', 'multivalued-functions']"
3418100,"If $a^k|b^{k+100} $ for every $k$, then show $a|b$.","If $a^k|b^{k+100} $ for every $k$ , then show $a|b$ . Attempt If $k=1$ it shows that if $p | a$ then $p|b$ too, so $b$ has every prime divisors which $a$ contains. But I don't Know what to do now. Could anyone help?","['elementary-number-theory', 'divisibility', 'discrete-mathematics']"
3418136,Prove $A+A=\mathbb{R}$ in which the measure of the complement of $A$ is zero,"Let $A\subset \mathbb{R}$ such that $m(\mathbb{R}\smallsetminus A)=0$ . Show
   that $A+A=\mathbb{R} $ , where $$A+A=\{a+b\mid a,b\in A\}$$ . This is a question in the past qualifying exam in my university. I do not know where to approach. I encountered a similar problem  that if $A$ is measurable and $m(A)>0$ , then $A-A$ contains an interval, but I used $m(A)$ finite to do this problem. Can you help?","['measure-theory', 'lebesgue-measure', 'real-analysis']"
3418169,Set of Matrices Over Finite Field Whose Pairwise Differences are Invertible,"Given the full matrix ring over a finite field, $M := M_{n \times n}(\mathbb{F}_q)$ for prime $q$ and integer $n$ , what can one say about subsets $S$ of $M$ satisfying the condition that: $A,B \in S$ implies $A - B$ is invertible unless $A = B$ (n.b. I'm not making any assumptions on the invertibility of elements of $S$ themselves). Is it possible to construct, or show the existence of, relatively large subsets satisfying this condition? I'm particularly interested in the case where $q$ scales asymptotically and $n$ is a fixed, small constant, and by large subsets I'm attempting to construct $S$ with $\vert S \vert \approx O(q^n)$ , but suspect this is infeasible. As for my own attempts, I thought initially that given the collection of all subspaces of $\mathbb{F}_q^n$ of dimension $n-1$ , one ought to be able to choose one element from each subspace so that the difference of any two elements is full rank, and thus invertible. This would lead to a set $S$ of size ${n \choose n-1}_q = 1+ \dots + q^{n-1}$ , but I am unable to prove that this is either achievable or impossible. Asymptotically, I've only been able to come up with obvious naive sets of size $q$ , but playing around with small cases suggests that larger sets are possible in some cases, just they do not exhibit an obvious pattern (at least not obvious to me).","['matrices', 'finite-fields', 'linear-algebra', 'matrix-rank']"
3418185,Show that $G = M \circledast N$ has a diagonal subgroup iff $M$ is isomorphic to $N$.,"A subgroup $D$ of $G = M \circledast N$ is a diagonal subgroup provided: $$D \cap M = 1 = D \cap N$$ $$DM = G = DN$$ (Where $\circledast$ is denoting the internal direct product of $M$ and $N$ .) $$$$ GOAL: Show that $G$ has a diagonal subgroup iff $M \cong N$ . First assume G has a diagonal subgroup as described. Since $G = M \circledast N$ is the internal direct product of $M$ and $N$ we know $M$ and $N$ are normal in $G$ , $G=MN$ and $M \cap N = 1$ . We must show that $M \cong N$ . $$$$ (Here are a few results that may or may not be helpful for getting to a solution. If $G = M \circledast N$ then $G \cong M \times N$ . If $G = M \circledast N = M \circledast L$ then $N \cong L$ .) I need help with the other direction as well. You have my appreciation in advance.","['normal-subgroups', 'group-theory', 'abstract-algebra', 'group-isomorphism']"
3418192,"If integral is 0 on any set of measure 1/pi, then the function is 0 a.e.","This is a problem in my Qualifying Exam. ""Suppose $f:[0,1]\to \mathbb{R}$ is in $L^1$ (Lebesgue measure) and for
   every measurable $A\subset [0,1]$ with $m(A)=\frac 1{\pi}$ we have $\int_A f dm=0$ . Prove that $f=0$ a.e."" I could not do it back then. I did my research and we have a similar problem here Integral vanishes on all intervals implies the function is a.e. zero . But the same method cannot be applied. Anyway, I cannot think of anything except for let $B$ be a set of measure $1/4$ and try to make the integral 0. However, I forgot that this is on the real line, so there is no monotonicity here. Anyone can help?","['measure-theory', 'lebesgue-measure', 'lebesgue-integral', 'real-analysis']"
3418257,Is $f(x) = x^2$ a function of $Z \rightarrow Z^{+}$?,"For some function $f: Z \rightarrow Z^{+}$ Is $f(x) = x^2$ a function that maps all integers to all positive integers? According to my textbook, it is, but
I am unsure because for $x=0$ , $f(0) = 0$ which is not in the target $Z^{+}$ . So does that mean it is not a function of $f: Z \rightarrow Z^{+}$ ? Textbook: Find a function whose domain is the set of all integers and whose
  target is the set of all positive integers that satisfies each set of
  properties. (a) Neither one-to-one, nor onto. Solution: $f(n) = n^2$",['functions']
3418323,Prove $\frac{\text{Area}_1}{c_1^2}+\frac{\text{Area}_2}{c_2^2}\neq \frac{\text{Area}_3}{c_3^2}$ for all primitive Pythagorean triples,"Important update Yam Mir has found a more general form and Mathlove has found a necessary condition but as of now the problem is still open. Earlier I posted this pretty gross equality that I was trying to prove, $a,b,c,d,e,f \in \mathbb{N}-0, \gcd(a,b)=1 \  \wedge \ \gcd(c,d) = 1 \ \wedge \ \gcd(e,f) = 1, (a,b) \neq (c,d) \neq (e,f)$ $$\Rightarrow \frac{4a^3b-4ab^3}{a^4+2a^2b^2+b^4} + \frac{4c^3d-4cd^3}{c^4+2c^2d^2+d^4} \neq \frac{4e^3f-4ef^3}{e^4+2e^2f^2+f^4}$$ Which I completely missed some beautiful underlying math for, I've found that the terms can be rewritten as such, $$\frac{4m^3n-4mn^3}{m^4+2m^2n^2+n^4}=\frac{4mn(m-n)(m+n)}{(m^2+n^2)(m^2+n^2)} = \frac{4mn(m^2-n^2)}{(m^2+n^2)^2}$$ Since it is parameterized as $\gcd(a,b) = 1 \ \wedge \ a>b>0$ . It can be parametrized as a primitive Pythagorean triple! So now let, $$a=2mn, b=m^2-n^2,c=m^2+n^2$$ we get, $$\frac{2a_1b_1}{c_1^2}+\frac{2a_2b_2}{c_2^2} \neq \frac{2a_3b_3}{c^2_3}$$ Where $a_n,b_n,c_n$ form a primitive Pythagorean triple dividing by four yields, $$\frac{ab}{2c^2} = \text{Area}\cdot\frac{1}{c^2}$$ For terminology sake let's call this the characteristic ratio of a primitive Pythagorean triple. My conjecture is that for all primitive Pythagoreon triples, $$\frac{a_1b_1}{2c_1^2}+\frac{a_2b_2}{2c_2^2}\neq \frac{a_3b_3}{2c_3^2}$$ Interestingly I've found, $$\frac{1}{c_n^2} \approx \frac{1}{4n^2\pi^2}$$ plotting ratios from the original equation gives this curve indicating some kind of cyclical phenomenon, Another thing I've observed, $$\max{\frac{2a_nb_n}{c_n^2}} = 1$$ Additionally the numerator of the original inequality appears to be all congruent numbers apart of this sequence ! So to sum things up I'm trying to show that, $$\frac{\text{Area}_1}{c_1^2} + \frac{\text{Area}_2}{c_2^2} \neq \frac{\text{Area}_3}{c_3^2}$$ For all primitive Pythagorean triples or find a counter example. I'd also like to know why this may be true and if there is any regularity to the cyclical phenomenon showed? Must these ratios be unique given that primitive triples are rooted in prime factorization? What geometric meaning can be drawn from $\frac{\text{Area}}{c^2}$ , why the hypotenuse squared? (note these ratio's might also flirt with the Dirichlet L-function and or elliptic curves.) Edit @mathlove found a counter example but I unfortunately wrote the wrong parameterization failing to list $a>b>0$ so I am still looking for a different counter example. The problem is still open Edit for bounty: To be very specific about what I'm asking for, I'd like to prove $\frac{\text{Area}_1}{c_1^2}+\frac{\text{Area}_2}{c_2^2} \neq \frac{\text{Area}_3}{c_3^2}$ for all primitive Pythagorean triples or find a counter example. The other questions would be nice but is in no way a requirement to receive the bounty. This bounty will cost me almost $1/3$ of my reputation so even just commenting and sharing thoughts/ideas would go a long way.","['number-theory', 'diophantine-equations', 'geometry', 'real-analysis']"
3418383,"Does $\sum_{n=1}^\infty a_n^2 \text{ converges },\ a_n \geq 0 \implies \sum_{n=1}^\infty \frac{a_n}{n} \text{ converges}$? [duplicate]","This question already has answers here : If $\sum_{n=1}^{\infty} a_n^{2}$ converges, then so does $\sum_{n=1}^{\infty} \frac {a_n}{n}$ (6 answers) Closed 4 years ago . Let $\sum_{n=1}^\infty a_n^2$ a converging series with $a_n \geq 0$ . Decide whether $\sum_{n=1}^\infty \frac{a_n}{n}$ is converging. Note: you may consider the fact that $\sum_{n=1}^\infty \frac{1}{n^2}$ is converging. I'm having problems with the above task. Given the hint in the note, I believe the latter series is indeed converging and am trying to prove it. Unfortunately, I fail to find a promising approach to do so. Obviously, $\sum_{n=1}^\infty a_n^2 \neq \left(\sum_{n=1}^\infty a_n\right) \cdot \left(\sum_{n=1}^\infty a_n \right)$ , which would be a rookie mistake. Similarly, $\sqrt{\sum_{n=1}^\infty a_n^2} \neq \sum_{n=1}^\infty a_n$ . Therefore, the square seems difficult to get rid of. I thought of arguing using the Cauchy product, however, I'm not sure whether it could be applied ""in reverse"" to show that if the series based on $a_n^2$ is converging, then the one based on $a_n$ is, too. What approach would you take to solve this problem?","['convergence-divergence', 'sequences-and-series', 'real-analysis']"
3418400,How to find explicitly the Klein j-invariant,"Let $K$ be a field, and let $K(x)$ be the associated field of rational functions. I want to find the subfield $L$ of $K(x)$ of the rational functions that are invariant under this set of transformations: $$x\mapsto x$$ $$x\mapsto \frac{1}{x}$$ $$x\mapsto 1-x$$ $$x\mapsto \frac{x}{x-1}$$ $$x\mapsto \frac{1}{1-x}$$ $$x\mapsto 1-\frac{1}{x}$$ By Lüroth theorem I know that there exist a rational function $\ell(x)$ such that $L=K(\ell(x))$ and I know that I can take $$\ell(x)=\frac{(x^2-x+1)^3}{x^2(x-1)^2}$$ In each book or lecture notes I have found, this object is only defined, and later is proved to be invariant, but I was interested in how to find it without too many calculations. Are there some references or can you give a hint? Many thanks in advance.","['mobius-transformation', 'galois-theory', 'cross-ratio', 'algebraic-geometry', 'rational-functions']"
3418437,Trying to find a converse (or counterexample of it) to implicit function theorem,"The implicit function theorem stated in Spivak's Calculus of Manifolds is as follows: My question is: if the above $f$ is continuously differentiable on an open set of $(a,b)$ and $f(a,b)=0$ and moreover $\exists$ open set $A$ containing $a$ and open set $B$ containing $b$ and a continuously differentiable function $g$ such
  that for each $x\in A$ there is a unique point $g(x) \in B$ such that $f(x,g(x))=0$ is it necessarily true that $M$ is invertible? I ask this because I was talking with my professor about this (and maybe I misheard) but he said that the converse to implicit function theorem is easy to see. However, I do not know where to begin with this.","['multivariable-calculus', 'implicit-function-theorem', 'examples-counterexamples']"
3418444,Find partial sum formula for $\frac{n^3}{n!}$,"I need to find the partial sum formula for $\sum\limits_{n=0}^\infty \frac{n^3}{n!}$ I started by calculating some elements of the formula, but I could not find any possible patterns. Could you please help me in finding the partial sum formula? I need it in order to be able to calculate the sum of the series. If there is any easier way to do this than finding a partial sum formula, please let me know. Thank you in advance","['analysis', 'sequences-and-series']"
3418461,"Let $A = \{1\}, B = \{3\}$ Suppose $f = \{(1,3),(5,6)\}$ Is $f: A \rightarrow B$?","Let $A = \{1\}, B = \{3\}$ Suppose $f = \{(1,3),(5,6)\}$ Is $f$ a function from $A$ to $B$ ? The definition of the function, say $F$ , that the book gives: $$\tag 1 \forall a \in A \exists! b \in B ((a,b) \in F)$$ Then $F: A \rightarrow B$ Coming back to our example: although we have $(5,6) \in f$ and $5 \notin A$ and $6 \notin B$ , but, if I get it correctly, main condition of the function is not violated, hence I believe that yes , $f$ is a function from $A$ to $B$ . I've tried to answer my own question, but I need your verification:  is $f$ a function from $A$ to $B$ ?","['elementary-set-theory', 'functions']"
3418481,Direct Image by a Blow up,"Let $\pi : \widetilde{X} \longrightarrow X$ be the blow up morphism of $X$ a long of $Y \subset X$ , with exceptional divisor $E$ and $\text{dim}Y > 0$ ,   where $X$ and $Y$ are smooth projectives schemes. For an answer to a question asked here in the forum ( Blowing up and direct image of line bundle ), Blowing up and direct image of line bundle), we have: $$\pi_{*}\mathcal{O}_{\widetilde{X}}(-E) = I_{Y/X} \tag{$**$}$$ here $I_{_{Y/X}}$ denotes the ideal sheaf of $Y$ . $(**)$ was obtained through the following exact sequence: $$0 \longrightarrow \mathcal{O}_{\widetilde{X}}(-E) \longrightarrow \mathcal{O}_{\widetilde{X}} \longrightarrow \mathcal{O}_{E} \longrightarrow  0$$ My doubt is: How to find $\pi_{*}\mathcal{O}_{\widetilde{X}}(-nE)$ for $n \geq 0$ ? What if $n < 0 $ ? Thank you.","['pushforward', 'algebraic-geometry', 'blowup', 'sheaf-theory']"
3418494,"Solve $(A- \operatorname{diag}(x) ) \,\nabla_x f(x) - c f(x)=0, \, f(0)=1$","Let $f : \mathbb{R}^n \to \mathbb{R}$ . How to solve the following differential equation $$
(A-  \operatorname{diag}(x) ) \nabla_x f(x) - c f(x)=0,  \qquad  f(0)=1.
$$ where $\operatorname{diag}(x)$ is a diagonal matrix with vector $x$ on the main diagonal, $A$ is some $n \times n$ matrix  and $ c \in \mathbb{R}^n$ ? In the scalar case, this is easy to solve since it is a first-order linear ordinary differential equation (ODE) whose solution is given by $$f(x) = a^{c} (a -  x)^{-c}.$$ I don't have have much experience solving matrix differential equations and would appreciate some references on this topic. Edit: Using the approach of NN2 the above PDE can be reformulated as: \begin{align}
(A-  \operatorname{diag}(x) )  \nabla_x g(x)=c,  \qquad g(0)=0.
\end{align}","['ordinary-differential-equations', 'reference-request', 'linear-algebra', 'vector-analysis', 'partial-differential-equations']"
3418528,Lift $k$-valued points $X(k)$ to $X(A)$ of a smooth scheme over Henselian local ring,"I have some problems to prove the exercise 4.13 (on page 39) from J.S. Milne's Etale Cohomology (the ""book"", not the online accessible script!): Let $A$ be a Henselian local ring and $k=A/m$ it's residue field. We consider a smooth $A$ -scheme $X$ with canonical map $f: X \to A$ . Problem The exercise is to show that the map $$\operatorname{Hom}(\operatorname{Spec}(A), X)=X(A) \to X(k) = \operatorname{Hom}(\operatorname{Spec}(k), X)$$ between $A$ -and $k$ -valued points induced by $A \to k$ is surjective. or in other words that each map $\varphi: \operatorname{Spec}(k) \to X$ obtains a lift in $X(A)$ . This is what I tried : let $x = \varphi(\{*\})$ the unique image of $\varphi$ . since $f$ is smooth, there exist an open subscheme $U \subset X$ containing $x$ and an open $V \subset \operatorname{Spec}(A)$ with $f(U) \subset V$ such that the restriction of $f$ to $U$ factorizes as $$U \xrightarrow{\text{g}} \mathbb{A}^n_V \xrightarrow{\text{h}} V$$ with $g$ étale and $h$ canonical. (this was application of Prop. 3.24(b) following the hint). Assume after restricting if neccessary that $U:=\operatorname{Spec}(B)$ and $V:=\operatorname{Spec}(R)$ are affine. since $V$ is open in $\operatorname{Spec}(A)$ , we can assume that $R= A_s$ (i.e. a localization of $A$ at a $a \in A$ ). The problem translates to comm. algebra: abusing notation we have an etale ring map $g:A_s[X_1,\ldots, X_n] \to B$ and $\varphi: B \to k$ , which we want to lift to $\bar{\varphi}: B \to A$ . Problems : (1) In genral localizations of Henselian ring are not Henselian, thus $A_s$ is not Henselian in general, thus we cannot at this point apply Henselian lifting theorem to lift $A_s[X_1,\ldots, X_n] \to k$ to $A$ . We need an argument that we can choose $s \in A$ such that $A_s=A$ . (2) assume we solved problem (1) and have a lift $A_s[X_1,\ldots, X_n] \to A$ . can we show that it factorizes through $B$ ? which characterization of étaleness of $g$ could at this point do it's job? Could anybody help me how to solve this problem? Update #1 : I think I have solved (1): $f \circ \varphi$ maps $\{*\}$ to the unique closed point $x_{\mathfrak{m}}$ of $\operatorname{Spec}(A)$ and every open $V \subset \operatorname{Spec}(A)$ , which contains $x_{\mathfrak{m}}$ is already $\operatorname{Spec}(A)$ , since $A$ local. therefore we can assume $V=\operatorname{Spec}(A)$ . What do we know about etale $g:A[X_1,\ldots, X_n] \to B$ and Hensel lifts? Is there a criterion which allows to lift zeros of more then one polynomial simulaneously?","['etale-cohomology', 'algebraic-geometry', 'schemes']"
3418562,What benefits does the two stage variant of Pollard's p-1 have? How is it different?,"There is a two stage variant of Pollard's p-1 . In terms of application what is the difference between it and the normal one? Is the two stage variant faster? Is the only practical difference that it can find one more factor larger than the bound $B1$ ? There are many two-stage implementations but I'm having trouble following the code. Some of the programming languages I don't get and some I suspect have bugs (because variables appear out of no where): Python example in Question Two more Python examples in Answers Scheme language implementation? Haskell language implementation In the first few links there is a ""cache"", what does cache have anything to do with it? Can anyone provide me with an example of using the two-stage variant in a situation where the one stage would fail?","['number-theory', 'factoring', 'discrete-mathematics', 'algorithms']"
3418603,multiplicative reduction of a elliptic curve $E$ splits,"In Silverman's ""The Arithmetic of Elliptic Curves"" in Cap. VII.5 (Good and Bad Reduction) a multiplicative reduction of a elliptic curve $E$ ,
is said to be split if the slopes of the tangent lines at the node
are in $k$ , and otherwise it is said to be nonsplit. Q: what is the intuition and the origin of the usage of the word ""split"" in this context? what ""splits""? can we associate a certain s.e.s. to this reduction, which then splits or does the notation split come from another reason?","['algebraic-geometry', 'elliptic-curves']"
3418629,Pigeon Hole Principle (most probably),"Prove that any set of 46 distinct 2-digit numbers contains two distinct numbers which are relatively prime. This is what I am trying to prove. I have a feeling that it would be using the pigeon hole principle but I just cannot figure it out.
This is what I found interesting so far:
There are 90 2-digits numbers ([10,99]), which means that there are exactly 45 even numbers, which means that in a set with 46 numbers, there must be at least one odd number. Though I do not know what to make of that...
Also, 46 is optimal, in the snse that there exists a set of 45 distinct 2-digit numbers so that no two distinct numbers are relatively prime.","['elementary-set-theory', 'pigeonhole-principle', 'proof-writing', 'discrete-mathematics']"
3418655,Find the general solution of $\theta$ for which the following quadratic equation is the square of a linear function.,Find the general solution of $\theta$ for which the quadratic equation $$\left(\sin\theta\right)x^2+(2\cos\theta)x+\dfrac{\cos\theta+\sin\theta}{2}$$ is the square of a linear function. $$D=0$$ $$4\cos^2\theta-2\sin\theta\left(\sin\theta+\cos\theta\right)=0$$ $$2\cos^2\theta-\sin^2\theta-\sin\theta\cos\theta=0$$ $$2\cos^2\theta-2\sin\theta\cos\theta+\sin\theta\cos\theta-\sin^2\theta=0$$ $$2\cos\theta(\cos\theta-\sin\theta)+\sin\theta(\cos\theta-\sin\theta)=0$$ $$(\cos\theta-\sin\theta)(2\cos\theta+\sin\theta)=0$$ $$\tan\theta=1 \text { or } \tan\theta=-2$$ $$\theta=n\pi+\dfrac{\pi}{4} \text {  or  } \theta=\tan(2\pi-\tan^{-1}(2))$$ $$\theta=n\pi+\dfrac{\pi}{4} \text {  or  } \theta=n\pi+2\pi-\tan^{-1}(2)$$ $$\theta=n\pi+\dfrac{\pi}{4} \text {  or  } \theta=\pi(n+2)-\tan^{-1}(2)$$ $$\sin\theta\ne 0$$ $$\theta\ne m\pi \text { where $m \in$ I }$$ $$\theta=n\pi+\dfrac{\pi}{4} \text { can't be integral multiple of $\pi$ as } \theta=\dfrac{\pi(4n+1)}{4}$$ $$\theta=n\pi+\dfrac{\pi}{4} \text { is the valid solution }$$ $$\theta=\pi(n+2)-\tan^{-1}(2) \text { cannot be the integral multiple of $\pi$ as $\tan^{-1}(2)$ is not the integral multiple of $\pi$ } $$ $$\theta=\pi(n+2)-\tan^{-1}(2) \text { is the valid solution }$$ Hence $\theta=\pi(n+2)-\tan^{-1}(2) \text { or } \theta=n\pi+\dfrac{\pi}{4}$ But actual answer is $\theta= 2n \pi+\dfrac{\pi}{4} \text{or }  \theta =(2n+1)\pi - \tan^{-1}(2) \text { where $n \in$ I}$ I tried to find out the mistake but didn't get any breakthrough. What am I missing.,['trigonometry']
3418691,Boundedness of Riesz Transform on (subsets of) Hölder spaces?,"Definition and setup The Riesz transform for say $C^\infty_c(\mathbb R^d)$ functions $f$ is defined by a principal value integral, $$ Rf(x) := c_d \operatorname{pv}\!\!\!\int_{\mathbb R^d} \frac{y}{|y|^{d+1}}f(x-y) \, dy := c_d \lim_{\epsilon\downarrow 0} \int_{|y|>\epsilon} \frac{y}{|y|^{d+1}}f(x-y) \, dy,$$ The integral is interpreted componentwise, the constant $c_d$ is chosen so that the Fourier transform $\int_{\mathbb R^d} Rf(x)e^{-2\pi i x\xi} \, dx =  \frac{- i\xi}{|\xi|}. $ Wikipedia link . It is well-known that the Riesz transform is bounded on $L^p$ spaces, $p\in(1,\infty)$ , and commutes with derivatives. Its therefore bounded on Sobolev spaces $W^{s,p}$ , $p\in(1,\infty)$ . Question I think I've seen before that the Riesz transform is bounded as a map $C^\alpha \cap L^p \to C^\alpha \cap L^p$ ? Is this true? A paper I've read casually remarked that the Riesz transform is bounded on Hölder and Sobolev Spaces, and I presume this is the kind of result they mean. I gave it a few naive tries. I can show that under the assumption that $f\in L^p \cap C^\alpha, p\in[1,\infty)$ , the integral form is well-defined and $Rf\in L^\infty$ , with for any $\lambda>0$ $$ |R_jf(x)| \lesssim_d  [f]_\alpha \lambda^\alpha + \|f\|_{L^p} \lambda^{-d/p}$$ or if you try to minimise in $\lambda$ you get something like $\|Rf\|_{L^\infty} \lesssim_d [f]_\alpha^{\frac{d/p}{\alpha+d/p}}\|f\|_{L^p}^{{\frac{\alpha}{\alpha+d/p}}}$ . But I feel like I'm missing a ""standard trick"" to continue to estimate $[Rf]_\alpha$ (in particular I don't know how to use the cancellation), and revisiting some books like Stein's, I couldn't find the result or the trick I feel I need. Any pointers? Update A friend has pointed out that it is in Stein's book, in the form of a ""Further Result"" (i.e. exercise). It is 6.9 on pages 50-51. He gives the hint that if the Kernel $\frac{\Omega(y)}{|y|^d}$ (in our case $\Omega(y) = y/|y|$ ) is sufficiently smooth then the proof is ""elementary"" (NB the quotation marks are Stein's), and directs the reader to 3 references: J. Privalov ""Sur Les fonctions conjuguées,"" Mat. Zeit. 26 (1927), 218-244. A. P. Calderón and A. Zygmund, ""Singular Integrals and Periodic Functions,"" Studia Math. 14 (1954), 249-271. M. H. Taibleson, ""The preservation of Lipschitz spaces under singular integral operators,"" Studia Math. 24 (1963), 105-111. So it might be possible to distill an answer from one of these papers...","['harmonic-analysis', 'holder-spaces', 'functional-analysis']"
3418701,The four-penny problem,"Suppose you have four pennies, say $P_1$ , $P_2$ , $P_3$ , and $P_4$ , a weighing balance with two pans, and a set of weights. You are allowed to make only four weighings of the pennies. In each weighing, you may place some (possibly none) of the pennies in the left pan, some (possibly none) in the right pan, and leave the remaining pennies (if any) off the balance. The problem is this: How should the four  weighings be designed so as to produce the most accurate determination of the weights of the four pennies, in the sense that the sum of the variances of the four weight estimators should be
as small as possible? Wee assume that weighings are independent and that measuring errors
have mean $0$ and a common variance, say $\sigma^2$ , that does not depend on the configuration of pennies on the scale. Let $\beta_1$ , $\beta_2$ , $\beta_3$ , $\beta_4$ be the unknown weights of the four pennies. For $i = 1,...,4$ , let $W_i$ be the random variable recording the result of the $i^th$ weighting; adopt the convention that weights placed in the
left (respectively, right) pan are treated as being positive (respectively, negative). The assumptions of the problem imply that for any given weighing design, the random vector $W=(W_1 ,W_2 ,W_3 ,W_4 )^T$ is weakly spherical and for each $i$ , $$E(W_i)=\sum^4_{j=1}x_{ij}\beta_j$$ where $$
x_{ij}=
\begin{cases}
-1,& \text{if penny} ~j~ \text{is to be placed in the left pan}\\
0,& \text{if penny} ~j~ \text{is to be left off the balance}\\
1,& \text{if penny} ~j~ \text{is to be placed in the right pan}
\end{cases}
$$ in the $i^th$ weighing. The four-penny problem then be restated as follows: For what weighing design, that is, for what $4\times 4$ (nonsingular) design matrix $X=(x_{ij})$ having for its elements only $1$ ’s, $0$ ’s, and $−1$ ’s, is $$\sum^4_{j=1} Var(\hat{\beta}_j)$$ the smallest? Where $\hat\beta_j=\langle cv(\beta_j),W\rangle$ a Gauss-Markov estimator of $\beta_j$ , with the coefficient vector $cv(\beta_i)$ .","['statistics', 'linear-algebra']"
3418712,"An elegant, purely topological definition of a manifold?","The standard definition of a topological manifold has at its core that it is locally homeomorphic to $\mathbb{R}^n$ at each point, with some other topological conditions to weed out pathological cases. This obviously works fine practically speaking, and captures the jist of what we want out of manifolds as generalisations of euclidean space (taken topologically). However, I wanted to know if there was an equivalent, purely topological definition for manifolds also, especially a nice one? The normal definition relies of the structure of the reals for its construction which I find a little displeasing philosophically and aesthetically, given that many spaces in mathematics and physics happen to intrinsically be manifolds without involving the reals at all. We could of course hack together an equivalent definition by replacing the real line with an equivalent space defined purely topologically, and use that to create a topological stand-in for $\mathbb{R}^n$ , but that feels very messy and bad (logically correct, but morally wrong). Very un-insightful. So, is there a good definition of a manifold, purely in terms of topological primitives?","['manifolds', 'general-topology', 'definition', 'soft-question']"
3418730,Hausdorff spaces and continous functions on dense sets,"I have a problem with an exercise regarding the equivalence of the Hausdorff property with a statement about continuous functions on dense sets. Let X be a topological space. Show that the following assertions are equivalent: a) X is a Hausdorff space. b) The diagonal $\Delta(X)=\{(x,y) \in X \times\ X: x=y \}$ is closed in $X \times \ X $ . c) For every space Z, every dense set D $\subset$ Z and every pair of maps $f_1, f_2: Z \to X$ one has $f_1=f_2$ if and only if $f_1|_D=f_2|_D$ . Additionally, show that the diagonal embedding $\Delta: X \to X \times \ X$ is continous. I have proven that the diagonal embedding is continuous and that the implications a) $\Rightarrow$ b), b) $\Rightarrow$ c) hold. All I have left to do is to prove that c) $\Rightarrow$ a). This is my idea: Suppose that $X$ is not a Hausdorff space. Then there are $x,y \in X, x \neq y$ such that for all open sets $O_x, O_y$ where $x \in O_x, y \in O_y$ we find $u \in O_x \cap O_y.\tag{1}\label{1}$ Now I wanted to define $$D=\{u \in X: u \,\rm satisfies\,\eqref{1} \}$$ and prove that $\bar{D}=X$ and define maps $f_1, f_2: X \to X$ where $f_1, f_2$ are the identity on $D$ but do not agree on $X$ , which would be a contradiction to c). The problem is that I got stuck with the proof that $\bar{D}=X$ . I have to show that for $v \in X$ and every open set $O_v$ containing v I get $O_v \cap (D \setminus\{v\}) \neq \emptyset$ , but I do not see how.",['general-topology']
3418734,"$f: A \rightarrow C$ and $g: B \rightarrow C$, prove $f \cup g: A \cup B \rightarrow C$ iff $f \restriction (A \cap B) = g \restriction (A \cap B)$","Suppose $f: A \rightarrow C$ and $g: B \rightarrow C$ . Prove that $f \cup g: A \cup B \rightarrow C$ iff $f \restriction (A \cap B) = g \restriction (A \cap B) $ My attempt: If I'm getting the definition of the restriction correctly, we have $$ f \restriction (A \cap B) = f \cap \bigl((A \cap B) \times C\bigr)$$ and $$ g \restriction (A \cap B) = g \cap \bigl((A \cap B) \times C\bigr)$$ $(\rightarrow)$ Suppose $f \cup g: A \cup B \rightarrow C$ Take $(x,y) \in f \restriction (A \cap B)$ . Then $(x,y) \in (A \cap B) \times C$ and $x \in B$ . Since $x \in B$ , there exists some $b \in B$ such that $(x,b) \in g$ . Since $(x,b) \in f \cup g$ and $(x,y) \in f \cup g$ , we conclude that $y = b$ . Hence $(x,y) \in g \restriction A \cap B$ , and $f \restriction (A \cap B) \subseteq g \restriction (A \cap B)$ . By the identical reasoning, we can show that $g \restriction (A \cap B) \subseteq f \restriction (A \cap B)$ $(\leftarrow)$ Suppose $f \restriction (A \cap B) = g \restriction (A \cap B)$ Existence: Take $x \in A \cup B$ . Then $x \in A$ or $x \in B$ , which implies that there exists some $c \in C $ such that either $(x,c) \in f$ or $(x,c) \in g$ . Hence $(x,c) \in f \cup g$ . Uniqueness: Suppose $(x,y),(x,p) \in f \cup g$ . If both elements are either in $f$ or in $g$ , then clearly $y = p$ . Suppose $(x,y) \in f$ and $(x,p) \in g$ . Since $x \in A \cap B$ and $y \in C$ , we have $(x,y) \in (A \cap B) \times C$ and therefore $(x,y) \in f \restriction A \cap B$ By the similar reasoning, we have $(x,p) \in g \restriction A \cap B$ . For all $x$ in $Dom(f \restriction A \cap B)$ there is only one $k \in Ran(f \restriction A \cap B)$ such that $(x,k) \in f \restriction A \cap B$ The same reasoning applies to $g \restriction A \cap B$ . Since $f \restriction (A \cap B) = g \restriction (A \cap B) $ , we have $p = y$ . If $(x,p) \in f$ and $(x,y) \in g$ , then by the same reasoning, we conclude that $p = y$ We've shown existence and uniqueness, hence the result $$f \cup g: A \cup B \rightarrow C$$ $\Box$ Is it correct?","['elementary-set-theory', 'functions', 'proof-verification']"
3418755,Prove the $\cos^n \phi$ and $\sin^n \phi$ property,"Question : Prove that $$\cos^n{\phi}=\frac{1}{2^{n-1}}\left(\cos {n\phi} + n\cos {(n-2)\phi}
 + \frac{n(n-1)}{2!}\cos{(n-4)\phi}+\cdots + R_n\right)$$ Where $$R_n= \begin{cases}
 &\frac{n!}{\left(\frac{(n-1)}{2}\right)!\left(\frac{n+1}{2}\right)!}\cos
\phi\quad &\text{if $n$ is odd}\\ &\frac{n!}{2\left[\left(\frac n
 2\right)!\right]^2}\quad &\text{if $n$ is even} \end{cases}$$ And derive the similar result for $\sin^n \phi$ Well, i have two ideas about it: First, i have to use power reducing formula. Second, i have to use De Moivre's Theorem (But, i doubt it) Please, which one of them will work? Or, maybe if you have a correct or better idea then tell me. And what about $\sin^n \phi$ ? Is the idea will be the same? Thanks for all. Edit : What about i'm using the fact that $$\cos^2 \phi=\frac{1+\cos {2\phi}}{2}$$ But my problem is the first term on that property, that is $1$ , cz as we know, we don't see the term $1$ in that equation if we use binomial expansion. Or could it be vanished by another coefficients?",['trigonometry']
3418767,What is the truth set of this logic?,"I am trying to solve this question .  From the prompt, I figure: $S = \{1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19\}$ $p = \{1,  3,  5,  7,  9\}$ $q = \{1,2,3,4,5,6,7,8,9,10,11,12,13,14,15\}$ I appears $p → q$ is $\{1,2,3,4,5,6,7,8,9,10,11,12,13,14,15\}$ , which is $q$ .  How come my answer is wrong?  Am I missing something?","['elementary-set-theory', 'logic']"
3418794,Empty set operations,"Could anyone correct me? $∅ × ∅ = ∅$ ${\{∅\}} × {\{\{∅\}\}} = \{∅, \{∅\}\}$ $∅^{\{∅\}} = ∅$ $\{∅\}^{\{∅\}} = \{\{∅, ∅\}\}$ $\{∅\} - P(\{∅\}) = ∅$ $2^{\{\{\{\}\}\}} = \{∅, \{\{\{\}\}\}\}$",['elementary-set-theory']
3418796,On an equivalent definition of cohomological dimension,"I'm having some trouble with an equivalent definition of cohomological dimension. What I have as definition is Given a group $\Gamma$ , the comological dimension is defined as $$cd\,\Gamma=\inf\{n\in \mathbb N: \exists \epsilon:P\to \mathbb Z\to 0\,\,\text{projective resolution of length}\,\, n\}$$ where $\mathbb Z$ is provided with the trivial $\mathbb Z \Gamma$ -module structure. I know that this is equivalent to taking $$\sup\{k\in \mathbb N:H^k(\Gamma, M)\neq0\, \text{for some module M}\}$$ which kinda fits the intuition of the cohomogical dimension being the largest non-cero degree of cohomology. However, when trying to prove this equivalent definition I come across the following: Take a projective resolution of minimal length $n=cd\, \Gamma$ (finite for the moment), And consider the co-chain complex that results from taking $Hom_{\Gamma}(P,M)$ , where $M$ is an arbitrary $\Gamma-module$ . Since $P_m=0,\forall m>n$ we get $$0\leftarrow Hom_\Gamma(P_n,M)\leftarrow\dots \leftarrow Hom_\Gamma(P_0,M)\leftarrow0$$ Now from what I tried so far, proving the result is a matter of finding $M$ such that $H^n(Hom_\Gamma(P_n,M))\neq0$ , which in turn is the same as $Hom_\Gamma(P_{n-1},M)\to Hom_\Gamma(P_n,M)$ being non-surjective for some $M$ . And here's where I'm stuck! I can't seem to be able to find such $M$ . I apologize if my question is rather trivial, but it's the first time I come across the definitions and all that. Any help would be greatly appreciated!","['projective-module', 'group-cohomology', 'group-theory', 'homology-cohomology', 'algebraic-topology']"
3418797,Transformation law Christoffel symbols,"I have to derive the transformation law for the Christoffel symbols: Let $\Gamma_{bc}^{a} = \frac{1}{2}g^{ad}\big(\partial_{b}g_{dc}+\partial_{c}g_{bd}-\partial_{d}g_{bc}\big)$ be the Chritoffel symbols in a basis denoted by $\{x^{i}\}$ and $\overline{\Gamma}_{\beta\gamma}^{\alpha} = \frac{1}{2}\overline{g}^{\alpha\delta}\big(\overline{\partial}_{\beta}\overline{g}_{\delta\gamma}+\overline{\partial}_{\gamma}\overline{g}_{\beta\delta}-\overline{\partial}_{\delta}\overline{g}_{\beta\gamma}\big)$ be the Chritoffel symbols in a basis denoted by $\{\overline{x}^{i}\}$ . Using the facts: (1) $\overline{g}^{\alpha\delta} = \frac{\partial\overline{x}^{\alpha}}{\partial x^{a}}\frac{\partial\overline{x}^{\delta}}{\partial x^{d}}g^{ad}$ (2) $\frac{\partial}{\partial \overline{x}^{\beta}}\overline{g}_{\delta\gamma} = g_{dc}\frac{\partial}{\partial \overline{x}^{\beta}}\big (\frac{\partial x^{d}}{\partial \overline{x}^{\delta}}\frac{\partial x^{c}}{\partial \overline{x}^{\gamma}}\big ) + \frac{\partial x^{d}}{\partial\overline{x}^{\delta}}\frac{\partial x^{c}}{\partial \overline{x}^{\gamma}}\frac{\partial x^{f}}{\partial \overline{x}^{\beta}}\frac{\partial}{\partial x^{f}}g_{dc}$ (3) $\frac{\partial}{\partial \overline{x}^{\gamma}}\overline{g}_{\beta\delta} = g_{bd}\frac{\partial}{\partial \overline{x}^{\gamma}}\big (\frac{\partial x^{b}}{\partial \overline{x}^{\beta}}\frac{\partial x^{d}}{\partial \overline{x}^{\delta}}\big ) + \frac{\partial x^{b}}{\partial\overline{x}^{\beta}}\frac{\partial x^{d}}{\partial \overline{x}^{\delta}}\frac{\partial x^{g}}{\partial \overline{x}^{\gamma}}\frac{\partial}{\partial x^{g}}g_{bd}$ (4) $\frac{\partial}{\partial \overline{x}^{\delta}}\overline{g}_{\beta\gamma} = g_{bc}\frac{\partial}{\partial \overline{x}^{\delta}}\big (\frac{\partial x^{b}}{\partial \overline{x}^{\beta}}\frac{\partial x^{c}}{\partial \overline{x}^{\gamma}}\big ) + \frac{\partial x^{b}}{\partial\overline{x}^{\beta}}\frac{\partial x^{c}}{\partial \overline{x}^{\gamma}}\frac{\partial x^{h}}{\partial \overline{x}^{\delta}}\frac{\partial}{\partial x^{h}}g_{bc}$ and rename $f\to b$ , $g\to c$ and $h\to d$ I get the result: $\overline{\Gamma}_{\beta\gamma}^{\alpha} = \frac{\partial \overline{x}^{\alpha}}{\partial x^{a}}\frac{\partial x^{b}}{\partial \overline{x}^{\beta}}\frac{\partial x^{c}}{\partial \overline{x}^{\gamma}}\Gamma_{bc}^{a} + N$ , where N is the cumbersome expression $N = \frac{1}{2}\frac{\partial\overline{x}^{\alpha}}{\partial x^{a}}\frac{\partial\overline{x}^{\delta}}{\partial x^{d}}g^{ad}\bigg [g_{dc}\frac{\partial}{\partial \overline{x}^{\beta}}\big (\frac{\partial x^{d}}{\partial \overline{x}^{\delta}}\frac{\partial x^{c}}{\partial \overline{x}^{\gamma}}\big ) +  g_{bd}\frac{\partial}{\partial \overline{x}^{\gamma}}\big (\frac{\partial x^{b}}{\partial \overline{x}^{\beta}}\frac{\partial x^{d}}{\partial \overline{x}^{\delta}}\big ) - g_{bc}\frac{\partial}{\partial \overline{x}^{\delta}}\big (\frac{\partial x^{b}}{\partial \overline{x}^{\beta}}\frac{\partial x^{c}}{\partial \overline{x}^{\gamma}}\big )\bigg ]$ According to the solution, this term should be reducible to $N = \frac{\partial x^{a}}{\partial \overline{x}^{\gamma}}\frac{\partial x^{b}}{\partial \overline{x}^{\beta}}\frac{\partial^{2} \overline{x}^{\alpha}}{\partial x^{a}\partial x^{b}}$ . I tried it many times but I failed allways....Can anyone give me a hint? Especially, what should I do with the term $g^{ad}g_{bc}$ , which is appearing in $N$ ? Thanks!","['riemannian-geometry', 'tensors', 'change-of-basis', 'transformation', 'differential-geometry']"
3418882,How is the golden ratio related to these problems?,"I have created two problems which both have the golden ration integrated into the answer, and I would like an intuitive understanding as to why and how the golden ratio is linked to them. Problem 1: $\sin(108^\circ)/\sin(36^\circ) = \phi$ I would like to know a geometric understanding as to why this is (not that ""sin of 108 equals whatever and sin of 36 equals whatever""). Problem 2: Draw a square with corners a (top left),b (top right),c (bottom left),d (bottom right) with sides length x. Now draw a line of length ""l"" starting from the a to the midpoint between the b and d. This line will have a length of $x\sqrt{5}/2$ through the Pythagorean theorem. Then create a semicircle in line with the bottom (cd), with a corner on d, and then enlarge it such that it is just tangent with line l, like so: This semicircle will have a radius of r, which is what we are solving for in terms of x. To do this we look at the length l, from the tangent point to the midpoint of bd, which we will call $l_2$ (the other part of the line from a to the tangent point will be $l_1$ ). Through the fact that tangents from the same point on a cirle are the same length, we see that $l_2$ is the same as $x/2$ . Therefore $l_1$ is equal to $l-l_2$ , which is $x((-1+\sqrt{5})/2)$ (or otherwise written as $x(\phi - 1)$ ) To solve for r we first define the midpoint of the semicircle's diameter as the point R (capital). Then we create a triangle with corners a, c and R. The vertical side will be length x and the horizontal side will be length x - r, so the hypotenuse (aR) will be length $\sqrt{2x^2 -2xr + r^2}$ . Now construct a triangle with corners a, R and the tangent point of the semicircle with l. The hypotenuse of this triangle will be $\sqrt{(x(\phi-1))^2 + r^2}$ However, these belong to the same hypotenuse (aR) so we can set them equal to each other and get this system of equations: $\sqrt{2x^2 -2xr + r^2}$ = $\sqrt{(x(\phi-1))^2 + r^2}$ You can then solve for r (I won't show the working for this) and manipulate your answer with some golden ratio identities ( $\phi^2 = \phi + 1$ , and $1/\phi = \phi - 1$ ), to get this; $r = x\phi/2$ Is there a intuitive, visceral understanding for why the golden ratio pops up or is it just a coincidence (although I don't think it is). Thanks for helping!","['golden-ratio', 'trigonometry', 'geometry', 'recreational-mathematics']"
3418898,"Question Let $T=(V,E)$ be a tree with exactly 3 leaves, and $n$ vertices, as $n\ge 5$. Prove the following","Let $T=(V,E)$ be a tree with exactly 3 leaves, and $n$ vertices, as $n\ge 5$ .
  Prove the following: There is exactly one vertex in the tree of the degree $3$ . Prove that for every vertex $v\in V$ :  if $d_{T}(v)\neq 1, 3$ , then $d_{T}(v)=2$ My Attempt: 
1. 
I tried to prove that using a contradiction proof:
suppose by contradiction that there exists no vertex in the degree of $3$ , and then suppose by contradiction that there exists more then 1 vertex with the degree of 3 in $T$ . First, I started with the case for a tree with $5$ vertices. Given that there are $5$ vertices, then $|V| = |E|-1 \Longrightarrow 6=|E| -1$ such that $|E| = 4$ . By that I conclude that $\sum_{v\in V} d(v) =2|E| = 2\cdot 4 =8$ $d(v_1) +d(v_2)+d(v_3)+d(v_4) +d(v_5) = 8$ There are exactly 3 leaves in the tree ,therefore: $d(v_1) +d(v_2) +1+1+1 = 8$ $d(v_1) +d(v_2) = 5$ because $v_1, v_2$ can't be leaves, therefore WLOT $d(v_1) =2, d(v_2) =3$ , and we finished. now, suppose that a tree with $k$ vertices has exactly one vertex in the degree of 3. Prove that for a tree with $k+1$ vertices (or a tree with $k-1$ vertices) has exactly one vertex in the degree if 3. Problem is I don't know how to finish the proof from here. As I said, I wanted to use a contradiction proof, but found myself using induction (I can't see how to not use induction for that argument). Because I got stuck at the first section, I can't see how I can solve section No.2 as it's based of the first section, As  I understand.","['graph-theory', 'trees', 'proof-verification', 'discrete-mathematics']"
3418931,When does $\mbox{tr} \left( A^2 \right) = \mbox{tr} (A)^2$ hold?,"Suppose $A$ is a square matrix whose eigenvalues are $\lambda_1, \lambda_2, \dots, \lambda_n$ . The equation $$\mbox{tr} \left( A^2 \right) = \mbox{tr} (A)^2$$ implies that $$\sum_{i\not=j}\lambda_i\lambda_j=0$$ What can we learn about the matrix $A$ from the equation above? What kind of matrix has this property?","['matrices', 'trace', 'quadratics', 'linear-algebra']"
3418946,Gradient does not exist at a point or just vanishes (zero vectors),"Given the function $$f(x,y)=\begin{cases}
\frac {xy}{\sqrt{x^2+y^2}}, & x^2+y^2≠ 0 \\
0, & x=y=0
\end{cases}$$ find its gradient at point $(0,0)$ . If I had the first part of the function only, I would calculate the partial derivatives first: $$\nabla f(x,y)=\left(\frac{\partial f}{\partial x}(x,y),\frac{\partial f}{\partial x}(x,y)\right) =
\begin{pmatrix}
\frac{y\sqrt{x^2+y^2}-\frac{x^2y}{\sqrt{x^2+y^2}}}{x^2+y^2}\\
\frac{x\sqrt{x^2+y^2}-\frac{xy^2}{\sqrt{x^2+y^2}}}{x^2+y^2}\\
\end{pmatrix}=
\begin{pmatrix}
\frac{y^3}{(x^2+y^2)^{3/2}}\\
\frac{x^3}{(x^2+y^2)^{3/2}}
\end{pmatrix}
$$ Then would find the gradient value at point $(0,0)$ , which of course would not exist since the partial derivatives are not defined at this specific point. However, my function is defined at the point of interest differently, namely as $0$ . The partial derivatives of this function are also equal to $0$ . Thus, the gradient is also a zero vector $$\nabla f(0,0)=\left(\frac{\partial f}{\partial x}(0,0),\frac{\partial f}{\partial x}(0,0)\right)=(0,0).$$ Is this line of reasoning correct or are there any other subtleties pertaining to the continuity and differentiability of functions?","['partial-derivative', 'continuity', 'functions', 'vector-analysis']"
3418970,Grasshopper jumping on circles,"Can we characterize the grasshopper sequence? Let $n\in\mathbb N$ be the number of stones $s\in\{0,1,2\dots,n-1\}=S$ on a circle that the grasshopper can jump on. Let $v(s)$ be the number of times the grasshopper visited the stone $s$ . Initially, $v(s)=0$ for all $s\in S$ . The grasshopper takes a running start and starts jumping around, not planning to slow down. At the $k$ th jump say he is on some stone $s\in S$ . Say stone $s_L$ is $k$ steps anticlockwise, and stone $s_R$ is $k$ steps clockwise. He will jump on $s_L$ if $v(s_L)\le v(s_R)$ , otherwise he will jump on $s_R$ . That is, if $s$ is the current stone, then he either moves to: $$s\to (s-k)\bmod n$$ $$s\to (s+k)\bmod n$$ Depending which of the two stones was visited least amount of times till this point. Let $a(n)$ be the number of jumps the grasshopper makes before visiting all stones at least once, when jumping on $n$ stones placed on a circle. For example, if $n=5$ , he takes $10$ jumps to visit all stones, so $a(5)=10$ . The jumps are: $$\begin{array}{cccc}
k & (v(0), v(1), v(2), v(3), v(4)) & \text{jump} & {s}_{L}/{s}_{R} \\
-  & (0, 0, 0, 0, 0) & \\
0  & (1, 0, 0, 0, 0) &  s \to 0  & \\ 
1  & (1, 0, 0, 0, 1) &  0 \to 4  & L\\
2  & (1, 0, 1, 0, 1) &  4 \to 2  & L\\
3  & (1, 0, 1, 0, 2) &  2 \to 4  & L\\
4  & (1, 0, 1, 1, 2) &  4 \to 3  & R\\
5  & (1, 0, 1, 2, 2) &  3 \to 3  & L\\
6  & (1, 0, 2, 2, 2) &  3 \to 2  & L\\
7  & (2, 0, 2, 2, 2) &  2 \to 0  & L\\
8  & (2, 0, 3, 2, 2) &  0 \to 2  & L\\
9  & (2, 1, 3, 2, 2) &  2 \to 1  & R\\
\end{array}$$ WLOG grasshopper prefers anticlockwise $s_L$ , and the zeroth jump sets $s=0$ as starting stone. What can we say about $a(n)$ ? One easy pattern I noticed, is that it appears to be: $$a(n)=n \iff n\in\{2^m,m\in\mathbb N\}\cup\{1,3,7\}$$ Otherwise, it appears that $a_n(n)\gg n$ is much greater. Here is the plot of around first $15000$ terms of $a(n)$ : The red line below is $y=x$ and intersects $n=2^m$ 's since $a(2^m)=2^m$ . All the other terms are above the blue line which is $y=x^{1.14}-4$ , an exponential lower bound that I guessed holds so far. The green line intersects the origin $(0,0)$ and the term $a(10496)=108893$ , which is the farthest term from the average so far. We have $10496=2^8\cdot 41$ . $(Q_1)$ Can we prove all terms $a(n)$ will be finite? For fixed $n$ the jumping must be periodic eventually, but will that period contain every $s$ ? Or would this be too hard? - Since this looks like something similar to proving  Recamán's sequence visits all numbers, which is a known unsolved problem. For comparison, if we would always jump anticlockwise and only switch to clockwise if the $v(s_R)=0$ , then the sequence would not terminate for all $n$ (would not be finite for all $n$ ). Can we say anything else about the sequence $a(n)$ ? $(Q_2)$ Can we find closed forms for patterns other than when $n=2^m$ ? I'm not sure how to approach this. Can we determine when the clockwise jumps $R$ occur? Since the grasshopper prefers jumping on $s_L$ , maybe finding when jumps on $s_R$ (the $R$ jumps) occur, can help characterize the sequence. Hence, define $R(r,n):=$ "" $k$ th jump at which the grasshopper preformed the $r$ th $R$ jump."" Recall that if $n=2^m$ , we never preform the $R$ jump as first $n$ $L$ jumps visit all stones. Hence assume $n\ne 2^m$ . We wanted to first find a closed form for when first $R$ jump occurs. The first $R$ jump. That is, we look at $R(1,n)$ values, and it appears that it is $\approx n/l$ , for some $l\in\mathbb N$ . if $n=p$ is prime, then $R(1,p)\approx n/2$ , to be precise: ( $\delta\in\mathbb N$ depends on exact $n$ ) $$R(1,p)=\frac12(n+1)+\delta$$ if $n=2p$ where $p$ prime, then $R(1,2p)\approx n/4$ , to be precise: ( $\delta\in\mathbb N$ depends on exact $n$ ) $$R(1,2p)=\frac14(n+2)+\delta$$ if $n=2^m q$ where $q$ is odd, then $R(1,2^m q)\approx n/q$ . Depending on $l$ value, we get terms $R(1,n)$ accumulating around lines $y=x/l$ . Notice how odd $l$ are rarely populated, and how even $l$ are dense. For example, as already mentioned, primes gather around $l=2$ , and even semiprimes around $l=4$ . For an odd $l$ example, it seems the numbers of form $2^m\cdot 3$ belong to $l=3$ . $(Q_3)$ Can we find a closed form for $R(1,n):=$ $k$ -step at which we make the first $R$ jump? That is, can we find the exact $\delta$ values, depending on $n$ ? The first $R$ jump, when $n$ is prime. In the case of primes $n=p$ , I mentioned we have: $$R(1,p)=\frac12(n+1)+\delta$$ It seems that the following patterns hold: If $p$ is of form $4m+3$ , then $\delta=0$ . If $p$ is of form $8m+5$ , then $\delta=1$ . If $p$ is of form $56m+\{17,33,41\}$ , then $\delta=4$ . If $p$ is of form $1288m+X_4$ , then $\delta=6$ , where $X_4$ contains: {57, 65, 113, 137, 249, 281, 337, 401, 457, 505, 513, 569, 585, 617, 641, 681, 697, 753, 793, 849, 865, 953, 977, 1009, 1017, 1033, 1065, 1073, 1121, 1201, 1233, 1241, 1257} If $p$ is of form $\dots$ And so on. Can we find a closed-form for these patterns (congruence classes) that determine $\delta$ ? What is so special about $p$ modulo $3,8,56,1288\dots$ ? The next one seems to be $21896$ , and if that is true, then $X_5$ would contain: {17921, 11265, 14337, 15873, 7169, 7177, 17929, 21001, 8201, 14345, 13841, 2577, 20497, 9241, 16921, 21529, 9753, 20505, 18425, 16417, 19489, 15905, 16937, 5161, 14897, 2097, 11825, 7729, 4153, 19513, 14393, 1593, 21057, 10313, 16969, 17481, 6729, 3665, 16977, 20049, 7249, 12881, 12889, 20057, 10329, 19545, 9305, 2657, 19553, 8289, 1129, 15465, 9321, 14953, 4601, 5233, 8817, 7281, 1145, 11897, 14457, 15481, 9857, 11393, 13441, 3201, 20609, 7809, 2697, 18057, 20105, 14473, 12433, 5777, 16017, 13969, 5273, 20633, 6297, 673, 8353, 9377, 18593, 1185, 4257, 12961, 19625, 1705, 7849, 177, 17585, 6329, 4281, 193, 9921, 13505, 14529, 5825, 21697, 6857, 1737, 8905, 2249, 16585, 8913, 17105, 8401, 4313, 11481, 15577, 4825, 19681, 11489, 15073, 5345, 1761, 737, 10977, 233, 21737, 7401, 3305, 12009, 17649, 15089, 14065, 16633, 13049, 21753, 20729, 19713, 5377, 5889, 6913, 20225, 1289, 12041, 12553, 1297, 15633, 19217, 9489, 18705, 20241, 785, 3865, 14617, 18201, 7961, 3361, 10529, 12065, 21281, 3873, 809, 17193, 10537, 20777, 6449, 20785, 9017, 13113, 20801, 15681, 6465, 2377, 4425, 18761, 5961, 11769, 8017, 9041, 19793, 11089, 11601, 7001, 8537, 4953, 18265, 11617, 13665, 21345, 11113, 9577, 2417, 13169, 16241, 6521, 10105, 4993, 13185, 16769, 15745, 5513, 11145, 5009, 3985, 21393, 2969, 8089, 921, 7065, 11673, 18841, 18337, 14249, 1961, 6057, 10665, 21417, 16297, 5545, 3497, 12721, 10161, 1457, 20913, 8633, 8121, 12217, 16825, 1465, 449, 5569, 15297, 15817, 7113, 11209, 21449, 4041, 14793, 10193, 17873, 4057, 473, 11225, 17369, 16377, 2017, 6113, 9185, 17889, 10721, 3049, 16361, 12777, 13801, 6633, 15849, 13297, 19441, 4593, 15345, 11761, 20465, 9209} And in this case, the $\delta=7$ would occur. The $\delta$ values whose congruence classes are unknown are: $\delta\in\{8, 9, 13, 10, 14, 16, 15, 17,\dots\}$ These congruence classes mentioned so far, that should determine $\delta\in\{0,1,4,6,7\}$ for all primes, are based on data on first $2\cdot 10^4$ primes, so far.","['elementary-number-theory', 'recreational-mathematics', 'modular-arithmetic', 'sequences-and-series']"
3418983,"For dinner, $n$ people came and sat at a round table at random. If Ana, Ivan and Mark were among them, how many ways could they sit so ...","Problem: For dinner, $n$ ( $n \geq 4$ ) people came and sat at a round table at random. If Ana, Ivan and Mark were among them, how many ways could they sit so that Ana and Ivan do not sit next to each other and at least one of them sits next to Mark? (Note: the round table implies seating arrangements that differ only in rotation.) My attempt: If I have $n$ people sitting around circular table, the number of different arrangements are $(n-1)!$ . If I have $2$ people Mark and Ana number of arrangements that they can sit next to each other is $2 \cdot (n-2)!$ .
So the number of arrangements that Mark sit next to Ivan is also $2 \cdot (n-2)!$ , and sitting next to Ana also $2 \cdot (n-2)!$ . That all I know about this problem.","['permutations', 'combinatorics', 'discrete-mathematics']"
3418994,"Integrate $\left(\frac{\cos^3x\>+\>\sin^3x}{\cos^4x\>+\>\sin^4x}\right)^2$ over $[-\frac\pi4,\frac\pi4]$","I encountered the task of evaluate the definite integral, $$\int_{-\frac\pi4}^{\frac\pi4}\left(\frac{\cos^3x+\sin^3x}{\cos^4x+\sin^4x}\right)^2dx$$ which came about when I had answered a post as to whether there is an analytical solution to the area of an enclosed loop in polar space. After some laborious expansion and substitutions, I was happy to arrive at the algebraic expression $\frac{3\sqrt2}{8}\pi$ , yet unhappy about the lengthy computation spent. I wonder there may be some efficient procedure out there and would love to know.","['integration', 'definite-integrals']"
3418995,Reference request for the dimension of intersection of affine varieties,"Let $X,Y \subset \mathbb{C}^n$ be two irreducible affine algebraic sets with nonempty intersection. I am looking for a reference where I can find the proof that $$\dim (X \cap Y) \geq \dim X+ \dim Y - n.
$$ Any suggestion or a proof would be appreciated!","['affine-varieties', 'algebraic-geometry', 'reference-request']"
3418998,Optional stopping theorem for backwards martingales,"Is there a optional stopping theorem for backwards martingales  ? When $T$ is a non-positive and bounded stopping time then what kind of random variable is the following, $\{X_{-T\wedge n}\}_{n\le-1}\tag1$ Is there a typo ? It doesn't make sense to me because $n$ is negative and $-T$ becomes positive ? I should actually show if $\{X_k\}_k$ is a backward martingale with respect to backward filtration then so is $(1)$","['martingales', 'probability-theory']"
3419003,"For a poset with ground set P, isn't every set containing a single element of $X$ a chain of $P$?","For example, consider this partially ordered set from my textbook solely through its Hasse diagram: My textbook says that one of the chains of the poset $P$ above is ${1}$ . This makes sense. Explicitly using definitions of partial orders and their chains, here is the logic: Posets are transitive, therefore $1$ is related to itself through the partial order relation, therefore $1$ is comparable to itself, therefore every pair of distinct elements in $\{1\}$ are comparable to each other, therefore $\{1\}$ is a chain of $P$ But apparently $\{3\}$ is not a chain, but instead is an antichain of $P$ . Why is this? I could follow the same logic as I did for $\{1\}$ to conclude that $\{3\}$ should also be a chain...","['order-theory', 'discrete-mathematics']"
3419017,Does every sufficiently long string contain many repetitions of a string of bounded length?,"Let $S$ be a finite set and $d > 0$ . Does there exist $\ell > 0$ such that the following holds? Every sufficiently long string with letters in $S$ contains at least $d$ consecutive copies of some string of length at most $\ell$ . For example, when $S = \{0, 1\}$ and $d = 2$ , we can take $\ell = 2$ : every string of length at least $4$ contains one of $00, 11, 0101, 1010$ . (I wondered about this when dealing with certain nilpotent elements in a certain ring.)","['combinatorics-on-words', 'combinatorics']"
3419052,1-Player Fair Gambler's Ruin,"Consider a one-player Gambler's Ruin, where a gambler starts with capital $k \in \mathbb{Z}^+$ and play fair games (i.e. in each game, he wins \$ $1$ with probability $\frac{1}{2}$ , and loses \$ $1$ with probability $\frac{1}{2}$ ) until he goes broke. There is no upper limit to how much the gambler wants to win before he stops (i.e. he stops iff he runs out of money). I want to show that the gambler goes broke eventually almost surely. In my attempt, I let $S_n$ denote the total capital of the gambler after $n$ games, with $S_0 = k$ . It is well known that $S_n^2 - n$ forms a martingale. Let $N := \inf\{n \geq 1 : S_n = 0\}$ be a stopping time, which implies that $S_{N \wedge n}^2 - (N \wedge n)$ also forms a martingale. Therefore: $$
\mathrm{E}[S_{N \wedge n}^2 - (N \wedge n)] = \mathrm{E}[S_0^2] = k^2 \Rightarrow \mathrm{E}[N \wedge n] = \mathrm{E}[S_{N \wedge n}^2 ] - k^2
$$ I'm not entirely sure how to proceed from here. The hint provided for this question says that I should invoke Martingale Convergence Theorem, but I fail to see how it helps. (Note that $S_n$ itself also form a martingale) Any help would be appreciated. Side question: If $X_n$ is an increasing martingale w.r.t. $n$ , for a stopping time $N$ , can we conclude that $X_{N \wedge n} \leq X_n$ ?","['martingales', 'probability-theory']"
3419082,Prove $\lim_{x \to 2} \frac{ x(x^2-1) }{x+3}=6/5$.,"Prove by $\epsilon$ - $\delta$ definition that $\displaystyle\lim_{x \to 2}  \frac{ x(x^2-1) }{x+3}=\dfrac{6}{5}$ . I know this is simple but I am stuck, If we assume $\delta <1$ and $|x-2|< \delta$ we can get rid of the factor $x-2$ in the following expression $$\dfrac{ x(x^2-1) }{x+3}-\dfrac{6}5=\dfrac{(x-2)(5x^2+10x+9)}{5x+15}$$ but I don't know how to proceed.","['limits', 'calculus']"
3419096,Equivalent definitions of standard Borel space?,"In some books, like Srivastava (1998) A Course on Borel Sets, p. 96, a standard Borel space is defined as a measurable space which isomorphic to a Borel subset of a Polish space (i.e. there exists a bimeasurable bijection between the two). Elsewhere, a standard Borel space is defined as a measurable space, say $(X,\mathcal{X})$ , for which a Polish topology $\mathcal{T}$ exists which generates  the $\sigma$ -algebra $\mathcal{X}$ . Are the two definitions equivalent?","['measure-theory', 'descriptive-set-theory', 'borel-measures', 'borel-sets', 'general-topology']"
3419099,Special representation of real numbers as infinite products,"Final update on 11/29/2019: I have worked on this a bit more, and wrote an article summarizing all the main findings. You can read it here . I am looking for a reference regarding the following. Every real number $x\in [1, 2]$ is uniquely representable as a product of distinct factors of the form $1+2^{-k}$ , that is $$x=\prod_{k=1}^\infty \Big(1+\frac{b_k}{2^k} \Big) \mbox{ with } b_k\in \{0, 1\}.$$ The algorithm to find the $b_k$ 's is a simple version of the greedy algorithm. I could not find any reference to this fact, except a small note in the Wikipedia entry for logarithms. It is said to be related to Feynman's algorithm, see here . If all $b_k$ 's are equal to one, then $x=2.384231029... = \frac{1}{1-z}$ where $z$ is the Pell constant (see here , here and here .) The reason for my interest is because I am studying numeration systems (see my numerous questions on Math.StackExchange and Stats.StackExchange, most recently this one ), and in particular, numeration systems where the digits are random numbers. In this case, it means I am interested in the case where the $b_k$ 's are independent Bernouilli random variables of parameter $p_k$ (for instance $p_k=\frac{1}{2}$ or $p_k = 2^{-k}$ .) Update It turns out that this system is ill-conditioned. A number can actually have two different representations, for instance $$ \frac{3}{2} = 1 + \frac{1}{2^1} = \Big(1+\frac{1}{2^2}\Big)\Big(1+\frac{1}{2^3}\Big)\Big(1+\frac{1}{2^4}\Big)\Big(1+\frac{1}{2^8}\Big)\Big(1+\frac{1}{2^{16}}\Big)\Big(1+\frac{1}{2^{32}}\Big)\cdots$$ More precisely, $$\frac{3}{2} = 1 + \frac{1}{2^1} =\Big(1+\frac{1}{2^3}\Big).\prod_{k=1}^\infty\Big(1+\frac{1}{2^{2^k}} \Big).$$ If you use the greedy algorithm to find the $b_k$ 's, it will lead to the first (leftmost) representation: $\frac{3}{2}= 1+\frac{1}{2^1}$ . Yet if $b_k$ has a Bernouilli distribution of parameter $\frac{1}{2}$ (that is 50% of zeros, 50% of ones), then $x$ also has some statistical distribution, pictured below (the chart below shows the percentile distribution): The distribution (CDF) in question is very well approximated by $F(x) = \log_{\lambda} x$ , with $x \in [1,  \lambda]$ . Here $\lambda =2.384231029...$ is the constant discussed earlier. See below the chart picturing $F(x)$ in blue, and its logarithmic approximation in red. The CDF is the inverse of the percentile distribution. Now this is becoming very interesting: compare this chart with the one obtained for continued radicals in section 2.2 in this article . How similar! I plan on sharing my spreadsheet with the computations required to produce the percentile and PDF distribution. The chart below shows the approximation error $\log_\lambda x - F(x)$ between the logarithmic approximation and the exact CDF, with $x\in [1, \lambda]$ . If $b_k$ has a Bernoulli distribution of parameter $p=\frac{1}{6}$ (rather than $p=\frac{1}{2}$ ), then $x$ has a more chaotic distribution, see percentile distribution below: To the contrary, if $b_k$ is uniform on $\{0, 1, 2, 3\}$ then the distribution is much smoother, see percentile distribution below:","['number-theory', 'binomial-distribution', 'decimal-expansion', 'sequences-and-series', 'probability-theory']"
3419123,Find perfect finite group whose quotient by center equals the same quotient for two other groups and has both as a quotient,"Let $G_1$ and $G_2$ be two finite perfect groups such that $G_1 / Z(G_1) \cong G_2 / Z(G_2)$ . Then there exists a finite perfect group $G$ and subgroups $Z_1, Z_2 \le Z(G)$ with $$
 G / Z(G) \cong G_i / Z(G_i) \quad
 \mbox{and} \quad
 G / Z_i \cong G_i, \quad i = 1,2.
$$ I want to construct the group $G$ . I tried to start from the direct product $G_1 \times G_2$ and factor out an appropriate normal subgroup, but everything I try does not give the desired properties. And also I have no idea how to incorporate the assumption that $G_1$ and $G_2$ are perfect. So any hints on this exercise? This exercise is taken from the book Theory of finite groups by H. Kurzweil and B. Stellmacher, and appears there on page 36.","['group-theory', 'abstract-algebra', 'finite-groups']"
3419135,Show that $h(x)$ has a minimum at $0$,Let $h:\mathbb R^n\rightarrow\mathbb R$ be a function that satisfies $x\cdot\nabla h(x)\geq0~~\forall~x\in\mathbb R^n $ . How can I show that h has a minimum at zero? I know that at a stationary point $\nabla h(x)=0$ .,"['multivariable-calculus', 'derivatives']"
3419216,Equality in Lebesgue measure theory,"I have found some notes about measure theory where it is used the following equality $$\int_A f \, dx=\int_0^\infty \mu_A(\lambda)\,d\lambda,$$ where $f$ is a positive Lebesgue integrable function and $\mu_A(\lambda)$ is the Lebesgue measure of the set $\{x\in A:f(x)>\lambda\}$ . Could somebody justify that equality?","['integration', 'measure-theory', 'lebesgue-measure', 'lebesgue-integral', 'real-analysis']"
3419249,Transformation of vertical line under $f(z)=\tan(z)$,"I've managed to prove that $$\tan(z)=\tan(x+iy)=\frac{\sin 2x}{\cos 2x+\cosh 2y}+i\,\frac{\sinh 2y}{\cos 2x+\cosh 2y}$$ Now, I've also been able to use this identity to show that the vertical line $x=\pi/4$ gets mapped onto a portion of a circle. However, I am wondering if there is a way to use the tangent identity above to show that if $x=a$ is a vertical line, where $-\pi/4<a<\pi/4$ , then $\tan(z)$ maps this line onto a portion of a circle. Update Thanks to Maxim's comment below, solving the first of the two equations below $$u=\frac{\sin 2a}{\cos 2a+\cosh 2y}\qquad\text{and}\qquad v=\frac{\sinh 2y}{\cos 2a+\cosh 2y}$$ gave: $$\cosh 2y=\frac{\sin 2a-u\cos 2a}{u}$$ Substituting this into the second equation and solving, gave: $$\sinh 2y=\frac{v\sin 2a}{u}$$ Substituting each of these into $\cosh^22y-\sinh^22y=1$ and expanding and manipulating and completing the square led to the result $$\left(u+\frac{\cos 2a}{\sin 2a}\right)^2+v^2=1+\frac{\cos^22a}{\sin^22a}$$ Substituting $a=\pi/8$ gives the equation $(u+1)^2+v^2=2$ . Here is the Matlab code and image, which shows that his suggestion worked. figure('Units','Normalized','Position',[0.1,0.1,0.8,0.5])
y=linspace(-5,5); x=pi/8*ones(size(y));
subplot(1,2,1), plot(x,y,'LineWidth',2,'Color','r'), hold on
grid on, axis([-3,3,-3,3]), xticks(-3*pi/4:pi/4:3*pi/4)
xticklabels({'-3\pi/4','-\pi/2','-\pi/4','0','pi/4','\pi/2','3\pi/4'})
xlabel('x'),ylabel('y'), ax=gca; ax.FontSize=14;
ax=gca; ax.XAxisLocation='origin'; ax.YAxisLocation='origin';
f=@(z) tan(z);
z=complex(x,y); w=f(z); 
subplot(1,2,2)
plot(w,'LineWidth',2,'Color','b'), hold on
g=@(u,v) (u+1).^2+v.^2-2;
fimplicit(g)
grid on, axis([-3,3,-3,3]), 
xlabel('u'),ylabel('v'), ax=gca; ax.FontSize=14;
ax.XAxisLocation='origin'; ax.YAxisLocation='origin'; Note that the blue part in the graph on the right is image of the vertical line under the transformation $f(z)=\tan(z)$ . It is only a portion of the full circle, but exactly what I wanted.",['complex-analysis']
3419312,Durrett Exercise 2.4.3 or 3.4.3 --- Symmetry of $Y_{k}\mathbb{1}_{|Y_{k}|\leq m}$ and $Y_{k}\mathbb{1}_{|Y_{k}|>m}$,"This is exercise from Durrett 2.4.3 or 3.4.3 depending on the version of the book. Part of this exercise asks to prove that For $m>0$ , let $U_{k}:=Y_{k}\mathbb{1}_{|Y_{k}|\leq m}$ and $V_{k}=Y_{k}\mathbb{1}_{|Y_{k}|>m}$ . Show that for every $u<\infty$ and all $n$ , we have $$\mathbb{P}\Big(\sum_{k=1}^{n}Y_{k}\geq u\sqrt{n}\Big)\geq\mathbb{P}\Big(\sum_{k=1}^{n}U_{k}\geq u\sqrt{n}, \sum_{k=1}^{n}V_{k}\geq 0\Big)\geq\dfrac{1}{2}\mathbb{P}\Big(\sum_{k=1}^{n}U_{k}\geq u\sqrt{n}\Big).$$ I have showed the first inequality using the fact that $Y_{k}=U_{k}+V_{k}$ , but I don't know how to show the second one. There have been a solution of this exercise but only said the second one is using symmetry. The same question was asked and answered here but I don't think the argument is correct (or precise) for the symmetric part. Does the convergence of $S_n / \sqrt{n}$ in distribution imply $EX_i = 0$? Could somebody explain to me in detail what symmetric we are using here?","['proof-explanation', 'probability-distributions', 'probability-theory', 'central-limit-theorem']"
3419358,Does this statement about natural logs of functions make sense?,"For two functions $a(x)$ and $b(x)$ , since $\ln(a/b) = \ln(a) - \ln(b)$ , does $$\frac{d}{dx} \ln(a/b) = \frac{d}{dx} \ln(a) - \frac{d}{dx} \ln(b) = \frac{1}{a} \cdot \frac{da}{dx} - \frac{1}{b} \cdot  \frac{db}{dx}?$$",['derivatives']
3419377,When am I allowed to integrate by parts?,"My book, on general integration, builds a lot of theory around e.g. the Fubini-Tonelli theorem and when it can/can't be applied, but does not do the same for integration by parts. Perhaps it's just always allowed, but it does not say that explicitly either. In particular, when evaluating ""real integrals"", my professor and others on the internet seem to just use it without justification. So my question is: What are the necessary and sufficient conditions for using integration by parts? Searching the internet does not particularly help, because the wording of the question is similar to that of ""when is it helpful to integrate by parts"", as opposed to ""when am I allowed to integrate by parts"".","['integration', 'measure-theory', 'real-analysis']"
3419462,Show that $\sum_{k=0}^{\infty} p^k\cos(kx) = \frac{1-p\cos(x)}{1-2p\cos(x)+p^2}$ using complex numbers.,"I have this equality that I'm trying to show. I have tried many times but I can't get it to work. Could you please help me? I would like to first make this finite sum equal something. $$\sum_{k=0}^np^k\cos(kx) = S_{n}(p, x), -1 < p < 1 $$ And then use it to calculate the infinite sum, which should be the right side. $$\lim_{n \to {\infty}} S_n(p, x)=\sum_{k=0}^{\infty}p^k\cos(kx) = \frac{1-p\cos(x)}{1-2p\cos(x)+p^2}$$ I would like to use complex numbers to show this. EDIT: I know that I have to use Euler's identity, I just can't get the algebra to work. I have gotten so far but I don't know how to continue. $$\frac{(pe^{ix})^{(n+1)/2}((pe^{ix})^{-(n+1)/2}-(pe^{ix})^{(n+1)/2})}{(pe^{ix})^{1/2}((pe^{ix})^{-1/2}-(pe^{ix})^{1/2})}$$ Thank you in advance!","['trigonometric-series', 'trigonometry', 'complex-numbers', 'sequences-and-series']"
3419488,"If $a = b$, then $f(a) = f(b)$ does $f(x)$ need to be a one-to-one function or can $f(x)$ be any type of function?","It shouldn't matter whether or not $f(x)$ is one-to-one because $f(x)$ is being evaluated at the same argument/input values if $a=b$ , then $f(a)=f(b)$ , right?","['algebra-precalculus', 'functions']"
3419509,Reference for the Gibbs variational principle/dual characterization of KL,"Let $P,Q$ be two probability distributions. Then, one has the following dual characterization of their Kullback-Leibler divergence (relative entropy): $$
D(P \,\|\,Q) = \sup_f ( \mathbb{E}_P[f(X)] - \log \mathbb{E}_Q[e^{f(X)}] ) \tag{1}
$$ This characterization is sometimes referred to as Gibbs variational principle , or Donsker-Varadhan formula; however, I couldn't track where it was first proven (there is often a reference to a paper of and Donsker Varadhan from 1983, but I couldn't find, in there, where (1) is actually established). Where was (1) established first, specifically? What to cite?","['math-history', 'probability', 'reference-request']"
3419541,"x independent of y, y independent of z, then is x independent of z?","using $\perp $ to indicate independence: If $x \perp y$ and $y \perp z$ then is $x \perp z$ ? I started with: $$p(x,z)=p(x,y,z)+p(x,\neg y,z)$$ $$=p(x)p(y|x)p(z|x,y)+p(x)p(\neg y|x)p(z|x,\neg y)$$ $$=p(x)p(y)\frac{p(x,z|y)}{p(x|y)}+p(x)p(\neg y)\frac{p(x,z|\neg y)}{p(x|\neg y)}$$ $$=p(x)p(y)\frac{p(x,z|y)}{p(x)}+p(x)p(\neg y)\frac{p(x,z|\neg y)}{p(x)}$$ $$=p(y)p(x,z|y)+p(\neg y)p(x,z|\neg y)$$ but now i am unsure of how to work with $p(x,z|y)$ .. if $x\perp y$ & $y \perp z$ then can you say $p(x,z|y)=p(x|y)p(z|y)=p(x)p(z)$ ? thank you",['probability']
3419594,How to prove this beautiful series by using Taylor and Maclaurin series,"I have been playing with Taylor and Maclaurin series lately and stumble on this beautiful identity. I don't know to expand the left hand side to yield the right hand side:
How to prove: $\dfrac{1}{\sqrt{1-x^2}} =1+\dfrac{1}{2}x^2+\dfrac{1 \cdot3}{2\cdot4}x^4+\dfrac{1\cdot 3\cdot 5}{2\cdot 4\cdot 6}x^6+\dfrac{1\cdot 3\cdot 5\cdot 7}{2\cdot 4\cdot 6\cdot 8}x^8...$ I can only expand this as followed: $$\frac{1}{\sqrt{1-x^2}} = 1+\frac{1}{2}x^2+\frac{3}{8}x^4+\frac{5}{16}x^6...,$$ How can you prove this by using Maclaurin series? 
I need two proofs, one in Maclaurin series and one in binomial theorem. Please don't use the sigma notation too much as I cannot see the pattern.","['power-series', 'calculus', 'sequences-and-series', 'real-analysis']"
3419612,Topological Obstructions to the Existence of Kähler--Einstein metrics on Fano Manifolds,"Let $(X, \omega)$ be a compact Kähler manifold. The cohomology class represented by $$\text{Ric}(\omega) = \frac{1}{2\pi} \text{Ric}_{i \overline{j}} dz^i \wedge d\overline{z}^j$$ is called the first Chern class of $X$ , denoted $c_1(X) : = c_1(-K_X)$ , where $-K_X$ is the anti-canonical bundle of $X$ . We say that $(X, \omega)$ is Fano if $c_1(X)$ is positive, i.e., the representative: $\text{Ric}_{i\overline{j}}$ is a positive-definite matrix. The existence of Kähler--Einstein metrics on Fano manifolds is known to be equivalent to an algebro-geometric notion of stability by the work of Tian and Chen--Donaldson--Sun. Moreover, there are obstructions given by Matsushima and Futaki relating to the Lie algebra of holomorphic vector fields. Question: Are there topological obstructions to the existence of Kähler--Einstein metrics on Fano manifolds?","['kahler-manifolds', 'complex-geometry', 'algebraic-geometry', 'riemannian-geometry']"
3419657,Difference between topologically complete space and complete metric space,"Definition: Topological space $(X,\tau)$ is called topologically complete if there is metric $d$ on $X$ which induces the topology $\tau$ of $X$ and $(X,d)$ is complete metric space. Also the following fact is true: If $f:X\to Y$ where $f$ is homeomorphism and $Y$ is topologically complete then $X$ is also topologically complete. The proof is not difficult because if $d$ is a metric on $Y$ which induces topology of $Y$ and $(Y,d)$ complete metric space then one can define the metric $\rho$ on $X$ as follows: $\rho(x_1,x_2):=d(f(x_1),f(x_2))$ . One can show that $\rho$ induces the topology of $X$ and $(X,\rho)$ is complete metric space. However, I was wondering about the following moment: the above reasoning shows that the notion of topologically complete is topological property. However completeness is not topological property. The standard example is $(0,1)$ and $\mathbb{R}$ , they are homeomorphic, $\mathbb{R}$ is complete but $(0,1)$ is not since the sequence $x_n=1-\frac{1}{n}$ is Cauchy sequence but does not converge in $(0,1)$ . Can anyone explain to me why the above reasoning cannot be applied to the case of $\mathbb{R}$ and $(0,1)$ ? I guess that $(0,1)$ is not complete in the standard euclidean metric inherited from $\mathbb{R}$ but it maybe complete in the different metric which induces its subspace topology. Anyway I would be very grateful for useful answer!","['general-topology', 'metric-spaces']"
3419712,Prove that continuous partial derivatives imply continuous total derivative,"Good morning, I'm trying to prove that Suppose $X$ is open in $\mathbb{R}^{n}$ and $F$ is a Banach space. Then $f: X \rightarrow F$ is continuously differentiable if $f$ has continuous partial derivatives. Could you please verify whether my attempt is fine or contains logical gaps/errors? Any suggestion is greatly appreciated! My attempt: For $a \in X$ , we define $A \in \mathcal L(\mathbb R^n,F)$ by $$h=\left(h_{1}, \ldots, h_{n}\right) \mapsto A h=\sum_{k=1}^{n} \partial_{k} f(a) h_{k}$$ Our goal is to show that $\partial f(a) = A$ or equivalently $$\lim _{h \rightarrow 0} \frac{f(a+h)-f(a)-A h}{|h|_\infty}=0$$ First, we choose $\varepsilon>0$ such that $\mathbb{B}(a, \varepsilon) \subseteq X$ and let $x_k = a+ (h_1,\ldots,h_k,0,\ldots,0)$ for all $k = \overline{1,n}$ . It follows that $$f(a+h)-f(a)=\sum_{k=1}^{n}\left(f\left(x_{k}\right)-f\left(x_{k-1}\right)\right)$$ Let $\{e_1,\ldots,e_n\}$ be the standard basis of $\mathbb R^n$ . By definition, we have $$\begin{aligned} 
\partial_{k} f\left(x_{k-1}+t h_{k} e_{k}\right) &=  \lim_{z \to 0} \frac{f\left(x_{k-1}+t h_{k} e_{k} + ze_k\right) - f\left(x_{k-1}+t h_{k} e_{k}\right)}{z} \\ 
 &=  \lim_{z \to 0} \frac{f\left(x_{k-1}+ (th_{k}+z)  e_{k}\right) - f\left(x_{k-1}+t h_{k} e_{k}\right)}{z}\\
 &=  \frac{\partial f\left(x_{k-1}+t h_{k} e_{k}\right)}{\partial (th_k)}   
\end{aligned}$$ By Fundamental Theorem of Calculus, we have $$\begin{aligned} 
h_k\int_0^1 \partial_{k} f\left(x_{k-1}+t h_{k} e_{k}\right) dt &= \int_0^1 \partial_{k} f\left(x_{k-1}+t h^{k} e_{k}\right) d(th_k)\\ &=  \partial_{k} f\left(x_{k-1}+t h_{k} e_{k}\right) \Big|_0^1 \\
&= f\left(x_{k}\right)-f\left(x_{k-1}\right)
\end{aligned}$$ As such, $$f(a+h)-f(a)=\sum_{k=1}^{n} h_{k} \int_{0}^{1} \partial_{k} f\left(x_{k-1}+t h_{k} e_{k}\right) d t$$ Consequently, $$\begin{aligned} \|f(a+h)-f(a) - Ah \| &=\left \|\sum_{k=1}^{n} h_{k} \int_{0}^{1} \left(\partial_{k} f\left(x_{k-1}+t h_{k} e_{k}\right) -\partial_{k} f(a)\right) d t \right \| \\ &\le \sum_{k=1}^{n} |h_{k}|  \int_{0}^{1} \left \| \partial_{k} f\left(x_{k-1}+t h_{k} e_{k}\right) -\partial_{k} f(a)\right \| d t \\ &\le |h|_\infty \sum_{k=1}^{n} \int_{0}^{1} \left \| \partial_{k} f\left(x_{k-1}+t h_{k} e_{k}\right) -\partial_{k} f(a)\right \| d t \\ &\le |h|_\infty \sum_{k=1}^{n}   \int_{0}^{1} \sup_{t \in [0,1]} \left \| \partial_{k} f\left(x_{k-1}+t h_{k} e_{k}\right) -\partial_{k} f(a) \right \| d t \\ &\le |h|_\infty \sum_{k=1}^{n}   \int_{0}^{1}  \sup_{x \in \mathbb{B}(a, \|h\|_\infty)} \left \| \partial_{k} f\left(x\right) -\partial_{k} f(a)\right \| d t \\&= |h|_\infty \sum_{k=1}^{n}  \sup_{x \in \mathbb{B}(a, \|h\|_\infty)} \left \| \partial_{k} f\left(x\right) -\partial_{k} f(a)\right \|\end{aligned}$$ We have $h \to 0$ implies $\|h\|_\infty \to 0$ , which in turn implies $x \to a$ . It follows from the continuity of $\partial_{k} f\left(x\right)$ that $ \sup_{x \in \mathbb{B}(a, \|h\|_\infty)} \left \| \partial_{k} f\left(x\right) -\partial_{k} f(a)\right \| \to 0$ as $x \to a$ . Finally, $$\lim _{h \rightarrow 0} \frac{\| f(a+h)-f(a)-A h \|}{|h|_\infty} \le \lim _{h \rightarrow 0} \sum_{k=1}^{n}  \sup_{x \in \mathbb{B}(a, \|h\|_\infty)} \left \| \partial_{k} f\left(x\right) -\partial_{k} f(a)\right \| = 0$$ Consequently, $$\lim _{h \rightarrow 0} \frac{f(a+h)-f(a)-A h}{|h|_\infty}=0$$ Hence $\partial f(a) = A$ . Next we prove that $\partial f(\cdot): X \to \mathcal L(\mathbb R^n,F)$ is continuous. We have $$\begin{aligned}\|\partial f(x)h - \partial f(a)h\| &= \left\| \sum_{k=1}^{n} \partial_{k} f\left(x\right) h_{k} - \sum_{k=1}^{n} \partial_{k} f\left(a\right) h_{k} \right\| \\ &= \left\| h_k \sum_{k=1}^{n} ( \partial_{k} f\left(x\right) - \partial_{k} f\left(a\right)) \right\| \\&\le  \sum_{k=1}^{n} \left\|\partial_{k} f\left(x\right) - \partial_{k} f\left(a\right) \right\| \cdot |h|_\infty \end{aligned}$$ Consequently, $$\|\partial f(x) - \partial f(y)\| = \sup_{h \in X} \frac{\|\partial f(x)h - \partial f(a)h\|}{|h|_\infty} \le \sum_{k=1}^{n} \left\|\partial_{k} f\left(x\right) - \partial_{k} f\left(a\right) \right\|$$ It follows from the continuity of $\partial_{k} f\left(\cdot\right)$ that $\sum_{k=1}^{n} \left\|\partial_{k} f\left(x\right) - \partial_{k} f\left(a\right) \right\| \to 0$ and thus $\|\partial f(x) - \partial f(y)\| \to 0$ as $x \to a$ . Hence $\partial f(x) \to \partial f(y)$ .","['partial-derivative', 'proof-verification', 'derivatives', 'real-analysis']"
3419715,Weird mistake that I cannot spot in a proof,"I was doing this exercise: Suppose that $(X,\mathcal{S},\mu )$ is a measure space, $1<p<\infty $ and $f,g\in \mathcal{L}^p(\mu )$ . Prove that Minkowski's s inequality is an equality if and only if there exists non-negative numebrs $a$ and $b$ , not both zero, such that $af(x)=bg(x)$ almost everywhere. My wrong proof below: We want to show that if $\left(\int |f+g|^p\right)^{1/p}=\left(\int |f|^p\right)^{1/p}+\left(\int |g|^p\right)^{1/p}$ then there are some $a,b\geqslant 0$ , not both zero, such that $af(x)=bg(x)$ a.e. WLOG we can assume that $\|f+g\|_p=1$ and $f,g\neq 0$ a.e., therefore $\|f\|_p,\|g\|_p\in(0,1)$ and so there is some constant $b>0$ such that $b\|f\|_p=\|g\|_p$ , but this means that $$
\int b^p|f|^p \,\mathrm d  \mu=\int |g|^p \,\mathrm d \mu \implies |bf|=|g|\text{ a.e. }\tag1
$$ Then there is some measurable function $h:X\to \Bbb F$ such that $|h(x)|=1$ a.e. and $b fh=g$ , thus we want to show that $h=1$ a.e. Now note that $$
\|f+g\|_p=\|f(1+hb)\|_p\quad \,\land\,\quad  \|f\|_p+\|g\|_p=(1+b)\|f\|_p\\
\therefore\, \|f(1+bh)\|_p=\|f(1+b)\|_p\implies |1+h(x)b|=1+b \,\text{ a.e. }\tag2
$$ Because $b>0$ and $|h(x)|=1$ a.e. then is easy to conclude that $h=1$ a.e., finishing one direction of the proof. The other direction is easy to see due to the homogeneity of $\|{\cdot}\|_p$ , so we are done. $\Box$ However above I didnt used the fact that $p\in (1,\infty )$ , indeed the fake proof above seems to hold for any chosen $p>0$ . However it doesn't hold for $p=1$ because if $f$ and $g$ are non-negative then is easy to check that $\|f+g\|_1=\|f\|_1+\|g\|_1$ , but in general $f$ and $g$ doesn't need to be proportional a.e. And probably the statement doesn't hold either when $p\in (0,1)$ . Where is my mistake in the above proof? EDIT: I see my mistake... It is the assertion that $|bf|=|g|$ a.e. I forget that $\int h\,\mathrm d \mu =0\Rightarrow h=0$ a.e. just holds when $h=|h|$ .","['fake-proofs', 'lp-spaces', 'lebesgue-integral', 'analysis']"
3419796,$X_n$ is bounded in probability and $Y_n$ converges to 0 in probability then $X_nY_n$ congerges to probablity with 0,"I want to show : $X_n$ is bounded in probability and $Y_n \rightarrow 0$ in probability  then $X_nY_n \rightarrow 0 $ in probablity. 
I know the following definitions that is Definition 2.17 : We say that $X_n$ is bounded in probability if $X_n = O_P (1)$ ,
i.e. if for every $\epsilon$ > 0, there exist $M$ and $N$ such that $P(|Xn| < M) >
1 − \epsilon $ for $n > N$ . So I want to show that for every $\epsilon$ > 0 and $\epsilon '$ > 0 there exist $M>0 $ and $n_o $ such that $P(|X_n| <M) > 1 -\epsilon/2$ $P(|X_n| <\epsilon /M) > 1 -\epsilon'/2$ for every $n>n_0$ Then I want to show that $P(|X_n| <M$ and $|Y_n| <\epsilon /M) > 1-\epsilon' $","['statistics', 'probability-theory', 'asymptotics']"
3419807,Application of Third Isomorphism Theorem: If $G'/G''$ and $G''/G'''$ are cyclic then $G''=G'''$,"Dummit and Foote, 6.1.18 Suppose both $G'/G''$ and $G''/G'''$ are cyclic, then $G''=G'''$ (you may assume $G'''=1$ ). Where $G' = [G,G], G'' = [G',G'],$ etc. Question The assumption that $G'''=1$ apparently follows from the third isomorphism theorem , but I don't see how. I know $G', G''$ , and $G'''$ are all characteristic (and hence normal) in $G$ . Then the third isomorphism theorem says $$ G''/G''' \triangleleft G/G'''  $$ and $$ (G/G''') \; \big{/} (G'/G''') \cong G/G' $$ I don't see why that this implies $G''' = \{ e\}$ ?","['proof-explanation', 'group-theory', 'abstract-algebra', 'group-isomorphism']"
3419834,Integrals related to the reciprocal beta function,"It is known that $$\int_0^\frac{\pi}{2}\cos^{a-1}x\cos bx~dx=\frac{\pi}{2^aa\mathrm{B}\left(\dfrac{a+b+1}{2},\dfrac{a-b+1}{2}\right)}$$ as https://dlmf.nist.gov/5.12 stated. How about $\int_0^\frac{\pi}{2}\cos^{a-1}x\sin bx~dx$ , $\int_0^\frac{\pi}{2}\sin^{a-1}x\sin bx~dx$ , $\int_0^\frac{\pi}{2}\sin^{a-1}x\cos bx~dx$ ? Or how about $\int_0^\pi\cos^{a-1}x\cos bx~dx$ , $\int_0^\pi\cos^{a-1}x\sin bx~dx$ , $\int_0^\pi\sin^{a-1}x\sin bx~dx$ , $\int_0^\pi\sin^{a-1}x\cos bx~dx$ ? I notice someone can evaluate $\int_0^\pi\sin^\alpha t\cos kt~dt$ for integer $k$ : How to do $\int_{0}^{\pi}(\sin t)^{\alpha}\cos (k t)dt$ please?","['integration', 'definite-integrals', 'special-functions', 'beta-function', 'hypergeometric-function']"
3419870,A problem in open mapping theoram from Kreyszig Section 4.12 Problem 6,I am studying Functional analysis from Kreyszig book and can somebody please help with this problem Problem: Let $X$ and $Y$ be Banach Spaces and $T : X \to Y$ be an injective bounded linear operator. Show that $ T^{-1} : \mathscr R(T) \to X$ is bounded iff $\mathscr R(T)$ is closed in $Y$ . Can somebody please give hints on how to proceed through this question.,['functional-analysis']
3419894,Almost sure convergence of $\frac1{\sqrt n}\max_{1\le i\le n}|X_i|$,"Let $X_1,X_2,\ldots$ be i.i.d random variables. Assuming $EX_1^2 < \infty$ , show that $$\frac{\max_{1 \leq i \leq n} |X_i|}{\sqrt n}\stackrel{\text{a.s.}}\longrightarrow 0.$$ Let $M_n = \max_{1 \leq i \leq n} |x_i|$ . I tried different ways, but none seemed to work. I can show that $\frac{\max_{1 \leq i \leq n} |x_i|}{\sqrt n}$ converges in probability to 0. And tried to say since it is a monotone sequence, it also converges almost surely. But I get stuck in showing the monotonicity (only $M_n \leq M_{n+1}$ , not really $\frac{M_n}{\sqrt n} \leq \frac{M_{n+1}}{\sqrt n}$ ). Then, I tried to use the Borel-Cantelli lemma to show i). $$\sum_{i = 1}^\infty P\left(\frac{M_n}{\sqrt n}>\epsilon\right) < \infty$$ or ii). $$\sum_{i = 1}^\infty P\left(\frac{M_n}{\sqrt n} \leq \epsilon\right) = \infty$$ I have $$P\left(\frac{M_n}{\sqrt n} > \epsilon\right) \leq nP(X_i > \epsilon \sqrt n) \leq n \frac{E(X^2)}{n \epsilon^2} = \frac{E(X^2)}{\epsilon^2}$$ But the sum of this series then does seem to converge. Then, I tried to see $$P\left(\frac{M_n}{\sqrt n} \leq \epsilon\right) = 1 - P\left(\frac{M_n}{\sqrt n} \geq \epsilon\right) = 1 - \left(P\left(\frac{X_i}{\sqrt n} \geq \epsilon\right)\right)^n \geq 1 - \left(\frac{E(X^2)}{n \epsilon^2}\right)^n$$ $$\sum_{n = 1}^{\infty } P\left(\frac{M_n}{\sqrt n} \leq \epsilon\right) \geq \sum_{n = 1}^{\infty } 1- \left(\frac{E(X^2)}{n \epsilon^2}\right)^n = \infty,  n \rightarrow \infty$$ Is this last approach enough to conclude that $\frac{\max_{1 \leq i \leq n} |x_i|}{\sqrt n}$ converges almost surely to 0??? I feel it seems I am missing something.","['convergence-divergence', 'probability-theory', 'random-variables']"
3419921,What is the measure of $\int_{A}^B a_{\frac{x-A}{dx} } f(x) dx$?,"Mathematicians like to ask covering various sets with open intervals
  and the answers to these riddles have strange tendencies to become
  strange lemma's or theorems Heine-Borel Theorem, lebesgue's Number
  Lemma, Vitali Covering Lemma, Besicovitch Covering Theorem, (etc) - 3blue1brown By open interval he means the stretch of real numbers: $a<x<b$ . My challenge is to strictly weighted-ly cover all the rational numbers between $A$ and $B$ with open intervals. By weighted-ly cover means each paticular rational number lies in a particular interval. However, each interval has a weight $a_i$ . The challenge here is that weighted sum must be less than $k'$ . Before we proceed let's return to normal measure theory where $a_i=1$ for all weights. Then we have $k' = \int_{A}^B dx= B-A$ . Now, let us look at limit of a sum as Riemann integral: $$ {\displaystyle \Delta x={\frac {B-A}{n}}}$$ $$  A + r \Delta x \to x$$ OR $$ r \to \frac{x -A}{\Delta x}$$ Note: $r$ is a dummy variable. Now, in the limit $\Delta x \to 0$ $$ \int_A^B f(x) dx = \lim_{\Delta x \to 0}\Delta x\left[f(a+\Delta x)+f(a+2\,\Delta x)+\cdots +f(b-\Delta x)\right]= \lim_{\Delta x \to 0}\sum_{r=1}^n f(a+ r \Delta x) \Delta x$$ So what will $k$ be in the case where all $a_i$ aren't the same? Let's put this question in light of the answer : Claim: If $\lim_{n \to \infty} \frac{\log^2(n)}{n}\sum_{r=1}^n |b_r| = 0$ and $f$ is smooth, then $$\lim_{k \to \infty} \lim_{n \to \infty} \sum_{r=1}^n a_rf\left(\frac{kr}{n}\right)\frac{k}{n} = \left(\lim_{s \to 1} \frac{1}{\zeta(s)}\sum_{r=1}^\infty \frac{a_s}{r^s}\right)\int_0^\infty f(x)dx.$$ where $a_r = \sum_{e|r} b_e$ All we now do is add the notation: $$ \int_{A}^B a_{\frac{x-A}{dx} } f(x) dx= \lim_{k \to B-A} \lim_{n \to \infty} \sum_{r=1}^n a_rf\left(A+\frac{kr}{n}\right)\frac{k}{n}$$ In fact using the co-ordinate transformation $dy = f(x) dx$ with a function. We also define a coordinate transformation or mapping $ g(x) = y$ . Hence, $$  r \to \frac{x-A}{dy} \frac{dy}{dx}= f(x) \frac{x-A}{dy} = f(g^{-1} (y))  \Big( \frac{g^{-1} (y)- A}{dy} \Big) $$ This enables us to talk about coordinate transformations. Question What is the measure of $\int_{A}^B a_{\frac{x-A}{dx} } f(x) dx$ ? If $$\lim_{n \to \infty} \frac{\log^2(n)}{n}\sum_{r=1}^n |b_r| = 0$$ and $$a_r = \sum_{e|r} b_e$$","['integration', 'number-theory', 'measure-theory', 'coordinate-systems']"
3419940,Solve an ODE containing a 3rd degree polynomial,"I need to solve the differential equation $$\frac{dy(t)}{dt}=C_0+C_1y+C_2y^2+C_3y^3$$ where $C_i$ are constants. My attempt: By separation of variables I have $$\int_{a}^{y}\frac{dy}{\frac{C_0}{C_3}+\frac{C_1}{C_3}y+\frac{C_2}{C_3}y^2+y^3}=C_3\int_{0}^{t}dt$$ If $r_1$ , $r_2$ , and $r_3$ denote the three roots of the polynomial in the denominator of the above relation, I obtain $$\int_{a}^{y}\frac{A_1dy}{y-r_1}+\int_{a}^{y}\frac{A_2dy}{y-r_2}+\int_{a}^{y}\frac{A_3dy}{y-r_3}=C_3t$$ from which it results $$A_1\ln(\frac{y-r_1}{a-r_1})+A_2\ln(\frac{y-r_2}{a-r_2})+A_3\ln(\frac{y-r_3}{a-r_3})=C_3t$$ Using properties of $\ln$ function, this simplifies to $$\ln\left[(\frac{y-r_1}{a-r_1})^{A_1}(\frac{y-r_2}{a-r_2})^{A_2}(\frac{y-r_3}{a-r_3})^{A_3}\right]=C_3t$$ but I can't extract $y$ as an explicit function of $t$ from the above equation. Does anyone have any idea about this ODE and the approach that must be taken to solve it? If it's not solvable analytically, could anyone suggest some good methods to approximate the analytical (not numerical) solution of this ODE, please? (There isn't any restriction on the kind of the solution functions, i.e. the solution could be in terms of special functions)",['ordinary-differential-equations']
