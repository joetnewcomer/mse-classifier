question_id,title,body,tags
4873388,Geometry of linear equations,"its my first question here. I'm re-self-studying linear algebra from different sources and one of them is Linear Algebra and Its applications by g.strang 4th ed. . While i have studied a bunch of material i still don't grasp some basics and i really struggle. Page 4-5 gives two approaches for the geometry of linear equations: my problem is the second graph. I get it, the operations, how we use them as a transformation to create the vectors and that we have to guess a solution to get
to the (1,5). I also get that he is trying to make a point for later on, on Gauss elimination. But how/why can he take the coefficients of each equation for example for the y part \begin{bmatrix}-1\\1\end{bmatrix} and use it
as x,y coordinates to map it to the graph? How can -1y and 1y coefficients turn into a (x,y) / ( -1,1) vector? It looks like to me that we got two coeffients and turned them from y,y to x,y.I don't get how that concept works. Do i make sense? I've searched a lot but i may lack knowledge of the correct terms, so even if the question is novice level/not worth answering please provide me with some keywords.And of course correct me for any mistakes. Much appreciated",['linear-algebra']
4873400,Help showing this integral is positive.,"I would some help on showing that the following integral is positive: Fix $1\leq p < \infty$ and let $(a,b) \in (0.5,1)^2$ $$
\int_0^1 \int_0^1 a(p+2)|(a,b)|^p \Big(\sqrt{(x-a)^2+(y-b)^2}\Big)^{p} - p|(a,b)|^{p+2}(a-x)\Big(\sqrt{(x-a)^2+(y-b)^2}\Big)^{p-2}dydx
$$ Context: I am trying to show that the quotient $$\frac{|(a,b)|^{p+2}}{\int_0^1\int_0^1 |(x,y) - (a,b)|^pdydx}$$ achieves its maximum at $(a,b) = (1,1)$ . So, I took its derivative with respect to $a$ , which resulted in the integral above (numerator only), and now I'm trying to show it is positive, i.e. the quotient is increasing as $a$ goes to 1. Something analogous for $b$ will allow me to conclude. I know that the maximum happens at $(1,1)$ for $(a,b) \in [0.5,1]^2$ by using a graphing calculator. See Here","['maxima-minima', 'multivariable-calculus']"
4873409,Different Hausdorff topologies with the same continuous mappings,"Fix a set $X$ . For a topology $\tau$ on $X$ , let $F_\tau = \{f: X \to X \mid f \text{ continuous w.r.t. } \tau\}$ . What would be an example of a set $X$ and two Hausdorff topologies $\tau$ and $\sigma$ on $X$ such that $\tau \neq \sigma$ but $F_\tau = F_\sigma$ ?","['continuity', 'general-topology', 'separation-axioms']"
4873436,"What axioms are needed to show that every vector space has a ""coordinate system""?","It is a well-known theorem that the axiom of choice (AC) is equivalent to the statement that every vector space over any field has a basis. In this question I am interested in the following somewhat dual concept to that of a basis: If $V$ is some vector space over a field $K$ , we call a subset $S\subseteq V'$ of the dual space a coordinate system for $V$ if $S$ separates points, i.e. for each $0\neq v\in V$ there is some $s\in S$ with $sv\neq 0$ and $S$ is irredundant in that no proper subset of it also separates points. (If this is known under a different name, please let me know.) This notion is somewhat less well behaved than that of a basis, but still mimics some of its properties. For instance: A subset $B\subseteq V$ is a basis if and only if the natural map $K^{\oplus B}\to V$ is an isomorphism (and also, linear independence and generation correspond to injectivity and surjectivity vice versa). Along these same lines, some subset $S\subseteq V'$ is a coordinate system for $V$ if and only if the natural map $V\to K^S$ is injective and ""uses all the space"" in that following this map with a projection to any $K^{S'}$ for a proper subset $S'$ is never injective. Under ZFC, every vector space has a coordinate system - simply pick a basis $B$ and take all the projections $\pi_b$ that extract the coefficient of $b$ from a linear combination (in other words, the dual basis $B'$ of $B$ ). It is also clear that for finite dimensional $V$ , every coordinate system arises in this way. But for larger spaces, these notions diverge. For instance, this procedure performed on the direct product $K^\mathbb{N}$ gives some uncountable family in its (huge) dual space. But we may also simply take the countably many coordinate projections and obtain a coordinate system that way. Neither family is a basis for $(K^{\mathbb{N}})'$ and we even see that there can be coordinate systems of different cardinality. However, it can be extrapolated from this example that having a coordinate system is a more robust property than having a basis: In ZF, every vector $V$ space that has a basis $B$ , also has a coordinate system (the dual basis). But we can also use this same basis to obtain a coordinate system on $V'$ , simply by taking the family of point evaluations at $b\in B$ (in other words, the image of $B$ under the double dual map $j:V\mapsto V''$ ). Hence, since it is consistent with ZF that not every dual space of a space with a basis itself has a basis (for instance $K^\mathbb{N}$ as the dual space of $K^{\oplus \mathbb{N}}$ ), there can be spaces without a basis that still have a coordinate system. Which finally brings me to the question: What is the set-theoretic strength of the statement that every vector space has a coordinate system? Are there concrete spaces that (consistently) do not have any? Or also, is there maybe some concrete procedure that, from a coordinate system of $V'''$ (say) produces a basis of $V$ ? Thanks for any information on that matter.","['linear-algebra', 'set-theory']"
4873467,"Taking stones game beginning with 1 to 4 stones in a 2 player game. If we started with 18 stones, is the a winning strategy for the first player?","Amy and Beck are playing 'taking the stones game'. There are 18 stones on the table, and the two  people take stones in turns. The first move of the starting player can take 1 to 4 stones. For the rest of the game, each player takes a number of stones from 1 up to twice the number of stones his/her opponent just took. The player taking the last stone wins. Amy comes first. If Amy has a winning strategy, how many stones should Amy take? (at the start) If Amy does not have a winning strategy, write '0'. I tried like copying same moves for Amy and Beck etc. and did a few manual computations to see that it's not possible. An answer key with no solution says that there is no winning strategy as it gives ""0"" as the answer. Anyone know how to prove it? Is there an invariant or something which I am missing? I have some background in Math Olympiad and a Math related degree so feel free to use any techniques.","['contest-math', 'combinatorics', 'combinatorial-game-theory', 'game-theory', 'recreational-mathematics']"
4873471,"If $B = x(xI-A)^{-1}$ for a generator matrix $A$, then $B-B^2$ has positive diagonal elements","Let $A$ be the generator matrix of a continuous-time Markov chain. This means that $A$ has positive off-diagonal elements $A_{ij} > 0$ , $i \ne j$ , and row sums $\sum_j A_{ij}$ equal to $0$ . For example, $A$ could be $$
A = \left(
\begin{matrix}
-7 & 4 & 3 \\
1 & -2 & 1 \\
3 & 5 & -8
\end{matrix}
\right).
$$ I am interested in proving the following claim about the matrix $B = x(x I - A)^{-1}$ for some $x > 0$ . Claim. For the matrix $B = x(x I - A)^{-1}$ , it holds that the diagonal elements of $B - B^2$ are non-negative. Using numerical simulations I have convinced myself that this claim is likely true; however, I have not been able to make much progress toward proving it. It is straightforward to show that the matrix $B$ is stochastic. However, the claim above is not true for all stochastic matrices $B$ ; there is something special about stochastic matrices of this particular form. Any ideas?","['markov-chains', 'matrices', 'stochastic-processes', 'linear-algebra', 'stochastic-matrices']"
4873501,Inner product of signatures of piecewise linear paths,"It is a well-know observation that, given two points $x_1,x_2 \in \mathbb{R}^d$ , the path signature associated to their linear interpolation is given by the tensor exponential. Precisely, if $\Delta x$ denotes the linear interpolation $x_1 \rightarrow x_2$ , then $$S(\Delta x) = \left(1 , \Delta x, \frac{\Delta x^{\otimes 2}}{2!}, \frac{\Delta x^{\otimes 3}}{3!}, ... \right) =: \exp_\otimes(\Delta x) \in T((\mathbb{R}^d)),$$ where $S(\cdot)$ denotes the signature map and $T((\mathbb{R}^d))$ is the extended tensor algebra. Actually, the signature takes value in a subset $T^1$ of $T((\mathbb{R}^d))$ which can be endowed with an inner product, and thus becomes a Hilbert space if one takes the appropriate completions. I refer to this work for a proper definition, but I assume most readers interested in this post are familiar with this. Now, instead of considering just a line segment, let us consider a piecewise linear path. Specifically, given a finite number of points $x_1,...,x_n$ in $\mathbb{R}^d$ , we consider the path corresponding to successive linear interpolations, i.e. $x_1 \rightarrow x_2$ concatenated with $x_2 \rightarrow x_3$ , which, in its turn, we concatenate with $x_3 \rightarrow x_4$ , and so on. Let us denote this piecewise linear path by $\textbf{x}$ . Thanks to Chen's identity, we know that the signature of $\textbf{x}$ is given by multiplying in the extended tensor algebra the signatures associated to each interpolation, i.e. $$S(\textbf{x}) = \exp_\otimes(\Delta_1 x) \ \otimes \ \exp_\otimes(\Delta_2 x) \ \otimes \ ... \ \otimes \exp_\otimes(\Delta_{n-1} x),$$ where $\Delta_i x$ denotes the interpolated path $x_i \rightarrow x_{i+1}$ , and $\otimes$ is the multiplication defined in $T((\mathbb{R}^d))$ . Lastly, let $\textbf{y}$ denote some other piecewise linear interpolation of $y_1,...,y_n\in \mathbb{R}^d$ , and let $\langle \cdot, \cdot \rangle_{T^1}$ denote the aforementioned inner product. My question is: Can we express $\langle S(\textbf{x}), S(\textbf{y}) \rangle_{T^1}$ via the simpler inner products $\langle \exp_\otimes(\Delta_i x), \exp_\otimes(\Delta_j y) \rangle_{T^1}$ with $i,j \in \{1,...,n-1\}$ ?","['abstract-algebra', 'rough-path-theory', 'tensor-products']"
4873502,Finding the analytical solution of a first order system with pure time delay,"I have a simple system and I am searching for the solution for f(t): $$\frac{\partial f(t)}{\partial t} = c_1 \left( f(t) + g(t) + c_2 \right)$$ . It turns out that, in this system $g$ can be related to $f$ by a pure time delay: $$g(t) = f(t-a)u(t-a)$$ where $u(t)$ is the Heaviside function. Applying the Laplace transform to the equation above and isolating $F(s)$ leads to: $$F(s) = \frac{f(0) + c_1c_2}{s - c_1(1 - e^{-as})}$$ . Since there is a complex exponential at the denominator I cannot use the inverse Laplace transform nor break this into partial fractions. How can one find the solution of such an equation? If I cannot directly resolve the equation, can I infer a given solution (from the numerical integration of this system) and identify the coefficients? If the analytical solution does not exists, is there a proof that explains why?","['inverse-laplace', 'delay-differential-equations', 'ordinary-differential-equations']"
4873570,Calculating the area of a square between two circles,"I have this problem : Two circles shown in the figure, of centers $O$ and $O'$ ,
both have radius equal to $r$ , are externally tangent
to each other and are further tangent to the line $t$ . The square $ABCD$ has side $AB$ on line $t$ and the other
two vertices, $C$ and $D$ , which belong respectively
to the circumferences bounding the two circles of centers $O '$ and $O$ . What is the measure of the side of the square $ABCD$ ? I have made this assumption with this figure: I have drawn from $O$ and $O'$ the perpendiculars to $t$ and joined $O$ with $D$ and $O'$ with $C$ . I have denoted by $H$ and $H'$ the orthogonal projections of the centers $O$ and $O'$ . From $P$ I have drawn the perpendicular to the line $t$ . This line $t$ cut the square in two parts. Named $\overline{DC}=x$ . The rectangular trapezoids $ADOH$ and $BCO'H'$ are equivalents. $\overline{BH'}=\overline{AH}=r-x/2$ , $\overline{PF}=r-x$ , hence $$\text{area}(DCO'O)=\frac{(2r+x)\cdot (r-x)}{2}$$ $$\text{area}(OO'H'H)=2r\cdot r=2r^2$$ $$\text{area}(AHOD)=\frac{(x+r)\cdot(r-\frac x2)}{2}=\text{area}(BH'O'C)$$ Definitely it must be: $$2r^2-(x+r)\left(r-\frac x2\right)-\frac{(2r+x)(r-x)}{2}=x^2 \tag 1$$ Solving the $(1)$ I obtained true for all $x$ and not $\frac 25r$ . Where did I make a mistake?","['algebra-precalculus', 'geometry']"
4873573,"$f(x) + f(y) = f ( xy - 2024x - 2024y + 2024*2025 ) ,\forall x,y \in (2024,\infty)$","The statement of the problem : Find all monotone functions $f : \mathbb R \rightarrow \mathbb R $ that verify the following $f(x) + f(y) = f ( xy - 2024x - 2024y + 2024*2025 ) ,\forall x,y \in (2024,\infty)$ My ideas (some of them are wrong ) : First of all, by making $x = y = 2025$ we easily get that f(2025) = 0. Thinking about the fact that it is monotonic, I thought that maybe I can prove that it is a constant(proving that it has different monotonies on certain intervals), which would result in $f \equiv 0$ , which verifies so it might be the right path. I thought how we can exploit the fact that $f(2025) = 0$ so I thought about the solutions of the equation $$xy - 2024x - 2024y + 2024*2025=2025$$ now the bad part is that $x,y\in (2024,\infty)$ , so there aren't any solutions .The next idea was to think of an additive function, but it was unsuccessful. Does it seem like a difficult problem? I want to see what ideas and solutions you have, and what helped you to intuit the answer, because maybe this will help me to develop a more analytical way of thinking, or at least make me understand the thought process behind the solution. Thank you very much for all the ideas and solutions !","['functional-equations', 'functions']"
4873582,How would one integrate over $SO(n)$?,"Suppose you want to find the average of an $n\times n$ diagonal matrix $A$ over all possible rotations, $$
\langle A\rangle = \int\limits_\text{SO($n$)} Q^T A Q \; dQ.
$$ It's easy enough to do this for $n=2$ because you have only one degree of freedom. I suspect that $n=3$ would be ""simple"", but tedious. Using Monte-Carlo, I conjecture that $\langle A\rangle = \text{diag}\left[\text{tr}(A)/n \right]$ . But I'm curious if there's a way that you could derive this relation analytically for a general $n$ . I suspect that you could make some symmetry arguments to take the problem of an integral over $\text{SO}(n)$ to a sum over permutation matrices. Why take $A$ to be diagonal? Because really I'm interested in $A$ being positive-definite, and when that's the case, you can just diagonalize it and then rotate into the principal coordinate-frame.","['integration', 'measure-theory', 'monte-carlo', 'orthogonal-matrices', 'group-theory']"
4873652,"If $x>0,y>0$ and $4xy=2^{x+y}$ then find the minimum and maximum values of $x+y$.","If $x>0,y>0$ and $4xy=2^{x+y}$ then find the minimum and maximum values of $x+y$ . My Attempt I tried by putting $t=x+y$ $\Rightarrow 4x(t-x)=2^t$ . On differentiation we have $4t-8x=\frac{dt}{dx}(2^tln2-4x)$ . For maximum/minimum put $\frac{dt}{dx}=0$ to obtain $t=2x$ $\Rightarrow x+y=2x$ and thus $y=x$ . Putting in given equation one obtains $4x^2=2^{2x}$ . The obvious solution here is $x=1,2$ . So the value of corresponding $y$ will be $y=1,2$ . So, the extreme values of $x+y$ can be $2$ and $4$ . Is above correct or am I missing something here. Can we solve this using $AM\geq GM$ inequality.","['a.m.-g.m.-inequality', 'maxima-minima', 'calculus', 'inequality', 'derivatives']"
4873680,Concurrency involving two tangents and a secant of a circle,"I found out this problem while fooling around with Geogebra. Let $(O)$ be a circle in the plane, $A$ be a point lies outside of the circle. From $A$ , draw two tangents $AB, AC$ to $(O)$ ( $B, C\in (O)$ ). A line $d$ passing through $A$ ( $O\not\in d$ ) intersects $(O)$ at two points $E, F$ . $BC$ and $EF$ intersects at $K$ ; $CE, CF$ intersects the line $AO$ at $X, Y$ respectively. a. Prove that $CK, FX, EY$ are concurrent. b. Let $D$ be the intersection of $BO$ and $(O)$ . $DE, DF$ intersect $AO$ at $M, N$ respectively. Prove that $DK, EN, FM$ are concurrent. What I have proved so far: $AEMC, AFCN$ ; $EMYF, EXNF$ are inscribed. $OM = ON$ . Indeed, since $\widehat{CEM} =\widehat{EBD}$ and $\widehat{OBE} = \widehat{BAM}=\widehat{MAC}$ then $AEMC$ is inscribed. Similarly, $AFCN$ is inscribed. Now since $\widehat{AME} = \widehat{ECA}$ and $\widehat{ECA} = \widehat{EFC}$ then $\widehat{EMY}+\widehat{EFY} = \pi$ , thus $EMYF$ is inscribed. Similarly, $EXNF$ is inscribed. Note that $MNDC$ is an isosceles trapezoid, then $BMDN$ is a parallelogram thus $O$ is the midpoint of $MN$ . Experimenting on Geogebra shows that the above results are true, however Still I can't prove them. Please help me, thanks. UPDATE: I've edited my post to include my attempts. And thanks to Reza Rajaei and D S solutions, I still love to see a solution without using trigonometric Ceva theorem. Thanks for answering my questions.","['contest-math', 'euclidean-geometry', 'geometry']"
4873723,Topological version of uniform convergence of functions,"We have a sequence of continuous functions $\{f_n\}$ on a Banach space $X$ and $f_n(x)\to f(x)$ for each $x\in X$ as $n\to\infty$ . Given an open ball $B\subset X$ and $\epsilon>0$ , we want to show that there exists another open ball $B_0\subset B$ and $m\geq 1$ such that $|f_m(x)-f(x)|\leq \epsilon$ , for all $x\in B_0$ . My approach: Let $Y$ be a closed ball in $B$ , fix $\epsilon>0$ and consider $E_l=\{x\in Y| \sup_{j,k\geq l} |f_j(x)-f_k(x)|< \epsilon\}$ . (Please check the following!) Then $E_l$ is open for each $l$ : this is because for any $x\in E_l$ , take the radius $r$ and corresponding ball $B_r(x)$ , such that both $d(f_j(x),f_j(x'))$ and $d(f_k(x),f_k(x'))<\delta/2$ where $\delta=\epsilon-\sup_{j,k\geq l} |f_j(x)-f_k(x)|$ (by taking the minimum $r$ at $x$ for $f_j$ and $f_k$ ), and therefore, we have $$\sup_{j,k\geq l} |f_j(x')-f_k(x')|=|f_{j'}(x')-f_{k'}(x')|$$ where $j', k'$ corresponds to the indices in the family $\{f_n\}$ corresponding to the supremum at the point $x'$ , then $$\leq |f_{j'}(x')-f_{j'}(x)+f_{j'}(x)-f_{k'}(x)+f_{k'}(x)-f_{k'}(x')|\leq \delta + \sup_{j,k\geq l}|f_j(x)-f_k(x)|< \epsilon$$ Then, notice that $E_1\subseteq E_2\subseteq E_3\dots$ and that $Y=\cup_{l=1}^\infty E_l$ . To prove this, take $(\cup_{l=1}^\infty E_l)^c=\cap_{l=1}^\infty E_l^c=\cap_{l=1}^\infty\{x\in Y: \dots \geq \epsilon \}=\varnothing$ by pointwise convergence of $f_n$ (for any $x$ , there is an $L$ such that for $l\geq L$ , $x\notin E_l^c$ . Now, I'm thinking that from this, the conclusion should follow but it feels like I'm missing something. We've covered Baire Category Theorem in class, and I'm sure this plays into the proof (at a higher level, whereas my approach is more bottom-up), but I'm unsure how to proceed and would therefore appreciate any help and/or feedback on my attempt. Notes: Recall a Baire space is a topological space such that the intersection of countably many dense open sets is still dense Recall Baire Category Theorem: Let $(X,\rho)$ be a complete metric space, then 1) if $\{U_n\}_1^\infty$ is an open dense set in $X$ , then $\cap_{n=1}^\infty \bar{U_n}=X$ , and also 2) $X$ is not meager. The version of BCT below follows from this. $A_l$ is clearly closed because for any $n$ and $x\in A_l$ , $\cdots\leq \epsilon$ , so $f(x)\leq \epsilon$ for any $x$ .","['analysis', 'real-analysis', 'functions', 'functional-analysis', 'general-topology']"
4873754,Find a vertex of a tetragon where three vertices are given,"Suppose that $V,W,U$ are three 3D points and $L,K$ are given positive values. Let $dist(A,B)$ represents euclidean distance between $A,B$ . Morover, assume that $M$ is a plane that passes through $V,W,U$ .
What I need is a point $P$ where: $$dist(V,P)=L \\
dist(U,P)=K \\
P\in M $$ Obviously, we must find the equations of 2 circles, $C_1(V,L), C_2(U,K)$ where $C_1\subset M, C_2\subset M$ and then find their intersections.
These possible intersections would be our points. Of course $L,K$ are given such a way that the intersections exist.","['analytic-geometry', 'linear-algebra', 'geometry', 'vector-analysis']"
4873783,Two formulations of Riesz–Markov–Kakutani representation theorem,"Let $ X $ be a locally compact Hausdorff space, and $ C_c(X) $ be the space of all complex-valued continuous functions with compact support on $ X $ . As far as I know, there are two formulations of Riesz–Markov–Kakutani representation theorem. Formulation 1 (e.g., D. L. Cohn, Measure Theory , 2nd edition, Theorem 7.2.8). Positive linear functionals on $ C_c(X) $ correspond bijectively to positive Borel measures $ \mu $ on $ X $ that is outer regular for all Borel subsets $ A \subseteq X $ , which means $$ \mu(A) = \inf \{\mu(U) \mid \text{open subsets $ U \subseteq X $ containing $ A $}\}, $$ inner regular for open subsets $ U \subseteq X $ , which means $$ \mu(U) = \sup \{\mu(K) \mid \text{compact subsets $ K \subseteq U $}\}, $$ finite on compact subsets. Formulation 2 (e.g., N. Bourbaki, Integration , Chapter IX, Section 3, No. 2, Theorem 2). Positive linear functionals on $ C_c(X) $ correspond bijectively to positive Borel measures $ \mu $ on $ X $ that is inner regular for all Borel subsets $ A \subseteq X $ , which means $$ \mu(A) = \sup \{\mu(K) \mid \text{compact subsets $ K \subseteq A $}\}, $$ finite on compact subsets. So, there should be a natural correspondence between the classes of positive Borel measures in Formulation 1 and 2. Question. Is it possible to describe and prove this correpondence directly (i.e., without using positive linear functionals)?","['integration', 'measure-theory', 'functional-analysis', 'analysis']"
4873794,Find the Domain of this irrational expression,"While finding the Domain of this expression $$\sqrt{(2x-8)(x-1)}$$ I got this: $(2x-8)(x-1)≥0$ $2x-8\geq0 \vee x-1\geq0$ $x\geq4 \vee x\geq 1$ So the Domain is: $x\in [1, +\infty)$ But the real domain is $x\in (-\infty,1] \cup [4,+\infty)$ So it means that $x\leq1$ Can someone please explain this?","['algebra-precalculus', 'functions', 'radicals']"
4873810,Defining a Plane with Two Parallel Lines,"I'm learning about planes in linear algebra, and my professor said that to define a plane two lines must intersect. However, I was thinking, what if the two lines are parallel? In that case, they would never intersect. Can't two parallel lines still define a plane, you can think of the plane as a piece of paper and if I draw 2 parallel lines on it I have a plane. How can we define a plane with two parallel lines mathematically or do I must have a point of intersection?","['linear-algebra', 'geometry']"
4873834,What's failing on the assumptions?,"Recall the following version of the Riemann-Lebesgue theorem: Theorem: Let $1 \leq p \leq \infty$ , $\Omega = \Pi_1^n(a_i,b_i)$ and $u \in L^p(\Omega)$ . Let $u$ be extended by periodicity from $\Omega$ to $\mathbb{R}^n$ and define $$u_\nu(x) = u(\nu x), \ \ \text{ and } \ \ \bar{u} = \frac{1}{meas \Omega}\int_\Omega u(x)dx,$$ then $u_\nu \rightharpoonup  \bar{u}$ in $L^p$ if $1 \leq p < \infty$ and if $p = \infty$ , then $\rightharpoonup^* \bar{u}$ in $L^\infty$ . My example is the following: Fix $n = 1$ and let $u(x)=1_{B(0,r)}(x)$ the indicator function of the ball centred at the origin with radius $r > 0$ . This function is clearly in $L^2$ with norm $|B(0,r)|^{1/2}$ . Define $$u_\nu(x) = u(\nu x) = 1_{B(0,r/\nu)}(x).$$ Now, for each $g \in L^2$ it is easy to see that $$\lim_{\nu \rightarrow 0}\int_\Omega u_\nu(x)g(x)dx = \lim_{\nu \rightarrow \infty}\int_{B(0,r/\nu)}g(x)dx = g(0).$$ But then $u_\nu \rightharpoonup \delta_0$ where $\delta_0$ is understood as the delta-dirac distribution. Question: What's wrong with this? Why isn't this converging to $\bar{u}$ and where should I have used the function extended by periodicity? Thanks!","['measure-theory', 'functional-analysis', 'real-analysis']"
4873844,An abelian group of order 35 must be cyclic [duplicate],"This question already has answers here : An abelian group $G$ of order $35$ with $g^{35}=e$ for all $g\in G$ is cyclic. (3 answers) Closed 4 months ago . I'm reading through Contemporary Abstract Algebra by Joseph A. Galian and I stumbled upon the following problem: Suppose $G$ is an Abelian group of order $35$ and every element of $G$ satisfies $x^{35} = e$ . Prove that $G$ is cyclic. My approach was the following: I divided the problem into these three cases: $G$ has elements of order $5$ and elements of order $7$ Every non-trivial element of $G$ has order $5$ Every non-trivial element of $G$ has order $7$ My idea was that if I manage to prove in the first case we will have an element of order $35$ and that the remaining two cases are impossible, I would have proved that $G$ must have at least an element of order $35$ which will make $G$ cyclic. I dealt the first case as follows:
Let $a\in G$ and $b\in G$ such that $\lvert a\rvert = 5$ and $\lvert b\rvert = 7$ . In that case, $(ab)^5 = b^5 \not = e$ and $(ab)^7 = a^2 \not = e$ , which means that $\lvert ab\rvert = 35$ hence $G =\lt ab \gt$ . However, I got stuck in the remaining two cases. Thank you in advance.","['group-theory', 'abstract-algebra', 'abelian-groups', 'cyclic-groups']"
4873848,Complex Conjugation is non-holomorphic,"If I consider $z_0 \in \mathbb{C}$ and denote $g(z)=\bar{z}$ , then I can verify that: For $\lambda \in \mathbb{R^*}$ : $$lim_{\lambda \to 0} \frac{g(z_0+\lambda)-g(z_0)}{\lambda}=\frac{\lambda}{\lambda}=1$$ In other hand, if we consider $w \in i\mathbb{R^*}$ : $$lim_{w \to 0} \frac{g(z_0+w)-g(z_0)}{w}=\frac{-w}{w}=-1$$ So that, $g$ is not holomorphic at $z_0$ . But I could see in an identical manner that $\frac{\partial u}{\partial x}+i\frac{\partial{v}}{\partial x}\neq -i(\frac{\partial u}{\partial y} + i\frac{\partial v}{\partial y})$ , i.e., $$
\frac{\partial u}{\partial x}\neq \frac{\partial v}{\partial y} \quad or \quad  \frac{\partial u}{\partial y}\neq -\frac{\partial v}{\partial x} $$ Is this a good approach?",['complex-analysis']
4873872,Find number of solutions to the equation $\sin(6\sin x)=\frac{x}{6}$.,"Find number of solutions to the equation $$\sin(6\sin x)=\frac{x}{6}.$$ By plotting it on any graphing software one is able to see instantly that it has $15$ solutions. However I am not able to work it out manually. Let $f(x)=\sin(6\sin x)$ Period of $f(x)$ is obviously $2\pi$ $f'(x)=\cos(6\sin x)\cdot 6\cos x$ Putting $f'(x)=0$ to obtain points of maxima/minima is the tricky part. $x=\frac{\pi}{2},\frac{3\pi}{2}$ are quite obvious but $6\sin x=\frac{\pi}{2},\frac{3\pi}{2}$ is difficult to get. After this it becomes even more confusing. Could there be a way out.","['trigonometry', 'calculus', 'graphing-functions', 'algebra-precalculus']"
4873874,Is this a finite set for which it is impossible to list its elements?,"Here is the idea for the set. Let $\alpha$ be a real number. Then $x_\alpha$ be a set of digits with the property that a digit is in the set if it appears infinitely many times in $\alpha$ . For many numbers $\alpha$ it is easy to determine $x_\alpha$ . For some it is challenging like $\alpha = \pi$ . However, what if $\alpha$ was a non-computable number? Then it would be impossible to list the elements of $x_\alpha$ , right?",['elementary-set-theory']
4873880,How to compute $\int\frac1x\Bigl(\sum\limits_{n\ge0}\frac{x^{2n}}{2^{2n}(n!)^2}\Bigr)^{-2}\mathrm dx$?,"The background of $$\int\frac{1}{x}\left(\displaystyle\sum_{n\geq 0}\frac{x^{2n}}{2^{2n}(n!)^2}\right)^{-2}\text dx$$ is that it yields a second solution to $xy''+y-xy=0$ if you multiply it by the series in the integrand (first solution). My professor found an aproximate answer by multiplying the two series up to $5$ th term and then he did long division with $1$ as numerator and the series squared as denominator. Nonetheless, what I was looking for is an exact solution. I thought about this: $$\frac{1}{\left(\displaystyle\sum_{n\geq 0}\frac{x^{2n}}{2^{2n}(n!)^2}\right)^2}=\sum_{k\geq 0}b_n x^k\implies 1=\left(\displaystyle\sum_{n\geq 0}\frac{x^{2n}}{2^{2n}(n!)^2}\right)^2\left(\sum_{k\geq 0}b_n x^k\right),$$ where $$
\begin{aligned}
\left(\sum_{n\geq 0}\frac{x^{2n}}{2^{2n}(n!)^2}\right)^2 &=\left(\sum_{i\geq 0}\frac{x^{2i}}{2^{2i}(i!)^2}\right)\left(\sum_{j\geq 0}\frac{x^{2j}}{2^{2j}(j!)^2}\right)\\
&=\sum_{i\geq 0}\sum_{j\geq 0}\frac{x^{2(i+j)}}{2^{2(i+j)}(i!j!)^2}\\
&\overset{n=i+j}{=}\sum_{n\geq 0}\sum_{i= 0}^n\frac{x^{2n}}{2^{2n}(i!(n-i)!)^2}\\
&=\sum_{n\geq 0}a_nx^{2n},
\end{aligned}
$$ and thus, $$
\left(\sum_{n\geq 0}a_n x^{2n}\right)\left(\sum_{k\geq 0}b_k x^k\right)=\sum_{n\geq 0}\sum_{k\geq 0}a_nb_kx^{2n+k}=\sum_{m\geq 0}\sum_{n=0}^{\lfloor m/2\rfloor}a_nb_{m-2n}x^m
$$ Finally, equating cofficients we find all $b_k$ and hence the antiderivative is simply $$\int\frac{1}{x}\sum_{k\geq 0}b_kx^k\text dx=\int\left(\frac{b_0}{x}+\sum_{k\geq 0}b_{k+1}x^k\right)\text dx=b_0\ln x +\sum_{k\geq 0}\frac{b_{k+1}}{k+1}x^{k+1}+C$$","['integration', 'power-series', 'ordinary-differential-equations']"
4873899,How to determine if function is differentiable?,"I am given this piecewise function $f: \mathbb{R} \rightarrow \mathbb{R}$ , $f(x)=   \left\{\begin{array}{ll}x^2 & x>0 \\ 0 & x ≤ 0 \\ \end{array} \right.  $ I have to determine if the function is differentiable over $\mathbb{R}$ or not. The way I think about differentiability is like this: ""Can a function be differentiated at every point?"" But that doesn’t really help me, I have been looking for a specific formula I can generally use to determine if a function is differentiable or not, but with no luck. How should I go about finding out if this function is differentiable or not?","['continuity', 'functions', 'derivatives', 'ordinary-differential-equations']"
4873905,Using Generator functions to solve following recursive formulas,"Assume we have: \begin{cases}
a_{n} = 2^{n} \times \left(1 - b_{{\left\lfloor \frac{n}{2} \right\rfloor}}\right) \\
b_{n} = b_{n - 1} + \frac{a_{n}}{4^{n}} \\
b_{0} = 0
\end{cases} I tried finding the closed formula for $a_{n}$ using generator functions. But I am stuck at this point: $$ \frac{1}{1 - x} - A(\frac{x}{2}) = \frac{1}{1 - x} \cdot \left(A(\frac{x^{2}}{4}) - 1\right) $$ in which: $$ A(x) = \sum_{i = 0}^{\infty} a_{i}x^{i},  B(x) = \sum_{i = 0}^{\infty} b_{i}x^{i}$$ This is how I got to this point: $$ A(\frac{x}{2}) = (a_{0}, 1 - b_{0}, 1 - b_{1}, \dots) \Rightarrow \frac{1}{1 - x} - A(\frac{x}{2}) = (0, b_{0}, b_{1}, b_{1}, b_{2}, b_{2}, \dots) $$ $$ B(x^{2}) + xB(x^{2}) = \frac{1}{1 - x} - A(\frac{x}{2}) $$ and we also have: $$ B(x) = \frac{A(\frac{x}{4}) - 1}{1 - x} $$ with a little algebra, you should get there. Can the closed formula be found? If so how can I proceed with the solution? If you have any other solution I would be glad to hear it. I'm not sure about the title of this question and which category of generator functions I should put this in. Please edit this if you know a better title. P.S. $a_{n}$ counts the number of binary strings of length $n$ which doesn't have the same prefix and suffix of any length. (except the prefix or suffix that covers all the string.) Keep this in mind if it is helpful. As mentioned in comments, you can find the sequence in OEIS","['combinatorics', 'recursion', 'generating-functions']"
4873964,Why must rational $x$ and $y$ satisfy $x+y=2$ and $2\sqrt{3xy}=3$ in order for $2\sqrt{3}-3=(x+y)\sqrt{3}-2\sqrt{3xy}$ to be true?,"I am going through a maths book and unable to understand the following: Given that $x$ and $y$ are rational numbers, then if the equation $$2\sqrt{3}-3=(x+y)\sqrt{3}-2\sqrt{3xy}$$ is true, then we must have $x+y=2$ and $2\sqrt{3xy}=3$ . I don't understand this. We cannot reach such a conclusion with equations involving only rational numbers. For example, let $a$ , $b$ , $c$ , and $d$ be rational numbers, and $a - b = c - d$ ; then it does not mean that $a = c$ and $b = d$ always for the equation to be true. How can we be sure that we must have $x + y = 2$ and $2 \sqrt{3xy} = 3$ for the equation to be true?","['algebra-precalculus', 'irrational-numbers']"
4874051,Solving non-homogenous differential equation $\ddot{y}+\frac km\dot{y}=-g\hat{j}$,"How to solve this non-homogeneous second-order linear ordinary differential equation $$\ddot{y}+\frac km\dot{y}=-g\hat{j}$$ $\hat{j}$ is just unit vector in $y$ direction. I found the solution to homogenous part $y_h(t)=c_1+c_2e^{-\frac{k}{m}t}$ . By variation of parameters assume $y_p(t)=c_1(t)+c_2(t)e^{-\frac{k}{m}t}$ . I calculated $\dot{y}_p=c_1'(t)+c_2'(t)e^{-\frac{k}{m}t}-\frac{k}{m}c_2(t)e^{-\frac{k}{m}t}$ and $\ddot{y}_p(t)=c_1''(t)+c_2''(t)e^{-\frac{k}{m}t}-c_2'(t)\frac{k}{m}e^{-\frac{k}{m}t}-c_2''(t)\frac{k}{m}e^{-\frac{k}{m}t}+(\frac{k}{m})^2e^{-\frac{k}{m}t}$ , if I didn't make any errors.",['ordinary-differential-equations']
4874055,Can we write every $C^1$ complex function on the unit circle as the the difference of two approriate functions?,"If $g$ is $C^1$ on the unit circle $C(0,1)$ . Then there is a function $f^+$ holomorphic on $B(0,1)$ and continuous on $\bar B(0,1)$ , a function $f^-$ holomorphic on $\mathbb{C}\backslash\bar B(0,1)$ and continuous on $\mathbb{C}\backslash B(0,1)$ , such that $g(z)=f^+(z)-f^-(z)$ on the unit circle. This seems like it could be a direct consequence of some of the lecture results (e.g. Cauchy's Integral Formula, Taylor series), but cauchy's formula just gives a holomorphic function on $B(0,1)$ , I do not know how that relates to the value of $g(z)$ . While Taylor series only applies for functions holomorphic on some domain. If we can extend $g$ to some holomorphic function on $\mathbb{C}$ then surely the result follows... But how do we do something like that? Any help will be appreciated.","['complex-analysis', 'analytic-functions']"
4874077,"Stein and Shakarchi, Complex Analysis, Chapter 2 Example 2","I'm reading through Stein and Shakarchi's Complex Analysis textbook, but I'm a bit confused by their proof that $$ \int_{0}^{\infty} \frac{1-\cos(x)}{x^2} dx = \frac{\pi}{2}$$ They consider the function $f(z) = \frac{1-e^{iz}}{z^2}$ and an indented semicircle as their contour The part where I'm confused is the integral of $f(z)$ over $\gamma_{\epsilon}^+$ . The way they evaluate this integral is by first noting that $$ f(z) = \frac{-iz}{z^2} + E(z) $$ where $E(z)$ is bounded as $z\rightarrow 0$ . I'm fine with the rest of the proof but I'm puzzled by $E(z)$ . My question: How is this function bounded as $z \rightarrow 0$ ? It seems like $E(z)$ would just be $$ E(z) = \frac{1+iz+e^{iz}}{z^2}$$ Is it because we're integrating over $\gamma_{\epsilon}^+$ and so $|z| = \epsilon$ and so $$ \left| E(z) \right| = \left| \frac{1}{z} + \frac{i}{z} + \frac{e^{iz}}{z^2} \right| \leq \left|\frac{1}{z} \right| + \left| \frac{i}{z} \right| + \left| \frac{e^{iz}}{z^2} \right| = \frac{1}{\epsilon} + \frac{1}{\epsilon^2} + \frac{1}{\epsilon^2} $$ ?","['complex-analysis', 'proof-explanation']"
4874078,How to understand the differential is a linear map? [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 4 months ago . The community reviewed whether to reopen this question 4 months ago and left it closed: Original close reason(s) were not resolved Improve this question I read the following claim in the book , P19 Eq.(3.9). For a smooth function $f:\mathcal{E} \rightarrow R$ , where $\mathcal{E}$ is a linear space. $Df(x): \mathcal{E}\rightarrow R$ is
the differential of $f$ at $x$ , that is, it is the linear map defined
by: $$Df(x)[v]=\lim\limits_{t\rightarrow 0} \frac{f(x+tv)-f(x)}{t}.$$ Is this the standard definition of differential? Is the claim "" $Df(x)$ is a linear map"" inferred from the limit expression?","['analysis', 'calculus', 'linear-algebra', 'linear-transformations', 'derivatives']"
4874096,Number of binary strings of length $56$ vs number of permutations of English alphabet,"This is exercise $1.2$ in Nicholas Loehr's book ""Combinatorics"". Which is larger: the number of binary strings of length $56$ , or the number of permutations of the English alphabet ( $26$ letters)? Letting $D$ and $P$ denote these sets, respectively, it is obvious that $|D|=2^{56}$ and $|P|=26!$ . I checked using a calculator that $26!$ is indeed larger, but I want to find a combinatorial argument for this. So far, I have been unsuccessful. I have been trying to find an injection $D\hookrightarrow P$ , though I'm not sure this is the best strategy. I know that any binary string can be represented uniquely in the form $0^{\alpha_1}10^{\alpha_2}1\cdots 10^{\alpha_k}$ where $0^j$ denotes a string of $j$ consecutive zeroes. However, this doesn't immediately help to get a permutation of $26$ letters, since the $\alpha_i$ could be between $0$ and $56$ , and they could repeat. But maybe there is something that can be done with the $\alpha_i$ to get an injection. On the other hand, one can uniquely represent a permutation as a $26\times 26$ matrix with exactly one $1$ per row and column and zeroes elsewhere. But I don't see a way to get a map from these matrices that would surject onto $D$ . I also considered that $2^{56}$ is the number of subsets of a $56$ -element set, but that didn't seem to lead anywhere so far yet either. Any hints on how to continue any of the above ideas to get a combinatorial proof, or alternative methods would be appreciated. Edit: I am aware that it can be shown, e.g., inductively, that $n!$ is larger than $2^n$ for $n$ sufficiently large (or that $n! > 2^{2n+4}$ in this case), but I am interested in a more combinatorial argument for this.","['permutations', 'combinatorics', 'discrete-mathematics']"
4874151,Subset of $X$ closed under countable disjoint unions and complements a $\sigma$-algebra?,"Be wary not to confuse this post with this one . In this case, $M$ is simply a subset of the power set of $X$ . Let $X$ be a set and $M$ be a nonempty collection of subsets of $X$ . Is it true that if $M$ is closed under complements and countable unions of disjoint sets, then $M$ is a $\sigma$ -algebra? [Hint: any countable union of sets can be written as a countable union of disjoint sets.] The hint confuses me because it seems to suggest that the question is true. But I don't think it is. Just choose $X= \{a,b,c,d\}$ and choose $$M = \{\varnothing, X, \{a,b\}, \{b,c\}, \{c,d\}, \{a,d\}\}$$ Then $M$ is certainly closed under countable disjoint union and complements, but $\{a,b\}\cup \{b,c\} = \{a,b,c\} \not\in M$ . So $M$ is not closed under countable unions and so $M$ is not a $\sigma$ -algebra. I suspect I'm missing something here because this was too easy to do, especially given the hint.","['measure-theory', 'analysis']"
4874152,Convergence of the double series,"Let $\{a_i\}_{i\in \mathbb{N}}$ be a sequence of positive real numbers, whose series converges i.e. $\sum\limits_{i\in \mathbb{N}}a_i = a < \infty.$ Does this imply the convergence of the double series $\sum\limits_{i,j \in \mathbb{N}}a_i a_j?$ If yes, how can this be proven? If not, what are some counterexamples? Any help is appreciated. Thanks in advance. P.S. : A double series $\sum\limits_{i,j \in \mathbb{N}}a_i a_j$ converges to A, if for all $\epsilon > 0,$ there exists $ N_0 \in \mathbb{N}$ such that \begin{eqnarray}
 \left|\sum\limits_{i=1}^m\sum\limits_{j=1}^n a_i a_j -A \right| \leq \epsilon, \qquad \text{for all } m,n \geq N_0.
\end{eqnarray}","['calculus', 'sequences-and-series', 'analysis', 'real-analysis']"
4874172,"If $f$ and $g$ are algebraic functions, is $x\mapsto (f(x),g(x))$ contained in an algebraic curve?","Let's consider the following definition (I'm not an expert, so let me know if this is not the usual one!). Let $I\subset \mathbb{R}$ be an open interval. A function $f:I\to \mathbb{R}$ is said to be algebraic on $I$ if there is a nonzero polynomial $p\in \mathbb{R}[x,y]$ such that $p(x,f(x)) = 0$ , for every $x\in I$ . Remark: Even if this is not usual, it is what I have in the situation I'm studying. My question is: Given two algebraic functions $f,g$ on $I$ , is there a nonzero polynomial $P\in \mathbb{R}[x,y]$ such that $P(f(x),g(x))=0$ for every $x\in I$ ? What I've tried: Let $p$ and $q$ be polynomials on $\mathbb{R}[x,y]$ such that $p(x,f(x))=q(x,g(x))=0$ , for every $x\in I$ . Write $$p(x,y)=\sum_{j=0}^n a_j(y) x^j,$$ $$ q(x,y)=\sum_{j=0}^m b_j(y) x^j,$$ with $a_j, b_j\in \mathbb{R}[y]$ and $a_n$ , $b_m$ both not identically zero. Whithout loss of generality, suppose $m\leq n$ . Then we have $$b_m(g(x)) x^m =-\sum_{j=0}^{m-1}b_j(g(x)) x^j,\quad \forall x\in I.$$ Then, multiplying the equation $0=p(x,f(x))$ by $b_m(g(x))$ as many times as needed, allows us to decrease the degree of $x$ in this equation to, at most, $m-1$ . More precisely, this yields an equation of the form $$Q(x,f(x),g(x))=0,\quad \forall x\in I,$$ for a certain polynomial $Q\in \mathbb{R}[x,y,z]$ , with degree at most $m-1$ on $x$ . Collecting again on $x$ and isolating its leading term, allows us again to decrease the degree of $x$ by multiplying the equation $q(x,g(x))=0$ by its coefficient (again, as many times as needed - and this complicates matters, since we're talking about powering polynomials). So eventually, we're going to have an equation ""free of $x$ "", i.e. $P(f(x),g(x))=0$ , for some $P\in \mathbb{R}[x,y]$ . Example: Consider $n=2$ , $m=n-1$ . For the sake of simplicity, let's write $a_j(f(x))$ and $b_j(g(x))$ just as $a_j$ , $b_j$ . Then we have $$a_2x^2+a_1x+a_0=0,$$ $$b_1x=-b_0.$$ Multiplying the first equation by $b_1^2$ , we get the relation $$a_2b_0^2-a_1b_0b_1+a_0b_1^2=0.$$ The case $n=3$ , $m=n-1$ will require two steps and yields a considerably more complicated relation... This seems a very promising algorithm, at least in theory (although computationally expensive). But is there any hope to pose conditions on $f$ and $g$ in order to guarantee that $P$ is nonzero? Actually, what $P\equiv 0$ would mean for $f$ and $g$ ? (this seems very unlikely in general).","['algebraic-curves', 'algebraic-geometry', 'abstract-algebra']"
4874187,Find surface which generated by revolving a line in $\mathbb{R}^3$,"Problem : Let $l$ be a line which passes two points : $(1,0,0), (1,1,1)$ . And $S $ be a surface which generated by revolving line $l$ around $z$ -axis. Find a volume enclosed by surface $S$ and two planes : $z=0, z=1$ . My Attempt Parametric equation of $l$ can be obtained easily : $$l(t) = (1,t,t)$$ And, distance $d(t)$ between $l$ and $z$ -axis is same with distance between $(0,0, t), (1,t,t)$ : $$d(t) = \sqrt{1+t^2}$$ Since $l(0)=(1,0,0), l(1)=(1,1,1)$ , volume $V$ what we want is : $$\begin{align} V &= \pi\int_0^1 d(t)^2 dt \\ &= \pi\int_0^1 (1+t^2)dt \\ &= \frac{4}{3}\pi\end{align}$$ Here are my main questions : Is this method legit? If not, I'd like to know what the problem is. If yes, is this method can be used anytime? (For problems which request volume enclosed by surface which generated revolving line or curve and planes.) Can I find equation of $S$ explicitly to evaluate $V$ with triple integral?","['multivariable-calculus', 'calculus', 'solid-of-revolution', 'volume']"
4874200,Trying to adapt an Elo-ranking system for an asymetric multiplayer game,"My friends and I have used an Elo-Ranking system to track our relative strength in the social deduction game ""Secret Hitler"". So far, the formula we used was a very basic adaptation of the original Elo-formula that uses a lot of assumptions and thus gives unfair results. First, I will describe the relevant features of the game to those unfamiliar with it. Second, I will show our old formula and describe its shortcomings. Third and last, I will show the draft for our new formula. I will outline some potential problems with it that we have noticed and ask for your help in spotting any that we have overlooked. The game is played with 5 to 10 people, seperated into 2 teams. The teams are of unequal, but predetermined size. (An 8-player game will always have teams of 3 vs 5 for example.) The smaller team (""red"") has a smaller chance of winning than the larger team (""blue""). (In our data it's ~40%/60%.) We record wins (1) and losses (0) for each player and can infer the team they were on from that (3 wins and 5 losses in a game must mean team red won). Old formula: https://i.sstatic.net/tIxrP.png $E_n$ is your new Elo. $E_a$ is your old Elo. $k$ is a constant that is the max. delta between $E_n$ and $E_a$ per game. $Er$ is your personal result (1 or 0, draws are impossible in this game). $E_G$ is the opposite team's average Elo. $E_T$ is your own team's average Elo (including your personal Elo). $S$ is the stepsize. (A value of 40 means that if your Elo is 40 points higher than the opponent's it means that your expected value is one order of magnitude higher than your opponents.) This makes a lot of assumptions, here are a few: Your likelihood of being on either team matches the theoretical distribution. You are just as good at playing on team red than you are at playing on team blue. We also combine your own Elo and the overall team average with a 50/50 distribution, without considering cases like a pro playing with a team of noobs, where the pro's actions are more responsible for overall team performance (in our experience). To eliminate some of these shortcomings, here is our new formula: https://i.sstatic.net/ZXHUQ.png (I am sorry for the links, this is my first question.) $F$ is the general winrate of the team you were on (red or blue). $A_E$ is your own contribution to the outcome relative to the rest of your team. $A_T$ is your teams contribution to the outcome relative to you. All of these values are extracted from the raw results with some very inefficient and 'hacky' excel-code, but that is the topic of another post in another forum. Some thoughts on the new formula: For some of these values, we can go two different routes, the comparative and the personal. For your faction's winrate for example we can either use the overall average (about 40/60) or take your own results into consideration: Maybe you are really good at playing on team red, bringing your average closer to 50/50. Shouldn't you then be rewarded for showing personal progress when you ""unexpectedly"" win a blue-team game? The latter has two drawbacks: While it gives you an Elo that more accurately tracks your personal progress, comparing these values between players is not fair. It also means that you need to have played a large number of games for your own values to become statistically relevant. In general, we 'officially' give players a ranking after their 10th game and gradually fade out the overall averages from the first to the 10th game, but that value was chosen arbitrarily without being rooted in data. Our current solution is to calculate two Elo values for each player: A personal Elo that is a (weak) indicator of potential and a comparable Elo that determines the ranking. Something that we are not able to consider at all, due to the too-small number of games played in total, is whether a player is better at playing games of a certain size, i.e. 7-player games. Have we overlooked something in our calculation, maybe another asymetry in the game that we are not aware of? Have we made a mistake in transfering our thoughts to a formula? Do you have a good idea how to incorporate some of our considerations that have not been implemented yet?","['statistics', 'probability']"
4874283,Proving a multivariate normal distribution gets the maximum entropy when mean and covariance are given,"I'm working on a homework question. The first part was: Given an unbounded one dimensional continuous random variable: $X\in\left(-\infty,\infty\right)$ , that satisfies: $\left\langle X\right\rangle =\mu,\;\left\langle \left(X-\mu\right)^{2}\right\rangle =\sigma^{2}$ Show that the distribution that maximizes entropy is Gaussian $X\sim N\left(\mu,\sigma^{2}\right)$ . I've solved this using Lagrange multipliers method. The next part is proving the same holds in the case of multivariate distributions. Generalize the previous part to a $k$ dimensional variable $X$ with given expectation value $\vec{\mu}$ and covariance matrix $\Sigma$ . I started the same way when I define the proper functional I wish to optimize: $$ F\left[f_{X}\left(\overline{x}\right)\right]=H\left(X\right)+\lambda\left(1-\intop_{\mathbb{R}^{k}}f_{X}\left(\overline{x}\right)d\overline{x}\right)+\sum_{i\in\left[k\right]}\varGamma_{i}\left(\mu_{i}-\intop_{\mathbb{R}^{k}}\overline{x}_{i}f_{X}\left(\overline{x}\right)d\overline{x}\right)+\sum_{i,j\in\left[k\right]}\Lambda_{ij}\left(\Sigma_{ij}-\intop_{\mathbb{R}^{k}}\left(\mu_{i}-\overline{x}_{i}\right)\left(\mu_{j}-\overline{x}_{j}\right)f_{X}\left(\overline{x}\right)d\overline{x}\right)$$ After taking the functional derivative with regard to $f_X(\overline{x})$ and extracting the PDF, I get the following term with the Lagrange multipliers: $$f_{X}\left(\overline{x}\right)=\exp\left(\lambda-1\right)\exp\left(-\Gamma\cdot\overline{x}-\left(\vec{\mu}-\overline{x}\right)^{T}\Lambda\left(\vec{\mu}-\overline{x}\right)\right)$$ Where $\Gamma(k\times 1),\Lambda (k\times k),\lambda(1\times 1)$ are the multipliers.
I wish to show that these multipliers must equal the correct terms for this PDF to be a multivariate Gaussian. For some reason this is where I get stuck, I've tried various algebraic manipulations, but the term $\exp{(-\Gamma\cdot\overline{x})}$ keeps messing up my calculations.
I'm leaving out the constraints themselves since they are written within the optimization term, and they leave me in the mess with $\exp{(-\Gamma\cdot\overline{x})}$ when I try to solve the integrals. I feel like I'm missing something.
Would really appreciate it! Edit: Badly enough the answers for this exercise just say - ""yeah this looks gaussian, so we can find the parameters so it works out"", albeit this isn't strictly a math course, I have a very hard time accepting this answer, so the question is highly relevant. There might be an easier way to solve this via KL divergence, but since the first part of the question was required for the next part I would really like to see this through. Another Edit: I get the calculations needed, my main issue is with the fact that the constraint $$\sum_{i\in\left[k\right]}\varGamma_{i}\left(\mu_{i}-\intop_{\mathbb{R}^{k}}\overline{x}_{i}f_{X}\left(\overline{x}\right)d\overline{x}\right)$$ is not needed, since when solving the equation I can assign $\Gamma=0$ and get the proper results. I would like a rigorous explanation why the mean is determined solely by the 3rd constraint.","['statistics', 'information-theory', 'real-analysis', 'optimization', 'probability']"
4874308,Compute $F(x)$ and $\displaystyle{\lim_{x \to \frac{\pi}{2}}} F(x)$,"Consider the function F defined on $]-\frac{\pi}{2}; \frac{\pi}{2}[$ by $$F(x) = \int_0^x t \tan^2{t} dt$$ Compute $F(x)$ and $\displaystyle{\lim_{x \to \frac{\pi}{2}}} F(x)$ Determine the sign of $F(x)$ My attempt: \begin{align}F(x) &= \int_0^x t \tan^2{t} dt \\
&= [t \tan{t}-t^2]_0^x - \int_0^x \tan{t} dt + \int_0^x t dt \\
&= x \tan{x}-x^2 + \ln{|\cos{x}|} + \frac{x^2}{2} \\
&= x \tan{x} + \ln{|\cos{x}|} - \frac{x^2}{2}\end{align} I don't see how to compute the limit knowing that $\displaystyle{\lim_{x \to \frac{\pi}{2}}}\tan{x}$ does not exist. EDIT: \begin{align}\displaystyle{\lim_{x \to \frac{\pi}{2}}} F(x) &= \displaystyle{\lim_{x \to \frac{\pi}{2}^-}} F(x) \\
&= \displaystyle{\lim_{x \to \frac{\pi}{2}^-}} (x \tan{x} + \ln{|\cos{x}|}) - \displaystyle{\lim_{x \to \frac{\pi}{2}^-}} \frac{x^2}{2} \\
&= \displaystyle{\lim_{x \to \frac{\pi}{2}^-}} \tan{x}(x + \frac{\ln{|\cos{x}|}}{\tan{x}}) - \displaystyle{\lim_{x \to \frac{\pi}{2}^-}} \frac{x^2}{2} \end{align} and $\displaystyle{\lim_{x \to \frac{\pi}{2}^-}}  \frac{\ln{|\cos{x}|}}{\tan{x}} = \displaystyle{\lim_{x \to \frac{\pi}{2}^-}}  \frac{\ln{\cos{x}}}{\tan{x}} = 0$ Thus \begin{align}\displaystyle{\lim_{x \to \frac{\pi}{2}}} F(x)  &= + \infty\end{align} Is it like this?","['integration', 'limits', 'improper-integrals', 'analysis']"
4874349,"What is the total number of words of length $500$ on $\{a,b\}$ such that the letter $""a""$ appears more than $""b""$ ( without Brute force)?","The question : What is the total number of words of length $500$ on $\{a,b\}$ such that the letter "" $a$ "" appears more than "" $b$ ""? $(*)$ We know that the total number of words is $ 2^{500} $ . At first glance it looks like half of them. But there are subsets of words which contain the same number of letters of "" $a$ "" and "" $b$ "" (I assume that if the length were odd, it would be half of $2^{500}$ . I'm trying to approach this question with even length which is countable. For example, when $length = 4$ , we have $""aaaa"", ""aaab"", ""aaba"", ""abaa"", ""baaa""$ which is $5$ out of $2^4$ options. For $length = 6$ , we have "" $aaaaaa$ "", $6$ words with $5$ "" $a$ "" and $1$ "" $b$ "", and ${2\choose 6}$ words with $4$ "" $a$ "" and $2$ "" $b$ "". So in total we have $15 + 6 + 1 = 22$ words out of $2^6$ words. For $length = 8$ , we have "" $aaaaaaaa$ "" , $8$ words with $7$ "" $a$ "" and $1$ "" $b$ "", ${2 \choose 8}$ words with $6$ "" $a$ "" and $2$ "" $b$ "", and so on. Hence we have ${8 \choose 0}$ + ${8 \choose 1}$ + ${8 \choose 2}$ + ${8 \choose 3}$ which reminds me of the Binomial Theorem, $\sum\limits_{k=0}^{3} {8 \choose k}$ Hence, for length $500$ , we have  which is the half of $ \sum\limits_{k=0}^{500} {500 \choose k}  $ , but we know that it's not true, due to $(*)$ The official answer is $ \frac {2^{500} - {500 \choose 250}} {2} $ . After looking at the answer we see they use the complement principle and actually those are the number of words which have the same number of "" $a$ ""and "" $b$ "" and that I forgot actually to handle the case of the number of the middle index which is not considered in both cases. But I want to know how to solve combinatorial questions without using Brute force which can waste a lot of time during exams (as you see, this way causes mistakes easily)","['combinatorics-on-words', 'binomial-coefficients', 'combinatorics', 'discrete-mathematics', 'binomial-theorem']"
4874353,Prove $XX’+ZZ’=YY’$,"Let the feet of angle bisectors of triangle $ABC$ are $X,Y$ and $Z$ . The circumcircle of triangle $XYZ$ cuts off three segment from lines $AB,BC $ and $CA$ , let it be $XX’, YY’, ZZ’$ . Prove that at least one combination exists such that sum of two segment's length is equal to third segment’s length. WLOG, I assumed $YY’\geq XX’\geq ZZ’$ , So we have to prove $YY’=XX’+ZZ’$ I tried it using coordinate geometry assuming $A,B,C$ to be $(x_i,y_i)$ for $i=1,2,3$ . Then we can find coordinates of $X,Y,Z$ and hence the equation of circumcircle of $\Delta XYZ$ . And then intercept with three sides. But it will consume a whole to day If a human follows this method without computer help. Can someone please help me finding a good solution for this problem?","['contest-math', 'coordinate-systems', 'circles', 'geometry', 'triangles']"
4874382,What is $\sqrt{-1}$? circular reasoning defining $i$.,"I am reading complex analysis by Gamelin and I am having trouble understanding the square root function. The principal  branch of $\sqrt{z}$ ( $f_1(z)$ ) is defined as $|z|^{\frac 1 2} e^{\frac{i \operatorname{Arg}(z)}{2}}$ for $z \in \mathbb{C} - (-\infty,0]$ and $f_2(z)$ is defined as $-f_1(z)$ where $\operatorname{Arg}(z) \in (-\pi , \pi]$ By this definition, what is $\sqrt{-r} $ where $r$ is a non negative real number? Of course the answer is $i\sqrt{r}$ but the definition of square root functions doesn't apply here What is $i$ then? $i:=\sqrt{-1}$ but how? We didn't define the square root function for negatives but we still use $i$ to define complex numbers. Shouldn't the definition of square root function taught before defining $i$ ? and defining $i^2=-1$ without   define $i$ as either $\pm \sqrt{-1}$ is also very strange because we want  extended function to  be continuous and choosing $\pm \sqrt{-1}$ will make this impossible although $i$ must be one of the two (after defining the square root of negatives). Why do we even care so much about continuity? (I am sure there is an answer is the rest of the book but I am still a beginner), all of this confusion will disappear if we just made $i:= \sqrt{-1}$ , we seem all to agree to use $i:= \sqrt{-1}$ in solving problems involving complex numbers,even wolfram alpha uses that and every high school math book and some engineering math books (I know the last know types of books lacks rigorousness in general but what I am saying is is seems normal to just define $i$ that way  ), so why all of that confusion? I asked some of my graduate friends and all of them told me the they just define $i:=\sqrt{-1}$ without any of these complications. Now I am more confused, do we define $i:=\sqrt{-1}$ or not? If we do what did I get wrong from the definition of principal root?","['riemann-surfaces', 'complex-analysis', 'continuity', 'algebra-precalculus', 'complex-numbers']"
4874387,"Probability that $b^2 - 4ac \geq 0$ where $a,b,c$ are normally distributed (numerical integration)","I would like to determine the probability that a random quadratic polynomial has positive discriminant, where the 3 coefficients $a, b, c$ are normally distributed and independent: That is, given $a,b,c \sim \operatorname{N}\left(0,1\right)$ , what is ( a numerical approximation to 5 digits of ) $\operatorname{Pr}\left(\,{b^2 - 4 a c \geq 0}\,\right)$ ? Thoughts : We have $$
\mathrm{Pr}\left(\,{b^2 - 4 a c \geq 0}\,\right)
=
\dfrac{1}{\left(\sqrt{\pi}\right)^3} \iiint_{\mathbb{R}^3} \mathbb{\large 1}_{b^{2}\ -\ 4ac\ \geq\ 0}\quad{\rm e}^{-a^2}
{\rm e}^{-b^2}{\rm e}^{-c^2}\,\mathrm{d}a \,\mathrm{d}b \,\mathrm{d}c
$$ This integral probably cannot be expressed explicitly, but even a numerical approximation is not so easy. Here is what I tried with SAGE , without success: var('a,b,c') 
RR = RealField(100)
 
I = integrate(integrate(integrate(exp(-a^2), a, 0, b^2/(4*c)) * exp(-c^2),    c, 0, oo) * exp(-b^2),    b, 0, oo) print(RR(I)) #error... Expermenting with SAGE seems to give a probability between 0.64 and 0.65 (compare the values given below if $a,b,c$ are uniformly distributed): N = 0
T = 10^5
for _ in range(T):
    a = gauss(0, 1)
    b = gauss(0, 1)
    c = gauss(0, 1)
    if b^2 - 4*a*c >= 0:
        N += 1

print(float(N/T)) Other comments : The idea to consider the normal distribution is that the vector $(a, b, c) / \sqrt{a^2 + b^2 + c^2}$ is uniformly distributed on the sphere, because the joint Gaussian distribution is spherically symmetric, and the property $b^2 - 4 a c \geq 0$ is invariant under rescaling. Note that when $a,b,c \sim \mathrm{Unif}(-N, N)$ for some $N > 0$ , the probability $\mathrm{Pr}(b^2 - 4 a c \geq 0)$ can be computed explicitly [1] : it is $$(41 + \log(64))/72 \approx 0.627207.$$ See also Probability that a quadratic polynomial with random coefficients has real roots for the case $a,b,c \sim \mathrm{Unif}(0, 1)$ .","['integration', 'discriminant', 'normal-distribution', 'numerical-methods', 'probability']"
4874418,On the definition of quadratic forms,"Let $\mathbb{F}$ be an arbitrary field of characteristic $\mathrm{char}(\mathbb{F})\neq 2$ and $V$ a $\mathbb{F}$ -vector space. The definition of a quadratic form I am used to is a map $\varphi\colon V\to\mathbb{F}$ with the property $\varphi(\lambda v)=\lambda^{2}\varphi(v)$ such that $$\beta_{\varphi}(v,w):=\frac{1}{2}(\varphi(v+w)-\varphi(v)-\varphi(w))$$ is bilinear. Now, clearly, $\beta_{\varphi}$ is symmetric and $\beta_{\varphi}(v,v)=\varphi(v)$ , by definition. In the case $\mathbb{F}=\mathbb{R}$ , I have seen the following claim: Proposition 1 : If $\varphi\colon V\to\mathbb{R}$ with the property $\varphi(\lambda v)=\lambda^{2}\varphi(v)$ satisfies the paralellogram law $$\varphi(v+w)+\varphi(v-w)=2\varphi(v)+2\varphi(w)$$ then it is a quadratic form, i.e. there exists a symmetric bilinear form $\beta_{\varphi}$ such that $\beta_{\varphi}(v,v)=\varphi(v)$ . This is essentially a similar statement as the Jordan-von Neumann theorem, which asserts that a norm $\Vert\cdot\Vert$ on $V$ admits a inner product $\langle\cdot,\cdot\rangle$ inducing it if and only if the paralellogram law $$\Vert v+w\Vert^{2}+\Vert v-w\Vert^{2}=2\Vert v\Vert^{2}+2\Vert w\Vert^{2}$$ holds. Now, my question is the following: Is Proposition 1 also true for arbitrary fields (excluding $\mathrm{char}(\mathbb{F})=2$ of course)? I was not able to find a proof of this statement. I tried to mimik the proof of Jordan-von Neumann, but all the proofs I know of this result are somehow specific to $\mathbb{R}$ . The idea of the proof is usually to define $$\beta_{\varphi}(v,w):=\frac{1}{2}(\varphi(v+w)-\varphi(v)-\varphi(w))$$ Then clearly $\beta_{\varphi}(v,v)=\varphi(v)$ and $\beta_{\varphi}$ is symmetric. The only thing to show is that $\beta_{\varphi}$ actually defines a bilinear form. Any literature on this is appreciated.","['inner-products', 'reference-request', 'abstract-algebra', 'linear-algebra', 'quadratic-forms']"
4874446,"How to integrate $\int_0^1\frac{\ln^3(1+x)\,\ln^3x}x\mathrm{d}x$","How to integrate $$\displaystyle{\Large\displaystyle \int_0^1\frac{\ln^3(1+x)\,\ln^3x}x\mathrm{d}x}$$ My idea \begin{align*}
\int_{0}^{1} \frac{\ln^3 (1+x) \ln^3 x}{x} \, \mathrm{d}x &= -18 \sum_{n=1}^{\infty} \left ( -1 \right )^{n+1} \left ( \mathcal{H}_{n+1}^2 - \mathcal{H}_{n+1}^{(2)} \right ) \frac{1}{\left (n+2  \right )^4}  \\
&= 18 \sum_{n=1}^{\infty} (-1)^n \left ( \mathcal{H}_{n+1}^2 - \mathcal{H}_{n+1}^{(2)} \right ) \frac{1}{\left ( n+2 \right )^4} \\
&= 18 \sum_{n=2}^{\infty} (-1)^{n-1} \left ( \mathcal{H}_{n}^2 -  \mathcal{H}_{n+1}^{(2)} \right ) \frac{1}{\left ( n+1 \right )^4}
\end{align*}","['integration', 'definite-integrals', 'harmonic-functions', 'euler-sums', 'calculus']"
4874470,Arranging Drilled Unit Cubes into a Rectangular Prism Without Breaking the Thread,"Given positive integers p , q , and r , we have $p \cdot q \cdot r$ unit cubes. Each cube has a hole drilled along one of its space diagonals. These cubes are then strung onto a very thin thread of length $p \cdot q \cdot r \cdot \sqrt{3}$ , resembling beads on a string. The task is to arrange these unit cubes into a rectangular prism with side lengths $p$ , $q$ , and $r$ without breaking the thread. The problem can be divided into two parts: a) For which values of $p$ , $q$ , and $r$ is it possible to arrange the cubes into the prism without breaking the thread? b) For which values of $p$ , $q$ , and $r$ can this arrangement be done in such a way that the beginning and end of the thread come together? I have attempted to visualize and manipulate various configurations but have yet to devise a systematic approach to solve this problem. Does anyone have ideas on how to tackle this problem, or are there any similar problems or theories that might shed light on a possible solution? Any guidance or references to relevant mathematical concepts would be greatly appreciated.","['combinatorics', 'hamiltonian-path']"
4874498,"Dual space of $L^p([0,1])$ for $0 < p < 1$.","I convinced myself that in the space $L^p([0,1])$ there is no other non-empty, open, convex subset different from $L^p([0,1])$ itself when $0 < p < 1$ . I was told that I can conclude from this statement that the topological dual of $L^p([0,1])$ if $\{0\}$ , but I can not see it. I suppose that Hahn-Banach theorem would be useful here but I am not sure how. Thanks in advance.","['functional-analysis', 'dual-spaces']"
4874546,Sequence of non-extreme digits of power sequence must be uniformely distributed,"Let $a>1$ be an integer. I wish to analyze the digits of the power sequence $(a^n)_n$ . The behave of extreme digits can be settle to the following: The last $k$ digits of $a^n$ are given by $a^n\pmod{10^k}$ , therefore the sequence of the $k$ last digits is eventually periodic and does not depend on subsequent digits. The first $k$ digits of $a^n$ are given by the number $s = \overline{(s_1s_2\dots s_k)}_{10}$ if, and only if, there is an integer $t$ such that $$\begin{alignat}{3}
&10^t(s+1)& &>a^n& & &\ge 10^ts\\\iff
&t+\log_{10}(s+1)& &>n\log_{10}(a)& & &\ge t+\log_{10}(s)\\\iff
&\log_{10}(s+1)& &>\{n\log_{10}(a)\}~& & &\ge \log_{10}(s)
\end{alignat}$$ Now the Equidistribution Theorem says that $(\{n\log_{10}(a)\})_n$ is equidistributed over $[0, 1]$ as long as $\log_{10}(a)$ is irrational (i.e., as long as $a$ is not a power of $10$ ), therefore $s$ occurs as the first set of digits of $a^n$ with probability $\log_{10}(s+1)-\log_{10}(s)$ , or $\displaystyle\log_{10}\left(1+\frac1s\right)$ . This not only says every string of digits occur as the first set of digits of $a^n$ for some $n$ but also says power sequences follows Benford's Law . But what about the digits in between? The study of first and last digits require very specific tools which seem not easy to adapt or combine to adresse this particular case. More precisely, I want to know the limit distribution of the digits which are not a fixed number of positions from the right or from the left of a power sequence. Is the sequence it periodic? Must every digit appear in it? What is the frequency of these digits? In an attempt to develop intuition I recurred to this Python script, which plots the frequency of each digit of $2^n$ in positions $\left\lfloor\dfrac 15m\right\rfloor, \left\lfloor\dfrac 25m\right\rfloor, \left\lfloor\dfrac 35m\right\rfloor, \left\lfloor\dfrac 45m\right\rfloor$ , where $m$ is the total number of digits. import numpy as np
import matplotlib.pyplot as plt
import sys

# increases limit for integer-string conversion
sys.set_int_max_str_digits(0)

freq1 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
freq2 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
freq3 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
freq4 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]

a = 2 # power basis
for n in range(10000):
    p = str(a**n) # power
    m = len(p)    # number of digits
    freq1[int(p[int(0.2*m)])]+=1
    freq2[int(p[int(0.4*m)])]+=1
    freq3[int(p[int(0.6*m)])]+=1
    freq4[int(p[int(0.8*m)])]+=1 The code produces the following graphs. The distribution seems to be approaching uniformity. The same behavior was observed for several other basis. This is my conjecture: Conjecture. Given an integer $a>1$ not divisible by $10$ , a digit $d$ and $\lambda\in(0,1)$ , we have $$\lim\frac{\#(\{n:\text{ the }\lfloor\lambda m(a^n)\rfloor\text{th digit of }a^n\text{ is }d\}\cap[1, N])}{N} = \frac1{10}.$$ where $m(x)\equiv\lfloor\log_{10} x\rfloor$ is the amount of digits of $x$ . Of course, I have no reason to believe this only applies to base $10$ .","['analytic-number-theory', 'number-theory', 'statistics', 'probability-distributions']"
4874553,"If there is a bijection $F : A \mapsto A / R$, then $R = \{(x, y) \in A^2 : x = y\}$?","Assume $R$ is an equivalence relation over $A$ and there is a bijection between $A$ and $A / R$ . Does this entail $R = \left\{ (x, y ) \in A^2  : x = y \right\} $ ? What I thought is the following. Assume $A$ is an infinite set. Then we can find a counter-example. For example, if $A = \mathbb{N}$ and $R$ the equivalence relation s.t. $A / R$ is the partition $$\left\{ \left\{ 1 \right\}, \left\{
    2, 3\right\}, \left\{ 4, 5, 6 \right\}, \left\{ 7, 8, 9, 10 \right\}, \ldots
\right\} $$ Then $F(1) = \left\{ 1 \right\} , F(2) = \left\{ 2, 3 \right\}, F(3) =
    \left\{ 4, 5, 6 \right\}, \ldots  $ is a bijection. However, if $A = \left\{ a_1, \ldots, a_n \right\} $ a finite set, and $F$ is bijective, we must have \begin{align*}
F(a_1) = X_1, \ldots, F(a_{n}) = X_{n}
\end{align*} with $X_i \neq X_j$ for $i, j \in [1, n]$ . This entails $|A / R| = |A|$ , which implies $A / R$ is a partition of $A$ into singleton sets. In other words, $$A / R = \left\{ \left\{ a_1 \right\}, \ldots, \left\{ a_n\right\}   \right\} \Rightarrow R = \left\{ (x, y) \in A^2 : x = y \right\} $$ So, the answer to the question seems to differ with regards to whether $A$ is finite or infinite. Is this correct?","['elementary-set-theory', 'equivalence-relations']"
4874557,Obtaining a connection on a trivial bundle by giving a matrix of $1$-forms,"I'm new to connections and I'm going over the page ( https://mathworld.wolfram.com/VectorBundleConnection.html ) in which they state the following For example, the trivial bundle $E=M\times \Bbb R^k$ admits a flat connection since any bundle section $s$ corresponds to a function $s:M\to \Bbb R^k$ . Then setting $\nabla s=ds$ gives the connection. Any connection on the trivial bundle is of the form $\nabla s=ds+s \otimes \alpha$ , where $\alpha$ is any one-form with values in $\text{Hom}(E,E)=E^*\otimes E$ , i.e., $\alpha$ is a matrix of one-forms. I understand from here that if $s: M \to E= M\times \Bbb R^k$ is a section of $E$ , then this is identifiable with a map $s :M \to \Bbb R^k$ . Now $ds$ is a bit ambiguous, but if I'm not mistaken this is under the identification $s=(s_1,\dots,s_k)$ with $s_i : M \to \Bbb R$ just $$ds=(ds_1,\dots,ds_k).$$ The second part of the paragraph is what confuses me a bit, they state that any connection on a trivial bundle is of the form $$\nabla s=ds +s\otimes \alpha$$ for $\alpha \in \Gamma(T^*M\otimes \text{Hom}(E,E))$ . Where is this coming from, why is any connection given by just giving a matrix of $1$ -forms? For example on the trivial bundle $M \times \Bbb R^2$ over $M$ , will something like $$\alpha = \begin{bmatrix}dx&0\\ 0&dy\end{bmatrix}$$ immediately give me a connection on $M \times \Bbb R^2$ ? A follow up to this would be that on a smooth manifold with a rank $k$ vector bundle $E \to M$ , given a trivializing open cover $\{U_i\}$ , can I define connections $\nabla_i$ on $E|_{U_i} \cong U_i \times \Bbb R^k$ and patch these together to get a connection on $E$ even if it isn't trivial itself?","['connections', 'vector-bundles', 'differential-geometry']"
4874573,Solving PDEs as ODEs under certain conditions,"(Before closing this question, yes, I've read this one , but my question is not about separation of variables). I'm currently studying second order PDEs with variable coefficients, and after reducing them to their canonical form, my professor solves them as if they were ODEs (except, of course, for the elliptical case, which I still don't know how to solve, although I reckon has something to do with Laplace's equation in 2 dimensions). I don't quite understand why we are allowed to do this. The only difference is the integration constants are now functions of the other variable. For instance, consider the equation: $$2u_{xx}+5yu_x+2y^2u=y$$ It's already reduced to its canonical parabolic form, and I've been taught to solve it by treating $y$ as a constant, which therefore leaves me with a constant coefficient ODE with the general solution being: $$u(x,y)=\frac{1}{2y}+C_1(y)e^{-xy/2}+C_2(y)e^{-2xy}$$ Again, why are we allowed to solve the PDE this way? Is there any theorem, proposition or some text I can refer to to address my doubt? To the obvious question, ""why couldn't we?"", I can give a couple of answers (which are all wrong, since we can do this): The coefficients $C_i$ could depend, for instance, on $y+a, \ \ a\in \mathbb{R}$ . This wouldn't affect the derivatives, but it could affect how the coefficients work, unless they are invariant under translation, which should be demonstrated Partial derivatives don't always behave as ordinary derivatives (e.g: the reciprocity relation), so we can't consider it trivial to treat them as such As always, thanks in advance for your kind responses.","['ordinary-differential-equations', 'partial-differential-equations']"
4874579,"Can $({\Bbb N}, \max)$ be topologized to be a compact Hausdorff monoid?","I am aware of this question and this one , the answers to which show that the natural numbers can be equipped with a compact Hausdorff topology. But what happens if one also requires the operation $$
\max: {\Bbb N} \times {\Bbb N} \to {\Bbb N}
$$ to be continuous? My intuition is that it is not possible because the profinite approach fails. More precisely, for each natural $t$ , let ${\Bbb N}_t = \{0, 1, \ldots, t \}$ . Then $({\Bbb N}_t, \max)$ is a finite (and hence compact Hausdorff for the discrete topology) monoid. Furthermore, the map $$
f_t: ({\Bbb N}, \max) \to ({\Bbb N}_t, \max) \text{ defined by } f_t(x) = \min(x, t)
$$ is a monoid morphism. Thus one can embed $({\Bbb N}, \max)$ into the compact Hausdorff product monoid $\prod_{t \in {\Bbb N}}({\Bbb N}_t, \max)$ by identifying $n$ with $(0, 1, 2, \ldots, n, n, n, \ldots)$ . Unfortunately, $({\Bbb N}, \max)$ is not closed for this topology, since the sequence $u_n = (0, 1, 2, \ldots, n, n, n, \ldots)$ converges to $(0, 1, 2, \ldots, n, n+1, n+2, \ldots)$ . But perhaps some other topology would work... I also tried to use the fact that a countable compact Hausdorff space contains at least one isolated point, but it does not seem to help. I am probably missing some elementary argument, and help would be welcome.","['monoid', 'general-topology', 'compactness']"
4874659,Ultraweak Continuity Implies Norm Continuity,"I was reading the section on Schur Multipliers in Ozawa's book ""C*-algebras and Finite-Dimensional Approximations"" and I am having troubles understanding the proof of Proposition D.6, namely the part about ultraweak continuity. I think I can skip most of the context and I am stuck on a particular part, so I'll write it differently. Let $H$ be a Hilbert (in the problem we have $H=\ell^2(\Gamma)$ ) and let $T: B(H) \rightarrow B(H)$ be an ultraweak continuous linear operator on $H$ , moreover, we know that the restriction $T|_{K(H)}:K(H) \rightarrow K(H) $ is well defined and not only ultraweak continuous, but also norm continuous. To my understanding of the proof in the book, this implies that $T$ is also norm continuous on the whole $B(H)$ , however I am not being able to prove this. I tried using the fact that trace class operators (predual of $B(H)$ ) are compact, but did not reach any relevant conclusion, moreover I am almost sure the Uniform Boundness Theorem is required. If this affirmation is false and more context is needed, it can be found in the mentioned book, Proposition D.6.","['c-star-algebras', 'operator-algebras', 'hilbert-spaces', 'completely-positive-maps', 'functional-analysis']"
4874671,"Showing $\frac16 \tan^6x +\frac18\tan^8x = \frac14 \cos^{-4}x -\frac13 \cos^{-6}x + \frac18\cos^{-8}x+c$ for some real $c$, without calculus","While teaching my calculus class I happened upon a strange identity. Consider the antiderivative $$\int \tan^5 x \sec^4 x dx.$$ You can do that by writing the integrand as $\tan^5 x (1+\tan^2 x) \sec^2x$ , then substituting $u=\tan x$ . This gives one answer. But another way to do it is to rewrite the integrand as $\sin^5 x \cos^{-9} x = \sin x (1-\cos^2 x)^2 \cos^{-9}x$ , then apply a substitution $u=\cos x$ . If you follow through with both of these methods, it seems to give the strange identity $$\frac16 \tan^6x +\frac18\tan^8x = \frac14 \cos^{-4}x -\frac13 \cos^{-6}x + \frac18\cos^{-8}x+c,$$ for some $c\in \Bbb R$ . I didn't believe it at first, but a student checked it on desmos and they seem to agree. Questions: Is there a simple way to see that without calculus? And is there some general way to generate these kinds of seemingly nontrivial trig identities, where some function of tangent equals some other function of sine/cosine? I thought maybe it could be related to some orthogonal polynomial families like Chebyshev polynomials etc, but it's unclear.","['integration', 'calculus', 'trigonometry']"
4874689,"In the set $P$ of all circles over $\mathbb{R}^2$, ordered by $(P, \subseteq)$, do two circles always have an infimum?","I was aked to determine whether the following statement is true or false. Let $$\mathcal{D} \left( (x_0, y_0), r \right) = \left\{ (x, y) \in \mathbb{R}^2 : (x - x_0)^2 + (y - y_0)^2 \leq r^2 \right\} $$ Let $P = \left\{ \emptyset \right\} \cup \left\{ \mathcal{D}\left( (x_0,y_0), r \right) : x_0, y_0 \in \mathbb{R}, r > 0  \right\} $ . In the poset $(P, \subseteq ) $ , there is always $\text{inf}\left\{ D_1, D_2\right\} $ , for any $D_1, D_2 \in P$ . I think I came to the right conclusion but I'm thinking one could be more rigorous than I was. Here is what I attempted. Consider that $\mathcal{D}\left( (x_0, y_0), r \right)$ is the set of points within a circumference with center $(x_0, y_0)$ and radius $r$ . So we can think of $P$ as the set of all (filled) circles, including $\emptyset$ . Two circles may be related in one of the ways schematized by the following Venn diagrams: Formally, for $D_1, D_2 \in P$ , the image depicts the following exhaustive and
mutually exclusive cases: $D_1 \subseteq  D_2$ $ D_1 \cap D_2 \neq \emptyset$ but $D_1 \not\subseteq D_2$ $D_1 \cap D_2 = \emptyset$ . (1) If $D_1 \subseteq  D_2$ then evidently $\text{inf}\left\{ D_1, D_2 \right\} =
D_1$ , because $D_1$ would be the greatest circle that is a subset of itself and
of $D_2$ . (2) If $D_1 \cap D_2 = \emptyset$ , evidently $\text{inf}\left\{ D_1,
D_2 \right\} = \emptyset$ ,. (3) The case $D_1 \cap D_2 \neq \emptyset$ with $D_1 \not\subseteq D_2$ is the one that ruins the beauty. Let $D_3$ a circle s.t. $D_3 \subseteq D_1
\cap D_3$ ---this is, $D_3$ is an arbitrary, non-empty lower bound of $\left\{
D_1, D_2 \right\} $ . Observe that we can choose any arbitrary point $(z_1, z_2) \not\in D_3$ that
lies in $D_1 \cap D_2$ , and make $D_z = \mathcal{D}((z_1, z_2), \epsilon)$ , with $\epsilon > 0$ a quantity sufficiently small to guarantee $D_z \cap D_3 =
\emptyset$ and $D_z \in D_1 \cap D_2$ . It is evident that $D_z \subseteq D_1$ and $D_z \subseteq D_2$ , so $D_z$ is a lower bound of $\left\{ D_1, D_2 \right\}
$ ; but since $D_z \not\subseteq D_3$ we cannot say $D_3$ is the greatest lower
bound. The argument above holds for any $D_3 \subseteq D_1 \cap D_2$ . In general terms, we
have shown that, in the case $D_1 \cap D_2 \neq \emptyset, D_1 \not\subseteq D_2$ , for any lower bound $D_3$ of $\left\{ D_1, D_2 \right\} $ , we can
find a lower bound $D_z$ that is not a subset of $D_3$ . Therefore no
greater lower bound exists and there is no infimum. Two questions arise. The most fundamental: Is this proof correct? But also, was there a simpler way to tackle the problem?","['elementary-set-theory', 'logic']"
4874740,On which natural numbers do we have injectivity of Euler's totient function?,"I was wondering about the Euler's function and I would like to know which are all the natural numbers $m$ such that: $$\phi (m) \neq \phi (n) \forall n \in \mathbb N\setminus \{m\}$$ After putting some thought into it: $m$ cannot be odd, because $\phi(m) = \phi(2m)$ . $m$ cannot be twice an odd number for the previous reason. So no prime numbers. $m$ must be a multiple of $4$ , but which ones? We know that $$ m = p_1^{\alpha_1} \cdot ... \cdot p_n^{\alpha_n} \implies \phi(m) = p_1^{\alpha_1-1} \cdot ... \cdot p_n^{\alpha_n-1}(p_1-1)\cdot...\cdot(p_n-1) $$ EDIT: I think I got something, $m$ must be a composite number, let's say it is $(4p) \cdot q$ with $gcd(4p,q)=1$ , so we have that $$\phi (m) = \phi(4p) \cdot \phi (q)$$ but we know for a fact that there is some $q_2 \in \mathbb N$ such that $\phi (q_2) = \phi (q)$ if we could know for sure that $\gcd(q_2,4p)=1$ , then we would have that $\phi$ is never injective for any natural number, as $\phi (4p \cdot q)$ would be the same as $\phi (4p \cdot q_2)$ .","['elementary-number-theory', 'functions']"
4874746,Kernel of the action of GL(V) on exterior square of V,"I wonder whether anyone knows a reference for the following result? I can give a shortish proof, but would prefer to cite the literature if possible. Theorem Let $V=F^n$ be an $n$ -dimensional $F$ -space where $F$ is a field.
The action of ${\rm GL}(V)$ on $\Lambda^2(V)$ induces a homomorphism $\phi_n\colon{\rm GL}_n(F)\to{\rm GL}_d(F)$ of matrix groups where $d=\binom{n}{2}$ .
The kernel of $\phi_n$ is ${\rm GL}_1(F)$ if $n=1$ , ${\rm SL}_2(F)$ if $n=2$ , and $\langle -I_n\rangle$ if $n\ge3$ . I will give a proof if there is no easy reference and one is requested.","['group-theory', 'multilinear-algebra', 'exterior-algebra', 'reference-request']"
4874777,"does there exist $T\in GL(n,\mathbb{C})$ so that $\sigma(g) = T^{-1} \rho(g)T$ for all $g\in G$?","Suppose that $\rho$ and $\sigma$ are degree $n$ irreducible representations of a group $G$ over $\mathbb{C}$ and that for every $g\in G,$ there is a matrix $T_g\in GL(n,\mathbb{C})$ depending on $g$ so that $\sigma(g) = T_g^{-1} \rho(g) T_g$ . Does there exist $T\in GL(n,\mathbb{C})$ so that $\sigma(g) = T^{-1} \rho(g)T$ for all $g\in G$ ? I think the answer is no, but I'm not sure how to come up with a counterexample. The question basically asks if $\sigma$ and $\rho$ are necessarily equivalent representations. As a first step, it seems reasonable to find a specific pair of degree n irreducible representations satisfying the constraints in the question, but I'm not sure how to do so. One irreducible representation that could be worth considering is the sign representation of $S_n$ .","['representation-theory', 'irreducible-representation', 'abstract-algebra', 'linear-algebra', 'group-theory']"
4874822,Is every smooth manifold the solution set of finitely many equations?,In the case of affine varieties we have finitely many (polynomial) equations defining the variety. It is a (smooth) manifold iff it satisfies the Jacobian criterion. I wonder whether this can be generalized in the following sense: Is it possible for every smooth submanifold $M\subseteq \mathbb{R}^n$ to find a smooth map $f: \mathbb{R}^k \rightarrow \mathbb{R}^l$ and a regular value $q\in \mathbb{R}^l$ of $f$ such that $M=f^{-1}(q)$?,['smooth-manifolds']
4874825,Is it really true that $\mathcal{S}(\mathbb{R}^n)$ is identified with smooth functions on $S^n$ vanishing at a fixed point?,"Let $\mathcal{S}(\mathbb{R}^n)$ be the Schwartz space on $\mathbb{R}^n$ and $C^\infty(S^n)$ be the space of smooth functions on $n$ -sphere. Now fix a point $x \in S^n$ and define \begin{equation}
C^\infty_x(S^n) := \{ f \in C^\infty(S^n) \mid f(x)=0\}
\end{equation} Then I heard that $\mathcal{S}(\mathbb{R}^n)$ may be identified with $C^\infty_x(S^n)$ . That is, a Schwartz function is one extending smoothly to $S^n$ while vanishing at infinity. However, I wonder why we need such a strong decay condition of Schwartz functions to extend it smoothly to $S^n$ ? Could anyone please clarify for me?","['smooth-functions', 'smooth-manifolds', 'functional-analysis', 'compact-manifolds', 'schwartz-space']"
4874828,Solving $p\sin^{4}{\theta}-q\sin^{4}{\phi}=p$ and $p\cos^{4}{\theta}-q\cos^{4}{\phi}=q$ for $\theta$ and $\phi$,"Given $$p\sin^{4}{\theta}-q\sin^{4}{\phi}=p$$ and $$p\cos^{4}{\theta}-q\cos^{4}{\phi}=q$$ find $\theta$ and $\phi$ . Here is my solution (help improving it would be much appreciated): $$p(1-2\cos^{2}{\theta}+\cos^{4}{\theta})-q(1-2\cos^{2}{\phi}+\cos^{4}{\phi})=p \tag1$$ Subtracting this from the original second equation gives $$p-2p\cos^{2}{\theta}-q+2q\cos^{2}{\phi}=p-q \tag2$$ Rearranging: $$\cos^{2}{\phi}=\frac{p}{q}\cos^{2}{\theta} \tag3$$ Which means $$\cos^{4}{\phi}=\frac{p^{2}}{q^{2}}\cos^{4}{\theta} \tag4$$ When substituted into the second original equation we get $$\left(p-\frac{p^{2}}{q}\right)\cos^{4}{\theta}=q \tag5$$ From this, $$\cos^{4}{\theta}=\frac{q^{2}}{qp-p^{2}} \tag6$$ and $$\cos^{4}{\phi}=\frac{p^{2}}{qp-p^{2}} \tag7$$ BUT... is it possible to obtain ""nicer"" expressions for $\theta$ and $\phi$ somehow?","['alternative-proof', 'trigonometry', 'solution-verification']"
4874834,Mistake computing $\sum_{n=1}^{+\infty} \frac{n}{e^{2\pi n}-1} = \frac{1}{24}-\frac{1}{8\pi}$,"I recently gave a try to show that $$\sum_{n=1}^{+\infty} \frac{n}{e^{2\pi n}-1}=\frac{1}{24}-\frac{1}{8\pi} $$ without using the Theta function or Mellin transform, but I ended up with twice the result and I can't figure out where my mistake was.
For my approach, I need to introduce some Lemmas to help the proof: (1): $\forall z \in \mathbb{R}^*, \frac{\pi z}{\sinh(\pi z)}=\int_{0}^{\infty} \frac{\cos(2zx)}{\cosh^2(x)}dx$ (2): $\forall z \in \mathbb{R} , \sum_{n=1}^{+\infty} \cos(2nz)e^{-n\pi} = \frac{\cos(2z)-e^{-\pi}}{2(\cosh(\pi)-\cos(2z))}$ (3): $\forall z \in \mathbb{R} , \frac{\sinh(\pi)}{4(\cosh(\pi)-\cos(2z))}=\frac{\pi}{2(\pi^2+4z^2)}+\int_{0}^{\infty} \frac{\sin(\pi t)\cosh(2zt)}{e^{2\pi t}-1} dt $ (4): $ \forall z \in \mathbb{R} , \frac{\pi}{\pi^2+4z^2}=\int_{0}^{\infty} \cos(2zu)e^{-\pi u} du $ (5): $ \frac{\pi^2}{6}=\int_{0}^{\infty} \frac{w}{e^w-1}dw$ Here is my approach: $$\begin{align}
I&=\sum_{n=1}^{+\infty} \frac{n}{e^{2\pi n}-1}\\
&=\frac{1}{2}\sum_{n=1}^{+\infty} \frac{n}{\sinh(\pi n)}e^{-\pi n}\\
&=\frac{1}{2\pi}\sum_{n=1}^{+\infty} \frac{\pi n}{\sinh(\pi n)}e^{-\pi n}
\end{align}
$$ Then using Lemmas (1) and (2) we get: $$
\begin{align}
I&=\frac{1}{2\pi}\sum_{n=1}^{+\infty}e^{-\pi n}\int_{0}^{\infty}\frac{\cos(2nx)}{\cosh^2(x)}dx\\
&=\frac{1}{2\pi}\int_{0}^{\infty}\sum_{n=1}^{+\infty}\frac{\cos(2nx)}{\cosh^2(x)}e^{-\pi n}dx\\
&=\frac{1}{4\pi}\int_{0}^{\infty}\frac{\cos(2x)-e^{-\pi}}{\cosh(\pi)-\cos(2x)}  \frac{dx}{\cosh^2(x)}\\
\end{align}
$$ From here, we can remove the cosine function in the numerator: $I=\frac{\cosh(\pi)-e^{-\pi}}{4\pi}\int_{0}^{\infty}\frac{1}{\cosh(\pi)-\cos(2x)}  \frac{dx}{\cosh^2(x)}-\frac{1}{4\pi}\int_{0}^{\infty}\frac{dx}{\cosh^2(x)}=\frac{\sinh(\pi)}{4\pi}\int_{0}^{\infty}\frac{1}{\cosh(\pi)-\cos(2x)}  \frac{dx}{\cosh^2(x)}-\frac{1}{4\pi}$ Using Lemma (3), then (1) on $\cosh(2xt)=\cos(2ixt)$ : $I=\frac{1}{\pi}\int_{0}^{\infty}\int_{0}^{\infty}\frac{\sin(\pi t)}{e^{2\pi t}-1}\frac{\cosh(2xt)}{\cosh^2(x)} dxdt+\frac{1}{2}\int_{0}^{\infty}\frac{1}{\pi^2+4x^2}\frac{dx}{\cosh^2(x)}-\frac{1}{4\pi}=\int_{0}^{\infty}\frac{t}{e^{2\pi t}-1}dt+\frac{1}{2}\int_{0}^{\infty}\frac{1}{\pi^2+4x^2}\frac{dx}{\cosh^2(x)}-\frac{1}{4\pi}$ $=\frac{1}{4\pi^2}\int_{0}^{\infty}\frac{t}{e^t-1}dt+\frac{1}{2}\int_{0}^{\infty}\frac{1}{\pi^2+4x^2}\frac{dx}{\cosh^2(x)}-\frac{1}{4\pi}=\frac{1}{24}-\frac{1}{4\pi}+
\underbrace{\frac{1}{2}\int_{0}^{\infty}\frac{1}{\pi^2+4x^2}\frac{dx}{\cosh^2(x)}}_{:=J}$ Let's evaluate the last integral $J$ , with Lemmas (4) and (1) : $$\begin{align}
J&=\frac{1}{2}\int_{0}^{\infty}\frac{1}{\pi^2+4x^2}\frac{dx}{\cosh^2(x)}\\
&=\frac{1}{2\pi}\int_{0}^{\infty}\int_{0}^{\infty}\frac{\cos(2xu)}{\cosh^2(x)} e^{-\pi u} dxdu\\
&=\frac{1}{2}\int_{0}^{\infty}\frac{u}{\sinh(\pi u)} e^{-\pi u}du\\
&=\frac{1}{4\pi^2}\int_{0}^{\infty}\frac{u}{e^u-1}du=\frac{1}{24}
\end{align}$$ Finally, $I=\frac{1}{12}-\frac{1}{4\pi}$ , which is twice the correct result.","['integration', 'improper-integrals', 'power-series', 'mellin-transform', 'theta-functions']"
4874858,Extracting the leading non-analytic piece of $\int_0^{1/2} \frac{t dx}{x^{-t} - x^t}$ at small real $t$,"Consider the integral $$I(t) = \int_0^{1/2} \frac{t dx}{x^{-t} - x^t}$$ in the vicinity of $t=0$ . When $t$ is viewed as a real variable, $I(t)$ is very well-behaved. For example, it's infinitely differentiable. On the other hand, when $t$ is on the imaginary axis, the integrand is very poorly behaved as a function of $x$ close to zero. The denominator becomes a rapidly oscillating function that repeatedly passes through zero. This is easiest to see by writing $x^{-t} - x^t = 2\sinh(t \log(1/x))$ which is $2i\sin(\Im(t) \log(1/x))$ for imaginary $t$ . Thus $I(t)$ is infinitely-differentiable for real $t$ , but is non-analytic in the complex plane about $t=0$ . Note that I expect that $I(t)$ will be asymptotic to some power series in $t$ for small real $t$ , $I(t) \sim \sum_{n=0}^\infty c_n t^n$ , but that is not quite what I'm looking for. Rather, I'm interested in the leading ""non-analytic"" contribution to $I(t)$ at small real $t$ . For example, one might guess such a piece to go something like $e^{-1/t^2}$ . Is there a systematic way to extract the leading ""non-analytic"" piece of $I(t)$ for small real $t$ ?","['asymptotics', 'analysis']"
4874861,Find the green area,"How to find the green area My attempt Each area is $\pi r_n^2=\dfrac{4\pi}{\left(n^2+2\right)^2}$ . We can approximate $\sum_1^\infty \dfrac{4\pi}{\left(n^2+2\right)^2}$ to be $\int_1^\infty \dfrac{4\pi}{\left(n^2+2\right)^2}dn$ . Let us evaluate the indefinite integral $\int\dfrac{4\pi}{\left(n^2+2\right)^2}=4\pi\int\dfrac1{\left(n^2+2\right)^2}dn$ first. We use a trig sub - let $n=\sqrt2\tan\theta\implies dn=\sqrt2\sec^2\theta~d\theta$ . We end up with: \begin{align*}
\int\dfrac1{\left(2\tan^2\theta+2\right)^2}~dn&=\int\dfrac1{4\left(\sec^2\theta\right)^2}~dn \\
&=\dfrac14\int\dfrac1{\sec^4\theta}\sqrt2\sec^2\theta~d\theta \\
&=\dfrac1{2\sqrt2}\int\dfrac1{\sec^2\theta}~d\theta \\
&=\dfrac1{2\sqrt2}\int\cos^2\theta~d\theta \\
&=\dfrac1{4\sqrt2}\int1+\cos(2\theta)~d\theta \\
&=\dfrac1{4\sqrt2}\left(\theta+\dfrac{\sin(2\theta)}2\right) \\
&=\dfrac1{8\sqrt2}(2\theta+\sin(2\theta)) \\
\theta&=\tan^{-1}\left(\dfrac n{\sqrt2}\right) \\
I&=\dfrac1{8\sqrt2}\left(2\tan^{-1}\left(\dfrac n{\sqrt2}\right)+\sin\left(2\tan^{-1}\left(\dfrac n{\sqrt2}\right)\right)\right).
\end{align*}","['area', 'solution-verification', 'problem-solving', 'geometry']"
4874873,"Find the largest value of $a = x^2 - y^2 + 2xy$, for real numbers $x, y$ such that $x^2 + y^2 \leq 1$","Find the largest value of $a = x^2 - y^2 + 2xy$ , for real numbers $x,y$ such that $x^2 + y^2 \leq 1$ Here's what I did, but it doesn't look quite convincing to me. $a = x^2 - y^2 + 2xy = 2x^2 - (x - y)^2 \leq 2x^2 $ The equality occurs when $x = y$ . However, $x^2 + y^2 \leq 1$ . When $x = y$ , this becomes $2x^2 \leq 1$ Thus, $a \leq 1$ , and $a = 1$ occurs when $x = y = \pm\frac{1}{\sqrt{2}}$ I hope someone could give me some feedback and perhaps also a more rigorous argument. Thank you!","['calculus', 'algebra-precalculus']"
4874902,Non-trivial closed maps on Banach spaces,"Let's call a map closed if it takes any closed subset to a closed subset. I'm wondering if there are some ""standard"" examples of closed (in this sense) linear maps on a Banach space, other than the zero map or a linear homeomorphism. More precisely: Question: What is an example of a bounded linear map $T\colon X\to X$ , where $X$ is a Banach space, that takes closed sets to closed sets, other than $T=0$ or a homeomorphism?","['banach-spaces', 'operator-theory', 'functional-analysis', 'linear-transformations']"
4874932,Convex set with convex complement has non empty interior?,"My question is if in a normed space (maybe complete if necessary), a convex (non empty) set with convex complement has non-empty interior. I cannot think of a counterexample, but neither how to prove it. Any ideas?",['functional-analysis']
4874944,Essentially infinite continuous maps $\prod_{\mathbf{N}} \mathbf{N} \rightarrow \mathbf{N}$?,"Let $\mathbf{N}$ be the natural numbers with the discrete topology (or really, any countable set), and consider the space of natural number sequences $\prod_{\mathbf{N}} \mathbf{N}$ with the product topology (sometimes called ""the Baire space""). Is there an ""explicit"" example of a continuous function $\prod_{\mathbf{N}} \mathbf{N} \rightarrow \mathbf{N}$ that does not factor through projection $\prod_{\mathbf N} \mathbf{N} \rightarrow \prod_{i=0}^{n} \mathbf{N}$ onto some finite product? I can't think of one off the top of my head. If no explicit example, does such a map at least exist abstractly?","['general-topology', 'descriptive-set-theory']"
4874948,an expansion in terms of norm and inner product in $\mathbb{R}^n$,"I want to prove the following inequality in $\mathbb{R}^n$ For any $s\geq2$ there exists some constant $C(s)>0$ such that $$|y|^s>|x|^s+s|x|^{s-2}\langle x,y-x\rangle+\frac{C(s)}{2^s-1}|y-x|^s$$ for all $x,y\in\mathbb{R}^n$ . Here $|\cdot|$ and $\langle\cdot,\cdot\rangle$ denote the usual norm and inner product in $\mathbb{R}^n$ respectively. I was initially thinking to use convexity of the map $x\mapsto|x|^s$ for $x\in\mathbb{R}^n$ . Then $$|y|^s\leq2^{s-1}(|x|^s+|y-x|^s)$$ But the inequality is in reverse order of what is required, also I have no idea to bring the inner product into picture. I have a intuition of the Taylor's expansion, but I am not very sure. Any help is appreciated.","['multivariable-calculus', 'linear-algebra', 'real-analysis']"
4874954,"Isomorphism between sections of $\text{Hom}(E,F)$ and bundle maps $E \to F$","Given vector bundles $E$ and $F$ over a smooth manifold $M$ do we have an isomorphism $$\Gamma(\text{Hom}(E,F)) \cong \text{Hom}_{C^\infty(M)}(E,F)?$$ That is, if I have a smooth bundle map $E \to F$ do I obtain a section of the bundle $\text{Hom}(E,F)$ and vice versa? I see this being used in multiple places, but I have not found a proof for this proposition.","['vector-bundles', 'differential-geometry']"
4874967,Analytical continuation of a Matsubara sum,"I want to numerically calculate the low temperature limit of the following Matsubara sum $$ S(\Omega) = \pi T \sum_{\omega_n} \frac{4\Delta^2+\Omega^2}{s_1 s_2 (s_1 + s_2)},$$ with $\omega_n = \pi T (2n+1)$ , $s_1(\omega_n) = \sqrt{\omega_n^2 + \Delta^2}$ and $s_2(\omega_n) = \sqrt{(\omega_n+\Omega)^2 + \Delta^2}$ .
In fact, in the limit $T\rightarrow0$ , I get $$ S(\Omega) = \frac{2\sqrt{4\Delta^2 + \Omega^2}}{\Omega} \tanh^{-1}{\frac{\Omega}{\sqrt{4\Delta^2 + \Omega^2}}}.$$ When I calculate the sum above directly I get the same result. This is nice but I am really interested in the real frequency analytical continuation of $S(\Omega)$ . To get it I do the usual change $\Omega \rightarrow i \Omega$ . This simply gives me $$ S_a(\Omega) = \frac{2\sqrt{4\Delta^2 - \Omega^2}}{\Omega} \tan^{-1}{\frac{\Omega}{\sqrt{4\Delta^2 - \Omega^2}}}.$$ This is the correct result. Nonetheless, when I try to calculate numerically the Matsubara sum above with the change $\Omega \rightarrow i \Omega$ , I only get the correct result for $\Omega < \Delta$ . I suspect that it comes from the fact that the real part inside the square root of $s_2(\omega_n)$ changes sign at $\Omega = \Delta$ . One trick I used was to do the change of variable $\omega_n \rightarrow \omega_n - \Omega/2$ . In this case the change of sign of the real part inside $s_2(\omega_n)$ changes at $\Omega = 2\Delta$ . And indeed the analytical result $S_a$ and the direct sum calculation give the same results for $\Omega <2\Delta$ . But the results remain different for $\Omega > 2 \Delta$ .
My question is: what should I do to get perfect agreement between the sum and the analytical solution $S_a$ for all $\Omega$ ? To be clear, after the analytical continuation $\Omega \rightarrow i \Omega$ , I use the positive real part value for $s_2(\omega_n)$ for all $\Omega$ . Edit: I use a python script to calculate the sum where the square root has branch cut in the negatif axis $] - \infty, 0]$ . I can provide my script on request.","['analytic-continuation', 'complex-analysis', 'calculus', 'sequences-and-series', 'numerical-methods']"
4874973,Maximal Convex Hull in Integer Grid,"What set of points on the $256 \times 256$ integer grid maximizes the number of vertices in its convex hull? For the full context, I was given an assignment to test algorithms that find the vertices of the convex hull of a set of points in $2D$ . One natural thing to test is when we set the input set such that all the vertices (size $n$ ) are convex hull vertices (size $h$ ). In other words, the case when $n=h$ . One major restriction was that the synthetic vertices I generate need to be integer pairs in $[0, 32767]^2$ . To further clarify, co-linear points are not considered part of the convex hull (hence I want maximum number vertices). I quickly ran into issues because the trivial way of putting the vertices around the circumference of a circle no longer worked as I end up with something like here (range smaller here to highlight issue). Indeed, the maximum $h$ I could produce was very small (around $2000$ ). I already talked with my professor and they told me it's possible to make $h \gg 2000$ but I don't personally believe it, so before I go to them again I'd like to know if it's possible to prove the fundamental limit to how large $h$ can get (an expression for a low upper bound).","['convex-hulls', 'convex-optimization', 'convex-geometry', 'geometry', 'optimization']"
4875002,The limit of Thomae's function at any a such that 0 < a < 1,"I'm currently reading Spivak's Calculus and I'm having trouble understanding the example with the limit of Thomae's function. The function \begin{align}
f(x) = \begin{cases}
0 & \text{$x$ irrational, $0 < x < 1$}\\
\frac{1}{q} & \text{$x = p/q$ in lowest terms, $0 < x < 1$}
\end{cases}
\end{align} approaches $0$ for any $a \in (0, 1)$ . Looking at this picture , I don't get how that could be true. For instance, if $a = \frac{1}{2}$ , then it seems to me that for $\epsilon = \frac{1}{10}$ there's no $\delta$ such that $0 < \lvert x - a \rvert < \delta \implies \lvert f(x) - 0\rvert < \epsilon$ . What am I missing here?","['calculus', 'analysis', 'real-analysis']"
4875037,Internal angle sum of a triangle,"I teach a 5th grade class geometry, and I came up with the following alternative proof (?) to show that the internal angle sum of a triangle is $180^\circ$ . I remember reading that this result is equivalent to the parallel postulate , however I can't see where I have used this axiom. I would be very happy if anyone could point out my mistake. Here we go: The pencil starts at the side $CB$ The pencil turns clockwise $\angle B$ The pencil turns clockwise $\angle A$ The pencil turns clockwise $\angle C$ . Since the pencil points in the opposite direction it has turned $180^\circ$ (Or maybe $180^\circ+360^\circ\cdot k$ ?)","['euclidean-geometry', 'solution-verification', 'geometry']"
4875047,Combinatorial game played on a grid,"Let the grid consist of r rows and k columns. Two players take turns moving a piece to an adjacent square (no diagonal moves). Once a square has been visited it cannot be visited again. The piece starts in the top left square, this square can therefore not be visited again. The loser is the player that does not have a legal move to make, as an example take a 2x1 grid, player 1 moves to the second square, and player 2 does not have a legal move, which means he loses. Is there a winning strategy, and what is it? Obviously since draws are impossible, and this is a combinatorial game, there is a winning strategy. I firstly tried to imagine the game as an unary game of Nim with r*k piles. But this does not accurately represent the game. (Maybe something with bogus-nim, but I'm not very good at that). I secondly tried to play against an RNG, trying to mirror their moves, if they vertically I move horizontally within reason of course, but that did not yield anything. Does anyone have a hint? I don't want to be bold and ask for a very leading hint, but at this point I don't know what to do.","['combinatorics', 'combinatorial-game-theory']"
4875076,Integrating $e^{-(x^2+y^2+z^2)/a^2}$ over $\mathbb{R}^3$,"I want to compute the following integral $$
\int_{\mathbb{R}^3}e^{-(x^2+y^2+z^2)/a^2}\,dxdydz
$$ and I thought that using spherical coordinates could make it easier, since $r^2=x^2+y^2+z^2$ . With this in mind, I tried $$
\int_{\mathbb{R}^3}e^{-(x^2+y^2+z^2)/a^2}\,dxdydz=\int_0^\infty\int_0^{2\pi}\int_0^\pi r^2\sin \theta \,d\theta d\phi dr=4\pi\int_0^\infty r^2e^{-r^2/a^2}\,dr
$$ but I am stuck. Any ideas? Should I simply try to integrate it in cartesian coordinates?","['integration', 'multivariable-calculus', 'spherical-coordinates']"
4875093,About the continuity of the partial derivative,"I'm trying to study if the following function has continuous partial derivatives at the origin: $$f(x, y) = \begin{cases}\frac{x^4y^3}{x^8 + y^4} & (x, y) \neq (0, 0) \\\\ \quad 0 & (x, y) = (0, 0)\end{cases}$$ I proved $f$ is continuous at the origin, and I also proved its partial derivatives exist at the origin. Now to show the continuity of $f'_x$ at $(0, 0)$ here is what I did: $$\frac{\partial f}{\partial x} = \frac{4x^3y^3(y^4 - x^8)}{(x^8 + y^4)^2}$$ Having observed it goes to zero along various paths, I did: $$\bigg|\frac{4x^3y^3(y^4 - x^8)}{(x^8 + y^4)^2}\bigg| \leq \frac{4x^2y^2|x| |y| (x^8 + y^4)}{(x^8+y^4)^2} \leq \frac{4x^2y^2|x| |y|}{y^4} = \frac{4x^2|x|}{|y|}$$ But now I am stuck. If for example I say $y = x^4$ , this reduces to $\frac{4}{|x|}$ which does not goes to zero. But the notes say the partial derivatives are continuous (without any proof...) Any help? Thank you! Notice We cannot use polar coordinates. We are demanded to find a distance function to make an upper bound to the function we have, in order to conclude it goes to $0$ as $(x, y) \to (0, 0)$ .","['partial-derivative', 'continuity', 'multivariable-calculus']"
4875106,Notational Ambiguity: Covariant Derivative,"Let $M$ be a smooth manifold and $\nabla$ the Levi-Civita connection. Now, I am a bit puzzled by a serious notational ambiguity, namely for the second covariant derivative. To explain myself, let us consider a vector field $v\in\Gamma(TM)$ . Then, the notation $\nabla_{a}\nabla_{b}$ can mean two things: Some authors use the notation $\nabla_{a}:=\nabla_{\partial_{a}}$ . In this case, $\nabla_{\partial_{b}}v\in\Gamma(TM)$ and $\nabla_{\partial_{a}}(\nabla_{\partial_{b}}v)\in\Gamma(TM)$ . In local coordinates, we get $$\nabla_{\partial_{a}}\nabla_{\partial_{b}}v=(\partial_{a}(\partial_{b}v^{c}+\Gamma_{db}^{c}v^{d})+\Gamma_{ad}^{c}(\partial_{b}v^{d}+\Gamma_{eb}^{d}v^{e}))\partial_{c}$$ Some authors, especially in the physics literature (but for example also in Wald's book on GR), write expressions like $\nabla_{a}\nabla_{b}v^{c}$ to indicate the coefficients of the $(1,2)$ tensor obtained by applying $\nabla$ twice. In other words, $\nabla_{a}$ is acting on the $(1,1)$ tensor with coefficients $\nabla_{b}v^{c}=\partial_{b}v^{c}-\Gamma_{bd}^{c}v^{d}$ , i.e. $$\nabla_{a}\nabla_{b}v^{c}=\partial_{a}(\partial_{b}v^{c}+\Gamma_{db}^{c}v^{d})+\Gamma_{ad}^{c}(\partial_{b}v^{d}+\Gamma_{eb}^{d}v^{e})\color{red}{-\Gamma_{ab}^{d}(\partial_{d}v^{c}+\Gamma_{de}^{c}v^{e})}\quad (=:v^{c}_{;a;b})$$ So, the difference is basically the red term. This is actually well-known: Usually, one defines the second covariant derivative globally as the operator $$\nabla^{2}_{X,Y}v:=\nabla_{X}\nabla_{Y}v\color{red}{-\nabla_{\nabla_{X}Y}v}$$ where the second piece essentially corresponds to the red stuff appearing in 2. In other words, the construction in (1.) is the piece $\nabla_{\partial_{a}}\nabla_{\partial_{b}}v$ , while the construction in (2.) corresponds to the full second covariant derivative $\nabla_{\partial_{a},\partial_{b}}v$ . Now, my problem is, since some authors unfortunately use the notation $\nabla_{a}\nabla_{b}$ for $\nabla_{\partial_{a}}\nabla_{\partial_{b}}$ , how can I know which one of the above someone is using? As an explicit example: Consider a Riemannian manifold $(M,g)$ . Then, one can find in many books that the linearization of the Ricci tensor takes the form $$\mathrm{Ric}(g)_{ab}\xrightarrow{g=g_{0}+\lambda h+\mathcal{O}(\lambda^{2})}-\frac{1}{2}g_{0}^{cd}\nabla_{c}\nabla_{d} h_{ab}+\frac{1}{2}(2\nabla_{c}\nabla_{(a}h^{c}_{b)}-\nabla_{a}\nabla_{b}\mathrm{tr}_{g_{0}}(h))$$ where all the covariant derivatives are with respect to the background metric $g_{0}$ . Now, in this formula, is $g_{0}^{cd}\nabla_{c}\nabla_{d}$ as in my point (2.) above (=the connection Laplacian ) or as in my point (1.), i.e. $g_{0}^{cd}\nabla_{\partial_{c}}\nabla_{\partial_{d}}$ ? I know this is kind of a vague question, but this a point that really puzzles me. I am just wondering whether there is a general way to tell which convention is used in some specific text (without going through the full calculation, like for example deriving the linearization of the Ricci tensor myself).","['riemannian-geometry', 'connections', 'laplacian', 'differential-topology', 'differential-geometry']"
4875124,Covariance Operator corresponding to multivariate covariance function,"The usual definition of a covariance operator on $L_2(D)$ is: $$
C : L_2(D) \to L_2(D), \qquad (C \psi)(x) = \int_D c(x,y) \psi(y) dy \qquad \forall x\in D, ~~\psi \in L_2(D),
$$ where $c(x,y): D \times D \to \mathbb{R}$ is the corresponding covariance function. The operator norm is then $$
\|C\|= \sup_{\|\psi\|_{L_2}=1} \sqrt{\int_D \left(\int_D c(x,y)\psi(y) dy\right)^2dx}
$$ I am interested in the covariance operator corresponding to a multivariate covariance function. For example in this paper the author describes the multivariate covariance function $K: D \times D \to \mathbb{R}^p \times \mathbb{R}^p$ , so now we have a matrix valued covariance function $K(x,y) = [c_{ij} (x,y)]$ where $c_{ij}$ is a univariate covariance function defined as above. My question is about the operator corresponding to $K$ , and the definition of the operator norm. Is it correct to write this as: $$
C_K: (L_2(D))^p \to (L_2(D))^p, \qquad 
(C_K \psi)(x) =\int_{D} (K(x,y))^T \psi(y) dy, \qquad \forall x \in D, ~~ \psi \in (L_2(D))^p
$$ and $$
\|C_K\|= \sup_{\| \psi \|_{(L_2(D))^p}=1}
\sqrt{\int_{D} \left ( \int_{D} (K(x,y))^T\psi(y) dy \right )^2dx}
$$ I am also wondering if there are any good references that discuss this setting in more detail.","['statistics', 'covariance', 'reference-request', 'stochastic-processes', 'functional-analysis']"
4875204,"Is $\emptyset : \emptyset \to \emptyset$ an isomorphism from $(\emptyset, \leq)$ to $(\emptyset, \leq)$?","I was asked to determine whether the following statement is true: If every function $F : P \to P$ is a homomorphism from $(P, \leq)$ to $(P, \leq)$ , with $\leq$ an arbitrary order, then $|P| = 1$ . It is straightforward to observe that $|P| \not> 1$ . However, $|P| = 0$ seems to satisfy the first part of the predicate. If $P = \emptyset$ there is one and only one function over $\emptyset^2$ that maps from the empty set to itself; namely, $\emptyset$ . The definition of a homomorphism $F$ involves statements of the form: for all $x, y$ in $P$ occurs this and that involving $F$ ... So to refute that $F$ is a homomorphism one is to find a counter-example to these properties. Of course, such counter examples cannot be found in the empty set. So $\emptyset : \emptyset \to \emptyset$ is a homomorphism from $(\emptyset, \leq)$ to $(\emptyset, \leq)$ . The notation $\emptyset : \emptyset \to \emptyset$ is odd but seems formally correct, because $\emptyset$ is a function and it does have itself as domain and range. However, I'd incidentally like to know if this notation is correct indeed. Now, my question is the following. Is $\emptyset : \emptyset \to \emptyset$ an isomorphism between $(\emptyset, \leq)$ and $(\emptyset, \leq)$ ? It seems to be the case that $\emptyset$ thus considered is not only a function, but that it is its own inverse ( $\emptyset^{-1} = \emptyset$ ).","['elementary-set-theory', 'order-theory', 'discrete-mathematics']"
4875255,Average length of longest duplicated substring in a random binary string of length N,"What is the average length of longest substring occurring at more than one position in a uniformly random binary string S of length N ? For example, n  answer
1  0/2
2  2/4
3  10/8
4  26/16 For n=4 0000  3
0001  2
0010  1
0011  1
0100  1
0101  2
0110  1
0111  2
1000  2
1001  1
1010  2
1011  1
1100  1
1101  1
1110  2
1111  3
-------
      26 Generalized version: in stead of a binary string, S is a uniformly random string of length N with alphabet of size M.","['expected-value', 'combinatorics-on-words', 'combinatorics', 'bit-strings']"
4875389,A question involving the expectation of the random variable $2^X$,"Let $X$ be a random variable with mgf $M_X(t)=a+be^{2t}$ .Given that mean of the random variable $X$ is $1.5$ . Then find the expected value of the random variable $2^X.$ My attempt: We know that $E(X^r)=\frac{d^r}{dt^r}M_X(t) \rvert_{t=0}$ . So, we have that $E(X) = 2b = 1.5$ . This means that $b=0.75$ . Also, we know that $M_X(0)=1.$ So, we get $a=0.25$ . I am now going to do some operation here. I want to know whether it is valid or not. Now, set $t= \ln 2$ .
So, we have that $e^{(\ln 2) X} = \big( e^{\ln2 } \big)^X = 2^X.$ From the definition $M_X(t)=E(e^{tX})$ , we get $M_X(\ln 2) = E(e^{\ln 2 X}) =  E(2^X) = 0.25 + 0.75 e ^ {2 \times \ln 2} = 3.25$ . Hence, we have that $E(2^X)=3.25$ . Have I gone wrong somewhere ? If yes, please let me know where I have gone wrong and what is the correct approach. If not, then suggest what property of the random variables or distributions I am using here. Please help. Thank you very much.","['moment-generating-functions', 'probability', 'random-variables']"
4875444,I was trying to understand QUAKE III fast inverse square root alg and i want to find best 'u' value in $\log_2(x+1)≈x+u$ approximation,I was trying to find best 'u' value for this approximation: $\log_2(x+1)≈x+u$ And I did think I can calculate error with this function. NOTE: for the x values between 0 and 1 i need becouse of IEEE 754 float point uses a scientific notation that (1 + x) * 2^n that 0 <= x < 1 $f(u)=\int_{0}^{1}|\log_2(x+1)-(x+u)|dx$ Then I wanted to find when the slope becomes zero. (means local min in this situation) $\frac{d}{du}f(u)=0$ $\frac{d}{du}(\int_{0}^{1}|\log_2(x+1)-(x+u)|dx)=0$ Then I get stuck what can I even do after that?,"['integration', 'logarithms', 'derivatives', 'algorithms']"
4875537,Why is differentiability defined on open interval?,"If I have a subset $A \subseteq \mathbb{R}$ , a function $f: A \to \mathbb{R}$ , and a point $a \in A$ , then (according to the definitions I came across) for $f$ to be differentiable at $a$ , then there must exist some open interval $I$ so that $a \in I$ and $I \subseteq A$ . I tried understanding why this requirement exists, by first discarding it and seeing if it naturally arises from the other bits of the definition. NOTATION: For any $x \in X\subseteq \mathbb{R}$ , I define $D[x;X] \subseteq \mathbb{R}$ as: $$D[x;X] = \{ \ h \in \mathbb{R} \setminus \{ 0 \} \ | \ x + h \in X \ \}$$ and for any $\phi: X \to S$ such that $S \subseteq \mathbb{R}$ , I define $\mathrm{Q}^{\phi}_{x} : D[x;X] \to \mathbb{R}$ as: $$\mathrm{Q}^{\phi}_{x}(h) = \frac{\phi(x+h) - \phi(x)}{h}$$ DEFINITION: First I will fix the point $a \in A$ , and then define the derivative of $f$ at $a$ as: $$f'(a) = \lim_{ h \to 0 } \mathrm{Q}^{f}_{a}(h)$$ REFLECTION: If $f'(a)$ exists, then $\lim_{ h \to 0 } \mathrm{Q}^{f}_{a}(h)$ exists. For that to exist, $0\in\mathbb{R}$ has to be an accumulation point of $D[a;A]$ (every punctured neighbourhood of $0\in\mathbb{R}$ contains at least one point from $D[a;A]$ ). From that, it follows that $a\in A$ must be an accumulation point of $A$ . But now I am stuck, I am missing some sort of step or requirement or something, that would allow me to say: $$a_{0} \ \text{is an accumulation point of} \ A \ \ \wedge \ \ \text{something else} \implies \text{there must exist some open interval $I$ so that $a_{0} \in I$ and $I \subseteq A$.}$$ Now, I could always restrict the definition of limits to require that the domain of a function includes within it a punctured neighbourhood of the limit point. But that kind of feels like cheating? What is it that I am missing to make this step? EDIT: I thought maybe some common properties of derivatives cannot be proven with this definition. For example: $$f'(a) = \lim_{ h \to 0 } \mathrm{Q}^{f}_{a}(h) \implies \lim_{ x \to a } f(x) = f(a)$$ But sure enough, if I consider the function $I: D[a;A] \to \mathbb{R}; \ \  I(h) = h$ , I can deduce that $\lim_{ h \to 0 } I(h) = 0$ , so I can use the limit composition to say that: $$
\begin{align} 
&\lim_{ h \to 0 } I(h) = 0 \ \ \text{and} \ \ \lim_{ h \to 0 } \mathrm{Q}^{f}_{a}(h) = f'(a)
&\implies& \begin{aligned}
\lim_{ h \to 0 } \left[ I(h) \cdot \mathrm{Q}^{f}_{a}(h) \right]
&= \lim_{ h \to 0 } \left[ h \cdot \frac{f(a+h)-f(a)}{h} \right] \\
&= \lim_{ h \to 0 } \left[ f(a+h)-f(a) \right] \\
&= 0 \cdot f'(a) \\
&= 0 \\
\end{aligned} \\
&&\implies& \lim_{ x \to a } f(x) = a \\
\end{align}
$$ In a similar vein, Hans Lundmark suggested that you need extra assumptions to prove the chain rule, but that doesn't seem to be true either (I might be wrong please check my proof🙏). For any subsets $A,B \subseteq \mathbb{R}$ , and for any functions $f: A \to \mathbb{R}$ and $g: B \to \mathbb{R}$ such that $f[A] \subseteq B$ , the composite function $g \circ f: A \to \mathbb{R}$ is well defined, which I will denote as $u$ .
I will now: Fix the point $a \in A$ and assume that $f'(a) \in \mathbb{R}$ exists. Define $b \in B$ as $b = f(a)$ and assume that $g'(b) \in \mathbb{R}$ exists. Which means that: $$f'(a) = \lim_{ h \to 0 } \mathrm{Q}^{f}_{a}(h) \ \ \text{and} \ \  g'(b) = \lim_{ h \to 0 } \mathrm{Q}^{g}_{b}(h)$$ I define the function $k: D[a;A] \to \mathbb{R}; \ \ k(h) = f(a + h) - f(a)$ ; I know that $\lim_{ h \to 0 } k(h) = 0$ by the proof above. I then define the function $\mathrm{Q}^B: D[a;A] \to \mathbb{R}$ as: $$
\mathrm{Q}^B(h) = \begin{cases}
\mathrm{Q}^{g}_{b}(k(h)) & k(h) \ne 0\\
g'(b) & k(h) = 0
\end{cases}
$$ We can then deduce the following: $$
\begin{align}
&\begin{aligned}
\underline{\textbf{We know that:}} &&& \lim_{ h \to 0 } k(h) = 0 &&\iff \forall \varepsilon > 0, \exists \delta > 0, \forall h \in D[a;A]: 0<|h|<\delta \implies |k(h)| < \varepsilon \\
&&& \lim_{ h \to 0 } \mathrm{Q}^{g}_{b}(h) = g'(b) &&\iff \forall \varepsilon > 0, \exists \delta > 0, \forall h \in D[b;B]: 0<|h|<\delta \implies |\mathrm{Q}^{g}_{b}(h) - g'(b)| < \varepsilon \\
\end{aligned} \\ \\
&\begin{aligned}
&\text{Take an arbitrary $\epsilon>0$, hence} && \text{$\exists \delta > 0, \forall h \in D[b;B]: 0<|h|<\delta \implies |\mathrm{Q}^{g}_{b}(h) - g'(b)| < \epsilon$} \\
&\text{Take the particular $d>0$ for which this is true, hence} && \text{$\forall h \in D[b;B]: 0<|h|<d \implies |\mathrm{Q}^{g}_{b}(h) - g'(b)| < \epsilon$} \\
&\text{Since $d>0$, we also have}&&\text{$\exists \delta > 0, \forall h \in D[a;A]: 0<|h|<\delta \implies |k(h)| < d$} \\
&\text{Take the particular $\varDelta>0$ for which this is true, hence}&&\text{$\forall h \in D[a;A]: 0<|h|<\varDelta \implies |k(h)| < d$} \\
&\text{Take an arbitrary $H \in D[a;A]$, hence}&&\text{$0<|H|<\varDelta \implies |k(H)| < d$}
\end{aligned} \\ \\
&\begin{aligned}
\underline{\text{Assume that $0<|H|<\varDelta$:}}
&&&\text{Therefore we have $|k(H)|<d$} \\
&&&\text{By law of excluded middle, we have $k(H) = 0$ or $k(H) \ne 0$} \\
&&&\begin{aligned}
\underline{\text{When $k(H) = 0$:}}
&&&\text{$\mathrm{Q}^{B}(H) = g'(b)$, and therefore $0 = |\mathrm{Q}^{B}(H) - g'(b)| < \epsilon$}
\end{aligned} \\
&&&\begin{aligned}
\underline{\text{When $k(H) \ne 0$:}}
&&&\text{$\mathrm{Q}^{B}(H) = \mathrm{Q}^{g}_{b}(k(H))$, and therefore $k(H) \in B[b;B]$} \\
&&&\text{Hence, we have $0<|k(H)|<d \implies |\mathrm{Q}^{g}_{b}(k(H)) - g'(b)| < \epsilon$} \\
&&&\text{Since $|k(H)| > 0$, we have $0<|k(H)|< d$} \\
&&&\text{By implication, we have $|\mathrm{Q}^{g}_{b}(k(H)) - g'(b)| = |\mathrm{Q}^{B}(H) - g'(b)| < \epsilon$}
\end{aligned} \\ 
&&&\text{By cases, we conclude that $|\mathrm{Q}^{B}(H) - g'(b)| < \epsilon$}
\end{aligned} \\ \\
&\begin{aligned}
&\text{Therefore, we have} && 0<|H|<\varDelta \implies |\mathrm{Q}^{B}(H) - g'(b)| < \epsilon \\
&\text{$H$ is arbitrary, $\varDelta$ is particular, $\epsilon$ is arbitrary, so we have}&&\text{$\forall \varepsilon > 0, \exists \delta > 0, \forall h \in D[a;A]: 0<|h|<\delta \implies |\mathrm{Q}^{B}(h) - g'(b)| < \varepsilon$} \\
&&& \iff \lim_{ h \to 0 } \mathrm{Q}^{B}(h) = g'(b)
\end{aligned} \\ \\
&\begin{aligned}
\underline{\textbf{We conclude that:}} &&& \lim_{ h \to 0 } \mathrm{Q}^{B}(h) = g'(b)
\end{aligned}
\end{align}
$$ From here it is rather trivial to show that $\mathrm{Q}^{u}_{a}(h) = \mathrm{Q}^{B}(h) \cdot \mathrm{Q}^{f}_{a}(h)$ , just consider the cases $k(h) \not= 0, k(h) = 0$ in turn. Finally, by limit composition, we have $$
\begin{align} 
&\lim_{ h \to 0 } \mathrm{Q}^{B}(h) = g'(b) \ \ \text{and} \ \ \lim_{ h \to 0 } \mathrm{Q}^{f}_{a}(h) = f'(a)
&\implies& \begin{aligned}
\lim_{ h \to 0 } \left[ \mathrm{Q}^{B}(h) \cdot \mathrm{Q}^{f}_{a}(h) \right]
&= \lim_{ h \to 0 } \mathrm{Q}^{u}_{a}(h) \\
&= u'(a) = (g \circ f)'(a) \\
&= g'(b)f'(a) = g'(f(a))f'(a)
\end{aligned} \\
&&\implies& u'(a) = g'(f(a))f'(a) \\
\end{align}
$$ Hence, the chain rule holds. To the best of my knowledge, I don't think I made use of any additional assumptions to deduce this, beyond the definitions I provided. EDIT 2: As pointed out by Hans Lundmark (thankyou), my assumption that $f[A] \subseteq B$ is a little strong. The most general case in which composition still makes sense, is if we define the following: $A^* = \{ \ x \in A \ | \ f(x) \in B \ \}$ the restricted domain of $f$ for which composition can occur $f_{r} = \{ \ \langle x, y \rangle \in f \ | \ x \in A^* \ \}$ the corresponding restricted $f$ I can now define the composite function $g \circ f: A^* \to \mathbb{R}; \ \ x \mapsto g(f_{r}(x))$ , and as before: $u = g \circ f$ . Fix the point $a \in A^*$ and assume that $f'(a) \in \mathbb{R}$ exists. Define $b \in B^*$ as $b = f(a)$ and assume that $g'(b) \in \mathbb{R}$ exists. That seems to be the most general case for which the concept of function composition can make sense. And so, if I pick $A = \left\{ \ \frac{1}{2x} \ | \ x \in \mathbb{Z} \setminus \{ 0 \}, \frac{x}{3} \not\in \mathbb{Z} \  \right\} \cup \{ 0 \}$ and $B = \left\{ \ \frac{1}{3x} \ | \ x \in \mathbb{Z} \setminus \{ 0 \}, \frac{x}{2} \not\in \mathbb{Z} \  \right\} \cup \{ 0 \}$ and $f(x)=x$ and $g(x) = x$ and $a=0$ , then we see that $f'(0)=1$ and $g'(f(0))=1$ , but $u'(0)$ does not exist. Because $A^*= \{ 0 \}$ , so $0$ is not an accumulation point of $A^*$ . Here is a plot I drew up real quick: The extra assumption needed to make this work, is to require that $a$ is an accumulation point of $A^*$ . So then $f_{r}'(a) = f'(a)$ , and since $f_{r}[A^*] = B^* \subseteq B$ , we can just use the above proof and deduce that $u'(a)=g'(f(a))f'(a)$ . And none of these extra assumptions is needed when you assume the open interval. Thank you Hans Lundmark that one :)","['calculus', 'analysis']"
4875562,Curves vs Lines: A Symmetry Question,"For every line in the 2d plane, we can construct a shape with an ""inside and outside"" (often a circle) such that the shape is cut by the line into two symmetrical parts. Does this property of lines extend to all curves? My idea is if you ""zoom in"" on a curve, if you get small enough, it might become a line segment, then it would obviously extend. But does anybody have any ideas on a real proof? REVISION: Let me rephrase my question as the following: Prove or disprove the following statement: For any $C^1$ curve, we can construct a shape (an object with a boundary and thus a clear inside and outside) such that the smooth curve cuts the shape into two congruent pieces.","['symmetry', 'geometry']"
4875566,Maximizing the Number of Vertices for a Regular n-gon Within a Specific Integer Coordinate Range [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 months ago . Improve this question I'm interested in identifying the largest possible number of vertices (𝑛) for a regular
𝑛-gon that can be formed within an integer coordinate plane. The constraints of my problem are as follows: The plane is defined by the coordinate range $[0,32767]^2$ Each vertex of the n-gon corresponds to a unique integer pair $(x,y)$ . the context of this problem is that I am really trying to find the greatest number of coordinates forming a convex hull (excluding colinear points) within this range.","['number-theory', 'geometry']"
4875572,Removing curve from simply connected domain's boundary : does it stay connected?,"Let $U \subset \mathbb R^2$ be an open, regular (meaning $U$ is the interior of its closure), bounded and simply connected set. Let $\gamma : [0,1] \rightarrow \partial U$ be a simple curve such that $\gamma(0) \neq \gamma(1)$ . Is $\partial U \setminus \gamma((0,1))$ still connected ? I think this should be true. Assume for a contradiction that this is not the case. Let $K$ be the connected component $\{\gamma(0)\} \subset K \subsetneq \partial U \setminus \gamma((0,1))$ , where we remark that $K$ and $\partial U \setminus \gamma((0,1))$ are compact. By Zoretti's Theorem, for any $\varepsilon > 0$ , we can find a simple closed curve $J : [0,1] \rightarrow \mathbb R^2$ for which $$
K \subset \mathrm{int}(J), \quad  J \cap \left( \partial U \setminus \gamma((0,1)) \right) = \emptyset, \quad \mathrm{dist}(K, j) \leq \varepsilon \quad \forall j\in J
$$ Note that $J$ cannot be contained in $U$ (otherwise, $K \subset \mathrm{int}(J) \subset U$ which contradicts the regularity of $U$ ) and $J$ can only cross $\partial U$ through $\gamma((0,1))$ . I think we should be able to deduce from this that $\gamma(0)$ would actually lie on the exterior of $J$ but I am not able to complete my proof. Any help is welcomed. EDIT : This is related to the notion of cross-cut . A cross-cut $\phi$ leads to a decomposition of $U$ into two simply connected domains, whose boundaries are then connected and contain $\phi$ . What would happen to these boundaries if one removed $\phi$ ? EDIT 2 : My above argument is incorrect because $\gamma((0,1))$ is not necessarily open in $\partial U$ topology and this is what leads to the counter-example below.","['general-topology', 'geometry', 'connectedness']"
4875656,Showing that a level set is not a submanifold,"Is there a criterion to show that a level set of some map is not an (embedded) submanifold? In particular, an exercise in Lee's smooth manifolds book asks to show that the sets defined by $x^3 - y^2 = 0$ and $x^2 - y^2 = 0$ are not embedded submanifolds. In general, is it possible that a level set of a map which does not has constant rank on the set still defines a embedded submanifold?",['differential-topology']
4875697,A problem on tangent and secant lines,"My question is the following: Assume a point $A$ outside some circle, and a point $X$ on the same circle, such that $AX$ is not a diameter of the circle. Draw $Y$ as the point of intersection between the circle and the (extesion of the) line $AX$ . Draw the tangents to the circle in points $X$ and $Y$ , and denote their point of intersection with $Z$ . Also draw the two tangents $AB$ , $AC$ to the circle, where $B$ , $C$ are points on the circle. Prove that $Z$ lies on the extension of the line $BC$ . The approaches I have tried until now involved either analyzing triangle similarities and searching for equal angles (many pairs of angles subtend the same arcs), calculating cross ratios (considering the drawing as a two-point
perspective drawing of a circle), establishing a particular origin of linear coordinates and searching vector expressions for the positions of all points, writing all points in polar coordinates, although without any significant progress. Can you please help me formally solve this problem? Thank you.","['analytic-geometry', 'geometry']"
4875761,How does one solve this ODE using a differential operator and a series expansion?,"I am solving a fairly basic non-homogeneous ODE, but I wanted to try using the differential operator, and a series expansion in order to find the particular solution. I am fairly convinced it is possible to solve an ODE using this method, however I an unable to understand how. This is what I have done thus far: $$
\begin{align*}
2y' + y &= \cos x \\
\end{align*}
$$ $$
\begin{align*}
(2D+1)y &= \cos x \\
\end{align*}
$$ $$
\begin{align*}
y &= \frac{\cos x}{2D+1} \\
\end{align*}
$$ $$
\begin{align*}
y &= \sum_{n=0}^\infty (2D)^{2n} \cdot (1 - 2D) \cdot \cos x \\
\end{align*}
$$ $$
\begin{align*}
y &= \sum_{n=0}^\infty (2D)^{2n} \cdot (\cos x + 2\sin x)
\end{align*}
$$ How do I proceed from here? I cannot spot any mistake I have made in anything I have done leading up to it, but trying to proceed seems almost impossible. Have I completely misunderstood how derivative operators work? I tried to solve using maclaurin series as well: $$
\begin{align*}
2y' + y &= \cos x \\
\end{align*}
$$ $$
\begin{align*}
(2D+1)y &= \cos x \\
\end{align*}
$$ $$
\begin{align*}
y &= \frac{\cos x}{2D+1} \\
\end{align*}
$$ $$
y= \sum_{n=0}^{\infty} \frac{f^{(n)}(0)}{n!} D^n \cdot(\cos x)
$$ If the non-homogeneous function was a polynomial $x^n$ I could proceed by setting the upper bound as $n$ and evaluating (since subsequent terms have a differential operator of higher order than $n$ which will therefore annihilate the polynomial). But once again I do not know how to proceed from here for $\cos(x)$ since the derivative is cyclical. I am aware of the Exponential response formula , but I want to try solve without it.","['derivatives', 'ordinary-differential-equations', 'sequences-and-series']"
4875765,Is there a permutation of a given length in which every element divides sum of the elements before it?,"For given positive integer $n$ , is it possible to construct a permutation $p$ of [1, n],  such that for each $k$ in
[2, n], $p_k$ divides $p_1 + p_2 + ... + p_{k-1}$ . Solutions for small $n$ : $n=3$ , p = 2 1 3 $n=4$ , p = 4 2 3 1 $n=5$ , p = 4 1 5 2 3 $n=6$ , p = 4 1 5 2 6 3 $n=7$ , p = 5 1 6 2 7 3 4 $n=8$ , p = 5 1 6 2 7 3 8 4 $n=9$ , p = 8 1 9 6 4 7 5 2 3","['permutations', 'number-theory']"
4875766,Correct method to evaluate the limit $\lim_{x\to 0}{\cfrac{\alpha e^x+\beta e^{-x}+\gamma\sin(x)}{x\sin^2x}}=2/3$?,"I couldn't solve this question so I looked for hints. One method of solving was to use the Taylor Series expansion of each of the functions.
It was a bit long.
So another solution used the L'Hospitals Rule instead. $$\lim_{x\to 0}{\cfrac{\alpha e^x+\beta e^{-x}+\gamma\sin(x)}{x\sin^2x}}$$ $$= \lim_{x\to 0}{\cfrac{\alpha e^x+\beta e^{-x}+\gamma\sin(x)}{x^3}}$$ For limit to be finite limit of numerator should be zero. Therefore $$\alpha +\beta =0.$$ But limit of $\sin x$ as $x$ tends to $0$ is taken as $x$ in most cases so the correct equation should be $$\alpha +\beta +\gamma x=0$$ and further equations are obtained similarly. Can the limit of $\sin x$ as $x$ tends to $0$ be taken as either $x$ or $0$ as per convenience? Is using L'Hopitals Rule a correct method of solving this question? OR Is Taylor series the more appropriate method?","['limits', 'calculus', 'taylor-expansion']"
4875775,(Hall's Theorem) Existence of two subfamilies of sets containing the same elements,"I came across the following claim in a textbook on combinatorics [1]. Claim (Lindström, Tverberg): Let $A_1, . . . , A_m \subseteq [n]$ be non-empty with $m > n$ . There are non-empty, disjoint $I, J \subseteq [m]$ , such
that $$\bigcup_{i \in I} A_i = \bigcup_{j \in J} A_j.$$ The book provides a very nice proof of this claim, using characteristic vectors of the sets $A_1, . . . , A_m$ and some linear algebra basics. It also includes a remark that says this claim can be proved using Hall's theorem . This second approach is not obvious to me but I would like to understand how you can use Hall's theorem here (it's really bugging me). If anyone could provide a solution, a hint, or a reference to a proof, I would really appreciate it. Reference:
[1] B. Sudakov, ""Algebraic Methods in Combinatorics"", 2023 Original Proof: Let $v_1, . . . , v_m$ be the characteristic vectors of $A_1, . . . , A_m$ . Since $m>n$ , they are linearly dependent over $\mathbb{R}$ . So, there is a nonzero $\alpha \in \mathbb{R}^m$ with $$\sum_{i=1}^m \alpha_i v_i = 0.$$ Let $$I = \{i : \alpha_i > 0\},$$ $$J=\{j : \alpha_j < 0\}.$$ The nonzero coordinates of $\sum\limits_{i \in I} \alpha_i v_i$ correspond to the
elements of $\bigcup\limits_{i \in I} A_i$ , which are the same as the nonzero coordinates of $\sum\limits_{j \in J} \alpha_j v_j$ corresponding to $\bigcup\limits_{j \in J} A_j$ , completing the proof.","['graph-theory', 'matching-theory', 'combinatorics', 'bipartite-graphs']"
4875783,Why are ultrametric spaces named as such?,"I'm beginning my study of $p$ -adic numbers, so naturally I've come across the non-Archimidean property of absolute values, i.e. $$|x+y| \le \max\{|x|,|y|\}.$$ A metric space with the analogous property i.e. $$d(x,y) \le \max\{d(x,z),d(z,y)\}$$ is called an ultrametric space. I was just wondering as to the etymology of this term, especially why ultrametric was chosen over non-Archimidean. I understand that this is a stronger condition than the usual triangle inequality, so I guess ultra-metric as in stronger than-metric is the idea behind it, but non-Archimidean would keep terminology consistent between metrics and absolute values.","['number-theory', 'terminology']"
4875818,Characterizing a Recursively Defined Rational Set [duplicate],"This question already has answers here : What is the set generate by ${1}$ and a function $1/(a+b)$? (2 answers) Closed 4 months ago . Let $S$ be the smallest set of rational numbers that contains $\tfrac{1}{2}$ , as well as the reciprocal of the sum of any two elements that are already in $S$ . Prove or disprove that $S=\mathbb{Q} \cap [\tfrac{1}{2},1]$ Note that the two elements may be identical. This is a repost of my previous post, which was answered partially as follows by @Tony Matthew (which I agree with): Partial answer Let's try building the set $S$ by starting with $\frac{1}{2}$ and adding new elements by taking two elements from the current set and calculating $f(a,b) \equiv \frac{1}{a + b}$ . One possible value for $S$ is the steady-state limit of this process. Since $\frac{1}{2}$ is the only element initially, the next new element must be $f(\frac{1}{2}, \frac{1}{2}) = 1$ , so that our set becomes $\left\lbrace\frac{1}{2}, 1\right\rbrace$ . Then, $f(1,1) = \frac{1}{2}, f(\frac{1}{2},1) = \frac{2}{3}$ , so the set next grows to $\left\lbrace\frac{1}{2}, \frac{2}{3}, 1\right\rbrace$ . Now the induction hypothesis: if the elements of the set lie between $\frac{1}{2}$ and $1$ , then so too will any new element generated via $f(a,b)$ . This can be proven by the following, $$\frac{1}{2}\leq a,b \leq 1 \rightarrow 1 \leq a + b \leq 2 \rightarrow \frac{1}{2}\leq f(a,b) \leq 1$$ Hence, since the set has started within the bounds of $\frac{1}{2}$ and $1$ , it will remain within those bounds, and so too $S$ . Therefore, since each new element must be a rational number, $$S \subseteq \mathbb{Q} \cap [\tfrac{1}{2},1]$$ I find this problem similar to that of the Calkin–Wilf tree, except the condition that two parent elements generate one child element is a bit more difficult to handle. In comments: For example, to obtain 5/7 I would try to find two elements such that their sum is 7/5, such as 3/5 and 4/5. These can generated by (2/3,1) and (3/4,1/2). 1 is generated by (1/2,1/2), 2/3 is generated by (1/2,1), 3/4 is generated by (2/3,2/3).","['number-theory', 'elementary-number-theory', 'recurrence-relations', 'discrete-mathematics', 'algorithms']"
4875855,Question on The Theorem of Existence and Uniqueness of The Solutions to Sylvester Equations,"Background I am self-studying linear algebra, and I got stuck on some steps of the proof of the following theorem: Theorem $\quad$ Let $A$ be an $n \times n$ complex matrix and $B$ be an $m \times m$ complex matrix. For any $n \times m$ complex matrix $C$ , the Sylvester equation $AX+XB=C$ has a unique solution $X$ , which is an $n \times m$ complex matrix, if and only if $A$ and $-B$ do not share any eigenvalue. Here is the proof: Proof $\quad$ The equation $AX+XB=C$ is a linear system with $mn$ unknowns and $mn$ equations. Hence, it is uniquely solvable for any given $C$ if and only if the homogeneous equation $AX+XB=0$ admits only the trivial solution. First suppose that $A$ and $-B$ do not share any eigenvalue. Let $X$ be a solution to the homogeneous system $AX+XB=0$ . Then $AX=X(-B)$ . Since \begin{align*}
&A(AX) = A(X(-B))\\
\implies\ &(AA)X = (AX)(-B) = (X(-B))(-B) = X((-B)(-B))\\
\implies\ &A^2X = X(-B)^2,
\end{align*} by mathematical induction, we have $A^kX=X(-B)^k$ for each $k\in\mathbb{N}$ . Then, $p(A)X = Xp(-B)$ for any polynomial $p$ . In particular, let $p$ be the characteristic polynomial of $A$ ; that is, let $p(r) = det(A-rI)$ . Then, $p(A) = 0$ . If $E$ is any square matrix, let $\sigma(E)$ denote the set of eigenvalues of $E$ . Then, $\sigma(p(-B)) = p(\sigma(-B))$ . Since $A$ and $-B$ do not share any eigenvalue, $p(\sigma(-B))$ does not contain zero, and hence, $p(-B)$ is nonsigular. Thus, $X$ is the zero matrix. Conversely, suppose that $A$ and $-B$ share an eigenvalue $\lambda$ . Let $\mathbf{u}$ be a corresponding right eigenvector for $A$ and $\mathbf{v}$ be a corresponding left eigenvector for $-B$ ; that is $A\mathbf{u}=\lambda\mathbf{u}$ and $\mathbf{v}(-B) = \lambda\mathbf{v}$ . Let $X=\mathbf{u}\mathbf{v}^*$ . Then, $X$ is not the zero matrix, and $AX+XB = A(\mathbf{u}\mathbf{v}^*) - (\mathbf{u}\mathbf{v}^*)(-B) = \lambda\mathbf{u}\mathbf{v}^* - \lambda\mathbf{u}\mathbf{v}^* = 0$ . Therefore, $X$ is a nontrovial solution to the homogeneous system $AX+XB=0$ . My Questions I could not understand certain steps in the above proof. For the ""if"" direction (the second paragraph in the proof): After it proved that $A^kX=X(-B)^k$ , it claimed that $p(A)X=Xp(-B)$ for any polynomial. Why is that? After it defined $p$ to be the characteristic polynomial, it said that $p(A)=0$ . Is this because one might just plug in $A$ to $p(r)$ to get $p(A) = det(A-AI) = det(0) = 0$ ? I have difficulties understanding the notation of $p(\sigma(-B))$ . While the $\sigma(-B)$ is the set of eigenvalues of $-B$ , what is $p(\sigma(-B))$ then? Is it the set of the image of the eigenvalues of $-B$ under the polynomial $p$ ? If so, does $\sigma(p(-B)) = p(\sigma(-B))$ mean that the two sets are equal? Why does the fact that $A$ and $-B$ do not share any eigenvalue imply that $p(\sigma(-B))$ does not contain zero, and how does it implies that $p(-B)$ is nonsingular? Why does the nonsigularity of $p(-B)$ imply that $X$ is the zero matrix? For the ""only if"" part (the third paragraph in the above proof): I am not familiar with the notation $\mathbf{v}^*$ . Is it the conjugate transpose of $\mathbf{v}$ ? If so, why cannot $\mathbf{u}\mathbf{v}^*$ be the zero matrix? I apologize for my lousy beginner linear algebra background. I would really appreciate it if someone could help me with these questions!","['determinant', 'eigenvalues-eigenvectors', 'matrices', 'linear-algebra', 'polynomials']"
4875860,The expected max inner product between a random vector and the sum of random vectors,"Given: Fix $n, k \in \mathbb{N}$ , we generate $n$ random vectors $X_i \in \mathbb{R}^k$ with $\|X_i\|_2 = 1$ . We want to compute: The expected max Euclidean inner product between a random vector and the sum of the random vectors, i.e., $$
\mathbb E \max_{i \in [n]} \langle X_i, \sum_{j \in [n]} X_j \rangle.
$$ What I have tried: When $n = 2$ , $\langle X_1, X_1 + X_2 \rangle = \langle X_2, X_1 + X_2 \rangle = 1 + \langle X_1, X_2 \rangle$ ; $\mathbb E \langle X_1, X_2 \rangle = 0$ due to symmetry, and thus $\mathbb E \max_{i \in [n]} \langle X_i, \sum_{j \in [n]} X_j \rangle = 1$ . When $n \geq 3$ , it becomes unclear to me. Intuitively it might get larger as $n$ increases since we have more options, and would approach some limit, possibly related to $\| \sum_{j \in [n]} X_j \|$ . Simulations (1,000,000 trials) with $n = 3$ and $k = 2$ gives $1.5319441791547532 \pm 0.7104635978561801$ . Simulations (1,000,000 trials) with $n = 4$ and $k = 2$ gives $1.6632600316282025 \pm 0.9042611238131294$ . Simulations (1,000,000 trials) with $n = 5$ and $k = 2$ gives $1.9472506556915035 \pm 0.9787530817053821$ . Simulations (1,000,000 trials) with $n = 100$ and $k = 2$ gives $8.86393808099978 \pm 4.6286431725480295$ . Simulations (1,000,000 trials) with $n = 100$ and $k = 3$ gives $9.083624932218164 \pm 3.8388222137766603$ Simulations (1,000,000 trials) with $n = 10,000$ and $k = 2$ gives $88.64022827148438 \pm 46.3698844909668$ . Simulations (1,000,000 trials) with $n = 10,000$ and $k = 3$ gives $92.19273147896439 \pm 38.86980525584397$ . Simulations (1,000,000 trials) with $n = 1,000,000$ and $k = 2$ gives $886.809001225146 \pm 463.51510862527164$ Simulations (1,000,000 trials) with $n = 1,000,000$ and $k = 3$ gives $921.3826932102985 \pm 388.68428531397456$ Simulations (1,000,000 trials) with $n = 1,000,000$ and $k = 4$ gives $939.8279114022998 \pm 341.27201929744365$ It looks like the order is near $\sqrt{n}$ , and I consequently have a conjecture that it would be something close to $\sqrt{n}$ . I saw something $0.886$ for $k = 2$ , and I know $\sqrt{\pi} / 2 \approx 0.8862$ ; not sure it is related or not. Limit of the integral is the square root of pi over 2 https://en.wikipedia.org/wiki/Gaussian_integral I also suspect that it is related to the sphere-cylinder ratio; see, e.g., Volume of Region in 5D Space As @Sal pointed out, this is related to random walks (see, e.g., Expected Value of Random Walk ), and seemingly $$
\mathbb E \| \sum_{j \in [n]} X_j \| \approx \sqrt{\dfrac{2n}{k}} \dfrac{\Gamma(\frac{k+1}{2})}{\Gamma(\frac{k}{2})},
$$ which is $\sqrt{n} \frac{\Gamma(1.5)}{\Gamma(1)} = \frac{\sqrt{\pi}}{2}\sqrt{n} \approx 0.886227 \sqrt{n}$ for $k = 2$ as I suspected, $\sqrt{\frac{8}{3\pi}} \sqrt{n} \approx 0.921318 \sqrt{n}$ for $k = 3$ , $\frac{3}{4} \sqrt{\frac{\pi}2} \sqrt{n} \approx 0.939986 \sqrt{n}$ for $k = 4$ , and indeed approaches $\sqrt{n}$ as $k \to \infty$ . However, this only gives us the asymptotic results as $n \to \infty$ , while the cases for small $n$ are still unclear.","['expected-value', 'probability', 'random-variables']"
4875862,Can any characterized property give a solid account of why multiplication is a harder computation operation than addition?,"For humans, in general, it's far easier to do an addition than a multiplication. One might argue this is just an effect of the particular way brains are wired. But from what I find,  multiplication is also more expensive on computers, where one could certainly arrange chips in an arbitrary way, or at least without any pre-established biological constraint. But from a group theory point of view, addition is not more fundamental than multiplication,  is it? Are there some characteristics,  like associativity or commutativity that explain that difference in terms of computational complexity? The answer might of course rely on other mathematical fields,  like topology, or even be related to some physical constraints that hold for both brains and common CPUs but are not strictly constrainted by mathematical characteristics. Related resources: https://stackoverflow.com/questions/21819682/is-integer-multiplication-really-done-at-the-same-speed-as-addition-on-a-modern How is addition different than multiplication? Is addition more fundamental than subtraction?","['group-theory', 'computational-complexity', 'arithmetic']"
4875936,Knights and Knaves variation,"I saw a problem where it asked: What question can you ask a knight (who always tells the truth) such that he cannot respond? For this, I came up with the question ""Is the answer to this question No?"" -- the knight can't say Yes (because he would be lying) and he also can't say no. Similarly for a knave, ""Is the answer to this question Yes?"" works. My question is: is there a question that you can ask to both the knight and the knave that they both can't answer?","['contest-math', 'puzzle', 'logic', 'discrete-mathematics']"
4876142,Derivative of $Ax x^\top A$ with respect to $x$,"I do not want to use index notation. I want to compute the derivative $$
D_x (Axx^\top A) = ?
$$ where $A$ is an $n\times n$ symmetric matrix and $x$ in a vector in $\mathbb{R}^n$ . I tried resources such as the matrix calculus cookbook but they don't deal with scenarios like this: Here the function $f(x) = Axx^\top A$ takes a vector as input and returns a matrix output. It is possible to express this without using index notation and I want this type of answers. I would like step by step, to figure out how I can go about performing similar calculations in the future. Attempt One attempt is using the Frechet derivative definition (I will use the Frobenius norm) $$
\begin{align}
\lim_{\|v\|\to 0} \frac{\|A(x+v)(x+v)^\top A - Axx^\top A - Dv\|_F}{\|v\|}
&= \lim_{\|v\|\to 0} \frac{\|A(xv^\top +vv^\top + vx^\top)A - Dv\|_F}{\|v\|}
\end{align}
$$","['tensors', 'frechet-derivative', 'multivariable-calculus', 'matrix-calculus', 'derivatives']"
