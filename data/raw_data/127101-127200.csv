question_id,title,body,tags
1938979,Convergence of integrals - both finite but different,"Given a measure space, is it possible to have a sequence of integrable functions that converges pointwise, such that the integral of the limit differs from the limit of the sequence of integrals, both quantities being finite ?
All counterexamples I've seen illustrating the utility of Lebesgue's convergence theorems involve at least one of those quantities being infinite. I'm thinking some kind of function with a triangle somewhere that rises and gets narrower such that the area remains constant would do it but I can't quite reach my point","['integration', 'convergence-divergence', 'limits']"
1939026,Compute $\int^{\pi/2}_0 \frac{dx}{(a^2\cos^2 x + b^2 \sin ^2 x)^2}$,"I have tried solving this for about an hour and will probably resort to head banging in some time: $$\int ^{\frac{\pi}{2}}_{0} \dfrac{dx}{(a^2\cos^2 x + b^2 \sin ^2 x)^2}$$ I first divided by $\cos^4 x$ and then subsequently put $\tan x = t$, to get: $$\int ^{\infty}_{0} \dfrac{1+t^2}{(a^2 + b^2 t^2)^2}dt$$ This has become unmanageable. Neither splitting the numerator, nor Partial fraction (taking t^2 = z and applying partial fraction) seems to work.","['integration', 'definite-integrals']"
1939066,How to prove $\sin 10^\circ = \frac{-1+\sqrt{9-8\sin 50^\circ}}{4}$?,"How to prove this identity? $$\sin 10^\circ = \frac{-1+\sqrt{9-8\sin 50^\circ}}{4}$$ Is this a particular case of a more general identity? Also, is it possible to give a geometric proof of this equality?","['trigonometry', 'geometry']"
1939138,The direct sum of a rank and null space,"Firstly, I have no idea how to use most of the fancy mathjax formatting stuffs yet. I'm currently too frustrated trying to figure out how to understand this problem (due in 10 hours), so please feel free to refer anything for that while answering. :) I've got a hw problem stated as such:
Let $V$ be a finite-dimensional vector space and T:V->V be linear. A) Suppose that V = R(T) + N(T). Prove that V = R(T) $\oplus$ N(T). B) Suppose that R(T) $\cap$ N(T) = {$0$}. Prove that V = R(T) $\oplus$ N(T). I've found two separate answer explanations for part A (both along the lines of $P$ = $P^2$ with $P$(1 - $P$) = 0), but I don't understand why that is the answer. The largest problem is probably my confusion with how direct sums work. I understand that the subspace $V$ $\oplus$ subspace $W$ = a vector space, when - 1) $V$ $\nsubseteq$ $W$ (I think that's the correct notation? Basically all members of V and all of W will not be equal/overlap), and when - 2) $V$ $\cap$ $W$ = {$0$} (i.e. they only have the zero vector in common) But I'm having issues correlating those two points to anything outside of the definition, so understanding and proving this is completely foreign to me. I'd love answers even after this is due, since I suspect this class will be heavy on understanding these concepts. Thanks in advance!","['direct-sum', 'linear-algebra', 'vector-spaces']"
1939147,Can you have two nonisomorphic elliptic curves with isomorphic global Tate modules?,"Let $K$ be a number field, and $E_1,E_2/K$ be elliptic curves. Let $G_K$ be the absolute Galois group. Is it possible for $T_\ell(E_1)$ and $T_\ell(E_2)$ to be isomorphic for all $\ell$ (as $G_K$-modules), even if $E_1\not\cong E_2$?",['algebraic-geometry']
1939154,the way of thinking of frequentist vs bayesian? [duplicate],"This question already has answers here : Describing Bayesian Probability (3 answers) Closed 9 months ago . I'm learning about bayesian inference and I've heard that there's also another inference called frequentist inference. I still can't understand the difference the way frequentist and bayesian count the probability, can someone give me a simple example of a problem, and how frequentist and bayesian solve the problem?","['statistics', 'probability']"
1939176,Correct spaces for quantum mechanics,"The general formulation of quantum mechanics is done by describing quantum mechanical states by vectors $|\psi_t(x)\rangle$ in some Hilbert space $\mathcal{H}$ and describes their time evolution by the Schrödinger equation
$$i\hbar\frac{\partial}{\partial t}|\psi_t\rangle = H|\psi_t\rangle$$
where $H$ is the Hamilton operator (for the free particle we have $H=-\frac{\hbar^2}{2m}\Delta$). Now I have often seen used spaces like $\mathcal{H}=L^2(\mathbb{R}^3)$ (in the case of a single particle), but I was wondering whether this is correct or not. 
In fact shouldn't we require to be able to derivate $\left|\psi_t\right>$ twice in $x$ and thus choose something like $\mathcal{H} = H^2(\mathbb{R}^3)$? If we treat directly $\psi(t,x) := \psi_t(x)$, shouldn't we require them to be in something like $H^1(\mathbb{R};H^2(\mathbb{R}^3))$? i.e., functions in $H^1(\mathbb{R})$ with values in $H^2(\mathbb{R}^3)$, e.g. the function $t\mapsto\psi_t$.","['quantum-mechanics', 'hilbert-spaces', 'partial-differential-equations', 'mathematical-physics', 'functional-analysis']"
1939187,Implications of a Curve being Tangent to a Line,"In my experience, I have usually dealt with finding or analyzing tangents to a curve. In this question I've encountered, I am asked to analyze a curve given it is a tangent to the line y=x . Here is the curve: $y=(x^3/3)+ax+b. $ I am tasked to prove that the above conditions imply that: $4(a-1)^3+9b^2=0$ I have deduced some things from the implication, but I feel that I have not deduced enough! Here is what I have understood so far: The slope of $y=x$ is 1. At the point the curve is tangent to this line, it's slope is 1. Therefore, I differentiated the equation of the curve with respect to $x$ and reached the following conclusion: $x^2+a=1$
so $x=\pm \sqrt(1-a) $ Substituting this result in the equation of the curve, I get $y=((\pm \sqrt(1-a))^3/3)+a(\pm \sqrt(1-a))+b. $ This is something, but does not seem to get me closer to deducing the required. At the point where the curve is tangent, we also have $y=x=\pm \sqrt(1-a)=((\pm \sqrt(1-a))^3/3)+a(\pm \sqrt(1-a))+b. $ My question is thus:
Am I not considering an implication of the curve being a tangent to this line that will help me deduce the required?","['derivatives', 'tangent-line', 'calculus', 'proof-verification']"
1939213,What are the solutions to $x^2+7=y^3$,"What are the integer solutions to $x^2+7=y^3$? I think that this would require algebraic number theory using the fact that $\mathbb{Z}[\sqrt{-7}]$ is a unique factorization domain, but I don't know enough number theory to proceed. The only integer solutions, according to SAGE is $$(\pm 1, 2) (\pm 181, 32)$$","['number-theory', 'elliptic-curves']"
1939223,Local Lipschitz continuity of x⋅sin(1/x) at $0$,"Prove that $f: \mathbb{R} \rightarrow \mathbb{R}$, with $$f(x) = \begin{cases} x\sin(\frac{1}{x}) & x \neq 0 \\ 0 & x = 0 \\ \end{cases} $$ is not Lipschitz continuous in any interval containing zero. Since a function is local Lipschitz (L.L.) when, given $x_0 \in (a,b)$, there is an open ball $B$ centered at $x_o$ with radius $r>0$ such that for all $x,y \in B$, $$|f(x) - f(y)| \leq K_{x_0}|x-y|.$$
Being so, the job is to show $f$ isn't L.L. in any open ball centered at $0$ (isn't L.L. at the origin). What I've tried is to bound $|f(x) - f(y)|$ by factoring out $|x - y|$ and using the triangular inequality for adding the terms that subtract at $(x-y)(sin(\frac{1}{x}) - sin(\frac{1}{y}))$ but are not on $sin(\frac{1}{x}) - sin(\frac{1}{y})$, which would get me an unbounded term of $|x||sin(\frac{1}{y})|$, which then would have no bound $K_{x_0}$. But that doesn't guarantee there isn't such a $K$ between $|f(x) - f(y)|$ and the upper bound we got, does it? Apart from that, I know that every function with a bounded derivative in an interval (which can be unbounded) is Lipschitz and a continuously differentiable function in the neighborhood of a point is L.L. at that point . Any tip that touches on those results are welcome, as well as intuitive advice. I've been looking for an answer on the trails of the one on this post: Lipschitz continuity of $\frac{1}{x}$ and $x^2$ (the $1/x$ part of the accepted answer, using the definition). Edit: not duplicate of Lipschitz continuity of $x\cdot\sin(1/x)$ , that regards about global Lipschitz continuity of the same function. The question addressed here is about local Lipschitz continuity at a specific point.","['real-analysis', 'continuity', 'general-topology', 'lipschitz-functions', 'metric-spaces']"
1939227,How to show simply that a field is a vector space over any of its subfields?,"I know that any field is a vector space over itself, see e.g. here -- Prove that the field F is a vector space over itself. Is there a correspondingly simple argument showing that this is the case for any subfield? For example, $\mathbb{C}$ can be thought of as a real vector space. The result in general is true, e.g. here https://cims.nyu.edu/~kiryl/Algebra/Section_5.1--Extension%20Fields.pdf To me the result is ""clear"" or ""obvious"", but when I go about trying to prove it, I cannot think of anything less clunky than to verify all of the scalar multiplication vector space axioms one by one (the additive axioms follow immediately from the fact that any field is an abelian group under addition), which means that I don't really gain anything from the observation. The example I have in mind is the field of rational functions over a projective curve $V(P) \subset \mathbb{CP}^2$; I want to argue that since all complex scalars $\lambda$ are polynomials hence rational functions on $V(P)$, that the field of rational functions is not only a vector space over itself, but also a complex vector space. Then given any divisor $D$, I can show that $L(D)$ is a complex vector space just by showing that it is closed under addition and scalar multiplication.",['abstract-algebra']
1939264,A ring with infinitely many maximal ideals,I am looking for a commutative ring with 1 and infinitely many maximal ideals such that for any infinite family $\{m_i\}_{i\in I}$ of maximal ideals there exists $ j \in I$ such that  $m_j\subseteq \cup_{j \not= i\in I}m_i$.,"['abstract-algebra', 'maximal-and-prime-ideals', 'algebraic-geometry', 'commutative-algebra']"
1939320,Why is this fact about homogeneous polynomials true?,"$\newcommand{\ord}{\operatorname{ord}}$ Let $V(P)$ denote a curve, i.e. a smooth projective variety of dimension one, over $\mathbb{C}$ , and let $\mathscr{K}$ denote the corresponding field of rational functions on $V(P)$ . To simplify notation, for any place (point) $p \in V(P)$ and divisor $D = \sum n_p p$ , we define $\ord_p(D) = n_p$ . (Paraphrased from here .) ... Then $$\ord_p(f+g) \ge \min( \ord_p(f), \ord_p(g))$$ for all $f,g \in \mathscr{K}^{\times}$ . Why does this inequality hold? It seems like a simple fact about homogeneous polynomials, but I have already spent more than a day trying to figure out why this is true and have failed. Version 2 of Question (Unnecessary to Read) $\newcommand{\div}{\operatorname{div}_{V(P)} }$ This question is based on Exercise 3.5.15 in Algebraic Geometry: A Problem Solving Approach by Garrity, Belshof, et al. Let $$ L(D) := \{ F \in \mathscr{K}(V(P)): F = 0\ \text{ or }\div(F) + D \ge 0 \} $$ ( see here , for example). If I have that $F, G \in L(D)$ , how do I show that $F+G \in L(D)$ ? This is necessary to show that $L(D)$ is a vector space and thus that the statement of the Riemann-Roch theorem makes sense. Another way to phrase this: How can one show that, where $I_p(\dots)$ denotes the intersection multiplicity at $p \in \mathbb{CP}^2$ , $$I_p(V(f_2 g_1 + f_1 g_2) \cap V(P)) - I_p(V(f_2 g_2) \cap V(P)) \ge \\ \min\{ I_p(V(f_1) \cap V(P))-I_p(V(f_2) \cap V(P)),\quad I_p(V(g_1) \cap V(P)) - I_p(V(g_2)\cap V(P))  \}?$$ Probably it is a corollary of Bezout's theorem somehow? Page 2 of this document [Proposition 1.7 (i)] says that closure under addition follows from the fact that $v_p (F+G) \ge \min\{ v_p(F), v_p(G) \}$ for any $F,G \in \mathscr{K}(V(P))$ and any $p \in V(P)$ . However I do not understand either why this is true or why it ""should"" be true. $v_p$ is supposed to denote something about ""valuation"" or ""discrete valuation"", I am not sure, since I have not taken a course in commutative algebra yet, it seems to be related to this , but there it is given as an axiom -- I am not sure how to show that the order of a zero/pole of a rational function is a ""discrete valuation"", although that seems to be what this comes down to. This page also makes the same claim (that the order of poles/zeros is a discrete valuation on a rational function field). Version 1 of Question (Unnecessary to Read) I get that the Riemann-Roch space is a subset of the complex vector space of rational functions (see my previous question ), therefore all I need to show is that it is closed under scalar multiplication and addition. The proof that it is closed under scalar multiplication is easy: $0 \in L(D)$ by definition, and for any $\lambda \in \mathbb{C} \setminus \{0 \}$ one has that $\div(\lambda F) = \div(F)$ where $F \in \mathscr{K}(V(P))$ . Question: How can one show that $L(D)$ is closed under addition? However, I do not understand at all how to show that it is closed under addition. I know that $\div(FG)=\div(F)+\div(G)$ (and that $\div(F^{-1})=-\div(F)$ ), but this doesn't tell me anything I need to know about $\div(F + G)$ . Specifically, if $F= \frac{f_1}{f_2}, G= \frac{g_1}{g_2}$ , then $$F+G = \frac{g_2 f_1 + f_2 g_1 }{f_2 g_2}.$$ What can we say about the zeros of the sum of two distinct polynomials? This is apparently a non-trivial issue, see e.g. https://mathoverflow.net/questions/30072/roots-of-sum-of-two-polynomials , http://www.sciencedirect.com/science/article/pii/S0022247X02000458 1. As far as I can tell (and I am probably incorrect), the zero set of $g_2f_1 + f_2g_1$ , denoted $V(g_2 f_1 + f_2 g_1)$ , is equal to $$V(g_2 f_1) \cap V(f_2 g_1). $$ Note that I am using intersection in the sense of generalized multisets, where an element can not only occur multiple times (a zero with multiplicity), but also appear negatively many times (a pole). I can not figure out a simple way to translate intersection into the arithmetic of divisors. For instance, if $D_1$ corresponded to $V(a)\cap V(P)$ and $D_2$ corresponded to $V(b) \cap V(P)$ , then $D_1 + D_2$ would correspond to $(V(a)\cap V(P))\cup(V(b) \cap V(P))= (V(a) \cup V(b)) \cap V(P)$ , i.e. the addition of divisors corresponds to the union of zero sets and pole sets and thus to the multiplication of rational functions. Since multiplication between divisors is undefined, nothing seems to correspond to the intersection of zero and pole sets, and thus to the addition of rational functions. (My thinking in terms of indicator functions due to exposure to probability theory may be hampering me here; however, it is supposedly a natural way to understand multisets .) 2. This webpage here says that closure under addition should follow immediately from the fact $$\div(F +G) \ge \min \{ \div(F) + \div(G)  \} $$ (specifically they use a term by term comparison for each point $p$ ). However, I don't see at all how this fact is true, or why it should be true. In fact, I would expect an inequality in the other direction to hold, since $V(a) \cap V(b) \subset V(a), V(a) \cap V(b) \subset V(b)$ . Defintions: $V(P) \subset \mathbb{CP}^2$ means the zero set of a homogeneous polynomial. I would suppose that in particular the $V$ stands for ""variety"". $F$ and $G$ are (equivalence classes of) rational functions in the field of rational functions $\mathscr{K}(V(P))$ corresponding to $V(P)$ . $D$ denotes a divisor. $\div (F)$ denotes the divisor generated by the function $F \in \mathscr{K}(V(P))$ with respect to $V(P)$ ; specifically if $F =\frac{f_1}{f_2}$ , where $f_1$ and $f_2$ are homogeneous polynomials of the same degree, then $$\div (F) = \sum_{p \in V(f_1) \cap V(P)} n_p p  -\sum_{p \in V(f_2) \cap V(P)} n_p p  $$ where $n_p$ is the order/multiplicity of the zero of $f_1$ , $f_2$ respectively -- note of course that zeroes of $f_2$ correspond in particular to poles of $F$ . Finally, $L(D)$ , which I think is sometimes called a ""Riemann-Roch space"", is the set of functions in $\mathscr{K}(V(P))$ defined to be $$L(D) := \{ F \in \mathscr{K}(V(P)): F = 0 \quad or \quad \div(F) + D \ge 0 \} $$","['valuation-theory', 'algebraic-geometry', 'abstract-algebra', 'roots', 'algebraic-curves']"
1939368,Number of non-negative whole number solutions if the coefficients of the variables are not equal to $1$,We know how to find the number of non-negative whole number solutions of the equation $x_1+x_2+x_3+...x_r=n$.It is given by $\binom{n+r-1}{r}$ by the method of stars and bars. But how to find the number of non-negative whole number solutions if the coefficients of the variables are not equal to $1$ for all. For example I take an equation like $2x_1+3x_2+5x_3=100$.How to find number of whole number solutions in this case? Is there any combinatorial way?Is there there any way to use binomial theorem/generating functions to solve this problem?,"['combinatorics', 'binomial-theorem', 'elementary-number-theory']"
1939381,Is the set of sinusoidal functions with period $2\pi$ closed under addition?,"Let $w: \mathbb{R} \to \mathbb{R}$ be an element of B iff there exist $A \in \mathbb{R}_{ \geq 0}$ and $\phi \in [0, 2\pi)$ such that $w(t) = A \sin(t+\phi)$. Is B a linear subspace of $\mathbb{R}^{\mathbb{R}}$ ? By a theorem , B is a subspace of $\mathbb{R}^{\mathbb{R}}$ iff: The zero function is in B ; If $w_{1} (t)$ and $w_{2} (t)$ are elements of B, then $w_{1} (t) + w_{2} (t)$ is an element of B, too ; If $w (t)$ is an element of B and $c(t)$ is a scalar function of $\mathbb{R}^{\mathbb{R}}$, then $(c \cdot w) (t) $ is an element of B as well. I'd like to verify the second point. So I define $w_{1} (t) := A_{1} \sin (t + \phi_{1})$ and $w_{2} (t) := A_{2} \sin( t + \phi_{2})$ and see if we can find an $A_{3}$ and $\phi_{3}$ such that $w_{1} (t) + w_{2} (t) = A_{3} \sin (t + \phi_{3} ) \qquad (3)$. We can rewrite the left side of $(3)$ by writing it as $$ A_{1} (\sin(t)\cos(\phi_{1} 
) + \cos(t)\sin(\phi_{1})) + A_{2} (\sin(t)\cos(\phi_{2}) + \cos(t)\sin(\phi_{2})) \quad .$$ Furthermore, we can rewrite the right side in a way that it is almost similar: (provided we set $A_{3} = A_{1} + A_{2}$)  $$ A_{1} (\sin(t)\cos(\phi_{3}) + \cos(t)\sin(\phi_{3})) + A_{2} (\sin(t)\cos(\phi_{3}) + \cos(t)\sin(\phi_{3})) \quad.  $$ However, I'm not sure I've shown closure under addition now. Because the way I did it, the two expressions are only equal when $\phi_{1} = \phi_{3}$ and $\phi_{2} = \phi_{3}$, so $\phi_{1} = \phi_{2}$. But I thought equality ought to be able to hold for arbitrarily chosen $\phi_{1}$ and $\phi_{2}$, so not necessarily when they're equal. Have I shown additivity now? If not, can you help me to do it?","['trigonometry', 'linear-algebra']"
1939394,What is the value of the nested radical $\sqrt[3]{1+2\sqrt[3]{1+3\sqrt[3]{1+4\sqrt[3]{1+\dots}}}}$?,"The closed-forms of the first three are well-known, $$x_1=\sqrt{1+\sqrt{1+\sqrt{1+\sqrt{1+\dots}}}}\tag1$$
$$x_2=\sqrt[3]{1+\sqrt[3]{1+\sqrt[3]{1+\sqrt[3]{1+\dots}}}}\tag2$$
$$x_3=\sqrt{1+2\sqrt{1+3\sqrt{1+4\sqrt{1+\dots}}}}\tag3$$
$$x_4=\sqrt[3]{1+2\sqrt[3]{1+3\sqrt[3]{1+4\sqrt[3]{1+\dots}}}}=\;???\tag4$$ with $x_1$ the golden ratio , $x_2$ the plastic constant , and $x_3=3\,$ (by Ramanujan). Questions: Trying to generalize $x_3$, what is the value of $x_4$ to a $100$ or more decimal places? (The Inverse Symbolic Calculator may then come in handy to figure out its closed-form, if any.) What is the Mathematica command to compute $x_4$? P.S. This other post is related but  only asks for its closed-form which resulted in speculation in the comments. (A method/code to compute $x_4$, and a verifiable numerical value is more desirable.)","['number-theory', 'recursion', 'constants', 'numerical-methods', 'nested-radicals']"
1939440,Proving that an operator is closed,"This is a question about the notes located at https://www.math.univ-toulouse.fr/~raymond/book-ficus.pdf In section 6.2.2, theorem 6.2.3, the author would like to prove that $(A,D(A))$ is the infinitesimal generator of a semigroup of contractions on $Y = L^2(\Omega) \times H^{-1}(\Omega)$, $D(A) = H_0^1(\Omega) \times L^2(\Omega)$, where $A$ is defined by $$A(y_1,y_2) = (y_2, \tilde{A}y_1),$$ and where $\tilde{A}$ is defined by its action as $$(\tilde{A}y_1, \xi) = - \int \nabla y_1\cdot \nabla (-\Delta)^{-1}\xi,$$ in the usual sense for $(-\Delta)^{-1}$. It must be shown that the operator is dense and closed.  No proof is given for this. Density seems to be immediate.  For closed, I tried to follow the proof from the previous page.  Let $(y_{1n},y_{2n})$ converge to $(y_1,y_2)$ in $H_0^1(\Omega)\times L^2(\Omega)$, and $A(y_{1n},y_{2n})$ converge to some $(f,g) \in L^2 \times H^{-1}$. Because $\nabla$ is bounded from $H_0^1 \mapsto L^2$, and bounded operators preserve (weak) convergence, by the definition, we have that $y_2 = f$, and $\tilde{A} y_1 = g$.  But then how can we show convergence of $y_{1n}$?  We would need a Cauchy estimate like $$||y_{1n}-y_{1m}||_{H_0^1} \leq C||\tilde{A}y_{1n} - \tilde{A}y_{1m}||_{H^{-1}}.$$
Is this somehow immediately true and I'm missing something? Edit:  The second part of this doesn't make sense either.  The author tries to prove the resolvent operator bound by using the $\Delta$ operator, but of course we must use $\tilde{A}$.  Is there a reason we can use $\Delta$ instead of $\tilde{A}$?","['functional-analysis', 'sobolev-spaces', 'partial-differential-equations']"
1939449,Limit of an integral (SEEMOUS 2014 problem 4),"Please show me how to solve this problem. I tried to make the substitution $x/n=y$ but I don't know how to go further. (a) Prove that $$\lim_{n\to\infty}n\int_0^n\frac{\arctan\frac{x}{n}}{x(x^2+1)}\,dx=\frac{\pi}{2}$$ (b) Find the limit $$\lim_{n\to\infty}n\left(n\int_0^n\frac{\arctan\frac{x}{n}}{x(x^2+1)}\,dx-\frac{\pi}{2}\right)$$","['integration', 'convergence-divergence', 'calculus', 'analysis']"
1939505,Prove that $\sum_{k=0}^nk{m+k \choose m}=n{m+n+1\choose m+1}-{m+n+1 \choose m+2}$,"Can someone please see the work I have so far for the following proof and provide guidance on my inductive step? Prove that if $m,n\in\mathbb{N}$, then $\sum_{k=0}^nk{m+k \choose m}=n{m+n+1\choose m+1}-{m+n+1 \choose m+2}$ Base Case. Let n=0. Then $\sum_{k=0}^{n=0}k{m+k \choose m}=0$ and $n{m+n+1\choose m+1}-{m+n+1 \choose m+2}=0-0=0$. Thus for $n=0$ our equation is satisfied. Inductive Step. Let $n\ge0$. Assume $\sum_{k=0}^nk{m+k \choose m}=n{m+n+1\choose m+1}-{m+n+1 \choose m+2}$. Now observe that $$\begin{align*}\sum_{k=0}^{n+1}k{m+k \choose m}&=\\
\sum_{k=0}^{n}k{m+k \choose m}+(n+1){m+n+1\choose m}&=n{m+n+1\choose m+1}-{m+n+1 \choose m+2}+(n+1){m+n+1\choose m} \end{align*}$$ This is where I'm getting stuck... using Pascal's Identity to combine some of these terms seems ideal. However, the factors of $n$ and $n+1$ are making a clever manipulation difficult for me.","['binomial-coefficients', 'proof-verification', 'induction', 'combinatorics', 'summation']"
1939513,"""one-to-one correspondence""(bijection) and the size of two infinite sets","I know little about theory, so the following maybe a stupid question . ""one-to-one correspondence""(bijection) is a good method  to judge whether two finite sets have the same size , while a lot of mathematical rules become invalid in the world of infinite , so 
(1).how Cantor make it sure that bijection of two infinite sets can also ensure the two infinite sets have the same size(or the same number of elements) ? 
(2). What ""one-to-one correspondence""(bijection) is used for in set theory ? P.S. The ""one-to-one correspondence""(bijection) caused many unexpected surprising result with infinite sets  , maybe wrong result from my perspective , like all positive integers and its subset all positive odd numbers have the same cardinality, the same size(or the same number of elements), however, lots of people think  the number of positive integers should be twice of  its subset all positive even numbers before accepted Cantor's conclusion about this.  Counter-intuitive result caused when comparing the size of two infinite sets using bijection cannot assert comparing the size of two infinite sets using bijection is wrong , but I think if we find another standard to compare the size of two infinite sets and also make the result  useful and accord with our intuition, that would be much better .",['elementary-set-theory']
1939518,How many distinct terms are there in $(1+x^{13}+x^7)^{100}$? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question How many distinct terms are there in $(1+x^{13}+x^7)^{100}$?
I can't seem to make any real progress.","['number-theory', 'combinatorics']"
1939571,Hartshorne exercise 1.8,"Question : Let $Y$ be an affine variety of dimension $r$ in $\mathbb A^n$ . Let, $H$ be a hypersurface in $\mathbb A^n$ and assume that $Y\nsubseteq H$ . Then every irreducible component of $Y\cap H$ has dimension $r-1$ . Answer :Let, $V$ be an irreducible component of $Y\cap H$ . Then, $\dim V=\dim k[x_1,\cdots ,x_n]/\mathcal{I}(V)$ . Now, $V\subseteq Y\cap H\subseteq Y$ . This implies, $\mathcal{I}(V)\supseteq\mathcal{I}(Y\cap H)\supseteq\mathcal{I}(Y)$ . So, \begin{align*}
\dim V &=\dim\frac{\frac{k[x_1,\cdots ,x_n]}{\mathcal{I}(Y)}}{\frac{\mathcal{I}(V)}{\mathcal{I}(Y)}}\\
&=\dim\frac{k[x_1\cdots ,x_n]}{\mathcal{I}(Y)}-\text{ht}\frac{\mathcal{I}(V)}{\mathcal{I}(Y)}\\
&=r-\text{ht}\frac{\mathcal{I}(V)}{\mathcal{I}(Y)}
\end{align*} Here, $\mathcal{I}(V)/\mathcal{I}(Y)$ is a prime ideal of the finitely generated $k$ algebra $k[x_1,\cdots ,x_n]/\mathcal{I}(Y)$ , which is also a domain as $V$ is a closed irreducible subset of $Y\cap H$ and hence of $X$ . $\underline{\text{Claim}}$ : ht $\mathcal{I}(V)/\mathcal{I}(Y)=1$ Proof: $k[x_1,\cdots ,x_n]/\mathcal{I}(Y)=B$ is a Noetherian integral domain. Let, $H=Z(g)$ and $Y=Z(f_1,\cdots ,f_k)$ . If $g\in\mathcal{I}(Y)$ then $H=Z(g)\supseteq Z(\mathcal{I}(Y))=\overline{Y}=Y$ , which is a contradiction to the given hypothesis. Hence, $g\notin\mathcal{I}(Y)$ . So, ( $\overline{0}\neq) \overline{g}$ is not a zero divisor in the integral domain $k[x_1,\cdots ,x_n]/\mathcal{I}(Y)$ . Now, $V\subseteq Z(g)$ which implies $\mathcal{I}(V)\supseteq\mathcal{I}(Z(g))=(g)$ . Hence, $g\in\mathcal{I}(V)$ . If $\overline{g}$ is a unit in $B$ , then $g-1\in\mathcal{I}(Y)\subseteq\mathcal{I}(V)$ . But, $g\in\mathcal{I}(V)$ which implies $1\in\mathcal{I}(V)$ and this is a contradiction. So, $\overline{g}$ is not a unit in $B$ . Now, we want to show that $\mathcal{I}(V)/\mathcal{I}(Y)$ is a minimal prime ideal containing $(\overline{g})$ . Any prime ideal in $B$ is of the form $\mathfrak p /\mathcal{I}(Y)$ where $\mathfrak p$ is a prime ideal in the polynomial ring containing $\mathcal{I}(Y)$ . Now Let, $\overline{g}\in\mathfrak p /\mathcal{I}(Y)$ where $\mathfrak p /\mathcal{I}(Y)\subseteq\mathcal{I}(V)/\mathcal{I}(Y)$ . Then, $g\in\mathfrak p\subseteq\mathcal{I}(V)$ . This implies, $Z(\mathfrak p)\supseteq Z(\mathcal{I}(V))=V$ . Now, $(g)\subseteq\mathfrak p$ and also $\mathcal{I}(Y)\subseteq \mathfrak p$ . Hence, $H=Z(g)\supseteq Z(\mathfrak p)$ and $Y=Z(\mathcal{I}(Y))\supseteq Z(\mathfrak p)$ . This implies, $Y\cap H\supseteq Z(\mathfrak p)$ . As $Z(\mathfrak p)$ is irreducible and $V$ is an irreducible component of $Y\cap H$ and $Z(\mathfrak p)\supseteq V$ , we have $Z(\mathfrak p)=V$ . This implies $\mathfrak p =\mathcal{I}(Z(\mathfrak p))=\mathcal{I}(V)$ . Hence, $\mathcal{I}(V)/\mathcal{I}(Y)$ is a minimal prime ideal containing $(\overline{g})$ . By Krull Hauptidealsatz ht $\mathcal{I}(V)/\mathcal{I}(Y)=1$ . So, we get $\dim V=r-1$ . Please check my proof. Also, it will be very helpful if I get some ""geometric"" or any other point of view about this exercise. Thank you very much.","['commutative-algebra', 'algebraic-geometry', 'proof-verification']"
1939572,Outer measure of rationals between 0 and 1. Royden.,"I've been studying Royden and I have found something that looks like a contradiction to me, and can't find where it is that my reasoning is wrong. On Chapter 3 he defines the outer measure $m^*$ of a set $A$ as: $m^*A = \inf_{A \subset I_n} \sum l(I_n)$ where $\{I_n\}$ is a countable collection of open intervals, and $l$ is the length. What troubles me is that corollary 3 on chapter 3 says: ""If $A$ is countable, $m^*A = 0$"". And right bellow problem 5 states: ""Let $A$ be the set of rational numbers between 0 and 1, and let $\{I_n\}$ be a finite collection of open intervals covering $A$. Then $\sum l(I_n) \geq 1$"". My question is, how can every open covering of the rationals between 0 and 1 have length grater or equal to 1 if at the same time, by virtue of being countable, the outer measure of the rationals is 0 (corollary 3) and hence: $m^*(\mathbb{Q}\cap[0,1]) = \inf_{\mathbb{Q} \cap [0,1] \subset I_n}\sum l(I_n) = 0$. Isn't there a contradiction in saying $\inf_{\mathbb{Q} \cap [0,1] \subset I_n} \sum l(I_n) = 0$, and for all $\{I_n\}$, $\sum l(I_n) \ge 1$?","['outer-measure', 'real-analysis']"
1939607,How to solve $x+\sin(x)=b$,"How can I solve $x+\sin(x)=b$ for $x \in [0,π]$? We take $b \in [0,π]$. I don't know how to find the solution.",['trigonometry']
1939624,Derivation of the Curl formula in cartesian coordinates.,"By calculating the circulation per area of a vector field $$F(x,y,z) = F_x(x,y,z)\vec{x} + F_y(x,y,z)\vec{y} + F_z(x,y,z)\vec{z}$$ in a small rectangle around $(x_0, y_0, z_0)$ on the $xy$ plane, it can be shown the limit as the sides of the rectangle approach zero is $$\left(\frac{\partial F_y(x_0, y_0, z_0)}{\partial x} - \frac{\partial F_x(x_0, y_0, z_0)}{\partial y}\right)$$ The same calculation however is not that straightforward if the rectangle does not lie in the $xy$, $yz$, or $xz$ planes. Now if $\vec{n}$ is the normal of the plane, I thought that by performing a change of basis such that $\vec{n} \rightarrow \vec{z'} $ and by following the previous calculations we could show that the limit of the circulation per area is $$ \left(\frac{\partial F_{y'}(x'_0, y'_0, z'_0)}{\partial x'} - \frac{\partial F_{x'}(x_0, y_0, z_0)}{\partial y'}\right) $$ This is also the inner product of the curl of the vector field and the normal $\vec{n}$ As such the two should be equal: $$\left(\frac{\partial F_{y'}(x'_0, y'_0, z'_0)}{\partial x'} - \frac{\partial F_{x'}(x'_0, y'_0, z'_0)}{\partial y'}\right) = \\
\left[\left(\frac{\partial F_z(x_0, y_0, z_0)}{\partial y} - \frac{\partial F_y(x_0, y_0, z_0)}{\partial z} \right)\vec{x} +
\left(\frac{\partial F_z(x_0, y_0, z_0)}{\partial x} - \frac{\partial F_x(x_0, y_0, z_0)}{\partial z} \right)\vec{y} + 
\left(\frac{\partial F_y(x_0, y_0, z_0)}{\partial x} - \frac{\partial F_x(x_0, y_0, z_0)}{\partial y} \right)\vec{z}\right] \cdot \vec{n} $$
I've been trying to prove the above equality for some time without success, specifically I am not sure how to handle the transformations correctly. Any help with this is much appreciated!","['multivariable-calculus', 'linear-algebra', 'differential-topology', 'derivatives']"
1939631,Use moment generating function to show that $f_Y(y)$ has chi squared distrubution,"Problem in my Stat class Let $Y_1, Y_2,\ldots, Y_n$ be arandom sample of size $n$ from $$f_Y(y)= \frac{1}{\theta } e^{-y/\theta}, \qquad y> 0,\theta > 0$$ Use moment-generating functions to show that the ratio $2n\bar{y}/\theta$ has a chi square distribution with $2n$ degrees of freedom. First question do they mean that $2n\bar{y}$ is the $y$ variable and $\theta$ is the $\theta$ variable? secondly how do you show that anything has a chi squared distribution? Small hints to guide me in the right direction would also be helpful.",['statistics']
1939643,Trying to understand existence theorem on Rudin's Principles,"I'm reading Rudin's Principles of Mathematical Analysis . It is said in the book, 1.19 Theorem (pg-8): ""there exists an ordered field $\mathbb{R}$ which has the least-upper-bound property. Moreover, $\mathbb{R}$ contains $\mathbb{Q}$ as a subfield"". 
So $\mathbb{Q}\subset\mathbb{R}$. But in the proof of the theorem (pg-17), it is said that the members of $\mathbb{R}$ will be certain subsets of $\mathbb{Q}$. That is to me a contradiction, I mean, if the members of $\mathbb{R}$ will be certain subsets of $\mathbb{Q}$, it can't be that $\mathbb{Q}\subset\mathbb{R}$. ¿What is the explanation for that? Thanks.","['real-analysis', 'real-numbers', 'rational-numbers', 'elementary-set-theory', 'analysis']"
1939693,Intuition on why the density function of a normal law is$\frac{1}{\sqrt{2\pi \sigma^2 }}e^{-\frac{(x-\mu)^2}{2\sigma^2 }}$,"What is the intuition of the fact that the density function of a normal law $\mathcal N(\mu,\sigma ^2)$ is given by $$f(x)=\frac{1}{\sqrt{2\pi \sigma^2 }}e^{\large-\frac{(x-\mu)^2}{2\sigma^2 }}?$$ It looks to come from nowhere. For example, the intuition of a poisson law is in fact very natural considering rare event (and binomial law). For the normal law, I really have no idea of the intuition behind, neither why it's so common in the nature ! How did we get to this (incredible) result. How does it work ?","['probability', 'normal-distribution']"
1939760,Möbius strip with multiple twists,"The Möbius strip is obtained from a closed band by fixing one end, taking a half twist of the other end and gluing the ends together. I wonder what happens, if one doesn't just take a half twist but multiple (half) twists of the one end. Questions: Is the resulting space homeomorphic to the classical Möbius strip ? If they are not homeomorphic, are they homotopy equivalent ?","['general-topology', 'homotopy-theory']"
1939769,Showing that two scalar random variables are independent when only their distributions are known,"Suppose that $X$ and $Y$ are two scalar random variables, and we know only their distributions. Is there some general strategy to show whether or not these two random variables are independent? In particular we have
$$\mathbb{P}(X \leq x) = \begin{cases} 1 - e^{-2x}, & x \geq 0, \\ 0 , & x < 0, \end{cases}$$
and
$$ \mathbb{P}(Y \leq y) = \begin{cases} 1 - e^{-y}, & y \geq 0, \\ 0 , & y < 0. \end{cases}$$
I feel as if I am missing something extremely trivial. If I knew the joint density or the distribution of the random vector $(X,Y)$ I could check whether
$$ \mathbb{P}((X,Y) \in (-\infty, a] \times (-\infty, b]) = \mathbb{P}(X \leq a) \mathbb{P}(Y \leq b) \ .$$
However I am not given this information. Any ideas? I feel as if what I am asking is impossible.","['probability-theory', 'probability', 'probability-distributions']"
1939906,Can the magnitude of a vector sum ever equal the sum of the magnitudes?,"I'm currently taking a university course in Linear Algebra and Matrix Theory. A recent problem set included a question that asked, What can you say about two nonzero vectors $\vec{\alpha}$ and $\vec{\beta}$ that satisfy the equation:
  $$\|\vec{\alpha}+\vec{\beta}\| \ = \ \|\vec{\alpha}\| + \|\vec{\beta}\| \  $$
  $$\vec{\alpha},\vec{\beta} \in \mathbb{R}^n$$ I am attempting to solve this by finding a solution from this equation derived from the law of cosines:
$$\|\vec{\alpha}+\vec{\beta}\|^2 \ = \ \|\vec{\alpha}\|^2 + \|\vec{\beta}\|^2 - \ 2\|\vec{\alpha}\| \|\vec{\beta}\|\cos(\pi-\theta)$$
...so far I have been unable to find a valid solution and am tempted to assert that there exists no $\vec{\alpha}$ and $\vec{\beta}$ for which that equation is true. Is there any case in which the magnitude of the sum of two vectors equals the sum of the magnitudes?","['linear-algebra', 'vectors']"
1939935,Counterexample to the Gronwall lemma,"Gronwall Lemma. Let $I := [x_0,x_1]$, $x_1, > x_0$, $a,b \in \mathbb{R}$ where $b \geqslant 0$, $y \in C(I)$, such that $$y(x) \leqslant a + b\int_{x_0}^x y(t) dt$$ holds for all $x \in I$. Then $$y(x) \leqslant a\exp\left( b(x - x_0)\right)$$ for all $x \in I$. Now I am asked to provide a counterexample if $b < 0$. My question is, how do I generally approach such a task? I think, I try some easy functions like $y \equiv 1$ or $y(x) = -x$, but somehow it did not work. Thanks for any hint.","['real-analysis', 'examples-counterexamples', 'ordinary-differential-equations']"
1939952,Complex sequence $(z_i)$ such that $\sum_i z_i^k$ converges and $\sum_i \vert z_i \vert^k$ diverges for all $k$,"How to find a complex sequence $(z_i)$ such that
$$\sum_i z_i^k \text{ converges for all } k \in \mathbb N$$ but
$$\sum_i \vert z_i \vert^k \text{ diverges for all } k \in \mathbb N ?$$","['conditional-convergence', 'complex-analysis', 'sequences-and-series', 'convergence-divergence']"
1939964,Closed sets in the lower limit topology.,"Would an interval of the form $[a,b]$ be closed in the lower limit topology $\mathbb{R}_\ell$. Here is why I think it is: Because $\mathbb{R}_\ell$ is finer than the standard topology on $\mathbb{R}$, then all the basis elements of this standard topology are in the lower limit topology; i.e., the sets $(a,b)$ are open in $\mathbb{R}_\ell$. Therefore, $\mathbb{R} - [a,b] = (- \infty, a) \cup (b, \infty)$ $= (\bigcup_{x_1 < a} (x, a) )~ \cup ~ (\bigcup_{x_2 > b} (b,x_2)$, which is a union of open sets. Therefore $\mathbb{R} - [a,b]$ is open and hence $[a,b]$ is closed So, is this argument correct?",['general-topology']
1939968,Fixed-point iteration and continuity of parameters,"Let $X$ a compact set and $A\subseteq \mathbb{R}$. Consider a continuous function $f\colon X\times A\to X$ and construct a fixed-point iteration as follows
$$
x_{k+1}=f(x_k,a),\quad x_0\in X, a \in A.\quad (\star)
$$ My question : If $(\star)$ admits a unique fixed point, denoted by $\mathrm{Fix}(f_a)$, for all $a\in A$, can we conclude that $\mathrm{Fix}(f_a)$ is a continuous function of $a$? What can be said in case $(\star)$ admits a set of fixed points for all $a\in A$? Thanks for your help. Comments. This question is different from this one . Indeed, here $f$ is not assumed to be contractive.","['functional-analysis', 'fixed-point-theorems', 'real-analysis', 'sequences-and-series']"
1939973,"Suppose $0< a,b,c < 1$ and $ab + bc + ca = 1$. Find the minimum value of $a + b + c + abc$.","Suppose $0< a,b,c < 1$ and $ab + bc + ca = 1$. Find the minimum value of $a + b + c + abc$. How can I use the first two equations to help solve the third?  I'm stuck.  Any solutions are greatly appreciated!","['algebra-precalculus', 'inequality']"
1940144,"structural (algebraic) sheaf on a Riemann surface ""inside"" the sheaf of holomorphic functions","This question consists of $3$ points, and each of them deals with the relationship between the ""algebraic"" structure on a projective curve and its ""analytic"" structure. I know that this argument has been made explicit by Serre's GAGA, but here I'd like to understand some elementary concepts. Let $(X,\mathscr O_X)$ an irreducible, smooth projective algebraic curve over $\mathbb C$. Then $X$ is also a Riemann surface equipped with the sheaf of holomorphic functions $\mathscr O_X^{\operatorname{an}}$. Every Zariski open set $U$ is also an open set in the strong topology and we have an embedding: 
$$\mathscr O_X(U)\subset\mathscr O^{\operatorname{an}}_X(U)$$ First of all I'd like to visualize each section $s\in\mathscr O_X(U)$ as an holomorphic map on $U$, so please tell me if the following argument is correct: for each point $x\in U $ consider the natural map $x\mapsto s_x+\mathfrak m_x\in k(x)=\mathbb C$, which sends a point to the image of $s$ in the residue field at $x$. This map should be holomorphic, one can check it by working on charts. We have also $\mathscr O_{X,x}\subseteq \mathscr O^{\operatorname{an}}_{X,x}$ so every element $s_x$ can be seen as a germ of holomorphic functions at $x$. Now consider the completion $\widehat{\mathscr O_{X,x}}$ with
respect the valuation $v_x$ associated to a closed point in $x$.
What is the relationship between $\widehat{\mathscr O_{X,x}}$  and
$\mathscr O^{\operatorname{an}}_{X,x}$? Roughly speaking I have the following idea:
note that $\widehat{\mathscr O_{X,x}}\cong \mathbb C[[t]]$ so here
it seems that we are considering the holomorphic functions in
$\mathscr O_{X,x}$ plus the functions with eliminable
discontinuity at $x$. One can repeat the same reasoning with the field of rational functions $K(X)$ (i.e. the meromorphic functions on $X$). What is its completion $K(X)_x=\operatorname{Frac }(\widehat{\mathscr O_{X,x}})\cong \mathbb C((t))$ in the framework of meromorphic functions? It seems that we are considering germs of meromorphic functions with a fixed pole in $x$.","['riemann-surfaces', 'algebraic-curves', 'sheaf-theory', 'algebraic-geometry', 'manifolds']"
1940170,Type of singularity for $\tan(z)$ at $z = \frac{\pi}{2}$,"Let $f(z) = \tan(z)$. I want to know what happens at $z=\frac{\pi}{2}$. More specifically, I want to know what type of singularity it is. I suspect it to be a pole of order $1$, but how do I show this? Do I write out the Laurent series of $\tan(z)$ and just observe? Or should I look at $\lim (z - \frac{\pi}{2})\tan(z)$?",['complex-analysis']
1940256,Proving $\sqrt{6}$ is not part of a field,"I'm currently in a beginning analysis course, and I am asked to prove that $F =\{a+b\sqrt2 +c\sqrt3 :a,b,c∈Q\}$ is not a field. I know that this violates the first multiplication axiom, that if $x,y \in F$ then $xy \in F$. However, I don't know how to prove that $\sqrt6$ cannot be written in the form $a+b\sqrt2 +c\sqrt3,$ where $a,b,c∈Q$. Is there a way to show this using elementary algebra, and not go into field extensions?","['real-analysis', 'field-theory']"
1940261,show that $ \limsup n\; | \;\{ (n+1)^2 \sqrt{2}\} - \{ n^2 \sqrt{2}\}\; | = \infty $,"Here is a theorem from Kuipers-Neiderreiter : If $\{ x_n \}$ is a sequence uniformly distributed mod 1, then $\overline{\lim} n |x_{n+1} - x_n| = \infty$ I'm not 100% sure what this means so let's put an equidistibuted sequence $\{ n^2 \sqrt{2}\}$ (or any irrational number, $\sqrt{7}$ etc) the theorem says:
$$ \limsup \hspace{0.0625in}n\; \Big| \;\{ (n+1)^2 \sqrt{2}\} - \{ n^2 \sqrt{2}\}\;\Big| = \infty $$ The proof would proceed by contradiction.  If the limsup were finite... $ | e^{2 \sqrt{2}\pi i \, (n+1)^2} - e^{2 \sqrt{2}\pi i \, n^2} | \leq 2\pi \Big| \;\{ (n+1)^2 \sqrt{2}\} - \{ n^2 \sqrt{2}\}\;\Big| = O(\frac{1}{N}) $ Weyl's equidistribution has that the average is zero: $\displaystyle \frac{1}{N}  \sum_{n=0}^{N-1} e^{2\pi i \, n^2\sqrt{2} } \to 0$ And by the Littlewood Tauberian Theorem $e^{2\pi i \, n^2 \sqrt{2}}\to 0$ but these numbers all have magnitude $1$. Littlewood Theorem If $\sum a_n x^n \to s$ as $x \to 1$ and $a_n = O(\frac{1}{n})$ then $\sum a_n \to s$ This argument is a proof by contradiction uses a Tauberian theorem does not use any features of the sequence $\{ n^2 \sqrt{2}\}$ is there an alternative proof that is more direct? That is not by contradiction or does not use Tauberian theory? A plot of $n \, \big( \{(n+1)^2 \sqrt{2}\} - \{ n^2 \sqrt{2}\} \big) $ for $0 < n < 10^6$.","['alternative-proof', 'number-theory', 'equidistribution', 'divergent-series', 'sequences-and-series']"
1940306,Taylor expansion of $\sqrt{f(x)}$ at $ x=0.$,"Let $f$ be a strictly positive function. Then we can compute the Taylor expansion of $\sqrt{f(x)}$ at zero. Wolfram alpha gives the first terms as $$\sqrt{f(x)}= \sqrt{f(0)} + \frac{f'(0)x}{2\sqrt{f(0)}}+...$$ Now, I was wondering if there is a closed representation of this as a power series? At first glance this does not look at hard, as we just apply chain and quotient-rule, but I could not come up with a closed equation.","['derivatives', 'real-analysis', 'taylor-expansion', 'functions', 'analysis']"
1940336,Is this true that $E(XY) =E(X) E(Y)$ implies the statistical independence of two normal random variables?,"Suppose $X$ and $Y$ are two normal variables and $$E(XY) = E(X) E(Y),$$ then is it true that $X$ and $Y$ are independent? I know that when $(X,Y)$ obeys bivariate normal distribution, the statement is true. But in general, I am not sure. If this statement is wrong, then is there some easy ways to prove the independence of two normal variables?","['probability', 'normal-distribution']"
1940363,Is there any difference between a parametric equation and a vector function?,"Perhaps it's due to the fact that I'm currently tackling introductory material on parametric equations, but it seems to me at the moment that there is no real difference between a set of parametric equations and a vector function. I'll give the standard example: Let's say I have a parametric curve defined by the parametric equations $x=\cos(t)$ and $y=\sin(t)$. What stops me from defining $r(t) = \left\langle  x, \  y   \right\rangle  = \left\langle  \cos(t), \  \sin(t)   \right\rangle$, which describes the exact same parametric curve (the unit circle) as the parametric equations for $x$ and $y$? Now I know assume that there has to be some difference between parametric equations and vector functions, but with the material I'm currently working with I can't seem to find a counter-example, or cases where they differ. I also realize that the concept of parameterization is critical to fields like Differential Geometry (based on what I've read so far in do Carmo's book ), and proofs of the big Integral Theorems (generalized Stokes' Theorem etc) rely on it, and this concept is something I want to understand rock solid. Can anyone give an example, as to why a set of parametric equations are different from vector functions of the form $f : \mathbb{R} \to \mathbb{R^m}$, and furthermore as to why they are so important to theorems in higher-dimensions?","['real-analysis', 'multivariable-calculus', 'parametric', 'differential-geometry', 'vectors']"
1940375,Does the exact sequence for reducing an elliptic curve over a local field split?,"Let $K$ be a local field  with uniformizer $\pi$ and residue field $k$ and let be $E$ an elliptic curve defined over $K$. We have the exact sequence of abelian groups $$0 \rightarrow E_1(K) \rightarrow E_0(K) \rightarrow \widetilde{E}_{\text{ns}}(k) \rightarrow 0$$ where $E_1(K)$ is the kernel of reduction mod $\pi$, $E_0(K)$ contains the points of nonsingular reduction, and $\widetilde{E}_\text{ns}(k)$ is the group of nonsingular $k$-rational points on the reduced curve $\widetilde{E}$. I'm aware of some examples where this sequence splits, but can anything be said in general?","['number-theory', 'elliptic-curves']"
1940384,"Prove that $\tan(x+\frac{\pi}8)>\mathrm{e}^x+\ln x$ for $x\in(0,\frac{3\pi}8)$","Prove that $$\tan\left(x+\frac{\pi}8\right)> \mathrm{e}^x+\ln x,\quad x\in\left(0,\frac{3\pi}8\right).$$ By plotting the graph I found that this is indeed true (in fact I found this inequality through plotting), but how can I prove this? There are various functions in this inequality and I don't know how to start. Any hints will be appreciated. Edit: I was suggested to post the graph (from WolframAlpha). This is the graph of $\tan(x+\frac{\pi}8)-\mathrm{e}^x-\ln x$ .","['real-analysis', 'inequality', 'trigonometry', 'calculus']"
1940401,How to determine if a Markov chain converge to equilibrium?,"For example, if there is a matrix$$ \begin{pmatrix} 
0 & 0 & 1 & 0 \\
0 & 0 & 0.5 & 0.5 \\
0.3 & 0.7 & 0 & 0 \\
1 & 0 & 0 & 0 \\
\end{pmatrix}$$
the stationary distribution is when $\pi_2$ = 1.07692$\pi_1$,$\pi_3$ = 1.53846$\pi_1$,$\pi_4$ = 0.53846$\pi_1$ So the stationary distribution exists. 
Does it mean that the matrix converges to equilibrium? I am learning this chapter on my own and I am quite confusing ... Thanks in advance!","['matrices', 'markov-chains', 'statistics', 'probability', 'convergence-divergence']"
1940444,Solve the equation $\cos^n(x) - \sin^n(x)=1$ [duplicate],"This question already has answers here : Solve $\cos^{n}x-\sin^{n}x=1$ with $n\in \mathbb{N}$. (3 answers) Closed 7 years ago . Solve the equation $\cos^n(x) - \sin^n(x)=1,n \in \mathbb{N}-\{0\}$ If $n$ is even then $\cos^n(x) = \sin^n(x)+1$ is only possible if $\sin(x)=0$ therefore the solution is $x=k\pi, k \in \mathbb{Z}$. I'm having problems with $n$ odd case. UPDATE For $n=1$ we have $\cos(x) - \sin(x)=1$ and by squaring we get $sin(x)cos(x)=0$ which leads to $x=k\pi$ or $x=\pm \frac \pi 2 + k\pi$. From these solutions, only $x=2k\pi$ and $x=- \frac \pi 2 + 2k\pi$ are valid. Also $x=2k\pi$ and $x=- \frac \pi 2 + 2k\pi$ are solutions for all odd $n$",['trigonometry']
1940450,"Infinite sum conundrum: $S_1 = 1 + 2 + \ldots$, $S_2 = \frac{1}{2} + 1 + 2 + \ldots$, $S_1-S_2$ finite or infinite?","$$S_1 = 1 + 2 + 3 + \ldots$$ $$S_2 = \frac{1}{2} + 1 + 2 + 3 + \ldots$$ Now case 1: $$\left.\Delta S = S_1 - S_2 = \frac{1}{2}\,\right\}\text{finite value}$$ Case 2: $$\Delta S = S_1 - S_2 = \frac{1}{2} + \underbrace{(1 + 1 + 1 + \ldots)}_{\infty\text{ value!}}$$ In one case, I see a finite  difference whereas in the other, an infinite. So, which one is right and why?","['algebra-precalculus', 'sequences-and-series', 'limits']"
1940455,Invertibility of Set Intersection Signed matrix,"I am interested in looking at the following problem in combinatorial matrix theory. Since I could not find a reference related to this types of matrices, I hope someone here could help me. Let $n \geq 1$ and $X=\{1, \ldots, n\}$. Assume that $\mathcal{F}(X)$ is the collection of all subsets of $X$. Now let $A$ be a matrix of size $2^n \times 2^n$ defined as follows: $A_{S_1, S_2}=(-1)^{|S_1 \cap S_2|}$, 
where $S_1, S_2 \in \mathcal{F}(X)$. Define $J_n$ as the matrix (size $n$) whose entries are $1$ everywhere.
Another related one is the matrix $B=(A+J_n)/2$, i.e., $B_{S_1, S_2}=1$ if $|S_1 \cap S_2|$ is even and $B_{S_1, S_2}=0$ if $|S_1 \cap S_2|$ is odd. Question : My main concern is whether $A$ or $B$ is invertible. Remark: We can define a matrix $C$ such that $C_{S_1, S_2}=|S_1 \cap S_2|$, where $S_1, S_2 \subseteq X$. This matrix $C$ is related to the incident matrix (see e.g., "" Matrices and set intersections "" by H. J. Ryser (1981))
Then our matrix $B$ can be regarded as the image of the matrix $J_n-C$ modulo $\mathbb{Z}_2$.","['matrices', 'combinatorics', 'linear-algebra', 'elementary-set-theory']"
1940493,Prove $(rs)^{t}=r^{t}s^{t}$ for a real number $t\gt 0$,"This lecture note proved $(rs)^{t}=r^{t}s^{t}$ for a real number $t\gt 0$ by using the following statement: $$\text{sup}(AB)=\text{sup}A\,\text{sup}B.$$ I found this specific step problematic For the property, if $r,s\gt 1$, then we can just use equation (1), noting that $\{(rs)^{a}:a\in\mathbb{Q}\,\text{and}\,a\le t\}=AB$, where $A=\{r^{a}:a\in\mathbb{Q}\,\text{and}\,a\le t\}$ and $B=\{s^{a}:a\in\mathbb{Q}\,\text{and}\,a\le t\}$. because $AB$ is defined as $\{ab:a\in A,b\in B\}.$ In other words, there are some elements in $AB$ where their exponents do not match, thereby not satisfying the above beautiful representation $(rs)^{a}.$ In conclusion, my question is Is my hypothesis right, which means the above proof does not hold?","['real-analysis', 'analysis', 'exponentiation']"
1940501,"About Hahn Banach, can I get an example problem outside of functional analysis?","I just started reading functional analysis and came across the Hahn-Banach theorem . Everywhere I looked stated this was very important and it is because it allows us to enlarge a dual space $X^+.$ I keep wondering to myself, so what ? (sorry for sounding rude). If we can enlarge the dual space of $X^+$ , then what? Why are the extensions of linear functionals so important? I mean for a subspace $Z \subset X$ , is there something important we need to know about $X-Z$ that we need to extend a linear functional $f: Z \to \mathbb{F}$ on? Is there an application or simple example of this outside of functional analysis that motivates the idea? Even something simple in Differential Equations would be insightful or a simple problem in Linear Algebra (actually this is probably dumb since I would imagine you would just extend the basis, so we have to give an example for a infinite dim space; maybe showing an analogue of Hahn-Banach in this setting would be neat and somewhat convincing ) Right now I just can't be bothered reading the full proof of something that I don't see the importance in and only to forget about it later... Thank you.","['functional-analysis', 'intuition', 'linear-algebra']"
1940518,What differentiates algebraic geometry over $\mathbb{C}$ from complex analysis?,"I have just begun learning algebraic geometry, and there are a lot more connections to complex analysis than I expected. For example: $\mathbb{CP}^1$ is the Riemann sphere Elliptic curves are parametrized by Weierstrass's elliptic functions Riemann-Roch is a theorem in complex analysis and in algebraic geometry This surprised me a lot at first, but it does make some more sense when one thinks about it -- analytic (i.e. holomorphic) functions are in a crude sense just ""polynomials with arbitrarily large degree"", and analysis over $\mathbb{C}$ probably should have some more algebraic properties over $\mathbb{R}$ because complex numbers were first investigated due to the fact that they are algebraically closed. After all, polynomials are holomorphic, rational functions are meromorphic, and all smooth algebraic curves are (to the best of my knowledge) Riemann surfaces. This leads me to the question: What most importantly differentiates complex analysis from the study of smooth algebraic varieties over $\mathbb{C}$? To quote Wikipedia , ""Algebraic varieties are locally defined as the common zero sets of polynomials and since polynomials over the complex numbers are holomorphic functions, algebraic varieties over C can be interpreted as analytic spaces. Similarly, regular morphisms between varieties are interpreted as holomorphic mappings between analytic spaces. Somewhat surprisingly, it is often possible to go the other way, to interpret analytic objects in an algebraic way ."" So another way to phrase the question: What are the most important examples of when it is impossible to interpret analytic objects in an algebraic way? The only ones I can think of with my limited knowledge: It's not possible to ""homogenize"" an arbitrary analytic function, like it is with a polynomial, because the terms of an arbitrary analytic function have unbounded degree. Only smooth algebraic varieties correspond to objects of study of complex analysis. Related questions: Connection between algebraic geometry and complex analysis? Complex analysis book for Algebraic Geometers Complex Analysis and Algebra","['complex-geometry', 'complex-analysis', 'soft-question', 'algebraic-geometry']"
1940544,A measure-theoretic analogue to the concept of derivative,"Consider the following measure-theoretic analogy to the concept of derivative. Let $(\Omega, m)$ be a metric space ($\Omega\neq\emptyset$). Denote the topology on $\Omega$ induced by $m$ as $\tau$. Let $\mu, \nu$ be measures on $\sigma(\tau)$, and let $\omega^*\in\Omega$ belong to $\mu$'s support. Suppose the following holds. There is a number $d \in \mathbb{R}$ such that, for every sequence $(B_1, B_2, \dots)$ in $\sigma(\tau)$ consisting of $\tau$-neighborhoods of $\omega^*$, whose $m$-diameters converge to $0$,
  $$
d = \lim_{n\rightarrow\infty} \frac{\nu(B_n)}{\mu(B_n)}.
$$
  Define $D^\nu_\mu(\omega^*):= d$. Has this concept been studied? Does it have a name? How this concept comes about This concept arises naturally in the settings of conditional probability. For instance, in his textbook Basic Probability Theory , Robert Ash writes: ""A reasonable approach to the conditional probability $P(R_2 \in B\ |\ R_1 = x_0)$ is to look at $P(R_2 \in B\ |\ x_0-h< R_1 < x_0+h)$ and let $h\rightarrow 0$."" (p. 136) The limit
$$
\lim_{h\rightarrow\infty} P(R_2 \in B\ |\ x_0-h< R_1 < x_0+h) = \lim_{h\rightarrow\infty}\frac{P(R_2 \in B, x_0-h< R_1 < x_0+h)}{P(x_0-h< R_1 < x_0+h)}
$$
can be restated (sort of) in the notation I introduced above as $D^\nu_\mu(x_0)$, where $\Omega = \mathbb{R}$, $m$ is the Euclidean metric, $\mu = P_{R_1}$ (i.e. $\mu$ is $R_1$'s distribution), and $\nu$ is the measure that assigns to every Borel set $A$, $\nu(A) := P(R_2 \in B, R_1 \in A)$. As a further evidence to how natural this intuitive view of conditional probability is, take this question posted yesterday on this forum, where a user who appears to be a beginning student of probability, is suggesting a definition of conditional probability essentially identical to Ash's ""definition"" above. In my answer to said question I proved the following result, which I reformulate using the notation introduced above. Let $(\Omega, \mathcal{F}, P)$ be a probability space and let $(S,m)$ be a metric space. Denote by $\tau$ the topology on $S$ induced by $m$. Let $Y:\Omega\rightarrow S$ be $\mathcal{F}/\sigma(\tau)$-measurable and let $A \in \mathcal{F}$. Denoting the Euclidean topology on $\mathbb{R}$ by $\mathcal{E}$, suppose that, for some version $f$ of the conditional probability $P(A\ |\ Y=y)$, $f$ is $\tau/\mathcal{E}$-continuous at some $y^*\in S$ that lies in $P_Y$'s support. Then
  $$
f(y^*) = D^\nu_\mu(y^*),\hspace{1cm}(*)
$$
  where $\mu := P_Y$, and $\nu$ is the measure on the Borel field on $\mathbb{R}$ that assigns to every Borel set $B$, $\nu(B):=P(A \cap \{Y\in B\})$. In particular, the expression on the right hand side of $(*)$ is well-defined.","['derivatives', 'real-analysis', 'reference-request', 'measure-theory', 'soft-question']"
1940549,Understanding a step in Stein's proof that the Schwartz space is closed under convolution.,"I am looking at Stein's proof that if $f,g \in \mathcal{S}(\mathbb{R})$, then so is $f*g$. The definition of the Schwartz space on $\mathbb{R}$ given is, the set of all indefinitely differentiable functions $f$ so that $f$ and all its derivatives are rapidly decreasing, in the sense that 
$$\sup_{x\in \mathbb{R}}|x|^k |f^{(l)}(x)| < \infty \; \text{for every}\; k,l\ge 0.$$
 In the proof below, I have four questions. First, how can I check the assertion that we have $\sup_x |x|^l |g(x-y)|\le A_l (1+|y|)^l$? I tried considering the two cases, but I cannot figure out how to get this bound. In fact, I don't really understand why we need to consider two separate cases. Shouldn't we be able to get $|x|^l |g(x-y)|\le |x-y|^l|g(x-y)|+|y|^l|g(x-y)|\le (1+|y|^l)A_l\le(1+|y|)^l A_l$, where $A_l=\sup_x |x|^l |g(x)|$? Next, how can we guarantee that the integral $\int |f(y)|(1+|y|)^l dy$ is bounded for every $l\ge 0$? Finally, in the last paragraph, how is the interchange of differentation and integration justified in the case by the rapid decrease of $dg/dx$? Also, why do we only take the differentiation on $g$ and not $f$? I am not aware of this kind of  theorem. I would greatly appreciate any hints or solutions.","['fourier-analysis', 'functional-analysis', 'complex-analysis', 'fourier-transform', 'analysis']"
1940582,How to define differentiable functions on manifolds?,"i have a question regarding the definition of differentiable functions on manifolds. Why is one atlas of the manifold not enough to define the differentiability of a function via local chart? Why is the maximal atlas, containing the original atlas + all compatible ones to it, necessary? Every introductory literature I have read on this subject does touch on this but I fail to understand why. Kossinki: For our purpose, which is to define differentiable functions, two different
  atlases may yield the same result. They certainly will if they are compatible,
  in the sense that their union is an atlas. This relation of compatibility is an
  equivalence relation; hence every atlas is contained in a maximal one: the
  union of all atlases compatible with it. PennU Differential Geometry Script: We must ensure that we have enough charts in order to carry out our program of generalizing
  calculus on $\mathbb{R}^n$ to manifolds. For this, we must be able to add new charts whenever
  necessary , provided that they are consistent with the previous charts in an existing atlas. I'd be thankful if anyone was able to clear me up. Kind regards.","['differential-geometry', 'differential-topology']"
1940590,Uniform Integrability - proof,"I came across a proof for the following proposition. However, there is one part of the proof which I don't understand. The proposition and proof are as follow. Proposition: A nonempty family $\mathcal{X} ⊆ L^0$ is uniformly integrable if and only if there exists a test function of uniform integrability $ϕ$ such that $\sup_{ X∈\mathcal{X}} \mathbb{E}[ϕ(|X|)] < ∞$. Moreover, if it exists, the function ϕ can be chosen in the class of nondecreasing convex functions. Proof (partial): Suppose, first, that this condition holds for some test function of uniform integrability and that the value of the supremum is $0 ≤ M < ∞$. For $n > 0$, there exists $C_n ∈ \mathbb{R}$ such that $ϕ(x) ≥ nMx$, for $x ≥ C_n$. Therefore, 
$$M ≥ \mathbb{E}[ϕ(|X|)] ≥ \mathbb{E}[ϕ(|X|)\textbf{1}_{{|X|≥C_n}}] ≥ nM\mathbb{E}[|X| \textbf{1}_{{|X|≥C_n}}],$$ for all $X ∈ \mathcal{X}$ . Hence, $$\sup_{X∈\mathcal{X}} \mathbb{E}[|X| \textbf{1}_{{|X|≥C_n}}] ≤ 1/n$$
 and the uniform integrability of X follows. In the above proof, can someone explain to me the part ""For $n > 0$, there exists $C_n ∈ \mathbb{R}$ such that $ϕ(x) ≥ nMx$, for $x ≥ C_n$.""?",['statistics']
1940606,Double integral of Periodic function,"Let $f$ be periodic with period 1: $f(t+1)=f(t)$. Is it correct to claim that: $$\iint\limits_{[-1,1]\times[0,1]} |f(x+t)-f(-x+t)|\,dt\,dx=2\iint\limits_{[0,1]^2} |f(x+t)-f(-x+t)|\,dt\,dx$$? My reasoning is that $\iint\limits_{[-1,1]\times[0,1]} |f(x+t)-f(-x+t)|\,dt\,dx\\=\iint\limits_{[-1,0]\times[0,1]} |f(x+t)-f(-x+t)|\,dt\,dx+\iint\limits_{[0,1]\times[0,1]} |f(x+t)-f(-x+t)|\,dt\,dx$ Thanks for any help. (this is an intermediate step in something I wish to prove)","['multivariable-calculus', 'real-analysis', 'calculus', 'analysis']"
1940621,More direct proof of Cauchy's Integral Formula for simply connected domains,"In the books I looked and also in the script of my former Complex Analysis Prof. we proved the Cauchy Integral Formula for simply connected domains and closed curves in the following order: Goursat's Theorem for Triangles for functions continuous everywhere and holomorphic everywhere except at a single point Cauchy's Theorem for convex domains for functions continuous everywhere and holomorphic everywhere except at a single point Cauchy's Integral Formula for convex domains Cauchy's Integral Theorem for null homologous cycles Introduction of homotopic curves and proof of Cauchy's Integral Theorem for those curves. What I really don't like is two things: First this artificial single point in (1) and (2). It is of course nice to have such a weak version of Goursat and Cauchy Integral Theorem, because then the Integral Formula is easy to derive. But as a student is seems very artificial at first to exclude a single point out of the domain of holomorphy, and then see later that this makes sense and is handy. 
Second, I don't like to go first over nice domains like convex or starlike domains and afterwards end up in arbitrary domains. So, what I know is the following version of Cauchy's Integral Theorem: Let $\Omega$ be a simply connected domain, let $f$ be holomorphic in $\Omega$ and let $\gamma$ be a closed curve (piecewise $C^1$) in $\Omega$. Then \begin{align*}
\int_\gamma f(z) dz=0.
\end{align*} I would like to deduce from this the following version of the Cauchy Integral Formula: Let $\Omega$ be a simply connected domain, let $f$ be holomorphic in $\Omega$ and let $\gamma$ be a closed curve (piecewise $C^1$) in $\Omega$. Then \begin{align*}
n(\gamma, z)f(z)=\frac{1}{2\pi i}\int_\gamma \frac{f(w)}{w-z} dw,\qquad z\in\Omega\backslash\gamma.
\end{align*}
Here, $n(\gamma,z)$ is the winding number of $\gamma$ at $z$. The only way i can think of to achieve my goal is to fix some $z\in\Omega\backslash\gamma$ and cut a line connecting $z$ and the boundary of $\Omega$ out of $\Omega$ to obtain a simply connected domain. Then I need to adjust $\gamma$ so that it does not run trough this slit anymore and goes around $z$. After that I must show that this detour goes to zero if I move along the slit sufficiently close. This is certainly a way to go, but I really don't like how technical this gets if one wants to write down a rigorous proof. My question is: Is there a more elegant way? Once again, I don't know that holomorphic functions have a continuous derivative, that they can be developed in a power series expension and so on. I only have the abovementioned version of Cauchy's Theorem.","['cauchy-integral-formula', 'complex-analysis']"
1940648,How to determine the horizontal scaling factor for which 4 points become co-circular? (Mostly stuck on the algebra),"at my university, we're doing a project with kinetic datastructures right now, and for that, I need to know at which time point 4 given points become co-circular (in a position such that they are all on one circle). Let $p_i, p_k, p_j, p_l$ be those points, which form at least a convex quadrilateral (that is guaranteed at this point). We determined that one way to determine co-circularity is that the sum of opposing inner angles must reach $\pi$. For the opposing angles $\angle d,a,b$ and $\angle b,c,d$. To simplify calculations, we limit ourselves to the cosine of the angles: So: $$\cos \angle p_i p_k p_j = 1 - \cos \angle p_i p_l p_j$$ Then, we apply the law of cosines: $$\cos \angle p'_i p'_k p'_j = \frac{{dist}^2(p'_i,p'_k) 
								+ {dist}^2(p'_i,p'_j)
                                - {dist}^2(p'_j,p'_k)}
                                {{dist}(p'_i,p'_k) \cdot {dist}(p'_i,p'_j)}$$ (The other side is similar) Now, expanding the dist functions, we get: $$\cos \angle p_i p_k p_j = \frac{\rho^2(x_i-x_k)^2 + (y_i-y_k)^2
								+ \rho^2(x_j-x_k)^2 + (y_j-y_k)^2
                                - \rho^2(x_i-x_j)^2 - (y_i-y_j)^2}
                                {\sqrt{\rho^2(x_i-x_k)^2 + (y_i-y_k)^2} \cdot 
                                 \sqrt{\rho^2(x_j-x_k)^2 + (y_j-y_k)^2}}$$ $$\cos \angle p_i p_l p_j = \frac{\rho^2(x_i-x_l)^2 + (y_i-y_l)^2
								+ \rho^2(x_j-x_l)^2 + (y_j-y_l)^2
                                - \rho^2(x_i-x_j)^2 - (y_i-y_j)^2}
                                {\sqrt{\rho^2(x_i-x_l)^2 + (y_i-y_l)^2} \cdot 
                                 \sqrt{\rho^2(x_j-x_l)^2 + (y_j-y_l)^2}}$$ $\rho$ is the scaling factor. Replacing these in the first equation, and factoring out $\rho$ where possible: $$\frac{\rho^2(x_i-x_k)^2 + (y_i-y_k)^2
								+ \rho^2(x_j-x_k)^2 + (y_j-y_k)^2
                                - \rho^2(x_i-x_j)^2 - (y_i-y_j)^2}
                                {\sqrt{\rho^2(x_i-x_k)^2 + (y_i-y_k)^2} \cdot 
                                 \sqrt{\rho^2(x_j-x_k)^2 + (y_j-y_k)^2}} = \\
1- \frac{\rho^2(x_i-x_l)^2 + (y_i-y_l)^2
								+ \rho^2(x_j-x_l)^2 + (y_j-y_l)^2
                                - \rho^2(x_i-x_j)^2 - (y_i-y_j)^2}
                                {\sqrt{\rho^2(x_i-x_l)^2 + (y_i-y_l)^2} \cdot 
                                 \sqrt{\rho^2(x_j-x_l)^2 + (y_j-y_l)^2}}$$ Aaand... that's where I got stuck. Algebra has never been one of my strong points. All those $x_i$ and $y_i$ are known, but I need to know $\rho$. I resolved it once, but then noticed I'd factored out the $\rho$ from the square roots, without noticing that the part with the y-coordinates did not have a $\rho$ factor. Now I'm not sure how to get it out of the square roots. How to I determine $\rho$? (Or at the very least, I how do I get rid of those square roots?)","['algebra-precalculus', 'trigonometry']"
1940685,SLLN for V-statistics - Strong consistency of distance covariance in metric spaces,"I'm trying to understand the proof for strong consistency of distance covariance in metric spaces (Proposition 2.6 Distance covariance in metric spaces by Russel Lyons published in The Annals of Probability). Let $(\mathcal{X},d_1)$ and $(\mathcal{Y},d_2)$ be two metric spaces and $((X_k,Y_k))_{k\in\mathbb{N}}$ be an i.i.d sequence of random elements with values in $\mathcal{X}\times \mathcal{Y}$ such that 
$$
Ed_1(X_1,x')<\infty \quad \quad and \quad \quad Ed_2(Y_1,y')<\infty, 
$$
for any $x'\in \mathcal{X}$ and $y'\in\mathcal{Y} $. The distance covariance $dcov(X,Y)$ is given by
$$
dcov(X,Y) = Eh((X_{1},Y_1),...,(X_6,Y_6)),
$$
where 
\begin{align*}
h((x_{1},y_1),...,(x_6,y_6))=&f_1(x_1,x_2,x_3,x_4)f_2(y_1,y_2,y_5,y_6) ,\\
\end{align*}
and
$$
f_i(z_1,z_2,z_3,z_4) =d_i(z_1,z_2)-d_i(z_1,z_3)-d_i(z_2,z_4)+d_i(z_3,z_4).
$$
The emperical distance covariance hence becomes a $V$-statistic with (in general) non-symmetric kernel $h$ of degree $6$, i.e.
$$
dcov_n(X,Y) = \frac{1}{n^6} \sum_{i_1=1}^n \cdots \sum_{i_6=1}^n h((X_{i_1},Y_{i_1}),...,(X_{i_6},Y_{i_6})).
$$
Now it is stated without reference to a SLLN that $dcov_n(X,Y) \to dcov(X,Y)$ almost surely. Problem: The weakest conditions for SLLN for $V$-statistics I know require that The kernel $h$ is symmetric and \begin{align*} E| h((X_{i_1},Y_{i_1}),...,(X_{i_6},Y_{i_6}))|^{\frac{\#\{i_1,...,i_6\}}{6}} < \infty, \end{align*} for any $1 \leq i_1 \leq \cdots \leq i_6 \leq 6$. So my approach was to let $\tilde{h}$ be the symmetrized version of $h$: $$\tilde{h}((x_1,y_1),...,(x_6,y_6))=\frac{1}{6!} \sum_{\sigma\in \Pi_6} h((x_{\sigma(1)},y_{\sigma(1)}),...,(x_{\sigma(6)},y_{\sigma(6)})),$$
where $\Pi_6$ is the set of all permutations on $\{1,...,6\}$. Then the above conditions of the SLLN reduces to
\begin{align*}
E| h((X_{i_1},Y_{i_1}),...,(X_{i_6},Y_{i_6}))|^{\frac{\#\{i_1,...,i_6\}}{6}} < \infty,
\end{align*}
for all $(i_1,...,i_6)\in \{1,...,6\}^6$. The only way (that i can see) we can create upper bounds for $h$ is by the triangle inequality, which amounts to the following inequalities (suppressing the $i$ index in the metric) \begin{align}
\frac{|f_i(z_1,z_2,z_3,z_4)|}{2}\leq \left\{
\begin{array}{lll}
d(z_1,z_4), & d(z_2,z_3), & d(z_1,z_2) \lor d(z_1,z_3), \\
d(z_1,z_2) \lor d(z_1,z_4), & d(z_1,z_2) \lor d(z_2,z_3), & d(z_1,z_2) \lor d(z_2,z_4), \\
d(z_1,z_4)  \lor d(z_1,z_3), & d(z_1,z_4)  \lor d(z_2,z_3), & d(z_1,z_4)  \lor d(z_2,z_4), \\
d(z_2,z_3) \lor d(z_1,z_3), & d(z_2,z_3) \lor d(z_2,z_4),  & d(z_3,z_4) \lor d(z_1,z_3), \\ 
d(z_3,z_4) \lor d(z_1,z_4), & d(z_3,z_4) \lor d(z_2,z_3), &d(z_3,z_4) \lor d(z_2,z_4). \\ 	\end{array}
\right.
\end{align}
and we realize that if for example $i_1=i_2$ then every combination of the above inequalities result in
$$
E| h((X_{i_1},Y_{i_1}),...,(X_{i_6},Y_{i_6}))|^{\frac{\#\{i_1,...,i_6\}}{6}} \leq 4E([\gamma(X_{i_1},...)\theta(Y_{i_1},...)]^{\frac{\#\{i_1,...,i_6\}}{6}}),
$$
where $\gamma$ and $\theta$ is any of the above upper bounds. We know that $E\gamma(X_{i_1},...)<\infty$ and $E\theta(Y_{i_1},...)<\infty$, for example
$$
Ed(X_{i_2},X_{i_3}) \lor d(X_{i_1},X_{i_3}) \leq E d(X_{i_2},x')+d(X_{i_3},x')+d(X_{i_1},x')+d(X_{i_3},x') < \infty.
$$
But we can't use independence to split up the expectation since $X_{i_1}$ may not be independent of $Y_{i_1}$. I realize that if $\#\{i_1,...,i_6\}\leq 3$ then Cauchy-Schwarz inequality can be used to say that
\begin{align*}
4E([\gamma(X_{i_1},...)\theta(Y_{i_1},...)]^{\frac{\#\{i_1,...,i_6\}}{6}})&\leq 4E([\gamma(X_{i_1},...)\theta(Y_{i_1},...)]^{1/2})+4 \\
&\leq 4[E\gamma(X_{i_1},...)]^{1/2}[E\theta(Y_{i_1},...)]^{1/2} +4< \infty,
\end{align*}
but if $3<\#\{i_1,...,i_6\}\leq 6$, then we can't utilize this either. Thus: Is there another version of SLLN for $V$-statistics which is suitable in this situation,  or is there another way to bound the above expectation?","['probability-limit-theorems', 'law-of-large-numbers', 'probability-theory', 'statistics', 'measure-theory']"
1940695,Do there exist numbers normal in every base except for one?,"A number $x$ is called normal in base $b$ if every sequence of base $b$ digits $b_1b_2...b_n$ occurs with natural density $1/b^n$ in the decimal expansion of $x$. There exist numbers normal in every base (called absolutely normal) and irrational numbers normal in no base (called absolutely non-normal), an example is given here . Is it known whether there exist numbers that are normal in every base except one or numbers non-normal in every base except one? The question can be stated rather easily but an answer probably will take a lot of effort, so thanks in advance, also for any reference to literature :).","['number-theory', 'decimal-expansion', 'irrational-numbers']"
1940700,Union of metric spaces,Is the union of two metric spaces a metric space? I tried it but could't define a suitable metric on intersection. Can somebody help me to understand it?,"['general-topology', 'metric-spaces']"
1940720,Trigonometric quadratic formula? And other trig solutions for roots of polynomials?,"I was experimenting with trig functions and their connections to roots of polynomials when I derived a sort of quadratic formula: $$ax^2+bx+c=0\implies x=n\cos\left(\frac12\arccos\left(\frac{b^2-4ac-2a^2n^2}{2a^2n^2}\right)\right)-\frac b{2a}\tag{for all $n$}$$ And here is the little proof I composed: $$\cos(2\arccos(x))=2x^2-1\tag{trig identity}$$ $$ax^2+bx+c=0$$ $$ay^2-\frac{b^2}{4a}+c=0\tag{$x=y-\frac b{2a}$}$$ $$y^2=\frac{b^2-4ac}{4a^2}$$ $$n^2t^2=\frac{b^2-4ac}{4a^2}\tag{$y=nt$}$$ $$2t^2=\frac{b^2-4ac}{2a^2n^2}$$ $$2t^2-1=\frac{b^2-4ac}{2a^2n^2}-1=\frac{b^2-4ac-2a^2n^2}{2a^2n^2}$$ $$\cos(2\arccos(t))=\frac{b^2-4ac-2a^2n^2}{2a^2n^2}\tag{apply trig identity}$$ $$t=\cos\left(\frac12\arccos\left(\frac{b^2-4ac-2a^2n^2}{2a^2n^2}\right)\right)$$ $$x=n\cos\left(\frac12\arccos\left(\frac{b^2-4ac-2a^2n^2}{2a^2n^2}\right)\right)-\frac b{2a}$$ Since $n$ can be anything, choose it so that we don't deal with complex numbers, if possible.  Feel free to comment on the above formula. Anyways, there exists similar derivations for cubic equations and quartic equations, but I was wondering on, more specifically, the solution to the quintic polynomial with trigonometric functions.  Is it possible?  And can general formulas be made to find the roots of even higher degree polynomials using trig functions?","['algebra-precalculus', 'polynomials', 'roots', 'trigonometry']"
1940878,"Given an $n \times n$ square grid, how many ways can we remove $k$ vertices with the graph still connected?","Suppose that on an $n \times n$ square grid, we place edges as we typically would to form unit squares  (horizontal, vertical edges on each vertex.) The question is:
if we remove $k$ points, how many ways are there to do this so that the graph is still connected? (Let's just agree to call the resulting graph after the removal of $k$ vertices an end graph. ) For ease of discussion, let the vertices be given by Cartesian co-ordinates, starting with $(0,0)$ and ranging up until $n-1$ on the $x-y$ axis (first quadrant.) To make clear what the problem is: If a vertex is removed, so is every edge connected to it. it matters what kind of graph is left after the vertex removals. Perhaps to clarify this point: say we are removing $n^2-1$ vertices. Then there are  $n^2$ ways to do this [the vertices are labelled.] The order in which the vertices are removed does not matter. You must remove precisely $k$ vertices, so I'm not asking to count the removal of $j \leq k$ vertices. Are there any known methods known for this sort of counting? the following is some of the work I've done thus far: If $k=2$ with $n>2$, I reasoned that the number of ways is $\binom{n^2}{2}-4$, since you can always ""clip off a corner."" the following is incorrect: However, the situation changes for $k=3$, since there are different ways of taking off a corner that leaves an isolated vertex (or even $3$ vertices.) In particular, I noticed that there are $3$ ways to clip off a corner, so we can take $\binom{n^2}{3}-4\cdot 4$ as the possible number of ways. Is it sensible to anticipate an explicit or recursive formula, in general? Update It appears as though I've only been considering cases where the vertices being removed could be ""drawn"", or that each choice of vertex was adjacent to one already chosen (vertically, horizontally, or diagonally.) 
 Otherwise, it seems annoying that you could ""section off"" a corner, say by removing $(1,0)$ and $(0,1)$, but then choosing a any other collection of vertices, in order to form a disconnected end-graph. Consequently, even though I'm asking that precisely vertices be removed, one must consider $k=2$ in this case, since for each way to disconnect the graph also has $n^2-3$ options, so for $k=3$, one must also subtract off $(\binom{n^2}{2}-4)\cdot (n^2-3)$ as well, to obtain: $\binom{n^2}{3}-4\cdot 4-(\binom{n^2}{2}-4)\cdot (n^2-3)$ ways without disconnecting the graph. Does this seem correct? sigh . feel free to edit the tags, I don't have a good sense about what would be appropriate for this problem.","['combinatorics', 'graph-theory']"
1940882,How to show this morphism does not depend on the choice of $U$?,"Let $X$ be a scheme, $x\in X$. Then we have a canonical morphism
$\mathrm{Spec}\,\mathcal O_{X,x}\to X$ defined as follows. Let us take an affine open set $U\ni x$. The
canonical homomorphism $\mathcal O_X(U)\to \mathcal O_{X,x}$ induces a morphism $\mathrm{Spec}\,\mathcal O_{X,x}\to U$ .
Composing this morphism with the open immersion $U \to X$, we obtain a morphism
$\mathrm{Spec}\,\mathcal O_{X,x}\to X$ . How to show this morphism does not depend on the choice of $U$?",['algebraic-geometry']
1940885,"Find the locus of the vertices of the right circular cones that pass through the ellipse $\frac{x^2}{a^2}+\frac{y^2}{b^2}=1, z=0$","Prove that the locus of the vertices of the right circular cones that pass through the ellipse $\frac{x^2}{a^2}+\frac{y^2}{b^2}=1, z=0$ is $\frac{x^2}{a^2-b^2}-\frac{z^2}{b^2}=1, y=0$ or $\frac{y^2}{a^2-b^2}+\frac{z^2}{a^2}=-1, z=0$. EDIT: Here $\frac{x^2}{a^2}+\frac{y^2}{b^2}=1, z=0$ is the base of the cone. Let the vertex of the cone be $(x_1,y_1,z_1)$. Let the generator be $$\frac{x-x_1}{l}=\frac{y-y_1}{m}=\frac{z-z_1}{n}.$$ Then $z=0$ implies any point on ellipse be $(x_1-lz_1/n, y_1-mz_1/n,0)$. It lies on the ellipse $\frac{x^2}{a^2}+\frac{y^2}{b^2}=1$ then the point satisfies the ellipse we get $$\frac{(nx_1-lz_1)^2}{a^2}+\frac{(ny_1-mz_1)^2}{a^2}=n^2$$ Eliminating $l,m,n$ we get the equation of the cone as $$\frac{1}{a^2}(zx_1-xz_1)^2+\frac{1}{b^2}(zy_1-yz_1)^2=(z-z_1)^2.$$ How to get the locus of vertex $(x_1,y_1,z_1)$? Edit 2 How to get the locus of the vertices in the given two forms using purely mathematical way? I not able to solve the problem. Please help.","['algebraic-geometry', 'analytic-geometry', 'geometry', 'conic-sections', 'surfaces']"
1940888,In search of nontrivial solutions to $f'(t)=f(t/2)$,"Just then my friend gave me a kinda interesting problem: find all non-trivial solutions to the DE $$f'(t)=f(\frac t2),\quad f:\Bbb R\to\Bbb R.$$
Regularity concerns are temporarily ignored. You may just assume sufficient smoothness if needed. (Note also that this is NOT an ODE!) So far the only solution I can find is $f\equiv 0$, the trivial one. And surprisingly, I can't find any useful results of this problem, simple as it looks. Any full or non full answer or just attempt is welcome!","['real-analysis', 'ordinary-differential-equations', 'functional-equations', 'recreational-mathematics']"
1940927,Antisymmetric vs. alternating $k$-linear forms and wedge-product,"In my linear algebra course we derived determinants via alternating $k$-linear forms $f: V^k \to K$. These alternating linear forms form a vector space $\Lambda^k(V^*)$. During this derivation we defined the wedgeproduct
$$a \wedge b := \frac{k + l}{k!l!}\text{Alt}(a \otimes b)$$
for $a \in \Lambda^k(V^*)$ and $b\in \Lambda^l(V^*)$. Further $\text{Alt}(f)$ is an operator which produces an alternating linear form given a (non-alternating) linear form $f$. My notes say that the wedge product is not necessarily alternating. Moreover, $(a \wedge b) = (-1)^{kl}(b \wedge a)$. Why is that correct? $\text{Alt}(\cdot)$ scaled by a scalar should be alternating by default! In class we did not talk about exterior algebras at all but only used the notions above to derive determinants. This wikipedia mentions anti-symmetric maps ( here ). What exactly is that? I only know about anti-symmetric matrices . What is the difference between anti-symmetric and alternating maps? Is it true that an alterating bilinear form $b(x,y) = -b(y,x)$ can always be represented by an anti-symmetric matrix? I proved the converse (anti-symmetric matrix always induces an alternating bilinear form) but cannot find an angle to prove this direction. Thanks!","['multilinear-algebra', 'exterior-algebra', 'linear-algebra', 'determinant']"
1940934,$S_n=\sum^n_{k=1} (-1)^{\Omega(q_k)}\cdot{q_k}$ change sign infinitely many times?,Let $q_k$ be the k-$th$ positive natural number that is divisible by a square larger than one. How to prove that $S_n=\sum^n_{k=1} (-1)^{\Omega(q_k)}\cdot{q_k}$ change sign infinitely many times as $n$ increases? $\Omega(n)$ is the number of (not necessarily different) prime factors of $n$. I have made some numerical experiments that indicates this conjecture. Adequate answers will be rewarded with bounties.,"['number-theory', 'conjectures', 'sequences-and-series']"
1940972,Variance of infinite sum.,"We have a sequence $(X_n)$ of independent real valued random variables with $EX_n=0$ and $EX_n^2<\infty$. I know that $X_n \to 0$ in $L^2$ and $X_n \to 0$ a.s. If we define $S_n=\sum_{k=1}^nX_k$, I have proved that $S_n$ converges almost surely to some limit for $n\to \infty$. If we let $S=\sum_{n=1}^\infty X_n$ denote the almost sure limit of $S_n$, I want to show that S has finite variance and compute the variance. Can someone help me get started?","['measure-theory', 'random-variables']"
1941003,How can I evaluate: $\lim_{x \to \infty} \sqrt{x^2 + 4x} - \sqrt{x^2 - 5x}$,I need to find this limit: $$\lim_{x \to \infty} \sqrt{x^2 + 4x} - \sqrt{x^2 - 5x}$$ The answer I got from using the limit laws is $\sqrt{\infty} - \sqrt{\infty}$. How do I proceed now? Added I took the conjugate of the function and I got a new and probably better function to work with: $$\lim_{x\to \infty}\frac{9x}{\sqrt{x^2 + 4x} + \sqrt{x^2-5x}}$$,['limits']
1941010,"$\int^\infty_0\frac{\sin x}{x} \, dx = \frac{1}{2i}\int^\infty_{-\infty} \frac{e^{ix}-1}{x} \, dx$, why?","How comes this true? $$\int^\infty_0\frac{\sin x} x \, dx = \frac{1}{2i}\int^\infty_{-\infty} \frac{e^{ix}-1} x \, dx$$","['complex-analysis', 'integration']"
1941021,"Suggest some ""unconventional"" books on probability & statistics","stackexchange, please suggest me some books on probability & statistics that are unconventional in their approach to these subjects. I think it is better to describe what i mean by ""unconventionality"" by example. Some of this kind of books are "" Probability Theory: The Logic of Science "" by E. T. Jaynes .
The famous book where the author uses Cox's theorems for the laws of probability and interprets probability as an extension of logic. "" Probability and Finance: It's Only a Game! "" by Shafer and Vovk .
In this book the authors derive the laws of probability from game theory. "" Probability via Expectation "" by Peter Whittle . The author develops the theory of probability from axioms on the expectation functional rather than on probability measure. "" Radically Elementary Probability Theory "" by Edward Nelson . The author uses non-standard analysis in his treatment of probability.","['reference-request', 'probability-theory', 'statistics', 'probability', 'soft-question']"
1941023,Find $f$ if $f'(x)=\dfrac{x^2-1}{x}$ knowing that $f(1) = \dfrac{1}{2}$ and $f(-1) = 0$,"I am asked the following problem: Find $f$ if $$f'(x)=\frac{x^2-1}{x}$$ I am not sure about my solution, which I will describe below: My solution: The first thing that I've done is separate the terms of $f'(x)$ \begin{align*}
f'(x)&=\frac{x^2-1}{x}\\
&=x-\frac{1}{x}\\
\therefore  \quad f(x)&=\frac{x^2}{2}-\ln|x|+c
\end{align*} For ( x > 0 ): \begin{align*}
f(x)&=\frac{x^2}{2}-\ln x+c\\
f(1)&=\frac{1^2}{2}-\ln 1+c=\frac{1}{2} \quad \Rightarrow \quad c=0\\
f(x)&=\frac{x^2}{2}-\ln |x|
\end{align*} For ( x < 0 ) \begin{align*}
f(x)&=\frac{x^2}{2}-\ln (-x)+c\\
f(-1)&=\frac{(-1)^2}{2}-\ln [-(-1)]+c=0 \quad \Rightarrow \quad c=-\frac{1}{2}\\
f(x)&=\frac{x^2}{2}-\ln |x|-\frac{1}{2}
\end{align*} Is my solution correct? Should I really find two different answers, one for $x > 0$ and another for $x < 0$? Thank you.","['derivatives', 'indefinite-integrals', 'integration', 'calculus']"
1941031,"If a group has exactly 24 elements of order 6, how many subgroups of order 6 does it have?",I have no idea how the two are related. I can figure out that the group should have at least 30 elements because the number has to be more than 24 and divisible by 6.,['group-theory']
1941044,Why is cross product defined in the way that it is?,"$\mathbf{a}\times \mathbf{b}$ follows the right hand rule? Why not left hand rule? Why is it $a b \sin (x)$ times the perpendicular vector?  Why is $\sin (x)$ used with the vectors but $\cos(x)$ is a scalar product? So why is cross product defined in the way that it is?
I am mainly interested in the right hand rule defintion too as it is out of reach?","['convention', 'cross-product', 'linear-algebra', 'vectors']"
1941062,Prove the following quantity is always nonnegative,"Prove that $$f(x, y) \equiv \arccos\left(\frac{x-y}{K}\right) - \arccos\left(\frac{x-y}{K}+y\right) - \frac{y}{x}\arccos(1-y^2) \ge 0$$ with the constraints: $K\ge 2$ is an integer, $g(x, y) = (K-1)y^2+x^2-K=0$, $1\le x\le \sqrt{K}$, $0\le y\le 1$. Furthermore, $f(x, y) = 0$ if and only if $x=y=1$ or $x=\sqrt{K}$ and $y=0$. Numerically it seems to be correct. Also $K=2$ it can be proven. Note that the inequality could also be replaced with $$f_2(x, y) = \arccos\left(\frac{x-y}{K}\right) - \arccos\left(\frac{x-y}{K}+y\right) - \frac{y}{x}\arccos(1-y) \ge 0$$ since $f \ge f_2$. So if $f_2 \ge 0$ then $f \ge 0$. Blue curve is $f$ and orange one is $f_2$ when $K=3$. The $x$-axis is $x$, variable $y$ has been replace by $y = \sqrt{\frac{K-x^2}{K-1}}$. What I have tried: Setting $y = \sqrt{\frac{K-x^2}{K-1}}$ and try showing $f''_x(x, y(x)) > 0$. Numerically it is correct but the expression seems to be too complicated to prove. Using $\arccos(a) - \arccos(b) = \int_a^b \frac{1}{\sqrt{1-z^2}}\mathrm{d}z$ to expand $f$ as the subtraction of two integrals. Then if the integrand is positive then things are proved. It works for most $z$ except for a small vicinity at $z=1$, in which the integrand is negative. Numerical test shows it is indeed the case. Use Lagrange multiplier. $\lambda$ is easy to compute. But the second derivative seems to be too messy..","['inequality', 'trigonometry']"
1941079,Construct Parabola given two points and axis of symmetry,"By constructing the parabola I mean Given two points $P_1$ and $P_2.~~$ Given axis of symmetry $L$ that is not $\overleftrightarrow{P_1P_2}$ nor perpendicular to $\overleftrightarrow{P_1P_2}.~~$ Find the directrix $\Gamma$ and the focal point $F$ such that $d(F,\, P_1) = d(P_1,\, \Gamma)$ and $d(F,\, P_2) = d(P_2,\, \Gamma)$ Algebraically this is easy to solve, and I have translated the solution to a geometric construct in GeoGebra . My problem is that I've basically just constructed the desired lengths (the vertex coordinates and the focal length) ""somewhere else"" and then shuffle the lengths to the appropriate place. As can be seen in my GeoGebra worksheet, this doesn't tell me why the focus and the directrix satisfy the ""length-matching"" criterion for a parabola. What I am hoping for is an informative compass-and-straightedge construct, where lengths are built around the given two points and the axis of symmetry, showing geometrically why/how the lengths equal $\overline{P_iF} = d(P_i,\, \Gamma)$ for $i =1,2$ Thank you. P.S. Below are some details regarding my algebraic formulation that got translated into my GeoGebra worksheet. Feel free to skip it: The given two points can be on the same side of $L$ or they can be on opposite sides. It doesn't matter since one can obtain the mirror images and end up with 4 points. Without loss of generality, set the given two points as $(0,0)$ the origin and $(a, b)$ in the first quadrant. Set the axis of symmetry as a vertical line $\Gamma:\, x = x_0$, where the two points are on the same side $x_0 > a > 0$ (one might wonder why I don't set the axis of symmetry as one of the coordinate axes$\ldots$ well, I made a bad choice I guess) At any rate, the parabola is completely determined with 2 unknowns and 2 equations: the focal length $f$ and vertex height $y_0$:
$$ y = \frac{ -(x - x_0)^2 }{4f} + y_0 \\
y_0 = x_0  \frac{b}a  \frac{x_0}{ 2x_0 - a} \qquad , \qquad f = \frac{ (2x_0 - a) a}{4b}$$
where I explicitly write $y_0$ as such to show the geometric construct I adopted, with similar triangles doing the scaling. As for $f$ I used the power-of-a-point.","['algebra-precalculus', 'conic-sections', 'geometric-construction', 'plane-geometry']"
1941097,Why am I allowed to assume x or y are equal to 0 in this Lagrange multipliers problem?,"I am working on a Lagrange multipliers problem where I have to find the extreme values of the function $f(x, y) = x^2 + y^2$ along the constraint curve $g(x, y) = x^4 + y^4 = 1$. Solving the Lagrange equations I get $\lambda = \frac{1}{2x^2}$ and $\lambda = \frac{1}{2y^2}$ i.e. $x = \pm y$. However, when plugging this result back into the constraint curve and solving for $g(x, y)$, I only get one extreme value, $\sqrt{2}$. Solutions for this problem say that when you obtain the Lagrange equations you can also assume that $x$ or $y$ are equal to $0$. And when you plug these results back into the constraint curve, you get a minimum of $1$ when one of the variables equals $0$ and the other equals $\pm 1$. What I am curious about is why exactly you are able to assume the $x$ or $y$ are equal to $0$ in this situation. My textbook says nothing about it and I can't find any examples of a problem where someone explains this. (The solution I found just assumed $x$ or $y$ equaled $0$ without any explanation.) My intuition is that it has something to do with the fact that $x$ and $y$ are in the denominator in this situation, but I'm not sure. If someone could clear this up for me it would be greatly appreciated.","['multivariable-calculus', 'lagrange-multiplier']"
1941108,A ping-pong ball lying inside a wine glass,"Show that a ping-pong ball with radius $r$, lying inside a wine glass described by the function $x^2$, has its center at $r^2+\frac{1}{4}$ units above the bottom of the glass. Here is a visualization of the problem My best attempt is trying to find the derivative of the circle and the function to find some relationship at the point where they meet. The problem looked very simple at first, but I can't figure it out now.","['calculus', 'functions']"
1941111,Excercise in Transcendental Number Theory,"I am currently working through some of the content in Murty and Rath's Transcendental Numbers , and in their section entitled ""Some Applications of Baker's Theorem"" they present the following excercise: Suppose that the sum
$$
F(z;x) = \sum_{n=1}^{\infty}\frac{z^n}{n+x}
$$
converges. If $z$ is algebraic and $x$ is rational, show that the sum is either zero or transcendental. Presumably I can prove this using Baker's Theorem, but I am honestly at a loss on how to approach this, and any of the other questions from that section for that matter. This isn't homework, so I'd really like a hand-holding walkthrough on how I'm suppose to tackle a problem like this. Edit: Added new tag for visibility.","['number-theory', 'transcendence-theory', 'transcendental-numbers']"
1941135,Let $G$ be a finite group and $g \in G$. Then the order of $g$ is finite.,"Let $G$ be a finite group and $g \in G$. Then the order of $g$ is finite. I know that  if $g$ has finite order, then $g^k = e$ for a finite $k$, but i'm not really sure how to get started showing this. Since there is a finite number of elements in $G$, it makes intuitive sense that this result should be true.","['finite-groups', 'abstract-algebra', 'group-theory']"
1941212,On the variance proxy of a positive (and bounded) sub-Gaussian variable,"Consider a random variable $X \ge 0$ which takes values in an interval $[0, b]$, and further
$$
\text{P}(X \ge t) \le C \exp\left(\frac{-t^{2}}{B}\right),
\quad
\forall t \ge 0,
$$
for given constants $C \gg 1$ and $B >0$. Since $X$ is bounded, it is a sub-Gaussian variable, and its variance proxy can be upper bounded by $O\left((b-0)^{2}\right)$ based on the length of the interval. Q1: First, a clarification on the definition:
if we temporarily ignore the fact that $X$ is bounded (but taking into account that $X \ge 0$), then is the above tail bound enough to say that $X$ is sub-Gaussian? (E.g., does the value of $C$ matter?) Q2: Using the tail bound, is it possible to get a better upper bound on the variance proxy?
In particular, I saw a claim that based on the above tail bound, the moments of $X-\mathbb{E}[X]$ can upper be bounded by those of a Gaussian with variance $O(B \sqrt{\log{C}})$. Is that true? Edit : To bound all moments of $X-\mathbb{E}[X]$ by those of a Gaussian with variance $\gamma$, I would need to show that $X-\mathbb{E}[X]$ is sub-Gaussian with variance proxy $\gamma > 0 $, i.e. , that $\mathbb{E}[e^{s(X-\mathbb{E}[X])}] \le e^{s^{2}\gamma/2}$.
Motivated by Michael's answer, which gives an upper bound on the variance $\sigma^{2}$ of $X$, we could put the question this way: is there a straightforward connection between $\gamma$ and  $\sigma^{2}$? I see a related question here: Bound variance proxy of a subGaussian random variable by its variance","['probability-theory', 'probability', 'distribution-tails']"
1941217,Let $\mathscr{A}$ be a sigma-algebra. Show that if $|\mathscr{A}| = ∞$ then $\mathscr{A}$ is uncountable.,"Asking for clarity of solution to $3(b)$: Let $\mathscr{A}$ be a sigma-algebra. Show that if $|\mathscr{A}| = ∞$ then $\mathscr{A}$ is uncountable. (Hint: You need to
  show that there’s an infinite sequence of non-empty disjoint
  measurable sets. How ? Take any non-empty set $A ∈ \mathscr{A}$. Then either $A$
  or $A^c$ contains infinitely many measurable sets. Continue by
  induction). (See http://homepages.uconn.edu/benari/math5111s09/restricted/hw1_R1sol.pdf ) The solution goes: First we construct an infinite sequence of nonempty disjoint
  measurable sets. Then we use that to show that $A$ contains
  uncountably many sets. Let $A_1 ∈ \mathscr{A}$ which is neither $∅$ nor $X$. WLOG $A^{c}_1$ has an infinite number of measurable subsets (make sure you
  understand why. This follows from the assumption that A is infinite).
  Continue by induction, assuming that $B_n = \{\bigcup_n^{j=1} A_j\}^c$
  contains an infinite number of measurable subsets, we pick nonempty
  $A_{n+1} ⊂ B_n$ in $\mathscr{A}$ such that $B_n-A_{n+1}$ has infinitely
  many measurable subsets. By construction $A_1$, . . . are measurable
  and disjoint. Now for each subset $I ⊂ N$, define $A_I = ∪_{n∈I}A_n$.
  Clearly, $A_I ∈ \mathscr{A}$. Note that that the mapping $I → A_I$
  from $P(N)$ to $\mathscr{A}$ is one-to-one. Since $P(N)$ is
  uncountable, the claim follows. I was wondering if this solution is legit. The proof argues that $A^{c}_1$ has an infinite number of measurable subsets. However, there could be infinitely many sets that intersects with $A_1^c$ and $A_1$ but these sets are not contained in either $A_1^c$ or $A_1$. Someone has argued this in reddit (which I don't think it make sense): https://www.reddit.com/r/cheatatmathhomework/comments/1mgj91/analysis_sequence_in_sigma_algebra/","['real-analysis', 'functional-analysis', 'probability', 'measure-theory', 'general-topology']"
1941224,Number of occurrences in contiguous subarrays,"How to count the number of occurrences of each element in all contiguous subarrays? For an array $[1,2,3]$, the contiguous subarrays are: $[1]$, $[2]$, $[3]$ $[1,2]$, $[2,3]$ $[1,2,3]$ $1$ occurs $3$ times , $2$ occurs $4$ times and $3$ occurs $3$ times .","['combinatorics', 'discrete-mathematics']"
1941244,Markov Chain Help,"I'm very far removed from Linear Algebra or statistics that I honestly do not remember Markov Chains. I'm trying to figure out this problem.  Hopefully someone can tell me how to complete this.  Excuse my lack to proper formatting with this question. How is s3 even solved?  I thought a Markov Chain rows has to be equal to 1.  Is this even a Markov Chain Question? Write a function answer(m) that takes an array of array of nonnegative ints representing how many times that state has gone to the 
  next state and return an array of ints for each terminal state giving the exact probabilities of each terminal state, represented 
  as the numerator for each state, then the denominator for all of them at the end and in simplest form. The matrix is at most 10 by 
  10. It is guaranteed that no matter which state the ore is in, there is a path from that state to a terminal state. That is, the 
  processing will always eventually end in a stable state. The ore starts in state 0. The denominator will fit within a signed 
  32-bit integer during the calculation, as long as the fraction is simplified regularly. For example, consider the matrix m: [
[0,1,0,0,0,1],  # s0, the initial state, goes to s1 and s5 with equal probability
[4,0,0,3,2,0],  # s1 can become s0, s3, or s4, but with different probabilities
[0,0,0,0,0,0],  # s2 is terminal, and unreachable (never observed in practice)
[0,0,0,0,0,0],  # s3 is terminal
[0,0,0,0,0,0],  # s4 is terminal
[0,0,0,0,0,0],  # s5 is terminal
] So, we can consider different paths to terminal states, such as: s0 -> s1 -> s3 s0 -> s1 -> s0 -> s1 -> s0 -> s1 -> s4 s0 -> s1 -> s0 -> s5 Tracing the probabilities of each, we find that s2 has probability 0 s3 has probability 3/14 s4 has probability 1/7 s5 has probability 9/14","['markov-chains', 'probability']"
1941274,The formula for $\cos nx$ without Demoivre's theorem?,"De Moirve's theorem easily derives $\cos\left(nx\right)$ in terms of decreasing powers of $\cos\left(x\right)$ and increasing powers of $\sin\left(x\right)$. 
But I'd like to use trignomoetry to derive this simple recursion.
I have tried but the recursion soon gets too complex. Is there a trignometric way ?.",['trigonometry']
1941354,Elementary solution of exponential Diophantine equation $2^x - 3^y = 7$.,"The title says it all. I would like to have a solution, preferably one which is as elementary as possible, of the exponential Diophantine equation
$$
2^x - 3^y = 7
$$
where $x,y$ are non-negative integers. Note that some small solutions are $(x,y)=(3,0)$ and $(x,y)=(4,2)$. If I really had to solve it at all costs, I would translate this to the problem of finding integral points on a bunch of curves of genus $1$. However, I would like to know if there are any simpler methods out there. As far as I can see, simple congruence tricks won't work: $2^x = 7$ is soluble $3$-adically and $-3^y = 7$ is soluble $2$-adically, so I can't see how we could get anything by looking $p$-adically for $p=2$ or $p=3$, and I think the fact that the solution set to the original problem is non-empty means that $p$-adic considerations for $p \neq 2,3$ have no chance of working either. (But maybe I'm wrong.)","['number-theory', 'diophantine-equations', 'exponential-diophantine-equations', 'elementary-number-theory']"
1941373,How to prove vectors are linearly independent based on determinant,"I am stuck on the following problem: Prove that the vectors $\vec{\text{u}}_1$, $\vec{\text{u}}_2$, and $\vec{\text{u}}_3$ are linearly independent if and only if
  $$ \det \begin{bmatrix} x_1 & y_1 & z_1 \\ x_2 & y_2 & z_2 \\ x_3 & y_3 & z_3 \end{bmatrix} \neq 0 $$ I'm having trouble choosing which direction to go about. I'm aware of the following: The above determinant is equivalent to the triple product of said vectors:
$$ \vec{\text{u}}_1 \cdot (\vec{\text{u}}_2 \times \vec{\text{u}}_3) $$ The three above vectors are linearly independent if the equation
$$ a_1 \vec{\text{u}}_1 + a_2 \vec{\text{u}}_2 + a_3 \vec{\text{u}}_3 = 0 $$
has only the trivial solution. Any two vectors are linearly independent if their dot product is equal to 0. My first attempt was to use the three combinations of dot products to create the following system :
\begin{gather*}
a_1 x_1 x_2 + a_1 y_1 y_2 + a_1 z_1 z_2 = 0 \\
a_2 x_1 x_3 + a_2 y_1 y_3 + a_2 z_1 z_3 = 0 \\
a_3 x_2 x_3 + a_3 y_2 y_3 + a_3 z_2 z_3 = 0
\end{gather*}
and somehow work it into the determinant, but I feel like this isn't the correct way to do it. Another idea was to use Sarrus' rule on the determinant to obtain a new equation through which I could work my way up to the second bullet point, but that doesn't seem to make any sense. Maybe I can't see the forest for the trees; I'm not really good at linear algebra proofs. I'm just looking for a hint to go in the right direction.","['matrices', 'determinant', 'proof-writing', 'linear-algebra', 'vectors']"
1941394,Birthday Problem: Asymptotics of Expected Time Until a Match Occurs,"I'm working on a variant of the birthday problem that I haven't found discussed on this site. Suppose the sequence $(X_n)$ of independent random variables takes values uniformly in $\{ 1,...,N \}$. Let $F_{N} = \min\{ m: X_m = X_k, k<m \}$ be the first time that a match is observed. I want to know what can be said about $E(F_N)$ as $N \to \infty$. It's easy to see that 
$$P(F_N = k) = \frac{N}{N} \frac{N-1}{N}... \frac{N - (k-2)}{N} \frac{k-1}{N}.$$ Hence, 
$$E(F_N) = \sum_{k=2}^{N+1} k \Big[\frac{N}{N} \frac{N-1}{N}... \frac{N - (k-2)}{N} \frac{k-1}{N} \Big]. $$ Any suggestions about where to go from here?","['probability-limit-theorems', 'probability-theory', 'probability']"
1941412,Geometric interpretation of complex intersection points,"In some intersection problems, like the 2D circle-circle intersection , there are two possible solutions that arise from a quadratic equation. If the circles do not intersect on the cartesian plane, the intersection points become complex numbers, because the discriminant of the quadratic is negative. Can these complex intersection points be visualized spatially? And what is the significance of the magnitudes and phases of these complex numbers?","['complex-geometry', 'linear-algebra', 'geometry']"
1941446,How to prove this Harmonic numbers identity?,"While answering a question involving Harmonic numbers $H_n$, I wanted to simplify the terms 
$$f_n = \sum_{i=0}^{n-1}{H_i}^2 - \frac{1}{n}\sum_{0\le i,j\le n-1}H_i H_j.
$$
To do so, I used SageMath to compute $f_n$ for $1\le n \le 100$ and then found the numerators of the resulting sequence to be OEIS sequence A187487 ; i.e., the $n$th numerator is the numerator of $n-H_n$. Indeed, all cases computed are consistent with the following being an identity for $n\ge 1$: $$\sum_{i=0}^{n-1}{H_i}^2 - \frac{1}{n}\sum_{i=0}^{n-1}\sum_{j=0}^{n-1}H_i H_j = n - H_n
$$
which is the same as 
$$\sum_{i=0}^{n-1}{H_i}^2 - \frac{1}{n}\left(\sum_{i=0}^{n-1}H_i\right)^2 = n - H_n.
$$
Does anyone have an idea of how to prove this? Or a reference?","['harmonic-numbers', 'sequences-and-series']"
1941473,"Are there irreducible polynomials of ""infinite"" degree?","If $\alpha$ is transcendental over $F$, we sometimes say that $[F(\alpha): F] =\infty$. I'm wondering if this is true in a literal sense, namely: is there some irreducible polynomial of ""infinite"" degree which has $\alpha$ as one of its roots? My first attempt at formalizing this question was: (*) Can we find some sequence of polynomials $f_1, f_2,\dots$ with roots $r_1, r_2,\dots$ such that $\lim_{n\to\infty} r_n =\alpha$? Unfortunately, at least in some cases the answer to this question is ""yes"" for an uninteresting reason: if we have some Cauchy sequence $q_1, q_2,\dots$ which converges to $\alpha$, then $\{(x - q_i)\}_i$ meets (*) but isn't describing a polynomial of ""infinite"" degree in any meaningful sense. So I'm wondering: is there some meaningful sense in which polynomials of infinite degree can have roots that polynomials of finite degree do not? And if so, can all transcendental numbers the written as the root of such polynomials?","['irreducible-polynomials', 'abstract-algebra', 'polynomials', 'field-theory']"
1941488,Summation of binomial with factor squared $\sum_{k=0}^n k^2 \binom nk^2 = n^2 {2n-2 \choose n-1}$,"How to prove that $$
\sum_{k=0}^n k^2 {n \choose k}^2 = n^2 {2n-2 \choose n-1}
$$ in combinatorial way?","['combinatorics', 'summation', 'binomial-coefficients', 'combinatorial-proofs']"
1941503,Number of equivalence classes of matrices under switching rows and columns [duplicate],"This question already has answers here : Number of equivalence classes of $w \times h$ matrices under switching rows and columns (4 answers) Closed 7 years ago . I have been thinking about this problem: Suppose we have all $R$ by $C$ matrices, where the values are integers in $[1,n]$. Two matrices are equivalent under interchanging rows and columns. For example, 1 5    
0 0 would be equal to itself and to 0 0
1 5 and would be equal to 0 0 
5 1 and would be equal to 5 1
0 0 How many unique such matrices are there? Any ideas how to go about it?
Any help much appreciated!","['matrices', 'combinatorics']"
1941532,Do two empty sets have any elements in common?,"I think that two empty sets do not have any element in common since they do not have any elements in the first place. Should I count $\emptyset$ as a common element? Edited: two empty sets as: $A=\{\;\}$, $B=\{\;\}$.",['elementary-set-theory']
1941590,How does Cramer's rule work?,I know Cramer's rule works for 3 linear equations. I know all steps to get solutions. But I don't know why (how) Cramer's rule gives us solutions? Why do we get $x=\frac{\Delta_1}\Delta$ and $y$ and $z$ in the same way? I want to know how these steps give us solutions?,"['matrices', 'determinant', 'linear-algebra', 'systems-of-equations']"
1941599,Conditional Probability with complements,"Events A and B are such that P(A)=0.7, P(B)=0.2, and P(A∩B)=0.2. Find P(A|B'). I found out that P(A u B) = 0.7, but I'm not sure how to work out the conditional probability - I've tried using the formula and I got P(A|B') = (0.7*0.8)/0.8, but that seems wrong.",['probability']
1941603,Series of binomial coefficients: $\sum\limits_{n=k}^{\infty}{\binom nk}x^n=\frac{x^k}{(1-x)^{k+1}}$,Any hint to prove this? I tried with properties of binomial coefficient but I can't get anything. $$\sum_{n=k}^{\infty}{{n}\choose{k}}x^n=\dfrac{x^k}{(1-x)^{k+1}}$$,"['power-series', 'binomial-coefficients', 'sequences-and-series']"
1941665,Expression for derivative of cross product of two vectors in $S^2$,"Let $a_1,a_2 \in S^2$ where $S^2= \{x\in \mathbb{R}^3 | x \cdot x = 1  \}$. From the tangent space structure we have, $\frac{d}{dt} a_1 = a_1 \times w_1, \quad \frac{d}{dt} a_2 = a_2 \times w_2, \quad w_1,w_2\in \mathbb{R}^3$ where $\times$ represents the standard vector product in $\mathbb{R}^3$. I am trying to find the following - if $a_3 = a_1 \times a_2$, can I find $w_3\in \mathbb{R}^3$ such that $\frac{d}{dt} a_3 = a_3 \times w_3$ ? Attempts so far : $\frac{d}{dt}(a_1 \times a_2) = (a_1 \times (a_2 \times w_2)) + ((a_1 \times w_1)\times a_2) $ $ \quad = -(w_2 \times (a_1 \times a_2)) - (a_2 \times (w_2 \times a_1))- (a_2\times a_1) \times w_1 - (w_1\times a_2) \times a_1 $ applying the vector triple product property, $ \quad = (a_1 \times a_2) \times w_2 - (a_2 \times (w_2 \times a_1))+ (a_1\times a_2) \times w_1 - (w_1\times a_2) \times a_1 $ stuck here.","['cross-product', 'differential-geometry', 'calculus']"
