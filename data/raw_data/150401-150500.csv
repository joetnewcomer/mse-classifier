question_id,title,body,tags
2502307,"If $ax + by = c$ has an integer solution for $x,y$ then it has infinitely many","Let $a, b, c$ be integers. Show that if the equation $ax + by = c$ has a solution where $x$ and $y$ are integers, then it has infinitely many such solutions. How would I go about showing this? I can start off by doing the following: $$by = c - ax$$ $$y = \frac{c - ax}b$$","['discrete-mathematics', 'elementary-number-theory']"
2502335,Random Walks and the Locus of Partial Sums of the Riemann Zeta Function,"It is a relatively well-known fact that Random Walks are a pretty good approximation of the locus of partial sums for the zeta function. For instance let $\zeta_k(s)$ denote the $k^\text{th}$ partial sum of the Riemann zeta function: $$
\zeta_k(s) := \sum_{n=1}^k n^{-s}
$$ Then we can consider the image of the partial sums up to $k$ by  $Z_k=\{\zeta_m(s):1\leq m\leq k \}$. Now I don't know very much about random walks; my knowledge of them can be summarized by: ""A random walk is a random process, that describes a path that consists of a succession of random steps on some mathematical space such as the integers"" as per the Wikipedia definition. Never the less I know what their graphs can look like. This random walk is a representation of Brownian Motion for example. I was playing around with the locus of partial sums and the thought occurred to me that these to objects may in fact be related since they look very similar. Computing $\zeta_k(\frac12 + i1.5\times 10^9)$ and computing the locus for $Z_{12500}$ gives the following graph: There is clearly some resemblance here between these two objects. As it turns out I was not the first to notice this and as it turns out it is fairly well known that certain random walks make excellent approximations for the locus of zeta function partial sums with a sufficiently high imaginary value. My question here is why? Random Walks are by their very nature random, but the zeta function is absolutely deterministic? How can we model random walks with something that is absolutely not random? Can anyone offer some kind of intuition for this? Or any suggested readings?","['random', 'random-walk', 'zeta-functions', 'number-theory', 'riemann-zeta']"
2502349,Show that exactly half of the integers in the set are quadratic residues,"Let p be an odd prime. Show that exactly half of the integers in the set {1,2,...,p− 1} are quadratic residues. Can somebody please dumb down the solution to this? I'm in the process of learning about modulo. Thank you!","['discrete-mathematics', 'elementary-number-theory']"
2502379,Annihilator polynomial and the Laplace Transform,"Let $$L(f(t)) = F(s)=N(s)/M(s)$$ be the Laplace Transform of a certain (exponential-bounded) function, $N(s)$ and $M(s)$ being minimal polynomials. Is it true that the operator $M(D)$ always annihilates $f(t)$ (i.e., $M(D)f(t) = 0$, where $D=d/dt$)? Does this fact generalize to some more general function types? I'll illustrate this with several examples: 1) $f(t) = \sum_{k=0}^n c_k t^k, c_k$ are real parameters, $n$ is a natural number. In this case, $F(s)= pol_n(s)/s^{n+1}$, where $pol_n(s)$ is a polynomial in $s$ of degree $n$. $M(s)=s^{n+1}$, so $M(D)=D^{n+1}$, which clearly annihilates $f(t)$. 2) $f(t) = e^{at}, a$ is a real parameter. In this case, $F(s)=1/(s-a)$. $M(s)=s-a$, so $M(D)=D-a$, which clearly annihilates $f(t)$. 3) $f(t) = c_1\sin(wt) + c_2\cos(wt)$, where $w, c_1$ and $c_2$ are real parameters. In this case, $F(s)= (c_1w + c_2s)/(s^2+w^2)$. $M(s)=s^2+w^2$, so $M(D)=D^2+w^2$, which clearly annihilates $f(t)$. You could actually take advantage of several theorems about the Laplace Transform (e.g. Shift on the $s$-plane) to calculate the annihilator of more complex functions: 4) $f(t) = c_1e^{at}\sin(wt) + c_2e^{at}\cos(wt)$, where $a, w, c_1$ and $c_2$ are real parameters. In this case, $F(s)= (c_1w + c_2(s-a))/((s-a)^2+w^2)$. $M(s)=(s-a)^2+w^2$, so $M(D)=(D-a)^2+w^2$, which annihilates $f(t)$. 5) $f(t) = c_1t\sin(wt) + c_2t\cos(wt)$, where $w, c_1$ and $c_2$ are real parameters. In this case, $F(s)= (2c_1ws + c_2(s^2-w^2))/(s^2+w^2)^2$. $M(s)=(s^2+w^2)^2$, so $M(D)=(D^2+w^2)^2$, which annihilates $f(t)$. 6) $f(t) = c_1t^ne^{at}$, where $a, c_1$ are real parameters and $n$ is a natural number. In this case, $F(s)= c_1n!/(s-a)^{n+1}$. $M(s)=(s-a)^{n+1}$, so $M(D)=(D-a)^{n+1}$, which annihilates $f(t)$. Finally, you could sum up some of the previous results to get the following: 7) $f(t) = (\sum_{k=0}^n c_k t^k)e^{at}\sin(wt) + (\sum_{k=0}^n d_k t^k)e^{at}\cos(wt)$, where $a, w, c_k$ and $d_k$ are real parameters. In this case, $M(D)=((D-a)^2+w^2)^{n+1}$, which annihilates $f(t)$. What about $f(t)= \sqrt{t}$, for which $F(s)=\sqrt{\pi}/(2s^{3/2})$?","['ordinary-differential-equations', 'laplace-transform']"
2502435,$A$ is consistent iff the augmented matrix has no pivot in last column.,"A linear system of equations is inconsistent (does not have a solution) if and only
  if there is a pivot in the last column of an echelon form of the
  augmented matrix. I can understand the if part, that is because $0=1$ is impossible. But how do you prove the only if part? All of the posts I found on this site prove the if part only. My book says for only if part, ""If we don’t have such a row, we just make the reduced echelon form and then read the solution off it."" I don't think this is a proof to the only if part.",['linear-algebra']
2502439,Expectation of product of independent random variables,"I'm stuck trying to show $E(XY) = E(X)E(Y)$ for $X, Y$ nonnegative bounded independent random variables on a probability space. The definition of independence is that $P(\{ X \in B\} \cap \{ Y \in C\}) = P(X \in B) P(Y \in C)$ for Borel sets $B$ and $C$. I'm not assuming $X$ or $Y$ have probability density functions so I cannot use them. Nor can I use conditional expectation.","['expectation', 'probability', 'random-variables', 'probability-distributions']"
2502457,Prove by induction that $\sum_{i=1}^{n} i \cdot 2^i = (n-1) \cdot 2^{n+1} +2$. Help finding my mistake,"Prove by induction:
  $$\sum_{i=1}^{n} i \cdot 2^i = (n-1) \cdot 2^{n+1} +2$$ Basis : let $p(n)$ be the predicate. Let $n=1$ this gives $(1-1) \cdot 2^1+1+2 = 2$ and $1 \cdot 2^1 = 2$ so its true for $p(1)$ Induction : assume $n=k$ thus $$\sum_{i=1}^{k} i \cdot 2^i = (k-1) \cdot 2^{k+1} +2$$ when $n=k+1$: $$\sum_{i=1}^{k+1} i \cdot 2^i = \sum_{i=1}^{k} i \cdot 2^i + ((k+1)-1) \cdot 2^{(k+1)+1} +2$$ $$ =(k-1) \cdot 2^{k+1} +2 + ((k+1)-1) \cdot 2^{k+2} +2$$ $$= 2^{k+1}k - 2^{k+1} +2 +2^{k+2}(k+1) - 2^{k+2}+2$$ $$=\frac{2^{k+1}k-2^{k+1}+2 \cdot 2^{k+1}(k+1)-2 \cdot 2^{k+1} +4}{2}$$ $$=2^{k+1}k-2^{k+1}+2^{k+1}(k+1)-2^{k+1}+2$$ $$= (k-1) \cdot 2^{k+1}+((k+1)-1) \cdot 2^{k+1} +2$$ I've been messing around with the arithmetic part of this for a while now and it's getting frustrating, I'm thinking that I've possibly made a mistake at the beginning of the induction step and that's why I cant get this to work, or maybe I'm just missing something in the algebra, either way can someone please help.","['induction', 'proof-verification', 'discrete-mathematics']"
2502499,For which $n$ does there exist a surjective homomorphism from $SL_n(\mathbf{R})\rightarrow PGL_n(\mathbf{R})$?,"Also, how does the situation change when replacing $\mathbf{R}$ with $\mathbf{Q}$? I have only very basic tools to approach this problem. My attempt at understanding it is that $PGL_n$ is the set of linear transformations which leave the norms of vectors the same, while $SL_n$ is the set of linear transformations which preserve volumes and their orientations. But I must have some misunderstanding because then one could consider in $PGL_4$ the matrix $$\left(\begin{array}{cccc}-1 & 0 & 0 & 0 \\ 0 & 1 & 0  &0 \\ 0& 0&1&0\\0&0&0&1
\end{array}\right)$$ for which I do not think you can come up with a matrix in $SL_4$ to be its preimage. But I have high confidence that this approach is incorrect so I would like to be pointed in a better direction.","['abstract-algebra', 'group-homomorphism', 'infinite-groups', 'group-theory', 'linear-algebra']"
2502533,"When solving a differential equation, why do we always start with guessing the solution is exponential function? [duplicate]","This question already has answers here : $e^{mx}$ in solving second order differential equations (4 answers) Closed 6 years ago . For example, when solving second order differential equation with constant coefficients, we start with guessing the solution is the form of the linear combination of two independent exponential functions. I read some explanations saying it's for the convenience of integral and derivation.  Is there any more reason? 
I'm still an undergraduate student, please write an answer in a way that I can understand.",['ordinary-differential-equations']
2502571,Why can I not do this substitution when counting integer solutions?,"$$a_1 + \ldots + a_5 = 10$$ where $2\leq a_k \leq 6$ for all $k=1,2,\ldots,5$. Let $x_k := a_k - 2$, so $0 \leq x_k \leq 4$, and has the same number of solutions as $$ x_1 + \ldots + x_5 + 2\times 5 = 10$$ $$x_1 + \ldots + x_5 = 0.$$ However, this has only one solution, that is, $x_1 = x_2 = \ldots = x_5 = 0$. The original equation seems to have more than one solution. What went wrong here?",['discrete-mathematics']
2502600,Solving recursion $f(n+1)=\frac n{n+2}f(n)$,"How can I find an expression for $$f(n), n \in \mathbb{N}$$ if $$f(n+1)=\frac{n}{n+2}\cdot f(n)$$ and $$f(1) = 999$$ I'm used to solve simple homogeneous recursion relations by the chacarcteristic equation, but it seems impossible in this situation. What can I do? Thank you for your attention.","['recurrence-relations', 'functional-equations', 'functions', 'number-theory', 'recursion']"
2502601,Are all probability density functions described by their mean and variance?,"The question might be trivial, but I would like someone to correct me or confirm it. I know that the Normal (Gaussian) distribution is completely determined by its mean and variance, but does that hold for any other distribution? I assume that the answer is no. I could notice this is true for many distributions, but there are some exceptions. For example, mean and variance are undefined for the Cauchy distribution.","['probability-theory', 'probability', 'density-function', 'probability-distributions']"
2502605,Factor rational function of two variables,"Is there a factorization $$\frac{1}{1-wz} = f(w)g(z),$$ where $f$ does not depend on $z$ and $g$ does not depend on $w$? My assumptions are that $w,z \in \mathbb{C}$ and that $f,g:\mathbb{C}\rightarrow\mathbb{C}$. The factorization does not need to hold for all $(w,z)\in\mathbb{C}\times\mathbb{C}$, I would also be interested in subsets of $\mathbb{C}\times\mathbb{C}$ for which it might hold.","['multivariable-calculus', 'complex-analysis', 'real-analysis']"
2502655,let $f(\frac{x}{3})+f(\frac{2}{x})=\frac{4}{x^2}+\frac{x^2}{9}-2$ then find $f(x)$,"let $f(\frac{x}{3})+f(\frac{2}{x})=\frac{4}{x^2}+\frac{x^2}{9}-2$
then find $f(x)$ My Try : $$f(\frac{x}{3})+f(\frac{2}{x})=(\frac{2}{x})^2-1+(\frac{x}{3})^2-1$$ So we have : $$f(x)=x^2-1$$ it is right ?Is there another answer?","['algebra-precalculus', 'functional-equations']"
2502751,Graphs and probabilities,I was given the next question: Given that I have an infinite graph of the natural numbers as its vertices and the fact that for two vertices there is a probability of $0.5$ that they have an edge between them. I was asked to find the probability that the graph is connected (the graph is undirected). My question is how do I approach this type of question? I tried looking at a smaller portion of a set of numbers yet I feel kind of lost. Any clues will be very helpful! I'd really like a clue instead of answering the question right away! Thank you very much.,"['graph-theory', 'probability-theory', 'probability']"
2502778,Problem on Extremal Graphs: three independent paths,"The problem states that: A simple (or strict) graph with $n$ vertices and $m>\dfrac{3(n-1)}{2}$ edges, has two vertex
  joined by three independent paths. Any hints, ideas or useful results, to attack this problem?","['combinatorics', 'graph-theory', 'extremal-graph-theory', 'discrete-mathematics']"
2502782,The difference between convergence in probability and convergence in distribution.,"I'm confused with the concepts of convergence in probability and convergence in distribution. After reading some examples in Wiki, can I say convergence in probability means the decrease of variance as n goes to infinity, that is we become more and more confident that one outcome in the sample space will happen? Convergence in distribution just indicates the probability distribution.","['probability-theory', 'convergence-divergence', 'probability-distributions']"
2502800,An Intuitive Understanding of Covariance,"I'm trying to intuitively understand what it means for two random variables to have non-zero covariance. At the moment, I imagine that the two random variables (which have non-zero covariance) both depend (at least in part) on a common random variable, and this is why they have non-zero covariance. E.g., if $U$, $V$, $Z$ and $W$ are independent random variables, and $X_1 = \frac{U + V}{Z}$ and $X_2 = W + Z$, then since both $X_1$ and $X_2$ depend on $Z$, the two variables will have non-zero covariance. Is this a good way of intuitively thinking about covariance? If not, is there a better way which might help?","['intuition', 'statistics', 'covariance', 'correlation']"
2502811,Bound for graph's size with cycles,"I have the following problem: Let $G$ be a simple graph with $n$ vertices and such that every cycle in $G$
  has length $\leq3$. Then $e(G)\leq\dfrac{3(n-1)}{2}$.
  (where $e(G)$ is the number of edges of the graph $G$) I think it could be a good idea to prove this by induction, but do not know how to use the bound for the length of the cycles.","['combinatorics', 'graph-theory', 'discrete-mathematics']"
2502813,Sequence is union of convergent subsequences. Are the limits of the subsequences the only cluster points the sequence have?,"Given a sequence $(a_n)_n$, when one is asked to find all the cluster points of this sequence in $(\mathbb{R},d_E)$, one can find convergent subsequences and their limit is a cluster point. However, to make sure we have found all the cluster points, does it suffice to show that if we can write the (range of the) sequence as finite union of (ranges of) convergent subsequences, then the limits of the subsequences are ALL cluster points. I managed to prove that all these limits of subsequences are cluster points, but I don't know how to prove that these are all possible cluster points. Formally, my question is: Is the following statement true, and why: Suppose $(a_n)_n$ is a sequence and $$\{a_n|n \in \mathbb{N}\} =
\bigcup_{i=1}^m \{a_{{ki}_n}|n \in \mathbb{N}\}$$ with $a_{{ki}_n} \to
 a_i$. Then $\{a_1, \dots , a_m\}$ is the set of all cluster points of
  $(a_n)_n$ My definition of cluster point: $x$ cluster point of a sequence $(a_n)_n$ in a metric space $(X,d)
 \iff \forall \epsilon > 0: \forall n \in \mathbb{N}: \exists m > n:
 d(x_m,x)  <\epsilon$ Can someone hint me into the right direction? (I don't even know if this statement is true or false actually)","['real-analysis', 'sequences-and-series', 'convergence-divergence']"
2502863,Twisted Sheaf of Serre identified with Moebius strip,"Let consider the twisted sheaf $\mathcal{O}_{\mathbb{P}^1 _k}(1)$ over projective line $\mathbb{P}^1 _k$. Can anybody explain to me where the visualisation ""twisted"" come from. I read that $\mathcal{O}_{\mathbb{P}^1 _k}(1)$ as line bundle can be associated with Moebius strip in some sence which isn't clear to me. So intuitively I suppose the ""twist"" comes from the geometrical analogy that Moebius strip is a ""twisted"" cylinder. Presumably. But I have no idea where the identification of $\mathcal{O}_{\mathbb{P}^1 _k}(1)$ with a line bundle that is isomorphically to Moebius strip comes from. Is there a way to imagine it preferably geometrically?","['sheaf-theory', 'algebraic-geometry']"
2502883,Proving the Markov inequality for a non-negative random variable,"If U is a non-negative random variable and it has pdf $f_U(u)$, how can we prove the Markov inequality $$E[U]\geq b P(U\geq b),$$ where b is a constant? Not sure how to prove this and haven't really gotten anywhere. Thanks for any help.","['probability-theory', 'statistics', 'random-variables']"
2502896,show that $\sum_{k=1}^{n}(1-a_{k})<\frac{2}{3}$,"Let $a_{1}=\dfrac{1}{2}$, and such $a_{n+1}=a_{n}-a_{n}\ln{a_{n}}$,show that
$$\sum_{k=1}^{n}(1-a_{k})<\dfrac{2}{3}$$ My attemp: let $1-a_{n}=b_{n}$,then we have
$$b_{n+1}=b_{n}+(1-b_{n})\ln{(1-b_{n})}<b^2_{n}<\cdots<(b_{1})^{2^{n}}=\dfrac{1}{2^{2^n}}$$
where use $\ln{(1+x)}<x,x>-1$
so
$$\sum_{k=1}^{n}(1-a_{k})<\sum_{k=1}^{n}\dfrac{1}{2^{2^{k-1}}}?$$ But $$\sum_{k=1}^{+\infty}\dfrac{1}{2^{2^{k-1}}}=0.816\cdots$$big than$\frac{2}{3}$,so this inequality How to prove it?","['inequality', 'sequences-and-series']"
2502932,What if the relative error is undefined?,The relative error is defined by the simple formula: $$\text{Rel. Error} = \frac{|v_\text{approx}-v_\text{analytical}|}{v_\text{analytical}}$$ but what if the theoretical value $v_\text{analytical}$ should be $0$? then our relative error is undefined.... this is also quite a common occurs. If our analytical function is $x^2$ then at its $x=0$ we have a problem. I'm trying to program this on a computer. How do I make sure that I don't have any problems with this formula?,['statistics']
2502963,Infinite series for $e$...,How do you prove that $e=\sum_{n=0}^{\infty}\frac{1}{n!}$? Here I am assuming $e:=\lim_{n\to\infty}(1+\frac{1}{n})^n$. Do you have any good PDF file or booklet available online on this? I do not like how my analysis text handles this...,['analysis']
2502976,Solving $a \sin\theta + b \cos\theta = c$,"Could someone help me with the steps for solving the below equation $$a \sin\theta + b \cos\theta = c$$
I know that the solution is $$\theta = \tan^{-1} \frac{c}{^+_-\sqrt{a^2 + b^2 - c^2}} - \tan^{-1} \frac{a}{b} $$
I just can't figure out the right steps to arrive at this solution.",['trigonometry']
2502982,Prove that: $1-\frac 12+\frac 13-\frac 14+...+\frac {1}{199}- \frac{1}{200}=\frac{1}{101}+\frac{1}{102}+...+\frac{1}{200}$,"Prove that: $$1-\frac 12+\frac 13-\frac 14+...+\frac {1}{199}- \frac{1}{200}=\frac{1}{101}+\frac{1}{102}+...+\frac{1}{200}$$ I know only this method: $\frac {1}{1×2}+\frac {1}{2×3}+\frac {1}{3×4}+....=1-\frac {1}{2}+\frac {1}{2}-\frac {1}{3}+\frac {1}{3}-...$ But, unfortunately, I could not a hint.","['number-theory', 'sequences-and-series']"
2503041,Convergence of probability measures converging on generating set,"Let $(X,\mathcal{A})$ and $(Y,\mathcal{B})$ be two measurable spaces and let $(\mathbf{P}_n)_{n=1}^{\infty}$ be a sequence of probability measures in $(X\times Y, \mathcal{A} \otimes \mathcal{B})$ that satisfies the following condition: There exists a set function $\mathbf{P}\colon \mathcal{A} \times \mathcal{B}\rightarrow \mathbb{R}$ such that for every $A\times B \in \mathcal{A} \times \mathcal{B}$ it holds that $\mathbf{P}_n(A\times B)\rightarrow \mathbf{P}(A\times B)$, when $n\rightarrow \infty$. The question is, can $\mathbf{P}$ always be extended to a measure $\hat{\mathbf{P}}$ in $(X\times Y, \mathcal{A} \otimes \mathcal{B})$ or are there counterexamples? Furthermore, is this extension unique if it exists?","['probability-theory', 'convergence-divergence', 'product-measure']"
2503050,Approximation of region of rejection,"Let $X_1,...,X_n$ be a random sample of size $n$ from the beta distribution $B(\theta, 1)$ whose pdf is given by $f(x;\theta)=\theta x^{\theta-1}(0<x<1)$ and the hypotheses are given by: $H_0: \theta=1$   vs.   $H_1: \theta \neq 1$ Using likelihood ratio test with significance level $\alpha$ ($0<\alpha<1$), I ""already found"" that$-2\sum_{i=1}^n \log X_i \sim \chi^2 (2n)$ and the rejection region is given by: $$-2\sum_{i=1}^n \log X_i \le 2nc_1 \text{ or } -2\sum_{i=1}^n \log X_i \ge 2nc_2$$ where the constants $c_1, c_2$ satisfy $\int_{2nc_1}^{2nc_2}f_{2n}(x) \, dx = 1-\alpha$ and $c_1- \log{c_1} =c_2-\log{c_2} $, $f_{2n}$ is the pdf of $\chi^2(2n)$ distribution. Here is the problem: Show that the constants $c_1,c_2$ can be approximated as $c_1 \approx \dfrac{ \chi^2_{1-\alpha/2}(2n)}{2n},c_2 \approx \dfrac{ \chi^2_{\alpha/2}(2n)}{2n}$ when $n$ is sufficiently large. I attempted to prove that the two integrals $\int_0^{2nc_1}f_{2n}(x) \, dx$ and $\int_{2nc_2}^\infty f_{2n}(x) \, dx$ both converge to $\alpha/2$ when $n \to \infty$. Since the sum of the two integrals is $\alpha$, it suffices to show that the first integral goes to $\alpha/2$. Here is where I stuck. Does anyone have ideas? Any hints or advice will help a lot! Thanks.","['statistics', 'hypothesis-testing', 'approximation']"
2503112,Geometry task about inequalities,"Given isosceles triangle ABC (AB=BC) and points D, E (AD=CE),
how do I prove that $BD+BE > AB+BC$? This is a task for middle school, so cosine theorem cannot be used (and I am not sure that it could help anyway). I guess (but I can be wrong) that the triangle inequality must be applied somehow to solve it, but I do not see how.","['geometric-inequalities', 'triangles', 'geometry']"
2503114,Find sum of all positive real numbers $x$ such that $\sqrt[3]{2+x} + \sqrt[3]{2-x}$ becomes an integer.,"Find sum of all positive real numbers $x$ such that $\sqrt[3]{2+x} + \sqrt[3]{2-x}$ becomes an integer. I thought Euler's identity may help,so letting $a=\sqrt[3]{2+x}, b=\sqrt[3]{2-x},c=-y$ ($y \in \Bbb Z)$ we have : $a+b+c=0$ leading to :$(2+x)+(2-x)+(-y^3)=3\sqrt[3]{2+x} \sqrt[3]{2-x}(-y)$, but at this point I think sum of such $x's$ can't be found.",['algebra-precalculus']
2503128,Extra distance travelled along a sine wave path,"Let's say I have a blue line which is 10 metres long.  I then draw a single cycle of a sine curve along this line, in red which has a maximum distance of 0.1 meter from the line (so an amplitude of 0.1m). If I walk along the sine curve, I will walk further than 10 metres.  But how do I calculate this new distance travelled? What about if I increase the frequency so that there are five full cycles along my 10 metre blue line?  How does the frequency affect it, in other words?","['curves', 'geometry']"
2503165,Analytification of algebraic differential forms,"Let $X$ be a complex projective variety and denote by $\Omega_X^k$ the (coherent) sheaf of algebraic differential $k$-forms on $X$. Via Serre's GAGA, we obtain a sheaf $(\Omega_X^k)^{\text{an}}$ on $X^{\text{an}}$. Does this coincide with $\Omega_{X^{\text{an}}}^k$, the sheaf of holomorphic $k$-forms on the complex manifold $X^{\text{an}}$? If so, why? I think I can see this in the case that $X$ is smooth, because then $\Omega_X^k$ is locally free, but I'm not sure for the general case.","['complex-geometry', 'differential-forms', 'algebraic-geometry']"
2503221,Every open subset is affine,"I have the following question: given a ring $R$, what conditions should verify $R$, such that  every open subset of $\text{Spec}(R)$ is affine?","['affine-schemes', 'algebraic-geometry']"
2503255,Figuring out the Venn Diagram [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question In a class that all students might play minimum piano, violin or guitar, there are $24$ students. In this class, the students who might play violin is equal to $2$ times of the students who might only play guitar. If there are $9$ students who might play piano but not violin, how many students who might play violin are there? I'm confused by this question. How do we figure out the venn diagram? Regards",['elementary-set-theory']
2503334,"Connection of gradient, Jacobian and Hessian in Newton's method","Suppose $f: \mathbb{R^n} \to \mathbb{R}$, the gradient of $f(\mathbf{x})$ is $$\mathop{\nabla} f(\mathbf{x}) = \begin{bmatrix} \frac{\partial{f}}{\partial{x_1}} \\ \vdots \\ \frac{\partial{f}}{\partial{x_n}} \end{bmatrix}$$ The Jacobian matrix of $\mathop{\nabla} f(\mathbf{x})$ is $$\begin{align} \mathbf{D} (\mathop{\nabla} f(\mathbf{x})) &= \begin{bmatrix} \frac{{\partial^2}f}{{\partial}x_1^2} & \frac{{\partial^2}f}{{\partial}x_2{\partial}x_1} & \cdots & \frac{{\partial^2}f}{{\partial}x_n{\partial}x_1}\\
\frac{{\partial^2}f}{{\partial}x_1{\partial}x_2} & \frac{{\partial^2}f}{{\partial}x_2^2} & \cdots & \frac{{\partial^2}f}{{\partial}x_n{\partial}x_2}\\
\vdots & \vdots & \ddots & \vdots\\
\frac{{\partial^2}f}{{\partial}x_1{\partial}x_n} & \frac{{\partial^2}f}{{\partial}x_2{\partial}x_n} & \cdots & \frac{{\partial^2}f}{{\partial}x_n^2}\\  \end{bmatrix} \\ &= \mathbf{H}^T \end{align}$$ where H is the Hessian matrix, which is consistent with the definition in Wikipedia . The affine approximation of $\mathop{\nabla} f(\mathbf{x})$ around $x_n$ is $$\mathop{\nabla} f(\mathbf{x}) = \mathop{\nabla} f(\mathbf{x_n}) + \mathbf{D}  (\mathop{\nabla} f(\mathbf{x_n})) (x - x_n) = \mathop{\nabla} f(\mathbf{x_n}) + \mathbf{H}^T (x - x_n)$$ Setting $\mathop{\nabla} f(\mathbf{x}) = 0$ gives the Newton-Raphson update as $$x_{n+1} := x_n - \mathbf{H}^{-T} \mathop{\nabla} f(\mathbf{x_n}) $$ However, in Wikipedia the Newton-Raphson update is given as $x_{n+1} := x_n - \mathbf{H}^{-1} \mathop{\nabla} f(\mathbf{x_n})$. The Hessian matrix is not symmetric if the entry of the matrix is not continuous. Did I do anything wrong with my calculation? If not, does this mean we can generally treat Hessian matrix as symmetric in practice for optimization?","['hessian-matrix', 'newton-raphson', 'multivariable-calculus', 'jacobian', 'vector-analysis']"
2503336,Prove that the value of the expression $|a_1-b_1|+|a_2-b_2|+\dots+|a_n-b_n|$ does not depend on the coloring.,"Now we have some $n$ of the number from the set $\{1,2,\dots,2n\}$ colored red and the rest of them are colored blue. Say $a_1<a_2<\dots<a_n$ are red and $b_1>b_2>\dots>b_n$ are blue. Prove that the value of the expression $$
E = |a_1-b_1|+|a_2-b_2|+\dots+|a_n-b_n|
$$ does not depend on the coloring. All I can do is to calculate this $E$ if we take $a_i=i$ and $b_i = 2n+1-i$ for all $i\leq n$ . In this case we get \begin{align}
E &= (n+1)+(n+2)+\dots+(2n) -1-2-\dots-n \\
&= n+n+\dots+n = n^2
\end{align} Clearly it wants us to prove that $E$ is invariant for such colorings. Does any one has any idea how to prove it. Strong assumption is that it should be done with induction.","['invariance', 'combinatorics', 'induction']"
2503363,"Pedagogical examples of distinguishing ""types of symmetry""","When speaking to interested parties lacking formal mathematical background, I've illustrated how different objects can have the same number of symmetries and yet have different types of symmetry with the example of an oriented hexagon (obtained by putting arrows on the edges in cyclic fashion) versus an unadorned triangle. These have cyclic and dihedral symmetry groups, respectively, each with six symmetries. There are multiple reasons these are distinct types of symmetry: (a) the rotations of the hexagon can all be generated by a single symmetry (unlike with the triangle, where flips and rotations cannot be generated by a single symmetry); (b) any two rotations of the oriented hexagon (by $x$ and $y$ degrees) can be performed in either order, but applying a flip and a rotation of the triangle in different orders yield distinct symmetries (they permute the vertices differently); (c) one can tabulate how many applications of each symmetry is required to achieve an initial state, and notice we obtain a different list of numbers for the two symmetry groups. In other words, cyclicity, commutativity, and order information. However, when I've asked people to identify a reason the triangle and oriented hexagon have different types of symmetry, they've consistently (and understandably) identified the fact the triangle has reflectional symmetry while the oriented hexagon doesn't. Which is a perfectly valid answer, because my question is ambiguous: they are identifying why the individual symmetries are different types of transformations, instead of looking at the internal structure of the symmetry group. When I explain I am interested in answers that look instead at how the symmetries interact with each other, I am met with confusion, although when I explain (a), (b) and (c) above it is more clear what I intend. Question : What are some pedagogical examples of distinguishing basic symmetry groups that avoid this pitfall? To be specific, two types of situations could fit the bill: Different symmetry groups where the individual symmetries are the same types of transformations of some figure or basic mathematical object. Different figures or mathematical objects with the same symmetry group but where some individual symmetries do different types of things. Examples of type (1) would be more powerful in my opinion, but for added effect examples of type (2) may be explored first. Preferably examples should be basic, close to napkin-level math. Here's the simplest case of (2) which I thought of: (i) on the one hand, consider a kind of ""sunset"" picture that has reflectional symmetry across the horizon, but say there's a seagull on the right or left side that precludes sliding or vertical reflections from being symmetries, and (ii) on the other hand, the outline of the yin-yang symbol (so, not painting it in), which has no reflectional symmetries but has a single nontrivial $180^{\circ}$ rotational symmetry.","['education', 'abstract-algebra', 'symmetry', 'group-theory']"
2503391,spectral projector of the Laplacian on $\mathbb{R}^d$,"I have been reading some notes on spectral theorem for unbounded operator on Hilbert spaces and my toy example is the Laplacian on $\mathbb{R}^d$. The author says that as an ""application"" one can define the spectral projector of the operator. For the Laplacian on $\mathbb{R}^d$ the spectral projector is 
$$ \chi_{[0, a]}(-\Delta)$$ where $\chi_A$ denotes the characteristic function of a set $A$, subset of the its spectrum, i.e. of $[0,\infty)$. Then, I saw the claim $$\chi_{[0, a]}(-\Delta): L^2(\mathbb{R}^d)\rightarrow E_a$$ where $E_a$ is the subspace of all $L^2(\mathbb{R}^d)$-functions with Fourier Transform supported in $[-a,a]^d$. I am trying to figure out now what $\chi_{[0, a]}(-\Delta) f$ for any $L^2$-function $f$ is but I am lost. Can anybody explain me that? I understand that in some sense the spectral projector gets rid of all the eigenvalues bigger than $a$, but what this has to do with Fourier Transform? Any help is welcome.","['eigenvalues-eigenvectors', 'partial-differential-equations', 'operator-theory', 'functional-analysis', 'spectral-theory']"
2503401,Information on Horseshoe Integration [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. This question is not about mathematics, within the scope defined in the help center . Closed 6 years ago . Improve this question I was watching the following video (around 2:56), and he mentioned ""horseshoe mathematics/horseshoe integration"", though I was unable to find any information on what this is. If someone know's more about the topic could you perhaps provide some resources to learn more about it? Thanks","['reference-request', 'integration', 'calculus']"
2503428,Derivative of Binary Cross Entropy - why are my signs not right?,"I'm trying to derive formulas used in backpropagation for a neural network that uses a binary cross entropy loss function.  When I perform the differentiation, however, my signs do not come out right: Binary cross entropy loss function:
$$J(\hat y) = \frac{-1}{m}\sum_{i=1}^m y_i\log(\hat y_i)+(1-y_i)(\log(1-\hat y)$$ where $m = $ number of training examples $y = $ true y value $\hat y = $ predicted y value When I attempt to differentiate this for one training example, I do the following process: Product rule:
$$ \frac{dJ}{d\hat y_i} = -1(\frac{d}{d\hat y_i}(y_i\log(\hat y_i)+(1-y_i)(\log(1-\hat y)))) $$ Sum rule:
$$ = -1(\frac{d}{d\hat y_i}y_i\log(\hat y_i)+\frac{d}{d\hat y_i}(1-y_i)(\log(1-\hat y))) $$ Product rule, deriv of constant (treating $y$ as a constant) and deriv of natural log:
$$ = -1(\frac{y_i}{\hat y_i} + \frac{1-y_i}{1 - \hat y_i})$$ However, this is different from the expected result:
$$ \frac{dJ}{d\hat y_i} = -1(\frac{y_i}{\hat y_i} - \frac{1-y_i}{1 - \hat y_i}) $$ Not sure what's going wrong.  I'm sure I'm doing something incorrectly, but I can't figure out what it is.  Any help is appreciated!","['derivatives', 'machine-learning', 'linear-algebra']"
2503430,Find $f(x)$ such that $f(\frac{3}{x})+f(\frac{x}{2})=\frac{9}{x^2}+\frac{x^2}{4}-2$ [duplicate],"This question already has answers here : let $f(\frac{x}{3})+f(\frac{2}{x})=\frac{4}{x^2}+\frac{x^2}{9}-2$ then find $f(x)$ (3 answers) Closed 6 years ago . How to find $f(x)$ from below relation ?
$$f\left(\frac{3}{x}\right)+f\left(\frac{x}{2}\right)=\frac{9}{x^2}+\frac{x^2}{4}-2$$ I have two question about this .I find $f(x)=x^2-1$  that work fine ,but $\\(1)$ how can be sure that there is no other solutions ? $\\(2)$ How can find $f(x)$ in general ?","['calculus', 'functions']"
2503445,Trace and Lie derivative of a tensor commute,"How can we show that trace and Lie derivative of a $(1,1)$-tensor commute? That is, I want to prove that
$$
\operatorname{tr}(L_X S) = L_X \operatorname{tr}S,
$$
where $S$ is a $(1,1)$-tensor. This is Exercise 2.5.10 of Riemannian Geometry (Third Edition), by Peter Petersen . My idea is to consider the definition of a trace of a linear operator $L : V \to V$ (where $V$ is a vector space) given by
$$
\operatorname{tr}L = \sum_{i=1}^n g(L(e_i),e_i),
$$
where the $e_i$'s form an orthonormal basis. Also, Exercise 2.5.9 (the exercise preceding this one) asks to demonstrate that $\operatorname{tr} (\nabla_X S) = \nabla_X \operatorname{tr}S$, and perhaps this may be used in the proof of the current problem? I also wanted to type change the $(1,1)$-tensor $S$ into a $(0,2)$-tensor $\hat S$ via the fomula $\hat S(X,Y) = g(S(X),Y)$, so that I can use Proposition 2.1.2 of Petersen: If $X$ is a vector field and $T$ a $(0,k)$-tensor on a manifold $M$, then
  $$
(L_X T)(Y_1,\ldots,Y_k)=D_X(T(Y_1,\ldots,Y_k))-\sum_{i=1}^k T(Y_1,\ldots,L_X Y_i,\ldots,Y_k).
$$ Can I request any thoughts on this matter?","['derivatives', 'lie-derivative', 'tensors', 'manifolds', 'trace']"
2503474,Equivalence between Brouwer fixed-point theorem and Borsuk-Ulam theorem. Is there a simple proof of equivalence between them?,"I wonder if Brouwer's fixed-point theorem and Borsuk-Ulam's theorem are equivalent. Brouwer's fixed-point theorem (simple form). Let $B_{\mathbb{R}^{n}}[0,1]=\{x\in \mathbb{R}^n: \|x-0\|\leq 1\}$ Then, any continuous $f:B_{\mathbb{R}^{n}}[0,1]\to B_{\mathbb{R}^{n}}[0,1] $ has a fixed point, i.e. there is a $x\in B_{\mathbb{R}^{n}}[0,1]$ such that $f(x)=x$.
  \ \ Borsuk-Ulam's theorem. Let a continuous function $f:\mathbb{S}^n\to \mathbb{R}^n$ (where $\mathbb{S}^n=\partial B_{\mathbb{R}^{n+1}}[0,1]$ is the $n$-sphere). Then there is a point $x\in \mathbb{S}^n$ for which $f(x)=f(−x)$. In other words, I would like to know if there are any proofs of the implications below: Brouwer's fixed-point theorem implies Borsuk-Ulam's theorem . Borsuk-Ulam's theorem implies Brouwer's fixed-point theorem . I would be happy if there were a simple (or as self-contained as possible) proof that could be reproduced here. But as the proofs of these theorems (at least in the bibliographic research I have done) are intricate depending on various definitions and lemas I think such simple proofs do not exist. In that case I would be satisfied with a set of references (books or articles) that had proof of confirmation or proof of the negative of both of the implications above. My bibliographic research is summarized as follows: Book: Using the Borsuk-Ulam Theorem: Lectures on Topological Methods in Combinatorics and Geometry (Universitext) MathOverflow question: Generalization of Borsuk-Ulam. Math.stackexchange question: Is there a simple proof of Borsuk-Ulam, given Brouwer? This question (my question is not a duplicate of this question) can provide answers related to the first implication above. But it does not touch upon the question of the equivalence of these two theorems. Thesis: Extensions of the Borsuk-Ulam Theorem. The Wikipedia entry on Borsuk-Ulam's theorem states that such theorems are equivalent ( see here ) but provide no direct proof. And if I understood correctly, it is not possible to conclude by any chain of implications on other theorems set forth therein. Also the references of Wikipedia do not bring any proof of this affirmation.","['reference-request', 'reference-works', 'algebraic-topology', 'fixed-point-theorems', 'analysis']"
2503485,Tautological Bundle yields Twisted Sheaf as Line Bundle,"Let $\mathbb{C} \mathbb{P}^1 $ the complex projective space and let consider the tautological bundle over $\mathbb{C} \mathbb{P}^1 $: In the excerpt below (whole document: here ) the map belonging to tautological bundle of $\mathbb{C} \mathbb{P}^1 $ is defined as the canonical map $s: \mathbb{C}^2 \backslash \{0\} \to \mathbb{C} \mathbb{P}^1 $ which assoziates every $(x_0:x_1)$ in canonical way the fiber $s^{-1}(x_0:x_1) = \mathbb{C}*(x_0,x_1)$, therefore the corresponding one dimensional vector space. My question is how does this constuction yields $\mathcal{O}(-1)$ as line bundle, therefore a twisted sheaf with empty global sections (because of $-1$)? Source:","['vector-bundles', 'sheaf-theory', 'algebraic-geometry']"
2503492,Probability - An assortment of 20 parts,"An  assortment of 20 parts is considered to be good if it has not more than 2
defective parts, it is considered bad if it contains at least 4 defective parts. Buyer and seller of the assortment agree to test 4 randomly picked out parts. Only when all 4 are good, the purchase will take place. In this procedure the seller bears the risk of not selling a good assortment, the buyer to buy a bad assortment. Who carries the greater risk? So I have two problems here: 1) If at least 1 part is defective, the purchase won't take place (The Seller has the risk of not selling a good assortment) 2) If all 4 are good, the purchase will take place (But the buyer has the risk of buying a bad assortment) My Idea for 1)  (at least 1 part is defective)
$$P(X\ge 1)=1-P(X=0)=1-\begin{pmatrix}
        1  \\
        0   
        \end{pmatrix}* \begin{pmatrix}
        19  \\
        4   
        \end{pmatrix}/ \begin{pmatrix}
        20  \\
        4   
        \end{pmatrix}=1-4/5=0,2$$ My idea for 2) (All 4 parts are good)
$$
       P(B)= \begin{pmatrix}
        4  \\
        0   
        \end{pmatrix}* \begin{pmatrix}
        16  \\
        4   
        \end{pmatrix}/ \begin{pmatrix}
        20  \\
        4   
        \end{pmatrix} =364/969=0,3756
$$
So the Buyer has a higher risk than the seller. However I think I made a mistake somewhere. Thanks in advance!","['stochastic-processes', 'probability-theory', 'probability', 'statistics']"
2503500,Does an ergodic random process imply stationarity or just wide-sense stationarity?,"Let $X(t)$ be a random process. It is well known that stationarity is more strict condition than wide-sense stationarity. In a stationary random process, the pdf of a random variable $X(t_i)$ is the same at any time instance $t_i$. On the other hand, in a wide-sense stationary process, every random variable $X(t_i)$ has the same mean and variance, while the pdf's may be different for different time instances $t_i$. In other words, we can say that the wide-sense stationary random process has the time-invariant ensemble average and variance. The autocorrelation function depends just on the time difference $\tau$, i.e. $R(\tau)$, no matter if we talk about the stationary of the wide-sense stationary process. In an ergodic random process, the time-invariant ensemble average must be equal (with probability one) to the time average of every single realization of the random process $X(t)$. Now assume that we have an ergodic random process $X(t)$. Is $X(t)$ stationary or wide-sense stationary then? I read somewhere that ergodicity implies stationarity, but I think that ergodicity implies just wide-sense stationarity. The reason follows from the definition of ergodic random process. That is, if we have an ergodic random process, we know that it has the time-invariant ensemble average which is the criteria set for the wide-sense stationary process. I would like someone to correct me if I am wrong, or to confirm it.","['stochastic-processes', 'stationary-processes', 'probability', 'random-variables']"
2503546,What is a compact 2-D submanifold?,"Show that the qeuations $x^3 + y^3 + z^3 + w^3 = 1$ $x^2 +y^2 + z^2 +w^2 =4$ define a compact 2-dimensional submanifold of $\mathbb{R}^4$. Write the equations for its tangent space at a point $(x_0,y_0,z_0,w_0)$. I'm not sure what a 2-dimensional submanifold is. The second equation looks like it would be a parabaloid of some sort which would be 3 dimensional right? I'm a little confused as to what this question is asking for. Can anybody help clarify?","['multivariable-calculus', 'compact-manifolds', 'real-analysis', 'manifolds']"
2503572,Combinatorics in ML: Counting Points with co-ordinates from among a set.,"I'm trying to re-prove a theorem in the book Understanding Machine Learning: From Theory to Algorithms by Shalev-Schwartz et. al to aid my understanding of the material. The proof in the book derives a crude bound, and I'd like to derive a tighter one by counting the relevant quantities. General Version of Problem: Given a finite domain $X$ of size $M$, I'd like to consider points in the $n$-fold product of X; these points have n-coordinates, each an element of X. There are a total of $M^{n}$ such points. Now, I'd like to partition these points into $n$ sets. The first set, $S_{1}$, will contain all points that contain exactly 1 unique element of $X$ among its co-ordinates. The second set, $S_{2}$, will contain all points that contain exactly two unique elements, and so on. I would like to determine $|S_{i}|$ for $1\leq i \leq n$. $|S_{1}| = M$, since, there are exactly $M$ points with one unique member of $X$ among their co-ordinates. Similarly, $|S_{n}| = \frac{M!}{(M-n)!}$. Using the inclusion-exclusion formula, I've been able to derive a long and messy formula for $|S_{i}|$. I'm wondering if anyone knows a simple way to do this, or a standard reference where this problem is solved. Here's an example: Let $M = 6$, and the members of $X$ are just labelled with integers. Also, suppose $n=3$. The point (1,1,1)) would belong to $S_{1}$. The point (6,2,2) would belong to $S_{2}$. The point (1,5,3) would belong to $S_{3}$. This case can be solved explicitly, since $|S_{1}|$ = 6, $|S_{3}|$ = 120, and 
$|S_{1}|+|S_{2}|+|S_{3}|$ = 216 I'm not very good at counting, and hope that there is a simple way to do this. Thank you!","['combinatorics', 'statistics']"
2503595,Maximal solution of a differential equation belongs to manifold $M$.,"I'm trying to prove the following: Let $U\subseteq\mathbb R^n$ be an open set and $g\colon U\to\mathbb R^n$ be a (continuous) vector field.
  Suppose that for every $p\in U$ the equation
  $$x' = g(x), \quad x\in U,$$
  admits a unique maximal solution $x(\cdot)$ satisfying $x(0)=p$.
  Now, let $M\subseteq U$ be a (at least) $C^1$ manifold closed in $U$.
  If $g$ is tangent to $M$, that is, if $g(x) \in T_xM$ for every $x\in M$, then every maximal solution of $x' = g(x)$ that intercepts $M$ ""lives"" entirely in $M$. I have no clue on how to attack this problem. The first thing I've thought is to use that $M$ is locally a level set of a $C^1$ function $f\colon U\to\mathbb R^m$, say $f^{-1}(0)$, but working ""locally"" is not compatible with ""maximal solution"", so I'm stuck.
I really don't know how to proceed. Any help would be appreciated.
Thanks in advance.","['smooth-manifolds', 'ordinary-differential-equations', 'analysis']"
2503613,$\mathbb{R}$ as a $\mathbb{Q}$ vector space has a lebesgue measurable basis,After studying some measure theory i found this sentence without proof. $$\mathbb R \quad \text{as a} \quad \mathbb Q \quad \text{vector space has a lebesgue measurable basis}$$ A hint is given : Analyse the family of all $\mathbb Q$-linear independent subsets of the Cantor set for maximal elements. I have zero clues on how to progress with this so if anyone is interested in checking this I would gladly appreciate it. Thanks!,"['lebesgue-measure', 'measure-theory']"
2503622,Finding bounded linear functionals on $L^{\infty}(\mathbb{R})$,"I'm trying to prove the next two propositions: a) There is a nonzero bounded linear functional on $L^{\infty}(\mathbb{R})$ which vanishes on $C(\mathbb{R}).$ b) There is a bounded linear functional $\lambda$ on $L^{\infty}(\mathbb{R})$ such that $\lambda(f)=f(0)$ for each $f\in C(\mathbb{R}),$ where $C(\mathbb{R})$ is the space of all bounded real valued continuous functions on $\mathbb{R}$ equipped with the sup norm . I'm stuck proving this. I guess Hahn-Banach could solve this: Part b) with $g:C(\mathbb{R})\rightarrow\mathbb{C}$ defined by $g(f)=f(0).$ Such map is linear and I think is bounded. If this were the case, a Corollary of Hahn-Banach solves this. With part a) I don't get any useful. How to prove this?","['functional-analysis', 'lp-spaces', 'measure-theory']"
2503627,Show that the projection function is continuous for product spaces,"Let X and Y be topological spaces, and let $X\times Y$ be the corresponding product space. Define the projection functions $$p_X:X\times Y \rightarrow X \mbox{ and } p_Y:X\times Y \rightarrow Y$$ by $p_X(x,y)=x$ and $p_Y(x,y)=y$. Prove that $p_X$ and $p_Y$ are continuous. So it's clear to me that if $x$ is open in $X$, then $p_X^{-1}(x)=(x,y)$ but I don't understand how we know that $(x,y)$ is open just because $x$ is open. Isn't the basis for a product space $B:=\{U×V∣U⊆X \mbox{ open },V⊆Y \mbox{ open }\}?$ How do we say $(x,y)$ is open in $X\times Y$ without knowing whether $y$ is open in $Y$?","['continuity', 'general-topology', 'projection']"
2503663,Representation of a group of prime power order,"Let $G$ be a group, $|G| = p^t$,  $p$ prime, $t \geq 2$. It is easily seen that $\frac{G}{[G,G]}$ has order at least $p^2$. Now, I want to prove that every irreducible representation of a group of order $p^4$, has dimension $1$ or $p$. Clearly, such a representation must have $\dim \rho < p^2$, since $\sum_{\rho \in \operatorname{Irr}(G)} \dim(\rho)^2 = |G|$, and there is at least the trivial representation of order $1$, so my claim follows. Now, with the additional fact that $\dim \rho | \ |G|$, I am done. However, I am asked not to use this . I do know the existence of representations of order $1$ and $p$, however ruling out any other dimension is not clear to me. I know basic group theory : say, up to Sylow's theorem related arguments, and representation theory up to the basics, which is Wigner's little groups method. I would not mind seeing proofs which are outside this prescription, however I would prefer proofs which are more elementary.","['finite-groups', 'representation-theory', 'group-theory']"
2503844,Divisor class group of noetherian local domain and its completion or henselization,"Assume $R$ is a noetherian local domain, we know that if $\hat R$ is UFD then $R$ is also a UFD, see this question . However, the converse is not true as the completion may not even be a domain (so we cannot define it's divisor class group in general), see tag/0AL7 . Inspired by above observation and the fact a normal domain is a UFD if and only if it's divisor class group vanishes, I am interested in the relationship of divisor class group between $R$ and $\hat R$. Here is my question: If $\hat R$ is a domain then we can define $Cl(\hat R)$. In this case, what's the relationship of $Cl(R)$ and $Cl(\hat R)$ ? Henselization behaves better than completion as $R^h$ is a normal domain if and only if $R$ is, see tag/07QL . So what's the relationship of $Cl(R)$ and $Cl(R^h)$ ? Is that $R$ is a UFD equivalent to that $R^h$ is a UFD?","['algebraic-geometry', 'commutative-algebra']"
2503883,What is the relationship between open and connected sets and statistical independence?,"$\newcommand{\vol}{\operatorname{vol}}$Suppose that $U \sim \operatorname{Unif}(C)$, $0 < \vol(C) < \infty$ with $U = (X, Y)$ on $\mathbb{R}^p = \mathbb{R}^r \times \mathbb{R}^s $. Verify that if $C$ is open and connected then $X\mathrel{\perp\!\!\!\perp}Y \iff C = A \times B$ I've come up with a proof but as far as I can tell it doesn't matter if the sets are open and connected which makes me suspicious. Where is that necessary? $\Rightarrow$ $$X\mathrel{\perp\!\!\!\perp}Y \Rightarrow P(X \in E_1, Y \in E_2) = P(X \in E_1)P(Y \in E_2)$$ Since $U = (X, Y)$ $$P(X \in E_1, Y \in E_2) = P(U \in E_1 \times E_2)=P(X \in E_1)P(Y \in E_2)$$ where for clarity I assumed $E_1 \times E_2 \subset C$, but this could be relaxed. Then, because $U$ has a uniform distribution on $C$, $$P(U \in E_1 \times E_2)=\frac{\vol(E_1 \times E_2)}{\vol(C)} = \frac{\vol(E_1)}{\vol(A)}\frac{\vol(E_2)}{\vol(B)}$$ where $A$ and $B$ are the sets where $X$ and $Y$ are defined, respectively. Now turning this into densities we have that $$\frac{I(E_1 \times E_2)}{\vol(C)} = \frac{I(E_1)}{\vol(A)}\frac{I(E_2)}{\vol(B)}$$
$$\frac{I(E_1)I(E_2)}{\vol(C)} = \frac{I(E_1)}{\vol(A)}\frac{I(E_2)}{\vol(B)}$$ where $I$ is the indicator function. Since the numerators are the same this means that $\vol(C) = \vol(A)\vol(B)$. Applying Fubini's theorem, $$\int_Cdx = \int_Adx \int_Bdy = \int_{A \times B}d(x, y)$$ and so the set $C$ is $A \times B$. $\Leftarrow$ Since $C = A \times B$, $$\frac{I(E_1 \times E_2)}{\vol(C)} = \frac{I(E_1)I(E_2)}{\vol(A)\vol(B)} $$ hence $X$ and $Y$ are independent.","['real-analysis', 'probability-theory', 'statistics', 'probability', 'measure-theory']"
2503886,Why does the inverse of the Hilbert matrix have integer entries?,Let $A$ be the $n\times n$ matrix given by $$A_{ij}=\frac{1}{i + j - 1}$$ Show that $A$ is invertible and that the inverse has integer entries. I was able to show that $A$ is invertible. How do I show that $A^{-1}$ has integer entries? This matrix is called the Hilbert matrix. The problem appears as exercise 12 in section 1.6 of Hoffman and Kunze's Linear Algebra (2nd edition).,"['matrices', 'hilbert-matrices', 'linear-algebra', 'inverse']"
2503911,Theorems similar to Dini's Theorem and Egoroff's Theorem,"Dini's Theorem states that Given a sequence of real-valued continuous functions $(f_n)$ on a compact set $E\subseteq \mathbb{R},$ if $(f_n)$ decreases to a continuous function $f$ pointwise on $E$ , then $(f_n)$ converges to $f$ uniformly on $E$ . Egoroff's Theorem states that Given a measure space $(E,\mathcal{A},\mu)$ where $E \subseteq \mathbb{R}$ has finite measure. $\mathcal{A}$ is an $\sigma$ -algebra and $\mu$ is a measure on $E.$ If a sequence of measurable functions $(f_n)$ converges pointwise to $f$ almost everywhere, then for every $\varepsilon>0,$ there exists a measurable subset $F\subseteq E$ with $\mu(E\setminus F) <\varepsilon$ such that $(f_n)$ converges uniformly to $f$ on $F.$ Question: Do there exist theorems, other than Dini and Egoroff, which give sufficient conditions for pointwise convergence to be uniform convergence? I would like to see techniques involved in proving those theorems.
Dini and Egoroff used similar technique, which is to define a set containing elements such that $f_n$ and $f$ are 'closed to each other'. If possible, I would like to know other technique to show pointwise convergence implies uniform convergence.","['real-analysis', 'uniform-convergence', 'reference-request', 'pointwise-convergence', 'measure-theory']"
2503975,Ramanujan's series $1+\sum_{n=1}^{\infty}(8n+1)\left(\frac{1\cdot 5\cdots (4n-3)}{4\cdot 8\cdots (4n)}\right)^{4}$,"Ramanujan gave the following series evaluation $$1+9\left(\frac{1}{4}\right)^{4}+17\left(\frac{1\cdot 5}{4\cdot 8}\right)^{4}+25\left(\frac{1\cdot 5\cdot 9}{4\cdot 8\cdot 12}\right)^{4}+\cdots=\dfrac{2\sqrt{2}}{\sqrt{\pi}\Gamma^{2}\left(\dfrac{3}{4}\right)}$$ in his first and famous letter to G H Hardy. The form of the series is similar to his famous series for $1/\pi$ and hence a similar approach might work to establish the above evaluation. Thus if $$f(x) =1+\sum_{n=1}^{\infty}\left(\frac{1\cdot 5\cdots (4n-3)}{4\cdot 8\cdots (4n)}\right)^{4}x^{n}$$ then Ramanujan's series is equal to $f(1)+8f'(1)$. Unfortunately the series for $f(x) $ does not appear to be directly related to elliptic integrals or amenable to Clausen's formula used in the proofs for his series for $1/\pi$. Is there any way to proceed with my approach? Any other approaches based on hypergeometric functions and their transformation are also welcome. Update : This question is now solved, thanks to the people at mathoverflow .",['sequences-and-series']
2504002,"Limit of $\frac{\sin(x)}{x}$, as $x \rightarrow \infty$","Recently, I showed my Calculus students how to show that $$  \lim_{x \to \infty} \frac{\sin(x)}{x} = 0, $$ by using the squeeze theorem. An interesting question that I was asked several times was, ""how come we couldn't just conclude that it is zero, without using the squeeze theorem, since it is obvious that the limit is zero."" I told them that the function is still oscillating, even though the oscillations are tiny, and so we need to make it rigorous by applying the squeeze theorem.  They weren't convinced ... What else could I say to them?  Is there a nice counter-example / explanation? (No epsilon-delta arguments, please ... ) Thanks,","['calculus', 'limits']"
2504019,"For function $f:[0,1]\rightarrow \Bbb R$ we know : $f(x+y)\geq f(x)+f(y)$.Prove that $f(x)\leq 2x$","Function $f:[0,1]\rightarrow \Bbb R$ holds in the following conditions : 1) $f(1)=1$ 2) $\forall x \in [0,1]: f(x)\geq 0$ 3) $\forall x,y,x+y \in [0,1]: f(x+y)\geq f(x)+f(y)$. Prove that $\forall x \in [0,1] : f(x)\leq 2x$ I tested values $0,\frac12$ in the given formula and got the following results: $$f(0)=0 , f(\frac12)\leq \frac12,f(x)\leq \frac{f(2x)}{2}$$ Please help complete the proof.",['real-analysis']
2504072,Equivalence of an alternative definition of the derivative.,My calculus professor defined the derivative of a real function $f$ as follows : “Let $f\colon \mathbb{R} \longrightarrow \mathbb R$ be a function. Then $f$ is said to be differentiable at a point $a \in\mathbb R$ if $\exists$ a function $\psi$ such that : 1) $(\forall h \in\mathbb{R}): f(a+h) = f(a) + mh + hΨ(h)$ 2) $\psi$ is continuous at $0$ 3) $\psi(0) = 0$ Then we denote the derivative of $f(x)$ as $f'(x) = m$” My problem in this definition is as follows: I can see that by rearranging the terms $f(a)$ and then dividing by h we get the regular definition of the limit. I do not see what is the significance of $\psi$.,"['derivatives', 'calculus', 'definition']"
2504081,"Is it okay to ""ignore"" small numbers in limits where $x$ approaches infinity?","I got a limit: $$\lim_{x\to\infty}\frac {(2x+3)^3(3x-2)^2} {(x^5 + 5)}$$ As far as $x$ approaches infinity, can I just forget about 'small' numbers (like $3$, $-2$ and $5$ in this example)? I mean is it legal to make a transition to: $$\lim_{x\to\infty}\frac {(2x)^3(3x)^2} {x^5}$$ Or if it is not always okay — in what cases such transitions are okay?","['calculus', 'limits']"
2504087,Why is repeated vertices allowed in Euler path?,"As wikipedia defines path as: A path is a trail in which all vertices (except possibly the first and
  last) are distinct. if we cannot repeat a vertex in a path then how can we repeat it in Euler path ? ( which is a path in which every edge is visited exactly once ) For example: Example Graph In the above graph A-B-C-D-E-F-C-A-F is an Euler path but as you can see A, F and C vertices are visited twice which is not allowed in a path then how can you say its a path ? Thanks for your precious time.","['graph-theory', 'discrete-mathematics']"
2504157,Open sets in the spectrum of a Dedekind domain are affine,"I'm trying to prove the following statement: All open sets in the spectrum $X=\operatorname{Spec}(A)$ of a Dedekind domain $A$ are affine. Since all open sets are cofinite, my idea is to prove that an open set of the form $X-\mathfrak{p}$ is affine, and then use induction. I've found this question , which explains the condition for all open sets to be principal. In particular, if $\mathfrak{p}$ is of finite order in the ideal class group, then $\mathfrak{p}^n=(f)$, $X-\mathfrak{p}=D_f$ is principal; if however $\mathfrak{p}$ is of infinite order, then $X-\mathfrak{p}$ is not principal. In this latter case, what is the coordinate ring? And how to prove that it is affine? Thanks.","['dedekind-domain', 'algebraic-geometry', 'commutative-algebra']"
2504176,Existence of a choice function for finite sets in Jech,"Jech notes in Chapter 5 of Set Theory, 2003 that, given a family of nonempty sets $S$, in some cases the existence of choice function can be proved in ZF: (iii) when every X ∈ S is a finite set of real numbers; let f (X) = the least element of X. However, in the very next sentence it is stated: ... one cannot prove existence of a choice function (in ZF) just from the assumption that the sets in $S$ are finite; even when every $X \in S$
  has just two elements (e.g., sets of reals) , we cannot necessarily prove that $S$ has a choice function. (emphasis mine) How is this not a contradiction with (iii)?","['elementary-set-theory', 'axiom-of-choice']"
2504180,"What has flipping a coin to do with $\int_0^{2 \pi}\sin^{2n}(x)\,dx$?","If you toss a coin $2n$ times the chance of getting heads $n$ times is
$$\frac{1}{4^n} \binom{2n}{n} $$ (simple combinatorics problem). Now I've made a seemingly obscure observation:
$$\int_0^{2 \pi}\sin^{2n}(x)\,dx = \frac{2 \pi}{4^n} \binom{2n}{n} $$ The identities only differ by a factor of $2 \pi$. Is there an explanation for this phenomenon or just a cool coincidence? UPDATE: As Aloizio Macedo pointed out, a fairly related question has been answered with a combinatorial proof by Qiaochu Yuan here .","['combinatorics', 'real-analysis', 'integration', 'analysis']"
2504252,Existence of solution of $f'(x) = f(\phi(x))$,"Let $\phi:[0,1]\to[0,1]$ be a none constant and continuous map. Prove that there exists a unique function $f\in C^1([0,1], \Bbb R)$ satisfying $$f'(x) = f(\phi(x))~~~f(0)=\alpha$$
where $\alpha\in\Bbb R$ is given. I attempted to use the Cauchy-Lipschitz Theorem but I couldn't go further. Any Hint?","['functional-analysis', 'ordinary-differential-equations', 'analysis']"
2504257,Equivalent Definitions of Twisted Sheaf $ \mathcal {O}(1)$,"Let $\mathcal {O}(-1)$ be the tautological line bundle $X$ of $ \Bbb CP^1$, where $X=\{(z,l) \in \Bbb C^2 \times \Bbb CP^1 : z \in l \}$ together with canonical projection $X \to \Bbb CP^1$ (line bundle property and co easy to prove). Futhermore we define $ \mathcal {O}(1):= \mathcal {O}(-1)^{\vee}$,  where $ \mathcal {O}(-1)^{\vee}$  can be defined in two equivalent ways : $ \mathcal {O}(-1) \otimes \mathcal {O}(-1)^{\vee}= \mathcal {O}_{\Bbb CP^1}$ is the same as to define it as $ \mathcal {O}(-1)^V :=  \underline{Hom}_{\mathcal{O}_{\Bbb CP^1}}(\mathcal{O}(-1),\mathcal{O}_{\Bbb CP^1})$  (follows from evaluation map). The other way to define $ \mathcal {O}(1)$ is the following (compare with eg Liu’s AG, page 165 or see image below): Obviously we have $ \Bbb CP^1 = Proj (B)$ where $B = \oplus _n  B_n:=\mathbb{C}[X,Y] $ is graded $\mathbb{C}$-algebra in canonical way (polynomial grade). We set $B(n)$ as a new graduated $\mathbb{C}$-algebra by defining recursively $B(n)_m := B_{n+m}$.
Liu defined the $ \mathcal {O}_{\Bbb CP^1 }$ -module $ \mathcal {O}(n)$ by setting $ \mathcal {O}(n) := \widetilde{B(n)}$ . My question is: Why this both definitions of $ \mathcal {O}(1)$ are equivalent? Here Liu's definition of ""twisting"":","['sheaf-theory', 'algebraic-geometry']"
2504304,"A path $\gamma:[a,b]\to \mathbb C$ which is not rectifiable.","In Conway's Functions of one complex variables (part 1) book , the definition of  a path is written as :  A continuous function $\gamma:[a,b]\to \mathbb C$ where $a,b \in \mathbb R, a<b$ is called a path and it  is rectifiable if  $\gamma $ is of bounded variation , i.e., $\gamma $ has finite length. Now my question is : As $\gamma$ is continuous , $\gamma[a,b]$  is compact , hence closed and bounded so any path must be of finite length. But why there is another definition of rectifiable path.","['complex-analysis', 'curves']"
2504309,"Basis of $C[a,b]$","The space $C[a,b]$ , space of all real valued continuous functions on $[a,b]$ is an infinite dimensional vector space over the field $\Bbb R$. As every vector space over a field has a basis so definitely $C[a,b]$ has a basis. I want to know a basis of $C[a,b]$. How I can find a basis of it ?","['real-analysis', 'functional-analysis', 'schauder-basis', 'linear-algebra', 'vector-spaces']"
2504354,Ways of arranging $n$ boys (and $m-n$ girls) in $m$ seats such that at least one boy is alone,"Given $m$ seats, as well as $n$ (non-distinct) boys and $m-n$ (non-distinct) girls, find the number of arrangements such that at least one boy sits alone (no adjacent boys). This is somewhat similar to the problem where everyone in a particular group as to be separated when seated, which was what I thought of (giving the answer $\binom{m-n+1}{n}$ in this case), but since we're looking for arrangements with at least one lone boy, I don't know where to start.",['combinatorics']
2504408,Solving continuous algebraic Riccati equation using generalized eigenproblem algorithm,"I want to solve the continuous algebraic riccati equation: $$A^TX + XA - XBR^{-1}B^TX + Q = 0$$ To solve this, I have been using Schur's method to solve algebratic riccati equations. Schur's method to solve algebratic riccati equations The problem is the solution $X$ is not the same solution when I use MATLAB's command X = CARE(A, B, Q, R). MATLAB is using Generalized Eigenproblem Algorithms and Software for Algebraic Riccati Equations. Generalized Eigenproblem Algorithms and Software for Algebraic Riccati Equations,'' Proc. IEEE, 72(1984), 1746--1754. So my question is how I can solve the algebraic riccati equation using the generalized eigenproblem algorithm? Assume that we have those matrices. A =

    0     1
   -2    -3

>> B

B =

     0
     1

>> Q

Q =

     1     0
     0     2

>> R

R =

     1 Using Schur's method. We will found the solution $X$ by using this: >> H = [A -B*inv(R)*B'; -Q -A']; % Hamiltonian matrix
>> [U, S] = schur(H); % Schur decomposition
>> [m,n] = size(U);
>> U11 = U(1:(m/2), 1:(n/2));
>> U21 = U((m/2+1):m, 1:(n/2));
>> X = U21*inv(U11) % The solution

X =

   -6.0000   -2.8074
   -8.1926   -3.0000

>> A'*X + X*A - X*B*inv(R)*B'*X + Q % Check if this equation is near 0

ans =

   1.0e-13 *

    0.0711    0.0888
    0.1776    0.1155 Yes it is! Now we use MATLAB's command CARE which use the generalized eigenproblem algorithm. >> X = care(A, B, Q, R)

X =

    1.5737    0.2361
    0.2361    0.3871

>> A'*X + X*A - X*B*inv(R)*B'*X + Q  % Check if this equation is near 0

ans =

   1.0e-14 *

   -0.1332   -0.0208
   -0.0208    0.0888 Let's try Octave/MATLAB's command fsolve. >> fun = @(P, A, B, Q, R) P*A+A'*P-P*B*inv(R)*B'*P + Q;
>> p0 = 1*ones(2);
>> X = fsolve(@(P) fun(P, A, B, Q, R), p0)
X =

   1.57368   0.23607
   0.23607   0.38705

>> A'*X + X*A - X*B*inv(R)*B'*X + Q
ans =

  -1.7419e-08  -1.1153e-08
  -1.1153e-08  -6.9926e-09 So both give good solutions. The only difference was that Schur's method give the solution $X$ as matrix only, but with the generalized eigenproblem algorithm, we got a positive semi-definite solution $X$. The fsolve command works very good and give a equal solution as MATLAB's command CARE do. Let's change matrix A. >> A = [0 1; -3 -5]; % The other matrices are the same
>> X = care(A, B, Q,R)

X =

    0.9118    0.0215
    0.0215    0.1994 Let's use fsolve with the new matrix A. X = fsolve(@(P) fun(P, A, B, Q, R), p0)
X =

   1.53014   0.16228
   0.16228   0.22729

>> A'*X + X*A - X*B*inv(R)*B'*X + Q
ans =

  -9.9554e-08  -5.1336e-08
  -5.1336e-08  -2.1732e-08 So here we got a change of the solution X, but the solution X has still positive values.","['matrices', 'control-theory', 'optimal-control', 'linear-control']"
2504468,"How to encode a set of whole numbers $\{a_1,a_2,...,a_n\}$ such that given a number $x$ we can test if $x \in \{a_1,a_2,...,a_n\}$","Suppose we have a set of whole numbers $\{a_1,a_2,...,a_n\}$. Is there a way to encode them into a new number $e$ such that we can use $e$ to test if a given number $x \in \{a_1,a_2,...,a_n\}$? So really I'm looking for two things: Encoding function
$$f(a_1,a_2,...,a_n)=e$$ Test function
$$p_e(x)=true \iff x \in \{a_1,a_2,...,a_n\}$$ I'm using this in a computer program so the numbers can't get too big, and it would also be very beneficial to be something that can be implemented to run quickly. Thank you all, cheers.","['number-theory', 'computer-science', 'discrete-mathematics']"
2504491,$ \lim_{n \to \infty} \left(\frac 1{n^2+1}+\frac 2{n^2+2}+\frac 3{n^2+3}+\cdots +\frac n{n^2+n}\right)$,"Evaluate: $$ L=\lim_{n \to \infty} \left(\frac 1{n^2+1}+\frac 2{n^2+2}+\frac 3{n^2+3}+\cdots +\frac n{n^2+n}\right)$$ My approach: Each term can be written as $$ \frac k{n^2+k}=\frac {n^2+k-n^2}{n^2+k}=1-\frac {n^2}{n^2+k}$$ $$ \therefore \lim_{n \to \infty}\frac k{n^2+k}=\lim_{n \to \infty}\left(1-\frac {n^2}{n^2+k}\right)=0$$ hence, $$ L=0$$ Problem: The correct answer is 1/2, please indicate the flaw in my approach or post a new solution. Thank You","['algebra-precalculus', 'proof-writing', 'calculus', 'proof-verification']"
2504526,Is the following theory consistent?,"Suppose, we have a  first-order logic theory over a signature {=, $\times$} (where $\times$ is a binary function symbol, and = is the equality symbol), that contains following axioms:
$$\forall x \forall y ( x \times (y \times y) = (x \times y) \times y)$$
$$\exists o \forall x (x \times o = o)$$
$$\exists e \forall x (x \times e = x)$$
$$\forall x \forall y \exists z (x \times z = y)$$
$$\exists x \exists y (\neg(x = y))$$
Is this theory consistent? It is quite easy to prove that if it is, then every its model has to be infinite. Also, it is quite obvious that the following statements are logically implied by this theory:
$$\neg \forall x \forall y ((y \times y) \times x = y \times(y \times x))$$
$$\neg \exists a \forall x (a \times x = a)$$
$$\neg \forall x \forall y \exists z (z \times x = y)$$
$$\neg \forall x \forall y (x \times y = y \times x)$$ However, I do not know how to proceed further… Any help will be appreciated.","['first-order-logic', 'model-theory', 'abstract-algebra', 'predicate-logic', 'logic']"
2504558,Infinitesimals in an old Euler text (1778),"I'm working through the text De curvis triangularibus of Leonard Euler. In this paper he defines orbiforms as some kind of curves with constant height. He uses these curves to find triangular curves (figure 1) as the evolute of these orbiforms. In the part where he starts working with these evolutes he uses infinitesimals in a formula which I have never seen before, can someone deduce his reasoning? Problem setting Consider the orbiform in figure 6 (drawn as a circle, but it doesn't need to be a circle) In the problem setting both $Ff$ and $Mm$ are normal to the curve (in each endpoint). Through a parameterization of the orbiform he deduced
$$
FP = x = \frac{dS}{dp} \qquad   \qquad PM = y = \frac{pdS}{dp} - S
$$
for a certain function $S$. Through further calculations he ends up with:
$$
\sin \phi = \frac{1}{\sqrt{1+p^2}} \quad \& \quad \cos \phi = \frac{p}{\sqrt{1+p^2}} \qquad\Rightarrow \qquad d\phi =\frac{-dp}{1+p^2}
$$
He then writes: Quod si iam brevitatis gratia ponamus $FN=v$, notum est, centrum circuli, curvam in $M$ osculantis, fore in puncto $U$, ita ut sit
  $$
			NU = \frac{d v\sin \phi}{d \phi};
$$ roughly translated as Let now for ease of use $FN=v$, it is known that the center of the osculationcircle in $M$ lies in the point $U$, then it is 
  $$
			NU = \frac{d v\sin \phi}{d \phi};
$$ How did he deduce this last formula? In 1778 calculus was done using infinitesimals, and I guess $d v$ should be interpreted as a tiny increase of $v$, but how should I look at $d\phi$ and how did he derive this formula? Figures EDIT 16 november I've been looking trough other Euler and L'Hopital texts on curves, evolutes and involutes, but can't find any references. I did come up with an possible explanation though. Would it be reasonable that this was Euler's reasoning as well? Consider the picture below, of a zoomed in version of fig 6. Let $NU$ progress by $d\phi$, then since two normals which are but an infinitesimal apart have the same radius of curvature, $U$ is still the same point of the osculating circle. Now apply the law of sines to $\triangle NUU'$.  Also note how $|NU'| = d v$. 
$$
\frac{NU'}{\sin d \phi} = \frac{NU}{\sin (\phi - d \phi)}
$$ If we replace $\sin d\phi \stackrel{?}{\approx} d\phi$ and $\sin(\phi-d \phi) \stackrel{?}{\approx}  \sin \phi$ it follows
$$
\frac{dv}{d \phi} = \frac{NU}{\sin \phi} \qquad \Rightarrow \qquad NU = \frac{d v\sin \phi}{d \phi}
$$ Far fetched?","['infinitesimals', 'math-history', 'differential-geometry']"
2504598,Solve differential equation: $ y' +1 = e^{-y}$,"I'm trying to solve it like this, but I have some troubles. $$ y' = e^{-y} -1 $$
$$ \frac{dy}{dx} = e^{-y} -1 $$
$$ \int{\frac{1}{e^{-y}-1}}dy = \int dx$$
but it doesn't look like the easiest way, so I tried different method:
$$ y' - e^{-y} =-1 $$
$$ y' - e^{-y} =0 $$
$$ \int{\frac{1}{e^{-y}}}dy = \int dx$$
$$ \int e^ydy = \int dx$$
$$  e^y = x+c$$
$$ y = ln(x+c)$$
$$ y = ln(x+c(x))$$
$$ y' = \frac{1}{(x+c(x))}$$
Puting it into first equation:
$$ \frac{1}{(x+c(x))} +1 = e^{-ln(x+c(x))}$$ Both ways seem not to be very goof for this equation. What can I do? (different method?)",['ordinary-differential-equations']
2504632,coefficient of $x^{17}$ in $(x^2+x^3+x^4+x^5+x^6+x^7)^3$,"coefficient of $x^{17}$ in $(x^2+x^3+x^4+x^5+x^6+x^7)^3$ - I'm reviewing for an exam and don't understand the answer, C(11+3−1,11)−C(3,1)×C(5+3−1,5).",['combinatorics']
2504652,"Geometry, Triangles","In the figure, $BC$ is parallel to $DE$. If area of ∆ $PDE$ is $3/7$ of area of ∆ $ADE$, then what is the ratio of $BC$ and $DE$? I tried finding ratios of height of ∆ $ABC$, $PDE$ & $BPC$, and trying to figure out some commonality, but it didn't​ work out. P.s. it is not my homework. Ratio is 5:2. Not sure how.","['trigonometry', 'triangles', 'geometry']"
2504715,I need help with an elementary number theory proof,"Let $a,b,c\in\mathbb{Z}$. Consider the following theorem: $a\mid c  \land  b\mid c\land \gcd(a,b)=1 \Rightarrow ab\mid c$. I'm currently attempting to understand a proof in which there is a step that I am unable to understand and the authers claim that the aforementioned  theorem justifies that step. Let $n_1,...n_k, N\in\mathbb{Z}:\gcd(n_i,n_j)=1, i\neq j \land n_i\mid N, i=1,...,k$. They claim that iteratively applying the theorem above leads to the conclusion that $n_1,...n_k\mid N$. It seems quite intuitive to me that this result ought to be true but I am unable to prove it. Could anyone help me? I get that for example if $k=3$ then $n_1n_2\mid N \land n_3\mid N$ is true but I fail to see how this implies $n_1n_2n_3\mid N$ since I don't know if $\gcd(n_1n_2,n_3)=1$ is true or false.","['discrete-mathematics', 'real-numbers', 'elementary-number-theory']"
2504724,"Let $E$ be a separable or reflexive infinite dimensional normed space, then there exists a sequence in $E'$ weak$^{*}$ convergent to $0$.","The following exercise I found in a book in which I found errors, I think that in the following exercise I can remove hypothesis. Let $E$ be a infinite dimensional normed space, suppose that $E$ is separable or reflexive.  Show that there exists a senquence  $(x_{n}')\subseteq E'$ such that $\left\|x_{n}'\right\|=1$, $n\in\mathbb{N}$, and $x_{n}'\overset{w^{*}}{\rightarrow} 0$  (that is, $x_{n}'(x)\rightarrow 0$ for all $x \in E$). The problem : I tink that the hypothesis  that $E$ is separable or reflexive can be removed. I want to know if my thinking is correct? My attepmt: Assuming we only have $E$ infinite dimensional normed vector space, we denote $S=\left\{x'\in E'\: :\: \left\|x'\right\|=1\right\}$, $\overline{B'}=\left\{x'\in E'\: :\: \left\|x'\right\|\leq 1\right\}$, and $\sigma(E',E)$ the weak$^{*}$ topology in $E'$. We know that 
$$\overline{S}^{\sigma(E',E)}=\overline{B'} \tag{$\bigstar$}$$
where $\overline{S}^{\sigma(E',E)}$ is the closure of $S$ respect to topology $\sigma(E',E)$. (for a proof of ($\bigstar$) see this post ). Therefore, $0\in \overline{S}^{\sigma(E',E)}$, then there exists $(x_{n}')\subseteq S$  such that $x_{n}'\rightarrow 0$ in the topology $\sigma(E',E)$, that is,    $\left\|x_{n}'\right\|=1$  and $x_{n}'\overset{w^{*}}{\rightarrow} 0$. Remark: Note that in my attempt I do not use the fact that $E$ is separable or reflexive. Am I making any mistakes? . Addendum The comment of @GiuseppeNegro is correct, but we know that   $\overline{B}'$ is $\sigma(E', E)$-compact. If $E$ is separable, then we know that  $\overline{B}'$ is metrizable with induced topology of $\sigma(E', E)$. Therefore, my attempt is the proof for the case $E$ separable. The problem is when $E$ is reflexive. I do not find a relation between reflexivity and metrizable . I'm thinking that when $E$ is reflexive it's not true that $\overline{B}'$ is $\sigma(E', E)$-metrisable, but I can not find a counter-example.","['functional-analysis', 'weak-convergence', 'weak-topology']"
2504737,Proving Wikipedia's formula on interior product of commutator of vector fields,"During one of my daily exercises, I was looking for properties of the elements of Cartan calculus. I stumbled on Wikipedia's page about interior products (here) , and I've noticed a property that sounds very useful:
$$
\iota_{[X,Y]}\omega=[\mathcal L_X,\iota_Y]\omega.
$$
In Wikipedia's notations, $\iota$ is the interior product, $\mathcal L_X$ is the Lie derivative with respect to the vector field $X$ (and $X$ and $Y$ are vector fields). $\omega$ is a differential form on a manifold $M$. As there is no source given, I'm trying to prove this equality, and failing due to a sign. Maybe I'm doing some stupid error somewhere. My attempt so far follows. First: I note that the operator on the right has the property
$$
[\mathcal L_X,\iota _Y](\omega\wedge\eta)=[\mathcal L_X,\iota_Y]\omega\wedge\eta+(-1)^k\omega\wedge[\mathcal L_X,\iota_Y]\eta,
$$
where $\omega$ is assumed to be a $k-$form, and $\eta$ is an arbitrary form. This is the same interaction with wedge product as the left hand side. It follows that I can decide the value of the right hand side locally, where I can expand any form as tensor product of the basis forms. Hence, if I prove that the equality holds for $0-$ and $1-$forms, I'm done. I start with $0-$forms: I take a function $f$ from $M$ to the field, and compute left and right hand side. Well, ""compute"": the left hand side is the application of an inner product to a function, that is zero by definition, while on the right hand side I either have a contraction first, annulling $f$, or I have a Lie derivative acting first. The Lie derivative of a function is a function, so it follows that $\iota_Y\mathcal L f=0$, and I'm done for this case. For $1-$forms: let $\alpha$ be such an $1-$form. The left hand side is
$$
\iota_{[X,Y]}\alpha=\alpha([X,Y]).
$$
I split the calculation of the right hand side in two, and use Cartan's formula $\mathcal L_X=d\iota_X+\iota_Xd$ whenever necessary. Lie derivatives are ugly, all hail Cartan.
$$
\mathcal L_X\iota_Y\alpha=\mathcal L_X(\alpha (Y))=X(\alpha(Y)),\\
\iota_Y\mathcal L_X\alpha=\iota_Yd\iota_X\alpha+\iota_Y\iota_Xd\alpha=Y(\alpha(X))+\underline{d\alpha(Y,X)}.
$$
Underlined for your convenience is the step in which it is most likely I've done an error, but I fail to see why. I state that $\iota_Y\iota_Xd\alpha=d\alpha(Y,X)$ as I am applying $X$ first, and $Y$ second, and $\iota$ places vectors at the beginning of a string. Is it correct? Was I stupid here? Continuing: I use
$$
d\alpha(Y,X)=Y(\alpha(X))-X(\alpha(Y))-\alpha([Y,X]).
$$
Here comes the failure. I'd expect stuff to cancel out, but that's not happening. The signs of $Y(\alpha(X))$ agree, so they do not cancel. Where am I doing wrong? I strongly suspect that it is something in the underlined passage, but I need clarification about what I did wrong. Thanks all in advance for your time. (p.s.: in some other places of this site there is a proof of that relying on a property of $\mathcal L_X$ acting on differential forms. As I said, I despise $\mathcal L_X$. I'd prefer to see the error in this proof, as it is short and nice. Lie has done many wonderful things, and an orrible derivative)","['differential-forms', 'lie-derivative', 'differential-geometry', 'exterior-algebra']"
2504760,Help solving variable separable ODE: $y' = \frac{1}{2} a y^2 + b y - 1$ with $y(0)=0$,"I am studying for an exam about ODEs and I am struggling with one of the past exam questions. The past exam shows one exercise which asks us to solve:
$$y' = \frac{1}{2} a y^2 + b y - 1$$with $y(0)=0$ The solution is given as
$$y(x) = \frac{2 \left( e^{\Gamma x} - 1 \right)}{(b + \Gamma)(e^{\Gamma x} - 1) + 2\Gamma},$$ with $\Gamma = \sqrt{b^2 + 2a}$ I am really getting stuck at this exercise and would love to have someone show me how this solution is derived. One thing I did find out is that this ODE is variable separable. That is,
$$y' = g(x)h(y) = (1) \cdot (\frac{1}{2} ay^2 + by - 1),$$ and therefore the solution would result from solving
$$\int \frac{1}{\frac{1}{2} ay^2 + by - 1} dy = \int dx + C,$$
where C is clearly zero because $y(0) = 0$. I am now getting stuck at solving the left integral. Could anyone please show me the steps? UPDATE So I came quite far with @LutzL solution, however my answers seems to slightly deviate from the solution given above. These are the steps I performed (continuing from @LutzL's answer): You complete the square $\frac12ay^2+by-1=\frac12a(y+\frac ba)^2-1-\frac{b^2}{2a}$ and use this to inspire the change of coordinates $u=ay+b$ leading to
$$
\int \frac{dy}{\frac12ay^2+by-1}=\int\frac{2\,du}{u^2-2a-b^2}
$$
and for that your integral tables should give a form using the inverse hyperbolic tangent. Or you perform a partial fraction decomposition for
$$
\frac{2Γ}{u^2-Γ^2}=-\frac{1}{u+Γ}+\frac{1}{u-Γ}
$$
and find the corresponding logarithmic anti-derivatives,
$$
\ln|u-Γ|-\ln|u+Γ|=Γx+c,\\ \frac{u-Γ}{u+Γ}=Ce^{Γx},\ C=\pm e^c
$$
which you now can easily solve for $u$ and then $y$. Given that $y(0) = 0$ we have $u(y(0)) = u(0) = b$ and therefore the final equation becomes
$$\frac{u(0)-Γ}{u(0)+Γ}= \frac{b-Γ}{b+Γ}=Ce^{Γ\cdot0} = Ce^{Γ \cdot 0} = C$$
Now by first isolating $u$ I get
$$u - \Gamma = u C e^{\Gamma x} + \Gamma C e^{\Gamma x} \Rightarrow \\
u \left( 1 - C e^{ \Gamma x} \right) = \Gamma \left( 1 + C e^{\Gamma x} \right) \Rightarrow \\
u = \frac{\Gamma \left( 1 + C e^{\Gamma x} \right)}{\left( 1 - C e^{ \Gamma x} \right)}$$
Now substituting u and C gives
$$ay + b= \frac{\Gamma \left( 1 + \frac{b-Γ}{b+Γ} e^{\Gamma x} \right)}{\left( 1 - \frac{b-Γ}{b+Γ} e^{ \Gamma x} \right)} \Rightarrow \\
y = \frac{\Gamma \left( 1 + \frac{b-Γ}{b+Γ} e^{\Gamma x} \right) - b \left( 1 - \frac{b-Γ}{b+Γ} e^{ \Gamma x} \right)}{a \left( 1 - \frac{b-Γ}{b+Γ} e^{ \Gamma x} \right)}$$ Now using the fact that $\Gamma = \sqrt{b^2 + 2a} \Rightarrow a = \frac{(\Gamma + b)(\Gamma - b)}{2}$ we get that
$$y = \frac{2 \left( \Gamma \left( 1 + \frac{b-\Gamma }{b+\Gamma } e^{\Gamma x} \right) - b \left( 1 - \frac{b-\Gamma }{b+\Gamma } e^{ \Gamma x} \right) \right)}{(\Gamma + b)(\Gamma - b) \left( 1 - \frac{b-\Gamma }{b+\Gamma } e^{ \Gamma x} \right)}  \\
= \frac{2 \left( (\Gamma - b) + (\Gamma + b) \frac{b-\Gamma }{b+ \Gamma } e^{\Gamma x} \right)}{(\Gamma + b)(\Gamma - b) \left( 1 - \frac{b-\Gamma }{b+\Gamma } e^{ \Gamma x} \right)} \\
= \frac{2 \left( (\Gamma - b) - (\Gamma + b) \frac{\Gamma - b }{b+ \Gamma } e^{\Gamma x} \right)}{(\Gamma + b)(\Gamma - b) \left( 1 - \frac{b-\Gamma }{b+\Gamma } e^{ \Gamma x} \right)}$$
Now cancelling the terms $(b + \Gamma)$ and $(b - \Gamma)$ wherever possible and multiplying denominator and nominator by -1 gives
$$y = \frac{2 \left(  e^{\Gamma x} - 1 \right)}{(\Gamma + b) \left( \frac{b-\Gamma }{b+\Gamma } e^{ \Gamma x} - 1 \right)} $$ So clearly, I got the nominator right, but I can not seem to get the denominator to equal $(b + \Gamma)(e^{\Gamma x} - 1) + 2\Gamma$. Can someone rescue me and show me what I did wrong? Maybe it helps if I say that x is always positive?","['integration', 'ordinary-differential-equations']"
2504847,Showing injectivity for an immersion $\mathbb R P^2 \to \mathbb R^4$,"Consider the projective space $\mathbb R P^2$, constructed as the quotient space of $S^2$ with the equivalence relation where we identify opposite points (or alternatively, the space of all lines in $\mathbb R^3$ through $0$). I now want to show that the mapping $$f: \mathbb R P^2 \to \mathbb R^4, (x : y : z) \mapsto (x^2 - y^2, x y, x z, y z)$$ is an injective immersion, in order to prove that $\mathbb R P^2$ is a submanifold of $\mathbb R^4$. Now I've found similar threads about this topic like this and this , but the part where I'm struggling is actually not showing that $f$ is an immersion, but that $f$ is injective , which I couldn't find anywhere. I tried to start with an image point $(a, b, c, d)$ of $f$ and tried to show that this already determines the equivalence class of any $(x, y, z) \in S^2$ with $f(x, y, z) = (a, b, c, d)$, but no matter which components of that equation I tried to add and subtract, I couldn't get anywhere. I was thinking that maybe there is an easy argument to it that I've just missed? Or it's really just a flat calculation that I can't seem to put together. Any help would be appreciated.","['submanifold', 'projective-space', 'differential-geometry', 'projective-geometry']"
2504864,"How to say that any group of order $pqr$ is cyclic with a provided relation between $p$, $q$, $r$, where they are all distinct primes.","I have given that, let $G$ be a group of order $455$. Then I have to show that $G$ is cyclic. Then by using Sylow-theorems it can be solved. But my question is, is there any method to see quickly that a group of order $pqr$ with a relation between them, where all of $p$, $q$, $r$ are distinct primes, is cyclic. Just like a group of order $pq$ is cyclic if $q>p$ and $p$ does not divide $q-1$. For example, any group of order $15$ is cyclic just from the above argument. Here $455=5.7.13$. So is there any simple way to say $G$ is cyclic?","['finite-groups', 'group-theory', 'cyclic-groups']"
2504903,Prove equivalence of definitions of submanifolds,"Definition 1 : A subset $M ⊂ \Bbb R^n$ is a $m$-dimensional submanifold of $\Bbb R^n$ if for all $x$ in $M$, there exists open neighborhoods $U$ and $W$ of $x$ and $0$ in $\Bbb R^n$ respectively, and a diffeomorphism $f : U \rightarrow W$ such that $f(U ∩M) = W ∩ (\Bbb R^m × \{0\}^{n-m})$. Definition 2 : A subset $M ⊂ \Bbb R^n$ is a $m$-dimensional submanifold of $\Bbb R^n$ if for all $x$ in $M$, there exists an open neighborhood $U$ of $x$ in $\Bbb R^n$, an open set $V$ in $\Bbb R^m$, and an injective immersion $\phi : V \rightarrow U$ such that $\phi(V) = U ∩ M$ and $\phi$ is an homeomorphism between $V$ and $U ∩ M$. I can prove $1 \Rightarrow 2$. Indeed, by definition, for all $x$ in $M$, there exists open neighborhoods $U$ and $W$ of $x$ and $0$ in $\Bbb R^n$ and a diffeomorphism $f : U \rightarrow W$ such that $f(U ∩M) = W ∩ (\Bbb R^m × \{0\}^{n-m})$. Define $V = \{(x^1, ..., x^m) \in \Bbb R^m \mid (x^1, ..., x^m, 0, ..., 0) \in W\}$ and $\phi : V \rightarrow U$ by $\phi(x) = f^{-1}(x, 0)$. However, I'm struggling with the other direction even though I have this theorem : Theorem (Local normal form for immersions) : Let $U$ in $\Bbb R^n$, $V$ in $\Bbb R^m$ be open set and let $\phi : V \rightarrow U$ be an injective immersion. Then for all $x$ in $V$, there exists open neighborhoods $V'$ and $U'$ of $x$ and $\phi(x)$ in $V \subset \Bbb R^m$ and $U \subset \Bbb R^n$ respectively, with $\phi(V') \subseteq U'$, and a diffeomorphism $F : U' \rightarrow F(U')$ such that $F \circ \phi(x^1, ...., x^m) = (x^1, ..., x^m, 0, ..., 0)$ on $V'$. The function we are looking for as $f$ in Definition 1 is $F$. However, I cannot find good neighborhoods (because of the inclusion $\phi(V') \subseteq U'$) to get the result. I have Jacques Lafontaine's book ""An Introduction to Differential Manifolds"" and in this book, he just says that we can restrict ourself so that it's good. Well, it's not enough of an argument for me. Any help here ?","['manifolds', 'smooth-manifolds', 'differential-geometry', 'differential-topology']"
2504918,Representing a unit speed curve on a sphere in terms of its Frenet Frame,"Let $\alpha$ be a unit speed curve with positive curvature $\kappa \gt 0$ and non-zero torsion $\tau \ne 0$, lying on a sphere of radius $r$ centred at $c \in \Bbb{R}^3$. Show that $\alpha - c = -\frac{1}{\kappa}N - (\frac{1}{\kappa})'\frac{1}{\tau}B$. Where N is the principle normal vector field and B is the binormal vector field to $\alpha$. I'm not sure how it is possible to show this as all the components on the right hand side of the equation involve derivatives of the curve on the left hand side, any help would be much appreciated. Then, deduce a formula for the radius of the sphere in terms of $\kappa,\tau$.",['differential-geometry']
2505047,How do you Evaluate $\int_{-\infty}^\infty \frac{\cos(x)}{x^2+1}dx$ Without Using Residue Calculus,"So I'm having trouble computing this integral: $$\int_{-\infty}^\infty \frac{\cos(x)}{x^2+1}dx$$ After seeing some other answers on this website, I've realized that many people are using residues to get an answer. However, I know nothing about complex analysis, so I wanted to know if there is a better way of doing it without it. I started by finding the indefinite integral which is: $$-\dfrac{\sinh\left(1\right)\left(\operatorname{Si}\left(x+\mathrm{i}\right)+\operatorname{Si}\left(x-\mathrm{i}\right)\right)-\mathrm{i}\cosh\left(1\right)\left(\operatorname{Ci}\left(x+\mathrm{i}\right)-\operatorname{Ci}\left(x-\mathrm{i}\right)\right)}{2}$$ but finding what this function comes out to when finding the limit of it towards $\infty$ and $-\infty$ I get $-\pi\sinh(1)$ Now, I could be very wrong with both the indefinite integral and the value. Am I doing something wrong? Am I forgetting to take into account something I should be? Thanks in advanced!","['improper-integrals', 'integration', 'definite-integrals', 'calculus']"
2505117,Empirical Distribution Function.,"Suppose $T$ denotes a nonnegative random variable representing the lifetimes of individuals in some population. Let $t_i,i=1,2,...,n,$ denotes an ordered observed value. Then the empirical survivor function(esf) is defined by $$S(t)=\frac{\text{number of observations}>t}{n}.$$ Consider the observed valus of $T$: $9,13,13,18,23,28,31,34,45,48,161$. According to the formula, The values of esf for this data are given below: $$
\begin{array}{c|cccccccccc}
t & 9&13&18&23&28&31&34&45&48&161 \\
\hline
S(t)&\frac{10}{11}&\frac{8}{11}&\frac{7}{11}&\frac{6}{11}&\frac{5}{11}&\frac{4}{11}&\frac{3}{11}&\frac{2}{11}&\frac{1}{11}&0
\end{array}.
$$ My question is: at $t=13$, the first individual with $t=9$ is died. That is, at $t=13$, I have actually $n=10$ individuals remaining. Then why doesn't the denominator (number of individuals) reduces with time in the formula of $S(t)$? Why is the calculation of esf  not like the following? $$
\begin{array}{c|cccccccccc}
t & 9&13&18&23&28&31&34&45&48&161 \\
\hline
S(t)&\frac{10}{11}&\frac{8}{10}&\frac{7}{8}&\frac{6}{7}&\frac{5}{6}&\frac{4}{5}&\frac{3}{4}&\frac{2}{3}&\frac{1}{2}&0
\end{array}?
$$","['probability-theory', 'probability', 'statistics']"
2505142,A question about sequence and subsequences,"Give example of Sequence and Subsequences which satisfy following $\{x_n\}$ is not increasing , but $\{x_n\}$ has increasing subsequence. $\{x_n\}$ is unbounded , but $\{x_n\}$ has a bounded subsequence. A sequence of integers $\{x_n\}$ which diverse , but which has infinitely many distinct subsequnetial limits for (1) $x_n=\begin{cases} n& n=2k\\\frac1n &n=2k+1\end{cases}$ this not increasing it has increasing subsequence (2)  consider $x_n = n$ if $n$ is even and $x_n=0$  if $n$ is odd here $\{x_n\}$ is unbounded but $\{x_n\}$ has subsequence is i am correct for (1) and (2) if i am please can any give example of (3)","['real-analysis', 'sequences-and-series']"
2505151,Mode of sum of two random variables (RVs) v.s. Mode of each RV,"I wonder is there any literature or answer of the following relationship. What I want to know is whether the maximum peak of $\mathbf{X}+\mathbf{Y}$ is reasonably close to the maximum peak of $\mathbf{X}$. Suppose the following things. $\mathbf{X}$ and $\mathbf{Y}$ are two random variables on the same probability space $\mathbf{X}$ should be any arbitrary distribution, while $\mathbf{Y}$ is unimodal (or uniform distribution within some range). $\mathbf{X}$ can have multiple modes, but there is only a single mode that has  a maximum probability. Doesn't matter whether discrete or continuous case for simplicity, $\mathbf{Y}$ has no shifted bias, i.e. zero-mean Can I quantify or bound the position of the mode of $\mathbf{X}+\mathbf{Y}$ by the positions of the maximum mode of $\mathbf{X}$ and $\mathbf{Y}$? 
I don't want to get some simulation examples, which has been already checked by myself. What I'm interested in is an explicit inequality by each of mode. If any generic relationship doesn't exist, what will be the condition for that? I appreciate in advance, thanks!","['convolution', 'probability-distributions', 'statistics', 'probability', 'random-variables']"
2505156,"Why is this zig zag function monotone increasing at $0$, but not monotone increasing in a neighborhood of $0$?","This is from The Way of Analysis by Strichartz. I believe see a neighborhood of $0$ such that $x_1 < x_2 \implies f(x_1) \le f(x_2)$. Why is this zig zag function monotone increasing at $0$, but not monotone increasing in a neighborhood of $0$?","['intuition', 'real-analysis', 'monotone-functions', 'calculus', 'functions']"
2505170,Making sense of the quaternion product,"I'm learning quaternions $\,Q=a+bi+cj+dk\,$ with $4$ real components $a,b,c,d\in\mathbb{R}$. I'm familiar with the scalar-vector representation of $\,Q=a+\vec{v}$, where $\,\vec{v}=bi+cj+dk\in\mathbb{R}^3$. But I'm not so familiar with the double-complex representation of $\,Q=(a+bi)\!+\!(c+di)j\,$ using $\,ij=k\,$ for the quaternions $\mathbb{H}$. I'm trying to make sense of the product of two quaternions $$(z_1+w_1j)(z_2+w_2j)=(z_1z_2-w_1w_2^*)+(z_1w_2+w_1z_2^*)j.$$ I understand how this is derived. But I'm looking for a geometric interpretation of its meaning. In the scalar-vector representation, I could understand $$(a_1+\vec{v}_1)(a_2+\vec{v}_2)=(a_1a_2-\vec{v}_1\cdot\vec{v}_2)+(a_2\vec{v}_1+a_1\vec{v}_2+\vec{v}_1\times\vec{v}_2),$$ by decomposing $\vec{v}_2=\vec{v}_{2\parallel}+\vec{v}_{2\perp}$ into components along $\vec{v}_1$ and perpendicular to $\vec{v}_1$. Then the first part $a_2+\vec{v}_{2\parallel}$ satisfies the complex product and the second part $\vec{v}_{2\perp}$ gets rotated about $\vec{v}_1$ following the right-hand rule. The geometry becomes clearer if $\,a_1+\vec{v}_1=r_1(\cos\theta_1+\hat{v}_1\sin\theta_1)\,$ is factorized into a norm times a unit quaternion. I'm not so sure what is the right way to understand the double-complex representation. Please help me. Thanks!","['quaternions', 'geometry']"
2505172,How to determine the value of a variable in a matrix to make it linearly independent of two other given matrices.,"I am given matrices: \begin{pmatrix}1&2\\ 0&1\end{pmatrix}\begin{pmatrix}1&0\\ \:1&0\end{pmatrix}\begin{pmatrix}1&0\\ a\:&-2\end{pmatrix} and asked to determine the value of $a$ such that the above matrices are linearly independent . I know how to work with vectors: put them together into a matrix and use row reduction, if no free columns then the vectors are linearly independent. But how to do this with matrices? Thank you.","['matrices', 'linear-algebra']"
2505202,What is the space of random variables equipped with the expectation inner product?,"While studying probability, I was always fascinated at how random variables seem to satisfy a host of well-known inequalities from linear algebra, such as the Cauchy Schwartz inequality. It was when I read this article on Wikipedia did I realize that you could define an inner product on the space of random variables. Recall that a random variable is a function that assigns outcomes to real numbers. Therefore, if we equip the set of random variables with the inner product taken as an expectation, i.e.,  ${\displaystyle \langle X,Y\rangle \triangleq  \operatorname {\mathbb{E}} (XY)}$, we obtain an inner product space of these functions. Observe that $\mathbb{E} (XY)$ is the correlation of $X,Y$. It seems that this space is under-discussed in applied probability literature. Can someone elaborate if there is a name for this particular inner product space, i.e. ""correlation space"" ? Is this inner product space complete, i.e., a Hilbert space?","['hilbert-spaces', 'reference-request', 'probability-theory', 'probability', 'vector-spaces']"
2505228,"Are absolutely convergent series ""many"" or ""few"" compared to conditionally convergent series?","We can identify absolutely convergent series with the $l^1$ space and conditionally convergent series with a subspace of $c_0 = \{\{a_n\} \in l^\infty : a_n \to 0\}$ Since $l^1$ contains all finite sequences, it is dense in $c_0$, so in this sense absolutely convergent series are dense, or ""common"" among all series. However, this result is not terribly illuminating, since finite sequences are also dense in $c_0$. Are there other ways to look at the relationship between absolutely and conditionally convergent series? Maybe absolutely convergent series are meager, open or dense when we consider a more appropriate topology on the set of convergent sequences.","['general-topology', 'real-analysis', 'baire-category', 'functional-analysis']"
2505279,Is this subset of the product space of $\mathbb R^{n+1}$ and the projective space $\mathbb R P^n$ a submanifold?,"Consider the projective space $\mathbb R P^n$, constructed as the quotient space of the sphere $S^n \subseteq \mathbb R^{n+1}$ with the equivalence relation where we identify opposite points. I now want to prove or disprove that the set $M := \{(x, [y]) : [y] \in \mathbb R P^n, x \in [y]\}$ is a submanifold of the manifold $N := \mathbb R^{n+1} \times \mathbb R P^n$. Now I rather suspect that it's not a submanifold, but I wasn't able to prove it so far. My thinking was, $M$ is a submanifold of $N$ if and only if there is an injective immersion $\iota: M \to N$. Now the first choice for such an immersion would be the natural map $M \to N, (x, [y]) \mapsto (x, [y])$. To check if that's an immersion, we would need to check that the Jacobean matrix is injective. I have not proven it yet but I suspect it will not be injective, basically because the components of a vector $x$ of the first component of $(x, [y])$ also appear in the $[y]$-component so the Jacobean it shouldn't add up to full rank? But let's assume that's true, then I would have shown that the identity map $M \to N$ isn't an immersion... how would I go about showing that there doesn't exist any other map $M \to N$ that happens to be an injective immersion? Is there an easy way to see on why this is or isn't the case? The most similar question I could find was this one and the set in question is not the same there as far as I can tell.","['projective-geometry', 'projective-space', 'manifolds', 'submanifold', 'differential-geometry']"
2505284,A number with an interesting property. $abcd=a^b c^d$,"I am finding a 4-digit number $abcd$ (in base-10 representation) satisfying the following property. $$abcd=a^b c^d$$ I have been running my mind around this problem since a long time, but got no success. I wrote a python program for a number with this property, and got the answer to be $2592$ since $$2592=2^5\times 9^2$$ But I am looking for a purely mathematical way to solve this problem. Thanks!","['number-theory', 'decimal-expansion']"
2505299,Problem on Euler's Phi function,"Let $S(n)$ be $S(n)=\left\{k\;\left|\;\left\{\frac{n}{k}\right\}\right.\geq \frac{1}{2}\right\}$,where $\{x\}$ is the fractional part of $x$ Prove that : \begin{align}
\sum_{k\in S(n)} \varphi(k)=n^2
\end{align} maybe the fact
\begin{align}
\sum_{k \leq 2n}\left\lfloor{\frac{n}{k}+\frac{1}{2}}\right\rfloor \varphi(k)=\frac{3n^2+n}{2}\end{align} and
\begin{align}
\sum_{k \leq 2n}\left\lfloor{\frac{n}{k}}\right\rfloor \varphi(k)=\frac{n^2+n}{2}\end{align} can help, but I can't prove these two equations Edited :Sorry that my question is to prove the two equation before, because I've already known how to relate the problem with these two equations, and I've also known how to prove the second equation from Identity involving Euler's totient function: $\sum \limits_{k=1}^n \left\lfloor \frac{n}{k} \right\rfloor \varphi(k) = \frac{n(n+1)}{2}$ but I can't solve the first one with the similar method.","['number-theory', 'totient-function', 'functional-equations']"
