question_id,title,body,tags
4503097,The least possible number of tokens on a $n√ón$ board,"Tokens are placed on the squares of a $2021
√ó
2021$ board in such a way that each square contains at most one token. The token set of a square of the board is the collection of all tokens which are in the same row or column as this square. (A token belongs to the token set of the square in which it is placed.) What is the least possible number of tokens on the board if no two squares have the same token set?. It's a problem from https://gonitzoggo.com/archive/problem/441/english Recently, I've been taking preparation for junior Math Olympiad Contest and found this problem. I have calculated least possible number of tokens for $ 2√ó2, 3√ó3, 4√ó4, 5√ó5$ but finding no pattern between them.","['contest-math', 'combinatorics', 'discrete-mathematics']"
4503099,What is the relation between the high school definition of a vector and the abstract algebra definition?,"I'm studying linear algebra, and I've been getting to grips with the idea of groups and fields and vector spaces. From what I understand, to be a vector is just to be an element of a vector space, and all sorts of unusual (to the beginner) things turn out to be vector spaces. For example, the set of all real-valued functions is a vector space over $\mathbb{R}$ . I take it this means that all real-valued functions (e.g. $\sin x$ ) are vectors over $\mathbb{R}$ . But how does this abstract definition fit with the earlier account we are given of vectors as quantities with magnitude and direction or directed line segments?","['vectors', 'linear-algebra', 'vector-spaces']"
4503105,Can you generalize this alternating series for $\log(2)$ to any logarithm series of a similar form?,"Look at the decimal expansion for the alternating sum for $\log(2)$ summed upto $n=100000$ and subtract it with the decimal expansion for the actual value for $\log(2)$ : $$\log 2=\sum_{n=1}^{100000}\frac{(-1)^{n-1}}n$$ (* Mathematica start*)
digits=100;
N[Sum[(-1)^(n + 1)/n, {n, 1, 100000}], digits]
N[Log[2], digits]
%% - %
(*end*) Decimal expansion for the alternating sum for $\log(2)$ truncated at $n=100000$ : 0.69314218058494530941598212145842656807539388436033275412059363449353\
01469694042252941638997436445553 Decimal expansion of actual value for $\log(2)$ 0.69314718055994530941723212145817656807550013436025525412068000949339\
36219696947156058633269964186875 The difference between the truncated series and actual value for $\log(2)$ : -4.9999750000000012499999997500000001062499999225000000863749998634750\
00290490311699427252774132*10^-6 Notice how it is correct at many decimal places beyond the first error. I find that strange. It is like it suggests that there is a series acceleration that would correct this. By guessing and adding fractions that I looked up in the OEIS, it suggests that the alternating sum together with the correcting part for parameters $p = 4$ and $r = 10$ is: $$\log(2)=\sum _{n=1}^{6\ 10 p r} \frac{2 (-1)^n \left(\left(1-2^{-2 n}\right) \zeta (2 n) \Gamma (2 n)\right)}{\pi ^{2 n} \left(r^{2 p}\right)^n}+\sum _{n=1}^{r^p} \frac{(-1)^{n+1}}{n}+\frac{r^{-p}}{2} \;\;\;\;\;\;\;\;(\ast)$$ For $r^p=10^4=10000$ , this gives the decimal expansion of $\log(2)$ to $6004$ correct digits,
as shown by this Mathematica program: (*start*)
p = 4;
r = 10;
digits = r^p;
N[Sum[(-1)^(n + 1)/n, {n, 1, r^p}] + 1/2*r^(-p) + 
   Sum[(-1)^(n)*2*(Gamma[2 n]*(1 - 2^(-2 n))*Zeta[2 n])/
       Pi^(2 n)/(r^(2*p))^n, {n, 1, 6*10*r*p}], digits];
N[Log[2], digits];
%% - %
(*end*) If this is a known result I apologize. What I really would like to know is how does this generalizes to series of similar form for $\log(3),\log(4),\log(5)$ and so on. Also, the requirement that $r=10$ is not optimal, and I don't know how to generalize it away so that $r^p$ can be any integer. For $\log(3)$ with Mathematica: I found the following repeating decimal expansion for the difference between the decimal expansion from the series: $$\text{log3}=\sum _{n=0}^{333333} \left(\frac{1}{(3 n+1)}+\frac{1}{(3 n+2)}-\frac{2}{(3 n+3)}\right) \;\;\;\;\; (\ast\ast)$$ truncated at $n=333333$ and the actual decimal expansion for $\log(3)$ : (*start*)
nn = 333333;
s = 1;
N[Sum[1/(3*n + 1)^s + 1/(3*n + 2)^s - 2/(3*n + 3)^s, {n, 0, nn}], 100]
N[Log[3], 100]
%% - %
(*end*) Truncated series for $\log(3)$ : 1.09861128867077635139526057022385909953625944698030440640216033343750\
3992932594561746334595530692166 Decimal expansion for the actual value of $\log(3)$ : 1.09861228866810969139524523692252570464749055782274945173469433363749\
4293218608966873615754813732089 Difference between decimal expansions for truncated series and actual $\log(3)$ : -9.9999733333999998466669866660511123111084244504533253400019999030028\
6014405127281159283039922*10^-7 Here again we could try to guess the fractions that are found inbetween the repeating digits: (*Mathematica start*)
-9.9999733333999998466669866660511123111084244504533253400019999030028\
6014405127281159283039922419278832912630462820405230189615775`93.\
65812459130527*^-7
(%*3) - 4/5*10^(-6 - 5)
% + 1/5*10^(-16)
% - (1 - 54/100)*10^(-22)
% + (1 - 5/100)*10^(-28)
(*end*)

{-4/5*10^(-6 - 5), +1/
   5*10^(-16), -(1 - 54/100)*10^(-22), +(1 - 5/100)*10^(-28)} The guess simplifies to the first few fractions: $$\left\{-\frac{1}{125000000000},\frac{1}{50000000000000000},-\frac{23}{500000000000000000000000},\frac{19}{200000000000000000000000000000}\right\}$$ An OEIS search . Question: Can you generalize the formula in $(\ast)$ for $\log(2)$ to apply to the
formula for $\log(3)$ in $(\ast\ast)$ ? That is can you generalize this formula: $$\log(2)=\sum _{n=1}^{6\ 10 p r} \frac{2 (-1)^n \left(\left(1-2^{-2 n}\right) \zeta (2 n) \Gamma (2 n)\right)}{\pi ^{2 n} \left(r^{2 p}\right)^n}+\sum _{n=1}^{r^p} \frac{(-1)^{n+1}}{n}+\frac{r^{-p}}{2} \;\;\;\;\;\;\;\;(\ast)$$ to apply to this formula: $$\log(3)=\sum _{n=0}^{\infty} \left(\frac{1}{(3 n+1)}+\frac{1}{(3 n+2)}-\frac{2}{(3 n+3)}\right) \;\;\;\;\; (\ast\ast)$$ Or even: $$\log(4)=\sum _{n=0}^{n=\infty} \left(\frac{1}{(4 n+1)}+\frac{1}{(4 n+2)}+\frac{1}{(4 n+3)}-\frac{3}{(4 n+4)}\right)$$ and so on .","['logarithms', 'sequences-and-series']"
4503110,Why does $1 - x + \frac{1}{2}x^2-\frac{1}{6}x^3+\dots$ converge to $e^{-x}$?,"The series $$
1 - x + \frac{1}{2}x^2-\frac{1}{6}x^3+\dots=e^{-x}
$$ It is not totally clear from (for example) limits like $$
\lim_{x \to \infty}( 1 - x + \frac{1}{2}x^2) = \infty \\
\lim_{x \to \infty}( 1 - x + \frac{1}{2}x^2-\frac{1}{6}x^3) = -\infty \\
\vdots
$$ why the sum to infinity has a limit of 0. What is ""happening"" after each partial sum diverges? I suppose the remaining part of the series diverges in the other direction in just the right way that we end up with facts like $$
\lim_{x \to \infty}( e^{-x}) = 0
$$ but this is not always true just from the oscillating infinities. The reason this is important is that there should/might be a general condition for a oscillating power series to have a limit of $0$ , of which this is a simple example. But how does even this simple case have limit $0$ , without first seeing that the series is $e^{-x}$ ?","['limits', 'sequences-and-series', 'taylor-expansion', 'real-analysis']"
4503128,"Is my proof wrong? ""Prove that the additive inverse is unique""","""Prove that for each x $\in V$ , where $V$ is a vector space, the additive inverse $-x$ is unique."" My proof is as follows; Let $V$ be a vector space, let $x \in V$ . For contradiction, assume the additive inverse $-x$ is not unique. Then, there exists $-x'$ such that \begin{align*}
x + (-x') = 0 \\
x + (-x) = 0 \\
\end{align*} We can rearrange this to see \begin{align*}
x = -(-x') \\
x = -(-x) \\
\Rightarrow -(-x) = -(-x') \\
-x = -x' \\
\end{align*} Therefore, the additive inverse $-x$ is unique. In the book and other places I've seen other approaches to this problem, and I'm wondering if this proof is wrong? Other proofs referred to the axioms of vector spaces but I didn't find it necessary. I guess rate my proof would be the point of this question as I'm still uncertain if my proofs are solid. edit: The axioms referenced are the 8 axioms on Vector Space operations found in ""A course in linear algebra"" by David Damiano.","['solution-verification', 'linear-algebra', 'axioms']"
4503151,Five bags of distinct colour each bag has 5 five balls of corresponding colour. One ball is transferred to other bag from each bag. Find probability.,"There are $5$ bags of colours Blue, Green, Pink, Red and yellow. Each of these bags have $5$ balls of same colour as that of the bag. A ball is drawn from blue bag and is transferred to one of the other bags. Then, a ball is drawn from green bag and transferred to one of the other bags. Similarly, $1$ ball is transferred from the pink bag, then from red bag and then from yellow bag (to one of the other bags). Find the probability that at the end each bag has $5$ balls of same colour, given that there are $5$ balls in each bag at the end. My working: To keep the number of balls same in each bag, none of the bags must receive more than 1 ball i.e., $ùê∑_5=44$ cases to do it. All Favourable Cycles: (1) $B\to G\to B; P\to R\to Y\to P$ (2) $B\to P\to B; G\to R\to Y\to G$ (3) $B\to R\to B; G\to P\to Y\to G$ (4) $B\to Y\to B; G\to P\to R\to G$ (5) $B\to G\to P\to B; R\to Y\to R$ (6) $B\to G\to R\to B; P\to Y\to P$ (7) $B\to G\to Y\to B; P\to R\to P$ (8) $B\to P\to R\to B; G\to Y\to G$ (9) $B\to P\to Y\to B; G\to R\to G$ (10) $B\to R\to Y\to B; G\to P\to G$ (11) $B\to G\to P\to R\to Y\to B$ Now, P(Each bags content remain same/Each bag has 5 balls at the end) $=\frac{\left[10\times 1\times \left(\frac16\right)^3\times \left(\frac14\right)^5+1\times\left(\frac16\right)^4\left(\frac14\right)^5\right]}{44\times\left(\frac14\right)^5}=\frac{61\times \left(\frac16\right)^4\times \left(\frac14\right)^5}{44\times\left(\frac14\right)^5}=\frac{61}{57024}$","['conditional-probability', 'probability']"
4503164,Can i use the inverse approximate Pi Function to approximate the nth prime?,"I'm using an approximation of $x/\ln(x)$ ~ $œÄ(x)$ and I am using the logic that the square root function counts how many squares there are and the log function counts how many powers of base b there are, and inverting those functions gives you the nth square or power. The Pi function works the same way as the other listed functions, where the value increases by 1 every new prime, and the same with the approximation, so does it make sense to take the inverse of $f(x)=x/\ln(x)$ , which is $f^{-1}(x)=-xW_{-1}(-1/x)$ as the approximation for the nth prime? If so, how good of an approximation is this? (I used only the branch of $W_{-1}$ which would experience a positive, unbounded growth since using $W_0$ would have given me only values below 2 which I didn't want.)","['number-theory', 'recreational-mathematics', 'prime-numbers']"
4503194,Intricate Inequality,"Let $c$ be the smallest positive real number such that for all positive integers $n$ and all positive real numbers $x_1,....,x_n$ , the below inequality holds. Compute $‚åà2022c‚åâ$ . $$\sum_{k=0}^n \frac{(n^3+k^3-k^2n)^\frac{3}{2}}{\sqrt {x_1^2+...+x_k^2+x_{k+1}+...+x_n}}‚â§\sqrt{3}(\sum_{i=1}^n \frac{i^3(4n-3i+100)}{x_i})+cn^5+100n^4$$ I was practicing Math Open Spring Contest (OMO Spring 2020) question for upcoming Junior Math Olympiad purpose and had found this interesting problem. Question link : https://drive.google.com/drive/u/1/folders/1-sJSrnuEFlrZhd4c-qpb_nURbfxqXbrr I tried solving this problem with random values of $x_i$ and $1‚â§n‚â§4$ but ended up with different values of $c$ each time.","['contest-math', 'inequality', 'summation', 'discrete-mathematics']"
4503207,"Let $G_1, G_2$ be groups, $H_1 < G_1, H_2 < G_2$ and $\phi: G_1 \to G_2$ an isomorphism with $\phi(H_1) = H_2$. Prove that $[G_1 : H_1] = [G_2 : H_2]$","The problem statement is: Suppose $G_1, G_2$ are groups and $H_1 < G_1, H_2 < G_2$ and $\phi: G_1 \to G_2$ an isomorphism with $\phi(H_1) = H_2$ Prove that $[G_1 : H_1] = [G_2 : H_2]$ So, for this problem I can do it in the case that $G_1, G_2$ finite (since it's almost trivial) Since $\phi$ an isomorphism then we know $\lvert G_1 \rvert = \lvert G_2 \rvert$ and since the image of $H_1$ is all of $H_2$ then it must be the case that $\lvert H_1 \rvert = \lvert H_2 \rvert$ . Thus, by Lagrange's Theorem $$[G_1:H_1] = \frac{\lvert G_1 \rvert}{\lvert H_1 \rvert} = \frac{\lvert G_2 \rvert}{\lvert H_2 \rvert} = [G_2 : H_2]$$ However the general case where either group is infinite is where I'm having trouble. Since we can't appeal to Lagranges Theorem, as well the fact that $\phi$ is an isomorphism does not necessarily mean that $\lvert G_1 \rvert = \lvert G_2 \rvert$ . Does the fact that $\phi(H_1) = H_2$ imply that $\lvert H_1 \rvert = \lvert H_2 \rvert$ in this case though? What exactly should I be thinking about to get moving in the right direction? I know this question is asking if there's an isomorphism between $2$ general groups each with subgroup such that the image of one subgroup under the isomorphism is exactly the other subgroup then the number of left cosets of $H_i$ in $G_i$ for $i = 1,2$ must be equal. The only other piece of information that could be useful that comes to mind is the fact that $G_i/H_i$ is a partition of $G_i$ . Hence, for example for $G_1$ we know that $\forall g_i, g_k \in G_1\ \text{either}\ g_iH_1 = g_kH_1\ \text{or}\ g_iH_1 \cap g_kH_1 = \emptyset$ $\bigcup_{g_iH_1 \in G_1/H_1} g_iH_1 = G_1$ But not sure how to proceed.","['group-theory', 'abstract-algebra', 'group-isomorphism']"
4503219,"Let $G$ be a group, $H < G$, and $A_H=\{g\mid\bar{g}\subset H\}$ where $\bar{g}$ is the conjugacy class of $g$. Prove $A_H\lhd G$.","Suppose $G$ is a group and $H < G$ and $A_H=\{g \mid \bar{g} \subset H \}$ where $\bar{g}$ is the conjugacy class of $g$ . I want to prove $A_H\lhd G$ . First approach: Define $X$ as the set of left cosets of $H$ , then define a homomorphism $ \varphi(g) = f_g$ from $G$ to $S_X$ where $f_g(aH)=gaH$ . $g_1=g_2$ implies $f_{g_1}=f_{g_2}$ , so $\varphi$ is well defined. Also $\varphi(g_1 g_2)=f_{g_1 g_2}=f_{g_1}(f_{g_2})=\varphi(g_1) \circ \varphi(g_2)$ shows $\varphi$ is a homomorphism. Now, $\varphi(g)=e_{S_X}$ implies $f_g(aH)=gaH=aH$ for all $a \in G$ . Equivalently, it implies $a^{-1}ga \in H$ for all $a \in G$ . Therefore, $\ker(\varphi)=A_H$ , and so $A_H \lhd G$ . Now, I want to directly prove, first, $A_H$ is a subgroup of $G$ , then it is normal in $G$ . I cannot even prove $A_H$ is a subgroup because I cannot prove $A_H$ is closed under inverse. Can anyone prove $A_H \lhd G$ directly?","['group-homomorphism', 'alternative-proof', 'normal-subgroups', 'abstract-algebra', 'group-theory']"
4503249,Prove that the $n$th difference of $x^n$ is $\sum _{i=0}^n(-1)^i\binom{n}{i}(x-i)^n$,"Prove that the $n$ th difference of $x^n$ is $\displaystyle \sum \limits _{i=0}^n(-1)^i\binom{n}{i}(x-i)^n$ (here the first difference is $\Delta (x^n):=(x)^n-(x-1)^n$ and the $n$ th difference is defined recursively as $\Delta ^n(x^n):=\Delta ^{n-1}((x)^n)-\Delta ^{n-1}((x-1)^n)$ ). I tried using induction, but just proving $\Delta ^n(x^n)$ is given by the indicated formula for all $n$ doesn't seem to work. Instead, I may need a more general formula involving $\Delta ^n(x^m)$ , where $m$ and $n$ are positive integers. However, I'm not sure how to come up with this more general formula.","['induction', 'polynomials', 'discrete-mathematics']"
4503267,Solve the following differential ecuation: $y'x^2 = 1+3y$,"Solve the following differential equation: $y'x^2 = 1+3y$ , with $y(3) = -\frac{1}3$ I tried the following but i'm a little bit lost at the end. $\frac{dy}{dx}x^2=1+3y$ $\frac{dy}{1+3y}=\frac{dx}{x^2}$ $\frac{1}{3}\ln(1+3y)=-\frac{1}{x}+c$ $\ln(1+3y)=-\frac{3}{x}+c$ $(1+3y)=e^{-\frac{3}{x}}e^c$ $y=\frac{(e^{-\frac{3}{x}}k)-1}{3}$ Well... here i dont know how to end the problem.",['ordinary-differential-equations']
4503313,Topology basis consisting of convex sets in metric spaces,"Let $(X,d)$ be a metric space. For all points $x,y \in X$ we define the metric segment between them as the following set: $$\left [ x,y \right ] =  \left \{ z \in X : d(x,z)+d(z,y)=d(x,y)\right \}$$ We then say that a set $S\subseteq X$ is convex if for all $x,y \in S$ it holds true that $\left [ x,y \right ] \subseteq S$ . Denote by $\tau$ the topology on $X$ induced by the metric $d$ . My question is does there exist a family $\mathcal{B}\subseteq \tau$ of convex sets such that $\mathcal{B}$ is a basis for the topology $\tau$ ? It should be noted that open and closed balls in metric spaces are not necessarily convex sets. Also, arbitrary intersection of convex sets in metric spaces is a convex set. Thus, we can define convex hulls.","['general-topology', 'convexity-spaces', 'metric-spaces', 'real-analysis']"
4503324,Intuitive Understanding of the Fisher Information?,"Within Statistics and Probability, the Fisher Information ( https://en.wikipedia.org/wiki/Fisher_information ) is generally said to describe the ""amount of information that an observed random variable (i.e. a random data point) carries about some hidden parameter"". For example: If we assume a Uniform Probability Distribution with parameters (a,b) - any random point carries as much information as any other random point about the parameters (a,b). But in a Normal Probability Distribution with parameters (mu, sigma), points from the ""tail"" areas of the distribution are said to carry more information about the parameters (mu, sigma) compared to points closer to the ""peak areas"" (I am not exactly sure why this is). Looking at the mathematical formula for the Fisher Information - it appears to be the ""expected value of the square of the second derivative of the log likelihood function (for some probability distribution). This leads me to my question: Why does the ""expected value of the square of the second derivative of the log likelihood function (for some probability distribution)"" tell us about the ""information that an observed random variable carries about some hidden parameter""? I can understand both of these arguments in isolation - I can accept that for some probability distributions, some random points are ""more informative"" about the probability distribution compared to other random points. And the mathematical formula used to calculate the Fisher Information seems (complex but) straightforward. But how does the ""expected value of the square of the second derivative of the log likelihood function (for some probability distribution)"" relate to amount of information carried by some random point? Just by looking at the mathematical formula of the Fisher Information in isolation, how am I supposed to recognize that this is a formula that is describing the informativeness of a random point? It still seems a bit ""arbitrary"" to me, even though I am sure its not. Thanks!","['statistics', 'normal-distribution', 'probability']"
4503358,Product of $i^4+4$ is a perfect square,"The following is the Problem $50$ (Folklore) from this pdf : Determine all positive integers $k$ such that $\prod_{i=1}^k (i^4+4)$ is a perfect square. I realised that $i^4+4$ is just the Sophie-Germain identity, so the problem resolves to proving $2(k^2+2k+2)(k^2+1)$ is square, and I realised $(k^2+2k+2)(k^2+1)=(k^2+k+1)^2+1,$ so we want to prove $(k^2+k+1)^2+1=2n^2$ (because if $2x$ is a perfect square, $x$ is $n^2$ ). This makes me think of the Pell equation, but it seems to be too complicated. Any help would be much appreciated, thanks in advance.","['contest-math', 'number-theory', 'square-numbers']"
4503377,"Suppose $f:(a,b)\to\mathbb{R},c\in(a,b),$ and $f'(c)>0.$ Prove that there exists $\delta>0$ such that $f(x)>f(c)$ if $c<x<c+\delta.$","This question was on a practice final exam for my Elementary Real Analysis course. It says: Suppose $f:(a,b)\to\mathbb{R},c\in(a,b),$ and $f'(c)>0.$ Prove that there exists $\delta>0$ such that $f(x)>f(c)$ if $c<x<c+\delta.$ My solution was slightly different than their solution, so I'm wondering if this solution is correct? Here is my attempt: Proof: Since $f'(c)$ exists, $\forall\epsilon>0,\exists\delta>0$ such that if $x\in(a,b)$ and $0<|x-c|<\delta,$ then $$\left|\frac{f(x)-f(c)}{x-c}-f'(c)\right|<\epsilon. $$ Now choose $\epsilon=\frac{f'(c)}{2}.$ Then there exists some $\delta>0$ such that if $c<x<c+\delta,$ then $\begin{gather} \left|\frac{f(x)-f(c)}{x-c}-f'(c)\right|<\frac{f'(c)}{2}\\
-\frac{f'(c)}{2}<\frac{f(x)-f(c)}{x-c}-f'(c)<\frac{f'(c)}{2}\\
\frac{f'(c)}{2}<\frac{f(x)-f(c)}{x-c}<\frac{3f'(c)}{2}\\
0<\frac{f'(c)}{2}(x-c)<f(x)-f(c)<\frac{3f'(c)}{2}(x-c),
\end{gather}$ So then $f(x)>f(c).\blacksquare$","['solution-verification', 'derivatives', 'real-analysis']"
4503396,$A^{\frac{1}{n}}$ converges strongly to a projection onto $(\ker{A})^{\perp}$,"I want to show the following: Let $H$ be a Hilbert space, let $A\in\mathbb{B}(H)_{+}$ be a positive operator, and let $P$ be a projection onto $(\ker{A})^{\perp}$ (i.e. $(\ker{A})^{\perp}=\{x\in H\mid Px=x\}$ ). Then, $\{A^{\frac{1}{n}}\}_{n=1}^{\infty}$ converges strongly to $P$ . First, I tried to the proof for $x\in (\ker{A})^{\perp}$ . In other words, I want to show $$ \|A^{\frac{1}{n}}x-Px\|=\|A^{\frac{1}{n}}x-x\|=\|(A^{\frac{1}{n}}-I)x\|\to0\quad(x\in (\ker{A})^{\perp}).$$ It's embarrassing, but I can't prove $\|(A^{\frac{1}{n}}-I)x\|\to0$ . Also, I don't know how to prove for $x\notin(\ker{A})^{\perp}$ . Please teach me that proof, thank you.","['hilbert-spaces', 'operator-theory', 'projection', 'functional-analysis']"
4503399,Finding all 15-ominoes that tile the plane and have distinct internal adjacencies,"Problem Description: This problem oddly came up in Minecraft with some friends. Not sure what the best terms are; but that's partly why I'm here. So a polyomino is built up from squares. This problem is to find 15-ominoes (pentdecominoes?) that tile the plane where each square inside has a distinct ""adjacency fingerprint"" so to speak; meaning that no two squares look the same when zoomed in on based on their directly adjacent neighbors. More formally, you give every square in the polyomino one of sixteen signatures $\sigma\in \{ 0,1\}^4 $ based on whether or not it's connected to another square in the same polyomino in the north/south/east/west direction (or +y/-y/+x/-x direction equivalently). The constraint is that no two squares in the polyomino have the same signature. Pictures of solutions might help to clarify what I'm trying to say. Solutions so far: Here's a couple we found already: two solutions image / minecraft screenshot We also found a few disconnected shapes with these properties (so 15 or 16 squares with distinct adjacencies forming a rigid-but-disconnected shape that can tile the plane): three disconnected tile solutions / minecraft screenshot The 2nd images shows the relationship to Minecraft a bit. Two fence blocks placed next to each other in Minecraft will link. Thus any fence block in the game can take on 16 possible different shapes depending on whether it has a connection to the north/south/east/west. We're looking for fence linkages having all 15 connecting shapes (or all 16 if disconnected solutions are allowed since the 16th shape is just a totally unconnected fence post). Our attempts: We've tried a lot to solve this from adjacency matrices to programs that brute-force generate every possible 15-omino with this square-adjacency-constraint. Brute-forcing doesn't even work even with our clever search-space-pruning tricks. My next thought was to look up lattices and 2d tilings and just learn what constraints a tiling 15-square shape has to obey. My hope was that tiling constraints paired with the adjacency constraints would be enough to fully enumerate all solutions. But when I started researching lattices and tesselations, I was quickly overwhelmed and wasn't sure even what terms to look for. We've also figured out, based on the adjacencies, that any such solution must be at least 3 blocks wide (4 wide if it's a single connected polyomino) and no more than 7 wide in either dimension. Lastly, with our brute-forcing program, we managed to generate all 15-ominoes (and some disconnected shapes) composed of distinct-adjacency-signature squares which fit in a 5x5 area: 40ish linkages of 15 distinct fence-shapes that fit in a 5x5 area","['tessellations', 'integer-lattices', 'geometry', 'polyomino', 'tiling']"
4503451,Monotonicity of the solution of a differential equation,"I am thinking over this exercise: ""Given the differential equation $y'=g(y)$ , where $g$ is a continuous function in an interval $I$ , prove that every solution of this equation is monotonic in any interval of extrems $x_0$ and $x_1$ , where $x_0$ and $x_1$ are two consecutive zeros in $I$ of the function $g$ "". I have thought that, as in $(x_0,x_1)$ there are no zeros of $g$ , then there are no zeros of $y'$ . Consequently, there can be no maxima or minima in this interval, so there is no change of monotonicity. However,it seems to me as if this reasoning is not enough because, in that case, the problem would be so trivial. Thanks for your help.",['ordinary-differential-equations']
4503472,Cardinality of integer parts of real closed fields,"Every real closed field $R$ has in integer part $I$ . That is, $I$ is a discrete ordered subring of $R$ such that for each $x \in R$ there is $z \in I$ such that $z \leq x < z + 1$ . If $R$ is Archimedean, then $\mathbb{Z}$ is the unique integer part and if $R$ is not Archimedean, my understanding is that $R$ may have non-isomorphic integer parts. My question is: Is it always possible to find an integer part with cardinality strictly less than the cardinality of $R$ ? Is the cardinality of any integer part always strictly less than the cardinality of $R$ ? Do different integer parts have same cardinality?","['first-order-logic', 'model-theory', 'reference-request', 'ordered-fields', 'elementary-set-theory']"
4503486,approximation for 'balls in bins' problem with upper restriction.,"I am dealing with the famous problem of finding the how many integer non negative solutions there are for the equation: $ x_1+\cdots +x_l=n$ with the restrictions $ \forall i: 1\leq x_i \leq k.$ The lower bound can be easly treated by defining $ x_i=y_i +1.$ and solving $ y_1+\cdots +y_l=n-l$ with the restrictions $ \forall i: 0\leq y_i \leq k-1.$ But the upper bound yields (by the conventional methods, inclusion-exclusion or generating functions, with generating functions it is possible to solve without defining the $ y_i-$ s) an alternating series sum, which is very inconvenient expression. I tried to look up for some other solution or a simplification of the expression but couldn't find any. All that I've got is te following source https://www.mathpages.com/home/kmath337/kmath337.htm which gives an approximation. but first, without any explanation or references, and second, it requires to solve an equation of the form $ ze^{-\frac{z^2}{2}}=\tau$ (where $ \tau $ is an expression defined in the article, you won't find $ \tau$ there, it is a name I gave to the expression) which I am not exactly sure how to solve. I've tried to use stirling's approximation by I'm still stuck with a very unpleasent expression.
There are few approximations for the binomial coefficients depending on the parametrs of the binomial coefficients but I'm still left with an inconvenient expression. Does anybody knows a good approximation for the above problem, or can direct me to information about that problem? it can be ssumed that both $ l$ and $ n$ are going to infinity so assymptotic approximation is fine. Thank you all!","['factorial', 'approximation-theory', 'binomial-coefficients', 'balls-in-bins', 'discrete-mathematics']"
4503551,maximal inequality for mean zero random walk with exponential moments,"Let $X_n$ be a sequence of iid random variables with $\mathbb{E}[X_1] = 0$ and $$\mathbb{E}[e^{\alpha X_1}] < \infty \quad \text{ for each $\alpha \in [0, 1/2]$}.$$ Let $S_k = X_1 + \dots + X_k$ .
What would be a minimum set of assumptions on $X$ to obtain a bound of the form $$\mathbb{P}\left(\max_{1\leq k \leq n} S_k \geq t\sqrt{n}\right) \leq e^{-ct^{0.001}}$$ where I do not care about the power on $t$ . One way to do this is to assume that $X_1$ is sub-exponential. By Doob's maximal inequality, we have \begin{align*}
\mathbb{P}\left(\max_{1\leq k \leq n} S_k \geq t\sqrt{n}\right) 
&= \mathbb{P}\left(e^{\lambda \max_{1\leq k \leq n} S_k} \geq e^{\lambda t\sqrt{n}}\right)\\
&\leq \frac{\left(\mathbb{E}[e^{\lambda X_1}]\right)^n}{e^{{\lambda t\sqrt n}}}.
\end{align*} Now we see that if $X_1$ is sub-exponential, i.e. $\log(\mathbb{E}[e^{\lambda X_1}]) \leq C_0\lambda^2$ for some fixed $C_0$ and $\lambda$ in a small fixed interval $[0, \lambda_0]$ . Then, we get $$\log\left(\frac{\left(\mathbb{E}[e^{\lambda X_1}]\right)^n}{e^{{\lambda t\sqrt n}}} \right) \leq nC_0\lambda^2 - \lambda t \sqrt n.$$ We see that $\lambda = \frac{t}{C_0 \sqrt n}$ would be a minimizer for $nC_0\lambda^2 - \lambda t \sqrt n$ . Thus if $ \frac{t}{C_0\sqrt n} \leq \lambda_0$ , then we have the maximal probability $$\mathbb{P}\left(\max_{1\leq k \leq n} S_k \geq t\sqrt{n}\right)  \leq e^{-ct^2}.$$ Otherwise, this estimate becomes a large deviation, but with a maximum in front of the random walk. I expect it would decay like $e^{-ct}$ instead of $e^{-ct^2}$ , and I would appreciate it if someone could find a reference for this as well.","['probability-theory', 'random-walk']"
4503579,Verifying the divergence theorem over B,"This is the exercise: Let $B$ be the region of $\mathbb{R}^3$ $$B = {(x,y,z) | x^2+y^2+z^2 \le 1,\ z^2\geq x^2+y^2}$$ and and be the field $$\mathbf{F}(x,y,z) = (x,y,z)$$ My guess was since we are dealing with a hyperbole (top part) and a sphere, use spherical coordinates for the triple integral, and for the other part of the theorem use spherical for the sphere and cylindric for the upper cone. the triple integral, I integrate 3, which is the result of the divergence of F with $0\leq \theta\leq2\pi$ ; $0\leq\phi\leq1\pi/4$ and $0\leq\rho\leq1$ but it didn't work, results are weird and for the other part of the theorem the sum of the double integrals did not give me as the triple integral. Sorry about my english, is so bad.","['integration', 'multivariable-calculus', 'divergence-theorem']"
4503603,"Evaluating $ \frac{24}{\pi}\int_0^\sqrt 2\frac{2-x^2}{(2+x^2)\sqrt{4+x^4}}\,\mathrm dx$","I recently came across a problem on definite integration and couldn't solve it despite my efforts. It goes as $$
\frac{24}{\pi}\int_0^\sqrt 2\frac{2-x^2}{(2+x^2)\sqrt{4+x^4}}\,\mathrm dx
$$ The $2-x^2$ term and the upper limit along with the $2+x^2$ term at the bottom motivated me to substitute $x=2^{1/2}\tan a$ . However subsequent steps proceeded to a stage I couldnt simplify.
I tried other substitutions but was unable to proceed Any help to solve this problem would be appreciated. Please feel free to share your first thoughts and intuition as well.","['integration', 'calculus', 'definite-integrals', 'substitution']"
4503638,Munkres' Topology Chapter 2 Lemma 13.2. Is this description of C in set-builder notation correct?,"This is the statement of Lemma 13.2 in Munkres' Topology Chapter 2. Let X be a topological space. Suppose that $\mathcal{C}$ is a collection of open sets of X such that for each open set U of X and each x in U, there is an element C of $\mathcal{C}$ such that x $\in$ C $\subset$ U. Then $\mathcal{C}$ is a basis for the topology of X. The English description confuses me. I would like to express the collection $\mathcal{C}$ in set-builder notation. Is what I have written below what the author intends?
Letting $\mathcal{T}$ denote the topology on X: $$\mathcal{C}=\{V\in\mathcal{T}:\forall U\in\mathcal{T}\forall x\in U\exists C\in\mathcal{C}(x\in C \subset U)\}$$ Is my understanding correct? If not, please explain to me where I may be mistaken.","['elementary-set-theory', 'general-topology', 'first-order-logic']"
4503644,Finding explicit flow of a vector field,"I am working in a Differential Geometry problem which involves finding the explicit flow of the following vector field $$X=\frac{\partial}{\partial y}-z\log z\tan y\frac{\partial}{\partial z}.$$ Its domain is the open set $W\subset\mathbb R^3$ , where $$W=\{(x,y,z)\in\mathbb R^3\,|\,-\pi/2<y<\pi/2,\,z>0\}.$$ By applying the separation of variables method, we find that $$\gamma(t)=\big(x,y+t,e^{\cos(y+t)}\big)$$ is an integral curve of $X$ that passes through the point $(x,y,e^{\cos(y)})$ at $t=0$ . I would like to find, however, integral curves that pass through $(x,y,z)$ , for any arbitrary $z>0$ . Thanks in advance for your time.","['integration', 'ordinary-differential-equations', 'curves', 'vector-fields', 'differential-geometry']"
4503652,let $f'(x)=\frac{x^2-f(x)^2}{x^2(f(x)^2+1)}$ prove that $\lim\limits_{x \to \infty}f(x) = \infty$.,"Let $f:(0,\infty)\to\mathbb R$ be a differentiable function such that $$f'(x)=\frac{x^2-f(x)^2}{x^2(f(x)^2+1)}$$ for all $x\gt1$ . Prove that $\lim\limits_{x \to \infty}f(x) = \infty$ . Here is what i thought: $$f'(x)=\frac{x^2-f(x)^2}{x^2(f(x)^2+1)}=\frac{1-\frac{f(x)^2}{x^2}}{f(x)^2+1}$$ if somehow it can be proved that $\frac{f(x)^2}{x^2}$ lies in range $(-1,1)$ then it can be concluded that $f'(x)\gt0$ , which means function is strictly increasing and for increasing function $\lim\limits_{x \to \infty}f(x) = \infty$ is always true. But am not sure how to do that and if some one can come up with other creative solution that would be great! and also please explain thought process behind your given solution. EDIT : My assertion that increasing function tends to limit $\infty$ is wrong for some cases, for an example in case of $f(x)=\arctan(x)$ . But it can be explained since derivative of $\arctan(x)$ is $\frac{1}{1+x^2}$ and $\lim\limits_{x \to \infty}\frac{1}{1+x^2}=0$ which means, if we can prove other condition which is $\lim\limits_{x \to \infty}f'(x)\ne0$ then it would be complete proof.","['limits', 'calculus', 'derivatives']"
4503658,Convert the double integral $\int_{-T/2}^{T/2}\int_{-T/2}^{T/2}g(\alpha-\beta)d\alpha d\beta$ to a single integral with respect to $\tau=\alpha-\beta$,"Question I am self-studying signal processing and ran into the following double integral: $$\int_{-T/2}^{T/2}\int_{-T/2}^{T/2}g(\alpha-\beta)d\alpha d\beta$$ The YouTube video I am watching says this integral can be converted to a single integral using variable substitution with $\tau=\alpha-\beta$ . $$
\int_{-T/2}^{T/2}\int_{-T/2}^{T/2}g(\alpha-\beta)d\alpha d\beta=T\int_{-T}^T g(\tau)\left(1-\frac{|\tau|}{T}\right)d\tau
$$ but I'm having trouble proving it. Attempt Following this question , I tried variable substitution with $\tau=\alpha-\beta$ and $\phi=\alpha+\beta$ . Then \begin{align*}
\alpha&=\tau+\beta\\
\beta&=\alpha-\tau\\
\alpha&=\phi-\beta\\
\beta&=\phi-\alpha
\end{align*} so the Jacobian is $$
\frac{\partial (\alpha,\beta)}{\partial (\tau,\phi)}=
\left|\begin{matrix}
1 & -1\\
1 & 1
\end{matrix}\right|=2
$$ If I'm doing this correctly, the new integral would be $$
\int_{-T}^{T}\int_{-T}^{T}g(\tau)(2)d\tau d\phi
$$ But this doesn't seem to be correct. What is the best way to approach this integral?","['integration', 'multivariable-calculus', 'substitution']"
4503716,sigma-algebra on the image of a random variable,"Usually a random variable is considered to be a function $X: (\Omega, \sigma, P) \to (\mathbb{R}, B(\mathbb{R}))$ but I wonder what advantage choosing $B(\mathbb{R})$ as a sigma algebra on $\mathbb{R}$ has over using the pushforward of $\sigma$ under $X$ .","['pushforward', 'measure-theory', 'probability']"
4503721,Suppose $f$ is an entire function such that $f(0) = 1$ and $\int_0^{2\pi} |f(e^{i\theta})| d\theta = 2\pi$. Show that $f$ must be a constant function.,"This is a problem from a previous qualifying exam in complex analysis that I'm working through to study for my own upcoming exam. I'm looking for whether or not my proof is correct and if there are ways to improve or simplify it. Thanks! Problem: Suppose $f$ is an entire function such that $f(0) = 1$ and $\int_0^{2\pi} |f(e^{i\theta})| d\theta = 2\pi$ . Show that $f$ must be a constant function. Original Attempt: Since $f(z)$ is entire, consider Cauchy's Integral Formula $$
f(a) = \frac{1}{2\pi i} \oint_\gamma \frac{f(z)}{z-a} dz
$$ for $a=0$ and with $\gamma$ as the unit circle, which yields $$
f(0) = \frac{1}{2\pi i} \oint_\gamma \frac{f(z)}{z} dz = \frac{1}{2\pi i} \int_0^{2\pi} \frac{f(e^{i\theta})}{e^{i\theta}}\cdot ie^{i\theta} d\theta = \frac{1}{2\pi} \int_0^{2\pi} f(e^{i\theta}) d\theta = 1.
$$ Thus, we have $$
\int_0^{2\pi} f(e^{i\theta}) d\theta = 2\pi \quad \implies \quad \left\vert \int_0^{2\pi} f(e^{i\theta}) d\theta \right\vert = |2\pi| = 2\pi. 
$$ Now, by the ML inequality (or estimation lemma), we also have that on the unit circle $$
\left\vert \int_0^{2\pi} f(e^{i\theta}) d\theta \right\vert \leq \max_{|z|=1} \left\vert f(e^{i\theta}) \right\vert \cdot 2\pi.
$$ Since we already know that we actually have equality here, it must be that $$
\max_{|z|=1} \left\vert f(e^{i\theta}) \right\vert = 1.
$$ But by the Maximum Modulus Principle, if $f(z)$ attains its maximum inside of the region $\gamma$ , which we have at $f(0)=1$ , then the function must be constant. CURRENT ATTEMPT: Since $f(z)$ is entire, consider Cauchy's Integral Formula $$
f(a) = \frac{1}{2\pi i} \oint_\gamma \frac{f(z)}{z-a} dz
$$ for $a=0$ and with $\gamma$ as the unit circle, which yields $$
f(0) = \frac{1}{2\pi i} \oint_\gamma \frac{f(z)}{z} dz = \frac{1}{2\pi i} \int_0^{2\pi} \frac{f(e^{i\theta})}{e^{i\theta}}\cdot ie^{i\theta} d\theta = \frac{1}{2\pi} \int_0^{2\pi} f(e^{i\theta}) d\theta = 1.
$$ Thus, we have $$
\int_0^{2\pi} f(e^{i\theta}) d\theta = 2\pi. 
$$ and from the problem statement, $$
\int_0^{2\pi} |f(e^{i\theta})| d\theta = 2\pi.
$$ Subtracting yields $$
\int_0^{2\pi} \Big(|f(e^{i\theta})| - f(e^{i\theta})\Big) = 0 \quad \implies \quad |f(e^{i\theta})| = f(e^{i\theta})
$$ on the unit circle. Therefore, $0 < f(z)\in \mathbb{R}$ for $|z|=1$ . Then on the unit circle $f(z) = u(x,y)+iv(x,y) = u(x,y)$ since the imaginary part $v(x,y)=0$ . Since $f(z)$ is entire and thus analytic, the Cauchy Riemann equations hold and $$
\frac{\partial u}{\partial x} = \frac{\partial v}{\partial y}, \quad \frac{\partial u}{\partial y} = -\frac{\partial v}{\partial x} .
$$ Since $v(x,y)=0$ on the unit circle, $$
\frac{\partial u}{\partial x} = 0, \quad \frac{\partial u}{\partial y} = 0,
$$ thus $f(z)=u(x,y) = \alpha, \; \alpha \in \mathbb{R}$ ) and is constant on the unit circle $\gamma$ . Then $$
\alpha \int_0^{2\pi} d\theta = 2\pi \quad \implies \quad \alpha = 1.
$$ By the Maximum Modulus Principle, if $f(z)$ attains its maximum inside of the region $\gamma$ , which we have at $f(0)=1$ , then the function must be constant.","['complex-analysis', 'solution-verification']"
4503723,Find area of the largest possible equilateral triangle inscribed in a isosceles triangle,"What is the area of the largest equilateral triangle that can be inscribed in an isosceles triangle? There is a similar question asked before that might be helpful: The main problem with this is that sometimes is it unusable. I tried it and it doesn't work. So here is what I find out from playing around with GeoGebra $$
l_\max = \frac{3\sin\alpha}{\sin(2\alpha-60^\circ)}.
$$ The formula seems to only work if two of the angles are equal or greater than $60^\circ$ . But if we use it with a triangle where two of the angles are less than $60^\circ$ , it just doesn't work. Can someone explain why? Why the formula can't be applied to a triangle that
two of the angle are less than $60^\circ$ ? If so is there any general formula for this?","['triangles', 'trigonometry', 'geometry']"
4503757,Connection between probability distribution and generalized function,"I want to understand the connection between probability density function and generalized function. There are 3 common used generalized function classes: $\mathscr{E}'\subset\mathscr{S}'\subset\mathscr{D}'$ . They are dual spaces of $C^\infty\supset S \supset C_c^\infty$ under Lebesgue integral. (All in $\mathbb{R}^n$ ). Formally probability density should be seen as a measure (a distribution) $\mu$ . $\mu$ is the Lebesgue-Stieltjes measure associated with a random variable $X$ . $\mu$ is a measure on the space $(\mathbb{R}^n,\mathscr{B}_{\mathbb{R}^n})$ , where $\mathscr{B}$ denotes Borel algebra. They are very similar, but there are several essential differences. The generalized function is based on the Lebesgue integral, while probability distribution is based on Lebesgue-Stieltjes integral. They are doing integral in different measurable spaces (one contains all the Lebesgue measurable sets, another contains only Borel sets). If we regard $\mu$ as a functional (but I don't know how), is $\mu$ in any of $\mathscr{E}'$ , $\mathscr{S}'$ or $\mathscr{D}'$ ? $\mathscr{E}'$ , $\mathscr{S}'$ and $\mathscr{D}'$ don't seem to be nice sets to represent probability distribution, because continuity is not so important in probability. So are there some good ways to unify these 2 different things? So that we could apply most of the theorems developed in $\mathscr{E}'$ , $\mathscr{S}'$ or $\mathscr{D}'$ to a probability distribution.","['measure-theory', 'probability-distributions', 'lebesgue-integral', 'schwartz-space', 'probability']"
4503762,$A_{n\times n}$ be a complex matrix with $A^2 = -I$,"Let $A_{n\times n}$ be a complex matrix with $A^2 = -I$ . I want to prove that $A^3 = A^{-1}$ , and disprove that $I - A = A^{-1}$ . I do not know if the matrix is invertible, else, this will simplify the rest of the work. Suppose that its not invertible, and suppose that i cant deduce from $A^2 + I = 0$ that $A$ is in polynomial vector space, particularly for $x^2 + 1$ dimension. How can I prove the first part? For the second part: $(1)\quad I - A = A^{-1} / \cdot A$ $(2)\quad A^{-1} - A^2 = I /$ substitute the given argument $(3)\quad A^{-1} + I = I \quad \leftrightarrow \quad A^{-1} = 0$ Which is sort of ""reductio ad absurdum"" because the zero matrix is not invertible. Is this a correct proof? Any advice and help would be highly appreciated!","['matrices', 'linear-algebra']"
4503799,A function with non-vanishing derivative at a point gives a coordinate chart at the point.,"Let $M$ be a smooth manifold and let $p\in M$ . Suppose we have $f\in C^\infty(M)$ such that $df(p)\neq 0$ . Then there exists $U\ni p$ open and $x^i\in C^\infty(U)$ , $i\in \{2,\dots,n\}$ , such that $(U,(f,x^2,\dots,x^n))$ is a chart at $p$ . I think one can the use the rank theorem directly. That is, there exists a chart $(V,x)$ such that $f|_{V}=x^1$ . So $(V,(f,x^2,\dots,x^n))$ (the original chart) does the trick and we are done. Is this very short answer correct? Alternatively, I though of using another argument (similar to part of the proof of the rank theorem)
Take a chart $(V,x)$ and reorder coordinate so that $\partial_1 f(p)\neq 0$ . Define a map $\phi:V\to\mathbb R^n$ by $\phi(p)=(f(p),x^2(p),\dots,x^n(p))$ . Then we have $$D\phi(p) =\begin{bmatrix}
\partial_1 f(p)& \mathbf{0}\\
\mathbf 0& \delta^i_j
\end{bmatrix}
$$ Thus, $D\phi$ is non-singular at $p$ (it is just the identity), hence, by the inverse function theorem, it is local diffeomorphism over some $U\subset V$ . Thus, $(U,\phi=(f,x^2,\dots,x^n))$ is a chart. Is any of these two answers correct?
Thanks in advance!","['smooth-manifolds', 'riemannian-geometry', 'differential-geometry']"
4503801,Limit of $n^2$ and a recurrence relation with ceiling function,"For all positive integer $n$ we define a finite sequence in the following way: $n_0 = n$ , then $n_1\geq n_0$ and has the property that $n_1$ is a multiple of $n_0-1$ such that the difference $n_1 - n_0$ is minimal among all multiple of $n_0 -1 $ that are bigger than $n_0$ . More generally $n_k \geq n_{k-1}$ and has the property that is a multiple of $n_0-k$ such that the difference $n_k - n_{k-1}$ is minimal among all multiple of $n_0 -k $ that are bigger than $n_{k-1}$ . We stop the procedure when $k=n-1$ so that we define $f$ to be $f(n_0) = n_{n-1} $ . Question : What is the value of $\lim_{n \to \infty} \frac{n^2}{f(n)} $ ? I'm able to prove that $$ \frac{8}{3} \leq \lim_{n \to \infty} \frac{n^2}{f(n)} \leq 4 $$ but I can't do better. Someone has an idea? I think that the limit is $ \pi $ but don't know how to prove that. My idea is to prove that $$ f(n) = \frac{n^2}{\pi} + O(n) $$ the general term for $n_k$ is given by $$ n_k = (n-k) \left \lceil \frac{n_{k-1}}{n-k} \right \rceil $$ So that $$ n_k = (n-k) \left \lceil \frac{n-(k-1)}{n-k} \left \lceil \frac{n-(k-2)}{n-(k-1)}   \left \lceil \ldots \left \lceil \frac{n-1}{n-2} \left \lceil \frac{n}{n-1} \right \rceil \right \rceil \right \rceil \right \rceil \right \rceil $$","['limits', 'ceiling-and-floor-functions', 'pi']"
4503818,Why do calculus textbooks gloss over absolute values?,"The archetypal example of this is in the equation $\frac{dy}{dx}=y.$ When solving by separation, you end up reaching the result \begin{align*}
|y|&=C_1 e^x \\
\Rightarrow y&=C_2 e^x. \\
\end{align*} The justification for this step, according to a lot of sources, is the fact that the + or - from the absolute value can be combined with the constant to yield a new constant. However, this falls into the pointwise trap: what prevents the function from taking the positive branch at some points and the negative branch at others? In this case, making the function piecewise destroys continuity, violating the initial differential equation, but an extra step would be needed to rigorously confirm this result. A bigger problem arises in the case of differential equations such as $\frac{dy}{dx}\cdot \frac{1}{2}x=y,$ let's say with initial condition $(1,1).$ Again, this equation is quite easy to solve by separation of variables to yield the result $y=x^2$ as your solution - or is it? As it turns out, $y=\textbf{sgn}(x)\cdot x^2$ also satisfies the differential equation everywhere as well as the initial condition. This, to me at least, is alarming, since I've never seen these kinds of ""pathological"" solutions addressed in the solving of differential equations, and I can't find anything on the internet about this either, despite this being an extremely simple differential equation which has definitely appeared in textbooks or tests before. Even WolframAlpha ignores this solution and gives only $y=x^2.$ Is this something that is just commonly overlooked, or did teachers/authors think it wasn't important, or... what? I have yet to find a satisfying explanation. I have also asked my teacher, again with no satisfying answer.","['calculus', 'derivatives', 'ordinary-differential-equations']"
4503895,What is the significance of Hermitian forms on local rings?,"I am a first year math student at UNMSM, in Lima, Peru. My father is an 80-year-old man, a retired university professor, a Ph.D. in pure mathematics, and a passionate algebraist. He kept his mental faculties perfectly until a year ago when a stroke took away some parts of his memory and personality. His love for mathematics did not vanish instantly, but his interest in talking about it waned as he found it increasingly difficult to follow the ideas presented to him. He would usually refrain from making comments when I would share something that I find interesting about some math. He tells me ""sure yes or sure no, but who cares"" (pretty basic things, calculus or basic number theory, mostly). In the past he spoke passionately about ring theory, homological algebra, among other topics. He finds impressive how structures and theories like these exist and the beautiful theorems that are proven in theorems he proved in them. In particular, was on ""Hermitian forms on local rings"" . Could someone give me the significance and interesting results of this field so I can try having conversations with my dad again?",['abstract-algebra']
4503912,"Spivak, Ch. 18, Problem 36: Understanding solution manual ""easily"" showing $\frac{1}{b-a}\int_a^b \log{f}\leq\log(\frac{1}{b-a}\int_a^b f )$","The following problem is from Chapter 18 of Spivak's Calculus (a) Let $f$ be a positive function on $[a,b]$ and let $P_n$ be a partition of $[a,b]$ into $n$ equal subintervals. Use Problem 2-22 to
show that $$\frac{1}{b-a}L(\log{f},P_n)\leq\log\left(\frac{1}{b-a}L(f,P_n)\right )$$ (b) Use the Appendix to Chapter 13 to conclude that for all integrable $f>0$ we have $$\frac{1}{b-a}\int_a^b \log{f}\leq \log\left (\frac{1}{b-a}\int_a^b
 f\right )$$ The Appendix to Chapter 13 contains one theorem which says Suppose $f$ is integrable on $[a,b]$ . Then for every $\epsilon>0$ there is a $\delta>0$ such that if $P=\{t_0,...,t_n\}$ is a partition
of $[a,b]$ with $t_i-t_{i-1}<\delta$ for all $i$ then $$\left | \sum\limits_{i=1}^n f(x_i)\Delta t_i-\int_a^b f(x)dx\right |<\epsilon$$ which basically says that any Riemann sum can be made arbitrarily close to the integral (which is defined in terms of lower and upper sums being equal). The solution manual solution to part $(b)$ Theorem 1 shows that if $f$ is integrable then for every $\epsilon>0$ there is a $\delta>0$ such that $$\left | \sum\limits_{i=1}^n f(x_i)\Delta t_i-\int_a^b f(x)dx\right
 |<\epsilon/2$$ for any partition $P=\{t_0,...,t_n\}$ of $[a,b]$ , and choices $x_i$ in $[t_{i-1},t_i]$ , for which all $t_i-t_{i-1}<\delta$ . It is easy to
conclude that we then have $$\left |L(f,P)-\int_a^b f(x)dx \right |<\epsilon$$ for such partitions (we need to increase $\epsilon/2$ to $\epsilon$ since $m_i$ may not actually be $f(x_i)$ for any $x_i$ in $[t_{i-1},t_i]$ ). I'm not sure how the relationship between $\epsilon/2$ and $\epsilon$ was reached. Why do we increase $\epsilon/2$ to $\epsilon$ ? All I came up with was that since $$L(f,P)\leq \int_a^b f\leq U(f,P)$$ for all partitions $P$ , and by definition of integrability of $f$ $$\forall\epsilon, \exists\text{ partition } P, 0<U(f,P)-L(f,P)<\epsilon$$ then $$0\leq \int_a^b f - L(f,P)\leq U(f,P)-L(f,P)<\epsilon$$ $$|L(f,P)-\int_a^b f|<\epsilon$$ What does this have to do with Theorem 1? In particular $$\left |L(\log{f},P_n)-\int_a^b \log{f} \right |<\epsilon$$ $$\left |L(f,P_n)-\int_a^b f \right |<\epsilon$$ For $n$ sufficiently large. The desired result then follows easily
from part $(a)$ . I don't think I've ever been so frustrated with the use of the word ""easily"". How does this last step follow from part $(a)$ ?","['integration', 'logarithms', 'proof-explanation', 'calculus', 'derivatives']"
4504028,Reference request: positive first chern class of tangent bundle implies anticanonical line bundle is ample,"I am searching for a reference (preferably with a proof) for the following result: Let $X$ be a smooth projective curve, $T_X$ its tangent bundle, $K_X$ its canonical bundle. If $c_1(T_X) > 0$ , then $K_X^{-1}$ is ample.","['reference-request', 'vector-bundles', 'algebraic-geometry', 'characteristic-classes', 'line-bundles']"
4504047,Who will win in this choosing nested intervals game? (a.k.a. Banach‚ÄìMazur game),"Question Alice and Bob are playing a game. The rules are as follows: First Alice chooses a compact interval $A_1\subset\mathbb R$ (in this question, intervals contain more than one points, they are not allowed to choose single-point sets,) then Bob chooses a compact interval $B_1\subset A_1$ with $|B_1|\leq\frac12|A_1|$ (here $|I|$ means the length of the compact interval $I$ .) After that, Alice chooses a compact interval $A_2\subset B_1$ with $|A_2|\leq\frac12|B_1|$ and so on. Hence they get a sequence of nested compact intervals $$A_1\supset B_1\supset A_2\supset B_2\supset \cdots\supset A_n\supset B_n\supset \cdots,$$ with $\lim_{n\to\infty} |A_n|=\lim_{n\to\infty} |B_n|=0$ . By nested interval theorem , there is a unique $x\in\mathbb R$ such that $$\{x\}=\left(\bigcap_{n=1}^\infty A_n\right) \bigcap \left(\bigcap_{n=1}^\infty B_n\right).$$ If $x$ is rational, then Alice wins; if $x$ is irrational, then Bob wins. Decide whether there exists a winning strategy for either of them. Background and thoughts This problem is from my imagination. It is clear that for any $x\in\mathbb R$ , whether $x$ is rational or not, we can always find a sequence of nested intervals $I_n=\left[x-\frac1n,x+\frac1n\right]$ such that $\{x\}=\cap_n I_n$ . The key point is that, Alice and Bob are both very very clever, and they are free to change the center of the intervals to any numbers they want. For example, if Alice choose $A_1=[-1,1]$ , which is centered at $0\in\mathbb Q$ , then Bob can find $B_1\subset A_1$ such that $B_1$ is centered at an irrational number; and then Alice can find $A_2\subset B_1$ such that $A_2$ is centered at a rational number; and so on... I cannot determine who will win in this kind of procedure. It seems that Bob will win, because irrational numbers are much more than rational numbers. But I can't figure out a rigorous proof. Any help would be appreciated!","['real-numbers', 'analysis', 'real-analysis', 'game-theory', 'limits']"
4504076,Real forms of $\mathbb{G}_m$ as a variety,"Notation: Denote $k=\mathbb{R}$ , $K=\mathbb{C}$ and let $G:=\text{Gal}(K/k)=\{\text{id},\sigma\}$ , where $\sigma$ is complex conjugation. Background: In the Notices article A Gentle Introduction to Arithmetic Toric Varieties the author introduces (in slightly different notation) three varieties over $k$ : $\mathbb{G}_m=\{uv-1=0\}$ , $\text{S}^1=\{x^2+y^2-1=0\}$ , $Y=\{a^2+b^2+1=0\}$ . He establishes that they are pairwise non-isomorphic: e.g. looking at the real points in Euclidean topology $\mathbb{G}_m(k)$ has two connected components, while $\text{S}^1(k)$ has only a single connected component and $Y(k)=\emptyset$ has no $k$ -points at all. He then explains that $\mathbb{G}_m$ and $\text{S}^1$ are twisted forms of each other, since they become isomorphic (as affine $K$ -varieties) after base change to $K$ , explicitly: $$\varphi:\text{S}^1\times_kK\rightarrow\mathbb{G}_m\times_kK,\quad (x,y)\mapsto(u,v)=(x+iy,x-iy).$$ Explicitly the inverse morphism is given by: $$\varphi^{-1}:\mathbb{G}_m\times_kK\rightarrow\text{S}^1\times_kK,\quad (u,v)\mapsto(x,y)=\left(\frac{u+v}{2},\frac{u-v}{2i}\right).$$ As far as I see $Y$ also becomes isomorphic to $\mathbb{G}_m$ and $\text{S}^1$ after base change to $K$ , an isomorphism is given e.g. by: $$\psi:\text{S}^1\times_kK\rightarrow Y\times_kK,\quad (x,y)\mapsto(a,b)=(ix,iy),$$ $$\psi^{-1}:Y\times_kK\rightarrow\text{S}^1\times_kK,\quad (a,b)\mapsto(x,y)=(-ia,-ib).$$ This would indicate that the complex affine variety $\mathbb{G}_m\times_kK$ has (at least) 3 distinct real forms, namely $\mathbb{G}_m, \text{S}^1$ and $Y$ . Note that I do not consider the structure of a group variety, with which you could endow $\mathbb{G}_m, \text{S}^1$ in the usual way but not $Y$ as it is pointless (over $k$ ). As is well known for classes of varieties $X$ that form a ‚Äòstack‚Äô (e.g. quasi-projective varieties - so affine $X$ is OK) $k$ -forms of some $X\times_kK$ are in bijection to $$\text{Tw}(X)=\text{H}^1_{\text{gal}}(k,\text{Aut}_{\text{var}}(X\times_kK))=\text{H}^1_{\text{grp}}(\text{Gal}(K/k),\text{Aut}_{\text{var}}(X\times_kK))$$ (cf., e.g., Gille/Szamuely, ""Central Simple Algebras and Galois Cohomology"", Thm. 2.3.3 and sec. 5.2 for special versions). The author claims $$\text{Tw}(\mathbb{G}_m)=\{\mathbb{G}_m, \text{S}^1\}$$ only contains 2 distinct elements. And indeed using this post (and using that morphisms of affine varieties are in bijection to algebra morphisms) we see that $\text{End}_{\text{var}}(\mathbb{G}_m\times_kK)\cong\left\{a\cdot t^n:a\in K^{\times},n\in\mathbb{Z}\right\}$ and therefore $A:=\text{Aut}_{\text{var}}(\mathbb{G}_m\times_kK)\cong\left\{a\cdot t^n:a\in K^{\times},n\in\mathbb{Z}^{\times}=\{\pm 1\}\right\}\cong K^{\times}\times\mathbb{Z}/2\mathbb{Z}$ . The Galois action of $\text{G}$ on $A$ is induced by $\tilde{\sigma}:A\rightarrow A, (a,\pm 1)\mapsto(\sigma(a),\pm 1)=(\bar{a},\pm 1)$ using the identification with $K^{\times}\times\mathbb{Z}/2\mathbb{Z}$ just given. Using the standard method to compute group cohomology for cyclic groups through norm and difference maps, $N=\sum\limits_{i=0}^{2-1}\tilde{\sigma}^i=\tilde{\sigma}+\tilde{\text{id}}$ and $\Delta=\tilde{\sigma}-\tilde{\text{id}}$ (cf., e.g., Gille/Szamuely, Ex. 3.2.9), as $$\text{H}^1_{\text{grp}}(\text{G},A)\cong\text{ker}(N)/\text{im}(\Delta)$$ this can easily seen to indeed be $\cong\mathbb{Z}/2\mathbb{Z}$ . Question: Since $3>2$ there must be a mistake in these considerations: What is this mistake and what is $\text{Tw}(\mathbb{G}_m)$ ? Remark 1: As indicated above one could also look at the additional structure of group varieties. For this variant we have, if I am not mistaken, $B:=\text{Aut}_{\text{grp-var}}(\mathbb{G}_m\times_kK)\cong\left\{1\cdot t^n:n\in\mathbb{Z}^{\times}=\{\pm 1\}\right\}\cong \mathbb{Z}/2\mathbb{Z}$ which yields $\text{Tw}_{\text{grp-var}}(\mathbb{G}_m)=\text{H}^1_{\text{grp}}(\text{G},B)\cong\mathbb{Z}/2\mathbb{Z}$ . Since $Y$ has no $k$ -point it cannot have the structure of a group variety, thus $Y$ is no real form of the complex group variety $\mathbb{G}_m\times_kK$ and there is no contradiction from the above arguments. The issue therefore lies in classifying real forms of $\mathbb{G}_m\times_kK$ as only affine $k$ -varieties without any (potential) group structure. Remark 2: I choose the notation $K/k$ to indicate that analogous issues should also arise for e.g. $\mathbb{Q}(i)/\mathbb{Q}$ the Gaussian rationals and probably even in positive characteristic (probably $\neq 2$ , since $\varphi^{-1}$ requires inversion of $2$ ).","['algebraic-geometry', 'galois-cohomology']"
4504128,"Finding the maximum value of $f(x)=\frac{|x|-2-x^2}{|x|+1},x\in\mathrm R$","Question: Find the maximum value of $f(x)=\frac{|x|-2-x^2}{|x|+1},x\in\mathrm R$ My Attempt: I took two cases. One where $x\ge0$ , so, $|x|=x$ and another where $x\le0$ , so, $|x|=-x$ . Then I cross multiplied, made a quadratic in $x$ and wrote discriminant greater than equal to zero. Accordingly, I got range of $f(x)$ . I wonder if there is another way to solve this question, where we don't have to take two cases.","['quadratics', 'calculus', 'functions', 'derivatives']"
4504141,How to add the derivative of a matrix to the chain rule?,"In machine learning, I'm optimizing a parameter matrix $W$ . The loss function is $$L=f(y),$$ where $L$ is a scalar, $y=Wx$ , $x\in \mathbb{R}^n$ , $y\in \mathbb{R}^m$ and the order of $W$ is $m\times n$ . In all math textbooks, it is usually $$\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y}\frac{\partial y}{\partial x}=\frac{\partial L}{\partial y}W.$$ Where $\dfrac{\partial L}{\partial y}$ is a $1\times m$ vector. This is quite easy to understand. However, in machine learning, $x$ is the input and $W$ is the parameter matrix to optimize, it should be $$\frac{\partial L}{\partial W}=\frac{\partial L}{\partial y}\frac{\partial y}{\partial W}.$$ But what is $\dfrac{\partial y}{\partial W}$ ? Is it $x$ ? Is it correct? According to wikipedia, the derivative of a scalar to a matrix is a matrix \begin{equation*}
\frac{\partial L}{\partial W} = 
\begin{pmatrix}
\frac{\partial L}{\partial W_{11}} & \frac{\partial L}{\partial W_{21}} & \cdots & \frac{\partial L}{\partial W_{m1}} \\
\frac{\partial L}{\partial W_{12}} & \frac{\partial L}{\partial W_{22}} & \cdots & \frac{\partial L}{\partial W_{m2}} \\
\vdots  & \vdots  & \ddots & \vdots  \\
\frac{\partial L}{\partial W_{1n}} & \frac{\partial L}{\partial W_{2n}} & \cdots & \frac{\partial L}{\partial W_{mn}} 
\end{pmatrix}
\end{equation*} where $$\frac{\partial L}{\partial W_{ji}}=\frac{\partial L}{\partial y_j}\frac{\partial y_j}{\partial W_{ji}}=\frac{\partial L}{\partial y_j}x_i$$ therefore \begin{equation*}
\frac{\partial L}{\partial W} = 
\begin{pmatrix}
\frac{\partial L}{\partial y_1}x_1 & \frac{\partial L}{\partial y_2}x_1 & \cdots & \frac{\partial L}{\partial y_m}x_1 \\
\frac{\partial L}{\partial y_1}x_2 & \frac{\partial L}{\partial y_2}x_2 & \cdots & \frac{\partial L}{\partial y_m}x_2 \\
\vdots  & \vdots  & \ddots & \vdots  \\
\frac{\partial L}{\partial y_1}x_n & \frac{\partial L}{\partial y_2}x_n & \cdots & \frac{\partial L}{\partial y_m}x_n \\ 
\end{pmatrix}
\end{equation*} Does this even fit the chain rule? To fit the chain rule $$\frac{\partial L}{\partial W} = \frac{\partial L}{\partial y}\frac{\partial y}{\partial W}$$ $\dfrac{\partial L}{\partial W}$ is a $n*m$ matrix, $\dfrac{\partial L}{\partial y}$ is a $1\times m$ vector, how to fit it? PS: I just found there is an operation called kronecker product, and $\dfrac{\partial L}{\partial W}$ can be written as $\dfrac{\partial L}{\partial y}\bigotimes x$ , but this is still beyond me. First, why does the chain rule lead to kronecker product? Isn't the chain rule about matrix multiplication? Second, does this mean $\dfrac{\partial y}{\partial W} = x$ ? I didn't see the definition of the derivative of a vector to a matrix in wikipedia. The third and most important question is, even I know the derivative $\dfrac{\partial L}{\partial W}$ , how should I update my parameter matrix? We all know the gradient descent works because of directional derivative $$\nabla_v f = \frac{\partial f}{\partial v}v$$ so we should take the negative gradient direction to lower $f$ . Does this even exist for the derivative of a matrix? I mean $\dfrac{\partial L}{\partial W}$ multiplies $\Delta W$ won't reproduce $\Delta L$ anyway.","['backpropagation', 'multivariable-calculus', 'partial-derivative', 'derivatives', 'chain-rule']"
4504154,Finding an error while doing algebra,"While I was trying to simplify an equation, I came up with a sort of inconsistency in two identical expressions. I want to simplify the equation $$x=-\frac{a+b}{a-b}(x+y),~\textrm{where}~ a>0,b<0$$ in a way that I have $ax+by$ on the left hand side and a factor of $x+y$ on the right hand side. I tried two simple methods but they end up different answers and I want to know why. Firstly, we can multiply $a-b$ on both sides. So we have $$ax-bx=-ax-ay-bx-by,$$ which is the same as $$ax+by=-ax-ay=-a(x+y).$$ This is the desired answer. However, if I start by adding $ax+by$ on both sides, I should have $$ax+by=ax+by-x-\frac{a+b}{a-b}(x+y),$$ which is equivalent to $$ax+by=\left (a-1-\frac{a+b}{a-b}\right )x+\left (b-\frac{a+b}{a-b}\right )y.$$ To have $ax+by=-a(x+y)$ , it should be that $$a-1-\frac{a+b}{a-b}=-a~\textrm{and}~b-\frac{a+b}{a-b}=-a,$$ which does not seem to be correct statements. Could anyone spot my error in manipulating the equation?","['algebra-precalculus', 'solution-verification']"
4504173,How to find the solution to $4^x + 9^x = 4?$,"I tried to solve this equation algebraically but couldn't. Maybe there isn't a way to extract the solution from the equation algebraically. If it's the latter, I'd really like to know why. Also, is there a general solution to equations with the form $a^x + b^x = c$ where $a, b,$ and $c$ are all different real numbers? Thanks.","['calculus', 'algebra-precalculus']"
4504181,Confusion on using central limit theorem on uniform distribution,"In lectures we said that if $X$ is independently uniformly distributed, the CLI statest that: $$\lim_{n\rightarrow \infty}   (\frac{\sum_{n=1}^{i}(x_i - \mu)}{\sqrt{n}\sigma}) =^{d} N(0,1)$$ However, I don't understand, what is $\sqrt{n}$ doing there, because we also did an example of binomial distribution (let's call it $S_n$ ), and we said: $$\lim_{n\rightarrow \infty}   (\frac{S_n-np}{\sqrt{np(1-p)}}) =^{d} N(0,1)$$ From which I would conclude that the correct formula would be: $$\lim_{n\rightarrow \infty}   (\frac{Distr - expectation }{\sqrt{variance}}) =^{d} N(0,1)$$ What is correct and why?","['statistics', 'probability-distributions', 'central-limit-theorem', 'probability']"
4504191,"Without a calculator, determine whether chords $AB, AC \text{ and }BD$ can divide a circle into five equal-area regions.","Without a calculator, determine whether chords $AB, AC \text{ and }BD$ can divide a circle into five equal-area regions. Context: I just made up this question. I have only been able to answer the question with a calculator. Surely there must be some clever way to answer the question without a calculator. Answer that uses a calculator: Proof by contradiction: assume the answer is yes. Let radius $=1$ . Call the centre $O$ . $\alpha=\angle{AOB}$ $\beta=\angle{AOC}$ $\text{Area}_{\text{minor segment }AB}=\dfrac12(\alpha-\sin{\alpha})=\dfrac{\pi}{5}\implies \alpha\approx2.1131$ $\text{Area}_{\text{segment }ADC}=\dfrac12(\beta-\sin{\beta})=\dfrac{2\pi}{5}\implies \beta\approx2.8248$ $\text{Area}_{\text{triangle}}=\dfrac{\pi}{5}\text{ and }AB=2\sin{\dfrac{\alpha}{2}}\implies ...\implies \angle{BAC}=\arctan{\left(\dfrac{\pi}{5\sin^2{\dfrac{\alpha}{2}}}\right)}\approx{0.692}$ But $\angle{BAC}=\dfrac12{\angle{BOC}}=\dfrac12{\left(2\pi-\alpha-\beta\right)}\approx{0.673}$ , contradiction. Therefore the answer is no. UPDATE: I found essentially the same question , except it does not request a calculator-free solution. So it doesn't answer my question.","['area', 'circles', 'geometry']"
4504209,When does a real symmetric matrix have $LDL^{T}$ decomposition? And when is the $LDL^{T}$ decomposition unique?,"Suppose that $A$ is an $n \times n$ real symmetric indefinite matrix, ${\rm rank}(A) = k$ and $k \leq n$ . If the $LDL^{T}$ decomposition of $A$ exists, we denote it as $A=LDL^{T}$ , where $L$ is a lower unit triangular matrix and $D$ is a diagonal matrix. I have two questions about it: (1) What is the necessity and sufficiency of ""the $LDL^{T}$ decomposition of $A$ exists""? (2) What is the necessity and sufficiency of ""the $LDL^{T}$ decomposition of $A$ is unique""? I read the wiki and some books but I can't find the answer. I have an example as follows: \begin{align}
    A_1 &= 
    \begin{bmatrix}
        1 & 0 & 3 \\
        0 & 0 & 3 \\
        3 & 3 & 3 \\
    \end{bmatrix} {\rm\ is\ invertible\ and\ has\ no\ } LDL^{T} {\rm\ decomposition},\\
    A_2 &= 
    \begin{bmatrix}
        1 & 1 & 3 \\
        1 & 1 & 3 \\
        3 & 3 & 3 \\
    \end{bmatrix} =
    \begin{bmatrix}
        1 & 0 & 0 \\
        1 & 1 & 0 \\
        3 & 3 & 1 \\
    \end{bmatrix} 
    \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 0 & 0 \\
        0 & 0 & -6 \\
    \end{bmatrix} 
    \begin{bmatrix}
        1 & 1 & 3 \\
        0 & 1 & 3 \\
        0 & 0 & 1 \\
    \end{bmatrix}{\rm\ is\ singular}, \\
    A_3 &= 
    \begin{bmatrix}
        0 & 1 & 1 \\
        1 & 1 & 1 \\
        1 & 1 & 1 \\
    \end{bmatrix} {\rm\ is\ singular\ and\ has\ no\ } LDL^{T} {\rm\ decomposition}.\\
\end{align}","['matrices', 'linear-algebra', 'matrix-decomposition']"
4504213,Convert trigonometric function to irrational fraction [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question for example $\cos(\frac\pi6)$ is $\frac{\sqrt3}{2}$ . How can I convert any other trigonometric  function into this type of fraction and preferably without a calculator?",['trigonometry']
4504250,How many ways to arrange $n$ points in $(\Bbb F_q)^2$ with no three collinear?,"How many ways are there to arrange $n$ points in the finite field plane $(\Bbb F_q)^2$ with no three of the points collinear? An easy upper bound is $(q^2)^n=q^{2n}$ , but of course it's less than that. (Of course, if I asked the same question over $\Bbb R^2$ , it would be infinite.) The collinearity condition can be expressed as follows: $(a_0,b_0)$ , $(a_1,b_1)$ , and $(a_2,b_2)$ are collinear iff: $$\begin{vmatrix}1&1&1\\a_0&a_1&a_2\\b_0&b_1&b_2\end{vmatrix}=0$$ Thus the question makes sense over a finite field. The way to do this is probably some inclusion-exclusion argument (or perhaps some fancier poset M√∂bius argument) but I'm unsure on the details. I hope, perhaps unreasonably, that for fixed $n$ , the answer is a polynomial in $q$ .","['finite-fields', 'finite-geometry', 'inclusion-exclusion', 'combinatorics', 'mobius-inversion']"
4504317,How does $\sqrt{2+\sqrt{2}}+\sqrt{2-\sqrt{2}}$ become $\sqrt{2(2+\sqrt{2})}$?,"I'd like to know how can one simplify the following expression $$\sqrt{2+\sqrt{2}}+\sqrt{2-\sqrt{2}}$$ into $$\sqrt{2(2+\sqrt{2})}.$$ Wolfram alpha suggests it as an alternative form, and numerically it's easy to verify, but I can't find the right algebra to show they are indeed equivalent. Note I ran into this problem, trying to do: $2\cos(\pi/8)+2\sin(\pi/8)$ , where $$2\cos(\pi/8)=\sqrt{2+\sqrt{2}},$$ $$2\sin(\pi/8)=\sqrt{2-\sqrt{2}}.$$","['radicals', 'algebra-precalculus', 'trigonometry', 'nested-radicals']"
4504323,Show that $\lim_{x \to a}{P(x)} = P(a)$ for any polynomial $P(x)$,"In my calculus textbook I was given the following Problem: If $P(x)$ is a polynomial, show that $\lim_{x \to a}{P(x)} = P(a)$ . I found the following solution here , were a proof was given using induction. In my textbook, the problem was given in an chapter regarding limit laws, a precise definition is only given later in the following chapter and inductions weren't used up until then. So I decided to try solve this problem using only the limit laws. In that, I am taking these for granted without proving them. Solution: Per definition we have a polynomial $P(x) = \sum_{i=0}^n k_i x^i$ were each $k_i$ is a constant and $k, x \in \Bbb{R}$ . Thus $P(a) = \sum_{i=0}^n k_i a^i$ . Now, using the limit laws for sum, multiplication and power we can write $$
\lim_{x \to a}{P(x)}
 = \lim_{x \to a}{\sum_{i=0}^n k_i x^i}
 = \sum_{i=0}^n{\lim_{x \to a}k_i \cdot \left(\lim_{x \to a}x\right)^i}
 = \sum_{i=0}^n{k_i \cdot \left(\lim_{x \to a}x\right)^i}$$ and since $\lim_{x \to a}x = a$ we get $$
\lim_{x \to a}{P(x)}
 = \sum_{i=0}^n{k_i \cdot \left(\lim_{x \to a}x\right)^i}
 = \sum_{i=0}^n{k_i a^i}
 = P(a)
$$ $\blacksquare$ Is my reasoning correct? I am not sure, if I can use these manipulations on the sum without using induction. Thanks in advance for any comments and answers.","['continuity', 'calculus', 'solution-verification', 'polynomials', 'limits']"
4504351,Reconciling two versions of Taylor's Theorem,"Lemma $1.4$ (Taylor's theorem with remainder). Let $f$ be a $C^{\infty}$ function on an open subset $U$ of $\mathbb{R}^{n}$ star-shaped with respect to a point $p=\left(p^{1}, \ldots, p^{n}\right)$ in $U$ . Then there are functions $g_{1}(x), \ldots, g_{n}(x) \in C^{\infty}(U)$ such that $$
f(x)=f(p)+\sum_{i=1}^{n}\left(x^{i}-p^{i}\right) g_{i}(x), \quad g_{i}(p)=\frac{\partial f}{\partial x^{i}}(p) .
$$ I am not used to this form of Taylor's theorem. Previously, I have seen in the single variable case that there exists a point between the approximation and the point we know for which is equal to the difference between the Taylor polynomial and the function. 5.15 Theorem Suppose $f$ is a real function on $[a, b], n$ is a positive integer, $f^{(n-1)}$ is continuous on $[a, b], f^{(n)}(t)$ exists for every $t \in(a, b)$ . Let $\alpha, \beta$ be distinct points of $[a, b]$ , and define $$
P(t)=\sum_{k=0}^{n-1} \frac{f^{(k)}(\alpha)}{k !}(t-\alpha)^{k} .
$$ Then there exists a point $x$ between $\alpha$ and $\beta$ such that $$
f(\beta)=P(\beta)+\frac{f^{(n)}(x)}{n !}(\beta-\alpha)^{n} .
$$ How can I reconcile these two viewpoints? is the $\frac{f^{(n)}(x)}{n!}$ what we take to be our $g$ in the single variable case? In the second, I haven't thoguht to view the remainder as a smooth function I've taken for granted that there is some constant which gives you equality. Is there something about the remainder in the second theorem which is $C^\infty$ ?","['manifolds', 'calculus', 'analysis', 'real-analysis']"
4504371,Use the definition of limit to prove that $\lim_{x \to 0} \frac{x}{x+1}=0$,"Use the definition of limit to prove that $\lim_{x \to 0} \dfrac{x}{x+1}=0$ My attempt: Let $\epsilon >0$ then I need to find $\delta>0$ such that if $0<|x|<\delta$ then $\left| \dfrac{x}{x+1} \right| < \epsilon$ What I was thinking is that, to get a $\delta$ that satisfies the definition, I should see what $\epsilon$ satisfies $|x|<\epsilon |x+1|$ and then I could define $\delta$ in terms of such an $\epsilon$ . If $\epsilon =1$ then: $0\leq|x|<|x+1|$ implies that $x^2 < x^2+2x+1$ and then $x>-\dfrac{1}{2}$ . In particular, if $-\dfrac{1}{2}<x<\dfrac{1}{2} \Rightarrow |x|<\dfrac{1}{2} \Rightarrow \dfrac{1}{2} < x+1<\dfrac{3}{2}$ so that $|x+1|=x+1$ and then $\dfrac{1}{2}<|x+1| \Rightarrow \dfrac{1}{2|x+1|}<1$ . Since $|x|<\dfrac{1}{2}$ , then $\left| \dfrac{x}{x+1}\right| = \dfrac{|x|}{|x+1|} <\dfrac{1}{2|x+1|}<1$ .
So $\delta=\dfrac{\epsilon}{2}$ stisfies the definition. If $\epsilon \neq 1$ then: $0 \leq |x|<\epsilon |x+1|$ Implies that: $x^2<{\epsilon}^2(x^2+2x+1) \Rightarrow (\epsilon ^2-1)x^2+2 \epsilon ^2x + \epsilon ^2 > 0$ . If $0< \epsilon <1 \Rightarrow \epsilon^2-1<0 \Rightarrow x^2 + \dfrac{2 \epsilon^2}{\epsilon^2-1}x+\dfrac{\epsilon^2}{\epsilon^2-1}<0 \Rightarrow \left(x+ \dfrac{\epsilon}{\epsilon-1} \right) \left(x+ \dfrac{\epsilon}{\epsilon+1} \right) < 0$ Let $ x_1 = -\dfrac{\epsilon}{\epsilon-1}$ , $x_2=-\dfrac{\epsilon}{\epsilon+1}$ . We see that $x_1 >0>x_2$ . Then solving the inequality for $x$ we would have $x_2=-\dfrac{\epsilon}{\epsilon+1} < x < -\dfrac{\epsilon}{\epsilon-1} = x_1$ . I am expecting to get something like: $-\phi(\epsilon) < x < \phi(\epsilon) \Rightarrow |x|< \phi(\epsilon)$ on each case so that $\delta = \phi(\epsilon)$ satisfies the definition, but I don't know how to continue here. Could someone give me a hint? Maybe there is a simpler way to approach the exercise.","['limits', 'calculus', 'epsilon-delta', 'real-analysis']"
4504412,"Deceptively simple problem to find expected number of seconds it takes a particle to exit $(-1, 1)$","I've been working through past MIT Primes problems, and got stuck on 2021 Problem M4: A particle is initially on the number line at a position of $0$ .  Every second, if it is at position $x$ , it chooses a real number $t \in [‚àí1, 1]$ uniformly and at random, and moves from $x$ to $x+t$ . Find the expected value of the number of seconds it takes for the particle to exit the interval $(‚àí1, 1)$ . I've been trying to solve it differently from the solution they provided. Let me outline my approach. Let $X$ denote the number of seconds it takes the particle to exit $[-1, 1]$ . Then, we are looking for $$
\text{E}[X] = \sum_x x \Pr(X=x)
$$ The PMF of $X$ is $$
\Pr(X = x) = \Big( 1 - \Pr(-1 < X_x < 1) \Big) \prod_{i=1}^{x-1} \Pr(-1 < X_i < 1)
$$ where $X_t$ is $X$ at time $t$ . We have now punted the problem to finding $\Pr(-1 < X_i < 1)$ for all $i$ . Remember, each $X_i$ is dependent on $X_{i-1}$ . We can make a step towards finding $\Pr(-1 < X_i < 1)$ using the law of total probability: $$
\Pr(X_i \leq x) = \int_{-\infty}^\infty \Pr(X_i \leq x \mid X_{i-1}=y) f_{X_{i-1}}(y) \ \text{d}y
$$ $\Pr(X_i \leq x \mid X_{i-1}=y)$ can be defined as a piecewise function (I derived this by fixing $y$ , and then considering $x$ ): $$
\Pr(X_i \leq x \mid X_{i-1}=y) =
\begin{cases}
\frac{x+1}{2+y} & -1 \leq y < 0 \; \land \; x \leq y + 1 \\
1 & -1 \leq y < 0 \; \land \; x > y + 1 \\
\frac{x-y+1}{2-y} & 0 \leq y < 1 \; \land \; x \geq y - 1 \\
0 & 0 \leq y < 1 \; \land \; x < y - 1
\end{cases}
$$ We also know that $f_{X_{i-1}}(y)$ is the derivative of $\Pr(X_{i-1} \leq y)$ . Because $\Pr(X_1 \leq x) = (x + 1)/2$ , we can now find $\Pr(X_2 \leq x)$ . Now here is the problem. I evaluated $\Pr(X_2 \leq x)$ using the following Mathematica script: (* the piecewise function *)
XiCDFconditional[x_, y_] = Piecewise[{
    {(x + 1) / (2 + y), -1 <= y < 0 && x <= y + 1},
    {1, -1 <= y < 0 && x > y + 1},
    {(x - y + 1) / (2 - y), 0 <= y < 1 && x >= y - 1},
    {0, 0 <= y < 1 && x < y - 1}
}];
X1CDF[x_] = (x + 1) / 2;
(* Pr(X2 <= x) *)
X2CDF[x_] = Integrate[XiCDFconditional[x, y] * X1CDF'[y], {y, -Infinity, Infinity}] when I execute X2CDF[1] - X2CDF[-1] (i.e. $\Pr(-1 \leq X_2 \leq 1)$ ), I get $1$ , which is obviously incorrect. Where did I screw up? I feel like I might have subtly screwed up $\Pr(X_i \leq x \mid X_{i-1}=y)$ . Or maybe I didn't screw up, and I'm almost surely (pun intended) getting confused in the continuous world.","['cumulative-distribution-functions', 'expected-value', 'stochastic-processes', 'problem-solving', 'probability']"
4504426,Proof that the set of equivalence classes of a relation on a set form a partition of that set.,"I was attempting to prove the following proposition. Proposition: Let $A$ be a non-empty set. Let $I$ be an indexing set. Let $R$ be an equivalence relation on $A$ . Prove that the set of all equivalence classes of $R$ form a partition of $A$ . In other words, show that for any two equivalence classes $A_j, A_k$ with $j, k\in I$ and $A_j\ne A_k$ we have that $A_j\cap A_k=\emptyset$ and $\bigcup_{i\in I}A_i=A$ . The following is my attempt at a proof. Proof: Suppose that $A_j\cap A_k\ne \emptyset$ . Then there exist $a_1,\dots,a_n\in A_k$ such that $a_1,\dots,a_n\in A_j$ . Because $R$ is an equivalence relation, from the transitive property of $R$ we have that $a_1,\dots,a_n \sim y$ for all $y\in A_k$ . Because $a_1,\dots,a_n\in A_j$ , we also see that every $y\in A_k$ is an element of $A_j$ . This also follows from the transitive property of $R$ . Thus, $A_k \subset A_j$ . By similar argument we see that $A_j\subset A_k$ . Showing that $A_j=A_k$ which contradicts the hypothesis that $A_j\ne A_k$ . Hence, $A_j\cap A_k=\emptyset$ . To show that $\bigcup_{i\in I}A_i=A$ . Note that for all $x\in A$ we have that $x$ is in some equivalence class $A_j$ with $j\in I$ . We also see that $x$ cannot be in another equivalence class $A_k$ as that would lead to $A_j$ being equal to $A_k$ . Thus, every element $x\in A$ is in a unique equivalence class $A_m$ . Hence, $\bigcup_{i\in I}A_i=A$ . Is the proof correct?","['elementary-set-theory', 'equivalence-relations', 'solution-verification', 'set-partition']"
4504441,"Assume $f(x)$ is such that $f'(x)>0$, $s(x)$ only takes two values $1$, $-1$ on $R$. If $g(x)=s(x)f(x)$ is differentiable, can one show $s$ constant?","Assume that $f(x): x\in \mathbb{R}$ differentiable, such that $f'(x)\neq 0$ for every point $x$ such that $f(x) = 0$ , and $s(x)$ only takes two values $1$ and $-1$ on $\mathbb{R}$ . If $g(x)=s(x)f(x)$ is differentiable, can one show that $s(x)$ is constant on $\mathbb{R}$ ? This question arises from a former question of mine . edit: I now think the appropriate statement of the original problem is as follows: Assume that $f(x): x\in \mathbb{R}$ differentiable, such that $f'(x)\neq 0$ for every point $x$ such that $f(x) = 0$ , and $s(x)$ only takes two values $1$ and $-1$ on $\mathbb{R}$ . If $g(x)=s(x)f(x)$ is differentiable, then $s(x)$ is continuous on $\mathbb{R}$ outside a set of removable discontinuous points $S$ , where $S$ is a subset of zeros of $f(x)$ . Both QC_QAOA's answer and mine actually establish the above fact. To provide an example contradicting the original statement: $f(x)=x$ , $s(x)$ takes $1$ for all $x \in \mathbb{R}$ but $-1$ for $x=0$ . Then $x=0$ is a removable discontinuous point of $s$ , and it doesn't sabotage the fact that $s(x)g(x)$ is still differentiable.","['continuity', 'calculus', 'derivatives']"
4504471,Integer solutions to $2^n - n = m^2$?,"Is $(n,m) = (7,11)$ the only solution in positive integers to the equation $2^n - n = m^2\text{?}\tag*{}$ It's not hard to verify by direct calculation that there are no other solutions for $n < 10000$ but that's no way to establish a general result. I'm aware that $n=7$ is also a solution to the Ramanujan-Nagell equation , $2^n - 7 = m^2$ . It also seems that $(n,m) = (5,3)$ may be the only solution in positive integers to the equation $2^n - n = m^3.$ So for a bigger challenge, one could consider all solutions in positive integers to the equation $2^n - n = m^k.\tag*{}$ (Slight correction, thanks to Chickenmancer): there's also the trivial solution $(n,m) = (1,1)$ which solves both the original equation and the generalized one for all $k$ .)","['exponential-diophantine-equations', 'number-theory', 'diophantine-equations']"
4504475,How to prove this (corollary of) hyperplane separation theorem?,"$X$ is a nonempty convex subset of $\mathbb{R}^n$ whose element is $x=\left(x_1,...,x_n\right)$ . The theorem is as follows. If for each $x\in X$ , there is an $i \in \left\{1,...,n\right\}$ such that $x_i>0$ ,
then there exists $\left(\lambda_1,...,\lambda_n\right)$ where $\lambda_i \geqslant 0$ for all $i$ and $\sum_{i=1}^n \lambda_i=1$ , such that $\lambda \cdot x \geqslant 0$ for all $x\in X$ and $\lambda \cdot x>0$ , for some $x \in X$ . I was wondering how to prove it. It looks like it is a corollary of the hyperplane separation theorem. "" $\geqslant 0$ for all $x$ and $>0$ for some $x$ "" is a little bit weird and I do not know whether there is a version of the hyperplane separation theorem that has this form and can be applied to prove it.","['convex-analysis', 'linear-algebra', 'real-analysis']"
4504499,Convergence of derivatives near the boundary of an open interval,"Suppose there are two continuously differentiable functions $a(x_1)$ and $b(x_2)$ , where $x_1, x_2\in \mathbb{R}_+$ . I assume that $a'(x_1) >0$ if $x_1<x^o$ and $a'(x_1)<0$ if $x_1>x^o$ . It follows that $a'(x^o)=0$ and $x^o$ is the point that gives the maximum of $a(x_1)$ . Similarly, I assume $b'(x_2) >0$ if $x_2<x^p$ and $b'(x_2)<0$ if $x_2>x^p$ . It follows that $b'(x^p)=0$ and $x^p$ is the point that gives the maximum of $b(x_2)$ . I assume $x^p > x^o$ and take the open interval $(x^o, x^p)$ . Suppose $\exists x^q \in (x^o, x^p): |a'(x_1)|>b'(x_2) \forall x_1 \in (x^o, x^p) $ and $\forall x_2 \in (x^q, x^p)$ . I would like to then say that $|a'(x^c)| = b'(x^q)$ ,  where $x^c$ is a point infinitesmally close to $x^o$ such that $|a'(x^c)| >0$ . The problem is there is always a point closer to $x^o$ than $x^c$ . How would I say that $|a'(x_1)|$ and $b'(x_2)$ converge at $x_1 = x^c$ and $x_2 = x^q$ , without negating all the values of $x_1$ that exist between $x^o$ and $x^c$ ?","['optimization', 'convergence-divergence', 'derivatives', 'infinitesimals']"
4504529,Side of the largest possible cube inside a cone?,"Question: What is the side of the largest possible cube inside a cone of height $12$ units and radius $3\sqrt 2$ units? Now, when this question came up (in my class), I instantly thought to myself that $1^{st}$ I must check how the cube must be placed. With the limited time I had, I drew the following cases: I reasoned to myself that any intermediate variations must ultimately settle at these $2$ supposedly extremities. Let the figure on the right be called $R$ and that on left, $L$ . Then by looking at the diagram itself, I concluded that since $R$ makes better use of the extended width of the cone down below, so it must be the largest possible cube that one may fit inside the cone. But my teacher directly proceeded with $L$ and no one else minded it so I questioned, to which, I was told by teacher that $L$ IS better. Thus, here. Please help. Finally, using similar triangles and the common vertex angle in $L$ , we arrived at $4$ cm for the side of the cube (in $L$ ).",['geometry']
4504579,"Evaluate $\int_0^{2\pi} \frac{\sin t}{a^2+k^2t^2}\, dt$","I need to compute $$ \int_\gamma \frac{dx}{x^2+y^2+z^2},$$ where $\gamma$ is the first turn of the helix $\gamma(t) = (a\cos t, a\sin t, kt),$ that is, $t \in [0, 2\pi].$ I don't see any option other than using the definition, which yields $$ \int_0^{2\pi} \left\langle \left(\frac{1}{a^2\cos^2 t+a^2\sin^2 t+k^2t^2}, 0, 0 \right), \left(-a\sin t, a \cos t, k \right) \right\rangle \,dt = \int_0^{2\pi} \frac{-a\sin t}{a^2+k^2t^2}\,dt.$$ But I don't know how to compute this integral. This exercise is supposed to be analytically solvable. Thanks a lot.","['multivariable-calculus', 'vector-analysis']"
4504583,"Why a measure over the points of a billiard which reflect infinitely often in finite time vanish, i.e. why $\int_{N\cap M}f(x^{-})d\mu_1(x^{-1})=0$","The crux of my question is that why $\mu(N^{(2)}) = 0$ when $N^{(2)}$ consists all points of a billiard that reflect infinitely often in finite time under a given flow (i.e. transformation) function. This is taken from the book Ergodic Theory by Cornfeld, Fomin and Sinai, Chapter 6 ""Billiards"", pages  138-142. I apologize in advance for the length of this question. You can always skip to the main question at the end. Definitions: Suppose that $Q_0$ is a closed smooth Riemann manifold which is possibly noncompact. Suppose that $r$ smooth functions $f_i, i = 1,\dots,r$ are given on $Q_0$ . Suppose $\Gamma_q$ for $q \in f_i^{-1}(0)$ is the tangent space to $f_i^{-1}(0)$ at the point $q$ . By $n(q)$ denote the unit normal vector to $\Gamma_q$ directed inside $Q$ . Denote by $M_0$ the unit tangent bundle over $Q_0$ so that the points of $M_0$ are of the form $x = (q, v), q \in Q_0, v \in S^{d - 1}, d = \mathrm{dim}(Q)$ . Suppose that $\pi:M_0\to Q_0$ is the natural projection $\pi(q, v) = q, x = (q, v) \in M_0$ . Define $M = \pi^{-1}(Q)$ . Note: While the authors do not say it explicitly, I am assuming that $\left<n(q), x\right>$ for $x \in M_0$ is defined implicitly as $\left<n(q), x\right> = \left<n(q), (q, v)\right> \equiv \left<n(q), \pi(q, v)\right>  = \left<n(q), q\right>$ . Furthermore define the sets $$\begin{cases}Q &= \{q \in Q_0\mid f_i(q) \geq 0, i = 1,\dots,r\}\\\
\partial Q_i &= f_i^{-1}(0)\cap Q, i = 1,\dots,r\\
\tilde{Q}_i &= \partial Q_i\setminus \cup_{k\neq i}\partial Q_k, i = 1,\dots,r\\ \partial \tilde{M}_i &= \pi^{-1}(\partial \tilde{Q}_i), i = 1,\dots,r\\ \partial \tilde{M} &= \bigcup_{i}\partial \tilde{M}_i\\M_1 &= \{x \in \partial \tilde{M}\mid \left<n(q), x\right> > 0, q = \pi(x)\}\\
\end{cases}$$ ( Measures ) Define the measure $\mu$ on $M_0$ by putting $d\mu = d\rho(q)d\omega_q$ , where $d\rho(q)$ is the volume element in $Q_0$ generated by the Riemann metric, $\omega_q$ is the Lebesgue measure on the $(d - 1)$ -dimensional sphere $S^{d-1}(q) = \pi^{-1}(x)$ . With this formula we have that for any Borel set $A \subset M, \mu(A) = \int_{Q_0}d\rho(q)\int_{A\cap S^{d-1}(q)}d\omega_q(x)$ . The same letter $\mu$ will denote the restriction of this measure to $M$ . Furthermore it will be assumed that $\mu$ is normalized on $M$ . Define also the measure $\mu_1$ on $\partial \tilde{M}$ by $d\mu_1(x) = d\rho_i(q)d\omega_q\left|\left<n(q), x\right>\right|, x \in \partial \tilde{M}_i$ and $d\rho(q)$ is the volume element induced by the Riemann metric on $\partial Q_i$ ( Geodesics ) Suppose a vector field $X$ generates a geodesic flow on $M_0$ , where $X(x)$ is the tangent vector to $M_0$ at $x \in M_0$ . By letting the same symbol $X$ denote the restriction of the vector field to $M$ , $X$ determines the motion of our point ( $x$ ) with unit velocity along geodesic lines in $M$ . Take any $x \in \mathrm{int}(M)\setminus \cup_{i\neq j}N_{i, j}$ where $\mathrm{int}(M)$ denotes the set of interior points of $M$ . According to the authors, there are two possible cases. 1.) The geodesic line constructed in the direction $x$ does not intersect the boundary $\partial Q$ ; 2.) The end point of a geodesic segment of some finite length $s$ is located at a point $q \in \bigcup_{i}\partial \tilde{Q}_i$ . ( Movement ) A point $x$ intersecting the boundary $\partial Q$ is reflected according the "" incidence angle equals reflection angle"" rule, i.e. $y' = y - 2\left<n(q), y\right>\cdot n(q)$ where $y$ is the tangent vector obtained from $x$ by parallel translation along the geodesic to the end point (i.e. the point of contant with $\partial Q$ ) of a segment. Suppose that $N_{i, j}$ is the set of all interior points $x\in M$ such that the segment of the geodesic line constructed in the direction $x$ intersects $\partial Q$ on $\partial Q_i\cap \partial Q_j$ . ( The set $N^{(2)}$ ) Denote by $N^{(2)}$ the set of points $x$ for which this construction leads to infinite number of reflections in finite time. Note: The authors do not really define what this means, so I am presuming that "" infinite number of reflections in finite time "" can mean e.g. that the number of reflections of a point $x$ is equal to $\frac{1}{C - t}$ at time $t < C < \infty$ . ( The point $x^{-}$ ) Let $x \in \mathrm{int}(M)$ . Denote by $x^{-}$ the nearest point in $\partial M$ to $x$ on the flow (i.e. billiard) trajectory so that $x = T^{\tau}x^{-}$ for some $\tau > 0$ . ( The function $f(x)$ ) Before the lemma below, the first instance at which the authors refer to a function $f(x)$ (note no subscript) is when they define the transformation $T_1$ in the following way: Define a one-parameter group of transformations $\{T^t\}$ on $M$ by setting $T^tx, x \in M, t \in \mathbb{R}$ equal to the tangent vector obtained by a translation of $x$ along the trajectory which it determines by a distance $t$ . [...] The billiards $\{T^t\}$ are directly related to the transformation $T_1$ of the set $M_1$ defined in the following way: consider the geodesic segment in the direction of $x$ with origin $q = \pi(x)$ and end point at the first intersection with the boundary and reflect the tangent vector from the boundary at the end of the segment. The vector $y$ thus obtained will be put equal to $T_1x$ . Clearly if $x$ is in the set where $\{T^t\}$ is defined, then $T_1x = T^{f(x)}x$ , where $f(x)$ is the length of the geodesic segment. Question: Consider the following lemma Lemma $\mu(N^{(2)}) = 0$ . Proof: It is clear that if $x = T^{\tau}x^{-}$ and $x \in N^{(2)}$ , then all the points $x' = T^tx^{-}$ , $0 \leq t \leq f(x^{-})$ , belong to the set $N^{(2)}$ . Therefore $\mu(N^{(2)}) = \int_{N^{(2)}\cap M_1}f(x^{-})d\mu_1(x^{-})$ . But the last integral vanishes since $N^{(2)}\cap M_1$ consists of all the points $x$ for which the sums $\sum_{k=0}^{n-1}f(T^k_1 x)$ remain bounded when $n\to \infty$ . The lemma is proved. $\square$ What I don't understand is why "" the sums $\sum_{k=0}^{n-1}f(T^k_1x)$ remain bounded "" $\implies$ "" the integral $\int_{N^{(2)}\cap M_1}f(x^{-})d\mu_1(x^{-})$ vanishes "". But gut feeling for the implication is some combination of ergodicity theorems which force the integral to vanish or else some contradiction is met. But as the presented proof does not refer to any particular point in the book itself (or really to anything for that matter) I don't know how to begin to improve my intuition about the problem.","['measure-theory', 'billiards', 'riemannian-geometry', 'ergodic-theory', 'dynamical-systems']"
4504599,How do we formally and precisely specify a directed angle?,"I haven't been able to find a precise explanation of what a directed angle is. I encountered the concept in Chapter 15 of Spivak's Calculus , where he simply says In elementary geometry an angle is simply the union of two half-lines
with a common initial point. More useful for trigonometry are
""directed angles"", which may be regarded as pairs $(l_1,l_2)$ of
half-lines with the same initial point. I googled and found this document , which sort of sheds a bit of light. We specify a directed angle as an ordered pair $(\vec{OA}, \vec{OB})$ plus a direction. However, they specify the direction with a picture, as in Also, apparently it is by convention that if the direction is counterclockwise, then the angle is positive, and if clockwise then it is negative. When I think about it, in common usage, it seems we say $50^{\circ}$ or $-70^{\circ}$ , and this is a measure of magnitude of an angle. We associate the magnitude with a particular angle. We know that if the magnitude is negative, for example, then the angle is being measured counterclockwise. Is this in fact the notation to specify a directed angle, ie the magnitude? I found the following definition of a directed angle here Definition: Given any two non-parallel lines $l$ and $m$ , we defined
the directed angle $\angle(l,m)$ to be the measure of the angle
starting from $l$ and ending at $m$ , measured counterclockwise. Then Notice that $$\angle(l,m)+\angle(m,l)=180^{\circ}\tag{1}$$ holds universally. This is kind of nice, but it's a bit annoying to
have that $180^{\circ}$ lying around there, and so we will also the
all angle measures modulo $180^{\circ}$ . That means that $-70^{\circ}=110^{\circ}=290^{\circ}=...$ Once we take
mod $180^{\circ}$ , $(1)$ becomes the following very important result Proposition: For any lines $l$ and $m$ , $$\angle(l,m)=-\angle(m,l)\tag{2}$$ (In other words, measuring the angle clockwise instead of
counterclockwise corresponds to negation). I'm not sure I follow the calculations. I'm not too familiar with modular arithmetic. We have $$180 \mod 180=0$$ Thus $$[\angle(l,m)+\angle(m,l)] \mod 180=0$$ But as far as I can tell $$[\angle(l,m)+\angle(m,l)] \mod 180=\left [[\angle(l,m)\mod 180]+[\angle(m,l)\mod 180]\right ] \mod 180=0$$ How do we obtain $(2)$ ? Also, are these calculations general in the sense that they explain why when we measure angles in a clockwise direction they are negative? I would guess not. Is the reason rather just convention? The convention being that when we specify a directed angle as an ordered pair $(l,m)$ , by default we mean counterclockwise?","['angle', 'geometry', 'calculus', 'trigonometry', 'derivatives']"
4504614,Solving $ \frac{r^{200}-1}{r^{199}-1} = \frac{\alpha}{\beta}$ in a question on Geometric Progressions,"Question: Let $a_n$ be the $n^{th}$ term of a geometric progression of positive numbers. Let $$\sum_{n=1}^{200} a_n = \alpha$$ and let $$\sum_{n=1}^{199} a_n = \beta$$ such that $\alpha \neq \beta$ . Then find the common ratio of the GP. My solution: $a_1 + a_2 + a_3 + ... + a_{200} = \alpha$ $a_1 + a_2 + a_3 + ... + a_{199} = \beta$ $\implies a_{200} = \alpha - \beta = a_1r^{199}$ Now, $a_1(1+r+r^2+r^3+...+r^{199}) = \alpha$ $a_1(1+r+r^2+r^3+...+r^{198}) = \beta$ $$\implies \frac{a_1(1-r^{200})}{1-r} = \alpha, \frac{a_1(1-r^{199})}{1-r} = \beta$$ $$\implies \frac{r^{200}-1}{r^{199}-1} = \frac{\alpha}{\beta}$$ After this step I am not able to find $r$ in terms of $\alpha$ and $\beta.$ I tried everything from cross-multiplying to trying to expand the terms(by reducing powers), componendo-dividendo etc. Please suggest me a way to solve it. Any help would be highly appreciated!","['algebra-precalculus', 'geometric-series', 'sequences-and-series']"
4504696,Regular Representation of infinite groups,"I am aware of the importance of the regular representation of finite groups, but I am curios if there is an analogous notion for infinite groups. How is the regular representation of a infinite group defined? Does it decompose as the sum of all irreducible representations? Can we generalise this to a regular module of an algebra?","['infinite-groups', 'representation-theory', 'modules', 'abstract-algebra', 'algebras']"
4504715,"Prove $f(x,y)= x^2 + y^2$ is convex function","I have trouble proving that $f(x,y)= x^2 +y^2$ is a convex function with the definition . I know that sum of convex functions is a convex function. But, I am confused, could we use this theorem when the variable is not the same? I mean when we know $f(x)=x^2$ and $g(x)=|x|$ is convex function so $h(x)=x^2+|x|$ is convex function. But when $f(x)=x^2$ and $f(y)=y^2$ convex function could we say that $f(x,y)=x^2+y^2$ is a convex function? Note : I am prohibited from proving this with the Hessian.","['convex-optimization', 'functions', 'convex-analysis']"
4504723,Does intersection of subgroups preserve the property of being generated by transpositions?,"I'm reading a book and the author seems to just assume this proposition at one point, but upon trying it myself I don't see why it's the case, even though I can't find a counterexample. Statement: Let $U = U_1 \cap U_2$ , where $U_1, U_2 \leq S_n$ , the symmetric group, and $U_1, U_2$ are generated by transpositions. Then $U$ is also generated by transpositions. The author assumes it in the case where $U_1$ is generated by transpositions of the form $(i, i+1)$ , so at least that case should be true. I don't really know where to start with this problem.
Writing $g = s_1s_2\cdots s_k = t_1t_2 \cdots t_l$ , where $s_i \in U_1, t_i \in U_2$ are transpositions, I don't really know how to continue in order to find an expression of $g$ in terms of transpositions in $U_1 \cap U_2$ . A more general version of the statement that I also don't know whether it's true is: Statement: Let $U = U_1 \cap U_2$ , where $U_1, U_2 \leq G$ , some larger group, and let $T$ be a subset of $G$ . Suppose $U_1$ is generated by $U_1\cap T$ and $U_2$ is generated by $U_2\cap T$ . Then $U$ is generated by $U_1\cap U_2 \cap T$ . I don't see how to start with this one either.","['group-theory', 'abstract-algebra']"
4504860,1956 Miklos Schweitzer Counting - Problem 10,"Source : 1956 Miklos Schweitzer Contest (a Hungarian undergraduate open-note math contest) Problem 10 In an urn there are balls of $N$ different colours, $n$ balls of each colour. Balls are drawn and not replaced until one of the colours turns up twice; denote by $V_{N,n} $ the number of the balls drawn and by $M_{N,n}$ the expectation of the random variable $v_{N,n}$ . Find the limit distribution of the random variable $\frac{V_{N,n}}{M_{N,n}}$ if $N \to \infty$ and $n$ is a fixed number. Attempt (wrong) : My thought process is that I'd 1. find the distribution of $V_{N,n}$ , 2. compute $\mathbb{E}[V_{N,n}]$ , and 3. apply a change of variables to get the distribution in question $Y=\frac{V_{N,n}}{\mathbb{E}[V_{N,n}]}$ , and 4. justify the limiting distribution (I assume not too bad if we fix $n$ ). We can justify the density by counting. If we desire $P(V_{N,n}=k)$ , then this event occurs if we select $k-1$ colors from $N$ possibilities, $\binom{N}{k-1}$ , and we select one ball from each of the $k-1$ colors for the first $k-1$ draws, $\binom{n}{1}^{k-1}$ , and finally we select the last ball to be one of the $k-1$ colors already chosen, $\binom{k-1}{1}$ . In summary, we have for $k\in\{2,\dots,N+1\}$ $$\mathbb{P}\left(V_{N,n}=k\right) = \frac{\binom{N}{k-1}n^{k-1}(k-1)}{\binom{Nn}{k}}.$$ From here, I thought oh maybe I'd apply some combinatorial identity like hockey-stick, Pascal's, the binomial theorem, look at generating functions or something similar to compute the expectation: $$\mathbb{E}\left[V_{N,n}\right]=\sum\limits_{k=2}^{N+1} \frac{\binom{N}{k-1}n^{k-1}(k-1)}{\binom{Nn}{k}}\cdot k.$$ However, I have no idea how to deal with the bottom - I assume I could apply Stirling's to get an approximation, but is there something exact here? Even if step 2 failed, I try to continue - by change of variables, we have $$\mathbb{P}\left(Y=y\right)=\mathbb{P}\left(V_{N,n}=\mathbb{E}[V_{N,n}]y\right)\cdot\mathbb{E}[V_{N,n}]=\frac{\binom{N}{\mathbb{E}[V_{N,n}]y-1}n^{\mathbb{E}[V_{N,n}]y-1}(\mathbb{E}[V_{N,n}]y-1)}{\binom{Nn}{\mathbb{E}[V_{N,n}]y}}\cdot \mathbb{E}[V_{N,n}].$$ I can obviously not obtain a closed form without finding the expectation; however, it is evident that $\mathbb{E}[V_{N,n}]\propto N$ and $\mathbb{E}[V_{N,n}]\propto n^{-1}$ . Thus as $N\rightarrow\infty$ , ... uh never mind, there's nothing I think we could deduce. Edit 1 (old method) : With Raskolnikov's help, I now have, after some manipulation, $$\mathbb{E}[V_{N,n}]=n(n-1)\sum\limits_{k=2}^N \frac{\binom{N}{k-1}n^{k-2}(k-1)}{\binom{Nn}{k}}+\frac{n^{N-1}(N+1)}{\binom{Nn-1}{N-1}}.$$ This looks a little more interesting - I really wanted to get the sum on the left to look like the derivative of the sum if I differentiate with respect to $n$ , but the denominator is still troubling me. I thought maybe Vandermonde's would be useful $$\binom{Nn}{k}=\sum\limits_{k_1+\cdots+k_n=k} \binom{N}{k_1}\cdots\binom{N}{k_n},$$ but that also seems to lead to a dead end I think. Edit 2 (new method) : Okay, with Raskolnikov's new method, I think it suffices to show the distribution of $V_{N,n}$ is exponential for some rate $r\in(0,\infty)$ . With this approach, I tried the following (setting $n=2$ WLOG) $$\mathbb{P}(V_{N,n}>k)=\frac{2^{k-1}\binom{N-1}{k-1}}{\binom{2N-1}{k-1}}$$ $$= 2^k\frac{(2N-k)\cdots(N-k+1)}{2N\cdots(N+1)}$$ $$= 2^k\left(1-\frac{k}{2N}\right)\cdots\left(1-\frac{k}{N+1}\right)$$ $$=^*\left(1-\frac{rk}{N}\right)^N\rightarrow e^{-rk}\;\;\text{as}\;\; N\rightarrow\infty$$ but I'm blanking on the choice of $r$ to get $=^*$ to hold (or if it's even possible). Question : I would appreciate some help for evaluating the limit in the new edited section. Thanks!","['contest-math', 'probability-distributions', 'combinatorics', 'probability-theory', 'probability']"
4504876,Confused by Rosen's explanation of Proof by Contradiction,"Suppose we want to prove that a statement $p$ is true. Furthermore, suppose that we can find a contradiction $q$ such that $\neg p \rightarrow q$ is true. Because $q$ is false, but $\neg p \rightarrow q$ is true, we can conclude that $\neg p$ is false, which means that $p$ is true. Chapter 1, page 86 of K.H. Rosen's Discrete Mathematics and Its Applications (7th ed) How can we show $ \neg p \rightarrow q$ is true? We'd have to show that if $\neg p$ is true, then $q$ is true (because this rules out the only case where the implication is false) -  but the latter is impossible, as a contradiction is always false. Maybe we don't need to show that the above implication is true?! Instead, can we argue that the above implication is true due to the fact that we've only used valid arguments to arrive at $q$ ? If so, how exactly can we formalizes this argument? To summarize my question: How does Rosen conclude that the implication $\neg p \rightarrow q$ is true?","['proof-explanation', 'propositional-calculus', 'logic', 'discrete-mathematics']"
4504921,Analogs of $\sum_{n\ge1}\frac{n^{13}}{e^{2\pi n}-1}=\frac{1}{24}$,"Context: As I and others were able to show in the answers to this question , we have that $$\sum_{n\ge1}\frac{n^{13}}{e^{2\pi n}-1}=\frac{1}{24}.$$ In my answer, I let $$E_{2k}(\tau)=1+c_{2k}\sum_{n\ge1}\frac{n^{2k-1}q^n}{1-q^n},$$ with $\tau\in\Bbb H$ and $q=e^{2\pi i\tau}$ and $c_{2k}=\frac{(2\pi i)^{2k}}{(2k-1)!\zeta(2k)}$ , be the Eisenstein series of weight $2k$ , where $k\in\Bbb Z_{>2}$ . Using the well known property that $$E_{2k}(-1/\tau)=\tau^{2k}E_{2k}(\tau),\tag1$$ it is easy to see that $$S_{2k-1}(e^{2i\pi/\tau})-\tau^{2k}S_{2k-1}(e^{-2i\pi\tau})=\frac{\tau^{2k}-1}{c_{2k}},\tag 2$$ where $$S_{\ell}(q)=\sum_{n\ge1}\frac{n^\ell}{q^n-1},$$ since $$E_{2k}(\tau)=1+c_{2k}S_{2k-1}(e^{-2i\pi\tau}).\tag3$$ Using the values $k=7$ and $\tau=i$ in $(2)$ , we get $$S_{13}(e^{2\pi})-i^{14}S_{13}(e^{2\pi})=\frac{i^{14}-1}{c_{14}},$$ which reduces to $$S_{13}(e^{2\pi})=\sum_{n\ge1}\frac{n^{13}}{e^{2\pi n}-1}=\frac{1}{24}.$$ My Problem: I am trying to find identities analogous to $S_{13}(e^{2\pi})=1/24$ using the same general method. It is well known that $(1)$ is a the special case of $$E_{2k}\left(\frac{a\tau+b}{c\tau+d}\right)=(c\tau+d)^{2k}E_{2k}(\tau),\tag4$$ corresponding to the choice $\begin{pmatrix}a & b \\ c & d\end{pmatrix}=\begin{pmatrix}0 & -1 \\ 1 & 0\end{pmatrix}$ . The formula $(4)$ is true for all $\begin{pmatrix}a & b \\ c & d\end{pmatrix}\in\text{SL}_2(\Bbb Z)$ . I was hoping of using $(4)$ instead of $(1)$ to generate identities analogous to the one in the title of this question. To this end, I denote $\gamma=\begin{pmatrix}a & b \\ c & d\end{pmatrix}$ and $\gamma(\tau)=\frac{a\tau+b}{c\tau+d}$ . Then using $(3)$ , we have $$S_{2k-1}(e^{-2i\pi\gamma(\tau)})-(c\tau+d)^{2k}S_{2k-1}(e^{-2i\pi\tau})=\frac{(c\tau+d)^{2k}-1}{c_{2k}}.$$ Then suppose we find $\tau\in\Bbb H$ such that $\gamma(\tau)=\tau$ . This would give $S_{2k-1}(e^{-2i\pi\gamma(\tau)})=S_{2k-1}(e^{-2i\pi\tau})$ and thus $$S_{2k-1}(e^{-2i\pi\tau})=-\frac{1}{c_{2k}}.\tag{*}$$ There is an infinite family for such $\tau\in\Bbb H$ , each corresponding to a unique element of $\text{SL}_2(\Bbb Z)$ . Using these I am able to find things like $$\sum_{n\ge1}\frac{n^{13}}{\zeta^ne^{n\pi\sqrt3/7}-1}=\frac1{24},\qquad \zeta=(-1)^{-5/7}, \gamma=\begin{pmatrix}-2 & 1 \\ -7 & 3\end{pmatrix},\tag5$$ $$\sum_{n\ge1}\frac{n^{13}}{\zeta^ne^{2n\pi/5}-1}=\frac{1}{24},\qquad \zeta=(-1)^{-2/5},\gamma=\begin{pmatrix}2 & 1 \\ -5 & -2\end{pmatrix},\tag6$$ and others. NOTE: I am unable to numerically test identities $(5),(6)$ because I only have desmos, which doesn't do complex numbers. So I am relying on the theory which I have laid out. My Question: The keen among you may have noticed that the formula $(*)$ does not hold for all values of $k\in\Bbb Z_{>2}$ . Indeed, we have all the values listed here which show that the values of $S_{2k-1}(e^{-2i\pi\tau})$ for certain fixed $\tau$ differ from $-1/c_{2k}$ for certain values of $k$ but not for others. My question is, why does this happen. For example, if we plug in $k=6$ and $\tau=\frac{1+i\sqrt3}{2}$ into $(*)$ we should get $$\sum_{n\ge1}\frac{n^{11}}{(-1)^ne^{n\pi\sqrt3}-1}=-\frac{691}{65520},$$ but actually, $$\sum_{n\ge1}\frac{n^{11}}{(-1)^ne^{n\pi\sqrt3}-1}=\frac{189\Gamma\left ( \frac{1}{4}  \right )^{24} }{272629760\pi^{18}}-\frac{691}{65520}.$$ Why does this happen? Where does that $\Gamma$ term come from? Why doesn't $(*)$ work for all $k\in\Bbb Z_{>2}$ and $\gamma\in\text{SL}_2(\Bbb Z)$ when there is a solution $z=\tau\in\Bbb H$ to $\gamma(z)=z$ ? I figure it has something to do with elliptic curves and elliptic integrals, but I don't know enough of the theory to see it.","['elliptic-curves', 'real-analysis', 'complex-analysis', 'analytic-number-theory', 'modular-forms']"
4504934,"is there any explicit injective function $f:\mathbb{P}_{2}(\mathbb{N})\longrightarrow \mathbb{N}$, such that $f$ is a polynomial in two variables?","Let $\mathbb{P}_{ 2}(\mathbb{N})=\{A\subset \mathbb{N}:|A|= 2\}$ , is there any explicit injective function $f:\mathbb{P}_{2}(\mathbb{N})\longrightarrow \mathbb{N}$ , such that $f$ is a polynomial in two variables? I have tried with $f(\{x,y\})= xy+y+x$ , however this function is not injective: $f(\{x,y\})=x+y+xy=f(\{0,x+y+xy\})$","['elementary-set-theory', 'combinatorics']"
4504968,Strong Induction vs Weak Induction,"I'm currently taking a discrete math course and came over strong vs weak induction. I conceptually understand the two and understand that one can always use strong induction, however using weak induction might be easier or vice versa (depending on the question). My question is this: Is there a general rule of thumb that makes a question easier to solve using strong induction? Thanks in advance!","['formal-proofs', 'induction', 'discrete-mathematics']"
4505014,"How to find the minimum value of $\sum _{i=1}^n\min (a_i,2i-1)$?","Let $n$ be a positive integer, $a_1,a_2,\ldots ,a_n$ is a permutation of $1,2,\ldots ,n$ , try to find the minimum value of $\sum _{i=1}^n\min (a_i,2i-1)$ . I have programmed and calculated the situations of $n$ from $1$ to $8$ , and found that the minimum value $\left \lceil \frac{n(n+1)}{3}\right \rceil$ ( This answer is my guess. I put the first eight items into OEIS, and then guess it should be the right answer. After all, it is very concise. ) can be obtained when the  permutation is $n,n-1,\ldots ,2,1$ . But I can not  prove it, and I want to know how many permutations can take the minimum value when taking $n$ .",['combinatorics']
4505018,limit $\lim_{k \to \infty} \int_{0}^{1} \frac{kx^k}{1+x} dx$,"Consider the following sequence of functions and their integrals on $[0,1]$ . Evaluate the limit of the following integral if possible $$\lim_{k \to \infty} \int_{0}^{1} \frac{kx^k}{1+x} dx$$ At first glance, I do not know if the limit exists or not. Wolfram alpha will not take the limit for me, but it did evaluate the integral. The result is terms of a function I do not understand, but from looking at the series expansion: $$x^k \left(\frac{kx}{k+1} - \frac{kx^2}{k+2} + \frac{kx^3}{k+3} - \dots\right)$$ it appears as though the limit is $0$ since $x \in [0,1]$ . However, I have attempted the problem two different ways, and I am getting that the result is infinity. I am curious where I maybe going wrong. Note: I am not looking for an answer. I just want to see where my logic is wrong (mainly in my 2nd attempt) so that I can try to make the correction. Attempt 1: I can pull the $k$ outside the integral and get $$\lim_{k \to \infty} k \int_{0}^{1} \frac{x^k}{1 + x} dx$$ . Now, $|\frac{x^k}{1+x}| \leq \frac{1}{1+x} = g(x)$ since we are on $[0,1]$ . Since $g(x)$ is integrable, and dominates $f(x)$ , the dominated convergence theorem tells me I can pass the limit inside. $$\int_{0}^{1} \lim_{k \to \infty} \frac{kx^k}{1+x} dx = \infty $$ due to the factor of $k$ . Now, outside of the fact that I have not done measure theory for quite some time, I am uncertain about this for two reasons. One, DCT refers to Lebesgue integrable functions. I am always hesitant when I use it during a discussion of Riemann integration. Secondly, while I am definitely allowed to pull the $k$ outside of the integral, I am not sure I am allowed to apply DCT on the result without $k$ , and then put the $k$ back inside along with the limit. If I had to dominate $\frac{kx^k}{1 + x}$ , I am not sure it would work. These uncertainties made me want to try to brute force this with Riemann integration. Attempt 2: I will rewrite the integral as $$\lim_{k \to \infty} k \int_{0}^{1} \frac{x^k}{(\sqrt{1 + x})^2}dx$$ so that I can apply Trigonometric substitution. I would get $\sec(\theta) = \sqrt{1 + x}$ $\tan(\theta) = \sqrt{x}$ $\tan^2(\theta) = x$ $2\tan(\theta)\sec^2(\theta)d\theta = dx$ Substituting, $$\lim_{k \to \infty} k \int \frac{\tan^{2k}(\theta)}{\sec^2(\theta)} 2 \tan(\theta) \sec^2(\theta) d\theta$$ Cleaning this up, $$\lim_{k \to \infty} 2k \int \tan^{2k+1}(\theta) d\theta$$ Now, I can pull out 2 of the tangents. $$\lim_{k \to \infty} 2k \int \tan^{2k - 1}(\theta) \tan^2(\theta)d\theta$$ Using the identity $\tan^2(\theta) = \sec^2(\theta) - 1$ , $$\lim_{k \to \infty} 2k \int \tan^{2k-1}(\theta) \sec^2(\theta) d\theta - \lim_{k \to \infty} 2k \int \tan^{2k-1}(\theta) d\theta$$ The first integral can be done with $u$ -substitution. Let $u = \tan(\theta)$ . Then, $du = \sec^2(\theta) d\theta$ $$\lim_{k \to \infty} 2k \left[\frac{u^{2k}}{2k}\right] - \lim_{k \to \infty} 2k \int \tan^{2k-1}(\theta) d\theta$$ Now, I can back substitute. $$\lim_{k \to \infty} 2k \left[\frac{\tan^{2k}(\theta)}{2k}\right] - \lim_{k \to \infty} 2k \int \tan^{2k-1}(\theta) d\theta$$ Again, $$\lim_{k \to \infty} 2k \left[\frac{x^k}{2k}\right]_{0}^{1} - \lim_{k \to \infty} 2k \int \tan^{2k-1}(\theta) d\theta$$ Evaluating, $$\lim_{k \to \infty} 2k \frac{1}{2k} - \lim_{k \to \infty} 2k \int \tan^{2k-1}(\theta) d\theta$$ At this point, I can say the first limit is $1$ . Now, I still have an odd power of tangent in the second integral. So, I can repeat this process again $$1 - \lim_{k \to \infty} 2k \int \tan^{2k - 3}(\theta) \sec^2(\theta) d\theta + \lim_{k \to \infty} 2k \int \tan^{2k - 3}(\theta) d\theta$$ Then, $$1 - \lim_{k \to \infty} 2k \left[\frac{u^{2k-2}}{2k-2}\right] + \lim_{k \to \infty} \int \tan^{2k - 3}(\theta)$$ Again, $$1 - \lim_{k \to \infty} 2k \left[\frac{\tan^{2k-2}(\theta)}{2k-2}\right] + \lim_{k \to \infty} \int \tan^{2k - 3}(\theta)$$ Then, $$1 - \lim_{k \to \infty} 2k \left[\frac{x^{k-1}}{2k-2}\right]_{0}^{1} + \lim_{k \to \infty} \int \tan^{2k - 3}(\theta)$$ Evaluating, $$1 - \lim_{k \to \infty} 2k \frac{1}{2k-2} + \lim_{k \to \infty} \int \tan^{2k-3}(\theta) d\theta$$ Then, this limit is also $1$ $$1 - 1 + \lim_{k \to \infty} 2k \int \tan^{2k-3}(\theta) d\theta)$$ So, $$\int \tan^{2k-3}(\theta) d\theta)$$ Well, this is very nice. I have ended up with a single integral with an odd power of tangent. So, I can repeat these two steps over and over until I am left with. $$\lim_{k \to \infty} 2k \int \tan(\theta) d\theta$$ after going through all of the powers. Well, this integral I know $$\lim_{k \to \infty} 2k \left[- \ln|\cos(\theta)| \right]$$ Going back to our trig sub, we can see that $$\cos(\theta) = \frac{1}{\sqrt{1 + x}}$$ . Therefore, $$\lim_{k \to \infty} 2k \left[ -\ln|\frac{1}{\sqrt{1+x}}|\right]_{0}^{1}$$ Evaluating, $$\lim_{k \to \infty} -2k\ln|\frac{1}{\sqrt{2}}| = \infty$$ This matched the result I got before, and I will much better about this because I just used straight up integration rather than a sophisticated tool that I don't know very well, the dominated convergence theorem. My uncertainty lies in that the answer does not appear to be $\infty$ based on the power series wolfram alpha gave. Does anyone see where I am going wrong?","['sequence-of-function', 'real-analysis', 'trigonometric-integrals', 'limits', 'riemann-integration']"
4505025,Questions about differential equation $xf''(x) + 3x[f'(x)]^2 = 1 - e^{-x}$,"I have some doubts regarding the following problem from Apostol's Calculus I book (problem 31 from section 8.28). I posted my solution attempt below. Can anyone please verify the solution and give me some hints concerning the doubts? Problem Given a function $f$ which satisfies the differential equation $$xf''(x) + 3x[f'(x)]^2 = 1 - e^{-x}$$ for all real $x$ . (a) If $f$ has an extremum at a point $c \neq 0$ , show that this extremum is a minimum. (b) If $f$ has an extremum at $0$ , is it a maximum or a minimum? (c) If $f(0) = f'(0) = 0$ , find the smallest constant $A$ such that $f(x) \leq Ax^2$ for all $x \geq 0$ . Solution (a) If $f$ has an extremum at a point $c \neq 0$ , then $f'(c) = 0$ . Plugging this into the differential equation, we get: $$cf''(c) = 1 - e^{-c} \implies f''(c) = \dfrac{1 - e^{-c}}{c}$$ By cases: If $c > 0$ , then $1 - e^{-c} > 0$ , so $f''(c)$ is positive. Otherwise, if $c < 0$ , then $1 - e^{-c} < 0$ , and so $f''(c)$ is positive. Either way, the second derivative, $f''(c)$ , is positive. Therefore, $f$ has a minimum at $c$ . (b) If $x \neq 0$ , we can divide both sides of the differential equation by $x$ to obtain: $$f''(x) + 3[f'(x)]^2 = \dfrac{1}{x} - \dfrac{1}{xe^x}$$ Consider the limit of the above expression as $x \to 0$ . If $f$ has an extremum at $0$ , then we have $f'(0) = 0$ , so the limit of the left-hand side expression is $\lim_{x \to 0} [f''(x) + 3[f'(x)]^2] = \lim_{x \to 0} f''(0)$ . Furthermore, the limit of the right-hand side expression is: $$\begin{aligned}
    \lim_{x \to 0} \left( \dfrac{1}{x} - \dfrac{1}{xe^x} \right) &= \lim_{x \to 0} \left( \dfrac{e^x - 1}{xe^x} \right) \\
    &= \lim_{x \to 0} \left( \dfrac{e^x}{e^x + xe^x} \right) & \text{(L'H√¥pital)} \\
    &= \lim_{x \to 0} \left( \dfrac{1}{1 + x} \right) = 1
\end{aligned}$$ Therefore, we have that $f''(0) \to 1$ as $x \to 0$ . Since this is a positive value, we can conclude that $f$ has a minimum at $0$ . DOUBT : Above, I am concluding that $f''(0)$ has a positive value based on the fact that $f''(0) \to 1$ . However, I think that this depends on assuming that $f''$ is continuous at $0$ . How can I justify this assumption? (c) DOUBT : I couldn't figure this one out. Any hint? EDIT : For item (c) , here is the beginning of an initial attempt. The Taylor expansion of $f(x)$ near $x = 0$ is: $$f(x) = f(0) + f'(0)x + f''(0)\dfrac{x^2}{2} + o(x^2)$$ We are given that $f(0) = f'(0) = 0$ . Also, from item (b), we concluded that $f''(x) \to 1$ as $x \to 0$ . If $f''(0)$ is continuous at zero (again: how can I justify this claim?), then we have $f''(0) = 1$ . So: $$f(x) = \dfrac{x^2}{2} + o(x^2)$$ If we can show that the remainder of this expansion is $\leq 0$ for all $x > 0$ , we will have $f(x) \leq x^2/2$ for all $x > 0$ , and so $A = \frac{1}{2}$ . Is this correct, and is there any way to proceed from here?","['calculus', 'ordinary-differential-equations']"
4505032,"If $x \mapsto \int_A f(x-t) \, d\mu(t)$ vanishes identically, do we have that $\mu=0$?","Suppose that $A \subset \mathbb R$ is a compact set and let $\mu$ be a complex regular Borel measure on $A$ . Further, let $f : \mathbb R \to \mathbb C$ be a non-zero function with a certain decay (for instance, assume that $f$ is a Schwartz function). Further, assume that $$
\int_A f(x-t) \, d\mu(t) = 0 \ \ \forall x \in \mathbb R.
$$ I want to conclude that $\mu = 0$ . My approach would be to take the Fourier transform of the map $$
x \mapsto \int_A f(x-t) \, d\mu(t)
$$ which gives $\hat f \hat \mu = 0$ and then use that $\hat f$ does not vanish identically and $\hat \mu$ is entire. Hence $\hat \mu = 0$ which gives $\mu=0$ . My problem is that I don't know if all these steps are justified since I'm working with an arbitrary complex regular Borel measure.","['complex-analysis', 'measure-theory']"
4505036,"Length relation between middle point, incircle touching point, angle bisector foot and altitude foot. $DE^2=DF\cdot DP$","I have the following exercise. In the $\triangle ABC$ , $D$ is the middle point of $AB$ , $P$ is the foot of the altitude $CP$ on $AB$ , the angle bisector $CF$ intersecting $AB$ at $F$ , $E$ is the touching point of the incircle on $AB$ . Prove that $$
DE^2=DF\cdot DP
$$ I can prove it by computing all segments: $DB=\frac{c}{2}$ , $BP=a\cdot\frac{b^2-a^2-c^2}{2ac}=\frac{b^2-a^2-c^2}{2c}$ , $BF=\frac{ac}{a+b}$ , so $DF=DB-BF=\frac{c(b-a)}{2(b+a)}$ , $DP=DB+BP=\frac{b^2-a^2}{2c}$ , $DE=\frac{c}{2}-(p-b)=\frac{b-a}{2}$ . Hence, $$
DE^2=DF\cdot DP
$$ But I want to see how to prove this by a geometrical way. Let $DG$ be the other tangent line from $D$ to the incircle. So $DG=DE$ . From this resulting identity, I need to show $\triangle DGP\sim\triangle DFG$ . I only know that $\angle DGF=\angle GTS$ . So it suffices if I show $ST\parallel DP$ . But I could not prove $ST\parallel DP$ . Do I miss some properties?","['triangles', 'geometry']"
4505053,Covariant Derivative of Matrix Times Vector,"Good evening! I was wondering if one could prove some sort of ""product rule"" for the covariant derivative by using parallel transport: Let $(M,g)$ be a Riemannian manifold, then it is known that a connection $\nabla$ is uniquely characterized by its parallel transport via $$(\nabla_\xi X)_x = \left.\frac{d}{d t}\right|_{t=0} P^\gamma_{t,0}(X(\gamma(t))) = \lim\limits_{t\rightarrow 0} \frac{P_{t,0}^\gamma (X(\gamma(t))) - X(x)}{t},$$ where $P_{t,0}^\gamma$ denotes the (reverse) parallel transport of a vector field $X\in \Gamma TM$ along the geodesic $\gamma:\mathbb{R}\longrightarrow M$ with $\gamma(0)=x$ and $\dot\gamma(0)=\xi$ for a $\xi\in T_x M$ , $x\in M$ . Now, for a $J\in \Gamma\operatorname{End}(TM)$ , my idea was to insert a useful zero like in the proof of the regular product rule: \begin{align*}(\nabla_\xi JX)_x &= \lim\limits_{t\rightarrow 0} \frac{P_{t,0}^\gamma (J(\gamma(t))X(\gamma(t))) - J(x)X(x)}{t}\\
&= \lim\limits_{t\rightarrow 0} \frac{P_{t,0}^\gamma (J(\gamma(t))X(\gamma(t))) - J(x)P_{t,0}^\gamma(X(\gamma(t)))+J(x)P_{t,0}^\gamma(X(\gamma(t))) -J(x)X(x)}{t}\\
&= \lim\limits_{t\rightarrow 0} \frac{[P_{t,0}^\gamma (J(\gamma(t))X(\gamma(t))) - J(x)P_{t,0}^\gamma(X(\gamma(t)))]+J(x)[P_{t,0}^\gamma(X(\gamma(t))) -X(x)]}{t}\\
&= \lim\limits_{t\rightarrow 0} \frac{[P_{t,0}^\gamma (J(\gamma(t))X(\gamma(t))) - J(x)P_{t,0}^\gamma(X(\gamma(t)))]}{t}+J(x)(\nabla_\xi X)(x)
\end{align*} However, even though the right term is exactly what one wants, I don't see how the term on the left can be brought into the form $$\lim\limits_{t\rightarrow 0} \frac{[P_{t,0}^\gamma (J(\gamma(t))) - J(x)]}{t}X(x) = (\nabla_\xi J)_x X(x).$$ So, was my idea just useless? Or does anyone see a way to save this attempt?","['riemannian-geometry', 'differential-geometry']"
4505057,"Finding the domain of the parabola with focus $(1,2)$ and directrix $2x+y=1$.","Problem. Find the domain of the parabola with focus $(1, 2)$ and directrix $2x+y=1$ . My attempt Using the distance from a point to a line formula and the point-to-point distance formula, I have gotten this equation: $$\frac{\left|2x+y-1\right|}{\sqrt{5}}=\sqrt{\left(x-1\right)^{2}+\left(y-2\right)^{2}}.$$ Clearly, simplifying this would be a pain and I'm  not sure it would help even if I did. By graphing the parabola on desmos, we find that domain is $\{x|x\ge\frac14\}$ . When I try to plug in smaller values such as $\frac15$ , I get $$\frac{|\frac25+y-1|}{\sqrt5}=\sqrt{\left(\frac15-1\right)^2+(y-2)^2}.$$ I am pretty sure simplifying and solving would yield no real solutions for $y$ , but I want An elegant solution(that does not involve a ton of bashing algebra) A method of finding the domain that does not rely on graphing. Thanks!","['analytic-geometry', 'algebra-precalculus', 'conic-sections']"
4505090,How close is $ GL_n (2) $ to being $3$ transitive?,"The group $ GL_n(2) $ acts transitively on the $ 2^n-1 $ nonzero vectors of $ \mathbb{F}_2^n $ . This action is in fact 2-transitive since any pair of distinct nonzero vectors in $ \mathbb{F}_2^n $ is linearly independent and thus for any two pairs there is an element of $ GL_n(2) $ taking one pair to another. How close to being $3$ transitive is this action? For the case of $ n=3 $ it seems that there are exactly 2 orbits of the action of $ GL_3(2) $ on the $ {7}\choose{3}$$=35 $ triples of distinct nonzero vectors from $ \mathbb{F}_2^3 $ . The first orbit is size $28$ and consists of all possible (unordered) bases of $ \mathbb{F}_2^3 $ . While the second orbit is size $ 7 $ and consists of all triples whose span has rank $2$ (in other words, triples of the form $ v,w,v+w $ ). How does this generalize to $ n > 3 $ ? Are there always just two orbits for the action on the space of triples, correspond to span having rank 3 versus span with rank 2, or does it get more complicated for large $ n $ ? How close is $ GL_n(2) $ to being 3-transitive for large $ n $ ?","['group-theory', 'group-actions', 'finite-groups']"
4505101,"Are there any topological spaces containing no proper retracts? i.e. $A\subset X$ s.t. $\exists r:X\to A$ continuous w/ $r(a)=a,\forall a\in A$","Are there any topological spaces containing no proper retracts? i.e. $A\subset X$ s.t. $\exists r:X\to A$ continuous w/ $r(a)=a,\forall a\in A$ Definition: Say that $A$ is a retract of a topological space $X$ if $A\subseteq X$ and there exists a continuous function (retraction) $r:X\to A$ such that $r(a)=a$ for all $a\in A$ . Is it the case that every topological space contains a proper retract? I'm not sure if ""proper retract"" is a legitimate term, but by that I mean that $A$ is not just a single point, or the empty set, or $X$ itself. It seems intuitive that any Euclidean space would have such a retract, but what about Hausdorff or metric spaces generally? Are there any obvious counterexamples?","['general-topology', 'retraction', 'algebraic-topology']"
4505110,A problem concerning continous function spaces and divergence of series,"Saw this on a website, having no clue. Let $C([0,1])$ be the space of real continous functions on the interval $[0,1].$ Let $\{a_n\}$ be a series such that $\sum_{n=1}^{\infty}\dfrac1{a_n}$ diverges, $a_0=0,$ and $a_n\to\infty$ when $n\to\infty.$ Show that, if $\int_0^1x^{a_n}f(x)dx=0$ for any $n\in \mathbb{N},$ then $f\equiv 0.$ I know that the proposition to be proved is equivalent to the completeness of the system $\{x^{a_n}\}$ in $C([0,1])$ with the integral(convolution) inner product. I can't see the relation between the series' divergence and the system's completeness. An example is that when $a_n=n,$ the series is well-defined and divergent, and we get the desired result by the Weierstrass Approximation Theorem. But there is still no clue when $a_n$ is a general series. I can't even construct an example when $a_n$ converges and a non-vanishing function that satisfies the orthogonal condition. Any thoughts, solutions, and other clues are welcome.","['continuity', 'orthogonality', 'functional-analysis', 'analysis']"
4505135,"How to derive the ""geometric interpretation"" of trig functions from their taylor series alone?","In trigonometry, we often define sine and cosine respectively to be the y and x coordinates given a ray coming of the origin that touches the unit circle. Later on using calculus we somehow arrive at their respective taylor series. My question is what if we start defining sine and cosine function to be their respective taylor series or as limits of some taylor polynomial, how can we arrive to the fact that these functions indeed represent the y and x coordinates of a point in the unit circle given some angle?","['trigonometry', 'taylor-expansion']"
4505208,Expectation of a factorial of a random variable.,"I am trying to solve a question pertaining to the quantity $$E(N!)$$ which is the expectation of a random variable factorial. Basically I would like to construct a non-negative discrete random variable $N$ such that $E(N!)<\infty$ while the quantity $$\| X\|_\infty = \sup\{t\ge 0: P(X\ge t)>0\}=\infty.$$ Basically, I need $N$ to barely assign mass to any of the integers since $N!$ can get very very large, but at the same time I dont want the mass to be negligible since I will require that $P(X\ge t)\not\to0$ as $t\to\infty$ . I do not really know how to start this. I have tried $P(N=n)=2^{-n!}$ and similar, but this does not work either.","['probability-theory', 'probability']"
4505231,Is this Fourier series some combination of Eisenstein series?,"Consider the twisted Eisenstein series $$
\begin{align}
	E_{k \ge 1}\left[\begin{matrix}
		\phi \\ \theta
	\end{matrix}\right] := & \ - \frac{B_k(\lambda)}{k!} \\
	& \ + \frac{1}{(k-1)!}\sum_{r \ge 0}' \frac{(r + \lambda)^{k - 1}\theta^{-1} q^{r + \lambda}}{1 - \theta^{-1}q^{r + \lambda}}
	+ \frac{(-1)^k}{(k-1)!}\sum_{r \ge 1} \frac{(r - \lambda)^{k - 1}\theta q^{r - \lambda}}{1 - \theta q^{r - \lambda}} \ ,
\end{align}
$$ where $\phi = e^{2\pi i \lambda}, q = e^{2\pi i \tau}$ . We know that they all can be written in Fourier series, for example, $$
E_1 \begin{bmatrix}
-1 \\ z
\end{bmatrix}
 = \frac{1}{2i}\sum_{n \in \mathbb{Z}, n \ne 0} \frac{1}{\sin n \pi \tau} e^{2\pi i n \mathfrak{z}} \ ,
\qquad
E_2 \begin{bmatrix}
1 \\ z
\end{bmatrix}
 = - \frac{1}{12} -\frac{1}{4}\sum_{n \in \mathbb{Z}, n \ne 0} \frac{1}{\sin^2 n \pi \tau} e^{2\pi i n \mathfrak{z}} \ ,
$$ where $z = e^{2\pi i \mathfrak{z}}$ . Recently I run into the following series $$
\sum_{\substack{m,n \in \mathbb{Z}\\m,n,m+n \ne 0}}\frac{1}{\sin m \pi \tau\sin n \pi \tau \sin (m + n) \pi \tau}e^{2\pi i (m+n) \mathfrak{z}} \ .
$$ I wonder if this can be rewritten in terms of Eisenstein series? Or is it known to be some other special functions? ============Update============ With some guess work and help from Mathematica, I think the answer is the following $$
\sum_{\substack{m,n \in \mathbb{Z}\\m,n,m+n \ne 0}}\frac{1}{\sin m \pi \tau\sin n \pi \tau \sin (m + n) \pi \tau}e^{2\pi i (m+n) \mathfrak{z}} \\
= - \frac{\vartheta'_1(\mathfrak{z})\vartheta''_1(\mathfrak{z})}{2\pi^3\vartheta_1(\mathfrak{z})^2}
+ \frac{\vartheta'''_1 (\mathfrak{z})}{6\pi^3\vartheta_1'(\mathfrak{z})}
+ 4 E_2(\tau) \frac{\vartheta_1'(\mathfrak{z})}{\pi\vartheta_1(\mathfrak{z})} \ ,
$$ where $E_2(\tau)$ is the standard quasi-modular Eisenstein series. In terms of twisted Eisenstein series, it reads $$
- 8 i \Bigg(
E_3 \begin{bmatrix}
1 \\ z
\end{bmatrix}
+ E_2 \begin{bmatrix}
1 \\ z
\end{bmatrix}
E_1 \begin{bmatrix}
1 \\ z
\end{bmatrix}
- E_2(\tau)E_1 \begin{bmatrix}
1 \\ z
\end{bmatrix}
\Bigg)
$$ However, a proof is still lacking. The guessed result is a quasi-Jacobi form of modular weight-three . If  this were known beforehand, one can always make an anzatz in terms of the twisted Eisenstein series and work out the relative coefficients using Mathematica. So a relevant question would be: how to argue that the series gives rise to such a quasi-Jacobi form?","['complex-analysis', 'fourier-series', 'modular-forms', 'special-functions']"
4505237,Bounded gradient implies Lipschitz on non-convex set,"There are loads of questions and answers concerning the following problem, but here we have a slight variation: Let $U$ be an open subset of $\mathbb{R}^n$ and $f: U \rightarrow \mathbb{R}$ be differentiable such that the derivative is bounded by the constant $L \geq 0$ . Show that $f$ is a globally lipschitz-function. So, usually the domain is $\mathbb{R}^n$ and then this is pretty easy to show by using the mean-value-theorem. However, now we have an arbitrary open set as the domain and as far as I know, we need convexity of the set in order to use the MVT for two arbitrary points in the set. Does this statement still hold? If yes, what would the argument be? As a side note I have found this post Sub-gradient and super-gradient are bounded implies globally Lipschitz. which goes in the direction of my question, but does not answer it.","['mean-value-theorem', 'derivatives', 'lipschitz-functions', 'real-analysis']"
4505251,How to evaluate $\int_{0}^{\frac{\pi}{2}} \frac{1}{1+\cos2x}dx$?,"This question came in my exam today. The options were: (a) $\,0$ (b) $\,1/2$ (c) $\,3/2$ (d) I forgot what this option was (they dont let us take the question paper home) My attempt: $$\int_{0}^{\frac{\pi}{2}} \frac{1}{1+\cos2x}dx$$ $$=\frac{1}{2}[\tan x]_0^{\frac{\pi}{2}}$$ $$=\frac{1}{2}\Big[\tan\Big(\frac{\pi}{2}\Big)-0\Big]$$ $$=\frac{1}{2}\tan\Big(\frac{\pi}{2}\Big)$$ $$=\color{red} {\text{undefined}}$$ Isn't this question wrong?","['integration', 'calculus', 'solution-verification', 'trigonometric-integrals', 'limits']"
4505276,Combinatorial or Probabilistic proof of the identity $\sum_{k=0}^n \binom{n}{k}\frac{1}{k+1} = \frac{2^{n+1}-1}{n+1}$,"While teaching Combinatorics to first year students, I like to discuss several ways to prove each identity involving binomial coefficients. For instance, one can show that $\sum_{k=0}^nk \binom{n}{k} = n2^{n-1}$ by: 1) replacing $k \binom{n}{k}$ with $n \binom{n-1}{k-1}$ ; 2) differentiation the binomial identity; 3) counting the number of pairs $(x,A)$ such that $x \in A \subseteq [n]$ . To deal with the related identity $\sum_{k=0}^n \binom{n}{k}\frac{1}{k+1} = \frac{2^{n+1}-1}{n+1}$ , one could easily adjust the first two approaches from the previous paragraph: 1) replace $\binom{n}{k}\frac{1}{k+1}$ with $\binom{n+1}{k+1}\frac{1}{n+1}$ ; 2) integrate the binomial identity. However, it took me a while to come up with a combinatorial or probabilistic proof of this identity. To be honest, I almost gave up and decided to ask for help here, but found the solution while typing this question. Nevertheless, I think it's still worth posting since it was not easy for me to find the desired argument either in my head or on google and math.stackexchange and since someone else might find it helpful. I agree that my arguments might be not quite comprehensible for some first year students. So, if you see an easier or more intuitive solution, feel free to post it here!","['summation', 'binomial-coefficients', 'combinatorics', 'discrete-mathematics', 'probability']"
4505306,Is WolframAlpha wrong? or am I?,"Problem : Determine a limit $$\lim_{x\to\infty}xe^{\sin x}$$ exists or not. If it exists, find a limit. Since $-1\le \sin x \le 1$ , I can say that $\displaystyle0<\frac{1}{e}\le e^{\sin x}\le e$ . Multiply $x$ both side, $\displaystyle0<\frac{x}{e}\le xe^{\sin x}\le ex$ Since $\displaystyle\lim_{x\to\infty} \frac{x}{e}=\infty$ , I think $\displaystyle\lim_{x\to\infty}xe^{\sin x}=\infty$ also. But, WA says it is indeterminate form : Am I correct? or Am I wrong? If I'm wrong, where did I make a mistake?","['limits', 'wolfram-alpha']"
4505341,Tao Analysis 1 Exercise 6.1.8.,"Let $(b_n)_{n=m}^\infty$ be a sequence of real numbers and let $L$ be a real number such that $\lim_{n\to\infty}b_n = L$ . Let also $b_n \neq 0$ for all n and $L \neq 0$ . Show that \begin{equation*}
\lim_{n\to\infty}\frac{1}{b_n} = \frac{1}{L}.
\end{equation*} I do not understand the hint behind the exercise. The hint states that one should prove that the sequence is bounded away from zero. However, if $b_n \neq 0$ for all $n$ why do I need to prove such a thing? In particular, can someone point out where the mistake is in my reasoning? My proof: Since $\lim_{n\to\infty} b_n = L$ the sequence is bounded; i.e., there exists a $B>0$ such that $|b_n|\leq B$ for all $n$ . Observe that \begin{align}
|\frac{1}{b_n} - \frac{1}{L}| &= |\frac{L-b_n}{b_nL}|\\
&= \frac{|b_n - L|}{|b_n||L|}\\
&\leq \frac{|b_n - L|}{B|L|}.
\end{align} Choosing now an appropriate $\epsilon$ for $|b_n - L|$ like $\epsilon B|L|$ should be sufficient to conclude the reasoning.","['analysis', 'real-analysis']"
4505347,How can I solve a differential equation of the form $c_1 \ddot{x}^2 + c_2 \ddot{x} + c_3 \dot{x}^2-c_4 =0$?,"The solution to it will give us the eqs. of motion of a car moved by a constant force due to combustion of fuel at the engine that produces the main torque at the wheels and that is ""fighting"" against friction and drag. The differential equation comes from the 2nd law of Newton and the torque equations. In brief, we'd have \begin{align*}
\sum F &= m(t) \ddot{x}\\
m(t)\ddot{x} &= F_{fuel} -F_f -F_d = F_{fuel} -\mu m(t)g-\kappa \dot{x}^2
\end{align*} Since there are two unknown functions we want to find out, $m(t)$ and $x(t)$ , $\dot{x}(t)$ , or $\ddot{x}(t)$ , we'll have to find a relation between $m(t)$ and any of these 3. We'll do it with the torque eqs.. We'll consider that the torque in one of the 4 wheels of the car is generated thanks to $\frac{1}{4}F_f$ and $F_{fuel}$ : \begin{align*}
\sum \tau = r \sin{\theta}\sum F &= I\ddot{\varphi}\\
r(F_{fuel}+\frac{1}{4}F_f) &= \frac{1}{2}m_w r^2 \ddot{\varphi}\\
F_{fuel}+\frac{1}{4}\mu m(t)g &= \frac{1}{2}m_w \ddot{x}\\
m(t) &= \frac{2 m_w}{\mu g} \ddot{x} -\frac{4}{\mu g}F_{fuel}
\end{align*} Now we substitute this expression in the Newton's equation and get the differential equation from the title for $c_1 \equiv \frac{2 m_w}{\mu g}$ , $c_2 \equiv 2m_w -\frac{4}{\mu g} F_{fuel}$ , $c_3 \equiv \kappa$ , and $c_4 \equiv 5 F_{fuel}$ : \begin{align*}
\Bigg[\frac{2 m_w}{\mu g} \ddot{x} -\frac{4}{\mu g}F_{fuel}\Bigg]\ddot{x} &= F_{fuel} -\mu g \Bigg[\frac{2 m_w}{\mu g} \ddot{x} -\frac{4}{\mu g}F_{fuel}\Bigg]-\kappa \dot{x}^2\\
\frac{2 m_w}{\mu g} \ddot{x}^2 -\frac{4}{\mu g}F_{fuel}\ddot{x} &= F_{fuel} -2 m_w\ddot{x} + 4F_{fuel}-\kappa \dot{x}^2\\
\frac{2 m_w}{\mu g} \ddot{x}^2 + (2m_w -\frac{4}{\mu g} F_{fuel})\ddot{x} + \kappa \dot{x}^2 -5 F_{fuel}&= 0
\end{align*} I don't have the enough knowledge to approach this diff. eq. yet, and I don't even know if it is solvable analitically.","['physics', 'calculus', 'ordinary-differential-equations']"
4505353,"What is the opposite to ""discretization""?","Solving an initially continuous problem using discrete math tools is known as "" discretization "". Does the opposite option of using continuous tools to solve discrete problems have a name? And if not, is there any reason why? Coming from natural language processing, that would apply for instance to resorting to word-embeddings , where one expresses words as vectors in order to escape from the essentially discrete nature of word combinatorics. In econometrics, that could apply to the standard modelling of discrete choices , where a fictional continuous quantity (utility) is maximised... and the list probably goes on for ever.","['word-problem', 'continuity', 'discrete-mathematics', 'soft-question', 'terminology']"
4505359,Basis for monomials $x^p_1x^q_2$ using $x^s$,"Let $s\in\mathbb{N}$ . I have a sum of the form $$
    \sum_{p+q\leq s}c_{p,q}x^p_1x^q_2,
$$ and want to write it in terms of functions taking the form $$
    g(x,(c_0,c_1,c_2)) = (c_0+c_1x_1+c_2x_2)^s.
$$ I want to use as few as possible terms $g(x,(c_0,c_1,c_2))$ to describe the entire sum. To understand how to do that I first want to find one set of $g(x,(c_0,c_1,c_2))$ for each $(p,q)$ , i.e. find a set $S\subset \mathbb{R}^3$ such that $$
  x_1^px_2^q = \sum_{(c_0,c_1,c_2)\in S}g(x,((c_0,c_1,c_2)).
$$ For the simple cases I have (p,q) S (0,0) {(1,0,0)} (s,0) {(0,1,0)} (0,s) {(0,0,1)} and I know that $g$ can also be written as $$
   g(x,(c_0,c_1,c_2)) = \sum_{k_0+k_1+k_2=s}\frac{s!}{k_0!k_1!k_2!}c^{k_0}_0c^{k_1}_1c^{k_2}_2x^{k_1}_1x^{k_2}_2
$$ Is there an explicit way of writing $S$ for the other $(p,q)$ ? -- edit -- For $s=2$ the full list is (up to multiplicative constants) (p,q) S (0,0) {(1,0,0)} (1,0) {(1,1,0),(0,-1,0),(-1,0,0)} (0,1) {(1,0,1),(0,0,-1),(-1,0,0)} (1,1) {(0,0,-1),(0,-1,0),(0,1,1)} (2,0) {(0,1,0)} (0,2) {(0,0,1)}","['systems-of-equations', 'multivariable-calculus', 'change-of-basis', 'linear-algebra', 'polynomials']"
4505377,Computing derivative with respect to a function,"Consider the functional $$
\begin{align*}
J\colon (X:= C^{1}( [ 0, 1]))\to \mathbb{R}, \quad J[ f] = \int_{ 0}^{1} f'( x)^{2}\mathrm{~d}x 
.\end{align*}
$$ The task asks to compute $\frac{\partial }{\partial f} J $ . So far I haven't
encounted functionals, so I tried to endow $X$ with the infinity norm
and consider some pertubation function $h \in X$ . One has $$
\begin{align*}
J[ f + h] - J[f]
= \int_{ 0}^{1} f'( x)h'( x)\mathrm{~d}x 
+ \int_{ 0}^{1} h'( x)^{2}\mathrm{~d}x 
.\end{align*}
$$ With the above computation wanted to try to exploit the linearity of the
differentiation/integral operator, but didn't succeed to bring it into the desired form $$
\begin{align*}
J[ f + h] = J[ f] + \mathrm{D}J\!\left( f\right)( h) + o(h )
.\end{align*}
$$ I would suspsect $$
\begin{align*}
\mathrm{D}J\!\left( f\right)( h) = \int_{ 0}^{1} f'( x) h'( x)\mathrm{~d}x , 
\quad \int_{ 0}^{1} h'( x)^{2}\mathrm{~d}x \in o( h )
\end{align*}
$$ but I don't even know whether this is a fruitful approach to solve this task.","['calculus', 'functional-analysis', 'real-analysis']"
4505408,Prove $f(-\frac12) \le \frac{3}{16}$ if all roots of $f(x) = x^4 - x^3 + a x + b$ are real,"Let $a, b$ be real numbers such that all roots of $f(x) := x^4 - x^3 + ax + b$ are real. Prove that $f(-1/2) \le 3/16$ . The question was posted recently which was closed,
due to missing of contexts etc. My attempt : $f(-1/2) \le 3/16$ is equivalent to $a \ge 2b$ . Let $x_1, x_2, x_3, x_4$ be the real roots of $f(x)$ .
By Vieta, we have \begin{align*}
	x_1 + x_2 + x_3 + x_4 &= 1, \\
	x_1 x_2 + x_1x_3 + x_1x_4 + x_2 x_3 + x_2x_4 + x_3x_4 &= 0, \\
	x_1x_2x_3 + x_1x_2x_4 + x_1 x_3 x_4 + x_2x_3 x_4 &= -a, \\
	x_1x_2x_3x_4 &= b.
\end{align*} The problem becomes: If $x_1, x_2, x_3, x_4$ are real numbers such that $x_1 + x_2 + x_3 + x_4 = 1$ and $x_1 x_2 + x_1x_3 + x_1x_4 + x_2 x_3 + x_2x_4 + x_3x_4 = 0$ ,
prove that $$x_1x_2x_3 + x_1x_2x_4 + x_1 x_3 x_4 + x_2x_3 x_4 + 2x_1x_2x_3x_4 \le 0.$$ This is true (e.g. verified by Mathematica). Is there a nice proof for it (or the original problem)?","['algebra-precalculus', 'quartics', 'polynomials', 'inequality']"
4505469,Best Embedding Constant for Weighted $L^p$ Space,"Let $\Omega \subset \mathbb{R}^n$ be a bounded Lebesgue measurable set of positive measure. Fix a function $A: \Omega \to \mathbb{R}$ such that $A \in L^{\infty}(\Omega)$ and there is a constant $a$ such that $A(x)>a>0$ for almost every $x \in \Omega$ . Fix also a $p \in [1,\infty)$ . We can norm the space $L^p(\Omega)$ in the following two ways: for every $f: \Omega \to \mathbb{R}$ , we set : \begin{equation*}
\|f\| = (\int_{\Omega}|f(x)|^pdx)^{\frac{1}{p}}; \|f\|_A = (\int_{\Omega}|f(x)|^pA(x)dx)^{\frac{1}{p}}
\end{equation*} Clearly $\|kf\|_A=|k|\|f\|_A$ for every $k \in \mathbb{R}$ . By viewing $A(x)dx$ as a measure, Minkowski's inequality shows that $\|f+g\|_A\leq\|f\|_A+\|g\|_A$ . Also, if $\|f\|_A=0$ , then by the condition $A(x)>a>0$ almost everywhere we get $f=0$ almost everywhere. Therefore, $\|\cdot\|_A$ is a norm. Clearly, one always has: \begin{equation*}
\|f\|_A \leq (\|A\|_{\infty})^{\frac{1}{p}} \|f\|
\end{equation*} By assumption, $\|A\|_{\infty} \in (0,\infty)$ . Therefore, $\| \cdot \|$ and $\| \cdot \|_A$ are equivalent norms in the Banach space $L^p(\Omega)$ . The following best embedding constants are well-defined and finite: \begin{equation*}
c = \inf \frac{\|f\|_A}{\|f\|}; d = \inf \frac{\|f\|}{\|f\|_A}
\end{equation*} The infinums are taken over every nonzero $f\in L^p(\Omega)$ . My goal is to compute $c$ and $d$ by expressing them in terms of $\Omega$ and $A$ (or even better to find an $f$ attaining these infinums but this is not required). So let us just focus on computing $c$ . Set $f(x)=1$ for every $x \in \Omega$ we see that: \begin{equation*}
c \leqslant (\frac{1}{m(\Omega)}\int_{\Omega}A(x)dx)^{\frac{1}{p}}
\end{equation*} My conjecture is that $c=(\frac{1}{m(\Omega)}\int_{\Omega}A(x)dx)^{\frac{1}{p}}$ . But I cannot play it out using H√∂lder's inequalities or find counterexamples. How do I proceed? Note1: as pointed out by Ryszard Szwarc, $A>0$ is not sufficient to make sure norms are equivalent: let $\Omega=(0,1)$ and $A(x)=1-x.$ For $f_n(x)=x^{n/2}$ we have $$\|f_n\|^2={1\over n+1},\qquad \|f_n\|^2_A={1\over (n+1)(n+2)}$$ In general let $$ U_n=\{x\in \Omega\,:\, |A(x)|\le n^{-1/2}\},\qquad m(U_n)>0$$ . Then for $f_n=\chi_{U_n}$ we have $$\|f_n\|^2=m(U_n), \qquad \|f_n\|^2_A\le {m(U_n)\over n}$$ I have updated the question to add the assumption that $A$ is bounded away from $0$ . Note2: if it is ever needed we can work in stronger set of hypothesis: $p \in (1,\infty)$ , $\Omega$ open connected with smooth boundary or even $\Omega$ is a unit ball. But I guess these conditions do not really matter.","['lebesgue-integral', 'real-analysis', 'lp-spaces', 'functional-analysis', 'optimization']"
4505522,"Spivak, Ch. 18, Problem 45: Find all functions satisfying $f^{(n)}=f^{(n-1)}$.","The following is a problem from Spivak's Calculus , Chapter 18 Find all functions satisfying (a) $f^{(n)}=f^{(n-1)}$ The solution manual says (a) We have $f^{(n-1)}(x)=ce^x$ , so $$f(x)=a_0+a_1x+...+a_{n-2}x^{n-2}+ce^x$$ Why did he conclude that $f^{(n-1)}(x)$ must be $ce^{x}$ ? How do we know this represents all solutions? I believe we can conclude that $ce^{x}$ is a solution to $f^{(n-1)}(x)=f^{(n)}(x)$ based on a previous problem (Problem 43, which I discuss below), but we never showed that this is the only solution. Here are the problems that came before 45 Problem 42: if we can find a root $\alpha$ with multiplicity $r$ of the equation $\sum\limits_{i=1}^n a_ix^i$ , then we also automatically have $r$ different roots of the differential equation $\sum\limits_{i=1}^n a_if^{(i)}(x)$ . These roots are $x^ke^{\alpha k}$ for $0\leq k\leq r-1$ . Also, any linear combination of these roots is a root, so we have infinite roots. A note at the end of the problem says that the set of these linear combinations represents all the possible solutions, though this is not proved here. Problem 43: This problem I found very strange and not well specified but here goes. If a function $f$ satisfies $f''-f=0$ and $f(0)=f'(0)=0$ then it follows that $f=0$ . Now the proof of this is done in three steps (though I will only talk about two here). First we show that $f^2-(f')^2=0$ follows from the initial assumptions. Second, we show that if $f\neq 0$ in some interval $(a,b)$ then either $f(x)=ce^x$ or else $f(x)=ce^{-x}$ for all $x$ in $(a,b)$ and some constant $c$ . Here is the solution manual proof of this Since $f(x)\neq 0$ for $x$ in $(a,b)$ , it follows from part $(a)$ that
either $f'(x)=f(x)$ for all $x$ in $(a,b)$ or else $f'(x)=-f(x)$ for
all $x$ in $(a,b)$ . Thus either $f(x)=ce^x$ or else $f(x)=ce^{-x}$ for
all $x$ in $(a,b)$ . I found this proof to be a bit strange, though probably only because it skips so many intermediate steps. Here is my proof of this result $$(f')^2=f^2 \implies f=f' \text{ or } f=-f'$$ Case 1: $f'-f=0$ As per Problem 42, if we can find a solution to the polynomial $x-1=0$ then we will have a solution to $f'-f=0$ . Since $1$ is the solution to
the polynomial, then $f(x)=e^x$ is a solution to the differential
equation. We can easily show that for any constant $c$ , $f(x)=ce^x$ is
also a solution. Case 2: $f'+f=0$ . Analogous proof shows that $f(x)=e^{-x}$ is a
solution. In any case, going back to problem 45a, $f^{(n)}=f^{(n-1)}$ means that $f(x)=ce^x$ is a solution for any constant $c$ . But I don't see how we've shown that these are the only possible solutions.","['integration', 'proof-explanation', 'calculus', 'derivatives', 'exponential-function']"
4505528,Left side is finite but right side becomes infinite despite using correct series expansions,"I want to evaluate (for integer $n,p$ ) $$ 
L_p = \sum_{m=1}^{n-1} \ln \left(  4 \sin^2 \frac{m \pi }{n}\right)  e^{- 2\pi i p m
/n}
$$ and I have got two tools $$ 
\sum_{k=1}^{\infty} \frac{\cos kx}{k} = - \frac{1}{2} \ln \left(4 \sin ^2 \frac{x}{2}\right) 
$$ and $$ 
\sum_{k=1}^{n-1} r^{k} \cos kx =  \frac{1 - r \cos x - r^{n} \cos (nx) + r^{n+1} \cos (nx -x)}{r^2 + 1 - 2 r \cos x} -1
$$ both of which are formulas tabulated in Gradshteyn, and I've individually
verified them. (I couldn't derive the first myself, but second was easy to prove.) Plugging in $L_p$ , we get $$ 
L_p =  \sum_{m=1}^{n-1} \left[-2 \sum_{k=1}^{\infty}  \frac{\cos \left(  k \cdot  \frac{2 m \pi}{n}\right)}{k}\right] e^{- 2\pi i p m/n}
$$ and exchanging the sum (as I understand can always be done according to this
question ), this becomes $$ 
L_p\; = -2\sum_{k=1}^{\infty}  \frac{1}{k} \cdot  \left[  \sum_{m=1}^{n-1} e^{-2 \pi i p m/n} \cdot  \cos \left( m \cdot  \frac{2\pi k}{n}\right)\right]
$$ Setting $r = e^{-2 \pi i p/n}$ , $x= 2\pi k/n$ and using above formula, the inner sum becomes $-1$ .  which means $$ 
L_p = 2 \sum_{k=1}^{\infty} \frac{1}{k}  
$$ which is of course divergent, but the left side was finite because $m=0$ never
occurs in the sum. What is going wrong, and how can I get $L_p$ ?","['trigonometry', 'convergence-divergence', 'sequences-and-series']"
4505555,limit of limit of sets is a limit of sets,"Let $\mathcal A$ be a collecction of subsets of some set $A$ . Whenever $\{ X_n\}_{n\in\mathbb N}$ is a sequence of sets such that $\liminf_n X_n \triangleq \bigcup_{n\geq 1}\bigcap_{m\geq n} X_m$ and $\limsup_n X_n \triangleq \bigcap_{n\geq 1}\bigcup_{m\geq n} X_m$ are both equal to some set $X$ then we say that $\lim_n X_n$ exists and equal $X$ . Suppose we have a sequence of sets $\{ X_{n,k} \}_{n,k\in\mathbb N}$ in $\mathcal A$ such that for all $n$ , $\lim_n X_{n,k}=X_n$ for some $X_n$ (that may not be in $\mathcal A$ ) and suppose that $\lim_n X_n = X$ for some $X$ . I want to prove (or disprove) the following There is a sequence of sets $\{ Y_n \}_{n\in\mathbb N}$ in $\mathcal A$ such that $\lim_n Y_n=X$ . My attempt uses a bijective mapping $\phi:\mathbb N\to\mathbb N^2$ and the sequence of sets $Y_n=X_{\phi(n)}$ , it feels that this might do the trick but I can't quite finish the argument. It would be enough to show that $\liminf_n X_n\subseteq \liminf_n Y_n$ and $\limsup_n Y_n\subseteq \limsup_n X_n$ , proving those two should be very similar so let's focus on the first one. \begin{align*}
\bigcup_{n\geq1}\bigcap_{m\geq n} \bigcup_{k\geq1}\bigcap_{\ell\geq k} X_{m,\ell}\subseteq \bigcup_{n\geq1}\bigcap_{m\geq n} X_{\phi(m)}
\end{align*} here it feels like we would like to swap the two middle intersection and union but this would result in the other inclusion. I am not sure what kind of condition ensures that we can do the swapping or if this idea is even a good one in the first place. Any idea is most welcome. An equivalent formulation using characteristic functions : it is known that $\lim_n X_n=X$ if and only if $\psi_{X_n}$ converges to $\psi_X$ point-wise. From $\mathcal A$ we can get $\mathcal F=\{ \psi_X : X \in\mathcal A\}$ which is a set of $\{0,1\}$ valued functions. Consider $\overline{\mathcal F}=\{ \psi : \psi = \lim_n \psi_n, \psi_n\in\mathcal F \}$ the sequential closure of $\mathcal F$ . What I am trying to prove is essentially that $\overline{\overline{\mathcal F}}=\overline{\mathcal F}$ .","['elementary-set-theory', 'limits', 'order-theory', 'limsup-and-liminf']"
