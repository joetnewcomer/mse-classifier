question_id,title,body,tags
1110164,What's the difference between cohomology theories of varieties and topological spaces,"There is defined several cohomology theories for algebraic varieties, but in the situation is very different for topological spaces (up to homotopy) for which there is only one cohomology theory for every given abelian group of coefficients. What is really the difference? Can one define a similar category of motives of topological  spaces? If so what is the property of this abelian tensor category which implies the uniqueness of fiber functors? Thanks!","['homology-cohomology', 'algebraic-geometry', 'algebraic-topology']"
1110168,Proof of the Box-Muller method,"This is Exercise 2.2.2 from Achim Klenke: »Probability Theory — A Comprehensive Course«. Exercise (Box–Muller method): Let $U$ and $V$ be independent random variables that are uniformly distributed on $[0,1]$. Define
  $$X := \sqrt{−2\log(U)}\, \cos(2\pi V) \quad \text{and} \quad Y := \sqrt{−2\log(U)}\, \sin(2\pi V)\, .$$ Show that $X$ and $Y$ are independent and $\mathcal{N}_{0,1}$-distributed. Solution: Define random variable $R:= \sqrt{-2\log(U)}$, then 
\begin{align*}
\mathbf{P}\bigl[R \leq r\bigr] & = \mathbf{P}\bigl[-2 \log(U) \leq r^2\bigr] = \\
& = \mathbf{P}\bigl[\log(U) \geq -\frac{r^2}{2}\bigr] = \\
& = 1 - \mathbf{P}\biggl[U < \exp\Bigl(-\frac{r^2}{2}\Bigr)\biggr]\, .
\end{align*}
$U$ is uniformly defined on $[0, 1]$, so the distribution of $R$ is $$\mathbf{P}[R\leq r] = 1 - \int_0^{\exp(-r^2/2)} \, dt = 1 - \exp\Bigl(-\frac{r^2}{2}\Bigr)\, .$$
For the density of $R$ we get: $f_R(t) = \exp\Bigl(-\frac{t^2}{2}\Bigr)\cdot t$ with $t> 0$. We also define the random variable $\Phi := 2\pi V$. Since $V$ is uniformly distributed on $[0, 1]$, $f_\Phi(t) = \frac{1}{2\pi}$ with $0< t \leq 2\pi$. Since $U, V$ are independent, $R, \Phi$ must also be independent and $$f_{R, \Phi}(t_1, t_2) = f_R(t_1) f_\Phi(t_2) = \frac{1}{2 \pi} \exp\Bigl(-\frac{t_1^2}{2}\Bigr)\cdot t_1 \, .$$ With \begin{align*}
g\colon (0,\infty)\times(0, 2\pi] &\rightarrow \mathbb{R}^2 \\
(r, \phi) &\mapsto \bigl(r \cos(\phi), r \sin(\phi)\bigr)
\end{align*} we see that
$$(X, Y) = g(R, \Phi)\, ,$$ so we want to find the image measure
$$\mathbf{P}_{X, Y} = \mathbf{P}_{R, \Phi}\circ g^{-1}\, .$$ We use the transformation formula for densities:
$$ f_{X, Y}(\tau_1, \tau_2) = \frac{f_{R, \Phi}(g^{-1}(\tau_1, \tau_2))}{|\det(g'(g^{-1}(\tau_1, \tau_2)))|}$$ $g$ is just the transformation for polar coordinates. With
$$ t_1 = \sqrt{\tau_1^2 + \tau_2^2} = |\det(g'(g^{-1}(\tau_1, \tau_2)))|$$
we finally get
$$f_{X, Y}(\tau_1, \tau_2) = \frac{1}{2 \pi} \exp\Bigl(-\frac{\tau_1^2 + \tau_2^2}{2}\Bigr) = \underbrace{\frac{1}{\sqrt{2 \pi}} \exp\Bigl(-\frac{\tau_1^2}{2}\Bigr)}_{=f_X(\tau_1)} \cdot \underbrace{\frac{1}{\sqrt{2 \pi}} \exp\Bigl(-\frac{\tau_2^2}{2}\Bigr)}_{=f_Y(\tau_2)}\, ,$$
that is: $X, Y$ are $\mathcal{N}_{0, 1}$-distributed and independent. $\square$ Could you please check my proof?  I'm sorry that it's so long — it seems right to me, but I'm self-studying and really need to catch any eventual mistakes... 
Thank you!","['probability-theory', 'proof-verification']"
1110181,Does there exist a closed form for $L_k$ for any $k>3$?,"I defined a sequence $L_k$ as the limit of a sequence of ""hyperharmonic"" series in this question . I was surprised to find that $L_3=(\sqrt{13+4\sqrt2}-1)/2$, but was unable to find a representation for any $k>3$. Does there exist a closed form for $L_k$ for any $k>3$? Definition: Let $H_k^{-1}=1$ for every positive integer $k$, and $$H_k^r=\sum_{n=1}^k\left(\sum_{m=1}^nH_m^{r-1}\right)^{-1}$$ for every positive integer $k$ and non-negative integer $r$, so that $$H_k^0=\sum_{n=1}^k\dfrac1n$$ coincides with the usual definition of the $k$th harmonic number. I defined $$L_k=\lim_{r\to\infty}H_k^r$$ Clearly $L_1=1$, it can be deduced from the continued fraction for $\sqrt2$ and the above definitions that $L_2=\sqrt2$, and I found the above value for $L_3$ with the use of high precision floating point. Edit: I guess it's obvious that $L_k$ satisfies $$L_k=\sum_{n=1}^k\left(\sum_{m=1}^nL_m\right)^{-1}$$ but I don't know how to apply this to calculating $L_k$. Wolfram Alpha was able to give me a hideous expression for $L_4={\small-\dfrac{5+3\sqrt2+\sqrt{13+4\sqrt2}+\sqrt{26+8\sqrt2}-\sqrt2(361+178\sqrt2+51 \sqrt{13+4\sqrt2}+64\sqrt{26+8\sqrt2})}{2(1+2\sqrt2+\sqrt{13+4\sqrt2})}}$ Edit: I'm not so sure of this last sum. I believe I was given a correct result by Wolfram Alpha, but I think I might've made an error in transcription. Unfortunately I'm having a hard time getting the result again. I put the simpler formula $L_k=\dfrac1{L_k-L_{k-1}}-\dfrac1{L_{k-1}-L_{k-2}}$ into Wolfram Alpha and it gave me the recurrence $${\scriptsize L_k=\dfrac{L_{k-1}^2-L_{k-1}L_{k-2}+\sqrt{L_{k-1}^4-2L_{k-1}^3L_{k-2}+L_{k-1}^2L_{k-2}^2+6L_{k-1}^2-10L_{k-1}L_{k-2}+4L_{k-2}^2+1}-1}{2(L_{k-1}-L_{k-2})}}$$ which seems to work.","['closed-form', 'sequences-and-series', 'harmonic-numbers']"
1110217,Constructing a Continuous Everywhere but Nowhere Differentiable Function,"In Neal Carothers' Real Analysis he claims that $$f(x)=\sum_{k \mathop = 0}^\infty 2^{-k}g(2^{k}x)$$ is a continuous but non-differentiable function over the real line if $g(x)$ is the distance between $x$ and the integer closest to $x$. He then defines a dyadic rational to be a rational number of the form $\dfrac{i}{2^n}$ where $i$ is an integer, and that by two successive dyadic rationals he means $\dfrac{i}{2^n}$ and $\dfrac{i+1}{2^n}$. Skipping a few steps ahead to the part I don't understand, he writes the equation: $$\dfrac{f(v_n)-f(u_n)}{v_n-u_n}=\sum_{k \mathop = 0}^{n-1} \dfrac{g(2^{k}v_n)-g(2^{k}u_n)}{2^kv_n-2^ku_n}$$ Where $u_n$ and $v_n$ are a pair of successive dyadic rationals, and $n\ge 1$. To me, the right hand side of the equation is the $(n-1)^{th}$ term of the sequence $$f_n(x)=\sum_{k \mathop = 0}^n 2^{-k}g(2^{k}x)$$ at $x=v_n$ minus the $(n-1)^{th}$ term of the above sequence at $x=u_n$, all divided by $v_n-u_n$. How can this be equal to the expression on the left hand side? Any help at all would be much appreciated!","['sequences-and-series', 'continuity', 'real-analysis', 'analysis', 'derivatives']"
1110231,"Chain rule in the Sobolev space $W^{1,p}$","(Chain rule) Assume $F : \mathbb{R} \to \mathbb{R}$ is $C^1$, with $F'$ bounded. Suppose $U$ is bounded and $u \in W^{1,p}(U)$ for some $1 \le p \le \infty$. Show $$v :=F(u) \in W^{1,p}(U) \quad \text{and} \quad v_{x_i}=F'(u)u_{x_i}.$$ From PDE Evans, 2nd edition: Chapter 5, Exercise 17. Here is what I understand conceptually so far: Since $u \in W^{1,p}(U)$, it follows $Du=u'$ exists, with $$\int_U u \phi' dx = -\int_U Du \phi \, dx.$$ I need to show that $D(F(u))=F'(u)Du$ exists, with $$\int_U F(u) \phi' dx = -\int_U D(F(u)) \phi \, dx.$$ Then, I can conclude that $F(u) \in W^{1,p}(U)$. This is all I know so far; how can I go about making the connection?","['sobolev-spaces', 'functional-analysis']"
1110268,Jacobian of a Riemann surface and double unramified covers,Take the jacobian of a riemann surface named $J(C)$ as the set of the line bundle of degree zero. Set $J_{2}(C)$ the subgroup of $J(C)$ of the element of orther two i.e. $L\in J(C)$ such that $L^{\bigotimes2}=O_C$. The assertion is that there is a way to realize  $J_{2}(C)$ as the set $\Delta(C)$ of the double unramified cover under the Riemann surface $C$. How can I do it?,['algebraic-geometry']
1110271,Maximum value of $f(x) = \log_{(\tan x + \cot x)}(\det A)$ for a diagonal matrix $A$,"If $$A =\begin{pmatrix}
        d_1 & 0 & 0 & 0 \\
        0 & d_2 & 0 & 0\\
        0 & 0 & d_3 & 0\\
        0 & 0 & 0 & d_4\\
        \end{pmatrix}$$ where $d_i>0$ $\forall$ $i=1,2,3,4$ is a diagonal matrix of order 4 such that $d_1+2d_2+4d_3+8d_4=16$ then the maximum value of $f(x)=\log_{(\tan x+\cot x)}(\det(A))$ where $x \in (0,\pi/2)$ is equal to My attempt at the solution- I have no idea how to approach this one. All I did was calculated the $|A|$, which came out to be $d_1 d_2 d_3 d_4$ . I further calculated the minimum value of $\tan x+\cot x$ by expressing it in form of $\sin(2x)$. The minimum value comes out to be 2 (at $x=\pi/2$) So I am now left with the expression $\log_2 d_1 d_2 d_3 d_4$ and cannot move further. I have no idea how to use the given equation in the solution. The answer comes out to be equal to $2$.","['matrices', 'linear-algebra', 'functions', 'determinant']"
1110274,if $\rho: H \to \text{GL}_n({\bf C})$ is faithful then $\text{Ind}_H^G \rho$ is faithful,How do I show that if $\rho: H \to \text{GL}_n(\mathbb{C})$ is faithful then $\text{Ind}_H^G \rho$ is faithful?,"['linear-algebra', 'representation-theory', 'group-theory', 'abstract-algebra']"
1110292,"Number of ways to pick N numbers from 0,1,...,N-1, with possible duplication, with sum equal 0 mod N","We have the numbers $0,1,2,....,N-1$ in $\mathbb Z_N.$ I want to pick $N$ numbers from these. These are the rules: Duplication may occur We don't care about ordering, $00041$ is equivalent to $40010$ If you add $1$ to all the numbers you picked, what you get will be equivalent to what you just had. In other words, if $N = 5, 00343$ is eqivalent to $11404.$ Question: How many equivalence classes are there s.t. the sum of the numbers in the class is $0$ mod$N$?","['permutations', 'discrete-mathematics', 'modular-arithmetic', 'group-actions', 'combinatorics']"
1110383,Zeilbergers algorithm in Maple,"I try to prove several  hard combinatorial identities. One of them is following
\begin{align*}
\sum_{s=0}^{\min\{k,n-1\}} \sum_{i=0}^{k-s} (-1)^{i} {2n+k-i-1 \choose k-s-i} {i-n \choose s} {n+i-1 \choose i} {n+k-s-1 \choose k} =\\
=\sum_{j=0}^{[\frac k2]} \sum_{i=0}^{min\{j, n-1\}}{ n-1 \choose i}^2   {{2n+j-i-2} \choose j-i}  { n+k-2j-1 \choose n-1} .\text{   ($n,k$ are nonnegative integer)}
\end{align*}
Using the identity of Le-Jen Shoo , we have \begin{align*}
\sum_{s=0}^{\min\{k,n-1\}} (-1)^s {n+k-s-1 \choose k}\sum_{i=0}^{k-s} (-1)^{i} {2n+k-i-1 \choose k-s-i} {n+s-i-1 \choose s} {n+i-1 \choose i}  =\\
=\sum_{j=0}^{[\frac k2]} {n+j-1 \choose j}^2 { n+k-2j-1 \choose n-1}.
\end{align*}
I tried to use many formulas from manuscripts of H.W. Gould,  books of Riordan, of Egorichev and other. But I can't prove this. I want to use Zeilbergers algorithm in Maple . But I I have no idea how to start it.","['maple', 'algebra-precalculus', 'combinatorics']"
1110385,Example of an integral domain that is not integrally closed and having some localization which is also not integrally closed,Can anyone show an example of integral domain that is not integrally closed and also has one of its localization with respect to a maximal ideal not integrally closed?,"['localization', 'ring-theory', 'integral-domain', 'abstract-algebra', 'commutative-algebra']"
1110422,Help me find the following limit : $\lim_{{n}\to{\infty}} (\frac{2^x+3^x+\cdots+n^x}{n-1})^\frac{1}{x} = ?$,"I have no idea where to start.$$\begin{align}\lim_{{n}\to{\infty}} \left(\dfrac{2^x+3^x+\cdots+n^x}{n-1}\right)^{1/x} = ?, n >1\\\end{align}$$","['sequences-and-series', 'calculus', 'limits']"
1110463,Solving $\frac{x+2}x>0$,"I want to find values of $x$ such that $\dfrac{x+2}{x}>0$ : $1+\dfrac{2}{x}=\dfrac{x+2}{x}>0 \implies \dfrac{2}{x}>-1 \implies \dfrac{1}{x}>\frac{-1}{2} \implies x<-2 $. But by intuition $x>0$ is also results in $\dfrac{x+2}{x}>0$. How possible that $x>0$ doesn't come from solving the original inequality? I mean, why $\dfrac{x+2}{x}>0$ results in $x<-2$ not the correct solution, i.e., $x<-2 \cup x>0$ ? Thank you.","['inequality', 'algebra-precalculus']"
1110484,Inequalites of triangle side with $abc = 1$,"Let $a,b,c$ be the sides of a triangle with $abc=1$.
Prove that $$ \frac{\sqrt{b+c−a}}{a} + \frac{\sqrt{c+a-b}}{b} + \frac{\sqrt{a+b−c}}{c} \ge a+b+c $$","['holder-inequality', 'geometry', 'inequality', 'triangles', 'sum-of-squares-method']"
1110495,Characteristic of a ring: intuitive explanation,"I know the following definition of characteristic of a ring: it is the smallest positive $n$ such that $$\underbrace{a+\cdots+a}_{n \text{ summands}} = 0$$
for every element a of the ring, if $n$ exists, otherwise it is zero. However, I don't understand what is the ""intuitive"" meaning of this. Could you give a physical analogy of anything that may help to see what it is.","['physics', 'ring-theory', 'soft-question', 'abstract-algebra']"
1110580,Why don't global coordinates always exist for a manifold?,"Let $M$ be a manifold and $(\phi,U)$ a patch. Then $\phi(P)=\bar{x}=\begin{bmatrix}
x^1\\
x^2\\
\vdots\\
x^n
\end{bmatrix}$ for each $P$ in $U$. But each $P$ in $M$ is in some patch, so this representation must hold for each $P$ in $M$. If $M$ is to represent physical space (or spacetime), then each $P$ in $M$ should be represented by a unique $n$-tuple in $R^n$. All differential geometry texts that I have seen go to great lengths at the outset to claim that, in general, there is no global coordinate system for $M$, but that the best one can do is to find local coordinates. Granted that finding a global coordinate system by piecing the patches together might be difficult, it seems to me that it must be possible if differential geometry is to be useful in applications. Could someone straighten me out on this issue? Why not simply postulate a global coordinate system and let the coordinate mapping from $M$ to $R^n$ and the resulting metric tensor describe the geometry? P.S. By $R^n$ I mean the set of all $n$-tuples with the usual definitions of sum and scalar multiple, not Euclidean $n$-space.","['coordinate-systems', 'differential-geometry']"
1110593,How do you solve the equation $ (z^2-1)^2 = 4 ? $,"$ (z^2-1)^2 = 4  \iff $$z_1 = 3 $ and $ z_2=-1$ $arg(z_1)= 0 ,  arg(z_2) = \pi$ $$ z_1 = \sqrt{3} \left(\cos\left(\frac{2\pi k}{2}\right) + i\sin\left(\frac{2\pi k}{2}\right)\right)$$ $$z_2 = i \left(\cos\left(\frac{2\pi k}{2} +\pi\right) + i\sin\left(\frac{2\pi k}{2} +\pi\right)\right) \mid k= \{ 0,1 \}$$ $$ z = \{ \sqrt{3}, -\sqrt{3}, i, -i \}$$ Is this correct?","['trigonometry', 'complex-numbers']"
1110603,"Geometrical or Physical significance (interpretation) of the inner-product $\langle A,B \rangle := Trace (AB^t)$ over $M_n(\mathbb R)$","$\langle A,B \rangle := Trace (AB^t)$ is an inner product over the vector space $M_n(\mathbb R)$ of all real matrices of size $n$ , I would like to know whether this inner-product has any Geometrical or Physical significance (interpretation) or not ? Please shed some light . Thanks in advance","['trace', 'geometry', 'inner-products', 'linear-algebra', 'soft-question']"
1110618,Are the 14 Bravais lattices really distinct?,"I have learned that there are 14 distinct Bravais lattices in 3D and any other thought lattice form could be reduced to or expressed in one of these 14 forms. But the primitive unit cell for f.c.c lattice is seen to be a special case of trigonal (rhombohedral) lattice (with angles equal to 60 deg). A similar case is true for b.c.c lattice.
So, is f.c.c really distinct while it could be expressed as a special case of trigonal lattice?","['geometry', 'group-theory']"
1110626,What is the equation for this wave?,"So it would be hard to describe it, it's better to see it yourself: http://physics.info/waves/surface-wave.html (Angular velocity of rotating points is constant I presume) What is it called? What does it describe in real world and what's the y=f(x-t) type equation for it? I tried exp(cos(x-t)) but doesn't really match.... It must be something easy","['geometry', 'trigonometry', 'circles', 'rotations']"
1110634,Convergence of $f_n(x) = 2^n \cdot F(2^n (x-a_n))$ with $F(x) = e^{-x^2}$ with different notions of convergence.,"I had my measure theory exam this morning, and one exercise was the following: I really can't see a solution. During the semester, we talked about almost everywhere convergence, almost uniform convergence, convergence in measure an convergence in $L^p$. The problem is as follows: Let be $(a_n)_n$ be a real sequence and let $F(x)=e^{-x^2}$. Define
$$f_n(x)=2^nF(2^n(x-a_n))$$
for all $x\in \Bbb{R}$ and all $n \in \Bbb{N}$. Show if and in which meaning of convergence $F_n(x)$ converges to (some) $f$ and in that case if $$\int f=\lim_{n \to +\infty} \int f_n.$$ This is my first post, I hope I did not miss anything. My attempt:
I noticed that $$\int f_n=\sqrt{\pi}$$ (it's quite trivial to prove this, just some substitution) and so I think that $f_n$ can converge to $F$ in $L^p$, the proof will be easy if I find a way to show that $f_n<F$ or $F<f_n$ but I can't show that (if it's possible).","['lebesgue-integral', 'measure-theory', 'lebesgue-measure']"
1110635,Pole on a contour. Problem with integration,"I have a problem with calculation of the complex integral
$$\int_{|z|=1}\frac{z^2+3z+2i}{(z+4)(z-1)}dz$$
Apparently integrand has a pole in $1$ lying on our circle. What can I do? I cant use Cauchy formula here...","['residue-calculus', 'integration', 'complex-analysis', 'contour-integration']"
1110637,Is the standard scalar product in a coordinate space basis independent?,"Would you say that the standard scalar product in $K^n$, $\left< x,y \right>=\sum_i x_i y_i$, is basis-independent or not ? I would argue that it is, because we don't use the components of the vectors $x,y,$ to define this function, but the vectors themselves (although there is a basis, namely the standart basis, which, if we were to define the standard scaler product from the beginning in a basis-dependent manner - i.e. as $\left< x,y \right>_ {\mathcal{B}}=\sum_i ([x]_{\mathcal{B}})_i ([y]_{\mathcal{B}})_i$, where $\mathcal{B}$ denotes some basis of $K^n$ and $([x]_{\mathcal{B}})_i$ the $i$-th component of the coordinate vector of $x$ w.r.t. the basis $\mathcal{B}$ - would give the same function).","['linear-algebra', 'terminology']"
1110664,Issue with Spivak's Solution,Here was the problem: Here is the solution from his solutions book: This is barely a proof.  How can he just say let $f(c) = 0$? How do you prove that $f(c) =0$ and how do you prove that $f(d) = 0$? How can I use the IVT to prove that criterion? Thanks!,"['calculus', 'continuity', 'real-analysis', 'analysis']"
1110671,Solve this limit $\lim_{n\to \infty} \int_{0}^1 nxe^{-nx}dx$,"I would like to solve that limit: $$ \lim_{n \to\infty}  \int_{0}^1  nxe^{-nx} dx$$ I would like to take the limit inside the integral but for doing this, I have to use Beppo Levi's theorem or Dominated convergence Theorem. 
But the sequence of function is not increasing so I cannot use Beppo Levi's theorem, am I correct?","['convergence-divergence', 'integration', 'limits']"
1110676,How to solve $C = X^\top C X$?,All matrices are $n \times n$. $C$ is real symmetric positive definite. How to solve $C = X^\top C X$ for $X$? I am interested in characterizing both the set of real matrices satisfying the equation and the (possibly larger) set of complex solutions.,"['matrix-equations', 'matrices', 'matrix-calculus', 'linear-algebra']"
1110743,Voltera equation,"Consider the Voltera integral equation:
$$ψ(x)=e^{-x}\cos(x)-\int_{0}^{x}e^{-(x-t)}\cos(x)ψ(t)dt$$ How can I solve this equation by converting it to a differential equation? 
The solution is $$\psi(x)=\frac{\cos(x)}{e^{x+\sin(x)}}$$
I mean $$\psi(x)=\cos(x)e^{-(x+\sin(x))}$$","['ordinary-differential-equations', 'integral-equations']"
1110763,Intuition behind functional dependence,"What is the intuition behind functional independence ? (This is defined in the following way: Let $k\leq n$. The $C^1$ functions $F_1,\ldots,F_k:\mathbb{R}^n\rightarrow \mathbb{R}$ are functionally independent if the matrix whose columns are the gradients  $\nabla F_1,\ldots,\nabla F_k$ has full rank, i.e. rank $k$, on the whole domain of definition. From what I gather from this answer, this is the same as saying that $F:=(F_1,\ldots,F_k):\mathbb{R}^n\rightarrow \mathbb{R}^k$ is submersion, but that doesn't help me much either, because I also don't have any intuition concerning submersions.) So what does it really mean if the functions are functional indepedent - or conversely, dependent ? Is there, in the latter case, then also a relationship like $g(\nabla F_1,\ldots,\nabla F_k)=0$ -- or maybe like $g(F_1(x),\ldots F_k (x))=0$ for some $x$ -- for $g$ ranging in some specific set, similar to the case of linear independence ( in which $g$ would be from the set $\{g:\mathbb{R}^k\rightarrow \mathbb{R}:g(x_1,\ldots,x_k)=\sum \lambda_i x_i \text{ for some nonzero } \lambda_i \in \mathbb{R}\}$).","['linear-algebra', 'intuition', 'analysis']"
1110782,"Solve $\textbf y'=A\textbf y$ with $\textbf y\in \mathbb R^4$ and $A\in \text{Mat}(4\times 4,\mathbb R)$","We consider
  $$\textbf y'(t)=A\textbf y(t)$$
  with $\textbf y(0)=\textbf y_0\in \mathbb R^4$ and $A\in \text{Mat}(4\times 4,\mathbb R)$. Let $\textbf y_1,\textbf y_2,\textbf y_3,\textbf y_4\in\mathbb R^4$ linearly independant and $\lambda_1\neq \lambda_2\neq \lambda_3\in\mathbb C$ s.t.
  $$A\textbf y_1=\lambda_1\textbf y_1$$
  $$A\textbf y_2=\lambda_2\textbf y_2$$
  $$A\textbf y_3=\lambda_3\textbf y_3$$
  $$(A-\lambda_3)\textbf y_4=\textbf y_3.$$ Write the solution $\textbf y(t)$ in function of $\textbf y_0, \textbf y_i$ and $\lambda_i$. Give a condition such that $$\limsup_{t\to\infty }\textbf y(t)<\infty .$$ I agree that $\textbf y_1,\textbf y_2,\textbf y_3$ are eigenvectors and $\lambda_1,\lambda_2,\lambda_3$ eigenvalues, but what can I do with $\textbf y_4$ ? Because I can not diagonalize this matrix. And even if I could, I would like to calculate the $e^A$, but it doesn't look easy since I don't have the change of basis. It's probably not a complicate exercise, but like that, I can't continue.",['ordinary-differential-equations']
1110785,"integral ring extension, maximal ideals","Let $\varphi:A\rightarrow A'$ be an integral ring extension. 1) Show that for every maximal ideal $m'\subset A'$ the ideal $\varphi^{-1}(m')\subset A$ is maximal. 2) and that for every maximal ideal $m\subset A$ there is a maximal ideal $m'\subset A'$ with $\varphi^{-1}(m')=m$. Now there is a tip given: for $m\subset A$ look at $S=A-m$ and the localization $S^{-1}A$ and $S^{-1}A'$. Now I didn't get any further with this tip! Please help me! 1) What I did: $\varphi^{-1}(m')$ is a prime ideal (else $m'$ wouldn't be one in $A'$ as well, so also not maximal). Now I define the map $\psi:A/\varphi^{-1}(m')\rightarrow A'/m',a+\varphi^{-1}(m')\mapsto a+m$. Now $\psi$ is well defined as if $a-b\in \varphi^{-1}(m')$ then $\psi(a-b)=0\iff\psi(a)=\psi(b)$. So now if $x\in A'$ then there are $a_i\in A$ s.t. $x^n+a_1x^{n-1}+\cdots+a_n=0$. But this is inducing $(x+m)^n+(a_1+m)(x+m)^{n-1}+...+(a_n+m)=m$, so $\psi$ is an integral ring extension of integral domains (as $\varphi^{-1}(m')$ is prime) with $A'/m'$ a field, so then $A/\varphi^{-1}(m')$ is also a field and thus $\varphi^{-1}(m')$ is maximal in $A$. For 2) I didn't use that the extension is integral, so I'm not sure if it's correct at all: $m\subset A\Rightarrow \exists m'\subset A'$ s.t. $m\subset m'$. Now $\varphi^{-1}(m')=m$, because if $x\in\varphi^{-1}(m')-m$ then $\varphi^{-1}(m')$ is still an ideal, not containing $1$, as $m'$ is maximal, so its a proper ideal which contains $m$ in $A$.
So now here the only detail I'm missing is in this step: $\exists m'\subset A'$ s.t. $m\subset m'$. But I figured I can look at the ideal $(m)$ in $A'$, which is the ideal generated by every element of $m$, so then there exists a maximal ideal which contains the ideal $(m)$ properly, which isn't possible. Now I would only need to show that $(m)$ is a proper ideal, but im not sure how, if its true at all in general, but I guess it should be... So now my question is a) how can I use the tip? b) did I make any mistakes or not show something important?","['ring-theory', 'maximal-and-prime-ideals', 'abstract-algebra', 'proof-verification', 'commutative-algebra']"
1110845,Does $\wp(A \cap B) = \wp(A) \cap \wp(B)$ hold? How to prove it?,"I'm currently working on some discrete mathematics work and I've encountered a question I'm not sure how to answer exactly. Precisely, I'm trying to prove that two power, intersected sets statements are equal to each other and my understanding of how to do that doesn't seem to be enough. The statements in question go as follows: Determine whether, for any sets $A$ and $B$, through proof, it is true that
  $$\wp(A \cap B) = \wp(A) \cap \wp(B)$$
  where $\wp$ denotes a power set. Should I make some form of example set to better understand how these statements are equivalent? Or even a venn diagram? Any help is appreciated.",['elementary-set-theory']
1110872,How to prove $\sum_{n=0}^{\infty} \frac{1}{1+n^2} = \frac{\pi+1}{2}+\frac{\pi}{e^{2\pi}-1}$,How can we prove the following $$\sum_{n=0}^{\infty} \dfrac{1}{1+n^2} = \dfrac{\pi+1}{2}+\dfrac{\pi}{e^{2\pi}-1}$$ I tried using partial fraction and the famous result $$\sum_{n=0}^{\infty} \dfrac{1}{n^2}=\frac{\pi^2}{6}$$ But I'm stuck at this problem.,"['sequences-and-series', 'calculus', 'algebra-precalculus', 'real-analysis', 'summation']"
1110874,Examples of categorical adjunctions in analysis and differential geometry?,"In a lot of introductory texts on category theory, it seems like the majority of examples come from algebraic topology, algebra, and logic. Are there any good examples of adjunctions in analysis and differential geometry (including Lie theory)? Obviously we have products, coproducts, etc., but those are common to a lot of categories we work with. Are there any examples more specific to analysis and differential geometry? (Crosspost to Math Overflow )","['soft-question', 'category-theory', 'differential-geometry', 'analysis']"
1110908,Differential equation type,"How can I solve this differential equation
$$(1 + x^2)(1+y^2)\mathrm dx +xy\mathrm dy = 0$$ It doesn't look like separable and I don't think it's neither homogenous. Maybe I need to use the integration multiplier to solve it? I just need to know which way I should go about it, I don't need it solved.","['ordinary-differential-equations', 'calculus']"
1110928,A calculus proof for the general term of the Fibonacci sequence [duplicate],"This question already has answers here : Inductive proof of a formula for Fibonacci numbers (3 answers) Closed 9 years ago . Let $a_0=1$,$a_1=1$ and $a_n=a_{n-1} + a_{n-2}$ for $n \geq 2$, I would like to prove:
$$a_n=\frac{1}{\sqrt{5}}\left(\left(\frac{1+\sqrt{5}}{2}\right)^{n + 1}- \left(\frac{1-\sqrt{5}}{2}\right)^{n + 1} \right)$$
for $n\in \mathbb{N^*}$. I used induction to prove it (not sure if it is a correct way), is there a way to find it doing some calculus ? I couldn't come to that result.","['fibonacci-numbers', 'sequences-and-series', 'calculus']"
1110939,"Subgroup generated by $2$ and $7$ in $(\mathbb Z,+)$","In group $(\mathbb Z,+)$ , the subgroup generated by $2$ and $7$ is $\mathbb Z$ $5\mathbb Z$ $9\mathbb Z$ $14\mathbb Z$ In general what's the result for any $n$, $m$ instead of $2$ and $7$. I think it should be $\gcd(n, m)$. Is that correct?",['group-theory']
1110958,A periodic entire function which must have a fixed point,"I would like to check my work on the following problem: Suppose $f(z)$ is a non-constant periodic entire function satisfying $f(z+1)=f(z)$. Show that $f(z)$ has a fixed point. So my attempt is: Suppose $f(z)$ does not have a fixed point. Then $g(z)=f(z)-z$ is entire and never $0$. We compute $g(z+1)=f(z+1)-(z+1)=f(z)-z-1=g(z)-1$. But, since $g(z+1)\neq 0$ then $g(z)\neq 1$ for all $z$, hence $g(z)$ is a nonconstant entire function omitting $2$ values, contradicting Picard's theorem. Looks good?","['complex-numbers', 'complex-analysis']"
1110978,Finding the limit of a sequence of sequences,"Take any $\bar{r} \in \mathbb{R}$ with $\bar{r}>0$. Assume that $f : \mathbb{R} \rightarrow \mathbb{R} $ is continuous. Assume that for all $r\in [0,\bar{r})$, there exists a strictly decreasing real sequence $\{t(r)^n\}_{n=1}^{\infty}$ such that $t(r)^1 = r$, and for all $n \in \mathbb{N}\backslash\{1\}$, $ 0 < t(r)^n < r, \qquad$    (1) and $f\big(t(r)^n\big) - f\big(t(r)^{n-1}\big) \geq 0. \qquad$    (2) Question : is it possible to have $f(0) < f(\bar{r})$ or does it necessarily induce a contradiction? What I have done so far: I have tried to work by contradiction, assuming that $f(0) < f(\bar{r})$ holds. Because $t(\bar{r})^n$ is bounded by (1), it has a converging subsequence, say $t(\bar{r})^{m(n)} \rightarrow r^*_1$. We know from (1) and the fact that the sequence is strictly decreasing that $0 \leq r^*_1 < \bar{r}$. Using (2) repeatedly together with the continuity of $f$, we get $f(r^*_1) - f(\bar{r}) \geq 0$. Then by the contradiction assumption we obtain $f(r^*_1) > f(0)$. If $r^*_1 = 0$, we reached a contradiction and we are done. Otherwise, we can repeat the process again and again, getting $0 \leq r^*_2 < r^*_1$, $0 \leq r^*_3 < r^*_2$, ... But I do not see why we would necessarily reach some $r^*_{n}$ such that  $r^*_{n}=0$. Is it the case? Are their counterexamples?","['sequences-and-series', 'continuity', 'real-analysis', 'limits']"
1110983,Group theory problems manual,"It would be really a worthy contribution if someone please,From the point of view ,of covering all the problems which are based on application of theorems of group theory, recommend a manual of problems with solutions. The book which satisfies following conditions: It should contain large set of problems on group theory with solution Problems should NOT be Proofs.., Show That.. types but instead They SHOULD be based on their applications.example : number of generators of of cyclic group having the given order... or number of homomorphisms between given groups etc... 
In short it should not focus on problems involving proofs,But their applications. Book like Abstract algebra Problem and solution by ayman badawi. Which unfortunately is probably not available in SAARC countries. Is it possible to list such books along with links?","['group-theory', 'abstract-algebra']"
1111008,How could we prove the correctness of the algorithm?,"Consider two sets $D=\{ d_1, d_2, \dots, d_n\}$ and $E=\{ e_1, e_2, \dots, e_m \}$ and consider an other variable $K \geq 0$. 
Show that we can answer in time $O((n+m) \lg (n+m))$ the following question:
Is there is a pair of numbers $a,b$ where $a \in D, b \in E$ such that $|a-b| \leq K$? The algorithm should answer the  above question with YES or NO.
Describe the algorithm, show its correctness and show that its time complexity is $O((n+m) \lg (n+m))$. I wrote the following algorithm: int binary_search(int A[], int key, int low, int high, int K) 
{ 
   if (high < low) return 0; 
   else{ 
       int mid=low+floor((high-low)/2); 
       if (A[mid]<key-K) return binary_search(A, key, mid+1, high); 
       else if (A[mid]>k+key) return binary_search(A, key, low, mid-1 ); 
       else return mid; 
   } 
} 



Algorithm(D,E,K) {
  HeapSort(E); 
  low=1; 
  high=m; 
  i=1; 
  p=0 
  while (i<=n and p==0){ 
         p=binary_search(E, D[i], low, high, K ); 
         i++; 
  } 
  if (p==0) printf(""NO \n""); 
  else printf(""YES \n""); 
} We can prove the correctness of binary_search as follows, right? Base case: We suppose that we have an array of size 1. Then it holds $low=high=mid$. If the if-statement holds then we call the function binary_search(A, key, low+1, low) and since $high<low$, the function will return $0$. The result is right, because we have only one element that does not satisfy the desired property , we are looking further for an other elemenent,but since there is no other one, it means that the desired element doesn't exist. Similarly for the case when the else-if-statament holds. Furthermore, no matter which sie the array has, if the element we are looking at has the desired property, then the else-statement holds,so the algorithm returns the right position. Induction hypothesis: We suppose that binary_search gives the right result for arrays with size $<n$. Induction step: Now we consider an array of size $n$.
If the if- or else-if statement holds, we call the binary_search for an array of size $<n$, so from the induction hypothesis we know that we will have the right result. Otherwise, we will have the right result, from the base-case. But how can we prove the correctness of the algorithm Algorithm ? EDIT : That's what I thought when I wrote the algorithm: We sort the array E in order to call the binary_search that will return $0$ if there are no two elements $a,b$ of $D,E$ such that $|a-b| \leq K$ or the position at which the element of $E$ is, which we subtract with an element of D and become a difference by absolute value that is less or equal to $K$. So if the binary_search returns 0, the algorithm will return N0, otherwise it will return YES.","['computer-science', 'discrete-mathematics', 'algorithms']"
1111014,Limit without L'Hopital's rule: $\lim_{x\to0} \frac{1-\cos^3 x}{x\sin2x}$,How can I solve the following problem without the use of the L'Hopitals's rule? $$\lim_{x\to0} \frac{1-\cos^3(x)}{x\sin{(2x)}}$$,"['limits-without-lhopital', 'limits']"
1111041,Showing y≈x for small x if y=log(x+1),"Given: $y=\log(1+x)$ Show that $y≈x$ if $x$ gets small (less than 1). I don't think we're supposed to use Taylor series (because they were never formally introduced in class), but I do think we have to differentiate and show that the derivative of $\log(1+x)$ is approximately equal to $\log(1+x)$ on the interval $0$ to $1$. How should I show this?",['calculus']
1111075,What does $C^{\infty}_0$ stand for,"In my course material I have the following notation: $$f\in C^{\infty}_0(\Omega, \mathbb{R}),$$ where $\Omega \subset\mathbb{R}^n$ is a bounded open set. I was wondering what does this notation mean? What is the set $f$ belongs into?","['notation', 'functional-analysis', 'analysis']"
1111081,Lagrange inversion theorem application,"Can someone give me an example of where the Lagrange inversion theorem is applied in such a way it inverts a formal series? For example, say I have $$\sum_{i>-1} a_it^i = u.$$ Can someone show me the step by step process by which $$\sum_{i>-1}b_iu^i = t$$ is obtained. I can seem to find any links which ""dumb"" it down for me, or deal with series inversion.","['sequences-and-series', 'complex-analysis']"
1111089,"How to show $\lim_{n \to \infty} \sqrt[n]{a^n+b^n}=\max \{a,b\}$? [duplicate]","This question already has answers here : Convergence of $\sqrt[n]{x^n+y^n}$ (for $x, y > 0$) (2 answers) Closed 8 years ago . Let $a\geq 0$ and $ b\geq 0$. Prove that $\lim_{n \to \infty} \sqrt[n]{a^n+b^n}=\max \{a,b\}$. [Hint: Use the identity $(a^n -b^n)=(a-b)(\sum_{i=0}^{n-1}a^ib^{n-1-i})$] I need some help! I cannot do it even with the hint... :(","['sequences-and-series', 'radicals', 'real-analysis', 'means', 'limits']"
1111112,Condition that a local homeomorphism be a covering map.,"Let be $f:Y\to X$ a local homeomorphism, with $Y$ a compact space and $X$ a Hausdorff connected space. How can I show that, for each $x\in X$, $f^{-1}(x)\subset Y$ is finite? So, is clear that $f$ is surjective and I know to do this with the hypothesis that $Y$ is a metric space, but not when $Y$ is just a topological space. The whole question is: Show that: If $f:Y\to X$ a local homeomorphism, with $Y$ a compact space and $X$ a Hausdorff connected space then $f$ is a covering map.","['general-topology', 'connectedness', 'covering-spaces', 'compactness']"
1111122,Discrete Gauß and geodesic curvature,"Imagine that you have an n-polygon $S$ and you wanted to calculated the discrete Gaussian or gedoesic curvature. How are they defined? If $p$ is a vertex of $S$ then Gauß-Bonnet suggests that the full integral $$\sum_{ p \in S} K(p) = 2\pi - \sum_{i=1}^n \theta_i ,$$ where $\theta_i$ are the external angles. Also, one is tempted to interpret $$\sum_{p \in S} \kappa_g(p) = \sum_{i=1}^n \theta_i$$ as the full geodesic curvature. Are these two equations correct and if yes, how is the Gaußian and geodesic curvature of a single point defined in the discrete case? If anything is unclear, please let me know.","['discrete-mathematics', 'differential-geometry', 'polygons', 'discrete-geometry', 'curvature']"
1111138,using Taylor's Theorem to find region of convergence of series,"!( https://i.sstatic.net/zXski.jpg ) I am a third year Electrical engineering student, and I was going through one of the example from my math module lecture notes but couldn't understand the solution printed on the note. Could anyone please help me out? the link above is the image of part of the handout that contains the question. I don't know how to get 1 as the answer of part (b). the 'an' I circled on the sheet, does it mean 'un'? Could someone give me a step by step solution of how it gets to that function (where I underlined). where did the Z term go? thank you.","['complex-analysis', 'taylor-expansion']"
1111145,Fatou's Lemma and Counting Measure,"I have a vague problem in a Measure and Integration book here. They ask me to consider $\mu$ the counting measure in $\mathbb{N}$ and interpret Fatou's lemma, monotone and dominated convergence theorems as statements about infinite series. I thought it would be easy but got stuck really quick... If we consider a sequence of non negative functions $f_n\in L^1$, Fatou's lemma says that $$\int_\mathbb{N}\liminf f_n \ d\mu \leq \liminf\int_\mathbb{N}f_n \ d\mu.$$ The real problem comes when I try to understand what are this integrals. Starting with a simple function $\Phi =\sum_{i=1}^kx_i\mathcal{X}_{E_i}$, we have that $\int_\mathbb{N}\Phi \ d\mu = \sum_{i=1}^kx_i\mu(E_i) = \sum_{i=1}^kx_i|E_i|$, where $|E_i|$ is the number of elements in $E_i$. Each function $f_n$ is the $\sup$ of simple functions, but it's not clear how I should use all this together. Even if I consider a sequence $0\leq \Phi_1\leq\Phi_2\leq\ldots\leq f_n$ converging to $f_n$, they are not partial sums. To make things worse, there is infinite $f_n$ to consider and the $\liminf$ after that. Any help is welcome to interpret all this. Thank you.","['measure-theory', 'integration']"
1111179,How to show that $ \int_{-\frac{\pi}{6}}^{\frac{\pi}{6}} \ln\left(\tan x+\tan\frac{\pi}{6}\right)\tan x\space dx=\frac{\zeta(2)}{6} $,"I was trying to prove the well known result: $$
\sum_{k=1}^\infty \frac{1}{\binom{2k}kk^2}=\frac{\zeta(2)}{3}
$$ and it came down to prove the following integral $$
\int_{-\frac{\pi}{6}}^{\frac{\pi}{6}} \ln\left(\tan x+\tan\frac{\pi}{6}\right)\tan x\space dx=\frac{\zeta(2)}{6}
$$ Can this integral be proven without using the above mentionend sum? I tried applying the angle sum and difference identities but I didn't get something helpful, so any help is highly appreciated! Edit: Can it be shown directly, in other words without knowing $\zeta(2)=\frac{\pi^2}{6}$ , that $$
\sum_{k=1}^\infty \frac{1}{\binom{2k}kk^2}=\frac{\zeta(2)}{3}
$$","['sequences-and-series', 'calculus', 'integration', 'definite-integrals', 'trigonometric-integrals']"
1111186,Contraction of curves on a surface and stalks,"Let $$f:X \rightarrow S$$ be a fibered surface over a Dedekind scheme of dimension $1.$ Let $$s_1, \ldots, s_n$$ be closed points of S and $\{E_{ij}\}$ irreducible vertical divisors of $X$ with $E_{ij} \subset X_{s_i}$. I want to show that the contraction of the $E_{ij}$ exists if for $i \leq n$, the contraction of the $E_{ij}$ exists in the fibered surface 
$$X \times_S \text{Spec } \mathcal{O}_{S,s_i} \rightarrow \text{Spec } \mathcal{O}_{S,s_i}.$$ Here are some short thoughts on way one could show this: One way of showing that contraction morphisms of a set of integral vertical curves $\mathcal{E}$ exists on an arithmetic surface $X$ is to find an effective Cartier divisor $D$ on $X$ such that $\mathcal{O}_X(D)$  is globally generated, $\mathcal{O}_X|_{X_\eta}$ is ample and for any integral vertical curve $E$ not contained in $\mathcal{E}$, the sheaf $\mathcal{O}_X(D)|_E$ is ample. I have thought of trying to apply the above to find Cartier divisors $D_i$ on $$X \times_S \text{Spec } \mathcal{O}_{S,s_i} $$ wuith the above properties, and try to extend them all to $X$ and them combine them in some way to get a globally defined Cartier divisor $D$ with the required properties. I believe I can extend the Cartier divisors $D_i$ to all of $X$, and add them together so I actually get a contraction, but my method of doing so is very ugly and could easily contain mistakes. Could anyone give me a proof, or hint of how to do this cleanly? If you don't see any clean proof, can you see why we can extend the Cartier divisors to all of $X$  so that they add to a Cartier divisor with the satisfied properties? Thankful for help.","['arithmetic-geometry', 'algebraic-geometry']"
1111215,Prove that the eigenvalues of skew-symmetric matrices are purely imaginary numbers,Prove that all of the eigenvalues of skew-symmetric matrix are complex numbers with the real part equal to $0$ . Has anyone got a clue how to do it?,"['eigenvalues-eigenvectors', 'matrices', 'linear-algebra', 'skew-symmetric-matrices', 'complex-numbers']"
1111220,Why is it that while taking the inverse matrix a Wronskian pops up in this solution?,"I was working on an ordinary differential equation solution when I saw another way that could be used to solve using matrices such that
\begin{align*}
\left(\begin{matrix}
y_1\left(x\right) && y_2\left(x\right) \\ y_1'\left(x\right) && y_2'\left(x\right)
\end{matrix}\right)\bigg(\begin{matrix}
v_1'\left(x\right) \\
v_2'\left(x\right)
\end{matrix}\bigg)&=\left(\begin{matrix}0 \\ \frac{G\left(x\right)}{a}\end{matrix}\right)\tag{1} \\
\implies\int\left(\begin{matrix}v_1'\left(x\right) \\ v_2'\left(x\right)\end{matrix}\right) & =\int\frac{\left(\begin{matrix}0 \\ \frac{G\left(x\right)}{a}\end{matrix}\right)}{\left(\begin{matrix}
y_1\left(x\right) && y_2\left(x\right) \\ y_1'\left(x\right) && y_2'\left(x\right)
\end{matrix}\right)}\:dx\tag{2}
\end{align*}
But now, I've seen that (2) is re-written as
\begin{align}
\frac{\left(\begin{matrix}0 \\ \frac{G\left(x\right)}{a}\end{matrix}\right)}{\left(\begin{matrix}
y_1\left(x\right) && y_2\left(x\right) \\ y_1'\left(x\right) && y_2'\left(x\right)
\end{matrix}\right)}&=\frac{1}{W\left[y_1,y_2\right]\left(x\right)}\left(\begin{matrix}
y_1\left(x\right) && y_2\left(x\right) \\ y_1'\left(x\right) && y_2'\left(x\right)
\end{matrix}\right)^{-1}\tag{3}\\ &=\frac{1}{W\left[y_1,y_2\right]\left(x\right)}\left(\begin{matrix}
y_2'\left(x\right) && -y_2\left(x\right) \\ -y_1'\left(x\right) && y_1\left(x\right)
\end{matrix}\right)\tag{4}\end{align}
So my question is, why is the left-hand side of (3) the same as (4)? I have not taken linear algebra as of yet, but it seems the inverse of the matrix is taken because its inverse on top is the same as it regularly stands in the denominator. I understood the rest of the problem, it's just this step that I couldn't understand. It seems like introducing a Wronskian, just to ensure that the solutions are linearly independent, is like adding an extra term. But it definitely adds up to give the same results (somehow) since I still end up with
\begin{align}
v_1\left(x\right) & = -\int\frac{y_2\left(x\right)f\left(x\right)\:dx}{aW\left[y_1,y_2\right]\left(x\right)},\tag{5} \\ v_2\left(x\right) & =\int\frac{y_1\left(x\right)f\left(x\right)\:dx}{aW\left[y_1,y_2\right]\left(x\right)},\tag{6}
\end{align}
such that $f\left(x\right)$ is the forcing term on the RHS of the original ODE. Thank you for your time,","['matrices', 'ordinary-differential-equations']"
1111234,The Expectation of a function of independent random variables,"Assume we have for some index $i>n$ ($n \in \mathbb{N} $) the following ${\it Independent \ Random \ Variables}$ $$h_i \sim \text {i.i.d   }\ \ \mathcal{CN}(0,1)  \ \ \text{ Complex Gaussian}$$ $$\Omega_i \sim \text {i.i.d  with pdf }\ \ f_{\Omega_i}(\omega_i)$$
$$\gamma_i \in \Xi \ \ \text{ a Poisson Point Process with intensity } \lambda $$ I would like to find the ${\it Laplace}$ transform of the function $$Y=\sum\limits_{i \ >\ n} \gamma_{i}^{-1} |h_{i}|^2G(\Omega_{ i}) $$ 
where $G(.)$ is some function of $\Omega_{ i}$ (let us not go through the details of what $G$ is but assume we know it is defined from $-\infty$ to $\infty$.) This is the way I proceed \begin{align*} \nonumber
\mathcal{L}_Y(s)&= \mathbb{E}\left(e^{-s Y}\right)\\
 &=\mathbb{E}\Bigg[e^{-s \sum\limits_{i \ >\ n} \gamma_{i}^{-1} |h_{i}|^2G(\Omega_{ i})}\Bigg]\\\nonumber
&=\mathbb{E}\Bigg[\prod_{i>n}\bigg(e^{-s \ \gamma_{i}^{-1}|h_{i}|^2G(\Omega_{ i}) }\bigg)\Bigg]\\\nonumber
&\stackrel{(a)}{=}\mathbb{E}_{\{\Omega_{ i}\},\Xi}\Bigg[\prod_{i>n}\mathbb{E}_{|h|^2}\bigg(e^{-s \ \gamma_{i}^{-1}|h|^2 G(\Omega_{ i}) }\bigg)\Bigg]\\\nonumber
&\stackrel{(b)}{=}\mathbb{E}_{\{\Omega_{ i}\},\Xi }\Bigg[\prod_{{i>n}}\Bigg(\frac{1}{1+s \ G(\Omega_{ i})\ \gamma_{i}^{-1} }\biggl)\Bigg]\\\nonumber
&\stackrel{(c)}{=}\mathbb{E}_{\Xi }\Bigg[\prod_{{i>n}}\mathbb{E}_{\Omega}\Bigg(\frac{1}{1+s G(\Omega) \gamma_{i}^{-1} }\biggl)\Bigg]\\\nonumber
&\stackrel{(d)}{=}\mathbb{E}_{\Xi }\Bigg[\prod_{{i>n}}\int_{-\infty}^{+\infty}\Bigg(\frac{1}{1+s  G(\omega)\gamma_{i}^{-1} }\biggl) f_{\Omega(\omega})\ d\omega\Bigg]\\\nonumber
&\stackrel{(e)}{=} ?????
\end{align*} where (a) follows from the fact that the $|h_i|^2$ are independent, so the expectation of the product is the product of the expectation, (b) follows from the moment generating function of an exponential random variable because $h_i$ is Gaussian so $|h_i|^2$ is exponential; (c)same reasoning as (a) but for $\Omega_i$;(d)from the taking the expectation of a function of the random variable $\Omega_i$. Do you agree with the steps above ? Part 2: Next I would like to utilize the {\it probability generating function for the Poisson Point Process} to continue part (e) The formula is stated next, let $\Phi$ be a Poisson Point Process with intensity $\lambda$. Then $$\mathbb{E}\left( \prod_{x\in \Phi} v(x)\right)= \text{exp} \left(-\int_{\mathbb{R}^d} [1-v(x)]\lambda dx\right)$$ How can I use this theorem to continue step (e)? any thoughts? Thanks.","['laplace-transform', 'independence', 'expectation', 'probability-theory', 'moment-generating-functions']"
1111245,Does wikipedia state the definition of probability correctly?,"In the wikipedia article on probability http://en.wikipedia.org/wiki/Probability it says: To qualify as a probability, the assignment of values must satisfy the
  requirement that if you look at a collection of mutually exclusive
  events (events with no common results, e.g., the events {1,6}, {3},
  and {2,4} are all mutually exclusive), the probability that at least
  one of the events will occur is given by the sum of the probabilities
  of all the individual events That 'at least one' troubles me. If the events are mutually exclusive, how could the number ever be greater than one? Should it not better read ""exactly one""?","['probability-theory', 'probability']"
1111263,Convolution of two indicator functions can't be constant,"Let $A,B \subset S^1$ be measurable sets (considering $S^1$ with say the lebesgue measure).
I'm trying to prove that if the convolution $1_A*1_B$ is constant then one of $A$ or $B$ is a full measure set or one of them is of measure zero. I didn't make much progress but i'm pretty sure that the measure of the intersection of two sets can't be invariant to translation of one of the sets. It's just too pathological.
Thank you for the help.","['convolution', 'measure-theory']"
1111272,Closed form for $ \prod_{k=1}^n (a+k^2) $,"I have come across the following product:
$$ \prod_{k=1}^n (a+k^2) $$
where $a$ is a positive constant. Could anyone suggest a closed form for this product? I need to approximate this for large $n$ , but the problem is that $a$ is also of order of $n$ (it does not change with $n$, but it is also a large number).","['closed-form', 'sequences-and-series', 'calculus', 'products', 'asymptotics']"
1111279,Example of continuous function whose Fourier series doesn't converge on an uncountable dense set.,"According to a well-known theorem (Theorem 5.12 in Rudin's Real and Complex Analysis ), there is a dense $G_\delta$ set of continuous periodic functions $f:\mathbb{R}\to\mathbb{C}$ such that the Fourier series of $f$ does not converge for all $x$ in a dense $G_\delta$ subset of $\mathbb{R}$ (hence uncountable). This is completely mind blowing, especially the fact there is a whole dense $G_\delta$ set of such pathological functions. But is there at least one known explicit example of such a function?","['fourier-series', 'examples-counterexamples', 'real-analysis']"
1111281,Morphism $f\colon X\to S$ is proper iff $f^{-1}(V_j)\to V_j$ is proper for some open cover $\{V_j\}$ of $S$? (Lemma 28.42.3 of Stacks Project),"I was browsing the Stacks Project, and Lemma 28.42.3 says that a morphism $f\colon X\to S$ is a proper morphism if and only if there exists an open covering $S=\bigcup V_j$ such that $f^{-1}(V_j)\to V_j$ is proper for all $j$. However, the proof there is omitted. Can anyone fill in this gap, or have a resource which explains it? Thanks.","['algebraic-geometry', 'reference-request']"
1111296,Is $W^3(t)$ a martingale if $W(t)$ is a Brownian motion,"Is $W^3(t)$ a martingale if $W(t)$ is a Brownian motion? The answer seems like no to me. Using Ito's lemma I can write
$$W^3(t)=\frac{3}{2}W^2(t)+\int_0^t3W(u)dW(u)$$
The second piece on the LHS is an Ito integral and thus a martingale. However the first piece on the LHS in not a martingale and thus $W^3(t)$ is not a martingale. Can someone confirm or refute my argument (or perhaps make an argument of their own)?","['probability-theory', 'stochastic-calculus', 'brownian-motion']"
1111299,If $F: \mathbb{R}^{m} \rightarrow \mathbb{R}^{m}$ is continuous and $\| F(x) - F(y)\| \geq \lambda \| x - y \|$ is $F$ a surjection?,"In my real analysis class my professor gave us the problem of proving that if $F: \mathbb{R}^{m} \rightarrow \mathbb{R}^{m}$ is continuous and satisfies $\| F(x) - F(y)\| \geq \lambda \| x - y \|$ then $F$ is a bijection with continuous inverse. ($∥⋅∥$ is the Eucliden norm and $\lambda$ is some positive real number.) The problem of injectivity is easy enough since if $x \neq y$ then $\|F(x) - F(y)\| \geq \lambda \|x-y\| > 0$. Also given that F is a continuous bijection then the continuity of the  inverse $g$ is also obvious since fixing $x = F(u)$ and $y = F(v)$ we have that $\|g(x) - g(y)\| = \|u-v\| \leq \frac{1}{\lambda}\|x-y\|$  so g is Lipschitz and therefore continuous. My question is, how exactly is one supposed to prove surjectivity? It seems easy enough by intermediate value theorem if we restrict $F:\mathbb{R} \rightarrow \mathbb{R}$. But I can't seem to figure it out more generally. any hints would be much appreciated!",['real-analysis']
1111304,How to find efficient not transitive pairs in relations? (Discrete math),"I'm doing at the moment some math and struggle with the following. So there are relations and they can or can not hol specific properties.
Most common are described reflexive, symmetric and transitive. I know how they are defined, how they come together and analyse relations due that.
But what I'm really strugglign with are finding pairs which ARE NOT transitive from equivalence relations. So, xRy, yRz, then xRz. Thats transitiviy. But as soon as I got some equi. relation in set notation I rather struggle with checking the relation on the transitive property. 
Do you guys have any ideas how to proceed from here to make life easier for me?
How do I find rather fast a pair xRz which isn't in the relation?","['relations', 'discrete-mathematics']"
1111308,Proving that something is a manifold from the definition,"Consider a set $$M = \{ (s\cos t, s\sin t, t) \colon s,t\in \mathbb{R}\}\subset \mathbb{R}^3.$$ I am asked to show from the definition that $M$ is a 2-dimensional submanifold of $\mathbb{R}^3$ which is say for each $x\in M$ there is an open set $U$ (relative to $M$), an open set $V\subset \mathbb{R}^2$ together with a diffeomorphism $\varphi\colon U\to M$. Building $\varphi$ (subject to $x$) seems to be difficult. I tried to employ some version of theorem about local dipheomorphisms but no version seems to be applicable here. May I ask for help?","['multivariable-calculus', 'manifolds', 'smooth-manifolds']"
1111319,"If every composition of a differentiable path and a function is differentiable at 0, means the function is differentiable at 0","I'll write the question more formaly: Let $f :\mathbb{R^n} \rightarrow \mathbb{R}$ a certain function. Assume that for every differentiable path $p: [-1,1] \rightarrow \mathbb{R^n}$ so that $p(0) = 0 $ the composition $f(p(t))$ is differentiable at $t = 0$.  I need to prove that $f$ is differentiable at the zero vector. I came up with something, but i'm not sure if its correct, or too convoluted. What I came up with, was to assume by contradiction that $f$ is not differentaible. So I just need to find a path which makes the composition $f(p(t))$ not diffferentiable. Because $f$ is not differentiable at 0, by definition for every liner map $A:\mathbb{R^n} \rightarrow \mathbb{R}$ the expression: $$g(x) = \frac{|f(x) - f(0) - <A(0),x>|}{||x||}\not\rightarrow 0$$ So by the limit definition there exists a sequence $x_n \rightarrow 0$ so that $g( x_n ) \not\rightarrow 0 $. Again by definition because of the convergence of $x_n$, For every $\epsilon > 0$ there is a $n_0 \in \mathbb{N}$ so for every $n > n_0$, $x_n \in B(0,\epsilon)$. Now if I take a path inside this ball, perhaps just a straight line inside this ball, it will be a differentiable path. If I compose it with $f$ I need to show that: $$h(t) = \frac{|f(p(t)) - f(0) - a*t|}{|t|}  \not\rightarrow 0$$ So I just need one sequence $t_n \rightarrow 0$ so that $h(t_n) \not\rightarrow 0$. I'm sort of unsure of the next step, $g(x)$ and $h(t)$ look quite similiar. I can fix a linear map $A$ so that for every $x \not= 0$, $<A(0),x> = a*t_n$ for every $a \in \mathbb{R}$. And then because for every $t_n$, $p(t_n) = x$ so that $x \in B(0,\epsilon)$,  $h(t_n)$ and $g(x_n)$ will behave the same way, therefore $h(t_n)$ wont converge to $0$. This last part I'm having a hard time to explain formally, and also something seems wrong, but I can't quite figure out what. Any help will be greatly appreciated.","['multivariable-calculus', 'derivatives']"
1111333,Net convergence in metric spaces,"This is a question about convergence of nets which I don't quite understand yet. In metric spaces convergence of sequences encodes the topology but suppose we want to study convergence of nets even though. When can we pass to countable subnets? In other words, Given a net $(x_\lambda)_{\lambda\in \Lambda}$ in a separable metric space $X$ that converges to some $x\in X$. Can we find a countable subnet $\Lambda^\prime \subset \Lambda$ such that $(x_\lambda)_{\lambda\in \Lambda^\prime}$ converges to $x$?","['general-topology', 'convergence-divergence', 'metric-spaces']"
1111334,Reason for LCM of all numbers from 1 .. n equals roughly $e^n$,"I computed the LCM for all natural numbers from 1 up to a limit $n$ and plotted the result over $n$.  Due to the fast-raising numbers, I plotted the logarithm of the result and was surprised to find a (more or less) identity curve ($x=y$). In other words, $LCM(1, 2, 3, ..., n)$ appears to be roughly the value $e^n$. Is a there a simple explanation on why this is so? $LCM(a, b, c, …)$ shall be defined as the least common multiple of all arguments $a, b, c, …$","['exponential-function', 'least-common-multiple', 'number-theory']"
1111351,"Calculate a matrix to the power of ""n"" given an eigenvector","I have a question that I simply cannot solve. I do not want a direct answer to the question but simply an explanation as to the steps one would take to go about solving it, that way I can try it myself. To avoid getting an answer to the problem I wish to solve, I will use variables in place of actual matrices. Calculate $A^n$x where both A and x are known , and x is an eigenvector of A . A is a 2-by-2 matrix. For this question, I'm supposed to use the idea of linear combinations, which is what confuses me, I assume that means I'm supposed to solve it using the following format: (This is my take on it, it is most likely wrong) First find $\lambda$ -->  Ax = $\lambda$x Then --> x = a $\lambda$x Solve for a and then do something like --> $A^n$ = a$\lambda^n$x I'm really unsure as to how to solve this, can someone point me in the right direction. Perhaps show me step by step what I need to do.","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
1111361,Solutions of the functional equation $f(2x) = \frac{f(x)+x}{2}$,"How can I solve the following functional equation?
$$f(2x) = \frac{f(x)+x}{2},$$
for $x \in \mathbb{R}$ with $f$ being a continuous function.","['functions', 'calculus', 'real-analysis', 'functional-equations']"
1111399,Invert a $2\times 2$ Matrix containing trig functions [duplicate],"This question already has answers here : Find the inverse of a Trig Matrix (3 answers) Closed 9 years ago . Invert the $2\times 2$ matrix: \begin{bmatrix}
\cos\theta & -\sin\theta  \\
\sin\theta &  \cos\theta
\end{bmatrix} My thought was to append the  $2\times 2$ identity matrix to the right of the trig matrix and use row operations to get the answer. I have to show all steps, so I cannot just flip it and call it a day.","['matrices', 'trigonometry', 'inverse']"
1111425,"If G is finite group with even number of elements and has identity e, there is a in G such that a*a=e","My approach is ; I subtract e from G then G-{e} has odd number of elements. For any element in G-{e}, there must be an inverse of that element in G-{e}. Take any element in G-{e}, say b,  If b*b=e, then proof is done. If inverse of b is not itself, then there must be one element in G-{e}, say c, such that b*c=e. Because there are odd number of elements, I keep doing this until I have left with one element that does not have a pair, say d. any other elements in G-{e} can not be inverse of d because they all have pairs, and e can not be an inverse of d. Therefore d has to be inverse of itself. Is my proof valid? or Can anyone modify it please?","['linear-algebra', 'group-theory', 'abstract-algebra']"
1111436,Mathematical justification for incorporating a conditional event in expectation?,"Let $X_1,X_2,\dots$ be independent and identically distributed random variables. Furthermore, consider the sum $$ Y = X_1 + X_2 + \dots + X_N $$ where the number of terms $N$ is itself a random variable, independent of the $X_i$, all defined on the sampe probability space. Given this preamble, the text I am reading claims the following \begin{align}
E[Y|N=n] &= E[X_1+X_2+\dots+X_N|N=n] \\
&=E[X_1+X_2+\dots+X_n|N=n] \\
&=E[X_1+X_2+\dots+X_n]
\end{align} While I understand the second and third equalities from an intuitive perspective (we know $N=n$, so this information can be incorporated into the number of terms in the sum), how can this be derived mathematically, or in a more pedantic/rigorous fashion using the laws of probability? Do we need to consider the joint distribution of the $X_i$ and $N$? Any ideas would be appreciated.","['conditional-expectation', 'probability', 'random-variables']"
1111440,A die is rolled 3 times. What is the probability that a five is rolled at least twice?,"The probability of not getting a five is $(\frac56)^3$, and I figure the probability of getting at least one 5 is $1-(\frac56)^3$, but I don't know how to figure out if it is rolled at least twice. Thoughts? Thanks in advance!","['discrete-mathematics', 'probability', 'combinatorics']"
1111467,"Is it true that $\sum_{i,j=1}^n A^{i,j}x^iy^j \leq \sqrt{\left(\sum_{i,j=1}^n A^{i,j}x^ix^j\right)\left(\sum_{i,j=1}^n A^{i,j}y^iy^j\right)}?$","If $A$ is a symmetric and positive semidefinite matrix is it true that $$\sum_{i,j=1}^n A^{i,j}x^iy^j \leq \sqrt{\left(\sum_{i,j=1}^n A^{i,j}x^ix^j\right)\left(\sum_{i,j=1}^n A^{i,j}y^iy^j\right)},$$ where $x,y \in \mathbb{R^d}$? I thought Holder's inequality could be applied but I don't think it works for signed measure spaces. Thanks.","['inequality', 'matrices', 'linear-algebra', 'algebra-precalculus']"
1111484,A quickie about set theory notation,"I'm reading the first chapters of my discrete mathematics textbook and I couldn't help but wonder (perhaps I haven't seen enough examples) -- is it more appropriate to write that $a$ is an integer and within a set in the following way:
\begin{align}
a\in\left\{x\in\mathbb{Z}\big|\:b<a<c\right\},\tag{1}
\end{align}
or would it be better to break it up into two statements instead to have something like
\begin{align}
a\in\mathbb{Z},a\in\left(b,c\right)\tag{2}.
\end{align}
And I apologize in advance if some of you feel this is not a good question (perhaps too beginner-ish). I would just like to get all of the formalities down so that in the future I don't make a mistake. Thanks,","['notation', 'elementary-set-theory']"
1111504,Differentiation with respect to a matrix (residual sum of squares)?,"I've never heard of differentiating with respect to a matrix. Let $\mathbf{y}$ be a $N \times 1$ vector, $\mathbf{X}$ be a $N \times p$ matrix, and $\beta$ be a $p \times 1$ vector. Then the residual sum of squares is defined by
$$\text{RSS}(\beta) = \left(\mathbf{y}-\mathbf{X}\beta\right)^{T}\left(\mathbf{y}-\mathbf{X}\beta\right)\text{.}$$ The Elements of Statistical Learning , 2nd ed., p. 45, states that when we differentiate this with respect to $\beta$, we get
$$\begin{align}
&\dfrac{\partial\text{RSS}}{\partial \beta} = -2\mathbf{X}^{T}\left(\mathbf{y}-\mathbf{X}\beta\right) \\
&\dfrac{\partial^2\text{RSS}}{\partial \beta\text{ }\partial \beta^{T}} = 2\mathbf{X}^{T}\mathbf{X}\text{.}
\end{align}$$
I mean, I could look at $\mathbf{y}$ and $\mathbf{X}$ as ""constants"" and $\beta$ as a variable, but it's unclear to me where the $-2$ in $\dfrac{\partial\text{RSS}}{\partial \beta}$ comes from, and why we would use $\beta^T$ for the second partial. Any textbooks that cover this topic would be appreciated as well. Side note : this is not homework. Please note that I graduated with an undergrad degree only, so assume that I've seen undergraduate real analysis, abstract algebra, and linear algebra for my pure mathematics background.","['matrices', 'matrix-calculus', 'linear-algebra']"
1111518,Differential Equations Hermitian Matrix Proof,"I would like some help with the following proof below. Thanks for any help in advance. Prove that if $u(t) ∈ \mathbb C^N$ is a solution to the initial value problem $iu’ =Au$, $u(0)=u_0$,
where $A$ is hermitian, then $||u(t)|| = ||u_0||$.","['matrices', 'ordinary-differential-equations']"
1111522,A future in mathematics [closed],"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 9 years ago . Improve this question I am a Junior in high school right now, trying to figure out what to do next mathematically.  I have familiarity with real analysis (Baby Rudin, and also a bit on the gauge integral), complex analysis (mostly contour integration), and am building a good algebra background (from Birkhoff, with help from Dummit and Foote).  I also have familiarity with many disparate topics (e.g. Fractional calculus, and elliptic curves).  I'm interested in studying...everything really, but I am currently trending towards algebraic geometry, after burning out a bit on analysis (I went from Calc AB through Rudin and beyond in the summer).  My problem is that I haven't really had guidance since auditing Differential Equations and Calc 3 over the summer, other than answers to elliptic curve questions from a prof at a semi-local university (there is nothing within an hour of me).  I've emailed a few professors at big universities, but so far response has been lukewarm (mostly due to time constraints for them).  I was hoping someone on this site, which I've sort of used as a teacher, could point me somewhere productive.  Maybe a math department that might be receptive, or a guide on what other topics to study in order to give myself a proper maths background?  Thanks in advance! (Also, I understand that this should probably be a community wiki, but don't know how to make it one).","['education', 'algebraic-geometry', 'abstract-algebra']"
1111536,When is the quotient space of a second countable space second countable?,"I am a bit confused about this concept because I have read that the quotient space is second countable if the quotient map is open. However, I thought the definition of a quotient map was a surjective, continuous, open mapping. Suppose that $X$ is a second countable topological space, and ~ is an equivalence relation. The canonical mapping $q: X \rightarrow X$/~ is a quotient map, so $X$/~ would also be second countable. Where am I going wrong with this idea?","['open-map', 'examples-counterexamples', 'general-topology', 'quotient-spaces', 'second-countable']"
1111574,Compact operator space is the greatest ideal of $B(H)$,"Suppose $H$ is a separable infinite dimensional Hilbert space. Show that if $A\in B(H)$ is noncompact, then there exist two operators $B,C$ such that $BAC=1$. Clearly if $A$ is invertible it holds, but if $0\in \sigma(A)$, I do not have any idea. Please help me. Thanks.","['operator-theory', 'compact-operators', 'functional-analysis', 'c-star-algebras']"
1111590,$\int_0^\infty\int_0^\pi\frac{k^2(e^{-it\sqrt{k^2+m^2}}-e^{it\sqrt{k^2+m^2}})\sin(\theta)}{e^{-ikx\cos{\theta}}\sqrt{k^2+m^2}}d\theta dk$,"$$\int_0^\infty\int_0^\pi\frac{k^2\left(e^{-it\sqrt{k^2+m^2}}-e^{it\sqrt{k^2+m^2}}\right)\sin(\theta)}{e^{-ikx\cos{\theta}}\sqrt{k^2+m^2}}d\theta dk$$ I saw this Integral at Quora, and I have not idea how to evaluate it. Therefore, I thought of posting it here. How do we Evaluate this?","['definite-integrals', 'improper-integrals', 'calculus', 'integration']"
1111602,"For $f$ an analytic function, what is","$f$ be analytic function, could any one tell me how to find the value of $$\int_{0}^{2\pi} f(e^{it})\cos t \,\mathrm dt$$ I am not able to apply any complex analysis result here, could any one give me hint?","['integration', 'complex-analysis']"
1111616,"weak convergence of $L^2$ implies weak convergence of $W_0^{1,2}$ (up to a subsequence)?","In the paper that I am reading, it says that if $\{u_n\}$ are bounded in $W_0^{1,2} (\Omega)$ (bounded $\Omega\subset \mathbb{R}^N$)  and $u_n \rightharpoonup u$ weakly in $L^2 (\Omega)$, then there exist a further subsequence such that $u_{n_k} \rightharpoonup u$ in $W_0^{1,2}(\Omega)$. How do we know it is still the same limit? I know that $I: W_0^{1,p}(\Omega) \rightarrow L^2(\Omega)$ is continuous and dense for bounded $\Omega$ and $p\geq 2$. And here is my second question, given $X,Y$ are Banach spaces and $T:X\rightarrow Y$ is continuous and dense, can we say if $x_n \rightharpoonup x$ weakly in $X$, then $T(x_n) \rightharpoonup T(x)$ weakly in $Y$?
If this is true, then the above is clear, since the subsequence also converges to $u$ weakly in $L^2$.
If not what other conditions we need? I know compact is too strong since we actually get if $x_n \rightharpoonup x$ weakly in $X$, then $T(x_n) \rightarrow T(x)$ strongly in $Y$. Thank you very much!","['sobolev-spaces', 'weak-convergence', 'operator-theory', 'real-analysis', 'functional-analysis']"
1111649,Calculate the number of Sylow $p$-subgroups of $A_5$,"Calculate the number of Sylow $p$-subgroups of $A_5$ We have $|G|=60=2^2\cdot 3\cdot 5$ Let $n_p$ be the number of Sylow $p$-subgroups of $G$. By Sylow's third theorem, we have $n_3\in\{1,4,10\}$. But $G$ contains $20$ elements of order $3$, each of which generates a Sylow $3$-subgroup, so $n_3=10$ Similarly $n_5\in\{1,6\}$ and contains $30$ elements each generating a Sylow $5$-subgroup so $n_5=6$ Consider $n_2$. By Sylow's third theorem, we have $n_2=\{1,3,5,15\}$ (1) Clearly, $n_2\in\{5,15\}$ since $G$ contains $15$ elements of order $2$ (2) If $n_2=15$ then $|G:N_G(H)|=15$ thus $H=N_G(H)$ (3) but this is false since $(1,2,3)\in N_G(H)\backslash H$ Therefore $n_2=5$ Can someone clarrify any of the three lines in bold (lines (1),(2),(3)) Line (1), why is $3$ excluded from the list for $n_2$ Line (2), $N_G(H)$ is the normalizer of $H$ in $G$ Please comment if you can clarify any of the lines, any help would be greatly appreciated.","['symmetric-groups', 'finite-groups', 'group-theory', 'abstract-algebra']"
1111651,Regular sequence of sections of line bundles over a coherent sheaf,"I am reading the first chapter from the book by Huybrechts and Lehn, where I encountered the following definition. I have the following doubts regarding this definition : What is the map $s:E\otimes L^\vee\longrightarrow E$? $E\otimes L^\vee \cong\mathcal{Hom}(L,E)$. So we have $s:\mathcal{Hom}(L,E)\longrightarrow E$. For any open set $U$ of $X$, $s(U):\mathcal{Hom}(L|_U,E|_U)\longrightarrow E(U)$ could be defined as $s(U)(\phi)=\phi(s|_U)$. This is my guess for the map $\phi$. Is this correct? They talk about $(s_1,s_2,\cdots,s_{i-1})(E\otimes L^\vee)$. What is this object? In case of a module $M$ over a commutative ring $R$ and an ideal $I$ in $R$, we know what $IM$ is. Is $(s_1,s_2,\cdots,s_{i-1})(E\otimes L^\vee)$ something analogous to that? How is that defined? What is the zero set $H$ of a section $s\in H^0(X,L)$. Since this $H\in|L|$, I am guessing that it could be the divisor of zeros of the section $s$ i.e. $(s)_0$ as denoted by Hartshorne. Is this $(s)_0$ the same as the set $\{x\in X|s_x\in\mathcal{m}_xL_x\}$, because this is the set which is generally called the zero set of $s$. And finally, what is this definition really saying. I am not able to really understand this definition. For example, what is a regular sequence for $L=\mathcal{O}(1)$ for a projective scheme $X$ over a field $k$. How can I find that out using this definition. If someone can clarify these doubts, or direct me to some references, I will be grateful! Thanks in advance!","['sheaf-theory', 'algebraic-geometry', 'schemes', 'vector-bundles']"
1111686,Classification of indecomposable modules over a given ring,"Let $K$ be a field, $x$ an indeterminate and $n$ a positive integer. How can one classify all the indecomposable modules up to isomorphism over the ring $R=K[x]/(x^n)$ ?","['modules', 'commutative-algebra', 'abstract-algebra']"
1111694,"Exercise: Evaluating integration $\int_{|z|=r} \frac{1}{(z-a)(z-b)}dz$, $|a|<r<|b|$","This is an exercise from Stein-Shakarchi's Complex Analysis: evaluate integration $$\int_{|z|=r} \frac{1}{(z-a)(z-b)}dz, \,\,\,\, |a|<r<|b|. $$ The problem I am facing is the following. It is sufficient to find $\int_{|z|=r} \frac{1}{z-a}dz$ and $\int_{|z|=r} \frac{1}{z-b}dz$ (and use partial fraction methd). This exercise is in first chapter, where the author introduces the integration of $f$ over a parametrized smooth curve $\gamma$. However, I didn't find any theorem in first chapter applicble to evaluate this integration. I tried to evaluate it through parametrization $\gamma(t)=re^{it}$ for $0\leq t\leq 2\pi$. Then $$\int_{|z|=r} \frac{1}{z-a}dz=\int_0^{2\pi} \frac{rie^{it}}{re^{it}-a}dt$$. But I couldn't solve this last integration. Can you help me? I have seen that this can be solved using some Cauchy's integration formua; BUT, this is taken in second chapter of the book, whereas this exercise is in first chapter.",['complex-analysis']
1111729,"$\frac{\sin(nx)}{\sin(x)}=(-4)^{(n-1)/2} \prod_{1\leq j \leq (n-1)/2}(\, \sin^2(x)-\sin^2(\frac{2\pi j}{n})\,)$","In Serre's A Course in Arithmetic, it states For $n$ odd and positive integer, proof that $\frac{\sin(nx)}{\sin(x)}=(-4)^{(n-1)/2} \prod_{1\leq j \leq (n-1)/2}(\,\sin^2(x)-\sin^2(\frac{2\pi j}{n})\,)$ It says this is elementary, and suggest first proving that $\frac{\sin(nx)}{\sin(x)}$ is a polynomial of degree $(n-1)/2$ in $\sin^2(x)$, then remark that it has $\sin^2(\frac{2\pi j}{n})$ as roots, then comparing coefficients of $e^{i(n-1)/2}$ to get $(-4)^{(n-1)/2}$. But I can't find useful form of polynomial to get roots, help please, other proof strategies are welcome.","['special-functions', 'functions']"
1111730,Prove that the intersection of $BM$ and $CN$ is on the circumcircle of triangle $ABC.$,"Let $P$ and $Q$ be on segment $BC$ of an acute triangle $ABC$ such that $\angle PAB$ = $\angle BCA$ and $\angle CAQ = \angle ABC$.Let $M$ and $N$ be the points on $AP$ and $AQ$, respectively, such that $P$ is the midpoint of $AM$ and $Q$ is the midpoint of $AN$.Prove that the intersection of $BM$ and $CN$ is on the circumcircle of triangle $ABC.$","['geometry', 'triangles', 'circles']"
1111738,Elementary way to show $\lim_{n \rightarrow \infty} \sqrt[n]{a_n} = \lim_{n \rightarrow \infty} \frac{a_{n+1}}{a_n}$?,"Let $a_n \gt 0$ for $n \in \mathbb{N}$. The convergence radius of the series $\sum_{n=0}^\infty a_n z^n$ is $\frac{1}{q}$ with $q = \lim_{n \rightarrow \infty} \sqrt[n]{a_n}$ or $q = \lim_{n \rightarrow \infty} \frac{a_{n+1}}{a_n}$, if these limits exist. Therefore the $\lim$s must be identical. However I was wondering whether there exists a more elementary way to show this identity? (Or generally any other way?)","['radicals', 'limits']"
1111748,Domain of a composition of log functions,"Domain of $$\log_3(\log_{1/3}(\log_4(\log_{1/4} x)))$$ Please guide me to solve this problem. Since it is a composition of logs, I am confused how to start.","['calculus', 'functions']"
1111761,What is the difference between coordinates transformation and change of coordinates?,"In the context on 3D computer graphics, what is the difference between coordinates transformation and change of coordinates ? It can just be a matter of notation, but my book makes a clear distinction between the 2 terms (that I do not understand completely). As far as I understand, change of coordinates means to change the reference frame of all points expressed in a given frame and coordinates transformation means, given a set of points (in some reference frame), use one of them as the new origin (of a new reference frame) and express all the others in terms of that one. It seems to me that the coordinates transformation concept is similar to global and object coordinates in 3D computer graphics applications (like openGL), but then again they seem highly similar. Can you make some examples to point out the difference (if any)?","['coordinate-systems', '3d', 'matrices', 'transformation', 'terminology']"
1111766,$U(n) \simeq \frac{SU(n) \times U(1)}{\mathbb{Z}_{n}}$ isomorphism,"I'm trying to proof the following isomorphism $$U(n) \simeq \frac{SU(n) \times U(1)}{\mathbb{Z}_{n}}$$ So I'm using the first Isomorphism theorem: http://en.wikipedia.org/wiki/Isomorphism_theorem It's easy to show that the following map is an homomorphism: $$f: SU(n) \times U(1) \rightarrow U(n): (S,e^{i\varphi}) \mapsto e^{i \varphi} S$$ But I'm having troube to show that: $$Ker f = \mathbb{Z}_{n}$$ How am I supposed to do that ?","['group-theory', 'group-isomorphism']"
1111804,"Prove a convergent sequence has either a minimum, a maximum or both.","Let $a_n$ be a convergent sequence. Prove $a_n$ has a minimum, a maximum or both. I am being prepared for a final exam, which is why it is important to me to know that $I$ am correct in $my$ attempt. Of course if I am completely wrong, hints or solution are welcome. Thanks. $Attempt$: $a_n$ converges to a limit $L\in \Bbb{R}$ as $n\to \infty$. Therefore, for $\epsilon=1$ we get, for large enough $N$ that $\forall n\ge N,$ $|a_n-L|<1$ $\Rightarrow$ $L-1 \le a_n\le L+1$, in particular $a_n\le L+1$. Therefore, for $M=max(a_1,a_2,...,a_N,L+1)$, $a_n\le M$ necessarily. i.e, $a_n$ is upper bounded. The same can be shown with lower bound. If $a_n$ is constant, we are done. Otherwise, the lower and the upper bounds are different. Suppose $a_n$ has no minimum nor maximum, then both lower and upper bound are accumulation point, a contradiction. Therefore $a_n$ has a maximum or a minimum in that case.","['sequences-and-series', 'convergence-divergence', 'calculus', 'limits']"
1111805,Using the Existence and uniqueness theorem to find the interval of validity of a unique solution in a Linear ODE,"From the existness and uniqueness Theorem,the initial value problem $$y'=3x(y-1)^{1/3}            ,          y(3)=-7$$ has a unique solution on some open interval that contains $x=3$. Find the solution and determine
the largest open interval on which it’s unique. What i tried,
             First i tried to solve the equation by the seperable equation method to get,$$y=1+(x^2-5)^{1.5}$$. Then from here i used the existence and uniquness theorem to calculate $f(x,y)$ and $f_{y}$. From the calculations,when $y$ not equals to $1$, $f_{y}$ will be continuous, hence there will be a unique solution when $y$ not equals to $1$, according to the wxistness and uniquness theorem. However im stuck from here onwards as to finding the interval of validaty. Is my working correct. Could anyone explain. Thanks",['ordinary-differential-equations']
1111837,"$f \in \mathcal{L}^1(\mathbb{R}) \cap \mathcal{C}^1(\mathbb{R}),f' \in \mathcal{L}^1(\mathbb{R}) \Longrightarrow f \in \mathcal{C}_{0}(\mathbb{R})$",I want to show the following theorem: Suppose $f \in \mathcal{L}^1(\mathbb{R}) \cap \mathcal{C}^1(\mathbb{R})$ and $f' \in \mathcal{L}^1(\mathbb{R})$. Then it holds that $f \in \mathcal{C}_{0}(\mathbb{R})$. How can I show that $\lim_{|x| \to \infty} f(x)=0?$,"['real-analysis', 'analysis']"
1111841,Proof by Iteration,"It seems that I suffer the ""too-much-logic-too-pedantic-too-confused""-disease. (You know? This very disease which lets you doubt everything and lets you yell for formalized proof. It's annoying, especially in real life.) 
The last days I've been trying to understand a certain argument which uses ""iteration"" (it is out of Hatcher's ""Algebraic Topology"", the proof for proposition 2B.1. a), page 169 -- but it doesn't matter, I would say). The generalized problem Generalized, what I want to proof are the following two claims: 1) For an intervall $I$, assuming $A(I)$, one can construct an intervall $J$, such that $J \subsetneqq I$, $length(I) = 2\cdot length(J)$ and that $A(J)$ holds. 2) Then , by iteration , one has intervals $$I_0 \supsetneqq I_1 \supsetneqq \ldots$$ with $A(I_m)$ for every $m\in\mathbb{N}$. Where I stumble While 1) just goes through by instantiating an arbitrary interval $I$ and constructing a $J$, part 2) seems quite hard to conclude, but just in a detail. In fact, 1) is the inductive step with the exception, that I have nothing which is a successor of something, as $J$ was constructed within the induction. If I would have a sequence of intervals $\{I_m\}_{m\in\mathbb{N}}$, then of course, I could just run induction on it, using 1). But $I$ and $J$ are not related in this sense. Ideas The very first idea is: I'm just confused. This can happen from time to time, especially the more (mathematical) logic I consume. And I did. The second idea: The sequence of intervals I would need to construct unravels (don't know if this is the right word here...). Take $I_0 = [0,1]$ for example. There are two possibilities (in Hatcher's proof) $[0,\frac{1}{2}]$ and $[\frac{1}{2},1]$. If it was the first, the sequence continues with either $[0,\frac{1}{4}]$ or $[\frac{1}{4},\frac{1}{2}]$. As I know the borders explicitly, maybe I could modify 1) into something where $$J = [\frac{b}{2},b]\text{ or }[a,\frac{b}{2}]$$ and use this? Not sure. The third idea: putting the borders as a ""depending sequence"", $a_{n+1} = f(a_n)$, but I have no clue what that should be. Thanks for any help. PS: Of course, this is just about formalizing as I cannot see how to give a more pedantic proof. I see the argument, but saying ""then run the argument again"", which has to happen infinite times, is not a proof at all, as it would need infinite steps.","['logic', 'induction', 'elementary-set-theory', 'recursion']"
1111868,"If $G$ is a group of even order, prove it has an element $a\neq e$ satisfying $a^2=e$. [duplicate]","This question already has answers here : Group of even order contains an element of order 2 (2 answers) Closed 7 years ago . If $G$ is a group of even order, prove it has an element $a \neq e$ satisfying $a^2 = e$ . My proof: Let $|G| = 2n$ . Since $G$ is finite, there exists, $a \in G$ such that $a^p = e$ and by Lagrange's Theorem, p divides 2n. By Euclid's lemma, since p does not divide 2, p divides n. Let $n = pk$ . Hence, $(a^n)^2 = (a^{pk})^2 = ((a^p)^k)^2 = (e^k)^2 = e$ . Therefore, $a^n$ is an element that satisfy the condition. Is my solution OK? For this problem, I am just wondering how I can solve this problem without using Lagrange's Theorem, as this problem is an exercise before the Lagrange's Theorem was taught.","['finite-groups', 'group-theory', 'abstract-algebra', 'solution-verification']"
1111872,Applying iterated function on the sum of the squares of the prime factors of $30$,"Let $f(n)$ denote the sum of the squares of the prime factors of $n$ with multiplicity. For example, $f(60)=f(2\cdot2\cdot3\cdot5)=2^2+2^2+3^2+5^2=42$. Denote the iterated function $f^k(n)=\underbrace{f(f(\dots(n)))}_{k\text{ times}}$. For example, $f^2(60)=f(f(60))=f(42)=2^2+3^2+7^2=62$. I want to know if there exists $k$ such that $f^k(30)$ is either prime or equal to $30$. Is there a way to answer this question without applying a full ""brute-force"" search? My goal is to be able to answer this question in general (for values other than $30$). Side note: I do not know any cases where $f^k(n)=n$, other than $n=16$ and $n=27$.","['prime-numbers', 'sums-of-squares', 'number-theory']"
1111873,Optional Stopping Theorem for Stochastic Processes with Constant Mean (and not a Martingale),"Let $(X_n)_{n\geq 1}$ be a martingale with respect to $(Y_n)_{n\geq 1}$, i.e., the martingale condition
$$
 \mathbb{E}[X_n|Y_1, \ldots, Y_{n-1}] = X_{n-1}
$$ 
holds. From this condition, it follows that the process $(X_n)$ has constant mean $\mathbb{E}[X_n] = \mathbb{E}[X_1]$ for all $n$. Under some assumptions, we then also have $\mathbb{E}[X_T] = \mathbb{E}[X_1]$, where $T$ is a stopping time. My question is, does this optional stopping theorem also apply to processes that have constant mean, but do not fulfill the martingale condition above? An example for such a process was given here: Example of a sequence of r.v.'s with constant stopping time that is not a Martingale","['probability-theory', 'martingales', 'stopping-times']"
1111894,Random points in spherical shell,"I have a sphere of radius $R_1$, and a smaller, concentric sphere of radius $R_2$. Let them be centered at the origin $(0,0,0)$. I need to generate random points with uniform density in the volume between the two sphere surfaces. Here I found a solution for picking random points in a sphere volume, using $$\frac{R_s U^{1/3}}{\sqrt{X_1^2 + X_2^2 + X_3^2}} (X_1, X_2, X_3)$$ where $X_1,X_2,X_3$ are independent normal random variables with mean 0 and variance 1, and $U$ is uniformly distributed between 0 and 1. That strategy could be used for my problem as well, combining it with a rejection method: Generate a random point $(x,y,z)$ within $R_1$, using the abovementioned solution If $x^2 + y^2 + z^2 \leq R_2^2$, go back to step 1 But this is quite inefficient when the spherical shell is small. I guess it is possible to use a transformation method to generate points between $R_1$ and $R_2$. Can this be done?","['geometry', 'random-variables']"
