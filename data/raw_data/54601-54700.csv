question_id,title,body,tags
584563,Trigonometric Limit: $\lim_{x\to 0}\left(\frac{1}{x^2}-\frac{1}{\tan^2x}\right)$,"I cannot figure out how to solve this trigonometric limit: $$\lim_{x\to 0} \left(\frac{1}{x^2}-\frac{1}{\tan^2x} \right)$$ I tried to obtain $\frac{x^2}{\tan^2x}$, $\frac{\cos^2x}{\sin^2x}$ and simplify, and so on. The problem is that I always go back to the indeterminate $\infty-\infty$ Has someone a different approach to solve this limit?","['trigonometry', 'limits']"
584564,Relation between exponential map and parallel transport [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question I'm starting to learn Riemannian geometry and have a question. Let $\mathcal{M}$ be a Riemannian manifold, $p \in \mathcal{M}$; $\tau_{p}^{q}$ be a parallel transport from $p$ to $q$ and $\operatorname{exp}_p$ be an exponential map. The question is: given a smooth curve $\gamma : [a, b] \to \mathcal{M}$ such that $\gamma([a, b]) \subset \mathcal{U}$, where $\mathcal{U}$ is some neighborhood of $p$ where $\operatorname{exp}_p, \tau_{p}^{(\cdot)}$, are defined, is it true that
  \begin{equation}
    \left(
      \frac{\mathrm{d}}{\mathrm{d}t}
      \operatorname{exp}_p^{-1}\gamma 
    \right)(t)
  = \tau_{\gamma(t)}^{p}\dot{\gamma}(t) \,?
\end{equation}","['riemannian-geometry', 'differential-geometry']"
584601,Is this function injective / surjective?,"A question regarding set theory. Let $g\colon P(\mathbb R)\to P(\mathbb R)$, $g(X)=(X \cap \mathbb N^c)\cup(\mathbb N \cap X^c)$ that is, the symmetric difference between $X$ and the natural numbers. We are asked to show if this function is injective, surjective, or both. I tried using different values of $X$ to show that it is injective, and indeed it would seem that it is, I can't find $X$ and $Y$ such that $g(X)=g(Y)$ and $g(X) \neq g(Y)$ but how do I know for sure? How do I formalize a proof for it? Regarding surjective: I think that it is. We can take $X=\mathbb R - \mathbb N$ and we get that $g(X)=\mathbb R$ What do I do about the injective part?","['elementary-set-theory', 'functions']"
584608,"Compute $H^1(X,\Bbb{Z}_U)$","Let $X = \mathbb{A}^1_k$ with $k$ infinite and $U = X - \{P,Q\}$ and $\mathbb{Z}_U= i_{!}(\mathbb{Z}|_U)$, $\Bbb{Z}$ the constant sheaf. I want to say that $H^1(X,\mathbb{Z}_U) \neq 0$. If it is zero we see exact sequence $$0 \to H^0(X,\mathbb{Z}_U) \to H^0(X,\mathbb{Z}) \to H^0(X,\mathbb{Z}_{\{P,Q\}} ) \to0$$ where $\mathbb{Z}_Y$ is $j_\ast(\mathbb{Z}|_{\{P,Q\}})$. The middle term is $\mathbb{Z}$ but can I say right term is $\mathbb{Z} \oplus \mathbb{Z}$? It seems like it's actually $\mathbb{Z}$. I am confused.",['algebraic-geometry']
584611,Problem on the strong law of large numbers for uncorrelated random variables,"It is known that if $(X_n)_{n \in \mathbb{N}} \subset L^2$ are uncorrelated and identically distributed random variables, then $$\frac{1}{n} \sum_{k=1}^n X_k \to \mathbb{E}[X_1]\text{ almost surely.}$$ Is it true to claim that $$
\frac{1}{n}\sum_{k=1}^n\tanh (X_k)\to \mathbb{E}\left[\tanh (X_1)\right]\text{ almost surely?}
$$","['probability-theory', 'law-of-large-numbers']"
584622,Existence of geodesics and sign of the gaussian curvature,"This is Problem 11 of this year's Miklos Schweitzer. (a) Consider an ellipse in the plane. Prove that there exists a Riemannian metric which is defined on the whole plane, and with respect to which the ellipse is a geodesic. Prove that the Gaussian curvature of any such Riemannian metric takes a positive value. (b) Consider two nonintersecting, simple closed smooth curves in the plane. Prove that if there is a Riemmanian metric defined on the whole plane and the two curves are geodesics of that metric, then the Gaussian curvature of the metric vanishes somewhere. I do not know how this can be approached, and my knowledge of Riemmanian geometry is limited. I posted this question because I'm curious what do we need to do in order to find the solution.","['riemannian-geometry', 'differential-geometry']"
584633,"How prove this $det\left(\frac{1}{\lambda^2_{i}+t\lambda_{i}\lambda_{j}+\lambda^2_{j}}\right)_{n\times n}>0,-2<t<2$","Question: Show that for $t\in (-2,2)$ and $0<\lambda_1<\lambda_2<\ldots<\lambda_n$ we have
$$det(A)=det\left(\dfrac{1}{\lambda^2_{i}+t\lambda_{i}\lambda_{j}+\lambda^2_{j}}\right)_{n\times n}>0$$ My try: few days ago,I have ask this problem How prove this matrix $\det (A)=\left(\frac{1}{\ln{(a_{i}+a_{j})}}\right)_{n\times n}\neq 0$ But I want try use achille hui methods,so 
$$\dfrac{1}{\lambda^2_{i}+t\lambda_{i}\lambda_{j}+\lambda^2_{j}}=\int_{0}^{\infty}f(\lambda_{i},\lambda_{j},x)dx$$
But I can't find this $f$. Thank you very much! Now Sanchez has prove for $t\le 0$ then $\det(A)>0$,so other case is true? Thank you","['matrices', 'linear-algebra', 'determinant']"
584645,Series solution to $y''-xy'-y=0$,"So I'm learning to solve ODE's with series on my own using Boyce and DiPrima and exercise #3 is irking me...just looking for power series solutions around the ordinary point...
$$y''-xy'-y=0$$
So I get started with $$y=\sum_{k=0}^{\infty}a_kx^k, y'=\sum_{k=0}^{\infty}(k+1)a_{k+1}x^k, y''=\sum_{k=0}^{\infty}(k+2)(k+1)a_{k+2}x^k$$
Now the first derivative term gets multiplied by $x$, so
$$xy'=x\sum_{k=0}^{\infty}(k+1)a_{k+1}x^k=\sum_{k=0}^{\infty}(k+1)a_{k+1}x^{k+1}$$
To see the series I can expand it out and thus
$$\sum_{k=0}^{\infty}(k+1)a_{k+1}x^{k+1}=a_1x+2a_2x^2+3a_3x^3+...$$
I need the summation in terms of $x^n$ so I can just rewrite as
$$\sum_{k=0}^{\infty}(k+1)a_{k+1}x^{k+1}=\sum_{k=0}^{\infty}ka_kx^k$$
Now I can substitute all the derivative series into my ODE ad I get
$$y''-xy'-y=\sum_{k=0}^{\infty}(k+2)(k+1)a_{k+2}x^k-\sum_{k=0}^{\infty}ka_kx^k-\sum_{k=0}^{\infty}a_kx^k$$
$$=\sum_{k=0}^{\infty}[(k+2)(k+1)a_{k+2}-ka_k-a_k]x^k=\sum_{k=0}^{\infty}[(k+2)(k+1)a_{k+2}-(k+1)a_k]x^k$$
So now to solve the ODE I need to first solve the recurrence, get the $a_k$ in terms of $a_0$ and $a_1$.  I get
$$a_{k+2}=\frac{a_k}{k+2}$$
To see a couple of terms, 
$$a_0=a_0, a_1=a_1, a_2=\frac{a_0}{2}, a_3=\frac{a_1}{3}, a_4=\frac{a_0}{2\cdot4}, a_5=\frac{a_1}{3\cdot5}, ...$$
So finally I'm ready to solve for y.  Substituting in my $a_k$, 
$$y=a_0+\frac{a_1}{1}x+\frac{a_0}{2}x^2+\frac{a_1}{1 \cdot 3}x^3+\frac{a_0}{2 \cdot 4}x^4+\frac{a_1}{1 \cdot 3 \cdot 5}+...$$
$$=a_0\sum_{k=0}^{\infty}\frac{x^{2k}}{2^kk!}+a_1\sum_{k=0}^{\infty}\frac{2^kk!x^{2k+1}}{(2k+1)!}$$
I think this is the answer (the answer page got ripped out by my son...)","['ordinary-differential-equations', 'sequences-and-series']"
584655,What is $\frac{d(\arctan(x))}{dx}$?,Let $v= \arctan{x}$. Now I want to find $\frac{dv}{dx}$. My method is this: Rearranging yields $\tan(v) = x$ and so $dx = \sec^2(v)dv$. How do I simplify from here? Of course I could do something like $dx = \sec^2(\arctan(x))dv$ so that $\frac{dv}{dx} = \cos^2(\arctan(x))$ but I am sure a better expression exists. I am probably just missing some crucial step where we convert one of the trigonometric expressions into an expression involving $x$. Thanks in advance for any help or tips!,"['trigonometry', 'calculus', 'derivatives']"
584656,Example of functions that grow faster than the exponential functions and/or factorial functions?,What is example of functions that grow faster than the exponential functions and/or factorial functions?,['functions']
584666,An irreducible curve of degree 3 has one singular point,"Good morning, i got stuck with these exercises. Let $X$ be an hypersurface of degree 3 and suppose that $X$ has two singular points $P$ and $Q$. Let $L_{PQ}$ the line containing $P$ and $Q$. Show that $L_{PQ}\subset X$. Let $F(x,y,z)$ be an homogeneous polynomial, $k$ algebraic closed and let $C=Z(F)\subset\mathbb P_k^2$ be an irreducible curve. Prove that is $deg(F) =3$ then $X$ has at most one singular point.",['algebraic-geometry']
584668,"Proof of l'Hôpital's rule, g'(0)=0","l'Hôpital's rule for limits where $\mathbf{x\to 0}$. Let $f$ and $g$ be continuous and differentiable in a neighborhood of $x=0$. Then, if 
$$\lim_{x\to 0} f(x) = \lim_{x\to 0} g(x) =0\,,$$ 
the following simplification can be made: 
$$\lim_{x\to 0} \frac{f(x)}{g(x)}=\lim_{x\to 0} \frac{f'(x)}{g'(x)}\,,$$
provided that the limit in the right-hand side exists. Problem. The proof in my textbook assumes that $g'(0)\neq 0$. In many cases, however, that is not true, but the theorem can still be used. So somehow there must be a way to prove the theorem without this, or with some weaker requirement. Failed attempt. Rewrite with Maclaurin's formula:
$$\lim_{x\to 0} \frac{f(x)}{g(x)}=\lim_{x\to 0}\frac{f(0)+f'(\xi)x}{g(0)+g'(\zeta)x}=\lim_{x\to 0}\frac{0+f'(\xi)x}{0+g'(\zeta)x}=\lim_{x\to 0}\frac{f'(\xi)}{g'(\zeta)}\,,$$
where $\xi$ and $\zeta$ are somewhere between 0 and $x$. When $x\to 0$, both $\xi$ and $\zeta$ go to zero, but when $g'(0)=0$, as some helpful people concluded here , that doesn't necessarily mean that last limit is equal to $\lim_{x\to 0}f'(x)/g'(x)$. Is this a dead end, or can I do something to make this work? Or are there any other proof methods that could be understood with basic calculus skills?","['calculus', 'limits']"
584670,Pairwise independence of Random variables does not imply indendence,"Show by a counterexample that for a family $(X_i)_{i\in I}$ of random variables the independence of all pairs $(X_i,X_j)$ with $i,j\in I, i\neq j$ does not imply the independence of the family (It is enough to consider $\mbox{card}(I)=3$.). Hello, I am searching for a simple example. The professor gave the hint that one might consider random variables which only can take two different values, but I do not know exactly what is meant. By the way: We call the family $(X_i)_{i\in I}$ of random variables independent, if $(\sigma(X_i))_{i\in I}$ is independent, whereat we call a family $(\mathcal{E}_i)_{i\in I}$ with $\mathcal{E}_i\subset\mathcal{A}$ independent, if for any finite subset $I_0\subset I$ and any choice $E_i\in\mathcal{E}_i, i\in I_0$, it is
$$
\mathbb{P}(\bigcap_{i\in I_0}E_i)=\prod_{i\in I_0}\mathbb{P}(E_i).
$$","['probability-theory', 'random-variables']"
584680,"Are convex function from a convex, bounded and closed set in $\mathbb{R}^n$ continuous?","If I have a convex function $f:A\to \mathbb{R}$, where $A$ is a convex, bounded and closed set in $\mathbb{R}^n$, for example $A:=\{x\in\mathbb{R}^n:\|x\|\le 1\}$ the unit ball. Does this imply that $f$ is continuous? I've searched the web and didn't found a theorem for this setting (or which is applicable in this case). If the statement is true, a reference would be appreciated.","['convex-analysis', 'real-analysis']"
584686,Prove using mathematical induction that $2^{3n}-1$ is divisible by $7$,"So, i wanna prove $2^{3n}-1$ is divisible by $7$, so i made this: $2^{3n}-1 = 7\cdot k$ -> for some $k$ value $2^{3n+1} = 1+2\cdot1 - 2\cdot1 $ $2^{3n+1} - 1-2\cdot1 + 2\cdot1 $ $2^{3n}\cdot2 - 1-2\cdot1 + 2\cdot1$ $2(2^{3n}-1) -1 +2$ $2\cdot7k+1$ -> made this using the hypothesis. so, i dont know if its right, or if its wrong, i dont know how to keep going from this, or if its the end. Thanks.","['induction', 'discrete-mathematics']"
584700,"Solve the boundary value problem $y''+y= -1$, $\,y(0)=y(\pi/2)=0$ with the Green's function method","Using the Green's Function method solve the boundary value problem: $$ y''+ y= -1,$$ with boundary conditions $$y(0)=0, \quad y(\pi/2)=0.$$ Verify the result by elementary technique.","['boundary-value-problem', 'ordinary-differential-equations']"
584734,Reflections in Dihedral Group,"In Dihedral Groups, what is the meaning of reflection ? A line needs to be specified for a reflection to take place, but, if you specify only one line how will $D_n$ give all the symmetries for a n-gon? as there might be more than one axis of symmetry in a n-gon?","['linear-algebra', 'finite-groups', 'group-theory']"
584748,Squarefree binomial coefficients.,"At $n=23$ , all binomial coefficients are squarefree. Is this the largest value for $n$ where this is the case? Edit A plot up to $n=50$ : A plot up to $n=500$ : plotted against $n+1$ and $\frac{112}{\sqrt{239}}\sqrt{n}$ and the same up to $n=2000$ : Do these bounds hold for all $n$ ? (Clearly, the bound $n+1$ holds for all $n$ .) Update Just out of interest, a plot up to $n=2000$ with bounding curve $24\log(n)$ ( $24\approx\frac{148}{\log 479}$ , where $148$ is the number of squarefree binomial coefficients at $n=479$ ), which seems to be the tightest curve which still holds for up to $n=3967$ : ... seems to suggest that the bound is $const. \log(n)$ , which so far shows $c\approx24$ (which, coincidentally, is the number of squarefree coefficients at $n=23$ ).","['binomial-coefficients', 'number-theory']"
584808,What is a coordinate shifting?,"I need to find the limit: $$\lim_{(x,y,z)\to(1,3-1)}\frac{(x-1)(y-3)+(z+1)^2}{(x-1)^2+2(y-3)^2+3(z+1)^2}.$$ A hint written below says: Perform a coordinate shifting to (0,0,0). What does coordinate shifting means, and how should I use it in this case?","['calculus', 'functions', 'limits']"
584860,"Using the Maclaurin series of $\arctan(x)$ to evaluate $\int_{0}^{\frac{\pi}{2}} \arctan( a \sin x) \, \mathrm dx$","I want to use the Maclaurin series of $\arctan (x)$ to show that \begin{align}\int_{0}^{\pi/ 2} \arctan (a \sin x) \,  \mathrm dx &= 2 \sum_{k=0}^{\infty} \frac{\left(\frac{\sqrt{1 + a^{2}}- 1}{a}\right)^{2k+1}}{(2k+1)^{2}} \\ &= \operatorname{Li}_{2} \left(\frac{\sqrt{1+a^{2}}-1}{a} \right) -  \operatorname{Li}_{2} \left(\frac{1-\sqrt{1+a^{2}}}{a} \right).
\end{align} I guess we should first impose the restriction $  |a| \le 1$ . Then we have $$ \begin{align} \int_{0}^{\pi/ 2} \arctan (a \sin x) \ dx  &= \int_{0}^{\pi /2} \sum_{k=0}^{\infty} (-1)^{k} \, \frac{(a \sin x)^{2k+1}}{2k+1} \, \mathrm  dx \\ &= \sum_{k=0}^{\infty} (-1)^{k} \, \frac{a^{2k+1}}{2k+1} \int_{0}^{\pi /2} \sin^{2k+1} (x) \, \mathrm  dx \\ &= \sum_{k=0}^{\infty} (-1)^{k} \frac{a^{2k+1}}{(2k+1)^{2}} \frac{2^{2k}}{\binom{2k}{k}}. \end{align}$$ But how do we proceed from here?","['sequences-and-series', 'integration', 'special-functions', 'definite-integrals', 'taylor-expansion']"
584866,Algorithm for reversion of power series?,"Given a function $f(x)$ of the form: $$f(x) = x/(a_0x^0+a_1x^1+a_2x^2+a_3x^3+a_4x^4+a_5x^5+...a_nx^n)$$ Let $A$ be an arbitrary (any) infinite lower triangular matrix with ones in the diagonal: $$A = \left(\begin{array}{cccc} 1 & 0 & 0 & 0 \\ 1 & 1 & 0 & 0 \\ 1 & 1 & 1 & 0 \\ 1 & 1 & 1 & 1 \end{array}\right)$$ Downshift the entries in matrix $A$ one row and call it $X$: $$X=\left(\begin{array}{cccc} 0 & 0 & 0 & 0 \\ 1 & 0 & 0 & 0 \\ 1 & 1 & 0 & 0 \\ 1 & 1 & 1 & 0 \end{array} \right)$$ Calculate matrix powers of $X$: $X^0,X^1,X^2,X^3,X^4,X^5,...,X^n$ Then replace the entries in matrix $A$ with: $$A=a_0X^0+a_1X^1+a_2X^2+a_3X^3+...+a_nX^n$$ Repeat process until the first column in $A$ has converged. Is it then true that the entries in the first column of $A$ will be the coefficients in the power series for the reverse function of $f(x)$? Calculations suggests it is. http://pastebin.com/fsCtBUe1 https://oeis.org/transforms.html Mathematica: (*program start*)
(*coefficients (coeff) in power series can be changed*)
Clear[t, n, k, i, nn, x];
coeff = {1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
  1}; mp[m_, e_] := 
 If[e == 0, IdentityMatrix@Length@m, MatrixPower[m, e]]; nn = 
 Length[coeff]; cc = Range[nn]*0 + 1; Monitor[
 Do[Clear[t]; t[n_, 1] := t[n, 1] = cc[[n]];
  t[n_, k_] := 
   t[n, k] = 
    If[n >= k, 
     Sum[t[n - i, k - 1], {i, 1, 2 - 1}] - 
      0*Sum[t[n - i, k], {i, 1, k - 1}], 0];
  A4 = Table[Table[t[n, k], {k, 1, nn}], {n, 1, nn}];
  A5 = A4[[1 ;; nn - 1]]; A5 = Prepend[A5, ConstantArray[0, nn]];
  cc = Total[
    Table[coeff[[n]]*mp[A5, n - 1][[All, 1]], {n, 1, nn}]];, {i, 1, 
   nn}], i]; cc
(*Mats Granvik,Jul 11 2015*)
(*program end*)","['matrices', 'power-series', 'recursive-algorithms']"
584869,Unbiased Estimator Question and Understanding,"I'm having some difficulty with unbiased estimators, and wondered if anyone could help me. I believe I understand the general concepts OK, however when I come to look at some sample questions to test my understanding, I feel a little lost! I must stress that this is not homework, or any other assessed work - purely for my own understanding, as statistic isn't an area I have studied in great detail to he truthful. For instance, one question I would like to get my head around is: $X \sim B(n,p)$ and $P=bX$. Assuming $n$ is known, determine $b$ such that $P$ is an unbiased estimator of $p$ and find its standard error. To start with, I have found the following: $E(P)=E(bX)=bE(X)=bnp$ - therefore, am I right in thinking that for P to be an inbiased estimator, we must have b equal to $1/n$? For the second part, I believe that I require the variance, as follows: $V(P)=V(bX)=b^2V(X)=b^2 np(1-p)$, and so where $b=1/n$ we have: $V(P)=\frac{p(1-p)}{n}$ and so the standard error will be: Standard Error $=\sqrt{\frac{p(1-p)}{n}}$. Does this solution look correct - I feel that it is, however would like to check my understanding. Thanks in advance for any help! Best,
Chris","['statistics', 'estimation', 'probability']"
584882,$\mathop {\lim }\limits_{n \to \infty } {1 \over {\sqrt n }} \left({1 \over {\sqrt 1 }} + {1 \over {\sqrt 2 }} +\cdots+{1 \over {\sqrt n }}\right)$ [duplicate],"This question already has answers here : Evaluation of the limit $\lim\limits_{n \to \infty } \frac1{\sqrt n}\left(1 + \frac1{\sqrt 2 }+\frac1{\sqrt 3 }+\cdots+\frac1{\sqrt n } \right)$ (8 answers) Closed 10 years ago . $$\mathop {\lim }\limits_{n \to \infty } {1 \over {\sqrt n }} \left({1 \over {\sqrt 1 }} + {1 \over {\sqrt 2 }} + {1 \over {\sqrt 3 }}+\cdots+{1 \over {\sqrt n }}\right)$$
( Without use of integrals ).
I was able to squeeze it from the bottom to ${\lim }=1$, but that's not good enough. I'd be glad for help.","['sequences-and-series', 'calculus', 'limits']"
584889,The Intuition behind l'Hopitals Rule,"I understand perfectly well how to apply l'Hopital's rule, and how to prove it, but I've never grokked the theorem. Why is it that we should expect that $$\lim_{x\to a}\frac{f(x)}{g(x)}=\lim_{x \to a}\frac{f'(x)}{g'(x)},$$
but only when specific criteria are met, like $f(x)$ and $g(x)$ approaching $0$ as $x \to a$? I've never found the proof of the theorem to shed much light on this (thought this might be because I don't have much of an intuitive sense as to why the Cauchy Mean Value Theorem holds, either). I've always been mystified as to how l'Hopital (Bernoulli?) ever discovered this theorem, beyond accidentally discovering that it holds in some cases and then generalizing it to a theorem. So, if you have any ideas as to where the theorem came from, and why it makes intuitive sense, I'd appreciate it if you shared them!","['limits', 'calculus', 'intuition', 'analysis']"
584908,"name of matrix of inner products $\langle f_i, f_j\rangle$","Given a Hilbert space $H$ and a number of elements $\phi_i\in H$, does the matrix $M$ with
$$
M_{i,j} := \langle\phi_i, \phi_j\rangle
$$
have any particular name?","['matrices', 'inner-products', 'hilbert-spaces']"
584916,Derivative of a map involving the matrix inverse,"I have $f: U\rightarrow \mathbb{R}$, $f(X):=\operatorname{tr}(X^{-1})$, $U$ contains all matrices $X$, which are  positive definite and symmetric. I want to show that $f$ is differentiable on $U$. To do so, I have to figure out 
$f(X+tY)=\operatorname{tr}[(X+tY)^{-1}]=\operatorname{tr}[ (X(I-(-t)X^{-1}Y))^{-1}]$
Now it is: $(X(I-(-t)X^{-1}Y))^{-1}=\sum_{i=0}^{\infty}(-t)^{i}(X^{-1}Y)^{i}X^{-1}$. This is the point where I get stuck.  The first term of the series vanishes with $\operatorname{tr}[X]$, when it comes to the derivation. But how can I fix the rest?","['linear-algebra', 'derivatives']"
584924,"For symmetric stable distributions, why is $\alpha \le 2$?","I'm preparing a lecture on stable distributions, and I'm trying to find a simple explanation of the following fact. Suppose we are trying to come up with stable distributions.  From the definition, it's clear that a distribution is stable iff its characteristic function $\phi$  satisfies $\phi(t)^n = e^{i t b_n} \phi(a_n t)$.  The normal distribution, with chf $\phi(t) = e^{-t^2/2}$ clearly satisfies this with $b_n = 0$, $a_n = \sqrt{n}$.  This suggests that we look for distributions with chfs of the form $\phi(t) = e^{-c |t|^\alpha}$.  For $0 \le \alpha \le 2$, this is indeed a chf, and there is a nice proof in Durrett's book, constructing it as a weak limit using Lévy's continuity theorem.  But: For $\alpha > 2$,  is there a simple reason why $\phi(t) = e^{-c |t|^\alpha}$ cannot be a chf? Breiman's Probability proves a general formula for the chf of a stable distribution, using a representation formula for infinitely divisible distributions, but it's  more work  than I want to do for this.","['probability-theory', 'fourier-analysis', 'probability-distributions', 'characteristic-functions']"
584941,Squarefree numbers,"From the Wolfram MathWorld page on squarefree numbers, ""There is no known polynomial time algorithm for recognizing squarefree integers or for computing the squarefree part of an integer."" Wolfram MathWorld says that ""This problem is an important unsolved problem in number theory because computing the ring of integers of an algebraic number field is reducible to computing the squarefree part of an integer."" Would there be any other applications for an algorithm that recognizes squarefree integers?",['number-theory']
585021,Graph Colouring - Eulerian Path,I am doing some studying for a test I have in my discrete math class and I have come across this question which I am very stuck on and keep seem to find any help... If you draw a closed curve in a plane without lifting your pen off of the paper intersecting yourself many times prove that the regions formed by this curve can be coloured with 2 colors. Thanks for the help!,"['graph-theory', 'coloring', 'discrete-mathematics']"
585026,A non orientable closed surface cannot be embedded into $\mathbb{R}^3$,"Can someone please remind me how this goes? Here's the idea of proof I'm trying to recall: let $S$ be a closed surface (connected, compact, without boundary) embedded in $\mathbb{R}^3$. Then one can define the ""outward-pointing normal unit vector"" to $S$ at any point, and subsequently an orientation of the surface. One would like to define this vector by saying that it points towards exterior points to $S$. So we need some kind of generalization of the Jordan curve theorem saying that the surface cuts $\mathbb{R}^3$ into two pieces (interior and exterior). What is this theorem exactly? Also, I apologize if this is silly, but is there an obvious argument that a piece of the surface cuts a small tubular neighborhood of it into interior and exterior points (this seems necessary to define the outward normal vector properly)? Is there a ""cleaner"" approach to prove this fact? Thanks in advance.","['differential-geometry', 'manifolds', 'compact-manifolds', 'differential-topology', 'surfaces']"
585035,The trace map in a finite field.,"Let $p$ be a prime number, and consider the mapping called the trace
$$ Tr \quad : \quad \mathbb{F}_{p^n} \ \longrightarrow \ \mathbb{F}_{p^n} \quad : \quad x \ \longmapsto \ x + x^p + x^{p^2} + \cdots + x^{p^{n-1}}$$
My syllabus Abstract Algebra states the following: Every element $x \in \mathbb{F}_{p^n}$, is mapped to $\mathbb{F}_p$. The restricted mapping $Tr' \ : \ \mathbb{F}_{p^n} \ \rightarrow \mathbb{F}_p $ is surjective. I failed to prove both these statements, and I ask you for some help. Research effort for the first statement We can see $\mathbb{F}_{p^n}$ as a vector space with the scalar field $\mathbb{F}_p$. I showed that $Tr$ is a linear mapping in this setting. I convinced myself that the mapping is not an ismorfism in general by looking at the case where $p=n=2$ and $x\neq 0 \neq y$. Assume that $Tr$ is multiplicative. then
$$Tr(xy) \ = \ Tr(x)Tr(y) \quad \iff \quad xy + x^2y^2 \ = \ (x+x^2)(y+y^2) \ = \ x^2y^2 +xy^2+x^2y +xy$$ 
and by substracting we see
$$x^2y+x^2y=0 \quad \iff \quad xy(x+y) = 0 \quad \iff \quad x = -y$$
And this is not generally true in a field with four elements... I had no inspiration to continue some other way. Research effort for the second statement I knew that for all elements $x \in \mathbb{F}_p, \ Tr(x) = \sum_{j=1}^n x^{p^j}=\sum_{j=1}^n x$. If this the mapping would be injective here, we would be done since the sum $x+ \cdots +x$ is an element of $\mathbb{F}_p$. If $p$ divides $n$ though, this won't work because every element $x \in \mathbb{F}_p$ would be mapped to 0. I hope you can provide me hints to prove the statements. Thank you for your time.","['finite-fields', 'abstract-algebra']"
585048,"Book on Complex Analysis, geometrical approach...","I have already taken a course on Complex Variable.
The course focused mainly on the analytical approach of the subject (power series, etc). Now, I want to study a more geometric view of the subject, specially regarding the work of the functions on the Riemann Sphere, and all the formalities behind that approach. I've been searching for a book in this line, but haven't found many good recomendations. Any recomendations on what books or what material may be helpful? I'm trying to get into complex dynamics through Milnor's book, and I wanted to get more familiarity on working with the Riemann Sphere...",['complex-analysis']
585064,Definition of a tangent space,"Today we defined a tangent space similar to the description here: enter link description here My problem is the following: Why do we need to refer to charts in this case? I mean, would it not be sufficient to say that a tangent space is given by all the tangent vectors of differentiable curves at a particular point on this manifold? I mean, if it is possible to put this manifold in a huge $\mathbb{R}^n$, then it would be also pretty sure how one would add two tangent vectors and so on, so this would clearly define nice vector space. I do not see the need that they are referring to charts in this case. Could anybody explain to me, why this idea is still necessary though?","['manifolds', 'differential-geometry']"
585082,How to notice that $3^2 + (6t)^2 + (6t^2)^2$ is a binomial expansion.,"The other day during a seminar, in a calculation, a fellow student encountered this expression:
$$\sqrt{3^2 + (6t)^2 + (6t^2)^2}$$
He, without much thinking, immediately wrote down:
$$(6t^2+3)$$ What bothers me, is that I didn't see that. Although I suspected there might be some binomial expansion, I was too confused by the middle term to see it. Even after actually seeing the answer, it took me a good deal to realize what's happening.
My initial idea, by the way, was to substitute for $t^2$ and solve a quadratic equation. Horribly slow. My questions is perhaps tackling a greater problem. I now see what happened, I can ""fully"" understand binomial expansions, yet I feel like I might end up encountering a similar expression and yet again not be able to see the result. My questions is thus: How can I properly practice so I don't repeat this mistake? This is perhaps too much of a soft-question, but I feel like many others have been in a position of understanding a problem, yet not feeling like they would be able to replicate it, and might actually have an answer that goes well beyond the obvious ""just practice more"". Thanks for any input.","['binomial-theorem', 'algebra-precalculus', 'soft-question']"
585097,Finding the smallest sub-family of subsets needed to form a new subset,"TL/DR I have a universe $U$ of items $u_i$ and a family $F$ of subsets of $U$ (call them $P_j$ ⊆ $U$). Given this family of subsets, I would like to find the sub-family $C$ ⊆ $F$ of subsets that can produce a new subset $P_{new}$ of members $u_i$ by adding together (or subtracting) as few subsets $P_j$ as possible. That's the best I can do. Hopefully an example is more clear: Example For instance, if we start with the following family of subsets: $$ \begin{align}
 F = \{&P_1 = \{a\},\ P_2 = \{b\},\ ...,\ P_{16} = \{p\}, \\
   &P_{17} = \{a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p\}, \\ 
   &P_{18} = \{a, b, c, d, e\}, \\
   &P_{19} = \{g, h, i\} \,\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \}&\\
\end{align} $$ When requested to compute $\{a, b, c, d, e, f, g, h, i\}$, the simplest thing to do is calculate: $$\{a, b, c, d, e, f, g, h, i\} =  \{a\} + \{b\} + \{c\} +\ ...\ + \{h\} + \{i\}$$ This isn't optimal though (requires 8 additions). For instance, I know that I could more quickly produce the set if I instead took advantage of my previously computed sets (using 2 additions): $$ \begin{align}
P_{new} &= \{a, b, c, d, e, f, g, h, i\}\\
  &=  \{a, b, c, d, e\} + \{f\} + \{g, h, i\} \\
  &=  P_{18} + P_{6} + P_{19} \\
 \mathord{\therefore}\ C ⊆ F &= \{ P_{6}, P_{18}, P_{19} \} \\
\end{align} $$ Example 2 What's even more complex is that (if possible) I want to know when involving subtraction might be optimal: $$\{e, f, g, h, i\} = \{e\} + \{f\} + \{g, h, i\}$$ This is the best solution using only addition (2 operations), But I could have gotten this even faster with 1 subtraction: $$\{e, f, g, h, i\} = \{a, b, c, d, e, f, g, h, i\} - \{a, b, c, d\}$$ Why I need this Each subset $P_j$ has a value $p_j = f(P_j)$ that can be computed. The function $f(P_j)$ is additive. So $p_{\{1,2\}} = p_{\{1\}} + p_{\{2\}}$ When I start my application, I start only by calculating the value $p_i$ for each item $l_i$ on its own. For example: $$ \begin{align}
   P_1 = \{a\} ,&\ \ p_1 = f(P_1) = 5 \\
   P_2 = \{b\} ,&\ \ p_2 = f(P_2) = 20 \\
   P_3 = \{c\} ,&\ \ p_3 = f(P_3) = 8 \\
   ...\ &
\end{align} $$ I then have to start servicing requests for different subsets. For example: $$ \begin{align}
    P_{18} &=  \{a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p\}\  &f(P_{18}) &= 400 \\
    P_{19} &=  \{b, c, d\}\                                         &f(P_{19}) &= 43\\
    P_{20} &=  \{g, h, i\}\                                         &f(P_{20}) &= 30 \\
    ...&
\end{align} $$ My goal is to be able to process these request as fast as possible. For early requests, I unavoidably have to spend a lot of time adding up $p_j$ values. But since these values are additive, I know there should be faster ways to process requests by taking advantage of sets for which I've already computed $p_j$. If $P_{21} = \{b, c, d, g, h, i\}$ is requested, I don't want to waste precious resources retrieving the the 6 values for $p_{2}$ to $p_{7}$, and then adding these values in 5 lengthy operations, when I could have just done a single operation $p_{21} = p_{19}+p_{20}$. Not set-theory? This might actually be a better fit for linear algebra, if formulated as follows: If I have the following known equations and values: $$ \begin{align}
    P_{1}  &=  a, P_{2}  =  b,\ ...,\ P_{8}  =  g &f(P_{1})  &= p_{1},\ ...\\
    P_{9}  &=  a + b + c + d   &f(P_{9})  &= p_{9} \\
    P_{10} &=  d + e + f + g   &f(P_{10}) &= p_{10} \\
\end{align} $$ And I wish to calculate $$ \begin{align}
    P_{11} &= a + b + c + d + e + f + g  &f(P_{11}) ? \\
\end{align} $$ I want to be able discover that the fastest solution comes from $$ \begin{align}
    P_{11} &= P_{9} + P_{10} - P_{4} \\
    P_{11} &=  (a + b + c + d) + (d + e + f + g) - (d) \\
           &=  (a + b + c + 2d + e + f + g) - (d) \\
           &=  a + b + c + d + e + f + g\ \checkmark\\
           \mathord{\therefore}\ p_{11} &= p_{9} + p_{10} - p_{4} \\
\end{align} $$ It's starting to look suspiciously like an np hard problem to me :( If no one can come up with an elegant way of solving the problem, I would also accept a more elegant way of wording the problem (perhaps in terms of an existing well known problem), or a bound on the complexity.","['optimization', 'linear-algebra', 'packing-problem']"
585112,Cantor construction is continuous,"I define a function $f:\mathbb{R}\to\mathbb{R}$ as follows: $f(x)=0$ for $x\le 0$. $f(x)=1$ for $x\ge1$. $f(x)=\dfrac12$ for $x\in\left[\dfrac13,\dfrac23\right]$. $f(x)=\dfrac14$ for $x\in\left[\dfrac19,\dfrac29\right]$, $f(x)=\dfrac34$ for $x\in\left[\dfrac79,\dfrac89\right]$. and so on. So this function has been defined on $\mathbb{R}$, except for the Cantor set. How can we fill in the function on the Cantor set, so that we get a continuous function?",['real-analysis']
585135,Why is this binary-relation antisymmetric?,"Definition of antisymmetric binary-relation is $$\forall a,b\in\mathrm{A},\left[ \left(aRb\wedge bRa\right)\rightarrow\left(a=b\right)\right].$$ Let  $\mathrm{A}=\left\{a\mid a\in\mathbb{R}\right\}$, and the relation set $R=\left\{\left(a,b\right)\mid a,b\in\mathbb{R}\wedge a\text{ is odd}\wedge b\text{ is even}\right\}.$ And $R$ is a antisymmetric binary-relation on $\mathrm{A}$. However, I don't understand. For example, $2R2$ is false and $2=2$ is true. Then the logical expression becomes $\mathrm{F}\wedge\mathrm{F}\rightarrow\mathrm{T}$. What am I missing? Is my concept on logical expression wrong?","['relations', 'logic', 'elementary-set-theory']"
585145,Proving the Shoelace Formula with Elementary Calculus?,"The shoelace formula found here or here tells you how to calculate the area of any polygon given its coordinates. The second link I mentioned gives a proof of it, but it is a bit beyond my level of comprehension. Could anyone try to simplify the proof (or provide their own) to a level up to and including single variable calculus?","['calculus', 'integration']"
585156,"Have mathematical structures equipped with ""generalized relations"" been considered in a systematic way?","A binary relation on $X$ is basically just a function $X^2 \rightarrow \mathbb{B}$, where $\mathbb{B}$ is the prototypical Boolean algebra $\{0,1\}.$ We can generalize by replacing $\mathbb{B}$ with a more complicated partially ordered structure. Thus, we have: Heuristic idea. A generalized relation on $X$ is a function $X^n \rightarrow P$, where $P$ is a partially ordered set possibly having additional structure. For example, a metric space can be viewed as set $X$ equipped with a generalized relation $d : X^2 \rightarrow [0,\infty)$ satisfying the usual axioms. Question. Have mathematical structures equipped with generalized relations been considered in a systematic way? A reference would be nice. Discussion . Here's the example I am most interested in. Suppose $X$ and $Y$ are sets, and that $R$ is a binary relation on $Y$. That is, $R$ is a function $Y \times Y \rightarrow \mathbb{B}$. Then the family $Y^X$ can naturally be equipped with a generalized binary relation $R' : Y^X \times Y^X \rightarrow \mathbb{B}^X$ defined by asserting that $R'(f,g)$ equals the characteristic function of $$\{x \in X \mid R(f(x),g(x))\}.$$ Anyway, the point is that if $\mathcal{Y} = (Y,R)$ is a relational structure, then really $\mathcal{Y}^X$ is most naturally viewed as equipped not with a relation $R'$ having codomain $\mathbb{B}$, but rather a generalized relation having codomain $\mathbb{B}^X.$ We might call such a beast a ""generalized relational structure."" Metric spaces presumably undergo a similar generalization, although I'm still trying to work out the details.","['logic', 'reference-request', 'abstract-algebra']"
585159,"Complex Analysis and showing that in disk $(0,1)$, $f(z)= sin\ z$.","Let $z_n$ be a sequence of distinct points in $D(0, 1)$ such that $z_n → 0$ and let $f : D(0, 1) → {\mathbb C}$ be holomorphic. Show
that if $f(z_n) = sin \ z_n$ for all $n$, then $f(z) = sin\ z$ for all $z \in D(0, 1)$.",['complex-analysis']
585160,Generating Function for Binary String Question,"The following is an assignment question I have been trying to work out for quite some time. Let $C(x,y)=\sum_{n,k \geq 0} c_{n,k} x^{n} y^{k}$, where $c_{n,k}$ is the number of binary strings of length $n$ with $k$ blocks. Prove that
  \begin{equation}
C(x,y)=\frac{1-x+yx}{1-x-yx}
\end{equation}
  Then show that $[x^{n}] \frac{\delta}{\delta y} C(x,y) \left|_{y=1} \right.$ is the total number of blocks among all binary strings of length $n$. I know that the first step is to find a recurrence relation for $c_{n,k}$. Consider a arbitrary binary string of length $n$ with $k$ blocks. The first digit is either zero or one; the difference is immaterial. The second digit is either the same or the opposite of the first digit. In the first case, when the second digit is the same as the first, there are $k$ blocks in the $(n-1)$ digit long substring. The number of such substrings is $c_{n-1,k}$. In the second case, when the second digit is different than the first, one block has been found and so $(k-1)$ blocks remain in the $(n-1)$ digit long substring. The number of such substrings is $c_{n-1,k-1}$. This gives the following recurrence relation:
\begin{equation}
c_{n,k}=c_{n-1,k}+c_{n-1,k-1}.
\end{equation} If $k>n$, then $c_{n,k}=0$ because there is no way to have more blocks of digits than digits in a binary string. If $k=n$, then $c_{n,k}=2$ because there are two binary strings with alternating digits. Let $C(x,y)= \displaystyle\sum_{n=0}^{\infty} \sum_{k=0}^{\infty} c_{n,k} x^{n}y^{k}$. Find the bivariate generating function.
\begin{equation}
\begin{aligned}
\sum_{k=1}^{\infty} c_{n,k} y^{k} &= \sum_{k=1}^{\infty} c_{n-1,k} y^{k} + \sum_{k=1}^{\infty} c_{n-1,k-1} y^{k} \\
\sum_{n=1}^{\infty} \sum_{k=1}^{\infty} c_{n,k} x^{n}y^{k} &= \sum_{n=1}^{\infty} \sum_{k=1}^{\infty} c_{n-1,k} x^{n}y^{k} + \sum_{n=1}^{\infty} \sum_{k=1}^{\infty} c_{n-1,k-1} x^{n}y^{k} \\
\sum_{n=1}^{\infty} \sum_{k=1}^{\infty} c_{n,k} x^{n}y^{k} &= x\sum_{n=1}^{\infty} \sum_{k=1}^{\infty} c_{n-1,k} x^{n-1}y^{k} + xy\sum_{n=1}^{\infty} \sum_{k=1}^{\infty} c_{n-1,k-1} x^{n-1}y^{k-1} \\
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
\sum_{n=0}^{\infty} \sum_{k=1}^{\infty} c_{n,k} x^{n}y^{k} - \sum_{k=1}^{\infty} c_{0,k}x^{0}y^{k} &= x\sum_{n=0}^{\infty} \sum_{k=1}^{\infty} c_{n,k} x^{n}y^{k} + xy\sum_{n=0}^{\infty} \sum_{k=1}^{\infty} c_{n,k-1} x^{n}y^{k-1} \\
\sum_{n=0}^{\infty} \sum_{k=0}^{\infty} c_{n,k} x^{n}y^{k} - \sum_{n=0}^{\infty} c_{n,0} x^{n} &= x \left( \sum_{n=0}^{\infty} \sum_{k=0}^{\infty} c_{n,k} x^{n}y^{k} - \sum_{n=0}^{\infty} c_{n,0} x^{n} \right) + xy\sum_{n=0}^{\infty} \sum_{k=0}^{\infty} c_{n,k} x^{n}y^{k} \\
\sum_{n=0}^{\infty} \sum_{k=0}^{\infty} c_{n,k} x^{n}y^{k} - 2 &= x \sum_{n=0}^{\infty} \sum_{k=0}^{\infty} c_{n,k} x^{n}y^{k} - 2x + xy\sum_{n=0}^{\infty} \sum_{k=0}^{\infty} c_{n,k} x^{n}y^{k} \\
C(x,y)-1&= xC(x,y)-x+xyC(x,y) \\
(1-x-xy)C(x,y) &= 2-2x \\
C(x,y) &= \frac{2-2x}{1-x-xy}
\end{aligned}
\end{equation} At this point, I am unsure how to proceed. Furthermore, I know that I have clearly made an error above but I cannot seem to find it. Any help would be appreciated!","['generating-functions', 'binary', 'combinatorics']"
585182,Why is the Riemann mapping theorem important?,"The Riemann mapping theorem is as follows: Let $U \neq \mathbb{C}$ be a simply connected domain and $w_{1}, w_{2} \in U$ any points. Then, there exists a unique conformal mapping $f: \mathbb{D} \rightarrow U$ such that $f^{-1}(w_{1}) = 0$ and $f^{-1}(w_{2}) > 0$ (where $\mathbb{D}$ is the unit disk). I would like to know the reason why the Riemann mapping theorem is so important. In particular I am curious to know if it is of interest to calculate the aforementioned function $f$ and if there is a technique to do it.",['complex-analysis']
585187,Limit of convolution of measures is Cantor function,"For positive integer $k$, let $\mu_k=\dfrac{1}{2}\left(\delta(x)+\delta\left(x-\dfrac{2}{3^k}\right)\right)$. Show that $$\lim_{k\rightarrow\infty}(\mu_1\ast\mu_2\ast\cdots\ast\mu_k)((-\infty,x))=C(x),$$ where $C$ is the Cantor function . The definition of measure convolution that I find is $$ (\mu_1\ast \mu_2)(I)=\int_\mathbb{R}\int_\mathbb{R}1_I(x_1+x_2)d\mu_1(x_1)d\mu_2(x_2)$$ for any interval $I$. I'm not sure how to start on this. It's a convolution of $k$ measures...","['measure-theory', 'real-analysis']"
585213,Selecting a subset from a set such that a given quantity is minimized,"Let $A$ be is a set of some $p$-dimensional points $x \in \mathbb{R}^p$. Let $d_x^A$ denote the mean Euclidean distance from the point $x$ to its $k$ nearest points in $A$ (others than $x$). Let $C \subset A$ be a subset of points chosen randomly from $A$. We have $\Phi(A) = \sum_{x \in A} d_x^C$. Now suppose that I remove a point $x'$ from $A$, I get a new set $A_2 = A \setminus \{x'\}$. Question: Which condition should a new set $C_2 \subset A_2$ satisfies, in order to have $\Phi(A_2) = \sum_{x \in A_2} d_x^{C_2} \leq \Phi(A)$ ? In other words, how can I choose a subset $C_2$ from $A_2$ such that reduces the quantity $\Phi(A)$ (if such $C_2$ exists) ? Note: $C$ and $C_2$ has a fixed size: $|C| = |C_2| = |A|/2$. And I have a constraint that $C \neq C_2$.","['optimization', 'metric-spaces', 'combinatorics']"
585215,Conic Sections with Matrices,"I'm reading ""Geometry"" by Brannan, Esplen, and Gray. Any conic has an equation of the form
$$
Ax^{2} + Bxy + Cy^{2} + Fx + Gy + H = 0,
$$
where $A,\ B,\ C,\ F,\ G,\ H\ \in {\mathbb R}$  and not all of $A, B, C$ are zero. The matrix form of this equation is
$$x^{\sf T}{\cal A}x + J^{\sf T}x + H = 0,$$
where $x = \begin{bmatrix} x \\ y\end{bmatrix}$ is a vector in $\;\Bbb R^2\;$,
${\cal A} = \begin{bmatrix} A & B/2 \\ B/2 & C\end{bmatrix}$, and $J = \begin{bmatrix} F \\ G\end{bmatrix}$. Given the equation $$3x^2 - 10xy + 3y^2 + 14x -2y + 3 = 0,$$ we are asked to find what type of conic this is and its center (if it has one). You start by diagonalizing the matrix $A$ and constructing an orthogonal matrix, $P$, with the normalized eigenvectors of $A$. Now, the book says that it's important to make sure that columns of $P$ are arranged s.t. ${\rm det}\ (P) = 1$, so $P$ represents a rotation in the plane. My question is, why is it necessary that $P$ represents a rotation of $x$? Why can't ${\rm det}\ (P) = -1$, so $P$ represents a reflection followed by a rotation? I tried seeing where this change would cause a problem, but can't see it. Can anyone please clarify?","['geometry', 'linear-algebra']"
585228,Finding the radius of convergence of the given power series.,"Consider the power series $\sum\limits_{n=1}^{\infty}$$a_nZ^n$ , where $a_n$ is the number of divisor of $n^{50}$ . Find the radius of convergence.",['complex-analysis']
585230,Prove: The infinite series $x_n$ converges if and only if the infinite series $2^nx_{2^n}$ converges.,I am in desperate need for hints to get me in the right direction for this proof. Let $(x_n)_{n\in \mathbb {N}}$ be a monotone decreasing null sequence. Prove that: $$\sum_{n=1}^\infty x_n \text{ converges } \ \iff \sum_{n=0}^\infty 2^nx_{2^n} \text{  converges}$$ Thanks.,"['convergence-divergence', 'sequences-and-series', 'real-analysis', 'analysis']"
585240,Bessel function with complex argument,"So I understand that the bessel functions of the first kind are the ones that satisfy this equation: $$x^2\frac{d^2y}{dx^2}+x\frac{dy}{dx}+(x^2-\alpha^2)y = 0$$ and the result is a linear combination of the bessel functions of the first and second kind. equation(1):
$$ A J_a(x) + B Y_a(x) $$ Now let: $x = iv$ $$ \dfrac{dy}{dx} = \dfrac{dy}{dv} \dfrac{1}{i} \\
\dfrac{d^2y}{dx^2} = -\dfrac{d^2y}{dx^2}$$ Substituting in the original equation we get: $$(iv)^2(-1)\frac{d^2y}{dv^2}+(iv)\dfrac{1}{i}\frac{dy}{dv}+((iv)^2-\alpha^2)y = 0 \\
v^2\frac{d^2y}{dv^2}+v\frac{dy}{dv}- (v^2+\alpha^2)y = 0 $$ This is the equation which has solutions the modified bessel functions. Is equation 1 with x = iv a solution to this equation? ( I think it is but not sure ) and the second equation is:
Why is this then not true? $$ J_a(ix) = I_a(x) \\
Y_a(ix) = K_a(x) $$","['ordinary-differential-equations', 'special-functions', 'complex-analysis']"
585261,Weak convergence and Fourier transform,"For positive integer $k$, let $\mu_k=\dfrac{1}{2}\left(\delta(x)+\delta\left(x-\dfrac{2}{3^k}\right)\right)$. Let $dC_k=\mu_1\ast\cdots\ast\mu_k$. We have that $dC_k$ converges weakly to $\mu_C$, where $C$ is the Cantor function. Now we want to show that $\hat{\mu}_C(y)=e^{ay}\prod_{k=1}^\infty\cos(y/3^k)$ for some $a\in\mathbb{C}$. For this, we also need to show that the infinite product converges. I want to use the weak convergence. Is it true that $\hat{dC_k}(y)$ converges to $\hat{\mu}_C(y)$?","['measure-theory', 'fourier-analysis', 'real-analysis']"
585268,Any finite algebraic subset of $\mathbb A^2_k$ can be determined by 2 equations,"Please give me a hint for this exercise. Show that every finite subset of the affine plane, $\mathbb A^2_k$, over an algebraically closed field, $k$, can be determined by two equations. I am using Shafarevich's book.",['algebraic-geometry']
585271,problem with recurrence relation for series solution for ODE,"I have
$$y''-xy'-y=0$$
and I'm trying to find the series solution around the ordinary point $x_0=1$.  My last post I muscled through to the solution when the ordinary point was $x_0=0$, but this is proving to be tougher.  Now I have obtained through power series analysis
$$y''-xy'-y=0=\sum_{k=0}^{\infty}[(k+2)(k+1)a_{k+2}-(k+1)a_{k+1}-(k+1)a_k](x-1)^k$$
which yields the recurrence relation
$$a_{k+2}=\frac{a_{k+1}+a_{k}}{k+2}$$
WHen I start plugging in sequential ""k"" values I'm not finding a very good pattern emerging.
$$a_2=\frac{a_0}{2!}+\frac{a_1}{2!}$$
$$a_3=\frac{a_0}{3!}+\frac{3a_1}{3!}$$
$$a_4=\frac{4a_0}{4!}+\frac{6a_1}{4!}$$
$$a_5=\frac{8a_0}{5!}+\frac{18a_1}{5!}$$
$$a_6=\frac{28a_0}{6!}+\frac{48a_1}{6!}$$
$$a_7=\frac{76a_0}{7!}+\frac{156a_1}{7!}$$
Outside just writing out term by term, substituting in the appropriate $a_k$, does this have a nice closed form?  Or is is just the case that I have the answer with the $a_k$'s that I have","['power-series', 'ordinary-differential-equations', 'sequences-and-series']"
585307,What is the structure of a directed graph with vertex set A which has a relation R,"I am studying for a test and found this question in the book:
Let $R$ be an equivalence relation on the set A (Non-empty). Let $D_R$ be the directed graph with vertex set $A$ and an arc from $x$ to $y$ iff $(x,y) \in R$. Describe $D$ in terms of how many components it has, and what is the structure of each component? My solution is: Let $R$ be an equivalence relation on A. Then $D$ could have isolated vertices, but each of these isolated vertices would have a cycle of length one, directed back toward itself.
Every vertex such that $(x,y) \in R$ will have 2-directional arcs between $x$ and $y$ (one from $x$ to $y$ and the other from $y$ to $x$). All vertices will also have a cycle of length one directed back toward itself. For every two vertices $(x,y) \in R$ and $(y,z) \in R$ there will be $(x,z) \in R$ with two directional arcs between $x$ and $z$ I think it's right, but I am not sure.
Also I don't know what else I can say about the dimensions of D","['relations', 'equivalence-relations', 'discrete-mathematics']"
585319,Discrete version of dominated convergence thm,"Let $f_1,f_2,\ldots,g\colon\mathbb{Z}\rightarrow\mathbb{R}$ be functions such that $|f_N(n)|\leq g(n)$, $\sum_{n=-\infty}^\infty g(n)<\infty$, and $\lim_{N\rightarrow\infty}f_N(n)=f(n)$. Then show that $$\lim_{N\rightarrow\infty}\sum_{n=-\infty}^\infty f_N(n)=\sum_{n=-\infty}^\infty f(n).$$ This looks like the dominated convergence theorem, but how can we prove it directly? Edit : As T. Bongers helpfully pointed out, this can be shown using the dominated convergence theorem. Is there a direct way to do it without the theorem?",['real-analysis']
585353,Rank-$1$ update for Cholesky factor,"I have covariance matrix known to be $$K = \sum_{i=1}^Nx_ix_i^T$$ where the dimension of $x$ is big (like $50000$ ) so I don't want to really compute any outer-product to expand it as a full matrix. Also, I know this covariance matrix is sparse Since K is guaranteed to be positive-definite, there is a unique Cholesky decomposition : $$K = L^TL$$ Two questions: is there a way to update $L$ sequentially (update Cholesky factor after seeing each data point). is there approximation of Cholesky that keeps $L$ sparse or low-rank to save memory ?","['matrices', 'linear-algebra', 'machine-learning', 'cholesky-decomposition']"
585358,Problem concerning limit,"My friend asked me the question while he is preparing the mathematical analysis exams. Let $\left\{a_{n}\right\}$ be a sequence satisfying
  $\lim\limits_{n\to\infty}\left(a_{n}\sum\limits_{k=1}^{n}a_{k}^{2}\right)=1$. Prove that $\lim\limits_{n\to\infty}\left(\,\sqrt[3]{3n\,}\,\ a_n\,\right) =1$. Here is my attempt:  let $S_n= \sum_{k=1}^{n} a_k^2$, then we get 
$$\lim_{n\to\infty} (S_n-S_{n-1}) S_n^2=1$$
What we need is $$\lim_{n\to\infty} \sqrt[3]{9n^2} (S_n-S_{n-1})=1$$
After trying Stolz theorem, I still cannot get the term $\sqrt[3]{9n^2}$. I wonder how to get this result? Any hints or solutions are welcomed, thanks!","['limits', 'analysis']"
585359,Drawing a simple graph with a certain number of vertices,"I am supposed to see if it is possible to draw a graph with 8 vertices given the degree sequence: 1,1,2,3,5,5,6,7 First I tried the handshaking lemma and it holds. So since drawing that graph is tedious I decided to remove the vertex with degree 7 since I know I can just add it back at the end and connect it to every point. So now the new degree sequence is 0,0,1,2,4,4,5 The handshaking lemma still holds for the degree sequence but Isn't it impossible to have a vertex of degree 5 since there aren't enough vertices in the degree sequence to have a degree of 5 for a vertex? I don't understand how such a graph could exist even though the handshaking lemma still holds. any thoughts/ suggestions?","['graph-theory', 'discrete-mathematics']"
585381,Wave-Particle Duality in PDE?,"I am reading Arnold's Lectures on Partial Differential Equations . It is definitely a good book, yet sometimes I am a little bit confused. One theme of the first chapter seems to be From the physical point of view this case is the duality that occurs in describing a phenomenon using waves or particles. The field satisfies a certain first-order partial differential equation, the evolution of the particles is described by ordinary differential equations, and there is a method of reducing the partial differential equation to a system of ordinary differential equations; in that way one can reduce the study of wave propagation to the study of the evolution of particles. When I first read it, it sort of makes sense. If one understands the evolution of each particle, then globally one should be able to describe the wave. However, when I read on, it starts to confuse me. For instance, in the $x-y$ plane one has the following equation  \begin{equation}
(\frac{\partial u}{\partial x})^2+(\frac{\partial u}{\partial y} )^2=1.
\end{equation} It can be shown that if one has a convex closed curve, the define a function on the 'outside' region by mapping a point to its distance to the curve, then this function solves that equation. Conversely, any function $u$ solving that equation is the distance to a certain curve. Then the author asks us to understand the wave-particle duality in this case, which is something quite puzzling to me. In another case, the author talks about Newton's equation and Euler's equation for a particle moving freely on a line: \begin{equation}
\frac{d^2}{dt^2}x=0,
\end{equation} which says the acceleration is $0$;
and \begin{equation}
u_t+u_x u=0,
\end{equation} where $u(t,x)$ is the velocity at location $x$ at time $t$. Then again he asks us to understand the duality in this case. So could someone give a hint on what the author exactly means by this duality? How one should understand it (in the examples above as well as in other cases)? Thanks!","['ordinary-differential-equations', 'physics', 'partial-differential-equations', 'reference-request', 'intuition']"
585394,Initial Value Problem with Repeated Eigenvalues,"Given the matrix
$$
A=\left(\begin{array}{ccc}
1 & 0 & 1 \\ 1 & 1 & 0 \\ 0 & 0 & 1\end{array}\right)
$$
For $X'= AX.\quad$
$X\left(0\right)=\left(\begin{array}{r}1 \\ 0 \\ -2\end{array}\right)\,.\quad$
What is the solution ?.",['ordinary-differential-equations']
585395,What is the Riemann Sphere?,Reading from wikipedia I understood that Riemann Sphere is used to represent extended complex plane. But it would be great if someone could explain it in a less technical manner.,"['intuition', 'complex-analysis', 'definition']"
585406,Calculate integral applying Stokes' theorem,"I am trying to solve the following exercise: Let $F$ be the vector field defined by $F(x,y,z)=(-y,yz^2,x^2z)$ and $S \subset \mathbb R^3$ the surface defined as $S=\{x^2+y^2+z^2=4, z\geq 0\}$, oriented according to the exterior normal vector. Calculate:
$\iint_S (\nabla\times F).dS$. The attempt at a solution: I've calculated the curl, it's not an easy integral to calculate. I can't apply Stokes' theorem because it is not a closed surface, but if I consider the surface $S^{*}=\{x^2+y^2+z^2=4, z\geq 0\} \cup \{ x^2+y^2\leq 4, z=0\}$, then this is a closed surface and $F$ is of class $C^1$, so Stokes'theorem says that: $\iint_S^{*} (\nabla\times F).dS=\int_CF.ds$ where $C$ is the boundary of the surface $S^{*}$. Now, my original integral is $\iint_S (\nabla\times F).dS=\iint_S^{*} (\nabla\times F).dS-\iint_D (\nabla\times F).dS $, where $D=\{ x^2+y^2\leq 4, z=0\}$. But as $D$ is a closed surface, I can also apply Stokes' theorem, so $\iint_D (\nabla\times F).dS=\int_{C'} F.ds $, where $C'$ is the boundary of $D$. Now, my question is: isn't $C=C'$?, I mean, the curve boundary of $S^{*}$ is the same boundary than the one of $D$. If this is the case $\iint_S (\nabla\times F).dS=\int_C F.ds-\int_{C'} F.ds=\int_C F.ds-\int_{C} F.ds=0$. Could someone tell me if my solution is correct?",['multivariable-calculus']
585422,"On the subset $nil(G):=\{x\in G \mid \langle x,y \rangle \text{ is nilpotent for all } y \in G\}$ of a group $G$.","Let $G$ be a group and $nil(G):=\{x\in G \mid \langle x,y \rangle \text{ is nilpotent for all } y \in G\}$. Is $nil(G)$ always a subgroup of $G$? Many thanks for any help.","['group-theory', 'abstract-algebra']"
585476,Probability of a typing monkey backspacing all letters. [duplicate],"This question already has answers here : Hitting probability of biased random walk on the integer line (4 answers) Closed 10 years ago . Suppose a monkey is typing on a keyboard into a word document. The word document has x letters already in the document. The keyboard has n number keys on it (i.e. keys that cause another digit to appear on the screen) and k backspace keys. If the monkey types for an infinitely long time, what is the probability that at any point there will be 0 characters on the screen?","['probability', 'combinatorics']"
585494,Set theory - elements not in both finite sets,"I have an understanding of the below problem but have little experience proving things in set theory so I don't know how to start it. For any two finite sets $A$ and $B$ , define $f(A,B)$ to be the number of elements that are contained either in $A$ or in $B$ , but not in both $A$ and $B$ . Three given sets $X,Y$ and $Z$ satisfy $$f(X,Y) = f(Y,Z) = f(Z,X).$$ (a) Prove that $f(X,Y)$ is even. (b) Find a set $W$ such that $$f(W,X) = f(W,Y) = f(W,Z) = \frac{1}{2} f(X,Y).$$",['elementary-set-theory']
585495,A problem about an $R$-module that is both injective and projective.,"Let $R$ be a domain that is not a field, and let $M$ be an $R$-module that is both injective and projective. Prove that $M= \left \{ 0 \right \}$. This is exercise 7.52 of Rotman's Advanced Modern Algebra . Using theorems before exercises, because $M$ is injective and $R$ is a domain, I conclude that $$\forall m\in M ,\forall r\in R\ (r\neq 0) ,\exists {m}'\in M \Rightarrow m=r{m}'$$
and also because $M$ is projective there is a surjective $\psi$ from free $R$-module $F$ with basis $\left \{ e_{i} \right \}_{i\in I}$ to $M$ and thus we can conclude that for every $m\in M$ we have $$m=\sum r_{i}\Psi (e_{i})$$
now I don't know how should I use these together. The idea of what is happening or a suggestion or a hint will be great.","['projective-module', 'abstract-algebra', 'homological-algebra', 'injective-module', 'modules']"
585501,How find this invertible matrix $C=\left[\begin{smallmatrix} A&B\\ B^T&0 \end{smallmatrix}\right]$,"let matrix $A_{n\times n}$,and $\det(A)>0$, and the matrix $B_{n\times m}$,and such $rank(B)=m$,and let
$$C=\begin{bmatrix}
A&B\\
B^T&0
\end{bmatrix}$$ Find this Invertible matrix
 $C^{-1}$ my try: I found this matrix Invertible matrix $C$,it must find $B^TAB$ Invertible matrix.But I can't Thank you for your help","['matrices', 'linear-algebra']"
585530,"Show: $\sum_{k=1}^{\infty}\mu(\left\{f\geq k\right\})\leq\int f\, d\mu\leq\sum_{k=0}^{\infty}\mu(\left\{f>k\right\})$","Show that für $f\colon (\Omega,\mathcal{A})\to\mathbb{R}_{\geq 0}$ measurable, it is
    $$
\sum_{k=1}^{\infty}\mu(\left\{f\geq k\right\})\leq\int f\, d\mu\leq\sum_{k=0}^{\infty}\mu(\left\{f>k\right\}).
$$ I do not know if one needs it here, but we defined the integral of a non-negative measurable function $f\colon (\Omega,\mathcal{A})\to (\overline{\mathbb{R}},\overline{\mathcal{B}})$ by
$$
\int_{\Omega}f\, d\mu=\sup\left\{\sum_{k=1}^{n}(\inf\left\{f(x): x\in A_k\right\})\mu(A_k): n\in\mathbb{N}, A_1,\ldots,A_n\in\mathcal{A}\mbox{ disjoint },\Omega=\bigcup_{k=1}^{n}A_k\right\}.
$$ My first idea was to use this definition, but I did not come along with it... Maybe you can help me to show that? (I think, maybe it has something to do with monotone convergence theorem or majorated convergence?)",['measure-theory']
585537,"If $\beta=0.{a_1}^{k}{a_2}^{k}{a_3}^{k}\cdots\in\mathbb Q$, then $\alpha=0.a_1a_2a_3\cdots\in\mathbb Q$?","Question : For every even $k\ge 4$ , is the following $(\star)$ true? $$\begin{align}\text{If $\beta=0.{a_1}^{k}{a_2}^{k}{a_3}^{k}\cdots\in\mathbb Q$, then $\alpha=0.a_1a_2a_3\cdots\in\mathbb Q$.}\qquad(\star)\end{align}$$ Here, $a_i$ is the $i$ -th decimal place of $\alpha$ . Example : For $k=2,\alpha=0.12345,$ we have $\beta=0.1491625.$ Motivation : I've been able to prove that the answer is NO for $k=2$ , and that the answer is YES for every odd $k\ge 3$ . The answer seems YES for any $k\ge 3$ , but I'm facing difficulty for treating every even $k$ in general. Can anyone help? Update : I crossposted to MO . P.S. The followings are the counterexamples for $k=2$ . $$\alpha=0.2372377237772377772377777\cdots\Rightarrow \beta=0.\overline{49}.$$ $$\alpha=0.1842184242184242421842424242\cdots\Rightarrow \beta=0.\overline{164}.$$ $$\alpha=0.183423471831834234718318318342347\cdots\Rightarrow \beta=0.\overline{1649}.$$","['rational-numbers', 'irrational-numbers', 'number-theory']"
585568,interchange stochastic and deterministic integration,"If $f$ is a function in $L^2([0,1]^m)$, W is one-dimensional Brownian motion, $a,b \in [0,1]$, are the following two integrals equal? $$\int_0^1\int_0^{t_{m-1}}\cdots \int_0^{t_2} \left(\int_a^bf(t_1, t_2, \cdots, t_{m-1}, s)ds\right) dW_{t_1}dW_{t_2}\cdots dW_{t_{m-1}}$$ $$\int_a^b\left(\int_0^1\int_0^{t_{m-1}}\cdots \int_0^{t_2}f(t_1, t_2, \cdots, t_{m-1}, s)dW_{t_1}dW_{t_2}\cdots dW_{t_{m-1}}\right)ds$$ I know we can use Fubini's theorem to interchage integral order in deterministic cases, but since here we have mixed deterministic and stochastic integral, I don't know how to make justification. Thank you for help.","['probability-theory', 'stochastic-calculus', 'stochastic-integrals']"
585578,Biology: Wright-Fisher model of genetic drift,"In evolutionary biology (in population genetics to be more accurate) exists the concept of genetic drift. It describes how an allele (gene variant) (that has no advantage or disadvantage in terms of reproductive succes) vary through time. Below is a classical model to describe this process of genetic drift. This model is called the Wright-Fisher model of genetic drift: $$\frac{(2N)!}{k!(2N-k)!}p^kq^{2N-k} \Leftrightarrow \binom{2N}{k}p^kq^{2N-k}$$ where $\binom{2N}{k}$ is the binomial coefficient. This formula gives the probability of obtaining $k$ copies of an allele at generation $t+1$ given that there is a frequency of $p$ of this allele in the population at generation $t$. $N$ is the population size and $2N$ is the number of copies of each gene (this model applies to diploid population only). My questions are: 1) From this formula, how can we calculate the probability of extinction of an allele in say 120 generations starting at a given frequency, let's say 0.2? and 2) How can we calculate the probability of extinction rather than fixation of an allele starting at frequency $p$ if we wait an infinite amount of time? Question 2) has already been answered. Remain Question 1) In 1969, Kimura and Ohta showed that assuming an initial frequency of $p$, the mean time to fixation $\bar t_1(p)$ is: $$\bar t_1(p)=-4N\left(\frac{1-p}{p}\right)\ln(1-p)$$ similarly they showed that the mean time to loss $\bar t_0(p)$ is $$\bar t_0(p)=-4N\left(\frac{p}{1-p}\right)\ln(p)$$ Combining the two, they found that the mean persistence time of an allele $\bar t(p)$ is given by $\bar t(p) = (1-p)\bar t_0(p) + p\bar t_1(p)$ which equals $$\bar t(p)=-4N\cdot \left((1-p)\cdot \ln(1-p)+p\cdot \ln(p)\right)$$ It does not answer my questions though! I asked this same question on Biology.SE ( here ) a month ago but did not get an answer yet. I hope mathematician will be able to help me with that. On this biology.SE post, @GriffinEvo found via simulations that the probability of extinction of an allele starting at frequency $p$ if we wait an infinite amount of time is $1-p$ (which answer to question 2) ). How can we mathematically demonstrate that result? And don't forget question 1) :D","['applications', 'biology', 'mathematical-modeling', 'probability-distributions', 'probability']"
585597,"Tensor product, Artin-Rees lemma and Krull intersection theorem","I asked another question about tensor product, but can't conclude from the answer, so here is another more concrete question. Let $(A,m)$ be a local ring then by Artin-Rees Lemma $m^k \bigcap I \subset m^sI$ for $s$ fixed and $k$ big enough. So now, is it true that for any module $M$ over $A$ we have $$(m^k \cap I) \otimes_A M \subset m^s(I\otimes_A M)$$ for $s$ fixed and $k$ big enough?","['commutative-algebra', 'algebraic-geometry', 'tensor-products']"
585599,Galois group of $f(x) = x^5 + x - 1$ over $\mathbb{Q}$,"I'm trying to compute the Galois group of the quintic polynomial $f(x) = x^5 + x - 1$. I first decomposed $f(x)$ into irreducible factors $f(x) = g(x)h(x)$ where $g(x) = x^2 - x + 1$ and $h(x) = x^3 + x^2 - 1$. I denoted by $K_g$, $K_h$ the splitting fields of $g(x), h(x)$ over $\mathbb{Q}$, respectively, and saw that $G_g = {\rm Gal}(K_g/\mathbb{Q}) \cong \mathbb{Z}_2$, $G_h = {\rm Gal}(K_h/\mathbb{Q}) \cong S_3$ where $S_3$ is the symmetric group on three letters. Then the splitting field of $f(x)$ over $\mathbb{Q}$ is $K_gK_h$ (compositum). Guessing that the Galois group $G_f$ of $f(x)$ is $\mathbb{Z}_2 \times S_3$, I have tried to show that $K_g \cap K_h = \mathbb{Q}$. Actually, it suffices to show that $\sqrt{-3}$ does not belong to $K_h$ but I'm stuck on here. Could anyone give me an idea? (If $K_g \cap K_h \neq \mathbb{Q}$, then $\mathbb{Q} \subsetneq K_g \subsetneq K_h$, so Galois theorem asserts ${\rm Gal}(K_h/K_g) \cong A_3$, but failed to reach a contradiction.) Thank for your answer.",['abstract-algebra']
585619,Positivity of the Coulomb energy in 2d,"Let $$D(f,g):=\int_{\mathbb{R}^3\times\mathbb{R}^3}\frac{1}{|x-y|}\overline{f(x)}g(y)~dxdy$$
with $f,g$ real valued and sufficiently integrable be the usual Coulomb energy. Under the assumption $D(|f|,|f|)<\infty$ it can be seen that $D(f,f)\geq 0$ (see for example Lieb-Loss, Analysis 9.8). The reason for that is basically that the Fourier transform of $\frac{1}{|\cdot|}$ is non-negative. In two dimensions the Newton kernel is given by $-\log|\cdot|$ which does not have a positive Fourier transform anymore. I saw the claim that under further assumptions the positivity of the Coulomb energy however does still hold true. Precisely: If $f\in L^1(\mathbb{R}^2)\cap L^{1+\epsilon}(\mathbb{R}^2)$ for some $\epsilon>0$ such that $$\int_{\mathbb{R}^2}\log(2+|x|)|f(x)|~dx<\infty$$
  and $$\int f=0,$$ then $$D(f,f):=-\int_{\mathbb{R}^2\times\mathbb{R}^2}\log{|x-y|}\overline{f(x)}f(y)~dxdy\geq0.$$ The reference given is Carlen, Loss: Competing symmetries, the logarithmic HLS inequalitiy and Onofir's inequality on $S^n$ . I don't see how the claim follows from that paper. Do you have any other reference for the claim above or see a reason why it should be true?","['inequality', 'reference-request', 'real-analysis']"
585620,Subgroups of $SO(4)$ with free transitive action on $S^3$,"By considering $S^3$ as the group manifold of $SU(2)$, the ordinary action of $SO(4)$ on the three sphere can be written as the $SU(2)\times SU(2)/\mathbb{Z}_2$ given by the group action of multiplication on the left and right: $(g,h).x=gxh^{-1}$ From this, there are two obvious subgroups acting freely and transitively on the sphere, by restricting to left and right multiplication. My question is, are there any others? Of less importance to me directly, but perhaps more general interest, is the generalisation: How many subgroups of $SO(n)$ act freely and transitively on $S^{n-1}$? For n=2, there's 1: $SO(2)$ itself. For n=3, there are none. For n=4, perhaps 2? Thanks!","['lie-groups', 'group-theory']"
585649,Analytic Functions. Prove that $\frac fg$ is analytic and the derivative is ${f'g-g'f\over g^2}$,"Let $\Omega$$\subseteq$$\mathbb{C}$ be open and suppose $f$ and $g$ are functions from $\Omega$ into $\mathbb{C}$. Suppose f and g are both analytic on $\Omega$ and $g(z)\neq$0 for all $z\in\Omega$. Prove that ${f\over g}$ is analytic and the derivative is ${f'g-g'f\over g^2}$. I know that ${f\over g}$ is equal to $f$ multiplied by ${1\over g}$ so since $f$ is analytic I only need to prove ${1\over g}$ is analytic which I'm having trouble doing. I've got as far as $\lim_{h\to 0}$${1\over g(z+h)h}$-${1\over g(z)h}$=${g(z)-g(z+h)\over g(z+h)g(z)h}$
but I'm unsure what to do next","['complex-analysis', 'analysis']"
585665,bijection from $\mathbb{Q} - \{a\}$ to $\mathbb{Q}$ using elementary functions only?,"I was wondering, can you define a bijection from $\mathbb{Q} - \{a\}$ to $\mathbb{Q}$ using elementary functions only ($a \in \mathbb{Q}$)? Of course there are many set theoretic bijections like that, but I'm looking for a constructive one, a formula, using elementary functions, a smile and a hug if you can do it using field operations only. Edit:
- Elementary functions are field operations, exponentials, logarithms, constants, roots and such.
- You may give piecewise functions, but only a finite number of convex pieces.","['elementary-functions', 'elementary-set-theory', 'field-theory']"
585684,Composition of Borel relations,"Let $X,Y,Z$ be Polish spaces, or standard Borel spaces, and let us consider two relations $A \subseteq X \times Y$ and $B \subseteq Y \times Z$ that are Borel sets. Define their composition as
$$
  C := \{(x,z): \; \exists y \text{ such that }(x,y)\in A \text{ and } (y,z)\in B\}.
$$
Is $C$ a Borel subset of $X \times Z$? I can show that it is at least analytic.","['measure-theory', 'descriptive-set-theory']"
585685,If a function is smooth is 1 over the function also smooth,If $f(x):\mathbb{R}\rightarrow\mathbb{C}$ is $C^\infty$-smooth. Is $1/f(x)$ also $C^\infty$-smooth? $f(x)\neq0$,"['calculus', 'complex-analysis', 'recreational-mathematics']"
585687,The normal approximation of Poisson distribution,"(I've read the related questions here but found no satisfying answer, as I would prefer a rigorous proof for this because this is a homework problem) Prove: If $X_\alpha$ follows the Poisson distribution $\pi(\alpha)$, then
$$\lim_{\alpha\rightarrow\infty}P\{\frac{X_\alpha-\alpha}{\sqrt{\alpha}} \leq u \} = \Phi(u)$$ where $\Phi(u)$ is the cdf of normal distribution $N(0,1)$ Hint: use the Laplace transform $E(e^{-\lambda(X_\alpha-\alpha)/\sqrt{\alpha}})$, show that as $\alpha\rightarrow\infty$ it converges to $e^{\lambda^2/2}$ I did the transform but failed to sum the series(which is essentially doing nothing) Here's what I got: $$g(\lambda)=\sum_{n=0}^{\infty} \frac{e^{-\alpha}}{n!}\alpha^n e^{-\frac{\lambda(n-\alpha)}{\sqrt{\alpha}}}$$ and $\lim_{\alpha\rightarrow\infty} g(\lambda)=e^{-\lambda^2}$ is what I'm trying to arrive at. I tried L'Hospital only to find that the result is identical to the original ratio.","['probability-theory', 'probability-distributions', 'sequences-and-series', 'probability']"
585688,A basic real analysis question on limit sup and limit inf,Suppose $f$ be a function from $\Bbb R$ to $\Bbb R$. Now $\limsup_{t->x} f(t) - \liminf_{t->x} f(t) < \frac{1}{k}$. Does this imply that there is an open interval $I_x$ containing $x$ such that $$\sup\{f(t):t \in I_x\} - \inf\{f(t):t \in I_x\} < \frac{1}{k}$$. I think we can tell the following : $$\sup\{f(t):t \in I_x(\epsilon)\} - \inf\{f(t):t \in I_x(\epsilon)\} < \frac{1}{k}+\epsilon$$ for any $\epsilon > 0$. But how the above ?,"['real-analysis', 'limits']"
585690,Does the center of a convex region lie within that region?,"There's probably a simple result that says this is true, but I sure can't find it.  It seems obvious, though. Let $D$ be a closed, compact region in $\Re^n$.  Further, let $D \subseteq [0,l]^n$ and let $B_d^\epsilon(a)$ be an $\epsilon$-ball with distance metric $d$ (e.g., the 1-norm) centered at $a \in [0,l]^n$.  Let $C$ be the non-empty intersection of $B_d^\epsilon(a)$ and $D$.  The diameter of $C$ is defined as $diam(C) = \max_{x,x' \in C} d(x,x')$. Defined analogously, $diam(B_d^\epsilon(a)) = 2 \epsilon$ and $ diam(B_d^\epsilon(a))\geq diam(C)$. Does the set of points $X^* = argmin_{x\in B_d^\epsilon(a))}[\max_{x' \in C}d(x,x')]$ include an element of $C$?  Similarly, does there exist $x \in C$ s.t. $C \subseteq B_d^{\epsilon}(x)$? Can we find a proof that this is true for C convex? Remarks:
What we're really trying to prove here is that $C$ lies within an $\epsilon$-ball of some point in its own interior if $C$ is convex. 1) We can construct examples with $C$ non-convex, such that the intersection of $X^*$ and $C$ is empty. 2) The minimizer of $d(x,x')$ for any $x'\in C$ is $x = x'$; the objective we're trying to minimize (itself convex) is a maximum over the loss incurred by any of infinitely many convex problems, each of which has its unique minimizer in $C$.  Is there a result which says that the minimum of such a problem is attained by $x \in$ the convex hull of the solutions of the subproblems (i.e., $C$ if it is convex)? 
3) The minimizing set $X^*$ is the infinite intersection of $B_d^{r}(x) \forall x \in C$, where $r$ is the minimum value such that this infinite intersection is non-empty.  Since the individual balls are convex and compact, so is $X^*$.  Further, since $x \in C$, the intersection of each $B_d^{r}$ with $C$ is non-empty.  If we could establish (somehow via convexity of $C$) that any collection of $n$ such $r$-balls with their centers in $C$ have a non-empty intersection with $C$, we could use Helly's Theorem to prove that $C$ intersects the infinite intersection as well. So, anyone with a) an authoritative source which confirms one of my missing links (or just gives the main result) or
b) an independent proof?","['geometry', 'convex-optimization', 'elementary-set-theory']"
585701,"Limit of $\frac{x_{n}}{n}$ if $ x_{n} = \sqrt{{n \over 2} + x_{n - 1}x_{n - 2}\,}$","Let real sequence  $\left\{x_{n}\right\}$ such $x_{0} = x_{1} =1$, and
  $$
x_{n} = \sqrt{{n \over 2} + x_{n - 1}x_{n - 2}\,}\,,\qquad n \geq 2\,;\qquad
\mbox{Find}\quad\lim_{n\to\infty}{x_{n} \over n} =\ {\large ?}$$ My try:
$\displaystyle{\mbox{Since}\quad
\left(~x^{2}_{n} = {n \over 2} + x_{n - 1}x_{n - 2}
\quad\Longrightarrow\quad 2x^{2}_{n} = n + 2x_{n - 1}x_{n - 2}~\right)}$
and I know this $x_{n}$ don't have simple form. I guess 
$\displaystyle{\lim_{n\to \infty}{x_{n} \over n} = {\sqrt{6} \over 6}}$. So I ask: How find this limit is $\displaystyle{\sqrt{6} \over6}$ ?. Then I can't. Thank you.  very much!","['sequences-and-series', 'limits']"
585713,QR factorization of a special structured matrix,"A friend asked me the following interesting question: Let $$A = \begin{bmatrix} R \\ \xi{\rm I} \end{bmatrix},$$ where $R \in \mathbb{R}^{n \times n}$ is an upper triangular and ${\rm I}$ is an identity matrix, both of order $n$ , and $\xi \in \mathbb{R}$ is a scalar. Is there an efficient way to compute a QR factorization of $A$ ? I have found this question with a very nice answer, but I'd like to avoid doing the SVD because it is computationally expensive and my $R$ is not a constant like $W$ in that other question. Also, my $R$ is already triangular, which I hope can somehow be used. Edit: There was a comment (turned into an answer while I was writing this edit) on using Givens rotations. Since this is a logical first idea, I'd like to explain why I don't like it. We could use Givens rotations to cancel out the elements of $\xi{\rm I}$ , but each Givens rotation is computing two linear combinations of two rows. That means that if I cancel out the first element of $\xi{\rm I}$ , I will also introduce a bunch of non-zeros to the rest of that row. This means that I would need to go through the whole upper triangle of the bottom block, same as I'd have to do if $\xi{\rm I}$ was a general upper triangular matrix. Given that it is a diagonal matrix (with all its diagonal elements being the same, although I suspect this doesn't help much), I am hoping to get more efficient than that.","['numerical-linear-algebra', 'matrices', 'linear-algebra', 'matrix-decomposition']"
585761,Classifying extensions of a $k$-algebra,"Background .
Let $k\to A$ be a finitely generated $k$-algebra, $I$ an $A$-module. According to Sernesi's notation, I'll denote by $\textrm{Ex}_k(A,I)$ the $A$-module of isomorphism classes of $k$-extensions
$$\eta: \,\,\,\,\,0\to I\to A'\to A\to 0$$
of $A$ by $I$ (the surjection is a $k$-algebra homomorphism, by definition). Let us write $A$ as a quotient of a polynomial algebra $P=k[x]$:
$$0\to J\to P\to A\to 0.$$
The conormal sequence $J/J^2\to \Omega_{P/k}\otimes_PA\to \Omega_{A/k}\to 0$
induces a map 
$$\alpha:\textrm{Hom}_A(\Omega_{P/k}\otimes_PA,I) \to \textrm{Hom}_A(J/J^2,I).$$ I am trying to prove the following: $$\color{blue}{\textrm{Ex}_k(A,I)\cong\textrm{coker } \alpha}.$$ We have thus fixed an extension $\eta$ as above. Now, as $P$ is a smooth $k$-algebra, the map $P\to A$ lifts to a map $g:P\to A'$ (this is the infinitesimal lifting property ). I think the main point here is: in how many ways? If $I$ has generators $\{e_i\}$ over $A$, and $y=\{y_i\}$ is a set of variables indexed like the $e_i$'s, $g$ induces exactly one morphism $$Q:=P\otimes k[y]\to A'$$ compatible with $g$. I want to show that moving in the isomorphism class of $\eta$ is the same as moving in the set of possible liftings $P\to A'$ or, which is the same, in the set of possible morphisms $Q\to A'$. There is only one thing I know, and I am unable to use it. What I know is that if I have two liftings $g,g'$, their difference $g-g':P\to I$ is an element of $\textrm{Der}_k(P,I)$. Conversely, the sum of a lifting and a such derivation is a new lifting. Moreover, I was able to show that
$$\textrm{Hom}_A(\Omega_{P/k}\otimes_PA,I)\cong\textrm{Hom}_P(\Omega_{P/k},I)\cong\textrm{Der}_k(P,I).$$
So I can view $\alpha$ as a morphism $\textrm{Der}_k(P,I)\to \textrm{Hom}_A(J/J^2,I)$.
But I cannot go further, e.g. I am not sure that $\eta$ defines a uniquely determined map $J/J^2\to I$ in the target of $\alpha$. How can I conclude from here? Thanks for any help.","['algebraic-geometry', 'abstract-algebra']"
585764,Essential supremum of a conditional expectation,"Given the function
\begin{equation}
  P(x,t) := \sup\limits_{t \le \tau \le T} E\left( g(X^{t,x}_{\tau}) \right)
\end{equation}
where $X^{t,x}$ is the unique solution to the SDE
\begin{equation}
  X_u = x + \int\limits_t^u \mu(X_s, s) \, ds + \int\limits_t^u \sigma(X_s, s) \, dB_s
\end{equation}
for $x >0$, $t \in [0,T]$, $u \in [t,T]$ and $\tau$ is a stopping time with $P(\tau \in [t,T]) =1$ and $g$ is a continuous R-valued function. Then we have for the process $X = X^{0,s}$ (defined analogously)
\begin{equation}
  P(X_t, t) = \text{ess sup}_{t \le \tau \le T} E\left(g(X_{\tau}) \, | \, \mathcal{F}_t \right)
\end{equation}
where $\mathcal{F}_t = \sigma(B_s: s \in [0,t])$. I found this statement but I don't know how to argue correctly to get that relation. Somehow the expectation in the definition of the function $P$ is conditioned on $X^{t,x}_t = x$ which is obviously an a.s.-event, but what happens when I have a random variable instead of a deterministic $x$? And how do I get the equality of the supremum and the essential supremum? Maybe you can have a look at my attempt I have figured out so far:
\begin{align}
  P(X_t, t) & = \sup\limits_{t \le \tau \le T} E\left( g(X^{t,X_t}_{\tau}) \right) \\
            & = \sup\limits_{t \le \tau \le T} E\left( g(X^{t,X_t}_{\tau}) \, | \, X^{t,X_t}_t = X_t \right) \\
            & = \sup\limits_{t \le \tau \le T} E\left( g(X^{t,X_t}_{\tau}) \, | \, X_t \right) \\
            & \text{(Is the expected value $E( \cdot \, | \, X^{t,X_t}_t = X_t)$ really equal to the conditional expectation $E( \cdot \, | \, X_t)$? If so, how do I argue here precisely?)} \\
            & = \sup\limits_{t \le \tau \le T} E\left( g(X_{\tau}) \, | \, X_t \right) \\
            & \text{(flow property of solutions to the SDE)} \\
            & = \sup\limits_{t \le \tau \le T} E\left( g(X_{\tau}) \, | \, \mathcal{F}_t \right) \\
            & \text{(Markov property of solutions to the SDE)} \\
            & = \text{ess sup}_{t \le \tau \le T} E\left( g(X_{\tau}) \, | \, \mathcal{F}_t \right) \\
            & \text{(???)}
\end{align}
Has anyone an idea how to get the a.s. equality of $\sup$ and ess sup? I think the key to that is the measurability of the sup, but I have no idea how to prove that for the set of stopping times.","['stochastic-calculus', 'stochastic-processes', 'probability', 'stopping-times']"
585768,"How to integrate the bump functions, i.e., $\int_a^{b}e^{-\frac{1}{x-a}+\frac{1}{x-b}}dx$, where $a<b$.","Since $$\lim_{x\to{a}}e^{-\frac{1}{x-a}+\frac{1}{x-b}}=\lim_{x\to{b}}e^{-\frac{1}{x-a}+\frac{1}{x-b}}=0,$$ $e^{-\frac{1}{x-a}+\frac{1}{x-b}}$ is continuous on the interval $[a,b]$ (taking $0$ if $x=a$ or $b$ ). So the integral $\int_a^{b}e^{-\frac{1}{x-a}+\frac{1}{x-b}}dx$ makes sense. But i do not know how to compute this integral. In particularly, taking $a=0$ and $b=1$ , we just need to compute $\int_0^{1}e^{-\frac{1}{x}+\frac{1}{x-1}}dx$ . My thought: Considering another integration $$I(\epsilon)=\int_0^{1}e^{-\frac{1}{x}+\frac{1}{x-1}}e^{-\epsilon{(\frac{1}{x^2}-\frac{1}{(x-1)^2}})}dx,$$ take the derivative of this with respect to $\epsilon$ (assuming it converges uniformly), we get $$I'(\epsilon)=-\int_0^{1}e^{-\frac{1}{x}+\frac{1}{x-1}}e^{-\epsilon{(\frac{1}{x^2}-\frac{1}{(x-1)^2}})}d_{-\frac{1}{x}+\frac{1}{x-1}}.$$ I can not continue, and i do not know whether it work. Can you provide me some methods?","['definite-integrals', 'integration', 'analysis']"
585772,"if the matrix such $B-A,A$ is Positive-semidefinite,then $\sqrt{B}-\sqrt{A}$ is Positive-semidefinite","Question: let the matrix $A,B$ such  $B-A,A$ is Positive-semidefinite show that: $\sqrt{B}-\sqrt{A}$ is Positive-semidefinite maybe The general is true? question 2: (2)$\sqrt[k]{B}-\sqrt[k]{A}$ is Positive-semidefinite This problem is very nice,because we are all know this
if $$x\ge y\ge 0$$,then we have
$$\sqrt{x}\ge \sqrt{y}$$ But in matrix,then this is also true,But I can't prove it.Thank you","['matrices', 'linear-algebra']"
585787,What's the difference between isomorphism and homeomorphism?,"I think that they are similar (or same), but I am not sure. Can anyone explain the difference between isomorphism and homeomorphism ?","['differential-topology', 'abstract-algebra']"
585790,Convergence in probability of product and division of two random variables,"How can I prove the following: Let $X_i$ and $Y_i$, $i = 1, \ldots, n$, $X$ and $Y$ be random variables defined on the probability space $(\Omega, \mathcal F, \mathbb P)$ and assume that $X_n$ converges in probability to $X$ and $Y_n$ to $Y$ (also in probability). Then: If $Y_n \ne 0$ and $Y \ne 0$ almost surely, then $X_n/Y_n$ converges in probability to $X/Y$. I tried the following: Let $\epsilon > 0$. We need to show that
\begin{align*}
\mathbb P(|X_n/Y_n - X/Y| > \epsilon) \xrightarrow{n \to \infty} 0.
\end{align*}
Define $Z_n := 1/Y_n$ and $Z := 1/Y$. $Z_n$ and $Z$ are well defined, since $Y_n, Y \ne 0$ a.s. It is
\begin{align*}
\mathbb P(|Z_n-Z| > \epsilon) &= \mathbb P(|1/Y_n - 1/Y| > \epsilon) \\
&= \mathbb P(|Y-Y_n|/|Y_nY| > \epsilon) \\
&= \mathbb P(|Y_n-Y| > \epsilon |Y_n||Y|) \\
&\le \mathbb P(|Y_n-Y| > \epsilon (|Y|-|Y_n-Y|)|Y|) \\
&= \mathbb P(|Y_n-Y| > \epsilon (|Y|-|Y_n-Y|)|Y|, |Y_n-Y| \le |Y|/2) \\
&\quad + \mathbb P(|Y_n-Y| > \epsilon (|Y|-|Y_n-Y|)|Y|, |Y_n-Y| > |Y|/2) \\
&\le \mathbb P(|Y_n-Y| > \epsilon Y^2/2) + \mathbb P(|Y_n-Y| > |Y|/2).
\end{align*} 
Now for any $A > 0$
\begin{align*}
\mathbb P(|Y_n-Y| > \epsilon Y^2/2) 
&= \mathbb P(|Y_n-Y| > \epsilon Y^2/2, |Y| \ge 1/A) + \mathbb P(|Y_n-Y| > \epsilon Y^2/2, |Y| < 1/A) \\
&\le \mathbb P\left(|Y_n-Y| > \frac{\epsilon}{2A^2}\right) + \mathbb P(|Y| < 1/A)
\xrightarrow{n \to \infty}  \mathbb P(|Y| < 1/A)
\end{align*}
and similarly
\begin{align*}
\mathbb P(|Y_n-Y| > |Y|/2) &= \mathbb P(|Y_n-Y| > |Y|/2, |Y| \ge 1/A) + \mathbb P(|Y_n-Y| > |Y|/2, |Y| < 1/A) \\
&\le \mathbb P\left(|Y_n-Y| > \frac{1}{2A}\right)  + \mathbb P(|Y| < 1/A)
\xrightarrow{n \to \infty} \mathbb P(|Y| < 1/A).
\end{align*}
Since we can choose $A$ arbitrarily large, we obtain
\begin{align*}
\mathbb P(|Z_n-Z| > \epsilon) \xrightarrow{n \to \infty} 0.
\end{align*}
The result follows since we know that $X_nZ_n$ converges to $XZ$ in probability. Edit: Is it complete now? Is there maybe a shorter proof? Maybe my estimations are too long or there is shorter way to do it.","['probability-theory', 'convergence-divergence', 'random-variables']"
585791,Countable product of complete metric spaces [duplicate],This question already has answers here : Countable product of Polish spaces (2 answers) Closed 10 years ago . Someone that can give me a proof of that a countable product of complete metric spaces is complete ?,"['general-topology', 'metric-spaces']"
585808,"Show that $|\sin\frac{1}{n^2}|<\frac{1}{n^2}$, $n=0, 1, 2, \dots$","As part of showing that
$$
\sum_{n=1}^\infty \left|\sin\left(\frac{1}{n^2}\right)\right|
$$
converges, I ended up with trying to show that
$$
\left|\sin\left(\frac{1}{n^2}\right)\right|<\frac{1}{n^2}, \quad n=1, 2, 3,\dots
$$
since I know that the sum of the right hand side converges. But I can't show this. I've tried searching but I haven't been able to find anything. What I've tried is that firstly, the absolute values are not needed since $\sin x>0$ if $0<x<1$. I rearranged a little bit:
$$
\sin\left(\frac{1}{n^2}\right)-\frac{1}{n^2}<0
$$
and the derivative is
$$
\frac{2}{n^3}\left(1-\cos\left(\frac{1}{n^2}\right)\right)> 0
$$
so my idea of showing that it is decreasing and negative for the first $n$ wouldn't work. How can I show this? Help is appreciated. Edit: Maybe I should add that I'm not completely sure it is true, but I tried it numerically and it seems like it.","['inequality', 'sequences-and-series']"
585825,"Proving that $\operatorname{codim}(X,Y)=\dim(\mathcal{O}_{X,\eta})$","I have a problem with the following exercise: Let $X$ be a noetherian scheme and $Y \subset X$ an irreducible closed subset. I have to prove that $$ \operatorname{codim}(X,Y)=\dim(\mathcal{O}_{X,\eta}), $$ where $\eta$ is the generic point of $Y$ . We can replace $X$ with an open affine $U_\eta$ , neighbourhood containing the generic point $\eta$ . So we have that $$\operatorname{codim}(X,Y)=\operatorname{codim}(U_\eta, U_\eta \cap Y) ,$$ but I don't know how to conclude the proof. In particular if $Z$ is an irreducible component of $X$ , can I conclude that $\operatorname{codim}(Z,X)=0$ ? Thanks in advance!",['algebraic-geometry']
585830,Prove that there is no homomorphism from $\mathbb{Z}_{8} \oplus \mathbb{Z}_{2}$ ont0 $\mathbb{Z}_{4} \oplus \mathbb{Z}_{4}$,"Prove that there is no homomorphism from $\mathbb{Z}_{8} \oplus \mathbb{Z}_{2}$ ont0 $\mathbb{Z}_{4} \oplus \mathbb{Z}_{4}$. My idea for the proof : Let $\phi$ be such homomorphism. Since Ker $\phi$ is a subgroup of $\mathbb{Z}_{8} \oplus \mathbb{Z}_{2}$ , then i must find all possible subgroups of it and then prove that $(\mathbb{Z}_{8} \oplus \mathbb{Z}_{2})/$Ker$\:\phi $ is not isomorphic with $\mathbb{Z}_{4} \oplus \mathbb{Z}_{4}$ where Ker $\phi$ can be any of the subgroup. To prove it isnt isomorphic is by finding elements in $\mathbb{Z}_{8} \oplus \mathbb{Z}_{2}$ that has order such that no elements in $\mathbb{Z}_{4} \oplus \mathbb{Z}_{4}$ have that order. Is there any better way..?","['finite-groups', 'abstract-algebra']"
585851,How do I 'reverse engineer' the standard deviation?,"My problem is fairly concrete and direct. My company loves to do major business decisions based on many reports available on the media. These reports relates how our products are fairing in comparison to the competitor's offerings. The latest report had these scores (as percentages): Input values (%)
73.5, 16.34, 1.2, 1.15, 0.97, 0.94, 0.9, 0.89, 0.81, 0.31 Our product in the 'long' tail of this list. I argued with them that besides spots #2 and #1 all the other following the tail are on a 'stand', since, probably, the standard deviation will be much bigger that the points that separates everyone in the tail. So the question is: How may I calculate the standard deviation having only these percentual values available?","['statistics', 'statistical-inference', 'data-analysis', 'probability-distributions', 'probability']"
585854,How to prove that $\limsup _{n \rightarrow \infty} f(x_n) \geq f(\limsup _{n \rightarrow \infty} (x_n))$,"We were given HW where it is asked to prove that, $\limsup_{n \rightarrow\infty} f(x_n) \geq f(\limsup_{n \rightarrow \infty} (x_n))$, for any bounded sequence $\{x_n\}$ and continuous function $f\colon\mathbb{R} \rightarrow \mathbb{R}$. Could anyone give me a direction? Thank you!","['inequality', 'calculus', 'continuity', 'limsup-and-liminf']"
585859,Change of Coordinate in Differential Equation,"I'm sorry, it's probably a very simple question but I'm confused between change of variable and change of coordinate in a differential equation. To take a very simple example, let's start with this equation for the function $f(x,t)$:
$$
\frac{\partial f}{\partial t} = \alpha \frac{\partial^2 f}{\partial x^2}
$$
and say that I want to find the new coordinate $y$ such that $f(y,t)$ verifies:
$$
\frac{\partial f}{\partial t} = \frac{\partial^2 f}{\partial y^2}
$$ Is the answer $y = x / \alpha$ or $y = x / \sqrt{\alpha}$? It seems to be the second one, but I can't explain rigorously why.. What if $\alpha$ was negative, would we change for a complex coordinate? This seems weird to me. What confuses me is that both the variable of f and the variable wrt which we differentiate change. The fact that both happen at once makes it unclear to me what happens exactly, and makes me suspicious about considering the $\partial \cdot^2$ simply as a ""squared"" $\partial \cdot$, as the notation suggests. Can anyone please explain clearly what's going on there? Thanks in advance.","['ordinary-differential-equations', 'partial-differential-equations']"
585897,Limit of a sequence with ln.,"even though it's not actually a homework rather than some training for myself, I'm posting it with tag ""Homework"": $ \ln(x)^4 $ means: $ \left( \ln (x) \right)^4 $ What is $\lim_{x\to 0} (x^a \cdot \ln(x)^4)$ ? I am not allowed to use L-Hospital. I tried writing it like: $\lim_{x\to 0} (x^a \cdot ln(x)^4) = \lim_{x \to 0} (e^{a\cdot \ln(x)} * e^{\ln(\ln(x)^4)}) = \lim_{x \to 0} (e^{a\cdot \ln(x) + \ln(\ln(x)^4})$ ... Well, I'm really not sure whether I made the right choice(s) ;-(",['limits']
585901,How to prove that $f(f(x))=-x$ implies that $f$ is not continuous? [duplicate],"This question already has answers here : Find a real function $f:\mathbb{R}\to\mathbb{R}$ such that $f(f(x)) = -x$? (7 answers) Closed 10 years ago . I am trying to prove that: Given an $f:\mathbb{R} \rightarrow \mathbb{R}$, if $f(f(x))=-x$
then  $f$ is not continuous? any help? Thank you!","['calculus', 'functional-equations']"
585914,Finding the GCD of $50!$ and $2^{50}$,"I've been trying to figure out how $n!$ and $x^n$ are related (where x is an integer) for most of the morning - I know it must be the key to unlocking this problem. Up to this point I've only used the Extended Euclidean Algorithm to find GCDs, but I know that's not going to work in this case. The hard part is obviously coming up with the key insight / theorem to apply. If anyone can help me get that insight without telling me outright, that would be awesome. Otherwise, a hint is much appreciated. Thanks.","['algebra-precalculus', 'arithmetic-functions']"
585948,Definition of tangent cone in continuous optimization .,"Looking at the definition of tangent cone in continuous optimization : If $M$ is a open subset of $\mathbb R^n$ $x \in M$, The tangent cone of $M$ at $x$ is defined by 
$$\mathbb T (M, x) = \big\{d \in \mathbb R^n |   \exists x_k \subset M , \eta_k \subset \mathbb R   : \eta_k \to 0  , x_k \to x \text{ and } \frac{x_k-x}{\eta_k} \to d\big\} $$ I don't understand the definition , what is actually happening here ? Can anyone explain me elaborately . I kind of make sense if i consider $M=\{y : y=x^2\} \subset \mathbb R^2$ . But for a general set $M$ i don't get the idea.","['optimization', 'nonlinear-optimization', 'analysis']"
585952,The cut-off function on Riemannian manifold,"Let $M$ be a complete Riemannian manifold. Can we find a constant $C>0$ so that for any $p\in M$ and $R>0, 2R<\text{inj}(M)$, we can find a function $\varphi \in C^\infty _c(B_p(2R))$, such that $\varphi=1 $ on $B_p(R)$ and $|\nabla \varphi| \leq C/R$? ($B_p(R)$ is the geodesic ball with center $p$)","['riemannian-geometry', 'differential-geometry']"
