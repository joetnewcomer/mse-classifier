question_id,title,body,tags
3851718,A Morse Function with Minimum Number of Critical Points,"Let $M$ be a closed manifold. If $f$ is a Morse function on $M$ , then by Morse inequalities we know that $f$ must have at least $\sum_i\beta_i(M;\mathbb{Z}_2)$ critical points. When is it possible to find such a function with exactly this number of critical points? If one can find such a function, then the boundary maps in the Morse complex must be zero. Does there exist any sort of topological property that can prevent from boundary maps being zero? For instance, on a closed surface, this is possible since if we take a Morse function with only one max point and one min point, then the boundary maps in
the Morse complex must be zero and we have $\beta_1(M;\mathbb{Z}_2)$ critical points of index $1$ , one with zero index and one with index $2$ .","['differential-topology', 'morse-theory', 'differential-geometry']"
3851790,Euclidean algorithm for greatest integers x and y for common divisor (GCD),"I have a problem with finding the gcd of two numbers: gcd(4620, 8190) = 210. I did the following: 8190 / 4620 = 1 with remainder: 3570 4620 / 3570 = 1 with remainder: 1050 3570 / 1050 = 3 with remainder: 420 1050 / 420 = 2 with remainder: 210 420 / 210 = 2 with remainder: 0 GDC = 210 So far so good, but I need to find x and y as integers that satisfy this condition: 4620x + 8190y How can I achieve that? I did find that -9 and 16 satisfy this condition, but I don't know how to justify that. Do I need to substitute the numbers in the steps from the algorithm?","['euclidean-algorithm', 'discrete-mathematics']"
3851827,Argue with Taylor's formula with remainder that this holds.,I get Taylorpolynomials and that the remainder is of course the difference of the function and its taylorpolynomial to the n'th degree. But the rest is kind of throwing me off and how to use the fact that we know the derivative of n'th degree and that this is true for x bigger than 1.,"['taylor-expansion', 'calculus', 'algebra-precalculus', 'analysis']"
3851858,Finding the basis from two subspaces,"This problem involves finding the basis of the union and intersection of two subspaces. We have $V$ and $U$ which are subspaces of $\mathbb R^4$ $$V = \begin{Bmatrix} {(x_1, x_2, x_3, x_4) : x_1 + x_2 = x_3 + x_4}\end{Bmatrix}$$ $$U = span \{ \begin{bmatrix}0\\0 \\1 \\1 \\
\end{bmatrix}, \begin{bmatrix}3\\0 \\1 \\1 \\
\end{bmatrix}, \begin{bmatrix}0\\-1 \\2 \\-1 \\
\end{bmatrix} , \begin{bmatrix}
0\\
1 \\
0 \\
3 \\
\end{bmatrix} \} \\ $$ We want to find the dimension and basis for: $a)\text{ } U + V$ $b)\text{ } U \cap V$ My attempt:
Let me first try to find the column space of U and a basis for V
The $$rref(U) = \begin{bmatrix}
1 & 0 & 0 & 2\\
0 & 1 & 0 & 0\\
0 & 0 & 1 & -1\\
0 & 0 & 0 & 0\\
\end{bmatrix}$$ Since only the first three columns have pivot elements, only the first three rows of $U$ make up the column space: $\begin{bmatrix}0\\0 \\1 \\1 \end{bmatrix}, \begin{bmatrix}3\\0 \\1 \\1 \end{bmatrix}, \begin{bmatrix}0\\-1 \\2 \\-1\end{bmatrix}$ Now let's find a basis for $V = \begin{Bmatrix} {(x_1, x_2, x_3, x_4) : x_1 + x_2 = x_3 + x_4}\end{Bmatrix}$ . I did this by just making vectors that satisfied the constraint until I couldn't anymore. If this is the wrong way to do so, please let me know! The vectors I found were: $\begin{bmatrix}1\\1 \\2 \\0 \end{bmatrix}, \begin{bmatrix}1\\1 \\1 \\1 \end{bmatrix}, \begin{bmatrix}1\\1 \\0 \\2\end{bmatrix}$ . So I know we want to find the the dimension and basis for $U + V$ . I think what I should do now is find the linearly independent vectors of all seven vectors above. The number of vectors is going to be the dimension and the basis is just the independent vectors. Correct? How should I go about doing the same for $U \cap V$ . I'm stumped by the ""intersect"" symbol. Do they just want me to list the basis of vectors that the two have in common?","['matrices', 'self-learning', 'linear-algebra', 'vector-spaces']"
3851965,Probability about center of mass of 2n+1 random variables based on the 4 vertices,"There are four points in the coordinate plane as following. A=(1,1), B=(1,-1), C=(-1,1), D=(-1,-1) We will choose $2n+1$ points and each point is one of these 4 points, but the probability of choice is different. P(point is A)= $p$ , $\quad$ P(point is B)= $\frac{1}{2}-p$ P(point is C)= $\frac{1}{2}-p$ , $\quad$ P(point is D)= $p$ This probability is for choice of one point and all $2n+1$ choice are independent. Let $x$ be the center of mass of these $2n+1$ points. and $f(n,p)$ =probability that $x$ is on the first quadrant. For example, $f(0,\frac{1}{2})=\frac{1}{2}$ , $f(1,\frac{1}{2})=\frac{1}{2}$ , $f(0,\frac{1}{3})=\frac{1}{3}$ , $f(1,\frac{1}{3})=\frac{17}{54}$ . Prove: If $\frac{1}{4}<p<\frac{1}{2}$ , $f(n,p)$ is decreasing at n. If you tell me references for this proof, it would be great thanks.","['random-walk', 'recurrence-relations', 'geometry', 'probability']"
3851972,A Laplacian identity from Evans,"In Theorem 1 (Solving Poisson's equation) on page 24 of Partial Differential Equations (2e) by Evans comparing equation (11) and (13) there appears to be the equality of $$ \int_{\mathbb{R}^n \setminus  B(0,\epsilon)} \Phi(y)\Delta_x f(x-y) \,dy  = \int_{\mathbb{R}^n \setminus B(0,\epsilon)} \Phi(y)\Delta_y f(x-y) \,dy.$$ For context: $\Phi$ is the fundamental solution to the Laplace equation, $-\Delta u = f$ in $\mathbb{R}^n$ , and $B(0,\epsilon)$ is the ball of radius $\epsilon$ centered on zero. Where does the equality with respect to the Laplacians come from? That is why does $$ \Delta_xf(x-y) = \Delta_yf(x-y) $$ hold? I have seen this in another reference (page 149 of Partial Differential Equations in Action (3e) - Salsa). Is this a general property of convolution or is it something more subtle? In terms of level of understanding, explanations without reliance on measure theory would be preferred.","['laplacian', 'multivariable-calculus', 'convolution', 'partial-differential-equations']"
3852008,How to solve $f(x)+f^{-1}(x)=f'(x)+\ln x$?,"When I was playing around with functional equations, I came up with a problem which has a simple trivial answer. I'm talking about the following equation: $f(x)+f^{-1}(x)=f'(x)+\ln x$ The trivial solution is $f(x)=e^x$ , but it's not clear to me how would one go about solving for $f(x)$ from scratch. Is it even possible to solve such an equation without guessing? Are there any methods that can be used to tackle functional differential equations involving the inverse function?","['functional-equations', 'inverse-function', 'ordinary-differential-equations']"
3852022,Lie groupoid associated to an incomplete vector field,"Let $M$ be a smooth manifold and $X \in \mathfrak{X}(M)$ be a vector field. If $X$ is complete, its flow defines a group action $\Bbb R \circlearrowright M$ via $t \cdot x \doteq \Phi_X(t,x)$ . This gives us an action groupoid $\mathcal{G} \rightrightarrows M$ , where $$\mathcal{G} = \{(t,x,y) \in \Bbb R \times M \times M \mid x = \Phi_X(t,y)\},$$ with ${\sf s}(t,x,y) = y$ , ${\sf t}(t,x,y) = x$ , $M \hookrightarrow \Bbb R \times M \times M$ via $x \mapsto (0,x,x)$ and the operation is $(t,x,y)\cdot (s,y,z) = (t+s,x,z)$ , everything standard here. In Exercise 1.9 of Eckhard Meinrenken's notes , he wants to consider the case where $X$ is not complete, and the flow domain is just an open set $U \subseteq \Bbb R \times M$ (containing $\{0\} \times M$ ). The exercise asks us to show that $U$ can be turned into a Lie groupoid $U\rightrightarrows \Bbb R$ instead, apparently unrelated to the action groupoid described above. It's completely unclear to me how to go about this. First because we're changing the manifold of units. Second because there doesn't seem to be a natural way to embed $\Bbb R \hookrightarrow U$ . Third because things will go wrong with the operation above, since for $x \in M$ with $(t,x),(s,x) \in U$ , one might not have $(t+s,x) \in U$ (duh). Naively writing $$U = \bigsqcup_{x \in M} (a_x,b_x)$$ and trying to define the source and target maps by relating $t$ in $(t,x)$ with $a_x$ and $b_x$ also seems bad, since we could have $a_x = -\infty$ and/or $b_x = +\infty$ even with $X$ incomplete. Can you give me some ideas? I don't care for a full solution, I'm doing this for fun. If you tell me how to embed $\Bbb R \hookrightarrow U$ I'd say I already have a decent chance of figuring out the rest on my own. I don't know.","['lie-groupoids', 'vector-fields', 'differential-geometry']"
3852027,Probability/Combinatorics,"The Question is: An urn contains 6 black and an unknown number ( $\le6$ ) of white balls. Three balls are drawn one by one without replacement and all of them are found to be white. What is the probability that the next ball drawn is black? I propose the following solution:
We calculate the probability that there are r white balls in the urn using Bayes' Theorem. So this will be: $$ f(r) = \frac{\frac{{r}\choose{3}}{{6+r}\choose{3}}}{\sum_{r=3}^6\frac{{r}\choose{3}}{{6+r}\choose{3}} }$$ where f(r) denotes the probability that their are r white balls.
Now, if their were r white balls, the probability that the next draw will be black is $g(r) = \frac{6}{6 + (r-3)} = \frac{6}{3+r}$ , because 3 white balls have already been picked. Now to get the total probability that the next ball drawn is black can be given by: $$ \sum_{r=3}^6 f(r)\cdot g(r)$$ Is this approach correct? The answer obtained (I calculated using a python script) does match in decimal value with one of the options ( $\frac{677}{909})$ which is equal to 0.744. If it is correct, I would like to know if there is a more elegant/less calculative way of approaching this question.
Thanks!","['combinatorics', 'probability']"
3852114,Could I ask where i mistook in using riccati matrix equation?,"I was trying to solve $$ \dot{x}=ax^2+bxy\tag{1}\label{eq1}$$ $$ \dot{y}=cy^2+dxy \tag{2}\label{eq2}$$ According to riccati equation's initial showing $$
\frac{d}{dt} \left( \begin{bmatrix}
M(t) \\
N(t)
\end{bmatrix} \right)
 = 
\begin{bmatrix}
-A&-B\\
C&D 
\end{bmatrix}
\begin{bmatrix}
M(t) \\
N(t)
\end{bmatrix}
where  \ P=N(t)M(t)^{-1}, then \ \dot{P}= PBP + AP + PD + C
\tag{3}\label{eq3}$$ To match \eqref{eq1} and \eqref{eq2} with the form \eqref{eq3}, I applied small change as follows, $$
\frac{d}{dt} \left( \begin{bmatrix}
x \\
y
\end{bmatrix} \right)
 = 
\begin{bmatrix}
x& 0\\
0& y 
\end{bmatrix}
\begin{bmatrix}
a& b\\
c& d 
\end{bmatrix}
\begin{bmatrix}
x \\
y
\end{bmatrix}
\tag{4}\label{eq4}$$ and changed to the following $$
\frac{d}{dt} \left( \begin{bmatrix}
x& 0\\
0& y 
\end{bmatrix} \right)\begin{bmatrix}
1 \\
1
\end{bmatrix}
 = 
\begin{bmatrix}
x& 0\\
0& y 
\end{bmatrix}
\begin{bmatrix}
a& b\\
c& d 
\end{bmatrix}
\begin{bmatrix}
x& 0\\
0& y 
\end{bmatrix} \begin{bmatrix}
1 \\
1
\end{bmatrix}\tag{5}\label{eq5}
$$ This form looks similar to \eqref{eq3} $$
\dot{P}=PBP \tag{6}\label{eq6}
$$ Therefore, I transformed \eqref{eq6} to \eqref{eq3} $$
\frac{d}{dt} \left( \begin{bmatrix}
M(t) \\
N(t)
\end{bmatrix} \right)
 = 
\begin{bmatrix}
0&-B\\
0&0 
\end{bmatrix}
\begin{bmatrix}
M(t) \\
N(t)
\end{bmatrix}
where  \ X=N(t)M(t)^{-1} \tag{7}\label{eq7} \ and \ \, H=\begin{bmatrix}
0&-B\\
0&0 
\end{bmatrix}$$ Unlike riccati matrix equation, H in \eqref{eq7} is singular. So, directly applying riccati method seems not possible. Therefore, I chose exponential matrix of H. $$
\begin{bmatrix}
M(t) \\
N(t)
\end{bmatrix} = e^{Ht}
\begin{bmatrix}
M(0) \\
N(0)
\end{bmatrix}
, where \ X=N(t)M(t)^{-1} \tag{8}\label{eq8}
$$ and I multiply $$ \begin{bmatrix} 1 \\ 1 \end{bmatrix}$$ to  X in \eqref{eq8}.$$ Ok, I tested the result at the 1 dimension such as $$\dot{x} =ax^2$$ I showed well. However, when I applied it into 2 dimensional problem that I wondered such as \eqref{eq1}  \eqref{eq2}, It does not satisfied. Could you let me know why and which part of my assumption was incorrect? Such as $$\begin{bmatrix}
x& 0\\
0& y 
\end{bmatrix} \begin{bmatrix}
1 \\
1
\end{bmatrix}$$ or etc.","['nonlinear-system', 'differential-topology', 'derivatives', 'ordinary-differential-equations']"
3852195,Right angled triangle inscribed in a regular pentagon; find angle,"$ABCDE~$ is a regular pentagon. Find $x$ . I am having a lot of trouble with this geometry problem. Simply using trigonometry seems to always lead me to a dead end and I can't leverage any circle theorems (I chose the midpoint of the long side as the radius as it is a right triangle, but it didn't lead me anywhere). I noticed that the angles of the triangle are akin to dividing the triangle you get when you divide the pentagon into $10$ pieces but I also do not know how to use that information. I tried cutting up the pentagon and gluing it back together, combining with different pentagons. I don't know any more ways of solving geometry problems any more.","['triangles', 'angle', 'geometry']"
3852246,"If $x$ maximizes $x^T(u+v)$ subject to $\|x\|^*\le 1$, then $x^T u \ge x^T v \Leftrightarrow \|u\| \ge\|v\|$","Suppose that $\|\cdot\|$ is a norm on $\mathbb{R}^n$ and $\|\cdot\|^*$ is its dual norm. Consider two vectors $u$ and $v$ in $\mathbb{R}^n$ such that $u+v\neq 0$ . Let $x$ be a vector that maximizes $x^T(u+v)$ subject to $\|x\|^* \le 1$ , the optimal value being $\|u+v\|$ as we know. If $\|\cdot\|$ is the $2$ -norm, then $x= t(u+v)$ with a scalar $t>0$ . Hence $$
x^T u -x^T v= t(u+v)^T(u-v) = t(\|u\|^2 -\|v\|^2),
$$ which tells us that $$
x^T u \ge x^T v \quad \Longleftrightarrow \quad \|u\|\ge \|v\|.
$$ Roughly speaking, if $u+v$ is closer to $u$ than to $v$ , then $x^Tu$ is larger than $x^T v$ with this special $x$ , and vice versa. Why is this interesting ? Imagine that $u$ is a signal and $v$ is a perturbation. In some scenarios, we can observe $u+v$ only by the linear functional $x$ . Naturally, we hope that the observation contains more truth than error provided that the signal is stronger than the perturbation. It turns out true if the geometry of $\mathbb{R}^n$ is the Eucleadian one. What if we consider other geometry? If $\|\cdot\|$ is induced by an inner product, we still have $x^T u \ge x^T v \Longleftrightarrow  \|u\|\ge \|v\|$ , which can be proved by a similar argument. However, the same implication does not hold if $\|\cdot\|$ is the $1$ -norm. Consider the following vectors in $\mathbb{R}^2$ : $$
u=(1,1)^T \quad \text{ and } \quad v = (-1-\epsilon, -1+2\epsilon)^T,
$$ where $\epsilon\in(0,1/2)$ . Then $u+v=(-\epsilon, 2\epsilon)^T$ , and hence $x = (-1, 1)^T$ . It is easy to check that $\|u\|=2>2-\epsilon = \|v\|$ whereas $x^T u = 0 < 3\epsilon = x^T v$ . Question . Under the setting specified in the first paragraph , can we guarantee the equivalence between $x^Tu\ge x^T v$ and $\|u\| \ge \|v\|$ for a class of norms more general than those induced by inner products? An inference that may be wrong : by continuity, the abovementioned example for the $1$ -norm may also work for $p$ -norm with $p>1$ but close to $1$ . Assuming this is true , the desired implication is not ensured if $\|\cdot\|$ is uniformly convex. Then maybe inner-product-induced norms are the only possible ones to ensure it. It would not be extraordinary if this turns out true because dot product and inner products are nothing but representations of linear functionals on $\mathbb{R}^n$ and they are naturally connected. A rigorous proof would be very intriguing. This is inspired by interactions with @supinf in the comments. Even though it is not the primary objective, an extension to more general spaces will also be interesting --- doing so for inner-product spaces should not be difficult. Any comments or criticism will be appreciated. Thanks.","['convex-optimization', 'inner-products', 'nonlinear-optimization', 'functional-analysis', 'optimization']"
3852291,"What is $\tan \alpha$, if $(a+2)\sin\alpha +(2a - 1)\cos\alpha =2a + 1$?","I tried the following: $$\begin{aligned}a\sin\alpha +2\sin\alpha + 2a\cos\alpha - \cos\alpha &= 2a+1\\
a(\sin\alpha +2\cos\alpha)+(2\sin\alpha-\cos\alpha)&=2a+1\end{aligned}$$ Therefore, $$\sin\alpha +2 \cos\alpha=2$$ $$2\sin\alpha - \cos\alpha=1$$ From these two equations, we get $$\sin\alpha=\frac{4}{5},\cos\alpha=\frac{3}{5}$$ Therefore, $$\tan\alpha = \frac{\sin\alpha} {\cos\alpha} = \frac{4} {3}$$ Is this a correct method to solve the question? Since $a$ is a constant, it does not seem necessary to me that its coefficients on the two sides of the equation be equal. Should I find $\sin\alpha$ and $\cos\alpha$ using some other method? Are there specific cases where this method of equating the coefficients will break?",['trigonometry']
3852293,Continuous game payoff,"I'm new in game theory. The lecturer said that is something wrong with a sentence that is highlighted in yellow, but I can't spot any inaccuracy in it. Please share your opinion, what is wrong in that sentence.","['partial-derivative', 'game-theory', 'multivariable-calculus']"
3852295,"Need help for my homework, wordy math problem","Given a manufacturing line of 4 phases with: Phase 1 (5s) -> Phase 2 (10s) -> Phase 3 (8s) -> Phase 4 (7s) -> finished In each phase, there can only be 1 product under working ( maximum capacity of 1). A product must follow this process. Assuming there is no failure in the manufacturing line, calculate the total number of output after time T= 100s and T= n (second) My attempt: i tried to calculate the interval after the first product is finished which is 7 second. It seems wrong to me and i dont know whether there is better approach","['word-problem', 'algebra-precalculus']"
3852322,Binomial theorem: divisibility by $n^2$,"Consider $X = (a+b)^n-a^n-b^n$ where $a$ and $b$ and $a+b$ are not divisible by $n$ , $n>=3$ is positive and prime, $a \ne b$ and both are greater than 1. By the binomial expansion we know $X$ is divisible by $n$ , but how can we study whether $X$ is divisible by $n^2$ or other powers of $n$ ? Specifically I am interested to prove $X$ is NOT divisible by $n^2$ , but I am not sure how to prove or disprove it.","['number-theory', 'prime-factorization', 'prime-numbers']"
3852359,very basic question about definition of constant group scheme,"Let $M$ be a finite set, and give it the discrete topology. Let $R$ be a commutative ring with unity. We have a ring $\mathbb{Z}^M = \prod_{m \in M}\mathbb{Z}$ . Why is the set of ring homomorphisms $$\text{Hom}(\mathbb{Z}^M, R)$$ in bijection with the set $$\text{Hom}_{\text{continuous}}(\text{Spec } R, M)$$ of continous maps $\text{Spec }R \to M$ ? Since $M$ has the discrete topology, this is the same as locally constant maps. Also, I believe ${\text{Hom}(\mathbb{Z}^M, R) = R^M}$ . (Edit: no, this claim is not true). The motivation for my question comes from the constant group scheme $\mathbb{M}$ (over $\mathbb{Z}$ ) associated to an abstract group $M$ . Then as a scheme $\mathbb{M}=\coprod_{m \in M}\text{Spec }\mathbb{Z} = \text{Spec }\mathbb{Z}^M$ . Its functor of points (on say Zariski site of affine schemes), evaluated on $\text{Spec }R$ , on the one hand is the set of maps $\text{Spec }R  \to \mathbb{M}$ and on the other hand is (claimed/defined) in many places to be given by sending $\text{Spec } R$ to the set/group of locally constant functions $\text{Spec }R \to M$ . I see this claimed everywhere (e.g. https://stacks.math.columbia.edu/tag/03YW ) but never proved so it may be very obvious but I have never understood it. If you can provide a reference for where this is proved that is also fine.","['group-schemes', 'algebraic-geometry', 'schemes']"
3852371,Equation of Conics,"The general equation of the conic section is : $ax^2+2hxy+by^2+2gx+2fy+c=0$ ,
where $$\Delta=\begin{vmatrix}a&h&g\\h&b&f \\g&f &c\\
\end{vmatrix}$$ This equation can also be analysed to distinguish whether it is an equation of pair of straight lines, parabola, ellipse or hyperbola . If $\Delta=0$ and $h^2=ab$ , it represents pair of straight lines If $\Delta \neq 0$ and $h^2=ab$ , it represents a parabola If $\Delta \neq 0$ and $h^2<ab$ , it represents an ellipse If $\Delta\neq 0$ and $h^2>ab$ , it represents a hyperbola So my question is Can you represent a proof for why these are the relations for different shapes? I would prefer a more analytic/geometric approach","['analytic-geometry', 'conic-sections', 'geometry']"
3852386,Find x so that $\left(\frac{3}{2}+ \sum_{i=1}^{x} 3^{i}\right)^2$,"Find $x$ so that $$\left(\frac{3}{2}+ \sum_{i=1}^{x} 3^{i}\right)^2 = \frac{3^{22}}{4}$$ I've tried with simpler values for $x$ such as $0, 1$ and $2$ . But I can't seem to find any pattern I can take advantage of. How do I solve it? Where would I learn how to solve things like these?","['number-theory', 'summation', 'discrete-mathematics']"
3852406,Determining Injectivity by Calculation of the First Derivative,"Firstly, I was taught that the injectivity of a function can be determined by calculating its derivative- that if $f'(x) > 0$ , it is injective. Can you give me the idea behind this method? Secondly, it appears that this method doesn't apply to all functions. For instance, consider $$f(x) = \tan x , \qquad f'(x) = \sec^2 x > 0$$ But $\tan x$ is not injective! So, it would be great if you could also specify the constraints for the use of this method.","['calculus', 'functions', 'derivatives']"
3852454,$f(x)\to A(x\to\infty)$ and $f''(x)+\lambda f'(x)\leq M.$How to prove $f'(x)\to 0(x\to\infty)$,"$f(x)\in C^{2}(0,\infty)$ , $f(x)\to A(x\to\infty)$ and $f''(x)+\lambda f'(x)\leq M.$ How to prove $f'(x)\to 0(x\to\infty)$ ? It's easy to see that without loss of generality we can assume $f(\infty)=0.$ So if we can prove $f'(x)$ is convergent at $\infty$ ,we can use $f(x+1)-f(x)=f'(t)\to 0$ to show $f'(\infty)=0.$ And I know another useful lemma.If $f(\infty)=A$ ,and $f''(x)$ is bounded then $f'(\infty)=0.$ But I found these lemmas don't work here.So how to solve this problem?Does it have any background?Any help will be thanked.","['derivatives', 'real-analysis']"
3852455,Nested quantifiers in an implication and its contrapositive,"I am working through a problem on implications, and I have confused myself.  I wish to state the contrapositive of: $$\forall n\in \mathbb{Z}, \exists k \in \mathbb{N}, \ n \mid 12k+5 \wedge n \mid 18k+1 \implies n=17$$ My issue is with the nested quantifiers.  Does the existential quantifier belong to the hypothesis, and therefore the contrapositive is: $$\forall n\in \mathbb{Z}, n \ne 17 \implies \forall k \in \mathbb{N}, n \nmid 12k+5 \vee n \nmid 18k+1$$ Alternatively, if the existential quantifier does not belong to the hypothesis, then the contrapositive should be: $$\forall n\in \mathbb{Z}, \exists k\in \mathbb{N}, \ n \ne 17 \implies n \nmid 12k+5 \wedge n \nmid 18k+1$$ These are two VERY different statements, and both might be the wrong one.  What is the right answer here, and why?","['predicate-logic', 'first-order-logic', 'quantifiers', 'logic', 'discrete-mathematics']"
3852464,Comparing $(2+\frac{1}{2})(3+\frac{1}{3})(4+\frac{1}{4})(5+\frac{1}{5})$ with $(2+\frac{1}{5})(3+\frac{1}{4})(4+\frac{1}{3})(5+\frac{1}{2})$,"This question appeared in one of the national exams (MCQs) in Saudi Arabia. In this exam; Using calculators is not allowed, The student have $72$ seconds on average to answer one question. PROBLEM: Compare $a=(2+\frac{1}{2})(3+\frac{1}{3})(4+\frac{1}{4})(5+\frac{1}{5})$ with $b=(2+\frac{1}{5})(3+\frac{1}{4})(4+\frac{1}{3})(5+\frac{1}{2})$ . CHOICES: A) $a>b$ B) $a<b$ C) $a=b$ D) Given information is not enough Using algebra to evaluate each expression is easy, and the correct choice is $A$ , but that will take a long time. Any suggestion to solve this problem in a short time? THANKS.","['fractions', 'algebra-precalculus']"
3852572,"Are we allowed to define a symmetric (1,1) tensor in the following way?","I've recently stumbled accross the following task: ""is it possible to define a symmetric and antisymmetric (1,1) tensor?"". This is in context of relativity, so we have a metric $g$ at our disposal. Moreover, anything I say here should be applicable beyond the context of relativity, on any manifold equipped with a metric. And we recall how (anti)symmetricity of a (0,2) tensor $M$ is defined. Two vectors $V$ and $W$ are considered and the following is demanded $$
M (V, W) = \pm M (W, V)
$$ (plus for symmetric and minus for antisymmetric) or, in indices (by plugging in $\partial_\mu$ and $\partial_\nu$ ) $$
M_{\mu\nu} = \pm M_{\nu\mu}
$$ Or similarly for tensor (2,0) we'd get $M^{\mu\nu} = \pm M^{\nu\mu}$ (in that case we are plugging in two one-forms) Now I cannot do that with a tensor (1,1) because I cannot simply flip the arguments, but I can do the ""next best thing"" (hence, we start thinking about how we can define an analogous property for a mixed tensor). First, we take a covector $\tilde{A}$ and a vector $V$ and since we have a metric at our disposal, we can map the covector $\tilde{A}$ to a vector $A$ as $A = g^{-1} (\tilde{A}, \, \cdot \,)$ and map vector $V$ to covector as $\tilde{V} = g (V, \, \cdot \,)$ , or in components $$
\begin{aligned}
\tilde{A} &= A_\mu \mathrm{d} x^\mu \\
V &= V^\mu \partial_\mu
\end{aligned}
\;\,
\begin{aligned}
&\to \\
&\to
\end{aligned}
\quad
\begin{aligned}
A &= A^\mu \partial_\mu \\
\tilde{V} &= V_\mu \mathrm{d} x^\mu
\end{aligned}
$$ Now we can do the ""next best thing"" for a (1,1) mixed tensor and demand, that an ""(anti)symmetric (1,1) tensor to satisfy"" $$
M (\tilde{A} ; V) = \pm M (\tilde{V} ; A)
$$ (again, plus for symmetric, minus for antisymmetric) Which translates to $$
M^\mu_{\;\; \nu} = \pm g^{\mu \rho} g_{\nu \sigma} M^\sigma_{\;\; \rho}
$$ and if we unwrap the $g$ 's, $$
M^\mu_{\;\; \nu} = \pm M_\nu^{\:\,\mu}
$$ which would be in a complete agreement with how a (2, 0) tensor $M^{\mu \nu}$ was symmetric, because if we take a tensor satisfying $M^{\mu \nu} = M^{\nu \mu}$ and simply drop an index by using the metric $$
g_{\sigma \nu} M^{\mu \sigma} = g_{\sigma \nu} M^{\sigma \mu}
$$ we get the same answer $$
M^\mu_{\;\; \nu} = M_\nu^{\:\,\mu}
$$ You can of course verify certain properties and little identities, that are analogous to (2,0) tensors. For example, for a general (1,1) tensor, the (anti)symmetrization would correspond to $$
(M^{(S)})^\mu_{\;\; \nu} = \frac{1}{2} \left( M^\mu_{\;\; \nu} + M_\nu^{\:\,\mu} \right) \quad \quad (M^{(A)})^\mu_{\;\; \nu} = \frac{1}{2} \left( M^\mu_{\;\; \nu} - M_\nu^{\:\,\mu} \right)
$$ Contracting a symmetric and antisymmetric tensor gives zero $$
S^\mu_{\;\; \nu} A^\nu_{\;\,\mu} = 0
$$ Contracting a general and (anti)symmetric tensor only ""reacts"" with the (anti)symmetric part of the first tensor $$
M^\mu_{\;\; \nu} S^\nu_{\;\; \mu} = (M^{(S)})^\mu_{\;\; \nu} S^\nu_{\;\; \mu} \quad \quad M^\mu_{\;\; \nu} A^\nu_{\;\; \mu} = (M^{(A)})^\mu_{\;\; \nu} A^\nu_{\;\; \mu}
$$ The ""official"" answer (book, professor, grader, ...) is that this is not possible and when I present this line of thinking I get a vague ""you cannot do that"" or answers that stray from my definitions (like ""you cannot swap indices like that"", which I would understand if I did $M^\mu_{\;\;\nu} + M^\nu_{\;\; \mu}$ however that's not what I'm doing here), or even answers that indicate that the person might not understand differential geometry, like "" $M^\mu_{\;\; \nu}$ is identically equal to $M_\nu^{\;\mu}$ , so what you call antisymmetric part, is always zero"". Which part of my reasoning is icky? The professor pointed out to me that in $M (\tilde{A} ; V) = M (\tilde{V} ; A)$ the objects fed to the tensor on the left-hand side are different from the objects fed to the tensor on the right-hand side, but that's part of my definition (*) . Is there something logically wrong with this? I'd like someone versed in this topic to give me clear reasons why this is a no-no approach. (*) And if this is the only objection, I find it rather weak. If this was the way people are thinking in mathematics, they would never define more general concepts stemming from simpler concepts, because they would never get past this kind of sentiment. A negative number? Have you ever seen minus two cows? A root of two? I've never had a root of two goats, what kind of nonsense is that? A factorial of a complex number? Come on, you cannot have $\pi + 2i$ numbers of people to divide $3-i$ marbles between, what will you come up with next? A fractional derivative? That doesn't make any sense! How do I write it as a d $y$ /d $x$ ? I thought that in mathematics, we look for a useful way to generalize concepts that are already known to us, even if it means to go a bit beyond what seems ""common sense"" at the first sight. Edit: I have two more ways to think about this now that I recall more details from the course of differential geometry. First way, the metric provides a canonical isomorphism, so if we can define a concept of a symmetric (2,0) tensor, we can also define this concept on (1,1) tensors by mapping the corresponding (2,0) tensor to a (1,1) tensor by the musical isomorphism. Let $M$ be an (anti)symmetric tensor of rank (2,0), then (in indices), the corresponding (1,1) tensor is $$
M^\mu_{\;\;\nu} = g_{\nu \sigma} M^{\mu \sigma}
$$ Second way, let $\left\langle \;, \; \right\rangle$ denote the inner product. Moreover, let's observe, that any tensor of the type (1,1) that has been fed a vector now provides a natural mapping from the cotangent space to $\mathbb{R}$ , $$
M : T^*M \otimes T M \to \mathbb{R} \quad \quad \implies M (\; ; V) : T^*M \to \mathbb{R}
$$ therefore, such object is a vector. We can then combine this with the inner product and define (anti)symmetric tensor of the rank (1,1) as follows $$
\left\langle M ( \;\cdot\; ; V), W \right\rangle = \pm \left\langle V, M (\;\cdot\; ; W) \right\rangle
$$ This way, the objects $V$ and $W$ enter the equation in a very symmetric way, so the complaint from before does not hold. This is, by the way, analogous to how we would conclude that a Laplacian is a ""symmetric (1,1) tensor""/operator. First, a Laplacian acts on a function and spits out another function, so if we somehow understand functions to be ""vectors"" of a certain space, then Laplacian maps every vector to another vector, therefore, is a (1,1) tensor. The inner product can be defined as an integral, i.e. $$
\left\langle \varphi, \psi \right\rangle = \int \mathrm{d}^3 x \, \varphi (x) \, \psi (x)
$$ and under certain conditions (we consider certain class of functions), the following is true for any two ""vectors"" $\varphi$ , $\psi$ and the operator $\Delta$ $$
\left\langle \Delta \varphi, \psi \right\rangle = \left\langle \varphi, \Delta \psi \right\rangle
$$","['tensors', 'differential-geometry']"
3852601,"Forming 2 teams of five from 11 people, but first person of each team has the captain role.","(a) A coach wants to form two (2) teams of five (5) from the eleven players on the team for a scrimmage game (i.e., just a small practice game where player positions are not important). The eleventh player will act as the referee. How many ways can the coach divide the team into two teams of five players? (b) A coach wants to form two (2) teams of five (5) from the eleven players on the team for a scrimmage game, with the eleventh player again acting as the referee, but with a small change. The first person chosen for a team of five will be the captain of the team and will have extra responsibilities. For the rest of the players, their roles and positions are not important. How many ways can the coach divide the team into two teams of five players with one captain for each team? For a), I have 11!/(5!5!2!) = 1386 ways. Dividing by 5! twice because there are two teams of five where the internal order doesn't matter, and then 2! to ignore the order of the two teams. Finally, I ignored the last person (referee). For b), I simply multiplied my answer in a) by 25: 1386 x 5 x 5 = 34650. Because there could be 5 permutations of the captain role in each of the two teams. I'd appreciate it if someone can tell me if my reasoning/answers are correct, thanks.","['permutations', 'combinatorics', 'probability']"
3852732,Voting intention poll,"In a voting intention polling, we use a a random sample of 400 people and the two candidates, party A and party B get 32% and 28% respectively. Check whether party A will win against party B with probability 99%. I assume that, by ""probability"" they mean ""confidence"". So the question is, if with the given sample size, we can secure a 99% confidence interval that A will win. All I managed to find is critical value *z for 99% confidence, $z = 2.58$ . Then we calculate $β = \frac {32}{100}$ . How do we continue? Thank you very much.",['statistics']
3852765,Does the complex exponential function $\exp(z)$ have an axiomatic definition?,"It is known that the real exponential function $e^{(\cdot)}:\mathbb{R}\rightarrow\mathbb{R}$ can be characterized as the unique real function satisfying these three properties: $e^{1}=e$ , where $e=\lim\limits_{n\to\infty}\left(1+\frac{1}{n}\right)^n$ . $e^{x+y}=e^{x}e^{y}$ for all $x,y\in\mathbb{R}$ For some real number $x_0$ , $e^{(\cdot)}$ is continuous at $x_0$ , that is $\lim\limits_{x\to x_0}e^{x}=e^{x_0}$ The complex exponential function $e^{(\cdot)}:\mathbb{C}\rightarrow\mathbb{C}$ is usually defined by $e^{z}=\sum_{n=0}^{\infty}\frac{z^n}{n!}$ . I'm wondering if it can be characterized using axioms analogous to the ones above. Context : Lately, I've been wondering whether the identity $e^{i\pi}+1=0$ is as ""beautiful"" or ""remarkable"" as people often make it out to be. To me, it seems like it is not a fascinating result so much as a consequence of the definition $$e^{z}=\sum_{n=0}^{\infty}\frac{z^n}{n!}$$ and the series expansions of $\sin$ and $\cos$ . Nonetheless, I often hear people saying things like "" $e^{i\pi}+1=0$ relates the constants $e$ , $\pi$ , $i$ , $1$ and $0$ "", leading me to wonder if there's a deeper connection I have not discovered yet. Reflecting on these thoughts, I realized that $e^z$ having an axiomatic characterization lends credence to the idea that $e$ is special in relation to the identity $e^{i\pi}+1=0$ , precisely because it is the only number satisfying <insert property 1 analogue here>. I apologize if my context is unclear. If you need clarification or have useful edits, please feel free to leave a comment or edit my post.","['complex-analysis', 'exponential-function', 'eulers-number-e']"
3852797,The optimal value is continuous in the parameter,"Let $f:(0,\infty) \to [0,\infty)$ be a continuous function satisfying $f(1)=0$ , which is strictly increasing on $[1,\infty)$ , and strictly decreasing on $(0,1]$ . Define $$
F(s)=\min_{xy=s,x,y>0} f(x)+ f(y), \, \,  \, \, \text{for } \, \, s \in (0,\infty).
$$ Claim: $F$ is continuous. I am looking for a reference for such a claim. (not that claim exactly, but perhaps a slightly more general claim which implies it , or is similar to it). I think that it follows from a result in the book ""Perturbation Analysis of Optimization Problems"" by Bonnans and Shapiro, but that book phrases things in much more abstract setting than I find necessary. BTW, here is my proof: Suppose that $s \le 1$ .
Define $g(x,y)=f(x)+f(y)$ . Suppose that $s_n \to s$ . Write $F(s_n)=g(x_n,y_n)$ for some $x_n,y_n \in [s_n,1]^2, x_ny_n=s_n$ . By compactness we may assume that $x_{n_{k}} \to x, y_{n_{k}} \to y$ . Thus $$
F(s) \le g(x,y)=\lim_{k \to \infty} g(x_{n_k},x_{y_k})=\lim_{k \to \infty}F(s_{n_k}) \le \liminf F(s_n).
$$ On the other hand, take $(x,y) \in (0,\infty)^2$ such that $xy=s$ and $F(s)=g(x,y)$ . Now take $x_n,y_n$ such that $x_ny_n=s_n$ , and $(x_n,y_n) \to (x,y)$ . Then $$
F(s_n) \le g(x_n,y_n) \Rightarrow \limsup F(s_n) \le \lim_{n \to \infty}g(x_n,y_n)=g(x,y)=F(s).
$$","['reference-request', 'real-analysis', 'multivariable-calculus', 'calculus', 'optimization']"
3852818,Convergence of infinitely divisible distributions,"I want to show $$\lim\limits_{t\to 0} \sup\limits_{n}\int (1-\cos tx)dM_n(x) = 0 \ \ \ \ \ \ \ \text{ implies } \ \ \ \ \ \ \ \sup\limits_{n}M_n\{ x : \varepsilon \leq |x| \} < \infty.$$ $M_n$ are positive measures and I need to show the above statement holds for any $\varepsilon > 0$ . I'm having a surprisingly hard time doing so. I know that $1-\cos tx \geq 0$ , and decreasing $t$ increases the period. I thought about choosing two small $t_1, t_2$ with $\frac{t_1}{t_2}$ irrational so that the function $$f(x) = \max\{1-\cos t_1x, 1 - \cos t_2x\} > 0
$$ on $\{x: \varepsilon \leq |x|\}$ . Then perhaps I could try using some estimate like $$M_n\{x: \varepsilon \leq |x|\} \cdot\inf_{\varepsilon \leq |x|} f(x) \leq \int (1-\cos t_1x)dM_n(x) + \int (1-\cos t_2x)dM_n(x).
$$ But unfortunately, the infimum occuring on the left of the inequality could be $0$ . So I don't know what to do. Perhaps I'm missing something easy. Please help me, I'm struggling :| Edit: If it helps, this is at the top of page 88 of Varadhan's notes . I should probably have added: The $M_n$ are positive measures on $\mathbb{R}$ , each assumed to satisfy $$\int \frac{x^2}{1+x^2} dM_n(x) < \infty.
$$ The book calls them Levy measures.","['levy-processes', 'measure-theory', 'probability-limit-theorems', 'probability-distributions', 'probability-theory']"
3852863,Shared eigen vectors of Hessian,"Note: this question is still unanswered! (I will provide an answer if I come to one on my own). Background Say we have a continuous twice differentiable function $f:\mathbb{R}^n \rightarrow \mathbb{R}$ . We know that this function has a symmetric, positive definite Hessian matrix, $\nabla^2 f$ . This Hessian can be decomposed as, $$\nabla^2 f = R \Lambda R^T, $$ where $\Lambda: \mathbb{R}^n \rightarrow \mathbb{R}^{n \times n}$ is a diagonal matrix of eigen valuaes and $R:\mathbb{R}^n \rightarrow \mathbb{R}^{n \times n}$ is a matrix of eigen vectors. Note that here $\Lambda$ and $R$ are themselves functions of the arguments to $f$ . Question Given the function $f$ has a Hessian with decomposition $R \Lambda R^T$ , is there a way to compute all functions $g:\mathbb{R}^n \rightarrow \mathbb{R}$ such that, $$\nabla^2 g = R D R^T, $$ where $D$ is a diagonal matrix? Updates: John provided a method for computing $g$ so that its Hessian has the correct decomposition at a chosen point. Unfortunately this does not hold over all of $\mathbb{R}^n$ .","['multivariable-calculus', 'linear-algebra', 'hessian-matrix', 'eigenvalues-eigenvectors']"
3852893,Koopman composition operator theoretical approach to Ergodic Dynamical Systems,"Given a dynamical system $T:X\to X$ , the Koopman  operator $U$ is defined on the space of (complex-valued) functions on $X$ . So for $f:X\to\mathbb C$ , the action of the composition on the function $f$ reads: $U(f):=f\circ T$ .
Then, since $U$ is then a linear operator in a vector space, one is interested in the eigenfunctions especially those with eigenvalue 1. But for ergodic transformations $T$ , it is a fact that the only $T$ -invariant function (i.e. $f\circ T=f$ ) is an a.e. constant  (almost everywhere, up to a zero-measure subset). So what is the application of this transition to ""observable space"" for ergodic systems, if there is any? Actually I think, ergodicity is a property mostly assumed for real physical systems, and here the opposite is required? References: Koopman Operator Spectrum and Data Analysis , Igor Mezic: ""Koopman Operator Theory for Dynamical Systems, Control and Data Analytics"" , Introduction to Koopman operator theory ofdynamical systems , Data-driven spectral decomposition and forecasting of ergodic dynamical systems","['operator-theory', 'ergodic-theory', 'data-analysis', 'functional-analysis', 'dynamical-systems']"
3852952,Lines on grassmannian,"Given a projective space $\mathbb{P}^n(\mathbb{C})$ , I can consider the Grasmannian of lines $G(2,n+1)$ , which has a structure  of projective variety inside $\mathbb{P}^N$ , where $N=\binom{2+n+1}{2}-1$ thanks to the Pl $\ddot{u}$ cker embedding. It has been told me -literally, I have no references, it was a speech- that while a  point $p\in G(2,n+1)$ obviously represents a line in $\mathbb{P}^n$ by definition, a line inside $G(2,n+1)$ -viewd as a projective variety in $\mathbb{P}^N$ -corresponds to a pencil of planes. Unfortunately I'm still trying to properly understand why this work -because unfortunately it is not crystal for me -, but I was also wondering if it makes sense -that is, if there is a geometrical meaning-, also for quadrics contained in $G(2,n+1)$ . Therefore I'd like to have an idea of what happens for quadrics, and moreover even a little help for lines in $G(2,n+1)$ . Since this question comes primarily from my curiosity, I apologize for the vagueness: any comment, reference or answer would be much appreciate!","['grassmannian', 'algebraic-geometry', 'projective-geometry']"
3853086,"Linear operator T on $ \mathbb{R}^5$. $T^4 \neq 0$, but $T^5 = 0$. What's the rank of $T^2$?","The problem: Linear operator T  on $ \mathbb{R}^5$ . $T^4 \neq 0$ , but $T^5 = 0$ . What's the rank of $T^2$ ? I am learning linear algebra and preparing for a test.
For this problem, I am able to show that $range(T^4) \subseteq nullspace(T)$ since there exists $T^4x = y$ , and $T^5x = Ty = 0$ . $y$ is arbitrary and can represent the range of $T^4$ . Then I am stuck. I understand this is a duplicated question but I don't know Jordan canonical form or eigenspaces yet. So I couldn't understand the answer there. Could you help with it using only linear transformation theorems?","['linear-algebra', 'linear-transformations']"
3853100,Isomorphism of cuspidal cubic and some geometric meaning,"Consider the integral domain $R:=\mathbb{C}[x,y]/(y^2-x^3)$ . Over its field of fraction let $t:=y/x\in R_{frac}$ . I want to show $\mathbb{C}[t] \cong R[t]$ . How can I show this? and what is geometric meaning over this? I think they are isomorphic because $\mathbb{C}[t]$ is a polynomial over the complex line which maps to a function defined on $V(y^2-x^3)\subset \mathbb{C}^2$ . Is this right interpretation? Thank you.","['algebraic-geometry', 'commutative-algebra']"
3853120,Is $TM \cong M \times \mathbb{R}^n$ as sets?,"To be clear, I know these sets are not diffeomorphic or even homeomorphic in general. However, I've been told that there doesn't even exist a bijection between these sets. But suppose $M$ is an $n$ -dimensional manifold and let $\{\partial_1|_p, \ldots, \partial_n|_p\}$ be the basis of $T_p M$ with respect to some chart containing $p \in M$ . If $v_p \in T_p M$ we have $v_p = v_p^i \partial_i|_p$ for unique real numbers $v_p^i$ . Define the function $\lambda: TM \to M \times \mathbb{R}^n$ by $\lambda(p, v_p)=(p, v_p^1, \ldots, v_p^n)$ . Surely this is a well-defined bijection?",['differential-geometry']
3853154,On the definition of the directional derivative,"In the multivariable calculus course I took the directional derivative of a multivariable function $f(x,y)$ at $(a,b)$ in the direction of the vector $\vec{s}$ was defined as the following: $$f_s(a,b) = \vec{\nabla f} \cdot \vec{u_s}$$ where $\vec{u_s}$ is the unit vector in the same direction of $\vec{s}$ . Now I have come across the following definition: $$\frac{d}{d\alpha} f(\vec{v} + \alpha\vec{s})$$ evaluated at $\alpha = 0$ $(\vec{v}$ is supposed to be the vector at which the derivative is evaluated). I am struggling to see why the two definition are equal.","['multivariable-calculus', 'derivatives']"
3853173,"If for two sets $S, R$ there exists a bijection between them and they are totally ordered, then there exists an order isomorphism between them?",I am trying to prove that some two sets with their respective total orders are similar (there exists an order isomorphism between them) but I haven't made a lot of progress and I have only proved that there exist a bijection between them. Is there any theorem that could guarantee that there exists said order isomorphism?,"['elementary-set-theory', 'order-theory']"
3853203,"Bias and MSE of $\hat{\theta} = \min(X_1, \ldots, X_n)$","Let $X_1, ... X_n$ iid with pdf given by $$p_{\theta, r} = r \theta^r x^{- (r+1)} \mathbb{1}\{x \geq \theta\}$$ for $\theta > 0$ , and some $r > 2$ that is known. Then $\hat{\theta} = \min(X_1, \ldots, X_n) = X_{(1)}$ . I want to determine the bias and MSE of $\hat{\theta}$ , so I need the pdf of $\hat{\theta}$ . If my calculations are correct, the pdf of $\hat{\theta}$ is given by: $$f_{X_{(1)}} = n(r+1)r^n \theta^{rn} x^{-n(r+1) - 1} \mathbb{1}\{x \geq \theta\}.$$ Wondering if this pdf is correct, and how one would calculate the bias and MSE using this variance? I know that the bias is given by $E[\hat{\theta}] - \theta$ , but I end up with a complicated expression, so I believe I am doing something wrong.","['mean-square-error', 'statistics', 'probability-distributions', 'probability']"
3853211,"Wronskian of functions $\sin(nx), n=1,2,...,k$.","Is it true that the Wronskian of the functions $\sin(nx), n=1,...,k$ is equal to $c(\sin(x))^p$ where $c$ is a constant and $p=1+2+...+k=k(k+1)/2$ ? That is true for $k=1,2,3,4,5$ . If it is true, how to find the constant $c=c(k)$ ?","['trigonometry', 'linear-algebra', 'wronskian', 'sequences-and-series']"
3853226,Definition of integration of a differential form (John Lee),"In Chapter 16 of John Lee's book Introduction to Smooth Manifolds , he defines integrals over subspaces of $\mathbb R^n$ as follows: If $D\subseteq\mathbb R^n$ is a bounded subset whose boundary has measure zero, and if $\omega$ is a continuous $n$ -form on $\overline D$ , then write $\omega=fdx^1\wedge\dots\wedge dx^n$ for some continuous function $f:\overline D\to\mathbb R$ . Then the integral of $\omega$ over $D$ is $$\int_D\omega=\int_DfdV.$$ My (possibly dumb) question is: Why does $\omega$ have to be defined on $\overline D$ ? Shouldn't it be enough for $\omega$ to be a continuous $n$ -form defined on $D$ ?","['differential-forms', 'definition', 'smooth-manifolds', 'differential-geometry']"
3853254,Differential equation of a central orbit,"I am solving dynamics(central orbits) and have a doubt in differential equation of a particle moving in central orbit. Suppose a particle moves in central orbit under influence of a central force.
Let a particle move in a plane with an acceleration P , always directed towards a fixed point O which is also the centre of force.Let (r,θ) be the polar coordinates of the position P of the moving particle at any instant t. Radial acceleration of particle is given by :- d 2 r/dt 2 - r (dθ/dt) 2 = -P Transverse acceleration of particle is given by :- $\frac{1}{r}$$\frac{d}{dt}$ (r 2 $\frac{dθ}{dt}$ ) I know about radial and transverse accelerations in circular motion. Central orbit motion seems similar . But still not able to relate how the author presented these equations. Any help is appreciated !!","['nonlinear-dynamics', 'ordinary-differential-equations']"
3853311,Is there a known example of a finitely presented group with subexponential growth that isn't polynomial?,The Grigorchuk group is finitely generated and has subexponential non-polynomial growth but I'm not aware of a finite presentation. Does a finite presentation imply that the group is polynomial or exponential as well?,"['geometric-group-theory', 'group-presentation', 'group-theory', 'subgroup-growth']"
3853319,Married couples at a table,"Six married couples are to be seated at a round table so that men and women alternate. In
how many ways can the seating be arranged if nobody wants to sit next to his or her spouse?
Two seatings are considered the same if every person’s answer to the question “Who is on your
immediate left?” would be the same for both seatings, and every person’s answer to “Who is
on your immediate right?” would be the same, as well.","['combinations', 'binomial-coefficients', 'combinatorics']"
3853327,How to evaluate $\int _0^{\pi }x\sin \left(x\right)\operatorname{Li}_2\left(\cos \left(2x\right)\right)\:dx$.,"How can i evaluate $$\int _0^{\pi }x\sin \left(x\right)\operatorname{Li}_2\left(\cos \left(2x\right)\right)\:dx$$ $$=\frac{\pi ^3}{6}-\frac{\pi ^3}{6\sqrt{2}}-4\pi +6\pi \ln \left(2\right)-\frac{\pi }{2\sqrt{2}}\ln ^2\left(2\sqrt{2}+3\right)-\sqrt{2}\pi \operatorname{Li}_2\left(2\sqrt{2}-3\right)$$ This is what I've done thus far. \begin{align*}
&\int _0^{\pi }x\sin \left(x\right)\operatorname{Li}_2\left(\cos \left(2x\right)\right)\:dx\\
&=\frac{\pi }{2}\int _0^{\pi }\sin \left(x\right)\operatorname{Li}_2\left(\cos \left(2x\right)\right)\:dx\\[2mm]
&=\frac{\pi }{4}\int _0^{\pi }\sin \left(\frac{x}{2}\right)\operatorname{Li}_2\left(\cos \left(x\right)\right)\:dx+\frac{\pi }{4}\int _{\pi }^{2\pi }\sin \left(\frac{x}{2}\right)\operatorname{Li}_2\left(\cos \left(x\right)\right)\:dx\\[2mm]
&=\frac{\pi }{2}\int _0^{\pi }\sqrt{\frac{1+\cos \left(x\right)}{2}}\operatorname{Li}_2\left(-\cos \left(x\right)\right)\:dx=\pi \int _0^{\infty }\frac{\operatorname{Li}_2\left(\frac{t^2-1}{1+t^2}\right)}{\left(1+t^2\right)\sqrt{1+t^2}}\:dt\\[2mm]
&=\frac{\pi }{2}\int _1^{\infty }\frac{\operatorname{Li}_2\left(\frac{x-2}{x}\right)}{x\sqrt{x}\sqrt{x-1}}\:dx=\frac{\pi }{2}\int _0^1\frac{\operatorname{Li}_2\left(1-2x\right)}{\sqrt{1-x}}\:dx\\[2mm]
&=\pi \zeta \left(2\right)+\frac{\pi }{\sqrt{2}}\int _{-1}^1\frac{\sqrt{1+x}\ln \left(1-x\right)}{x}\:dx
\end{align*} But I'm not sure how to proceed with either that polylogarithmic integral on the $5$ th line nor the last one. I'd appreciate any hints or ideas, thanks.","['integration', 'improper-integrals', 'definite-integrals', 'real-analysis', 'polylogarithm']"
3853351,Do ellipsoids cast ellipsoidal shadows?,"Given an n-dimensional ellipsoid in $\mathbb{R}^n$ , is any orthogonal projection of it to a subspace also an ellipsoid? Here, an ellipsoid is defined as $$\Delta_{A, c}=\{x\in \Bbb R^n\,:\, x^TAx\le c\}$$ where $A$ is a symmetric positive definite n by n matrix, and $c > 0$ . I'm just thinking about this because it gives a nice visual way to think about least-norm regression. I note that SVD proves immediately that any linear image (not just an orthogonal projection) of an ellipsoid is also an ellipsoid, however there might be a more geometrically clever proof when the linear map is an orthogonal projection.","['conic-sections', 'geometry']"
3853457,How to use triconfluent Heun's function?,"When solving an eigenvalue problem, I encounter the triconfluent Heun's function , $HeunT[-\lambda^2,0,-2\alpha,0,-b^2](x)$ , which is the solution of $$u''-(b^2x^2+2\alpha)u'+\lambda^2u=0.$$ I need a $u(x)$ not divergent at $\infty$ and this condition hopefully can give us the eigenvalue $\lambda$ . (This is just like Legendre function can be cut off to Legendre polynomial.) However, I can't find any clear info about this as far as I've searched. Maybe I missed something. The numerical eigensolution to the original problem is very well-behaved. So I presume there must be some condition like this to help find the eigenvalue.","['special-functions', 'ordinary-differential-equations', 'eigenvalues-eigenvectors']"
3853490,Betting game with numbers,"Someone is challenged to play the following game: there are 36 marbles in an urn. 21 of them are red, 9 are blue and 6 are green. Each red is worth 0 rupees, each blue 100 rupees and each green 1000 rupees. By betting 1000 rupees only once, you must pick as many marbles as you want (obviously without seeing its color), one by one (you draw each marble, look at its color and either stop or continue) and without replacement and you can stop at any time, and you earn the product of the values of the marbles you have picked so far. Assuming that the player plays optimally, what is the average percentage of the average percentage of profit for the game? My understanding is that we must calculate the probability (not) to draw a 0 in a series of consecutive draws from 1 to 36. For the 1st draw, the probability to get a 0 is $P_1 = \frac {21}{36}$ and the game ends, so with probability $P'_1 = \frac {15}{36}$ the player continues (""optimal"" playing refers to this, otherwise if he continues, he will get 0 anyway).
Up to now, the player has earned 100 rupees with probability $Pw'_1 = \frac {15*9}{36*15}$ and 1000 rupees with probability $Pww'_1 = \frac {15*6}{36*15}$ .
For the 2nd draw, the probability not to get a 0 is $P'_2 = \frac {14}{35}$ and so on.
The player will earn again the product of either 100 or 1000 of the first earning, with probability equal to the product of $P'_2$ by $\frac {8}{14}$ or $\frac {5}{14}$ . Can you please help me continue? It is getting really confusing from this point forward! Thank you very much!","['gambling', 'probability']"
3853544,A conic inside a hexagon,Can you prove or disprove the following claim: Construct a hexagon circumscribed around a conic section. Intersection points of its non-principal diagonals lie on a new conic section. GeoGebra applet that demonstrates this claim can be found here .,"['euclidean-geometry', 'projective-geometry', 'conic-sections', 'geometry', 'polygons']"
3853614,Why do we need topological spaces?,"Please correct me if I am wrong: We need the general notion of metric spaces in order to cover convergence in $\mathbb{R}^n$ and other spaces. But why do we need topological spaces? What is it we cannot do in metric spaces? I have read the answers at Motivation of generalizing the theory of metric spaces to the theory of topological spaces and want to emphasize this example I found in Preuss ""Foundations of Topology"": Sorry for the big image, but I want to be sure you know what I mean. So does this mean we cannot describe pointwise convergence in a metric space? Can you elaborate more on this specific example? I don't really see the conclusion. Another point is that Preuss explains that continuous convergence cannot be described in topological spaces (I am not sure if he is referring just to Hausdorff-spaces here).","['pointwise-convergence', 'general-topology', 'functional-analysis', 'metric-spaces']"
3853673,Difference equation corresponds to $y''=-y$,"Convert the differential equation $y''=-y$ into a difference equation using the leapfrog method. My reference says: $$
\begin{bmatrix}1&0\\\Delta t&1\end{bmatrix}\begin{bmatrix}Y_{n+1}\\Y_{n+1}^{'}\end{bmatrix}=\begin{bmatrix}1&\Delta t\\0&1\end{bmatrix}\begin{bmatrix}Y_{n}\\Y_{n}^{'}\end{bmatrix}\implies \begin{bmatrix}Y_{n+1}\\Y_{n+1}^{'}\end{bmatrix}=\begin{bmatrix}1&\Delta t\\-\Delta t&1-(\Delta t)^2\end{bmatrix}\begin{bmatrix}Y_{n}\\Y_{n}^{'}\end{bmatrix}\\
Y_{n+1}=Y_n+\Delta t.Y_n^{'}\implies Y_{n+1}-Y_n=\Delta t.Y_n^{'} \\
\Delta t.Y_{n+1}+Y_{n+1}^{'}=Y_n^{'} \implies Y_n^{'}-Y_{n+1}^{'}=\Delta t.Y_{n+1}
$$ Looks like the first equation is obtained by taking forward difference and later by backward. How do I justify such a choice ? My Attempt $$
Y_n^{'}=\frac{Y_{n+1}-Y_{n-1}}{2\Delta t}\\
Y_{n}^{''}=\frac{Y_{n+1}^{'}-Y_{n-1}^{'}}{2\Delta t}=\frac{\frac{Y_{n+2}-Y_{n}}{2\Delta t}-\frac{Y_{n}-Y_{n-2}}{2\Delta t}}{2\Delta t}=\frac{Y_{n+2}-2Y_{n}+Y_{n-2}}{(2\Delta t)^2}=\frac{Y_{n+1}-2Y_{n}+Y_{n-1}}{(\Delta t)^2}=-Y_{n}\\
\boxed{Y_{n+1}=Y_{n-1}+2\Delta t.Y_{n}^{'}\\
Y_{n+1}^{'}=Y_{n-1}^{'}-2\Delta t.Y_{n}}
$$ $$
Y_{n-1}=(2-(\Delta t)^2)Y_n-Y_{n+1}\implies Y_{n+1}=(2-(\Delta t)^2)Y_n-Y_{n+1}+2\Delta t.Y_{n}^{'}\\
\boxed{Y_{n+1}=\frac{(2-(\Delta t)^2)}{2}Y_n+\Delta t.Y_n^{'}}
$$ How do I approach this and obtain the matrix difference equation corresponds to $y''+y=0$ using midpoint differences ? Reference: Example 3-Difference Equations (Page 323) and Q.28 Problem Set 6.3 (Page 335), Chapter 6 : Eigenvalues and Eigenvectors
, Introduction to Linear Algebra, Gilbert Strang, Fifth Edition (2016)","['ordinary-differential-equations', 'recurrence-relations', 'linear-algebra', 'numerical-methods', 'finite-differences']"
3853723,Show that given five points in $\mathbb{Z^2}$ there are at least one pair of them whose middle point also lies in $\mathbb{Z^2}$,"While brushing up on my old discrete mathematics skills I stumbled upon this problem that I can't solve. In $\mathbb{R^2}$ the middle point of two coordinates is $(\frac{x_1+x_2}{2}, \frac{y_1+y_2}{2})$ . Show that given five points in $\mathbb{Z^2}$ (points with intetger coordinates) there are at least one pair of them whose middle point also lays in $\mathbb{Z^2}$ . Let us consider the corresponding question in $\mathbb{R^3}$ . How many points in $\mathbb{Z^3}$ would you need to be sure that at least one pair of them has a middle point in $\mathbb{Z^3}$ ? I am thinking that using the pigeonhole principle is appropriate, how I would use it is unclear to me though. $\mathbb{Z^2}$ "" />","['elementary-number-theory', 'pigeonhole-principle', 'discrete-mathematics']"
3853754,Construct the isomorphism from $\operatorname{Proj} A$ to $\operatorname{Proj} A'$,"In Gortz, Wedhorn book Algebraic Geometry - Remark 13.7 (p.371) They say, Let $A=\bigoplus_{d\ge0}A_d$ be a graded ring. We can ""thin out"" $A$ and ""change $A_0$ "" without changing the scheme $\operatorname{Proj} A$ . More precisely, fix integers $k,\delta\ge1$ and define a new graded ring $A'$ by $A_0'=\mathbb Z$ , $A'_d=0$ for $0<d<k$ and $ A'_d=A_{d\delta}$ for $d\ge k$ . $\mathfrak{p}\mapsto \mathfrak{p}\cap A'$ defines a bijection $\operatorname{Proj} A\to \operatorname{Proj} A'$ . For any homogeneous element $f\in A_+$ we find $f^{k\delta}\in A'$ . It is clear that $D_+(f)=D_+(f^{k\delta})$ and it is easy to see that $A_{(f)}=A'_{(f^{k\delta})}$ . Thus we have an isomorphism $$\operatorname{Proj} A\to\operatorname{Proj} A'$$ [Question and try] I wonder if the map $\mathfrak{p} \mapsto \mathfrak{p} \cap A'$ is actually $\mathfrak{p}= \bigoplus_d \mathfrak{p}_d \mapsto \bigoplus_d(\mathfrak{p}_{d\delta} \cap A_{d\delta}$ ). Then, in $d=0$ term, $\mathfrak{p}_{0} \cap A_0 = \mathfrak{p}_0 \cap \mathbb{Z}$ ? I think it is nonsense.
In fact, I don't understand how to intersect $\mathfrak{p}$ with $ A'$ For the simplicity take $k=2 , \delta=2$ . Then, maybe $$A= A_0 \;\oplus \;A_1\;\oplus \;A_2\;\oplus \;A_3\;\oplus \;A_4\;\oplus \;A_5 \;\oplus A_6\;\oplus \;\dots $$ $$A'=\mathbb{Z}\;\oplus \;\;0\;\;  \;\oplus A_4\;\oplus \;A_6\;\oplus \;A_8 \;\oplus \;A_{10} \;\oplus A_{12} \;\oplus \;\dots$$ In this situation, how can I construct graded homomorphism from $A$ to $A'$ ? That is, how can I have an isomorphism from $\operatorname{Proj} A\to\operatorname{Proj} A'$ . Thank you.","['algebraic-geometry', 'graded-rings']"
3853769,What is the sum of products of pairs of integers: $\sum_{0\le i<j\le n} ij$?,"It is well known that $S_1\equiv \sum_{k=0}^n k = \binom{n+1}{2}$ . How is this formula generalized for sums of products of pairs of integers smallest than $n$ ?
In the simplest case, this is $$S_2 \equiv \sum_{0\le i<j\le n}ij = \frac12 \left(\sum_{i,j=0}^n ij - \sum_{i=0}^n i^2\right).$$ I can rewrite this as $$S_2 = \sum_{i=1}^n i \sum_{j=i+1}^n j
= \sum_{i=1}^n i \left[\binom{n+1}{2}-\binom{i+1}{2}\right].$$ Is there a more explicit formula for this? Or maybe a more direct or geometrical argument to get to this? More generally, are there formulae for $S_k\equiv \sum_{0\le i_1<...<i_\ell\le n}i_1\cdots i_\ell$ ? One context in which these numbers arise is in the coefficients of $s!/(s-k)!$ with $k\le s$ : $$\frac{s!}{(s-k)!} = \sum_{j=0}^k S_k s^k.$$","['summation', 'integers', 'combinatorics', 'sequences-and-series']"
3853806,"Use the $\varepsilon - \delta$ definition of the limit to verify that $\lim_{(x,y)\to(2,5)} xy = 10$.","Question: Use the $\varepsilon - \delta$ definition of the limit to verify that $\lim\limits_{(x,y)\to(2,5)}xy = 10$ .
Hint given: $xy − 10 = (x − 2)(y − 5) + 5(x − 2) + 2(y − 5).$ My solution $\forall \space\varepsilon\gt0\space\space\exists\space\delta(\varepsilon)\gt0:|xy-10|\lt\varepsilon$ where $(x,y)\in D$ , whenever $0\lt\sqrt{(x-2)^2+(y-5)^2}\lt\delta$ Note: $(x-2)^2 \le (x-2)^2+(y-5)^2$ $\implies |x-2|\le\sqrt{(x-2)^2+(y-5)^2}$ , similarly, $|y-5|\le\sqrt{(x-2)^2+(y-5)^2}$ So if $(x,y)\in D\space$ and $\space0\lt\sqrt{(x-2)^2+(y-5)^2}\lt\delta$ . We choose $ \delta :=\frac{-7+\sqrt{49+4\varepsilon}}2$ . We obtain... $\begin{aligned}|xy-10|&=|(x − 2)(y − 5) + 5(x − 2) + 2(y − 5)|\\&\le|x − 2||y − 5|+ 5|x − 2| + 2|y − 5|\\&\le \left((x-2)^2 +(y-5)^2\right) +5\sqrt{(x-2)^2+(y-5)^2}+2\sqrt{(x-2)^2+(y-5)^2}\\&=\left(\sqrt{(x-2)^2+(y-5)^2}\right)^2+7\sqrt{(x-2)^2+(y-5)^2}\\&\lt \delta^2+7\delta\\&=\frac{49\pm\sqrt{49-4\varepsilon}+49+4\varepsilon}4+\frac{-98\pm14\sqrt{49+4\varepsilon}}4\\&=\frac{4\varepsilon}4\\&=\varepsilon\end{aligned}$ Comments This is a question on my Analysis 2 module. Would be great if someone could check my solution. Also, I realise you have to use a minimum for $\delta$ , but unsure on how to unpack it when writing the solution, would be great if anyone can make my understanding of this clearer.","['epsilon-delta', 'real-analysis', 'multivariable-calculus', 'solution-verification', 'limits']"
3853858,Differentiation: $y = 9x + \frac{3}{x}$,"Differentiate $y = 9x + \frac{3}{x}$ I think the first step is to turn $\frac{3}{x}$ into a more ""friendly"" format, so $x$ to the power of something maybe? How do I get $x$ with an index from $\frac{3}{x}$ ?","['calculus', 'derivatives']"
3853868,Refinements of the inequality $f(x)=x^{2(1-x)}+(1-x)^{2x}\leq 1$ for $0<x<0.5$,"Hi it's related to Showing the inequality $f(x)=x^{2(1-x)}+(1-x)^{2x}\leq 1$ for $0<x<1$ We want to show 1 : Let $0<x<0.5$ such that  then we have : $$f(x)=x^{2(1-x)}+(1-x)^{2x}\leq q(x)=(1-x)^{2x}+2^{2x+1}(1-x)x^2\leq 1$$ The Lhs is equivalent to : $$x^{2(1-x)}\leq h(x)=2^{2x+1}(1-x)x^2$$ Or : $$\ln\Big(x^{2(1-x)}\Big)\leq \ln\Big(2^{2x+1}(1-x)x^2\Big)$$ Making the difference of these logarithm and introducing the function : $$g(x)=\ln\Big(x^{2(1-x)}\Big)-\ln\Big(2^{2x+1}(1-x)x^2\Big)$$ The derivative is not hard to manipulate and we see that it's positive and $x=0.5$ is an extrema .The conclusion is : $$g(x)\leq g(0.5)=0$$ And we are done with the LHS. For the Rhs I use one of the lemma (7.1) due to Vasile Cirtoaje we have : $$(1-x)^{2x}\leq p(x)=1-4(1-x)x^{2}-2(1-x)x(1-2 x)\ln(1-x)$$ So we have : $$q(x)\leq p(x)+h(x)$$ We want to show that : $$p(x)+h(x)\leq 1$$ Wich is equivalent to : $$-2(x-1)x((4^x-2)x+(2x-1)\ln(1-x))\leq 0$$ It's not hard so I omitt here the proof of this fact . We are done . Question : Is it right ? Thanks in advance . Regards Max. 1 Vasile Cirtoaje, ""Proofs of three open inequalities with power-exponential functions"",
The Journal of Nonlinear Sciences and its Applications (2011), Volume: 4, Issue: 2, page 130-137. https://eudml.org/doc/223938","['exponentiation', 'logarithms', 'solution-verification', 'inequality', 'derivatives']"
3853963,Covariance of sum and maximum,"I have the task :) $X_1, X_2$ are independent and have uniform distribution on $(0,1).$ Calculate $\operatorname{Cov}(X_1+X_2,\max(X_1,X_2))$ . I did it in this way.
The distriburion of $\max(X_1,X_2)$ is $P(\max(X_1,X_2)=x)=2x$ on $(0,1)$ . In this way we have: $E(X_1+X_2)\cdot E\max(X_1,X_2)=1 \cdot \frac{2}{3}$ \begin{align}
& E((X_1+X_2) \cdot \max(X_1,X_2))=2 E(X_1\cdot \max(X_1,X_2)) \\[6pt]
= {} &2 \cdot  \int_0^1 E(t \cdot \max(t,X_2))\cdot f_{X_1}(t) \,dt=2\cdot  \int_0^1 t \cdot \frac{t+1}{2} \, dt=\frac{5}{6}
\end{align} So the covariance is equal $\frac{1}{6}$ But I have the correct answer to this task and it is $\frac{1}{12}$ Where did I mistake? Thanks in advance.","['covariance', 'uniform-distribution', 'probability-distributions', 'expected-value', 'probability']"
3853980,Prove that A is zero matrix,"Let $A$ be an $n×n $ complex matrix such that the three matrices $A+I$ , $A^2+I $ , $ A^3+I$ are all  unitary .Prove that $ A$ is the zero matrix I try to show that $Trace( A^{\theta}A) =0$ where $A^{\theta }$ is conjugate transpose of matrix $A$ $\because $ $Trace( A^{\theta}A)$ = $|a_{11}|^2 + |a_{12}|^2....|a_{nn}|^2$ $A+I$ is unitary ,so $(A+I)^{\theta}(A+I)= I $ $\implies (A^ {\theta}+I)(A +I) =I $ $A^ {\theta}A+ A^ {\theta}+A = 0$ $ Trace( A^{\theta}A)= -( Trace( A^{\theta}+A))$ $ \implies  Trace( A^{\theta}A)=-2$ ( sum of real parts of each diagonal entry of A I don't know how to proceed further
Please help","['matrices', 'unitary-matrices', 'linear-algebra']"
3853989,Treasure box with a bomb and $100 probability game,"There's a treasure box with \$100. There's a 0.5 probability that the box contains a bomb. The bomb has a probability of exploding on the i-th (for $i \leq 100$ ) day given by a uniform distribution. The bomb will eventually explode if there is a bomb. If it does not explode in the end, you can take all the money inside the treasure box. On the $i$ -th day, how much are you willing to pay to play the game? I am a little confused by the question, but I proceeded with finding the conditional probability that the treasure box doesn't have a bomb given that it doesn't explode the first $i$ days. Let $A$ denote the event that it doesn't have a bomb. Let $D_i$ denote the event that it went the first $i$ days without exploding. We want to derive $P(A | D_i)$ . \begin{align}
    P(A|D_i) = \frac{P(D_i|A)P(A)}{P(D_i)} \\
    P(A) = 0.5 \\
    P(D_i | A) = 1 \\
    P(D_i) = P(D_i | A) P(A) + P(D_i|A^c) P(A^c) \\
    = 1 * 0.5 + P(D_i|A^c) * 0.5 
\end{align} For $P(D_i|A^c)$ , which is given that there's a bomb in the box, what is the probability that the bomb doesn't explode in the first $i$ days. Finding the complement is easier. The complement is the probability that it explodes on any given day in the first i days, which is $i/100$ . So then the complement of this is $1 - i/100$ . So we have \begin{align}
    P(D_i) = (2 - i/100) * 0.5 \\
    P(A|D_i) = \frac{0.5}{(2 - i/100) * 0.5} \\
    = \frac{1}{(2-i/100)}
\end{align} This probability gets smaller and smaller as the number of days increases without the box exploding. After finding this, I am not sure how to answer the definitively answer the question of how much I'd pay to play the game on a given day. Because the conditional probability that the box has a bomb decreases with the number of days where the bomb doesn't explode, I would be inclined to say that I'd pay less and less, i.e., the amount I'd pay to play the game decreases as days go by. Is there an actual amount that is optimal to offer to play such a game?",['probability']
3854022,A graph satisfying $\lim_{x \to 0} f(x) = 0$ and $\lim_{x \to 0} f(f(x)) = 1$,I am wondering how $\lim_{x \to 0} f(x) = 0$ but $\lim_{x \to 0} f(f(x)) = 1$ is possible. Since $\lim_{x \to 0} f(f(x))$ = $f(\lim_{x \to 0} f(x)) = \lim_{x \to 0} f(x) = 0$ but not $1$ . I think this has something to do with discontinuity at $0$ but I am not able to sketch a graph satisfying this.,"['limits', 'limits-without-lhopital']"
3854027,"Suppose $\sum_{n\ge 1} |a_n| = A<\infty.$ Under what conditions is $\sum_{n\ge 1} \epsilon_n a_n = [-A,A]$, for $\epsilon_n \in \{-1,1\}$?","Consider the space of sequences: $$
\mathcal{E} = \{\{\epsilon_n\}_{n= 1}^{\infty}: \epsilon_n = \pm 1\}
$$ This can be considered a ""random choice of sign"" in the probabilistic context, for example. My question: if $\{a_n\}_{n=1}^{\infty}$ is an absolutely summable sequence with $\sum_{n\ge 1} |a_n|=A$ , under what conditions on $\{a_n\}$ is the following map a surjection? $$
f: \mathcal{E} \to \left[-A,A\right],\,
\{\epsilon_n\}_{n=1}^\infty \mapsto \sum_{n\ge 1}{\epsilon_n} a_n
$$ Note: I'm asking this question as a follow-up to a special case where $a_n=n^{-2}$ and have reused some of the language for continuity. In that question, the answer was no because $\pi^2/6 \approx 1.645,$ so one could never 'get back' to zero. Cases where the question is affirmative include $a_n=0$ and $a_n=2^{-n}$ , but I don't think other geometric series work. A necessary condition is $|a_1|\le \sum_{n\ge 2} |a_n|$ , and in fact I think its generalization is sufficient: if for all $m\in\mathbb{N}$ $$
|a_m|\le \sum_{n>m}|a_n|,
$$ then $f$ is a surjection. Heuristically, this is because you can 'double back' as much as you'd like, allowing you to reach every number in $[-A,A]$ . But maybe a weaker condition suffices, or perhaps even an explicit description of admissible $\{a_n\}$ ?","['real-numbers', 'functions', 'sequences-and-series', 'real-analysis']"
3854062,Show that mod-function is surjective,"We are to show that the following function is surjective, when n is from the set of integers: $$(4n+6)mod(1729)$$ The codomain are the integers from {0, 1728} How do I proceed? In advance, thank you for your help.","['elementary-number-theory', 'modular-arithmetic', 'discrete-mathematics']"
3854076,Does $\sin\vartheta=2^n\sin\frac{\vartheta}{2^n}\prod\limits_{k=1}^n\cos\frac{\vartheta}{2^k}$ imply $\vartheta=1$?,"From the formula, $$\sin\vartheta = 2\cos\frac{\vartheta}2\sin\frac{\vartheta}2\tag1$$ by dividing both sides by $\cos\vartheta$ we could derive that $$\tan \vartheta = \frac 2{\cos \vartheta}\cos^2\frac{\vartheta}2\tan\frac{\vartheta}2$$ and hence by iteration through $\tan\vartheta\mapsto\tan\frac{\vartheta}2$ and simplifying, we can show that $(1)$ is a special case $n=1$ of the broader theorem, $$\sin\vartheta=2^n\sin\frac{\vartheta}{2^n}\prod_{k=1}^n\cos\frac{\vartheta}{2^k}\tag2$$ for a natural number $n\geqslant 1$ . When we iterate through $\frac{\sin\vartheta}{\vartheta}\mapsto\cfrac{\sin\frac{\vartheta}{2^n}}{\frac{\vartheta}{2^n}}$ , we obtain \begin{align}\frac{\sin\vartheta}{\vartheta}&=\bigg(\prod_{k=1}^n\cos\frac{\vartheta}{2^k}\bigg)\bigg(\prod_{k=1}^n\cos\frac{\vartheta}{2^{k+n}}\bigg)\bigg(\prod_{k=1}^n\cos\frac{\vartheta}{2^{k+2n}}\bigg)\cdots \\ &=\prod_{k=1}^\infty\cos\frac{\vartheta}{2^k}\tag3\end{align} and this is where I am stuck. By $(3)$ , wouldn't $(2)$ imply that $$\eta\sin\frac{\vartheta}{\eta}\stackrel{\eta\to\infty}{\longrightarrow}1$$ for some $\eta$ (in this case, $\eta=2^n$ ), which could only mean $\vartheta=1$ ? This is strange to me because $(2)$ is meant to work for all $\vartheta$ and not just $\vartheta=1$ . Thanks in advance, and apologies for the nonrigorous approach.","['induction', 'recurrence-relations', 'infinite-product', 'limits', 'trigonometry']"
3854081,"Prove that the set ${\{} \frac{1}{x-c}{\}}_{\displaystyle\ c \in \mathbb{R}\setminus[0,1]}$ is linearly independent.","Question: Let $V$ be the vector space of all real valued functions defined on the unit interval $[0,1]$ .
Show that the set $\displaystyle\ \bigg{\{} \frac{1}{x-c}\bigg{\}}_{\ c \in \mathbb{R}\setminus[0,1]}$ is linearly independent. Attempt: Assume towards a contradiction that the set is linearly dependent. So, $\exists$ a finite subset $\displaystyle\bigg{\{}\frac{1}{x-c_i}\bigg{\}}_{i=1}^n$ which is linearly dependent. Therefore, for some $(d_1,d_2,...,d_n)\neq (0,0,..,0)$ we must have $\displaystyle f(x)=\sum_{i=1}^n\frac{d_i}{x-c_i}=\frac{d_1(x-c_2)...(x-c_n)+d_2(x-c_1)(x-c_3)...(x-c_n)+...+d_n(x-c_1)...(x-c_{n-1})}{(x-c_1)...(x-c_n)}=0$ for all $x\in[0,1]$ . Now, the denominator is $\neq 0$ $\forall x\in [0,1]$ . So, $f(x)=0$ only when $d_1(x-c_2)...(x-c_n)+d_2(x-c_1)(x-c_3)...(x-c_n)+...+d_n(x-c_1)...(x-c_{n-1})=g(x)=0$ . However, $g(x)$ is a polynomial of degree $\leq n-1$ and $g(x)=0$ for every $x \in[0,1]$ , which implies that number of zeros of $g(x)$ is $>deg(g(x))$ , hence $g(x)$ must be identically equal to $0 \implies (d_1,d_2,...,d_n)= (0,0,..,0) $ . Therefore, our assumption that the given set is linearly dependent is not tenable, i.e. the set is linearly independent. Is this correct?","['solution-verification', 'linear-algebra', 'polynomials']"
3854254,is any linear function $C^\infty(M) \rightarrow \mathbb{R}$ a velocity?,"Let $M$ a smooth manifold, $C^\infty(M)$ is space of smooth functions on $M$ , $\gamma$ is a curve $\mathbb{R}\rightarrow M$ , and $\gamma(\lambda_0)=p\in M$ . Define the velocity of $\gamma$ at $p$ is the linear map $v_{\gamma,p}:C^\infty(M)\rightarrow \mathbb{R}$ as $$v_{\gamma,p}(f):=(f\circ \gamma)'(\lambda_0)$$ Is any linear function $C^\infty(M) \rightarrow \mathbb{R}$ a velocity? i.e. for an arbitrary linear function $\phi:C^\infty(M) \rightarrow \mathbb{R}$ , we can find a velocity $v_{\gamma,p}$ such that $\phi=v_{\gamma,p}$ . If not, give an example.","['manifolds', 'differential-geometry']"
3854262,Can someone help me to compute this integral with a delta function,"I don't know how to compute this integral: $$\int_{0}^{\infty}  \prod_{i=1}^a dx_i \,\delta \left(\sum_{i=1}^a x_i - a\right)$$ The result should be: $$\frac{a^{a-1} }{(a-1)!}$$ Thanks very much for helping! edit: Thanks to the link I am one step further: $$\int_0^\infty dx_a\delta \left(\sum_{i=1}^a x_i - a\right)=1$$ if $$x_a=a-\sum_{i=1}^{a-1}x_i\geq 0\\
\Leftrightarrow \quad \sum_{i=1}^{a-1}x_i \leq a$$ So: $$\int_{0}^{\infty}  \prod_{i=1}^a dx_i \,\delta (\sum_{i=1}^a x_i - a)=\int_0^a dx_1 \int_0^{a-x_1} dx_2 \ldots \int_0^{a-x_1 - \ldots - x_{a-2}} dx_{a-1}$$ But how do I now show: $$\int_0^a dx_1 \int_0^{a-x_1} dx_2 \ldots \int_0^{a-x_1 - \ldots - x_{a-2}} dx_{a-1}=\frac{a^{a-1} }{(a-1)!}$$","['multivariable-calculus', 'multiple-integral', 'dirac-delta', 'simplex']"
3854287,When is $an+b$ a square?,"Pick two integers $a$ and $b$ , and construct the arithmetic progression $an+b$ . My question is, when does this contain infinitely many squares? And which terms in that sequence are the squares (i.e., for which $n$ )? For instance: $2n+3$ is a square infinitely often (by setting $n=2m^2+2m-1$ for $m\in\bf Z$ ), Similarly $5n+1$ is a square when $n=0,3,7,16,\dots$ . This time the formula for which $n$ to take is not as simple, it's $n=\frac{1}{8} \left(10 m^2+2
   \left((-1)^m-5\right)
   m+(-1)^{m+1}+1\right)$ . $9n+4$ is a square whenever $n=9m^2+4m$ , but $9n+5$ is never a square $8n+1$ is a square when $n$ is triangular Can someone point me to a general study of this diophantine problem, $k^2=an+b$ ?","['number-theory', 'diophantine-equations']"
3854345,Define a map that looks like a solid,"Let $S$ be a solid in $\mathbb{R}^3$ . If we are viewing it from the x-axis, in the front, it looks like a circle, a rectangle from the side, and equilateral triangle from above, all centered at the origin. I'm not even sure what a solid like this will look like, so how can we define the map $f : \mathbb{R}^3 \to \mathbb{R}^2$ by $f(x,y,z) = (y,z)$ ? And how can we describe $f(S)$ using sets?","['functions', 'vectors']"
3854353,Any shortcut to remember least upper bound $\vee$ and greatest lower bound $\wedge$ in Lattice concept,I know this is silly but I am every time forgetting lub (least upper bound) in lattice as $\vee$ and glb (greatest lower bound) as $\wedge$ . Is there any shortcut  or mnemonic for remembering which one is join and which one is meet? Is there any historical reason for choosing such symbols?,"['lattice-orders', 'soft-question', 'discrete-mathematics']"
3854385,Intersection multiplicities,"Let $X \subset \Bbb P^n$ be a curve defined by $f_1 = \dots = f_r = 0$ and $H$ an hyperplane. How to compute the multiplicity $m$ at a point $p \in X \cap H$ ? Specific example : In my case, $X \subset \Bbb P^3$ is the twisted cubic, given by $xw = yz, xz = y^2, yw = z^2$ . It can be also parametrised as $[s^3:s^2t:st^2:t^3]$ , and $H$ is the hyperplane given by $y=0$ . What I tried : Set theoretically, $X \cap H = \{ [1:0:0:0]\} \cup \{ [0:0:0:1]\}$ . However, since $X$ is a cubic curve one should have multiplicity $2$ . Around the first point, we can set $x=1$ and take local coordinates $y,z,w$ . The curve is then given by $(t,t^2,t^3)$ and $y=0$ b the equation is $t=0$ , multiplicity one. Around the second point, we can take $w=1$ and then local coordinates $x,y,z$ .  The curve is given by $(s^3,s^2,s)$ and $y=0$ becomes $s^2 = 0$ , hence the multiplicity should be 2. Specific questions : Is my argument correct ? How to get the same answer without using the parametrisation, i.e only using the equations defining $X$ ?","['algebraic-curves', 'algebraic-geometry', 'intersection-theory']"
3854388,Does an identity exist for distributing the inverse for a product including nonsquare matrices?,"For example, if $A$ and $B$ are invertible square matrices, we can write $(AB)^{-1} = B^{-1} A^{-1}$ . Now, consider $A$ is an $n \times n$ matrix and $C$ is an $n \times m$ matrix. If $A$ is invertible, does an identity exist for distributing the inverse inside parenthesis of a product of matrices including a nonsquare matrix such as $C$ ? For example, if $(C^T A C)^{-1}$ exists, does some identity exist for $(C^T A C)^{-1}$ ?","['matrices', 'linear-algebra', 'inverse']"
3854481,Solutions to Spring 2020 UCLA Analysis Qual Problem 1? Or: an identity implies function is odd,"I would like to see an elegant/simple solution  to the following problem from the Spring 2020 UCLA Analysis Qual. Suppose that $f\in C_c^{\infty}(\mathbb R)$ satisfies \begin{equation}
\int_{\mathbb R}e^{-tx^2}f(x)\,dx=0\qquad\text{for any }t\geq0.
\end{equation} Show that $f$ is odd; that is, show that $f(x)=-f(-x)$ for each $x\in\mathbb R$ . The proof that I have in mind is quite long. Any help is appreciated!","['even-and-odd-functions', 'real-analysis']"
3854495,Proving Using Set Identities $(A \cap B) \cup (A \cap \overline{B}) = A$,"I was wondering if the following is valid: $$(A \cap B) \cup(A\cap \overline B) \space \\ \text{Where $\overline B$ is the complement of B.} \\ A \cap(B\cup \overline{B}) \space \text{Distributive law, but in reverse.} \\ A\cap U \space \text{Complement Laws.}\\ A \space \text{Identity Laws.} \space \square$$","['elementary-set-theory', 'solution-verification']"
3854505,Solve $\dot{S}(t)=S(t)A+A^TS(t)$.,I am faced with the matrix differential equation $\dot{S}(t)=S(t)A+A^TS(t)$ for which I have to find a solution. I remember I learned that the matrix differential equation $\dot{S}(t) = AS(t)$ has the solution $S = S(0)e^{At}$ but I am not sure this helps me here.,"['ordinary-differential-equations', 'matrices', 'matrix-calculus', 'linear-algebra', 'matrix-equations']"
3854590,Construct following semidecidable sets,"Being an undergrad, was looking through our previous year task books during exam preparation and got stuck on this one: Are there such semidecidable (recursively enumerable) sets X, Y that their composition $X \cdot Y=\{xy| \forall x\in X, \forall y\in Y \}$ is recursive? Do you have any ideas?","['recursive-algorithms', 'discrete-mathematics', 'computability']"
3854592,What finitely-generated amenable groups arise as subgroups of compact Lie groups?,I am  looking for examples of (edit: amenable) finitely-generated subgroups of any compact Lie group which are infinite and not virtually abelian.  An example with polynomial growth would be especially nice.,"['geometric-group-theory', 'group-theory', 'lie-groups']"
3854600,Find the total Gaussian curvature of a surface in $\mathbb{R}^3$,"The surface is defined by $$z^2=-(x^2+y^2-16)((x-2)^2+y^2-1)((x+2)^2+y^2-1).$$ How would I get the total Gaussian curvature? I'm aware of that if the surface is represented by $X=(x,y,z)$ , then the Gaussian curvature is $$k=\frac{LM-N^2}{EG-F^2}$$ where $E=X_x\cdot X_x,\, F=X_x\cdot X_y,\, G=X_y\cdot X_y$ and $L=X_{xx}\cdot n,\, M= X_{xy}\cdot n,\, N=X_{yy}\cdot n$ . Here $$n=\frac{X_x\times X_y}{|X_x\times X_y|},$$ the unit normal vector. But the calculation is extremely overwhelming, because of the square power of $z$ . Is there an other way other than just calculating everything? And my definition of the total curvature is the surface integral of Gaussian curvature $\int_S k\,dA$ , where $S$ is the surface defined above. Is my definition correct?","['curvature', 'differential-geometry']"
3854610,Proof such recursive set existence.,"Recently my teacher gave me the following task to think about, while I've failed to solve it properly and already lost my opportunity to increase term mark, I am still have no ideas of how to really solve it. Unfortunately my teacher has no enough time to explain me this task properly. Maybe someone here has any ideas? X, Y - recursive sets, such as $X / Y=\{t\in \mathbb{N}| \exists x\in X, \exists y\in Y: x=yt\} is non-recursive$ Whether following recursive sets X and Y may exist?","['recursive-algorithms', 'computational-complexity', 'discrete-mathematics', 'computability']"
3854687,Functional Derivative with Discrete Variable,"Problem $$\text{Find}\quad\frac{\delta F_k}{\delta G} \quad \text{given} \quad F_k=\left(\sum_{r=0}^{N-1} e^{ikr}\int_{-\infty}^{\infty} dt \ e^{i\omega t} G(r,t)\right)^{-1}$$ noting that $k$ and $r$ are discrete while $\omega$ and $t$ are continuous. Background I am trying to show that $F_k[G(r,t)]$ is very sensitive to perturbations in $G(r,t)$ . By numerical work I know this is (often) the case. Ultimately I'd like to argue that constructing $F_k$ by measuring $G(r,t)$ won't work well when the measurement is noisy. I figured a good way to show that was to compute the functional derivative and show it gets large (or perhaps diverges) in some places. However, I am doing something wrong. Attempt By the definition here : $$\frac{\delta F_k}{\delta G}:\int dt \frac{\delta F_k}{\delta G}\eta(t)=\left[\frac{d}{d\epsilon}\left(\sum_{r=0}^{N-1} e^{ikr}\int dt \ e^{i\omega t} G(r,t)+\epsilon\eta(t)\right)^{-1}\right]_{\epsilon=0}$$ Using the chain rule, $\sum_{r=0}^{N-1} e^{ikr}=N\delta_{k,0}$ i.e. the Kronecker delta, and the definition $G(k,\omega)\equiv\sum_{r=0}^{N-1} e^{ikr}\int dt \ e^{i\omega t} G(r,t)$ transforms the above to $$\frac{-\int dt\sum_{r=0}^{N-1} e^{i(kr+\omega t)}\eta(t)}{\left(\sum_{r=0}^{N-1} e^{ikr}\int dt \ e^{i\omega t} G(r,t)\right)^{2}}=\int dt\frac{-\delta_{k,0} \ e^{i\omega t}}{G(k,\omega)^2}\eta(t)\to \frac{\delta F_k}{\delta G(r,t)}=\frac{-\delta_{k,0} \ e^{i\omega t}}{G(k,\omega)^2}$$ I know this is wrong because this numerics show this isn't $0$ for all $k\neq 0$ . I suspect the problem has something to do with $\eta$ only depending on $t$ and not $r$ , but I'm not sure how to account for a discrete variable like $r$ . Resolution? I think my understanding of the definition above is likely wrong, and probably needs a test function that depends on both $r$ and $t$ ? I'd be pretty jazzed if the right way to do it looked something like $$\frac{\delta F_k}{\delta G}:\sum_{r=0}^{N-1}\int dt \frac{\delta F_k}{\delta G}\eta(r,t)=\left[\frac{d}{d\epsilon}F_k\left[G(r,t)+\epsilon\eta(r,t)\right]\right]_{\epsilon=0}$$ but my math is too weak to assert this is the right way, even though it looks like it makes some sense (to me). I really have no idea how to handle the fact that $r$ is discrete. If the above is correct, this would give $$
\frac{\delta F_k}{\delta G(r,t)}=\frac{-e^{i(\omega t+kr)}}{G(k,\omega)^2}
$$ which is what I was hoping to show since $G(k,\omega)=0$ for at least one $\omega$ at every $k$ . Seeking Any of the following $\dfrac{\delta F_k}{\delta G}$ How to approach such functional derivatives? What's wrong with my attempt? Confirm/deny the correctness of my ""resolution"". Thanks in advance!","['functional-calculus', 'complex-analysis', 'calculus', 'functional-analysis', 'derivatives']"
3854690,Resolving Recurrence by Induction,"Problem: $T(0)=0, T(1)=1$ and $T(n)=T(n-1)+T(n-2)$ $\forall n\geq 2$ Given $T(2n)=T(n-1)T(n)+T(n)T(n+1)$ $\forall n\geq 1$ , prove by induction that $T(2n)=T(n+1)^2 -T(n-1)^2$ $\forall n \geq 1$ My attempt : Base Case : $T(0)=0$ , $T(1)=1$ Inductive Hypothesis : Assume $T(2n)=T(n+1)^2-T(n-1)^2$ for $n=k$ $\forall n \geq 1$ Inductive Step : Show the inductive hypothesis works for $n=k+1$ \begin{align}
T(2(n+1)) &= T(2n+2)\\
&= T((2n+1)-1) + T((2n+1)-2)&&\text{By definition}\\
&= T(2n+1) + T(2n)&&\text{Simplify}\\
&= T(2n+1) + T(n+1)^2 - T(n-1)^2&&\text{Apply hypothesis}
\end{align} But after this, I'm stumped. I'm not sure how to continue this proof, or if this is dead end. Any help would be greatly appreciated!","['induction', 'recurrence-relations', 'discrete-mathematics']"
3854736,Challenging problem: Calculate $\int_0^{2\pi}x^2 \cos(x)\operatorname{Li}_2(\cos(x))dx$,"The following problem is proposed by a friend: $$\int_0^{2\pi}x^2 \cos(x)\operatorname{Li}_2(\cos(x))dx$$ $$=\frac{9}{8}\pi^4-2\pi^3-2\pi^2-8\ln(2)\pi-\frac12\ln^2(2)\pi^2+8\ln(2)\pi G+16\pi\Im\left\{\operatorname{Li}_3\left(\frac{1+i}{2}\right)\right\}$$ My only try is writing $$\operatorname{Li}_2(\cos(x))=-\int_0^1\frac{\cos(x)\ln(y)}{1-\cos(x)y} \, dy$$ and have no idea how to continue with the double integral. I also tried $\cos(x)=u$ , didn't do much. Any help would be much appreciated.","['integration', 'definite-integrals', 'real-analysis', 'calculus', 'polylogarithm']"
3854805,Why are the derivatives of trigonometric functions again trigonometric?,"The derivative of a periodic function would of course be periodic, but I am just curious to know why the derivative of a trigonometric function does not go out of the trigonometric world?","['calculus', 'trigonometry']"
3854868,"Assume $(G,\times)$ is a group and for $a,b \in G$: $ab=ba$, $\text{ord}(a)=n$, $\text{ord} (b)=m$ [duplicate]","This question already has answers here : $\operatorname{ord}(u)=r,\,\operatorname{ord}(v)=s$ $\,\Rightarrow\,\operatorname{ord}(uv)=rs\,$ if $\,r,s\,$ (co)primes (3 answers) Completing the Proof: If $a$ and $b$ commute, then ${\rm ord}(ab)$ is a divisor of ${\rm lcm}(m,n)$ (3 answers) Closed 3 years ago . Assume $(G,\times)$ is a group and for $a,b \in G$ : $ab=ba$ , $\text{ord}(a)=n$ , $\text{ord}
(b)=m$ Show that if $\gcd(m,n)=1$ then $G$ has an element of order $nm$ . If $m,n$ are arbitrary,then $G$ has an element of order $\text{lcm}(m,n)$ Since $G$ is not cyclic I don't have any idea how to start, any help is appreciated. Lemma : Assume $(G,\times)$ is a group and $a,b \in G$ , Moreover $ab=ba$ .
let $\text{ord}(a)=n$ and $\text{ord}(b)=m$ ,then $\text{ord}(ab)\mid \text{lcm}(n,m)$ . $\text{lcm}(n,m)=ns$ and $\text{lcm}(n,m)=mr$ for some $r,s \in \mathbb Z^+$ ,then: $$(ab)^{\text{lcm}(n,m)}$$ Since $ab=ba$ ,hence $$=a^{\text{lcm}(n,m)}b^{\text{lcm}(n,m)}$$ $$=a^{ns}b^{mr}=(a^n)^s(b^m)^r$$ $$=e^se^r=e$$ Follows $\text{ord}(ab)\mid \text{lcm}(n,m)$ . Since $\text{ord}(ab) \mid \text{lcm}(n,m)=\frac{nm}{\text{gcd}(n,m)}$ ,By the assumption $\text{gcd}(n,m)=1$ So $\text{ord}(ab) \mid nm$ If $\text{ord}(ab)
 \mid \text{lcm}(n,m)$ then there is $g \in G$ such that $g^{\text{lcm}(n,m)}=e$","['gcd-and-lcm', 'group-theory', 'solution-verification']"
3854872,Embedding of $S^n$ to the smooth strictly convex region in $R^{n+1}$,"Assuming $D\subset R^{n+1}$ is smooth and strictly convex region, one can construct an embedding $\varphi$ of $S^n$ to the boundary $\partial D$ of $D$ which associates to each direction $z\in S^n$ the point of $\partial D$ with normal $z$ , where $S^n$ is unit  sphere. This is given by $$
\varphi(z)= s(z)z+\nabla s(z)   \tag{1}
$$ where $s$ is support function of $D$ , $$
s(z)=\sup_{y\in D} \langle y, z\rangle
$$ and $\nabla s$ is the gradient vector of $s$ with respect to the standard metric $g$ on $S^n$ . The above statement is from Andrews, Ben , Entropy estimates for evolving hypersurfaces , Commun. Anal. Geom. 2, No. 1, 53-64 (1994). ZBL0839.53049 . I want to show the $\varphi(z)$ is the point of $\partial D$ with normal $z$ , but don't know how to begin.   There are two things need to be proved: 1, How to show $\varphi(z)\in \partial D$ ? 2, How to show the normal of $\varphi(z)$ is $z$ ? Besides, assuming $\{e_1, ...,e_n\}$ is a local basis of $S^n$ , I understand the gradient vector $\nabla s$ is $$
\nabla s = g^{ij}(\nabla _{e_i} s)  e_j
$$","['convex-geometry', 'support-function', 'differential-geometry']"
3854894,"Let $f:[-\frac 12,2]\to R$ and $g: [-\frac 12,2] \to R$ be defined as $f(x)=[x^2-3]$ and $g(x)=|x|f(x)+|4x-7|f(x)$","CONT
Let $f:[-\frac 12,2]\to R$ and $g: [-\frac 12,2] \to R$ be defined as $f(x)=[x^2-3]$ and $g(x)=|x|f(x)+|4x-7|f(x)$ . Find the number of points of non differentiability of $g(x)$ The points we need to check for are $0,1,\sqrt 2, \sqrt 3, 2, \frac 74$ I don’t really know the proper way to do this, but instinctually $1,\sqrt 3, \sqrt 2$ will be the correc points. By computation, I proved that the function is differentiable at $x=\frac 74$ In $g(x)$ , the first term is non differentiable at $x=0$ but the second term isn’t, so the function should be non differentiable at $0$ (Emphasis on should because the grap shows that the function is continuous and differentiable at $x=0$ ) I can’t speak for $2$ , because the function is continuous over that point, but I don’t if extremes are counted in differentiability Is my answer correct?","['continuity', 'derivatives']"
3854915,How can I prove that $f'(x)=1+\left[f(x)\right]^2$ has no solution over $\mathbb{R}$ without solving the equation explicitly?,"It is known that the only solutions to the ODE $$f'(x)=1+\left[f(x)\right]^2$$ are of the form $f(x)=\tan(c+x)$ (this is also easy to verify by hand). This shows that the differential equation can't have a solution over $\mathbb{R}$ because $\tan (c+x)$ is undefined whenever $c+x=\frac{\pi}{2}+\pi n$ for integer $n$ . But what if I didn't know this? What if I didn't know that $$f'(x)=1+\left[f(x)\right]^2\iff f(x)=\tan(c+x)$$ Heck, what if I've never even heard of the tangent function nor any of the other trigonometric functions? Presumably, I could prove that there can't be a solution over $\mathbb{R}$ from the ODE alone, but how would I go about doing it? For the record, I have no idea how to approach this. From the assumption that $f$ is differentiable everywhere, nothing from the equation seems to ""break"": you get two everywhere-continuous functions, $f'$ and $1+f^2$ , and they are equal to each other.","['calculus', 'ordinary-differential-equations']"
3854945,"If $f$ is Riemann integrable but not continuous on $[0,1]$, does $\lim_{n \to \infty} \left( \int_0^1 |f(x)|^n dx \right)^{\frac{1}{n}}$ exists?","Suppose $f: [0,1] \to \mathbb R$ is Riemann integrable on $[0,1]$ , but not continuous on $[0,1]$ . Let $$a_n = \left( \int_0^1 |f(x)|^n dx \right)^{\frac{1}{n}}$$ for $n \in \mathbb N$ . Does $\lim_{n\to\infty} a_n$ exists? If it does, what is it equal to? If $f$ is continuous, I know that $(a_n)$ converges to $M = \sup\{{|f(x)|: x \in [0,1]}\}$ . But the proofs that I found for this case rely the continuity of $f$ to show that $\liminf_{n\to\infty} a_n \geq M$ . My idea is to use $M = \sup\{|f(x)|: x\in C\}$ instead, where $C\subseteq [0,1]$ is the set of all points at which $f$ is continuous. Then, if my reasoning is correct, there must be a $c \in C$ such that $\lim_{x\to c^+} f(x) = M$ or $\lim_{x\to c^-} f(x) = M$ . Given an arbitrary $\varepsilon > 0$ , perhaps I can then construct an interval $I \subset [0,1]$ such that $|f(x)| \geq M-\varepsilon$ for all $x \in I$ . After that, the remaining parts should be similar to the continuous case. Nevertheless, I'm quite sure that there is some error in my line of reasoning, or maybe there is much more to be demanded for this argument to be complete. From what I read, the term $a_n$ is actually $\|f\|_n$ (the $L^n$ norm), so $(a_n)$ should converge to $\|f\|_\infty$ as $n \to \infty$ . However, my current understanding is limited to Riemann integration, without any knowledge whatsoever on measure theory and function spaces. Is there a way to prove the convergence of $(a_n)$ without resorting to measure theory, or even Lebesgue's Criterion?","['integration', 'limits', 'riemann-integration', 'real-analysis']"
3855038,"Suppose $X$ is a Banach space and $Y$ is a closed subspace. If $Y$ and $X{/}Y$ are reflexive, then is $X$ also reflexive?","My intuition is that the answer is affirmative. I have shown that a Banach space $X$ is reflexive iff the closed unit ball of $X$ is weakly compact. I was trying to use this, but it hasn't been a success so far.","['banach-spaces', 'functional-analysis', 'reflexive-space']"
3855097,Commuting Matrices have a common eigenvector (using Hilbert's Nullstellensatz),"I am aware that there exists elementary proof of the fact that commuting matrices have a common eigenvector. But recently I came across this following statement from the Wikipedia article ""https://en.wikipedia.org/wiki/Triangular_matrix#Simultaneous_triangularisability"" which goes as: ""The fact that commuting matrices have a common eigenvector can be interpreted as a result of Hilbert's Nullstellensatz: commuting matrices form a commutative algebra $k[A_1,A_2, \cdots A_n]$ over $K[x_{1}, x_{2}, \cdots x_n]$ which can be interpreted as a variety in k-dimensional affine space, and the existence of a (common) eigenvalue (and hence a common eigenvector) corresponds to this variety having a point (being non-empty), which is the content of the (weak) Nullstellensatz. In algebraic terms, these operators correspond to an algebra representation of the polynomial algebra in k variables.""
I am having a hard time understanding the statements made here. In particular, I want to know a)How can we interpret the algebra $k[A_1,A_2, \cdots A_n]$ as a variety? b)What does the article mean by the statement ""a common eigenvalue and hence a common eigenvector""?
We know that a common eigenvalue for two matrices does not mean they have the same eigenvector! It will be nice if someone can help me out with this.","['algebraic-geometry', 'abstract-algebra', 'linear-algebra']"
3855108,Prove a bijection between $\mathbb{N}^2$ and $\mathbb{N}$. [duplicate],"This question already has answers here : Proving the Cantor Pairing Function Bijective (4 answers) Closed 3 years ago . Prove that the function $$f(m,n)=\frac{1}{2}\left(m^2+2 m n+n^2+m+3 n\right)$$ is a bijection between $\mathbb{N}^2$ and $\mathbb{N}$ . The problem arose in a series problem. I have to show that for each couple $(m,n)$ we get a different natural number and that all natural numbers are got applying $f$ . Below an example of what happens for $m,n$ from $0$ to $6$ $$
\begin{array}{ccccccc}
 0 & 2 & 5 & 9 & 14 & 20 & 27 &\ldots\\
 1 & 4 & 8 & 13 & 19 & 26 & 34 &\ldots\\
 3 & 7 & 12 & 18 & 25 & 33 & 42 &\ldots\\
 6 & 11 & 17 & 24 & 32 & 41 & 51 &\ldots\\
 10 & 16 & 23 & 31 & 40 & 50 & 61 &\ldots\\
 15 & 22 & 30 & 39 & 49 & 60 & 72 &\ldots\\
 21 & 29 & 38 & 48 & 59 & 71 & 84 &\ldots\\
\ldots\\
\end{array}
$$",['elementary-set-theory']
3855127,"If $f$ is complex analytic on $S=\{x+iy : |x|<1, |y|<1\}$, continuous on $\bar{S}$ and bounded by $1,2,3,4$ on each side, then is $|f(0)|>2$ possible?","I'm a second-year undergraduate taking an introductory course in complex analysis. I am stuck on this problem from one of the previous year's exam: True or False: For a function $f$ analytic on $S = \{ x + iy : x \in \mathbb{R}, y \in \mathbb{R}, |x| < 1, |y| < 1 \}$ and continuous on $\bar{S} = \{ x + iy : x \in \mathbb{R}, y \in \mathbb{R}, |x| \leq 1, |y| \leq 1 \}$ , and satisfying that $|f|$ is bounded on the four sides $\gamma_1, \gamma_2, \gamma_3, \gamma_4$ of the square $\bar{S}$ respectively by $1, 2, 3, 4$ , it is possible to have $|f(0)| > 2$ . I'm not able to disprove the existence of such a function or construct an example of such a function, but my guess is that it should be false. We have learnt about the Maximum Modulus Theorem, which says that A non-constant holomorphic function on an open connected domain never attains its maximum modulus at any point in the domain. Maybe by shifting the function $f$ by some constant or linear function I can show that it violates this Theorem, and so $f$ cannot exist, but I am not able to come up with a proof.
Another result that we were taught that seems relevant is the Schwarz Lemma, which says that: Let $\mathbb{D} = \{ z : |z| < 1 \}$ be the open unit disk and let $f \colon \mathbb{D} \to \mathbb{C}$ be a holomorphic map such that $f(0) = 0$ and $|f(z)| \leq 1$ on $\mathbb{D}$ . Then $|f(z)| \leq |z|$ $\forall\ z \in \mathbb{D}$ and $|f'(0)| \leq 1$ . Moreover, if $|f(z)| = |z|$ for some non-zero $z$ or $|f'(0)| = 1$ , then $f(z) = az$ for some $a \in \mathbb{C}$ with $|a| = 1$ . Maybe by considering the restriction of $f$ to the unit disk and rescaling I could apply Schwarz Lemma, but I'm not sure how to go about this either. Of course, I could be wrong and there is indeed such a function $f$ , but in that case, I don't know how to go about constructing it. How can I solve this problem? Any useful hints are also fine, a complete solution is not necessary.","['complex-analysis', 'analytic-functions']"
3855133,Question on tiling of a $2n\times 2n$ square,"$\text{Introduction}$ This is a classical question: How many pavements of an $m\times n$ board (such that $mn$ is even) with $1\times 2$ and $2\times 1$ tiles? There are several beautiful results and articles related to this. If you aren't familiar with the problem, read this and this (there are many results and refferences and proofs). The main thing I want to focus on is the formula for the number of such tilings: For an $m\times n$ board with $m$ even (WLOG), we have $$\prod_{k=1}^{\frac{1}{2}m}\prod_{l=1}^{n}2\sqrt{\cos^2{\frac{k\pi}{m+1}}+\cos^2{\frac{l\pi}{n+1}}}$$ $\text{My question}$ However, I want to ask this: Suppose we place one $1\times 2$ tile (or $2\times 1$ tile) on an $m\times n$ board (such that $mn$ is even) and call it $\mathcal{T}$ . How many pavements with $1\times2$ and $2\times1$ tiles are there, which contain $\mathcal{T}$ . Lets call this number $f(\mathcal{T})$ This seems very hard. To begin with, analysing some small cases, like $2\times 3$ and $4\times 4$ boards, different $\mathcal{T}$ s lead to different $f(\mathcal{T})$ s. However, on the plus side, Using complex numbers mathematicians have developed some pretty powerful methods of controlling tilings and configurations. Of course this can be both generalized to more $\mathcal{T}$ s (which is very unlikely however) and reduced to special cases such as $2n\times 2n$ boards or $2\times n$ boards (well, this $2\times n$ case is actually really simple using induction) . Any progress upon the problem is appreciated! I highly belive that a beautiful result awaits in the $2n\times 2n$ case, as for the general case. I am not a specialist in combinatorics or pavements and sincerely apologize for not providing more context. However, will continue to try to solve this and post updates if I find anything. Thank you! P.S. If anyone can code a program to find the number of such tilings for a constant tile $\mathcal{T}$ which we can input, it would help a lot and I count it as an accepted answer, unless someone shows up with a proof.","['combinatorics', 'problem-solving', 'tiling']"
3855155,proving a relation has the transitive property,"I am trying to solve the following question: For all $ f,g∈ \Bbb N^ \Bbb N $ we say that f and g are almost identical if there does not exist $ X⊆ \Bbb N,where|X|=∞$ ,such that $ ∀i∈X:f(i)≠g(i) $ .Given a relation R,where $ R⊆ \Bbb N^\Bbb  N×\Bbb N^\Bbb N $ ,and defined∶  R≡{ $(f,g) ∈\Bbb N^\Bbb N×\Bbb N^\Bbb N $ | f and g are almost identical }. Prove that R is a transitive relation.
I had the idea to suppose that R is not transitive ,and to arrive to a contradiction by somehow find a natural number t such that $f(t)=g(t)$ and $g(t)=w(t)$ and conclude that $f(t)=w(t)$ (which is a contradiction), but it seems  not easy to do so. I would be happy for some help or hints!","['elementary-set-theory', 'relations']"
3855248,Banach space is product of quotient space,"Motivation: If $a$ and $b \ne 0$ are real numbers, then $a = b \cdot (a / b)$ . Question: Let $X$ be a Banach space and $M \subset X$ a closed subspace. Then, the quotient space $X / M$ is also a Banach space. Do we have $$
X = M \times (X / M)
$$ in any sense? (For example, "" $\times$ "" could denote the product Banach space and "" $=$ "" could mean ""isomorphic"".)","['banach-spaces', 'functional-analysis', 'quotient-spaces']"
3855257,Number of rays intersecting at a point inside a polygon,"I'm working on a project to do with bouncing rays inside polygons and now I've reached a crucial stage of this project in which I need help with and is related to the problem stated below. Your help will be highly appreciated. $\\$ Problem Let $P_n$ be a polygon such that if I shoot a ray from some point inside the polygon at a certain angle and I let the ray bounce off the edges continuously, I'll have at least one point $p$ inside the polygon with exactly $n$ rays (with different slopes) going through such point. We can refer to such polygon as a $P_n$ -polygon. Does a $P_n$ -polygon exist for every $n\geq 3?$ And if so, is such polygon also a $P_0, P_1,...,P_{n-1}$ polygon $?$ $\\$ (A square is an example of a $P_0,P_1$ and $P_2$ -polygon depending on the position and the angle I shoot the ray from, inside the square.) $\\$ UPDATE: $\\$ Based on a comment below and after some tries, here's an example of a $P_0, P_1, P_2$ and $P_3$ -Polygon. In the first picture, if we choose $p$ as the starting point to shoot the ray from, inside the equilateral triangle, we can see that after the reflections the ray goes through $p$ from two different angles (slopes). The ray goes through point $q$ , from only one angle (slope) and it never goes through $r$ . As for the second picture we can see that using $s$ as the starting point, the ray goes through it from three different angles (slopes). Hence an equilateral triangle is an example of a $P_0, P_1, P_2$ and $P_3$ -polygon. $\\$ Whilst this is not a proof for the general case hopefully it should give some indication as to how to approach it.","['contest-math', 'euclidean-geometry', 'billiards', 'geometry', 'recreational-mathematics']"
3855311,What are some lesser-known examples where increasing the dimensionality makes the problem easier to solve? [duplicate],"This question already has answers here : What problems are easier to solve in a higher dimension, i.e. 3D vs 2D? (3 answers) Closed 3 years ago . I feel like there is a common pattern in mathematics where increasing the dimensionality makes the problem easier to solve or provides a solution where otherwise one would not exist. Some examples: Going from real numbers to complex numbers Working with quaternions (4D) instead of (roll, pitch, yaw) or rotation matrices which have singularities Laplace transform which lets us solve differential equations with algebra (ok this isn't really increasing the dimensionality, more like working in a different dimension) I'm not a mathematician but I imagine this appears in other areas. What else is there? Is there a ""method"" or systematic way for increasing the dimensionality of a problem to make it easier to solve? Bonus if there is something in the field of optimization and/or linear algebra, which I am most interested in at the moment.","['optimization', 'general-topology', 'linear-algebra']"
3855368,Show that $-1^2+3^2-5^2\mp ...+(2^n-1)^2=2^{2n-1}$,"Playing with numbers, I construct following expression. Can it be shown that $$\sum_{i=1}^{2^{n-1}}(-1)^i(2i-1)^2=2^{2n-1}$$ attempt We can construct following, using finite calculus as $(-1)^2+3^2+7^2+...+(4k-5)^2 = \binom{k}1+8\binom{k}2+32\binom{k}3\quad\quad eq(1)$ $1^2+5^2+9^2+...+(4k-3)^2 = \binom{k}1+24\binom{k}2+32\binom{k}3\quad\quad eq(2)$ Let $4k-1=2^n-1$ so we can write above claim as $eq(1)-eq(2)-1+(4k-1)^2=2^{2n-1}$ Which is equivalent to show $(2^n-1)^2-16\binom{2^{n-2}}2-1= 2^{2n-1}$ Here I'm stuck. Thanks","['elementary-number-theory', 'algebra-precalculus', 'induction']"
3855386,Bagel probability intuition; unordered vs ordered selection,"I was solving the following problem: The bagel shop serves 4 types of bagels; plain, poppy seed, asiago, and everything. If 6 bagels were randomly selected for a breakfast meeting, what is the probability that no poppy seed bagels were selected? My thought processes was; if I were to reach in and grab a bagel, there is a $\frac{3}{4}$ chance that it is not a poppy seed bagel. If I repeated this experiment 6 times, then I would have a $\left(\frac{3}{4}\right)^6 \approx 18\%$ chance of not drawing a poppy seed bagel. However, when reviewing the solution it was calculated as $$\frac{\text{Number of unordered sets not containing poppy seed}}{\text{Total number of unordered sets}}$$ or $$\frac{\binom{6+3-1}{6}}{\binom{6+4-1}{6}} \approx 33\%$$ I understand intuitively why the solution makes sense, calculating the fraction of the number of unordered sets that do not contain poppy seed, my question is Why does my intuition fail me in the first case?","['permutations', 'combinatorics', 'discrete-mathematics', 'intuition', 'probability']"
3855399,Proving $e^z$ is holomorphic,"I read somewhere that $e^z$ is holomorphic function, but I can't think of an easy way to prove that. I thought of using Cauchy Riemann equations, but that's probably overkill. Is there a simple approach to show that this function is holomorphic?",['complex-analysis']
3855401,How do you prove that $A\cap B=B\Leftrightarrow B\subseteq A$?,"How do you prove that $A\cap B=B\Leftrightarrow B\subseteq A$ ? My thought process so far was as follows: By the way, this is what the exercise looks like When is $B\subseteq A$ ( $B$ is a subset of $A$ )? if $B\subset A$ or if $B=A$ $(B\subset A) \Leftrightarrow (x\in A \; \forall x\in B)$ $(A\subset B) \Leftrightarrow (x\in B \; \forall x\in A)$ $(A=B) \Leftrightarrow (A\subset B) \land (B\subset A) \Leftrightarrow (x\in A \;\forall x\in B) \land (x\in B \;\forall x\in A)$ $(B=A) \Leftrightarrow (A=B)$ $(B\subseteq A) \Leftrightarrow (B\subset A) \lor (B=A) \Leftrightarrow (x\in A \;\forall x\in B) \lor ((x\in A \;\forall x\in B) \land (x\in B \;\forall x\in A))$ and because in general $A \lor (A \land B) \Leftrightarrow A$ : $(x\in A\;\forall x\in B) \lor ((x\in A \;\forall x\in B) \land (x\in B \;\forall x\in A))\Leftrightarrow (x\in A\;\forall x\in B)$ . But this would mean that $B\subset A\Leftrightarrow B\subseteq A$ . So I must have done something wrong here or misunderstood, I guess 😓 If I change it to $B\subsetneq A$ ( $B$ is a proper subset of $A$ ): $(B\subsetneq A) \Leftrightarrow (x\in A\;\forall x\in B) \land (\exists x\in A : x\notin B)$ $A\subsetneq B \Leftrightarrow (x\in B\;\forall x\in A) \land (\exists x\in B : x\notin A)$ $A=B \Leftrightarrow (x\in A\;\forall x\in B) \land (x\in B\;\forall x\in A)$ $B=A \Leftrightarrow A=B$ $B\subseteq A \Leftrightarrow (B\subsetneq A) \lor (B=A) \Leftrightarrow ((x\in A\;\forall x\in B) \land (\exists x\in A : x\notin B)) \lor ((x\in A\;\forall x\in B) \land (x\in B\;\forall x\in A))$ and because in general $(A \land B) \lor (A \land C) \Leftrightarrow (B \lor C) \land A$ (because of distributivity) $B\subseteq A \Leftrightarrow ((\exists x\in A : x\notin B) \lor (x\in B\;\forall x\in A)) ∧ (x\in A\;\forall x\in B)$ But is this even correct so far? And now I looked at the other part, of which I don't even fully understand the meaning $(A\cap B)=B$ : I assume that you can write it like this $x \in A\cap B = x \in B$ $\Leftrightarrow (x\in A \land x\in B) = x \in B$ And this is where I stopped, because I have no idea how to continue with this information, assuming it is even correct so far.","['elementary-set-theory', 'propositional-calculus']"
