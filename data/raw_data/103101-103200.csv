question_id,title,body,tags
1440947,Definition of annihilator is not clear,"I have a question regarding the following definition of an annihilator of a finite dimensional vector space. I think I understand the two definitions but I don't really get the link implied by the last sentence. Definition: If $A \subset V$, the annihilator of $A, A^{°}$, is the set of all $f$ in $V^*$ such that $f(a) = 0$ for all $a$ in $A$. Similarly, if $A \subset V^*$, then $$
A^{°} = \{a\in V\colon\ f(a) = 0 \text{ for all } f\in A \}.
$$ If we view $V$ as $(V^{*})^{*}$, the second definition is included in the first. So now my questions are why may we view $V$ as $(V^{*})^{*}$, I know they're equivalent by the isomorphism but this doesn't mean that they're equal, does it? And how does the first definition include the second one?","['linear-algebra', 'duality-theorems']"
1440960,Probability question using fixed steps,"I start life at $0$, I aim to make it to $1$. I can take steps of $\dfrac{1}{2^k}, k>0$, and do so with probability  $\dfrac{1}{2^k}$. What is the expected number of steps to reach $1$ or beyond. What is the probability I will land on $1$? APPENDUM: $\begin{array} {c|c}
values&expected\\
\hline
222&2\\
22N&2\\
2N2&3\\
N22&3\\
NN2&?\\
N2N&?\\
2NN&?\\
NNN&?
\end{array}$ Let $2$ be the event that we walk $\dfrac12$, and $N$ that we don't. We have $8$ outcomes, but we don't know the value of $?$, so let it be $5$ for those with one $2$, on the grounds that we will need on average $2$ more throws to 'guarantee' a $2$, and $7$ for $NNN$. So the expected value is $\dfrac{32}{8}=4$. This obviously needs some refinement.",['probability']
1440972,Evaluation of $ \lim_{n\rightarrow \infty}\left[\prod^{n}_{r=1}\left(1+\frac{n}{r}\right)^{\frac{r}{n}}\right]^{\frac{1}{n}}$,"Evaluation of $\displaystyle \lim_{n\rightarrow \infty}\left[(1+n)^{\frac{1}{n}}\cdot \left(1+\frac{n}{2}\right)^{\frac{2}{n}}\cdot \left(1+\frac{n}{3}\right)^{\frac{3}{n}}.........2\right]^{\frac{1}{n}}$ $\bf{My\; Try::}$ Let $$ y = \lim_{n\rightarrow \infty}\left[(1+n)^{\frac{1}{n}}\cdot \left(1+\frac{n}{2}\right)^{\frac{2}{n}}\cdot \left(1+\frac{n}{3}\right)^{\frac{3}{n}}.........2\right]^{\frac{1}{n}}$$ Now taking $$ \ln y = \lim_{n\rightarrow \infty}\frac{1}{n}\cdot \ln\left[(1+n)^{\frac{1}{n}}\cdot \left(1+\frac{n}{2}\right)^{\frac{2}{n}}\cdot \left(1+\frac{n}{3}\right)^{\frac{3}{n}}.........2\right]$$ So we get $$\ln y = \lim_{n\rightarrow \infty}\frac{1}{n}\left[\frac{1}{n}\cdot \ln(1+n)+\frac{2}{n}\ln\left(1+\frac{n}{2}\right)+........+\frac{n}{n}\ln\left(1+\frac{n}{n}\right)\right]$$ So $$\ln y = \lim_{n\rightarrow \infty}\frac{1}{n}\sum^{n}_{r=1}\frac{r}{n}\ln\left(1+\frac{n}{r}\right)$$
Now Convertinto Reinmann Sum of Integral So put $\displaystyle \frac{r}{n} = x\;,$ Then $\displaystyle \frac{1}{n}=dx$ and Calculate limit So we get $$\ln y = \int_{0}^{1}x\cdot \ln\left(1+\frac{1}{x}\right)dx=\int_{0}^{1}x\cdot \left[\ln(1+x)-\ln x\right]dx$$ So we get $$\ln y = \int_{0}^{1}x\cdot \ln(x+1)dx-\int_{0}^{1}x\ln xdx$$ Now after Integrate we wil get $$\ln y = \frac{1}{2}\ln 2+\frac{1}{4}-\frac{1}{2}\ln 2+\frac{1}{4} = \frac{1}{2}$$ So we get $$\ln y = \frac{1}{2}\Rightarrow y = e^{\frac{1}{2}}=\sqrt{e}$$ Can we solve it any Shorter way, If yes then plz explain here Thanks",['calculus']
1441001,"Prove $f:GL(n,\mathbb{R})\rightarrow GL(n,\mathbb{R})$ defined by $f(x):=x^{-1}$ is continuous","I have to prove that $f:GL(n,\mathbb{R})\rightarrow GL(n,\mathbb{R})$ defined by $f(x):=x^{-1}$ is continuous. What I have so far: Consider $Inc \circ f: GL(n,\mathbb{R})\rightarrow \mathbb{R}^{n^2}$, where $Inc$ is the inclusion map from $GL(n,\mathbb{R})$ into $\mathbb{R}^{n^2}$. Throughout we take the standard topology on $\mathbb{R}^{n^2}$ and the relative topology on $GL(n,\mathbb{R})$. I've managed to prove that $Inc$ is continuous, $Inc \circ f$ continuous  $\iff f$ continuous and that $GL(n,\mathbb{R})$ is open in $\mathbb{R}^{n^2}$. How can I use this information to prove that $f$ is continuous?","['continuity', 'real-analysis', 'functional-analysis', 'matrix-calculus', 'general-topology']"
1441036,Evaluate limit without L'hopital,"$$\lim_{x\to2}\dfrac{\sqrt{x^2+1}-\sqrt{2x+1}}{\sqrt{x^3-x^2}-\sqrt{x+2}}$$ Please help me evaluate this limit. I have tried rationalising it but it just can't work, I keep ending up with $0$ at the denominator... Thanks!","['calculus', 'limits']"
1441046,Structure theorem (PIDs) from Smith Normal Form,"How exactly does the structure theorem follow from Smith Normal Form? ( Wikipedia statement ) It is said that a presentation (map from relations to generators) is put into Smith Normal form. Now, I see that the Smith Normal form applies to such a presentation, but I dont see how the structure theorem follows. I must say that the term presentation is new for me, I read through Free Presentation , Group presentation and looked up the terms relators and generators which I think I understood. Smith Normal Form A linear transformation $f$ between free modules $M$ and $N$ over a PID $R$ can be represented by the $m \times n$ following matrix where $a_i | a_{i+1} \forall 1 \leq i \lt r$. $$ \begin{bmatrix} a_1 & 0 &0 &0 &... \\
0 &a_2 &0 &0 &... \\
0 &0 &a_3 &0 &... \\
...
\end{bmatrix} $$ Structure theorem for modules over PIDs For every finitely generated module $M$ over a PID $R$ there exist unique ideals $(d_1) \supset (d_2) \supset (d_3) ... (d_n)$ with
$M \cong R/(d_1) \oplus R/(d_2) \oplus ... R/(d_n)$","['abstract-algebra', 'principal-ideal-domains', 'smith-normal-form', 'modules']"
1441049,"Property of $\{0^n, 1^n, \ldots\}$ [duplicate]","This question already has answers here : Repeatedly taking differences on a polynomial yields the factorial of its degree? (2 answers) Closed 5 years ago . The differences between adjacent squares are the odd numbers: 0 1 4 9 16 25 … 1 3 5 7  9 … The differences between adjacent odd numbers are 2 = 2!  I found that this truth is more general: If the differences between adjacent powers of $n$ are written out, and the differences of those differences etc, the $(n+1)$th sequence is {n!, n!, …} Is this a well-known theorem? Does it have an easy proof?",['number-theory']
1441063,Can a surjective continuous function from the reals to the reals assume each value an even number of times?,"Suppose $f: \mathbb{R} \rightarrow \mathbb{R}$ is continuous and onto.
Is it possible for $f$ to assume each of its values an even number of times? To clarify, some values might be taken 2 times, some 4, some 6, etc., but always an even (and therefore finite) number.
I don't require that there be a value that is assumed any particular number of times. 
For example, the function might never take on any value exactly twice. Here is a closely related question with an excellent answer.","['continuity', 'calculus', 'real-analysis']"
1441072,Rudin's proof theorem 7.23,"I'm totally lost on that proof. Recall, the theorem is the following: If $(f_n)_{n \in \mathbb{N}}$ is a pointwise bounded sequence of complex functions on a countable set $E$, then $(f_n)_{n \in \mathbb{N}}$ has a subsequence $(f_{n_k})_{k \in \mathbb{N}}$ such that $(f_{n_k}(x))_{k \in \mathbb{N}}$ converges for each $x \in E$. I understand the beginning of the proof, i.e. defining $f_{1, k}$ a subsequence of functions s.t. $\lim_{k \rightarrow + \infty}f_{1, k}(x_1)$ exists. But once he defines his matrix of functions, I can't understand the reasons of the properties he mentioned. Recall: \begin{matrix}
   S_1: & f_{1, 1} & f_{1, 2} & \ldots \\
   S_2: & f_{2, 1} & f_{2, 2} & \ldots \\
   \vdots \\
   S_n: & f_{n, 1} & f_{n, 2} & \ldots \\
\end{matrix} Why $S_n$ should be a subsequence of $S_{n-1}$? I can't get the point... Thanks,",['sequences-and-series']
1441082,Find polynomial equation for a cardioid in $\mathbb{R}^2$,"We have the cardioid with equations:
$$x(\theta)=\cos\theta+\frac{1}{2}\cos(2\theta)$$
$$y(\theta)=\sin\theta+\frac{1}{2}\sin(2\theta)$$ I have to show that you can define this cardioid with a polynomial in two variables. My approach, I defined the auxiliary variables $u,v$ such that,
$$\begin{matrix}
u=\cos(2\theta)&\Rightarrow&\cos\theta =\frac{\sqrt{1+u}}{2}\\
v=\sin(2\theta)&\Rightarrow&\sin\theta =\frac{\sqrt{1+-u}}{2}
\end{matrix}$$
and since $u^2+v^2=1$, it is,
$$x^2+y^2-\frac{1}{4}=\frac{1}{2}+u\frac{\sqrt{1+u}}{2}+v\frac{\sqrt{1-u}}{2}=ux+vy$$
but I got stuck here, any tips on how can I continue?","['algebraic-geometry', 'algebraic-curves']"
1441119,Dual basis with respect to bilinear form,"Let $V, W$ be $n$-dimensional $K$-vector spaces with a non-degenerate bilinear form $(\cdot, \cdot) : V \times W \to K$. We call a basis $(\beta_1, \dots, \beta_n)$ for $W$ dual to a basis $(\alpha_1, \dots, \alpha_n)$ for $V$ with respect to $(\cdot, \cdot)$ if for all $i, j$ we have $(\alpha_i, \beta_j) = \delta_{i j}$, where $\delta_{i j}$ denotes the Kronecker delta. The above I've found is fairly standard terminology. I've seen assertions that such a dual basis always exists and it is unique, but I haven't really seen a proof. Can anyone point me in the right direction for this proof? Is it constructive?",['linear-algebra']
1441123,"Prove that $\int_0^\infty\frac{x\cos(x)-\sin(x)}{x\left({e^x}-1\right)}\,dx = \frac{\pi}2+\arg\left(\Gamma(i)\right)-\Re\left(\psi_0(i)\right)$","While I was working on this question , I've found that
$$
I=\int_0^\infty\frac{x\cos(x)-\sin(x)}{x\left({e^x}-1\right)}\,dx = \frac{\pi}2+\arg\left(\Gamma(i)\right)-\Re\left(\psi_0(i)\right),
$$ where $\arg$ is the complex argument , $\Re$ is the real part of a complex number , $\Gamma$ is the gamma function , $\psi_0$ is the digamma function . How could we prove this? Are there any more simple closed-form? A numerical approximation:
$$
I \approx -0.3962906410900101751594101405188072631361627457\dots
$$","['calculus', 'special-functions', 'closed-form', 'definite-integrals', 'integration']"
1441163,Evaluate the limit of ratio of sums of sines (without L'Hopital): $\lim_{x\to0} \frac{\sin x+\sin3x+\sin5x}{\sin2x+\sin4x+\sin6x}$,"Limit to evaluate:
$$\lim_{x \rightarrow 0} \cfrac{\sin{(x)}+\sin{(3x)}+\sin{(5x)}}{\sin{(2x)}+\sin{(4x)}+\sin{(6x)}}$$ Proposed solution:
$$
\cfrac{\sin(x)+\sin(3x)+\sin(5x)}{\sin(2x)+\sin(4x)+\sin(6x)}
\Bigg/ \cdot\ \cfrac{1/x}{1/x}\Bigg/=
\frac{\cfrac{\sin(x)}x + \cfrac{\sin(3x)}{3x} \cdot 3 + \cfrac{\sin(5x)}{5x} \cdot 5}
{\cfrac{\sin(2x)}{2x} \cdot 2 + \cfrac{\sin(4x)}{4x} \cdot 4 + \cfrac{\sin(6x)}{6x} \cdot 6}
$$ Using $\lim_{x \rightarrow 0} \frac{\sin x}x=1$, we get
$$\frac{1+1\cdot 3+1\cdot 5}{1\cdot 2+1\cdot 4+1\cdot 6} = \frac 9{12} = \frac 3 4$$ Please tell me if I am correct.","['limits-without-lhopital', 'calculus', 'limits', 'trigonometry']"
1441166,How can one distribute six dots within a semicircle in order to minimise the distance between any single point and one of the six dots?,"I am a biologist studying flight behaviour in the Manx Shearwater. For a project I am doing I am looking at the influence of wind on flight behaviour. I know my birds are within a semi-circle of radius 50km from their nest sites, but I do not know their exact positions. But knowing their position or somewhere close by is important to be able to estimate the wind vectors they are being exposed to. I am able to acquire six locations of modelled wind data from the Met Office. To make the most of this I want to choose six locations that would enable at least on of these locations to at least be representative of any possible position a bird is at within this semi-circle. So I imagine there is an optimal distribution of the 6 locations within this semi-circle that minimises the maximum distance a bird could be from any one location. I have a possible way of working out this distribution below and it would be very much appreciated if anyone could comment on the suitability of this method or come up with any other methods that would enable a solution to the problem. Thank you. Let $S$ be the unit semicircle in the plane. We want to find points $x_1$, $x_2$, $x_3$, $x_4$, $x_5$, $x_6$ in $S$ so as to minimise $\max\{\min\{d(x,x_1), ... ,d(x,x_6)\} : x \in\ S\}$.",['geometry']
1441170,Number of ways of distributing balls into boxes,"I know that the formula for counting the number of ways in which $n$ indistinguishable balls can be distributed into $k$ distinguishable boxes is $$\binom{n + k -1}{n}$$ but I am having a hard time understanding why this formula counts that. I mean, suppose we have $4$ boxes and $3$ balls, then the problem is equivalent to count the permutations of  5 vertical lines with 3 circles except that two lines have to be fixed (the first and last lines). I would appreciate if someone could help me to relate and translate this way of thinking the problem to the formula with this. Thanks in advance.","['balls-in-bins', 'combinatorics']"
1441208,Could this odd insight help explain part of the difficulty in proving the Collatz Conjecture?,"Background: Here's a crash course on the Collatz Conjecture . Basically, you take a number and if it is even you divide it by two. If a number is odd, you multiply it by three and then add one. You keep doing this until the number becomes 1. The conjecture says that performing this operation for any number eventually results in 1. A quick note on the numbers used, they must be positive non-zero integers. As pointed out by Gottfried, there are contradictions otherwise. Stating the above in mathematical form, $$n_{t+1} =
\begin{cases}
n_t/2,  & \text{if $n_t$ is even} \\
3 \cdot n_t+1, & \text{if $n_t$ is odd}
\end{cases}$$ What I found: Here's what was kind of surprising to me. If you perform $3 \cdot x+1$, where $x$ is odd, then you will always get an even number. However, ${x \over 2}$ doesn't always result in an odd number, if $x$ is even. My assumption is this, if one could figure out how many times you have to divide an even number by two to get an odd number, headway could be made in proving the conjecture. However, when I went to find $v_2(e)$, where $e$ is an even number, and $v_2(e)$ is the number of times you can divide the number by 2, I discovered a fractal like pattern!
If you plot the values of this function, you get, $$1,2,1,3,1,2,1,4,1,2,1,3,1,2,1,5,1,2,\ldots$$ The first value is $v_2(2)$. The second is $v_2(4)$, and so on. The sequence corresponds to sequence A001511 . If you define an L-system with, $$C_n \rightarrow C_n (+)^{n-1} P (-)^{n-1} C_n$$ Where $P$ is a plot move right command, $+$ is a move up command, $-$ is a move down command, and $C_n$ is the string. The up/down commands just add/substract from a counter. The plot command will actually plot the value, and move the independent variable up one. The values of $v_2(e)$ can be extracted from $C_n$ by ""reading"" $C_n$ left to right and adding $1$. The exponents, denote moving up/down $n-1$ times. Graphically, we have, Questions: Since this function $v_2(e)$ has a self-similar (fractal-like) nature, does that mean that the Collatz Problem is related to fractals? Could this be used to study the problem? Since fractals are inherently complex to compute in a linear fashion, could this explain why the conjecture is so hard to prove? I believe it explains some of the complex behavior of the iterates under the Collatz system. Addendum: Given an odd number, the next odd number in the Collatz iteration is given by, $$(1) \quad o_{t+1}={{3 \cdot o_t+1} \over {2^{v_2(3 \cdot o_t+1)}}}$$ Since any even number fed into the iteration is reduced to an odd number under the iteration, we need only consider odd numbers. Therefore, the Collatz conjecture is equivalent to the conjecture that the Dynamical System $(1)$ has one and only one fixed point and that this fixed point is $1$ and attractive. In addition, the basin of attraction for this fixed point is the entire set of positive odd numbers. Now we can finally see relevance of the self-similarity of $v_2(o_n)$. Its highly complex behavior results in unusually a complex behavior for the Dynamical System.","['collatz-conjecture', 'number-theory', 'fractals', 'dynamical-systems']"
1441249,"Regularity of PDEs, general question (Example: Kolmogorov equation)","Let us consider a differential operator $L$ and the PDE
$$Lu=0$$
with some boundary conditions (assume Neumann conditions). For example, Kolmogorov's equation
$$(1) \quad Lu(t,x) = \partial_t u(t,x) + b(t,x) \partial_x u(t,x)+ \frac{1}{2}\partial_x^2 u(t,x) =0, \quad u(T,x) = g(x), \quad t\in [0,T].$$ Then one can consider the adjoint equation, namely
$$L^\ast u =0$$
which in the example above is the Fokker-Planck equation
$$(2) \quad L^\ast u(t,x) = -\partial_t u(t,x) -\partial_x [b(t,x)u(t,x)]+\frac{1}{2} \partial_x^2 u(t,x), \quad u(0,x)=g(x), \quad t\in [0,T].$$ My question is: Whatever regularity properties you obtain for $u$ from (1), are these properties transferable to (2)?. In other words, studying (1) is equivalent to studying (2) when it comes to well-posedness and regularity of the solution? I am specially interested in this matter for this particular PDE (Kolmogorov) Thanks for any feedback or ideas!","['real-analysis', 'functional-analysis', 'partial-differential-equations']"
1441259,Find a branch of $\log{(z^2+1)}$,"I have this problem right here: Find a branch of $\log{(z^2+1)}$ that is analytic as $z=0$ and takes the value $2\pi i$ there. If I just plug in  $z=0$ and use the principal branch I would just get $0$, $\log{1}$ is $0$ and the argument is $0$? So what do i do? Can i just cut the plane at the negative real axis and define the branch as $\pi \leq \arg{z} < {3\pi}$ ? If so how do i state that mathematically?",['complex-analysis']
1441268,Absolute c0ntinuity of infinite product measure,"Suppose $\mu, \nu$ are Borel probability measures on $\mathbb{R}$ and that $\mu^\infty$ and $\nu^\infty$ are the infinite products, in the sense of the Kolmogorov extension theorem, of $\mu$ and $\nu$ with themselves respectively. Does $\mu \ll \nu$ imply $\mu^\infty \ll \nu^\infty$? If not, what additional assumptions would make this true?","['probability-theory', 'measure-theory']"
1441275,Point of maximal error in the normal approximation of the binomial distribution,"I am sorry for the long question! Thanks for taking the time reading the question and for your answers! Context: Let $B_n\sim\text{Binomial(n,p)}$ be the number of successes in $n$ Bernoulli trials of probability $p\in(0,1)$. Let $$\tilde B_n=\frac{B_n-np}{\sqrt{np(1-p)}}$$ be the standardized random variable and let $N\sim\text{N}(0,1)$ have the standardized normal distribution. Let $\epsilon_n(x)$ be the error between the cumulative distribution function of $\tilde B_n$ and $N$, i.e. $$\epsilon_n(x) = \left|\mathcal P(\tilde B_n \le x)-\mathcal P(N \le x)\right|$$ The central limit Theorem shows, that $\lim_{n\to\infty} \epsilon_n = 0$ (uniform in $x$). By doing numerical calculations I get always the result, that the supremum of $\epsilon_n$ is attained for $x\in[-1,1]$ (see below). My question: Is there a proof, that the maximal error of $\epsilon_n(x) = \left|\mathcal P(\tilde B_n \le x)-\mathcal P(N \le x)\right|$ is always attained in the interval $x\in[-1;1]$, i.e. that the point $x$ where $\left|\mathcal P(\tilde B_n \le x)-\mathcal P(N \le x)\right|$ is maximal fulfills $-1\le x\le 1$? Is this true? Some diagrams: Here is a plot of $f(x)=\mathcal P(\tilde B_n \le x)-\mathcal P(N \le x)$ for $p=0.336$ and $n=762$: Here is a plot showing the position of the maximal error, i.e the point $x$ where $\epsilon_n(x)$ is maximal. On the x-axis is the value $p\in(0,1)$. The y-axis shows the point $x$ where $\epsilon_n(x)$ is maximal in the calculation: You can see, that the maximal error is always attained for $-1\le x \le 1$. Note: I know, that because $\mathcal P(\tilde B_n \le x)$ has steps, the function $\epsilon_n$ is not continuous and thus $\sup_{x\in\mathbb R}\epsilon_n(x)$ may not be attained. But as you can see in the diagram the preimage of a sufficiently small neighborhood of $\sup_{x\in\mathbb R}\epsilon_n(x)$ lies in $[-1;1]$... This question is also related to my follow up question Normal approximation of tail probability in binomial distribution (which describes my motivations behind this question).","['central-limit-theorem', 'normal-distribution', 'reference-request', 'statistics', 'probability']"
1441282,Little o notation in central limit theorem proof,"I'm reading the CLT proof and am struggling with the following: (I'm skipping some details of the proof and only getting to the part I don't understand). The complex logarithm has $\log{(1+z)}=z+o(|z|)$ as $z\to 0$. So, for $t\in\mathbb{R}$ fixed, as $n\to\infty$, 
$$n\log{\left(1-t^2/(2n)+o(t^2/n)\right)}=-t^2/2 + o(1).$$ My question is how exactly does he get this equality. I've tried using $\log{(1+z)}=z+o(|z|)$ from above but I still can't see it. 
Any help will be appreciated!","['probability-theory', 'central-limit-theorem', 'asymptotics', 'measure-theory']"
1441288,"Inequality relating connectivity, edge connectivity and minimum degree (Whitney's theorem)","The following is a theorem given in Bella Bollobas's Modern graph theory (Springer 2002) Page-73 If $G$ is nontrivial (that is, has at least two vertices), then the parameters $\delta(G)$ , $\lambda(G)$ and $\kappa(G)$ satisfy the following inequality: $\kappa(G) \le \lambda(G) \le \delta(G)$ , where $\delta(G)$ is the minimum degree of a vertex, $\lambda(G)$ is the edge connectivity, and $\kappa(G)$ is the vertex connectivity. Proof: Indeed, if we delete all the edges incident with a vertex, the graph becomes
disconnected, so the second inequality holds. To see the other inequality, note
first that if $G$ is complete then $\kappa(G) = \lambda(G) = |G| - 1$ , and if $\lambda(G) \le 1$ then $\lambda(G) = \kappa(G)$ . Suppose now that $G$ is not complete, $\lambda(G) = k \ge 2$ , and $\{x_1 y_1, x_2 y_2, \ldots , x_k y_k\}$ is a set of edges disconnecting $G$ . If $G - \{x_1, x_2, \ldots , x_k\}$ is disconnected then $\kappa(G) \le k$ . Otherwise, each vertex $x_i$ has degree at most $k$ (and so exactly $k$ ). Deleting the neighbours of $x_1$ , we
disconnect $G$ . Hence $\kappa(G)\le \lambda(G)$ . I did not understand the following part of the proof: ""Otherwise, each vertex $x_i$ has degree at most $k$ (and so exactly $k$ )"" . Why is this true? I would really appreciate an elaborate answer to this.","['graph-connectivity', 'graph-theory', 'discrete-mathematics']"
1441292,Show that $e^{\sqrt 2}$ is irrational,"I'm trying to prove that $e^{\sqrt 2}$ is irrational. My approach:
$$
e^{\sqrt 2}+e^{-\sqrt 2}=2\sum_{k=0}^{\infty}\frac{2^k}{(2k)!}=:2s
$$
Define $s_n:=\sum_{k=0}^{n}\frac{2^k}{(2k)!}$, then:
$$
s-s_n=\sum_{k=n+1}^{\infty}\frac{2^k}{(2k)!}=\frac{2^{n+1}}{(2n+2)!}\sum_{k=0}^{\infty}\frac{2^k}{\prod_{k=1}^{2k}(2n+2+k)}\\<\frac{2^{n+1}}{(2n+2)!}\sum_{k=0}^{\infty}\frac{2^k}{(2n+3)^{2k}}=\frac{2^{n+1}}{(2n+2)!}\frac{(2n+3)^2}{(2n+3)^2-2}
$$
Now assume $s=\frac{p}{q}$ for $p,q\in\mathbb{N}$. This implies:
$$
0<\frac{p}{q}-s_n<\frac{2^{n+1}}{(2n+2)!}\frac{(2n+3)^2}{(2n+3)^2-2}\iff\\
0<p\frac{(2n)!}{2^n}-qs_n\frac{(2n)!}{2^n}<\frac{2}{(2n+1)(2n+2)}\frac{(2n+3)^2}{(2n+3)^2-2}
$$
But $\left(p\frac{(2n)!}{2^n}-qs_n\frac{(2n)!}{2^n}\right)\in\mathbb{N}$ which is a contradiction for large $n$. Thus $s$ is irrational. Can we somehow use this to prove $e^\sqrt{2}$ is irrational?","['sequences-and-series', 'irrational-numbers']"
1441316,Find $\lim\limits_{n\to\infty}\left(\frac{a_1}{a_0S_1}+\frac{a_2}{S_1S_2}+...+\frac{a_n}{S_{n-1}S_n}\right)$,"Problem: Find $\lim\limits_{n\to\infty}\left(\frac{a_1}{a_0S_1}+\frac{a_2}{S_1S_2}+...+\frac{a_n}{S_{n-1}S_n}\right)$ where $n=0,1,2,...$ and $a_n=2015^n,S_n=\sum\limits_{k=0}^{n}a_k$ $S_n$ can be written as the geometric sum $S_n=\frac{2015^{n+1}-1}{2014}$ . Applying the values for $a_k$ and $S_k$ can't give a closed form in the limit. How to transform sequence in the limit so it gives closed form (if possible)?","['sequences-and-series', 'calculus', 'limits']"
1441327,Normal approximation of tail probability in binomial distribution,"From the Berry Esseen theorem I know, that $$\sup_{x\in\mathbb R}|P(B_n \le x)-\Phi(x)|\in O\left(\frac 1{\sqrt n}\right)$$ whereby $B_n$ has the standardized binomial distribution and $N$ has the standardized normal distribution. I can prove this for $x\approx 0$ with Stirling's formula and a similar proof shown here . Unfortunately Stirling's approximation becomes worse the bigger $|x|$ is, so that I can only provide that $$\sup_{|x| \le c}|P(B_n \le x)-\Phi(x)|\in O\left(\frac 1{\sqrt n}\right)$$ for a fixed $c > 0$. My question: Is there a good proof for $\sup_{|x| > c}|P(B_n \le x)-\Phi(x)|\in O\left(\frac 1{\sqrt n}\right)$? How can I estimate the error of the difference in the tail probability of the binomial distribution to the normal distribution? Note: Chebyshev's inequality does not provide the right convergence speed. As zhoraster shows in his answer to Finding an error estimation for the De Moivre–Laplace theorem , also Chernoff's inequality does not provide the right convergence speed, too. That's why I am looking for another method. I also want to use more easy and direct approximations than Stein's method which is used in the proof of the Berry Esseen theorem. Update: I reasked this question on MO, see https://mathoverflow.net/questions/220030/normal-approximation-of-tail-probability-in-binomial-distribution","['probability-theory', 'probability-distributions', 'normal-distribution', 'statistics', 'reference-request']"
1441363,Statistics dice question,"$6$ people, $A, B, C, D, E, F$ sit in a circle to play a dice game. Rule of game : as you roll a die and get a number which is a multiple of $3$ , give the die to the person on the right(counterclockwise). If the number is not a multiple of $3$ , give it to the person on the left. What is the probability that $B$ has the die after five trials starting from $A$ ? My approach: The probability that the die would go right is $\frac 13$ and left is $\frac 23$ . Am I supposed to find all the possible outcomes and add the probabilities? For example, it could go $A-B-C-D-C-B$ . So $\frac 13\times \frac 13 \times \frac 13 \times \frac 23 \times \frac 23$ . Do I find all the paths that it could take and add all these probabilities? Or is there another way to solve this??","['dice', 'probability', 'statistics']"
1441403,How small can affine sets be?,"Quick question: let $V$ be a closed subset in the Zariski topology on $k^n$ ($k$ algebraically closed), and let $U$ be an open subset of $V$.  Is it possible to find another open subset $U_1$ of $V$ such that $U_1$ is contained in $U$, and the subringed space structure on $U_1$ is affine (that is, $U_1$ is isomorphic in the category of ringed spaces to an affine variety)?  Here I'm not requiring varieties just to be irreducible. I think the answer should be no, for example if you take $k$ in the Zariski topology, and any open set $U$ is just $k$ with finitely many points missing, I don't think you can find an open subset of $U$ that's affine (that is, I doubt any cofinite subset of $k$ besides $k$ itself is affine).",['algebraic-geometry']
1441425,Matrix calculus - computing the Hessian of a vector-Matrix equation.,"Let $\vec{b}=\langle b_1,\dots,b_n\rangle ^T$ be an n-dimensional vector of coefficients. Let $\vec{x}_1,\dots,\vec{x}_n$ be $n$ $p$-dimensional vectors. Let $G(\vec{b})=\log\det\left( \sum_{i=1}^n b_i \vec{x}_i\vec{x}_i^T\right)$. Let $A=\sum_{i=1}^n b_i \vec{x}_i\vec{x}_i^T$. If one wants to compute the $i$-th component of the gradient, we get \begin{eqnarray}
\nabla_i G(\vec{b}) &=& \text{Tr}\left(\partial_i A \right) \\
&=& \text{Tr}\left( A^{-1} \vec{x}_i\vec{x}_i^T \right) \\
&=& \text{Tr}\left(\vec{x}_i^T A^{-1} \vec{x}_i  \right) \\
&=& x_i^T A^{-1} x_i
\end{eqnarray} I am filling in the details so far of this paper (page 19, before equation (33)). So far I agree with their calculation. However, I do not understand their calculation of the line (33) and (34) in which they calculate the Hessian. They claim that 
$$
\nabla^2_{ij} (G(\vec{b})) = -(\vec{x}_i^T A^{-1}\vec{x}_j)^2.  \tag1
$$ I get something different. Using the Matrix Cookbook (equation (61)), I see that 
\begin{eqnarray}
\partial_j(\vec{x}_i^T A^{-1} \vec{x}_i) &=& -A^{-1}\vec{x}_i\vec{x}_i^T A^{-1}\cdot\partial_i(A) \tag2\\
&=& -A^{-1}\vec{x}_i\vec{x}_i^T A^{-1} \vec{x}_j\vec{x}_j^T,
\end{eqnarray}
which is a matrix and not a scalar! I know I must be making a mistake somewhere. I am still not quite comfortable with matrix calculus. Can someone help me figure out where I'm going wrong?","['multivariable-calculus', 'matrix-calculus', 'derivatives']"
1441459,Description of filter $F$ generated by a subset $E$ of $\mathcal P(W)$,"I am trying to solve exercises from the book Modal Logic by  Patrick Blackburn, Maarten de Rijke and Yde Venema. I am having a problem to solve one of the exercises in the section 2.5, the exercise is 2.5.1(b) in page 98. Please help me. Exercise :
  Let $E$ be any subset of $\mathcal P(W)$ which is the power set of $W$, and let $F$ be the filter generated by $E$. Show that $F$ is the set of all $X \in \mathcal P(W)$ such that either $X = W$ or for some $Y_1,. . . , Y_n$ with each $Y_i \in E$, $Y_1 \cap ...   \cap Y_n \subseteq X$. Now I am able to show one part of the problem. Call the set of all $X \in \mathcal P(W)$ such that either $X = W$ or for some $Y_1,. . . , Y_n$ with each $Y_i \in E$, $Y_1 \cap ...   \cap Y_n \subseteq X$ as $S$. We also know that from definition, $F=\cap \{G|G\text{ is filter over } W,E\subseteq G\}$ Now if $X\in S$ then say $X=W$ and so by definition of filter $X$ is in every filter and thus $X\in F$ Otherwise $X\neq W$ and for some $Y_1,. . . , Y_n$ with each $Y_i \in E$, $Y_1 \cap ...   \cap Y_n \subseteq X$. Take $G$ to be a filter over $W$ and $E\subseteq G$. So $Y_i\in G$ and since $G$ is filter we have $Y_1 \cap ...   \cap Y_n \in G$. But $Y_1 \cap ...   \cap Y_n \subseteq X$ means $X\in G$. hence $X\in F$ So we have $S\subseteq F$. Am I correct upto this part? And now I have stuck in the opposite direction. I am not finding a way to show this. Perhaps I am missing something or there may be some other approach to solve the problem. I am completely stuck here. Please help me to solve this problem. Thnx in advance.","['elementary-set-theory', 'filters']"
1441478,"$H^2(X,\mathbb{Z})$,$H_2(X,\mathbb{Z})$of smooth complex projective variety$X$","is there some example that: $X$ is a smooth complex projective variety, second singular cohomology $H^2(X,\mathbb{Z})$ has nontrivial torsion subgroup?","['complex-geometry', 'algebraic-geometry', 'algebraic-topology']"
1441492,Is local Lipschitz continuity sufficient for an ODE to have a unique solution?,"I have learned that for an ordinary differential equation of the form: \begin{align}
\dot{x}(t)&=f(x,t) \\
x(t_{0})&=x_{0}
\end{align}
If $\;\;f:\mathbb{R}^{n}\rightarrow{}\mathbb{R}^{n}$ is globally Lipschitz continuous on $\mathbb{R}^{n}$, then there exists a unique solution to the ODE. My question is: since only global Lipschitz continuity is sufficient for this ODE to have a unique solution, does this mean that local Lipschitz continuity is not sufficient? If this is true, can someone please provide an example where $f$ is locally, but not globally, Lipschitz continuous, and there does NOT exist a unique solution to the ODE?","['lipschitz-functions', 'ordinary-differential-equations']"
1441502,How can one distribute six dots within a semicircle in order to minimise the distance between any single point and one of the six dots?,"I am a biologist studying flight behaviour in the Manx Shearwater. For a project I am doing I am looking at the influence of wind on flight behaviour. I know my birds are within a semi-circle of radius 50km from their nest sites, but I do not know their exact positions. But knowing their position or somewhere close by is important to be able to estimate the wind vectors they are being exposed to. I am able to acquire six locations of modelled wind data from the Met Office. To make the most of this I want to choose six locations that would enable at least one of these locations to at least be representative of any possible position a bird is at within this semi-circle. So I imagine there is an optimal distribution of the 6 locations within this semi-circle that minimises the maximum distance a bird could be from any one location. I have a possible way of working out this distribution below and it would be very much appreciated if anyone could comment on the suitability of this method or come up with any other methods that would enable a solution to the problem. Thank you. Let $S$  be the unit semicircle in the plane. We want to find points $x_1  , x_2  , x_3  , x_4  , x_5  , x_6$   in $S$  so as to minimise $\max${$\min${$d(x,x_1),\ldots ,d(x,x_6 )$}:$x∈ S$} .",['geometry']
1441524,If $G$ is a $p$-group then $\Phi(G)=G'G^p$,"Okay this problem is quite the confounding one for me. If $G$ is a $p$ -group then it follows that $\Phi(G)=G'G^p$ . Where: $\Phi(G)$ - Frattini subgroup (which in this case is the intersection of all subgroups of index $p$ ) $G'$ commutator subgroup $G^p=\{x^p:x\in G\}$ I am having trouble tackling a couple sub-problems with this problem. Why is $G'G^p$ a subgroup? In general $G^p$ isn't a subgroup, so why does $G'G^p$ become a subgroup? While I understand that $G^p\subset \Phi(G)$ , why would $G'G^p\subseteq M_i$ for all $i$ where $\{M_i\}$ is a collection of subgroups of index p? Alot of my problems, center around $G^p$ not being a subgroup in general. But even if I were to prove that $G'G^p$ is a normal subgroup, I would still have to prove (2.) which would be easy if it is true that every maximal class is conjugate to one another, but I don't know that (or maybe it isn't true). Is there a way for me to see that $G'G^p$ is a normal subgroup and that it is contained in every subgroup of index $p$ .","['group-theory', 'p-groups']"
1441530,Is $(f(A))^c=f(A^c)$?,"Is $(f(A))^c=f(A^c)$ for any function? I can only prove the forward direction:
$y\in(f(A))^c\implies y\notin f(A)\implies A$ has no $x$ such that $f(x)=y \implies f^{-1}(y)\notin A \implies f^{-1}(y)\in A^c$. But we know that $y\in f(f^{-1}(y))\in f(A^c)$ hence $y\in f(A^c)$ finally. Is this proof lacking? Can anyone prove the other way. Thing is, I'm not even convinced this is true, but it says so in the slides I have.","['elementary-set-theory', 'functions']"
1441535,Locales with no points,"I'm very puzzled by the concept of a locale with no points. I understand that once one switches to the language of open sets and operations on them, points become optional: an open set may or may not have points. More puzzling are locales which cannot have points: an example of such a thing given in nLab , considers surjections $N \rightarrow R$ from natural to real numbers. This locale has no points because there are no such surjections and that's fine: looks like an empty ""something"" (an empty set is an abstraction of this sort). However this emptiness also has a bunch of sub-locales generated by pairs $(n,x): n \rightarrow x$. None of these can exist either (or rather has no elements). Formally, these descriptors do look different because a different $n$ is NOT mapped to a different $x$, but I'm not sure when this point of view becomes useful since the reason why either of these pairs fails to define a surjection is the same. Maybe this example is too boiled down? what's the context when these logical subtleties start to ""work""?",['general-topology']
1441539,"$(\partial_{tt}-\nabla^2+\partial_t)f=g,\quad (\partial_t-\nabla^2+b)g=\partial_t f$","Hi I am looking for complete solutions for $f(r,t),g(r,t)$ given in the coupled linear partial differential equations below:
$$
(\partial_{tt}-a\nabla^2+b\partial_t)f(r,t)=bg(r,t)
$$
$$
(\partial_t-c \nabla^2+b)g(r,t)=b\partial_t f(r,t)
$$
Initial and boundary conditions are given by
$$
\partial_t f(0,t)=0,\ \partial_t f(R,t)=d\cos(\omega t)
$$
$$
g(0,t)=0, \  g(R,t)=d\cos (\omega t)
$$
where $a,b,c,d,R,\omega>0.$  Note
$$
\nabla^2\equiv \frac{1}{r}\partial_r(r\partial_r)-\frac{1}{r^2}=\partial_{rr}+\frac{1}{r}\partial_r -\frac{1}{r^2}.
$$
Thank you!  Some comments: If the right hand side of both equations are zero, (equations become homogenous), then the kernel of both linear operators are known and are in terms of Bessel functions $J_1$ (we're in a cylindrical geometry hence the Laplacian like term) times oscillating functions of time: $\sin \omega t,\cos \omega t$.","['calculus', 'special-functions', 'real-analysis', 'ordinary-differential-equations', 'partial-differential-equations']"
1441545,Intuitive/heuristic explanation of Polya's urn,"Suppose we have an urn with one red ball and one blue ball. At each step, we take out a single ball from the urn and note its color; we then put that ball back into the urn, along with an additional ball of the same color. This is Polya's urn, and one of the basic facts about it is the following: the number of red balls after $n$ draws is uniform over $\{1, \ldots, n+1\}$. This is very surprising to me. While its not hard to show this by direct calculation, I wonder if anyone can give an intuitive/heuristic explanation why this distribution should be uniform. There are quite a few questions on Polya's urn on math.stackexchange, but none of them seem to be asking exactly this. The closest is this question , where there are some nice explanations for why, assuming as above that we start with one red and one blue ball, the probability of drawing a red ball at the $k$'th step is $1/2$ for every $k$ (it follows by symmetry).","['probability', 'recreational-mathematics', 'polya-urn-model']"
1441557,What are half-forms?,"Apparently, objects called half-forms exist in differential geometry. If for instance a one form could be written as $d\omega$, a half-form might be denoted $\sqrt{d\omega}$. These objects are very peculiar and I have not been able to find any real info online. Could someone give a short working-man's introduction to half-forms? How does one work with them? How are they related to one-forms? What pitfalls should one look out for? Thanks for any suggestion. EDIT I was asked in the comments to point to a source where the notion of half-forms is being used. Most recently I found them in this paper . See eq. (4.21) and the paragraph below it. The authors use these objects without making a fuss about it, which indicates that this should be standard knowledge.",['differential-geometry']
1441580,Proving a limit of a multivariable function does not exist,"Theorem: If $f(x,y)$ approaches two different values as $(x,y)\to (a,b)$ along two different paths in the domain of $f$,
  then $\lim_{(x,y)\to(a,b)}f(x)$ does not exist. In class we had a function $f(x,y)$ with $(x,y) \to (0,0)$ and we showed that the limit of $f$ as $x \to 0$ was the same for all linear paths defined by $y=mx$. In contrast, by making the substitution $y=ax^2$ we showed that the limit of $f$ as $x\to 0$ was different for parabolic paths, and therefore that the limit didn't exist. My question is: How is this possible? Why can't we find some line of the form $y=mx$
  for any given parabolic path such that these paths both approach the
  limit in the same way? Why is it insufficient to prove that a limit exists just by
  substituting $y=mx$? If I remember calculus 2 correctly, we can pretty
  much approximate any function near $x=0$ with a linear function. So,
  what's going on?","['continuity', 'multivariable-calculus']"
1441593,How to find a real $3\times3$ matrix that has no cubic root,"How does one find a real $3\times3$ matrix that does not have a cubic root? If given a matrix without a cubic root, how can one prove that it does not have a cube root?","['linear-algebra', 'matrices']"
1441596,Show the sigma algebra of a countable set is generated by a partition,"A $\sigma$-algebra $F$ is said to be generated by a partition if there
    is some partition $\{B_i\}$ of $\Omega$ so that every set $A$ in $F$ is a
    union of some parts in the partition, and every such union in in $F$.
    Show that any $\sigma$-algebra on a countable set $\Omega$ is generated
    by a partition of $\Omega$. Proof: Let $\Omega$ be a countable set and $F$ be a $\sigma$-algebra on $\Omega$, then $F$ $\subset$ $2^\Omega$. Let $x\in\Omega$, consider $\cap_i A_i$ such that $x\in A_i$ for all $i$, and $A_i \in F$. Note there maybe uncountably many $A_i \in F$ such that $x \in A_i$, but since $\Omega$ is countable, $\cap_i A_i$ can be written as an intersection of sets in $F$, so $\cap A_i \in F$. I know that I need to work work with the intersections and see if the sigma algebra can or cannot distinguish $x,y \in \Omega$. But my question is how do I prove the intersections of all sets $A_i$ containing $x$ can be written as intersections of sets in $F$?","['probability-theory', 'probability', 'measure-theory']"
1441608,Is this function a solution to the ODE? My textbook insults the students that don't know....,"It's true. Given: $y^{(4)} + 4y^{(3)} + 3y = t$, determine if $y_1(t) = \frac{1}{3}$ and $y_2(t) = t^{ - 1}$ are solutions. $\frac{d(y_1)}{dt} = \frac{d\left( \frac{t}{3} \right)}{dt} = \frac{1}{3}$ and the 2nd, 3rd, and 4th derivatives and all $0$. So, $0 + 4(0) + 3(1/3) = t$ so, since $1/3$ is not equal to $1$ then this is not a solution?? Checking the answer in the student solutions manual it says ""Since $d/dt=1/3$, and the 2nd-4th derivatives are $0$...clearly, $y_1(t)$ is a solution."" Well then.  Thanks. Can anyone help me find the error?","['derivatives', 'calculus', 'ordinary-differential-equations', 'integration']"
1441614,Are there any fields with a matrix representation other than $\mathbb{C}$?,"There has been a lot of research into matrix groups, and even matrix rings, but the only field that I have heard of that has a matrix representation is $\mathbb{C}$. Are there any other fields with a matrix representation? Also, if so, what is the name of the study of matrix representations for fields?","['field-theory', 'representation-theory', 'matrices']"
1441662,Flipping $n$ coins in a circle until they are all gone,"You have $n$ coins arranged in a circle, labeled $1$ to $n$. You start at the first coin and go around. At each coin you flip it - if it lands heads you keep it, if it lands tails you remove it. Which coin is most likely to be the last coin remaining? The answer is the coin labeled $n$ is the most likely. Indeed, the last coin is twice as likely than the first and the probabilities are strictly increasing from $1$ to $n$. This is an interview question I had and I came up with a couple intuitive ways to explain it. The clearest one is that when you reach coin $k$ for the $m$th time, $k-1$ coins have had $m$ chances to disappear and $n-k+1$ coins have had $m-1$ chances to disappear (including coin $k$). When $k$ is the last coin, then every other coin has had one more chance to be removed than that coin and when $k$ is the first coin, then every other coin has had the same amount of chances to be removed than it (which is why the last coin is twice as likely). What other ways, intuitive or rigorous, can you use to explain this phenomenon? Also, can you find exact evaluations for $P(k, n)$, i.e. the probability that the $k$th coin in a circle of $n$ coins is the last remaining coin?",['probability']
1441664,Curiosity about the wedge product and the Levi-Civita symbol,"Let us use the definition of the wedge product of two vectors: $$\vec{u}\wedge\vec{v} = \vec{u}\otimes\vec{v} - \vec{v}\otimes\vec{u}$$ writing $\vec{u}$ and $\vec{v}$ in dyadic form as $\vec{u} = u_a\vec{e}^a$ and $\vec{v} = v_a\vec{e}^a$ the wedge product above yields: $$\vec{u}\wedge\vec{v} = u_av_b\vec{e}^a \otimes \vec{e}^b - u_av_b\vec{e}^b \otimes \vec{e}^a$$ $$ = u_av_b(\delta_l^a\delta_m^b - \delta_l^b\delta_m^a)\vec{e}^l \otimes \vec{e}^m$$ $$ = u_av_b\epsilon_{lm}^{ab}\vec{e}^l \otimes \vec{e}^m = u_av_b\epsilon^{ab}\epsilon_{lm}\vec{e}^l \otimes \vec{e}^m$$ Assuming all the above is correct, expanding the $a,b$ contraction above in say $\Bbb{R}^2$ does indeed give $u_av_b\epsilon^{ab} = u_1v_2 - u_2v_1$ and on the other hand the second contraction on the basis gives $\epsilon_{lm}\vec{e}^l \otimes \vec{e}^m = \vec{e}^1\otimes\vec{e}^2 - \vec{e}^2\otimes\vec{e}^1 = \vec{e}^1\wedge\vec{e}^2$. And I believe the above still holds even when the indices are in $\{1,2,3\}$ My question(s) is(are): following a similar analogy and trying to stick to as little 'new' notation as possible, extending the above to the wedge product of multiple vectors is proving problematic for me. And I cant seem to find any text that connects the wedge product to the Levi-Civita symbol intimately. So what is the definition for the product $\vec{u}\wedge\vec{v}\wedge\vec{w}$ were the vectors are in $\Bbb{R}^3$ in similar notation to above if correct?","['multilinear-algebra', 'linear-algebra', 'differential-forms', 'tensors']"
1441671,ODE: to show the solution is periodic,"Suppose $f(t)$, defined on $\Bbb R$, is periodic with period $\omega\ne0$, and that $k\ne0$ is a constant, prove: for the following differential equation
$$\frac{dx}{dt}=kx+f(t)$$
there exists one and only one solution $x(t)$, which is also a periodic function with period $\omega$. My approach: First of all, it is easy to write the solution out explicitly, suppose that $x(0)=x_0$, then
$$x(t)=e^{kt}\int_{0}^{t}f(s)e^{-ks} \, ds+x_0e^{kt}$$
satisfies the DE. To show the uniqueness, suppose both $x_1,x_2$ are solutions, let $\delta=x_1-x_2$, then subtract the two DEs and the two initial values to get
$$\frac{d\delta}{dt}=k\delta\quad\text{with}\quad \delta(0)=0$$
and hence $\delta$ has the unique solution $\delta\equiv 0$. But its' been quite tough for me to show that the solution $x(t)$ is an $\omega$-periodic function. Actually, it suffices to show $x(\omega+t)\equiv x(t)$. By letting $\phi(t)=x(\omega+t)$, it is clear that $\phi(t)$ fits the DE. If $\phi(t)$ also fits the initial value condition, namely, if $\phi(0)=x_0$, then by the subtraction technique it is clear that $\phi(t)-x(t)\equiv 0$ and hence the proof. But $\phi(0)=x_0$ means that
$$x(\omega)=e^{k\omega}\int_{0}^{\omega}f(s)e^{-ks}ds+e^{k\omega}x_0=x_0$$
which I cannot prove, because I know virtually nothing about $f(t)$ than its period. So can you help me? Thanks in advance!","['calculus', 'ordinary-differential-equations', 'integration']"
1441683,Let $A_n$ be a sequence of Events. Find the Lim Sup,"Let $(\Omega, \mathcal{F})$ be a Measurable Space Let $\{A_n\}_{n=1}^{\infty}$ be a sequence of events. Consider,$$ F_n = \liminf_k \; (A_n \cap A_k^c ) $$ Prove that $\limsup_n F_n = \emptyset $ This seems almost intuitively obvious but I cant make an arguement to show it. My reasoning, $$F_n = \{x \in \Omega : \exists  \; k_n ; \; x \in (A_n \cap A_k^c ) \; \forall k \geq k_n\}$$ If I can somehow show now that there is some $n \geq k_n$ then I would be done but I'm finding this difficult to show.","['probability-theory', 'elementary-set-theory', 'limsup-and-liminf']"
1441709,"$f,g,h$ be a holomorphic functions such that $|f(z)|+|g(z)|+|h(z)|=1$","Let $U\subseteq \mathbb{C}$ be a connected open set, and let $f,g,h:U\to\mathbb{C}$ be holomorphic functions such that $$|f(z)|+|g(z)|+|h(z)|=1$$ for all $z\in U$. How does one prove that $f,g,h$ are constant functions? Any hints would be appreciated.",['complex-analysis']
1441717,Exactness of the tensor product,considering the tensor products of abelian groups: could you tell me if (and why?) the following is true? For any free abelian group $A$ the functor $A\otimes (-)$ is exact. Thanks! [I extracted this Proposition from a step in a proof of some lectures notes that I dont understand.],"['abstract-algebra', 'tensor-products', 'abelian-groups']"
1441733,How do I evaluate $\int \frac {x+4}{ 2x+6 } dx $?,"$$\int  \frac {x+4}{ 2x+6 } dx$$ This is a problem from Khan Academy that I was reading about how to solve when I accidentally clicked next and lost the explanation. I was reading something about how there is a clever way to divide the function to make it easier to integrate. Can someone please explain this to me? No actual solution, please. I want to get it by myself.","['partial-fractions', 'calculus', 'indefinite-integrals', 'integration']"
1441747,The definition of strong continuity via joint continuity,"A semigroup $S(t)$ on a Banach space $E$ is a family of bounded linear operators $\{S(t)\}_{t\ge 0}$ with the property that $S(t)S(s)=S(t+s)$ for any $s,t\ge 0$ and that $S(0)=I$. A semigroup is furthermore called strongly continuous if the map $(x,t)\mapsto S(t)x$ is continuous.
I was told that this is equivalent of saying $t\to S(t)x$ is continuous for every $x$. How can I see the equivalence of two ways of defining strong continuity?
Could anyone expand what $(x,t)\mapsto S(t)x$ is continuous really mean? Can one show this via the usual strategy of 2-sided continuity? How could this be the same as saying  $t\to S(t)x$ is continuous for every $x$? Appreciate for any helps.","['calculus', 'real-analysis', 'semigroup-of-operators', 'multivariable-calculus']"
1441749,Does there exist a $C^1$-path path-homotopic to a rectifiable curve?,"Related: https://math.stackexchange.com/questions/1441725/winding-number-and-cauchy-integral-formula Let $G$ be an open connected subset of $\mathbb{C}$. Let $\gamma:[0,1]\rightarrow G$ be a rectifiable curve. Then, does there exist a $C^1$-curve  $\Gamma:[0,1]\rightarrow G$ such that $\gamma$ and $\Gamma$ are homotopic relative to $\{0,1\}$ in $G$?","['algebraic-topology', 'complex-analysis']"
1441769,Solve for the gradient of $\log \sum\limits_{i = 1}^{m} \exp(a_i^Tx + bi)$,"This is a standard problem in convex optimization with well known solution but I cannot seem to follow the procedure given in Boyd's CVX book pg 643 Suppose I am given $f(x) = \log \sum\limits_{i = 1}^{m} \exp(a_i^Tx + bi)$, $f: R^n \to R$, I need to find the gradient Then, by the chain rule: $Df(x) = Dg(h(x))Dh(x)= D(\log \sum\limits_{i = 1}^{m} \exp(a_i^Tx + bi))D( \sum\limits_{i = 1}^{m} \exp(a_i^Tx + bi))$ $Df(x) = \dfrac{1}{\sum\limits_{i = 1}^{m} \exp(a_i^Tx + bi)} [?? \text{what is } D( \sum\limits_{i = 1}^{m} \exp(a_i^Tx + bi))??] $ I would really appreciate if someone can show me how you would get a closed form of the expression $D( \sum\limits_{i = 1}^{m} \exp(a_i^Tx + bi))$. The final solution is $\nabla f(x) = \dfrac{1}{1^Tz}A^Tz$, where $z_i = \exp(a_i^Tx + b_i)$","['vector-analysis', 'matrix-calculus', 'convex-optimization', 'convex-analysis', 'multivariable-calculus']"
1441805,Convergence of conditional means,Suppose that $X$ and $Y$ are random variables and $\{b_n\}$ is a sequence such that  $b_n \rightarrow b$ as $n\rightarrow \infty$. Under what conditions $E[X|Y\leq b_n] \rightarrow E[X|Y \leq b]$?,"['conditional-expectation', 'probability', 'measure-theory']"
1441854,Why does the solution set for this inequality turn out this way?,"$$x^2-6x-7\ge 0$$ Steps I took: $$(x-7)(x+1)\ge 0$$ $$x-7\ge 0\quad x+1\ge0$$ $$x\ge 7\quad x\ge -1$$ When I test the solution set, I realize that it must be $x\ge 7\quad or\quad x\le -1$ What is going on here?","['quadratics', 'algebra-precalculus', 'inequality']"
1441883,How many ways to show $\sum_{n=1}^{\infty}\ln \left|1-\frac{x^2}{n^2\pi^2}\right|$ is pointwise convergence?,"$\sum_{n=1}^{\infty}\ln \left|1-\frac{x^2}{n^2\pi^2}\right|$. where $x\not= k\pi, k\in \mathbb{Z}$ is pointwise convergence. Assume $n$ is sufficiently large($\geq N$),for a fixed point $x_0$,$-\ln \left|1-\frac{x_0^2}{n^2\pi^2}\right|=\frac{x_0^2}{n^2\pi^2}+\frac{1}{2}\frac{x_0^4}{n^4\pi^4}+\cdots+\frac{1}{n}\left(\frac{x_0^2}{n^2\pi^2}\right)^n+\cdots < 
\frac{x_0^2}{n^2\pi^2}+\left(\frac{x_0^2}{n^2\pi^2}\right)^2+\left(\frac{x_0^2}{n^2\pi^2}\right)^3+\cdots+\left(\frac{x_0^2}{n^2\pi^2}\right)^n+\cdots=\frac{x_0^2}{n^2\pi^2-x_0^2}$
then $\sum_{n\geq N}\ln \left|1-\frac{x^2}{n^2\pi^2}\right| \leq \sum_{n \geq N}\frac{x_0^2}{n^2\pi^2-x_0^2} \Rightarrow 0$ Is there any other way to show this? because my textbook omits this process.Maybe there is a straightforward way to solve this.","['analysis', 'sequences-and-series']"
1441885,Determine the number of different variable names,"Let name of a variable be a string of between 1 and 65535, inclusive, where each character can be an uppercase or a lowercase letter , a dollar sign, an underscore or a digit, except that the first character must not be a digit. What is the number of different variable names possible ?
Solution Given that the first char must not be a digit, this first character can be chosen in 26(lowercase) + 26(uppercase) + 1(dollar sign) + 1(underscore) = 54 ways. All subsequent chars can be chosen in $64^{65534}$ ways. As each subsequent digit cab be chosen in (26(uppercase) + 26(lowercase) + 1(dollar sign) + 1(underscore) + 10(digits)) = 64. As there are 65534 digits left, to get the number of possible combination 64 is multiplied 65534 times = $64^{65534}$. Hence total number of possible names is: 
$$
54 * 64^{65534}
$$
Can this be the correct answer or I'm missing something ?",['combinatorics']
1441888,"If $f$ is integrable on $[0,1]$, and $\lim_{x\to 0^+}f(x)$ exists, compute $\lim_{x\to 0^{+}}x\int_x^1 \frac{f(t)}{t^2}dt$.","If $f$ is integrable on $[0,1]$, and $\lim_{x\to 0}f(x)$ exists, compute $\lim_{x\to 0^{+}}x\int_x^1 \frac{f(t)}{t^2}dt$. I'm lost about what the value is for this limit in the first place. How can I make a guess for this kind of limit?","['analysis', 'calculus', 'real-analysis']"
1441910,Words from $\text{ACCOMODATION}$,"How many 4 letter words can be formed by the letters of the word $ACCOMODATION$ ? I thought of first taking the letters $A$ , $C$ and $O$ together as separate individual units, but couldn't get any farther. PS. I know that accommodation has a double m, but the question had spelt accommodation with a single $m$ .","['combinations', 'algebra-precalculus', 'combinatorics', 'permutations']"
1441947,Order of a group with all elements as self-inverse,"It is a common result that, if all elements of a group are self-inverse, then the group is Abelian. Is it necessarily finite also?","['abstract-algebra', 'group-theory']"
1441953,A generalization of Zeeman-Gossard perspector theorem,"I found a conjecture generalizing the Zeeman Gossard theorem a year ago, but I haven't found a solution for this conjecture.  I'm an electrical engineer, I am not a mathematician. I don't know how to prove this result. Could you give a proof? Zeeman-Gossard theorem: Let $ABC$ be a triangle, the three Euler lines of the triangle formed by the Euler line and the sides, taken by pairs, of a given triangle, form a triangle homothetic and congruent with the given triangle and having the same Euler line . A generalization of Zeeman-Gossard theorem: Let $ABC$ be a triangle, Let $P_1,P_2$ be two points on the plane, the line $P_1P_2$ meets $BC, CA, AB$ at $A_0,B_0,C_0$ respectively. Let $A_1$ be a point on the plane such that $B_0A_1$ parallel to $CP_1$, $C_0A_1$ parallel to  $BP_1$. Define $B_1, C_1$ cyclically. Let $A_2$ be a point on the plane such that $B_0A_2$ is parallel to $CP_2$, $C_0A_2$ is parallel to  $BP_2$. Define $B_2, C_2$ cyclically. The triangle formed by the three lines  $A_1A_2,B_1B_2,C_1C_2$ is homothetic and congruent to $ABC$, the homothety center lies on $P_1P_2$. When $P_1P_2$ is parallel to the Euler line, this problem is the Zee-Man Gossard theorem. Another conjecture (Concurrence of four Newton lines) (Same notations as in the cọnjecture above): the Newton lines of four quadrilaterals bounded by four lines $AB, AC, A_1A_2, P_1P_2$; four lines $BC, BA, B_1B_2, P_1P_2$; four lines $CA, $ $CB, C_1C_2, P_1P_2$; and four lines $AB, $ $BC, CA, P_1P_2$ pass through the homothety center. 1- http://faculty.evansville.edu/ck6/tcenters/recent/gosspersp.html 2- https://groups.yahoo.com/neo/groups/AdvancedPlaneGeometry/conversations/messages/2643 3- http://tube.geogebra.org/material/simple/id/1645559 4- http://tube.geogebra.org/m/1645551","['euclidean-geometry', 'projective-geometry', 'geometry']"
1441969,$PA^2\sin A+PB^2\sin B+PC^2\sin C$ is minimum if $P$ is the incenter.,"Let $ABC$ be a triangle and $P$ is a point in the plane of the triangle $ABC$ . If $a,b,c$ are the lengths of sides $BC,CA,AB$ opposite to angles $A,B,C$ respectively then prove that $PA^2\sin A+PB^2\sin B+PC^2\sin C$ is minimum if $P$ is the incenter. $PA^2\tan A+PB^2\tan B+PC^2\tan C$ is minimum if $P$ is the orthocenter. $PA^2+PB^2+PC^2$ is minimum if $P$ is the centroid. $PA^2\sin 2A+PB^2\sin 2B+PC^2\sin 2C$ is minimum if $P$ is the circumcenter. How do I have to formulate this expression into a form that is differentiable wrt a single variable or is there some different hidden approach will be used here. Please guide me, how should I solve this question?","['geometry', 'triangles', 'trigonometry']"
1442055,Do we conclude from these relations that $ny-hx \mid x(nx-h)$?,"We have the following relations $$p^i \mid ny-hx \\ (ny-hx)q=(nx-h)n^f \\ p^i \mid x(nx-h)$$ where $p$ is a prime, $x, y \in \mathbb{Z}$, $n>1$, $|h|<n$, $hx\geq 0$, $i>0$. Do we conclude from these relations the following? $$ny-hx \mid x(nx-h)$$ $$$$ EDIT: I am looking at the following proof: $$$$ At the part ""By the Chinese Remainder Theorem ... $ny-hx \mid x(nx-h)$."" I haven't understood how from the relations $(7)$ and $(5)$ we conclude that $ny-hx \mid x(nx-h)$. Could you explain it to me?","['abstract-algebra', 'number-theory', 'divisibility', 'elementary-number-theory']"
1442064,"integral, show identity","let $t>0$. consider the functions $$F(t)=\int_0^{\infty} e^{-tx^2}cos(x^2)\, dx,\quad G(t)=\int_0^{\infty} e^{-tx^2}sin(x^2)\, dx.$$ i want to show that $$F(t)^2-G(t)^2=\frac{\pi}{4}\frac{t}{1+t^2}=2tF(t)G(t).$$ as regards the first equality, i get \begin{split}
F(t)^2-G(t)^2
& =\int_0^{\infty} e^{-tx^2}cos(x^2)\, dx \int_0^{\infty} e^{-ty^2}cos(y^2)\, dy - \\
& \quad \int_0^{\infty} e^{-tx^2}sin(x^2)\, dx \int_0^{\infty} e^{-ty^2}sin(y^2)\, dy \\
& = \iint_0^{\infty} e^{-t(x^2+y^2)} \left(cos(x^2)cos(y^2) -sin(x^2)sin(y^2)\right)\, d\lambda(x,y) \\
& = \iint_0^{\infty} e^{-t(x^2+y^2)} cos(x^2+y^2)\, d\lambda(x,y) \\
& = \int_{]0,\infty[ \times ]0,\pi/2[} r e^{-tr^2}cos(r^2)\, d\lambda(r,\phi) \\
& = \frac{\pi}{2} \int_0^{\infty}r e^{-tr^2}cos(r^2)\, dr \\
& = \frac{\pi}{4} \frac{t}{t^2+1}.
\end{split} however, i don't know how to show the second equality. neither am i able to  explicitly evaluate $F(t)$ nor can i proceed with $$F(t)G(t)=\iint_0^{\infty}e^{-t(x^2+y^2)}cos(x^2)sin(y^2)\, d\lambda(x,y).$$ can i somehow get rid of this trigonometric stuff? merci!","['polar-coordinates', 'real-analysis', 'definite-integrals', 'trigonometry', 'integration']"
1442114,On the nature of saddle points,"Given the function: $D(x,y)=f_{xx}f_{yy}-f^2_{xy}$ If $D(a,b)<0$ then this implies $(a,b)$ is a saddle point. There are three possible ways for $D<0$: 1)If $f_{xx}f_{yy}<0$. This case is represented in the following three images:
This is a surface: We have $f_{xx}|_{(a,b)}>0$, which means that the orange curve- that results from the intersection of a plane parallel to the $xz$ plane and the surface-has positive convexity, therefore $(a,b)$ is minimum along that orange curve. We have $f_{yy}|_{(a,b)}<0$, which means that the orange curve- that results from the intersection of a plane parallel to the $yz$ plane and the surface-has negative convexity, therefore $(a,b)$ is maximum along that orange curve. Since along one curve $(a,b)$ is minimum and along another it's maximum, therefore it's a saddle point. 2) If either  $f_{xx}|_{(a,b)}$ or $f_{yy}|_{(a,b)}=0$, where $(a,b)$ is a point of inflection, therefore $D=-f^2_{xy}|_{(a,b)}<0$. This case is shown by the following image: The equation of this surface is: $f(x,y)=x^3-y^2$. The black dot is $(a,b)$. $f_{xx}|_{(a,b)}=0$ where $(a,b)$ is an inflection point along the orange curve, and $f_{yy}|_{(a,b)}<0$ which means the blue curve has negative concavity therefore $(a,b)$ is maximum along that blue curve. Therefore $(a,b)$ is a saddle point. 3)If $f_{xx}$ and $f_{yy}$ have the same signs i.e:$f_{xx}f_{yy}>0$, and $f_{xx}f_{yy}<f^2_{xy}$. I illustrate this case with the following three images: Point 3) does not make sense to me at all. If $f_{xx}f_{yy}>0$, then this implies the two orange curves -that result from the intersection between the surface on one hand and xz and yz planes(or planes parallel to them), on the other hand- have the same convexity. So according to this reasoning point 3) should imply that $(a,b)$ is an extremum not a saddle point , shouldn't it?",['multivariable-calculus']
1442133,Multivariable limit one-sided path,"After looking at this answer Problem with multivariable calculus: $\lim_{(x,y)\to (0,0)} \frac{x^3 + y^3}{x^2 + y}$ I wondered if you have a limit $$\lim_{(x,y)\to(0,0)}f(x,y)$$
And you found paths such that the limit is equal to something $0$ in this case but you take a path $y=g(x),\lim_{x\to0}g(x)=0$ and you have that $$\lim_{(x,y)\to(0^+,0^-)}f(x,g(x))=-\infty,\lim_{(x,y)\to(0^-,0^+)}f(x,g(x))=\infty$$
Does that imply the limit doesn't exist or the path we take must have two-sided limit when approaching $(0,0)$",['multivariable-calculus']
1442246,Express $\arcsin(x)$ in terms of $\arccos(x)$. Solve the equation 2 arctan x=arcsin x + arccos x,"Express $\arcsin(x)$ in terms of $\arccos(x)$. Using the same, solve the equation $$ 2\,\tan^{-1}x  = \sin^{-1} x + \cos^{-1} x $$ I'm not sure if I am on the right track, but here is what i did: 
$$\sin\left(\frac{\pi}{2}-x\right) = \cos(x)$$
$$\sin(x) = \frac{\pi}{2}-\cos(x)$$",['trigonometry']
1442258,Solve $y'=(x+y)^2$,$y'=(x+y)^2$ The equation above is in the form $y'=P(x)y^2+Q(X)y+C(x)$ which is known as Ricatti equation. I set $z=x+y$  so ${dz\over dx}={dy\over dx}+1 \implies {dy\over dx}={dz\over dx}-1 \qquad(1)  $ From the initial equation I get ${dy\over dx}=z^2  \qquad   (2)$ Finally $(1)=(2) \implies {dz\over dx}-1=z^2 \implies {1\over z^2+1}dz=dx $ If I integrate both sides with respect to x I get $atan(z)=x+c$ where c constant Then $z=tan(x+c) \implies y=tan(x+c) -x$ Is the logic above solid or am I mistaken somewhere?,['ordinary-differential-equations']
1442382,Give a counterexample to $\bigcup\limits_{t \in T} (A_t \cap B_t)= \bigcup\limits_{t \in T} A_t \cap \bigcup\limits_{t \in T} B_t$,"Let $\{A_t\}_{t \in T}$ and  $\{B_t\}_{t \in T}$  be  two non-empty indexed families of set. 
Find a counterexample to
$$\bigcup\limits_{t \in T} (A_t \cap B_t)= \bigcup\limits_{t \in T} A_t \cap  \bigcup\limits_{t \in T} B_t.$$
It is clear that $T$  must be an infinite set but I have no idea about the families $\{A_t\}_{t \in T}$ and  $\{B_t\}_{t \in T}.$","['elementary-set-theory', 'examples-counterexamples']"
1442392,"$A$ is a unitary ring with $xy=1$ implies $yx=1$, prove that if $a^2=3b^2$ and $3ab=1+4ba$, then $ab=ba$","Let $(A,+, \cdot)$ be a unitary ring with the property that if $xy=1$ for $x,y \in A$, then $yx=1$. Let $a,b \in A$ with $a^2=3b^2$ and $3ab=1+4ba$. Prove that $ab=ba$. We easily get that $(a-2b)(2a+3b)=1$, so, by hypothesis, we have that $(2a+3b)(a-2b)=1$. From here we get that $3ba=1+4ab$. Then, $3ab-4ba=1=3ba-4ab$, and so $7ab=7ba$. Now, $9ab=3+12ba$ and $12ba=4+16ab$, so $9ab=3+4+16ab$, thus $7ab=-7$. Of course, we have $7ba=-7$ too. Next, from $3ab=1+4ba$ and $3ba=1+4ab$ we get that $1+ba=3ab-3ba=-1-ab$, so $ab+ba=-2$. We can go further and write that $aba+ba^2=-2a$ and $a^2b+aba=-2a$. We get that $a^2b=ba^2$ and, analogously $ab^2=b^2a$. Now, we can easily prove that $ab=6ba+5$, $ba=6ab+5$, $2ab=5ba+3$ and $2ba=5ab+3$. Here I'm stuck.","['abstract-algebra', 'ring-theory']"
1442402,"Solve the differential equation: $dy/dx=\sqrt y,\ y(0)=0$",I am trying to solve this. But I see that this equation can not satisfy the LIPSCHITZ condition in the interval containing 0.  Can this be solved by separation of variables?,['ordinary-differential-equations']
1442414,Is the FC-center of a finitely generated group itself finitely generated?,"For a group $G$, its FC-Center $FC(G)$ is the subgroup consisting of the elements of $G$ which have a finite conjugacy class (in $G$). Let $G$ be a finitely generated group. Is $FC(G)$ necessarily finitely generated as well? In case the answer is no, what if we assume further that $G$ is finitely presented?","['group-theory', 'finitely-generated']"
1442418,Is there a 4-dimensional picture making a geometrical proof of Heron's formula,"Heron's formula states that if you have a triangle $T \subset \Bbb R^2$of sides $a,b,c$ then the hypervolume of a right-angled hyper-parallelepiped (is there a better word for this) of sides $a+b+c,a+b-c,a+c-b,b+c-a$ is $4$ times the hypervolume of $T \times T$. Is there a procedure to cut them up into a finite number of (polyedral, Banach–Tarski is not quite welcome here) pieces and rearrange them and/or add a finite number of identical pieces to both shapes, to show that they have equal hypervolume ? For comparison I'm thinking of a geometrical proof of Pythagoras' indentity with a square inscribed in a another square. I would want a procedure that's mostly independant on $a,b,c$ (requiring the triangle to be acute would be okay I guess), for example with endpoints whose coordinates are affine functions on $a,b,c$ and the coordinates of the triangles.","['volume', 'geometry', 'polyhedra']"
1442419,$f(x)=\sqrt{(1-\cos x)+\sqrt{(1-\cos x)+\sqrt{(1-\cos x)+\cdots}}}$,"Let $$f(x)=\sqrt{(1-\cos x)+\sqrt{(1-\cos x)+\sqrt{(1-\cos x)+\cdots}}}$$ and $$\phi(x)=\sqrt{x^2+\sqrt{x^2+\sqrt{x^2+\cdots}}}$$ Then find $\lim_{x\to 0}\frac{1-f(x)}{1-\phi(x)}.$ I tried to solve but got stuck. I simplified $f(x)=\frac{1\pm\sqrt{5-4\cos x}}{2}$ and $\phi(x)=\frac{1\pm\sqrt{1+4x^2}}{2}$ but when i calculated $\lim_{x\to 0}\frac{1-f(x)}{1-\phi(x)}=\frac{1\mp\sqrt6}{1\mp1}$ ,but the answer given in the book is $\frac{1}{2}$ Where have i gone wrong?Please help me.","['limits', 'functions']"
1442442,Which odd composite numbers $n$ are strong-pseudoprime to no base $a$ with $1<a<n-1$?,"In this question : A composite odd number, not being a power of $3$, is a fermat-pseudoprime to some base I did not hit my own intend. I only asked for the numbers $n$ that are not a fermat-pseudoprime to any base $a$ with $1<a<n-1$. What I really meant is, for
which composite numbers $n$, there is no number $a$ with $1<a<n-1$, such that
$n$ is strong-pseudoprime to base $a$. In short : Which odd composite numbers $n$ are strong-pseudoprime to no base $a$ with $1<a<n-1$ ? This is a weaker requirement because $a^{n-1}\equiv 1\ (\ mod\ n\ )$ does
not imply that $n$ is strong-pseudoprime to base $a$. This is the case,
if $n=2^m\ u+1$ with $u$ odd and either $a^u\equiv 1\ (\ mod\ n)$ holds
or $a^{2^ku}\equiv -1\ (\ mod\ n)$ holds for some $k$ with $0<k<m$. According to my PARI/GP-program, the numbers below $1000$ are : 9  15  21  27  33  35  39  45  51  55  57  63  69  75  77
81  87  93  95  99  105  111  115  117  119  123  129  135  141  143
147  153  155  159  161  165  171  177  183  187  189  195  201  203  207
209  213  215  219  225  235  237  243  245  249  253  255  261  267  273
275  279  285  287  291  295  297  299  303  309  315  319  321  323  327
329  333  335  339  345  351  355  357  363  369  371  375  381  387  391
393  395  399  405  407  411  413  415  417  423  429  437  441  447  453
455  459  465  471  473  477  483  489  495  497  501  507  513  515  517
519  525  527  531  535  537  539  543  549  551  555  567  573  575  579
581  583  585  591  597  603  605  609  611  615  621  623  627  633  635
639  649  655  657  663  665  667  669  675  681  687  693  695  699  705
707  711  713  717  723  729  731  735  737  741  747  749  753  755  759
765  767  771  777  779  783  789  791  795  799  801  803  807  813  815
819  825  831  833  835  837  843  849  851  855  867  869  873  875  879
885  893  895  897  899  903  909  913  915  917  921  923  927  933  935
939  943  945  951  955  957  959  963  969  975  979  981  987  989  993
995  999","['prime-numbers', 'number-theory', 'pseudoprimes']"
1442445,Show that $A\setminus (B\setminus C)=(A\setminus B)\cup(A\cap B\cap C)$,"I'm having difficulty showing this equality (assuming that the question doesn't have a typo).. I've tried in both directions and I can't seem to get what I need. \begin{align}
A\setminus (B\setminus C)&=A\cap\overline{(B\cap \overline{C})}\\
&=A\cap(\overline{B}\cup C)\\
&=(A\cap \overline{B})\cup(A\cap C)\\
&=(A\setminus B)\cup (A\cap C)
\end{align} And in the other direction (A bit more convoluted): \begin{align}
(A\setminus B)\cup (A\cap B\cap C)&=(A\cap\overline{B})\cup(A\cap B\cap C)\\
&=[A\cup (A\cap \overline{B})]\cup [B\cup (A\cap \overline{B})]\cup [C\cup (A\cap \overline{B})]\\
&=[A\cup (A\cap \overline{B})]\cup [(B\cup A)\cap(B\cup\overline{B})]\cup [C\cup (A\cap \overline{B})]\\
&=[A\cup (A\cap \overline{B})]\cup [(B\cup A)\cap\mathcal{U}]\cup [C\cup (A\cap \overline{B})]\\
&=[A\cup (A\cap \overline{B})]\cup (B\cup A)\cup [C\cup (A\cap \overline{B})]\\
&=[(A\cap \overline{B})\cup (A\cap \overline{B})]\cup(A\cup A)\cup B\cup C\\
&=(A\setminus B)\cup A\cup B\cup C
\end{align} Both of these seem to be ""close"", but neither is the same, and neither quite get to the goal. Is there a flaw in my algebra? Or is this not even possible using this method?",['elementary-set-theory']
1442447,Proofs from Ch. 1 of Arnold's ODEs,"I've started reading Vladimir Arnold's Ordinary Differential Equations on my own.  I like it so far, the only problem is that all of the exercises (as yet) are of the type ""prove $X$"" and without an instructor or TA to grade me, I have no idea if I'm proving these things correctly and rigorously.  I'm hoping you guys can let me know where I've made mistakes and where I can improve the language to sound a bit more professional. I provided relevant definitions at the bottom. Problem 2 (pg 4): Prove that a one-parameter group of transformations is a commutative group and that every mapping $g^t: M \to M$ is one-to-one. Proof: Let's start with proving that this is a commutative group: Closure: $g^rg^s=g^{r+s}$ by definition.  $r+s \in \Bbb R$, thus $g^{r+s} \in \{g^t \mid t \in \Bbb R\}$ for all $r,s \in \Bbb R$. Associativity: $(g^rg^s)g^t=g^{r+s}g^t=g^{(r+s)+t} = g^{r+(s+t)}=g^rg^{s+t}=g^r(g^sg^t)$. Identity Element: $g^0$ is the identity element because $g^tg^0 = g^{t+0} = g^t$ and $g^0g^t=g^{0+t}=g^t$. Inverse Element: Because $t\in \Bbb R$ and $\Bbb R$ is an abelian group wrt addition, there exists a $-t$ for every $t$.  Then I just show that $(g^t)^{-1}=g^{-t}$ because $g^tg^{-t} = g^{t+-t}=g^0$ and $g^{-t}g^t=g^{-t+t}=g^0$. Commutativity: $g^tg^s = g^{t+s} = g^{s+t} = g^sg^t$. Now I'll prove that every element of this group is one-to-one.  I never know if one-to-one means injective or bijective.  I'm pretty sure these functions are bijective, though, so that's what I'll try to prove. First injectivity: Because there exists an inverse element $g^{-t}$ for every mapping $g^t$ and because the image of $g^t$ is in $M$ (because the codomain is $M$), we can see that $$g^tx=g^ty \implies g^{-t}g^tx=g^{-t}g^ty \implies x=y$$ thus these functions are injective. Now surjectivity: Consider an arbitrary element $y \in M$.  I need to show that there exists an $x\in M$ s.t. $g^tx=y$.  I propose $x=g^{-t}y$ because then $g^t(g^{-t}y) = g^tg^{-t}y = g^0y=y$.  I know that $x=g^{-t}y$ can always be chosen because $g^{-t}$ exists (as proven above) and because $y\in M = \operatorname{domain}(g^{-t})$. Because $g^t$ is injective and surjective, it must be bijective.$\ \ \ \ \ \square$ Problem 3 (pg 5): Prove that there is one and only one phase curve passing through every point of phase space. Proof: To prove that there is one phase curve passing through every point of phase space means that there should exist a $t\in \Bbb R$ and an $x\in M$ s.t. $g^tx=y,\ \forall y \in M$.  But I already proved that $g^t$ is surjective on $M$ above so this is true. Next I prove that there is only one phase curve passing through every point of phase space.   I think what need to prove here is that if $y\in \{g^tx_1\}$ and $y\in \{g^tx_2\}$ then $x_2 \in \{g^tx_1\}$.  Because $y\in \{g^tx_1\}$ and $y\in \{g^tx_2\}$, there exists $t_1, t_2 \in \Bbb R$ s.t.
$$g^{t_1}x_1=y=g^{t_2}x_2$$
This implies that $x_2 = g^{-t_2}g^{t_1}x_1 = g^{t_1-t_2}x_1$. Therefore $x_2 \in \{g^tx_1\}$.$\ \ \ \ \ \square$ Problem 4 (pg 5): Prove that there is one and only one integral curve passing through every point of extended phase space. Proof: First I prove that there exists an integral curve passing through every point $(t_0,x_0)\in \Bbb R \times M$.  Consider the arbitrary integral curve $\{(t,g^tx) \in \Bbb R \times M\}$.  Because $t$ is unrestricted, this really means that there exists an $x\in M$ s.t. $g^tx=x_0$ for every $x_0\in M$.  But this just means that the function $g^t$ is surjective on $M$ which was proven above. Next I prove that there is a unique integral curve passing through every point of extended phase space.  Not only is $g^t$ surjective on $M$, it is injective.  Meaning if $g^tx=x_0$ and $g^tx' = x_0$, then $x=x'$.  Thus for every $(t_0,x_0)\in \Bbb R\times M$, there is only one $x\in M$ s.t. $g^{t_0}x=x_0$.  This then means that there is only one integral curve $\{(t,g^tx)\}$ containing the point $(t_0,x_0)$.$\ \ \ \ \ \ \square$ Problem 5 (pg 5): Prove that the horizontal line $\Bbb R \times x,\ x\in M$ is an integral curve if and only if $x$ is an equilibrium position. Proof: $$\Bbb R\times x = \{(t,g^ty)\in \Bbb R\times M \mid t\in \Bbb R\ \&\ g^ty=x\}$$
But $g^ty=x$ for all $t$ means that $g^0y=x$.  Which implies $y=x$.  Which implies $g^tx=x$.  Which implies $x$ is an equilibrium position.$\ \ \ \ \ \square$ Problem 6 (pg 5): Prove that a shift $$\begin{matrix}h^s: (\Bbb R \times M) \to (\Bbb R \times M), & h^s(t,x)=(t+s,x)\end{matrix}$$ of extended phase space along the time axis carries integral curves into integral curves. Proof: Consider an arbitrary integral curve $A=\{(t,g^tx)\}$.  The image of $A$ under $h^s$ is $$\{h^s(t,g^tx)\} = \{(t+s,g^tx\} = \{(r,g^{r-s}x)\} = \{r,g^r(g^{-s}x)\} = \{r,g^ry\}$$ for $r=s+t$ and $y=g^{-s}x$.  But this is just an integral curve.  Thus $h^s$ takes integrals curves to integral curves.$\ \ \ \ \ \square$. Definition: A family $\{g^t\}$ of mappings of a set $M$ into itself, is called a one-parameter group of transformations of $M$ if $g^{t+s}=g^tg^s$ for all $s,t \in \Bbb R$ and $g^0$ is the identity mapping. Definition: A pair $(M,\{g^t\})$ consisting of a set $M$ and a one-parameter group $\{g^t\}$ of transformations of $M$ into itself is called a phase flow . Definition: The set $M$ is called the phase space of the flow. Definition: The image of $\Bbb R$ under mapping $$\begin{matrix}\varphi: \Bbb R \to M, & \varphi(t)=g^tx\end{matrix}$$ is called a phase curve of the flow $(M, \{g^t\})$. Definition: By the extended phase space of a flow $(M, \{g^t\})$ is meant the direct product $\Bbb R\times M$ (I think Arnold is using direct product to mean Cartesian product) of the real $t$-axis and the phase space $M$. Definition: The graph of the function $$\begin{matrix}\varphi: \Bbb R \to M, & \varphi(t)=g^tx\end{matrix}$$ is called an integral curve of the flow $(M, \{g^t\})$. Definition: By an equilibrium position $x\in M$ of a flow $(M,\{g^t\})$ is meant a phase point which is itself a phase curve:
$$\begin{matrix}g^tx=x & \forall t\in\Bbb R\end{matrix}$$","['proof-verification', 'ordinary-differential-equations']"
1442453,Symmetry group of product polytope,"The symmetry group of the interval $[-1,1]$ is $\mathbb Z_2$, since it consists only of the identity and the reflection at the origin. Consider now the square $[-1,1]^2$. Obviously, its symmetry group contains the product $\mathbb Z_2 \times \mathbb Z_2$, since we reflect the square across the x-axis and the y-axis. But we can additionally rotate the square, so we recognize $\mathbb Z_4$ as an additional factor. Question: How can you calculate the symmetry group of the product of two polytopes from the symmetry group of the factors?","['polytopes', 'group-theory']"
1442458,Why in normed vector spaces we can define infinite series but in metric space we can not?,"We usually define infinite series by partial sums and an inifinite series is said to converge if its partial sum converges. So, if $X$ is a normed vector spaces and $s_n=x_1+...+x_m$ is a partial sum then infinite series converges to s if
\begin{align}
\lim_{n \to \infty}||s-s_n||=0.
\end{align} My question why can't the same be done in vector metric spaces?
We can still define converges as
\begin{align}
\lim_{n \to \infty}d(s,s_n)=0
\end{align} Certainly when a metric is induced by a norm this is not a problem. But why is it a problem in the case when metric can not be defined by a norm?","['metric-spaces', 'functional-analysis', 'normed-spaces']"
1442509,"When we have the power set $2^S$, does the 2 actually mean anything?","I have seen that most math books refer to the power set as $2^S$, usually in a cursory manner and without much detail. I was wondering if the 2 meant anything, because I normally just interpret it as a cardinality thing, like if S has two elements, then the power set has $2^2 = 4$ elements. Thanks!",['elementary-set-theory']
1442536,Can $2^{1947}\times 5+1|2^{2^{1945}}+1$ be shown by hand?,"A long tima ago, I read in a book that it would be easy to show that the
number $2^{1947}\times 5+1$ divides the Fermat number $2^{2^{1945}}+1$ I do not know, if the author meant, that it can be done by hand, or that it
can be quickly checked with a computer. I have done it with PARI/GP, and this
is actually the case. So, I wonder Can $2^{1947}\times 5+1\ |\ 2^{2^{1945}}+1$ be shown by hand ? If this would be possible, it could be proven easily that $2^{1947}\times 5+1$ is prime by hand, and this could work for even larger examples.","['prime-numbers', 'number-theory', 'divisibility']"
1442547,How to formulate $(1-x)^k(1+x)^{n-k}$ as a polynomial sum expression,"Let $n$ and $k$ be natural numbers with $0\le k\le n$. I would like to write
$$(1-x)^k(1+x)^{n-k}$$ in the form of an explicit polynomial as
$$\sum_{j=0}^nb_j(n,k)x^j$$ Is there a way to write the integer functions
$b_j(n,k)$ as an expression hopefully with the help of binomials - perhaps as a sum of them ? 20150921 edited and added : I hope to have correctly expanded and collected the double sum obtained using the binomial theorem on both factors. I obtained
$$\sum_{j=0}^n(-1)^{j}\left(\sum_{l=0}^j(-1)^l\binom{n-k}{l}\binom{k}{j-l}\right)x^j$$
The sum in the large parentheses reminds and is similar to the Vandermonde identity $$\sum_{l=0}^j\binom{x}{l}\binom{y}{j-l}=\binom{x+y}{l}$$
So my problem is now whether $$\sum_{l=0}^j(-1)^j\binom{x}{l}\binom{y}{j-l}=?$$ has a similar simplification (as a function probably) with $1$ binomial ? Additional comment Perhaps at least a solution can be found/is possible in the special case that $x=n-k$ and $y=k$.","['polynomials', 'combinatorics']"
1442592,Integration of forms on product manifolds,"Let $M$ be a compact, connected and oriented smooth manifold of dimension $m$ and let $\pi_1,\pi_2:M\times M\rightarrow M$ be the projections to each factor. Given $\alpha\in H^m(M;\mathbb{R})$, I would like to compute
$$
\int_{M\times M}\pi_1^*\alpha\smile\pi_2^*\alpha
$$
in terms of $\int_M\alpha$. My approach is as follows:
$$
\int_{M\times M}\pi_1^*\alpha\smile\pi_2^*\alpha=\int_M\alpha\smile\pi_1^!\pi_2^*\alpha=\pi_1^!\pi_2^*\alpha\int_M\alpha,
$$
where $\pi_1^!$ denotes integration along the fibre and hence $\pi_1^!\pi_2^*\alpha\in H^0(M;\mathbb{R})\simeq\mathbb{R}$ can be thought as a number. Now let $\iota:F_p\hookrightarrow M\times M$ be the inclusion of the fibre over a point $p\in M$ under the projection $\pi_1$. Then $\pi_1^!\pi_2^*\alpha=\int_{F_p}\iota^*\pi_2^*\alpha=\int_M\alpha$, where the second equality follows from the fact that $\pi_2\circ\iota$ is a diffeomorphism between $F_p$ and $M$. In conclusion I got
$$
\int_{M\times M}\pi_1^*\alpha\smile\pi_2^*\alpha=\left(\int_M\alpha\right)^2.
$$ Everything looks correct to me, but I'll appreciate a double check.","['differential-geometry', 'algebraic-topology', 'differential-forms']"
1442620,Proof writing involving power set and cartesian product: $(P(A) \times P(B)) \subseteq P(A \times B)$,"Prove: $(P(A) \times P(B)) ⊆ P(A \times B)$ Suppose $(x, y) ∈ (P(A) \times P(B))$ By Cartesian definition $x ∈ P(A)$ and $y ∈ P(B)$. Then by power set definition $x ⊆ A$ and $y ⊆ B$. So, $(x, y) ⊆ A \times B$. Therefore, $(x, y) ∈ P (A \times B)$. Is this proof correct? If so, how can I improve my proof-writing? Any constructive-criticism will be welcomed. Thanks.",['elementary-set-theory']
1442658,Derive discrete formula for priorization algorithm,"For priorized scheduling of event-driven jobs in a software system I came up with the following algorithm: Let there be $k$ priority classes, represented by the first $k$ natural numbers, a finite set of job classes $J$ and for each $j \in J$ there exists an infinite set of possible job instances $I(j)=\{j_1,j_2,\dots\}$. Define function $prio$, which maps job classes to priority classes and job instances to priority values: $prio: J\to \{1,\dots ,k\}$ $prio: I(j)\to \mathbb{N}$ For each priority class $p$, let $c_p$ be the priority counter associated with that class. These counters are initialized with $c_p:=0$ When job instance $j_i$ enters the system, set: $c_{prio(j)}:=c_{prio(j)}+prio\left(j\right)$ $prio(j_i) := c_{prio(j)}$ Job instances are scheduled in the relative order of their respective priority values. If priority values of two jobs from different classes are equal, the job with lower priority class is scheduled first. In my intuition, this should yield an algorithm where, under constant workload in all priority classes, jobs with class 1 are executed approximately twice as often as jobs with class 2 and three times as often as jobs with class 3, etc. I tried with a short example of 3 priority classes and 3 instance insertions for each class. _____|_____executions per class_____
time | class 1 | class 2 | class 3 |
-----|------------------------------
  0  |  0      |  0      |  0      |
-----|------------------------------
  1  |  1      |  0      |  0      |
-----|------------------------------
  2  |  2      |  0      |  0      |
-----|------------------------------
  3  |  2      |  1      |  0      |
-----|------------------------------
  4  |  3      |  1      |  0      |
-----|------------------------------
  5  |  3      |  1      |  1      |
-----|------------------------------
  6  |  3      |  2      |  1      | I wanted to derive a discrete formula for this, so I can programatically generate a table like the above without simulation . This would be a function of the 3 parameters $k$ (number of prio. classes), $i$ (priority class we are looking at) and $n$ (number of timesteps). I tried for a while and fiddled around with those parameters but i couldn't get to a result. I also found that I lack methodology in searching for the right solution, which is why I decided to finally sign up on stackoverflow after having lived off others questions and answers for quite some time now. I am not necessarily looking for answers with the right formula, i would love if someone could give a hint or two on how real mathematicians would tackle this problem, maybe some literature for beginners. I honestly don't even know which field of math this procedure (finding formulas for algorithms) would resort to, or where to start digging. Every single effort is very much appreciated.","['computer-science', 'algorithms', 'discrete-mathematics']"
1442665,A noetherian topological space is compact,"Have to prove that every noetherian topological space $(X,\mathcal{T})$ is also compact. I don't understand the proof I found: Let $\{\mathcal{U}_\alpha\}_{\alpha\in\Lambda}$ be an open cover of $X$, and let,
$$A=\{\bigcup^n_{i=1}\mathcal{U}_{\alpha_i}:\alpha_i\in\Lambda,n\in\mathbb{N}\}$$
since $X$ is noether and every set in $A$ is open, it follows that $A$ has a maximal element $M$ (I'm not quite sure why should it belong to $A$, since the set who has maximal element is $\mathcal{T}$). Then if there is $x\in X\setminus M$ then $x\in\mathcal{U}_\alpha$ for some $\alpha\in \Lambda$, so $M\cup\mathcal{U}_\alpha\in A$ and $M$ wouldn't be maximal. So must be $X\subset M$ and since $M$ was a finite union of elements of the cover, we have the result.","['noetherian', 'general-topology']"
1442676,How can the variance of data be represented as this product of the principal direction and the covariance matrix?,"I am coming from reading the selected answer to this question . I have a question about the following bit: It’s not hard to show that if the covariance matrix of the original data points $x_i$ was $\Sigma$ , the variance of the new data points is just $u^T \Sigma u$ . I have been playing around with projecting two dimensional data for random variables $(x_1,x_2)$ onto the horizontal axis corresponding to $x_1$ . As expected, with $u$ being in the direction of the horizontal axis, the result is $u^T \Sigma u = \operatorname{var}[x_1]$ since we are only taking into account $x_1$ . However, when setting $u$ to be in direction of the identity line, the result is $$u^T \Sigma u = \operatorname{var}[x_1] + \operatorname{var}[x_1] + 2\operatorname{cov}[x_1,x_2].$$ I don’t know much about statistics and don’t understand how this represents the variance of the data when projected onto the identity line. Is there a more formal proof for why the quoted bit is true? An intuitive explanation of the result would be appreciated as well. Edit: My question is why $u^T \Sigma u$ is the variance of the new data points as stated in the question linked above.","['eigenvalues-eigenvectors', 'covariance', 'statistics', 'matrices']"
1442771,"Does the limit $\lim_{(x,y)\to (0,0)} \frac {x^3y^2}{x^4+y^6}$ exist","$$
\lim_{(x,y)\to (0,0)} \frac {x^3y^2}{x^4+y^6} 
$$
Does this limit exist? I've tried substituting y=x^0.5 and y=x^(2/3) which both goes to 0.","['calculus', 'limits', 'multivariable-calculus']"
1442783,Even-order derivative of $y = x\sin (x)$,"How do I find the general formula for the even-order derivative of $y = x\sin (x)$? I tried using integration by parts and separation followed by mathematical induction, but I failed to obtain the correct answer, which is: $$\frac {\mathsf d^{2n}y}{\mathsf dx^{2n}} = (−1)^n(x\sin (x) − (2n\cos (x))).$$","['calculus', 'induction', 'derivatives']"
1442853,Level curves of powers of a sphere,"If we have $f(r) = r^n$, where $r^n = (x^2+y^2+z^2)^{n/2}$, and one wishes to examine the level curves of $f(r)$, namely $f(r) = k$ for some real number $k$, then one might proceed as follows: there would be two cases - (i) take $n>0$ and (ii) $n<0$. Now, for case (i), taking $n=1$, we'll have the upper part of a sphere in 3-space of radius $\sqrt{k}$. Taking $n=2$, we'll obtain a whole sphere of radius $\sqrt{k}$. But what about taking $n > 2$? That's where I'm getting lost. What kind of a surface are we going to get? For case (ii), it becomes even more complicated. For example, for $n=-2$, we'll have something like a ""reversed sphere"". The question is that we can actually view these surfaces in two ways: (1) For example, for $n=5$, we can arrive at $x^2+y^2+z^2=k^{2/5}$. For $n=-2$, we can arrive at $x^2+y^2+z^2=\frac{1}{k}$, and both are just spheres in 3-space. (2) We can expand (for $n=5$) the expression $(x^2 + y^2 + z^2)^{5/2}= k$ and get a very long expression with powers of 4 and less. Which approach is ""more correct"" - (1) or (2)?","['curves', 'exponentiation', 'functions']"
1442869,Prove $\|x\|\le\sum_{i=1}^n|x_i|\le\sqrt{n}\|x\|$ for all $x\in\mathbb{R}^n$. [duplicate],"This question already has an answer here : Prove $\|x\|_1\le \sqrt n \|x\|_2$ [closed] (1 answer) Closed 8 years ago . Prove $\|x\|\le\sum_{i=1}^n|x_i|\le\sqrt{n}\|x\|$ for all $x\in\mathbb{R}^n$. I'm stumped, can't find any intuition here or where to begin. Any suggestions?","['analysis', 'linear-algebra', 'general-topology']"
1442888,Chain rule for a function of a multivariable function,"I've got confused in something which should not be too confusing. But... If we have some function  $f(r)$, where $r=\sqrt{x^2+y^2+z^2}$, then what is $f'(r) = \frac{df}{dr}$? I thought it would be this: $\frac{df}{dr} = \frac{∂r}{∂x} + \frac{∂r}{∂x} + \frac{∂r}{∂x}$, but something is not right here. What is confusing me so much is the absense of the part differentiating the variable $r$ itself. Say, if I had $f(r) = r^2$, I would say $f'(r) = 2r(\frac{∂r}{∂x} + \frac{∂r}{∂x} + \frac{∂r}{∂x})$. But $2r$ is $\frac{df}{dr}$... Please help :)",['derivatives']
1442938,Lebesgue measure and absolutely continuous,"Let f be a homeomorphism from the real line $\mathbb R$ to $\mathbb R$ which maps each null set ( its Lebsegue measure is zero) to a null set. The question is that :
Is $f$ absolutely continuous? If we strengthen the condition to be that 
$f$ is a quasisymmetric homeomorphism from $\mathbb R$ to $\mathbb R$ which maps each null set to a null set, then what is the answer? (addition: for the detail definition of a quasisymmetric map, please see https://en.wikipedia.org/wiki/Quasisymmetric_map )","['lebesgue-measure', 'absolute-continuity', 'measure-theory']"
1442989,Why Dominated Convergence Theorem is not applicable in this case?,"Suppose $\omega$ is distributed uniformly over $(0,1]$. Define random variables $$X_n:=n\mathbf{1}_{(0,1/n]}.$$
Obviously, $X_n\rightarrow X=\mathbf{0}$ and $\lim_{n}E[X_n]$ is not equal to E[X]. In this case the Dominated Convergence Theorem (DCT) is not applicable here. I was wondering how to show that we could not find an integrable random varible Y (i.e. $E[|Y|]<\infty$) such that $|X_n| < |Y| \;a.e.$ At first I think if $Y$ is integrable, then it should be essentially bounded. But afterwards I found it was not true. Then I got stuck...","['probability-theory', 'probability', 'real-analysis', 'measure-theory']"
1443010,Can we have an uncountable number of isolated points?,"Is this possible? I've been trying to think of an example or defend why not, and I'm struggling in both directions.",['real-analysis']
1443015,"Why do we say ""almost surely"" in Probability Theory?","I recently asked a question and got a great answer that involved proving that ""X is almost surely one of the roots of P"". I know (now) that ""almost surely"" means ""with probability 1"", but I've never understood why that phrase exists. When something has a probability of 1, it's going to happen, no almost about it. What's the story there?  I've tried looking this up online, but I just got more definitions, not meaningful explanations.","['probability-theory', 'terminology']"
1443026,Limits at Infinity proof,"The problem is prove the limit using definition 6, 
$$\lim_{x\rightarrow-3} \frac{1}{(x+3)^4} = \infty$$ The book gives definition 6 as: Let $f$ be a function defined on some open interval that contains $a$, except possibly at $a$ itself. Then $\lim_{x\rightarrow a} f(x) = \infty$ means that for every positive number $E$ there is a positive number $\delta$ such that $0 <|x-a| < \delta$ then $f(x) > E$. Can you please explain it step by step all the way to the answer? Thank you.","['infinity', 'calculus', 'limits']"
1443031,Schur complement condition for positive definiteness of operators,"To verify if a symmetric block matrix is positive definite, one can check the definiteness of its diagonal blocks and the Schur complement of the respective blocks. Is this also true in the infinite dimensional setting? Precisely, being $A$, $B$ and $C$ be linear bounded operators defined on a Hilbert space $\mathcal{H}$, it is true that if $C$ is invertible and $C^{-1}$ is also a bounded linear operator, then the operator block matrix $$ \begin{bmatrix} A & B^{*} \\ B & C \end{bmatrix} $$ on $\mathcal{H} \oplus \mathcal{H}$ is positive if and only if $C$ is positive and $A - B^{*} C^{-1} B$ is positive?","['schur-complement', 'functional-analysis']"
1443036,Prove that matrix multiplication and inversion is differentiable.,"Let $f:GL(n,\mathbb{R})\rightarrow GL(n,\mathbb{R})$ be defined by $f(x)=x^{-1}$ and let $g:GL(n,\mathbb{R})\times GL(n,\mathbb{R}) \rightarrow GL(n,\mathbb{R})$ be defined by $g(x,y)=xy$ where we take the relative topologies from $\mathbb{R}^{n^2}$ and $\mathbb{R}^{n^2}\times \mathbb{R}^{n^2}$ respectively. 
Prove that $f$ and $g$ are differentiable when considered as functions on open subsets of $\mathbb{R}^{n^2}$ and $\mathbb{R}^{n^2}\times \mathbb{R}^{n^2}$ respectively mapping into $\mathbb{R}^{n^2}$. So far I've managed to prove that $GL(n,\mathbb{R})$ is open in $\mathbb{R}^{n^2}$ and disconnected. I've also shown that $f$ and $g$ are continuous, but I'm not sure how to show that differentiable follows. Any help would be greatly appreciated.","['continuity', 'real-analysis', 'functional-analysis', 'general-topology', 'differential-topology']"
1443049,Show that $f$ is an epimorphism of groups if and only if $f$ is surjective as a map of groups.,"A homomorphism between groups, $f:H\to K$ is said to be an epimorphism if for any group $L$, and for any homomorphisms $u,v:K\to L$, we have $u\circ f=v\circ f$ holds if and only if $u=v$ holds. Show that $f$ is an epimorphism of groups if and only if $f$ is surjective as a map of groups. Firstly, suppose $f$ is surjective, we can know the image of $f$ is the entire $K$, so $u\circ f=v\circ f$ can imply $u=v$. Also $u=v$ can also imply $u\circ f=v\circ f$. But how can I prove another way? Can someone tell me how to prove it? Or,can someone give me some hints?","['abstract-algebra', 'group-theory', 'category-theory']"
