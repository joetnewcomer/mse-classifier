question_id,title,body,tags
807056,Constructing a Cone and its Normal Vectors in Spherical Coordinates,"I am attempting to construct a right circular cone of maximum radius $R$ and angle $\theta$ in spherical coordinates, then find the normal vector of the surface of this cone at all points. Here's what I have: $$\text{cone}(r, \theta, \phi) = 
\begin{cases} 
x & = & r\cos{\theta}\cos{\phi} \\ 
y & = & r\cos{\theta}\sin{\phi} & \\
z & = & r\sin{\theta}
\end{cases}
$$
$$
\text{Such That:}
\begin{cases}
0 \leq \phi \leq 2\pi \\
\theta = \text{constant} \\
0 \leq r \leq R
\end{cases}$$ For the normal vector, we know that the equation of a cone in cartesian coordinates is $~~x^2 + y^2 - z^2 = 0$. To find the normal vector to this surface, we take the gradient of the equation and convert it to spherical coordinates: $$\nabla(x^2 + y^2 - z^2) = ~ <2x, 2y, -2z> ~ = ~2\cdot \text{cone}(r,-\theta,\phi)$$ Is this correct? Although it may be correct, there is some part of my brain that doesn't fully grasp what I'm doing here. I think I lack a way of thinking of the construction of the normal vector geometrically. Can anyone give me some insight?","['geometry', 'multivariable-calculus']"
807060,term for a sum of diagonal and skew-symmetric matrix?,"Is there a term for a matrix that is a sum of a diagonal and a skew-symmetric matrix? One particular example of this is a 2x2 matrix of the form $$ M = \begin{bmatrix} a & b \\ -b & a \end{bmatrix} $$ where a and b are real coefficients. There's something special about this form vs. general matrices with off-diagonal coefficients, in the context of multivariable differential equations, that I can't put my finger on (something to do with the symmetry of the eigenvalues and eigenvectors), but maybe I can figure it out if I know what search term to use. edit: magic property encountered here in the 2x2 case: the eigenvalues are $\lambda = a\pm jb$ and the eigenvectors are $v=\begin{bmatrix}1\\\pm j\end{bmatrix} $. (p.s. yes, I'm an engineer, I use $j=\sqrt{-1}$ instead of $i$)","['matrices', 'terminology']"
807062,Showing that a sequence of Picard iterates converges,"I have a sequence of functions: $$y_{n}(x) = 1 + \int \limits_0^x 1 + t^2 + y_{n-1}^2(t)\,\mathrm dt$$ With $y_0 = 1$. I'm trying to show that this converges in a box $-1 \le x \le 1$ and $-10 \le y \le 10$. However when I try to show this converges in this box, the bound on the integral gives $|y-1| \le xM$ where $M$ is the maximum of $1 + t^2 + y^2$ in the box, however this maximum is $102$ so the iterates only converge in the box when $|y-1| \le 102$ but this isn't true? Am I missing something obvious here? Thanks","['ordinary-differential-equations', 'integral-equations']"
807071,Inverse of identity operator not continuous,"Let $\mathbb 1: (C^1([0,1]), \|f\|:=\|f\|_\infty + \|f'\|_\infty)\to (C^1([0,1]),\|\cdot\|_\infty)$ denote the identity mapping between $C^1([0,1])$ with different norms. Then $f$ is linear, continuous and one-to-one, but the inverse Operator $\mathbb 1^{-1}$ is not continuous. I am trying to convince myself that the inverse operator is indeed not continuous, but I don't know how. I tried to show that it is not bounded, but I didn't really know how to proceed after writing down the definition of the operator norm for $\mathbb 1^{-1}$. How can I show that $\mathbb 1^{-1}$ is not continuous? Thanks.",['functional-analysis']
807090,Continuity and Leximin,"Consider a relation $\geq$ over the set of real-valued vectors. We say that $\geq$ is continuous if for any positive integer $n$, and any $\pi\in\mathbb Z^n,u\in\mathbb R ^n$ we have that the sets $\{v\in\mathbb R ^n:(\pi,v)\geq(\pi,u)\}$ and $\{v\in\mathbb R ^n:(\pi,v)\leq(\pi,u)\}$ are closed, where $(\pi,v)$ indicates the vector formed by appending $v$ to $\pi$. (This is kind of a weird definition, but I think it's implied by [but isn't identical to?] this one .) The Leximin ordering is one where $x\geq y$ if the smallest value of $x$ is greater than the smallest value of $y$. If the smallest values are equal, we compare the second smallest, etc. Blackorby, Bossert and Donaldson claim that the leximin ordering is not continuous. I'm struggling to understand why. For example, consider $S=\{v:v\geq (0,0)\}$. This is the set $\{x:x_i\geq 0\}$. That set seems closed to me. For example, there doesn't seem to be anything less than $(0,0)$ itself. What am I not understanding?","['general-topology', 'elementary-set-theory', 'order-theory']"
807121,Prime Number Theorem and sum of reciprocals of primes,"This is not a homework problem.  I am a mathematician (group representations and classical analysis) who never studied number theory and am beginning with Niven’s book. My question concerns the second part of a problem from Chapter $1, \S 3$ of Niven’s Introduction to Number Theory: I've done the first part.  Here it is: With $\pi(x)$ = number of primes $\leq  x,$ show that the sum of the reciprocals of primes  $\leq x$ is equal to $$\frac{\pi(x)}{x}   + \int_{2}^{x} \frac{\pi(u)}{u^{2}}du,$$ 
that is,
$$\sum_{p\leq x } \frac{1}{p}  = \frac{\pi(x)}{x}   + \int_{2}^{x} \frac{\pi(u)}{u^{2}}du $$ I am hunting for a hint on how do the second part: Use theorem $1.19$ (below) to prove that $$\limsup_{x\rightarrow \infty}~\frac{\pi (x)}{x/\log x}  \geq  1$$ Theorem $1.19$ says:
For every real $y\geq 2,$ the sum of the (reciprocals of primes  $\leq y$) $$\sum_{p\leq y} \frac{1}{p}  >  \log \log y - 1 $$ I have no idea how to begin this.  I realize that this problem is asking for what looks like a partial proof of the Prime Number Theorem, but it appears in Niven’s book far before he fully addresses the PNT.  I've tried to use MathJax, with great difficulty.","['prime-numbers', 'number-theory']"
807144,Proof that eigenvector corresponding to simple eigenvalue is continuous,"Let $\lambda$ be a simple eigenvalue of $A \in L(C^n)$ and let $x$ be the corresponding eigenvector. Then for $E \in L(C^n)$,  $A+E$ has an eigenvalue $\lambda(E)$ and an eigenvector $x(E)$ such that as $E\longrightarrow 0$,
$\lambda(E) \longrightarrow \lambda$ and $x(E) \longrightarrow x$.
This is 3.1.3 (Chapter 3, page 46) from Numerical Analysis: a second course by J. Ortega. I am using the book  for self study. The continuity of the eigenvalue is clear, but I do not understand the proof for the eigenvector bit.
Could someone help me understand what is going on ? I looked at Continuity of a simple eigenvalue and its corresponding eigenvector but that does not answer my question. I would like to have a proof that does not invoke implicit/inverse function theorem.","['numerical-linear-algebra', 'linear-algebra', 'continuity', 'eigenvalues-eigenvectors']"
807149,The closed form of $\sum_{n=1}^{\infty} \left(\frac{1}{\lfloor\sqrt{3n}\rfloor^2}-\frac{1}{3n}\right)$,"I need some ideas to exploit for finding the closed form of 
$$\sum_{n=1}^{\infty} \left(\frac{1}{\lfloor\sqrt{3n}\rfloor^2}-\frac{1}{3n}\right)$$","['sequences-and-series', 'calculus', 'real-analysis', 'number-theory']"
807156,Equivalent norm in Sobolev space,"Let $\rho\in H^{1}(0,\pi)$ be a function, and consider the functional
$$
I(\rho)=\bigg(\int_{0}^{\pi}{\sqrt{\rho^2(t)+\dot\rho^2(t)}\,dt}\bigg)^2.
$$ 
I'm asking if it is equivalent to the norm 
$$
\lVert \rho \rVert_{H^1}=\lVert \rho \rVert_{L^2}+\lVert \dot\rho \rVert_{L^2}  
$$
on $H^{1}(0,\pi)$. Obviously $I(\rho)\leq \lVert \rho \rVert_{H^1}^2$, i'm asking if the other inequalities holds.","['calculus-of-variations', 'sobolev-spaces', 'real-analysis', 'analysis', 'functional-analysis']"
807166,Eigenvalues in terms of trace and determinant for matrices larger than 2 X 2,"The eigenvalues of a $2\times2$ matrix can be expressed in terms of the trace and determinant. $\lambda_\pm = \frac{1}{2}\left(\textrm{tr} \pm \sqrt{\textrm{tr}^2-4\det}\right)$ Is there a similar formula for higher dimensional matrices? Approach The trace and determinant of a matrix are equal to the trace and determinant of the matrix in Jordan normal form. For a matrix in Jordan canonical form, $\textrm{tr } =\sum \lambda$ and $\det =\prod \lambda $. Substituting these latter two identities into the first results in an identity, which is encouraging. I'm not sure how to check this assumption for larger matrices. I'm not sure how generate more than two eigenvalues from the first formula. For the $3\times3$ case, the first formula seems to break down.",['linear-algebra']
807172,Swapping rows or columns of Toeplitz matrix changes sign of one eigenvalue,"Given some arbitrary Toeplitz matrix, if I swap two rows, one of the eigenvalues change its sign. For example, $$X =
\begin{bmatrix}
A & B & C \\
D & A & B \\
E & D & A
\end{bmatrix}$$ and $$Y = \begin{bmatrix}
D & A & B \\
A & B & C \\
E & D & A
\end{bmatrix}$$ have the same eigenvalues up to sign. I can see this for small examples, but how would I go about proving this? In particular, if I flip the matrix upside down or left-side-right, half (rounding down) of the eigenvalues flip sign, and the singular values are the same. (It becomes a Hankel matrix.) Numerical demonstration with MATLAB code: d = 5;
X = toeplitz(randn(d,1));
   -0.8655   -0.1765    0.7914   -1.3320   -2.3299
   -0.1765   -0.8655   -0.1765    0.7914   -1.3320
    0.7914   -0.1765   -0.8655   -0.1765    0.7914
   -1.3320    0.7914   -0.1765   -0.8655   -0.1765
   -2.3299   -1.3320    0.7914   -0.1765   -0.8655

J = flipud(eye(d));
   0     0     0     0     1
   0     0     0     1     0
   0     0     1     0     0
   0     1     0     0     0
   1     0     0     0     0

svd(X)'
    4.0897    2.0381    1.8456    0.8649    0.8198

svd(X*J)'
    4.0897    2.0381    1.8456    0.8649    0.8198

eig(X)'
   -4.0897   -2.0381   -0.8649    0.8198    1.8456

eig(X*J)'
   -4.0897   -1.8456   -0.8649    0.8198    2.0381

eig(J*X)'
   -4.0897   -1.8456   -0.8649    0.8198    2.0381 EDIT: non-symmetric toeplitz example","['matrices', 'linear-algebra', 'toeplitz-matrices', 'eigenvalues-eigenvectors']"
807176,"Compact operators, injectivity and closed range","Let $X$ be a an infinite dimensional Banach space. $A\in B(X)$ is a compact operator. If its range $\operatorname{Im}(A)$ is closed in $X$ then $A$ cannot be injective because $A:X\to \operatorname{Im}(A)$ would be a compact bijection between Banach spaces and the unit ball $B_X=A^{-1}AB_X$ would be compact. Now if $A$ is not injective, can we say that $\operatorname{Im}(A)$ must be closed ? Or if this is false, can we find a non injective compact operator with non closed range (i.e. infinite dimensional range) ?","['operator-theory', 'functional-analysis', 'banach-spaces']"
807182,Proving Injectivity $x + \sin(x)$,"I'm trying to prove injectivity of a particular function (without calculus), but I've come across a bit of a problem. The function is: $$f(x) = x+\sin(x)$$ I started by (abiding by common standards) assuming the following: For any arbitrary $x_1$ and $x_2$, if $f(x_1) = f(x_2)$, $x_1=x_2$ So we begin: 1. $\quad x_1 + \sin(x_1) = x_2 + \sin(x_2)$ 2. $\quad(x_1 - x_2)+(\sin(x_1)-\sin(x_2))=0$ 3. $\quad x_1-x_2 =-2\cos\left(\dfrac{x_1+x_2}{2}\right)\sin\left(\dfrac{x_1-x_2}{2}\right)$ 4. $\quad\dfrac{x_1-x_2}{2}=-\cos\left(\dfrac{x_1+x_2}{2}\right)\sin\left(\dfrac{x_1-x_2}{2}\right)$ I'm stuck at step 4. I know that, of course, if $x_1=x_2$, we get a true statement for all possible $x_1$ and $x_2$, but I'm trying to get to that fact (not just assume it). Is there any way to break this (step 4) down further, or should I go down a completely different route? I'm really just requesting a bit of guidance. P.s : I also tried assuming that there existed an inverse function of the form $$f^{-1}(x) = c_0 + c_1x + c_2x^2 + \cdots,$$ plugging that in, and finding proper coefficients, but that didn't work out so well.","['trigonometry', 'algebra-precalculus', 'functions']"
807200,Compute $ \lim_{n\to \infty}\prod_{i=1}^n B(p_i^{-2})$,"Let $B(x) = \begin{pmatrix} 1 & x \\x & 1 \end{pmatrix}$ , and $2=p_1<p_2<\cdots <p_n <\cdots$ primes number. Compute $$\displaystyle \lim_{n\to \infty}\prod_{i=1}^n B(p_i^{-2})$$ I am sorry to post this but it's the first time I see an exercice about the convergence of infinite product of matrices. I do not know how can I deal. Can someone enlighten me ?","['prime-numbers', 'matrices', 'sequences-and-series', 'limits']"
807201,Continuity of sum/product using characteristic property of product topology,"I'm self studying Lee's Introduction to Topological Manifolds , and I'm familiarising myself to the characteristic/universal property view of topology. 
One of the exercises in chapter 3 goes as follows: Suppose $\,f_1,\,f_2:X \rightarrow \mathbb{R}$ are continuous
   functions. Let 
  $$(f_1+f_2)(x) = f_1(x) + f_2(x),$$ 
  $$(f_1\cdot f_2)(x) = f_1(x)\cdot f_2(x)$$ Use the characteristic property of the product topology to show that the pontwise sums and products of continuous
  functions are continuous. Now, I'm still new to this way of dealing with things, so my attempt, which I think is a bit convoluted, was as follows: The characteristic property states that if $\prod_{\alpha \in A}X_\alpha$ has the product topology and Y is a topological space, a map $\,f:Y\rightarrow\prod_{\alpha \in A}X_\alpha$ is continuous if and only if each of its component functions $f_i = \pi_\alpha \circ f$ are continuous, where $\pi_\alpha$ is the canonical projection. The book just showed that if $\,f_1$ and $\,f_2$ are continuous, the product map $\,f_1\times f_2$ is continuous. We build a commutative diagram as follows: $$X \overset{(Id)}\rightarrow \Delta \overset{(f_1\times f_2)}\rightarrow \mathbb{R}^2 \overset{(+\, \text{or} \,\cdot)}\rightarrow \mathbb{R} \overset{(\pi = \text{Id})}\rightarrow \mathbb{R}$$
and 
$$X\overset{(f_1+f_2 \,\text{or}\, f_1 \cdot f_2)} \rightarrow \mathbb{R}$$
where $\Delta$ is the diagonal of $X^2$. The first function is a composition of continuous functions (either inclusion or sum/multiplication within the reals). By the characteristic property, the component function below must also be continuous. Is this proof correct and is there a better one?","['general-topology', 'product-space', 'universal-property']"
807228,A hard problem on exponential integration,"Suppose $a : [0 , 1] \to \Bbb R$ is an infinitely smooth function. For $\lambda\ge1$, define $$F(\lambda) := \lambda \int_0^1 e^{\lambda t} a(t) \, dt.$$
If $\sup_{\lambda\ge1}|F(\lambda)|\lt\infty$, then $a$ is the identically zero function.
Below are the some results I have derived: Derivatives of any order of $a$ vanishes at $t = 1.$ For any $\delta\lt1$, $a(t)$ has a zero in the open interval $(\delta , 1).$ 
The same is true for all the derivatives of $a(t)$. This tells us that the $n^\text{th}$ derivative of $a$ has infinitely many distinct zeros for all natural $n$. If $a$ is analytic, then I can show that $a(t)\equiv 0.$ If $a(t)\ge0$ on $[0 , 1]$, then it is obvious that $a\equiv 0.$ I would appreciate any hint. Actually $(1)$ and $(3)$ follows from $(2)$. For $(2)$, suppose that $a(1)\gt0$ (the case that $a(1)\lt0$ can be argued in the same way as the following), then there exists $\delta\lt1$ such that $a(t)$ is strictly positive on $I = [1-\delta , 1]$. Then write $F(\lambda) = \lambda\int_0^{1-\delta}e^{\lambda t}a(t)dt + \lambda\int_{1-\delta}^1e^{\lambda t}a(t)dt$. Now there exists $c\gt0$ such that $a(t)>c$ on $I$ since $I$ is compact, so that the second term is bounded below by $c (e^{\lambda}-e^{\lambda(1-\delta)})$. But the first term is only $O(e^{\lambda(1-\delta)})$, so this contradicts the fact that $F(\lambda)$ is bounded. Morever, using the same idea and integration by parts, one can see that $n^{th}$ derivative of $a$ must vanish at $1$.","['integration', 'real-analysis']"
807276,Something about $\frac{\log x}{x}$,"Denote $\log x = \log_ex$. Let's consider the below function $$\frac{\log x}{x}$$. Apparently, It's maximum is $\frac{1}{e}$. and strictly increasing in $(0,e]$, strictly decreasing in $[e,+\infty)$. If we draw a line $y=a$, where $0<a<\frac{1}{e}$. It will have two intersection point $x_1,x_2$. the question is how to prove $$x_1x_2 > e^2$$","['logarithms', 'inequality', 'analysis']"
807290,Proof of existence of primitive roots,"In my book ( Elementary Number Theory , Stillwell), exercise 3.9.1 asks to give an alternative proof of the existence of a primitive root for any prime. Let $p$ be prime, and consider the group $\mathbb{Z}/p\mathbb{Z}$. Suppose that the non-zero elements $\text{mod}\ p$ have maximum order $n < p - 1$. Show that this implies $x^n \equiv 1 \ (\text{mod}\ p)$ for all the $p - 1$ non-zero values of $x$, $\text{mod}\ p$, contrary to Lagrange's polynomial congruence theorem . What I've considered so far is that all non-zero elements of the group $\mathbb{Z}/p\mathbb{Z}$ generate subgroups of order $k \leq n < p - 1$, such that $k \mid p - 1$ (by Lagrange's theorem for groups). Showing that $k \mid n$ eludes me however. Any further ideas?","['primitive-roots', 'elementary-number-theory', 'group-theory']"
807303,Degree of map using Poincare Duality,"I have a very basic question. I want to compute the topological degree of the map $\phi: \mathbb{C}P^n \rightarrow \mathbb{C}P^n$ mapping $(z_0:\dots:z_n)$ to $(z_0^d:\dots:z_n^d)$, which, if life is not hopeless, will be $d$. I am especially interested in a proof using Poincare Duality and intersection numbers, but I appreciate other solutions as well. What I have tried (and I am especially looking for a proof along these lines if they happen to have any hope at all, which I am still wondering. But, again, others are also welcome) is as follows, but we need some backround and notation (which is a little overdone maybe, I apologize if it is hard to read) first: We have that $$H^*(\mathbb{C}P^n;\mathbb{Z})=\mathbb{Z}[\alpha]/\alpha^{n+1},$$ where $\alpha \in H^2(\mathbb{C}P^n;\mathbb{Z})$ can be taken to be the Poincare Dual of the homology class $[\mathbb{C}P^{n-1}] \in H_{2n-2}(\mathbb{C}P^n;\mathbb{Z}),$ represented by the submanifold $\mathbb{C}P^{n-1} \subseteq \mathbb{C}P^n$, and similarly $\alpha^k=PD[\mathbb{C}P^{n-k}]$, represented by $\;\mathbb{C}P^{n-k} \subseteq \mathbb{C}P^n$ (with $\mathbb{C}P^0=pt$). We have a canonical isomorphism $$H^k(\mathbb{C}P^n;\mathbb{Z}) \simeq Hom(H_k(\mathbb{C}P^n;\mathbb{Z}),\mathbb{Z})\simeq \mathbb{Z}$$ given by evaluation in the generator $[\mathbb{C}P^k]$. We have that, under this isomorphism, the element $\alpha^{k}=PD[\mathbb{C}P^{n-k}]$ is the Hom dual of $[\mathbb{C}P^k]$, since $$PD[\mathbb{C}P^{n-k}]([\mathbb{C}P^{k}])= PD[\mathbb{C}P^{n-k}]([\mathbb{C}P^{n}]\cap PD[\mathbb{C}P^{k}])=(PD[\mathbb{C}P^{k}]\cup PD[\mathbb{C}P^{n-k}])([\mathbb{C}P^{n}])=(\alpha^{n-k}\cup\alpha^k)([\mathbb{C}P^{n}])=PD[pt]([\mathbb{C}P^n])=1,$$ since this last thing is the intersection number $[pt].[\mathbb{C}P^{n}]$ of a point and the whole space (i.e the above shows that the intersection number $[\mathbb{C}P^{k}].[\mathbb{C}P^{n-k}]=1$) Now, by definition, for any map $\phi: \mathbb{C}P^n\rightarrow\mathbb{C}P^n$ we get $$\phi^*(\alpha^n)=\phi^*(\alpha)^n=\phi^*(PD[\mathbb{C}P^{n-1}])^n=deg\phi. \alpha^n$$ Also, $$\phi^*(PD[\mathbb{C}P^{n-1}])([\mathbb{C}P^{1}])=PD[\mathbb{C}P^{n-1}](\phi_*([\mathbb{C}P^1]))=$$$$=PD[\mathbb{C}P^{n-1}]([\phi(\mathbb{C}P^1)])=[\mathbb{C}P^{n-1}].[\phi(\mathbb{C}P^1)]$$ So, if the above is not rubbish, we could compute this last intersection number, and power up to the $n$ to get the degree. In the case in hand, if we take $$\mathbb{C}P^1=\{(z_0:z_1:0:\dots:0)\},\;\mathbb{C}P^{n-1}=\{(0:w_1:\dots:w_n)\}$$ we have that $$\phi(\mathbb{C}P^1)=\{(z_0^d:z_1^d:0:\dots:0)\}$$ intersects $\mathbb{C}P^{n-1}$ in the single point $(0:1:0:\dots:0)$, and here I start to wonder if I haven't messed up somewhere, for unless there is something like ""they intersect in one point but with multiplicity"" (which I don't think, since ones adds up with multiplicity $\pm 1$ over points in the intersection, and in this holomorphic context one can even drop the minus since intersections are always positive, so we literally look at its cardinal), this would imply that $[\mathbb{C}P^{n-1}].[\phi(\mathbb{C}P^1)]=1$ and so $deg\phi=1$, which is hopeless. Also, I know that in order to talk about intersection number one has to have transverse intersection (maybe that is what is failing here? I have no clue how to tell if two submanifolds given like this intersect transversely). I appreciate any help, especially if someone can point me to some reference. Thanks in advance.","['algebraic-geometry', 'algebraic-topology']"
807321,Analytical solution to a nonlinear ODE,How might I analytically solve the following differential equation? $$yy'' = y' + y^3$$ I've tried certain substitutions ($y = ux$ etc.) but none of them work.,"['ordinary-differential-equations', 'contest-math']"
807331,"Proof or find a counterexample:For all sets $A;B;C$ if $A\subseteq B,\ B\subseteq C,$ and $C\subseteq A,$ then $A=B=C.$","Proof or find a counterexample:For all sets $A;B;C$ if $A\subseteq B,\ B\subseteq C,$ and $C\subseteq A,$ then $A=B=C.$ My solution: True. Let $x\in A$, and since $A\subseteq B$
  this implies that $x\in B$
  and since $B\subseteq C$
  this implies that $x\in C$. Thus, 
$A\subseteq C$. Since $C\subseteq A$, 
then $A=C$. Then let $y\in C$
  and since $C\subseteq B$
  this implies that $y\in B$. And since $C\subseteq A$
  this implies that $y\in A$. Thus, 
$A\subseteq B$. Since $B\subseteq A$, then $A=B$. Hence this proves that $A=B=C$. Can I get feedback on my answer?","['elementary-set-theory', 'proof-verification']"
807345,Proof that $J_{\nu}(x) \sim (x/2)^\nu / \Gamma(\nu+1) \; \text{as} \; \nu \rightarrow \infty$,"I'm working through the exercises of Bender and Orszag's famous book, but I got stuck in 6.25 (a) , in which it is asked to prove that $$J_\nu (x) \sim (x/2)^\nu / \Gamma(\nu+1) \; \text{as} \; \nu \rightarrow \infty,$$ by using the following integral representation $$J_\nu(x)=\frac{(x/2)^\nu}{\sqrt{\pi}\Gamma(\nu+1/2)} \int^\pi_0 \cos(x\cos\theta) \sin^{2\nu}\theta \, d\theta,$$ which is valid for $\nu > -1/2.$ ($J_\nu(x)$ is the $\nu$th-order Bessel function of the first kind.) As the exercise belongs to section 6.4, which deals with Laplace's method and Watson's lemma, I thought I first had to perform a change of variables in order to get an integral of the form $$I(x)=\int^b_a f(t)e^{x\phi(t)} \, dt.$$ So, I took $t=\cos\theta$ and obtained $$\frac{(x/2)^\nu}{\sqrt{\pi}\Gamma(\nu+1/2)} \int^{1}_{-1} (1-t^2)^{p-\frac{1}{2}} e^{ixt} \, dt.$$ However, I cannot apply either Laplace's method or Watson's lemma, because the function $\phi$ I got is complex: $\phi(t)=it$. What am I missing?","['special-functions', 'integration', 'definite-integrals', 'asymptotics', 'analysis']"
807371,"Does $\sum_{k=0}^{k=n} {n \choose k} k!$ have a closed form for integers $k,n$?","While doing research in computer system, I came across the following summation: $$S_n = \sum_{k=0}^{n} {n \choose k} k! = \sum_{k=0}^{n} \frac{n!}{(n-k)!}$$ where both $n$ and $k$ are integers. $S_n$ has an intuitive meaning: Suppose we have $n$ distinct numbers. Each time we choose $k$ numbers and permute them. What is the total number of different permutations? Though I only need the value of $S_2 = 5$ and $S_3 = 16$ , I am interesting in its general form.  I have tried myself and googled with keywords such as ""counting permutations, Stirling numbers"", however I still have no idea. EDIT: I am quite satisfied with the accepted answer which, as one of the first answers, claims that $S_n = \lfloor n! e \rfloor$ and presents some numerical results. However, if you want to know how to prove it, please refer to the answer given by @Omran Kouba and give it an upvote.","['binomial-coefficients', 'summation', 'reference-request', 'combinatorics']"
807392,"Is it true that $f: S\to S$ be a function: $(f \circ f)$ is bijejective if, and only if $f$ is bijective?","Let $f
 :S\rightarrow S$
    be a function.  Show that $f\circ f$
    is bijective if, and only if, $f$
    is bijective. My solution. If $f\circ f$
  is bijective if, and only if, $f$
  is bijective and $f$
  is bijective if, and only if $f\circ f$
  is bijective. Suppose $f\circ f
  :S\rightarrow S$
  is bijective if and only if $f$
  is bijective. We know that $f\circ f$
  is a fuction. Suppose $x = y$, then we know that : $$f\circ f
  (x) = f\circ f
  (y)$$ $$f(f(x)) = f(f(y))$$ This implies that $f(x) = f(y)$. Hence $f$ is a function. We know that $f\circ f$
is injective. Suppose $f(x)=f(y)$. Then applying $f$ again we have $f(f(x)) = f(f(y)) \Longrightarrow
 f\circ f
  (x) = f\circ f
  (y) \Longrightarrow
 x=y$. Hence $f$ is injective. Let $y\in
 S$. We know that $f\circ f$
is surjective so there exists an $x \in
 S$. such that $f\circ f
  (x) = y$ and $f(f(x)) = y$. Since $f$ is mapped from $S$ to $S$, then $f(x)\in
 S$. Denote $f(x) = z$, then $f(z) = y$. Hence $f$ is surjective. So $f$ is a function, injective and surjective, thus $f$ is bijective which implies that $f\circ f$
  is bijective. Can anyone provide me with some feedback?","['proof-verification', 'functions']"
807426,An inequality of $L^p$ norms of linear combinations of characteristic functions of balls,"Let $1<p<\infty$. Let $(a_n)_{n=1}^\infty$ be a sequence of nonnegative real numbers and $\{B_{r_i}(x_i)\}_{i=1}^\infty$ be a sequence of open balls in $\mathbb{R}^n$. Prove that there exists $C>0$ such that 
\begin{equation*}
\Big\|\sum_i a_i\chi_{3B_i}\Big\|_p\leq C\Big\|\sum_ia_i \chi _{B_i}\Big\|_p.
\end{equation*}
Here $B_i=B_{r_i}(x_i)$, $3B_i=B_{3r_i}(x_i)$. Moreover, $C$ does not depend on the choice of $(a_n)_{n=1}^\infty$. Hint: Let $g\in L^q(\mathbb{R}^n)$ for $1/p+1/q=1$. Let 
\begin{equation*}
g^*(x)=\sup_{x\in B}\dfrac{\int_B|g| d\mu}{Vol(B)}.
\end{equation*}
Show that there exists $C_0>0$ such that 
\begin{equation*}
\int_{\mathbb{R}^n}\sum_i a_i\chi_{3b_i}(x)|g(x)|d\mu\leq C_0\int_{\mathbb{R}^n}\sum_i a_i\chi_{B_i}(x)g^*(x)d\mu.
\end{equation*} How to solve  this? How to prove the hints and how to use the hints to prove the result? I get totally lost. Thanks.","['functional-analysis', 'inequality', 'real-analysis', 'analysis', 'lp-spaces']"
807468,How prove this sum $1+\sum_{n=1}^{\infty}(1+x^n)(\frac{(1-y)(1-yx)(1-yx^2)\cdots(1-yx^{n-1})}{(y-x)(y-x^2)(y-x^3)\cdots(y-x^n)}=0$,"let $|x|<1,|y|>1$, show that
  $$1+\sum_{n=1}^{\infty}\left((1+x^n)\left(\dfrac{(1-y)(1-yx)(1-yx^2)\cdots(1-yx^{n-1})}{(y-x)(y-x^2)(y-x^3)\cdots(y-x^n)}\right)\right)=0$$ by this sum,I can't it.this problem is from a book,and The author did not give a proof. I fell this reslut is strange,maybe this condition can help me to solve this problem?
Thank you","['summation', 'calculus']"
807474,Exercise from Rotman: formal power series ring as inverse limit,"Let $A$ be a commutative ring with unit, $J = (x)$ an ideal of $A[x]$. Thus we can consider the inverse system defined as $$\psi_{n,m}: A[x]/J^m \to A[x]/J^n$$ $$g(x) + J^m \to g(x) + J^n$$ $$\forall \ m\geq n$$ Then I have to prove that $$A[[x]] \cong \underleftarrow{\lim}A[x]/J^n$$ $A[[x]]$ is a cone over this inverse system via the projections $\pi_n : A[[x]] \to A[[x]]/ J^n \ \ \forall \ n$. But how to prove that it is an universal cone ?  i.e. for every other cone $(M, \delta_n)_{n \in \mathbb{N}}$ over the inverse system there is an unique map $\lambda : M \to A[[x]]$ such that $$\pi_n \circ \lambda = \delta_n$$","['commutative-algebra', 'ring-theory', 'category-theory', 'abstract-algebra']"
807480,If $\frac{f'}{f}=\frac{g'}{g}$ then $f=cg$ for some complex constant $c$,Any ideas to solve the following problem Suppose that $f$ and $g$ are two non vanishing holmorphic functions on a domain $D$. If $\frac{f'}{f}=\frac{g'}{g}$ then $f=cg$ for some constant $c\in D$.,['complex-analysis']
807492,Study of a function and other facts,"Study the function $f:(0,+\infty) \rightarrow \mathbb{R}$ defined by: $$f(x) = \int_0^{+\infty} \arctan(t/x)e^{-t}dt \qquad (x>0)$$ Prove that $f$ is continuous, monotone and convex. Evaluate:
$$\lim_{x \rightarrow 0}f(x), \qquad \lim_{x \rightarrow +\infty}f(x), \qquad \sup_{x>0} f(x), \qquad \inf_{x>0} f(x).$$ Show that $f$ is differentiable and find $f'$. My try: I'm not sure about this, but I think that if I show that: $$f(x) = \int_0^{+\infty} \arctan(t/x)e^{-t}dt$$ is bounded and the integrating is continuous I've done. Easy to see that: $$0<\int_0^{+\infty} \arctan(t/x)e^{-t}dt<\frac{\pi}{2}e^{-t}$$ and the integrating is continuous for each $x>0$. Moreover to prove that $f$ is monotone I have to show that $f(x+1)<f(x)$ for all $x>0$ that could be easy shown by the fact that $\arctan(x)$ is strictly increasing. About the convexity I should prove first that $f$ is two times differentiable… maybe it's easier to use the definition?","['functions', 'calculus', 'integration', 'real-analysis']"
807498,A hard problem on exponential integration,"Suppose $a : [0 , 1] \to \Bbb R$ is an infinitely smooth function. For $\lambda\ge1$, define $$F(\lambda) := \lambda \int_0^1 e^{\lambda t} a(t) \, dt.$$
If $\sup_{\lambda\ge1}|F(\lambda)|\lt\infty$, then $a$ is the identically zero function.
Below are the some results I have derived: Derivatives of any order of $a$ vanishes at $t = 1.$ For any $\delta\lt1$, $a(t)$ has a zero in the open interval $(\delta , 1).$ 
The same is true for all the derivatives of $a(t)$. This tells us that the $n^\text{th}$ derivative of $a$ has infinitely many distinct zeros for all natural $n$. If $a$ is analytic, then I can show that $a(t)\equiv 0.$ If $a(t)\ge0$ on $[0 , 1]$, then it is obvious that $a\equiv 0.$ I would appreciate any hint. Actually $(1)$ and $(3)$ follows from $(2)$. For $(2)$, suppose that $a(1)\gt0$ (the case that $a(1)\lt0$ can be argued in the same way as the following), then there exists $\delta\lt1$ such that $a(t)$ is strictly positive on $I = [1-\delta , 1]$. Then write $F(\lambda) = \lambda\int_0^{1-\delta}e^{\lambda t}a(t)dt + \lambda\int_{1-\delta}^1e^{\lambda t}a(t)dt$. Now there exists $c\gt0$ such that $a(t)>c$ on $I$ since $I$ is compact, so that the second term is bounded below by $c (e^{\lambda}-e^{\lambda(1-\delta)})$. But the first term is only $O(e^{\lambda(1-\delta)})$, so this contradicts the fact that $F(\lambda)$ is bounded. Morever, using the same idea and integration by parts, one can see that $n^{th}$ derivative of $a$ must vanish at $1$.","['integration', 'real-analysis']"
807502,Proving a differential equation is linear,Prove that the following differential equation is linear: $$y(t)\frac{df(t)}{dt} - 3f(t)x(t)= 0.$$ I thought it was linear looking at it. However is there any way I can prove it? Any help would be much appreciated.,['ordinary-differential-equations']
807523,A question about the minesweeper game,"This is just out of curiosity. Suppose the game has $m \times n$ boxes for positive integers $m$ and $n$. How can we make the sum of the numbers on a finished game the most? There are two extreme cases, i.e., no mine, or each box is a mine. In these extreme cases, no number is written. Thus the sum is $0$. So I think maybe there exists a maximum number for the sum.","['graph-theory', 'discrete-optimization', 'recreational-mathematics', 'combinatorics']"
807537,Function which derivative at $0$ is $1$ but is not monotonic increasing,"Please, I need help in order to understand the following assumption that I've found in Bartle's book Introduction to Real Analysis page 171. It says:
One might suppose that, if the derivative is strictly positive at a point, then the function is increasing at this point. However, this supposition is false; indeed, the differentiable function defined by \begin{equation}
  g(x)=\begin{cases}
    x+2x^2\sin(\frac{1}{x}), & \text{if $x\neq 0$},\\
    0, & \text{$x=0$}.
  \end{cases}
\end{equation} Ok. So I've got that $g$ is continuous at zero and 
$$g'(0)=1, g'\left(\frac{1}{2n\pi}\right)=-1<0, \text{ and } g'\left(\frac{1}{(2n+1)\pi}\right)=3>0.$$ So this function has positive derivative at zero but $g$ is not increasing in any neighborhood of $x = 0$? What am I doing wrong here? So far what I know is that if a function has positive derivative at a point we can find a neighborhood of that point where the function is increasing.
Thanks for any help on this.","['calculus', 'derivatives', 'functions']"
807560,How can I get a good estimation of the following function,"The function is 
$$ f(n) = \sum_{i=1}^{n} \frac{1}{2i-1}$$
How can I compute for example $f(20)$ or $f(50)$ without using a calculator. I want to have an approximation","['approximation', 'recreational-mathematics', 'functions']"
807579,Bounded (from below) continuous local martingale is a supermartingale,"Suppose $M(t)$ is a continuous local martingale. That is, there exists a sequence of stopping times $T_n$ which almost surely increase to $\infty$, and such that $M(t\wedge T_n)$ is a martingale for all $n$. By continuous I mean that it is almost surely continuous. Suppose that $M(t)$ is almost surely bounded from below by some fixed constant $c$ (to clarify, we choose $c$ before we ""roll""). Does it follow that $M(t)$ is a supermartingale? Edit: it should probably be assumed in addition that $\mathbb{E}(M(0))<\infty$.","['probability-theory', 'martingales']"
807592,Uncountable set has uncountably many limit points. (Proof Checking Request.),"Show that any uncountable subset of the reals has uncountably many limit points. Let $S\subseteq \mathbb R$  be uncountable and let $L$ be the set of all the limit points of $S$. Assume on the contrary that $L$ is at most countable. Now.
For all $x\in S\setminus L$, there exist $a_x,b_x\in \mathbb Q$ be such that the open interval $U_x=(a_x,b_x)$ contains exactly one point of $S$. Note that for all $x,y\in S\setminus L$, we have $x\neq y\iff U_x\neq U_y$.
Write $\mathcal C=\{U_x:x\in S\setminus L\}$. Note that $\mathcal C$ is in bijection with $S\setminus L$. But $\mathcal C$ can be viewed as a subset of $\mathbb Q\times \mathbb Q$ and      hence $\mathcal C$ is countable. This forces $S$ to be countable which contradicts the hypothesis. Hence we achieve the required contradiction and the proof is complete.","['proof-verification', 'real-analysis']"
807601,Non-integral power of a singular matrix,"I know, that if $A$ is nonsingular matrix, so $\det{A} \ne 0$, then $A^p=\exp\left(p\ln A\right)$ is true for any real exponent, but what about if $A$ is singular? Then $A$ has a zero eigenvalue, so the matrix logarithm doesn't exist. Is there any extension in this case?","['matrices', 'logarithms', 'linear-algebra', 'exponential-function']"
807672,There are more functions from $T$ to $S$ than there are subsets of $T$,"Question Let $S$ be the set of stars in our galaxy and let $T$ be the set of cars on earth right now. There are more functions $f:T\rightarrow S$ than there are subsets of $T$ . Solution True. Suppose that $|S|=m$ and $|T|=n$ . To find the subsets of $|T|$ such that $|P(T)|=2^{|T|}=2^{n}$ . To find set of all functions of $f:T\rightarrow S$ , such that $|S|^{|T|}=m^{n}$ . Assume that $m^{n}>2^{n}$ but will depend on the size of $m$ as it has to be greater than $2$ . Since $m$ counts the total start in our galaxy and it is greater than $2$ . Therefore, it is True that there are more functions $f:T\rightarrow S$ than there are subsets of $T$ . Can anyone please give feedback on my answers and tell me whether the solution is correct or no.","['elementary-set-theory', 'combinatorics']"
807687,Dimension of some moduli spaces,"I expect that this is a very easy question, but somehow I can't get it.  What is the dimension of the moduli space of complete intersections of degree 2 and 4 in $\mathbb{P}^5$?  The answer should be 89. I apologize again that if the question is too easy.",['algebraic-geometry']
807694,About generator of symmetric group $S_n$,"I read this link . In Theorem $2.7 $, it is mentioned that for $n\geq 3$ except for $n = 5, 6, 8$, symmetric group $S_n$ is generated by an element of order $2$ and an element of order $3$. However,  we also know that for $n\geq 2$, $S_n$ is generated by the transposition $(1 2)$ and the $n$-cycle $(12\ldots n)$. If we use later result then $S_4$ is generated by transposition $(1 2)$ and  $4$-cycle $(1234)$ which contradicts result of Theorem $2. 7$ since order of four cycle is four. Could anybody explain me where I am going wrong? I would be very much grateful. Thanks for your time.","['permutations', 'symmetric-groups', 'group-theory', 'abstract-algebra']"
807703,a group with specific orders of elements,"I want to find a group with elements of order $1,2,3,4$ and $5$  (at least one of each order). 
All I can say is that the order of the group is $60$ itself, but cannot find the correct one. Please let me know about your solutions too.","['finite-groups', 'group-theory', 'abstract-algebra']"
807715,Elementary Proof of Bertrand's Postulate? (Proof Check),"I was working on a problem in Dummit and Foote which had to do with Euler's totient function and I ended up needing to use Bertrand's postulate for my solution. I've never seen a proof for it, so I tried to prove it myself. I finally came up with a proof, but when I went to check out other proofs I didn't see mine and the ones I saw were much more involved, leading me to believe my proof is incorrect. However, after checking it multiple times I still can't find an error. Any input would be appreciated. BP : For all $n\geq 3$ there exists a prime $p$ with $n<p<2n$ Proof. Suppose for some $n\geq3$,    $n<l<2n$ implies that $l$ is composite. Let $A= \{2, ... , n-1 \}$ and $B=\{n+1, ..., 2n-1\}$ then $\left\vert{A}\right\vert= n-2$ and $\left\vert{B}\right\vert=n-1$. Consider the function $f: B\to A$ defined by $m \mapsto m/c$, where $c$ is the smallest prime dividing $m$. If $f$ is injective we have a contradiction. Suppose $r=f(a)=f(b)$, by the FTOA $a=p_{1}^{\alpha_{1}}\cdot\ldots\cdot p_{k}^{\alpha_{k}}$ and $b=q_{1}^{\beta_{1}}\cdot\ldots\cdot q_{j}^{\beta_{j}}$ with $p_{1}<\cdots<p_{k}$ and $q_{1}<\cdots<q_{j}$ and then $f(a)=p_{1}^{\alpha_{1}-1}\cdot\ldots\cdot p_{k}^{\alpha_{k}}$ and $f(b)=q_{1}^{\beta_{1}-1}\cdot\ldots\cdot q_{j}^{\beta_{j}}$. The uniqueness of representation for $r$ by the FTOA implies that $p_{i} = q_{i}$ and $\alpha_{i} = \beta_{i}$ for all $i$, thus $a=b$. Can anyone spot an error?",['number-theory']
807730,factorial of infinite Cardinals,"Let $S_A$ be set of all bijections over $A$ such that
  $Card(A)=\kappa$. Define foctorial as $\kappa!:=Card(S_A)$. Show that if $\kappa$ is  infinite, then : $\kappa!=2^\kappa$ First, I've proved this definition is well-defined. Then I wanted to use Cantor-Schroeder-Bernstein 's theorem to find injections between $S_A$ and $2^A$ or $\mathcal{P}A$, but I'm still searching for it. If it is possible, then there's no need to use Axiom of Choice and it is provable in ZF . Actually, injection of desired functions must be proved directly from injection of $f\in S_A$. So my first attempt was following function which isn't injective ! $H(f)(a) = \left\{ \begin{array}{lc} 1 & f(a)=a\\
0 & \#
\end{array}\right.$ Now let's find an injection from $S_A$ to $2^A$ and from $\mathcal{P}A$ to $S_A$, or another injections !","['cardinals', 'elementary-set-theory', 'axiom-of-choice']"
807759,"Show that $\lim_{n\rightarrow \infty} \sqrt[n]{c_1^n+c_2^n+\ldots+c_m^n} = \max\{c_1,c_2,\ldots,c_m\}$ [duplicate]","This question already has an answer here : The $ l^{\infty} $-norm is equal to the limit of the $ l^{p} $-norms. [duplicate] (1 answer) Closed 6 years ago . Let $m\in \mathbb{N}$ and $c_1,c_2,\ldots,c_m \in \mathbb{R}_+$. Show that $$\lim_{n\rightarrow \infty} \sqrt[n]{c_1^n+c_2^n+\ldots+c_m^n} = \max\{c_1,c_2,\ldots,c_m\}$$ My attempt: Since $$\lim_{n\rightarrow \infty} \sqrt[n]{c_1^n+c_2^n+\ldots+c_m^n} \leq \lim_{n\rightarrow \infty}\sqrt[n]{\max\{c_1,c_2,\ldots,c_m\}} = \lim_{n\rightarrow \infty} \sqrt[n]{n}\sqrt[n]{\max\{c_1,c_2,\ldots,c_m\}}=\lim_{n \rightarrow \infty} \max\{\sqrt[n]{c_1^n},\sqrt[n]{c_2^n},\ldots,\sqrt[n]{c_m^n}\}=\lim_{n \rightarrow \infty}\max\{c_1,c_2,\ldots,c_m\}=\max\{c_1,c_2,\ldots,c_m\}$$ it follows that $\lim_{n\rightarrow \infty} \sqrt[n]{c_1^n+c_2^n+\ldots+c_m^n}$ is bounded, but I don't think it's monotonically decreasing, at least I can't prove this. Can anybody tell me whether the approach I have chosen is a good one, whether what I have done is correct and how to finish the proof?","['radicals', 'real-analysis', 'limits']"
807853,Prove that the gradient of a unit vector equals 2/magnitude of the vector,"Let $\vec r=(x,y,z)$ Firstly find $\vec \nabla (\frac 1 r)$ where r is the magnitude of $\vec r$.
I think I've done this correctly to get $-x(x^2+y^2+z^2)^{-\frac32} \hat i-y(x^2+y^2+z^2)^{-\frac32} \hat j-z(x^2+y^2+z^2)^{-\frac32} \hat k$ Secondly prove that $\vec \nabla. \frac{\vec r}{r}=\frac2r$ I've really got no idea for the second part.","['multivariable-calculus', 'vector-analysis']"
807870,"$\int_{0}^{1}\int_{0}^{1}\frac{1}{\sqrt{x^2+y^2}}\,dx\,dy=$?","I'm having difficulties thinking of a good variable change for 
$$\int_0^1 \int_0^1 \frac{1}{\sqrt{x^2+y^2}}\,dx\,dy=?$$
the most natural choice would be something like $x=r\cos\theta$ and $y=r\sin\theta$ since that would make it a very simple integral to calculate, but the bounds of $r$ and $\theta$ are all mixed up. This is a square. not a circle.","['multivariable-calculus', 'calculus', 'integration']"
807878,curve integral - intersection between plane and sphere,"I am going to calculate the line integral
$$ \int_\gamma z^4dx+x^2dy+y^8dz,$$
where $\gamma$ is the intersection  of the plane $y+z=1$ with the sphere $x^2+y^2+z^2=1$, $x \geq 0$, with the orientation given by increasing  $y$. Since $\gamma$ is an intersection curve, I decided to use Stoke's theorem,  applied to  the vector field $(z^4,x^2,y^8)$ and an oriented surface $Y$ with  boundary $\gamma$. But how am I going to parametrize the surface so I can use it with Stoke's thoerem? If I parametrize the surface by $(x(s,t),y(s,t),z(s,t))=(0,t,1-t),$ $x^2+y^2+z^2\leq 1$, I will get the normal vector $(0,0,0)$, but the normal vector is going to point upwards, I think.","['multivariable-calculus', 'integration', 'differential-geometry', 'vector-analysis', 'real-analysis']"
807891,A circular proof in Rudin that $\mathbb{R}$ is a field.,"Today I'm afraid, I found a circular reasoning in Rudin's Principles of Mathematical Analysis( I found  no errata that mentions this). Before actually going through the actual question, I have compiled two documents one on fields (hereafter called document 1) and an incomplete document on construction of real numbers from rational numbers (hereafter called document 2), the second document contains the definitions and proofs that satisfy the criteria that are done before proving that $\mathbb{R}$ is a field (All theorems except the last one is okay). In the document 2 , the last theorem, I have tried to prove is that $\mathbb{R}$ satisfies all the Field axioms for addition.(Referring to step 4 of Apendix in chapter 1 of rudin's book) Rudin's book could only prove the Axiom-5 using the theorem 1.20(in Rudin's book) popularly known as the Archemedian property(You can see page-4 of my document 2 for a proof how it is done). Now I shall show that the proof of theorem 1.20 requires the fact that $\mathbb{R}$ satisfies at least the axioms of addition. Theorem 1.20 (a) If $x\in \mathbb{R}$, $y\in \mathbb{R}$, and $x>0$, then there is a positive integer $n$ such that $nx>y$. The proof follows from least-upper-bound property of real numbers(this can be proven with no problem, see my document) to claim that the set $A=\{nx:n\in\mathbb{N}\}$, if doesn't satisfy the theorem has a supremum and hence take $\alpha-x<\alpha$ and using definition of supremum to arrive at a contradiction. Now existence of $-x$ follows from A5 of field, and the property that If $y<z$ then $x+y<x+z$ where $x,y,z$ belong to a field(a part of definition of ordered field) is required for proving that $\alpha-x<\alpha$, to show that $\mathbb{R}$ satisfies this property we require the proposition 1.14-a (in rudin) or proposition 1-a (in document 1) whose proof again requires axioms A4, A5, A3, A2 and A1, existence of A5 is at stake right now. So to conclude things proving that $\mathbb{R}$ satisfies axiom A5 and the archemedian property are circular proofs. This is the question: Is there any way you can prove A5 without using archemedian property? ( I doubt this) If the above mentioned one is not possible, is there any way to prove archemedian property without help of field axiom A5? ( I seriously doubt this) Or am I missing something? If you have not understood my explanation then continue reading.
I'm sorry, I admit I am bad at explaining things, so here is a problem I am encountering. See my document 2, page 4 theorem 2. Proving $A5$ requires a theorem called archemedian property, can you add the proof of Archemedian property, before the beginning of the theorem so that I can complete the proof of theorem 2. Note: To talk about the seriousness of the matter, to me right now the whole properties of $\mathbb{R}$ is at stake, especially the universally accepted fact that $\mathbb{R}$ is a field, an ordered one.","['real-numbers', 'real-analysis', 'definition']"
807900,Question of an isomorphism of $\epsilon_ 0$ and a subset of the rationals.,"I don't know if this question is appropriated for this site. Anyway, I'm searching for an isomorphism of order $f:K \longrightarrow \epsilon_o $, such that $(K, \leq)$ is a subset(proper or not) of $(\mathbb{Q}, \leq)$ and $\epsilon_o = \sup\{\omega, \omega^{\omega}, \omega^{\omega^{\omega}}, ... \}$. Actually,  I'm not finding neither an isomorphism between $K$ and $\omega^{\omega}$.
 Thanks in advanced.","['set-theory', 'ordinals', 'order-theory']"
807926,"Injectivity of the operator $(Ax)(t)=\int_0 ^1 k(s,t) x(s)ds$","Let $X=C([0,1],\mathbb{R})$ (equipped with the supremum norm). Let $A$ be the operator defined for each $x\in X$ by
$$(Ax)(t)=\int_0 ^1 k(s,t) x(s)ds,$$
where $k:[0,1]\times [0,1]\to \mathbb{R} $ is continuous. Can we find a sufficient condition on $k$ to make $A$ injective ?","['operator-theory', 'functional-analysis', 'banach-spaces']"
807932,Characterization of Sobolev Space,"I have just started learning about Sobolev spaces. So this might be trivial. I am working through the book ""Partial Differential Equations"" by Lawrence Evans, it came highly recommended. Taking $\Omega \subset \mathbb{R}^{n}$ to be some open set. He defines the space $$W^{k,p}_{o}(\Omega) := \lbrace u \in W^{1,p}(\Omega): u|_{\partial \Omega} = 0 \rbrace$$ for $1 \leq p < \infty$. He does not however define $W^{1,\infty}_{o}(\Omega)$. Does anyone know how this is generally defined? Is it simply $$W^{1,\infty}_{o}(\Omega) := \lbrace u \in W^{1,\infty}(\Omega): u|_{\partial \Omega} = 0 \rbrace$$ Why is it not dealt with in the same manner as for $1 \leq p < \infty$? I also checked Brezis book, he also does not deal with the case $p = \infty$. Thanks.","['reference-request', 'sobolev-spaces', 'functional-analysis', 'partial-differential-equations']"
808000,The probability that $\dfrac{p-1}2$ is square-free,"Let $Q(x)$ denote the number of square-free integers between $1$ and $x$, we obtain the approximation $$\eqalign{
&Q(x)\approx x\prod_{p\,{\rm prime}}\left(1-\dfrac1{p^2}\right)=x\prod_{p\,{\rm prime}}\dfrac{1}{\left(1-\tfrac1{p^2}\right)^{-1}} \\ 
&Q(x)\approx x\prod_{p\,{\rm prime}}\dfrac1{1+\tfrac1{p^2}+\tfrac1{p^4}+\cdots}=\dfrac x{\sum_{k=1}^\infty\tfrac1{k^2}}=\dfrac x{\zeta(2)}.
}$$ Thus the asymptotic density of squarefree integers is $\dfrac{6}{\pi ^2}.$ Let $P(n)$ denote the number of odd primes $p$ among the first $n$ primes for which $\dfrac{p-1}2$ is square-free, see also A066651 .
I guess it may be true that $\dfrac{P(n)}{n}\approx \dfrac{Q(n)}{n} \approx \dfrac{1}{\zeta (2)}\approx 0.607927.$ I run a program to check my conjecture and get some results: $$
\begin{array}{c|lcr}
n &\text{P(n)}& \text{P(n)/n} \\
\hline
10 & 7 & 0.7\\
10^2 & 59 & 0.59\\
10^3 & 567 & 0.567\\
10^4 & 5604 & 0.5604\\
10^5 & 56182 & 0.56182\\
10^6 & 561104 & 0.561104\\
\end{array}
$$ Now we can see that $\dfrac{P(n)}{n}$ is getting smaller when $n$ is getting greater (except for $n$ from $10^4$ to $10^5$), hence it's very likely that $$\lim_{n\to \infty}\dfrac{P(n)}{n}\lt \dfrac{6}{\pi^2}.$$ I'm not sure if this is another example of Strong Law of Small Numbers , so that $\dfrac{P(n)}{n}$ will $\to \dfrac{6}{\pi^2}$ when $n$ is big enough. Hence, there are two problems: Does $\lim_{n\to \infty}\dfrac{P(n)}{n}$ exist? Is it true that $\lim_{n\to \infty}\dfrac{P(n)}{n}=\dfrac{6}{\pi^2}$? Any tips or help is welcome, thanks in advance!","['analytic-number-theory', 'number-theory']"
808005,Eigenvalues of $AB$ and $BA$ where $A$ and $B$ are rectangular matrices,"This question is a generalisation of Eigenvalues of $AB$ and $BA$ where $A$ and $B$ are square matrices . Let $A$ and $B$ be $m\times n$ and $n\times m$ complex matrices, respectively,
with $m < n$. If the eigenvalues of $AB$ are $\lambda_1, \ldots, \lambda_m$, what are the eigenvalues of $BA$? If the matrices were square, then the conclusion would follow  from the fact that $AB$ and $BA$ have  the same characteristic polynomial . With rectangular matrices this is not going to happen; how to proceed then?","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
808049,volume of projective space $\text{Vol}(\mathbb CP^N)$,How can we compute the volume of projective space $$\text{Vol}(\mathbb CP^N)$$,"['symplectic-geometry', 'differential-geometry']"
808053,"Is there a known function $f(n) = P_n$, where $P_n$ denotes the $n$th prime number?","Is there a known function $f:\mathbb{R}\to\mathbb{R}$, such that: The definition of $f$ does not contain the $!$ operator The definition of $f$ does not contain the $\sum$ operator The definition of $f$ does not contain the $\prod$ operator The definition of $f$ does not contain a continued fraction $\forall n\in\mathbb{N}:f(n) = P_n$, where $P_n$ denotes the $n$th prime number Needless to say, the definition of $f$ does not make an explicit use of $P_n$ If no such function is known, then is there any known proof that no such function exists?","['prime-numbers', 'functions']"
808074,"Intersection of two tetrahedra, point reflexion","We are given a regular tetrahedron $ABCD$ ($ABC$ is its` base and $D$ is its vertex) and we reflect it through the middle of its height (point reflexion) - and thus we obtain a congruent regular tetrahedron $A'B'C'D'$. $D'$ lies in the center of $ABC$, and $D$ in the center of $A'B'C'$. Planes $\pi (ABC) \ || \ \pi (A'B'C'), \ \ \ \pi (ABD) \ || \ \pi (A'B'D'), \ \ \ \pi (B'C'D') \ || \ \pi (BCD)$, $ \ \ \ \pi (A'C'D') \ || \ \pi (ACD)$. I drew a picture and I think that the intersection of the two tetrahedrons is a parallelepiped, but I don't know how to prove it more formally (I mean, I know that the respective sides of the tetrahedrons are parallel, because we reflect $ABCD$ in a point, but I am not sure if that's enough). Secondly, how can we calculate the volume of the intersection? Could you help me with that? Thank you!","['geometry', 'symmetry', 'platonic-solids']"
808085,$\int_{0}^{\infty}\int_{1}^{\infty}\frac{x^2-y^2}{(x^2+y^2)^2}dxdy$ diverges?,"I want someone to review my proof that $$\int_{1}^{\infty}\int_{0}^{\infty}\frac{x^2-y^2}{(x^2+y^2)^2}dxdy$$ does not converge. To make things easier, I said let's look at the entire first quadrant and then subtract the integral over the small rectangle that we added. move to polar coordinates: $$\int_{0}^{\frac{\pi}{2}}\int_{o}^{\infty}r^2\frac{(\cos^2\theta-\sin^2\theta)}{r^3}drd\theta=\int_{0}^{\frac{\pi}{2}}\int_{o}^{\infty}\frac{\cos^2\theta-\sin^2\theta}{r}drd\theta=\int_{0}^{\frac{\pi}{2}}\int_{o}^{\infty}\frac{1-2\sin^2\theta}{r}drd\theta\leq\int_{0}^{\frac{\pi}{2}}\int_{o}^{\infty}\frac{1}{r}drd\theta$$ The inequality part is true since $-2\sin^2 x$ is always negative. So: $$\int_{0}^{\frac{\pi}{2}}\int_{o}^{\infty}\frac{1}{r}drd\theta=\frac{\pi}{2}\ln(r)|_0^{\infty}=\frac{\pi}{2}\ln(\frac{\infty}{0})=\frac{\pi}{2}\ln({\infty})=\infty$$ So over the entire first quadrant it diverges. Is there a point to checking the small rectangle that we added? I mean, even if it diverges, the answer would still diverge.","['multivariable-calculus', 'improper-integrals', 'calculus', 'integration']"
808098,Is the two-dimensional Koch curve space-filling?,"Say, we'd like to make a Koch curve with self-similarity dimension of two. A Koch curve with the following generator seems to be two-dimensional, since if we double its size by scaling we'll find four of the original size inside, so $$\text{self-similarity dimension}=\log_2 4=2$$ Therefore I was expecting the curve to be space-filling. After my initial mistake of failing to make the segments equal, words that end in GRY provided an image of the limit set: $\ $ Is the curve filling any space? If yes, what space? If no, why not?","['dynamical-systems', 'geometry', 'curves', 'fractals']"
808122,Examples of quasigroups with no identity elements,"If you scroll to the bottom of this page , there is a table claiming quasigroups have divisibility but not identity (in general). What would be some examples of quasigroups without an identity element?","['quasigroups', 'examples-counterexamples', 'abstract-algebra']"
808132,Continuous subgroup of SO(3)?,"I read from a paper arXiv: cond-mat/0602109 by a theoretical physicist, Prof. Frank Bais,  close subgroups of $SO(3)$ is given by ${C_n,D_n,T,O,I,SO(2)\rtimes Z_2}$, where $C_n$ is the  cyclic group of order $n$, $D_n$ is dihedral group of order $n$, $T$ is the tetrahedral group, $O$ is the octahedral group, and $I$ is the icosahedral group. However, I have some questions to understand this. Is $SO(2)$ a subgroup of $SO(3)$? The subgroups listed in the paper mentioned above is about close subgroups. What's the meaning of 'close' here? If $SO(2)$ is a subgroup of $SO(3)$, can it be parameterized as a $3\times 3$ matrix as \begin{align}
\left(
\begin{array}{ccc}
 \cos\theta & -\sin\theta & 0 \\
 \sin\theta & \cos\theta & 0 \\
 0 & 0 & 1
\end{array}
\right)
\end{align}
with a imagined '$z$' axis? Can the subgroup $SO(2)\rtimes Z_2$ be parameterized as a $3\times 3$ matrix? E.g,
\begin{align}
\left(
\begin{array}{ccc}
 \cos\theta & -\sin\theta & 0 \\
 \sin\theta & \cos\theta & 0 \\
 0 & 0 & \ 1
\end{array}
\right),
\left(
\begin{array}{ccc}
 \cos\theta & \sin\theta & 0 \\
 \sin\theta & -\cos\theta & 0 \\
 0 & 0 & \ -1
\end{array}
\right)
\end{align} Moreover, any suggestions of tutorial references or textbooks that I can find the list of subgroups or the way I can calculate them by myself are also warmly welcome. I am not familiar with continuous groups.","['matrices', 'representation-theory', 'group-theory', 'lie-groups']"
808136,Speediness and correctness when graphing by hand .,"First of all thank you for visiting this question! I believe it's a pretty simple problem but get's kinda hairy and time consuming on each step as I have done it, so my question (the one you are here for) is the following : What is the fastest and most reliable way to sketch a given equation determining zeroes, asymptotes, extreme values, and points of inﬂection? (While doing this problem here , I realized of other doubts I had about the procedure I applied, I've listed those doubts at the bottom) I was given by the university I'm looking forward to join an example of my math admission exam and on the last section and the one that it's worth the most I encountered the following : Let $f(x) = \frac{x}{x^2+1}$ Determine the zeroes, asymptotes, extreme values, and points of inﬂection of the
graph. Use this information to sketch the graph of $f(x)$ supported by complete
arguments. My first idea was to create a table that I would be filling while doing this problem, the table was the following : +--------------------------------+
|Known Information               |
+--------------------------------+
|Zeroes :                        |
|Vertical Asymptote :            |
|Horizontal Asymptote :          |
|Extreme Values :                |
|Points of Inflection :          |
+--------------------------------+ Now I will separate each procedure by sections on the post , in the same order I did the problem. 1.- Determining the zeros I proceeded to find the zeroes by doing the following $$\frac{x}{x^2+1}=0$$ $$·(x^2+1) ·(x^2+1)$$ $$x=0$$ So when $x=0$ the equation is $0$ as well (Yes, I could have determined this by just looking at the pure fraction, but I'm trying to be as general as possible , my test might contain some other than just x) We proceed to fill the table +--------------------------------+
|Known Information               |
+--------------------------------+
|Zeroes : 0                      |
|Vertical Asymptote :            |
|Horizontal Asymptote :          |
|Extreme Values :                |
|Points of Inflection :          |
+--------------------------------+ 2.- Determining Vertical Asymptote I proceeded to determine the vertical asymptote by setting the denominator to zero as I illustrate next $$x^2+1=0$$ $$-1 -1$$ $$x^2=-1$$ By having this I determined that there was no vertical asymptote . +--------------------------------+
|Known Information               |
+--------------------------------+
|Zeroes : 0                      |
|Vertical Asymptote : None       |
|Horizontal Asymptote :          |
|Extreme Values :                |
|Points of Inflection :          |
+--------------------------------+ 3.- Determining the horizontal asymptote. To do this, I knew that Given $$\frac{t^a}{d^b}$$ if $b>a$ then we have an horizontal asymptote at $y=0$ Consequently +--------------------------------+
|Known Information               |
+--------------------------------+
|Zeroes : 0                      |
|Vertical Asymptote : None       |
|Horizontal Asymptote : y=1      |
|Extreme Values :                |
|Points of Inflection :          |
+--------------------------------+ 4.- Determining the extreme values. This is where it gets time consuming and complicated , to proceed I differentiated my $f(x)$ as illustrated next $$y = \frac{x}{x^2+1}$$ $$\frac{dy}{dx}=\frac{dy}{dx}[\frac{x}{x^2+1}]$$ $$\frac{dy}{dx}[\frac{x}{x^2+1}] = \frac{dy}{dx}[x·((x^2+1)^-1)]$$ Note : On the last illustration the last term on the right is raised to the negative one power, which is equal to $\frac{1}{x^2+1}$ (I do not know why I can't express the negative one power here..) And I also knew that Given $$\frac{dy}{dx}[f(x)g(x)]=f'(x)g(x)+g'(x)f(x)$$ So I proceeded first to find the derivatives of each term being multiplied , the first one resulting as below : $$\frac{dy}{dx}[x] = 1$$ The second one resulting as below as well
(Using chain rule) $$\frac{dy}{dx}[(x^2+1)^-1] = -1((x^2+1)^-2)·2x$$ Or equivalently $$\frac{dy}{dx}[(x^2+1)^-1] = -\frac{2x}{(x^2+1)^2}$$ So ended up having $$\frac{dy}{dx}[x] = 1$$ and $$\frac{dy}{dx}[(x^2+1)^-1] = -\frac{2x}{(x^2+1)^2}$$ Proceeded to use the product rule as illustrated below $$\frac{dy}{dx}[f(x)g(x)]=f'(x)g(x)+g'(x)f(x)$$ $$\frac{dy}{dx}[x·\frac{1}{(x^2+1)}]=(1·(\frac{1}{(x^2+1)}))+(-\frac{2x}{(x^2+1)^2} · x)$$ $$\frac{dy}{dx}[x·\frac{1}{(x^2+1)}]=(\frac{1}{(x^2+1)})-(\frac{2x^2}{(x^2+1)^2})$$ I proceeded to find a common denominator, and accomplished this by multiplying $\frac{1}{(x^2+1)}$ by $(x^2+1)$ as illustrated next $$\frac{1}{(x^2+1)}·\frac{x^2+1}{x^2+1} = \frac{x^2+1}{(x^2+1)^2}$$ Substituting it and resulting in $$\frac{dy}{dx}[x·\frac{1}{(x^2+1)}]=(\frac{x^2+1}{(x^2+1)^2})-(\frac{2x^2}{(x^2+1)^2})$$ $$\frac{dy}{dx}[x·\frac{1}{(x^2+1)}]=\frac{(x^2+1)-(2x^2)}{(x^2+1)^2}$$ So our final derivative is $$\frac{dy}{dx}[x·\frac{1}{(x^2+1)}]=\frac{-x^2+1}{(x^2+1)^2}$$ To find it's extreme values we set the derivative equation equal to zero $$0=\frac{-x^2+1}{(x^2+1)^2}$$ And solve for x doing the following $$0=\frac{-x^2+1}{(x^2+1)^2}$$ $$·(x^2+1)^2·(x^2+1)^2$$ $$0=-x^2+1$$ $$-1 -1$$ $$-1=-x^2$$ $$·-1 ·-1$$ $$1=x^2$$ $$\sqrt(1)\sqrt(x^2)$$ $$\pm1=x$$ So finally, we have determined the extreme values which are $x=1$ and $x=-1$ +--------------------------------+
|Known Information               |
+--------------------------------+
|Zeroes : 0                      |
|Vertical Asymptote : None       |
|Horizontal Asymptote : y=1      |
|Extreme Values : x=1 & x=-1     |
|Points of Inflection :          |
+--------------------------------+ 5.- Determining nature of extreme values. This is were I was completely frustrated on how long it was taking for me to determine the nature of the extreme values I had obtained (Maxima or minima), I firstly remembered that to determine this I needed to determine the second derivative of the first equation and evaluate it at the obtained extreme values , if when doing so the value resulted positive that meant that It would be a minimum value , if negative it would be a maximum value . So I would need to do as illustrated next $$y=\frac{-x^2+1}{(x^2+1)^2}$$ $$\frac{dy}{dx}=\frac{dy}{dx}[\frac{-x^2+1}{(x^2+1)^2}]$$ To do this I decided to use the quotient rule as It was easier for me to derive with no negative terms on this more complicated functions, so the quotient rule states $$\frac{dy}{dx}[\frac{f(x)}{g(x)}] = \frac{g(x)f'(x)-f(x)g'(x)}{(g(x))^2}$$ So I proceeded to derive each term of my equation, the top term resulting in $$\frac{dy}{dx}[-x^2+1] = -2x$$ And the bottom term resulting in $$\frac{dy}{dx}[(x^2+1)^2] = 2·(x^2+1)·2x$$ or $$\frac{dy}{dx}[(x^2+1)^2] = 4x(x^2+1)$$ So ended up having $$\frac{dy}{dx}[-x^2+1] = -2x$$ and $$\frac{dy}{dx}[(x^2+1)^2] = 4x(x^2+1)$$ Now proceeded to use the quotient rule $$\frac{dy}{dx}[\frac{f(x)}{g(x)}] = \frac{g(x)f'(x)-f(x)g'(x)}{(g(x))^2}$$ $$\frac{dy}{dx}[\frac{-x^2+1}{(x^2+1)^2}] = \frac{(x^2+1)^2·(-2x)-(-x^2+1)·(4x(x^2+1))}{((x^2+1)^2)^2}$$ Evaluate at our extreme values $x=1$ and $x=-1$ $$\frac{((1)^2+1)^2·(-2(1))-(-(1)^2+1)·(4(1)((1)^2+1))}{(((1)^2+1)^2)^2} = -\frac{1}{2}$$ So we can say that at point $x=1$ theres a maximum value. $$\frac{((-1)^2+-1)^2·(-2(-1))-(-(-1)^2+-1)·(4(-1)((-1)^2+-1))}{(((-1)^2+-1)^2)^2} = \frac{1}{2}$$ So we can say that at point $x=-1$ theres a minimum value. +--------------------------------+
|Known Information               |
+--------------------------------+
|Zeroes : 0                      |
|Vertical Asymptote : None       |
|Horizontal Asymptote : y=1      |
|Extreme Values:x=1 Max x=-1 Min |
|Points of Inflection :          |
+--------------------------------+ 6.- Determining Inflection Points It's taking me longer than expected to write this post so I'll leave for anyone interested this part as a practice, what I basically did was simplify the second derivative , in this case the following $$0 = \frac{(x^2+1)^2·(-2x)-(-x^2+1)·(4x(x^2+1))}{((x^2+1)^2)^2}$$ After taking a bunch of time simplifying it I ended up with $$2x^4+6=0$$ and guessed that for the function to be equal to zero, then 2x^4 should be equal to exactly -6 $$2x^4=-6$$ I ended up with an imaginary number so guessed that there was no points of inflection ? NOTE: This part of determining the inflection points , I'm not pretty sure that I could have come with such a value (Imaginary) , please let me know of my mistakes. +--------------------------------+
|Known Information               |
+--------------------------------+
|Zeroes : 0                      |
|Vertical Asymptote : None       |
|Horizontal Asymptote : y=1      |
|Extreme Values:x=1 Max x=-1 Min |
|Points of Inflection : None?    |
+--------------------------------+ 6.- Graphing With my final information table +--------------------------------+
|Known Information               |
+--------------------------------+
|Zeroes : 0                      |
|Vertical Asymptote : None       |
|Horizontal Asymptote : y=1      |
|Extreme Values:x=1 Max x=-1 Min |
|Points of Inflection : None?    |
+--------------------------------+ I graphed the function by using the extreme values (evaluating it's y value on the function) and the zeroes (zero) , and ended up having the following that was actually a good result. My doubts left after this entire procedure in conjunction with the main question (which I'm excluding here on the bottom) of this topic are the following Is it quite possible that I could have gotten an imaginary number as my inflection point? Is trying close numbers to the found extreme values with the purpose of determining if the point is a maxima or minima a valid argument ? And thank you very much for reading the question! (:","['graphing-functions', 'calculus', 'derivatives']"
808148,Is every continuous function measurable?,"In non-Hausdorff topology it is standard to define the Borel algebra of a topological space $X$ as the $\sigma$-algebra generated by the open subsets and the compact saturated subsets. Recall that a subset is saturated if it is an intersection of open subsets, and that compact saturated subsets play the role of compact subsets when the space $X$ is not $T_1$ (which is typically the case for a partially ordered set equipped with the Scott topology for instance). In this situation, for a continuous function $f : X \to Y$ between topological spaces, is $f$ necessarily measurable? This question is equivalent to the following. If we write $\uparrow y$ for the intersection of all open subsets containing $y$, which happens to be compact saturated, is it true that $f^{-1}(\uparrow y)$ is measurable for all $y \in Y$? Thank you very much for your help. 
Paul Edit: this question has now been migrated to MathOverflow, see here .","['general-topology', 'measure-theory', 'functional-analysis', 'order-theory']"
808176,How to show that the Volterra operator is not normal,"How to show that the Volterra operator: $$V:L_2(0,1)\rightarrow L_2(0,1): x\mapsto \int^t_0 x(s) \, ds$$ is not normal.  $t\in (0,1)$ Could you please help with this question.","['operator-theory', 'functional-analysis', 'real-analysis']"
808193,Log trig integral with radical,"Show that: $$\int_{0}^{\Large\frac{\pi}{2}}\frac{\log(\sin\theta)}{\sqrt{1+\sin^{2}\theta}}\ d\theta=-\frac{\Gamma^{2}\left(\dfrac14\right)\sqrt{\dfrac\pi2}}{16},$$  or some other equivalent form.","['definite-integrals', 'integration']"
808203,What are the surfaces of constant Gaussian curvature $K > 0$?,"Besides the sphere, is there any other surface with constant and positive Gaussian Curvature $K$?","['riemannian-geometry', 'curvature', 'differential-geometry']"
808213,Do you feel comfortable with integral u-substitution? (reverse chain rule),"I've made this post both to see if I'm thinking right and to let others read and understand where the ""u-substitution"" method for integration comes from. I really hate substitutions, because you lost track of what's happening. I've read the related posts in this forum and concluded the following: The integral u-substitution is a nice method to find some integrals. It comes from the chain rule: $$\frac{df(g(x))}{dx} = \frac{df(g(x))}{dg(x)}\frac{dg(x)}{dx}$$ For me, $\frac{df(x)}{dx}$ is just a notation for the derivative of the function $f$ with respect to $x$, so there's no mean for just $df$ or just $dx$ alone. When we integrate both sides: $$\int \frac{df(g(x))}{dx}dx = \int\frac{df(g(x))}{dg(x)}\frac{dg(x)}{dx}dx$$ Then:
$$\underbrace{f(g(x)) + C}_{\text{integral of a derivative}}= \int\frac{df(g(x))}{dg(x)}\frac{dg(x)}{dx}dx\tag{1}$$ So if we want to integrate some function in the form $\int\frac{df(g(x))}{dg(x)}\frac{dg(x)}{dx}dx$ this is gonna be equal $f(g(x)) + C$. That's why we can integrate $\cos(2x)$ this way: $$\int \cos(2x)dx = \int \frac{d\sin(2x)}{d2x}\frac{2}{2}dx = \frac{1}{2}\int\frac{d\sin(2x)}{d2x}\cdot2 \ \ dx$$ See how I didn't change the integrand at all, but I multiplied and divided by $2$ to get the form $$\frac{d\sin(2x)}{d[2x]}\frac{d[2x]}{dx} = \frac{d\cos(2x)}{dx}$$ 
Then, I can match the pattern in $(1)$ to integrate like this:
$$\begin{align}  &\int\frac{d\color{#F01C2C}{f(}\color{Blue}{g(x)}\color{#F01C2C}{)}}{d\color{Blue}{g(x)}}\color{#01cf84}{\frac{dg(x)}{dx}}dx = \color{#F01C2C}{f(}\color{Blue}{g(x)}\color{#F01C2C}{)} + C \\
\int \cos(2x)dx = \frac{1}{2}&\int\frac{d\color{#F01C2C}{\sin(\color{Blue}{2x})}}{d\color{Blue}{2x}}\cdot\color{#01cf84}{\ \ 2} \ \ \ \ dx = \frac{1}{2}\color{#F01C2C}{\sin(\color{Blue}{2x})} + C\end{align}$$ So... am I right? Do you feel comfortable doing substitutions? Would this technique be acceptable in my math tests? (I really prefer this than the substitution method). Update : Let's do this integral: 
$$\int x\ln(\cos(x^2))\sin(x^2)\mathrm dx$$
I will derivate $\cos(x^2)$: $$\frac{d}{dx}\cos(x^2) = -2x\sin(x^2)$$ Then I'll multiply and divide the integrand by this result: $$
\begin{align}
\int x\ln(\cos(x^2))\sin(x^2) \color{#F01C2C}{\frac{-2x\sin(x^2)}{-2x\sin(x^2)}}dx = \color{#F01C2C}{-\frac{1}{2}}&\int \ln(\cos(x^2))\cdot\color{#F01C2C}{-2x\sin(x^2)}dx
\\ &\int\frac{df(g(x))}{dg(x)} \ \ \ \ \ \ \frac{dg(x)}{dx} \ \ \ \ \ \ \ dx
\\=&f(g(x)) + C
\end{align}
$$
So to integrate this, we just have to find the antiderivative of $\ln$ and apply it to the 'point' $\cos(x^2)$. The antiderivative of $\ln$ is $x(\ln(x) - 1)$ by integration by parts. Applying it to $\cos(x^2)$ we have: $\cos(x^2)(\ln(\cos(x^2))-1)$ (this is the antiderivative at $\cos(x^2)$ or $g(x)$. Back in our integral: $$\color{#F01C2C}{-\frac{1}{2}}\int \ln(\cos(x^2))\cdot\color{#F01C2C}{-2x\sin(x^2)}dx = \color{#F01C2C}{-\frac{1}{2}}\cos(x^2)(\ln(\cos(x^2))-1)$$","['calculus', 'integration', 'derivatives']"
808233,Integral $\int_0^\infty \frac{\cos x}{x}\left(\int_0^x \frac{\sin t}{t}dt\right)^2dx=-\frac{7}{6}\zeta(3)$,"I am trying to prove this below. $$
I:=\int_0^\infty \frac{\cos x}{x}\left(\int_0^x \frac{\sin t}{t}dt\right)^2dx=-\frac{7}{6}\zeta(3)
$$ where $$
\zeta(3)=\sum_{n=1}^\infty \frac{1}{n^3}.
$$ I am not sure how to work with the integral over $t$ because it is from $0$ to $x$ .   If we can somehow write $$
\int_0^\infty \frac{\cos x}{x} \left(\int_0^\infty \frac{\sin t}{t}dt-\int_x^\infty \frac{\sin t}{t}dt     \right)^2dx=\int_0^\infty \frac{\cos x}{x}\left(\frac{\pi}{2}-\int_x^\infty \frac{\sin t}{t}dt\right)^2dx.
$$ I do not want to use an asymptotic expansion on the integral over $t$ from $x$ to $\infty$ , I am looking for exact results.  Note we can use $\int_0^\infty \frac{\sin t}{t}dt=\int_0^\infty \mathcal{L}[\sin t(s)]ds=\frac{\pi}{2}.$ Other than this approach I am not really sure how to go about this.  Note by definition $$
\int_0^x \frac{\sin t}{t}dt\equiv Si(x),
$$ but I'm not too sure what this definition can be used for in terms of a proof.  Also note $$
\int_0^\infty \frac{\cos x}{x}dx \to \infty.
$$","['special-functions', 'integration', 'definite-integrals', 'real-analysis', 'complex-analysis']"
808246,"Partial derivative of $g(x,y)=f(h(x,y),l(x,y))$","Let $f,h,l: \mathbb{R}^2 \to \mathbb{R}$ be derivable functions. If $g(x,y)=f(h(x,y),l(x,y))$,
is the following formula true? $$\frac{∂g}{∂x}(x,y)= \left(\frac{∂f}{∂x}(h(x,y),l(x,y)) +\frac{∂f}{∂y}(h(x,y),l(x,y))\right)\left( \frac{∂h}{∂x}(x,y)+\frac{∂l}{∂x}(x,y)\right) $$
It looks close to the chain rule $\frac{∂f}{∂x} \cdot \frac{∂x}{∂t}$ Also does this notation $\frac{∂f}{∂x}(h(x,y),l(x,y)) $ make sense as a derivative of $f(x,y)$ by the variable $x$ and after the derivation, substituting $h,l$ in the place of $x,y$ ?","['multivariable-calculus', 'partial-derivative']"
808263,Spin manifold and the second Stiefel-Whitney class,"We know that: Spin structures will exist if and only if the second Stiefel-Whitney class $w_2(M)\in H^2(M,\mathbb Z/2)$ of $M$ vanishes. Can someone use simple words and logic to show why the above is true? Note. More precisely, from Wikipedia : André Haefliger found necessary and sufficient conditions for the existence of a spin structure on an oriented Riemannian manifold (M,g). The obstruction to having a spin structure is certain element [k] of $H^2(M,\mathbb{Z}/2)$. For a spin structure the class [k] is the second Stiefel-Whitney class $w_2(M)\in H^2(M,\mathbb{Z}/2)$ of M. Hence, a spin structure exists if and only if the second Stiefel-Whitney class $w_2(M)\in H^2(M,\mathbb Z/2)$ of M vanishes. A. Haefliger (1956). ""Sur l’extension du groupe structural d’un espace fibré"". C. R. Acad. Sci. Paris 243: 558–560.","['characteristic-classes', 'differential-geometry', 'spin-geometry', 'manifolds', 'algebraic-topology']"
808272,"Relations of Characteristic classes: Chern, Stiefel-Whitney, Pontryagin, Euler, Wu class. [closed]","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 10 years ago . Improve this question We have a quite some characteristic classes: Chern class, Stiefel-Whitney class,  Pontryagin class, Euler class, Wu class, etc. I wonder whether some math experts can use simple words and basic equations to give explanation on their relations and correspondence between them? (it is a more or less on the illuminating the concept.)","['characteristic-classes', 'differential-geometry', 'fiber-bundles', 'algebraic-geometry', 'algebraic-topology']"
808279,Open Source Abstract Algebra Textbooks,"Does anyone know of any open source abstract algebra textbooks other than Judson's ? I am about to write a small program for a friend that will generate a random algebra problem (for preparing for quals) and my idea is to go through judson (and hopefully some other open source algebra textbooks) and get the exercises, put them in a database and pick them randomly and then show it to them using MathJax or something. But I'm having trouble finding a list of open source abstract algebra textbooks that have the .tex files. I would be content with finding online lists of abstract algebra problems with latex code that is easily parseable? Thanks!","['book-recommendation', 'reference-request', 'abstract-algebra']"
808285,Why are integrals called integrals?,"What is the historical background for this term? I cannot quite see what is integral about an integral, even if we go back to the viewing it as the area under a curve.  It seems to me a strange choice of word.","['calculus', 'integration', 'math-history', 'soft-question', 'terminology']"
808314,Probability of getting a full house,"If five cards are selected at random from a standard 52 card deck, what is the probability of getting a full house. This is what I am thinking.
$(52*\binom{4}{3}*\binom{4}{2})/_{52}C_5$ Is that right?",['probability']
808324,Algebra of limits vs. L'Hospital,"We have to evaluate the following limit: $$\displaystyle\lim_{x\to 0} \dfrac{x - \sin(x)}{x^3}$$ When I evaluate it using L'Hospital rule I get $\dfrac16$ but when I simplify the limit using simple algebraic rules of limit I get $0$. We can use the this rule:
$\displaystyle\lim [f(x) \pm g(x)] = \lim[f(x)] \pm \lim[g(x)]$ to simplify the given limit. $$\lim_{x\to 0} \dfrac{x - \sin(x)}{x^3} = \lim_{x\to 0}\dfrac{x}{x^3} - \lim_{x\to 0}\dfrac{\sin(x)}{x^3}$$ Also, $\lim[f(x).g(x)] = \lim[f(x)].\lim[g(x)]$. $$\therefore \lim_{x\to 0}\dfrac{x}{x^3} - \lim_{x\to 0}\dfrac{\sin(x)}{x^3} = \lim_{x \to 0}\dfrac{1}{x^2} - \left(\lim_{x\to 0} \dfrac{\sin(x)}{x} \right)\left(\lim_{x\to 0} \dfrac{1}{x^2}\right)$$ Now, since, $\lim_{x\to 0} \dfrac{\sin(x)}{x} = 1$, $$\lim_{x \to 0}\dfrac{1}{x^2} - \left(\lim_{x\to 0} \dfrac{\sin(x)}{x} \right)\left(\lim_{x\to 0} \dfrac{1}{x^2}\right) = \lim_{x \to 0}\dfrac{1}{x^2} - \lim_{x\to 0} \dfrac{1}{x^2} = \lim_{x \to 0}\left(\dfrac{1}{x^2} - \dfrac{1}{x^2}\right) = 0$$ Why am I getting $0$ when I use algebra of limits.","['calculus', 'limits']"
808342,Problem on Solving Stochastic Differential Equation,"Let $(Xt)$ be a solution to the equation $dX_t = aX_t dt + \sqrt{(1+X_t^2)} dW_t$ where $W_t$ is a Brownian motion process at time t Let $Y = F(X_t)$ for a certain function $F$. Find $F$ for which $(Y_t) solves the equation $dY_t = f(Y_t) dt + dW_t$ where the drift term f should be explicitly determined. You may use the fact that $$
\int \frac{1}{\sqrt{1+x^2}}dx\ = ln (x + \sqrt{1+x^2}).
$$ My attempt $dY_t = dF(X_t)$ Applying ito's formula we obtain. $dF(X_t) = F'(X_t) dX_t + 1/2 F''(X_t)(dX_t)^2$ Substituting $dX_t$ and $(dX_t)^2$ $dF(X_t) = F'(X_t) (aX_tdt + \sqrt{1+X^2_t} dW_t) + \frac{1}{2} F''(X_t)(1+X^2_t)dt$
$dF(X_t) = F'(X_t)(aX_t + \frac{1}{2} F''(X_t)(1+X^2_t)) dt + F'(X_t)\sqrt{1+X^2_t} dW_t$ Equating coefficients for $dW_t$ $F'(X_t)\sqrt{1+X^2_t} = 1$ $F(X_t) = \int \frac{1}{\sqrt{1+X_t^2}}dx$ $F(X_t) = ln (X_t + \sqrt{1+X_t^2}) + C$, where C is a constant Now my question is ; Can we find this constant? If not, is the answer above correct for $F(x)
$
now after equating the coefficients of $dt$, the drift term f is $f(Y_t) = f(F(X_t)) = aX_t + \frac{1}{2} F''(X_t)(1+X^2_t)$ Question ; How can I find the function f?","['stochastic-calculus', 'stochastic-processes', 'ordinary-differential-equations']"
808355,When is $\mathbb{Z}[\alpha]$ dense in $\mathbb{C}$?,"Let $\alpha$ be a nonreal algebraic number. I'm interested in conditions that imply that $\mathbb{Z}[\alpha]$ is dense in $\mathbb{C}$. I'm particularly interested algebraic integer $\alpha$. This is what I know so far: if there is a $n \in \mathbb{N}$ such that $\alpha^n \in \mathbb{R} \setminus \mathbb{Z}$, then $\mathbb{Z}[\alpha]$ is dense in $\mathbb{C}$; algebraic integers of degree two don't satisfy the condition, although algebraic nonintegers of degree two may. Many thanks in advance.",['analysis']
808375,What is $\varlimsup \frac{\omega(n)}{\log n}$?,"$\omega(n)$ is the number of distinct prime divisors of $n$. How to figure out?

 $$\varlimsup_{n\to\infty} \frac{\omega(n)}{\log n}$$

or $ \dfrac{\omega(n)}{\log n}$ is convergent, so  $\lim\limits_{n\to\infty} \frac{\omega(n)}{\log n}=0 $? It is obvious that $2^{\omega(n)}\leq n$, or $$\omega(n)\leq \frac{\log n}{\log2} $$ so $\varlimsup_{n\to\infty} \frac{\omega(n)}{\log n}\leq\frac1{\log 2}$ Continue to move forward,   $n\geq7$, then  $\omega(n)\leq \log n $. so $$\varlimsup_{n\to\infty} \frac{\omega(n)}{\log n}\leq1$$ P.s.  I think about this problem after reading To show: $\omega(n)\ne\pi(n) , \forall n>2$","['analytic-number-theory', 'number-theory']"
808379,What is the real meaning of Hilbert's axiom of completeness,"According to Greenberg's book of geometry it is sufficient to consider the axiom of Dedekind along with Hilbert's axioms (except of course for the Archimedian Principle and his Axiom of Completeness) in order for Euclidian Geometry to be categorical (in this case the only model is that of order pairs of numbers used in Analytic Geometry). From this I can say that, because Dedekind's axiom of continuity is equivalent to Cantor's axiom (Nested Intervals Principle) and because any of them implies the Archimedian Principle but not the other way around then I guess Hilbert's Axiom of completeness is necessary in order to fix that ""something"" missing if the Archimedian property is assumed as an axiom instead of the others. So, my problem is basically that Hilbert's axiom of completeness is pretty weird and I cannot imagine in what sense it can be used along with Archimedes' Axiom to obtain Dedekind's Axiom. Now, this is what Hilbert says in his book: This axiom gives us nothing directly concerning the existence of limiting points, or of
the idea of convergence. Nevertheless, it enables us to demonstrate Bolzano’s theorem by
virtue of which, for all sets of points situated upon a straight line between two definite
points of the same line, there exists necessarily a point of condensation, that is to say,
a limiting point. From a theoretical point of view, the value of this axiom is that it
leads indirectly to the introduction of limiting points, and, hence, renders it possible to
establish a one-to-one correspondence between the points of a segment and the system
of real numbers. These are the definitions I'm talking about: Hilbert's Axiom of completeness: To a system of points, straight lines, and planes, it is impossible to add other elements in such a manner that the system thus generalized shall form a new geometry obeying all of the five groups of axioms. In other words, the elements of geometry form a system which is not susceptible of extension, if we regard the five groups of axioms as valid. Dedekind's Axiom: Suppose that the set ${l}$ of all points on a line $l$ is the disjoint union $\Sigma_1\cup \Sigma_2$ of two nonempty subsets such that no point of either subset is between two points of the other.Then there exists a unique point $o$ on $l$ such that one of the subsets is equal to a ray of $l$ with vertex $o$ and the other subset is equal to the complement. Cantor's Axiom: Let $\{\overline{A_nB_n}\}_{n\in N}$ be a sequence of segments on
    some straight line such that $\overline{A_{n+1}B_{n+1}}\subset \overline{A_{n}B_{n}}$ for all $n\in \mathbb{N}$. Then the intersection of such segments is not empty and there is
    a point $o$ that belongs to all of them. Archimedes' Axiom: For any two segments $\overline{AB}$ and $\overline{OE}$ there is
    a positive integer $n\in \mathbb{N}$ such that $\overline{AB}<n\cdot\overline{OE}$.","['geometry', 'foundations', 'model-theory']"
808384,Why does simplifying a function change its domain?,"Perhaps this is a silly question, but if you have a function, such as $$f(x) = \frac{x^2}{x}$$ the domain is all real numbers except x = 0. However, this function simplifies to $$f(x) = x$$ which has a domain of all real numbers. The domains for the two functions are different. Why are you permitted to simplify the first function if the domain changes?","['algebras', 'functions']"
808391,orthogonal projection onto orthogonal complement,"If $V=M \oplus M^{\perp}$. For any $v\in V$, the orthogonal projection of $v$ onto $M$ along $M^{\perp}$ is well defined. 
Can we take the orthogonal projection of $v$ onto $M^{\perp}$ along $M$?","['linear-algebra', 'functional-analysis']"
808415,Verifying Stokes' Theorem,"Verify Stokes' Theorem for the given vector field $f(x, y, z)$ and surface $\Sigma$.
$$f(x, y, z) = 2y \textbf{i} - x \textbf{j} + z \textbf{k}; \quad \Sigma : x^2 + y^2 + z^2 = 1, z \ge 0$$ This was the solution given. I understand the line integral part, but not the surface integral. Could someone explain each of the steps? Also, the previous answers to the question said to parameterize in spherical coordinates, but this doesn't. Could someone explain that alternate solution as well? Thank you.",['multivariable-calculus']
808436,simple games with cute winning strategies?,"Im thinking of games of two players ($A$ goes first and $B$ second) like the following: There are 35 chips in a table, during each turn a player can remove 1,2,3 or 4 chips. Prove player $B$ can always win (here the trick is to for $B$ to always leave $A$ with a multiple of 5 number of chips. There are two piles in a table, one with $2013$ chips and the other with $4017$ chips. During each turn a player must select a pile and remove a positive integer number of chips, the player that removes all the chips wins. Prove player $A$ can always win. (here the trick is for player $A$ to always leave both piles with the same number of chips. The nim game. In each turn a player places a knight in a position not threatened by another knight. Prove player $B$ can always win (player $B$ always choses the spot that is mirrored by $A$ over the diagonal, so if $A$ picks $(x,y)$ $B$ picks $(8-x,8-y)$. and other examples","['game-theory', 'reference-request', 'contest-math', 'combinatorics']"
808452,Proof of a Ramanujan Integral,"While studying Ramanujan's Collected Papers I came across a paper titled ""Some Definite Integrals"" which appeared in Messenger of Mathematics , ${\tt XLIV}, 1915, \mbox{10-18}$ .
It contains lot of weird integrals for which Ramanujan has given proofs. However in one instance he discusses about the integral \begin{align}
&\int_{0}^{\infty}\frac{dx}{\left(1 + x^{2}\right)\left(1 + r^{2}x^{2}\right)\left(1 + r^{4}x^{2}\right)\cdots}
\\[5mm] = &\ \frac{\pi}{2\left(1 + r + r^{3} + r^{6} + r^{10} + \cdots\right)}\label{1}\tag{1}
\end{align} where $0 < r < 1$ . Ramanujan derives this formula from \begin{align}
&\int_{0}^{\infty}\frac{\left(1 + arx\right)\left(1 + ar^{2}x\right)\cdots}{\left(1 + x\right)\left(1 + rx\right)\left(1 + r^{2}x\right)\cdots}x^{n - 1}\,\mathrm{d}x
\\[5mm] = &\
\frac{\pi}{\sin\left(n\pi\right)}
\prod_{m = 1}^{\infty}\frac{\left(1 - r^{m - n}\,\,\right)\left(1 - ar^{m}\,\right)}{\left(1 - r^{m}\,\right)\left(1 - ar^{m - n}\,\,\right)}\label{2}\tag{2}
\end{align} where $0 < r < 1, n > 0, 0 < a < r^{n - 1}$ and $n$ is not an integer and $a$ is not of the form $a = r^{p}$ where $p$ is a positive integer. Unfortunately, Ramanujan does not prove the formula (\ref{2}). Is there any direct approach to establish
(\ref{1}) without using (\ref{2}) or some way to establish (\ref{2}) $?$ .","['definite-integrals', 'improper-integrals', 'calculus']"
808454,"Integral $\int_0^\infty \log(1-e^{-a x})\cos (bx)\, dx=\frac{a}{2b^2}-\frac{\pi}{2b}\coth \frac{\pi b}{a}$","$$\mathcal{J}:=\int_0^\infty \log(1-e^{-a x})\cos (bx)\, dx=\frac{a}{2b^2}-\frac{\pi}{2b}\coth  \frac{\pi b}{a},\qquad \mathcal{Re}(a)>0, b>0.
$$
I tried to write
$$
\mathcal{J}=-\int_0^\infty  \sum_{n=1}^\infty\frac{e^{-anx}}{n}\cos(bx)\,dx 
$$
but the taylors series, $\log (1-\xi)=-\sum_{n=1}^\infty \xi^n/n, \ |\xi|<1$, thus this is not so useful for doing the integral.  I tried to also write
$$
\mathcal{J}=\frac{1}{b}\int_0^\infty \log(1-e^{-ax})d(\sin bx)=\frac{1}{b}\left(\log(1-e^{-ax})\sin (bx)\big|^\infty_0  -a\int_0^\infty 
\frac{\sin (bx)}{{e^{ax}-1}}dx \right),
$$
the boundary term vanishes so we have
$$
\mathcal{J}=\frac{a}{b}\int_0^\infty \frac{\sin(bx)}{1-e^{ax}}dx=\frac{a}{b}\mathcal{Im}\bigg[\int_0^\infty \frac{e^{ibx}}{e^{ax}-1}dx\bigg]
$$
which I am not sure how to solve.  Notice there are singularities at $x=2i\pi n/a, \ n\in \mathbb{Z}$. We need to calculate the residue for all the singularities along the imaginary axis.  The residue contribution to the integral
$$
2\pi i\cdot \sum_{n= 0}^\infty \frac{ e^{-2\pi  nb/a}}{e^{2i \pi n}}=2\pi i \sum_{n=0}^\infty e^{n(
-2\pi b/a-2i\pi)}=\frac{2\pi i}{e^{-(2\pi b/a+2\pi i)}}$$
Taking the imaginary part gives and re-writing the integral gives a different result.
Where did I go wrong?  How can we calculate this?  Thanks","['calculus', 'integration', 'definite-integrals', 'real-analysis', 'complex-analysis']"
808465,Fixed Field of Automorphisms of $k(x)$,"Fixed field of automorphisms of $k(x)$, with $k$ a field, induced by $I(x)=x$, $\varphi_1(x) = \frac{1}{1-x}$, $\varphi_2 (x)=\frac{x-1}{x}$? Since $I(x)=x$, $\varphi_1(x)=\frac{1}{1-x}$, $\varphi_2 (x)=\frac{x-1}{x}$ form a group of order 3 the group is cyclic, so it is generated by $\varphi_1$ then I have to find the fixed field of $\varphi_1$. If $a(x)=\frac{f(x)}{g(x)} \in k(x)$ with $(f,g)=1$ and $\varphi_1$ fix to $a(x)$ then $a(x)=a(\frac{1}{1-x}) \Rightarrow  \frac{f(x)}{g(x)} = \frac{f(\frac{1}{1-x})}{g(\frac{1}{1-x})} \Rightarrow f(x) \mid f(\frac{1}{1-x)})$ and by th same reason $ f(\frac{1}{1-x}) \mid f(x)$ so  $f(\frac{1}{1-x})=f(x)$ so $\varphi_1$ fix to $a(x)$, $f(x)$ then $\varphi_1$ fix to $g(x)$. So $\varphi_1$ fix to $\frac{f(x)}{g(x)}$. Someone can tell me if it is correct.","['galois-theory', 'abstract-algebra', 'field-theory']"
808485,Counting all homomorphism from $S_4$ to $\mathbb{Z_6}$,"I have to count number of homomorphism from $S_4$ to $\mathbb{Z_6}$. One approach that I know is by finding the possible kernel of homomorphism from $S_4$ to $\mathbb{Z_6}$. I am using another approach by finding the generators of $S_4$. We know that 
$S_4$ can  be generated by a $2$-cycle, say $\sigma$ and a $3$-cycle, say $\tau$: Thus any homomorphism $f\colon S_4\to \mathbb{Z_6}$ can be determined by finding  $f(\tau)$ and $f(\sigma)$ completely. It is clear that $f(\sigma)^2=1$ since $\sigma^2=1$ and $f(\tau)^3=1$ since $\tau^3=1$. Now we have to search for possible number of elements in $Z_6$ whose order divide $2$ and $3$ respectively. My confusion: We know that $S_4$ can also be generated by a $2$-cycle and a $4$-cycle. Perhaps, above written procedure may give wrong results in such case since order of four cycle is $4$. I am not able to understand where I am going wrong. I would be very much grateful if anybody could clear my doubt.  Thank you very much for your time.","['symmetric-groups', 'group-theory', 'abstract-algebra']"
808496,Is $\left\{x\mid x\text{ is a countable set}\right\}$ a set?,I found this question in an exam: Is $\left\{x\mid x\text{ is a  countable set}\right\}$ a set? We are working in ${\sf ZFC}$.,['elementary-set-theory']
808521,$L^1$ is complete in its metric,"Theorem: The vector space $L^1$ is complete in its metric. The following proof is from Princeton Lectures in Analysis book $3$ page $70$. Some of my questions about the proof of this theorem are as follows. First assume a Cauchy sequence $(f_n)\in L^1$, then we try to extract a subsequence $\left(f_{n_k}\right)$ of $(f_n)$ which converges to $f$, both point-wise almost everywhere and in the norm. Why do we need to show convergence point-wise almost everywhere? The theorem only says that $L^1$ is complete in its metric, i.e. $L^1$ norm. Right? As an extension to the first question, what is the difference between point-wise convergence and convergence in certain norms? When do I need to show point-wise convergence and when to show convergence in norm and when to show both, please? In the proof, we defined two new functions, and one of the two is $$f := f_{n_1}+\sum_{k=1}^\infty (f_{n_{k+1}}-f_{n_k}).$$ This is, in fact, $$\lim_{k\rightarrow\infty} f_{n_k}=\lim_{n\rightarrow \infty} f_n.$$ This is confusing to me since we are trying to find a limit for $(f_n)$. But in the above definition of $f$ we already assume the existence of $\lim_{n\rightarrow\infty} f_n$. Then what is the point of doing this, please? In addition, the proof states that the series defining $f$ converges almost everywhere. Why is this true? How can I see this, please? Thank you!","['integration', 'measure-theory', 'self-learning', 'lebesgue-integral', 'lebesgue-measure']"
808542,Lebesgue measure compact symmetric intervals,"Let $\mu$  be a measure on $L_m$ , ($m \ge 1$) - $\sigma$ - algebra of Lebesgue-measurable sets, such that its values on compact, symmetric intervals (cubes) are equal to Lebesgue measure of those cubes. Then $\mu = \mathcal{L}^m$. Could you tell me how to prove this observation? I know we can define Lebesgue measure by symmetric cubes, but I am not sure how to use it here. Thank you.","['measure-theory', 'lebesgue-measure']"
808553,De Rham cohomology for $\mathbb{R^2}$,"De Rham cohomology groups for $\mathbb{R^2}$. $H^{0}_{dR}(\mathbb{R}^{2})=\mathbb{R}$ since  $Z^{0}(\mathbb{R}^{2})$ is the one dimensional space of locally constant functions on $\mathbb{R}^{2}$ and $B^{0}(\mathbb{R}^{2})=0$. Or one can use the fact $\mathbb{R}^{2}$ is connected. Can we also use Poincaré lemma in this case? $H^{1}_{dR}(\mathbb{R}^{2})$  One can use Poincare lemma  and deduce that any closed form on $\mathbb{R}^{2}$ will be exact and then
$H^{1}_{dR}(\mathbb{R}^{2})=0$, but I would like to understand the following way of showing it explicitly: -we choose an arbitrary closed $\omega=adx+bdy$ on  $\mathbb{R}^{2}$, if we are to show it is exact then we shall find $\Phi\in\Omega^{0}(\mathbb{R}^{2})$ such that $d\Phi=adx+bdy$ and here I have problems with understanding how to obtain the following: $\Phi(x,y)=\displaystyle x\int_{0}^{1}a(tx,ty)dt+y\int_{0}^{1}b(tx,ty)dt$ What change of variables I have to make to get it? And how to verify that $\frac{\partial \Phi}{\partial x}=a(x,y)$ and $\frac{\partial \Phi}{\partial y}=b(x,y)$ Thank you","['homology-cohomology', 'differential-forms', 'algebraic-topology', 'differential-geometry']"
808630,Why in formulas a return value of a function sometimes shown as an argument?,"Sorry for a perhaps newbie question, I had a hard time in the school. Well, the title says the problem, let's look at example, which I stole from the coursera video-lectures about an algorithms : Claim: if $T(n) = a_{k}n^{k}+...+a_{1}n+a_{0}$ then $T(n)=O(n_{k})$ It is a formula of one of a videos, you may see here that $T(n)$ used as usual, it is a declaration of a function $T(n)$, which takes one argument n . That's pretty clear. As you probably know the so called Big-Oh $O()$ returns a speed of an algorithm(in a worst case, but for now this doesn't matter). I can understand for now that in the statement $T(n)=O(n_{k})$ the autor wanted to say: ""speed of the function $T(n)$ is equal to $n^{k}$"". But it is absolutely not what I see! I see this ""A result of a function $T(n)$  is equal to speed of calculating $n^{k}$"". Because Big Oh is a function that takes a function as an argument, and returns it's speed. So this formula should be written as $O(T(n) )=n^{k}$ (or we may neglect an argument, as we know that the T is a function of one argument, and then the formula going to look like $O(T)=n^{k}$). It is pretty confusing. If in the begin  of a lectures I could somehow guess what the author talking about, then some videos later I stuck; I have no time to understand all the calculations of the autor(mostly because I am not Englishman), then I am trying to look at a pictures, and all the mess just blows my mind!","['education', 'discrete-mathematics', 'algorithms']"
808667,How can I calculate the integral??,"I have to solve the following problem:
$$u_t=u_{xx}, x \in \mathbb{R}, t>0$$
$$u(x,0)=f(x)=H(x)=\left\{\begin{matrix}
1, x>0\\ 
0, x<0
\end{matrix}\right.$$ I have done the following: We use the method seperation of variables, $u(x,t)=X(x)T(t)$. I have found that the eigenfunctions are $X_k(x)=e^{ikx}, \lambda=k^2, k \in \mathbb{R}$ $T_k(t)=e^{-k^2t}$ $$u(x,t)=\frac{1}{2\pi} \int_{-\infty}^{+\infty}{\widetilde{f}(k) e^{ikx} e^{-k^2t}}dk$$ $$u(x,0)=H(x) \Rightarrow H(x)=\frac{1}{2 \pi} \int_{-\infty}^{+\infty}{\widetilde{f}(k)e^{ikx}}dk$$ $$\widetilde{f}(k)=\int_0^{\infty}{e^{-ikx}}dx=\frac{1}{ik}$$ $$u(x,t)=\frac{1}{2 \pi} \int_{-\infty}^{+\infty}{\frac{1}{ik} e^{ikx} e^{-k^2t}}dk$$ Is this correct so far?? How can I calculate the last integral?? EDIT: I found that the solution of the problem has the following form:
$$u(x,t)=\frac{1}{2 \pi} \int_{-\infty}^{+\infty}{\frac{1}{ik} e^{ikx}e^{-k^2t}}dk$$ Then, I derivated this in respect to $x$:
$$u_x=\frac{1}{2 \pi} \int_{-\infty}^{+\infty}{\frac{1}{ik} e^{ikx}e^{-k^2t}}dk$$ Then, from the formula:
$$\int_{-\infty}^{+\infty}{e^{ikb}e^{-k^2a}}dk=\sqrt{\frac{\pi}{a}}e^{-\frac{b^2}{4a}}$$ we get the following:
$$u_x=\frac{1}{2 \pi} \sqrt{\frac{\pi}{t}}e^{-\frac{x^2}{4t}}=\frac{1}{\sqrt{4 \pi t}} e^{-\frac{x^2}{4t}}$$ So to find $u(x,t)$, I have to calculate the integral $\displaystyle{\int u_x dx}$ but with what limits??","['ordinary-differential-equations', 'partial-differential-equations']"
808678,Evaluate $\int_0^\infty\frac{\ln x}{1+x^2}dx$,Evaluate $$\int_0^\infty\frac{\ln x}{1+x^2}\ dx$$ I don't know where to start with this so either the full evaluation or any hints or pushes in the right direction would be appreciated.  Thanks.,"['definite-integrals', 'improper-integrals', 'calculus', 'integration']"
808679,Compact subsets of an inverse limit of topological spaces,"Denote by $\mathcal C(X)$ be the space of compact subsets of a topological space X.
Let $(X_\alpha)_\alpha$ be an inverse system of topological spaces, then $(\mathcal C(X_\alpha))_\alpha$ is also an inverse system with the induced bonding maps (a compact subset goes to its image). I'm trying to understand why $\mathcal C(\lim_\leftarrow X_\alpha)=\lim_\leftarrow\mathcal C(X_\alpha)$. There is a map $T:C(\lim_\leftarrow X_\alpha)\to\lim_\leftarrow\mathcal C(X_\alpha)$ given by $T(H)=(p_\alpha(H))_\alpha$ where $p_\alpha:\lim_\leftarrow X_\beta\to X_\alpha$ is the projection.
What I don't understand is why is this map onto ? Given an element $(H_\alpha)_\alpha\in \lim_\leftarrow\mathcal C(X_\alpha)$, we have to find a compact subset of $\lim_\leftarrow X_\alpha$ s.t. its projection of the $\alpha$th coordinate is $H_\alpha$ for all $\alpha$. I tried to take $H=\lim_\leftarrow H_\alpha=\Pi H_\alpha \cap \lim_\leftarrow X_\alpha$, but I can't prove that indeed $p_\alpha (H)=H_\alpha$. Can someone explain this to me, or point me in the right direction? thanks!","['general-topology', 'category-theory']"
808688,Proof of a trigonometric identity,"I need to prove that:
$$1+\cos(a+b)-\cos(a-b)=\cos^2(a)+\cos^2(b)$$ I tried to start from both ways but it always took me back to the same one again 
I tried also to prove that their - equal zero but I didn't work also",['trigonometry']
808765,Find $f$ such as $f(x) = \sum_{n=1}^\infty \frac{f(x^n)}{2^n}$,"Find $f \in C^0([0,1] , \mathbb{R})$ such as $$f(x) = \sum_{n=1}^\infty \frac{f(x^n)}{2^n}$$ My try : Constant functions work fine. We can notice : $$f(x) = \frac{f(x)}{2}+\sum_{n=2}^\infty \frac{f(x^n)}{2^n}$$ so
$$f(x) = \sum_{n=1}^\infty \frac{f(x^{n+1})}{2^n}$$
which make me believe constant functions are the only one to please the problem. Any help will be appreciated.","['sequences-and-series', 'functions']"
808784,Induction: Prove that it is possible to seat people in a circle so that everyone sits beside a friend,"Use induction to prove the following: If each person in a group of $n$ people is a friend of at least half the people in the group, then prove that it is possible to seat them in a circle so that every one sits next to a friend of his/hers. No idea how to solve this problem.","['induction', 'discrete-mathematics']"
808793,How to embed a total ordering into the real field.,"Let $(S,<_S)$ be a total ordering with $card(S)\leq card(2^{\aleph_0})$. Does there exist a subset $A$ of the real numbers such that $(A,<_A)$, being a total ordering, is isomorphic to $(S,<_S)$? I have tried with the following: Suppose that $S'$,a subordering of $S$, has already been mapped isomorphically into a subset of R through $U$. Pick an element $p$ in $S-S'$. 2.1 If $p$ is a limit point of $S'$ in the order topology of $(S,<_S)$, there will be a sequence ${p_n}$ in $S'$ which converges to $p$ in $S$. Then $U(p_n)$ should be a converging sequence in R so that I could define $U(p)$ to be the limit of $U(p_n)$. 2.2 If $p$ is an isolated point, then simply define $U(p)$ to be the midpoint of $sup\{U(q):q<_Sp\}$ and $inf\{U(q):p<_Sq\}$. Is my reasoning correct?If $S$ is countable then I could well-order it and use recursion to complete the construction, but I am still wondering how to deal with the limit ordinal case if $S$ is uncountable. Thank you.","['elementary-set-theory', 'real-analysis', 'analysis']"
808812,Countably infinite set and uncountable collection of subsets,How can I Prove or disprove that every uncountable collection of subsets of a countably infinite set must have two members whose intersection has at least 2010 elements?,['elementary-set-theory']
808828,"Questions--Heat equation with $x>0,t>0$","I have the following problem: $$u_t=u_{xx}, x>0, t>0$$
$$u(x=0,t)=0 , t>0$$
$$u(x,t=0)=f(x), x>0$$ The solution of the problem is: 
$$u(x,t)=\int_0^{+\infty} a(k) \sin(kx) e^{-k^2t} dk$$ $$u(x,0)=f(x)=\int_0^{+\infty} a(k) \sin(kx) dk$$ $$\sin(k'x) f(x)= \sin(k'x) \int_0^{+\infty} a(k) \sin(kx) dk \Rightarrow \int_{0}^{\infty}\sin(k'x) f(x) dx =  \int_0^{+\infty} a(k) \sin(kx) \sin(k'x) dk dx$$ We know the integral: $$\int_{-\infty}^{+\infty} e^{-i(k-k')x}dx= 2 \pi \delta(k-k')$$ $$e^{-ikx} e^{ik'x}=\cos(kx) \cos(k'x)+\sin(kx) \sin(k'x)+ i(\cos(kx) \sin(k'x)-sin(kx) \cos(k'x)) $$ Why do we know that $e^{-ikx} e^{ik'x}=$ is real,so  $\cos(kx) \sin(k'x)-sin(kx) \cos(k'x)=0$ ? Also, why $\int_{-\infty}^{+\infty} (\cos(kx) \cos(k'x)+\sin(kx) \sin(k'x))dx=2 \int_{\infty}^{+\infty} \sin(kx) \sin(k'x) dx$ ?","['ordinary-differential-equations', 'partial-differential-equations']"
