question_id,title,body,tags
972902,Cyclic representation on $L^2(\mu)$,"Show that if $(X,\Omega,\mu)$ is a $\sigma-$ finite measure space and $H=L^2(\mu)$, then $\pi:L^\infty(\mu)\to B(H)$ defined by $\pi(\phi)=M_\phi$ is a cyclic representation and find all the cyclic vectors. I do not have any idea about it. I'm studying Functional analysis by myself and it is an Example of Conway's Functional Analysis. Please help me to understand it. Thanks in advance.","['operator-theory', 'banach-algebras', 'representation-theory', 'functional-analysis']"
972937,Expected number of times until getting two 6's,"What is the expected number of times we need to roll a die until we get two consecutive 6's? By definition, it is $\sum_{i=1}^\infty i\cdot Pr[X=i]$. If we need $i$ rolls, that means the last two rolls are 6's. But how do we compute the probability that no two consecutive 6's occur before that?","['probability', 'expectation']"
972945,Identifying a Hilbert space with its dual,"Let $H$ be a Hilbert space. Often people say ""we identify $H$ with its dual $H^*$ with the Riesz representation theorem"". I know there is a map $R:H^* \to H$ which is ismometric and isomorphic. So by identifying, given an element $f \in H^*$, do we just replace $f$ with $Rf \in H$ in all formulaes and texts? Really my question is, given a Hilbert space $H$, how do we know whether it has been identified with its dual or not?","['hilbert-spaces', 'functional-analysis']"
972991,Lower bound for expectation of squared log?,Is there a (tight) lower bound for $\mathbb{E}[(\log x)^2]$ where $x$ is a non-negative random variable? Jensen's inequality doesn't seem to apply here since the squared of a log isn't convex. Thanks!,"['statistics', 'probability', 'expectation']"
973009,Black and white beads on a circle,"There are $n$ beads placed on a circle, $n\ge 3$. They are numbered in random order as viewed clockwise. Beads for which the number of the previous bead is less than the number of a next bead are painted in white color,and others - in black. Two colourations that can be made equal by rotation are considered identical. How many different colourations can occur? I've write a programm and for $n=3...11$ I've got answers $2 , 1 , 6 , 7 ,  18 , 25 , 58 , 93 , 186$",['combinatorics']
973012,Are stable manifold for gradient flows embedded submanifold?,"Generally, the stable manifolds $W^s(p)$ of a diffeomorphism $\phi:M\to M$ is no embedded submanifold. The injective immersion
$$ E^s:T_p^sM\to M $$
does not need to be a homeomorphism onto its image $W^s(p)$. This is a popular image that illustrates this fact: However, in the case that the diffeomorphism $\phi$ comes from a gradient flow, I think the stable manifold must be a submanifold. (More generally I think this is the case when there exists a strict Lyapunov function). Can anybody tell me how to prove this?","['dynamical-systems', 'manifolds', 'differential-geometry', 'analysis']"
973013,"If matrix $AB=A$, does it mean B must be an identity matrix?","If matrix multiplication $AB=A$, does it mean $B$ must be an identity matrix?
If not, why? What conditions? $A$ is not a zero matrix.","['matrix-equations', 'matrices', 'matrix-calculus', 'linear-algebra']"
973031,Integer functions,"For $x>0$ consider the following three functions: $$\begin{align}
f(x)&=x+1;\\g(x)&=2x;\\t(x)&=3x
\end{align}$$ Let $A(x)$ be the minimum number of operations using only functions $f(x)$ and $g(x)$ needed to get $X$ from $0$. Let $B(x)$ be the minimum number of operations using only functions $f(x) $ and $t(x)$ needed to get $X$ from $0$. Endless or no set of numbers for which $A(x)<B(x)$.",['functions']
973037,Log base 10 not equal to log while differentiating?,"I was looking at sample questions from my textbook and I came across something interesting that I need a little help understanding The question was to find the derivative of: $\log_{10} (\frac{7}{\sqrt {x+3}})$ My solution was: $\frac{1}{2(x+3)}$ My problem is that when I later tried to check my answer online, it resulted in: $\frac {1}{2(x+3)*\log(10)}$ So this led me to believe that, in the case of deriving with logs, $\log_{10}$ is not the same thing as $\log$, yet in every other scenario it is the same thing. My question is, are these two cases actually different, is one of the results wrong, or are both results the same with the second one being a simple expansion (which I don't think it is but I could be wrong). Any help will be great, this is a really interesting case. Thank you!","['logarithms', 'calculus', 'derivatives']"
973049,Does there exist an $a_0$ such that the sequence $a_{n+1} = 2a_n + 1$ is prime for all $n \ge 0$?,I believe I see that $a_n = 2^n(a_0+1) - 1$ but am somewhat uncertain where to proceed afterwards. I am a complete beginner at number theory and would appreciate it if someone could point me in the right direction--surely there is some obvious argument I am missing.,"['prime-numbers', 'elementary-number-theory', 'number-theory']"
973073,Roots of simultaneous power sum equations (numerically or otherwise),"I'm a physicist, and I've come across a problem in my research where I need to solve a set of equations looking like (e.g. in 3D) $$r_1 + r_2 + r_3 = k_1$$
$$r_1^2 + r_2^2 + r_3^2 = k_2$$
$$r_1^3 + r_2^3 + r_3^3 = k_3$$ Where the $\{k_n\}$ are known and the $\{r_n\}$ are the roots I need to solve for. The roots can be complex, but will always appear in conjugate pairs (by the way I construct these things in the first place). Ideally I need to generalise this to the nth case (where there will always be $n$ unknowns and $n$ equations following the pattern above). Numerical solutions are fine, as long as they're reasonably cheap. If it's the case that there isn't always a unique solution to these things, then I may have to rethink my strategy for the problem as a whole. Thanks in advance.","['symmetric-polynomials', 'algebra-precalculus', 'systems-of-equations']"
973079,"Dominated convergence for sequences with two parameters, i.e. of the form $f_{m,n}$","Let $f_{m,n}(x)$ be a sequence (dependent on $m$, $n$) of Lebesgue integrable functions on $\mathbb{R}$. Suppose that $f_{m,n}(x)\to 0$ as $m,n\to+\infty$, for almost $x\in\mathbb{R}$; in addition, $\left|f_{m,n}(x)\right|\le g(x)$ for all $m,n\in\mathbb{N}$, for all $x\in\mathbb{R}$, where $g\in L^1(\mathbb{R})$. Can we apply the Lebesgue dominated convergence theorem to conclude that $\int_\mathbb{R} {{f_{m,n}}\left( x \right)dx}  \to 0$ as $m,n\to+\infty$? (Here notice that $a_{m,n}\to 0$ iff for all $\forall\varepsilon>0$, $\exists N\in\mathbb{N}$: $\forall m,n\ge N\Longrightarrow|a_{m,n}|<\varepsilon$).","['measure-theory', 'real-analysis']"
973094,Prove that the kernel of a group homomorphism $\phi$ is a subgroup and that $\phi$ is injective,"I am solving the following exercise: Let $\phi : G_1 \rightarrow G_2$ be a homomorphism (where $G_1$ and
  $G_2$ are groups) and $\ker \phi := \{ g \in G_1 \mid \phi(g) = e \}$ now I have to prove that a) $\ker \phi$ is a subgroup of $G_1$, b) $\phi$ is injective if and only $\ker \phi = \{ e \}$ My Problem: Until yet we have not really covered the topic of homomorphism between groups in our lectures. Anyhow I looked it up on wikipedia and found the definition for a subgroup as the following: $(U, \circ)$ is a subgroup of $(G, \circ)$ if $U$ is not an empty set. Therefore: $a,b \in U \Rightarrow a \circ b \in U$ $a \in U \Rightarrow a^{-1} \in U$ $a,b \in U \Rightarrow a \circ b^{-1} \in U$ so I began to work with these definitions. I somehow managed to prove what I'm supposed to but I'm not sure if I did it the right way. I would be very thankful about some additional words to my attempt and also corrections. Thank you a lot in advance. My Attempt: a) $\ker \phi$ is a subgroup of $G_1$. So we can take two elements $x,y \in \ker \phi$ which are $x := \phi(g_1) = e $ and $y := \phi(g_2) = e $ and show that $x^{-1}$ and $x \circ y$ $\in$ $\ker \phi$. Since $x\circ x^{-1} \in \ker \phi$ we can say: $x\circ x^{-1} = e \ \Leftrightarrow  \ \overbrace{\phi(g_1)}^{= \ e} \circ x^{-1} = e \ \Rightarrow \ x^{-1} = e \ \Rightarrow \ x^{-1} \in \ker \phi$. It must also be true that $x \circ y \in \ker \phi$ this is easily shown by: $x \circ y \ \Leftrightarrow \ \overbrace{\phi(g_1)}^{= \ e} \circ \overbrace{\phi(g_2)}^{= \ e} = e \ \Rightarrow \ x \circ y \in \ker\phi$ b) To show that $\phi$ is injective when $ \ker\phi = \{ e_{G_1} \}$ we must show that $ \ker\phi = \{ e_{G_1} \}$ has only one fiber which then has to be $\phi^{-1}(e_{G_2})$. So we can take two elements $g_1,g_2 \in G_1$ and if $\phi(g_1) = \phi(g_2) = e \ \Rightarrow \ g_1 = g_2$ we can state that $\phi$ with $\ker \phi = \{e\}$ is injective.","['group-theory', 'abstract-algebra']"
973101,How to generate points uniformly distributed on the surface of an ellipsoid?,"I am trying to find a way to generate random points uniformly distributed on the surface of an ellipsoid. If it was a sphere there is a neat way of doing it: Generate three $N(0,1)$ variables $\{x_1,x_2,x_3\}$, calculate the distance from the origin $$d=\sqrt{x_1^2+x_2^2+x_3^2}$$ and calculate the point $$\mathbf{y}=(x_1,x_2,x_3)/d.$$ It can then be shown that the points $\mathbf{y}$ lie on the surface of the sphere and are uniformly distributed on the sphere surface, and the argument that proves it is just one word, ""isotropy"". No prefered direction. Suppose now we have an ellipsoid $$\frac{x_1^2}{a^2}+\frac{x_2^2}{b^2}+\frac{x_3^2}{c^2}=1$$ How about generating three $N(0,1)$ variables as above, calculate $$d=\sqrt{\frac{x_1^2}{a^2}+\frac{x_2^2}{b^2}+\frac{x_3^2}{c^2}}$$ and then using $\mathbf{y}=(x_1,x_2,x_3)/d$ as above. That way we get points guaranteed on the surface of the ellipsoid but will they be uniformly distributed? How can we check that? Any help greatly appreciated, thanks. PS I am looking for a solution without accepting/rejecting points, which is kind of trivial. EDIT: Switching to polar coordinates, the surface element is $dS=F(\theta,\phi)\ d\theta\ d\phi$ where $F$ is expressed as
$$\frac{1}{4} \sqrt{r^2 \left(16 \sin ^2(\theta ) \left(a^2 \sin ^2(\phi )+b^2 \cos
   ^2(\phi )+c^2\right)+16 \cos ^2(\theta ) \left(a^2 \cos ^2(\phi )+b^2 \sin
   ^2(\phi )\right)-r^2 \left(a^2-b^2\right)^2 \sin ^2(2 \theta ) \sin ^2(2 \phi
   )\right)}$$","['stochastic-processes', 'analytic-geometry', 'ellipsoids', 'random-variables', 'probability']"
973111,Differential in Lie groups,"I am trying to make sense of the Lie group machinery and relate it to the calculus. Suppose that $\psi(t)=\phi(s)\phi(t), s, t \in I$. Where $\phi(t)$ is a one-parameter subgroup of the Lie group $G$ and $I$ is an open interval containing $0$. So we have $\phi: \mathbb R \to G$ (a smooth homomorphism). Now using simple calculus we can find the differential of $\psi(t)$, treating it as a matrix, here each element is a function of $t$. Using the above, and for a fixed $s$, we can say as usual in the calculus $d\psi(t)=\phi(s)d\phi(t)$. But if instead of calculus we use the Lie group definitions I cannot easily make sense of the differential. Based on the definition if $ \psi(t): \mathbb R \to G, t \in \mathbb R ,\psi(t) \in G$ (assuming the group structure of the manifold) then $d\psi(t)$ is $d\psi(t): T_t\mathbb R \to T_{\psi(t)}G$ here, by definition we have $d\psi(t)(v)(g)=v(g \circ \psi) \in T_{\psi(t)}G, v\in T_t\mathbb R, g\in \mathcal F(G) $ Here $\mathcal F$ is the set of all smooth real-valued function on $G$. So $g: G \to \mathbb R$ Now my question is that how we can use the above definition of differential in Lie groups to say that $d\psi(t)=\phi(s)d\phi(t)$. or more explicitly, $d\psi(t)(v)(g)=\phi(s)d\phi(v)(g)$","['lie-algebras', 'differential-geometry', 'manifolds', 'lie-groups', 'group-theory']"
973137,Solution of Definite integral:$\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \frac{1}{\sqrt{x^{2}+y^{2}+z^{2}}}e^{i(k_{1}x+k_{2}y)}dxdy$,"I'm trying to evaluate the following two dimensional integral: $\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \frac{1}{\sqrt{x^{2}+y^{2}+z^{2}}}e^{i(k_{1}x+k_{2}y)}dxdy$ The paper that i'm following reports the following solution: $\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \frac{1}{\sqrt{x^{2}+y^{2}+z^{2}}}e^{i(k_{1}x+k_{2}y)}dxdy = \frac{1}{\sqrt{k_{1}^{2}+k_{2}^{2}}}e^{-\sqrt{k_{1}^{2}+k_{2}^{2}}z}$ it might be useful using the following definitions:
$r = \sqrt{x^{2}+y^{2}+z^{2}}$
and
$k = \sqrt{k_{1}^{2}+k_{2}^{2}}$ I think this should follow from some kind of substitution;
anyway i tried to solve it with mathematica but i'm not getting any solution... Any idea on how to obtain the result shown by the authors?
Thanks in advance","['definite-integrals', 'fourier-analysis', 'integration']"
973153,Prove $g(x)=\sqrt{f(x)}$ is regulated,"Let $f:[a,b]→\mathbb{R}$ be regulated and non-negative. Prove that $g:[a,b]→\mathbb{R}$ defined by $g(x)=\sqrt{f(x)}$ is regulated. A function $f:[a,b]\to\Bbb R$ is a regulated function if $\forall$ $\varepsilon>0$ $\exists$  a step function $\varphi:[a,b]\to\Bbb R$ such that $\Vert f-\varphi\Vert<\varepsilon$. I've tried to use the definition of a regulated function but haven't been able to make any progress. Is there a way of using the fact that a linear combination of regulated functions is regulated? Or am I not even close? @Arthur Is this attempt at all correct or at least along the lines of what you mean: We have a step function $\varphi_f$ for $f$ and $\varepsilon_f$. Let $\sqrt{\varphi_f}=\varphi_g$. We know $\Vert f-\varphi_f\Vert<\varepsilon_f \implies \Vert f-\varphi_g^2\Vert<\varepsilon_f$ $\implies \Vert (\sqrt{f}+\varphi_g)(\sqrt{f}-\varphi_g)\Vert<\varepsilon_f$ $\implies \Vert (g+\varphi_g)\Vert \Vert(g-\varphi_g)\Vert<\varepsilon_f $ $\implies \Vert(g-\varphi_g)\Vert < (\varepsilon_f)/\Vert (g+\varphi_g)\Vert $ So letting $\varepsilon_g = (\varepsilon_f)/\Vert (g+\varphi_g)\Vert $ proves that $\Vert(g-\varphi_g)\Vert < \varepsilon_g$ and hence $g(x)$ is regulated.","['calculus', 'functional-analysis', 'real-analysis', 'analysis']"
973158,"How prove $f(x)\le f(b)$. if $f(x)$ is continuous everywhere in [a,b], differentiable except at a countable number of points in [a,b]","QUestion: let $f(x)$  is continuous everywhere in [a,b], differentiable except at a countable number of points in [a,b].and  $f'(x)\ge 0$ show that
  $$f(x)\le f(b)$$ This problem is from this: [china BBS] http://www.duodaa.com/?qa=4999 /一个证明题 My idea: if $f'(x)$ is integrable,Assmue that $f(x)$ is not derivative on $x_{i},x_{i}>x_{i-1},i=1,2,3,\cdots,n$,then we have
$$0 \le \int_{x_{i-1}}^{x_{i}}f'(x)dx=f(x_{i})-f(x_{i-1}),i=1,2,\cdots, x_{0}=a,x_{n}=b$$
then have
$$f(b)\ge f(x)$$ But I know this methods is not true,so How prove it? Thank you","['inequality', 'real-analysis', 'analysis']"
973163,Uniqueness of Duals in a Monoidal Category,"Given a monoidal category ${\cal C}$, and $X \in {\cal C}$, we define a left dual of $X$ to be an object $X^*$ such that there exist morphisms $\epsilon:X^* \otimes X \to I$, and $\eta:I \to X \otimes X^*$, for $I$ the identity of the category, satisfying certain axioms, see here for details. When is the left of a dual unique up to isomorphism, and when is this isomorphism unique.","['category-theory', 'linear-algebra', 'abstract-algebra', 'monoidal-categories']"
973196,Basic Open set question,"In $\mathbb{R}^2$, if I have an open set, call it $U$, and a point $u_0\in U$. Is the set $U^{'} = U - \{ u_0 \}$ is open as well? Or perhaps it is non-open & non-closed? My intuition is that it still remains open, but I'm not sure on how to formalize this. Is it enough if I say that since for each $u\in U$ exists a neighbourhood $V_u$ such that $V_u\subseteq U$, so it is also true for each $u\in U- \{u_0 \}$, thus making $U^{'}$ open as well? Not sure if this is formal enough (or perhaps I missed something and it's wrong) And assuming this is correct, can I remove more than one point? i.e. remove any $u_0,...,u_n$ from $U$ and still it will remain an open set? Thanks!","['general-topology', 'real-analysis']"
973199,Linearly dependent eigenvectors of a matrix,"I read a theorem that says squared matrix $A_{n\times n}$ is diagonalizable iff there is a set of $n$ linearly independent vectors ,each of which is an eigen-vector of $A_{n\times n}$ . I understand from the theorem that if $A_{n\times n}$ has at least two dependent eigen-vectors
then $A_{n\times n}$ is not diagonalizable. Is it true?and according to this I want to understand when a matrix has linearly dependent eigenvectors ? (eigenvectors is a set of vectors)
In other words if there is'nt any set of linearly independent set of eigenvectors for a matrix, then does that mean that the matrix is not diagonalizable?","['matrices', 'linear-algebra']"
973222,"Prove that if both $ab$ and $a + b$ are even, then both $ a$ and $b$ are even.","Let $a$ and $b$ be integers. Prove that if both $ab$ and $a + b$ are even then
  both $a$ and $b$ are even. I've seen some solutions but they're not worded in a very simple way. Any help would be much appreciated!",['discrete-mathematics']
973242,Sum of cubes proof [duplicate],This question already has answers here : Proving $1^3+ 2^3 + \cdots + n^3 = \left(\frac{n(n+1)}{2}\right)^2$ using induction (16 answers) Closed 9 years ago . Prove that for any natural number n the following equality holds: $$ (1+2+ \ldots + n)^2 = 1^3 + 2^3 + \ldots + n^3 $$ I think it has something to do with induction?,"['discrete-mathematics', 'proof-writing']"
973346,Proper use of implication and equivalence,"I think I have a pretty good understanding of implication and equivalence (I also found this question ), but there are some things I am unsure about. First of all, in maths class in high school, when for example given a quadratic function $f(x)$ with the derivative $f'(x)=2ax+b$, to compute the extremum of $f$ we would write something like $$
f'(x)=0 \\
\Downarrow \\
2ax+b=0 \\
\Updownarrow \\
x=-\frac{b}{2a}
$$ formatting notwithstanding. I understand why the second implication is an equivalence, but I'm not so sure why the first is not. (I also think I have seen Sal Khan on Khanacademy write a double arrow in cases like this, though I'm not sure.) I guess my confusion springs from the following, considering the growth of a linear function $f(x)=ax+b$ in the interval $\Delta x$: $$
f(x+\Delta x)=a(x+\Delta x)+b=ax+b+a\Delta x=f(x)+a\Delta x
$$ (By the way, this was printed in my sophomore textbook.) Here, in the third step, the function $f(x)$ is inserted into the expression. Then, if these are equal (which they obviously are), why is the first implication above not an equivalence? Secondly, I previously asked this question , but later stumbled upon these notes . I'm a bit confused about the use of arrows on page 29. First, this: Here, I understand the use of the arrows to mean, that each expression on the right sides are equivalent to each other, the last expression being equivalent to the first one on the right, which in turn is equivalent to the left hand expression. Then, this: Here, I assume the three right hand expressions are equivalent? In that case, what I don't understand is the use of single arrows. I assume the formatting could imply that the first expression implies the three right hand expressions and that the right hand expressions are not explicitly stated to be equivalent About the question I asked, I was also wondering if this would be an acceptable way of formatting for instance the solving of an equation (like the quadratic above)? I would very much appreciate if someone could shed some light on this. Thanks.","['logic', 'algebra-precalculus']"
973377,Solving the differential equation: $f(x)yy'=(y')^2-0.5$,I am trying to solve this equation: $f(x)yy'=(y')^2-0.5$ I have already tried traditional methods... Any ideas?,"['ordinary-differential-equations', 'calculus']"
973392,l'Hôpital and it's use in derivation,"In for example $$\lim_{x\rightarrow 0} \frac{e^{ax} - 1 - ax}{1 - \cos x}$$ We would use l'Hôpital rule and derive it twice to get $a^2$ How do you see this when just looking at the given function, when do you know you should use l'Hôpital and can someone give a real simple explanation on why it works?","['exponential-function', 'calculus', 'functions']"
973396,"an urn contains two green, three yellow and five red balls.","An urn contains two green, three yellow and five red balls. We draw one ball at random and put it aside. then we draw another ball until there are no more balls. Find the probability of drawing at first the two green balls, then the three yellow balls and finally the red balls My answer: We have two green balls so the probability of pulling two greens is: $\large\frac{1}{10} \cdot \frac{1}{9}$ for the yellow balls then we have $\large\frac{3}{8} \cdot \frac{2}{7} \cdot \frac{1}{6},$ and for the red ones we have: $1$ (since there are only $5$ balls left and there are five red balls)
if we multiply all of these we get: $0.00019$ correct answer is: $0.0004$",['probability']
973411,Properties of the 'forgotten' symmetric polynomials,"In I.G. Macdonald ""Symmetric Functions and Hall Polynomials"" pg.22, the forgotten symmetric functions $f$ are introduced very briefly as the result of applying an involution $\omega$ to the monomial symmetric functions $m$ . This involution $\omega$ is defined earlier as the transformation of elementary $e$ into complete homogeneous symmetric $h$ polynomials. The relation is $\sum_{r=0}^n (-1)^r e_r h_{n-r} = 0$ for $n > 0$ . It obviously works on $e$ and $h$ with integer arguments. My specific question is whether the $f$ are (only?) defined as taking a partition as argument (like the monomials $m$ and the Schur $s$ ), or if they obey $f_{a,b,c}=f_a f_b f_c$ like $e$ , $h$ and $p$ do. If the $f$ are only defined for partition argument, than it would suffice to substitute $f$ for $m$ and $h$ for $e$ in the following example: $m_{2,1}=e_{2,1}-3 e_3$ and thus obtain $f_{2,1}=h_{2,1}-3 h_3$ . Since the definition of $f$ is only based on the (formal) relation to $m$ by involution $\omega$ , it is unclear to me what kind of reasoning leads to the correct answer.","['symmetric-polynomials', 'symmetric-functions', 'algebraic-combinatorics', 'combinatorics']"
973422,Expected number of cycles in permutation [duplicate],"This question already has answers here : Name Drawing Puzzle (3 answers) Closed 8 years ago . Consider a random permutation of $1,2,\ldots,n$. What is the expected number of cycles in it? I thought about using linearity of expectation, but here it's not clear how we can break down the main random variable into different ones.","['expectation', 'probability', 'combinatorics']"
973444,"Dose ""optional stopping theorem"" imply ""optional sampling theorem""?","Suppose $X$ is a martingale, $\tau$ and $\sigma$ are two stopping times which satisfy (a) $\sigma\le\tau$ and (b)the ""optional stopping theorem"" holds,that is to say: $$\mathbb E[X_\sigma]=\mathbb E[X_0],\mathbb E[X_\tau]=\mathbb E[X_0] ---(1)$$ Could we get the ""optional sampling theorem"": $$\mathbb E[X_\tau|\mathcal F_\sigma]=X_\sigma ---(2)$$ by(1)? What I really want to say is the following : There are many conditions leads to (1) such as : $\tau$ is bounded or $X$ is UI or $\mathbb E[\tau]<\infty $ and bounded increments or $|X_{\tau\wedge n} | $ is bounded or $|X|$ is bounded. etc. (apply bounded convergence theorem /dominate convergence theorem to interchange the
limit and expectation in $ \lim\mathbb E[X_{\tau\wedge n}]=\mathbb  E[X_0]$ ) But I have only seen two conditions leads to (2): $\tau$ is bounded or $X$ is UI I wonder the other conditions can leads to (2),moreover whether any conditions lead to (1) can lead to (2) and whether (1) itself can lead to (2).","['probability-theory', 'stochastic-processes', 'stopping-times', 'martingales']"
973470,Graph of measurable function has measure 0 in the product measure space,"I have the following homework problem: Let $(X, M , \mu )$ be a $\sigma$-finite measure space. Show that the graph of any measurable function $f: X \rightarrow \mathbb{R}$ has measure 0 in the product measure space $(X×\mathbb{R},M⊗B_{\mathbb{R}}, \mu\times\lambda)$. Where $\lambda$ is the Lebesgue measure on $\mathbb{R}$. My idea was to cover the graph by countably many generalized rectangles of arbitrarily small measure or, in other words, to prove that the graph has outer measure 0 but I'm not sure how to go with that idea since we'll probably need $\mu(X)< \infty $. Any help will be appreciated.","['measure-theory', 'real-analysis']"
973509,Proving or disproving matrix $A+B$ is invertible,"Given $A, B \in M_n (\Bbb F)$ , where $A$ is $k$ -nilpotent and $B$ is invertible, is $A+B$ also invertible? I was having trouble on how to prove this, and then I thought maybe this statement is incorrect, but couldn't find a counter example. Perhaps someone can assist?","['matrices', 'linear-algebra', 'examples-counterexamples']"
973559,Outer Product of Two Matrices?,"How would I go about calculating the outer product of two matrices of 2 dimensions each?  From what I can find, outer product seems to be the product of two vectors, $u$ and the transpose of another vector, $v^T$. As an example, how would I calculate the outer product of $A$ and $B$, where 
$$A = \begin{pmatrix}1 & 2 \\ 3 & 4\end{pmatrix} \qquad B = \begin{pmatrix}5 & 6 & 7 \\ 8 & 9 & 10\end{pmatrix}$$","['matrices', 'tensor-products']"
973603,Fix point theorem for measures? metric on space of measures?,"I have the following problem: I consider a probability space $(\Omega, \mathcal{F}, \mu)$ where $\Omega= C_0([0,1])$ (space of continuous functions on $[0,1]$ starting from 0), $\mathcal{F}$ is a $\sigma$ algebra on $\Omega$ (for instance the one generated by point-evaluations) and $\mu$ a probability measure on $(C_0([0,1]),\mathcal{F})$. Denote by $\mathcal{M}(\Omega)$ the space of probability measures on $(\Omega, \mathcal{F})$. I define a mapping from $\mathcal{M}(\Omega)$ onto itself by
$$\mu \mapsto P^\mu = \int f d\mu$$
where $f$ (btw $f$ is not trivial, so that the mapping is not trivial) is fixed and it depends also on $\mu$ in fact. I can give more details if necessary. I'm very interested in finding the measure $\mu^\ast$ such that $\mu^\ast = P^{\mu^\ast}$ if it exists and the first naiv thought was a ""fix point theorem"" but then the question is... 1) What topology/metric should I use in $\mathcal{M}(\Omega)$ so that it makes sense to consider this operator? 2) Having found a metric $d$, and proving $d(P^\mu, P^{\mu'})\leq d(\mu,\mu')$, would this imply there is $\mu^\ast$ such that $\mu^\ast = P^{\mu^\ast}$? (Banach fix point theorem) In conclusion, I'd like to find a fix point of this mapping :( Is there any idea or standard trick? Thank you very much for your kind help!","['general-topology', 'measure-theory', 'probability', 'real-analysis', 'functional-analysis']"
973631,"How to easily prove Euler's theorem, $OI^2=R(R-2r)$?","If $R$ is the circumradius and $r$ is the inradius of some triangle $ABC$, with its circumcenter being $O$ and incenter being $I$, then how to prove: 
$$OI^2=R(R-2r)$$
I have seen many mentions of this theorem, and Euler's inequality is a corollary. Wikipedia has a proof, but its quite tough to follow and long, so does anyone have a nicer proof? :)","['geometry', 'triangles', 'trigonometry']"
973637,Limit $\lim\limits_{n \to \infty} n \cdot\ln(\sqrt{n^2+2n+5}-n)$,"How should this limit be solved ?
$$\lim_{n \to \infty} n \cdot \ln(\sqrt{n^2+2n+5}-n)$$ I've tried to multiply and at the same time divide $\sqrt{n^2+2n+5}-n$ by $\sqrt{n^2+2n+5}+n$, and then make $n$ as the power of $\frac {2n+5}{\sqrt{n^2+2n+5}+n}$. But I got stuck.  I dont think it was the best idea.","['radicals', 'limits']"
973679,Proving AM-GM for the special case $n=3$,"I know the AM-GM inequality and its proof which is relatively complex, though the case for $n=2$ is quite simple. However, I don't know of any special easier proof for the case $n=3$, specifically: $$\frac{a+b+c}3\ge \sqrt[3]{abc}$$ What is the most elegant proof for this? :)","['inequality', 'algebra-precalculus']"
973692,Lyapunov-Schmidt reduction.,"Use Lyapunov-Schmidt reduction to find an expression, or
  approximation, of the set of equilibria, as a function of the
  parameter $\lambda$, of the planar vector field
  $$f(x,y,\lambda)=(\lambda + 2x + y - x^2, 2x + (1+\lambda)y - xy)$$
  near the equilibrium $(x,y) = (0,0)$ at $\lambda = 0$. So, usually, we tackle this problem by using the Implicit Function Theorem twice. Firstly, we define
$f_1(x,y,\lambda) = \lambda + 2x + y - x^2, \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ $
$f_2(x,y,\lambda) =2x + (1+\lambda)y - xy$. Since $f_1(0,0,0) = 0$ and $D_x f_1(0,0,0)=2$, so it's invertible, the implicit function theorem states that there exist open neighborhoods $U \subseteq \mathbb{R}$ and $V \subseteq \mathbb{R}^2$ of $0$ and a function $x^*: V \to U$ such that $f_1(x,y,\lambda) = 0 \iff x^*(y,\lambda) = x$, with $x^*(0,0) = 0$. Furthermore, $D_y x^*(0,0) = -(D_x f_1 (0,0,0))^{-1} D_y f_1 (0,0,0) = -(2)^{-1}1 = -\tfrac{1}{2}$. Now, we substitute $x^*(y,\lambda)$ for $x$ in $f_2$, and again we want to apply the IFT to express $y$ in terms of $\lambda$. So we define $g(y,\lambda) = f_2(x^*(y,\lambda), y, 
\lambda) = 2x^*(y,\lambda) + (1+\lambda)y - x^*(y,\lambda)y$. However, $D_y g(0,0) = 2D_y x^*(0,0) + 1 + 0 - D_y x^*(0,0)\cdot 0 -x^*(0,0)  =0$, so we can't apply the IFT. Is this just an ill-chosen example, or is there a different approach possible?","['bifurcation', 'ordinary-differential-equations', 'implicit-function-theorem']"
973713,Problem in the second-derivative symbol.,"The second derivative of this symbol according to the rules that we have learned the correct mathematical, I wish to know why this symbol is not used.","['calculus', 'terminology', 'derivatives']"
973747,Showing a set is a subset of another set,I need to show that $(A \cup B) \subseteq (A \cup B \cup C)$ My Work So Far: What I really need to show is that $x \in (A \cup B)$ implies $x \in (A \cup B \cup C)$ So I translated my sets into their logical expressions $x \in A \vee x \in B \longrightarrow x \in A \vee x \in B \vee x \in C$ This is where I'm stuck. How do I show membership of $A \cup B$ implies membership of $A \cup B \cup C$?,"['logic', 'proof-writing', 'discrete-mathematics', 'elementary-set-theory']"
973753,"List the elements of $\langle\frac{1}{2}\rangle$ in $(\mathbb{Q},+)$ and in $(\mathbb{Q}^*,\times)$.","List the elements of  $\langle\frac{1}{2}\rangle$ in $(\mathbb{Q},+)$ and in 
  $(\mathbb{Q}^*,\times)$. where $\mathbb{Q}^*:=\mathbb{Q}\setminus\{0\}$ My attempt: Well, I know that $\langle a\rangle=\{a^n:n\in\mathbb{Z}\}$, so $\langle \dfrac{1}{2}\rangle=\left\{\left( \frac{1}{2}\right)^n:n\in\mathbb{Z}\right\}$ Well, for $(\mathbb{Q},+), \langle\frac{1}{2}\rangle= n\cdot \frac{1}{2}$, since the group is under addition, so the elements are: $$\left\{\ldots,-\frac{3}{2},-1,-\frac{1}{2},0,\frac{1}{2},1,\frac{3}{2},\ldots\right\}$$ As for $(\mathbb{Q}^*,\times), \langle \frac{1}{2}\rangle=\left(\frac{1}{2}\right)^n$ the group is under multiplication so the elements are: $$\left\{\ldots,8,4,2,\frac{1}{2},\frac{1}{4},\frac{1}{8},\ldots\right\}$$ I was just wondering if this was correct.","['group-theory', 'abstract-algebra']"
973801,How to evaluate $\int_0^\infty \frac{1}{x^n+1} dx$ [duplicate],"This question already has answers here : Prove $\int_0^{\infty}\! \frac{\mathbb{d}x}{1+x^n}=\frac{\pi}{n \sin\frac{\pi}{n}}$ using real analysis techniques only (5 answers) Closed 9 years ago . Noticed that the integral $$\int_0^\infty \frac{1}{x^n+1} dx$$ is often approached with partial fraction decomposition, but this gets increasingly ugly as $n$ gets bigger.  Is there a neat trick to do these all in one fell swoop? Or a famous name for these integrals that I can look up for more info?","['improper-integrals', 'closed-form', 'calculus', 'integration', 'definite-integrals']"
973823,Maximum likelihood estimate of $N$ (trials) in Binomial,"Suppose, we throw a biased coin $N$ times with $p(\text{head}) = \pi$, and we observe the number of heads as $k$ (could be any number, say $k=4$ for simplicity). We are interested in to find the most likely $N$ as a function of $\pi$. The likelihood can be written as (for $k=4$), $$p(x = 4 | N,\pi) = {N\choose 4} \pi^4 (1-\pi)^{N-4}$$ I aim to calculate,$$N^* = \text{argmax}_N p(x=4|N,\pi)$$which is, it turns out, pretty hard to solve analytically for $N$ (you can try it yourself). Although it is a discrete variable, I tried to differentiate the log-likelihood wrt $N$ (since log is monotone, the result stays same) and tried to solve for $N$ which resulted in insolvable equations for me. So far so good. What makes this interesting for me is that, solving the problem for $\pi$ and finding most likely values of $\pi$ as a function of $N$, and then leaving $N$ alone seems to give the correct result. If you differentiate the likelihood (not log-likelihood) with respect to $\pi$, then set it to zero, and solve for $\pi$, you will find $\pi = 4/N$. Now choosing $N = 4/\pi$ is consistent with empirical results, it seems true; although, I couldn't calculate it via maximizing $N$ directly. Now see the figure. Blue line is the computationally calculated for maximum $N$'s for corresponding $\pi$'s and red is the $4/\pi$. I wonder how it can be true via solving for $\pi$ instead of $N$. Is there a general property about this likelihood that I am missing?","['statistics', 'maximum-likelihood', 'probability']"
973841,An empty subdifferential,"Can you give me an example of function $f$ defined on an Hilbert space, real valued (extended with $+ \infty$), lower semi continuous, convex and proper for which $\operatorname{dom}(\partial f)= \emptyset$? For $\operatorname{dom}(\partial f)$ I mean the domain of subdifferentiability of $f$.","['convex-analysis', 'hilbert-spaces', 'analysis']"
973849,What is the example of pseudo-effective divisor which is not an effective divisor,"By definition, a pseudo-effective $\mathbb{R}$-divisor is the limit of effective $\mathbb{R}$-divisors in $N^1(X)$, I was wondering what is the example of pseudo-effective divisor which is not an effective divisor? One thing I don't understand is why the limit of effective divisor may not be effective.","['algebraic-geometry', 'birational-geometry']"
973879,Statistics book recommendation [duplicate],"This question already has answers here : Recommend a statistics fundamentals book (6 answers) Closed 3 years ago . this is a subject that time and again shows up, and I've read old postings. Still:
I am taking an intro class in statistics from a Math department (Junior/Senior level). It is pretty intense (it's statistics and probability) and has a strong math component (calculus as well as linear algebra are required and used throughout the class). Exercises are done using R most of the time. The book we follow is an unedited book from the lecturer, and it's OK, but even though it's strong in the math part, it doesn't give a lot of examples nor tackles the very important aspects of why we do some things or what's the importance of this or that. I read here that a lot of people love Sheldon Ross' books, but when I looked at them, the equation parts are way below what we are doing. Is there a book like that, but more in depth in the math part? 
Thank you so much for any suggestion and why.","['statistics', 'book-recommendation', 'probability']"
973884,Prove $\sum_{k=0}^{n}\frac{\binom{n}{k}(-1)^k}{k+1}$ = $\frac{1}{n+1}$,"Any tips on where to start? I tried induction, using the inductive property of Binomial coefficients and the Mean Value Theorem for divided differences however I haven't made any progress.",['combinatorics']
973909,Ratio of expectations for integer-valued random variable,"For a nonnegative integer-valued random variable $Y$ with positive expectation, show that $$\dfrac{E[Y]^2}{E[Y^2]}\leq\Pr[Y\neq 0].$$ I suppose that the probability that $Y=i$ is $x_i$, for $i=0,1,2,\ldots$. Then the inequality becomes $$\dfrac{(x_1+2x_2+3x_3+\cdots)^2}{x_1+4x_2+9x_3+\cdots}\leq 1-x_1-x_2-\cdots$$ How can this be shown?","['probability', 'random-variables']"
973968,Why is the canonical filtration of a Brownian motion left-continuous?,"Let $\{W_t, t\geq 0\}$ be a Brownian motion, and has a.s. continuous sample paths. Let $\{\mathcal{F}^W_t, t\geq 0\}$ be the canonical filtration, i.e. $\mathcal{F}^W_t=\sigma(W_s, 0\leq s\leq t)$. So why is $\{\mathcal{F}^W_t, t\geq 0\}$ left-continuous? i.e. $\displaystyle{\bigcup_{s<t}}\mathcal{F}^W_s= \mathcal{F}^W_t$. Thank you so much!","['probability-theory', 'stochastic-processes', 'stochastic-analysis', 'brownian-motion']"
973987,Eigenvalues of 3D rotation matrix,"I'm having some trouble calculating the eigenvalues for this rotation matrix, I know that you subtract a $\lambda$ from each diagonal term and take the determinant and solve the equation for $\lambda$ but I think I'm having some trouble with trig identities since I can't seem to find the correct answer. Since this is a rotation matrix i know that the $|\lambda|$ must = 1 I just can't seem to get it to work out. Attempt: $\begin{bmatrix}\cos(\theta)&0&-\sin(\theta)\\0&1&0\\\sin(\theta)&0&\cos(\theta)\end{bmatrix}$ $\begin{bmatrix}\cos(\theta)-\lambda&0&-\sin(\theta)\\0&1-\lambda&0\\\sin(\theta)&0&\cos(\theta)-\lambda\end{bmatrix}$ $((\cos(\theta)-\lambda)*det\begin{bmatrix}1-\lambda&0\\0&\cos(\theta)-\lambda\end{bmatrix})$ - $0$ +$((-\sin(\theta)*det\begin{bmatrix}0&1-\lambda\\\sin(\theta)&0\end{bmatrix})$ $=0$ $(\cos(\theta)-\lambda)*[(1-\lambda)(\cos(\theta)-\lambda)]$ - $0$ +$(-\sin(\theta)*[0-(1-\lambda)(\sin(\theta)]$ = $0$ $(\cos(\theta) - \lambda)(\cos(\theta)-\lambda-\lambda\cos(\theta)+\lambda^2)-[\sin(\theta)(-(\sin(\theta)-\lambda\sin(\theta)]$ =$0$ $[-\lambda^3+\lambda^2+\lambda^2(2\cos(\theta))-\lambda(\cos^2(\theta))-\lambda(2\cos(\theta))+\cos^2(\theta)]+[\sin(\theta)-\lambda\sin^2(\theta)]=0$ combining like terms i end up with $[-\lambda^3+\lambda^2(1+2\cos(\theta))-\lambda(\cos^2(\theta)+2\cos(\theta)+\sin^2(\theta))+\cos^2(\theta)+\sin^2(\theta)$=$0$ I think this is correct so far, but its very possible i made a sign error or something like that. I can't seem to solve for $\lambda$ in any way that i can see.I think I'm very close, just not really good with my trig identities. Help would be greatly appreciated, Thanks a lot!","['matrix-equations', 'matrices', 'eigenvalues-eigenvectors', 'rotations']"
974006,"Let U, W be subspaces of a vector space V. Suppose U ⊆ W. Prove or disprove: U + W = W","So, I know that W + W = W. And it makes sense that there is no counterargument that the claim isn't true. So, here is my attempt: Claim: $U + W = W$ Proof: $$W \subset U + W$$ Let $w \in W$. Then $w = w + 0$, which is a sum of an element from $W$ and an element of $U$. Thus, $w \in W + U$, and hence $W \subset U + W$. $$U + W \subset W$$ By definition, $U + W = \{u + w ~:~ u \in U \text{ and } w \in W\}$. That is, every element $v \in U + W$ is of the form $v = u + w$, with $u \in U$ and $w \in W$. And since $U \subseteq W$, it follows that $u \in U$ implies $u \in W$. Thus, every element of $U + W$ is a sum of two elements of $W$, and hence $u + w \in W$ since $W$ is a subspace. Thus, $U + W \subset W$.","['linear-algebra', 'proof-verification']"
974011,The rationals as a direct summand of the reals,"The rationals $\mathbb{Q}$ are an abelian group under addition and thus can be viewed as a $\mathbb{Z}$-module. In particular they are an injective $\mathbb{Z}$-module. The wiki page on injective modules says ""If $Q$ is a submodule of some other left $R$-module $M$, then there exists another submodule $K$ of $M$ such that $M$ is the internal direct sum of $Q$ and $K$, i.e. $Q + K = M$ and $Q \cap K = \{0\}$."" Take the reals $\mathbb{R}$ as a $\mathbb{Z}$-module and $\mathbb{Q}$ as a submodule of $\mathbb{R}$. What can the submodule $K$ be in this case? If $\mathbb{Q}$ and $K$ are supposed to have trivial intersection then it seems like the only thing $K$ can be is a set of irrational numbers together with $0$, but I don't see how that can be a submodule of $\mathbb{R}$. What am I missing here? Thanks.","['modules', 'injective-module', 'abstract-algebra']"
974051,On a Proof in Galois Theory,"We have the following lemma (used in the proof of Abel's Theorem): If $\text{Char}(F)=0$, $E/F$ is a radical extension, and $K/E$ is the Galois closure of $E/F$, then $K/F$ is also a radical extension. Proof: We have
$$
F=:E_0\subset E_1\subset E_2\subset\cdots\subset E_n:=E
$$
such that $E_{i+1}=E_i(\alpha_i)$ where $\alpha_i^m\in E_i$. Define $K_{i+1}:=K_i(\sigma(\alpha_i),\sigma\in\text{Gal}(K/F))$ and $K_0:=E_0=F$. Note that $E_i\subset K_i$ for all $i$. Since $
(\sigma(\alpha_i)))^m=\sigma(\alpha_i^m)$ where $\alpha_i^m\in E_i$, we have $(\sigma(\alpha_i)))^m\in K_i$ and therefore $K_{i+1}/K_i$ is a radical extension. But $K_n=K$. Indeed, since $K$ is the Galois closure of $E/F$ and we have the tower of fields $K/K_n/E/F$, it suffices to show $K_n/F$ is a Galois extension. But this is the case because $K_{i+1}$ is the splitting field of the minimal polynomial of $\alpha_i$ over $F$ for all $i$. Hence the result. $\blacksquare$ Questions: $E/F$ must be finite, right? Where is used the hypothesis $\text{Char}(F)=0$ ? To even talk about the Galois closure, we must verify that $E/F$ is a separable extension. Why is it in this case? Why is $K_{i+1}$ the splitting field of $\alpha_i$ for all $i$ ? (Why) was it essential to consider a new tower of fields $K_i$ ? I thought that $K$ must be the splitting field of the minimal polynomial of $\alpha_{n-1}$ because of Proposition: If $E/F$ is a finite separable extension, then its Galois closure exists. Proof: We know that if $E/F$ is a finite separable extension, then $E=F(\alpha)$ for a certain $\alpha\in E$. Let $m\in F[x]$ be the minimal polynomial of $\alpha$. Then by hypothesis $m$  is separable. Let $E'$ denote the splitting field of $m$ over $F$. We know $E'/F$ is a Galois extension. Also, suppose $E\subset\tilde{E}\subset E'$ is such that $\tilde{E}/F$ is a Galois extension. Since $E'/F$ is finite, we know $\tilde{E}/F$ must be finite and we know that a finite Galois extension is, in particular, normal. Therefore we have $\alpha\in\tilde{E}$ with $m$ splitting over $\tilde{E}$. By unicity of the splitting field, $\tilde{E}=E'$. $\blacksquare$ So, does the proof actually amount to show that this splitting field is part of a radical extension of $F$ ? Thoughts: I think so because I only know that the Galois closure exists if $E/F$ is finite. I think $\text{Char}(F)=0$ tells us that there exists an extension of $F$ in which there is a primitive root of unity $\zeta$. In turn I think this shows that $x^m-\alpha_i^m\in F[x]$ has $m$ different roots, namely $\zeta^k\alpha_i$ for $k=1,2,\ldots,m$. Hence $x^m-\alpha_i^m$ is separable and this implies that the minimal polynomial of $\alpha_i$ over $F$ is separable. Hence this would show that $E/F$ is separable because $E=F(\alpha_0,\alpha_1,\ldots,\alpha_{n-1})$ and we know that an extension generated by separable elements is separable. See 2. I understand that every $\sigma(\alpha_i)$ must be a root of the minimal polynomial of $\alpha_i$ over $F$. However I don't see why we get all the roots. I see $\#\text{Gal}(K_{i+1})=[K_{i+1}:F]\geq [E_{i+1}:F]=\deg(m_{\alpha_i})$ where $m_{\alpha_i}\in F[x]$ is the minimal polynomial of $\alpha_i$, but maybe some automorphisms in $\text{Gal}(K/F)$ send $\alpha_i$ to a same image...?","['galois-theory', 'extension-field', 'abstract-algebra', 'field-theory']"
974057,Show that the set U is unbounded,I am working on a practice qualifier problem: Let $f : \mathbb{C} → \mathbb{C}$ be an entire function with $f(z) \ne 0$ for all $z ∈ \mathbb{C}$. Define U = {z ∈ C : |f(z)| < 1}. Show that all connected component of U is unbounded. I know that $f$ is holomorphic on all of $\mathbb{C}$ I'm assuming that I have to use consider $\frac{1}{f(x)}$ since $f(z) \ne 0$ anywhere.  Any thoughts would be grealy appreciated.,"['connectedness', 'complex-analysis']"
974064,Can someone walk me through how this expression simplifies to y/x?,"I am just wondering how this equation comes to be: it is from an economics problem involving marginal utilities. I have my two variables, $x$ and $y$. Intuitively, how does $$\frac{0.5\times x^{-0.5}\times y^{0.5}}{0.5\times x^{0.5}\times y^{-0.5}}= \frac{y}{x}?$$ I'm guessing that the $0.5$ and $(1/2)$ both cancel out, but I am not sure what happens next. Thank you!","['economics', 'algebra-precalculus', 'functions']"
974088,Natural isomorphisms of the forgetful functor,Let $U: \mathbf{Groups} \rightarrow \mathbf{Sets}$ be the forgetful functor. Must every natural transformation $\eta: U \rightarrow U$ be a natural isomorphism?,"['category-theory', 'abstract-algebra']"
974091,Prove that $\left(\sum_{k=1}^{n}k\right)^2=\sum_{k=1}^{n}k^3$ holds true for $n ≥ 1$,"I've been trying to figure out this proof for way too long now, I'm just not sure where to begin for the inductive step. Any guidance would be greatly appreciated.","['induction', 'summation', 'discrete-mathematics']"
974099,Probability of server cluster failure,"I have a problem where I'm trying to derive an expression for the availability of a server cluster. Suppose I have 100 servers, where each one of them individually has a probability of failure 0.05. Now, I'm trying to find an expression for the probability that at most 3 of the servers fail . Currently I have reasoned that: $Pr$(at most 3 of the servers fail) = $Pr$(0, 1, 2, or 3 of the servers fail) = $Pr$(0 fail) + $Pr$(1 fails) + $Pr$(2 fail) + $Pr$(3 fail) $Pr$(0 servers fail) = $0.95^{100} = 0.00592$ $Pr$(1 server fails) = $0.95^{99} \times 0.05^1 = 0.05623$ $Pr$(2 server fails) = $0.95^{98} \times 0.05^2 = 0.00906$ $Pr$(3 server fails) = $0.95^{97} \times 0.05^3 = 0.00702$ Thus, $Pr$(at most 3 of the servers fail) = $0.00592 + 0.05623 + 0.00906 + 0.00702 = 0.07823$. Is that correct? That probability of 7.8% seems like a really small number for having at most 3 servers fail.","['statistics', 'probability']"
974128,Eigenvalues of Hankel matrices,"Let $\mathbf{A}$ be a $4-$ dimensional symmetric matrix with real entries, whose elements are given as
\begin{equation}
\mathbf{A} = \left( \begin{array}{cccc}
a & b & c & d \\
b & c & d & e \\
c & d & e & f \\
d & e & f & g  \end{array} \right)
\end{equation}
Let $\mathbf{B}$ be another $4-$ dimensional matrix whose elements are given as
\begin{equation}
\mathbf{B} = \left( \begin{array}{cccc}
b & c & d & e \\
c & d & e & f \\
d & e & f & g \\
e & f & g & h  \end{array} \right)
\end{equation}
One can see that the elements of $\mathbf{B}$ are shifted by one with respect to $\mathbf{A}$. $\mathbf{A}$ and $\mathbf{B}$ are also called Hankel Matrices. My query is: ""Does any relationship exist between the eigenvalues of $\mathbf{A}$ and those of $\mathbf{B}$?""","['matrices', 'eigenvalues-eigenvectors', 'spectral-theory']"
974129,Proving that $\limsup_{n\to\infty}\frac{1}{n}\sum_{m=1}^n s_m\leq \limsup_{n\to\infty}s_n.$,"I am reviewing for my first year analysis exam and am stuck on a problem. Let $\sigma_n=\frac{1}{n}\sum_{m=1}^n s_m$.  I am trying to show that, if $(s_n)$ is a bounded sequence of real numbers, $$\limsup_{n\to\infty}\sigma_n\leq \limsup_{n\to\infty}s_n.$$ I understand what's supposed to happen here, but I'm having trouble showing it.  This is what I have so far: I know from a previous exercise that $s_n\rightarrow s$ implies that $\sigma_n\rightarrow s$. I'll call this fact $\star$. If $(\sigma_{n_k})_{k\in K}$ is a convergent subsequence of $(\sigma_n)$, let $(s_{n_i})_{i\in I\subseteq K}$ be a convergent subsequence of $(s_{n_k})_{k\in K}$.  We know this must exist because $(s_{n_k})_{k\in K}$ is bounded (because $(s_n)$ is bounded).  Let $I_k=\{i\in I:i \leq k\}$, $I_k^\prime =\{i\in K\setminus I:i \leq k\}$, and $m_k=|I_k|$.  Then $$\sigma_{n_k}=\frac{m_k}{n_k}\left(\frac{1}{m_k}\sum_{i\in I_k}s_{n_i}\right)+\frac{n_k-m_k}{n_k}\left(\frac{1}{n_k-m_k}\sum_{i\in I_k^\prime}s_{n_i}\right)$$
Because $s_{n_i}$ is convergent by assumption, $\frac{1}{m_k}\sum_{i\in I_k}s_{n_i}$ is convergent by $\star$. But, now I don't know where to go.  The coefficients are a little ugly and I don't know what the other sum might converge to.  What should I do next?  Or am I taking the wrong approach?","['proof-verification', 'real-analysis', 'analysis']"
974178,How to calculate the angle between 2 vectors in 3D space given a preset function,"In my application, I am attempting to connect 2 points in 3d space with a cylinder via a function taking in 2 vectors. I understand that I need the angle to apply to the cylinder.
As I understand, I can calculate this angle with the dot product of both vectors. How can I know how to apply the angle given this function: func SCNMatrix4MakeRotation(angle: Float, x: Float, y: Float, z: Float) -> SCNMatrix4 where: angle: The amount of rotation, in radians, measured counterclockwise around the rotation axis.
x: The x-component of the rotation axis.
y: The y-component of the rotation axis.
z: The z-component of the rotation axis. How should I apply the correct angle here? Parameters x, y, and z take in a number between 0 and 1. With respect to which plane does the rotation occur?","['multivariable-calculus', 'computer-science', 'calculus', 'vectors']"
974193,Why does SVD provide the least squares and least norm solution to $ A x = b $?,"I am studying the Singular Value Decomposition and its properties. It is widely used in order to solve equations of the form $Ax=b$ . I have seen the following: When we have the equation system $Ax=b$ , we calculate the SVD of A as $A=U\Sigma V^T$ . Then we calculate $x'= V \Sigma^{+}U^Tb$ . $\Sigma^{+}$ has the reciprocals ( $\dfrac{1}{\sigma_i}$ ) of the singular values in its diagonal and zeros where $\sigma_i=0$ .  If the $b$ is in the range of $A$ then it is the solution that has the minimum norm (closest to origin). If it is not in the range, then it is the least-squares solution. I fail to see how exactly this procedure always produces a $x'$ which is closest to origin if $b$ is in the range of A. (I can see the least-squares solution is an extension of this ""closest to origin"" property). From a geometric intuitive way if possible, how can we show this property of SVD?","['optimization', 'linear-algebra', 'svd', 'least-squares']"
974201,Combinatorial group theory books,"I would please like some recommendations for an introductory level book on combinatorial group theory, by which I mean a group theory book which places emphasis on generators and relations and free groups, and then discusses common concepts such as quotient groups in terms of these. Thank you.","['soft-question', 'book-recommendation', 'group-theory', 'abstract-algebra']"
974225,Zero mean but not a martingale,I am looking for a simple stochastic process which has zero mean for all $t\geq0$ but it is not a martingale. I been looking in to local martingales but having trouble keeping the mean  zero for all t. Any advise on how I should think?,"['probability-theory', 'stochastic-processes', 'examples-counterexamples', 'martingales']"
974293,Comparing cardinalities,"Why these two sets are equinumerous? $$[0,1]^\Bbb N\text{ and }\Bbb Q^\Bbb N$$ Here is my reason:
The set of rational numbers $\Bbb Q$ is countably infinite. However, $[0, 1]$ is not countable and is infinite.
So, they shouldn't be equinumerous. Even, there is the power of $\Bbb N$, it shouldn't change anything. But, I am wrong. Can anybody tell me what is wrong please? Thank you in advance!","['cardinals', 'elementary-set-theory']"
974317,Hausdorff Topological Space,"Wanted to explain what I think a Hausdorff is in my own words because maybe that is the root of the problem. A Hausdorff Space is one in which for every x and y in X with x does not equal y, there exists an open set containing x and an open set containing y within x and the intersections of that is the empty set. My first problem is I need to find a topology that isn't a Hausdorff, and that is the Cofinite topology I know from class, but I don't quite understand why. Also, my second question is if I am given (X,T) is a metrizable space, how to I prove the topology (X,T) is a Hausdorff? I know the properties of a metrizable space, but not how they would apply to prove something is a topology, or even a Hausdorff topology. Thanks!","['general-topology', 'separation-axioms']"
974342,how find all the zeroes of the polynomial,Find all the zeroes of the polynomial $f(x) = 2x^7  - 17x^6  -45x^5  +390x^4  + 28x^3  + 1832x^2  +960x$ this is my try $f(x)= 2x^7  - 16x^6-x^6-42x^5-3x^5+390x^4  + 28x^3  + 1820x^2+12x^2  +960x$ $f(x)=2x^6 (x - 8)-x^5 (x+42)-3x^4 (x-130)+28x^2 (x + 1820)+12x(x+80)$,"['calculus', 'algebra-precalculus']"
974355,Prove that $1 + 4 + 9 ... + n^2 = (n/6)(n+1)(2n+1)$,I know that it is true but not sure how to write the proof for: $1 + 4 + 9 ... + n^2 = (n/6)(n+1)(2n+1)$. I need help to guide me in the right direction. Thanks in advance. edit: Okay at n=k I have $ 1+4+9 ... + k^2 = (k/6)(k+1)(2k+1)$ and at $n=k+1$ I have $((k+1)/6))((k+1)+1)(2(k+1)+1) = ((k+1)/6)(k+2)(2k+3).$ Does my base step need to begin at $n=0$ or $n=1?$ How do I tell?,['discrete-mathematics']
974381,Almost equal probable sums with loaded dice,"It's known that it's impossible to assign probabilities to a pair of loaded dice so that the sums $2,...,12$ are equally probable. How would one set the probabilities $\{p_i: 1\le i\le 6\}$ and $\{q_i: 1\le i\le 6\}$ for the two dice so that $\sum_{i=1}^{11}\|s_i-1/11\|_2$ is minimal? ( for $1\le i\le 11$, $s_i$ is the probability that the sum is $i+1$).",['probability']
974384,Topological space which is not locally connected,"In class we defined a locally connected space as a space that has a basis consisting of connected sets. I don't quite understand what a space which is not locally connected would look like. At least one basis element in every basis would have to be disconnected, so the union of two non-empty non-intersecting open sets. That would give us another basis, which would have to contain a disconnected element, and so on. I can't think of a single example of a space that satisfies this. Have I misunderstood the definition? Is there a good, simple example of a space that is locally disconnected? I have tried to understand the infinite broom as an example of a connected space that is not locally connected. I can see why it is connected, but I don't understand why it is not locally connected. Could someone help me understand why?","['general-topology', 'connectedness']"
974406,Integer solutions of $\log a \log b = \log c \log d$,"Four positive integers $a,b,c,d>1$ satisfy $\log a \log b = \log c \log d$. Is necessarily $\frac{\log a}{\log c} \in \mathbb{Q}$ or $\frac{\log a}{\log d} \in \mathbb{Q}$? I tried to use Gelfond–Schneider theorem and Lindemann–Weierstrass theorem from transcendence theory but it didn't work.","['transcendence-theory', 'number-theory']"
974453,General Weak Law of Large numbers,"I came across a question regarding the WLLN. Suppose for $X \geq 0$ ,  $\mathbb{E}[X] = \infty $ , $S_n = \sum_{i \leq n} X_i$, $X_i$ are iid copies of $X$ , and $\frac{\mathbb{E}[X \mathbf{1} _{X \leq s} ] }{s(1 - F_X(s))} $ $\rightarrow$ $\infty$ . Let $\mu(s) =  \mathbb{E}[X \mathbf{1} _{X \leq s} ]  $ , then for $n$ large enough we can choose $b_n$ such that $n\mu(b_n) = b_n$ . I am stuck at the existence of $b_n$. Basically, the problem goes onto to ask us to prove that $\frac{S_n}{b_n} \rightarrow 1$ , which I am able to prove if I assume the existence of $b_n$.","['probability-theory', 'law-of-large-numbers', 'random-variables']"
974514,Simple proof that $\|p(A)\|\le \sup_{|z|\le 1}|p(z)|$ for polynomials $p$ and $\|A\| \le 1$.,"Let $\mathcal{H}$ be a complex Hilbert space, and let $A$ be a bounded operator linear operator on $\mathcal{H}$ with $\|A\| \le 1$. It is known that $\|p(A)\|\le \sup_{|z|=1}|p(z)|$ for all complex polynomials $p(z)$. Does anyone know a short proof of this fact?","['operator-theory', 'spectral-theory', 'functional-analysis']"
974546,Is $x=0$ an inflection point?,"Consider $f(x)=x^{\frac {5}{7}}$, is it $x=0$ an inflection point?
$$f'(x)=\frac {5}{7}x^{\frac {-2}{7}}$$
$$f''(x)=\frac {-10}{49}x^{\frac {-9}{7}}$$
As far as I know, the inflection point is the point in which $f''(x)=0$ or $f''(x)$ does not exist and $f''(x)$ also changes sign at that point. However, here $f'(0)=+\infty $ from both sides, so it should have verical tangent at point $x=0$. My calculation shows that $f_+''(0)=-\infty$ and $f_\_''(0)=-\infty$ so the $f''(x)$ changes sign around the point $x=0$. I'm not sure whether I'm correct, please tell me the safe steps to find the inflection point.","['calculus', 'derivatives']"
974548,Linear Algebra solution when determinant is zero,"I am doing practice questions in my book and I came upon this True/False question: If $\det(A) = 0$, then the linear system $Ax=b$, $b\neq 0$, has no solution. The book is saying that the answer is false. But why is that? I thought the answer is true because of something like this $  Ax  = b$ 
$$\left(\begin{array}{ccc|c}1&0&0&1\\0&0&0&2\\0&0&1&3\end{array}\right)$$ When a matrix has its rref taken, the resulting matrix, when the determinant is zero, would always have a zero in its diagonal, right? This would result in a matrix with no solution because row 2 is impossible. Am I misunderstanding the question somehow? I am also confused by this question because I am not sure how augmented matrices work with square matrices because you can only find the determinant of a square matrix. Can someone please explain why the answer is false?",['linear-algebra']
974560,Proof of formula for the antiderivative of $\sec x$ [duplicate],"This question already has answers here : Ways to evaluate $\int \sec \theta \, \mathrm d \theta$ (15 answers) Closed 9 years ago . How do I prove that indefinite integral of $\sec x$ is equal to $\ln(\sec x + \tan x) + C$? I tried to substitute $t = \cos x$ but that didn't help. I have no idea how to integrate it any other way, and my textbook doesn't offer a derivation.","['trigonometric-integrals', 'calculus', 'integration', 'indefinite-integrals']"
974608,Taking the Inner Product with the Zero Vector,"Is it possible for the inner product of any vector with the zero vector $ \mathbf{0} $ to be nonzero? Or must it always be zero? I'm struggling to find a counterexample. That is, is the following statement correct? $$ \langle \mathbf{v}, \mathbf{0}\rangle = 0 \space \forall \space v \in V$$",['linear-algebra']
974611,Is every smooth function Lipschitz continuous?,"Is every function of class $C^∞$ also (locally) Lipschitz continuous ? If so, how can this be proven?","['ordinary-differential-equations', 'calculus', 'differential-geometry', 'functions', 'real-analysis']"
974636,Solving a recurrence with a term $T(\frac n 2 + 2)$,"I'm stuck trying to solve the following recurrence: $$\begin{align*}
T(n) &= 4T(\frac n 2 + 2) + n : n > 8\\
T(n) &= 1 : n \leq 8
\end{align*}$$ In particular, I'm not sure how to deal with the expression $T(\frac n 2 + 2)$. I'm trying to use a recursion tree. I've found that the ""cost"" at the $k$th level of the tree is $$4^k\left(\frac n {2^k} + \sum^k_{i = 1} 2^{2- i}\right)$$ by looking at the cost of the first few levels of the tree, which I understand to be: $$4(\frac n 2 + 2)\\
16(\frac n {2^2} + 2 + 1)\\
64(\frac n {2^3} + 2 + 1 + \frac 1 2)\\
256(\frac n {2^4} + 2 + 1 + \frac 1 2 + \frac 1 4)$$ etc. So the whole cost would be $$T(n) = \sum_{k = 1}^{\log_2 n} 4^k\left(\frac n {2^k} + \sum^k_{i = 1} 2^{2- i}\right)$$ but I'm not at all clear how to go from this to a nice, closed-form expression, or how to deal with the value of $T(n)$ over the range $[0, 8]$. What's the right way to approach this?","['recurrence-relations', 'discrete-mathematics']"
974639,Differentiability of chart functions,In the definition of a smooth (or $C^k$) manifold the charts $\varphi: U \to \mathbb R^n$ are assumed to have the property that for any two of them $\varphi \circ \psi^{-1}$ is smooth ($C^k$). Does $\varphi \circ \psi^{-1}$ is smooth ($C^k$) imply that both $\varphi$ and $\psi$ are smooth ($C^k$) or is it weaker?,"['differential-geometry', 'definition']"
974642,Using IVT prove that a polynomial of even degree has atleast two real roots if $a_n a_0 \lt 0$,$P(x) = a_n x^{n} + a_{n-1}x^{n-1} + \cdots +a_1x+ a_0$ $n = 2k$ Show that $P(x)$ has at least two real roots if $a_na_0 \lt 0$ I think I need to find some interval of length $|N|$ in which the graph has a different sign compared to the sign everywhere else. Any ideas/help on how to start  ? Thanks in advance!,"['calculus', 'algebra-precalculus']"
974646,Using the Cantor-Bernstein theorem,"I'm working through Kolmogorov and Fomin's Introductory Real Analysis text, and I came to a question about showing that some sets have the same power as the continuum. I have seen this question posted on here before, but I decided to post this anyways because I have solutions to the 3 parts of the question and am specifically looking for feedback on if I am using the Cantor-Bernstein theorem correctly. I'll just post one of the sets, since the method I used for the other two is identical. So, the first set to be considered is the set of all $n$-tuples of real numbers, denoted as $N$. Let $f:N\rightarrow [0,1]$ be a function that takes each entry of an $n$-tuple, takes their non-decimal part and interleaves this with the decimal part like so: $(x_1,x_2,x_3,...,x_n) \mapsto 0.r_1r_2r_3...r_nx_{11}x_{21}x_{31}...x_{n1}...x_{ij}$, where $1 \leq i \leq n$ and $1 \leq j < \infty$, with $n$ incrementing by one each time a decimal part would repeat infinitely. In this way, we identify an unique real number in $[0,1]$ with each $n$-tuple. Now, for the injection from $\mathbb{R} \to N'$. For each real number $x$, let the function $g$ identify $x$ with an unique ""degenerate"" $n$-tuple of the form $(x,0,0,...,0)$. These degenerate $n$-tuples clearly form a subset $N' \subset N$, and we have the second injection. Since we have an injection $g$ from $\mathbb{R} \to N'\subset N$ and an injection $f$ from $N\rightarrow [0,1] \subset \mathbb{R}$, by the Cantor-Bernstein theorem, $N \sim \mathbb{R}$. So, my questions are (1) is my proof correct, (2) is this proper usage of Cantor-Bernstein theorem, and (3) is it conventional to show injectivity of the functions even after defining them explicitly? Edited to change the definition of the injection from $N\rightarrow [0,1]$ based on suggestions in the comments.","['elementary-set-theory', 'real-analysis']"
974673,Meaning of a discrete topological sub-space?,"Given a topological space $X$ and a set $U\subseteq X$, what is the meaning of $U$ being a discrete sub-space of $X$? I do know what a discrete space is, so as far as I understand it, the meaning is that each $A\subseteq U$ is open in the relative topology in $U$? And if I understand it correctly, given $U$ is open in $X$, will this apply that $A$ is open in $X$ as well? Please fix me if I'm wrong, just wanted to make sure I understand it correctly.",['general-topology']
974691,I have a question about a proof I am doing with partial ordered sets.,"$L$ is a partially ordered set in which every subset has a least upper bound. Suppose that $L$ has a bottom element. Prove that $L$ is a complete lattice. I understand that I need to show that any subset has a GLB. So I can take some set, $S$ and talk about the set $T$ of all lower bounds for $S$. We know $T$ is nonempty because our bottom element is in it, and we have that $T$ has a least upper bound, call is $s$. Now I can easily show that if the least upper bound is actually in $T$ then its the greatest lower bound of $S$. But what if $s$ is not in $T$? Then it's not a lower bound for $S$ and I am confused. any help would be great.",['elementary-set-theory']
974695,Finding function $f(x)$,How do we find the function(s) $f(x)$ given that $$f(x)=\int_{0}^{x} te^tf(x-t) \ \mathrm{d}t$$ My Try : I first used the property $\int_{0}^{a}g(x) \ \mathrm{d}x=\int_{0}^{a}g(a-x) \ \mathrm{d}x$ and then differentiated to find that $$f'(x)=f(x)-e^x\int_{0}^{x}e^{-t}f(t) \ \mathrm{d}t$$ Then I differentiated again to find that $$f''(x)=2f'(x)-2f(x)$$ I entered this differential equation into wolfram alpha and it returned $f(x)=c_1e^x\sin x+c_2e^x\cos x$ But I don't think that this satisfies the given initial condition. Please help me out. Hints or answers appreciated. Thank you.,"['ordinary-differential-equations', 'integral-equations']"
974697,Normalize gradient,"I want to minimize a function $f \, : \, \mathbb{R}^{N} \, \longrightarrow \, \mathbb{R}$ (with $N \in \mathbb{N}^{\ast}$. In my problem, $N = 315$). I know that $f$ is differentiable on $\mathbb{R}^{N}$ and convex. Numerically, when I compute the gradient of $f$ at a given point, I have noticed that the coordinates of $f$ have different orders of magnitude ($\frac{\partial f}{\partial x_{1}}  \gg \frac{\partial f}{\partial x_{2}}$ at every point). In practice, when I minimize $f$ using a gradient descent ($x_{k+1} \, \leftarrow \, x_{k} - t_{k}\nabla f(x_{k})$, with $t_{k}$ determined using a backtracking line-search), $x_{2}$ almost do not change through the iterations. I have been told that my problem is badly scaled and that I can address this issue by minimizing $\tilde{f}(x) = f(Dx)$ where $D$ is a diagonal matrix with positive coefficients. Doing so, I have : $$ \nabla \tilde{f}(x) = D \nabla f(Dx) $$ for all $x \in \mathbb{R}^{N}$. My question is : how do I choose this matrix $D$ ? Are there (empirical) rules to choose this matrix ?","['optimization', 'multivariable-calculus', 'convex-optimization', 'derivatives']"
974706,Finding the limits of a multivariable function $\frac{x^2y^2}{x^4+y^2}$,"Question: Given the following function, determine whether the following function is continuous at $(0,0)$ $$
f(x,y)=\begin{cases}\frac{x^2y^2}{x^4+y^2}, &x^2+y^2 \neq 0,\\
0 ,&x^2+y^2=0. \end{cases}$$ I know of three methods for approaching such problems: rewrite the function in terms of polar coordinates, then try to find the limit, bound the function from above and below, then apply the squeeze theorem, and take limits along the line $y=x$ . Using the first technique of rewriting the function in terms of polar coordinates, it appears that the function is continuous at $(0,0)$ .  I would like to be able to answer the question using the squeeze theorem.  I can bound the numerator, but I don't know how to bound the denominator so that the limit goes to zero at the origin.","['multivariable-calculus', 'limits']"
974762,Extinction probabilities of binomial tends to Poisson distribution,"I am stuck on exercise 11.2 From Grimmett's  probability on graphs. Here is a link to the pdf on his website. Consider a branching process whose family-sizes have the binomial distribution bin$(n, \frac{\lambda}{n})$. Show that the extinction probability converges to $\eta(\lambda)$ as $n \rightarrow \infty$, where $\eta(\lambda)$ is the extinction probability of a branching process with family-sizes distributed as $\text{Po}(\lambda)$. To solve this exercise, we use a theorem from his other book, which states that if you have a branching process whose family sizes are $X$ distributed then the extinction probability of this branching process is the smallest non-negative fixed point of the equation $s = G(s)$ where $G$ is the probability generating function $G(s) = \sum_{k = 0}^\infty \mathbb{P}(X = k)s^k$. My next step was computing these probability generating functions for $\text{bin}(n,\lambda)$, these are $G_n(s) = (1+\frac{\lambda( s -1)}{n})^n$. When the family sizes are Poisson distributed with parameter $\lambda$ we get $G(s) = e^{\lambda(s-1)}$. and so clearly $G_n$ converges pointwise to $G$. This however is not enough, as we want to show that the extinction probabilities also converge, so we have $s_n$ extinction probabilities of the $n$-th process, i.e. $s_n$ is the smallest non-negative solution to $s = G_n(s)$, and we want to show that this converges the smallest non-negative solution of $s = G(s)$. I hoped I could use some theorem which states that if you have pointwise convergence then the fixed points also converge, but this is false, see this counterexample . During writing this I thought of an attempt to solve this using Hurwitz theorem, namely, show that the smallest non-negative fixed point of $G(s)$ is smaller than 1, then show that the function $G'$ is complex differentiable with non-zero derrivative, use inverse function theorem, get an open neighbourhood of our fixed point, find sequence of fixed points which converge to the smallest non-negative fixed point $\eta(\lambda)$ of $G$. Here I am stuck trying to show that $s_n$ are the smallest non-negative fixed points of $G_n$. How to proceed?","['probability-theory', 'fixed-point-theorems', 'generating-functions']"
974765,Projective bundle is projective?,"Let $\mathbb{P}(E)$ be a projective bundle over some smooth projective variety $X$, defined over $\mathbb{C}$ for definiteness. Then this bundle is also a smooth projective variety. Smoothness is clear from the trivialization, and it is also clear using the Segre embedding that every patch can be embedded in some projective space. Does is automatically follow that the entire bundle can be embedded in some projective space? One can definitely glue them to get a variety, by using the triple intersection rules, but is it necessarily projective? EDIT: in Tyurin's Vector Bundles, one reads that a vector bundle over a complete variety is neither affine nor complete, but the projectivization is an actual projective variety. This is what I am wondering about.",['algebraic-geometry']
974801,Problem in conditional expectation,"I came across this problem recently. Let $X$ be a non-negative random variable on $(\Omega,\mathscr{F},\mathbf{P})$ and let $\mathscr{G} \subseteq \mathscr{F}$ be a sub-sigma-algebra. 1) Show that $X>0 \implies \mathrm{E}[X|\mathscr{G}]>0$ almost surely. 2) Show that $\{\mathrm{E}[X|\mathscr{G}]>0\}$ is the smallest $\mathscr{G}$ measurable event that contains the event $\{{X>0}\}$ almost surely. Now 1) can be proved defining $A=\{Y:\mathrm{E}[X|\mathscr{G}]\le0\}$. Then by the tower property $\mathrm{E}[X\mathbb{1}_A(Y)]=\mathrm{E}[\mathrm{E}(X|Y)\mathbb{1}_A(Y)]$. So $\mathrm{E}(\mathbb{1}_A(Y))=0$ a.s. so $\mathrm{E}(X|Y))>0$ a.s. Could anyone show how to get the second part?","['conditional-expectation', 'probability']"
974803,Express $\sin4\theta$ in terms of powers of $\sin\theta$ and $\cos\theta$,"As far as I know $\sin4\theta$ = $4\sin\theta \cos\theta$, but I don't know if that's correct or what to do from there?",['trigonometry']
974808,Limit of $\int_0^1\frac1x B_{2n+1}\left(\left\{\frac1x\right\}\right)dx$,"Set $$u_n= \int_0^1 \frac{B_{2n+1}\left(\left\{\frac1x\right\}\right)}{x}dx\tag1$$
where $\left\{t\right\}=t-\lfloor t \rfloor$ denotes the fractional part of $t$ and where $B_n(\cdot)$ are the Bernoulli polynomials:
$$ \begin{aligned}B_{1}(x)&=x-\frac12\\B_{3}(x)&=x^3-\frac32x^2+\frac12x\\\ldots\,&= \ldots\end{aligned}$$ I'm interested in finding $\lim\limits_{n \to +\infty}|u_n|.$ 
I've tried to see what Wolfram|Alpha gives with no success. My guess is that $$\lim\limits_{n \to +\infty}|u_n|=+\infty. \tag2$$ Could you prove/disprove $(2)$? Thank you.","['sequences-and-series', 'convergence-divergence', 'calculus', 'integration']"
974814,Continuous Measures: Range,Let $\Omega$ be a sigma-finite measure space with no atoms. (Reminder: A subset $A\in\Sigma$ is an atom if $\mu(E)<\mu(A)$ implies $\mu(E)=0$ for all $E\subseteq A$.) Then the measure attains every number $\mu(F)=c$ for $\mu(\varnothing)\leq c\leq\mu(\Omega)$. By sigma-finiteness there are subsets of arbitrarily large but finite mass and since it is atomless there are subsets of arbitrarily small but nonvanishing mass. How can I combine these two properties to reach any mass up to any error?,"['general-topology', 'measure-theory']"
974825,"What's an easy way to show that $GL(n,\mathbb C)$ is connected? [duplicate]","This question already has answers here : How to show path-connectedness of $GL(n,\mathbb{C})$ (6 answers) Closed 9 years ago . I think I've to show it's path connected, but can't figure out the path functions explicitly. Can anyone give these path maps?","['general-topology', 'matrices', 'connectedness']"
974864,"Proving a function is an open map, with limitations","Following from a previous question I posted in here , given the same thing, i.e. an open set $U$ in $\mathbb{R}^2$ and a continuous function $f:U\rightarrow \mathbb{R}^2$ such that for each $u\in U$ exists a neighbourhood $V_u$ in $U$ such that $f\uparrow_{V_u}$ is one-to-one. I'd like to show $f$ is an open map. Now I was wondering what will happen if this last statement will be true except for a single point, i.e. exists a $u_0$ such that for each $u\in U$ if $u\neq u_0$ then there exists a neighbourhood $V_u$ in $U$ such that $f\uparrow_{V_u}$ is one-to-one. Is it still true to say that $f$ is an open map? Or perhaps I need to add another assumption to make it correct? I tried proving it by taking any open set $V\subseteq U$, and writing it as a union of open sets $\{V_i\}_{i\in I}$ (a solution suggested by @Joe Manlove ), by taking  $ \hat V_v $ to be a neighborhood with $ f\uparrow_{\hat{V}_v}$ that is $1:1$, for every $v\in V $, and then taking $ V_v := \hat V_v \cap V $, so that $V_v\subseteq V$, thus making their union be exactly $V$. Then I can use Invariance of Domain Theorem on each $V_i$, thus getting that $f(V)$ is a union of open sets, thus open as well. 
But the problem now is that I might not be able to find such cover, because of the possibility of $a$ being in $V$. What I tried to do was to take the union for all the other $V_v$ where $v\neq a$, and then proving that $a$ must be in this union, because $U$ is an open set. But then again I'm not so sure this is correct, and I thought perhaps there's another assumption (possibly a small one) I need to make before being able to prove this? Other than this case, I was wondering, if this is correct, does it mean I can have more than a single ""bad"" point, i.e. $u_0,...,u_n$ and $f$ will still be an open set? Edit : Does the following will help me - that the set $\hat{A}=\{a\in U \,\,\, | \,\,\,\, f(a)=f(u_0) \}$ is a discrete sub-space on $U$?",['general-topology']
974874,"A first-countable, compact space which is non-separable [duplicate]","This question already has answers here : Looking for examples of first countable, compact spaces which is not separable (3 answers) Closed 9 years ago . Let $X$ be a space which is first countable and compact. Is $X$ necessarily separable?  Is $X$ necessarily second countable?",['general-topology']
974886,Welford's algorithm for standard deviation: combine multiple sets of results,"Suppose I use Welford's algorithm to compute the standard deviation of multiple sets of values. I only store all the n, mean and M2 results that the algorithm calculates, thus I have these three results per set. Is it possible to calculate the standard deviation over all samples in all sets, by using only the stored results? Thus I'm looking for a way to combine the n, mean and M2 results. For n and mean this is trivial, but I don't know how to combine the M2 values. Note: this would be easy when I would use the naive sum of squares algorithm , but wikipedia recommends against using this algorithm in practice. Note2: I cannot store all values in all sets, I'm writing software that only gets one chance to do something with each value, thus the algorithm needs to be 'incremental' or 'online'.","['statistics', 'standard-deviation']"
974891,"Intermediate fields of Gal($x^5-2$, $\mathbb{Q}$)","I'm having trouble findind the fixed field of each subgroup of the Galois group of $\mathbb{Q(\alpha, \omega)}$, where $\alpha = 2^{\frac{1}{5}}$ and $\omega$ is a 5-th primitive root of unity. I list here the 20 automorphisms of the Galois group, $G$: \begin{array}{|c|c|c|c|}
\hline
&I & \sigma & \sigma^2 & \sigma^3 & \sigma^4 & \tau & \tau^2 & \tau^3 & \sigma\circ\tau &\sigma\circ\tau^2\\ \hline
\alpha& \alpha & \alpha\omega &\alpha\omega^2&\alpha\omega^3&\alpha\omega^4&\alpha&\alpha&\alpha&\alpha\omega&\alpha\omega  \\ \hline
\omega& \omega  &\omega &\omega&\omega&\omega&\omega^2&\omega^4&\omega^3&\omega^2&\omega^4\\ \hline
\end{array} \begin{array}{|c|c|c|c|}
\hline
 &\sigma\circ\tau^3 &\sigma^2\circ\tau &\sigma^2\circ\tau^2 &\sigma^2\circ\tau^3 &\sigma^3\circ\tau &\sigma^3\circ\tau^2 &\sigma^3\circ\tau^3 &\sigma^4\circ\tau &\sigma^4\circ\tau^2 &\sigma^4\tau^3 \\ \hline
\alpha& \alpha\omega &\alpha\omega^2 &\alpha\omega^2 &\alpha\omega^2 &\alpha\omega^3 &\alpha\omega^3 &\alpha\omega^3 &\alpha\omega^4 &\alpha\omega^4 &\alpha\omega^4\\ \hline
\omega &\omega^3 &\omega^2& \omega^4& \omega^3& \omega^2&\omega^4&\omega^3&\omega^2&\omega^4 &\omega^3\\ \hline
\end{array} The subgroups are: {I}, {I, $\sigma \circ \tau^2$}, {I,$\sigma^2 \circ \tau^2$ }, {I,$\sigma^3 \circ \tau^2$ }, {I,$\sigma^4 \circ \tau^2$ }, {I,$\tau^2$ } {I, $\tau, \tau^2, \tau^3$} {I, $\sigma, \sigma^2, \sigma^3, \sigma^4$} {I,$\sigma, \sigma^2, \sigma^3, \sigma^4, \tau^2, \sigma \circ \tau^2, \sigma^2 \circ \tau^2, \sigma^3 \circ \tau^2, \sigma^4 \circ \tau^2 $} $G$ {I} fixes $\mathbb{Q(\alpha, \omega)}$; G fixes $\mathbb{Q}$; {I, $\tau, \tau^2, \tau^3$} fixes $\mathbb{Q(\alpha)}$; {I, $\sigma, \sigma^2, \sigma^3, \sigma^4$} fixes $\mathbb{Q(\omega)}$ However, I can't see what are the others fixed fields... I believe {I,$\sigma, \sigma^2, \sigma^3, \sigma^4, \tau^2, \sigma \circ \tau^2, \sigma^2 \circ \tau^2, \sigma^3 \circ \tau^2, \sigma^4 \circ \tau^2 $} fixes $\mathbb{Q(\omega + \omega^4)}$, but then I'd have to show that $[\mathbb{Q(\omega + \omega^4)}:\mathbb{Q}]=2$, but I don't know what is the minimal polynomial of $\omega + \omega^4$. Same for the subgroups of order 2. I'd have to find extensions of $\mathbb{Q}$ of order 10 that are fixed by the automorphisms in the subgroup. For example, I know that {I, $\sigma\circ\tau^2$} fixes $\mathbb{Q(\alpha\omega^3)}, \mathbb{Q(\alpha + \alpha\omega)}, \mathbb{Q(\alpha\omega^4+\alpha\omega^2)}, \mathbb{Q(\omega+\omega^4)}, \mathbb{Q(\omega^2+\omega^3)}$, so the fixed field must contain all of this subfields and have degree 10 over $\mathbb{Q}$, but how to find it? Can someone give a help? Thanks!","['galois-theory', 'abstract-algebra', 'field-theory']"
974917,Is this argument on positive definite matrices correct?,"Let $A$ be a $N\times N$ positive definite matrix. Then, there exists a $N\times 1$ gaussian random vector $a$ such that $A=E[aa^T]$ where $E[.]$ denotes expectation. Then for any given vector $x$, $x^TAx=x^TE[aa^T]x=E[|a^Tx|^2]>0$. Since it is independent of choice of $x$, $x^TAx>0$ for all $x$. What is wrong with the above sort of argument? How will one justify the interchange of expectation and inner product in the equality $x^TE[aa^T]x=E[|a^Tx|^2]$?","['matrices', 'linear-algebra', 'probability']"
974918,The category of vector fields on smooth manifolds,"In my differential geometry lecture today we learnt about the push-forward of a vector field by a diffeomorphism. I know some basic category theory and I noticed a functor popping up. Here's what I've got (in class at the moment we're only looking at open subsets of $\mathbb{R}^n$ but I think this ought to generalise): Let $U,V\subseteq \mathbb{R}^n$ be open, $F\in \text{Diff} (U,V)$ a diffeomorphism, and let $\mathbb{X}:U\rightarrow \mathbb{R}^n$ be a vector field on $U$ (which we have been thinking of just as a function to $\mathbb{R}^n$). Denote the set of all vector fields on $U$ by $\mathcal{X} (U)$. Then the push-forward of $\mathbb{X}$ by $F$ is the vector field $F_{*} \mathbb{X}\in\mathcal{X} (V)$ defined by $(F_{*} \mathbb{X})\circ F = F'\mathbb{X}$ where $F'$ is the derivative of $F$. In components, $(F_{*} \mathbb{X})^i (y) = \partial_j (F^i (F^{-1} (y)))\mathbb{X}^j (F^{-1} (y))$. You can check that ${id_U}_{*} = id_{\mathcal{X} (U)}$ and if $U\xrightarrow{F} V\xrightarrow{G} W$ is a sequence of diffeomorphisms then $(G\circ F)_{*}\mathbb{X} = G_{*} F_{*} \mathbb{X}$. Therefore we've got a covariant functor $h_{*}(F:U\rightarrow V) = F_{*} : \mathcal{X}(U)\rightarrow \mathcal{X}(V)$ from the category $\bf{Man}$ of smooth manifolds (here I just used open subsets of $\mathbb{R}^n$) to the category $\bf{VecFld}$ of vector fields on smooth manifolds.
However, I don't have a clue what $\bf{VecFld}$ looks like as a category itself. What are the morphisms? Is every morphism $f:\mathcal{X}(V)\rightarrow \mathcal{X}(U)$ the pullback $f(\mathbb{X}) = \mathbb{X} \circ F$ of some diffeomorphism $F:U\rightarrow V$? Does $h_{*}$ have an adjoint? Any information on related questions would be greatly appreciated, and sorry if this is very basic; I hope to learn more about vector fields later in this course!","['category-theory', 'differential-geometry']"
