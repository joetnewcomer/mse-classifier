question_id,title,body,tags
2602144,Conjectured value of a difficult integral with Dedekind eta functions,"In my research on quantum groups I have the following conjecture:
\begin{equation}
\int_0^\infty\frac{\eta (2 i x)^8}{\eta (i x)^2 \eta (4 i x)^2}\,dx\,{\stackrel?=}\,\frac{K(\tfrac{1}{\sqrt{2}})}{\pi}\tag{1}
\end{equation}
where
\begin{equation}
\eta(ix)=e^{-\frac{\pi x}{12}}\prod_{n=1}^\infty (1-e^{-2n\pi x})
\end{equation}
is the Dedekind eta function and
\begin{equation}
K(\tfrac{1}{\sqrt{2}})=\frac{\Gamma^2(\tfrac14)}{4\sqrt{\pi}}
\end{equation}
is the Elliptic integral singular value . This value has been guessed using Inverse symbolic calculator and then checked numerically to a high precision, but I do not have a proof. Question : Is (1) true?","['conjectures', 'residue-calculus', 'integration', 'definite-integrals', 'contour-integration']"
2602148,Continuity by the right of a function defined by an improper integral,"Suppose we have a continuous function defined by $$f:\Bbb R\times(0,\infty)\to\Bbb R,\quad (t,x)\mapsto f(t,x)$$ such that $F(x):=\int_0^\infty f(t,x)\,\mathrm dt$ converges uniformly in $[c,\infty)$ for any chosen $c>a$, where the integral is an improper integral of Riemann. When we can say that $$\lim_{x\to a^+}F(x)=\int_0^\infty \lim_{x\to a^+}f(t,x)\,\mathrm dt=\int_0^\infty f(t,a)\,\mathrm dt$$ for some $a>0$? That is, when we can say that $F$ is continuous at $a$ by the right? Background: this question comes just from curiosity, I dont have a clear answer by now.","['real-analysis', 'improper-integrals', 'limits']"
2602168,Slick proof that $x^a(1-x)^{r-a}$ are linearly independent?,"I'm interested in showing that for every real number $r>0$, the set of functions $$\{f_a : (0,1) \to \mathbb R \,|\, a \in (0,r)\}$$
is linearly independent where
$$f_a(x) = x^a(1-x)^{r-a}.$$ The proposition can be shown for $a,r \in \mathbb N$ by the binomial theorem. For real parameters, my approach was to evaluate the Wronskian at $x=\frac 1 2$ (by symmetry, this is sort of the canonical value to try, e.g. all $f_a$ agree there). Some calculations suggest that
$$ W(f_{a_1},\ldots,f_{a_n})\left(\frac 1 2\right) 
= \left(\frac 1 2\right)^{nr - n(n-1)} \cdot V(a_1,\ldots,a_n)$$
where $V(a_1,\ldots,a_n)$ is the Vandermonde determinant. This would indeed show the proposition and I suppose I could just brute-force my way through the proof with determinant expansions. I am wondering though if there is a more elegant proof. I am a bit reminded of the orthogonality relations behind Fourier or Laplace transforms. A generalization of the linear independence question would be to consider the transform $$\{Bw\}(s) = \int_0^1 w(s)x^s(1-x)^{r-s}\mathrm dx$$ Is anything known about that? Anyway, I suspect that a suitable integration in the complex plane might show the proposition. Or I am missing something entirely elementary about the $f_a$ and some factorization/limit argument is enough as in the case of the power functions $x^a$.","['complex-analysis', 'real-analysis']"
2602202,Check differentiability of function in $x_0 = 0$,"Check differentiability of function in $x_0 = 0$. Let $f: \mathbb{R} \to \mathbb{R},\quad  f(x) =
 \begin{cases} 
 \begin{align}
      0  \qquad  &,  x < 0\\
      \sin x \qquad &, x \ge 0 \\
\end{align}   
\end{cases}
 $ I think that the function is differentiable in $0$. How can I show that (I have to use the difference quotient)? What I have done so far: $\begin{align}\lim_{h\to 0}\frac{f(x_0+h) - f(x_0)}{h} &= \lim_{h \to 0}\frac{\sin(x_0 + h) - \sin(x_0)}{h} \\
 &= \lim_{h \to 0}\frac{\sin(0+h) - \sin(0)}{h} = \lim_{h \to 0}\frac{\sin(h)}{h} \\
&= \lim_{h \to 0} \frac{\sum_{n=0}^{\infty}\frac{h^{2n+1}}{(2n+1)!}}{h} =  \lim_{h \to 0} \frac{\sum_{n=0}^{\infty}\frac{h^{2n}h}{(2n+1)!}}{h}\\
&= \lim_{h \to 0}\sum_{n=0}^{\infty}\frac{h^{2n}}{(2n+1)!}
\end{align} \\
$ How could I go on ?","['real-analysis', 'ordinary-differential-equations', 'analysis', 'limits']"
2602204,Locally compact Stone duality: Can a boolean space be recovered from its boolean algebra of clopen sets alone?,"During my beginning PhD-research I have encountered the following question/problem: Let $X$ be a boolean space . Is it possible to reconstruct the space $X$ from its boolean algebra of clopen sets, $Cl(X)$ (viewed as a boolean algebra)? By a boolean space I mean a (non-empty) zero-dimensional locally compact Hausdorff space, or equivalently, a Hausdorff space with a basis consisting of compact open sets. The particular spaces I am studying are second countable and have no isolated points as well -- in case it makes a difference. The set of clopen subsets of $X$, $Cl(X)$, forms a boolean algebra under the usual set-operations of union, intersection and complement. It is generally not a complete boolean algebra. In the case that $X$ is in fact compact (i.e. a Stone space ) the answer is of course yes, this being the classical Stone duality. In the locally compact case however, it seems to me like the answer in general is no, based on the references below, as these seem to require the boolean algebra of compact open sets instead. And it is not possible to ""detect"" compactness of an element $A \in Cl(X)$, as one only has finite unions/joins in this boolean algebra. Dimov – Some Generalizations of the Stone
Duality Theorem ( http://rmi.tsu.ge/tolo2/presentations/Dimov.pdf ) Doctor – The Categories of Boolean Lattices, Boolean Rings and Boolean Spaces ( https://cms.math.ca/openaccess/cmb/v7/cmb1964v07.0245-0252.pdf )","['boolean-algebra', 'general-topology', 'duality-theorems']"
2602215,$\int_0^1\int_0^1\binom{\text{something}}{\text{something}}\binom{\text{something}}{\text{something}}\cdot \text{something }dxdy$ with closed-form,"I've calculated an approximation of integrals like than $$\int_0^1\int_0^1\binom{f(x)}{f(y)}\binom{f(y)}{f(x)}dxdy\tag{1}$$ for simple functions $f(x)$. I don't know if some of these were in the literature or have a nice closed-form. Question . I would like to know how to create, if it is feasible, nice examples of double integrals of binomials similar than $(1)$. Do you know how to calculate a nice example using different functions
  $$\int_0^1\int_0^1\binom{\text{something}}{\text{something}}\binom{\text{something}}{\text{something}}\cdot \text{something }dxdy\,?\tag{2}$$
  If you know an example from the literature with a nice closed-form, please  answer this question as a reference request, then I am going to try search such literature and read the example. Many thanks. Your closed-form can be expressed as a series of special functions (
I'm especially interested in how to create such an example).","['real-analysis', 'binomial-coefficients', 'reference-request', 'integration', 'gamma-function']"
2602227,$a_{n+1}=a_n^{n+1}+a_n+1$,"Let $a_1>1$ and $$a_{n+1}=a_n^{n+1}+a_n+1, \: \forall n \geq 1$$ Edit: Prove that there is a real number $x \neq 0$ such that $$\lim_{n \to \infty}\frac{a_n}{x^{n!}}=1$$ I think that $x=a_1$ works, and I tried to use the Squeeze Theorem to prove that. From $a_{n+1}>a_n^{n+1}>a_{n-1}^{n(n+1)}>\dots>a_1^{(n+1)!}$ we get that $1<\frac{a_{n+1}}{a_1^{(n+1)!}}$ and then I tried to find another bound for $a_{n+1}$ such that $\frac{a_{n+1}}{a_1^{(n+1)!}}<b_{n+1} \to 1$, but nothing seems to work.","['real-analysis', 'limits', 'sequences-and-series', 'calculus', 'convergence-divergence']"
2602230,Better way of identifying pairs of natural numbers that satisfy certain conditions?,"Recently, I encountered the following problem: Let $(a, b)\in(\Bbb{N}^*\space \times\space \Bbb{N}^*)$ such that $$\frac{a^2+2b}{b^2-2a}\text{ and }\frac{b^2+2a}{a^2-2b}$$ are both integers. (i) Show that the absolute difference of $a$ and $b$ is less than or equal to $2$ . (ii) Find all the pairs $(a, b)$ with the given properties. (From the National Mathematics Olympiad (Romania), 2012 – Grade 7 ). The first part ( i ) was quite easy to prove: $$(b^2-2a)\:\mid\:(a^2+2b)\text{ and }(a^2-2b)\:\mid\:(b^2+2a)$$ Since $(a, b)\in(\Bbb{N}^*\space \times\space \Bbb{N}^*)$ , both $a^2+2b$ and $b^2+2a$ are positive, and therefore $b^2-2a\le a^2+2b$ and $a^2-2b\le b^2+2a$ . Forming squares of binomials and switching from the RHS to the LHS and backwards: $$b^2-2b+1\le a^2+2a+1\implies(b-1)^2\le (a+1)^2$$ And since both $a$ and $b$ are positive, the sign does not change when taking the square root: $b-1\le a+1$ so $b\le a+2$ . Similarly, but working with the first relation, $a\le b+2$ . And thus, $|a-b|\le 2$ . So far so good, but (ii) is quite confusing to me, in the sense that I cannot think of an approach that doesn't lead to a lot by-hand checking and ""fluffy"" arithmetic. My attempt was to consider each case separately, so taking each $a$ in $\{b,\:b+1,\:b+2\}$ (of course assuming $a\ge b$ and then swapping each pair at the end and using what we proved at (ii) ). Since this method is (naturally) quite lengthy, it makes no sense to include it all in the question, but here are the results I obtained: $(a, b) \in \{(1, 1), (3, 3), (4, 4), (6, 6), (2, 4), (4, 2), (3, 5), (5, 3), (4, 6), (6, 4)\}$ I was wondering if there is another, hopefully better way of solving this ( ii )? (because this is the solution given by the authors of the problem too :(...)","['algebra-precalculus', 'inequality']"
2602248,Curvature of dual connection,"Let $d_A \colon \Omega^0(E) \longrightarrow \Omega^1(E)$ be a connection on a bundle $E\longrightarrow M$, and suppose its connection matrix is $\theta=(\theta_{ij})$ for some local frame of $E$. I know that the curvature matrix is given by
$$ \Theta = d\theta + \theta \wedge \theta.$$ I want to prove that the curvature of the dual connection $d_{A^*}$ vanishes iff the curvature of $d_A$ vanishes. But, if i take the dual coframe of the previous frame, I know that the connection matrix of $d_{A^*}$ is given by $-\theta^t$. Then  the curvature matrix of the dual connection would be
$$\Theta^* = - ( d\theta) ^t + (\theta\wedge\theta)^t$$
which needn't vanish. Am I missing something?","['connections', 'differential-geometry']"
2602265,Which points on the curve $5x^2+4xy+2y^2-6=0$ are closest to the origin.,"Which points on the curve $5x^2+4xy+2y^2-6=0$ are closest to the origin. I have solved countless of problems like this but this one is just giving me such a hard time. I'm supposed to solve this with Lagrange's method. So I want to minimize $f(x,y)=x^2+y^2$ due to the constraint $g(x,y)=5x^2+4xy+2y^2-6=0$. Ok easy: Find $x,y$ so the following equations are satisfied: $2x+\lambda(10x+4y)=0$ $2y+\lambda(4y+4x)=0$ $5x^2+4xy+2y^2-6=0$ Right? But however i do, i get very complicated equations with root terms to solve, getting me nowhere. I would love to see how you would solve this. Thanks.","['optimization', 'calculus', 'multivariable-calculus', 'lagrange-multiplier', 'quadratics']"
2602271,Finite simple groups with abelian Sylow 2-subgroups,"Is it true that in a simple group with abelian Sylow 2-subgroups a Sylow 2-subgroup is not centralized by a nontrivial element of odd order? Maybe the paper of John Walter ""The characterization of finite groups with abelian Sylow 2-subgroups"" is useful.","['finite-groups', 'abstract-algebra', 'abelian-groups', 'group-theory']"
2602280,Why is $BB^T$ always invertible?,"In Karmarkar’s method, we use $$[I - B^T(BB^T)^{-1}B]v$$ Why does $BB^T$ always have an inverse? Karmarkar’s method is applied to an LP in the following form: $\min z = cx$ subject to $AX=0$ $x_1 +x_2 +......+ x_n =1$ $X\ge0$ $x =[x_1 ,x_2,.....,x_n]^T$, $A$ is an $m \times n$ matrix, $c = [c_1, c_2, .....			 ,c_n]$ ,and 0 is an
n-dimensional column vector of zeros. The LP must also satisfy
$[\frac{1}{n},\frac{1}{n},.....,\frac{1}{n}]^T$ is feasible ,
Optimal $z-$value $=0$ B is the $(m * 1) * n$ matrix
whose first m rows are A and whose last row is a vector of $1’$s. $B = \begin{bmatrix}A\\1 \end{bmatrix}$","['optimization', 'linear-programming', 'matrices', 'inverse', 'linear-algebra']"
2602296,Question about Proposition II.6.9 from Hartshorne,"I see that there are already several questions about this Proposition at MSE, I want to add one more. Proposition 6.9: Let $X \to Y$ be a finite morphism of non-singular curves, then for any divisor $D$ on $Y$ we have $\deg f^*D=\deg f\deg D$. Let $Q$ be a closed point in $Y$. Let $\mathrm{Spec}\,B$ be an open affine neighbourhood of $Q$ and $\mathrm{Spec}\,A=f^{-1}(\mathrm{Spec}\,B)$. Denote $A'=S^{-1}A$, where $S=B\backslash\mathfrak m_Q$. Let $\mathfrak m_i$ be a maximal ideal of $A'$. Let $t$ be a local parameter at the point $Q$. Hartshorne uses in the proof that
$$A'/(tA'_{\mathfrak m_i}\cap A')\cong A'_{\mathfrak m_i}/tA'_{\mathfrak m_i}.$$
This would follow from the Second Isomorphism Theorem if $A'_{\mathfrak m_i}=A'+tA'_{\mathfrak m_i}$. To show this it is enough to check that $1/s\in A'+tA'_{\mathfrak m_i}$ for any $s\in A'\backslash \mathfrak m_i$. I tried to show this but it seems that this is wrong if $s\in\mathfrak m_j\backslash\mathfrak m_i$ for some maximal ideal $\mathfrak m_j$.","['proof-explanation', 'algebraic-geometry', 'commutative-algebra']"
2602317,If $f^2$ and $f^3$ are $C^{\infty}(\mathbb R)$ then $f$ is $C^{\infty}(\mathbb R)$,"Since it is an exercise from an oral exam, I have added some indications I had. $f : \mathbb R \to \mathbb R$, such that $f^2$ and $f^3$ are $C^{\infty}$, show that $f$ is $C^{\infty}$. The two isolated hypothesis are not sufficient, since there is a problem in zero for the cube root, and since you can take $f$ that takes $1$ and $-1$ without regularity but its square will be constant. How to proceed with the two hypothesis? Here, the indications I have from the candidate who has taken the oral exam: -First, show that $f$ is $C^1$:
Suppose $f(0) = 0$ (it is exactly the problem, and by translation we consider that it is in $0$). Suppose they exist and use the Taylor expansions of $f^2$ and $f^3$ up to a non-zero order. Find a polynomial link between both coefficients using equivalents. If they do not exist, $f^2 =o(h^4)$ so $f$ is $C^1$ in $0$ (?). -Use the general Taylor expansion (with integral remain) for $f^3$.  Suppose the coefficients are not all zeros, (change the bounds to $0$ and $1$), show that the parameter integral is $C^{\infty}$, then take the cube root (??).","['derivatives', 'real-analysis', 'taylor-expansion']"
2602427,Solution of heat equation.,"Let $u(x,t)$ be the bounded solution of $\frac{\partial  u}{\partial t} -\frac{\partial ^2  u}{\partial x^2}=0$ with $u(x, 0)=\dfrac{e^ {2x} -1}{e^{2x} +1}.$ Then $\lim _{t \rightarrow \infty}u(1, t)=$ $$(A)-\frac{1}{2}~~~~~~(B)\frac{1}{2}~~~~~~(C)-1~~~~~~(D)1.$$ By separation of variable the solution of the above PDE is: 
  $$u(x, t)=\begin{cases}
e^{- \lambda ^2 t}(c_1 \cos(\lambda x)+c_2 \sin(\lambda x)) & ~~~\text{if}~k=-{\lambda}^2<0 ,\\ 
e^{\lambda ^2 t}(c_1 e^{\lambda x}+c_2 e^{\lambda x}) & ~~~\text{if}~k={\lambda}^2>0 ,\\
c_1x+c_2 & ~~~\text{if}~k=0 .\\
\end{cases}$$
  where $k$ is separation constant.
  But when $t=0$, I cannot compare the solution with given $u(x,0)$, that's why I cannot find  $\lim _{t \rightarrow \infty}u(1, t).$ Please help.","['heat-equation', 'ordinary-differential-equations', 'partial-differential-equations']"
2602478,Prove every one-to-one conformal mapping of a disc onto another is a linear fractional transformation.,"Question : Prove by use of Schwarz's lemma that every one-to-one conformal mapping of a disc onto another (or a half plane) is given by a linear fractional transformation. I have known that there exists LFT such that it maps unit disc onto itself, but if holomorphic function $f$ is a 1-1 mapping of a disc onto another disc, can we conclude $f$ is LFT? My try is simplifying the question as Every one-to-one conformal mapping of a unit disc onto itself is given by a linear fractional transformation. Am I right? Sincerely thanks for your help!","['complex-analysis', 'conformal-geometry']"
2602500,An interesting identity involving Jacobi $\theta_4$ and $\zeta(2)$,"A recent question mentioned an integral identity involving Dedekind $\eta$ function and a special value for the complete elliptic integral of the first kind. I refrained from providing a complete answer, I rather tried to guide the OP through some hints, but we apparently reached a dead spot, concerning the following simplified version of the original problem:
$$\boxed{ \int_{0}^{+\infty}\left[\sum_{n\geq 1}(-1)^n e^{-n^2 x}\right]^2\,dx = \frac{\pi^2-3\pi\log 2}{12}.} \tag{A}$$
My solution goes as follows: The LHS of $(A)$ can be written in terms of $\sum_{m,n\geq 1}\frac{(-1)^{m+n}}{m^2+n^2}$, to be dealt with care since it is not absolutely convergent; We have $\frac{(-1)^{m+n}}{m^2+n^2}=\int_{0}^{+\infty}\frac{(-1)^n\sin(nx)}{n}(-1)^m e^{-mx}\,dx$, where $\sum_{m\geq 1}(-1)^m e^{-mx}$ is a simple geometric series and $\sum_{n\geq 1}\frac{(-1)^n \sin(nx)}{n}=-\arctan\tan\frac{x}{2}$ almost everywhere; The problem boils down to integrating over $\mathbb{R}^+$ the product between a sawtooth wave and the function $\frac{1}{e^x+1}$. Through the dilogarithms machinery or the residue theorem, to reach the RHS of $(A)$ is not difficult. I would use this question for collecting alternative/shorter/slicker proofs .","['fourier-series', 'alternative-proof', 'theta-functions', 'special-functions', 'sequences-and-series']"
2602517,"What will be area of shaded region, in two rectangles, where 2 vertex are common?","In a figure, there are two rectangles $ABCD$ and $DEBG$, each of lengh $7\ cm$ and width $3\ cm$. The area of shaded region. in $cm^2$ is approximately ? Options given : $12$, $10$, $8$, $4$","['rectangles', 'area', 'geometry']"
2602548,Is every self-adjoint operator bounded?,"This is a problem our teacher gave us and I have a feeling he forgot to mention some additional data. Let $H$ be a Hilbert space and $T : H \to H$ a everywhere-defined linear operator such that $T$ is self-adjoint, i.e. $\forall x,y \in H : \langle Tx,y\rangle=\langle x,Ty\rangle$ . Show that $T$ is bounded! Using Cauchy-Schwarz inequality it is clear that if $T$ is idempotent then it's bounded, but with just the information initially provided I don't feel like it's possible to prove the boundedness. Can somebody shed some light on this? Perhaps with a counter-example?","['functional-analysis', 'self-adjoint-operators', 'operator-theory', 'hilbert-spaces']"
2602556,Showing that the Lebesgue measure of a single point is zero.,"I want to show that the Lebesgue measure of a single point $x \in \mathbb{R}$ is $0$ i.e. $\lambda(\{x\})=0$. 
I have thought of something, but I'm not sure if this is correct. ATTEMPT: We choose $x \in \mathbb{R}$ and $\epsilon > 0$ arbitrarily. We know per the definition of the Lebesgue measure, that $\lambda( (x-\epsilon,x]) = x- (x-\epsilon)=\epsilon \leq \epsilon$. Thus, we can conclude, that $\lambda( (x-\epsilon,x])\rightarrow0$ when $\epsilon \rightarrow 0$. We know that $(x-\epsilon,x] \rightarrow \{x\}$ when $\epsilon \rightarrow 0$. Can we conclude now that $\lambda(\{x\})=0$? Thanks for your time, K. Kamal","['lebesgue-measure', 'measure-theory']"
2602561,"Find the angle between the two tangents drawn from the point $(1,2)$ to the ellipse $x^2+2y^2=3$.","Find the angle between the two tangents drawn from the point $(1,2)$ to the ellipse $x^2+2y^2=3$. The given ellipse is $\dfrac{x^2}{3}+\dfrac{y^2}{\frac{3}{2}}=1$ Any point on the ellipse is given by $(a\cos \theta,b\sin \theta)$ where $a=\sqrt 3,b=\frac{\sqrt 3}{\sqrt 2}$. Now  slope of  the tangent to the curve at  $(a\cos \theta,b\sin \theta)$ is $\dfrac{-a\cos \theta}{2b\sin \theta}$. Hence we have $\dfrac{b\sin \theta- 2}{a\cos \theta -1}=\dfrac{-a\cos \theta}{2b\sin \theta}$. On simplifying we get $4b\sin \theta +a\cos\theta =3$ If we  can find the value of $\theta $ from above then we can find the two points on the ellipse where the tangents touch them but I am unable to solve them. Please help to solve it. Any hints will be helpful","['angle', 'analytic-geometry', 'geometry', 'tangent-line', 'conic-sections']"
2602593,Orientability and trivialization of the tangent bundle over the 1-skeleton,"I was reading the following post on mathoverflow: https://mathoverflow.net/questions/80081/what-are-good-examples-of-spin-manifolds in an answer there is written that Orientability means the tangent bundle trivializes over a 1-skeleton. Dually you could think of that as saying the complement of a co-dimension 2 subcomplex has a trivial tangent bundle. As Lee Mosher suggests in the comment section, this means that the tangent bundle of an $n$ dimensional $M$ (homeomorphic to a simplicial complex) is trivial when restricted to the $1$-skeleton of $M$. Unfortunately, I do not manage to prove that this is equivalent to the definition of orientability that I am used to (for smooth manifolds). Namely that exists a  never vanishing section of the bundle of $n$-forms over an $n$ dimensional manifold.
Can someone explain it to me? Question: Show that the above definition of orientability  (in the smooth setting) is equivalent to the existence of a volume form.
  Also I wonder why we have that ""dual"" definition.","['algebraic-topology', 'smooth-manifolds', 'differential-geometry', 'differential-topology']"
2602623,101 town problem with connected road,"There are $101$ towns 
  There are $50$ roads entering each town and $50$ roads leaving each town. Each town is connected with every other town by a one way road. Prove that you can reach one from other by driving along at most two roads. Please help without graph theoretic solution. I am thinking of applying contradiction. I am not being able to think of a solution.","['combinatorics', 'graph-theory', 'discrete-mathematics']"
2602670,Degree of the union is the sum of degrees,"I've been proposed the following exercise: Let $X,Y\subseteq\mathbb{P}^n_k$ two algebraic sets (non-necessarilly irreducible varieties) of dimension $d$ such that $\dim(X\cap Y)<d$. Show that $\deg(X\cup Y)=\deg(X)+\deg(Y)$. Using this question I've been able to treat the case where $X$ and $Y$ are irreducible. Now I'm trying to deal with the case where they're not. For this case, the idea is that only the irreducible components of maximal dimension add something to the degree. Now, let $X=X_1\cup\dots\cup X_r$ a decomposition of $X$ in irreducible components and suppose WLOG that $X_1,\dots, X_i$, $i\leq r$, are those of maximal dimension $d$. Using the same argument of the question I mention above I get to the following point: $$
P_{X}+P_{\bigcap X_j}=\sum_{j=1}^r P_{X_j}
$$ Where $P$ denotes the Hilbert polynomial. On the RHS I know that the leading coefficient is $\sum_{j=1}^i \deg(X_j)/d!$. But on the LHS I've got that intersection. If I could say that the dimension of the intersection is strictly less than $d$, then I would have that $\deg(X)=\sum_{j=1}^i\deg(X_j)$, but I'm not sure about that. Any ideas of how I could continue? Edit: I've tried using the whole union instead of dealing with $X$ and $Y$ separately. I denoted $Y_j, j=1,\dots, s$ the irreducible components of $Y$, where $Y_1,\dots, Y_k$ are of dimension $d$. There, I can say by hypothesis, that $\dim(\bigcap X_j\cap\bigcap Y_j)<d$, and then $\deg(X\cup Y)=\sum_{j=1}^r\deg(X_j)+\sum_{j=1}^k\deg(Y_j)$. Since they're now irreducible, I can say that $\sum_{j=1}^r\deg(X_j)=\deg(\bigcup_{j=1}^r X_j)$ and $\sum_{j=1}^k\deg(Y_j)=\deg(\bigcup_{j=1}^k Y_j)$. I would need to add the $X_j$ and $Y_j$ that are of lower dimension. Can I do that because they don't change the degree and thus $\deg(\bigcup_{j=1}^k Y_j)=\deg(\bigcup_{j=1}^s Y_j)=\deg(Y)$ (same with $X$)?","['polynomials', 'projective-geometry', 'algebraic-geometry', 'commutative-algebra']"
2602706,Kernel of the map $\bigoplus_{k\geq 0} \Lambda^k \pi: \bigoplus_{k\geq 0} \Lambda^k V\longrightarrow \bigoplus_{l\geq 0} \Lambda^l(V/W)$?,"Let $V$ be a vector space over a field $\mathbb K$ and $U\subseteq V$ a subspace. Let $$\pi:V\longrightarrow V/U,$$ the cannonical projection. For every $k\geq 0$ this induces a linear map $$\Lambda^k \pi:\Lambda^k V\longrightarrow \Lambda^k (V/U),$$ where $\Lambda^0 \pi=\pi$. This allows use to define a map $$\bigoplus_{k\geq 0}\Lambda^k \pi: \bigoplus_{k\geq 0} \Lambda^k V\longrightarrow \bigoplus_{k\geq 0} \Lambda^k (V/U).$$ This is the only linear make which make the diagram below to commute for every $k$: where $\jmath_k$ and $\jmath_k^\prime$ are the inclusions. What is the kernel of the map $\displaystyle\bigoplus_{k\geq 0} \Lambda^k \pi$? Well, I know that: $$\mathsf{Ker}(\bigoplus_{k\geq 0} \Lambda^k \pi)=\bigoplus_{k\geq 0} \mathsf{Ker}(\Lambda^k \pi),$$ so the problem boils down to determining $\mathsf{Ker}(\Lambda^k \pi)$. Thanks.","['linear-algebra', 'linear-transformations']"
2602723,Find value of $\lambda$,"$S$ is a circle having center at $(0, a)$ and radius $b\lt a$. A variable circle centred at $(\alpha, 0)$ and touching the circle $S$ meets $X$ axis at $ M$ and $N$. A point $P=(0,\pm \lambda\sqrt {a^2-b^2})$ on the $y$ axis such that angle $MNP$ is a constant for any choice of $\alpha$ then find $\lambda$. My work: 
I wrote down the equation of circle $S$ and I have tried using the cosine rule in the two triangles having the same constant angle $MNP$. I also tried using basic algebra and the Stewart's theorem in some of the triangles I could do work in.  But this was of no use. Moreover so many of variables are confusing me a lot.  Any help is greatly appreciated.","['circles', 'trigonometry', 'coordinate-systems', 'geometry']"
2602786,Proof that poisson process has right continuous modification,"How can one prove that a Poisson process ($X_t$) has a right-continuous modification? That is, we say that $Y_t$ is a modification of $X_t$ if for each $t $,$$\ \mathsf{P}(X_t = Y_t) = 1,$$
and $Y_t$ is a (almost everywhere) right-continuous process if for each $t$, $$\mathsf{P}\left(\lim_{s\rightarrow t+0} Y_s = Y_t\right) = 1.$$ Here is what I have so far: Let $\Omega_1 = \{\omega\ |\ X_s < X_t\ \forall s, t \in \mathbb{Q}\}$ Perhaps $\mathsf{P}(\Omega_1) = 1$. Then let $$
Y_t(\omega) = \begin{cases}
\lim_{s \rightarrow t+0,\ s\in\mathbb{Q}}X_s(\omega) & \omega \in \Omega_1\\
0 & \omega \notin \Omega_1
\end{cases}
$$ $Y_t$ is limit of countable number of random variables, so it is random variable. But I don't know why $Y_t$ is right continuous and why it is modification.","['stochastic-processes', 'poisson-process', 'probability-theory']"
2602799,Every infinite subset of $E$ in $\mathbb R^k$ having a limit point in $E$ implies $E$ is closed,"This is exactly what is written in Walter Rudin chapter 2, Theorem 2.41: If $E$ is not closed, then there is a point $\mathbf{x}_o \in \mathbb{R}^k$ which is a limit point of $E$ but not a point of $E$. For $n = 1,2,3, \dots $ there are points $\mathbf{x}_n \in E$ such that $|\mathbf{x}_n-\mathbf{x}_o| < \frac{1}{n}$. Let $S$ be the set of these points $\mathbf{x}_n$. Then $S$ is infinite (otherwise $|\mathbf{x}_n-\mathbf{x}_o|$ would have a constant positive value, for infinitely many $n$), $S$ has  $\mathbf{x}_o$ as a limit point, and $S$ has no other limit point in $\mathbb{R}^k$. For if $\mathbf{y} \in \mathbb{R}^k, \mathbf{y} \neq \mathbf{x}_o$, then
\begin{align}
|\mathbf{x}_n-\mathbf{y}| \geq{} &|\mathbf{x}_o-\mathbf{y}| - |\mathbf{x}_n-\mathbf{x}_o|\\
\geq {} & |\mathbf{x}_o-\mathbf{y}| - \dfrac{1}{n} \geq \dfrac{1}{2} |\mathbf{x}_o-\mathbf{y}| 
\end{align}
for all but finitely many $n$. This shows that $\mathbf{y}$ is not a limit point of $S$. The question: I'm stuck in understanding the reason behind why $S$ is infinite. 
Also I need clarification why the last inequality holds. May someone help, please?","['general-topology', 'real-analysis', 'compactness']"
2602819,What's the relationship between $\frac{\text{d}^n\gamma}{\text{d}s^n}$ and $\left(\gamma\circ s^{-1}\right)^{(n)}|_{s(t)}$?,"Let $\gamma : I\subset\mathbb{R}\to\mathbb{R}^n$ be a $C^\infty$ curve such that $\|\gamma'\|>0$ everywhere. We let $s(t)$ be the arclength of $\gamma$ at time $t$, which has a smooth inverse everywhere (where defined), meaning $\gamma\circ s^{-1}$ is a unit-speed reparametrization of $\gamma$ (that is, $\|(\gamma\circ s^{-1})'\| = 1$). Disclaimer: Everywhere I use a $'$ symbol, that indicates differentiation with respect to $t$. Now, it's easy to see that $\left( \gamma\circ s^{-1} \right)'|_{s(t)}$ (that is, the derivative of $(\gamma\circ s^{-1})$ with respect to $t$, evaluated at $t=s(t)$) is equal to $\frac{\text{d}\gamma}{\text{d}s}$. In fact both are simply $1$, but for good measure
$$
\left.(\gamma\circ s^{-1})'\right|_{s(t)} = \left.(\gamma'\circ s^{-1})(s^{-1})'\right|_{s(t)} = \left.\frac{\gamma'\circ s^{-1}}{s'\circ s^{-1}}\right|_{s(t)} = \frac{\gamma'}{s'} = \frac{\text{d}\gamma/\text{d}t}{\text{d}s/\text{d}t} = \frac{\text{d}\gamma}{\text{d}s}
$$ Above we used the inverse function theorem to write $(s^{-1})' = \frac1{s'\circ s^{-1}}$. Similarly, $(s^{-1})'' = -\frac{s''\circ s^{-1}}{(s'\circ s^{-1})^3}$, and $s'' = \frac{\text{d}}{\text{d}t}\|\gamma'\| = \frac{\left\langle\gamma',\gamma''\right\rangle}{\|\gamma'\|}$. We can try to extend this for higher order derivatives. First we get
$$
\left.(\gamma\circ s^{-1})''\right|_{s(t)} = \kappa(t) = \frac{\gamma''\left\langle\gamma',\gamma'\right\rangle - \gamma'\left\langle\gamma',\gamma''\right\rangle}{\|\gamma'\|^4}
$$
(in $\mathbb{R}^3$ you can simplify this significantly using the triple vector product formula, but we will leave it as this). For $\frac{\text{d}^2\gamma}{\text{d}s^2}$ we have
$$
\frac{\text{d}}{\text{d}s}\left(\frac{\text{d}\gamma}{\text{d}s}\right) = \frac{\text{d}}{\text{d}s}\left(\frac{\text{d}\gamma/\text{d}t}{\text{d}s/\text{d}t}\right) = \frac{\text{d}\left(\frac{\text{d}\gamma/\text{d}t}{\text{d}s/\text{d}t}\right)/\text{d}t}{\text{d}s/\text{d}t}
$$
The numerator is given by
$$
\begin{aligned}
\frac{\text{d}}{\text{d}t}\left(\frac{\text{d}\gamma/\text{d}t}{\text{d}s/\text{d}t}\right) & = \frac{(\text{d}\gamma/\text{d}t)'(\text{d}s/\text{d}t)-(\text{d}\gamma/\text{d}t)(\text{d}s/\text{d}t)'}{(\text{d}s/\text{d}t)^2} \\
& = \frac{\gamma''\|\gamma'\|-\gamma'\left\langle\gamma',\gamma''\right\rangle/\|\gamma'\|}{\|\gamma'\|^2} \\
& = \frac{\gamma''\left\langle\gamma',\gamma'\right\rangle - \gamma'\left\langle\gamma',\gamma''\right\rangle}{\|\gamma'\|^3}
\end{aligned}
$$
and so we see that
$$
\frac{\text{d}^2\gamma}{\text{d}s^2} = \frac{\gamma''\left\langle\gamma',\gamma'\right\rangle - \gamma'\left\langle\gamma',\gamma''\right\rangle}{\|\gamma'\|^4} = \left.(\gamma\circ s^{-1})''\right|_{s(t)}
$$ I imagine this continues for higher order derivatives, and I'm almost certain I'm going about this in a really, really, really roundabout way, but I don't have an intuition for what the relationship between the objects $\frac{\text{d}^n\gamma}{\text{d}s^n}$ and $(\gamma\circ s^{-1})^{(n)}(s(t))$ actually are . Can someone give me a thorough explanation as to why these two objects should be the same (or if they aren't, why they aren't, and how they're related), so that I can skip the heavy computation each time I need to use either of the two?","['derivatives', 'curves', 'arc-length', 'differential-geometry']"
2602834,Compactification of separable metric spaces,"Suppose $(X,d)$ is a separable metric space with $d\leq 1$, and let $(x_n)$ and $(x'_n)$ be two dense sequences. Define embeddings $$\varphi,\varphi':X\to[0,1]^\mathbb{N}$$
by putting $\varphi(x)=(d(x,x_n))_{n}$ and $\varphi'(x)=(d(x,x'_n))_{n}$ respectively. Then $\mathcal{X}=\overline{\varphi(X)}$ and $\mathcal{X}'=\overline{\varphi'(X)}$ are two metrizable compactifications of $X$. It seems to me that $\mathcal{X}$ and $\mathcal{X}'$ always homeomorphic. In order to define a maps $\mathcal{X}\xrightarrow\theta\mathcal{X}'$ and $\mathcal{X}'\xrightarrow{\theta'}\mathcal{X}$ we define $\theta$ on $\varphi(X)$ as $\varphi'\circ\varphi^{-1}$ (I'm cutting corners here), and extend it by uniform continuity (and similarly for $\theta'$). I haven't checked the details, but this will produce (uniformly) continuous maps $\mathcal{X}\xrightarrow\theta\mathcal{X}'$ and
$\mathcal{X}'\xrightarrow{\theta'}\mathcal{X}$ satisfying $\theta\circ\varphi=\varphi'$ and $\theta'\circ\varphi'=\varphi$ so that $\theta\circ\theta'$ coïncides with the identity function on the dense subset $\varphi(X)\subset\mathcal{X}$, and $\theta'\circ\theta$ coïncides with the identity function on the dense subset $\varphi'(X)\subset\mathcal{X}'$. And thus we should get that $\theta$ and $\theta'$ are inverse homeomorphisms of one another. Question : Is the above reasoning sound, and if so, what is the name of this compactification ? Does it have a universal property ? Maybe something similar to the Stone-Cech compactification but in the category of compact metric spaces ?","['general-topology', 'metric-spaces', 'separable-spaces', 'compactification']"
2602853,Common denominator example,"Assuming sum:
$$\sum_{k=0}^n \binom{n}{k}\frac{(-1)^k}{k+\alpha+1} =\frac{1}{0!(\alpha+1)}-\frac{n}{1!(\alpha+2)} + \frac{n(n-1)}{2!(\alpha+3)}+\cdots+\frac{n!(-1)^n}{n!(\alpha+n+1)}$$
How does one derive generally, that this can be rewritten as:
$$\frac{n!}{(\alpha+1)(\alpha+2)(\alpha+3)\cdots(\alpha+n+1)}$$
I've got to derive this for smaller $n$, maybe $n=2,3,\ldots$ But i can't seem to get to derive it generally for all $n$. I don't think mathematical induction is the way here. Would someone help me out please?","['combinatorics', 'summation', 'fractions', 'sequences-and-series']"
2602906,Inverting a derivative: convex conjugate and envelope theorem [closed],"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 6 years ago . Improve this question I need your help to go through the following mathematical steps. I am completely new to the rules used below and any help would be extremely appreciated. Consider the function $G(\boldsymbol{x}): \mathbb{R}^J \rightarrow \mathbb{R}$, where $\boldsymbol{x}\equiv (x_1,..., x_J)$. $G(\cdot )$ is known: give me any $\boldsymbol{x}$ and I can tell you the value $G(\boldsymbol{x})$. We know that $G(\cdot )$ is convex. Fix $\boldsymbol{x}=\boldsymbol{x}^*$ and let 
$$
(1) \hspace{1cm}\frac{\partial G(\boldsymbol{x}^*)}{\partial x_j}=a^*_j \text{ }\forall j=1,...,J
$$
where $a^*_j\in \mathbb{R}$ is a (known) parameter $\forall j=1,...,J$ and $\boldsymbol{a}^*\equiv (a_1,..., a_J)$. My objective is to recover $\boldsymbol{x}^*$ by ""inverting"" (1). The source that I found uses the following steps: A) Consider the function $\bar{G}(\boldsymbol{a}): \mathbb{R}^J\rightarrow \mathbb{R}$ prescribed by
$$
\bar{G}(\boldsymbol{a})\equiv 
\begin{cases}
 \max_{\boldsymbol{\tilde{x}}} \Big(\sum_{j=1}^J \tilde{x}_j a_j- G(\boldsymbol{\tilde{x}}) \Big) & \text{if  $\sum_{j=1}^Ja_j\leq 1$} \\
 \infty & \text{otherwise}
 \end{cases}
$$
This is the Legendre-Fenchel transform or convex conjugate of $G(\boldsymbol{x})$ and we know that $\bar{G}(\boldsymbol{a})$ is convex. Since $G(\cdot )$ is known, also $\bar{G}(\cdot)$ is known. B) Let $\boldsymbol{x}^{\text{opt}}(\boldsymbol{a}): \mathbb{R}^J\rightarrow \mathbb{R}^J$ be the function delivering the (unique?) solution of 
$$
\max_{\boldsymbol{\tilde{x}}} \Big(\sum_{j=1}^J \tilde{x}_j a_j- G(\boldsymbol{\tilde{x}}) \Big)
$$
for every $\boldsymbol{a}\in \mathbb{R}^J$. C) By the envelope theorem
$$
\frac{\partial \bar{G}(\boldsymbol{a})}{\partial a_j}=x_j^{\text{opt}}(\boldsymbol{a}) \text{ }\forall j\in \{1,...,J\}
$$ D) The source concludes that 
$$
\frac{\partial \bar{G}(\boldsymbol{a}^*)}{\partial a_j}=x_j^* \text{ }\forall j\in \{1,...,J\}
$$ E) Since $\frac{\partial \bar{G}(\boldsymbol{a}^*)}{\partial a_j}$ is known $\forall j\in \{1,...,J\}$, we have recovered $\boldsymbol{x}^*$. My doubts: (i) How do we know that only $\boldsymbol{x}^*$ satisfies (1)? (ii) Which properties of $G(\cdot)$ are sufficient to go through the steps above (convexity, strict convexity, C1,...)? And could you highlight where do we use those properties? (iii) Where do we use the convexity (strict?) of $\bar{G}(\cdot)$? Do we need other properties of $\bar{G}(\cdot)$ to apply the envelope theorem? (iv) How can we go from (C) to (D)?","['derivatives', 'convex-optimization', 'optimization', 'convex-analysis']"
2602909,modular form $q-$expansion not quite same as Laurent expansion to define the order of function at a point?,"Let $f$ be a modular form where $f:H\to \bar{C}$ is any meromorphic modular form. Say I want to study the singularity behaviour at $i\infty$ of $f$. I can consider $z\to \frac{1}{z}$ map. Then $f(\frac{1}{z})$ will give rise to laurent expansion $g(z)$ around $z=0$. The other thing to get is $q-$ expansion $h(q)$ for $f$ at $q=0$ where $q=e^{2\pi iz}$. Q1: Why order of $f$ at $i\infty$ is defined in terms of lowest degree of $q$ expansion here instead of laurent expansion $f(\frac{1}{z})$ at $z=0$'s degree? Q2: Should not they contain the same information? Q3: Say $f$ is weight $0$, then $f'$ is modular form of weight $2$.(This can be proven very easily.) Now I can consider either Laurent expansion's order at $i\infty$ and $q-$expansion's order of $f'$.($f'=\frac{df}{dz}$). They give me different order here for $f=j$ where $j$ is absolute modular invariant defined over the $H$ upper half plane. Was there anything wrong with my reasoning below? Q3 a) Since $j$ has simple pole at $i\infty$, $j(w)\sim\frac{1}{w}$ where $w=\frac{1}{z}$.(I am only concerned about the order the pole here.) So $j'(w)\sim-\frac{1}{w^2}$ by taking derivative. This gives me a second order pole from laurent expansion's derivative. So order of $j'(z)$ at $z=i\infty$ is $-2$. Q3 b) Consider $q=e^{2\pi iz}$ expansion and $j$ and one sees $j(q)\sim \frac{1}{q}$ as leading order. Take derivative to see $\frac{dj}{dz}=\frac{dj(q)}{dq}\frac{dq}{dz}\sim \frac{1}{q^2}\times -2\pi i q\sim\frac{1}{q}$. This says order of $j'$ at $i\infty$ is $-1$.","['number-theory', 'complex-analysis', 'modular-forms', 'meromorphic-functions']"
2602924,Is there a proof (apart from contradiction) to show that identity does not exist.,"I have example in which there is only option of contradiction to prove that identity does not exist. Consider the binary operation : $a*b= 2a +b, \forall a,b \in \mathbb{Z}$. Suppose that $\mathbb{Z}$ has an identity $e$ w.r.t. $*$. Then, $e*1 = 2e + 1  = 1 \implies  2e = 0 \implies   e=0$.
But, $1*0 = 2 + 0 = 2 \ne 1$, so $0$ cannot be an identity.",['functions']
2602939,What is the most general algebraic structure that a finite set has?,"For an object $X$ in a category with finite products, define its endomorphism Lawvere theory to be the Lawvere theory generated by $X$: its $n$-ary operations are given by $\text{Hom}(X^n, X)$, and so accordingly it describes the most general algebraic structure that $X$ possesses. Some quick examples: The endomorphism Lawvere theory of the abelian group $\mathbb{Z}$ is the Lawvere theory of abelian groups. (This is specific to abelian groups.) The endomorphism Lawvere theory of the set $2 = \{ 0, 1 \}$ is the Lawvere theory of Boolean algebras, or equivalently Boolean rings. This recently came up here . The endomorphism Lawvere theory of Sierpinski space is, I believe, the Lawvere theory of (bounded) distributive lattices, although I haven't checked this. As an interesting generalization of the second example, the endomorphism Lawvere theory of the set $3 = \{ 0, 1, 2 \}$ is the Lawvere theory of a ternary generalization of Boolean rings: $\mathbb{F}_3$-algebras such that every element $x$ satisfies $x^3 = x$. This follows from the fact that every ternary function $3^n \to 3$ can be represented as a polynomial over $\mathbb{F}_3$ which is unique if we require that the degree of the polynomial in each variable is at most $2$, or equivalently if we impose the relation $x^3 = x$ on each variable. Unfortunately, I'm not sure how to continue this pattern: that is, What is the endomorphism Lawvere theory of the finite set $4 = \{ 0, 1, 2, 3 \}$? Of any finite set? (An answer should be a description in terms of generators and relations - some operations that generate all the others under composition and product, and the axioms they satisfy - which is ideally related to familiar algebraic structures such as rings.) I am only confident I know the answer for a finite set of prime size $p$, which again generalizes the Boolean case: it should be $\mathbb{F}_p$-algebras such that every element satisfies $x^p = x$.","['universal-algebra', 'abstract-algebra', 'ring-theory']"
2602960,Proof of weak convergence of empirical measure,"Let's say that we have a complete and separable metric space in other words a Polish space $(S, d)$. If we say that we have $n$ I.I.D random elements $(Y_0, Y_1 ...)$ in our space $S$ with a common distribution that we'll denote $P$. If we define an empirical measure $P_{n,w}$ using observations $(Y_1(w), ..., Y_n(w))$. $$P_{n,w} = \frac{1}{n}\sum_{i=1}^{n}\delta_{X_i}(w)$$ Where the $\delta_x$ is a measure that places a unit mass on $x$. How do we show that given that S is seperable and complete, then $P_{n,w} \rightarrow P$ as $n$ goes to $\infty$ The convergence is weak. Could we use the equivalences from Portmanteau theorem's to prove this?","['functional-analysis', 'real-analysis', 'weak-convergence', 'measure-theory']"
2602967,Intuition behind the irrationality measure,"The irrationality measure $\mu(x)$ of a real number $x$ is defined to be the supremum of the set of real numbers $\mu$ such that the inequalities
$$0 < \left| x - \frac{p}{q} \right| < \frac{1}{q^\mu} \qquad (1)$$
hold for an infinite number of integer pairs $(p, q)$ with $q > 0$. Wikipedia says that $\mu(x)$ measures ""how 'closely' $x$ can be approximated by rationals,"" but I'm very unclear about exactly how it does it, because the ""approximability"" of a real number seems to depend non-monotonically on $\mu$, with real numbers with low and high values of $\mu(x)$ easily approximable by rationals, and real numbers with intermediate values of $\mu(x)$ difficult to approximate by rationals. Specifically, we have $\mu(x) \geq 1$, with the preimage of $\mu(x) = 1$ is exactly the rationals $\mathbb{Q}$. the preimage of $\mu(x) = 2$ contains all of the irrational algebraic numbers $\bar{Q} \setminus Q$ (by Roth's theorem), as well as almost all of the transcendental numbers (in the Lebesgue-measure sense), including $e$ and $\varphi$. the preimage of $\mu(x) \in (2, \infty)$ is a measure-zero subset of the transcendental numbers the preimage of $\mu(x) = \infty$ is the set of Liouville numbers (this set is ""large"" in the sense of having the cardinality of the continuum and being dense in the reals, but ""small"" in the sense of having Lebesgue measure zero). The name ""irrationality measure"" seems to imply that if $\mu(x) > \mu(y)$, then $x$ is ""more irrational"" than $y$, i.e. is harder to approximate by a sequence of rational numbers. But in fact the opposite is true; the Louiville numbers, which have $\mu(x) = \infty$, are unusually easy to approximate by a sequence of rationals, although of course not as easy as the rationals themselves, which have $\mu(x) = 1$. How do I understand this strange non-monotonicity? As I understand it, the problem stems entirely from the first inequality in (1), which seems extremely arbitrary and conceptually unnatural. If we remove that inequality, then the second inequality has a very nice interpretation: the error in the Diophantine approximation sequence decreases with $q$ as a power law with exponent $\mu$, and higher $\mu$ means that the error decays faster. So under this proposed modification, $\mu$ would be interpreted as a rationality measure: almost all irrational numbers would have the minimal value $\mu(x) = 2$, but a few numbers would be unusally easy to approximate and have $\mu(x) > 2$. For a rational number we would trivially have $\mu = \infty$ (under this modified definition), because the errors would vanish identically after some finite $q$. Liouville numbers would be unusual in that their Diophantine approximations would vanish with $q$ faster than any power law, although never hitting zero, so they would also have $\mu(x) = \infty$ just like the rationals. Is there some motivation for the first inequality that I'm missing? It seems to enormously decrease the conceptual clarity of $\mu$ by making it a non-monotonic measure of Diophantine approximability.","['real-analysis', 'real-numbers', 'liouville-numbers', 'diophantine-approximation', 'rational-numbers']"
2602976,Stability of differential system when eigenvalue is zero,"I'm trying to figure out the stability of the origin $O(0,0)$ for the following system of differential equations :
$$x'= -x^2 + 2xy$$
$$y'=-2y+y^2+xy$$
using the following method/theorem/lemma which I found in my notes (it's the only thing that is mentioned about systems which yield an eigenvalue equal to zero) : Zero Eigenvalue : Stability Let $f=(f_1,f_2) : \mathbb R^2 \to \mathbb R^2$, where $f\in C^k(\mathbb R^2), k\geq 1, f(0) = 0, Df(0)=0$. Consider the following system :
  $$x'=f_1(x,y)$$
  $$y' = -y + f_2(x,y)$$ which can be translated to : $$\begin{bmatrix} x \\ y \end{bmatrix}'= \begin{bmatrix} 0 & 0 \\ 0 & -1 \end{bmatrix}\begin{bmatrix} x \\ y \end{bmatrix} + f(x,y)$$ Observe that the linearized system has an eigenvalue equal to zero and that the critical point $O(0,0)$ is non-hyperbolic. Solving the system of equations : 
  $$\begin{cases} -y(x) + f_2(x,y(x)) = 0 \\ y(0) = y'(0) = 0 \end{cases}$$ the function $y(x)$ can be expressed in terms of $x$. Finally : 
  $$f_1(x,y(x))=ax^k+O(|x|^{k+1})$$
  which leads to the origin $O(0,0)$ being asymptotically stable if $a<0$ and $k$ odd, else it's unstable. Attempt/Discussion : I'll let : $$f_1(x,y) = -x^2 + 2xy$$
$$f_2(x,y) = y^2 + xy$$ since having $-2$ instead of $-1$ at my system's matrix will not change anything (just a multiplication constant). Then, it's indeed easy to see that the matrix of the linearized system at the origin describes a non-hyperbolic critical point, as : $$\det(J(0,0)) = 0$$ and that we have a zero eigenvalue, since $\det(J(0,0)-λI)=0$ yields a solution $λ=0$ (where $J(0,0)$ is the Jacobian at $(0,0)$). Solving now : $$-2y(x) + f_2(x,y(x)) = 0 \Rightarrow -2y(x) + y^2(x) + xy(x) = 0 \Rightarrow y^2(x) + (x-2)y(x) = 0 \Rightarrow y(x)[y(x) + x-2] = 0$$ which means that $y(x)=0$ or that $y(x) = 2-x$. Now, I guess I'll have to check both cases, which means that at first : $$f_1(x,y(x)=0) = -x^2$$ which, according to the theorem, yields that the origin $O(0,0)$ is unstable. Checking for the other case : $$f_1(x,y(x)=2-x)=-(2-x)^2 + 2x(2-x)=-4 + 4x - x^2 + 4x - 2x^2$$ 
$$=$$
$$-3x^2 + 8x - 4$$ Now, again, the power of $x$ is not odd so it means automatically that the origin is unstable ? Questioning this though, since the form is not the one reported by the theorem. What does the notation $O(|x|^{k+1})$ specifically refers too in such example ? Also, wanted to ask, is my approach correct ? It seems that this is the only theorem-method we have been taught/expected to study for systems with an eigenvalue being zero, but I found a hard time applying it to more complicated systems (let's say with constants and higher powers).","['eigenvalues-eigenvectors', 'dynamical-systems', 'stability-theory', 'stability-in-odes', 'ordinary-differential-equations']"
2602984,"Is the relation on $\Bbb R \times\Bbb Z$ given by $(a,b) \sim (c,d) \iff (\exists t \ne0)(\exists n \in \Bbb Z)(c = at \land b^2 = 3n+d^2)$ symmetric?","Check if $R$ defined on $\mathbb R \times \mathbb Z$ is symmetric, when $$(a,b) \sim (c,d) \iff (\exists t \ne0)(\exists n \in \mathbb Z)(c = at \land b^2 = 3n+d^2)$$ I have attempted to solve this but I got stuck very quickly. If $R$ was symmetric, then it would imply that \begin{align*}
&(\exists t\ne0)(\exists n \in \mathbb Z)(c = at \land b^2 = 3n+d^2) \\
\implies &(\exists t_2 \ne 0)(\exists m \in \mathbb Z)(a = ct_2 \land d^2 = b^2 + 3m)
\end{align*} Is enough to set $t_2 = \frac 1 t$ and $3n = -3m$ ?","['relations', 'solution-verification', 'elementary-set-theory', 'discrete-mathematics']"
2603031,Unsolved Elementary Integrals,"I am currently in Integral Calculus, and I wondered if I could get a little creative with my practice. I was curious if there were any unsolved, but rather simple (solvable using methods taught in calc I/II), so that I could be practicing the integration tehniques I have learned while also solving a problem that hasn't been solved yet. I have consulted lists of integrals, but those are in a more general form (constants are represented by letters, etc.), and I was wondering if there was a place I could find lists of indefinite integrals that had never been calculated but weren't that difficult. Also, on a side note, if you happen to know any good resources for reading about more techniques of integration, I am currently looking to expand my ""toolbox"" of integration methods. Thank you all, and I hope each and every one of you is having a nice day.","['integration', 'soft-question', 'calculus']"
2603043,Example of quotient restriction of quotient mapping which is not quotient,"Here's the problem that I'm trying to solve: Let $f: X \to Y$ be a quotient mapping. Find an example of $f, X, Y$ where for some $A \subseteq X$ , $f|_{A}:A \to f(A)$ is not a quotient mapping. If $B \subseteq Y$ is open or closed, prove that $f|_{f^{-1}(B)}:f^{-1}(B) \to B$ is a quotient mapping. Find an example of $f, X, Y$ and $B \subseteq Y$ which is neither open nor closed, such that $f|_{f^{-1}(B)}:f^{-1}(B) \to B$ is not a quotient map. For 1, I noticed that $f:[0,1] \to \mathbb{S}^{1}$ , $f(x)=e^{2\pi i x}$ is a quotient map, but $f|_{[0,1)}:[0,1) \to \mathbb{S}^{1}$ is not: $f^{-1}(f([0, \frac{1}{8})))=[0,\frac{1}{8})$ , which is open in $[0,1)$ , but $f([0,\frac{1}{8}))$ is not open in $\mathbb{S}^{1}$ . For 2, things are straightforward: if $U \subseteq B$ is open in $B$ , then $U = B \cap V$ , where $V$ is open in $Y$ .Now, $f|_{f^{-1}(B)}(U)=f^{-1}(B) \cap f^{-1}(V) \in \tau_{f^{-1}(B)}$ . The other direction follows because the restriction of a conitnuous function is continuous. However, for $3$ , the counterexample for $1$ doesn't work, since $[0,1)$ is not the inverse image of any subset of $\mathbb{S}^{1}$ . What would be a good example of illustrating $3$ ?",['general-topology']
2603060,Approximation by simple functions on a product $\sigma$-algebra,"Let $(\Omega_i,\mathcal A_i)$ be a measureable space $\mathcal M_i\subseteq2^{\Omega_i}$ be a $\pi$-system with $\Omega_i\in\mathcal M_i$ and $\sigma(\mathcal M_i)=\mathcal A_i$ $\mathcal E(M_1\times\mathcal M_2)$ be the $\mathbb R$-vector space of functions $H:\Omega_1\times\Omega_2\to\mathbb R$ with $$H=\sum_{i=1}^k1_{A_i}x_i$$ for some $k\in\mathbb N$, pairwise disjoint $A_1,\ldots,A_k\in\mathcal M_1\times\mathcal M_2$ and $x_1,\ldots,x_k\in\mathbb R$ Note that $\mathcal M:=\mathcal M_1\times\mathcal M_2$ is a $\pi$-system with $\Omega_1\times\Omega_2\in\mathcal M$. Let $$\mathcal H:=\left\{H:\Omega_1\times\Omega_2\to\mathbb R\mid\exists(H_n)_{n\in\mathbb N}\subseteq\mathcal E(\mathcal M_1\times\mathcal M_2):H_n\xrightarrow{n\to\infty}H\right\}\;.$$ Note that $\mathcal H$ is a $\mathbb R$-vector space with $$1_A\in\mathcal H\;\;\;\text{for all }A\in\mathcal M\;.$$ I would like to show that $\mathcal H$ contains any $\mathcal A_1\otimes\mathcal A_2$-measurable function $\Omega_1\times\Omega\to\mathbb R$. As Veridian Dynamics pointed out, this would follow from the functional monotone class theorem , if we are able to show the following: If $H:\Omega_1\times\Omega_2\to\mathbb R$ and $(H_n)_{n\in\mathbb N}\subseteq\mathcal H$ with $$0\le H_n\le H_{n+1}\;\;\;\text{for all }n\in\mathbb N$$ and $H_n\xrightarrow{n\to\infty}H$, then $H\in\mathcal H$. How can we show that?","['real-analysis', 'measure-theory']"
2603064,"Prove that any point I pick on the plane will lie on a line defined by integer coordinates and $(0,0)$?","I was playing around with the idea of infinity, and I thought of this question. It may or may not have a solution, but nonetheless it is interesting. I start at the origin of a cartesian plane $(0,0)$. Then, I draw a line to each and every lattice point. $~$i.e. I draw a line from $(0,0)$ to all $(x,y)$ for $x,y \in \mathbb Z$. It might look something like this, except there would obviously be a lot more lines: The question is to prove (or disprove) that any point $(p,q)$ for $p,q \in \mathbb R$ I pick will lie on a line. For example, I'll pick the point $(2.5,-7)$. That lies on the line created by joining $(0,0)$ with $(5,-14)$. It is simple to prove rational numbers, as you just multiply until you get a whole number. I am more interested in the irrationals. Say I pick the point $(\pi, \sqrt 2)$. Because the lattice points go on forever, intuitively, there must be a line which the point $(\pi, \sqrt 2)$ lies on. But I have no way to find it. Is this some sort of a paradox?","['elementary-number-theory', 'infinity', 'geometry']"
2603068,Symbol for set operation,"I'd like to know if there is already a symbol (notation) for grouping similar elements in a set. For example, suppose
$$
S=\{\underbrace{a_1, a_2, \cdots}_{G_1}, \underbrace{b_1,b_2, \cdots}_{G_2},\cdots\}=\{G_1, G_2\}
$$
I want to use $G_1$ to represent $a_1, a_2, \cdots$ and $G_2$ to represent $b_1, b_2, \cdots$. So basically $G_1$ is the collection of $a_i$ but without curly bracket. Is there a symbol or notation for such operation?","['notation', 'elementary-set-theory']"
2603073,Doubt on a proof that $\lim_{x\rightarrow0}{\frac{\sin{x}}{x}}=1$,"The full proof can be found here . Basically, we compare the three areas that depend on $x$ in the circle of radius $1$ shown below. Regardless of the value of $x$, we should have $$\text{area of sector OAC} < \text{area of triangle OAP} < \text{area of sector OBP}$$
$$\frac{1}{2}x(\cos{x})^2<\frac{1}{2}(\cos{x})(\sin{x})<\frac{x}{2}$$ I have two questions about it: 1) Shouldn't there be a $\le$ instead of the $<$ sign in the inequality above, since the areas of sector $OAC$ and the area of triangle $OAP$ both become zero when $x=\frac{\pi}{2}$? 2) If the value of $x$ is such that we end up in the fourth quadrant, the value of $\sin{x}$ becomes negative and the inequality no longer holds (since $\frac{1}{2}x(\cos{x})^2>0$ and $\frac{1}{2}(\cos{x})(\sin{x})<0$). How can we go around this? Thanks in advance.","['trigonometry', 'calculus', 'limits']"
2603079,Characterize sigma algebra generated by a set,Let $E$ be a set. Let $C$ be a collection of subsets of E. Denote by $\sigma(C)$ the $\sigma$-algebra generated by $C$ i.e. the intersection of all $\sigma$-algebra containing $C$. Is it true that any element $A \in \sigma(C)$ can be written as a countable union/intersection/complement of elements of $C$? I would say that it is true but was told that it is only true if $E$ is a separable set. Why is that so?,['measure-theory']
2603105,$L^1(\mathbb{R}/\mathbb{Z})$ norm of a trigonometric polynomial of Hardy-Littlewood,"For $x \in \mathbb{R}$ and $N \geq 1$ an integer, let 
$$S_N(x) = \sum_{n \leq N}n^{\frac{1}{2} + in}e^{2\pi i n x}.
$$ 
I wonder if anyone knows what is the order of 
$$\int_{0}^{1}|S_N(x)|dx.$$ 
This polynomial was considered long ago by Hardy-Littlewood. It appears for instance in the book Trigonometric Series by Zygmund (if you have access to the book, it's in the 3rd edition, Vol 1, Chapter 5, Section 4, page 197). It follows from Theorem 4.7 there that $S_N(x) = O(N)$ uniformly in $x$. But does anyone know if one can do better in $L^1([0,1])$? Thanks!","['analytic-number-theory', 'fourier-series', 'fourier-analysis', 'complex-analysis', 'analysis']"
2603123,"Bound for largest eigenvalue of symmetric matrices of uniform random variables over $[0,1]$ and fixed $1$s along diagonal and scattered $1$s","Given a $n\times n$ symmetric random matrix whose diagonals are all fixed as $1$. In addition, there are $k$ $1$s will be randomly scattered in upper triangular (of course, the corresponding places in the lower-triangle will be filled with $1$, and $2k < n^2-n$). All other elements are independent uniform random variables over $[0,1]$. Help with the bound (lower and upper) for the largest eigenvalue of such random matrices. Gershgorin circle could help with the upper bound. For example, if we assume all those  $1$s are in the same row, then we should be able to find the probabilistic bound for this case with Irwin–Hall distribution; but I currently have trouble dealing with the ""randomly scattered"" $1$s. I am not familiar with the random matrix theory. I am not sure if there is anything from it can help this.","['random-matrices', 'matrices', 'probability-theory', 'probability', 'combinatorics']"
2603128,Convergent arithmetic mean implies convergent sub-sequence?,"We have a non-negative sequence $\{a_n\},~n\in\mathbb{N}$ and its arithmetic mean $\frac{\sum_{i=0}^{N-1}a_i}{N}$ goes to 0 as $N\to\infty$. Can we claim that in this case, there exists a subsequence $\{a_{k_n}\}$ such that $\lim_{n\to\infty}a_{k_n}=0$? I have a proof of yes as follows: We prove it by contradiction. Suppose that such a subsequence does not exist. Then there exists an integer $N$ and positive constant $\epsilon$ such that $\forall n\ge N$, $a_n\ge\epsilon$. However, in this case, $\lim_{N\to\infty}\frac{\sum_{i=0}^{N-1}a_i}{N}=\epsilon>0$, which contradicts the fact that $\lim_{N\to\infty}\frac{\sum_{i=0}^{N-1}a_i}{N}=0$. My question is: “Suppose that such a subsequence does not exist. Then there exists an integer $N$ and positive constant $\epsilon$ such that $\forall n\ge N$, $a_n\ge\epsilon$.” Is this statement correct?","['real-analysis', 'sequences-and-series', 'limits']"
2603169,Evaluate $\int_0^1 \frac {1- \tan(x)}{\sqrt{x}+\tan(x)}dx$,"I am having an extremely hard time finding the anti-derivative of this function: $$f(x)=\dfrac{1-\tan(x)}{\sqrt{x}+\tan(x)}$$ while the lower bound is 0 and the upper bound is 1. I tried to expand by $\cos(x)$ so it would look like $$f(x)=\dfrac{\cos(x)-\sin(x)}{\sqrt{x}\cos(x)+\sin(x)}$$ and then linearly separate it into 2 integrals but it didn't help much at all. Any help would be appreciated, thank you very much.","['improper-integrals', 'integration', 'definite-integrals', 'trigonometric-integrals']"
2603202,does my proof on inverse functions make sense?,"I am proving this question: let $g : N \rightarrow M$ and $ A \subseteq M$. Prove that if $f$ is surjective then $g(g^{-1}(A)) = A$ For the proof this is what I have said: (forwards)Let $y \in g(g^{-1}(A)) $, also lets say that $\exists x \in g^{-1}(A)$. Then by subjectivity $g(x) = y$. Then $g(x)=y \in A$. Wich leads to the conclution that $ g(g^{-1}(A)) \subseteq A$ (reverse) let  $y \in A$, also lets say that $\exists x \in N$, such that $g(x)=y\in A$.  This leads to $x \in g^{-1}(A)$ which is equivelent to $y = g(x) \in g(g^{-1}(A))$. This leads to the conclution that $A \subseteq g(g^{-1}(A))$ Hence   $g(g^{-1}(A)) = A$. Thanks for the help, any improvements welcome","['real-analysis', 'proof-writing', 'functions', 'proof-verification']"
2603233,how is my proof on composition functions,"I am proving this, all improvements are good. If I am completely wrong tell me. Let $h:M\rightarrow N$ and $i:N\rightarrow O$. Prove that if $i \circ h$ is injective, then $h$ is injective. This is my proof for this. 
Lets assume that $i \circ h$ is one-to-one. This means that if $i(h(a_1))=i(h(a_2)) $ then $a_1=a_2$. assume contradiction. $h$ is not injective, therefore if $h(b_1) = h(b_2)$
then $b_1 \neq b_2$. Then if you look at  $i(h(a_1))=i(h(a_2)) $ now you will see that now $a_1 \neq a_2$. This contradicts $i \circ h$ being injective. Therefor $h$ must be injective","['proof-writing', 'functions', 'proof-verification']"
2603244,Limit of definite integral with parameter,"How would I go around a problem like below: $$\lim_{\alpha\to +\infty}\alpha \int_{-\alpha}^{1+\alpha}\frac{dx}{1+x^2+\alpha}$$ I believe I should deal with the denominator somehow and achieve $\arctan$ as a  primitive out of all of this. I cannot think of a solution, though. Thank you in advance.","['integration', 'definite-integrals', 'limits']"
2603275,how is my proof on equinumerous sets,"Hello i am hoping that you can tell me how my proof is and the improvements i can do. If i am complexity wrong please do tell. Prove that if $W$ is denumerable, then $W$ is equinumerous with a proper subset of itself. $S$ is denumerable. therefore the is a bijection between S and the natural number: $f : S \rightarrow \mathbb{N}$. Say that we have a infinite Set $ A \subset \mathbb{N}$. $A$  would also be denumerable as a infinite subset of a denumerable set is also denumerable. Then we do $f^{-1}(A) = B$. Then $ T \subset S $ is a proper subset.  Lets take another function $g: \mathbb{N} \rightarrow A$. g wouuld also be a bijection. Now let $h = g\circ f, h$ would be a function that went $S \rightarrow B$ and would also be a bijection . Therefor you can say that S and T are equinumermous. Hence $S$ is equinumermous with a proper subset of itself. How did i do?","['real-analysis', 'proof-writing', 'elementary-set-theory', 'proof-verification']"
2603295,The Runge-Kutta method for a system of equations,"In many textbooks, the Runge-Kutta method is introduced for a single 1st order equation $$ \frac{d y }{dt } = f(t, y ) . $$ It is stated that the method generalizes directly to the multi-component case too, i.e., $$ \frac{d \vec{y }}{dt} =\vec{ f} (t, \vec{y }) .  $$ Can anyone explain that why this is obvious ?","['numerical-methods', 'ordinary-differential-equations']"
2603330,What is (mathematically) minimal computer architecture to run any software [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. This question is not about mathematics, within the scope defined in the help center . Closed 6 years ago . Improve this question Computer consist of hardware, literally hard wiring machine language instructions (operations) and operands to predetermined results. At the lowest level of interest, this hard wiring is physical implementation of boolean algebra or some other discrete mathematical system, implemented with electronic logic gates. At the most basic level, a logic gate takes one or two binary inputs (signals), and return a binary output in respect to one of these logical operations: AND, OR and NOT. Any truth table of arbitrary binary inputs and outputs can be reduced to canonical boolean expression composed of these logical operators. I.e any chip performing some logical operation on arbitrary binary data can be composed of these logic gates. Machine instruction are binary control bits to parameterize the chip behavior. From these elementary logic gates, higher level of abstraction is derived: Adders, ALU, multiplexers, registers, CPU, memory etc. At this level, computer performs arithmetic operations on hardware sequentially, moving the binary results back and forth between ALU, registers and memory, synchronized by the clock signal. These physical transformations (arithmetics) of binary data logically obeys the mathematical axioms on which the logic gates are based. Software is the ultimate abstraction over hardware. With it, computer can be instructed to perform anything. Software can be defined using any syntax, and it is completely decoupled from the hardware, until compiled. Even then, it only depends on the binary code, which is abstract interface to hardware. It never meets the hardware. Neither knows the existence of the other. But ultimately, any software is reduced (and limited) to the sequence of arithmetic or logical operations performed by the hardware. My question is, what is the minimal design of computer circuitry and architecture (bit width, ALU operations, registers, instruction set etc.) to be able to theoretically compile and run any software which modern PC can run (OS and applications) on the computer, when performance isn't considered? And how do you mathematically proof this? Here is the question in the nutshell: What is the minimal required instruction set for hardware to be able to emulate a modern PC?","['computer-science', 'boolean-algebra', 'binary', 'logic', 'discrete-mathematics']"
2603385,Is this a commonly known paradox?,"I would like to know if the paradox below is commonly known and has a name. Graham Priest , in his book Logic: A Very Short Introduction , at the end of chapter 12 “Inverse Probability“, asks the reader to consider the following. “Suppose a car leaves Brisbane at noon, travelling to a town 300km away. The car averages a constant velocity somewhere between 50km/h and 100km/h. What can we say about the probability of the time of its arrival? Well, if it is going at 100km/h it will arrive at 3 p.m.; and if it is going at 50km/h, it will arrive at 6 p.m. Hence, it will arrive between these two times. The mid-point between these times is 4.30 p.m. So by the Principle of Indifference, the car is as likely to arrive before 4.30 p.m. as after it. But now, half way between 50km/h and 100km/h is 75km/h. So again by the Principle of Indifference, the car is as likely to be travelling over 75km/h as under 75 km/h. If it is travelling at 75 km/h, it will arrive at 4 p.m. So it is as likely to arrive before 4 p.m. as after it. In particular, then, it is more likely to arrive before 4.30 p.m. than after it. (That gives it an extra half an hour.)” Priest merely mentions that this is somehow related to the Principle of indifference and that Thomas Bayes ( Inverse probability ) as well as Colin Howson and Peter Urbach ( Probability theory ) have done some work in that general area. However, I have been unable to find any concrete information about this problem itself.","['probability-theory', 'probability', 'logic', 'paradoxes']"
2603437,"Using only the digits 2,3,9, how many six-digit numbers can be formed which are divisible by 6? [closed]","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Using only the digits $2,3,9$ , how many six-digit numbers can be formed which are divisible by $6$ ? The options are: (A) $41$ (B) $80$ (C) $81$ (D) $161$ The last digit must be $2$ . But I faced problem when calculating the number of number which are divisible by $3$ . Somebody please help me.","['combinatorics', 'elementary-number-theory']"
2603460,What are the functions satisfying $f\left(2\sum_{i=0}^{\infty}\frac{a_i}{3^i}\right)=\sum_{i=0}^{\infty}\frac{a_i}{2^i}$,"I am interesting of finding for all monotone functions $f:[0,1]\to [0,1]$ satisfying 
   the relation 
  $$f\left(2\sum_{i=1}^{\infty}\frac{a_i}{3^i}\right)=\sum_{i=1}^{\infty}\frac{a_i}{2^i}$$
  Where $a_i\in \{0,1\}$ I have tired to see whether I could make some conjecture. Namely, such function must satisfy the point-wise relations $$f(0)=0~~~~~a_i=0~~~~\mbox{for $i\ge 1$}~~~and~~~f(1)=1~~~~~a_i=1~~~~\mbox{for $i\ge 1$}$$ $$f(\frac23)=\frac12~~~~~a_1=1,a_i=0~~~~\mbox{for $i\ge 2$}$$ $$f(\frac13)=\frac12~~~~~a_1=0,a_i=1~~~~\mbox{for $i\ge 2$}$$ continuing this way we can find particular values. But I cannot find a conjecture on this Can any one help?","['real-analysis', 'functions', 'calculus', 'algebra-precalculus', 'analysis']"
2603485,Question on minimizing the infimum distance of a point from a non compact set,"Let the non-compact $\;E\subset \mathcal X\;$ where $\;(\mathcal
 X,d_0)\;$ is a metric space and denote the distance of a point $\;x\in
 \mathcal X\;$ from $\;E\;$ as: $\;d(x,E)=\inf_{y\in E\;}\{d_0(x,y)\}\;$. After some research on the Internet I found that in general , if I would like to show that $\;\inf_{y\in E\;}\{d_0(x,y)\}=d_0(x,y^*)\;$ for some $\;y^* \in E\;$, then all I have to do is to take a sequence $\;y_n \in E\;$ such that $\;d(x,y_n) \to \inf_{\;y\in E\;}\{d_0(x,y)\}\;$ and find a subsequence $\;y_{n_k}\;$ of $\;y_n\;$ with $\;y_{n_k} \to y^*\in E\;$. Why is the above sufficient? Is it related to the sequential compactness of $\;E\;$? But if it does, how is it possible since $\;E\;$ is non-compact? Did I miss some steps in the process of minimizing the infimum distance? I would really appreciate if somebody could help me understand how this kind of exercises work and provide me (if possible) some Analysis Theorems that I might miss. Thanks in advance!","['general-topology', 'metric-spaces']"
2603492,The function $f(x)=$ ${b^mx^m}\over(1-bx)^{m+1}$ is a generating function of the sequence $\{a_n\}$. Find the coefficient of $x^n$,"Find the coefficient of $x^{27}$ in the sequence generated by the function $\lambda x\in\mathbb{R}.$ ${b^mx^m}\over(1-bx)^{m+1}$ where $b\in\mathbb{R^{+}}$ and $m\in\mathbb{N}$ are some parameters. Hello all. I need to find the coefficient of $x^{27}$. I know the function $h(x)=(1-bx)^{-m-1}$ generates the sequence $\lambda n\in\mathbb{N}.\binom{m+1+k-1}{m+1-1}$ So overall if $m>27$ then $a_{27}=0$, else, I get $a_n=b^{27}\binom{m+27}{m}$, I know I'm doing something wrong because the correct answer is $b^{27}\binom{27}{m}$. Would love to get your help on that one, thank you!","['generating-functions', 'combinatorics', 'discrete-mathematics']"
2603494,Show that $\omega^2+1$ is a prime number.,"Show that $\omega^2+1$ is a prime number. Is there easy way to show it? I tried, as a warm up, to show that $(\omega+1)\omega\neq \omega^2+1$ and I failed. I am pretty sure I miss something trivial here. $\gamma$ is a prime number iff for any $\alpha,\beta<\gamma$ we have $\alpha\beta\neq\gamma$ $\omega$ is first infinite ordinal number.",['elementary-set-theory']
2603501,A Convention of Set Builder Notation,"Given the set $$ X = \{\{a,b\} : a \in \mathbb N ~\wedge~ b \in\{0,1\} \},$$
would the set $\{1\}$ be contained in $X$? I'm not sure how to interpret what happens when $a$ takes on the value $1$ in the set-builder notation. Do we discard $\{1,1\}$ or place it in the set as $\{1\}$?",['elementary-set-theory']
2603514,Prove or disprove the following inequality,"If $X$ is a non-constant random variable with $X \geq 0$ then
$$[E(X^{\alpha+1})]^{\alpha}-[E(X^{\alpha})]^{\alpha+1} > 0$$ 
for 
$\alpha=1,2,3,4,...$ It is easy for $\alpha=1$, because this is same as variance. But for other values of $\alpha$, I am confused. Also I could not find any counter example for $\alpha=2,3$ etc. But how to prove then if this is correct. Please help me. Thanks.","['inequality', 'expectation', 'integral-inequality', 'functional-analysis', 'probability']"
2603554,Is hedgehog of countable spininess separable space?,"My question is about hedgehog space, which is defined in the same way as it can be found on Wikipedia. It seems obvious that hedgehog of countable spininess is separable, because it is countable sum of an intervals with standard topology. But there is theorem which says that every separable metric space is strongly paracompact (hypocompact). Let us cover hedgehog by a ball of radius $1/3$ centered at the origin and and by balls of radius $3/4$ centered at ends of spines. This cover is an open cover and I can't see how we can find star-finite refiniment (seems like set containing origin should intersect some set from every spine). Where is flaw in my reasoning?","['general-topology', 'metric-spaces', 'separable-spaces']"
2603594,Power series solution of $y''+e^xy' - y=0$,"I'm stuck with finding the recursion relation of this differential equation using the power series method. So I started by setting:
$$y(x)=\sum\limits_{n=0}^{\infty}{a_n x^n} \\
e^x=\sum\limits_{n=0}^{\infty}{\dfrac{x^n}{n!}}$$ But when I implemented this in the equation, I got stuck with finding the recursion relation because of the double summation. This is what I have: $\sum\limits_{n=2}^\infty (n)(n-1) a_n x^{n-2}+\left(\sum\limits_{n=0}^\infty\dfrac{x^n}{n!}\right)\left(\sum\limits_{n=1}^\infty n a_nx^{n-1}\right) - \sum\limits_{n=0}^{\infty}{a_n x^n} =0$ I know there exist the Cauchy product but I don't know how to use it in this case. PS:
I know it is also possible to set $y=\sum\limits_{n=0}^\infty\dfrac{a_nx^n}{n!}$ , but I chose to use $y(x)=\sum\limits_{n=0}^{\infty}{a_n x^n}$.","['ordinary-differential-equations', 'power-series']"
2603599,Hard combinatorial identity: $\sum_{l=0}^p(-1)^l\binom{2l}{l}\binom{k}{p-l}\binom{2k+2l-2p}{k+l-p}^{-1}=4^p\binom{k-1}{p}\binom{2k}{k}^{-1}$,"Does anyone have idea to prove that?
$$\sum_{\ell=0}^{p}(-1)^{\ell}\dfrac{\binom{2\ell}{\ell}\binom{k}{p-\ell}}{\binom{2k+2\ell-2p}{k+\ell-p}} = \dfrac{4^p\binom{k-1}{p}}{\binom{2k}{k}}$$ is true for all
$k \in \mathbb{N}, p \in \mathbb{N} \cup \{0\}$.
We use the convention that, if $p>k-1$, then $\binom{k-1}{p}=0$",['combinatorics']
2603609,Compute Thom and Euler class,"If $\gamma \colon S^1 \rightarrow SO(2)$, we define $E_{\gamma}= D_{a}^2 \times \mathbb{R}^2 \sqcup D_{b}^2 \times \mathbb{R}^2 / \sim$, where $(x,v) \sim (x,\gamma(x)\cdot v)$ for $(x,v)\in S_{a}^1 \times \mathbb{R}^2$.
  I need to prove that it is an oriented vector bundle, and compute the Euler class in terms of $\gamma$. I have defined $\pi \colon E_{\gamma} \rightarrow S^2$ to be $\pi([(x,v])=[x]$, if we take $S^2= D_{a}^2 \sqcup D_{b}^2 / (S_{a}^1 \sim S_{b}^1)$. I have proved that it is a vector bundle of dimension 2, but I have troubles when computing the Thom class and the Euler class. First of all, I have to define a Riemannian metric on the bundle. I have defined it in this way: $g\colon E_{\gamma} \times_{\pi} E_{\gamma} \rightarrow \mathbb{R}$ such that $g([(x_{1},v_{1})],[(x_{1},v_{2}])=\langle v_{1},v_{2} \rangle$. To compute $D(E_{\gamma})= \{[(x,v)]\in E_{\gamma} \mid g([(x,v)],[(x,v)])\leq 1 \}$, I have thought that it is $\text{int}(D_{a}^2)\times D^2 \cup \text{int}(D_{b}^2)\times D^2  \cup S^1 \times D^2$, but that is not possible since $H^2(D(E_{\gamma}))$ must be $\mathbb{Z}$. Can anyone help me with the Thom and Euler class, please? Thank you.","['algebraic-topology', 'algebraic-k-theory', 'vector-bundles', 'differential-geometry']"
2603624,Powers of a simple matrix and Catalan numbers,"Consider $m \times m$ anti-bidiagonal matrix $M$ defined as: $$M_{ij} = \begin{cases}
      -1, & i+j=m\\
\,\,\  1, & i+j=m+1\\
\,\,\, 0, & \text{otherwise}
\end{cases}$$ Let $S_n$ stand for the sum of all elements of the $n$-th power of the matrix: $$S_n=\sum_{ij}\left(M^n\right)_{ij}.$$ Prove that for integer $0 < k < m$: $S_{2k}=C_{k-1}$, where $C_k$ is $k$-th Catalan number; $S_{2k+1}=0$, and, additionally, $S_{2m+1}=(-1)^{m-1}$. PS: In fact, the first equality holds much longer up to $k=2m$. Besides $S_{4m+2}=C_{2m}-1$.","['matrices', 'catalan-numbers', 'experimental-mathematics', 'algebraic-combinatorics', 'sequences-and-series']"
2603726,Position of point with respect to hyperbola,I know we can find position of point with respect to hyperbola using put the coordinate of point to formula of hyperbola . If the answer is negative then it's outside and if positive then it's inside . Why this is true ?,"['analytic-geometry', 'geometry']"
2603729,Change of variables in a differential equation,"I have the following differential equation:
$$\frac{d^{2}y}{dx^{2}} + \frac{1}{b^{2}} y = -\frac{\pi}{b} J(1, x).$$
$J$ is the Bessel function of first type. In order to be able to solve it exactly (at least according to Mathematica), I need to change the variables so that the two terms on the LHS have equal coefficients. So, I try $x = b t$, and get
$$\frac{1}{b^{2}}\frac{d^{2}y}{dt^{2}} + \frac{1}{b^{2}} y = -\frac{\pi}{b} J(1, bt)$$
Is this correct? Is this how I change variables in a differential equation, especially in the inhomogeneous term? When I find a solution, what variable is it in exactly? How do I recover the solution in terms of $x$? Thanks.",['ordinary-differential-equations']
2603776,How to find the unit tangent vector of a curve in R^3,"I have this curve defined by :
\begin{align}
x & =\int_0^t \frac{(1+\cosh^3 u)\cos u \,du}{\cosh^2 u}; \\[10pt]
y & =\int_0^t \frac{(1+\cosh^3 u)\sin u \,du}{\cosh^2 u}; \\[10pt]
z & =\int_0^t \frac{(1+\cosh^3 u)\sinh u \, du}{cosh^2 u}
\end{align}
I should find $\vec{T}$,the unit tangent vector to(C) in M,so I have to use t as a parameter,is there any easy way to integrate x,y,z?","['multivariable-calculus', 'integration']"
2603799,Prove all tangent plane to the cone $x^2+y^2=z^2$ goes through the origin,"Good morning, i need help with this exercise. Prove all tangent plane to the cone $x^2+y^2=z^2$ goes through the origin My work: Let $f:\mathbb{R}^3\rightarrow\mathbb{R}$ defined by $f(x,y,z)=x^2+y^2-z^2$ Then, $\nabla f(x,y,z)=(2x,2y,-2z)$ Let $(a,b,c)\in\mathbb{R}^3$ then
$\nabla f(a,b,c)=(2a,2b,-2c)$ By definition, the equation of the tangent plane is \begin{eqnarray}
\langle(2a,2b,-2c),(x-a,y-b,z-c)\rangle &=& 2a(x-a)+2b(y-b)+2c(z-c)\\
&=&2ax-2a^2+2by-2b^2+2cz-2c^2 \\
&=&0
\end{eqnarray} In this step i'm stuck, can someone help me?","['multivariable-calculus', 'real-analysis']"
2603809,Jacobson radical = nilradical iff every open set of $\text{Spec}A$ contains a closed point.,"I believe there is a typo in this problem statement. It is exercise 2.3 in Algebraic Geometry I: Schemes, with examples and exercises Ulrich Görtz and Torsten Wedhorn. In what follows below $A$ is a commutative ring with unit. The problem statement reads: Show that the nilradical of $A$ is equal to the Jacobson radical of $A$ if and only if every open subset of $\text{Spec}A$ contains a closed point of $\text{Spec} A.$ My attempted proof of the forward direction: Proof. We first parse the definitions in the problem statement. We recall that the nilradical of $A,$ denoted $\mathfrak N,$ is the ideal containing all nilpotent elements of $A,$ or equivalently the intersection of all the prime ideals of $A. $ The Jacobson radical of $A,$ denoted $\mathfrak R$ is defined as the intersection of all maximal ideals of $A.$ By definition 2.2. in GW, open sets of $\text{Spec} A$ are of the form 
$$\text{Spec} A \smallsetminus V(\mathfrak a) := \left\{\mathfrak p \in \text{Spec} A; \mathfrak a \not\subset \mathfrak p\right\}$$
for some ideal $\mathfrak a$ of $A.$ Finally we recall that by example 2.9 in GW, a closed point $\mathfrak m$ of $\text{Spec} A$ is precisely a maximal ideal of $A.$ With these definitions in mind, we need to show that $\mathfrak N = \mathfrak R$ if and only if for every ideal $\mathfrak a$ of $A$ there is a maximal ideal $\mathfrak m$ of $A$ so that $\mathfrak a \not\subset \mathfrak m.$ For the forward direction, we proceed by contrapositive. Let $\mathfrak a \subset A$ be an ideal and suppose it is contained in every maximal ideal of $A.$ In particular this implies that $\mathfrak a \neq (1).$ But then $V(\mathfrak a) \neq \emptyset$ which in turn implies that $\text{Spec} A \smallsetminus V(\mathfrak a) \neq \text{Spec} A.$ This can only be true if there exists a prime ideal $\mathfrak p$ of $A$ that does not contain $\mathfrak a.$ In other words, $\mathfrak a \subseteq \mathfrak R$ but $\mathfrak a \not\subset \mathfrak N,$ i.e., $\mathfrak R \neq \mathfrak N.\\$ I believe that one of the conditions in the problem statement cannot be met. Indeed, for any commutative ring $A,$ the open set $U:=\text{Spec}A \smallsetminus V(\mathfrak R),$ where $\mathfrak R$ is the Jacobson radical of $A,$ by definition could never include a closed point. Is there a typo in the problem statement? It seems like the problem would make sense if all open sets except $U$ were considered. On the other hand, I am quite new to Algebraic geometry and commutative algebra, so there is a good chance I have bungled the meaning of the problem statement. I would also appreciate any hints for the reverse direction of the proof (please do not write out a whole proof, I am just looking for hints). Thank you!","['ring-theory', 'nilpotence', 'algebraic-geometry', 'commutative-algebra']"
2603931,$e^{\left(\pi^{(e^\pi)}\right)}\;$ or $\;\pi^{\left(e^{(\pi^e)}\right)}$. Which one is greater than the other?,"$\newcommand{\bigxl}[1]{\mathopen{\displaystyle#1}}
\newcommand{\bigxr}[1]{\mathclose{\displaystyle#1}}
$
$$\large e^{\bigxl(\pi^{(e^\pi)}\bigxr)}\quad\text{or}\quad\pi^{\bigxl(e^{(\pi^e)}\bigxr)}$$
Which one is greater? Effort. I know that $$e^\pi\ge \pi^e$$ Then $$\pi^{(e^\pi)}\ge e^{(\pi^e)}$$ But I can't say
$$e^{\bigxl(\pi^{(e^\pi)}\bigxr)}\le \pi^{\bigxl(e^{(\pi^e)}\bigxr)}$$ or $$e^{\bigxl(\pi^{(e^\pi)}\bigxr)}\ge \pi^{\bigxl(e^{(\pi^e)}\bigxr)}$$","['real-analysis', 'inequality', 'exponential-function', 'pi']"
2603943,Finding subspaces with trivial intersection,Let $V$ be a $n$-dimensional real vector space and $P$ and $P'$ be subspaces with dimension $k$. I need to prove that it is possible to find a subspace $Q$ of dimension $n-k$ such that $Q$ has trivial intersection with both $P$ and $P'$. I tried to manipulate the bases but cannot find a way... Could anyone please help me?,"['direct-sum', 'linear-algebra', 'vector-spaces']"
2604024,Fisher information of sufficient statistic,Why is it true that if $X \sim f_{\theta}(x) $ (let's assume for simplicty that theta is one dimensional) is some random variable and $T(X)$ a sufficient statistic then $I_{X}(\theta)$ (Fisher information ) is equal to $I_{T(X)}(\theta) $? It is said that it can be derived from factorization theorem ($f_{\theta}(x) = g_{\theta}(T(x)) \cdot h(x) $) but I can't see how exactly since we don't know the distribution of $T$.,"['fisher-information', 'statistics']"
2604139,Is $ X \to \mathrm{CH}^i (X) $ covariant or contravariant?,Let $ X $ be a scheme of finite type over $k$. Let $ \mathrm{CH}^i(X)$ be the Chow group of $ \ i \ $ cocycles on $X$. Is $ X \to \mathrm{CH}^i (X) $ a covariant or a contravariant functor ? Why ? Let $ \mathrm{CH}_i(X)$ be the Chow group of $ \ i \ $ cycles on $X$. Is $ X \to \mathrm{CH}_i (X) $ a covariant or a contravariant functor ? Why ? Thanks in advance for your help.,"['category-theory', 'schemes', 'sheaf-theory', 'algebraic-geometry']"
2604183,$G$ abelian when $Z(G)$ is a proper subset of $G$?,"These two exercises confuses me: Let $G$ be a group of order $p^3$, where $p$ is a prime Show that if $|Z(G)| = p^2$ then $G/Z(G)$ is cyclic Show that if $G/Z(G)$ is cyclic, then $G$ is abelian What? Isn't a group $G$ abelian iff $|Z(G)| = |G|$, which is obviously not the case when $|Z(G)| = p^2$?","['abelian-groups', 'abstract-algebra', 'group-theory']"
2604190,Find the truth value of... empty set?,"Here is my Discrete Math question Let $P(n)$ denote the statement "" $n$ is prime."" Find the truth value of ""very $n$ in empty set is a prime number"". And explain your answer. I understand every $n$ in empty set is NOT a prime number. And what does it mean by ""Find the truth value of"" in this question? Professor gave me this hint but I don't understand. Hey this is a hint. What is the truth value of NOT every $n$ in empty set is a prime number?","['logic-translation', 'first-order-logic', 'predicate-logic', 'logic', 'discrete-mathematics']"
2604215,Eiegenspectrum on subtracting a diagonal matrix,"Suppose I have a (psd) matrix $A$ and a diagonal matrix $\Sigma$. I want to know how the eigenvectors and eigenvalues of $A-\Sigma$ behave. The elements of $\Sigma$ are very small (compared to the eigenvalues of $A$). I know that when $\Sigma$ is a multiple of identity, it only shifts the spectrum of $A$. Such a thing does not hold for other diagonal matrices. But are their results from matrix perturbation which tells how the eigenvectors change (upper and lower bounds) on such perturbation to the matrix?","['matrices', 'linear-algebra']"
2604222,A group of order 189 is not simple,"I seek to prove that a group G of order 189 is not simple. So, for contradiction, I assume G is simple. $|G|=189=3^3 7$. Now, by the Sylow theorems, $n(7)=1+7k$ divides $3^3=27$. But this is only true when $n(7)=1$, thus G has a normal Sylow 7-subgroup, and so G is not simple. Is this correct? This is the first time I've encountered this situation where $n(p)=1$ because $n(p)$ couldn't be $m$, which is $3^3=27$ in this case. All other examples I've done, it can usually be $m$ and then I prove it isn't by counting the number of elements.","['sylow-theory', 'group-theory', 'simple-groups']"
2604244,derive the expectation of exponential function $e^{-\left\Vert \mathbf{x} - V\mathbf{x}+\mathbf{a}\right\Vert^2}$ or its upper bound,"I need to analyze this expectation with respect to $n$-dimensional random vectors $\mathbf{x} \in \mathbb{C}^n$. $$E=\int_\mathbf{x} e^{-\left\Vert \mathbf{x} - V\mathbf{x}+\mathbf{a}\right\Vert^2} p_\mathbf{X}(\mathbf{x}) d\mathbf{x}$$ where $V$ is a fixed $n\times n$ unitary matrix and $\mathbf{a}$ is a fixed $n\times 1$ vector. Note that $V$ is not positive definite. It is clear that $E$ must be a function of $\mathbf{a}$ and $V$. 1) Is there any non trivial distribution $p_\mathbf{X}(\mathbf{x})$ that can facilite the calculation of $E$ ? I have tried the uniform distribution over a fixed $(n-1)$-sphere without success. 2) Is there any inequality that can yield upper bound $E$ by a function of $\mathbf{a}$ and $V$. I mean something like  $E = f(\mathbf{a},V) < g(\mathbf{a},V)$ without having $f(\mathbf{a},V)$ ? I appreciate any hints and references.","['normed-spaces', 'exponential-function', 'statistics', 'integration', 'probability']"
2604247,Prove that $R[x]$ is an integral domain if and only if $R$ is an integral domain. [duplicate],"This question already has answers here : Prove that $R$ is an integral domain $\Leftrightarrow$ $R[x]$ is an integral domain (4 answers) Closed 5 years ago . I want to prove that for a ring $R$ , $R[x]$ is an integral domain if and only if $R$ is an integral domain. I have one direction of the proof ( $R$ an integral domain implies $R[x]$ ) an integral domain, but I am having trouble proving the other direction. Here is the first direction: Let $R$ be an integral domain, and suppose on the contrary that $R[x]$ is not an integral domain, meaning that it has zero divisors. In particular, let $f(x),g(x)\neq 0\in R[x]$ such that $\deg(f)=n$ and $\deg(g)=m$ and $f(x)g(x)=0$ . Specifically, $f(x)=a_nx^n+...a_1x+a_0$ and $g(x)=b_mx^m+...b_1x+b_0$ where $a_i,b_j\in R$ for all $0\leq i,j\leq n,m$ . Now, consider the polynomial term $a_nb_mx^{n+m}\in f(x)g(x)$ . Because $f(x)g(x)=0$ , this means that each polynomial coefficient must be zero, so in particular $a_nb_m=0$ . But, $R$ is an integral domain so either $a_n=0$ or $b_m=0$ , contradicting the degree of $f(x)$ or $g(x)$ . Thus, $R[x]$ is an integral domain. And here is what I have so far for the other direction, but I'm not sure if I have the right set-up. Any hints or comments are appreciated. Now, suppose conversely that $R[x]$ is an integral domain, and suppose on the contrary that $R$ is not, meaning that it has zero divisors. In particular, let $ab=0$ where $a\neq 0$ and $b\neq 0$ for $a_0,b_0\in R$ . Furthermore, consider the polynomial product $f(x)g(x)=0$ for $f(x),g(x)\in R[x]$ , where $f(x)$ and $g(x)$ are defined in the same way as above. Because $R[x]$ is an integral domain, either $f(x)=0$ or $g(x)=0$ .","['abstract-algebra', 'ring-theory', 'polynomial-rings', 'integral-domain']"
2604251,Is division inherently the last operation when using fraction notation or is the order of operation always PEMDAS?,"Are parenthesis needed when adding or subtracting in the dividend or divisor when using fraction notation to divide, or is it inherent that division is the last step using this notation? $
X = \frac{n}{a - b + c}
\\or\\
X = \frac{n}{(a - b + c)}
$","['algebra-precalculus', 'notation', 'arithmetic']"
2604252,"The general term of the sequence : 1, 1, -1, -1, 1, 1, ...? [duplicate]","This question already has answers here : Generalizing a sequence. (4 answers) Closed 6 years ago . Suppose we have the following sequence: {$a_n$} such as: $a_0 = 1, \\
a_1 = 1, \\
a_2 = -1, \\
a_3 = -1, \\
a_4 = 1, \\
a_5 = 1, \\
...
$ How can we find the general term of this sequence? I tried using a trigonometric function e.g. $\alpha \sin(x+\phi)$, then we impose some constraints on $\alpha$ and $\phi$ to get that sequence, but I get lost, is there any clever way to find the general term? EDIT: The question is identified as duplicate, but that answer does not solve the question, because I am looking for a solution that does not involve floor function.",['sequences-and-series']
2604299,"Determine the marginal distributions of $(T_1, T_2)$","A random variable $(T_1, T_2)$ is called two-dimensional exponentially
  distributed with parameters $(\lambda_1, \lambda_2, \lambda_3)$, if
  the distribution function has the form $$F(t_1,t_2) = \left\{\begin{matrix}
1-e^{-(\lambda_1+\lambda_3)t_1}-e^{-(\lambda_2+\lambda_3)t_2}+e^{-\lambda_1t_1-\lambda_2t_2-\lambda_3 \text{max}(t_1,t_2)}, \text{ if } t_1>0, t_2>0\\  0
 \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\text{
else } \end{matrix}\right.$$ Determine the marginal distributions of $(T_1,T_2)$ I never do such task that's why I find this example to see how it work correct so I understand better and do it correct in test hopefully. When I understand correct, I need to calculate limit to $\infty$ of the distribution function for $t_1$ and seperately for $t_2$ then I have the marginal distributions of $(T_1,T_2)$. $$\lim_{t_1 \rightarrow \infty}f(t_1,t_2) = 1-e^{-\infty}-e^{-(\lambda_2+\lambda_3)t_2}+e^{-\infty} = 1+0-e^{-(\lambda_2+\lambda_3)t_2}+0=1-e^{-(\lambda_2+\lambda_3)t_2}$$ $$\lim_{t_2 \rightarrow \infty}f(t_1,t_2) = 1-e^{-(\lambda_1+\lambda_3)t_1}-e^{-\infty}+e^{-\infty} = 1-e^{-(\lambda_1+\lambda_3)t_1}-0+0 = 1-e^{-(\lambda_1+\lambda_3)t_1}$$ Is it good like that or is all wrong? :(","['probability-theory', 'probability', 'statistics', 'discrete-mathematics']"
2604318,Decimal expansion of $\frac{1}{p}$: what is its period?,"Consider the decimal expansion of $\frac{1}{p}$ (where $p$ is an odd prime). Let $F(p)$ be the period of this decimal expansion. For instance, $F(3) = 1$ (as $\frac{1}{3} = 0.3333$, which has period $1$) $F(7) = 6$ $F(11) = 2$ $F(13) = 6$ Now consider the function $G(x)$, defined as the number of different primes for which $F(p)=x$. For instance, $G(1) = 1$. (There is only one prime $(3)$ which results in a decimal expansion of period $1$.) $G(2) = 1$. (There also seems to be only one prime $(11)$ which results in an expansion of period $2$.) How does this function behave? Are there any values of $x$ for which $G(x) = 0$. (That is, is there a length of period that never occurs?)","['fractions', 'prime-numbers', 'sequences-and-series']"
2604319,Solution to ODE with Dirac Delta satisfies ODE,"I am working on a problem where I have the following ODE. $$m\dot{v}+bv=\delta_I(t)$$
where
$$\delta_I(t)=\begin{cases}0, & \text{for}&t\ne0\\ I, & \text{for} &t=0\end{cases}.$$
The solution $v(t)$ was derived using Laplace transforms, the ODE in the Laplace domain is (with $0$ initial conditions)$$(ms+b)V(s)=I$$ giving $$v(t)=\frac{I}{m}e^{-bt/m}.$$
How does this solution satisfy the original ODE though? At $t\ne0$ everything is good,$$-\frac{Ib}{m}e^{-bt/m}+\frac{Ib}{m}e^{-bt/m}=0,$$ while at $t=0$, $$-\frac{Ib}{m}+\frac{Ib}{m}=I$$ the result seems to be saying $0=1$ which is obviously false. What am I missing here?","['dirac-delta', 'ordinary-differential-equations', 'laplace-transform']"
2604329,How to find the asymptotic behaviour of $(y'')^2=y'+y$ as $x$ tends to $\infty$?,"Can someone help me find the asymptotic behavior of $(y'')^2=y'+y$ as $x$ tends to $\infty$? I tried neglecting the $y'$ term in the equation and ended up with $y\sim x^4/144$. The derivative of $x^4/144$ is indeed neglegible as $x$ goes to  $\infty$ . But when I tried to find the full asymptotic behaviour and plugeg the series $x^4/144+\sum_{n=-3}^\infty \frac{1}{x^n}$, into the equation and equate powers, I found there is no free parameter at all. I also tried different substitution such as $\log$, but none of them seems to be working out. Substituiting exponential clearly fail because not all the terms are multiplied by the same number of $y$ or its deriatives.","['asymptotics', 'ordinary-differential-equations']"
2604332,Counting argument proof or inductive proof of $F_1 {n \choose1}+...+F_n {n \choose n} = F_{2n}$ where $F_i$ are Fibonacci,"Prove that $$F_1 {n\choose1} +...+F_n {n\choose n} = F_{2n}$$
where $F_i$ is the $i$th Fibonacci number. This question is mentioned on Putnam and Beyond book number 861. I looked at the proof of the question, and the book's proof uses Binet's formula (i.e. the explicit formula for $n$th Fibonacci number), binomial formula, and the fact that $\frac{3+\sqrt{5}}{2} = (\frac{1+\sqrt{5}}{2})^2$. I am wondering whether there is a counting argument for this proof (or an inductive proof), and if so, it would be great if anyone posts them here.","['combinatorics', 'fibonacci-numbers']"
2604337,How many digits are in $125^{100}$?,What I can think of thus far is that $125^{100} = (\frac{1000}{8})^{100} = \frac{1000^{100}}{2^{300}}$ I know that $2^{10} = 1024$ so $\frac{1000^{100}}{1024^{30}}$. That's all I can figure out this far. I was thinking to divide the numerator and denominator of $\frac{1000^{100}}{1024^{30}}$ by $1000^{30}$ and I think that would give me $\frac{1000^{70}}{1.024^{30}}$ but I'm not even sure if this is correct. Can someone please help me solve this? Edit: How can I solve this without the use of logarithms?,['number-theory']
2604346,Convergence in distribution of a discretized random variable and generated sigma-algebras,"Let $X:\Omega\to \mathbf{X}$ be an integrable random variable where $\mathbf{X}\subset \mathbb{R}^d$ is a compact set. Denote $\mathbf{X}_n = \{\mathbf{X}_n(1),\dots, \mathbf{X}_n(n)\}$ to be a n-component partition of $\mathbf{X}$ such that partition $\mathbf{X}_{n+1}$ is a refinement of $\mathbf{X}_n$. Set $x_{n,i} \in \mathbf{X}_n(i)$ for all $n\in\mathbb{N}$ and $i\leq n$. Define random variable $X_n = \sum_{i=1}^n x_{n,i} \mathbb{1}(X \in \mathbf{X}_n(i))$ where $\mathbb{1}(\mathbf{B})$ is the indicator function for the set $\mathbf{B}$. What conditions do I need for $X_n$ to converge in distribution to $X$??? How about if the sigma-alegbras generated by $ \sigma^{(n)} =\sigma(\{X \in \mathbf{X}_{(n)}(i)\},
\enspace i=1, \dots, n)$ satisfies
$\sigma(X) = \sigma(\cup_{n\in\mathbb{N}}\sigma^{(n)})$? Is this sufficient?","['weak-convergence', 'probability-theory', 'measure-theory', 'random-variables']"
2604347,Preferred method for solving a certain non-homogeneous linear ODE,"The problem is as follows. Consider the following linear second-order non-homogeneous ordinary differential equation with constant coefficients
  $$\frac{d^2y}{dx^2}+y=x\cdot\sin(x),\quad x>0.$$
  Find the homogeneous solution and the general solution by any method. The method of unknown coefficients fails me (assuming $y_p(x)=(Ax+B)\sin(x)+(Cx+D)\cos(x)$). I can solve it using LaPlace transforms, but it's very time consuming. The homogeneous solution is easy ($y_h(x)=c_1\sin(x)+c_2\cos(x)$), but the particular solution is trickier. What is the preferred method to solve this?",['ordinary-differential-equations']
2604364,Compute $\lim_{x\to 1^+} \lim_{n\to\infty}\frac{\ln(n!)}{n^x} $,May someone give me a hand on this double limit? Does the order of limits impact the result? $$\lim_{x\to 1^+} \lim_{n\to\infty}\frac{\ln(n!)}{n^x} $$ I showed that the interior of the limits is inferior to the following expression: $$  \frac{\ln(n)}{{n^{x-1}}} $$ Thanks in advance :),['limits']
2604377,Getting a self-homeomorphism of the cylinder from a self-homeomorphism of the circle,"Given an orientation-preserving homeomorphism of the circle $$f : S^1 \rightarrow S^1,$$ I want to define a homeomorphism of the cylinder $$F : S^1 \times \mathbb{I} \rightarrow S^1 \times \mathbb{I}$$ such that for all $x \in S^1$, we have: $F(x,0) = (f(x),0)$ and $F(x,1) = (x,1)$. The same goes with 'homeomorphism' replaced by 'diffeomorphism.' It seems likely that this is possible, but I haven't been able to find an explicit definition. Ideas, anyone?","['low-dimensional-topology', 'general-topology', 'differential-topology']"
2604383,Which type of Riemann Sum is the most accurate?,"When estimating the area under a curve, is it more accurate to average out the values of the upper and lower sums or use the midpoints of the intervals instead? I would assume it varies but I just wanted to know what the best way is to estimate the area under a curve when no instructions on how to do so have been given.","['integration', 'calculus', 'riemann-sum']"
2604404,A sequence of absolutely continuous functions whose derivatives converge to $0$ a.e,"Here's the problem I'm having trouble with: Let $f_{n}:[a,b] \to \mathbb{R}$ be a sequence of absolutely continuous functions such that $f_{n}(a)=0$ , $||f_{n}'||_{2} \leq 1$ and $f_{n}' \to 0$ a.e. (in the standard Lebesgue measure): Prove that $||f_{n}'||_{1} \to 0$ . Prove that $f_{n} \to 0$ a.e. I know how to do part 2. once I do part 1: $|f_{n}(x)| = | \int_{a}^{x} f_{n}'(t)|dt \leq \int_{a}^{x}|f_{n}'(t)|dt \leq \int_{a}^{b} |f_{n}'(t)|dt \xrightarrow{n \to +\infty}0.$ I've tried solving one using the Holder inequality, as well as through the dominated convergence theorem (my attempt was, if $f_{n}$ is abs. cont, and $f'_{n}$ is bounded a.e, then $f_{n}$ is Lipschitz and has bounded derivative, so I can use the dominated convergence theorem. Unfortunately, $f_{n}'$ needn't be bounded, even if it tends to zero at a.e. point. How can I prove 1?","['lebesgue-measure', 'lebesgue-integral', 'measure-theory']"
2604472,What kind of curvature does a cylinder have? [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 6 years ago . Improve this question Gaussian curvature is a kind of like an infinitesimal curvature. For example, all points of a sphere has positive Gaussian curvature. Angular defect is kind of like a ""point charge"" of curvature. For example a cube has positive angular defect at its 6 corners. So what do you call the curvature on the two circles of a cylinder? It has positive infinite gaussian curvature, but 0 angular defect. Note: The cylinder is closed, i.e. has a top and bottom (like this ).","['differential-geometry', 'curvature']"
2604503,A new type of curvature multivector for surfaces?,"A surface, parametrized by $U$ and $V$, has a tangent bivector given by the wedge product $$I = \vec x_U\wedge\vec x_V$$ where subscripts represent partial derivatives. The First Fundamental Form coefficients are defined as usual, but the Second Fundamental Form coefficients are vectors perpendicular to the surface: $$E=\vec x_U\cdot\vec x_U=\lVert\vec x_U\rVert^2$$
$$F=\vec x_U\cdot\vec x_V$$
$$G=\vec x_V\cdot\vec x_V$$
$$\lVert I\rVert^2 = -I^2 = EG-F^2$$ $$\vec L=\vec x_{UU}^\perp=(\vec x_{UU}\wedge I)I^{-1}$$
$$\vec M=\vec x_{UV}^\perp$$
$$\vec N=\vec x_{VV}^\perp$$ If the surface is embedded in 3D space, then $\vec L$, $\vec M$, and $\vec N$ are all parallel to each other. In higher dimensions, they can be independent. In terms of the above quantities, the Gaussian and mean curvatures are $$K=\frac{\vec L\cdot\vec N-\vec M\cdot\vec M}{\lVert I\rVert^2}$$
$$\vec H=\frac{G\vec L-2F\vec M+E\vec N}{2\lVert I\rVert^2}$$ which are both invariant with respect to re-parametrization. But I found another such invariant while deriving a different formula* for $K$, and it has the same units as $K$ (inverse area). This quadvector quantity $Q$ seems to be another type of curvature, which is always $0$ in 3D. $$Q = I\frac{(\vec L\wedge\vec M)G+(\vec M\wedge\vec N)E+(\vec N\wedge\vec L)F}{\lVert I\rVert^4}$$ I also discovered that $Q$ is proportional to the area of an ellipse traced by the normal curvature vector $(\frac{d^2\vec x}{ds^2}=k\hat n=\vec k)$ of geodesics with varying direction passing through the given point on the surface. (This ellipse also has neat relationships with $\vec H$ and $K$ .) Has $Q$ ever been studied before? Is there a general method for finding or classifying such invariants of embedded manifolds? *
$$i = \frac I{\lVert I\rVert}$$
$$i\frac{\langle i_Ui_V\rangle_2}{\lVert\langle\vec x_U\vec x_V\rangle_2\rVert}=I\frac{\langle i_Ui_V\rangle_2}{\lVert I\rVert^2}=K-Q$$ (The angle brackets denote grade-projection in Geometric Algebra, and inside the brackets is a product of partial derivatives.)","['curvature', 'invariance', 'geometric-algebras', 'differential-geometry', 'surfaces']"
