question_id,title,body,tags
3217255,How we can find the sum of all roots for $x^2+\cos x=2019$,"How to find the sum of all real roots such that, $$x^2+\cos x=2019$$ Firstly I think to use numerical methods , but this shortcut is weak and depend on % error .As result, im searching for an algebraic way to solve such exercise. 
As result , is there any hint to crack this task ?","['calculus', 'algebra-precalculus']"
3217363,Number of six digit numbers divisible by $3$ but none of the digits is $3$,"Find number of six digit numbers divisible by $3$ but none of the digits is $3$ My try: Let the six digits are $a,b,c,d,e,f$ such that $$a+b+c+d+e+f=3p$$ where $1 \le p \le 18$ Now since $a \ge 1$ we have by Stars and Bars Technique number of solutions of the above equation as: $$S=\sum_{p=1}^{18}\binom{3p+4}{5}$$ But if we use Exclusion method, its very lengthy. Any hint?","['permutations', 'number-theory', 'binomial-coefficients', 'combinatorics']"
3217375,A $C^1$ function $f:\mathbb{R}^2\to \mathbb{R}$ whose critical value has a non-zero measure.,"Does there exist a function $f:\mathbb{R}^2\xrightarrow{C^1}\mathbb{R}$ such that the critical value has a non-zero measure? It is not satisfying the condition of Sard's theorem , as in the Sard's theorem, we need at least $C^2$ regularity. So I believe such a function exists, but unable to construct.","['differential-topology', 'differential-geometry', 'real-analysis']"
3217409,Is the tensor product (of vector spaces) commutative?,"I've just learned a bit about the tensor product and I couldn't find a real answer to this. I've read something about, that in some cases it could be or not. Let's consider next example: In the vector space $\mathbb{R}^n\otimes_\mathbb{R}\mathbb{R}^n$ with standard basis $\mathbb{B}=(e_1,...,e_n)$ of $\mathbb{R}^n$ , can we say that $e_1\otimes e_2=e_2\otimes e_1$ ? If yes can we say that $\otimes$ is commutative in a vector space $V\otimes V$ generated by the tensor product of a vector space $V$ with itself? If not, when can it be considered?","['tensor-products', 'linear-algebra', 'vector-spaces']"
3217424,Is $\int_X d(F^{*}\omega) = 0$ because of a corollary of Stokes' Theorem?,"My book is From Calculus to Cohomology by Ib Madsen and Jørgen Tornehave. The last part of Proposition 11.11 goes $$\int_X d(F^{*}\omega) = \int_X F^{*}(d\omega) = 0$$ Can we just skip commutativity of pullback and exterior derivative and directly say that $\int_X d(F^{*}\omega) = 0$ by Corollary 10.9 of Stokes' Theorem (Theorem 10.8) ? If we cannot skip, then why is it that $\int_X F^{*}(d\omega) = 0$ ? By the way, if anyone wants to know the definition of domain with smooth boundary: I think this is the book's term for a subset, of a manifold, that is manifold with boundary. See here: Why is there a form with compact support on a connected oriented manifold with integral one but with support contained in a given open proper subset? I realized my error: $X$ is not a manifold (without boundary)! We'll have to use commutativity and the fact that for any smooth $n$ -dimensional manifold (without boundary) $M$ , $\Omega^nM = Z^nM$ , as pointed out below.","['integration', 'multivariable-calculus', 'calculus', 'stokes-theorem', 'differential-geometry']"
3217431,Invertibility of a matrix in portfolio optimization,"Let $A$ be an $n\times n$ symmetric matrix with non-negative entries. Let $\mathbf{1}$ be the column vector of dimension $n$ with all entries being $1$ . 
Consider the $(n+1)\times (n+1)$ matrix $$ B=
\begin{bmatrix} 
A & \mathbf{1} \\
\mathbf{1}^T & 0 
\end{bmatrix}
$$ Question: what is the condition for $A$ so that $B$ is invertible? Remark: This matrix is related to portfolio optimization problems in finance. I note that when $A$ is a constant matrix, the determinant of $B$ is $0$ and thus $B$ is not invertible.","['determinant', 'finance', 'matrices', 'linear-algebra', 'inverse']"
3217434,Spherical Gaussian MLE,"I am having trouble doing a derivation. I want to find the MLE estimate of $\sigma^2$ in a spherical gaussian, i.e when we have set $\Sigma = \sigma^2I$ . I have already seen https://stats.stackexchange.com/questions/238199/mle-of-multivariate-normal-distribution-with-diagonal-covariance-matrix but wanted to do the derivation more thoroughly. Say that we have $m$ points, and that our data consists of $p$ -features. We then know that the log likelihood of a multivariate gaussian is (from https://stats.stackexchange.com/questions/351549/maximum-likelihood-estimators-multivariate-gaussian/351550 ) $$l(\mu, \Sigma )  = - \frac{mp}{2} \log (2 \pi) - \frac{m}{2} \log |\Sigma|  - \frac{1}{2}  \sum_{i=1}^m  \mathbf{(x^{(i)} - \mu)^T \Sigma^{-1} (x^{(i)} - \mu) }  $$ Now, inserting the fact that $\Sigma = \sigma^2I$ we have that $|\Sigma| = |\sigma^2 I| = (\sigma^2)^p$ , we get that $$l(\mu, \sigma^2 )  = - \frac{mp}{2} \log (2 \pi) - \frac{mp}{2} \log \sigma^2  - \frac{1}{2 \sigma^2}  \sum_{i=1}^m  \mathbf{(x^{(i)} - \mu)^T (x^{(i)} - \mu) }$$ Deriving based on $\sigma^2$ we get that $$\frac{\partial l}{\partial \sigma^2} = -\frac{mp}{2}\frac{1}{\sigma^2} + \frac{1}{2(\sigma^2)^2} \sum_{i=1}^m  \mathbf{(x^{(i)} - \mu)^T (x^{(i)} - \mu) }$$ Setting the derivative to 0 we get that $$\hat \sigma^2 = \frac{1}{mp} \sum_{i=1}^m  \mathbf{(x^{(i)} - \mu)^T (x^{(i)} - \mu) } $$ But this is not the same as other results I have found online (for example https://stats.stackexchange.com/questions/238199/mle-of-multivariate-normal-distribution-with-diagonal-covariance-matrix which I think argue that $\hat \sigma^2 = \frac{1}{m} \sum_{i=1}^m  \mathbf{(x^{(i)} - \mu)^T (x^{(i)} - \mu) }$ and http://cs229.stanford.edu/section/gaussians.pdf which states that it should equal the MLE from the univariate case). What is going on here? I have for a while suspected that I calculate the determinant incorrectly, thus giving extra $p$ in the denominator, but I think that is not the case. Can you help me find out what is wrong in my reasoning?","['statistics', 'probability-distributions', 'parameter-estimation', 'normal-distribution', 'maximum-likelihood']"
3217451,How can one define the pullback of a metric $g$ from that of 2-forms?,"Let $M,N$ be smooth manifolds of dimensions $m,n$ respectively and $\phi:M\to N$ be a smooth map. Let $g$ be a metric on $N$ . If $\mathbf{x}$ and $\mathbf{y}$ are local coordinates on $M$ and $N$ respectively then the metric takes the form: $$g=g_{ij}dy^i\otimes dy^j,\;\;\;\;\;i,j=1,\ldots,m.$$ How can one define the pullback of $g$ given that I know the pullback of 2-forms (and $k$ -forms in general)? My Try Lets work with 2 forms for simplicity. The pullback of a two form $\alpha$ is defined to be $$F^*(\alpha)(v_1,v_2)=\alpha(F_*v_1,F_*v_2),$$ where $v_1,v_2$ are tangent vectors at some point $p\in M$ and $F_*$ is the pushforward map. Now suppose that $\alpha=\beta\otimes\gamma$ where $\beta,\gamma$ are 1-forms on $N$ . Using the above I can define the map $$F^*(\beta\otimes\gamma)(v_1,v_2)=(\beta\otimes\gamma)(F_*v_1,F_*v_2)=\beta(F_*v_1)\cdot\gamma(F_*v_2).$$ But $\beta(F_*v_1)=F^*\beta(v_1)$ and $\gamma(F_*v_2)=F^*\gamma(v_2)$ and $$ (F^*\beta\otimes F^*\gamma)(v_1,v_2)=F^*\beta(v_1)\cdot F^*\gamma(v_2)$$ so that I get $$ F^*(\beta\otimes\gamma)=F^*\beta\otimes F^*\gamma.$$ Which looks like a 2-form on $M$ . So given this $$F^*(g)=F^*(g_{ij}dy^i\otimes dy^j)=(g_{ij}\circ F)\;F^*(dy^i)\otimes F^*(dy^j)=(g_{ij}\circ F)\frac{\partial F^i}{\partial x^k}dx^k\otimes\frac{\partial F^j}{\partial x^\ell}dx^\ell,$$ which looks like a metric. Is my reasoning consistent?","['tensors', 'riemannian-geometry', 'differential-geometry']"
3217459,Lie algebra of a closed discrete subgroup is zero.,"If $H$ is a closed subgroup of Lie group $G$ , then show that $\mathfrak{h}=0$ if and only if $H$ is discrete, where $\mathfrak{h}$ is the Lie algebra of $H$ . We know that $\mathfrak{h}=\{X\in \mathfrak{g}: \mathrm{exp}(tX)\in H\  \forall t\in \mathbb{R}\}$ . By closed subgroup theorem, we know that $H$ is an embedded Lie subgroup of $G$ . Now, how to use the fact that $H$ is discrete? Thanks!","['manifolds', 'lie-algebras', 'lie-groups', 'differential-geometry']"
3217483,Solution to equation $-u''=\cos(x)$,"The question tells us to solve $-u''=\cos(x)$ $u(0)=0, u'(0)=1$ I have not solved these types of problems in a long time so my first attempt is that $u''=d^2u/dx^2$ . I'm not sure if that's what's actually implied here but using that I simply integrated twice, $u''=-\cos(x)$ $u'=-\sin(x)+c$ $u=\cos(x)+cx+d$ Using initial conditions: $u(0)=1+d=0, d=-1$ $u'(0)=c=1$ So my answer is, $u(x)=\cos(x)+x-1$ Is this actually the correct way to solve this?","['derivatives', 'ordinary-differential-equations']"
3217489,What is the difference between P(dx) and P(x)dx?,"I often come across the notation $P(dx)$ in statistics papers. Is there a strict mathematical definition for $dx$ inside $P$ ? Examples :
What is the exact meaning of $$\int f(x)P(dx)$$ and $$\int f(x)P(x)dx$$ Also sometimes I saw $\frac{P(dx)}{Q(dx)}$ , is it equivalent to $\frac{P(x)}{Q(x)}dx$ ?","['statistics', 'probability']"
3217498,Making $H^*(\mathbf{P}^\infty)=\lim H^*(\mathbf{P}^n)=k[t]$ precise using stacks,"The stack $B\mathbf{G}_m$ , i.e. morally $\mathbf{P}^\infty$ , has (etale) cohomology $\mathbf{Q}_\ell[t]$ . The scheme $\mathbf{P}^n$ has cohomology $\mathbf{Q}_\ell[t]/t^n$ . In algebraic topology, the first fact follows immediately from (the proof of) the second, using the CW complex decomposition of $\mathbf{CP}^\infty$ . Is there a sense in which $B\mathbf{G}_m=\lim\mathbf{P}^n$ as stacks, and if so does this allow us to compute its cohomology?","['etale-cohomology', 'algebraic-stacks', 'algebraic-geometry', 'homology-cohomology']"
3217528,Question on Wiki proof of Vandermonde matrix,"On this page, https://proofwiki.org/wiki/Vandermonde_Determinant#Proof_3 , I understand how they are creating the $n-1$ degree polynomial $P(x)$ by calculating the determinant based on the final row. I understand how they find the $n-1$ zeroes of $P(x)$ , and I understand how they got that $P(x) = C(x-x_1)\ldots(x-x_{n-1})$ but I don't understand how they found that $C=V_{n-1}$ . It's not very clear on that step. Even if the coefficient of the $x^{n-1}$ is $V_{n-1}$ , how are we sure that the other coefficients will match up ? If the above is true, does that mean that $$\begin{vmatrix} x_1 & x_1^2 & \ldots & x_{1}^{n-1} \\
x_2 & x_2^2 & \ldots & x_{2}^{n-1} \\
\vdots & & \ddots & \vdots \\
x_{n-1} & x_{n-1}^2 & \ddots & x_{n-1}^{n-1} 
\end{vmatrix}$$ is equal to $x_1x_2x_3\ldots x_{n-1}$ because they are both the constant term of $P(x)$ ? If so, are these identities useful in any way? Thanks for any help!",['matrices']
3217554,Prove that the graph of a function is a manifold,"I know how to show this if $X$ and $Y$ are euclidean spaces using IFT but wanted to confirm proofs about the abstract case. Q) a) $X$ , $Y$ are smooth manifolds and $f:X\rightarrow Y$ is smooth. Show that $\operatorname{graph}(f)$ is a manifold. My attempt: Since $X$ and $Y$ are smooth manifolds, $\exists$ atlases $\{U_{\alpha},\phi_{\alpha}\}_{\alpha \in I}$ , $\{V_{\beta},\psi_{\beta}\}_{\beta \in J}$ on $X$ and $Y$ . Let $\stackrel{\sim}\phi$ be a coordinate chart on $\operatorname{graph}(f)$ such that $\stackrel{\sim}\phi(p,f(p)) = \phi(p)$ on $U\times V$ and thus $\operatorname{graph}(f)$ is a manifold? b) Use (a) to show that $F: X\rightarrow \operatorname{graph}(f)$ is a diffeomorphism. My attempt: I mean the coordinate functions of $F$ which are identity and $f$ are smooth, thus $F$ is smooth and $\exists F^{-1}$ which is the projection map which is also smooth. Thus $F$ is a diffeomorphsim but wanted to check how to use (a) to prove the same. I can again define a coordinate chart $\stackrel{\sim}\phi(p,f(p)) = \phi(p)$ on $\operatorname{graph}(f)$ and thus $\phi^{-1}\circ F \circ \stackrel{\sim}\phi(x) = x$ which is smooth and thus $F$ is smooth? Same logic holds for the inverse.","['manifolds', 'smooth-manifolds', 'differential-geometry']"
3217592,Darboux integrability implies Riemann integrability,"I have searched the site for posts regarding Darboux integrability $\implies$ Riemann integrability, but haven't found any that specifically adress this question. My definition of Darboux integrability: Let $f$ be defined and bounded on $[a,b]$ , then $f$ is Darboux integrable if for all $\epsilon >0$ there exists a partition $P$ of $[a,b]$ such that $U(f,P)-L(f,P)<\epsilon$ (where $U$ and $L$ are the upper and lower Riemann sums respectively). My definition of Riemann integrability: Let $f$ be defined and bounded on $[a,b]$ , then $f$ is Riemann integrable if $$\lim_{N\to\infty} \sum\limits_{k=1}^{N} f(c_k)(x_{k}-x_{k-1})$$ has the same limit for all sequences of partitions $P_N$ and all choices of $c_k\in[x_{k-1},x_{k}]$ . If my definitions are correct, it seems that Darboux integrability only requires one partition to fulfil the epsilon-inequality, whereas Riemann integrability requires all sequences of partitions to be fulfilled. How can this lead to an implication nevertheless?","['calculus', 'riemann-integration', 'real-analysis']"
3217687,Continuity of a general implicit function,"I have an implicitly defined function $F(g_1(x_1),\dots,g_n(x_n))=0$ , on the $n$ -th dimension euclidean space, bounded and continuous in its $n$ arguments. $g_i$ are also real-valued bounded and continuous functions. What I'm trying to figure out is if I additionally need any conditions to guarantee that the implicit relation $x_i=f_i(x_1,\dots,x_{i-1},x_{i+1},\dots, x_n)$ is continuous in its arguments. I know that if $F$ was differentiable in all of its arguments, by the Implicit Function Theorem, the cross partial derivatives would exists and thus I would have continuity. However, in my case it is not necessarily true that $F$ is differentiable in its arguments. My intuition says that as everything is compositions of continuous functions, the implicit relation among variables should also be, but I want to make sure. Many thanks!","['continuity', 'analysis', 'real-analysis']"
3217695,Find number of trees with $\deg(v) = 1$ or $\deg(v) = 3$,"Find number of trees where $$ \forall{v}, \text{ }\deg(v) \in \left\{1,3 \right\} $$ Generally I have idea for recurrence there: $$t_n = n\left( t_{n-1} + \sum_{a,b,c \in \mathbb{Z_+} \wedge a+b+c = n-3} t_a t_b t_c \right) $$ but it is too hard to solve so there is probably tricky observation which could help to write a explicit pattern for that.","['graph-theory', 'trees', 'discrete-mathematics']"
3217698,references for algebraic topology of (complex) manifolds,"Now I want to study some fundamental theorems of cohomology of (complex) manifolds.
e.g., Poincare duality, Kunneth formula, weak and hard Lefschetz, cup product, cycle maps, ""de Rham cohomology = singular cohomology = derived sheaf cohomology = Cech cohomology"", and so on.
I'm reading Bott-Tu's ""differential forms in algebraic topology"", so I know some of these for De Rham cohomology.
I want to know these theorems for integers coefficient cohomology. Glancing through the table of contents, it seems that the chapter 0 of Griffiths-Harris contains all of these (for complex manifolds).
But many people say that it is too brief for the people who study it first time.
(I've read Hartshorne, so I think it may be readable for me...) So please suggest to me some references. Thank you very much.","['complex-geometry', 'algebraic-topology', 'differential-geometry']"
3217750,Why can the Kullback-Leibler information function be negative?,"Let $\{ f(x, \theta) ; \theta \in \Theta  \}$ be a set of parametric density functions, $\Theta \subset \mathbb{R}$ . Let $N_\phi$ be a neighborhood of a point $\phi$ in $\Theta$ . The Kullback-Leibler information function for discriminating between $f(X; \theta)$ and $f(X; \phi'), \phi' \in N_{\phi}$ is $$I(\theta, N_{\phi}) = E_{\theta}\left[ \inf_{\phi' \in N_{\phi}} \log \frac{f(X;\theta)}{f(X; \phi')} \right]$$ I am told this quantity can even be negative (unlike the Kullback-Liebler divergence), what would be an example of this fact occuring?","['optimization', 'statistics', 'probability', 'real-analysis']"
3217776,Eigenvalues and eigenfunctions of Airys; DE,"We have following ODE $$ -y'' + xy = - \lambda y $$ $$ x \in (0,1), \; \;\;\; y(0)=y(1)=0$$ I proved using integration by parts that the eigenvalues are positive. Is there a way to compute or plot the eigenvalues in matlab?",['ordinary-differential-equations']
3217835,Linear Independence for Vectors of Cosine Values,"For every integer $n\geq3$ define an $n\times n$ matrix $A$ using the following row vector as the first row: $(c_0,c_1,c_2,\ldots, c_{n-1})$ where $c_k=\cos (2\pi k/n)$ . The subsequent rows are obtained as cyclic shifts. This matrix seems to be always of rank 2. Actually I was working with some families of  representations of dihedral groups, arising in another context and was computing the degrees of the representations. That calculation implied this matrix should be of rank 2.
But definitely there must be some direct way of doing it. Can anyone tell me how to accomplish it?","['matrices', 'trigonometry']"
3217852,Existence of a global involution extension.,"I'm studying the paper ""Local and simultaneous structural stability of certain diffeomorphisms. - Marco Antonio Teixeira"". At the beginning of the paper, the author gives the following definition Besides that, the author states the following lemma: However, he does not demonstrate such a result, just saying ""the lemma is easy to proof"". I know how to find an extension $\omega:\mathbb{R}^2\to \mathbb{R}^2$ of $\left. \varphi \right|_{U}$ such that $\varphi'(0) + \beta$ where $\beta \in \mathcal C_b^0 (\mathbb{R}^2)$ is Lipschitz with bounded constant by $\varepsilon$ . However, I don't have the faintest idea how to guarantee $\omega\circ \omega= \mathrm{Id}.$ Can anyone help me? How do I constructed the function $\omega$ Define $F= \varphi$ and $L = \mathrm{d} \varphi(0)$ . First, choose a $\mathcal{C}^\infty$ real function $\beta:\mathbb{R} \to \mathbb{R}$ satisfying: $\beta(x) = 0 $ , $\forall$ $x$ $\in$ $\mathbb{R} \setminus [-1,1]$ . $\beta(x) =1$ , $\forall$ $x$ $\in$ $ \left[-\frac{1}{2},\frac{1}{2}\right].$ $\beta (x)$ is a increasing function in $\left[-1,-\frac{1}{2}\right]$ and $\beta(x)$ is a decreasing function in $\left[\frac{1}{2}, 1\right]$ . Note that, $\beta$ is compact support $\mathcal{C}^\infty$ , so, there exists $M$ $\in$ $\mathbb{R}$ , such that, $M = \sup \{|D\beta(x)|; \hspace{0.1cm} x \in \mathbb{R}\} $ . We define $\phi: U \rightarrow \mathbb{R}^2$ as $\phi(x) = F(x) - Lx$ . Observe that the function $\phi$ is a $\mathcal{C}^\infty$ , moreover, $D\phi(0) = 0$ . Therefore, for the real number $\widetilde{\varepsilon} = \min\left\{ \frac{\varepsilon}{2M} , \frac{\varepsilon}{2}\right\} > 0$ there exists $1 \geq r >0$ such that $$x \in B_r (0) \subset U \Rightarrow \Vert D\phi(x) \Vert < \widetilde{\varepsilon} $$ implying, by the mean value theoream that, if $x \in B_r (0)  \Rightarrow \Vert \phi(x) \Vert < \widetilde{\varepsilon}\Vert x\Vert $ . Now, consider $\omega: \mathbb{R}^2 \rightarrow \mathbb{R}^2$ defined as: $$\omega(x) = \left\{\begin{array}{l}
	Lx + \beta\left(\frac{\Vert x \Vert}{r}\right)\phi(x), \hspace{0.1cm} \text{if} \hspace{0.1cm} x \in U,\\ 
    Lx, \hspace{0.1cm} \text{if} \hspace{0.1cm} x \in \mathbb{R}^n \setminus U. \end{array} \right. $$ Thus, defining $r_1 = r$ and $r_2 = \frac{r}{2}$ . It is easy to verify that $\omega = F$ in $B_{r_2} (0)$ . $\omega = L$ outside $B_{r_1}(0)$ . The function $\alpha = G - L$ is bounded by $\varepsilon$ . $\alpha$ is $\varepsilon$ -Lipschitz. Once $\omega = L +\alpha$ , we constructed such an extension. However, $\omega \circ \omega $ is not necessarily equal to $\mathrm{Id}$ .","['real-analysis', 'functions', 'germs', 'differential-topology', 'dynamical-systems']"
3217883,Every group the homology of some space?,"Given a specific group, up to isomorphism, is there a way to determine a topological space, up to homeomorphism, with said group as the nth homology? In other words, is there an established algorithm to work backwards from a specific group (like $\mathbb{Z}_2$ ) and end up with some topological space?","['general-topology', 'abstract-algebra', 'category-theory', 'algebraic-topology']"
3217895,Are the connected components of the pullback of a connected covering space all isomorphic?,"Given a connected covering space $Y\rightarrow X$ (both $Y$ and $X$ are connected). Let $T\rightarrow X$ be any map, with $T$ connected. The pullback $Y\times_X T$ may not be connected. At least in the case of $Y/X$ Galois, the pullback will be a disjoint union of isomorphic covers of $T$ (the Galois group acts transitively on the connected components). Without the assumption that $Y/X$ is Galois, is it still true that the connected components of $Y\times_X T$ are isomorphic as covers of $T$ ? I'm happy to assume that $T\rightarrow X$ is a finite covering map, and $Y\rightarrow X$ is also finite, if that helps.","['general-topology', 'algebraic-topology', 'group-theory']"
3217928,Different ways gives different results - solving $\tan 2a = \sqrt 3 $,"Different ways gives different results - solving $\tan 2a = \sqrt 3$ Case 1). $$ \tan 2a = \sqrt 3 =\tan(\frac{\pi}{3}) $$ $$2a = n\pi + \frac{\pi}{3} $$ $$a = \frac{n\pi}{2} + \frac{\pi}{6},\qquad   n \in\mathbb{Z}   $$ Case 2) $$ \dfrac{2\tan a}{1-\tan^2 a}  = \sqrt 3$$ Solving above equation gives $ \tan a =  \dfrac{1}{\sqrt 3}$ or $-\sqrt 3 $ Thus    either $$ a = n\pi+ \frac{\pi}{6},\qquad   n \in \mathbb{Z} $$ Or $$    a = m\pi - \frac{\pi}{3},\qquad     m\in\mathbb{Z} $$ Or either I did something wrong or both these results are same. If it's the later, I tried a lot to convert one form to other by adding the case 2 results. But that doesn't work.","['algebra-precalculus', 'trigonometry']"
3217930,"In a Hilbert space $x_n \overset{\text{w}}{\to} x$ and $y_n \to y$. Prove $\langle x_n, y_n \rangle \to \langle x, y \rangle$","Proposition In a Hilbert space, suppose $x_n \overset{\text{w}}{\longrightarrow} x$ and $y_n \longrightarrow y$ . Then, $\langle x_n, y_n \rangle \longrightarrow \langle x, y \rangle$ . Proof By definition of weak convergence, we have for any bounded linear functional $f \in H'$ \begin{align}
\lim f(x_n) = f(x) &\implies \lim\langle x_n, y\rangle = \langle x, y\rangle \\
&\implies \lim \langle x_n- x, y\rangle = 0
\end{align} From there we observe \begin{align}
0 \leq |\langle x_n, y_n\rangle - \langle x, y\rangle| &= |\langle x_n, y_n\rangle -\langle x, y_n\rangle + \langle x, y_n\rangle - \langle x, y\rangle| \\
&\leq |\langle x_n, y_n\rangle -\langle x, y_n\rangle | + |\langle x, y_n\rangle - \langle x, y\rangle| \\
&= |\langle x_n - x, y_n\rangle| + |\langle x, y_n - y\rangle| \longrightarrow 0 
\end{align} as $n \longrightarrow \infty$ . Seemed a bit too simple so I'm afraid. So I'm requesting to verification or where I went wrong. Thanks.","['proof-verification', 'functional-analysis', 'real-analysis']"
3217954,An ODE involving bump functions,"Consider the following initial value problem $$ 
\begin{cases}
\frac{d}{dt} y(t) = \rho(y(t))\\
y(0) = 0
\end{cases}
$$ where $\rho(x)$ is a bump function supported near $0$ on $\mathbb{R}^1$ .
That is, $\rho(x)$ is a $C^\infty$ function on $\mathbb{R}^1$ such that $\rho(x) \geq 0$ and $\operatorname{supp} (\rho(x) ) \subseteq (a,b)$ for some bounded 
open interval $(a,b)$ containing $0$ . 
By abstract reasoning, a solution $y(t)$ exists
and the image of $y(t)$ is bounded. I'm wondering if it's actually possible to solve $y(t)$ in an exact form. 
Say if $\rho(x)$ is instead an strictly increasing function larger than $0$ . 
Then the naive dividing- $\rho(y(t))$ -on-both-side method can lead to
an somewhat exact expression of $y(t)$ . 
However, as in any first ODE course, 
this is not what we are supposed to do. So I wonder if there is a similar
method to the one for ODE of the form: $$ 
\begin{cases}
\frac{d}{dt} y(t) = f(t)y(t)\\
y(0) = 0
\end{cases}
$$","['analysis', 'ordinary-differential-equations']"
3218008,Evaluating a limit of $xf(2)$ - $2f(x)/x-2$ as $x$ approaches $2$,"I was doing exercises on limit and I came across this question: find the limit of ( $xf(2)$ - $2f(x)$ ) $/$ ( $x-2$ ) as $x$ approaches 2 given $f(2) = 7$ and $f'(2) = 5$ . so I proceeded this way: as $ x $ approaches 2, $ xf(2) $ approaches $ 2f(2) $ . Therefore, $xf(2)$ - $2f(x)$ $/$ $x-2$ $ = $ $ 2f(2)$ - $2f(x)$ $/$ $x-2$ Factoring $2$ and since $ lim f(x) - f(a)/x-a $ is $f'(x)$ The limit evaluates to $2f'(x)$ Is the way I proceeded the right way? How do you do similar questions like: $ lim$ $ nf(x) - f(a)/x-a $ or $ lim f(x) - nf(a)/x-a $","['limits', 'derivatives']"
3218019,Evaluating the limit of a sum using integration,"One of the first results we learn in definite integral is that if $f(x)$ is Riemann integrable in $(0,1)$ then we have $\lim_{n \to \infty}\dfrac{1}{n}\sum_{i=1}^{n}f\Big(\dfrac{i}{n}\Big) = \int_{0}^{1}f(x)dx$ . I was playing around with this to see if this can be generalized and I found the following. We can rewrite the above result as $$
\lim_{n \to \infty}\frac{1}{1+1+\ldots\text{$n$-times}}\sum_{i=1}^{n}1\times f\Big(\frac{1+1+\ldots\text{$i$-times}}{1+1+\ldots\text{$n$-times}}\Big) = \int_{0}^{1}f(x)dx.
$$ The LHS can be written in the general form given below and we ask ourselves for which sequence $a_i$ does the following hold $$
\lim_{n \to \infty}\frac{1}{a_1 + a_2 + \ldots + a_n}\sum_{i=1}^{n}a_i f\Big(\frac{a_1 + a_2 + \ldots + a_i}{a_1 + a_2 + \ldots + a_n}\Big) =\int_{0}^{1}f(x)dx.
$$ Trivially this holds for $a_i = c$ where $c$ is a non-zero constant and the above result is the case when $c=1$ . I also observed that this holds for sequence of natural numbers $a_i = i$ since $$
\lim_{n \to \infty}\frac{2}{n^2+n}\sum_{i=1}^{n}i f\Big(\frac{i^2+i}{n^2+n}\Big) =\int_{0}^{1}f(x)dx.
$$ Experimentally, this also holds for the sequence of prime numbers $a_i = p_n$ and also for the sequence of composite numbers $c_n$ . Question : What are the necessary and sufficient conditions on $a_i$ for the above relation to hold? Related question","['integration', 'summation', 'number-theory', 'real-analysis', 'limits']"
3218036,"What is the solution of the equation $xyp^2 + (3x^2 - 2y^2)p - 6xy=0$, where $p = \frac{dx}{dy}$","What is the solution of the equation $xyp^2 + (3x^2 - 2y^2)p - 6xy=0$ ,  where $p = \frac{dx}{dy}$ I was trying to solve it by dividing the whole equation by $xy$ and then integrate it $\frac{dy}{dx}[\frac{dy}{dx} + (\frac{3x}{y} - \frac{2y}{x})] = 6$ ,  but still this equation is non separable. Please tell me how to solve it.","['nonlinear-system', 'ordinary-differential-equations']"
3218078,Notation for a Cartesian product or tuple except one element?,"If $X$ is a set containing element $0$ , and we want the set that contains all elements in $X$ except $0$ , we can write $X-\{0\}$ . If $X=\prod_{i=1}^n X_i$ and we want to denote the cartesian product $Y$ of all $X_i$ , except some $X_j$ , can we similarly write $Y=X/\{X_j\}$ ? This seems wrong, because $X_j$ is not a subset of $X$ . How do we write this? Similarly, if we have an element $x\in X$ , but we want the element $y\in Y$ that is a tuple equal to $x$ , except with the $j^{th}$ element removed, (i.e. the element in $X_j$ ), then how do we write this? (I think it's called the ""projection"" of $x$ onto $Y$ ? Is that true?)","['elementary-set-theory', 'notation', 'terminology']"
3218099,Geodesics and Distance in Hyperbolic Space,"I am trying to understand geodesics and distance in hyperbolic space $H^n$ (Inverse image of $-1$ under $f(x_0,x_1,\dots,x_n)=-x_0^2+x_1^2+\dots+x_n^2$ and with the inherited metric). More precisely, I want to show that the Riemann distance between two points $p,q$ is $d(p,q)=arccosh(-p\cdot q)$ . Easy I thought, just compute the geodesics between two points $p,q$ and evaluate the length. However, I don't really understand how to do this and would appreciate some help. How do I get a nice analytical formula which I can just ""evaluate"" to obtain the distance? In all the books I've read there are only very geometric arguments which don't really produce formulas and I don't really understand how to go between the two...","['riemannian-geometry', 'differential-geometry']"
3218117,What is the condition that allows a triangle to be enclosed between two homothetic hexagons?,"Let us suppose, as on the figure, that I have a (given) hexagon $DEFIHG$ . Then, I build the hexagon $KLMNOP$ , which is homothetic of ratio $\alpha$ of the first one ( $\alpha$ is a parameter). That is, the outer hexagon is nothing more than the inner ""scaled"" by a factor $\alpha$ . Remark that each hexagon has three couples of sides parallel, as suggested on the figure. It is an assumption. I am looking for a criterion such as $\alpha \geq ...$ for which there exists a triangle enclosed between the two hexagons, as the green triangle. By this, I mean, a triangle whose vertices are inside the outer hexagon and the edges do not cross the inner hexagon. I would also be interested by the conditions leading to two different triangles Thanks","['geometry', 'plane-geometry']"
3218121,Total curvature of a closed polygonal curve,"Let $\tau$ be a closed polygonal curve in the plane, that is, $\tau$ consists of $n$ piecewise-linear segments between the vertices $v_1, \ldots, v_n, v_1$ . The total curvature of $\tau$ is defined as the sum of the angles between the consecutive segments of the curve (i.e. the sum of the external angles). This picture from Wikipedia is self-explanatory: It is known that the total curvature of such a curve is always an integer multiple of $2\pi$ . But how to prove it? I was able to prove the case in which $\tau$ defines something convex, i.e. a polygon. My idea was to triangulate the resulting polygon with vertex set $V$ , edge set $E$ and face set $F$ (where F does not contain the outer face). Using then that in this particular case the total curvature $\kappa$ equals $\kappa = \pi(|V|-|F|)$ and applying Eulers formula gave the result $\kappa = 2\pi$ . What about non-convex ""polygons""? What if the exterior angle actually can become ""negative""? I am stuck and would greatly appreciate some help!
Thanks in advance UPDATE: It would absolutely suffice to just consider the case of non-intersecting segments!","['curvature', 'plane-curves', 'discrete-geometry', 'discrete-mathematics']"
3218129,Different methods give different answers Solve $ \sec x- 1 = (\sqrt 2 - 1) \tan x $,"Solve $ \sec x- 1 = (\sqrt 2 - 1) \tan x $ Case 1) Square both the sides and  using $ \sec ^2 x = 1+ \tan^2 x. $ And solving the quadratic we get answer $\tan x = 1$ or $\tan x = 0$ .
Putting them back in also solves the equation. Thus $x$ is either $n \pi $ or $x =  n \pi + \frac{\pi}{4}, n \in \mathbb{Z}$ Case 2) $ \sec x- 1 = (\sqrt 2 - 1) \tan x $ $ \frac{1-\cos x}{\cos x} = (\sqrt 2 - 1) \frac{\sin x}{\cos x} $ $ 2\sin^2 \frac{x}{2}= (\sqrt 2 - 1) 2 \sin \frac{x}{2} \cos \frac{x}{2}$ Thus solution is $\sin \frac{x}{2} = 0$ and $\tan \frac{x}{2} = \frac{\pi}{8}$ Thus $x$ is either $x = 2n \pi$ Or $x = 2n\pi + \frac{π}{4}$ These two answer are not same.  So something is wrong. Even though below question look similar to this one. None of the concepts in its answers really help this question. Different ways gives different results - solving $\tan 2a = \sqrt 3 $","['algebra-precalculus', 'trigonometry']"
3218177,Induction step of proof of 'Every element of $O(n)$ is product of hyperplane reflections',"The following came up in induction step of proof of 'Every element of $O(n)$ is product of hyperplane reflections'. An element $A$ in $O(n)$ is called hyperplane reflection if $$A=Pdiag(1,\cdots , 1,-1)P^T$$ where $P\in O(n)$ . If $$A'=P \begin{bmatrix}
A_{n-1} & \\
 & \pm 1 
\end{bmatrix} P^T
$$ and $A_{n-1}$ is in $O(n-1)$ then $A'$ is product of hyperplane reflections in $O(n)$ . Why is this true? All we know is $A_{n-1}$ is product of hyperplane reflections. How does this give result?","['matrices', 'orthogonal-matrices', 'linear-algebra', 'group-theory', 'lie-groups']"
3218214,"Find the distribution function of $Z=YX_{1}+(1-Y)X_{2}$ , Y Bernoulli random variable","Let Y a Bernoulli random variable with parameter p and  let $X_{1}$ and $X_{2}$ random variables with distribution functions $F_{1}$ and $F_{2}$ , respectively.
Y, $X_{1}$ , $X_{2}$ are independent. Proof that the distribution function of $Z=YX_{1}+(1-Y)X_{2}$ is $F=pF_{1}+(1-p)F_{2}$ (Don't use characteristic functions, generating moment functions). I really don't have idea. I know how find the distribution function of W =U+V, but I don't have idea about $W=UV$ or similar In the notes of my professor's class he has not explained anything similar","['probability-distributions', 'probability-theory', 'probability']"
3218313,"If $ Av=Bv=\lambda v$, can we conclude that $A=B$?","Let $A $ and $B$ be $2\times2$ matrices with integer entries. Let $v$ be an eigenvector of both $A$ and $B$ with the same eigenvalue $\lambda \neq 0$ . So we have $$
Av=Bv=\lambda v.
$$ My question is the following: Under which restrictions can we conclude that $A=B$ ? Or is it always $A=B$ ? Does it depend on the singularity of $A$ and $B$ ? In other words, if a vector $v$ is an eigenvector with a non-zero eigenvalue for some matrix $A$ , is $A$ unique?","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
3218328,Circles generated by three-fold iterations $f(x)=\frac{1}{1-x}$,"I came across a weird property of the function $f(x)=\dfrac{1}{(1-x)}$ Observe the following: $$f(x) = \frac{1}{(1-x)}, \quad\quad f^2(x) = f(f(x)) = \frac{(x-1)}{x}, \quad\quad
f^3(x) = f(f(f(x))) = x$$ ultimately implying that $f^2(x)=f^{-1}(x)$ . (Mini question: Do you know of any other functions $g(x)$ where $g \circ (g \circ g(x)) = g^3(x)=x$ aside from $f(x)$ and aside from the trivial case where $g(x)=x$ ? I was pretty shocked when I noticed this pattern with $f(x)$ .) Anyway, notice for every $x$ , there is a set of triplets generated by repeatedly applying the function $f(x)$ . Specifically $\langle x\rangle =\{x,f(x),f^{-1}(x)\}=\{x,\frac{1}{(1-x)},\frac{(x-1)}{x}\}$ For an illustrative example let $x=2$ , so then $\langle 2\rangle=\{2, -1, \frac{1}{2}\}$ . See now that this can be thought of as 3 points on the graph of the function $f(x)$ , where Point $A$ : $x \mapsto f(x)$ Point $B$ : $f(x) \mapsto f^{2}(x)=f^{-1}(x)$ Point $C$ : $f^{-1}(x) \mapsto x$ Explicitly, still using $x=2$ as the example: Point $A$ : $(x, f(x)) = (2,-1)$ Point $B$ : $(f(x), f^{-1}(x)) = (-1,\frac{1}{2})$ Point $C$ : $(f^{-1}(x),x) = (\frac{1}{2},2)$ OK so now my question! Since 3 points uniquely define a circle, I'd like to know if we can derive a closed-form function $r(x)$ that calculates the radius of circle $R$ , where circle $R$ is the circle uniquely defined by the 3 points $A$ , $B$ and $C$ generated by $\langle x\rangle$ . Continuing the example where $x=2$ , circle $R$ has center at Point $R=(\frac{3}{4},\frac{1}{4})$ (i.e. the circumcenter of points $A$ , $B$ and $C$ ). The radius of circle $R$ is then simply: $$|\overline{AR}|=\sqrt{{\left(2-\frac{3}{4}\right)}^2+{\left(-1-\frac{1}{4}\right)}^2}= \frac{5\sqrt{2}}{4}.$$ So evaluating $r(x)$ at $x=2$ gives us $r(2)=\dfrac{5\sqrt{2}}{4}\approx1.76777$ . Another cool example to consider is $x=\phi$ , where $\phi=\dfrac{1+\sqrt{5}}{2}\approx1.61803$ (the Golden Ratio). Some cool characteristics that make $\phi$ unique among all numbers are: $$\phi-1=\frac{1}{\phi}\quad\text{and}\quad \phi+1=\phi^2$$ You can calculate this on your own, but applying $f(x)$ on $x=\phi$ repeatedly results in $\langle\phi\rangle=\{\phi,-\phi,\frac{1}{\phi^{2}}\}$ . With the help of Wolfram Alpha, I was able to calculate $r(\phi)\approx1.93649$ (Circumcenter: https://tinyurl.com/y59trfn5 | Radius: https://tinyurl.com/y6jxs9sn ) Calculating the circumcenter seems to be the biggest issue, but maybe there's a cleaner way with the help of linear algebra? I was reading that there's a way to calculate the formula of a circle using matrices and determinants, but that seemed too complex for this. Maybe circles and triangles aren't the way to approach this at all -- I'd be happy to take suggestions and hear your thoughts! Just some last conceptual thoughts... 1) $r(x)$ should always be positive (i.e. there is no $x$ where $r(x)$ is $0$ or negative), and therefore somewhere hit some positive minimum value for $r(x)$ (assuming/implying that $r(x)$ is smooth and differentiable on the interval $x \in (-\infty,1)\cup(1,+\infty)$ ). 2) $\lim\limits_{x \to 1^-}r(x)=+\infty$ and $\lim\limits_{x \to 1^+}r(x)=+\infty$ 3) $\lim\limits_{x \to -\infty}r(x)=+\infty$ and $\lim\limits_{x \to +\infty}r(x)=+\infty$ 4) $r(x)$ is NOT symmetric around $x=1$ . Just as a quick check, $r(3)\approx2.12459$ and $r(-1)\approx1.76777$ 5) $r(x)$ is actual VERY NOISY as a function since for any 1 value of $r(x)$ , there are at least 3 unique variables that result in that value (i.e. all $x \in \langle x\rangle$ )(e.g. $r(2)=r(-1)=r(\frac{1}{2})\approx1.76777$ ) That last point makes me feel there's no true closed-form function for $r(x)$ . Regardless, I'd be really curious to find out what the minimum radius is... (placing \$1 on $r(x)$ for $x \in \langle\frac{\pi^2}{4}\rangle$ !)","['circles', 'geometry', 'linear-algebra', 'numerical-methods', 'algebra-precalculus']"
3218351,B-Splines and sum of uniform variables,"Exercise 5.2 in Elements of Statistical Learning Goal is to show that an order $M$ B-Spline basis function is the density function of a convolution of $M$ uniform random variables. Although I feel the idea, I am looking for an elegant exhaustive solution. Below my attempts. We denote $B_{i,m}(x)$ the $i$ -th B-Spline basis function of order $m$ for the knot-sequence $\tau$ , $m<M$ , defined as $$B_{i,1}(x)=\begin{cases}
               1 & \text{if } \: \: \: \tau_{i} \leq x \leq \tau_{i+1}\\
               0 & \text{otherwise}
            \end{cases}$$ for $i=1,\dotsc ,K+2M-1$ $$B_{i,m}(x)= \frac{x-\tau_{i}}{\tau_{i+m-1}-\tau_{i}} B_{i,m-1}(x) + \frac{\tau_{i+m}-x}{\tau_{i+m}-\tau_{i+1}} B_{i+1,m-1}(x)$$ for $i-1, \dotsc ,K+2M-m$ . The distribution for the sum of $M$ uniform RVs is the convolution of density functions. I read the exercise as ""show that the $i$ -th B-Spline of order $M$ is the density function for the sum of $M$ uniform RVs"". Using the characteristic function, we can write $$P_{ X_1 + \dotsb +X_M }(u)=\mathcal{F}^{-1}\!\!\left[ \left( \frac{i(1-e^{it}) }t \right)^M \right]\!\!(u)$$ ( Elaboration on how to obtain this last result is welcome. ) After calculation, $$P_{ X_1 + \dotsb +X_M }(u) = \frac{1}{2(n-1)!}\sum^{M}_{k=0}(-1)^{k}\binom{n}{k}(u-k)^{M-1} \mathrm{sgn}(u-k)  \tag{*}$$ I propose to proceed by induction. $$M=2: \qquad P_{X_{1}+X_{2}}(x)=\begin{cases}
               x & \text{if } \: \: \: 0 \leq x \leq 1\\
x-2 & \text{if } \: \: \: 1 \leq x \leq 2\\
               0 & \text{otherwise}
            \end{cases}$$ while $$B_{i,2}(x)=\begin{cases}
               \dfrac{x-\tau_{i}}{\tau_{i+1}-\tau_{i}} & \text{if } \: \: \: \tau_{i} \leq x \leq \tau_{i+1}\\\\
\dfrac{\tau_{i+2}-x}{\tau_{i+2}-\tau_{i+1}} & \text{if } \: \: \: \tau_{i+1} \leq x \leq \tau_{i+2}\\\\
               0 & \text{otherwise}
            \end{cases}$$ Which is the same as $P_{X_{1}+X_{2}}$ up to a change in variable, approximately $X = \dfrac{x-\tau_{i}}{\tau_{i+1}-\tau_{i}}$ . More rigorous elaboration on this change in variable or an argument to show the equivalence or both expressions is welcome. Induction. We assume property true at order $M$ . $$B_{i,M+1}(x)= \frac{x-\tau_{i}}{\tau_{i+M}-\tau_{i}} B_{i,M}(x) + \frac{\tau_{i+M+1}-x}{\tau_{i+M+1}-\tau_{i+1}} B_{i+1,M}(x)$$ How to properly show that $B_{i,M+1}(x)$ expresses as $(*)$ where $M+1$ uniform RVs are added ? One idea would be to express by induction the density of the sum of $M+1$ RVs in $(*)$ as a function of the density of the sum of $M$ RVs. How to write it properly ?","['statistical-inference', 'statistics', 'probability-distributions', 'probability']"
3218380,If $G$ be a group of order $8$ and $x$ be a element of order $4$. Prove that $x^2 \in Z(G)$,"Now as $x$ is of order $4$ and it generates $H=\langle x\rangle$ a cyclic subgroup of $G$ of order $4$ , hence index of $H$ in $G$ is $2$ . 
So $H=\langle x\rangle$ is normal in $G$ . Now $gH=Hg $ , for all $g$ in $G$ . => $g(x^2)=(x^2)g\:$ (since $x^2$ is an element of $H$ ), for all $g$ in $G$ . Hence $x^2 \in Z(G)$ . This was my approach. (I think that I am wrong in here, $g(x^2)=(x^2)g$ )
Please, correct me wherever necessary.
And if you could provide a better solution, please do.","['group-theory', 'normal-subgroups']"
3218384,Getting everyone to meet everyone else,"There are 25 students in a class who sit in five rows of five. Each week they sit in a different order. After a number of weeks every student has sat next to every other student, next meaning side by side, one behind the other, or sitting diagonally together. What is the fewest number of weeks in which this can happen? The case for 16 students sitting in four rows of four has been dealt with at: https://puzzling.stackexchange.com/questions/83720/my-sixteen-friendly-students?noredirect=1#comment243063_83720","['graph-theory', 'combinatorial-designs', 'combinatorics', 'discrete-mathematics']"
3218423,prove a series weakly converges to its mean value,"Let $u ∈ L^∞ (\Bbb R)$ such that $u(x + 1) = u(x)$ almost for every $x ∈ \Bbb R$ . Let $ū = \int_0^1 u(y) dy$ and let's define $(u_n )_{n∈\Bbb N}$ by $u_n (x) = u(nx)$ $\forall n ∈ \Bbb N$ and for almost every $x ∈ \Bbb R$ . Prove that $u_n \rightharpoonup ū$ weakly in $L^p (0, 1)$ . My attempt: Let $f\in L^{p'}$ where $1/p+1/p'=1$ : $|\langle f,u_n\rangle  - \langle rf,ū\rangle | \le ||u_n-ū||_{L^P(0,1)}||f||_{L^{p'}(0,1)}$ by Holder. Now I should prove this quantity vanishes to 0 when $n\to \infty$ but I have no clue on how to do it? I am not sure what limitations do we have due to the ""almost everywhere"" 1-periodicity of $u$ and $u_n$ . Thank you for your help.","['weak-convergence', 'functional-analysis', 'analysis']"
3218464,Find the adjoint of the right shift operator in $\ell^1$.,"Find the adjoint of the right shift operator $T$ in $\ell^1$ . More specifically, $T: X \longrightarrow Y$ defined by $$Tx= T(x_1, x_2, \dots, x_n, \dots) = (0, x_1, x_2, \dots, x_n\dots) = y$$ Attempt I fist showed $T$ was linear, then bounded by noticing sum of the series generated by $(|x_n|)$ is the same as the series generated by $(|y_n|)$ . Now it remains for me to find the adjoint operator of $T$ . I'm sort of confused at this step though because I think I'm supposed to be finding the adjoint in the sense of $T^\times: Y' \longrightarrow X'$ defined by (in Kreyszig's functional analysis book) $$f(x) = \left(T^\times g\right)(x) = g(Tx), \text{where } g \in Y'$$ which I don't believe is the same as the Hilbert adjoint (which my brain keeps circling back to). Do I need to find $f$ explicitly here? More or less I'm hoping someone can clearly explain what the idea is and maybe give me a step in the right direction. Thank you.","['adjoint-operators', 'functional-analysis', 'real-analysis']"
3218477,Relationship between tangents and double roots,"I am dealing with the proof of the following Theorem, taken from Dale Husemöller's book Elliptic Curves : I have trouble to understand the following underlined section of the proof: Could you please elaborate this section for me? I only know that the tangent has something to do with the derivative: $$L: \frac{\partial F}{\partial x}(P)(x-x_P) + \frac{\partial F}{\partial y}(P)(y-y_P)=0$$ where $P = (x_P,y_P)$ and $F(x,y) = y^2 - x^3 - ax$ . However, I do not see what is has to do with double roots. For real differential functions $f:\mathbb{R} \to \mathbb{R}$ in one variable, I know that something has a double root $a$ if and only if $f(a) = f'(a) = 0$ . But I do not know how this can be applied to functions with more than one variable. Thank you in advance!","['elliptic-curves', 'proof-explanation', 'tangent-line', 'algebraic-geometry', 'abstract-algebra']"
3218482,Continuous extension from Cantor set to unit interval,"I have the following assignment: Let $C \subset [0, 1]$ be the Cantor set, $x ∈ C$ satisfying $$x = \sum_{n = 1} ^ \infty \frac{a_n}{3^n} \; ,$$ and $\varphi: C \to [0,1]$ a function defined as $$\varphi(x) = \sum_{n=1} ^ \infty \frac{a_n}{2^{n+1}} .$$ I want to prove: a) The function $\varphi: C \to [0,1]$ is continuous, surjective and monotone . b) There exists a continuous extension $\hat{\varphi}:[0,1] \to [0,1]$ of $\varphi$ , such that $\hat{\varphi}$ is constant in any open set $U$ of $[0,1] \setminus C$ . c) The function $\hat{\varphi}$ is almost everywhere differentiable . My thoughts a) The function $\hat{\varphi}$ seems to be an identification of the Cantor set with the binary representation of elements in the unit interval. From this, I don't know how to prove continuity. Which topology of $C$ should I consider? How to prove continuity? b) $[0,1]$ is a complete metric space. I was thinking that if $\hat{\varphi}$ is uniformly continuous, this could show the existence of a continuous extension over $[0,1]$ . However, this fails since the Cantor set is not dense in $[0,1]$ . c) Should I work with the weak derivative of $\hat{\varphi}$ ? Thanks for any help","['continuity', 'cantor-set', 'derivatives', 'functional-analysis']"
3218484,Locally compact limits of countably many finite $T_0$-spaces,"Consider any diagram of finite $T_0$ -spaces where the spaces involved are at most countably many. So we have $X_0, X_1, \ldots, X_n, \ldots$ each of which is $T_0$ and finite and some continuous maps between them. Suppose $L$ is the limit of that diagram in the category $\text{Top}$ . Is $L$ locally compact (or even compact)? Here we take the definition of locally compact as definition 2.1 on nLab : every point has a neighbourhood base consisting of compact subspaces. In other words: a topological space $X$ is locally compact if for every $x \in X$ and every open neighbourhood $U$ of $x$ there is some compact neighbourhood $K$ of $x$ with $K \subseteq U$ .","['general-topology', 'limits-colimits', 'category-theory', 'compactness']"
3218496,Finding the position where a picture was taken,"Assuming I have a photo where a few significant points are visible, and I can point them out on a map. How many points would I have to identify, in order to find the precise position? Would I also find the altitude? A small example: Points: point   coordinates               elevation    pixel in photo (of 4032x3024)
red      -13.156197,-72.546324    2494.0 m     1229x1201 px
blue     -13.161653,-72.545699    2450.0 m     1050x1938 px
pink     -13.158724,-72.540743    1994.0 m     2156x2436 px
yellow   -13.166027,-72.542900    2394.0 m     1604x2545 px
green    -13.163450,-72.544715    2456.0 m     1208x2165 px picture with annotated points: map with points:","['coordinate-systems', 'trigonometry', 'triangulation']"
3218505,A train starts from a Jaipur towards Delhi at 10 am. ..,"A train starts from a Jaipur towards Delhi at 10 am. Another train starts from Delhi towards Jaipur at 11 am. Both reach their destinations at same time. At 1 pm, the sum of distances that had been traveled by them together is 90% of distance between Delhi and Jaipur. At what time did they reach their destinations? I could only make equation like this : Train(Jaipur's speed) = x Train(Delhi's speed) = x+1 (since it starts one hour later and and both reaches at same time) so equation : 3x + 2(x+1) = 90% of distance now I'm out of any clue where to move... Thanks in advance :)",['algebra-precalculus']
3218542,"If $f$ is in the span of eigenfunctions, then $|f|$ is also in the span of eigenfunctions.","Let $(X,\mathscr{B},\mu,T)$ be a measure preserving dynamical system. Then $U_T:L^2(X,\mu)\rightarrow L^2(X,\mu)$ defined by $U_Tf=f\circ T$ is an isometry. Let $\mathscr{E}$ be the eigenspace(closed) of $U_T$ . Problem: If $f$ is in $\mathscr{E}$ , then show that $|f|$ is also in $\mathscr{E}$ . Maybe a possible hint. The above problem will be solved if we can prove that $L^\infty(X)\cap \mathscr{E}$ is dense in $\mathscr{E}$ . See the discussion before Proposition 4.19 . This result is essentially used to prove Lemma 4.23 of Recurrence in Ergodic Theory and Combinatorial Number Theory by Furstenberg, but I am not able to decode the argument given there.","['measure-theory', 'eigenvalues-eigenvectors', 'ergodic-theory', 'eigenfunctions', 'dynamical-systems']"
3218551,Independent increment vs independent sigma-algebras,"Suppose we want to define a Lévy process $\{ X_t \vert \ t \geq 0\} $ . Is it equivalent to demand independent increments i.e. $$ \forall n \geq 1, \forall t_n \geq t_{n-1} \geq ...\geq t_1 \geq 0: X_{t_1}, X_{t_2}-X_{t_1},..., X_{t_n} - X_{t_{n-1}} \ \text{are independent} $$ versus demanding that $X_t-X_s$ is independent of the sigma-algebra generated by $\{X_k, 0\leq k \leq s\}$ ?","['levy-processes', 'measure-theory', 'independence', 'stochastic-processes', 'probability-theory']"
3218557,Continuous function with infinitely many zeros,"Let $f:[0;1]\rightarrow\mathbb{R}$ a continuous function. Let $Z$ denote the set of zeros of $f$ . If $Z$ is finite, it's easy to prove that $\lim\limits_{n\rightarrow+\infty}\left|\displaystyle\int_{0}^1\text{e}^{nx}f(x)\,\text{d}x\right|=+\infty$ . But is it also true if $f\ne0$ and $Z$ is infinite ?","['integration', 'functions', 'exponential-function', 'real-analysis']"
3218574,A proof for the Final Value theorem using Dominated convergence theorem,"I'm going over the proof for the Final Value theorem using the Dominated Convergence theorem on Wikipedia , I don't understand how from the equation $$sF\left(s\right)=\int_{0}^{\infty}f\left(\frac{t}{s}\right)e^{-t}dt$$ and the fact that $\left|f\left(\frac{t}{s}\right)e^{-t}\right|$ is dominated by $Me^{-t}$ they get to the limit $$\lim_{s\searrow0}sF\left(s\right)=\int_0^{\infty}{\alpha}e^{-t}dt=\alpha$$","['functional-analysis', 'laplace-transform']"
3218625,Show that the probability of this sequence of random variables being a root of this function tends to $0$,"Let $f\in C^3(\mathbb R)$ with $f>0$ , $$\int f(x)\:{\rm d}x=1$$ and such that $(\ln f)'$ is Lipschitz continuous, $$p_n(x):=\prod_{i=1}^nf(x_i)\;\;\;\text{for }x\in\mathbb R^n$$ and $$h_n^{(x)}(z):=\frac{p_n(x+z)}{p_n(x)}-1\;\;\;\text{for }x,z\in\mathbb R^n$$ for $n\in\mathbb N$ $(\sigma_n)_{n\in\mathbb N}\subseteq(0,\infty)$ be decreasing with $\sigma_n\xrightarrow{n\to\infty}0$ $(\Omega,\mathcal A,\operatorname P)$ be a probability space $X^{(n)},Y^{(n)}$ be $\mathbb R^d$ -valued random variables with $X^{(n)}\sim p_n\lambda^n$ ( $\lambda^n$ denoting the Lebesgue measure on $\mathcal B(\mathbb R^n)$ ) and $$\operatorname P\left[Y^{(n)}\in B\mid X^{(n)}\right]=\mathcal N_d\left(X^{(n)},\sigma_n^2I_n\right)\;\;\;\text{almost surely for all }B\in\mathcal B(\mathbb R^n)\tag1$$ ( $I_n$ denoting the $n\times n$ identity matrix) for $n\in\mathbb N$ Note that by $(1)$ there is a $\mathbb R^d$ -valued random variable $Z^{(n)}$ on $(\Omega,\mathcal A,\operatorname P)$ independent of $X^{(n)}$ with $Z^{(n)}\sim\mathcal N_d(0,\sigma_n^2I_n)$ and $$Y^{(n)}=X^{(n)}+Z^{(n)}\tag2$$ for all $n\in\mathbb N$ . I want to show that $$\operatorname P\left[h_n^{\left(X^{(n)}\right)}\left(Z^{(n)}\right)=0\right]\xrightarrow{n\to\infty}0\tag3.$$ Obviously, $$h_n^{(x)}(z)=0\Leftrightarrow\prod_{i=1}^n\frac{f(x_i+z_i)}{f(x_i)}=1\;\;\;\text{for all }x,z\in\mathbb R^d.\tag4$$ We may note that the Lipschitz continuity (with Lipschitz constant $c$ , say) of the derivative of $g:=\ln f$ implies the following: $g(y)-g(x)-g'(x)(y-x)\ge-\frac c2|y-x|^2$ for all $x,y\in\mathbb R$ $f(y)\ge f(x)e^{\frac{|g'(x)|^2}{2c}}e^{-\frac c2\left|y-x-\frac{g'(x)}c\right|^2}\ge f(x)e^{-\frac c2\left|y-x-\frac{g'(x)}c\right|^2}$ for all $x,y\in\mathbb R$ $f(x)\le\sqrt{\frac c{2\pi}}e^{-\frac{|g'(x)|^2}{2c}}\le\sqrt{\frac c{2\pi}}$ for all $x\in\mathbb R$ $f$ is Lipschitz continuous with constant $\frac c{\sqrt{2\pi e}}$","['measure-theory', 'probability-theory', 'geometric-measure-theory', 'real-analysis']"
3218641,"Finding $\lim_{n\to\infty}\sqrt n\int_1^n \frac{\arctan (x/\sqrt n)}{x^2+n}\,dx$","Let $I_n = \int_{1}^{n} \frac{\arctan \frac{x}{\sqrt n}}{x^{2}+n}\,dx$ , for $n \geq 1$ . How can I show that $\lim_{n \to \infty} \sqrt nI_n=\frac{\pi}{8}$ ? I've tried to first solve the integral but I'm really stuck, I couldn't find an ideea to start from, any help or suggestions would be truly appreciated!","['integration', 'limits', 'definite-integrals']"
3218644,Polynomial equation for $z_{k}=\cos\frac{2k\pi}{5}+i\sin\frac{2k\pi}{5}$,"I have $z_{k}=\cos\frac{2k\pi}{5}+i\sin\frac{2k\pi}{5}, k=1,2,3,4$ . I need to find the polynomial equation for the roots $z_k(k=1,2,3,4)$ The right answer is $x^4+x^3+x^2+x+1=0$ .I tried to replace k with 1,2,3,4 to find the roots and then to use $a(x-x_1)(x-x_2)(x-x_3)(x-x_4)$ but I didn't get too far.","['trigonometry', 'polynomials', 'complex-numbers']"
3218647,"Restricted Totient Sums $\sum_{n=1}^{\sqrt {L-1}}\widehat \varphi(B_n,n)$","Let the Euler totient function be: $$ (1) \qquad \qquad\varphi(n) = \sum_{k=1, \atop gcd(k,n)=1}^{n}1$$ And let the following 2 functions be variations of the original Euler totient function: $$(2) \qquad \qquad \widehat \varphi(n) = \sum_{k=1, \atop {gcd(k,n)=1 \atop k \space mod \space 2 \space \neq \space n \space mod \space 2}}^{n}1$$ $$(3) \qquad \qquad \varphi(B,n) = \sum_{k=1, \atop gcd(k,n)=1}^{B}1$$ To summarize and clarify: $(1)$ Is simply the Euler totient function without modifications, i.e, the count of number smaller than $n$ that are coprime to $n$ . $(2)$ Is Euler totient function, modified so that it only counts the numbers smaller than $n$ that are coprime to it, but also do not have the same parity, i.e - if $n$ is odd, then it only counts coprimes that are even, and if $n$ is even, it only counts coprimes that are odd. Obviously, for the case where $n$ is even, it does not matter, but for the case of $n$ odd, it does change the count, because odd $n$ may have coprimes that are odd as well. In this function (2) we do not count them. $(3)$ In that modification, we count numbers smaller than $n$ which are coprime to it, up to a certain bound $B$ . For example, $\varphi(3,7) = 3$ . Although all numbers smaller than $7$ are coprime to it, we only count those $\leqslant B = 3$ . Finally, I am interested in a sum which involves both modifications. For a limit $L$ , consider the following sum: $$(4) \qquad \qquad S = \sum_{n=1}^{\sqrt {L-1}}\widehat \varphi(\sqrt {L-n^2},n)$$ To express it in a more clear way, further denote $B_n = \sqrt {L-n^2}$ , so we can rewrite the sum as follows: $$(5) \qquad \qquad S = \sum_{n=1}^{\sqrt {L-1}}\widehat \varphi(B_n,n)$$ Calculating totients $(1)$ is well known and I won't go into details about it. I am also aware of efficient ways to evaluate the totient summatory function $\sum_{k=1}^n \varphi (k)$ . However, efficiently calculating $(2)$ and $(3)$ is something I do not know how to do. Evaluating them in a naive way, as well as evaluating the sum in a naive way is simple enough. But for large $L$ it is inefficient. Any insights regarding $(2), (3), (4) , (5)$ will be appreciated. Thank you.","['number-theory', 'elementary-number-theory', 'functions', 'combinatorics', 'computer-science']"
3218655,Metric of positive curvature and Homology group,Is it possible to decide that whether the manifold $M$ admit a metric of positive curvature by knowing all Homology group of $M$ ?,"['differential-topology', 'riemannian-geometry', 'differential-geometry']"
3218663,Martingale exercise,"I'm struggling with an exercise from a martingales theory book: Let $M$ be an $F$ -martingale and $Z$ an adapted (bounded) continuous process. Prove that: $$
\mathbb{}E\left(M_t\int_s^tZ_u \, du \mid \mathcal{F}_s \right)=\mathbb{}E\left(\int_s^t M_u Z_u \, du \mid \mathcal{F}_s\right)
$$ It was given just after the Doob-Meyer decomposition theorem. I started from the right term interverting integral and expectation signs: $$
\mathbb{}E\left(\int_s^t M_u Z_u \, du \mid \mathcal{F}_s\right)=\mathbb{}\int_s^tE\left( M_u Z_u \mid \mathcal{F}_s\right) \, du
$$ but I didn't see where it could lead. Any hint (not a solution) would be appreciated","['stochastic-processes', 'probability-theory', 'martingales']"
3218664,"If $\lim_{x \rightarrow 0}\frac{f(x)}{x^2} = 5$, then what is $\lim_{x \rightarrow 0}f(x)$?","I know the answer to the above question, but I have a question on some of the reasoning. The way I know how to solve it is $$\lim_{x \rightarrow 0}f(x) = \lim_{x \rightarrow 0}\left(f(x)\cdot \frac{x^2}{x^2}\right) = \lim_{x \rightarrow 0}\left(\frac{f(x)}{x^2}\cdot x^2\right) = \left(\lim_{x \rightarrow 0}\frac{f(x)}{x^2}\right)\left(\lim_{x \rightarrow 0}x^2\right) = 5\cdot0 = 0.$$ I saw another solution elsewhere that gets the right answer, but I am unsure if the steps are actually correct. \begin{align*}
&\lim_{x \rightarrow 0}\frac{f(x)}{x^2} = 5 \\
\Longrightarrow &\frac{\lim_{x \rightarrow 0}f(x)}{\lim_{x \rightarrow 0}x^2} = 5 \\ 
\Longrightarrow &\lim_{x \rightarrow 0}f(x) = 5\cdot \lim_{x \rightarrow 0}x^2\\ \Longrightarrow &\lim_{x \rightarrow 0}f(x) = 5\cdot 0 = 0.
\end{align*} My issue is with that first step. I know that $\lim_{x \rightarrow a} \frac{f(x)}{g(x)} = \frac{\lim_{x \rightarrow a}f(x)}{\lim_{x \rightarrow a}g(x)}$ , but only when $\lim_{x \rightarrow a}g(x) \neq 0$ . Since $\lim_{x \rightarrow 0}x^2 = 0$ , wouldn't this invalidate the above work? However, it still got the same answer, so my real question is why did it work and when will it work in general? EDIT: Does anyone have a nice example for when the logic in the second method doesn't work?","['limits', 'calculus', 'limits-without-lhopital']"
3218687,"Show $\int_0^{2\pi}\cos^2t\,dt=\pi$ using Pythagorean Theorem","The 'traditional' way I always see this integral calculated is with the identity $$\cos^2t=\frac{1+\cos(2t)}{2}$$ My alternative method uses $\cos^2t+\sin^2t=1$ . It's obvious that $$\int_0^{2\pi}\cos^2t+\sin^2t\,dt=\int_0^{2\pi}1\,dt=2\pi$$ The interval of integration is an integer multiple of the periods of each function ( $\cos^2t$ has a period of $\pi$ ), and so it seems reasonable to me that given the Pythagorean identity used above, $\cos^2t$ and $\sin^2t$ contribute, for lack of a better word, equally to this final answer of $2\pi$ above, and so the integral in the title should be half of $2\pi$ , or just $\pi$ . Is this method valid? What additional statements, if any, are necessary to make it rigorous enough to be valid?","['integration', 'trigonometry']"
3218693,Canonical transformation $T_pV \cong V$?,"Apparently there is a natural isomorphism between a vector space and the tangent space of this vector space at one point. I.e., $\forall p \in V$ we have $T_pV \cong V$ . I know that the isomorphism is $v \mapsto D_{v\vert a}$ . But what is the isomorphism explicitely in the other direction? If I take a derivation $X\in T_p V$ to what vector $v \in V$ is it sent?","['canonical-transformation', 'vector-space-isomorphism', 'differential-geometry']"
3218739,Gradient and Directional Derivative in Riemannian Manifold,"Probably this question is too dumb to be asked, but I am an engineer trying to learn differential geometry, please go easy on me. I am trying to understand that, in Riemannian space, gradient satisfies the property $g(\nabla f, v) = df(v)$ . Here, $f$ is a scalar-valued function, $v$ is a vector in the tangent space at point $P$ on the manifold $\mathcal{M}$ and the directional derivative along $v$ is $df(v)$ . Essentially, this property links the gradient to the directional derivative. In Orthonormal Euclidean Space $\{x,y,z\}$ , the gradient is $\nabla f  = \left(\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y},\frac{\partial f}{\partial z} \right)$ .
So if one wants to know how the function $f$ changes along a particular direction $\mathbf{v} = v_xx + v_yy + v_zz$ , we take the inner product of $\langle \mathbf{v} , \nabla f \rangle$ divide by the norm of $\mathbf{v}$ . Question: Why are we not normalizing in the Riemannian space
  similarly?","['metric-spaces', 'riemannian-geometry', 'differential-geometry']"
3218771,Characterization of Reflexive Banach Space.,"Prove that a real Banach Space $X$ is reflexive if and only if each pair of disjoint closed, convex subsets of $X$ , one of which is bounded, can be strictly separated by a hyperplane. The theorem is stated and proved in the book Geometric Functional Analysis and its Applications by Richard Holmes on page-161, which goes like this We shall show that the disjoint closed convex sets $H$ and $U(M)$ cannot be strictly separated. Suppose otherwise; then there would exist $\psi\in X^*$ and a positive number $\gamma$ such that $\psi(x)<\gamma<\psi(y)$ for all $x\in U(M)$ and $y\in H$ . Let us assume that $\phi$ has been extended to all of $X$ via the Hahn-Banach Theorem, and let us call the extension $\phi$ also. Choose any point $\overline{x}\in X$ such that $\phi(\overline{x})=\|\phi\|$ and any $z\in X$ for which $\phi(z)=0$ . Then for all $\lambda\in \mathbb{R}, \psi(\overline{x}+\lambda z)>\gamma$ . Hence, $\psi(z)=0$ and so $\ker(\phi)\subset \ker(\psi)$ . This means that the set $\{\phi,\psi\}$ is linearly dependent. We can therefore choose a constant $\alpha$ so that $\psi=\left(\frac{\alpha \gamma}{\|\phi\|}\right) \phi$ . Now if $y\in H$ then $\gamma<\psi(y)=\alpha \gamma$ , so that $1<\alpha$ . On the other hand, if $x\in U(M)$ then $\psi(x)<\gamma$ , and so $\phi(x)=\left(\frac{\|\phi\|}{\alpha\gamma}\right)\psi(x)<\frac{\|\phi\|}{\alpha}$ ; since $\alpha>1$ , this contradicts the definition $\|\phi\|=\sup\{\phi(x):x\in U(M)\}$ . 
  I am trying to decipher this proof. It uses the contrapositive logic and is based on the James Theorem a Banach space $X$ is reflexive if and only if every continuous linear functional on $X$ attains its supremum on the closed unit ball in $X$ . And, here is what I could understand in this proof Let $X$ be a Banach Space and $M$ be a subspace. Let $\phi\in M^*$ be a continuous linear functional.  Let $H=\{x\in M:\phi(x)=\|\phi\|\}$ be a hyperplane in $M$ and $U(M)=\{x\in M:\|x\|=1\}$ be a unit ball in $M$ . Suppose that there exists a continuous linear functional $\psi\in X^*$ and a positive number $\gamma$ such that $\psi(x)<\gamma<\psi(y)$ for all $x\in U(M)$ and $y\in H.$ Choose a point $\overline{x}\in X$ such that $\phi(\overline{x})=\|\phi\|$ , i.e. $\overline{x}\in H$ and any $z\in X$ such that $\phi(z)=0$ . 
  Then, for a real number $\lambda$ , $\phi(\overline{x}+\lambda z)=\phi(\overline{x})+\lambda \phi(z)$ .
  Since $\overline{x}\in H$ and $\phi(z)=0$ , $\phi(\overline{x}+\lambda z)=\|\phi\|+\lambda. 0$ . $\implies \phi(\overline{x}+\lambda z)=\|\phi\|$ . $\implies \overline{x}+\lambda z\in H$ . $\implies \psi(\overline{x}+\lambda z)>\gamma$ .
  What I couldn't understand is How the idea of extending $\phi$ is applied in all of this? How do you infer $\psi(z)=0$ ?","['banach-spaces', 'functional-analysis']"
3218776,Differentiation of the law of cosines,"$$s^2=R^2+r^2-2Rr\cos(\theta)$$ differentiated to give $$2s\text ds=2Rr\sin(\theta)\text d\theta$$ $$\sin\theta\text d\theta=\frac{s\text ds}{Rr}$$ I found this differentiation in a site that was explaining the gravitational force inside a hollow sphere and it used these differentiation as a proof.
I don't understand that why does this work, when its differentiating with respect to different values.","['derivatives', 'geometry']"
3218803,Is a differentiable (but not $C^1$) function $f: \mathbb{R}^n \rightarrow \mathbb{R}^n$ with invertible derivative everywhere an open map?,"Is a differentiable (but not $C^1$ ) function $f: \mathbb{R}^n \rightarrow \mathbb{R}^n$ with invertible derivative everywhere an open map?  I know that if we assume the function is $C^1$ , then this is a consequence of the inverse function theorem (or a step on the way to proving the inverse function theorem).  I've convinced myself no counter-example exists if $n = 1$ , but I haven't been able to prove it or come up with a counter-example in the case $n = 2$ .  I've spent more time trying to come up with a counter-example, mostly trying variants involving $x^2 \sin(1/x)$ .","['open-map', 'inverse-function-theorem', 'real-analysis']"
3218913,Understanding principal component analysis,"Let $X$ be $m\times n$ sample matrix where each row is a sample point. We want to find matrix $P$ of dimension $n \times r$ such that $XP$ is the dimension reduced matrix of samples after applying the principal component technique. We find $P$ by maximizing the trace of the covariance matrix $C_Y^{'}=\frac{1}{m}(XP)^T(XP)=P^T(\frac{1}{m}X^TX)P$ . Because we want the variance of each variable to be maximized. We let $C=\frac{1}{m}X^TX$ and we want to maximize $tr(P^TCP)$ subject to $P^TP=I$ . They said that we can use lagrange method to find partial of $f(P)=tr(P^TCP)+\lambda(P^TP-I)$ . I don't understand this, please explain. Also, they used $\frac{\partial tr(AB)}{\partial A}=B^T$ , and $\frac{\partial X^TX}{\partial X}=X$ . I need help understanding that as well. They did $\frac{\partial f}{\partial p}= \frac{\partial tr(P^TCP)}{\partial P}+\lambda \frac{\partial (P^TP)}{\partial P} =\frac{\partial tr(PP^TC)}{\partial P}+\lambda P=(P^TC)^T+\lambda P=C^TP+\lambda P=CP+\lambda P$ , and when set to $0$ , we get $CP=(-\lambda)P$ . And that shows why we need to calculate eigenvalues. I need clarification on that as well, for example, how to choose size of $P$ ?","['multivariable-calculus', 'statistics', 'linear-algebra']"
3218931,Probability of taking balls without replacement from a bag question,"A bag contains $N$ balls, $2$ of which are red. Balls are removed, one by one, (without replacement), stopping when both red balls have emerged. Find the probability that exactly $n$ balls are removed. I'm honestly not sure where to even start. I know it's an intersection of ( $1$ Red ball in $n-1$ attempts) and (red on $n$ -th attempt). I think the $Pr(\text{Red on}\;n\text{-th attempt}\; | \;1 \; \text{Red already})$ is $$ \frac{1}{N-(n-1)} $$ but I'm not sure for the other probability, or whether this one is correct for that matter. Any help would be appreciated. :)","['statistics', 'probability']"
3218973,Compactness in normed vector spaces.,"Let $(X,\Vert\cdot\Vert)$ be a normed $\mathbb{K}$ -vector space and $A \subset X$ be closed and bounded. My problem is how to determine whether $A$ is compact? I know that a compact subset is always closed and bounded. And that in $\mathbb{R}$ the converse holds due to Bolzano–Weierstraß. But I think I am missing something. Do there exist subsets in normed vector spaces which are closed and bounded but not compact?","['normed-spaces', 'functional-analysis', 'compactness']"
3218988,On a proof of strong consistency of the maximum likelihood estimator.,"let $X_1, \dots, X_n$ be IID random variables with continous density $g(x|\theta_0)$ , where $\theta_0 \in \Theta \subset \mathbb{R}$ and $\Theta = \{ \theta_0, \theta_1, \dots, \theta_m \}$ (the parameter space is finite, we are in a very simple case). We define the log-likelihood as $$ \ell_n(\theta) : = \sum_{i= 1}^n \log g(X_i | \theta)$$ Suppose we have proven that $\ell_n(\theta_0) - \ell_n(\theta) \rightarrow \infty$ almost surely. Fixed an $\epsilon > 0$ there exists a $n_0 \in \mathbb{N}$ s.t defining $$A_j = \{ \ell_n(\theta_0)  -   \ell_n(\theta_j) > \epsilon , \ \forall{n} > n_0 \}$$ we have $P(A_j) > 1- \delta$ where $\delta > 0$ is arbitraty. ( this can be done since we know that $\ell_n(\theta_0) - \ell_n(\theta) \rightarrow \infty$ almost surely). Then we have that $$ P \left( \bigcap_{j = 1}^m A_j \right) \ge 1-  \sum_{j=1}^mP \left(  A_j^c \right) \ge 1- m \delta $$ we have thus shown that $ P\{ \ell_n(\theta_0)  -   \ell_n(\theta_j) > \epsilon , \ \forall{j} \ne 0 , \ \forall{n} > n_0 \} \ge 1- m \delta  \tag{1}$ it is then apparently immediate (and here is my problem) that the maximim likelihood estimator $ \hat{\theta} : = \arg \max \ell_n(\theta)$ converges almost surely to $\theta_0$ . Why is this last step ""obvious""? My attempt: In particular $(1)$ implies that $ P\{ \ell_n(\theta_0)  -  \max_{\theta} \ell_n(\theta) > \epsilon ,  \ \forall{n} > n_0 \} \ge 1- m \delta$ and somehow we should get that $\ell_n ( \hat{\theta}) $ converges almost surely to $\ell(\theta_0)$ that should give us that $\hat{\theta} \rightarrow \theta_0$ .","['statistics', 'convergence-divergence', 'probability-theory', 'probability']"
3218995,Inverse functional derivatives: Find a functional whose functional derivative is a given function,"https://en.wikipedia.org/wiki/Functional_derivative Is there a straightforward way to find a functional whose functional derivative has a particular form? That is, is there something like functional integration? Here's my specific problem. I want to find a functional $F(\rho)$ whose functional derivative is $$\frac{\delta F}{\delta\rho}(x) = f((g\star\rho)(x)) $$ Here $g\star\rho$ means the convolution of functions $g:\mathbb{R}\rightarrow\mathbb{R}$ and $\rho:\mathbb{R}\rightarrow\mathbb{R}$ . $f:\mathbb{R}\rightarrow\mathbb{R}$ is some other (generally nonlinear) function. The functions $f$ and $g$ are given. Feel free to assume $f$ has as many continuous derivatives as you need, and that $f$ and $g$ can be integrated analytically. If it matters to anyone, this question arises out of a strategy to develop a Lyapunov functional for a nonlinear PDE system. A numerical solution would be good enough for my purposes. Thanks for any ideas.","['functional-analysis', 'functional-calculus', 'calculus-of-variations']"
3219009,What is the solution to linear ODE $\dot x = Ax + b$?,"Suppose I have a system of linear ODE $$\dot x = Ax + b$$ where $A \in \mathbb{R}^{n \times n}, b \in \mathbb{R}^n$ I know that when $b = 0$ , using Laplace transform we have $x(s) = (sI-A)^{-1} x(0)$ , which upon inverting (pg.13) we obtain $x(t) = e^{At}x(0)$ However, when $b$ is non-zero, the Laplace transform gives $$x(s)  = (sI-A)^{-1}x(0) + (sI-A)^{-1}\dfrac{b}{s}$$ I have no idea how to invert $(sI-A)^{-1}\dfrac{b}{s}$ Does anyone know how to solve for this? Some thoughts, $(sI-A)^{-1}\dfrac{b}{s} = (sI-A)^{-1}(sI)^{-1}b = ((sI)(sI-A))^{-1}b = (s^2I - sA)^{-1}b$","['matrices', 'control-theory', 'laplace-transform', 'ordinary-differential-equations']"
3219020,Correlation coefficient between two binominals,"My problem is the following: Roll a dice 30 times, X is the number of 1's and Y is the number of 6's. Find the correlation coefficient $R(X,Y)=\frac{\operatorname{Cov}(X,Y)}{\sigma_x \sigma_y}$ . 
The covariance is $\operatorname{Cov}(X,Y)=E(XY)-E(X)E(Y)=0-5*5=-25$ and the product of the ${\sigma}$ -s is $30\cdot\frac{1}{6}\cdot\frac{5}{6}=\frac{25}{6}$ , that gives $R=-6$ which is a bad solution for $R$ . 
Where did I mistake? Can anybody fix it? Thank you for your answers","['statistics', 'probability']"
3219025,"Why $(-2)^{2.5}$ isn't equal to $((-2)^{25})^{1/10}?\,$ [Fractional powers of negative numbers]","I've tried both calculations on Wolfram Alpha and it returns different results , but I can't get a grasp of why it is like that.
From my point of view, both calculations should be the same, as $2.5=25/10,$ and $(-2)^{2.5}$ is equal to $(-2)^{25/10},$ relying on a general rule $(a^m)^n=a^{mn}$ . Links to sources: https://www.wolframalpha.com/input/?i=(-2)%5E(2.5) https://www.wolframalpha.com/input/?i=((-2)%5E(25))%5E(1%2F10)","['exponentiation', 'algebra-precalculus', 'complex-numbers']"
3219085,What is the fundamental group of this complex surface?,"Let $X\subset A^3_{\mathbb{C}}$ be the affine complex surface defined by $x^r+y^s+z^t=0 $ , and let $Y=X-(0,0,0)$ . Here $r,s,t\geq2$ are positive integers. Then what is the fondamental group of $Y$ ? What can we know from the map $f:Y\to A^2_{\mathbb{C}}-(0,0),\,\,\,x\mapsto x^r,y\mapsto y^s$ ?","['complex-geometry', 'algebraic-geometry', 'algebraic-topology']"
3219098,notation in abstract algebra: $\mathbb{Q}(\sqrt{2})$,"What does $\mathbb{Q}(\sqrt{2})$ mean? Is it $\{ a+b\sqrt{2}  \mid a, b \in \mathbb{Q} \}$ ?","['notation', 'abstract-algebra']"
3219147,Equation of the line that lies tangent to both circles,"Consider the two circles determined by $(x-1)^2 + y^2 = 1$ and $(x-2.5)^2 + y^2 = (1/2)^2$ . Find the (explicit) equation of the line that lies tangent to both circles. I have never seen a clean or clever solution to this problem. This problem came up once at a staff meeting for a tutoring center I worked at during undergrad. I recall my roommate and I - after a good amount of time symbol pushing - were able to visibly see a solution by inspection, then verify it by plugging in. I have never seen a solid derivation of a solution to this though, so I would like to see what MSE can come up with for this! I took a short stab at it today before posting, and got that it would be determined by the solution to the equation $$\left( \frac{\cos(\theta)}{\sin(\theta)} + 2\cos(\theta) + 3 \right)^2 -4\left( \frac{\cos(\theta)^2}{\sin(\theta)^2}+1 \right)\left(\frac{-\cos(\theta)^3}{\sin(\theta)^2}+\frac{\cos(\theta)^4}{\sin(\theta)^2}+3 \right).$$ The solution $\theta$ would then determine the line $$y(x) = \frac{-\cos(\theta)}{\sin(\theta)}(x) + \frac{\cos(\theta)^2}{\sin(\theta)} + \sin(\theta).$$ Not only do I not want to try and solve that, I don't even want to try expanding it out :/","['geometry', 'calculus', 'algebra-precalculus', 'recreational-mathematics']"
3219157,Why is a solution to $y'' + y = 0$ periodic,Solutions to $y'' + y = 0$ are 2 $\pi$ periodic. Is this accidental or does this ODE have some symmetries associated with it that force the solutions to be periodic?,"['symmetry', 'ordinary-differential-equations']"
3219191,Find exact value of $\sum_{n=1}^{\infty} \frac{1}{2^{n}} \tan \left( \frac{\pi}{2^{n+1}} \right)$,"How can I compute the series $$\sum_{n=1}^{\infty} \frac{1}{2^{n}} \tan \left( \frac{\pi}{2^{n+1}} \right)$$ I just guess using half-angle formula to compute this series, but I can't do any approaches. How should I do to solve this infinite sum?","['calculus', 'sequences-and-series', 'real-analysis']"
3219290,Round number problem involving CLT.,"Every time Jim uses his credit card, he rounds the amount he was charged to the nearest dollar
in his records. Assume the number of cents involved in each purchase is a uniformly distributed
whole number between 0 and 99. He has used the credit card a total of 150 times. (a) If the total amount in Jim's records differs from the actual total by \$10, what is the mean error (difference between amount charged and amount recorded) per purchase? (b) Find the probability that the total amount in Jim's records differs from the actual total by more than \$10. (Hint: The Central Limit Theorem implies the distribution of the mean
error per purchase in a random sample of 150 purchases is normal) a) I think the answer should be $\frac{10}{150} \approx \$0.07$ per purchase. b) I do not know how to solve this problem. Here is what I have done so far: Let $X$ be the random variable for the error in total. Since the number in cents is selected in the discrete interval $[0,99]$ randomly, the probability for rounding any price amount is $p = 0.5$ because for the first 50 digits [0 , 49], we don't round, but we round for [50 , 99]. Then... $\bar X = 0.005{}\cdot{}150 = \$0.75$ $ \mu  = \$10 $ ? I don't really know what to put here. I guess it is given by the problem. $\sigma^2 = E[(X-\mu)^2] = \frac {1}{100} 2(0^2 +0.1^2 +...+0.50^2)= 0.4583 $ $  \frac{\sigma}{\sqrt{n}} = \frac{\sigma}{\sqrt{150}} = 0.05528$ $P( Z > \frac{\bar X-\mu}{\frac{\sigma}{\sqrt{n}}}) = 1- P( Z < \frac{0.75-10}{0.05528}) = 1 - P( Z < -167.33)$ ... whose z value is too big to be on the z-table. Where is my mistake?","['statistics', 'probability']"
3219322,Problem 2-37(a) Spivak's Calculus on Manifolds,"2-37(a) Let $f: \Bbb{R}^2 \rightarrow \Bbb{R}$ be a continuously differentiable function. Show that $f$ is not $1$ - $1$ . Hint : If for example, $D_1f(x,y) \neq 0$ for all $(x,y)$ in some open set $A$ , consider $g:A \rightarrow \Bbb{R}^2$ defined by $g(x,y)=(f(x,y),y)$ . My Attempt: For each $y \in \Bbb{R}$ , define $ h_y:\Bbb{R} \rightarrow \Bbb{R}$ s.t. $h_y(x)= f(x,y)$ . $\therefore D_1f(x,y)=h_y'(x)$ . If $D_1f(x_1,y_1)= 0$ for some $(x_1,y_1)$ , $\implies h_{y_1}'(x_1)=0 \implies h_{y_1}$ is not $1$ - $1$ i.e. there exist $x_2,x_3 \in \Bbb{R}$ s.t. $h_{y_1}(x_2)=h_{y_1}(x_3) \implies f(x_2,y_1)=f(x_3,y_1)$ . Thus $D_1f(x,y)$ must be nonzero for all $(x,y)$ in some open set $A$ if it is $1$ - $1$ . Assume $D_1f(x,y) \neq 0$ for all $(x,y)$ in some open set $A$ . consider $g:A \rightarrow \Bbb{R}^2$ defined by $g(x,y)=(f(x,y),y)$ . $g$ is continuously differentiable. Now $$\det\, g'(x,y)=\det\,
\begin{pmatrix} 
D_1f(x,y) & D_2f(x,y) \\
 \frac{\partial y} {\partial x}   & \frac{\partial y} {\partial y}
\end{pmatrix} \neq 0$$ $\therefore g^{-1}$ exists and is differentiable and $(g^{-1})'(x,y)=[g'(g^{-1}(x,y))]^{-1}$ Note: $f(g^{-1}(x,y))=x$ Now $$ [g'(g^{-1}(x,y))]^{-1}=\begin{pmatrix} 
 \frac{\partial f(g^{-1}(x,y)) } {\partial x}& \frac{\partial f(g^{-1}(x,y)) } {\partial y}\\
\frac{\partial y} {\partial x}& \frac{\partial y} {\partial y}
\end{pmatrix}^{-1}=\begin{pmatrix} 
 1& 0\\
0&1
\end{pmatrix}$$ Thus $(g^{-1})'$ is the identity matrix I don't know how to proceed. Can someone give a hint ?","['multivariable-calculus', 'real-analysis']"
3219384,If D$f(0)\neq 0$ then I need to show that a change of variable exists,"I don´t know how to start this problem. Could someone help me please? Let $f$ : $\mathbb {R}^N$ $\to$ $\mathbb {R}$ and $f$ is a function of class $C^1$ in $\mathbb {R}^N$ . Suppose D $f(0) \neq 0$ , then I need to show that a change of variable $\varphi$ : $\mathbb {R}^N$ $\to$ $\mathbb {R}^N$ exists such that: $\varphi(0)= 0$ , D $\varphi(0)$ is the identity. $f(\varphi(x)) = f(0) + Df(0)\cdot$ x,  for a small x","['multivariable-calculus', 'calculus']"
3219391,An unusal feature of a usual double sum of product of two binomial coefficiens,"We know that $$\sum_{0 \le i \le j \le n}  A_i~ A_j= \frac{\left(\sum_{k=0}^{n} A_k\right) ^2+\sum_{k=0}^{n} A_k^2}{2}~~(1)$$ One can find $$ S= \sum_{0 \le i \le j < \infty} ~ 2^{-i-j}$$ using (1), we get $S=8/3$ . We can also find $S$ as below: $$S=\sum_{j=0}^{\infty} 2^{-j} \sum_{i=0}^{j} 2^{-i}= \sum_{j=0}^{\infty} 2^{-j} \frac{(2)^{-j-1}-1}{2^{-1}-1} =2\sum_{j=0}^{\infty}[2^{-j}-(2)^{-2j-1}]=8/3.$$ Using (1). we usually get $$T=\sum_{0 \le i \le j \le n}  {n \choose i} {n \choose j} = \frac{4^n+{2n \choose n}}{2}.$$ But we cannon get $T$ as $$ T =\sum_{j=0}^{n} {n \choose j} \sum_{i=0}^{j} {n \choose i},$$ because the second term is not doable in a closed form. Any help!","['combinations', 'binomial-coefficients', 'sequences-and-series']"
3219404,Does finiteness of expectation of $X$ imply $X$ is finite almost surely?,"Formally,
let $\left(\Omega, \mathscr{F}, \mathbb{P}\right)$ be a probability triple. Let $X$ be a nonnegative random variable such that $\mathbb{E} \left(X\right) < \infty$ , then is it true that $\mathbb{P} \left(X < \infty \right) = 1$ ? Is this something trivial? I just do not know how to begin the proof","['measure-theory', 'probability']"
3219426,"If $M$ is a martingale, why $M\mapsto \left<M\right>$ is a quadratic form?","In the book of Schilling and Partzsch (Brownian motion, an introduction to stochastic process), page 206, they say that $$M\mapsto \left<M\right>,$$ is a Quadratic form where $M$ is a martingale and $\left<M\right>=\left(\left<M\right>_n\right)_n $ where $\left<M\right>_n$ is the unique adapte increasing process in the Doob decomposition $$M^2=M_0^2+N_n+\left<M\right>_n,$$ where $(N_n)$ is a martingale. The thing I don't get it's that a quadratic form $q:V\to \mathbb F$ is a homogeneous polynomial of degree 2 where $V$ is a $\mathbb F-$ vector space. Here, $\left<M\right>$ is a stochastic process not an element of a field... so it's a quadratic form in which sense ?","['martingales', 'quadratic-variation', 'probability-theory']"
3219437,Periodic orbit in vector field with positive divergence,"The Dulac-Bendixson Theorem states that a vector field with positive divergence defined on a 2-connected set cannot have more than one periodic orbit. I am looking for an exemple of a field defined on an open set minus one point, such that the field has positive divergence everywhere and the differential equation defined by such field has a periodic orbit. Many thanks.","['multivariable-calculus', 'ordinary-differential-equations']"
3219449,How to count the number of subsets where no two elements have sum equal to $n + 1$?,"I'm given a set of $n$ integers, where the integers are numbered from $1$ to $n$ . For example if $n = 3$ , the set would be $\{1, 2, 3\}$ . The question is: how many subsets don't have a pair whose sum = $n + 1$ . If I consider all subsets of $n = 3$ , $\{\}$ , $\{1\}$ , $\{2\}$ , $\{3\}$ , $\{1, 2\}$ , $\{2, 3\}$ , $\{1, 3\}$ , $\{1, 2, 3\}$ . In this case the number of subsets in the above condition is $6$ because I excluded $\{1, 3\}$ , $\{1, 2, 3\}$ and there exists a pair add up to $n + 1$ , which is $(1, 3)$ . I noticed that the number of pairs whose sum add up to $n + 1$ = $\lfloor{\frac{n}{2}}\rfloor$ .","['combinatorics', 'discrete-mathematics']"
3219521,"Solve for $x$ and $y$, The equations are $x \cos^{3} y+3x \cos y \sin^{2} y =14 $ and $ x \sin^{3} y+3x \cos^{2} y \sin y = 13 $","Consider the system of equations $$x \cos^{3} y+3x \cos y \sin^{2} y =14 $$ $$ x \sin^{3} y+3x \cos^{2} y \sin y = 13 $$ $1)$ the values of $x$ is /are.. Answer is $ \pm\sqrt 5 $ The number of values of $y$ in $(0,6 \pi)$ is Answer is $6$ $Sin^2 y + 2cos^2 y $ is Answer is $ \frac95 $ I added both the equation and make whole cube I subtracted both the equation and make whole cube then I divided these equations to  find the value of $ \tan y $ = $ \dfrac{1}{2} $ I divided the first equation with $ cos^2 y $ and plugged the value of $\tan y$ which gives $x= +5\sqrt 5 $ . But I also need - $ 5\sqrt 5 $ I have no idea where I go wrong.","['algebra-precalculus', 'trigonometry']"
3219580,1 Black and 1 White Ball in an Urn Black Balls Added After Each Black Draw,"As in title I am looking for the answer to the following problem: We have an urn with 1 black and 1 white ball in it. If you pick a black ball, then you put in another black ball each time until you pick the white ball. Once the white ball is picked the game stops. What is the expected number of balls to be drawn in order for the game to end? Here is what I did: I denoted the probability of stopping at the $k$ -th draw with $P(X=k)$ . Then, $$P(X=1) = \frac 12, $$ $$P(X=2) = \frac 12*\frac13, $$ $$P(X=3) = \frac12*\frac23*\frac14,$$ $$P(X=4) = \frac12*\frac23*\frac34*\frac15$$ and continuing in this manner I find $$P(X=k) = \frac1{k(k+1)}$$ Now, to calculate the expected value, I multiply with $k$ and sum over all $k$ . $$E[X] = \sum_{k=1}^\infty \frac k{k*(k+1)} = \sum_{k=1}^\infty \frac 1{(k+1)}$$ which as we know from p-test, diverges. What am I doing wrong? Any suggestions? Thanks.","['expected-value', 'polya-urn-model', 'probability']"
3219584,How to simplify $\frac{1+\frac{1}{2^p} + \frac{1}{3^p} + \frac{1}{4^p} + \cdots}{1 - \frac{1}{2^p} + \frac{1}{3^p} - \frac{1}{4^p} + \cdots}$,Let $p$ is real number which satisfies $\quad p > 1$ How can I simplify the fraction $$\frac{1+\frac{1}{2^p} + \frac{1}{3^p} + \frac{1}{4^p} + \cdots}{1 - \frac{1}{2^p} + \frac{1}{3^p} - \frac{1}{4^p} + \cdots}$$ Numerator is $\zeta(p)$ but I don't know the closed form of denominator. Is there any idea to simplify this fraction?,"['limits', 'analysis']"
3219594,Covariance of sum of two dependent random vectors,"$\newcommand{\cov}{\operatorname{cov}}\newcommand{\corr}{\operatorname{corr}}$ I have two dependent multivariate random variables, or random vectors, $X$ and $Y$ , their respective covariance matrices $\Sigma_X = \cov(X,X)$ , $\Sigma_Y = \cov(Y,Y)$ and a non-zero correlation matrix $P_{XY} = \corr(X, Y)$ . I would like to compute the covariance matrix of the sum of the two random vectors: \begin{align}
Z {}={}& X + Y
\\
\Sigma_Z {}={}& \Sigma_X + \Sigma_Y + 2 \cov(X, Y)
\end{align} Intuitively, I would assume the cross-covariance can be computed as follows: $$\cov(X, Y) = P_{XY} \sqrt{\Sigma_X} \sqrt{\Sigma_Y}$$ But I can see that this matrix is not necessarily symmetric, and therefore the resulting $\Sigma_Z$ wouldn't be either. Am I wrong? Is it at all possible to compute $\Sigma_Z$ from the information I have? If not, is there some way to approximate it? EDIT: I think it isn't correct to write: $$ \Sigma_Z = \Sigma_X + \Sigma_Y + 2 \cov(X, Y) $$ as $\cov(X,Y) \ne \cov(Y,X)$ , but instead $\cov(X,Y) = \cov(Y,X)^T$ . Rewriting the equation for $\Sigma_Z$ to: \begin{align}
\Sigma_Z {}={}& \Sigma_X + \Sigma_Y + \cov(X, Y) + \cov(Y, X)
\\
{}={}& \Sigma_X + \Sigma_Y + \cov(X, Y) + \cov(X, Y)^T
\end{align} I can now prove that indeed $\Sigma_Z$ is guaranteed to be symmetric. As antkam points out in the chat below, it isn't necessary that $\cov(X, Y) + \cov(X, Y)^T$ be positive semi-definite for $\Sigma_Z$ to be positive semi-definite.","['statistics', 'covariance', 'probability-theory', 'probability', 'random-variables']"
3219625,N-fold sum of random variables,"the topic itself might not be something new, but I don't think this particular problem was posted before. What I am hoping for is advice from somebody who might be more experienced than me on how to tackle the problem I got on my hands. In particular I looking for a solution to the following problem: Find the probability density of $x$ , where $x$ is defined as $x=\sum_{j=1}^{N}{\frac{\cos{\phi_j}}{\sqrt{j}}}$ , when $\phi_j$ are independent uniformly distributed random variables between $0$ and $2\pi$ . I have gotten so far to say that: The probability density of $\cos{\phi_j}$ is given by $\frac{1}{\pi\sqrt{1-x^2}}$ for $-1 < x < 1$ . Then it should follow that the distribution of $\frac{\cos{\phi_j}}{\sqrt{j}}$ is given by $\cos{\phi_j}$ is given by $\frac{1}{\pi\sqrt{1-\frac{x^2}{j}}}$ for $-\frac{1}{\sqrt{j}} < x < \frac{1}{\sqrt{j}}$ . So the last step would be to do an N-fold convolution of the aforementioned distributions. As I mentioned in the beginning, I am hoping for somebody who might know of a similar problem or a trick to simplify this problem. Greets and thank you very much,
FB.","['statistics', 'probability-distributions', 'convolution']"
3219631,Inducing almost complex structure in tensor bundle,"Let $E\to M$ be a (smooth) vector bundle and $J$ be a section of ${\rm End}(E)$ with $J^2= -{\rm Id}$ . Can $J$ induce something in $$\mathscr{T}^{(r,s)}(E)=\bigsqcup_{x\in M} E^{\otimes  r}\otimes (E^*)^{\otimes s},$$ in general? For example, in $E^* = \mathscr{T}^{(0,1)}(E)$ one can set $(J^*\zeta)(\psi)= \zeta(J\psi)$ , this $J^*$ squares to $-{\rm Id}$ and has the nice property that whenever we have a connection $\nabla$ in $E$ , the identity $(\nabla_XJ^*)(\zeta)= \zeta\circ \nabla_XJ$ holds. But if I try to combine these two in the general case and set $$(J\Phi)(\zeta^1,\ldots,\zeta^r,\psi_1,\ldots,\psi_s) = \Phi(\zeta^1\circ J,\ldots ,\zeta^r\circ J,J\psi_1,\ldots, J\psi_s),$$ I get the awkward sign $J^2 = (-1)^{r+s} {\rm Id}$ (which at least is consistent what with I did for $E^*$ ). To summarize my question: I don't like this sign. I wanted $J^2=-{\rm Id}$ always. Is this fixable? If not, are there deeper reasons?","['complex-geometry', 'vector-bundles', 'almost-complex', 'differential-geometry']"
3219706,Blow up at point is finite?,"Let $X$ be an affine algebraic curve with $0 \in X$ and $\tilde{X}$ the strict transform of $X$ w.r.t the blowup of $X$ at $0$ . How to prove that $\pi \colon \tilde{X} \to X$ is finite? Is it even correct in general? I'm thinking of something like the blow up of $V(y^2 - x^2 - x^3)$ . If I 
consider $V(y^2 - x^2 - x^3, xs-yt)$ in $\mathbb{A}^2 \times \mathbb{P}^1$ and go to the chart $t=1$ I get the equation $s^2 - 1 - x$ where I easily see, that $s$ in integral over $k[x,y]/(y^2 - x^2 - x^3)$ . On the chart $s=1$ , I can't see it that easily or if I go to a chart where one of the points in the fiber over $0$ is missing I don't get a finite morphism. My idea was to use relationship affine + proper = finite for morphism of schemes like here https://lovelylittlelemmas.rjprojects.net/proper-affine-finite/ . But Im not very familiar with (non)-affine schemes. So it is known that a blowing up is proper. And the closed immersion of the strict transform in the blow up is proper too? Because I can find a chart with all points of the fiber over $0$ , $\pi$ must be affine too? and is then finite. My questions are now: 1) Is this argumentation correct? 2) I actually only need the fact $\mathrm{Spec}(A[x]) \to \mathrm{Spec}(B[x])$ is closed, where $A$ and $B$ are the coordinate rings of (affine) $\tilde{X}$ resp. $X$ ,  which follows from $\tilde{X} \to X$ universally closed. Is there a more elementary way to see this. 3) How can I easily prove that $\pi$ is quasi-finite, i.e. the fiber over $0$ finite? Can I see more easily, that $\pi$ is affine? 4) The whole argumentation shouldn't be true for blow ups at ideals with dimension > 0, otherwise I could finitely desingularize every variety. Would this be because $\pi$ is no longer affine? 5) What if the affine curve is not irreducible, i.e. $X$ not integral. There wouldn't be any problem with properness? I'm asking because Hartshorne Prop. 7.16 is asking for integral schemes to show that blowing up is proper, but in the proof he is referring to a Proposition where this property is not used. The reason I'm interested in a finite blow-up is because I want to calculate the normalization this way. This question is actually a reformulation of this question , where there wasn't any answer. Thank you very much!","['algebraic-curves', 'blowup', 'morphism', 'algebraic-geometry', 'commutative-algebra']"
3219788,Proving that $\iint_{B_2(0)}e^{(x^2+y^2)^2}dA\leq e(1+3e^{15})\pi$,"Show that $$\iint_{B_2(0)}e^{(x^2+y^2)^2}dA\leq e(1+3e^{15})\pi$$ [Hint: If you cannot get the desired estimate directly, try using domain decomposition.] I am having trouble with this problem. I understand that using polar coordinates, $$\iint_D{e^{x^2+y^2}dA}=\pi(e-1)$$ where $D$ is the unit disk of radius one, because it becomes $$\int_0^{2\pi}\int_0^1{re^{r^2}dr \ d\theta}=\pi(e-1)$$ So, in the problem above, does this simply become $$\int_0^{2\pi}\int_0^1{re^{{r^2}^2}dr \ d\theta}=\pi(e-1)$$ $$\Rightarrow\int_0^{2\pi}\int_0^1{re^{r^4}dr \ d\theta}=\pi(e-1)?$$ If not, what is it? How would one then prove the inequality?","['integration', 'multivariable-calculus', 'definite-integrals', 'polar-coordinates']"
3219822,Show that $CD\parallel AB$ with square $FEBD$,"As the figure shows, $FEBD$ is a squre. $AE=GE$ , $FA=FB$ and $CD=BA$ . Show that $CD\parallel AB$ . I have found that $\triangle BGD$ must be an equilateral triangle, but I have no proof yet. Please help. It's better to offer a synthetical solution without much computation. Thanks in advance.",['geometry']
3219842,Iwasawa decomposition for $GL_n(\mathbb{A}_{\mathbb{Q}})$. What does $K$ look like?,"I am trying to understand the Iwasawa decomposition for $GL_n(\mathbb{A}_{\mathbb{Q}})$ and $GL_n(\mathbb{Q})$ , where $\mathbb{A}_{\mathbb{Q}}$ is the adeles. The statement for the case of adeles reads there exists a maximal compact subgroup $K$ of $GL_n(\mathbb{A}_{\mathbb{Q}})$ such that $$
GL_n(\mathbb{A}_{\mathbb{Q}}) = P(\mathbb{A}_{\mathbb{Q}}) K
$$ (Here I am taking $P$ to be the upper triangular matrices).
Could someone please explain to me what does $K$ look like in this case? Also what does $P(\mathbb{Q}) \backslash GL_n(\mathbb{Q})$ look like? I would greatly appreciate if someone could explain me both. Thank you.","['number-theory', 'automorphic-forms', 'adeles', 'algebraic-groups']"
3219882,Proof that $ p^2 \equiv 1 \mod{30} \vee p^2 \equiv 19 \mod{30} $ for $p>5$,Proof that $$ p^2 \equiv 1 \mod{30} \vee p^2 \equiv 19 \mod{30} $$ for $p>5$ and $p$ is prime. $\newcommand{\Mod}[1]{\ (\mathrm{mod}\ #1)}$ My try Let show that $$p^2 - 1 \equiv 0 \Mod{30} \vee p^2 - 1\equiv 18 \Mod{30}$$ Let check $$p^2 -1 = (p-1)(p+1) $$ We know that (for example from here ) that this is dividable by $2$ and by $3$ so by $6$ Let consider $5$ cases: $$\exists_k p=5k \rightarrow \mbox{false because p is prime}$$ $$\exists_k p=5k+1 \rightarrow p^2 - 1 = 5k(5k+2) \rightarrow \mbox{ dividable by 6 and by 5 so we have rest 0}$$ $$\exists_k p=5k+2 \rightarrow p^2 - 1 = (5k+1)(5k+3) \rightarrow \mbox{ ??? }$$ $$\exists_k p=5k+3 \rightarrow p^2 - 1 = (5k+2)(5k+4) \rightarrow \mbox{ ??? }$$ $$\exists_k p=5k+4 \rightarrow p^2 - 1 = (5k+3)(5k+5) \rightarrow \mbox{ dividable by 6 and by 5 so we have rest 0}$$ I have stucked with $???$ cases...,"['number-theory', 'modular-arithmetic', 'discrete-mathematics']"
3219885,Analysis and Calculus [closed],"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 5 years ago . Improve this question Is there a good book that combines Calculus I, II, III, Analysis I, II with the routine calculation exercises and that also shows rigorous proofs of the theorems? I have looked at Vladimir Zorich's Mathematical Analysis I but does not have many routine questions in it. So what are your suggestions? So the idea is I want to study Calculus with Analysis together.","['calculus', 'analysis', 'reference-request']"
3219888,Different methods of solving a linear system of first order DE?,"$$ x'(t)=\begin{pmatrix} 1&\frac{-2}3\\3&4\end{pmatrix}x(t).$$ This is the method described in my book: finding the eigenvalues of matrix $A  = \begin{pmatrix} 1&\frac{-2}3\\3&4\end{pmatrix}$ : $$ \begin{vmatrix} 1-\lambda&\frac{-2}3 \\3&4-\lambda\end{vmatrix}=0\iff \lambda=3, \lambda=2.$$ eigenvector corresponding to $\lambda=3$ : $\begin{pmatrix} -1/3 \\ 1\end{pmatrix}$ , eigenvector corresponding to $\lambda=2$ : $\begin{pmatrix} -2/3 \\ 1\end{pmatrix}$ . define $C = \begin{pmatrix} -2/3 & -1/3 \\ 1&1\end{pmatrix}$ , then $C^{-1} = \begin{pmatrix} -3 & 3 \\ -1&2\end{pmatrix}$ , and $$ A = C\operatorname{diag}(\lambda_1,\lambda_2)C^{-1} = \begin{pmatrix} -2/3 & -1/3 \\ 1&1\end{pmatrix} \begin{pmatrix} 2&0\\0&3\end{pmatrix}\begin{pmatrix} -3 & 3 \\ -1&2\end{pmatrix}.$$ calculate $e^{tA} $ . Using previous expression for $A$ we get $$ e^{tA}=\begin{pmatrix} -2/3 & -1/3 \\ 1&1\end{pmatrix} \begin{pmatrix} e^{2t}&0\\0&e^{3t}\end{pmatrix}\begin{pmatrix} -3 & 3 \\ -1&2\end{pmatrix} = \begin{pmatrix} 2e^{2t}+\frac13e^{3t} & -2e^{2t}-\frac23e^{3t} \\ -3e^{2t}-e^{3t} & 3e^{2t}+2e^{3t}\end{pmatrix}.$$ the solution of the given system, considering the initial condition $x_0=(x_1,x_2)^t$ , equals to $e^{tA}x_0$ : $$ e^{tA}x_0 = \dots = x_1\begin{pmatrix}2e^{2t}+\frac13e^{3t} \\-3e^{2t}-e^{3t} \end{pmatrix}+x_2\begin{pmatrix} -2e^{2t}-\frac23e^{3t} \\3e^{2t}+2e^{3t}\end{pmatrix}$$ Now, I have looked up some extra exercises online, but these seem to solve such systems in a shorter way: the solution of the system above with be given by $c_1e^{2t}\begin{pmatrix} -2/3 \\ 1 \end{pmatrix} + c_2e^{3t}\begin{pmatrix} -1/3 \\ 1\end{pmatrix}$ . What is the difference between both approaches? The method used by my book seems to have more coefficients (and is therefore maybe a little more detailed/exact?). Are these solution methods equivalent? Which one would you use? Thanks.","['systems-of-equations', 'proof-verification', 'ordinary-differential-equations']"
