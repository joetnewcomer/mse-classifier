question_id,title,body,tags
2198754,Prove that $\lim_{n \rightarrow \infty} \int_0^\infty f_n(x) dx = \int_0^\infty f(x) dx$.,"This problem is from Chapter 7 of Rudin's Principles of Mathematical Analysis . Suppose $g$ and $f_n$ ($n = 1,2,3,...)$ are defined on $(0,\infty)$, are Riemann-integrable on $[t,T]$ whenever $0 < t < T < \infty$, $|f_n| \leq g, f_n \to f$ uniformly on every compact subset of $(0,\infty)$, and $$\int_{0}^{\infty}g(x) dx < \infty.$$ Prove that $$\lim_{n\rightarrow \infty} \int_{0}^\infty f_n(x) dx = \int_{0}^{\infty} f(x) dx.$$ My attempt at the solution: Since $|f_n| \leq g$ and $\int_0^\infty g(x) dx < \infty$, it follows that $\int_0^\infty f_n(x) dx < \infty$ (and also $\int_0^\infty f(x) dx < \infty$ by uniform convergence). Also,
$$
\lim_{n \to \infty} \int_{0}^\infty f_n(t) dt = \lim_{n \to \infty} \lim_{T \to \infty} \int_{0}^T f_n(t) dt.
$$
If we could interchange the two limits $\lim_{n \to \infty} \lim_{T \to \infty}$, then we would have our result due to the fact that $f_n \in \mathscr{R}$ and $f_n \to f$ (Theorem 7.16). My question: What is the formal justification for being able to interchange the two limits? I have had a few ideas, but none of them have seemed any good.","['real-analysis', 'uniform-convergence', 'analysis']"
2198780,Construction of the cotangent bundle involves a non-canonical choice(?),"In the book that I am reading (Introduction to Differential Manifolds by Barden and Thomas), they note that, for each chart $(U,\phi)$ in the atlas of some smooth $m$-manifold and each $p \in U,$ we have a bijection $\phi_*(p):T_p(M)\to \mathbb{R}^m$ given by $$\phi_*(p):[\gamma]_p \mapsto (\phi\circ\gamma)'(0).$$ Transporting the linear structure from $\mathbb{R}^m$ onto $T_p(M)$ makes each $\phi_*(p)$ into a linear isomorphism. They then say that, by taking the dual map of $(\phi_*(p))^{-1}\!,$ we have, for each $p \in U,$ a linear isomorphism $T_p^*(M)\to(\mathbb{R}^m)^*$ and so taking these maps all together we obtain a bijection
$$\phi^*:T^*(U)=\coprod_{p \in U} T_p^*(M) \to \phi(U)\times (\mathbb{R}^m)^*$$
The next step is to view $\phi(U)\times (\mathbb{R}^m)^*$ as an open subspace of $\mathbb{R}^{2m}$ and then transport the topological structure of this onto $T^*(U)$ via the map $\phi^*.$ BUT... Surely this last step surely requires us to choose an identification of $(\mathbb{R}^m)^*$ with $\mathbb{R}^m$ and so, since this cannot be done canonically, I'm a bit confused... Any explanations or advice is very welcome. Thanks!","['tangent-bundle', 'smooth-manifolds', 'differential-geometry']"
2198797,$L^{\infty}$ is a Banach Space,"Show that $L^{\infty}$ is a Banach Space with respect to the norm $||.||_{\infty}$ where $||f||_{\infty}=\inf\{ a\ge 0: \mu\left(\{x: |f(x)| \gt a\}\right)=0\}$ I am going to prove the following lemmas and then use them in the proof. Lemma-1: $||.||_{\infty}$ is a norm on $L^{\infty}$ Proof: Suppose that $||f||_{\infty}=0$.  Let $A=\{x \in X : f(x) \ne 0\} $ and $A_n=\{x \in X: |f(x)| \gt \frac{1}{n}\}$ . Then $A=\cup_{n}A_n$. Then there exists $'a'$ such that $\mu\left(\{x: |f(x)| \gt a\}\right)=0$ and $a \lt \frac{1}{n}$ (by the definition of $||f||_{\infty}$). Thus $$\mu\left(\{x: |f(x)| \gt \frac{1}{n}\}\right) \le \mu\left(\{x: |f(x)| \gt a\}\right)=0$$ $\implies \mu(A_n)=0$ and hence $\mu(A)=0$. Thus $f =0 ,\mu$-a.e. Now $$|f(x)+g(x)| \le |f(x)| +|g(x)| \le ||f||_{\infty}|+||g||_{\infty} ,\mu-\text{a.e}$$ Thus $||f+g||_{\infty} \le ||f||_{\infty}+||g||_{\infty}$ Now $$||\lambda f||_{\infty}=\inf\{a \ge 0: \mu\left(\{x: |\lambda f(x)| \gt a\}\right)=0\}$$
$$=\inf\{|\lambda|a \ge 0:\mu\left(\{x: |\lambda||f(x)| \gt |\lambda|a\}\right)=0\}$$
$$=|\lambda|\inf\{a \ge 0:\mu\left(\{x: |f(x)| \gt a\}\right)=0\}=|\lambda|||f||_{\infty}$$ Lemma-2: $||f_n-f||_{\infty} \to 0$ iff there exists $E \in \mathcal{M}$ such that $\mu(E^c)=0$ and $f_n \to f$ uniformly on $E$. Proof: $(\implies)$ For every $k \in \mathbb{N}$, there exists $n_0(k) \in \mathbb{N}$ such that for all $n \ge n_0(k)$, $||f_n-f||_{\infty} \lt \dfrac{1}{k}$. Then for each $n \ge n_0(k)$ there exists $E_n^{k}$ such that $\mu(E_n^k)=0$ and $|f_n(x)-f(x)| \lt \dfrac{1}{k}$ for all $x \in (E_n^k)^{c}$.  Let $$E^k=\cup_{n \ge n_0(k)} E_n^k$$. Then $\mu(E^k)=0$ and for all $x \not \in E^k, |f_n(x)-f(x)| \lt \frac{1}{k}, \forall n \ge n_0(k)$.
Now Let $E=\cup_{k=1}^{\infty} E^k$. Then $\mu(E)=0$. Let $\epsilon \gt 0$. Then there exists a $k_0 \in \mathbb{N}$ such that $\frac{1}{k} \lt \epsilon$ for all $k \ge k_0$. Let $x \in E^c$. In Particular $x \not \in E^{k_0}$ . Then for all $n \ge n_0(k_0), |f_n(x)-f(x)| \lt \frac{1}{k_0} \lt \epsilon$ ($\impliedby$) Let $\epsilon \gt 0$. Then there exists $n_0 \in \mathbb{N}$ such that for all $n \ge n_0, |f_n(x)-f(x)| \lt \epsilon, x\in E$. Then for all $n \ge n_0$, $$\left(\{x: |f_n(x)-f(x)| \gt \epsilon\}\right) \subset E^c$$
$$\implies \mu\left(\{x: |f_n(x)-f(x)| \gt \epsilon\}\right)=0  $$ which in turn gives us that for all $n \ge n_0$, we have $$||f_n-f||_{\infty} \lt \epsilon$$. (Proof that $L^{\infty}$ is a Banach Space) : Let $\{f_n\}_{n \in \mathbb{N}} \in L^{\infty}$ be  a cauchy sequence. Then for $\epsilon \gt 0$, there exists a $n_0(\epsilon) \in \mathbb{N}$ such that for all $n,m \ge n_0(\epsilon)$, we have $||f_n-f_m||_{\infty} \lt \dfrac{\epsilon}{2}$. Then there exists $E^{\epsilon} \in \mathcal{M}$ such that $\mu(E^{\epsilon})=0$ and for $x \in (E^{\epsilon})^{c} ,|f_n(x)-f_m(x)| \lt \dfrac{\epsilon}{2}$. Let $$E= \cup_n E^{\frac{1}{n}}.$$ Then $\mu(E)=0$ and for all $x \in E^c, \{f_n(x)\}$ is a  cauchy sequence. Let $f(x)=\lim_n f_n(x)$ for $x \in E^c$. Let $\epsilon \gt 0$. Then there exists $k_0 \in \mathbb{N}$ such that for all $k \ge k_0,\frac{1}{k} \lt \frac{\epsilon}{2}$. Then for all $n, m \ge n_0(k_0)$, we have $$|f_n(x)-f_m(x)| \lt \frac{1}{k} \lt \frac{\epsilon}{2}$$
Letting $m \to \infty$ we have $$|f_n(x)-f(x)| \lt \epsilon, x\in E^c.$$ By Lemma-2, we have $||f_n-f||_{\infty} \to 0$. From here by use of triangle inequality, we see that $ f\in L^{\infty}$. Is this proof alright? Thanks for the help!!","['real-analysis', 'functional-analysis', 'lp-spaces', 'measure-theory', 'analysis']"
2198802,What is this O function?,"I came across such a function written by $O$. Can you please tell me what is this? Actually I see this function during proofs or error finding. I am familiar with big-O notation in algorithmic complexity, but I am talking about when it is used in math.
Example:","['proof-writing', 'error-function', 'functions']"
2198810,Why is a Dirac operator involutive only if the curvature and torsion are in the image of the kernel of the symbol under exterior multiplication?,"I am trying to understand the classic paper by Atiyah, Hitchin, and Singer https://www.jstor.org/stable/79638 and I'm getting stuck on part of the proof of proposition 3.1.  The proposition is determining the conditions for involutivity of the vector bundle $V(\bar{D})$ associated with a differential operator $\bar{D}=\sigma\nabla$, spanned by one forms $\theta_i$ and $\sigma_i$.  In particular, it says that $d \theta_i$ and $d \sigma_i^\nu$ are sections of $V_2$ only if the curvature of $\nabla$ and $D_1 \sigma_i^\nu$ are sections of $V(S_2)$.  Here $V_2$ is the image of $V(\bar{D})$ under exterior multiplication, and $V(S_2)$ is the image of the kernel of $\sigma$ under exterior multiplication.  I don't understand why it is possible to conclude that the curvature and $D_1 \sigma_i^\nu$ must be sections of $V(S_2)$ instead of the presumably larger space $V_2$ (spanned by $V(S_2)$ in addition to two forms of the form $\theta_i\wedge \alpha_i$ for any $\alpha_i$), it is not really explained and not obvious to me.","['fiber-bundles', 'differential-geometry', 'manifolds', 'vector-bundles', 'connections']"
2198813,Properties of the Minimum of Two Poisson Random Variables,"I stumbled upon the following problem in my research. We are trying to analyze $Z=\min(X,Y)$ where $X \sim Pois(p\lambda)$ and $Y\sim Pois((1-p)\lambda)$. Note that the RVs expectation is related yet not identical but are independent. What we are most interested in is a closed form expression for $\mathbb{E}Z$. Or, alternatively, an expression simple enough to prove with that the expectation $\mathbb{E}Z$ is attained at $p=\frac{1}{2}$ I managed to find very little literature on the subject. I saw that in some places this scenario is called a ""Poisson Race"", but couldn't find anything that is relevant to me. I tried to go the manual way:
\begin{equation}
\begin{split}
\mathbb{E} Z &  = \sum_{n\geq 1} \Pr(min(X,Y) \geq n) \\
 & =  \sum_{n\geq 1} \Pr(X\geq n\ \text{and}\ Y\geq n) \\
 & = \sum_{n\geq 1} \Pr(X\geq n)\cdot \Pr(Y\geq n) \\
& = \sum_{n\geq 1}\Bigg[\Bigg(\sum_{i\geq n} \frac{(p \lambda)^i e^{-p\lambda}}{i!} \Bigg)\Bigg(\sum_{i\geq n} \frac{((1-p) \lambda)^i e^{-(1-p)\lambda}}{i!} \Bigg)\Bigg] \\
& = e^{-\lambda}\sum_{n\geq 1}\Bigg[\Bigg(\sum_{i\geq n} \frac{(p \lambda)^i}{i!} \Bigg)\Bigg(\sum_{i\geq n} \frac{((1-p) \lambda)^i }{i!} \Bigg)\Bigg] \\
& = e^{-\lambda}\sum_{n\geq 1}\Bigg[\Bigg(e^x-e_{n-1}(p\lambda) \Bigg)\Bigg(e^x - e_{n-1}((1-p)\lambda) \Bigg)\Bigg] \\
\end{split}
\end{equation} But this didn't lead to any relatively simple terms. Tried looking into Gamma Taylor partial sums of $e^x$ and Gamma functions $\Gamma (x)$ but again, with no result. What is obvious, due to the symmetry of the function is that the max is attained at $p=\frac{1}{2}$. Does one see any way to prove so without having to derive once and twice and do all the dirty work? $e_n(x)$ is the Exponential Sum Function","['maxima-minima', 'probability', 'optimization', 'poisson-distribution']"
2198850,Irreducible Components of Higher Order Tensors,"In Richard Hamilton's first paper on the Ricci Flow, there's a proposition (Lemma 11.6; see page 34 of this document https://projecteuclid.org/download/pdf_1/euclid.jdg/1214436922 ) who's proof involves taking the tensor $\nabla Rc$ and splitting it into certain components: $$\nabla_iR_{jk}=E_{ijk}+F_{ijk} $$ The essential point is that $F$ is trace-free with respect to any two indices. So $F$ is to $\nabla Rc$ what the Weyl curvature tensor is to the Riemann curvature tensor. (I imagine Hamilton had something like this analogy in mind when thinking this through.) One can reason in an ad hoc way as follows: Based on the traces of $\nabla Rc$ and the symmetry of the $jk$ indices, one can guess that $$E_{ijk}=a\nabla_iRg_{jk}+b(\nabla_jRg_{ik}+\nabla_kRg_{ij})$$ and solve for the constants $a$ and $b$ using that $F$ should be trace-free. My question is whether there is a well-established procedure or theorem which tells you how any 3-tensor, or even k-tensor, breaks up into these more fundamental components. For example, I'm certainly aware of how this works for a 2-tensor: $$a_{ij}=\textstyle\frac{tr(a)}{n}g_{ij}+ [\textstyle\frac{1}{2}(a_{ij}+a_{ji})-\textstyle\frac{tr(a)}{n}g_{ij}]+\textstyle\frac{1}{2}(a_{ij}-a_{ji})$$ and that these components are irreducible in the sense of the representation theory of the orthogonal group on tensors. (It's my suspicion that the decomposition $E+F$ above is not irreducible in this sense, as I would naively expect more symmetries at the level of indices.)","['representation-theory', 'riemannian-geometry', 'differential-geometry']"
2198858,Is right inverse function unique?,"Let $f\colon A\to B$ be a map. Can there be two different $g, g'\colon B\to A$ such that $fg = id = fg'$? Or is a right inverse always unique?",['elementary-set-theory']
2198893,Vertical harpoon (half-arrow) notation,"I am studying a paper about Subgroups of Infinite Symmetric Groups by Macpherson and Neumann; throughout the paper, the authors use the notation $\upharpoonright$. For example, when they seek to topologize an infinite symmetric group $Sym(\Omega)$, they define the closure of a subset $X$ of $Sym(\Omega)$ as such: $\{f \in Sym(\Omega)  \ |$ for all finite subsets $\Phi$ of $\Omega$ there exists $x \in X$ such that $x\upharpoonright\Phi=f\upharpoonright\Phi\}$ The authors don't define this notation, but they use it in several proofs. What do the authors mean when they use it?","['abstract-algebra', 'topological-groups', 'notation']"
2198901,Is $\prod_{p}{\frac{p-1}{p+1}}=0$?,"Is it true that the 
$$\prod_{p}{\frac{p-1}{p+1}}=0$$ where the product runs over the prime numbers $p$?","['limits', 'number-theory', 'products', 'arithmetic', 'prime-numbers']"
2198967,The minimum value of $x^8 – 8x^6 + 19x^4 – 12x^3 + 14x^2 – 8x + 9$ is,"The minimum value of $$f(x)=x^8 – 8x^6 + 19x^4 – 12x^3 + 14x^2 – 8x + 9$$ is (a)-1 (b)9 (c)6 (d)1 Apart from trying to obtain $1$, which in this case is simple and $f(2)=1$ is there a standard method to approach such problems. Please keep in mind that this is an objective question in one of the competitive exam and you get around 5 mins to solve it. Also this is asked in elementary section, so only knowledge of basic calculus and polynomials is assumed.","['elementary-functions', 'polynomials', 'calculus', 'algebra-precalculus', 'maxima-minima']"
2198997,"If the $n^{th}$ partial sum of a series $\sum a_n$ is $S_n = \frac{n-1}{n+1}$, find $a_n$ and the sum.","If the $n^{th}$ partial sum of a series $\sum a_n$ is $S_n = \dfrac{n-1}{n+1}$, find $a_n$ and the sum. By definition,  the sum of the series is the $\lim n\to\infty$ of it's $n^{th}$ partial sum. $$\text{ Sum = } \lim_{n\to\infty} = \dfrac{n-1}{n+1} = 1$$ I am asked to find $a_n$. How do I do this? What is the procedure?","['convergence-divergence', 'sequences-and-series', 'calculus', 'limits']"
2199013,From the Spectral Theorem in Operator Theory to the Spectral Theorem in Linear Algebra,"I'm still a bit confused about the very general Spectral Theorem in Operator Theory, since it's very abstract. So I thought it might be a good idea to apply the general theorem to the finite-dimensional case. Here's the theorem I got in my notes: Spectral Theorem: Let $H$ be a complex Hilbert-space and $A: H \to H$ a normal operator. There is a unique spectral measure $\Phi$ on the spectrum $\sigma(A)$ such that $$f(A) = \int_{\sigma(A)} f d \Phi$$ for any continuous function $f$ on the spectrum $\sigma(A)$. Moreover $\Phi(U) \neq 0$ for all non-empty open subsets $U \subset \sigma(A)$, and for every bounded operator $B : H \to H$ we have $BA = AB$ if and only if $B \Phi(U)  =  \Phi(U) B$ for all $U$. Let's now assume that $H := \mathbf{C}^n$ and that $A$ is self-adjoint. The spectrum of $A$ is real and finite, hence discrete. I want to show that there exists a orthonormal basis of eigenvectors of $A$. So I figured to take the function $f := \mathbf{1}_{\{\lambda\}}$ for a $\lambda \in \sigma(A)$. We get $f(A) = \Phi(\{\lambda\})$. The spectral theorem then gives me a finite family of pairwise orthogonal projections $\{ \Phi(\{\lambda \})\}_{\lambda \in \sigma(A)}$, such that $\sum_\lambda \Phi(\{\lambda\})= \text{Id}$. But I don't see how this does lead anywhere close to the Spectral Theorem in Linear Algebra. Can anyone help? Thanks!","['functional-analysis', 'linear-algebra', 'operator-theory']"
2199016,Approaching Tricky Combinatorial Proofs - Tough Example,"I'm struggling with finding effective ways to approach questions (especially proofs) that use summations and Taylor Series. I've worked through several simpler examples, but always get stuck once a non-trivial question arises. In particular, I'm hoping to get some help in proving the following statement
$$\sum_{i=0}^{n-1} {2i \choose i} \frac{1}{i+1}\frac{1}{2^{2i}}=2(1-\frac{1}{2^{2i}}{2i \choose i})$$ As a hint, it's stated that $ \frac{1-\sqrt{1-x}}{\frac{1}{2}x}=\sum_{i=0}^{\infty}{2i \choose i}\frac{1}{i+1}\frac{x^{i}}{2^{2i}} $ and that it may potentially be necessary to derive the Taylor seires centered at $x=0$ for $\frac{1-\sqrt{1-x}}{x} $ and $\frac{1}{\sqrt{1-x}} $ With some help, I've done the following work, but am not sure if I'm either headed in the right direction, or even correct: $$\sum_{i=0}^{n-1} {2i \choose i} \frac{1}{i+1}\frac{1}{2^{2i}}=\sum_{i=0}^{\infty}{2i \choose i} \frac{1}{i+1}\frac{1}{2^{2i}}-\sum_{i=n}^{\infty}{2i \choose i} \frac{1}{i+1}\frac{1}{2^{2i}}=2-\sum_{i=n}^{\infty}{2i \choose i} \frac{1}{i+1}\frac{1}{2^{2i}}$$
From here, I've tried to get to the point of proving the following:
$$2-\sum_{i=n}^{\infty}{2i \choose i} \frac{1}{i+1}\frac{1}{2^{2i}}=2-\frac{2}{2^n}{2n \choose n}$$
$$\sum_{i=n}^{\infty}{2i \choose i} \frac{1}{i+1}\frac{1}{2^{2i}}=\frac{2}{2^n}{2n \choose n} $$
I really don't know where to proceed next, or which direction the proof will continue in. Any additions/corrections would be greatly appreciated. Thank you! Edit: to add on another potential solution, would it hold any water to try to write down a recurrence relation, assuming the left side of the first equation to be $ a_n $?","['combinatorics', 'summation']"
2199052,Proving a function is continuous at a point,"I'm given a function $g$ defined $g:[0,1] \to \mathbb{R}$ such that $p \in [0,1]$. Let $q \in \mathbb{R}$ where $$\lim_{x \to p}g(x) = q$$ $h$ is defined such that $h(x) = \begin{cases} 
g(x)  & x \neq p \\
q & x = p
\end{cases} $ I need to show that $h$ is continuous at $p$. What I know : I know that to show $h$ is continuous at $p$ I need that for any $\epsilon > 0$ there is a  $\delta > 0$ such that $d(h(x),h(p)) <\epsilon$ for all $x \in [0,1]$ where $d(x,p) < \delta$. Since $\lim_{x \to p}g(x) = q$ then for any $\epsilon > 0$ there is a  $\delta > 0$ such that $d(g(x),q) < \epsilon$ for all $ x \in [0,1]$ where $d(x,p) < \delta$ Can I say that when $x=p$ clearly the results from 2 satisfy the requirements of being continuous and when $x \neq p$ then as $x \to p$ it satisfies 1? I'm not sure how to tie everything together and if anyone could help me in the right direction I would be very grateful.","['real-analysis', 'convergence-divergence', 'functions', 'limits']"
2199057,Finding the probability density function...,"I've found the probability density function for other functions, but this one seems rather difficult. $$F(x)=P(X\le x)=1-e^{-x^2}$$ My teacher's solution manual is showing: $$
\begin{cases} 0, & x<0 \\ 2xe^{-x}, &  x>0 \end{cases}
$$ Maybe I am just having a hard time finding this integral, but I don't seem to get the same results. If I use u-substitution I am setting $u = x^2$ where $du = 2x$ then, but how does that get into the answer, am I selecting a wrong $u$? Is it integration by parts maybe? Thanks.","['statistics', 'probability']"
2199058,Is $\frac{1}{H(x) \pm i0}$ a distribution if $|\nabla H| \neq 0$ for $H(x)=0$?,"I know that $\frac{1}{x \pm i0}$ is a tempered distribution in $\mathcal{S}'(\mathbb{R})$, see e.g. the Sokhotski–Plemelj theorem. In some lecture notes online I found the following statement (without proof): If $H:\mathbb{R}^n \to \mathbb{R}$ and $|\nabla H| \neq 0$ at any point where $H(x)=0$ then $\frac{1}{H(x) \pm i0}$ is in $\mathcal{S}'(\mathbb{R}^n).$ We can also assume that $H$ ""behaves nice"" and is a $C^\infty$-function, so let's say $H$ is a polynomial, e.g. $H(x_1, \dots, x_n)=x_1^2+ \dots +x_n^2-1.$ So far I could not find any proof for this statement but it should be true since similiar distributions are used in PDE theory. Remark: $\frac{1}{H(x) + i0}$ is defined as $\phi \mapsto \lim_{\varepsilon \to 0}\int_{\mathbb{R}^n}\frac{\phi(x)}{H(x) + i\varepsilon}dx$.","['schwartz-space', 'partial-differential-equations', 'distribution-theory', 'functional-analysis', 'complex-analysis']"
2199061,Applications of Monadicity theorems,"Crossposted to MO . Having carefully read the proof of Beck's monadicity theorems and some related variations, I'm now hungry for cool applications. For instance, I found these blackboard pictures of a lecture by David Jordan, but I do not understand any of the examples - too terse for me. What are some interesting applications of monadicity theorems, especially in geometric contexts (algebraic geometry would be especially cool)? Can anyone spell out the example(s) presented in the linked pictures? Added. I know about monadic descent and am trying to slowly understand it. I am hoping for additional examples.","['category-theory', 'big-list', 'monads', 'algebraic-geometry']"
2199077,Does a summation of two negative Gaussians with different standard deviations have a stationary point?,"I wanted to find the critical points of the (simple!) equation: $$ B(w; \mu_1, \sigma_1, \mu_2, \sigma_2 ) = 1 - e^{ - \beta_1 (w - \mu_1)^2 } - e^{ - \beta_2 (w - \mu_2)^2}$$ Where $\beta_i = \frac{1}{2 \sigma^2_i}$ usually called the precision parameter of the Gaussian or RBF. Anyway, what I wanted in particular to solve is: $$ B'(w; \mu_1, \sigma_1, \mu_2, \sigma_2 ) = - \beta_1 (w-\mu_1) e^{ - \beta_1 (w - \mu_1)^2 } - \beta_2(w - \mu_2)e^{ - \beta_2 (w - \mu_2)^2} = 0$$ looking at the equation above I can't seem to find an easy way to make it zero. To make the first term zero we can choose $w = \mu_1 $ but that doesn't make the second term zero. Similarly, the other way round. However, it is super intuitively obvious that there are 2 minimums and 1 maximum since there are two dips defined by the upside down Gaussian (and in between them some maximum). I plotted this in mathematica to confirm that there must be these critical points and it seems I am correct: however surprisingly I cannot find an easy way to express the critical points analytically. My initial goal is to at least have a way to express these 3 critical points for this 1 dimensional example if ever want to hope to find the critical points for higher dimensions. Anyone knows how to do this? My real intention is to be able to programmatically/systematically get the x-axis value between the two minimums that has exactly gradient equal to zero. Ideally able to generalize in higher dimensions. For 1D and 2D I can sort of eye ball what the value should be by plotting things and then asking mathematics to tell me which value of $x$ leads to $\nabla f'(x) = 0$.","['nonlinear-optimization', 'optimization', 'normal-distribution', 'calculus', 'multivariable-calculus']"
2199129,Surface area of paraboloid $z=x^2+y^2$ and below $z=9$,"I have attempted to convert to cylindrical coordinates and have gotten to the equation r*sqrt(4r^2+1) but I am unsure of how to set up the triple integral limits from here. If this is correct so far, how do you find the limits for each integral, if not then how would you go about solving it? Thanks. Also, I know the answer is pi/6(37^(3/2)*-1) which is about 117.319","['multivariable-calculus', 'calculus']"
2199156,Perfect polish space $G_\delta$ partition.,"I'm trying to prove that If $\left(X,d\right)$ is a perfect polish space and $\epsilon>0$, then there exists $A_1, A_2,\dots$ perfect and nonempty $G_\delta$-sets, such that $A_i\cap A_j=\emptyset$ if $i\not=j$, $\operatorname{diam}\left(A_i\right)<\varepsilon$ and $\bigcup_{i\in \omega} A_i=X$. I've tried taking a numerable dense subset of $X$ and playing with the balls of radius 1 with center the points of the dense.","['descriptive-set-theory', 'general-topology', 'metric-spaces', 'elementary-set-theory']"
2199157,"Prove that the second derivative are symmetric for this piecewise function at (0,0)","Based of Wikipedia -> Requirement of continuity I'm trying to figure out why the second derivative of the function is not symmetric. The function is: $$
f(x,y) = \begin{cases}
                     \frac{xy(x^2 - y^2)}{x^2+y^2} & \mbox{ for } (x, y) \ne (0, 0)\\
                      0                            & \mbox{ for } (x, y) = (0, 0).
                \end{cases}
$$ (1) I want to be sure that i'm right doing the the first derivative at (0,0)
$$
{\partial _xf(0,0) = } \lim_{t \to 0} \frac{f(0+t,0) - f(0,0)}{t} = 0
$$ (2) I don't understand how they obtain the second derivative, they say the second partial derivatives are not continuous at (0,0), and the symmetry fails. Can you explain how to obtain the second derivatives  $\partial _x \partial _yf$  and   $\partial _y \partial _xf$ at (0,0) and why the symmetry fails. Thank you in advance.",['multivariable-calculus']
2199163,"How find the $x$ such that $\{|AB|,|BC|,|CD|,|DA|,|AC|,|BD|\}=\{\sin x,\cos x,\tan x,\cot x,\sec x,\csc x\}$","Find $x$, such there exist four points $A$, $B$, $C$, $D$ in the plane, such that
  $$\{|AB|,|BC|,|CD|,|DA|,|AC|,|BD|\}=\{\sin x,\cos x,\tan x,\cot x,\sec x,\csc x \}$$ We have solved one case: There can't exist $x$ such that $$|AB|=\sin x, |BC|=\cos x, |CD|=\tan x,|DA|=\cot x,|AC|=\sec x,|BD|=\csc x$$ But there are $6!-1$ other cases. I think there must be a simple method for handling them all.","['trigonometry', 'geometry']"
2199172,does a rectangular matrix have an inverse?,"I know all square matrices have easily to identify inverses, but does that continue on with rectangular matrices?",['linear-algebra']
2199175,"Is $f$ differentiable at $(0, 0)$?","I asked a question earlier relating to the equation: Let $f : \mathbb{R^2} \rightarrow \mathbb{R}$ be given by
$$f(x, y) = 
\cases{
    \frac{xy}{x+y}& if $x+y\neq0$  \\
    0 & if $x+y=0$}$$ I have another question relating to this equation which goes as follows:
Is $f$ differentiable at $(0, 0)$? Im really not sure how to find if its differentiable or not so any help will be appreciated","['derivatives', 'real-analysis', 'analysis']"
2199236,Is there something fundamental that makes integrals nontrivial to solve? [duplicate],"This question already has answers here : Why is integration so much harder than differentiation? (6 answers) Closed 7 years ago . First of all, I apologize if this isn't the right forum. The thing with derivatives is that once you learn product/quotient/chain rule and the formulas for trig/exponential/logarithmic functions, you can take the derivative of any function (as far as I can tell). With integrals, the methods to solving are much more complex, with u-subs, integration by parts, partial fraction decomposition, etc. And then you have equations like $\int \sqrt{\sin x\cos x}dx$ which are simple to write but impossible to solve in terms of elementary functions . Does this have something to do with the fundamental theorem of calculus, and how the integral is defined as the area under a curve, or something else? I'm only in Calc II right now, so it's possible that I will learn more about why this is later on in my math ""schedule"".","['definite-integrals', 'calculus']"
2199241,Prove $\sum_{k=0}^{58}\binom{2017+k}{58-k}\binom{2075-k}{k}=\sum_{k=0}^{29}\binom{4091-2k}{58-2k}$,"Show that : $$\sum_{k=0}^{58}\binom{2017+k}{58-k}\binom{2075-k}{k}=\sum_{k=0}^{29}\binom{4091-2k}{58-2k}$$ It seems interesting, but how to prove?","['combinatorics', 'binomial-coefficients']"
2199282,Parametric integral over a circle [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Let us live in $\mathbb{R^2}$ , $\vec{e_1} = \vec{(1,0)}, \vec{e_2} = \vec{(0,1)}$ are two standard vectors. Evaluate $\Large{\int_{C_1} \frac{y\vec{e_1}-x\vec{e_2}}{x^2+y^2} d\vec{r}}$ where $C_1$ is a circle of radius $3$ centered at $(0,0)$ I am completely stuck.","['multivariable-calculus', 'line-integrals']"
2199289,How the quotient group of following matrix group looks like?,"Consider the following subsets of the group of $2\times 2$ non-singular matrices over $\mathbb{R}$: 
  $G=\left\{ \begin{bmatrix}a&b\\ 0&d\end{bmatrix}: a,b,d\in \mathbb{R}, ad=1\right\}$ $H=\left\{ \begin{bmatrix}1&b\\ 0&1\end{bmatrix}: b\in \mathbb{R}\right\}$ Which of the following statements are correct? $G$ forms a group under matrix multiplication. $H$ is a normal subgroup of $G$. The quotient group $G/H$ is well defined and is abelian. The quotient group $G/H$ is well defined and is isomorphic to the group of 2x2 diagonal matrices (over reals) with determinant 1. So I checked that option 1 is true. Also for all $g\in G$, $g^{-1}Hg=H$ so $H$ is a normal subgroup of $G$. Option 2 is also correct. But I am stuck with option 3 and 4. Since $H$ is normal, we can talk about the quotient group $G/H$, but I don't know how this quotient group is look like ? and also the well define part. Also I need help with the 4th option. How can I do this? Any help would be great. thanks.","['matrices', 'normal-subgroups', 'group-theory', 'quotient-group']"
2199314,Proving independence of 2 random variables from a 4-variable joint distribution,"A book I'm reading has the following problem. Even looking at the author's solution, I can't make heads or tails of it - it seems wrong to me, unless I'm missing something. Problem The joint probability $\Pr(w,x,y,z)$ over four variables factorizes as
$$
\Pr(w,x,y,z)=\Pr(w)\Pr(z|y)\Pr(y|x,w)\Pr(x).
$$
Demonstrate that $x$ is independent of $w$ by showing that $\Pr(x,w)=\Pr(x)\Pr(w)$. Author's Solution We compute the distribution $\Pr(x,w)$ by marginalizing the joint distribution $\Pr(w,x,y,z)$ with respect to the unwanted variables $y$ and $z$: $$
\begin{align}
\Pr(x,w)&=\int\int\Pr(w,x,y,z)\,dy\,dz\\
&=\int\int\Pr(w)\Pr(z|y)\Pr(y|x,w)\Pr(x)\,dy\,dz\\
&=\Pr(x)\Pr(w)
\end{align}
$$ where in the third line we have simply integrated over the unwanted variables $y$ and $z$, removing them from the equations. My Objection I'm obviously okay until the second line. I understand that $\Pr(x)$ and $\Pr(w)$ can be moved outside the integrals because they are not functions of $y$ and/or $z$. We're left with: $$
\begin{align}
\Pr(x,w)=\Pr(x)\Pr(w)\int\int\Pr(z|y)\Pr(y|x,w)\,dy\,dz\\
\end{align}
$$ But in general, when we evaluate that integral we get a function of $x$ and $w$, because they're both present in the second factor. Why is this function necessarily equal to $1$?","['statistics', 'probability', 'probability-distributions']"
2199325,Determine the acceleration given the position function of a particle,"The position function of a particle in a test laboratory is $s(t) = \frac{10t}
{t^2+3}$.
  Determine the acceleration of the particle of particle after 4 seconds. The speed $v(t)$ of the particle is the derivative of the position $\frac{ds(t)}{dt}$. I got $v(t) = \frac{10(-t^2+3)}{(t^2+3)^2}$ For the acceleration $a(t)$ I tried taking the derivative of $v(t)$ but got the wrong answer at $a(4)$. $$a(t)=\frac{-2t\cdot (t^3+3)^2-10(-t^2+3)\cdot 2(t^2+3)\cdot(2t)}{(t^2+3)^4}$$","['derivatives', 'calculus']"
2199329,Asymptotic expansion of a given integral,"I need to find the first three terms in the asymptotic expansion of given integral $$\int_0^1ln(1+t)e^{ixsin^2t}dt$$ as $x$ tends to infinity. Using steepest descent method, we deform contour C:$0\lt t \le 1$ into contours along which $\Im A(t)$ is constant, where $A(t)= i \sin^2t$. To find constant contour along $t=0$, set $\sin t=u+iv$, $i \sin^2t=-2uv+i(u^2-v^2)$. Thus $\Im A(t)=u^2-v^2$. At $t=0$, $\Im A(t) =0$. Thus, at $t=0$, $u=v$. $\Re A(t)=-2v^2$. Hence, $e^{ix \sin^2t}=e^{-2xv^2}$. I think I'm in the wrong way, can anybody help me to solve this.","['complex-analysis', 'asymptotics', 'calculus']"
2199367,Is there a quick way of finding the coefficients in an expression like $(ax^3+bx^2+cx+d)^3$?,"We can raise a sum to the power of $n$ quickly and easily using Pascal's triangle, due to the binomial theorem: $$(a+b)^n = \sum_{i=0}^n {n \choose i} a^i b^i$$ For sums of more than one term, we can still do this kind of thing, using multinomial coefficients. For example: $$(a+b+c)^n = \sum_{i,j,k \in \mathbb{N}}{n \choose i,j,k} a^i b^j c^k,$$ where the multinomial coefficient is assumed to be $0$ if $n \neq i+j+k$. But there's a related problem that I don't know how to do quickly. Suppose we wish to raise a univariate polynomial to the power of $n$. For example, suppose we're trying to find $$(ax^2+bx+c)^2.$$ It would be nice to have a quick way of doing this. There's a slow way, of course: using the multinomial theorem and collecting like terms, we can show that this is $$a^2 x^4+2ab x^3+(2ac+b^2)x^2+2bc x+c^2.$$ This formula is quite useful for pen-and-paper/mental arithmetic. In particular, suppose we're trying to square a three digit number, like $431$. Let $x=10$. Then: $$431^2 = (4x^2+3x+1)^2 = 16 x^4+24x^3+17x^2+6x+1$$ $$= x^5+8x^4+5x^3+7x^2+6x+1 = 185761,$$ which the calculator confirms the correctness of. Anyway, suppose I wish to cube a four digit number, or something like that, it would be nice to have a way of writing down the formula for $$(ax^3+bx^2+cx+d)^3$$ that's quicker than using the multinomial formula and then carefully collecting like terms. Question. Is there a quick way of finding the coefficients in an expression like $(ax^3+bx^2+cx+d)^3$?","['binomial-theorem', 'binomial-coefficients', 'polynomials', 'mental-arithmetic', 'algebra-precalculus']"
2199374,Conjugate classes for 2x2 matrices,"I've read an unproved claim that every real 2 by 2 matrix is similar to a matrix in exactly one of the following categories: $\alpha \begin{bmatrix} 1 & 0 \\ 0 & 1\end{bmatrix}$ $\alpha \begin{bmatrix} 1 & 0 \\ 0 & 1\end{bmatrix} + \beta \begin{bmatrix} 0 & 1 \\ 1 & 0\end{bmatrix}$ $\alpha \begin{bmatrix} 1 & 0 \\ 0 & 1\end{bmatrix} + \beta \begin{bmatrix} 0 & -1 \\ 1 & 0\end{bmatrix}$ $\alpha \begin{bmatrix} 1 & 0 \\ 0 & 1\end{bmatrix} + \beta \begin{bmatrix} 0 & 1 \\ 0 & 0\end{bmatrix}$ where $\alpha, \beta \in \mathbb{R}$ and $\beta \neq 0$. What is a general procedure for finding the appropriate class and the appropriate values of $\alpha$ and $\beta$ for an arbitrary real 2 by 2 matrix? I presume that one starts by computing the Jordan normal form of the matrix, which looks like $P\begin{bmatrix} \lambda_1 & 0 \\ 0 & \lambda_2\end{bmatrix}P^{-1} \quad \quad $ or $\quad \quad P\begin{bmatrix} \lambda & 1 \\ 0 & \lambda\end{bmatrix}P^{-1}$. In the latter case, it's pretty clear how to express (a conjugate of) the matrix in the 4th form above. In the former case (the diagonalizable case) when $\lambda_1 == \lambda_2$, it's even easier to express the matrix in the 1st form above. The hard part (the part I need help with) is how to express a diagonalizable matrix with distinct eigenvalues in either the 2nd or 3rd form above. In addition, I'd like an explanation of how one could generalize this result to larger matrices. Thanks","['matrices', 'linear-algebra', 'lie-algebras']"
2199384,Proving Cauchy when given a sequence,"Let $\left\{x_n\right\}$ be a sequence and $0 < a < 1$. Suppose that for all $n \ge 3$ we have $$
\left\lvert x_n - x_{n-1}\right\rvert \le a\left\lvert x_{n-1} - x_{n-2} \right\rvert.
$$ Prove that $\left\{x_n\right\}$  is Cauchy. I don't even know where to start here. To prove a sequence is cauchy I have to somehow reach the conclusion of $\left\lvert x_m - x_{k}\right\rvert < \varepsilon$ right? How do I even do that with this inequality. I'm completely lost.",['analysis']
2199445,Solving Linear Congruences.,"Question is -: Solve the linear congruence $3x  \equiv  4\left(mod\, \, \, \, 7\right)$, and find the smallest positive integer that is a solution of this congruence My Approach-: $3x  \equiv  4\left(mod\, \, \, \, 7\right)$ $\Rightarrow x  \equiv  3^{-1}\, \,4\left(mod\, \, \, \, 7\right)$ $3^{-1}$ means it is the multiplicative inverse of $3\, \,mod\, \,7$ multiplicative inverse of $3\, \,mod\, \,7$ $\Rightarrow 7=3*2+1$ $\Rightarrow 3=1*3+0$ $\Rightarrow 1=1*7+\left(-2\right)3$ thus $-2$  or   $5$ is the inverse. Thus i am getting $\Rightarrow x  \equiv  3^{-1}\, \,4\left(mod\, \, \, \, 7\right)$ $\Rightarrow x  \equiv 20\left(mod\, \, \, \, 7\right)$ But in the solution they are multiplying the inverse $5$ to both sides and get equation as-: $15 \,x \equiv20 \, \left(mod\,\,7\right) $ and then $x \equiv 15x\,\equiv\,20\,\equiv\,\,6\,\left(mod\,\,7\right)$ The solution is given here Please help me out ,where i am wrong! thanks!","['number-theory', 'elementary-number-theory']"
2199453,What's the limit of a square root function?,"$$\lim_{x \to \sqrt{3}^{-}} \sqrt{x^2-3}$$ What's the answer of this limit?
There are two hyppothesis: $0$ and undefined. Undefined Because the square root of a negative number is undefined, $0$ because if we plug $\sqrt{3}$, we obtain $0$; I am not so sure. Please help Edit: Why do we calculate this limit? According to my teacher, a function limit at sqrt(3) exists if limit at sqrt(3)- and limit at sqrt(3)+ both exists and are the same. So that's why we tried to find it . Is that right?",['limits']
2199509,Intuition for bases having the same orientation,"Let $V$ be a finite dimensional real vector space.  I'm trying to develop an intuitive understanding of what it means for two ordered bases $\alpha = (u_1,\ldots,u_n)$ and $\beta = (v_1,\ldots,v_n)$ of $V$ to have the same orientation. A popular definition is that $\alpha$ and $\beta$ have the same orientation if and only if the change of basis matrix from $\alpha$ to $\beta$ has a positive determinant.  (For example, on p. 238 [section 21.1] of An Introduction to Manifolds by Tu, there is a statement, ""We say that two ordered bases are equivalent if the change-of-basis matrix $A$ has positive determinant."") However, this definition does not seem very intuitively clear to me. I can't see directly what this definition means in an intuitive way. Here is an attempt at a more intuitive definition.  Ordered bases $\alpha$ and $\beta$ have the same orientation if and only if there exist continuous functions $w_i:[0,1] \to V$ (for $i = 1,\ldots, n$) such that: The set $\{w_1(t),\ldots,w_n(t)\}$ is linearly independent for all $t \in [0,1]$. $w_i(0) = u_i$ for $i = 1,\ldots, n$. $w_i(1) = v_i$ for $i = 1,\ldots, n$. Intuitively, this definition means that $\alpha$ can be ""continuously deformed"" into $\beta$ without passing through a degenerate configuration.  I like this because it matches my intuition from $\mathbb R^3$: when I try to imagine deforming a right-handed basis into a left-handed basis, I always have to pass through a degenerate configuration.  I can also visualize one and two dimensional examples. I would like to show that this definition is equivalent to the more standard definition that I gave at the beginning.  Here is an attempted proof.  First of all, suppose that $\alpha$ and $\beta$ have the same orientation according to my proposed definition.  Let $R(t)$ be the change of basis matrix from $\alpha$ to the ordered basis $(w_1(t),\ldots, w_n(t))$.  Because $\det R(t)$ is a continuous function that is initially equal to $1$ and is never $0$, it must be true that $\det R(t) > 0$ for all $t \in [0,1]$.  In particular, $\det R(1) > 0$.  This shows that the change of basis matrix from $\alpha$ to $\beta$ has positive determinant. Conversely, I need to show that if $\alpha$ and $\beta$ have the same orientation according to the standard definition (i.e., the change of basis matrix from $\alpha$ to $\beta$ has positive determinant), then $\alpha$ and $\beta$ have the same orientation according to my proposed definition. My question: How do I show this? I've considered some ideas based on using the SVD of the change of basis matrix, or the QR factorization, but I'm not sure they will work.  (Also, I suspect a simpler approach should be available.) Additional question: Please provide any comments that you think might be relevant, or tangentially relevant -- anything that you suspect might improve my understanding of this topic.  Maybe a completely different way of looking at orientation would be more intuitive or enlightening.","['smooth-manifolds', 'linear-algebra']"
2199536,Real representation,"Define a representation $\rho$ of a finite group $G$ over a $\mathbb{C}$-vector space to be real if the space admits a basis for which matrix $\rho(g)$ has real coefficients $\forall g \in G$.
I have to show that for ever $\rho$ it is true that $\rho \otimes \rho^*$ is always real ($\rho^*$ is the dual representation). I think I've got an answer but it's pretty ugly so i would like to know if there is a clever solution to this question.","['finite-groups', 'representation-theory', 'group-theory']"
2199561,Boundedness of $a+\frac 1a$ when iterated,"Here's something I was wondering... Is $$a + \frac 1a$$ for any positive real number $a$ bounded when iterated? For example,, if we start at $a=1$, continuing gives us $a= 1+ \frac 11=2$, then $a=2+\frac 12=2.5$ and so on. A quick program shows that it seems to grow without bound, but how would one prove this mathematically? If it is possible that is... Any hints would be appreciated.",['sequences-and-series']
2199596,Misunderstanding of de Rham Cohomology theorems,"There's a theorem in my book that says the nth de Rham cohomology of a contractible space is zero for positive n.  Another theorem says that if a manifold is orientable, connected, and compact, then the top de Rham comology has rank 1.  I'm asking why these two theorems don't contradict each other.  For example, the nth unit ball satisfies the conditions of both theorems, so it has top de Rham cohomology group equal to both 1 and 0.  Thanks for helping me clear up the misunderstanding.","['algebraic-topology', 'differential-geometry', 'differential-topology']"
2199661,What is an algebraic map?,"From Milne's Algebraic Varieties over Complex Numbers note: COROLLARY 15.6. Any holomorphic map from one projective algebraic
  variety to a second projective algebraic variety is algebraic . PROOF. Let $\varphi:V\rightarrow W$ be the map. Then the graph
  $\Gamma_{\varphi}$ of $V$ is a closed subset of $V\times W$, and hence
  is algebraic according to the theorem [Chow's Theorem]. Since
  $\varphi$ is the composite of the isomorphism $V\rightarrow \Gamma_{\varphi}$ with the projection $\Gamma_{\varphi}\rightarrow W$
  and both are algebraic , $\varphi$ itself is algebraic . I found the following definition from planetmath A map $f:X\to Y$ between quasi-affine varieties $X\subset k^n,Y\subset k^m$ over a field $k$ is called algebraic if there is a map $f':k^n\to k^m$ whose component functions are polynomials, such that $f'$ restricts to $f$ on $X.$ Alternatively, $f$ is algebraic if the pullback map $f^*:C(Y)\to C(X)$ maps the coordinate ring of $Y$, $k[Y],$ to the coordinate ring of $X$, $k[X]$. But it didn't make much sense in the above context. 
What is the definition of an algebraic map in the context of this corollary?","['complex-geometry', 'algebraic-geometry']"
2199662,Maximization of sum of squared Frobenius norms,"Let $\{X_1,\dots,X_K\}$ is a set of random matrices, where $X_k\in\mathbb{R}^{M\times N}, k=1,\dots,K$, and $U\in\mathbb{R}^{M\times r}$ and $V\in\mathbb{R}^{N\times r}$ are two matrices containing orthogonal columns (i.e.,  $U^\top U =I, V^\top V =I$). I was wondering, if the following question has a analytical solution: $$\displaystyle\max_{U,V} \sum_{k=1}^K \|U^\top X_k V\|_F^2$$ If not, how should I solve it? Alternating optimization? (At first, I thought it may be related to the SVD of the sum of the matrices $\{X_k\}$, but so far I have no hint to prove it.)","['matrices', 'matrix-calculus', 'optimization', 'linear-algebra']"
2199702,Why isn't $f(z)=\bar{z}$ complex differentiable,It is quite easy to see why $f(z)=\bar{z}$ isn't complex differentiable. $\frac{\partial{u}}{\partial{x}}=1\neq-1=\frac{\partial{v}}{\partial{y}}$ But I struggle to see why this is the case. Visualization of the conjugate function in my head is simply flipping the complex plane upside down. I don't see how this creates any discontinuities. Am I having the wrong visualization or have I misunderstood complex differentiability?,"['derivatives', 'complex-analysis', 'cauchy-riemann-equations', 'partial-differential-equations']"
2199717,Quick and easy way to show whether a matrix is injective / surjective?,"I'm afraid there could be a task like that in my exam. What way would you recommend me if there was a quadratic matrix given, such as $A= \begin{pmatrix}
2 & 0 & 4\\ 
0 & 3 & 0\\ 
1 & 7 & 2
\end{pmatrix}$? There are several (for me confusing) ways doing it I think. The one we had in our readings is to check if the column vectors are linearly independent (or something like that :S). But I think this would only tell us whether the linear mapping is injective. But I think there is another, faster way with rank? I hope you can explain with this example?","['matrices', 'abstract-algebra', 'linear-algebra']"
2199752,Proving $\int_{0}^{\infty}\sin\left({1\over 4x^2}\right)\ln x\cdot{\mathrm dx\over x^2}=-\sqrt{\pi\over 2}\cdot\left({\pi-2\gamma \over 4}\right)$,"Consider these two similar integrals $$\int_{0}^{\infty}\sin\left({1\over 4x^2}\right)\ln x\cdot{\mathrm dx\over x^2}=-\sqrt{\pi\over 2}\cdot\left({\pi-2\gamma \over 4}\right)\tag1$$ $$\int_{0}^{\infty}\sin\left({1\over 4x^2}\right)\ln^2x\cdot{\mathrm dx\over x^2}=\sqrt{\pi\over 2}\cdot\left({\pi-2\gamma \over 4}\right)^2\tag2$$ How does one prove $(1)$ and $(2)$ ? An attempt: Using the $\sin x$ series, and let $u=\ln x$ , then $(1)$ becomes $$\sum_{n=0}^{\infty}{(-1)^n\over 2^{2n+1}(2n+1)!}\int_{-\infty}^{\infty}ue^{-2(n+1)u}\mathrm du\tag3$$ Integral $(3)$ diverges. Another attempt: $u={1\over 4x^2}$ then $(1)$ becomes $$-{1\over 8}\int_{0}^{\infty}\sin u\ln{(4u)}\cdot{\mathrm du\over u^{3/2}}\tag4$$ Using $\ln u$ series, then $(4)$ becomes $$-{1\over 4}\sum_{n=0}^{\infty}{1\over 2n+1}\int_{0}^{\infty}\left({4u-1\over 4u+1}\right)^{2n+1}\cdot{\sin u\over u^{3/2}}\mathrm du\tag5$$ Using $\coth^{-1} x={1\over 2}\ln{x-1\over x+1}$ $$-{1\over 4}\sum_{n=0}^{\infty}{1\over 2n+1}\int_{0}^{\infty}e^{2(2n+1)\coth^{-1}4x}\cdot{\sin u\over u^{3/2}}\mathrm du\tag6$$ How else can we proceed?","['integration', 'definite-integrals', 'calculus']"
2199758,"$Pr[\frac{1}{n}(X_1+\dots+X_n)\geq c]$ for $c\geq E[X_i]$ if $X_1,\dots,X_n$ are identically distributed but not independent?","Consider the sum of $n$ identically distributed, but not necessarily independent random variables $X_1\dots X_n$; I am trying to prove some concentration bounds on the sum based on concentration bounds for the individual $X_i$ (with concentration bounds, I mean upper bounds on the probability of exceeding some threshold at least as large as the expectation ). Now, since I do not have independence, I can’t apply anything like a Chernoff-Hoeffding bound; but I had naively hoped, since intuitively the worst case is that all the $X_i$ are perfectly correlated, that I could prove for $c\geq E[X_1]$ something like $Pr[\frac{1}{n}(X_1+\dots+X_n)\geq  c] \leq Pr[X_1\geq c]$. While this is true for perfectly correlated variables, it is not true in general: e.g. assuming $X_1$ and $X_2$ are independent $0-1$  variables with expectation $0.1$ the probability that $\frac{1}{2}(X_1+X_2) > 0.3$ is $1-(0.9)^2=0.19$, almost twice the probability that $X_1 \geq 0.3$, which is $0.1$ (note how I’ve been conservative with the inequality signs). I briefly wondered whether this might due to the discrete nature of the variables in this example (so the inequality $Pr[\frac{1}{n}(X_1+\dots+X_n)\geq c] \leq Pr[X_1\geq c]$ would hold for $c$ equal to some value that $X_1$ can actually take, rather than an ""intermediate"" value), but again, this does not seem to be the case. I’d be surprised if nobody had faced a similar problem before, so I’m asking: Are there any known (non-trivial) upper bounds on the probability that the average of identically distributed dependent random variables exceeds a certain threshold equal to or greater than the expectation , given corresponding bounds on the individual variables? Or, more formally, if $X_1,\dots,X_n$ are identically distributed but not necessarily independent random variables, can I find a non-trivial $f(x)\geq Pr[\frac{1}{n}(X_1+\dots+X_n) \geq x]$ given $g(x)\geq Pr[X_1\geq x]$ for $x \geq E[X_1]$?","['probability-theory', 'probability']"
2199766,Probability that at least one of 2 balls taken randomly from a pile of 2 red and 3 black is red,"I get 2 different answers, depending how I approach this, and I need help to see why the error arises. One solution is to calculate unfavorable combinations probability and substract from 1:
$$1-\frac{C_3^2}{C_5^2}=\frac{7}{10}$$
The other solution is when calculating favorable combinations, to first choose one of the 2 reds, and then choose one of the remaining 4:
$$\frac{C_2^1 \times C_4^1}{C_5^2} = \frac{8}{10}$$
Which is obviously $\neq \frac{7}{10}$.","['combinatorics', 'probability']"
2199798,"Normal form of trigonometric ""polynomials""","In the context of trigonometry, we often meet terms like $\sin(\alpha)\cos(\beta/2) + \tan(\beta)\cos(\alpha)$ which are ""polynomials"" in $\sin$, $\cos$, $\tan$, and the parameters are rational multiples of angles (like $\alpha$ or $\beta/2$). I wonder whether there is well-defined notion of ""normal form"" of these expressions, in the sense we can compare any two of these expressions by normalising them in a finite series of steps.",['trigonometry']
2199804,Prove this Generalizing AM-GM inequality,"Let $n\ge 2$ and $a_{i} \ge 0,i=1,2,\cdots,n$, show that
  $$(n-1)^{n-1}(a^n_{1}+a^n_{2}+\cdots+a^n_{n})+n^na_{1}a_{2}\cdots a_{n}\ge (a_{1}+a_{2}+\cdots+a_{n})^n$$ When $n=2$,
$$a^2_{2}+a^2_{2}+4a_{1}a_{2}=(a_{1}+a_{2})^2+2a_{1}a_{2}\ge (a_{1}+a_{2})^2$$ When $n=3$, it is
$$4(a^3_{1}+a^3_{2}+a^3_{3})+27a_{1}a_{2}a_{3}\ge (a_{1}+a_{2}+a_{3})^3$$
By
$$(a_{1}+a_{2}+a_{3})^3=a^3_{1}+a^3_{2}+a^3_{3}+3a_{1}a_{2}(a_{1}+a_{2})+3a_{1}a_{3}(a_{1}+a_{3})+3a_{2}a_{3}(a_{2}+a_{3})+6a_{1}a_{2}a_{3}$$ so it's enough to prove $$a^3_{1}+a^3_{2}+a^3_{3}+7a_{1}a_{2}a_{3}\ge a_{1}a_{2}(a_{1}+a_{2})+a_{1}a_{3}(a_{1}+a_{3})+a_{2}a_{3}(a_{2}+a_{3})$$
which is clear by using Schur inequality :
$$a^3+b^3+c^3+3abc\ge ab(a+b)+bc(b+c)+ac(a+c)$$","['multivariable-calculus', 'inequality', 'a.m.-g.m.-inequality', 'calculus']"
2199812,$\ell_{\infty}$ and the bounded approximation property,"I know from several references that the Banach space of bounded sequences, namely $\ell_{\infty}$, has the bounded approximation property. But I do not have a reference where this assertion is proven. So I would be very grateful if you can give me a reference for the result asserting that $\ell_{\infty}$ has the bounded approximation property ? If you do not have a reference but you know how to do it, please provide me a sketch of your proof. I just recall here the definition of the approximation property.
We say that a Banach space $X$ has the approximation property (AP) if for every $\varepsilon>0$, for every compact set $K \subset X$, there exists a finite rank operator $T \in \mathcal B(X)$ such that $\|Tx-x\| \leq \varepsilon$ for every $x \in K$. Let $\lambda \geq 1$, if in the above definition T can always be chosen so that $\|T\|\leq \lambda$, then we say that $X$ has the $\lambda$-bounded approximation property ($\lambda$-(BAP)). Thank you in advance.","['functional-analysis', 'banach-spaces']"
2199835,Boxplot Skewness,"I do know there are some rules about boxes and whiskers to determine the skewness in a boxplot, but I am confused with some rules in this particular case: Keeping in mind the rules, in this boxplot the median falls to the right of the center of the box, thus its distribution is negatively skewed. But I also can see that the right line is larger than the left line, thus ""according to the rules"" the distribution is positively skewed. How do I know the real skewness. Thanks in advance.",['statistics']
2199920,Randomly dropping needles in a circle?,"If we were to randomly drop $n$ needles of random length in a circle,
  what would be the odds of finding $k$ intersections? This can be asked
  as: Randomly place $n$ line segments in a circle. Their length and position is
  determined by $2$ random points uniformly and independently set in that circle. What are the odds
  that we will find $k$ intersections? After seeing the main part of the calculation for what I believe to be the solution for $P(2,1)$ , I gave up on calculating $P(n,k)$ because the things behind it are out of my scope ( we barely started mentioning integrals ). But I still want to find the $P(n,k)$. Trying to first calculate the same thing, but with points always being on the circle, seems like a simpler thing to do first: Connecting random points on a circle?","['recreational-mathematics', 'probability', 'geometric-probability']"
2199956,Relations between functions,"Let $f: A \to B ,~h: A \to B,~g: B \to A,~\ell: B \to A$. $ \ell \circ h = g \circ h = g \circ f = \operatorname{Id}_A $ From here can I show that $\ell \circ f =\operatorname{Id}_A $? Here $\operatorname{Id}_A$ is the identity function sending $A$ to itself. I cannot think of any counterexamples. Thanks in advance!","['elementary-set-theory', 'functions']"
2199970,"Show that $f(x,y)=\frac{xy^2}{x^2+y^4}$ is bounded","Let $f\colon\mathbb R^2\to\mathbb R$ be a function given by:
  $$
f(x,y)=\begin{cases}\frac{xy^2}{x^2+y^4}&\text{if }(x,y)\neq(0,0),\\
0&\text{if }(x,y)=(0,0).
\end{cases}
$$ I need to show that $f$ is bounded on $\mathbb R^2$, so I need to show that there exists $M>0:\vert f(x,y)\vert\leq M$ for all $(x,y)\in\mathbb R^2$. We can rewrite $\begin{align}f(x,y)=\frac{x/y^2}{1+(x/y^2)^2}=\frac{z}{1+z^2}\end{align}$, where $z=x/y^2$ (and $y\neq0$). So for $z$ large enough, our expression will go to 0. Now we only need to worry for the case that $z$ approaches 0. We rewrite again: $\begin{align}f(x,y)=\frac{1}{1/z+z}\end{align}$, so $\lim_{z\to0}\frac{1}{1/z+z}=0$. Whatever $(x,y)$ do, if their values get small enough, we see that $f(x,y)$ will get arbitrarily close to zero, en if their values get big enough, $f(x,y)$ also comes arbitrarily close to zero. However, this can't be true, because $f(cy^2,y)=\frac{c}{1+c^2}$, so the function isn't even continuous on 0 to begin with. I'm stuck; can someone help me with this?","['functions', 'limits']"
2200019,"How do I prove that if I have a circle of infinite radius, any finite segment of it is equivalent to a straight line?","I haven't done any math in at least 5 years, so please be gentle :D I have this intuition that if I took a circle of infinite radius, any finite length segment taken from it would be a straight line segment. I know that a general circle equation in 2d x-y coords is x^2 + y^2 = r^2, but I'm having some troubles coming up with more ideas, because if I use this equation, quite a few things turn infinite, and are freaking me out. Any clues would be welcome. P.S. This is not homework, I'm too old for that, and out of school... this is just a random hobby question. Thanks. Please don't down vote! :D","['circles', 'infinity', 'geometry']"
2200074,Find the orthogonal trajectories of the family of curves,"Find the orthogonal trajectories of the family of curves $y^5=kx^2$. I start by knowing that I need a differential equation satisfying all members of family, which means $k$ needs to be eliminated. Differentiating both sides with respect to $x$: \begin{align*}
y^5 &= kx^2\\
5y^4 \frac{dy}{dx} &= 2kx\quad\text{Replacing }k\\
5y^4 \frac{dy}{dx} &= 2 \frac{y^5}{x^2}x\\
\frac{5}{y} \frac{dy}{dx} &= \frac{2}{x}\\
\frac{dy}{dx} &= \frac{2}{5} \frac{y}{x}
\end{align*} Now I have a differential equation, and the orthogonal trajectory should be represented by the negative reciprocal.
\begin{align*}
\frac{dy}{dx}&=-\frac{5}{2}\frac{x}{y}\\
\int ydy&=\int-\frac{5}{2}xdx\\
y&=-\frac{5}{4}x^2+C
\end{align*} This equation looks orthogonal to me if I plot it, but all the valid answers are of a higher order (e.g., $y^2+\frac{5}{2}x^2=C$). There must be a gap in my understanding somewhere. https://www.desmos.com/calculator/v6on063jfn","['ordinary-differential-equations', 'calculus']"
2200141,An example of a martingale converge almost surely but not in $L^1$?,"Q1 An example of a martingale converge almost surely but not in $L^1$? This I saw example $P(X_{n+1}=j|X_n=i)=\frac{1}{2i+1}$ where $0\le j\le 2i$ and $X_0=10$ but how to actually prove it? Q2 An example of a martingale both converge almost surely and also in $L^1$? a stopped coin tossing game is an example I think. Additionally if you think these are important, An example of a martingale converge in probability but not in $L^1$? An example of a martingale converge in probability but not in a.s.?","['probability-theory', 'examples-counterexamples', 'martingales']"
2200155,Elementary proof that $|x|^p$ is convex.,I'm writing some notes about analysis and want to use the fact that $|x|^p$ is convex for every $p>1$ to prove Minkowski's inequality. However I didn't wrote anything about derivatives nor limits yet. Is there a simple way to prove this? EDIT: The only non-trivial inequalities proved yet are the triangular inequality and the Cauchy-Schwarz inequality.,"['real-analysis', 'inequality']"
2200156,Definition of flows on manifolds,"I'm currently studying Differential Geometry with the book Introduction to Smooth Manifolds by Lee and got confused by the following definition of a flow: A global flow $\theta$ on a smooth manifold $M$ is a continuous map $\theta:\mathbb{R}\times M\rightarrow M$ satisfying the following properties for all $s,t\in\mathbb{R}$ and $p\in M$ : (1) $\theta(t,\theta(s,p))=\theta(t+s,p)$ (2) $\theta(0,p)=p$ For specific subsets of $\mathbb{R}\times M$ we can also define a local flow with the same properties but some additional technicalities (usually if the time-domain cannot be the whole real axis). We know that every smooth vector field induces a unique maximal local flow (i.e. with a maximal time-domain). Now to the problem: For $M=\mathbb{R}$ and the vector field $V=x^2\frac{\partial}{\partial x}$ we get the (local) flow $\theta(t,p)=\frac{p}{1-t}$ by solving the ODE $\dot{x}=x^2$ with initial value $x(0)=p$ . However, $\theta(t,\theta(s,p))=\frac{p}{1-s}\frac{1}{1-t}\neq\theta(t+s,p)$ so (2) is not fulfilled and $\theta(t,p)$ is not a flow - although it is the solution of the corresponding ODE... So apparently I made a mistake in interpreting above definition. I would be happy if someone could point it out. Thanks!","['vector-fields', 'smooth-manifolds', 'differential-geometry']"
2200160,When does a (polynomial) ring homomorphism commute with scalar multiplication?,"Definitions: Let $k$ be an arbitrary field, $I$ a prime ideal of $k[x_1, \dots, x_n]$, $J$ a prime ideal of $k[y_1, \dots, y_m]$, $X = \{ x \in k^n: \forall f \in I, \ f(x) = 0\  \} \subset k[x_1, \dots, x_n]$, $Y=\{y \in k^m: \forall g \in J, \ g(y)=0  \}$, $\mathcal{O}_X = k[x_1,\dots,x_n]/I$, and $\mathcal{O}_Y = k[y_1, \dots, y_m]/J$. (I.e. $X$ and $Y$ are affine varieties, and $\mathcal{O}_X$ and $\mathcal{O}_Y$ are their respective coordinate rings.) Both $\mathcal{O}_X$ and $\mathcal{O}_Y$ are commutative rings with a multiplicative identity, so when considering a ring homomorphism $\varphi: \mathcal{O}_Y \to \mathcal{O}_X$ we will mean a function $\mathcal{O}_Y \to \mathcal{O}_X$ such that for all $g_1,g_2 \in \mathcal{O}_Y$ $\varphi(g_1+g_2)=\varphi(g_1)+ \varphi(g_2)$, $\varphi(g_1g_2)=\varphi(g_1)\varphi(g_2)$, and $\varphi(1+J)=1+I$. Question: Under what conditions on the field $k$ is any ring homomorphism $\varphi: \mathcal{O}_Y \to \mathcal{O}_X$ a $k$-algebra homomorphism? I.e. under what conditions on the field $k$ does any ring homomorphism $\varphi: \mathcal{O}_Y \to \mathcal{O}_X$ commute with scalar multiplication, for all $c \in k, g \in k[y_1, \dots, y_m]$, $$\varphi(cg+J)=c\varphi(g+J)? $$ Because $k \subset k[x_1, \dots, x_n]$, $k\subset k[y_1, \dots, y_m]$, the above is equivalent to requiring, for all $c \in k$, that: $$\varphi(c+J) = c+I\,, $$ with the previously stated property following from the fact that $\varphi$ commutes with multiplication. Attempt: In special cases, e.g. $k = \mathbb{Q}$ or $k=\mathbb{R}$, this should be true (provided in the latter case that $\varphi$ is also continuous, which I don't see why we can assume in general). For more general $k$, I don't see at all why this should be true, even if $\varphi$ is continuous. For $k=\mathbb{Q}$, I think this follows from the facts that $\varphi(1+J)=1+I$, $\varphi(g_1g_2+J)=\varphi(g_1+J)\varphi(g_2+J)$, and that $\varphi((g_1+g_2)+J)= \varphi(g_1+J)+\varphi(g_2+J)$. Namely, $$\varphi(2+J) = \varphi((1+1)+J)=\varphi(1+J)+\varphi(1+J)=(1+I)+(1+I)=2+I\,, $$ and then by using induction we get that $\varphi(n+J)=n+I$ for all $n \in \mathbb{N}$. Then for any $n \in \mathbb{Z}$, we get that $\varphi(n+J)=n+I$ because $\varphi(J)=I$ and $I=\varphi(J)=\varphi((n+(-n))+J)=\varphi(n+J)+\varphi(-n+J) \implies \varphi(n+J)=-\varphi(-n+J)$. Then for any $q = \frac{m}{n} \in \mathbb{Q}$, we have that $$m+I=\varphi(m+J)=\varphi\left(\left(\frac{m}{n}\right)n+J\right)=\varphi\left(\frac{m}{n}+J\right)\varphi(n+J)=\varphi\left(\frac{m}{n}+J\right)(n+I)\\ \implies \varphi\left(\frac{m}{n}+J\right)=\frac{m}{n}+I\,.$$ Then, assuming that $\varphi$ is continuous , since any real $r \in \mathbb{R}$ is a limit of some sequence of rational numbers $\{q_n\}$, and continuous functions commute with limits of sequences, we get: $$ \varphi(r+J)=\varphi\left(\lim_{n \to \infty} (q_n+J)\right) = \lim_{n \to \infty} \varphi(q_n +J) = \lim_{n \to \infty} (q_n + I) = r+I\,. $$ Thus $\varphi(c+J)=c+I$ for all $c \in k$ for $k=\mathbb{R}$ (or $k=\mathbb{Q}$). (Related) For $k=\mathbb{C}$, since $-1 \in \mathbb{Z}$, we have that: $$-1+I= \varphi(-1 +J)= \varphi(i^2 +J)= \varphi(i+J)^2 \implies \varphi(i+J) = \pm i +I \,. $$ Since the Galois group of $\mathbb{R}[i]$ is non-trivial, I don't see how it could be possible to exclude the possibility that $\varphi(i+J)=-i+I$. Similar problems would arise with other algebraic field extensions of $\mathbb{Q}$ or $\mathbb{R}$ (I imagine). For fields without straightforward relationships to $\mathbb{Q}$, I imagine that this could fail to be true in even more dramatic ways, but I don't know, hence my question. Context: This question is inspired by Problem 4.18.5 of Algebraic Geometry: A Problem Solving Approach by Garrity et al., which asks one to show that each ring homomorphism $\mathcal{O}_Y \to \mathcal{O}_X$ corresponds to a morphism between the affine varieties $X \to Y$, for $k$ an arbitrary field. The only way I could get a proof to work was with the additional assumption that $\varphi(c+J)=c+I$ for all $c\in k$, which as I noted above is equivalent to $\varphi$ commuting with scalar multiplication and thus being a $k$-algebra homomorphism. This seems to be corroborated by Problem 4.8.4 of the same book, Wikipedia , Theorem 4.14 here , and other references I remember finding. Thus, I want to understand the extent of the strength of this additional assumption, and whether or not it actually follows for any fields $k$ from the fact that $\varphi: \mathcal{O}_Y \to \mathcal{O}_X$ is a ring homomorphism.","['abstract-algebra', 'galois-theory', 'algebraic-geometry', 'commutative-algebra']"
2200201,absolute value of supremum is smaller than or equal to supremum of absolute values,"Is the following statement correct? ""The absolute value of supremum of a set is smaller than or equal to the supremum of the absolute values of the first set"" It would be helpful to proof that the absolute value of the integral of a function is smaller than or equal the integral of the absolute value.","['real-analysis', 'supremum-and-infimum', 'proof-writing', 'integration', 'proof-explanation']"
2200216,"$f :\mathbb R \to \mathbb R$ be a bijective Lebesgue measurable function , then is $f^{-1}:\mathbb R \to \mathbb R$ Lebesgue measurable?","Let $f :\mathbb R \to \mathbb R$ be a bijective Lebesgue measurable function , then is $f^{-1}:\mathbb R \to \mathbb R$ Lebesgue measurable ? I don't think this is true but I can't find any counterexample , or any way through . Please help . Thanks in advance NOTE : Any counter example using non Lebesgue measurable set is fine with me","['measurable-functions', 'lebesgue-measure', 'measure-theory']"
2200245,Why are there no nonzero morphisms between these two sheaves?,"Let $C$ be a nodal curve, $w\in C$ a node, and let $p: C'\rightarrow C$ be its normalization, with $p^{-1}(w) = \{z_1,z_2\}$. Let $\Omega_{C',z_1}$ be the stalk of the cotangent sheaf of $C'$ at $z_1$. Why is the following true?
$$\text{Hom}_{\mathcal{O}_{C,w}}(\Omega_{C',z_1},\mathcal{O}_{C',z_2}(-z_2)) = 0$$
(This is claimed on page 182 in Arbarello-Cornalba-Griffiths ""Geometry of Algebraic Curves"", Volume 2)",['algebraic-geometry']
2200277,Are squares dense in C*-algebras?,Suppose that we have a (complex) C*-algebra $A$. Let us say that $x\in A$ is square if $x=y^2$ for some $y\in A$. Is the set of squares dense in $A$? What if $A$ is commutative?,"['functional-analysis', 'c-star-algebras', 'operator-algebras', 'banach-algebras']"
2200291,Obtaining higher order terms of the Baker-Campbell-Hausdorff formula as higher order commutators,"Let $G$ be a Lie group, then $(X,Y) \mapsto \log(\exp(X)\exp(Y))$ locally defines a group law on a neighbourhood of  $0 \in T_eG$. This local group law is analytic, and the first term is given by $X + Y$ and the second by $\frac{1}{2}[X,Y]$. According to Tits' textbook on Lie groups the Lie bracket may be obtained as the second order derivative of $G \times G \to G, \; (x,y) \mapsto xyx^{-1}y^{-1}$. (This is well-defined, because the first derivative of $(x,y) \mapsto xyx^{-1}y^{-1}$ vanishes.) My question is then: Are there maps $\pi_r: G \times G \to G$ for every $r \in \mathbb{N}$ such that $d^k \pi_r$ vanishes for all $k < r$ (so that we obtain a well defined $r$-linear form $d^r \pi_r: T_eG \times \underbrace{\dots}_{2 r\, \times} \times T_eG \to T_eG$), such that the $r$-th term in the Baker-Campbell-Hausdorff formula is given by $\frac{1}{r!}d^r \pi_r$? Furthermore, can the maps $\pi_r$ be constructed from the multiplication and inverse maps on $G$?","['analyticity', 'manifolds', 'differential-geometry', 'lie-algebras', 'lie-groups']"
2200297,Limit of a sequence in an $L^{p}$ space.,"Problem Statement: Suppose $f_{n}\in L^{p}(\mathbb{R}^{d})$ with $\lVert f_{n}\rVert_{p}\leq M$ ( $1\leq p<\infty$ ). Suppose $f_{n}(x)\rightarrow f(x)$ pointwise almost everywhere. Show that $f\in L^{p}(\mathbb{R}^{d})$ with $\lVert f\rVert_{p}\leq c_{p}M$ . ( $c_{p}=2^{p-1}$ , so $\lvert x+y\rvert^{p}\leq c_{p}(\lvert x\rvert^{p}+\lvert y\rvert^{p})$ , $p\geq 1$ ). I am trying to solve this problem but I am not completely clear about what exactly I need to show, and what I can assume. I was told to first assume that $f$ is bounded, and use Egorov's theorem to show that $\lVert f\rVert_{L^{p}(B_{R})}\leq c_{p}M$ for each $R>0$ . Egorov's Theorem $D\subset \mathbb{R}^{n}$ , $m(D)<\infty$ and $f_{n}, f$ are measurable functions over $D$ with $f_{n}(x)\rightarrow f(x)$ almost everywhere. Then $\forall \varepsilon >0$ $\exists E\subset D$ closed with $m(D\setminus E)<\varepsilon$ and $f_{n}\rightarrow f$ uniformly in $E$ . First, with the original assumption "" $f_{n}(x)\rightarrow f(x)$ pointwise almost everywhere"", does this mean with respect to the $L^{p}$ norm? That is, $\exists E\subset \mathbb{R}^{d}$ with $m(E)=0$ such that $\forall \varepsilon>0$ $\exists N=N(\varepsilon)>0$ such that $n\geq N$ and $x\in \mathbb{R}^{d}\setminus E$ (or $x\in B_{R}\setminus E$ ) implies $\lVert f_{n}(x)-f(x)\rVert_{p}<\varepsilon$ . Then to show that $f\in L^{P}(B_{R})$ . We must show that $\lVert f\rVert_{p}$ exists? That is, show that $\Big(\int_{B_{R}}\lvert f\rvert^{p}\Big)^{1/p}$ exists. Is it enough to say that because $f$ is measurable, then $\lvert f\rvert^{p}$ is measurable, and thus has an integral? But by Egorov's Theorem, $f_{n}\rightarrow f$ uniformly in $B_{R}$ , so is it enough to use the fact that $L^{p}$ is a Banach space to conclude that since $f_{n}\rightarrow f$ uniformly implies that $f\in L^{p}(B_{R})$ ? Then we have $$\lVert f\rVert_{p}\leq \lVert f_{n}-f\rVert_{p}+\lVert f_{n}\rVert_{p}<\varepsilon +M.$$ But we must show that $\lVert f\rVert_{p}\leq c_{p}M$ . Yet, I am not seeing where the $c_{p}$ comes into play, nor why the inequality $\lvert x+y\rvert^{p}\leq c_{p}(\lvert x\rvert^{p}+\lvert y\rvert^{p})$ is useful. I appreciate any hints to push me in the right direction.","['real-analysis', 'functional-analysis', 'lp-spaces', 'proof-explanation', 'convergence-divergence']"
2200303,How to find the intersection of two congruence classes,"Hi Mathematics Stack Exchange community! I have a question. I want to know how one would go about taking an intersection of two congruence classes. Are there any well known methods to do it? i.e.
$$\overline{2}_3 \cap \overline{1}_4=\overline{5}_{12}$$
$$\{3n+2|n\in \mathbb Z\}\cap\{4n+1|n\in \mathbb Z\}=\{12n+5|n\in \mathbb Z\}$$
$$\{\ldots, -7,-4,-1,2,5,8,\ldots\}\cap\{\ldots, -7, -3, 1, 5,9\ldots\}=\{\ldots,-19, -7,5,17,\ldots\}$$ I found a way to do it by myself but when I tried to google about intersections of congruence classes, I found only one site which wasn't even about the thing I was after. Is it just assumed that you have to be able to find the intersection by yourself? I'm not yet in uni and I haven't taken any courses in modular arithmetic so I don't know, what is taught and what is not...","['number-theory', 'modular-arithmetic', 'elementary-set-theory', 'elementary-number-theory']"
2200308,"How was ""Number of ways of arranging n chords on a circle with k simple intersections"" solved?","The problem whose solution is based on the solution to the problem in the title came up as I was trying to find a simpler variant of my needle problem . I we were to uniformly, randomly and independently set $2n$ points on
  a circle, and then randomly connect them in a way such that each point
  has its own pair, what would be the odds of finding $k$ intersections? Based on the maximum number of intersections we see that if $k \gt \frac{n(n-1)}{2}$, $P=0$. Otherwise we have some $P>0$. When connecting the points, all that matters is the ordering of points. Data Analysis I can write $P(n,k) = a / b$. Then $b$ is the number of ways to connect the points uniquely, and $a$ is the number of cases with $k$ intersections for $n$ lines. There are $b = (2n-1)!!$ ways of connecting the points uniquely. I wrote a piece of code in java to try to brute-force solutions of $a$, for $n$ up to $10$. I wrote them out in a spreadsheet as a image. Here is the raw data as text. After closely analyzing the values of $a$, OEIS provided me with a sequence. Looks like someone already calculated $a$ which actually is the number of ways of arranging n chords on a circle with k simple intersections . But the given formula is not correct given as it is. Thanks to Paul for fixing up a valid formula , since the OEIS one I stumbled upon seems to be wrong. The formula for $P(n,k)$ then is: $$ \frac{\displaystyle \sum_{j=1}^{\left\lfloor \tfrac12 +
 \tfrac12\sqrt{1+8k} \right\rfloor} (-1)^j \cdot
 \binom{n+k-1-\binom{j}{2}}{n-1} \cdot  \left( \binom{2n}{n+j} -
 \binom{2n}{n+j-1} \right)}{(2n-1)!!} $$ Which solves my initial problem. But I'm still curious to know how someone came up with this in the
  first place, starting out with just a circle and some cords? Regarding the title; How was "" Number of ways of arranging n chords on a circle with k simple intersections "" solved to produce the expression in the numerator?","['combinatorics', 'recreational-mathematics', 'probability', 'geometric-probability']"
2200335,Is the homomorphic image of an ideal an ideal?,"Let $f:A\to B $ be a homomorphism between the rings $A$ and $B$ and let $J$ be an ideal of $A$, then $f(J)$ is an ideal of $B$. If $f(x)\in f(J)$ and $f(y)\in B$ then $f(x)f(y)=f(xy)$ and since $x\in J$ then $xy\in J$ and therefore $f(x)f(y)\in f(J)$. For $f(x),f(y) \in f(J)$ then $f(x)-f(y)=f(x-y)\in f(J)$ since $x,y \in J$ and $J$ is closed under subtraction. However, nowhere in this proof did I use the fact that $\text {ker} f \subset J$, which was a part of the problem statement, leading me to believe that my proof is wrong or incomplete. I cannot see my mistake, are there hidden assumptions I am making?","['abstract-algebra', 'ring-theory']"
2200337,$\triangledown_{X_{p}}Y$ actually depends only on the values of $Y$ along any curve tangent to $X_{p}$,"I'm reading John Lee's Riemannian Manifolds where I encountered the following reading exercise: Improve Lemma 4.1 by showing that $\triangledown_{X_{p}}Y$ actually depends only on the values of $Y$ along any curve tangent to $X_{p}$. More precisely, suppose that $\gamma: (-\epsilon, \epsilon) \rightarrow M$ is a curve with $\gamma(0)=p$ and $\gamma'(0)=X_{p}$, and suppose that $Y$ and $\tilde{Y}$ are vector fields that agree along $\gamma$. Show that $\triangledown_{X_{p}}Y = \triangledown_{X_{p}}\tilde{Y}$ Here we are taking $M$ to be a smooth manifold and $\triangledown$ to be a linear connection. Lemma $4.1$ essentially tells us that the vector field, $\triangledown_{X}Y$ is determined locally by $Y$ and pointwise by $X$. The linearity of the connection, I hope to show that if $Y=0$ along $\gamma$ then $\triangledown_{X_{p}}Y=0$. Earlier in the text, the notion of covariant derivatives of vector fields along curves were discussed, in particular it was mentioned that when the vector field $V$ along a curve is extendible, then for any extension $\tilde{V}$, we have that $D_{t}V(t)=\triangledown_{\gamma'(t)}\tilde{V}$ in which case we have a formula for $D_{t}V(t)$. I'm a bit confused on how to involve this. Any help is appreciated.","['riemannian-geometry', 'differential-geometry']"
2200366,Numerical integration over a triangle in 3D,"I want to numerically integrate a surface discretized in triangles. Let's say I have a triangle with vertices: $(x_1,y_1,z_1)$, $(x_2,y_2,z_2)$ and $(x_3,y_3,z_3)$. In order to integrate it easily I would like to map it to the reference triangle defined as: $T_{ref} = \{(0,0,0),(1,0,0),(0,1,0)\}$. However, I do not the form of the jacobian in order to perform the change of variables over the integral. What I tried was to define the affine transformation:
$$B\hat{x} + c = \begin{bmatrix} x_1-x_3 & x_2-x_3 & 0\\ y_1-y_3 & y_2-y_3 & 0\\z_1-z_3 & z_2-z_3 & 0\end{bmatrix}\begin{bmatrix}\hat{x}\\\hat{y}\\\hat{z}\end{bmatrix} + \begin{bmatrix}x_3\\y_3\\z_3\end{bmatrix}$$
and to define the jacobian as $B^T$ but of course it does not work because the determinant would be zero. Moreover, the third column of $B$ could be any value so it does not make sense to me. Can anyone explain to me how to integrate a triangle define in 3D by passing it to the reference triangle? Thanks.","['multivariable-calculus', 'numerical-methods', 'integration', 'calculus']"
2200385,Settle a classroom argument - do there exist any functions that satisfy this property involving Taylor polynomials?,"I'm going to apologize in advance; I might at some points say Taylor series instead of Maclaurin series. OK, so backstory: My calculus class recently went over Taylor series and Taylor polynomials. It seemed basic enough. Using the ratio test we were able to prove the radius of convergence of these series as well. For example, we derived that: $$
e^x = \sum_{n=0}^\infty\dfrac{x^n}{n!}
$$ using the ratio test we can find that the series converges $\forall x$ However, today we had a substitute that talked about Taylor's theorem and Taylor's formula defined as the sum of an $n$th order Taylor polynomial plus the remainder. $$
f(x) = P_n(x) + R(x)
$$
$$
R(x) = \dfrac{f^{n+1}(c)(x-a)^{n+1}}{(n+1)!}
$$ The substitute teacher then told us that in order to prove that the Taylor polynomial converges to the original function, you must show that 
$$
\lim_{n\rightarrow\infty}R(x)=0
$$ Well, after this statement the flood gates opened with a few students asking why you can't just use the ratio test to show the Taylor series converges $\forall x$ like we did for $e^x$. The substitute said that the ratio test only proved convergence, while this proved it converged to the actual function. The students then said that if we already proved that the Taylor series is the function at an infinite amount of points, if the series converges, doesn't that mean that it converges to the function? We had already done an example previously in class where:
$$
f(x)=\begin{cases}
0,&\text{ if }x=0;\\
e^{-\frac{1}{x^2}},&\text{ if }x\neq 0.
\end{cases}
$$ This function's Taylor polynomial converges to 0 at every point. However, it doesn't converge to the function at every point. My classmates said this was a cop-out and ""didn't count"" because it was a piecewise function. So is there an example of a function whose Taylor polynomial converges on some interval, but does not converge to the function entirely on that interval? Also a proof would be cool if you could explain why the students or the teacher were wrong.","['taylor-expansion', 'uniform-convergence', 'sequences-and-series', 'calculus', 'convergence-divergence']"
2200406,Analytically derive the reproduction number of an SI model,"For an assignment I have to create an SI model with Python and answer some questions about it. The model itself is very simple. I use a Python package to create a graph with an average degree of $\langle k \rangle$. In this graph an infected node, for each timestep, will infect a neighbouring node with a probability of $i$. The problem I am having is understanding one of the questions that has nothing to do with programming, but with mathematics! Specifically we have to analytically derive the reproduction number $R_0$ (which they define as ""the expected number of new infections in the
first step of time per infected node""). I have Googled a lot and found out the following: \begin{align*}
    \frac{dI}{dT} &= \beta \frac{SI}{N}\\
    \frac{dS}{dT} &= -\beta \frac{SI}{N}
\end{align*} where $I$ is the total number of infected nodes, $S$ the total number of susceptible nodes and $N$ the total number of nodes ($N = S + I$). In most places I've read that $\beta$ is defined as the infection/contact rate or the probability of infection times the average contact between susceptible and infected nodes. In my case I assume that $\beta = i \cdot \langle k \rangle$, right? How do I get $R_0$ from this, and is this even what they mean by ""analytically derive""?","['graph-theory', 'self-learning', 'ordinary-differential-equations', 'mathematical-modeling']"
2200435,How to calculate congruence for all $n\in\mathbb{N}$?,"We have $43 \equiv 1 \mod n$. How to find all $n\in\mathbb{N}$? So I know formula $b = a + kn$, where $k\in\mathbb{Z}$. It looks to me like I need to calculate all combinations of $k$ and $n$ which will give $42$, or to find all natural numbers which are dividing $42$. Am I missing something obvious, and if so, what is the easiest way to calculate all n's?","['congruences', 'discrete-mathematics']"
2200436,To what extent is linear stability analysis of numerical methods relevant to nonlinear ODEs?,"Some context. Consider a numerical method $y_{n+1} = \Psi(y_n, h)$ for solving ODEs, where $h$ is the step size. The following definition of linear stability is ubiquitous in the introductory numerical analysis literature on numerical ODEs (see Definition 8 in the these notes ): Suppose $y' = \lambda y$ for some $\lambda\in\mathbb C$. Then the
  numerical method $\Psi$ is linearly stable if $y_n\to 0$ as
  $n\to\infty$. As a numerical methods novice, I wonder to what extent this definition is relevant to non-linear ODEs given that it is formulated in terms of a linear model problem.  Later in the same notes, the following statement appears and seems to address this: If a numerical method is stable in the above sense for a certain range
  of values of $\lambda$, then it is possible to show that it will be
  stable for the ODE $y' = f(t,y)$ as long as $\frac{\partial f}{\partial y}$
   is in that range of $λ$ (and $f$ is smooth enough). We
  won’t prove this theorem here. This statement has intuitive appeal since $\partial f/\partial y$ at a given point determines the behavior of the linearized equation, and one might imagine that applying stability analysis to the linearized equation at a point would give relevant information about the performance of the numerical method near that point, but I'm left wanting a more detailed discussion of this and proof. Questions. Where can I find a more detailed discussion of that second quoted statement? In general, is linear stability analysis considered relevant to nonlinear systems because one can prove statements similar to that second quoted statement to the effect of ""if the numerical method is stable for the linearized equation at every point, then it will be stable for the full, non-linear equation?"" References appreciated.","['numerical-methods', 'ordinary-differential-equations']"
2200482,How do I prove that the sum: $1/\ln(n)^p$ diverges for $p>1$,So I need to prove that the infinite sum $\frac{1}{(\ln(n)^p)}$ diverges for all values of $p$ . I managed to prove it for $p\leq 1$ via comparison test with $1/n$ . but for this I can't seem to find a way to prove it diverges for $p>1$ .,"['summation', 'sequences-and-series', 'calculus']"
2200505,Show that a functional has no minimum in a given set,"This is a problem from my calculus of variations class: Let $X=\{v:[0,1]\rightarrow\mathbb{R}$ of class $C^1$ , $v(0)=1$ , $\ v(1)=0\}$ , let $F:X\rightarrow\mathbb{R}$ be the functional $$F(v):=\int_0^1 (e^{v'(x)}+v^2(x))dx.$$ Integrate the Euler-Lagrange equation with conditions $v(0)=1$ , $\ v'(0)=a$ , $a\in\mathbb{R}$ and show that $F$ has no minimum on $X$ . The professor also said that we could restrict ourselves to the case $a<0$ . Ok, I did my homework: first we define $X_a:=\{v:[0,1]\rightarrow\mathbb{R}$ of class $C^1$ , $v(0)=1$ , $\ v'(0)=a\}$ . The Euler-Lagrange equation for $F$ is $v''e^{v'}=2v$ . Since the functional $F$ doesn't depend explicitely on $x$ the quantity $v'e^{v'}-e^{v'}-v^2$ is some constant, say $k$ , thus imposing the boundary conditions at $x=0$ we get $e^{v'}(v'-1)-v^2=e^{a}(a-1)-1$ (which is the du Bois-Reymond equation and it is exactly what we would have got integrating $v'(v''e^{v'}-2v)=0$ ). Now, if a minumum exists in $X$ it must have bounded derivative in $x=0$ , so it must be in $X_a$ for some $a<0$ . Let's see if the boundary conditions of $X$ are compatible with the boundary conditions of $X_a$ . In $x=1$ we have $e^{v'(1)}(v'(1)-1)=e^{a}(a-1)-1$ . Since $v'(1)$ is something in $\mathbb{R}$ , we reduced the problem to the study of the zeros of the function $g(y):=e^{y}(y-1)-e^{a}(a-1)+1$ . It turns out that $g$ has no zeros for $a<0$ , so we are done. Let's get to the question: why can we restrict ourselves to $a<0$ ? For $a\geq0$ g has one zero, so there could be a  candidate minimum in $X\cap X_a$ . Graphically, I can believe that the function $v$ has to be decreasing to minimize the functional, but this is not a proof, nor a satisfactory justification. I thought that if we can bound from below the values that $F$ assumes on $X\cap X_a$ , $a\geq0$ with $F(w)$ for some $w$ not in $X\cap X_a$ , this will be enough to show that $F$ has no minimum in $X$ . Nevertheless this seems not so simple, since we don't have an explicit formula for the candidate minimum. $\boldsymbol{Edit}$ : A colleague of mine told me that for the case $a>0$ I have to look at the sign of the second derivative. From the Euler-Lagrange equation we get $sign(v''(x))=sign(v(x))$ $\forall x\in(0,1)$ , which implies that the first derivative is increasing in $(0,1)$ . By continuity and positivity at 0 of the first derivative we get that $v$ is increasing in $[0,1]$ , thus it cannot reach the point 0 at $x=1$ . My question is: does this argument cover also the case $a=0$ ?","['variational-analysis', 'real-analysis', 'calculus-of-variations', 'euler-lagrange-equation']"
2200506,How to prove that there must be a bijection between two different implementations of the same function,"This question emerged in a conversation with a neuroscientist this morning, in which I wondered aloud whether one could prove that if there exists two different implementations of a function then there must exist a bijection between the implementations. This would be useful for instance, if you wanted to train an artificial neural network to actual sensory input of an organism and ask it to produce precisely the same motor output as the model organism does. Then with the help of such a theorem, you'd be able to show that while the two functions have the same mappings and are ostensibly implemented differently, there does exist a direct correspondence between what is being accomplished in with linear algebra in your model and the chemical computations in the real organism. For example, take the function $$F(x) = \sin x$$ and take the function $$G(x) = \sum\limits_{n=0}^{\infty} (-1)^n \frac{x^{2n+1}}{\Gamma(2n+2)}$$ For any $x$ we know that $F(x) = G(x)$. Provided we do not define $\sin x \equiv G(x)$, i.e. perhaps we define $\sin \theta$ implicitly as: $\forall x,y,r \in \mathbb{R}:\big\{ \frac{y}{x}= \frac{\sin\theta}{\cos \theta} \implies x^2 + y^2 = r^2 \big \}$ and then prove that the mappings $\sin \theta$ and $\cos \theta$ are uniquely defined by this assertion. Is it possible to show that the two ""implementations"" of $\sin x$ must have a bijection. That somehow the formal machinery that allows $\sin x$ to emerge from my implicit definition is actually equivalent to the formal machinery that enables $\sin x$ to emerge from the Taylor series? What I do not want to show is that the mappings are the same. That I know how to show. What I want to show and am struggling to express is that the actual formal implementations of the functions are the same. Perhaps both definitions require certain things to happen underground that are symmetric. I don't know. Can you help me Express this idea Tell me what has been thought about it before (perhaps in theory of computation?) Do so in a way that a pre rigorous guy can appreciate","['logic', 'functions']"
2200507,Derivations vs automorphisms of deformations,"Let $A$ be a ring, and $p : R'\rightarrow R$ a surjection of $A$-algebras with kernel $I$ a square zero ideal - ie $I^2 = 0$. Let $f,g : R'\rightarrow R'$ be automorphisms of $R'$ over $R$ (ie, $pf = pg = p$), then it is easy to check that $f-g : R'\rightarrow R'$ has image in $I$, and actually defines an $A$-linear derivation, ie an element of $Der_A(R',I)$. Here we probably want to assume that $f,g$ are also $A$-linear. Now let $f\in\text{Aut}_A(R')$ respecting the map $p : R'\rightarrow R$, and $D\in Der_A(R',I)$. Must $f+D$ also be an automorphism of $R'$? If not, what are the ""minimal reasonable assumptions"" we need to impose for this to be true? I can show that $f+D : R'\rightarrow R'$ is a homomorphism of $R$-algebras, but am having trouble proving bijectivity. Variations of this are given in Hartshorne's ""Deformation theory"", where it's given as exercise 5.2, and in Sernesi's book ""Deformations of Algebraic Schemes"", where it's ""Lemma 1.2.6"", but he doesn't actually prove that $f+D$ is an automorphism.","['deformation-theory', 'algebraic-geometry', 'commutative-algebra']"
2200563,Covariance of two jointly continuous random variables,"I need to solve the following question. I apologize in advance not quite sure how to note piece wise functions. $$
f(x,y)=\begin{cases}72x^2y(1-x)(1-y),& 0 \leq x\leq1, 0\leq y \leq 1\\
0,& \mathrm{otherwise.}
\end{cases}
$$ Now I know that the $\operatorname{Cov}(X)=E[XY]-E[X]E[Y]$ and I have attempted to solve by using the same equation for the above continuous function, however, I'm not sure if I am doing this right. By taking the expected values of $x$ and $y$ seperately, there will be variables left and it won't give an exact constant as an answer. For example: $$E[X]=\int_0^1x\times72x^2y(1-x)(1-y)dx$$ I'm not sure if I'm doing this right. Also, the next question is: Determine $P({X>Y})$.
Which I don't know how to solve","['means', 'statistics', 'probability', 'covariance']"
2200572,Trace of a nilpotent operator,"Let $V$ be a finite-dimensional vector space over a field $\mathbb{F}$, and $T:\,V \longmapsto V$ a linear map. The trace of $T$, $tr(T)$, is the trace of any matrix $A \in M_n(\mathbb{F})$ related to $T$ with respect any basis of $V$. I have to show that Lemma If $T$ is a nilpotent linear map, then $tr(T)=0$. proof. We know that if $T:\,V \longmapsto V$ a linear nilpotent map of a a finite-dimensional vector space, then then there exists a basis of $V$ such that the matrix representation of $T$ is upper triangular with zero diagonal elements. Then $tr(T)=0$. QUESTION: My teacher uses a different proof of this lemma, but I think there is something missing... proof.2 First of all it is easy to show that the trace of a linear transformation is also equal to the sum of the roots of his characteristic polynomial $p_T$ in a splitting field $K$ of $p_t$ over $\mathbb{F}$. Also if $T:\,V \longmapsto V$ a linear nilpotent map then its only eigenvalue is 0. Then $tr(T)=0$. I think this proof is true only if the field $\mathbb{F}$ is algebraically closed: if so, $\mathbb{F}$ contains all the roots of $p_T$. Then every root of $p_T$ is an eigenvalue of $T$ and so it must be zero. But if $\mathbb{F}$ is not algebraically closed, it is possible that there are some roots of $p_T$ that are not in $\mathbb{F}$. So we are not sure that every root of $p_T$ is an eigenvalue of $T$, and then I cannot use the fact that every eigenvalue of $T$ is 0 to show that $tr(T)=0$. Am I wrong?? Thanks a lot!!","['eigenvalues-eigenvectors', 'linear-algebra', 'nilpotence']"
2200667,Polar Coordinates as a Definitive Technique for Evaluating Limits,"A lot of questions say ""use polar coordinates"" to calculate limits when they approach $0$. But is using polar coordinates the best way to evaluate limits, moreover, prove that they exist? Do they account for every single possible direction to approach a limit, for example, along a parabola. Specifically, if I were to show that 
$$\lim_{(x,y)\rightarrow (0,0)} f(x,y)=L$$
using polar coordinates, is that enough to asser that the limit is indeed, $L$. ?",['multivariable-calculus']
2200670,Confidence interval in statistics,"I really need help on this question. I am taking an introductory statistics class, and here is the question: A taxi company wants to determine if customers will accept self driving cars. The company authorized a survey and subjects were to be selected at random from their current customer database (you can assume independence and a large population). Using the entire customer database, the company determined that the amount paid per trip was normally distributed with a mean of $\$50$ and a standar deviation of $\$5$ and $98\%$ of customers owned a cellphone and $59\%$ of the customers used paypal. The company then selected a simple random sample of size $27$ and conducted in person interviews. Analysis of the sample revealed that the $27$ customers paid an average of $\$53$ with a standard deviation of $\$12$, $47\%$ slept during their ride, and $55\%$ used paypal and 64% are males. Please construct a $95\%$ confidence interval for the population proportion of customers who slept during their ride. My question is: why is the question asking for population proportion when the scenario is giving information that indicates I would have to use a t-test for sample means? I am confused whether to use $2.056$ or $1.96$ as the multiplier to construct my confidence interval. I am guessing that I would have to use $1.96$ as my multiplier since the question is asking for population proportion instead of sample means. (and the fact that I would only use the degree of freedom to find my multiplier when they're asking for confidence levels not intervals)? If any of you guys could approve or disprove my line of thinking, I would greatly appreciate it. 
Thanks",['statistics']
2200673,Finding flux over a surface (parameterization),"I'm trying to evaluate $\iint_D \langle x,y,-2\rangle \textbf{n} \cdot \textbf{dS} $, where $D$ is given by $z=1- x^2 - y^2$, $x^2+y^2 \leq 1$, oriented up. Now, I'm trying to find a parameterization for D but I'm struggling to do so. Graphically, we have a paraboloid with maximum $z=1$ and a cylinder with radius $\leq 1$. So that means I'm trying to parameterize an area that is like a hemisphere. Can I use spherical coordinates then such that: $$r(\phi,\theta) = \langle r\sin(\phi)\cos(\theta), r\sin(\phi) \sin(\theta), r\cos(\phi), \quad  0\leq \phi \leq \frac \pi 2, \quad 0 \leq \theta \leq 2\pi $$ Then I can find the flux using the formula 
$\iint\textbf{F}\cdot(r_\phi \times r_\theta) \, dA $. P.S. I'm weak at parameterizing..","['multivariable-calculus', 'surface-integrals']"
2200711,Is there a way to find the bounds of integration for triple integrals without drawing pictures (analytically)?,"I'm able to solve integrals and draw graphs in 2D, but I have a lot of trouble drawing 3D graphs. As such, I'm seeking a method of finding the bounds of integration for triple integrals without having to graph them. It seems reasonable to me that there is such a method, since we also need a way to solve higher dimensional integrals (4D, 5D, 6D, etc), without the ability to graph them. I found this (relevant) question from 3 years ago, but it was left unanswered: Is there a way to find limits of integration numerically for triple integrals? I would greatly appreciate if someone could please explain or direct me to a method for solving triple integrals (finding their bounds of integration) without having to graph them.","['multivariable-calculus', 'real-analysis', 'integration', 'definite-integrals']"
2200740,Prove this sequence takes every rational number,"Given the sequence $a_1 = 0$ and $a_{n+1} = \dfrac{1}{2 \cdot\lfloor{a_n}\rfloor-a_n+1}$ and $p,q\in \mathbb N$ and coprime find $x$ so that $a_x = \dfrac{p}{q}$. I do not even know where would you start with a problem like this.",['sequences-and-series']
2200826,How to directly generate permutations without fixed points?,"I have read this , however, the recursive formula still requires the complete information(all cycles with and without fixed points) of the ""last step"". I wonder if there is an algorithm that can directly generate permutations without fixed points? For example, when $n=4$, the result is (1 2 3 4) (1 2 4 3) (1 3 2 4) (1 3 4 2) (1 4 2 3) 
(1 4 3 2) (1 2)(3 4) (1 3)(2 4) (1 4)(2 3) some clarification I was struggling with this part: for a given $n$, and given the partition(the number of cycles and the cycle length), how to generate all the possible, non-duplicate cycles? for example, given $n=4$ and two cycles of length 2, 2, respectively, how are (1 2)(3 4) (1 3)(2 4) (1 4)(2 3) generated? Firstly, I understand that there is a one-to-one mapping between ""derangement"", and ""permutation cycle representation with all cycle of length bigger than one"". So solving either is a solution to my question. Secondly, I just want to generate all such cycles/derangements for a specific n , not a specific one, or some probability, for example, given $n=4$, I expect to get the collection above, or a collection of derangements of length 4. Thirdly, the only requirement for the method/algorithm/formula is: it does not generate all the permutations and then remove the ones that does not satisfies the condition. i.e., it directly generates the answers one by one.","['permutations', 'combinatorics', 'symmetric-groups']"
2200840,Can basis of kernel be extended to a Jordan basis?,"Let $A\in\mathbb C^{n\times n}$ be nilpotent. A Jordan basis of $A$ is a basis of $\mathbb C^n$ with respect to which $A$ has Jordan normal form. Assume that we do not know the Jordan structure of $A$. Given a basis of the kernel of $A$, is there a criterion to decide on whether this basis can be extended to a Jordan basis of $A$ (maybe in terms of powers of $A^*$ or whatever)?","['jordan-normal-form', 'linear-algebra']"
2200939,Locus of decomposable elements,"Let $V$ be a vector space with basis $\{v_1,v_2,v_3,v_4,v_5\}$ and $W$ be the subspace of $\wedge^2(V)$ generated by $\{v_1 \wedge v_3, v_1 \wedge v_4, v_1 \wedge v_5, v_2 \wedge v_3, v_2 \wedge v_4, v_2 \wedge v_5 \}$. An element in $\wedge^2(V)$ is said to be decomposable if it is of the form $u \wedge v$ for $u,v \in V$. I need to find out the set of all decomposable elements in $W$. Of course the basis vectors of $W$ are decomposable but my guess is that the set is the image of $\mathbb P^1 \times \mathbb P^2$ under the Segre embedding but I have no clue how to show this.","['algebraic-geometry', 'projective-geometry', 'projective-space', 'differential-geometry', 'linear-algebra']"
2200952,Intersection of cones tangent to a common sphere,"Suppose I have two right circular cones $C1$ and $C2$ (of different size/shape) that are both tangent to a sphere $S$, as shown in the picture. In general, intersection of two conical surfaces is a nasty curve of degree 4. But in our case, because of the common tangency, I suspect that the intersection is actually just a pair of ellipses. Numerical experiments seem to suggest this, anyway. I expect this is a known result from classical geometry, but I'd like to have a proof or a reference, please. All of this is in plain ordinary 3D space. Edit I found an answer myself (see answer below). But the proof provided is essentially high-school coordinate geometry. That's fine with me -- I like high-school coordinate geometry. But I wonder if there is some more sophisticated reasoning that makes the result patently obvious without all the algebraic computations. For example, are there tricks of projective geometry that reduce the cone-cone case to the (much easier) cylinder-cylinder case.","['quadratic-forms', 'projective-geometry', 'geometry']"
2200982,Notation for partial derivatives,"I thought that the meaning of
$$
\frac{\partial f(x, y, z)}{\partial x}
$$
is differentiation on $x$ with fixed $y$ and $z$. So $(x, y, z)$ in the numerator is just saying which variables are fixed. If I need to indicate where the derivative is evaluated, I write it in the right of a vertical bar as a subscript. But today my teacher used $(x, y, z)$ in the numerator to denote where the derivative is evaluated. So, for example,
$$
\frac{\partial f(0, 0, 0)}{\partial x}
$$
means
$$
\frac{\partial f(x, y, z)}{\partial x} \bigg\rvert_{x=0,y=0,z=0}
$$
Is that a standard convention? If so, what is the meaning of
this?
$$
\frac{\partial f(x, y, g(x, y))}{\partial x}
$$
I have two candidates. One is a partial derivative of the composition of $f$ and $g$ where $g$ has some fixed value, and the other is the partial derivative of $f$ on $x$ evaluated at $(x, y, g(x, y))$. I think the two are not the same.","['partial-derivative', 'notation', 'calculus']"
2201045,Is possible to find a close form for $\sum_{k=1}^{n-1} \frac{1}{k(n-k)}$?,"$\sum_{k=1}^{n-1} \frac{1}{k(n-k)}$
Thank you for your help","['summation', 'discrete-mathematics']"
2201060,Is $\operatorname{SL}_2(\mathcal O)$ finitely generated?,"Let $F$ be a number field and $\mathcal O$ its integer ring: is $\operatorname{SL}_2(\mathcal O)$ finitely generated? It's well known that $\operatorname{SL}_2(\mathbf Z)$ is generated by $T=\begin{pmatrix}1&1\\0&1\end{pmatrix}$ and $S=\begin{pmatrix}0&-1\\1&0\end{pmatrix}$. It seems easy enough when $\mathcal O$ is a Euclidean domain, because then we can replace $T$ with $T_i:=\begin{pmatrix}1&x_i\\0&1\end{pmatrix}$ where $\mathcal O=\bigoplus x_i\mathbf Z$ and use the algebraic proof here . But what happens when $\mathcal O$ is not a Euclidean domain?","['algebraic-groups', 'algebraic-number-theory', 'group-theory']"
2201061,Differential Geometry and Categories,"Is there such a thing as a category of manifolds? If so what is the functor from that category to the one of vector spaces? (It seems natural that this would correspond to the push-forwards), sounds like a free functor somehow but I have not been able to find sufficiently good treatments on that. Also any good references combining differential geometry and category theory would be appreciated.","['category-theory', 'differential-geometry']"
2201110,Definition of integrability in the extended sense in Spivak's Calculus on manifolds.,"In page 65 of his book, Spivak is saying $\int_A \varphi . |f|$ exists if 1.$\Phi$ is a partition of unity subordinate to an open cover $O$ of $A \subset  \mathbf{R}^n$, $\varphi \in O$ 2.Discontinuity of $f:A \rightarrow \mathbf{R} $ is measure $0$ 3.$f$ is bounded in some open set around each point of $A$. But I can't understand why it exists. I know in his proof of existance of partition of unity, he actually proved each $\varphi \in O$ has compact support, so we can think above integral as integration on a subset of a rectangle $\prod^n_{i=1}[a_i,b_i]  $. but I think still $A$ should be Jordan measurable since otherwise the integral may not exist.","['multivariable-calculus', 'real-analysis', 'integration']"
2201113,Probability of decreasing random structures,"Given that we have $N$ consecutive structures that can have a maximum height of $H$, where the individual height $h_i$ depends on random coin flips (for each structure, an additional height has a probability of 0.5. The probability is given by $p(h_i=x) = 0.5^{x + 1}$, except when $x=H$, then we get $p(h_i=x)=0.5^H$ Now I want to know, given $H$, what is the probability of having a random set of $N$ structures, where each structure is followed by another structure with the same height or less? Visual Representation Probabilities and possible structures +---+  +---+  +---+       +---+
0.5^x     H   |   |  |   |  |   |  ...  |   |
              +---+  +---+  +---+       +---+
               ...    ...    ...         ...
              +---+  +---+  +---+       +---+
0.0625    3   |   |  |   |  |   |  ...  |   |
              +---+  +---+  +---+       +---+
0.125     2   |   |  |   |  |   |  ...  |   |
              +---+  +---+  +---+       +---+
0.25      1   |   |  |   |  |   |  ...  |   |
              +---+  +---+  +---+       +---+
0.5       0   |   |  |   |  |   |  ...  |   |
              +---+  +---+  +---+       +---+
p(h_i=x)  h_i   1      2      3    ...    N Structures that satisfy the search criteria with N = 3, H = 2 (the zero height-level is always present) (1) +---+  +---+  +---+
|   |  |   |  |   |
+---+  +---+  +---+
|   |  |   |  |   |
+---+  +---+  +---+
|   |  |   |  |   |
+---+  +---+  +---+ (2) +---+
|   |
+---+  +---+
|   |  |   |
+---+  +---+  +---+
|   |  |   |  |   |
+---+  +---+  +---+ (3) +---+
|   |
+---+
|   |
+---+  +---+  +---+
|   |  |   |  |   |
+---+  +---+  +---+ (4) +---+  +---+  +---+
|   |  |   |  |   |
+---+  +---+  +---+ ... Structures that do not satisfy the search criteria N = 3, H = 2 (again, the zero-level is present for all structures) (1) +---+         +---+
|   |         |   |
+---+  +---+  +---+
|   |  |   |  |   |
+---+  +---+  +---+
|   |  |   |  |   |
+---+  +---+  +---+ (2) +---+
|   |
+---+         +---+
|   |         |   |
+---+  +---+  +---+
|   |  |   |  |   |
+---+  +---+  +---+ (3) +---+
|   |
+---+         +---+
|   |         |   |
+---+  +---+  +---+
|   |  |   |  |   |
+---+  +---+  +---+ (4) +---+
              |   |
+---+  +---+  +---+
|   |  |   |  |   |
+---+  +---+  +---+ ... Partial Solution To my understanding (not a math-major), the first part (all structures having the same height-level) can be calculated by $$ p(\mbox{all same level}) = \sum_{i=1}^H 0.5^{i*n} $$ But I fail to see a possibility to calculate the second part (structures with decreasing levels). Any help is greatly appreciated. Background The solution to this problem finds the probability of having a Skip List with the worst setup, resulting in a search-time of $O(n)$ instead of $O(log(n))$ (and thus also an insertion- and deletion time of $O(n)$). Edit/Addition: Building the levels of the Structures As there are some discussions in the comments, I wanted to elaborate on the process that determines the height of each structure. The general idea is to toss a coin multiple times. If we get heads (1) that means we add a level, if we get tails (0) that means we leave the structure at the current level and move to the next structure. In pseudo-code, we would express the algorithm like this lvl = 0
while coin_flip() == heads:
    lvl = lvl + 1 Addition 2: Sampled probabilities Inspired by Jens code, I wrote my own c++-function that runs 1mil. simulations per setting and computes the probabilities. You can run the code and experiment with it here: http://cpp.sh/5vtpl (alternative link at GitHub Gist ). So far I have gotten the following results Running 1000000 simulations each:
-----------------------------------
N =   2 & H =  3 | chance = 0.671773
N =   3 & H =  3 | chance = 0.387352
N =   4 & H =  3 | chance = 0.208013
N =   5 & H =  3 | chance = 0.107015
N =   6 & H =  3 | chance = 0.054868
N =   7 & H =  3 | chance = 0.027476
N =   8 & H =  3 | chance = 0.013878
N =   9 & H =  3 | chance = 0.006773
N =  10 & H =  3 | chance = 0.003364
N =  11 & H =  3 | chance = 0.001791
N =  12 & H =  3 | chance = 0.000869
N =  13 & H =  3 | chance = 0.000439
N =  14 & H =  3 | chance = 0.000218
N =  15 & H =  3 | chance = 0.000139
N =  16 & H =  3 | chance = 0.000047
N =  17 & H =  3 | chance = 0.000022
N =  18 & H =  3 | chance = 0.000013
N =  19 & H =  3 | chance = 0.000009",['probability']
2201143,"compute $\text{Hom}(\mathbb Z_p, \mathbb Z)$ (p-adic integers)","How can I compute $\text{Hom}(\mathbb Z_p, \mathbb Z)$ or $\text{Hom}(\mathbb Z_p, \mathbb Q)$? (By $\mathbb Z_p$ I mean p-adic integers.)","['group-theory', 'p-adic-number-theory']"
2201155,A question about the PDE: $u_t-\Delta u=au-bvu$,"Consider the following system of partial differential equations (see here for more details): I just want to know how the author got that expression after multiplying by $u^-$ and integrating over $\Omega$ in the proof of Lemma 2.3. I think the function $u^-=\min(0,u)$ is not differentiable  in time.","['functional-analysis', 'lp-spaces', 'ordinary-differential-equations', 'partial-differential-equations']"
2201156,Why Study Critical Polynomials?,"In dynamical systems, I often read about the post-critical orbits. 
As in take a moduli space of functions $f$ which are self maps. Find general critical points, and see where they orbit. They would then be polynomials in some variables if we allow $f$ to be parameterised. Those are called critical polynomials. It could go by another name also, but I'm unsure. For instance the moduli space of quadratic polynomials can be a parameterised as  $f = z^2 + c : \mathbb{C} \cup \{\infty\} \to \mathbb{C} \cup \{\infty\}$, where $c \in \mathbb{C}$. Then $f$ has critical points $0$ and $\infty$, the latter being fixed.
 Iterations of $0$ would give us the critical polynomials. $0,\, c,\, c^2 + c, (c^2 + c)^2 + c\dots$ I know that they are important in dynamical systems, and their roots have some importance. But I don't know why they are important at all.","['dynamical-systems', 'algebraic-geometry', 'complex-dynamics', 'soft-question', 'arithmetic-dynamics']"
