question_id,title,body,tags
4674596,Describe $f(S)$ where S is the solution set of equation $\lfloor5\sin x\rfloor+\lfloor\cos x\rfloor+6=0$,"If $$\lfloor 5\sin x\rfloor+\lfloor \cos x\rfloor+6=0$$ then the solution set of range of $$f(x)=\sqrt{3}\cos x+\sin x$$ I tried solving this question by doing the following $5\sin x$ ranges from $[-5,5]$ So $\lfloor 5 \sin x\rfloor$ has the range $-5,-4,-3,-2,-1,1,2,3,4,5$ . Similar logic for $\lfloor\cos x\rfloor$ ; $-5$ and $-1$ satisfies $\lfloor 5\sin x \rfloor+ \lfloor\cos x\rfloor+6=0$ But substituting these values in the second equation spent yield me the answer But seems like I'm totally wrong as the correct answer would be $$\left(\frac{3Ã—\sqrt3+4}{5},-1\right)$$ I think Im missing something conceptual here. Could anyone help me out? Note : $\lfloor \cdot \rfloor$ indicates floor function","['trigonometry', 'functions']"
4674599,Do general Riemannian manifolds satisfy the SAS (side-angle-side) postulate?,"By triangle I have in mind something where all sides must be length minimizing geodesics, i.e. geodesic segments. In particular the distances between their endpoints will be the length of the geodesic. Cf. this question . Question: Assume $M$ is a connected and geodesically complete smooth Riemannian manifold. Then given points $A$ , $B$ , $C$ , $A'$ , $B'$ , $C'$ , and $d(A,B) = d(A',B')$ , $d(B,C)=d(B',C')$ , and $\angle (A,B,C) = \angle(A',B',C')$ , can we conclude that $d(A,C)=d(A',C')$ , $\angle(B,C,A)=\angle(B',C',A')$ , and $\angle(C,A,B)=\angle(C',A',B')$ ? Question 2: Same as the above but assume $M$ is additionally simply connected. (E.g. no tori.) Because this is probably a basic and well-known result, a pointer to a reference would suffice. Background/motivation/what I've considered so far: The SAS postulate is true in absolute geometry (i.e. Euclidean or hyperbolic), cf. this question . And apparently it is also true in spherical geometry using the correct notion of ""triangle"" or ""side"", i.e. length-minimizing geodesics, cf. this question . So that covers all of the ( $2$ -dimensional, but the proofs probably don't depend on the dimension) Riemannian manifolds of constant curvature. But those are a very special class, so it's not clear to me that it would generalize. There would seem to potentially be topological obstructions in general, i.e. due to the Riemannian manifold being mildly ""pathological"", hence my restriction to connected and geodesically complete (= complete metric space by Hopf-Rinow theorem ) smooth Riemannian manifolds. (To be honest those are the only Riemannian manifolds I am interested in anyway.) But even for ""non-pathological"" Riemannian manifolds, it still seems like the most possible obstruction to generalizing a result that holds for those with constant curvature is topological. E.g. with a torus, if the hypothetical missing thirds segment ""passes through the donut hole"" for $A$ and $C$ , but not for $A'$ and $C'$ , then I could imagine this failing. But would that actually be a valid counterexample? And if so, is lack of simple connectedness the only possible obstruction? A ""global"" topological obstruction (such as lack of simple connectedness) being the only possible obstruction seems conceivable to the extent that Riemannian manifolds are ""locally"" or ""infinitesimally"" Euclidean. But of course all of this is mostly hand-waving and speculation on my part. Related questions: Riemannian Triangles Third Side Bounding angles in Riemannian triangles with bounded sides","['riemannian-geometry', 'metric-geometry', 'geometry', 'examples-counterexamples', 'reference-request']"
4674642,Have generalisations of dual numbers/complex numbers/quaternions/octonions... been studied?,"Can anyone point me to any generalisations of the notions in the title? For example say you have: $$
(a_1, a_2, a_3, ...,a_n) \in \mathbb{R}^n
$$ and $$
\gamma_1, \gamma_2, \gamma_3, ..., \gamma_n
$$ such that $$
\gamma_{1}^k= \gamma_{2}^k= \gamma_{3}^k= ...= \gamma_{n}^k=c, \text{    } c \in \mathbb{R}
$$ in order to obtain some sort of generalisation of a quaternion: $$
a_1\gamma_1+ a_2\gamma_2+a_3\gamma_3+ ...,a_n\gamma_n
$$ in application to the cases mentioned in the title, the complex numbers would be $a_1 +a_2\gamma_1$ , where $\gamma_1 ^2=-1=i^2 $ , or the dual numbers $a_1 +a_2\gamma_1$ where $\gamma_1 ^2=0 =\epsilon^2$ and finally the quaternions, $a_1+a_2\gamma_1+a_3\gamma_2+a_4\gamma_3$ with $\gamma_1^2=\gamma_2^2=\gamma_3^2=-1 =i^2=j^2=k^2$ I don't mean to talk about say the octonions for example, although they are a generalisation of sorts, I mean to find something that is much more general... have generalisations of the type been studied? If so where?","['octonions', 'number-systems', 'dual-numbers', 'group-theory', 'quaternions']"
4674671,Show that $B \setminus A \sim B$ where $A \subseteq B$ countable and $B \setminus A$ is infinite [duplicate],"This question already has an answer here : Bijection from $A$ to $S\setminus A$, where $A$ is countably infinite (1 answer) Closed last year . Given $A\subseteq B$ such that $A$ is countable and $B\setminus A$ is infinite. I want to show that the cardinality of $B\setminus A$ is the same as the cardinality of $B$ . I know that since $B \setminus A$ is infinite, there is a function $g: \mathbb{N} \rightarrow B \setminus A$ injective. But how does this help me find a bijective function $f: B \setminus A \rightarrow B$ ?","['elementary-set-theory', 'real-analysis']"
4674686,"What is a function $f : 2^n \rightarrow n$ such that for each $y \in n$ and $A \in 2^n$ there is $x \in n$ such that $f(\tau(x,A)) = y$?","We first set up some notations and definitions: We view each positive integer $n$ as a (particular) set of $n$ elements. We also pick some bijection $\phi_n : 2^n \xrightarrow{\approx} \mathcal{P}(n)$ , where the codomain denotes the power set of $n$ . Define the function $\tau_n : n \times 2^n \rightarrow 2^n$ given by $$
\tau(x,A) = 
\left\{\begin{matrix}
\phi^{-1}( \phi(A) \setminus\{x\} ) \,, & x \in \phi(A) \\ 
\phi^{-1}( \phi(A) \cup\{x\} ) \,, & x \notin \phi(A) \\  
\end{matrix}\right.
$$ where the implied subscript on each of the $\tau$ 's, $\phi$ 's in the above equation is $n$ . Then my question is: For which positive integers $n$ does there exist a function $f_n : 2^n \rightarrow n$ such that for each $A \in 2^n , y\in n$ there exists $x \in n$ such that $f_n(\tau_n(x,A)) = y$ ? And for such $n$ , how would we find such a function? For background, the above question arises in considering the following puzzle: Alice and Bob are in prison, and the warden suggests a deal to them, where they will go free if they can win the following game: 0. First Alice and Bob are allowed to devise a strategy on their own, after being told about this game. However, after they agree to start the game, Alice and Bob may have no further contact. 1. The warden shows Alice an oriented $m\times m$ grid of coins, each of which will be randomly in either the heads or tails position. 2. Next, the warden points to a (randomly chosen) particular coin position, say position $P$ , which is seen by Alice. (He doesn't change the coins in any way.) 3. Alice is then required to flip exactly one coin, of her choosing. (She may not avoid flipping any coins.) 4. Alice leaves and Bob is shown the grid of coins. The two have had no contact since Alice saw the coins and Bob has not seen the coins before this. 5. Bob must determine the position $P$ , with no further input aside from looking at the coins. If this is accomplished, then Alice and Bob go free. The question is, for which $m$ is a winning strategy possible, and what would be such a strategy? Note: with the above notation, $n=m^2$ .",['discrete-mathematics']
4674693,"Let $F_m$ be the $mth$ Fibonacci number given by $F_1=F_2=1$ and $F_{m+2}=F_m+F_{m+1}$ for all $m\geq 1.$ Show that $\sum\binom nk=F_{m+1},$ [duplicate]","This question already has answers here : How to show that this binomial sum satisfies the Fibonacci relation? (6 answers) Closed last year . Let $\binom{n}{k}$ denote the binomial coefficient $\frac{n!}{k!(n-k)!}$ and $F_m$ be the $mth$ Fibonacci number given by $F_1=F_2=1$ and $F_{m+2}=F_m+F_{m+1}$ for all $m\geq 1.$ Show that $\sum\binom nk=F_{m+1},$ $\forall m\geq 1.$ Here, the above sum is over all pairs of integers $n\geq k\geq 0,$ with $n+k=m.$ My solution goes like this: We see that $\sum\binom nk=F_{m+1},$ $\forall m\geq 1,$ is valid, for $m=1.$ Now, assuming $\sum\binom nk=F_{m+1},$ is true. It is to be noted, $n\geq k\geq 0,$ with $n+k=m.$ This, means, $n=m-k$ and $$n\geq k\implies m\geq 2k\implies k\leq \frac m2.$$ We observe, if $$k=0,1,2,\cdots, \lfloor \frac m2\rfloor$$ , $$n=m,m-1,m-2,\cdots,m-\lfloor \frac m2\rfloor$$ , respectively. $$\sum\binom nk=\binom{m}{0}+\binom{m-1}{1}+\binom{m-2}{2}+\cdots +\binom{m-\lfloor \frac m2\rfloor}{\lfloor \frac m2\rfloor}=F_{m+1}.$$ Now, we try to represent $\sum\binom nk=F_{m},$ , such that $n\geq k\geq 0,$ with $n+k=m-1,$ in the same form as above. This, means, $n=m-k-1$ and $$n\geq k\implies m-1\geq 2k\implies k\leq \frac {m-1}{2}.$$ We again observe, if $$k=0,1,2,\cdots, \lfloor \frac {m-1}{2}\rfloor$$ , $$n=m-1,m-2,\cdots,m-\lfloor \frac {m-1}{2}\rfloor-1.$$ So, $$\sum\binom nk=\binom{m-1}{0}+\binom{m-2}{1}+\cdots +\binom{m-1-\lfloor \frac {m-1}{2}\rfloor}{\lfloor \frac {m-1}{2}\rfloor}=F_{m}.$$ Now, for $m+1$ we should have, $\sum\binom nk=F_{m+1},$ such that $n\geq k\geq 0,$ with $n+k=m+1.$ Also, if $n\geq k$ , then, $m+1\geq 2k\implies k\leq \frac {m+1}{2}.$ We observe,  if $$k=0,1,2,\cdots, \lfloor \frac {m+1}{2}\rfloor$$ , $$n=m+1,m,\cdots,m-\lfloor \frac {m+1}{2}\rfloor+1.$$ So, $$\sum\binom nk=\binom{m+1}{0}+\binom{m}{1}+\binom{m-1}{2}+\cdots +\binom{m+1-\lfloor \frac {m+1}{2}\rfloor}{\lfloor \frac {m+1}{2}\rfloor}=F_{m+2}.$$ Now, we compute, $$F_{m+1}+F_m=(\binom{m}{0}+\binom{m-1}{1}+\binom{m-2}{2}+\cdots +\binom{m-\lfloor \frac m2\rfloor}{\lfloor \frac m2\rfloor})+(\binom{m-1}{0}+\binom{m-2}{1}+\cdots +\binom{m-1-\lfloor \frac {m-1}{2}\rfloor}{\lfloor \frac {m-1}{2}\rfloor})\implies \binom{m}{0}+(\binom{m-1}{1}+\binom{m-1}{0})+(\binom{m-2}{2}+\binom{m-2}{1})+\cdots +(\binom{m-\lfloor \frac m2\rfloor}{\lfloor \frac m2\rfloor} +\binom{m-1-\lfloor \frac {m-1}{2}\rfloor}{\lfloor \frac {m-1}{2}\rfloor}).$$ Using, Pascal's identity and the idenitity: $\lfloor\frac m2\rfloor=\lfloor\frac{m-1}{2}\rfloor +1,$ we have, $$F_{m+1}+F_m=\binom m0+\binom{m}{1}+\binom{m-1}{2}+\cdots+(\binom{m-\lfloor \frac m2\rfloor}{\lfloor \frac m2\rfloor} +\binom{m-\lfloor \frac {m}{2}\rfloor}{\lfloor \frac {m}{2}\rfloor-1})=\binom {m+1}{0}+\binom{m}{1}+\binom{m-1}{2}+\cdots+\binom{m-\lfloor \frac m2\rfloor+1}{\lfloor \frac m2\rfloor}.$$ Now, the problem is, in the expression $$F_{m+1}+F_m$$ , every term matches with $$F_{m+2}$$ except the last term, $$\binom{m-\lfloor \frac m2\rfloor+1}{\lfloor \frac m2\rfloor}$$ , which should be equivalent to, $\binom{m+1-\lfloor \frac {m+1}{2}\rfloor}{\lfloor \frac {m+1}{2}\rfloor}$ ? I don't get where is the problem occuring?","['solution-verification', 'combinatorics']"
4674701,How to compute $\sum_{k=1}^\infty \frac {1}{((2k-1)(2k+1))^2}$,I was computing a series and solved until $$\sum_{k=1}^\infty \frac {1}{((2k-1)(2k+1))^2}$$ I know Telescopic series but this doesnâ€™t seem like it can be transform into that. Upon solving this I got $$\frac{1}{4} \sum_{k=1}^\infty \frac {1}{(2k-1)^2} + \frac {1}{(2k+1)^2} + \frac {1}{(2k+1)} - \frac {1}{(2k-1)} $$,['sequences-and-series']
4674710,Is this property true for subspaces of a Hilbert Space?,"Suppose $W$ is a subspace of a Hiblert space(We can assume separability if it simplifies things) and fix an orthonormal family $\{e_{n}\}$ .(Note that this need not be an orthonormal basis as otherwise my question is trivial) . Suppose $W$ has the property that if for some $w\in W$ , we have $\langle w,e_{n}\rangle =0$ for all $n$ , then $w$ has to be $0$ . Then is $W\subset\overline{span\{e_{n}\}}$ ? The reason why I am asking is that I was reading the proof that Haar basis for $L^{2}[0,1]$ is complete . So we already have that if the integral of a continuous function is $0$ on all dyadic intervals then the function itself is $0$ . I was thinking whether we can use this to conclude for all $L^{2}$ functions by using the density of $C[0,1]$ . So to do this, I would need that $C[0,1]\subseteq\overline{span\{e_{n}\}}$ What I have tried is just naively defined $w_{n}=\sum_{n=1}^{N}\langle w,e_{n}\rangle e_{n}$ which will converge due to Bessel's inequality . Now all we have to do is show that $w_{n}\to w$ .
We directly have Cauchyness of $w_{n}$ so it does have a limit to some $w'\in\overline{W}$ . But to show that $w'=w$ , I'll end up using that $\overline{W}$ also has the property that $\langle w',e_{n}\rangle =0$ for all $n$ implies $w'=0$ . Any suggestions?","['complete-spaces', 'orthonormal', 'orthogonality', 'hilbert-spaces', 'functional-analysis']"
4674711,Is there a good way to simplify this expression?,"The Short Version Is there a way to simplify this expression? $$
\left(\left(\left(d Ã— \left(\frac{j}{2}\right)^2\right)^2 âˆ’ \frac{1}{27} Ã— \left(\frac{j^2}{2 s}\right)^6\right)^\frac{1}{2} + d Ã— \left(\frac{j}{2}\right)^2\right)^\frac{1}{3}
 + \frac{1}{3} Ã— \left(\frac{j^{2}}{2 s}\right)^2 Ã— \left(\left(\left(d Ã— \left(\frac{j}{2}\right)^2\right)^2 âˆ’ \frac{1}{27} Ã— \left(\frac{j^2}{2 s}\right)^6\right)^\frac{1}{2} + d Ã— \left(\frac{j}{2}\right)^2\right)^\frac{âˆ’1}{3}
 âˆ’ \frac{j^2}{2 s}
$$ Specifically, I'd like to condense the cube root portion and the inverse cube root portion into a single root, if possible. The Longer Version We have five positive-valued variables: $s_{max}$ $j_{max}$ $a_{max}$ $v_{max}$ $d_{max}$ We also have the function $\operatorname{Min}(n_1,\, n_2,\, â€¦,\, n_k)$ , which returns the input with the lowest value. Finally, we have three variables whose values are each based on combinations of the previously-defined variables: $$
j_{limit}\,=\,{\operatorname{Min}\begin{pmatrix}
\left(j_{max} Ã— \frac{s_{max}^0}{0!}\right)^\frac{1}{1},\\
\left(a_{max} Ã— \frac{s_{max}^1}{1!}\right)^\frac{1}{2},\\
\left(v_{max} Ã— \frac{s_{max}^2}{2!}\right)^\frac{1}{3},\\
\left(d_{max} Ã— \frac{s_{max}^3}{3!}\right)^\frac{1}{4}
\end{pmatrix}}
$$ $$
a_{limit}\,=\,{\operatorname{Min}\begin{pmatrix}
a_{max},\\
\left(v_{max} Ã— j_{limit} + \left(\frac{j_{limit}^2}{2 s_{max}}\right)^2\right)^\frac{1}{2} - \frac{j_{limit}^2}{2 s_{max}},\\
\left(\left(\left(d_{max} Ã— \left(\frac{j_{limit}}{2}\right)^2\right)^2 âˆ’ \frac{1}{27} Ã— \left(\frac{j_{limit}^2}{2 s_{max}}\right)^6\right)^\frac{1}{2} + d_{max} Ã— \left(\frac{j_{limit}}{2}\right)^2\right)^\frac{1}{3} + \frac{1}{3} Ã— \left(\frac{j_{limit}^{2}}{2 s_{max}}\right)^2 Ã— \left(\left(\left(d_{max} Ã— \left(\frac{j_{limit}}{2}\right)^2\right)^2 âˆ’ \frac{1}{27} Ã— \left(\frac{j_{limit}^2}{2 s_{max}}\right)^6\right)^\frac{1}{2} + d_{max} Ã— \left(\frac{j_{limit}}{2}\right)^2\right)^\frac{âˆ’1}{3} âˆ’ \frac{j_{limit}^2}{2 s_{max}}
\end{pmatrix}}
$$ and $$
v_{limit}\,=\,{\operatorname{Min}\begin{pmatrix}
v_{max},\\
\left(d_{max} Ã— a_{limit} + \left(\frac{a_{limit}^2}{2 j_{limit}}\right)^2\right)^\frac{1}{2} - \frac{a_{limit}^2}{2 j_{limit}}
\end{pmatrix}}
$$ The third option for the value of $a_{limit}$ stands out for being so much longer than all the other expressions in the values of the limit-variables. Can it be condensed any? Is there a better way to write it? As-is, it feels very unsatisfying.","['continuity', 'calculus', 'polynomials', 'piecewise-continuity']"
4674723,Compute the limit $\lim_{z \to ia}\arctan(\frac{y}{x})$,"Let $a\in \mathbb R$ and $z=x+iy$ . Compute the limit $$\lim_{z \to ia}\arctan\left(\frac{y}{x}\right).$$ My try: $$\lim_{z \to ia}\arctan\left(\frac{y}{x}\right)=\lim_{z \to ia}\arctan\left(\frac{\operatorname{Im}(z)}{\operatorname{Re}(z)}\right),$$ so $$\lim_{z \to ia}\arctan\left(\frac{y}{x}\right) =
\begin{cases}
\pi/2 , & \text{if $a>0$}, \\[2ex]
-\pi/2 , & \text{if $a<0$} .
\end{cases}$$ Is this correct?
Thank you very much!","['complex-analysis', 'limits']"
4674745,"Instantiating single-variable ""ordinary"" derivatives from total derivatives","We've been learning total derivatives in class and I had some questions about how our ""ordinary"" single-variable derivatives are instantiated from all this. Let's say we have a differentiable real-to-real function $f: \mathbb{R} \rightarrow \mathbb{R}$ . Just to see how everything relates, I will try to compute every ""derivative-related object"" that we've learned (e.g. total, ordinary, directional etc.) By definition, the total derivative $Df(x)$ is the linear function $T : \mathbb{R} \rightarrow \mathbb{R}$ satisfying the following limit: $\lim_{h \to 0} \frac{|f(x+h)-f(x)-A(h)|}{|h|} = \lim_{h \to 0} |\frac{f(x+h)-f(x)}{h} - \frac{A(h)}{h}| = |\lim_{h \to 0}[\frac{f(x+h)-f(x)}{h}-\frac{A(h)}{h}]|$ $= |f'(x) - lim_{h \to 0}\frac{A(h)}{h}|$ (I believe it can be shown that for all x, $Df(x)$ exists iff $f'(x)$ exists?) This makes it clear that, if $Df(x)$ exists, then it must be $(Df(x))(v) = f'(x)v$ The directional derivative $D_{v}f(x)$ is simply the total derivative evaluated at v i.e. $D_{v}f(x)=(Df(x))(v)=f'(x)v$ Thus we get that: $$(1) f'(x) = Df(x)(1) = D_{1}f(x)$$ or equivalently $$ (2) f'(x) = Df(x)(e_{1}) = D_{e_{1}}f(x)$$ where $D_{e_{1}}f(x)$ is the partial derivative of f wrt x (the only possible direction). My questions are: Is it only accidental that (1) and (2) holds or is there some special reason behind it? Given that (1) and (2), is it fair to say that partial derivatives (each of which is the total derivative evaluated on vectors of the standard basis) is a true generalization of a ""ordinary derivative"" like $f'(x)$ in higher dimensions?","['multivariable-calculus', 'derivatives', 'analysis']"
4674769,"Determine the solutions $(x,y,z)$ of the system $3^x + 4^y = 5^x$, $3^y + 4^z = 5^y$, and $3^z + 4^x = 5^z$","Determine the solutions $(x,y,z)$ for the following system: $$\begin{cases}3^x + 4^y = 5^x \\\ 3^y + 4^z = 5^y \\\ 3^z + 4^x = 5^z \end{cases}$$ My initial guess was that $(2,2,2)$ is the only possible solution, and after checking on Desmos, that is correct as far as I can tell. However, I'm not quite sure how to prove that is the only solution. What I tried is the following: $$4^y = 5^x-3^x \iff y\ln(4) = \ln(5^x-3^x) \iff y = \frac{\ln(5^x-3^x)}{\ln(4)} \\\ z = \frac{\ln(5^y-3^y)}{\ln(4)} \\\ x = \frac{\ln(5^z-3^z)}{\ln(4)} \ \text{and by substituting } z \text{ with } y \text{ and } y \text{ with } x \text{ I reach an equation only in } x.$$ From where what I would normally do is creating a helping function by subtracting one of the sides of the equation and then trying to find the roots (although in this case it would be the numbers of roots) of that function by differentiating, based on monotony. However, differentiating such a function is probably not the point of this problem, so I'm quite stuck with how one should start (I'm also thinking that I shouldn't prove that $2$ is the only solution by guessing, but to prove that $2$ is the solution algebraically). I'd appreciate any hints towards solving this exercise!","['algebra-precalculus', 'systems-of-equations', 'exponential-function']"
4674821,Comparing two hypothetical investment opportunities.,"I came across a problem comparing two hypothetical investment opportunities. in both cases, you will invest some money and get a daily return on it. However, the return will follow one of these roles: You will get $\$35$ each day. You will have a $36\%$ probability of getting $\$70$ each day, however, if you did not receive it, the probability the next day will increase by $20\%$ . and if you receive it, the probability will reset back to $36\%$ .
(for example, if you did not receive money on day $1$ , you will have a probability of $56\%$ of getting the $\$70$ . And if you did not get any money on days $1$ and $2$ , the probability of getting $\$70$ on the third day is $76\%$ . and so on) I tried to formulate it mathematically and found that the second option is better, On the other hand, I model the problem in Excel and found that both of them are equal. my mathematical solution was based on: First: find the success probability of some sequenced days, and then average the $\$70$ on these days. \begin{align*}
\Bbb{P}(\text{return at day 1}) &= 36\%\\
\Bbb{P}(\text{return at day 2}) &= \Bbb{P}(\text{no return at day 1})\cdot (36\% + 20\%) = 35.84\%\\
\Bbb{P}(\text{return at day 3}) &= \Bbb{P}(\text{no return at day 1})\cdot\Bbb{P}(\text{no return at day 2}) \cdot (36\% +2 \cdot 20\%)= 21.4016\%\\
\Bbb{P}(\text{return at day 4}) &= 6.488\%\\
\Bbb{P}(\text{return at day 5}) &= 0.27\%
\end{align*} Second, find the equivalent daily return after getting the \$70, which is: $\$70$ if you get the return on the first day. $\$35(70/2)$ if you get the return on the second day. $\$23.33(70/3)$ if you get the return on the third day. $\$17.5 then \$14$ for the fourth and fifth days. finally, the total equivalent daily return of the second option is: $$Eq_r = \sum_{i=1}^5  P(\text{return at day i}) \cdot \frac{70}{i} =43.91 $$ What option will result in more return over time? Notes: The probabilities in the second option will increase by $20$ until it reaches $100\%$ , so they are $\{36,56,76,96,100\}$ . The investment in both cases will go on forever.","['statistics', 'probability']"
4674830,How to rewrite $\frac{1}{\sum_{j=1}^{N}\frac{1}{R_j}}$?,"This is a very simple question but I don't seem to find a solution online.
I would like to rewrite this sum which appears very often in series and parallel electrical circuits. $$R_{\text{Eq}} = \frac{1}{\sum_{j=1}^{N}\frac{1}{R_j}}$$ In the case of two resistances, we have: $$R_{\text{Eq}} = \frac{1}{\frac{1}{R_1} + \frac{1}{R_2}} = \frac{1}{\frac{R_1 + R_2}{R_1R_2}} = \frac{R_1R_2}{R_1+R_2}$$ In the case of three, we have: $$R_{\text{Eq}} = \frac{1}{\frac{1}{R_1} + \frac{1}{R_2} + \frac{1}{R_3}} = \frac{1}{\frac{R_1R_2 + R_1R_3 + R_2R_3}{R_1R_2R_3}} = \frac{R_1R_2R_3}{R_1R_2+R_1R_3+R_1R_2}$$ And so on... With this in mind, I would like to write something like: $$R_{\text{Eq}} = \prod_{j=1}^{N} R_j \sum_{j=1}^{N} \frac{1}{\cdots}$$ But I don't have enough knowledge in math to write those combinatory terms in the denominator. Thanks for reading!","['physics', 'algebra-precalculus', 'summation']"
4674831,Why is this solution about combinations correct?,"This is Exercise 9 from Section 3.5 of Book of Proof by Hammock. This problem concerns lists of length 6 made from the letters A,B,C,D,E,F,
without repetition. How many such lists have the property that the D occurs
before the A? I solved it, correctly, as follows: The case when D appears first in the list: $ \underline{1} \; \underline{5} \; \underline{4} \; \underline{3} \; \underline{2} \; \underline{1} = 120 $ The case when D appears second in the list: $\underline{4} \; \underline{1} \; \underline{4} \; \underline{3} \; \underline{2} \; \underline{1} = 96$ The case when D appears third in the list: $ \underline{4} \; \underline{3} \; \underline{1} \; \underline{3} \; \underline{2} \; \underline{1} = 72 $ The case when D appears fourth in the list: $\underline{4} \; \underline{3} \; \underline{2} \; \underline{1} \; \underline{2} \; \underline{1} = 48 $ The case when D appears fifth in the list: $  \underline{4} \; \underline{3} \; \underline{2} \; \underline{1} \; \underline{1} \; \underline{1} = 24 $ The sum is the answer: $ 120 + 96 + 72 + 48 + 24 = 360 $ However, the correct answer (that relates to combinations) is as follows: Begin with six blank spaces and select two of these spaces. Put the D in the first selected space and the A in the second. There are $ C(6, 2) = 15 $ ways of doing this. For each of these 15 choices there are 4! = 24 ways of filling in the remaining spaces. Thus the answer to the question is 15Ã—24 = 360 such lists. I do not understand why $ C(6,2) = 15 $ accounts for the possible combinations of $ A $ and $ D $ , although I do understand the arithmetic behind it: $ \frac{6!}{2!(6-2)!} = \frac{30}{2} = 15 $ . I thought $C(6,2)$ represented the possible non-repeated subsets of length $ k $ from a set with $ n $ elements. Therefore, would not C(6,2) include all elements of the set $ \{A, B, C, D, E, F, G\} $ ? Could someone explain this solution?",['combinatorics']
4674841,Generalization of $y-x$ vector space,"I was wondering about, if we have two vectors $x=\begin{pmatrix} 2 \\ 1 \end{pmatrix}$ and $y=\begin{pmatrix} 3 \\ 2 \end{pmatrix} \in \Bbb R^2$ , then $y-x=\begin{pmatrix} 1 \\ 1 \end{pmatrix}$ . If we think about the subspace spanned by this $y-x$ vector, then we know it could be represented by the line along the $\begin{pmatrix} 1 \\ 1 \end{pmatrix}$ diagonal. However, Iâ€™m wondering how this generalizes to higher dimensions. Imagine the $3 \times 2$ case, where $x$ and $y$ are both $3 \times 2$ matrices and would represent planes in three dimensional space. How do you represent the difference between the basis vectors? I imagine you canâ€™t just look at $y-x$ because then youâ€™re only looking at the difference between the first column of $y$ and the first column of $x$ and so on for the second column. I hope this idea makes sense, thanks if possible.","['matrices', 'vectors', 'linear-algebra', 'vector-spaces']"
4674856,Why does the largest square inside a triangle share a side with said triangle?,"There are many questions on this side asking to compute the area of the largest square that fits in a triangle (equilateral or more general). All otherwise excellent answers (and some of the questions) seem however to take for granted that this square should have one of its side lying entirely on one of the sides of the triangle, while the two corners of the square not on this side should each touch one of the other sides of the triangle. When we accept this, calculating the area's is not that hard, but how do we know that the largest square does indeed have this position? Why can a square 'balancing' on one of its corners never be largest? How would you prove this?",['geometry']
4674865,Nilpotent thickenings flat over the base,"Let $R$ be a Noetherian commutative ring, $A$ a flat $R$ -algebra of finite type, $I\subset A$ an ideal such that $A/I$ is $R$ -flat. For $n\ge 1$ , is $A/I^n$ also $R$ -flat? In a similar vein, suppose $M$ is a finite $A$ -module which is $R$ -flat and consider the annihilator ideal $\text{Ann}_A(M)$ of $M$ in $A$ . Is $A/\text{Ann}_A(M)$ flat over $R$ ?","['algebraic-geometry', 'abstract-algebra', 'commutative-algebra', 'reference-request']"
4674876,Show that exists a canonical symplectomorphism $T^{*}E^{*}\cong T^{*}E$,"I need to prove that for $\pi: E â†’ M$ a vector bundle then exists a canonical symplectomorphism $T^{*}E^{*}\cong T^{*}E$ .  I star with definitions: Let be $\pi : E â†’ M$ a  vector bundle, $\pi$ is a continuous surjection,
for every $x\in M$ then $\pi^{-1}(\{x\})$ has the vectorial structure  and it is finite dimensional and such that: every point in $M$ there is a neighborhood $U$ , there is $ n$ natural number and some diffeomorphism ${\displaystyle \varphi \colon U\times {\mathbb {R} }^{n}\to \pi ^{-1}(U)}$ such that for all $x\in U$ $\pi Ï†(x, v) = x$ for all $v\in R^{n}$ the app $ v\rightarrow  Ï†(x, v)$ is a ismomorphism between $ R^{n}$ and $Ï€^{-1}(\{x\})$ . The other side we have that a symplectomorphism is a diffeomorphism between two symplectic manifolds $(M,\omega)$ and $(N,\omega')$ such that $f^{*}\omega '=\omega$ where $f^{*}$ is the pullback of $f$ Besides, $E^{*}$ is the dual of $E$ and $T^{*}E^{*}$ represent the pullback bundle. Now i guess that i need to show that exists some function $g: (T^{*}E^{*},\omega_1)\to (T^{*}E,\omega_2)$ such that $g^{*}\omega_2=\omega_1$ , so how can to start or how i need to define $g$ ? $E^{*}$ and $T^{*}E^{*}$ are simplectic manifolds and i guess that $T^{*}:E^{*}\to E$ such that $T^{*}E^{*}=\{(h,e)\in E^{*}\times E : g(h)=\pi(e)\}$ am I right? This excersise is very difficult to me, i dont know how to start, please can give me hints? Thank you. Note: I am not sure about my definitions about $T^{*}E^{*}$ just it was intuition but i will appreciate if somebody can help me understand this point too. The other side i found some recipe and i attach next, where an 1-form tautological is $\alpha= \sum \xi_i dx_i$ with $x_1,\ldots,x_n$ are coordinates in $U\subset E$ and $x_1,\ldots,x_n,\xi_1,\ldots,\xi_n$ are coordinates of $T^{*}E$ for example and the canonical 2-form is given by $$\omega=\sum d\xi_i\wedge dx_i$$ The book is Lectures on symplectic geometric , Ana Cannas Da Silva. Even this i am stuck i do not how to proceed. As a @Chris H said before, when the bundle is trivial we have the following: $T^{*}E$ is isomorphism with $$T^{*}M \times \mathbb{R^{n}}\times \mathbb {R^{n^{*}}}$$ and $T^{*}E^{*}$ is isomorphism with $$T^{*}M \times \mathbb{R^{n^{*}}}\times \mathbb{R^{n}}$$ then the canonical forms in $T^{*}E$ and $T^{*}E^{*}$ can be describe by $$\sum^{n}dx_i\wedge dy_i +dr_i\wedge ds_i$$ and $$\sum^{n}dx_i\wedge dy_i +ds_i\wedge dr_i$$ . Therefore, we can define a difeomorphism by $$\phi:T^{*}E\to T^{*}E^{*} $$ by $\phi(x,y,s,r)=(x,y,r,s)$ and this satisfies that $\phi^{*}\omega_1=\omega_0$ . How can continued by the general case? Thank you","['geometric-topology', 'vector-bundles', 'symplectic-geometry', 'differential-geometry']"
4674878,"Does quasi-finite and ""Ã©tale locally"" closed enough to imply finite?","Let $f:X\to Y$ be a morphism of schemes of finite type. Then it is well-known that $f$ is finite iff $f$ is quasi-finite and proper. And proper is separated and universally closed. But I'm wondering if one can check the universally closed property just by ""Ã©tale locally"", i.e. for any $S\to Y$ Ã©tale, $X\times_YS\to S$ is closed. So my question is: if $f$ is quasi-finite, separated and ""Ã©tale locally"" closed then is $f$ finite? Many thanks!","['algebraic-geometry', 'commutative-algebra']"
4674898,When is $L^2(\nu) \cap L^2(\mu)$ dense in $L^2(\mu)$?,"Let $(\Omega, \Sigma)$ be some measurable space with measures $\nu$ and $\mu$ .
So long as $\nu \ll \mu$ it makes sense to think of $L^2(\mu) \cap L^2(\nu)$ as a subspace of $L^2(\mu)$ since they'll have the same underlying set. Is there a general criteria for $L^2(\nu)$ to actually be dense in $L^2(\mu)$ ? What if we specialise to the case $\Omega = \mathbb{R}^n$ and $\mu$ the Lebesgue measure? Edit: To provide some motivation, I'm wondering in which situations it makes sense to define $-\Delta + V$ as an unbounded operator on a Hilbert space where $V$ is a multiplication operator. $-\Delta$ naturally acts on $H^1$ as an unbounded operator and I'm wondering if there's a general criterion on $V$ so that $L^2( V(x) dx )$ is dense in $L^2$ .","['measure-theory', 'lebesgue-measure']"
4674910,How to prove that a $n\times n$ matrix has distinct eigenvalues?,"I am trying to solve the following question: Let A be an $n \times n$ matrix, and suppose that the only eigenvalues of $A$ are $0,$ $1,$ and $2.$ (a) Prove that $\dim E_1(A) + \dim E_2(A) \leq \operatorname{rank} A.$ Where $E_1$ and $E_2$ is the eigenspace corresponding to eigenvalues $1$ and $2$ respectively. I have found a way to prove this if all of the eigenvalues are distinct, because then the geometric multiplicity must be 1 for each of them, and thus the dimension there must be less than or equal to Rank(A). That's the approach I've taken to the problem so far. My assumption of the eigenvalues being distinct also allows for an easy proof that A is diagonalizable. However, I have no way to prove my assumption, given the information in the question, that each of the eigenvalues is actually distinct. I am lost on how I would prove that, if I even can. There must be a way, because I don't really see another approach to showing that dim E1(A) + dim E2(A) â‰¤ rank A","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
4674912,"Weakly mixing implies ""weakly mixing on all orders""","Suppose a probability perserving system $(X,\mathcal{B}_X,\mu,T)$ is weakly mixing (here $\mathcal{B}_X$ is Borel algebra of topology space $X$ and $\mu$ is a finite/probability measure), i.e. $$
\frac{1}{N}\sum_{n=0}^{n=N-1}|\mu(A\cap T^{-n}B) - \mu(A)\mu(B)| \xrightarrow{N\to+\infty} 0
\quad \forall A,B \in \mathcal{B}_X
$$ And there are many equivalent description of weakly mixing. In Furstenberg's ergodic proof of SzemerÃ©di's theorem (One can refer Furstenberg's paper in 1977) he proved weakly mixing implies ""weakly mixing on all order for sets"" like $$
\frac{1}{N}\sum_{n=0}^{n=N-1}|\mu(T^{-n}A_1 \cap T^{-2n}A_2 \cap \cdots \cap T^{-kn}A_k) - \mu(A_1)\cdots \mu(A_k)| \xrightarrow{N\to+\infty} 0
\quad \forall A_1,..,A_k \in \mathcal{B}_X\tag*{(A)}
$$ In almost all existing references on this topic a so-called ""van der Corpt trick
"" on Hilbert space is used and by that technique one can prove stronger results than (A) for commutative weakly mixing operators. etc. I suspect there is a more-straight way to prove (A) because using induction it is sufficient to show weakly mixing implies $$
\frac{1}{N}\sum_{n=0}^{n=N-1}|\mu(T^{-n}A_1 \cap T^{-in}A_2 \cap T^{-jn}A_3) - \mu(A_1)\mu(A_2) \mu(A_3)| \xrightarrow{N\to+\infty} 0
\quad \forall A_1,A_2,A_3 \in \mathcal{B}_X,i,j\in\mathbb{N}^+\tag*{(B)}
$$ which means something like ""3-weakly mixing"". If one can prove (B) then using some estimations on measures one can prove (A) by induction. My question is does there exist a direct way to get (B)?","['measure-theory', 'dynamical-systems']"
4674919,"Show that every real matrix $B$ such that $AB = BA$ has the form $B = aI + bA + cA^2$ for some real numbers $a$, $b$ and $c$.","I am trying to solve the following problem: Let $$A = \begin{pmatrix} 2 & -1 & 0 \\ -1 & 2 & -1 \\ 0 & -1 & 2 \end{pmatrix}.$$ Show that every real matrix $B$ such that $AB = BA$ has the form $$B = aI + bA + cA^2$$ for some real numbers $a$ , $b$ and $c$ . My attempt: Let us consider the set $$V=\{B\in\mathcal{M}_{3\times 3}(\mathbb{R}) : AB = BA\}.$$ It is easy to prove that $V$ is a subspace of $\mathcal{M}_{3\times 3}(\mathbb{R})$ . Likewise, we can verify that $\mathcal{B}=\{I,A,A^2\}$ is a linear independent subset of $V$ , because $x$ , $Ax$ , and $A^2x$ are linear independent vectors for $x=(1,0,0)^T$ . If we proved that the dimension of $V$ is $3$ , then $\mathcal{B}$ would be a basis of that subspace, and the proof would be done. However, I don't know how to show it. Can you give me an advice to do it, or tell me a different way to solve the problem?","['matrices', 'linear-algebra', 'vector-spaces']"
4674927,Eigenvectors of $\text{diag}(x) - xx^T$,"Given a vector $x$ what are the eigenvectors of $\text{diag}(x) - xx^T$ ? My observations so far: If we had only the first term, the matrix would already be diagonal. The second term, on the other hand, has a diagonal form with a single nonzero entry corresponding to the direction parallel to $x$ This matrix acts on the vector $(1,1,\ldots,1)$ to yield a multiple of $x$ . I also see how to invert the matrix using the Woodbury identity.","['matrices', 'diagonalization', 'linear-algebra', 'eigenvalues-eigenvectors']"
4674936,Definition of the Limit: Question about $|f(x)-L|$ manipulation to find a suitable delta,"In the definition of the limit, often the first step to find a suitable delta is to manipulate $|f(x)-L|$ to look something like $|x-a|$ , after that we can see delta in terms of epsilon that we can choose as a guess. I would like to ask why is it that we can use this manipulation as delta and end up satisfying the statement: $|f(x)-L|<\epsilon$ whenever $0<|x-a|<\delta$ All the videos I have watched, after extracting $|x-a|$ from $|f(x)-L|$ goes like 'look! we have found an appropriate delta, now we have to check if this works'. I am having trouble understanding how this indeed satisfies the condition, or how the extraction provides us with a good guess for delta. Say for example, given $\lim_{x\to 1}(2x+3)=5$ We write $|2x+3-5|$ , and then manipulate that to be: $|2x-2|$ , $2|x-1|$ . We found $|x-a|$ (in this case, $|x-1|$ ), so we say that the suitable $\delta$ is $\epsilon/2$ , because we manipulated $|2x-2|<\epsilon$ to be $|x-1|<\epsilon/2$ How does this manipulation give us a good guess for delta?","['limits', 'calculus', 'epsilon-delta']"
4674937,The boundedness of $L_1$ norm $\|(I+A)^{-1}\|_1$ if both $\|A\|_1$ and $\|A^{-1}\|_1$ are bounded,"Assume that $A \in \mathbb{R}^{N \times N}$ is a positive semi-definite matrix, both $\|A\|_1$ and $\|A^{-1}\|_1$ are uniformly bounded as $N \to \infty.$ Here $\| \cdot \|_1$ is the induced $L_1$ Norm.
Prove that there exists a constant $\kappa > 0$ such that $$\|(I_N+A)^{-1}\|_1 \leq \kappa $$ as $N \to \infty.$ Else show a counterexample. I attempted to demonstrate that the boundedness of $\|(I_N+A)^{-1}\|_1$ can be proven using the definition of $\| \cdot \|_1$ as follows $$ \|(I_N + A)^{-1} \|_1 = \sup_{x \neq 0} \frac{ \|(I_N + A)^{-1} x\|_1 }{\|x\|_1} = \sup_{z \neq 0} \frac{ \|z\|_1 }{\|(I_N+A)z\|_1} \leq \sup_{z \neq 0} \frac{ \|z\|_1 }{|\|Az\|_1 - \|z\|_1|} = \sup_{z \neq 0} \frac{1}{ \Big| \frac{\|Az\|_1}{\|z\|_1} - 1 \Big| } $$ However, it seems possible that $\inf_{z} \Big|\frac{\|Az\|_1}{\|z\|_1} - 1 \Big|=0$ , so I doubt this proof is wrong. But intuitively, $(I_N + A)^{-1}$ seems to be smaller than $A^{-1}$ , and when we consider $L_2$ norm, it is easy to find that $\|(I_N + A)^{-1}\|_2 \leq 1$ , so I guess the boundedness still holds when we consider $L_1$ norm. Any guidance on how to tackle this problem? Thanks a ton!",['linear-algebra']
4674943,Name for this type of Markov process?,"I'm experimenting with this type of stochastic process and I'm wondering if there is a specific name for it. So far I've described it as a discrete-time continuous-state Markov process but curious to see if there's a special name. I have random variables $X_t$ where $t \in \mathbb{N}$ on the state-space $\mathbb{R}$ , given by: $X_{t+1}=
    \begin{cases}
      0, & \text{with prob.}\ \alpha \\
      X_t +\mu, & \text{with prob.}\ 1-\alpha
    \end{cases}$ where $\mu \in \mathbb{R}$ . Thanks in advance!","['stochastic-processes', 'statistics', 'definition', 'probability']"
4674947,"Is my solution ok? Exercise 3 on p.45 in Exercises 2C in ""Measure, Integration & Real Analysis"" by Sheldon Axler.","I am reading ""Measure, Integration & Real Analysis"" by Sheldon Axler. The following exercise is Exercise 3 on p.45 in Exercises 2C in this book. Exercise 3 Give an example of a measure $\mu$ on $(\mathbb{Z}^+,2^{\mathbb{Z}^+})$ such that $$\{\mu(E):E\subset\mathbb{Z}^+\}=[0,1].$$ I solved Exercise 3. But I am not sure if my solution is ok or not. I used the following Example 2.55 on p.42 in section 2C in this book to solve Exercise 3. Suppose $X$ is a set, $\mathcal{S}$ is a $\sigma$ -algebra on $X$ , and $w:X\to [0,\infty]$ is a function. Define a measure $\mu$ on $(X,\mathcal{S})$ by $$\mu(E)=\sum_{x\in E} w(x)$$ for $E\in\mathcal{S}.$ [Here the sum is defined as the supremum of all finite subsums $\sum_{x\in D} w(x)$ as $D$ ranges over all finite subsets of $E.$ ] My solution is here: Let $w:\mathbb{Z}^+\to [0,\infty]$ be a function such that $w(i):=\frac{1}{2^i}$ for each $i\in\mathbb{Z}^+.$ Define a measure $\mu$ on $(\mathbb{Z}^+,2^{\mathbb{Z}^+})$ by $$\mu(E)=\sum_{x\in E} w(x)$$ for $E\in 2^{\mathbb{Z}^+}.$ Then, by Example 2.55 on p.42 in section 2C in the book, $\mu$ is really a measure on $(\mathbb{Z}^+,2^{\mathbb{Z}^+}).$ $\mu(\mathbb{Z}^+)=\sum_{x\in \mathbb{Z}^+} w(x)=1.$ So, $0\leq\mu(E)\leq\mu(\mathbb{Z}^+)=1$ for $E\subset\mathbb{Z}^+.$ Let $a\in [0,1].$ Then we can write $a=a_1w(1)+a_2w(2)+\cdots$ , where $a_i\in\{0,1\}$ for each $i\in\{1,2,\dots\}.$ Let $E_a:=\{i\in\{1,2,\dots\}:a_i=1\}.$ Then, $E_a\subset\mathbb{Z}^+$ and $\mu(E_a)=a.$ Therefore, $\{\mu(E):E\subset\mathbb{Z}^+\}=[0,1]$ holds.","['measure-theory', 'solution-verification', 'real-analysis']"
4674959,whether the difference of two ode solutions monotone,"This is a problem from my ODE course: Let $f(x,y)$ and $F(x,y)$ be continuously differentiable functions defined on an open region $\Omega$ in the plane, and for any $(x,y) \in \Omega$ , we have $f(x,y) \leq F(x,y)$ . Let the solutions to the initial value problem $y'=f(x,y)$ and $y'=F(x,y)$ with the same initial condition $y(x_0)=y_0$ be $y=\varphi(x)$ and $y=\Phi(x)$ , respectively, where $(x_0,y_0) \in \Omega$ . Let $d(x)=\Phi(x)-\varphi(x)$ . Is $d(x)$ monotonically increasing? This is not always true. I use MATLAB and find $F(x,y)=f(x,y)+1=x-y^2+1$ is a counterexample. But I need some examples that are easy to prove.",['ordinary-differential-equations']
4674961,What is the metric for this product manifold?,"Consider a spacetime $(\zeta^{3,1},g)$ where $$g=\frac{dudv}{uv}-\frac{dr^2}{r^2}-\frac{dw^2}{w^2} \quad \forall u,v,w,r \in (0,1)$$ Now this is just Minkowski space in different coordinates (related to Dirac/Light cone coordinates/null coordinates). I'm looking to take a Cauchy foliation of $\zeta^{3,1},$ change the metric back to $g'=dudv-dw^2-dr^2,$ and use the induced measure from $g'$ by means of the volume form to transform the foliated past light cone region onto a new manifold $(\Psi^{~3,1},d).$ I'm not sure how this will work so I will try to construct a $(1+1)$ dimensional example and then define a product manifold to acheive a $(3+1)$ dimensional spacetime. But the idea is similar. Let's look at the steps involved for the $(1+1)$ dimensional case $(\zeta^{1,1},u)$ where $u=\frac{dxdq}{xq}.$ The Cauchy foliation is simply $\ln(b)\ln(y)=t$ (solving for $y$ gives an explicit representation) which can then be transformed via Mellin-like transform: $$\Phi_s(t)= \int_{S=(0,1)} \exp {\frac{t s}{\ln b}}~db = \int_0^1 tb^{t-1} \exp \frac{s}{\ln b}~db = 2\sqrt{ts}K_1(2\sqrt{ts})$$ Observe that this is an unnormalized K-distribution and it's inverse transform yields an unnormalized distribution call it the ""Zeta-distribution"" (our Cauchy foliation). Observe that the Fisher information metric of the Zeta-distribution is $\Phi_s(t)$ up to a factor. Observe that the Zeta-distribution yields solutions to the Killing field $X=\langle x \ln x, -y\ln y \rangle.$ It also provides a distributional solution to the backwards heat equation with diffusivity depending on both space and time: $$\frac{\partial^2}{\partial t^2}\Phi(t,b)=-\frac{b}{t}\frac{\partial}{\partial b}\Phi(t,b)$$ In short, $\Phi_1(t)=v$ is a Lorentzian metric on the smooth $(1+0)$ manifold $(\Psi^{~1,0}, v).$ We have that $\Psi^{~3,1}=\Psi^{~1,1}\times \Psi^{~1,0} \times \Psi^{~1,0}$ so we can define the product metric on $\Psi^{~3,1}.$ What is the exact form of the metric (line element) for $(\Psi^{~3,1},v)?$ I believe it is of the form $v=v_1 \oplus v_2 \oplus v_3$ and should be expressed in terms of $K_1$ (Bessel functions). I think I am not too far off from getting the precise form. Am I on the right track here?","['semi-riemannian-geometry', 'physics', 'manifolds', 'mathematical-physics', 'differential-geometry']"
4675020,Application of Rolle's theorem for finding roots of a function and its derivative,"The question is as follows Consider a non-constant thrice differentiable function $f:\mathbb R \to \mathbb R$ , such that $\,f(x) = f(6-x) \, \forall\, x \in \mathbb R $ . And f'(0)=f'(2)=f'(5) = 0.
If n denotes the minimum no. of roots of the equation $$ [f''(x)]^2+f'(x)f'''(x) =0$$ in [0,6]. Then the minimum value of n is? The answer is 12 and the approach used requires us to find the $x \in [0,6]$ where $f'(x) = 0$ , the solution only used the equation $f(x) = f(6-x)$ Differentiating both sides $f'(x) = -f'(6-x)$ Using this equation and the fact that $f'(0)=f'(2)=f'(5) = 0$ , it deduces that $f'(x) = 0$ at $x =0,1,2,3,4,5,6$ . My question is can we use the fact $f(x) = f(6-x)$ and when x = 0, $f(0) = f(6)$ , and then by rolle's theorem $\exists \, c \in (0,6)$ such that f'(c) = 0. and this c isn't necessarily in  {1,2,3,4,5}. And we could probably extend this and find distinct $c \in \mathbb R$ for every $x \in (0,3)$ by using f(x) = f(6-x). Is my assertion correct and does it work in this question as in since we require minimum n should such c's be ignored? Edit:
1.)I came across this question similar to the one, I have asked question , and even the accepted answer under this question doesn't use Rolle's theorem to deduce existence of a possible root for f'(x) different from {0,1,2,3,4,5,6} 2.) Corrected the equation $[f''(x)]^2-f'(x)f'''(x) =0$ to $[f''(x)]^2+f'(x)f'''(x) =0$","['mean-value-theorem', 'derivatives', 'real-analysis']"
4675057,Implicit function theorem: from local to global,"Suppose that we have $F(x,y)=0$ satisfying the usual hypotheses of the IFT at $(0,0)$ , but such that not only $F_y(0,0)\neq 0$ , but $F(x,y)=0$ and $F_y(x,y)\neq 0$ for all $x\in U\ni0$ , where the open interval $U\subset\mathbb{R}$ . Then instead of having only a unique local function $y(x):(-\varepsilon,\varepsilon)\longrightarrow V'$ such that $F(x,y(x))=0$ for some $V'\subset\mathbb{R}$ , we should be able to assert the existence of a unique global function $y^*(x):U\longrightarrow V''$ such that $F(x,y^*(x))=0$ . The way I was thinking of this was the following: say we have $y=y_0$ , $\varepsilon=\varepsilon_0$ on $(-\varepsilon_0,\varepsilon_0)$ . Then we can consider $F(\varepsilon_0,y_0(\varepsilon_0))=0$ , $F_y(\varepsilon_0,y_0(\varepsilon_0))\neq 0$ and find another uniques $y_1:(\varepsilon_0-\varepsilon_1,\varepsilon_0+\varepsilon_1)\longrightarrow V'''$ and on the overlap by uniqueness $y_0\equiv y_1$ . So defining $y^*$ by cases as $y_0$ on $(\varepsilon_0,\varepsilon_0)$ and $y_1$ on $[\varepsilon_0,\varepsilon_1)$ we have, repeating this inductively, a $y_*$ globally defined not only on $(-\varepsilon_0,\varepsilon_1)$ , but on $U$ . However this can be only if the sequence $\varepsilon_i$ does not decrease too fast and the boundary values $y_i(\varepsilon_i)$ satisfy $F_y(\varepsilon_i,y_i(\varepsilon_i))\neq 0$ and $F(\varepsilon_i,y_i(\varepsilon_i))=0$ at each step. The second of these issues is taken care of by the hypothesis that $F(x,y)=0$ for every $x\in U$ , since this ensures that if $F(\varepsilon_i,y_i(\varepsilon_i))\neq0$ for some $i$ , we have covered the whole of $U$ (I am thinking only about its right half, the reasoning extends symmetrically). My question is: what are usual hypotheses which on the other hand ensure that the $\varepsilon_i$ do not decrease too fast? Are the ones given above already enough? Is it possible to devise one that ensures this? I think perhaps by contradiction one can show that the $\varepsilon_i$ can only stop at the boundary of $U$ , since if, say, they converge at some $-u<b<u$ , where $U=(-u,u)$ for example. Then one can take $b$ and apply the IFT there, extending the unique solution a little further, against the hypothesis that it could not be done. Is this the right idea? Thank you all for any help.","['implicit-function', 'real-analysis', 'multivariable-calculus', 'calculus', 'implicit-function-theorem']"
4675089,A circle is inscribed in a triangle; prove that a certain angle is bisected.,"My student showed me the following question. A circle inscribed in triangle $ABC$ touches $AB$ at $D$ , $BC$ at $E$ , $AC$ at $F$ . $G$ is on $DE$ such that $FG\perp DE$ . Prove $FG$ bisects $\angle{AGC}$ . I managed to prove this using cartesian coordinates, but my proof takes about two pages of equations. I'm looking for a more elegant proof. (Here is an outline of my proof. Let the circle have cartesian equation $x^2+y^2=1$ and assign coordinates: $D\space (-p,-\sqrt{1-p^2})$ , $E\space (p,-\sqrt{1-p^2})$ , $F\space (-q,\sqrt{1-q^2})$ . In terms of $p$ and $q$ , find the equations of the lines that make the three sides of the triangle, then find the coordinates of $A$ , $C$ and $G$ . Then show that the sum of gradients of $AG$ and $CG$ is $0$ , so $FG$ bisects $\angle{AGC}$ .)","['euclidean-geometry', 'geometry']"
4675125,Solve the following differential equation: $\frac{d^2y}{dx^2}-y=2+5x$ in the mentioned method.,"I was reading a book called ""Introductory course in Differential Equations "" by D.A Murray. A problem solving approach was presented in the book, which goes as follows: Solve $(D^3+3D^2+2D)y = x^2$ The solution given was : The roots of $f(D)$ are $0,-2, -1$ ; and hence the complementary function is $c_1 + c_2e^{-2x}+ c_3e^{-x}.$ The particular integral $$\frac{1}{D^3+3D^2+2D}x^2=\frac{1}{2D}(1+\frac 32D+\frac {D^2} {2})^{-1}(x^2)=\frac{1}{2D}(1+\frac 32D+\frac {7D^2} {4}+...)(x^2)=\frac{1}{2D}(x^2-3x+\frac 72)=\frac{x}{12}(2x^2-9x+21),$$ and $\frac 1D x$ being merely $\int xdx.$ The complete solution is $$y=c_1 + c_2e^{-2x}+ c_3e^{-x}+\frac{x}{12}(2x^2-9x+21).$$ Then, they asked to: Solve $$\frac{d^2y}{dx^2}-y=2+5x$$ by this method ( method described above). I tried solving it,like this: Given, $$\frac{d^2y}{dx^2}-y=2+5x.$$ This implies $(D^2-1)y=2+5x.$ Now, we wish to calculate solution of the equation $(D^2-1)y=0,$ i.e the complementary function. The roots of the function $f(x)=x^2-1=0,$ is $x=\pm 1.$ So, the complementary function is $c_1e^{1}+c_2e^{-1}.$ Now, we proceed to calculate the particular integral of $$\frac{d^2y}{dx^2}-y=2+5x.$$ This is given by $y=\frac{1}{D^2-1}(2+5x).$ We can write this as $$y=\frac{1}{D^2-1}(2+5x)\implies \frac{1}{D}(D-\frac 1D)^{-1}(2+5x)=\frac 1D(1+D-\frac 1D-1)^{-1}(2+5x)=\frac 1D[1-(D-\frac 1D-1)+(D-\frac 1D-1)^2](2+5x)=\frac 1D(1-D+\frac 1D+1+D^2+\frac{1}{D^2}+1-2-2D+2\frac 1D)(2+5x)=\frac 1D[(2+5x)-D(2+5x)+\frac 1D(...$$ But the problem lies here. I dont have to worry about the terms of the form $D^n$ where $n\geq 2$ as then $D^n(2+5x)=0$ but the problem lies for the terms of the form $\frac{1}{D^n},n\in\Bbb Z,$ in the infinite series. How should I go on evaluating  this? I am not quite getting it...","['calculus', 'ordinary-differential-equations']"
4675140,The proof of additivity of total variation of complex measure by Axler.,"I'm reading the proof of ""total variation function is positive measure"" on p.261 in Measure, Integration and Real Analysis by Axler.
(The book pdf: https://measure.axler.net/MIRA.pdf ) Let $\nu$ be a complex measure on measurable space $(X,\mathscr S)$ , and $|\nu|:\mathscr S\to[0,\infty]$ be the total variation of $\nu$ , i.e., $$|\nu|(E)
=\{|\nu(E_1)|+\cdots+|\nu(E_n)| \ ;\ n\in\mathbb N, E_1,\cdots,E_n \in\mathscr S \ \mathrm{are \ disjoint,\ }\ E_1\cup\cdots\cup E_n\subset E \}.$$ Then, $|\nu|$ is a positive measure on $(X,\mathscr S)$ . What I have to see is: (i) $|\nu|(\emptyset)=0$ (ii) $|\nu|(\cup_{k=1}^\infty A_k)=\sum_{k=1}^\infty|\nu|(A_k)$ for disjoint $\{A_k\}_k\subset\mathscr S.$ I understand (i), and for (ii), I understand $|\nu|(\cup_{k=1}^\infty A_k)\leqq\sum_{k=1}^\infty|\nu|(A_k)$ but I don't see the proof of the other direction. The written proof is as follows. Fix $m\in\mathbb N.$ For each $k\in\{1,\cdots,m\}$ , suppose $n_k\in\mathbb N$ and $E_{1,k}\ ,\dots,E_{n_k\ ,k}\in\mathscr S$ are disjoint s.t. $$E_{1,k}\cup\cdots\cup E_{n_k\ ,k}\subset A_k.$$ Then, $\{E_{j,k}\ ; 1\leq k\leq m, 1\leq j\leq n_k\}$ is a disjoint collection of sets in $\mathscr S$ that are contained in $\cup_{k=1}^\infty A_k.$ Thus $$\sum_{k=1}^m\sum_{j=1}^{n_k}|\nu(E_{j,k})|\leqq |\nu|(\cup_{k=1}^\infty A_k).$$ Taking the spremum of the left side over all choices of $\{E_{j,k}\}$ , we get $$\sum_{k=1}^m|\nu|(A_k)\leqq |\nu|(\cup_{k=1}^\infty A_k).$$ Letting $m\to\infty$ , I get the result. I don't understand the part Taking the spremum of the left side over all choices of $\{E_{j,k}\}$ , we get $$\sum_{k=1}^m|\nu|(A_k)\leqq |\nu|(\cup_{k=1}^\infty A_k).$$ Now, for each $k,$ we have, by the definition, $$|\nu|(A_k)=\{|\nu(E_{1,k})|+\cdots+|\nu(E_{n_k\ ,k})| \ ;\ n_k\in\mathbb N, E_{1,k}\ \dots,E_{n_k\ ,k} \in\mathscr S \ \mathrm{are \ disjoint\ }\ E_{1,k}\cup\cdots\cup E_{n_k\ ,k}\subset A_k \}.$$ At the first part, we picked arbitrarily $n_k\in\mathbb N$ and $E_{1,k},\dots,E_{n_k,k}\in\mathscr S$ so that they are disjoint and satisfy $$E_{1,k}\cup\dots\cup E_{n_k,k}\subset A_k,$$ so I think it is appropriate to take the supremum over all choices of $E_{1,k}\ ,\cdots,E_{n_k,k}$ , instead of $\{E_{j,k}\}$ . If we take the supremum over the choices of $\{E_{j,k}\}$ , we have to pick $\{E_{j,k}\}$ arbitrarily at first. What's going on here? How did he take the supremum and get $\displaystyle\sum_{k=1}^m|\nu|(A_k)\leqq |\nu|(\cup_{k=1}^\infty A_k)$ ? Maybe I'm missing something easy, but I don't notice. Thanks for the explanation.","['measure-theory', 'supremum-and-infimum', 'real-analysis']"
4675180,convergence in probability (uniform),"${{X}_{1}},{{X}_{2}},...,{{X}_{n}} \text{ iid }\sim U(0,a)$ $Z=\max \{{{X}_{1}},...{{X}_{n}}\}$ I found that ${{Z}_{n}}\xrightarrow{i.p.}a$ but the question asks for me to prove ${{Z}_{n}}\xrightarrow{i.p.}\sqrt{a}$ What is the actual value ${{Z}_{n}}$ converges in probability?","['statistics', 'convergence-divergence', 'probability']"
4675201,Solve the following differential equation: $\frac{d^2y}{dx^2}+a^2y=\cos ax.$,"Solve the following differential equation: $\frac{d^2y}{dx^2}+a^2y=\cos ax.$ The solution given is as follows: The complementary function is $c_1\cos ax + c_2\sin ax$ ; the particular integral is $\frac{1}{D^2+a^2}(\cos ax)=\frac{1}{-a^2+a^2}\cos ax$ ; and thus the method fails. In this case, change $a$ to $a+h;$ this gives for the value of the particular integral, $\frac{1}{ D^2 + a^2}\ cos(a + h)x$ ; this expression, on the application of the
principle above and the expansion of the operand by Taylor's series, becomes $$\frac{1}{ -(a+h)^2 + a^2}(\cos ax - \sin ax. hx - \cos ax\frac{h^2x^2}{1.2}+...).$$ The first term is already contained in the complementary function, and hence need not be regarded here; the particular integral will accordingly be written $$\begin{align}\frac{1}{2a+h}(x\sin ax + \frac{hx^2}{1.2}\cos {ax} +\space \text{terms with higher powers of h}\space)\end{align}$$ on making $h$ approach zero, this reduces to $$\frac{x\sin ax}{2a}.$$ The complete integral is $y= c_1\cos ax + c_2\sin ax + \frac{x\sin ax}{2a}.$ Can anyone please help me, understand the Taylor Series expansion for $\cos(a+h)x$ ? They are expanding $f(x)$ about which point ? (I think there's a lack of clarity in here). Neither do I understand, the rest part after that expansion. I am not quite getting it...","['proof-explanation', 'ordinary-differential-equations']"
4675215,Can we prove Weierstrass function is not of bounded variation according to the definition of bounded variation rather than its indifferentiability?,"As we know, the Weierstrass function defined by $$
f(x) = \sum_{n=0}^{\infty} a^n \cos(b^n \pi x), 0<a<1,b \in 2\mathbb N+1,ab > 1+\frac{3\pi}{2}
$$ is differentiable nowhere (see here for a proof of the same). As a consequence, $f$ is not of bounded variation on any finite interval, ( $f$ is said to be of bounded variation on $[a,b]$ if for any partition $a=x_0<x_1<x_2<\cdots<x_{n-1}<x_n=b$ , $\sum_{k=1}^n|f(x_k)-f(x_{k-1})|\leq M$ for a constant $M$ independent of the choice of partition) since functions of bounded variation are differentiable a.e. (as a corollary of this and the Lebesgue theorem ). In particular, the proof that $f$ is not of bounded variation is not shown explicitly via the definition of functions of bounded variation. Question: Can we construct a sequence of partitions on any interval $[a,b]$ for which the sequence of sums $\sum_{k=1}^n|f(x_k)-f(x_{k-1})|$ increases to positive infinite, showing that $f$ is not of bounded variation on $[a,b]$ ?","['bounded-variation', 'real-analysis']"
4675237,Negating an existential conditional statement,"$âˆƒ$ quadrilaterals $x$ such that if $x$ is a parallelogram, then $x$ is a kite. I understand the above statement to be false. Using the formulae $$Â¬(âˆƒxâˆˆU)[P(x)]â‰¡(âˆ€xâˆˆU)[Â¬P(x)]\\
Â¬(pâŸ¹q)â‰¡Â¬(Â¬pâˆ¨q)â‰¡Â¬Â¬pâˆ§Â¬qâ‰¡pâˆ§Â¬q,$$ its negation is: $âˆ€$ quadrilaterals $x, x$ is a parallelogram and $x$ is not a kite. Since the original statement is false, the negation of that statement should be true. However, the above negation is also false! I am not sure what I'm doing wrong.","['quantifiers', 'predicate-logic', 'logic', 'discrete-mathematics']"
4675242,"Where can geodesics on $G:=\operatorname{SL}(n,\mathbb{R})$ intersect?","I'm using the Frobenius inner product on $\operatorname{Lie}(G)$ ; $(A|B):= \operatorname{Trace}(AB^T)$ . These notes show that the geodesics for the induced Riemannian metric (and connection) which start at the identity are of the form \begin{equation}
e^{sY^T}e^{s(Y-Y^T)}.
\end{equation} In particular, $e^{tX}$ is a geodesic when $X$ is symmetric. With respect to this question , consider a situation where $X, Y \in \operatorname{Lie}(G)$ are distinct with $X$ diagonal and $Y$ arbitrary. Question 1: Is it possible that there exist times $s, t>0$ for which \begin{equation}
e^{sY^T}e^{s(Y-Y^T)}= e^{tX}?
\end{equation} Question 2: If yes to the above, can one describe the situations in which this can happen? (Note that $X$ is assumed to be diagonal). I'm perhaps looking for an answer that contrasts/compares the situation on the sphere where the geodesics emanating from a point $p$ all come together after a fixed time ( $\pi R$ ) at the antopodal point $-p$ . I'm not really familiar with Riemannian geometry but I remember reading something about 'conjugate points' and 'Jacobi vector fields' at some point.","['geodesic', 'riemannian-geometry', 'curvature', 'lie-groups', 'differential-geometry']"
4675311,Suppose $G$ and $H$ are two countably infinite abelian groups s.t. every nontrivial element of $G\times H$ has order $7$. Then $G\cong H$.,"Suppose $G$ and $H$ are two countably infinite abelian groups such that every nontrivial element of $G \times H$ has order $7$ . Then $G$ is isomorphic to $H$ . My idea is that each non-trivial element of $G$ and $H$ has order $7$ . Therefore, we can consider them as vector spaces over ${\mathbb{Z}}/{7\mathbb{Z}}$ . Since their order is the same, their dimension over $\mathbb{Z}/7{\mathbb{Z}}$ will also be the same. This implies that they are isomorphic as vector spaces and hence isomorphic as groups. Is my approach correct? Is there any other way to prove this ?","['direct-product', 'group-theory', 'abelian-groups', 'infinite-groups']"
4675339,Why can we only integrate top forms?,"I understand some of the mechanics behind integration over manifolds, but I am lacking on some of the intuition. In Tu's An Introduction to Manifolds , when introducing integration of differential forms (Section 23.4), he says: Our approach to integration over a general manifold has several distinguishing features: The manifold must be oriented On a manifold of dimension $n$ , one can integrate only $n$ -forms, not functions The $n$ -forms must have compact support. My current understanding is that (1) is necessary to give a sign to the integral, just as we have in $\mathbb{R}^n$ . (3) is necessary to eliminate cases where the domain is infinite (for example, to rule out $\int_{-\infty}^\infty dx$ ). However, I am less certain about why (2) is necessary. In Lee's An Introduction to Smooth Manifolds , he gives some intuition about how top forms provide a way of measuring ""signed volume"" at each point of a manifold. Thus I see why they're useful in integration, but I don't see why this rules out integrating differential forms that are not top forms (including $0$ forms, i.e. functions). What is the intuition behind this?","['integration', 'differential-forms', 'smooth-manifolds', 'differential-geometry']"
4675358,Maximising the expected sum of random variables below a given threshold,"I'm trying to understand the solution to Jane Street's Robot Long Jump puzzle. The optimal strategy ""waiting until a robotâ€™s position was at least some threshold $x$ and then jumping"" makes sense to me, however, I'm not sure what's behind $(x^3 - 3x + 2)e^x = 3x$ equation for the threshold value. First of all, the solution of this equation $\sim0.416195355$ is not in vicinity of what my numerical simulation produces ( $\sim0.302$ ; the result fluctuates a bit from run to run). Secondly, I fail to come up with the analytical solution that agrees with either Jane Street's equation or simulation. Here is my attempt: Expected value of the jump after a single wait is $\int_x^{1}t\,dt=\frac{1-x^2}{2}$ as PDF for uniform distribution is $1$ . Expected value of the waiting further is harder to calculate as it gets recursive. I tried using Irwin-Hall PDF with increasing number of steps but I reckon this is wrong: $$\sum_{n=2}^{\infty}\int_x^1\frac{x^{n-1}}{(n-1)!}\, tdt=\sum_{n=2}^{\infty}\frac{x^{n-1}(1-x^2)}{2(n-1)!}=\frac{1}{2}(e^x-1)(1-x^2)$$ which then leads to the equation $\frac{1-x^2}{2}=(e^x-1)\frac{1-x^2}{2}$ or $x = \ln(2) \approx 0.69315$ . What's the correct approach to this problem?","['probability-distributions', 'probability']"
4675388,What function would be $\rm abs(\sin(x))$ over a circle (like flower petals),"Imagine I have the equation of a unit circle.
I wish to draw (programmatically) something similar to $\rm abs(\sin(x))$ running over and around the circumference. Just like drawing the external ""petals"" of a flower. How can I proceed to create this equation of a half sine wave going around the circle (it would be nice if it were a simple equation I could write in code)? Would it be easier by using the complex plane? Which is probably more difficult to program?","['functions', 'circles', 'complex-numbers']"
4675411,"Does there exist a sequence of antiderivatives $f_0,f_1,f_2,\dots$ with $f_i(0),f_i(1)$ integers?","Does there exist an infinite sequence of differentiable functions $f_0,f_1,f_2,\ldots:[0,1]\to\Bbb R$ , not all the zero function, such that, for all $i$ , $f_{i+1}'=f_i$ , and $f_i(0)$ and $f_i(1)$ are both integers? I conjecture the answer is no , though I'm unsure of how to prove it. I can answer some variations of the question, though. It is natural to ask what happens if the $1$ in the above question is replaced with other numbers. If $1$ in the above question is replaced with $\ln2$ (meaning we want $f_i(0)$ and $f_i(\ln2)$ to be integers), then there is such a sequence, namely $(e^x,e^x,e^x,\ldots)$ . Similarly, if $1$ is replaced with $\pi$ , then $(\cos x,\sin x,-\cos x,-\sin x,\cos x,\dotsb)$ is such a sequence. A little thought will reveal a generalization. Consider again the above question with $1$ replaced with some other number $\alpha$ . If $e^\alpha\in\Bbb Q$ then there is such a sequence of functions, and if both $\sin\alpha,\cos\alpha\in\Bbb Q$ then there is such a sequence of functions (in each case take some constant multiple of the sequences mentioned above). It is interesting to note that the sets $\{x:\sin x,\cos x\in\Bbb Q\}$ and $\{x:e^x\in\Bbb Q\}$ are both closed under addition and subtraction. I conjecture that this gives us all solutions to these variations. That is, I conjecture that if there exists an infinite sequence of differentiable functions $f_0,f_1,f_2,\ldots:[0,\alpha]\to\Bbb R$ , not all the zero function, such that, for all $i$ , $f_{i+1}'=f_i$ , and in addition $f_i(0)$ and $f_i(\alpha)$ are both integers for all $i$ , then in fact $\alpha\in\{x:\sin x,\cos x\in\Bbb Q\}\cup\{x:e^x\in\Bbb Q\}$ .","['indefinite-integrals', 'calculus', 'integers', 'real-analysis']"
4675439,Maximum area of a triangle with two points on the circle and third point outside the circle,"$A, B$ are two points on the circle $x^2+y^2=1.$ If a triangle is made connecting $A, B$ and point $C(5,2),$ what is the maximum area of the $\triangle ABC $ ? I guess the maximum area will be obtained when $A, B$ are at the opposite side of the diameter and diameter $AB$ is perpendicular to $OC$ , where $O$ is the origin. But I don't know how to justify or reject this guess.","['optimization', 'area', 'geometry']"
4675474,Problem about the golden ratios and circles.,"Here is a geometry problem that is driving me nuts!  I feel like I am missing something (the author glosses over proving the part I am having trouble with - really makes me feel dumb), and the problem is very easy to state. Problem: Start with two small central circles of unit diameter. Then find the radius $R$ of the two circles on their left and right. The requirement is that there exists a pair of congruent circles (dotted) that are simultaneously tangent to all the other circles. The problem also states that repeated use of the Pythagorean theorem reveals that $R_1$ is the golden ratio.  I have come up with three equations (one of them is not from the Pythagorean theorem) in three unknowns: $\quad 2 \cdot R_1+1 = R_2 + h$ $\quad (R_1+1)^2 + h^2 = (R_1+R_2)^2$ $\quad (1/2)^2 + h^2 = (R_2+1/2)^2$ A linear system of three equations in three unknowns would be easy to solve, but this isnâ€™t that unfortunately.  Any help would be greatly appreciated.",['geometry']
4675509,How to evaluate $\int[(\frac{x}{2})^x + (\frac{2}{x})^x]\log_2x\mathrm dx$?,"This integral appeared in my exam but I couldn't solve it within the time limits (~2 minutes) $$I = \int\left[\left(\frac{x}{2}\right)^x + \left(\frac{2}{x}\right)^x\right]\log_2x \mathrm dx$$ A) $\left(\frac{x}{2}\right)^x + \left(\frac{2}{x}\right)^x$ B) $\left(\frac{x}{2}\right)^x -\left(\frac{2}{x}\right)^x$ C) $\left(\frac{x}{2}\right)^x\log_2x$ D) $\left(\frac{x}{2}\right)^x\log_x2$ E) $\left(\frac{2}{x}\right)^x\log_x2$ F) $\left(\frac{2}{x}\right)^x\log_2x$ I observed that the first two terms in addition are reciprocal. However, I got failed by trying substitution techniques ( like $t = (\frac{x}{2})^x$ ) and integration by parts is making it more messy. Can you give a hint/solution (like a proper substitution or method) to solve this ?
Thanks !","['integration', 'indefinite-integrals', 'calculus']"
4675622,Interchanging limits when computing the Fourier transform,In the top answer of this post here why can we interchange the two limits $\lim_{R\rightarrow \infty}$ and $\lim_{\epsilon \rightarrow 0^+}$ ?,"['fourier-transform', 'functional-analysis', 'real-analysis']"
4675636,"The average of 10 scores is $25,$ and the lowest score is $20.$ So, the highest score must be...","The average score of $10$ students in a test is $25.$ The lowest score is $20.$ Then the highest score is: $$A.100,$$ $$B.70,$$ $$C.30,$$ $$D.75$$ The answer key suggests option $B$ as the answer. I don't understand why this differs from my solution: Let the marks obtained by each student be $x_1,x_2,...,x_{10}$ and $S=\sum_{i=1}^{10} x_i$ .  Then, we have $\frac{S}{10}=\frac{\sum_{i=1}^{10} x_i}{10}=20\implies S=200.$ If say, only one student obtained $25$ , i.e let $x_k=25$ , then, $$S-x_k=200-25=175.$$ Considering $S'=\sum_{i\neq k}x_i=175$ . Now, if $\exists x_p=100$ , then $100+(S'-x_p)=175\implies (S'-75)=75$ , thus we are getting a situation, where it's possible for a student to score hundred, then, the rest students, will have to score, $75$ marks in total. This is the highest option given. Hence, the highest possible  score can be $100.$ According to the information given, this can be a possible case, when 1 student scores 20, 8 students have a total score of  75 and one student scores 100, i.e then, average of this will still be 20. So, option $A$ is correct.","['statistics', 'means', 'solution-verification', 'average', 'algebra-precalculus']"
4675661,Solving $(x+1)(4x-3) = 2(x+1)(2x+3)$. Why does dividing by $x+1$ cause problems?,"I was just now trying to solve the following simple equation: $$(x+1)(4x-3) = 2(x+1)(2x+3)$$ In my naivety I thought dividing by $(x+1)$ would shorten the equation and lead to a quick solution, but unfortunately it lead to x disappearing from the equation. Even though I figured out that expanding both sides leads me to find the solution of $x=-1$ , I don't understand why dividing by $(x+1)$ causes such problems. Can you explain to me why that is and what I need to know to avoid such mistakes in the future?",['algebra-precalculus']
4675680,What does it mean for one theorem to depend on another?,"Recently, there is a happy result by some high-schoolers: a proof of Pythagoras by using trigonometry without using circular reasoning i.e. $\sin^2A + \cos^2A = 1$ . Good for them, hurray! But it got me wondering, what does it mean for one result to depend upon another? I can understand what it means for a result to depend upon an axiom, because one can drop the axiom, and explore the consequences. However I don't see how we could drop a theorem. And mathematics is complicated: how do we know truly whether we used one result to reach upon another. I am not posing this as a trig question (although the example may help elucidate the general principles). I'm looking for an understanding of the ""depend"" relation between theorems. Thanks.","['trigonometry', 'logic', 'meta-math']"
4675692,"In a 52-card deck, 3 cards are drawn. Calculate the probability of obtaining at least 1 ace.","I have a problem with defining the events in probaility like for this example. In a 52-card deck, 3 cards are drawn. Calculate the probability of obtaining at least 1 ace. I dont understand How to seperate the events
As for this example I multiplied the choice of an ace card among the four ace type cards I multiplied it by all the combinations of choosing 2 remaining cards among 51 cards and then I divided the whole by the choice of 3 cards among 52. I would like to understand my mistake I know the solution is to use 1-p(a)","['computational-mathematics', 'card-games', 'discrete-mathematics', 'probability-theory', 'probability']"
4675694,$ \int_{-\infty}^{\infty}\frac{-1}{T}\frac{1}{2(\cosh(\omega/T)+1)}\sqrt{\frac{1+\sqrt{1+(\omega/\Delta)^{2}}}{1+(\omega/\Delta)^{2}}}d\omega$,"I want to solve the integral \begin{align}
 X = \int_{-\infty}^{\infty}f'_{T}(\omega)\tau_\Delta(\omega)d\omega
\end{align} with $f'_T(\omega)=\frac{-1}{T}\frac{1}{2(\cosh(\omega/T)+1)}$ the derivative of the Fermi function, $\tau_\Delta(\omega)=\sqrt{\frac{1+\sqrt{1+(\omega/\Delta)^{2}}}{1+(\omega/\Delta)^{2}}}$ and $T,\Delta\in\Bbb{R}_{\ge0}$ . The integral is related to the study of the conductance of a quantum impurity at low temperatures.","['integration', 'improper-integrals']"
4675745,"Finding sets $A$, $B$, and $C$ such that $A\in B$, $B \in C$, but $A \notin C$","I am reading a book on mathematical proofs by Albert D. Polimeni, Ping Zhang, Gary Chartrand. I came across a practice problem that states Give examples of 3-sets: $A$ , $B$ and $C$ ; such that $A\in B$ , $B \in C$ but $A \notin C$ . How can I find such sets? The problem tells us $B = \{A,b_1,b_2,\ldots\}$ and $C = \{B,c_1,c_2\}$ . To me it looks as if $A \in B$ , $B \in C$ then it is always the case that $A \in C$ . How can I go about understanding this problem?","['elementary-set-theory', 'discrete-mathematics']"
4675884,"$A^{64}=A^{27}=I$, prove $A=I$","$A^{64}=A^{27}=I$ , prove $A=I$ . I tried finding the eigenvalues of A, by factoring $A^{64}-I$ but this method is too long. I'm not really sure what to do, I thought of showing $A$ is similar to $I$ but I don't know how to do that either.",['matrices']
4675921,Let $f \in C^2(\mathbb{R}^n)$ harmonic and $g(x):=tan(f(x))+e^{-|x|^2}$ bounded,"I am working on the following exercise: Let $f \in C^2(\mathbb{R}^n)$ be a harmonic function. Let further $g(x):=tan(f(x))+e^{-|x|^2}$ be bounded. Show that f is constant. My approach:
To show the above, I want to use the liouville theorem for harmonic functions.
Since I know that $f$ is harmonic, I need to show that it is also bounded:
Since $g(x):=tan(f(x))+e^{-|x|^2}$ I can rewrite to $arctan(g(x)-e^{-|x|^2})=f(x)$ . Because $arctan$ is bounded and equal to $f$ it follows that $f$ is bounded. Applying the liouville theorem shows that $f$ is constant.
Question: Is my approach correct?","['harmonic-functions', 'analysis', 'real-analysis']"
4675952,Finding closed form of the following recurrence relation,"Find the closed form of the following recurrence relation $$
a_n=a_{n-1}+6a_{n-2}+3^n   
$$ where $$
a_0=5,a_1=0
$$ So I am comfortable finding closed form of these recurrences when it is only two terms. The $3^n$ is what is confusing me. So my question is that is the characteristic polynomial of this problem $$
\lambda^2=\lambda+6+9
$$ into $$
\lambda^2-\lambda-15
$$ or am I not working with $3^n$ correctly? IF this is correct, i know we would move into finding the zeroes and subbing them back into the equation, but I am not sure that this is correct EDIT: Using the comments, I got that $a_n = 2(3)^n+3(-2)^n+3^n $ I believe this works with $a_3$ .","['recurrence-relations', 'discrete-mathematics']"
4676056,Bats in a number of Caves (THE BAT CAVES!!!),"This problem is kind of like a combinatorics type of problem, I believe.
But also has the feeling that it kind of makes use of the so-called ""pigeonhole principle"".
I recall doing a problem like this many years ago in University in a Discrete Mathematics course. See problem below: For the part of this problem, where any 7 caves in a row contains exactly 77 bats.
I just did simple algebra of 7K =77, and got K = 11 bats in each cave.
It satisfies the condition that there has to be at least 2 bats in every cave.
But I am getting confused as to how to handle the very beginning and very last of the caves.
Hope someone can provide some clarity on this.","['pigeonhole-principle', 'algebra-precalculus', 'combinatorics']"
4676077,"When $\cos(x)+\cos(y)=\sin(x)+\sin(y)=1$, $x$ or $y$ must be a multiple of $2\pi$?","We are given that $\cos x+\cos y=1$ and $\sin x+\sin y=1$ . Show that $x+y=(2n+\frac{1}{2})\pi$ , when $n$ is an integer. Also show that $x$ or $y$ must be a multiple of $2\pi$ . I could get the first result by transforming above two equations to $2\sin\frac{x+y}{2}\cos\frac{x-y}{2}=1$ and $2\cos\frac{x+y}{2}\cos\frac{x-y}{2}=1$ , which implies $\tan\frac{x+y}{2}=1$ . To get the second result, I tried several things, but could not come up with it. First, I squared the given two equations and added them up. It gave $x-y=m\pi+\pi/2$ where $m$ is an integer, but I later realised it was wrong. Can anyone find a good way of doing this?",['trigonometry']
4676100,Integral over circular disk region to Integral over rectangular region.,"I was going through the double integral over the general region in the plane and had a question. When we integrate over a circular disk : $$
D=\{(x, y) \mid \delta(x) \leq y \leq \gamma(x), a \leq x \leq b\}
$$ We do, $$
\int_a^b \int_{\delta(x)}^{\gamma(x)} f(x, y) d y d x
$$ I would like to integrate over the semi circular disk defined by, $\gamma(x)=\sqrt{1-(x-2)^2}$ and $\delta(x)=0$ .
Instead of this if I wanted to make my work easier and instead of dealing with a case where the elementary region is a circle or semi-circle, I want to make it a rectangle. Because I need the same volume, I would need the base area to be equal at every height, which essentially boils down to making the base area the same. As in both cases, the base area won't decrease as a function of height.(i) So, now let's make the area equal. $\operatorname{Radius}(r)=\frac{|b-a|}{2}$ Area of the circular disk $=$ Area of the rectangle. $$
[D=\{(x, y) \mid d \leq y \leq c, a \leq x \leq b\}
$$ $$
\frac{\pi|b-a|^2}{4}=|b-a| \times l
$$ $l=\frac{\pi|b-a|}{4}$ $l$ is $|d-c|$ I can make $c=0$ , and that would mean $d=\frac{\pi|b-a|}{4}$ Now, I do $$
\int_{\mathrm{a}}^{\frac{\mathrm{a+b}}{2}} \int_0^{\frac{\pi|b-a|}{4}} f(x, y) d y d x
$$ We do $\frac{a+b}{2}$ because I want to find the volume which has the elementary region as a semi-circle, so I should make the rectangle's area equal to the semi-circle which we can do by halving the length of the rectangle. (ii) My question is what is the conceptually wrong thing I am doing here? There must be some conceptual gap that I am not seeing. Are the statements in (i) and (ii) wrong?","['integration', 'multivariable-calculus']"
4676104,Cotangent space and pushforward,"I wonder if there is a connection between the following two notions: Pushforward: For a smooth map $f:M\to N$ between smooth manifolds $M$ and $N$ , we define the pushforward $$df:TM\to TN$$ between the tangent spaces $TM$ and $TN$ . For a derivation $X\in T_xM$ , we define $df(X)=df\vert_x(X)$ to send a smooth map $g:N\to\mathbb R$ to $X(g\circ f)$ . Cotangent Space: On the other hand, we also define the cotangent space of $M$ at $x\in M$ to be $T^*_xM$ , the dual space of $T_xM$ . If we have a chart $\phi=(x_1,\dots,x_n)$ around $x$ , we have a basis $\frac{\partial}{\partial x_i}$ with $1\leq i\leq n$ . It induces a dual basis on $T^*_xM$ , given by the linear maps $dx_i\vert _x:T_xM\to\mathbb R$ , defined by $dx_i\vert_x\big(\frac{\partial}{\partial x_j}\big)=\delta_{ij}$ (Kronecker delta). Why do we use the same notation for both? If I consider for instance the map $$x_i:\mathbb R^n\ni(x_1,\dots,x_n)\mapsto x_i\in\mathbb R,$$ it is unclear if by $dx_i\vert_x$ I mean the pushforward map or an element of the dual basis of $T_x^*\mathbb R^n$ . I figured both cannot be the same, though: If we consider $dx_i\vert_x$ as a pushforward, then $dx_i\vert_x(X)$ is a derivtion (hence a map). If we consider $dx_i\vert_x$ as a basis element of $T_x^*\mathbb R^n$ , then $dx_i\vert_x(X)$ is a real number. While writing this, I got the idea that the derivation $dx_i\vert_x(X)$ is just an element of a one-dimensional vector space with a canonical basis, so we can just identify it with the corresponding scalar. Is this the connection between both?","['pushforward', 'co-tangent-space', 'differential-geometry']"
4676119,"Let $a,b,c> 0$, $abc = 1$. Prove that $\frac{a}{(a+3)^2} + \frac{b}{(b+3)^2} + \frac{c}{(c+3)^2} â‰¤ \frac{3}{16}$.","Let $a$ , $b$ , $c$ be positive reals such that $abc = 1$ . Prove that $\frac{a}{(a+3)^2} + \frac{b}{(b+3)^2} + \frac{c}{(c+3)^2} â‰¤ \frac{3}{16}$ What I've tried : Well I was trying directly multiplying both sides by $(a+3)^2(b+3)^2(c+3)^2$ but it's too lengthy and seemed complex so any better way to solve it ?","['algebra-precalculus', 'inequality']"
4676123,Sigma-algebra used in the theorem of Lebesgue-Radon-Nikodym of Rudin's Real and Complex Analysis,"The theorem of Lebegue-Radon-Nikodym in page 121 of Rudin's Real and Complex Analysis reads as: Let $\mu$ be a positive $\sigma$ -finite measure on a $\sigma$ -algebra $\mathfrak{M}$ on a set $X$ , and let $\lambda$ be the complex measure on $\mathfrak{M}$ . Then there is a unique pair of complex measures $\lambda_a,\lambda_s$ on $\mathfrak{M}$ , called the \textbf{Lebesgue decomposition} of $\lambda$ relative to $\mu$ , such that $\lambda = \lambda_a + \lambda_s$ $\lambda_a\ll \mu$ $\lambda_s\perp \mu$ There is a unique $h\in L^1(\mu)$ such that $$\lambda_a(E) = \int_Ehd\mu$$ for every $E\in\mathfrak{M}$ . Rudin seems to be keen on using the symbol $\mathfrak{M}$ as a sigma-algebra throughout RCA. At first I thought that in the theorem of Lebesgue-Radon-Nikodym, $\mathfrak{M}$ is any sigma-algebra. However, prior to LRN, Rudin has introduced the Riesz Representation Theorem on page 40, in which $X$ is a locally compact Hausdorff space $\mathfrak{M}_F$ as the class of all $E\subset X$ which satisfy i.) $\mu(E) < \infty$ and ii.) $\mu(E) = \sup\{\mu(K):K\subset E,\text{$K$ compact}\}$ $\mathfrak{M}$ is the class of all $E\subset X$ such that $E\cap K\in \mathfrak{M}_F$ for every compact set $K$ ( Question: ) Is the sigma-algebra $\mathfrak{M}$ defined in the Riesz Representation Theorem related in any way to the sigma-algebra used in the theorem of Lebesgue-Radon-Nikodym? Or can one use really any sigma-algebra in LRN?","['measure-theory', 'riesz-representation-theorem', 'real-analysis', 'functional-analysis', 'radon-nikodym']"
4676173,What's the result of $\sum_{k=1}^{\infty} {k \cdot \left[ (1-2^{-k})^n - (1-2^{-k+1})^n \right]}$?,"Came from the problem ""Expectation of maximum times of doing $n$ times of flipping coin tests until getting the reverse side"". The probability of maximum times of flipping among $n$ independent tests should be: $$
\begin{align}
\text{Pr}(X = 1) &= \frac{1}{2^n}
\\
\text{Pr}(X = 2) &= \sum_{i=1}^n {}_n C_i \left(\frac{1}{2^2} \right)^i \left(\frac{1}{2} \right)^{n-i} = \left(\frac{3}{4} \right)^n - \left(\frac{1}{2} \right)^n
\\
\text{Pr}(X = 3) &= \sum_{i=1}^n {}_n C_i \left(\frac{1}{2^3} \right)^i \left(\frac{1}{2} + \frac{1}{2^2} \right)^{n-i} = \left(\frac{7}{8} \right)^n - \left(\frac{3}{4} \right)^n
\\
& \cdots
\\
\text{Pr}(X = k) &= \sum_{i=1}^n {}_n C_i \left(\frac{1}{2^k} \right)^i \left(\sum_{j=1}^{k-1}\frac{1}{2^j} \right)^{n-i} = \left(\frac{2^k-1}{2^k} \right)^n - \left(\frac{2^{k-1}-1}{2^{k-1}} \right)^n
\end{align}
$$ Therefore, I get the sum of series: $$
E(X) = \sum_{k=1}^{\infty} k \cdot P(X=k) = \sum_{k=1}^{\infty} {k \cdot \left[ (1-2^{-k})^n - (1-2^{-k+1})^n \right]}
$$ See also Expectation of the maximum of i.i.d. geometric random variables for similar problem. Unfortunately, there seems to be no closed-form solution to this problem.",['sequences-and-series']
4676204,Combinatorial proof $\binom{2n}{3} = 2 \binom{n}{3} + 2n \binom{n}{2}$,"Give a Combinatorical Proof for the identity $\binom{2n}{3} = 2 \binom{n}{3} + 2n \binom{n}{2}$ The LHS is pretty easy binomial(2n, 3) represents the number of ways to choose a subset of 3 elements from a set of 2n elements.
I'm struggling with the RHS to find a combinatorial story , I tried to divide the problem into two different problems, any suggestions ?","['binomial-coefficients', 'combinatorics', 'combinatorial-proofs', 'discrete-mathematics']"
4676206,Proving $S_1-S_2$ is monotone decreasing,"I have system number (1) as: $S_1â€™=-bS_1I_1+(1-c)aI_1$ $I_1â€™=bS_1I_1-aI_1$ And on the other hand I have system number (2) as: $S_2â€™=-bS_2I_2+aI_2$ $I_2â€™=bS_2I_2-aI_2$ All parameters $a,b,c>0$ . Both systems have the same initial conditions as in $S_1(0)=S_2(0)=s_0>0$ and $I_1(0)=I_2(0)=i_0>0$ . I want to prove that $S_1-S_2$ is decreasing for $s_0>\frac{a}{b}$ .
What I proved so far: I proved $S_1-S_2$ and $I_1-I_2$ are negative for all $t>0$ . Using the initial conditions then I have $S_1=  \frac{(1-c)a}{b}+(s_0- \frac{(1-c)a}{b}$$ )e^{-\int_{0}^{t} bI_1(t) \ dt}$ . Similarly, I obtained $S_2=  \frac{a}{b}+(s_0- \frac{a}{b}$$ )e^{-\int_{0}^{t} bI_2(t) \ dt}$ . From 2 and 3 we conclude that $S_1,S_2$ are monotone decreasing and that for $s_0>\frac{a}{b}$ we have $\frac{-ca}{b}<S_1-S_2<0$ . Initially we have $(S_1-S_2)â€™<0$ and because $S_1,S_2$ are $C^\infty$ then $(S_1-S_2)$ decreases initially and $$\lim_{t\rightarrow \infty}(S_1-S_2)â€™(t)=0.$$ I expressed $(S_1-S_2)â€™=F(t)(S_1-S_2)+G(t)$ where $G(t),F(t)$ are negative ( $F(t)$ starts being negative from a certain point $t_0$ in case that $F(0)>0$ ) and descending toward finite limit. Despite all that I canâ€™t seem to prove that $(S_1-S_2)(t)$ is decreasing for $t>0$ . I checked this via simulations and indeed it is the case but Iâ€™m trying by contradiction and canâ€™t find something that can lead to a contradiction in anything. Help is appreciated!","['monotone-functions', 'ordinary-differential-equations', 'real-analysis']"
4676226,Proving a set of functions $\mathbb{N}\to\mathbb{N}$ with a certain property is uncountable,"Let $A$ be a set of functions $\mathbb{N}\to\mathbb{N}$ with the following property: for each $f:\mathbb{N}\to\mathbb{N}$ there exists a function $g \in A$ such that for all $n \in \mathbb{N}$ there exists an $m \geq n$ with $f(m)<g(m)$ . Prove that $A$ is uncountable. Does anyone have an idea? I thought to suppose $A$ is countable (so there is a surjection from $\mathbb{N}$ to $A$ ) and then try to have a contradiction, but I don't know how.",['elementary-set-theory']
4676261,"$T: V \to V$ is a linear map. If $\dim(\ker T \cap \text{Im}T)\neq0$, prove $\dim\ker T^2\geq2$","$T: V \to V$ is a linear map. If $\dim(\ker T \cap \text{Im}T)\neq0$ , prove $\dim\ker T^2\geq2$ so I can deduce $\dim\ker T\geq 1$ Because the intersection of the image and the kernel, is contained in the kernel. And it is known that $\dim\ker T^2\geq \dim\ker T$ So $\dim\ker T^2\geq 1$ . But I have no idea how to prove that it can't be $1$ .",['linear-algebra']
4676311,How to reconcile the Fourier inversion on probability measures and on tempered distributions?,"In probability theory, the Fourier Inversion theorem for a Borel probability measure $\mu$ on $\mathbb{R}$ reads: $$ \mu((a,b)) + \frac{1}{2} \mu(\{ a,b \}) = \frac{1}{2\pi} \lim_{T\to\infty} \int^T_{-T}\int^b_a \hat{\mu}(t) e^{-ixt} dx dt $$ On the other hand, in  distribution theory, the Fourier Inversion theorem for a tempered distribution $u$ on $\mathbb{R}$ is exceptionally simple: $$ u = \frac{1}{2\pi} \mathcal{R}(\hat{\hat{u}}) $$ where $\mathcal{R}$ is the reflection operator. Of course, this identity has to be understood in the distributional sense. My question is: how to reconcile these two inversion theorems? My try: First, note that any Borel probability measure $\mu$ is a tempered distribution with Fourier transform given by: $$ \hat{\mu}(t) = \int_\mathbb{R} e^{ixt} d\mu(x) $$ By the Fourier Inversion theorem for a tempered distribution, for any Schwartz function $\psi \in \mathcal{S}(\mathbb{R}) $ : $$
\begin{align*}
\int_\mathbb{R} \psi d\mu &= \frac{1}{2\pi} \langle \mathcal{R}(\hat{\hat{\mu}}), \psi \rangle
= \frac{1}{2\pi} \langle \hat{\mu}, \widehat{\mathcal{R}\psi} \rangle \\
&= \frac{1}{2\pi} \int_\mathbb{R} \hat{\mu}(t) \mathcal{R}(\hat{\psi})(t) dt \\
&= \frac{1}{2\pi} \int_\mathbb{R} \hat{\mu}(t) \int_\mathbb{R} \psi(x) e^{-ixt} dx dt
\end{align*}
$$ Next I want to ""substitute"" $\psi = \chi_{(a, b)}$ , but here are two problems: The function $\chi_{(a, b)}$ is not a Schwartz function. I guess this is where we cook up the limit $T \to \infty$ . I don't know how the term $\frac{1}{2} \mu(\{ a,b \})$ pops up. Remark: To be consistent with the notation in probability theory, I define the Fourier transform of an integrable function $f$ on $\mathbb{R}$ to be $$ \hat{f}(t) = \int_{-\infty}^\infty f(x) e^{ixt} dx $$","['fourier-analysis', 'probability-theory', 'distribution-theory']"
4676328,Hartshorne Exercise V.5.5 Every geometrically ruled surface over a fixed curve $C$ is birationally equivalent,"Let $C$ be a curve, and let $\pi: X \rightarrow C$ and $\pi^{\prime}: X^{\prime} \rightarrow C$ be two geometrically ruled surfaces over $C$ . Show that there is a finite sequence of elementary transformations (5.7.1) which transform $X$ into $X^{\prime}$ . [Hints: First show if $D \subseteq X$ is a section of $\pi$ containing a point $P$ , and if $\tilde{D}$ is the strict transform of $D$ by elm ${ }_P$ , then $\tilde{D}^2=D^2-1$ (Fig. 23). Next show that $X$ can be transformed into a geometrically ruled surface $X^{\prime \prime}$ with invariant $e \gg 0$ . Then use (2.12), and study how the ruled surface $\mathbf{P}(\mathscr{E})$ with $\mathscr{E}$ decomposable behaves under $\operatorname{elm}_P$ .] I am currently working on the exercise from Hartshorne above. I can prove the claim that $\widetilde{D}^2=D^2-1$ by using Figure 23 and noting that the computation of this self-intersection can be done on $\widetilde{X}$ in Figure 23 since this is a blow-up. So of course, pulling-back $\widetilde{D}$ is the same thing as the strict transform plus the exceptional divisor. For the second part regarding the invariant $e\gg 0$ , I can use Hartshorne's Proposition V.2.9 to modify a section of $\pi:X\rightarrow C$ so that I can turn $-e$ into an $-e-1$ and so my new, say $e'$ , is strictly larger than $e$ from $X\rightarrow C$ . So, I can get the invariant $e$ to be sufficienrtly large. Theorem V.2.12 just says that if $e>2g-2$ , then $\mathscr{E}$ is decomposable. So I can reduce to the case where $\pi:X\rightarrow C$ and $\pi':X'\rightarrow C$ are two geometrically ruled surfaces which are projective bundles of some decomposable rank $2$ vector bundles. How does one describe the behavior of $\mathbb{P}(\mathscr{E})$ under $\operatorname{elm}_P$ ? I am having trouble doing this particular computation. I want to say that this is a twisting of some factor of $\mathscr{E}$ , due to my computations with rational surfaces, but I have been unsuccessful...",['algebraic-geometry']
4676373,Proving that a curve is closed,"Consider the $2$ -nd order dynamic system defined as: $\dot{x}_1=x_2, \qquad\dot{x}_2=-g(x_1)$ where $g$ is a continuously differentiable function and $\xi\cdot g(\xi)>0, \forall \xi \in (-a,a),$ with $a$ being a known positive scalar. Defining the following energy function: $V(x)=\frac{1}{2}x_2^2 + \int_{0}^{x_1}g(\xi) d\xi$ Is it true that, for sufficiently small $||x(0)||$ , every solution is periodic? (We can easily show that $\dot{V}(x)=0$ over any phase trajectory, namely $V(x)=constant$ . However, before analysing whether a solution is periodic, I do not know how to show that the curve $V(x)=constant$ is closed). Thanks in advance!","['classical-mechanics', 'ordinary-differential-equations', 'dynamical-systems']"
4676447,Why does there seem to be a relation between the fifth roots of $32$ and $\varphi$ (the golden ratio)?,"I have the following problem on one of my assignments: find all five fifth roots of 32. What I find interesting is how $\varphi$ is related to the real component of most of the solutions. My intuition points me towards the definition of $\varphi$ , namely $\varphi$ = $\frac{\sqrt{5} + 1}{2}$ Here are the solutions: $2$ , $0.618+0.951i$ , $-1.618+1.176i$ , $-1.618-1.176i$ , $0.618-1.902i$ Four of the solutions have real component (of what seems to be) either $-\varphi$ or $\varphi - 1$ . It is not just a coincidence to do with rounding, it is exactly equal to $-\varphi$ and $\varphi - 1$ out to 10 decimal places. Is there some deeper relation that I am missing? I was shocked to get this answer.","['golden-ratio', 'trigonometry', 'roots-of-unity', 'complex-numbers']"
4676467,Is direct image of finite morphism exact?,"Let $f:X\to Y$ be a finite morphism of schemes (or algebraic varieties if you prefer). Then is the direct image functor $f_*:Mod(O_X)\to Mod(O_Y)$ exact? If we restrict to quasi-coherent modules, then $f_*:Qch(X)\to Qch(Y)$ is exact by Serre vanishing.","['algebraic-geometry', 'sheaf-cohomology']"
4676468,Find function $F(s)$ such that $F(s+1) = F(s) - \frac{1}{(s-2)^2}$,"I want to find a function $F(s)$ such that $F(s+1) = F(s) - \frac{1}{(s-2)^2}$ . I first consider the function $\psi(s) = \frac{\Gamma'(s)}{\Gamma(s)}$ . Then I have $\psi(s+1) = \frac{\Gamma'(s+1)}{\Gamma(s+1)}$ . Then I use the identity that $\Gamma(s+1) = s\Gamma(s)$ , and the product rule of the derivative. I get $$\psi(s+1) = \frac{\Gamma(s) - s\Gamma'(s)}{s\Gamma(s)}$$ Then I have $$\psi(s+1) - \psi(s) = \frac{\Gamma(s) + s\Gamma'(s) - s\Gamma'(s)}{s\Gamma(s)}$$ which is equal to $\frac{1}{s}$ . But I'm confused about to get the desired function from here. Could anyone help me please? Thanks! UPDATE I take the derivative on the both sides of the equation and I get $$\psi'(s+1) - \psi'(s) = -\frac{1}{s^2}$$ Then I think that my next step would be shifting $s$ to the right by 2 CONCLUSION I choose my $F(s)$ to be $\psi'(s-2)$ . Then I have $$F(s+1) - F(s) = -\frac{1}{(s-2)^2}$$ which is desired","['complex-analysis', 'functional-equations']"
4676487,"The LÃ©vyâ€“Ciesielski construction of a Brownian motion for $t\in \mathbb R$ (and not only $t\in[0,1]$)","The LÃ©vyâ€“Ciesielski construction of a Brownian motion is based on $$
W(t):=\sum_{k=0}^{\infty} A_k s_k(t) \quad \quad \quad\quad \quad \quad\quad \quad \quad (1)
$$ for times $0 \leq t \leq 1$ , where the coefficients $\left\{A_k\right\}_{k=0}^{\infty}$ are independent, $N(0,1)$ random variables defined on some probability space. (see ""Evans, Lawrence C. An introduction to stochastic differential equations. Vol. 82. American Mathematical Soc., 2012."") The function $s_k$ is the $k$ -th Schauder function: $$
s_k(t):=\int_0^t h_k(s) d s \quad(0 \leq t \leq 1)
$$ where $k=0,1,2, \ldots$ and the family $\left\{h_k(\cdot)\right\}_{k=0}^{\infty}$ of Haar functions are defined for $0 \leq t \leq 1$ as follows: $$
\begin{gathered}
h_0(t):=1 \quad \text { for } 0 \leq t \leq 1 \\
h_1(t):=\left\{\begin{array}{lr}
1 & \text { for } 0 \leq t \leq \frac{1}{2} \\
-1 & \text { for } \frac{1}{2}<t \leq 1
\end{array}\right.
\end{gathered}
$$ If $2^n \leq k<2^{n+1}, n=1,2, \ldots$ , we set $$
h_k(t):=\left\{\begin{array}{l}
2^{n / 2} \text { for } \frac{k-2^n}{2^n} \leq t \leq \frac{k-2^n+1 / 2}{2^n} \\
-2^{n / 2} \text { for } \frac{k-2^n+1 / 2}{2^n}<t \leq \frac{k-2^n+1}{2^n} \\
0 \text { otherwise. }
\end{array}\right.
$$ In Evan's book, I read that: $$
W(t, \omega):=\sum_{k=0}^{\infty} A_k(\omega) s_k(t) \quad(0 \leq t \leq 1) \quad \quad \quad \quad \quad (2)$$ defines a Brownian motion for $0 \leq t \leq 1$ . Probably my question is very naive, but I would like to know if there exists for $W(\cdot)$ and $t\in\mathbb R$ an expression in infinite sum using the Haar system like the (1) and (2). For example, if we let the indices $k$ and $n$ vary over the whole $\mathbb Z$ , can we get an expression of the Brownian motion over $\mathbb R$ ? (For the latter, I think not because otherwise, I would have found it written in some book...)","['brownian-motion', 'probability']"
4676510,If two elliptic functions share the same poles and zeros (including multiciplity) then they are proportional,"Iâ€™m trying to understand the following statement found on my lecture notes: If two elliptic functions share the same poles and zeros (including
multiciplity) then they are proportional Iâ€™m trying to reason the following way: if $f_1$ and $f_2$ are not constant, they must be meromorphic (since holomorphic elliptic functions are constant). Then they can be written as a rational function of two polynomials. I could now write the denominator and numerator of each of them as a product of linear factors (right?). So $f_i=g_i/h_i$ where the zeros of $g_i$ are the zeros of $f_i$ and the zeros of $h_i$ are the poles of $f_i$ . Finally, if $f_1$ and $f_2$ have the same zeros and poles then their numerators and denominators can only differ by a multiplicative constant respectively. Is that correct?","['complex-analysis', 'riemann-surfaces', 'elliptic-functions', 'meromorphic-functions']"
4676518,Using algebra to obtain trigonometric inequalities,"I found this in a text book. We are asked to show $\cos(x)+\cos(y)+\cos(z)\le3/2$ , when $x+y+z=\pi$ . Textbook's explanation has used the completing square method as follows: $$1-2\left(\sin\frac{z}{2}-\frac{1}{2}\cos\frac{x-y}{2}\right)^2+\frac{1}{2}\cos^2\frac{x-y}{2}\le1+\frac{1}{2}\cos^2\frac{x-y}{2}\le3/2$$ The problem I have is how do you exactly know it is maximum when the middle term goes to zero? It is okay when the last term is a constant. But, here when taking middle term to zero, the last term also changes. How do you explain the reasoning used to obtain the inequality? Can you obtain such an inequality if the final result is not given?","['trigonometry', 'inequality']"
4676587,Homotopy lifting as iterated path lifting,"Let $X$ be a topological space, with $\gamma : [0,1] \to X $ some path in $X$ . Suppose $p : \hat{X} \to X$ is a covering map. Then, by the path-lifting lemma, we know that for each $\hat{x} \in p^{-1} ( \gamma(0))$ , there exists a unique lift $\hat{\gamma} : [0,1] \to \hat{X}$ such that $\hat{\gamma}(0) = \hat{x}$ . Similarly, the homotopy lifting lemma asserts the existence of unique lifts for maps $I \times I \to X$ . It has previously been remarked that homotopies may be considered as paths in $C(X,Y)$ endowed with an appropriate topology (e.g. compact-open). Therefore, my question is: Question : can the homotopy lifting lemma be deduced from the path-lifting lemma applied twice? I offer an outline of a plausible proof, which I would struggle to verify as I am not well-acquainted with the compact-open topology (or any topology on $C(X,Y)$ ). Let $H : I \times I \to X$ be our candidate map for lifting. Define $\gamma_i : [0,1] \to X, x \mapsto H(x,i)$ (for $i = 0,1$ ). These give unique lifts $\hat{\gamma_i} : [0,1] \to \hat{X}$ . Consider $\gamma_i$ as elements of $C([0,1],X)$ . Note that there is a path $\eta$ between them in this space, described by $H$ . Prove that $C([0,1] ,\hat{X})$ is a covering space of $C([0,1],X)$ . By the path-lifting lemma, there exists a unique lift of $\eta : [0,1] \to C([0,1],X)$ to $\hat{\eta}:[0,1] \to C([0,1],\hat{X})$ . This implies a unique lift $\hat{H}:I \times I \to \hat{X}$ of $H$ , concluding the proof. The steps (1),(2) and (5) are consequences of the path-lifting lemma. (3) may be assumed given a suitable topology on $C(X,Y)$ . Thus, this question boils down to proving that [there exists a topology on $C(X,Y)$ such that] steps (4) and (6) are valid. Edit : a natural suggestion for the covering map in (4) is: $$P : \hat{\gamma} \mapsto p \circ \hat{\gamma}$$ I am likely to have made a typographical error somewhere. Please do mention any you find.","['general-topology', 'homotopy-theory', 'algebraic-topology']"
4676596,Is there a measurable injective function from the plane to a line?,"Let $f:\mathbb{R}^2\to\mathbb{R}$ be a measurable function. Is it possible that $f$ is injective? It is easy to prove that there is no such continuous function using connectedness, and it is straightforward to use the axiom of choice to construct an injection $\mathbb{R}^2\to\mathbb{R}$ , but I see no way to prove or disprove the existence of such a measurable function.",['measure-theory']
4676642,Limit with non square roots without l'HÃ´pitalâ€™s rule [duplicate],"This question already has answers here : Calculus Bonus Problem (3 answers) Closed last year . I want to calculate the following limit: $\lim_{x\to0}\frac{\sqrt[3]{1+x^2}-\sqrt[4]{1-2x}}{x+x^3}$ without the l'HÃ´pital rule. This limit can be found in Kitchen's book Calculus of one Variable, exercise 5t), section 3-5, during that section the book has substitution techniques and rationalization, but the latter is only done when both roots are square roots, so if there is a specific factorization that works, I also would thank how to find it. By now i have tried the substitution $u=1+x^2$ and $u=1-x$ , but none has got me anywhere.","['limits', 'calculus', 'limits-without-lhopital']"
4676700,Prove the difference of 2 solutions of different initial values problems is increasing with time,"I'm trying to solve this problem. In fact, I'm not really sure how to solve this, but, with guesses, I do have an attempt which is the following: Since for the second coordinate(that is $y$ coordinate), $f_{2}$ only depends on $y$ and they have the same initial values, the second coordinate of the solution curve to be the same, so I just consider the first coordinate Suppose the solution in the first coordinate is $x(t,x_{0})$ . Here I use $x_{0}$ to denote the initial value which is a vector in $\mathbb{R}^2$ and $X$ to denote this coordinate (direction). Then I have the following calculation $$\begin{align}
\frac{\partial x}{\partial t}(t,x_{0})&= f_{1}(x,y) \\ \\ \frac{\partial^2 x }{\partial X\partial t}(t,x_{0}) &= \frac{\partial f}{\partial x}(x,y) \cdot \frac{\partial x}{\partial X}(t,x_{0}) 
\end{align}$$ Then consider the ode $$\begin{align}
\frac{\partial }{\partial t}\left( \frac{\partial x}{\partial X}(t,x_{0}) \right)=  \frac{\partial f_{1}}{\partial x}(x,y) \cdot \frac{\partial x}{\partial X}(t,x_{0}) 
\end{align}$$ This is a first order linear ode with respect to $\frac{\partial x}{\partial X}$ . By solving it, I can obtain $$\begin{align}
\frac{\partial x}{\partial X}(t,x_{0})=C\exp\left( \int _{0}^t  \frac{\partial f_{1}}{\partial x} \ d s \right)
\end{align}$$ So far, I think that if all things above are right, then as long as $C>0$ , with $g(t)=\int_{0}^t\frac{\partial f_{1}}{\partial x} \ d s>0$ , then I can say that $\frac{\partial x}{\partial X}$ is increasing as $t$ is increasing, and hence if I have 2 different solutions $$\begin{align}
x(t,x_{1})< \tilde{x}(t,x_{2})
\end{align}$$ at $t=0$ . That is, $x_{1}<x_{2}$ , we can have, when $t>s$ $$\begin{align}
\frac{\tilde{x}(t,x_{2})-x(t,x_{1})}{x_{2}-x_{1}}&> \frac{\tilde{x}(s,x_{2})-x(s,x_{1})}{x_{2}-x_{1}} \\  \tilde{x}(t,x_{2})-x(t,x_{1})&> \tilde{x}(s,x_{2})-x(s,x_{1})
\end{align}$$ which is done. Now I have 2 questions: 1 I strongly doubt my attempt. If the above is wrong can anyone show me how to approach this problem? 2 If the reasoning above is right, how should I compute the constant $C$ ? Thank you very much.","['solution-verification', 'ordinary-differential-equations', 'dynamical-systems']"
4676779,"Given a triangle $ABC$ how to find a point P such that the triangles $ABP, ACP$ and $BCP$ have equal perimeters","WLOG, let $O = (0,0), A = (a,0)$ and $B = (b,c)$ be the vertices of a triangle and let $P = (x,y)$ be a point on the same plane such perimeters of the triangles $OAP, OBP$ and $ABP$ are equal. Then, $\text{Perimeter}(\triangle OAP) = a + \sqrt{(x-a)^2 + y^2} + \sqrt{x^2 + y^2}\tag 1$ $\text{Perimeter}(\triangle OBP) = \sqrt{b^2 + c^2} + \sqrt{(x-b)^2 + (y-c)^2} + \sqrt{x^2 + y^2} \tag 2$ $\text{Perimeter}(\triangle ABP) = \sqrt{(a-b)^2 + c^2} + \sqrt{(x-a)^2 + y^2} + \sqrt{(x-b)^2 + (y-c)^2} \tag 3$ Theoretically, we can proceed by equating any two of the above equations. For equating $(1)$ and $(2)$ solving for $x$ gives incredibly complicated expression for $x$ . Then by equating $(2)$ and $(3)$ and solving for $x$ we get another equally complicated expression for $x$ . Equating these two expressions for $x$ we should get $y$ in terms or $a,b$ and $c$ . In practice, But due to the complicated nature of the expressions involved, this approach seems hopeless. My alternative approach was a brute force computational estimate of the coordinates $(x,y)$ which at the moment is scanning the entire solution space. This can be made more efficient if we can put bounds on $(x,y)$ . Question 1 : For what triangles does $P$ exist? Are there any known estimates for $P$ Question 2 : Is there a computationally efficient way to calculate $P$","['euclidean-geometry', 'analytic-geometry', 'geometry', 'computational-mathematics', 'computational-complexity']"
4676824,Possible link between the representation theory of the twisted cubic and number of tangents to $y=x^3$ from an external point?,"In my answer to this question I first noted that the intersection point of three tangents to the original curve $y=x^3$ would be $(x_0,y_0)=(\frac23 (a+b+c),-2 abc).$ And then that the condition for three tangents meeting in a point $(x_0,y_0)$ would be $ab+ac+bc=0, a + b \neq 0.$ This reminded me of the elementary symmetric polynomials: $$\pi(\zeta-a)(\zeta-b)(\zeta-c)=\zeta^3-(a+b+c) \zeta^2+ (ab+ac+bc)\zeta-abc=\zeta^3-\frac32 x_0 \zeta^2+0\zeta+\frac12 y_0.$$ Noting that the discriminant is $-27y_0(y_0-x_0^3)$ solved the problem there. But might there be a representation theoretic way of giving meaning to this? I know that $(t,t^3)$ is in the affine piece $x_0=1$ of the projection from $(x_0:x_1:x_2:x_3)=(s^3:s^2t:st^2:t^3), s=1$ by skipping $x_2.$ Could the representation theory of $\frak{sl}$ $V$ where $V$ is of dimension two on $s, t$ figure here? Interacting with the $\Sigma_3$ -action?","['algebraic-geometry', 'representation-theory', 'algebraic-groups']"
4676865,Reflections of Graphs?,When I reflect a function over the $y$ axis: $$f(-x)$$ Consider the function $f(x+3)$ . When reflecting this over the y axis: $$f(-x+3)$$ I cannot intuitively understand why the reflection is not $f(-x-3)$ ? Why do we only change the sign of $x$ and not the $3$ . I'm extremely confused. Could someone explain intuitively how reflections of functions work?,"['reflection', 'algebra-precalculus', 'functions']"
4676881,Can De Moivre's quintic be extended to other F20 quintics?,"For the DeMoivre's quintic $x^5 - 5ax^3 + 5a^2x -  b = 0$ exist formulas for the relations between the roots. When two roots, say $x_1$ and $x_2$ , are known then one can find the other roots with the formulas: $\frac{x_1^3 â€“ 3ax_1 â€“ ax_2}{x_1x_2 + a}, \frac{x_2^3 â€“ 3ax_2 â€“ ax_1}{x_1x_2 + a},\frac{(x_1 + x_2)(3a â€“ (x_1^2 + x_2^2))}{x_1x_2 + a}.$ I discovered these formulas on the internet in the article â€˜DeMoivre's quintic and a theorem of Galoisâ€™ by B.K. Spearman and K.S. Williams. An irreducible DeMoivre's quintic has Galois group F20. Does anyone know if there are analogous formulas for other quintics with Galois group F20? Thank you in advance.","['abstract-algebra', 'polynomials', 'quintics']"
4676900,Norm of Hermiticity-preserving linear map attained on rank-1 state?,"Given some linear map $T:(\mathbb C^{n\times n},\|\cdot\|_1)\to (\mathbb C^{m\times m},\|\cdot\|_1)$ --- where $\|\cdot\|_1={\rm tr}(|\,\cdot\,|)$ is the trace norm, i.e. the sum of the input's singular values --- the question is the following: If $T$ preserves Hermiticity (i.e. $T(A)^*=T(A^*)$ for all $A\in\mathbb C^{n\times n}$ ) is the operator norm of $T$ attained on a Hermitian matrix, that is, is it true that $$
\|T\|_{1\to 1}=\sup_{\substack{A\in\mathbb C^{n\times n}\\A\text{ Hermitian, }\|A\|_1\leq 1}}\|T(A)\|_1\,?
$$ Two observations: 1. $\geq$ is obvious so we would only have to show $\leq$ .
2. Because the trace norm is convex and the trace norm unit ball is the convex hull of rank-1 matrices $uv^*$ (with $\|u\|=\|v\|=1$ ) this can be re-formulated as follows: If $T$ preserves Hermiticity , is it true that the operator norm of $T$ is attained on a rank-1 state, that is, $$
\sup_{v,w\in\mathbb C^n,\|v\|=\|w\|=1}\|T(vw^*)\|_1\leq\sup_{v\in\mathbb C^n,\|v\|=1}\|T(vv^*)\|_1\,?
$$ This is true if $T$ is positive (i.e. maps positive semi-definite elements to positive semi-definite elements) as a consequence of the Russo-Dye theorem, cf. Theorem 3.39 in ""The Theory of Quantum Information"" by Watrous, 2018. However, the proof given therein breaks down if $T$ is not positive but only Hermiticity-preserving as it relies on the fact that $\|\sum_j u_jP_j\|\leq\|u\|_\infty\|\sum_j P_j\|$ for all $P_j\geq 0$ (Lemma 3.3 in his book). Interestingly, this property holds for the ""stronger"" completely bounded trace norm --- sometimes called diamond norm --- which is defined as $$
\|T\|_\diamond:=\max_{k\in\mathbb N}\|T\otimes{\rm id}_{\mathbb C^{k\times k}}\|_{1\to 1}\,.
$$ The idea here (cf. Lemma 3.50 ff.) is that if $X$ attains the operator norm of $T$ , then the norm of $T\otimes{\rm id}$ on $$
\frac12\begin{pmatrix}0&X\\X^*&0\end{pmatrix}
$$ is $\|T\|_{1\to 1}$ because $T(X^*)=T(X)^*$ . Using the spectral decomposition of this block matrix together with convexity of the norm yields ""the"" desired rank-1 state $uu^*$ . So in some sense having access to ""more dimensions"" allows for a symmetrization of the problem.
However, numerics suggest that this is not necessary:
using that every Hermiticity-preserving map is the difference of two completely positive maps (cf. Theorem 2.25) I wrote some rough Mathematica code generating random cp maps and finding $\sup_{u,v}\|T(uv^*)\|_1$ and $\sup_u\|T(uu^*)\|_1$ via random search: (*Initialization*)d = 4;
n = 0;
c = 0;
cnew = 0;
nmax = 20000;
A = IdentityMatrix[d];
B = IdentityMatrix[d];
A0 = IdentityMatrix[d];
v = UnitVector[d, 1];
RND = ResourceFunction[""RandomUnitVector""][d, 2];
dmax = RandomInteger[{1, d^2}];
Vs1 = Table[RandomReal[{-1, 1}, {d, d}], {dmax}];
Vs2 = Table[RandomReal[{-1, 1}, {d, d}], {dmax}];
f1[X_] := Total[ConjugateTranspose[#] . X . # & /@ Vs1];
f2[X_] := Total[ConjugateTranspose[#] . X . # & /@ Vs2];
f[X_] := f1[X] - f2[X];
(*While loop for sup of||f(uv*)||*)
While[n < nmax, RND = ResourceFunction[""RandomUnitVector""][d, 2];
 A = Transpose[{RND[[1]]}] . {RND[[2]]};
 cnew = Total[SingularValueList[f[A]]];
 If[cnew > c, B = A; c = Max[c, cnew];
  If[n > nmax/50, PrintTemporary[c]]; n = 0];
 n++]; Print[""non-Herm="", c]; c = 0; n = 0;
(*While loop for sup of||f(uu*)||*)
While[n < nmax, v = ResourceFunction[""RandomUnitVector""][d];
 A = Transpose[{v}] . {v};
 cnew = Total[SingularValueList[f[A]]];
 If[cnew > c, B = A; c = Max[c, cnew];
  If[n > nmax/50, PrintTemporary[c]]; n = 0];
 n++]; Print[""Herm="", c] All the times I ran this code I did not find a violation of the conjecture, i.e. not once was the norm approximation (on $uv^*$ ) larger than the rank-1-state value (on $uu^*$ ). What makes this problem feel so annoying is that it seems like it should be obvious but so far I have not been able to prove this, not even for some simple classes of maps such as distances of unitary channels (e.g., $A\mapsto A-UAU^*$ for some $U$ unitary) or Hadamard product maps (e.g., $A\mapsto X\circ A$ for $X$ Hermitian). Problems which prevent a straightforward proof are that the usual convexity arguments do not work because the convex hull operator of Hermitian and skew-Hermitian matrices from the unit ball are a strict subset of the unit ball (consider $e_1e_2^*$ ) $T$ preserving Hermiticity does not hand down any nice properties to $T^*(V)$ for $V$ unitary; at least I don't see why it should. Therefore there is no straightforward application of some kind of Russo-Dye argument","['matrices', 'quantum-information', 'linear-algebra', 'matrix-norms']"
4676906,"For a Vitali set $V$, can the set $V+V$ be measurable?","I have some curiosity that for a Vitali set $V$ , whether the set $V+V$ is measurable or not. Heuristically, I think there is no reason to be measurable. I tried to express each element as the sum of representive and rational number, but this method fails since it depends on choice function.. Is there any idea to get a clever answer? Or is this undecidable in standard ZFC axioms?","['measure-theory', 'lebesgue-measure']"
4676909,Cyclotomic Polynomial and Isomorphic rings,"Setup. Here are the objects we are going to work with: $m\geq 1$ denotes an integer and $\Phi_m(X)$ the $m$ -th cyclotomic polynomial $P=p^e$ is the power of a prime number $p$ not dividing $m$ It is well-known that $\Phi_m(X)\in\mathbb F_p[X]$ factors into distinct irreducible polynomials $f_1,\dots, f_n\in\mathbb F_p[X]$ , all of which are of the same degree $d$ equalling the multiplicative order of $p\in\mathbb Z/m\mathbb Z$ . We can lift each $f_i$ to an element $F_i\in(\mathbb Z/P\mathbb Z)[X]$ so that $\Phi_m(X)\in(\mathbb Z/P\mathbb Z)[X]$ is the product of the $F_i$ . Goal. My goal is to prove that the quotient rings $(\mathbb Z/P\mathbb Z)[X]/(F_i)$ are all isomorphic. My progress. I can prove it in case $P=p$ . For this, let $R_i:=\mathbb F_p[X]/(f_i)$ for $1\leq i\leq n$ . We can write $R_1=\mathbb F_p[\zeta]$ with $\zeta:=X+(f_1)\in R_1$ . Consider next the group $G:=\langle p\rangle\leq (\mathbb Z/m\mathbb Z)^\times$ . The quotient group $(\mathbb Z/m\mathbb Z)^\times/G$ will contain $n$ elements, say $a_1G,\dots,a_nG$ . We can order the $a_i$ in such a way that for every $1\leq i\leq n$ , the $d$ zeros of $f_i$ are given precisely by the elements of $a_iG$ . We then consider the ring morphism $$\mathbb F_p[X]\ni f\mapsto f(\zeta^{a_i})\in R_1.$$ Its kernel equals $(f_i)$ , hence we get an injective ring morphism $R_i\to R_1$ ; it is necessarily bijective because all the $R_i$ contain $p^d$ elements. Where I am stuck. It is now unclear to me how I can get the result for the general case. The proof above doesn't work here since we won't work over a field.","['number-theory', 'abstract-algebra']"
4676958,Relation between Variance and Sub-gaussian Norm,"In Vershynin's Introduction to the Non-asymptotic Analysis of Random Matrices , the sub-gaussian norm $\|X\|_{\psi_2}$ of a sub-gaussian random variable $X$ is defined by $$ 
\|X\|_{\psi_2}=\sup_{p\ge1}p^{-1/2}\left(\mathbb{E}|X|^p\right)^{1/p}.
$$ According to this post , it is possible that $\|X\|_{\psi_2}^2\neq\text{Var}(X)$ . My question is: Do there exist constants $C_2\ge C_1>0$ such that for all sub-gaussian variables $X$ , it holds that $$
C_1\text{Var}(X)\le\|X\|_{\psi_2}^2\le C_2\text{Var}(X)?
$$","['statistics', 'probability-distributions', 'probability-theory', 'probability', 'random-variables']"
4676968,Bayes' Theorem Probability Question,"You go on vacation and ask your friends to water your plant. If your friends water the plant, it
will survive your absence with probability $0.9$ . Otherwise, if they forget to water it, it will die with
probability $0.6$ . Your friends will forget to water your plant with probability $0.3$ .
You return home from your vacation and find your plant dead. What is the probability that your
friends did not water it? Solution attempt:
Let A denote event your friends water the plant. Let B denote event the plant dies. We have $P(B^C|A)=0.9, P(B|A^c)=0.6,P(A^c)=0.3$ We want to calculate $P(A^c|B) = \frac{P(B|A^c) \cdot P(A^c)}{P(B|A^c) \cdot P(A^c)+P(B|A)P(A)} = \frac{0.6 \cdot 0.3}{0.6 \cdot 0.3 + (1-0.9) \cdot (1-0.3)} = 0.72$ . Is this correct?","['conditional-probability', 'probability-distributions', 'solution-verification', 'bayes-theorem', 'probability']"
4677022,Prove that $\Sigma \mathbb CP^2\cong S^3\vee S^5$,"My question: Prove that $\Sigma \mathbb CP^2\cong S^3\vee S^5.$ The following is a proof of this result. As well known, the complex projective plane $\mathbb{C}P^2$ can be obtained from the sphere $\mathbb S^2$ by attaching a 4-dimensional cell along the Hopf map $\rho: \mathbb S^3\to \mathbb S^2.$ In other words, we have the following CW complex structure for $\mathbb{C}P^2$ : $$ \mathbb{C}P^2 = (\mathbb S^2\sqcup \mathbb D^4)/\sim\ = \mathbb S^2\cup_{\rho} \mathbb D^4$$ where the identification is given by $x\sim \rho (x)$ for $x\in \mathbb S^3$ . Taking the suspension of $\mathbb{C}P^2$ , we have $$\Sigma\mathbb{C}P^2 = (\mathbb S^3\sqcup S^1\sqcup \mathbb D^5)/\sim\ = \mathbb S^3\cup_{\rho} (\mathbb S^1\sqcup \mathbb D^5)$$ where the identification is given by identifying the basepoints of $\mathbb S^3$ and $\mathbb S^1$ and collapsing the $\mathbb S^1$ factor to a point. We can further simplify this space by noting that $\mathbb S^1\sqcup \mathbb D^5$ is homeomorphic to $\mathbb S^5$ via the map $(\theta,x)\mapsto(\cos\theta x,\sin\theta x)$ for $\theta\in[0,\pi]$ and $x\in \mathbb D^5$ , which identifies the equator of $\mathbb S^5$ with $\mathbb S^1\subseteq \mathbb S^5$ . Under this homeomorphism, the Hopf map $\rho$ becomes a map $g: \mathbb S^3\to \mathbb S^5$ that sends the equator of $\mathbb S^3$ to the equator of $\mathbb S^5$ and the north and south poles of $\mathbb S^3$ to the two points where the equator of $\mathbb S^5$ intersects the $\mathbb S^3$ factor of $\Sigma\mathbb{C}P^2$ . Thus, we must have $\Sigma\mathbb{C}P^2\cong \mathbb S^3\cup_g \mathbb S^5\cong \mathbb S^3\vee \mathbb S^5.$ Give me your comments on this proof. Does it have any errors? I appreciate someone has another idea that allows proof.","['general-topology', 'homotopy-theory', 'algebraic-topology']"
4677044,Invariant subspaces in the context of representation theory,"In what follows, let $G$ be a finite group, $F$ a field and $V$ a vector space over $F$ . First we fix some definitions: Representation: A homomorphism $\phi:G\to GL(V)$ . In this sense, if $V$ is a finite- $n$ -dimensional vector space over $F$ , we can conflate $\rho$ with the image of $\rho(G)$ a finite collection of $n\times n$ matrices over $F$ corresponding to the action of $g$ on $v\in V$ . Sometimes, we will conflate $\rho(G)$ with $V$ itself. Subrepresentation: If $W$ is a subspace of $V$ then we say $W$ is a subrepresentation of $V$ if it is $G$ -invariant. i.e. the action of $g\in G$ on vectors in $W$ map to other vectors in $W$ . It is clear to me from this that $V,\{0\}$ are $G$ -invariant. Now, my confusion with these concepts arises when trying to justify why we can conflate $V$ with $\rho(G)$ . This becomes particularly apparent when I try to prove Schur's lemma. First, I assume the following form of Maschke's Theorem: If $V$ is a representation of some finite group (by which we mean that there is a $rho:G\to GL(V)$ ), and there exists $U$ a subrepresentation of $V$ (taken to mean that there is a $G$ -invariant subspace $U$ of $V$ ), then there is another subrepresentation $W$ of $V$ so $V=U\oplus W$ . i.e. $V$ is the direct sum of $U, W$ . The version of Schur's lemma I am trying to understand in light of the above is as follows: If $M,N$ are irreducible representations of $G$ and $\phi$ is a $G$ -linear transformation in the sense that $\phi$ commutes with the action of $g\in G$ on vectors $V$ in the representation $M$ , then $\phi$ is either the zero map or an isomorphism. Furthermore, $\phi$ must take the form $\lambda I_V$ for $\lambda\in F$ . Here is proof attempt/understanding of the proof: We will show that $\ker{\phi},\text{Im}{\phi}$ are $G$ -invariant since they are subspaces of $V, W$ respectively, where $V,W$ are the vector spaces we conflate with the representations $M,N$ . Let $v\in\ker{\phi}$ . Then $\phi(v)=0$ . So $\phi(gv)=\rho(g)\phi(v)=\rho(0)=0$ , as required. Similarly for the image, let $y\in\text{Im}{\phi}$ then there is $x\in V$ so $\phi(x)=y$ . So $\phi(gx)=g\phi(x)=\rho(g)y$ . I think in this context $\rho$ is associated with $N$ since otherwise $\rho(g)y$ makes no sense. Please correct me if I am wrong. But anyway, this immediately implies that the kernel and image are subrepresentations. By the irreducibility of $M,N$ we conclude that $\phi$ is an isomorphism. A question which occurs to me immediately at this point in the proof is the necessity of all of this. i.e. could we not also prove this statement by the following reasoning: if $\phi$ is linear from $V\to W$ , then $V,W$ have the same dimension, otherwise the kernel fails to be non-trivial? Why the need to show $G$ -invariance? Does this statement alone not already give us the fact that $\phi$ is bijective and hence an isomorphism since certainly it is a homomorphism by its linearity which is assumed? As for the second bit of the proof, the logic seems to go completely over my head. We would like that show that $\phi$ takes the form $\lambda I_V$ . Now, in particular, I guess that $\phi$ is really a homomorphism of linear transformations since $M,N$ are representations which are simply a collection of transformations on some ambient vector spaces $V,W$ . i.e. It seems to me that $\phi$ maps elements of $L(V,V)$ onto elements of $L(W,W)$ . Am I mistaken in thinking this? Now since we just established that $V$ and $W$ have the same cardinality, $L(V,V)$ should be isomorphic to $L(W,W)$ ? I guess in some sense since $\phi$ is a map from $L(V,V)\to L(W,W)$ , we can identify it with a map from $V\to W$ ? Is this line of reasoning correct? If so, then I guess if $\phi$ was not of the form $\lambda I_v$ for some lambda, then it would have a non-trivial proper eigenspace in which case $W$ wouldn't be irreducible? Is this moving in the right direction? I guess my largest concern lies in understanding the justification behind why we can conflate all of these objects. Apologies for the long-winded question. Any help clearing all this up for me is immensely appreciated as I've been muddling around with it for a good while now. Thanks in advance! Edit: As a comment kindly pointed out, I should specify $F$ is an algebraically closed field. I figured out my point of confusion thanks to the poster below. As for the second part of Schur's Lemma, a proof is as follows. If $\phi$ is a $G$ -linear map over a complete field, it has eigenvalues in this field. Say one of them is $\lambda$ . Then consider the transformation $\phi-\lambda I$ . If this does not have the trivial kernel, then since $V$ is irreducible the kernel must be all of $V$ . i.e. $\phi-\lambda I=0$ . The result follows.","['eigenvalues-eigenvectors', 'representation-theory', 'vector-spaces', 'matrices', 'linear-algebra']"
4677061,Matrix multiplication seen as derivative or integral,"I'm reading the book ""Introduction to Linear Algebra 5th edition"" page 24 about Inverse Matrix, below is the background info. $$ \mathbf{A} = \begin{bmatrix} 1 & 0 & 0 \\ -1 & 1 & 0 \\ 0 & -1 & 1 \end{bmatrix}, \qquad
\mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix}, \qquad
\mathbf{b} = \begin{bmatrix} b_1 \\ b_2 \\ b_3 \end{bmatrix}.
$$ In which the author mentioned
""The differences $\mathbf{A}\mathbf{x}$ become the derivative $\frac{\mathrm{d}x}{\mathrm{d}t} = b(t)$ . In the inverse direction, the sums $\mathbf{A}^{-1}\mathbf{b}$ become the integral of $b(t)$ ."" I'm so confused about these statements, I'm thinking he meant vector $\mathbf{x}$ as a function, which applied to matrix $\mathbf{A}$ , so $\mathbf{x}(t)$ is the $\mathbf{A}\mathbf{x}$ .  Then how could: "" $\mathbf{A}\mathbf{x}$ become the derivative $\frac{\mathrm{d}x}{\mathrm{d}t} = b(t)$ ""? As I cant imagine the derivative of $\mathbf{A}\mathbf{x}$ or $x(t)$ here. $$ \mathbf{A}\mathbf{x} = \begin{bmatrix} x_1 \\ x_2 - x_1 \\ x_3 - x_2 \end{bmatrix} $$","['integration', 'derivatives', 'calculus', 'linear-algebra']"
4677096,Show that the product of the sigma-fields $F_j$ is the minimal $\sigma-$field over measurable rectangles,"I'm reading and solving ""probability and measure theory"" by Robert B. Ash and I'm stuck with the following exercise: Show that $\Pi_{j=1}^\infty\mathscr{F}_j$ is the minimal $\sigma-$ field over the measurable rectangles. the definitions are as follows: For each $j=1,2,...$ let $(\Omega_j,\mathscr{F}_j)$ be a measurable space. If $\Omega=\Pi_{j=1}^\infty\Omega_j$ and $B^n\subset\Pi_{j=1}^n \mathscr{\Omega_j}$ we define $B_n=\{\omega\in\Omega: (\omega_1,\dots,\omega_n)\in B^n\}$ . The set $B_n$ is called the cylinder with base $B^n$ ; the cylinder is said to be measurable if $B^n\in\Pi_{j=1}^n\mathscr{F}_j$ . If $B^n=A_1\times\dots\times A_n$ where for each $i\in\{1,\dots,n\}$ , $A_i\in\mathscr{F}_j$ then $B_n$ is called a measurable rectangle. The minimal $\sigma-$ field over the measurable cylinders is called the product of the $\sigma-$ fields $\mathscr{F}_j$ , written $\Pi_{j=1}^\infty\mathscr{F}_j$ . Since the measurable rectangles is a subset of the measurable cylinders, one contention is done. Is the other one I am having trouble with. I suspect it has something to do with the ""good sets principle"" (exposed in the book) since I'm not very good at using it.",['measure-theory']
4677120,Understanding $1$-forms as 'onions',"I stumbled across this paper providing an intuition of differential forms. On page $3$ the paper reads ""Think of a vector as a pin and a one-form as an onion. You evaluate a one-form on a vector by counting how many onion layers it goes through. More generally we represent a one-form on an $n$ -manifold by drawing $(n âˆ’ 1)$ -dimensional surfaces on it. We call these surfaces leaves . In the general case we can still count how many of these leaves each vector goes through. [...] This picture makes it easy to integrate a one-form."" I understand the definition of a $k$ -form, as well as this (different?) interpretation of them, yet the phrase ""You evaluate a one-form on a vector by counting how many onion layers it goes through"" remains a mystery to me. What does the quoted passage mean?","['differential-forms', 'definition', 'intuition', 'differential-geometry']"
4677137,"Taking contrapositive of the statement ""If $G$ is of even order, then there exists an element $x \in G$ such that $x \neq e$ and $x^2=e.$""","Statement : If $G$ is of even order, then there exists an element $x \in G$ such that $x \neq e$ and $x^2=e.$ I am doing it as $\forall x\in G (x=e $ or $ x^2 \neq e) $ $\implies G$ is of odd order $\forall x(\neq e) \in G$ such that $ (x^2 \neq e) $ $\implies G$ is of odd order Which of these is correct  ? Thanks","['group-theory', 'discrete-mathematics']"
4677160,Understanding blowups at nonreduced loci,"Blowing up at a reduced subscheme is geometrically clear to me: it is just like blowing up at a reduced point, and point on the exceptional divisor corresponds to different tangent lines through the base point. However, I always have problem understanding blowing up at a nonreduced subscheme. Is it a geometric way to see it as the reduced cases? As examples, I would like to ask the following questions: Consider the blowup of $\mathbb A^2$ at a double point $x={\rm Spec}(k[x,y]/(x^2,y))$ . What is the exceptional divisor? Is it isomorphic to $\mathbb P^1$ ? Is the blowup smooth? (By the comment of mtrying46, the exceptional divisor is $\mathbb P^1$ but the blow up is not smooth) Consider that first blow up $\mathbb A^2$ at one point and then blow up at one point in the exceptional divisor of the blowup. The composition can be seen as blow up once (at a nonreduced center). Then what is the center? From the two examples above, it seems that blowup at nonreduced center will either result in singularity or central fiber not being projective space. I would like to know whether this is true in general. That is: Suppose $Y$ is the blow up of $X$ at $Z\subset X$ and both $X,Y$ are smooth. If we know every fiber of $Z$ is isomorphic to some $\mathbb P^m$ , can we conclude that $Z$ is reduced? Everything is over $\mathbb C$ . Any comments will be very helpful!","['complex-geometry', 'algebraic-geometry', 'blowup']"
4677161,Where is the error in this argument that convergence in probability implies convergence almost surely?,"Suppose $X_n\to X$ in probability. Then $X_n \to X$ almost surely if and only if every subsequence has a further subsequence converging almost surely to $X$ . However, given any subsequence $X_{n_k}$ we have $X_{n_k}\to X$ in probability so that $X_{n_k}$ has a subsequence converging to $X$ almost surely. Hence $X_n\to X$ almost surely. Where is the error?","['measure-theory', 'probability-theory', 'fake-proofs']"
