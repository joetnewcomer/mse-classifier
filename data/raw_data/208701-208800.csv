question_id,title,body,tags
4185606,Is the magnitude of a component of a vector always less than its norm?,"Let $x\in \mathbb R^n$ and $\| \cdot\|$ a norm defined on $\mathbb R^n$ . Is it true that $|x_i| \leq \| x\|,\forall i \in \{1, 2, \dots, n\}$ ?
I know this easily proved for $\|\cdot \|_2$ or $\| \cdot\|_{\infty}$ ,
and that the statement may be true using the fact that norms are equivalent on $\mathbb R^n$ but haven't been able to show how.","['inequality', 'normed-spaces', 'linear-algebra']"
4185612,Shouldn’t $\tan^{-1}(\mathbb{R})$ be closed?,"I’m quite confused about some topological results. I know there must be something wrong in my reasoning, but I cannot find out what is wrong here. We know that: $\mathbb{R}$ is closed (and is also opened, but that’s not what confuses me) $\tan$ is a continuous function on $]-\pi/2, \pi/2[$ My question is quite simple: since the inverse image under a continuous function of a closed set is closed, why do we have $\tan^{-1}(\mathbb{R})=]-\pi/2,\pi/2[$ , which is not closed?",['general-topology']
4185622,"Finding ""beautiful"" necklaces with regular gaps","I am looking for ""beautiful"" arrangements of $k$ -ary necklaces of length $ak$ where each of the $k$ types of bead appears $a$ times ( $a \geq 1$ a natural number). A necklace is considered beautiful if the sequence of gap lengths for all $k$ types of bead are equivalent under rotation. A gap length is the number of beads between one bead of a given type and the next bead of that type in the necklace. The sequence is formed by taking the gap lengths between pairs of beads of that type going, say, clockwise. A trivial solution is to define the sequence of gap lengths as $(k-1,\ldots,k-1)$ , where a beautiful necklace arranged per this trivial solution could then be $(1,\ldots,k,\ldots,1,\ldots,k)$ . Another trivial solution is to define the sequence of gap lengths as $(0,\ldots,0,a(k-1))$ , where a beautiful necklace arranged per this trivial solution could then be $(1,\ldots,1,2,\ldots,2,\ldots,k,\ldots,k)$ . We'll call other solutions non-trivial. There appear to be non-trivial solutions, at least for some values of $a$ and $k$ . Here's a quick (and slightly dirty) illustration of one for $k=5$ , $a=4$ , with gaps $(2,5,4,5)$ . To help see the rotational symmetry of the gaps, I've added edges to connect the pairs of nearest beads of the same type. All I can say right now is (trivially) that the gap lengths must sum to $a(k-1)$ for each type. I suspect some relation of the gap lengths to the factors of $ak$ , but I am not certain. Given particular values for $a$ and $k$ , how can I find beautiful arrangements of necklaces with non-trivial solutions? Or what can I say about the gaps in such arrangements?","['combinatorics', 'necklace-and-bracelets']"
4185675,Prove by Brouwer's fixed point theorem that there is an integer m such that the equation has a periodic solution of period m.,"Consider the two dimensional system $\dot{x}=f(t,x)$ , $f(t + 1, x) = f (t, x)$ , where $f$ has continuous first derivatives with respect to $x$ . Suppose $\Omega$ is a subset of $\mathbb{R}^2$ which is homeomorphic to the closed unit disk. Also, for any solution $x(t, x_0)$ , $x(0, x_0) = x_0$ , suppose there is a $T(x_0)$ such that $x(t, x_0)$ is in $\Omega$ for all $t\geq T(x_0)$ . Prove by Brouwer's fixed point theorem that there is an integer $m$ such that the equation has a periodic solution of period $m$ . Does there exist a periodic solution of period $1$ ? Suppose there is a $\lambda >0$ such
that $$x'f(t, x) \leq -\lambda|x|^2 $$ for all $t, x$ .
If $g(t) = g(t + 1)$ is a continuous function, prove the equation $\dot{x} = f(t, x) + g(t)$ has a periodic solution of period $1$ .","['analysis', 'ordinary-differential-equations']"
4185676,Prove $(A \setminus B) \times C = (A \times C) \setminus (B \times C)$ [closed],Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 3 years ago . Improve this question However. I am stucked with this -->,['elementary-set-theory']
4185681,"Let $f$ be a real valued function. Prove the set of points $x\in\mathbb{R}$ such that $F(y)\leq F(x)\leq F(z)$ for all $x\leq z$, $y\leq x$ is Borel.","Question: Let $F:\mathbb{R}\rightarrow\mathbb{R}$ be any function (not necessarily measurable, continuous, or anything else.  Prove the set of points $x\in\mathbb{R}$ such that $F(y)\leq F(x)\leq F(z)$ for all $x\leq z$ , $y\leq x$ is Borel. This question was asked here: Prove a function is Borel set , so I am trying to go off the hint in the most recent comment, but I can't seem to quite wrap my head around it.  I like what the first comment said about getting a ""line"" of sorts, but I just can't seem to wrap my head around what I am trying to see graphically.  Any help would be greatly appreciated!","['borel-sets', 'measure-theory', 'real-analysis']"
4185682,Maximal subset iff maximum size,"Let $X$ be a finite set with at least one element and $\eta\subseteq 2^X$ a collection of subsets of $X$ such that $$A\cup B\in\eta\text{ for every }A,B\in\eta$$ $$X=\bigcup_{A\in\eta}A$$ $$\text{For every }x,y\in X\text{ there exists }A\in\eta\text{ which contains one of them but not the other}$$ Now, for every $x\in X$ let $$\eta_x=\{\,A\in\eta\;:\;x\in A\,\}$$ The $\eta_x$ can be ordered either by size or by set inclusion. Is it true that $\eta_x$ is maximal (not a subset of any other $\eta_y$ ) if and only if $|\eta_x|=\max_{z\in X}|\eta_z|$ ? Of course, if $|\eta_x|$ is the maximum then $\eta_x$ is maximal, but I'm having trouble proving the converse. All I know is that 1) If $A\in\eta$ has maximum size then $|A|=|X|-1$ and 2) Every $\eta_x$ contains one such set of maximum size. For an attempt at a proof, if $|\eta_x|$ is not the maximum then 1) There exists $y\in X$ with $|\eta_x|<|\eta_y|$ and 2) One can choose an $A\in\eta_x$ of maximum size and take the only element $z\in X\setminus A$ . The proof must continue trough one of these two because there aren't any other ""interesting"" points that we can focus on, but I can't finish the proof. Thanks!","['elementary-set-theory', 'order-theory', 'combinatorics']"
4185702,Alternative approaches to maximize $y=x\sqrt{100-x^2}$,"I could find three good approaches to find maximum of the function $y=x\sqrt{100-x^2}$ . I will explain them briefly : First: Finding $x$ satisfies $y'=0$ then plugging it in the function. Second: Using the substitution $x=10\sin\theta$ (or $x=10\cos\theta)$ for $\theta\in(0,\frac{\pi}2)$ to get $y=100\sin\theta\cos\theta=50\sin(2\theta)$ hence the maximum is $50$ . Third: Using AM-GM inequality: It is obvious that maximum occurs for $x>0$ So we can rewrite $y$ as $y=\sqrt{x^2(100-x^2)}$ . Now the sum of $x^2$ and $100-x^2$ is $100$ so the maximum of product happens when $x^2=100-x^2$ or $x^2=50$ Hence $y_{\text{max}}=50$ . Just for fun, can you maximize $y=x\sqrt{100-x^2}$ with other approaches?","['optimization', 'algebra-precalculus']"
4185703,"Solve the equation with respect to $k_1,k_2\in \mathbb{Z}_{+}$","I am struggling with solving the following equation for positive integers $k_1$ and $k_2$ in terms of $n\in \mathbb{Z}_+$ and $i,j\in \mathbb{Z}_+$ : $$n-1=\sum_{i\le k_1,j\le k_2}\sum_{\text{gcd}(i,j)=1}1.$$ Note that this equation can be interpreted also as $n-1=\sum_{i\le k_1}\sum_{j\le k_2}\sum_{d\ge 1,d|\text{gcd}(i,j)}\mu(d)$ where $\mu$ is the Mobius function defined as $\mu(n)=\begin{cases}
1 & \text{if $n$ is a square-free positive integer with an even number of prime factors}\\
-1 & \text{if $n$ is a square-free positive integers with an odd number of prime factors}\\
0 & \text{if $n$ has a squared prime factor.}
\end{cases}$","['number-theory', 'mobius-function', 'combinatorics', 'discrete-mathematics']"
4185709,Help me choose from these Differential Geometry + Riemannian Geometry texts? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Questions about choosing a course, academic program, career path, etc. are off-topic. Such questions should be directed to those employed by the institution in question, or other qualified individuals who know your specific circumstances. Closed 3 years ago . Improve this question I want to choose introductory graduate differential geometry and Riemannian texts that minimally overlap with one another. I have narrowed my selection to these books. Please help me decide which ones are superfluous or whether I should read all of them. Thanks Differential Geometry by Tu (1st edition, Springer, 2017) Manifolds and Differential Geometry by Jeffrey Lee (1st edition, AMS, 2009) Introduction to Riemannian Manifolds by John Lee (2nd edition, Springer, 2018) Riemannian Geometry by Petersen (3rd edition, Springer, 2016) Riemannian Geometry and Geometric Analysis by Jost (7th edition, Springer, 2017)","['book-recommendation', 'reference-request', 'riemannian-geometry', 'differential-geometry']"
4185730,these two norms are equivalent,"We have from book Bases in Banach spaces II by ivan singer
""Proposition""
Let $\{G_n\}$ be a sequence of closed linear subspaces of a Banach space $E$ and let $D_1$ be the linear space of sequences of elements $$D_1 = \Bigl\{\{y_n\} \subset E \big| y_n \in G_n \text{ }(n=1,2,...),\text{ }\displaystyle\sum_{i=1}^\infty y_i \text{ }converges\Big \},$$ endowed with the norm $$\|\{y_n\}\| = \displaystyle \sup_{1 \leq n <\infty}\|\displaystyle\sum_{i=1}^n y_i\|.$$ Then $D_1$ is a Banach space. and ""Proposition""
Let $\{G_n\}$ be a decomposition of a Banach space $E$ , such that each $G_n (n = 1,2, ...)$ is closed, and let $\{v_n\}$ be the associated sequence of coordinate projections to $\{G_n\}$ . Then
a) The Banach space $D_1$ introduced in the last proposition is isomorphic
to $E$ , by the mapping $$w: \{y_n\} \rightarrow \displaystyle\sum_{i=1}^\infty y_i.$$ b) The numbers $$|||x||| = \displaystyle\sup_{1 \leq n < \infty} \|\sum_{i=1}^n v_i(x)\| (x \in E)$$ define a norm on the space $E,$ equivalent to the initial norm of $E.$ On the other hand, If we have a sequence of Banach spaces $(X_n)_{n=1}^\infty$ and a number $1 \leq p <\infty$ , we define the $l_p$ -sum of $X_1, X_2,...$ to be the space of all sequences $(x_n)_{n=1}^\infty$ , with $x_n \in X_n$ for $n=1, 2, ...$ , for which $\sum_{n=1}^\infty \|x_n\|_{X_n}^p < \infty,$ in case $p < \infty,$ or $\|(x_n)\|_\infty = \sup_n \|x_n\|_{X_n} < \infty,$ in case $p = \infty,$ and we use also the shorthand $(X_1 \oplus X_2 \oplus ...)_p$ to denote this new space. In brief, for any $1 \leq p \leq \infty,$ we have $$(X_1 \oplus X_2 \oplus ...)_p = \{(x_n): x_n \in X_n \text{ and } (\|x_n\|)_{n=1}^\infty \in l_p \}.$$ Also,
The $c_0$ -sum of spaces is defined in an entirely analogous fashion. In this case we write $$(X_1 \oplus X_2 \oplus ...)_0 = \{(x_n): x_n \in X_n \text{ and } (\|x_n\|)_{n=1}^\infty \in c_0\}.$$ We also always have $$(l_p \oplus l_p \oplus ...)_p = l_p \text{ and } (c_0 \oplus c_0 \oplus ...) = c_0,$$ So say in $(l_p \oplus l_p \oplus ...)_p = l_p$ space if we take $x = (v_i(x))_{i=1}^\infty \in l_p$ space then
so $|||x|||= \displaystyle\sup_{1 \leq n < \infty} \|\sum_{i=1}^n v_i(x)\|$ is equivalent to $(\sum_{i=1}^\infty \|v_i(x)\|^p)^{1/p}$ Question So, if $x \in l_p$ space, as the two norms are equivalent
does $$|||x||| = \sup_{1 \leq n < \infty} \|\sum_{i=1}^n (0,0,...,0,v_i(x),0,...)\| \leq c (\sum_{i=1}^\infty \|(0,0,...,0,v_i(x),0,...)\|_{l_p}^p)^{1/p} = c (\sum_{i=1}^\infty ((\|v_i(x)\|^p)^{1/p})^p)^{1/p} = c (\sum_{i=1}^\infty \|v_i(x)\|^p)^{1/p} = c \|x\|??$$","['banach-spaces', 'schauder-basis', 'normed-spaces', 'functional-analysis', 'sequences-and-series']"
4185733,Interchanging integral and sum in case of absolute and uniform convergence,"Suppose $(X, \mathcal{X}, \mu)$ is a measure space, $\{f_n\}_{n \ge 1} \subseteq L^1(X,\mathcal{X}, \mu)$ is a sequence of real-valued functions on $X$ , and $f \colon X \to \mathbb{R}$ is a function such that $$
f(x) = \sum_{n \ge 1} f_n(x), \quad x \in X
$$ where the convergence is absolute and uniform. When can I say that $$
\int_X \sum_{n \ge 1} f_n \, \mathrm{d}\mu = \sum_{n \ge 1} \int_X f_n \, \mathrm{d}\mu \;? \tag{1}
$$ I know that to be able to say that, a sufficient condition would be $$
\int_X \sum_{n \ge 1} |f_n| \, \mathrm{d}\mu < \infty
$$ (or equivalently $\sum_{n \ge 1} \int_X |f_n| \, \mathrm{d}\mu < \infty$ ) by Fubini's theorem. Now, absolute convergence implies that for $x \in X$ , $g(x) := \sum_{n \ge 1} |f_n(x)| < \infty$ , and uniform convergence implies that for all $\varepsilon > 0$ there exists an integer $N \in \mathbb{N}$ such that for every $n \ge N$ we have $\sup_{x \in X} |f(x) - \sum_{i=1}^n f_i(x)| < \varepsilon$ . I am unable to show (1) with just these assumptions. Is it even possible? If not what other regularity conditions can I impose on $f$ to be able to say that.","['measure-theory', 'real-analysis', 'functional-analysis', 'absolute-convergence', 'uniform-convergence']"
4185758,How to understand $R^{\rho}_{\sigma\mu\nu}$ for constructing geometries?,"I'm trying to understand the curvature tensor $R^{\rho}_{\sigma\mu\nu}$ by playing with it in certain contexts. I already have an understanding, to some degree, of how it measures the change in a vector as it moves along a path in curved space, and how it's related to parallel transport. The issue is that there are just too many components for me to understand succinctly which parts correspond to what and why, in detail. I tried constructing a differential equation with the intention of having the curvature of a manifold embedded into Euclidean space decrease as a function of some Euclidean distance from a point source. I began by writing $$u^{\alpha}\partial_{\alpha}R^{\rho}_{\sigma\mu\nu}=$$ where $u$ is a vector pointing towards the source, and am at a loss for what the right hand side should be. Fundamentally, why does this curvature tensor need as many components as is does to describe something which is a relatively simple concept? Also any help with the differential equation above would be nice. Again, I'm trying to build a solid concept of this thing, so what is the best way to understand it?","['curvature', 'tensors', 'riemannian-geometry', 'differential-geometry']"
4185763,limit of quotient of two series,"Let $Y_{n} > 0$ for all $ n\in \mathbb{N} $ , with $\sum{Y_{n}}= +\infty$ . If $\displaystyle\lim\limits_{n\rightarrow \infty}\frac{X_{n}}{Y_{n}}= a$ then $\displaystyle\lim\limits_{n\rightarrow\infty}\frac{X_{1}+X_{2}+X_{3}+\dots+X_{n}}{Y_{1}+Y_{2}+Y_{3}+\dots+Y_{n}}= a$ I need a idea for solution. Can I supose that $\lim\limits_{n\rightarrow\infty}Y_{n}$ exist?","['limits', 'sequences-and-series']"
4185771,"Are there holes in my road map from calculus to malliavin differential geometry, Bayesian hypergraphs, and causal inference?","I've constructed a directed acyclic graph that leads from introductory subjects, such as calculus (single and multivariable) to some of my current interests including causal inference, Bayesian hypergraphs, and Malliavin differential geometry. The arcs indicate that something in one subject is required by another, with some relaxation in noting that there is transitivity across the graph. I've excluded set theory as too obvious and universal, and hypernetworks and simplicial complexes as merely borderline to my interests as of this moment. You can produce the diagram for yourself with the following code which is provided below. from graphviz import Digraph

g = Digraph()

# Calculus
g.edge('Calculus', 'Real\nAnalysis')
g.edge('Calculus', 'Topology')
g.edge('Calculus', 'Differential\nEquations')

# Linear Algebra
g.edge('Linear\nAlgebra', 'Differential\nEquations')
g.edge('Linear\nAlgebra', 'Multilinear\nAlgebra')

# Abstract Algebra
g.edge('Abstract\nAlgebra', 'Multilinear\nAlgebra')

# Real Analysis
g.edge('Real\nAnalysis', 'Measure\nTheory')

# Topology
g.edge('Topology', 'Measure\nTheory')

# Measure Theory
g.edge('Measure\nTheory', 'Probability\nTheory')

# Probability Theory
g.edge('Probability\nTheory', 'Bayesian\nProbability')
g.edge('Probability\nTheory', 'Stochastic\nProcesses')

# Bayesian Probability
g.edge('Bayesian\nProbability', 'Bayesian\nNetworks')

# Graph Theory
g.edge('Graph\nTheory', 'Bayesian\nNetworks')
g.edge('Graph\nTheory', 'Hypergraph\nTheory')
g.edge('Bayesian\nProbability', 'Bayesian\nHypergraphs')

# Hypergraph Theory
g.edge('Hypergraph\nTheory', 'Bayesian\nHypergraphs')

# Bayesian Networks
g.edge('Bayesian\nNetworks', 'Causal\nInference')

# Stochastic Processes
g.edge('Stochastic\nProcesses', 'Stochastic\nCalculus')

# Differential Equations
g.edge('Differential\nEquations', 'Partial\nDifferential\nEquations')
g.edge('Differential\nEquations', 'Stochastic\nCalculus')

# Partial Differential Equations
g.edge('Partial\nDifferential\nEquations', 'Calculus\nof\nVariations')
g.edge('Partial\nDifferential\nEquations', 'Differential\nGeometry')

# Stochastic Calculus
g.edge('Stochastic\nCalculus', 'Stochastic\nDifferential\nEquations')
g.edge('Stochastic\nCalculus', 'Stochastic\nDifferential\nGeometry')

# Stochastic Differential Equations
g.edge('Stochastic\nDifferential\nEquations', 'Malliavin\nCalculus')

# Calculus of Variations
g.edge('Calculus\nof\nVariations', 'Malliavin\nCalculus')

# Topology
g.edge('Topology', 'Differential\nGeometry')

# Multilinear Algebra
g.edge('Multilinear\nAlgebra', 'Differential\nGeometry')

# Differential Geometry
g.edge('Differential\nGeometry', 'Stochastic\nDifferential\nGeometry')

# Malliavin Calculus
g.edge('Malliavin\nCalculus', 'Malliavin\nDifferential\nGeometry')


# Stochastic Differential Geometry
g.edge('Stochastic\nDifferential\nGeometry','Malliavin\nDifferential\nGeometry')


g.view() What I would like to know is if there are any subjects that I have left out. You're welcome to comment on other flaws in the DAG, and feel free to use the code to show me how you think it 'should' be structured in your answer, but I am primarily interested in missing subjects.","['differential-geometry', 'hypergraphs', 'causality', 'causal-diagrams', 'malliavin-calculus']"
4185778,Why can we not accept the alternate hypothesis in Chi Squared Testing?,"I'm a math teacher, but this aspect of stats is not my strong point.  I've asked several other teachers as to why, and their responses was just ""don't do it"" the why was not very compelling, so I come here. $H_\text{null}$ = $m$ and $n$ are independent. $H_\text{alt}$ = $m$ and $n$ are NOT independent. If condition $p$ is met, we accept the null hypothesis. If condition $p$ is not met, we reject the null hypothesis. Isn't the rejection of the null hypothesis logically equivalent to the alt hypothesis?   Isn't the negation of ( $m$ and $n$ are independent) = ( $m$ and $n$ are Not independent)? Thank you kindly for your response.","['chi-squared', 'statistics', 'logic', 'hypothesis-testing']"
4185829,Sum of a list of numbers compared to the sum of the absolute value of a list of numbers in infinite limit,"I have a list of $d$ real numbers $v=[b_1,b_2,...b_d]$ . I now wish to consider a sequence which depends on this list. For $n=1$ I have the following two terms $$X_1=\frac{1}{d}\left|\sum_ib_i\right|, \quad Y_1=\frac{1}{d}\sum_i|{b_i}|.$$ Clearly $Y_1\geq X_1$ . For $n=2$ , I consider $$X_2=\frac{1}{d^2\times 2}\left|\sum_{i,j}(b_i+b_j)\right|=X_1, \quad Y_2=\frac{1}{d^2\times2}\sum_{i,j}|b_i+b_j|.$$ And so on. For $n=m$ , \begin{gather*}
X_m=\frac{1}{d^m\times m}\left|\sum_{a_1,\cdots,a_m}(b_{a_1}+b_{a_2}+\cdots+b_{a_m})\right|=X_1,\\
Y_m=\frac{1}{d^m\times m}\sum_{a_1,\cdots,a_m}|b_{a_1}+b_{a_2}+\cdots+b_{a_m}|.
\end{gather*} I see numerically that as $n\rightarrow\infty$ , $$Y_m\to X_m.$$ Does anyone know when this is true? Or can anyone point me to any resources that might talk about similar ideas? In other words is the following true: \begin{gather*}
\lim_{m\to\infty} \frac{1}{md^m}\sum_{1\leqslant a_1,\cdots,a_m\leqslant d}|b_{a_1}+\cdots+b_{a_m}|\\
=\lim_{m\to\infty} \frac{1}{md^m}\left|\sum_{1\leqslant a_1,\cdots,a_m\leqslant d}(b_{a_1}+\cdots+b_{a_m})\right|=\frac{1}{d}\left|\sum_{i=1}^{d}b_i\right|\ ?
\end{gather*} In addition to seeing that numerically this appears to be true I have some intuition for why it may be true. There are infinitely many $b_{a_i}$ terms being summed in each term in the summation. So terms which have only a few negative $b_{a_i}$ terms will be asymptotically the same as terms with zero negative $b_{a_i}$ terms (for which the two sums coincide). Also when there are a roughly equal number of positive and negative $b_{a_i}$ terms the two sums will also be asymptotically the same. Obviously this is not at all rigorous and this is where I need help.","['limits', 'absolute-value', 'sequences-and-series']"
4185844,Help for $\int _0^1\int _0^z\int _0^y\frac{1}{\left(1-x^2\right)\left(1+y^2\right)\left(1+z^2\right)}\:\mathrm{d}x\:\mathrm{d}y\:\mathrm{d}z$,"With the help of programs I have been able to conjecture $$\int _0^1\int _0^z\int _0^y\frac{1}{\left(1-x^2\right)\left(1+y^2\right)\left(1+z^2\right)}\:\mathrm{d}x\:\mathrm{d}y\:\mathrm{d}z=\frac{\pi }{8}G-\frac{7}{32}\zeta \left(3\right)$$ But is there a simple way to prove this? What I have got thus far is: $$\int _0^1\int _0^z\int _0^y\frac{1}{\left(1-x^2\right)\left(1+y^2\right)\left(1+z^2\right)}\:\mathrm{d}x\:\mathrm{d}y\:\mathrm{d}z=-\frac{1}{2}\int _0^1\frac{1}{1+z^2}\int _0^z\frac{\ln \left(\frac{1-y}{1+y}\right)}{1+y^2}\:\mathrm{d}y\:\mathrm{d}z$$ $$=-\frac{1}{2}\int _0^1\frac{\ln \left(\frac{1-y}{1+y}\right)}{1+y^2}\int _y^1\frac{1}{1+z^2}\:\mathrm{d}z\:\mathrm{d}y=-\frac{1}{2}\int _0^1\frac{\left(\frac{\pi }{4}-\arctan \left(y\right)\right)\ln \left(\frac{1-y}{1+y}\right)}{1+y^2}\:\mathrm{d}y$$ But I'm not sure how to advance from here, is the current path I'm taking correct?","['integration', 'definite-integrals']"
4185904,Is the infinite product $\prod_{i=0}^{\infty}(1+\frac{1}{2^{3^i}})$ transcendental?,"Is the following number algebraic or transcendental? $$P:=\prod_{i=0}^{\infty}\left(1+\frac{1}{2^{3^i}}\right)$$ We could also define it as follows: let A be the set of natural numbers which contain only 0's and 1's in base 3. Then our number is $$\sum_{a\in A}2^{-a}.$$ I have found a proof that $P$ is irrational (which is a partial result on this question): for $n\geq1$ , let $P_n:=\prod_{i=0}^n(1+\frac{1}{2^{3^i}})$ . Then $P_n$ is a rational number with denominator $D_n=2^{3^0+3^1+\dots+3^n}$ . Now we have $$0<|P_n-P|=\sum_{i\in A\mid i\geq3^{n+1}}2^{-i}<\sum_{i=3^{n+1}}^{\infty}2^{-i}=\frac{1}{2^{3^{n+1}-1}}=\frac{1}{D_n^2}.$$ This holds for all $n$ , and we have $D_n\rightarrow\infty$ as $n\rightarrow\infty$ . Therefore, $P$ has irrationality measure at least $2$ , which implies that $P$ is irrational.","['irrational-numbers', 'power-towers', 'transcendental-numbers', 'sequences-and-series', 'infinite-product']"
4185940,Is the p-adic density of the image of a polynomial always rational?,"Let $P(x)$ be a polynomial with integer coefficients, and let $p$ be a prime number. For $n\in\mathbb N$ , let $I_n$ be the number of integers $i\in\{1,\dots,p^n\}$ such that there is an integer $x$ for which $P(x)\equiv i\mod p^n$ . Now define $$\delta:=\lim_{n\rightarrow\infty}\frac{I_n}{p^n}.$$ Remark that this limit exists since $\frac{I_n}{p^n}\geq \frac{I_{n+1}}{p^{n+1}}$ for all $n$ . One could say that $\delta$ is `the p-adic density of the image of $P$ '. Now I have the following question: is $\delta$ a rational number for all polynomials $P$ and primes $p$ ?","['modular-arithmetic', 'number-theory', 'p-adic-number-theory', 'polynomials', 'prime-numbers']"
4185967,Discontinuity of Greatest Integer Function,Let $f(x)$ be an arbitrary function. Let $g(x) = \lfloor x\rfloor$ be the greatest integer function. We know that $g(x)$ is discontinuous whenever $x$ is integer. Can we say that $g(f(x)) = \lfloor f(x) \rfloor$ is discontinuous whenever $f(x)$ takes integer values?,"['continuity', 'calculus', 'functions', 'ceiling-and-floor-functions']"
4185993,Connection between Determinant and the Quotient Rule,"For the function $\frac{f(x)}{g(x)}$ , by the Quotient Rule, $\frac{d}{dx} (\frac{f}{g}) = \dfrac{\frac{df}{dx}g-f \frac{dg}{dx}}{g^2}$ . We can write the numerator as $\begin{vmatrix} \frac{df}{dx} & f \\ \frac{dg}{dx} & g \end{vmatrix}$ . I know how to prove the formula by definition of derivative, but I wonder why determinant appears in the numerator? Is there any mathematical relation between derivative of $\frac fg$ and determinant that gives us $\begin{vmatrix} \frac{df}{dx} & f \\ \frac{dg}{dx} & g \end{vmatrix}$ right away?","['calculus', 'determinant', 'derivatives']"
4186005,Weakly harmonic function is identically zero,"Let $ 1 < p < 2 $ and $ u \in W_0^{1,p}( \Omega) $ , where $ \Omega $ is some smooth open set of $ \mathbb{R}^N,\ N \geq 2. $ Suppose that $$ \int_{\Omega} \nabla u \nabla \phi dx = 0,\ \forall\ \phi \in C_0^{\infty}( \Omega). $$ Is it true that $  u = 0? $ The use of approximation of $ u $ by smooth functions is not useful. Any idea is welcome.","['harmonic-functions', 'elliptic-equations', 'sobolev-spaces', 'functional-analysis', 'partial-differential-equations']"
4186014,Probability of $0\leq X\leq Y$ for two standard Gaussian random variables random variables,"We are given two independent standard Gaussian random variables $X\sim N(0, \sigma_x^2), Y\sim N(0, \sigma_y^2)$ . Compute $Pr(X-Y\leq 0 \cap X\geq 0)$ . Here is what I did so far: Denote by $\mathbb{1}(x)$ a function which is $1$ if $0\leq x \leq y$ and $0$ else. Moreover let $F(x)$ be th CDF of $X$ and $G(y)$ the CDF of $Y$ . $Pr(X-Y \leq 0 \cap X\geq 0)=Pr(0\leq X \leq Y)=\int \int \mathbb{1}(x) dF(x)dG(y)=\int \int_0^y F'(x) dx dG(y)=\int \frac{1}{2} erf(\frac{y}{\sqrt{2\sigma_x^2}})dG(y)  $ But now I don't know which bound do apply to the integral? $0$ to $\infty?$ EDIT: Please note that $X$ and $Y$ have different variances.","['integration', 'probability-theory', 'probability', 'normal-distribution']"
4186045,How is injectivity enough to show function is an isomorphism?,"I have some problem understanding one part of the proof of Cayley's theorem. The theorem states that if $G$ is a group, then $G$ is isomorphic to a group of permutations. The first part of the proof is to use an earlier theorem stating that there exists a homomorphism $\phi :G\rightarrow S_G$ where $(\phi(a))(x)=ax$ , $x\in G$ . Then they show that $ker(\phi)=\{e\}$ , which implies that the functions is injective. This is the end of the proof. How does this show that $\phi$ is an isomorphism? Don't we need to show that it is surjective as well, or is there something I am missing?","['symmetric-groups', 'group-homomorphism', 'group-theory', 'group-isomorphism']"
4186052,Wrong variable substitution that I am doing,"I spent 3 hours today on the following integral, (I am trying to verify why my variable substitution fails): $$\int\int\frac{x^2\sin(xy)}{y}dxdy$$ over $$\Omega=\{x^2<y<2x^2, y^2<x<2y^2\}$$ which is (everything is positive): $$\Omega=\{1<y/x^2<2, 1<x/y^2<2\}$$ So, I deduced that for a proper transform $T$ we will get that: $$T\Omega=\{(u,v)=T(x,y):1<u<2, 1<v<2\}$$ So we want the opposite of $T(x,y)=(y/x^2,x/y^2)$ . I will skip some of the calculations, but I got that: $$|J_T|=3x^-2y^-2\Longrightarrow |J_{T^-1}|=3^{-1}x^2y^2$$ and that $$T^{-1}(x,y)=(y^{-\frac{1}{3}}x^{-\frac{2}{3}},y^{-\frac{2}{3}}x^{-\frac{1}{3}})$$ and that $$\int_\Omega f=\int_{T\Omega}f\circ T^{-1}|J_{T^{-1}}|$$ which is after putting all of the information: $$\frac{1}{3}\int_1^2\int_1^2xy^2\sin(y^{-1}x^{-1})$$ And this is not leading to anywhere, and I also compared to a full solution that does somewhat similar variable substitution, and after that, he got similar, but solveable integral: $$\frac{1}{3}\int_{\frac{1}{2}}^1\int_{\frac{1}{2}}^1u\sin(uv)$$ I will appreciate any kind of help, I spent too much time on this","['multivariable-calculus', 'change-of-variable', 'multiple-integral']"
4186119,Finding the norm of a bounded linear operator,"I've been trying to find the norm of the linear operator $(Tf)(x) = \int_{-1}^1 xyf(y)dy$ , where $T:L_{\infty}(-1,1) \rightarrow L_{1}(-1,1)$ and $f\in L_{\infty}(-1,1)$ . From definition the norm $||T||$ is defined as $\sup_{f\neq 0}\frac{||Tf||}{||f||_{\infty}}$ , where $||f||_{\infty}$ is the essential supremum of function $f$ over $(-1,1)$ . I've already shown that $||T||$ is bounded by $1$ . In order to find the lower bound of $||T||$ I've been trying to find some sequence of functions that show $\sup_{f\neq 0}\frac{||Tf||}{||f||_{\infty}} \geq \sup_{n}\frac{||Tf_{n}||}{||f{n}||_{\infty}}=1$ but to no avail. Thanks in advance for all the answers.","['normed-spaces', 'functional-analysis', 'linear-transformations']"
4186121,Elliptical billiard table - a ball shot from one focus reaching the other focus (proof using $\tan (\theta -\phi)=...$),"It is known that if there is an elliptical billiard table and we shoot a ball from one focus of the ellipse in any direction, it is necessary that the ball reaches the other focus in only one bounce (or no bounce). You can see here that the angles $\beta$ are the same. I wanted to prove this, so using implicit differentiation I calculated the slope of the line F1P (called $l$ ), the slope of the line F2P (called $m$ ), and the slope of the line nP (called $n$ ). Then I got the following formula, for $F_1(-\alpha,0)$ and $F_2(\alpha, 0)$ and $P(x,y)$ : $$l = \frac{y}{x+\alpha}$$ $$m = \frac{a^2}{b^2}\frac{y}{x}$$ $$n = \frac{y}{x-\alpha}$$ I wanted to verify that the both $\beta$ are the same by comparing the tangent values: $$\frac{m-l}{1+ml} = \tan(\beta) = \frac{n-m}{1+nm}$$ Here I used the formula $$\tan (\theta -\phi)=\frac{\tan(\theta)-\tan(\phi)}{1+\tan(\theta)\tan(\phi)}$$ I put everything into the Computer Algebra System and I get the following result: $$\frac{\alpha^2(x+a^2)y}{a^2b^2+\alpha^2b^2x} = \frac{-\alpha^2(x-a^2)y}{a^2b^2-\alpha^2b^2x}$$ which is not an identity. This contradicts with the obvious result that we can guarantee (that $\beta$ are the same). What is wrong?","['calculus', 'conic-sections', 'trigonometry', 'tangent-line']"
4186151,Find constant in system of function equations,"Question: Let $a \in \mathbb{R}$ . Find all possible values of $a$ such that there exists a function $f: \mathbb{R} \to \mathbb{R}$ satisfying $f(x + 2) = -f(x)$ and $f(x + 3) = f(x) + a$ for any real number $x \in \mathbb{R}$ . My (maybe wrong) solution : Start listing down some values of $f(x)$ with $f(2) = -f(0)$ and $f(3) = f(0) + a$ . $f(2) + f(3) = - f(0) + f(0) + a = a$ . Similarly, you can write $f(1) = -(f(-1))$ and $f(2) = f(-1) + a$ . Hence, $f(1) + f(2) = -(f(-1)) + f(-1) + a = a$ . So, $f(1) + f(2) = f(2) + f(3) \Longrightarrow f(1) = f(3).$ But, according to the problem, $f(x + 2) = -f(x) \Longrightarrow f(3) = -f(1)$ . Since $f(1) = f(3)$ and $f(1) = -f(3) \Longrightarrow f(1) = f(3) = 0.$ Doing similar steps, we find out that $f(0) = f(2) = 0$ . Hence, $a = f(2) + f(3) = 0 + 0 = 0$ . The only possible value of $a$ is $\boxed{0}$ . $\\$ If I'm doing something wrong, can someone please explain? Thanks in advance! (Thanks to the answers, I now know I only need to prove that $a = 0$ can satisfy a valid function :))","['functional-equations', 'functions']"
4186159,Find the domain on which $\log(z^2+9)$ is analytic.,"I have the following function $$f(z)=\log(z^2+9)$$ I need to find the set on which this function is analytic. So far I know that $\log(z)$ is analytic in $D= \left \{ z \in \mathbb{C} | z \notin (-\infty, 0] \right \}$ . So, I understand that for my function $D= \left \{ z \in \mathbb{C} | z^2+9 \notin (-\infty, 0] \right \} = \left \{ z \in \mathbb{C} | z^2 \notin (-\infty, -9] \right \}$ . How should I continue?","['complex-analysis', 'functional-analysis', 'analytic-functions']"
4186184,$(\frac{x}{x+1})^x$ decreasing,"I want to show that $f(x) = \left(\dfrac{x}{x+1}\right)^x$ is decreasing for $x > 0$ ; this is clear from plotting its graph. Taking its derivative, $$f'(x) = \frac{x^x}{(x+1)^{x+1}}\left(1+(x+1)\log\left(\frac{x}{x+1}\right)\right).$$ So we need to show that $1+(x+1)\log\left(\frac{x}{x+1}\right) < 0$ , which seems to require taking another derivative. Is there an easier way to show that $f(x)$ is decreasing?","['calculus', 'functions', 'real-analysis']"
4186211,"Prove $3\int_{0}^{1}\frac{x\arctan x}{3x^2+1}\,\mathrm dx -\int_{0}^{1}\frac{x\arctan x}{x^2+3}\,\mathrm dx =\frac23 G-\frac {\pi}{12}\ln(2+\sqrt{3})$","Prove that $$
I
=3\int_{0}^{1}\frac{x\arctan x}{3x^2+1}\,\mathrm dx
-\int_{0}^{1}\frac{x\arctan x}{x^2+3}\,\mathrm dx
=\frac23 G-\frac {\pi}{12}\ln(2+\sqrt{3}).
$$ where, $G$ is catalan's constant Above two Integrals are a part of a integral which I was trying to solve. Let $I=3I_{1}-I_{2}$ Attempt:-1 $$I_{1}=\int_{0}^{1}\frac{x\arctan x}{x^2+3}\,\mathrm dx$$ $$\implies I_{1}=\int_{0}^{1}\frac{x}{x^2+3}\sum_{n=1}^{\infty}\frac{(-1)^{n+1}}{2n-1} x^{2n-1}\,\mathrm dx$$ $$\implies I_{1}=\sum_{n=1}^{\infty}\frac{(-1)^{n+1}}{2n-1}\int_{0}^{1}\frac{x^{2n}}{x^2+3}\,\mathrm dx.$$ From my previous question 1 we have $$
\int_{0}^{1}\frac{x^{2n}}{x^{2}+3}\,\mathrm dx
=(-3)^{n}\frac{\pi}{6\sqrt{3}}+\sum_{k=0}^{n-1}\frac{(-3)^{n-1-k}}{2k+1}.
$$ Therefore, $$
I_{1}=\sum_{n=1}^{\infty}\frac{(-1)^{n+1}}{2n-1}\bigg[(-3)^{n}\frac{\pi}{6\sqrt{3}}+\sum_{k=0}^{n-1}\frac{(-3)^{n-1-k}}{2k+1}\bigg],
$$ $$
I_{1}=\sum_{n=1}^{\infty}\sum_{k=0}^{n-1}\frac{(-1)^{2n-k}\space 3^{n-1-k}}{(2k+1)(2n-1)}-\frac{\pi}{6\sqrt{3}}\sum_{n=1}^{\infty}\frac{3^{n}}{2n-1}.
$$ Using Desmos Both of the series diverges so $I_{1}$ is of the form $\infty -\infty$ which have a finite answer. Same thing goes with $I_{2}$ . Attempt:- 2: Try to convert one integral into another. Substitute $x=\frac{1}{x}$ in $I_{1}$ , we get $$I_{1}=-\frac{3\pi}{2}\int_{1}^{\infty}\frac{x}{x^2+3} \,\mathrm dx
 +3\int_{1}^{\infty}\frac{x \space tan^{-1}x}{x^2+3} \,\mathrm dx $$ $$\implies I=-\frac{3\pi}{2}\int_{1}^{\infty}\frac{x}{x^2+3} \,\mathrm dx -\int_{0}^{\infty}\frac{x \space tan^{-1}x}{x^2+3} \,\mathrm dx  +4\int_{1}^{\infty}\frac{x \space tan^{-1}x}{x^2+3} \,\mathrm dx$$ Surprisingly all three integrals diverges and convergence of $I$ is maintained by negative and positive sign. How can I prove the original result? Thank you for your help!","['integration', 'calculus', 'definite-integrals', 'sequences-and-series']"
4186234,"integration by parts in Sobolev space $W^{1,1}(\mathbb R^d)$","Let $f,g\in W^{1,1}(\mathbb R^d)$ such that: $$ \int_{\mathbb R^d}|\nabla f|\,|g|\,<\infty\,,\quad \int_{\mathbb R^d}|f|\,|\nabla g| <\infty \,.$$ Can I say that: $$ \int_{\mathbb R^d} \nabla\!f\ g \,=\, -\int_{\mathbb R^d} f\;\nabla g \quad?$$ I remember this holds true if $f,g\in W^{1,2}(\mathbb R^d)$ . Can this hypothesis be replaced by my weaker hypothesis?","['sobolev-spaces', 'functional-analysis', 'weak-derivatives']"
4186288,need help with a Markov chain for weather conditions,"This is my second time posting the question as I failed to do so the first time, because I did not know the proper way. My apologies. Question: Suppose that whether or not it rains today depends on weather conditions through the
previous teo days. If it has rained for the past two days, then it will rain today with
probability 0.7. If it did not rain for any of the past two days, then it will rain today with
probability 0.4. In any other case the weather today will, with probability 0.5, be the same
as the weather yesterday. Describe the weather condition over time with a Markov chain. Suppose that it rained today and yesterday. Calculate the expected number of days until it rains three consecutive days for the first time in the future. I have found 4 different states that I named RR(0), RN(2), NR(1), and NN(3). R stands for when it rains and N is for when it does not. As the question asks, I have tried finding the possible ways of three consecutive days being rainy.
At time n, we are given it was rainy today and yesterday, meaning we are in State 0. 1-) First possibility is when it rains tomorrow, which gives us RRR (we got the three consecutive days) 2-) Second possibility is when we go from 0 to 2, from 2 to 1, from 1 to 0, and stay in 0 one day. That follows as : RRNRRR (In 4 days we can get rain for 3 consecutive days) 3-) Third is when we go from 0 to 2, from 2 to 3, from 3 to 1, from 1 to 0, and stay in 0 one day. That follows as: RRNNRRR. To conclude what I have in mind about the question, wherever we go, when we get to State 0, we need to stay there one more day to get three consecutive rainy days. That means the minimum # of days to get rain for three consecutive days is one day. However, after this point, I am not able to proceed with the question. Any help would be appreciated, thank you!! Edit: I think the maximum # of days is just a random number, which leads me to the expectation of the sum of a random number of geometric random variable, but still I can't go any further beyond that. Thank you!","['statistics', 'stochastic-analysis', 'markov-chains', 'stochastic-processes', 'probability']"
4186301,What is an approximate closed form for sum of $n^n$ series?,"I read about this https://en.wikipedia.org/wiki/Euler%E2%80%93Maclaurin_formula which is Euler-Maclaurain and I did my calculation to find the constants. However, the answer differ greatly from expected value. Are there any other series which is more compatible to deal with sum of n^n so that we get a nice closed form? Sum of 1.5-powers of natural numbers Here is an example but using sum of $n^{1.5}$ but how about approximating a closed form for sum of $n^n?$ If there is a nice closed form (a short one), it would be awesome .","['summation', 'euler-mascheroni-constant', 'discrete-mathematics', 'sequences-and-series', 'power-series']"
4186304,"Algebraic tensor product $C_c(G)\otimes X$ dense in $C_c(G,X)$ w.r.t. $\|\cdot\|_1$, where $G$ is a locally compact group and $X$ a Banach space","Let $G$ be a locally compact group with Haar measure $\mu$ and let $X$ be a Banach space. Define a norm on $C_c(G,X)$ via $$\|f\|_{1}:=\int_{G}\|f(t)\|_{X} \ \text{d}\mu(t).$$ Also note that $C_c(G)\otimes X$ can be viewed as a linear subspace of $C_c(G,X)$ . Indeed, any tensor $a\otimes x\in C_c(G)\otimes X$ acts on $t\in G$ via $(a\otimes x)(t):=a(t)x$ . I am trying to prove that $C_c(G)\otimes X$ is dense in $C_{c}(G,X)$ w.r.t. $\|\cdot\|_1$ . I found a prove that shows density with respect to the inductive limit topology, and apparently this already implies density w.r.t. $\|\cdot\|_{1}$ . This proof uses a partition of unity (which is possible, since $G$ is paracompact). Is there a more elementary direct proof of this statement without the use of the inductive limit topology? UPDATES AND PROGRESS: I think I can reduce the proof to the following claim: Let $G$ be a locally compact group with identity element $e$ . Given $\delta>0$ , $f\in C_{c}(G,X)$ and compact neighbourhood $W\ni e$ (i.e. W is compact and contains an open neighbourhood of $e$ ), there exists an open neighbourhood $V\ni e$ contained in $W$ such that $$r^{-1}s\in V\quad\implies\quad\|f(s)-f(r)\|\leq\delta\quad\quad\forall r,s\in G.$$ Namely, assuming the claim is true, here is what I tried so far: Let $f\in C_{c}(G,X)$ and $\varepsilon>0$ be given. Let $W\ni e$ be a compact neighbourhood. Choose $V$ as in the claim for $\delta:=\epsilon/\mu(KW)$ , where $K:=\text{supp}(f)$ . Since $K$ is compact, there exist $r_{1},\ldots,r_{n}\in K$ such that $K\subset\bigcup_{j=1}^{n}r_{j}V$ . Then $G\setminus K, r_{1}V,\ldots,r_{n}V$ is an open cover of $G$ . Using a partition of unity argument, we find $a_{0},a_{1},\ldots,a_{n}\in C_{c}(G)$ such that $\text{supp}(a_{0})\subset G\setminus K$ and $\text{supp}(a_{j})\subset r_{j}V\subset r_{j}W$ for $j=1,\ldots,n$ . $0\leq\sum_{j=0}^{n}a_{j}\leq1$ . $\sum_{j=0}^{n}a_{j}(s)=\sum_{j=1}^{n}a_{j}(s)=1$ for all $s\in K=\text{supp}(f)$ . Define $g:=\sum_{j=1}^{n}a_{j}\otimes f(r_{j})\in C_{c}(G)\otimes X$ . Then we have \begin{align*}
\|f-g\|_{1}&=\int_{G}\bigg\|f(s)-\sum_{j=1}^{n}a_{j}(s)f(r_{j})\bigg\| \ \text{d}\mu(s)\\
&=\int_{G}\bigg\|\sum_{j=1}^{n}a_{j}(s)(f(s)-f(r_{j}))\bigg\| \ \text{d}\mu(s),\quad (\text{by 3.})\\
&\leq\int_{G}\sum_{j=1}^{n}a_{j}(s)\|f(s)-f(r_{j})\| \ \text{d}\mu(s)\\
&\leq\delta\int_{G}\sum_{j=1}^{n}a_{j}(s) \ \text{d}\mu(s),\quad(\text{by 1. and by def. of $V$})\\
&\leq\frac{\epsilon}{\mu(KW)}\mu(KW)=\epsilon,
\end{align*} where we used in the last step that $\sum_{j=1}^{n}a_{j}\leq1$ by 2. and $\text{supp}(\sum_{j=1}^{n}a_{j})\subset KV\subset KW$ by 1. Is this proof correct? And is the claim I use true?","['real-analysis', 'haar-measure', 'functional-analysis', 'general-topology', 'locally-compact-groups']"
4186308,Non-trivial dense subalgebra of continuous function,"When we talk about dense subalgebra of continuous function on a compact interval, we usually think of smooth functioon or polynomials. Can we find another dense subalgebra which does not contain any non-constant polynomials? I know Stone-Weierstrass theorem gives us the condition of subalgebra being dense iff it separate points, but I haven't seen any concrete examples.","['functional-analysis', 'real-analysis']"
4186322,Minimum of $\cos^2(x) (\csc^2(\frac{\pi}{2n} - \frac{x}{n}) + \csc^2(\frac{\pi}{2n} + \frac{x}{n}))$ for $0 \leq x < \pi/2$ and $n \geq 3$,"I am trying to proove that $x=0$ is the minimum point of the function $$f(x) = \cos^2(x)\left( \csc^2(x_+) + \csc^2(x_-) \right)$$ in the interval $0 \leq x < \pi/2$ where $x_\pm = \frac\pi{2n} \pm \frac x n$ ,
and $n \in \mathbb{N}$ such that $n \geq 3$ . I have unsuccessfully tried the following approaches. Attempted to show that $f(x) - f(0) \geq 0$ , i.e. $$\cos^2(x)\left(
    \csc^2\left(\frac\pi{2n} - \frac x n\right) +
    \csc^2\left(\frac\pi{2n} + \frac x n\right)
\right) - 2\csc^2\left(\frac\pi{2n}\right) \geq 0;$$ Computed $f'(x) = \frac\partial{\partial x} f(x)$ and tried to show that $f'(x) \geq 0$ in the interval, where \begin{align}
\frac\partial{\partial x} f(x) = 2\cos^2(x) \left(
    \frac{1}{n} \cot(x_-)\csc^2(x_-) - 
    \frac{1}{n} \cot(x_+)\csc^2(x_+) -
    \tan(x)\left(\csc^2(x_-) + \csc^2(x_+)\right)
\right);
\end{align} Used Taylor Series Expansion in $f(x)$ attempting to find a tight lower bound for $f(x)$ . Any help or hint would be appreciated.","['optimization', 'calculus', 'trigonometry', 'real-analysis']"
4186323,Can a power series conditionally converge outside its radius of convergence?,"Having learned about conditional and absolute convergence of series, I was confused when the methods used to find the interval of convergence of a power series seemed to consider only absolute convergence. Let $\sum_{n = 0}^\infty b_n (x-c)^n$ be a power series with $\lim_{n\to \infty} \lvert \frac{b_n}{b_{n+1}} \rvert = R$ where $R \ne 0$ . Usually, to find the interval and radius of convergence of $\sum_{n = 0}^\infty b_n (x-c)^n$ , we check absolute convergence. Let $a_n = \lvert b_n(x-c)^n \rvert$ . Using the Ratio Test, $$ \lim_{n\to \infty} \frac{a_n}{a_{n+1}} = \lim_{n\to \infty} \lvert \frac{b_{n+1}(x-c)^{n+1}}{b_n(x-c)^n} \rvert $$ $$= \lvert x-c \rvert\lim_{n\to \infty} \lvert \frac{b_{n+1}}{b_n} \rvert$$ $$= \frac{\lvert x-c \rvert}{R}$$ By the Ratio Test, $\sum_{n = 0}^\infty a_n$ converges for $\frac{\lvert x-c \rvert}{R} < 1 \iff \lvert x-c \rvert < R$ . Absolute convergence implies convergence, so $\sum_{n = 0}^\infty b_n (x-c)^n$ is convergent for $\lvert x-c \rvert < R$ . For the endpoints $x = c$ or $x = -c$ , the Ratio Test for $\sum_{n = 0}^\infty a_n$ is inconclusive, so we usually test $\sum_{n = 0}^\infty b_n (x-c)^n$ using methods other than the Ratio Test at the endpoints. We usually consider $[-R+c,R+c], (-R+c,R+c], [-R+c,R+c),$ or $(-R+c,R+c)$ to be the radius of convergence depending on the convergence of the series at the endpoints. According to the calculations above, $\sum_{n = 0}^\infty a_n$ is divergent for $\lvert x-c \rvert > R$ . However, this only means that $\sum_{n = 0}^\infty b_n (x-c)^n$ is not absolutely convergent. Could it be conditionally convergent for some $\lvert x-c \rvert > R$ ? I would be grateful if you could explain this in a way that is understandable to an undergraduate student like myself. Thank you.","['power-series', 'conditional-convergence', 'absolute-convergence', 'sequences-and-series']"
4186348,"What does ""contain full conjugacy classes"" mean (in simple English please)?","I'm trying to understand the solution to question part $(vi.)$ below: A subgroup is invariant if $gHg^{−1} = H$ for any $g \in G$ . This is equivalent to saying that
a subgroup $H$ is invariant if it has the same right and left cosets, or $\color{red}{\text{the same as saying that it should contain full conjugacy classes}}$ . This is true for $\{E\}$ , $\{E, D, F \}$ and $G$ which contain full classes, but not for $\{E, A\}$ , $\{E, B\}$ and $\{E, C\}$ . Also since $\{E, D, F\}$ has index two $\big(\mid G\mid/\mid H\mid = 2 \big)$ , from question 2 and my previous question we know that this subgroup is invariant.
As a check, let’s see if $\{E, A\}$ has the same left and right cosets: Left cosets of $\{E, A\} \,\text{are}\, A\{E, A\} = \{A, E\},\quad B\{E, A\} = \{B, F \},\quad C\{E, A\} = \{C, D\}$ Right cosets of $\{E, A\}\,\text{are}\, \{E, A\}A = \{A, E\},\quad \{E, A\}B = \{B, D\},\quad \{E, A\}C = \{C, F\}$ We see that the left cosets are not the same as the right cosets. Hence $\{E, A\}$ is not an invariant subgroup. The same thing holds for $\{E, B\}$ and $\{E, C\}$ .
Therefore only $\{E\}, \color{blue}{\{E, D, F \}}$ and $G$ are invariant subgroups. I marked the part in red for which I don't understand. In part $iii.$ it was found that the conjugacy classes are $\{E\}$ , $\{A,B,C\}$ , $\{D,F\}$ . So (from the quote above - the last part of the solution to $vii.$ ), $\color{blue}{\{E,D,F\}}$ 'contains full conjugacy classes', but what does this mean in simple English (where possible)?","['normal-subgroups', 'group-theory', 'abstract-algebra', 'terminology']"
4186355,How to show spectral radius of a special matrix is weakly lower than one?,"Let $A$ be a strictly substochastic matrix, let $x$ be a scalar with $x\in[0,1]$ , and let $$\Lambda(x)\equiv(I-xA^{T})^{-1}.$$ Let $v$ be a vector satisfying $v_{i}\in[0,1],\forall i$ and $\sum_{i}v_{i}=1$ .  Consider the matrix $$J(v,x)=\left(\mathrm{diag}\left(\Lambda(x)v\right)\right)^{-1}\Lambda(x)\left[x\mathrm{diag}\left(v\right)+(1-x)\mathrm{diag}\left(\Lambda(x)v\right)\right]\Lambda^{T}(x)\mathrm{diag}\left(I-A\iota\right),$$ which can also be written as $$J(v,x)=S(v,x)\left[xI+(1-x)\left(\mathrm{diag}\left(v\right)\right)^{-1}\mathrm{diag}\left(\Lambda(x)v\right)\right]\Lambda^{T}(x)\mathrm{diag}\left(I-A\iota\right),$$ where $$S(v,x) \equiv \left(\mathrm{diag}\left(\Lambda(x)v\right)\right)^{-1}\Lambda(x)\mathrm{diag}\left(v\right).$$ I want to show that the spectral radius of $J(v,x)$ is weakly lower than one for all $v,x$ . Simulations suggest that this is true and that the maximum is achieved for $v$ at a corner (i.e., $v_{i}=1$ for some $i$ ) and $x=1$ , with the spectral radius at that point equal to $1$ . For the case in which $x = 1$ we have $$J(v,1)=S(v,1) (I-A)^{-1}\mathrm{diag}\left(I-A\iota\right).$$ $S(v,\iota)$ is a stochastic matrix, while $$\left(I-A\right)^{-1}\mathrm{diag}\left(I-A\iota\right)\iota=\left(I-A\right)^{-1}\left(I-A\right)\iota=\iota,$$ so matrix $\left(I-A\right)^{-1}\mathrm{diag}\left(I-A\iota\right)$ is also stochastic, so it follows that $J(v,\iota)$ is substochastic and hence has spectral radius weakly lower than one. However, if $v$ is at a corner with $v_{i}=1$ then $S(v,x)$ is a matrix with ones in column $i$ , implying that $J(v,1)$ is a matrix with all rows equal to row $i$ of matrix $(I-A)^{-1}\mathrm{diag}\left(I-A\iota\right)$ , which adds up to one, and hence has spectral radius equal to one. Thus, the spectral radius of $J(v,1)$ is everywhere weakly lower than one except at corners of $v$ at which it is equal to 1. If $x = 0$ then $J(v,0) = \mathrm{diag}(I-A\iota).$ This is a diagonal matrix with all elements between zero and one and hence the spectral radius is $1-\min_i \sum_j a_{ij}$ , which by assumption is weakly less than one. In the special case in which $A$ is a diagonal matrix then $S(v,x) = I$ and $$ J(v,x) = \left[xI+(1-x) \Lambda(x)\right]\Lambda^{T}(x)\left(I-A\right).$$ Each element of this diagonal matrix is equal to $$ \left( x + \frac{1-x}{1-x a_{ii}} \right) \frac{1-a_{ii}}{1-x a_{ii}},$$ which is lower than one except at $x = 1$ ,  in which case it is equal to one. How can I extend this to show that $\rho(J(v,x))\leq1$ for all $v,x$ ? One idea is to do this in two steps. In step (1) one would show that for any $v,x$ there is a $k$ such that moving to the corner $v$ with $v_k = 1$ (let's call this corner vector by $v^k$ ) increases the spectral radius, $\rho(J(v^k,x))\geq \rho(J(v,x))$ . In step (2) we would then show that $\rho(J(v^k,1 )) \geq \rho(J(v^k,x))$ . Since $ \rho(J(v^k,1))=1$ then these two steps combined would imply that for any $v,x$ we have $\rho(J(v,x)) \leq 1$ . Unfortunately, neither of these two steps seems easy, so this could be a false lead. Something that could be useful is that $$\Lambda^T(x)\mathrm{diag}\left(I-xA\iota\right)\iota=\left(I-xA\right)^{-1}\left(I-xA\right)\iota=\iota,$$ so $\Lambda^T(x)\mathrm{diag}\left(I-xA\iota\right)$ is a stochastic matrix. This implies that $$\Lambda^T(x)\mathrm{diag}\left(I-A\iota\right)\iota \leq \Lambda^T(x)\mathrm{diag}\left(I-xA\iota\right)\iota=\iota,$$ so $\Lambda^T(x)\mathrm{diag}\left(I-A\iota\right)$ is a substochastic matrix. However, we cannot just use the bound $$J(v,x) \leq S(v,x)\left[xI+(1-x)\left(\mathrm{diag}\left(v\right)\right)^{-1}\mathrm{diag}\left(\Lambda(x)v\right)\right]\Lambda^{T}(x)\mathrm{diag}\left(I-xA\iota\right),$$ because for example in the diagonal case above we would no longer have $\rho(J(v,k)) \leq 1$ (the bound is too lose). Another idea would be to use the fact that $$\rho(J)=\rho(B^{-1}JB)\leq||B^{-1}JB||_{\infty}.$$ The trick here would be to find a matrix $B(v,x)$ such that $$||B^{-1}(v,x)J(v,x)B(v,x)||_{\infty} \leq 1.$$ Since we already have $||J(v^k,1)||_{\infty} = 1$ for any $k$ then presumably one would need that $B(v^k,1) = I$ for any $k$ .","['matrices', 'spectral-radius', 'linear-algebra']"
4186365,"Does a general procedure exist for reducing ${_2F_1}(a,b;c;z)$ when $a,b,c\in\Bbb Q$?","This question is related to a previous question of mine. A quick visit to the Wolfram Functions site reveals a rather extensive list of reduction formulae for the hypergeometric function ${_2F_1}(a,b;c;z)$ when $a,b,c$ are rational numbers. I am curious about how these reduction formulae are derived and if there is a general procedure for finding them? It was rather interesting that the link above includes reduction formulae for rational parameters when the parameters have denominators $1,2,3,4,5,6$ , and $8$ but not $7$ . Is there an interesting reason for this besides the list simply being incomplete? I know of one trick for the case where $c=b+1$ , which takes advantage of the differential formula $$
(z\partial_z+\beta_k-1){}_pF_q\left(
\begin{array}{c}\alpha_1,\ldots,\alpha_p \\ 
\beta_1,\ldots,\beta_k,\ldots,\beta_q\end{array};z\right)
=\left(\beta_k-1\right)
{}_pF_q\left(
\begin{array}{c}\alpha_1,\ldots,\alpha_p \\ 
\beta_1,\ldots,\beta_k-1,\ldots,\beta_q\end{array};z\right).
\tag{1}
$$ Take as an example $y(z)={_2F_1}(1,5/4;2;z)$ . Then using $(1)$ we can derive the ODE $$
(z\partial_z+1)y=(1-z)^{-5/4}.
$$ Coupling this equation with the initial condition $y(0)=1$ and the product rule for derivatives gives the simple result $$
\partial_z(zy)=(1-z)^{-5/4},\quad y(0)=1,
$$ which is easily solved by integrating and using the initial condition to determine the constant of integration. Doing so yields $$
{_2F_1}(1,5/4;2;z)=\frac{4}{z}((1-z)^{-1/4}-1).
$$ Of course, this is a very specialized case of the general approach I am interested in. If a general procedure for arbitrary rational parameters does not exist, I would also be interested in procedures for families of parameters, e.g. a procedure for the case where all parameters have denominator of $2$ . Any references are also greatly appreciated.","['hypergeometric-function', 'special-functions', 'ordinary-differential-equations', 'reference-request']"
4186387,Does probability of $1$% means that it is guaranteed to get a success event if I do $100$ tries?,"Let us suppose I have $100$ balls in a container, each one has a different number ( from $1$ to $100$ ). I want to pick a ball, and we'll suppose this is a random process ( i.e. choosing the ball is random). Question $1$ Given that the whole universe of balls ( a.k.a the set Ω ) is made up of only those $100$ balls in the container, I can say that probability of picking up the ball i is 1% ( i is any number from $1$ to $100$ )? Question $2$ If the answer to question $1$ is ""yes"", and if I repeat the picking of a ball $100$ times (each time I put it back in the container, and I will suppose that the next pickup is unrelated to the previous one, hence randomness), Is it guaranteed that the ball i will appear at least one time during the $100$ pickups? Note: I am not a total beginner in maths, but I am confused about something: does a n% chance means that it is guaranteed to get n successes out of $100$ tries, or it is not guaranteed unless I repeat the tries till infinity?","['probability-theory', 'probability']"
4186426,Let $f_n$ be real & measurable w/ $f_n(x)\rightarrow 0$. Show there is a positive measurable function $h$ such that $f_nh\rightarrow 0$ in measure.,"Question: Let $f_n:\mathbb{R}\rightarrow\mathbb{R}$ be measurable functions such that $f_n(x)\rightarrow 0$ as $n\rightarrow\infty$ for every $x\in\mathbb{R}$ .  Show there exists a positive measurable function $h$ such that $f_nh\rightarrow 0$ in measure. My thoughts: I had two ideas for how to approach this problem: First, I could find some function $h$ , in terms of $f_n$ , that converges to $0$ pointwise almost everywhere which would then imply convergence to $0$ in measure.  Or, second, use function $h$ (again, I feel like I need it in terms of $f_n$ ) and show that $f_nh$ converges to $0$ in $L_2$ (by integrating), thus in $L_1$ , and so in measure.  However, I am struggling to find such an $h$ .   Maybe it would be best to just try and do it directly by trying to show $\lim_{n\rightarrow\infty}m(x\in\mathbb{R}:|f_n(x)h(x)|\geq\epsilon)=0$ for all $\epsilon>0$ . Any help would be greatly appreciated!  Thank you","['measure-theory', 'lebesgue-measure', 'pointwise-convergence', 'real-analysis']"
4186439,Prove that the set of measures on $\mathbb{R}$ isn't separable.,"I think I've got the main idea to prove this, but I'm stuck on a single step.  Here is the problem statement. Here, $\mathbb{F} = \mathbb{R}$ or $\mathbb{C}$ and $\mathcal{B} :=$ the Borel subsets of $\mathbb{R}$ .  This problem is from Axler's Measure and Integration Ch.9. Let $M_{\mathbb{F}}(\mathcal{B}) = \{\nu: \mathcal{B} \longrightarrow \mathbb{F} \: | \: \text{$\nu$ is a measure on the Borel subsets of $\mathbb{R}$}\}$ .  Prove that $(M_{\mathbb{F}}(\mathcal{B}),||\cdot||)$ isn't separable, where $||\cdot||$ is given by $||\nu|| = |\nu|(\mathbb{R})$ . My proof attempt: Let $X = \{\nu_1,\nu_2,\dots\}$ be a countable collection of measures $\nu_k \in M_{\mathbb{F}}(\mathcal{B})$ for $k \in \mathbb{N}$ . Define the sequence $(E_k) \subset \mathcal{B}$ such that $E_j \cap E_k = \emptyset$ if and only if $j \neq k$ .  Define $\nu: \mathcal{B} \longrightarrow \mathbb{F}$ such that $\nu(E_k)$ is given by: \begin{cases}
                                 2 & \text{if $|\nu_k(E_k)| < 1$} \\
                                 0 & \text{if $|\nu_k(E_k)| \geq 1$} \\
  \end{cases} Then given any $j \in \mathbb{N}$ , notice that $$||\nu-\nu_j|| \geq \sum_{k=1}^{j} |(\nu-\nu_j)(E_k)| = \sum_{k=1}^{j} |\nu(E_k)-\nu_j(E_k)| \geq \sum_{k=1}^{j} \bigg||\nu(E_k)|-|\nu_j(E_k)| \bigg|$$ $$ = \left(\sum_{k=1}^{j-1} \bigg||\nu(E_k)|-|\nu_j(E_k)| \bigg|\right) + \bigg||\nu(E_j)|-|\nu_j(E_j)| \bigg|  \geq \bigg||\nu(E_j)|-|\nu_j(E_j)| \bigg| \geq 1.$$ Therefore, $||\nu-\nu_j|| \geq 1 \implies B(\frac{1}{2},\nu) \cap X = \emptyset \implies$ $X$ isn't dense in $M_{\mathbb{F}}(\mathcal{B})$ . The main issue here is that i don't know how to prove that the 'measure' $\nu$ i defined is actually a measure.  Any ideas? Thanks.","['measure-theory', 'analysis', 'real-analysis']"
4186518,"Random Matrix Theory, semicircular law, and Bai-Yin Theorem","I am trying to find a proof for the following lower bound Bai-Yin theorem. Theorem: For an ensemble of Wigner matrices $(W_n)_{n\in\mathbb{Z}_+}$ , we have for any $\epsilon>0$ , almost surely, \begin{align*}
||W_n||>(2-\epsilon)\sqrt{n}
\end{align*} for large enough $n$ . Here we denote the operator norm of matrix $A$ with $||A||$ . In fact, we have almost surely \begin{align*}
\lim_{n\rightarrow\infty}\inf||W_n||/\sqrt{2}\geq 2
\end{align*} My question: Only thing I found on the internet was the following proof fragment, which looks kind of strange for me. For instance, the used function $g$ seems to be not defined for some arguments ( $x=2-\epsilon$ ) and overall I am not able to follow the argumentation. Proof: By the semicircular law, we know that the empirical spectral distribution of $(1/\sqrt{n})W_n$ converges to a semicircular distribution almost surely. Fix $\epsilon>0$ . Let $g:\mathbb{R\rightarrow}[0,1]$ be a continuous function with $g(x)=1$ if $|x|<2-\epsilon$ and $g(x)=0$ if $|x|>2-\epsilon/2$ . If $||W_n||<(2-\epsilon)\sqrt{2}$ , then \begin{align*}
\int_{\mathbb{R}}g(x)d\mu_{\frac{1}{\sqrt{n}}W_n}(x)=1.
\end{align*} But we also see that \begin{align*}
\int_{\mathbb{R}}g(x)d\mu_{sc}(x)<1.
\end{align*} This means that we can almost surely find $N$ such that \begin{align*}
||W_n||>(2-\epsilon)\sqrt{n}
\end{align*} for any $n>N$ .","['random-matrices', 'probability-theory']"
4186553,Cauchy derivation theorem of integrals demonstration,"I am studing the cauchy derivates theorem and there is one step which i don't understand.
When we are proving for $f^{(n)}$ for math induction. The hipothese are: $$f^{(n-1)}(z):=\frac{(n-1)!}{2 \pi i} \int_{\partial D} \frac{f(\xi)}{(\xi - z )^n}d \xi$$ The step is: \begin{align}
f^{(n)}&=\lim_{h \to 0} \frac{f^{(n-1)}(z+h)-f^{(n-1)}(z)}{h}
\\&=\frac{(n-1)!}{2 \pi i} \lim_{h \to 0} \frac{1}{h} \bigg[ \int_{\partial D} \frac{f(\xi)}{(\xi - z - h )^n}d \xi - \int_{\partial D} \frac{f(\xi)}{(\xi - z )^n}d \xi\bigg]
\\&=\frac{(n-1)!}{2 \pi i} \lim_{h \to 0} \frac{1}{h} \int_{\partial D} f(\xi)\frac{(\xi -z)^n-(\xi -z-h)^n}{(\xi - z - h )^n(\xi-z)^n}d \xi &(1)
\\ &=\frac{(n-1)!}{2 \pi i} \lim_{h \to 0} \frac{1}{h} \int_{\partial D} f(\xi)\frac{nh(\xi -z)^{n-1}-\cdots}{(\xi - z - h )^n(\xi-z)^n}d \xi  &(2)
\\ &=\frac{(n-1)!}{2 \pi i} \int_{\partial D}\lim_{h \to 0} \bigg[ f(\xi)\frac{n-\cdots}{(\xi - z - h )^n(\xi-z)} \bigg]d \xi  &(3)
\\ &\qquad \qquad \qquad \qquad \qquad \qquad \qquad \vdots
\end{align} I don't understand the step from $(1)$ to $(2)$ .","['complex-analysis', 'induction', 'derivatives', 'cauchy-integral-formula']"
4186565,Showing there's no exotic solutions to this (kind of) differential equation,"We want to solve $$ty'(t) = y$$ for all once-differentiable real functions. My attempt: It looks like a separable ODE, so first let's try solving it over an interval that doesn't contain $0$ , it should be easy then to extend it to 0 by continuity and differentiability. Unfortunately, we also have to divide by $y$ which could be $0$ . If we assume our solution doesn't pass through $0$ at $t\neq 0$ , it is easy to see the solutions are of form $$y = Ct, C\neq 0.$$ Another trivial solution would be of the above form with $C=0$ . Now we'd need to prove whether these are the only solutions, or if there is some other solution that passes through $0$ at some $t_0 \neq 0$ . What i've tried: Since $y(t_0)=0$ then plugging in the DE, $y'(t_0)=0$ . However now i have no idea how to proceed. Any hints? Is there a general technique for this kind of problem?","['calculus', 'ordinary-differential-equations']"
4186593,A chessboard Combinatorics Problem,"How many ways are there to put numbers $1,2,3,\cdots, n^2$ into a $n\times n$ chessboard s.t. the sum of the numbers on every row and every column is even. My approach to this problem (Ideas) I want to use parity to solve the problem. First, see all odd numbers as $1$ , and all even numbers as $2$ .  Using the addition principle, we can figure out how many $0,1$ arrangements are there. After that, for each arrangement, using the multiplication principle, we can solve for all the arrangements. Observations When $n=1$ , there is no way to satisfy the question. (Trivial) When $n=2$ , there is no way to satisfy the question. (Brute force) When $n=3$ , there is no way to satisfy the question. (Brute force) When $n=4$ ? When I got to $n=4$ , I got stuck because the are way to many cases to write down if I brute force the problem. Trying to solve the general formula I GUESS there exists more than $0$ ways only for $4\mid n$ . I already turned the original problem into an equivalent problem s.t. the numbers are all $0$ s and $1$ s. Now I may have to solve for the general  formula for all $4m\times 4m\;(m\in \mathbb{N})$ chessboards, and prove that for all $n$ s.t. $4\nmid n$ , there are $0$ possible arrangements. Note I hope this is not a duplication. If there are any mistakes in my question, I will edit it. If two or more arrangements can be turned into one another by rotations and flips, these arrangements are still different arrangements. Edit The GUESS above is wrong. I should prove $\forall m\in \mathbb{N}$ s.t. $2\mid m$ and $m>2$ , there exists at least one possible arrangement for a $m\times m$ chessboard. Then, I should find the general formula for the $m\times m$ chessboard.","['chessboard', 'combinatorics']"
4186602,What is the dual of $l^p$ for $0<p<1$?,"$l^p$ for $1\leq p<\infty$ is defined as the vector space on which $$x\mapsto\sqrt[p]{\sum_j{|x_j|^p}}$$ is a norm.  For $0<p<1$ , this cannot be a norm, but (as Wiki indicates ) the function $$\mapsto\sum_j{|x_j|^p}$$ is still a metric, so it's a (complete) topological vector space. If $1\leq p<\infty$ , the dual of $l^p$ is well-known to be $l^q$ , where $\frac{1}{p}+\frac{1}{q}=1$ . What is the (topological) dual of $l^p$ for $0<p<1$ ? In that case, $l^p\subseteq l^1$ , so $l^{\infty}\subseteq(l^p)^*$ .  But I don't know what else is in there.","['banach-spaces', 'functional-analysis', 'dual-spaces']"
4186648,How to evaluate the finite power tower $\tan(1°)^{\tan(2°)^{\tan(3°)^{\cdot^{\cdot^{\cdot^{\tan(44°)^{\tan(45°)}}}}}}}$,"Consider the following finite power tower: $$\Large \tan(1°)^{\tan(2°)^{\tan(3°)^{\cdot^{\cdot^{\cdot^{\tan(44°)^{\tan(45°)}}}}}}}$$ I'm wondering if there is a way to solve this that doesn't rely on brute force (i.e. simply typing the entire power tower out). As, I see it, there are two possibilities: There's some magic trig identity that simplifies this to a small closed form.  I haven't found any such yet. I need to develop some sort of approximation scheme for a computer. Is (1) possible?  If not, how do I do (2)? Edit: For clarification, I have manually typed out the problem inside Desmos and arrived at the following answer: I'd just like to know if this answer is correct, and if so, how I would go about finding an easier way to achieve this answer (whether that be by using a trig identity or some form of computerization, I don't mind) without having the type out every step like I've done above. Thank you,","['trigonometry', 'power-towers', 'tetration']"
4186652,Number of extensions of a character in an abelian group,Question: Let $A$ be an abelian finite group and $B$ a subgroup of $A$ . Let $\phi$ be an irreducible character of $B$ . Show there is an irreducible character $\Phi$ of $A$ such that $\Phi|_B=\phi$ . The number of possible $\Phi$ is $[A:B]$ . I was able to prove the existence of the irreducible character $\Phi$ but I don't know how to prove that the number of possible $\Phi$ is $[A:B]$ . Any help on that part is appreciated.,"['abelian-groups', 'group-theory', 'finite-groups', 'characters']"
4186664,"Use continuous limit theorem to prove if $f$ is uniformly differentiable, then $f'$ is uniformly continuous","This is my first approaching to answer my own question in math stack exchange. Before that, I would define several terms according to Steven Abbott's Understand Analysis, which, by the way, the origin of this question. Derivative Let $g:\mathbb{A}\rightarrow\mathbb{R}$ be a function defined on an interval $\mathbb{A}$ . Given $c\in\mathbb{A}$ , the derivative of $g$ at $c$ is defined by $g'(c)=\lim_{x\to c}\frac{g(x)-g(c)}{x-c}$ . Continuous limit theorem Let $(f_n)$ be a sequence of functions defined on $\mathbb{A}\subseteq\mathbb{R}$ that converges uniformly on $\mathbb{A}$ to a function $f$ . If each $f_N$ is continuous at $c\in\mathbb{A}$ , then $f$ , is continuous at $c$ . Uniformly differentiable Given a differentiable function $f:\mathbb{A}\rightarrow\mathbb{R}$ , let's say that $f$ is uniformly differentiable on $\mathbb{A}$ if, given $\epsilon>0\exists\delta>0$ such that if $0<|x-y|<\delta$ , then $|\frac{f(x)-f(y)}{x-y}-f'(y)|<\epsilon$ .","['continuity', 'derivatives', 'uniform-convergence', 'real-analysis']"
4186692,True or False: A Borel set is a countable union of disjoint intervals and a measure zero set,"Basically the title. To give more context, suppose I have a Borel set $B$ with $B\subset [0,1]$ . As an example, consider a situation where I am interested in computing some expectation over $B$ with respect to the uniform distribution over $[0,1]$ . If my conjecture is true, then I can write: $$
\int_B \dfrac{g(x)dx}{\mu(B)} = \sum_{i=0}^n \int_{a_i}^{b_i} \dfrac{g(x)dx}{b_i-a_i}
$$ for some $(a_i, b_i)_{i\in N}$ . This is indeed what I need. However, I couldn't find any reference to such a result or a counter-example to my conjecture. In fact, if it is not the case that any Borel set can be written in this manner, what is the largest class of measurable sets for which this is true?","['measure-theory', 'probability-theory', 'real-analysis']"
4186704,Burnside before Burnside's transfer theorem,"As I already said, I am reading Part VIII of Burnside's ""Notes on the Theory of Groups of Finite Order"". (Proceedings of the London Mathematical Society, vol. XXVI, p. 325-338, 1895; The Collected Papers of William Burnside, vol. 1, p. 589-596.) It is the paper where Burnside proves that a finite non-abelian simple group with even order has order divisible by 12, by 16 or by 56. From p. 591 of the Collected Papers, seventh line by the end, Burnsides assumes that (hyp. 1) $G$ is a finite group whose Sylow 2-subgroups are of order 8; (hyp. 2) if $P$ is a Sylow 2-subgroup of $G$ , two elements of $P$ are conjugate in $G$ only if they are conjugate in $P$ and he announces that he will prove that G is not simple. Note that if the Sylow 2-subgroups of $G$ are abelian, hyp. 2 is equivalent to say that for every Sylow 2-subgroup $P$ of $G$ , $N_{G}(P) = C_{G}(P)$ , thus, in this case, the non-simplicity of $G$ is today an easy consequence of Burnside's transfer theorem. But, in his 1895 paper, Burnside doesn't use his transfer theorem (I presume that he had not yet found it) and I would like to understand his reasonings. It could be a good little work on the history of mathematics. At the start of his paper, Burnside proved that if the Sylow 2-subgroups of $G$ are cyclic, then $G$ is not simple. Thus, if we assume that the Sylow 2-subgroups of $G$ are abelian, we are left with two cases : (i) the Sylow 2-subgroups of $G$ are isomorphic to $C_{2} \times C_{2} \times C_{2}$ ; (ii) the Sylow 2-subgroups of $G$ are isomorphic to $C_{2} \times C_{4}$ . I understand Burnside's proof (that $G$ is not simple) for case (i), but not for case (ii). Here is his proof (p. 593 in the Collected Papers, vol. I) for case (ii) : ""If next, the sub-groups of order $2^{3}$ are of of type (ii), and if the main group contains 3 distinct sets of conjugate operations of order 2, one of these sets contains exclusively operations which are the squares of operations od order 4, and the other two sets those that are not. Let, now, $A$ be on operation of order 2 which is the square of an operation of order 4, and let $B$ an operation of order 2 belonging to a different conjugate set from $A$ . Then $A$ and $B$ must generate a dihedral group of order $4n$ , where $n$ is odd. Suppose [why ""suppose"" ? Isn't it necessarily true ?] that $AB$ is an operation of this group of order $2n$ , and write $(AB)^{n}= C$ , $(AB)^{2}=S_{n}$ , so that $C$ is an operation of order 2, and $S_{n}$ an operation of order $n$ . The operation $C$ must clearly belong to a different conjugate set from both $A$ and $B$ . Now $A S_{n} A = S_{n}^{-1}, B S_{n} B = S_{n}^{-1}, C S_{n} C = S_{n}$ ."" So far, so good. (Instead of ""operation of a group"", read ""element of a group"", and instead of ""conjugate set of operations"", read ""conjugacy class of elements"".) A complete proof of the preceding results is a bit long, but I can give it if anyone asks for it. Then Burnsides says : ""If $A^{1}$ is any operation contained in the sub-group within which the cyclical sub-group generated by $S_{n}$ is self-conjugate, and belonging to the same conjugate set as A [I understand : if $A^{1}$ is any $G$ -conjugate of $A$ belonging to $N_{G}(<S_{n}>)$ ], then $A^{1} S_{n} A^{1} = S_{n}^{-1}$ ,"" Why ? How do we know that we don't have, for example, $A^{1} S_{n} A^{1} = S_{n}$ ? I copy the rest of Burnsides's proof : ""and, therefore, $S_{n}$ cannot certainly be permutable [read : commute] with any operation of order 4, since it is not permutable with the square of any such operation. The operation $S_{n}$ therefore forms one of a set of $4r$ conjugate operations, where $r$ is odd; [I read : the number of $G$ -conjugates of $S_{n}$ is $4r$ , where $r$ is odd;] and, when these are transformed by any operation of order 4, the resulting substitution [read : permutation] of the permutation-group consists of $r$ cycles of 4 symbols each. This is an odd substitution, and, therefore, again, in this case, the group cannot be simple."" I agree that $S_{n}$ commutes with no element of order 4 of $G$ (I can give a proof if anybody asks), but I don't see how Burnside can say that the number of $G$ -conjugates of $S_{n}$ is $4r$ , where $r$ is odd. I obtain something different : [Edit : if $G$ is simple,] the number of $G$ -conjugates of $S_{n}$ is $\equiv 2 \pmod{4}$ . So my question is : do you think that Bunside's proof is correct and, if it is the case, could you explain it ? Thanks in advance.","['group-theory', 'finite-groups']"
4186713,Computation of a Hilbert Samuel function,"I am trying to solve the following exercise from Eisenbud's Commutative Algebra with a View Toward Algebraic Geometry: Exercise 12.1: Let $f\in R = k[x,y,z]_{(x,y,z)}$ be a homogeneous form of degree $d$ , monic in $x$ . Show that $(y,z)$ is an ideal of finite colength on $M = R/(f)$ . Compute the corresponding Hilbert-Samuel functions. I am aware that a similar question has been asked here: Compute the Hilbert-Samuel function . Nonetheless, since there are no answers or comments at the linked post, I'm posting it again here. Here is what I have tried: I have been able to show that $(y,z)$ has finite colength on $M$ , as it's quite easy to see that some large power of $(x,y,z)$ is contained in $(y,z) + \text{ann } M$ . For the second part however, I think I am stuck. To find the Hilbert Samuel polynomial, I need to compute the length of the module $M_n = (y,z)^nM/(y,z)^{n+1}M$ . I am tempted to think that this length is $d(n+1)$ , because as far as I can see the $M_n$ is a finite dimensional $k$ vector space, with basis $\{x^ay^bz^c |0\le a\le d - 1, b + c = n\}$ . However, I'm not sure if this is useful, or how I can translate this to a statement about the length of $M_n$ . I would be glad if someone could point out how to proceed.","['modules', 'ring-theory', 'abstract-algebra', 'hilbert-polynomial', 'commutative-algebra']"
4186723,Continuous surjections between non-homeomorphic spaces,"It is very easy to give examples of topological spaces $A$ and $B$ that are not homeomorphic to each other, with a continuous injection from $A$ to $B$ , and a continuous injection from $B$ to $A$ . See, for example $(0,1)$ and $[0,1]$ , as demonstrated in the answer to a similar question on this forum, or this related question . In other words, there exist topological spaces that do not satisfy the Cantor-Schroeder-Bernstein property , when considering injective maps. I was wondering if this fails when considering surjective maps as well. That is, given two topological spaces $A$ and $B$ , if there exists a continuous surjection from $A$ to $B$ , and a continuous surjection from $B$ to $A$ , must $A$ and $B$ be homeomorphic to each other? I think this shouldn't be true: there should be a simple counterexample, but I just can't find one! It would be great to have a counterexample for locally compact Hausdorff spaces, because I want to eventually understand problems with the Cantor-Schroeder-Bernstein property for $C^\star$ algebras...","['general-topology', 'analysis']"
4186725,Why do waiting times in a queue tend to follow a gamma distribution?,"I’m trying to get an intuition for gamma distributions, and why they are the model of choice for waiting times. In addition, I’d love to hear about any other distributions that are useful for modeling wait times for the same reason. I’d say I’ve got a pretty solid understanding of why everyday random variables tend to follow a normal distribution. If you drop balls into a line of tubes from above, Galton board style, you’ll tend to see them arrange into a normal distribution. Add pegs to provide something for the balls to bounce off of, allowing small deviations in initial position to translate into larger deviations in which tube the balls end up in, and the standard deviation increases. This makes intuitive sense, and establishes very good intuitions around what types of random variables will tend to be normally distributed, and why. Is there a way to explain the gamma distribution’s connection to wait times, an empirical argument, example, or combination of the two that makes it clear why wait times will tend to look like a gamma distribution? Further, are there other distributions that will tend to model wait times well for reasons that can be understood from a similar explanation? Image courtesy of Seeking Wisdom .","['statistics', 'probability-distributions', 'intuition', 'queueing-theory', 'gamma-distribution']"
4186731,Asymptotic expansion of $\sum_{n=1}^\infty\frac{H_n}{n!}z^n$ for $z\to\infty$,"What is the asymptotic expansion of $\sum_{n=1}^\infty\frac{H_n}{n!}z^n$ for $z\to\infty$ where $H_n=\sum_{k=1}^n \frac{1}{k}$ ? I thought of using the Euler-Mascheroni constant, the fact that $\gamma=\lim_{n\to\infty}(H_n-\log(n))$ and expressing it in form of $Ei(z)$ of which I know the asymptotic expansion of. But I'm not getting anywhere. Can someone help me out and suggest a direction in which way to look? Is there maybe a way to represent the series as an integral?","['complex-analysis', 'power-series', 'asymptotics']"
4186774,An ODE $-x '' + x = |x|^p x$,"In the context of nonlinear Schrödinger equations(NLS), the following elliptic PDE is important and its solution is called ground state: $- \Delta Q + Q = |Q|^{p}Q$ on $\mathbb{R}^d$ , with $p > 0$ . When the dimension $d = 1$ , this is just an ODE, and it is written as $ - x'' + x = |x|^p x$ . A lot of textbook says that we can solve this ODE explicitly by elementary method. However, I cannot do that.","['ordinary-differential-equations', 'analysis', 'real-analysis', 'calculus', 'partial-differential-equations']"
4186780,ODE with discontinuous function,"This exercise was given to me: to solve the following ODE over $[0,+\infty)$ $$y'+y=H(t),$$ where $$H(t) = \begin{cases} 
          1 & 0\leq t\leq 1 \\
          -1 & t\gt 1 
       \end{cases}
      $$ So i've multiplied both sides by $e^t$ to get that the ODE is equivalent to $$(e^ty)' = e^tH(t).$$ However RHS of this clearly doesn't satisfy IVT, so it is not the derivative of a function, therefore the ODE has no solutions. Is this correct or am i missing something?","['calculus', 'ordinary-differential-equations']"
4186807,A doubt on the proof for a criterion for checking if two stochastic processes are indistinguishable.,"Let $[[S]]$ be the graph of the stopping time $S$ , and $\pi(A)$ the projection of $A$ , which is a subset of $[0,\infty[ \times \Omega$ , into $\Omega$ . In the following picture, why would the author want to look at $X_{S\wedge t}$ and not simply at $X_S$ ?
Since $[[S]]\subset A$ , when $S< \infty$ , we'll have $X$ and $Y$ different. Also, do we need $P(\pi(A))>0$ just so that we have $P(S<\infty)>0$ ? What would happen otherwise?","['probability-theory', 'stochastic-calculus']"
4186842,How did Facebook calculate the 3.5 degrees of separation of everyone on their platform? (Bitwise Operators question),"A few years ago the Facebook team calculated that everyone was an average 3.5 degrees of separation away from every other user on the FB platform. https://research.fb.com/three-and-a-half-degrees-of-separation/ They explain the maths later in the article. First they use a statistical approximation of the number of friends you have based on the max number of zeros (n) in a binary hash of your friends ID's. Friends ~= c * 2^n where c is a constant This much I understand. Then they go on to say... By using a bitwise OR operation on the hash, this process can be repeated recursively to estimate the number of unique friends-of-friends, and then friends-of-friends-of-friends. What does this part mean?","['statistics', 'logic']"
4186847,"If $R$ is a noetherian ring, then $ab=1$ implies $ba=1$ $\forall a,b\in R$","The following question was part of an exam at my university: if $R$ is a noetherian ring, then $ab=1$ implies $ba=1$ $\forall a,b\in R$ . As far as I know, the result would hold if $a$ and $b$ aren’t zero divisors . However, I don’t see why this should hold for a general noetherian ring, because $R$ being noetherian doesn’t imply that it doesn’t contain zero divisors, right?","['ring-theory', 'abstract-algebra', 'noetherian']"
4186848,What is the minimum value of $8 \cos^2 x + 18 \sec^2 x$?,"As per me the answer should be $26$ . But when we apply AM-GM inequality it gives $24$ as the least value but as per the graph 24 can never come. What I think is that in AM-GM, it gives $8 \cos^2 x = 18 \sec^2 x$ which gives $\cos x > 1$ which is not possible and because of this, AM-GM is giving a wrong minimum value. If we had $18 \cos^2 x + 8 \sec^2 x$ , then AM-GM would have worked and $24$ would be a right answer since $18 \cos^2 x = 8 \sec^2 x$ , which gives $\cos x < 1$ which is true. Is this reason correct?","['maxima-minima', 'trigonometry']"
4186880,"If $f(x+y) = f(x)f(y)$ and $f(x)$ is monotone, prove $f(x)$ differentiable.","If $f(x)$ is monotone and $f(x)$ is not a constant, $\forall x,y \in (-\infty,+\infty)$ $f(x+y) = f(x)f(y)$ , prove that: (1) $f(0) = 1$ ; (2) $f(x)>0$ ; (3) $f(x)$ is continuous; (4) $f(x)$ is differentiable; I can prove (1) (2) but got stuck on (3) and (4), as $$
f'(x) = \lim_{\Delta x \to 0}\frac{ f(x+\Delta x)-f(x)}{\Delta x}=\lim_{\Delta x \to 0}\frac{f(x)[f(\Delta x)-1]}{\Delta x} = f(x)f'(0)
$$ I Guess if it's differentiable at one point $x=0$ , it will be differentiable at any x, but how to prove $x=0$ is differentiable? Thank you!","['limits', 'calculus', 'analysis', 'real-analysis']"
4186891,Find coordinates of right angle vertex in a right triangle.,"I have a right triangle like shown in the image: Right triangle I know the coordinates of $\mathbf V_1$ and $\mathbf V_3$ , as well as the lengths of all sides $(A, B, C)$ and angles of the vertices $(a, b, c)$ . In this case, $\mathbf V_1$ is fixed, and $\mathbf V_3$ rotates around $\mathbf V_1$ ; the angle $c$ is always a right angle; there is no 3D component. How would I go about calculating the coordinates of $\mathbf V_2$ ? I found this answer, but I fail to see how the solution would fall on the circle with center $\left(\frac{x_1+x_2}2,\frac{y_1+y_2}2\right)$ . Wouldn't that only apply if angles $a$ and $b$ were the same? First time posting a question here, so I hope I've provided enough information. Edit: Follow up question. \begin{align}
V_1=(x_1,y_1), V_2=(X,Y),V_3=(x_3, y_3)\\
\overrightarrow{V_1V_3}^2&=\overrightarrow{V_2V_3}^2+\overrightarrow{V_1V_2}^2\\
\sqrt{(x_1-x_3)^2+(y_1-y_3)^2}^2&=\sqrt{(x_3-X)^2+(y_3-Y)^2}^2+\sqrt{(X-x_1)^2+(Y-y_1)^2}^2\\
\end{align} I've come up to this equation where my unknowns are (X,Y). However, this leaves me with 1 equation and 2 unknowns. Is there another property that I can use to solve for X and Y?","['computational-geometry', 'triangulation', 'geometry', 'triangles', 'trigonometry']"
4186917,Why inner variations are diffeomorphisms?,"Let $\Omega \subset \mathbb{R}^n $ be an open, bounded, connected domain with smooth boundary. Let $\eta \in C_c^{\infty}(\Omega,\mathbb{R}^n)$ be a smooth compactly supported $n$ -tuple of real valued functions. Define $\phi(x)=x+\epsilon\, \eta(x)$ . How to prove that for sufficiently small $\epsilon$ , $\phi$ is a diffeomorphism $\Omega \to \Omega$ ? I can see why $\phi(\Omega)\subseteq \Omega$ for small $\epsilon$ , but I don't see why $\phi|_{\Omega}$ is injective, or why $\phi(\Omega)\supseteq \Omega$ . I do see why $d\phi$ is invertible for small $\epsilon$ . Edit: Here is some progress: (Is there a more elementary way, which do not use the fact the Jacobian is null-Lagrangian?). Assume $\phi$ is injective (see comment below). First, we have $$
\phi(\bar \Omega) \subseteq \overline{\phi( \Omega)} \subseteq \bar \Omega.
$$ The injectivity of $\phi$ implies $$
\text{Vol}(\phi(\bar \Omega))=\int_{\bar \Omega} J\phi\stackrel{(1)}{=}\int_{\bar \Omega} \text{Id}=\text{Vol}(\bar \Omega),
$$ where equality $(1)$ follows, since the Jacobian is null-Lagrangian. Thus, we have $$
\phi(\bar \Omega) \subseteq \bar \Omega, \,\,\, \text{Vol}(\phi(\bar \Omega))=\text{Vol}(\bar \Omega).
$$ This implies that $\phi(\bar \Omega)=\bar \Omega$ . Thus, $\phi:\bar \Omega \to \bar \Omega$ is bijective, hence by the inverse function theorem, it is a diffeomorphism. Is there a direct way to see $\phi:\bar \Omega \to \bar \Omega$ is surjective?","['euclidean-geometry', 'calculus-of-variations', 'smooth-manifolds', 'differential-topology', 'differential-geometry']"
4186957,Is $\prod_{m=1}^{\infty}\frac{\Gamma(2mx+x+1)}{(2mx)^{2x+1}\Gamma(2mx-x)}=\frac{2x^{x+\frac 12}}{\pi^x\ \Gamma(x+1)}$ also for $x\notin\mathbb Z^+$?,"By calling upon the roots of unity, it can be proved that for $n\in\mathbb Z^+$ , $$\prod_{k=1}^{n-1} \sin\frac{k\pi}{2n}= \frac{\sqrt n}{2^{n-1}} $$ (see, for example, this .) Using the infinite product representation for $\sin x$ , the LHS becomes $$\prod_{k=1}^n \frac{k\pi}{2n}\prod_{m=1}^{\infty}\left( 1-\frac{k^2}{4m^2n^2}\right)\\ = n!\left(\frac{\pi}{2n} \right)^n \prod_{m=1}^{\infty}\prod_{k=1}^n \frac{(2mn-k)(2mn+k)}{4m^2n^2}\\ = n!\left(\frac{\pi}{2n}\right)^n \prod_{m=1}^{\infty}\frac{\Gamma(2mn+n+1)}{(2mn)^{2n+1}\Gamma(2mn-n)} $$ and therefore $$  \prod_{m=1}^{\infty}\frac{\Gamma(2mx+x+1)}{(2mx)^{2x+1}\Gamma(2mx-x)}= \frac{2x^{x+\frac 12}}{\pi^x \ \Gamma(x+1)} $$ for $x\in\mathbb Z^+$ . Naturally, I wondered whether this result also holds for other $x\notin\mathbb Z^+$ , and indeed on checking I saw that this is true for $x=\frac 12, \frac 32, \frac 13$ or even $x=\pi$ , so I suspect that it holds atleast for all $x\gt 0$ . How can this be proven/disproven?","['gamma-function', 'trigonometry', 'infinite-product', 'real-analysis']"
4186977,How do I prove that $\left( 1-\frac{a}{b} \right)\left( 1+\frac{c}{d} \right)=4$?,"The challenge is to prove $$\left(1-\dfrac{a}{b}\right)\left(1+\dfrac{c}{d}\right)=4.$$ Apart from the Pythagorean theorem, the tangent and secant theorem, and the cosine theorem, I could not invent anything. The calculations here turn out to be very cumbersome and I'm not sure that this is how it should be initially solved. I had an idea to start from the similarity in the figure, there is a lemma about a right angle on a chord, there is a property of a tangent and a secant...But all this again will lead to cumbersome calculations.","['triangles', 'trigonometry', 'geometry']"
4187029,Exact value for the continued fraction of $\tiny 1+\cfrac{1}{3+\cfrac{3}{5+\cfrac{5}{7+\cfrac{7}{9...}}}}$?,"Does anyone know the exact value for the continued fraction of $$1+\cfrac{1}{3+\cfrac{3}{5+\cfrac{5}{7+\cfrac{7}{9+\ddots}}}}?$$ I already know that $$1+\cfrac{1}{3+\cfrac{1}{5+\cfrac{1}{7+\cfrac{1}{9\ddots}}}}=\frac{e^2+1}{e^2-1},$$ but I only figured that out by typing the decimal approximation into google of the first few terms of the continued fraction (before I knew the exact value) which took me to a math paper saying that $\frac{e^2+1}{e^2-1}$ roughly equals the decimal approximation I typed in. I then typed in the continued fraction of $\frac{e^2+1}{e^2-1}$ into wolfram alpha and it spat out $$1+\cfrac{1}{3+\cfrac{1}{5+\cfrac{1}{7+\cfrac{1}{9\ddots}}}}.$$ I have no idea how to solve these so please don't downvote, I'm just doing this in case it's useful to someone one day, and out of curiosity of course.","['number-theory', 'continued-fractions']"
4187033,"Given an equation with trigonometric roots $\tan a$ and $\cot a$, determine $\sin a + \cos a$.","Question : Given that $\tan a$ and $\cot a$ are two real roots of the equation $x^2 + k^2 - kx - 3 = 0$ , and $3\pi < a < \frac{7\pi}{2}$ , find the value of $\sin a + \cos a$ . My solution can be found below in the answers section.","['algebra-precalculus', 'roots', 'trigonometry']"
4187052,"Proving that $f(t)=\frac{n^2}{2}\cdot t^{n-4}(1-t^2)\left(t^2-\frac{n-3}{n}\right)$ is bounded above by $1$, for $n\geq6$ and $t\in[0,1]$","I have a problem that looks like a typical problem of maximizing functions in a compact interval. However, I am not being able to prove the bound I need. Let $n\geq 6$ be an integer number. Consider the function: $$f(t) = \frac{n^2}{2} \cdot t^{n-4}(1-t^2) \left(t^2 - \frac{n-3}{n}\right) $$ Prove that for all $t\in [0,1]$ it holds $f(t) \leq 1$ . The points where the derivative $f'$ is zero are very ugly expressions. By maximizing the factor $t^{n-4}(1-t^2)$ and using that the last factor is at most $\frac{3}{n}$ it is possible to deduce that for $n\geq 6$ it is $f(t) \leq \frac{3}{2}$ (in fact, it is possible to bound it in the limit by $\frac{3}{e}\approx 1.1036...$ but that is far from $1$ . I have checked that the claim is true for several random values of $n$ (in fact, I think that the bound can be reduced to something like $0.61...$ for $n>20$ say).","['lagrange-multiplier', 'real-analysis', 'multivariable-calculus', 'calculus', 'optimization']"
4187062,Observers in General Relativity and adapted coordinates,"A recent discussion here got me thinking about observers and changes of coordinates in General Relativity, from a more mathematical point of view. Let us consider a timelike curve $\gamma:I\subset\mathbb R\to M$ , where $M$ is a $n$ -dimensional pseudo-Riemannian manifold and $\varphi_\alpha: U_\alpha\subset M\to V_\alpha\subset\mathbb R^n$ is a local coordinate system. By possibly restricting $V_\alpha$ to a suitable subset $V'_\alpha$ , it should be possible to find a map $\psi_{\alpha\beta}:V'_\alpha\subset V_\alpha\to V_\beta$ in such a way that \begin{align}
(\psi_{\alpha\beta}\circ\varphi_\alpha\circ\gamma)(\lambda)\equiv y^\mu(\lambda)=(y^0(\lambda),\underbrace{0,\ldots,0}_{n-1}).
\end{align} The coordinate system defined in this way seems to be ""adapted"" to the curve $\gamma$ , in the sense that an observer sitting on it feels standing still (no velocity on spatial directions). Then I would interpret $\lambda$ as the proper time of the observer. Is this true/correct? Consider now the following two push forwards \begin{align}
(\varphi_\alpha\circ\gamma)_*: &\,T_\lambda I\to T_{\varphi_\alpha\circ\gamma(\lambda)}V_\alpha\\
&\partial/\partial\lambda\to\frac{dx^\mu(\lambda)}{d\lambda}\frac{\partial}{\partial x^\mu}\\
(\psi_{\alpha\beta}\circ\varphi_\alpha\circ\gamma)_*:& \,T_\lambda I\to T_{\psi_{\alpha\beta}\circ\varphi_\alpha\circ\gamma(\lambda)}V_\beta\\
&\partial/\partial\lambda\to\frac{dx^\mu(\lambda)}{d\lambda}\frac{\partial y^a}{\partial x^\mu}\frac{\partial}{\partial y^a}=\frac{dy^a(\lambda)}{d\lambda}\frac{\partial}{\partial y^a}=\frac{dy^0(\lambda)}{d\lambda}\frac{\partial}{\partial y^0}
\end{align} In the second case I find that the velocity of the observer standing still on the particle is only in the time direction, as claimed. However in the first case, coordinates are not adapted to the curve, so the tangent vector to the curve in principle has components along all directions. Is this computation correct? If so, how do we interpret in the first coordinate system the parameter $\lambda$ ? This cannot be the proper time and I expect this to be unphysical.","['coordinate-systems', 'general-relativity', 'curves', 'differential-geometry']"
4187071,"""Determine all functions $\Bbb{Z}\to\Bbb{Z}$ such that $f(2a)+2f(b)=f(f(a+b))$""","I came across this problem: Let $\Bbb{Z}$ be the set of integers. Determine all functions $f$ : $\Bbb{Z}\to\Bbb{Z}$ , such that for all integers $a$ , $b \in \Bbb{Z}$ $f(2a)+2f(b)=f(f(a+b))$ . The unsatisfying solution that was presented was to substitute $a=0$ and $a=1$ , and notice that $f$ is an arithmetic progression, and find both coefficients. A satisfying solution, in my view, should go along these lines: What special property of $\Bbb{Z}\to\Bbb{Z}$ functions allows us to resolve equations involving such functions, and their convolutions? For that I come for your help.","['functions', 'integers']"
4187103,Structure of coefficients of polynomials giving a specified Galois group,"For any $a = [a_0; \dots; a_n]\in \mathbb{P}^n(\mathbb{Q})$ , the corresponding Galois group $G_a$ of $f(X) = a_n X^n + \cdots + a_1 X + a_0\in \mathbb{Q}[X]$ is a subgroup of $S_n$ . For a given group $G \subset S_n$ , what exactly can be said about the size of the set of points $a$ with $G_a = G$ ? For $G = S_n$ , the answer from here is that $G = S_n$ iff the resolvent of $f$ is irreducible, and this happens for $a$ in a Zariski-dense set by Hilbert's irreducibility theorem. Generalizing from that, is there a specific sense in which $A(X) = \{a\in \mathbb{P}^n(\mathbb{Q}):\, G_a\subset X\}$ has $A(H)\subset A(G)$ ""small"" in $A(G)$ for $H < G$ ? That is, is there a result invoking some notion like the dimension of a variety (although we're necessarily working over $\mathbb{Q}$ here), a thin set in the sense of Serre, etc. that makes this vague idea of ""smallness"" more precise?","['galois-theory', 'algebraic-geometry']"
4187128,BFGS Formula from Kullback-Leibler Divergence,"On page 411 in this book , the authors give the following BFGS formula $$ \boxed{\boldsymbol C_{\textrm{BFGS}} = \boldsymbol C + \underbrace{\frac{\boldsymbol g^\top\boldsymbol\delta+\boldsymbol g^\top\boldsymbol C\boldsymbol g}{(\boldsymbol g^\top\boldsymbol\delta)^2}\boldsymbol\delta\boldsymbol\delta^\top-\frac{1}{\boldsymbol g^\top\boldsymbol\delta}\left(\boldsymbol\delta\boldsymbol g^\top\boldsymbol C + (\boldsymbol\delta\boldsymbol g^\top\boldsymbol C)^\top\right)}_{\textrm{BFGS update}}}\tag{1} $$ by considering the constrained optimisation problem: $$\begin{array}{rlcll}
\min_{\boldsymbol A} & \mathcal D(\boldsymbol 0, \boldsymbol C \mid \boldsymbol 0, \boldsymbol A) \\
\textrm{subject to}: & \boldsymbol A\boldsymbol g  =  \boldsymbol\delta, \\
& \boldsymbol A ~~=  \boldsymbol A^\top,
\end{array}\tag{2}$$ where $$ \mathcal D(\boldsymbol 0, \boldsymbol C \mid \boldsymbol 0, \boldsymbol A) := \frac12\left(\mathrm{tr}\left(\boldsymbol A^{-1}\boldsymbol C\right) - \log\left(\det(\boldsymbol A^{-1}\boldsymbol C)\right) - n\right)$$ is the Kullback-Leibler divergence between the normal distributions $\mathcal N(\boldsymbol 0, \boldsymbol C)$ and $\mathcal N(\boldsymbol 0, \boldsymbol A)$ . Using Lagrange multipliers with the Lagrangian $$ \mathcal L(\boldsymbol A, \boldsymbol \beta) := \mathcal D(\boldsymbol 0, \boldsymbol C \mid \boldsymbol 0, \boldsymbol A) + \boldsymbol \beta^\top(\boldsymbol A\boldsymbol g - \boldsymbol \delta)$$ gives (also using symmetry of $\boldsymbol A$ and $\boldsymbol C$ ) $$ \frac{\partial}{\partial\boldsymbol A}\mathcal L(\boldsymbol A, \boldsymbol \beta) = \frac12\left(-\boldsymbol A^{-1}\boldsymbol C\boldsymbol A^{-1} + \boldsymbol A^{-1}\right) + \boldsymbol \beta\boldsymbol g^\top, \quad \boldsymbol{\nabla}_{\boldsymbol \beta}\mathcal L(\boldsymbol A, \boldsymbol \beta) = \boldsymbol A\boldsymbol g - \boldsymbol \delta,$$ and equating these both equal to zero gives $\boldsymbol A\boldsymbol g = \boldsymbol \delta$ and $\boldsymbol A^{-1}(\boldsymbol I - \boldsymbol C\boldsymbol A^{-1}) = -2\boldsymbol \beta \boldsymbol g^\top$ . It is not obvious to me how this last expression could be used to identify the corresponding values of $\boldsymbol \beta$ and $\boldsymbol A$ . I tried multiplying through by $\boldsymbol \delta$ to use the $\boldsymbol A\boldsymbol g = \boldsymbol \delta$ constraint, yielding $\boldsymbol{\delta}\boldsymbol \beta^\top\boldsymbol \delta = \frac12\left(\boldsymbol C\boldsymbol g - \boldsymbol \delta\right)$ , but it is still unclear from this (1) how $\boldsymbol \beta$ is specified and (2) how $\boldsymbol{A}$ would be recovered. What is the correct way to solve the constrained optimisation problem (2) to obtain the BFGS formula (1) with the method of Lagrange multipliers that gives values for $\boldsymbol A$ and $\boldsymbol \beta$ ?","['statistics', 'newton-raphson', 'linear-programming', 'machine-learning', 'optimization']"
4187133,"Evaluate $\int_{0}^{\frac{\pi}{2}}\ln(2+\sin x) \,\mathrm dx $ and $\int_{0}^{\frac{\pi}{2}}\ln(2-\sin x) \,\mathrm dx$","I had to calculate $$I= \int_{0}^{1}\frac{1}{1+x^2}\ln\bigg[\frac{x^2+x+1}{x^2-x+1}\bigg]\,\mathrm dx$$ from my Previous Question [ 1 ] (which is now solved) but I wanted to have another solution ,So I proceed like this. $$I=\int_{0}^{1}\frac{\ln(x^2+x+1)}{x^2+1}\,\mathrm dx-\int_{0}^{1}\frac{\ln(x^2-x+1)}{x^2+1}\,\mathrm dx$$ Let $x=\tan t \implies \mathrm dx=\sec^2 t \,\mathrm dt$ $$\implies I=\int_{0}^{\frac{\pi}{4}}\ln(\tan^2 x+\tan x+1)\,\mathrm dx-\int_{0}^{ \frac{\pi}{4}}\ln(\tan^2 x-\tan x+1)\,\mathrm dx$$ $$2(\tan^2 x+\tan x+1)=(2+\sin 2x)(1+\tan^2 x)$$ $$\implies I=\int_{0}^{ 
\frac{\pi}{4} }\Big[\ln(2+\sin 2x)+\ln(1+\tan^2 x)-\ln(2)\Big]\,\mathrm dx-\int_{0}^{ 
\frac{\pi}{4} }\Big[\ln(2-\sin 2x)+\ln(1+\tan^2 x)-\ln(2)\Big]\,\mathrm dx$$ $$\implies I=\frac12 \Bigg[\int_{0}^{ 
\frac{\pi}{2} }\ln(2+\sin x)\,\mathrm dx-  \int_{0}^{ 
\frac{\pi}{2} }\ln(2-\sin x)\,\mathrm dx\Bigg]$$ We can group together the two log terms in the integral but then this Question will become same as my previous question[ 1 ], I want to calculate both of the integrals separately.","['integration', 'calculus', 'definite-integrals', 'logarithms']"
4187140,Intuition for sigma algebra representing information,"I have a vague understanding of (sub)sigma-algebras in probability theory representing information, especially when conditioning. I want to make this intuition more exact. Suppose we are working in a probability space $(\Omega, F, P)$ . I understand that $\{\emptyset,\Omega\}$ represents no information and that $F$ represents all ""possible"" information. For some specific other examples I also have intuition on their meaning. For a random variable $X$ , $\sigma(X)$ represents knowing the value of $X$ . For an event $A\in F$ , $\sigma(1_A)=\{\emptyset, \Omega, A, A^c\}$ represents knowing whether $A$ happened or not. Now I'm interested in how to interpret conditioning on some arbitrary (usually infinite) subsigma-algebra $G\subset F,$ but the intuition from the above examples seems to break down. I first suspected that we could interpret conditioning on $G$ as knowing for each event in $G$ whether it occurs or not, but this leads to problems. If we take for example $F$ to be the Borel sigma-algebra on $\mathbb{R}$ , then this would mean that any sigma-algebra containing all singletons (which are often null sets) leads to ""full knowledge"". Since we could then for each point $x\in\mathbb{R}$ determine whether the event $\{x\}$ occurred, which should allow us to know exactly in which point of $\mathbb{R}$ we are. In particular, the sigma-algebra $H:=\sigma(\{x\in\mathbb{R}\})$ would contain all the information that the full Borel sigma-algebra $F$ contains. It seems to me that this cannot be the case, so I'm wondering how to solve this flaw in my understanding. What information does conditioning on $F$ give that $H$ does not? I suspect that my intuition breaks down because sigma-algebras in principle only allow countable operations, but to know in what point $x\in\mathbb{R}$ I am, I need to check uncountably many singletons. Still, I'm having trouble formulating what the events in a conditioning sigma-algebra then represent in terms of information. It seems to be something like ""you know of each event individually whether it occurs, but you can't combine this information for different events"", which seems strange/unintuitive to me.","['conditional-expectation', 'measure-theory', 'probability-theory', 'intuition']"
4187175,Rigorously evaluating $\lim\limits_{t\to 1^-}\int_{\sin^{-1}(t/2)}^{\sin^{-1}(t)}\tan^{-1}\left(\frac{t - \sin x}{\cos x}\right)dx$.,"I'd like to find a way to rigorously evaluate the limit $$\lim\limits_{t\to 1^-}\int_{\sin^{-1}\left(\frac{t}{2}\right)}^{\sin^{-1}(t)}\tan^{-1}\left(\frac{t - \sin x}{\cos x}\right)dx$$ This limit arises when one attempts to solve the Basel problem by evaluating the integral $$I(t)=\int_0^t\int_0^t\frac{1}{1-xy}dydx$$ using two methods: (1) expanding the integrand as a geometric series, and (2) using the change of variables $x=(u-v)/\sqrt2$ , $y=(u+v)/\sqrt2$ . Doing so, we find that for every $t\in(-1,1)$ , $$I(t)=\sum_{n=1}^{\infty}\frac{t^{2n}}{n^2}$$ $$\text{ and }$$ $$I(t)=2\left(\sin^{-1}\frac{t}{2}\right)^2+4\int_{\sin^{-1}\left(\frac{t}{2}\right)}^{\sin^{-1}(t)}\tan^{-1}\left(\frac{t - \sin x}{\cos x}\right)dx$$ Now, the series $\sum t^{2n}/n^2$ evaluated at $t=1$ is $\sum 1/n^2$ . This series is convergent, so Abel's theorem implies that $I$ is continuous from the left at $1$ , and that $I(1)$ is the sum of the series $\sum 1/n^2$ (a similar conclusion can be reached for the value $-1$ ). Since $2\left(\sin^{-1} t/2\right)^2$ is continuous on $[-1,1]$ , it follows that the integral $$\int_{\sin^{-1}\left(\frac{t}{2}\right)}^{\sin^{-1}(t)}\tan^{-1}\left(\frac{t - \sin x}{\cos x}\right)dx = \frac{1}{4}\left(I(t)-2\left(\sin^{-1}\frac{t}{2}\right)^2\right)$$ is continuous on $[-1,1]$ , and that we should be able to compute $$\lim\limits_{t\to 1^-}\int_{\sin^{-1}\left(\frac{t}{2}\right)}^{\sin^{-1}(t)}\tan^{-1}\left(\frac{t - \sin x}{\cos x}\right)dx$$ by directly substituting in the value $t=1$ . I'm a bit skeptical about the validity of this reasoning, though. What makes me uncomfortable is the fact that the integrand of the resulting integral $$\int_{\frac{\pi}{6}}^{\frac{\pi}{2}}\tan^{-1}\left(\frac{1-\sin x}{\cos x}\right)dx$$ has a singularity at $\pi/2$ , an element of the domain of integration, so the integrand has suddenly gone from being free of singularities to having one. Granted, the singularity in question is removable, but the fact that singularities aren't present for $t\in(-1,1)$ and that $I$ is continuous at $1$ really makes me wonder ""Why does the integrand suddenly accrue a singularity at $t=1$ ?"". Can someone reassure me that this procedure is valid, and, if you can, explain what's going on for $t=1$ ? If it's not valid, how can I rigorously evaluate the limit in question? Edit I should've mentioned this in the original post. Heuristically, I know that the limit is exactly $\pi^2/36$ . This follows from ""evaluating"" the limiting integral as follows: \begin{align*}
\int_{\frac{\pi}{6}}^{\frac{\pi}{2}}\tan^{-1}\left(\frac{1-\sin x}{\cos x}\right)dx &= \int_{\frac{\pi}{6}}^{\frac{\pi}{2}}\tan^{-1}\left(\frac{1-\cos\left(\frac{\pi}{2}-x\right)}{\sin\left(\frac{\pi}{2}-x\right)}\right)dx\\
&= \int_{\frac{\pi}{6}}^{\frac{\pi}{2}}\tan^{-1}\left(\tan \frac{\frac{\pi}{2}-x}{2}\right)dx\\
&= \int_{\frac{\pi}{6}}^{\frac{\pi}{2}}\left(\frac{\pi}{4}-\frac{x}{2}\right)dx\\
&= \frac{\pi^2}{36}
\end{align*} What I'm more interested in is learning a way to establish this limit rigorously, provided that this integral method does not suffice. I apologize for any confusion.","['integration', 'calculus', 'real-analysis']"
4187234,"I need ""intuition"" about fraction exponents, like $4^{1.2}$. What exactly is it meant to do with number $4$? [duplicate]","This question already has answers here : What's the intuition behind non-integer exponents/powers (4 answers) Closed 2 years ago . $4^3$ is $4\cdot 4\cdot 4$ Then $4^{1.2}$ is what, $4\cdot\dotso$ ?? What happens to the number with a fraction exponent when we try to represent it only using numbers and basic operations (like $+$ , $-$ , $\div$ , and $\cdot$ )? $4^{1.2} = 4*4^{0.2}$ ; but there's still a fractional exponent, $0.2$ , there. I can't represent $4^{1.2}$ only using basic operations. Is there a way to do that? Is it impossible fundamentally in math, or is it possible using imaginary numbers? i checked answers form this 8 year old question but it's not giving the answer i'm looking for.",['algebra-precalculus']
4187253,"Proving $\int_{|x| \leq a} \mathrm d^n x \,\, x_\alpha^4 =\frac{3}{n(n+2)} \int_{|x| \leq a} \mathrm{d}^n x \, |x|^4$","I am trying to compute the following $n$ dimensional integral $$I_4(n)\equiv\int_{|x| \leq a} \mathrm d^n x \,\, x_\alpha^4$$ where $x=(x_1,x_2,\ldots,x_n)\in\mathbb{R}^{n}$ and $x_\alpha$ is one of its components. I have not been able to proceed, but have managed to compute an easier integral: $$I_2 (n) \equiv \int_{|x| \leq a} \mathrm d^n x \,\, x_\alpha^2 = \frac{1}{n} \int_{|x| \leq a} \mathrm d^n {x} \,\, |x|^2 = \frac{a^{n+2} S_{n-1}}{n(n+2)}.$$ The first equality follows from the symmetry of exchanging the component $\alpha$ with any other of the components, and in the last result I have used the surface area of a unit sphere in $n$ dimensions . I tried computing $I_4$ using spherical coordinates but it seems unnecessarily involved. From the paper I am reading, I know I should be able to show that $$I_4 = \frac{3}{n(n+2)} \int \mathrm d^n x \, |x|^4,$$ from which the final result can be computed easily, but I haven't been able to do so. Are there any nice tricks to show this equality? I was also wondering if, in general, we can say $$\int \mathrm d^n x \, x_{\alpha_1} x_{\alpha_2} \cdots x_{\alpha_{2k}} 
= A_{\alpha_1 \alpha_2\cdots\alpha_{2k}} \int \mathrm d^n x \, |x|^{2k}$$ and if there is a way to compute the proportionality $A$ in this case (which is likely to involve some combinations of $\delta_{\alpha_i \alpha_j}$ due to symmetry - I have a very naive guess that this has some connection to constructing traceless symmetric tensors (see this question )…) Edit Here is an attempt using spherical coordinates: From the symmetry, we know that $I_4$ will not depend on the specific index $\alpha$ , so we can choose this as the direction with the first axis. In the $n$ -dimensional spherical coordinates, we have $$\mathrm d^n x = r^{n-1} \sin^{n-2}(\phi_1) \sin^{n-3}(\phi_2)\cdots\sin(\phi_{n-2}) \,\mathrm dr \mathrm d\phi_1 \cdots \mathrm d\phi_{n-1} = r^{n-1} \,\mathrm dS_{n-1} \mathrm dr.$$ Writing $x_\alpha = r \cos(\phi_1)$ gives $$I_4(n) = \int_{|x|\leq a} r^{n+3} \,\mathrm dr \, \cos^4(\phi_1) \sin^{n-2}(\phi_1) \,\mathrm d\phi_1 \mathrm dS_{n-2}  = \frac{a^{n+4}}{n+4} \, \frac{3\sqrt{\pi}\, \Gamma(\frac{n-1}{2})}{n(n+2)\Gamma(\frac{n}{2})} S_{n-2},$$ where I used the integration result (thanks to Mathematica) $$\int_0^\pi \cos^4(\phi_1) \sin^{n-2}(\phi_1) \,\mathrm d\phi_1 = \frac{3\sqrt{\pi}\, \Gamma(\frac{n-1}{2})}{n(n+2)\Gamma(\frac{n}{2})}.$$ The final expression for $I_4$ can be simplified since $S_{n-2} \sqrt{\pi} \,\Gamma((n-1)/2)=\Gamma(n/2) S_{n-1}$ . This leaves us with $$I_4 = \frac{a^{n+4}}{n+4} \frac{3 S_{n-1}}{n(n+2)} = \frac{3}{n(n+2)} 
\int \mathrm d^n x \, |x|^4.$$ However, this seems overkill to me, and I also don't know how to extend it to integrals over higher (even) powers of $x_\alpha$ .","['multivariable-calculus', 'spherical-coordinates', 'multiple-integral']"
4187271,If $P(X\in A)=0$ or $1$ then $X$ has a degenerate distribution,"Let $X$ be a real random variable on a probability space. Suppose that $P(X\in A)=0$ or $1$ for all $A\in\mathcal B(\mathbb R)$ . Is true that $X$ has a degenerate distribution? Is it also true if $X$ takes values in a more general space? Regarding the real case I did the following: Consider the partition $\mathbb R=\bigcup_{n \in\mathbb Z} [n,n+1)$ . By $\sigma$ -additivity we have $\sum_{n \in\mathbb Z}P_X\big([n,n+1)\big)=1$ . Therefore there exists some integer $n$ such that $X\in[n,n+1]$ almost surely. Using additivity again we get $P_X\big([n,n+1/2)\big)+P_X\big([n+1/2,n+1]\big)=1$ , so either $X\in[n,n+1/2]$ almost surely or $X\in[n+1/2,n+1]$ almost surely . Continuing inductively, we obtain a decreasing sequence of closed intervals $I_k$ , each  with length $|I_k|=1/k$ and $P_X(I_k)=1$ . By the nested interval theorem we have $$\bigcap_{k=1}^\infty I_k=\{x\}$$ for some $x\in\mathbb R$ . Moreover, by the properties of measures, we have $P_X(\{x\})=\lim_{k\to\infty} P_X(I_k)=1$ . Hence $X=x$ almost surely. Is this correct?","['measure-theory', 'real-analysis', 'probability-theory', 'probability', 'random-variables']"
4187294,uniform convergence fn to f,"I'm given the following function sequence: $$f_n = \frac{nx}{1+nx^2}, \forall x \in A = [0,\infty].$$ I show the following that: $$\lim_{n \to \infty} \frac{nx}{1+nx^2} \le \frac{nx}{nx^2} \le \frac{1}{x}.$$ And thus my convergent function I compute is $f(x) = \frac{1}{x}.$ However the answer appears to be $f(x) = \frac{1}{2x}$ using A/G mean inequality. This leads to my next question that if $f_n \to f$ converges EITHER point wise or uniformly to $f$ , is $f$ unique?","['uniform-convergence', 'real-analysis']"
4187357,What is the correct formula for the washer method?,"Almost everywhere I look the formula is: $$ \pi \int_b^a {\left(f(x)^2 - g(x)^2\right) dx} $$ where f(x) is the big function and g(x) is the smaller function. Though I've run into problems while calculating the volume using this formula, and through trial and error have found that sometimes the correct formula to use is: $$ \pi \int_b^a {(f(x) - g(x))^2 dx} $$ These are vastly different, and I'm getting quite confused as to which formula is the correct one. The questions that use the second formula are all from my textbook, while my professor uses the first formula during lectures. Is one of these correct? Are they both correct and just for different questions? Thanks for any help.","['integration', 'definite-integrals', 'volume', 'calculus', 'solid-of-revolution']"
4187369,How to evaluate the following limit $\lim_{n\to \infty} \prod_{k=0}^n \left(1+\delta^{2^{k}}\right)$?,"If $\delta \in ]0,1[$ then the following limit is : $$\lim_{n\to \infty} \prod_{k=0}^n \left(1+\delta^{2^{k}}\right)$$ I tried to simplify the following product : $$\begin{align} \prod_{k=0}^n \left(1+\delta^{2^{k}}\right)&= (1+\delta)(1+\delta^2)...\left(1+\delta^{2^{n}}\right)\\
\ln\left(\prod_{k=0}^n \left(1+\delta^{2^{k}}\right)\right) &=\sum_{k=0}^n \ln\left(1+\delta^{2^{k}}\right)
\end{align}$$ I couldn't really go far from here I tried the following : $$0<\delta^{2^{k}}<1 \Leftrightarrow 0<\ln\left(1+\delta^{2^{k}}\right)<\ln(2)$$ But it seems useless. Any idea to evaluate the product hence the limit ?","['infinite-product', 'limits', 'sequences-and-series']"
4187423,When does the sequence $a_{n+2}=a_{n+1}+\frac{a_n}{n^2}$ converge in the p-adics?,"For what p-adic numbers $a_1,a_2$ does the recurrence $a_{n+2}=a_{n+1}+\frac{a_n}{n^2}$ converge? This is inspired by a question that was asked originally but to do with real numbers; this is for fun just to see ""what if we change the field?"". If it does converge, then because the Cauchy criteria in an ultrametric space is $|a_{n+2}-a_{n+1}|\to 0$ then this means $|\frac{a_n}{n^2}|\to 0$ and so $a_n \to 0$ . As a trivial case, $a_1=a_2=0$ works, and I would suspect this is the only time it converges. Since $\frac{1}{n^2}$ can become arbitrarily large, it is competing against $a_n$ terms which get arbitrarily small, but I can't seem to work out a way to force a contradiction that the $a_n$ terms don't miraculously get small fast enough for it to converge. Some partial results, if two consecutive terms are $0$ , then all the terms are $0$ . So there is no alternative way to end up with an eventually all zero sequence. We also have $-a_2 = \sum_{n=1}^\infty \frac{a_n}{n^2}$ which I don't see a way to make useful unfortunately.","['p-adic-number-theory', 'recurrence-relations', 'sequences-and-series']"
4187498,Laurent expansion of $\zeta(s)$,"I am studying the proof of the Prime Number Theorem and I want to show that the function $\frac{\zeta'(s)}{\zeta(s)}$ has a simple pole at $s=1$ . I think that if I can find the Laurent series expansion of $\zeta(s)$ , I could then find the same for $\frac{\zeta'(s)}{\zeta(s)}$ and then conclude that it has a simple pole at $s=1$ .(Correct me if I am wrong.) But, how do I find the Laurent expansion ? I know that $\zeta(s)$ has a simple pole at $s=1$ but how can I use this to find the complete expansion ?
Also, do I even need to find the complete expansion to show that $\frac{\zeta'(s)}{\zeta(s)}$ has a simple pole at $s=1$ ? Is there any other way ? Please help. Any help/hint shall be highly appreciated.","['complex-analysis', 'riemann-zeta', 'analytic-number-theory']"
4187503,For what interval of $a$ is $\int_0^{\frac\pi2}\frac{\cos a\sin x}{1+\sin a\sin x}dx =a\csc a-\frac\pi2\tan \frac a2$ valid?,"In my previous Question [ 1 ], @Quanto has defined $$J(a) = \int_0^{\frac\pi2}\ln(1+\sin a\sin x)\,dx$$ and stated $$J'(a)
=\int_0^{\frac\pi2}\frac{\cos a\sin x}{1+\sin a\sin x}\,dx
=a\csc a-\frac\pi2\tan \frac a2
$$ But, I was wondering if this is valid $\forall a\in \Bbb R-{{n\pi}}$ . Using Desmos, it seems that this is valid for $a\in \Big(-\frac{3\pi}{2},\frac{\pi}{2}\Big)$ . Is there any explanation can we offer? On desmos, I run $a$ from $-1000$ to $1000$ and I see that the the Integral $\Big(J'(a)\Big)$ achieves value only from $-\pi$ to $\pi$ .","['integration', 'calculus', 'definite-integrals', 'convergence-divergence']"
4187530,A question arising from rank of $A^tA$ and rank of $A$,"If $A$ is any real matrix, then $A^tA$ and $A$ have same rank. This result is not necessarily true for other fields; one can see various nice comments on this here . The difference comes from following thing: If $(a_1,a_2,\ldots, a_k)$ is a vector with entries in the field $\mathbb{R}$ , then the pointwise (dot) product of this vector with itself is $0$ only if the vector is $\mathbf{0}$ . This is not true for vectors with complex entries, such as $(1,i)$ . I came to the following natural question: Question: Are there non-real fields $F$ , which have property that $$
\left(a_1^2+a_2^2+ \dots + a_k^2 =0\right) \Rightarrow \left(a_1=a_2=\dots = a_k=0\right)
$$ for some $k\ge 2$ , or for all $k\ge 2$ .","['matrices', 'abstract-algebra', 'linear-algebra', 'field-theory']"
4187550,Matrix Group under multiplication,"Suppose the collection $\{A_1, A_2,...,A_k\}$ forms a Group under matrix multiplication, where each $A_i$ is an $n \times n$ real matrix. Let $A = \sum_{i=1}^{k} A_i$ Show that $A^2 = kA$ If the trace of $A$ is zero, then show that $A$ is the zero matrix. Context I am new to Group Theory and while browsing the web, I recently stumbled upon this question.
I have tried to prove the first part but have no idea how to proceed with the next. Attempt: Part 1 Let us construct a Cayley Table, T $\times$ $A_1$ $A_2$ $...$ $A_k$ $A_1$ $A_1 A_1$ $A_1 A_2$ $...$ $A_1 A_k$ $A_2$ $A_2 A_1$ $A_2 A_2$ $...$ $A_2 A_k$ $...$ $...$ $...$ $...$ $...$ $A_k$ $A_k A_1$ $A_k A_2$ $...$ $A_k A_k$ $$
A^2 = \{A_1 + A_2 + ... + A_k \}\{A_1 + A_2 + ... + A_k \}\\
A^2 = \{A_1 A_1 + A_1 A_2 + ... + A_1 A_k + A_2 A_1 + A_2 A_2 + ... + A_2 A_k +...A_k A_1 + A_k A_2 + ... + A_k A_k\}\\
A^2 = \sum_{i=0}^k \sum_{j=0}^k a_{ij},   \forall a_{ij} \in T\\ 
$$ Since, each row of a Cayley table consists of unique elements of the Group, hence the sum of all elements in each row should be equal to $A$ . As there are k such rows, thus, the sum of all the elements of all the rows must be $kA$ . Therefore, $$
A^2 = kA
$$ Part 2
If, $$
A^2 = kA \\
\implies A^2 - kA = 0 \\
$$ As eigenvalues of $A$ must satisfy the characteristic equation, hence $$
\lambda^2 - k\lambda = 0
$$ Comparing this with the general format of a quadratic characteristic equation $$
\lambda^2 - (trace (A))\lambda + det(A)=0
$$ We get, $k=trace(A)=0$ , as per the given data. This gives as $\lambda=0,0$ .
Hence, all the eigen values are 0. Question: Is there any more elegant proof of the first part than this? How to proceed in the second part? I am completely lost.","['matrices', 'group-theory', 'solution-verification', 'cayley-table']"
4187579,"Showing that if the operator norm of $Dg$ is $< 1$, then $g$ has a fixed-point","I'm trying to solve this following question: Let $g:\mathbb{R^{n}\to R^{n}}$ be a differentiable function. Show that if exists $0\le r<1$ s.t. $\forall a\in \mathbb{R^{n}} \quad \vert\vert{(Dg)_a}\vert\vert_{op}\le r$ , when $\vert\vert{(Dg)_a}\vert\vert_{op} =\max \left\{\vert\vert(Dg)_a \vert\vert_2: \vert\vert x\vert\vert _2=1\right\} $ , then $g$ has a fixed-point in $\mathbb{R^{n}}.$ I wanted to show that $g$ is a contraction mapping, and since $\mathbb{R^{n}}$ is a complete metric space - g admits an unique fixed-point (Banach fixed-point theorem), but I couldn't prove that $g$ is indeed a contraction mapping. Any hint would be appreciated. Thank you!",['multivariable-calculus']
4187589,Are the discrete and trivial topologies dual in some category theory sense?,"Wikipedia's page about the trivial topology says it is also commonly called the codiscrete topology. In it, it also says In some sense the opposite of the trivial topology is the discrete topology, in which every subset is open. This leads to the question: does the codiscrete result from reversing arrows on some category from the discrete topology?","['general-topology', 'category-theory']"
4187621,Shortest paths on a manifold with boundary are composed solely of geodesics and boundary sections?,"Is the following true? Proposition : Let $M$ be a manifold with boundary $\partial M$ . For any $p, q \in M$ let $P$ be a shortest path from $p$ to $q$ . Then $P = \bigcup_{k=1}^n P_k$ where: $P_k$ is a geodesic if $k$ is odd $P_k \subseteq \partial M$ if $k$ is even My intuition is that the proposition (or something like it with suitable corrections) is well known ... but I haven't been able to find a reference. Why I am asking When $M$ is the Euclidean plane and $\partial M$ arises from removing simple polygons, the following Visibility Graph Method can be used to find a shortest path from $p$ to $q$ : Locate the vertices $V$ of the polygons Construct the visibility graph $G$ on $V \cup \{ p, q \}$ : $x, y$ are adjacent in $G$ if the line segment $xy$ is contained by $M$ Search $G$ for a shortest path $P$ from $p$ to $q$ A graph search algorithm (eg Dijkstra's algorithm or A*) will find $P$ as a shortest path in $G$ .
The Proposition then ensures that $P$ is a shortest path in $M$ from $p$ to $q$ . It is natural to ask about the following generalizations of the Visibility Graph Method: $M$ is a 2-manifold and $\partial M$ arises from removing simple polygons where the polygons' edges are geodesics. Here, ""line segment $xy$ "" is replaced with ""geodesic $xy$ "". See for example this question about using visibility graphs to find shortest paths on spheres . $M$ is a 2-manifold and $\partial M$ arises from removing simple regions. (see for example this question about algorithms for finding shortest paths in manifolds ). What I have found so far The Visibility Graph Method for polyline paths in the plane amidst polygon obstacles is studied in introductions to computational geometry and path planning. See for example de Berg et al (2008) who prove the following: Lemma 15.1 (de Berg et al, 2008). Any shortest path between a start point and a goal point among a set of $S$ of disjoint polygonal obstacles is a polygonal path whose inner vertices are vertices of $S$ .  (An inner vertex is a vertex that is neither the begin- nor end-point of the path) The proof of Lemma 15.1 is that if a path $P$ can be locally shortened then it isn't a shortest path. Hence non-polygonal paths get shortened into polygonal paths, and then they get shortened until the interior vertices are vertices of the polygons. Outwardly, the proof of Lemma 15.1 resembles the preamble discussion in Pressley (2010, Section 9.4) in which it is established that the shortest path in a surface between two points is always a geodesic. The similarity is in applying local shortening until the path becomes a geodesic. Consequently for the Proposition, we can at least get to a handwaving proof that (conjecture) the portions of $P$ that are contained in $M - \partial M$ are geodesics . de Berg et al (2008) then set a exercise for extending to obstacles other than polygons. Exercise 15.3 (de Berg et al, 2008). Let $S$ be a set of $n$ disjoint disc-shaped obstacles, not necessarily of equal radius. Prove that the shortest path between two points not seeing each other consists of parts of boundaries of the discs, and/or common tangents of discs, and/or tangents from the start or goal point to the discs. This led me to think that in general (conjecture) shortest paths are composed of geodesics or boundary points . Alexander & Alexander (1981) proved Theorem (Alexander & Alexander, 1981). Let $M$ be a Reimannian $C^3$ -manifold-with- $C^1$ -boundary. A) Any shortest path of $M$ is $C^1$ . B) At any point where it touches the boundary, a shortest path of $M$ possesses an osculating plane which is normal to the boundary. They also commented If $M$ is a Euclidean space with an open convex body removed, then it is easy to see that a shortest path joining parts of the boundary $\partial M$ must lie in $\partial M$ . The Visibility Graph Method is otherwise not in the mainstream of methods for finding shortest paths on surfaces. See for example Bose et al (2011), Crane et al (2020). There appear to be two reasons: the geodesics are not easy to obtain (except in special cases such as the Euclidean plane or the sphere - see for example this question on the kind of computations that are needed generally ), and the feeling that the visibility graph is too large to work with. References de Berg, Mark; van Kreveld, Marc; Overmars, Mark; Schwarzkopf, Otfried. (2008). Computational geometry. Algorithms and applications: Third Edition. , Berlin: Springer. Pressley, Andrew. (2010). Elementary Differential Geometry: Second Edition , Springer Undergraduate Mathematics Series. London: Springer-Verlag. DOI 10.1007/978-1-84882-891-9_9 Alexander, Ralph; Alexander, S. (1981) Geodesics in Riemannian manifolds-with-boundary , Indiana Univ. Math. J. 30, 481-488. ZBL0469.53039 . Bose, Prosenjit; Maheshwari, Anil; Shu, Chang; Wuhrer, Stefanie. (2011) A survey of geodesic paths on 3D surfaces , Comput. Geom. 44, No. 9, 486-498 ZBL1231.65038 . Crane, Keenan; Livesu, Marco; Puppo, Enrico; Qin, Yipeng. (2020) A Survey of Algorithms for Geodesic Paths and Distance Maps . ArXiV 2007.10430","['riemannian-geometry', 'metric-spaces', 'manifolds-with-boundary', 'reference-request', 'differential-geometry']"
4187624,Prove that if $\mathbb Z \cap \{x(t)\ |\ t \in \mathbb R\}$ is non-empty then $x$ is constant.,"Let $f : \mathbb R \longrightarrow \mathbb R$ be a $C^{\infty}$ -function such that $f(x) = 0$ iff $x \in \mathbb Z.$ Suppose that the function $x : \mathbb R \longrightarrow \mathbb R$ satisfies $x'(t) = f(x(t)),$ for all $t \in \mathbb R.$ If $\mathbb Z \cap \{x(t)\ |\ t \in \mathbb R\}$ is non-empty then $x$ is a constant. This question appeared in one of the entrance examinations. It is clear that $x'(t_0) = 0,$ for some $t_0 \in \mathbb R.$ But how do I show that $x'(t) = 0,$ for all $t \in \mathbb R\ $ ? Any help would be much appreciated. Thanks!","['smooth-functions', 'ordinary-differential-equations', 'real-analysis']"
4187631,"Evaluating $\int_0^\infty\frac{x\tan(ax)}{x^2+b^2}\, \mathrm dx$","The question asks to show that $$\mathcal{I}=\int_0^\infty \dfrac{x\tan(ax)}{x^2+b^2}\mathrm dx=\frac{\pi}{e^{2ab}+1}$$ for $a>0$ , $b>0$ . I found this on the internet but searched using Approachzero but found no question. I have been trying this to evaluate using Real methods. Substituting $ax=u$ , I got \begin{align}\mathcal{I}&=a^2\int_0^\infty \frac{u\tan u}{u^2+a^2b^2}\, \mathrm du\\&=\frac{a^2}{2}\int_0^\infty \tan u\, \mathrm d(\ln(a^2b^2+u^2))\end{align} Applying Integration by part here seems problematic since the first term seems to diverge. Probably this method cannot be applied. Then I searched Wolfram Alpha to find ''Computational time exceeded.'' With Wolfram, I found one result that $$\mathcal{M}\bigg(\frac{1}{x^2+b^2}\bigg)=\int_0^\infty \frac{x^{s-1}}{x^2+b^2}\mathrm dx=\frac{\pi}{2}b^{s-2}\csc\bigg(\frac{\pi s}{2}\bigg)$$ , where $\mathcal{M}$ denotes the Mellin Transform. I have just learned some basics of Mellin Transform so I don't know how this can help with this question. How can I solve this question or at least can you give some hints? Edit: After comments by @Dr. Wolfgang Hintze, I searched the book ''Table of Integrals, Series and Products by I S Gradshteĭn'' book and found the same integral without proof.",['integration']
4187666,How close are birational surfaces to being isomorphic?,"Birational geometry is extremely difficult for me to comprehend, because this type of transformation is so general and seems to preserve so little of the geometry. For example cubic surfaces are birational to hyperboloids, but the former class has 27 lines on it and the latter has infinitely many. I have little to no intuition regarding how birational maps behave geometrically and how one should ""imagine"" them, and would like to lessen that gap. Now, for algebraic curves the situation isn't so bad, because we have the theorem that two smooth birational algebraic curves are isomorphic. I'm looking for similar results regrading algebraic surfaces - results that give insight (at least in special cases) about how close birational surfaces are to being isomorphic. I find the above theorem about curves to be a nice result because isomorphism of algebraic varieties does preserve geometric properties reasonably well - like singularities, local structure (Puiseaux expansion) etc, so I just think about smooth birational curves as having the same shape and it seems enough. For surfaces however, there is a very rich theory concerned with birationally classifying them, but I can't seem to understand what is the importance of this theory, because I can't imagine what is ""similar"" about birational surfaces. Are there similar (probably partial) results concerning the relationship between birational equivalence and isomorphism of algebraic surfaces? (I'm aware that every birational map is a sequence of blow-ups and blow-downs, but I don't really understand from that how similar birational surfaces are.)","['birational-geometry', 'algebraic-geometry', 'geometry', 'surfaces']"
4187684,How to play a winning move at this Nim board?,"assuming that I have this situation at Nim : 1,6,12,34,45,23,56,101,212 If I understand right, the is a winning situation because the Nim number is: 154 .
This is the binary numbers of the piles: '00000001', '00000110', '00001100', '00100010', '00101101', '00010111', '00111000', '01100101', '11010100' . When I do addition without carry (XOR) - I got the result - 10011010 = $154$ . So first of all I want to know if I'm right, and then I trying to understand what is the next move that will bring the Nim-Value to $0$ , but I don't understand how to to so this. If I'm taking 154 from the 212 pile, it's still not $0$ ... Can you help me please? Thank you!","['recreational-mathematics', 'discrete-mathematics', 'combinatorial-game-theory']"
4187730,Claim seems incorrect in 19.36b of Spivak's Calculus,"In Spivak's Calculus chapter 19, problem 36 b there is the following claim to prove: Given $a_1, \dots, a_n$ and $b_1, \dots, b_n$ , with $\{b_n\}$ being nonincreasing and nonnegative,  and with $m \leq a_1 + a_2 + \dots + a_k \leq M $ for all $k <= n$ . Prove that $b_1 m \leq a_1 b_1 + \dots +a_n b_n \leq b_1 M$ , and in general: $b_k m \leq a_k b_k + \dots +a_n b_n \leq b_k M$ I managed to prove the first claim easily, but I struggled with the second, so I looked up the solution. The author simply applies the first result for $ a_k, \dots, a_n$ and $ b_k, \dots, b_n$ , and concludes that the inequality must hold. Except that from $m \leq a_1 + a_2 + \dots + a_n \leq M $ it does not necessarily follow that $m \leq a_k + \dots + a_n \leq M $ , so in my opinion the reasoning is incorrect. Furthermore, as a counterexample, if we consider $a_n = \frac{(-1)^{n+1}}{n^2}; b_n = 1$ with $m=0.75, M=1$ , it's easy to see, that with n=3 and k=2 the claim doesn't hold: $$ a_2 b_2 + a_3 b_3 = a_2 + a_3 =  -\frac{1}{4} + \frac{1}{9} = -0.13\dot{8} $$ $$ b_2 m = 0.75 \nleq -0.13\dot{8}  $$ Is my reasoning correct? Am I missing something?","['integration', 'calculus', 'sequences-and-series']"
4187735,"Convergence of a positive series, given that $\sum_{k=1}^n(a_k-a_n)$ is bounded","Question: Assume $\sum_{n=1}^\infty a_n$ is a positive series, with $a_n$ decreases to $0$ , and $$\sum_{k=1}^n(a_k-a_n)$$ is bounded with respect to $n$ . Prove that $\sum_{n=1}^\infty a_n$ is convergent. My idea: Suppose $$(a_1-a_n)+(a_2-a_n)+\cdots+(a_n-a_n)\leq M$$ for any integer $n$ . Then for any integer $m$ , choose a greater $n$ to have $$\begin{align}&\quad\,(a_1-a_n)+(a_2-a_n)+\cdots+(a_m-a_n)\\
&\leq (a_1-a_n)+(a_2-a_n)+\cdots+(a_m-a_n)+\cdots+(a_n-a_n)\\
&\leq M\end{align}$$ since every term in brackets is nonnegative. Let $n\to\infty$ in $$(a_1-a_n)+(a_2-a_n)+\cdots+(a_m-a_n)\leq M$$ to conclude $a_1+a_2+\cdots+a_m\leq M$ . Then let $m\to\infty$ to conclude that $\sum a_n$ is convergent. Is that a reasonable proof? Thank you!!","['convergence-divergence', 'sequences-and-series', 'real-analysis']"
