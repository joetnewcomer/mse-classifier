question_id,title,body,tags
4325188,What is Double Counting?,"Can someone please explain what double counting is? I have no idea what it is, and Google search yields results that are too complicated for me to understand at this point in time. If there's some simple way and example to understand the principle, and apply it to problems, I'd appreciate it a lot. Thanks.","['permutations', 'combinatorics', 'discrete-mathematics']"
4325195,Proof that $\text{det}(AB) = \text{det}(A)\text{det}(B)$ without explicit expression for $\text{det}$,"Overview I am seeking an approach to linear algebra along the lines of Down with the determinant! by Sheldon Axler. I am following his textbook Linear Algebra Done Right . In these references the author takes an approach to linear algebra that avoids using the explicit expression for determinants as much as possible. The motivation for this is that the the explicit expressions for the determinant, while useful for calculating things, are hard to glean intuition from. This question is not about debating the merits of this approach however! I use Axler's terminology which is a little different from what I am used to. What he calls an isometry I would call unitary and what he calls positive I would call positive semi-definite. Further Motivation I would like to provide further information for why I am seeking this proof. I know that in highly general settings (differential geometry for example) it is possible to perform integrals on manifolds using differential $N$ -forms from $\Lambda^N$ . The essential feature of these objects is that they are alternating and multilinear. I want to understand why there is a deep relationship between alternating and multilinear forms. The suggestion is that this relationship occurs because volumes can be characterized, fundamnetally, as something that transforms in an alternating and multilinear way. Unfortunately I don't find this to be a compelling statement about volumes. See my searching here. Help proving that signed volume of n-parallelepiped is multilinear . Rather, the fact that volumes transform in a multilinear and alternating way is actually hard to prove directly and seems to be a little bit particular to the properties of parallelepipeds specifically. And then of course we can tile $\mathbb{R}^N$ using parallelepipeds so the general reesult follows. What seems more intuitive to me is to ""fundamentally"" characterize the signed volume as something scales linearly with coordinate axis scalings (multiplication by a diagonal matrix) and either stays the same or flips sign under a proper or improper rotation (multiplication by a unitary matrix with positive or negative product of eigenvalues). These two properties SEEM like they can proven by only looking at the eigenvalues of transformation matrices but a proof does not seem evident without relying in some way on multilinear/alternating functions. This would imply that the multilinear/alternating characterization of volumes is somehow more fundamental than the one I am proposing. This is just very surprising to me, especially given how tricky it is to prove directly for volumes... Hence my search for a proof about the product of eigenvalues of a product of matrices. Problem Setup My question: Every complex $N\times N$ matrix $A$ has $N$ eigenvalues $\lambda_1, \ldots, \lambda_N$ . Some of these eigenvalues may be repeated and some of them may be equal to zero. Define the determinant of $A$ as $$
\text{det}(A) = \prod_{i=1}^N\lambda_i
$$ It is well known that for two $N\times N$ matrices $A$ and $B$ that $$
\text{det}(AB) = \text{det}(A)\text{det}(B)
$$ However, this proof typically (see below) relies on knowing an explicit expression for $\text{det}$ in terms of the matrix elements of $A$ and $B$ such as $$
\text{det}(A) = \sum_{\sigma \in S_N}\prod_{i=1}^N A_{i, \sigma(i)} = \sum_{i_1, \ldots, i_N=1}^N \epsilon_{i_1\ldots i_N} \prod_{j=1}^N A_{j, i_j}.
$$ I am curious if there is a proof of $\text{det}(AB)  = \text{det}(A)\text{det}(B)$ using the definition I give above and which does not rely on these explicit alternating summations, e.g. in the vein of the Axler approach. One Attempt at a Solution In his textbook, Axler proves a few key theorems, all without using the alternating expressions above. These include facts about eigenvalues of matrices and various spectral and singular value decomposition theoerems. I think polar decomposition may be useful here. Something like: \begin{align}
A =& S_A P_A\\
B =& P_B S_B\\
AB =& S_A P_A P_B S_B
\end{align} With $S_{A,B}$ isometries (unitary) and $P_A$ and $P_B$ are positive (positive semi-definite) matrices. With two facts the proof would be complete: If we could prove $\text{det}(PS) = \text{det}(PS) = \text{det}(S)\text{det}(P)$ for isometry $S$ and positive $P$ (or arbitrary $P$ ) then we would have $$
\text{det}(AB) = \text{det}(S_A)\text{det}(S_B)\text{det}(P_A P_B)
$$ If we could then also show $\text{det}(P_A P_B) = \text{det}(P_A)\text{det}(P_B)$ then the proof would be complete. Other Attempts at a Solution Some other useful things I've learned: It is clear that multiplying a matrix by a unitary matrix does not change the singular values. Also, if I could prove that the product of eigenvalues or singular values of $C = \Sigma_1 U \Sigma_2$ with $U$ unitary and $\Sigma_{1, 2}$ diagonal and positive, is equal to the product of the product of eigenvalues/singular values of $\Sigma_1$ and $\Sigma_2$ I would be able to show that multiplying $A$ and $B$ arbitrary results in multiplying the product of their singular values. The $\text{det(A)}$ could then alternately be defined as something like $A = U \Sigma V^*$ and $\text{det}(A)$ is equal to the product of diagonal elements on $\Sigma$ times the product of eigenvalues of $U$ and $V$ . Finally another approach. All of my approaches so far have basically relied on properties of matrices and matrix manipulations. It might be that this apporach is misguided. Eigenvalues are related to the characteristic polynomial of a matrix (which Axler defines without needing the determinant) and the fundamental theorem of algebra. Perhaps one needs to return to this algebraic domain to get the answer to what I am looking for. I'm pretty inexperienced in that domain, so any ideas would be greatly helpful. Explicit Questions My questions are: Can someone provide a proof for $\text{det}(PS)=\text{det}(SP) = \text{det}(S)\text{det}(P)$ for isometry $S$ and positive (or arbitary) $P$ ? Can someone provide a proof for $\text{det}(P_A P_B) = \text{det}(P_A)\text{det}(P_B)$ for positive $P_A$ and $P_B$ ? Let $C = \Sigma_1 U \Sigma_2$ with $\Sigma_{1, 2}$ diagonal (with positive non-zero entries) and $U$ unitary. Let $\Pi(A)$ equal the product of the singular values of $A$ . Knowing that $\Pi(UA)=\Pi(A)$ for unitary $U$ , Can someone provide a proof that $\Pi(C) = \Pi(\Sigma_1)\Pi(\Sigma_2)$ ? Alternatively, maybe there's an entirely different approach to proving this without the calculational alternating formula? If someone has reason to believe what I am asking for is impossible for some reason I would appreciate comments or answers explaining why it might be impossible. Proofs involving explicit alternating expressions for $\text{det}$ This website builds up the fact that $\text{det}(A)\text{det}(B)$ by expanding $A$ and $B$ into elementary matrices. It then proves that multiplying a matrix by an elementary matrix multiplies the determinant by a known value. The result follows. The issue is that the effect of multiplying by elementary matrix on the determinant is proven using an expansion by minors definition or property of the determinant. https://sharmaeklavya2.github.io/theoremdep/nodes/linear-algebra/matrices/determinants/elementary-rowop.html Pretty much all the proofs on this MSE use some alternating formula or another How to show that $\det(AB) =\det(A) \det(B)$? Wikipedia and Hubbard and Hubbard give a nice proof which (1) uses knowledge that the determinant is the unique alternating multilinear normalized $N$ -form on the columns of a matrix shows that the function $D(A) = \text{det}(AB)$ is alternating and multilinear so that $D(A)$ must be a multiple of $\text{det}(A)$ . We also have $D(I) = \text{det}(B)$ so $D(A) =\text{det}(AB) = \text{det}(A)\text{det}(B)$ . This proof is nice in that it doesn't rely on the explicit alternating expression for $\text{det}$ , but it has the problem that I don't know how to prove the product of eigenvalues satisfies the alternating multilinear property without providing the explicit alternating expression for $\text{det}$ . Other example of this sort of proof can be found on MSE such as Determinant of matrix product Here is a proof that uses an identity on the levi-civita symbol: proving the determinant of a product of matrices is the product of their determinants in suffix / index notation Recent Insights After thinking more about this problem I think I am becoming convinced that what I am asking for is not possible, but I'm realizing a more convincing way to think about thing. There are three related concepts. (1) How a linear transformation scales volumes, $\text{svol}$ and $\text{vol}$ , (2) The product of eigenvalues or singular values of a linear transformation, $\Pi_e$ and $\Pi_s$ , and (3) the alternating multilinear form, $\text{det}$ and $|\text{det}|$ . Above, I have been hoping to give a treatment in which $\Pi_e$ is somehow the most fundamental property and to derive all properties about $\text{svol}$ and $\text{det}$ from the properties of eigenvalues. As I said above, it's starting to look like this may not be possible. Instead I think a more satisfactory approach may be the following. Take $\text{vol}$ or $\text{svol}$ to be fundamental and work from there. The interesting insight I've had is that there are two ways to break down the properties of $\text{vol}$ or under linear transformations. In the first approach, we can see that (a1) when scaling by a diagonal matrix $\text{vol}$ scales by the absolute value of the product of the entries and $\text{svol}$ scales by the product, (b1) when multiplying by a reflection on a single axis that $\text{vol}$ is unchanged while $\text{svol}$ flips sign, and (c1) both $\text{vol}$ and $\text{svol}$ are unchanged by proper rotations. In the second approach we have that (a2) when scaling by a diagonal matrix $\text{vol}$ scales by the absolute value of the product of the entries and $\text{svol}$ scales by the product, (b2) when two axes are swapped $\text{vol}$ is unchanged while $\text{svol}$ flips sign and (c2) $\text{vol}$ and $\text{svol}$ are both unchanged by shear operations. The first approach lends itself to a polar decomposition of the form $$
A = RUP
$$ Where $R$ is either the identity or a reflection across a single axis, $U$ is a special orthogonal matrix, and $P$ is positive-semi-definite. This decomposition lends itself to show the relationship between the volume scaling properties of a transformation and the product of singular values or eigenvalues. The second approach lends itself do a decomposition of the form $$
A = E_1\ldots E_k
$$ Where $E_i$ are elementary matrices. This decomposition lends itself to show the relationship between the volume transforming properties of a transformation and the alternating multilinear form. One can see that properties (a1), (a2) are the same, (b1) and (b2) are essentially the same, and that (c1) and (c2) are similar and equivalent when considered in light of the other properties. So the first approach focuses on rotations while the second focuses on shears. Considering positive definite matrices for the moment, I think one way to think about the relationship between the product of eigenvalues, $\Pi_e$ and the alternating multilinear form $\text{det}$ is that they are equivalent because (1) $\Pi_e$ is related to rotations and axis stretching of volumes and (2) $\text{det}$ is related to shears and axis stretching of volumes. It is still a little bit unsatisfying that we would need to make this relationship with volumes and shears to explain why $\Pi_e(AB) = \Pi_e(A) \Pi_e(B)$ , but one reason I could see for it is that $\Pi_e$ is a very complicated object algebraically. The existence of eigenvalues is only guaranteed by the very non-constructive fundamental theorem of algebra and triangularizability of complex matrices also feels a bit abstract. We know how to work with a transformation and a single eigenvalue or eigenvector, but there is not much machinery to work with ALL the eigenvalues at once, i.e. $\Pi_e$ . Rather, we can use the fact that the product of eigenvalues stretch volumes, and the fact that signed volume are alternating and multilinear, to get at an explicit expression for the product of eigenvalues that allows us to derive properties we wouldn't be able to otherwise, such as $\Pi_e(AB) = \Pi_e(A)\Pi_e(B)$ . Curious for others' thoughts on this. I may be asking (and answering perhaps) a new question related to this last point. I have some partial proofs and will hopefully have a full, hopefully intuitively satisfying, explanation a bit later but not sure.","['determinant', 'eigenvalues-eigenvectors', 'matrices', 'linear-algebra', 'matrix-decomposition']"
4325209,Expected number of iterations until all the cars can no longer move,"I am studying probability questions and I would like to revive this question for more attention. Assume we have an array of length 2n. The first n slots are cars. Each round, we flip n coins where each coin $X_i$ corresponds to car i. $X_i$ is a fair coin and if it is heads, we will move car i to the right if the right space is free. We are interested in the number of expected rounds until all the cars have moved to their final position at the end. The previous question states that this is asymptotically O(nlogn) but I am unable to reason why. Thank you!","['expected-value', 'statistics', 'probability']"
4325263,Generalisation of the Symmetric Group,"For $m\in\mathbb{N}$ , consider the group $G_m=\langle s_1,\dots,s_{n-1}\rangle$ generated by the relations \begin{align*}
s_i^m&=1\\
s_is_j&=s_js_i &|i-j|>1 \\
s_is_js_i&=s_js_is_j & |i-j|=1
\end{align*} If $m=1$ , $G_m$ is trivial. If $m=2$ , $G_m$ is the symmetric group $\mathrm{Sym}_n$ . If $m=0$ , the first relation is trivial and we get the braid group $B_n$ . Is there a name for this group for general $m$ ? Does it have any interesting properties (for example, its irreducible representations in the case $m=3$ )? Any comments or references are appreciated!","['symmetric-groups', 'group-presentation', 'group-theory', 'braid-groups']"
4325298,Prove that the limit $\lim_{x\to 0^+}\frac{1}{x}\sin{(\frac{\pi}{x})}$ does not exist,"I have to prove that $\lim_{x\to 0^+}\frac{1}{x}\sin{\left(\frac{\pi}{x}\right)}$ does not exist. My idea: from the definition of function limit, if I found $x_n\to\infty$ and $y_n\to\infty$ and $x_n,y_n\neq 0$ such that $\frac{1}{x_n}\sin{\left(\frac{\pi}{x_n}\right)}\to l_1$ and $\frac{1}{y_n}\sin{\left(\frac{\pi}{y_n}\right)}\to l_2$ with $l_1\neq l_2$ then I have proved the limit does not exist. I have taken: $x_n=\frac{1}{2n}$ and $y_n=\frac{1}{1/2+2n}$ and so $$\lim_{n\to\infty}\frac{1}{x_n}\sin{\left(\frac{\pi}{x_n}\right)}=\lim_{n\to\infty}2n\sin{(2\pi n)}=0=l_1\\
\lim_{n\to\infty}\frac{1}{y_n}\sin{\left(\frac{\pi}{y_n}\right)}=\lim_{n\to\infty}(1/2+2n)\sin{\left(\frac{\pi}{2}+2\pi n\right)}=\infty=l_2$$ Since $l_1\neq l_2$ then the limit does not exist. Question: my work is right?","['limits', 'functions', 'real-analysis']"
4325334,A density one problem for diophantine equations,"I read that the set of integers that can be written in the form $$n = a^2 + b^4 + c^6$$ is of zero density, since the sum of inverses of exponents $1/2+1/4+1/6$ is less than $1$ . I do not understand the argument, is there a probabilistic/density way of seeing/writing this?","['analytic-number-theory', 'number-theory']"
4325363,Volume between two cones with intersecting axes,"I have two cones with given vertex coordinates, axis direction and aperture angle. I know that the axes of the two cones are intersecting in a point (see the figure attached), which coordinates are straightforward to find. I'd like to find the volume of the intersection (green volume in the figure). Two intersecting cones sketch I made some lit search, but I found nothing and all the approaches I am using seems to fail. Is there a closed form or iterative method to compute that volume? Additionally, what if the cones are more than two?","['euclidean-geometry', 'multivariable-calculus', 'volume']"
4325386,How did Gauss sum Eisenstein series?,"Entry 61 in Gauss's diary states (this is a translation from latin): From the integer powers of $$\int_0^1 \frac{dx}{\sqrt{1-x^4}}$$ depends $$\sum_{m,n}(\frac{m^4 - 6m^2n^2 +n^4}{(m^2+n^2)^4})^k$$ . The expression inside the summation is: $\frac{m^4 - 6m^2n^2 +n^4}{(m^2+n^2)^4} = \frac{1}{(m+n\sqrt{-1})^4}+\frac{1}{(m-n\sqrt{-1})^4} $ , so this entry apparently connects Eisenstein series with integral powers of half the ""lemniscate constant"" $\varpi = 2\int_0^1 \frac{dx}{\sqrt{1-x^4}}$ . Note also of the analogy with ""Basel problem""; here the summation of the fourth powers of inverse ""Gaussian integers"" rationally depends on the fourth power not of $\pi = 2\int_0^1 \frac{dx}{\sqrt{1-x^2}}$ but rather on the fourth power of its lemniscatic analogue. Felix klein and Ludwig Schlesinger, the mathematicians who commented on this diary entry, interpreted Gauss's entry as the statement that the sum of Eisenstein series of weight $4k$ is a rational number times $\varpi^{4k}$ . I also found additional verification of the conjecture that Gauss apparently summed up Eisenstein series in several other sources (mentioned below). Schlesinger and Klein offer a reconstruction of the way Gauss arrived at the statement in entry 61 in p. 516 of volume 10.1 of his works. They say he began with the infinite product expansion of $\mathbb{sinlemn}(z) = \frac{M(z)}{N(z)}$ , and then arrived somehow at an infinite series for the logarithm of the numerator $M(z)$ and the denominator $N(z)$ . The numerator $M(z)$ has zeros at Gaussian integers multiples of $\varpi$ (that is: $M((m+ni)\varpi) = 0$ , where $m,n$ are integers), while $N(z)$ has zeros at Gaussian half-integers multiples of $\varpi$ , and therefore: $$M(z) = z\cdot \prod_{(m,n)\ne (0,0)}(1-\frac{z}{(m+ni)\varpi})$$ $$N(z) = \prod (1-\frac{z}{((m+\frac{1}{2})+(n+\frac{1}{2})i)\varpi})$$ Since the logarithm of an infinite product equals an infinite sum of logarithms, one gets: $$(*)\mathbb{log}M(z) = \mathbb{log}(z)+\sum_{(m,n)\ne (0,0)}\mathbb{log}(1-\frac{z}{(m+ni)\varpi})$$ $$(**)\mathbb{log}N(z) = \sum \mathbb{log}(1-\frac{z}{((m+\frac{1}{2})+(n+\frac{1}{2})i)\varpi})$$ . In addition, on p.159 of volume 10.1 in Gauss's werke, Gauss somehow gives the following infinite series: $$(***)\mathbb{log}M(z) = \mathbb{log}(z) - \frac{1}{60}z^4 - \frac{1}{4200}z^8 -\frac{1}{321750}z^{12} - ...$$ $$(****)\mathbb{log}N(z) =\frac{1}{12}z^4 - \frac{1}{280}z^8 +\frac{1}{4950}z^{12} - ...$$ . Comparing the coefficients of the series for $\mathbb{log}M(z)$ with the coefficients that result from (*) apparently enabled Gauss to sum particular cases of Eisenstein series, and to come up with the statement in his diary entry. Now, I have two misunderstandings concerning this conjecture: The sum $$\sum_{m,n} (\frac{1}{(m+n\sqrt{-1})^4}+\frac{1}{(m-n\sqrt{-1})^4})^k $$ is not exactly   the canonical form of an Eisenstein series because of the second fraction inside the basis of the $k$ power. Therefore, how to show the equivalence of the two sums? I don't understand how the evaluation of the logarithm of the numerator $\mathbb{log}M(z)$ amounts to summation of Eisenstein series. I simply need to see the algebra written more clearly. So can someone explain to me how this reasoning amounts to summing Eisenstein series? Update (13 February, 2022) An additional point of interest that I noted now is that Gauss's series for $\mathbb{log}N(z)$ corresponds to a kind of ""generalized Eisenstein series"" in which the lattice is shifted by $(\frac{1}{2}+\frac{1}{2}i)\varpi$ . Since  such shifted lattice cannot be generated by some action of the modular group on the lattice of Gaussian integers (if it was, one could use the modularity of the Eisenstein series and deduce the series for $\mathbb{log}N(z)$ from that of $\mathbb{log}M(z)$ ), I wondered what tools enable to sum such series and what was Gauss's original method in this case. I tried to make a Google search about ""modular forms defined on shifted lattices"", but without success. Sources Chapter 8 of the book "" Groups Acting on Hyperbolic Space: Harmonic Analysis and Number Theory "" (p.388-390). On p. 390 there it is mentioned that entry 61 in Gauss's diary suggests that the summation of Eisenstein series of weight 4 (to $\frac{\varpi^4}{15}$ ) was already known to Gauss in March 1797. "" Abel's Theorem on the Lemniscate "", article by Michael Rosen (1981). In this article there is also (on p.8) a reference to entry 61. Gauss's collected works, in particular Klein's and Schlesinger's comments on Gauss's works on the lemniscatic elliptic functions (I have linked to Gauss's diary entry 61 in the beginning of my post). Their comments are necessary to gain insight into Gauss's work since Gauss himself isn't particulary clear on this point and doesn't explain what he is doing.","['complex-analysis', 'elliptic-functions', 'math-history']"
4325408,The finite powerset is countable,"I've tried to prove the following theorem: the set of all finite subsets of a countable set is countable. My proof is different from that suggested by my professor and I'm therefore trying to understand if it is correct. To prove such a claim I have to show the existence of a bijection between $A$ and $\mathcal{P}_f(A)$ (the set of all finite subsets). Given that $A$ is countable ex hypothesi I can write it as $\{a_1, a_2,\cdots, a_n,\cdots\}$ . At this point I want to write $\mathcal{P}_f(A)$ in an appropriate way so that I can show the pattern of the bijection. This way would be the following: $$\mathcal{P}_f(A)=\{\emptyset, \{a_1\}, \{a_2\}, \{a_2, a_1\}, \{a_3\}, \{a_3, a_2\}, \{a_3,a_1\}, \{a_3, a_2, a_1\},\cdots, \{a_n\}, \{a_n, a_{n-1}\}, \{a_n, a_{n-2}\},\cdots, \{a_n, a_1\}, \{a_n, a_{n-1}, a_{n-2}\},\cdots\{a_n,a_{n-1},\cdots, a_1\}\}$$ Shown that I can write $\mathcal{P}_f(A)$ with such a patter is my proof complete? I think so since, to prove that $2\mathbb{Z}$ is countable is enough to show that I can write it as $\{0, 1, -1, 2, -2, \cdots\}$ and therefore assigning a natural number in progressive order. Is my solution correct? If not, why?
Thanks in advance for any answer.",['elementary-set-theory']
4325424,Baby rudin theorem 10.7,"We need this definition for the proof of the theorem: Here is the theorem:
Suppose $F$ is a $\mathscr C'$ - mapping ( that means continuously differentiability) of an open set E $\subset R^n$ into $R^n$ , $0 \in E $ , $F(0) = 0$ , and $F'(0)$ is invertible. Then there is a neighborhood of $0$ in $R^n$ in which a representation: $$\mathbf{F}(\mathbf{x})=B_1\cdots B_{n-1}\mathbf{G}_n\circ \cdots \mathbf{G}_1(\mathbf{x})$$ . is valid.
with each $\mathbf{G}_i$ being a primitive $\mathscr{C'}$ mapping in some neighborhood of $0$ , $\mathbf{G}_i(\mathbf{0})=0$ , and $\mathbf{G'}_i(0)$ is invertible, and each $B_i$ is either a flip or the identity operator. Here is the proof: Put $F = F_1$ . Assume $1 \leq m \leq n - 1,$ and make the following induction hypothesis ( which evidently holds for $m$ = 1): $V_m$ is a neighborhood of $0$ , $F_m$ $\in$ $\mathscr C'(V_m)$ , $F_m(0)$ = $0$ , $F_m'(0)$ is invertible, and $$P_{m-1}F_m(x) = P_{m-1}x (  x \in V_m). \tag{$\star$}$$ by ( $\star$ ), we  have: $$F_m(x) = P_{m-1}x + \sum_{i=m}^n \alpha_i(x)e_i\tag{$\ast$}$$ where $\alpha_m,...,\alpha_n$ are real $\mathscr C'$ -functions in $V_m$ . I don't understand from where does the last equality ( $\ast$ ) comes. I would be grateful for any kind of help.","['analysis', 'real-analysis', 'complex-analysis', 'linear-algebra', 'functional-analysis']"
4325436,"The structure of $\sigma(A,\mathcal F)$-measurable functions","Let $(\Omega,\mathcal A)$ be a measurable space, $\mathcal F$ a sub- $\sigma$ -algebra of $\mathcal A$ , and $A\in \mathcal A$ . Show that $f:\Omega \to \mathbb R$ is $\sigma(A,\mathcal F)$ -measurable if and only if $f=1_A f_1 + 1_{A^c} f_2$ for some $\mathcal F$ -measurable maps $f_1,f_2 \to \mathbb R$ . My attempt: Sufficiency is clear so lets show necessity. We first check that $\sigma(A,\mathcal F)=\Big\{(A\cap B_1) \cup (A^c\cap B_2) : B_1,B_2\in\mathcal F \Big\}$ . Suppose first that $f$ is an indicator function, i.e. $f=1_C$ for some $C\in \sigma(A,\mathcal F)$ . Then $f=1_A 1_{B_1} + 1_{A^c} 1_{B_2}$ for some $B_1,B_2\in\mathcal F$ so the claim holds in this case. By linearity the claim also holds if $f$ is simple, i.e. $f=\sum_{i=1}^n c_i 1_{C_i}$ with $c_i\in\mathbb R$ and $C_i\in \sigma(A,\mathcal F)$ for each $i$ . Now let $f$ be arbitrary and choose a sequence $(f_n)=(1_A f_{1,n} + 1_{A^c} f_{2,n})$ of simple functions converging pointwise to $f$ . Define $$f_1(\omega):=\begin{cases} \limsup f_{1,n}(\omega) &  \text{if  } \limsup f_{1,n}(\omega) < \infty \\ 0 & \text{otherwise} \end{cases} $$ $$f_2(\omega):=\begin{cases} \limsup f_{2,n}(\omega) &  \text{if  } \limsup f_{2,n}(\omega) < \infty \\ 0 & \text{otherwise} \end{cases} $$ Then $f_1,f_2$ are real-valued, $\mathcal F$ -measurable, and $f=1_A f_1 + 1_{A^c} f_2$ . Is this correct?","['measure-theory', 'solution-verification', 'measurable-functions']"
4325440,Computation real integral with residue theorem,I need to compute $$I:=\int_{-\infty}^{\infty} \frac{\sin(ax)}{x(\pi^2-a^2x^2)}dx$$ ( $a>0$ ) Probably there is a way to compute it with residue theorem. My thoughts: The singularity at $x=0$ is removable $I=\displaystyle \DeclareMathOperator{Im}{Im} \Im \left( \int_{-\infty}^{\infty} \frac{e^{iax}}{x(\pi^2-a^2x^2)}dx \right)$ . I usually solve those integrals by using the residue theorem to integrate over the upper semicircle (then the integral over the arc goes to 0 as the radius goes to $\infty$ and we are left with I) but this doesn't work since we have non-removable singularities. I also thought about integrating over a circular sector whose border doesn't contain the singularities  (the integrand is an even function) but then the integral over the arc does not go to $0$ anymore and I don't get the expression for $I$ anyway.,"['complex-analysis', 'residue-calculus', 'improper-integrals']"
4325460,A question about the proof of Bernstein-Type Lemmas,"In the book Bahouri, Hajer, Jean-Yves Chemin, and Raphaël Danchin. Fourier analysis and nonlinear partial differential equations. Vol. 343. Springer Science & Business Media, 2011. ,
In the proof of Lemma 2.1. there are the following inequalities $$
\begin{split}
\left \| \partial^\alpha g  \right \|_{L^r} &\leq \left \| \partial^\alpha g  \right \|_{L^\infty} + \left \| \partial^\alpha g  \right \|_{L^1} \quad \color{Red}{\mathbf{(1)}}\\
&\leq C\left \| (1 + |\cdot|^2)^d \partial^\alpha g  \right \|_{L^\infty} \quad \color{Red}{\mathbf{(2)}}\\
&\leq C\left \| (\mathrm{Id} + \Delta)^d ((\cdot)^\alpha \phi)  \right \|_{L^1} \quad \color{Red}{\mathbf{(3)}}\\
&\leq C^{k+1} \quad \color{Red}{\mathbf{(4)}}
\end{split}
$$ where, $g = \mathcal{F}^{-1} \phi$ , $\phi$ is a function of $\mathcal{D}(\mathbb{R}^d)$ with value $1$ near $B = \{ξ ∈ \mathbb{R}^d : |ξ| ≤ R\}$ with $R > 0$ , $\mathcal{D}(\mathbb{R}^d)$ is the space of smooth compactly supported functions on $\mathbb{R}^d$ , $C$ is a constant, $1/r = 1-1/p+1/q$ with $p,q \in [1, \infty]$ . My questions are: for $\color{Red}{\mathbf{(1)}}$ , is it also true for the inequality $\left \| \partial^\alpha g  \right \|_{L^r} \leq \left \| \partial^\alpha g  \right \|_{L^1}$ ? I can understand from $\color{Red}{\mathbf{(1)}}$ to $\color{Red}{\mathbf{(2)}}$ . But I'm not clear from $\color{Red}{\mathbf{(2)}}$ to $\color{Red}{\mathbf{(3)}}$ . Why is it the same constant $C$ from $\color{Red}{\mathbf{(3)}}$ to $\color{Red}{\mathbf{(4)}}$ ?","['normed-spaces', 'fourier-analysis', 'functional-analysis']"
4325461,Can the perimeter of a convex polygon X be an odd number?,"Polygon X is a convex polygon which: is not a triangle has no pair of parallel sides has all vertices with both integer coordinates has sides with a length expressed by a positive integer Can the perimeter of the polygon X be an odd number? In Geogebra I have already drawn several dozen of such figures, but each of them had an even perimeter. I have no idea what I can do to get there.","['convex-geometry', 'geometry', 'polygons']"
4325472,Asymptotic behaviour of a shifted sinus function,"Consider the function $f(x)=\cos(x)-\cos((1+t)x)$ for $t>0$ on $x\in\mathbb{R}$ . I would like to show the following claim which is obvious from looking at the graph of the function. Claim. If $0<t<1$ and $\varepsilon>0$ then there is $x\in\mathbb{R}$ with $|f(x)|>2-\varepsilon$ . What I tried: First I tried to characterise the local maxima, by looking at $f'(x)=(1+t)\sin((1+t)x)-\sin(x)$ . However the equation we obtain by setting $f'(x)=0$ is probably not solvable. Or does somebody know a trick to solve $$
\frac{\sin(x)}{\sin((1+t)x)}=1+t?
$$ Second I tried to find an $x\in\mathbb{R}$ that is close to $2k\pi$ and for which $(1+t)x$ is close to $(2l+1)\pi$ for $k,l\in\mathbb{N}$ . However when I do this I run into an issue I cannot solve. For example: Set $x=\frac{(2l+1)\pi}{1+t}$ for $l$ to be determined. Since the rationals are dense in the continuum we can certainly find $m,n\in\mathbb{R}$ so that $\frac{m}{n}$ is close to $\frac{1}{1+t}$ . Perhaps we can also choose $m=2k$ to be even and $n=2l+1$ to be odd. Hence for all $\varepsilon>0$ we have $$
x=\frac{n\pi}{1+t}=(\frac{1}{1+t}-\frac{m}{n})n\pi+m\pi\leq\varepsilon n\pi+m\pi.
$$ It seems that no matter what I do, the $\varepsilon$ will be in a product with $n$ , however $n$ depends on $\varepsilon$ and thus my proof does not show that $x$ is close to $m\pi$ . How to get rid of this problem? Context. The goal is to show that the semigroup $T_tu(x)=u((1+t)x)$ for $x\in\mathbb{R},t\geq0,u\in C_b(\mathbb{R})$ is not strongly continuous. Obviously any periodic function will do the job, so if you suggest I should better look at a simpler version of $cos$ like the sharktooth-function, then let me know. Edit: So something to note is that these operators do not form a semigroup. In order to solve it, take $T_tu(x)=u(e^t x)$ instead. By continuity of the exponential the calculation will still work.","['semigroup-of-operators', 'analysis', 'real-analysis']"
4325531,Is there any condition that makes a measure zero set necessarily countable?,"Background : Let us consider the Lebesgue measure space $(\Bbb{R}, \mathcal{L}(\Bbb{R}),m) $ . Here measurable set means Lebesgue measurable and measure means Lebesgue measure. $\mathcal{S}\subset \mathcal{P}(\Bbb{R}) $ is called a class of small sets ( or a $\sigma$ -ideal) if $\emptyset \in \mathcal{S}$ $A\in\mathcal{S}$ and $B\subset A$ implies $B\in \mathcal{S}$ $\bigcup_{n\in \Bbb{N}} A_n\in\mathcal{S}$ whenever $A_n\in\mathcal{S}$ for all $n\in \Bbb{N}$ The class of countable sets, null sets, meager sets are all small in one sense and other. In this thread, we want to study the relationship between sets which are small in sense of cardinality and measure. Small in sense of cardinality implies small in sense of measure: $A\subset \Bbb{R}$ is countable implies $m(A)=0$ . Small in sense measure may not implies small in sense of cardinality: Example: The Cantor set . Question: Is there any condition that makes a measure zero set
necessarily countable? UPDATE : Strong measure zero set : $A\subset \Bbb{R}$ is strong measure $0$ set if for every sequence $(\delta_n)\subset \Bbb{R}^+$ there exists a sequence $(I_n)$ of intervals such that $\ell(I_n) < \delta_n$ for all $n$ and $A$ is contained in the union of the $I_n$ . From the definition, it is clear that a strong measure zero set is a null set. The Cantor set is a measure $0$ set but fails to be countable.But the Cantor set is not strong measure zero set(see here ). In a celebrated paper, Borel conjectured that every Strong measure zero set of reals was countable. This statement known as Borel Conjecture . It is now known that this statement is independent of ZFC.","['measure-theory', 'lebesgue-measure', 'metric-spaces', 'real-analysis', 'descriptive-set-theory']"
4325534,How does this expression follow algebraically from the last one?,"I was reading this paper: Global stability for an HIV/AIDS epidemic model with different latent stages and treatment Everything is understood apart from on page 7 of the pdf (page 1486 in the document). How does the author algebraically go from the second line to the last line for the equation of $\dot V$ ? I don't understand how he ""generates"" more terms in the last line compared to the one preceding it. EDIT: For those who cannot access the paper The system: \begin{align*}
\dot S &= \Lambda - (\beta_1 S I_2 +\beta_2 S J )-\mu S \\
\dot I_1 &= p\beta_1 S I_2  +q\beta_2 S J  +\xi_1 J -b_1 I_1\\
\dot I_2 &= (1-P)\beta_1 S I_2  +(1-q)\beta_2 S J +\epsilon I_1 +\xi_2 J -b_2 I_2\\
\dot J &= p_1 I_2 -b_3 J\\
\dot A &= p_2 J - b_4 A
\end{align*} Equilibrium point: \begin{align*}
S^* &= \frac{\Lambda}{\mu \mathcal{R}_0}\\
I_1^* &= \frac{1}{b_1}\left[ p \beta_1 \frac{\Lambda b_3 }{\left(\beta_1 b_3 +\beta_2 p_1 \right)J^* +\mu p_1}\right.
+\left. q \beta_2 \frac{\Lambda  p_1}{\left(\beta_1 b_3 +\beta_2 p_1 \right)J^* +\mu p_1}+\xi_1\right]J^*\\
I_2^* &= \frac{b_3}{p_1}J^*\\
J^*&= \frac{\mu  p_1 }{\beta_1 b_3  +\beta_2 p_1}\left(\mathcal{R}_0-1\right)\\
A^*&=\frac{p_2}{b_4}J^* 
\end{align*} Theorem: If $p=q$ and $\mathcal{R}_0 >1$ then the above equilibrium point is globally stable. Proof: Define Lyapunov function: $$V = S-S^* \ln S+ B( I_1-{I_1}^* \ln I_1) +C(I_2-{I_2}^* \ln I_2) + D( J -J^* \ln J)$$ Derivative is given by: $$\dot V =\left(1-\frac{S^*}{S}\right) \dot S + B\left(1-\frac{{I_1}^*}{I_1} \right)\dot I_1  + C\left(1-\frac{{I_2}^*}{I_2}\right)\dot I_2+D\left(1-\frac{J^*}{J} \right)\dot J$$ The author then does substitutions i.e replaces $\Lambda$ , $b_1$ , $b_2$ , $b_3$ by making the original system equal to $0$ . He finds the constants $B$ , $C$ and $D$ by killing the variable co-efficients, giving: \begin{align*}
B &= \frac{\epsilon}{\epsilon p +b_1(1-p)}\\
C&= \frac{b_1}{\epsilon p +b_1(1-p)}\\
D &= \frac{b1 b_2}{p_1[\epsilon p +b_1(1-p)]} -\frac{\beta_1 S^*}{p_1}
\end{align*} Next he does another substitution $ x=\dfrac{S}{S^*}$ , $y=\dfrac{I_1}{I_1^*}$ , $z=\dfrac{I_2}{I_2^*}$ and $u=\dfrac{J}{J^*}$ to which he arrives at: \begin{align*}
\dot V &= -\mu S^* \frac{\left(1-x\right)^2}{x}+ \left[ \beta_1 S^* {I_2}^*+\beta_2 S^* J^*+B p\beta_1 S^* {I_2}^* + B q \beta_2 S^* J^*\right.\\
&+ B \xi_1 J^*+ C(1-p)\beta_1 S^* {I_2}^*+C(1-q)\beta_2 S^* J^*\\
&+\left. C \epsilon {I_1}^*+C \xi_2 J^*+D p_1 {I_2}^*\right]-x\left[ C(1-p)\beta_1 S^* {I_2}^* \right] -\frac{xz}{y}B p\beta_1 S^* {I_2}^*\\
& -\frac{xu}{y}B q \beta_2 S^* J^* -\frac{u}{y}B \xi_1 J^* - \frac{xu}{z}C(1-q)\beta_2 S^* J^*\\
&- \frac{y}{z}C \epsilon {I_1}^* - \frac{u}{z}C \xi_2 J^*-\frac{z}{u}D p_1  {I_2}^*\\
&- \frac{1}{x}\left[\beta_1 S^* {I_2}^*+ \beta_2 S^* J^*\right]\\
&= -\mu S^* \frac{\left(1-x\right)^2}{x} + \frac{b_1}{\epsilon p +b_1(1-p)}(1-p)\beta_1 S^* I_2^* \left(2-x-\frac{1}{x}\right)\\
&+\frac{b_1}{\epsilon p +b_1(1-p)}\xi_2 J^*\left(2-\frac{u}{z}-\frac{z}{u}\right)\\
&+\frac{b_1}{\epsilon p +b_1(1-p)}(1-q)\beta_2 S^* J^* \left(3-\frac{1}{x}-\frac{xu}{z}-\frac{z}{u}\right)\\
&+\frac{\epsilon}{\epsilon p +b_1(1-p)}p \beta_1 S^* I_2^*\left(3-\frac{1}{x}-\frac{xz}{y}-\frac{y}{z}\right)\\
&+\frac{\epsilon}{\epsilon p +b_1(1-p)} q\beta_2 S^* J^*\left(4-\frac{1}{x}-\frac{y}{z}-\frac{z}{u}-\frac{xu}{y} \right)\\
&+ \frac{\epsilon}{\epsilon p +b_1(1-p)}\xi_1 J^*\left(3-\frac{y}{z}-\frac{z}{u}-\frac{u}{y} \right)
\end{align*} I don't understand how he ""generates"" more terms in the last line compared to the one preceding it.","['lyapunov-functions', 'nonlinear-dynamics', 'ordinary-differential-equations']"
4325553,"Let $f:\mathbb{R}^n\rightarrow \mathbb{R}^n$ be continuously differentiable. $\forall x,y\in \mathbb{R}^n$, $d\big(f(x),f(y)\big)\geq a\cdot d(x,y)$","Let $f:\mathbb{R}^n\rightarrow \mathbb{R}^n$ be continuously differentiable. If there exists a real number $a>0$ , such that $\forall x,y\in \mathbb{R}^n$ , $d\big(f(x),f(y)\big)\geq a\cdot d(x,y)$ , please show that $f:\mathbb{R}^n\rightarrow \mathbb{R}^n$ is a $C^1$ -diffeomorphism. It's clear that $f$ is injective, and the differential $df(x)$ is invertible for any $x\in \mathbb{R}$ , since $f(x+v)=f(x)+df(x)(v)+o(v)$ , as $v\rightarrow 0$ . Then by the inverse function theorem, we have $f$ is a locally diffeomorphism. If $f$ is surjective, then we can show that $f$ is a $C^1$ -diffeomorphism. How can I show that $f$ is surjective?","['multivariable-calculus', 'diffeomorphism', 'real-analysis']"
4325649,"Is there a distribution over all distributions in [0,1]?","Let's call $D$ to the set of all distributions over the interval $[0,1]$ , including the uniform, the triangular, the Dirac-delta at $0$ , and in general any measure $\mu$ such that $\mu([0,1])=1$ . We can define several types of distributions over $D$ . For example, provided a sequence of distributions $\mu_1, \mu_2, ...\in D$ , we could just draw a random index $k$ using a fixed distribution over $\{1,2,...\}$ and output $\mu_k \in D$ .
Similarly as a second example, provided a family of distributions $\mu_\theta$ parametrized by a vector $\theta\in\mathbb R^d$ , we can draw $\theta$ from a fixed distribution over $\mathbb R^d$ and output $\mu_\theta \in D$ . But in both examples, we are restricted to a fixed family of distributions ( $\mu_i$ or $\mu_\theta$ ).
I wonder if there is an ""unrestricted"" distribution over $D$ that ""captures all distributions in $D$ "". Can you provide an example of a distribution $\phi$ over $D$ whose support is $D$ ? To define support formally, use the earth's mover distance , or any Wasserstein metric, to define the distance between any two distributions $\mu,\mu'\in D$ . Since these distances are metrics for $D$ , they define a topology over $D$ and enable the notion of support of any distribution $\phi$ over $D$ .","['measure-theory', 'probability-distributions', 'probability-theory']"
4325651,"Prove that sets $A=(-1,5]$ and $B=(3,11)$ have the same cardinality","Prove that sets $A=(-1,5]$ and $B=(3,11)$ have the same cardinality. Using Schröder-Bernstein theorem Injection $f:A\to B$ $$f(x)=x+5$$ Injection $g:B\to A$ $$g(x)=\frac{1}{2}x-1$$ Since $|A|\leq|B|$ and $|B|\leq|A|$ , then $|A|=|B|$ . Constructing a bijection Let $L\subset A$ where $L=\{l_k=\frac{6}{2^{k}}-1 \:|\: k \in \mathbb N\}$ .
Let $A_1=(-1,5)$ and $p:A\to A_1$ such that: $$p(x)=\begin{cases}
l_1\quad\quad &\text{if } x=5 \\
l_{k+1}\quad &\text{if } x=l_k\in L \\
x\quad &\text{if } x \in (-1, 5] \setminus (L \cup \{5\}) \\
\end{cases}
$$ Let $q:A_1\to B$ such that $$q(x)=\frac{3}{2}x+\frac{13}{2}.$$ A bijection between $A$ and $B$ would be the composite function $h:A\to B$ defined as $h(x)=(p\circ q)(x)$ . Can someone verify that I've done this correctly? Is there a way to create a bijection between $A$ and $B$ without using a composite function?","['functions', 'solution-verification', 'discrete-mathematics']"
4325700,Different ways to evaluate $\sum_{n=1}^\infty\frac{H_nH_n^{(2)}}{(n+1)(n+2)(n+3)}$,"The following question: How to compute the harmonic series $$\sum_{n=1}^\infty\frac{H_nH_n^{(2)}}{(n+1)(n+2)(n+3)}$$ where $H_n=\sum_{k=1}^n\frac{1}{k}$ and $H_n^{(2)}=\sum_{k=1}^n\frac{1}{k^2}$ , was proposed by @Tolaso (The original question had been closed and deleted). My friend Cornel and I managed to compute it in different ways and would like to see other solutions if possible . Cornel's solution : We start with the following key identity presented in the book (Almost) Impossible Integrals, Sums, and Series ) , Sect. $4.19$ , pages $290-291$ , $$\varphi(n)=\sum_{k=1}^{\infty} \frac{H_k H_k^{(2)}}{(k+1)(k+n+1)}$$ $$\small =2\zeta(3)\frac{H_n}{n}+\frac{\zeta(2)}{2}\frac{H_n^2}{n}-\frac{\zeta(2)}{2}\frac{H_n^{(2)}}{n}-\frac{H_n^{(4)}}{4n}-\frac{(H_n^{(2)})^2}{4n}-\frac{H_n}{n}\sum_{i=1}^n \frac{H_i}{i^2}+\frac{1}{2n}\sum_{i=1}^{n} \frac{H_i^2}{i^2}.\tag1$$ Then, based on $(1)$ we obtain that $$\sum_{k=1}^{\infty} \frac{H_k H_k^{(2)}}{(k+1)(k+2)(k+3)}=\varphi(1)-\varphi(2)=\frac{1}{2}\zeta(3)-\frac{1}{4}\zeta(2)-\frac{1}{32}.$$ End of story A note: the whole process is completed by using series manipulations only, with no use of integrals. My solution : Replace $x$ by $xyz$ in the generating function : $$\sum_{n=1}^\infty H_nH_n^{(2)}x^n=
\frac{\operatorname{Li}_3(x)+\operatorname{Li}_3(1-x)+\frac12\ln x\ln^2(1-x)-\zeta(2)\ln(1-x)-\zeta(3)}{1-x}=f(x),$$ we get $$\sum_{n=1}^\infty H_nH_n^{(2)}(xyz)^n=f(xyz).$$ Multiply both sides by $yz^2$ then integrate w.r.t $x$ , $y$ and $z$ using the fact $$\int_0^1\int_0^1\int_0^1 x^n y^{n+1}z^{n+2}dxdydz=\frac{1}{(n+1)(n+2)(n+3)}$$ we obtain $$\sum_{n=1}^\infty\frac{H_nH_n^{(2)}}{(n+1)(n+2)(n+3)}=\int_0^1\int_0^1\int_0^1 yz^2f(xyz)dxdydz$$ $$\overset{x=t/y}{=}\int_0^1\left[\int_0^1\int_0^y z^2f(zt)dtdy\right]dz=\int_0^1\left[\int_0^1\int_t^1 z^2f(zt)dydt\right]dz$$ $$=\int_0^1\left[\int_0^1 z^2f(zt)(1-t)dt\right]dz\overset{t=u/z}{=}\int_0^1\int_0^z zf(u)\left(1-\frac{u}{z}\right)dudz$$ $$=\int_0^1\int_u^1 f(u)(z-u)dzdu=\frac12\int_0^1(1-u)^2f(u)du=\frac12\int_0^1 u^2f(1-u)du$$ $$=\frac12\underbrace{\int_0^1 u\operatorname{Li}_3(1-u)du}_{1-u\to u}+\frac12\int_0^1 u\operatorname{Li}_3(u)du$$ $$+\frac12\int_0^1 u\left[\frac12\ln(1-u)\ln^2(u)-\zeta(2)\ln(u)-\zeta(3)\right]du$$ $$=\frac12\int_0^1 \operatorname{Li}_3(u)du+\frac12\int_0^1 u\left[\frac12\ln(1-u)\ln^2(u)-\zeta(2)\ln(u)-\zeta(3)\right]du$$ $$=\frac12\left(\zeta(3)-\zeta(2)+1\right)+\frac12\left(\frac12\zeta(2)-\frac{17}{16}\right)$$ $$=\frac12\zeta(3)-\frac14\zeta(2)-\frac1{32}.$$","['integration', 'alternative-proof', 'harmonic-numbers', 'polylogarithm', 'sequences-and-series']"
4325720,Prove Wold's theorem from abstract Wold's decomposition,"I see that Wold's theorem in time series analysis is proven by the abstract Wold's decomposition in operator theory. The abstract theorem states that: Every isometry is a direct sum of copies of the unilateral shift and a unitary operator. I want to understand intuitively the connection between the two theorems: so in the time series version $Y_t = \sum_{j=0}^\infty b_j \epsilon_{t-j} + \eta_t$ , which are the isometry, the shift and the unitary operator? PS: I see in some document which states the lag operator is the isometry, then which is the shift operator in the time series version?","['statistics', 'operator-algebras', 'operator-theory', 'functional-analysis', 'time-series']"
4325742,Proving the finiteness of Expectation given a geometric condition,"For a non-negative integer-valued random variable $X$ , it is given that $P(X > kN) \le p^k$ for any $k \ge 1$ with some fixed $0 < p < 1$ and fixed $N > 0$ , then show that $E[X] < \infty$ . My approach : We know that for an integer valued RV which is non-negative, $E[X]=\sum_{i=0}^{\infty}P[X>i]$ . My plan was to take $k=\frac{i}{N}$ and thus $E[X]$ is bounded above by a geometric series. But the only caveat is that $k$ in my case might not be $\ge 1$ . I am stuck here! Anyone with other ideas?","['probability-distributions', 'expected-value', 'martingales', 'probability-theory', 'probability']"
4325760,Are there any continuous time-limited Linear and Time-Invariant (LIT) functions with unbounded derivative?,"Are there any continuous time-limited Linear and Time-Invariant (LIT) functions with unbounded derivative? Let think about a continuous and time-limited function $q(t)$ that is representing a classical mechanics phenomena, which can be represented as the output of a LIT system with impulse response $h(t)$ . Now, I want to know which is the maximum rate of change that can possible achieve the function $q(t)$ , so as a worst case I test this system against a discontinuous jump-alike change by using as input the unitary standard step function $\theta(t)$ , so: $$q(t) = h(t)\circledast\theta(t)$$ related through the convolution operator as every LIT system. Now, I believe that since $q(t)$ is continuous and time limited, and $\theta(t)$ is not a compacted-supported function, the only alternative to made $q(t)$ as it is, is by an $h(t)$ function that is also continuous and time-limited ( please correct me if I am wrong, this is the most important assumption on the presented line-of-thought ). With this, since $q(t)$ and $h(t)$ are both continuous and compact-supported (because they are both time-limited), they are also bounded functions $\sup_t |q(t)| < \infty$ and $\sup_t |h(t)| < \infty$ . And also remember, that since $q(t)$ and $h(t)$ are both time-limited, it implies that both are also of unlimited bandwidth (i.e., they aren´t band-limited functions). Now, following the exercise n° 4.49 of the book ""Signals and Systems, 2nd Edition"" (Alan V. Oppenheim, Alan S. Willsky, with S. Hamid) [1] , and the following properties: $\frac{d}{dt}\Big(x(t) \circledast y(t) \Big) = \frac{dx(t)}{dt} \circledast y(t) = x(t) \circledast \frac{dy(t)}{dt}$ as is shown on Wiki : Algebraic Properties --> Relation with differentitation. The derivative of the unitary step function is the Dirac's delta function $\theta'(t) = \delta(t)$ , as is shown on Wiki . and $f(t) = f(t) \circledast \delta(t)$ as is shown on Wiki : Algebraic Properties --> Multiplicative identity. So, I will have that $$q'(t) = h(t) \circledast \theta'(t) = h(t) \circledast \delta(t) = h(t)$$ and since $|h(t)|< \infty$ is bounded if my first assumptions are right, I will have that the maximum rate of change of $$\sup_t |q'(t)| = sup_t |h(t)| < \infty$$ so even they are functions of unlimited bandwidth they will ALWAYS have a bounded derivative : A) Does this means that every continuous and time-limited LIT functions have a bounded maximum rate of change $\sup_t |q'(t)|< \infty$ ? By construction, I already know that if impulse response $h(t)$ is continuous and time-limited then the maximum rate of change of $q(t)$ will be bounded, if the analysis against the unitary step $\theta(t)$ is general enough ( Is that so? ). But in the other way, if $q(t)$ is continuous and time-limited, I feel that it could not be necessarily implying that $h(t)$ is also continuous and time-limited, since $q(t) = h(t) \circledast \theta(t) = \int_{-\infty}^t h(t)\,dt$ and I am not sure if the integral of a discontinuous function could lead to a continuous function, hope you can explain this too . Added later: I have just notice that any function can be described as: $$q(t) = \int_{-\infty}^t q'(t)\,dt = q'(t)\circledast \theta(t)$$ so requiring that the impulse response function $h(t) \equiv q'(t)$ to be continuous and time-limited is already requiring that $|q'(t)|<\infty$ been bounded. Other interesting facts from exercise 4.49: Since from the properties of the Laplace Transform of $q(t)$ given by $Q(s)$ , I will have that: $$ \lim_{t \to \infty} q(t) = q(\infty) = \lim_{s \to 0} s\,Q(s) = \lim_{s \to 0} \int_{-\infty}^\infty \frac{dq(t)}{dt}e^{-st}dt = \int_{-\infty}^\infty q'(t)\,dt = \int_{-\infty}^\infty h(t)\,dt$$ so I can model the ""minimum possible response time to the step function"" $\Delta t_{\min}$ as: $$ \sup_t |q'(t)| = \Bigg| \frac{q(\infty)}{\Delta t_{min}}\Bigg| \Rightarrow \Delta t_{min} = \frac{|q(\infty)|}{\sup_t|q'(t)|} = \frac{|\int_{-\infty}^\infty h(t)\,dt|}{\sup_t |h(t)|} $$ so I could define an ""effective Bandwidth"" for this system response to the step function using as maximum frequency the quantity $f_{max} = 1/ \Delta t_{min}$ : $$B_W = 2 f_{max} = \frac{2}{\Delta t_{min}} = \frac{2\sup_t |h(t)|}{|\int_{-\infty}^\infty h(t)\,dt|}$$ with Uncertainty relation $$ B_W \cdot \Delta t_{min} > 2$$ .","['fourier-analysis', 'derivatives', 'physics', 'signal-processing', 'finite-duration']"
4325764,Demonstration of inequality between 2 variances expressions,"Just to remind, $C_\ell$ is the variance of random variables $a_{\ell m}$ following a centered Gaussian PDF (in spherical harmonics of Legendre) : $$C_{\ell}=\left\langle a_{l m}^{2}\right\rangle=\frac{1}{2 \ell+1} \sum_{m=-\ell}^{\ell} a_{\ell m}^{2}=\operatorname{Var}\left(a_{l m}\right)$$ Second observable : $$
\sigma_{D, 2}^{2}=\dfrac{2 \sum_{\ell_{\min }}^{\ell_{\max }}(2 \ell+1)}{\left(f_{s k y} N_{p}^{2}\right)}
$$ so : $$
\sigma_{o, 2}^{2}=\dfrac{\sigma_{D, 2}^{2}}{\left(\sum_{\ell_{\min }}^{\ell_{\max }}(2 \ell+1) C_{\ell}\right)^{2}}
$$ First observable : $$
\sigma_{D, 1}^{2}=\sum_{\ell_{\min }}^{\ell_{\max }} \dfrac{2}{(2 \ell+1)\left(f_{s k y} N_{p}^{2}\right)}
$$ so : $$
\sigma_{o, 1}^{2}=\dfrac{\sigma_{D, 1}^{2}}{\left(\sum_{\ell_{\min }}^{\ell_{\max }} C_{\ell}\right)^{2}}
$$ Goal :
I would like to prove than $\sigma_{o, 1}^{2}<\sigma_{o, 2}^{2}$ but I have difficulties to derive this inequality. UPDATE : from the preliminary results of a colleague, it may show the contrary, i.e that the inequality is rather : $\sigma_{o, 2}^{2}<\sigma_{o, 1}^{2}$ But I have to double check, there should be an error since numerically, I find that $\sigma_{o, 1}^{2}<\sigma_{o, 2}^{2}$ . Any help is welcome","['statistics', 'variance', 'estimation', 'spherical-harmonics', 'probability']"
4325785,"Why, in the rules regarding the discriminant determining whether roots are real or not, does it matter that the quadratic has real coefficients?","Why, in the rules regarding the discriminant determining whether roots are real or not, does it matter that the quadratic has real coefficients? For the roots to be real we have $$\Delta=b^2-4ac\ge	0 \implies b^2 \ge 4ac$$ If $b$ was imaginary $b^2$ would end up being negative. If the product $4ac < b^2$ we can still have $b^2 - 4ac < 0 $ . In this scenario the rules regarding the discriminant does not hold.","['algebra-precalculus', 'solution-verification', 'quadratics', 'polynomials']"
4325829,expectation value - box and balls,"Problem: A box contains 2 green, 4 blue and 2 yellow balls, one ball is chosen $n$ times (independently, with repetitions). Let $X$ be the total number of chosen green balls, $Y$ be the total number of chosen blue balls. Find $E(X|X+Y)$ . Now first of all $P(X|X+Y)=\frac{P(X\cap(X+Y))}{P(X+Y)}$ but $X$ and $Y$ seem like numbers to me and not events so calculating $P(X\cap(X+Y))$ seems weird to me, or maybe I am missing something. Secondly, how do I calculate $\frac{P(X\cap(X+Y))}{P(X+Y)}$ this? Thanks in advance.","['random-graphs', 'probability-theory', 'probability']"
4325837,Independence of $A$ and $B$ implies the independence of $\neg A$ and $B$,Does the following apply? $$P(A\mid B)=P(A)\implies P(\neg A\mid B)=P(\neg A)$$ My rough answer is that suppose $A$ is the probability of rainy and $B$ is the probability of toothache. Then both $P(A\mid B)=P(A)$ and $P(\neg A\mid B)=P(\neg A)$ apply. But can we prove this mathematically?,"['independence', 'probability']"
4325845,Evaluating $\mathbb E [\lvert \frac{1}{B_{t}}\int\limits_{0}^{t} K_{s}dB_{s}\rvert ^{1/4}]$ as $t \to 0$,"Let $B$ be a standard Brownian motion starting in $0$ and $K$ be a continuous, adapted process starting in $0$ and such that $\sup\limits_{0\leq s \leq t}\lvert K_{s}\rvert < \infty$ I want to show that $\mathbb E [\lvert \frac{1}{B_{t}}\int\limits_{0}^{t} K_{s}dB_{s}\rvert ^{1/4}]\to 0, \; t \to 0$ Any ideas? I thought of using the Cauchy-Schwarz inequality so that eventually I get a term $$ \mathbb E [\lvert \frac{1}{B_{t}}\int\limits_{0}^{t} K_{s}dB_{s}\rvert ^{2}]$$ and perhaps then to use Ito's isometry but the problem is the $B_{t}$ in the denominator.","['stochastic-integrals', 'stochastic-processes', 'brownian-motion', 'probability-theory', 'stochastic-calculus']"
4325846,How do I formally show that the Zariski tangent space of the intersection of two closed subschemes is the intersection of the tangent spaces?,"I am looking for help in writing down the following formally/mathematically: Let $(A, \frak p)$ be a local ring with $I \subset \frak p$ and $J \subset \frak p$ . The Zariski cotangent space of $A/I$ at $\frak p$ can be identified with $\frac{\frak p/\frak p^2}{(I+\frak p^2)/\frak p^2}$ which tells us that the tangent space of $A/I$ at $\frak p$ is a subspace of the tangent space of $A$ at $\frak p$ . Similarly for the  closed subscheme cut out by $J$ . The cotangent space of $A/(I+J)$ at $\frak p$ can be identified with $\frac{\frak p/\frak p^2}{(I+J+\frak p^2)/\frak p^2}$ . Informally the tangent space of $A/I$ at $\frak p$ is cut out by the ideal $(I+\frak p^2)/\frak p^2$ , the tangent space of $A/J$ at $\frak p$ is cut out by the ideal $(J+\frak p^2)/\frak p^2$ , the tangent space of $A/(I+J)$ at $\frak p$ is cut out by the ideal $(I+J+\frak p^2)/\frak p^2$ . From this I gather that the intersection of the tangent spaces of $A/I$ and $A/J$ at $\frak p$ will be cut out by  the ideal $(I+J+\frak p^2)/\frak p^2$ ? Not sure how to really write this out. Therefore, the tangent space of the intersection $\operatorname{Spec}A/I  \cap \operatorname{Spec} A/J=\operatorname{Spec}A/(I+J)$ at $\frak p$ is the intersection of the tangent spaces of $\operatorname{Spec}A/I$ and $\operatorname{Spec}A/J$ at $\frak p$ .","['proof-writing', 'algebraic-geometry', 'linear-algebra', 'solution-verification', 'commutative-algebra']"
4325988,How to derive the decomposition of the MSE of a Rao-Blackwellized estimator?,"In the Wikipedia article for the Rao-Blackwell theorem , the following step is used in the proof, namely a decomposition of the Rao-Blackwellized estimator's decomposition: $$\mathbb{E}[(\delta_1(X) - \theta)^2] = \mathbb{E}[(\delta(X) - \theta)^2] - \mathbb{E}[\text{Var}(\delta(X) | T(X))]$$ I'm aware there's other questions regarding proofs of the Rao-Blackwell theorem, but none seem to be about this specific step. I've tried replacing $\delta_1$ with its expression in terms of $\delta$ and expanding the square to no avail...","['statistics', 'probability', 'estimation']"
4326002,Are the following definitions of conjugates related?,"I have seen the following two definitions of conjugates and I'm wondering if these definitions are related somehow? In ""A Book of Abstract Algebra"" by Pinter, the definition is Let $G$ be a group (with operation $*$ ) and $a, x \in G$ . then $x * a * x^{-1}$ is called a conjugate . In ""Linear Algebra Done Right"" by Axler, the definition is For the real field, the conjugate of $x \in \mathbb{R}$ is $x$ . For the complex field, the conjugate of $(a, b) \in \mathbb{C}$ is $(a, -b)$ .","['group-theory', 'definition']"
4326112,Alternating sum of binomials divided by a polynomial of the index,"I'm working with a sequence of $k$ by $k$ matrices $M^n$ whose entries satisfy $$M^n_{ij} = \binom{j-1}{i-1} \sum_{l=0}^{j-i} \binom{j-i}{l} (-1)^{l} (\frac{1}{i+l})^n, \quad n \in \mathbb{N}$$ I've been trying without luck to simplify this sum, specifically $$  \sum_{l=0}^{j-i} \binom{j-i}{l} (-1)^{l} (\frac{1}{i+l})^n $$ One avenue is to expand out the binomials in the denominator: $$\sum_{l=0}^{j-i}\frac{\binom{j-i}{l} (-1)^l}{\sum_r^n \binom{n}{r}i^nl^{r-n}} $$ But I think this may be going backwards rather than forwards. I also tried the strategy in https://math.stackexchange.com/a/3369814/978915 of using generating functions, i.e. $$\bigg(\frac{1}{l+i}\bigg)^n = n![x^n]\exp\big(\frac{x}{l+i}\big)$$ where $[]$ is the ""coefficient of"" operator. This leads to $$\sum_{l=0}^{j-i} \binom{j-i}{l} (-1)^{l} \big(\frac{1}{i+l}\big)^n =  n![x^n] \sum_{l=0}^{j-i} \binom{j-i}{l} (-1)^{l} \exp\big(\frac{x}{l+i}\big) $$ $$= n![x^n] \sum_{l=0}^{j-i} \binom{j-i}{l} \exp(-\mathbf{i}\pi l) \exp\big(\frac{x}{l+i}\big) = n![x^n] \sum_{l=0}^{j-i} \binom{j-i}{l} \exp\big(\frac{\mathbf{i}\pi l(l+i) + x}{l+i}\big) $$ Unfortunately the $(-1)^l$ and the exponential can't be simplified in the same way as in that example. There's also this interesting identity from https://math.stackexchange.com/a/3657580/978915 , which does not cover this case because here $0 \leq \tau \leq k$","['generating-functions', 'combinatorics', 'discrete-mathematics', 'sequences-and-series']"
4326128,"How do I find $S(p)=1+\frac{x^{p}}{p !}+\frac{x^{2 p}}{(2 p) !}+\frac{x^{3 p}}{(3 p) !}+\cdots$ by complex numbers, where $p \in N$?","In my post on Quora , I had expressed the three series in terms of sine and cosine: $$1+\frac{x^{3}}{3 !}+\frac{x^{6}}{6 !}+\frac{x^{9}}{9!}+\cdots \cdot=\frac{1}{3}\left[e^{x}+2 e^{-\frac{x}{2}} \cos \left(\frac{\sqrt{3}}{2} x\right)\right]\\
\frac{x^{2}}{2 !}+\frac{x^{5}}{5 !}+\frac{x^{8}}{8 !}+\cdots=\frac{1}{3}\left[e^{x}-e^{-\frac{x}{2}}\left(\sqrt{3} \sin \left(\frac{\sqrt{3}}{2} x\right)+\cos \left(\frac{\sqrt{3}}{2} x\right)\right)\right]\\
x+\frac{x^{4}}{4 !}+\frac{x^{7}}{7 !}+\cdots=
\frac{1}{3}\left[e^{x}+e^{-\frac{x}{2}}\left(\sqrt{3} \sin \left(\frac{\sqrt{3}}{2} x\right)-\cos \left(\frac{\sqrt{3}}{2} x\right)\right)\right].$$ They all have the same “length” $3$ . I then think that the proof can be extended for a general series $S(p)$ with an arbitrary length $p$ and try to prove similarly that for any natunal number $p$ , $$
S(p)=1+\frac{x^{p}}{p !}+\frac{x^{2 p}}{(2 p) !}+\frac{x^{3 p}}{(3 p) !}+\cdots=\frac{1}{p}\left(e^{x}+e^{\omega x}+e^{\omega^{2} x}+\cdots+e^{\omega^{n-1} x}\right) \tag*{(*)} 
$$ where $\omega$ is the complex $p$ -th root of unity satisfying $$1+\omega+\omega^{2}+\cdots+\omega^{p-1}=0.$$ My question is how to prove (*) and express , in terms of sine and cosine, the series $$e^{x}+e^{\omega x}+e^{\omega^{2} x}+\cdots+e^{\omega^{p-1} x}$$","['trigonometry', 'abstract-algebra', 'complex-numbers', 'sequences-and-series']"
4326242,Probability question with a deck of cards,"Just started to learn maths, so I'm sorry if this an elementary question. The question is: here is a deck with 40 cards ; 10 cards, each 10 with one of the 4 shapes (hearts, diamonds, clubs and spades). All the cards are numbered from 1 to 10. Every card has the same probability to be drawn.
We start drawing cards from the deck. If the 11th card which was drawn is the first one with number 10 on it, what is the probability that the next card is a clubs? What I have tried:
We need to find P(A|B),
where A is the probability that the 12's card is a clubs, and B is the probability that the 11's card is a first ten. I chose the sample space to be picking 12 cards from 40. $P$ ( $\Omega$ ) = $\binom{40}{12}$ $P(B)$ = $\binom{36}{10}\binom{4}{1}\binom{29}{1}$ . Where in $P(B)$ , i chose 10 cards from 36 cards(without 10's), the picking 1 card from the the 10, the choosing from the rest. $P(A\cap B)$ = $\binom{9}{9}\binom{26}{1}(\binom{3}{1}\binom{1}{1} +\binom{1}{0})$ $+$ $\binom{9}{8}\binom{26}{2}(\binom{3}{1}\binom{2}{1}+\binom{1}{1})$ $+$ $\binom{9}{7}\binom{26}{3}(\binom{3}{1}\binom{3}{1} + \binom{2}{1})$ $+$ $\binom{9}{6}\binom{26}{4}(\binom{3}{1}\binom{4}{1}+\binom{3}{1})$ $+$ $\binom{9}{5}\binom{26}{5}(\binom{3}{1}\binom{5}{1}+\binom{4}{1})$ $+$ $\binom{9}{4}\binom{26}{6}(\binom{3}{1}\binom{6}{1}+\binom{5}{1})$$+$ $\binom{9}{3}\binom{26}{7}(\binom{3}{1}\binom{7}{1}+\binom{6}{1})$$+$ $\binom{9}{2}\binom{26}{8}(\binom{3}{1}\binom{8}{1}+\binom{7}{1})$$+$ $\binom{9}{1}\binom{26}{9}(\binom{3}{1}\binom{9}{1}+\binom{8}{1})$$+$ $\binom{9}{0}\binom{26}{10}(\binom{3}{1}\binom{10}{1}+\binom{9}{1})$ . Here, $\binom{9}{8}$ is the possibility that we choose 8 from 9 clubs ( we dont want to pick 10), $\binom{26}{2}$ is completing the picking of the rest 10 cards, $\binom{3}{1}\binom{3}{1}$ is picking 1 10 from 3 and picking a club from the 3 that left, $\binom{1}{1}$ is picking the ten of clubs and then picking the only left clubs. The rest of the calculations are the same. So, $P(A|B)  =  \frac {P(A\cap B)}{P(B)} = \frac {5271351254}{29485675300} $ $ \approx $ 0.178 The correct answer is $ \frac 14$ . My questions are: am I wrong, and, if so, is there a ""nicer"" way to compute $P(A\cap B)$ (or generally solving this exercise)? *Sorry if there are mistakes in grammar.",['probability']
4326244,Ideals generated by Polynomials and coprime polynomials (with more progress),"a) Let $f(X),g(X)\in K[X]$ be two polynomials and $I=\left<f,g\right>$ be an ideal in $K[X]$ generated by $f$ and $g$ . Prove that $I=(\gcd(f,g))$ , i.e., $I$ is generated by $\gcd(f,g)$ . Then conclude $I=K[X]$ if $f$ and $g$ are coprime polynomials. b) Let $p_1,p_2,....p_r\in K[X]$ be coprime polynomials. Prove there exists $h_1,....h_r\in K[X]$ such that $$\sum_{i=0}^rp_ih_i=1$$ EDIT: Something I worked out in the meantime: PS: The professor said $I=(\gcd(g,f))$ means that $I$ is generated by $\gcd(g,f)$ (here however, I wonder if the statement is correct) Let. $I=\left<f,g\right>=\{\,h\mid h= f\cdot k_1+g\cdot k_2\,\}$ where $k_1,k_2$ are in $F[X]$ and let $a=\gcd(f,g)$ . So we can say any element $s$ in $I$ is some polynomial $p\cdot a$ . Now, (here, I'm quite unsure), based on the property of an ideal, the multiplicative product as well as the addition product of the elements in $I$ are also in $I$ and $\gcd(f,g)$ can be divided by both elements. And since $k_1$ and $k_2$ are in $K$ , $\gcd(f,g)$ can be divided by some function $k_3$ in $K$ such that $h=a*k_3$ . So we conclude. $I$ is generated by $\gcd(g,f)$ and $I=K[X]$ . 2nd try: Let $I=<f,g>$ and $a=gcd(f,g)$ . Then any element $s \in I$ can be divided by a, i.e. $s_i=p_i•a$ for some $p_i \in K[x]$ . Since a is in I we can say I is generated by gcd(f,g). Now I have no idea if this is correct, even if it is, I'd be glad if someone could formalise my proof, i know what i wrote isn't formal at all! Thanks","['elementary-set-theory', 'linear-algebra', 'polynomials']"
4326264,Why does Tao define limits this way?,"When defining the limit of a function at a point, Terence Tao (Analysis I, 2016, 3e) also adds ""in $E$ "". I think with the example below, most texts would simply say that $\lim_{x\rightarrow 0}f(x)=0$ . But Tao makes a distinction between $\lim_{x\rightarrow 0;x\in \mathbb{R}- \{0\}}f(x)$ (equals $0$ ) and $\lim_{x\rightarrow 0;x\in \mathbb{R}}f(x)$ (undefined). Why does he do this? What's the point/advantage gained?",['limits']
4326308,How many employees do I need to fill 33 positions? Real world problem,"thanks for reading this thread! This is a problem which I don't know how to put in a formula properly: We're filling 33 positions with workers across a full year (12 months).
Each worker will take 1 month off during the year. We will need to fill the positions with other extra workers. Extra workers also will take 1 month off during the year. The problem is, how many extra workers do we need to fill these 33 positions (taking into account off-times for all workers)? The number may contain decimals, but will be rounded-up. [Updated:]
Notes:
(1) We have a say in the decision for timing of the vacation, so yes we can offer time-off when we decide.
(2) Also I understand that we need 3 other employees for a total of 36. I actually solved this using MS Excel Solver add-on, but I don't know how to derive/explain this mathematically. Is there any sort of formula for this?",['algebra-precalculus']
4326317,How to solve this DE for $v^3$?,"I've been looking over this for a while now and can't find where I went wrong in my attempt. A car's velocity $v$ is related to its displacement $x$ by the differential equation $$v\frac{dv}{dx}=\frac{p}{v}-kv^2$$ where $p$ and $k$ are constants. Show that $$v^3=\frac{1}{k}(p-pe^{-3x})$$ My attempt: $$\frac{dv}{dx}=\frac{p-kv^3}{v^2}$$ $$\frac{v^2}{p-kv^3}\frac{dv}{dx}=1$$ $$\int\frac{v^2}{p-kv^3}dv=x+c$$ Let $u=p-kv^3$ , $du=-3kv^2dv$ $$-\frac{1}{3k}\int\frac{1}{u}du=x+c$$ $$\ln(p-kv^3)=-3k(x+c)$$ $$v^3=\frac{1}{k}(p-e^{-3k(x+c)})$$ After substituting $v=0$ , $x=0$ I found that, $$ -3kc=\ln(p)$$ $$ v^3=\frac{1}{k}(p-e^{-3kx}\cdot e^{-3kc})$$ $$v^3=\frac{1}{k}(p-pe^{-3kx})$$ But this does not match up with the required form so where is the mistake I have made? Thanks.","['calculus', 'ordinary-differential-equations']"
4326359,Extension of a measure defined in a filtration to the whole space,"Let $(\Omega,\left(\mathcal F_t\right)_{t\geq 0},\mathcal F,\mathbb P)$ a filterated probability space fulfiling the usual measurability conditions (see  ""Diffusions, Markov processes and martingales - Roger and Williams"" Definition II.67.1). If needed it can be assumed that: $\Omega := \mathcal C_0^0(\mathbb R_+,\mathbb R)=\{\omega:\mathbb R_+\to \mathbb R;\ \omega\ \text{is continuous and }\omega(0) =0\}.$ $\mathcal F_{t} = \sigma\left(W_s; 0\leq s\leq t\right),$ where $W_s$ is the standard Brownian motion. $\mathcal F = \sigma(W_s; s\geq 0),$ $\mathbb P$ the Wiener measure. Let $\{M_s:\Omega \to \mathbb R_+\}_{s\geq 0},$ be a martingale with respect to the filtration $\{\mathcal F_s\}_{s\geq 0},$ satisfying $\mathbb E[M_s] = 1.$ Consider the measure $\mathbb Q_s$ on $\mathcal F_s$ defined as $$\mathbb Q_s[A] = \mathbb E[\mathbb 1_A M_s] \ \text{for every }s\geq 0\ \text{and }A\in\mathcal F_s.$$ My question: Is it possible to guarantee that there exists a measure $\mathbb Q_\infty$ on $(\Omega,\mathcal F),$ such that $$ \mathbb Q_\infty (A_s) = \mathbb Q_s [A_s], \ \text{for every }A_s\in\mathcal F_s?$$ I do not know if this is true. Can anyone help me?","['measure-theory', 'probability-distributions', 'brownian-motion', 'probability-theory', 'probability']"
4326402,Induction proof on a sequence,"A sequence $a_0,a_1,\dotsc$ is defined by letting $a_0=a_1=1$ and $a_k=a_{k-1}+2a_{k-2}$ for all integers $k>1$ . Prove that for all integers $k \geq 0$ , $a_k=\frac{2^{k+1}+(-1)^k}{3}$ . Base case: $S(2)$ Let $k=2$ .
Then $a_2 =\frac{2^{2+1}+(-1)^2}{3}=\frac{9}{3}=3$ $S(3)$ Let $k=3$ .
Then $a_3 =\frac{2^{3+1}+(-1)^3}{3}=\frac{15}{3}=5$ Base is true. Induction step: Assume $S(k)$ and $S(k-1)$ are true.
Then $$S(k)=a_k=\frac{2^{k+1}+(-1)^k}{3}$$ and $$S(k-1)=a_{k+1}=\frac{2^{k}+(-1)^{k-1}}{3}$$ Since $S(k)=a_k$ and $S(k-1)=a_{k-1}$ , by definition $S(k+1)=S(k)+2\cdot S(k-1)$ $$\frac{2^{k+2}+(-1)^{k+1}}{3}=\frac{2^{k+1}+(-1)^k}{3}+2\cdot \frac{2^{k}+(-1)^{k-1}}{3}$$ Solving this should lead to that $$k=-2$$ Therefore $S(k+1)$ is true. I think I've got the right idea but I'm not completely sure that I've done this correctly. Any help or suggestions are much appreciated!","['induction', 'solution-verification', 'discrete-mathematics', 'sequences-and-series']"
4326430,How to solve this integral $\int_0^{1}(2x^2+1)\sqrt{\frac{1-x^2}{1-x^8}}\ dx$,$$\int_0^{1}(2x^2+1)\sqrt{\frac{1-x^2}{1-x^8}}\ dx$$ substituting $x^4=\sin u; dx=\frac{\cos u}{4(\sin u)^{3/4}}$ $$\frac1{4}\int_0^{\pi/2}\frac{\left(2\sqrt{\sin u}+1\right){\sqrt{1-\sqrt{\sin u}}}}{(\sin u)^{3/4}}\ du$$ $$\frac 1{2}\int_0^{\pi/2}\frac{\sqrt{1-\sqrt{\sin u}}}{(\sin u)^{1/4}}du+\frac1{4}\int_0^{\pi/2}\frac{\sqrt{1-\sqrt{\sin u}}}{(\sin u)^{3/4}}\ du$$,"['integration', 'calculus', 'definite-integrals']"
4326432,Isomorphisms for Infinite Direct Products of Groups,"Here is a question from Section 2.13 of Herstein's ""Topics in Algebra"" (2nd edition): If $G_{1}$ , $G_{2}$ , $G_{3}$ are groups, prove that $(G_{1} \times G_{2}) \times G_{3}$ is isomorphic to $G_{1} \times G_{2} \times G_{3}$ . Care to generalize? The first part of the problem is simple for me; I'm interested in the generalization. For a finite direct product, we can prove that $\prod_{i = 1}^{n} G_{i}$ is isomorphic to the direct product $G_{1} \times \cdots \times  G_{n}$ (with parentheses placed arbitrarily. Suggestions are welcome for formalizing the notation) by induction. But what about the case of $\prod_{i \in I} G_{i}$ , where $I$ is an infinite index set, and $G_{i_{1}} \times \cdots$ (also indexed by $I$ , and with a (possibly) infinite number of arbitrarily placed parentheses)? Is there still an isomorphism between these two infinite direct products, and, if so, how does one show this? (If this requires some advanced mathematics, an outline to give me a general understanding of the proof will be enough) I have looked at several, possibly related, questions: Commutativity and Associativity in Infinite Direct Sums and Products of R-Modules , (Direct) Product is Associative , and Is there such a thing as direct product of an infinite number of groups? . The first two questions/answers I don't understand, as I don't know anything about modules or category theory, and I'm not sure if the final question can be applied to this problem.","['direct-product', 'group-theory', 'abstract-algebra', 'infinite-groups']"
4326553,Stability of steady states using the Jacobian (linear approximation),"I'm studying the stability of steady states by means of the eigenvalues of $J$ . So far the criteria is this: All eigenvalues $\gt 0 \implies$ unstable All eigenvalues $\lt 0 \implies$ stable . In 2D: one eigenvalue $\gt 0$ , and another $\lt 0 \implies$ saddle . All eigenvalues $\leq 0 \implies$ critical (stability cannot be analyzed using
this approximation). But what about the following cases: Saddle points in $n$ -dim (how do they look like?), can an eigenvalue be
zero? Eigenvalues $\geq 0$ , how do I classify these?","['eigenvalues-eigenvectors', 'jacobian', 'multivariable-calculus', 'nonlinear-system', 'steady-state']"
4326581,Lunch possibilities and permutations,"Problem: Owners of a restaurant advertise that they offer 1,114,095 different lunches based on the fact that they have 16 ""free fixins"" to go along with any of their 17 menu items (sandwiches, hot dogs, and salads). How did they arrive at that number? This is a precalculus textbook exercise for which the answer is provided in the back of the book.  It's in the ""Permutations"" section. My first thought was to sum up the number of possibilities of choosing no toppings, of choosing one topping, of choosing two toppings, and so on, but that is not practical. So instead I think of event one as choosing a menu item with 17 possible outcomes.  Then event two is whether I put topping #1 on so there are two outcomes.  Event three is whether I put topping #2 on so there are two outcomes.  And so on.  So there are a sequence of 17 events. By the fundamental counting principle I get that the number of lunches is $$
17 \cdot 2^{16}
$$ but the back of the book shows $$
17 \cdot (2^{16} - 1)
$$ . Is it possible that they are not considering the event of no toppings?  I mean are toppings required or something?","['permutations', 'combinatorics']"
4326584,How to show that an upper bound is $\frac{2-b}{1-b}\mathbb E [Y^{b}]$ exists for the following integral,"Consider for $b \in (0,1)$ and two continuous positive random variables $X$ and $Y$ , that we have the following inequalities already given: $$1.\; \mathbb E [X^{b}] \leq \int\limits_{0}^{\infty}b x ^{b-1}\left(\mathbb P(X\geq  x, Y\leq x)+\mathbb P(Y>x)\right)dx\\ 2. \; \mathbb P(X\geq  x, Y\leq x)\leq \frac{1}{x}\mathbb E[Y\land x]$$ I want to show that: $$ \mathbb E [X^{b}] \leq \frac{2-b}{1-b}\mathbb E [Y^{b}]$$ My ideas so far: $$ \int\limits_{0}^{\infty}b x ^{b-1}\left(\mathbb P(X\geq  x, Y\leq x)+\mathbb P(Y>x)\right)dx\leq \int\limits_{0}^{\infty}b x ^{b-1}\left(\frac{1}{x}\mathbb E[Y\land x]+\mathbb P(Y>x)\right)dx\\=\int\limits_{0}^{\infty}b x ^{b-2}\mathbb E[Y\land x]dx+\int\limits_{0}^{\infty}b x ^{b-1}\mathbb P(Y>x)dx$$ Note that the second term can be written as $\int\limits_{0}^{\infty}b x ^{b-1}\mathbb P(Y>x)dx=\mathbb E[Y^{b}]$ But I am having trouble computing the first term $\int\limits_{0}^{\infty}b x ^{b-2}\mathbb E[Y\land x]dx$ . Any ideas?","['expected-value', 'inequality', 'probability']"
4326586,Proving that $\Bbb Z$ has the same cardinality has $\Bbb N_0$,"I have seen some proofs concerning that $\Bbb Z$ is countable and in most of them, it's done by defining the following function: \begin{equation*}
f(n) = \begin{cases}
					 f(n) = \frac{n}{2} \text{ if $n$ even } \\[.15cm]
					 f(n) = -\left(\frac{n-1}{2}\right) \text{ if $n$ odd}
		\end{cases}
\end{equation*} But my only problem is the following: Consider the case where $\Bbb N_0$ is the domain of $f$ . Thus we would have $f(0) = 0/2 = 0$ and $f(1) = -0/2 = 0$ which would prove that we aren't in a bijection. This is indeed a surjective function, is it being a surjective funciton enough to prove $\Bbb Z$ has the same size has $\Bbb N_0$ ? Thanks for all the help in advance.",['elementary-set-theory']
4326626,How to determine covariance while only knowing variances,"The question defines a series of random variables U, V, X, Y, and Z. X, Y, and Z are ""uncorrelated"". U = X + Z, V = Y + Z. The variances of X, Y, and Z are known (let's say for simplicity that they are 1, 2, and 3, respectively). The question is asking for the covariance of (U,V). I'm aware that I can compute cov(U,V) as cov(X+Z,Y+Z) = cov(X,Y) + cov(X,Z) + cov(Y,Z) + cov(Z,Z) and that cov(Z,Z) = var(Z) = 3. But I'm not aware of anything else I can be doing to work out these other covariances. I understand that covariance depends on expected values, and I have no information regarding any of the data, just given the variances themselves. Where do I go from here?","['covariance', 'statistics', 'variance']"
4326653,Identifying cells in cell complexes,"Let $G=\mathbb{Z}\oplus \mathbb{Z}$ with generators $a,b \in G$ . We can define cell complexes $[a,b]$ and $[a|b]$ as I did in the images below. For example, $[a,b]$ is simply the 2-simplex with edges labelled by $[a]$ , $[b]$ and $[ab]=[ba]$ and vertices labelled by $[\;]$ . However, in the end I would like to identify cells with the same label/color (respecting the orientation!). A reversed orientation is indicated by a minus sign. For $[a,b]$ , $[a|b]$ and $[a,a]$ you can see the result on the right hand side. Note that $[a|b]$ is the solid torus, for simplicity I drew just its boundary. Now, I'm not very good in topology, so I have two questions: What kind of 3-fold do we get after identifying cells of the cell complex $[a|a]$ (see picture below)? What kind of 3-fold do we get after identifying cells of the cell complex $[a|b]+[b|a]$ (see picture below)? EDIT: I believe that $[a|a]$ and $[a|b]+[b|a]$ are lens spaces since we glue solid tori together, but I'm not sure which ones. Please feel free to ask, if some part of my question is too vague. I hope that the pictures are self-explanatory, but maybe I'm wrong. EDIT: I added a picture of $[a,a]$ above. Here's another one: It's quite easy to see how to turn it over, but then I don't understand how to glue both copies together.","['manifolds', 'general-topology', 'algebraic-topology']"
4326800,Great difficulty in finding the residues of $\frac{\mathrm{Log}\Gamma\left(\frac{z+ai}{2\pi i}\right)}{\cosh(z)+1}$,"$\newcommand{\log}{\operatorname{Log}}\newcommand{\res}{\operatorname{Res}}\newcommand{\d}{\mathrm{d}}$ Let $\Lambda(z)=\log\Gamma(z)$ , $a\gt0$ , let $\psi$ denote digamma. It is written here , Page 49, equation 47: $$\pi\cdot\Im\left[\res_{z=\pi i}\frac{\Lambda\left(\frac{z+ai}{2\pi i}\right)}{\cosh(z)+1}\right]=\psi\left(\frac{1}{2}+\frac{a}{2\pi}\right)$$ To motivate the calculation of this hard residue, it is shown in that paper that knowledge of this residue indirectly solves the integral: $$\int_1^\infty\frac{\ln\ln x}{(x+1)^2}\,\d x=\frac{1}{2}\int_0^\infty\frac{\ln x}{\cosh x +1}\,\d x$$ And finds it to be equal to: $$\frac{1}{2}\left\{\ln2\pi+\lim_{a\to0+}\pi\cdot\Im\left[\res_{z=\pi i}\frac{\Lambda\left(\frac{z+ai}{2\pi i}\right)}{\cosh(z)+1}\right]\right\}=\frac{1}{2}\left(-\gamma+\ln\frac{\pi}{2}\right)$$ Which is equivalently the limit of Malmsten’s integral as $\varphi\to0$ . Formally, I can attempt to find, as the singularity is an order two pole (the reader should note that the following working is mistaken): $$\begin{align}\lim_{z\to\pi i}\frac{\d}{\d z}(z-\pi i)^2\frac{\Lambda\left(\frac{z+ai}{2\pi i}\right)}{\cosh(z)+1}&=\lim_{z\to\pi i}\frac{2(z-\pi i)\Lambda\left(\frac{z+ai}{2\pi i}\right)+\frac{1}{2\pi i}(z-\pi i)^2\psi\left(\frac{z+ai}{2\pi i}\right)}{\cosh(z)+1}-\frac{\sinh(z)(z-\pi i)^2\Lambda\left(\frac{z+ai}{2\pi i}\right)}{(\cosh(z)+1)^2}\\&=\lim_{z\to\pi i}\frac{2\Lambda\left(\frac{z+ai}{2\pi i}\right)+\frac{2}{\pi i}(z-\pi i)\psi\left(\frac{z+ai}{2\pi i}\right)-\frac{1}{4\pi^2}(z-\pi i)^2\psi'\left(\frac{z+ai}{2\pi i}\right)}{\sinh(z)}\\&-\frac{\cosh(z)(z-\pi i)^2\Lambda\left(\frac{z+ai}{2\pi i}\right)+2\sinh(z)(z-\pi i)\Lambda\left(\frac{z+ai}{2\pi i}\right)+\frac{1}{2\pi i}\sinh(z)(z-\pi i)^2\psi\left(\frac{z+ai}{2\pi i}\right)}{2\sinh(z)(\cosh(z)+1)}\\&=\lim_{z\to\pi i}\frac{\frac{2}{\pi i}\psi\left(\frac{z+ai}{2\pi i}\right)-\frac{1}{\pi^2}(z-\pi i)\psi'\left(\frac{z+ai}{2\pi i}\right)-\frac{1}{4\pi^2}(z-\pi i)^2\psi''\left(\frac{z+ai}{2\pi i}\right)}{\cosh(z)}\\&-\frac{\sinh(z)(z-\pi i)^2\Lambda\left(\frac{z+ai}{2\pi i}\right)+2\cosh(z)(z-\pi i)\Lambda\left(\frac{z+ai}{2\pi i}\right)+\frac{1}{2\pi i}\cosh(z)(z-\pi i)^2\psi\left(\frac{z+ai}{2\pi i}\right)}{2\cosh(z)(\cosh(z)+1)+2\sinh^2(z)}\\&-\frac{\Lambda\left(\frac{z+ai}{2\pi i}\right)+\frac{1}{\pi i}(z-\pi i)\psi\left(\frac{z+ai}{2\pi i}\right)-\frac{1}{8\pi^2}(z-\pi i)^2\psi'\left(\frac{z+ai}{2\pi i}\right)}{\sinh(z)}\\&=-\frac{2}{\pi i}\psi\left(\frac{1}{2}+\frac{a}{2\pi}\right)-\lim_{z\to\pi i}[\cdots]\end{align}$$ But now there is a problem. The latter "" $\cdots$ "" contains a ratio which cannot be resolved by L'Hopital (dominator $\sinh$ , numerator non-zero). This is also a horribly complicated expression, and I am assuming that the author would have mentioned it if it is was this difficult. There must surely be some more straightforward way... Any advice? I've never done residue calculations this difficult before. I have found other residues from integral earlier in the paper by using small epsilon and Taylor series arguments, but they avail me nought here. Edit: I see I have misused L'Hopital again, on the third line! The numerator goes not to zero.","['polygamma', 'complex-analysis', 'gamma-function', 'residue-calculus', 'limits']"
4326833,"How many solutions does $f'(x)=1/2$ have, given that $|f(x)-f(y)|\ge|x-y|$?","Let $F: \mathbb R \to \mathbb R$ be a continuously differentiable function such that $$|f(x)-f(y)| \ge |x-y|,\ \  \forall x,y \in \mathbb R$$ Then $ f '(x)=1/2$ has how many solutions? The way I approached this problem was like this, $$\frac{|f(x) - f(y)|}{|x-y|} \ge 1.$$ So this implies that the slope of this function is either $1$ or more than $1$ , over $\mathbb R$ . And the differential equation given has the solution $$y = x/2 + c$$ implying that it has a slope of $1/2$ . Is it wrong that I am taking the given function and the solution of the differential equation as two distinct graphs and then looking at their solution? Edit: As it turns out, I am indeed wrong. This was a much simpler problem and I complicated it. I cannot take 2 different functions because they are talking about the same function. Thanks for the comments and replies.","['derivatives', 'real-analysis']"
4326859,Car Auction with Uniformly distributed Value,"Suppose the true value $V$ of a car is uniformly distributed, between $0$ and $1000$ . You can bid any amount
for the car, and if you bid the true value or more, then you pay your bid and get the car.
You know a very good salesman, and are confident in your ability to sell the car for $50\%$ more than its true value.
What should you bid to maximize your expected profit? My reasoning is this: The expected value of the car is $500$ based on the distribution we are given.
Therefore, on average I will be selling it for $1.5*500 =750$ . This means that if I bid $501$ , then I should win more often than I lose. When I lose my payoff is $0$ and for the wins, my payoff should average out to be $750-501=249$ . I was however presented with the following solution, which contradicts mine. let our bid $=X.$ True value of car $=V.$ Expected value $= P(\text{win})*(\text{Payoff from winning}).$ $P(\text{win})=X/1000$ , since we win when our bid is above or at true value, and there are $x$ values less than or equal to our bid out of $1000.$ Payoff from winning $= 3/4(X)- X.$ $3/4\,x$ is the value I sell it for since if I win, that means that $X>V$ so $$V \sim \text{Unif}[0,X],$$ and therefore expected value of $V = 0+X/2=X/2$ . Selling price (as mentioned in question) $= 3/2*V=3/2\,(x/2)=3/4\,X.$ This suggests that the selling price will always be less than what we bid, so our expectation is negative. Both of the above solutions make sense to me, so I'm having a hard time understanding why one is wrong; appreciate any insight.","['expected-value', 'statistics', 'probability']"
4326862,Sum of the components of a vector [duplicate],"This question already has answers here : Every combination of $v = (1, -2, 1)$ and $w = (0, 1, -1)$ has components that add to ________. (2 answers) Closed 2 years ago . Introduction to Linear Algebra (Strang 5th ed) problem 6 page 8. What does it mean when components of a vector add to zero? The solution from the instructor's manual doesn't give additional clues, and maybe it doesn't mean more than that?
Does someone have an insight on the sum of components of vectors? The problem set: The instructor's manual answer:","['linear-algebra', 'vectors']"
4326864,How far is this view correct on strengthening and weakening of the topologies of the domain and the codomain?,"Let $f:X \to Y$ be a map between infinite topological spaces. I want to know how correct is this view on how refining (strengthening) and coarsening (weakening) the topologies on both the domain and the codomain affects a mapping being continuous and open mapping. Question: Is there in general such diagram, (or a similar one,) that holds true whenever one starts from the trivial topologies and refines them (taking anyway) towards the discrete topologies ? Starting from the trivial (chaotic) topologies, there are too many paths toward the discrete topologies, not only one, but is there a diagram of that sort above that holds no matter what, i.e. will change ""homeomorphically"" under changing the path of refinment. Feel free to suggest corrections and diagrams.","['continuity', 'general-topology', 'open-map']"
4326908,Inequality from Chapter 5 of the book *How to Think Like a Mathematician*,"This is from the book How to think like a Mathematician, How can I prove the inequality
$$\sqrt[\large 7]{7!} < \sqrt[\large 8]{8!}$$ without complicated calculus? I tried and finally obtained just $$\frac 17 \cdot \ln(7!) < \frac 18 \cdot \ln(8!)$$","['exponentiation', 'inequality', 'factorial', 'radicals', 'algebra-precalculus']"
4326960,Number of line of curvature meets at one point,"I am now consider the surface given by $$f=(x,y,x^3-3xy^2)$$ And I have been asked to prove that there are three line of curvature meet at origan point. I can compute that \begin{align*}
		f_x&=(1,0,3x^2-3y^2)\\
		f_y&=(0,1,-6xy)\\
		f_x\times f_y&=(-3(x^2-y^2),6xy,1)\\
		|f_x\times f_y|&=\sqrt{9(x^2-y^2)^2+36x^2y^2+1}=\sqrt{9(x^2+y^2)^2+1}\\
		n&=\frac{f_x\times f_y}{|f_x\times f_y|}=\frac{(-3(x^2-y^2),6xy,1)}{\sqrt{9(x^2+y^2)^2+1}}\\
		f_{xx}&=(0,0,6x)\\
		f_{xy}&=(0,0,-6y)\\
		f_{yy}&=(0,0,-6x)\\
		E&=<f_x,f_x>=1+9(x^2-y^2)^2\\
		F&=<f_x,f_y>=-18(x^2-y^2)xy\\
		G&=<f_y,f_y>=1+36x^2y^2\\
		L&=<n,f_{xx}>=\frac{6x}{\sqrt{9(x^2+y^2)^2+1}}\\
		M&=<n,f_{xy}>=\frac{-6y}{\sqrt{9(x^2+y^2)^2+1}}\\
		N&=<n,f_{yy}>=\frac{-6x}{\sqrt{9(x^2+y^2)^2+1}}.\\
	\end{align*} When $x=y=0$ , we have that $$(g)=\begin{pmatrix}
		E & F\\
		F & G
	\end{pmatrix}=\begin{pmatrix}
		1 & 0\\
		0 & 1
	\end{pmatrix}\mbox{ and }(b)=\begin{pmatrix}
		L & M\\
		M & N
	\end{pmatrix}=\begin{pmatrix}
		0 & 0\\
		0 & 0
	\end{pmatrix}.$$ Hence the shape operator $$(s)=(g^{-1})(b)=\begin{pmatrix}
		0 & 0\\
		0 & 0
	\end{pmatrix}.$$ So the principal curvatures at point $x=y=0$ is that $k_1=k_2=0$ . I do not know how to continuous and I think I can not even calculate the principal direction at the origan and how can I know how the number of lines of curvature.
But I have observed that this surface is mirror symmetrical to $(x,z)-$ plane. Does it have any relations? I have also calculated that a line of curvature $f\circ \gamma$ with $\gamma=\begin{pmatrix}
\gamma_1\\
\gamma_2
\end{pmatrix}$ should fullfill the following equation $$\begin{vmatrix}
\gamma_2^{\prime 2} & -\gamma_1^\prime \gamma_2^\prime & \gamma_2^{\prime 2}\\
E & F & G\\
L & M & N \end{vmatrix}=0$$ but how can I know a line of curvature cross the origan point?",['differential-geometry']
4326973,Changing the bounds in an improper double integral,"Suppose we have a double integral of the form $$ \int_0^{\infty} \int_0^{\infty} f(xy) g(x,y) \, dx \, dy $$ Let's suppose the functions behave well so that the integrals converge, etc. I'm trying to perform a substitution to change this to an iterated integral. If we set $y=\frac{t}{x}$ so that $dy=\frac{dt}{x}$ , would the bounds on the new variable $t$ also be zero and infinity, so we could write $$ \int_0^{\infty} \int_0^{\infty} f(xy) g(x,y) \, dx \, dy = \int_0^{\infty} \int_0^{\infty} f(t) g\left(x,\frac{t}{x}\right) \, dx \, \frac{dt}{x} = \int_0^{\infty} f(t) \int_0^{\infty} \frac{g \left(x,\frac{t}{x} \right)}{x} \, dx \, dt $$ or would the bounds on $t$ be something different? How should I think about this to determine what the correct bounds should be?","['multivariable-calculus', 'improper-integrals']"
4327053,Proving existence and uniqueness of a function $f':\mathbb{N}\rightarrow A$,"Suppose $A$ is a group with operation $\times$ . Let $f:\mathbb{N}\rightarrow A$ be a function where, for any natural number $m,n$ $f(m+n)=f(m)\times f(n)$ We want to show that this can be extended to a function $f':\mathbb{Z}\rightarrow A$ where, for any integer $m,n$ $f'(m+n)=f'(m)\times f'(n)$ Show that such a $f'$ exists and that it is unique. So far, I have $f'(n)=f(n)$ when $n>0$ and $f'(n)=1$ (identity) when $n=0$ and $f'(n)=f(-n)^{-1}$ for $n<0$ Does this function work, if not, whats missing?
How do I demonstrate that the function i defined works and that its unique?","['functions', 'group-theory', 'abstract-algebra', 'relations']"
4327079,Provably Unwinnable Spider Games,"I would like to know what fraction of Spider Solitaire games (played with 104 cards and four suits) are provably unwinnable.  It seems various people have various ideas about what constitutes an unwinnable game, and so I will restrict my question to ""provably unwinnable"" games.  By ""provably unwinnable"" I mean that it can be demonstrated from logical argument that a game, or a class of games, is fundamentally unwinnable. I am (so far) aware of two classes of what I currently believe to be provably unwinnable games.  The wording of this last sentence was carefully chosen to convey the error bars that I assign to my present understanding. The ""No Possible Move"" Class: This is a class of games in which there are no moves at all, regardless of the skill of the player.  Two criteria must be satisfied for a game to fall into this class: 1) in each row of face-up dealt cards there must be no moves between the face-up dealt cards, and 2) there must be no moves that are enabled by picking up multiple cards.  An example will help to make this clear.  Consider the following game in which the cards denoted by ""X"" are dealt face-down and rows 7..11 are in the stock: Row  1 >>   X           X           X           X
         Row  2 >>   X   X   X   X   X   X   X   X   X   X
         Row  3 >>   X   X   X   X   X   X   X   X   X   X
         Row  4 >>   X   X   X   X   X   X   X   X   X   X
         Row  5 >>   X   X   X   X   X   X   X   X   X   X
         Row  6 >>   AS  AS  3S  3S  5S  5S  7S  7S  9S  9S


         Row  7 >>   AH  AH  3H  3H  5H  5H  7H  7H  9H  9H
         Row  8 >>   AC  AC  3C  3C  5C  5C  7C  7C  9C  9C
         Row  9 >>   AD  AD  3D  3D  5D  5D  7D  7D  9D  9D
         Row 10 >>   2S  2S  4S  4S  6S  6S  8S  8S  10S 10S
         Row 11 >>   2H  2H  4H  4H  6H  6H  8H  8H  10H 10H Such a game has no possible moves and is therefore provably unwinnable. The ""Wall of Death"" Class: This is a class of games in which all of the cards that would enable a lifting of a card are dealt face down and ""walled off"" by other cards that also cannot be lifted for the same reason.  I call this wall the Wall of Death . An example will be of benefit. Consider the following game in which rows 1..5 are dealt face down, ""Y"" denotes a card of arbitrary value and suit, and rows 7..11 are in the stock: Row  1 >>   Y           QC          Y           Y
         Row  2 >>   Y   QS  Y   Y   Y   Y   4C  Y   4D  Y
         Row  3 >>   4C  Y   Y   Y   QD  Y   Y   QS  QH  Y
         Row  4 >>   Y   4D  Y   Y   Y   Y   4S  Y   Y   QD
         Row  5 >>   QC  QH  4S  Y   Y   Y   4H  Y   4H  Y
         Row  6 >>   JS  JS  3S  JD  3C  3H  JC  3D  3S  JH


         Row  7 >>   Y   Y   Y   Y   Y   Y   Y   Y   Y   Y 
         Row  8 >>   Y   Y   Y   Y   Y   Y   Y   Y   Y   Y
         Row  9 >>   Y   Y   Y   Y   Y   Y   Y   Y   Y   Y
         Row 10 >>   Y   Y   Y   Y   Y   Y   Y   Y   Y   Y
         Row 11 >>   Y   Y   Y   Y   Y   Y   Y   Y   Y   Y Row 6, composed solely of Jacks and 3's, is the Wall of Death .  None of the Jacks can ever be moved because all eight Queens are buried face down in rows 1..5 behind the Wall of Death . Similarly, none of the 3's can ever be moved because all eight 4's are buried behind the Wall of Death . In such a game all of the cards that comprise the Wall of Death can never be moved. Such a game is provably unwinnable.  Note that each column must contain a member of the Wall of Death but that the member of the Wall of Death need not be in row 6.  For instance, the Jack in column 4 of the above game could be relocated from row 6 to row 2: Row  1 >>   Y           QC          Y           Y
         Row  2 >>   Y   QS  Y   JD   Y   Y   4C  Y   4D  Y
         Row  3 >>   4C  Y   Y   Y   QD  Y   Y   QS  QH  Y
         Row  4 >>   Y   4D  Y   Y   Y   Y   4S  Y   Y   QD
         Row  5 >>   QC  QH  4S  Y   Y   Y   4H  Y   4H  Y
         Row  6 >>   JS  JS  3S  Y   3C  3H  JC  3D  3S  JH


         Row  7 >>   Y   Y   Y   Y   Y   Y   Y   Y   Y   Y 
         Row  8 >>   Y   Y   Y   Y   Y   Y   Y   Y   Y   Y
         Row  9 >>   Y   Y   Y   Y   Y   Y   Y   Y   Y   Y
         Row 10 >>   Y   Y   Y   Y   Y   Y   Y   Y   Y   Y
         Row 11 >>   Y   Y   Y   Y   Y   Y   Y   Y   Y   Y and the game remains unwinnable. It is also possible for a King to be part of a Wall of Death , but the nature of what cards must be buried behind the wall is somewhat different.  An example will help to make this clear. Row  1 >>   Y           QC          3S          Y
         Row  2 >>   Y   QS  Y   Y   Y   Y   4C  Y   4D  Y
         Row  3 >>   4C  Y   Y   Y   QD  Y   Y   QS  QH  Y
         Row  4 >>   Y   4D  Y   Y   Y   3S  4S  Y   Y   QD
         Row  5 >>   QC  QH  4S  Y   Y   Y   4H  Y   4H  Y
         Row  6 >>   JS  KS  3S  JD  3C  3H  JC  3D  3S  JH


         Row  7 >>   Y   Y   Y   Y   Y   Y   Y   Y   Y   Y 
         Row  8 >>   Y   Y   Y   Y   Y   Y   Y   Y   Y   Y
         Row  9 >>   Y   Y   Y   Y   Y   Y   Y   Y   Y   Y
         Row 10 >>   Y   Y   Y   Y   Y   Y   Y   Y   Y   Y
         Row 11 >>   Y   Y   Y   Y   Y   Y   Y   Y   Y   Y In this example I have inserted the King-of-Spades into row 6, column 2.  But note that I have also buried both of the Three-of-Spades behind the wall.  In doing so, I have created a situation in which the King-of-Spades can never be lifted because to do so would require completing a continuous series of Spades from the King-of-Spades down to the Ace-of-Spades, and such a continuous series cannot be constructed without a Three-of-Spades.  So to utilize a King as part of a Wall of Death one must bury at least one pair of value-equivalent cards of the same suit as the King behind the Wall of Death . Note also that a game may belong to both the No Possible Move class and the Wall of Death class. At this juncture I think it worthwhile to ask questions that are somewhat more refined than my original question. Q1: What fraction of games are in the No Possible Move class of games? Q2: What fraction of games are in the Wall of Death class of games? Q3: What fraction of games are in both the No Possible Move class and the Wall of Death class of games? Q4: Are there other classes of provably unwinnable games, and if so, what fraction of games are in those classes? Q5: Are there games that are unwinnable but not provably unwinnable? This needs some elaboration. Recall that my definition of provably unwinnable was that a game can be demonstrated unwinnable by means of logical argument. This question concerns whether there exist truly unwinnable games that cannot be proven to be unwinnable by means of logic.  Such games could in principle be proven unwinnable by a brute-force search of the entire tree of possible moves. I am interested in both analytic and numerical-simulation answers to these questions.  I have made a very crude estimate of the answer to Q1.  I estimate that roughly $1$ in $10^{13}$ games (give or take an order of magnitude) are in the No Possible Move class. I suspect (but do not know) that one could play 10 games of Spider per day for their entire life and never come upon a provably unwinnable game. The implication (of course) is that when you loose a game it is almost certainly because you gave up, not because the game was unwinnable. Note Added: A Spider-playing computer program ( plspider ) is reported to have a win rate of 99.994%, loosing just 2 out of 32,000 unique games that were tested.  This lends credence to the notion that provably unwinnable games are extremely rare. Of some interest is game #14934 - a game that the website author(s) says ""appears to be unwinnable."" Upon examination it can be readily discerned that game #14934 is neither a No Possible Move game or a Wall of Death game.  If it is truly unwinnable then either: 1) it is a member of a new class of provably unwinnable games, or 2) it proves the existence of unwinnable games that are not provably unwinnable games (except by brute-force searching of the entire tree of moves). Kudos to the author(s) of the website for their care in stating that the game ""appears"" to be unwinnable.","['statistics', 'combinatorics', 'card-games', 'probability']"
4327101,Isometry group(s) of flat surfaces,"(1) The flat plane always has isometry group $ E_2 = O_2 \ltimes \mathbb{R}^2 $ . (2) A full classification of the four possible isometry groups of a flat torus are given in Isometry of Torus The isometry group of a product of (irreducible, non isometric) Riemannian manifolds is the product of the isometry groups. This point is basically addressed in How does one prove that $\text{Isom}(\mathbb{S}^2 \times \mathbb{R}) = \text{Isom}(\mathbb{S}^2) \times \text{Isom}(\mathbb{R})$? and https://mathoverflow.net/questions/351646/the-isometry-group-of-a-product-of-two-riemannian-manifolds so (3) So the isometry group of a flat cylinder $ \mathbb{R} \times S^1 $ is always $ Iso(\mathbb{R}) \times Iso(S^1)= E_1 \times O_2=(O_1 \ltimes \mathbb{R} ) \times O_2 $ Now my question: is a full classification of isometry groups, as done for the torus, possible for the other flat surfaces: (4) The flat Moebius band (I think the isometry group is always $ Iso(\mathbb{R})=E_1=O_1 \ltimes \mathbb{R} $ , just isometries of the fiber) (5) The flat Klein bottle (I think the isometry group is always $ Iso(S^1)=O_2 $ , just isometries of the fiber) EDIT: Using the hint from Kajelad and applying it to coverings of both of these manifolds by the flat cylinder I compute the following. The Moebius strip is the quotient of the cylinder by an isometry acting on points in the cylinder $ (t,z) \in \mathbb{R}\times U_1 $ by $$
(t,z) \to (t,\overline{z})
$$ this generates a two element cyclic group $ \Gamma $ of isometries which intersects two of the connected components of the isometry group $ (O_1 \ltimes \mathbb{R} ) \times O_2 $ of the flat cylinder. Since $ \Gamma $ has only two elements the normalizer is the same as the centralizer. And the centralizer is the full isometry group of the line together with the isometries of the circle $ z \to -z $ , $ z \to \overline{z} $ and product. So modding out $ N(\Gamma)/\Gamma $ is the isometries of the fiber (line) direct product a cyclic two group acting on the base (circle) so the full isometry group of the flat Moebius strip is $ O_1 \ltimes \mathbb{R} ) \times O_1 $ . The Klein bottle is the quotient of the cylinder by an isometry acting on points in the cylinder $ (t,z) \in \mathbb{R}\times U_1 $ by $$
(t,z) \to (t+1,\overline{z})
$$ this generates an infinite cyclic group $ \Gamma $ of isometries which intersects two of the connected components of the isometry group $ (O_1 \ltimes \mathbb{R} ) \times O_2 $ of the flat cylinder. The normalizer of $ \Gamma $ is a group of isometries consisting of all the translations in the $ \mathbb{R} $ factor together with the isometries $ z \mapsto \overline{z} $ and $ z \mapsto -z $ in the circle factor. The quotient $ N(\Gamma)/ \Gamma $ is isomorphic to $ O_2 $ (since $ \Gamma $ intersects two different connected components it cuts down the four connected components of the normalizer to just two in the quotient and since $ \Gamma $ is cyclic it cuts down the $ \mathbb{R} $ translation factor in the normalizer to just a compact piece $ \mathbb{R}/\mathbb{Z} $ in the quotient. So in conclusion the isometry group of the Klein bottle is just $ O_2 $ the isometries of the circle fiber.","['euclidean-geometry', 'riemannian-geometry', 'geometric-topology', 'lie-groups', 'differential-geometry']"
4327106,Is the set of all points where any two spheres of different size (Such as the Earth and Moon) subtend the same angle a sphere?,"A recent question on the Astronomy Stack Exchange asked "" Where in space would the Earth and Moon appear to be the same size? "". Though the question asked specifically about points on the line drawn between the Earth and the Moon, idle curiosity and the comment chain on one of the answers got me to kick open Geogebra to try to see what the set of all points where the two objects appeared to be the same size was. Calculating the subtended angle of a sphere of radius $r$ whose center is distance $d$ from point $P$ as: $$\theta=2 \arcsin\left(\frac{r}{d}\right)$$ GeoGebra Graph of points where Earth and Moon have the Same Angular Size parameter value Radius of Earth $6\,371\,\mathrm{km}$ Radius of the Moon $1\,737\,\mathrm{km}$ Semi-major axis of the Moon's orbit $384\,000\,\mathrm{km}$ In GeoGebra, I graphed the set of points where the angular size of Earth and the Moon were equal. Based on the GeoGebra graph , it looks very much like the Equal Angular size region is a sphere of radius $113\,000\, \mathrm{km}$ centered on a point about $415\,000\, \mathrm{km}$ from Earth. My question is: Is the set of all points where two spheres of different sizes subtend the same angle actually a sphere, or just an ovoid that is too close to a sphere for me to tell in GeoGebra?","['spheres', 'geometry']"
4327118,Showing there is a node in the graph with only one edge,"I saw this question recently, Showing there is a node in the graph with one and only one edge and I am just wondering how the approach would be different if we added the following restraint:
We have an undirected simple graph with n vertices where for every pair of vertices $v_1,v_2$ , if $d(v_1)=d(v_2)$ then the set of neighbours of $v_1$ is disjoint from the set of neighbours of $v_2$ where there may be an edge between $v_1$ and $v_2$ . Assuming the graph contains at least one edge, prove that there is a vertex of degree exactly 1 in the graph.","['graph-theory', 'pigeonhole-principle', 'combinatorics']"
4327130,Determining $A$ such that $\lim \limits_{x \to\infty }(\sqrt{Ax^2+2x}−5x)$ exists and is finite,"Determine the value of the real number $A$ so that a finite limit exists, and then compute the limit: $$\lim \limits_{x \to \infty}(\sqrt{Ax^2+2x}−5x)$$ Where would I start with this? I thought I could factor the highest degree variable from each term but that lets any real A work with it.","['limits', 'calculus']"
4327143,Proof of discrete Hodge decomposition,"In this survey by Lubotzky , he has the following: Proposition 2.1 (Hodge decomposition): The following are true: $C^i=B^i\oplus\mathcal{H}^i\oplus\mathcal{B}_i$ , $\mathcal{H}^i\cong H^i(X;\mathbb{R})$ , and $\Delta_i^\wedge(B^i\oplus\mathcal{H}^i)=0$ . I'll make all the definitions clear momentarily, but to avoid getting bogged down, first I'll ask my questions: Why is this true? In particular I've thought a bit about the third one and don't see how it could possibly be true on $\mathcal{H}^i$ (will elaborate below). I don't really see how any part is as elementary as is claimed in the survey though. He traces the result back to this paper of Eckmann , which I can't seem to find in English. Does there exist a translation or a later document recapping its proofs? More generally, the survey doesn't give many proofs (which makes sense, as it's a survey). Is there a treatment of the topic which does provide more proofs? Thanks in advance for any thoughts. Now, here are all the definitions. $X$ is a finite $d$ -dimensional simplicial complex. We study real (co)homology, so the $i$ th cochain group $C^i$ is the $\mathbb{R}$ -space of $\mathbb{R}$ -valued functions on $X^i$ (the set of pure $i$ -faces). The coboundary map is defined on pure faces $F\in X^{i+1}$ and extends linearly: $$(\delta_if)(F):=\sum\limits_{G\in X^i}[F:G]f(G)$$ where $[F:G]$ is $-1$ if $G$ comes from $F$ by removing an odd-indexed entry, $1$ if it arises by removing an even-indexed entry, and $0$ otherwise. Put $B^i:=\mathrm{im\,}\delta_{i-1}$ . Cohomology is defined in the standard way here (kernel mod image). We define $\deg F:=\#\{G\in X^d:F\subseteq G\}$ , so that we have the adjoint map generated by $$(\delta_i^*f)(G)=\frac{1}{\deg G}\sum\limits_{F\in X^{i+1}}[F:G]\deg F \cdot f(F)$$ and the inner product $$(f,g):=\sum\limits_{F\in X^i}\deg F\cdot f(F)g(F).$$ Put \begin{align*}
\Delta_i^\wedge&:=\delta_i^*\circ\delta_i, \\
\Delta_i^\vee&:=\delta_{i-1}\circ\delta_{i-1}^*, \\
\Delta_i&:=\Delta_i^\wedge+\Delta_i^\vee. 
\end{align*} We put $\mathcal{H}^i:=\ker\Delta_i$ and $\mathcal{B}_i:=\mathrm{im\,}\delta_i^*$ . Thinking just about the third claim in the Proposition, it's clear to me from the definitions why $\Delta_i^\wedge(B^i)=0$ . However, $\Delta_i^\wedge(\mathcal{H}^i)=0$ doesn't make much sense: $\mathcal{H}^i$ is precisely those $f$ for which $\Delta_i^\vee f=-\Delta_i^\wedge f$ . But if the RHS here is $0$ (as the Proposition seems to claim) then this would mean that individually, $\Delta_i^\vee$ and $\Delta_i^\wedge$ vanish on their sum's kernel. This seems like way too strong a statement. EDIT: This paper by Parzanchevski and Rosenthal also mentions the decomposition (page 10) and chalks it up to just linear algebra (and also clarifies that $\mathcal{H}^i$ is precisely the intersection of the two kernels). I guess what I'm looking for then is how to get started on thinking about the linear algebraic computations needed to show the results. EDIT: This paper by Parzanchevski, Rosenthal, and Tessler seems to have most of the details so I think my questions should be mainly resolved.","['hodge-theory', 'homology-cohomology', 'discrete-mathematics', 'simplicial-complex']"
4327153,Converting odds to probabilities,"Gambling odds on sports betting are designated with a number in the set $(-\infty , -100) \cup [+100 , +\infty)$ . If one places a wager of $w$ dollars at $p \in (-\infty , -100) \cup [+100 , +\infty)$ odds and your bet is successful, then your winnings $E(w , p)$ are calculated as follows: $$
E(w , p) : = \begin{cases} 
\left( 1 + \dfrac{100}{p} \right)w & p < -100\\
\left(1 + \dfrac{p}{100} \right)w & p \geq +100.
\end{cases} 
$$ So large positive odds correspond to events that are less likely to happen and thus have greater payouts while large negative odds correspond to events that are more likely to happen and thus have smaller payouts. +100 odds on an event correspond to a 50% chance of that event occurring, but how does one convert odds to the probability that event will occur in general ? Basically, is there a function $L : (-\infty , -100) \cup [+100 , +\infty) \rightarrow [0,1]$ that takes the odds $p$ of an event and maps it to the probability it will occur? I am particularly interested in applying this to betting on who will score the first basket of a basketball game. Betting on a single player scoring the first basket has relatively high (positive) odds and therefore a good payout should the bet be successful. While betting on a single player is risky and not the most likely to happen, betting on multiple players increases the likelihood of winning while good individual odds can offset the losses on the other bets. Take for instance the following odds (which are real, but writing out the names will take too long) on who will score the first basket in tomorrow's Bulls v. Cavaliers game: Player 1: +490 Player 2: +500 Player 3: +550 Player 4: +600 Player 5: +850 Player 6: +850 Player 7: +1000 Player 8: +1100 Player 9: +1300 Player 10: +1500 Is there a function $L$ with the properties I have described above and satisfies $$
1 = L(490) + L(500) + L(550) + L(600) + L(850) + L(850) + L(1000) + L(1100) + L(1300) + L(1500)?
$$ Will I need to adapt the function $L$ if I was to look at a different game with different odds?","['gambling', 'recreational-mathematics', 'probability']"
4327160,An Easy Looking Positive Semidefinite Matrices Implication,"I'm reading the article ""Controlling the false discovery rate via knockoffs"" by Candes and Barber ( https://arxiv.org/abs/1404.5609 ) and faced the following problem that I couldn't handle. We have $X\in\mathbb{R}^{n\times p}$ with rank $p$ . We define $\Sigma:=X^TX$ We have a vector $s\in \mathbb{R}^p$ with non-negative entries and $diag(s)$ is defined as the $p\times p$ matrix with diagonal entries $s$ , zero otherwise. As I've understood from the beginning of the page 9 in the article, the authors claim that: \begin{equation}
2\Sigma \succeq \text{diag}(s) \iff 2\text{diag}(s)-\text{diag}(s)\Sigma^{-1}\text{diag}(s)\succeq 0
\end{equation} (From $X \succeq Y$ , I understand that $X-Y$ is positive semidefinite) I have shown the $2\text{diag}(s)-\text{diag}(s)\Sigma^{-1}\text{diag}(s)\succeq 0 \implies 2\Sigma \succeq \text{diag}(s)$ as follows: $\Sigma$ is symmetric positive definite. So, we can say, for any coordinate $i\in\{1,2,\dots,p\}$ , $\Sigma_{ii}(\Sigma^{-1})_{ii}\geq 1$ . What we have is equivalent to having $e_i^T(2\text{diag}(s)-\text{diag}(s)\Sigma^{-1}\text{diag}(s))e_i\geq0$ for every $i$ . So, we have $2s_i-s_i^2(\Sigma^{-1})_{ii}\geq 0$ . Using $\Sigma_{ii}(\Sigma^{-1})_{ii}\geq 1$ , we obtain $2\Sigma_{ii}-s_i\geq 0$ for all $i$ . So we can conclude $2\Sigma \succeq \text{diag}(s)$ . But I cannot do the same for the converse, and couldn't figure it out. I would be glad for any idea. It looks like there could be a counterexample. I feel like I may misunderstood the article.","['statistics', 'matrices', 'linear-algebra', 'positive-definite', 'matrix-decomposition']"
4327184,"How many different number are there in sequence $⌊n/1\rfloor$, $⌊n/2⌋, ⌊n/3⌋, ..., ⌊n/n]$?","$1 \leq n \leq 10^{12}$ and n is an integer. Through some documents on the internet, I vaguely know that the answer is $\sqrt{n}$ or maybe $2\sqrt{(n)}$ . Can anyone help me to prove this? My English is not very good, sorry if it makes you confused.",['algebra-precalculus']
4327209,Question on evaluation of a limit of a sequence.,Find the following limit $:$ $$\lim_{N \to \infty} \frac {1} {\sqrt {N}} \sum\limits_{n=1}^{N} \frac {1} {\sqrt {n}}.$$ It is quite clear that $\sum\limits_{n=1}^{N} \frac {1} {\sqrt {n}} \gt  \sqrt {N}$ for $N \gt 1.$ So the limit (if it exists finitely) has to be $\geq 1.$ But I believe that the limit is infinty. For that I need the sum to be greater than some scalar multiple of $N^s$ for sufficiently large $N$ where we require $s \gt \frac {1} {2}.$ Is it possible to attain this lower bound eventually? Any help in this regard would be greatly appreciated. Thanks a lot.,"['limits', 'convergence-divergence', 'sequences-and-series', 'real-analysis']"
4327282,How can I prove that this limit is equal to the Natural Logarithm?,"Earlier, I was messing around a little bit with the reverse power rule/power rule for integration which is typically written as follows. Let $f\colon\mathbb{R} \to \mathbb{R}$ be a function satisfying $f(x) = x^r$ for all $x$ , with $r \in \mathbb{R}$ . Then, $$ \int f(x) \,\mathrm{d}x = \int x^r \,\mathrm{d}x = \frac{x^{r+1}}{r+1} + C $$ for any real number $r\neq -1$ and some arbitrary constant $C$ . I was trying to figure out what happens as $r\to -1$ and how in the limit it might approach the natural logarithm of $x$ because of $$ \int x^{-1} \,\mathrm{d}x = \int \frac{1}{x} \,\mathrm{d}x = \begin{cases} \ln(x) + C &\text{if $x>0$} \\ \ln(-x) + C&\text{if $x<0$} \end{cases} = \ln|x| + C.$$ I am aware that $$ \int_{1}^{x} t^{-1} \,\mathrm{d}t = \int_{1}^{x} \frac{1}{t} \,\mathrm{d}t = \ln(x) - \ln(1) = \ln(x) $$ and that $$ \begin{aligned}
\lim_{r\to -1}\,\int_{1}^{x} t^{r} \,\mathrm{d}t &= \lim_{r\to -1} \left(\frac{x^{r+1}}{r+1} - \frac{1}{r+1} \right) \\[10pt]
&= \lim_{r\to -1} \frac{x^{r+1}-1}{r+1} \\[10pt]
&\overset{\scriptscriptstyle\text{L'H}}{=} \lim_{r\to -1} \frac{\frac{\mathrm{d}}{\mathrm{d}r} (x^{r+1}-1)}{\frac{\mathrm{d}}{\mathrm{d}r} (r+1)} \\[10pt]
&= \lim_{r\to -1} \frac{x^{r+1}\ln(x) \cdot 1}{1} \\[10pt]
&= \ln(x).
\end{aligned} $$ I tried graphing both $\dfrac{x^{r+1}}{r+1}$ and $\ln(x)$ on Desmos and after manually tinkering with the values of $r$ close to $-1$ (e.g., $-0.9,-0.99,-0.999$ , etc.) for a little bit, I came up with this function $g\colon\mathbb{R} \to \mathbb{R}$ with $r \in \mathbb{R}$ and $n \in \mathbb{Z}^+$ where $$ \begin{align}
r &= -\sum_{k=0}^{n-1}\, 9 \cdot 10^{k-n}, \tag{$\lim_{n\to\infty} r = -1$} \\[10pt]
g(x) &= \frac{x^{r+1}}{r+1} - 10^n.
\end{align} $$ From what I gathered, it seems that $g$ approaches $\ln(x)$ as $n$ increases. Finally, my question is (if my assumption is true) how can I prove that the following limit is equal to the natural logarithm? $$ \begin{align}
\lim_{n\to\infty} g(x) &= \lim_{n\to\infty} \left(\frac{x^{r+1}}{r+1} - 10^n\right) \\[10pt]
&= \lim_{n\to\infty} \left(\frac{x^{\displaystyle\left(-\sum_{k=0}^{n-1}\, 9 \cdot 10^{k-n}\right)+1}}{\displaystyle\left(-\sum_{k=0}^{n-1}\, 9 \cdot 10^{k-n}\right)+1} - 10^n \right) \\[10pt]
&\overset{\text{?}}{=} \ln(x). 
\end{align}$$ Thank you.","['integration', 'limits', 'graphing-functions', 'logarithms']"
4327310,Does $\sum_{n=2}^\infty \frac{(-1)^n}{n+(-1)^n}$ converges?,"I try to study the convergence or divergence of the series $\displaystyle\sum_{n=2}^\infty \frac{(-1)^n}{n+(-1)^n}$ . We can see that this series diverges absolutely, because $\displaystyle\sum_{n=2}^\infty\frac{1}{|n+(-1)^n|}$ is a rearrangement of $\displaystyle\sum_{n=2}\frac{1}{n}$ which equals to $+\infty$ . Consequently, we can't say for sure that the rearrangement $\displaystyle\sum_{n=2}^\infty \frac{(-1)^n}{n+(-1)^n}$ of $\displaystyle\sum_{n=2}^\infty \frac{(-1)^n}{n}$ converges. That's the part where I am stuck. How exactly can I study the convergence (or not) of the first series? Should I apply Cauchy's criterion and prove that $s_n=\displaystyle\sum_{k=2}^n \frac{(-1)^k}{k+(-1)^k},n\in\mathbb{N} $ is a basic sequence? Thanks.","['calculus', 'convergence-divergence', 'analysis', 'sequences-and-series']"
4327381,Confusion regarding geodesics and (linear) transformations,"I'm having an embarrassingly hard time reconciling some basic calculations that I think are correct (but given my confusion, I won't make a warranty) and the discrepancy in pheneomenology of the geodesics on an ellipsoid (which are rather complex) versus on a sphere (which are just arcs of great circles). Here's the basic calculations first: Let $g$ be (the matrix of) a Riemannian metric on a sub(psuedo)manifold of $\mathbb{R}^n$ relative to coordinates $x$ . Let $R \in GL_n$ and $x \mapsto Rx =: x'$ be the corresponding linear transformation. The matrix of the metric relative to the coordinates $x'$ is then $g' = R^{-T}gR^{-1}$ , and (eschewing the Einstein convention in order to write matrices conventionally) the Christoffel symbols transform as $$(\Gamma')_{ij}^k = \sum_{\ell m p} \frac{\partial x_\ell}{\partial x'_i} \frac{\partial x_p}{\partial x'_j} \frac{\partial x'_k}{\partial x_m} \Gamma_{\ell p}^m.$$ Now $\frac{\partial x'_i}{\partial x_j} = R_{ij}$ and similarly $\frac{\partial x_i}{\partial x'_j} = (R^{-1})_{ij}$ , so the preceding becomes $$(\Gamma')_{ij}^k = \sum_{\ell m p} (R^{-1})_{\ell i} (R^{-1})_{pj} R_{km} \Gamma_{\ell p}^m.$$ Now the geodesic equation is $$\ddot x'_k = -\sum_{ij} (\Gamma')_{ij}^k \dot x'_i \dot x'_j$$ which after unpacking the transformations becomes $$ \begin{align} \sum_q R_{kq} \ddot x_q & = -\sum_{ij} (R^{-1})_{\ell i} (R^{-1})_{pj} R_{km} \Gamma_{\ell p}^m \cdot \sum_r R_{ir} \dot x_r \cdot \sum_s R_{js} \dot x_s \\ 
& = -\sum_{\ell m p} R_{km} \Gamma_{\ell p}^m \cdot \sum_{ir} (R^{-1})_{\ell i} R_{ir} \dot x_r \cdot \sum_{js} (R^{-1})_{pj} R_{js} \dot x_s \\ 
& = -\sum_{\ell m p} R_{km} \Gamma_{\ell p}^m \cdot \sum_r \delta_{\ell r} \dot x_r \cdot \sum_s \delta_{ps} \dot x_s \\ 
& = -\sum_{\ell m p} R_{km} \Gamma_{\ell p}^m \cdot \dot x_\ell \cdot \dot x_p. \end{align} $$ Multiplying by $R^{-1}$ on the left, we get $$ \begin{align} \ddot x_a & = -\sum_k (R^{-1})_{ak} \sum_{\ell m p} R_{km} \Gamma_{\ell p}^m \dot x_\ell \dot x_p \\ 
& = -\sum_{\ell m p} \sum_k (R^{-1})_{ak} R_{km} \Gamma_{\ell p}^m \dot x_\ell \dot x_p \\ 
& = -\sum_{\ell m p} \delta_{am} \Gamma_{\ell p}^m \dot x_\ell \dot x_p \\ 
& = -\sum_{\ell p} \Gamma_{\ell p}^a \dot x_\ell \dot x_p. \end{align} $$ This is just the geodesic equation in the original coordinates. Now of course the whole point of differential geometry is to study quantities that are ultimately independent of any choice of coordinates, so this doesn't seem weird. BUT: an ellipsoid is just a linear (or affine, if one prefers) transformation of a sphere. And the geodesics on these two surfaces behave very differently.","['ellipsoids', 'geodesic', 'riemannian-geometry', 'differential-geometry']"
4327384,Finding the limit of a complex function,"Calculate the limit, if possible $\lim_{z \to -3i} \frac{z^3-27i}{z+3i}$ My approach was initially to calculate the conjugate and simplify, but after $$ \frac{(z-3i)^2(z^2+3iz+9i^2)}{z^2+9}$$ it can't be simplified further. If I continue going on, I get $$ \frac{0}{0}$$ which seems to be wrong. I'm suspecting this limit don't exist, any tips?","['limits', 'complex-numbers']"
4327396,Bounds on Cut Sizes on an Erdos-Renyi Graph,"Let us have an Erdos-Renyi graph $\mathcal{G} = \mathcal{G}(n,p)$ . For a subset of vertices $S$ we define the cut-size $c(S)$ as the number of edges $(u,v)$ such that $u \notin S$ and $v \in S$ . Let us assume that $p$ is a constant probability and finite. For the maximum cut it is known that, with high-probability, ${\rm max} \  c(S) \leq \frac{1}{4}n^{2}p + \mathcal{O}(n^{3/2})$ - a bound which becomes sufficiently tight as $n \rightarrow \infty$ . I am wondering if I can make a more broad statement about the cut sizes. Specifically let |S| equal the number of vertices in the cut, and assume that |S| is proprtional to $n$ , i.e. $|S| = \lambda n$ . \begin{equation}
\lim_{n \rightarrow \infty} \frac{c(S)}{n^{2}} = \lim_{n \rightarrow \infty}\frac{E[c(S)]}{n^{2}} = \lambda (1-\lambda)p,
\end{equation} where $E[c(S)] = |S|(n-|S|)p$ is the expected value of the cut. More specifically I am asking if the cut cannot differ in any significant (quadratic) way from it's expected value, so you have some inequality like \begin{equation}
 |c(S) - E[c(S)]|\leq \mathcal{O}(n^{\alpha}) \ \qquad \alpha < 2.
\end{equation} Can this be proven? Or does anyone know of any references which show something like this?","['graph-theory', 'combinatorics', 'cut-space', 'network']"
4327408,Proving coefficients of a polynomial form a subvariety,"I encountered this problem while studying algebraic geometry: Let $f \in \mathbb{C}[x, y, z]$ be a homogeneous polynomial of degree 3. The
coefficients of $f$ represent a point $P_f$ in $\Bbb{P}^9$ . Show that $$\Bbb{P}^9\setminus\{P_f \,|\, \text{the variety defined by}\, f\, \text{is smooth, irreducible of degree}\, 3\}$$ is a closed subvariety of $\Bbb{P}^9$ I tried to do some computing with the Jacobian matrix but I think it is the wrong path. Someone can explain how to proceed?",['algebraic-geometry']
4327537,Regarding probability and the birthday paradox,"I have a question which states that
""In a group of 23 people what is the probability that there are two people with the same birthday? Assume there are 365 days in a year. Ignore leap years and such complications. Assume there is an equal probability of a person being born on each day of the year."". I solved it using the complement. I first computed the number of ways in which we can assign the birthdays to 23 people out of 365 days (without replacement). That gave 365 * (365-1).. (365-k+1). Then I divided this by 365^k. Then I subtracted the result from 1. But, the probability which I have now got may also contain 3 people having the same birthday or 4 people having the same birthday, etc. I want to know the probability of exactly two people having the same birthday. In short, what I have computer is, "" what's the probability that AT LEAST TWO PEOPLE HAVE SAME BIRTHDAY"" and what I'm looking for is ""WHAT IS THE PROBABILITY OF EXACTLY TWO PEOPLE HAVING THE SAME BIRTHDAY"".How do I compute that probability?","['permutations', 'combinatorics', 'probability-theory', 'probability']"
4327549,If $(a_{2n} -a_n)$ is convergent to zero then $(a_n)$ is convergent . if $(a_n)$ is convergent then $(a_{2n}-a_n)$ is convergent to zero,"1.If $(a_{2n} -a_n)$ is convergent to zero then $(a_n)$ is convergent. True/False? 2.If $(a_n)$ is convergent then $(a_{2n}-a_n)$ is convergent. True/False? I solved $2$ this way - let $\lim\limits_{n\to\infty }a_n  =L$ and $(a_{2n})$ is a subsequence of $(a_n)$ then they must converge to the same limit , therefore $\lim\limits_{n\to\infty }a_{2n}  =L$ so we get that $\lim\limits_{n\to\infty }a_{2n}- a_n  =L-L=0$ so the statement is True for the first part I know that the statement is not true but I cannot find a counterexample or a general way to show it , the practice book used this counterexample and explanation: let $a_n=\begin{cases}
  1 &\text{if $n=2^k$ $k \in \Bbb N$}\\ 
 0&\text{otherwise}\\ 
\end{cases}$ if $n=2^k$ then for $k \in \Bbb N$ we get $2n=2^{k+1}$ therefore $a_{2n}=a_n=1$ and $a_{2n}-a_n=0$ if $n\not=2^k$ then $a_{2n}=a_n=0$ and $a_{2n}-a_n=0$ meaning for all $n$ we get $a_{2n}-a_n=0$ so $\lim\limits_{n\to\infty }a_{2n}- a_n  =0$ but $(a_n)$ is not convergent $(a_{2n-1})$ is a subsequence of $(a_n)$ in the odd indexes so $(a_{2n-1})=0$ and $\lim\limits_{n\to\infty }a_{2n-1}=0$ . the sequence $n_k =2^k$ is a sequence that is stricly increasing of natural indexes because for all $k$ we get $2^k$ is also a natural number and $n_{k+1}=2^{k+1}=2\cdot2^k <2^k =n_k$ , we get that $(a_{n_k})=(a_{2^k})$ is a subsequence of $(a_n)$ but all of its elements are the same $\lim\limits_{n\to\infty }a_{2^k}=\lim\limits_{n\to\infty }1=1$ but this subsequence has a different limit which means that $(a_n)$ is not convergent and the statement is false my question is if there is another way to show this? this counterexample seems too complicated , I understand the explanations but I would not have figured to use $a_n=\begin{cases}
  1 &\text{if $n=2^k$ $k \in \Bbb N$}\\ 
 0&\text{otherwise}\\ 
\end{cases}$ .. is there another counterexample? or a general way to show this other than the way they did in the book? thank you","['limits', 'convergence-divergence', 'sequences-and-series', 'real-analysis']"
4327555,Formally smooth morphism of formal schemes $\mathfrak X \to\mathfrak Y$ induces formally smooth morphisms of schemes $\mathfrak X_n \to\mathfrak Y_n$?,"Let $f:\mathfrak Y \to \mathfrak X$ be a morphism of formal schemes. We say that $f$ is formally smooth if it satisfies the infinitesimal lifting property, that is if for every affine $\mathfrak X$ -scheme $Z$ and for every closed subscheme $T\hookrightarrow Z$ defined by a square zero ideal, the natural map $$\mathrm{Hom}_{\mathfrak X}(Z,\mathfrak Y) \to \mathrm{Hom}_{\mathfrak X}(T,\mathfrak Y)$$ is surjective. The same definition can be stated for morphisms of schemes. Let $J$ be an ideal of definition of $\mathfrak X$ , and let $I$ be an ideal of definition of $\mathfrak Y$ containing $f^{*}(J)\,\mathcal O_{\mathfrak Y}$ . This is possible due to the continuity of $f$ . For $n\geq 0$ , we consider the schemes $\mathfrak X_n := (|\mathfrak X|,\mathcal O_{\mathfrak X}/J^{n+1})$ . Then $\mathfrak X$ can be identified with the inductive system $(\mathfrak X_n)$ . We can do similarly for $\mathfrak Y$ . Then, the morphism $f$ induces a family of compatible morphisms $f_n:\mathfrak X_n \to \mathfrak Y_n$ of schemes. If $f$ is formally smooth, is it true that all the $f_n$ 's are also formally smooth ? I have done the following. I fix $n$ and I consider $\mathfrak X_n$ -schemes $Z$ and $T$ as above. I also consider a morphism $T\to \mathfrak Y_m$ over $\mathfrak X_m$ . Using compositions with $\mathfrak Y_m \hookrightarrow \mathfrak Y$ and $\mathfrak X_m \hookrightarrow \mathfrak X$ , I can look at $Z$ and $T$ over $\mathfrak X$ and I get a $T$ -point of $\mathfrak Y$ over $\mathfrak X$ . By hypothesis, it lifts to a $Z$ -point of $\mathfrak Y$ over $\mathfrak X$ . To conclude, I would like to justify that it factors through $\mathfrak Y_m$ . That is, if I write $g:Z \to \mathfrak Y$ , I would have to check whether the map $g^{*}I^m \to \mathcal O_{Z}$ is zero. Because the structure map $f\circ g: Z \to \mathfrak X$ factors through $\mathfrak X_m$ , I know that $g^{*}f^{*}J^m\to \mathcal O_Z$ is zero. Thus, it would be over with the additional assumption that $f$ is adic . But what if $f$ is not adic ? Currently I do not see how I could progress further. However, I have also been unable to come up with a counter example...","['ringed-spaces', 'algebraic-geometry', 'schemes']"
4327580,"Find a point $P_2$ on an ellipse, whose chord with $P_1$ is a max distance $d$ from its nearest side","I'm not sure if this solution is available in closed form, but after drawing it out I do think there will be two unique solutions always. I unfortunately have no clue where to start. Given: An ellipse with x radius $a$ , y radius $b$ and centre point $(0, 0)$ A point on that ellipse $P_1$ A distance $d$ Find all points $P_{2j}$ (i.e. find their coordinates) on the ellipse which satisfy the following condition (where $C$ is the chord joining $P_1$ and $P_{2j}$ ): The distance between $C$ and a line which is tangent to the ellipse and parallel to $C$ (on the side of the smaller segment formed by $C$ i.e. the ""nearer"" side) is equal to $d$ In other words, the maximum distance between $C$ , and the nearer side of the ellipse from $C$ , must equal $d$ . The diagram I've drawn below is an example of this. $P_{21}$ and $P_{22}$ are the desired points, whereas $P_{23}$ is an example of an invalid point. (The distances are not fully accurate) The reason I believe the solution is closed, is because as you sweep the point $P_2j$ around the ellipse, the distance (required to be $d$ ) increases, reaches a turning point, and then decreases.","['analytic-geometry', 'trigonometry', 'conic-sections', 'geometry']"
4327611,Lemma by Riemann-Lebesgue,"The purpose of this problem is that I want to prove that for any $\lambda$ integrable function $f$ on a bounded closed interval $[a, b]$ holds $$
\lim _{n \rightarrow \infty} \int_{[a, b]} f(x) \sin (n x) d \lambda=0.
$$ I have submitted a proof below.","['measure-theory', 'lebesgue-measure']"
4327663,Show that $\mu^*(M\cup N)=\mu^*(M)+\mu^*(N)$ for outer measure $\mu^*$,"Let $\mu:\mathscr{H}\to\mathbb{R}$ be a content where $\mathscr{H}\subset X$ a semiring and $\mu^*$ the outer measure defined by $\mu$ . I have already proven that that for any $A,B\subset X:\quad \mu^*(A\cup B)+\mu^*(A\cap B)\leq \mu^*(A)+\mu^*(B)$ and equality holds if $A\in\mathscr{A_{\mu^*}}$ or $B\in\mathscr{A_{\mu^*}}$ where $\mathscr{A_{\mu^*}}=\{A\subset X \;|\;\mu^*(A\cap B)+\mu^*(A^C\cap B)=\mu^*(B) \;\;\forall B\subset X\}$ . $M, N\subset X$ are given and $A,B\in\mathscr{A_{\mu^*}}$ such that $M\subset A,\; N\subset B$ and $\mu^*(A\cap B)=0$ . I now want to prove $$\mu^*(M\cup N)=\mu^*(M)+\mu^*(N)$$ My attempt is to deduce that $\mu^*(M)=\mu^*(M\setminus N)$ from $\mu^*(B\cap M)+\mu^*(B^C \cap M)=\mu^*(M)$ and use this to show that $\mu^*(M)=\mu^*(M\setminus N)$ . Then, because $M$ and $M\setminus N$ are disjoint $\Rightarrow \mu^*(M\cup N)=\mu^*((M\setminus N)\cup N)=\mu^*(M\setminus N)+\mu^*(N)=\mu^*(M)+\mu^*(N)$ , but I know that the measure of a union of disjoint sets is not necessarily the sum of the measure of those sets, so my proof is probably wrong.","['measure-theory', 'outer-measure']"
4327729,Why does integral equation for arcsec have absolute value in its argument rather than the denominator of the integrand?,"In this question, I would like to investigate the location of the absolute value in the arcsecant integral. Following this answer and this answer , we know the following is true: $$
\frac{d}{dx}\sec^{-1}(x)=\frac{1}{|x|\sqrt{x^2-1}}.
$$ Taking indefinite integrals of both sides, we get $$
\sec^{-1}(x)+C=\int \frac{1}{|x|\sqrt{x^2-1}} dx
$$ How can one use this result to deduce that $$
\sec^{-1}(|x|)+C=\int \frac{1}{x\sqrt{x^2-1}} dx?
$$ (Notice that the absolute values used to be in the denominator of the right hand side, and now they are in the argument of the $\sec^{-1}$ function.) What logical rule of deduction allows us to interchange the location of the absolute values on opposite sides of this equation?","['integration', 'proof-writing', 'calculus', 'inverse', 'trigonometry']"
4327752,"A recurrence for the number of non-crossing partitions without singletons, using Dyck paths","Let $f(n+1)$ be the number of non-crossing partitions without singletons of $\{1,2,\dots,n+1\}$ . There is a well known bijection between the non crossing-partitions (counting also those with singletons) and the Dyck paths by putting $UD^{\lambda_1}UD^{\lambda_2}\dots UD^{\lambda_{n+1}}$ , where $$\lambda_i =
\begin{cases}
\text{the cardinality of the block},  & \text{if $i$ is maximal in its block} \\
0, & \text{otherwise}
\end{cases}$$ The number of Dyck paths of half-length $n+1$ is given by the famous Catalan numbers recurrence: $$C_{n+1}=\sum_{k=0}^nC_kC_{n-k}$$ Now, I want to find a recurrence for $f(n+1)$ which will be not that far from the Catalan decomposition, since we can see the non-crossing partitions of $\{1,2,\dots,n+1\}$ as a subset of the Dyck paths: to do this, starting from $(0,0)$ , I can set the very first step that is required to be a $U$ , similarly I can fix the first step, to the right of $(0,0)$ , at which the path hits the $x$ -axis. Between those two steps I can settle a Dyck path of half-length $k$ , with $k = 0, 1, \dots , n$ (and I have $f(k)$ ways to do that with paths which are images of non-crossing partitions without singletons), the remaining path of half-length $n-k$ will be covered in $f(n-k)$ ways.
By adding for all of the possible values of $k$ I have found again the Catalan decomposition: $$\sum_{k=0}^nf(k)f(n-k)$$ To have $f(n+1)$ I must subtract $1$ from that sum when $n$ is even and I must add $1$ whenever $n$ is odd, resulting in: $$f(n+1)=- (-1)^n+\sum_{k=0}^nf(k)f(n-k)$$ however, I can't see how to justify that in a combinatorial way, any idea?","['catalan-numbers', 'set-partition', 'combinatorial-proofs', 'combinatorics', 'discrete-mathematics']"
4327787,Combinatorial identity from squaring the binomial expansion,"It is trivial to obtain, from squaring the binomial expansion of $(1+t)^{1/2}$ and comparing coefficients, that $$\tag1
\sum_{k=0}^m\binom{1/2}k\binom{1/2}{m-k}=0,\qquad m\geq2.
$$ This can be rewritten as $$\tag2
\sum_{k=1}^{m-1}\frac{(2k-2)!(2(m-k)-2)!}{k!(k-1)!(m-k)!(m-k-1)!}=\frac{(2m-2)!}{m!(m-1)!},\qquad m\geq2.
$$ I wonder if there is a direct proof of these equalities, in either version.","['binomial-coefficients', 'combinatorics', 'combinatorial-proofs', 'binomial-theorem']"
4327856,How many ways are there to prove Cayley-Hamilton Theorem?,"I see many proofs for the Cayley-Hamilton Theorem in textbooks and net, so I want to know how many proofs are there for this important and applicable theorem?","['cayley-hamilton', 'big-list', 'reference-request', 'abstract-algebra', 'linear-algebra']"
4327888,Find the sum $\sum\limits_{k=0}^\infty (-2)^k\frac{k+2}{k+1}x^k$,"$$\displaystyle\sum_{k=0}^\infty (-2)^k\dfrac{k+2}{k+1}x^k.$$ I showed that this series converges when $|x|<\dfrac{1}{2}$ because $$\displaystyle\lim_{k\to\infty}\left|\dfrac{a_{k+1}}{a_k}\right|=2|x|.$$ Now I have to find the sum result. I tried so far trying to combine $$\ln(x+1)=\displaystyle\sum_{k=0}^\infty \dfrac{(-1)^kx^{k+1}}{k+1}$$ and $$\arctan(x)=\displaystyle\sum_{k=0}^\infty \dfrac{(-1)^kx^{2k+1}}{2k+1},$$ and trying many substitutions with multiples of $x$ , $x^2$ and got close to the result, but $\dfrac{k+2}{k+1}$ confuses me. Any help would be appreciated.","['power-series', 'calculus', 'sequences-and-series', 'real-analysis']"
4327910,Derivative of floor function using epsilon/delta,"For every $x\in\mathbb{R}$ , let $[x]$ denote the floor of $x$ . I read that the derivative of $[x]$ over non-integers is zero, and now I want to show it using epsilon/delta, i.e. show $$0=\lim_{x\rightarrow a}\frac{[x]-[a]}{x-a}$$ for all nonintegers $a$ . Let $\epsilon>0$ . There exists $\delta>0$ such that $0<|x-a|<\delta$ implies $|[x]-[a]|<\epsilon$ . If I can get $|x-a|>1$ , then we are done. But I am not able to make $|x-a|>1$ . So maybe going through this way is not right. In fact, if I can show that $$\frac{|[x]-[a]|}{|x-a|}\leq C$$ for some constant $C$ , I'll be done. What should be my delta?","['ceiling-and-floor-functions', 'epsilon-delta', 'real-analysis', 'continuity', 'derivatives']"
4327950,Random variable defined on the IID,"I understand that the mean and variance of the Cauchy distribution is undefined. I also understand that if we try to take independent and identically distributed random variable from the Cauchy distribution and attempt to use the Central Limit Theorem, it doesn't work. But what about the following case: Let $X_1,X_2,X_3$ be independent and identical random variables from the Cauchy distribution ( $x_0,\gamma$ ). Define $$Y=median(X_1,X_2,X_3)$$ Is it possible to find the expectation and variance of $Y$ ? If so, how? If not, is it because of the fundamental fact that the central limit theorem cannot be applied to the Cauchy distribution? Thanks for reading.","['statistics', 'variance', 'expected-value', 'sampling', 'probability']"
4327983,"If on a piece of paper you can draw a collection of points that satisfies the vertical line test, then there is a function that contains these points","I recently posed the following question in a comment section: If I draw a curve on a piece of paper that passes the vertical line test and exhibits some particular property (e.g. 'Always greater than $0$ '), can I assume that there is some corresponding/matching function whose assignment rules can be explicitly written out? A reputable user responded with: The problem is with ""assignment rules can be explicitly written out"" -- what are allowable explicit operations/formulas? (polynomials, step functions, trig, exponential, inverses, expressible as a convergent infinite series of such functions, elliptic functions, hypergeometric functions, etc.) But if we're not asking whether the function can be expressed in a certain way, but simply whether it exists, then any set of points (regardless of how weird) in the plane satisfying the vertical line test is the graph of a function $(*)$ Assuming that I interpreted $(*)$ correctly (and accepting that, perhaps, the response was meant to be informal), it seems to me that something like the following claim is being made: If on a piece of paper you can draw a collection of points that satisfies the vertical line test, then there is a function that contains these points $(\dagger)$ From searching around the forum, I see that there are a lot of technical nuances regarding ""definability"" (e.g. https://mathoverflow.net/questions/44102/is-the-analysis-as-taught-in-universities-in-fact-the-analysis-of-definable-numb/44129#44129/ ) that go way beyond my skillset. Further, the act of ""drawing on a piece of paper"" is clearly some meta-concept. Nonetheless, I would like to see if someone could help me understand why $(\dagger)$ is a reasonable claim. Although the idea of ""drawing"" doesn't seem to be directly reconcilable within the language of set theory, I know that the notion of a graph is ( Difference between a function and a graph of a function? ). Is it safe to say that, by convention: drawing on a piece of paper a collection of points that satisfies the vertical line test $\iff$ graph $G_f$ of some function $f$ ? In which case, $(\dagger)$ is more precisely stated as: If $G_g$ is the graph of $g$ , then there exists a function $f$ such that $f$ 's graph $G_f$ is equal to $G_g$ ? The proof of which is trivial. I pose this question because, if my goal is to prove that there exists a function satisfying some property...and this property happens to be something that can be visually identified through  drawing (e.g. "" $f$ is always greater than $0$ ), then is a drawn collection of points (satisfying the desired properties) a satisfactory means of proof?","['soft-question', 'functions', 'logic']"
4327991,"If $f: X\to Y$ and $g: Y\to X$ are continuous bijections, not necessarily inverses, is $X\cong Y$","I was thinking about to show two sets are bijective, it suffices to construct an injective function both ways. That is, if $f: X\to Y$ and $g: X\to Y$ are both injective (or both surjective), then there exists a bijection between $X,Y$ . But, what if $X, Y$ were topological spaces, and $f, g$ were continuous? Clearly, surjective isn't enough, because I can construct surjective mappings both ways between the interval and the circle, but the two spaces aren't homeomorphic. But, I first notice that this fails because the canonical quotient map from the interval to the circle isn't injective. What if we strengthen this condition to ensure that $f, g$ are continuous AND bijective? Intuitively, this makes sense: I can continuously map from one space to the other, and my intuition of a homeomorphism is I can bendy-wendy squiggly-wiggly one topological space into the other without rips or gluing together. But, something tells me this isn't sufficient, because I don't see why $f: X\to Y$ and $g: Y\to X$ continuous bijections guarantee there exists some $h: X\to Y$ such that $h, h^{-1}$ are continuous bijections.",['general-topology']
4328014,"Infinite cartesian product equals Finite Cartesian product when the index is $\{1, 2, ... , n\}$ Confusion","First we denote the set of all functions from $A$ to $B$ by $B^A$ Definition of Finite Cartesian product: $\prod_{1 \leq i \leq n} X_{i} := \{(x_i)_{1 \leq i \leq n} : x_i \in X_i \space   \forall \space   1 \leq i \leq n\} $ Definition of Infinite Cartesian product: $\prod_{\alpha \in I} X_{i} = \{(x_\alpha)_{\alpha \in I} \in (\bigcup_{\beta \in I}X_\beta)^I : x_\alpha \in X_\alpha \space \forall \space \alpha \in I\}$ Now im stuck on: If $I$ is a set of the form $ I := \{i \in \mathbb{N} : 1 \leq i \leq n\}$ then $\prod_{\alpha \in I} X_{i} $ is the same set as $\prod_{1 \leq i \leq n} X_{i}$ Heres what im stuck on,
take $z \in \prod_{1 \leq i \leq n} X_{i}$ so $z = (x_i)_{1 \leq i \leq n}$ where $x_i \in X_i \space \forall \space  i \in I$ so we have $x_i \in X_i \space \forall \space  i \in I$ now we just need to show $z \in (\bigcup_{\beta \in I}X_\beta)^I$ since then we can conclude $z \in \prod_{\alpha \in I} X_{i}$ but how do we show this? Does this just simply come from the fact $n-tuples$ are functions from $I$ to an arbitrary set $X$ (in our case $X$ is just $ (\bigcup_{\beta \in I}X_\beta)^I$ )",['elementary-set-theory']
4328085,Random walk with stopping time,"$T_n=\Sigma_{j=1}^n Y_j$ is a symmetric simple random walk with $P(Y_j=1)=\frac{1}{2}, P(Y_j=-1)=\frac{1}{2}$ and $T_0=0$ . Define $M=\mbox{min}\{i:|T_i|=n\}$ . We are require to find $\mathbb{E}(M)$ and $\mathbb{E}(M^2)$ . My thought: We can have $\mathbb{E}(Y_j)=0$ and $\operatorname{Var}(Y_j) = 1$ . Also $\mathbb{E}(T_n)=0$ and $\operatorname{Var}(T_n)=n$ . But how to calculate $\mathbb{E}(M)$ and $\mathbb{E}(M^2)$ . Please help.","['statistics', 'probability-limit-theorems', 'random-walk', 'probability-distributions', 'markov-chains']"
4328215,Prove $\mu^*(A)=\nu(A)$ if there exists a cover $A\subset \cup_{n\geq1} B_n$ and $\mu^*(B_n)<\infty \;\forall n\geq 1$,"Given $\mu : \mathscr{H} \to \mathbb{R}$ a pre-measure on $\mathscr{H}\subset X$ a semiring and $\mu^*$ the outer measure defined by $\mu$ , $\mathscr{A}=\sigma(\mathscr{H})$ and $\nu$ a measure such that $\nu|_\mathscr{H}=\mu$ , I want to show that If for $A\in\mathscr{A}$ there exist $B_n\subset X$ such that $A\subset\cup_{n\geq 1}B_n$ and $\mu^*(B_n)<\infty$ for all $n\geq 1$ , then $\mu^*(A)=\nu(A)$ . My attempt: I have already proven that $\mu^*(A)=\nu(A)$ for all $A\in\mathscr{A}$ that satisfy $\mu^*(A)<\infty$ but I am struggling to prove that this holds. (Original title edited)","['measure-theory', 'outer-measure']"
4328279,Deriving boundary conditions for Faraday's law and Ampére's law by letting the width $\delta$ approach zero,"I am currently studying the textbook Physics of Photonic Devices , second edition, by Shun Lien Chuang. Section 2.1.1 Maxwell's Equations in MKS Units says the following: The well-known Maxwell's equations in MKS (meter, kilogram, and second) units are written as $$\nabla \times \mathbf{E} = - \dfrac{\partial}{\partial{t}}\mathbf{B} \ \ \ \ \text{Faraday's law} \tag{2.1.1}$$ $$\nabla \times \mathbf{H} = \mathbf{J} + \dfrac{\partial{\mathbf{D}}}{\partial{t}} \ \ \ \ \text{Ampére's law} \tag{2.1.2}$$ $$\nabla \cdot \mathbf{D} = \rho \ \ \ \ \text{Gauss's law} \tag{2.1.3}$$ $$\nabla \cdot \mathbf{B} = 0 \ \ \ \ \text{Gauss's law} \tag{2.1.4}$$ where $\mathbf{E}$ is the electric field (V/m), $\mathbf{H}$ is the magnetic field (A/m), $\mathbf{D}$ is the electric displacement flux density (C/m $^2$ ), and $\mathbf{B}$ is the magnetic flux density (Vs/m $^2$ or Webers/m $^2$ ). The two source terms, the charge density $\rho$ (C/m $^3$ ) and the current density $\mathbf{J}$ (A/m $^2$ ), are related by the continuity equation $$\nabla \cdot \mathbf{J} + \dfrac{\partial}{\partial{t}} \rho = 0 \tag{2.1.5}$$ Section 2.1.2 Boundary Conditions then says the following: By applying the first two Maxwell's equations over a small rectangular surface with a width $\delta$ (dashed line in Fig. 2.1a) across the interface of a boundary and using Stokes' theorem between a line integral over a contour $C$ and the surface $S$ enclosed by the contour $$\oint_C \mathbf{E} \cdot d \mathscr{l} = \int_S \nabla \times \mathbf{E} \cdot \mathbf{\hat{n}} \ dS = - \dfrac{d}{dt} \int_S \mathbf{B} \cdot \mathbf{\hat{n}} \ dS \tag{2.1.9a}$$ $$\oint_C \mathbf{H} \cdot d \mathscr{l} = \int_S \nabla \times \mathbf{H} \cdot \mathbf{\hat{n}} \ dS = \int_S \mathbf{J} \cdot \mathbf{\hat{n}} \ dS + \dfrac{d}{dt} \int_S \mathbf{D} \cdot \mathbf{\hat{n}} \ dS, \tag{2.1.9b}$$ the following boundary conditions can be derived by letting the width $\delta$ approach zero: $$\mathbf{\hat{n}} \times (\mathbf{E}_1 - \mathbf{E}_2) = 0 \tag{2.1.10}$$ $$\mathbf{\hat{n}} \times (\mathbf{H}_1 - \mathbf{H}_2) = \mathbf{J}_s, \tag{2.1.11}$$ where $\mathbf{J}_s(= \lim\limits_{\mathbf{J} \to \infty, \ \delta \to 0} \mathbf{J} \delta)$ is the surface current density (A/m). Note that the unit normal vector $\hat{n}$ points from medium 2 to medium 1. How does letting $\delta$ approach zero get us 2.1.10 and 2.1.11? And why do we have $\mathbf{E}_1 - \mathbf{E}_2$ and $\mathbf{H}_1 - \mathbf{H}_2$ instead of $\mathbf{E}_2 - \mathbf{E}_1$ and $\mathbf{H}_2 - \mathbf{H}_1$ ?","['physics', 'multivariable-calculus', 'stokes-theorem']"
4328318,Expected value of steps taken to hit +1 on a 1D integer random walk given you start from zero?,"I am trying to calculate the expected number of steps taken on a 1-dimensional random walk ( starting from zero ) to reach +1. So far my approach has been to use recursive expectation ( first step analysis ) technique but I end up creating infinite equations because there are infinite states ( since boundary is not closed and there is chance, albeit small that we go to negative infinity before coming to +1 ). By intuition ( and simulation results below ) I feel that the answer may be infinite number of steps ( for the expected value of steps taken to reach +1, starting from zero ) but I am not able to come up with a mathematical solution to it. Can someone please help me find the solution / answer to this question? Aside: I ran a simulation on my computer for this process 10,000 times ( assuming law of large numbers will help me get an answer close to theoretical average ). The simulation took roughly 2 hrs to run. Here are some few observations - You reach +1 only in odd # of steps 50.6% of times you reach +1 in 1 step ( law of large numbers in action ) Average # of steps taken to reach +1: 101,050 steps Max. steps taken ( extreme case ) to reach +1: 951,391,959 steps Distribution of steps taken is concentrated towards lower # of steps with few extreme outliers heavily skewing the mean.","['expected-value', 'markov-process', 'random-walk', 'probability']"
4328340,Does composition with itself convex implies function convex?,"Let $f:[0,1] \to [0,1]$ be a twice differentiable increasing function and suppose that $g:=f \circ f$ is a convex function. Is it true that $f$ itself is convex? I am aware that the converse implication is true (namely that if $f$ is increasing and convex then $g$ is convex) but this seems to be more difficult.","['convex-analysis', 'derivatives', 'real-analysis']"
4328348,Prove that there is a unit $u \in R$ such that $ub = bu = a$,"Let $R$ be a ring with identity containing elements $a$ and $b$ with $ab = b$ and $b^2 = a$ . Prove that there is a unit $u \in R$ such that $ub = bu = a$ . Source: Problem $15.3.6$ , Algebra in Action: A Course in Groups, Rings, and Fields by Shahriar Shahriari. My work: From $ab = b$ and $b^2 = a$ , we have $ab^2 = a^2 =  b^2$ . So, $a^2 = a$ , i.e. $a$ is idempotent. $u = b$ satisfies $ub = bu = a$ , but we don't know if $b$ is a unit. I believe the main reason I'm stuck is that $a$ and $b$ may not be units, but from the given information, we likely want to find $u$ in the form $u = a^\alpha b^\beta$ for some $\alpha,\beta\in \mathbb N$ . How do I proceed? Thank you!","['ring-theory', 'abstract-algebra']"
4328385,How does this expression follow algebraically from the last one? (continued),"Continuing from here: How does this expression follow algebraically from the last one? The ""new"" system: \begin{align*}
\dot S &= \Lambda - (\beta_1 S I_2 +\beta_2 S J+ \beta_3 S A )-\mu S \\
\dot I_1 &= p\beta_1 S I_2  +q\beta_2 S J +r \beta_3 S A +\xi_1 J -b_1 I_1\\
\dot I_2 &= (1-P)\beta_1 S I_2  +(1-q)\beta_2 S J+(1-r) \beta_3 S A +\epsilon I_1 +\xi_2 J -b_2 I_2\\
\dot J &= p_1 I_2 -b_3 J\\
\dot A &= p_2 J - b_4 A
\end{align*} Reproduction number: \begin{align}
\mathcal{R}_0 &= \frac{\Lambda\left[ \beta_1 b_3 b_4 \left( \epsilon p +b_1(\left( 1-p\right)\right)+\beta_2 p_1 b_4 \left( \epsilon q +b_1(\left( 1-q\right)\right)+\beta_3 p_1 p_2 \left( \epsilon r +b_1(\left( 1-r\right)\right) \right]}{\mu b_4 \left[ b_1 b_2 b_3 - p_1\left( \epsilon \xi_1 +b_1 \xi_2 \right) \right] }
\end{align} Equilibrium point: \begin{align}
S^* &= \frac{\Lambda}{\mu \mathcal{R}_0}\\[2ex]
I_1^*  &= \frac{1}{b_1}\left[ p \beta_1 \frac{\Lambda b_3 b_4}{\left(\beta_1 b_3 b_4+\beta_2 p_1 b_4+\beta_3 p_1 p_2 \right)J^* +\mu b_4 p_1}\right.\\[1ex]
&\quad\;\;+\left. q \beta_2 \frac{\Lambda b_4 p_1}{\left(\beta_1 b_3 b_4+\beta_2 p_1 b_4+\beta_3 p_1 p_2 \right)J^* +\mu b_4 p_1}\right.\\[1ex]
&\quad\;\;+\left. r \beta_3 \frac{\Lambda p_1 p_2}{\left(\beta_1 b_3 b_4+\beta_2 p_1 b_4+\beta_3 p_1 p_2 \right)J^* +\mu b_4 p_1}+\xi_1\right]J^*\\[2ex]
I_2^* &= \frac{b_3}{p_1}J^*\\[2ex]
J^*&= \frac{\mu b_4  p_1 }{\beta_1 b_3 b_4 +\beta_2 p_1 b_4 +\beta_3 p_1 p_2}\left(\mathcal{R}_0-1\right)\\[2ex]
A^*&=\frac{p_2}{b_4}J^*
\end{align} Theorem: If $p=q=r$ and $\mathcal{R}_0 >1$ then the above equilibrium point is globally stable. To prove this: Define the following Lyapunov function \begin{align}
V &= S-S^* \ln S+ B\left( I_1-{I_1}^* \ln I_1\right) +C\left(I_2-{I_2}^* \ln I_2\right) + D\left( J -J^* \ln J\right)\\
&+E\left(A-A^* \ln A\right).
\end{align} The derivative of $V$ \begin{align*}
\dot V &=\left(1-\frac{S^*}{S}\right) \dot S + B\left(1-\frac{{I_1}^*}{I_1} \right)\dot I_1  + C\left(1-\frac{{I_2}^*}{I_2}\right)\dot I_2+D\left(1-\frac{J^*}{J} \right)\dot J\\[1ex] 
&+ E\left(1-\frac{A^*}{A}\right) \dot A
\end{align*} where: \begin{align*}
B &= \frac{\epsilon}{\epsilon p +b_1(1-p)}\\[1ex]
C&= \frac{b_1}{\epsilon p +b_1(1-p)}\\[1ex]
D &= \frac{b_1 b_2}{p_1[\epsilon p +b_1(1-p)]} -\frac{\beta_1 S^*}{p_1}\\[1ex]
E &= \frac{\beta_3 S^*}{b4}
\end{align*} We get to the following: \begin{align*}
\dot V &= -\mu S^* \frac{\left(1-x\right)^2}{x}+ \left[ \beta_1 S^* {I_2}^*+\beta_2 S^* J^*+  \beta_3 S^* A^* +B p\beta_1 S^* {I_2}^* + B q \beta_2 S^* J^*\right.\\[1ex]
&+\left. B r \beta_3 S^* A^*+ B \xi_1 J^*+ C(1-p)\beta_1 S^* {I_2}^*+C(1-q)\beta_2 S^* J^*+C(1-r)\beta_3 S^* A^*\right.\\[1ex]
&+\left. C \epsilon {I_1}^*+C \xi_2 J^*+D p_1 {I_2}^*+ E p_2 J^* \right]-x\left[ C(1-p)\beta_1 S^* {I_2}^* \right] -\frac{xz}{y}B p\beta_1 S^* {I_2}^*\\[1ex]
& -\frac{xu}{y}B q \beta_2 S^* J^* -\frac{xv}{y}B r \beta_3 S^* A^*-\frac{u}{y}B \xi_1 J^* - \frac{xu}{z}C(1-q)\beta_2 S^* J^*\\[1ex]
&- \frac{xv}{z}C(1-r)\beta_3 S^* A^* - \frac{y}{z}C \epsilon {I_1}^* - \frac{u}{z}C \xi_2 J^*-\frac{z}{u}D p_1  {I_2}^*-\frac{u}{v}E p_2 J^*\\[1ex]
&- \frac{1}{x}\left[\beta_1 S^* {I_2}^*+ \beta_2 S^* J^*+\beta_3 S^* A^*\right]\\[2ex]
\end{align*} How do we get this expression to the 'final form' like in the question I referred at the beginning of this question?","['nonlinear-system', 'lyapunov-functions', 'ordinary-differential-equations']"
4328393,Laplace and Orlicz characterizations of sub-Gaussianity,"Many different characterizations of sub-Gaussianity for a centred r.v. X exist including the ""Laplace formulation"" where $\exists \sigma > 0$ such that $$ \mathbb{E} \exp(tX) \le \exp(t^2 \sigma^2 / 2) $$ for all $t \in \Re$ , and the Orlicz norm formulation, that $\|X\|_{\psi_2} < \infty$ in $$ \|X\|_{\psi_2} := \inf \{t > 0 : \mathbb{E} \exp(X^2 / t^2) \le 2 \}. $$ I am interested in the relationship between $\sigma$ and $\|X\|_{\psi_2}$ , particularly as applied to tail bounds. Clearly this is not a bijection, because for $Z \sim N(0, 1)$ we have $\sigma_Z = 1$ and $\|Z\|_{\psi_2} = \sqrt{8/3} \approx 1.63$ by direct calculation (as setting $\mathbb{E}\exp(Z^2/t^2) = (1-2/t^2)^{-\frac12} = 2$ gives $t^2 = 8/3$ ), while for $Y$ with a Rademacher distribution $\sigma_Y = 1$ but $\|Y\|_{\psi_2} = 1/\sqrt{\log 2} \approx 1.20$ . Thus the Orlicz norm gives Chernoff bounds that are weaker and tighter respectively than those obtained through the Laplace formulation. Based on the equivalences reported in Wainwright (High Dimensional Statistics, p.47 or see p.39 in the draft here: https://www.stat.berkeley.edu/~mjwain/stat210b/Chap2_TailBounds_Jan22_2015.pdf , implication I to IV) we have (since the Laplace condition implies $E \exp(s X^2 / 2\sigma^2) \le (1-s)^{-\frac12}$ ) the upper bound $$ \|X\|_{\psi_2} \le \sqrt{\frac{8}{3}} \, \sigma. $$ which is tight for $Z$ , i.e. a Gaussian r.v. maximizes the sub-Gaussian norm for fixed $\sigma$ . My question is if there is a reversed inequality, of the form $$ \sigma \le C \|X\|_{\psi_2} $$ and I am interested in the tightest possible numerical value of this constant (if it exists). Any other comments on the relationship between these formulations would also be much appreciated. I have tried manipulating the equivalences of sub-Gaussianity reported in Wainwright and Vershynin (High Dimensional Probability), but these appear to lead to upper bounds rather than the lower bounds I'm interested in.","['statistics', 'laplace-transform', 'probability', 'orlicz-spaces']"
4328409,An inequality about quasi-linear function,"Let $\gamma$ be a positive, nondecreasing, continuous, function defined on $[0,\infty]$ . Suppose that $\gamma(x+y)\le C(\gamma(x)+\gamma(y))$ . In addition, suppose $$ \int_{2}^{\infty}\frac{dr}{\gamma(r)}=\infty.$$ suppose further that $$
f(t)\le f(0)+\int_0^t\gamma(f(r))dr+\gamma \Big(\int_0^tf(r)dr\Big).
$$ How to show that $$
\gamma(\int_0^t f(r)dr)\le C(t+1)\int_0^t \gamma(f(r))dr
$$ Actually, this question come from lemma 1.2 in this paper . My effort: Following the hint in this paper, I want to show that $\gamma$ cannot grow faster than $t^2$ . However, I can't rule out the possibility that $$\limsup_{t\to +\infty}\frac{\gamma(t)}{t^2}=\infty.$$ I have no idea how to proceed it. Thanks for any help. Update: If $C\le 1$ , I can give a proof. \begin{align*}
\gamma(\int_0^t f(r)dr)&=\gamma(\sum_{k=1}^n\int_{\frac{(k-1)t}{n}}^{\frac{kt}{n}}f(r)dr)\\
&\le \sum_{k=1}^{n}C^k\gamma(f(\xi_k))\frac{t}{n},\xi_k\in (\frac{k-1}{n}t,\frac{k}{n}t)
\end{align*} If $C\le 1$ , we can send $n$ to $\infty$ and get the proof. (Sorry. I find that the last inequality above this line is false. Because $\gamma(ax+by)\not \le a\gamma(x)+b\gamma(y)$ ) In addition, we can get a upper bound of $\gamma$ . In fact, $$\gamma(2^n)\le 2C\gamma(2^{n-1})\le (2C)^n\gamma(1).$$ Using $\gamma$ is increasing, we have that $$
\gamma(t)\le Ct^{1+\frac{\ln C}{\ln 2}}
$$","['inequality', 'analysis', 'partial-differential-equations']"
4328507,Which statements using quantifiers is true?,"I have a task here with the expression $(m|p^n-n)$ and some quantifiers in front, where I need to pick the one that is true. The universal set is ${R}$ . Edit: I read wrong, universal set is $(2,3,4...)$ . The four options are $∀m∀n∃p(m|p^n-n)$ $∃m∃n∀p(m|p^n-n)$ $∀m∃n∀p(m|p^n-n)$ $∃m∀n∃p(m|p^n-n)$ I tried to look at $(m|p^n-n)$ as $p^n≡n(modm)$ . In the first one, if $n=0$ and random $m=3$ , we end up with $1≡0(mod3)$ and I think/hope that is enough to disprove that one. But is that the way to go? Just finding exceptions? I'm not quite sure how I need to think when solving a problem like this. Is thinking modulo correct? According to the answers 4 is the correct one, but I don't know how to prove/disprove these types of questions in general. (Sorry for bad title, don't know how to formulate this question into nice title and English isn't my first language so all my ""math language"" is in another language)",['discrete-mathematics']
