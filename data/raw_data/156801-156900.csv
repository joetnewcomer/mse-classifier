question_id,title,body,tags
2669177,What is the difference between exact and partial differentiation?,"My understanding of partial $\left( \frac{\partial}{\partial} \right)$ and total 
$\left( \frac{d}{d} \right)$ differentiation/derivative is that assuming $f(x_1, x_2, ...,x_n )$ where $x_i$s are not necessarily independent: $$\frac{d f}{ dx_i}=\sum^n_1 \left(\frac{\partial f}{\partial x_j}\frac{d x_j}{dx_i} \right)$$ Where $\frac{\partial f}{\partial x_i}$ is the symbolic derivative of the equation $f(x_1, x_2, ...,x_n )$ assuming all $x_j$s except $x_i$ are constants. Of course when $x_i$s are independent: $$\frac{\partial f}{\partial x_i}=\frac{d f}{ dx_i}$$ But in thermodynamics I see that they have this exact differential $$\left(\frac{\partial f}{\partial x_i} \right)_{x_j}$$ which to me looks exactly the same as partial differential. For example see these videos of thermodynamic lectures from MIT . I find this concept/notation redundant and confusing. I would appreciate if you could explain the difference between partial and exact differentials and give me a tangible example when they are not the same. P.S.1. This post also approves my point: In fact, the constancy of the other variables is implicit in the partial differential notation (∂/∂x) but it is customary to write the variables that are constant under the derivative when discussing thermodynamics, just to keep track of what other variables we were considering in that particular case. Which if true, is an awful idea. Partial differential equations are already long and confusing enough without these redundant notations. Why on earth should we make it even more difficult?",['derivatives']
2669208,On the proof of Meusnier's Theorem: Direction of derivative of unit velocity vector,"In Andrew Pressley's ""Elementary Differential Geometry"" (second edition) on page 168 he gives a proof of Meusnier's Theorem, which is stated as follows: Theorem: Let $\mathbf p$ be a point on the surface $\mathcal S$ and let $\mathbf v$ be a unit tangent vector to $\mathcal S$ at $\mathbf p$. Let $\Pi_\theta$ be the plane containing the line through $\mathbf p$ parallel to $\mathbf v$ and making an angle $\theta$ with the tangent plane $T_{\mathbf p}\mathcal S$, and assume that $\Pi_\theta$ is not parallel to $T_{\mathbf p}\mathcal S$. Suppose that $\Pi_\theta$ intersects $S$ in a curve with curvature $\kappa_\theta$. Then $\kappa_\theta\sin\theta$ is independent of $\theta$. The proof is short, and begins as follows: Assume that $\gamma_\theta$ is a unit-speed parametrization of the curve of intersection of $\Pi_\theta$ and $\mathcal S$. Then at $\mathbf p$, $\gamma_\theta' = \pm\mathbf v$, so $\gamma_\theta''$ is perpendicular to $\mathbf v$ $\underline{\text{and is parallel to $\Pi_\theta$}}$. My question is about the underlined statement. How do we know that $\gamma_\theta''$ lies on $\Pi_\theta$? Looking at the diagram given, it seems obvious, in the sense that $\gamma_\theta''$ is parallel to the principle normal to $\gamma_\theta$, which, along with $\gamma_\theta'$ forms the basis for the oscullating plane, which also ""looks"" like the plane $\Pi_\theta$, but other than that heuristic argument I can't formalize why $\gamma_\theta''$ should lie on $\Pi_\theta$. I'm sure its easy, I'm just missing the obvious. Can anyone help out?","['differential-geometry', 'proof-explanation']"
2669305,Ideals of the ring of rational numbers with odd denominators,"Consider the subring $R\subset\Bbb Q$ , $R=\{\frac ab\ | \ a,b\in\Bbb Z, b\text{ odd}\}$ I am struggling with the following questions: (1) Prove that the ideals of $R$ are $\{0\}$ and $2^nR$ for $n\ge 0$ (2) Prove that $R$ has 2 prime ideals and 1 maximal ideal For (1) I can see that $2^nR$ is an ideal for $n\geq0$ but I'm not sure how to show all ideals other than $\{0\}$ are of this form With question (2), $2^nR\subsetneq 2^{n-1}R\subsetneq...\subsetneq 2R\subsetneq R$ so $2R$ is clearly the only maximal ideal and is therefore prime, but I am not sure where to go from here. Any suggestions?","['abstract-algebra', 'ring-theory', 'maximal-and-prime-ideals']"
2669359,"Given finite subset of $\mathbb C[[X]]$, does there exist a ring automorphism of $\mathbb C[[X]]$, fixing $A$, but not fixing $\mathbb C[X]$ set wise?","For every  finite subset $A $ of $\mathbb C[[X]]$, does there exist a ring automorphism (bijective ring endomorphism ) $f$ of $\mathbb C[[X]]$ such that $f(a)=a, \forall a \in A$ but $f(\mathbb C[X]) \ne \mathbb C[X]$ ? Related Are $\mathbb C$ , $\mathbb C[X]$ definable in $\mathbb C[[X]]$? because if $D \subseteq M$ is definable by a set $A \subseteq M$  then $f(D)=D, \forall f \in Aut_A M$ , and in the related case we mean definable by some finite subset of $M$.","['polynomials', 'complex-analysis', 'ring-theory', 'commutative-algebra', 'formal-power-series']"
2669369,First order nonlinear differential inequality,"I have a differential inequality as follows:
$$f'(x)\geq cf(x)^a,\quad \forall x\in[0,1]$$
where $0<a<1,\, f(x)\geq 0$ Wolfram Alpha gives the following answer for the equality:
$$f(x)= ((a - 1) (k_1 - c x))^{1/(1 - a)}$$ I'm interested to find the answer to the inequality. But I don't know where to begin.","['nonlinear-analysis', 'ordinary-differential-equations']"
2669378,"What is the stalk $F_0$ of the presheaf $F$ at $0 \in X = \{0, 1\}$ given the discrete topology?","If you can look at example 1.5 here , it is shown that $F_0= F(\{0\})$. I was wondering if anyone here can explain in some more detail the example given. I am confused by how we know that there is some $b \in F(\{0\})$ such that $a\sim  b$? How does it follow that $F_0 \subseteq F(\{0\})$ and why do we have $a = b$? How do we get $F(\{0\})\subseteq F_0$?",['algebraic-geometry']
2669396,Multivariate polynomials as matrix products?,"Sorry if this is a silly question.  But if I have a polynomial in two variables, with the maximum degree of each individual variable no greater than N,
$$p(x, y)=c_{00}+c_{10}x+c_{01}y+c_{11}xy+c_{20}x^2+c_{02}y^2+...
=\sum_{i=0}^N{\sum_{j=0}^N{c_{ij}x^iy^j}}$$ ...then I can write this as a product, $$p(x,y)=\textbf{x}'A\textbf{y},$$ where $$\textbf{x}=\begin{matrix}\begin{pmatrix}1&x&x^2&...&x^N\end{pmatrix}\end{matrix}'$$
$$\textbf{y}=\begin{matrix}\begin{pmatrix}1&y&y^2&...&y^N\end{pmatrix}\end{matrix}'$$
$$A=\begin{matrix}\begin{pmatrix}c_{00}&c_{01}&c_{02}&...&c_{0N}
\\c_{10}&c_{11}&c_{12}&...&c_{1N}
\\c_{20}&c_{21}&c_{22}&...&c_{2N}
\\...
\\c_{N0}&c_{N1}&c_{N2}&...&c_{NN}\end{pmatrix}\end{matrix}$$ This seems convenient, since the elements of A correspond exactly to the coefficients on each term of the function. My questions are: 1) Is this a ""standard"" way of representing a polynomial of two variables? 2) Is there some convenient way to extend this to polynomials of more than two variables? I don't know if there's a more general way to state this question, or if there is some theory out there regarding multivariate polynomials that I could learn, but any answers or links to resources would be helpful...I'm not quite sure what to Google for this.","['matrices', 'polynomials', 'matrix-decomposition']"
2669416,Study the Irreducibility of polynomial [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question How could I study the irreducibility of the polynomial below $((x-1)(x-2) \cdots (x-n))+1$ in $\mathbb{Q}[x]$","['irreducible-polynomials', 'ring-theory', 'extension-field', 'algebraic-geometry']"
2669439,What if replacing the anticommutativity by commutativity in the definition of Lie algebra?,"As the definition of Lie algebra in Wikipedia. A Lie algebra is a vector space together with a bilinear map, called Lie bracket, satisfying the alternativity and the Jacobi identity. The alternativity can be replaced by anticommutativity since they are equivalent under the Jacobi identity bilinearity. I know the motivation of this definition. But what's going to happen if we replace the alternativity or anticommutativity by commutativity in the definition? Are there some mathematical notions related to this? I just wonder. Maybe it makes no sense at all. Does anyone know about this? TIA...","['differential-geometry', 'lie-algebras', 'lie-groups']"
2669470,Composition of two pivot-angle rotations in 2D,"Suppose I have a set of points $P$ in the 2D plane. Let $(\theta_1, p_1)$ and $(\theta_2, p_2)$ be two axis-pivot rotation, where for each $i \in \{1, 2\}$, $\theta_i$ is a rotation angle and $p_i$ the pivot point of this rotation. 
I would like to rotate each point of $P$ by an angle of $\theta_1$ around $p_1$, then rotate each resulting point by an angle of $\theta_2$ around $p_2$. The resulting location of each point of $P$ can easily be computed by calculating the result of the first rotation, then the second. However, can this be described by a single axis-pivot rotation $(\theta_3, P_3)$? Semi-related is this question: Composition of two axis-angle rotations However, it applies to 3D and seems more complicated than it needs to be.","['rotations', 'geometry']"
2669544,Show that $a_n = f(0) + f(1) + ... + f(n-1) - \int^n_0 f(x) dx$ is a Cauchy sequence,"Suppose $f(x)$ is continuous and decreasing on $[0, \infty]$, and $f(n) \rightarrow 0$. Define $\{a_n\}$ by
$$a_n = f(0) + f(1) + ... + f(n-1) - \int^n_0 f(x) dx$$
(a) Prove $\{a_n\}$ is a Cauchy sequence directly from the definition. (b) Evaluate $\lim a_n$ if $f(x) = e^{-x}$. I started out by examining $a_{n+1} - a_n$: $$a_{n+1} - a_n = f(n) - \int_n^{n+1} f(x)dx$$ Then, by the Triangle Inequality: $$|a_m - a_n| \leq |a_{n+1} - a_n| + |a_{n+2} - a_{n+1}| + ... + |a_{m} - a_{m-1}|$$ and so, $$|a_m - a_n| \leq \sum_{k = n}^m f(k) - \int_n^{m} f(x)dx$$ And I am lost afterwards. Am I on the right track? How can I use this expression to also prove the limit for $f(x) = e^{-x}$?","['real-analysis', 'cauchy-sequences', 'sequences-and-series', 'limits']"
2669574,A Subspace of Discrete Topology is Compact iff. It is Finite,"Let $X$ be a discrete topology and $A$ is a subspace of $X$. Then, $A$ is compact $\iff$ A is finite. My attempt is general: ($\implies$) Let $X$ be compact. Suppose that X is infinite. Consider a open-cover $C=\{\{x\} : x \in X\}  $. By the hypothesis, and by the definition of compactness of a space, $\exists H\in C : \bigcup_{H\in C}H=X$ where H is finite. Since $X$ is infinite but the subcover $H$ is finite, this is a contradiction. Now, my question is: How can I solve this through a subspace $A$ of $X$?",['general-topology']
2669641,"If $(a)$ and $(b)$ are principal ideals of $R$, when is $(a)(b) = (ab)$?","Assume $R$ is a ring and $(a)$ and $(b)$ are principal ideals in $R$. Under what conditions is the product of the ideals equal to the principal ideal generated by the product, i.e. when is $(a)(b)=(ab)$? My guess is that they're guaranteed to be equal if $R$ is a U.F.D. (or maybe just an integral domain?), but I do not know how to prove this for an arbitrary ring. I can't think of any counterexamples that would suggest my intuition to be wrong though. Any thoughts?","['abstract-algebra', 'ring-theory', 'ideals']"
2669672,Is algebraic closure necessary for Rosenlicht's Theorem?,"I've seen ""Theorem 2"" in Rosenlicht's ""Some Basic Theorems on Algebraic Groups"" ( http://www.jstor.org/stable/2372523 ) in different forms in varying texts. I'm having some trouble with the language in the aforementioned paper. As I understand it, Rosenlicht's Theorem says that for an algebraic group $G$ over a field $k$ acting on an irreducible variety $X$ over $k$, there exists a Zariski-open, $G$-invariant, subset $X_0\subset X$ such that the geometric quotient exists. The geometric quotient is defined as a morphism $\pi:X_0\rightarrow Y$ satisfying (1) $\pi$ is surjective, and for $x\in X_0$, $\pi^{-1}(\pi(x))$ equals the orbit of $x$ under the action of $G$. (2) $\pi$ is an open map. (3) For any open subset $U\subset Y$, the ring homomorphism $\pi^*$ is an isomorphism from $k[U]$ to $k[\pi^{-1}(U)]$. In particular this implies that the orbits of $G$ on $X_0$ are closed. Is this correct? More specifically, do we need the condition that $k$ is algebraically closed? Can $k=\mathbb{R}$?","['algebraic-groups', 'algebraic-geometry']"
2669755,Counter-example for closed mapping theorem in normed spaces,"I am searching a reference to following counter-example for closed mapping theorem in normed spaces: Given $X$ a Banach space, and $Y$ a normed space that is not a Banach space, Let $T: X \to Y$ to be a linear operator such that $graf(T)$ is a closed set and $T$ is an unbounded operator.","['functional-analysis', 'examples-counterexamples', 'banach-spaces']"
2669765,"Summation involving binomial coefficient, exponent and another term",I am trying to find if a closed form formula exists for the following summation: $$\sum_{i=0}^{n} {n\choose i}\frac{x^{i}}{i+k}$$ where $x$ and $k$ can be any real numbers. I tried to search but the closest I found was the following identity (from vol.2 at https://www.math.wvu.edu/~gould/ ): $$\sum_{i=0}^{n} {n\choose i}\frac{x^{i}}{i+1}=\frac{(x+1)^{n+1}-1}{(n+1)x}$$ Any help would be greatly appreciated.,"['combinatorics', 'summation']"
2669796,"Does this ${[a,b]}^{[a,b]}$ make sense in mathematics?","if a real valued function defined for example as :$[a,b]\times[a,b] \to \mathbb{R}$ , then I ask if there is a function defined as :${[a,b]}^{[a,b]} \to \mathbb{R}$ or Does this ${[a,b]}^{[a,b]}$ make sense in mathematics? Note: $a, b$ are real numbers  with  $a < b $","['area', 'functions']"
2669828,Combinatorics using Linear Transformation,"Determine the number of integer solutions to $x_1+x_2+x_3+x_4 \le 72$ such that: $1 \le x_1 \le 12$ $0 \le x_2\le 10$ $3 \le x_3\le 13$ $5 \le x_4\le 36$ At first, I introduced a variable $x_5$ such that $x_1+x_2+x_3+x_4+x_5=72$ and applied a linear transformation such that: $y_1 = x_1-1$ $y_2 = x_2$ $y_3 = x_3-3$ $y_4 = x_4-5$ $y_5 = x_5$ and then regarranging and substituting values into the original equation, I get $y_1+y_2+y_3+y_4+y_5=63$ such that: $y_1 \le 12$ $y_2 \le 10$ $y_3 \le 13$ $y_4 \le 36$ $y_5 \ge 0$ my instincts tell me I should apply another linear transformation, but I'm not quite sure how to go from here. Any tips would be appreciated!","['combinatorics', 'discrete-mathematics']"
2669849,Tricky subset counting problem,"I had a combinatorics test today and I came upon this question which I just couldn't figure out how to do. Can anyone explain in depth how to do this? Let $m \ge 34$ be an even integer, Let $n \ge 1$ be an integer. Consider the two sets $$
\begin{gather} 
\begin{aligned} 
A &= \{1,2,\ldots,m\} \\ 
B &= \{m+1,m+2,\ldots,m+n\}
\end{aligned} \end{gather}
$$ Let $k$ be an integer with $17 \le k \le n+17$. Consider the subsets $X$ of $A \cup B$, such that $|X| = k$, $|X \cap A| = 17$, and all elements of $X \cap A$ are even . How many such subsets are there? The answer is: ${{m/2}\choose17} \cdot {{n}\choose{k-17}} $ I am having trouble figuring out what this means. Consider the subsets $X$ of $A \cup B$, such that $|X| = k$, $|X \cap A| = 17$, and all elements of $X \cap A$ are even . I believe I will better understand this problem if I understand how to break down these set notations and get a more intuitive view of this problem which is easier to grasp. Thank you!","['combinatorics', 'discrete-mathematics']"
2669850,"Generate a $5 × 5$ matrix such that the each entry is an integer between $1$ and $9$, inclusive, and whose determinant is divisible by $271$.","Generate a $5 × 5$ matrix such that the each entry is an integer between $1$ and $9$, inclusive, and whose determinant is divisible by $271$. This is a practice problem for a linear algebra exam I have coming up, and can't for the life of me figure it out. I was thinking maybe making a triangular matrix ($0$s below main diagonal, determinant would be the product of the diagonal), but that wouldn't work because it says to use integers $1$-$9$. Been thinking of this problem all day. The only way we've covered the determinant for a large matrix ($3 × 3$ or larger) has been through summing up the signed elementary product , but for a $5 × 5$ matrix, you'd need to make sure that all $5! = 120$ signed elementary products would need to be divisible by $271$. If anyone has a better way to approach and solve this problem, it would be very much appreciated.","['matrices', 'linear-algebra', 'determinant']"
2669864,Finding the derivative of an integral function,"I had some homework for my differential equations class, and one of the questions completely stumped me, reproduced here: Find $\frac{dy}{dx}$ for $y = ce^{-x} + e^{-x}\int_0^x\frac{tan(t)}{t}dt$ My next line looked like $\frac{dy}{dx} = -ce^{-x} + \frac{d}{dx}(e^{-x}\int_0^x\frac{tan(t)}{t}dt)$ and by using the Fundamental Theorem of Calculus and the Product Rule (I don't know how correctly), my next line looked like $\frac{dy}{dx} = -ce^{-x} + (e^{-x}\frac{d}{dx}\int_0^x\frac{tan(t)}{t}dt - e^{-x}\int_0^x\frac{tan(t)}{t}dt )$ and subsequently $\frac{dy}{dx} = -ce^{-x} + e^{-x}\frac{tan(x)}{x} - e^{-x}\int_0^x\frac{tan(t)}{t}dt$ I tried evaluating the integral, since a simple derivation made it clear that it couldn't be avoided. However, I was not able to do so, and when looking it up on www.symbolab.com, it turns out to have no elementary antiderivative/is non-integrable. Does anyone know how to solve the original question? NOTE: the assignment deadline already passed and I have already been graded on my attempt at this question. This is not an attempt to pass off anyone's insights and work as my own.","['derivatives', 'improper-integrals', 'definite-integrals']"
2669942,"Justification behind the working of ""integration by substitution""","I am very confused about ""integration by substitution"". For example: We know that $\int\ x^2 dx$ = $x^3/3 +C$. Just for doing it, maybe for the sake of practicing, or for testing with integration by substitution, we could have made the substitution $x^2=u$. Then $x=u^{1/2}$ and $dx= (u^{-1/2}/2) du$. Consequently: $\int\ x^2 dx = \int\ u (u^{-1/2}/2) du = (1/2) \int\ u^{1/2} du = (1/3) u^{3/2} + C$ Since $u^{3/2} = x^{3}$, we see that we got the right answer. My question is, why did we get the right answer? This method look like magic to me, since I don't know the justification behind it. How do we prove that we can make the change of variable and change the $dx$ accordingly to obtain the right answer. ""Integration by parts"" is based on the product rule, for example. On what differentiation rule or rules is based integration by substitution? Is it the chain rule? By the way, I have no idea what I am doing. I just want to know the logic behind it. I have not see an explanation in the books I have searched. It seems to be introduced by examples, without justification. I am sorry if my question is bad, I am really confused right now.","['integration', 'calculus']"
2670012,Example of infinite dimensional linear spaces where the space is equal to its dual.,"My understanding is that in finite dimensions, every linear space $V$ is isomorphic to its dual $V^\ast$.
In infinite dimensions, we have that any Hilbert space $\mathcal{H}$ is isomorphic (specifically, anti-isomorphic) to its dual $\mathcal{H}^\ast$ (Riesz Representation Theorem). Furthermore, every Hilbert space is also isomorphic to the square summable sequence space $\ell^2$. I am wondering if there are examples of infinite dimensional linear spaces where the dual is equal to itself, and the space is not isomorphic to $\ell^2$. Edit : We assume the underlying field to be $\mathbb{R}$ or $\mathbb{C}$.","['hilbert-spaces', 'dual-spaces', 'functional-analysis', 'linear-algebra', 'vector-spaces']"
2670032,Do there exist any subsets of $\mathbb{R}$ with positive measure but not of size continuum? [duplicate],"This question already has answers here : Let $X\subset \mathbb{R}$ Lebesgue measurable, $|X|<|\mathbb{R}|$, is it true that $X$ is null? (2 answers) Closed 6 years ago . As title goes, without assuming continuum hypothesis, is there a subset of $\mathbb{R}$ with positive measure but not being continuum? That is, Does there exist $A\subseteq \mathbb{R}$ such that $\mu(A)>0$ and $\aleph_0<|A|<2^{\aleph_0}$ ? I believe that maybe in some system without assuming continuum hypothesis, there exists. And by the regularity of Lebesgue measure, it suffices to deal with compact set.","['cardinals', 'real-analysis', 'set-theory', 'measure-theory']"
2670045,Guillemin-Pollack: application of the Transversality Theorem,"I'm working on two exercises from Guillemin-Pollack which have the same flavor: (General Position Lemma) Let $X$ and $Y$ be submanifolds of $\mathbb R^N$. Show that for almost
  every $a\in \mathbb R^N$, the translate $X+a$ intersects $Y$
  transversally. Suppose that $X$ is a submanifold of $\mathbb R^N$. Show that almost every vector space $V$ of any fixed dimension $l$ in $\mathbb R^N$ intersects $X$ transversally. [HINT: The set $S\subset  (\mathbb R^N)^l$ consisting of all linearly independent $l$-tuples if vectors in $\mathbb R^N$ is open in $\mathbb R^{Nl}$, and the map $\mathbb R^l\times S\rightarrow \mathbb R^N$ defined by $[(t_1,\dots,t_l),v_1,\dots,v_l]\mapsto t_1v_1+\dots t_lv_l$ is a submersion.] In both cases, I believe I need to apply the following version of the Transversality Theorem (see this answer): Theorem : Suppose that $F:X\times S\to Y$ is a smooth map of manifolds and $Z$ is a submanifold of $Y$, all manifolds without boundary . If $F$ is transverse to $Z$ then for almost every $s\in S$ the map $f_s : x\mapsto F(x,s)$ is transverse to $Z$. I have the same problem in both exercises. In the first exercise, the theorem guarantees that for almost every $a\in \mathbb R^N$, the map $f_a: X\rightarrow \mathbb R^N$ given by $x\mapsto x+a$ is transversal to $Y$. Note that the image of this map is $X+a$. I need to show that the image is transversal to $Y$. How does it follow? In the second exercise, the theorem guarantees that for almost every $v=(v_1,\dots,v_l)\in S$, the map $f_v: \mathbb R^l\rightarrow \mathbb R^N$ given by $(t_1,\dots,t_l)\mapsto t_1v_1+\dots+t_lv_l$ is transversal to $X$. Note that the image of this map is an $l$-dimensional subspace of $\mathbb R^N$. I need to show that the image is transversal to $X$. How does it follow?","['differential-topology', 'smooth-manifolds', 'manifolds', 'transversality', 'differential-geometry']"
2670063,Let $(a_n)_{n=1}^\infty$ be an infinite sequence of complex numbers. Prove the following limit.,"Given $$\lim_{x\to \infty} \frac 1x \sum_{n\le x} a_n = k,$$
I want to prove that 
$$\lim_{x\to \infty} \frac {1}{\log x} \sum_{n\le x} \frac {a_n}{n} = k.$$ I'm much more interested in learning the technique(s) necessary in order to prove this rather than a direct proof. Specifically, we have learned about asymptotic estimates and summation by parts, but I don't see how I can use those techniques to prove the problem statement. Thank you for any insight!","['analytic-number-theory', 'summation', 'asymptotics', 'limits']"
2670070,Prove that a power series that is zero on a sequence that converges to zero is the zero function,"I've been trying to solve this problem from Abbott's Understanding Analysis for hours and can't seem to get the last piece of the proof. Let $g(x) = \sum_{n=1}^\infty b_n x^n$ be a power series that converges for all $x \in (-R,R)$. Let $x_n \rightarrow 0$, while $x_n \neq 0$, and $g(x_n) = 0$ for every $n$. Show that $g(x)$ must be identically zero on $(-R,R)$. I know that $g(0) = 0$ since $g$ is continuous and the set of zeros of a continuous function is closed. I know that $g'(0) = b_1$, and I need to show that this is zero. I know that if $f(x) = 0$ on an interval, then $f'(x) = 0$ on that interval as well, but I can't seem to find a parallel of that result to this problem.","['power-series', 'real-analysis', 'sequences-and-series']"
2670108,What does the notation $\pi$ mean in probability and statistics,"I have noticed that in the presentation of probability distribution, sometimes a certain probability is dentoed by $\pi(...)$. For example, in the wike page of Beta binomial distribution https://en.wikipedia.org/wiki/Beta-binomial_distribution , they use $P(X=k|p,n)$ to denote the distribution of $X$, while using $\pi(p|\alpha,\beta) = Beta(\alpha,\beta)$ to denote the distribution of $p$. May I ask why the notation $\pi$ is used here? Is there a specific meaning?","['statistics', 'probability']"
2670133,How can I prove that $f(x)=\sum_{k=1}^{\infty}\frac{\{kx\}}{k^{2}}$ is continuous?,"Let $f:\mathbb{R}\to\mathbb{R}$ be defined as $$f(x)=\sum_{k=1}^{\infty}\frac{\{kx\}}{k^{2}},$$ where $$\{x\}=\begin{cases}0; & x = \dfrac{n}{2}\ \text{for some odd integer } n\\x - m(x);& \text{otherwise}\end{cases}$$ and $m(x)$ is the nearest integer to $x$. I have already proved that $f$ is continuos at $x=0$ (actually I have proved that it is at any integer and, maybe, at rational numbers of the form $\dfrac{q}{2^{n}}$, with $q$ odd). To see this it is enough to see that if $x\in\mathbb{R}$ is such that $|x|<\dfrac{1}{2^{N}}<ε$ then
$$\left|\sum_{k=1}^{\infty}\frac{\{kx\}}{k^{2}}\right|\leq \left|\sum_{k=1}^{S}\frac{\{kx\}}{k^{2}}\right|+\left|\sum_{k=S+1}^{\infty}\frac{\{kx\}}{k^{2}}\right|<\left|\sum_{k=1}^{S}\frac{\{kx\}}{k^{2}}\right|+ε=\left|\sum_{k=1}^{S}\frac{kx}{k^{2}}\right|+ε.$$ Τhese last steps are true because $f(x)$ converges and then there is $J\in \mathbb{N}$ such that $$\left|\sum_{k=n}^{\infty}\frac{\{kx\}}{k^{2}}\right|<ε$$ for all $n\geq J$ and then, if $S=\max\{N,J\}$, $|kx|<\dfrac{k}{2^{S}}<\dfrac{1}{2}$ for all $k\leq S$, and so $|f(x)|<{\mit Γ}ε + ε$ for some constant $\mit Γ$ and therefore $f(x)$ is continuous at $0$. This same argument can be used to prove that $f(x)$ is continuous at any integer and (maybe) at any rational of the form $\dfrac{q}{2^{n}}$. Can I generalize for any real number? Or how can I find where $f(x)$ is not continuous?","['continuity', 'real-analysis', 'analysis']"
2670151,Derivative of $BAA^Tx$ with respect to $A$,"Find the derivative of $B A A^T x$ with respect to $A$, where $A, B$ are $n \times n$ matrices and $x$ is a vector. Clarification: When I say derivative I want to find the derivative of each element of the expression $BAA^Tx$ with respect to each element in $A$. In other words we are trying to find the derivative of a vector with respect to a matrix. Edit: This question and answer here is relevant. What I know/tried: (some wishful thinking) Well I know that I can write $y:=BAA^Tx$ in vectorized form as $y = (x^T \otimes B) \mathrm{vec}(AA^T)$. Then by ""chain rule"" we should get
$$
\frac{\partial y}{\partial A} = (x^T \otimes B)\, \circ\,\frac{\partial({\mathrm{vec}(AA^T)})}{\partial A}
$$ Thus, the problem actually boils down to finding the derivative of $AA^T$ with respect to $A$. How does one go about finding that last derivative?. Am I even on the right track?. Thanks.","['matrices', 'matrix-calculus', 'derivatives']"
2670168,Are there any other ways for infinite product to diverge/converge to zero (other than two mentioned in description),"An infinite product of complex terms can diverge/converge to zero if (a) one or more of the terms are zero and all other terms are finite or (b) infinitely many terms have $ |z| < 1 $ and atmost finite terms are such that $ 1 < |z| < \infty $ Examples: 
$$ P_1 = 2e^{i\theta} \cdot 1e^{i2\theta} \cdot \frac{1}{2}e^{i3\theta} \cdot \frac{1}{4}e^{i4\theta} \cdots $$
$$ P_2 = \frac{1}{2}e^{i\theta} \cdot \frac{3}{4}e^{i\theta} \cdot \frac{7}{8}e^{i\theta} \cdot \frac{15}{16}e^{i\theta}  \cdots $$ Are there any other reasons for an infinite product $ \prod_{n=0}^\infty z_n $ to converge/diverge to zero? ($ z_n \in \mathbb {C} $)","['complex-analysis', 'infinite-product']"
2670200,Separate two connected components of a closed set in a plane,"I'm trying to prove that: Let $\Omega \subset \mathbb{C}$ be an open set, $K$ a bounded
  connected component of $\mathbb{C} \setminus \Omega$. If $\,\{p_n(z)\}$
  is a sequence of polynomials that converges uniformly on every compact
  subset of $\Omega$, then it converges uniformly on $K$. By the maximum modulus principle, it suffices to prove that there exists a compact set $C \subset \mathbb{C} \,$s.t.$\, K\subset C\,$and$\,\partial C\subset \Omega$. I reduced this problem to the assertion below: Suppose $X \subset \mathbb{C}$ is a closed set, not connected. Let $K,L\,$ be two distinct connected components of $X$. (One may assume that $K$
  is bounded.) Assume that $K$ is bounded. Then there exist two disjoint closed sets $Y, Z \subset X$ satisfying
  $$K \subset Y,\, L \subset Z, \,\text{and} \,\,X = Y \sqcup Z .$$ How can I show this? Thank you. Edited: My final answer. Lemma 1. Let $X$ be a Hausdorff space and $K\subset X$ have a compact
  neighbourhood $T$. Then $K$ is a connected component of $X$ if and only if $K$
  is a  connected component of $T$. Proof: See here . $\square$ Put $X = \mathbb{C} \setminus \Omega \cup \{\infty\} \subset S^2$. Then $X$ is compact and Hausdorff. Take $R>0$ so large that $K\subset B(0,R)$. Applying the Lemma 1 to $T = X \cap \overline{B(0,R)}$ one can verify that $K$ is a connected component of $X$. Let $L$ be the connected component of $X$ that contains $\infty$. Then clearly $K, L$ are distinct connected components of $X$. Regarding $K$ and $L$ as quasi-components of $X$, one can choose two disjoint closed sets  $Y, Z \subset X$ satisfying
 $$K \subset Y,\, L \subset Z, \,\text{and} \,\,X = Y \sqcup Z .$$
Then both $Y$ and $Z$ are closed in $S^2$, and thus by normality there exists an open set $U$ s.t.
$$Y\subset U \subset \overline{U} \subset S^2 \setminus Z.$$
Since $\infty \notin \overline{U}$, $\overline{U}$ is a compact subset of $\mathbb{C}$. Moreover $\partial U \cap X = \emptyset$, i.e., $\partial U \subset \Omega$. Then by maximum modulus theorem, we can show that $\{ p_n (z)\}$ is uniformly Cauchy on $K$!!","['complex-analysis', 'general-topology']"
2670210,Why do we care about quotient maps?,"In topology class, I often hear statements like ""It turns out that $f$ is always a quotient map.""  I don't understand what is so special about quotients that the question of whether a given surjective map is or isn't a quotient is so interesting.  What motivates these statements, and what should I think about when someone mentions that $f$ is a quotient?",['general-topology']
2670228,Prove that $B$ is non singular and that $AB^{-1}A=A$,"$$A_{n\times n}=\begin{bmatrix}a & b & b & b &. &.&.&&b\\b & a
&b&b&.&.&.&&b\\b & b &a&b&.&.&.&&b\\b & . &.&.&.&.&.&&b\\b & .
&.&.&.&.&.&&b\\b & b &b&b&.&.&.&&a\end{bmatrix}\text{ where } 
a+(n-1)b =0$$ Define $l^t=\begin{bmatrix}1&1&1&1&1&....1\end{bmatrix}$ Where $l$ is
  a $ n\times1$ vector, and: $$B= A+ \frac{l\cdot l^t}{n}$$ Prove that $B$ is non singular and that $AB^{-1}A=A$ What i did: $\text{A has a 0 eigenvalue , so A is a singular matrix}$
$\text{B  has an eigenvalue of 1 with eigenvector} $$\,\, v^{t}= \begin{bmatrix}1&1&1&1&1&....1\end{bmatrix}$ Any idea about how to proceed? Thanks.","['matrices', 'linear-algebra']"
2670286,"If $A+B+C=0$, then prove that the value of the determinant is $0$.","I'll state the question from my textbook below: If $A+B+C=0$, the prove that $\begin{vmatrix}1 & \cos C & \cos B \\ \cos C & 1 & \cos A \\ \cos B & \cos A & 1 \end{vmatrix} = 0$. This is how I tried solving the problem: $LHS = \begin{vmatrix}1 & \cos C & \cos B \\ \cos C & 1 & \cos A \\ \cos B & \cos A & 1 \end{vmatrix}$ $= 1(1- \cos^2 A) - \cos C (\cos C - \cos A \cos B) + \cos B (\cos A \cos C - \cos B)$ $= 1 + 2 \cos A \cos B \cos C - (\cos^2 A +\cos^2 B + \cos^2 C)$ I don't know how to proceed further. I tried using the fact that $A+B+C=0$ but it didn't lead to anything I could solve. I don't know where is it supposed to be used. Also, I read a solution to this problem somewhere in which the term $(\cos^2 A +\cos^2 B + \cos^2 C)$ was replaced by $1 + 2 \cos A \cos B \cos C$ as $A+B+C=0$. Are these two terms equal for the given condition? Also, is there a way to prove the statement without using this fact? Any help would be appreciated.","['trigonometry', 'linear-algebra', 'determinant']"
2670314,General product rule formula for multivariable functions?,"Fix a natural number $n.$ Let $f,g:\mathbb{R}\to\mathbb{R}$ be $n$ times differentiable functions. General Leibniz rule states that $n$ th derivative of the product $fg$ is given by $$\sum_{k=0}^m\binom{n}{k}f^{(n-k)}(x) g^{(k)}(x)$$ where $g^{(k)}$ means that $g$ is differentiated $k$ times. In the wiki page, product rule for partial derivatives of multivariable functions is given by $$\partial^\alpha (fg) = \sum_{\{\beta:\beta\leq\alpha\}}\binom{\alpha}{\beta}(\partial^\beta f) (\partial^{\alpha-\beta}g)$$ Question : What is a meaning of $\beta\leq \alpha?$ Since the formula is for multivariable, I suppose that $\beta$ and $\alpha$ are vectors, like $(1,2)$ in $\mathbb{R}^2.$ However, I do not understand the ordering between vectors. 
Any help is appreciated.","['derivatives', 'real-analysis', 'notation', 'calculus']"
2670350,Operator norm calculation for simple matrix [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Suppose $$ A = \left( \begin{array}{cc} 1 & 4 \\ 5 & 6 \end{array}\right) $$ How do I calculate $\|A\|_{\text{OP}}$? I know the definition of operator norm, but I am clueless on how to calculate it for real example like this. Can somebody please give me a step-by-step instruction on how to do this?","['matrices', 'normed-spaces', 'matrix-norms', 'spectral-norm']"
2670401,Determinant of Union Jack matrix without Scottish diagonal,"Let $n \ge 1$ be an integer , and let $A_n$ be the matrix in $M_{2n-1}\mathbb(F)$ with entries $(a_{ij})$ where $a_{ij}=1 $ if $i+j=2n$  or $i=n$ or $j=n,$ and $a_{ij}=0$ otherwise . Find det$(A_n)$ my idea: for $n=4$ the matrix of the form from the given data $$\begin{bmatrix}
 &  &  & 1 &  &  &1 \\ 
 &  &  &  1&  &  1& \\ 
 &  &  &  1& 1 &  & \\ 
 1&1 & 1 & 1 & 1 &1  &1 \\ 
 &  &  1&  1&  &  & \\ 
 &  1&  &  1&  &  & \\ 
 1&  &  &1  &  &  & 
\end{bmatrix}$$ How to find genearl formula for determent :","['matrices', 'linear-algebra', 'determinant']"
2670428,The D module pushforward to a point is de Rham cohomology?,"I have read and understood the definition of the pushforward of a $\mathcal{D}_X$-module $\mathcal{M}$ under a morphism of varieries $$f:X\longrightarrow Y$$
In particular, if $Y=\{ \star \}$ is a point and $\mathcal{M}=\mathcal{O}_X$ then
$$f_* \mathcal{O}_X= \ \ \ \left( H^n(X)  \to \cdots \to H^0(X) \right)$$
is the de Rham cohomology complex of X. However, I don't know why this should be true. Can anyone explain why, before reading the definition of a pushforward, one would guess this as the only possible reasonable answer? (Maybe part of the problem is that I've not got a good picture of what this process is meant to be doing geometrically).","['d-modules', 'algebraic-geometry']"
2670467,Control theory - feedback control of a damped oscillator to stabilise velocity,"I am looking at applying some simple control theory to a damped oscillator. If I have the following dynamics \begin{equation}
  \begin{bmatrix}
    \dot{x}\\
    \ddot{x} \\
  \end{bmatrix}
  =
  \begin{bmatrix}
    0 & 1 \\
    -\omega^2 & -\Gamma
  \end{bmatrix}
  \begin{bmatrix}
    x\\
    \dot{x} \\
  \end{bmatrix}
+
  \begin{bmatrix}
    0\\
    \dfrac{1}{m} \\
  \end{bmatrix}
  u
\end{equation} Such that my $A$ matrix is $\begin{bmatrix}
    0 & 1 \\
    -\omega^2 & -\Gamma
  \end{bmatrix}$ and by B matrix is $  \begin{bmatrix}
    0\\
    \dfrac{1}{m} \\
  \end{bmatrix}$ and my control is $u$, an external force on the oscillator. I can calculate the controllability matrix \begin{equation}
  \mathcal{C} =
  \begin{bmatrix}
    0 & \dfrac{1}{m} \\
    \dfrac{1}{m} & -\dfrac{\Gamma}{m} \\
  \end{bmatrix}
  \label{controllability_matrix}
\end{equation} which has rank 2. This means the controllability matrix has full column rank, this means, as I understand it, that this system is controllable. This means that we can arbitrarily place the
eigenvalues (also sometimes called poles) of the system dynamics by tuning $\mathbf{K}$ in $u =
-\mathbf{K}\vec{x}$ because the system dyamics becomes $\dot{\vec{x}} = (\mathbf{A} -
\mathbf{B}\mathbf{K})\vec{x}$. This also means we have reachability, meaning we can drive the system
to any state, the reachable set of states $R_t = \left\{ \xi ~\epsilon ~\mathbb{R}^n \right\}$. If I then plug in $u = -K\vec{x}$ where I change $\vec{x}$ to $\vec{x}-\vec{x_t}$ where $\vec{x_t}$ is my target state I want to set the system to be driven towards. \begin{equation}
  \begin{bmatrix}
    \dot{x}\\
    \ddot{x} \\
  \end{bmatrix}
  =
  \begin{bmatrix}
    0 & 1 \\
    -\omega^2 & -\Gamma
  \end{bmatrix}
  \begin{bmatrix}
    x\\
    \dot{x} \\
  \end{bmatrix}
  -
  \begin{bmatrix}
    0\\
    \dfrac{1}{m} \\
  \end{bmatrix}
  \begin{bmatrix}
    K_0 & K_1 \\
  \end{bmatrix}
  \begin{bmatrix}
    x - x_t \\
    \dot{x} - \dot{x}_t \\
  \end{bmatrix}
\end{equation}
which results in \begin{equation}
  \begin{bmatrix}
    \dot{x}\\
    \ddot{x} \\
  \end{bmatrix}
  =
  \begin{bmatrix}
    0 & 1 \\
    -\omega^2 & -\Gamma
  \end{bmatrix}
  \begin{bmatrix}
    x\\
    \dot{x} \\
  \end{bmatrix}
  -
  \begin{bmatrix}
    0\\
    \dfrac{1}{m}K_0(x-x_t) + \dfrac{1}{m}K_1(\dot{x}-\dot{x}_t) \\
  \end{bmatrix}
\end{equation} However when I put in some values for $\omega$, $\Gamma$ and $m$ and calculate the K matrix by setting the eigenvalues to be $n\times eig(A)$ [where n>1, the larger n is than 1 the more aggressive the feedback, I've used values like 1.5, 2, 3 ... etc ](this was just an initial guess - I wasn't sure where to place the eigenvalues to start - other than that they want to have a negative real value for stability and the more negative they are the more aggressive the feedback) by using K = place(A, B, eigs(A)*n) in matlab then I get a K matrix where $K_1$ is 0, and therefore I cannot control $\dot{x}$, why is this and how can I control $\dot{x}$? I've been able to simulate this the see that it can control $x$. Also, is it possible to set the system to be driven to any state by this control? It doesn't make sense that the system could be prepared in a state such as $x = 5cm$, $\dot{x} = 5m/s$ stably for example, as the positive velocity means it won't stay at $x = 5cm$. How can I calculate what states are reachable and stable?","['matrix-equations', 'optimization', 'control-theory', 'linear-control', 'ordinary-differential-equations']"
2670475,how to prove that $ {P^{'}}$ has no real root?,"Let $P(z)$  be a monic polynomial with complex coefficients with all
  roots distinct and in $\{z \in C : \Im(z) \lt 0\}$. $(a)$ Prove that the sum of all the residues of $\frac{P^{'}}{P}$
  is the degree of the polynomial $P$. $(b)$ Prove that  $ P^{'}$
   has no real root. My idea  was that  option $(a)$  as I take $f(z)$=$\frac{p^{'}(z)}{p(z)}$ $\deg(p(z))\ge \deg(p^{'}(z))+2$ Residue theorem: If $f$ is analytic in a domain except for isolated singularities at $a_1,\dots a_k$ then for any closed  contour $\gamma\in D$ on which none of the points $a_k$ lie, we have $$\frac{1}{2\pi i}\int_{\gamma}f(z)dz=\sum_{1}^{k}n(\gamma;a_k)Res[f(z);a_k].$$ Here I don't know  how to proceed further. For option $(b)$ if I  take even polynomial degree that $p(x) =x^2+1$ then it will not have real roots As I don't know the actual proof. Please assist and help me. Thanks in advance for helping.",['complex-analysis']
2670505,"zero-mean RV $𝑿$ with finite fourth moment, probability of being positive [closed]","Closed. This question is off-topic . It is not currently accepting answers. This question does not appear to be about math within the scope defined in the help center . Closed 6 years ago . Improve this question For zero-mean RV $X$ with finite fourth moment, prove that 
$$
P(X>0)\ge \frac{\mathbb{E}(X^2)^2}{4\mathbb{E}(X^4)}
$$ I tried Chebyshev with adding $t$ to both sides, but I could not get forth moment.","['inequality', 'probability', 'expectation', 'upper-lower-bounds']"
2670514,What is the use of the disjoint union in the definition of the sheaf of rings on $\mathrm{Spec}(A)$?,"Hartshorne on pg. 70 defines a sheaf $\mathcal O$ of rings on $\mathrm{Spec}(A)$ to be the set of functions $$s : U \to \bigsqcup_{\mathfrak{p} \in U} A_{\mathfrak{p}},$$
where for each $\mathfrak{p}$, $s(\mathfrak{p})\in A_{\mathfrak{p}}$ and for any $\mathfrak{p}\in U$, there is a neighborhood $V$ of $\mathfrak{p}$ in  $U$ and elements $a, f \in A$ such that for all $\mathfrak{q}$ in $V$, $f \notin \mathfrak{q}$ and $s(\mathfrak{q})=\frac af \in A_{\mathfrak{q}}$. Question: What is the motivation for including a disjoint union of the localizations of the ring $A$? Why isn't it a simple union?",['algebraic-geometry']
2670553,Does $A\in B$ imply $A\subset B$?,"Question: Does $A\in B$ imply $A\subset B$ and does $A\in B$ and $B\in C$ imply $A\in C$? I've been trying to find examples to get some intuition for this and I've come up with the following: Example 1: Suppose that $A = \{1\}$, $B = \{\{1\},2\}$. I'd say that $A$ is an element of $B$ and $A$ is a subset of $B$. Example 2: Suppose that $A = \{1\}$, $B = \{\{1\},2\}$ and $C = \{\{\{1\},2\},3\}$. Now $A\in B$, $B\in C$ but $A\not\in C$, right? I think my confusion stems from the fact that I'm not sure how $B = \{\{1\},2\}$ vs $B = \{1,2\}$ determines whether $A$ is an element and/or a subset of $B$.",['elementary-set-theory']
2670564,Ramification index of an ideal vs. ramification index of a point,"I am familiar with two uses of the term ""Ramification index"": 1. The ramification index in the context of algebraic number fields, i.e. the exponent of a prime ideal in the decomposition of a prime ideal it lies over, and 
2. The ramification index in the context of Riemann surfaces and algebraic curves, spacifically in the context of the  Riemann-Hurwitz formula. Is there a connection between the two?","['riemannian-geometry', 'algebraic-number-theory', 'algebraic-geometry']"
2670572,Fixed point problem dealing with a trivial homomorphism between fundamental groups,"I'm preparing for an exam by looking at an old exam archive. I encountered this problem which I believe should be solvable using only material from James Munkres' Topology or possibly John Lee's Introduction to Smooth Manifolds as that is how these exams are designed: ""Let $\mathbb{R}\mathbb{P}^n$ denote the $n$-dimensional projective space where $n$ is an odd integer. Suppose that a continuous map $f:\mathbb{R}\mathbb{P}^n \to \mathbb{R}\mathbb{P}^n$ induces the trivial homomorphism $$f_*:\pi_1(\mathbb{R}\mathbb{P}^n,x_0) \to \pi_1(\mathbb{R}\mathbb{P}^n,f(x_0)).$$ Show that $f$ has a fixed point."" I was thinking that perhaps $n$ being odd is important mainly because $\mathbb{R}\mathbb{P}^n$ is orientable in those cases. My attempt so far: lift $f$ to a map $\hat{f}:\mathbb{R}\mathbb{P}^n \to S^n$ which is possible since $f$ is trivial and so $f_*(\pi_1(\mathbb{R}\mathbb{P}^n)) \subset p_*(\pi_1(S^n))$ where $p:S^n \to \mathbb{R}\mathbb{P}^n$ is the usual quotient map. From there, I was considering $\hat{f} \circ p: S^n \to S^n$ and trying to say something about the degree of the map or using something like Brouwer Fixed Point Theorem (first extend $f$ because it is nullhomotopic). So far, some friends have suggested using suspensions (as in Hatcher) or the Lefschetz fixed point theorem but I think these are too advanced and wouldn't score points on the exam. Any hints would be a huge help; thank you. Edit: I've written up a possible solution below; let me know what you think.","['algebraic-topology', 'general-topology']"
2670577,Is there a semi-norm that respects matrix similitary?,"It is not possible to have a norm on $M_n(\mathbb C)$ that respects similitary for $n > 1$ since, for example, if $A \sim 2A$ and $A \neq 0_n$ then $N(A) = 2N(A)$ contradicts the separating property. For example, $A$ s.t. $A_{1,2} = 1$ and with $0$ elsewhere. So the question is: Is there a semi-norm (i.e. without the separating property) on $M_n(\mathbb C)$ that respects matrix similitary?","['matrices', 'normed-spaces', 'linear-algebra', 'vector-spaces']"
2670592,"Evaluating $J_0(z)=\frac{1}{\pi}\int_{0}^{\pi}\cos\left(z\sin\theta\right)\,d\theta$","I was fascinated by a question I asked before, in which the following equation suddenly appeared. $$J_0(z)=\frac{1}{\pi}\int_{0}^{\pi}\cos\left(z\sin\theta\right)\,d\theta$$ My question was can this be proved? The anwser was yes. I started with defining: $$\cos(z) = 1-\frac {z^2}{2!} + \frac{z^4}{4!} ...$$ Further I know that: $$\sin(\theta)=\frac{e^{i\theta}-e^{-i\theta}}{2i}$$ And by definition:
$$J_0(t)=\sum_{n=0}^{\infty}\frac{(-1)^n t^{2n}}{(n!)^2 2^{2n}}$$ I think I am close but just some missing links. Anyone that can help?","['special-functions', 'complex-analysis', 'bessel-functions']"
2670693,derived category of quotient category,"If $\mathcal B$ is a Serre subcategory of an abelian category $\mathcal A$,then we have a new abelian category $\mathcal A/B$ and an exact functor $q:\mathcal A \rightarrow \mathcal A/\mathcal B$. Since $q$ is exact,$q$ maps quasi-isomorphism to quasi-isomorphism.
So $q$ induces an exact functor $Dq:D(\mathcal A)\rightarrow D(\mathcal A/\mathcal B)$. Consider $D(\mathcal B)$ as triangulated subcategory of $D(\mathcal A)$.(That is:the object of $D(\mathcal B)$ is isomorphic to a object in $D(\mathcal A)$ I have the following two question: 1.Is $D(\mathcal B)$ thick subcategory of $D(\mathcal A ）$？ 2.there is a natural induced  exact functor $q^-：D(\mathcal A)/D(\mathcal B)\rightarrow D(\mathcal A/\mathcal B)$.Is $q^-$ an equivalence? We can also consider the induced exact functor $Kq:K(\mathcal A)\rightarrow K(\mathcal A/B)$.Is the same two questions true for this case? Thank you in advance!","['derived-categories', 'homological-algebra', 'algebraic-geometry']"
2670713,Translating Set-Builder Notation to Predicate Logic,"I am struggling to understand how one converts a set written using the set-builder notation to elementary predicate logic. Just for you to have an idea of my background, I studied engineering in college, and I used ""high-level"" maths (high level in the informatics sense, i.e. without digging to deep in the meaning of what I was writing) and I decided to start learning about ZFC for its own sake. I have no trouble understanding and manipulating sets written using predicate logic, but whenever a set-builder notation pops, I become lost because I am unable to go back to pure logic. For simple cases such as $S = \{x|x\in A\}$, I clearly understand that it can be translated to ${\forall x(x \in S \leftrightarrow x \in A)}$. I really started to realize that I did not understood this notation when solving an exercise involving $\{dom R | R \in A\}$. The mistake I made was that I was translating $x\in\{dom R | R \in A\}$ by $\exists y (x,y)\in R$ which is obviously wrong since $x$ should be a domain and not an element of a domain. Although I am not sure of this, I would expect the solution to be something like $x=dom R \wedge R \in A$. After this long introduction, my question is pretty simple: how do I turn ANY set written using the set-builder notation, even a complicated one, to a predicate logic formula? Thanks in advance!","['predicate-logic', 'logic', 'elementary-set-theory']"
2670728,Finding the Right Initial Conditions for a Three-Body Problem Periodic Solution,"The planar restricted circular three-body problem is about computing the trajectory of a small mass, usually a speccraft, that is affected by two larger masses, Earth and the Moon. The differential equation for this problem is given as $$ 
y_1'' = y_1 + 2y_2' - \mu_2 \frac{y_1+\mu_1}{D_1} - \mu_1 \frac{y_1-\mu_2}{D_2}
$$ $$ 
y_2'' = y_2 - 2y_1' - \mu_2 \frac{y_2}{D_1} -\mu_1 \frac{y_2}{D_2}
$$ where $$\mu_1 = \frac{m_1}{m_1+m_2}$$ $$\mu_2 = 1-\mu_1$$ $$ D_1 = ((y_1+\mu_1)^2 + y_2^2 )^{3/2}$$ $$ D_1 = ((y_1-\mu_2)^2 + y_2^2 )^{3/2}$$ For these equations for a certain initial conditions a so-called Arenstorf orbit can be computed which is periodic. The initial conditions are given as (for example in Solving Ordinary Differential Equations I, Nonstiff Problems , pg. 130), $y_1(0) = 0.994$, $y_1'(0)=0$, $y_2'(0) = -2.0015851063790825224053786222$ Every textbook, lecture note repeats these values, which produces, after integration, a nice orbit around earth and moon. But noone seems to know where they come from. Were they computed analytically? Or a trial and error approach was used to find them, or a combination of both? If someone has code that can compute these values it would be much appreciated. Thanks,","['classical-mechanics', 'numerical-methods', 'ordinary-differential-equations', 'calculus']"
2670754,Using summation by parts to evaluate an alternating sum,"I want to evaluate
$$
 \sum_{k=0}^n (-1)^k \binom{n}{k} k.
$$
I tried summation by parts , i.e. the formula
$$
 \sum_{k=0}^n (f(k+1) - f(k))g(k)
  = f(n+1)g(n+1) - f(0)g(0) - \sum_{k=0}^n f(k+1) (g(k+1) - g(k))
$$
with $f(k+1) - f(k) = (-1)^k \binom{n}{k}$ and $g(k) = k$. As
$$
 (-1)^{k+1}\binom{n-1}{k+1} - (-1)^k \binom{n-1}{k}
 = (-1)^{k+1} \left( \binom{n-1}{k+1} + \binom{n-1}{k} \right)
 = (-1)^{k+1} \binom{n}{k+1} 
$$
we have $f(k) = (-1)^k \binom{n-1}{k}$. Plugging into the formula
\begin{align*}
 \sum_{k=0}^n (-1)^k \binom{n}{k} k
  & = (-1)^{n+1} \binom{n-1}{n+1} (n+1) - (-1)^0 \binom{n-1}{0}\cdot 0
    - \sum_{k=0}^n (-1)^{k+1} \binom{n-1}{k+1} \\
  & = \sum_{k=0}^n (-1)^{k} \binom{n-1}{k+1}.
\end{align*}
But for example if $n = 3$ then
$$
 \sum_{k=0}^n (-1)^k \binom{n}{k} k = -3 + 6 -3 = 0
$$
but
$$
 \sum_{k=0}^n (-1)^{k} \binom{n-1}{k+1} 
  = \binom{2}{1} - \binom{2}{2}
  = 1
$$
which is not equal, but I cannot see whats wrong with the above derivation??","['combinatorics', 'summation', 'proof-verification', 'discrete-mathematics']"
2670781,Can a parallelogram have whole-number lengths for all four sides and both diagonals?,"Is it possible for a parallelogram to have whole-number lengths for all four sides and both diagonals? One idea I had was to arrange four identical right triangles such that the right angles are adjacent. For example if we take four triangles that are all 3-4-5 right triangles, and arrange them so that their legs form a cross with two arms that are 3 units and two arms that are 4 units, their hypotenuses will form a parallelogram in which all sides are 5 units – that is, a rhombus. A second idea was to take two pairs of isosceles triangles with all legs the same length, where one vertex angle is supplementary to another, and arrange them so that their vertex angles are adjacent. For example we could take two triangles with two sides of 6 units that form a 25° angle, and another two triangles with two sides of 6 units that form a 155° angle, and arrange them to form a quadrilateral with 12-unit diagonals – that is, a rectangle. That's all I can imagine. My hunch is that a parallelogram can have whole-number lengths for all four sides and both diagonals only if it is either a rhombus or a rectangle. Is that right? If so, can this limitation be explained elegantly?","['diophantine-equations', 'elementary-number-theory', 'polygons', 'geometry']"
2670804,Question about Method of Undetermined Coefficients?,"Here's the problem: 
Consider the DE $y''−5y'+6y=e^t\cos(2t)+e^{2t}(3t+4)\sin(t).$
Determine a suitable form $Y (t)$ if the method of undetermined
coefficient is to be used. So I understand that I'm supposed to come up with a ""guess"" for $Y(t)$ which contains an undetermined coefficient. For instance, if it were simply $y''−5y'+6y=e^t,$ I would begin with $Y(t)=Ae^t.$ Then I would take the derivative of $Y(t)$ twice and plug that back in to the original DE. I also realize that when $\cos/\sin$ are involved, you need to include both (like if it were $y''−5y'+6y=\cos(2t),\; Y(t)$ would be $A\cos(2t)+B\sin(2t)$). But since they're already both in $g(t),$ I don't need to do that, right? Right now my initial guess for $Y(t)$ is $Ae^t\cos(2t)+Be^{2t}(3t+4)\sin(t)$... Is this correct?",['ordinary-differential-equations']
2670844,$X^2Y^2-Z^2(X^2+Y^2)=0$ is rational,"I'm trying to verify whether or not the curve $X^2Y^2-Z^2(X^2+Y^2)=0$ is rational. I've verified that the only singularities are $(0:0:1),(0:1:0), (1:0:0)$, which are all ordinary with multiplicity $2$, so we can use the formula $g=\frac{(d-1)(d-2)}{2}-\sum_{P\in C}\frac{m_P(m_P-1)}{2}$. Then:
$$g =\frac{(4-1)(4-2)}{2}-3\frac{(2)(2-1)}{2}=3-3=0$$ (hopefully this is correct) So we've concluded the curve is rational. But I can't see any obvious way to find a birrational map between the curve and $\mathbb{P}^1$. Is there some kind of heuristic to find this map?","['algebraic-curves', 'singularity', 'projective-geometry', 'algebraic-geometry']"
2670929,Find a simple sequence that is asymptotically equivalent to $u_n = \sum_{k=n+1}^{+\infty} \frac{\ln{k}}{k^2}$,"Given that $$\int_x^{+\infty} \frac{\ln t}{t^2}\ dt = \frac{\ln x+1}{x} \sim \frac{\ln x}{x}$$ I believe that $$u_n= \sum_{k=n+1}^{+\infty} \frac{\ln{k}}{k^2}\sim\frac{\ln n}{n}$$ as $n \to +\infty$. Numerical evidence suggests that this is the case. Using Riemann sums on the function $f(t) = \ln t\ /\ t^2$ on the interval $[1/n,1]$ I was able to show that $u_n =O(n/\ln{n})$ and $n/\ln{n} = O(u_n)$, but I couldn't show that the two sequences are equivalent.","['sequences-and-series', 'riemann-sum']"
2670942,Modified Gram Schmidt,How does the Modified Gram Schmidt works? I want to use it but I am confused by the notations and I could not find any example online.,"['matrices', 'orthonormal', 'linear-algebra', 'gram-schmidt']"
2670944,Interior of the boundary of an open set is empty [duplicate],"This question already has answers here : Prove that the Interior of the Boundary is Empty (3 answers) Closed 6 years ago . Let $(X, \mathcal{T})$ be a topological space. Suppose $A \in \mathcal{T}$. We define the interior of $A$, the closure of $A$ and the boundary of $A$ as follows: $\operatorname{int}A=\lbrace x \in A\mid A \in \gamma(x) \rbrace$, where $\gamma(x)$ denotes the neighborhood filter of $x$. $\operatorname{cl}A=\lbrace x \in X\mid \forall V \in \gamma(x): V \cap A \neq \emptyset \rbrace$ $\partial A= \operatorname{cl}A \cap \operatorname{cl}(X\setminus A)$ How do you prove that $\operatorname{int}(\partial A) = \emptyset$? I already have that $\operatorname{int}(\partial A)=\operatorname{int}(\operatorname{cl}A \setminus A)$, but how do you continue from this point?",['general-topology']
2670955,"Pointwise limit of continuous functions, but not Riemann integrable.","I am trying to find a simple example of a function $f:[0,1]\rightarrow\mathbb{R}$ which is a pointwise limit of continuous functions, but is not Riemann integrable. I know the classical example where we build some functions $F_1,F_2,\dots$ on a cantor-like set, and then define $f_n = F_1.F_2.\dots . F_n$, and so on. But I was thinking whether there is a simpler example, one that you could present to students with no experience in Measure Theory. Any help is welcome.","['pointwise-convergence', 'integration', 'sequences-and-series']"
2671021,Calculating a multivariable limit.,"Find if it exists the limit : $$\lim_{(x,y)\to(0,0)}\frac{x^4y^4}{(x^2+y^4)^3}$$ I've tried the following : 1st attempt : Using polar coordinates: Set $x= r\cos\theta$ and $y=r\sin\theta$ $$\frac{x^4y^4}{(x^2+y^4)^3}=\frac{r^8\cos^4\theta \sin^4\theta}{(r^2\cos^2\theta+r^4\sin^4\theta)^3}=\frac{r^2\cos^4\theta\sin^4\theta}{(\cos^2\theta+r^2\sin^4\theta)^3}$$ $$\lim_{r\to0}\frac{r^2\cos^4\theta\sin^4\theta}{(\cos^2\theta+r^2\sin^4\theta)^3}=\frac{0}{\cos^6\theta}$$ Now the limit mentioned above would be equal to zero if and only if the denominator is different than zero. Hence we need to calculate the limit in the case where $\theta = \frac{\pi}{2}+k\pi$ and compare it with the precalculated limit.
However I was not able to get rid of the indeterminate form. 2nd attempt : Choosing a specific path $y = ax$. $$\frac{x^4y^4}{(x^2+y^4)^3}=\frac{a^4x^2}{(1+a^4x^2)^3}$$ $$\lim_{x\to0}\frac{a^4x^2}{(1+a^4x^2)^3}=0.$$ No conclusion about the limit. If anyone could give me hints or point me in the right direction I would be grateful. Thanks in advance.","['multivariable-calculus', 'polar-coordinates', 'calculus', 'limits']"
2671036,Wronskian: How to prove that $W'(t)=\text{tr}(A) W(t)$,"I have this system of differential equations:
$$
X'(t)= A(t) X(t),
$$ where $A$ is an $n\times n$ matrix of continuous functions. We know that if $\{X_1,\ldots, X_n\}$ is a system of solutions, then the Wronskian is  $$W(t)=\det( X_1(t),\ldots,X_n(t)).$$ 
How do we prove that $$W'(t)=\text{tr}(A)\, W(t)?$$ Thank you.","['real-analysis', 'ordinary-differential-equations']"
2671083,Show that $f(x)=x^{5/3}-kx^{4/3}+k^2x$ is increasing for $k\neq0$,"So to show the function is increasing/decreasing we differentiate and show it is more than zero/less than zero:
We have $$f(x)=x^{5/3}-kx^{4/3}+k^2x$$ Hence, $$f'(x)=\frac{5}{3}x^{2/3}-\frac{4k}{3}x^{1/3}+k^2$$ But how do I show $$\frac{5}{3}x^{\frac{2}{3}}-\frac{4k}{3}x^{\frac{1}{3}}+k^2>0$$","['derivatives', 'monotone-functions', 'calculus']"
2671111,All roots $\lambda$ of $\det(A-\lambda B)=0$ are $\ge1$ when $B$ is p.d and $A-B$ is n.n.d.,"I am trying to prove the following statement: Let $A,B\in M(n,\mathbb{R})$ . If $B$ is positive definite and $(A-B)$ is non-negative definite, then $\det(A-\lambda B)=0$ has all its roots $\lambda\geqslant1$ and conversely, if all roots $\lambda\geqslant 1$ , then $(A-B)$ is non-negative definite. If $(A-B)$ is n.n.d and $B$ is p.d, then I have $x^\top(A-B)x\geqslant0$ for all $x\in\mathbb{R}^n$ . This implies $x^\top Ax\geqslant x^\top Bx>0$ for all $x\ne0$ , so that $A$ is p.d. Moreover, as $B$ is p.d, $B$ is nonsingular. So, $\det(A-\lambda B)=0\implies\det((AB^{-1}-\lambda I)B)=0$ $\qquad\qquad\qquad\qquad\quad\implies\det(AB^{-1}-\lambda I)=0$ , as $\det(B)\ne0$ Thus $\lambda$ is an eigenvalue of the matrix $AB^{-1}$ . Now for the eigenvector $x\ne0$ corresponding to $\lambda$ we have, $(AB^{-1})x=\lambda x\implies(AB^{-1}-I)x=(\lambda-1)x$ . If I can show that $AB^{-1}-I$ is n.n.d given that both $A$ and $B$ are p.d, then that would possibly imply $\lambda-1\geqslant0$ and I am done. But I am not sure if this is true or not. In a different approach using the fact that a p.d matrix can be expressed as $D^\top D$ for some nonsingular matrix $D$ , I was able to show that $AB^{-1}-I=PQ$ for some n.n.d matrix $P$ and p.d matrix $Q$ . Does that help me conclude that $AB^{-1}-I$ is indeed n.n.d? Any simpler or alternate approach is welcome.","['matrices', 'eigenvalues-eigenvectors', 'quadratic-forms', 'linear-algebra']"
2671120,Extending foliations of submanifolds,"Take a manifold $M \cong \Sigma \times \mathbb{R}$, in which there are two submanifolds (of the same dimensions), $S_1$ and $S_2$, such that $S_i \cong \sigma_i \times \mathbb R$, and both are disjoint. Each of these submanifolds has a foliation $\sigma_{i}(t)$. Does there exist a foliation of $M$, $\Sigma(t)$, such that it is compatible with both? That is, for $t \in \mathbb R$, we have $\sigma_i(t) \subset \Sigma(t)$. If this is not true, what are possible conditions for this to be true? It's obviously true in some cases at least (when the submanifolds inherit the foliation from the manifold). I suspect that for instance taking two copies of $(0,1) \times \mathbb R$ in $\mathbb R^2$ along the axis $y$ would not admit any such foliation if the first copy was foliated by lines according to their coordinates $y$ while the other was foliated by the reverse $-y$.","['manifolds', 'differential-geometry', 'foliations']"
2671135,A split random variable problem,"Let $X$ be a random variable with the density function: 
\begin{equation}
f_{X}(t) = \begin{cases} 
\dfrac{1}{2}, & 1 \leq t < 0 \\
\dfrac{1}{9}t, & 0 \leq t \leq 3 \\
0, & \text{else}
\end{cases}
\end{equation} and $Y$ a random variable defined as 
\begin{equation}
Y = \begin{cases}
X + 1, & \quad X \leq 0 \\
-X, & \quad 0 \leq X \leq \dfrac{1}{2}\\
2, & \quad \dfrac{1}{2} < X < 1\\
4 - X, & \quad X \geq 1
\end{cases}
\end{equation} How can I find the CDF of $Y$? I'm having a hard time since it is split this way.","['probability-theory', 'probability', 'transformation', 'random-variables']"
2671153,Show that: $\lim \limits_{n\to\infty}\frac{x_n-x_{n-1}}{n}=0 $,"Here is an exercise: Suppose that $\{x_n\}$ is a sequence such that $\lim \limits_{n\to\infty}(x_n-x_{n-2})=0$. 
Show that: $$\lim \limits_{n\to\infty}\frac{x_n-x_{n-1}}{n}=0 $$ Thanks.",['limits']
2671173,"Isometry between $C[a,b]$ and $C[0,1]$","I need to find isometry between two spaces of continuous functions $C[a,b]$ and $C[0,1]$ . That means to find function $ \phi\colon C[a,b] \longrightarrow C[0,1] $ which is bijection and $d_{\infty}(f,g)=d_{\infty}(\phi(f),\phi(g))$ . I know that there is bijection between $[a,b]$ and $[0,1]$ $y=(b-a)x +a$ . I have idea to every function $g(y) $ from $C[a,b]$ join function $g((b-a)x +a)$ from $C[0,1]$ . But I don't know how to prove that this is bijection. I would accept any other way of solving this.","['functional-analysis', 'metric-spaces', 'isometry']"
2671240,Do (closed) Riemannian manifolds admit anti-self-adjoint vector fields (locally or globally)?,"We have the following set up. Let $(M^n,g)$ be a (possibly closed) Riemannian manifold, then a vector field $X\in\mathcal{T}(M)$ is said to be anti-self-adjoint if for any $\phi,\eta\in\mathscr{C}_0^\infty(M)$, we have the identity 
$$
\int_M\phi(X\eta)\,\mathrm d\mu_g = -\int_M(X\phi)\eta\,\mathrm d\mu_g.
$$
Note that $\mathbb T^n$ admits a global tangent frame of anti-self-adjoint vector fields. But can we find any anti-self-adjoint vector fields in general, or at least locally (i.e. imposing that $\phi$ and $\eta$ vanish outside of some neighborhood)?","['riemannian-geometry', 'differential-geometry']"
2671259,Flawed proof that $\sum_{n=3}^\infty \frac{1}{n\ln{n}(\ln{(\ln{n})})}$ converges.,"My proof: If $\ n \ge 25$, then $$ \frac{1}{n\ln{n}(\ln{(\ln{n})})} \le \frac{1}{n(\ln{n})^{1.1}}$$. Since $$\sum_{n=2}^\infty \ \frac{1}{n(\ln{n})^{p}}$$
converges when $p \gt 1$. It follows that $$\sum_{n=3}^\infty \frac{1}{n\ln{n}(\ln{(\ln{n})})}$$ also converges. I know this series is supposed to diverge and therefore my proof is flawed by I can't see where exactly I'm going wrong. I think it's the first inequality that is the problem given that I obtained it using Mathematica and didn't actually proof it.","['real-analysis', 'sequences-and-series', 'convergence-divergence', 'limits']"
2671280,Compute $\frac{d}{dx(t)}\int_0^Tx(\tau)^TAx(\tau)d\tau$,"I need to compute: $$
\frac{d}{dx(t)}\int_0^Tx(\tau)^TAx(\tau)d\tau,
$$ where $t\in (0,T)$, $A\in\mathbb R^{n\times n}$ and $x\in\mathbb R^n$. Using Leibniz differentiation under an integral sign, I have: $$
\frac{d}{dx(t)}\int_0^Tx(\tau)^TAx(\tau)d\tau = \int_0^T\frac{dx(\tau)^TAx(\tau)}{dx(t)}d\tau,
$$ My course notes say this equals $Ax(t)$, which is almost as if: $$
\int_0^T\frac{dx(\tau)^TAx(\tau)}{dx(t)}d\tau = \int_0^TAx(\tau)\delta(\tau-t)d\tau
$$ But I am not sure whether this is true or if something else tells us that it is $=Ax(t)$. Thanks for helping!","['derivatives', 'functional-calculus', 'calculus', 'integration', 'calculus-of-variations']"
2671288,Prove $\sum\limits_{n=0}^\infty\frac{5^n(3^{5^{n+1}}-5\cdot3^{5^n}+4)}{(729)^{5^n}-(243)^{5^n}-5\cdot3^{5^n}+1}=\frac12$,"This problem is taken from Algerian Olympiad and asks to prove that $$\sum_{n=0}^{\infty} \dfrac{5^n(3^{5^{n+1}} -5\cdot3^{5^n} + 4)}{(729)^{5^n} - (243)^{5^n}-5\cdot3^{5^n}+1} = \frac 12.$$ Noticing that $729=3^6$ and $243 = 3^5$, I tried simplifying the general terms by setting $x=3^{5^n}$ but it seems to give no simplifications. Thanks in advance for any advice / ideas.","['summation', 'sequences-and-series']"
2671302,Prove that if $\mathcal P(A) \cup \mathcal P(B)= \mathcal P(A\cup B)$ then either $A \subseteq B$ or $B \subseteq A$. [duplicate],"This question already has answers here : Stuck with proof for $\forall A\forall B(\mathcal{P}(A)\cup\mathcal{P}(B)=\mathcal{P}(A\cup B)\rightarrow A\subseteq B \vee B\subseteq A)$ (3 answers) Closed 11 years ago . Prove that for any sets $A$ or $B$, if $\mathcal P(A) \cup \mathcal P(B)= \mathcal P(A\cup B)$ then either $A \subseteq B$ or $B \subseteq A$. ($\mathcal P$ is the power set.) I'm having trouble making any progress with this proof at all. I've assumed that $\mathcal P(A) \cup \mathcal P(B)= \mathcal P(A\cup B)$, and am trying to figure out some cases I can use to help me prove that either $A \subseteq B$ or $B \subseteq A$. The statement that $\mathcal P(A) \cup \mathcal P(B)= \mathcal P(A\cup B)$ seems to be somewhat useless though. I can't seem to make any inferences with it that yield any new information about any of the sets it applies to or elements therein. The only ""progress"" I seem to be able to make is that I can conclude that $A \subseteq A \cup B$, or that $B \subseteq A \cup B$, but I don't think this gives me anything I don't already know. I've tried going down the contradiction path as well but I haven't been able to find anything there either. I feel like I am missing something obvious here though...","['proof-writing', 'elementary-set-theory']"
2671307,Existence of smallest circle containing a polygon,"Given a convex polygon $P$, is there a way to find the smallest (meaning, with the smallest possible radius) circle containing $P$? It seems clear that it must exist and that it must go through (at least) one vertex of $P$. In my mind, it makes sense that it has to go through (at least) two vertices of $P$, but I haven't got a proof yet. Of course, if $P$ is cyclic, we already know what happens, but what about in any other case? Also… is it unique?","['computational-geometry', 'geometry']"
2671327,Pigeonhole Principle - Show two subsets have the same age,"Suppose that there are $10$ people at a party whose (integer) ages range from $0$ to $100$. Show that there are two distinct, but not necessarily disjoint, subset of people that have exactly the same total age. So my thoughts so far are that you could split the ages $0-100$ $(101$ numbers$)$ into $11$ groups (pigeons) and then categorize these groups into the $10$ people (holes). However this seems backwards and not sound to me. Is there a different way to think about this problem? Any help is appreciated.","['combinatorics', 'pigeonhole-principle']"
2671445,Proving the product rule for limits,"Proof: $\lim_{x \to a} f(x)g(x) = \lim_{x \to a} f(x) \lim_{x \to a} g(x)$ Let $L_1 = \lim_{x \to a} f(x)$ and $L_2 = \lim_{x \to a} g(x)$. We assume $L_1, L_2 \neq 0$ (for scenarios where $L_1$ or $L_2$ are $0$, the result is trivially true). Let $\epsilon > 0$. We need a $\delta$ such that $|f(x)g(x) - L_1L_2| < \epsilon$ whenever $0 < |x-a| < \delta$. We can rewrite $$\begin{align}|f(x)g(x) - L_1L_2| &= |(f(x)-L_1)(g(x)-L_2) + L_1(g(x)-L_2) + L_2(f(x)-L_1)| \\
&\leq |f(x)-L_1||g(x)-L_2| + |L_1||g(x)-L_2| + |L_2||f(x)-L_1| \\
 &< \epsilon\end{align}$$ by triangle inequality. Let $\delta_1 > 0$ such that $|f(x)-L_1| < \sqrt{\frac{\epsilon}{3}}$ whenever $0 < |x-a| < \delta_1$. Let $\delta_2 > 0$ such that $|g(x)-L_2| < \sqrt{\frac{\epsilon}{3}}$ whenever $0 < |x-a| < \delta_2$. Let $\delta_3 > 0$ such that $|g(x)-L_2| < \frac{\epsilon}{3|L_1|}$ whenever $0 < |x-a| < \delta_3$. Let $\delta_4 > 0$ such that $|f(x)-L_1| < \frac{\epsilon}{3|L_2|}$ whenever $0 < |x-a| < \delta_4$. Suppose that $0 < |x-a| < \delta$ where $\delta = \min(\delta_1, \delta_2, \delta_3, \delta_4)$. Then we have: $$\begin{align}|f(x)-L_1||g(x)-L_2| &+ |L_1||g(x)-L_2| + |L_2||f(x)-L_1| \\&< \sqrt{\frac{\epsilon}{3}} \cdot \sqrt{\frac{\epsilon}{3}} + |L_1|\frac{\epsilon}{3|L_1|} + |L_2|\frac{\epsilon}{3|L_2|} \\&= \epsilon \end{align}$$ Is this a correct proof?","['limits', 'proof-verification', 'calculus', 'epsilon-delta', 'proof-writing']"
2671490,$p$-groups in which the centralizers are normal,"Let $|G|=p^n$ and $p$ a prime and let $|G:C_G(x)|\leq p$ for all $x\in G$ . Prove: $(a)~~~~~C_G(x)\trianglelefteq G$ for all $x\in G;$ $(b)~~~~~G’\leq Z(G);$ ${\color{red}{{(c)~~~~~|G’|\leq p}.}}$ Edit: I’m now only confused about $\bf (c)$ . For $\bf(c)$ , the nice answers below are not complete. I was told that $\bf (c)$ has something to do with this paper ( or here Knoche,H.G.: Überden Frobeniusschen Klassenbegriff in nilpotenten Gruppen , Math. Z. 55 (1951), 71–83.
), but reading the whole paper would be difficult for me, since I’m no German speaker; more unfortunately, I have not even been able to figure out which part I need to read . How does that paper help? I’m quite a beginner in group-theory, and I would be grateful if I could receive your instruction.","['abstract-algebra', 'p-groups', 'group-theory']"
2671545,Find the general solution in implicit form $dy/dx =xy/(x-y)$,"I am trying to find the solution to the following differential equation in implicit form, and I seem not to be getting anywhere: $$\frac{dy}{dx} = \frac{xy}{x-y}$$ This is not separable, but I tried separating them anyway, such that on the $dx$ side there were only $x$ terms, and then I figured the $x$ terms of the $dy$ side I could hold constant since $x$ is not a function of $y$, but I realized I don't actually know what $y$ is so I can't say that for sure. Any ideas? EDIT: I've tried it on Wolfram and it won't/can't do it. EDIT: The actual problem I had to solve was: $$\frac{dy}{dx} = \frac{xy}{x^2-2y^2}$$ I thought this was of the same form as the equation I wrote above, and that If I knew how to solve that, I could solve this. I see however that they are actually quite different. Thank you for the responses.",['ordinary-differential-equations']
2671569,Strange Pattern in Decimal Expansion,I noticed something weird when I was fooling around with my calculator. I calculated several powers of $30$ of the form $30^{\left(\frac{10^n-1}{10^n}\right)}$ and I noticed a pattern in the fractional part: $$30^{\left(\frac{9999}{10000}\right)}=29.9897981429$$ $$30^{\left(\frac{99999}{100000}\right)}=29.9989796581$$ $$30^{\left(\frac{999999}{1000000}\right)}=29.9998979643$$ The rest of the powers just keep sticking a $9$ in the tenths place and shifting all the other digits down. Why is the decimal expansion for $30^{\left(\frac{10^n-1}{10^n}\right)}$ when $n\ge 5$ $29.9\cdots 989796$?,"['number-theory', 'exponentiation', 'elementary-number-theory']"
2671663,Derivative w.r.t orthogonal matrix,"Let $A$ be an orthogonal matrix with elements $a_{ij}$ so that $\sum_k a_{ik} a_{jk}  = \delta_{ij}$. I'd like to know what is $\frac{ \partial a_{ij} }{ \partial a_{kl}}$. If $A$ was a generic matrix (with no constraints on its elements) then the answer would be $\delta_{ik} \delta_{jl}$. However, now we have the quadratic constraint on the matrix. What is the answer in this case?","['matrices', 'orthogonal-matrices', 'matrix-calculus', 'derivatives']"
2671684,Is the power of a Holder continuous function still Holder continuous?,"Question: If $f:\mathbf R\to\mathbf R_+$ is a Holder continuous function with exponent $0<\alpha<1$, how about the function $f^p$ with $p>1$? Does it still possesses the Holder continuity? - If yes, then what's the Holder exponent of it? - If no, what does the counterexample look like? - Can we add some assumptions such that it is still Holder continuous? If $|f(x)^p-f(y)^p|$ can be dominated by $|f(x)-f(y)|^p$, then the answer is obviously affirmative. But then the question amount to compare $|a^p-b^p|$ with $|a-b|^p$ for $a,b\in\mathbf R_+, p>1$. Still I got no idea... And I'm not able to give a counterexample to myself... Any hints or comments will be appreciated. TIA...","['inequality', 'partial-differential-equations', 'calculus', 'functional-analysis', 'holder-spaces']"
2671686,P(Z+) is uncountable (Power set of Positive Integers) [duplicate],This question already has answers here : Proving Power Set of $\mathbb N$ is Uncountable (2 answers) Closed 6 years ago . I have been trying to prove that the Power set of Positive Integers is uncountable using : Is $i$ an element of the $i$-th subset But I can not seem to prove it.,"['cardinals', 'elementary-set-theory']"
2671688,Limit Supremum of Sets and Functions,"Let $f:X\longrightarrow Y$ be a function, $A,A_1,A_2$ subsets of $X$ and $B,B_1,B_2$ be subsets of $Y$1 Prove that $f(\lim\sup A_n)\subset \lim\sup f(A_n)$. $\textbf{Proof:}$ Assume $y\in f(\lim\sup A_n).$
Then $\exists x\in\lim\sup A_n$ such that $y=f(x)$ I know my ultimate goal here is to show that $y\in\lim\sup f(A_n)$ but im not sure how to go about proceeding to the next step. What does it mean for the existence of $x\in\lim\sup A_n$? Please help on proceeding to the next step.",['elementary-set-theory']
2671747,What is the use/significance of Farkas' lemma?,"I worked on an exercise to prove Farkas' lemma, which states that for $A \in \mathbb{R}^{m,n}$ and $b \in \mathbb{R}^n$ exactly one of the following is true: There exists $x \ge 0$ such that $Ax=b$. There exists $y$ such that $A^T y \ge 0$ and $y^T b < 0$. This is simply a trivial fact about the separation between two closed convex sets: $S_1 = \{ b \}$ and $S_2 = \{ Ax \mid x \ge 0 \}$. Given its simplicity, there must be some broader significance or application of this fact. Can anyone enlighten me as to what it is?","['linear-algebra', 'linear-programming', 'convex-analysis']"
2671753,Verifying $\sin 4θ=4\cos^3 θ \sin θ - 4\cos θ \sin^3θ$,"$$\sin 4θ=4\cos^3 θ \sin θ - 4\cos θ \sin^3θ.$$ Ηere is what I have so far
$$\sin 4θ = 2\sin 2θ \cos 2θ = 4\sin θ \cos θ \cos 2θ.$$ Not sure if this is the correct path I should take to solve this problem. I have been stuck hard for about an hour now.",['trigonometry']
2671782,How many solutions are possible for $\log_4 (x-1) = \log_2 (x-3)$?,"$$\log_4 (x-1) = \log_2 (x-3)$$ 
I can calculate value of $x$ by two methods, one gives single solution whereas other one results in two values of $x$. I am confused which one in true.","['algebra-precalculus', 'logarithms']"
2671819,What do Indeterminate Forms mean? [duplicate],"This question already has answers here : What really is an indeterminate form? (6 answers) Closed 4 years ago . I know they can be $\frac{0}{0}$ or $\frac{\pm\infty}{\pm\infty}$, but what do they mean in English? For example, $$\lim_{n\rightarrow \infty}A$$ In the context of limits, when the above example get a limit that is an indeterminate form. I assume it means that $A$ does not give enough information to ""determine"" a limit. Fine. But why is it that when we cancel out some terms out in $A$, *poof* now we have enough information to get a definite limit even though no extra information (i.e. additional expressions) have been added to $A$. Another example: $\lim_{x\rightarrow3}\frac{(x-3)(x+3)}{x-3}=\frac{0}{0}$, not enough information to determine the limit. $\lim_{x\rightarrow3}\frac{(x-3)(x+3)}{x-3}=\lim_{x\rightarrow3}(x+3)=6$, Hey! Now we have enough information to determine the limit! (I thought you said you didn't have enough information)",['limits']
2671854,"Given a sufficiently large integer $N$, is it true that there are more primes than perfect squares in $[1,N]$?","We know that the sum of the reciprocals of all prime numbers diverges (that is, $\displaystyle\sum_{p\text { is prime}}p^{-1}=\infty$) and the sum of the reciprocals of all perfect squares converges (that is, $\displaystyle\sum_{k=1}^{\infty}k^{-2}<\infty$). Does this result imply that the set of prime numbers is more ""dense"" than the set of perfect squares; that is, given a sufficiently large integer $N$, there are always more primes than perfect squares in the closed interval $[1,N]$? If so, how about the answer to the following generalised question: For a fixed positive number $\varepsilon$, does there exist a positive integer $N_{\varepsilon}$ such that if $N>N_{\varepsilon}$, there are always more primes than numbers of the form $k^{1+\varepsilon}$ (where $k$ is a positive number) in the closed interval $[1,N]$? My guess is that the answer appears to be yes, since $\displaystyle\sum_{p\text { is prime}}p^{-1}=\infty$ whilst $\displaystyle\sum_{k=1}^{\infty}k^{-1-\varepsilon}<\infty$.","['prime-numbers', 'number-theory', 'convergence-divergence', 'natural-numbers', 'sequences-and-series']"
2671909,Geometric Distribution: Tossing a Coin Question,"Suppose that $A$ tosses a coin which lands heads with probability $p_A$ and $B$ tosses a coin which lands heads with probability $p_B$ . They toss their coins simultaneously over and over again, in a competition to see who gets the first head. The one to get the first head is the winner, except that a draw results if they get their first heads together. Calculate P(A wins). Let $X_A$ be the # of tosses that it takes $A$ to get the first head, $X_A \ge 1$ . Let $X_B$ be the # of tosses that it takes $B$ to get the first head, $X_B \ge 1$ . $P(A $ wins $)$ $= P(X_A \lt X_B)$ $=\sum_{i=1}^{\infty}P(X_A<X_B|X_A=i)P(X_A=i)$ $=\sum_{i=1}^{\infty}(1-p_B)^i(1-p_A)^{i-1}p_A$ $=\frac{p_A}{1-p_A}\sum_{i=1}^{\infty}(1-p_B)^i(1-p_A)^i$ $=\frac{p_A}{1-p_A}\sum_{i=1}^{\infty}((1-p_B)(1-p_A))^i$ $=\frac{p_A}{1-p_A}\left(\sum_{i=0}^{\infty}((1-p_B)(1-p_A))^i - ((1-p_B)(1-p_A))^0\right)$ $=\frac{p_A}{1-p_A}\left(\frac{1}{1-(1-p_A)(1-p_B)}-1\right)$ Textbook Answer: $\frac{(1-p_A)p_B}{1−(1−p_A )(1−p_B )}$ They're not the same, I tested with $p_A = 0.2$ , $p_B=0.5$",['statistics']
2671911,Where should I start to solve this problem?,"If $$\lim_{x\to +\infty}\frac{f(x+1)}{f(x)}=2$$
then calculate
$$\lim_{x\to +\infty}\frac{f(x+14)-3147f(x+1)}{f(x+2)+f(x)}$$ Where should I start to solve this problem? This is not a homework. Only I need a hint.","['functions', 'calculus', 'limits']"
2671936,Prove the function $f(x)= \begin{cases}x^2 & x\in\mathbb{Q}\\-x^2 & else\end{cases}$ is differentiable at $x=0$,"Prove the function $f:\mathbb{R}\rightarrow\mathbb{R}$ defined by $f(x)= \begin{cases}x^2 & x\in\mathbb{Q}\\-x^2 & else\end{cases}$ is differentiable at $x=0$ and that $f'(0)=0$ . Hey everyone, this is a simple calculus problem I've encountered, but I don't really know how to prove this using the definition $lim_{x\to 0} \frac{f(x)-f(0)}{x-0}=f'(x)$ because of the cases. It is trivial that if $x\in\mathbb{Q}$ then $f'(x)=2x$ , else $f'(x)=-2x \Rightarrow$ both limits of these functions are zero when $x$ is approaching zero, but how do I formally prove the function is differentiable at $0$ ?
Thanks :)","['derivatives', 'real-analysis', 'calculus']"
2671952,Why the generalized derivatives defined? Why was it needed?,"While I was searching ""why the generalized derivatives defined?"", I saw the expression: ""An extension of the idea of a derivative to some classes of non-differentiable functions.""
on https://www.encyclopediaofmath.org/index.php/Generalized_derivative . But I need more knowledge about the emergence of the generalized derivative (or in the sense of Sobolev). Why this type derivatives defined? Why was it needed? Can somebody recommend a source to me?","['derivatives', 'weak-derivatives', 'sobolev-spaces']"
2671978,Comparing largest root of degree three polynomial,"Given $n\ge 12$, consider the three polynomials:
$$f(x)=x^2-\left(\frac{(n-4)^2}{n-3}+\frac{n-2}{3(n-3)}+\frac{4}{3}\right)x+\frac{4(n-4)^2}{3(n-3)},$$
$$g(x)=x^3-\left(2+\frac{(n-5)^2}{n-4}\right)x^2+\left(\frac{3}{4}+\frac{3(n-5)^2}{2(n-4)}\right)x-\frac{(n-5)^2}{4(n-4)},$$
$$h(x)=x^3-\left(2+\frac{(n-5)(n-4)}{n-3}\right)x^2+\left(\frac{3}{4}+\frac{(n-5)(n-4)}{n-3}\right)x-\frac{(n-5)(n-4)}{4(n-3)},$$ Suppose $f_\lambda, g_\lambda,h_\lambda$ are the largest root of $f(x),g(x),h(x)$ respectively. In order to prove one result I need to show that
 $f_\lambda >g_\lambda >h_\lambda.$ For some specific values I have verified it. Is it true for all $n\ge 12$ ? Partial Answer: As mathlove mentioned in the below answer, the result does not hold. But still I feel at least $f_{\lambda}>h_{\lambda}$ is true. Any idea for this one?","['real-analysis', 'inequality', 'polynomials', 'roots']"
2671984,How to calculate $ \int \frac{\sin^{6}(x)}{\sin^{6}(x) + \cos^{6}(x)} dx? $,"How to calculate 
$$ \int \frac{\sin^{6}(x)}{\sin^{6}(x) + \cos^{6}(x)} dx? $$ I already know one possible way, that is by :
$$ \int \frac{\sin^{6}(x)}{\sin^{6}(x) + \cos^{6}(x)} dx = \int 1 - \frac{\cos^{6}(x)}{\sin^{6}(x) + \cos^{6}(x)} dx $$
$$= x-
\int \frac{1}{1+\tan^{6}(x)} dx $$
Then letting $u=\tan(x)$, we must solve
$$\int \frac{1}{(1+u^{6})(1+u^{2})} du $$
We can reduce the denominator and solve it using Partial Fraction technique. This is quite tedious, I wonder if there is a better approach. Using same approach, for simpler problem, I get
$$\int \frac{\sin^{3}(x)}{\sin^{3}(x)+\cos^{3}(x)} dx = \frac{x}{2} - \frac{\ln(1+\tan(x))}{6} + \frac{\ln(\tan^{2}(x)- \tan(x)+1)}{3} - \frac{\ln(\sec(x))}{2} + C$$","['partial-fractions', 'integration', 'calculus']"
2672028,Construction of a star (triangle inequality),"I was trying out some problems from a Russian mathematics book when this question came up. Though it seems pretty obvious that this question is to be solved through triangle inequality, yet I am unable to find sufficient conditions to get the conditions right. As per the problem we need to prove that it is impossible to construct a star (like the one given in the picture) which satisfies: $$BC>AB,\ DE>CD,\ FG>EF,\ HI>GH,\ KA>IK.$$ P.S.: I tried to work out with some triangles like $\triangle BIF, \triangle BEH$ but unfortunately the equations seemed inconclusive.","['euclidean-geometry', 'geometry']"
2672128,Minimizing the size of a cylinder sliding in a tube,"Lets define a spine curve $\Gamma$ given in terms of its arc length parameter $s$, and a family of closed curves $\Omega = \Omega \left( s \right)$. Lets say $\Gamma$ has a finite length, from $s=0$ to $s=s_{end}$. By sliding $\Omega$ throughout $\Gamma$, a tubular region $T$ is generated. Lets assume that the sliding process is perfectly defined, for instance in the case where $\Omega$ is plane, it remains always perpendicular to $\Gamma$, and its centroid is allways on $\Gamma$. My first question is how to analytically describe $T$ in terms of $\Gamma$ and $\Omega$. Now the main question: lets define a cylinder $C$, with parallel bases and a fixed height $L$. The lateral surface of the cylindric is not fixed, nor the shape and size of the bases. The problem consists on sizing minimally this body (i.e. minimum volume), and provide its motion law. $C$ should be moved somehow throughout the tube $T$, previously defined. The condition is that $C$ should cover the whole transversal section of $T$ at each possition, i.e. for a specific $s=s^*$, the region of $T$ between $s=s^*$ and $s=s^*+L$ should be inside $C$. I would like to get some hints or references to face this problem. Both analytical and numerical approaches are welcome. I know the problem is quite generic, so I propose a simplified version below: Lets say $\Omega$ is actually a fixed closed curve instead of a family of curves. Lets also say that $\Omega$ is plane. $T$ is now generated by extruding $\Omega$ perpendicular to $\Gamma$, keeping its centroid always on the spine curve $\Gamma$. Lets say that $C$ is actually a right circular cylinder of unknown diameter $d$, and it will be moved keeping its centroid $G$ on $\Gamma$, and its cross section at $G$ perpendicular to $\Gamma$. Under these conditions the only free parameter of the problem is the diameter of the cylinder, $d$. Thus, the problem is to determine the minimum $d$ that satisfies the condition that $C$ covers the whole transversal section of $T$ at each possition. Any light shed on these problems would be very welcome :)","['optimization', 'numerical-optimization', 'geometry', 'calculus-of-variations', 'differential-geometry']"
2672176,Probability that someone has the disease,"The percentage of people that have a disease A is $0,01$. 
  We apply twice a test for that disease, each of which give the correct answer with probability $0,95$. 
  What is the probability that someone has that disease if at least one test is positive and what is the probability if both tests are positive? I have done the following: We have that $$P(\text{at least one positive})=1-P(\text{no positive})=1-P(NN)$$ where N: ""negative"". Let D:""has the disease"". Then we have that $$P(NN)=P(NN\cap D)+P(NN\cap D^C)=P(NN\mid D)\cdot P(D)+P(NN\mid D^C)\cdot P(D^C)$$ We have that $P(D)=0,01$ and $P(D^C)=0,99$. Does it hold that $P(NN\mid D)=P(N\mid D)^2$ and $P(NN\mid D^C)=P(N\mid D^C)^2$ ? If yes, does it hold that $P(N\mid D)=0,05$ and $P(N\mid D^C)=0,95$ ?","['probability-theory', 'probability']"
2672196,Coincidence of classical notion of covering in classical topology and covering sieve in grothendieck topology,"Let $X$ a topological space, $\mathcal{O}(X)$ the usual category of the open sets of $X$ and $(\mathcal{O}(X),J)$ any site. Do we necessarly have for each open set U and  for each sieve in $J(U)$, $(U_i \rightarrow U)_i$, the union $U \subseteq \bigcup U_i $ or can we have for a certain grothendieck topology the reverse in some cases (strict subset $\bigcup U_i \subsetneqq U) $ ? (and why of course if yes or no) Thanks (extremely basic question i know)","['category-theory', 'general-topology', 'topos-theory', 'grothendieck-topologies']"
2672288,"If $\cos2\theta=0$, then $\Delta^2=$?","I'll state the question from my textbook here: If $\cos2\theta=0$, then $\begin{vmatrix} 0 & \cos \theta & \sin \theta \\ \cos \theta & \sin \theta & 0 \\ \sin \theta & 0 & \cos \theta \end{vmatrix}^2=$? This is how I solved the problem: $\begin{vmatrix} 0 & \cos \theta & \sin \theta \\ \cos \theta & \sin \theta & 0 \\ \sin \theta & 0 & \cos \theta \end{vmatrix}^2$ $= (\cos^3 \theta + \sin^3 \theta)^2$ $= (\cos \theta + \sin \theta)^2(\cos^2 \theta - \cos \theta \sin \theta + \sin^2 \theta)^2$ $= (1+ \sin2\theta)(1-\sin2\theta + \sin^2 \theta \cos^2 \theta)$ $= (1+ \sin2\theta)(1-\sin2\theta) + (1 + \sin2\theta)\sin^2\theta \cos^2 \theta$ $= \cos^2 2\theta + \frac 14 (1 + \sin2\theta)\sin^2 2\theta$ $= \frac 14 (1 + \sin2\theta)\sin^2 2\theta$ Now since $\cos2\theta=0$, $\sin2\theta = \pm 1$. Therefore the above expression can take the values 0 and $\frac12$. My textbook gives the answer as $\frac12$. I don't see any grounds on rejecting the other answer of 0. Have I made a mistake somewhere? Or am I forgetting something?","['trigonometry', 'linear-algebra', 'proof-verification', 'determinant']"
2672360,Rubik's cube function,"I'm thinking in a function and if it's possible to solve that. I have been playing with the cube using the following move: $R U L' U'.$ I notice that the cube solves itself with a certain number of moves: 28 moves to $2\times2\times2$ and 112 to $3\times3\times3.$ (If the cube already is solved). Then I'm trying to create a formula to calculate the number of moves for the another cubes like $4\times4\times4, 5\times5\times5, 6\times6\times6\dots$ Since: $x \rightarrow y$ $1 \rightarrow 0$ $2 \rightarrow 28$ $3 \rightarrow 112$ $4 \rightarrow z$ Where x is the number of the cube $(2 = 2\times2\times2, 3 = 3\times3\times3 \dots)$ and y is the amount of moves, I came up with two formulas: 28*$((x-1)^{(x-1)})$ and 28*$(x-1)^2$ . Thus, the value for $z$ could be 252 or 756 , My questions are: Are any of these formulas correct? If so, which one? Can be my reasoning corret about the formulas? If I'm wrong, answer me why!","['rubiks-cube', 'functions']"
2672379,Notation of iterated composition of functions [duplicate],"This question already has answers here : Iterated self-composition of arbitrary function (2 answers) Closed 6 years ago . Let $f$ be a function from $A$ to $B$ such that the image $f(A)\subset A$. Is there a widely accepted notation for the expression
$$f\circ\left(f\circ\cdots(f\circ f)\right),$$
where $f$ composite with itself $n$ times? I failed to find a natural way to include the information $n$ in the notation.","['notation', 'functions']"
