question_id,title,body,tags
2327556,How to prove that $\binom{n}{2k+1}=\sum_{i=k+1}^{n-k}\binom{i-1}{k}\binom{n-i}{k}$?,"I've tried to prove that $\binom{n}{2k+1}=\sum_{i=k+1}^{n-k}\binom{i-1}{k}\binom{n-i}{k}$ by using combinatorial proof. The LHS is the number of binary vectors of size $n$ with $2k+1$ zeros. 
I can't find an explanation regarding the RHS.",['combinatorics']
2327606,"The integral $\int_0^1 (-x^2+x)^{k}\cdot \lfloor kx \rfloor \, dx$ (BMT 2017)","Find the integral $$\int_0^1 (-x^2+x)^{k}\cdot \lfloor kx \rfloor \, dx$$
where $k = 2017.$ My attempt: By splitting up the integral and then using floor properties, I was able to transform the integral into $$\sum_{n=0}^{k-1} n\int_{n/k}^{(n+1)/k} (-x^2+x)^k\,dx$$ However I was unable to find a better representation for the sum. I did notice that the integrand was quite similar to a Beta function; however, the limits are wrong and I believe that the beta function cannot be applied. Any suggestions?","['contest-math', 'integration', 'definite-integrals']"
2327635,Finding probability of picking a higher or lower number card from the deck given an even 13 sided (6 + 8 sided) dice is rolled,"I am designing a game in which a person rolls 2 dice (a 6 sided and a 8 sided). So there are 13 outcomes possible (2 to 14). A person has to then choose a card from the deck and guess whether it is higher or lower than the outcome on the dice. 
If he guesses it right than he gets a point or else he doesn't get a point. What will be the expected value (of points) and probability of winning the game?
Note: 
1) he doesn't get a point if outcome on dice = value of card
2) value of ace is 14",['combinatorics']
2327643,Non-singularity of a square matrix,"The diagonal elements of a square matrix $M$ are odd integers while the off diagonals are even integers. Then can we say that $M$ must be non-singular?
We can easily comment on the singularity of the matrix $M$. Let us take the identity matrix of order $2$ which is non-singular.So can I say the $M$ must be non-singular for every such case?
please help me to clear this idea.","['matrices', 'singularity']"
2327656,"Detail of a proof about about trace-zero functions in $W^{1,p}$, Thm 2 p.273 in L. Evans' PDE","(PDE, Lawrence Evans, second edition 2010, § 5.5  Theorem 2, p. 273-275) The little step I don't understand in the calculations does not require to state the theorem, but I simply copied it from this question about that same theorem Theorem (Trace-zero functions in $W^{1,p}$): Assume $U$ is bounded and $\partial U$ is $C^1$. Suppose furthermore that $u\in W^{1,p}$. Then $$u\in W^{1,p}_0(U)\quad \Longleftrightarrow\quad  Tu=0\text{ on }\partial U$$ The step I don't understand is the two equations between (8) and (9) p.274. For those who don't have access to the book, I'll rewrite with general notations: Let $\varphi \in C^1(\overline{\mathbb{R}}{}^n_+)$ where $\overline{\mathbb{R}}{}^n_+$ denotes the ""half-space"" with boundary, i.e. $\mathbf{x}:=(x', x_n)\in \overline{\mathbb{R}}{}^n_+\enspace \Leftrightarrow\enspace x' \in \mathbb{R}^{n-1},\ x_n \geq 0$. We have $$ \lvert \varphi(x', x_n) \rvert \leq \lvert \varphi(x', 0) \rvert + \int_0^{x_n}\lvert \varphi_{x_n}(x', t) \rvert\, dt $$
($\varphi_{x_n}$ means partial derivative w.r.t. $x_n$). That's ok. Now it seems that he raises everything to the power $p$ and then integrates w.r.t. $x'$ to get
$$ \int_{\mathbb{R}^{n-1}} \lvert \varphi(x', x_n) \rvert^p\, dx' \leq C \left( \int_{\mathbb{R}^{n-1}} \lvert \varphi(x', 0) \rvert^p\, dx' + x_n^{p-1}\int_0^{x_n}\int_{\mathbb{R}^{n-1}}  \lvert D\varphi(x', t) \rvert^p\, dx'\, dt \right)$$
where $\lvert D\varphi(x', t) \rvert$ is the norm of the gradient following the notations of the book. I naively imagined writing the binomial. Then I somehow need to find an upper bound and transform the product of integrals into an integral of a product (2nd term r.h.s.). Oh, writing things down already helped me see that L. Evans indeed already performed $p-1$ integrations $\int_0^{x_n} (\cdots )\, dt $, but I still don't know how the inequality is justified, i.e. what are the suitable constant $C$","['functional-analysis', 'multivariable-calculus', 'partial-differential-equations']"
2327657,Is $\left(\sum_{n=1}^N\frac{a_n}{N}\right)^N\left(\sum_{n=N+1}^{2N}\frac{a_n}{N}\right)^N≠\left(\sum_{n=1}^{2N}\frac{a_n}{2N}\right)^{2N}$?,"Let
$$G_N= \prod_{n=1}^Na_n$$
and 
$$A_N=\left(\frac{\sum_{n=1}^Na_n}{N}\right)$$
So
$$G_{2N}= \prod_{n=1}^{2N}a_n \\
=\left(\prod_{n=1}^{N}a_n\right)\left(\prod_{n=N+1}^{2N}a_n\right) \\
≤_{IH}\left(\sum_{n=1}^N\frac{a_n}{N}\right)^N\left(\sum_{n=N+1}^{2N}\frac{a_n}{N}\right)^N$$ I failed to understand why the following is true $$\left(\sum_{n=1}^N\frac{a_n}{N}\right)^N\left(\sum_{n=N+1}^{2N}\frac{a_n}{N}\right)^N$$ does it imply $$\left(\sum_{n=1}^{2N}\frac{a_n}{2N}\right)^{2N}$$ But I use N=10 as example with Mathematica $$\left(\sum_{n=1}^N\frac{a_n}{N}\right)^N\left(\sum_{n=N+1}^{2N}\frac{a_n}{N}\right)^N≠\left(\sum_{n=1}^{2N}\frac{a_n}{2N}\right)^{2N}$$ See the following image","['algebra-precalculus', 'summation']"
2327661,Show properties of the linear operator $L((a_n)_n)=\left(\frac{1}{n}*a_n\right)_n$,"Let $L\colon \ell^p \rightarrow \ell^p$ such that $L((a_n)_n)= \left(\frac{1}{n}*a_n\right)_n$. 1) Determine $L'$(adjoint operator), $\ker(L)$, $\ker(L')$, $\operatorname{rg}(L)$, $\operatorname{rg}(L')$ as well as $\operatorname{cl}(\operatorname{rg}(L))$ and $\operatorname{cl}(\operatorname{rg}(L'))$. 2) Let $K$ be a bounded linear operator. Using $L$, show that $\operatorname{cl}(\operatorname{rg}(K')) \subseteq \ker(K)^\perp$ but equality does not hold in general. 3) $K$ surjective $\rightarrow$ $K'$ injective, but $\leftarrow$ does not hold i.g. 4) $K'$ surjective $\rightarrow$ $K$ injective, but $\leftarrow$ does not hold i.g. So far, I know: $\langle Lx,y \rangle = \langle x,L'y\rangle$ and so it follows $L=L'$. The kernel of $L$ is $(0)$. Then, from the lecture, I get $\operatorname{cl}(\operatorname{rg}(L))= \ker(L')_\perp = (here) \ker(L)_\perp = (0)_\perp = \ell^p$. How can I find $\operatorname{rg}(L)$? I need it to solve $2)$, $3)$ and $4)$. I appreciate any help.","['functional-analysis', 'lp-spaces', 'adjoint-operators']"
2327671,Show that exists a continuous function in $E$ that is not bounded,"Let $E\subset\mathbb{R}$ a non-compact set. Show that i) Exists a continuous function in $E$ that is not bounded. ii) Exist a continous and bounded function in $E$ that doesn't have
  maximum. iii) If $E$ is bounded then, exists a continuous function in $E$ that
  is not uniformly continuous I started writting one definition that I have doubt Def : A set $E\subset \mathbb{R}$ is compact if and only if it is
  closed and bounded. If a set is non-compact then I can say that that it is not bounded or not closed? i) $f(x)=\sqrt{x}$ is a continous function and it is not bounded. ii) From the doubt that I have in definition, if a set is non-compact and it is bounded then it is not closed so doesn't have a maximum. iii) I don't know how to find a example.","['general-topology', 'real-analysis']"
2327674,"Fundamental Theorem of Calculus, integral from upper bound to $x$","Suppose that $f$ is defined on the interval $[a,b]$, where $\;a<b\;$. According to the fundamental theorem of calculus:
$$\frac {d} {dx} \left( \int_{a}^{x}f(t) dt \right) = f(x)$$ What can be said about the following?:
$$\frac {d} {dx} \left( \int_{b}^{x}f(t) dt \right)$$","['integration', 'calculus']"
2327691,"Finite group of ""linear substitutions""","From what I can tell, a linear substitution is an operation on a set of variables $x_1,\ldots,x_n$ which sends them to a new set of variables $y_1,\ldots, y_n$ via a linear transformation $$\vec{y} = \mathbf{A}\vec{x}$$ Provided the matrix $\mathbf{A}$ is invertible, the transformation $\vec{x}\rightarrow \vec{y}$ is invertible. Because these linear substitutions are composable, invertible, and have unit, it is possible to form group structures out of them. In a paper, Painlevé considers an arbitrary group $\alpha$ of linear substitutions for two variables. From what I understand, Painlevé asks us to consider a pair of functions $\varphi$ and $\psi$ which constitute an invariant with respect to the group operation; that is to say, $$\langle x, y\rangle =\langle \varphi(t,u), \psi(t,u)\rangle$$ and whenever $\langle T, U\rangle$ are a solution to this equation, the complete set of solutions is exactly obtained by applying all the transformations in the group $\alpha$ to $T$ and $U$:  $\{ f(T,U) : f\in \alpha\}$. I am mystified, however, when Painlevé subsequently treats $t$ and $u$ as functions of $x$ and $y$, forming all second-order partial derivatives of $$
\begin{align*}
t(aT + bU + c) = a^\prime T + b^\prime U + c^\prime\\
u(aT + bU + c) = a^{\prime\prime} T + b^{\prime\prime}U + c^{\prime\prime}\\
\end{align*}
$$ My questions are: Where did these equations come from? How is it that we treat $t$ and $u$ as functions of $x$ and $y$? Why are there constant terms $c$? (Surely a ""linear substitution"" cannot include constant terms or else the transformation wouldn't be invertible?) What does it mean to call $\langle \phi, \psi\rangle$ invariants? They look like a description of an arbitrary member $f\in \alpha$. Perhaps something was lost in translation? The relevant excerpt, in French (which I don't speak), is included below: I surmise that it says something like: Consider a finite group $\alpha$ of linear substitutions involving two variables and two fundamental invariant functions which correspond to them: $$x=\phi(t,u),\qquad y=\psi(t,u);$$
  If, for a system $(x,y)$, the values $(T,U)$ satisfy the equations (1), all the other solutions of these equations are obtained by applying to the values $(T,U)$ all of the substitutions in the group $\alpha$. We differentiate the equations $$
\begin{align*}
t(aT + bU + c) = a^\prime T + b^\prime U + c^\prime\\
u(aT + bU + c) = a^{\prime\prime} T + b^{\prime\prime}U + c^{\prime\prime}\\
\end{align*}
$$ with respect to $x$ and $y$ up to second order, inclusive. In this way, we form twelve equations [i.e. $\partial_x, \partial_y, \partial_{xx}, \partial_{xy}, \partial_{yx}, \partial_{yy}$ for the two equations?], homogeneous and linear in $a$, $b$, $c$, $a^\prime$, ...., and, if we eliminate those constants, the remaining four equations involve partial derivatives (first and second) of $t$ and of $u$. I discovered that in Transformations of the fundamental equations of thermodynamics , Buckley describes a similar kind of (reversible differentiable but not necessarily linear?) substitution which might shed light on this one. To paraphrase: Suppose the state of a physical system can be described by variables $x_1,\ldots x_n$. Then there exists a characteristic function $E$, which is the solution of a particular exact differential equation $$\partial E - \sum_i x_i\,dx_i = 0,$$ such that the behavior of the system in any state can be conveniently described in terms of $E$ and its partial derivatives with respect to the variables $x_1,\ldots, x_n$.  Specifically, in thermodynamics, we can use the state variables volume $V$ and entropy $S$; all general thermodynamic formulas are derivable from $E$ and the basic equations 
   $$\begin{align*}H &\equiv E - V\, \partial_V E\\ F &\equiv E - S\partial_S E\\G &\equiv E - V\,\partial_V E - S\partial_S E = H-F+E\end{align*}$$ ""Legitimate operations"" (?) on these equations generate a collection $C$ of thermodynamic formulas which are valid with respect to the state variables $V, S$. If we change variables $\mathbf{\mathsf t}:(V,S)\mapsto (V^\prime, S^\prime)$, the new variables \begin{align*}V^\prime = \varphi(V)\\S^\prime = \psi(S)\\\end{align*} are functionally independent and hence soluble for the originals. Under this change of variables, the functions $EHFG$ and formulas $C$ will, generally speaking, change form, but they must remain identically true. Suppose, however, that the functions $EHFG$ retain the same form under a particular change of variables. Then the formulas $C$ must also retain the same form. If the collection of all such transformations forms a group $\mathsf{G}$, then the fundamental formulas will remain unchanged under $\mathsf{G}$, and hence the group characterizes a fundamental arbitrariness in the thermodynamic description of all systems. This is analogous to the fact that, in relativity, the description of the laws of electromagnetic radiation remain unchanged under a Lorentz(-Einstein) transform. But I guess I still don't understand formally what it means to say that an equation has ""the same form"" under a transformation $T$, e.g. expressed as a composition of functions. Perhaps it means that unlike in the general case where of course a change of variables doesn't affect the equation $$(f\circ T^{-1})[T(x)] = (g\circ T^{-1})[T(x)]\;\iff\; f(x) = g(x),$$ a change of variables has the same functional form if not only that, but also (?): $$f(T(x)) = g(T(x)) \iff f(x) = g(x)$$","['group-theory', 'ordinary-differential-equations', 'linear-algebra', 'linear-transformations']"
2327738,Use the Comparison Test,"Use the Comparison Test to determine for what values of $p$ the integral: $\int_{8}^{\infty} \frac{1}{x^p} \ ln(x)  \ dx$ converges. (Use interval notation.) $$\\$$ This is what I have so far: I'm comparing to $\frac{1}{x}$, which diverges. I'm unsure of what to do next. I know it starts to converge from 2. I would really appreciate to your help.","['integration', 'convergence-divergence', 'calculus']"
2327739,Differential equation - fundamental matrix,"Prove using differentiation that $\phi(t)= \int^{t}_{t_0} X(t,\lambda) f(\lambda) \,d \lambda $, is the solution of the system below,  where $X(t,\lambda) =X(t)X^{-1}(\lambda)$ and $X$ is fundamental matrix of this system: $$
\begin{cases}
x'=A(t)x + f(t)\\
x(t_0)=0
\end{cases}.$$ So I have that $$x'= \frac{d}{dt}\phi(t)= \frac{d}{dt}\int^{t}_{t_0} X(t,\lambda) f(\lambda) \,d \lambda =X(t,t) f(t)=f(t)$$ but then  from the first equation $f(t)=A(t)x+ f(t)$ thus $A(t)x =0$ which is not necessarily true? Where am I wrong?","['matrices', 'ordinary-differential-equations']"
2327762,weak convergence/convergence in distribution of random variable $\sqrt{n}(\frac{\mu^2}{\bar{X}_n}-\bar{X_n})$,"I'm working on my problem sheet of a probability theory course and can't solve this exercise: Let for every $k\in \mathbb{N}$ $X_k$ be positive, independently identical distributed random variables with $\mathbb{E}X_1 = \mu$ and $\mathbb{V}ar(X_1)=\sigma^2<\infty$. Let furthermore $\bar{X}_n = \frac{1}{n}\sum_{k=1}^nX_k$, that is the empirical mean. Show that $\sqrt{n}(\frac{\mu^2}{\bar{X}_n}-\bar{X_n})$ converges in distribution to a random variable, which is N(0,$4\sigma^2$)-distributed. So far I have thought about the following: I think I want to use the central limit theorem and/or Etemadi's-theorem.
CLT ensures that $\sqrt{n}(\bar{X}_n-\mu)$ converges to a random variable, whis is N(0,$\sigma^2$)-distributed. I don't know how to handle the other term. If you give solution instead of hints, I would appreciate if you wrote spoiler alert or something like it at the begining of the solution. Thanks in advance!","['normal-distribution', 'probability-theory', 'probability-distributions', 'central-limit-theorem', 'probability']"
2327765,Show that $|\frac {\sin x}{x}| < 1$ for all $ x \in \Bbb R $\ $\{0\}$,"Problem : Show that $\left|\frac {\sin x}{x}\right| < 1$ for all $ x \in \Bbb R $\ $\{0\}$. The result seems obvious for $|x| \geq 1$, but I am not sure how to show it on the interval $(-1,0) \cup (0, 1)$. it seems this amounts to showing that $\sum_{n=0}^\infty \frac {|x^{2n}|}{(2n+1)!} < 1$, but I am not sure how to proceed, hints appreciated.","['trigonometry', 'calculus']"
2327770,Natural Isomorphism in Definition of Tangent Spaces,"I was reading about Differentiable manifolds and stumbled upon a definition of a tangent space. Now, i know there are many definitions, but this concerns the definition via derivations on germs of smooth functions at a point p, that is: Let $M$ be a differentiable manifold, $p$ $\in$ $M$, then let $\widetilde{F_p}$ be the algebra of germs of smooth functions at $p$.Then the tangent space at p: $T_p$ is then defined as derivations $\widetilde{F_p} \to \mathbb{R}$ (that is, linear functionals $v$ satisfying $v(fg) = f(p)v(g) + v(f)g(p)$) Let $F_p$ be the ideal of functions that vanish on p (which is of course well defined on germ classes), and $F_p^2 = F_p F_p$ (product of ideals). My book then proves a (rather beautiful in my opinion) proposition that $T_p \cong (F_p/F_p^2)^*$ naturally Of course in the context of linear algebra this means ""without choosing a basis"", but I'm wondering whether this can be made more formal by considering this as an actual natural isomorphism (between functors). My first guess was that we can consider something like $p \mapsto T_p$ and $p \mapsto (F_p/F_p^2)^*$, and i think this works, but i don't think this result should depend on the manifold i'm considering (since we don't actually mention it ever) So my question is: what is the most ""canonical"" pair of functors that makes this into a natural isomorphism?","['category-theory', 'differential-geometry']"
2327804,How to prove that reduction modulo N is onto,"I have already showed that if 
$\begin{pmatrix}a & b\\c & d\end{pmatrix} \in \operatorname{SL}(2,\mathbb{Z}/N\mathbb{Z})$ then $c$ and $d$ are coprime modulo $N$. Now I have to show that for any $(c,d)$ coprime modulo $N$, there exist $c',d'\in \mathbb{Z}$ such that $c'\equiv c\pmod{N}$, $d'\equiv d\pmod{N}$ and $\gcd(c',d')=1$. The definition I am using of coprime modulo $N$ is: $c$ and $d$ are coprime modulo $N$ if there isn't any $f\neq 0$ in $\mathbb{Z}/N\mathbb{Z}$ such that $fc=fd=0$. 
If I define for every prime $p_i\mid d$ a $\lambda_{p_i}$ such that $c+\lambda_{p_i}N$ is divisible by $p_i$ then how can I use the Chinese Remainder Theorem to prove this?","['number-theory', 'discrete-mathematics']"
2327845,"find all continuous functions $f : [0,1] \to \mathbb Q$ such that $f(\frac{1}{2})=\frac{2015}{2016}$","I have to find all continuous functions $f : [0,1] \to \mathbb Q$ such that $f(\frac{1}{2})=\frac{2015}{2016}$. but I'm stuck and I don't even know how to start. can someone explain me in details how to do this?","['real-analysis', 'functions']"
2327896,Does the alternating sum of prime reciprocals converge?,"$$\sum_{n = 1}^\infty \frac{(-1)^n}{p_n}$$ where $p_n$ is the $n$th prime. I have computed this to 10000 rather than infinity. My results suggest that convergence does happen but it's very slow. But I can't even be sure about the first few digits: $-0.26959$? I have looked at the ""questions that may already have your answer"" and some of the ""similar questions,"" but they involve somewhat different formulas.","['prime-numbers', 'sequences-and-series']"
2327909,Why the determinant of an invertible matrix $A$ must be equal to $\pm1$?,"I've been asked the following question: considering that $A$ is an invertible matrix / $A$ and $A^{-1}$ have integer coefficients, why both determinants must be $1$ or $-1$? We know that, in linear algebra, an $n$-by-$n$ square matrix $A$ is called invertible if there exists an $n$-by-$n$ square matrix $A^{-1}$ such that  $AA^{-1}=I$ where $I$ is the identity matrix. So, if we also consider the following properties: $A$ is invertible $\Leftrightarrow$ $\det(A)\not=0$ and that $\det(I)=1$. Then, let $A\in\mathbb Z^{n\times n}$ such that $A^{-1}\in\mathbb Z^{n\times n}$ and in consequence, $\det\colon\mathbb Z^{n\times n}\to \mathbb Z$, now we can say: $\det(A)\cdot\det(A^{-1}) =\det(AA^{-1}) =\det(I) =1$. I cannot realize why it could also be $-1$. Any idea or suggestion about how can I prove it?","['algebra-precalculus', 'linear-algebra', 'determinant']"
2327931,Assign n professors to 2n courses in two semesters and in spring no professor teaches the same pair of course,"So the problem is A math department has n professors and 2n courses, two assigned to each professor each
  semester. How many ways are there to assign the courses in the fall semester? How many
  ways are there to assign them in the spring so that no professor teaches the same two
  courses in the spring as in the fall? If all the assignments are equally likely, what is the probablity of this event. For the first part, I believe it is 2n!/(2)**n. But I keep having trouble wrapping my head around the second and third one. I was thinking if there are no professor teaches the same course, then it is a lot like getting them assigned first, and ask each professor to switch course with another professor. So the first one has (n-1) choice, second one has (n-2) choice, and it will end up being (n-1)!, and by rule of product, there are [2n!/(2)**n]*(n-1!). But I feel that this number might be too big, and I might be double counting something. And for the third question, I think the denominator would be [2n!/(2)**n]**2 by rule of product. But I don't know what to do with nominator. Thanks in advance!","['combinatorics', 'discrete-mathematics']"
2327937,How do I find this PDE solutions?,"For $f:\mathbb{R}^{n}\rightarrow\mathbb{R}$ and $g:\mathbb{R}^{n}\rightarrow\mathbb{R}$ could someone help me to find what are the solutions of
\begin{align*}
\sum_{i=1}^{n}\partial_{x_{i}} f + \frac{1}{2}f\left[\frac{\displaystyle\sum_{i=1}^{n}\partial_{x_{i}}(\|x\|^{2} + g)}{\|x\|^{2} + g}\right] = \frac{n}{\sqrt{\|x\|^{2} + g}}\quad\text{where } g(\textbf{x})\geq \left[\sum_{i=1}^{n}x_{i}\right]^{2} - \|x\|^{2} 
\end{align*}
It seems that $\displaystyle f(\textbf{x}) = \frac{\sum_{i=1}^{n}\partial_{x_{i}}\|x\|^{2}}{2\sqrt{\|x\|^{2} +g}}$ satisfies such equation for each $n$, but I seek for a general solution if it is possible. Any contribution is appreciated.","['real-analysis', 'ordinary-differential-equations', 'partial-differential-equations']"
2327948,"Contour integration $ \int_0^\infty \frac{\log x}{1-x^{8}}\,dx\, $","I am tasked to compute the integral $$
\int_0^\infty \frac{\log(x)}{1-x^{8}}\,dx\,
$$ using contour integration. I've seen some approaches to complex logarithms, but never with such a function in the denominator. I can't seem to figure out which contour fits this problem best. If I choose a semicircle in the upper right quadrant I can avoid the singularity at the origin, but I still can't deal with the singularity at $z = i$ (or the integral along that line, in fact). I'd really appreciate some help here.","['complex-analysis', 'integration', 'contour-integration', 'complex-integration']"
2327970,What does the d represent in the equation of a plane ($ax+by+cz+d=0$)?,"Look at the following problem: Consider the plane $\alpha$ defined by $x-2y+z+3=0$ and $A(0;0;2)$. Write an equation of the plane that is parallel to $\alpha$ and goes
  through A. My book states the solution is: The direction vector of $\alpha$ is $\vec{n}(1;-2;1)$ so the equation
  of the plane is $x-2y+z+d=0$. A is in the plane, so: $$0 - 2 \cdot 0 + 2 +d = 0 \Leftrightarrow d = -2$$ So the equation is: $x-2y+z-2=0$ What was the reasoning behind this? What does $d$ represent?","['calculus', 'geometry']"
2327987,$x^4 -ax^3 +2x^2 -bx +1$ has real root $\implies$ $a^2+b^2 \ge 8$ [duplicate],"This question already has answers here : Prove that $a^2 + b^2 \geq 8$ if $ x^4 + ax^3 + 2x^2 + bx + 1 = 0 $ has at least one real root. (4 answers) Closed 6 years ago . it is requested to show that if the quartic polynomial $f(x) \in \mathbb{R}[x]$, defined by: $$
f(x) = x^4 -ax^3 +2x^2 -bx +1,
$$
has a real root, then
$$
a^2 +b^2 \ge 8
$$
this question was asked by @medo, then deleted a few minutes ago. however having spent a little time on it, i think the problem seems sufficiently instructive to be worth resuscitating. it is not deep or difficult, but to find the right way of rewriting the polynomial to demonstrate the result is an interesting coffee-break challenge.","['inequality', 'polynomials', 'cauchy-schwarz-inequality', 'algebra-precalculus', 'quadratics']"
2327996,Tension of Each Half (Using Vectors),"A clothesline is tied between two poles, 8m apart. The line is quite
  taut and has negligible sag. When a wet shirt with a mass of 0.8kg is
  hung at the middle of the line, the midpoint is pulled down 8cm. Find
  the tension in each half of the clothesline. Most of the examples I have seen for this type of problem are given two angles that can be used to write the vector representation of each of the two tensions, but for this problem, none are given. How does one solve this problem using vectors? EDIT* inverse trig rules can be used to find the angle","['physics', 'trigonometry', 'vectors']"
2327999,About primes and cyclotomic extensions,I have the following problem Let $p\geq3$ a prime. Show that $\mathbb{Q}(\sqrt[p]{p})$ is not contained in any cyclotomic extension. I don't know how to start the problem. Any hint or help will be appreciated ! Thanks in advance.,"['abstract-algebra', 'galois-theory', 'extension-field', 'field-theory']"
2328007,$\Bbb Z_+\times \Bbb Z_+$ is countable,"This question is from Munkres' Topology: Munkres proves $\Bbb Z_+\times \Bbb Z_+$ is countably infinite thus: First define $f:\Bbb Z_+\times \Bbb Z_+\rightarrow A$ , where $A$ is the subset of $\Bbb Z_+\times \Bbb Z_+$ consisting of pairs of $(x,y)$ for which $y\le x$ , by the equation $$f(x,y)=(x+y-1,y).$$ Then we construct a function $g: A\rightarrow \Bbb Z_+$ by the formula $$g(x,y)=\frac12 (x-1)x+y.$$ $g \circ f$ is bijection since $f$ and $g$ bijection. I can't understand how did we guess function $g$ . Also, help me visualize it!",['elementary-set-theory']
2328009,When is composition of functions defined?,"Given any functions $f:X\to Y$ and $g:A\to B$ the function $h(x)=f(g(x))$ is well defined for any elements $x\in g^{-1}(X\cap g[A])$ can one then write $h=f\circ g$? Or is composition of $f$ and $g$ only defined when the domain of $g$ equals the codomain of $f$? If the composition is still well defined for some values, then why limit the definition? I understand that this could give rise to cases where you have functions with empty domains, in the circumstance the composition isn't defined anywhere but is that really a problem? Would it still be okay to write $h=f\circ g$?","['elementary-set-theory', 'function-and-relation-composition', 'functions']"
2328032,Inverse of a function - Set Theory,"Please help me with this exercise... I already showed that the function is bijective, and I do not know how to find the inverse of the function... Be the function $f : \mathbb{N} \times  \mathbb{N}  \rightarrow \mathbb{N}$ defined by $f(m,n) = 2^m (2n+1) - 1$","['elementary-set-theory', 'inverse-function', 'functions', 'inverse']"
2328044,Differentiable inverse of a vector valued function,"Let $f:\mathbb R^2\to \mathbb R^2$ be a continuously differentiable function and suppose that there is some $(x_0,y_0)\in \mathbb R^2$ such that $\det[f^{\prime}(x_0,y_0)]=0$ . I have to prove that $f$ can not have a differentiable inverse $f^{-1}:\mathbb R^2\to \mathbb R^2$ . The main issue is that I do not know anything regarding global inverse. I know only that $f$ may be an invertible function even though there exists $(x_0,y_0)\in \mathbb R^2$ such that $\det[f^{\prime}(x_0,y_0)]=0$ . But how to comment on the differentiability of $f^{-1}$ ? Any help is appreciated.","['derivatives', 'real-analysis', 'inverse-function']"
2328045,Tangent spaces of SO(3),"My major is mechanical engineering. Recently, I'm reading a paper about tangent spaces of rotation group SO(3). The following is a part of it. The notations used in the figure are: $\mathbf I $ is the 3$\times$3 identity matrix, $\mathbf R=exp(\widetilde{\psi}) $ is an arbitrary rotation tensor on SO(3), and $\widetilde{\psi}$ and $\widetilde{\theta}$ represent the associated skew-symmetric tensors for vectors $\psi$ and $\theta$, respectively. According to this paper, there are two definitions for tangent spaces of SO(3), given by Makinen and Simo et al. respectively. Here are my questions: 1) Which definition is more precise and why? 2) Is an element $\widetilde{\theta}_{R}$ of any tangent space $T_{R}SO(3)$ a skew-symmetric tensor, as Makinen stated? Thank you very much! References: Mäkinen, Jari , Rotation manifold $\mathrm{SO}(3)$ and its tangential vectors , Comput. Mech. 42, No. 6, 907-919 (2008). ZBL1163.74472 . Simo, J.C.; Vu-Quoc, L. , On the dynamics in space of rods undergoing large motions - A geometrically exact approach , Comput. Methods Appl. Mech. Eng. 66, No.2, 125-161 (1988). ZBL0618.73100 .","['smooth-manifolds', 'differential-geometry', 'lie-groups']"
2328068,"Prove that if $d \leq n$, then $S_n$ contains elements of order $d$.","I've came up with a proof to the above problem, but I'm stuck on whether or not this should be done with induction instead. I get stuck in this pit a lot (the ""is or isn't this enough for a proof"") so I wanted to see what others think. Here's my proof: Let $d \leq n$. Consider the the element $x\in S_n$ where $$x= \begin{bmatrix} 
    1 & 2 & 3 & \dots  & (n-d) & (n-d+1) & (n-d+2) & \dots & (n-1) & n \\
    1 & 2 & 3 & \dots  & (n-d) & n & (n-d+1) & \dots & (n-2) & (n-1) \\
\end{bmatrix}$$
  which cycles the last $d$-many elements, that is, shifts the last $d$-many elements 'one spot' to the right, with the very last element being mapped to the start of the cycle again (i.e. $(n-d+1)$). It clearly follows that $x^{d}$ corresponds to shifting each element $d$-many times, returning it to its starting position, that is, $x^d=e$. Since any choice of $0 \leq d \leq n$ can be made, this concludes the proof. $\square$ As for the matrix notation (which is used in Chapter 0 by Aluffi), the element in the top row is mapped to the element directly below it -- so the entire ""matrix"" is merely notation for the element its representing (i.e. the bijective function in $\text{Aut}_{\textbf{Set}}(A)$ for some set $A$.) Just in case it's not clear what I'm asking: I just want someone to verify this proof is valid, and if not, what I could do to improve it.","['symmetric-groups', 'group-theory', 'proof-verification']"
2328092,Combinatorial proof of Recurrence for the number of permutations of odd order,"Let $a_n$ denote the number of permutations of odd order in $S_n$. This is sequence A000246 in OEIS. Then there is the recurrence relation on OEIS that $a_{2n} = (2n-1)a_{2n-1}$ and $a_{2n+1} = (2n+1)a_{2n}$, or equivalently $a_n = a_{n-1} + (n-1)(n-2)a_{n-2}$. It seems like there should be a combinatorial proof for this result. Since there isn't a simple recurrence for the number of partitions with odd parts, it doesn't seem like working on the level of partitions will work.","['permutations', 'combinatorics', 'combinatorial-proofs']"
2328118,Pick out true statements about the limit of $f_n(x)=\frac{1}{1+n^2x^2}$,"For the sequence of functions $f_n(x)=\frac{1}{1+n^2x^2}$ for $n \in \mathbb{N}, x \in \mathbb{R}$ which of the following are true? (A) $f_n$ converges point-wise to a continuous function on $[0,1]$ (B) $f_n$ converges uniformly on $[0,1]$ (C) $f_n$ converges uniformly on $[\frac{1}{2},1]$ (D) $\lim\limits_{n \to \infty} \int_0^1 f_n(x)dx=\int_0^1\lim\limits_{n \to \infty} f_n(x) dx$ So here is my take on this and I want to know if I am correct. Me and a friend are having quite a debate on this question and I can't see how he can disagree with the following logic. Obviously A is false because the limit of $f_n$ on $[0,1]$ is $$ F(x) =\begin{cases} 1 & if \ x=0\\
0 & otherwise \end{cases} $$ Statement B is also false because if $f_n$ were to converge uniformly on $[0,1]$ then its limit would have been a continuous function which is obviously not the case. C is true. This is because the uniform norm $||f_n - 0||$ converges to zero. D is false because $$\lim\limits_{n \to \infty} \int_0^1 f_n(x)dx=\lim\limits_{n \to \infty} \int_0^1 \frac{1}{1+n^2x^2}dx=\frac{\pi}{2}$$ But $$ \int_0^1\lim\limits_{n \to \infty} f_n(x) dx = 0$$","['real-analysis', 'integration', 'real-numbers', 'analysis']"
2328177,Conditional expectation on a filtered space [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Let $\mathcal F_n$ be the $\sigma$-algebra generated by the
intervals
$((j - 1)2^{-n}; j2^{-n}], j = 1, 2, ... , 2^n$, on a probability space $(\Omega,\mathcal F, P)$, where $Ω$ is $[0, 1]$, $\mathcal F$ the Borel $\sigma$-algebra, and $P$ Lebesgue measure. Let $X$ be a bounded continuous function on $[0, 1]$.
Show that the sequence ${\{Y_n}\}={\{E(X|\mathcal F_n)}\}, n = 1, 2, ..., $ converges.","['probability-theory', 'conditional-expectation', 'measure-theory']"
2328207,Complex Integration - $\int_{-\infty}^{\infty} \frac{x-1}{x^5-1} dx$,"Exercise : Show that : $$\int_{-\infty}^{\infty} \frac{x-1}{x^5-1} dx = \frac{4\pi}{5}\sin\bigg(\frac{2\pi}{5}\bigg)$$ Attempt : $$\int_{-\infty}^{\infty} \frac{x-1}{x^5-1} dx = \int_{-\infty}^{\infty} \frac{1}{x^4 + x^3 + x^2 + x + 1}dx $$ $$x^4 + x^3 + x^2 + x + 1=0 \Leftrightarrow x =  \{-(-1)^{1/5},(-1)^{2/5},-(-1)^{3/5},(-1)^{4/5} \}$$ So, the function $f(z) = \frac{1}{x^4 + x^3 + x^2 + x + 1}$ has poles  at the points : $$\{-(-1)^{1/5},(-1)^{2/5},-(-1)^{3/5},(-1)^{4/5} \}$$ Now, I know you have to integrate through a closed curve $C$ and on a line $γ_R$ and then continue on with residues for the poles that reside in this curve, but I am stuck on how to apply it here and I also miss it a bit on how to split the integral for the curve and the line . Most examples I've saw get simpler due to the even function trick, but that cannot be applied here. I would really appreciate a thorough solution and explanation, since I've just started working on generalized integrals and I have to clear my mind on them. Thanks for your time !","['calculus', 'complex-analysis', 'improper-integrals', 'integration', 'contour-integration']"
2328219,"Can anyone explain this solution (contractive function, interval, first derivative related)","We are trying to find a positive null $x \geq 0$ of $$f(x) =
\sqrt{x+1}+\frac{\text{sin}(x)}{10}-2$$ Choose $X=\left[0,8\right]$. For $x \in X$ we have that
  $$\frac{1}{15}= \frac{1}{6}-\frac{1}{10} \leq f'(x) =
\frac{1}{2\sqrt{x+1}}+\frac{\text{cos}(x)}{10}\leq
\frac{1}{2}+\frac{1}{10} = \frac{3}{5}$$ (YOU COULD STOP READING HERE BECAUSE I'M RATHER INTERESTING IN JUST UNDERSTANDING THE STEPS TILL HERE.) Now try to find a $\beta$ such that $\text{sup} \left|1+\beta
f'(x)\right|<1$, i.e. $-2 < \beta f'(x) <0 \text{ }\forall  x \in M$. Because of the
  inequality above, we have that $$-\frac{10}{3} < \beta <0$$ We can thus set $\beta = -3$ and $\phi(x) = x-3f(x) =
6+x-3\sqrt{x+1}-\frac{3}{10}\text{sin}(x)$. Thus
  $\left|\phi'(x)\right| \leq \frac{4}{5}= 0.8$ So, we have a contractive mapping. [... further things that need to be shown so we satisfy all conditions (that there even exists a null). I leave them out to not make this question longer.] I'm mainly interesting in understanding the beginning where we have $$\frac{1}{15}= \frac{1}{6}-\frac{1}{10} \leq f'(x) =
\frac{1}{2\sqrt{x+1}}+\frac{\text{cos}(x)}{10}\leq
\frac{1}{2}+\frac{1}{10} = \frac{3}{5}$$ Can you tell me how you get that? I understand that in the middle we have the first derivative of $f$. On the right side, we took the beginning of interval zero and put it into the first fraction, so $\frac{1}{2\sqrt{0+1}}= \frac{1}{2}$. Then put zero in the second fraction: $\frac{\text{cos}(0)}{10}=\frac{1}{10}$. This explains why we have $\frac{3}{5}$ on the right side. But how do you get the left side, especially how you get $-\frac{1}{10}?$ We insert the end of interval on the left, $8$, I think. But why we have $-\frac{1}{10}$ ?","['real-analysis', 'calculus', 'functions']"
2328223,Inner product of dual basis,"Assume $V$ is an inner product space over $\mathbb{R}$ with inner product $\left<\cdot,\cdot\right>$. Let $u_1,\cdots,u_n$ be a basis of $V$ and $v_1,\cdots,v_n$ be the dual basis, i.e., $\left<u_i,v_j\right> = \delta_{ij}$. Prove that if $\left<u_i,u_j\right>\leq 0$ for all $1\leq i < j \leq n$, then $\left<v_i,v_j\right>\geq 0$ for all $1\leq i < j \leq n$. If we let $B=(b_{ij})$ be the matrix from basis $\{v_i\}$ to $\{u_i\}$, that is, $(u_1,\cdots,u_n) = (v_1,\cdots,v_n)B$, then $u_j = \sum_{i=1}^n b_{ij}v_i$, hence $\left< u_j, u_k \right> \leq 0$ and the duality imply $\left< u_j, u_k \right> = \left< \sum_{i=1}^n b_{ij}v_i, u_k \right> = \left<  b_{kj}v_k, u_k \right> = b_{kj} \leq 0$ for $k\neq j$. Now $(v_1,\cdots,v_n) = (u_1,\cdots,u_n) B^{-1}$, if we let $B^{-1} = (c_{ij})$ then $v_j = \sum_{k=1}^n c_{kj}u_k$, so $\left< v_i, v_j \right> = \left< v_i, \sum_{k=1}^n c_{kj}u_k \right> = \left< v_i, c_{ij}u_i \right> = c_{ij}$ for $i\neq j$. Since $BB^{-1} = I_n$, we have $\sum_{k=1}^n b_{ik}c_{kj} = \delta_{ij}$. But it seems difficult to determine whether $c_{ij}\geq 0$ from this condition. How should I do next?","['matrices', 'linear-algebra']"
2328241,Topology generated by norm and Topology generated by the norm map,"I have a problem stated, ""Let $X$ be a normed space. Prove that the topology generated by norm is exactly the coarsest topology on $X$ s.t. the norm and all translations are continuous."" Here's what I've done so far, it seems easy to prove that the norm and all translations are continuous in the topology  generated by norm since it would be the same if we consider $X$ as a metric space. The coarsest topology part, however, seems not immediate. Actually, I'm a little confused here. I think the coarsest topology for the norm map and the translations to be continuous must be the Topology generated by the norm map (since it's really vague about the topology in which the translations are continuous). But
I read in Is the norm topology the same as the initial topology generated by the norm function? that Topology generated by norm and Topology generated by the norm map are not the same. Can anyone show me how to solve this problem? THank you",['general-topology']
2328256,"Epsilon-delta definitions, inequality strict / non-strict?","Reading some of the analysis related posts, I have a question regarding the epsilon-delta language. What we are taught is the inequality in the definition is strict. E.g $$\forall\varepsilon >0\ \exists\delta >0: \forall x\in D\left ( |x-a|<\delta \Longrightarrow |f(x)-f(a)|<\varepsilon \right )$$ ( definition of continuity at $a\in D$ ). If this is satisfied, we conclude for suitable choice of $x$ the difference between $f(x)$ and $f(a)$ is strictly less than any positive number hence it must be zero. Intuitively, it also makes sense that it's sufficient if the inequality involving $\varepsilon$ is not strict. But how does one justify that? Let $R(\varepsilon)$ represent a definition with strict $\varepsilon$ -inequality. Let $M(\varepsilon)$ be the same definition, but let $\varepsilon$ -inequality be non-strict.  Then $R(\varepsilon)$ is satisfied iff $M(\varepsilon)$ is satisfied? The question really is if $M(\varepsilon)$ is satisfied, is then $R(\varepsilon)$ satisfied? Does one simply say that since $M\left (\frac{\varepsilon}{2}\right )$ , then $R(\varepsilon)$ ? I can almost be sure that we can't allow the inequality involving $\delta$ to be non-strict, otherwise we could potentially permit points where $f$ tends to infinity? [On second thought, just make $\delta$ smaller]","['functional-analysis', 'epsilon-delta', 'real-analysis', 'continuity']"
2328257,How to solve this Trigonometric integral involving sine?,"$$\int_{0}^{\pi/2}\frac{\sin t}{1+\sqrt{\sin 2t}}\,dt $$
i used properties of definite integrals to reduce it upto this 
$$I=\frac{1}{2}\int_{0}^{\pi/2}\frac{\sin t+\cos t}{1+\sqrt{\sin 2t}}\,dt$$
I'm not sure how to approach from here , it was asked in AMM ,problem 11961","['definite-integrals', 'integration', 'trigonometry']"
2328278,grasping uniform convergence for function series,"I would like some help with my grasping the idea of uniformly convergence for a series of function. I know that if $\lim_{n\rightarrow\infty}\sup|f_n(x)-f(x)| = 0$ than the series $ {f_n(x)}$ converges uniformly, where $f(x)$ is the limit function. If we look at the function $x^n$ when $0\le x\le1$ than: $f(x)$ is $0$ when $0\le x\lt1$ and $1$ when $x=1$ on one hand, the function $f(x)$ is not continuous so I think it's safe to assume the function series is not uniform convergence. is that enough to answer the question? on the second hand, when i tried to use the notation above I got different result when I look at the two situations separately. when $0\le x\lt1$ than $\lim_{n\rightarrow\infty}\sup|f_n(x)-f(x)| =0$ when $x=1$ than $\lim_{n\rightarrow\infty}\sup|f_n(1)-f(1)| =0$ how to use the notation above for that question?","['sequences-and-series', 'functions']"
2328284,Approximating $\int_0^1 \cos(x^2)dx$ with power series,"I would like to calculate $\int_0^1 \cos(x^2)dx$ with an error smaller than $10^{-6}$ (this error should be proven.). I have a strategy, but I am not quite sure if this is a valid one. Here is what I did: I know that the series representation of the cosine is:$$\cos(x) = \sum_{k=0}^\infty (-1)^k \frac{x^{2k}}{(2k)!}$$
Since the integral boundaries I want to calculate only range from $0$ to $1$, I can use the fact that $a_k :=\frac{x^{2k}}{(2k)!}$ is a falling sequence for $x \in [0,1]$. Then I would use the estimation from the Leibnitz criterion, that for positive falling $a_k$ with $$s_n := \sum_{k=1}^n (-1)^k a_k$$
the inequality $|s - s_n|\le a_{n+1}$ holds, whereas $s$ denotes the limit of $s_k$. Then I could say that for $n=9$: $$|\cos(x)-s_9| \le \frac{1}{(2\cdot 10)!}$$ Then I would calculate $\int_0^1 \sum_{k=0}^9 (-1)^k \frac{x^{4k}}{(2k)!}dx$ The estimate I get seems to be correct, however I am not sure whether or not my prove above is sufficient. Any help would be greatly appreciated!","['fresnel-integrals', 'integration']"
2328308,"Theorem 6.12 (e) in Baby Rudin: If $f \in \mathscr{R}(\alpha)$ and $c > 0$, then $\ldots$","If $f \in \mathscr{R}(\alpha)$ on $[a, b]$ and if $c$ is a positive constant, then $f \in \mathscr{R}(c \alpha)$ , and $$ \int_a^b f d (c \alpha) = c \int_a^b f d \alpha.$$ This is part of Theorem 6.12 (e) in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: My proof: Here is my Math Stack Exchange post on the definition of integral as used in Baby Rudin. As $c > 0$ and as $\alpha$ is a monotonically increasing function on $[a, b]$ , so $c \alpha$ is also  monotonically increasing on $[a, b]$ . And, for any partition $P = \left\{ \ x_0, x_1, \ldots, x_n \ \right\}$ of $[a, b]$ , we also have $$ 
\begin{align}
L(P, f, c \alpha) &= \sum_{i=1}^n \left[ \left(  \inf_{x_{i-1} \leq x \leq x_i} f(x) \right) \left[ c \alpha\left( x_i \right) - c \alpha\left( x_{i-1} \right) \right] \right] \\ 
&=  c \sum_{i=1}^n \left(  \inf_{x_{i-1} \leq x \leq x_i} f(x) \right) \left[  \alpha\left( x_i \right) -  \alpha\left( x_{i-1} \right) \right]  \\
 &= c L(P, f, \alpha);
\end{align}
$$ that is, $$  L(P, f, c \alpha) = c L(P, f, \alpha ) \ \mbox{ and similarly} \   U(P, f, c \alpha) = c U(P, f, \alpha). \tag{1} $$ Now as $f \in \mathscr{R}(\alpha)$ on $[a, b]$ , so for every real number $\varepsilon > 0$ we can find a partition $P$ of $[a, b]$ such that $$ U(P, f, \alpha) - L(P, f, \alpha) < { \varepsilon \over c }, $$ and so from (1) we conclude that for this same partition $P$ of $[a, b]$ , we have $$ U(P, f, c \alpha) - L(P, f, c \alpha) < \varepsilon, \tag{2} $$ from which it follows that $f \in \mathscr{R}( c \alpha)$ . Also from (1) and  (2) we see that \begin{align}
\int_a^b f d (c \alpha) &\leq U(P, f, c \alpha) \\
&<  L(P, f, c \alpha ) + \varepsilon \\
&= c L(P, f, \alpha) + \varepsilon \\ 
&\leq c \int_a^b f d \alpha + \varepsilon
\end{align} for every real number $\varepsilon > 0$ , which implies that $$ \int_a^b f d (c \alpha) \leq c \int_a^b f d \alpha. \tag{A}$$ And, again from (1) and  (2) we also have \begin{align}
c \int_a^b f d \alpha & \leq c U(P, f, \alpha) \\
&= U(P, f, c \alpha) \\
&< L(P, f, c \alpha) + \varepsilon \\
&\leq \int_a^b f d (c \alpha) + \varepsilon 
\end{align} for every real number $\varepsilon > 0$ , which implies that $$ c \int_a^b f d \alpha \leq \int_a^b f d (c \alpha). \tag{B}$$ From (A) and (B) the required result follows. Is my proof satisfactory enough?","['real-analysis', 'integration', 'definite-integrals', 'analysis', 'solution-verification']"
2328312,Quadratic extensions of local fields,"For a given prime number $p$, for which quadratic extensions $\mathbb{Q}(\sqrt{d})$ of $\mathbb{Q}$ is it true that for all places $w$ of $\mathbb{Q}(\sqrt{d})$ such that $w|v$, $\mathbb{Q}(\sqrt{d})_w$ is a quadratic extension of $\mathbb{Q}_v$? I'm trying to use the following theorem from Janusz's ""Algebraic Number Fields"": Let $L=K(\theta)$ be an extension field of $K$ (where $L$ and $K$ are number fields) and let $f(x)$ be the minimum polynomial of $\theta$ over $K$. Let $K_p$ be the completion of $K$ at a prime $p$ and let $f(x)=\Pi_{i=1}^g f_i(x)$ be the factorization of $f$ over $K_p$. Then the primes of $L$ that extend $p$ correspond one-to-one with the factors $f_i(x)$ of $f(x)$. If $p_i$ corresponds to $f_i(x)$, then the completions $L_i$ of $L$ at $p_i$ satisfies $L_i \cong K_p/(f_i(x))$. In the context of my question, $f(x)=x^2-d$. I want the $L_i$ to be quadratic extensions of $\mathbb{Q}_p$. So I need $d$ so that $x^2-d$ factors into exactly quadratic polynomials in $\mathbb{Q}_p$. How can I do that?","['number-theory', 'algebraic-number-theory']"
2328317,Can $\sin\left(\frac{x+y}{\sqrt{2}}\right)=\frac{y-x}{\sqrt{2}}$be written as a function of $x$,"I started by noticing that the derivative of $\sin(x)$ is always between $1$ and $-1$. Therefore if i have a line that intersects the $x$ axis at $45°$ it will aways pass through the line once or be tangent to it. In other words  $\sin(x)=\pm x-a$ has only one solution for any $a$ and sign for $x$. So using the rotation matrix to rotate $\sin(x)$ $-45°$ around the origin I get
 $$\sin\left(\frac{x+y}{\sqrt{2}}\right)=\frac{y-x}{\sqrt{2}}$$ 
I got stuck on trying to write this as a function of $x$ but i do believe it can be. And as an add on question can the same be done for $\sin(x+y)=y-x$","['algebra-precalculus', 'functions']"
2328324,Product rule of Curl,"I know how to do a to d. 
For e, i let $\phi = \frac{1}{|r|^3}$ and $A = a \times r$, tried to simplify but did not reach the answer. Anything can help. THank you",['multivariable-calculus']
2328354,Can the identity function be expressed as a countable-linear combination of indicator functions?,"Question. Can the function $$\mathrm{id} : \mathbb{R} \rightarrow \mathbb{R}, \qquad \mathrm{id}_\mathbb{R}(x) = x$$ be expressed as a countable-linear combination of indicator functions of subsets of $\mathbb{R}$ ? Remark. One idea for constructing such a thing is to try to find a function $a : \mathbb{Q} \rightarrow \mathbb{R}$ such that $$(\forall x \in \mathbb{R}) \qquad x = \sum_{q \in \mathbb{Q}}a_q[x<q],$$ where the square brackets connote the Iverson bracket . But it's not really clear how to choose the $a_q$ 's. Honestly, I think the answer is probably ""no."" Ideas, anyone? I'm also interested in the (simpler?) problem where $\mathbb{R}$ is replaced by $\mathbb{R}_{\geq 0}$ .","['real-analysis', 'measure-theory']"
2328369,What are the diagonals in the matrix called?,"How correctly to designate these diagonals? They are highlighted in different colors. \begin{pmatrix}\color{green}1&\color{orange}2&\color{red}3&\color{blue}4\\
\color{orange}5&\color{red}6&\color{blue}7&\color{green}8\\
\color{red}9&\color{blue}{10}&\color{green}{11}&\color{orange}{12}\\
\color{blue}{13}&\color{green}{14}&\color{orange}{15}&\color{red}{16}\end{pmatrix} $\color{green}{1, 14, 11, 8}$ $\color{orange}{5, 2, 15, 12}$ $\color{red}{9, 6, 3, 16}$ $\color{blue}{13,10, 7, 4}$ - secondary diagonal","['matrices', 'terminology']"
2328394,Union of infinite open and close sets,"Suppose $ \{A_i: i ∈ I\}$ is a collection of clopen sets (i.e., open and closed at the same time), indexed by $I$. Is it true that
$$
\bigcup_{i\in I} A_i
$$
is closed?","['general-topology', 'analysis']"
2328415,Extreme points of the set of two probability measures with same marginals,"I am interested in the extreme points of the set $S$ of pairs of probability measures on $[0, 1]^2$ having the same marginals. More specifically, $(\mu_1, \mu_2) \in S$ , where $\mu_1$ and $\mu_2$ are two probability measures on $[0, 1]^2$ , if: $$\mu_1(A\times[0,1]) = \mu_2(A\times[0,1]), \forall A \in \mathcal{B}([0, 1]) \\
\mu_1([0,1] \times B) = \mu_2([0,1] \times B), \forall B \in \mathcal{B}([0, 1]).$$ This set is a convex set and I would like to characterize it's extreme points. My hypothesis is that the extreme points of this set are made of a finite number of Dirac measures. It's clear that measures such as $(\delta_{a_1, b_1}, \delta_{a_1, b_1})$ for $(a_1, b_1) \in \mathbb{R}^2$ , where $\delta_{a_1, b1}$ denotes the Dirac measure supported at $(a_1, b_1) \in [0, 1]^2$ , are extreme points but you can also find more exotic ones such as: $$\left( \frac{1}{2} (\delta_{a_1, b_1} + \delta_{a_2, b_2}), \frac{1}{2} (\delta_{a_1, b_2} + \delta_{a_2, b_1}) \right), ~\text{for}~a_1, a_2, b_1, b_2 \in [0, 1]^4.$$ You can construct extreme measures similar as this one with $n$ Dirac measures for all $n \in \mathbb{N}$ . For a given $n$ , they are equivalent to: $$\left( \frac{1}{n} \sum_{k=1}^n \delta_{a_k, b_k}, \frac{1}{n}( \sum_{k=1}^{n-1} \delta_{a_k, b_{k+1}} + \delta_{a_n, b_1})\right), ~\text{for}~a_1, \ldots, a_n, b_1, \ldots, b_n \in [0, 1]^{2n}$$ Let's denote the set made of all extreme probability measures with a finite number of Dirac measures $A$ . I've been able to prove that the convex hull of $A$ is dense in $S$ . From Milman theorem it follows that the extreme points are included in the closure, but I'm unable to conclude from there. Is there a way to conclude or to disprove my hypothesis ? I've looked at some elements of the closure but they are not extreme points.","['probability-theory', 'convex-analysis']"
2328421,A trigonometric equation,"So I have a solution $x = 5° + n20°$ for a trigonometric equation and I want to find the exact same solutions, but in different form. Here is what I tried: $x=−115°+m20°$ $x=25°+m40°$ and $x=85°+m40°$ $x=−45°+m20°$ $x=5°+m40°$ and $x=−35°+m40°$ Are my solutions correct?","['trigonometry', 'calculus']"
2328451,Generalizing results on dynamical systems on $\mathbb{R}^n$ to results on general manifolds,"I am trying to self-learn dynamical systems but I am having the following problem: most books, especially introductory texts, all results are given as results about dynamical systems defined by evolution functions $f: \mathbb{R}^n \rightarrow \mathbb{R}^n $. To what degree can I expect these results to generalize to dynamical systems on manifolds given by $f: M \rightarrow TM$ and is there anything in particular I should be ""careful"" about. Maybe to make the situation a little more specific here are some theorems I have thought about in particular: 1) Smales result that the (un)stable invariant manifold of a hyperbolic fixed point is an injective immersion of the (un)stable tangent space. Is there an analogy for manifolds? How would such a thing work on compact manifolds? It implies that there is some canonical way to map a subspace of the tangent space into a immersed submanifold, this should require extra structure on the manifold. Is it obvious where this comes from? 2) Shadowing lemmas For these clearly we need a metric on the manifold. Again, is there an obvious way to choose this? What if we have a hamiltonian system defined by a symplectic form, there is no ""natural"" way to define length, so how should I think about these things? I would appreciate any help or perhaps even a recommended resource which will help me with these embarrassingly easy questions.","['symplectic-geometry', 'dynamical-systems', 'hamilton-equations', 'differential-geometry']"
2328461,Relation between minimal polynomials of $AA^T$ and $A^TA$,"Let $A$ be a real $m\times n$ matrix with $m<n$, of full row rank. Let $f$ be the minimal polynomial of $AA^T$, and $g$ be the minimal polynomial of $A^TA$. Prove that $g=xf$. What I know is that $A, A^T, AA^T, A^TA$ all have the same rank, so $AA^T$ must be invertible and $A^TA$ is not invertible, thus having eigenvalue 0, which means that $g$ must have a factor $x$. However, I don't know how to proceed further. Does anyone have idea?","['matrices', 'linear-algebra', 'minimal-polynomials']"
2328468,"Find the limit $\lim\limits_{n\to \infty} \int_{0}^{n} {1\over{1+n^2\cos^2x}}\,dx$","$$\lim_{n\to \infty} \int_{0}^{n} {1\over{1+n^2\cos^2x}}\, dx$$ Some help please. I don't have any idea. Thank you!","['integration', 'trigonometry', 'calculus', 'limits']"
2328473,"If $p(x)$ is a polynomial of degree 3 such that $p(i) = {1\over1+i}$ for all $a=\{1,2,3,4\}$. Then find $p(5)$.","If $p(x)$ is a polynomial of degree $3$ such that $p(i)$ = $\frac{1}{1+i}$ for all $a=\{1,2,3,4\}$. Then find $p(5)$. My attempt : (1) First obviously I thought of solving the four
  equations which can be generated by assuming polynomial as
  $ax^{3}+bx^{2}+cx+d$. But that would be a very lengthy solution. (2) In other attempt, I let $g(x)=p(x) - \frac1{1+x}$ believing that some calculus concepts might be
  applicable since $g(x)=0$ at $1,2,3,4$ but I couldn't think of anything
  that works out. Also, we ""cannot"" make $g(x) =k(x-1)(x-2)(x-3)(x-4)$ since we can only
  write that for polynomials since we can comment on their maximum
  number of real roots by looking at the degree. That rules out another possible method. So, how can this question be solved?","['polynomials', 'functions']"
2328493,A difficulty in understanding the solution of Exam GRE 0568 Q31.,"The question and its answer is given in the following picture: The question was asking about ""the graph of a solution to the differential equation"", so why in the solution he is speaking about $dy/dx$ and not speaking about $y$? could anyone explain this for me please? Also I do not understand why our selection is narrowed to only (A) and (B) and not to (A)(B)(D) and (E), why the author exclude (D) and (E)? could anyone explain this for me? Finally, it is not clear for me why he exclude (B), could anyone explain this for me please?","['ordinary-differential-equations', 'calculus', 'gre-exam']"
2328507,Do exist an injective function from $\mathbb{R}^5 \rightarrow \mathbb{R}^4$,"Do exist an injective function from $\mathbb{R}^5 \rightarrow \mathbb{R}^4$? I think it is true, but I am unable to find a simple example... Please help me. I think it should be something like $$ q:\mathbb{R}^5 \rightarrow \mathbb{R}^4, \ q(x,y,z,k,l) = (f(x,y),\ f(y,z),\ f(z,k),\ f(k,l)); $$
where $f:\mathbb{R}^2\rightarrow\mathbb{R}$, an injective function, as I just read here about that: Injective function from $\mathbb{R}^2$ to $\mathbb{R}$? Am I wrong? If there exists a such function, please give me an example. Thank you very much!","['special-functions', 'functions']"
2328510,A question about poles of function,"Consider the function $f(z)=\dfrac{\sin \frac{\pi z}{2}}{\sin (\pi z)}$. Then $f$ has pole at 1) all integers 2) all even integers 3) all odd integers 4) all integers of the form $4k+1$, $k \in \mathbb{Z}$. My idea: given function  $f(z)=\dfrac{\sin \frac{\pi z}{2}}{\sin (\pi z)}$ so for poles $\sin (\pi z)=0 \rightarrow \pi z=n\pi \rightarrow n=z $ hence $1$ is right?","['complex-analysis', 'residue-calculus']"
2328539,Question on Teichmuller metric as infimum of the dilatations,"I'm learning Teichmuller theory from the book ""Primer on Mapping class group"". There is something I can not understand, I hope someone could clarify it. Let $\mathcal{T}_g$ be the Teichmuller space of closed Riemann surfaces of genus $g\ge 2$. The authors say that, given two marked Riemann surfaces $(X_1,f_1:S_g\rightarrow X_1),(X_2,f_2:S_g\rightarrow X_2)\in \mathcal{T}_g$, their Teichmuller distance (which I write as $d_{\mathcal{T}}(X_1,X_2)$) is defined as the infimum of the dilatations of the quasi-conformal maps homotopic to the change of markings $f_2\circ f_1^{-1}:X_1\rightarrow X_2$. This fact suggest that the dilatation of the quasi-conformal maps should be calculated in the coordinates of the conformal atlases of $X_1$ and $X_2$. 
Then, by the two Teichmuller's theorems (existence and unicity of Teichmuller map) we get that this infimum is realized by the Teichmuller map between $X_1$ and $X_2$.
The proof of the Teichmuller's unicity theorem relies on the fact that the Teichmuller map is affine in the natural coordinates of some quadratic differentials $q_1$ (on $X_1$) and $q_2$ (on $X_2$) and that its horizontal stretch factor is $\sqrt{K}$ and its vertical stretch factor is $1/\sqrt{K}$ ($K>1$) and so the dilatation is $K$. So we get that $d_{\mathcal{T}}(X_1,X_2)$ equals the dilatation of the Teichmuller map (for the quadratic differentials $q_1$ and $q_2$) computed in the natural coordinates of $q_1$ and $q_2$. This is my question: Why is the dilatation of the Teichmuller map computed in the natural coordinates of $q_1$ and $q_2$ and not in the coordinates of the conformal atlases of $X_1$ and $X_2$? Is it the same thing? If yes, why? Thank you.","['riemann-surfaces', 'teichmueller-theory', 'differential-geometry']"
2328543,"$f$ is a homeomorphism iff $f$ is bijective, continuous and open","I am trying to prove a topology statement. Let $X,Y$ be topological spaces, and let $f: X \to Y$ be a bijection. Prove that $f$ is a homeomorphism if and only if $f$ is continuous and open. My Attempt: Suppose that $f$ is a homeomorphism.
Then by definition, $f$ is continuous.
And by definition of continuity, for every open $U \subset Y$, $f^{-1}(U)$ is open in X. Let $f^{-1}(U) =V$. Then, since $f$ is bijective, we have that  $f(V)=f(f^{-1}(U))$. Since $f$ is continuous, all the open subsets of $X$ can be obtained as $f^{-1}(U)$ and since $U$ is open, $f(V)$ is open for all $V \subset X$. (Other direction) Assume that $f$ is continuous and open. Then Since $f$ is bijective, it has an inverse $f^{-1}$. Since $f$ is open, for any open set $U$ of $X$, $f(U)$ is open in $Y$. We need to prove that $f^{-1}$ is continuous. Since $f$ is open, for all open sets $U \in X$, $(f^{-1})^{-1}(U)=f(U)$ is open in $Y$. Thus $f^{-1}$ is continuous and $f$ is a homeomorphism Any help would be appreciated.","['continuity', 'general-topology', 'open-map']"
2328573,Simplification of Trigo expression,"Simplify $$\frac{\tan^2 x+\cos^2 x}{\sin x+ \sec x}$$ My attempt, $$=\frac{(\cos^2x+\frac{\sin^2x}{\cos^2x})}{\frac{\sin x \cos x+1}{\cos x}} $$ $$=\frac{\cos^4 x+\sin^2 x}{\cos^2 x}\cdot \frac{\cos x}{\sin x \cos x+1}$$ $$=\frac{\cos^4 x+\sin^2 x}{\cos x(\sin x \cos x+1)}$$ I'm stuck at here. The given answer is $\sec x -\sin x$","['algebra-precalculus', 'trigonometry']"
2328582,Question on Concatenation of Prime Numbers,"Concatenation Concatenation of two numbers $(p,q)$ in some number base is defined as combining the digits of those numbers, written as $p||q$ . For example, $123||456=123456$. Concatenation simply combines digits of two numbers. Concatenator Let $p,q$ be prime numbers. If the result of $p||q$ is a composite number, I call it the Concatenator of $p,q$. If the result is a new prime number, then pair $(p,q)$ does not have a Concatenator . Always a Concatenator ? From this you can see that all pairs of form $(p,2)$ and $(q,5)$ always have a Concatenator , since the result is divisible by $2$ for the first case and by $5$ for the second case. Also, if $(|p-q|=2)$ and the smaller prime is $\gt3$, then the pair $(p,q)$ always has a Concatenator since the result is always divisible by $3$ . These pairs are twin primes. If $p=q$ then those pairs are also always a Concatenator of course. Perfect Concatenators I also wanted to mention that some pairs share the same Concatenator. If a pair $p,q$ has a unique Concatenator, then I call it the Perfect Concatenator . Example: $37193$ is not a perfect one, since pairs $(3719,3)$ and $(37,193)$ and $(3,7193)$ all share it. Example: Trivially perfect pairs are pairs where both $p$ and $q$ are one digit primes. Delayed Concatenator Furthermore, if a pair does not have a Concatenator, we can multiply $p$ by $10$ before the concatenation, and check if the result is composite. If it is, the pair is a Delayed-$1$ Concatenator . If the result is still not a Concatenator, multiply by $10$ again and repeat until you get a Concatenator. If you multiplied by $10^n$ in total, then the result is a Delayed-$n$ Concatenator . What is the most delayed concatenator? Below are the smallest most delayed concatenators I've found so far for $p=2,3,5,7,11,13$ 203, 20083, 200011, 200004133, 20000029, (5)

3013, 3007, 300011, 300002411, 30000089, (5)

5041, 500101, 50003, 500002237, 50000020063, (5)

703, 70043, 700019, 700002551, (4)

1107, 110071, 110003, 1100005879, (4)

13011, 130037, 130007, 130000307, 1300000457, (5) Below are the smallest most delayed concatenators I've found so far for $q=2,3,5,7,11,13$ [doesn't exist] 

203, 29003, 50003, 27100003, 5527000003, (5)

[doesn't exist] 

1107, 3007, 130007, 103300007, 1069000007, 76810000007 (6)

13011, 230011, 200011, 857000011, 14990000011 (5)

3013, 190013, 15100013, 43000013, 4870000013 (5) Where you see that the best I could find was a delayed $6$ concatenator. The smallest $\text{D}6$ concatenator so far is $76810000007$. This means $7681000007,768100007,76810007,7681007,768107,76817$ are all prime. This is the result of concatenation of $(7681,7)$ by delaying it by $10^6$. But the real smallest $\text{D}6$ would be of form $2000000||q$, if such $q$ exists. Questions Can you find a more delayed concatenator? Is there such a thing as the most-delayed-concatenator ? Can a more delayed concatenator be computed/calculated without brute force search? Is it possible to define a more efficient
  algorithm? Is there anything similar to this already analyzed somewhere? Are there any more trivial pairs $(p,q)$ such that they always have a concatenator, other than ones with $p=2,5$ and twin primes? Asking if there is a delayed $n$ concatenator for some $(p,q)$ is like asking if there exists a sequence of prime numbers of length $n$ of form $$p||\underbrace{0\dots0}_k||q$$ 
for $k=0,1,2\dots n-1$.","['number-theory', 'decimal-expansion', 'recreational-mathematics', 'prime-numbers']"
2328594,"Integration by parts, the cases when it does not matter what $u$ and $dv$ we choose.","I was reviewing Integration By Parts on Brilliant.org where an example they use is $$\int x \ln x \;dx$$
  Let $u=\ln x$ and $dv=x\;dx$ such that
  $$\begin{align}
\int x \ln x\;dx&\;=\;\frac 12x^2\ln x\;-\;\frac 12\int \frac{x^2}x\;dx\\
\\
&\;=\; \frac 12x^2\ln x\;-\;\frac 14x^2\;+\;C 
\\
\end{align}$$ Personally, I would have solved it by letting $u=x$ and $dv=\ln x\;dx$ such that $$\begin{align}
\int x \ln x\;dx&\;=\;x^2\left(\ln x -1\right)\;-\;\int x \ln x\;dx\;+\;\int x\;dx\\
\\
&\;=\; \frac 12\left[x^2\left(\ln x - 1\right)\;+\;\frac 12x^2\right]\;+\;C\\
\\
&\;=\; \frac 12x^2\ln x\;-\;\frac 14x^2\;+\;C 
\\
\end{align}$$ So both choices for $u$ and $dv$ yield a correct result that also is the same both ways up to a constant and I would like to know if that is trivial or what that means. Is there something interesting about integrals such as $\int x\ln x\;dx$ where it does not matter what $u$ and $dv$ we choose to perform Integration By Parts and would there be other examples of this? Edit This edit is to add to the main post the integral $$\int \cos(ax)e^{bx}\;dx$$ from @ Michael Hardy 's comment Letting $u=\cos(ax)$ and $dv=e^{bx}dx$ yields $$\begin{align}
\\
\int \cos(ax)e^{bx}\;dx&\;=\;\underbrace{\frac 1b\cos(ax) e^{bx}}_A\;+\;\underbrace{\frac ab}_B \int \sin(ax)e^{bx}\;dx\\
\\
&\;=\;A\;+\;B \left[\frac 1b\sin(ax)e^{bx}\;-\;B\int \cos(ax)e^{bx}\;dx\right]\\
\\
&\;=\;\frac 1{1+B^2}\left(A\;+\;\frac a{b^2}\sin(ax)e^{bx}\right)\;+\;C\\
\\
&\;=\;\frac {e^{bx}}{a^2+b^2}\left(b\cos(ax)+a\sin(ax)\right)\;+\;C\\
\\
\end{align}$$ which is the same result as when letting $u=e^{bx}$ and $dv=\cos(ax)\;dx$ such that $$\begin{align}
\\
\int \cos(ax)e^{bx}\;dx&\;=\;\underbrace{\frac 1a\sin(ax) e^{bx}}_A\;-\;\underbrace{\frac ba}_B \int \sin(ax)e^{bx}\;dx\\
\\
&\;=\;A\;-\;B \left[-\frac 1a\cos(ax)e^{bx}\;+\;B\int \cos(ax)e^{bx}\;dx\right]\\
\\
&\;=\;\frac 1{1+B^2}\left(A\;+\;\frac b{a^2}\cos(ax)e^{bx}\right)\;+\;C\\
\\
&\;=\;\frac {e^{bx}}{a^2+b^2}\left(a\sin(ax)\;+\;b\cos(ax)\right)\;+\;C\\
\\
\end{align}$$ Edit (2) This edit is to add the $3$ following integrals: $$\int x \tan^{-1}x\;dx$$
  $$\int x \cos^{-1}x\;dx$$
  $$\int x \sin^{-1}x\;dx$$ that I got from @ User8128 's answer, after reading "" logarithms and inverse trig functions become algebraic functions when differentiated "". $\int x \tan^{-1}x\;dx$ letting $u=x$ and $dv=\tan^{-1}x\;dx$ $$\begin{align}
\int x \tan^{-1}x\;dx&\;=\;\underbrace{x^2\tan^{-1}x-\frac x2\ln|1+x^2|}_\alpha-\int x\tan^{-1}x\;dx+\frac 12\int\ln|1+x^2|\;dx\\
\\
&\;=\;\frac 12\left[\alpha+\frac 12 x \ln|1+x^2|-2x+2\tan^{-1}x\right]+C\\
\\
&\;=\;\frac {x^2}2\tan^{-1}x-x+\tan^{-1}x+C\\
\\
\end{align}$$ $\int x \tan^{-1}x\;dx$ letting $u=\tan^{-1}x$ and $du=x\;dx$ $$\begin{align}
\int x \tan^{-1}x\;dx &\;=\;\frac {x^2}2 \tan^{-1}x-\frac 12 \int \frac {x^2}{1+x^2}\;dx\\
\\
&\;=\;\frac {x^2}2 \tan^{-1}x-x+\tan^{-1}x+C\\
\\
\end{align}$$ $\int x \cos^{-1}x\;dx$ letting $u=x$ and $dv=\cos^{-1}x\;dx$ $$\begin{align}
\int x \cos^{-1}x\;dx&\;=\;\underbrace{x^2\cos^{-1}-x\sqrt{1-x^2}}_\beta -\int x \cos^{-1}x\;dx+\int \sqrt{1-x^2}\;dx\\
\\
&\;=\;\frac 12 \left[\beta+\frac 12 \left(x\sqrt{1-x^2}+\sin^{-1}x\right)\right]+C\\
\\
&\;=\;\frac {x^2}2 \cos^{-1}x-\frac 14 x\sqrt{1-x^2}+\frac 14 \sin^{-1}x+C\\
\\
\end{align}$$ $\int x \cos^{-1}x\;dx$ letting $u=\cos^{-1}x$ and $dv=x\;dx$ $$\begin{align}
\int x\cos^{-1}x\;dx&\;=\;\frac {x^2}2\cos^{-1}x+\frac 12 \int \frac {x^2}{\sqrt{1-x^2}}\;dx\\
\\
&\;=\;\frac {x^2}2 \cos^{-1}x-\frac 14 x\sqrt{1-x^2}+\frac 14 \sin^{-1}x+C\\
\\
\end{align}$$ $\int x \sin^{-1}x\;dx$ letting $u=x$ and $dv=\sin^{-1}x\;dx$ $$\begin{align}
\int x \sin^{-1}x\;dx&\;=\;\underbrace{x^2\sin^{-1}+x\sqrt{1-x^2}}_\gamma -\int x \sin^{-1}x\;dx-\int \sqrt{1-x^2}\;dx\\
\\
&\;=\;\frac 12 \left[\gamma-\frac 12 \left(x\sqrt{1-x^2}-\sin^{-1}x\right)\right]+C\\
\\
&\;=\;\frac {x^2}2 \sin^{-1}x+\frac 14 x\sqrt{1-x^2}-\frac 14 \sin^{-1}x+C\\
\\
\end{align}$$ $\int x \sin^{-1}x\;dx$ letting $u=\sin^{-1}x$ and $dv=x\;dx$ $$\begin{align}
\int x\sin^{-1}x\;dx&\;=\;\frac {x^2}2\sin^{-1}x-\frac 12 \int \frac {x^2}{\sqrt{1-x^2}}\;dx\\
\\
&\;=\;\frac {x^2}2 \sin^{-1}x+\frac 14 x\sqrt{1-x^2}-\frac 14 \sin^{-1}x+C\\
\\
\end{align}$$ I see that in the case of $\cos^{-1}x$ and $\sin^{-1}x$, one choice of $u$ and $dv$ either turns the inverse trig function into an algebraic function right away (i.e. $d\left(\cos^{-1}x\right)=\frac{-1}{\sqrt{1-x^2}}\;dx$) by differentiation or the other choice of $u$ and $dv$ makes the original function ""re-appear"" along with an algebraic function (i.e. $\int \cos^{-1}x\;dx = x\cos^{-1}x-\sqrt{1-x^2}+C$) by integration. In the case of $\tan^{-1}x$, a $\log$ appears if we integrate the function but then $\log$s turn into algebraic functions upon derivation as well or make the original function ""re-appear"" along with an algebraic function upon integration (i.e $\int \ln x\;dx = x\ln x - x +C$ ) and so we get the same result. I understand that this is not special but I'm still trying to fully wrap my head around the reason why these integrals are such that when we apply Integration By Parts , any choice for $u$ and $dv$ yield correct results that are also identical to each other. Edit (last) I got "" Calculus: An Intuitive and Physical Approach "" by Morris Kline and at the end of chapter $14$ on Further Techniques of Integration in section $6$ on The Use of Tables he says: "" [...] One answer would be to study more techniques or seek to discover a new one. However, the number of techniques and special tricks is quite extensive. It is neither wise nor efficient to spend months or years on what is really an incidental process or means to an end at the expense of the acquisition of more significant knowledge. "" So, I will follow this advice and (for now) be content with the fact that I understand much better how integrals are obtained, avoid wasting time on knowing why some peculiar results of this "" incidental process or means to an end "" do occur and be going on my merry mathematical way.","['integration-by-parts', 'calculus', 'indefinite-integrals', 'integration', 'soft-question']"
2328599,Prove that any interval is measurable.,"The goal is to show that an interval is Lebesgue measurable. As it's been pointed out in my notes that it is enough to show that an interval of the form $(a,\infty)$ is measurable. So here's my attempt. Note that $(a,\infty)=\bigcup_{n=1}^\infty (a,a+n)$. Let $\epsilon>0$ and $n\in\mathbb{N}$. Then $(a,a+n)$ is measurable iff there exists an open set $U_\epsilon$ such that $(a,a+n)\subseteq U_\epsilon$ and $m^*(U\setminus(a,a+n))<\epsilon$. Taking $U_\epsilon=(a,a+n)$ we have $(a,a+n)$ is measurable. Therefore $(a,\infty)$ is measurable. Hence any interval is measurable. (Here $m^*(A)$ denotes Lebesgue outer measure of the set $A$). Is this argument alright?","['measure-theory', 'proof-verification']"
2328611,On constructing the definition of differentiation,"In the text that I'm following, the construction of the definition of differentiation is given as follows: My problem : In the second paragraph of point-2 , we see that if the condition $\lim_{x \to 0}\frac{E(x)}{x}=0$ holds, then it is easy to see that $a=f(0)$. I'm encountering a problem here. The limit in the condition has absolutely nothing to do with what happens to $f$ at the point $0$. If $f$ is continuous at $0$, then we can enforce the claim easily from the definition of continuity . But we're not given that $f$ is continuous (of course, differentiability implies continuity, but we cannot use that fact before defining differentiation, otherwise circularity creeps in). What I thought about it : Well one possibility is that I'm missing something obvious and I'll greatly appreciate if someone points it out. Otherwise, I think we need an initial condition that $E(0)=0$, which enforces $a$ to be $f(0)$. Now we can impose the previously stated limit condition to try to compute $b$. So in total, we need two conditions:
\begin{align}
&1.\,\, E(0)=0\\
&2. \,\, \lim_{x \to 0}\frac{E(x)}{x}=0
\end{align} Is this extra initial condition really needed? Or the limit condition alone does the job? Any help would be greatly appreciated. Thank you.","['derivatives', 'real-analysis', 'definition']"
2328642,Discrepancy between closed form solution and numerical solution,"I have the following differential equation $$ \frac{dx}{dt} = -\frac{1}{\alpha t}x(1-x) \,,\,x\in [1,20) \tag 1$$
The solution on this interval  by taking $x(1) = 0.9999$ is given by $$ x(t) = \frac{1}{1+0.0001\exp\left( \frac{\ln(t) - \ln t(1)}{ \alpha}\right)}  ~. \tag 2$$ Now if $\alpha = 0.2$, then $x(20) = \lim_{x\to 20^-} x(t)$, so that $$ x(20) =\frac{1}{1+0.0001\exp\left( \frac{\ln(20)}{ 0.2}\right)}\approx 0.0031 ~. \tag 3$$ On the other hand discretizing (1) using Euler approximation yields 
$$ x(t_i)= x(t_{i-1}) - \frac{1}{\alpha t_{i-1}}x(t_{i-1}) (1-x(t_{i-1}) )\Delta t_i~. \tag 4$$ Then
$$ x(20) = 0.9999 - \frac{1}{0.2 \cdot 1} 0.9999\cdot (1-0.9999)\cdot (20-1) \approx  0.9904  \tag 5$$ I know the Euler approximation is very conservative but this result is puzzling. I'm sure I'm missing something but I don't know what. Why is there such a ""big"" discrepancy between (3) and (5)?","['numerical-methods', 'real-analysis', 'ordinary-differential-equations']"
2328646,What is the general expression for a unitary $3 \times 3$ matrix?,"The Wikipedia page for unitary matrices gives a general expression for the $2 \times 2$ unitary matrix: \begin{pmatrix}
a & b \\
-e^{i\phi}b^* & e^{i\phi}a^*
\end{pmatrix} with $|a^2| + |b^2| = 1$ . Is there a similar general construction for the $3 \times 3$ unitary matrix?","['matrices', 'linear-groups', 'linear-algebra', 'unitary-matrices']"
2328647,Is $Y_n=(1-\frac{a}{n})^{X_1+\cdots+X_n}$ an unbiased estimator?,"Let $X_1, X_2, \ldots$ be i.i.d. and have the Poisson distribution with parameter $\lambda$. We define the estimator of $y=e^{-a\lambda}$ (where $a \neq 0$ is some constant value) as:
  $$Y_n= \left( 1-\frac a n \right)^{\sum\limits_{i=1}^nX_i}$$
  Is it an ubiased estimator of $y$? So $E(X_i) = \lambda$. I'm trying to calculate the bias with: $$E(Y_n)=E\left((1-\frac a n)^{\sum\limits_{i=1}^nX_i}\right) = E\left( (1-\frac{a}{n})^{X_1} \cdots (1-\frac{a}{n})^{X_n} \right)$$ which, since $X_i$ are i.i.d., equals: $$=\left( E\left((1-\frac{a}{n})^{X_1}\right)\right)^n$$ I wanted to go further by creating an additional random variable defined as
$Z_i = (1-\frac{a}{n})^{X_i}$, calculating its expected value and then plugging it into the formula above, but I have problems with doing so. How should I approach this? $E(Z_i)=(1-\frac{a}{n})^{x}\cdot \frac{\lambda^x e^{-\lambda}}{x!}$ seems awful to calculate and I'm not even sure if it is the correct way.","['probability-theory', 'probability', 'statistics', 'parameter-estimation']"
2328667,Odds of rolling $K$ or fewer of a particular die face with $N$ dice.,"I have a real world problem that boils down to this: I have $S$ sensors that each send a message once every two minutes to the server.  Each sensor sends its data at a time uniformly at random within the two minute period, and is independent of all other sensors. The server can only receive $R$ messages per second (if more are sent errors result) How large must $R$ be to ensure that no more than $R$ messages are received within one second 99.995% of the time? Equivalently, if I roll $S$ dice, each with 120 different faces, how large must $R$ be so that 99.995% of the time no more than $R$ of any given face comes up?","['combinatorics', 'probability', 'dice']"
2328669,An integral containing a product of logarithms and a polylogarithm,"We consider a following quantity:
\begin{eqnarray}
{\mathcal I}^{(p,q)}_r(\xi,t) := \int\limits_\xi^t \frac{[\log(\frac{t}{\eta})]^p}{p!} \cdot \frac{[\log(\frac{\eta}{\xi})]^q}{q!} \cdot \frac{Li_r(\eta)}{\eta} d\eta
\end{eqnarray}
where $p$,$q$ and $r$ are non-negative integers and $0\le \xi \le t \le 1$ and $L_r(\eta) := \sum\limits_{m=1}^\infty \eta^m/m^r$ is the poly-logarithm. By using integration by parts we found that the quantities above satisfy following recurrence relations: 
\begin{eqnarray}
{\mathcal I}^{(p,q-p)}_{W-q} &=& 1_{2 p \ge q} \cdot \sum\limits_{j=0}^{q-p-1} \binom{q-p}{j} (-1)^j {\mathcal I}^{(2p-q+j,q-p-j)}_{W-p} + 1_{2p < q}\sum\limits_{j=1}^p \binom{p}{j} (-1)^j {\mathcal I}^{(j,q-p-j)}_{W-q+p} + \\
&&1_{2 p \ge q} \cdot (-1)^{q-p}\left(Li_{W+1}(t) - \sum\limits_{l=1}^{p+1} Li_{W-p+l}(\xi) \cdot \frac{[\log(\frac{t}{\xi})]^{p+1-l}}{(p+1-l)!}\right)-\\
&&1_{2p < q} \cdot (-1)^{q-p}\left(Li_{W+1}(\xi)-\sum\limits_{l=1}^{q-p+1} Li_{W-q+p+l}(t) \frac{\log(\frac{\xi}{t})]^{q-p+1-l}}{(q-p+1-l)!}\right)
\end{eqnarray}
for $0\le p \le q \le W$. This is a system of $\binom{W+2}{2}$ linear equations for all the unknown quantities $\left\{ {\mathcal I}^{(p,q-p)}_{W-q} \right\}_{0\le p \le q \le W}$ which for a given $W \ge1$ is straightforward to solve on any CAS. For example we have the following:
\begin{eqnarray}
{\mathcal I}^{(1,2)}_1&=&\frac{1}{2} \text{Li}_3(t) \log ^2\left(\frac{\xi }{t}\right)+2 \text{Li}_4(t) \log \left(\frac{\xi }{t}\right)-\text{Li}_4(\xi ) \log \left(\frac{t}{\xi }\right)+3 \text{Li}_5(t)-3 \text{Li}_5(\xi )\\
{\mathcal I}^{(1,3)}_2&=&  -\frac{1}{6} \text{Li}_4(t) \log ^3\left(\frac{\xi }{t}\right)-\text{Li}_5(t) \log ^2\left(\frac{\xi }{t}\right)-3 \text{Li}_6(t) \log \left(\frac{\xi }{t}\right)+\text{Li}_6(\xi ) \log \left(\frac{t}{\xi
   }\right)-4 \text{Li}_7(t)+4 \text{Li}_7(\xi )\\
{\mathcal I}^{(1,4)}_3 &=& \frac{1}{24} \text{Li}_5(t) \log ^4\left(\frac{\xi }{t}\right)+\frac{1}{3} \text{Li}_6(t) \log ^3\left(\frac{\xi }{t}\right)+\frac{3}{2} \text{Li}_7(t) \log ^2\left(\frac{\xi }{t}\right)+4 \text{Li}_8(t) \log
   \left(\frac{\xi }{t}\right)-\text{Li}_8(\xi ) \log \left(\frac{t}{\xi }\right)+5 \text{Li}_9(t)-5 \text{Li}_9(\xi )
\end{eqnarray}
Now, my question is can we actually find a closed form expression for the quantities in question or otherwise do we always have to resort to CAS to solve the equations in question?","['integration', 'polylogarithm']"
2328685,Formula for MOLS generation when n is prime power - really?,"I remember from the university that there is a procedure to simply generate MOLS when n is a prime number. Basically it is about cyclic rotation of the symbols in the rows, for square k by k positions.
Implementation in Java was easy: (k * (i - 1) + (j - 1)) mod n I remember that this was supposed to work only for n that is a prime number. And yet I found in a book that this is ""guaranteed"" to work also for prime powers. Which I doubt (does not work for n=4, e.g.), but I am not really that good at math and would like to understand whether the book is wrong or not. https://books.google.de/books?id=yU-rTcurys8C&pg=PA294&dq=MOLS+construct+software+testing&hl=cs&sa=X&ved=0ahUKEwjluuz6rcrUAhVCDZoKHVuxBnIQ6AEIJzAA#v=onepage&q=MOLS%20construct%20software%20testing&f=false , page 295.","['combinatorial-designs', 'statistics', 'latin-square']"
2328689,Is $T(p(x))=p(x+1)$ a linear map if $T:\mathbb{R}[x]\to\mathbb{R}[x]$?,"Given $T:\mathbb{R}[x]\to\mathbb{R}[x]$ is $T(p(x))=p(x+1)$ a linear map? $\Bbb{R}[x] $ is a space of all polynomials with coefficients over $\mathbb{R}$. It seems too easy so I suspect I'm doing something wrong but let:
$$
T(\alpha p(x) + \beta q(x))=(\alpha p + \beta q)(x+1)=\\
=\alpha p(x+1)+\beta q(x+1) = \alpha T(p(x))+\beta T(q(x))
$$
therefore it's a linear map. Is it correct?","['linear-algebra', 'proof-verification', 'linear-transformations']"
2328805,what does $+$ mean in $f(x) = (1 - |x|)_{+}$?,"I'm doing homework and in one of the problem a density function is given by $f(x) = (1-|x|)_{+}$. I don't know if the $+$ is a mistypo or not. Anyone familiar with this? If it matters, $Y$ is a random variable with density $f(x) = (1-|x|)_{+}$ and I'm supposed to calculate the characteristic function of $Y$","['probability', 'notation']"
2328817,"On convergence of a sequence $\{x_n\}$ , given that the sequence $\{x_{n+1}+f(x_n)\}$ converges for some function $f$ on real line","If $\{x_n\}$ is a real sequence such that the sequence $\{2x_{n+1}+\sin x_n\}$ is convergent , then is it true that $\{x_n\}$ convergent ?","['real-analysis', 'sequences-and-series', 'convergence-divergence']"
2328842,Metric space consisting two elements.,"Suppose that $S$ be a set consisting exactly $2$ elements. Suppose we define a function $\displaystyle d:S \times S \to [0,\infty)$ by $\displaystyle d(x,y)=\begin{cases}1 &\text{ , if }x\not=y\\0 &\text{ , if} x=y\end{cases}$ How I can show that $d$ defines a metric on $S$ ? Problem is on triangular inequality..To prove triangular inequality we need at least three points. How I can show the triangular inequality ? Same problem for a set consisting only $1$-element or empty set. What's the idea behind these ?","['general-topology', 'real-analysis', 'metric-spaces', 'analysis']"
2328843,Properties of entrywise L1 matrix norm,"Consider the entrywise $L_1$ norm on matrices, given by $$\|M\|_1 = \sum_{i,j} |M_{i,j}|.$$ I'm looking for useful properties this norm might have.  Is there anything we can say about $\|A \cdot B \|_1$, in terms of the matrices $A,B$? (e.g., upper-bound $\|A \cdot B\|_1$, based on some quantities about $A$ and $B$?)","['matrices', 'normed-spaces']"
2328886,"Evaluating $I=\int_{-\infty}^\infty \frac{u^2}{5u^2\left(u^2+1\right)+2}\,\mathrm{d}u$?","This integral popped up when I was trying to solve this integral : $$\int_{-\pi/2}^{\pi/2}\frac{\sin^2x}{4(\cos^4x+2\sin^4x)+2\sin^2 2x}\,dx $$ I simplified it a little bit, substituted $\tan(x)=u$ and came up with, $$I= \int_{-\infty}^\infty \frac{u^2}{5u^2\left(u^2+1\right)+2}\,\mathrm{d}u$$ any suggestions for how I can take it from here ?",['integration']
2328888,The intersection of powersets is contained in the powerset of intersection,Prove or Disprove: For any family of sets $\{A_n\}_{n\in\mathbb N}$ $$\bigcap_{n=1}^\infty\mathcal P \left({A_n}\right)\ = \mathcal P \left({\bigcap_{n=1}^\infty A_n}\right)$$ How do I approach proving this? I know how to unpack the definition of powersets ($\mathcal P \left({A}\right) = \{x | x \subseteq A\}$) but I'm not sure what else I can do. I've done powerset proofs before but none involving indexed family of sets.,['elementary-set-theory']
2328902,Number of sequences interpreted with a directed graph.,"I have a problem with an exercise and no real solution: Determine for $n\geq 1$ the number $a_n$ of sequences $(S_1,...,S_n)$ of $n$
subsets of the set $\{1,2\}=[2]$, so that $S_1=S_n$ and $S_{i+1}\subseteq S_i$ and $|S_i\backslash S_{i+1}|=1$ or $S_i\subseteq S_{i+1}$ and $|S_{i+1}\backslash S_i|=1$ for all $i\in [n-1]$. My idea is somehow to identify the sequences with the pathes of an directed graph, but I am not sure how to construct that. Help is highly appreciated. Many thanks in advance!","['graph-theory', 'discrete-mathematics']"
2328967,Nice Example for $\mathfrak{L}^1 \not \subset \mathfrak{L}^2$,"I am searching for an example of an measurable space $(\Omega,\mathfrak{A},\mu)$ and a measurable function f: $\Omega \to \mathbb{R}$ such that $f \in \mathfrak{L}^1 (\Omega,\mathfrak{A},\mu)$ but $f^2 \notin \mathfrak{L}^1(\Omega,\mathfrak{A},\mu)$ I guess the standard example for this woulf be $f =\frac{1}{\sqrt{|x|}}$, but I thought you can construct something better, having the free choice of the measurable space.","['lebesgue-measure', 'measure-theory']"
2328977,Finding integer part of $(3+\sqrt{3})^4$,"I need to find the integer part of $(3+\sqrt{3})^4$, ie the floor of it and I am unable to proceed. I can find integer part of $(2+\sqrt{3})^4$ using the binomial theorem as $2-\sqrt{3}<1$. Please give a hint. Thanks!","['algebra-precalculus', 'binomial-theorem', 'ceiling-and-floor-functions']"
2329039,Find all holomorphic functions $ f:\mathbb{C}\to\mathbb{C}$ satisfying $|f(z)-3|\geq 1$ for all $z\in\mathbb C $,"Find all holomorphic functions $ f:\mathbb{C}\to\mathbb{C}$ satisfying $|f(z)-3|\geq 1$ for all $z\in\mathbb C $ Please don't post a solution, I just want a little hint. Obviously certain constant functions satisfy the condition. So for now let's assume f is not constant. We have $1\leq |f-3|=\sqrt{(f-3)(\bar{f}-3)}$ so $1\leq(f-3)(\bar{f}-3)$ Since f is holomorphic it follows $0\leq f'\cdot(\bar{f}-3)+(f-3)\cdot\bar{f}\\ \Leftrightarrow f'\cdot\bar{f}+f\cdot\bar{f}'\geq 3(f+\bar{f})'\\ \Leftrightarrow (f\bar{f})'\geq6\cdot\Re(f)'\\ \Leftrightarrow |f|^2\geq6\cdot\Re(f)\\ \Leftrightarrow \Im^2(f)+\Re^2(f)\geq6\cdot\Re(f)$ Since this doesn't look even close to solving the problem, I assume that there must be some theorem which says $f$ must be constant.","['complex-analysis', 'functions']"
2329041,Probability with random intersecting lines,"Suppose we have $n$ random lines that intersect the coordinates $[0,1]^2$. To choose a line randomly, we must first randomly choose a point on a side, choose any point on either of the other three sides and connect the points with a line. The question then is: what is the probability that one of the regions formed by these intersecting lines has area greater than $1/2$?  Obviously the probability for $n=1$ is $1$.  Does anyone see the answer for higher values? Thanks very much!","['combinatorics', 'discrete-mathematics']"
2329046,Show that the set of odd integers has the same cardinality as $\{2^n\mid n\in\mathbb N\}$,How do you show that the set of odd integers $(2k + 1)$ has the same cardinality as the set of positive powers of $2$ $(2^n)?$,"['cardinals', 'elementary-set-theory']"
2329110,Does a continuous function map a closed set to closed set?,"True or False?: If D is closed and f is continuous, then f(D) is closed. The answer is false. However, i can prove that it is true and i can't find what i did wrong. Here's my proof: Whenever $x_k$ is a sequence in D with $x_k \rightarrow x$, we have that $x \in D$ since D is closed. Also, since f is continuous on D, it follows that $f(x_k) \rightarrow f(x)$ and $f(x) \in f(D)$ because $x \in D$. which implies that f(D) is closed. Which step is wrong? question added: If D is open, then f(D) is open. true or false?","['calculus', 'analysis']"
2329116,Discontinuous group of isometries on $\mathbb{R}^{2}$ is generated by 1 or 2 elements,"I am trying to prove the following theorem : A discontinuous, fixed point free group G of isometries of $\mathbb{R}^2$ is generated by one or two elements. We may find an outline of the demonstration here although it only consider the case of the group containing only translations. If $G$ contains only translations the theorem follows from the demonstration found in the link above. When G contains both glide reflections and translations Stillwell's hint is : If $G$ is a discontinuous,fixed point free group of glide reflections and translations,let g be a glide reflection of minimal lenght in $G$ and let h be an element of minimal lenght not in the direction of g . Show that g , h must have perpendicular directions (e.g. by finding shorter elements when the directions of g , h are not perpendicular. As in the previous case I tried to show that when g and h are not in perpendicular direction then I can find a $m\mathbb{Z}; |h^{-1}g^{m}(P)-P|<|h(P)-p|$ but having little information about this composition I got little success in this approach. Any hint is appreciated. Definition 1: A fixed point free group of isometries is a group $G$ such that for all $g\in G-{e}$ and $x\in \mathbb{R}^{2}$ we have $g(P)\neq P$. Here $e$ is the identity. Definition 2: A glide reflection is the composition of a reflection in a line and a translation along that line.","['isometry', 'euclidean-geometry', 'group-theory']"
2329128,Graphical explanation for non-differentiability of $z \mapsto \bar z$,"Let $f(z) = \bar z$ where $z \in \mathbb C$ and $\bar z$ is the complex conjugate of $z$. I can prove that $f$ is nowhere differentiable, but I can't picture it. With a real-valued function I have a good mental picture of what a lack of differentiability looks like, but I don't have that here. Plotting the real and imaginary parts with Wolfram Alpha , I get this: . I just don't see the lack of differentiability in this. Any clarification would be very appreciated.","['derivatives', 'complex-analysis', 'complex-numbers', 'graphing-functions']"
2329172,Density of analytic functions in Sobolev spaces,"I've seen this mentioned in a few physics books (Hawking-Ellis, Stewart), but never with any details. Let $(M,g)$ be an analytic Riemannian manifold, and $U\subset M$ an open set with compact closure. Define the Sobolev space $H^k(U)$ as usual. Is $C^\omega(\bar U)$ dense in $H^k(U)$? I think a proof could make use of Whitney's $C^k$-norm analytic approximation theorem (Hirsch, p. 66, or maybe the stronger Grauert-Remmert approximation theorem), but I'm not sure how to proceed without partitions of unity. Namely, if $\bar U$ is contained in a chart with compact closure, one should dominate the Sobolev norm by the $C^k$ norm:
$$||f||_{H^k}\le C\sup_{0\le p\le k}|D^p f|,$$
where $C$ depends on $\dim M$, and bounds of Christoffel symbols, curvature tensors, and derivatives of curvatures (hence it is important that $U$ is precompact). Whitney's theorem then gives $C^\omega\subset C^k$ being dense in $H^k$-norm, which gives density in $H^k$. I don't understand Hirsch's proof of Whitney's analytic atlas theorem, which might be useful? I'm looking an idea of how to proceed. Also, what is the situation when $\bar U$ is noncompact?","['riemannian-geometry', 'differential-geometry', 'partial-differential-equations']"
2329173,What can we say about $\sum x_n$ if $\lim n x_n =0$?,"I proved that if $(x_n)$ is a decreasing sequence of non-negative real numbers such that $\sum x_n$ converges, then $\lim nx_n=0$. I ask about the other direction. If $(x_n)$ is any sequence of real numbers such that $\lim nx_n=0$ it is true that $\sum x_n$ converges?
What if $(x_n)$ is also a decreasing sequence of non-negative real numbers? It seems to me that the answer to both of these questions is no ! So I searched counter-examples. Unfortunatelly I couldn't find any. Neither could I prove these statements. Can someone help me? Thanks!","['real-analysis', 'sequences-and-series']"
2329176,Probability of the occurrence of a random event x in a population X where n = $\infty$? [duplicate],"This question already has answers here : Is getting a random integer even possible? (2 answers) Closed 7 years ago . Given a random event x from a discrete population X where the population $n=\infty$, and the population is uniformly distributed, what is P(x)? My intuition is that it is infinitesimal, because as $lim_{n\to\infty}$, $ lim_{p\to0}$","['probability', 'limits']"
2329183,Dynamical system defined with a non-abelian group,"Soft question. I'm taking an introductory mini-course in dynamical systems, and the professor defined a continuous dynamical system in a topological space $M $ (or metric space, smooth manifold, or whatever) as a map (continuous, smooth, etc) as a map  $\Phi:M\times \Bbb R \to M$ satisfying (i) $\Phi(\cdot,0)={\rm Id}_M$ and (ii) $\Phi(\Phi(x,t),s)=\Phi(x,t+s)$ for all  $x\in M$ and $t,s\in \Bbb R$. He also defined a discrete dynamical system as same as above with $\Bbb Z$ instead of $\Bbb R$, and made a remark saying that we could use a group instead of $\Bbb R$ or $\Bbb Z$, which I guess it would be like this: let $G$ be a group and ask $\Phi:M\times G\to M$ to satisfy (i') $\Phi(\cdot,e_G) ={\rm Id}_M$ and (ii') $\Phi(\Phi(x,g),h)=\Phi(x,g\cdot h)$ for all $x\in M$ and $g,h\in G$. Although we'd probably want $G$ to be a nice topological group or Lie group (we could start talking about continuity or smoothness of $\Phi$), what I'd like to know is: what interesting stuff can be modeled using a dynamical system like this? I'm curious about the usefulness of non-abelian groups in this context, since we'd probably want the parameter space $G$ to represent instants in time.","['mathematical-physics', 'applications', 'ordinary-differential-equations', 'dynamical-systems']"
2329197,"$Hom(\mathcal{P},\mathcal{P'})$ is additive abelian group for $\mathcal{P},\mathcal{P'}\in pSh(X,Ab)$","This is problem 5.38(ii) of Rotman Homological Algebra. I am not certain about my last step of showing the group is additive. Let $P,P'\in pSh(X,Ab)$ where $pSh(X,Ab)$ is presheaf category of abelian groups over a topological space $X$. Natural transformation $Hom(\mathcal{P},\mathcal{P'})$ is an additive abelian group. It is clear that I have a $0$ map in $Hom(\mathcal{P},\mathcal{P'})$ by $0-sheaf$ is zero object in presheave category with fixed $X$. Every element has an inverse by sending $F\in Hom(\mathcal{P},\mathcal{P'})$, $-F\in Hom(\mathcal{P},\mathcal{P'})$ and one can check $F_U-F_U$ for any $U\subset X$ open resulting in $0$ map. I am not very clear on the additive property. Suppose I take $F,G\in Hom(\mathcal{P},\mathcal{P'})$. Then for any $\mathcal{U}\subset X$, $F_\mathcal{U}:\mathcal{P(U)}\to\mathcal{P'(U)}$. It is clear that I can perform the operation $F_\mathcal{U}+G_\mathcal{U}$ on each $U\subset X$ open. So I can define $(F+G)_\mathcal{U}:=F_\mathcal{U}+G_\mathcal{U}$ Similarly given any $F_1:P_1\to P_2$ and $F_2,F_3:P_2\to P_3$ where $P_i$ are presheaves of abelian groups. $F_i$ are natural transformations. I could check that $F_1\circ(F_2+F_3)=F_1\circ F_2+F_1\circ F_3$ on the abelian group level. The following are my questions. Can I define $(F+G)_\mathcal{U}=F_\mathcal{U}+G_\mathcal{U}$ for each $U\subset X$? Was there a easier trick to check $(F_2+F_3)\circ F_1=F_2\circ F_1+ F_3\circ F_1$ without checking the equality on abelian group level?","['abstract-algebra', 'sheaf-theory', 'homological-algebra', 'algebraic-geometry']"
2329199,Continuous integral function $F$,"If $f$ is continuous on rectangle $[a,b]\times[c,d]$, and if $g\in R$ on $[a,b]$, then the function $F$ defined by equation $$F(y)=\int_a^bg(x)f(x,y)dx$$ is continuous on $[c,d]$. Proof: If $G(x)=\int_a^xg(t)dt$, then by Theorem 1, $F(y)=\int_a^bf(x,y)dG(x)$. Now by Theorem 2 , $F(y)=\int_a^bg(x)f(x,y)dx$ is continuous on $[c,d]$. Theorem 1: Suppose $f\in R(\alpha)$ and $g\in R(\alpha)$ on $[a,b]$ where $\alpha$ is increasing on $[a,b]$. We define $F(x)=\int_a^xf(t)d\alpha (t)$ and $G(x)=\int_a^xg(t)d\alpha (t)$ if $x\in [a,b]$. Then $f\in R(G),g\in R(F), fg\in R(\alpha )$ on $[a,b]$ and $\int_a^bf(x)g(x)d\alpha (x)=\int_a^bf(x)dG(x)=\int_a^bg(x)dF(x)$. Theorem 2: Let $f$ be a continuous function in each point $(x,y)$ of rectangle $[a,b]\times[c,d]$. Suppose $\alpha$ is bounded variation on $[a,b]$ and let $F$ be the function defined on $[c,d]$ by $F(y)=\int_a^bf(x,y)d\alpha (x)$. Then $F$ is continuous on $[c,d]$. How the proof apply theorem 1 ? I don't see it. When using theorem 2, how do we know that g is bounded variation?","['riemann-integration', 'continuity', 'integration', 'proof-explanation', 'analysis']"
2329202,Infinitesmal generator of gauge group action,"This is from Chapter 3 of Uhlenbeck and Freed's Instantons and Four-Manifolds . As a quick setup of the relevant notation, we fix a complex rank $2$ bundle $\eta$ with gauge group $SU(2)$ and examine the space of connections. Let $P$ be the associated frame bundle, itself a principal $SU(2)$-bundle. The gauge transformations of $\eta$ are then just the bundle automorphisms of $P$. The connections on $\eta$ can be identified with the affine space of $1$-forms on the adjoint bundle $\text{ad} P = P \times_{SU(2)} \mathfrak{su}(2)$, written as $\Omega^1(\text{ad} P)$. Furthermore, any connection $D$ also has an associated covariant derivative on $\text{ad} P$, so it is realized as a differential operator $D: \Omega^0(\text{ad} P) \to \Omega^1(\text{ad} P)$. The group of gauge transformations acts on a connection $D$ by pullback, i.e. $s^*(D)(\sigma) = s^{-1}D(s \sigma)$ where $s$ acts on $\sigma$ by conjugation. We can also realize the group of gauge transformations as sections of the bundle $\text{Ad} P = P \times_{SU(2)} SU(2)$, where the group action is conjugation. The Lie algebra of this group is therefore clearly $\Omega^0(\text{ad} P)$. Given the action of the gauge group, the authors then claim the infinitesmal action of a Lie algebra element $u \in \Omega^0(\text{ad} P)$ on a connection $D \in \Omega^1(\text{ad} P)$ is equal to $Du \in \Omega^1(\text{ad} P)$. I wrote out the definition of infinitesmal action, but I am not sure how exactly to evaluate the relevant derivative. Can anyone provide a hint/insight into this calculation?","['gauge-theory', 'differential-geometry', 'differential-topology']"
2329212,Uniform convergence and bounded variation functions,"Is this proof correct? I did it myself so I'm not sure if it's right. Let ${f_k}$ be a sequence of bounded variation on $[a,b]$ such that for each $x\in [a,b]$ $f(x)=\lim_{k\to \infty}f_k(x)$. If there exist $K>0$ such that $V(f_n,[a,b])\le K$ for all $n\in \mathbb N$, then $f$ is bounded variation on $[a,b]$. Proof: Let $\epsilon >0$. Then $V(f,[a,b])=\sum_{i=1}^{k}|f(x_i)-f(x_{i-1})|=\sum_{i=1}^{k}|\lim_{n\to \infty}f_n(x_i)-\lim_{n\to \infty}f_n(x_{i-1})|=\sum_{i=1}^{k}\lim_{n\to \infty}|f(x_i)-f(x_{i-1})|=\lim_{n\to \infty}\sum_{i=1}^{k}|f(x_i)-f(x_{i-1})|=\lim_{n\to \infty}K=K$. Therefore $f$ is bounded variation on $[a,b]$. Please tell me if it is correct or if it's not.","['uniform-convergence', 'limits', 'proof-verification', 'bounded-variation', 'analysis']"
2329217,Product rule for scalar-vector product,"Let $\mathbf F : \mathbb R^p \to \mathbb R^s$ and $\phi : \mathbb R^p \to \mathbb R$ be differentiable functions. Let the function $\mathbf G$ be defined as follows:
$$\mathbf G : \mathbb R^p \to \mathbb R^s \qquad  \mathbf G(\mathbf y) = \phi(\mathbf y)\mathbf F(\mathbf y)$$ Furthermore, let $y_0$ be a point in $\mathbb R^p$. Then the Jacobian of $\mathbf G$ and of $\mathbf F$ at $y_0$, denoted respectively $D\mathbf G(y_0)$ and $D\mathbf F(y_0)$ are $s \times p$ matrices, whereas the Jacobian of $\phi$ at $y_0$, denoted $D\phi(y_0)$, is a row vector $p$ entries long and may thus be turned into a gradient:
$$\nabla \phi(y_0) \doteq D\phi(y_0)^\top$$ Now the question is, how can I express $D\mathbf G(y_0)$ in terms of the other two Jacobians? I tried recklessly applying the product rule,
$$D\mathbf g(y_0) \stackrel{?}{=} \phi(y_0) D\mathbf F(y_0) + D\phi(y_0) \mathbf F(y_0) $$
but the dimensions of the matrices do not match up correctly. What am I doing wrong?","['derivatives', 'vector-analysis']"
2329223,Discrete Mathematics: Predicate Logic,"Is the following implication valid? $(∃(x))(P(x)∨Q(x))⟹\lnot (∀(x))P(x)∨(∃(x))Q(x)$ For proving this I used to follow a method to by making L.H.S as 1 and try to make R.H.S as 0. My work:- I am having a small trouble while reading the R.H.S of the implication.The scope of the negation is bounded to only Universal quantifier or to the entire statement? If the R.H.S is :- $\lnot ∀(x)P(x)∨(∃(x))Q(x)$,then it means the negation of the entire statement.But as there are braces, I am getting a bit confused.As I have read that Quantifiers has higher precedence, does this mean it is the negation of only the quantifier? How should I read it:-
 $(∃(x))P(x)∨(∃(x))Q(x)$ or $(∃(x))\lnot P(x)∨(∃(x))Q(x)$","['predicate-logic', 'quantifiers', 'discrete-mathematics']"
2329233,"Given $A$, is there a quick way to compute the $(i,j)$th entry of $A^n$","I am studying elementary graph theory and there is a theorem which states that if you take the nth power of the adjacency matrix, the $(i,j)$ entry corresponds to the number of walks from $v_i$ to $v_j$ with length $n$. Is there a quick way to compute that one entry without having to compute the whole matrix? For example: $$A = \begin{bmatrix}
0 & 2 & 0\\ 
2 & 0 & 1\\ 
0 & 1 & 1
\end{bmatrix}$$ and $$A^2 = \begin{bmatrix}
4 & 0 & 2\\ 
0 & 5 & 1\\ 
2 & 1 & 2
\end{bmatrix}$$ This means that there are 5 walks of length 2 from v2 to v2 in the graph represented by A. Is there a method to directly compute the $(2,2)$ entry of $A^2$ without computing the whole matrix? What about other powers like $n = 3$ or $n = 4$?","['matrices', 'graph-theory']"
2329245,Are there Complex eigenvalues in a given matrix?,"If a matrix is Hermitian, then its eigenvalues are all real. But given any real matrix that is not Hermitian , how to determine whether there are complex eigenvalues or not? Or the question can be re-formulated in this way: rather than Hermitian, are there any more general rules which can be used to determine whether the eigenvalues of a matrix are all real numbers? EDIT:
I could think of another solution to this (it is only an indirect way compared with the Hermitian matrix): If a matrix is similar to a Hermitian matrix, then its eigenvalues are all real.","['matrices', 'eigenvalues-eigenvectors', 'linear-algebra']"
