question_id,title,body,tags
4844184,Proving limit to infinity using epsilon - N definition,So I'm practicing with proving limits at infinity using $\epsilon$ - $N$ definition. And I ran into this question in my textbook: $$\lim_{x\to\infty}\frac{x-3}{3x-1}=\frac{1}{3}$$ So far I have: $$\left\vert\frac{x-3}{3x-1} - \frac{1}{3}\right\vert = \left\vert\frac{-8}{9x-3}\right\vert  < \epsilon \quad\Rightarrow\quad x > \frac{8}{9\epsilon} + \frac{1}{3} = N$$ and I am not sure how to proceed at this point. Can someone clarify the next steps? Thanks.,"['limits', 'calculus']"
4844217,Digit sum equality $S(a^n + n) = 1 + S(n)$ implies that $a$ is a power of ten,"Let $a$ be a positive integer such that for the digit sum $S(\cdot)$ the equality $S(a^n + n) = 1 + S(n)$ holds for every
sufficiently large $n$ . Then $a$ is a power of ten. So I know that $$
S(a+b)=S(a)+S(b)-9c(a,b),
$$ where $c(a,b)$ is the number of carryovers when adding $a$ and $b$ . Using this gives $$
S(a^n) +S(n)-9c(a^n,n) = 1 + S(n)
$$ and hence $$
S(a^n)=1+9c(a^n,n).
$$ Now if we set e.g. $a=10$ , we get $$
S(10^n)=1+9c(10^n,n)
$$ and $$
9c(10^n,n)=0
$$ for large $n$ , so the formula makes sense. But the thing that we have to prove is that our condition definitely gives $a=10^z$ . So maybe this way of proving it is wrong, maybe one has to assume that $a\neq 10$ in the beginning. Any help would be great, this is a problem for olympiad training.","['contest-math', 'number-theory', 'arithmetic']"
4844251,Strange results from implicit differentiation,"Disclaimer: this is probably just a giant misunderstanding of implicit differentiation, but I'm not sure where! I'm looking at an initial-value problem for a simple damped harmonic oscillator but where damping is proportional to the square of the velocity instead: $$ m\ddot{x} + kx + b\dot{x}^2 = 0 $$ for the initial conditions: $$x(0) = 0$$ $$\dot{x}(0) = v $$ where $v>0$ is the initial velocity and only from $t=0$ until $\dot{x} = 0$ (to ensure damping always points in the correct direction opposing velocity, as in the above formula!). In particular, I'm interested in the maximum/minimum values of the acceleration $\ddot{x}$ , which will occur when $\dddot{x} = 0$ . So, if we differentiate again with respect to time $t$ , we obtain $$ m\dddot{x} + k\dot{x} + 2b\dot{x}\ddot{x} = 0 $$ and thus when $\dddot{x} = 0$ , it will follow that $$ k\dot{x} = -2b\dot{x}\ddot{x} \Rightarrow \boxed{\ddot{x} = -\frac{k}{2b}}, \text{when }\dot{x} \ne 0$$ But, this makes no sense! It suggests that $\min\ddot{x}\to -\infty$ as $b\to 0$ (as in the undamped case). So, there must be a mistake (or misunderstanding) here, but I can't seem to determine where! *Edit: when $b \ne 0$ , all minima/maxima values of $\ddot{x}$ occur when $\dot{x}\ne 0$ anyway, so this isn't a division-by-zero thing either... *Edit #2: the above statement that $\dot{x} \ne 0$ at extreme values of $\ddot{x}$ may not be so true after all -- the numerical methods I use for IVP's to $$ m\ddot{x} + kx + b\dot{x}^2 = 0 $$ only list very small differences between the times when $\dot{x} = 0$ and when $\ddot{x}$ is at a maximum/minimum. So, it may actually be the case that every extreme value of $\ddot{x}$ occurs when $\dot{x} = 0$ , and that my numerical solutions are simply just way too imprecise (I'm using WinPlot ) to identify whether there is a real difference or not! I'm also thinking this is case because my math here appears to be correct...","['ordinary-differential-equations', 'calculus', 'implicit-differentiation', 'physics', 'derivatives']"
4844268,"""Let $G$ be a finite group with a normal subgroup $N\cong S_3$. Show there is $H\le G$ s.t. $G=N\times H$."" Does this $=$ really mean $\cong$?","I'm attempting Problem 60 on this problem set . Let $G$ be a finite group with a normal subgroup $N\cong S_3$ . Show that there is a subgroup $H$ of $G$ such that $G = N \times H$ . I'm guessing the author meant to say "" $G\cong N\times H$ "" instead of what he wrote? What do you think? Edit: I just saw he repeats this notation in other exercises, like A group $N$ is said to be complete if the center of $N$ is trivial and every automorphism of $N$ is inner. Show that if $G$ is a group, $N\lhd G$ , and $N$ is complete, then $G = N \times C_G(N)$ . So maybe it's a lazy notation for $\cong$ ? Or does it mean something else?","['direct-product', 'group-isomorphism', 'notation', 'normal-subgroups', 'group-theory']"
4844289,Break a stick at two random points. The probability that the longest piece is at least twice as long as each of the other pieces is $1/2$. Why?,"Choose two independent uniformly random points on a stick, and break the stick at that those points. The probability that the longest piece is at least twice as long as each of the other pieces is $1/2$ . My proof below is fairly technical. Given the simplicity of the final answer, I am looking for  a more intuitive explanation that does not involve so much calculation, possibly based on symmetry. ( Here is another question of mine about a breaking a stick at random points, that has an intuitive explanation. And here is one with a simple answer but perhaps no intuitive explanation.) My non-intuitive proof: Assume the stick has length $1$ . Hold the stick horizontally. $x=$ first chosen point's distance from left end. $y=$ second chosen point's distance from left end. In the graph, the blue regions contain combinations that meet the condition in the question. The red region contains combinations that do not meet the condition. The regions can be reflected across the diagonal, by symmetry. I worked out the lines by considering $x$ -values one by one, starting from $x=0$ and going up in small increments. Critical points emerged: $(0,\frac13),(0,\frac23),(\frac14,\frac12),(\frac14,\frac34),(\frac13,\frac13),(\frac13,1),(\frac12,\frac34),(\frac23,\frac23),(\frac23,1)$ . The areas of the blue and red regions are equal, so the probability that the condition is met is $1/2$ . Related fact: The probability that the longest piece is at least twice as long as the shortest piece is exactly $90\text{%}$ (an amusing geometrical probability).","['intuition', 'geometric-probability', 'geometry', 'probability']"
4844341,Mixed-radix maximal digits $\sum_{i=0}^\infty \frac{(2i)i!}{(2i+1)!!} = 2$,"In A Spigot Algorithm for the Digits of π by Rabinowitz and Wagon , the following lemma says the ""maximal digit"" representation $(0;2,4,6,8\dots)$ in the mixed-radix base $(\frac 1 3, \frac 2 5, \frac 3 7, \dots)$ is bounded by 2: $$0 + \frac 1 3 \left(2 + \frac 2 5 \left( 4 + \frac 3 7 ( 6 + \cdots ) \right) \right) = \sum_{i=0}^\infty \frac{(2i)i!}{(2i+1)!!} = 2$$ They sketch a proof by induction based on remainders: the differences of the partial sum and 2. Is there a direct proof based on combinatorial interpretation or well-known series? The $\pi/2$ formula of $(1;1,1,1,1,\dots)$ is apparently due to Newton and can be derived from the arctan Taylor series.","['factorial', 'taylor-expansion', 'sequences-and-series']"
4844346,Can a Fréchet-Urysohn hemicompact Hausdorff space fail to be locally compact?,"This recent question led to some discussion on hemicompactness.  A topological space $X$ is said to be hemicompact if there is an increasing sequence of compacta $K_1\subseteq\dots \subseteq K_n\subseteq \dots$ such that every compact $K\subseteq X$ lies in some $K_n$ . This is always the case when $X$ is $\sigma$ -compact and weakly locally compact (every point has a compact neighborhood), and the converse is true under the additional assumption of first countability (since a hemicompact space is trivially $\sigma$ -compact, the nontrivial part of the converse is the deduction of weak local compactness). On the other hand, here it was shown that we cannot relax the first countability assumption to sequentialness.  The counterexample is the Arens space , which is sequential (though not Fréchet-Urysohn ) and hemicompact, yet not weakly locally compact.  It should be noted that the Arens space has rather good separation properties ( $T_6$ ). A natural question becomes, given that sequentialness is too weak, can we relax first countability to the Fréchet-Urysohn property? The short answer is no, at least not without further assumptions.  I'll describe a counterexample next, then conclude with the titular question: Counterexample. Let $X=\mathbb R\cup \infty$ , where $\mathbb R$ has the Euclidean topology and neighborhoods of $\infty$ are those of the form $\{\infty\}\cup\mathbb R\backslash D$ , where $D\subset \mathbb R$ is discrete and closed (equivalently, $D$ has no limit points in $\mathbb R$ ). Lemma. $K\subseteq X$ is compact if and only if either $K\subseteq \mathbb R$ is Euclidean compact, or $\infty\in K$ and $K\cap \mathbb R$ is bounded. Proof. If $K\subseteq \mathbb R$ , then since $\mathbb R$ has the Euclidean topology $K$ is compact if and only if $K$ is Euclidean compact.  On the other hand, if $\infty\in K$ , and $K\cap \mathbb R\subseteq [-M,M]$ , then every open cover of $K$ has a member $U\ni \infty$ , whereby $\mathbb R\backslash U$ is closed and discrete, so that $K\backslash U\subseteq [-M,M]\backslash U$ is finite. Finally, if $K\cap \mathbb R$ is unbounded, then let $x_n\in K\cap \mathbb R$ be a sequence eventually leaving every bounded subset of $\mathbb R$ .  Then the sets $D_n=\{x_k\mid k\geq n\}$ are discrete and closed, and so the sets $U_n= \{\infty\}\cup\mathbb R\backslash D_n$ form an open cover of $K$ with no finite subcover. Corollary. $X$ is hemicompact, Fréchet-Urysohn, and not weakly locally compact. Proof. Every neighborhood of $\infty$ intersects $\mathbb R$ in an unbounded set, so there are no compact neighborhoods of $\infty$ , hence $X$ is not weakly locally compact.  On the other hand, the sets $K_n:=[-n,n]\cup\{\infty\}$ are an increasing sequence of compacta in $X$ , and every compact $K\subset X$ lies in some $K_n$ , so $X$ is hemicompact. Finally, to verify the Fréchet-Urysohn property, since every point in $\mathbb R$ has a countable basis, it suffices to consider $S\subseteq  \mathbb R$ with $\infty\in \overline{S}$ .  In this case, $S$ must have some limit point $x\in \mathbb R$ (otherwise $\{\infty\}\cup \mathbb R\backslash S$ is a neighborhood of $\infty$ ).  But then if $s_n\in S$ is a (distinct) sequence with $s_n\to x\in\mathbb R$ , then $s_n$ eventually leaves every closed discrete subset of $\mathbb R$ , whereby $s_n\to\infty$ as well. Question. Now, we could let the matter rest there, but compared to the Arens space, the space $X$ constructed above has rather poor separation properties:  it is $T_1$ , but just barely, in the sense that it is not even $US$ (convergent sequences do not have unique limits, as seen from the proof above). Is there a Hausdorff counterexample, i.e., is there a Fréchet-Urysohn hemicompact Hausdorff space that fails to be locally compact*?  More generally, if a space is hemicompact and Fréchet-Urysohn, might any separation axioms, anywhere from $US$ through to $T_6$ **, guarantee weak local compactness? (* Note that in the Hausdorff case, weak local compactness and local compactness coincide - the latter term indicating the existence of a basis of compact neighborhoods at each point.) (** See here for an in depth description of a chain of properties between $T_2$ and $T_1$ , with the weakest being the aforementioned $US$ .)","['separation-axioms', 'general-topology', 'examples-counterexamples', 'compactness']"
4844390,Uniform Convergence of the Sequence $f_n(x) = n(x^{1/n} - 1)$ to $\ln (x)$,"I am examining the uniform convergence of the sequence of functions $\{f_n\}$ defined by $$ f_n(x) = n(x^{1/n} - 1) $$ on intervals of the form $(1/k, k)$ for $k \in \mathbb{N}$ . I am particularly interested in understanding whether this sequence converges uniformly to $f(x) = \ln(x)$ on these intervals. Problem Statement: The sequence $\{f_n\}$ is given by: $$ f_n(x) = n(x^{1/n} - 1) $$ We aim to show that $\{f_n\}$ converges uniformly to $f(x) = \ln(x)$ on intervals of the form $(1/k, k)$ . Analysis of Pointwise Convergence It is known that $\{f_n\}$ converges pointwise to $f(x) = \ln(x)$ on $(0, \infty)$ . This is established through the limit: $$ \lim_{n \to \infty} f_n(x) = \ln(x) $$ Analysis of Uniform Convergence on $(1/k, k)$ The confusion arises when examining the uniform convergence on the intervals $(1/k, k)$ . Specifically, the points of contention are: For $x = 1/k$ and $x = k$ , the limits of the differences $|f_n(x) - \ln(x)|$ as $n \to \infty$ do not tend to $0$ , but instead, they approach $|\ln(k)|$ and $|\ln(1/k)|$ , respectively. Supremum of the Difference: Despite the behavior at the endpoints, there is a claim that the supremum of the difference $|f_n(x) - \ln(x)|$ over the interval $(1/k, k)$ goes to $0$ as $n \to \infty$ . This would imply uniform convergence, but it's unclear how this aligns with the non-zero limits at the endpoints. How does the behavior of $|f_n(x) - \ln(x)|$ at the endpoints $x = 1/k$ and $x = k$ influence the argument for uniform convergence on $(1/k, k)$ ? If the supremum of the difference on the interval $(1/k, k)$ indeed goes to $0$ , how does this reconcile with the endpoint behavior where the differences approach non-zero constants? Any insights or clarifications on this matter would be greatly appreciated.","['functions', 'uniform-convergence', 'real-analysis']"
4844439,Consider a man who travelled exactly 2 km in two hours. Is there a one-hour interval when he traveled exactly 1 km?,"Question : Consider a man who travelled exactly 2 km in two hours. Is there a one-hour interval when he traveled exactly 1 km? Can we make a mathematical argument? I have written my attempt in an answer below. Does anyone else have a better approach? Are my assumptions not necessary, or we can produce counter examples without them? Can we make-do with some intermediate assumptions? (This is not the same as the Universal Chord question because there's no $f$ with $f(0)=f(1)$ and there's no continuity assumption. Answers to the cyclist question are not satisfactory.)","['applications', 'calculus', 'solution-verification', 'intuition', 'algebra-precalculus']"
4844441,Is there any connection between Legendre relation and Ramanujan's formula?,"Background Consider the Legendre duplication formula: $$
\begin{aligned}
\prod_{i=1}^{k-1}Γ\left(n+\frac{i}{k}\right)=(2π)^{\frac{k-1}{2}}k^{\frac{1}{2}-nk}Γ(nk)
\end{aligned}
$$ I chose different k values to calculate the Γ product table, but I discovered something strange in the calculation. When written in factorial form, it seems to have some connection with Ramanujan's reciprocal formula of π
​ $$
\begin{aligned}
Γ\left(n+\frac{1}{2}\right)Γ\left(n+\frac{1}{2}\right)Γ\left(n+\frac{1}{2}\right)=8π^{3/2}\frac{\color{red}{(2 n)!^3}}{2^{6n}×(2n)^3}
\end{aligned}
$$ $$
\begin{aligned}
\frac{1}{π}=\frac{1}{2}\sum_{k=0}^∞(-1)^k\left(\frac{\color{red}{(2 k)!}}{(k!)^2}\right)^{\color{red}{3}}\frac{(4 k+1)}{2^{6k}} 
\end{aligned}
$$ $$
\begin{aligned}
Γ \left(n+\frac{1}{2}\right) Γ \left(n+\frac{1}{3}\right) Γ \left(n+\frac{2}{3}\right)=\frac{2\color{blue}{\sqrt{3}}π^{3/2}}{3}\frac{\color{red}{(2 n)!(3 n)!}}{n^2×2^{2n}×3^{3n}}
\end{aligned}
$$ $$
\begin{aligned}
\frac{1}{π}=\frac{\color{blue}{\sqrt{3}}}{36}\sum_{k=0}^∞\frac{\color{red}{(2k)!(3k)!}}{(k!)^5}\frac{(51 k+7)}{(-2^6×3^3)^{k}} 
\end{aligned}
$$ $$
\begin{aligned}
Γ \left(n+\frac{1}{2}\right) Γ \left(n+\frac{1}{4}\right) Γ \left(n+\frac{3}{4}\right)=\color{blue}{\sqrt{2}}π^{3/2}\frac{\color{red}{(4 n)!}}{n× 4^{4n}}
\end{aligned}
$$ $$
\begin{aligned}
\frac{1}{π}=\frac{2\color{blue}{\sqrt{2}}}{9801}\sum_{k=0}^∞\frac{\color{red}{(4k)!}}{(k!)^4}\frac{(58⋅455 k+1103)}{396^{4k}} 
\end{aligned}
$$ $$
\begin{aligned}
Γ\left(n+\frac{1}{2}\right) Γ\left(n+\frac{1}{6}\right) Γ\left(n+\frac{5}{6}\right)=2 π ^{3/2} \frac{\color{red}{(6 n)!}}{1728^n (3 n)!}
\end{aligned}
$$ $$
\begin{aligned}
\frac{1}{π}=\frac{\sqrt{10005}}{66733350}\sum_{k=0}^∞\frac{\color{red}{(6 k)!}}{(3k)!}\frac{(163⋅3344418k+13591409)}{(-2)^{15k+6} (20010^k⋅k!)^3} 
\end{aligned}
$$ Question Is there any connection between the two, or is it just a coincidence? If it is not a coincidence, does the following equation predict the existence of Ramanujan's equation for $π^2$ ? $$
\begin{aligned}
Γ\left(n+\frac{1}{2}\right) Γ\left(n+\frac{1}{5}\right) Γ\left(n+\frac{2}{5}\right) Γ\left(n+\frac{3}{5}\right) Γ\left(n+\frac{4}{5}\right)
&=π^{5/2} 2^{3-2 n} 5^{\frac{1}{2}-5 n} Γ(2 n) Γ(5 n)\\
Γ\left(n+\frac{1}{2}\right) Γ\left(n+\frac{1}{8}\right) Γ\left(n+\frac{3}{8}\right) Γ\left(n+\frac{5}{8}\right) Γ\left(n+\frac{7}{8}\right)
&=π^{5/2} 2^{\frac{7}{2}-18 n} \frac{Γ(2 n) Γ(8 n)}{Γ(4 n)}\\
Γ\left(n+\frac{1}{2}\right) Γ\left(n+\frac{1}{12}\right) Γ\left(n+\frac{5}{12}\right) Γ\left(n+\frac{7}{12}\right) Γ\left(n+\frac{11}{12}\right)
&=π^{5/2} 2^{3-14 n} 3^{-6 n} \frac{Γ(2 n)^2 Γ(12 n)}{Γ(4 n) Γ(6 n)}
\end{aligned}
$$",['sequences-and-series']
4844450,"If $(f(x)-x)f''(x)>0$ , then $f(x)=f^{-1}(x)$ has no solution.","If $f:\mathbb{R}\to\mathbb{R}$ is a double differentiable bijective function, then which of the following statements may be true ? (A) $(f(x)-x)f''(x)\leq 0$ for all $x\in\mathbb{R}$ (B) $(f(x)-x)f''(x)\geq 0$ for all $x\in\mathbb{R}$ (C) If $(f(x)-x)f''(x)>0$ , then $f(x)=f^{-1}(x)$ has no solution. (D) If $(f(x)-x)f''(x)>0$ , then $f(x)=f^{-1}(x)$ has at least one real solution. My Attempt Clearly if $f(x)=e^{-x}$ then both (A) and (B) are NOT true. If I take $f(x)=e^x$ then (C) appears to be true. But is there a general explanation","['calculus', 'inverse-function', 'derivatives', 'real-analysis']"
4844497,Combinatorics under specific conditions,"There is problem called D. Count the Arrays on Codeforces. Array is set of elements. In this case, set can (and have to) contain one duplicate I started learning combinatorics but I still can't solve it. Here's the formula I derived: For first two conditions, I started with: $\binom{m}{n}$ . There are n possible positions on which any element from 1 to m can be placed. For the third condition, I made it: $\binom{m}{n-1} * \binom{n-1}{1}$ because we will have one fixed element which will be duplicate. I use $n$ and not $m$ because not all elements available from $m$ are included in $n$ available positions. The duplicate, as the word says, must be the same as some of included elements. Then I substract $1$ because due to 4-th condition, the element at index i will never have duplicate. For the fourth condition, I have no idea. Due to this point I know that the first duplicated element will be before i-th element and the another after i-th . But how can I count only arrays that are ascending to some point and then descending? I see too many possibilites and can't derive formula for it. Any idea? How would you approach this problem?","['combinations', 'combinatorics']"
4844505,"Probability that among $n$ random people, at least two have coinciding or successive birthdays","I found the following riddle: What is the probability that among $n$ people chosen randomly, at least two of them have their birthdays with at most one day difference. Note: December 31 and January 1 are considered as having one day difference. For simplicity, we will consider that a year always contains 365 days. I've found that the probability is $$P\left( n\right) =
1-\prod\limits_{i=n+1}^{2n-1}
\left( 1-\frac{i}{365}\right) $$ Numerically, it gives the following (for example): $P\left( 14\right) \approx 53.75$ , $P\left( 24\right) \approx 90.86$ , and $P\left( 33\right) \approx 99.07$ . I'm not sure about my reasoning, since it is not rigorous and the results are quite surprising. So I wonder if anyone can check if these results are true and give a simple and comprehensive solution. Thank you!","['puzzle', 'birthday', 'probability']"
4844510,"Finding supremum of a given function, over a given domain.","Consider arbitrary $0 < \lambda \leqslant n$ , $1 \leqslant p < \infty$ and $\alpha,\beta > 0$ such that $\alpha < \frac{n-\lambda}{p} < \beta.$ My goal is to compute the supremum of the function $$ f(r) = r^{-\lambda}\left[ \frac{c(n)}{-\alpha p + n} + k(n)\left( \frac{r^{-\beta p + n}}{-\beta p + n} - \frac{1}{-\beta p + n}\right)\right]$$ where $c(n)$ and $k(n)$ are unknown positive constants, over $ r \geqslant 1$ . My attempt. Since $ \alpha < \frac{n-\lambda}{p}$ , we know that $-\alpha p + n > \lambda > 0,$ which implies that the term $c(n)/(-\alpha p + n)$ is positive. Therefore, $r^{-\lambda}\frac{c(n)}{-\alpha p + n}$ attains its supremum exactly when $r = 1$ . On the other hand, the other terms are quite hard to analyze: First, the inequality $\frac{n-\lambda}{p} < \beta$ doesn't give me any conclusion on the signal of $-\beta p + n$ , which complicates the calculations; On the other hand, we know that $-\beta p + n -\lambda < 0,$ which implies that the supremum of $r^{-\beta p + n - \lambda}$ is attained when $r=1$ ; If we assume that $-\beta p + n > 0,$ then the supremum of $k(n)\frac{r^{-\beta p+n-\lambda}}{-\beta p+n}$ is also attained when $r=1$ , just from what I wrote on the last point; Now, if we keep assuming that $-\beta p + n>0$ , then the sumpremum of $-k(n)\frac{r^{-\lambda}}{-\beta p + n}$ is attained when $r \to \infty$ . Clearly, from the points I stated above it is impossible to reach any conclusion. So, I am looking for a new way of approaching this problem. Ideally, I am looking for a solution that doesn't assume anything about $-\beta p + n$ , but I strongly believe this is not possible, so a solution that assumes that $-\beta p + n > 0$ will be good enough. Furthermore, if you have any other suggestion of assumptions (perhaps changing the initial inequalities to make the exercise easier) I am open to listen to them. Thanks for any help in advance.","['real-analysis', 'functions', 'solution-verification', 'inequality', 'supremum-and-infimum']"
4844512,On the induced map in cohomology,"This comes from page 48/49 in Bott and Tu's book on Differential forms in Algebraic Topology . Let $M$ and $N$ be two manifolds and consider two projections $\pi_M : M \rightarrow N$ and $\pi_N : M \times N \rightarrow N$ . With these maps we can bring forms from both $M$ and $N$ to $M \times N$ : $$\pi_M^* : \Omega^i(M) \rightarrow \Omega^i(M \times N),$$ and $$\pi_N^* : \Omega^j(N) \rightarrow \Omega^j(M \times N).$$ With these then we can edge them to get a new $i+j$ form on $M \times N$ , $$\Omega^i(M) \times \Omega^j(N) \rightarrow \Omega^{i+j}(M \times N),$$ given by $(\omega,\eta) \mapsto \pi_M^*\omega \wedge \pi_N^*\eta.$ This map is clearly bilinear since everything is linear. By the universal mapping property for bilinear maps we have an unique induced map in the their tensor product such that $$\Omega^i(M) \times \Omega^j(N) \rightarrow \Omega^i(M) \otimes \Omega^j(N) \rightarrow \Omega^{i+j}(M\times N),$$ corresponds to $$(\omega,\eta) \mapsto \omega \otimes \eta \mapsto \pi_M^*\omega \wedge \pi_N^*\eta.$$ Now the authors claim this gives rise to a map in cohomology $$\psi : H^*(M) \otimes H^*(N) \rightarrow H^*(M \times N).$$ This is the part that I am not understanding: I believe we need to check that the map given by the universal property is a cochain map?","['differential-topology', 'homology-cohomology', 'differential-forms', 'differential-geometry']"
4844521,Will $AB$ be always equal to $AC$ in this case? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 months ago . Improve this question As the picture shows, in $\triangle ABC$ , $D$ is on $AB$ and $E$ is on $AC$ , $AD=AE$ , $DC$ and $EB$ interact at $F$ , $FB=FC$ . Question: Whether it's always true that $AB=AC$ ? If it's always true, please explain it in steps, otherwise, give a counter example. I tried to solve it, but found it hard to prove any conclusions.",['geometry']
4844526,Non-nilpotent group where the union of the upper central groups equals the whole group,"I am studying residually nilpotent groups. I am using the following defintions. Lower central series: Let $G$ be a group. Then we inductively define the lower central series of $G$ as $$\gamma_1(G) = G,\ \gamma_{i}(G) = [G,\gamma_{i-1}(G)]\ \forall i> 1.$$ Nilpotent: Let $G$ be a group. Then $G$ is said to be nilpotent if there exists an integer $c\geq 1$ such that $\gamma_c(G) = \{1\}$ . Residually nilpotent: Let $G$ be a group. Then we say that $G$ is residually nilpotent if $$ \bigcap_{n\geq 1} \gamma_n(G) = \{1\}.$$ It's clear that every nilpotent group is also residually nilpotent. However, the converse does not hold. For example, if $\mathcal{D}_\infty$ is the infinite dihedral group, then one can show that $\mathcal{D}_\infty$ is residually nilpotent but not nilpotent. My question is the following: can we define similar concepts surrounding the upper central series of a group $G$ . This series is defined as follows: $$ Z_0(G) = \{1\},\ \frac{Z_i(G)}{Z_{i-1}(G)} = Z\left(\frac{G}{Z_{i-1}(G)}\right)\ \forall i>0.$$ With the definition of residual nilpotency in mind, I was wondering if there exist non-nilpotent groups $G$ such that $$ \bigcup_{n\geq 0} Z_n(G) = G.$$ I can't seem to find any example of such a group, but I don't know why it wouldn't be possible. Has this counterpart of residually nilpotent groups been studied somewhere? I did some research, but could not find anything concrete. Thanks in advance!","['nilpotent-groups', 'group-theory', 'abstract-algebra', 'infinite-groups']"
4844564,Linear approximation VS exact value for this function in $\mathbb{R}^2$,"Consider $f: \mathbb{R}^2\to \mathbb{R}$ to be a differentiable function. We have $f\left(\frac{9}{10}, \frac{1}{10}\right) = 3$ , $f'_x\left(\frac{9}{10}, \frac{1}{10}\right) = 1$ , $f'_y\left(\frac{9}{10}, \frac{1}{10}\right) = -2$ . Using the best linear local approximation of $f$ around the point $\left(\frac{9}{10}, \frac{1}{10}\right)$ , calculate an approximate value for $f(1, 0)$ . Then, assuming that $f$ is strictly concave, say if the exact value of $f(1, 0)$ is greater or lower than the approximate value. Attempts So for the first request, I just applied Taylor series: $$f(x, y) = f(x_0, y_0) + (x-x_0)f'_x(x_0, y_0) + (y-y_0)f'_y(x_0, y_0) + O(||x-y||^2)$$ With $x_0, y_0$ the given point and hence I got $$f(x, y) = x + y + 2$$ hence $$f(1, 0) \approx 3$$ Now for the second part I don't know how to move. I thought about using the definition of concave function in two variables namely $$f(\lambda x_1 + (1-\lambda)x_2, \lambda y_1 + (1-\lambda)y_2) \geq \lambda f(x_1, y_1) + (1-\lambda)f(x_2, y_2)$$ I have no other information.
I tried to chose $\lambda = 1/2$ but I got no result since I don't know the values of the function at other points. Any help? Thank you!","['convex-optimization', 'multivariable-calculus', 'convex-analysis']"
4844565,"Finitely generated, nilpotent, torsion-free group that is also radicable","I am currently working with Mal'cev completions, using the following definition:
Let $N$ be group that is Nilpotent Torsion-free Finitely generated Then the Mal'cev completion or radicable hull is the unique group $N^\mathbb{Q}$ (up to isomorphism) that satisfies the four conditions below $N^\mathbb{Q}$ contains $N$ as a subgroup $N^\mathbb{Q}$ is nilpotent and torsion-free $N^\mathbb{Q}$ is radicable , i.e. we can take unique roots in $N^\mathbb{Q}$ : $$ \forall x\in N^\mathbb{Q}, m\in \mathbb{Z}_{>0},\ \exists! y \in N^\mathbb{Q}: y^m = x$$ For all $x\in N^\mathbb{Q}$ , there exists a power $m\in \mathbb{Z}_{>0}$ such that $x^m\in N$ Now I was wondering if there existed any groups $N$ that were equal to their Mal'cev completion. It's clear that this would be the case if and only if $N$ was finitely generated, nilpotent, torsion-free and radicable. However, I can't seem to find an example of such a group and I don't know if one exists. Does anyone have any ideas? Thanks in advance","['infinite-groups', 'finitely-generated', 'nilpotent-groups', 'abstract-algebra', 'group-theory']"
4844680,"If $g^3 = e$ for all $g \in G$, then $hgh^{-1}$ and $g$ commute for all $h,g \in G$.","This is an exercise from a lecture on Introduction to group theory and it is given in the introductory part so I am not sure what methods to use to prove this. It is given right after an exercise that if $g^2=e$ for all $g \in G$ then $G$ is abelian, so I assume this problem can be solved with similar basic algebraic manipulations but I have got stuck on it. Suppose that $g^3 = e$ for all $g \in G$ for a group $G$ . Then show that $hgh^{-1}$ and $g$ commute for all $h,g \in G$ . I've been thinking about this problem for a while but I can't figure out a way to show this. I would greatly appreciate any help.","['group-theory', 'abstract-algebra']"
4844710,"Show by Local Class Field Theory $\mathbf Q_p$ has unique Galois ext. iso. to $(Z/2Z)^2$ if $p > 2$, and unique Galois ext. iso. to $(Z/2Z)^3$ o/w.","Show that $\mathbf{Q}_p$ has a unique Galois extension isomorphic to $(Z/2Z)^2$ if $p > 2$ , and that $\mathbf{Q}_2$ has a
unique Galois extension isomorphic to $(Z/2Z)^3$ I have already completed this exercise using regular methods (hensel to classify all quadratic extensions). I have been instructed to use local class field theory but I am unsure on how to make use of things such as Artin reciprocity as I'm not sure how the norm function behaves.","['algebraic-number-theory', 'number-theory', 'alternative-proof', 'local-field', 'class-field-theory']"
4844737,"Is the function $x \mapsto \mu(A+x)$ continuous, where $\mu$ is a finite Borel measure on $\mathbb R^n$ and $A \in \mathcal B(\mathbb R^n)$","Let $\mu$ be a finite regular Borel measure on $\mathbb R^n$ and $A$ is a Borel set. I am trying to prove that $x \mapsto \mu(A+x)$ is continuous. Here $\mu$ is regular means it satisfies assumptions in this link . In fact, there are several posts on this site similar to this question, but they all assume $\mu$ is ab. continuous w.r.t Lebesgue measure $m$ . I found myself a proof without assuming this but couldn't find something wrong with it. Also, there is an answer by John Dawkins poitning out that if $\mu$ is not ab. continuous w.r.t $m$ then $x \mapsto \mu(A+x)$ is not continuous. I hope you can help me find what's wrong in my proof. Here is it: Since $\mu$ is finite and regular, $C_c(\mathbb R^n)$ is dense in $L^p(\mathbb R^n, \mu)$ . To prove $x \mapsto \mu(A+x)$ is continuous, fix one $y \in \mathbb R^n$ I use $$ |\mu(A+x) - \mu(A+y)| \leq \int |1_{A+y}(u+y-x)- 1_{A+y}(u)| \mu(du) $$ Let $g \in C_c(\mathbb R^n)$ be such that $ \| g - 1_{A+y} \|_{L^1(\mathbb R^n, \mu)} \leq \epsilon$ and since $g$ is uniformly continuous so when $h = y-x$ is going to 0, we have \begin{align*}
\int |1_{A+y}(u+h)- 1_{A+y}(u)| \mu(du) &\leq \| g - 1_{A+y} \|_{L^1(\mathbb R^n, \mu)}  + \int |g(u+h)- g(x)| \mu(du)\\
&\qquad + \int |1_{A+y}(u+h)- g(u+h)| \mu(du) \\ &< 3 \epsilon.
\end{align*} So that $x \mapsto \mu(A+x)$ is continuous. In particular, I would like to chose $\mu = \delta_{0}$ , the Dirac measure at 0, and $A$ is an open set so that $ x \mapsto \delta_{0}(A+x):= \phi(x)$ is continuous. However, $\phi^{-1}(-\infty, 1/2) = - \bar A$ is closed, which I suspect something is off but counldn't tell what went wrong","['integration', 'measure-theory', 'real-analysis', 'lp-spaces', 'functional-analysis']"
4844780,Show that $e^z-1=z e^{\frac{z}{2}} \prod_{n=1}^{\infty}\left(1+\frac{z^2}{4 n^2 \pi^2}\right)$ using Residue Theorem,"I was studying previous complex analysis exams and I came across the following question: Use a result obtained from the residue theorem to justify that following infinite product is representation is valid $$
e^z-1=z e^{\frac{z}{2}} \prod_{n=1}^{\infty}\left(1+\frac{z^2}{4 n^2 \pi^2}\right)
$$ I saw some solutions using a product expansion for $\frac{\sin(\pi z)}{\pi z}$ that didn't use the residue theorem. Our instructor also presented the formula without proof $$
f(z)=f(0) e^{\frac{f^{\prime}(0)}{f(0)}}\prod_{k=1}^{\infty}\left(1-\frac{z}{a_k}\right) e^{\frac{z}{a_k}}.
$$ Where $f(z)$ is an entire function with zeroes of order $1$ on $a_k$ .
I would like to know a reference, or a proof, for the above formula and how it is related to the residue theorem.","['complex-analysis', 'residue-calculus', 'infinite-product', 'reference-request']"
4844796,Are all solutions f(x) for f(x) = f(cos(x)) constant?,"Under a post in this forum, I found a comment ( https://math.stackexchange.com/a/46936/1173827 ) by Beni Bogosel that mentions the problem of finding all solutions for a function $f:\mathbb{R}\rightarrow\mathbb{R}$ with the property $f(x)=f(\cos x), \forall x \in \mathbb{R}$ . Out of interest, I began searching for a solution.
Immediately the solution $f(x)=0$ came to mind. The next set of solutions is $f(x)=k, k\in\mathbb{R}$ . But now I wondered if there are non-constant solutions. I figure that the domain of $f$ must be confined to the interval [-1,1], since this is the output range of $\cos(x)$ . For my first attempt I considered if $f(x)=g_1(x)+g_2(x)$ , where the new two functions are switch functions that just turn into the other if $x$ or $\cos(x)$ is the input. But this is just a rephrasing of the problem and did not get me far. Then I remembered that the question under which I found this problem was about a solution for $x=\cos(x)$ , so I decided to approach the issue from that direction. When plugging $\cos(x)$ into $f(x)$ , I get $$f(\cos(x))=f(\cos(\cos(x)))$$ But from the defining property of $f(x)$ I know that $$f(x) = f(\cos(x))=f(\cos(\cos(x)))$$ I can repeat this forever until I have an infinitely nested $\cos(x)$ inside of $f(x)$ so that $$f(x) = f(\cos(\cos(\cos( \cdots ))))$$ I know that these nested $\cos(x)$ approach a constant with the approximate value $q\approx0.739085$ . But would this not imply that $f(x)=f(q)=k$ again? Is it true then that all solutions for $f(x)=f(\cos(x))$ are just constant? Does a non-constant solution truly not exist?","['functions', 'transcendental-equations']"
4844851,Example of map to the trivial representation that does not split,"What is an example of a short exact sequence of finite dimensional $ \mathbb{C}[G] $ modules $$
0 \to W \to V \to \mathbb{C} \to 0  
$$ which does not split? In other words, what is an example of a finite dimensional complex representation $ (G,V) $ that surjects $ G $ -equivariantly onto the trivial representation $ \mathbb{C} $ but has no trivial subrepresentation? Note that if $ G $ is a linearly reductive group (for example any finite group, compact group, semisimple group, or the complex points of a compact group e.g. the general linear group) then every $ G $ representation is completely reducible so the SES must split. Pf. Since $ G $ is linearly reductive then $ V $ and $ W $ are completely reducible so $ V= \bigoplus_\rho m_\rho \rho $ and $ W= \bigoplus_\rho n_\rho \rho $ with $ \rho $ all irreducible.  Then $$
V/W= \bigoplus_\rho (m_\rho-n_\rho) \rho \cong \mathbb{C}
$$ so we must have $ m_\rho=n_\rho $ for all nontrivial $ \rho $ and $ m_\rho=1+n_\rho $ when $ \rho $ is the trivial irrep. Let $ 1 $ denote the trivial irrep. Let $ W_1 $ denote the $ 1 $ isotypic subspace of $ W $ and let $ V_1 $ denote the $ 1 $ isotypic subspace of $ V $ . Recall $ m_1=n_1+1 $ so $ W_1 $ has codimension $ 1 $ in $ V_1 $ . Since $ G $ is linearly reductive we can take a complement of $ W_1 $ in $ V_1 $ , call it $ U $ $$
V_1 = W_1 \oplus U
$$ The map $ k \to U $ is the desired splitting. What I am asking about is groups $ G $ that are not linearly reductive so this argument fails. Any counterexample is ok, but an example with $ G $ a complex linear algebraic group and the short exact sequence algebraic is best. I thought about some basic nonreductive group like the solvable affine group $ \begin{bmatrix} a & b \\ 0 & \frac{1}{a} \end{bmatrix} $ or the nilpotent Heisenberg group $ \begin{bmatrix} 1 & a & b \\ 0 & 1 & c \\ 0 & 0 & 1 \end{bmatrix} $ but I couldn't get any obvious representations to work as counterexamples.","['group-theory', 'representation-theory', 'lie-groups']"
4844875,Show that $f(A) \setminus f(B) \subseteq f(A \setminus B)$ - difficulty with quantifiers.,"I'm self-teaching and really struggling with this exercise from Terence Tao's Analysis I 4th ed, exercise 3.4.3 I did post about this question previously to ask where my error was, and my error was pointed out - but I have not since made progress. I apologise for my slow progress, self-teaching without a teacher or mentor means slow progress. Exercise: Let $A$ , $B$ be two subsets of a set $X$ , and let $f:X \to Y$ be a
function. Show that $f(A) \setminus f(B) \subseteq f(A \setminus B)$ Question: I can follow the solutions posted online but when I try to write one myself I struggle with the logic, and I think my lack of knowledge is related to quantifiers. My Attempt In Very Small Steps I'll take this in small steps so that my error(s) can be isolated. Step 1: To show $f(A) \setminus f(B) \subseteq f(A \setminus B)$ we need to show the following implication: $$y \in f(A) \setminus f(B) \implies y \in f(A \setminus B)$$ Step 2: Our next step is to understand which $x \in X$ relate to $f(A) \setminus f(B)$ . If $y \in f(A) \setminus f(B)$ then there is an $x \in A$ such that $f(x) =y \in f(A) \setminus f(B)$ . Query: Is this the same as "" for some $x \in A$ ""? Why can't we say "" for all $x \in A$ ""? So far we have established that $f(A) \setminus f(B) \subseteq f(A)$ , which is not wrong, but doesn't take into account what we're told about $f(B)$ . Step 3: Having established that $x \in A$ is relevant, we now focus on the exclusion of $f(B)$ . Comment: This is where I have trouble with quantifiers and logic. If $y \in f(A) \setminus f(B)$ then there is an $x \in A$ such that $f(x) =y \in f(A) \setminus f(B)$ but also (logical conjunction) Option 1: there is an $x \in B$ such that $f(x) = y \notin f(A) \setminus f(B)$ Option 2: for all $x \in B$ we have $f(x) = y \notin f(A) \setminus f(B)$ Query: I'm not sure which of these two is right. Intuitively the second feels right, but I can't justify it in words. Step 4: If Option 1 was correct, we can say: There is an $x \in A$ such that $f(x) = y \in f(A) \setminus f(B)$ and there is an $x \in B$ such that $f(x) = y \notin f(B)$ . This doesn't seem sufficient because it doesn't exclude the possibility of other $x \in B$ such that $f(x) =y \in f(A) \setminus f(B)$ . If Option 2 was correct, we can say: There is an $x \in A$ such that $f(x) = y \in f(A) \setminus f(B)$ and for all $x \in B$ we have $f(x) = y \notin f(B)$ . This seems sufficient to exclude all $x \in B$ . Then we can say: If $y \in f(A) \setminus f(B)$ then $x \in A \land x \notin B$ such that $f(x) = y \in f(A) \setminus f(B)$ . Step 5: Continuing Option 2, we have concluded that $x \in A \land x \notin B$ . Under $f$ this maps to $f(A) \setminus f(B)$ . Thus we conclude: if $y \in f(A) \setminus f(B)$ then $y \in f(A) \setminus f(B)$ . This is equivalent to $f(A) \setminus f(B) \subseteq f(A \setminus B)$ . $\square$ I would like to than you for your patience with this.","['elementary-set-theory', 'functions']"
4844892,Surjectivity of $T: l_2 \to l_2$ defined as $(x_n)_{n=1}^\infty \mapsto (x_n+x_{n+1})_{n=1}^\infty$,"\begin{aligned}
T: l_{2}(\mathbb{C}) &\longrightarrow l_{2}(\mathbb{C}) \\
\left(x_{n}\right)_{n=1}^{\infty} &\longmapsto\left(x_{n}+x_{n+1}\right)_{n=1}^{\infty}
\end{aligned} Any element of $l_2$ can be uniquely expressed in the basis $\{e_n\}_{n=1}^{\infty}$ : $$
\left(x_{n}\right)_{n=1}^{\infty}=\lim _{k \rightarrow \infty} \sum_{n=1}^{k} x_{n} e_{n}, \quad e_{n}=\left(\delta_{n_{j}}\right)_{j=1}^{\infty} \\
$$ I found these $e_n$ in Im $(T)$ : \begin{aligned}
v_{1} & =(1,0,0,0, \ldots) & \longmapsto & &(1,0,0,0, \ldots) \\
v_{2} & =(-1,1,0,0, \ldots) & \longmapsto & &(0,1,0,0, \cdots) \\
v_{3} & =(1,-1,1,0, \ldots) & \longmapsto & &(0,0,1,0, \cdots) \\
\vdots\\
v_{n} & =\left((-1)^{n+1},(-1)^{n+2}, \ldots,(-1)^{n+n}, 0, \ldots\right) & \longmapsto & &(0, \ldots, 0,1,0, \ldots) \\
\vdots
\end{aligned} Now, for any $\left(y_{n}\right)_{n=1}^{\infty} \in l_2$ , $$
\sum_{n=1}^{\infty} y_{n} e_{n}=\sum_{n=1}^{\infty} y_{n} T\left(v_{n}\right)=T\left(\sum_{n=1}^{\infty} y_{n} v_{n}\right).
$$ However, the limit \begin{equation}
\lim _{k \rightarrow \infty} \sum_{n=1}^{k} y_{n} v_{n}=\left(y_{n}^{\prime}\right)_{n=1}^{\infty}
\end{equation} would have to exist, every element $y_{n}^{\prime}$ of the sequence, \begin{equation}
\begin{aligned}
y_{1}^{\prime} & =y_{1}-y_{2}+y_{3}-y_{4} \cdots \\
y_{2}^{\prime} & =y_{2}-y_{3}+y_{4}-y_{5} \cdots \\
\vdots\\
\end{aligned}
\end{equation} would have to converge and, finally, that \begin{equation}
||\left(y_{n}^{\prime}\right)_{n=1}^{\infty}||=\sum_{n=1}^{\infty}\left|y_{n}^\prime\right|^{2}<\infty
\end{equation} I have arrived to the conclusion that $T$ is not surjective, but I am afraid I could be wrong. Also, as $T$ is inyective,there is an inverse that is again defined, for every element of the transformed sequence, with another alternating series of elements of the input sequence... Thank you; I hope you find it enjoyable!","['hilbert-spaces', 'functional-analysis']"
4844901,"Series which is a product of sequences which converges, but diverges when one of them is shifted by 1","I'm trying to find two sequences $\{ a_n \}, \{ b_n \}$ such that $\sum a_n$ converges, $\{ b_n \}$ is bounded, $\sum a_n b_n$ converges, but $\sum a_{n+1} b_n$ diverges.  If this is not possible, a sketch of the proof would be greatly appreciated.","['examples-counterexamples', 'real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
4844914,How to prove that $\binom{k}{2}=\left\lfloor\frac{k^2}4\right\rfloor+\left\lfloor\frac{(k-1)^2}4\right\rfloor$,"I believe the following equation holds. $$
\binom{k}2=\left\lfloor\frac{k^2}4\right\rfloor+\left\lfloor\frac{(k-1)^2}4\right\rfloor.
$$ But I can only prove one direction: Since for any real numbers $a$ and $b$ , $\lfloor a  \rfloor + \lfloor b  \rfloor \le  \lfloor a +b  \rfloor$ , we have $$
\begin{align*} 
\left\lfloor\frac{k^2}4\right\rfloor+\left\lfloor\frac{(k-1)^2}4\right\rfloor &\le \left\lfloor\frac{k^2}4+\frac{(k-1)^2}4\right\rfloor \\
&= \left\lfloor \frac{2k^2-2k}4+\frac{1}{4} \right\rfloor\\ 
&=\binom{k}2.
\end{align*} 
$$ How can the other side be proven?","['inequality', 'discrete-mathematics']"
4844921,Why do the lattice points of these ellipses obey a curious relationship?,"Starting with, $$x^3+y^3+z^3 = (z+1)^3$$ we do the substitution $z = 3 n^2x + (3 n^2 + 1)(y - 1)$ like Adam Bailey in this post to get the ellipse , $$x^2 - x y + y^2 - (27 n^4 - 1) x - (27n^4 + 18 n^2 + 2 ) y + (27n^4 + 9 n^2 + 1 ) = 0$$ after removing a trivial factor. For general integer $n$ , this family of ellipses seems to have nice properties. The case $n=1$ was in the linked post. For $n=2$ , $$x^2  - x y + y^2 - 431 x - 506 y + 469 = 0$$ which yields the ellipse, graphed by the Desmos calculator . Using the $6$ blue lattice points (and flipping $x,y$ as needed), we get the nice $6$ -cycle of equalities, \begin{align} \color{blue}{(-66)}^3\; +\; \color{blue}{97}^3 + 456^3 &= 457^3\\
 \color{blue}{97}^3\; +\; \color{blue}{594}^3 + 8376^3 &= 8377^3\\    
 \color{blue}{594}^3 + \color{blue}{1003}^3 + 20154^3 &= 20155^3\\ 
 \color{blue}{1003}^3 + \color{blue}{840}^3 + 23106^3 &= 23107^3\\ 
 \color{blue}{840}^3 + \color{blue}{343}^3 + 14526^3 &= 14527^3\\
 \color{blue}{343}^3 + (-66)^3 + 3654^3 &= 3655^3\end{align} thus the sextuple $a_i = (-66, 97, 594, 1003, 840, 343)$ and similarly for the given $24$ lattice points. But the Alpertron calculator says this ellipse has a total of $48$ , with the others being, So we in fact have $48/6 = 8$ sextuples, \begin{align}a_i & = (-66, 97, 594, 1003, 840, 343)\\ 
b_i & = (-72, 115, 618, 1009, 822, 319)\\ 
c_i & = (90, -47, 294, 847, 984, 643)\\ 
d_i & = (72, -41, 318, 865, 978, 619)\\
e_i & = (-78, 139, 648, 1015, 798, 289)\\ 
f_i & = (264, 823, 990, 673, 114, -53)\\  
g_i & = (-42, 49, 522, 979, 888, 415)\\  
h_i & = (24, -17, 390, 913, 954, 547)\end{align} each one yielding a sextuple system $x^3+y^3+z^3 =(z+1)^3$ as demonstrated above for $a_i$ . But notice the curious relationship, $$\alpha = a_1 + a_3 + a_5 = b_1 + b_3 + b_5 = \dots = h_1 + h_3 + h_5 = 1368$$ $$\beta = a_2 + a_4 + a_6 = b_2 + b_4 + b_6 = \dots = h_2 + h_4 + h_6 = 1443$$ Question : Using the theory of conic sections, what is the geometric interpretation of \begin{align}\alpha &= 3(27n^4+6n^2)\\ \beta &= 3(27n^4+12n^2+1)\end{align} and why is it invariant using different lattice points? P.S. For this family, I've tested it with other $n$ and the same relationship seems to hold. Addendum : As Jan-Magnus pointed out in his answer $\big(\dfrac{\alpha}3,\dfrac{\beta}3\big)$ should be the center of the ellipse . So, \begin{align}
\alpha &= a_1 + a_3 + a_5 = -66 + 594 + 840 = 1368 = 3\times456\\
\beta &= a_2 + a_4 + a_6 = \;97 + 1003 + 343 = 1443 = 3\times481\end{align} Yup, looks like $(456,481)$ is indeed the center.","['number-theory', 'algebraic-geometry', 'conic-sections', 'diophantine-equations']"
4844947,"Given point in square and its distance to two vertices, how many distinct integer values are possible for the area?","Let $ABCD$ be a square. Suppose $P$ be a point strictly inside the square such that $AP = 5$ and $BP = 13$ . How many distinct integer values are possible for the area of $ABCD$ ? The possible choices were: $143, 144, 179, 180, 181$ If you start with the triangle equality then the minimum side would be $\sqrt{65}$ and the maximum side length would be $\sqrt{323}$ . However, that is too many possibilities. I also thought that the triangle had to be acute, which means that the maximum side length had to be less than $\sqrt{169+25}$ . But this is not true either. I am not sure how to proceed further.","['triangles', 'geometry']"
4844956,"Given dense $A: X\to X$, is $A^2$ dense?","Let $A: X\to X$ be a densely defined linear operator acting on Banach Space $X$ . Is $A^2$ also dense? Context: I was trying to think of an alternative proof for the Hille-Yosida theorem - specifically $$R(A, \lambda)\leq\frac{M}{\lambda-\omega} \text{ and } A \text{ dense}\Rightarrow e^{tA} \text{ exists and } e^{tA}\leq Me^{t\omega}$$ Similarly to Rudin's proof (Theorem 13.37 of Functional Analysis ) I first define $S(\varepsilon)=(I-\varepsilon A)^{-1}$ and note $AS(\varepsilon)f=S(\varepsilon)Af$ for $f\in\mathcal{D}(A)$ . I also show $\lim_{\varepsilon\to0}S(\varepsilon)f=f$ for all $f\in X$ . I next consider $T(t,\varepsilon)=\exp(tAS(\varepsilon))$ and show $$\|T(t,\varepsilon)\|\leq M\exp\left(\frac{t\omega}{1-\varepsilon A}\right)$$ My proof then diverges from Rudin's (I didn't really understand it). I wanted to show $T$ was differentiable and therefore continuous, and we could define $Q(t)=\lim_{\varepsilon\to 0}T(t,\varepsilon)$ . Given $f\in\mathcal{D}(A^2)$ we have $$\left\|\frac{\partial}{\partial \varepsilon}T(t,\varepsilon)f\right\|=\left\|-te^{tAS(\varepsilon)}S(\varepsilon)^2A^2f\right\|<\infty$$ then given $A^2$ is dense over $X$ , we can show that $Q(t)$ is well-defined over all of $X$ , is generated by $A$ , and is bounded by $M\exp(\omega t)$ as desired. However, I can't figure out how to prove that $A^2$ must be dense, and I'm not sure if this is the case.","['semigroup-of-operators', 'banach-spaces', 'functional-analysis', 'analysis']"
4844959,Proving the Gamma function is analytic over the positive reals without invoking complex analysis,"The gamma function can be defined in a few different ways, the most well known possibly being Euler's second integral definition $$\Gamma(x) = \int_0^\infty t^{x-1}e^{-t}dt, \quad x>0$$ another being Euler's infinite product definition $$\Gamma(x) = \frac{1}{x} \prod_{n=0}^\infty \frac{\left(1+\frac{1}{n}\right)^x}{\left(1+\frac{x}{n}\right)},\quad x>0.$$ It is a standard exercise to show that these agree over the positive real numbers. It is an interesting question to ask whether or not this function is analytic and if so over what intervals? i.e for which $x_0>0$ can we write it as an infinite series of the form $\Gamma(x) = \sum_{n=0}^\infty a_n (x-x_0)^n$ for some real constants $a_n$ such that it converges in an open neighbourhood of $x_0?$ The two definitions of $\Gamma(x)$ given above are well defined for complex $x$ as well (with some restrictions). Since analyticity is a stronger property in the complex plane, it is easier to prove that $\Gamma(z), \Re(z)>0,$ is holomorphic everywhere in it's domain then use analytic continuation to extend elsewhere in the plane. We then know that $\Gamma(x)$ is analytic over $x>0$ , since we can just restrict to the positive reals. There does not seem to be a proof of this theorem that doesn't use complex analysis in someway. To avoid the singularities at negative integer arguments, and to allow us to use the integral definition, I am only interested in proving analyticity over the positive reals. How can we prove the gamma function is analytic over the positive real numbers without using the theorems of complex analysis? My attempt Here is my attempt with gaps. Consider Prym's representation $$\Gamma(x)=\sum_{n=0}^\infty \frac{(-1)^n}{n!}\frac{1}{x+n}+\int_1^\infty t^{x-1}e^{-t}dt.$$ We'll show that both parts of this sum is analytic, thus their sum is analytic and $\Gamma(x)$ is analytic. The infinite sum is analytic because [...] To prove the right hand is analytic, we'll use Bernstein's Theorem from Krantz and Park's book A Primer of Real Analytic Functions Theorem: Let $f$ be a smooth function $(C^\infty)$ on an open interval $I\subset \mathbb{R}$ . If $f$ and all it's derivatives are non-negative on the entire interval $I$ then $f$ is real analytic on $I$ . The smoothness property follows from the fact that $t^{x-1}e^{-t}$ is smooth over $(x,t)\in \left[0,\infty\right) \times \left[1,\infty\right)$ . Then an application of Leibniz theorem gives that \begin{align*}
    \frac{d^n}{dx^n}\int_1^\infty t^{x-1}e^{-t}dt &= \int_1^\infty \frac{d^n}{dz^n} t^{x-1}e^{-t}dt \\
    &= \int_1^\infty \ln(t)^n t^{x-1}e^{-t}dt
\end{align*} now since the integrand is positive for $x>0$ and $1<t<\infty$ , the integral must also be positive. Thus $\int_1^\infty t^{x-1}e^{-t}dt$ is smooth and has positive derivatives for all $x>0$ and so $\int_1^\infty t^{x-1}e^{-t}dt$ is analytic over $x>0$ . Thus $\Gamma(x)$ is analytic over $x>0$ . $\blacksquare$ So it remains to show that the infinite sum defines an analytic function (does the infinite sum of analytic functions converge to an analytic function?) and it would be nice to be less handwavy with the smoothness property.","['gamma-function', 'special-functions', 'analysis', 'real-analysis']"
4844981,Improvement of Simpson's rule,"Let $I_n$ denote the approximation of $$I = \int_a^b f(x)dx$$ obtained by applying Simpson's rule with $2n$ intervals of uniform length. Define a new approximation $$J_n=\frac{16I_{2n}-I_n}{15}.$$ Find the formula for the error $\int_a^b f(x)dx-J_n$ ? We have studied Simpson's rule and proved it's error term $$-\frac{(b-a)^5}{2880n^4}f^{(4)}(c)$$ by applying Cauchy's mean value theorem repeatedly to the quotent $\frac{E(x)}{x^5}$ where $$E(x)=\int_{-x}^x f(t) dt - \frac{x}{3} \left[f(-x)+4f(0)+f(x)\right]$$ I tried apply same method but it was very confused. Let define $$E_*(x)=\int_{-x}^{x} f(x)dx-J_n$$ Choose an interval [-c,c] for two intervals we have Simpson's approximation $$I_n=\frac{c}{3}\left(f(-c)+4f(0)+f(c)\right)$$ The we can check that $E_*(x)$ is quite smooth function, it's first, second and third derivative is zero at $x=0$ By applying Cuachy's mean value theorem repeteadly we get $$\frac{E_*(x)}{x^5}=\frac{E_*^{(1)}(u)}{5u^4}=\frac{E_*^{(2)}(v)}{20v^3}=\frac{E_*^{(3)}(z)}{60z^2}=\frac{E_*^{(4)}(t)}{120t}$$ which $0<t<z<v<u<x$ $${\frac{E_*(x)}{x^5}={\frac{\frac{1}{45}[-7tf^{(4)}(-t)-2tf^{(4)}(-\frac{t}{2})-2tf^{(4)}(\frac{t}{2})-7tf^{(4)}(t)-17f^{(3)}(-t)+16tf^{(3)}(-\frac{t}{2})-16tf^{(3)}(\frac{t}{2})+17f^{(3)}(t)]}{120t}}}$$ Another approach would be $$\int_a^b f(x)dx-I_n =-\frac{(b-a)^5}{2880n^4}f^{(4)}(c)$$ For some $c$ in $[a,b]$ $$\int_a^b f(x)dx-I_{2n} =-\frac{(b-a)^5}{2880n^4*16}f^{(4)}(k)$$ For some $k$ in $[a,b]$ $$J_n=-\frac{(b-a)^5}{2880n^4}(f^{(4)}(c)-f^{(4)}(k))$$ I couldn't proceed whatever these are right approachs. I have looked up some texts which describes Richardson extrapolation, Taylor series and $O(h)$ notation. And nowhere I have seen eroor terror in explicit form. I am not familiar with them. So could anybody help me please.","['integration', 'simpsons-rule', 'real-analysis', 'calculus', 'numerical-methods']"
4844984,Proving that a Graph without 6-cycles has subgraph without 4-cycle,"I am currently trying to prove that for every graph G such that the 6-cycle $C_6$ is not a subgraph of G, there exists a subgraph H with $|E(H)|\geq|E(G)|/2$ (with E(G) being the edge set of G) which doesnt contain a 4-Cycle. I first tried to attack this by constructing subgraphs using random 2-colorings and then deleting monocolored edges, but this yielded a worse approximation for the size of the edge set. Furthermore, I tried to rewrite the claim (however i am unsure if this is even correct): $$ex(n, C_6)\leq2ex(n, C_4)$$ I found a result (by Erdős, Rényi and Sós among others) which says that $ex(n,C_4)=1/2n^{3/2}+o(n^{3/2})$ and $ex(n,C_6)=𝛩(n^{4/3})$ , which would then imply (assuming i reworded the claim correctly) that there are two positive constants a,b with b<1 such that $a\cdot n^{4/3}≤n^{3/2}(1+2b)$ , which is equivalent to $a\leq n^{1/6}(1+2b)$ . This above result is the only one I found in my graph theory material (not just my own notes, but also the lecture notes and the lecture script), which is why it seems to me that i am on the right path. However, this is where i am unsure if it would already be proven and if not i am a bit lost on what to do next, so any help would be appreciated!","['graph-theory', 'discrete-mathematics']"
4844996,One or no smallest element on a partial order,"Suppose we have some partial order R on a set A, and B is a subset of A. With partial we mean a relation which is reflexive, transitive and anti-symmetric. With anti-symmetric we mean that if there exists a pair aRb and bRa where a and b are some arbitrary elements in R then a = b. The following question is now posed: Suppose B has a smallest element, show there there is either 1 or no smallest element. More specifically, the R-smallest element of B. The smallest element is defined as: given some arbitrary element of B named x the pair bRx exists. I have written the following proof: Suppose more then 1 smallest element exists, we shall call them b1 (element of B) and b2 (element of B). We note that b1 does not equal b2 because we are talking about 2 smallest elements. For them to both be a smallest elements the pairs (b1, b2) and (b2, b1) must exist. However this leads to a contradiction, since a partial order must be anti-symmetric and b1 does not equal b2. This proof seems quite short compared to the one that is given in my book (Velleman how to prove it). But I wonder if this is sufficient? Any feedback is highly appreciated!","['elementary-set-theory', 'solution-verification']"
4845022,Why is computing powers in modular arithmetic $O(2^k)$?,How do they get the exponential time claim? Is this a mistake in my lecture notes? I am told that to multiply two k-bit integers roughly takes $≤2k^2$ bit operations; thus shouldn't $b-1$ multiplications take $≤(b-1)2k^2$ bit operations = $O(k^2)$ ?,"['computational-complexity', 'asymptotics', 'discrete-mathematics']"
4845061,Connecting two points inside the Koch snowflake without getting too close to the boundary,"Let $\Omega \subset \mathbb{R}^n$ be a bounded domain. We say $\Omega$ is a uniform domain with constant $c \geq 1$ if for any $x,y \in \Omega$ there is a rectifiable curve $\gamma : [0, l_\gamma] \to \Omega$ , parameterized by arc length, such that $\gamma(0) = x$ , $\gamma(l_\gamma) = y$ , and \begin{align}
        l_\gamma &\leq c \lVert x - y\rVert, \label{1}\tag{i}\\
        \operatorname{min}(t, l_\gamma - t) &\leq c \operatorname{dist}(\gamma(t),\partial\Omega)\label{2}\tag{ii}
\end{align} for all $t \in [0,l_\gamma]$ . A typical example of a uniform domain which is not a Lipschitz domain is the interior of the Koch snowflake . Let me call the Koch snowflake $S$ and its interior $\mathrm{int}(S)$ . So far, I couldn't find an explicit proof for why $\mathrm{int}(S)$ is a uniform domain as it is often stated as ""obvious"" fact. I tried to prove it myself. Here are my ideas: I need to pick two points and connect them via a curve. The length of the curve should be comparable to the distance between the two points and it should not get too close to the boundary. The curve can probably be defined piecewise affine. When picking $x \in \mathrm{int}(S)$ , $x$ must lie in exactly one triangle of the construction process of $S$ , except when $x$ lies on a common boundary of two triangles. If $x$ and $y$ lie in the same triangle of the construction process, call it $D$ , I have an idea on how to construct the curve. First, construct a new triangle $\triangle xyz$ , where $z$ is the barycenter of $D$ and then connect $x$ and $y$ with straight lines via the barycenter of $\triangle xyz$ . If $x$ and $y$ lie in different triangles of the construction process, I don't really know how to proceed. Questions. How do you constructively prove that the Koch snowflake is a uniform domain? Is there another proof idea which might be easier? Any hint is appreciated, thank you!","['euclidean-geometry', 'plane-curves', 'geometry', 'triangles', 'fractals']"
4845116,Find the angle x formed in the figure below,"Outside the square $ABCD$ , a semi-circle of diameter $BC$ is constructed. On the arc $BC$ , point $L$ is marked. If $AL=2(BL)$ , calculate $m \sphericalangle ALC$ . (Answer: $60^\circ$ ) I made the drawing and got a solution by trigonometry. Does anyone have a solution using geometry? $x= BL$ , $AL = 2x$ $α,β$ : $\sphericalangle LBC, \sphericalangle ALC$ $\triangle CLB_{(ret.)}\\
\angle BCL = 90-\alpha \implies \angle ACL =135^o +\alpha\\
\angle ACB = 90-\beta \implies \angle BAL = \beta - \alpha$ $\begin{array} :\triangle ACL:\dfrac{l\sqrt{2}}{\sin\beta}=\dfrac{2x}{\sin(135^{\circ}-\alpha)}(I)\\\triangle LAB:\dfrac{2x}{\sin(90^{\circ}+\alpha)}=\dfrac{x}{\sin(\beta-\alpha)}=\dfrac{l}{\sin(90^{\circ}-\beta)}\end{array}$ $\dfrac{2x}{\sin(90^o+α)}=\dfrac{l}{\sin(90^o−β)}
⟺
2x=\dfrac{l\cos α}{\cos β}\\
sen(135^o−α)=\frac{\sqrt2(\cos α+\sin α)}{2}\\
\text{From (I) : } \dfrac{l\sqrt2}{\sin β}=\dfrac{l\cos α}{\dfrac{\sqrt2}{2}(\cos α+\sin α)\cos β}\\
\therefore\  1+\tan α=\tan β\ (∗)\ . \\
\dfrac{2x}{\sin(90^o+α)}=\dfrac{x}{\sin(β−α)}
\\\qquad
\implies 2(\sin β \cos α−\cos β \sin α)= \cos α\qquad(\div \cos\alpha \cos\beta)
\\\qquad
\implies\tan β−\tan α=\dfrac{1}{2\cos β}\\
\text{From $(∗)$ : }1=\dfrac{1}{2\cos β}\ ⟹\ \cos β=\dfrac{1}{2} \therefore \boxed{β=60^\circ }
$ (Solution by ani_pascual)","['euclidean-geometry', 'geometry', 'plane-geometry']"
4845153,A disc contains $n$ random points. Each point is connected to its nearest neighbor. What does the average cluster size approach as $n\to\infty$?,"A disc contains $n$ independent uniformly random points. Each point is connected by a line segment to its nearest neighbor, forming clusters of connected points. For example, here are $20$ random points and $7$ clusters, with an average cluster size of $\frac{20}{7}$ . What does the average cluster size approach as $n\to\infty$ ? My attempt: I made a random point generator that generates $20$ random points. The average cluster size is usually approximately $3$ . I considered what happens when we add a new random point to a large set of random points. Adding the point either causes no change in the number of clusters, or it causes the number of clusters to increase by $1$ ( Edit: this is not true, as noted by @TonyK in the comments). The probability that adding a new point increases the number of clusters by $1$ , is the reciprocal of the answer to my question. (Analogy: Imagine guests arriving to a party; if 25% of guests bring a bottle of wine, then the expectation of the average number of guests per bottle of wine is $4$ .) But I haven't worked out this probability. Context: This question was inspired by the question Stars in the universe - probability of mutual nearest neighbors . Edit: Postd on MO .","['geometric-probability', 'geometry', 'expected-value', 'percolation', 'probability']"
4845211,morphism on the commutator subgroup of the free group/picture-hanging-puzzle,"Let $F_n=\langle x_1,\dots,x_n \rangle$ denote the free group on $n$ generators. Let \begin{align}
\varphi:F_n\rightarrow \prod_{i=1}^n\langle x_1,\dots,x_n \mid x_i=e \rangle
\end{align} be the canonic morphism making one of the generators trivial in each entry. I want to understand $\ker \varphi$ . Its clear that $\ker\varphi \subset [F_n,F_n]$ . Ultimately I am interested in finding the element in $\ker\varphi$ with the least symbols for each $n$ . Obviously when $n=2$ this is $[x_1,x_2]$ up to relabelling of the generators. In the case $n=3$ it probably is $[x_1,x_2]x_3[x_2,x_1]x_3^{-1}$ . Motivation for this is the picture-hanging-puzzle from topology where we you want to hang a picture on $n$ nails such that removing a nail makes the picture fall. It is known that there are solutions $\leq 2n^2$ symbols long, I'm just wondering if there is anything with less than $n^2$ symbols. Is there anything more known about this?","['group-theory', 'abstract-algebra', 'algebraic-topology']"
4845222,How to create infinitely many disjoint sets from infinitely many sets,"Suppose we have a countably infinite set $X$ and we have (countably) infinitely many subsets $A_1,A_2,\cdots\subseteq X$ which are non-empty and distinct (i.e. for any $i\neq j$ either $A_i\setminus A_j$ or $A_j\setminus A_i$ is non-empty). Is it possible (in ZF) to disjoint these sets i.e. come up with sets $B_1,B_2,\cdots\subseteq X$ that are non-empty and pairwise disjoint. Edit: it is not necessarily the case that $X$ is in bijection with $\omega$ in our model of ZF, we know that $X$ is countably infinite from outside our model.","['elementary-set-theory', 'model-theory', 'set-theory']"
4845234,"Determine whether there's permutation $p$ such that for matrices $A_{i}$, $A_{p_{1}}A_{p_{2}} \dots A_{p_{n}} = B$","Given $m$ $n\times n$ matrices $A_{1},A_{2} \dots A_{m}$ and a matrix $B$ , is there a way to determine whether there's permutation $p$ such that for matrices $A_{i}$ , $A_{p_{1}}A_{p_{2}} \dots A_{p_{m}} = B$ . The solution should be ""almost correct"" in all cases, and hopefully, in polynomial time. Moreover, one way is to check whether $\prod{\det(A_{i})} = \det(B)$ , however it is ""correct on almost every cases"". I tried to find some other values to estimate its possibility but failed. Are there any possible solutions? EDIT: Almost correct here means for $\epsilon$ possibility being wrong, you can get it in $O(f(\epsilon))$ time for some $f$ , still we want $f$ not being to large. And some one pointed out that it may be a NP problem in the comment.",['linear-algebra']
4845250,Is intersection of curves continuous in some sense?,"I don't know algebraic geometry but hopefully the following makes sense: let $X$ be the set of all pairs of plane projective (say) conics $(F,G)$ s.t. $F,G$ have no common components; I imagine this is a dense open subset of the space of pairs of conics. By Bezout's Theorem each pair of conics intersect in exactly $4$ points counting multiplicities. The intersection may be viewed as a multiset, or a sum of dirac measures on $\mathbb{P}^2$ . Equip $\mathbb{P}^2$ with any reasonable metric. Question: Is the map $(F,G)\mapsto F\cap G$ continuous w.r.t. the Wasserstein (earth mover's) distance on $\mathbb{P}^2$ ? If not, is it continuous at least in some sense? Question2: Is the topology induced by earth mover's distance the same as $(\mathbb{P}^2)^4/S_4$ , where $S_4$ acts on $(\mathbb{P}^2)^4$ by permuting coordinates?","['algebraic-curves', 'algebraic-geometry', 'metric-spaces']"
4845322,A continuous injective group homomorphism $GL_n(\mathbb{R}) \to O(n)$,"Is there a continuous injective group homomorphism $GL_n(\mathbb{R}) \to O(n)$ ? I'm struggling to construct one - given a matrix $A$ in $GL_n(\mathbb{R})$ , how can we construct a matrix $O(n)$ in terms of $A$ ? I've tried constructions like $AA^T$ , $A^{-1}A^T$ , $A+A^T$ , $A^{-1}+A^T$ , and none work (not even mapping to $O(n)$ , let alone being injective).","['group-homomorphism', 'continuous-homomorphisms', 'continuity', 'linear-algebra', 'group-theory']"
4845323,"What is the derivative of $f(x, y) = x + y^2$ with respect to $x$ if $y^2 = x^4$?","What is the derivative of $f(x, y) = x + y^2$ with respect to $x$ if $y^2 = x^4$ ? This is what I have tried: $ \frac{df}{dx} = \frac{\partial f}{\partial x} \frac{d x}{d x} + \frac{\partial f}{\partial y} \frac{d y}{d x} = 2y * (\pm) 2x $ . Could you please help me understand if my solution is correct?","['multivariable-calculus', 'calculus']"
4845364,"Closed form for $\psi^{1/k}(1)$, where $k$ is an integer","I have proven the identity $$
\sum_{k=1}^{\infty} \dfrac{\operatorname{_2F_1}(1, 2, 2-1/t,-1/k)}{{k}^{2}} = Γ(2-\dfrac{1}t){\psi^{1/t}(1)}+\psi(-\dfrac{1}t)(\dfrac{1}t(1-\dfrac{1}t))+\gamma(1-\dfrac{1}{t^2}
)$$ Which means any fractional derivative of $$\psi(z)$$ at z=1 has a closed form in terms of this summation of hypergeometric functions I have found 2 special cases with closed forms so far, $$\sum_{k=1}^{\infty} \dfrac{\operatorname{_2F_1}(1, 2, 2-1/2,-1/k)}{{k}^{2}} = \sum_{k=1}^{\infty} \dfrac{\operatorname{arccsch}(\sqrt{k})}{(k+1)^{3/2}} = {\sqrt{\pi}\psi^{1/2}(1)}+\gamma-\operatorname{ln}2
$$ and $$\sum_{k=1}^{\infty} \dfrac{\operatorname{_2F_1}(1, 2, 2-1/4,-1/k)}{{k}^{2}} = \sum_{k=1}^{\infty} \dfrac{{\operatorname{arccoth}({(k+1)}^{1/4})}-\operatorname{arccot}({(k+1)}^{1/4})}{(k+1)^{5/4}} = 2Γ(\dfrac{3}{4})\psi^{1/4}(1)+\dfrac{π}{4}+2\gamma-\dfrac{3}{2}\operatorname{ln}2
$$ I cannot find a closed form for the general case, or closed forms for rational numbers 1/3, 1/5, 1/6, 1/7, 1/8, although I presume they exist. So my question is is there any other cases that allow for a closed form for the polygamma term without hypergeometric functions? (ie can the sum of hypergeometric functions be expressed in terms of elementary functions like it does in the cases t=2 and t=4) If so, what are they and what are their closed forms in terms of the fractional derivatives of the polygamma function? This is purely for my curiousity for closed forms. I was intrigued by the idea of putting fractional or even real values for ""v"" in the polygamma(v,z) function. My goal is to determine a generalized formula to do so, and additionally determine closed forms for the function with a fractional derivative. I am not sure if the first identity shown at the top has been documented before, but I cannot find any evidence or help with it online.","['summation', 'calculus', 'functions', 'closed-form', 'fractional-calculus']"
4845388,"cardinality of the set of order isomorphism functions from (R,<) to itself","Let $A$ be the set of order isomorphism functions (bijective and order preserving) from $(R,<)$ to itself. What is the cardinality of $A$ ? It's suppose to be $\mathfrak{c}$ , but I'm having a hard time proving it.","['elementary-set-theory', 'set-theory']"
4845413,Is the set of rational numbers $\frac{a}{b}$ such that $b \mid (a^2d+1)$ dense in $\mathbb{R}$?,"Fix some integer $d \geq 1$ for the problem. I'm hoping to show that the set of rational numbers of the form $a/b$ where $\gcd(a,b) = 1$ and $b$ divides $a^2d + 1$ is still dense in the real line. For motivation, this problem is actually closely tied to showing certain vector bundles on a degree $d$ polarized K3 surface exist — unfortunately my number theory is lacking. Any help would be appreciated","['number-theory', 'elementary-number-theory']"
4845434,sin(A + B) + sin(B + C) + cos(A + C) = 3/2. Find each angle.,"I'm given the fact that in $\triangle ABC$ , $\sin(A + B) + \sin(B + C) + \cos(A + C) = \frac{3}{2}$ .
I'm asked to get the angles $A, B, C$ .
So far what I've done is I've substituted $A+B = 180-C$ and so on in order to get $\sin(180 - C) + \sin(180 - A) + \cos(A + C) = \frac{3}{2}$ . From here I've used the sin addition identity to get $\sin(C) + \sin(A) + \cos(A + C) = \frac{3}{2}$ From here I'm stuck. Help!","['trigonometry', 'triangles']"
4845438,Differentiability of $\lvert\cos x\rvert$?,"I don't know if I understand differentiability and continuity. So by my understanding, if a function is continuous at a point, it may or may not be differentiable at that point; but if it isn't continuous at some point, then it can't be differentiable at that point. So  for functions I can first test the continuity, then the differentiability. If a function is continuous at that point I can use the differentiation rules, if I am not allowed to test continuity first, then I have to do it by definition of a derivative. For $f(x)=\lvert\cos x\rvert$ , that function is continuous on all $\mathbb R$ , but we have to check at a point $x= \pi/2$ , since the absolute value is $0$ there, and check it by definition. I don't quite understand is the function differentiable or not. When  find the right and left derivative, they turn out the same to me, but in my answer sheet it says it isn't for $\pi/2$ .","['continuity', 'calculus', 'functions', 'derivatives']"
4845452,"For each $n\in\mathbb{N},$ let $x_n:=\min_{1\leq k < n}\lvert\sin n-\sin k\rvert.\ $ Does $\sum_{n=1}^{\infty} x_n $ converge?","For each $n\in\mathbb{N},$ let $x_n:= \displaystyle\min_{1\leq k <
 n} \lvert\sin n - \sin k\rvert.\ $ Does $\displaystyle\sum_{n=1}^{\infty} x_n $ converge? Consider instead, $a_1 = 0,\ a_2=1, a_3 = \frac{1}{2}, a_4 = \frac{1}{4}, a_5=\frac{3}{4}, a_6 = \frac{1}{8},  a_7 = \frac{3}{8},\ldots . $ Then for each $n\in\mathbb{N},$ let $x_n:= \displaystyle\min_{1\leq k < n} \lvert a_n - a_k\rvert.\ $ This is an example of a bounded sequence $(a_n)_{n\in\mathbb{N}}$ with the property that $\displaystyle\sum_{n=1}^{\infty} x_n $ diverges. The question is: does the convergence of $\displaystyle\sum_{n=1}^{\infty} x_n $ occur if we instead define $(a_n) = (\sin n). $ But it seems unlikely that $\sin$ behaves similar enough to the sequence in the previous paragraph to enable $\displaystyle\sum_{n=1}^{\infty} x_n $ to diverge, although I don't know how to prove this, although it could be as simple as applying a trigonometric identity in a specific way.","['trigonometry', 'convergence-divergence', 'diophantine-approximation', 'equidistribution']"
4845538,Why does $y^2=(x^2-1)(x^2-4)(x^2-9)$ have genus 2? Why not genus 10? Isn't it nonsingular?,"If a curve is nonsingular then the arithmetic genus and the geometric genus should be the same. Using the genus-degree formula, I obtain that the genus of this curve should be 10. However, the book I'm reading states that this curve has genus 2.",['algebraic-geometry']
4845560,Limit distribution of the number of longest runs in a random binary sequence of length $n\to\infty.$,"Let $X_n$ be the number of longest runs in a sequence of $n$ i.i.d. Bernoulli(1/2) random variables. Here, ""run"" means a nonempty block of adjacent $1$ s; e.g., in the sequence $110110001$ , there are two longest runs (each being $11$ ). By using a program from OEIS, the limiting distribution of $X_n$ , as $n\to\infty,$ is ""empirically"" suggested to be $$p_k:=\lim_{n\to\infty}P[X_n=k]={1\over k\ 2^k\ \log(2)}\ \ (k=1,2,3,...)$$ Question : As this is probably a known result, can someone cite a published proof? (Or provide any information on how to prove it.) ""Derivation"": As described elsewhere , for $n\ge 1$ , there is a bijection between (a) the compositions of $n$ with exactly $k$ largest parts, and (b) the binary strings of length $n-1$ with exactly $k$ longest runs of $1$ s, such that these are equal in number. This number is listed as $T(n,k)$ at OEIS A238341 , where a  program for $T(n,k)$ is also given. (I've posted a separate question asking for explanation/sources of the algorithm implemented by this program.) NB : Letting $A(n,k)$ be the number of length- $n$ binary strings with $k$ longest runs, and $T(n,k)$ be the number of compositions of $n$ with exactly $k$ largest parts, there is an ""edge effect"" due to which these are related as follows: $$A(n,k)=\begin{cases}
1&\text{if $k=0$}\\[1ex]
T(n+1,k) &\text{otherwise}
\end{cases}$$ and it is to be noted that in fact $A(n,k)=0$ if $k\gt\lceil{n\over 2}\rceil$ . Thus, we have $$P[X_n=k]={A(n,k)\over 2^n} \ \ (k=0,1,2,3,...)$$ Using the OEIS program gives $P[X_n=1]\approx 0.7213$ for $n\gt 100,$ which led me to guess that $p_1={1\over 2 \log 2}$ . I then let $c_k:= p_k\ \log 2$ , which implies $\sum_{k=1}^\infty c_k=\log 2;$ i.e.,  a series converging to $\log 2.$ One of the best-known such series is $c_k={1\over k\ 2^k},$ which is strongly supported by computing $(P[X_n=k] \log 2)_{n\gt 100}\approx(1/2,1/8,1/24,1/64, 1/160,...)=({1\over k\ 2^k})_{k=1,2,...}.$ A Python/SageMath rendition of the OEIS program is online at SageMathCell . It takes about a minute there to plot $(P[X_n=k])_{n=1..45}$ for $k=0,1,2,3$ . Aside :
(Sorry for the length, but I thought someone might be interested ...) @kodlu's comment has led me to show that the limit distribution of the number of longest runs in a random binary sequence of length $n\to\infty$ is the same distribution for all of the following different definitions of ""run"" : D1:  ""run"" means a nonempty block of adjacent $1$ s. D2:  ""run"" means a nonempty block of adjacent $1$ s, or a nonempty block of adjacent $0$ s. D3:  ""run"" means a nonempty block of adjacent $1$ s, flanked on each side either by a $0$ or by the start/end of the sequence. D4:   ""run"" means a nonempty block of adjacent $1$ s, flanked on each side either by a $0$ or by the start/end of the sequence, or a nonempty block of adjacent $0$ s, flanked on each side either by a $1$ or by the start/end of the sequence. Proof: Clearly, D1 and D3 give the same distribution for all $n$ because the longest run is the same by either definition (hence the limit distributions are the same). Similarly, D2 and D4 give the same distribution for all $n$ (though different from  those for D1 and D3). Therefore, all that remains to show is that the limit distribution for D1 is the same as that for D2. To do so, let $A(n,k)$ (resp. $B(n,k)$ ) denote the number of length- $n$ binary strings with exactly $k$ longest runs,  according to definition D1 (resp. D2), and let $X_n(s)$ (resp. $Y_n(s)$ ) denote the number of longest runs in sequence $s$ , according to definition D1 (D2, resp.).  We will show that $$B(n,k)=2\ A(n-1,k)  \ \ (0\lt k\lt n)$$ from which it follows that for $0\lt k\lt n$ , $$P[Y_n=k] = {B(n,k)\over 2^n}={A(n-1,k)\over 2^{n-1}}=P[X_{n-1}=k]
$$ and hence $$\lim_{n\to\infty}P[Y_n=k]=\lim_{n\to\infty}P[X_n=k]\ \ (k=1,2,3,\dots).$$ (We can dispense with $k=0$ , because $P[Y_n=0]=0$ and $P[X_n=0]=2^{-n}\to 0.$ ) Thus proceeding, with $s[1]$ denoting the first element of $s$ , we have, for $0\lt k\lt n$ : $$\begin{align}B(n,k)
&\overset{1}{=}|\{s\in\{0,1\}^n:\ Y_n(s)=k\}|\\
&\overset{2}{=}2\ |\{s\in\{0,1\}^n:\ Y_n(s)=k,\ s[1]=0\}|\\
&\overset{3}{=}2\ |\{s\in\{0,1\}^{n-1}:\ X_{n-1}(s)=k\}\\
&\overset{4}{=}2\ A(n-1,k)
\end{align}$$ Eq.1 implies Eq.2 because there is a bijection between the sequences beginning with $0$ and those beginning with $1$ , pairing each $s$ to its complement $\bar s$ (i.e. with all bits complemented), such that $Y_n(s)=Y_n(\bar s).$ (Any run in $s$ exactly matches a run of the complementary symbol in $\bar s.$ ) To show that Eq.2 implies Eq.3, denote the corresponding sets (suppressing $k$ ) as $$T_n:=\{s\in\{0,1\}^n:\ Y_n(s)=k,\ s[1]=0\}\\[1ex] S_{n-1}:=\{s\in\{0,1\}^{n-1}:\ X_{n-1}(s)=k\}$$ Now, any element of $T_n$ can be written,  with unique positive integers $(n_1,...,n_j)$ summing to $n$ , in the usual ""run-length encoding"" form $$t =  \ 0^{n_1}1^{n_2}0^{n_3}1^{n_4}\dots b^{n_j}, \ \ b=(j+1)\bmod 2$$ and any element of $S_{n-1}$ can be written, with unique positive integers $(n_1,...,n_j)$ summing to $n$ , in the special form $$s =  \ 1^{n_1-1}01^{n_2-1}01^{n_3-1}\dots01^{n_j-1}$$ thus setting up the bijections $$\underbrace{0^{n_1}1^{n_2}0^{n_3}1^{n_4}\dots b^{n_j}}_{\in T_n}\leftrightarrow(n_1,\dots,n_j)\leftrightarrow\underbrace{1^{n_1-1}01^{n_2-1}01^{n_3-1}\dots01^{n_j-1}}_{\in S_{n-1}}$$ Now, $Y_n(0^{n_1}1^{n_2}0^{n_3}1^{n_4}\dots b^{n_j})=X_{n-1}(1^{n_1-1}01^{n_2-1}01^{n_3-1}\dots01^{n_j-1})=$ the multiplicity of $\max(n_1,...,n_j)$ , the sole exception being when $n_1=\dots=n_j=1$ (which occurs iff $k=0$ or $k=n$ ; i.e. when $Y_n(0101...)=n\ne 0=X_{n-1}(000...)).$ Consequently, $|T_n| = |S_{n-1}|$ for $0\lt k\lt n$ . QED","['probability-distributions', 'combinatorics', 'probability']"
4845580,"How can I prove that if a polynomial has its coefficients specified by continuous functions, its roots will also be continuous functions?","So I want to prove that if we have a polynomial in the form of $f(t,z)=\sum_{i=0}^n a_i(t) z^i$ where the coefficients $a_i(t)$ are continuous w.r.t. $t$ , there then exists a continuous function $r_k(t)$ such that $f(t,r_k(t))=0$ , or in other words such that at least one of the roots of $f(t,z)$ is continuous w.r.t. $t$ . I think I understand proofs of continuity like in: https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://aalexan3.math.ncsu.edu/articles/polyroots.pdf&ved=2ahUKEwjJgfDHj-GDAxW5ITQIHZA5C5wQFnoECBgQAQ&usg=AOvVaw3akZPgEB32DtPskNGBcsF _ which show through Rouche that for a given polynomial which has a minimum magnitude of the difference between its roots as $\tilde{r}$ and for any $0<\epsilon<\frac{\tilde{r}}{2}$ , we will have that there exists a $\delta>0$ s.t. if we perturb the coefficients by any values less than or equal to $\delta$ we will have that the roots of the resulting perturbed polynomial will be within $\epsilon$ of the original roots (magnitude wise). I can see how this is pretty much the definition of continuity, but the upper bound on $\epsilon$ is confusing to me. Isn't it possible that the upper bound on this $\epsilon$ gets arbitrarily small as we perturb the coefficients and roots begin to approach each other? Also, how could we prove an intermediate value theorem for the roots with this upper limit on $\epsilon$ ? Sorry if these are foolish questions. $\textbf{EDIT}$ : oh I just realized that if the above is true for some $\epsilon$ , then it is obviously true for any larger value of $\epsilon$ . Also, looking at the Rouche proof, I think that $\epsilon$ just has to be such that the balls of radius $\epsilon$ about the roots of the original polynomial do not contain other roots on their boundary, so I think I can just consider a larger $\epsilon$ that bounds roots that are becoming arbitrarily close together in order for the resulting $\delta$ to be well defined even as roots potentially become identical as we perturb the coefficients, and I think that pretty much gives me a straightforward $\epsilon-\delta$ continuous relation between the coefficients and resulting roots. I am still not sure though, but I think the above can give me the result for any $\epsilon>0$ where I can consider $0<\epsilon_1<\frac{\tilde{r}_1}{2}$ as in the original proof to get a corresponding $\delta(\epsilon_1)$ , and this $\delta(\epsilon_1)$ also covers the case for $\hat{\epsilon}_1=\frac{\tilde{r}_1}{2} $ . Then I can consider $\frac{\tilde{r}_1}{2}<\epsilon_2<\frac{\tilde{r}_2}{2}$ where $\tilde{r}_2$ is the next minimal difference in the polynomial roots. Since this $\epsilon_2$ is such that all of the balls centered on the roots still do not have any roots on their boundaries, I am pretty sure I can still use the linked Rouche proof to obtain a suitable $\delta(\epsilon_2)$ . Continuing in this manner for all the possible root differences and beyond, I think I can arrive at the familiar ""for all $\epsilon>0$ "" definition of continuity even if multiple roots begin to converge towards being identical.","['complex-analysis', 'roots', 'polynomials', 'real-analysis']"
4845584,Pólya's urn - Probability first ball is blue given subsequent draws,"I refer to the solution contained in this post here . I know it must be simple but I cannot deduce why $$
\begin{align*}
P(B_1|B_2\cap \dots \cap B_{n+1})&=P(B_{n+1}|B_1\cap\dots\cap B_n)
\end{align*}
$$ I presume it is using the fact that $P(B_1|B_2)=P(B_2|B_1)$ so there might be some inductive argument being made but can't seem to get this one. Any help would be appreciated.","['polya-urn-model', 'conditional-probability', 'induction', 'probability-theory', 'probability']"
4845615,"A fair coin is tossed 9 times, then find the probability that at least 5 consecutive heads occur.","I tried to use the principle of inclusion-exclusion. The number of ways of getting 5 consecutive heads= $5×2^4$ The number of ways of getting 6 consecutive heads= $4×2^3$ Similarly, the number of ways of getting 7, 8, and 9 consecutive heads are $3×2^2, 2×2,$ and $1$ , respectively. So, the number of ways of getting at least 5 consecutive heads= $5×2^4-4×2^3+3×2^2-2×2+1$ = $57$ But the correct number would be 48. Where did I go wrong?","['permutations', 'combinatorics', 'probability']"
4845617,Is open set and interiority a necessary condition for differentiability?,"I am going by what I am seeing on Wikipedia:  In 1D, the standard definition for differentiability is, A function $f:U\to\mathbb{R}$ , defined on an open set $U\subset\mathbb{R}$ , is said to be ''differentiable'' at $a\in U$ if
the derivative $$f'(a)=\lim_{h\to0}\frac{f(a+h)-f(a)}{h}$$ exists. Other times, $a$ is defined to be a point which is in the interior of a not necessarily open domain. However, when I read some other references such as Terence Tao's Analysis I, this assumption does not seem to exist and I don't think it is a mistake or an omission. Can someone help me understand whether openness and interiority are necessary in order to define differentiability?","['real-analysis', 'jacobian', 'definition', 'derivatives', 'soft-question']"
4845623,Solving Sine Equation [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 months ago . Improve this question I'm trying to find all the real solutions of the following trigonometric equation : $$ \frac{\sin 3x}{\sin 2x} = A $$ I can see that $x$ is some integer function of $\pi$ , but I cannot find the exact expression for it using normal trigonometric identities due to $A$ being present. Any help would be greatly appreciated!","['real-numbers', 'trigonometry', 'transcendental-equations']"
4845632,Concentration bound on product of random Gaussian vector and random unit-sum vector,"Suppose that $X = (X_1, \dots, X_n) \in [0,1]^n$ is a random vector with $\Vert X \Vert_1 = 1$ . Let $Q = (Q_1, \dots, Q_n)$ be a Gaussian random vector, where $Q_i$ 's are i.i.d. from $N(0, \sigma^2)$ . Suppose that $X$ and $Q$ are independent. What can be said about the following probability ? $$\mathbb{P}\left(\left\vert \sum_{i=1}^n X_i Q_i \right\vert \ge t\right)$$","['inequality', 'concentration-of-measure', 'probability-theory', 'normal-distribution']"
4845639,What is the relation between these pairs of triangles on an ellipse for $x^3+y^3+z^3 = (z+1)^3$?,"Recall that given, $$x^3+y^3+z^3 = (z+1)^3$$ we do the substitution $z = 3 n^2x + (3 n^2 + 1)(y - 1)$ like Adam Bailey in this post to get the ellipse , $$x^2 - x y + y^2 - (27 n^4 - 1) x - (27n^4 + 18 n^2 + 2 ) y + (27n^4 + 9 n^2 + 1 ) = 0$$ after removing a trivial factor. I found two triples of polynomial solutions to this. The first triple $(x,y) = (a_j,a_k)$ , \begin{align} 
(a_1,\, a_2) &= (3n + 3n^2 + 9n^3,\, 1 + 6n^2 - 9n^3)\\
(a_3,\, a_4) &= (-3n + 3n^2 - 18n^3 + 27n^4,\, 1 - 3n + 15n^2 - 9n^3 + 54n^4)\\ 
(a_5,\, a_6) &= (12n^2 + 9n^3 + 54n^4,\, 1 + 3n + 15n^2 + 18n^3 + 27n^4)\end{align} These three lattice points are the vertices of the first triangle. And the second triple $(x,y) = (b_j,b_k)$ , \begin{align} 
(b_1,\, b_2) &= (-9n^3,\, 1 - 3n + 9n^2 - 18n^3 + 27n^4)\\
(b_3,\, b_4) &= (-3n + 9n^2 - 9n^3 + 54n^4,\, 1 + 18n^2 + 9n^3 + 54n^4)\\
(b_5,\, b_5) &=  (3n + 9n^2 + 18n^3 + 27n^4,\, 1 + 3n + 9n^2 + 9n^3)\qquad\qquad\end{align} are the vertices of the second triangle. Naturally, these satisfy, $$(a_j)^3+(a_{j+1})^3+(u_j)^3 = (u_j+1)^3$$ $$(b_j)^3+(b_{j+1})^3+(v_j)^3 = (v_j+1)^3$$ for integer polynomials $u_j,\,v_j.$ I found these empirically, but the natural question to ask is: can we derive one triangle from the other? Note the curious relationships, \begin{align}
\alpha &= a_1 + a_3 + a_5 = b_1 + b_3 + b_5 = 3(27n^4+6n^2)\\
\beta &= a_2 + a_4 + a_6 = b_2 + b_4 + b_6 = 3(27n^4+12n^2+1)\end{align} \begin{align}
\frac23(\alpha+\beta)\, 
&= a_1 + a_2 + b_3 + b_4\\
&= a_3 + a_4 + b_5 + b_6\\
&= a_5 + a_6 + b_1 + b_2 \,=\, 2(54n^4+18n^2+1)\end{align} In a previous post , I asked if $(\alpha,\beta)$ had a geometric interpretation and Jan-Magnus Økland answered that $\big(\dfrac{\alpha}3,\dfrac{\beta}3\big)$ in fact is the center of the ellipse . And this post , it is stated that ""if the center is known, then 3 points are enough to uniquely define an ellipse"". A triangle, in other words. To illustrate, let $n=\pm2$ and we get the same ellipse, $$x^2  - x y + y^2 - 431 x - 506 y + 469 = 0$$ graphed below by the Desmos calculator with relevant lattice points, The 3 blue points determine the first triangle, while the 3 green points determine the second triangle. By reflecting these $6$ points along the semi-major and semi-minor axes, then one will get $6\times4 = 24$ points. (However, the Alpertron calculator says this has a total of $48$ .) Question : Is there a general way to derive the second triangle from the first triangle using some basic principles of conic sections? P.S. The reason I ask is I found three families of ellipses for $x^3+y^3+z^3=(z+1)^3$ , each with a pair of triangles with polynomial vertices. It can't be coincidence they come in pairs.","['number-theory', 'algebraic-geometry', 'conic-sections', 'diophantine-equations']"
4845674,A question regarding bounded variation and differentialibility as well as integration,"Let $a, b \in \mathbb{R}$ with $a < b$ , and $f: [a, b] \to \mathbb{R}$ be a monotonically increasing, right-continuous function. Show that there exists a monotonically increasing function $g: [a, b] \to \mathbb{R}$ and a Lebesgue-null set $N$ such that $f$ and $g$ are differentiable on $[a, b] \setminus N$ , the derivative of $g$ is zero everywhere on $[a, b] \setminus N$ , and the derivative $f'$ satisfies $$f(x) - f(a) = \int_a^x f'(t) \,d\lambda(t) + g(x)$$ for all $x \in [a, b] \setminus N$ . My first approach: Let $N$ be the set of points of discontinuity of $f$ . Define $g: [a, b] \to \mathbb{R}$ as follows: $$g(x) = \lim_{{t \to x^+}} f'(t)$$ Note that $g(x)$ is well-defined for each $x$ because $f$ is right-continuous. Furthermore, since $f$ is monotonically increasing, $g(x)$ is monotonically increasing as well. Both $f$ and $g$ are differentiable on the set $[a, b] \setminus N$ , since we have basically cut out the ""bad"" points for $f$ and since $f$ is monotonically increasing on $[a,b]/N$ , it is also differentiable. The differentialibility of $g$ follows from its definition as a limit. Which is the same reasion the derivative of $g$ is zero almost everywhere. However, I am a little bit lost regarding this integral identity. How can I derive that one? I know the fundamental theorem of calculus gives me $$f(x) = f(a) + \int_{a}^{x} f'(t) d\lambda(t)$$ if $f$ is continuous. But how can I argue that adding the function $g$ on the right side ""compensates"" the missing condition of $f$ being continuous? And how does my first approach look like in general? Are there any mistakes which I made? Could anybody give me feedback on that?","['integration', 'measure-theory', 'derivatives', 'bounded-variation']"
4845727,3-Point Compactification of $\mathbb{R}_{\geq 0} \times \mathbb{R}_{\geq 0}$,"Does there exist a compactification $X$ of $\mathbb{R}_{\geq 0} \times \mathbb{R}_{\geq 0}$ with the following properties? $X$ is compact $X$ is Hausdorff $\mathbb{R}_{\geq 0} \times \mathbb{R}_{\geq 0}$ is dense in $X$ $X \setminus (\mathbb{R}_{\geq 0} \times \mathbb{R}_{\geq 0}) = \{\infty_1, \infty_2, \infty_3\}$ If $A \subseteq \mathbb{R}_{\geq 0}$ is compact and $B \subseteq \mathbb{R}_{\geq 0}$ is closed but not compact, then $\overline{A \times B} = (A \times B) \cup \{\infty_1\}$ If $A \subseteq \mathbb{R}_{\geq 0}$ is closed but not compact and $B \subseteq \mathbb{R}_{\geq 0}$ is compact, then $\overline{A \times B} = (A \times B) \cup \{\infty_2\}$ If $A \subseteq \mathbb{R}_{\geq 0}$ is closed but not compact and $B \subseteq \mathbb{R}_{\geq 0}$ is closed but not compact, then $\overline{A \times B} = (A \times B) \cup \{\infty_1, \infty_2, \infty_3\}$ I was thinking of taking $X$ to be quotient of $[0,1] \times [0,1]$ where we identify $[0,1) \times \{1\}$ to the single point $\infty_1$ , $\{1\} \times [0,1)$ to the single point $\infty_2$ and $(1, 1)$ to $\infty_3$ and then embed $\mathbb{R}_{\geq 0} \times \mathbb{R}_{\geq 0}$ as $[0,1) \times [0,1)$ , but I don't think the resulting space is Hausdorff. The point of this is that we can informally express a sequence $(a_n)_{n \in \mathbb{N}}$ being Cauchy as "" $d(a_n, a_m) \to 0$ for $n, m \to \infty$ "". If we have a compactification with the above properties, we can formally express this as "" $d(a_n, a_m) \to 0$ for $(n, m) \to \infty_3$ "".","['general-topology', 'compactification']"
4845745,"Fubini's theorem for differential forms? Why does $\int_{t_0}^{t_1}(\oint_{\partial\Omega}j)dt=\int\limits_{[t_0,t_1]\times\partial\Omega}dt\wedge j$?","In an electrodynamics book I came across the following claim for the electric current density (twisted) 2-form $j$ along the boundary of some 3-dimensional volume $\Omega$ : $$\int_{t_{0}}^{t_{1}}\left(\oint_{\partial\Omega}j\right)dt=\int\limits _{\left[t_{0},t_{1}\right]\times\partial\Omega}dt\wedge j$$ It looks to me like an attempt to apply the Fubini's theorem for differential forms, but I do not see how the LHS can be reduced to a 2-argument function so that Fubini applies. The context is the conservation of charge law $\frac{dQ}{dt}=-\mathcal J$ , where $\mathcal J=\oint_{\partial\Omega}j$ is the electric current flowing out of $\Omega$ and $Q=\int_\Omega \rho$ is the total charge inside $\Omega$ . Integrating it along time from $t_0$ to $t_1$ , the book claims the equation above. The argument then continues with the definition of $J=-j\wedge dt+\rho$ (the $j\wedge dt$ part coming from the equation above) so that the conservation law can be written in a ""super-global"" form $\oint_{\partial\Omega_4}J=0$ for the 4-dimensional volume $\Omega_4=[t_0,t_1]\times\Omega$ . The book does it by an intermediate step that I omitted above: $$\int_{t_{0}}^{t_{1}}\left(\oint_{\partial\Omega}j\right)dt=\int_{t_{0}}^{t_{1}}dt\wedge\oint_{\partial\Omega}j=\int\limits _{\left[t_{0},t_{1}\right]\times\partial\Omega}dt\wedge j$$ (the middle term), but $dt\wedge\oint_{\partial\Omega}j$ seems syntactically invalid to me because $\oint_{\partial\Omega}j$ is a (time-dependent) number (and not a form) and so cannot be wedge-multiplied by $dt$ . Why does it hold? Does Fubini's theorem apply at all here?","['integration', 'electromagnetism', 'differential-forms', 'differential-geometry']"
4845760,Question about Ornstein–Uhlenbeck process,"Suppose $X(t) = e^{-t/2}W(e^{t})$ is an Ornstein–Uhlenbeck process ( $W(t)$ is a Wiener process) and $\epsilon > 0$ . I'm trying to find a constant $\delta>0$ so that $$
\lim_T\mathbb{P}\Big( \sup_{0 \leq t\leq T}|X(t)|^2 + \epsilon < \sup_{0 \leq t\leq T(1+\delta)}|X(t)|^2\Big) \approx 1.
$$ By a version of the law of the iterated logarithm, I know that $$
\lim_T (2\log(T))^{-1}\sup_{0 \leq t \leq T}|X(t)|^2 = 1 \quad a.s.,
$$ so that $$ \sup_{0 \leq t \leq T}|X(t)|^2 \approx 2\log(T), \quad \sup_{0 \leq t \leq T(1+\delta)}|X(t)|^2 \approx 2\log(T(1+\delta)) = 2\log(T) + 2\log(1+\delta) \quad a.s. $$ Thus, I'm tempted to just choose $\delta$ so that $ 2\log(1+\delta) > \epsilon $ , but I'm worried this intuition is flawed. Any help making this more rigorous would be much appreciated.","['stochastic-processes', 'probability-limit-theorems', 'probability-theory', 'stochastic-calculus']"
4845762,A metric on $L^1$ using measure of set where $f$ is greater than a value,"I am working on a problem showing that the following function describes a metric on $L^1$ for any measure space $(X,\mathcal{S},\mu)$ for a measurable set $E$ : Let -- for $f, g: E \to \mathbb{R}$ measurable -- $d(f)$ be defined as $$d(f) := \inf\{r > 0 \mid \mu(\{x \in E \mid |f(x)| > r\}) \leq r\}$$ and $d(f,g) := d(f - g)$ . I want to show that $d$ is a metric on $L^1(E)$ . So I need to prove $d(f,f) = 0 \quad\forall f$ $d(f,g) = d(g,f) \quad\forall f,g$ $d(f,h) \leq d(f,g) + d(g,h) \quad \forall f,g,h$ The first two traits were pretty straight forward using the definition of the absolute value on $\mathbb{R}$ , but I struggle showing the triangle inequality.","['measure-theory', 'metric-spaces', 'real-analysis']"
4845766,If $a_i/b_i$ converges then $\Sigma_m^na_i$ is infinitesimal iff $\Sigma_m^nb_i$ is too,"I am trying to solve question ( $6$ ) in section $6.11$ of Goldblatt's Lectures on the Hyperreals . The question asks: Given two series of positive terms $\sum_1^\infty a_i$ and $\sum_1^\infty b_i$ such that the sequence $(a_i/b_i:i\in\mathbb{N})$ converges in $\mathbb{R}$ , show that for unlimited $m$ and $n$ , $\sum_m^n a_i$ is infinitesimal if and only if $\sum_m^n b_i$ is infitesimal. I have not got very far. I tried to first show the forward implication.
For all naturals $i$ , $a_i>0$ so $a_i$ is positive for all hypernatural $i$ . For all $a_j$ with $n\le j\le m$ the sum $\sum_m^na_i>a_j>0$ , so if $\sum_m^na$ is infinitesimal,all $a_j$ are also infinitesimal. The sequence $a_i/b_i$ converges to a limit $L$ , so for all hypernaturals $j$ the term $a_j/b_j=L+\varepsilon_j$ where $\varepsilon$ is infinitesimal. So $a_j=b_j(L+\varepsilon_j)$ . From this, if $L$ is non-zero $b_j$ must be infinitesimal. Here, I tried rewriting the sum $\sum_m^n a_i=\sum_m^nb_i(L+\varepsilon_j)=L\sum_m^nb_i+\sum_m^nb_j\varepsilon_j$ . If the rightmost sum is infinitesimal, then it would follow that $\sum_m^n b_j$ is infinitesimal. The rightmost sum $\sum_m^nb_j\varepsilon_j$ is a sum of infinitesimals, so for all finite sums it is infinitesimal. However, if $n-m$ is unlimited, I don't know how to fix this argument.","['nonstandard-analysis', 'sequences-and-series']"
4845796,Show that the number of zeros of $e^{2z}-P(z)$ is not finite,"I'm trying to solve the following problem: Let $P(z) \neq 0$ be a complex polynomial. Use Jensen's Formula to show that the set of zeros of $e^{2z}-P(z)$ is not finite. ATTEMPT Suppose that it were finite, say the zeros are $\alpha_1,\dots,\alpha_n$ , and they are inside of $D(0,R)$ . Then, by Jensen's Formula, \begin{align*}
\log |f(0)|=-\sum_{k=1}^n \log \left(\frac{R}{\left|a_k\right|}\right)+\frac{1}{2 \pi} \int_0^{2 \pi} \log \left|f\left(R e^{i \theta}\right)\right| d \theta,
\end{align*} where $f(z)= e^{2z}-P(z)$ . Now my idea was to show that $\int_0^{2 \pi} \log \left|f\left(R e^{i \theta}\right)\right| d \theta$ is divergent, and therefore the formula wasn't valid, I'm not sure how to compute $\log \left|e^{Re^{i\theta}} -P(Re^{i\theta})\right|$ . I'm also aware of the following inequality derived from Jensen's Formula: \begin{align*}
n(R)\leq CR^{\rho},
\end{align*} where $\rho$ is the order of growth and $n(R)$ is the number of zeros inside $D(0,R)$ , but it doesn't seem relevant here since it's a $\leq$ -type inequality.","['complex-analysis', 'roots', 'polynomials']"
4845817,Calculating the Haar integral on $SU(2)$ in practice,"I'm trying to calculate the Haar integral on $SU(2)$ of a given function $f: SU(2) \to \mathbb{R}$ . For this particular function, I know the value of it on the subgroup $$T = \left\{ \begin{pmatrix} z & 0 \\ 0 & z^{-1} \end{pmatrix}: z \in \mathbb{C}, |z|=1  \right\} \leq SU(2)$$ and that every element of $SU(2)$ is conjugate to something in $T$ . I happen to also know that $f$ is a class function, i.e. $f(A) = f(XAX^{-1}) \forall X$ , indeed, I only have an explicit formula for $f$ on $T$ , say $f\left( \bigl( \begin{smallmatrix} z & 0 \\ 0 & z^{-1} \end{smallmatrix}\bigr) \right) = \overline{f}(z)$ . So if I were to calculate the Haar integral $\int_{SU(2)} f(A)dA$ , I'd ideally want to equate it to an integral over $T$ in some way, that is $$\int_{SU(2)} f(A)dA = \int_{T} f(A') \omega(A') dA' = \int_{0}^{2 \pi} \overline{f}(z) \overline{\omega}(z)dz$$ Where $\omega$ is some kind of weight function quantifiying how common $A' \in T$ is a conjugate of $A \in SU(2)$ over all of $SU(2)$ . That is, something similar to how the Jacobian acts during change of variables in multivariate integration. $\overline{\omega}$ is defined like how $\overline{f}$ is. But I'm struggling to figure out how I might begin calculating $\omega$ (all I really need is to calculate $\overline{\omega}$ ) or if this is even the ideal approach to calculate the Haar integral.","['integration', 'unitary-matrices', 'lie-algebras', 'haar-measure', 'lie-groups']"
4845854,No two adjacent bulbs on,"The problem is to count number of configuration of $9$ bulbs on a $3\times 3$ grid, where no two bulbs that are adjacent are switched on. I solved this problem in a very ad-hoc kind of manner, the answer is $63$ , here's how (I am representing configuration row wise as : $\mathtt{row_1}$ , $\mathtt{row_2}$ , $\mathtt{row_3}$ ) These are the only unique configurations of the bulbs $101|010|101$ or $010|101|010$ or $100|001|100$ * 4(depending on the $4$ possible position of 2nd row bulb on middle cell of every boundary) or $100|001|010$ * 4 (depending on the position of row 1 bulb on corners) So a total of $2^5 + (2^4-1) + 4\times 1 + (2^2-1)\times 4$ (subtraction is done to ensure no double counting) My question is, is there a more systematic way of solving this problem ?","['puzzle', 'combinatorics', 'probability']"
4845872,Unbounded symmetric probability density function with finite $\int_{-\infty}^\infty |z|^3 f(z) dz$ and $\int_{-\infty}^\infty f^2(z)dz$,"Condition $f(z)$ is a probability density function (p.d.f.) $f(z) = f(-z)$ $\int_{-\infty}^\infty f^2(z)dz < \infty$ $\int_{-\infty}^\infty z^2 f(z)dz < \infty$ $\int_{-\infty}^\infty |z|^3 f(z) dz < \infty$ $f$ is unbounded My idea $$
f(z)=\frac{|z|^{-\frac{1}{2}}}{4}\mathbb I_{[-1, 1]\backslash\{0\}}
$$ This unfortunately does not meet the third condition. And Gaussian does not meet the sixth condition. Is there any function f that satisfies 1~6?","['statistics', 'real-analysis', 'functions', 'probability-theory', 'probability']"
4845873,Does a vector space over $F$ necessarily contain a field isomorphic to $F$?,"This is a question I had and answered myself, and I didn't find this anywhere on Google so I thought it would be interesting to share. Initially posted this in Math Overflow but deleted that question because users notified me that it didn't fit there, but I still thought it was an interesting question so I'm putting it here again. By definition, a vector space $V$ over $F$ is the tuple $(V, F, \cdot)$ where $F$ is a field, $(V, +)$ is an abelian group, and $\cdot: V \times F \rightarrow V$ satisfies associativity, distributivity, and compatibility with the multiplicative identity in $F$ . I always thought of vector spaces as a generalization of field extensions, but upon reflection I couldn't actually prove using the definition that $V$ must contain something isomorphic to $F$ . If I think about vector spaces like $F^n$ and any ring extension over $F$ , it seems almost 'obvious' that $V$ should contain something isomorphic to $F$ . Or am I just missing a counterexample?","['linear-algebra', 'vector-spaces']"
4845941,Palett Jack trigonometric problem,"I have bit of mechanical problem in real life that I need to solve by math. Not sure is this trigonometric problem or something else but here's what I have. I'm not mathematician so don't blame me if I used incorrect markings on illustration above or not using proper terminology. Feel free to edit title or whatever is needed because this is the best I could do to explain. Anyway, I will try to explain the problem in my own way. I have 3 lines, R1A , AB and R2B in 2D space (XY). Line R1A is rotating around point R1 by angle α in negative direction (CCW). Line R2B is rotating around point R2 by angle β in positive direction (CW). Points A and B are connected with a line of known length ( c - distance AB has constant value). Coordinates of points in 2D space A , R1 and R2 are known and angles of rotation α and β are known. How to find coordinates of point B (distance R2B works too instead) so when we rotate line R1A by α angle, line R2B rotates by β angle? To visualize this (white values are known, orange is not ):",['trigonometry']
4846028,If $+\infty\leq\mu(A)$ then $\mu(A)=+\infty$,"Let $(X,\mathbb{X},\mu)$ be a positive measure space. Let $A$ belong to $\mathbb{X}$ . If $+\infty\leq\mu(A)$ then $\mu(A)=+\infty$ . Is this statement correct? My answer is yes. I believe this is very intuitive. However, I'm not entirely convinced of the formality. Indeed, $+\infty\leq\mu(A)$ and $\mu(A)\leq +\infty$ then $\mu(A)=+\infty$ . In this example, I would be treating infinity as a real number and applying the property: $a,b\in\mathbb{R}$ , $a\leq b$ and $b\leq a$ then $a=b$ . I would greatly appreciate your help.","['real-numbers', 'measure-theory', 'infinity', 'real-analysis']"
4846031,Why Applying A Continuous Linear Elliptic Differential Operator Gives A Functional and Clarifying Dual Space Definition,"I'm reading a paper on Multiscale FEM Methods and I just need a bit of help better visualizing how to interpret the dual space in the PDE setup. We are given a continuous, linear, elliptic differential operator $O: [H^1(\Omega)]^k \to X_0^*$ for $k=1 ,2 ,3$ . We are considering the problem $$
Ou=f\quad\text{ in }X^*_0
$$ where $X^* _0$ is defined as the dual space of $X_0$ , defined as $$
X_0 = \{ x \in [H^1(\Omega)]^k : x=0 \text{ on the Dirichlet boundary, $x = 0$ on the Multiscale FEM border} \}.
$$ We take $u \in X = \{ x \in [H^1(\Omega)]^k : x=0 \mbox { on the Dirichlet boundary} \}$ . In order to visualize this I took $k=1$ and the operator $O$ to be the Laplacian: furthermore I took $u = \sin(\pi x)$ on $\Omega = [0,2]$ . Then we know that $u$ is zero on the boundaries and $u \in H^1(\Omega)$ since $$
\|u\|_{H^1(\Omega)} = 1+\pi^2 < \infty.
$$ However, when we apply the differential operator I get $$
f = \pi^2 \sin(\pi x).
$$ This function does map input values to the field of real numbers but it is not a linear function. this goes against my understanding of what it means to be an element of the dual space. So my questions are: Am I misunderstanding something about the definition of a dual space? (I thought you just had to map elements from the space to a field via a linear function). Am I misunderstanding something else fundamental to the setup of this problem? Sorry if this is a silly question and thank you very much for any insights.","['dual-spaces', 'finite-element-method', 'functional-analysis', 'partial-differential-equations']"
4846034,"Solving a non-linear, coupled system of equations","So, say I have $$\frac{dx}{dt} = y \sqrt{x + y} \qquad \textrm{and} \qquad \frac{dy}{dt} = x \sqrt{x + y},$$ with initial conditions $$x(0) = 1, \quad y(0) = 0 .$$ I have managed to solve for $y$ in terms of $x$ : $y = \sqrt{x^2 - 1}$ . How would I proceed to solve for $x(t)$ and $y(t)$ , based on the given initial conditions?",['ordinary-differential-equations']
4846044,Function making a series convergent,"I have been trying to solve the following problem Assume $f : \mathbb{R} \to (0, \infty)$ is a function satisfying $\lim_{x \to 0} \frac{f(x)}{x} = 0$ . Prove that there exists a sequence $(x_n) \subset \mathbb{R}$ such that the series $\sum_{n = 1}^\infty x_n$ diverges, but $\sum_{n = 1}^\infty f(x_n)$ converges. While the claim seems pretty obvious, I feel I lack the necessary intuition to prove it.","['convergence-divergence', 'sequences-and-series', 'real-analysis']"
4846046,Is there an efficient way to check if there exists a linear map with the following property?,"Suppose I have two sets of non-zero vectors: $S_1 =\{v_1, \dots v_n\}$ and $S_2 = \{w_1, \dots w_n \}$ lying in $\mathbb{R}^m$ where $m < n$ . Further suppose that any $m$ of the vectors from $S_1$ or from $S_2$ is a basis for $\mathbb{R}^m$ . I want to know if there's an elegant way of knowing whether there exists an invertible linear map $T$ such that $T(S_1) = T(S_2)$ . A naive way which I could theoretically write a program for is as follows: Fix any choice of $m$ vectors in $S_1$ which I please. Then any choice of $m$ vectors in $S_2$ induces a linear map $T$ . I can represent this map as a matrix and check if $T(S_1) = S_2$ or not. This requires the construction of $\binom{n}{m}m!$ linear maps, which I would likely represent as matrices in a program. Thus $\binom{n}{m}m!$ matrices of dimension $m \times m$ . That's quite the calculation if $n$ is large. Is there a faster way to do this? Given the simplicity of the problem, it seems like there should be well-known faster way to do this if it exists.","['matrices', 'linear-algebra', 'linear-transformations']"
4846057,Proving the monotone class theorem (Any circular reasoning?),"I tried to prove the Monotone Class Theorem. We denote $A$ to be an algebra on $X$ , $M(A)$ be the monotone class generated by $A$ , $\sigma(A)$ be the sigma-algebra generated by $A$ . What we need to show is that $M(A)$ is an algebra. $M(A)$ is closed under complement is not hard for me. However, it is closed under finite union is not natural. My way goes as $\forall E \in M(A)$ , define $F_{E} =\{K \in M(A); K \cup E \in M(A)\}$ , it is not hard to check $F_{E}$ is a monotone class, and we have $\forall E \in M(A)$ , $F_{E} \subset M(A)$ Now if $W \in A$ , we have $F_{W} = M(A)$ (1) Can we extend (1) to all the elements of $M(A)$ , i.e. $\forall E \in M(A)$ , $F_{E} = M(A)$ ? The answer is 'Yes'. We need to show $\forall E \in M(A), A \subset F_{E}$ , Proof: $\forall V \in A$ , $F_{V}=M(A)$ , so $E \in F_{V}$ , so $E \cup V \in M(A)$ , so $V \in F_{E}$ . The proof seems right to me, but I always think there is some circular reasoning. Can you help me explain whether my proof is true and why the steps involved are logically reasonable?","['measure-theory', 'real-analysis']"
4846104,Asymptotic calculations for Bell numbers with little-o notation,"I'm having trouble doing calculations with the little-o notation. Let's say that the following asymptotic formula holds for $n\to\infty$ : $$ B_n = \frac{1}{\sqrt{r+1}} \exp\left( n \left(r+\frac{1}{r}-1\right)-1 \right)(1+o(1)) $$ where $r$ is the (unique and positive) solution to $re^r=n$ . In the following you can assume that you can use the same $r$ for $B_n$ , $B_{n+1}$ and $B_{n+2}$ (you can show that with some more asymptotics). Somehow you can then prove the identity $$\frac{B_{n+1}}{B_n} -1 = \frac{n}{r} (1+o(1))$$ and $$ \frac{B_{n+2}}{B_n} - \left( \frac{B_{n+1}}{B_n} \right)^2 -1 = \frac{n}{r^2} (1+o(1)). $$ How do I go about proving these formulas? I attempted to derive the first identity by just plugging in the asymptotic formula and got stuck: $$ \begin{align*} \frac{B_{n+1}}{B_n} - 1 &= \frac{ \frac{1}{\sqrt{r+1}} \exp\left((n+1)\left(r+\frac{1}{r}-1\right)-1\right) (1+o(1)) }{ \frac{1}{\sqrt{r+1}} \exp\left(n\left(r+\frac{1}{r}-1\right)-1\right) (1+o(1)) } -1 \\\\
&= \exp\left(r+\frac{1}{r}-1\right)(1+o(1)) - 1 \\\\
&= e^r e^{1/r} e^{-1} (1+o(1)) -1 \\\\
&= \frac{n}{r} e^{-1} (1+o(1)) -1 .
\end{align*} $$ In the last equality I used $e^r = \frac{n}{r}$ and that $e^{1/r}=1+o(1)$ (at least I think that's true b/c $n\to\infty$ implies $r\to\infty$ and $e^{1/r}\xrightarrow{r\to\infty} 1$ and afaik that's precisely what $1+o(1)$ means for $r\to\infty$ ). But now there's still the term $e^{-1}$ left, of which I can't say it's $1+o(1)$ , b/c $e^{-1}(1+o(1)) = e^{-1} + o(1)$ . So somewhere I made a mistake but I don't know where... (The $B_n$ are Bell numbers btw.)","['asymptotics', 'notation', 'combinatorics', 'discrete-mathematics', 'limits']"
4846121,Quotient Spaces (with respect to the Kernel of a Differential Operator),"I have some continuous, linear, elliptic linear differential operator $O: H^1(\Omega) \to X_0^*$ . $X_0^*$ is the dual space of $X = \{x \in H^1(\Omega): x=0 \mbox{ on } \partial \Omega \}$ . The differential equation setup is: $Ou=f$ in $X^*$ $u=0$ on $\partial \Omega$ The quotient space I am trying to visualize better is defined as $Q = \{ q-P_{\ker(O)}(q) , q \in X_0 \}$ . $P_{\ker(O)}(q)$ is a projection operator here. It is defined as a mapping from $H^1(\Omega)$ to $\ker(O)$ and is defined as $Pq = \sum_{i=1} ^{dim(\ker(O))} (q,\varphi_i) \varphi_i$ where $\varphi_i$ is an orthonormal basis of $\ker(O)$ . From my understanding of a quotient space, the set $Q$ should be the set of all affine subsets of $X_0$ parallel to $\ker (O)$ . So for example, if $O$ is the Laplacian, then $\ker(O) = $ the set of constant functions. Therefore, $Q$ , as I understand it should be all functions in $X_0$ parallel to the constant functions (here we would use the $L^2$ inner product to determine what is parallel). Question 1: Is this the right interpretation of a quotient group? Also, based on definitions of quotient groups online I would expect the elements to be $q+P_{\ker(O)}(q)$ . Does this make a difference in the interpretation? I was confused because later in the same paper that I'm reading they introduce a method and within that method they say ""orthogonalize the vectors
to the span of set S by using $g-P_{span(S)} (g)$ for each $g \in S$ ."" This looks just like how they defined the quotient group above. But I thought the quotient group would result in things parallel to the span of S in this setup, not orthogonal. Question 2: How can this expression be used to orthogonalize the vectors to the span of set $S$ when it is so close to the definition of a quotient group?","['partial-differential-equations', 'orthogonality', 'functional-analysis', 'real-analysis']"
4846133,"Prove that if $F: \mathbb R^n \to \mathbb R^n$, wth $n \geq 2$, is a smooth proper map with finitely many critical points, then $F$ is surjective","Prove that if $F: \mathbb R^n \to \mathbb R^n$ , $n \geq 2$ , is a smooth proper map with finitely many critical points, then $F$ is surjective. Here is my work so far: the set $R$ of all regular points of $F$ is open and path-connected, because $n \geq 2$ , and hence connected. For any $u \in R$ we have that $F_{*, p}$ for any $p \in F^{-1}(u)$ is surjective linear map from $\mathbb R^n$ to $\mathbb R^n$ so that it's invertible; thus, by the inverse function theorem, $F$ is locally invertible from a neighborhood of $p$ to a neighborhood $U$ of $u$ . By the inverse function theorem again, all points of $U$ are regular values. I am not sure how to proceed from here. I know that $R$ is path connected. If $v \in R$ is another regular point, then there exists is a path entirely contained within $R$ going from $u$ to $v$ . IF there exists a sequence of points that are in the image of $F$ that connects $u$ and $v$ in a dense way (I'm being very loose in language here, I hope you will get the picture), then we'll be done by using the compactness of the path and the inverse function theorem. However, I am not sure if this a priori is the case. I am not sure how to use the fact that $F$ is a proper map. How can I solve this problem?","['multivariable-calculus', 'differential-topology', 'derivatives', 'analysis']"
4846134,Triangle Inequality for Le Cam's Metric?,"Let $P, Q$ be probability distributions (say absolutely continuous with respect to the Lebesgue measure for simplicity).
The Le Cam metric (also sometimes called the Triangular Discrimination) is defined as $$\Delta(P, Q)^2 = \frac{1}{2}\int \frac{(dP-dQ)^2}{dP+dQ}$$ In Le Cam's book Asymptotic Methods in Statistical Decision Theory , it is stated (page 48) that the non-trivial part of verifying $\Delta(P, Q)$ is a metric reduces to the following inequality Let $c \geq b\geq a\geq 0$ . Then $$
\frac{c-a}{\sqrt{c+a}} \leq \frac{c-b}{\sqrt{c+b}}+\frac{b-a}{\sqrt{b+a}}.
$$ This itself follows from convexity of $x\mapsto 1/\sqrt{x}$ . I am trying to verify this myself.
I see how an inequality of the above form may be useful, in particular if the condition $c\geq b\geq a\geq 0$ may be ignored, then one can define $f(P,Q) = \frac{dP-dQ}{\sqrt{dP+dQ}}$ , a signed measure with respect to the Lebesgue measure.
The above inequality (ignoring the condition on $a,b,c$ ) can then be used to prove that $f(P,Q) \leq f(P,R)+f(R,Q)$ .
One can then argue that $$\Delta(P,Q) = \sqrt{1/2}\lVert f(P,Q)\rVert_2 \leq \sqrt{1/2}\lVert f(P,R)+f(R,Q)\rVert_2 \leq \sqrt{1/2}\lVert f(P,R)\rVert_2+\sqrt{1/2}\lVert f(R,Q)\rVert_2.$$ E.g. the result from sub-additivity of $\lVert \cdot\rVert_2$ . I'm stuck when trying to extend the proof to the case the condition on $(c,b,a)$ does not hold.
I've tried extending the proof of the above inequality to (for example) when $b\geq c\geq a\geq 0$ , but this goes nowhere. Can someone help me fill in this proof sketch?","['measure-theory', 'statistics', 'triangle-inequality', 'real-analysis', 'probability']"
4846240,Prove that: $(A\Delta B)\Delta C = (A \Delta C)\Delta (A\setminus B)$.,"For sets A, B and C, prove that: $(A\Delta B)\Delta C = (A \Delta C)\Delta (A \setminus B)$ . I've tried arbitrary element and tabular methods. Weird thing is that tabular (basically, truth table with 8 possible combinations) worked for me for the first 3 proofs of the exercise (which were pretty similar to this one) but not for this one. When, picking arbitrary element, I try to prove the $(A\Delta B)\Delta C \subset (A \Delta C)\Delta (A \setminus B)$ containment part, and I consider the case that $x\notin C$ (which, i think, implies that $x\in A\Delta B$ ) i always get to the conclusion that the property does not hold, so there is clearly something i'm missing. Edit: I contacted the author and indeed it was an error in the exercise. The answer i marked as the most useful in this post is correct: that is the same mistake the author identified. Thanks",['elementary-set-theory']
4846292,A question on pigeonhole principle? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 months ago . Improve this question I came across the following question which I could solve using a ""trial-and-error"" approach. However, is there a systematic way of solving it using the pigeonhole principle? ""When cleaning her children’s room, a mother found 9 socks. In a group of any 4 of the socks at least two belonged to the same child. In a group of any 5 of the socks no more than 3 had the same owner. How many children are there in the room and how many socks belong to each child?""","['pigeonhole-principle', 'combinatorics']"
4846312,Simple question on differential forms,"Let $\omega = dp_i \wedge dq^i$ be the standard symplectic form on $\mathbb{R}^{2n}(q^i, p_i)$ . (We use the Einstein summation convention throughout). Assume $dq^i = h^{ij} dp_j$ on a Lagrangian near $0$ for some function $h^{ij} = h^{ij}(p)$ . Then, in particular, $h^{ij} = h^{ji}$ . I want to show that there exists a function $f$ such that $$
h^{ij} = \frac{\partial f}{\partial p_i \partial p_j}
$$ and $\frac{\partial f}{\partial p_i}(0) = 0$ . I am stuck: Clearly, $0 = d^2 q^i = dh^{ij} \wedge dp_j$ and hence, $$
\frac{\partial h^{ij}}{\partial p_k} dp_k \wedge dp_j = 0.
$$ How to conclude from here? Edit: We see that $h^{ij} = \frac{\partial q^i}{\partial p_j}$ . Is it always true that in the case where $h^{ij} = h^{ji}$ , we have $h^{ij}$ is a second partial derivative of a function (due to Schwarz)?",['differential-geometry']
4846327,"Show that $\int_0^1 \left \lbrace (-1)^{\left \lceil \frac{1}{x} \right \rceil }\frac{1}{x} \right \rbrace \, {\rm d}x = \frac{\pi}{2}$","Show that $$\displaystyle{\int_0^1 \left \lbrace (-1)^{\left \lceil \frac{1}{x} \right \rceil }\frac{1}{x} \right \rbrace \, {\rm d}x = \frac{\pi}{2}}$$ My try $$\displaystyle{\int\limits_0^1 {\left\{ {{{\left( { - 1} \right)}^{\left[ {1/x} \right]}}\frac{1}{x}} \right\}dx}  = \mathop  = \limits^{x \to 1/x}  = \int\limits_1^\infty  {\left\{ {{{\left( { - 1} \right)}^{\left[ x \right]}}x} \right\}\frac{1}{{{x^2}}}dx}  = }$$ $$\displaystyle{\sum\limits_{n = 1}^\infty  {\left( {\int\limits_{2n - 1}^{2n} {\left\{ {{{\left( { - 1} \right)}^{\left[ x \right]}}x} \right\}\frac{1}{{{x^2}}}dx}  + \int\limits_{2n}^{2n + 1} {\left\{ {{{\left( { - 1} \right)}^{\left[ x \right]}}x} \right\}\frac{1}{{{x^2}}}dx} } \right)}  = }$$ $$\displaystyle{ = \sum\limits_{n = 1}^\infty  {\left( {\int\limits_{2n - 1}^{2n} {\left\{ { - x} \right\}\frac{1}{{{x^2}}}dx}  + \int\limits_{2n}^{2n + 1} {\left\{ x \right\}\frac{1}{{{x^2}}}dx} } \right)}  = }$$ $$\displaystyle{\sum\limits_{n = 1}^\infty  {\left( {\int\limits_{2n - 1}^{2n} {\left( {1 - \left\{ x \right\}} \right)\frac{1}{{{x^2}}}dx}  + \int\limits_{2n}^{2n + 1} {\left\{ x \right\}\frac{1}{{{x^2}}}dx} } \right)}}$$ I'm not sure about the closed form; my friends say it's either $\log\left(\frac{\pi}{2}\right)$ , $1 - \log\left(\frac{\pi}{2}\right)$ , or $\frac{\pi}{2}$ .","['integration', 'calculus', 'definite-integrals', 'ceiling-and-floor-functions']"
4846353,"Is a measure, whose distributional derivative is a measure, absolutely continuous wrt Lebesgue?",Suppose $\mu$ is a finite Borel measure on $\mathbb{R}^n$ with the property that it's distributional gradient $\nabla\mu$ is a vector-valued finite Borel measure. Does it follows then that $\mu$ itself is absolutely continuous with respect to Lebesgue measure?,"['lebesgue-measure', 'borel-measures', 'bounded-variation', 'distribution-theory', 'derivatives']"
4846362,How to fit an ODE to data?,"Consider the following ODE $$
y'(t)=\alpha x(t)-\beta y(t)
$$ and the following datasets $$
X=\{(t_0,x_0),...,(t_n,x_n)\}\\
Y=\{(t_0,y_0),...,(t_n,y_n)\}
$$ How can I find $\alpha$ and $\beta$ that best fits this data? In particular, is it possible to use the following solution $$
y(t) = e^{-\beta t} \left( \alpha \int_0^t e^{\beta \tau} x(\tau) \, d\tau + C \right),
$$ directly, where $C$ is a constant? Thoughts: This a fairly new field for me and I have briefly heard about inverse problems in ODEs , but rather than setting up a global minimizer that numerically finds it, I am wondering whether there is an analytical way of solving this problem, to the best possible fit in some sense . Example: As an example, one can consider the following data $$
\begin{align}
X&=\{91, 110, 125, 105, 88, 84\}\\
Y&=\{1.0, 0.97, 1.0, 0.95, 0.92, 0.89\}
\end{align}
$$ at time points $$
T=\{0, 5, 9, 18, 28, 38\}
$$ Using a numerical approach (see Python script here ), I get the following fit with $\alpha\simeq 0.000486$ , $\beta\simeq 0.057717$ , and a chi-square error of $0.0017296$ . Is it possible to improve this? I have tried global optimizers, but to no avail, hence thinking of a potential analytical approach.","['regression', 'optimization', 'ordinary-differential-equations', 'linear-regression']"
4846385,Cancelations on the limits of occupation times of Brownian motion,"Let $(W_t)_{t \ge 0}$ be a Brownian Motion in $\mathbb R^d$ for $d \in \{1,2\}$ (so that the process is recurrent). Consider the occupation time of Brownian motion given of a set $A \subset \mathbb{R}^d$ (of positive Lebesgue measure) $$
O_A(T) = \int_{0}^T 1_{A}(W_t)dt
$$ For fixed $A$ , it is clear that $O_A(T)$ diverges to $\infty$ almost surely as $T\to \infty$ . However, I was wondering whether if, for two disjoint sets $A_1,A_2$ (say with same Lebesgue measure) if the variable $$O_{A_1}(T)-O_{A_2}(T)$$ stays bounded as $T$ goes to infinity. My intuition being that a cancellation similar to the one that allows us to define the Green function takes place here. Furthermore, I would be interested if one can obtain good integrability control of this random variable as $T \to \infty$ . Say, control arbitrary integer moments or possibly even exponential ones. The question could also be posed in terms of local times, but I found it simpler to pose this way. Any references or remarks are welcome. Here I am asking for $A$ 's very general, but any particular choice would $A_1$ and $A_2$ would also be very enlightening. EDIT (24/Jan/24) : My language might have been imprecise on the above, so just as a further clarification. By ""good integrability control of this random variable as $T \to \infty$ .  Say, control arbitrary integer moments or possibly even exponential ones. "", I mean to understand how moments such as $$
(O_{A_1}(T)-
O_{A_2}(T))^k
,
|O_{A_1}(T)-
O_{A_2}(T)|^k,
e^{\theta|O_{A_1}(T)-
O_{A_2}(T)|}
$$ behave as $T \to \infty$ (where $k \in \mathbb{N},\mathbb{R}^+$ ). This allows for the moments to diverge as $T \to \infty$ , but I would like to understand the order of such divergence in terms of $T$ .","['local-time', 'brownian-motion', 'probability-theory', 'reference-request']"
4846417,Monotone (measurable) selection theorem,"Given a Polish space $X$ and a set valued map $\Psi:X\to 2^X$ we way $\Psi$ is weakly Borel if for every open $U\subset X$ , $$
\Psi^{-1}(U):=\{x\,|\,\Psi(x)\cap U\ne\varnothing\}\in\mathcal{B}(X).
$$ With the additional assumption that $\Psi(x)$ is a closed set for every $x$ , one can establish measurable selection theorems, and better even, a collection of selections whom are dense in $\Psi$ (see Castaing representations). In short, the denseness can be used to get a sequence of measurable maps, $\zeta_n$ , who get 'closer' to $\Psi$ (they are not necessarily selections yet) and upon taking a limit the closed-valuedness implies $\lim_n \zeta_n(x)=:\zeta(x)\in \Psi(x)$ and usual measurability arguments give $\zeta$ measurable. Now for the question: Proposition : Given a closed valued, weakly Borel set map $\Psi$ with the property $y\in\Psi(x)\iff x\in\Psi(y)$ , does there exist a Borel measurable selection $\zeta$ such that $\zeta=\zeta^{-1}?$ In general, the symmetry assumption alone is not sufficient to conclude the statement, in fact it is easy to find counterexamples (Take the set map from $\mathbb{R}$ to $\mathbb{R}$ with graph $\{(x,y)\,|\,x+y>(x-y)^2\}$ ). But instead suppose one relaxes the conclusion of the proposition to only require a Borel measurable involution on a set of positive Lebesgue measure, then counterexamples are less clear. Furthermore, one realizes that the above (relaxed) question (for $X=\mathbb{R}$ ) is equivalent to finding a measurable monotone selection of a set valued map and reflecting appropriately. To this end I have read some of the Economics papers on comparative statics which use order theory to get monotone selections. This said, they were not concerned about measurability, and I am having trouble branching the two theories. In summary, if anyone has insight on the question on finding a monotone measurable selection in $\mathbb{R}\to\mathbb{R}$ for some positive measure set this would be much appreciated.
Maybe I am missing something clear","['borel-sets', 'general-topology', 'set-valued-analysis', 'measure-theory']"
4846484,An upper bound of a function using its Taylor series.,"Let $f \in C^{m}(\mathbb{R}^n)$ of compact support. Then Taylor's theorem give us $$
f(x) = \sum_{|\alpha| \leq m} \frac{D^{\alpha} f(x_0)}{\alpha!} (x - x_0)^{\alpha}
+
\sum_{|\alpha| = m} h_{\alpha}(x)  (x - x_0)^{\alpha}
$$ using the notation in [https://en.wikipedia.org/wiki/Taylor%27s_theorem#Taylor's_theorem_for_multivariate_functions]. Suppose $D^{\alpha} f(x_0) = 0$ for all $|\alpha| \leq m$ .
Take some $|\beta| < m$ . I want to prove that $$
|D^{\beta} f(x)| < |x - x_0|^{m - |\beta|} C \sum_{|\alpha| \leq m} | D^{\alpha} f |_{\infty}
$$ for some $C > 0$ depending only on $m$ and $n$ . I am sure this is true but I am having difficulty dealing with the $h_\alpha$ function coming from the remainder term of the Taylor expansion.... how can I prove this? Thank you","['multivariable-calculus', 'taylor-expansion', 'real-analysis']"
4846487,let $f : \mathbb{R} \rightarrow \mathbb{R}^{+} C^2$ s.t. $f' < 0$ and $\sup_x \frac{f(x)f''(x)}{f'(x)^2} < 2$ show that $f=o\left(\frac{1}{x}\right)$,Problem $$ \text{Let } f : \mathbb{R} \rightarrow \mathbb{R}^+ \text{be a }C^2 \text{ function}\text{ such that } \sup_{x \in \mathbb{R}} \frac{f(x)f''(x)}{f'(x)^2} < 2 \text{ and } f'< 0  $$ $$ \text{Show that } f(x) = o\left(\frac{1}{x}\right) \text{ as } x \rightarrow \infty. $$ My attempt : First let's notice the fact that $1-(\frac{f}{f'})' = \frac{ff''}{(f')^2}$ . We have using the first hypothesis : $-(\frac{f}{f'})' \leq 1$ so by integrating we have $-\frac{f(x)}{f'(x)} \leq x - \frac{f(0)}{f'(0)}$ and thus $f'(x) \leq f(x)\frac{1}{x - c}$ then by Gronwall lemma we get $f(x) \leq f(0)\frac{1}{c(c-x)}$ where $c = \frac{f(0)}{f'(0)}$ which proves the fact that $f = O\left(\frac{1}{x}\right) \text{ as } x \rightarrow \infty.$ I can't find a way to have a better asymptotic result.,"['derivatives', 'analysis', 'real-analysis']"
4846608,Showing the symmetry of $\int_0^{\sinh^{-1} b}\frac{\tan^{-1}(\frac ab \cosh t)}{a\cosh^2t+\frac1a\sinh^2t} dt $,"The trigonometric integral $$I(a,b)= \int_0^{\sinh^{-1} b}\frac{\tan^{-1}(\frac ab \cosh t)}{a\cosh^2t+\frac1a\sinh^2t}\ dt
$$ exhibits the symmetry between the parameters $a$ and $b$ ,
i.e. $$I(a,b)=I(b,a)$$ I established the relationship based on heuristics as well as with enough trials and fails. Besides, I have verified it numerically with a large number of parameter pairs. However, I was unable to prove it. To be clear, I am not interested in a closed-form result for the integral, which may be more challenging. For the question, I am merely interested in showing that the symmetry can be derived analytically, perhaps via some conventional yet clever integration techniques.","['integration', 'trigonometric-integrals', 'symmetry', 'definite-integrals']"
4846610,Obtaining a bound on a function given a bound on its derivative,"Suppose $f\in C^1([0,1])$ satisfies $f(0)=0$ , $|f'(0)| <\infty$ and $|f'(t)| \leq \alpha t^{-1}$ on $(0,1]$ for some fixed positive constant $\alpha$ . I would like to show $f$ satisfies a lower bound on $[0,1]$ depending on $t$ and $\alpha$ , but independent of the value of $f'(0)$ . My guess is that $f(t) \geq c\ln(d)[1-\ln(t+d)]$ for some constants $c\in\mathbb{R}$ and $d>0$ depending only on $\alpha$ , since this satisfies $f(0)=0$ and $f'(t) = -\frac{c\ln(d)}{t+d}$ . I tried using the fundamental theorem of calculus: if $x\in(0,1]$ and $\epsilon<x$ , then $$f(x) = f(\epsilon) + \int_\epsilon^x f'(t)dt \geq f(\epsilon) - \alpha\int_\epsilon^x t^{-1}dt = f(\epsilon) - \alpha\ln(x) + \alpha \ln(\epsilon).$$ As $\epsilon\rightarrow 0$ , we have $f(\epsilon)\rightarrow 0$ , but the $\ln(\epsilon)$ term blows up. So I'm not really sure how to proceed, or if even such a bound is possible. Any help would be appreciated!","['integration', 'calculus', 'derivatives', 'real-analysis']"
4846625,Embedding for Hilbertian Metric?,"Let $X, Y$ be random variables with densites $X = f_x dx$ and $Y = f_ydx$ with respect to the Lesbegue measure. I'm interested in the metric $$d(X, Y)^2 = \frac{1}{2}\int \frac{(f_x - f_y)^2}{f_x+f_y}dx$$ This is called the Le Cam metric, or triangular discrimination (technically they differ by a constant factor). I had a previous question about it here . An alternative way to prove that it is a metric is done via this paper . I have reproduced the argument below By Berg, Christensen,
and Ressel [4, Ch. 3 (Proposition 3.2)] it suffices to show that the kernel
 is negative-definite. So let there be given two finite sequences of real
numbers, $(c_i)$ and $(p_i)$ , and assume that $\sum_i c_i = 0$ and that all the $p_i$ are positive. Then $$
\sum_{i,j}\frac{(p_i-p_j)^2}{p_i+p_j}c_ic_j = -4\sum_{i,j}\frac{p_ip_j}{p_i+p_j}c_ic_j
=-4\int_0^\infty\left(\sum_i c_i p_i \exp(-tp_i)\right)^2dt\geq 0.
$$ The citation is to Harmonic Analysis on Semigroups . My understanding is that this implies that there is some map $F : \mathbb{R}_{\geq 0}^n\to \mathcal{H}$ into a hilbert space such that $$d(X,Y) = \lVert F(X) - F(Y)\rVert_\mathcal{H}$$ I can almost see this directly.
In particular, one can write $$d(X,Y) = \lVert \frac{f_x + f_y}{2} - \frac{1}{(1/f_x) + (1/f_y)} \rVert_2$$ E.g. $d$ is the $\ell_2$ -norm of the difference between the arithmetic mean and harmonic mean. But I do not see the existence of the map $F$ that I am interested in yet. Does such an $F$ have an explicit form?","['hilbert-spaces', 'probability-theory', 'reproducing-kernel-hilbert-spaces']"
4846639,Identically distributed random variables and events of probability $0$,"Let $(\Omega,\mathcal{F},\mathbb{P})$ be a probability space and $X_1,\dots,X_{n+1}:\Omega\to \mathbb{R}$ be random variables. Suppose that the random variables are identically distributed, i.e. $$\mathbb{P}\circ X_1^{-1}(B) = \ldots = \mathbb{P}\circ X_{n+1}^{-1}(B), \ \ \forall B \in \mathcal{B}(\mathbb{R}).$$ Also suppose that there exist real-valued functions $f:\mathbb{R} \to \mathbb{R}$ and $h:\mathbb{R} \to \mathbb{R}$ such that, $$(\mathbb{P}\circ X_1^{-1})(\{x_1\in \mathbb{R}: f(x_1)\not = h(x_1) \}) \stackrel{\text{(1)}}{=} \mathbb{P}[\{\omega \in \Omega :f(X_1(\omega))\not = h(X_1(\omega))\}] = 0.$$ Now consider the remaining random variables as a random vector $X(\omega) = (X_2(\omega),\ldots , X_{n+1}(\omega)):\Omega \to \mathbb{R}^n$ . I want to prove that, $$(\mathbb{P}\circ X^{-1})(\{(x_2,\ldots , x_{n+1})\in \mathbb{R}^n: f(x_2)\not = h(x_2), \ldots, f(x_{n+1})\not = h(x_{n+1}) \}) \stackrel{\text{(2)}}{=} \mathbb{P}[\{\omega \in \Omega :f(X_2(\omega))\not = h(X_2(\omega)),\dots , f(X_{n+1}(\omega))\not = h(X_{n+1}(\omega))\}] = 0.$$ I don't know whether $(1)$ and $(2)$ are true but anyway, here is my proof. My try: Note that $$\mathbb{P}[\{\omega \in \Omega :f(X_2(\omega))\not = h(X_2(\omega)),\ldots , f(X_{n+1}(\omega)) \not = h(X_{n+1}(\omega))\}] = \mathbb{P}[B_2 \cap \ldots \cap B_{n+1}],$$ where $B_j = \{\omega \in \Omega: f(X_j(\omega))\not = h(X_j(\omega))\}$ and $j=2,\ldots , n+1$ . We know $\mathbb{P}[B_j] = 0$ , since random variables are identically distributed, and that the events of probability $0$ are independent of all other events. So this implies that, $$\mathbb{P}[B_2 \cap \dots \cap B_{n+1}] = \mathbb{P}[B_2]\ldots \mathbb{P}[B_{n+1}] = 0.$$ Is this reasoning correct? Is it possible to use this line of reasoning with push-forward measure $\mathbb{P}\circ X^{-1}?$ The push-forward measure assigns probability to a vector of real numbers, so I don't know how to describe the correct subset of $\mathbb{R}^n$ and apply the measure to it.","['measure-theory', 'measurable-functions', 'probability-theory', 'probability', 'random-variables']"
4846658,Question regarding rewriting sum of sine and cosine into a single cosine term,"Let's say that we have the following function: $$
x(t) = A_1\cos(\omega t) + A_2\sin(\omega t)
$$ Then we can rewrite it by defining a right triangle with an angle $\delta$ , which has adjacent side $A_1$ , opposite side $A_2$ , and hypotenuse $A = \sqrt{A_1^2 + A_2^2}$ . So $$
\delta = \arctan \frac{A_2}{A_1}.
$$ It then follows that: $$
x(t) = A \biggl( \frac{A_1}{A}\cos(\omega t) 
+ \frac{A_2}{A}\sin(\omega t) \biggr) 
= A \bigl( \cos(\delta)\cos(\omega t) 
+ \sin(\delta)\sin(\omega t) \bigr) 
= A\cos(\omega t - \delta).
$$ However, let's say we now have the following function: $$
x(t) = A_1e^{i\omega t} + A_2e^{-i \omega t} 
= (A_1+A_2)\cos(\omega t) + (A_1-A_2)\,i\sin(\omega t).
$$ It turns out that this can still be rewritten as $$
x(t) = A\cos(\omega t - \delta).
$$ But how and why? Because we now have a complex amplitude $(A_1-A_2)i$ for the $\sin(\omega t)$ part. So using the same definition of the right triangle now doesn't make sense right? How are $A$ and $\delta$ supposed to be defined now compared to before? Reference:","['complex-analysis', 'trigonometry']"
4846679,"Understanding limits of integration after transformation $(x,y) \mapsto (x-y,x+y)$","Consider the following double-integral: $$
\iint_{[0,a]\times[0,a]} (x-y)^2 dxdy \stackrel{*}{=} \int_0^a \int_0^a (x-y)^2 dxdy = \frac{a^4}{6}
$$ where $*$ follows by iterated integration. Suppose instead we consider the substitution $w=x-y, z=x+y$ . The Jacobian of this transformation is $1/2$ , and so $$
\iint_{[0,a]\times[0,a]} (x-y)^2 dx dy
=
\frac{1}{2} \iint_{R_2} w^2 dw dz
=\frac{1}{2} \int_{Y_1}^{Y_2} \left (\int_{X_1}^{X_2} dz\right )w^2 dw.
$$ My question is: how do I go about solving this via iterated integration (with respect to $z$ first, then with respect to $w$ ), or more specifically, what should the limits of integration $Y_1,Y_2,X_1,X_2$ be? If we consider the transformation $(x,y)\mapsto (w,z)$ as a linear map $$
A = \begin{bmatrix}1 & -1 \\ 1 & 1\end{bmatrix}
$$ then this transforms the square $[0,a]\times [0,a]$ into the parallelogram with vertices: $(0,0), (a,a),(0,2a), (-a,a)$ , which is the region bounded by the lines: $$
y=x, \quad y=x+2a, \quad y=-x+2a, \quad y=-x,
$$ or equivalently by the lines $$
w=0, \quad w=-2a, \quad z=0, \quad z=2a,
$$ and so $$
R_2 = \{(w,z) : -2a \le w \le 0, ~ 0\le z \le 2a \}.
$$ This is not a rectangular region in $(w,z)$ -space, but clearly it is not correct to take $Y_1 = -2a, Y_2=0, X_1 = 0, X_2=2a$ , since it does not lead to the answer found by direct integration mentioned in the first display. My confusion is about how to relate $z$ and $w$ and choose the inner integral limits properly.","['integration', 'definite-integrals', 'multivariable-calculus', 'calculus', 'multiple-integral']"
4846752,Is this equivalent to the definition of directional derivative?,"Let $f=f(x_1,x_2)$ be a $C^1$ scalar valued function of two variables at the point $\vec{x}_0$ . We know the directional derivative of $f$ at $\vec{x}_0$ in the direction of $\vec{v}$ (unit vector) is given by $$ \lim_{h \to 0} \frac{f(\vec{x}_0+h\vec{v})-f(\vec{x}_0)}{h} = D_{\vec{v}}f(\vec{x}_0).$$ What can we say about the following limit: $$\lim_{h \to 0} \frac{f(\vec{x}_0+h\vec{v} + h^2 \vec{w})-f(\vec{x}_0)}{||h\vec{v} + h^2 \vec{w}||}$$ where $\vec{w}$ (unit vector, linearly independent from $\vec{v}$ ) is another direction ? Is this the same as $D_{\vec{v}}f(\vec{x}_0)$ ?","['gateaux-derivative', 'derivatives']"
