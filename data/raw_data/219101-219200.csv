question_id,title,body,tags
4479627,Is this shock/rarefaction problem solved correctly?,"So I'm coursing a subject in PDE and the theory for these kind of problems is quite scarce, and I don't seem to find many solved problems online. The problem is stated as follows: Analyse the following shock-fitting problem. Draw the solution u(x, t) for several times. \begin{equation}
 \begin{cases}
  u_t + uu_x = 0, & t>0, \ \ x\in \mathbb{R} \\
  u(x, 0) =
   \begin{cases}
     1, \\
    -1, \\
     0, 
   \end{cases}
   & 
   \begin{aligned}
    x < 0 \\
    0 < x < 1 \\
    x> 1
   \end{aligned}
 \end{cases}
\end{equation} Notation: The equation can be written as $u_t + c(u)u_x = 0$ where $c(u) = u$ . Solution: Let $\Phi(x) := u(x,0)$ . We can rewrite the system as: \begin{cases}
\frac{dX}{dt} = U, & X(0) = \xi \\
\frac{dU}{dt} = 0, & U(0) = \phi(\xi)
\end{cases} From here, we see that \begin{equation}
U(t) = U(0) = \Phi(\xi) = 
\begin{cases} 
1, &  \xi < 0 \\
-1, & 0 < \xi < 1 \\
0, & \xi > 1
\end{cases}
\end{equation} So, it's easy to figure out \begin{equation}
X'(t) = \Phi(\xi) \Rightarrow X(t) = \Phi(\xi)t + \xi = 
\begin{cases} 
2t + \xi, &  \xi < 0 \\
-2t + \xi, & 0 < \xi < 1 \\
\xi, & \xi > 1
\end{cases}
\end{equation} Now, the case for $\xi = 0$ . We introduce the flux $q$ and identify $q'(u) = c(u) = u$ , so $q(u) = \frac{u^2}{2}. We then can find the shock wave \begin{equation}
 \frac{ds}{dt} = \frac{[q]}{[u]} = \frac{1}{2} \cdot \frac{1^2-(-1)^2}{1-(-1)} = 0 \Rightarrow s(t) = 0
\end{equation} For the case $\xi = 1$ \begin{equation}
 c(u) = c^{-1}(u) = u \Rightarrow u(x,t) = \frac{x-1}{t}
\end{equation} The final solution then would be \begin{equation}
u(x,t) = 
\begin{cases}
1, & x<0 \\
-1, & 0<x<1 \\
\frac{x-1}{t}, & 1<x<t \\
0, & x \geq t
\end{cases}
\end{equation} I feel very insecure about this answer. If it is indeed right, I'd also appreciate to know why . Thanks in advance. EDIT: Also, are there any resources where I can learn about this kind of problems?","['ordinary-differential-equations', 'real-analysis', 'functional-analysis', 'wave-equation', 'partial-differential-equations']"
4479645,Proving rank deficiency of a matrix whose elements are given by trigonometric functions,"I want to show that a specific $(N^2+1)\times 3N$ matrix ( $N\geq 3$ ) is rank deficient, specifically that it has rank $3N-1$ . Ideally, I would like to show that this is the case for all $N\geq 3$ but I'm also very happy if I can show the case $N=3$ . Below is the matrix for $N=3$ . Each element is either $0$ , $1$ or a trigonometric function in two out of $2N+1$ variables ( $x_i, y_i, z$ ; these can be chosen at random with $z\neq 2\pi m$ for $m\in\mathbb{Z}$ ). A formal definition of the matrix is given below. In addition, I provide the Python code which generates this matrix; this code also verifies the rank deficiency numerically. I would like to know if there exist any techniques for showing rank deficiency which I could use to approach this problem. This is what I tried so far: I removed any factors that are constants per column in order to arrive at a simpler version of the column space; the matrix below is the most simple version I could come up with after removing a bunch of such factors. I asked Mathematica to compute the rank of the $N=3$ matrix, however, it returned $9$ as an answer, so most likely it failed the constant problem . I also asked Mathematica to compute the singular values (so I could check them myself) but it didn't finish the computation within a reasonable amount of time. I asked SymPy to compute the rank, the singular values, the QR decomposition and the nullspace of the $N=3$ matrix but it didn't finish the computation within a reasonable amount of time for either task. For $N=3$ , the reduced matrix by removing the first row must have rank no larger than the original matrix. Since this reduced matrix happens to be a square matrix, I can compute the determinant. After doing trigonometric expansion of all the involved terms, the problem of the determinant being zero should be decidable (albeit computationally expensive). However, then it remains to show that the first row $[0,0,0,0,0,0,1,1,1]$ can be constructed from a linear combination of all the remaining rows. This amounts to solving $cM = \mathrm{row}_1$ which again might be too difficult to compute (as it involves division by $\sin,\cos$ terms). When looking at the $N^2\times N$ matrix that consists of all the column factors stacked row-wise, there are always exactly $3N-2$ non-duplicated rows in that matrix (see below for $N=3$ and $N=4$ examples). In fact, for each specific combination of all three factors $\{-1,0,1\}$ , there are always two such rows in this factor matrix. Each other combination of only two factors occurs only once. So I thought about using the $3N-2$ row vectors from the actual matrix that correspond to the non-duplicated rows in the factor matrix, together with the $[0,0,0,0,0,0,1,1,1]$ row vector and show that all other row vectors can be constructed from linear combinations of these, but I couldn't come up with a solution (the problem is that every row in the actual matrix contains a different combination of $x_i,y_j$ ). I tried to find the coefficients of the nullspace ""by hand"" (or, rather, by being creative), however, I couldn't come up with a solution. The Harmonic Addition Theorem seemed promising and it helped me to get an expression for the sum of trigonometric terms per row, however, I couldn't figure out how to show that these expressions can be made zero for all of the rows by an appropriate choice for the linear coefficients (as these coefficients are contained inside the $\tan(\delta) = \dots$ expressions). And that's all the techniques/approaches that I am aware of. Thus, I would be glad if someone could point me in the right direction for approaching this problem. Formal definition of the matrix Let $x_i,y_i,z \in \mathbb{R}$ with $0\leq i < N$ and $z \neq 2\pi m$ ( $m\in\mathbb{Z}$ ) and $N\geq 3$ . Let $F_{kl}$ be a matrix of shape $N^2\times N$ . Assume the row ordering given by $k = iN+j$ (where $i,j$ refer to the symbols $x,y$ ). Then $F_{kl}$ is defined in the following way: $$
F_{kl} = \begin{cases}
-1 & \; , \quad 0           &\leq l < \min(i,j+1) \\
 0 & \; , \quad \min(i,j+1) &\leq l < \max(i,j+1) \\
+1 & \; , \quad \max(i,j+1) &\leq l < N \\
\end{cases}
$$ Further let $M_{kl}^{(1)}, M_{kl}^{(2)}, M_{kl}^{(3)}$ be matrices of shape $N^2\times N$ ; again $k = iN+j$ . $$
\begin{aligned}
M_{kl}^{(1)} &= \cos\left(x_i + y_j + F_{kl}z\right)\\
M_{kl}^{(2)} &= \sin\left(x_i + y_j + F_{kl}z\right)\\
M_{kl}^{(3)} &= \left(1-|F_{kl}|\right)\cdot \begin{cases}
\sin\left(x_i - y_j + \frac{z}{2}\right) & \; , \quad i \leq j\\
\sin\left(y_j - x_i + \frac{z}{2}\right) & \; , \quad i > j\\
\end{cases}\\
\end{aligned}
$$ Then the matrix $M$ of interest is given by a column stack of the individual matrices $M_{kl}^{(1)},M_{kl}^{(2)},M_{kl}^{(3)}$ , augmented by an additional row $[0,\dots,0,0\dots,0,1\dots,1]$ : $$
M = \begin{bmatrix}
& & & & & & & & \\
& M^{(1)} & & & M^{(2)} & & & M^{(3)} & \\
& & & & & & & & \\
0 & \dots & 0 & 0 & \dots & 0 & 1 & \dots & 1 \\
\end{bmatrix}
$$ Objectives Conjecture 1: $M$ is rank deficient. Conjecture 2: $M$ has rank $3N-1$ . I am mainly interested in conjecture 1. The matrix for $N=3$ $$
\left[\begin{matrix}0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1\\\cos{\left(x_{0} + y_{0} \right)} & \cos{\left(x_{0} + y_{0} + z \right)} & \cos{\left(x_{0} + y_{0} + z \right)} & \sin{\left(x_{0} + y_{0} \right)} & \sin{\left(x_{0} + y_{0} + z \right)} & \sin{\left(x_{0} + y_{0} + z \right)} & \sin{\left(x_{0} - y_{0} + \frac{z}{2} \right)} & 0 & 0\\\cos{\left(x_{0} + y_{1} \right)} & \cos{\left(x_{0} + y_{1} \right)} & \cos{\left(x_{0} + y_{1} + z \right)} & \sin{\left(x_{0} + y_{1} \right)} & \sin{\left(x_{0} + y_{1} \right)} & \sin{\left(x_{0} + y_{1} + z \right)} & \sin{\left(x_{0} - y_{1} + \frac{z}{2} \right)} & \sin{\left(x_{0} - y_{1} + \frac{z}{2} \right)} & 0\\\cos{\left(x_{0} + y_{2} \right)} & \cos{\left(x_{0} + y_{2} \right)} & \cos{\left(x_{0} + y_{2} \right)} & \sin{\left(x_{0} + y_{2} \right)} & \sin{\left(x_{0} + y_{2} \right)} & \sin{\left(x_{0} + y_{2} \right)} & \sin{\left(x_{0} - y_{2} + \frac{z}{2} \right)} & \sin{\left(x_{0} - y_{2} + \frac{z}{2} \right)} & \sin{\left(x_{0} - y_{2} + \frac{z}{2} \right)}\\\cos{\left(x_{1} + y_{0} - z \right)} & \cos{\left(x_{1} + y_{0} + z \right)} & \cos{\left(x_{1} + y_{0} + z \right)} & \sin{\left(x_{1} + y_{0} - z \right)} & \sin{\left(x_{1} + y_{0} + z \right)} & \sin{\left(x_{1} + y_{0} + z \right)} & 0 & 0 & 0\\\cos{\left(x_{2} + y_{0} - z \right)} & \cos{\left(x_{2} + y_{0} \right)} & \cos{\left(x_{2} + y_{0} + z \right)} & \sin{\left(x_{2} + y_{0} - z \right)} & \sin{\left(x_{2} + y_{0} \right)} & \sin{\left(x_{2} + y_{0} + z \right)} & 0 & \sin{\left(- x_{2} + y_{0} + \frac{z}{2} \right)} & 0\\\cos{\left(x_{1} + y_{1} - z \right)} & \cos{\left(x_{1} + y_{1} \right)} & \cos{\left(x_{1} + y_{1} + z \right)} & \sin{\left(x_{1} + y_{1} - z \right)} & \sin{\left(x_{1} + y_{1} \right)} & \sin{\left(x_{1} + y_{1} + z \right)} & 0 & \sin{\left(x_{1} - y_{1} + \frac{z}{2} \right)} & 0\\\cos{\left(x_{1} + y_{2} - z \right)} & \cos{\left(x_{1} + y_{2} \right)} & \cos{\left(x_{1} + y_{2} \right)} & \sin{\left(x_{1} + y_{2} - z \right)} & \sin{\left(x_{1} + y_{2} \right)} & \sin{\left(x_{1} + y_{2} \right)} & 0 & \sin{\left(x_{1} - y_{2} + \frac{z}{2} \right)} & \sin{\left(x_{1} - y_{2} + \frac{z}{2} \right)}\\\cos{\left(x_{2} + y_{1} - z \right)} & \cos{\left(x_{2} + y_{1} - z \right)} & \cos{\left(x_{2} + y_{1} + z \right)} & \sin{\left(x_{2} + y_{1} - z \right)} & \sin{\left(x_{2} + y_{1} - z \right)} & \sin{\left(x_{2} + y_{1} + z \right)} & 0 & 0 & 0\\\cos{\left(x_{2} + y_{2} - z \right)} & \cos{\left(x_{2} + y_{2} - z \right)} & \cos{\left(x_{2} + y_{2} \right)} & \sin{\left(x_{2} + y_{2} - z \right)} & \sin{\left(x_{2} + y_{2} - z \right)} & \sin{\left(x_{2} + y_{2} \right)} & 0 & 0 & \sin{\left(x_{2} - y_{2} + \frac{z}{2} \right)}\end{matrix}\right]
$$ The Python code for generating the matrix and for verifying the rank deficiency numerically: from argparse import ArgumentParser

from sympy import cos, sin, symbols, Matrix, latex


parser = ArgumentParser()
parser.add_argument('N', type=int, default=3, help='Matrix will be of shape (N^2+1, 3N)')
N = parser.parse_args().N


x = symbols([f'x{i}' for i in range(N)], real=True)
y = symbols([f'y{i}' for i in range(N)], real=True)
z = symbols('z', real=True)


def column_factors():
    """"""For each row of the matrix, yield the two relevant variables (v1, v2)
       as well as a list of N factors which are applied to N sets of 3 columns (type 1,2,3).""""""
    for i in range(N):
        yield from ((x[i], y[j], [-1]*(i)   + [0]*(j-i+1) + [1]*(N-j-1)) for j in range(i  ,N))
        yield from ((y[i], x[j], [-1]*(i+1) + [0]*(j-i-1) + [1]*(N-j))   for j in range(i+1,N))


# def column_factors():
#     """"""Alternative way to produce the column factors. Basically, using this function will
#        result in a row permutation with respect to the other definition of `column_factors`
#        above.
#        This definition here produces variable combinations in sequential form:
#        (x0,y0), (x0,y1), ..., (x0,y{N-1}), (x1,y0), (x1,y1), ..., ...
#     """"""
#     for i in range(N):
#         yield from ((y[j], x[i], [-1]*(j+1) + [0]*(i-j-1) + [1]*(N-i))   for j in range(0, i))
#         yield from ((x[i], y[j], [-1]*(i)   + [0]*(j-i+1) + [1]*(N-j-1)) for j in range(i, N))


def type_1(v1, v2, z, factor):
    return cos(v1 + v2 + factor*z)


def type_2(v1, v2, z, factor):
    return sin(v1 + v2 + factor*z)


def type_3(v1, v2, z, factor):
    return (1-abs(factor))*sin(z/2 + v1 - v2)  # 1-abs(factor) == (1+factor)%2   (does this help?)


M = [  # The matrix; will be of shape (N**2+1, 3*N) after building.
    [0, 0]*N + [1]*N,
]
for v1, v2, factors in column_factors():
    M.append(
          [type_1(v1, v2, z, f) for f in factors]
        + [type_2(v1, v2, z, f) for f in factors]
        + [type_3(v1, v2, z, f) for f in factors]
    )
M = Matrix(M)
assert M.shape == (N**2+1, 3*N)


with open('matrix.tex', 'w') as fh:
    print(latex(M), file=fh)


# === Mathematica ===

with open('matrix.nb', 'w') as fh:
    fh.write(
        str(M)
        .removeprefix('Matrix(')
        .removesuffix(')')
        .replace('[', '{')
        .replace(']', '}')
        .replace('sin(', 'Sin[')
        .replace('cos(', 'Cos[')
        .replace(')', ']')
    )

# ===================


# === SymPy ===

def my_iszero(x):
    """"""From: https://docs.sympy.org/latest/tutorial/matrices.html#zero-testing""""""
    from sympy import exp
    print('.', end='', flush=True)  # track progress
    try:
        return x.rewrite(exp).simplify().is_zero
    except AttributeError:
        return None

# print(f'\nDet: {Matrix(M.tolist()[1:]).det().expand(trig=True).simplify()}')  # this computes forever
# print(f'\nRank: {M.rank(iszerofunc=my_iszero)}')  # this computes forever

# =============


# === Verify numerically ===

import numpy as np

rng = np.random.default_rng()
values = rng.uniform(low=0, high=100, size=2*N+1).tolist()  # x, y and z

M = M.subs(list(zip([*x, *y, z], values))).evalf().tolist()
M = np.array(M, dtype=float)

print(f'Rank: {np.linalg.matrix_rank(M)}')
print(f'Singular values (largest, smallest): {np.linalg.svd(M)[1][[0, -1]]}')

# ========================== Exploring non-duplicated combinations of column factors The following is the code for exploring non-duplicated rows in the factor-matrix $F_{kl}$ (it can be put before the # === Mathematica === block): from collections import Counter
from pprint import pprint

counts = Counter('  '.join(f'{i: d}' for i in f) for _,_,f in column_factors())
print(
    'Number of non-duplicated rows:',
    sum(1 for f,c in counts.items() if c == 1)  # this equals 3*N-2
)
pprint(counts) This is the output for $N=3$ : Number of non-duplicated rows: 7
Counter({'-1    0    1': 2,
         ' 0    1    1': 1,
         ' 0    0    1': 1,
         ' 0    0    0': 1,
         '-1    1    1': 1,
         '-1    0    0': 1,
         '-1   -1    1': 1,
         '-1   -1    0': 1}) This is the output for $N=4$ : Number of non-duplicated rows: 10
Counter({'-1   0   1   1': 2,
         '-1   0   0   1': 2,
         '-1  -1   0   1': 2,
         ' 0   1   1   1': 1,
         ' 0   0   1   1': 1,
         ' 0   0   0   1': 1,
         ' 0   0   0   0': 1,
         '-1   1   1   1': 1,
         '-1   0   0   0': 1,
         '-1  -1   1   1': 1,
         '-1  -1   0   0': 1,
         '-1  -1  -1   1': 1,
         '-1  -1  -1   0': 1})","['matrix-rank', 'matrices', 'linear-algebra', 'combinatorics', 'trigonometry']"
4479648,Sum of all integers up to $x$ with digit sum $t$,"If $S(x,t)$ is the sum of all integers up to $x$ whose sum of digits is $t$ , is there a way to calculate it? I mean for high arbitrary $x$ . For example, $S(120,11) = 29+38+47+56+65+74+83+92+119 = 603$ I was trying to work with the integers mod $9$ (because an integer mod $9$ is equal to its sum of digits mod $9$ ) but haven't seen any pattern emerge. I assume it is somehow related to the count of such integers. It would be best to find an exact formula or a way to calculate it exactly, but an asymptotic approximation for its limit as $x$ is large would be nice to see as well.","['summation', 'number-theory', 'discrete-mathematics', 'sequences-and-series', 'limits']"
4479674,Puzzling function,"Let $f$ be a function whose domain is the set of positive integers, and for positive integers $a$ , $b$ and $n$ , if $a + b = 2^{n}$ , then $f(a) + f(b) = n^2$ . What is $f(2021)$ ? I started by testing values for $a$ , $b$ and $n$ , with the hope of finding a pattern, but so far I can't say I've made any headway; For $a=b=n=1,\ f(1)+f(1)=2^1, so f(1)=\frac{1^2}{2}=\frac12$ I realize that it becomes easier to find $f(a)$ if $a$ = $b$ So $a = b = 2^{n-1}$ then $f(a) = \frac{n^2}{2}$ that is, $f(2^{n-1}) = \frac{n^2}{2}$ At this point, I cannot see where to move forward.","['contest-math', 'algebra-precalculus']"
4479739,"Using integration by parts to show $\int_{\Sigma} |\nabla^N_{\Sigma} X|^2 = - \int_{\Sigma} \langle X, \Delta^N_{\Sigma} X \rangle$","I'm trying to work through a derivation of the stability operator from minimal surface theory. Suppose $\Sigma^k$ is a minimal submanifold of $\mathbb{R}^n$ , and suppose $X$ is a normal vector field on $\Sigma$ with compact support vanishing on the boundary. Part of the derivation involves the integration by parts $\int_{\Sigma} \langle \nabla_{\Sigma}^N X, \nabla_{\Sigma}^N X \rangle = - \int_{\Sigma} \langle X, \Delta^N_{\Sigma} X \rangle$ , where $\nabla_{\Sigma}^N$ is the normal projection of the covariant derivative on $\Sigma$ , and $\Delta^N_{\Sigma}$ is the normal Laplacian defined by $\Sigma_{i = 1}^k \nabla_{E_i}^N \nabla_{E_i}^N X - \nabla^N_{\left(\nabla_{E^i} E_i\right)^T} X$ , with $E_i$ being an orthonormal frame for $\Sigma$ . Why does this hold? I'm aware of Green's identity holding for the Laplacian and gradient of scalar functions, but the operators involved here are normal projections, and I want to know why the identity still holds in this case. I have tried to compute $\nabla_{\Sigma}^N (\nabla_{\Sigma}^N X)$ in hopes that its inner product with $X$ is the same as that with $\Delta_{\Sigma}^N X$ , up to some terms which vanish under the integral like a divergence, but I haven't gotten anywhere.","['riemannian-geometry', 'minimal-surfaces', 'vector-analysis', 'differential-topology', 'differential-geometry']"
4479770,Limit does not preserve strict inequality,"When I was reading Rudin's proof of L'Hospital's Rule, I was confused with the following two lines. Since my question is not directly related to the proof itself but more on the limiting behavior of functions, I will only mention the part I don't understand. The set up is that functions $f,g$ are continuous on $[a,b]$ and differentiable on $(a,b)$ , the limit of $\frac{f'(t)}{g'(t)}$ exists and quals to $A$ , which could be anything on the extended real line. Given that $$\frac{f(x) - f(y)}{g(x) - g(y)} =\frac{f'(t)}{g'(t)} < r,$$ where we can treat $x$ as the variable, $y$ and $t$ as fixed, then as $x\to a$ , Rudin wrote $$\frac{f(x)}{f(y)}\le r. $$ Given that $$\frac{f(x)}{g(x)}< r - \frac{g(y)}{g(x)}+\frac{f(y)}{f(x)},$$ if we let $x\to a$ , then $$\frac{f(x)}{g(x)} < r .$$ Notice the first one changes to $\le$ after taking the limit while the second one remains $<$ . May I ask if there is a general rule on this? Could it be related to limsup or liminf? New update: I was actually misquoted the second part - Rudin wrote $\frac{f(x)}{g(x)}< q$ directly.","['limits', 'derivatives', 'continuity', 'real-analysis']"
4479774,Completion of the stalk of a flat scheme over a complete local Noetherian ring,"This is an exercise from Brian Conrad's 2006 problems on group schemes and p-divisible groups. Let $X$ be a flat scheme, locally of finite type over a complete local Noetherian ring $R$ with residue field $k$ . Let $x\in X(R)$ be a section. If $X$ has smooth geometric closed fibre, prove that $\mathcal O_{X,x_0}^\wedge$ ( $x_0$ the closed point of the section) is isomorphic to a formal power series ring over $R$ in finitely many variables. This seems to me like a boosted form of Cohen's structure theorem, but instead of a coefficient field, it's a pretty nice ring. Since the question is local on $X$ , we can assume $X = \operatorname{Spec}A$ is affine. Then the section $x$ corresponds to a ring map $A\to R$ with kernel $I$ such that $A/I\simeq R$ , and this is in fact an $R$ -splitting, so $A \simeq R\oplus I$ as $R$ -modules. Since $R$ is local, there is a unique maximal ideal lying over $I$ , say $\mathfrak m_0$ . Since $R$ is local, the map $R\to A$ is in fact faithfully flat. Since $X_{\bar{k}}$ is smooth, $\mathcal O_{X_{\bar{k}},(x_0)_{\bar{k}}}^\wedge$ is isomorphic to $\bar{k}[[t_1,\dots,t_n]]$ for some $n$ . I want to show that this implies $\mathcal O_{X,x_0}^\wedge\simeq R[[t_1,\dots,t_n]]$ . As a module, $A = R\oplus I$ , so it seems like the completion of $A$ at $\mathfrak m_0$ should be the sum of the completion of $R$ along $\mathfrak m$ (which is just $R$ ) and the completion of $I$ along $\mathfrak m_0$ , which ""feels like"" a power series ring over $R$ , but I don't know how to justify this.","['formal-completions', 'noetherian', 'commutative-algebra', 'algebraic-geometry', 'schemes']"
4479789,Is the smooth mapping space a deformation-retract of the continuous one?,"Let $X,Y$ be smooth manifolds. Let $F \subset G$ denote the spaces of, respectively, smooth and continuous functions $X\rightarrow Y$ ; we give $F,G$ the compact-open topologies. (This should be compatible with the subspace topology on $F$ from $G$ .) Since every continuous map is homotopic to a smooth one by e.g. Whitney approxiomation,
we know that $F$ meets every path-component of $G$ . I was wondering, is $F$ furthermore a deformation-retract of $G$ ?","['function-spaces', 'general-topology', 'analysis', 'smooth-manifolds']"
4479812,Product of distributions under wavefront set condition is zero,"Assume $u, v \in \mathcal{D}'(\mathbb{R}^n)$ are distributions with compact support. Denote by $\operatorname{WF}(\bullet) \subset T^*\mathbb{R}^n \setminus 0$ the wavefront set of a distribution $\bullet$ . If $\operatorname{WF}(u) \cap \operatorname{WF}(v) = \emptyset$ , then their product $uv$ is well defined. If $uv = 0$ , does this imply that at an open and dense set of points one of the two distributions vanish, that is, $\operatorname{supp}(u)^c \cup \operatorname{supp}(v)^c \subset \mathbb{R}^n$ is open and dense? For the background on the wavefront set and distribution theory, see Chapter 8 of Hörmander's book The Analysis of Linear Partial Differential Operators I . For products under the wavefront set condition, see Theorem 8.2.10 of the same book. I've also asked this question on Mathematics Overflow . Remarks: As noted below by Vinicius in the comments, if $u, v \in C^\infty_c(\mathbb{R})$ with $\operatorname{supp}(u) = [-1, 0]$ and $\operatorname{supp}(v) = [0, 1]$ , then $uv = 0$ but $\operatorname{supp}(u)^c \cup \operatorname{supp}(v)^c = \mathbb{R}\setminus 0$ , so open and dense is the most we can hope for. The same question makes sense also if $u$ and $v$ don't have compact support. In that case, if for simplicity we set $n = 2$ and there are transversal smooth vector fields $X$ and $Y$ such that $X u = 0$ and $Y v = 0$ , then the wavefront set condition is automatically satisfied. In fact, in a suitable local coordinate system adapted to $X$ and $Y$ , one can show that $uv$ is locally a tensor product and so at each point either $u = 0$ or $v = 0$ . One should keep in mind the trivial case when $X = \partial_{x_1}$ and $Y = \partial_{x_2}$ .","['microlocal-analysis', 'analysis', 'distribution-theory']"
4479817,Reasoning for the Power Set,"Suppose we define a function $f:X\rightarrow Y$ . Since, for each $x\in X$ there are $\#Y$ possibilities for $x$ to be mapped onto $Y$ , it's natural to understand that: \begin{equation}
Y^X=\prod_{x\in X}Y
\end{equation} denotes how many functions from $X$ to $Y$ there are. Thing is, in the specific case where $x$ has only $\textbf{2}$ possible mappings, one gets: \begin{equation}
P(X)=2^X
\end{equation} being defined as the power set of $X$ . It can be defined via the statement: \begin{equation}
x\in P(X)\iff x\subseteq X
\end{equation} which yields the definition as the set of all subsets of $X$ . I've been not able to understand that. I mean, why does restricting $x$ to two mappings immediately gives you that the number of different mappings are exactly the number of subsets of $X$ ?",['elementary-set-theory']
4479842,Optimize $xyz$ where $x+y+z=1$ and $x^2+y^2+z^2=1$?,"Im trying to optimize $f(x,y,z)=xyz$ restricted to $g(x,y,z)=x+y+z=1$ and $h(x,y,z)=x^2+y^2+z^2=1$ . $∇f=(yz,xz,xy)$ , $∇g=(1,1,1)$ and $∇h=(2x,2y,2z)$ . I tried using the determinant $det(∇f,∇g,∇h)=yz(2z-2y)-xz(2z-2x)+xy(2y-2x)=0$ which I dont know what to do with and I cant simplify the determinant in a good way with row operations. I also tried solving $z$ from $g=1$ . $z=1-x-y$ . $f(x,y,1-x-y)=xy-x^2y-xy^2$ restricted to $h(x,y)=2x^2+2y^2-2x-2y+2xy+1$ with Lagrange multiplier but I made no progress there either as the partial derivatives got too messy.","['multivariable-calculus', 'calculus', 'optimization', 'lagrange-multiplier']"
4479852,Estimating the Number of Pokemon Without Knowing How Many There Are,"I was imagining the following scenario: Suppose you are playing a Pokemon game for the first time and you don't know how many Pokemon are there. You spend some time playing today, encounter some random Pokemon, and record how many unique Pokemon you came across (e.g. you saw 15 total Pokemon, but only 8 of them were unique). Tomorrow, you spend some time playing and you encounter some Pokemon (e.g. 25, but only 7 were unique ) - now you record how many unique Pokemon you saw today and add these to the number of unique Pokemon you saw yesterday. You repeat this process a few times, and after collecting ""n"" number of samples, you have observed ""m"" number of unique samples. Using this information, are there any mathematical formulas that you can use to estimate the total number of Pokemon that might exist in the game? I am thinking that there might already exist some statistical/probability formulas that can be used for estimating the population size based on some finite samples, but so far I have not found any such formulas that can be directly used for this problem. I came across two somewhat related concepts in math ( https://en.wikipedia.org/wiki/German_tank_problem , https://en.wikipedia.org/wiki/Coupon_collector%27s_problem ), but I am not sure how to apply these concepts to this problem of estimating the number of Pokemon. To make things a little more concrete, I wrote some computer code (R programming language) that attempts to simulate this problem. Suppose we are playing the original Pokemon game and there are 150 Pokemon : library(dplyr)
library(ggplot2)

pokemon_id = 1:150

pokemon_names = names = c(""Bulbasaur"",""Ivysaur"",""Venusaur"",""Charmander"",""Charmeleon"",""Charizard"",""Squirtle"",""Wartortle"",""Blastoise"",""Caterpie"",""Metapod"",""Butterfree"",""Weedle"",""Kakuna"",""Beedrill"",
          ""Pidgey"",""Pidgeotto"",""Pidgeot"",""Rattata"",""Raticate"",""Spearow"",""Fearow"",""Ekans"",""Arbok"",""Pikachu"",""Raichu"",""Sandshrew"",""Sandslash"",""Nidoran"",""Nidorina"",""Nidoqueen"",""Nidorino"",""Nidoking"",
          ""Clefairy"",""Clefable"",""Vulpix"",""Ninetales"",""Jigglypuff"",""Wigglytuff"",""Zubat"",""Golbat"",""Oddish"",""Gloom"",""Vileplume"",""Paras"",""Parasect"",""Venonat"",""Venomoth"",""Diglett"",""Dugtrio"",""Meowth"",""Persian"",
          ""Psyduck"",""Golduck"",""Mankey"",""Primeape"",""Growlithe"",""Arcanine"",""Poliwag"",""Poliwhirl"",""Poliwrath"",""Abra"",""Kadabra"",""Alakazam"",""Machop"",""Machoke"",""Machamp"",""Bellsprout"",""Weepinbell"",""Victreebel"",""Tentacool"",
          ""Tentacruel"",""Geodude"",""Graveler"",""Golem"",""Ponyta"",""Rapidash"",""Slowpoke"",""Slowbro"",""Magnemite"",""Magneton"",""Farfetch’d"",""Doduo"",""Dodrio"",""Seel"",""Dewgong"",""Grimer"",""Muk"",""Shellder"",""Cloyster"",""Gastly"",""Haunter"",
          ""Gengar"",""Onix"",""Drowzee"",""Hypno"",""Krabby"",""Kingler"",""Voltorb"",""Electrode"",""Exeggcute"",""Exeggutor"",""Cubone"",""Marowak"",""Hitmonlee"",""Hitmonchan"",""Lickitung"",""Koffing"",""Weezing"",""Rhyhorn"",""Rhydon"",""Chansey"",""Tangela"",
          ""Kangaskhan"",""Horsea"",""Seadra"",""Goldeen"",""Seaking"",""Staryu"",""Starmie"",""Mr.Mime"",""Scyther"",""Jynx"",""Electabuzz"",""Magmar"",""Pinsir"",""Tauros"",""Magikarp"",""Gyarados"",""Lapras"",""Ditto""
          ,""Eevee"",""Vaporeon"",""Jolteon"",""Flareon"",""Porygon"",""Omanyte"",""Omastar"",""Kabuto"",""Kabutops"",""Aerodactyl"",""Snorlax"",""Articuno"",""Zapdos"",""Moltres"",""Dratini"",""Dragonair"",""Dragonite"",""Mewtwo"",""Mew"")

pokemon_data = data.frame(pokemon_id, pokemon_names) Now, suppose you have 20 days to play this game - each day, you encounter a random number of Pokemon and keep track which of these Pokemon were unique (note: In the real example, we don't know there are 150 Pokemon, but I have included this number to facilitate some of the calculations) : pokemon_function <- function() {

  pokemon_results <- list()

  for (i in 1:20) {

    run_i <- i
     pokemon_caught_i <- abs(sample.int(10, 1))
    sample_i <- pokemon_data[sample(nrow(pokemon_data), pokemon_caught_i), ]
    pokemon_tmp <- data.frame(run_i, sample_i)
      pokemon_results[[i]] <- pokemon_tmp
  }
  results_df <- do.call(rbind.data.frame,   pokemon_results)

  pokemon_int <- data.frame(results_df %>%
                         group_by(pokemon_id) %>%
                         filter(run_i == min(run_i)) %>%
                         distinct)

  pokemon_caught <- data.frame(pokemon_int %>%
                               group_by(run_i) %>%
                               summarise(Count=n()))

  cumulative <- cumsum(pokemon_caught $Count)
  pokemon_caught$ Cumulative <- cumulative
  pokemon_caught$unseen <- 150 - cumulative
  return(pokemon_caught)
} We can now see how many (cumulative) unique Pokemon we encountered each day for 20 days (note: I am assuming that there is an equal probability of encountering any given Pokemon) : [1]  1 11 15 24 27 28 34 35 36 38 45 53 56 59 62 64 68 71 In theory, we could repeat this simulation experiment many times (e.g. 50 times) and visualize the results: #Repeat Simulation 50 Times:

final <- list()
for (i in 1:50) {
  round_i <- i
  s_i <- pokemon_function()
  final_tmp <- data.frame(round_i, s_i)
  final[[i]] <- final_tmp
}

visualization_file <- do.call(rbind.data.frame, final)

visualization_file $round_i = as.factor(visualization_file$ round_i)

 g1 = ggplot(data=visualization_file, aes(x=run_i, y=Cumulative, group = round_i, colour = round_i)) + geom_line() +labs(y= ""Total Pokemon Seen"", x = ""Iterations"") +geom_point() + ggtitle(""Pokemon Simulation: Number of Unique Pokemon Seen in Different Simulations"")

g2 = ggplot(data=visualization_file, aes(x=run_i, y=unseen, group = round_i, colour = round_i)) + geom_line() +labs(y= ""Total Pokemon Not Seen"", x = ""Iterations"") +geom_point() + ggtitle(""Pokemon Simulation: Number of Unique Pokemon Not Seen in Different Simulations"") Obviously, with enough time, we would surely encounter every single unique Pokemon in this game.  But is there some mathematical formula that can be used to estimate the total number of Pokemon based on a single random sample? If I have the following measurements ( 1, 11 ,15 ,24 ,27 ,28 ,34 ,35, 36, 38, 45, 53, 56, 59, 62, 64, 68, 71) - is there some mathematical formula that can be used to estimate the total population size? Thank You!","['statistics', 'probability']"
4479909,Proof that plane has small inductive dimension 2?,"By definition, the small inductive dimension $\operatorname{ind}(\emptyset) = -1$ and, recursively, the small inductive dimension $\operatorname{ind}(X)$ of a nonempty topological space $X$ is the least integer $n \geq 0$ such that each point $x$ has a local base (i.e., a neighborhood base) of open sets $V$ such that $\operatorname{ind}(\operatorname{\partial} V) \leq n - 1$ , where $\operatorname{\partial} V$ denotes the boundary of $V$ . (Some versions of the definition require that each neighborhood $U$ of $x$ contain such a $V$ with $\operatorname{cls} V \subset U$ . However, that requirement here is redundant since the spaces involved are regular.) That $\operatorname{ind}(\mathbb{R}) = 1$ is easy to see: $\mathbb{R} \neq \emptyset$ ; and each point in $\mathbb{R}$ has arbitrarily small neighborhoods of the form $V = (a, b)$ , and $\partial\,V$ is the two-point discrete space $\{a, b\}$ , which is 0-dimensional. But what about $\mathbb{R}^2$ : is there an elementary proof that $\operatorname{ind} (\mathbb{R}^2) = 2$ ? Evidently $\operatorname{ind}(\mathbb{R}^2) \leq 2$ , since each point has arbitrarily small neighborhoods that are open disks, and the boundary of such a disk has small inductive dimension $1$ . Moreover, $\operatorname{ind}(\mathbb{R}^2) \neq 0$ , since the plane is connected; and of course $\operatorname{ind}(\mathbb{R}^2) \neq -1$ . So the thing that remains to prove is that $\operatorname{ind} (\mathbb{R}^2) \neq 1$ . I know there are not-so-elementary proofs that $\operatorname{ind} (\mathbb{R}^n) = n$ , but I'm looking for an elementary proof just in the case $n = 2$ .","['general-topology', 'dimension-theory-analysis']"
4479912,Difference between running maximum and reflected Brownian motion,"Let $(B_t)_{t > 0}$ be a Brownian motion and let $M_t = \sup_{0 \leq s \leq t} B_s$ . I was wondering whether $M_t - |B_t|$ was a Brownian motion as well ? If so, why ? From this question, I would naively expect $M_t - |B_t|$ to behave like $B_t$ .","['brownian-motion', 'probability-theory']"
4479934,"Root in $(1,2]$ of Equation $x^n-x-n=0$","Consider the equation $x^n-x-n=0$ with $n\in\mathbb{N},n\geq2.$ $a)$ Show that this equation has exactly one solution $u_n\in(1,2].$ $b)$ Show that the sequence $\left\{u_n\right\}$ is decreasing. $c)$ Determine $L=\lim_{n\rightarrow\infty}u_n.$ Here is what I've done so far on this problem. $a)$ Let $f_n(x)=x^n-x-n,$ then clearly $f_n$ is continuous and $f_n(1)=-n<0.$ Let $g(n)=f_n(2)=2^n-2-n,$ then $g'(n)=2^n\cdot\ln2-1>0$ as $n\geq2,$ hence $$f_n(2)=g(n)\geq g(2)=2^2-2-2=0.$$ Furthermore, $\frac{d}{dx}f_n(x)=nx^{n-1}-1\geq1-1=0$ as $n\geq2$ and $x\geq1.$ Therefore, $f_n$ is non $-$ decreasing on $[1,2]$ and $f_n(1)<0\leq f_n(2),$ and the Intermediate Value Theorem implies the unique root $u_n$ as desired. $c)$ Note that $u_n^n-u_n-n=0\Leftrightarrow u_n=\sqrt[n]{u_n+n}$ and for $a>0$ fixed, $$\lim_{n\rightarrow\infty}\ln(\sqrt[n]{a+n})=\lim_{n\rightarrow\infty}\frac{\ln(a+n)}{n}=\lim_{n\rightarrow\infty}\frac{1}{a+n}=0$$ hence $$1=\lim_{n\rightarrow\infty}\sqrt[n]{1+n}\leq\lim_{n\rightarrow\infty}\sqrt[n]{u_n+n}\leq\lim_{n\rightarrow\infty}\sqrt[n]{2+n}=1$$ so $L=1.$ I'm currently stuck with part $b),$ so any hints/ideas/comments are appericated. Thank you!","['calculus', 'convergence-divergence', 'sequences-and-series']"
4479957,"What (if anything) is wrong with this ""proof"" of $\frac{d}{dx}e^x=e^x$?","I'm in physics and am not super adept at developing completely rigorous proofs. I was curious to prove that $\frac{d}{dx}e^x=e^x$ for myself and I came up with the following proof, but I can't find anyone else doing the proof in precisely this way, which makes me suspect that there is some subtle issue with it. I'd like to understand what that issue is or if it is, in fact, a valid proof. I begin with the limit definition of $e$ $$e\equiv\lim_{h\to0}(1+h)^{\frac{1}{h}}$$ The definition of the derivative (obviously): $$\frac{df}{dx}\equiv\lim_{h\to0}\frac{f(x+h)-f(x)}{h}$$ Then evaluate these as most people do: $$\frac{d}{dx}e^x=\lim_{h\to0}\frac{e^{x+h}-e^x}{h}$$ $$=\lim_{h\to0}\frac{e^{x}e^{h}-e^{x}}{h}$$ $$=e^x\lim_{h\to0}\frac{e^h-1}{h}$$ Plugging in the definition of $e$ : $$=e^x\lim_{h\to0}\frac{((1+h)^{\frac{1}{h}})^h-1}{h}$$ $$=e^x\lim_{h\to0}\frac{(1+h)-1}{h}=e^x\lim_{h\to0}(1)=e^x(1)=e^x$$ The only thing I can think that may be wrong with this proof is if it is not valid to fold the limit that comes from the derivative and the limit that comes from the definition of $e$ into the same limit index $h$ , but if that is the case, I don't see why.","['calculus', 'solution-verification', 'derivatives', 'real-analysis']"
4479971,What is the relationship between these two versions of BSD?,"The BSD conjecture is usually formulated like this. If $E/\mathbf{Q}$ is an elliptic curve, then $$ \text{rank }E/\mathbf{Q} = \text{ord}_{s=1} L(E,s), $$ where $L(E,s)$ is the Hasse-Weil $L$ -function of $E$ . But in Anthony Knapp's book Elliptic Curves (see Conjecture 1.9 on page 17), there is an another more ""elementary"" formulation of BSD, given like this: If $E/\mathbf{Q}$ is an elliptic curve, then $$ \prod_{p<X} \dfrac{\#E(\mathbf{F}_p)}{p} \sim (\text{const}) (\log X)^r, $$ where $r = \text{rank }E/\mathbf{Q}$ . The main difference between the two versions of BSD is that the ""analytic"" sides are formulated differently. In the first version, the analytic quantity is $\text{ord}_{s=1}L(E,s)$ . In the second version, the analytic quantity is the rate of growth of the product $\prod_{p<X} \dfrac{\#E(\mathbf{F}_p)}{p}$ . My question is : why are these two analytic quantities the same? That is, why is the exponent $r$ in the formula $$\prod_{p<X}{} \dfrac{\#E(\mathbf{F}_p)}{p} \sim (\text{const})(\log X)^r$$ actually equal to $\text{ord}_{s=1} L(E,s)$ ? The heuristic reason for this is that if we formally evaluate $L(E,s)$ at $s=1$ via the Euler product definition, we get $$L(E,1) ""="" \prod_{p<X} \dfrac{p}{\#E(\mathbf{F}_p)},$$ but this does not rigorously make sense because the Euler product definition only converges for $\text{Re }s> 3/2$ . So is there a rigorous way to justify this phenomenon?","['number-theory', 'l-functions', 'elliptic-curves']"
4479984,"New ""Ace-or-King"" cards are added to a 52-card deck. What is the chance of drawing at least 1 Ace and 1 King in an opening hand of 5 cards?","Background: Ace-or-King (AoK) cards can be either an Ace or a King, but not at the same time. In a standard 52-card Bicycle deck, there are already 4 Aces and 4 Kings. To the deck, we will be adding 4 additional Ace-or-King (AoK) cards. Question: What is the chance of drawing 5 cards from this non-standard 56-card deck in which those cards consist of at least one Ace and at least one King? (Order does not matter.) What I've tried: I haven't tried much fruitfully. There's too many combinations of mixings of these cards for me to keep track of. I can have a deck consisting of Aces, Kings, AoKs, and Other. Normally, for combinations of cards which can't be used as semi-wildcards, I would use the following logic. $$
P({\text{at least 1 ace and at least 1 king}}) = 1 - \frac{\binom{52 - 4 \text{ aces}}{5} + \binom{52 - 4 \text{ kings}}{5} - \binom{52 - (8 \text{ aces or kings}}{5}}{\binom{52}{5}}
$$ Build the complement up as a sum of each of the configurations in which an opening hand has none of the cards of each class, excluding the double counts across configurations without both Aces and Kings. Then, I take 1 - the ratio of those configurations with all possible configurations. This would give me the probability of drawing at least 1 Ace and at least 1 King, however, by adding the 4 AoK cards. There would now be a case in which I draw no Aces and no Kings, drawing one AoK will not be sufficient but two AoKs will make an appropriate hand. There are also the cases in which an AoK could be used for an Ace, but it's needed for a King. So, I would need the following configurations: at least 1 Ace, at least 1 King or AoK at least 1 Ace or AoK, at least 1 King at least 2 AoK So would I compute the configurations like above for each of the above and combine all of them? Or is there some simpler model for this kind of thing?",['combinatorics']
4479997,"Solution of non-linear differential equation $y''+ay'+b\sin(y)\cos(y)=c$, where $a,b,c$ are constants.","I am working on a project, in that project model I arrived at a differential equation. For further analysis, I need the solution of this equation. I tried to solve the equation but couldn't get success. Please anyone give some idea to solve this. $y''+ay'+b\sin(y)\cos(y)=c$ , where $a,b,c$ are the constants means they are combinations of parameter values of real life materials. If $y'=0$ then it can be solved by multiplying the equation by $y'$ and integrating it. But, in the present case I'm not getting any clue. Thanks in advance. Note: Ideas in the direction of approximated solution are also invited.",['ordinary-differential-equations']
4480012,Show that $\sum_{n\le x}\max(n)=O(x)$,"[An Introduction to Sieve Methods and Their Applications- M Ram Murty, pg.14, Q35,36] Let $\max(n)$ denote the largest exponent appearing in the unique factorization of $n$ into distinct prime powers. Show that $$\sum_{n\le x}\max(n)=O(x)$$ Now, show that for some constant $c>0$ , we have $$\lim_{x\to \infty}\sum_{n\le x}\max(n)\sim cx$$ What can be said about the error term? What can be said about the constant $c$ ? For the first one, I tried using Abel summation on the function $\max(n)$ , but couldn't make much progress. The second one, I feel, should follow quite easily from the first, but I can't see how.","['analytic-number-theory', 'number-theory']"
4480031,"What makes $\frac{x(x+1)}{2}$ a ""better"" interpolant of the sum of the first $n$ positive integers than any other, and likewise in similar cases?","A long-time interest of mine has been trying to determine if there is some ""natural"" criterion by which we can characterize various interpolants of functions that are at first only defined for integer or positive-integer inputs, such as exponentiation and the factorial function, which generalize to the exponential and gamma functions. The aim ultimately is to create an explicit and efficient series formula for the interpolation of tetration $$^n a = \underbrace{a^{a^{a^{\cdots^a}}}}_\text{$n$ copies of $a$}$$ to noninteger values of $n$ , in some suitably natural manner. But before we can get there, it seems more profitable to first look at simpler cases, and one of the simplest such cases seems to be interpolating discrete sums. For example, consider $$f(n) = \sum_{k=1}^{n} k.$$ As written, this definition makes no sense for, say, $f\left(\frac{1}{2}\right)$ . However, it is not hard to see that this can be converted into a ""closed form"" that, even better, doubles as an interpolative extension: $$\sum_{k=1}^{n} k = \frac{n(n+1)}{2}$$ which allows us to expand to real $x$ ""lengths"" via $$\sum_{k=1}^{x} k := \frac{x(x+1)}{2}, x \in \mathbb{R}$$ (or even $x \in \mathbb{C}$ !) And then we can find $$\sum_{k=1}^{1/2} k = f\left(\frac{1}{2}\right) = \frac{3}{8}.$$ Another example where we can find such a sum is in the case of exponential functions. Consider $$\sum_{k=0}^{n-1} 2^k$$ for positive integer $n$ . It is easy to see this sums to $$\sum_{k=0}^{n-1} 2^k = 2^n - 1$$ which lets us find that $$\sum_{k=0}^{1/2} 2^k\ \text{""should be""}\ \sqrt{8} - 1 \approx 1.8284.$$ And of course for arbitrary $b$ , we have the geometric series $$\sum_{k=0}^{n-1} b^k = \frac{b^n - 1}{b - 1}.$$ However, what about for more general functions $f$ ? The fundamental problem is that, strictly speaking , these interpolants are not unique: most generally, we can ""connect the dots"" with any curves we like. Even if we impose some ""natural"" restrictions, say that $$\sum_{k=0}^{x} f(k) = f(x) + \sum_{k=0}^{x-1} f(k)$$ it isn't enough - the above reduces that freedom to now just an arbitrary 1-cyclic shift.  Yet, ""for some intuitive reason"", the above interpolants ""seem just right"": they are simple, comparable to the functions they came from, and are suitably ""graceful"", while other interpolants are necessarily more ""wiggly"" owing to the 1-cyclic displacement just mentioned. But is there something rigorous that both singles them out over all others, and which allows us to do similar interpolations on a very general range of functions $f$ , with such interpolations considered their analogues? Or to make it simpler, what distinguishes $\frac{x(x+1)}{2}$ uniquely from, say, $\frac{x(x+1)}{2} + \sin(2\pi x) -  \pi^{\gamma \Gamma\left(\frac{5}{3}\right)} \cos(4\pi x)$ or something, as an interpolant of that particular sum and which it shares with $2^x - 1$ (up to translation) versus $2^x$ ?","['functional-equations', 'interpolation', 'special-functions', 'analysis', 'calculus']"
4480035,Automorphisms of elliptic curves over general rings,"Let $E$ be an elliptic curve over a field $k$ . In this case we perfectly know how to compute $\operatorname{Aut}(E)$ depending on the $j$ -invariant of $E$ . Namely, for a general elliptic curve it is $\mathbb{Z}/2$ but if the $j$ -invariant is either $0$ or $1728$ then it gets bigger.
My question is: let us consider an elliptic curve $E$ over a ring $\mathbb{Z}/p^2$ . What is $\operatorname{Aut}(E)$ depending on the $j$ -invariant? I suspect that if $p > 3$ it should be the same as for char. $0$ picture but what happens for $p=2, 3?$","['algebraic-geometry', 'elliptic-curves']"
4480052,Solve the ordinary differential equation $\frac{d^2}{dx^2}F(x)=\frac{1}{F(x)^2}$,I've been trying to solve the ordinary differential equation $$\frac{d^2}{dx^2}F(x)=\frac{1}{F(x)^2}$$ I tried simplifying and then simplifying even further and found that this function has to be in the form $e^{-cx}$ where $c$ is expressed in terms of the function itself therefore it means its a recurring function...So I need to know a function(that is not recurring) that is inversley proportional to its second derivative.I've  been trying to solve this for a long time but couldn't find anything online that was much help either.Help would be appreciated. Thanx in advance,"['calculus', 'ordinary-differential-equations']"
4480143,Proving that the origin is unstable in a dynamical system,"Prove that the origin is an unstable equilibrium point for the system $$\begin{align*}\dot{x}&=x^3+yx^2\\\dot{y}&=-y+x^2\end{align*}$$ I've already tried to linearize the system in order to use the Hartman–Grobman theorem, but the eigenvalues of the respective Jacobian matrix are $0$ and $-1$ ; non of them are positive, then Lyapunov's indirect method doesn't work either. Do you have any idea of another theorem that allow us to conclude this?","['ordinary-differential-equations', 'lyapunov-functions', 'stability-in-odes', 'stability-theory', 'dynamical-systems']"
4480183,"$f(x)>0, f''(x)>0$. Prove: $\int_a^b f(x) dx > (b-a) f(\frac{a+b}{2})$","On $[a,b]$ function $f$ is differentiable for arbitrary order, and $f(x)>0, f''(x)>0$ . Prove: $\int_a^b f(x) dx > (b-a) f(\frac{a+b}{2})$ . I first try Taylor expansion at $x_0=\frac{a+b}{2}$ , and drop higher order term ( $(x-x_0)^3$ terms). But this works only locally at the neighborhood of $x_0$ . How to prove this inequality on a finite interval? Thank you.",['analysis']
4480188,Convergence in law of maximum likelihood estimator and the method of moments estimator of the uniform distribution,"So I have these random variables $X_1,\ldots,X_n $ iid. and a uniform distribution: $f_X(x)=1/\theta*\textbf{1}_{0\leq x\leq\theta} $ . The maximum likelihood estimator is $\tilde{\theta}_n=max\{ X_1,\ldots,X_n\}$ and the method of moments estimator $\hat{\theta}_n=2/n\sum_{i=1}^nX_i$ . These are estimators for $\theta$ . Now I need to find the asymptomatic laws of these two: $$ \sqrt n (\tilde{\theta}_n-\theta)$$ and $$n(\hat{\theta}_n-\theta)$$ For the first one I want to use the central limit theorem somehow but the estimator is not a sum and I'm a bit lost. I also cannot see how I can use Slutsky, the delta method or the continuous application theorems. For the second I have: $$n(\hat{\theta}_n-\theta)=2(\sum X_i-\theta n/2)=2\sum (X_i-\theta/2)$$ and $(X_i-\theta/2)$ is a $Unif[-\theta/2,\theta/2]$ but the dostribution associated to this sum (that I find online) is a distribution that we haven't seen in class so I am not so sure. Any help is appreciated! Thanks Edit: additional question. Also how can I show that the second one ( $\hat{\theta}_n$ ) is not a sufficient statistic? It is quite obvious how to show that the first one IS.","['statistics', 'probability', 'random-variables']"
4480219,Can 1 matrix represent 2 different linear maps?,"I understand that matrices represent linear maps and all of the exercises I have done have been representing a specific example linear map with a matrix. It got me wondering: can a unique matrix represent 2 different linear maps (i.e., can 2 different linear maps be represented by the same matrix)? If it is possible, could someone give me an example? Thank you!","['soft-question', 'linear-algebra']"
4480256,"Convergence of $\sum \frac{b_n}{n}$ Where $b_n = 1, -1, -1, 1, 1, 1, -1, -1,-1,-1,1,1,1,1,1,....$","How can I show convergence or divergence of the following sequence? $$\sum \frac{b_n}{n}$$ Where $b_n = 1, -1, -1, 1, 1, 1, -1, -1,-1,-1,1,1,1,1,1,....$ Im not sure how to use any of the standart theorems to show convergence or divergence. Any attempts to use comparison test, or finding an upper bound failed. Any ideas?","['calculus', 'sequences-and-series', 'real-analysis']"
4480269,"Ratio of heights of a sphere,over and under water [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question When a sphere is floating in the water, only 10 % of the volume is above the surface, while the rest is below. I need to calculate the relationship between height for the part above, and below the water.","['integration', 'calculus', 'geometry']"
4480340,Define the domain in which $f(z)=z\cdot \text{Im} (z)$ is differentiable and calculate its derivative.,"Define the domain in which the below function is differentiable and calculate its derivative: $$f(z)=z\cdot \text{Im} (z)$$ I tried checking the analyticity of the function by definining $z=x+iy$ , I get $$f(z)=(x+iy)y$$ but by the Cauchy-Riemann equations, it is not an analytic function: $$ \dfrac{\partial u(x,y)}{\partial x} = \dfrac{\partial(xy)}{\partial x} = y \neq \dfrac{\partial v(x,y)}{\partial y} = \dfrac{\partial (y^{2})}{y} = 2y$$ In which $$Re(f(z)) = u(x,y)$$ and $$Im(f(z)) = v(x,y)$$","['complex-analysis', 'derivatives', 'complex-numbers']"
4480341,Expected value of $100$ briefcases each with $1$ dollar except for one briefcase that resets your accumulated amount to zero?,"In this game there are $100$ suitcases and each contains the same dollar amount (let's say $1$ dollar) except for one suitcase that contains a bomb that resets the total value you accumulated so far to $0$ .  Naturally, you don't know which suitcase contains the bomb and you need to open every suitcase, what is the expected value of the amount you end up with? This is not a homework question, just a question I wondered about after seeing it in a gameshow. Is there a way to solve this analytically? And if so, how would you do that?","['expected-value', 'combinatorics', 'probability']"
4480352,Why is the number of ways to open up a cube $4! \times 2^4$?,A cube has $12$ edges. Cut seven of them and lay out the remaining ones on a table. It's known that the number of distinct connected meshes with non-overlapping faces is $11$ ( How many distinct ways to flatten a cube? ). The number of ways you can choose seven of the edges to cut is ${12 \choose 7}=792$ . But a lot of them aren't valid spanning trees since some faces of the cube will be isolated from the rest. I wrote some code to loop through the $792$ combinations and count the instances where the spanning tree property is preserved (and hence a valid connected mesh is possible). This turned out to be $384$ . This number is highly composite. It turns out that $384=4! \times 2^4$ . This can't be just a coincidence. There must be a reason why it turns out to be such a nice number. I'm looking for the connection I can't see. Perhaps something to do with the cube having $4$ main diagonals?,"['solid-geometry', 'combinatorics', 'geometry']"
4480362,"set theory: $A = \{3r+5s+8t \mid r,s,t \in\mathbb N, r = s + t\}$ and $C = \{n \in\mathbb N\mid 0 \le 𝑛𝑛 \le 12\}$. Find $A\cap C$","Can someone kindly explain this question to me, I am not sure how to do this. My approach: Since for Set $A = (r = S + T)$ $A = \{ 8s + 11t\mid s,t \in\mathbb N\}$ $C = \{0,1,2,3,4,5,6,7,8,9,10,11,12\}$ based on the interception of these two sets, $A$ must be $0 \le 8s + 11t \le 12$ Is my approach correct, from here I calculate set $A$ elements to be $\{0,8,11\}$",['discrete-mathematics']
4480434,Equations for Polygons in the Plane,"I've notice that one can describe a subset of polygons in the plane with equations of the form $$\sum_{i}|x-x_i|+|y-y_i| = c>0.$$ Consider for example this highly-symmetrical $16$ -sided polygon $$|x|+|y|+|x-1|+|y-1|+|x-2|+|y-2|+|x-3|+|y-3| = 15.$$ One interesting aspect is that the number of sides changes with $c$ .  Correct me if I'm wrong, but it seems that $16$ sides is the maximum we can achieve with $i=\{0,1,2,3\}$ . My questions are firstly, if there a name for this type of equation (perhaps a generalization of an ellipse in the taxi-cab geometry?).  Secondly, is there a way to calculate, given an equation of the form above, the number of sides of the polygon?","['euclidean-geometry', 'geometry']"
4480478,Intuition behind discrete valuation rings,"I'm trying to understand what DVRs are. I have seen two formulations, one in terms group homomorphisms and a discrete valuation function satisfying an axiom of the sort: $$ v(x+y) \geq \text{min} \left( v(x) , v(y) \right)$$ and another one saying it is a ring generated by one element and is a maximal ideal. Neither of these definition help me understand what sort of thing the DVRs are. Could someone please explain through a concrete intuitive example what DVRs are, why they are important and how these two different types of definitions fit together?","['ring-theory', 'group-theory', 'abstract-algebra']"
4480505,How to remove a removable singularity,"I learned about Riemann’s theorem on removable singularities, which states: Let $D \subset \mathbb{C}$ be a open subset, $a \in D$ and let $f$ be a holomorphic function defined on $D\setminus\{a\}$ . The following are equivalent: $f$ is holomorphically extendable over $a$ $f$ is continuously extendable over $a$ There exists a neighborhood of $a$ such that $f$ is bounded $\displaystyle \lim_{z \to a} (z-a) f(z) = 0$ Because of this theorem I know that a function can be extendable over a, but how does the extending work in practice? I tried on this example. Let $$
f(z) := \frac{\sin(z)-z}{z^2}.
$$ First I tried 4. to see if it is extendable. $$
\lim_{z \to 0} z \frac{\sin(z)-z}{z^2} 
= \lim_{z \to 0} \frac{\sin(z)}{z} - 1 
= \lim_{z \to 0} \frac{\cos(z)}{1} - 1 
= 0
$$ Thus $f$ is holomorphically extendable over $0$ . Now I want to find the value $f(0)$ My idea would be to use a taylor series.
I can't do the Taylor series of $f$ in $0$ , but I can do the Taylor series of $\sin(z)-z$ in $0$ : $$
\sin(z)-z 
= -z + \sum_{k=0}^{\infty} (-1)^k \frac{z^{2k+1}}{(2k+1)!} 
= \sum_{k=1}^{\infty} (-1)^k \frac{z^{2k+1}}{(2k+1)!}
$$ Now I can divide the taylor series of $\sin(z)-z$ by $z^2$ : $$
\frac{\sin(z)-z}{z^2} 
= \sum_{k=1}^{\infty} (-1)^k \frac{z^{2k-1}}{(2k+1)!}
$$ Now if I take this Taylor series and let $z \to 0$ , then I get $$
\lim_{z \to 0} \sum_{k=1}^{\infty} (-1)^k \frac{z^{2k+1}}{(2k+1)!} 
= 0
$$ This would mean, if I define $f(0):=0$ , then $f$ is continuous and thus by 2. also holomorphic. I have 3 Questions: (i)
First, is the way I solved this correct? (ii)
Is there another method to do this? (iii)
In this case the denominator was a (Taylor) polynomial which was the reason why I was able to find the Taylor series of $f$ . How should one approach the problem if the denominator is not a polynomial? Is there a more general way?","['complex-analysis', 'taylor-expansion', 'analysis']"
4480520,Evaluate triple integrals $\int_{0}^{1}\int_{0}^{1}\int_{0}^{1}\sqrt[3]{\log{(xyz)}}dxdydz$,"I am trying to evaluate this integral: $$\int_{0}^{1}\int_{0}^{1}\int_{0}^{1}\sqrt[3]{\log{(xyz)}}dxdydz$$ Honestly, I have no ideas to deal with this.
I hope I can be helped by everyone.
I just need a hint to process; thank you.",['multivariable-calculus']
4480528,Use of limits of sequences for the non-existence,"Let $f(x) = x +1$ for $x \geq 0$ and $f(x) = x- 1$ for $x < 0$ . I want to use the limits to show that $\lim_{x \rightarrow 0} f(x)$ does not exist. I tried with using $x_n = \frac{1}{n} \rightarrow 0$ (as $n \rightarrow \infty$ ) which leads to $f(x) \rightarrow \pm 1$ (as $x \rightarrow 0$ ). Is this enough to show the desired conclusion ? Edit : The following excerpt is from Wade's Analysis book. I wonder how the expression form for $f(x_n)$ was followed, whether it's a typo or not.","['sequence-of-function', 'limits', 'real-analysis']"
4480558,$365(x)^{364}-365(x)^{365}+x^{365}=0.9$,"I can't seem to solve this, I tried using multiple software but it says it doesn't support this kind of equation: $$365(x)^{364}-365(x)^{365}+x^{365}=0.9$$ Context:
Hey! I was having fun with tricked coin flip probability and I came up with that equation at a certain point. If you replace $365$ for $n$ (the exponent $364$ would be $n-1$ ), and consider it as the number of coin flips, and $p$ as the probability that the favored side of the coin comes up, then $0.9$ (or whatever number, say $y$ ) is the probability that $A$ happens strictly more than $B$ , that the favored side happens strictly more than the unfavored. So: Given that the favored side is greater than the unfavored $90\%$ of the time after $365$ throws, what is the probability of the favored side happening every throw? The equation I came up with would be formally written: $np^{n-1}-np^{n}+p^{n}=y$","['exponentiation', 'algebra-precalculus']"
4480613,What is the distribution of the number of boys standing between the leftmost girl and the rightmost girl?,"$10$ Boys and $10$ Girls get ordered in a line.  How is $X$ , the number of boys standing between the leftmost girl and the rightmost girl, distributed? I tried thinking of selecting one place from the $20$ for the leftmost girl, and then selecting k places from the $19$ left for the boys. Or selecting one place from the $19$ left for the rightmost girl. I can't figure how to solve this. Any help is appreciated.","['statistics', 'probability-distributions', 'combinatorics', 'probability']"
4480672,$\sin(25°)+\cos(115°)$?,"What is the value of $\sin(25°)+\cos(115°)$ ? Using $\cos(90°+\theta)=-\sin(\theta)$ , we get, $$\sin(25°)+\cos(115°)=\sin(25°)-\sin(25°)=0$$ But when I searched the same on Google, it showed $-0.45816155531$ as result on their calculator. Which result is correct? Also, why the other one is incorrect?",['trigonometry']
4480685,"If vectors are maxima of $\frac{a^T a}{a^T \Sigma^{-1} a}$, then they are eigenvectors of $\Sigma$.","Let $\Sigma \in M_q$ be symmetric and positive definite matrix. If vectors $a_1,\dots, a_q \in \mathbb R^q$ satisfy: $(i)$ $$i \not = j \implies a_i^T a_j = 0$$ $(ii)$ $$\frac{a_1^T a_1}{a_1^T \Sigma^{-1} a_1} =\sup_{a \not = 0} \frac{a^T a}{a^T \Sigma^{-1} a} $$ $(iii)$ $$i \gt 1 \implies \frac{a_i^T a_i}{a_i^T \Sigma^{-1} a_i} = \sup\left\{ \frac{a^T a}{a^T \Sigma^{-1} a} : a \not = 0, a^T a_j = 0, j=1,\dots, i-1 \right\}$$ then $a_1,\dots, a_q$ are eigenvectors of $\Sigma$ . I am stuck trying to prove this claim. The text gives the hint to use induction, but I am not sure how to proceed. The claim is part of the proof that $a_1^T Y, \dots, a_q^T Y$ are main components of $Y$ . Given this result, rest of the proof is simple. Edit Attempt :
Let $A, \Lambda$ be the square matrices such that $$\Sigma A = A\Lambda, \quad A^T \Sigma^{-1} A = I$$ where $\Lambda = \text{diag}(\phi_1,\dots, \phi_q)$ has eigenvalues of $\Sigma$ in descending order. Let: $$F(a) = \frac{a^T a}{a^T \Sigma^{-1} a}.$$ Then $$R(c) := F(Ac) = \frac{c^T A^T A c}{c^T A^T \Sigma^{-1} A c} = \frac{c^T A^T\Sigma^{-1} \Sigma A c}{c^T c} = \frac{c^T A^T\Sigma^{-1} A \Lambda c}{c^T c} = \frac{c^T \Lambda c}{c^T c}.$$ It is clear that $a$ is supremum of $F$ is and only if $A^{-1}a$ is supremum of $R$ . Let $c_i = A^{-1} a_i, i = 1,\dots q$ . Then: $$a_i^T a_j = c_i^T A^T A c_j = c_i^T A^T \Sigma^{-1}\Sigma A c_j =c_i^T A^T \Sigma^{-1} A  \Lambda c_j = c_i^T \Lambda c_j.$$ Therefore the $c_1,\dots, c_q$ satisfy: $(i')$ $$i \not = j \implies c_i^T \Lambda c_j = 0$$ $(ii')$ $$\frac{c_1^T \Lambda c_1}{c_1^T  c_1} =\sup_{c \not = 0} \frac{c^T \Lambda c}{c^T  c} $$ $(iii')$ $$i \gt 1 \implies \frac{c_i^T \Lambda c_i}{c_i^T  c_i} = \sup\left\{ \frac{c_i^T \Lambda c_i}{c_i^T  c_i} : c \not = 0, c^T \Lambda c_j = 0, j=1,\dots, i-1 \right\}$$ which seem somewhat simpler since $\Lambda$ is diagonal, but i am still unsure how to proceed. Attempt 2. Thanks to Mason's answer i feel i am closer to the solution: $$\frac{\partial F}{\partial a}(a) = \frac{2a \cdot (a^T \Sigma^{-1} a)-2\Sigma^{-1} a (a^T a)}{(a^T \Sigma^{-1}a)^2} = 0 \implies \Sigma^{-1}a = \frac{a^T\Sigma^{-1}a}{a^Ta} a$$ which implies every critial value of $F$ must be eigenvalue, and that $a_1$ is the eigenvector. Now that this is done, my idea is to look at the function: $$F:\{a_1\}^\perp \to \{a_1\}^{\perp}$$ and doing the same procedure shows that critical values of $F$ must be eigenvectors of $\Sigma^{-1}$ . However I am not sure i can conclude that the minimum and maximum of $F$ on the set $\{a_1\}^\perp$ are critical points of $F$ since $\{a_1\}^\perp$ is not open.","['statistics', 'matrix-calculus', 'linear-algebra']"
4480717,Radically different answers for $\frac{\mathrm d}{\mathrm dx}\left(\arccos\frac{\sqrt{1 - x^3} - \sqrt{1 + x^3}}{2}\right)$,"Find the derivative with respect to $x$ of $$\cos^{-1}\left(\frac{\sqrt{1 - x^3}  - \sqrt{1 + x^3}}{2}\right).$$ Here's my work: Substituting $x^3 = \cos(2\theta):$ $$\begin{aligned}\cos^{-1}\left(\frac{\sqrt{1 - x^3}  - \sqrt{1 + x^3}}{2}\right) &= \cos^{-1}\left(\frac{\sqrt{1 - \cos(2\theta)}  - \sqrt{1 + \cos(2\theta)}}{2}\right)\\& = \cos^{-1}\left(\frac{\sqrt{2\sin^2\theta}  - \sqrt{2\cos^2\theta}}{2}\right)\\& = \cos^{-1}\left(\frac{1}{\sqrt{2}}\sin\theta - \frac{1}{\sqrt2}\cos\theta\right) \\& = \cos^{-1}\left(\sin\theta\cos(\pi/4) - \sin(\pi/4)\cos\theta\right) \\& = \cos^{-1}\sin\left(\theta - \frac{\pi}{4} \right)\\& = \cos^{-1}\cos\left(\frac{\pi}2 - \theta + \frac{\pi}{4} \right)\\& = \frac{3\pi}4 - \theta \\& = \frac{3\pi}4 - \frac12\cos^{-1}\left(x^3\right).\end{aligned}$$ Differentiating this gives $$\boxed{\frac{3x^2}{2\sqrt{1 - x^6}}}.$$ So far so good... My textbook also shows this answer. But the problem is that this , this , this , and the symPy package of Python all give this clunky result $$\boxed{-\frac{-\frac{3x^{2}}{2\sqrt{1-x^{3}}}-\frac{3x^{2}}{2\sqrt{1+x^{3}}}}{2\sqrt{1-\frac{1}{4}\left(\sqrt{1-x^{3}}-\sqrt{1+x^{3}}\right)^{2}}}}$$ (not even neutralising the three negative signs) instead of the above simpler one. WolframAlpha and Desmos shows that these two expressions have the same domain $(-1,1)$ and are identically equal. Why do these software not give the answer in simplified form?","['calculus', 'solution-verification', 'math-software', 'trigonometry', 'derivatives']"
4480736,Computing the log of a sum of exponentials,"in a Coursera course by UW I've come across this piece of code computing the log of a sum of exponentials. def log_sum_exp(Z):
    """""" Compute log(\sum_i exp(Z_i)) for some array Z.""""""
    return np.max(Z) + np.log(np.sum(np.exp(Z - np.max(Z)))) I've been trying to figure out how this computes: $$log (\Sigma_{i}^n e^{Z_i})$$ I tried factoring it for some time now but at least for today, I'm at my wit's end.
Could someone explain, please? Thank you very much!","['algebra-precalculus', 'exponential-function', 'logarithms']"
4480745,A question on conditional probability in sde,"Let $s \in[a, b]$ and $x \in \mathbb{R}$ be fixed and consider the following SIE: $$
X_{t}=x+\int_{s}^{t} \sigma\left(u, X_{u}\right) d B(u)+\int_{s}^{t} f\left(u, X_{u}\right) d u, \quad s \leq t \leq b
$$ Use $X_{t}^{s, x}$ to denote the solution of the SIE in above equation. Let the initial condition $x$ of equation be a constant. So the solution $X_{t}^{s, x}$ is independent of the $\sigma$ -field $\mathcal{F}_{s}$ for each $t \in[s, b]$ . If follows that for any $\mathcal{F}_{s}$ -measurable random variable $Z$ , we have the equality $$
P\left(X_{t}^{s, Z} \leq y \mid \mathcal{F}_{s}\right)=\left.P\left(X_{t}^{s, x} \leq y\right)\right|_{x=Z} \quad \forall y \in \mathbb{R}
$$ So, why can I have the last equality? I am confused how can I substitue an $\mathcal{F}_{s}$ -measurable random variable $Z$ by a constant $x$ then put $|_{x=Z}$ outside of the conditional probability? And what is the difference between $\left.P\left(X_{t}^{s, x} \leq y\right)\right|_{x=Z}$ and $P\left(X_{t}^{s, Z} \leq y\right)$ ?","['measure-theory', 'conditional-probability', 'conditional-expectation', 'probability-theory', 'stochastic-calculus']"
4480777,Why does $\int_{-x}^x f^\alpha\leq f(-x)+f(x)$ imply $f=0$?,"Let $0\leq f\in C(\mathbb{R})$ , and for some $\alpha>1$ , we have \begin{equation*}\begin{aligned}
\int_{-x}^x f^\alpha\leq f(-x)+f(x), \forall\ x\in [0,+\infty).
\end{aligned}\end{equation*} Prove that $f\equiv 0$ . Let $F(x)=\int_{-x}^x f^\alpha$ , then $F'(x)=f^\alpha(x)+f^\alpha(-x)
\leq [f(x)+f(-x)]^\alpha$ . How to use the assumption? What to do next? Any ideas?","['integration', 'calculus', 'real-analysis']"
4480779,Is the cosine angle between two R.V. an (approximation) not equality to the correlation coefficient?,"I have seen in websites that given two R.V. $X,Y$ , if $$
\cos(\theta)=\frac{X\cdot Y}{\|X\|_2\|Y\|_2}
$$ and $$
\rho=\frac{\text{Cov}(X,Y)}{\sqrt{\text{Var}(X)\text{Var}(Y)}}
$$ then $$
\cos(\theta)=\rho
$$ This identity implies $\text{Cov}(X,Y)=X\cdot Y$ . Isn't $X\cdot Y$ the Maximum Likelihood Estimate for the covariance missing some factors? If true then the equation above is not equality but rather $≈$ as the samples become bigger. Next is the denominator which implies  that $$ \text{Var}(X)= \| X\|_2 ^2$$ .
Again, isn't the right side not an identity but rather an estimator (MLE) to the variance of $X$ ?
Isn't $$ \rho ≈ \cos(\theta)$$ I have also seen the dot product (without the denominator in the first equation I've given but being more general using inner products) being used to measure correlation in some papers like Least Angle Regression. I am confused about the relationship between dot products and correlation. This leads me to a general question: Is $$
\langle X,X\rangle = \text{Var}(X)
$$ in Euclidean space.","['statistics', 'covariance', 'variance', 'inner-products', 'correlation']"
4480819,Anyone know function that is like Sin or Cos but with pointy tip? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Basically like what the title saying, anyone know function that is like Sin or Cos but with pointy tip? image of function that I want to achieve Edit:
There seems to be misunderstanding, since I can't draw it well on the image. I don't want it to be striped, but I want them to be connected line like Sin and Cos the only difference is just that the tip is pointy. better image for the function","['functions', 'graphing-functions']"
4480833,Using complex number in integration,"I know, $$I_1 = \int \dfrac{dx}{\sqrt{x^2 - 1}} = \ln|x + \sqrt{x^2-1}| + c$$ But if I factor out $i$ from the denominator, I get: $$I_2 = -i \int \dfrac{dx}{\sqrt{1 - x^2}} = -i \sin^{-1}x + ic$$ Are these 2 expressions equivalent?",['integration']
4480874,Nonlinear differential equation $u''(r) = (r^2 -3) u(r)$,"I am trying to solve the following equation, this being a form of the Schrodinger equation for the harmonic oscillator, E=3 being the 1st energy level: $u''(r) = (r^2 - 3) u(r)$ I am going trough a book on numerical analysis and this is solved with the Numerov method, the exact solution is only cited: $A r e^{-r^2/2}$ How would I go about obtaining this solution? I tried Laplace transforms but this being a nonlinear equation, that method won't work, even Mathematica failed me Thanks in advance","['quantum-mechanics', 'ordinary-differential-equations']"
4480882,Does every sufficiently long string contain consecutive permutations of another string?,"Let $\mathcal{C}$ be a finite set, let $\mathcal{F}(\mathcal{C})$ be the free (non-abelian) monoid over $\mathcal{C}$ , and let $n\in\mathbb{N}$ be an integer. For every $k\in \mathbb{N}$ , write $S_k$ for the full symmetric group of degree $k$ . Formally my question is whether the following statement holds: There exists an integer $N\in \mathbb{N}$ such that for every word $\alpha\in \mathcal{F}(\mathcal{C})$ of length at least $N$ . Then
there exists a word $\gamma\in \mathcal{F}(\mathcal{C})$ of length $k$ , together with permutations $\sigma_1,\dots, \sigma_n\in S_k$ such
that $$\alpha =\beta\cdot \sigma_1(\gamma)\sigma_2(\gamma)\cdots \sigma_n(\gamma)\cdot \delta,$$ for certain $\beta,\delta\in\mathcal{F}(\mathcal{C})$ . What I mean to ask is: If a string is sufficiently long, will it contain a repetition of a number of symbols, placed in any order.
For example, the string $$ babcbaca,$$ can be split as $$b|abc|bac|a,$$ where $abc$ and $bac$ are permutations of one another. If $\mathcal{C}=\{a,b,c\}$ one can show that any string of length 10 or more, will contain at least 2 consecutive permutations of the same string, but this becomes exponentially more tedious as the order of the set and $n$ increase. I was wondering if anyone knows a proof (or counter-example) for this, or has any suggestions on where to search?","['additive-combinatorics', 'combinatorics', 'necklace-and-bracelets']"
4480888,"Feynman´s trick to solve $\int_0^\infty \frac{\arctan(x)}{\sqrt{x}(1+x^2)}\,dx$","I wanted to evaluate the integral \begin{align*}
\int_0^\infty \frac{\arctan(x)}{\sqrt{x}(1+x^2)}\,dx=\frac{\pi^2}{4\sqrt{2}}-\frac{\pi \ln(2)}{2\sqrt{2}} \tag{1}
 \end{align*} I thought of using Feynman´s trick by considering the integral $$
\begin{align*}
I(a)&=\int_0^\infty \frac{\arctan(ax)}{\sqrt{x}(1+x^2)}\,dx \tag{2}
 \end{align*}
$$ Differentiating $(2)$ w.r. to $a$ we obtain: \begin{align*}
I^\prime(a)&=\int_0^\infty \frac{x}{\sqrt{x}(1+x^2)(1+a^2x^2)}\,dx\\
&=\int_0^\infty \frac{x^{1/2}}{(1+x^2)(1+a^2x^2)}\,dx\\
&=\frac{1}{a^2-1}\left(a^2 \int_0^\infty \frac{x^{1/2}}{1+a^2x^2}\,dx-\int_0^\infty \frac{x^{1/2}}{1+x^2}\,dx\right)\\
&=\frac{1}{a^2-1}\left(\sqrt{a} \int_0^\infty \frac{x^{1/2}}{1+x^2}\,dx-\int_0^\infty \frac{x^{1/2}}{1+x^2}\,dx\right)\\
&=\frac{1}{a^2-1}\left(\frac{\sqrt{a}}{2} \int_0^\infty \frac{x^{-1/4}}{1+x}\,dx-\frac12\int_0^\infty \frac{x^{-1/4}}{1+x}\,dx\right)\\
&=\frac{\pi}{\sqrt{2}}\left( \frac{\sqrt{a}}{a^2-1}-\frac{1}{a^2-1}\right)\\
\end{align*} Integrating back \begin{align*}
I(a)&=\frac{\pi}{\sqrt{2}}\left( \int\frac{\sqrt{a}}{a^2-1}\,da-\int\frac{1}{a^2-1}\,da\right)\\
&=\frac{\pi}{\sqrt{2}}\left( \int\frac{1}{(1-a)(1+a)}\,da-\int\frac{\sqrt{a}}{(1-a)(1+a)}\,da\right)\\
&=\frac{\pi}{\sqrt{2}}\left(\frac12 \int\frac{da}{1-a}+\frac12 \int\frac{da}{1+a}-\frac12 \int\frac{\sqrt{a}}{1-a}\,da-\frac12 \int\frac{\sqrt{a}}{1+a}\,da\right)\\
&=\frac{\pi}{\sqrt{2}}\left(\frac12\ln\left(\frac{1+a}{1-a} \right)- \int\frac{u^2}{1-u^2}\,du- \int\frac{u^2}{1+u^2}\,du\right)\\
&=\frac{\pi}{\sqrt{2}}\left(\frac12\ln\left(\frac{1+a}{1-a} \right)+ \int\frac{u^2-1+1}{u^2-1}\,du- \int\frac{u^2+1-1}{u^2+1}\,du\right)\\
&=\frac{\pi}{\sqrt{2}}\left(\frac12\ln\left(\frac{1+a}{1-a} \right)- \int\frac{du}{1-u^2}+ \int\frac{du}{u^2+1}\right)\\
&=\frac{\pi}{\sqrt{2}}\left(\frac12\ln\left(\frac{1+a}{1-a} \right)- \frac12\ln\left(\frac{1+u}{1-u} \right)+ \arctan(u)+C\right)\\
&=\frac{\pi}{\sqrt{2}}\left(\frac12\ln\left(\frac{1+a}{1-a} \right)- \frac12\ln\left(\frac{1+\sqrt{a}}{1-\sqrt{a}} \right)+ \arctan\left(\sqrt{a}\right)+C\right)\\
&=\frac{\pi}{\sqrt{2}}\left(\operatorname{arctanh}(a)- \operatorname{arctanh}\left(\sqrt{a} \right)+ \arctan\left(\sqrt{a}\right)+C\right)\\
\end{align*} Now, if we set $a=0$ in $(2)$ we find that $C=0$ . Therefore \begin{align*}
\int_0^\infty \frac{\arctan(ax)}{\sqrt{x}(1+x^2)}\,dx=\frac{\pi}{\sqrt{2}}\left(\operatorname{arctanh}(a)- \operatorname{arctanh}\left(\sqrt{a} \right)+ \arctan\left(\sqrt{a}\right)\right) \tag{3}
 \end{align*} Supposedly, we should now let $a \to 1$ in $(3)$ to find $(1)$ , but for $\lim_{a \to 1} \operatorname{arctanh}(a) \to \infty$ . Is there a way to fix this problem so Feynman´s trick is still applicable? To add a backgroud to the question. I was originally trying to solve the integral $\int_0^{\pi/2}\frac{x}{\sqrt{\tan(x)}}\,dx$ . By a obvious change of variable we find that $\int_0^{\pi/2}\frac{x}{\sqrt{\tan(x)}}\,dx=\int_0^\infty \frac{\arctan(x)}{\sqrt{x}(1+x^2)}\,dx$ .
Now observe that \begin{align*}
\int_0^{\pi/2}\frac{x}{\sqrt{\tan(x)}}\,dx&=\frac{\pi}{2}\int_0^{\pi/2}\sqrt{\tan(x)}\,dx-\int_0^{\pi/2}x\sqrt{\tan(x)}\,dx & \left(x \to \frac{\pi}{2}-x \right)\\
&=\frac{\pi}{2}\frac{\pi}{\sqrt{2}}-\int_0^{\pi/2}x\sqrt{\tan(x)}\,dx & ( \text{by beta function})\\
&=\frac{\pi^2}{2\sqrt{2}}-\int_0^\infty \frac{\sqrt{x}\arctan(x)}{1+x^2}\,dx\\
&=\frac{\pi^2}{2\sqrt{2}}-\left(\frac{\pi}{2\sqrt{2}}\ln(2)+\frac{{\pi}^2}{4\sqrt{2}}\right)
 \end{align*} last line result follows from this post",['integration']
4480897,Locally isomorphic singularities = Locally isomorphic minimal blow-ups?,"Let's say that one has two varieties $V_1, V_2$ , with singularities at points $x_1$ and $x_2$ respectively, and that there exist open neighbourhoods $U_1$ and $U_2$ of these singularities such that $U_1$ and $U_2$ are isomorphic, i.e. such that $V_1$ and $V_2$ are locally isomorphic around their singularities. Is it then always a given that if we have minimal resolutions $W_1$ and $W_2$ of $V_1$ and $V_2$ at these singularities, then around the exceptional divisors $D_1$ and $D_2$ , there exists open neighbourhoods $U'_1$ and $U'_2$ such that $U'_1$ and $U'_2$ are isomorphic? Further, if this is the case, is it then a given that $\mathcal{O}_{W_1} (U'_1) \cong \mathcal{O}_{W_2} (U'_2)$ ?","['affine-varieties', 'algebraic-geometry', 'singularity-theory']"
4480947,Constructing a contact form for which a given contact vector field is Reeb,"Given a contact manifold $(M, H)$ and a smooth vector field $X$ on $M$ , I'm trying to show that $X$ is the Reeb field of some contact form for $H$ if and only if it's a contact vector field that's nowhere tangent to $H$ . First, the definitions as I understand them: A Reeb field for some contact form $\theta$ is a smooth vector field $T$ such that $\theta(T) = 1$ and $i_T d\theta = 0$ A contact vector field is a smooth vector field $X$ such that its flow preserves the contact structure $H$ . Explicitly, if $\psi$ is the flow of $X$ , then this means $\psi_{t*}Y \in \Gamma(H)$ for any $Y \in \Gamma(H)$ and any $t \in \mathbb{R}$ . Proving that Reeb $\Rightarrow$ contact is a straightforward application of the definitions, but proving contact $\Rightarrow$ Reeb has been more difficult. My attempt: If $X$ is nowhere tangent to $H$ , then for any contact form $\theta$ , $\theta(X)$ is nowhere zero. Thus we can define a contact form $\phi = \frac{1}{\theta(X)}\theta$ which clearly satisfies $\phi(X) = 1$ , and it remains only to show that $i_Xd\phi = 0$ . Applying Cartan's magic formula, we can write $$
\require{cancel} i_X d\phi = \mathcal{L}_X \phi - \cancelto{0}{d(\theta(X))} = \mathcal{L}_X \phi.
$$ This Lie derivative vanishes if and only if $\phi$ is invariant under the flow of $X$ . Because $X$ is contact, the pullback of any contact form by $\psi_t$ will still annihilate $H$ for any $t$ , and so $\psi_t^* \phi = f\phi$ for some $f \in C^\infty(M)$ . Thus, we will be done if we can show that $f = 1$ , which will in turn follow if we can show that $\psi_t^*\phi(X) = \phi(X) = 1$ . \begin{align}
(\psi_t^* \phi)(X) &= \left(\psi_t^* \left( \frac{1}{\theta(X)} \theta \right)\right)(X) \\\
&= \left( \frac{1}{\theta(X)} \circ \psi_t\right)(\psi_t^* \theta)(X) \\\
&= \frac{\theta(\psi_{t*} X)}{\theta(X) \circ \psi_t} \\\
&= \frac{\theta(X)}{\psi_t^* (\theta(X))}.
\end{align} (On the last line, we should be careful to note that the pullback $\psi_t^*$ is acting on the function $\theta(X)$ , not on $\theta$ itself.) Here is where I find myself in a corner. The result will follow if I can show that $\theta(X)$ is constant on every integral curve of $X$ , which seems like it ought to be true, but I don't know how to show it. Should this be obvious? Or should I perhaps modify my definition of $\phi$ ?","['contact-geometry', 'differential-geometry']"
4480979,Is the complement of a quadrant a manifold with corners?,"The definition of a manifold with corners is analogous to that of a smooth manifold, except that, instead of being  locally diffeomorphic to $\mathbb R^n$ , is locally diffeomorphic to $[0,+\infty)^k\times \mathbb R^{n-k}$ where $0\leq k\leq n$ . The classical example is the quadrant $Q=[0,+\infty)^2\subset \mathbb R^2$ , which has $(0,0)$ as corner point. Consider $X := \mathbb R^2 \setminus \mathrm{int}(Q).$ Is $X$ a manifold with corners? The only problematic point is the origin which clearly has to be a corner point. We would like to show that a neighbourhood of the origin in $X$ is diffeomorphic to $Q$ . The naive idea would be to consider a map $\varphi: X\to Q$ , that informally  closes $X$ like a book so that it becomes $Q$ .
This would give us an isotopy of $X$ over $Q$ , but it does not seem to be a smooth map. Indeed, this isotopy at a certain time would also show that $X$ (and hence $Q$ ) is diffeomorphic to $[0,+\infty)\times \mathbb R$ which is a manifold with boundary (no corners).","['differential-topology', 'manifolds-with-boundary', 'smooth-manifolds', 'differential-geometry']"
4480984,Prove that $\frac{a}{a^2+\lambda}+\frac{b}{b^2+\lambda}+\frac{c}{c^2+\lambda} \leq \frac{3}{\lambda +1}$,"If $a+b+c=3$ , $ a,b,c>0$ and $\lambda \geq 1$ ,
prove that : $$\frac{a}{a^2+\lambda}+\frac{b}{b^2+\lambda}+\frac{c}{c^2+\lambda} \leq \frac{3}{\lambda +1}.$$ my attempt: using CBC  twice and  AM_AG inequality $\frac{3}{3}\Big(\frac{a}{a^2+\lambda}+\frac{b}{b^2+\lambda}+\frac{c}{c^2+\lambda}\Big)^2\leq 3 \Big(\frac{a}{a^2+\lambda}\Big)^2+\Big(\frac{b}{b^2+\lambda}\Big)^2+\Big(\frac{c}{c^2+\lambda}\Big)^2\leq 3\Bigg( \bigg(\frac{a}{\frac{(a+\sqrt{\lambda})^2}{2}}\bigg)^2+ \bigg(\frac{b}{\frac{(b+\sqrt{\lambda})^2}{2}}\bigg)^2+\bigg(\frac{c}{\frac{(c+\sqrt{\lambda})^2}{2}}\bigg)^2\Bigg) =12\bigg(\frac{a^2}{(a+\sqrt{\lambda })^4}+\frac{b^2}{(b+\sqrt{\lambda})^4}+\frac{c^2}{(c+\sqrt{\lambda })^4}\bigg)\leq12 \bigg(\frac{a^2}{\sqrt{\lambda}^4}+\frac{b^2}{\sqrt{\lambda}^4}+\frac{c^2}{\sqrt{\lambda}^4}\bigg)=\frac{48(a^2+b^2+c^2)}{4\lambda ^2 }\leq \frac{48\cdot9}{4\lambda^2} \leq \frac{9}{(\lambda +1)^2}$ beacuse $f(\lambda )=\frac{9}{(\lambda+1)^2}-\frac{48\cdot9}{4\lambda^2} \geq 0 $ for $\lambda \geq 1$ so finally $\frac{3}{3}\Big(\frac{a}{a^2+\lambda}+\frac{b}{b^2+\lambda}+\frac{c}{c^2+\lambda}\Big)^2\leq\frac{9}{(\lambda +1)^2}$$\Leftrightarrow$$\Big(\frac{a}{a^2+\lambda}+\frac{b}{b^2+\lambda}+\frac{c}{c^2+\lambda}\Big)\leq\frac{3}{(\lambda +1)}$ I have just one question: -does my attempt is true?","['calculus', 'cauchy-schwarz-inequality', 'algebra-precalculus', 'inequality']"
4480998,trying to solve the nested sum $\sum_{n=1}^{\infty} \frac{1}{1^2+2^2+\dots+n^2} $,"This sum grabbed my curiousity $$
\sum_{n=1}^{\infty}  \frac{1}{1^2+2^2+\dots+n^2}
$$ I want to solve it with Calc II methods (that's the math I have). By getting a closed form for the expression in the denominator, it becomes this: $$
6 \sum_{n=1}^{\infty}  \frac{1}{n(n+1)(2n+1)} 
$$ First thing I did was partial fractions $$
6 \sum_{n=1}^{\infty} \left( \frac{1}{n}  + \frac{1}{n+1}-  \frac{4}{2n+1} \right)
$$ You can reindex the first two terms, but I couldn't reindex the $2n+1$ term to telescope with that combined term, my thought is it's not possible because it isn't in the form (n+integer). So I tried expressing it as an integral, viewing each term as multiplied by an $x$ evaluated between 0 and 1. Then deriving with respect to $x$ to produce an integral. $$
6 \sum_{n=1}^{\infty}   \int_0^1  (x^{n-1} +x^n  - 4x^{2n}) dx
$$ Switching the order of integration and summation, and doing some reindexing: $$
6 \int_0^1  \left(3+ 2\sum_{n=0}^\infty x^n - 4 \sum_{n=0}^\infty (x^2)^n\right)dx
$$ I've been doing decimal approximations the whole time, checking that my steps work. They do up to and including that point, the answer is about $1.3$ Next I used the geometric series expansion, and this is where the approximation goes wrong, so I believe this is a problematic step: $$
6 \int_0^1  \left(3+  \frac{2}{1-x} - \frac{4}{1-x^2} \right)dx
$$ This integral is easy to solve using partial fractions on the $x^2$ term. You get an answer of $6(3-2\ln2)$ , which is about $9.6$ The correct answer, based on some website that doesn't show steps, is $6(3-4\ln2)$ , which matches my approximation. I'm really stumped on what I'm doing wrong and how to get that correct answer. I've checked my algebra over and over and learned some latex to write this post. Any help is appreciated!","['calculus', 'polynomials', 'summation']"
4481000,Real part of $ \quad 1- \frac{2}{\pi} \arctan(r^{\rho}e^{i\rho\theta}).$,"To solve the Dirichlet problem  using mellin transform, i needed to find the real part of $ \quad 1- \displaystyle\frac{2}{\pi} \arctan(r^{\rho}e^{i\rho\theta}).$ I already know the result will be \begin{cases}
\quad 1- \displaystyle \frac{1}{\pi}\arctan(\frac{2r^{\rho}\cos(\rho \theta)}{1-r^{2\rho}}) & \text{ si } r\in [0, 1]\\ \quad \\
\quad  \displaystyle \frac{1}{\pi}\arctan(\frac{2r^{\rho}\cos(\rho \theta)}{r^{2\rho}-1}) & \text{ si } r\in ]1,+\infty[.\\
\end{cases} I find it in ""Dautray R., Lions J.L.,  Mathematical analysis and numerical calculation for science and technology."" I want to know how they found it ?","['complex-analysis', 'calculus', 'complex-numbers', 'real-analysis']"
4481008,Inverse of the transform in the Box-Muller transform,"I am following this writeup of the Box-Muller method but I am confused how they derived the inverse. In this method, they write Box-Muller Algorithm is a classic method to generate identical and independent standard normal random variables.
Box-Muller Algorithm Generate $U_{1} \sim$ uniform $(0,1)$ and $U_{2} \sim$ uniform $(0,1)$ where $U_{1} \perp U_{2}$ Set $R=\sqrt{-2 \log \left(U_{1}\right)}$ and $\theta=2 \pi U_{2}$ Set $X=R \cos (\theta)$ and $Y=R \sin (\theta)$ Overall,
$$
\begin{aligned}
&X=\sqrt{-2 \log U_{1}} \cos \left(2 \pi U_{2}\right) \
&Y=\sqrt{-2 \log U_{1}} \sin \left(2 \pi U_{2}\right)
\end{aligned} This can be verified by solving $U_{1}$ and $U_{2}$, \begin{aligned}
&U_{1}=e^{-\left(X^{2}+Y^{2}\right) / 2} \
&U_{2}=\frac{1}{2 \pi} \arctan \left(\frac{X}{Y}\right)
\end{aligned} I am trying to understand how exactly the inverse of $X=\sqrt{-2 \log U_{1}} \cos \left(2 \pi U_{2}\right)$ and $Y=\sqrt{-2 \log U_{1}} \sin \left(2 \pi U_{2}\right)$ was computed, i.e. $$
\begin{aligned}
&U_{1}=e^{-\left(X^{2}+Y^{2}\right) / 2} \\
&U_{2}=\frac{1}{2 \pi} \arctan \left(\frac{X}{Y}\right)
\end{aligned}
$$ Looking at these formulas, I can't directly compute it so I'm not sure how they came up with their result.","['statistical-inference', 'statistics', 'real-analysis', 'calculus', 'probability']"
4481076,Is there a relationship between the Ricci scalar and the determinant?,"On the one hand the determinant (technically the absolute value of the determinant) represents the ""volume distortion"" experienced by a region after being transformed. On the other hand the scalar curvature represents the amount by which the volume of a small geodesic ball in a Riemannian manifold deviates from that of the standard ball in Euclidean space. Is there a link between the two?  Can the determinant of a matrix be interpreted as a scalar curvature? If I makes a difference, I am mostly interested in the context of general relativity.","['matrices', 'curvature', 'geometry', 'riemannian-geometry']"
4481089,Does the functional equation $f(x)-f\left(x-\frac{x^2}3\right)=\frac{x}{3-x}-\log\left(\frac3{3-x}\right)$ define an analytic function?,"When trying to solve functional equation $$f(x)-f\left(x-\frac{x^2}3\right)=\frac{x}{3-x}-\log\left(\frac3{3-x}\right)\,,$$ by assuming that $\displaystyle f(x)=\sum_{k=1}^{\infty}a_k x^k$ , we could get equations: $$\sum_{s=1}^{\lfloor\frac k2\rfloor}{k-s \choose s}(-1)^{s-1}3^{k-s} a_{k-s}=\frac{k-1}{k}\,,$$ so that we could get $$\begin{matrix} f(t)&=\frac t6+\frac{t^2}{27}+\frac{13t^3}{972}+\frac{113t^4}{19440}+\frac{1187t^5}{437400}\\&+\frac{877t^6}{688905}+\frac{14569t^7}{25719120}+\frac{176017t^8}{793618560}+\frac{1745717t^9}{26784626400}\\&+\frac{88217t^{10}}{15345358875}-\frac{147635381t^{11}}{19445638766400}-\frac{3238110769t^{12}}{827323540243200}\\&+\frac{63045343657t^{13}}{37643221081065600}+\frac{24855467017t^{14}}{7125970336860375}+O(t^{15})\end{matrix}$$ I have thought that the coefficients of the expansion above is bounded so that $f(t)$ is an analytic function. But after computing more terms, we could find the absolute value of coefficients increase rapidly after around $50$ terms, such as term $51$ to $60$ is around: $$[78.513621003297250420074188707102682220, 202.58713829274578879553773336619242812, -449.99520262010819218204869756666128052, -1873.4863539770463771221218334858172231, 2366.9955239364074357260471634171465472, 17249.065560767631956266214434624779288, -8940.7592181048466843451670448875449069, -160960.78661872319570785726231645359908, -24829.450164495412949086768442609647625, 1533556.2377505922967945825482291368540]$$ So it seems the expansion above only provides an approximation of $f(x)$ like Stirling's approximation. So, my question is: Does the expansion above really defines an analytic function? Or does it really accurately defines a function? Let $v(x)=\frac{3x^2}{3x-1}, v_1(x)=v(x), v_{k+1}(x)=v(v_k(x))$ We could prove that $c(x)=\lim_{n\to\infty}\left(3v_n(x)-n-\log(n)\right)$ exists for all x large enough. It is easy to show that $c(v(x))=c(x)+1$ . So $c'(v(x))v'(x)=c'(x)$ and $c'(v_{n+1}(x))v'(v_n(x))=c'(v_n(x))$ so that $c'(x)=c'(v_{n+1}(x))\prod_{k=0}^{n}v'(v_k(x))$ Since $v_n(x)=\frac n3+\frac{\log(n)}3 +\frac{c(x)}3 +o(1)$ and $v'(x)=1+O(\frac1{x^2})$ , we could get $\lim_{n\to\infty}c'(v_{n+1}(x))=1$ and $c'(x)=\prod_{k=0}^{\infty}v'(v_k(x))$ uniformly converges and it likely converged to an analytic function. Finally we could prove that $c(x)=3x-\log(x)+f(\frac1x)$ so that f(x) should be analytic function too.","['functions', 'real-analysis']"
4481102,"Are there fractal ""manifolds""?","Differentiable manifolds are a generalization of the local geometry of Euclidean space. In fact, every differentiable manifold of dimension $m$ is locally diffeomorphic to the Euclidean space of the same dimension. On the other hand, in general, curves and surfaces (and other fractal objects of higher dimension) that can be embedded in a Euclidean space in general, do not admit a simple differentiable structure, so they cannot be treated as differentiable manifolds. I wonder if anyone has defined a kind of ""fractal variety/manifold"" that considers fractal geometries that are not explicitly constructions within a Euclidean space and in some sense can be analyzed intrinsically as are differentiable manifolds.","['analytic-geometry', 'fractal-analysis', 'differential-geometry']"
4481145,"If $g(y)=\max\limits_{x \in X} f(x,y)=f(h(y),y).$ Prove that $h(y)$ is continuous","Let $ X\subset\mathbb{R}^m$ and $Y\subset\mathbb{R}^n$ non empty, convex and compact sets, and let $f : X\times Y\rightarrow\mathbb{R}$ continuous funtion.
If $f(\cdot,y):Y\rightarrow\mathbb{R}$ convex for every $y\in Y$ $f(x,\cdot):X\rightarrow\mathbb{R}$ concave for every $x\in X$ If $g(y)=\max\limits_{x \in X} f(x,y)=f(h(y),y).$ Prove that $\cdot \quad g(y)$ is continuous My attempt. Proof: Suppose g(y) is not continuous. So there exist $ \epsilon>0$ and  a sequence $\{y_i\}\subseteq Y$ : $ y_i \rightarrow y_0 \in Y$ ,such that $\Vert g(y_i)-g(y_0)\Vert>\varepsilon$ for every $i$ .
and for the function $f(h(y).\cdot)$ : $|f(h(y_i),y_0)-f(h(y_0),y_0)|>\delta,\quad i=1,2,...$ we subtract and add $f(h(y_0),y_i)$ : $\delta < |f(h(y_i),y_0)-f(h(y_0),y_0)+f(h(y_0),y_i)-f(h(y_0),y_i)|$ $ = |f(h(y_i),y_0)-f(h(y_0),y_i)+f(h(y_0),y_i)-f(h(y_0),y_0)|$ $\leq |f(h(y_i),y_0)-f(h(y_i),y_0)|+ |f(h(y_0),y_i)-f(h(y_0),y_0)|$ I'm not sure if this the correct way of proving it. I suppose i could try applying that f is continuous on a compact set.","['multivariable-calculus', 'calculus', 'real-analysis']"
4481186,What is the role of genera in the development of complex-oriented cohomology?,"I am reading about complex-orientable cohomology theories for the first time, elliptic cohomology theories in particular. An inconsistency(?) in the literature is the explicit reference to genera . Some references define and use the term, see for instance: Graeme Segal's survey, ""Elliptic Cohomology."" Charles Rezk's lecture notes for elliptic cohomology. Novikov's paper introducing the ANSS apparently uses genera (although I have trouble reading this paper.) Yet several make no mention of it, for instance: Mike Hopkin's COCTALOS lecture notes. Haynes Miller's (Co)bordism lecture notes. Doug Ravenel's Complex cobordism and stable homotopy groups of spheres . My understanding of even the topics foundational to this subject area is sparse (working on it!), which is making it hard to see why we ought to pick out and give a special name to these homomorphisms. Which brings me to my question: What is the role of genera in the development of complex-oriented cohomology? And why the hot-cold nature of genera's presence in literature? My impression is that genera are somehow related to the geometric/physical story of elliptic cohomology, and that perhaps(?) homotopy theorists just don't need all that. But that's speculation. I'd especially appreciate an answer that provides historical context.","['algebraic-geometry', 'differential-topology', 'math-history', 'homology-cohomology', 'algebraic-topology']"
4481191,Extending a continuous function defined on a subset of $\mathbb{R}$,"Let $E$ be a subset of $\mathbb{R}$ and let $f$ be a continuous function defined on $E$ . Is it true that $f$ can always be extended to a function $\tilde{f}$ defined on $\mathbb{R}$ , which is still continuous on $E$ ? I know that
we cannot ask $\tilde{f}$ be to continuous on all $\mathbb{R}$ ,
which is shown by the following example: Does every continuous map from $\mathbb{Q}$ to $\mathbb{Q}$ extends continuously as a map from $\mathbb{R}$ to $\mathbb{R}$? Thank you in advance! Edit: I wanted ask if $\tilde{f}$ could be continuous at every point of $E$ . I hope it's clearer phrased this way! Edit2: From comments. For example, if $E=\{0\}$ , then $f$ with $f(0)=0$ is continuous. If we define $\tilde{f}(x)=1$ for $x\in\mathbb{R}\backslash\{0\}$ (and $\tilde{f}(x)=f(x)$ for $x\in E$ ), then the
restriction of $\tilde{f}$ to $E$ is continouous but $\tilde{f}$ is not continuous at $0$ .",['analysis']
4481230,Existence of continuous function $g$ with $\int_a^b f(x)g(x){\rm d}x\ge \frac{1}{2}\int_a^b |f(x)|{\rm d}x.$,"Assume $f(x)\in C[a,b]$ . Prove there exists $g(x)\in C[a,b]$ such that $\int_a^b f(x)g(x){\rm d}x\ge \frac{1}{2}\int_a^b |f(x)|{\rm d}x.$ Denote $J:=\int_a^b|f(x)|{\rm d}x$ and define $s(x):=\text{sgn}f(x),x\in[a,b]$ , then $\int_a^b f(x)s(x){\rm d}x=\int_a^b|f(x)|{\rm d}x=J. $ Since the bounded $s(x)$ is discontinuous at some of the zero points of $f(x)$ , and such points is at most countable, hence $s(x)$ is integrable. As per the approximation theorem, there exists a continuous function $g(x)$ such that $ \int_a^b|s(x)-g(x)|{\rm d}x\le\frac{1}{2}$ .
Therefore, $$\left|\int_a^bf(x)[g(x)-s(x)]{\rm d}x\right|\le \int_a^b|f(x)||g(x)-s(x)|{\rm d}x\le\frac{1}{2}J$$ Finally, we obtain \begin{align*} \int_a^b f(x)g(x){\rm d}x&=\int_a^b f(x)[g(x)-s(x)]{\rm d}x+\int_a^bf(x)s(x){\rm d}x\ge \frac{1}{2}J. \end{align*} This is correct?","['calculus', 'functions', 'real-analysis']"
4481245,Effect of Caratheodory criterion on a subset of a measurable set,"I met with a multiple choice question, Q. Let $E\subset M \subset \Bbb R^n$ , where $M$ is a measurable (Lebesgue) set with $m(M)<\infty, $ then $E$ is measurable if ( $m^*$ denotes the Lebesgue outer measure) $m(M)=m^*(E)+m^*(M\backslash E)$ . $m(M)=m^*(E)+m^*(M\cap E)$ . $m(M)=m^*(E)+m^*(M \triangle E)$ , $\triangle$ denotes the symmetric difference. $m(M)=m^*(E)+m^*(M\cap E^c)$ . Caratheodory criterion says ' $E$ is measurable if and only if $m^*(A)=m^*(A \cap E)+m^*(A \cap E^c)$ for all $A \subseteq \Bbb R^n$ '. Thus, here I can see the measurability of $E$ implies the statements of First, Third and Fourth options (which are exactly the same). But how can we make a necessary and sufficient statement using the condition 'being a subset of a measurable set $M$ '?","['measure-theory', 'real-analysis']"
4481248,For which $s\in\mathbb{C}$ do we have $\sum_{n=0}^\infty{s \choose n}=2^s$?,"By comparison with the binomial expansion of $(1+x)^s$ , a sufficient condition for the formula $\sum_{n=0}^\infty{s\choose n}=2^s$ to hold is that $A_s=\sum_{n=0}^\infty{s\choose n}$ is absolutely convergent, since then the binomial expansion of $(1+x)^s$ is normally convergent on $|x|\leq 1$ and represents $(1+x)^s$ for $|x|<1$ - by continuity of both sides of the equation, we get the formula for $|x|=1$ . I manage to prove that $A_s$ is absolutely convergent for real $s\geq 1$ . Indeed, then we have with $N=[s]+1$ and $m=[s]\geq 1$ the majorant $$
\sum_{n=0}^\infty\left|{s\choose n}\right|\leq\sum_{n=0}^m\left|{s\choose n}\right|+N!\zeta(m+1)\,.
$$ Further, for $s=\frac{1}{2}$ , we have by the Wallis product that $\left|{\frac{1}{2}\choose n}\right|\sim\frac{1}{2n\sqrt{\pi n}}$ , and we also get the absolute convergence. For the other values of $s$ , I am stuck. The series under consideration should be absolutely convergent for all real $s\geq 0$ . However, for $s=-\frac{1}{2}$ the series $A_s$ is convergent but not absolutely convergent by the Leibniz criterion and according to numerical evaluation it indeed represents $2^s$ . Question . For which complex values $s$ does the formula $A_s=2^s$ hold? Question' . For which complex values $s$ is the series $A_s$ convergent on the nose, not necessarily absolutely convergent?","['real-analysis', 'calculus', 'binomial-coefficients', 'sequences-and-series', 'complex-numbers']"
4481278,Wrong example of derivative of algebra in Sadri Hassani's mathematical physics book?,"The definition of the derivative of an algebra: Definition 3.4.1 A vector space endomorphism $\mathbf D$ : $\mathcal{A} \rightarrow \mathcal{A}$ is called a derivation on $\mathcal{A}$ if it has the additional property $$
\mathbf{D}(\mathbf{a b})=\lceil\mathbf{D}(\mathbf{a})\rceil \mathbf{b}+\mathbf{a}[\mathbf{D}(\mathbf{b})\rceil
$$ And he gave an example that all the algebra $C^r(a,b)$ , i.e., all the function in interval $(a,b)$ have derivatives up to order $r$ with multiplication defined as $(f g)(t) \equiv f(t) g(t) \quad \forall t \in(a, b)$ . And the  ordinary differentiation is a derivative of $C^r(a,b)$ . My question is , differentiation will map a function $f$ to $f^\prime$ while might have derivative up to order $r-1$ instead of $r$ , hence the map is not endomorphism, then how can we say that this map is an example of the derivative of an algebra?","['abstract-algebra', 'derivatives']"
4481289,Schreier-Sims variant to produce a compact/minimal representation,"I am currently looking into the generation of a base alongside a strong generating set for permutation groups. Naturally, I am digging into the details of the Schreier-Sims algorithm. After having played around with a few examples in GAP, I got the impression that the choice of a BSGS is anything but unique (after all there generally seems to be plenty of room for arbitrariness in choosing the points from which to generate a base and even for the same base, it seems to be possible to arrive at different generating sets). However, when one wants to do some further processing with the group, it seems to me as if it would be beneficial to choose a BSGS that uses as few base points and as few generators as possible (as then e.g. iterations over all basis points would get fewer as well). Thus, my question is whether there exist some variations of the Schreier-Sims algorithm that are tuned to exactly this purpose?","['permutations', 'group-theory']"
4481294,Probability to draw specific ball from the bag as last one,"Let's say I have a bag with 8 numbered balls inside, and I'm drawing them 1 by 1 until I get ball #1. What's the chance of me drawing ball #1 as last one? I was under the impression that the chances for me to draw it last is $$\frac18* \frac17*\frac16*  \frac15* \frac14* \frac13*\frac12=\frac1{40320}$$ since I'm drawing them 1 by 1, so 1st time I draw, I have 1/8 chance to pull #1, next draw I have 1/7, and so on? Is that wrong? A friend of mine says it's just 1/8, am I misunderstanding this?",['probability']
4481340,Is $M_t:=\exp\left(-\int_0^tc(Y_s)\:{\rm d}s\right)$ differentiable at $0$?,"Let $(\Omega,\mathcal A,\operatorname P)$ be a probability space $E$ be a topological space $(Y_t)_{t\ge0}$ be an $E$ -valued right-continuous process on $(\Omega,\mathcal A,\operatorname P)$ $c:E\to[0,\infty)$ be Borel measurable with $$\int_0^tc(Y_s)\:{\rm d}s<\infty\;\;\;\text{for all }t\ge0$$ and $$M_t:=\exp\left(-\int_0^tc(Y_s)\:{\rm d}s\right)\;\;\;\text{for }t\ge0$$ Note that $(M_t)_{t\ge0}$ is a $[0,1]$ -valued continuous nondecreasing process on $(\Omega,\mathcal A,\operatorname P)$ with $M_0=0$ . We can infer that every path of $(M_t)_{t\ge0}$ is differentiable Lebesgue almost everywhere. But are we able to show that $(M_t)_{t\ge0}$ is differentiable at $0$ and $$\operatorname E\left[\frac{M_t-M_0}t\right]\xrightarrow{t\to0+}\operatorname E\left[\left.\frac{\rm d}{{\rm d}t}M_t\right|_{t=0+}\right]\tag2?$$ If not can we somehow turn the claim to be true? EDIT : What I actually want to show is $$\operatorname E\left[\frac{M_t-M_0}t\right]\xrightarrow{t\to0+}-\operatorname E\left[c(Y_0)\right]\tag3$$ which might be weaker than what I'm asking for above. EDIT 2 : If we assume that $c$ is continuous, we easily see that $$f_\omega(t):=\int_0^tc(X_s(\omega))\:{\rm d}s\;\;\;\text{for }t\ge0$$ is differentiable at $0$ for all $\omega\in\Omega$ . In order to conclude $(2)$ , a sufficient condition should be that $c$ is bounded, since then $$\frac{\left|f_\omega(t)\right|}t\le\left\|c\right\|_\infty<\infty\;\;\;\text{for all }t>0\text{ and }\omega\in\Omega\tag4.$$ Using $e^{-x}\ge1+x$ for all $x\in\mathbb R$ , we see that $$0\le\frac{M_0-M_t}t\le\left\|c\right\|_\infty$$ and hence Lebesgue ` s dominated convergence theorem is applicable. But are there weaker assumptions?","['measure-theory', 'stieltjes-integral', 'probability-theory', 'real-analysis']"
4481342,Time derivative of the blend of a pair of quaternion curves,"I have two curves ${\bf q}_0(t), {\bf q}_1(t)$ . Each curve maps time $t$ to a unit quaternion. Construction of these curves is not important here, although we do have the respective time derivatives for the curves given by $\dot{\bf q}_0(t), \dot{\bf q}_1(t)$ . My goal is to find a unit quaternion and its time derivate of the two curves blended by some parameter $p \in [0, 1]$ . Using a slerp for the blended unit quaternion gives ${\bf q} = {\bf q}_0 ({\bf q}^*_0 {\bf q}_1)^p$ , where ${\bf q}^*$ is the quaternion conjugate, and ${\bf q}^p = e^{p \log({\bf q})}$ . For the sake of this discussion, assume $p$ is not time dependent. It will be in the final solution but for now let's not be bothered by that. My question is how to compute the time derivative $\dot{\bf q}$ . Intuitively, it appears to be something of the form $\dot{\bf q} = ((1 - p)\dot{\bf q}_0{\bf q}^*_0 + p\dot{\bf q}_1 {\bf q}^*_1) {\bf q}$ , which is linearly interpolated angular velocities times blend result. This formula obviously is correct for $p = 0$ and for $p = 1$ . I'm trying to find some mathematical backing for the general case. Naively, taking $\dot{\bf q} = \dot{\bf q}_0({\bf q}^*_0 {\bf q}_1)^p + {\bf q}_0 p ({\bf q}^*_0 {\bf q}_1)^{p - 1} (\dot{\bf q}^*_0 {\bf q}_1 + {\bf q}^*_0 \dot{\bf q}_1)$ can't be correct in the general case, since the product operator does not commute. I could use some help in finding the proper time derivative. In particular, I would like to know whether there is a general formula for $\frac{d}{dt}{\bf q}(t)^p$ , for constant $p \in [0, 1]$ . Perhaps someone with a grasp of Lie groups/algebras could shed some light on this.  Thanks for your help.","['interpolation', 'lie-algebras', 'derivatives', 'lie-groups', 'quaternions']"
4481358,Examples where geometry/topology helps algebra,"I think that the most beautiful parts of mathematics are the ones where algebra and geometry/topology interact with each other. For example, in algebraic topology we often transform geometric problems in algebraic ones, so we could say that in this case ""algebra helps geometry"". Are there examples the other way around? I know, for example, that given a commutative ring $R$ , we can functorially construct the topological space $\text{Spec}(R)$ . But does studying the spectrum through topological methods actually offer algebraic results on $R$ that were harder to find out in a purely algebraic manner? Is it useful in this sense? More in general, I would like to find out examples where geometry/topology helps algebra.","['commutative-algebra', 'algebraic-geometry', 'algebraic-topology', 'differential-geometry']"
4481361,Average of ratios compared to ratio of averages,How to formalize (in simple terms) when average of ratios would be  greater than ratio of averages? I found that sometimes the former is greater than the latter and sometimes the latter is greater than the former. I want to understand in what cases one would be greater than the other. What happens at asymptotic? And what happens in day to day calculations? Which should I expect to be greater?,"['average', 'statistics', 'intuition', 'ratio']"
4481363,Simpler approach when calculating $\lim_{x\to0} \frac{e^{2x} - e^x}{4^x - 2^x}$?,"I know this is a really simple problem, but I can't figure out what logical mistake I am making on my approach : $$\lim_{x\to0} \frac{e^{2x} - e^x}{4^x - 2^x}$$ Dividing numerator and denominator with $4^x$ , $$ \lim_{x\to0} \frac{({\frac{e^2}{4}) }^x-{(\frac{e}{4}})^x}{1 - ({\frac{1}{2}})^x} $$ $\left(\frac{1}{2}\right)^x$ and $\left(\frac{e}{4}\right)^x$ converge to $0,$ so I thought this limit converges to $1$ . (+ additional approach) $$
    \lim_{x\to0} \frac{e^{2x} - e^x}{4^x - 2^x} \\
    = \lim_{x\to0} \frac{e^{2x} - e^x}{x} \cdot \frac{x}{4^x - 2^x} \\
    = \lim_{x\to0} \left(\frac{e^{2x} - 1}{x} - \frac{e^x - 1}{x} \right) \cdot \frac{1}{\left(\frac{4^x - 1}{x} - \frac{2^x - 1}{x} \right)}
$$ And this becomes $(2-1) \times \frac{1}{\ln4 - \ln2 } = \frac{1}{\ln2}$ , using the definition of derivative. I wonder if there's a simpler solution without using L'Hopital's rule.","['limits', 'convergence-divergence', 'eulers-number-e']"
4481421,The formula for $g\frac{d}{dg}g\frac{d}{dg}...g\frac{d}{dg}f(g)$,"Yesterday, I asked if there is a formula given by a finite sum for the expression. Having experimented with Wolfram Alpha I found that it can be represented as $$ \sum_{j=0}^{n}a_{j}g^{j}\frac{d^{j}}{dg^{j}}f(g),$$ where $a^{j}$ is given by the coefficient attached to $z^{n-1}$ in the sum expansion of the generating function $$\frac{z^{j}}{(1-jz)(1-(j-1)z)...(1-z)},$$ assuming I did not make any mistakes. How does one go about proving it? In the title there are $n$ iterations of $g\frac{d}{dg}$ .","['summation', 'theorem-provers', 'sequences-and-series', 'generating-functions', 'derivatives']"
4481438,Book recommendation for geometric intuition,"I know that this may be off-topic. The gist is: I get frustrated when I solve questions with a large integral (both in physics and mathematics) or some other heavy machinery, and then see a solution which is a geometrically-inspired proof. For eg. Let $\theta_1, \theta_2, \theta_3, … , \theta_{10}$ be positive values angles in radians such that $\theta_1+\theta_2+ \theta_3+… + \theta_{10}=2\pi$ . Define complex numbers $z_1=e^{i\theta_1}$ , $z_k=z_{k-1}e^{i\theta_k}$ for $2\leq k\leq 10$ . Then which of these is true? P: $|z_2-z_1|+|z_3-z_2|+…|z_1-z_{10}|\leq 2\pi.$ Q: $|z_2^2-z_1^2|+|z_3^2-z_2^2|+…|z_1^2-z_{10}^2|\leq 4\pi.$ Note that I am not asking for a solution to this question, which can be solved by considering that the perimeter of any n-sided polygon is less than the circumference of its circumscribing circle. I want some book recommendations where these kinds of hard-but-geometrically-easy problems (algebra, complex numbers, Calculus-I,II, elementary combinatorics) are shown with beautiful geometrical interpretations. Please keep the level to advanced high school level (should not focus too much on linear algebra (though vectors and matrices except eigenvalues are welcome), multivariable calculus, topology, graph theory, fields and rings etc.)","['book-recommendation', 'geometry', 'intuition']"
4481457,Integrals involving the error function and the gaussian,"I would like to know if the following integral has an explicit expression or not. $$F(r)=\int_1^r \frac{1}{t^2}e^{t^2/2} \operatorname{erf}\left(\frac{t}{\sqrt{2}}\right)dt$$ where the error function inside is defined as $$\operatorname{erf}\left(\frac{t}{\sqrt{2}}\right)=\frac{1}{\sqrt{\pi}}\int_0^t e^{-x^2/2}dx.$$ Thus, $$F(r) = \frac{1}{\sqrt{\pi}}\int_{1}^{r}\int_0^{t}\frac{1}{t^2}e^{\frac{t^2-x^2}{2}}dt dx$$ which on making a change of variable takes the form, $$F(r) = \frac{1}{\sqrt{\pi}}\int_1^r\int_{0}^1 \frac{\exp\left(\frac{t^2}{2}\left(1-y^2\right)\right)}{t}dy dt$$ Now $$\int_1^r \frac{\exp\left(\frac{t^2}{2}\left(1-y^2\right)\right)}{t} dt = \frac{1}{2}\left[\Gamma\left(0,\frac{y^2-2}{2}\right)-\Gamma\left(0,\frac{r^2(y^2-2)}{2}\right)\right].$$ To finish to computation one needs to compute, $$\frac{1}{2}\int_{0}^1\left[\Gamma\left(0,\frac{y^2-2}{2}\right)-\Gamma\left(0,\frac{r^2(y^2-2)}{2}\right)\right]dy,$$ but I am not sure how to compute expression involving the incomplete gamma function. I would like to know if there are any explicit expressions of the above integral involving the Exponential Integral or any other special function?","['integration', 'multivariable-calculus', 'calculus', 'real-analysis']"
4481465,Integrate $\int\frac{3x}{x^5+x^4+1}dx$,"I have an integral which I solved. But, I am not sure whether my answer is right or not. The integral is $$\int\frac{3x}{x^5+x^4+1}dx$$ My answer $$3\left(-\dfrac{\displaystyle\sum_{\left\{Z:\>Z^3-Z+1=0\right\}}\frac{\left(2Z^2-3Z-1\right)\ln\left(\left|x-Z\right|\right)}{3Z^2-1}}{7}\\+\dfrac{\ln\left(x^2+x+1\right)}{7}-\dfrac{4\arctan\left(\frac{2x+1}{\sqrt{3}}\right)}{7\sqrt{3}}\right)$$","['integration', 'calculus', 'solution-verification', 'partial-fractions']"
4481477,An equivalent condition of 0 is in the convex hull of a set of vectors,"Let $V=\{v_1,...,v_k\}$ be a set of vectors in $\mathbb{R}^d$ . And for each $v_i$ we define the half-space $$H(v_i):=\{u\in \mathbb{R}^d:u\cdot v_i\le 0\}.$$ The convex hull of $V$ is defined as $conv(V):=\{u=\sum_{i=1}^k \alpha_iv_i: \alpha_i\ge 0, \sum_{i=1}^k \alpha_i=1\}$ . Edited: We can assume that $0$ is not in the convex hull of any proper subset of $V$ and $v_2-v_1,...,v_k-v_1$ spans $\mathbb{R}^d$ and are linearly independent. One claims that $0\in conv(V)$ if and only if $\cap_{i=1}^kH(v_i)=\{0\}$ . And I am trying to prove this claim. For the $\Rightarrow$ direction, assume $u\in \cap_{i=1}^kH(v_i)$ . As 0 is in the convex hull, then $0=\sum_{i=1}^k\alpha_iv_i$ . And then $$0=0\cdot u =\sum_{i=1}^k\alpha_i (v_i\cdot u),$$ which implies $u\cdot v_i=0$ that for all $i$ (as we assume $\alpha_i>0$ ). Therefore $u\cdot(v_i-v_1)=0$ for all $i$ , which implies $u=0$ . I heard separation theorem is involved in the proof of the other direction. But I am not sure.","['linear-algebra', 'geometry', 'combinatorics']"
4481503,Extended stars-and-bars problem(where the upper limit of the variable is bounded),"The problem of counting the solutions $(a_1,a_2,\ldots,a_n)$ with integer $a_i\geq0$ for $i\in\{1,2,\ldots,n\}$ such that $$a_1+a_2+a_3+\ldots+a_n=N$$ can be solved with a stars-and-bars argument. What is the solution if one adds the constraint that $a_i\leq r_{i}$ for certain integers $r_{1},\ldots,r_n$ ? e.g. for $n=3$ , $N=6$ and $(r_1,r_2,r_3)=(3,3,2)$ , the tuple $(a_1,a_2,a_3)=(2,3,1)$ is a solution, but $(2,1,3)$ is not a solution because $a_{3}=3>2=r_3$ .","['combinatorics', 'multisets']"
4481514,"Show that $(1 + x^2 y^2)^{-\frac{1}{x^2 + y^2}}$ is $1$ as $(x,y) \to(0,0)$","I want to show that: $$\lim_{(x,y)\to(0,0)} (1 + x^2 y^2)^{-\frac{1}{x^2 + y^2}} = 1$$ Direct substitution doesn't work, because of the denominator of $\frac{1}{x^2 + y^2}$ . Usually these problems are solved by bounding the following expression: $$\left|(1 + x^2 y^2)^{-\frac{1}{x^2 + y^2}} -1 \right| = \left| \frac{1 - (1 + x^2 y^2)^{\frac{1}{x^2 + y^2}} }{ (1 + x^2 y^2)^{\frac{1}{x^2 + y^2}}} \right|$$ I don't know how to continue.","['limits', 'multivariable-calculus', 'real-analysis']"
4481618,How many points uniquely define a square?,"I was wondering how many points uniquely define a square. Now it is clear to me that if we have $n$ random points on a square then this does not necessarily uniquely define the square (for example they may all lie on the same edge). My question was more the following: supposing I was able to choose exactly which points of my square to use, what is the minimum number of points I could pick to uniquely define the square, and which points should I pick? Edit: the points need not lie on the vertices.",['geometry']
4481642,Find a single convex closed curve of fixed length in the plane that has the smallest average distance to the origin,"(Reformulation of this question ) I want to find which convex closed curve of length $2\pi$ has the smallest average distance of all its points to the origin. For instance, given a curve $\gamma(s)$ with arc-length parametrization, the average distance to the origin is given by: $$\dfrac{1}{2\pi} \int_{0}^{2\pi} |\gamma| \, ds$$ So I need to minimize that integral over all $\gamma(s)$ that satisfy the condition of being: closed, convex, and with length $2\pi$ . Here is my approach: First I calculated that integral for the family of ovals with curvature $\kappa(s)=1 /\left(a^{2} \cos ^{2}(s)+a^{-2} \sin ^{2}(s)\right)$ , for several $a$ . For $a=1$ , it's just the circle with average distance to the origin of 1. As $a$ increases, the curve gets flatter and the value for the average distance decreases to a value of $\pi/4$ , when $a\to \infty$ . So $\pi/4$ is my best lower bound so far, I tried using Euler-Lagrange equation but I'm not sure how many constraints to use.","['calculus-of-variations', 'plane-curves', 'curves', 'geometry', 'differential-geometry']"
4481652,Calculate $\lim_{x \to 0} \frac{\cos(x)}{\sin(x)}$,I have a question regarding the following limit calculation: $\lim_{x \to 0} \frac{\cos(x)}{\sin(x)}$ The only way I can solve this is by looking at the one-sided limits: $\\$ From above: $\lim_{x \to 0^{+}} \frac{\cos(x)}{\sin(x)}$ . The numerator approaches $1$ with a positive sign. The denominator approaches $0$ with a positive sign. $\implies$ the limit is $\infty$ $\\$ From below: $\lim_{x \to 0^{-}} \frac{\cos(x)}{\sin(x)}$ . The numerator approaches $1$ with a positive sign. The denominator approaches $0$ with a negative sign. $\implies$ the limit is $-\infty$ The one-sided limits do not agree and so the limit does not exist. My concern is this: would you give full marks for an answer like this? It feels very informal but I do not know how to argue the same thing formally.,['limits']
4481748,How to obtain the period of this nonlinear differential equation?,"Lately, I've been trying to find the period of an angle included in the following differential equations, but only could with the basic model: Basic or original: $$\mathrm{For}\ (\Phi (0), \Omega (0))=(\Phi_{o},0),\ \frac{d^2\Phi}{dt^2}= \frac{g}{\ell_{o}}\sin{\Phi}-\frac{g}{\ell_{o}}\zeta\ \mathrm{sgn\ \Phi}\ ;$$ Modified: $$\mathrm{For\ the\ same\ initial\ conditions},\ \frac{d^2\Phi}{dt^2}= \frac{g}{\ell_{o}}\frac{\sin{\Phi}}{f(\Phi)}-\frac{g}{\ell_{o}}\zeta \frac{\mathrm{sgn\ \Phi}}{f(\Phi)}\ -2\dot{\Phi}^2 \frac{f'(\Phi)}{f(\Phi)}.$$ Where $g$ is gravity, $\ell_{o}$ is the length of the inverted pendulum, $\zeta$ a group of other constants, $\operatorname{sgn}\left(\cdot\right)$ is the signum function, $\dot{\Phi}=\Omega=\frac{d\Phi}{dt}$ , $f(\Phi)=\sqrt[3]{1-\eta\cos{\Phi}}$ ( $\eta$ is another constant) and $f'(\Phi)=\frac{df(\Phi)}{d\Phi}$ . And so, the method I used to get the period was basically this: Let $F(\Phi)= \frac{g}{\ell_{o}}\sin{\Phi}-\frac{g}{\ell_{o}}\zeta\ \mathrm{sgn\ \Phi}$ , then the diff. eq. reduces to $\frac{d^2\Phi}{dt^2}=F(\Phi).$ And now I just proceed. \begin{align}
  \int \frac{d^2\Phi}{dt^2}d\Phi &= \int F(\Phi)\ d\Phi\\
  \frac{1}{2}\dot{\Phi}^2 &= \int F(\Phi)\ d\Phi\ +C\\
  \dot{\Phi} &= \frac{d\Phi}{dt} = \sqrt{2\int F(\Phi)\ d\Phi +C}\\
  \frac{T}{4}=\int_{t_{o}}^{t_{1}}dt &= \int_{0}^{\Phi_{o}}\frac{d\Phi}{\sqrt{2\int F(\Phi)\ d\Phi +C}}\\
  T &=2\sqrt{2} \int_{0}^{\Phi_{o}}\frac{d\Phi}{\sqrt{\int F(\Phi)\ d\Phi +C}}.
\end{align} This worked for the basic model; but didn't for the modified one. The issue was the integral of $F(\Phi)$ since in the modified version it included all terms divided by $f(\Phi)$ and also the $\dot{\Phi}^2 \frac{f'(\Phi)}{f(\Phi)}$ one too. Can someone tell me any easier way to attain the period of this modified system? Or what approximation could I use to make it easier to deal with?","['integration', 'computational-mathematics', 'calculus', 'physics', 'mathematical-physics']"
4481789,"Intuitively, $(\prod_{\lambda\in\Lambda}A_\lambda)\cap(\prod_{\lambda\in\Lambda}B_\lambda)=\prod_{\lambda\in\Lambda}(A_\lambda\cap B_\lambda)$. But ..","I am reading ""Topology 2nd Edition"" by James R. Munkres. The definition of a cartesian product of an indexed family of sets is here: src) Definition. Let $\{A_{\alpha}\}_{\alpha \in J}$ be an indexed family of sets; let $X = \bigcup_{\alpha \in J} A_{\alpha}$ . The cartesian product of this indexed family, denoted by $$ \prod_{\alpha \in J} A_{\alpha} $$ is defined to be the set of all $J$ -tuples $(x_{\alpha})_{\alpha \in J}$ of elements of $X$ such that $x_{\alpha} \in A_{\alpha}$ for each $\alpha \in J$ . That is, it is the set of all functions $$ \mathbf{x} \, : \, J \to \bigcup_{\alpha \in J} A_{\alpha} $$ such that $\mathbf{x}(\alpha) \in A_{\alpha}$ for each $\alpha \in J$ . The following problem (Problem 9 on p.51) is from ""Introduction to Set Theory and Topology"" (in Japanese) by Kazuo Matsuzaka: Prove that $$\left (\prod \limits _{\lambda \in \Lambda}A_\lambda \right )\cap \left (\prod \limits _{\lambda \in \Lambda}B_\lambda \right )=\prod \limits _{\lambda \in \Lambda}(A_\lambda \cap B_\lambda ).$$ Let $A_\lambda :=\mathbb{N}$ . Let $B_\lambda :=\mathbb{Z}$ . The codomain of elements of $\displaystyle \prod \limits _{\lambda \in \Lambda}A_\lambda$ is $\mathbb{N}$ . The codomain of elements of $\displaystyle \prod \limits _{\lambda \in \Lambda}B_\lambda$ is $\mathbb{Z}$ . The codomain of elements of $\displaystyle \prod \limits _{\lambda \in \Lambda}(A_\lambda \cap B_\lambda )$ is $\mathbb{N}$ . Then, $\displaystyle \left (\prod \limits _{\lambda \in \Lambda}A_\lambda \right )\cap \left (\prod \limits _{\lambda \in \Lambda}B_\lambda \right )=\varnothing$ since the codomain of elements of $\displaystyle \prod \limits _{\lambda \in \Lambda}A_\lambda$ and the codomain of elements of $\displaystyle \prod \limits _{\lambda \in \Lambda}B_\lambda$ are not the same. Let $a$ be a function from $\Lambda$ to $\mathbb{N}$ such that $a(\lambda ):=1$ for any $\lambda \in \Lambda$ . Then, $\displaystyle a\in \prod \limits _{\lambda \in \Lambda}(A_\lambda \cap B_\lambda )$ . So, $\displaystyle \prod \limits _{\lambda \in \Lambda}(A_\lambda \cap B_\lambda )\neq \varnothing$ . But intuitively, I think $\displaystyle \left (\prod \limits _{\lambda \in \Lambda}A_\lambda \right )\cap \left (\prod \limits _{\lambda \in \Lambda}B_\lambda \right )=\prod \limits _{\lambda \in \Lambda}(A_\lambda \cap B_\lambda )$ holds. What is the answer to this problem?",['elementary-set-theory']
4481802,Brownian motion and uniform distribution,"We consider a $q$ -dimensional Brownian motion $(W_r)_r.$ Let $x \in \mathbb{R}^q$ and $\delta \geq 0.$ Let $T:=\inf\{r \geq 0, W_r \notin B(x,\delta)\}.$ Prove that $P_x(T<\infty)=1,$ and that $W_T,$ under $P_x,$ is uniformly distributed on a sphere and determine it. For the first part we know that since $\limsup_{r \to \infty}\frac{1}{\sqrt{r}}\Vert W_r \Vert_2\geq\limsup_{r \to \infty}\frac{1}{\sqrt{r}}|W^1_r|=\infty \ P_x$ -a.s. where $\Vert. \Vert_2$ denotes the Euclidean norm that $\limsup_{r \to \infty}\Vert W_r \Vert_2=\infty \ P_x$ -a.s. and therefore $T$ is finite a.s. How can we prove that under $P_x, W_T$ is uniformly distributed on a sphere?","['stochastic-processes', 'martingales', 'brownian-motion', 'probability-theory', 'stochastic-calculus']"
4481852,How many ways can $7$ professors and $5$ students be seated at this long rectangular table so that no student sits across from another student?,"So, I was doing a simple combinatorics problem, and I merely wanted to know where I actually went wrong. So the question was: My Method:
So originally, I realised that the only possible combinations which would not be included were once where $2$ students or $4$ students were sitting across from the table, and in the case where $4$ students were sitting across from each other, $2$ students were still in some way shape or form sitting across from each other. So I originally assumed that $2$ students were sitting across from each other, something like this (s means students): This includes all the combinations that we don't want (I think), because as I explained before, $2$ students always sit across each other when we discard a permutation. So the total number of such permutations would be $(5 \cdot 4) \cdot 6 \cdot 10!$ ( $5 \cdot 4$ represents the way in which we can choose the $2$ students who would be sitting across from each other, $6$ because the $2$ students can be in any $6$ columns, and $10!$ to order the remaining $10$ people since we don't particularly care how they are seated since we have violated the condition anyways) The actual answer includes seating the students first, which can be seated in $12 \cdot 10 \cdot 8 \cdot 6 \cdot 4$ and then the professors in $7!$ ways. But I'm confused as to why what I described does not work, where am I missing something?","['combinations', 'combinatorics']"
4481857,"How many arrangements we can make from the word ""SINGAPORE"" where the letters $E$ and $I$ do not occur together?","Find the number of arrangements for the letters in SINGAPORE given that I and E must be separated by at least one letter. My answer: S N G A P O R ${}\implies 7!$ ways to arrange There are $8$ spaces between S N G A P O R so $\binom{8}{2} \cdot 2!$ ways to slot the I and E . Therefore, $\binom{8}{2} \cdot 2! \cdot 7! = 282\,240$ Am I correct?","['permutations', 'solution-verification', 'combinatorics', 'discrete-mathematics']"
4481867,"Solve in 75 seconds: $\displaystyle{\iiint_{\Omega} {\dfrac{(3x + y − 4)^2}{9x^2 + y^2 + z^2 − 18x − 2y + 10}} \,dz\,dy\,dx}$","(how in the world do I solve this in 75 seconds) $${\iiint\limits_{\Omega} {\dfrac{(3x + y − 4)^2}{9x^2 + y^2 + z^2 − 18x − 2y + 10}} \ \mathrm dz\ \mathrm dy\ \mathrm dx}$$ where $\displaystyle{\Omega = \{ (x, y, z) \in \mathbb{R}^3 \mid 9 (x − 1)^2 + (y − 1)^2 + z^2 \le 1 \}}.$ It's $\dfrac{8\pi}{27}$ . I'm not asking for solutions. This is a competition problem. I'm just amazed how people could solve this that fast. Perhaps, exposure to a lot of problems like this during training enables them. Maybe this is easy for you too.","['integration', 'multivariable-calculus', 'calculus', 'contest-math']"
4481891,Counting Binary Sequences (Juggling States) with Constraints,"Short version: Consider a set of positive integers $\{m_i\}_{i=1}^n$ with $m_i < m_{i+1}$ and form an infinite binary sequence $s$ with $b$ $1$ s and arbitrarily many $0$ s subject to the following constraint: If there is a $0$ in position $j$ , i.e., $s_j = 0$ , then $s_{j+m_i} = 0$ for all $i$ . It follows that $s_{j+M} = 0$ for any $M = \sum_i a_i m_i$ , where the $a_i$ are non-negative integers. We can thus remove from the set any $m_i$ that is a multiple of some other $m_i'$ . How many such sequences are there for fixed $b$ and $\{m_i\}$ ? (Feel free to only consider $n = 2$ if it helps.) Medium version: The binary sequences described above are known as juggling states with $b$ balls, specifically those that are contained in juggling patterns of period $m_i$ for all $i$ . For more on juggling states, see this paper . Several properties of these states are quickly clear. We let $N(b)$ be the number of states with $b$ $1$ s. All states start with either a $0$ or a $1$ , so we can write $N(b) = N_1(b) + N_0(b)$ . Any state with $b$ $1$ s can be mapped to a state with $b+1$ $1$ s by simply prepending a $1$ to the state, so it follows that $N_1(b) = N(b-1)$ . We then find, for $b > 1$ , $$N_0(b) = N(b) - N(b-1).$$ Thus if we know $N(1)$ and $N_0(b)$ for all $b$ , we can find $N(b)$ . In general, letting $N_x(b)$ denote the number of states with some initial sequence $x$ , we can use the same trick to show that $N_{1x}(b) = N_x(b-1)$ , where by $1x$ we mean $x$ with a prepended $1$ . Setting $d = \gcd(m_1, \ldots, m_n)$ , we can make some additional observations. If $d = 1$ , $N_0(b)$ must be $0$ for sufficiently large $b$ , and thus $N(b)$ must be eventually constant, since after the first $0$ there will be finitely many positions where the $b$ $1$ s can be placed, as every sufficiently large positive integer is expressible in the form $\sum_i a_i m_i$ . Thus when $b$ is sufficiently large, the only states that are found for $b+1$ are the same $N(b)$ states with an additional $1$ prepended. For general $d$ , letting $N_{0^k}(b)$ be the number of states with $b$ $1$ s beginning with $k$ consecutive $0$ s, we can prove that $N_{0^d}(b)$ is $0$ for sufficiently large $b$ . (We let $k = 0$ represent $N(b)$ .) In every case I have tried, I have found that for $k < d$ , $N_{0^k}(b)$ is a polynomial of degree $d - k - 1$ for sufficiently large $b$ , so $N(b)$ is eventually a polynomial of degree $d - 1$ , but I am unable to prove this. Long version: To phrase this more technically, we use the concept of a numerical semigroup (Wikipedia link) , which is an infinite set of nonnegative integers containing $0$ that is closed under addition. Every element of a numerical semigroup $S$ can be written in the form $\sum_i a_i n_i$ for some finite set of non-negative integers $\{n_i\}$ with $\gcd(n_1, \ldots, n_r) = 1$ , which are called the generators of $S$ . We can then denote $S$ as $\langle n_1, \ldots, n_r\rangle$ . The positive integers not contained in $S$ are called the gaps of $S$ , and the number of gaps is denoted $g(S)$ . We see that for $d=1$ , the positions of the zeros in the state, minus the position of the first $0$ , form a numerical semigroup, specifically $S = \langle m_1, \ldots, m_n\rangle$ . The gaps of $S$ are the possible positions for the $1$ s after the first $0$ . It follows that $N_0(b) = 1$ for $b = g(S)$ , when all the gaps are filled, and $N_0(b) = 0$ for $b > g(S)$ . We can extend this to general $d$ by only looking at the positions of the $0$ s, minus the position of the first $0$ , that are congruent to $0$ modulo $d$ . For example, for $\{m_i\} = \{4,10\}$ , where $d = 2$ , we see that the positions after the first $0$ where there must be additional $0$ s are (4,8,10,12,14,...). Including $0$ to account for the first $0$ , these values are the elements of the numerical semigroup $S = \langle 2,5 \rangle$ multiplied by 2. We see that $g(S) = 2$ , corresponding to the positions 2 and 6. Thus if we have $0$ s in positions $0$ and $1$ , it follows that the state can only have $1$ s in positions 2, 3, 6, and 7. Thus $N_{00}(b) = 1$ for $b = 4$ and $N_{00}(b) = 0$ for $b > 4$ . In general, we can use this numerical semigroup analysis to show that $N_{0^d}(b) = 1$ for $b = d \cdot g(S)$ and $N_{0^d}(b) = 0$ for $b > d \cdot g(S)$ , where $S = \left\langle \frac{m_1}{d}, \ldots, \frac{m_n}{d}\right\rangle$ . However, it is unclear how to work backwards from this point to show that $N(b)$ must eventually be a polynomial of degree $d – 1$ . One possible route would be to extend the identity $N_0(b) = N(b) – N(b–1)$ to states with more initial $0$ s. Empirically, we find $$ N_{0^{k+1}}(b) = N_{0^k}(b) - N_{0^k}(b-1)$$ for $0 \leq k \leq d–1$ , but I have been unable to prove it. We can rewrite the right hand side as $$\begin{align*}
N_{0^k}(b) - N_{0^k}(b-1) 
&= N_{0^{k+1}}(b) + N_{0^k 1}(b) - N_{0^k}(b-1)\\
&= N_{0^{k+1}}(b) + N_{0^k 1}(b) - N_{10^k}(b),
\end{align*}$$ so it follows that proving the desired identity is equivalent to proving $N_{0^k 1}(b) = N_{10^k}(b)$ for $0 \leq k \leq d–1$ , but I have been unable to do this either. In either form, this identity, combined with the already proven fact that $N_{0^d}(b)$ is eventually 0, would be sufficient to showing that $N(b)$ is a polynomial of degree $d-1$ for sufficiently large $b$ , so any ideas would be helpful.","['binary', 'combinatorics']"
4481985,Negative Binomial mean,"Let $X\sim BN(r,p)$ . If I try to compute E[X] through the Moment Generating Function I get the following: \begin{aligned}
E[X] &=\left.\frac{d}{d t} M_{X}(t)\right|_{t=0} \
&=\left.\frac{d}{d t} p^{r}\left(1-p e^{t}(1-p)\right)^{-r}\right|_{t=0} \
&=p^{r+1} r(1-p)(1-p(1-p))^{-r-1}
\end{aligned} But if I find it through the first factorial moment I get $E\left[X^{(h)}\right]$ $=\sum_{x=0}^{\infty} \frac{x !}{(x-h) !} \frac{(x+r-1) !}{x !(r-1) !} p^{r}(1-p)^{x}$ $=\sum_{x=h}^{\infty} \frac{(x+r-1) !}{(x-h) !(r-1) !} p^{r}(1-p)^{x},\left(x^{*}=x-h\right)$ $=\sum_{x^{*}=0}^{\infty} \frac{\left(x^{*}+r+h-1\right) !}{x^{*} !(r-1) !} p^{r}(1-p)^{x^{*}+h}$ $=\frac{(r+h-1) !}{(r-1) !} \frac{(1-p)^{h}}{p^{h}} \sum_{x^{*}=0}^{\infty} \frac{\left(x^{*}+h+r+1\right) !}{x^{*} !(r+h-1) !} p^{r+h}(1-p)^{x^{*}}$ $=\frac{(r+h-1) !}{(r-1) !} \frac{(1-p)^{h}}{p^{h}}$ Therefore, $E[X]=\frac{r(1-p)}{p}$ Where did I do wrong?","['statistics', 'negative-binomial', 'probability']"
4482030,Geometry in complex numbers.,"Let $\theta_1, \theta_2, \theta_3, … , \theta_{10}$ be positive valued angles (in radian) such that $\theta_1+\theta_2+ \theta_3+… + \theta_{10}=2\pi$ . Define complex numbers $z_1=e^{i\theta_1}$ , $z_k=z_{k-1}e^{i\theta_k}$ for $2\leq k\leq 10$ . Then which of these is true? P: $|z_2-z_1|+|z_3-z_2|+…|z_1-z_{10}|\leq 2\pi.$ Q: $|z_2^2-z_1^2|+|z_3^2-z_2^2|+…|z_1^2-z_{10}^2|\leq 4\pi.$ What I understand: $z$ is a unimodular complex number with argument $\theta_1$ . The expression $z_k=z_{k-1}e^{i\theta_k}$ just indicates that rotating $z_{k-1}$ anticlockwise by $\theta_{k}$ gives $z_k$ . Also, $\theta_1+\theta_2+ \theta_3+… + \theta_{10}=2\pi$ tells us that $z_{10}$ is the complex number $1+0\cdot i$ . (X axis rotated on to itself). So I get a figure: Now, statement P refers to just the perimeter of the polygon $z_1z_2…z_{10}z_1$ , which is less than the circumference of the circle which is $2\pi$ . I have a problem with Q. My solution is to think that the $z_i^2$ ‘s are just all unimodular complex numbers with all the angles $\theta_i$ doubled. Thus the as we go from $z_1^2$ till $z_{10}^2$ , we span the whole unit circle twice, ending at $z_{10}^2=1.$ So the total length of the sides of this (?? Double polygon?) is less than twice the circumference of the circle which is $4\pi$ . I’m a bit uncomfortable with the justification of Q. I think I can prove that the perimeter of a polygon is less than the circumference of its circumcircle (I haven’t attempted it yet)(#). This is a multiple choice question, so no extremely rigorous proofs are required. Is it okay? How would you make the justification of Q rigorous? I’d also appreciate it if anyone could post an algebraic proof here. EDIT :  I managed to prove (#). I wrote the perimeter as $$\displaystyle P=2R\sum_{k=1}^{10} \sin\frac{\theta_k}{2}.$$ Now, by Jensen’s inequality, since $\sin x$ is a concave function in $[0,\frac12\pi]$ , we have $$\displaystyle \frac{1}{10}\sum_{k=1}^{10} \sin\frac{\theta_k}{2}\leq \sin\left(\frac{1}{10} \sum_{k=1}^{10} \frac{\theta_k}{2}\right)=\sin \frac{\pi}{10}=\frac{\sqrt 5-1}{4}$$ so that $$\displaystyle P=2R\sum_{k=1}^{10} \sin\frac{\theta_k}{2}\leq 5(\sqrt 5-1)R\lt 2\pi R= Circumference. $$","['circles', 'geometry', 'polygons', 'solution-verification', 'complex-numbers']"
4482033,"Proof that $\lVert u\rVert_{L^p(M)}\leq C\lVert\Delta u\rVert_{L^p(M)}$ for each $u\in W_0^{2,p}(M)$","Let $(M,g)$ be a compact Riemannian manifold with nontrivial boundary and $W_0^{2,p}(M)$ the elements of $W^{2,p}(M)$ that vanish on $\partial M$ . I'm trying to understand a proof that there exists a constant $C$ such that $$\lVert u\rVert_{L^p(M)}\leq C\lVert\Delta u\rVert_{L^p(M)}$$ for each $u\in W_0^{2,p}(M)$ . Here is the proof: Suppose there does not exist such a constant. Then we can find a sequence $u_i$ with $\lVert u_i\rVert_{L^p}=1$ and $\Delta u_i\to 0$ in $L^p$ . By the elliptic estimate $$\lVert u\rVert_{W^{k+2,p}}\ \leq K(\lVert Lu\rVert_{W^{k,p}}+\lVert u\rVert_{L^p})$$ (existence of the constant $K$ ), it follows that $u_i$ is uniformly bounded in $W^{2,p}$ . By the Rellich-Kondrachov compactness theorem, there exists a subsequence of $u_i$ converging to some function $u$ weakly in $W^{2,p}$ and strongly in $L^p$ . Then for any compactly supported smooth test function $\psi$ , $$\langle\Delta u,\psi\rangle=\langle u,\Delta\psi\rangle=\lim_{i\to\infty}\langle u_i,\Delta\psi\rangle=\lim_{i\to\infty}\langle\Delta u_i,\psi\rangle=0,$$ and therefore $\Delta u=0$ . By elliptic regularity, $u$ must be smooth. Since it vanishes on the boundary, the maximum principle implies that $u$ is identically zero. But this is a contradiction since we must have $\lVert u\rVert_{L^p}=1$ . I feel confused about many of its details: The uniform boundedness here seems strange to me. In my introductory analysis course, a uniform bound is a constant $M$ such that $|u_i(x)|\leq M$ for every $x$ and for every $i$ , but in the proof, it doesn't seem to work this way. I have looked up the Rellich-Kondrachov compactness theorem in my PDE book and Wikipedia, and it doesn't refer to subsequences. How could the author grab such a subsequence? Where does the equality $\langle\Delta u,\psi\rangle=\langle u,\Delta\psi\rangle$ come from? Is there an inner product on $L^p$ ? I only know $L^2$ can be equipped with an inner product, which makes it a Hilbert space. Any suggestion is welcome. Thank you.","['riemannian-geometry', 'real-analysis', 'functional-analysis', 'partial-differential-equations', 'differential-geometry']"
4482039,I am missing some point about Cantor's Theorem,"I don't get the argumentation of Cantor's theorem proof . I must have gone wrong or misunderstood something somewhere so I will try to explain my reasoning below. Theorem (Cantor) Let $f$ be a map from set $A$ to its power set $\mathcal{P}(A)$ .  Then $f : A \to \mathcal{P}(A)$ is not surjective.  As a consequence, $\operatorname{card}(A) < \operatorname{card}(\mathcal{P}(A))$ holds for any set $A$ The theorem is straightforward enough, we need to show that $f$ is not surjective, meaning that there is some element $y \in \mathcal{P}(A)$ such that there is no $x \in A$ such that $y = f(x)$ . The proof is where I run into trouble. Proof Consider the set $ B=\{x \in A \mid x \notin f(x)\}$ .  Suppose to the contrary that $f$ is surjective.  Then there exists $\xi\in A$ such that $f(\xi)=B$ .  But by construction, $\xi \in B \iff \xi \notin f(\xi)= B $ .  This is a contradiction.  Thus, $f$ cannot be surjective.  On the other hand, $g : A \to \mathcal{P}(A)$ defined by $x \mapsto \{x\}$ is an injective map.  Consequently, we must have $\operatorname{card}(A) < \operatorname{card}(\mathcal{P}(A))$ . $\tag*{$\blacksquare$}$ I do not understand the third sentence in the proof. It says that ""if $f$ is surjective, then there exists some $\xi \in A$ such that $f(\xi) = B$ "". I do not see why such a $\xi$ must exist if $f$ is surjective. because consider the injective/surjective mapping of $f: A \mapsto C$ where $f$ is the identity function and $A$ and $C$ both contain the same single element. There exists no $\xi \in A$ such that $f(\xi)$ = B and $f$ is surjective. Have I misunderstood something? How can I rectify my above statements with the logic of the proof?","['elementary-set-theory', 'set-theory']"
4482071,"For real-valued continuous $f(x)$, $g(x)$, when does $h(f(x))$ = $g(x)$ exist such that h is continuous?","Let $f(x)$ and $g(x)$ be real-valued, continuous, differentiable functions $\mathbb{R}\to\mathbb{R}$ . Under what conditions does there exist a continuous and differentiable $h$ such that $h(f(x)) = g(x)$ ? For continuity, a sketch of an argument might be; By continuity we have that for $|x-x_0| < \delta$ , $|f(x) - f(x_0)| < \epsilon_f$ and $|g(x) - g(x_0)| < \epsilon_g$ . Then there exists continuous $h$ since for: $|f(x) - f(x_0)| < \epsilon_f$ , $|h(f(x)) - h(f(x_0))| = |g(x) - g(x_0)| < \epsilon_g$ . What about differentiability?","['continuity', 'functions', 'real-analysis']"
4482076,"If $f$ is holomorphic and injective, then $f'$ is non zero","I know there are already many posts about this question, and I've read and understood the proof in Stein and Shakarchi, but I don't understand why my “shortcut” is wrong. Assume $f$ is injective and holomophic in $\Omega$ open set and $f'(z_0) = 0$ for a $z_0 \in \Omega$ . Consider $g(z) = f(z_0+z)-f(z_0)$ , this way $g(0) = g'(0) = 0$ and $g$ is holomorphic and injective in $\Omega-z_0$ . Now, since $g$ and $g'$ are analytic and non-constant, $0$ is an isolated zero for both of them, that is, there exists a disc $D \subseteq \Omega-z_0$ around $0$ such that $g$ and $g'$ have no zeroes in $D \setminus \{0\}$ . Take $w$ such that $0 < |w| < \min_{\partial D} |g|$ , then for Rouché's Theorem $g$ and $g-w$ have the same number of zeroes (counting the multiplicity) in $\mathring{D}$ . Since $0$ is a zero of multiplicity at least $2$ for $g$ , $g-w = 0$ has at least $2$ solutions in $\mathring{D} \setminus \{0\}$ which are different, because otherwise $g'$ would have to be $0$ in that point, leading to a contradiction.","['complex-analysis', 'solution-verification']"
4482094,Property of Krawtchouk polynomials involving $K_s(x)$ and $K_s(x+y)$,"Fix $n$ and consider the Krawtchouk polynomials $$
K_s(\ell) = \sum_{k=0}^s(-1)^k\binom{\ell}{k}\binom{n-\ell}{s-k} \quad \text{defined for} \quad 0 \leq s \leq n
$$ Are you aware of any property linking the evaluations in some points $x$ and $x+y$ , i.e. between $K_s(x)$ and $K_s(x+y)$ ?","['polynomials', 'discrete-mathematics']"
