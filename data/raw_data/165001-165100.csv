question_id,title,body,tags
2868266,Leibniz rule for double integral,"I'm attempting to differentiate the following double integral with respect to $u$: $$I(u) = \int_a^u\int_b^v [(y-u) + (v - x)] f(x,y)\,dx \,dy$$ where $f(x,y)$ is the joint density function of RV $X$ and $Y$. I'm trying to find $I'(u)$, that is, the derivative of $I$ with respect to $u$. I know this involves Leibniz integral rule , however I'm getting stuck on the double integral part. If I let $g(x,y,u) = \int_b^v [(y-u) + (v - x)] f(x,y)\,dx$ then I think the problem to solve is: $$\frac{d}{du}\bigg(\int_a^ug(u,x,y)\,dy\bigg) = g(u,x,u) + \int_a^u\frac{\partial}{\partial u}g(u,x,y)\,dy$$ However I'm getting stuck evaluating the two expressions on the RHS. Any help?","['integration', 'multivariable-calculus', 'calculus', 'derivatives']"
2868273,Why is the rate function given by Schilder's theorem infinite outside of CM space? Can we understand Schilder's theorem through CM theorem?,"Schilder's theorem from large deviations theory tells us that scaled Brownian motion $\sqrt{\varepsilon} W_t$ on Wiener space $C_0([0,T],\Bbb R^d)$ satisfies a large deviation principle with good rate function: $$I(\omega)=\begin{cases}\frac{1}{2}\int_0^T |\dot{\omega}(t)|^2~dt &\text{ if } \omega\in \mathcal H\\
\infty &\text{ if } \omega\not\in\mathcal H\end{cases}$$ Where $\mathcal H$ is the Cameron Martin space of Brownian motion (see also this ). This seems to be too coincidental. Why do we only care about the paths that are in the Cameron Martin space? Cameron martin theorem tells us that the law of Brownian motion is quasi invariant under translations by Cameron Martin directions. This theorem seems to tell us that paths outside the CM space decay very quickly in probability. What is the connection? Is there anyway of understanding Schilder's theorem by CM theorem?","['stochastic-processes', 'large-deviation-theory', 'probability-theory']"
2868299,"Evaluating $\lim_{n\to\infty}\frac1{n}\int_{0}^{\pi /2}{\frac{\sin^2nx}{\sin^2 x}f(x)\;dx}$, for continuous $f$","Let $f:\left[ 0,{\pi }/{2}\; \right]\to \mathbb{R}$ be a continuous function. What is the value of the following limit? $$\underset{n\to \infty }{\mathop{\lim }}\,\frac{1}{n}\int_{0}^{{\pi }/{2}\;}{\frac{{{\sin }^{2}}\left( nx \right)}{{{\sin }^{2}}\left( x \right)}f\left( x \right)dx}$$ I strongly believe that the solution depends on this integral $$\int_{0}^{{\pi }/{2}\;}{\frac{{{\sin }^{2}}\left( nx \right)}{{{\sin }^{2}}\left( x \right)}dx}=\frac{n\pi }{2}$$
and using Weierstrass Approximation Theorem .","['integration', 'limits', 'trigonometry', 'real-analysis']"
2868305,Concerning the existence of a divergent monotone sequence with a Cauchy subsequence.,"Is my argument correct? Proposition. There is no divergent monotone sequence with a Cauchy subsequence. Proof. Assume that we have a divergent monotone sequence $(a_n)$ with a Cauchy sequence $(a_{n_k})$, from theorem $\textbf{2.6.3}$ we know that there exists an $M>0$ such that $|a_{n_k}|<M,\forall k\in\mathbf{N}$. We now examine the case where $(a_n)$ is increasing, the case where $(a_n)$ is decreasing will be handled similarly. Let $r\in\mathbf{N}$, evidently $r\leq n_r$ but then $a_r\leq a_{n_r}$ and since $a_{n_r}\leq |a_{n_r}|\leq M$ consequently $a_r\leq M$, implying that $(a_n)$ is bounded, but $(a_n)$ cannot be bounded since this together with $(a_n)$ being increasing would imply $(a_n)$ is convergent by theorem $\textbf{2.4.1}$. $\blacksquare$ Note: $\textbf{2.6.3}$ All Cauchy sequences are bounded. $\textbf{2.4.1}$ All bounded monotone sequeces are convergent.","['proof-verification', 'sequences-and-series', 'cauchy-sequences', 'real-analysis']"
2868337,How do I linearise the rational function to analyze the critical points?,"For the system $$\frac{dx}{dt}=\frac{3xy}{1+x^2+y^2}-\frac{1+x^2}{1+y^2}\\\frac{dy}{dt}=x^2-y^2,$$ the point $\begin{pmatrix}1\\1\end{pmatrix}$ is A. an unstable node B. a stable node C. a saddle point D. a stable spiral point E. an unstable spiral point. I understand I need to shift the point to the origin and just eliminate $x^2$ and $y^2$ , but how do I deal with denominators? Thank you","['calculus', 'linearization', 'ordinary-differential-equations']"
2868342,Degree of a map $f:S^1 \to S^1$ given a polynomial $p$,"I'm preparing for some comprehensive exams and this is a question from a previous year that I think I'm close to solving but some details may need to be filled in. Any help would be great. ""Let $p:\mathbb{C} \to \mathbb{C}$ be a polynomial with simple roots, none of which lie on the unit circle $S^1$. Show that the number of roots inside the unit disk $D$ is the degree of the map $f:S^1 \to S^1$ defined by $f(e^{i \theta}) = \frac{p(e^{i \theta})}{|p(e^{i \theta})|}$."" First, let $p, q$ be two polynomials satisfying the above. If $f_p (e^{i \theta}) = \frac{p(e^{i \theta})}{|p(e^{i \theta})|}$ and $f_q(e^{i \theta}) = \frac{q(e^{i \theta})}{|q(e^{i \theta})|}$, observe that $f_{pq}$ defined as just taking the polynomial $pq = p(z)q(z)$ (multiplication, not composition) and churning out a function $f$ as we have been doing, then $f_{pq} = f_p \cdot f_q$ (it is multiplicative). Then, 
$$\frac{f'_{pq}}{f_{pq}} = \frac{d}{d \theta}(\log f_{pq}) = \frac{d}{d \theta}(\log f_{p}) + \frac{d}{d \theta}(\log f_{q}) =\frac{f'_p}{f_p}+\frac{f'_q}{f_q}.$$ Now, these $f$ functions are maps from $S^1 \to S^1$ so they're loops in $S^1$; thus, $[f] \in \pi_1(S^1) = \mathbb{Z}$. So we can consider degree of these $f$ by considering their winding numbers. The winding number of $f$ can be given as an integral: $$\deg  f = \int^{2 \pi}_0 \frac{f'}{f} \, d \theta = \text{winding number of $f$ around 0.}$$ From the above, we have then that $\deg f_{pq} = \deg f_p + \deg f_q$. So now just consider one polynomial $p(z) = \prod^n (z-a_i)$. Setting $p_i = z-a_i$, we have that $\deg f = \sum^n \deg f_{p_i}$. We want to show that if $a_i \notin D$, then $\deg f_{p_i} = 0$. WLOG, we can suppose that $a_i \in \mathbb{R}^+$ (just rotate the picture). Draw a vector from $0$ to $z \in S^1$ and a vector from $0$ to $a_i$. Then the vector $z-a_i$ makes an obtuse angle with the real axis; this means that $z-a_i$ lies somewhere in the left half plane and when normalized, is on the left semicircle. Thus, $f_{p_i}$ is not surjective as everything on the right semicircle is missed. So $\deg f_{p_i} = 0$. On the other hand, if $a_i \in D$, then we can use similar simple geometry as above to show that for each point on $S^1$, there is a unique point mapping to it. Just choose your point $w \in S^1$ and draw its vector. Then translate that vector to $a_i$ and extend it till it hits $S^1$. Where it intersects $S^1$ is the point $z$ which maps to $w$. Thus, $f_{p_i}$ in this case is bijective and hence, the winding number must be $1 = \deg f_{p_i}$ (all other positive degrees = other positive winding numbers imply a non-injective map $f$). Therefore, if there are $k$ roots of $p$ inside $D$, $\deg f = k$. Issues that I would like help resolving: $\log$ isn't well-defined though I think this isn't a problem when we consider its derivative. The last part about $f_{p_i}$ being bijective when $a_i \in D$ doesn't seem airtight to me and could be wrong. However, when picturing the map, I think I can visualize a clear homotopy between $f$ and $g(z)=z$. I'm not really familiar with this definition for degree on $S^1$; I was told that it's equivalent and the bit about $[f] \in \pi_1(S^1)$ convinces me that it's true but I couldn't justify it that rigorously from the definition about regular values or differential forms.","['complex-analysis', 'differential-topology', 'differential-geometry']"
2868349,Find a set $X$ such that $X\cap P(X)\neq\emptyset$,"Find a set $X$ such that $X\cap P(X)\neq\emptyset$ My answer is: Take $X=\left\{ \emptyset \right\}$. Then, $\left\{ \emptyset \right\}\cap P(\left\{ \emptyset \right\})=\left\{ \emptyset \right\}\neq \emptyset$. So we are done. Can you check my answer and proof-writing?","['elementary-set-theory', 'proof-writing']"
2868357,ODE problem reading do Carmo's book of Riemannian geometry,"I'm reading do Carmo's book, Riemannian geometry. I have a problem at the Jacobi fields. He talks about the case of constant curvature. He gets to the ODE above and my problem is how dose he solve it? Can some one fill in the details? Thanks a lot! As a result, the Jacobi equation can be written as $$\frac{D^2J}{dt^2}+KJ~=~0$$ Let $\omega(t)$ be a parallel field along $\gamma$ with $\langle\gamma'(t),\omega(t)\rangle=0$ and $|\omega(t)|=1$ . It is easy to verify that $$J(t)~=~\begin{cases}\frac{\sin(t\sqrt{K})}{\sqrt{K}}\omega(t),~~~&\text{if}~K>0\\t\omega(t),~~~&\text{if}~K=0\\\frac{\sinh(t\sqrt{-K})}{\sqrt{-K}}\omega(t),~~~&\text{if}~K<0\end{cases}$$ is a solution of $(2)$ with initial conditions $J(0)=0,J'(0)=\omega(0)$","['riemannian-geometry', 'ordinary-differential-equations', 'partial-differential-equations', 'derivatives', 'differential-geometry']"
2868358,A generalization of the airplane seating puzzle,"Let me say immediately that this isn't my puzzle.  Someone posted it earlier, and I was working on it when it was deleted.  It seems to me to be an excellent puzzle, too good be deleted, so I'm reposting it.  If there's a good reason why I should delete this puzzle, please tell me. In some world, everyone has $k\geq1$ feet.  Everyone wears $k$ identical socks, but the socks vary from person.  Each person can easily identify his own socks.  When the people go to worship, they remove their socks and place them in a communal pile.  At the close of the service, each person removes his socks from the pile. One day, the first person to leave is in a hurry, and grabs $k$ socks uniformly at random.  After that, each person removes all of his own socks from the pile, and if any are missing, he randomly picks just enough so that he will have one for each foot. What is the probability that the last person to leave will find exactly $j$ of his own socks in the pile, for $0\leq j \leq k?$  (When $k=1,$ this is the airline seating puzzle.) I've done some experimenting by computer simulation for small $n$ and $k,$ and the results lead me to believe that for given $k,$ the answer is independent of the number of people $n\geq2.$  Of course, when $n=2$ the puzzle is trivial, so I guess that the answer is $$
{{k\choose k-j}{k\choose j}\over{2k\choose k}}, 0\leq j\leq k
$$
I don't have a clue how to prove this, though.  Any ideas?","['puzzle', 'probability']"
2868377,Hessian matrix vs differential 2-form,"Could someone clarify the convention that the second derivative of a scalar function $f: \Bbb R^n \rightarrow \Bbb R$ is sometimes defined as a linear operator $D^2f : \Bbb R^n \rightarrow L(\Bbb R^n, L(\Bbb R^n, \Bbb R))$, which is also identified with the Hessian matrix (filled with second order partial derivatives), and sometimes as a differential 2-form? It seems that both happens in vector calculus. Is the definition of derivative just context dependent, and the first example is just a case of ""standard derivative"" and the second one of the ""exterior derivative""? Or am I really confused about something?","['differential-forms', 'multivariable-calculus', 'hessian-matrix', 'vector-analysis']"
2868407,"What is the definition of ""canonical""? [duplicate]","This question already has answers here : What do people mean by ""canonical""? (4 answers) Closed 5 years ago . I've seen it first while studying about dual spaces. A canonical isomorphism between a vector space and its double dual. I've been searching for a while in a little bit more advanced-topic books likes algebraic structures, category theory and representation theory for a formal definition. They are using a LOT the word ""canonical"", but I couldn't see any record of it in any of those books. How come it is so ""hard"" for the writer to define the word he used so much? I mean, you probably can find any definition you want in a book just by finding the first time it's been used - this will probably be the definition. Why is it so comfortable to use the word ""canonical"" so much without any formal definition?","['abstract-algebra', 'representation-theory', 'category-theory']"
2868451,Nested Interval Property and the intersection of infinite sequences,"Given sequences such that $A_n = \{n,n+1,\cdots \}$, then it can be shown that $\bigcap\limits_{n=1}^\infty A_n = \emptyset$ Now, according to nested interval property if $\mathbf{I}_n = [a_b,b_n] = \{x \in \Re: a_n\leq x\leq b_n\}$, then $\bigcap\limits_{n=1}^\infty \mathbf{I}_n \neq \emptyset$ The above two statements looks very similar, but the results are just opposite. From what I can see is, $A_n$ is a countable infinite set whereas $\mathbf{I}_n$ is an uncountable infinite set.  Is that the only difference, if it is true ?. Or is there anything more than that which relates two statements above ?","['elementary-set-theory', 'real-analysis']"
2868467,Solving this Fokker-Plack Equation,"I have a bidimensional Fokker-Planck equation to solve, namely, $$
\frac{\partial p}{\partial t} =  \frac{\sigma_{1}^{2}}{2} \frac{\partial^{2} p}{\partial I^{2}} - \frac{\partial }{\partial S} (b_{1}(S,I) p) - \frac{\partial }{\partial I}(b_{2} (S,I) p) \,\, ,
$$ where, $$
b_{1} = -aSI \,\, , \\
b_{2} = aSI-\mu I + cI -cSI-cI^2 \,\, .
$$ My initial and Neumann conditions are the following, $$
p(S,I,0) = \delta (S-S_{0},I-I_{0})  \,\, , \\
\lim_{S,I \to \pm \infty} p(S,I,t)= \lim_{S,I \to \pm \infty} \partial_{S} p(S,I,t) = \partial_{I} p(S,I,t) = 0 \,\, .
$$ How can I solve this FP equation? Can I apply separation of variables?","['linear-pde', 'ordinary-differential-equations', 'partial-differential-equations']"
2868487,Hartshorne Exercise II. 1.18,"Exercise 1.18 of chapter II asks to show that given a map of topological spaces $f:X\to Y$, the functors $f_* : \mathsf{Sh}_Y \longrightarrow\mathsf{Sh}_X$ and $ f^{-1}: \mathsf{Sh}_X\longrightarrow \mathsf{Sh}_Y$ are adjoint. The suggestion is to define the unit and the counit and then check they satisfy the usual equations. Now I have defined the unit and counit, and want to check that if $\mathscr  G$ is a sheaf on $Y$ then $$\varepsilon_{f^{-1}\mathscr G}\circ f^{-1}(\eta_\mathscr G) : f^{-1}\mathscr G\to f^{-1}f_*f^{-1}\mathscr G\to f^{-1}\mathscr G$$ is the identity, and similarly for sheaves on $X$. This is a bit cumbersome, and the only idea I have to bypass this is perhaps to look at the maps on stalks, which I think are easily seen to be the identity because of how $f_*$, $f^{-1}$ and the unit and counit are defined. Does this suffice? If not, what is the ""right"" way to proceed here?","['adjoint-functors', 'algebraic-geometry', 'sheaf-theory']"
2868588,Two points no matter how you choose from the six points in the unit disk are at distance at most 1?,"Six points are to be chosen in a unit disk ($x^2 +y^2 \leq 1$) , such that distance between any two points is greater than 1? I am unable to, I think I want to prove formally that no matter how the six points are chosen, there are two points with distance at most 1 from each other. For seven points, I think pigeonhole principle could be used to prove there is no such arrangement. We can easily choose seven points such that maximum distance between any two points is equal to 1. Something like this in the figure. Now you can't increase the distance between the points(orange) chosen on the circle as the side length itself is one.","['extremal-combinatorics', 'discrete-geometry', 'geometry']"
2868705,An open cover of regular surface,"$\quad$$\quad$Let a subset $S\subseteq \mathbb{R}^{3}$ is a regular surface .(that is to say,for each $\mathbb {x}\in S$,there exists an open set $U\subseteq \mathbb{R}^{2},$ and open neighborhood $V$ of $\mathbb {x}\in \mathbb{R}^{3}$,and a surjective continuous function $\varphi:U\rightarrow V\cap S$ such that $\textit {1.} \varphi$ is continuously differentiable;   $\textit {2.} \varphi$  is  a homeomorphism;$\textit {3.}$ For each $(u,v)\in U$, the differential $d\varphi_{(u,v)}:\mathbb{R}^{2}\rightarrow \mathbb{R}^{3}$ is a one-to-one linear transformation.)$\quad$ $\quad$$\quad$Define $\partial S=\overline{S}\backslash S$ (We can think of $\partial S$ as the boundary of the regular surface $S$). Let$K_{n}=\{\mathbf{x}\in S:|\mathbf{x}|\leq n,dist(\mathbf{x},\partial S)\geq \frac{1}{n}\},$ here $dist(\mathbf{x},\partial S)=\inf\{|\mathbf{x}-\mathbf{y}|:\mathbf{y}\in \partial S\}.$Then$S=\bigcup_{n=1}^{\infty}K_{n},$and that each $K_{n}$ is closed and bounded in $\mathbb{R}^{3}.$$\qquad$$\qquad$ $\quad$$\quad$Since $S$ is a regular surface, choose for every $\mathbf{x}\in S$ an open set $U_{\mathbf{x}}\subset \mathbb{R}^{2},$ an open subset$V_{\mathbf{x}}\subset S,$ and a $C^{1}$ map $\varphi_{\mathbf{x}}:U_{\mathbf{x}}\rightarrow V_{\mathbf{x}}$ that is injective with injective derivative.Choose $B_{\mathbf{x}}\subset U_{\mathbf{x}}$ an open ball with $\overline {B_{\mathbf{x}}}\subset U_{\mathbf{x}}$ and set $V^{'}_{\mathbf{x}}=\varphi_{\mathbf{x}}(B_{\mathbf{x}}).$$\quad$$\quad$ $\quad$$\quad$Show that we can choose a sequence $\mathbf{x}_{1},\mathbf{x}_{2},\cdots$ and integers $p_{1} \leq p_{2}\leq\cdots$ such that $\bigcup_{i=1}^{p_{n}}V^{'}_{\mathbf{x}_{i}}$ is an open cover of $K_{n}$,and that we can choose the $U_{\mathbf {x}_{i}}$ all disjoint, and such that only finitely many intersect any ball. $\quad$$\quad$ I'm really puzzled that words $\textit{""and that we can choose the $U_{\mathbf {x}_{i}}$ all disjoint,}$
$\textit{and such that only finitely many intersect any ball.""}$ Actually,it took me lots of time to prove it.Until now, there is no progress at all. What role does it play? How to deal with this problem? Your useful help will be greatly appreciated！","['manifolds', 'general-topology', 'smooth-manifolds', 'differential-geometry']"
2868712,Are branch cuts always ‘cancellable’?,"Many functions can be analytically continued to $\mathbb C$ except the branch cut. However, it appears to me that for every function $f(z)$ that has a branch cut, there always exists a non-constant meromorphic/entire function $g(z)$ such that $f(g(z))$ can be analytically continued to the whole $\mathbb C$. For example,
$$\sqrt {x^2}=x$$
$$\ln e^x=x$$
$$\arccos\cos x =x$$
$$\ln\ln e^{e^x}=x$$
$$\operatorname{W}(xe^x)=x$$
More complicated examples:
$$f(x)=\sqrt{(x+1)(x+3)}=\sqrt{(x+2)^2-1}$$
$$g(x)=-2+\cosh x$$
$$f(x)=x^\alpha\qquad{\alpha\in\mathbb C}$$
$$g(x)=e^x$$ Is this true?",['complex-analysis']
2868721,How to simplify $\int{\sqrt[4]{1-8{{x}^{2}}+8{{x}^{4}}-4x\sqrt{{{x}^{2}}-1}+8{{x}^{3}}\sqrt{{{x}^{2}}-1}}dx}$?,"I have been asked about the following integral: $$\int{\sqrt[4]{1-8{{x}^{2}}+8{{x}^{4}}-4x\sqrt{{{x}^{2}}-1}+8{{x}^{3}}\sqrt{{{x}^{2}}-1}}dx}$$ I think this is a joke of bad taste . I have tried every elementary method of integration which i know, also i tried integrating using Maple but as i suspected, the integrad doesn't have an anti derivative. Any ideas?","['integration', 'nested-radicals', 'real-analysis', 'indefinite-integrals', 'radicals']"
2868745,"If $f$ is proper, lsc, and $\frac{f(x) + f(y)}{2} = f^{**}\left(\frac{x + y}{2}\right) \implies x = y$, is $f$ necessarily convex?","Suppose $X$ is a real Hilbert Space and $f : X \to (-\infty, \infty]$ is a lower semicontinuous, proper function. Further, suppose $f$ satisfies the following, for all $x, y \in \operatorname{dom} f$:
  $$\frac{f(x) + f(y)}{2} = f^{**}\left(\frac{x + y}{2}\right) \implies x = y.$$
  Is $f$ necessarily a convex function? Here $^*$ refers to the Fenchel conjugate, and $\operatorname{dom} f$ is the set of points $x \in X$ such that $f(x) \neq \infty$. I know that: $f^{**}(x) \le f(x)$ for all $x$ and $f^{**}(x) = f(x)$ for all $x$ if and only if $f$ is convex (and lsc). In fact, $f^{**}$ is the greatest lsc convex minorant of $f$. This means that
$$\frac{f(x) + f(y)}{2} \ge \frac{f^{**}(x) + f^{**}(y)}{2} \ge  f^{**}\left(\frac{x + y}{2}\right)$$
for all $x, y \in \operatorname{dom} f$. Therefore,
$$\frac{f(x) + f(y)}{2} = f^{**}\left(\frac{x + y}{2}\right)$$
implies that $f(x) = f^{**}(x)$ and $f(y) = f^{**}(y)$. Another consequence is that $f^{**}(\lambda x + (1 - \lambda y)) = \lambda f^{**}(x) + (1 - \lambda)f^{**}(y)$ for all $\lambda \in [0, 1]$. My thoughts: Really, I just need to establish that $f^{**}(x) = f(x)$ for all $x$. Despite biduals showing up both in the premises and the above desired conclusion, there doesn't seem to be a direct path to manipulate one to the other, especially since not every point in $\overline{\operatorname{conv}} \operatorname{dom} f$ can be expressed as $\frac{x+y}{2}$ where $x, y \in \operatorname{dom} f$. The function $g(x, y) = \frac{f(x)+f(y)}{2} - f^{**}\left(\frac{x + y}{2}\right)$ is not a metric in general, even if $g(x, y) = 0 \implies x = y$. I get a feeling that Stegall's variational principle might help, for a variety of reasons, but one handy reason is that we may add any linear functional to $f$, without changing $g$. Any thoughts are welcome!","['variational-analysis', 'convex-analysis', 'functional-analysis']"
2868753,"Prove that $x\,\int_a^x\,\frac{1}{s^2}\,\int_a^s\,t\,f(t)\,\text{d}t\,\text{d}s - \int_a^x\,\int_a^s\,f(t)\,\text{d}t\,\text{d}s$ is linear in $x$.","Let $a$ be a positive real number and $f:[a,\infty)\to\mathbb{C}$ a continuous function.  Prove that the function $g:[a,\infty)\to\mathbb{R}$ defined by
  $$g(x):=x\,\int_a^x\,\frac{1}{s^2}\,\int_a^s\,t\,f(t)\,\text{d}t\,\text{d}s - \int_a^x\,\int_a^s\,f(t)\,\text{d}t\,\text{d}s\text{ for each }x\in[a,\infty)$$
  is a linear function. I wonder if there is a solution involving only directly tackling the integrals (e.g., with integration by parts and/or with swapping the order of the integrals), without differentiating $g$.  My solution is to differentiate $g$ twice and show that $g''\equiv0$.  If $p$ and $q$ are constants such that $$g(x)=p(x-a)+q\text{ for all }x\geq a\,,$$ then it is not difficult to see that $q=0$.  It may be good to find out how the $p$ depends on $f$ and $a$. Spoiler: The constant $p$ does not depend on $f$ and $a$---it is just $0$.  Below is my solution. Using the Leibniz Integral Rule, we obtain $$g'(x)=\int_a^x\,\frac{1}{s^2}\,\int_a^s\,t\,f(t)\,\text{d}t\,\text{d}s+\frac{1}{x}\,\int_a^x\,t\,f(t)\,\text{d}t-\int_a^x\,f(t)\,\text{d}t\,.$$  Thus, applying the Leibniz Integral Rule again yields $$g''(x)=\frac{1}{x^2}\,\int_a^x\,t\,f(t)\,\text{d}t-\frac{1}{x^2}\,\int_a^x\,t\,f(t)\,\text{d}t+f(x)-f(x)=0\,.$$  The result follows. In fact, from this calculation, $g'(a)=0$.  Therefore, $p=0$ as well.  Hence, $g\equiv 0$.  Consequently, we indeed have that $$x\,\int_a^x\,\frac{1}{s^2}\,\int_a^s\,t\,f(t)\,\text{d}t\,\text{d}s = \int_a^x\,\int_a^s\,f(t)\,\text{d}t\,\text{d}s\text{ for every }x\in[a,\infty)\,.$$","['integration', 'definite-integrals', 'analysis', 'alternative-proof', 'calculus']"
2868765,"Vector space with x+y=xy, cx=x^c","If $x+y=xy$ and $cx=x^c$ ($c$ is a real number) where $x,y$ posivitive. I want to prove that this can be a vector space for all $x,y>0$. Isn't there a problem with the axiom that states ""For every $v \in V$, there exists an element $−v \in V$, called the additive inverse of $v$, such that $v + (−v) = 0$"" because by the way that addition is defined $x+y=xy$ for every positive number x theatre element will be zero so that $x+0=x0=0$. Except if the ""zero"" is actually $1$ because $x+1=x$ and the additive invent will be $1/x$. 
$x+1/x=1$. Can somebody clear things a bit?","['linear-algebra', 'vector-spaces']"
2868769,How to prove that $x^2+1=5^y$ has no positive integer solutions for $y\geq 2$? [duplicate],"This question already has answers here : On equations $m^2+1=5^n$ (7 answers) Closed 5 years ago . I am sure I saw a similar question like this one before but I can't find it now. I tried using ""order"" but failed. It is quite obvious when $y$ is an even number. The real problem is when $y$ is odd. Is there any easy way to solve this?
Thanks.","['number-theory', 'diophantine-equations']"
2868778,Why is a connection on the bundle $SO(M)$ metric compatible?,"If we have an orientable manifold $M$ with a metric $g$ and signature $(r, s)$, we can define the principal-$SO(r, s)$ bundle $SO(M)$, the bundle of orthonormal frames of $TM$. This is a subset of the principal-$GL(n, \mathbb{R})$ bundle $Fr(M)$, the frame bundle of $M$. As a Koszul connection $\nabla$, any such connection is metric compatible. That is, for all $X, Y, Z \in \mathrm{Vect}(M)$, we have $$X g(Y, Z) = g(\nabla_X Y, Z) + g(Y, \nabla_Z X).$$ My question is this: how can we prove that all connections on the bundle $SO(M)$ are metric compatible?","['riemannian-geometry', 'principal-bundles', 'connections', 'tangent-bundle', 'differential-geometry']"
2868805,Subgroups of $\text{Spin}(7)$.,"I am interested in subgroups of $\text{Spin}(7)$ and identifying certain elementary properties that they have. Specifically relating to commutativity. Let me apologise at this point for my ignorance and sloppy presentation below. I know little about this subject, which has recently come up in my work. I write down the extended Dynkin diagram $\widetilde B_3$ corresponding to the Lie algebra of $\text{Spin}(7)$ with appended long root $\alpha_0$ , and short root $\alpha_3$ . Removing the short node $\alpha_3$ I get $A_3$ coresponding to the $SU(4)\cong \text{Spin}(6)$ subgroup. Cutting out $\alpha_1$ leaves something with a fold symmetry that gives the $G_2\leq \text{Spin}(7)$ subgroup inclusion. On the other hand, if we take out $\alpha_2$ then we are left with three $A_1$ 's. It's pretty clear that the pair given by $\alpha_0$ , $\alpha_1$ are the two commuting $\text{Spin}(3)$ -subgroups that together constitute the canonical $\text{Spin}(4)\leq \text{Spin}(7)$ . The final $A_1$ corresponds to the short root $\alpha_3$ , and is what is confusing me. My intuition is telling me that it is a third copy of $\text{Spin}(3)$ , covering the $SO(3)\leq SO(7)$ which is complementary to the $SO(4)$ subgroup. As such this third $A_1$ should be another $\text{Spin}(3)$ which commutes with the $\text{Spin}(4)$ identified previously. However, my understanding runs out here, and the presence of the half-length root indicates to me that what I have actually identified is in fact an $(S^3\times S^3\times S^3)/\mathbb{Z}_2$ subgroup containing a $\text{Spin}(4)$ subgroup which does not commute with the other factor. Have I identified a commuting $\text{Spin}(3)$ and $\text{Spin}(4)$ subgroups, or is it rather the second option $(S^3\times S^3\times S^3)/\mathbb{Z}_2$ ? Moreover, the diagram indicates to me the presence of three distinguished homotopy classes of maps $S^3\rightarrow \text{Spin}(7)$ . The first two come from $\text{Spin}(4)$ , with $\alpha_0$ corresponding to the canonical generator $i_0:S^3\cong \text{Spin}(3)\hookrightarrow \text{Spin}(7)$ , and $\alpha_1$ its negative, $i_1=-i_0$ . Since $\frac{|\alpha_0|^2}{|\alpha_3|^2}=2$ , the corresponding homotopy class of $\alpha_3$ should be $i_3=2\cdot i_0$ , which would follow from elementary properties of the Dynkin index. I'm not entirely sure this is what I want to see. Am I correct about the identifications here?","['group-theory', 'lie-algebras', 'lie-groups']"
2868824,Calculating the series $1/8+1/88+1/888+....$,I wonder whether this series is calculable or not. Attempt: $S=1/8+1/88+1/888+....=\dfrac18\displaystyle\sum_{k=0}^\infty\dfrac{1}{\sum_{n=0}^k10^n}$ where $$\displaystyle\sum_{n=0}^k10^n=\dfrac{10^{k+1}-1}{9}$$ then $S=\dfrac98\displaystyle\sum_{k=0}^\infty\dfrac{1}{10^{k+1}-1}$ I have tried to calculate $\displaystyle\sum_{k=0}^K\dfrac{1}{10^{k+1}-1}$ for finite values but I failed. What methods can we try?,['sequences-and-series']
2868906,"In $\triangle ABC$, angles $B$ and $C$ satisfy $2\tan x - k ( 1 + \tan 2 x)=0$; what is $A$? [closed]","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question In a $\triangle ABC$, $\angle B <  \angle C$ and values of $B$ and $C$ satisfy equation $2 \tan x – k (1 + \tan ^2x) = 0$ where $(0 \lt k \lt 1)$ . Then measure of angle $A$ is? Note that $\tan B$ and $\tan C$ are its roots , So 
$$\text{sum of roots} = \dfrac2k$$
and
$$\text{product of roots} = 1\,.$$ 
Then , 
$$\tan ( B + C) = \tan ( \pi - A)\,$$
Expanding $\tan( B + C)$, we found it not defined.  The tangent is not defined at when the argument is $\dfrac{\pi}{2}$. 
So, $$\tan (\pi - A) = \tan\left(\frac{\pi}{2}\right)\text{ and }\angle A=\frac{\pi}{2} $$","['triangles', 'algebra-precalculus', 'trigonometry']"
2868927,On the proof of existence of algebraic closure using Zorn's Lemma in Patrick Morandi's *Field and Galois Theory*,"I am studying from Patrick Morandi's Field and Galois Theory , and on page 32 he proves the existence of an algebraic closure of an arbitrary field, as follows: Lemma 3.13. If $K/F$ is algebraic, then $|K| \leq \max\{ |F|,|\mathbb{N}| \}$ . Theorem 3.14. Let $F$ be a field. Then $F$ has an algebraic closure. Proof. Let $S$ be a set containing $F$ with $|S| > \max\{ |F|, |\mathbb{N}| \}$ . Let $\mathcal{A}$ be the set of all algebraic extension fields of $F$ inside $S$ . Then $\mathcal{A}$ is ordered by defining $K \leq L$ if $L$ is an extension field of $K$ . By Zorn's lemma, there is a maximal element $M$ of $\mathcal{A}$ . We claim that $M$ is an algebraic closure of $F$ . To show that $M$ is algebraically closed, let $L$ be an algebraic extension of $M$ . By Lemma 3.13, $$
|L| \leq \max\{ |M|, |\mathbb{N}| \} \leq \max\{ |F|,|\mathbb{N}| \} < |S|.
$$ Thus, there is a function $f:L \to S$ with $f|_M = \operatorname{id}$ . By defining $+$ and $\cdot$ on $f(L)$ by $f(a) + f(b) = f(a+b)$ and $f(a) \cdot f(b) = f(ab)$ , we see that $f(L)$ is a field extension of $M$ and $f$ is a field homomorphism. Maximality of $M$ shows that $f(L) = M$ . Thus, $M$ is algebraically closed. Since $M$ is algebraic over $F$ , we see that $M$ is an algebraic closure of $F$ . $\tag*{$\blacksquare$}$ I do not understand the second line in the proof, where he says Let $\mathcal{A}$ be the set of all algebraic extension fields of $F$ inside $S$ . Since we are assuming $S$ is just a set containing $K$ and so it does not have any field structure defined on it a priori , how can we talk about field extensions of $F$ contained in $S$ ? The rough idea that I am currently keeping in mind is that we take an algebraic extension $K$ of $F$ and then use a bijection of this set onto a subset of $S$ to define the addition and multiplication on this subset, in the same way we do later on for $L$ . And then we possibly do this process for every algebraic extension of $K$ . But then what I don't see at all is how to ensure that the different mappings are compatible with each other. And, if I have two isomorphic algebraic extensions of $K$ , do I then map it to the same subset of $S$ or something? It would be really helpful if someone could guide me in filling out the details here.","['cardinals', 'field-theory', 'abstract-algebra', 'elementary-set-theory', 'extension-field']"
2868936,A synthetic geometrical proof required on a result regarding Isosceles triangles.,"I need a synthetic proof on this problem without the use of trigonometry. Question: Let $ABC$ be a triangle with $AB=AC$. If $D$ is the midpoint of $BC$, $E$ the foot of perpendicular drawn from $D$ to $AC$ and $F$ the midpoint of $DE$, prove that $AF\perp BE$. The proof is very easy using coordinate geometry. I'm stuck at: Since point $F$ is midpoint of $DE$, it is fixed and is not a trivial information. But I can't see a way to use this information.","['euclidean-geometry', 'triangles', 'geometry']"
2868981,Motivation for the Basel problem,"I realized that I know of several ways how to prove that $\sum_{n=1}^{\infty}\frac{1}{n^2}=\frac{\pi^2}{6}$, but I have no idea why I would want to know the
answer in the first place. Answers I have found by myself: the probability of two integers chosen at random to be prime to each other is $\frac{6}{\pi^2}$. The proof is 
understandable by a smart undergraduate. This is the kind of motivation I am looking for. by the inverse square law of sound, a line of cars blowing their horns at a car stopped in a one lane road will sound  $\frac{\pi^2}6$ louder than a single car. Or similarly, intensity of traffic lights on a long road at night,... destructive testing of $n$ wooden beams will break on average $H_n=1+\frac12+\frac13+\ldots+\frac1n$ beams with a variance of $H_n-\sum_{k=1}^{n}\frac1{k^2}$. $\zeta(2)=\frac{\pi^2}{6}$ is all over quantum mechanics. The simplest, most understandable example I have found is Johnson-Nyquist noise. Still, there is a lot of physics background required to figure it out, so this does not satisfy me much. Are there any other good reason why to compute $\zeta(2)$ ? Why were Mengoli, Euler and other mathematicians from the Enlightenment interested in the answer ? Any application in physics, chemistry, economics,... ? As far as I am concerned, the closer to reality, the better. Thanks in advance for any help.","['riemann-zeta', 'soft-question', 'applications', 'sequences-and-series']"
2868985,Unique solution to homogenous second order differential equation,"I have a problem which gives me the homogenous second order differential equation
$$y''-6y'+9y=0 \tag{1}\label{D.E.}$$
and had me find the expression which describes the solution to the differential equation (\ref{D.E.}). I found the solution to be: $$y(t)=c_1 e^{3t}+c_2 te^{3t} \tag{2}\label{solution}$$ The problem continues by stating: The differential equation has a unique solution $y(t)$ with initial conditions $y(1)=3e^3, y'(1)=10e^3$. Mark the value $y(0)$ of that solution at $t=0$ . With the possible multiple choice answers: a) 6, b) 2, c) $0$ , and d) None of these. I know how to solve for the IVP, but it throws me off that I have to solve for a unique solution (especially when the class and I haven't recieved any litterature on how to solve for a unique solution). CALCULATIONS BELOW HAVE BEEN EDITED: (to correct $y'(t)$ this time) I have tried solving for the IVP for $y(1)$ and $y'(1)$ where:
$$y'(t)=3c_1e^{3t}+c_2\left(e^{3t}+3e^{3t}t\right) \tag{3}\label{y'solution}$$ And if I plug in $y(1)=3e^3$ in equation (\ref{solution}), and $y'(1)=10e^3$ in equation (\ref{y'solution}) then: $$y(1)=c_1 e^{3\cdot1}+c_2\cdot1\cdot e^{3\cdot1}= 3e^3 \Rightarrow 
c_1 e^{3}+c_2e^{3}= 3e^3
\tag{4}\label{y(1)solution}$$ $$y'(1)=c_1e^{3\cdot1}\cdot 3+c_2\left(e^{3\cdot1}+3e^{3\cdot1}\cdot1\right) = 10e^3 \Rightarrow 
3c_1e^{3}+c_2\left(e^{3}+3e^{3}\right) = 10e^3
\tag{5}\label{y'(1)solution}$$ If we continue, and divide both sides of $y(1)$ with $e^3$ we get:
$$y(1)=\frac{(c_1 e^3+c_2e^3)}{e^3}=\frac{3e^3}{e^3} \Rightarrow
c_1+c_2=3 $$ We then multiply the $c_2(e^{3}+3e^{3})$ part out and do the same: $$y'(1)=\frac{(3c_1e^{3}+c_2e^{3}+3c_2e^{3})}{e^3} = \frac{10e^3}{e^3} \Rightarrow
3c_1+4c_2=10$$ and get: $$ 
\begin{cases} 
c_1 + c_2 = 3 \\ 
3c_1 + 4c_2 = 10
\end{cases}
$$ which yields: $y(t) = 2e^{3t} + te^{3t}\tag{6}\label{y(t)}$ as @pointguardo answered. Finishing it of with $y(0)$ at $t=0$ we get:
$$y(0) = 2e^{3\cdot0} + 0\cdot e^{3\cdot0} = 2$$ Thank you for the help guys =)","['calculus', 'ordinary-differential-equations']"
2868994,"Projective Resolution of $C[x,y,t]/(x^2+y^2-t)$","I need to find a free projective resolution of $\mathbb{C}[x,y,t]/(x^2+y^2-t)$ as a $\mathbb{C}[t]$-module.  A projective resolution of an $R$-module $M$ is a long exact sequence 
$$...\rightarrow P_n\rightarrow P_{n-1}\rightarrow...\rightarrow P_0\rightarrow M\rightarrow 0.$$  Here each $P_i$ is required to be a projective $R$-module (hence the name).  I require a free resolution, so the $P_i$ must be free as $C[t]$-modules.  I can do simple examples, such as projective resolutions of $\mathbb{Z}/2$ as a $\mathbb{Z}$ module, i.e
$$0\rightarrow \mathbb{Z} \xrightarrow{2} \mathbb{Z} \rightarrow \mathbb{Z}/2\rightarrow 0.$$ I can do slightly harder example as well, but am at a loss whenever it comes to more complex examples.  It would be a great help if some one could describe a projective resolution of this $R$-module, and even better if they could give some general techniques for tackling these sorts of questions. Note: This is not a homework question, but rather me trying to understand some algebraic geometry machinery, and this came up in the process.","['projective-module', 'homological-algebra', 'free-modules', 'exact-sequence', 'algebraic-geometry']"
2869002,An invisible ghost jumping on a regular hexagon,"Given a regular hexagon and an invisible ghost at one of the vertices of the hexagon (we don’t know which). We have a special gun, that can kill ghosts. In a step we are able to shoot the gun twice (i.e. choose two vertices and see if the ghost is there). After every step, the gost moves to an adjacent vertex. What is the minimum number of moves required to kill the ghost? I have an example with 6 steps. I am sure this is not the minimum. My friend has an example for 4. So what is the minimum? And can you generalize for a regular $n$-gon? Thanks!","['puzzle', 'combinatorics', 'extremal-combinatorics', 'combinatorial-game-theory']"
2869031,Are there domains that are not commutative?,"Let $A$ a domain, i.e. $ab\in A\implies a=0$ or $b=0$. It's written that all domains are commutative. Is it by definition, or can we prove that domains are commutative? I mean, do we only consider domaind for commutative rings, or is a ring that is a domain then commutative?","['definition', 'abstract-algebra', 'terminology']"
2869047,Python Compute Jacobian numerically,"Hi I have a class with the purpose to solve system of Differential equations, this class contains a class named Rhs (right hand side) that contains all the feature of the differential problem: init_time , final_time , Initial value , function(that is a numpy.array containing lambda function) when I have a system of differential equation the functions are managed in this way : eq1 = lambda t,u : a*(u[0]-u[0]*u[1]);
eq2 = lambda t,u : -c*(u[1]-u[0]*u[1]);

func1 = np.array([eq1,eq2])

y0      = np.array([2.,1.])
system1 = rhs.Rhs(func1, 0,10,y0,500 ) using the parameter above the class Rhs is completed ! it have a method to return the value of each function, and a method for compute the derivate of each of function: def f(self,ti,ui):
       return  np.array([function(ti,ui) for function in self.func])     


def Df(self,ti,ui):
      eps = 10e-12
      return (self.f(ti,ui+eps) - self.f(ti,ui-eps) )/(2*eps) Now I have a big problem, in order to create a class to solve an implicit method I have to compute the Jacobian of the function ! but I have no idea how to do this ! EDIT no I need to define the jacobian matrix yes I wrote the method for derivative but I really have not idea how to define J[i][j] @saulspatz may you help me pls ? I don't need to compute it's determinant just to write the terms $J_{i,j}$ I wrote this but doesn't works : def J(self,ti,ui):   
     self.t = ti 
     self.u = ui
     Jac = np.zeros((len(ui),len(ui)))
     n = len(ui)
     eps = 1e-12
     for i in range(n):
         for j in range(n):
             Jac[i,j] = self.Df(ti,ui) 

     return Jac EDIT thank you very much @caverac how can I generalize for any system ? 
is that straightforward ?","['jacobian', 'python', 'derivatives', 'ordinary-differential-equations']"
2869072,Proof of Velu's formulas in Washington's Elliptic Curves,"The proof of Velu's formulae in Washington's ""Elliptic Curves"" uses two exercises (Ex. 12.6 and Ex.12.8). One part in Ex.12.6 is the following: Let $E:y^2+a_1xy+a_3y=x^3+a_2x^2+a_4x+a_6$ be an elliptic curve over a field $K$. Let $P,Q$ be two points on $E$. Let $x_{P}, x_Q, x_{P+Q},y_P,y_Q$ denote the $x$- or $y$-coordinates of the points. Let $Q$ be a $2$-torsion point, so $2Q=\infty$ and $Q=-Q$. The exercise claims the following equality holds: $x_{P+Q}-x_Q=\frac{3x_Q^2+2a_2x_Q+a_4-a_1y_Q}{x_P-x_Q}$ Since the cases $P=Q=-Q$ don't need examination, we assume $x_P\neq x_Q$ and we can use the addition formula as follows: $x_{P+Q}=\frac{(y_P-y_Q)^2}{(x_P-x_Q)^2}+a_1\frac{y_P-y_Q}{x_P-x_Q}-a_2-x_P-x_Q$ So we need to show that: $\frac{(y_P-y_Q)^2}{(x_P-x_Q)^2}+a_1\frac{y_P-y_Q}{x_P-x_Q}-a_2-x_P-x_Q-x_Q=\frac{3x_Q^2+2a_2x_Q+a_4-a_1y_Q}{x_P-x_Q}$ After working on it for a while I noticed that I can't solve it due to the following dead end: The LHS has a $y_P^2$ term while the RHS has not. After replacing $y_P^2$ using the equation of the curve the LHS has a $a_6$ term while the RHS has not. For all i know dividing those terms by $(x_P-x_Q)$ doesn't change anything. I'm thankful for any kind of information, especially any $equalities$ I might have missed. Note: Since $Q$ is a $2$-torsion point, we can use the negation formulae for $y$-coordinates $y_Q=-a_1x_Q-a_3-y_Q$ to replace $y_Q^2$ as an expression in $x_Q$. That's why that part isn't an issue for now.","['algebraic-geometry', 'elliptic-curves']"
2869093,How many elements of a 3x3 Rotation matrix are redundant?,"I read the following and got curious: A rotation matrix is an array of nine numbers. These are subject to
  the six norm and orthogonality constraints, so only three degrees of
  freedom are left: if three of the numbers are given, the other six can
  be computed from these equations. I know that the matrix only has three degrees of freedom, but the last statement (emphasis mine) is obviously not literally true. Given these seven elements:
$$  \left[ \begin{array}{rrrr} 
. & 0 & 0 \\
0 & . & 0 \\
0 & 0 & 1 \end{array}\right] $$ There are at least two possible solutions (1, 1 or -1, -1) which gives me valid but different rotation matrices (a rotation of 180° around the Z axis). So, if I can't choose which elements are revealed to me, I apparently need to know eight elements to be certain about the last. Is there any way to rephrase the quote above which makes it true (are diagonal matrices an exception?). I know for instance, that if I have the following six elements: 
$$  \left[ \begin{array}{rrrr} 
a & b & c \\
d & e & f \\
. & . & . \end{array}\right] $$
I can deduce the last row by using the cross product of the first two (and choose a sign based on handedness). Can I get away with less than these six elements?","['matrices', 'rotations']"
2869097,Finding the probability distribution using transformations,"The question : Suppose $X_1,X_2$ are iid with a common standard normal distribution. Find the
joint pdf of $Y_1=X_1^{2}+X_2^{2}$ and $Y_2=X_2$ and the marginal pdf of $Y_1$ .
Hint:Note that the space of $Y_1$ and $Y_2$ is given by $-\sqrt{y_1}< y_2< \sqrt{y_1}$ , $0< y_1<\infty $ . My attempt : $X_1^{2}\sim \chi ^{2}(1)$ and $X_2^{2}\sim \chi ^{2}(1)$ , so $Y_1\sim \chi ^{2}(2)$ . The pdf of $Y_1$ should be $\frac{1}{2}e^{\frac{-y}{2}}$ . I tried to get the result by using the transformations of random variables. If $0< y_2<\sqrt{y_1}$ , then $x_1=\sqrt{y_1-y_2^{2}}$ . If $-\sqrt{y_1}< y_2<0$ , then $x_1=-\sqrt{y_1-y_2^{2}}$ In both cases, the absolute value of the jacobian is $\frac{1}{2\sqrt{y_1-y_2^{2}}}$ . So, $f_{Y_1 ,Y_2}(y_1,y_2)=f_{X_1,X_2}(x_1,x_2)\left | J \right |$ , where $\left | J \right |$ is $\frac{1}{2\sqrt{y_1-y_2^{2}}}$ . $f_{X_1,X_2}(x_1,x_2)=\frac{1}{2\pi }e^{\frac{-y_1}{2}}$ (multiplication of two standard normal pdf's) $f_{Y_1 ,Y_2}(y_1,y_2)=f_{X_1,X_2}(x_1,x_2)\left | J \right |=\frac{1}{2\pi }e^{\frac{-y_1}{2}}\frac{1}{2\sqrt{y_1-y_2^{2}}}$ $f_{Y_1}(y_1)=\frac{e^{-y_1/2}}{4\pi }\int_{-\sqrt{y_1}}^{\sqrt{y_1}}\frac{1}{\sqrt{y_1-y_2^{2}}}dy_2$ . I used this integral: $\int_{-a}^{a}\frac{1}{\sqrt{a^2-x^2}}=\pi $ for $a>0$ . Then $f_{Y_1}(y_1)=\frac{e^{-y_1/2}}{4 }$ , which is not same as $\frac{e^{-y_1/2}}{2 }$ . I cannot find out where it went wrong. I need some help.","['multivariable-calculus', 'probability-distributions']"
2869120,the relation $f^2+g^2=1$ is followed only by trigonometric functions for differentiable functions,"Let $U\subseteq \mathbb{C}$ be an open, convex and connected subset and let $f,g:U\to \mathbb{C}$ be two differentiable functions so that $f',g'$ are continuous functions ($f,g \in C^1$). I want to show that if $f^2+g^2=1$ over $U$ then there is a function $z:U\to \mathbb{C}$ ($z\in C^1$) such that $f(t)=\sin(z(t))$ and $g(t) = \cos(z(t))$. My try (the unimportant part): I took this question from a book, in which it's said to use the following statement (which I've already proved): if $f:U\to \mathbb{C}$ ($f\in C^1$) with $f(U)\neq \mathbb{C}$ then there exists a function $g:U\to \mathbb{C}$ ($g\in C^1$) and $C_0\in \mathbb{C}$ so that $f(t)=\exp(g(t))+C_0$. For my question it was advised that I should use this statement for the functions $f+ig , f-ig$ I'm not sure how to continue, please help me.","['complex-analysis', 'trigonometry', 'derivatives', 'complex-numbers']"
2869144,Schwarz symmetrization is equimeasurable,"Suppose $\Omega\subset\mathbb{R^2}$ is open and bounded, and let $f:\Omega\rightarrow [0,\infty)$ be measurable. Moreover, let $\Omega^{\ast}$ denote the closed disk with midpoint $0\in\mathbb{R}^2$ and the same area as $\Omega$ (i.e. $\vert\Omega\vert=\vert \Omega^{\ast} \vert$). The Schwarz symmetrization is defined as the function 
$$f^{\ast}:\Omega^{\ast}\rightarrow[0,\infty),\quad f(x):=\sup\lbrace{ c\geq 0:x\in \Omega_c^{\ast} \rbrace},$$ where $\Omega_c:=f^{-1}([c,\infty))\subset\Omega$ and $\Omega_c^{\ast} $ denotes the closed disk with midpoint $0\in\mathbb{R}^2$ and $\vert\Omega_c^{\ast} \vert= \vert\Omega_c \vert$. I would like to understand the following two problems: 1) Why are $f$ and $f^{\ast}$ equimeasurable , i.e. $\vert \lbrace x\in\Omega^{\ast}: f^{\ast}(x)\geq c\rbrace \vert=\vert \Omega_{c} \vert$ for all $c\geq 0$ ? 2) Sometimes, the function $f^{\ast}$ is defined alternatively as $$f^{\ast}(x):=\int_{0}^{\infty} \chi_{ {\lbrace f \geq t \rbrace}^{\ast} }(x) dt.$$ I am wondering if that definition is equivalent to the first one ? Unfortunately, I do not know how to answer 1) or 2). I think that if we use $\vert\Omega_c^{\ast} \vert= \vert\Omega_c \vert$, it might be helpful. Then it remains to show:
$$\vert \lbrace x\in\Omega^{\ast}: f^{\ast}(x)\geq c\rbrace \vert = \vert \lbrace x\in\Omega: f(x)\geq c\rbrace ^{\ast} \vert$$ Are both sets within $\vert\cdots \vert$ in the last equation the same? That is, do we have :
$$\lbrace x\in\Omega^{\ast}: f^{\ast}(x)\geq c\rbrace = \lbrace x\in\Omega: f(x)\geq c\rbrace ^{\ast}$$ If the last statement is true, then 1) and 2) becomes trivial. But I do not know how to prove any of the three statements.","['decreasing-rearrangements', 'measure-theory', 'geometric-measure-theory', 'real-analysis']"
2869167,"Non homogeneous Sturm-Liouville problem and solution: Is the solution given in terms of $\lambda$, not particular eigenvalues?","Standard form of non homogeneous S.L. $$\dfrac{d}{dx}(p(x)y'(x))+(q(x)+\lambda r(x))y(x)=f(x)\tag1$$ with Boundary Conditions: let's say $(0<x<1)$+ B.C. The derivation of the solution is given as following, if you know this you can skip. For example we found the eigenvalues and corresponding eigenfunctions solving homogeneous S.L. : $\lambda_n\to \phi_n(x)$ and we assume that solution can be in form of these eigenfunctions. Such that $$y(x)=\displaystyle\sum_n^\infty a_n\phi_n(x)\tag2$$ Playing with eq (1)
  $$\dfrac{d}{dx}(p(x)y'(x))+(q(x)+\lambda_n r(x)+\lambda r(x)-\lambda_n r(x))y(x)=f(x)$$ And plugging into the assuming solution (2) $$\displaystyle\sum_n^\infty a_n \left\{\underbrace{\dfrac{d}{dx}(p(x)\phi_n'(x))+(q(x)+\lambda_n r(x))\phi_n(x)}_{0}+(\lambda r(x)-\lambda_n r(x))\phi_n(x)\right\}=f(x)$$ Then: $$\displaystyle\sum_n^\infty a_n(\lambda-\lambda_n)r(x)\phi_n(x)=f(x) \tag3$$ and using orthogonality property: $$\displaystyle\int_0^1r(x)\phi_n(x)\phi_m(x)dx=\delta_{mn}$$ Then from eq (3) we get $$a_n=\dfrac1{(\lambda-\lambda_n)}\displaystyle\int_0^1 f(x)\phi_n(x)dx\tag4$$ So we have the solution: $$y(x)=\displaystyle\sum_n^\infty \left\{\dfrac{\phi_n(x)}{(\lambda-\lambda_n)}\displaystyle\int_0^1 f(x)\phi_n(x)dx\right\}\tag5$$ Question: What is work of $\lambda$ in the eq (5)? What is the difference between $\lambda$ and $\lambda_n$, do we just left the solution like this?","['sturm-liouville', 'ordinary-differential-equations']"
2869178,"If $G = \langle A, N \rangle$ ... , show that $Z(G)$ has finite index.","Trying to solve the following problem in preparation for pre-lims! Thanks in advance for any help! Let $G$ be a group. Assume that $G$ is generated by two subgroups, $N$ and $A$, such that $A$ is abelian and $N$ is finite and normal in $G$. Show that the center $Z(G)$ has finite index in $G$. We note that if $G$ is finite or abelian, the result falls out easily. We are stuck on the infinite, non-abelian case.","['normal-subgroups', 'group-theory', 'abstract-algebra']"
2869308,Equivalent conditions for two functions to be equal almost everywhere in terms of integral,"My question is.... if two measurable functions $f$ and $g$ on a measure space $(X, M, \mu)$ are such that for all $A \in M$,
$$
\int_{A}f\,d\mu = \int_{A}g\,d\mu,
$$
does this imply that $f=g$ a.e. $\mu?$ If this isn't true, then are there any restrictions we can impose (positivity or $\sigma$-finiteness of $\mu$, etc.) to salvage the statement? I've tried defining $E$ to be the set such that $f \neq g$ and show that it must have measure zero, but I can't find the right manipulations of the integral expressions to squeeze that out.... any suggestions?","['measure-theory', 'lebesgue-integral', 'real-analysis']"
2869314,Solving for a stationary point of a multivariate Gaussian parameter (CCA),"I am reading a paper, probabilistic CCA , and am stuck on a particular derivation. Given the following multivariate Gaussian: $$
x \sim N(u, W W^{\top} + P)
$$ where $x \in \mathbb{R}^p$. Let $S = W W^{\top} + P$ where $S \in \mathbb{R}^{p \times p}$, $W \in \mathbb{R}^{p \times n}$ and $P \in \mathbb{R}^{p \times p}$. Given the negative log-likelihood, $L$: $$
L
=
\frac{1}{2} \sum_{i=1}^{n}(x_i - u)^{\top} S^{-1} (x_i - u) + \frac{n}{2} \ln |S| + \text{const}
$$ Taking the derivative, I have a stationary point $\frac{dL}{dW} = 0$, and I would like to solve for $W$ (Equation 3 in the paper): $$
(S^{-1} - S^{-1} Z Z^{\top} S^{-1}) W = 0
$$ where $Z = (x - u)$. How can I solve for $W$? I can reduce it to this form: $$
\begin{align}
(S^{-1} - S^{-1} Z Z^{\top} S^{-1}) W &= 0
\\
S^{-1} W - S^{-1} Z Z^{\top} S^{-1} W &= 0
\\
S^{-1} W &= S^{-1} Z Z^{\top} S^{-1} W
\\
W &= Z Z^{\top} S^{-1} W
\end{align}
$$ This looks like an eigenvector problem, $A v = \lambda v$, but $S^{-1}$ contains $W$: $$
W = Z Z^{\top} (WW^{\top} + P) W
$$ I'm not sure how to simplify this further.","['partial-derivative', 'statistics', 'matrix-calculus', 'derivatives']"
2869349,Manifolds whose boundaries are real / complex projective spaces,"What are some simple manifolds $M$ whose boundaries are $\mathbb{RP}^2$ or $\mathbb{RP}^n$ in general? Namely $\partial M= \mathbb{RP}^n.$ What are some simple manifolds $N$ whose boundaries are $\mathbb{CP}^2$ or $\mathbb{CP}^n$ in general? Namely $\partial N= \mathbb{CP}^n.$ Some trial attempt: I know that we can have
$\partial D^2= \mathbb{RP}^1.$ and
$\partial D^3= \mathbb{CP}^1.$ I dont know whether there are simple topology that also do the job for the above two questions?","['manifolds', 'general-topology', 'geometric-topology', 'projective-space']"
2869355,Implicit Differentiation Misunderstanding.,"Say we are given the unit circle, defined by all real numbers $x,y$ such that $x^2+y^2=1$. Now, if one tried to solve explicitly for $y$, one would get that $y=\pm{\sqrt{1-x^2}}$ and this is obviously not a function, so traditional calculus techniques cannot be used. As $x^2+y^2=1$ defines our curve, we can take the derivative of all terms with respect to $x$ to get that $2x+2y\frac{dy}{dx}=0$, as by the chain rule $\frac{d(y^2)}{dx}=\frac{d(y^2)}{dy}\frac{dy}{dx}$, and one can therefore do some algebraic manipulation to get that $\frac{dy}{dx}=-\frac{x}{y}$. However, I have a few questions: 1) I thought the whole point was that $y$ is not a function, and so $\frac{dy}{dx}$ does not make sense as this notation is defined only when $y$ is a function (or am I wrong in this regard?) 2) Why are we allowed to take the derivative with respect to $x$ on both sides of the equation, but not for example to solve $x^2=1$ to get $2x=0$ which would produce the wrong answer that $x=0$ (my inkling tells me that this is because $x^2=1$ is not an identity but an equation, but I just want to check up on this...)","['calculus', 'implicit-differentiation', 'derivatives']"
2869360,The Lusternik–Schnirelmann Theorem For Open and Closed Sets,"The generalized Lusternik-Schnirelmann Theorem states that If $S^n$ is covered by $n+1$ sets $A_1, A_2, ... ,A_{n+1}$ such that each $A_i$ is either open or closed, then there exists an $i$ such that $A_i$ has a pair of antipodal points. I'm having trouble proving this theorem for ""$A_i$ is either open or closed"". I can prove the theorem given the hypothesis that all $A_i$ are closed. Define $d_i : S^n \to \mathbb{R}$ by $d_i(x) = \inf \{ |x - y| : y \in A_i \}$. Clearly, this function is continuous. Now, consider the map 
\begin{align*}
\Psi: S^n & \longrightarrow \mathbb{R}^n \\
x & \longrightarrow (d_1(x), ..., d_n(x)).
\end{align*}
From the Borsuk-Ulam theorem, there exists an $x \in S^n$ such that $\Psi(x) = \Psi(-x) = \Omega$. If any of the coordinates of $\Omega$ are $0$, then, since the $A_i$ are closed, $x, -x$ must be limit points of some $A_i$ and hence are in that $A_i$. If none of the coordinates are $0$, then $x, -x$ are both in $A_{n+1}$. If the $A_i$'s could also be open, the above argument does not work. Any suggestions on how to prove the theorem would be appreciated.","['general-topology', 'real-analysis']"
2869367,Matrix vector form. Is this in the correct form?,"I have this question: Write the linear system
  $$\begin{array}{rcr}-2x_1+x_2-4x_3 & = & 1 \\ x_1-2x_2 & = & -3 \\ x_1+x_2-4x_3 & = & 0 \end{array}$$
  in the matrix-vector form $A\mathbf{x}=\mathbf{b}$. Is this what they want? $$ 
x_1*
\begin{bmatrix}
  -2 \\ 1 \\ 1
\end{bmatrix}
+
x_2*
\begin{bmatrix}
  1 \\ -2 \\ 1
\end{bmatrix}
+
x_3*
\begin{bmatrix}
  -4 \\ 0 \\ -4
\end{bmatrix}
=
\begin{bmatrix}
  1 \\ -3 \\ 0
\end{bmatrix}
$$","['matrices', 'systems-of-equations', 'linear-algebra']"
2869406,Show that a semigroup is semisimple iff $A^2 = A$ for every two-sided ideal $A$,"Let $S$ be a semigroup, a subset $I\subseteq S$ is called an ideal if $SI \subseteq I$ and $IS \subseteq S$. We denote by $S^1$ the semigroup $S$ adjoined with a identity if it does not contains one, and for $a \in S$ we set $J(a) = S^1aS^1$, the principal ideal generated by $a$. Also we set for $a,b \in S$
$$
  a \mathcal J b :\Leftrightarrow S^1 a S^1 = S^1 b S^1.
$$
For an ideal we can form the Rees factor semigroup denotes by $S / I$, which essentially means collapsing everything in $I$ to a zero in the factor semigroup. Denote the $\mathcal J$-equivalence class of some $a \in S$ by $J_a$. A principal factor is a factor semigroup of the form $J(a) / (J(a) \setminus J_a)$, see the Ecyclopdia of mathematics . The unique minimal ideal, called the kernel of $S$, is among the principal factors. A null semigroup (or semigroup with zero multiplication) is a semigroup $S$ with a zero $0 \in S$ such that $ab = 0$ for all $a,b \in S$. A semigroup is called semisimple if none of the principal factors is a null semigroup. I want to show that a semigroup $S$ is semisimple if and onyl if $A^2 = A$ for every ideal $A \subseteq S$. This is an exercise from the book Fundamentals of Semigroup Theory by J. Howie, page 95. What I do not understand is that if a semigroup contains a zero $0$, then the minimal ideal must be $\{0\}$, in particular this is a null semigroup. But I can very well find semigroups such that $A^2 = A$ for every ideal that have a zero. For example take any group $G$, adjoin a zero $0$ by setting $g0 = 0g =0$ and observe that the only ideals are $\{0\}$ and $G \cup\{0\}$ itself, and they both fulfill the condition on ideals... So any hints on this exercise, or what I have understood wrong here? EDIT : Maybe there is a typo in the exercise, and just the principal factors not equal to the kernel are meant. But the same exercise appears in the classic Algebraic Theory of Semigroups by Clifford/Preston, and the above link to the Encyclopedia does not excludes the kernel. But if we exclude the kernel, than $A^2 = A$ for every ideal $A$ iff no principal factor not equal the kernel is null. For $A^2 \subsetneq A$ choose $a \in A \setminus A^2$, then $J(a) / N(a)$ is null, for if $x,y \in J(a)$ with $xy \notin N(a)$ would imply $a = u(xy)v$ for some $u,v \in S^1$ and $(ux)(yv) \in A^2$. Conversely,
as $S^2 / I = (S/I)^2$ for every semigroup $S$ and ideal $I$ we have that $(J(a)/N(a))^2 = J(a)^2 / N(a) = J(a) / N(a)$ and as $|J(a)/N(a)| > 1$ if $N(a) \ne\emptyset$ those principal factors not equal the kernel are not null. But in case is is not a typo I ask for feedback/clarification...","['group-theory', 'abstract-algebra', 'semigroups']"
2869431,"Open set $(0,1)$ as union of disjoint open sets","One theorem says ""Any open subset of  $\mathbb{R}$ can be written as the countable union of disjoint open intervals "". I am trying to see how one can partition the open set $(0,1)$ such that it is the union of disjoint open sets? (other than the trivial partition of $(0,1)$ itself).  Is it possible? For example, if we take the sets $(0,\frac{1}{2})$ and $(\frac{1}{2},1)$, clearly the point $\frac{1}{2}$ does not lie in both open sets, and it can not be in the union either. After the union we get a set $(0,1)\setminus\{\frac{1}{2}\}$. What is a meaningful partition for the open set $(0,1)$ such that it partitions are open?  Can I do something like $(0,\frac{1}{2}+\epsilon) \cup (\frac{1}{2}-\epsilon,1)$?","['elementary-set-theory', 'general-topology', 'real-analysis']"
2869450,"Each member of a population dies with probability $\frac12$ each day, what is the probability that there will be exactly $1$ person alive?","Suppose that there are $n$ people alive in a population. Due to a deadly disease, each person dies with probability $\frac12$ each day (and there are no births). What is the probability that there will be exactly one person alive at some time? Thoughts: Let $p_k$ be the probability that the population reaches exactly $1$ person given that there are currently $k$ people alive. Then $p_0 = 0$ and $p_1 = 1$. The probability of going from $k$ people alive to $k - j$ being alive (where $0 \leq j \leq k$) is the probability that $j$ die:
$$
\binom{k}{j} \left(\frac{1}{2}\right)^j \left(\frac{1}{2}\right)^{k - j} = \binom{k}{j} \left(\frac{1}{2}\right)^k
$$
And using conditional probability we have the recursion:
$$
p_k = \frac{1}{2^k} \binom{k}{0} p_k + \frac{1}{2^k} \binom{k}{1} p_{k - 1} + \cdots + \frac{1}{2^k} \binom{k}{k - 1} p_1 + \frac{1}{2^k} \binom{k}{k} p_0,
$$
or
$$
(2^k - 1)p_k = \binom{k}{1} p_{k - 1} + \cdots + \binom{k}{k - 1} p_1.
$$
Is it possible to solve a recursion like this? Is there a better way to solve the puzzle?","['probability', 'recursion']"
2869455,Applications of model theory - where are the sheaves?,"I know very basic model theory (compactness, Lowenheim-Skolem, EF games, at that level), and I'd like to pick up more, mostly out of intrinsic interest and partially because I think it would give me an interesting perspective as I dive into my main subject, which is algebraic geometry. I've been told that the best source for this sort of thing is Marker. After flipping through it I'm kind of confused about how connections between the subjects arise. Are the applications of model theory to algebraic geometry strictly classical? Are nonclassical applications too advanced to show up in something like Marker? I ask because there's no mention at all of sheaves in Marker and nothing really about categories, and from a little googling this doesn't seem to be unusual. I'm not one to demand categorification for no reason but these are how the basic objects of modern a.g. are defined so I would expect them to show up in applications from a closely linked subject.","['model-theory', 'algebraic-geometry']"
2869498,Constructing a continuous path between two matrices,"Let $A_1, A_2 \in GL_n(\mathbb R)$ be two fixed matrices with eigenvalues lying on the open left half plane of $\mathbb C$, i.e., with negative real parts and $A_1 \neq A_2$. We may assume $A_1, A_2$ are diagonalizable if necessary. Let $(A_1)_\alpha$ and $(A_2)_\beta$ be the corresponding matrix that scales the eigenvalues by $\alpha \in \mathbb R$ and $\beta \in \mathbb R$ respectively. That is if $(\lambda_1, \dots, \lambda_n)$ are eigenvalues of $A_1$, then $(\alpha \lambda_1, \dots, \alpha \lambda_n)$ are eigenvalues of $(A_1)_{\alpha}$. I am wondering whether it is possible to construct a continuous path $\gamma: [0,1] \to GL_n(\mathbb R)$ with eigenvalues all lying on the left half plane along the path: by choosing a sequence of nonzero real numbers, $(\alpha_n)$ and $(\beta_n)$, $\gamma$ is piecewise linear. For example, suppose there exist some finite sequences $(\alpha_1, \dots, \alpha_k)$ and $(\beta_1, \dots, \beta_l)$, then the path would be
\begin{align*}
A_1 \xrightarrow{} (A_1)_{\alpha_1} \xrightarrow{} \dots \xrightarrow{} (A_1)_{\alpha_k} \xrightarrow{}  (A_2)_{\beta_1} \xrightarrow{} \dots \xrightarrow{} (A_2)_{\beta_l} \xrightarrow{} A_2.
\end{align*}
In between, each arrow stands for a convex path. Assume we can construct a continuous path between $A$ and $(A)_{\alpha}$ . If we take a convex combination of $A_1, A_2$, i.e., $A(t) = (1-t)A_1 + tA_2$. Then $\det(A(t))$ is a nonconstant polynomial in $t$ and has at most $n$ zeros. Intuitively I am thinking of taking the path until we meet a singular point of $t$ (by continuity, all eigenvalues should stay in the left half plane) and change path to some $(A_1)_{\alpha}$ and doing this repeatedly. Not sure whether this will work. p.s. The reason for my consideration was because I need to guarantee certain matrix structure along the path. I have worked out scaling eigenvalues can be done within the structure of interest.","['matrix-analysis', 'general-topology', 'path-connected', 'linear-algebra']"
2869527,"How can one generalize the Gauss map to higher dimensions? More specifically, bi-dimensional manifolds in $\mathbb{R}^4$","It's easy to define the unitary tangent fields of a $2$-dimensional surface $S: I \times J \to \mathbb{R}^4$, but since I don't have the cross product anymore, an unitary normal field is harder to find.","['surfaces', 'geometry', 'riemannian-geometry', 'differential-geometry']"
2869540,Players A and B repeatedly flip a (possibly unfair) coin until one loses everything. Each starting with a different amount of money,"There are two players $A$ and $B$. At the beginning $A$ has $a_0 ∈ [0,1]$ amount of money and $B$ has $b_0 = 1 - a_0$ amount of money. They have one possibly unfair coin which they flip repeatedly. $A$ always guesses $Heads$ and $B$ always guesses $Tails$. Before each flip, they both bet a minimum of what each of them has (i.e. $bet_i = min(a_i, b_i)$). After the coin is tossed, the winner takes the $bet_i$ and they play again until one of them has no more money to play with. I've been thinking about it for some time now and was unable to find a closed formula nor find it mentioned anywhere. So any more info would be appreciated. The best I could do was a simulation. Interestingly it seems that when the coin is fair, the probability of $A$ winning is proportional to $a_0$ but when the coin is unfair, the graph looks more peculiar: Here, the horizontal axis represents $a_0$ and the vertical one represents $P(A\ wins\ given\ that\ the\ coin\ is\ fair)$ In the next graph, the horizontal axis also represents $a_0$ and the vertical one represents $P(A\ wins\ given\ that\ the\ coin\ ends\ up\ Heads\ 1\ in\ 3\ times)$. The code for the simulation can be found here .","['economics', 'probability']"
2869579,Set of Discontinuities of arbitrary function is F sigma set,"I am trying to prove that, given an arbitrary function $f$, the set of discontinuities of $f$, denoted $D_f=[{x\in{\Bbb{R}}:\mbox{f is not continuous at $x$}}$], is a $F_\sigma$ set - that is, a countable union of closed sets. This question may have been asked before, but after searching thoroughly, this is the only one to approach it in a certain manner with specific goals (taken from Stephen Abbott's Understanding Analysis). First, a crucial piece of terminology (for those who do not know it): ""Let $f$ be defined on $\Bbb{R}$, and let $\alpha>0$. The function $f$ is said to be $\alpha$-continuous at $x\in{\Bbb{R}}$ if there exists a $\delta>0$ such that for all $y,z\in{V_\delta{(x)}}$ it follows that $|f(y)-f(z)|<\alpha$. Also, we let $D_\alpha=[x\in{\Bbb{R}}:\mbox{f is not $\alpha$-continuous at $x$}]$ There are 4 steps in this proof, 3 of which I have already proved: 1) Show that, for a fixed $\alpha>0$, $D_\alpha$ is closed (already proved myself) 2) Show that, if $\alpha_1<\alpha_2$, then $D_{\alpha_2}\subset{D_{\alpha_1}}$ (already proved myself) 3) Let $\alpha>0$ be given. Show that if $f$ is continuous at $x$, then it is also $\alpha$-continuous at $x$. Show therefore that $D_\alpha\subset{D_f}$ (already proved myself) 4) Show that if $f$ is not continuous at $x$, then it is not $\alpha$-continuous for some $\alpha>0$. Show why this guarantees that $D_f=\bigcup_{n=1}^{\infty}D_{1/n}$. As, by point 1, each $D_{1/n}$ is closed, it thus follows that $D_f$ is a $F_\sigma$ set. Now, I have proved the first sentence of point 4 myself, however, the bit where I run into trouble is how to show that this guarantees that $D_f=\bigcup_{n=1}^{\infty}D_{1/n}$. My idea was that this is true because continuity and $\alpha$-continuity are equivalent, so we simply take the union over all $\alpha$, and by point 2, the only $\alpha$ that 'matter' are those infinitesimally close to 0, and so as (1/n) converges to 0, all possible $\alpha$ infinitesimally close to 0 are 'covered'. This is a very loose argument that I would like to make formal, however, this only allows for $\alpha$ of the form $1/n$ even though $\alpha$ can be an irrational number... Please could someone try and explain how this is guaranteed thus (out of this v. long proof only bit I don't get bloody hell). Thank you. EDIT: $f$ is a function from the reals to the reals (although this argument can easily be extended to general complete metric spaces)","['continuity', 'general-topology', 'functions', 'real-analysis']"
2869581,Why do two definitons of curvature give different answers?,"In section 13.4 problem 33 of Calculus third edition early transcedentals by Jon Rogawski and Colin Adams, it states Let $$s(t)=\int_{-\infty}^t\|r'(u)\| \ du$$ for Bernoulli spiral
  $$r(t)=
\langle e^t\cos(4t),e^t\sin(4t) \rangle$$ Show that the radius of curvature is
  porportional to $s(t)$ Though my textbook states the curvature $k(t)$ is equal to $$\frac{\|T'(t)\|}{\|r'(t)\|}$$ which is also (for arbitrary regular parametrizations) $$\frac{\|r'(t) \times r''(t)\|}{\|r'(t)\|^3}$$ I get different answers applying both methods. Method one $$T(t)=\frac{r'(t)}{\|r'(t)\|}$$ $$r'(t)=\langle e^t\cos{\left(4t\right)}-4e^t\sin{\left(4t\right)}, e^t\sin(4t)+4e^t
\cos(4t) \rangle$$ $$\|r'(t)\|=\sqrt{17}e^t$$ $$T(t)=\frac{1}{\sqrt{17}e^t}{\langle e^t\cos{\left(4t\right)}-4e^t\sin{\left(4t\right)}, e^t\sin(4t)+4e^t
\cos(4t) \rangle}=\frac{1}{\sqrt{17}}{\langle \cos{\left(4t\right)}-4\sin{\left(4t\right)}, \sin(4t)+4
\cos(4t) \rangle}$$ $$T'(t)=\frac{1}{\sqrt{17}}{\langle -4\sin{\left(4t\right)}-16\cos{\left(4t\right)}, 4\cos(4t)-16
\sin(4t) \rangle}$$ $$\|T'(t)\|=\frac{1}{\sqrt{17}}\sqrt{16+256}=\frac{\sqrt{272}}{\sqrt{17}}=4$$ $$k(t)=\frac{\|T'(t)\|}{\|r'(t)\|}=\frac{4}{\sqrt{17}e^{2t}}$$ The eqution $s(t)$ is $$\int_{-\infty}^{t}\|r'(t)\| \ du=\int_{-\infty}^{t}\sqrt{17}e^t=\sqrt{17}e^t$$ Hence the ratio of $s(t)$ to the radius of curvature $k(t)$ is $$\frac{\sqrt{17}e^t}{4\sqrt{17}e^t}=1/4$$ Method Two In the second method we calculate $$\frac{\|r'(t) \times r''(t)\|}{\|r'(t)\|^3}$$ We already know $$\|r'(t)\|=\sqrt{17}e^t$$ $$\|r'(t)\|^3=17\sqrt{17}e^{3t}$$ Calcuating $$r''(t)$$, we get $$r''(t)=\langle -15e^t\cos(4t)-8e^t\sin(4t), -15e^t\sin(4t)+8e^t\cos(4t) \rangle $$ Hence $\|r'(t)\times r''(t)\|$ is equals to $$\sqrt{68}e^t=2\sqrt{17}e^{t}$$ Hence $$\frac{\|r'(t) \times r''(t)\|}{\|r'(t)\|^3}=\frac{2}{17 e^{2t}}$$ With this we know the ratio of $s(t)$ to radius of curvature is $$\frac{\sqrt{17}e^t}{\frac{2}{17} e^{2t}}=\frac{17\sqrt{17}}{2}e^t$$ Reasons for different answers One possibility is in the textbook. They state, ""In practice we compute the curvature using the following formula,
  which is valid for abitrary regular paramterizations $$\frac{\|r'(t) \times r''(t)\|}{\|r'(t)\|^3}$$ However I'm not sure what ""arbitrary regular parmaterization"" means. How does this give us two different answers?","['calculus', 'algebra-precalculus', 'curvature']"
2869605,Distinguishing between inner product and outer product in matrix notation,"As a recent field transferee from chemist to data scientist, I find myself wading through more matrix multiplication than I'm used to. I did some linear algebra way back, but I struggle with identifying 'which way' (i.e. inner or outer product) a given matrix multiplication is going. I feel like there's some sort of convention with vector multiplication where the  vector $\mathbf X$ is treated as a column vector, and $\mathbf X^T \mathbf X$ and $\mathbf X\mathbf X^T$ are the inner and outer product of  $\mathbf X$ respectively. Is this true in all cases? Are there any tricks or mnemonics to help me keep track of the product of a string of such multiplications? EDIT: There's a comment stating that it is common to assume that vectors are treated as column matrices. How common? Does it vary between disciplines? How likely (given that i'm looking through Wikipedia and stackexchange, not centuries-old manuscripts) am I to encounter the converse scenario?","['inner-products', 'convention', 'outer-product', 'linear-algebra']"
2869628,Notation of symmetric sum notation,"When you use the symmetric sum notation, for example, $$\sum_\text{sym}abc+a$$ if there are 3 variables, then does abc count once, 3 times or 6 times? I am confused about repetitions of the same expression in a symmetric sum notation.","['notation', 'algebra-precalculus', 'permutations', 'summation']"
2869634,Deconvolution question,"Suppose $a,b,x:\mathbb{R}\mapsto\mathbb{R}$ are three functions of which $a$ and $b$ are known and $x$ is unknown. Suppose they are related by the integral equation. $$\int_{-\infty}^\infty a(t-s)\,x(s)\,ds = b(t)~,\forall\ t\in\mathbb{R}~.$$ Assume $a(t)$ is a symmetric function ($a(t) = a(-t)$) so that it has a Fourier transform $A(\omega)$ that is real. Assume also that $A(\omega)$ is positive everywhere. If the Fourier transform of $b(t)$ is $B(\omega),$ then we can solve for the Fourier transform of $x(t)$, as $$A(\omega)X(\omega) = B(\omega)~,$$ from which we can write out $x(t)$ explicitly by using the inverse Fourier transform. Is there such an explicit solution for $x$ when $a:[-1,1]\mapsto\mathbb{R}$ is a symmetric function, $b,x:[0,1]\mapsto\mathbb{R}$ and are related by $$\int_{0}^1 a(t-s)\,x(s)\,ds = b(t)~,\forall\ t\in [0,1]~.$$ You can impose any regularity condition on $a(t)$ if that helps in obtaining a solution.","['integration', 'fourier-analysis']"
2869676,"In combinatorics, why is asking the opposite problem often times easier? [closed]","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 5 years ago . Improve this question Consider the birthday problem. Given $N$ people, how many ways are there for there to exist some pair of people with the same birthday? Enumerating the possibilities quickly becomes tedious However, the complement problem (Given $N$ people, how many ways are there for no one to have the same birthday?) is trivial. In fields like probability, this has obvious applications, due to the ""complement law"": if $A \cup A^c = S$, where $S$ is the entire sample space, then $$P(A) + P(A^c) = 1 \implies P(A) = 1 - P(A^c)$$ In general, this pattern is very common. Intuitively, I sense: somehow, the complement problem is asking for a lot less information if one has something like the ""complement law"" in probability, then in some restricted scope of problems, the ""complement law"" gives in some sense, the ""same amount of information"" What do mathematicians call what I am getting at here? Am I overblowing how common a trend it is?",['combinatorics']
2869725,Minimal surfaces for planar octagons and nonagons,"4 , 6, 8 triangles can make a tetrahedron and up. 6 , 8, 9 , 10 quadrilaterals can make a cube and up. 12 , 16, 18, 20 pentagons can make a tetartoid or dodecahedron and up . 7 , 8, 9, 10 hexagons can make make a Szilassi toroid and up . 12 , 24 heptagons can make a heptagonal dodecahedron or Klein quartic 3-torus (shown below). 4, 6, 12, 7, 12, ... what is next in this sequence? Could it possibly be 15, with Foster graph F040, which I call the Moving Day graph after Loyd's Moving Day puzzle?  If so, how can those octagons be made planar to contain a 3d-printable space?  Or is it some other graph?  Perhaps 24 octagons can be linked together in a way similar to the 24-cell ? For nonagons, is it Foster F060A, or the Biggs-Smith graph ? Anything between 12 and 24 for heptagons?","['polyhedra', '3d', 'graph-theory', 'solid-geometry', 'sequences-and-series']"
2869734,"Show that $[a,b] = \bigcap_{n=1}^{\infty} (a-1/n, b+1/n).$","Show that $[a,b] = \bigcap_{n=1}^{\infty} (a-1/n, b+1/n).$ Attempt: $a> a -1/n$ and $b < b+1/n$. This implies $[a,b] \subset (a-1/n, b+1/n)$. This implies $[a,b] \subset \bigcap_{n=1}^{\infty} (a-1/n, b+1/n) $. Show that $(a,b) = \bigcup_{n=1}^{\infty} [a+1/n, b-1/n].$ Attempt: Similarly, $a< a+1/n$ and $b> b-1/n$. Therefore, $[a+1/n, b-1/n] \subset (a,b)$. This implies $\bigcup_{n=1}^{\infty} [a+1/n, b-1/n] \subset (a,b)$. Is this correct? and could you give some hint for how to proceed the other side? Thank you in advance.",['elementary-set-theory']
2869744,What is the higher-order derivative test in multivariable calculus?,"In single-variable calculus, the second-derivative test states that if $x$ is a real number such that $f'(x)=0$, then: If $f''(x)>0$, then $f$ has a local minimum at $x$. If $f''(x)<0$, then $f$ has a local maximum at $x$. If $f''(x)=0$, then the text is inconclusive. But there's no need to despair if the second-derivative test is inconclusive, because there is the higher-order derivative test.  It states that if $x$ is a real number such that $f'(x)=0$, and $n$ is the smallest natural number such that $f^{(n)}(x)\neq 0$, then: If $n$ is even and $f^{(n)}>0$, then $f$ has a local minimum at $x$. If $n$ is even and $f^{(n)}<0$, then $f$ has a local manimum at $x$. If $n$ is odd, then $f$ has an inflection point at $x$. Similarly, in multivariable calculus the second-derivative test states that if $(x,y)$  is an ordered pair such that $\nabla f(x,y) = 0$, then: If $D(x,y)>0$ and $f_{xx}(x,y)>0$, then $f$ has a local minimum at $(x,y)$. If $D(x,y)>0$ and $f_{xx}(x,y)<0$, then $f$ has a local maximum at $(x,y)$. If $D(x,y)<0$, then $f$ has a saddle point at $(x,y)$. If $D(x,y)=0$, then the test is inconclusive. where $D(x,y)=f_{xx}(x,y)f_{yy}(x,y)-(f_{xy}(x,y))^2$ is the determinant of the Hessian matrix of $f$ evaluated at $(x,y)$. My question is, what do you do if this test is inconclusive?  What is the analogue of the higher-order derivative test in multivariable calculus?","['multivariable-calculus', 'calculus', 'hessian-matrix', 'real-analysis']"
2869753,Is $∅$ proper subset of $\{\{∅\}\}$? [duplicate],"This question already has answers here : If null set is an element of a set then will it belongs to set or subset? [closed] (4 answers) Closed 5 years ago . I get that $∅$ is subset of every set thus $∅ ⊆ \{\{∅\}\}$.
However, I'm not sure if $∅ ⊂ \{\{∅\}\}$.
From definition of proper subset, the relation between two sets require the larger set to have at least one element not in the other one.
What I'm confused is, does $\{\{∅\}\}$ have an element that $∅$ doesn't have?",['elementary-set-theory']
2869792,"How to determine parameters $a, b,$ and $d$ so that a rational function models a given graph?","I'm trying to solve a function problem. It states: Determine the values of $a$, $b$, and $d$ so that the rational function $$f(x) = \frac{(x+a)(x-1)(x-b)}{(x-c)(x+d)(x-3)}$$ correctly models this graph: I've been looking at this for a while and I just can't figure out how I'm supposed to approach it. I think it has something to do with limits and I know they're all going to be integer numbers. Any ideas beyond randomly plugging numbers into Desmos until I get the right graph?","['algebra-precalculus', 'rational-functions']"
2869804,Find the first moment of a probability distribution governed by a nonlinear first order ODE,"May I ask if there is any standard way to find the first moment of a probability distribution governed by a nonlinear first-order ODE. For example,
$$
\frac{\mathrm d p(x)}{\mathrm dx} = \alpha(x) p(x)
$$
where $\alpha(x)$ is a non-linear function in terms of $x$. As far as I know, the most straightforward method is trying to solve the distribution $p(x)$ directly, and then find the first moment by 
$$
\langle x\rangle = \int x p(x)\mathrm dx 
$$
but sometimes the analytical form of this distribution is very hard to obtain, so instead of doing this is there other method?","['moment-generating-functions', 'ordinary-differential-equations', 'probability']"
2869816,The measurability of truncation function,"Show directly that if $f$ is measurable and $A>0$ , then the truncation $f_A$ defined by $$f_A(x)=\begin{cases}f(x)&\text{ if }|f(x)| \le A,\\ A &\text{ if }f(x) > A,\\-A&\text{ if }f(x) < -A\end{cases}$$ is measurable. "" $f$ is measurable"" means that $\{x \in X | f(x) > \alpha\} \in \sum$ . If $f$ is Borel measurable, I can divide the interval into $(-\infty, -A) \cup[-A,A] \cup (A, \infty).$ By doing this, I can prove that $f_A$ is measurable since the interval of $f_A$ is the complement of $f$ in $(-\infty, -A)\cup(A, \infty)$ . However, if I don't know the interval of $\sum$ , how can I approach this question? Could you give some help? Thank you in advance.",['measure-theory']
2869827,Why Martingales are so important?,"I'm studing probability, an I can see martingale everywhere. I can work with them, but I don't really see in what they are so important everywhere. I know that $M_n$ is a martingale wrt the filtration $\mathcal F_n$ if $M_n$ is $\mathcal F_n-$measurable and if $$\mathbb E[M_{n+1}\mid \mathcal F_n]=M_n.$$
In what this definition makes them so important ? And why are they everywhere ?","['martingales', 'probability-theory']"
2869838,How does Amoeba forcing add a measure 1 set of random reals?,"Amoeba forcing is defined as follows: $\begin{equation*}
(\mathbb{A}, \leq) := (\{X \subset 2^{\omega} : X \text{ is open and } \mu(X) < 1/2\}, \supseteq)
\end{equation*}$. It is claimed that this forcing notion adds a measure 1 set of random reals. I get that if $G$ is $(\mathbf{V}, \mathbb{A})$-generic then $2^{\omega} - \bigcup G$ is a perfect set of random reals over $\mathbf{V}$ of measure 1/2. But how do we recover a set of random reals of measure 1 from the generic filter?","['measure-theory', 'forcing', 'set-theory']"
2869856,Intuition contradicts fact: $\frac{z}{1+z^2}$ has a pole at $\infty$?,"Consider the function $$f(z)=\frac{z}{1+z^2}$$ Clearly, $$\lim_{z\to\infty}f(z)=0$$ from any directions. However, it still has a pole at infinity as the residue there is non zero:
$$\operatorname*{Res}_{z=0}\frac1{z^2}\frac{1/z}{1+1/z^2}\ne 0$$ The function does not blow up to infinity but still has a pole there: how to reconcile? Is there an intuitive explanation for this phenomenon?",['complex-analysis']
2869864,last 2 digits of a sequence,"$x+\frac{1}{x} = 3$, what are the last 2 digits of  $x^{2^{2013}}+\frac{1}{x^{2^{2013}}}$? Getting the next value, we have to square then subtract by 2, I am clueless in getting to the next step",['sequences-and-series']
2869884,Looking for function name,"This may seem trivial, but I need to find a name for the following function for a publication. Its general $N$-dimensional form is
$$y = \sum_{i=0}^{2^N-1} a_i \prod_{j=0}^{N-1} x_{j+1}^{\left \lfloor i/2^j \right \rfloor \mod 2}$$
where $\lfloor x \rfloor$ indicates the floor (round down) operator. When $N=1$ the function is a straight line $y = a_0 + a_1 x_1$ and when $N=2$ it's a parabolic hyperboloid $y = a_0 + a_1 x_1 + a_2 x_2 + a_3 x_1 x_2$. In the general case it maintains at least one of the properties of the parabolic hyperboloid, i.e. by fixing the value of $N-1$ variables, the function becomes a straight line in the $N$-th variable: $y = A_0 + A_1 x_k$. I'm not sure I can call it an $N$-dimensional parabolic hyperboloid, because the PH is a 2nd degree function (it can be written alternatively as $y = b_1 x_1^2 - b_2 x_2^2$, with $b_1 b_2 > 0$ plus some translations for the two variables), while the general $N$-dimensional form should be of $N$-th degree. Is there a proper name I can give to this function? If necessary, I'm particularly interested in the $N=4$ case.","['functions', 'terminology']"
2869887,"If $g:[0,1] \to \Bbb{R}$ such that $g(x)=g(y) \implies g'(x)=g'(y)$ for all $x,y \in (0,1)$, then $g$ is monotonic?","Is this true or false? Let $g:[0,1] \to \Bbb R$ be a function continuous on $[0,1]$ and differentiable on $(0,1)$, such that $g'$ is a function of $g$, i.e. for every $x, y \in (0,1)$, if $g(x) = g(y)$ then $g'(x) = g'(y)$. Then $g$ is monotonic. I posted my proof as a self-answer, and wonder if there are any easier proofs.","['alternative-proof', 'monotone-functions', 'real-analysis']"
2869895,Conditional probability in conditional probability,"Suppose we have some general set $X$ with a finite measure $\mu$ (WLOG $\mu(X)=1$). We assume that there is a random partition on $X$ to ""good"" and ""bad"", and each $x\in X$ is either good or bad. For each $x$ define the event $A_x=$ {$x$ is bad}. 
Define $A$ to be the event that the set of bad $x$ has measure larger than $\epsilon$. I want to show that there exists some $x\in X$ s.t. $\mathbb{P}(A_x | A)>\epsilon$.
 Intuitively, it should be clear that , since the measure of all the bad $x$ is larger than $\epsilon$, but I am stuggling proving it.","['probability-theory', 'probability']"
2869903,Classifying groups of order 585,"I am trying to classify the groups of order 585. (It is known that there are 4 of distinct non-isomorphic groups, but I am not assuming it.) The question further asks to show that any group of this size has a cyclic subgroup of prime index, and the center is non-trivial with a composite order. My attempt so far: Note that $|{G}|=585=3^2\cdot 5\cdot 13$. By Sylow, $n_3\in \{1,13\}$, $n_5=1$, and $n_{13}=1$. Denote by $S_{3}$, $S_5$, $S_{13}$, the Sylow subgroups of respective order. It then follows that $S_5\cong \mathbb{Z}_5$, $S_{13}\cong \mathbb{Z}_{13}$, and $S_3$ is isomorphic to either $\mathbb{Z}_9$ or $\mathbb{Z}_3\times\mathbb{Z}_3$. Take $\mathbb{Z}_5\times \mathbb{Z}_{13}$. This is a normal subgroup that is cyclic (but not with prime index), hence all groups of order 585 are either of the form $(\mathbb{Z}_5\times \mathbb{Z}_{13})\rtimes \mathbb{Z}_9$ $(\mathbb{Z}_5\times \mathbb{Z}_{13})\rtimes (\mathbb{Z}_3\times\mathbb{Z}_3)$. The immediate ones are: (i) $\mathbb{Z}_5\times \mathbb{Z}_{13}\times \mathbb{Z}_9$ (ii) $\mathbb{Z}_5\times \mathbb{Z}_{13}\times \mathbb{Z}_3\times \mathbb{Z}_3$ These clearly have cyclic subgroups of prime index, namely, $\mathbb{Z}_{13}\times \mathbb{Z}_9$ (or $\mathbb{Z}_5\times \mathbb{Z}_9$) and $\mathbb{Z}_5\times \mathbb{Z}_{13}\times \mathbb{Z}_3$. Also, these are abelian groups, so they obviously have non-trivial center of composite order (namely, 585). Okay, let's find then the non-trivial semi-direct products. To investigate the Case 1, we need to identify the homomorphisms $\varphi:\mathbb{Z}_9\to\mathrm{Aut}(\mathbb{Z}_5\times \mathbb{Z}_{13})\cong\mathbb{Z}_5^*\times\mathbb{Z}_{13}^*$. Since $|\varphi(1)|\mid 9$, it follows that the only non-trivial homomorphism is given by $1\mapsto (1,3)$. This gives rise to the presentation $$
<a,b,c,d:a^5=b^{13}=c^9=e,ab=ba,ac=ca,cbc^{-1}=b^3>.
$$ This has a cyclic subgroup generated by $ac$ whose index is 13. This group has a non-trivial center since $a\in Z(G)$. Moreover, the order of $Z(G)$ is composite since it also contains $c^3$: $c^3bc^{-3}=b^{27}=b$. Hence the order of $Z(G)$ is divisible by 15. Let's move on to the Case 2 where $G$ is given by $(\mathbb{Z}_5\times \mathbb{Z}_{13})\rtimes (\mathbb{Z}_3\times\mathbb{Z}_3)$. To define the homomorphism, we need to determine where such a homomorphism sends $(1,0)$ and $(0,1)$ of $\mathbb{Z}_3\times\mathbb{Z}_3$. By the same logic, we determine that either $(1,0)\mapsto (1,3);\,(0,1)\mapsto (1,1)$ OR $(1,0)\mapsto (1,3);\,(0,1)\mapsto (1,3)$. The map $(1,0)\mapsto (1,1);(0,1)\mapsto (1,3)$ gives rise to an isomorphic group. The corresponding presentations are: $$
\begin{split}
<a,b,c,d
&:a^5=b^{13}=c^3=d^3=e,ab=ba,cd=dc,ac=ca,bc=cb,ad=da\\
&;dbd^{-1}=b^3>,
\end{split}
$$
(namely, $a$ commutes with everything, but $b$ only commutes with $a$ and $c$) and
$$
\begin{split}
<a,b,c,d
&:a^5=b^{13}=c^3=d^3=e,ab=ba,ac=ca,ad=da,cd=dc\\
&;cbc^{-1}=b^3,dbd^{-1}=b^3>.
\end{split}
$$ Here is where I have the problem (by cheating, shamefully): These two better be isomorphic! But I don't see how they can be isomorphic... Any help?","['semidirect-product', 'group-theory', 'abstract-algebra', 'finite-groups']"
2869913,Use De Moivre–Laplace to approximate $1 - \sum_{k=0}^{n} {n \choose k} p^{k}(1-p)^{n-k} \log\left(1+\left(\frac{p}{1-p}\right)^{n-2k}\right)$,"I am trying to use De Moivre–Laplace theorem to approximate 
$$1 - \sum_{k=0}^{n} {n \choose k} p^{k}(1-p)^{n-k} \log\left(1+\left(\frac{p}{1-p}\right)^{n-2k}\right)$$ The idea of an approximation is that we don't have the sum term which is difficult to calculate if $n$ is high. Using the  De Moivre–Laplace theorem gets us that: $${n \choose k} p^{k}(1-p)^{n-k} \approx \frac{1}{\sqrt{2 \pi np(1-p)}}e^{-\frac{(k-np)^2}{2np(1-p)}}$$
Now we see that
\begin{align}
F &= 1 - \sum_{k=0}^{n} {n \choose k} p^{k}(1-p)^{n-k} \log\left(1+\left(\frac{p}{1-p}\right)^{n-2k}\right) \\&\approx 1 - \int_{-\infty}^{\infty} \frac{1}{\sqrt{2 \pi np(1-p)}}e^{-\frac{(x-np)^2}{2np(1-p)}}\log_2\left(1+\left(\frac{p}{1-p}\right)^{n-2x}\right) dx
\end{align} my calculation is inspired by Entropy of a binomial distribution If one has an other suggestion to approximate $F$ or get a closed for i would like to hear those. So far i've tried approximating $F$ with a least squares method using a tanh function as the fit function.","['integration', 'central-limit-theorem', 'calculus', 'sequences-and-series', 'probability']"
2869928,Definition of the Weil group: Question about exact sequence with Inertia Group and absolute Galois group over a local field,"Let $K$ be a local field, $k$ be its residue field, $G_K, G_k$ be the absolute Galois groups of $K, k$ and $I_K$ be the inertia group of $K$. In several books and papers, I found the following exact sequence: $$ 1 \longrightarrow I_K \longrightarrow \ G_K \longrightarrow G_k \longrightarrow 1. $$ Now I would like to know why because my motivation is it to understand the definition of the Weil group $W_K$. If I recall correctly, this is defined by the exact sequence $$ 1 \longrightarrow I_K \longrightarrow W_K \longrightarrow \mathbb{Z} \longrightarrow 0,$$ but I did not understand every detail of it. First, let me assemble what I understood (or not understood entirely): I heard that $G_k$ is isomorphic to $\hat{\mathbb{Z}} = \varprojlim\limits_n \mathbb{Z}/n\mathbb{Z}$. I think this follows from the fact that $k$ is a finite field and the absolute Galois group is a profinite group but I was not yet able to put these facts together. Anyway, I think that $\mathbb{Z}$ is a subgroup of $\hat{\mathbb{Z}}$ but I do not know how the embedding would look like. One defines $W_K$ to be the inverse image of $\mathbb{Z}$ under the map $\phi: G_K \to \hat{\mathbb{Z}} $. I know that $I_K = \{ \sigma \in G_K : \sigma(x) \equiv x \mod{\mathcal{P}_K} \: \forall x \in \mathcal{O}_K \} $ where $\mathcal{P}_K = \{ x \in K : |x|<1 \}$ denotes the unique maximal ideal over $K$ and $\mathcal{O}_K$ denotes the ring of integers over $K$. As the first sequence is exact, this must mean that $I_K$ is the kernel of some map $G_K \to G_k$ resp. $G_K \to \hat{\mathbb{Z}}$. But I do now know how this map would look like. How does the Frobenius automorphism comes into play? The Frobenius automorphism is the map $\operatorname{Frob}_k \in G_k$ with $\operatorname{Frob}_k : x \mapsto x^q $ where $q$ is the size of the finite field $k$. Could you please explain this to me? Thank you!","['algebraic-number-theory', 'galois-theory', 'local-field', 'abstract-algebra', 'group-theory']"
2870000,What is the intuition behind conditional expectation in a measure-theoretic treatment of probability?,"What is the intuition behind conditional expectation in a measure-theoretic sense, as opposed to a non-measure-theoretic treatment? You may assume I know: what a probability space $(\Omega, \mathcal{F}, \mathbb{P})$ refers to probability without measure theory really well (i.e., discrete and continuous random variables) what the measure-theoretic definition of a random variable is that a Lebesgue integral has something to do with linear combinations of step functions, and intuitively, it involves partitioning the $y$-axis (as opposed to the Riemann integral, which partitions the $x$-axis). I haven't had time to learn measure-theoretic probability lately due to graduate school creeping in as well as other commitments, and conditional expectation is often covered as one of the last topics in every measure-theoretic probability text I've seen. I have seen notations such as $\mathbb{E}[X \mid \mathcal{F}]$, where I assume $\mathcal{F}$ is some sort of $\sigma$-algebra - but of course, this looks very different from, say, $\mathbb{E}[X \mid Y]$ from what I saw in my non-measure-theoretic treatment of probability, where $X$ and $Y$ are random variables. I was also surprised to see that one book I have ( Essentials of Probability Theory for Statisticians by Proschan and Shaw (2016)), if I recall correctly, explicity states that conditional expectation is defined as a conditional expectation, rather than the conditional expectation, which implies to me that there's more than one possible conditional expectation when given two pairs of random variables. (Unfortunately, I don't have the book on me right now, but I can update this post later). The Wikipedia article is quite dense, and I see words such as ""Radon-Nikodym"" which I haven't learned yet, but I would at least like to get an idea of what the intuition of conditional expectation is in a measure-theoretic sense.","['conditional-expectation', 'measure-theory', 'probability-theory', 'intuition']"
2870056,Computing an infinite limit involving a double integral,"$$\lim_{T\to \infty}\frac{\int_0^T\cos^2(s) \exp(-s)\int_0^s \cos(\cos a)\exp(a)\,\mathrm{d}a\,\mathrm{d}s}{T}$$ I tried computing this limit in maple but got the answer: 'undefined'. How to compute this limit, I accept special functions.","['limits', 'special-functions']"
2870101,Contraction of Tensors is Independent of the Choice of Basis,"Definition: Let $ T:(V^*)^k \times V^l \rightarrow \mathbb{R}$ be a tensor of type $ (k,l)$. Let $ \{v_1,...,v_n\}$ be a basis of $V$ and $ \{v^{1^*},...,v^{n^*}\}$ be the corresponding dual basis. The contraction of $ T$ with respect to the $ i$th (dual vector) and $j$th slot (which is a tensor of type $(k-1,l-1)$) is given by $\displaystyle CT=\sum_{k=1}^n T(\bullet,...,v^{k^*},...,\bullet; \bullet,...,v_k,...,\bullet)$ where the substitutions are done at the $i$th dual vector slot and the $j$th vector slot. Theorem: The contraction of a tensor is well-defined, i.e. it is independent of the choice of the basis of $V$. Proof: Recall that if $v \in V$, $w \in V^*$, then we have $$\displaystyle v=\sum_{k=1}^n v^{k^*}(v)v_k$$ $$\displaystyle w=\sum_{k=1}^n w(v_k)v^{k^*}$$ Let $\{v_1',...,v_n'\}$ be another basis of $V$ and $\{v^{{1^*}'},...,v^{{n^*}'}\}$ be the corresponding dual basis. Then we have $$\displaystyle v_i'=\sum_{k=1}^n v^{k^*}(v_i')v_k$$ $$\displaystyle v^{{i^*}'}=\sum_{k=1}^nv^{{i^*}'}(v_k)v^{k^*}$$ Hence, we get $$\begin{aligned} \displaystyle \sum_{k=1}^n T(\bullet,...,v^{{k^*}'},...,\bullet; \bullet,...,v_k',...,\bullet) &=\sum_{k=1}^n T(\bullet,...,\sum_{j=1}^nv^{{k^*}'}(v_j)v^{j^*},...,\bullet; \bullet,...,\sum_{p=1}^n v^{p^*}(v_k')v_p,...,\bullet)\\&= \sum_{k=1}^n\sum_{p=1}^n\sum_{j=1}^n v^{{k^*}'}(v_j)v^{p^*}(v_k')T(\bullet,...,v^{j^*},...,\bullet; \bullet,...,v_p,...,\bullet)\\&=\sum_{k=1}^n\sum_{p=1}^n\sum_{j=1}^n v^{p^*}(v^{{k^*}'}(v_j)v_k')T(\bullet,...,v^{j^*},...,\bullet; \bullet,...,v_p,...,\bullet)\end{aligned}$$ I have no idea how to proceed. Can anyone give me some suggestions? Thanks.","['multilinear-algebra', 'tensors', 'linear-algebra', 'differential-geometry']"
2870108,Software for computation of Ricci tensor on a Kähler manifold,"I have a metric on a Kähler manifold and want to compute the Ricci tensor. There are packages for Mathematica to do this on a real manifold, but I have not found any software to do explicit calculations with say complex differential forms.","['math-software', 'kahler-manifolds', 'differential-geometry']"
2870136,Munkres-Analysis on Manifolds: Theorem 20.1,"I am studying Analysis on Manifolds by Munkres. I have a problem with a proof in section 20: It states that: Let $A$ be an $n$ by $n$ matrix. Let $h:\mathbb{R}^n\to \mathbb{R}^n$ be the linear transformation $h(x)=A x$. Let $S$ be a rectifiable set (the boundary of $S=BdS$ has measure $0$) in $\mathbb{R}^n$. Then $v(h(S))=|\det A|v(S)$ ($v=$volume). The author starts his proof by considering tha case of $A$ being a non-singular matrix (invertible).
I think I understand his steps in that case (I basically had to prove that $h(int S)=int$ $h(S)$ and $h(S)$ is rectifiable, if anybody knows a way this statements are proven autumatically please tell me). He proceeds by considering the case where $A$ is singular, so $\det A=0$. He tries to show now that $v(T)=0$. He states that since $S$ is bounded so is $h(S)$ (I think thats true because $|h(x)-h(a)|\leq n|A||x-a|$ for each $x$ in $S$ and fixed a in $S$, if there is again a better explanation please tell me). Then he says that $h(\mathbb{R}^n)=V$ with $\dim V=p<n$ and that $V$ has measure $0$ (for each $ε>0$ it can be covered by countably many open rectangles of total volume less than $ε$), a statemant that I have no clue how to prove. Then he says that the closure of $h(S)=cl(h(S))$ is closed and bounded and has neasure $0$ (of course $cl((h(S))$ is closed but why is it bounded with measure $0$?). Then makes an addition step (which I understand) and proves the theorem for that case too. Cound someone help me clarify the points of the proof that I don't understand? Thank you in advance!","['lebesgue-measure', 'determinant', 'multivariable-calculus', 'linear-algebra', 'linear-transformations']"
2870139,Is there another way of evaluating $\lim_{x \to 0} \Gamma(x)(\gamma+\psi(1+x))=\frac{\pi^2}{6}$,"I was messing around with the Zeta-Function and I got what I thought was an interesting limit: $$\lim_{x \to 0} \Gamma(x)(\gamma+\psi(1+x)) = \frac{\pi^2}{6} $$
Where $\Gamma$ is the gamma function, $\gamma $ is the Euler-Mascheroni constant, and $\psi$ is the digamma function.
I got it by writing the Zeta Function as: $$\sum_{n=1}^{\infty} \frac{1}{n^2} = -\int_{0}^1 \frac{\ln(x)}{1-x}dx$$
Then using the beta function and differentiating with respect to x you get the limit. What other ways can be used to evaluate the limit? Here's how I got to my answer: We know
$$\sum_{n=1}^{\infty}\frac{1}{n^2} = \frac{\pi^2}{6} $$
To convert the sum into an integral consider:
$$\int_{0}^1 x^{n-1}dx=\frac{1}{n}$$
Differentiating once:
$$\int_{0}^1 \ln(x)x^{n-1}dx=\frac{-1}{n^2}$$
Plugging in the integral into the sum you get:
$$-\sum_{n=1}^\infty \int_0^1 \ln(x)x^{n-1}dx=-\int_0^1\ln(x)\sum_{n=1}^\infty  x^{n-1}dx=-\int_0^1 \frac{\ln(x)}{1-x}dx$$ Euler's Beta Function is defined as: $$\operatorname{B(x,y)}=\int_0^1 t^{x-1}(1-t)^{y-1} = \frac{\Gamma(x)\Gamma(y)}{\Gamma(x+y)}$$ Differentiation with respect to x:
$$\operatorname{B_x(x,y)}=\int_0^1 \ln(t)t^{x-1}(1-t)^{y-1} = \frac{\Gamma(x)\Gamma(y)}{\Gamma(x+y)}(\psi(x)-\psi(x+y))$$ $$-\int_0^1 \frac{\ln(x)}{1-x}dx = -\operatorname{B_x(1,0)}=\frac{\pi^2}{6} $$ Taking the limit as (x,y)->(1,0) $$-\lim_{(x,y)\to(1,0)} \frac{\Gamma(x)\Gamma(y)}{\Gamma(x+y)}(\psi(x)-\psi(x+y))=-\lim_{y \to 0} \Gamma(y)(\psi(1)-\psi(1+y)) = \lim_{y \to 0} \Gamma(y)(\gamma+\psi(1+y))$$","['integration', 'summation', 'gamma-function', 'riemann-zeta', 'limits']"
2870146,"Prove geometrically that $\sum\limits_{n=1}^\infty\,\dfrac{1}{n(n+1)}=1$.","$\mathrm{S_1}$ and $\mathrm{S_2}$ are two circles of unit radius touching at point $P$; $T$ is a common tangent to $\mathrm{S_1}$ and $\mathrm{S_2}$ touching them at $X$ and $Y$; $\mathrm{C}{_1}$ is the circle touching $\mathrm{S_1}$, $\mathrm{S_2}$, and $T$. $\mathrm{C}{_n}$ is the circle touching $\mathrm{S_1}$ , $\mathrm{S_2}$, and  $\mathrm{C}{_{n-1}}$ for $n > 1.$ By computing diameters of $\mathrm{C}{_n}$ prove that $$\frac{1}{1\cdot 2}+\frac{1}{2\cdot 3}+\frac{1}{3\cdot 4}+\ldots =1\,.$$","['tangent-line', 'circles', 'geometry', 'sequences-and-series', 'convergence-divergence']"
2870157,Combining Matrices?,"Let’s say there are two matrices $A$ and $B$ where
$$A=\begin{bmatrix}a&b\\b&a\end{bmatrix}$$
$$B=\begin{bmatrix}c&d\\d&c\end{bmatrix}$$ $A$ and $B$ together make up a third matrix $C$ where
$$C=\begin{bmatrix}A&B\\B&A\end{bmatrix}=\begin{bmatrix}\begin{bmatrix}a&b\\b&a\end{bmatrix}&\begin{bmatrix}c&d\\d&c\end{bmatrix}\\\begin{bmatrix}c&d\\d&c\end{bmatrix}&\begin{bmatrix}a&b\\b&a\end{bmatrix}\end{bmatrix}$$ My question is of syntax. Specifically, did I define $C$ using $A$ and $B$ properly? It seems ambiguous to me in the sense of $C$ being a matrix of matrices and not of $A$ and $B$ ‘s elements; when I in fact want $C$ to be a matrix of the elements and not of the matrices. Edit: it looks like I have described a block matrix.",['matrices']
2870172,How to check for local extrema or saddle point given an semidefinite matrix,"I've computed the Hessian of a given function $f(a,b,c) = y-a\sin(bx-c)$ and got the following result: $\begin{pmatrix}
0 & -x\cdot\cos(bx - c) & \cos(bx - c)  \\
-x\cdot\cos(bx - c) & ax^2\cdot\sin(bx - c) & -ax\sin(bx - c) \\
\cos(bx - c) & -ax\cdot\sin(bx - c) & a\cdot\sin(bx - c)
\end{pmatrix}$ This matrix is positive semi-definite and thus one can not state for a given point $P=(a_i,b_i,c_i)$ if it is a local min, max or a saddle point. Is there any other way to explicitly determine if we have a loc. min, max or saddle point?","['hessian-matrix', 'positive-semidefinite', 'analysis']"
2870180,Shorter Way to Solve $\lim_{x\to 0} \frac{x^3-\ln^3(1+x)}{\sin^2x-x^2}$,"The limit to find is
$$\lim_{x\to 0} \frac{x^3-\ln^3(1+x)}{\sin^2x-x^2}$$ What I've tried was using the factorisation for $a^3-b^3$ and $a^2-b^2$ like so: $$\lim_{x\to 0}\frac{(x-\ln(1+x))(x^2+x\ln(x+1)+\ln^2(x+1))}{(\sin x-x)(\sin x+x)}$$ but I don't know how to make a product of this so I get something meaningful (like a standard limit or something).
Lastly I've tried l'Hospital's rule and it works in the end, but after the second time the numerator and denominator are pretty long. Is there maybe a shorter solution to this?",['limits']
2870219,Show that a vector field is not conservative (example),"Let $\Omega=\mathbb{R^2}\smallsetminus\{(0,0)\}$ and $$\vec{F}(x,y)=-\frac{y}{x^2+y^2}\vec{i}+\frac{x}{x^2+y^2}\vec{j}$$ To show that the vector field $F$ is not conservative $$\vec{\nabla}\times \vec{F}=0\,\vec{i}+0\,\vec{j}+\bigg(\frac{y^2-x^2}{(x^2+y^2)^2}-\frac{y^2-x^2}{(x^2+y^2)^2}\bigg)\vec{k}=\vec{0}$$ which indicates that it is conservative(?). Alternatively, the scalar function $$f(x,y)=-\arctan\bigg(\frac{x}{y}\bigg)$$ is the potential function of the field $F$. If $$\int_C\vec{F}\cdot\mathrm{d}\vec{r}\neq f\big(x(\beta),y(\beta)\big)-f\big(x(\alpha),y(\alpha)\big)$$ $\big($where $r(t)$ is the parametrization of an arbitrary curve $c$ and $\alpha$ and $\beta$ are its starting and end point respectively$\big)$ then the fundumental theorem for line integrals is not satisfied and thus $F$ is not conservative.
I tried using $r(t)=t\vec{i}+t\vec{j}, \space t\in[\alpha,\beta]$, but it didn't work. What curve would be a better choice for $C$ and what's the deal with $\mathrm{rot}\,F$ being zero?","['integration', 'multivariable-calculus', 'vector-analysis']"
2870237,One-forms are dual to tangent vectors,"In my class it was said that ""A tangent vector $X \in T_p(\mathbb{R}^n)$ acts on a one-form to give a real number"" and ""A one-form acts on a tangent vector to give a real number"" Now the 'tangent space' $T_p(\mathbb{R}^n)$ is a $n$-dimensional vector space and the elements of $T_p(\mathbb{R}^n)$ which we call tangent vectors are actually derivations, which are linear maps $w : C^{\infty}(\mathbb{R}^n) \to \mathbb{R}$ satisfying a product rule. One-forms are elements of the dual vector space $T_p^*(\mathbb{R}^n)$, which we call the cotangent space. They are by definition of a dual vector space, linear maps from $T_p(\mathbb{R}^n)$ to $\mathbb{R}$, e.g $f : T_p(\mathbb{R}^n) \to \mathbb{R}$. From this it is easy to see that a one-form takes as input a tangent vector and outputs a real number. However I'm having trouble seeing how a tangent vector (derivation) takes as input a one-form to output a real number since it's domain isn't even $T_p^*(\mathbb{R}^n)$.","['multivariable-calculus', 'vector-spaces', 'differential-geometry']"
2870306,How does $\mathbb R^n\setminus \{0\}$ being simply connected follow as a corollary from $\mathbb S^{n-1}$ being a strong deformation retract?,"From Introduction to Topological Manifolds by Lee: We know $\mathbb S^{n-1}$ is simply connected for $n\ge 3$. We know that since  $\mathbb S^{n-1}$ is a strong deformation retract of $\mathbb R^n\setminus \{0\}$, then $r \circ \iota = \mathrm{Id}_{\mathbb S^{n-1}}$ and $\iota \circ r \simeq \mathrm{Id}_{\mathbb R^n\setminus \{0\}}$, where $\iota:\mathbb S^{n-1} \hookrightarrow \mathbb R^n\setminus \{0\}$ is inclusion. We do not know homotopy invariance of $\pi_1$. So, how does the corollary follow from the proposition?","['proof-explanation', 'general-topology', 'homotopy-theory', 'algebraic-topology']"
2870314,Finding a set of continuous functions with a certain property [duplicate],"This question already has answers here : Finding a set of continuous functions with a certain property 2 (4 answers) Closed 5 years ago . I need help finding the set of continuous functions $f : \Bbb R \to \Bbb R$ such that for all $x \in \Bbb R$, the following integral converges: $$\int_0^1 \frac {f(x+t) - f(x)} {t^2} \ \mathrm dt$$ I am thinking it could be the set of constant functions but i havent been able to prove it :( 
 I have also noticed that you can kind of  take any two functions and stick them together (continuously extend one into the other) the resulting function verifies the property in question. I hope you can provide some insight and thank you .","['general-topology', 'real-analysis']"
2870327,Upper and Lower bounds on the nth prime number,"I know from papers like Dusarts's that $$n\left(\ln n +\ln \ln n -1+ \frac{\ln \ln n -2}{\ln n}-\frac{\ln^2 \ln n -6 \ln \ln n +12}{2 \ln^2 n}\right) \leq p_n \leq n\left(\ln n +\ln \ln n -1+ \frac{\ln \ln n -2}{\ln n}-\frac{\ln^2 \ln n -6 \ln \ln n +10}{2 \ln^2 n}\right) $$ But could any one give and explicit number $n_0$ for which for all $n \geq n_0$ the above inequalities hold true ? It does not have to be the smallest one, also if there is way, could you please show me how to get this number I read a paper that estimate that the $n_0 \approx 3.9*10^{31}$ under the R.H.
if we don't assume R.H. could we still have $n_0 \ll 10^{100}$ Thanks in advance","['number-theory', 'prime-numbers']"
2870328,Convergence of a family of series,"I want to show that for $D>0, D\equiv 0,1 \pmod4$ , $z \in \mathbb{C}$ with $ \operatorname{Im}(z)>0$ and $k>1$ the sum $$\sum_{a,b,c \in \mathbb{Z} \\ b^2-4ac=D}(az^2+bz+c)^{-k}$$ converges absolutely.","['complex-analysis', 'convergence-divergence', 'sequences-and-series']"
2870330,Random walk probability,"I encounter a problem:Consider a two dimensional map, with an x-axis (horizontal direction) and a y-axis (vertical direction). Coordinates on the map can therefore be represented as a two-dimensional vector (x, y).
A tourist is standing at coordinates (0, 0), and is looking for the the tourist information centre, located at coordinates (−10, 30). However, the tourist is completely lost and instead of asking for directions, begins a random walk to search for the tourist information centre.
The tourist moves one step at a time, either horizonally or vertically (but can not move diagonally). Steps can also be forwards (in a positive direction) or backwards (in a negative direction). Therefore, at every point, there are 4 possible moves the tourist can make. For instance, when standing at the origin (0,0), the tourist can move either to (0, 1), (0, −1), (1, 0) or (−1, 0), and has an equal probability of moving in each direction, thus a probability of 0.25 for each option. What is the probability that this tourist locates the tourist office in 1000 steps
or less? Aaron Montgomery gave the hint that the simulation is a good way to estimate the probability. Could any expert give a full code, like the C++ or R code, to help us better understand this kind of question? Thanks in advance.","['random-walk', 'probability']"
2870336,Are matrices which yield a given characteristic polynomial and have specified structure connected?,"Let us consider a subset $S$ of $M_4(\mathbb R)$ which has following form
\begin{align*}
\begin{pmatrix}
0 & * & 0 & * \\
1 & * & 0 & * \\
0 & * & 0 & * \\
0 & * & 1 & *
\end{pmatrix},
\end{align*}
where $*$ can assume any real number. It is also clear for any monic $4^{th}$ degree real polynomial, we can at least find one realization in $S$ since the upper left block and lower right block can be considered as in companion form. Let $f: S \to \mathbb R^n$ be the map sending the coefficients of characteristic polynomial to $\mathbb R^n$. My question is: suppose we have a square-free polynomial $p(t) = t^4 + a_3 t^3 + a_2 t^2 + a_1 t + a_0$, is $f^{-1}(a)$ a connected set in $M_n(\mathbb R)$ where $a=(a_3, a_2, a_1, a_0)$? If we let $C$ denote the companion form of $p(t)$, which is an element of $S$, then $$f^{-1}(a) = \{V C V^{-1}: V \in GL_4(\mathbb R), V C V^{-1} \text{ is in above form }\}.$$
I have a feeling we can first choose a realization in block form,
\begin{align*}
\begin{pmatrix}
0 & * & 0 & 0 \\
1 & * & 0 & 0 \\
0 & 0 & 0 & * \\
0 & 0 & 1 & *
\end{pmatrix},
\end{align*}
and then continuously change everything in $f^{-1}(a)$ into this form without changing the characteristic polynomial. Indeed, if we consider a matrix
\begin{align*}
A = \begin{pmatrix}
0 & -b_1 &  0 & -c_1 \\
1 & -b_2 &  0 & -c_2 \\
0 & -b_3 & 0 & -c_3 \\
0 & -b_4 & 1  & -c_4 \\
\end{pmatrix},
\end{align*}
then the charateristic polynomial is
\begin{align*}
t^4 + (c_4+b_2)t^3 + (c_3 + b_1 + b_2c_4 - b_4 c_3)t^2 + (b_1 c_4 - b_3 c_3 + b_2 c_3 - b_4 c_1) t + (b_1 c_3 - b_3 c_1).
\end{align*}
I am thinking we should be able to continuously decrease $b_3, b_4, c_1, c_2$ to $0$ while keeping the polynomial unchanged by changing $b_1, b_2, c_3, c_4$ accordingly.","['connectedness', 'matrices', 'linear-algebra', 'polynomials', 'general-topology']"
2870346,On the eigenvalues of a block-symmetric matrix,"I'm trying to find some kind of property that can tell me something about the invertibility of the matrix
$$ M = \begin{bmatrix} A & B \\ -B^{T} & A \end{bmatrix}$$ Where both $A$ and $B$ are square-invertible (but not symmetric) and the eigenvalues of $A$ are pure-imaginary (I don't know if this is a plus or not). Is there any ""easy"" condition to say that $M$ is going to be invertible? Thanks in advance!","['matrices', 'inverse', 'linear-algebra', 'linear-transformations', 'symmetric-matrices']"
2870352,Examples of the Carathéodory extension theorem,What are examples of the Carathéodory Extension Theorem that is not Lebesgue measure or Hausdorff measure?,"['measure-theory', 'analysis', 'real-analysis']"
2870358,$(x)$ prime ideal in $R[x]$ iff $R$ integral domain by contrapositive,"I've done this proof a few ways and I like this one but since it wasn't the ""official"" one, I wanted to ask if anyone sees a reason it's invalid. It just makes more sense to me on a concrete level. Thanks! Statement: $R$ is a commutative ring with unity. Show that (1) $(x)$ is a prime ideal in $R[x]$ if and only if (2) $R$ is an integral domain. Proof: By contrapositives. NOT (2) $\implies$ NOT (1). Assume $R$ is not an integral domain; i.e. there exist some zero divisors, nonzero $a, b \in R$ such that $ab = 0$. Then consider a counterexample using $p(x) = x+a$ and $q(x) = x + b$ which are both not in the ideal $(x)$, but 
$$
p(x)q(x) = (x+a)(x+b) = x^2 + (a+b)x + ab  = x(x + a+b) \in (x).
$$ 
Thus $(x)$ is not a prime ideal. NOT (1) $\implies$ NOT (2). Assume $(x)$ is not a prime ideal in $R[x]$; i.e. there exist some $f(x), g(x) \in R[x]$ such that neither are in $(x)$ but $f(x)g(x) \in (x)$. To be specific, 
$$ f(x) = a_n x^n + ... + a_1 x + a_0, \quad a_0 \neq 0,$$
$$ g(x) = b_n x^n + ... + b_1 x + b_0, \quad b_0 \neq 0,$$
but the term $a_0b_0$ in $f(x)g(x)$ is $0$. Then since $a_0, b_0 \in R$, they are zero divisors in $R$ and $R$ cannot be an integral domain. Basically I was looking for a way around using the intermediate $R[x]/(x) \cong R$ theorem because although I know it and can prove it, I'm paranoid I won't have time on my qual, and I don't want to just use it without justification so counterexamples seemed faster. If I've broken logic somewhere please let me know!","['maximal-and-prime-ideals', 'integral-domain', 'ring-theory', 'abstract-algebra', 'ideals']"
2870376,Question based on a continuous function [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Let $f(x)$ be a continuous and differentiable function such that $f(-1)=-2,f(0)=-1,f(1)=0,f(2)=1,f(3)=0,f(4)=-1$ and $f(5)=1$.Then what is the minimum number of roots of $g(x)=f'(x)+xf''(x)=0$, in the interval $[-1,5]$. Any hint or clue will be appreciated.","['continuity', 'derivatives']"
2870385,Relation between area and perimeter of an ellipse in terms of semi-major and semi-minor axes.,"We know the perimeter of a circunference (in terms of its radius $r$), $2 \pi r$, can be obtained by differentiating its area (with respect to its radius), $\pi r^2$. This fact generalizes to higher dimensions. Using partial derivatives, is there something similar if we consider: i) an ellipse ($\frac{x^2}{a^2} + \frac{y^2}{b^2} =1$) instead of a circunference and ii) its semi-major and semi-minor axes $a$ and $b$ instead of the radius $r$? Does it generalize to higher dimensions?","['multivariable-calculus', 'geometry', 'differential-geometry']"
