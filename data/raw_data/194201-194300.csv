question_id,title,body,tags
3730460,Prove that $\frac{1}{a^2}+\frac{1}{(a+1)^2}+\frac{1}{(a+2)^2}+\dotsm\infty=\frac{1}{a}+\frac{1}{2a(a+1)}+\frac{2!}{3a(a+1)(a+2)}+\dotsm\infty$,"Question:-  Prove that $$\frac{1}{a^2}+\frac{1}{(a+1)^2}+\frac{1}{(a+2)^2}+\dotsm\infty=\frac{1}{a}+\frac{1}{2a(a+1)}+\frac{2!}{3a(a+1)(a+2)}+\dotsm\infty$$ Nothing is mentioned in question about nature of $a$ I write it in summation form,but I got stuck and unable to proceed further. $$\sum_{k=0}^{\infty}\frac{1}{(a+k)^2}=\sum_{n=0}^{\infty}\frac{n!}{(n+1)\prod_{k=0}^{n}(a+k)}$$ Then I take all the terms to LHS in hope that terms may cancel out each other to give zero but that also doesn't help me since with each term degree of both numerator and denominator increases. Can anybody help me to Prove the result!!","['calculus', 'sequences-and-series']"
3730470,How is the chi-square confidence interval derived from the inverse gamma function?,"I had to derive the chi-squared confidence intervals for a AR(1) red noise model generated theoretically to fit the power spectra of a time series. The shape function of the power spectra of the red noise model is given by $$\frac{1-\rho^2}{1-2\rho\cos\frac{k\pi}{f_s}+\rho^2}$$ However, to derive the $95\%$ and $99\%$ confidence intervals I had to first do this (python 3.7): Ci95=(2*sc.gammaincinv(nw2/2, 0.95))/nw2 Ci99=(2*sc.gammaincinv(nw2/2, 0.99))/nw2 where nw2 is, I figured out (and please correct me if I am wrong), the degrees of freedom. Then I had to multiply the theoretically derived red noise spectra with Ci95 and Ci99 to get the confidence levels. Can anyone please explain the connection between the chi-square confidence intervals and the inverse gamma function? I had looked up for the connection between chi-square and gamma function, but none actually touched upon this particular aspect.","['statistics', 'confidence-interval', 'gamma-function', 'chi-squared', 'gamma-distribution']"
3730498,zero-one-law in percolation theory,"I am currently working on a paper by Lyons from 1990 which can be found at https://projecteuclid.org/download/pdf_1/euclid.aop/1176990730 . In chapter 6 Percolation (page 951, 21 respectively) the basic setting is given: We have $\Gamma$ a countable graph and $p\in[0,1]$ . Every edge is removed with probability $1-p$ independently of the other edges. The random graph which is left after removing is denoted by $\Gamma(\omega_p)$ , where $\omega_p$ is a point in a probabilty space $\Omega_p$ . I think it is something like $\{0,1\}^{(\operatorname{edges} \operatorname{of} \Gamma)}$ , e.g. a $0/1$ -vector indexed by the edges of the graph $\Gamma$ , where $1$ stands for ""edge is there"" and $0$ stands for ""edge has been removed"". Now, for any vertex $\sigma \in \Gamma$ , we denote by $\Gamma_\sigma(\omega_p)$ the connected component of $\sigma$ in $\Gamma(\omega_p)$ , which is the subgraph containing $\sigma$ , where any two vertices are connected to each other by a path. Now Lyons states: By the zero-one-law, the probabilty that $\Gamma_\sigma(\omega_p)$ is infinite for some $\sigma \in \Gamma$ is either $0$ or $1$ . My question now is, which zero-one-law is meant here and how does it apply? I first thought about the Borel-Cantelli Lemma, which gives a statement about the probability of the limes superior of a sequence of events. So let $\sigma \in \Gamma$ . Now as a sequence of events I thought about taking the events $A_1,...$ where $A_n$ stands for the event that the connected component of $\sigma$ has cardinality $n$ , e.g. $|\Gamma_\sigma(\omega_p)| = n$ . In general, the probability of having a component of size $n$ is the probability of having an edge to the power of the probability, that an edge is not removed, namely $p^n$ . Is this right so far? If yes, then Borel-Cantelli will give me that the probability that $\Gamma_\sigma(\omega_p)$ is infinite for some $\sigma \in \Gamma$ is $0$ , because of the geometric series $\sum_{n=1}^\infty P(A_n) = \sum_{n=1}^\infty p^n < \infty$ . But what about the probability that $\Gamma_\sigma(\omega_p)$ is infinite for some $\sigma \in \Gamma$ being $1$ ? Therefore I would need an independent sequence of events $A_1,...$ with $\sum_{n=1}^\infty P(A_n) = \infty$ , but in my case I wont have that, so I conclude that I have a basic misunderstanding of the scenario or the construction of the events $A_n$ . Another zero-one-law which came in my mind is the one of Kolmogorow, where I need a sequence of sigma-algebras. Do you have some ideas on which and how a zero-one-law applies in the probability that $\Gamma_\sigma(\omega_p)$ is infinite for some $\sigma \in \Gamma$ ?","['random-graphs', 'probability-limit-theorems', 'graph-theory', 'borel-cantelli-lemmas', 'probability-theory']"
3730521,Is it true that there is an interesting entry in each row of a matrix with nonzero determinant? [duplicate],"This question already has answers here : Can the determinant of a matrix can be made $0$? (4 answers) Closed 4 years ago . We call an entry of an $ n × n $ matrix with nonzero determinant interesting, if by changing this entry
(and only this) the determinant of the matrix can be made $0$ .
Is it true that there is an interesting entry in each row of a matrix with nonzero determinant?
I know that each entry of every matrix with nonzero determinant is not interesting. However I could not find proof if this is a case in each row.",['linear-algebra']
3730531,Twisting prisms : Do all polygon prisms behave in the same manner?,"Defining the process of twisting a prism : Twisting the top face of a prism with no walls. The prism can show these two behaviors while getting twisted: An ideal prism (side length not constant) will simply have it's face twisted with no other change, A real scenario where the side length is constant and hence there is a slight compression perpendicular to the top face, For this post I am concerned about the second point i.e when the side length is constant. Some more examples I constructed: I am providing the link of a  google drive folder where I have uploaded the Geogebra files so you guys can experiment with them. As I was constructing these, I noticed that the length all the figures were getting compressed was equal (the polygons had equal radii (circum-radii), and the side length was also equal). I did it only till pentagon $>1$ . I hypothesize that it will be equal for every regular polygon given the radius and the side length are equal. Is my hypothesis correct? If yes how to prove it? I noticed another thing—every $180^\circ$ rotation resulted in the first intersection for every polygon prism not depending on the radius/side length. I tried thinking a lot about it but wasn't able to visualize it. Why does the first intersection happens after rotating $180^\circ$ ? My last but not the least question: How can we find the relation between the angle by which the top face gets twisted and the changing angle between the polygon side and the side length i.e. In the process of construction, I found out the locus of the vertices : taking the example of a square prism the vertex $\text{B}_1$ follows : $$x=\sqrt{l^2 - (r\cos (\phi + \pi /2)-h)^2 - (r\sin (\phi + \pi /2)-k)^2}-m \\ y=r\cos(\phi +\pi /2) \\ z=r\sin(\phi + \pi /2) \\ \text{the prism is along x axis}\\ \text{ $(m,h,k)$ are the $x$-, $y$-, and $z$-coordinates of $\text{A}_1$ respectively} \\ \text{$\phi$ is the angle by which the top face is getting rotated.} \\ \text{ $r,l$ are the radius and length of the prism respectively.}$$ Note that I have added a ' $+\pi /2$ ' in the angle to denote the initial coordinate of the vertex.","['geometry', '3d']"
3730550,"If $\sum_{i=k}^n {n \choose i} p^{i}(1-p)^{n-i} \approx 0.05$, how can we find $k$?","Let $n$ be any natural number, let $k\in\{0, \dots, n\}$ , and let $p \in [0, 1]$ . If $\sum_{i=k}^n {n \choose i} p^{i}(1-p)^{n-i} \approx 0.05$ , how can we find $k$ (in terms of $n$ and $p$ )?","['statistics', 'approximation', 'binomial-distribution', 'algebraic-equations', 'probability']"
3730567,Integrate $\int_{-\infty}^{\infty} \frac{e^{2020x}-e^{x}}{x\left(e^{2020x}+1\right)\left(e^x+1\right)} \mathop{dx}$,A challenge problem says integrate $$\int_{-\infty}^{\infty} \frac{e^{2020x}-e^{x}}{x\left(e^{2020x}+1\right)\left(e^x+1\right)} \mathop{dx}$$ I thought $u=-x$ helps but I get $I$ so it is even.  I also try partial fraction to $$\int_{-\infty}^{\infty} -\frac{1}{x\left(e^{2020x}+1\right)} + \frac{1}{x\left(e^{x}+1\right)} \mathop{dx}$$ Now what?  Help please thanks,"['integration', 'improper-integrals', 'definite-integrals', 'real-analysis', 'calculus']"
3730617,Method of Indicators Random Variables Problem and the Hiring Problem,"I'm trying to do the following problem from The Probability Tutoring Book by Carol Ash. Pick numbers at random between $0$ and $1$ . The $i^{th}$ number sets a ""record"" if it's larger than all of it's predecessors.
For example, the sequence $.1, .04, .3, .12, .6, .5$ has $3$ record setters $(.1, .3, .6)$ . We always take the first number to be a record setter. Find the expected number of record setters. This problem reminded me of the hiring problem which is set up like so: Each day a new candidate comes to interview for a job. We hire the $i^{\text{th}}$ person if that person is more qualified than everyone who came before. We interview for $n$ days. What is the expected number of people we hire? We can proceed to do the problem by reasoning like so: Let $X$ be the number of people we hire. $$X = X_1 + X_2 + ... + X_n$$ where $X_i = 1$ if we hire that person and $X_i = 0$ otherwise.
Well the probability that the best person is on the $i^{th}$ day is $\frac {1}{i}$ since all of the people have the same potential to be the most qualified candidate. Now this becomes a simple exercise in linearity of expectation : $$E[X] = \sum_i {E[X_i]} = \sum_i {\frac{1}{i}} = O(\log(n))$$ At first I thought this reasoning would easily apply to my ""record setting"" problem (which according to the solutions at the end of the book it does). However, something rubs me the wrong. Why doesn't the probability that $i^{th}$ number is the highest so far depend on what values the previous numbers are? For example, $$P(2^{\text{nd}}\ \text{number highest}) = 1 - n_1 $$ where $n_1$ is the first number we choose and $$P(i^{\text{th}}\ \text{number highest}) = 1 - \max(n_1, n_2, n_3, ..., n_{i-1}) $$ . Suddenly my reasoning for the hiring problem, ""all of the people have the same potential to be the most qualified candidate"" doesn't hold up. Can someone see where my logic went wrong? Any help would be helpful. :)","['random-variables', 'discrete-mathematics', 'probability', 'algorithms']"
3730695,Geometry Proof to Find Maximum area of $\triangle PIE$,"Circle $\omega$ is inscribed in unit square $PLUM,$ and points $I$ and $E$ lie on $\omega$ such that $U, I,$ and $E$ are collinear. Find, with proof, the greatest possible area for $\triangle PIE.$ I'm not sure if there is a solution possible without trigonometry. Also, for my diagram in my solution, I'm not sure how to center it. Sorry about that.","['trigonometry', 'geometry']"
3730702,"Evaluation of integral $\int_{S^2} \frac{dS}{((x-a)^2 +y^2+z^2)^{1/2}}$, where $a>1$ and $S$ is the unit sphere.",I want to evaluate $$\int_{S^2} \frac{dS}{((x-a)^2 +y^2+z^2)^{1/2}}$$ where $a >1$ and $S$ is the unit sphere. I'm not sure how to do this using only multivariable calculus techniques. My only idea was to use the fact that the function $(x^2+y^2+z^2)^{-1/2}$ is harmonic away from the origin and use the mean value formula.,"['integration', 'multivariable-calculus', 'spherical-coordinates']"
3730708,Prove that $\sum_nF_{2n+1}x^n=\frac{1-x}{1-3x+x^2}$,Prove that $$\sum_nF_{2n+1}x^n=\frac{1-x}{1-3x+x^2}$$ This is a problem of generating functions. I know that $\sum_nF_{2n}x^=\frac{x}{1-3x+x^2}$ and I am guessing this is a good point to start. But I don't really know where to go from here. Could someone help me?,"['fibonacci-numbers', 'discrete-mathematics', 'generating-functions']"
3730744,Dimension of union of two varieties,"Suppose $X$ and $Y$ are two varieties. By varieties, I mean affine varieties or quasi-affine varieties or projective varieties or quasi-projective varieties. Suppose Krull dimension of $X$ is $n$ and Krull dimension of $Y$ is $m$ and, without lost of generality, assume $n\leq m$ . Is it true that Krull dimension of $X\cup Y$ is $m$ ? Intuitively, if I add a point or a curve to a curve, it should still look like a curve. A complete proof or counter example would be greatly appreciated.","['algebraic-geometry', 'general-topology', 'abstract-algebra', 'commutative-algebra']"
3730784,Arrangements of BANANAS where the A's are separated,"How many arrangements of the word BANANAS are there where the $3$ A's are separated? I know that once chosen the places for the three A's, there are $\dfrac{4!}{2!}=12$ possible arrangements for the rest of the letters (we divide by $2!$ because there are $2$ N's). But I am having trouble with choosing the places for the A's. If I do this manually, I count $10$ different arrangements for the $3$ A's, and that would mean that there is a total of $12\cdot 10$ possible arrangements that fit the initial condition. However, I would like to learn to calculate the $10$ cases with a combinatorics argument, instead of just counting. Could someone help me?","['combinatorics', 'discrete-mathematics']"
3730786,"Prove Stirling's formula given that for $I_n = \int_0^{\pi/2} \sin^n\theta \, d\theta$ we have $I_{2n+1}/I_{2n} \rightarrow 1$","For $I_n = \int_0^{\pi/2} \sin^n\theta \, d\theta$ it is possible to show (using integration by parts and $\sin^2\theta = 1-\cos^2\theta$ ) that: $nI_n = (n-1)I_{n-2}$ We can then repeatedly apply this relation to $\frac{I_{2n+1}}{I_{2n}}$ to show: $$\frac{I_{2n+1}}{I_{2n}} = \frac{2^{4n+1}(n!)^4}{\pi (2n)!(2n+1)!}$$ This ratio converges to $1$ since $I_{2n+2}/I_{2n} \rightarrow 1$ (easy to show with the above recursive relation)  which is always larger than $I_{2n+1}/I_{2n}$ . Since $I_{2n}/I_{2n} = 1$ , we can sandwich the desired ratio. I also know that (from Show that $n!e^n/n^{n+1/2} \leq e^{1/(4n)}C$ ) that: $r_n = n!e^n/n^{n+1/2} \leq e^{1/(4n)}C$ for all $n$ and for $C = \lim_{n\rightarrow \infty} n!e^n/n^{n+1/2}$ . Now I want to show Stirling's formula: $$n! \sim \sqrt{2\pi}n^{n+1/2}/e^n$$ What I've tried Using the upper bound on $r_n$ it is clear that all that remains is to show that $C = \sqrt{2\pi}$ . I have seen trick on related problems involving expressing the integral $I_n$ in terms of the beta function, but I think another trick is needed as well (perhaps one involving introducing a term $e^{i\theta}$ but I am struggling to get anything more specific than that.","['integration', 'approximation', 'sequences-and-series']"
3730836,Is $C_0$ dense in $l^{\infty}$,"Is $C_0$ dense in $l^{p}$ with $1\leq p\leq \infty$ where $C_0=\{ (x_n): x_n\rightarrow 0, x_n\in R\}$ . Well I think that if $p<\infty$ is true because by definition if i take $y=(y_n)\in l^p$ then $\sum (y_n)^p <\infty$ so $(y_n)^p \rightarrow 0$ imply $y_n \rightarrow 0$ then I can choose the same $y_n \in C_0\cap l^\infty$ such that $y_n \rightarrow y_n$ and this is the definition of density, for all $z$ in the big set exist one succession $z_n$ in the small set such that $z_n\rightarrow z$ . But I don't know how to do with $p=\infty$ . Please somebody can you help me? Thank you","['analysis', 'real-analysis', 'calculus', 'lp-spaces', 'functional-analysis']"
3730887,Question about how to use Z-score to help us determine which test the student did better,"Here is the question:
A student scores 56 on a geography test and 267 on a mathematics test. The geography test has a mean of 80 and a standard deviation of 20. The mathematics test has a mean of 300 and a standard deviation of 22. If the data for both tests are normally distributed, on which test did the student score better? You can find the solution here: https://study.com/academy/answer/a-student-scores-56-on-a-geography-test-and-267-on-a-mathematics-test-the-geography-test-has-a-mean-of-80-and-a-standard-deviation-of-20-the-mathematics-test-has-a-mean-of-300-and-a-standard-deviati.html My problem:
Q1: I attempted the problem without looking at the answer. I do not understand why the solution use z-score here and not use the numbers: 56 and 261. Why can I just use the numbers 56 and 267? Since 267>56, the student did better in math.
\ Q2: I do not understand how the z scores help determine which test the student did better. I realize I do not understand z scores. Thank you so much.","['statistics', 'normal-distribution']"
3730907,$z^{\frac{4}{3}} = -2$ ; How to know which complex roots to keep from this equation,"So I recently came upon the following complex algebra problem: $$
z^{\frac{4}{3}} = -2
$$ So, to solve it I have to find the z values that solve the following: $$
z = (-2)^{\frac{3}{4}} 
$$ To do this I express -2 in exponential form: $$
z = (2e^{i(\pi + 2\pi n)})^{\frac{3}{4}} 
$$ Then, I solve for that trying for $n=0,1,2,3$ and I come up with 4 roots: $$
z_1 = 2^{\frac{3}{4}}e^{i\frac{\pi}{4}}
$$ $$
z_2 = 2^{\frac{3}{4}}e^{i\frac{3\pi}{4}}
$$ $$
z_3 = 2^{\frac{3}{4}}e^{i\frac{5\pi}{4}}
$$ $$
z_4 = 2^{\frac{3}{4}}e^{i\frac{7\pi}{4}}
$$ However, if I try to check these solutions for the original problem, only $z_2$ and $z_3$ succeed, while $z_1$ and $z_4$ do not solve the initial equation. Even plugging the original equation into Wolfram, gives me just those two roots. I have been thinking about this over and over and don't understand where I'm going wrong or what is it that I'm failing to consider. Does anybody have any idea of where I'm going wrong? Thank you in advance","['complex-analysis', 'algebra-precalculus', 'roots']"
3730931,Does anyone recall this linear algebra survey of results?,"I'm not sure if math stackexchange is the appropriate place for this, but I am looking for a paper that consisted of results in linear algebra, since I've lost the pdf of it and can't recall the author. It was a survey of results, with either proofs or published references, in the field of finite dimensional linear algebra. It assumed familiarity with the subject, but not expert knowledge, and was motivated as a presentation of nontrivial results that deserve to be known more broadly. There were lots of results proved within, but I can only recall one specifically, which hopefully is obscure enough to track down the paper. The result was (if I recall correctly) that a linear operator is similar to its inverse if and only if it is the product of two involutions. Hopefully someone recognises something here, as it was a great reference for more advanced aspects of finite dimensional linear algebra.","['linear-algebra', 'reference-request']"
3730956,"Evaluate $\int_0^1 \log (1-x)\ _3F_2\left(1,1,1;\frac{3}{2},\frac{3}{2};x\right) \, dx$","I encountered a hypergeometric integral while investigating harmonic sums $$\int_0^1 \log (1-x)\ _3F_2\left(1,1,1;\frac{3}{2},\frac{3}{2};x\right) \, dx$$ Based on my experience I suspect a nice closed-form exists but have found none.  Any kind of help will be appreciated. Update: To complete the solution following @Jack D'Aurizio's derivation, $1$ . Let $uz\to u$ in expression $f(z)=\int_{0}^{1}\frac{\arcsin\sqrt{uz}}{\sqrt{uz(1-uz)}}\log(1-u)\,du$ $2$ . Apply Fubini to $\int_0^1 f(4x(1-x))dx$ , then it become $\int_0^1 du \int_{\frac{1-\sqrt{1-u}}2}^{\frac{1+\sqrt{1-u}}2}dx\cdots$ $3$ . Integrate w.r.t $x$ by brute force, then let $u\to \frac{4t^2}{1+2t^2+t^4}$ $4$ . These integrals are evaluated using method of arXiv $2007.03957$ . Whence $$-\frac{1}4 \sum _{n=1}^{\infty } \left(\frac{4^n}{\binom{2 n}{n}}\right)^2\frac{ H_n}{n^3}=\int_0^1 \log (1-x)\ _3F_2\left(1,1,1;\frac{3}{2},\frac{3}{2};x\right) \, dx=-8 C^2+8 \pi  C \log (2)-32 \pi  \Im(\text{Li}_3(1+i))-16 \text{Li}_4\left(\frac{1}{2}\right)+\frac{413 \pi ^4}{360}-\frac{2}{3}  \log ^4(2)+\frac{8}{3} \pi ^2 \log ^2(2)$$","['integration', 'definite-integrals', 'polylogarithm', 'closed-form', 'hypergeometric-function']"
3730962,Characteristics for second order PDE in higher than 2 dimensions,"I'm trying to understand characteristics of second order hyperbolic equations. When the number of dimensions is 2, say $$
au_{xx} + 2b u_{xy} + cu_{yy} = 0, \tag{1}
$$ we have a change of variables $(x,y) \mapsto (\xi, \eta)$ that satisfies very particular conditions. In particular we reduce (1) to $$
w_{xy} = 0 \tag{2}
$$ which can be solved. In doing so we learn a wealth of information about the characteristics of this equation, and the solution's behavior along these characteristics. My question is: is there a generalization of this technique to higher dimensions? For higher dimensions there are more than one mixed partial term, which is where I'm getting stuck. The motivation for this is understanding wave propagation in 3 spatial (and 1 time) dimensions. Edit: For an equation of the form $g^{ij}(x)\partial_{ij}u = 0$ , with $g_{ij}$ a Lorentzian metric, I think this is a bit easier (theoretically, at least). Since it's a Lorentzian metric, we can pick ""null coordinates"" $\xi$ so that, say, in $n$ -dimensions, $\xi^1$ and $\xi^2$ are null coordinates. Then along the null hypersurfaces $\{\xi^1 = a\}$ and $\{\xi^2 = b\}$ the solution's ""outgoing"" derivative (i.e. the outgoing derivative to $\{\xi^1 = a\}$ is $\partial_{\xi^1} u$ ) must satisfy a first order PDE. In a manner similar to the transport equation, I think this gives ""propagation equations"" for the outgoing derivative along the null (characteristic) hypersurfaces. See Rendall's paper (1990) on the characteristic initial value problem.","['ordinary-differential-equations', 'analysis', 'wave-equation', 'partial-differential-equations', 'hyperbolic-equations']"
3730969,Interpretation of probability statements in Nina Zubrilina's paper,"In the paper https://content.sciendo.com/view/journals/dmgt/ahead-of-print/article-10.7151-dmgt.2210/article-10.7151-dmgt.2210.xml?language=en The main result is $$\operatorname{edim}(G(n,p)) \leq (1+o(1))\frac{4 \log n}{\log(1/q)},$$ where $q= 1-2p(1-p)^2(2-p)$ My first question is how should I interpret the result, what is $\operatorname{edim}$ of random graph. Should I interpret it as $$\mathbb{P} \left[ \operatorname{edim}(G(n,p)) \leq (1+o(1))\frac{4 \log n}{\log(1/q)} \right] \rightarrow 1 \text{ as } n \rightarrow \infty \text{ ?}$$ My second question is concerned with how to interpret lemma 2.2, it is stated that Let $G=G(n,p)$ be the random graph. Let $V,E$ denote the vertex and edge sets. Let $\omega \in \{1,\cdots,n\}$ be such that for any two distinct edges $e_1,e_2$ of $E$ , a uniformly random subset $W \subset V$ of size $\omega$ satisfies $$\mathbb{P}( W \text{ does not distinguish } e_1,e_2) \leq 1/n^4p^2 $$ Then $$\operatorname{edim}(G) \leq \omega$$ So, firstly how should I understand $E$ as subset of a random graph, and how can I fix two edges of this seemingly random set by saying ""for any two distinct edges $e_1,e_2 \in E$ "". I am confused about how I interpret such statement. Can any one clarify them ?","['graph-theory', 'combinatorics', 'probability']"
3730982,Fourier Legendre expansion for $\frac{\text{Li}_2(x)}{x}$,"Background : I'm computing harmonic series using FL expansion. For instance, the following $\frac{\log (1-x)}{x}=\sum _{n=0}^{\infty } 2 (-1)^{n-1} (2 n+1) P_n(2 x-1) \left(\sum _{k=n+1}^{\infty } \frac{(-1)^{k-1}}{k^2}\right)$ Can be used to compute $\sum_{n=1}^\infty\frac{H_n}{n}\left(\frac{(2n)!}{4^n(n!)^2}\right)^2$ (see here ). This expansion (and the solution linked) is given by On the interplay between hypergeometric series, Fourier-Legendre expansions and Euler sums by M. Cantarini, J. D’Aurizio. Problem : As I confront higher weight sums FL expansion of the following are needed: $\large\frac{\text{Li}_2(x)}{x},\frac{\log ^2(1-x)}{x},\frac{\log (x) \log (1-x)}{x}$ I haven't figured out how to compute them based on known results. Any help will be appreciated. Update: I summarize Jack's result here for sake of others' convenience. If $f(x)\sim\sum_{n=0}^\infty c_n P_n(2x-1)$ , then $$\frac{f(x)}x\sim\sum_{n=0}^\infty(-1)^n (2n+1)\left(\int_0^1 \frac{f(x)}x dx+2\sum_{m=1}^{n}\frac{1}{m}\sum_{k=m}^\infty (-1)^k c_k\right)P_n(2x-1)$$","['integration', 'legendre-polynomials', 'sequences-and-series', 'orthogonal-polynomials', 'zeta-functions']"
3731010,"Prove that there are at least three balls which lie in the same box, have the same colour and are of the same size depending upon following condition.","Two boxes contain between them 65 balls of several different sizes. Each ball is white,
black, red, or yellow. If you take any five balls of the same colour, at least two of them will always be of the same size (radius). Prove that there are at least three balls which lie in the same box, have the same colour and are of the same size. My approach:- Ans:- Making repeated use of pigeon-hole- principle (PHP)., there are 65 balls and 2 boxes, one of these boxes must contain at least $\left[\frac{65}{2}\right]+1=33$ balls.
Consider that box, now we have four colours (white, black, red, yellow) and hence there must be at least $\left(\frac{33}{4}\right)+1=9$ balls of the same colour. What to do next? How can i proof  atleast three balls are of the same size.",['combinatorics']
3731029,Divergence in local frame,"I am trying to solve a problem from John M. Lee's Riemannian Manifolds . The goal is to show that if we have a vector field $X = X^i E_i$ in terms of some local frame $\{E_i\}$ on $U\subset M$ , then we can write the divergence in terms of covariant derivatives: $$\text{div}(X) = X^i_{;i}$$ Where (in a coordinate frame/coframe): $$\nabla X = X^i_{;j}\partial_i\otimes dx^j = \left(\partial_j X^i + X^k\Gamma_{jk}^i\right)\partial_i\otimes dx^j$$ The hint says to ""show that it suffices to prove the formula at the origin in normal coordinates."" Let $p\in U$ be given. Since $\{E_i\}$ is a local frame, $\{E_i|_p\}$ forms a basis of $T_pM$ . Using the Gram-Schmidt algorithm, we can construct an orthonormal basis $\{F_i|_p\}$ . With the isomorphism $F : \Bbb R^n\to T_pM$ together with the exponential map, $\varphi = F^{-1} \circ \exp_p^{-1} : \mathcal U \to \Bbb R^n$ gives us normal coordinates centered at $p$ with $\mathcal U \subset U$ . Now using the coordinate expression for divergence along with properties of normal coordinates: \begin{align*}
\text{div}(X)|_p &= \left(\frac{1}{\sqrt{\text{det}(g_{ij})}}\sum_{k=1}^n\frac{\partial X^k\sqrt{\text{det}(g_{ij})}}{\partial x^k}\right)\Bigg|_p\\
&= \sum_{k=1}^n\frac{\partial X^k}{dx^k}\bigg|_p\\
&= X^k_{;k}(p) - X^\ell\Gamma_{k\ell}^k(p)\\
&= X^k_{;k}(p)
\end{align*} What I'm having a difficult time understanding is the ""suffices"" part of the hint. How do I extend that $\text{div}(X)=X^i_{;i}$ in normal coordinates to an arbitrary local frame? Especially since the definition of $X^i_{;j}$ includes a partial derivative, which need not be defined on an arbitrary frame? A similar question is asked here: Divergence as the trace of total covariant derivative? But I don't think it answers my question. That $X^i_{;i} = \text{tr}(\nabla X)$ gives a reason for frame-independence, but I don't see how it relates to the calculation in normal coordinates.","['divergence-operator', 'riemannian-geometry', 'differential-geometry']"
3731063,Grothendieck group of coherent sheaves is not a ring?,"My question is motivated by the fact that the Grothendieck group $K^0(X)$ of vector bundles on $X$ can be given a ring structure via the tensor product. But it seems to me that the Grothendieck group of coherent sheaves $K_0(X)$ has no such structure. Why? Let $X$ be any scheme. Denote by $K_0(X)$ the Grothendieck group of coherent sheaves on $X$ , defined as the quotient of the free abelian group $G_{\text{coh}}$ generated by formal symbols $[\mathscr F]$ , where $\mathscr F$ is a coherent sheaf on $X$ , by the relations $[\mathscr F] = [\mathscr F_1] + [\mathscr F_2]$ whenever there is a short exact sequence $0\to \mathscr F_1 \to \mathscr F\to \mathscr F_2 \to 0$ . It seems that the tensor product defines a ring structure on $G_{\text{coh}}$ , so I assume that the subgroup generated by $[\mathscr F] - [\mathscr F_1] - [\mathscr F_2]$ is not an ideal in $G_{\text{coh}}$ . Is there a concrete example of this? Replacing every occurrence of ""coherent sheaf"" by ""vector bundle"", we obtain the Grothendieck group $K^0(X)$ of vector bundles on $X$ . My understanding is that tensor product on $G_{\text{vb}}$ descends to a ring structure on $K^0(X)$ , i.e. the subgroup of $G_{\text{vb}}$ generated by $[E] - [E_1] - [E_2]$ is an ideal of the ring $G_{\text{vb}}$ . Is there a philosophical reason why this should hold for $K^0$ but not $K_0$ ?",['algebraic-geometry']
3731073,"what was/is motivation and short history/story behind ""class number""?","We know that there are some formula to calculate class number of a quadratic field $Q(\sqrt{d})$ , where $d\in Z-\{0,1\}$ , in terms of Dirichlet L-functions. Please correct me if I am wrong: for example one motivation to build $Q(\sqrt{d})$ is to define a field (extension of $Q$ ) in which the equation $x^2-d=0$ is solvable. I looked at some books and I did not see a simple explanation of what is a class number (without using the Ideals language) in a simple way. I saw some explanation in "" An introduction to number theory Book by Harold Stark "", but it was very short. I would like to know a little bit more about what was/is the motivation to define class number of a quadratic field and what are its applications? what is the use of knowing that a class number of a field is 1,2,...? giving some references would be appreciated as well.","['algebraic-number-theory', 'number-theory', 'elementary-number-theory', 'class-field-theory', 'extension-field']"
3731078,How many equivalence classes will there be?,"Consider the subset $T\subseteq \mathbb{Z}\times \mathbb{Z}\times \mathbb{Z}$ where the three numbers will be the corner angles (in degrees)
of a (real) triangle. For example $(30, 70, 80)\in T$ but $(10, 30, 50) \not\in T$ (since $10 + 30 + 50 < 180$ ), and $(−10, 20, 170) \not\in T$ (since there would not be a negative angle).
We define a relation on $T$ by $(a_1, b_1, c_1)\sim(a_2, b_2, c_2)$ if and only if the triangles that these triples are from have the same largest angle. This is the exact question from my e-book. And I am literally confused to tackle with this one. I know it must include something with combinations but in that way their will be a lot of cases. If I am right then yes I can solve this by $a+b+c=180$ (probably $15931$ is answer) *not sure by applying a particular formula here but I know my professor can't gonna allow me to go with this formula because we haven't covered that in our course. So is there any other way to solve this.
I really appreciate you to read this question. It will be very helpful if you will answer this question. Thanks","['combinations', 'equivalence-relations', 'relations', 'combinatorics', 'discrete-mathematics']"
3731126,Model Epidemic Random Graph,"I'm reading on random graphs and understand that they can be used to model disease spread (seems particularly germane at the moment). The papers I've found so far are focused on quite complex models. I'm wondering if someone could point me to explanation of or explain how a random graph could be used to model a disease with a simple set of assumptions. For example, Uniform infection transfer from an infected individual to others. People can only be infected once.
No countermeasures. I'm trying to understand what exactly it means to model an epidemic with a random graph but am struggling with the complexity of the examples I've found so far. Thank you.","['graph-theory', 'random-graphs', 'mathematical-modeling', 'probability']"
3731152,Difference between stationary and homogeneous point process,"I do not understand the difference between a stationary and a homogeneous point process. The definitions I found are as follows: A process is stationary if the entire configuration of the process is invariant under translation. A process is homogeneous if the mean number of points in each set A is given by $\alpha \cdot \parallel A \parallel$ , where $\alpha \geq 0$ is a constant. These definitions appear the same to me. Could anyone give me an example of a process that is stationary but not homogeneous or the other way around?
Thank you!!","['statistics', 'stationary-processes', 'point-processes']"
3731170,"Show that $\sum\limits_{j,k=2}^\infty\frac{1}{j^k}$ converges and calculate the limit of the series","Show that $\sum\limits_{j,k=2}^\infty\frac{1}{j^k}$ converges and calculate the limit of the series. My approach: We look if one of the iterated series converges absolutly. $$\sum\limits_{j=2}^\infty\left(\sum\limits_{k=2}^\infty\left|\frac{1}{j^k}\right|\right)=\sum\limits_{j=2}^\infty\left(\frac{1}{1-\frac{1}{j}}-1-\frac{1}{j}\right)=\sum\limits_{j=2}^\infty\left(\frac{j}{j-1}-1-\frac{1}{j}\right)=\sum\limits_{j=2}^\infty\left(1+\frac{1}{j-1}-1-\frac{1}{j}\right)$$ $$=\sum\limits_{j=2}^\infty\left(\frac{1}{j-1}-\frac{1}{j}\right)=\sum\limits_{j=2}^\infty\left(\frac{j-(j-1)}{j(j-1)}\right)=\sum\limits_{j=2}^\infty\left(\frac{1}{j(j-1)}\right)=\sum\limits_{j=1}^\infty\left(\frac{1}{j(j+1)}\right)=1$$ Since one of the iterated series is abosult convergent, cauchys product rule implies that the double series $\sum\limits_{j,k=2}^\infty\frac{1}{j^k}$ is also absolute convergent. Cauchys product rule also states in that case that: $$\sum\limits_{j,k=2}^\infty\frac{1}{j^k}=\sum\limits_{j=2}^\infty\left(\sum\limits_{k=2}^\infty\frac{1}{j^k}\right)=1$$ Would be great if someone could look over it and give me feedback if my work is correct , thanks alot :)","['convergence-divergence', 'sequences-and-series', 'real-analysis']"
3731176,Rigorous and comprehensive textbooks on precalculus [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 4 years ago . Improve this question I am looking for comprehensive and rigorous textbooks on precalculus that provide proof for all the formulas and theorems they mention. You can suggest multiple books on different topics like trigonometry, algebra, and geometry . I am aware that there are similar questions on this website but this question distinguishes from them mainly by the requirement of rigorousness.","['big-list', 'book-recommendation', 'reference-request', 'algebra-precalculus', 'soft-question']"
3731238,How to find fisher information for this pdf?,"Compute the maximum likelihood estimator for the unknown (one or two
dimensional) parameter, based on a sample of n i.i.d. random variables
with that distribution. In each case, is the Fisher information well
defined ? If yes, compute it. We have a shihifted exponential distribution with parameters $\alpha \in \mathbb{R},\:\lambda >0:$ $\:f_{\alpha ,\lambda }\left(x\right)=\lambda e^{-\lambda \left(x-\alpha \right)}1_{x\ge \alpha },\:\forall x\in \mathbb{R}$ I want to find fisher information for this pdf. How can I do that? I tried to find the second derivative of a log-likelihood function of $a$ but it is zero, so fisher information of $a$ is zero?","['fisher-information', 'statistics']"
3731249,Decomposing a $2\times 2$ matrix into rotation and scaling,"How do you comprehensively decompose a 2x2 matrix into a scaling and a rotation matrix? I understand that a rotation matrix looks like: $$
    \begin{pmatrix}
    \cos \theta & -\sin\theta \\
    \sin\theta  & \cos \theta\\    
    \end{pmatrix}
$$ and a scaling matrix looks like: $$
    \begin{pmatrix}
    \alpha & 0 \\
    0  & \alpha\\    
    \end{pmatrix}
$$ The matrix I want to decompose is $$
    \begin{pmatrix}
    2 & -2 \\
    2  & 2\\    
    \end{pmatrix}
$$ The way they do it in my book is by defining the first column of a as vector $ r =(2,2)$ . Then $|r| = 2 \sqrt{2} $ . So the scaling factor $\alpha = 2\sqrt{2}$ and the rotation is $ \pi / 4 $ Can this be done with every matrix? And how about the second column. Doesn't that matter at all? Many thanks in advance!","['matrices', 'matrix-decomposition', 'linear-algebra', 'rotations']"
3731279,Existence of solution to $ax^3 + bx^2 + cx + d \equiv 0 \pmod{p}$,"Given a polynomial $f(x) = ax^3 + bx^2 + cx + d \equiv 0 \pmod{p}$ , $a \not\equiv 0 \pmod{p}$ , I would like to classify all primes $p$ so that there exists $\alpha \in \Bbb{F}_p$ in which $f(\alpha) \equiv 0 \pmod{p}$ . This can be done in the case of quadratic equations, in which quadratic reciprocity is the answer. Is there a similarly simple method of checking if $f(x)$ has a root mod $p$ ?","['irreducible-polynomials', 'number-theory', 'polynomials']"
3731285,Tangents imply secants,"I am stuck with proving a limit which I think should be immediate... I will explain the problem and comment one of my attempts.  Let $x: [0, \infty) \to \mathbb{R}^n$ be a differentiable arc with $\lim\limits_{t \to \infty} x(t) = x_0 \in \mathbb{R}^n$ , $x(t) \not = x_0, \dot{x}(t) \not = 0, t \geq 0$ , of which we know the following limit of tangents exists: $$ \lim\limits_{t \to \infty} \frac{ \dot{x} (t) }{|| \dot{x} (t) ||} ,$$ say it tends to a certain $L \in S^1$ . Now I want to prove $ \lim\limits_{t \to \infty} \frac{ x(t) - x_0}{ || x(t) - x_0 ||}= -L$ (this can be graphically seen to make sense). My best attempt of a proof was just trying to apply the limit definitions, but I can not get the result... The problem arises when I have: $$ \left| \frac{x(t)- x_0}{ || x(t)- x_0 ||} - \frac{x(t)- x(s)}{ || x(t)- x(s)||} \right| + \left| \frac{x(t)- x(s)}{ || x(t)- x (s) ||} - \frac{\dot{x} (t)}{ ||\dot{x} (t)||}\right| + \left| \frac{\dot{x} (t)}{ ||\dot{x} (t)||}  - L \right|,$$ since the last term is smaller than $\varepsilon /3$ if $t >M_1 >0$ , the second term is smaller than $\varepsilon/3$ if $s \in (t- \delta, t + \delta)$ , but the first term is smaller than $\varepsilon/3$ if $s > M_2 (t)$ , i.e., the constant $M_2$ depends on $t$ , so I can not make the three summands ""small"" at the same time. I have tried to prove that $M_2$ does not depend on $t$ but I have not achieved it, I think it can't be done. So, any help or suggestion is appreciated, thanks in advance!","['secant', 'tangent-line', 'differential-geometry']"
3731331,How does topology work when taking charts on a Psuedo-Riemannian manifold?,"I'll first explain why I think taking charts is sane when working with Riemannian manifolds, and then show what I believe breaks down in the Pseudo-Riemannian case with a particular choice of a Pseudo Riemannian manifold (Minkowski space). I'd like to understand where I am going wrong. A Riemannian manifold is a differentiable manifold $M$ equipped with a positive definite inner product $d: T_p M \times T_p M \rightarrow \mathbb R$ . Let us concentrate attention on some chart $(U \subseteq M, \phi : U \rightarrow \mathbb R^n)$ . Here $\phi$ is a homeomorphism, hence we can ""push forward"" $d$ along $\phi$ to get some inner product structure on $\mathbb R^n$ : $d^\star: \mathbb R^n \times \mathbb R^n \rightarrow \mathbb R$ . Now since this $d^\star$ is an inner product structure, it induces a metric, which induces a topology on $\mathbb R^n$ . However (and this is the saving grace), due to equivalene of norm in a finite dimensional vector space , the topology induced by $d^\star$ will match the 'usual topology' on $\mathbb R^n$ . So the differential calculus that we do (which depends on having limits) cannot see the difference between $d^\star$ and the regular topology, and thus we can just do 'calculus on $\mathbb R^n$ ' and it transfers. Now let us look at the contrast in the Pseudo-Riemannian case.
Let us assume we have Minkowski space, which is $\mathbb M \equiv (\mathbb R^4, d')$ where the manifold structure on $M \equiv \mathbb R^4$ is the 'stupid chart': we have a single chart $\phi: M \rightarrow \mathbb R^4; \phi(x) = x$ . Now, we take the bilinear form to be $d': T_p \mathbb M \times T_p \mathbb M \rightarrow \mathbb R$ as given by $d'(\mathbf p, \mathbf q) \equiv - p_0 q_0 + p_1 q_1 + p_2 q_2 + p_3 q_3$ . This is no longer positive definite! Nor is it an inner product, and this cannot even induce a norm . However, intuitively, the way $d'$ sees space is very different from the way the usual topology sees space. For example, the distance between the points $\mathbf p =(t, x, 0, 0)$ and $ \mathbf q = (x, t, 0, 0)$ is $0$ according to $d'$ but $\sqrt{2xt}$ according to the Euclidian distance. So, how is it legal for us to do things like take limits inside minkowski space? We seem to have two choices: Claim that we treat $d'$ as simply some bilinear form, while still obeying the topology of $\mathbb R^4$ . This seems really weird to me, because now the structure of the topology is no longer 'intrinsic' to the manifold + bilinear form. It is rather induced by the chart into $\mathbb R^n$ I am going wrong somewhere in my explanation above, and I'd love to know where.","['riemannian-geometry', 'special-relativity', 'semi-riemannian-geometry', 'general-relativity', 'differential-geometry']"
3731368,Clarifying the chain rule terminology in differential geometry calculuations,"Let $M$ be a manifold and $f:M\to\mathbb{R}$ a smooth function on it. Let $p\in M$ have the coordinates $\{x^i\}$ under the chart $(U,\phi)$ . Finally, let $\gamma:I\to M$ be a curve ( $I$ is an open interval in $\mathbb{R}$ ). Let $u$ be the generic argument of the $\gamma$ map, i.e. $u\in I$ . I'm trying to understand the chain rule: $$\frac{\partial f}{\partial u}=\frac{\partial f}{\partial x^i}\frac{\partial x^i}{\partial u}$$ Now I'm aware that the change in the function value as we move along the curve is $\frac{\partial f}{\partial u}\big|_{p\in M}$ , which is actually $\frac{\partial (f\circ\gamma)}{\partial u}\big|_{\gamma^{-1}(p)\in I}$ if we want to reconcile the domains. Similarly, $\frac{\partial f}{\partial x^i}\big|_{p\in M}$ is actually $\frac{\partial (f\circ\phi^{-1})}{\partial x^i}\big|_{\phi(p)\in \mathbb{R}^n}$ For the last term, I have two ways of looking at it: either $\frac{\partial x^i}{\partial u}\big|_{x^i(p)\in \mathbb{R}^n}$ , or $\frac{\partial x^i}{\partial u}\big|_{p\in M}$ . I'm not sure which is correct , so I'll leave that as is for now. The chain rule equation becomes $$\frac{\partial (f\circ\gamma)}{\partial u}\ \bigg|_{\gamma^{-1}(p)}=\frac{\partial (f\circ\phi^{-1})}{\partial x^i}\ \bigg|_{\phi(p)}\frac{\partial x^i}{\partial u}$$ My thought process was that I could express $f\circ\gamma$ as $(f\circ\phi^{-1})\circ(\phi\circ\gamma)$ , but I haven't been able to understand just how the chain rule is working out. I'd be grateful if someone could help me understand. I'm a beginner, so I would really appreciate a step by step answer meant for a beginner to the subject, without omitting any details .","['chain-rule', 'differential-geometry']"
3731409,Counting the number of strings with at least $2$ numbers,"Let $k$ and $n \ge 3$ be two natural numbers. How many strings in $\{1,...,n\}^k$ contain at least one occurrence of $1$ and $2$ , or at least one occurrence of $2$ and $3$ or at least one occurrence of $1$ and $3$ ? I tried to break it down and first count the number of strings that contain at least one $1$ which is $n^k - (n-1)^k$ and similarly to at least one $2$ and $3$ , but how do we proceed from here?","['combinatorics', 'discrete-mathematics']"
3731414,Evaluate $\int_{-\pi}^{\pi} \frac{x^2}{1+\sin{x}+\sqrt{1+\sin^2{x}}} \mathop{dx}$,I came across this integral: $$\int_{-\pi}^{\pi} \frac{x^2}{1+\sin{x}+\sqrt{1+\sin^2{x}}} \mathop{dx}$$ I tried $u=x+\pi$ $$\int_{-\pi}^{\pi} \frac{(x+\pi)^2}{1-\sin{x}+\sqrt{1+\sin^2{x}}} \mathop{dx}$$ but had no success. I also tried $u=-x$ : $$\int_{-\pi}^{\pi} \frac{x^2}{1-\sin{x}+\sqrt{1+\sin^2{x}}} \mathop{dx}$$ Does this help?  Any suggestions. Answer is $\dfrac{\pi^3}{3}$ by the way.,"['integration', 'calculus', 'definite-integrals', 'real-analysis']"
3731432,"How would one use Bézout's theorem to prove that if $d = \gcd(a,b)\ \text{then} \ \gcd(\dfrac{a}{d}, \dfrac{b}{d}) = 1$.","Note: I have checked the questions with the same title and I am after something more specific. I am doing my first course in discrete mathematics, and came across the following proposition that I was asked to prove: Let $a,b,d \in \mathbb{Z}$ . If $d = \gcd(a,b)\ \text{then} \ \gcd\Bigl(\dfrac{a}{d}, \dfrac{b}{d}\Bigr) = 1$ . My first thought was to prove it by contradiction, and I did it as follows, Let $a,b,c,d \in \mathbb{Z}$ and suppose that $d = \gcd(a,b).$ Assume that $\ \gcd\Bigl(\dfrac{a}{d}, \dfrac{b}{d}\Bigr) = c$ , where $c \neq 1 $ . Then $c\mid\frac{a}{d} \ \text{and} \   c\mid\frac{b}{d}$ , that is, $a = cmd \ \text{and} \ b = cnd$ , where $m,n \in \mathbb{Z}$ This implies that there is a integer $cd$ that divides both $a$ and $b$ , where $cd > d$ . But, $d$ is the greatest common divisor of $a$ and $b$ , which yields a contradiction. Therefore, the assumption is false, and $\ \gcd\Bigl(\dfrac{a}{d}, \dfrac{b}{d}\Bigr) = 1$ . This is my approach, but the solution presented by the TA's notes uses a different approach which, given the way it was presented, I could not understand how it would prove the proposition. The approach uses Bézout's theorem, which was presented to us in following manner: Consider the equation $$ax+by=c,$$ where $a,b,c$ are integers, with $a$ and $b$ not both zero. if $ c=d$ , where $d$ is the greatest common divisor of $a$ and $b$ then the equation has a solution in integers $x,y$ . if $d\mid c$ then the equation has a solution in integers. if $d\nmid c$ then the equation has no solution in integers. The proof presented went on to apply this theorem to prove the proposition: Consider the equation $$ax+by=d,$$ where $d = \gcd(a,b)$ (with integer coefficients). Dividing both sides by $d$ yields, $$\frac{a}{d}x+\dfrac{b}{d}y=1,$$ where $\dfrac{a}{d}$ and $\dfrac{b}{d}$ are both integers (follows from definition of that $\gcd$ ) and then it goes on to say by Bézout's theorem, we can conclude that $\ \gcd\Bigl(\dfrac{a}{d}, \dfrac{b}{d}\Bigr) = 1$ Now, I am really confused as to what role Bézout's theorem (the way it is presented to us) has played in their conclusion; the theorem does not say that if there are integer solutions then the RHS must be the gcd of the coefficients. $\textbf{And}$ if they claimed that $\dfrac{a}{d}$ and $\dfrac{b}{d}$ are relatively prime, wouldn't that mean that, by definition, their greatest common divisor must be 1? Because if so, then I really do not see the need to use the theorem in the first place. Given that some of the proofs for this proposition (that I have seen) here and on other websites use Bézout's theorem, I am inclined to believe that there is something wrong with my way of thinking, as in there is something I am not seeing, so I would appreciate it if you could clarify this for me. I have attached my proof just to see if it is correct in case the proof presented in my notes turned out to be incomplete or incorrect. edit: small corrections","['elementary-number-theory', 'gcd-and-lcm', 'discrete-mathematics']"
3731433,Basic 2 form exercise,"Given $\omega = fdx+gdy+hdz$ , such that $\omega\wedge dz=0$ , what can we conclude about $f$ , $g$ and $h$ ? Let's write what we have: $$
0 = \omega\wedge dz = f dx\wedge dz + g dy \wedge dz.
$$ We could take $d$ here and get $g_x = f_y$ .
Then we could use this result and $d^2 \omega = 0$ and obtain $f_{zy}=g_{zx}$ . So I derived something, but not really understand the geometric sense or what this all was about. But it should be smth.","['differential', 'exterior-derivative', 'differential-geometry']"
3731434,For which $n$ is $n+\varphi(n)^2$ a perfect power?,"For which positive integers $n$ is $$n+\varphi(n)^2$$ a perfect power , where $\varphi(n)$ denotes the totient-function ? The following pari/gp program searches for solutions : gp > for(n=1,10^8,if(ispower(n+eulerphi(n)^2)>0,print1(n,"" "")))
4 19 2880 5859
gp > So, upto $n=10^8$ , there are $4$ positive integers : $4,19,2880,5859$ , each giving a perfect cube.
For a perfect square I tried the approach $$n+\varphi(n)^2=(\varphi(n)+d)^2$$ which leads to $$n=2d\varphi(n)+d^2$$ hence $$\frac{n}{\varphi(n)}>2d$$ Hence for $d\ge 4$ , $n$ must already have at least $22$ prime factors and exceed $3\cdot 10^{30}$ $d=3$ gives $n-9=6\varphi(n)$ , hence $3\mid n$ . If we have $n=3^s\cdot t$ with $3\nmid t$ , we get $$n-6\varphi(n)=3^s\cdot t-6\cdot 3^{s-1}\cdot 2\varphi(t)=3^s(t-4\varphi(t))=9$$ So either $\ t-4\varphi(t)=1\ $ or $\ t-4\varphi(t)=3\ $ . In both cases we can easily conclude that $t$ must be odd. Because of $\frac{t}{\varphi(t)}>4$ , we can conclude $\omega(t)\ge 140$ and $t>4\cdot 10^{337}$ $d=2$ gives $n-4=4\varphi(n)$ , hence $4\mid n$ . With $n=2^s\cdot t$ with odd $t$ we get $$n-4\varphi(n)=2^s\cdot t-2^{s+1}\varphi(t)=2^s(t-2\varphi(t))=4$$ and because of $s\ge 2$ we can conclude $t-2\varphi(t)=1$ which would solve the Lehmer problem , which asks for a composite number $n$ with $\varphi(n)\mid n-1$ Finally $d=1$ gives $n-1=2\varphi(n)$ solving the Lehmer problem. Since no solution for the Lehmer problem exists upto $10^{20}$ , for a perfect square we must have $n>10^{20}$","['number-theory', 'totient-function', 'elementary-number-theory']"
3731454,Alternating Harmonic Series Spin-off,"We know that the series $\sum (-1)^n/n$ converges, and clearly every other alternating harmonic series with the sign changing every two or more terms such as $$\left(1+\frac{1}{2}+\frac{1}{3}\right)-\left(\frac{1}{4}+\frac{1}{5}+\frac{1}{6}\right)+\left(\frac{1}{7}+\frac{1}{8}+\frac{1}{9}\right)-\cdots$$ must converge. My question here is that does the series below also converge? $$\sum\frac{\textrm{sgn}(\sin(n))}{n}\quad\textrm{or}\quad\sum\frac{\sin(n)}{n|\sin(n)|}$$ Loosely speaking, the sign changes every $\pi$ terms. I'd be surprised if it doesn't converge. Wolfram Mathematica, after a couple of minutes of computing, concluded the series diverges but I can't really trust it. My first approach (assuming the series converges) was that if we bundle up terms with the same sign like the example above every bundle must have three or four terms, and since the first three terms of all bundles make an alternating series I was going to fiddle with the remaining fourth terms but they don't make an alternating series so I guess there's no point in this approach. edit: I don't think we can use Dirichlet's test with $$b_n=\textrm{sgn}(\sin(n)).$$ The alternating cycle here is $\pi$ and I don't believe it would bound the series. For example if the cycle was a number very slightly smaller than $3+1/4$ , then $B_n$ (sum of $b_n$ ) would get larger and larger every four bundles for some time. I believe this should happen for $\pi$ as well since it is irrational. I'm not entirely sure why but $|B_n|\leq3$ for most small $n$ though I guess it's because $\pi-3$ is slightly smaller than $1/7$ ? Anyway $B_{312\ 692}=4$ , $B_{625\ 381}=5$ , $B_{938\ 070}=6$ , $B_{166\ 645\ 135}=-7$ , and $B_{824\ 054\ 044}=8$ . $|B_n|$ does not hit $9$ up to $n=1\ 000\ 000\ 000$ with $B_{1\ 000\ 000\ 000}=-2$ .",['sequences-and-series']
3731476,Calculating the volume of an ellipsoid with triple integrals,"I am trying to find the volume of the ellipsoid $\left(\frac{x}{a}\right)^2 + \left(\frac{y}{b}\right)^2 + \left(\frac{z}{c}\right)^2=1$ by making the substitution $u=x/a$ , $v=y/b$ and $w=z/c$ . With this substitution, the equation becomes $u^2+v^2+w^2=1$ . Projecting this on the $uv$ -plane, we get a circle of radius $1$ . Hence, the triple integral is $I=\int_{u=-1}^{1} \int_{v=-\sqrt{1-u^2}}^{\sqrt{1-u^2}} \int_{w=-\sqrt{1-u^2-w^2}}^{\sqrt{1-u^2-w^2}} abc \, \mathrm{dw} \, \mathrm{dv} \, \mathrm{du}$ . The $abc$ comes from the Jacobian. Then, to evaluate this, I decided to switch to cylindrical coordinates. So, $u=r\cos\theta$ , $v=r\sin\theta$ , and $w=w$ . This gave me $I=\int_{\theta=0}^{2\pi}\int_{r=0}^{1}\int_{w=-\sqrt{1-r^2}}^{\sqrt{1-r^2}} abc r \, \mathrm{dw} \, \mathrm{dr} \, \mathrm{d\theta}$ , where the $r$ comes from the Jacobian of the second transformation. I then evaluated this: \begin{align*}
I &= \int_{\theta=0}^{2\pi}\int_{r=0}^{1}\left[abcrw\right]_{w=-\sqrt{1-r^2}}^{\sqrt{1-r^2}} \, \mathrm{dr} \, \mathrm{d\theta} \\
&= \int_{\theta=0}^{2\pi}\int_{r=0}^{1} 2abcr\sqrt{1-r^2} \, \mathrm{dr} \, \mathrm{d\theta} \\
&= \int_{\theta=0}^{2\pi} \frac{2abc}{3} \, \mathrm{d\theta} \\
&= \frac{4\pi}{3}abc
\end{align*} My answer is correct, which I am happy about, but is my solution correct? Is it clear to understand? Also, how else could I calculate the volume? I think spherical coordinates could also be useful, but I don't know if it would be easier than using cylindrical coordinates.","['multivariable-calculus', 'multiple-integral']"
3731491,How to find area of rectangle inscribed in ellipse.,"In an ellipse $4x^2+9y^2=144$ inscribed is a rectangle whose vertices lies on the ellipse and whose sides are parallel with the ellipse axis. Longer side which is parallel to major axis, relates to the shorter sides as $3:2$ . Find area of rectangle. I can find the values of $a$ and $b$ as $$\frac{4x^2}{144}+\frac{9y^2}{144}=1$$ $$\frac{x^2}{6^2}+\frac{y^2}{4^2}=1$$ Comparing with $\frac{x^2}{a^2}+\frac{y^2}{b^2}=1$ , gives $a=6$ & $b=4$ . From here I have no idea how to solve further?","['rectangles', 'conic-sections', 'geometry']"
3731496,Is it possible to put inequality to null hypothesis?,"Based on a sample of i.i.d. Normal random variables $X_1, . . . , X_n$ with mean µ and variance $σ^2$ , propose a test with asymptotic level 5% for the hypotheses: $$ H_0: µ > σ$$ $$ H_1: µ \leq σ$$ What is the p-value of your test if the sample has size n = 100, the sample average is 2.41 and the sample variance is 5.20 ? If the sample size is n = 100, the sample average is 3.28 and the sample variance is 15.95 ? In the latter case, do you reject H0 at level 5% ? At level 10% ?","['p-value', 'statistics', 'normal-distribution']"
3731584,Question about the strong Markov property of Brownian motions,"In the lecture we learned the following strong Markov property of the Brownian motions Theorem . Let $T$ be a stopping time, and let $Y$ be a bounded random variable. Then we have $$E[Y\circ \theta^T|\mathcal{F}_T]=E_{B_T}[Y] \text{ on } {T<\infty}.$$ Here $\theta$ denotes the shift operator. It is the version that is stated in the book of Durrett. I saw another version of the Markov Strong Property. Can we prove that they are equivalent? Theorem. Let $T$ be a stopping time that is almost surely finite. Then the process $X$ defined by $$X_t=B_{T+t}-B_{T}$$ is a standard Brownian motion independent of $\mathcal{F}_T$ . Thanks for any comment!","['markov-process', 'brownian-motion', 'probability-theory']"
3731596,Show $(A-\lambda)^{-1}=-\int_0^\infty e^{-\lambda t}e^{tA}dt$ using spectral theorem,"Let $A$ be a bounded self-adjoint operator. Use the spectral theorem to show $$(A-\lambda)^{-1}=-\int_0^\infty e^{-\lambda t}e^{tA}dt,\ \ \ \mathrm{for\ }\lambda>\|A\|$$ This seems like a beautiful and therefore maybe well know result. Do you where I could look it up? Or can you give me any hints how to show this? I don't even know how to start. Thanks! Spectral theorem for bounded self-adjoint operators: Let $H$ be a Hilbert space and let $A \in L_b(H)$ ( $L_b$ means linear and bounded) self-adjoint. Let $\mathcal A$ be the $\sigma$ -Algebra of the Borel sets on $\sigma(A)$ . Then there exists a unique spectral measure $E$ on $\sigma(A).$ This is the so called spectral measure of $A$ or the spectral decomposition of $A$ , such that $$A=\int(t\mapsto t)dE=:\int t dE(t)$$ For a operator $T \in L_b(H)$ the following statements are equivalent: $(i): TA=AT$ $(ii): TE(\Delta)=E(\Delta)T\ \ \ \forall \Delta \in \mathcal A$ We also used the notation $A=\int id_{\sigma(A)}dE$ instead of $A=\int(t\mapsto t)dE$","['self-adjoint-operators', 'spectral-theory', 'functional-analysis']"
3731746,Hartshorne 5.11 twisted sheaf,"Question 5.11 of Hartshorne Let $S$ and $T$ be two graded rings with $S_0=T_0=A$ . We define the Cartesian product $S\times_A T$ to be  the graded ring $\bigoplus_{d\geq 0}S_d\otimes T_d.$ If $X= \operatorname{Proj}(S)$ and $Y= \operatorname{Proj}(T)$ , then show that : $\operatorname{Proj}(S\times_A T)\cong X\times_A Y$ and the sheaf $\mathcal{O}(1)$ on $\operatorname{Proj}(S\times_A T)$ is isomorphic to the sheaf $p_1^{*}(\mathcal{O}_X(1))\otimes p_2^{*}(\mathcal{O}_Y(1))$ on $X\times_A Y$ . Attempt I have shown that $\operatorname{Proj}(S\times_A T)\cong X\times_A Y$ . However I am facing difficulty proving the second part. Let $X\times_A Y$ be represented as $(Z,\mathcal{O}_Z)$ Firstly I am unable to define the map from $p_1^{*}(\mathcal{O}_X(1))\otimes p_2^{*}(\mathcal{O}_Y(1))$ to $\mathcal{O}_Z(1)$ . Secondly can anyone give hints for proving it an isomorphism.(It seems that Proposition 5.12 might be useful but how, I don't know). Any help is appreciated.","['algebraic-geometry', 'graded-rings', 'quasicoherent-sheaves']"
3731756,Contitional expectation contraction inequality two sub-sigma-algebras,"Let $(\Omega,\mathcal{F},P)$ be a probability space and $\mathcal{H}\subset\mathcal{G}\subset\mathcal{F}$ two $\sigma$ -algebras. We know from Jensen's inequality, that for $X\in L^2(\Omega,\mathcal{F},P)$ $$
\mathbb{E}[|\mathbb{E}[X|\mathcal{H}]|^2]\leq\mathbb{E}[|X|^2].
$$ Can this be generalized to an inequality like $$
\mathbb{E}[|\mathbb{E}[X|\mathcal{H}]|^2]\leq\mathbb{E}[|\mathbb{E}[X|\mathcal{G}]|^2]\leq\mathbb{E}[|X|^2]\
$$ even if $X$ is not necessarily $\mathcal{G}$ -measurable?
I couldn't find a proof of that version (I don't even know if it even holds or we need further restrictions) but I'm pretty sure it was used in this paper https://arxiv.org/abs/1907.06474 only justified by As the conditional expectation is an orthogonal projection, we clearly have that ... Thanks for any help!","['measure-theory', 'linear-algebra', 'probability-theory', 'inequality']"
3731797,"Beyond angle trisection: Constructing regular polygons by dividing angles into 5, 7, 11, (et cetera) equal parts","I've read a paper by Andrew Gleason where he was able to come up with a way to construct heptagons and tridecagons using angle trisection to supplement the usual compass and straightedge. This post questions the ability to use angle quintisection (dividing into five) to construct an undecagon (11 sides). Gleason also mentioned that a 19-gon (enneadecagon) requires 2 angle trisections to construct using compass/straightedge/trisector, since $18=2*3^2$ and the 3 is raised to the power of two. My questions are as follows: Given a circle of radius $19-1=18$ , how would one proceed with the construction? I can't seem to follow along with Gleason in his methods and I'm totally lost beyond constructing $\sqrt{19}$ . Could someone help me work out a construction? In addition to compass and straightedge, does this mean: a. A regular 41-gon can be constructed with one angle quintisection? $41-1=2^3*5$ b. A regular 61-gon can be constructed with one angle trisection and one quintisection? $61-1=2^2*3*5$ c. A 101-gon with two quintisections? $101-1=2^2*5^2$ d. A 433-gon with three trisections? $433-1=2^4*3^3$ Quintisection, heptasection, et cetera can be done with an Archimedean spiral. So if the above statements are true, one can construct a regular polygon with any number of sides even without ""cheating"" (using the spiral to construct 360/n). An 89-gon can be constructed using 1 angle undecasection (dividing into 11 equal parts), a 331-gon / 661-gon / 1321-gon can all be constructed with one each of trisection / quintisection / undecasection, et cetera. My goal is to use Gleason's principles to come up with a way to construct a 433-gon using compass, straightedge, and angle trisector. It would be extremely long, but I would like to see it worked out.","['geometric-construction', 'polygons', 'galois-theory', 'trigonometry', 'cyclotomic-fields']"
3731853,If $a_0=1$ and $a_{n+1}=a_n +e^{-a_n}$ then does the limit of $a_n-\log{n}$ exist and if so then what is it?,I don't  want the full answer but hints for solving this question.My idea to attempt the question so far is the following by back tracing if we calculate the limit of $\dfrac{e^{a_n}}{n}$ we will get our answer just by applying logarithm.As $\dfrac{e^{a_n}}{n}$ might be useful in the sense that we can use the given conditions of the problem and with Taylor series expression.But how should I proceed to get to $\dfrac{e^{a_n}}{n}$ . I think we need to construct a new sequence.Is my approach correct or I need to think differently? Hints required,"['sequences-and-series', 'real-analysis']"
3731916,Show the $\arcsin$ identity: $ \arcsin(1 - 2x) + 2\arcsin(\sqrt{x}) = \pi / 2$,"Can somebody find an elementary proof of the following identity: $$
\arcsin ( 1 - 2x) + 2 \arcsin(\sqrt{x}) = \frac\pi2
$$ I noticed it while solving the following integral: $$ I = \int \frac{\sqrt{x}}{\sqrt{1 - x}} = -2 \sqrt{1 - x}\sqrt{x} + \int \frac{\sqrt{1 - x}}{\sqrt{x}}
$$ where the first equality follows after applying an integration by parts with $f = \sqrt{x}$ and $\mathrm{d} g = 1/\sqrt{1 - x}$ . For the sake of simplicity we omit $\mathrm{d}x$ in each integral. Adding the integral to itself: \begin{align} 2I = \int \frac{\sqrt{x}}{\sqrt{1 - x}} &= -2 \sqrt{1 - x}\sqrt{x} + \int \left(\frac{\sqrt{1 - x}}{\sqrt{x}} + \frac{\sqrt{x}}{\sqrt{1- x}}\right) \\&= -2 \sqrt{1-x}\sqrt{x} + \int\frac{1}{\sqrt{x}\sqrt{1-x}}\end{align} The last integral on the RHS evaluates to $\arcsin(1 - 2x)$ , so $$I = - \sqrt{1-x}\sqrt{x} + \frac{1}{2}\arcsin(1 - 2x) + C.$$ On the other hand, the integral can also be evaluated by applying a $u$ -sub with $u = \sqrt{x}$ . We find that: $$
I = 2 \int\frac{u^2}{\sqrt{1 - u^2}} =2 \left(-u \sqrt{1 - u^2} + \int\sqrt{1 - u^2}\right)  = - u\sqrt{1 - u^2} + \arcsin u + C_2
$$ So then it follows that $I$ is also equal to $-\sqrt{x}\sqrt{1-x} + \arcsin{\sqrt{x}} + C_2$ . Equate the results of the two methods and plug in a random point to find $C - C_2$ and the subsequent identity.","['indefinite-integrals', 'trigonometry', 'inverse-function']"
3731917,uniform integrability of all conditional expectations of a fixed $L^1$ function,"Let $Z$ be a real $L^1$ random variable on a probability space $(\Omega, \mathscr{A}.\mu).$ Why is the family of all $E[Z|  \mathscr{B}]$ uniformly integrable when $ \mathscr{B}$ ranges over the sub-sigma-algebras of $ \mathscr{A}$ ?  This is supposed to be trivial but even assuming $Z\geq 0$ i don't know how to control $\int_A  E[Z|  \mathscr{B}]d\mu$ when $A\notin \mathscr{B}$ .","['stochastic-processes', 'probability-theory']"
3731932,Expected number of keys to try,"You have n keys on your key ring. One of them unlocks the front door, but you don't know which one. Find the expected number of tries it takes to open the front door by using the method of indicator random variables. Note: We try keys without repeats since that key will never work if it doesn't work once. My attempt: Let X be the number of tries we need to open the front door.
We see that $$X = 1 + X_1 + X_2 + X_3 + ... + X_n $$ where $X_i = 1$ if the $i^{th}$ try doesn't open the door and $X_i = 0$ if the try opens the door.
We need at least one try to open the door which is why there's a $1$ as the first term in the sum. Using linearity of expectation: $$E[X] = 1 + \sum_i {E[X_i]} = 1 + \sum_i {P(X_i = 1)}$$ Now attempting to find $P(X_i = 1)$ . Well we've already tried $i - 1$ keys at this point, so there are $n - i + 1$ keys left. Of those $n - i + 1$ keys, $n - i$ of them are incorrect. Therefore, the probability that the $i^{th}$ try doesn't work is $\frac{n - i}{n - i + 1}$ . So, $$E[X] = 1 + \sum_i {E[X_i]} = 1 + \sum_i {P(X_i = 1)} = 1 + \sum_{i = 1}^{n} \frac{n - i}{n - i + 1}$$ I typed this summation into wolframalpha and got some ugly answer with a $\psi$ in it, while the answer from the book I'm using gives the answer as $\frac{(n+1)}{2}$ . Where am I going wrong? Any advice is appreciated :)","['discrete-mathematics', 'probability-theory', 'probability', 'random-variables']"
3731940,Dice: Rolling at least N successes where number of succeses vary by dice value,"Rules I have four different types of dice: six-, eight-, ten- and twelve-sided (d6, d8, d10 & d12, respectively). The number of successes vary by the value rolled (and thus indirectly by dice type). One success is gained by rolling 6 or 7. Two successes are gained by rolling 8 or 9. Three successes are gained by rolling 10 or 11. Four successes are gained by rolling 12. This means that a 1d6 can result in at most 1 success, 1d8 1-2 successes, 1d10 1-3, and 1d12 1-4. Successes are added together after the roll, so rolling 6 dice and getting [12, 3, 8, 7, 10, 1] will result in 4 + 2 + 1 + 3 = 10 successes. Input is the number of dice and how many sides they have, and the minimum amount of successes I want to achieve. Question My main question is this: Given that I roll a known combination of d6s, d8s, d10s and d12s, how do I calculate the probability of rolling N or more successes? Q1 (though feel free to answer any other questions in this post as well, indexed Q $n$ for your convenience) Context I know how to calculate the probability of rolling at least $N$ successes for an arbitrary number of d6's, since they can only yield one success at most. I am stuck, however, when it comes to calculating at least $N$ successes when rolling a mix of differently sided dice, where some of them can yield more than one success. For example, with $5$ d6, $1$ d8, $1$ d12, how likely am I to roll $\geq$ 4 successes? Q2 EDIT: It's been brought to my attention that there is no closed form solution to this question. That is fine; any solution or clever approximation that's more efficient than running 100k simulated rolls is a sufficient answer. Can the problem be split into separate probabilities that can later be combined? E.g., given 5d6 & 1d12 and that I'm looking for the probability of at least $k$ successes, can I calculate the probabilities for each die type separately and later combine them somehow? Q3 Also, how would I go about calculating $\geq k$ successes for 1d12? For 2d12? For $n$ d12? Q4 Currently, I can 'solve' the problem by running a simulation, but it irks me that I am not able come up with anything better.","['dice', 'combinatorics', 'probability']"
3731969,An important corollary of Hahn - Banach Theorem,"Let $X$ a normed linear space. Denote with $\mathbb{F}=\mathbb{C}\;\text{or}\;\mathbb{R}$ Suppose that (1) $\;M$ is a closed subspace of $X$ ; (2) $\;x_0\in X\setminus M$ ; (3) $\;d=\text{dist}(x_0, M)=\inf\{\lVert x_0-m\rVert\;:\; m\in M\}$ . Then exists $\Lambda\in X^*$ such that $$\Lambda(x_0)=d,\quad \Lambda|_M=0\quad\text{and}\quad \lVert\Lambda\rVert_{X^*}=1.$$ $\textit{Proof}$ Observe that $d>0$ since $M$ is closed. Define $M_1=\text{span}\{M,x_0\}$ . Then each $x\in M_1$ can be written as $x=m_x+t_x x_0$ for same $m_x\in M$ e $t_x\in \mathbb{F}$ and since $x_0\notin M$ this representation is unique. Define $\lambda\colon M_1\to \mathbb{F}$ as $\lambda(x)=t_xd$ . Observe that $\lambda$ is linear, $\lambda|_M=0$ , and $\lambda(x_0)=d.$ If $x\in M_1$ e $t_x\ne 0$ , then we have that $-m_x/t_x\in M$ .
Then, $$\lVert x\rVert_X=\Vert t_x x_0+m_x\rVert_X=\lvert t_x\rvert \bigg\lVert x_0-\bigg(\frac{-m_x}{t_x}\bigg)\bigg\rVert_X\ge\lvert t_x\rvert d.$$ If $t_x = 0$ (so $x\in M$ ), this is still true. Hence, $$\lvert\lambda(x)\rvert=\lvert{t_x}\rvert d \le\lVert x\rVert_X\quad\text{for all}\;x\in M_1.$$ Therefore $\lambda$ is continuous on $M_1$ and $\lVert\lambda\rVert_{M_1^*}\le1.$ On the other hand, exist vectors $m_n\in M$ such that $\lVert x_0-m_n\rVert_X\to d$ for $n\to \infty$ . Since $\lambda|_M=0$ , we have that $$d=\lambda(x_0)=\lambda(x_0-m_n)\le\lVert x_0-m_n \rVert_X\lVert\lambda\rVert_{M_1^*}\to d\lVert\lambda\rVert_{M_1^*}.$$ Therefore $\lVert\lambda\rVert_{M_1^*}\ge 1.$ Then we have $$\lVert\lambda\rVert_{M_1^*}=1.$$ For a Corollary of Hahn Banach Theorem, exists $\Lambda\in X^*$ such that $$\Lambda|_{M_1}=\lambda\quad\text{and} \quad\lVert\Lambda\rVert_{X^*}=\lVert\lambda\rVert_{M_1^*}=1.$$ Since $x_0\in M_1$ and $M\subseteq M_1$ we have $\Lambda(x_0)=\lambda(x_0)=d$ e $\Lambda|_M=\lambda|_M=0.$ Is everything formally correct?","['proof-writing', 'solution-verification', 'functional-analysis']"
3732005,"If $f$ is $C^1$ and $\Vert[Df(x)]^{-1}\Vert \leq c$ for all $x$, does the inverse $f^{-1}$ exists?","Let $V$ be a normed space, $U \subset V$ an open nonempty set and $f: U \rightarrow V$ be such that $f$ is $C^1$ , $Df(x)$ is invertable and $\Vert[Df(x)]^{-1}\Vert \leq c$ for all $x \in U$ . Does $f^{-1}$ exists? I tried to use the inverse function theorem or use the derivative to show something like $\Vert x-y\Vert \leq c \Vert f(x) - f(y) \Vert$ , but it only works locally. It seems to me that exists a $f$ with this properties with no inverse but trying to find it has been equally difficult.","['derivatives', 'real-analysis']"
3732017,Does there exist a monotonically decreasing function that is its own derivative?,I know that $f(x) = e^x$ is its own derivative. It is a monotonically increasing function. It seems intuitively plausible to me that there might be a monotonically decreasing function with the same property. Does one exist?,"['calculus', 'derivatives', 'real-analysis']"
3732021,"Let $f$ be a bounded linear operator $X \to X$ s.t that $\|f(x)\|\geq m\|x\|$ for some $m$, $\forall x \in V$. Prove that $f$ cannot be compact.","Let $V$ is an infinite dimensional subpace of a Banach space and let $f$ be a bounded linear operator $X \to X$ s.t that $\|f(x)\|\geq m\|x\|$ for some $m$ , $\forall x \in V$ . Prove that $f$ cannot be compact. My solution relies ont he fact that we can find a sequence in $V$ s.t $\|e_n-e_m\|>\frac{1}{2}$ . Then if $f$ is compact we know $f(e_{n_k})$ has a convergent subsequence. But that is a contradiction as that would imply $e_{n_k}$ converges. I was wondering whether there is a different approach to the problem, one that relies on the fact that $f(B_1)$ would be precompact perhaps. I do not like using the existence of the sequence $e_n$ .",['functional-analysis']
3732082,Computing a Gromov-Witten invariant,"Some background that is not necessary for answering the question: Let $X = \mathbb{P}(\mathcal{O}_{\mathbb{P}^2}\oplus\mathcal{O}_{\mathbb{P}^2}(2))$ be a threefold. This is a $\mathbb{P}^1$ -bundle over $\mathbb{P}^2$ . Let $f$ be the cohomology class of the fiber. This bundle has a section whose image has normal bundle $\mathcal{O}(-2)$ as a hypersurface in $X$ . Let $\beta \in H_2(X, \mathbb{Z})$ be the class of a line on this hypersurface. A localization computation gives me the Gromov-Witten invariant $GW^X_\beta\langle f \rangle = -1$ . I would like to directly compute this invariant using obstruction theory. My actual question: The moduli space of lines in the hypersurface in question (which is isomorphic to $\mathbb{P}^2$ ) is the dual $(\mathbb{P}^2)^*$ . The obstruction bundle on this dual space is the bundle whose fiber over each point (which is a line $i:L \hookrightarrow \mathbb{P}^2$ ) is $H^1(L, i^*\mathcal{O}(-2))$ . How do I see directly that this bundle is in fact $\mathcal{O}(-1)?$","['vector-bundles', 'algebraic-geometry', 'characteristic-classes', 'obstruction-theory']"
3732087,How far can we take this probabilistic argument?,"$\newcommand{\set}[1]{\{#1\}}$ $\newcommand{\E}{\mathbb E}$ The Problem Let $G=(V, E)$ be a (finite, simple) graph.
Let $\chi:V\to \set{R, B}$ be a $2$ -coloring of $V$ .
We say that a vertex $v\in V$ is $\chi$ - good if the number of neighbors of $v$ which get the same color as $v$ is no more than the number of neighbors of $v$ which get different color than $v$ .
In other words, $$
|\set{u\in N(v):\ \chi(u) = \chi(v)}| \leq |\set{u\in N(v):\ \chi(u)\neq \chi(v)}|
$$ where $N(v)$ is the set of all the neighbors of $v$ .
A $2$ -coloring $\chi$ is said to be admissible if each vertex $v\in V$ is $\chi$ -good. Consider the following problem. Problem. Show that every graph admits an admissible $2$ -coloring. A proof of this problem can be given as follows. Given a $2$ -coloring $\chi$ of $G$ , we call an edge nice if the endpoints of this edge have different colors.
Choose a $2$ -coloring $\chi_0$ of $V$ which has the maximum number of nice edges.
We claim that any such coloring is admissible.
To see this, assume on the contrary that $\chi_0:V\to \set{R, B}$ is not admissible.
So there is a vertiex $v$ whihc is not $\chi_0$ -good.
But now flipping the color of $v$ increases the number of nice edges, producing a coloring which has more nice edges than present in $\chi_0$ .
This contradicts the maximality assumption. The Probabilistic Argument However, I would like to see how much can we get out of a probabilistic argument.
The following argument shows that there exists a $2$ -coloring which has at least $|V|/2$ good vertices. Color each vertex independently red or blue with probability $1/2$ .
So we can now talk about a random coloring.
For each $v\in V$ , let $G_v$ be a random variable valued in $\set{0, 1}$ which takes the value $1$ if $v$ is good, otherwise it takes the value $0$ .
Clearly, $P[G_v=1]=1/2$ , because a vertex will be good of not good with equal probability.
Therefore, $\E[G_v]=1/2$ , and hence, by linearity of expectation, we have $\E[\sum_{v\in V} G_v]=|V|/2$ .
This means that there must exist a coloring where we have at least $|V|/2$ good vertices. Question. How much further can we take this? Can we show, for instance, that there must exist a coloring with at least $3|V|/4$ good vertices via a probabilistic approach too?","['graph-theory', 'probabilistic-method', 'combinatorics']"
3732133,Probabilitic inequality for sum of squares of zero mean Gaussian random variables,"Let $X_1,...,X_n$ be i.i.d. standard normal random variables. How to show that there is constant $c>0$ such that for every $a_k>0$ : $P(\sum_{k=1}^{n}a_kX_k^2>\sum_{k=1}^{n}a_k+c\cdot\sqrt{\sum_{k=1}^{n}a_k^2})>c$","['inequality', 'probability']"
3732139,Argument of Feynman for equivalence of dot product definitions,"The following argument from the Feynman Lectures on Physics ( Vol I, Lecture 11 ), which relates to the equivalence of the algebraic and geometric definitions, does not particularly convince me. Also, there is a simple geometrical way to calculate $\vec{a} \cdot \vec{b}$ , without
having to calculate the components of $\vec{a}$ and $\vec{b}$ : $\vec{a} \cdot \vec{b}$ is the product of
the length of $\vec{a}$ and the length of $\vec{b}$ times the cosine of the angle
between them. Why? Suppose that we choose a special coordinate system
in which the x-axis lies along $\vec{a}$ ; in those circumstances, the only
component of $\vec{a}$ that will be there is $a_x$ , which is of course the whole
length of $\vec{a}$ . Thus Eq. (11.19) reduces to $a \cdot b = a_x b_x$ for this case, and
this is the length of $\vec{a}$ times the component of $\vec{b}$ in the direction of $\vec{a}$ , that is, $b \cos \theta$ : $a \cdot b = a b \cos \theta$ . Therefore, in that special coordinate
system, we have proved that $\vec{a} \cdot \vec{b}$ is the length of $\vec{a}$ times the length of $\vec{b}$ times $\cos \theta$ . But if it is true in one coordinate system, it is true
in all, because $\vec{a} \cdot \vec{b}$ is independent of the coordinate system ; that is
our argument. In fact, most of this argument seems just fine, but it seems like Feynman is casually asserting a priori that the dot product should be independent of the coordinate system. This is something I do not like, since I can't see an obvious justification for it. (Indeed, if by ""coordinate system"" he means basis, then there are clearly bases for which this is not true, e.g., ${2\hat{i}, 2\hat{j}, 2\hat{k}}$ .) Could someone who is better at reading between the lines of Feynman please clarify this for me?","['linear-algebra', 'vector-spaces']"
3732147,Crux problem #33 with vector approach,"On the sides $CA$ and $CB$ of an isosceles right-angled triangle $ABC$ , points $D$ and $E$ are chosen such that $|CD|=|CE|$ . The perpendiculars from $D$ and $C$ on $AE$ intersect the hypotenuse $AB$ in $K$ and $L$ respectively. Prove that $|KL|=|LB|$ . Proposed by Victors Linis, University of Ottawa. Crux Mathematicorum Vol. 1, No. 4, June, 1975 I want a solution via vectors and I'll explain why in the end of the question, tl;dr. The question consists of: the basic things we can do with vectors, how did I come to a regular geometric solution, regular geometric solution, motivation for vectors approach. To give more explicit context, I'll explain the basic things we can do with vectors to approach real geometry problems. We can add or subtract vectors, e.g. $\overrightarrow{AB}+\overrightarrow{BC}=\overrightarrow{AC}$ . We can scale a vector by a coefficient (say $k$ ) so if $A,B,C$ lie on the same line and $k=\frac{AC}{AB}$ then $\overrightarrow{AC}=k\overrightarrow{AB}$ . In particular, 1. and 2. follows that if $X$ is on $AB$ , such that $\frac{AX}{XB}=\frac{t}{1-t}$ then $\overrightarrow{OX}$ $=\overrightarrow{OA}+\overrightarrow{AX}$ $=
\overrightarrow{OA}+t\,\overrightarrow{AB}$ $=
\overrightarrow{OA}+t(\overrightarrow{OB}-\overrightarrow{OA})$$=
t\,\overrightarrow{OB}+(1-t)\,\overrightarrow{OA}$ . If some vectors form a basis, then every vector has unique representation as a linear combination of basis vectors with coefficients called ""coordinates"" (e.g. $\overrightarrow{i},\,\overrightarrow{j},\,\overrightarrow{k}$ is a classical basis for 3d Cartesian coordinates). Knowing only 1.-4. some problems like this (not in a way attention drawing) may be solved when a convinient basis is chosen, and even Ceva's_theorem , Menelaus's theorem , Thales' theorem can be proven, almost in an algebraic way. I'd call such ""linear vector problems"". But we know also Scalar (dot) product. By definition $\cos\angle BAC=\frac{\overrightarrow{BA}\cdot\overrightarrow{BC}}{
|\overrightarrow{BA}|\cdot|\overrightarrow{BC}|}$ , or, alternatively, $(\overrightarrow{BA}\cdot\overrightarrow{BC})=BA\cdot BC\cdot \cos\angle BAC$ . This implies such things like $(\overrightarrow{BA}\cdot \overrightarrow{BA})=(\overrightarrow{BA})^2=|\overrightarrow{BA}|^2=BA^2$ and $(\overrightarrow{BA}\cdot\overrightarrow{BC})=0\Leftrightarrow BA\perp BC$ unless $BA$ or $BC$ equals to zero. All distributive laws holds for addition/subtraction related to scalar or/and dot product. With 1.-5. such things like cosine rule , Heron's formula , Ptolemy's_theorem can be proven and I believe the problem above can be solved too.) We also know (though it's usage mostly limited by 3d Cartesian space) Cross product Having these tools, we can approach problems, where all conditions given and things to be proven/found are: parallelity, perpendicularity, fixed angles, intersection, intersection at a ratio (and maybe some others). But apparently we can't deal with circles, addition/subtraction of angles and many other things. But boiling a geometric problem down to algebra can be useful when no other ways seen. Other approaches are complex numbers or Cartesian coordinates, but vectors are unfairly less popular/known. I'd say, many vectors excercises are constructed just to train using vectors, instead of showing how real geometrical problems may be solved in an algebraic way. Arriving at regular geometrical solution I made the figure above in geogebra and started moving free point $D$ back and forth and see how the things change and I noted that somewhat asymmethrical that we have $3$ points on $AB$ and only two on $AE$ , I wanted inverse-image of $B$ to be present to. To construct it, I mirrored $B$ relative to $AE$ into $B'$ . By moving $D$ I noted that $BB'||CL||DK$ (and indeed, they all are perpendicular to $AE$ ) and that reminded me of Thales' theorem -- if we have say $F=BB'\cap AC$ then it would suffice to show that $DC=CF$ and use Thales' theorem. By ""method of gazely staring"" I found that $\triangle CFB\sim \triangle HEC$ , but it's obvious that $\triangle HEC\sim\triangle CEA$ , but $CA=CB$ and thus $CE=CF$ , but it's given that $CD=CE$ , which completes the proof. Geometrical solution, refined We take $F$ on the line $AC$ such that $BF||CL$ . $\angle FBC=\angle ECH$ , where $H=CL\cap EA$ . From right-angled $\triangle ECH$ : $\angle ECH=90^\circ -\angle CEH$ , but from right-angled $\triangle ECA$ : $\angle CAE=90^\circ -\angle CEH$ thus $\angle FBC=\angle ECH=\angle EAC$ hence $\triangle FBC$ and $\triangle EAC$ are congruent by ASA that follows $CF=CE$ , but it's given that $CD=CE$ thus $CF=CD$ and using Thales' theorem on lines $AB$ , $AC$ intersected by $BF \parallel CL \parallel DK$ we obtain $BL=LK$ , QED. But imagine I were at a contest without being able to use geogebra and move the point $D$ and to want to construct $BB'$ , then arriving at this solution with such additional constructions is highly doubtful. While vectors approach is pretty straightforward: algebraically express what's given and what's needed, solve algebraical problem, usually a linear equations system. That's why I want vectors solution. Other algebraical solutions, like Cartesian coordinates, complex coordinates or even something like barycentric coordinates are welcome as well. Thanks for reading this through out.)","['alternative-proof', 'euclidean-geometry', 'vectors', 'geometry']"
3732176,Doubt in buiding a bump function in a manifold,"This definition of a bump function is given in ""Introduduction to Manifolds"" by Loring W. Tu : Given a point $ p $ in a manifold $ M^n$ , a bump function in $p$ supported in $V$ is any non-negative function $ \rho: M \rightarrow \mathbb{R} $ which is identically $ \mathbf{1} $ in some neighborhood of $ p $ with $ supp (\rho) \subset V $ . I understand the process of creating a $C^\infty$ bump function in $R$ and $R^n$ , but when I move on to manifolds, the following happens: Take $V$ a neighborhood of $p$ and $(\varphi,U)$ a chart over $p$ such that $V \subset U$ . We have a $C^\infty$ bump function $\rho:\mathbb{R}^n \rightarrow \mathbb{R}$ in $\varphi(p)$ which is identically $\mathbf{1}$ in the closed ball $B[\varphi(p),a]$ supported in $B[\varphi(p),b]$ with $a<b<d(\varphi(p),\partial \varphi(V))$ . And now, the composition $\rho \circ \varphi:U\rightarrow \mathbb{R}$ have domain $U$ , not $M$ as I wish. I'm forgetting to do something here? Like, consider the null extension of $\varphi$ over $M$ ... But, if this is the case, what guarantees me that the composition $\rho \circ \varphi:M\rightarrow \mathbb{R}$ will be differentiable? I know that $\varphi:U\rightarrow \varphi(U)$ a diffeomorphism, but I can't solve the domain issue. Thanks!","['manifolds', 'multivariable-calculus', 'vector-analysis', 'differential-geometry']"
3732261,"Number of permutations $(p_1,\dots,p_6)$ of $\{1,\dots,6\}$ such that for any $1\le k\le5,(p_1,\dots,p_k)$ is not a permutation of $\{1,\dots,k\}$","Problem (INMO 1992 problem #4) Find the number of permutations $(p_1,p_2,p_3,p_4,p_5,p_6)$ of $\{1,2,3,4,5,6\}$ such that for any $k$ such that $1 \le k \le 5,$ $(p_1,...,p_k)$ does not form a permutation of $\{1,2,....k\}$ My attempt I have done a very ugly approach i.e doing case work and counting each case separately. After a long time after going through many mistakes,over countings, i have reached the correct answer $461$ . Initially, i have tried to come up with a recursive relation but i ended up missing too many cases. Question Since this an olympiad there must be a nicer and elegant solution. Can anybody share their insights to this problem? Thank you.","['permutations', 'combinatorics', 'contest-math']"
3732267,Equivalent definitions of the support of a measure,"It's not a homework, actually I was reading an article where the following was stated. Let $\Omega$ be a toplogical space and $\mathcal{F}$ its Borel $\sigma$ -algebra, i.e. the $\sigma$ -algebra generated by its open sets. Let $\mu$ be a probability measure on $\Omega$ . The support of $\mu$ , denoted by $\text{supp}\mu$ , is a closed subset of $\Omega$ which can be defined in three equivalent ways: (1) the set of all $\omega \in \Omega$ such that every neighborhood of $\omega$ has nonzero measure. (2) the intersection of all closed sets of measure 1. (3) the complement of the union of all open sets with measure zero. The equivalence between (2) and (3) is obvious since if $A$ is an open set of $\Omega$ with measure zero its complement $A^{c}$ is closed and has measure 1 and vice-versa. However, I have no idea how to prove that (1) is equivalent to (2) or/and (3). How do I address this problem?","['measure-theory', 'probability-theory', 'borel-measures']"
3732364,Nonisomorphic free ultrafilters on $\omega$,"Any bijection from $\Bbb N$ to itself transforms an ultrafilter on $\Bbb N$ to another (isomorphic) ultrafilter.  Any two principal ultrafilters are isomorphic in that sense. For free ultrafilters on $\Bbb N$ , there are $2^{2^{\aleph_0}}$ of them.  Since there are $2^{\aleph_0}$ bijections of $\Bbb N$ to itself, there are also $2^{2^{\aleph_0}}$ isomorphism classes of free ultrafilters on $\Bbb N$ .  So lots of free ultrafilters must be nonisomorphic to each other. Question: Can you give an explicit example or construction of two free ultrafilters on $\Bbb N$ that are not isomorphic?  Assume ZFC. (Added at the suggestion of @bof in the comments below, in case the question proves too difficult to answer directly): Give an explicit example of two filters such that no free ultrafilter extending one of them can be isomorphic to a free ultrafilter extending the other. Can you state a property, preserved by isomorphism, possessed by some but not all free ultrafilters? (1) is as good as the original question as far as I am concerned.","['filters', 'general-topology', 'set-theory']"
3732370,$f(xy + x +y) = f(xy) + f(x) + f(y)$ and $f(x)(y - x) + f(y)(x - y) \geq 0$.,"Let $f : \mathbb R \to \mathbb R$ that satisfies both 2 conditions , $f(xy + x + y) = f(xy) + f(x) + f(y)$ and $f(x)(y - x) + f(y)(x - y) \geq 0$ $\forall x,y \in \mathbb R$ .
Determine all such $f$ . My solution. Let $P(x,y) $ be $f(x)(y - x) + f(y)(x - y) \geq 0$ . Let $Q(x,y)$ be $f(xy + x + y) = f(xy) + f(x) + f(y)$ . First , I show that $f$ is decreasing $\forall x \in \mathbb R$ Proof : Let $x \gt y$ , from $P(x,y)$ we get that $f(x) \leq f(y)$ .  (Because $(x - y) \leq 0)$ So, $f$ in decreasing. $Q(0,0)$ $\to$ $f(0) = 3f(0)$ $\to$ $f(0) = 0$ . $Q(x,-x)$ $\to$ $f(-x^2) = f(-x^2) + f(x) + f(-x)$ . $\to$ $f(x) = -f(-x)$ . $P(x,2x)$ $\to$ $xf(x) \geq xf(2x)$ . $\to$ $f(x) \geq f(2x)$ . Since $f(x) \geq f(2x)$ , take $x \lt 0$ , we get that $f$ is increasing $\forall x \lt 0$ . This implies $f(x) = c$ and $f(x) = -c$ ; $c \in \mathbb R$ . $P(x,y)$ when $x,y \gt 0$ implies $-c = -3c$ , $c = 0$ . So, $f(x) = 0$ $\forall x \in \mathbb R$ .   Q.E.D. Is my proof correct? (This is from 2016 Thailand POSN Camp 2)","['contest-math', 'functional-equations', 'functional-inequalities', 'functions', 'solution-verification']"
3732372,On the joint numerical range of a pair of symmetric matrices,"Proposition 13.4 of Alexander Barvinok's A Course in Convexity shows the existence of the following result: Let $n\ge 3$ . For two $n\times n$ symmetric matrices $A$ and $B$ , and a PSD matrix $X$ with $\mbox{trace}(X) = 1$ , there exists a unit vector $x$ such that $x^T A x = \mbox{trace}(AX)$ and $x^T B x = \mbox{trace}(BX)$ . This book does not show how to construct such a vector! In Boyd & Vandenberghe's Convex Optimization , on page 656, there exists a constructive method for a looser version of above (basically, no constraints on the trace of $X$ and consequently, the magnitude of $x$ ). I could not so far use their proof for the above stronger version result and basically construct such $x$ . Any proof, idea, or help?","['positive-semidefinite', 'common-root', 'linear-algebra', 'optimization', 'matrix-decomposition']"
3732383,A proof of existence of canonical divisors,"I am confused with the proof of Lemma 1.5.10 in Algebraic Function Fields and Codes by Henning Stichtenoth. Let $0\ne\omega\in\Omega_F$ . Then there is a uniquely determined divisor $W\in M(\omega)$ such that $A≤W$ for all $A\in M(\omega)$ . $\Omega_F$ is the set of Weil diﬀerentials of function field $F/K$ , and $M(\omega)$ is the set of divisors $A$ such that $\omega$ vanishes on the adele space $\mathcal A_F(A)+F$ . In the proof he said: ... we can choose a divisor $W\in M(\omega)$ of maximal degree. Suppose $W$ does not have the property of our lemma. Then there exists a divisor $A_0\in M(\omega)$ with $A_0\not\le W$ , i.e. $v_Q(A_0)>v_Q(W)$ for some divisor $Q$ . We claim that $W+Q\in M(\omega)$ , which is a contradiction to the maximality of $W$ . In fact, consider an adele $\alpha=(\alpha_P)\in\mathcal A_F(W+Q)$ . We can write $\alpha=\alpha^\prime+\alpha^{\prime\prime}$ with $$\alpha_P^\prime=\begin{cases}\alpha_P&\text{ for }P\ne Q,\\0&\text{ for }P=Q,\end{cases}\quad\alpha_P^{\prime\prime}=\begin{cases}0&\text{ for }P\ne Q,\\\alpha_Q&\text{ for }P=Q.\end{cases}$$ Then $\alpha^\prime\in\mathcal A_F(W)$ and $\alpha^{\prime\prime}\in\mathcal A_F(A_0)$ . As far as I know, $\alpha^\prime\in\mathcal A_F(W)$ only if $0=v_Q(\alpha_Q^\prime)\ge-v_Q(W)$ , but I can see nowhere that $v_Q(W)\ge0$ , as well as $v_P(A_0)\ge0$ . Forgive me if this is trivial.","['function-fields', 'proof-explanation', 'algebraic-geometry']"
3732404,Compare two numbers without comparative operators,"Is it possible to have a function which compares two numbers without comparative operators so that it returns 1 if they are equal and 0 if they are not? (No <, >, ==, etc.) e.g. $f(x, y) = ?$ $f(20, 20) = 1$ $f(15, 20) = 0$ It's possible to get 1 if they are equal - $1 - (x-y)$ But I cannot figure any way to get 0 if they are not equal","['functions', 'linear-algebra']"
3732439,Finite additivity of a measure in the proof of Riesz representation theorem,"I am reading a proof of a version of the Riesz representation theorem and am stuck. Statement of Riesz representation theorem: Let $(X,d)$ be a compact metric space and $L$ a linear functional on the set $C(X)$ of continuous functions $f: X \to \mathbb{R}$ that satisfies: $L$ is continuous If $f \geq 0$ then $L(f) \geq 0$ The indicator function on $X$ satisfies $L(\mathbf{1}_X) = 1$ Then there is a unique Borel probability measure $\mu$ with $L(f) = \int f \, d\mu$ for each $f \in C(X)$ . The step that is troubling me: First, $\nu$ is defined: if $C \subseteq X$ is closed, $$
\nu(C) = \inf \{L(f): \mathbf{1}_C \leq f\}.
$$ And for general Borel subsets $A \subseteq X$ , $$
\mu(A) = \sup \{\nu(C): C \subseteq A \text{ is closed}\}.
$$ Then it is established that $\mu$ is the measure we are searching for. It is stated, but not proved, that $\mu$ is finitely additive. I could not verify this. What I could prove: If $C_1$ and $C_2$ are disjoint and closed, then $\mu(C_1 \cup C_2) = \mu(C_1) + \mu(C_2)$ . If $A_1$ and $A_2$ are disjoint Borel subsets, then $\mu(A_1 \cup A_2) \geq \mu(A_1) + \mu(A_2)$ . But why is the reverse inequality true, i.e., why is $\mu(A_1 \cup A_2) \leq \mu(A_1) + \mu(A_2)$ ?","['measure-theory', 'probability-theory']"
3732463,"If $\operatorname{lcm}(m, m + k) = \operatorname{lcm}(n, n + k)$, then $m = n$","Let $m, \ n, \ k \in \Bbb N $ be such that $ \operatorname{lcm}[m , m + k] = \operatorname{lcm}[n , n + k],$ then prove that $ m = n.$ Though I wasn't able to proceed much, but here is a sketch of what I tried.
First let $l = \operatorname{lcm}(m, m + k) = \operatorname{lcm}lcm(n, n + k)$ now we have $ m \mid l, \ (m + k) \mid l, \ n \mid  l, \ (n + k)\mid l $ then
next since $\gcd(m , m + k) \cdot \operatorname{lcm}(m, m + k) = m \cdot (m+k)$ but we also have $\gcd(m, m + k) = \gcd(m, k)$ and now we have $ \frac{m \cdot (m + k)}{\gcd(m, k)} = \frac{n \cdot (n + k)}{\gcd(n, k)} $ . Now I noticed if $ m \mid  k $ , then we are done, but that is not always possible, so I let $\gcd(m, k) = d_{1}$ and $\gcd(n, k) = d_{2}$ and started replacing, but it becomes more and more difficult in that way, so I quit here, Then I also thought to consider it as an equation asking us to solve $\operatorname{lcm}(m, m + k) = l$ but again since after dividing by $\gcd(m, k)$ it will give two corresponding quadratic so, this method also failed. Also, the book I am using hasn't introduced much of congruence and even the fact that $\gcd(a, b) \cdot \operatorname{lcm}(a,b) = a \cdot b $ so a solution without that will be nicer. Any help/hints are appreciated, Thanks in advance.","['number-theory', 'gcd-and-lcm', 'elementary-number-theory']"
3732475,How to solve this ODE with the Laplace transform?,"I want to solve this ODE $$ y'''-y''-y'+y= -10 \cos (2t-1)+5 \sin(2t-1) $$ with $y( \frac12)= 1 $ , $ y'( \frac12 )=2 $ , $y''( \frac12 )=1 $ $ t \in [ \frac12 , + \infty [ $ using the Laplace-Transformation. usually I use the differential-approach: $$ ( L( f^{(k)}))(s)=s^k(Lf)(s)- \sum_{j=0}^{k-1} s^j f^{(k-1-j)} (0) $$ so setting $Y(s)= L \{ y(t) \} $ I get $s^3 Y(s)-s^0y''(0)-s^1y'(0)-s^2y(0)-s^2Y(s)+s^0 y'(0)+s^1y(0)-sy(s)+s^0y(0)+Y(s)... $ I can't continue here, because of the initital values.
How can I solve this ODE? Do I need to shift it somehow?
Or do $ y''(0) etc..$ vanish, because $ t \in [ \frac12, \infty[ $ ? Appreciate any help ! EDIT
Thank you for the help so far ! after applying the shift theorem I solve $$z'''-z''-z'+z=-10 \cos (2t) +5 \sin(2t) $$ with $ z(0)=1,z'(0)=2,z''(0)=1 $ Using the differential approach it comes to $$s^3 Z(s)-1-2s-s^2-s^2Z(s)+2+s-sZ(s)+1+Z(s)= \frac{-10s+10}{(s^2+4} $$ $$ \Leftrightarrow Z(s)(s^3-s^2-s+1)= \frac{-10s+10}{(s^2+4)} -2+s^2+s $$ $$ \Leftrightarrow Z(s)= \frac{(-10s+10)-(s^2+4)(2+s^2+s)}{(s^3-s^2-s+1)(s^2+4)} $$ $$ \Leftrightarrow Z(s)= - \frac32 \frac{1}{s-1}- 2 \frac{1}{(s-1)^2} + \frac12 \frac{1}{s+1} + \frac{2}{s^2+4} $$ looking up the inverse it comes to $z(t)= - \frac32 e^t - 2t e^t+ \frac12 e^{-t}+ \sin(2t) $ but $ z'(t)= \cos(t)-2te^t- \frac{7 e^t}{2}- \frac{e^{-t}}{2} $ and so $ z'(0)=-2 \neq 2 $ where is my Mistake?","['derivatives', 'laplace-transform', 'ordinary-differential-equations']"
3732483,Quotient by a topological groupoid.,"Let $G$ be a group acting on a topological space $X$ , then the quotient map $X \to X/G$ is open. I want to ask, whether this fact generalizes to orbit spaces of groupoids. More precisely: Let $G$ be a topological groupoid. I will denote objects by $G_0$ , morphisms by $G_1$ , the source map by $s$ and the target map by $t$ . There is an equivalence relation $\sim$ on $G_0$ : $$x\sim y \text{ iff. there exists a morphism } g \in G_1 \text{ such that } s(g) = x \text{ and } t(g) = y.$$ It is easy to check that this is an equivalence relation. The class corresponding to $x$ will be called its orbit. Note that, the orbit of $x$ contains all those points in $G_0$ which are targets of morphisms originating at $x$ , or in other words, the orbit of $x$ is the set $t(s^{-1}(x))$ . Then, $G_0/\sim$ equipped with the quotient topology
is called the orbit space of the groupoid $G$ . My question is: is the quotient map $G_0 \to G_0/\sim$ open? Remark . We can recover the original statement about quotient by a group, by considering the translation gropoid $G \ltimes X$ (since its orbit space is precisely $X/G$ ). I haven't made any progress in the general case. EDIT: I was making things unnecessarily complicated. At least, for the étale case, the proof is laughably trivial; its just a point-set argument similar to what we do in the group case. First of note that if $p : X \to X/\sim$ is a quotient map and $A\subset X$ , then $p^{-1}(p(A))$ is the set of all the elements of $X$ which are related by $\sim $ to some element in $A$ . Now, let $G$ be an étale topological groupoid with orbit space $G_0/\sim$ and quotient map $q : G_0 \to G_0/\sim$ . Suppose $U$ is an open subset of $G_0$ . We need to show that $q(U)$ is open. It suffices to show that $q^{-1}(q(U))$ is open because $q$ is a quotient map. By the preceding observation, $q^{-1}(q(U))$ is the set of those elements $G_0$ which are in the orbit of some element $U$ , or in other words, it is the set of all points in $G_0$ which are targets of morphisms originating in $U$ . Thus, $q^{-1}(q(U)) = t(s^{-1}(U))$ , which is open because both $s$ and $t$ are local homeomorphism. So, that does it for the étale case.","['lie-groupoids', 'groupoids', 'topological-groups', 'quotient-spaces', 'general-topology']"
3732514,Is it possible to construct a Haar measure on a locally compact group assuming the existence of such a measure on compact groups?,"At first I thought that, once one proves the existence of a Haar measure on compact groups, it should be relatively straightforward to construct it on locally compact groups, by adequately piecing together Haar measures on compact neighbourhoods of points in a locally compact group. However, I then realized that these neighbourhoods need not have a group structure, which means that I can't make direct use of the existence of a Haar measure on compact groups. So, I was wondering if there is a way around this that doesn't require a completely different approach. In other words, is there a way to use the existence of a Haar measure on compact groups to prove the existence of a Haar measure on locally compact groups? If I think of the special case of $\mathbb{R}$ there seems to be no obvious way of doing this. Moreover, since no additive subgroup of $\mathbb{R}$ is compact, it doesn't seem possible to construct a Haar measure in this way.","['measure-theory', 'functional-analysis']"
3732573,Pseudo determinant of product of two square matrices,Let $A$ and $B$ be square symmetric matrices. $A$ is singular and $B$ is non-singular. Is there a way to decompose: $Det(AB)$ in terms of $Det(A)$ and $det(B)$ . $Det(.)$ refers to the pseudo determinant and $det(.)$ refers to the usual determinant of a square non-singular matrix.,"['matrices', 'determinant', 'linear-algebra']"
3732602,Arithmetic Progression Question (involving modulus inequalities and equations),OPTIONS: A) 10 B) 15 C) 25 D) None of the above,"['inequality', 'arithmetic-progressions', 'sequences-and-series', 'elementary-set-theory', 'algebra-precalculus']"
3732639,How to prove that $1^n+2^n+...+(p-1)^n \equiv 0\pmod p$? [duplicate],"This question already has answers here : What is $1^k+2^k+\cdots+ (p-1)^k$ modulo $p$? (From Ireland and Rosen). (3 answers) Closed 4 years ago . I have a homework for the university and I am 'on this' for the entire week, so I really need help. The question: let $p>2$ be a prime number and $n\in \Bbb N$ , $\ p-1\nmid n$ . Prove that $1^n+2^n+...+(p-1)^n \equiv 0\pmod p$ . I thought: it is pretty clear that $p-1$ is composite so I can write $p-1=q_1^{t_1}q_2^{t_2}...q_k^{t_k}=\prod_{i=0}^{k} q_i^{t_i}$ and I know that $2 \le q_i\le p-1 \ , \ \ 0 \le i \le k \ $ and that $ \ 2\mid p-1$ so $ \ p-1=2k$ . The sum is $\sum_{i=0}^{p-1} i^n=1^n+2^n+...+(p-1)^n \equiv 0\pmod p$ but I don't know what can I understand from that. I really need help. Thank you","['number-theory', 'elementary-number-theory', 'modules', 'primitive-roots', 'prime-numbers']"
3732730,"Book: Ron Larson, Calculus, Find the Limit if it exists?","I answered a question to determine whether a particular limit of a function exists and I got the right answer that it didn’t exist but for a different reason to what the book states. Question. Find the limit (if it exists) and if it does not exist explain why. $$\lim_{x\to-3^-}\left(\frac{x}{\sqrt{x^2-9}}\right)$$ My answer. The domain is restricted to the closed interval $[-3 , 3]$ .
The limit does not exist because the function is only continuous on the right-hand side of $x=3$ . Books Solution. The limit does not exist. The function decreases without bound as $x$ approaches $-3$ from the left. Can someone please explain if my solution would suffice and also what the book means by this, since I thought that the function was undefinded for values of $x<-3$ ?","['limits', 'calculus']"
3732735,Strong law of large numbers without independence,"Let $(X_n)_n$ be a sequence of independent random variables and identically distributed, $d \in \mathbb{N},$ $f: \mathbb{R^{d+1}} \rightarrow \mathbb{R}$ a measurable function, $Y_n=f(X_n,...,X_{n+d}),W_n=\frac{1}{n}\sum_{k=1}^nY_k.$ a) Prove that $Y_1 \in L^1$ if and only if $(W_n)_n$ converges a.s. In this case, Show that $(W_n)_n$ converges also in $L^1.$ b) If $k_1,...,k_{d+1} \in \mathbb{N},U_n=f(X_{n+k_1},...,X_{n+k_{d+1}}),$ deduce that a) remains true with $R_n=\frac{1}{n}\sum_{l=1}^n U_l.$ We suppose that there exists a sequence $(x_n)_n$ such that $W_n-x_n$ converges a.s.
Is it true that $Y_1 \in L^1?$ Attempt : In this problem, $(Y_n)_n$ are not independent, so we have to work with subsequences, and grouping terms. For the first part, $W_n$ converges a.s this implies that $\frac{Y_n}{n}$ converges a.s to $0,$ and that $\frac{Y_{(d+1)n}}{(d+1)n}$ converges a.s to $0$ , so $\frac{Y_{n(d+1)}}{n}$ a.s to $0$ , and since $(Y_{(d+1)n})_n$ is a sequence of i.i.d random variables, which means that $Y_1 \in L^1.$ If $Y_1 \in L^1,$ then we should write $$W_n=\frac{1}{n}\sum_{k=0}^d\sum_{l=0}^{ \left \lfloor{\frac{n-k}{d+1}}\right \rfloor }Y_{l(d+1)+k}$$ and we conclude using the strong law of large numbers. b) is simple, taking the projection, and considering $k=\max(k_{1},..,k_{d+1})$ and we apply a) Having problems with 2), if only, we can remove $x_n.$ Any ideas?","['measure-theory', 'law-of-large-numbers', 'probability-theory']"
3732799,How can I find the interval of x? I apply ratio test and get indeterminate. $\sum_{n=1}^\infty({(n+1)(n+2)....(2n)\over n^n})\space (x-2)^n$,"$$\sum_{n=1}^\infty({(n+1)(n+2)....(2n)\over n^n})\space (x-2)^n$$ Find the radius and interval of convergence of the power series given above. Hi! I am trying to find the interval of x first; however, whenever I apply ratio test, I get indeterminate. How can I find the interval of x? Here is what I've done: Convergence: $$\lim_{n\to\infty}{\cfrac{\lvert(n+2)(n+3)...(2n+2)\space(x-2)^{n+1}\rvert}{\lvert(n+1)^{(n+1)}\rvert}\over\cfrac{\lvert(n+1)(n+2)...(2n)\space(x-2)^n\rvert}{\lvert n^n\rvert}}<1$$ $=>$ $$\lim_{n\to\infty}\cfrac{\lvert 2(2n+1)(x-2)n^n\rvert}{\lvert(n+1)^{n+1}\rvert}<1$$ $=>$ L'Hospital: $$\lvert x-2\rvert\space\lim_{n\to\infty}\cfrac{\lvert (4n+2)\space n^n\rvert}{\vert(n+1)^{n+1}\rvert}<1$$ $=>$ $$\lvert x-2\rvert\space\lim_{n\to\infty}\cfrac{\lvert 4n^n+(4n+2).n^n.\ln(n)\rvert}{(n+1)^{n+1}.\ln(n).(\ln1)}$$ As you can see, ln1 makes it indeterminate and I am unable to find. How can I find the interval of x, so that I can find the interval of convergence and then the radius of convergence?","['analysis', 'sequences-and-series']"
3732846,"UC Berkeley Integral Problem: Show that $\int_0^{2\pi} \frac{\min(\sin x, \cos x)}{\max(e^{\sin x},e^{\cos x})}\ {\rm d}x = -4\sinh(1/{\sqrt2})$.","Show that $$\int_0^{2\pi} \frac{\mathrm{min}(\sin{x},\, \cos{x})}{\mathrm{max}\left(e^{\sin{x}},\, e^{\cos{x}}\right)}\ \mathrm{d}x = -4\sinh\left(\frac{1}{\sqrt{2}}\right).$$ this problem comes from the 2020 UC Berkeley Integration Bee and was not solved by either of the contestants. Any hints? My initial approach was to compute the maximum and minimum of the specified function by observing the graph for $x\in (0, 2\pi)$ but could not get very far. Thank you!","['integration', 'contest-math', 'definite-integrals', 'real-analysis', 'maxima-minima']"
3732876,Generalization of Jensen's inequality,"Let $X=(X_1,\dots,X_n)$ be a $\mathbb R^n$ -valued random vector such that $E(|X_i|)<\infty$ for all $i$ . Let $f: \mathbb R^n \to \mathbb R$ be a convex function. Jensen's inequality tells us that $E(f(X_1,\dots,X_n))$ exists (in $]-\infty,\infty]$ ) and that $$E(f(X_1,\dots,X_n)) \ge f(E(X_1),\dots,E(X_n)).$$ So if we replace each $X_i$ by its expectation $E(X_i)$ we get something smaller. Does this still hold if we substitute only some of the $X_i$ by their expectations? Question: Does it hold that $E(f(X_1,\dots,X_n)) \ge E(f(E(X_1),X_2\dots,X_n))$ ? Here are my thoughts: Using the conditional Jensen's inequality we get that \begin{align*}
E(f(X_1,\dots,X_n)) &= E(E(f(X_1,\dots,X_n)|X_2,\dots,X_n))\\
                    &\ge E(f(E(X_1|X_2,\dots,X_n),X_2\dots,X_n))
\end{align*} holds whenever $E(|X_1||X_2,\dots,X_n)$ is a.s. finite. If $X_1,\dots,X_n$ are independent it follows that $$E(f(X_1,\dots,X_n)) \ge E(f(E(X_1),X_2\dots,X_n))$$ and we can iterate this to get $$E(f(E(X_1),X_2\dots,X_n)) \ge E(f(E(X_1),E(X_2),X_3\dots,X_n)),$$ etc. But what if $X_1, \dots, X_n$ are not independent?","['measure-theory', 'jensen-inequality', 'real-analysis', 'probability-theory', 'probability']"
3732993,Proof that Expected Lifetime is longer than Remaining Lifetime if the Hazard Rate is increasing.,"Let $X$ be a positive, continuous random variable. Denote the density of $X$ by $f(x)$ and its CDF by $F(x)$ . Let $\bar{F}(x) = 1- F(x)$ be the survival function of $X$ . Given that the Hazard Rate, \begin{align}
\lambda(x) &= \frac{f(x)}{\bar{F}(x)}
\end{align} is increasing, i.e. $\lambda'(x) \geq 0$ , I want to prove that \begin{align}
\mathbf{E}\left[ X-c \mid X>c \right] &\leq \mathbf{E}\left[ X \right] 
\end{align} for any constant $c >0$ . Here's what I have tried so far: Let \begin{align}
\Lambda(x) &= \int_0^x \lambda(s) \mathop{}\!\mathrm{d} s.
\end{align} This implies the following \begin{align}
\lambda(x) &= \Lambda'(x),
\\
-\Lambda(x) &= \log \left(\bar{F}(x) \right),
\\
\bar{F}(x) &= e^{-\Lambda(x) }.
\end{align} Writing the expected values as integrals (using the Darth Vader rule) and using the above, I can rewrite the inequality as \begin{align}
\int_c^\infty e^{-\int_c^s \lambda(x) \mathop{}\!\mathrm{d}x } \mathop{}\!\mathrm{d}s &\leq
\int_0^\infty e^{
-\int_0^s \lambda(x) \mathop{}\!\mathrm{d}x } \mathop{}\!\mathrm{d}s.
\end{align} However, I do not see how this holds given $\lambda$ in increasing, and I can't think of a way using that assumption.","['conditional-expectation', 'expected-value', 'inequality', 'probability-theory', 'probability']"
3733001,Prove the following inequality $\sum_{k=0}^{n}(-1)^{k}f(a_{k})\geq f ( \sum _ { k = 0 } ^ { n } ( - 1 ) ^ { k } a _ { k } )$,"Suppose that a function $f$ is convex and increasing on $[0,+\infty)$ and $f(0)=0$ .Show that $$\sum _ { k = 0 } ^ { n } ( - 1 ) ^ { k } f ( a_ { k } ) \geq f \left( \sum _ { k = 0 } ^ { n } ( - 1 ) ^ { k } a _ { k } \right)$$ For any number $a _ { 0 } \geq a _ { 1 } \geq \ldots \geq a _ { n } \geq 0$ Please help me to solve this I have try using Jensen’s inequality.
But I have thought it many times. I still can’t do this.
Thank beforehand!","['jensen-inequality', 'calculus', 'convex-analysis', 'derivatives']"
3733013,Is convergence of expectations preserved by multiplication (under some special conditions)?,"Let $\{A_n\}_n$ and $\{B_n\}_n$ be two sequences of real-valued random variables.
Assume $A_n$ is nonnegative and $A_n \to A$ in distribution, with $E[A_n]=E[A]<\infty \  \forall n $ . $B_n$ is uniformly bounded, $B_n \to B$ in distribution, and $E[B_n]\to E[B]$ . $A_n B_n \to AB$ in distribution. So both sequences converge in distribution and there is convergence of the means (not in mean).
Can we conclude that $E[A_n B_n] \to E[AB]$ ?
Information on uniform integrability of $A_n$ is not available.","['probability-theory', 'functional-analysis', 'weak-convergence', 'random-variables']"
3733044,When can you divide out $dx$ in an integral as if it is a fraction?,"A recent question on Math SE included finding the antiderivative of $$\int y'y''\,dx,$$ where $y'=\frac{dy(x)}{dx},y''=\frac{d^2y(x)}{dx^2}$ as $y=y(x)$ . One approach to solve this problem is by direct substitution. Let $$u=\frac{dy}{dx}=y' \implies du=y''\,dx,$$ then the left hand side becomes $$\int  y'y''\,dx=\int u\,du=\frac{1}{2}u^2+C=\frac{1}{2}\left(y'\right)^2+C.$$ A second approach involves writing out the integral completely and then canceling the $dx$ $$\int  \frac{dy}{dx} \frac{d^2y}{dx^2}\,dx=\int \frac{d^2y}{dx^2}\,dy=\int y''\,dy=\int\frac{dy'}{dx}\,dy=\int\frac{dy'}{dx}y'\,dx=\int y'\,dy'=\frac{1}{2}\left(y'\right)^2+C.$$ In this approach, it is crucial to make the following observation: $$dy=\frac{dy}{\require{cancel} \cancel{dx}}\,\require{cancel} \cancel{dx}=y'\,dx \implies \frac{dy'}{dx}\,dy=\frac{dy'}{\require{cancel} \cancel{dx}}y'\,\require{cancel} \cancel{dx}=y'\,dy'.$$ Through reviewing other questions on Math SE and Math Overflow, it appears that you are always able to ""divide out"" the $dx/dx$ . This is because $dx$ is an infinitesimally small positive change in $x$ . Therefore, as $dx\neq 0$ you can divide out $dx$ with itself to conclude $$\frac{dx}{dx}=1$$ at any point in integration (assuming that what you are integrating is well-defined). This is further represented by the fact that the Riemann integral can be expressed as the limit of Riemann sums $$\int_a^b f(x)\,dx=\lim_{n\to\infty}\sum_{i=1}^nf(x_i)\Delta x,$$ where $\Delta x$ means an infinitesimally small step on the x-axis to correspond with the infinitesimally small change in $x$ associated with the Riemann integral. One can justify canceling the $dx$ terms by the answer shown inside this Math Overflow question . However, they would need to know differential forms which is a topic that I am unfamiliar with. A different answer on Math SE provides a more familiar explanation in which one can write the first fundamental theorem of calculus in Leibnitz notation as: $$\int _a^b \frac{df}{dx}\,dx = f(b) - f(a).$$ Inside this answer, it is shown that you can ""cancel"" the two $dx$ terms even though you are not literally cancelling $dx/dx$ . The fact that these two terms cancel is directly due to notational convenience. I'm curious if this sort of notational convenience will fail. I think that one could write $$\int \frac{dy}{dx}\,dx=\int \frac{dy}{\require{cancel} \cancel{dx}}\,\require{cancel} \cancel{dx}=\int dy,$$ $$\int \frac{dy}{dx}\frac{dy}{dx}\,dx=\int \frac{dy}{\require{cancel} \cancel{dx}}\frac{dy}{dx}\,\require{cancel} \cancel{dx}=\int \frac{dy}{dx}\,dy,$$ $$\int \frac{dy^n}{dx^n}\frac{dy}{dx}\,dx=\int \frac{dy^n}{dx^n}\frac{dy}{\require{cancel} \cancel{dx}}\,\require{cancel} \cancel{dx}=\int \frac{dy^n}{dx^n} \,dy,$$ $$\int \frac{dy}{dx}dy\,dx=\int \frac{dy}{\require{cancel} \cancel{dx}}dy\,\require{cancel} \cancel{dx}=\int (dy)^2.$$ Is there a scenario in which you cannot cancel $\frac{dx}{dx}$ inside an integral as if it were a fraction equal to $1$ ? Can you also cancel these two terms through a substitution made inside the integral?","['integration', 'calculus', 'ordinary-differential-equations']"
3733055,Lower bound on the roots of polynomial,"To solve a problem almost identical to the one in this question I would like to locate the zeros of the function $$
Q(z) = \sum_{k=0}^{m-1}(m-k)z^k = \frac{m - (m+1)z + z^{m+1}}{{(z-1)}^2}
$$ where $m$ is a positive integer.
In particular, I want to show that $Q(z)=0 \implies |z|>R$ for some $R \geq 1$ . Based on computer solutions it seems that the statement holds at least for $R=\frac{m+1}{m}$ , perhaps even $R=\frac{m+2}{m}$ , so I have mostly worked with discs of such radii while trying to prove the statement. I have tried using Rouché's theorem, but so far it has only given me bounds with $R<1$ . Inspired by plots of the roots I considered the equation $Q(z-c)=0$ shifting the roots by some real number $c$ but it didn't seem to help. A possibly related observation that I have not proven is that the roots of $Q(z)$ sum up to $-2$ .","['complex-analysis', 'roots', 'polynomials']"
3733081,computing $\pi_1(\mathbb{R}P^2 \vee\mathbb{R}P^2)$ and $\pi_1(\mathbb{R}P^2 \times \mathbb{R}P^2)$,"could someone please check my solution? I'm trying to learn algebraic topology on my own. First, I calculate the fundemantal group of $\mathbb{R}P^2$ . We know that $\mathbb{Z}/2 \rightarrow S^2\rightarrow \mathbb{R}P^2$ is a fibre sequence. Hence we have a long exact homotopy sequence $\dots\rightarrow \pi_2(\mathbb{R}P^2) \rightarrow \pi_1(\mathbb{Z}/2)\rightarrow \pi_1(S^2)\rightarrow \pi_1(\mathbb{R}P^2)\rightarrow \pi_0(\mathbb{Z}/2)\rightarrow \pi_0(S^2)\rightarrow\dots$ .
This becomes $\dots\rightarrow \pi_2(\mathbb{R}P^2) \rightarrow 0\rightarrow0\rightarrow \pi_1(\mathbb{R}P^2)\rightarrow \pi_0(\mathbb{Z}/2)\rightarrow 0\rightarrow\dots$ . Hence $\pi_1(\mathbb{R}P^2)\cong \pi_0(\mathbb{Z}/2) \cong \mathbb{Z}/2$ . Since $\mathbb{R}P^2$ is path connected, it follows that $\pi_1(\mathbb{R}P^2 \times \mathbb{R}P^2)\cong \pi_1(\mathbb{R}P^2) \times \pi_1(\mathbb{R}P^2)\cong \mathbb{Z}/2 \times \mathbb{Z}/2.$ To calculate $\pi_1(\mathbb{R}P^2 \vee \mathbb{R}P^2)$ , I use Van Kampen's Theorem with $A=B=\mathbb{R}P^2$ and $A \cap B \simeq \ast$ (i.e. it's contractible). Since $\pi_1(A \cap B)$ is trivial, we get that $\pi_1(\mathbb{R}P^2 \vee \mathbb{R}P^2)\cong \mathbb{Z}/2 \ast \mathbb{Z}/2.$ My questions: 1. have I made any mistakes?
2. Why is $\pi_i(\mathbb{Z}/2)=0$ , for $i \ge 1$ ?
3. is there another method besides Van Kampen Theorem to calculate $\pi_1(\mathbb{R}P^2 \vee \mathbb{R}P^2)$ ?","['general-topology', 'homotopy-theory', 'cw-complexes', 'algebraic-topology']"
3733094,Commutativity of morphisms and existence of unique map,"Let $X,Y,Z$ be sets
Suppose $f:X\rightarrow Z$ and $g: X\rightarrow Y$ are two maps. Suppose $g$ is surjective. Show that there exists a unique map $h: Y\rightarrow Z$ such that $h\circ g= f$ This problem is really a problem in elementary set theory. Existence: Since $g$ is surjective, for each $y\in Y$ there exists atleast one $x\in X$ such that $g(x)=y$ .
So define $h:Y\rightarrow Z$ by specifying $h(y)=f(x)$ . Now, we need to verify that the map $h$ is well defined. In other words, If $y_1=y_2$ then $h(y_1)=h(y_2)$ . To this end, suppose $y_1=y_2$ for $y_i\in Y$ . By surjectivity of $g$ , there exists some $x_1,x_2\in X$ such that $g(x_1)=g(x_2)$ . Since $y_1=y_2$ , we may assume $x_1=x_2$ . Since $f$ is well defined, $f(x_1)=f(x_2)$ and so, $h(y_1)=h(y_2)$ . Clearly, $h\circ g=f$ . Uniqueness: Suppose there exists another map $h':Y\rightarrow Z$ for which $h'\circ g=f$ .
I must now show that $h=h'$ . For $y\in Y$ , there exists some $x\in X$ , such that $g(x)=y$ . Hence, $h(y)=f(x)=h'(g(x))=h'(y)$ . Therefore $h=h'$ . Is this correct?","['elementary-set-theory', 'functions', 'solution-verification']"
3733146,How can I evaluate: $\int_a^b \frac{1}{\sqrt{1-\cos \theta}}d\theta$?,I want to evaluate: $$\int_a^b \frac{1}{\sqrt{1-\cos \theta}}d\theta$$ I get stuck if I try a $u$ substitution and I have tried changing the expression using trigonometric identities but still no luck. How would I go about evaluating this ?,"['integration', 'calculus', 'trigonometry']"
3733156,"Is it a necessary condition for an even function to have a local extremum (for $f(x)=k,$ derivative${}=0$) at $x=0$","Let $f(x)$ be an even function ( $f(-x)=f(x)$ ) if $f(x)$ is continuous and differentiable at $x = 0$ will it be necessary for it to have a local extremum? Or more generally, have it's derivative $=0$ at $x=0$ ? I thought this as: $$f(x+h)-f(x)=f(x-h)-f(x) \\ \text{(for $x=0 , h>0$)}$$ so the derivative should also be zero. Am I correct or is there a counter example?","['calculus', 'derivatives']"
3733175,Calculate $\lim_{x \rightarrow -\infty} |x \sin x|$,$\lim_{x \rightarrow -\infty} |x \sin x|=\lim_{x \rightarrow -\infty} |x| |\sin x|=+\infty$ because $|\sin x|$ is bounded but the result is that the limit doesn't exist. Why?,"['limits', 'real-analysis']"
3733233,How to solve this ODE system with an implied integration within it?,"I have this system of differential equations in time, which I am trying to solve numerically through a matlab code: $$
\frac{df(x,t)}{dt} = K(t) -Af(x,t) - B\sigma \left[C f(x,t)g(x,t)+\frac{(1-C)}{D^2}\int_0^\infty x'e^{\frac{-x'}{D^2}}f(x,t)g(x,t)dx'\right]$$ $$
\frac{dg(x,t)}{dt} = \beta Af(x,t) + B\sigma f(x,t)g(x,t) - \frac{B}{x}g(x,t)
$$ Where A, B, $\beta$ , $\sigma$ , C and D are known constants. I also know K(t) time dependency is gaussian. I have no idea how to solve this, as I have no information about neiter $f(x,t)$ nor $g(x,t).$ The only thing I know is that both are $0$ at $t=0$ and at $x=0$ . I don't want the solution to the problem, just possible ways to solve it numerically.","['integration', 'ordinary-differential-equations']"
3733244,$X=S^2/{\sim}$ where any point on the equator is identified with its antipodal point. Compute $\pi_1(X)$ and $H_\ast(X)$,"My instructor gave me an idea. He used Van Kampen's Theorem to calculate the fundamental group, with $A=B=\mathbb{R}P^2$ and $A \cap B=S^1$ (the equator). I know $\mathbb{R}P^2$ is the quotient space of $S^2$ where each point is identified with its antipodal point. How can $A$ and $B$ can be $\mathbb{R}P^2$ if we only identify points on the equator of $S^2$ with their antipodal points? By Van Kampen's Theorem, $\pi_1(X)$ is the pushout of $\pi_1(\mathbb{R}P^2) \leftarrow \pi_1(S^1) \rightarrow \pi_1(\mathbb{R}P^2)$ or $\pi_1(X)\cong \pi_1(\mathbb{R}P^2) \ast \pi_1(\mathbb{R}P^2)/N$ . How exactly do I find the normal subgroup $N$ ? I know that $\pi_1(\mathbb{R}P^2)\cong \mathbb{Z}/2$ and $\pi_1(S^1) \cong \mathbb{Z}$ . I think I can use Mayer Vietoris sequence to calculate homology, with the same $A$ and $B$ . If $n>1$ , then $H_n(S^1)=0$ so $H_n(\mathbb{R}P^2) \oplus H_n(\mathbb{R}P^2) \cong H_n(X)$ . However, I'm stuck on calculating $H_0(X)$ and $H_1(X)$ . I believe the Mayer Vietoris sequence looks like this $\rightarrow H_2(X)\rightarrow H_1(S^1) \rightarrow H_1(\mathbb{R}P^2) \oplus H_1(\mathbb{R}P^2)\rightarrow H_1(X) \rightarrow H_0(S^1) \rightarrow H_0(\mathbb{R}P^2)\oplus H_0(\mathbb{R}P^2)\rightarrow H_0(X)$ . This reduces to $\rightarrow \mathbb{Z} \oplus \mathbb{Z}\rightarrow \mathbb{Z} \rightarrow \mathbb{Z}/2 \oplus \mathbb{Z}/2\rightarrow H_1(X) \rightarrow \mathbb{Z} \rightarrow \mathbb{Z} \oplus \mathbb{Z}\rightarrow H_0(X)$ . I know the sequence is exact so the kernel of the map going out of $H_1(X)$ is the image of the map coming in, but how does that help or is there another way to calculate $H_1(X)$ and $H_0(X)$ ? I don't know what the image or kernel of any of these maps is.","['cw-complexes', 'homotopy-theory', 'general-topology', 'homology-cohomology', 'algebraic-topology']"
3733261,Derive the recurrence relations,"Let $P_{m,n}=P_{m,n}(x,y)$ be a polynomial family. Here is some initial terms $$ P_{0,0}=1, P_{1,0}=2x, P_{0,1}=2y, P_{1,1}=8xy.$$ I  know that the polynomials for any $m,n \geq 0$ satisfies the five  differential recurrence relations \begin{align}
&n \frac{\partial P_{m,n-1}}{\partial x}=m \frac{\partial P_{m-1,n}}{\partial y},\\
& x \frac{\partial P_{m,n}}{\partial x}=m P_{m,n}+m\frac{\partial P_{m-1,n}}{\partial x},\\
& y\frac{\partial P_{m,n}}{\partial x}=m P_{m-1,n+1}+n \,\frac{\partial P_{m,n-1}}{\partial x},\\
& y \frac{\partial P_{m,n}}{\partial y}=n P_{m,n}+n\frac{\partial P_{m,n-1}}{\partial y},\\
& x\frac{\partial P_{m,n}}{\partial y}=n P_{m+1,n-1}+m \,\frac{\partial P_{m-1,n}}{\partial y}.
\end{align} Also, they satisfies the differential equation $$
(1-x^2) \frac{\partial^2 P_{m,n}}{\partial x^2} -x y \frac{\partial^2 P_{m,n} }{\partial x \partial y} -(n+3) x \frac{\partial  P_{m,n}}{\partial x }+m y \frac{\partial  P_{m,n}}{\partial y }+m(m+n+2) P_{m,n}=0,
$$ for any $m,n.$ I need to eliminate all the derivatives and get  pure  recurrence relations for $P_{m,n}$ . By numeric expеriments I guess such recurrence relations $$
2 (1{+}m{+}n) x P_{m,n}=P_{m+1,n}{-}n(n{-}1)P_{m+1,n-2}{+m(m+2n+1)}P_{m-1,n},\\
2 (1{+}m{+}n) y P_{m,n}=P_{m,n+1}{-}m(m{-}1)P_{m-2,n+1}{+}n(n+2m+1)P_{m,n-1},
$$ but I still  cant prove it. Any help? P.S. There  is an exact expression $$
P_{m,n}=m! n! 2^{m+n}  \sum_{i=0}^{\frac{m}{2}} \sum_{j=0}^{\frac{n}{2}} (-1)^{i+j}\frac{(m{+}n{-}i{-}j)!}{i! j! (m{-}2i)! (n{-}2j) 2^{2(i+j)}} x^{m-2i} y^{n-2j}.
$$","['combinatorics', 'special-functions', 'recurrence-relations']"
3733275,Continuity of this Piecewise function $f:\mathbb{R}^2\to \mathbb{R}$,"I have shown that $f:\mathbb{R}^2\to\mathbb{R}$ given by $f(0,0) = 0$ and $\displaystyle f(x,y)=\frac{x|y|}{\sqrt{x^2+y^2}}$ if $(x,y)\ne (0,0)$ isn't differentiable at $(0,0)$ , now I'm trying to show whether it is continuous or not. My attempt: I must show that $\displaystyle\lim_{(x,y)\to(0,0)}\frac{x|y|}{\sqrt{x^2+y^2}}=f(0,0) = 0$ . But $x^2+y^2-2|xy| = (|x|-|y|)^{2}\ge 0$ so $\displaystyle|xy|\le \frac{x^2+y^2}{2}$ . So $\displaystyle\left|\frac{x|y|}{\sqrt{x^2+y^2}}\right| =\frac{|xy|}{\sqrt{x^2+y^2}}\le \frac{x^2+y^2}{\sqrt{x^2+y^2}} = (x^2+y^2)^{1/2}$ and once $(x^2+y^2)^{1/2}\to0$ as $(x,y)\to(0,0),$ we would have it. Is this correct?","['real-analysis', 'continuity', 'multivariable-calculus', 'solution-verification', 'limits']"
