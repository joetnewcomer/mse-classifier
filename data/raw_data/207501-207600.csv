question_id,title,body,tags
4151438,Recurrence Relation: $a_{n+2}+a_n=5\cos (n\pi/3)-7\sin(n\pi/4)$,"Recurrence Relation: $a_{n+2}+a_n=5\cos \left(\frac{n\pi}{3}\right)-7\sin\left(\frac{n\pi}{4}\right)$ Attempt: The solution of the associated homogeneous relation is \begin{align}
a_n^{(h)}&=c_1\left(\cos \left(\frac{n\pi}{2}\right)+i\sin \left(\frac{n\pi}{2}\right)\right)+c_2\left(\cos \left(\frac{n\pi}{2}\right)-i\sin \left(\frac{n\pi}{2}\right)\right)\\ &=k_1\cos \left(\frac{n\pi}{2}\right)+k_2\sin \left(\frac{n\pi}{2}\right)
\end{align} where $k_1=c_1+c_2$ and $k_2=(c_1-c_2)i$ then \begin{align*}
a_n^{(p)} = A\cos \left( \frac{n\pi}{3} \right)+B\sin \left( \frac{n\pi}{3} \right) +C \cos \left( \frac{n\pi}{4} \right)+D\sin \left( \frac{n\pi}{4} \right)
\end{align*} is the particular solution. The four constants $A, B, C, D$ can be calculated by substituting $a_n^{(p)}$ in the given non-homogeneous recurrence relation. But I find it difficult to calculate.",['discrete-mathematics']
4151439,Reference and proof of binomial tail identity,"May I have a reference and/or proof of this identity? I saw it mentioned on mathoverflow and don't see how to show it. For $p \in (0,1)$ and $0 \leq k < n$ , $$ \sum_{i=0}^k {n \choose i} p^i (1-p)^{n-i} = (1-p)^{n-k} \sum_{i=0}^k {n-k-1+i \choose i} p^i .$$ I've verified this numerically and tried some applying the binomial theorem to $(1-p)^{k-i}$ on the left, but that didn't seem to help, so I thought I'd ask for a reference. Update : I have found a combinatorial proof and posted it as an answer below.","['reference-request', 'binomial-coefficients', 'combinatorics', 'binomial-theorem', 'probability']"
4151447,Definitions of solvable group,"A solvable group seems to be variously defined as one with a composition series where all the composition factors are Abelian, or as one with a subnormal series where all the quotients are Abelian. Unlike with the first definition, this definition does not explicitly seem to require that the quotients be simple. Are they nonetheless equivalent? Similarly, when looking at e.g. proofs that finite $p$ -groups are solvable, I have seen the inductive argument which constructs a subnormal series with all the quotients Abelian but they never seem to show that the quotients are also simple. Why? Can one give a proof which does show that they are simple, and therefore complies with the composition series definition of solvable? Thanks in advance for assistance.","['definition', 'abstract-algebra', 'p-groups', 'group-theory', 'solvable-groups']"
4151473,Locus of point M such that its projections onto two fixed lines are always at the same distance.,"A point moves so that the distance between the feet of the perpendiculars drawn from it to the lines $ax^2+2hxy+by^2=0$ is a constant $c$ . Prove that the equation of its locus is $$4(x^2+y^2)(h^2-ab)=c^2(4h^2+(a-b)^2).$$ What I have done: Suppose $P$ is any point on the locus, $A$ and $B$ be the feet of the perpendiculars, and $O$ is the origin. Then I want to show that $$OP=\frac{AB}{\sin AOB}=\frac{c}{\sin \theta}$$ where $\theta$ is the angle between the two lines. Note that the arc $PAO$ is a semi-circle, since $\angle PAO=90$ . Similarly the arc $PBO$ is a semi-circle since $\angle PBO=90$ . Also $\angle APB=\angle AOB$ , as it is the angle of the same arc. Now, $\tan \theta =\frac{2\sqrt{h^2-ab}}{a+b}$ . From here I can find $\sin \theta$ and $OP=\sqrt{x^2+y^2}$ . Putting all these in, $$OP=\frac{c}{\sin \theta}$$ will give me the equation of the locus. The problem is how to show that $$OP=\frac{AB}{\sin AOB}.$$","['analytic-geometry', 'geometry']"
4151484,Proof of fundamental theorem of calculus part 1 Rudin Theorem 6.20,"I have a question about the following proof from Rudin's Principles of Mathematical Analysis. 6.20 Theorem Let $f \in \Re$ on $[a,b]$ . For $  a \leq x \leq b$ , put $$F(x)  = \int_a^x f(t)dt$$ Then $F$ is continuous on $[a,b]$ ; furthermore, if $f$ is continuous at a point $x_0$ of $[a,b]$ , then $F$ is
differentiable at $x_o$ and $$F'(x_0) = f(x_0)$$ (I have omitted the proof of continuity of $F$ on $[a,b]$ ) Suppose $f$ is continuous at $x_0$ . Given $\epsilon > 0 $ choose $\delta > 0$ such that $$\vert f(t)- f(x_o) \vert < \epsilon $$ if $\vert t- x_0 \vert < \delta$ , and $a \leq t \leq b $ . Hence, if $x_0 - \delta < s \leq x_0 \leq t < x_0 + \delta$ $\enspace$ with: $a\ \leq s < t \leq b$ we have by theorem 6.12(d) $$\left| \frac{F(t) - F(s)}{t-s} - f(x_0) \right| = \left| \frac{1}{t-s} \int_s^t [f(u) - f(x_0)]du \right| < \epsilon$$ it follows that $F'(x_0) = f(x_0)$ Why does Rudin use $s$ rather than $x_0$ in the epsilon portion of proving the derivative exists? If we are proving the derivative exists at $x_0$ , I would expect that we would prove that $$\left| \frac{F(t) - F(x_0)}{t-x_0} - f(x_0) \right| = \left| \frac{1}{t-x_0} \int_{x_0}^t [f(u) - f(x_0)]du \right| < \epsilon$$","['riemann-integration', 'derivatives', 'real-analysis']"
4151505,Finding a basis and dimension for a symmetric matrices subspace,"Question: Let $\mathbb{F}$ be $\mathbb{Z}_7$ . Let $A=\begin{bmatrix}
2 & 5\\
5 & 3
\end{bmatrix} \in M_{2\times 2}( F)$ . Let $W=\left\{B\in M_{2\times 2}( F)\Bigl|( AB)^{t} =AB\right\}$ be a subspace over $\mathbb{F}$ . Find a basis and a dimension for $W$ . My attempt: $Solution.$ $\text{By the given information we have the following: }$ \begin{gather*}
AB=\begin{bmatrix}
2 & 5\\
5 & 3
\end{bmatrix} \cdotp \begin{bmatrix}
a & b\\
c & d
\end{bmatrix} =\begin{bmatrix}
2a+5c & 2b+5d\\
5a+3c & 5b+3d
\end{bmatrix}\\
\end{gather*} \begin{gather*}
AB^{t} =\begin{bmatrix}
2a+5c & 5a+3c\\
2b+5d & 5b+3d
\end{bmatrix}\\
\end{gather*} $\text{By W's condition we get:}$ \begin{gather*}
AB^{t} =AB\Longrightarrow \begin{bmatrix}
2a+5c & 5a+3c\\
2b+5d & 5b+3d
\end{bmatrix} =\begin{bmatrix}
2a+5c & 2b+5d\\
5a+3c & 5b+3d
\end{bmatrix}\\
\\
\Longrightarrow 2b+5d=5a+3c\\
\\
\Longrightarrow d=\frac{5a+3c-2b}{5} =a+\frac{3c-2b}{5} =a+\frac{3c+\overbrace{7c}^{0} -2b+\overbrace{7b}^{0}}{5} =a+2c+b
\end{gather*} $\text{So $AB$ is depends on 3 free-parameters:}$ \begin{equation*}
 \begin{aligned}
AB=\begin{bmatrix}
2a+5c & 2b+5d\\
5a+3c & 5b+3d
\end{bmatrix} & & = & & \begin{bmatrix}
2a+5c & 5a+3c\\
5a+3c & 5b+3( a+2c+b)
\end{bmatrix}\\
 & &  & & \begin{bmatrix}
2a+5c & 5a+3c\\
5a+3c & 5b+3a+6c+3b
\end{bmatrix}\\
 & &  & & \begin{bmatrix}
2a+5c & 5a+3c\\
5a+3c & 8b+3a+6c
\end{bmatrix}\\
 & &  & & \begin{bmatrix}
2a+5c & 5a+3c\\
5a+3c & b+3a+6c
\end{bmatrix}
\end{aligned}
\end{equation*} $\text{We take all the paramaters out of the marices, so we can get the basis vectors.}$ $\text{We get the following:}$ \begin{equation*}
\begin{bmatrix}
2a+5c & 5a+3c\\
5a+3c & b+3a+6c
\end{bmatrix} =a\cdotp \begin{bmatrix}
2 & 5\\
5 & 3
\end{bmatrix} +b\cdotp \begin{bmatrix}
0 & 0\\
1 & 0
\end{bmatrix} +c\cdotp \begin{bmatrix}
5 & 3\\
3 & 6
\end{bmatrix}
\end{equation*} $\text{Which isomorphic to the following vectors: }$ \begin{equation*}
( 2,5,5,3) ,( 0,0,1,0) ,( 5,3,3,6)
\end{equation*} $\text{respectively.}$ $\text{Now, we shall check whether those vectors are linear independent. }$ \begin{gather*}
\begin{bmatrix}
2 & 5 & 5 & 3\\
0 & 0 & 1 & 0\\
5 & 3 & 3 & 6
\end{bmatrix}\xrightarrow[ \begin{array}{l}
\mathcal{L}_{3} +\mathcal{L}_{1}\rightarrow \mathcal{L}_{3}\\
\end{array}]{}\begin{bmatrix}
2 & 5 & 5 & 3\\
0 & 0 & 1 & 0\\
0 & 1 & 1 & 2
\end{bmatrix}\xrightarrow[\mathcal{L}_{1} -5\mathcal{L}_{3}\rightarrow \mathcal{L}_{1}]{}\begin{bmatrix}
2 & 0 & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 1 & 1 & 2
\end{bmatrix}\\
\\
\xrightarrow[ \begin{array}{l}
\frac{\mathcal{L}_{1}}{2}\rightarrow \mathcal{L}_{1}\\
\mathcal{L}_{3} -\mathcal{L}_{2}\rightarrow \mathcal{L}_{3}
\end{array}]{}\mathcal{\begin{bmatrix}
1 & 0 & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 1 & 0 & 2
\end{bmatrix}}\xrightarrow[\mathcal{L}_{3}\leftrightarrow \mathcal{L}_{2}]{}\mathcal{\begin{bmatrix}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 2\\
0 & 0 & 1 & 0
\end{bmatrix}}
\end{gather*} $\text{Therefore, the vectors are linear independent, so they are basis of $\displaystyle W$,  }$ $\text{and since we have 3 linear independent vectors, we conclude that:}$ \begin{equation*}
\dim W=3
\end{equation*} Thoughts: Is what I wrote correct? or perhaps I missed something? I can't see if I am right or wrong, because I haven't solved questions of finding a basis for matrices subspaces. I will be glad for some help. Thank you!","['matrices', 'linear-algebra']"
4151510,Question on Taylor theorem (for $n+1$ differentiable function),"Taylor's theorem says the following: Let $f:\mathbb{R}\to \mathbb{R}$ $n+1$ times differentiable on $]a,b[$ and $x_0 \in ]a,b[$ . Then, $\forall x \in ]a,b[ \ \exists y \in ]x_0,x[$ s.t $f(x)=\sum_{k=0}^{n}\frac{f^{(k)}(x_0)}{k!}(x-x_0)^k+\frac{f^{(n+1)}(y)}{(n+1)!}(x-x_0)^{n+1}$ where $\frac{f^{(n+1)}(y)}{(n+1)!}(x-x_0)^{n+1}$ is the rest. Moreover, if $f^{(n+1)}$ is continuous on $[a,b]$ , we have that $\frac{f^{(n+1)}(y)}{(n+1)!}(x-x_0)^{n+1}=o(|x-x_0|^n)$ . So, now i have a question: In one of my exercices I have a function $f \in C^3$ such that at $x_0$ the function has a minimum and $f''(x_0)=0$ (the goal is to show then that $f'''(x_0)=0$ or not, but It is not my question). In corrections the function $f$ in a neighborhood of $x_0$ is written as the following (we put $x=x_0+h$ ) $f(x_0+h)=f(x_0)+\frac{1}{6}f'''(x_0)h^3+o(h^3)$ What I don't understand, is how we get the previous equality. By Taylor's theorem, we can express $f$ as the following: $f(x_0+h)=f(x_0)+\frac{f'''(y)}{3!}(x-x_0)^{n+1} $ for some $y \in ]x_0,x[$ . I don't understand how to get the same equality as before. Thank you in advance for help.","['taylor-expansion', 'functions', 'derivatives', 'polynomials']"
4151516,Towers of Hanoi if big disks can go on top of small disks,"The Tower of Hanoi puzzle is concerned with moving $n$ disks between three pegs so that a larger disk cannot be placed on top of a smaller disk. Based on a (now deleted) StackOverflow question, suppose that one can place larger disks above smaller ones. One can represent the game state by a 3-tuple of ordered sets $(A, B, C)$ . For example, the solved state with $3$ disks is given by $([3,2,1], [], [])$ : 1
2
3
* * * Question: given an arbitrary game state, what is a minimal sequence of moves that reaches the solved state? ( this thread suggests that reaching the solved state is always possible). There is a unique solved state with all disks placed on the first peg in order (illustrated above). Ideally, I am interested in an algorithm that reaches the solved state with fewest moves. If describing such an algorithm is difficult, I would also be interested in the minimal number of moves required to reach an arbitrary game state from any other game state (the diameter of the game state graph). Calculation by @PeterLang suggests that the diameter of the game state graph is given by [1, 4, 7, 10, 13, 16, 19, 22, 26, 29] for the number of disks ranging from 1 to 10. There appears to be only one OEIS sequence matching this pattern, and I have no clue if it should generalize. Here is an example of solving the game with $3$ disks: Given an initial state $([2], [1], [3])$ , one can reach the solved state as follows: 1        
           2         2      2         2     
2 1 3      1 3     3 1      3 1       3    
* * * => * * *  => * * * => * * * =>  * * * with associated sequence of moves $[1 \to 2], [3\to 1], [2 \to 1], [2 \to 1]$ . I computed a graph $G$ with vertices corresponding to game states, so that an edge is drawn between two game states whenever one can be reached from another with a single legal move. Here is an example with two disks: Surprisingly, the graph diameter seems to grow slowly. I wonder if it is always possible to reach the solved state in at most $1 + 3n$ moves, where $n$ is the number of disks (Peter's answer disproves this). Graph Diameter  Number of Vertices
Number of Disks                                    
1                             1                   3
2                             4                  12
3                             7                  60
4                            10                 360
5                            13                2520","['puzzle', 'graph-theory', 'combinatorics', 'discrete-mathematics', 'recreational-mathematics']"
4151523,"For $\int_{x}^{x+dx} p(x) \,dx = P(x)$ which one is the correct interval, $(x,x+dx)$ or $[x,x+dx]$?","In Physics (both in Statistical & Quantum Mechanics) when we describe the probability function of finding a particle between $x$ and $x+dx$ , we write $\int_{x}^{x+dx} p(x) \,dx = P(x)$ . Here in some books the interval is chosen as $(x,x+dx)$ and in some other books it's chosen as $[x,x+dx]$ . Depending on the chosen interval, the writing will either say: “between $x$ and $x+dx$ “ or “from $x$ to $x+dx$ “, respectively. Though the probability in finding a particle at the endpoints $x$ , $x+dx$ is zero, so it's not necessary to include or exclude the boundary points, as the integral will give the same result, but for the Physics and Physical arguments, which one is correct and why? Edit: My understanding (I may be wrong): if we consider a spherical shell at a distance $x$ from the origin with width $dx$ , then the particle would be in a small shell of $dx$ from the distance $x$ . My question is if we define the open interval and closed interval in the probability definition, then we are respectively excluding and including the fact that the particle may not or may access the position $x$ and $x+dx$ respectively. Both can't be correct according to my understanding. So, which one should be correct and why?","['quantum-mechanics', 'calculus', 'integration', 'probability']"
4151612,"System of two ODEs, problem with finding eigenvectors","I am fairly new to systems of ODEs so bear with me. $$
\left\{ 
\begin{array}{c}
x' = 2x-y \\ 
y' = x+2y \\ 
\end{array}
\right. 
$$ So, I know I have to find the eigenvalues, which is not a problem for me. The eigenvalues are $2 \pm i$ After inserting the first value in the matrix of the system and multiplying it by a column-matrix consisting of $C_1$ and $C_2$ , I get the system: $$
\left\{ 
\begin{array}{c}
-iC_1 - C_2 = 0\\ 
C_1-iC_2=0 \\ 
\end{array}
\right. 
$$ Now, my question is as follows: If I express $C_2$ from the first equation I get $C_2 = -i C_1$ . After inserting that in the second equation I get that $C_1 - C_1 = 0$ , which leads me to think that $C_1$ is any real number. After choosing the number $1$ , I get that the eigenvector is $v_1 = (1,-i)$ However, WolframAlpha says that the eigenvector is $(i,1)$ , which leads me to think that I should have expressed $C_1$ from the second equation and substituted it into the first, which is when I get the same solution as in WolframAlpha. But does it matter what variable I express though? I'm also new to eigenvectors so I don't know a whole lot about them. Can anyone help?","['systems-of-equations', 'ordinary-differential-equations']"
4151735,About the remark of the proof that every operator on non-zero finite dimensional complex vector space has an eigenvalue in Linear Algebra Done Right,"In the 2nd Edition of Linear algebra Done Right, theorem 5.10 (Every operator on a finite-dimensional, nonzero, complex vector space has an eigenvalue), the author writes as a side note: Compare the simple proof of this theorem given here with the standard
proof using determinants. With the standard proof, first the difficult
concept of determinants must be defined, then an operator
with 0 determinant must be shown to be not invertible, then
the characteristic polynomial needs to be defined, and by the time the
proof of this theorem is reached, no insight remains about why it is
true. However, the approach given by the author does not seem to give me any more insight for why this theorem is true than the standard proof using determinants. Both proofs seem to just find some trick to transform this into a problem about roots of polynomials, in order to apply FTA. Am i missing some intuition about this Here's the proof for reference:","['soft-question', 'linear-algebra']"
4151761,"How can a set be both countable and ""dense""?","A countable set is one that can be put on a one-to-one correspondence with the natural numbers. This means that you can explicitly write down a table with the ordered Naturals in one column, and each member of the countable set in the other column. You literally ""count"" them. On the other hand, an ordered set can be such that, for any two members a and b, you can find another member c that is between them. I'm not sure what the name for this property is, so I call it ""dense"" . (If there is another term for this please clarify). At first glance, it seems to me that both those properties are incompatible. Edit: Why I think so: If I try to enumerate them, I would always fail at the second step: $0; 1; .. $ no wait. I forgot $0.1$ . So I start over: $0; 0.1; .. $ no wait, I forgot $0,01$ . So I start over: $0; 0.001; .. $ no wait... etc. This won't happen with the naturals: $0; 1; 2; 3; .. $ , since they are not ""dense"". Yet, the set of the Rationals satisfies both. It is clear that it is ""dense"". And this is a way to count the Rationals. I have read it and I understand it. But I still can't wrap my mind about how can a set be countable and ""dense"" at the same time. To me it seems contradictory. Could someone explain it to me in layman terms? Edit 2: As the comments have shown me, my confusion was that, I stated that it was contradictory to have a set that was dense and also had ""an always-increasing bijection with the naturals"". But I confused that last property, with ""Countable"", and that was wrong. I could ask a replacement question, if that's possible: Is it correct to state that a set that is ""dense"", cannot have ""an always-increasing bijection with the naturals""?","['elementary-set-theory', 'rational-numbers']"
4151770,Proving that the set of real numbers with digit 5 appearing infinitely often is Borel by using a Borel measurable function,"I am trying to prove the following statement: Show that the set of real numbers that have a decimal expansion with the digit 5 appearing infinitely often is a Borel set. $\textbf{using a Borel measurable function}$ (similar questions have been asked before but none (as far as I have been able to see) have tried to prove it in this way. The only thing that I have been able to come up with after several tries is to use the indicator function $\chi_{E}:\mathbb{R}\to\mathbb{R}$ , $\chi_{E}(x):=\begin{cases} 1 & \text{if }x\in E\\ 0 &\text{if }x\notin E\end{cases}$ where $E:=\{x\in \mathbb{R}: x \text{ has a decimal expansion with the digit 5 appearing infinitely often} \}$ . Then if $B$ is a Borel set we have that $\chi_E^{-1}(B)=\begin{cases}E &\text{ if }0\notin B\text{ and }1\in B\\ \mathbb{R}\setminus E & \text{ if }0\in B\text{ and }1\notin B\\ \mathbb{R} & \text{ if }0\in B\text{ and }1\in B\\ \emptyset & \text{ if }0\notin B\text{ and }1\notin B\end{cases}$ so in particular if we set $\mathcal{S}:=\{\emptyset,E,\mathbb{R}\setminus E,\mathbb{R}\}$ this should be an $\mathcal{S}$ -measurable function and $\textbf{I was wondering if it is also a Borel measurable function}$ so I can say that $\chi_{E}^{-1}(\{1\})=E$ is Borel. So, I would be very grateful if someone either gave me an hint about how to do this or disproved my approach explaining to me why this isn't a feasible approach.","['borel-sets', 'measure-theory', 'measurable-functions']"
4151784,Alternative method for evaluating double infinite sum,"I've come across the following double infinite sum through my current work: $$\sum_{k=0}^\infty \sum_{m=0}^\infty \binom{m+k}{m} \frac{a^k b^m}{k! \; m!},$$ where a and b are real constants. I found a method to evaluate this sum using complex analysis, in particular Egorychev's method stating that $$\binom{m+k}{m} = \frac{1}{2\pi j} \oint_C \frac{(1+z)^{m+k}}{z^{m+1}} \mbox{d}z,$$ where the countour is a unit circle centered around the origin Substituting this and using the complex integral representation of the modified Bessel function gives the apparent result that $$\sum_{k=0}^\infty \sum_{m=0}^\infty \binom{m+k}{m} \frac{a^k b^m}{k! \; m!} = \exp(a + b) I_0(2\sqrt{ab}).$$ Is there another way to evaluate this sum to a) check this result and b) to extend the result to higher dimensions e.g. $$\sum_{k=0}^\infty \sum_{m=0}^\infty \sum_{n=0}^\infty  (k+m+n)!\frac{a^k b^m c^n}{k!^2 \; m!^2 \; n!^2}$$ for real a, b and c? Egorychev's method becomes significantly more complicated in 3 or more dimensions! If it's possible to reduce the dimensionality of the sum, that would still be useful. As always, any help greatly appreciated!","['complex-analysis', 'sequences-and-series']"
4151803,Find the exact value of $\tan⁡(\cos^{-1} (-\sqrt{3}/2))$.,"The question: Find the exact value of $\tan⁡(\cos^{-1} (-\sqrt{3}/2))$ . The link is the image of the method I used. However it isn't the right answer, how come this method doesn't work?",['trigonometry']
4151832,"Gaussian processes with covariance $K(s,t)=G(s)G(t)$, what properties do they satisfy?","Let $(X_t)$ be a Gaussian process with covariance $K(s,t)=G(s)G(t)$ for a given $G$ . What does that mean for the process $(X_t)$ ? For example if the covariance has the form $K(s,t)=G(\min(s,t))$ where $G$ is non decreasing and positive means that $(X_t)$ has independent increments. Do you have an example of such an $(X_t)$ ? Maybe based on Brownian motion ? What properties should it satisfy ? It feels like the covariance is so rigid that $(X_t)$ should be very specific. EDIT: $X_t=G(t)X$ where $X \sim \mathcal N(0,1)$ would yield such a process. I am wondering if this is the only way to get such a covariance. This would confirm the ""triviality"" of those processes.","['real-analysis', 'stochastic-processes', 'functional-analysis', 'probability-theory', 'probability']"
4151874,"Compute the double sum $\sum_{n, m>0, n \neq m} \frac{1}{n\left(m^{2}-n^{2}\right)}=\frac{3}{4} \zeta(3)$","I am trying to compute the following double sum $$\boxed{\sum_{n, m>0, n \neq m} \frac{1}{n\left(m^{2}-n^{2}\right)}=\frac{3}{4} \zeta(3)}$$ I proceeded as following $$\sum_{n=1}^{\infty}\sum_{m=1}^{\infty}\frac{1}{n}\frac{1}{\left(m^2-n^2 \right)}=\sum_{n=1}^{\infty}\frac{1}{n}\sum_{m=1}^{\infty}\frac{1}{\left(m+n \right)\left(m-n \right)}$$ $$=\sum_{n=1}^{\infty}\frac{1}{n}\sum_{m=1}^{\infty}\frac{1}{2m}\left[\frac{1}{\left(m+n \right)}+\frac{1}{\left(m-n \right)}\right]$$ $$=\frac{1}{2}\underbrace{\sum_{n=1}^{\infty}\frac{1}{n}\sum_{m=1}^{\infty}\frac{1}{m}\frac{1}{\left(m+n \right)}}_{2\zeta(3)}+   \frac{1}{2}\sum_{n=1}^{\infty}\frac{1}{n}  \sum_{m=1}^{\infty}\frac{1}{m}\frac{1}{\left(m-n \right)}$$ The first sum is given here and I have also evaluated here , therefore we get that $$\sum_{n=1}^{\infty}\sum_{m=1}^{\infty}\frac{1}{n}\frac{1}{\left(m^2-n^2 \right)}=\zeta(3)-\frac{1}{2}\sum_{n=1}^{\infty}\frac{1}{n^2}  \sum_{m=1}^{\infty}\frac{1}{m}-\frac{1}{\left(m-n \right)}$$ $$=\zeta(3)-\frac{1}{2}\sum_{n=1}^{\infty}\frac{1}{n^2}  \int_{0}^{1}\frac{1-x^{-n}}{1-x}dx$$ $$=\zeta(3)-\frac{1}{2}  \int_{0}^{1}\frac{1}{1-x}\left[ \sum_{n=1}^{\infty}\frac{1}{n^2}-\sum_{n=1}^{\infty}\frac{x^{-n}}{n^2}\right]dx$$ $$=\zeta(3)-\frac{1}{2}  \int_{0}^{1}\frac{\zeta(2)-Li_{2}\left( \frac{1}{x} \right)}{1-x}dx$$ From this post we can use the relation $$Li_{2}\left( \frac{1}{x} \right)-Li_{2}\left( x \right)=\zeta(2)-\frac{1}{2}\log^2(-x)$$ To get $$\sum_{n=1}^{\infty}\sum_{m=1}^{\infty}\frac{1}{n}\frac{1}{\left(m^2-n^2 \right)}=\zeta(3)-\frac{\zeta(2)}{2}  \int_{0}^{1}\frac{1}{1-x}dx+\frac{1}{2}  \int_{0}^{1}\frac{Li_{2}\left( x \right)+\zeta(2)-\frac{1}{2}\log^2(-x)}{1-x}dx$$ $$=\zeta(3)+\frac{\zeta(2)}{2}  \int_{0}^{1}\frac{1}{1-x}dx+\frac{1}{2}  \int_{0}^{1}\frac{Li_{2}\left( x \right)}{1-x}dx-\frac{1}{4}\int_{0}^{1}\frac{\log^2(-x)}{1-x}dx$$ $$=\zeta(3)+\frac{\zeta(2)}{2}  \log(1-x)\Big|_{0}^{1}+\frac{1}{2}\left\{ -Li_{2}(x)\log(1-x)\Big|_{0}^{1}-\int_{0}^{1}\frac{\log^2(1-x)}{x}dx\right\}  -\frac{1}{4}\int_{0}^{1}\frac{\log^2(-x)}{1-x}dx$$ $$=\zeta(3)-\frac{1}{2}2\zeta(3)  -\frac{1}{4}\int_{0}^{1}\frac{\log^2(-x)}{1-x}dx$$ $$\sum_{n=1}^{\infty}\sum_{m=1}^{\infty}\frac{1}{n}\frac{1}{\left(m^2-n^2 \right)}=-\frac{1}{4}\int_{0}^{1}\frac{\log^2(-x)}{1-x}dx$$ My questions are the following: (1)Is the last equality correct? (2)If so, what is the meaning of $\log^2(-x)=\log^2(e^{i\pi}x)$ ? (3) How do we solve the last integral?","['integration', 'digamma-function', 'harmonic-numbers', 'polylogarithm', 'sequences-and-series']"
4151904,About a family of functions that are linearly independent,"For an integer $k$ , define a function $f_k :  \Bbb R\to \Bbb R$ to take any value on $(k,k+1)$ (not all zero) and $0$ on $\Bbb R\setminus(k,k+1).$ Let $\mathcal F=\{f_k : k\in\mathbb Z\}$ be a family of these functions. Note that the functions in $\mathcal F$ are linearly independent. Now, let $D=\{A_i : i\in \mathbb Z\}$ be collection of subsets of $\mathbb Z$ such that each $A_ i$ is infinite and $|A_i\cap A_j|<\infty$ . Let $$B={\left\{\sum_{k\in A} f_k: A\in D \right\}}.$$ Question: Is B linearly independent? To see that $B$ is linearly independent, by using the definition, we need to show if $$c_1\sum_{k\in A_1} f_k+c_2\sum_{k\in A_2} f_k+\cdots+c_n\sum_{k\in A_n} f_k=0,\tag {1}$$ then $c_1=c_2=\cdots=c_n=0.$ As in the assumption about elements in $D,$ we may have like $A_1\cap A_2=\{1,2,\dots, m\}.$ So, the first two terms can be written like that $$(c_1+c_2)(f_1+f_2+\cdots+f_m)+ c_1\sum_{k\in A_1\setminus(A_1\cap A_2)} f_k+c_2\sum_{k\in A_2\setminus(A_1\cap A_2)} f_k.$$ Here is where I got stuck. Should I consider cases for each $A_i$ ? I might be making it more difficult than it could be. Any help would be greatly appreciated.","['functions', 'linear-algebra', 'vector-spaces']"
4151916,Are the sets of maximal ideals containing particular elements of a Boolean algebra closed under infinite unions?,"I was perusing a problem for a project I am doing, and the set up is kind of a mouthful. But basically we have a boolean algebra $B$ ; for each $x$ in $B$ , let $M_x$ be the set of maximal ideals of $B$ that don't contain $x$ . And let $M$ be the set of all maximal ideals of $B$ . Now the book just remarks that $\{M_x \mid x\in B\}$ can be all the open sets of a topology on M. However, I don't quite see how it's closed under infinite union, since, well, $B$ isn't necessarily complete, right? Any pointers would be appreciated, as I am a little bit stumped. (FYI, the screenshot is from Lattices and Ordered Algebraic Structures by TS Blyth.)","['boolean-algebra', 'general-topology']"
4151936,Modified Bessel function near zero,"Let $K_0$ be the modified Bessel function of the second kind, that is for instance $$ K_0(x)=\int_0^{+\infty}e^{-x\cosh(t)}dt $$ How can we prove that $K_0(x)\underset{x\rightarrow 0^+}{\sim}-\log x$ ? I tried to modify the integral by doing the change of variable $u=x\cosh(t)$ but it didn't help me, and Laplace method only gives the equivalent when $x\rightarrow +\infty$ .","['integration', 'asymptotics', 'bessel-functions']"
4151951,"If a Graph have Eulerian Cycle and Hamiltonian Path, does it mean that the Graph have Hamiltonian Cycle?","Let $G$ be a Graph that have Eulerian Cycle and Hamiltonian Path , does it mean that $G$ must have Hamiltonian Cycle ? I tried to find a counter example but I always got stuck since when I notice an Eulerian Cycle I always find a Hamiltonian Cycle. I know that if $G$ have Eulerian Cycle then all the degrees are even, and we visited all the edges exactly one but it doesn't necessary means that we have visited all the vertices exactly once. I will be happy if someone can help by getting a counter example or a proof if its true.","['graph-theory', 'discrete-mathematics', 'hamiltonian-path']"
4151972,Between which two consecutive integer numbers is $\sqrt{2}+\sqrt{3}$? [duplicate],"This question already has answers here : How prove $ \sqrt{2}+\sqrt{3}>\pi$? (4 answers) Closed 3 years ago . Between which two consecutive integer numbers is $\sqrt{2}+\sqrt{3}$ ? My thoughts: $\sqrt{2}$ is $\approx1,4$ and $\sqrt{3}$ is $\approx{1,7}$ so their sum must be of the interval $(3;4)$ . Any more strict approaches? Thank you in advance!",['algebra-precalculus']
4151986,a.s. convergence and Glivenko-Cantelli theorem,"Assume that we have a sample $X_1, X_2, \dots, X_n$ is a sample from, for example, Poisson distribution with probability mass function $p_{i}$ and cdf $\mathbb{F}$ . Then, for $i\in\mathbb{N}$ let $$
\bar{p}_{n,i} = \frac{\sum_{j=1}^{n} 1 \{ X_j = i \}}{n}
$$ is empirical estimator. The Glivenko-Cantelli theorem says: $$
\|\bar{\mathbb{F}}_n - F\|_\infty \overset{\text{a.s.}}{\to} 0
$$ where $\bar{\mathbb{F}}_n$ is empirical cdf. The queston: assume that $\tilde{p}_n$ is another strongly consistent estimator of $p$ , i.e. $\tilde{p}_{n}$ converges a.s. to $p$ in $l_{2}$ norm. Note, in $l_{2}$ , not point-wise. Is then $$
\|\tilde{\mathbb{F}}_n - F\|_\infty \overset{\text{a.s.}}{\to} 0
$$ where $\tilde{\mathbb{F}}_n$ is cdf for the estimator $\tilde{p}_n$ ?","['statistics', 'measure-theory', 'functional-analysis', 'convergence-divergence', 'probability-theory']"
4151994,"New to Bayesian statistics, getting confused with the definitions and sub-understanding of how it works","I'm sorry if these seem like rudimentary questions, but I'm self taught trying to grasp an understanding that seems a but beyond me right now. Here's my situation. A meteorologist is interested in predicting temperature extremes for a location in the southern United States. From daily temperature records for this location she extracts the annual maximum temperature over a 10 year period. The mean of the observations is $98.2~.$ She assumes that each observation is independently distributed as $N(\mu, 3^2).$ A historical study suggests that $\mu \sim N(90, 102).$ This was the scenario I was given, I was then asked to derive the $95\%$ posterior Highest Density Interval for $\mu$ with posterior $N(98.1,0.94^2)$ which I managed to do. What I am confused about is this :
The meteorologist says: “There is a probability of $0.95$ that $\mu$ lies between $a$ and $b.$ ”
Is her claim correct? I assume not because of the differences of historical data which may skew the results. I also have no clue how to describe why a Bayesian analysis mught be preferable to a frequentist analysis of these annual temperature data.","['statistics', 'probability']"
4152080,Operating modulus on inverse trigonometric inequality,"So, I was going through this example of an inverse trigonometric inequality in my Math textbook and I've come up to this step: $$-2\pi < 3\tan^{-1}x-\pi /2< \pi $$ Now in the next step they have operated modulus on the inverse trigonometric function like this: $$ 0 \leq  |3\tan^{-1}x-\pi /2|< 2\pi $$ I am not clear as to how the LHS and RHS inequality, i.e. 0 and 2, respectively, have arrived by  operating modulus on the inverse trigonometric function
as they seem to have skipped that step in the example. Could someone please help me out?","['inequality', 'functions', 'inverse-function']"
4152108,Gluing two functions from Sobolev spaces.,"I am studying Galdi's Introduction to the mathematical theory of Navier-Stokes equations and somewhere in a proof, he ""glued"" two functions in $W^{1, 2}(\Omega_1)$ and $W^{1, 2}(\Omega_2)$ to obtain a function in $W^{1, 2}(\Omega_1 \cup \Omega_2)$ and I don't really understand why it is true. Let me explain it in more details. Let $B_1$ and $B_2$ be the open balls in $\mathbb R^n$ of radius $1$ and $2$ respectively. Let $\Omega_1 = B_1$ and $\Omega_2 = B_2 \cap (\overline{B_1}^c)$ . We consider $\boldsymbol{u}_1 \in W^{1, 2}(\Omega_1)$ and $\boldsymbol{u}_2 \in W^{1, 2}(\Omega_2)$ (where $\boldsymbol{u}= (u^1, \ldots, u^m)$ and $\boldsymbol{u}_i \in W^{1, 2}(\Omega_i)$ actually means $\boldsymbol{u}_i \in [W^{1, 2}(\Omega_i)]^m$ ) verifying $$\nabla \cdot \boldsymbol{u}_i = 0 ~~\text{in }\Omega_i \quad \text{and} \quad \boldsymbol{u}_1 = \boldsymbol{u}_2 ~~~\text{at }\partial \Omega_1,$$ in the trace sens. My question is, if we consider $$\boldsymbol{u}: x \in \overline{\Omega_1} \cup \Omega_2 = \Omega \mapsto 
\begin{cases}
\boldsymbol{u}_1(x) & \text{if } x \in \Omega_1\\
\boldsymbol{u}_2(x) & \text{if } x \in \Omega_2\\ 
\end{cases},
$$ is that true that $\boldsymbol{u} \in W^{1, 2}(\overline{\Omega_1} \cup \Omega_2)$ and that $\nabla \cdot \boldsymbol{u} = 0$ in $\overline{\Omega_1} \cup \Omega_2 $ ? I don't really see why this should be true because if we take $\boldsymbol{\psi} \in C^\infty_0(\overline{\Omega_1} \cup \Omega_2)$ , we should be able to show that $$\int_{\overline{\Omega_1} \cup \Omega_2} \boldsymbol{u} \cdot \partial_i\boldsymbol{\psi} = - \int_{\overline{\Omega_1} \cup \Omega_2} \partial_i\boldsymbol{u} \cdot \boldsymbol{\psi}$$ but $$\int_{\overline{\Omega_1} \cup \Omega_2} \boldsymbol{u} \cdot \partial_i\boldsymbol{\psi} = \int_{\Omega_1} \boldsymbol{u} \cdot \partial_i\boldsymbol{\psi}  + \int_{\Omega_2} \boldsymbol{u} \cdot \partial_i\boldsymbol{\psi} \stackrel{?}{=}-\int_{\Omega_1} \partial_i\boldsymbol{u} \cdot \boldsymbol{\psi}  - \int_{\Omega_2} \partial_i\boldsymbol{u} \cdot \boldsymbol{\psi} = -\int_{\overline{\Omega_1} \cup \Omega_2} \partial_i\boldsymbol{u} \cdot \boldsymbol{\psi} $$ where I don't see why the second equality could be satisfied as $\boldsymbol{\psi}|_{\Omega_i} \notin C^\infty_0(\Omega_i)$ . Could someone help me with this ?","['sobolev-spaces', 'functional-analysis', 'weak-derivatives']"
4152124,"Pseudo-Tree , Proving $ 1 \leq \frac{|E|}{|V| - 1} \leq \frac{3}{2} $","Problem: Graph $ G = \langle V,E \rangle $ is called Pseudo-Tree if it is connected and also every edge of $ G $ is on one simple-circuit at-most. Prove that $ 1 \leq \frac{|E|}{|V| - 1} \leq \frac{3}{2} $ So I didn't know what to do at all. Is the best strategy to approach this problem would be induction on the number of vertices $ |V| = n $ ? ( Strong induction maybe? ), also what happens if I have only 1 vertex? ( In that case $ |V| - 1 = 0 $ the inequlity doesn't occur. But I don't fully understand why a such a case is not possible where $ |V|=1 $ ? ). I'd appreciate any guidance on this problem and thanks in advance for help!","['graph-theory', 'discrete-mathematics']"
4152244,Clarifying the definition of the directional derivative of a curve,"I am reading Kuhnel's Differential Geometry - Curves, Surfaces, Manifolds and I feel like I am missing something about the definition of directional derivative (4.1). Kuhnel defines it as: Let $Y$ be a differentiable vector field defined on an open set of $\mathbb{R}^{n+1}$ , and let $(p, X) \in T_p\mathbb{R}^{n+1}$ . Then the directional derivative of $Y$ in the direction $X$ at $p$ is (the vector) $D_XY|_p := DY|_p(X)$ , where $DY|_p$ is the Jacobian matrix of $Y$ at $p$ . A few pages later, Kuhnel begins talking about geodesics and mentions the following: ""If $c(t)$ is the motion of the mass of a particle, then $D_{\dot{c}}\dot{c} = \ddot{c}$ is just the acceleration vector in Euclidean space."" But $\dot{c}$ is a curve, not a vector field, so how is the expression $D_{\dot{c}}\dot{c}$ even defined? Even if we consider $\dot{c}$ as a map $c(t) \mapsto \dot{c}(t)$ (i.e. vectors based at points on the curve), it is not defined on an open set of $\mathbb{R}^{n+1}$ , so I don't really know how to calculate $D_{\dot{c}}\dot{c}$ .","['multivariable-calculus', 'definition', 'differential-geometry']"
4152250,$\int_C \frac{f(z)}{(z-1)^{2020}}dz$,"Let $f(z)=u(x,y)+iv(x,y)$ be an entire function such that $au+bv\ge \ln(ab), a>1,b>1.$ Then evaluate $$\int_C \frac{f(z)}{(z-1)^{2020}}dz,$$ where $C$ is an equilateral triangle of side $1$ with centroid at $z=1.$ It seems that I can use the Cauchy's Integral formula here and by doing so the integral would be $$\frac{2\pi i}{2019!}f^{(2019)}(1)$$ I have no idea how to connect the first part of the question in solving this problem. Help please",['complex-analysis']
4152294,Do there exists infinitely many primes that satisfy $p_a-p_b=k$,"I have read that Terence Tao proved that there exists infinitely many primes that satisfy $p_n-p_{n-1}\le246$ ( $p_n$ denotes the $n^{th}$ prime) I want to know whether it has been proven that there exists infinitely many primes that satisfy $p_a-p_{b}=k$ , where $k$ is an integer ( $k$ can be greater than 246)? I am specifically asking for equality (in the case of $p_n-p_{n-1}\le246$ there is only an inequality ). I am asking for a specific value of $k$ . Edit-1 The two primes need not be consecutive. Edit-2 Suppose $p_a-p_{b}=k$ is true only for finite number of primes. This would mean that there does not exists infinitely many primes that satisfy $p_n-p_{n-1}\le246$ (because it is not true for $k=1,2,....246$ ) leading to a contradiction. This means that there exists infinitely many primes that satisfy $p_n-p_{n-1}= k~$ for some $k\le246$ even though we don't know the exact value of $k$ for which it is true . Is this reasoning correct?","['number-theory', 'prime-gaps', 'elementary-number-theory', 'prime-numbers']"
4152307,Shift a rectangle located on circle circumference so they do not overlap,"Suppose I have a rectangle with its center point located on the circumference of a disk or circle. How can I move the rectangle along the radius just enough so that it does not overlap the disk anymore? That is, how to calculate the new center point of the rectangle or ( x′ , y′ )? I have these parameters available: r : radius of the circle (maybe irrelevant) θ : angle of radius w , h : width and height of the rectangle ( x , y ): coordinates of the current center point of the rectangle (starting yellow dot) I tried the following but it does not result in what I want: $$x' = x + \frac{w}{2} * \cos \theta$$ $$y' = y + \frac{h}{2} * \sin \theta$$ It should work for any angle. Another example:","['trigonometry', 'circles']"
4152309,Is the Medieval proof of the divergence of the Harmonic series valid?,"As far as I understand it, the Medieval argument for the divergence of the Harmonic series: $S=1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\frac{1}{5}+\frac{1}{6}+\frac{1}{7}+\frac{1}{8}+\ldots$ relies on replacing each term with the largest power of $2$ less than or equal to it, term by term, giving: $S'=1+\frac{1}{2}+\frac{1}{4}+\frac{1}{4}+\frac{1}{8}+\frac{1}{8}+\frac{1}{8}+\frac{1}{8}+\ldots$ and concluding, that since the $i^{\text{th}}$ element of $S'\leq$ the $i^{\text{th}}$ element of $S$ for all $i$ , $S'\leq S$ . But $S'=1+\frac{1}{2}+\frac{1}{2}+\frac{1}{2}+\ldots$ , which diverges, and therefore so does $S$ . My question is, to what extent is it legitimate to argue like this, replacing each term in an infinite series with a smaller term and drawing a valid conclusion from that? For instance if I were to argue that I had the series: $T=2+3+4+5+\ldots$ And I then subtracted, term by term, $1$ from each element of $T$ , giving: $T'=1+2+3+4+\ldots$ Then the $i^{\text{th}}$ element of $T'\leq$ the $i^{\text{th}}$ element of $T$ , as before, suggesting that $T'\leq T$ , according to this argument. Except in this case you can see that $T'=T+1>T$ . So arguing like this seems to lead to an absurdity. So my question is, to what extent is element-by-element subtraction like in the Medieval Harmonic series argument legitimate? What is the difference between the Harmonic series argument and my (absurd) $T$ argument?","['math-history', 'sequences-and-series']"
4152319,How to find the standard matrix of a shear / reflection transformation from $\mathbb R^2$ to $\mathbb R^2$,"Let $T : \Bbb{R}^2 \rightarrow \Bbb{R}^2$ be the transformation that first performs a horizontal shear so that $e_2 \rightarrow e_2 + 2e_1$ (leaving $e_1$ unchanged) and then reflects the points through the line $y = x$ (a) Find the standard matrix $A$ for $T$ (b) Find the standard matrix for the inverse mapping directly by finding a transformation that first undoes the reflection through the line y=x and then undoes the horizontal shear. Show that this matrix is the same as the matrix you found for $A^{-1}$ I'm on (a) and have $\underbrace{
        \begin{bmatrix}
            x_1 & y_1\\
            x_2 & y_2\\
        \end{bmatrix}
        }_{M}$ $\xrightarrow{(2)e_1 + e_2 \rightarrow e_2}
    \begin{bmatrix}
        x_1 & y_1\\
        2x_1+x_2 & 2y_1+y_2\\
    \end{bmatrix}$ $\begin{bmatrix}
        0 & 1\\
        1 & 0\\
    \end{bmatrix}
    \begin{bmatrix}
        x_1 & y_1\\
        2x_1+x_2 & 2y_1+y_2\\
    \end{bmatrix}
    =
    \begin{bmatrix}
        2x_1+x_2 & 2y_1+y_2\\
        x_1 & y_1\\
    \end{bmatrix}
    = T$ (a) $A = T \cdot M^{-1}$ $A=\begin{pmatrix}2x_{1+x_2}&2y_1+y_2\\ \:\:\:\:x_1&y_1\end{pmatrix}\begin{pmatrix}\frac{y_2}{x_1y_2-y_1x_2}&-\frac{y_1}{x_1y_2-y_1x_2}\\ \:\:\:\:-\frac{x_2}{x_1y_2-y_1x_2}&\frac{x_1}{x_1y_2-y_1x_2}\end{pmatrix}$ $= \begin{pmatrix}\frac{2x_{1+x_2}y_2-x_2\left(2y_1+y_2\right)}{y_2x_1-y_1x_2}&\frac{-2x_{1+x_2}y_1+x_1\left(2y_1+y_2\right)}{y_2x_1-y_1x_2}\\ 1&0\end{pmatrix}$ However the result for $A$ seems wonky. Am I on the right track?","['matrices', 'inverse', 'linear-transformations', 'discrete-mathematics']"
4152323,Binomial Transform of a Particular Binomial Sum,"Let $$
a_n = \sum_{k=0}^n \binom{n + 1}{k}b_k.
$$ I am trying to write $b_n$ in terms of $a_k$ . Of course, if the binomial coefficient was $\binom{n}{k}$ instead of $\binom{n + 1}{k}$ , then this is simple to do using the binomial transform, $$
c_n = \sum_{k=0}^n\binom{n}{k}d_k \iff d_n = \sum_{k=0}^n\binom{n}{k}(-1)^{n-k}c_k.
$$ But in my case I'm at a loss. I tried using Pascal's identity, thinking I might be able to use the formula for the binomial transform if I could get $a_n$ expressed in terms of $b_k$ in the right way. This got me to $$
a_n = \sum_{k=0}^n \binom{n}{k}b_k + \sum_{k=0}^{n-1}\binom{n}{k}b_{k + 1},
$$ which is almost right, but not quite. Is there a simple way to do this?","['binomial-coefficients', 'combinatorics']"
4152338,Solve the inequality $\frac{3-x}{x^2-2x-3}\le\frac{3-x}{x^2+2x-3}$,"Solve the inequality $$\dfrac{3-x}{x^2-2x-3}\le\dfrac{3-x}{x^2+2x-3}$$ We have $D: \begin{cases}x^2-2x-3\ne0\Rightarrow x\ne -1;3 \\ x^2+2x-3\ne0\Rightarrow x\ne-3;1\end{cases}$ Is the given equality equivalent (in $D$ ) to $$x^2-2x-3\ge x^2+2x-3\\\iff x\le0$$ So the solutions are $x\le0\cap D$ ? It seems like the inequalities I wrote aren't equivalent, because I don't get the answer.",['algebra-precalculus']
4152366,Let $n$ lines be drawn in the plane so that each of them intersects the others but any three lines do not coincide at any point.,"a) Let $n$ lines be drawn in the plane so that each of them intersects the others but any three lines do not coincide at any point. For $n\geq 0$ , let $a_n$ be the number of regions into which the plane is separated by these $n$ lines. Find and solve a recurrence relation for $a_n$ . b) For the situation in part a), let $b_n$ be the number of infinite regions obtained that way. Find and solve a recurrence relation for $b_n$ . a) If we look at what happens when we add the nth line, it will intersect with all the other $n-1$ lines, so it will pass through $n$ regions, thus dividing them in two. This means that the number of regions will increase by $n$ . Therefore, our recurrence relation is: \begin{align*}
a_n-a_{n-1}=n,\ n>0,\ a_0=1.
\end{align*} We solve it this way in a simple way: \begin{align*}
a_n=a_{n-1}+n=a_{n-2}+(n-1)=a_0+1+2+a_{n-2}+(n-1)=a_0+1+2+a_0+1+2+\dots +n.
\end{align*} Therefore, we have: \begin{align*}
a_n=\frac{n^2+n+2}{2},n\geq 0.
\end{align*} b) Let $b_n =$ be the number of infinite regions resulting in $n$ such lines. When the nth line is drawn it is divided into $n$ segments. The first and the nth segment each create a new infinite region. Therefore, \begin{align*}
b_n=b_{n-1}+2,\ n>1,\ b_1=2
\end{align*} I don't know if this is correct for a), since for example By adding the red line, it fulfills the given conditions, but does not divide all regions in two.","['problem-solving', 'discrete-mathematics']"
4152471,Minimize sum of maximum dot product between unit vectors,"Let $\mathbb{S}_{d-1}$ denote the unit $(d-1)$ -sphere . Let \begin{align}
    f(d, n) = \operatorname*{argmin}_{x \in (\mathbb{S}_{d-1})^n} \sum_{i \in n} \max_{j \in n - \{i\}} x_i \cdot x_j
\end{align} Let $g(d, n)$ be the corresponding minimum value. What is known about $f$ and $g$ ? Do they have a name in the literature? Are the following solutions correct? Are other solution classes known? $d$ $n$ $f(d,n)$ $1$ $n$ $\lfloor n/2 \rfloor$ on one point and $\lceil n/2 \rceil$ on the other $2$ $n$ vertices of a regular $n$ -gon $d$ $n \leq d+1$ vertices of a regular $(n-1)$ - simplex $3$ $5$ vertices of a triangular bipyramid $3$ $6$ vertices of a regular octahedron $3$ $8$ vertices of a uniform square antiprism $3$ $12$ vertices of a regular icosahedron See the Thomson problem and spherical codes for related problems. These questions might have relevant information: Minimizing the maximum dot product among k unit vectors in an n-dimensional space Is the Fibonacci lattice the very best way to evenly distribute N points on a sphere? Evenly distributing n points on a sphere","['spheres', 'inner-products', 'geometry', 'optimization', 'constraints']"
4152579,What mathematical structure best describe a Microsoft Excel spreadsheet?,"Here is a basic question: how would you model an excel spreadsheet in terms of linear algebra, set theory, matrices, vector spaces? A excel spreadsheet consists of cells, whereby cells can contain (for the sake of simplicity) two types of  numerical value or symbols (such as alphabets/characters). For example, it could appear as, \begin{bmatrix} 1 & 2 &  3\\ 4 & 5 & 6 \\  7 & 8 & 9 \end{bmatrix} or \begin{bmatrix} 1 & 2 & \cdots \\ 4 & \ddots & \cdots \\ \vdots & \vdots & \ddots \end{bmatrix} or \begin{bmatrix} 1 & a & c \\ b & 2 & d \\ e & 3 & 4 \end{bmatrix} or \begin{bmatrix} ! & a & c \\ b & ? & d \\ e & $ & @ \end{bmatrix} or perhaps other types (imagine an empty spreadsheet or cells with functions $f(x)$ ). What mathematical structure would best correspond to each of these examples?","['matrices', 'math-software', 'elementary-set-theory', 'recreational-mathematics', 'soft-question']"
4152629,Help in solving the following Sturm-Liouville problem $-x^2y''-2xy'-\lambda y=0$,"I have problems finding the eigenvalues and eigenfunctions of the equation: $$-x^2y''-2xy'-\lambda y=0$$ The domain is $[1,\pi]$ , with conditions $y(1)=y(\pi)=0$ .
I have proved the values of $\lambda$ have to be nonnegative. By making the subsitution $y=x^m$ , I get to the condition: $$m^2+m+\lambda=0$$ Which leads to the solutions: $$y=ax^{-\frac{1+\sqrt{1-4\lambda}}{2}}+bx^{-\frac{1-\sqrt{1-4\lambda}}{2}}$$ I don't know if I'm going the right way about this or if I'm missing information about how to solve this type of problems.","['calculus', 'sturm-liouville', 'ordinary-differential-equations']"
4152681,"What does it mean when you use the ""d"" in a derivative as a variable? [duplicate]","This question already has answers here : Notation of the second derivative - Where does the $d$ go? [duplicate] (5 answers) Closed 3 years ago . It's been over 15 years since I last did anything with calculus at school, and I've forgotten most things, so please be gentle with your answers. :) One thing that has always bugged me when getting into more advanced stuff was that the derivative simbols are suddenly manipulated in a way that doesn't make sense to me. For example, let's take the simple equation $$y=12x^2+24x+45$$ An alternative notation for the same thing would be: $$f(x)=12x^2+24x+45$$ Here we merely substituted $y$ for $f(x)$ . It means the same thing. If memory serves me right, when we take the first derivative of this function, we have three ways we can write it: $$y'=24x+24$$ $$f'(x)=24x+24$$ $$\frac{dy}{dx}=24x+24$$ Now, it's this third notation that I'm talking about here. To my understanding the $\frac{dy}{dx}$ doesn't really mean anything by itself. It's just another way to specify that we're talking about a derivative of a function, just like the $'$ in the other two notations. But then sometimes I come across an equation like this : $$m\frac{\text{d}^2\mathbf{r}}{\text{d}t^2} = \frac{kqq'}{|\mathbf{r}|^2},$$ And here suddenly the $d$ is being treated like a variable, not to mention the $t$ and... And if memory serves me right, I've seen ever more extreme examples where the fraction $\frac{dy}{dx}$ itself is taken apart and each of the components used as a variable. I think this was popular back in university when I was talking course on differential equations. Barely passed that, half of it didn't make sense, and this was a large part of the reason why. I don't remember why I never asked my teacher about it back then. So... what does it mean when we start to break the $\frac{dy}{dx}$ apart?","['notation', 'calculus']"
4152700,"Is it true that if $x_n$ has at least 2 distinct terms, then $\liminf_{n\to\infty} (x_1 + \dots + x_n - n\sqrt[n]{x_1\dots x_n}) > 0$?","Is it true that for every positive sequence $x_n$ with at least 2 distinct terms, $\liminf_{n\to\infty} (x_1 + \dots + x_n - n\sqrt[n]{x_1\dots x_n}) > 0$ ? The equality condition of the AM-GM inequality gives $x_1 + \dots + x_n - n\sqrt[n]{x_1\dots x_n} > 0$ for all $n$ large enough but this is not enough to decide. I tried to find a counterexample to no avail either; I investigated $x_n = \frac{1}{n}$ and $x_n = 1 + \frac{1}{2^n}$ but I could not obtain a contradiction. If the statement is true or false can we explain intuitively/heuristically why we expect it to be such? Any comments and help are welcome.","['limits', 'inequality', 'limsup-and-liminf']"
4152702,"Is there a name for ""co-normal"" subgroups?",I have in mind a situation where a group $G$ has two subgroups $H$ and $K$ with the property that each of them is closed under conjugation by elements of the other. More formally: for $h \in H$ and $k \in K$ then $h^{-1} k h \in K$ and $k^{-1}h k \in H$ . Is there a name for this?,"['group-theory', 'normal-subgroups']"
4152717,Find the value of $\int_{-\pi}^{\pi}\left(4\arctan\left(e^{x}\right)-\pi\right)dx$,"I need to find the value of $$\int_{-\pi}^{\pi}\left(4\arctan\left(e^{x}\right)-\pi\right)\mathrm dx$$ I've tried to show this is an odd function in order to show the answer is $0$ , but I wasn't able to do that. How can I prove this is an odd function?","['integration', 'calculus', 'definite-integrals', 'trigonometry']"
4152723,Stuck on using WKB to solve $\epsilon\frac{d^2u}{dx^2}-x(1+x)\frac{du}{dx}+xu=0$ as $\epsilon\to 0$,"I want to use the WKB method to solve the uniform leading order approximation for the ODE $$\epsilon\frac{d^2u}{dx^2}-x(1+x)\frac{du}{dx}+xu=0,~u(-0.5)=\alpha,~u(0.5)=\beta$$ My attempt: Let $u(x)\approx\exp\left(\frac{1}{\epsilon}\sum_{n=0}^{\infty}\epsilon^nS_n(x)\right)$ , then we can compute $u'(x)$ and $u''(x)$ . And finally I have $$\epsilon\left[\frac{1}{\epsilon ^2}\left(\sum_{n=0}^{\infty}\epsilon^nS_n'(x)\right)^2+\frac{1}{\epsilon}\sum_{n=0}^{\infty}\epsilon^nS_n''(x)\right]-x(1+x)\left(\frac{1}{\epsilon}\sum_{n=0}^{\infty}\epsilon^nS_n'(x)\right)+x=0$$ . However I'm not sure how to continue the proccess. How to deal with the $x$ terms? And how to finish computing these all messing stuffs? Need some help! PS:
I know the finally answer shall be $u(x)\approx2\alpha(x+1)+(\beta-3\alpha)\exp^{\frac{6x-3}{8\epsilon}}$","['asymptotics', 'ordinary-differential-equations']"
4152731,Limit of Powers of Brownian motion divided by time,"Let $p > 0$ and $W$ be a standard Brownian motion.  I want to investigate the following limits $$L(p) \equiv \lim_{t \rightarrow \infty} \frac{W_t^p}{t}$$ Obviously if $p = 1$ , then the SLLN for BM implies that $L(1) = 0$ . $L(0)$ is also trivially $0$ .  For $p \in (0,1)$ we see $$\frac{W_t^p}{t} = \left(\frac{W_t}{t^{1/p}}\right)^p = \left(\frac{W_t}{t}\right)^p \cdot \frac{1}{t^{1-p}} \rightarrow 0$$ so that $L(p) = 0 \quad \forall p \in [0,1]$ . Now if $p \in (1,2)$ , we see that $$\begin{aligned}
E \left(\sup_{2^n \leq t \leq 2^{n+1}} \frac{|W_t|^p}{t} \right) \leq E \left(\sup_{2^n \leq t \leq 2^{n+1}} \frac{|W_t|^2}{t^{2/p}} \right)^{p/2} \\
\leq 2^{-n} E \left(\sup_{2^n \leq t \leq 2^{n+1}} |W_t|^2\right)^{p/2} \\
\leq C 2^{-n} E \left( W_{2^{n+1}}^2\right)^{p/2} \propto 2^{-n(1-p/2)}
\end{aligned}$$ where the first inequality is Holder's, the second is trivial, the third is Doob's martingale inequality.  I have abused notation with the constant.  Now $2^{1-p/2} > 1$ so that $$\begin{aligned} \sum_{n =1}^\infty P\left(\sup_{2^n \leq t \leq 2^{n+1}} \frac{|W_t|^p}{t} > \epsilon \right) \leq C \sum_n \frac{1}{(2^{1-p/2})^n} < \infty\end{aligned}$$ and from Borel-Cantelli we see immediately that $L(p) = 0 \quad \forall p \in (1,2)$ . I don't know how to address the case where $p \ge 2$ .  Does anyone have any ideas?","['stochastic-processes', 'brownian-motion', 'probability-theory']"
4152804,Evaluate $\int_0^\infty \frac{dx}{(x+\sqrt{1+x^2})^2}$,"How to evaluate this definite integral from MIT Integration Bee 2006? $$\int_0^\infty \frac{dx}{(x+\sqrt{1+x^2})^2}.$$ So far, I have shown that the indefinite integral is $$\frac{2x^3 + 3x - 2(1+x^2)^{3/2}}{3}.$$ At $x = 0$ , the expression above equals $-\dfrac{2}{3}.$ Using WolframAlpha, I also know that the definite integral equals $\dfrac{2}{3}$ . So the only thing left to show is $$\lim_{x\rightarrow \infty} \frac{2x^3 + 3x - 2(1+x^2)^{3/2}}{3}=0.$$ I'm not sure how to calculate this limit.","['improper-integrals', 'limits', 'calculus', 'definite-integrals']"
4152812,Alternative proof of problem 16-21 of Lee's ISM: $\iota_X\mathsf{dVol}=\star(X^\flat)$,"This is the problem 16-21 of Lee's Introduction to Smooth manifolds: Problem 16-21: Let $(M,g)$ be an oriented Riemannian manifold. Show $\iota_X\mathsf{dVol}_g=\star(X^\flat)$ for a vector field $X$ . I know a proof of that as follows: suppose $v_1,v_2,\dots,v_n$ be an orthonormal basis at $p\in M$ then $$\iota_X\mathsf{dVol}_g(v_2,\dots,v_n)=\mathsf{dVol}_g(X,v_2,\dots,v_n)=g(X,v_1)$$ on the other hand and using $X=X^iv_i$ and $X^\flat=g_{ij}X^iE^j$ $$\star(X^\flat)(v_2,\dots,v_n)=\star(g_{ij}X^iE^j)(v_2,\dots,v_n)=g_{ij}X^i\star E^j(v_2,\dots,v_n)=\sum_jg_{ij}X^i \mathsf{dVol}_g(v_2,\dots,v_j,\dots,v_n)=g_{i1}X^i \mathsf{dVol}_g(v_1,\dots,v_n)=g(X,v_1). $$ I messed up a bit in the last two equalities. Anyway I want to know is there another proof and global (basis-free) proof? I think one can prove this just by using properties of interior multiplication and Hodge star. i.e. suppose $\eta$ be an arbitrary $n-1$ -form then $$\langle \iota_X\mathsf{dVol}_g,\eta\rangle = \langle \mathsf{dVol}_g,X^\flat\wedge\eta\rangle=??= \langle \star(X^\flat),\eta\rangle $$ How to fill the above gap?","['differential-forms', 'riemannian-geometry', 'differential-geometry']"
4152820,$\int f_kd\mu_k\to\int fd\mu$? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question If a measure $\mu_k$ converges weakly to a measure $\mu$ , i.e. $\int fd\mu_k\to \int fd\mu$ for every $f\in C_b(\mathbb{R})$ (contiuous and bounded functions), and let $f_k\to f$ a.e. pointwise. Then I was wondering what extra condition we need so that $\int f_kd\mu_k\to\int fd\mu$ . I have $$\int f_kd\mu_k=\underbrace{\int fd\mu_k}_{\to\int fd\mu}+\int (f_k-f)d\mu_k$$ but I am not sure when the second term goes to zero.","['measure-theory', 'probability-theory', 'weak-convergence']"
4152838,Fermat's Last Theorem: arbitrarily near-integer solutions,"Fermat's Last Theorem states that there exist no positive integers $x, y, z$ such that $x^n + y^n = z^n$ , for integer $n > 2$ . If we allow for $x, y, z \in \mathbb{R}_{>0}$ , it is obvious that there are infinitely many solutions (take arbitrary $x, y$ and choose $z = \sqrt[n]{x^n + y^n}$ ). However, do there always exist solutions with arbitrarily near-integer values? In other words, for each integer $n>2$ and real number $\varepsilon > 0$ , we want to find $x, y, z \in \mathbb{R_{>0}}$ so that: $|x-x'| < \varepsilon, |y-y'| < \varepsilon, |z-z'| < \varepsilon$ with $ x', y', z' \in \mathbb{Z}_{>0}$ $x^n + y^n = z^n$",['number-theory']
4152839,Finding a suitable universe for a given structure,"I am new to this forum, so I hope my mathematical way of writing here is correct, as I have not done it much before. :( I am taking my logics class currently and stumbled upon the following task. We are given a structure $\mathcal{G}$ from universal algebra entailing the domain (or universe ) $G$ := $\wp(\mathbb{N})$ (the power set of natural numbers), a signature $\omega:=\{\cup,\cap\ , ^c \}$ where $\{\cup,\cap\, ^c \}$ is the set of function symbols and $^c$ is merely the complement of $\{G'^c= \mathbb{N} \setminus G'$ | $G'\in\wp(\mathbb{N})\}$ - in accord with the definition . (We have a purely algebraic structure without relation symbols). Furthermore, we are given two subsets of the universe $\wp(\mathbb{N})$ (that are, again, universes in themselves) with (1) $\{G'' \subseteq \mathbb{N}$ | $G''$ finite}. (2) $\{G'' \subseteq \mathbb{N}$ | $G''$ , $G''^c$ infinite}. And, the way I understood it, we are supposed to prove if there exists a substructure of $\mathcal{G}$ over the universe (1). If not, we shall give the smallest substructure of $\mathcal{G}$ which contains (1) . The same for universe (2). So, what I do know is that a substructure $\mathcal{F}$ (with its universe $F$ ) has to fulfill the properties i. The domain of $\mathcal{F}$ is contained in the domain of $\mathcal{G}$ , i.e. $|\mathcal{F}| \subseteq |\mathcal{G}|$ . (or $F\subseteq G$ ) ii. $\mathcal{F}$ and $\mathcal{G}$ have the same signature $\omega(\mathcal{F}) = \omega(\mathcal{G}) $ . Now, I do understand that, trivially, (1) cannot be the universe of a substructure of $\mathcal{G}$ because its subsets are not closed under the complement, and (2) cannot be, either, since its subsets are not closed under union. However, I don't really know how I could find the smallest substructure of $\mathcal{G}$ that would contain (1), (2), or both. So I would really appreciate your help. Thanks so much,
Gianna","['universal-algebra', 'model-theory', 'logic', 'relations', 'functions']"
4152861,Inverse function of $x^{x^x}$,"How can I find the inverse function of $f(x)=x^{x^x}$ ?
Has this inverse function ever been defined / studied? are there any asymptotic expansions? It would be nice if the inverse of $f(x)$ could be expressed in terms of standard mathematical functions, but it would be enough for me to know even just a few properties. The inverse of $x^{x^x}$ can be linked to the inverse of $(x + a)^x$ and to the inverse of $x e^x +a x$ , and many other functions, so knowing its properties can have many applications in solving a wide range of equations Thanks edit: The series expansion of $x^{x^x} $ at $x=0$ can be expressed as: $$x^{x^x}=x \sum_{j=0}^{\infty} \frac{\left( \ln(x^x)\right)^j}{j!}B_j\left( \ln(x)\right)\tag{1}$$ $B_j(x)$ is the Bell Polynomial. By the General Leibniz rule we have that the nth derivative of $x^{x^x}$ can be shown as $$\sum_{k=1}^n \binom{n}{k}f^{(n-k)}(x)P_j^{(k)}(\ln(x))$$ $$f(x)=x^{j+1}\Rightarrow f^{(n-k)}(x)=x^{j+1-n+k} \frac{(j+1)!}{(j+1-n+k)!}$$ $$P_j(\ln(x))=S_{j}^{(1)}\ln(x)^{j+1}+S_{j}^{(2)}\ln(x)^{j+2}+\dots+S_{j}^{(j)}\ln(x)^{j+j}$$ $S_j^{(k)}$ is the Stirling number of the second kind $$P^{(n)}_j(\ln(x))=\sum_{k=1}^j \ \sum_{r=n-k-j}^{n-1}\frac{(j+k)!}{(j+k-n+r)!}s_{n}^{(n-r)}S_{j}^{(k)}\frac{\ln(x)^{j+k-n+r}}{x^n}$$ $s_j^{(k)}$ is the Stirling number of the first kind $$P_j^{(n)}(\ln(1))=P_j^{(n)}(0)=\sum_{k=1}^j (j+k)!s_{n}^{(j+k)}S_{j}^{(k)}$$ Therefore the n th derivative of $x^{x^x}$ in $x = 1$ can be expressed as: $$D_n(1)=\sum_{j=1}^n\sum_{k=1}^n\sum_{h=1}^n(j+1)\frac{(h+j)!}{(j+1+k-n)!} \binom{n}{k}s_{n}^{(j+k)}S_{j}^{(k)}$$ With $n$ between $2$ and $10$ we have $D_n(1)={2,9,32,180,954,6524,45016,360144,3023640} $ A179230 obtaining an explicit form for the Taylor expansion coefficients for $x\to 1:$ $$ x^{x^x}=f(x)=x+\sum_{n=2}^{\infty}\frac{D_{n}(1)}{n!}(x-1)^n$$ Since the series has no constant terms, it is possible to express the inverse function $f^{-1} (x)$ by Series Reversion $$f^{-1}(x)=x-(-1+x)^2+ \frac{1}{2}(-1+x)^3+\frac{7}{6}(-1+x)^4-\frac{17}{4} (-1+x)^5+O(x^6) $$","['functions', 'inverse', 'inverse-function', 'sequences-and-series']"
4152868,If bisector of angle $C$ of $\triangle ABC$ meet $AB$ in $D$ and circumcircle in $E$ prove that $\frac{CE}{DE}=\frac{(a+b)^2}{c^2}$.,If bisector of angle $C$ of $\triangle ABC$ meets $AB$ in point $D$ and the circumcircle in point $E$ then prove that $$\frac{CE}{DE}=\frac{(a+b)^2}{c^2}$$ . My Attempt Using the fact that $$CD=\frac{2ab}{a+b}\cos\frac{C}{2}$$ and $$AD.DB=CD.DE$$ we get $$\left(\frac{bc}{a+b}\right)\left(\frac{ac}{a+b}\right)=\left(\frac{2ab}{a+b}\cos\frac{C}{2}\right)DE\Rightarrow DE=\frac{c^2}{2(a+b)\cos\frac{C}{2}}$$ and then I did lot of calculation to obtain $$CE=CD+DE=\frac{a+b}{\cos\frac{C}{2}}$$ to get the final ratio. But I believe there would be a more generic way to do it perhaps a geometrical solution rather than relying upon the formula of angle bisector $CD=\frac{2ab}{a+b}\cos\frac{C}{2}$,"['triangles', 'trigonometry', 'geometry']"
4152916,Sufficient condition for $|1-P(z)|>\left|\frac{(z-a)(1-az)}{b}\right|$ with $|z|=1$,"Question Let $P:\mathbb C \to \mathbb C$ be a polynomial with real coefficients, $a\in(0,1)$ and $b\in\mathbb R_+$ . I want to find a   sufficient condition for $$|1-P(z)|>\left|\frac{(z-a)(1-az)}{b}\right|, \quad \forall \;z\in\mathbb C \quad\text{such that}\; |z|=1,$$ that is not too restrictive. I understand ""not too restrictive"" is somewhat subjective, but any improvement on the condition I obtained in my attempt below would be helpful. My Attempt On one hand, it can be shown that $$|z|=1\;\Rightarrow\;\left|\frac{(z-a)(1-az)}{b}\right|\le \frac{(1+a)^2}{b}.$$ On the other hand, by the reverse triangle inequality $$|z|=1\;\Rightarrow\;|1-P(z)|\ge|1-|P(z)||,$$ and since for $|z|=1$ we have $|P(z)|\le||P||_2$ , then if $||P||_2<1$ we have that $$|z|=1\;\Rightarrow\;|1-P(z)|\ge 1-||P||_2.$$ Therefore, a sufficient condition is that $$1-||P||_2>\frac{(1+a)^2}{b}$$ or $$||P||_2<1-\frac{(1+a)^2}{b}.$$ This condition appears to work, but it seems to me to be unnecessarily restrictive. By looking at the two sides of the inequality separately I have not made use of the fact that $z$ is the same on both sides. I suspect, there may be a way to explore the relationship between the sides.","['complex-analysis', 'inequality', 'polynomials']"
4153054,Cootie catcher function,"Consider the piecewise linear function $f:[0,1]^2 \longrightarrow [0,\infty[$ given by the following plot I was able to find the ugly expression $$f(x,y)=\begin{cases}
a \min\{x,1-x\} + b \min\{y,1-y\}, \ |x-\tfrac{1}{2}| < |y-\tfrac{1}{2}|\\
b \min\{x,1-x\} + a \min\{y,1-y\}, \ \text{otherwise}
\end{cases},$$ (with $a= 1.8, b= 0.6$ in this case). I'm looking for a more elegant expression for $f$ preferably without 'cases' and 'min/max'. Any ideas? Background: I'm trying to derive a 2D orthonormal basis similar to the Haar-Wavelet which becomes a 'zigzag' when integrated.","['calculus', 'functions', 'real-analysis']"
4153082,local martingale remains local martingale when lowering the localizing stopping times,"Assume we have a stochastic process $(X_t)_{t \geq 0}$ which is a local martingale in respect to some filtration $F=(F_t)_{t \geq 0}$ . That means by definition that there exists an almost surely increasing sequence of stopping times $(T_k)_{k \in \mathbb{N}}$ , that diverges almost surely and such that the stopped process $X^{T_k} = (X_t^{T_k})_{t \geq 0}$ with $X_t^{T_k} = X_{\min(t,T_k)}$ is a martingale for every $k \in \mathbb{N}$ , e.g. for every $s \geq 0$ with $s \leq t$ it holds that $\mathbb{E}[X^{T_k}_t | F_s] = X^{T_k}_s$ . Now take another sequence of stopping times $(S_k)_{k \in \mathbb{N}}$ with $S_k \leq T_k$ and $S_k \rightarrow \infty$ . Does it hold that the stopped process $X^{S_k}$ is a martingale too for all $k \in \mathbb{N}$ ? In other words, would the $S_k$ be a valid choice of stopping times in the definition for a local martingale? I thought about applying the optional stopping theorem. I know that the stopped process for every $k$ is a martingale. Using this martingale in the optional stopping theorem and stopping again with a stopping time $S_n$ for some $n$ would yield that the stopped process $(X^{T_k})^{S_n}$ is again a martingale. But this seems to easy, since I don't need that the sequence of stopping times $S_k$ is smaller than $T_k$ . Some help would be really appreciated or an counterexample, if the statement doesn't even hold.","['stochastic-processes', 'local-martingales', 'martingales', 'probability-theory', 'stochastic-calculus']"
4153102,how to find the limit of f(x) when x tends to infinity,"Let $f:[1,\infty]\to \mathbb{R}$ be a differentiable function such that $f(1)=1$ and $f'(x)=\frac{1}{1+f(x)^2}$ . Then how can I find the limit of $f(x)$ when $x$ tends to $\infty$ . Clearly $f'(x)>0$ and so $f$ is strictly increasing. Again $f(1)=1$ . So $f(x)\geq 1$ for $\forall x\in \mathbb{R}$ . Now I don't get any way to calculate the limit of $f(x)$ . Please help.","['derivatives', 'real-analysis']"
4153125,Shannon's proof that joint entropy is less or equal to the sum of marginal entropies,"I am reading Shannon's famous paper and I stuck on ""It is easily shown that"" part. More precisely I mean the inequality $$H(X, Y)\leq H(X) + H(Y),$$ where $X, Y$ are some discrete random variables. Following the paper we have that $$H(X,Y)=-\sum_{ij}p(i, j)\log(p(i,j)) \\ 
H(X)=-\sum_{ij}p(i, j)\log\left(\sum_{k}p(i,k)\right) \\
H(Y)=-\sum_{ij}p(i, j)\log\left(\sum_{l}p(l,j)\right)$$ And if we sum $H(X) + H(Y)$ we get that $$H(X) + H(Y) = -\sum_{ij}p(i, j)\log(p(i)p(j)).$$ And this is where I stuck. It is obvious from the above that if $X,Y$ are independent then the equality holds. However, the inequality is a different story. In general $p(i,j)$ and $p(i)p(j)$ are incomparable by simple inequality, thus some other methods must be used. What is the most straightforward proof of this inequality? (Or if one knows, what proof Shannon had in mind?). Shannon states the inequality before defining conditional entropy $H_X(Y)$ so I suspect that it in fact this should be easy... or did he lie?","['entropy', 'probability', 'information-theory']"
4153139,When can functions of a complex variable be integrated 'normally'?,"In our complex functions lecture notes, the lecturer integrates the complex function $f(z) = \frac{1}{z}$ around a circle of abitrary radius (i.e. over $z = r e^{i \theta}$ ). He first does a normal contour integral: $$\oint_C f(z) dz = i \int_0^{2\pi}\frac{re^{i \theta}}{re^{i \theta}}d\theta = 2\pi i $$ But then he says 'we could have done this directly': $$\oint_C \frac{dz}{z} = \ln(z) |^{re^{2\pi i}}_r = 2\pi i$$ . I don't understand why this 'direct' method is allowed. I could understand if we was integrating $f(z) = z$ instead of $f(z) = \frac{1}{z}$ since the former is analytic everywhere (entire). However the latter is not-defined at $z=0$ so I'm not sure why this is allowed, he seems to just get the correct answer with his limits in the final step (which I don't understand why he used instead of $r$ and $r$ etc).","['integration', 'cauchy-integral-formula', 'complex-analysis', 'complex-integration', 'complex-numbers']"
4153154,Homotopy cardinality of fiber bundles,"Consider the Homotopy cardinality (or $\infty$ -groupoid cardinality) $\chi(X):=\sum_{[x]\in\pi_0(X)}\prod_{i\geq0}|\pi_i(X,x)|^{(-1)^{i+1}}$ associated to a space $X$ . Suppose we have a fibre bundle $F\to E\stackrel{p}{\to}B$ such that for each $\chi(F),\chi(E),\chi(B)<\infty$ . If we make that assumption that each space have just finitely many components with each component having finite homotopy groups that vanish from a certain point, then the cardinalities are certainly finite. I am trying to calculate $\chi(E)$ in terms of $\chi(B)$ and $\chi(F)$ . Is it true that $\chi(E)=\chi(F)\chi(B)$ in these circumstances? I have tried to show it using the long exact sequence of the fibration: For each $[x]\in\pi_0(E)$ the long exact sequence yields $\frac{|\pi_{2i}(E,x)|}{|\pi_{2i-1}(E,x)|}= \frac{|\pi_{2i}(F,x)|| \pi_{2i}(B,p(x)) |}{|\pi_{2i-1}(F,x)|| \pi_{2i-1}(B,p(x)) |}$ , which takes a little bit of work decomposing the long exact sequence into short ones, but it is not particularly hard. Hence $\prod_{i\geq1}\frac{|\pi_{2i}(E,x)|}{|\pi_{2i-1}(E,x)|}= \prod_{i\geq1} \frac{|\pi_{2i}(F,x)|| \pi_{2i}(B,p(x)) |}{|\pi_{2i-1}(F,x)|| \pi_{2i-1}(B,p(x)) |}$ and if we sum over $[x]\in\pi_0(E)$ we obtain $$\chi(E)= \sum_{[x]\in\pi_0(E)}\prod_{i\geq1} \frac{|\pi_{2i}(F,x)|| \pi_{2i}(B,p(x)) |}{|\pi_{2i-1}(F,x)|| \pi_{2i-1}(B,p(x)) |} $$ and it appears that we are quite close to finishing an argument. However, I don’t see any slick way to split the rhs into the product $\chi(B)\chi(F)$ . My question is: If it is possible to split the rhs into that product, can someone tell me how to do it? If it however it s not the case that $\chi(E) = \chi(F)\chi(B)$ , can someone tell me how to express $\chi(E)$ in terms of $\chi(F)$ and $\chi(B)$ ? Thank you very much.","['higher-homotopy-groups', 'fiber-bundles', 'homotopy-theory', 'general-topology', 'algebraic-topology']"
4153236,Logic and Set Theory Question,"Demonstrate a logical system, and proposition set $S$ , such that: There exists a formula $\beta$ such that $S\nvdash \beta$ There exists a formula $\alpha$ such that $S \vdash \alpha \wedge S \vdash ¬(\alpha)$ Can someone please help me with that? I do not understand what I should do here.. Thanks a lot!!!","['elementary-set-theory', 'first-order-logic', 'logic', 'discrete-mathematics']"
4153322,"Solve system of equations $3(x+\frac{1}{x}) = 4(y + \frac{1}{y}) = 5(z+\frac{1}{z})$, $xy+yz+zx = 1$","Find all $x,y,z>0$ such that $$3(x+\frac{1}{x}) = 4(y + \frac{1}{y}) = 5(z+\frac{1}{z})$$ $$xy+yz+zx = 1$$ The only solution should be $x=\frac{1}{3}$ , $y = \frac{1}{2}$ , $z=1$ .
There is a way to do it with $x = \tan \alpha$ , etc., but I would like to find an even more elementary way. So far, I have written $$x^2 - \frac{k}{3}x + 1 = y^2 - \frac{k}{4}y + 1 = z^2 - \frac{k}{5}z + 1 = 0$$ and noticed that at least two of $x$ , $y$ , $z$ must be smaller than $1$ . (So in the bad cases I can express uniquely $x,y,z$ .) But then? Any help appreciated!","['algebra-precalculus', 'systems-of-equations', 'quadratics', 'trigonometry']"
4153340,Walter Rudin Theorem 1.20 (a) Proof,"Theorem 1.20 (a) If $x\in\mathbb{R}$ , $y\in\mathbb{R}$ and $x > 0$ , then there is a positive integer $n$ such that $nx > y$ . In the proof, set $A$ is assumed to have a least upper bound in $\mathbb{R}$ that implies set $A$ is bounded above. But I don't known how set $A$ is bounded above, given the constraint, $x$ is a positive real number and $n$ is a natural number. More clarification: $$A = \{nx\in\mathbb{R}_{>0} \mid (n\in\mathbb{N})\wedge(x\in\mathbb{R}_{>0})\}$$ My question is: how this set $A$ have a upper bound? And the whole theorem and proof didn't really make sense to me. I mean, if this theorem were reversed, i.e. $nx < y$ , I wouldn't be able to make difference between these two scenario. Potential duplicate post: is referring to possibility of set A being empty. In this post, I’m asking(implicitly) about contradiction statement of theorem and bdd property of set. Non empty set is just one of the condition must satisfy to talk about bddness of sets.","['upper-lower-bounds', 'real-analysis']"
4153361,How to calculate confidence interval (normal distribution)?,"There seems to be two formulas for calculating confidence interval of a sample with a normal distribution (in both cases, $P$ refers to the sample proportion): Method A: $$P \pm \frac{1}{\sqrt n}$$ where $n$ is the population sample size Method B: $$P \pm z \cdot \sqrt{\frac{P (1-P)} n}$$ where $n$ is the population sample size and $z$ is the $z$ -score radius for the intended confidence level (for example $1.96$ for $95\%$ ) Which method should I use or when should I use each method? What is the difference between the confidence level of the two formulae?","['statistical-inference', 'statistics', 'normal-distribution']"
4153364,Domain of the composition of functions,"If $f(x) = x^2$ and $g(x) = \sqrt{x-1}$ , find $f(g(x))$ and specify the domain. My solution $$
f(g(x)) = \left(\sqrt{x-1}\right)^2 = x-1,
$$ Domain: $x$ such that $x$ is any real number. Solution at the back of the book states that the domain is, $x$ such that $x \ge 1$ . Now I understand why that is the domain for $g(x)$ , for any number less than 1 would make $g(x)$ negative and the square root of a negative number is an imaginary number. But, that is not the cases with $f(g(x))$ . When is $(\sqrt{x-1})^2$ not a real number? isn't any input valid?","['radicals', 'functions']"
4153373,The $L^p$ Norm of an Integral Average,"Im generally curious about $L^p$ estimates of integral averages. In particular, let $M$ be a (possibly noncompact) space and $f$ a (probably) smooth function with (probably, but in general maybe not) compact support. I want to consider the integral averages of f at fixed scale r>0: $$ f_r(x):=\frac{1}{vol(B_r(x))}\int_{B_r(x)}f(y)\mathrm{d}\mu(y).$$ Here M may be a Riemannian manifold, or some other space where $vol(B_r(x))$ may be non-constant, but where we assume noncollapsing so that $\inf_{x\in M}vol(B_r(x))>0$ . I would like to know what can generally be said about the $p$ norms of $f_r$ , in terms of $f$ itself. For instance, it is trivial to prove a $(1,\infty)$ estimate: $$\|f_r\|_\infty\leqslant \frac{1}{\inf_{x\in M}vol(B_r(x))}\|f\|_1.$$ I am led to suspect that an estimate like $$\|f_r\|_2\leqslant \frac{1}{\inf_{x\in M}vol(B_r(x))^{1/2}}\|f\|_1$$ should hold, perhaps via an application of the $(1,\infty)$ estimate, Holders inequality, Jensen, or something of this nature. In any case, I havent been able to pin such a thing down. Writing out the $L^2$ norm of $f_r$ leads me an estimate like $$\|f_r\|^2_2\leqslant \frac{1}{\inf_{x\in M}vol(B_r(x))}\|f\|_1\|f_r\|_1$$ which would be perfect if there were a $(1,1)$ estimate with constant $1$ . Any ideas or directions would be very appreciated!","['integration', 'harmonic-analysis', 'functional-analysis', 'real-analysis']"
4153417,A closed set in $\operatorname{Spec}(R)$ is irreducible iff $I(X)$ is prime,"Let $R$ be a ring. Prove that a closed set $X\subseteq\operatorname{Spec}(R)$ is irreducible iff $I(X)=\bigcap_{p\in X}p$ is prime. I proved $\rightarrow$ . My attempt at $\leftarrow$ is as follows: I assume for a contradiction that $X$ is reducible. I can then write $X=(V(I_1)\cap X)\cup(V(I_2)\cap X)$ with $V(I_1),V(I_2)\subseteq\operatorname{Spec}R$ closed sets. Since each of them is contained (and not equal) in $X$ , I can find $p_i\in V(I_i)$ s.t $p_1\not\subseteq p_2$ and $p_2\not\subseteq p_1$ . I know that in the finite case, that would mean that the intersection above would not be prime (it's easy to prove for two prime ideals and I guess you can inductively prove it for any finite number). But is it still true for any intersection? Otherwise, how can I get a contradiction? Thanks in advance.","['algebraic-geometry', 'abstract-algebra', 'maximal-and-prime-ideals', 'commutative-algebra']"
4153440,Definition of Ergodicity in Theodoridis' Machine Learning,"This is related, but is not the same as https://stats.stackexchange.com/questions/319190/wide-sense-stationary-but-not-ergodic . Note that I am not assuming stationarity. Theodoridis, in his Machine Learning (2nd ed., p. 44), states the following: Definition 2.3 ( Ergodicity ). A stochastic process is said to be ergodic if the complete statistics can be determined by any one of the realizations. Theodoridis then goes on to state that ergodicity implies stationarity with a vague explanation that I can't seem to parse through: In other words, if a process is ergodic, every single realization carries identical statistical information and it can describe the entire random process. Since from a single sequence only one set of PDFs [probability density functions] can be obtained, we conclude that every ergodic process is necessarily stationary . What does ergodicity mean in this situation? In particular, the ""complete statistics"" phrase is throwing me off. I get the feeling that Theodoridis isn't referring to the $$\text{""}\mathbb{E}[g(T) \mid \theta] = 0 \implies \mathbb{P}(g(T) = 0 \mid \theta) = 1$$ for all $\theta$ "" definition of a complete statistic $T$ for $\theta$ . I recently completed a measure-theoretic probability sequence, so if this requires measure-theoretic probability tools to make precise, please use those tools in answering this question. Some searching talks about how a ""time-average is equal to the ensemble average,"" but I'm not really understanding this explanation and how it relates to the definition in Theodoridis' text. Sources that I can use to supplement this with a more detailed coverage of the theory would be appreciated as well. Hamilton's Time Series Analysis (as well as most of my other texts and most coverage I find online on this topic) only covers ergodicity with the assumption that we have stationarity to begin with. Robert and Casella's Monte Carlo Statistical Methods only discuss ergodicity in the context of Markov chains.","['statistics', 'ergodic-theory', 'machine-learning', 'stochastic-processes', 'probability']"
4153445,Pigeonhole principle in terms of measure of sets,"I'm wondering if this pigeonhole-like statement holds. Let $(S_i)_{i \in N}$ be a countable family of subsets of $[0,1]$ such that each $S_i$ is (either Jordan or Lebesgue) measurable and there is some $\delta$ s.t. each $S_i$ has measure at least $\delta$ . Is it true that there is some infinite subset $I \subset N$ s.t $\cap_{i \in I}S_i$ is nontrivial? Would this depend on the measure used/in case this doesn't hold, could additional assumptions on the sets be added to make it true?","['pigeonhole-principle', 'measure-theory']"
4153470,Statistical Brain Teaser - Randomized speed dating,"You and 25 strangers are in a room for 25 rounds of ""speed dating."" Each round, you are randomly paired with another person, regardless of whether you have seen them before or not. What is the probability that after 25 rounds, you have talked to only 1 person? 2 people? All 25 people? The probability of talking to only 1 person is $\left(\frac{1}{25}\right)^{25} * {25 \choose 1} $ , but after 2 people is where I get lost. Running a simulation, I get about a $25\%$ chance of having talked to exactly 15 people. How do I get to this mathematically? I thought it might be $$ \left(\frac{1}{25}\right)^{25} * (25 * 24 * 23 *...*15)  * {25 \choose 15} ,$$ however it does not get me anywhere close to $25\%$ .","['statistics', 'combinatorics', 'probability']"
4153501,"Binary table containing all the combinations 00, 01, 10, 11 in every two columns","The following table 6x5 11111
10000
01000
00100
00010
00001 has an interesting property: if you select any two columns and delete the rest, the resulting table 6x2 will always contain rows 00, 01, 10, 11 I want to prove that this example has the minimal number of rows, that is if an $m\times n$ table with 0's and 1's satisfies the property, then $m \ge n +1$ . The only interesting case is $m=n$ . (if $m<n$ , add zero rows until $m=n$ ) I tried induction, to no avail. (If you delete the first column of an $n \times n$ table, you receive an $n \times (n-1)$ table and so the induction proposition doesn't apply) Also, one can try replacing 1's with 0's so long as this doesn't break the property. This idea has also led me nowhere. At last, there is a geometric interpretation: read rows as points of $\mathbb{R}^n$ , all that we want is a set of n vertices of an $n$ -cube, such that projection of this set onto any 2-dimensional face is a square. Any n points must lie in a hyperplane, so this fact is kinda about sections of a cube. Any hints are welcome.",['combinatorics']
4153508,Are There Always More Conjugacy Classes in the Kernel of a Morphism to $\Bbb Z_2$ than Not?,"Let $G$ be a finite group and let $\phi:G\to\Bbb Z_2$ be a homomorphism to the group with two elements. Is it always the case that there are at least as many conjugacy classes in the kernel of $\phi$ as conjugacy classes not in the kernel of $\phi$ ? I've tried a little bit of messing around algebraically and written down some exact sequences of $G$ -modules to try to apply the methods of group cohomology, but I haven't gotten anything to work. My inspiration here is the special case when $G$ is the symmetric group $S_n$ and $\phi$ is the sign homomorphism. In this case conjugacy classes of $G$ correspond to partitions, and the problem becomes about counting partitions of $n$ with an even number of even parts versus an odd number of even parts. I was able to prove (via generating functions and also bijectively) that the number of partitions of $n$ with an even number of even parts minus the number of partitions of $n$ with an odd number of even parts is equal to the number of partitions of $n$ with all parts odd and distinct. I could not find a reference for this fact after some googling, so I would be interested to know if this is a well-known partition identity. I'm also interested in possible extensions of this problem where $\Bbb Z_2$ is replaced by another group $H$ (possibly required to be abelian).","['finite-groups', 'abstract-algebra', 'combinatorics', 'group-cohomology', 'group-theory']"
4153537,Discontinuity of $b$-Metric,"Here I have a definition of $b$ -metric on a set: Let $s \geq 1$ , $X$ is any nonempty set, and $p:X \times X \rightarrow [0,\infty)$ that satisfied $p(x,y)=0$ iff $x=y$ $p(x,y)=p(y,x)$ $p(x,z)\leq s[p(x,y)+p(y,z)]$ for all $x,y,z\in X$ . The function $p$ is called $b$ -metric on $X$ . I have showed that if I have $X = \mathbb{N}\cup\{\infty\}$ and $d: X \times X \rightarrow \mathbb{R}$ where $$d(m,n) = \begin{cases}
		0&,\text{for }m = n\\
		\left|\dfrac{1}{m} - \dfrac{1}{n}\right|&, \text{ for } m\text{ and } n\text{ are both even or } m\text{ is even and } n=\infty\text{ or}\\ 
		&\ \ m=\infty\text{ and } n\text{ is even }\\
		8&, \text{ for } m\text{ and } n\text{ are both odd or } m\text{ is odd and } n=\infty \text{ or }\\ 
		&\ \ m=\infty\text{ and } n\text{ is odd }\\
		5&, \text{ others}.
		\end{cases}$$ then $d$ is $b$ -metric on $X$ with $s=3$ , I want to show that $d$ (as function on a metric space) discontinuous at $(\infty,1) \in X \times X$ and this was how I tried to show the discontinuity. Let $f: X\rightarrow\mathbb{R}$ where $f(x) = d(2x,1)$ for all $x\in X$ (the metric I use for $X$ and $\mathbb{R}$ is usual metric). I choose $\varepsilon_0 = 2$ . By Archimedean Properties, for all $N > 0$ we have $m_N \in \mathbb{N}$ such that $N \leq m_N$ . Thus \begin{align*}
		|f(2m_N) - f(\infty)| = {} & |d(2m_N,1) - d(\infty,1)|\\
		= {} & |5 - 8| = 3 > \varepsilon_0.
		\end{align*} So, $f$ discontinuous at $\infty$ . And intuitively, $d$ discontinuous at $(\infty,1) \in X \times X$ . My question is:
Is there any properties I can use such that my intuitive (the last line) is right? Any help is appreciated :(","['functions', 'metric-spaces', 'real-analysis']"
4153565,What's wrong with this way of manipulating Grandi's series,"Problem: evaluate $S = 1 - 1 + 1 - 1 + 1-\cdots$ $1 = \lim_{t \rightarrow 1^{-}} t^n$ for any positive integer $n$ . Here $t \rightarrow 1^{-}$ means $t \rightarrow 1$ and $t<1$ . $$S = \lim_{t \rightarrow 1^{-} } 1 - \lim_{t \rightarrow 1^{-} } t + \lim_{t \rightarrow 1^{-} } t^2 - \lim_{t \rightarrow 1^{-} } t^3 + \cdots = \lim_{t \rightarrow 1^{-}} (1-t+t^2-t^3 + t^4 - t^5 +\cdots ) = \lim_{t \rightarrow 1^{-}} \frac{1}{1+t} = \frac{1}{2}$$ What's wrong with this reasoning? It seems like the limit exists for $1 = \lim_{t \rightarrow 1^{-}} t^n$ , so every step seems to follow?","['limits', 'fake-proofs', 'infinity', 'real-analysis']"
4153582,Evaluating $\Gamma$-function at $x=1/2$,"I was following an explanation of the gamma function and everything made sense until the author evaluated the function at 1/2. The $\Gamma$ -function is defined as the following integral: $$\Gamma(x)=\int_0^\infty x^{n-1}e^{-x}\,dx$$ And now evaluating $\Gamma$ ( $1/2$ ) we get: $$\Gamma(1/2)=\int_0^\infty x^{-1/2} e^{-x} \, dx$$ This is further simplified by setting $x=y^2$ and therefore $dx=2y\,dy$ . From this, we can get the following integral: $$2\int_0^\infty e^{-y^2}\,dy$$ This next step is where I am confused. The author proceeds to make the following equality: $$2\int_0^\infty e^{-y^2}\,dy = 2\int_0^\infty e^{-x^2} \, dx$$ I don't understand how this equality is reached. Doesn't this mean $\int_0^\infty x^{-1/2}e^{-x} \, dx$ is equal to $2\int_0^\infty e^{-x^2} \, dx$ ? Does it have something to do with the bounds? Thanks for the help in advance!","['integration', 'gamma-function']"
4153605,Equivalent Probability Measures and Girsanov's Theorem,"I know that if we have Brownian motion ( $W$ ) on canonical space (denote the probability measure by $P$ ) and let $Q$ be the unique probability measure such that $$Q(A) = E_P\left(\mathbf{1}_A \exp \left(\alpha W_t - \frac{\alpha^2 t}{2} \right) \right) \quad \forall A \in \mathcal{F}_t$$ then $$\widetilde W_t \equiv W_t - \alpha t$$ is Brownian motion under $Q$ .  In order to use Girsanov's to get this conclusion, we needed that $Q$ and $P$ are equivalent (i.e. $P(A) = 0 \iff Q(A) = 0$ ).  However, by the SLLN for Brownian motion, $$ \lim_{t \rightarrow \infty} \frac{W_t}{t} = 0 \quad P\text{ - a.s.} \quad \quad \text{and} \quad \quad\lim_{t \rightarrow \infty} \frac{\widetilde W_t}{t} = 0 \quad Q\text{ - a.s.} \implies \lim_{t \rightarrow \infty} \frac{W_t}{t} = \alpha \quad Q\text{ - a.s.}$$ My question is: how does this not contradict the equivalence of the probability measures $P$ and $Q$ ?","['stochastic-processes', 'measure-theory', 'brownian-motion', 'probability-theory']"
4153610,Proving there is no matrix in $\mathbb{F}_2^{2\times2}$ that commutes with every invertible matrix,"Consider $\mathbb{F}_2^{2\times2}$ , the $2\times2$ -matrices over the finite field $\mathbb{F}_2$ . It seems to me (by trial and intuition, if I'm being honest), that there should be no matrix (besides $\mathbf{1}, \mathbf{0}$ ) that would commute with every invertible matrix in $\mathbb{F}_2^{2\times2}$ . Note that I am not requiring that this matrix commute with all other matrices in $\mathbb{F}_2^{2\times2}$ , only with the invertible ones. For one, we do know that the group of invertible matrices in $\mathbb{F}_2^{2\times2}$ has trivial center, so I would only need to check singular matrices. I tried to prove this by brute-force calculation, but since that is rather tedious, I would be interested to know a more analytical approach to this problem (or if I'm mistaken entirely).","['matrices', 'linear-algebra']"
4153658,Finding closed form of $\sum\limits_{r=0}^n r^2 \binom nr p^{n-r}q^r$ where $p>0$ and $q=1-p$ [duplicate],"This question already has answers here : Can $n(n+1)2^{n-2} = \sum_{i=1}^{n} i^2 \binom{n}{i}$ be derived from the binomial theorem? (5 answers) Closed 3 years ago . Here is a question from Indian Statistical Institute (ISI) Entrance Exam CSA-2020, Question 11: $\binom n0, \binom n1,..., \binom nn$ denote the binomial coefficients in the expansion
of $(1 + x)^n \ \text{where} \ 
 p > 0$ is a real number and $q = 1 − p$ then $$ 
\sum\limits_{r=0}^n r^2 \binom nr p^{n-r}q^r 
$$ is equal to $
(A)\ np^2q^2 \
(B)\ n^2p^2q^2 \
(C)\ npq + n^2p^2 \
(D)\ npq + n^2q^2
$ My Thoughts: (1) If I put the formula for $\binom nr$ in the given sum. I am getting $$ 
\sum\limits_{r=0}^n r^2 \frac{n!}{r!(n-r)!} p^{n-r}q^r 
$$ How do I proceed after this? Edit: Taking help from here and the comments below I wrote the following, although I am stuck in the last step, please help me out. $$
(1+x)^n=\sum\limits_{r=0}^n \binom nr x^r
$$ Taking derivative on both sides we have: $$
n(1+x)^{n-1}=\sum\limits_{r=1}^n r\binom nr x^{r-1}
$$ Multiplying both sides by $x$ we get: $$
nx(1+x)^{n-1}=\sum\limits_{r=1}^n r \binom nr x^r
$$ Taking derivative another time: $$
n(1+x)^{n-1}+nx(n-1)(1+x)^{n-2}=\sum\limits_{r=1}^n r^2 \binom nr x^{r-1}
$$ Multiplying both sides by $x$ we get: $$
nx(1+x)^{n-1}+nx^2(n-1)(1+x)^{n-2}=\sum\limits_{r=1}^n r^2 \binom nr x^r
$$ Given that $p>0$ and $q=1-p$ . So let $x=\frac qp$ then we have: $$
n\frac qp\left(1+\frac qp\right)^{n-1}+
n\left(\frac qp\right)^2(n-1)\left(1+\frac qp\right)^{n-2}
=
\sum\limits_{r=1}^n r^2 \binom nr \left(\frac qp \right)^r
$$ $$
\implies 
n\frac qp\left(\frac{p+q}{p}\right)^{n-1}+
n\left(\frac qp\right)^2(n-1)\left(\frac{p+q}{p}\right)^{n-2}
=
\sum\limits_{r=1}^n r^2 \binom nr \left(\frac qp \right)^r
$$ $$
\implies 
n\frac qp \left(\frac{1}{p}\right)^{n-1}+
n\left(\frac qp\right)^2(n-1)\left(\frac{1}{p}\right)^{n-2}
=
\sum\limits_{r=1}^n r^2 \binom nr \left(\frac qp \right)^r \ \ \ \ \ 
[\text{Since}\ p+q=1]
$$ $$
\implies 
n\frac{q}{p^n}+
n(n-1)\frac{q^2}{p^n}
=
\sum\limits_{r=1}^n r^2 \binom nr \left(\frac qp \right)^r
$$ $$
\implies 
nq+
n(n-1)q^2
=
p^n\sum\limits_{r=1}^n r^2 \binom nr \left(\frac qp \right)^r
$$ $$
\implies 
nq+
n(n-1)q^2
=
\sum\limits_{r=1}^n r^2 \binom nr p^{n-r}q^r
$$ $$
\implies 
n(q-q^2)+
n^2q^2
=
\sum\limits_{r=1}^n r^2 \binom nr p^{n-r}q^r
$$ $$
\implies 
n\{(1-p)-(1-p)^2\}+
n^2q^2
=
\sum\limits_{r=1}^n r^2 \binom nr p^{n-r}q^r
$$ $$
\implies 
n\{(1-p)-(1+p^2-2p)\}+
n^2q^2
=
\sum\limits_{r=1}^n r^2 \binom nr p^{n-r}q^r
$$ $$
\implies 
np(1-p)+
n^2q^2
=
\sum\limits_{r=1}^n r^2 \binom nr p^{n-r}q^r
$$ $$
\implies 
npq+
n^2q^2
=
\sum\limits_{r=1}^n r^2 \binom nr p^{n-r}q^r
$$ Hence option (D) is correct. This question is almost similar to this question but is slightly different. So I have added this question.","['binomial-coefficients', 'combinatorics', 'binomial-theorem']"
4153673,How to proof that a manifold covered by two charts satisfying $f_i(\mathbb{R}^m \setminus B^o) \subset f_j(B^o)$ for $i\not=j$ is simply connected?,"While studying for my comprehensive exam, I came across the following problem, which I am unable to solve. Let $B$ denote the closed unit ball centered at the origin in Euclidean space $\mathbb{R}^m$ , $\partial B$ denote the boundary sphere of $B$ and $B^o$ denote the interior of $B$ . Let $f_i: \mathbb{R}^m \to M$ , $i=1,2$ denote smooth embeddings into a smooth $m$ -dimensional manifold $M$ which satisfy $$f_1(\mathbb{R}^m) \cup f_2(\mathbb{R}^m) = M$$ $$f_1(\mathbb{R}^m \setminus B^o) \subset f_2(B^o) $$ $$f_2(\mathbb{R}^m \setminus B^o) \subset f_1(B^o) $$ Suppose that $m \geq 2$ . Show that $M$ is a simply connected closed manifold. Here is what I have been able to do so far. Proof that M is closed: Let $\{x_i\}$ be a sequence in $M$ , then we can subtract a subseqeunce $\{x_{i_j}\}$ which is contained entirely within $f_1(\mathbb{R}^m)$ or entirely within $f_2(\mathbb{R}^m)$ . Suppose without loss of generality that $\{x_{i_j}\}$ is contained entirely within $f_1(\mathbb{R}^m)$ , then we can subtract a subsubsequence $\{x_{i_{j_k}}\}$ which is contained entirely within $f_1(B)$ or entirely within $f_1(\mathbb{R}^m \setminus B^o)$ . Suppose first that $\{x_{i_{j_k}}\}$ is contained entirely within $f_1(B)$ then by compactness of $f_1(B)$ there is subsubsubsequence $\{x_{i_{j_{k_l}}}\}$ which converges in $f_1(B)$ . If instead the subsubsequence $\{x_{i_{j_k}}\}$ is contained entirely within $f_1(\mathbb{R}^m \setminus B^o)$ , then by assumption the subsubsequence is contained within $f_2(B)$ , which is compact, so there is a subsubsubsequence $\{x_{i_{j_{k_l}}}\}$ which converges. Hence $M$ is compact. Since $f_1$ , and $f_2$ are charts without boundary that covers $M$ , then $M$ is a manifold without boundary, so $M$ is closed. I am unable to show that $M$ is simply connected. It is connected, as it is the union of two connected sets with non-empty intersection. If $f_1(\mathbb{R}^m) \cap f_2(\mathbb{R}^m)$ is connected then by the Seifert-Van Kampen Theorem $M$ would be simply connected. My idea has been to try and come up with suitable open subsets $U, V$ and then use the Mayer-Vietoris sequence to show that the 0th de Rham Cohomology group of $f_1(\mathbb{R}^m) \cap f_2(\mathbb{R}^m)$ has dimension 1. None of my candidates for $U$ and $V$ has worked so far. The result fails when $m=1$ since then $M=S^1$ . I am not sure how to invoke the $m \geq 2$ hypothesis, if the problem is to be solved without a Mayer-Vietoris argument. The problem should be solvable using techniques from Topology (By Munkres) and Introduction to Smooth Manifolds (By Lee). This means no Generalized Jordan Curve Theorem, Reduced Integral Homology or Alexander Duality. Any help/solution would be much appreciated.","['de-rham-cohomology', 'smooth-manifolds', 'fundamental-groups', 'differential-topology', 'differential-geometry']"
4153690,Difference Triangles,"In a difference triangle , a row of $n$ integers is given, then their differences are written underneath, and then another row of difference is added, until there is a triangle of $n (n+1)/2$ integers. For example: The above triangle also has the property that it has 15 distinct values with no values from 1 to 15 either missing or repeated.  According to this query , this is the last difference triangle with nothing missing and nothing repeated. That's similar to the last perfect ruler, with marks ${0,1,3,6}$ . These marks measure all differences 1 to 6 with no lengths missing or repeated.  For longer rulers, there are two options. No repeats allowed. The smallest ruler with $n$ marks is called a Golomb ruler . No missing values. The longest ruler with $n$ marks is an optimal sparse ruler . We can look at difference triangles with these same conditions. Order 6. $T_6 = 21$ . The smallest difference triangle with no repeats has large value 22 ( $T_6  +1$ ) and misses 15. The largest difference triangle with no misses has large value 20 ( $T_6  -1$ ) and repeats 4. Order 7. $T_7=28$ . The smallest known difference triangle with no repeats has large value 33 ( $T_7  +5$ ) and misses ${16, 20, 22, 29, 30}$ . The largest known difference triangle with no misses has large value 24 ( $T_7  -4$ ) and repeats 1 and 3. What happens with orders 8 and beyond?  With $T_8=36$ , how close can we get to $36$ with either no repeats or no misses?","['recreational-mathematics', 'puzzle', 'combinatorics', 'integers']"
4153810,"Is $x \mapsto (x, |x|)$ locally diffeomorphic to $\mathbb R$ at $x = 0$? (there is a philosophical question)","Typical Example: $f: \mathbb R \rightarrow \mathbb R^2$ , $f(x) = (x, |x|)$ . Let manifold $M = f(\mathbb R)$ .
If you ask me, ""is $M$ locally diffeomorphic to $\mathbb R$ at $x = 0$ ?"", then I would immdiately answer: ""No. There is a sharp corner at $x = 0$ ."" I could answer this question since I know what an embedding of $M$ in $\mathbb R^2$ : I can clearly define a homeomorphism $(x, |x|) \mapsto x$ . With this knowledge, I could infer that since there is a sharp corner, local diffeomorphism does not hold. Now, suppose I have a $\vee$ shape stretching out in both direction. (Of course, we know how this shape can be embedded in $\mathbb R^2$ just by looking, but let us suppose we do not know such information.) Then, how can I be so sure that $\vee$ is not diffeomorphic to the real line $\mathbb R$ at the ""sharp corner?"" We do not have a notion of ""sharp corner"" just by looking at $\vee$ itself. Moreover, is it even correct that the embedding of $\vee$ in $\mathbb R^2$ has a corner? Why cannot we assert that it is smooth?","['general-topology', 'differential-geometry']"
4153826,Trying to write the bundle structure for $\Lambda^2(T^*(\mathbb P^3\times \mathbb P^3))$,"I'm trying to find an atlas and in general the bundle structure for $\Lambda^2(T^*(\mathbb P^3\times \mathbb P^3))$ respect to, for example, the local chart $(U_{01},\psi_{01})$ , where $$U_{01}:=\{((x^0:x^1:x^2:x^3),(y^0:y^1:y^2:y^3))\in\mathbb P^3\times \mathbb P^3|x^0\ne 0,y^1\ne 0\}$$ and $\psi_{01}:U_{01}\subset\mathbb P^3\times \mathbb P^3\to\mathbb R^3\times \mathbb R^3$ is the standard chart that divides for the non-zero component. In particular I was trying to build the bundle structure for $\Lambda^2(T^*(\mathbb P^3\times \mathbb P^3))$ starting from $T^*(\mathbb P^3\times \mathbb P^3)$ and I did this: Could you tell me if it's correct? How could I ""extend"" this idea to build a smooth structure for $\Lambda^2(T^*(\mathbb P^3\times \mathbb P^3))$ ? Thank you in advance.","['projective-geometry', 'tangent-bundle', 'differential-forms', 'differential-geometry']"
4153846,"Solution Verification: Find the Global Max and Min of $z=x+y$, in $D=\{x\ge 0, y\ge 0, y\le 3x, x^2+y^2\le 25 \}$","$z=x+y$ $D=\{x\ge 0, y\ge 0, y\le 3x, x^2+y^2\le 25 \}$ My attempt: Plan: Find inner critical points. Find maxmin of border curves. Find intersecting points of border curves. Find values of all points and decide global maximum and minimum. Inner critical points: First of all, I checked the inner points of the domain by taking $z_x,z_y=0$ . And obviously there's no critical points there since $z_x=z_y=1$ . Borders or domain curves (not sure what it's called): Next step, is finding the maximum and minimum on the domain curves (Or the borders), $x^2+y^2=25 \Longrightarrow x = \sqrt{25-y^2}$ , Substituting that into my function: $z = \sqrt{25-y^2}+y \Longrightarrow z'=\frac{-2y}{2\sqrt{25-y^2}}+1=0$ (To find critical points). $-y+\sqrt{25-y^2}=0 \Longrightarrow y^2=25-y^2 \Longrightarrow y^2 =\frac{25}{2} \Longrightarrow y=\frac{5}{\sqrt{2}}$ (I took positive value only because $y \ge 0$ ). The point $(x,\frac{5}{\sqrt{2}})$ must be on the circle, so $x^2+\frac{25}{2}=25 \Longrightarrow x=\frac{5}{\sqrt{2}}$ ( $x \ge 0$ ). So I've found a point $(\frac{5}{\sqrt{2}},\frac{5}{\sqrt{2}})$ Next, $y=0 \Longrightarrow z=x \Longrightarrow z_x=1$ There's no critical points. (same for $x=0$ ). Next, $y=3x\Longrightarrow z=4x \Longrightarrow z_x = 4$ (no critical points too). Intersecting points of border curves: Now my next step is to check points where $x^2+y^2=25$ and $y=3x$ and $y=0$ and $x=0$ meet, I can see points $(0,0)$ , $(0,5)$ , now I need to find the intersection of the circle and $y=3x$ . $(x^2 + 9x^2)=25 \Longrightarrow 10x^2=25 \Longrightarrow x=\frac{5}{\sqrt{10}}$ Substituting in the $y=3x \Longrightarrow y=\frac{15}{\sqrt{10}}$ And so my last point is $(\frac{5}{\sqrt{10}}, \frac{15}{\sqrt{10}})$ Calculating the value of the function in the points I've found: My next and last step, is to calculate the value of $z=f(x,y)$ in all the points I have found: $f(\frac{5}{\sqrt{2}},\frac{5}{\sqrt{2}})= \frac{10}{\sqrt{2}} = 7.071...$ $f(0,0)=0$ $f(0,5)=5$ $f(\frac{5}{\sqrt{10}}, \frac{15}{\sqrt{10}})= \frac{20}{\sqrt{10}}=6.324...$ Final Answer: And so, my global maximum is $(\frac{5}{\sqrt{2}},\frac{5}{\sqrt{2}})$ , Global minimum is $(0,0)$ . I would love to hear feedback about my solution and to know if I missed some points, checked some unnecessary points or some of my work flow doesn't seem okay. Thanks in advance!","['maxima-minima', 'multivariable-calculus', 'solution-verification']"
4153864,What is a good category for probability theory?,"I am searching for a good category to think about probability theory in, with arrows as something like stochastic maps. There are certain nice structural features I would like the category to have (listed below) and I was hoping somebody could tell me if there are any categories meeting a lot of these conditions: A monoidal category with a tensor product corresponding to something like the Cartesian product. Actually it would be good if it was a Markov category (like $\text{FinStoch}$ is for example). In addition to having objects corresponding to finite sets/spaces it would be good to have infinite sets/spaces appearing as objects. Ideally I want to be able to talk about continuous probability distributions (using real numbers etc.) in addition to discrete probability distributions. It would be great if I can form disjoint unions or coproducts of objects. Ideally I would like the category to be a bimonoidal category / rig category with $ \otimes $ corresponding to the Cartesian product and $\oplus$ corresponding to the disjoint union. It would be great if the deterministic maps/functions from something like Set are included in the category too, so I can think about functional programming and recursion there too. I realize I am asking for a lot, but there are many different categories for doing probability theory which are mentioned in the literature, so I was hoping somebody could highlight some categories which get close to fulfilling my conditions. In particular there seem to be many categories such as $\text{FinStoch}$ , $\text{Stoch}$ , $\text{CGStoch}$ and $\text{BorelStoch}$ defined by Fong and Fritz . Also 28:38 of Perrone's YouTube video defines more categories (like the category of finitary stochastic maps, which is the Kleisli category of Set using the distribution monad, and may meet some of my conditions).","['stochastic-processes', 'monoidal-categories', 'probability-theory', 'category-theory']"
4153872,"Solution Verification: Find Extrema points of the function $f(x,y)=2^{3x+8y}$ that are on $x^2+y^2=1$. [duplicate]","This question already has answers here : Solving trigonometric equations of the form $a\sin x + b\cos x = c$ (7 answers) Closed 3 years ago . $f(x,y)=2^{3x+8y}$ $x^2+y^2=1$ Polar Coordinates approach: Let $x=cos(t), y=sin(t), 0\le t \le 2\pi$ (We can see that this solved the circle equation). Substituting them into my function: $f = 2^{3\cos(t)+8\sin(t)} \Longrightarrow f=e^{(3\cos(t)+8\sin(t))ln(2)}$ Taking derivative to find critical points: $f' = \ln(2)(-3\sin(t)+8\cos(t))e^{(3\cos(t)+8\sin(t))ln(2)}=0$ Now $e^x>0$ , so I need to find when $-3\sin(t)+8\cos(t)=0 \Longrightarrow 3 \sin(t)=8\cos(t) \Longrightarrow \tan(t) = \frac{8}{3} \Longrightarrow t=\arctan(\frac{8}{3}), \arctan(\frac{8}{3}) + \pi $ So I get two Points: $(\cos(\arctan(\frac{8}{3})) ,\sin(\arctan(\frac{8}{3})))$ $(\cos(\arctan(\frac{8}{3}) + \pi), \sin(\arctan(\frac{8}{3}))$ and now I need to find $f_{xx}, f_{yy}, f{xy}$ in order to know which point is minimum and which is maximum: $f_{x}=8^x3ln(2)$ $f_y = 256^y 8ln(2)$ $f_{xx} = 9\ln^2(2) 8^x$ $f_{yy} = 64ln^2(2) 256^y$ $f_{xy} = 0$ $f_{yx} = 0$ . for first point: $f_{xx} = 8.97$ , $f_{yy} = 5530.131$ .  So $f_{xx}*f_{yy} - 0 > 0$ , and $f_{xx} > 0 $ , which means it is a local minimum point. for second point: $f_{xx} = 2.03$ , $f_{yy} = 0.17$ . and same as above, it is a minimum point. Would appreciate any help in approving this solution or finding mistakes in it, and if there's any more efficient approaches, thanks in advance. EDIT: Approach from Parcly Taxel answer: Checking maximum and minimum for $3x+8y$ : $\phi (x,y,\lambda) = 3x+8y + \lambda(x^2+y^2-1)$ $\phi_x = 3 + 2\lambda x = 0 \Longrightarrow x= \frac{-3}{2\lambda}$ $\phi_y = 8 + 2 \lambda y = 0 \Longrightarrow y= \frac{-4}{\lambda}$ $\phi_{\lambda} = x^2 +y^2 -1 = 0 \Longrightarrow \frac{9}{4\lambda^2} + \frac{16}{\lambda^2}-1=0 \Longrightarrow 9+64=4\lambda^2 \Longrightarrow \lambda^2 = \frac{73}{4} \Longrightarrow \lambda = \pm \frac{\sqrt{73}}{2}$ So: $x= \pm 0.3511 $ $y = \pm 0.9363$ $(0.3511, 0.9363) , (-0.3511, -0.9364)$","['maxima-minima', 'multivariable-calculus', 'solution-verification']"
4153996,Proof that $|\mathscr{F}_1 \cap \mathscr{F}_2| \geq \frac{|\mathscr{F}_1| \cdot |\mathscr{F}_2|}{2^n}$,"A subsets family $\mathscr{F}$ of set $\{1, 2, \ldots, n\}$ is called hereditary set if for any $A \in \mathscr{F}$ and any $B \subset A,$ $ \ B \in \mathscr{F}$ . Proof that $|\mathscr{F}_1 \cap \mathscr{F}_2| \geq \frac{|\mathscr{F}_1| \cdot |\mathscr{F}_2|}{2^n}$ holds if $\mathscr{F}_1$ and $\mathscr{F}_2$ are hereditary sets of $\{1, 2, \ldots, n\}$ . It is possible to solve this problem with pure math induction, but I don't have any idea how to do it. Could somebody give me a hint?","['general-topology', 'proof-writing', 'discrete-mathematics', 'hypergraphs']"
4153999,Spectral Sequences from Derived Categories,"I'm trying to understand how derived categories ""replace"" spectral sequences. More specifically the derived category statement of Grothendieck Spectral Sequence vs the normal version. I been reading mostly in Weibel as well as this mathoverflow post https://mathoverflow.net/questions/91785/derived-functors-versus-spectral-sequences . As far as I understand the picture is the following. When first doing Grothendieck spectral sequence one proofs it by using the theory of hyper(co)homology and the associated spectral sequences we get from Cartan-Eilenberg resolutions. This version I feel is understandable. Here comes the part which I don't really understand. The Grothendieck spectral sequence in the derived category setting is ""just"" that there is an natural isomorphism $$ \textbf{R}F \circ \textbf{R}G \cong \textbf{R}(F \circ G)$$ which seems to be fine. What I dont understand is how we can recover the spectral sequence from this statement? Is there some naturally associated bicomplex? I know that if we take the homology of the derived functors then we get the hyperderived functors, but as said I don't understand how one gets a spectral sequence. Thank you very much for any help! :)","['derived-categories', 'spectral-sequences', 'algebraic-geometry', 'derived-functors']"
4154025,Textbook reference for ergodicity condition for stationary sequences?,"In a set of lecture notes, I have the following result: Theorem . Let $X_n$ be random variables on $(\Omega, \mathcal{F}, \mathbb{P})$ with values in a Polish metric space $S$ . Suppose $X = (X_n)_{n \geq 1}$ is a stationary sequence. Then $X$ is ergodic if and only if for any bounded Borel measurable function $g: S^p \to \mathbb{R}$ with $p \geq 1$ an arbitrary integer, $$\dfrac{1}{n}\sum_{m=0}^{n-1}g(X_{m+1}, \dots, X_{m+p}) \overset{a.s.}{\to} \mathbb{E}[g(X_1, \dots, X_p)]\text{.}$$ Note that $\overset{a.s.}{\to}$ denotes almost sure convergence as $n \to \infty$ . I have been trying to find this result in the 20-30 measure-theoretic probability books I have to no avail, as well as An Introduction to Ergodic Theory by Walters. Does anyone know of a textbook where I can find this result? I would strongly prefer a reference with a proof, but would be willing to take those without as well. Edit : Adding definitions as requested. Given $X$ above, it is ergodic if for any invariant set $A \in \mathcal{F}$ , $\mathbb{P}(A) \in \{0, 1\}$ . By ""invariant set,"" we say a set $A \in \mathcal{F}$ is invariant with respect to $X$ if for some $B \in \mathcal{B}(\mathbb{R}^{\infty})$ ( $\mathcal{B}(\mathbb{R}^{\infty})$ denoting the Borel $\sigma$ -algebra generated by $\mathbb{R}^{\infty}$ ), $A = \{(X_n, X_{n+1}, X_{n+2}, \dots)\} \in B$ for all $n \geq 1$ . [I suspect that $S^{\infty}$ should be used in place of $\mathbb{R}^{\infty}$ in the above definitions and that $\in$ should be $\subset$ , but that's how they are presented in the lecture notes.] Edit 2 : I found this claim in some other sources, though not in great detail. It would be nice to find a textbook. Last sentence of http://www.columbia.edu/~ks20/6712-14/6712-14-Notes-Ergodic.pdf Appendix A of GARCH Models: Structure, Statistical Inference and Financial Applications uses the theorem above as the definition of an ergodic stationary process. This passage cites Billingsley (1995), which I assume is Probability and Measure - but I know that this theorem is not in there.","['stochastic-processes', 'ergodic-theory', 'probability-theory', 'reference-request']"
4154082,How can I guess a function just by looking at the graph of it?,"I have been asked to guess function of this graph.
Can anyone please tell me how can I say this is a graph of an exponential function ?
Is there any other function which looks like this ?","['calculus', 'functions', 'exponential-function', 'graphing-functions']"
4154106,"Find range of $f(x)=3\cos^4x-6\cos^3x-6\cos^2x-3$ in the interval $[-π/2, π/2]$","While solving some questions related to functions, I came across this question and I am unable to find  its range. $$f(x)=3\cos^4x-6\cos^3x-6\cos^2x-3$$ in the interval $[-π/2, π/2]$ I tried graphing this function on Desmos and got this result. However, since I can't use a graphing tool in exams, I need to be able to solve this algebraically. When I tried factorising the function, I could only get this far: $$f(x)=3(\cos x+1)(\cos^3x-3\cos^2x+\cos x-1)$$ Can anyone help how to solve this further?",['functions']
4154111,Why use exponentials in the Markov inequality?,"The Markov inequality gives an upper bound to the probability of a non-negative rv: $$P(X\geq a) \leq \frac{E(X)}{a}$$ in terms of the mean of the random variable. But the next, rather unmotivated step in this topic (distribution bounds) is to say... Well, since the exponential is a monotonically increasing function, we can just instead do this... $$P(X\geq a)=P(e^{tX}\geq e^{ta}) \leq \frac{E(e^{tX})}{e^{ta}}$$ the numerator on the RHS is the moment generating function of the variable. And the last inequality is the Chernoff bound. So I guess we can say that as long as there is an MGF for $X$ , there is a bound. That sounds remotely useful. But perhaps the intention is to get some expression where the probability decays exponentially to conclude that asymptotic values are rare, motivating the introduction of the exponents in the MGF. But I don't know the reason... What is the motivation to all of a sudden do the switch from $X$ to the MGF?","['probability-distributions', 'probability-theory', 'upper-lower-bounds']"
4154150,"Proper use of ""without loss of generality""","I'm trying to understand exactly when I can assume something ""without loss of generality in a proof."" The technical explanation, I believe, is that it's allowed provided that the case that I'm omitted ""reduces"" to the one I prove, either through interchanging of labels or through a rather trivial extension. I can really only understand this through examples, though. The example I have in mind, where I'm not totally sure I can use it, is as follows. Suppose I have countable (as in, finite or countably infinite) sets $X_1, \ldots, $ and want to show that $\bigcup\limits_{i \in \mathbb{N}} X_i$ is countable. I want to say ""without loss of generality, suppose $X_i \neq \emptyset$ for all $i$ ,"" the justification being that if I take $I = \{j \in \mathbb{N} \mid X_j = \emptyset\}$ , then I have $$ 
\bigcup\limits_{i \in \mathbb{N}} X_i = \bigcup\limits_{i \in \mathbb{N}} X_i \setminus \bigcup\limits_{i \in I} X_i,
$$ i.e., they contribute nothing at all to the union, so having in the union is somewhat ""harmless."" I don't know if I'm sacrificing generality by making this assumption, though. I would appreciate any help on understanding this.","['elementary-set-theory', 'proof-writing', 'terminology']"
4154188,"Conditional expectation $E[f(X,Y,Z)|Y,Z]$ with $X,Y,Z$ independent","Am trying to prove the following generalization of this result. Proposition. Let $(\Omega,\mathcal F,P)$ be a probability space, and let $X,Y,Z$ be independent random variables taking values in the measurable spaces $(\Omega_X,\mathcal F_X)$ , $(\Omega_Y,\mathcal F_Y)$ , $(\Omega_Z,\mathcal F_Z)$ respectively. Let $f:(\Omega_X \times\Omega_Y \times\Omega_Z,\mathcal F_X\otimes\mathcal F_Y\otimes\mathcal F_Z)\to\mathbb R$ be measurable. If $f(X,Y,Z)$ is $P$ -integrable, then $$E[f(X,Y,Z)|Y,Z]=E[f(X,y,z)]\circ (Y,Z) \quad P\text{-almost surely}.$$ Proof. Since $X,Y,Z$ are independent, the distribution of $(X,Y,Z)$ on $\Omega_X \times\Omega_Y \times\Omega_Z$ is $P_X\otimes P_Y\otimes P_Z$ ,where $P_X,P_Y,P_Z$ denote the distributions of $X,Y,Z$ on $\Omega_X,\Omega_Y,\Omega_Z$ respectively. From the $P$ -integrability $f(X,Y,Z)$ and the transformation theorem for image measures we get the $P_X\otimes P_Y\otimes P_Z$ -integrability of $f(x,y,z)$ . Therefore Fubini's theorem implies $$x\mapsto f(x,y,z) \quad P_X\text{-integrable for }  (P_Y\otimes P_Z) \text{-almost every } (y,z),  $$ $$(y,z)\mapsto \int f(x,y,z) dP_X \quad (P_Y\otimes P_Z)\text{-integrable},  $$ $$\int f(x,y,z) d(P_X\otimes P_Y\otimes P_Z)=\int \bigg[\int f(x,y,z)dP_X\bigg] d(P_Y\otimes P_Z).$$ In particular $(y,z)\mapsto E[f(X,y,z)]$ is a measurable map defined for $(P_Y\otimes P_Z)$ -almost every $(y,z)$ . Let $A_Y\in\mathcal F_Y$ and $A_Z\in\mathcal F_Z$ . Then another application of Fubini's theorem gives $$E\big[f(X,Y,Z)1_{Y\in A_Y}1_{Z\in A_Z}\big]=\int f(x,y,z)1_{A_Y}(y)1_{A_Z}(z) d(P_X\otimes P_Y\otimes P_Z)$$ $$=\int \bigg[\int f(x,y,z)1_{A_Y}(y)1_{A_Z}(z) dP_X \bigg] d(P_Y\otimes P_Z)$$ $$=\int 1_{A_Y}(y)1_{A_Z}(z) \bigg[\int f(x,y,z) dP_X \bigg] d(P_Y\otimes P_Z)$$ $$=\int 1_{A_Y}(y)1_{A_Z}(z) E[ f(X,y,z)] d(P_Y\otimes P_Z)$$ $$=E\bigg[1_{Y\in A_Y}1_{Z\in A_Z} E[ f(X,y,z)]\circ (Y,Z)\bigg]$$ Since the collection all sets $\{Y\in A_Y\}\cap\{Z\in A_Z\}$ with $A_Y\in\mathcal F_Y$ and $A_Z\in\mathcal F_Z$ is $\cap$ -stable and generates $\sigma(Y,Z)$ , an application of the Dynkin's $\pi-\lambda$ theorem shows that in fact $$E\big[f(X,Y,Z)1_A\big]=E\bigg[1_{A} E[ f(X,y,z)]\circ (Y,Z)\bigg]$$ holds for all $A\in \sigma(Y,Z)$ . Is this correct? Thanks a lot for your help.","['measure-theory', 'independence', 'conditional-expectation', 'fubini-tonelli-theorems', 'probability-theory']"
4154213,Geometrical proof that $b\cos\beta+c\cos\gamma=a\cos(\beta-\gamma)$,"Prove that for triangle $ABC$ with side lengths $a,b,c$ and corresponding angles $\alpha,\beta,\gamma$ \begin{align}b\cos\beta+c\cos\gamma&=a\cos(\beta-\gamma)\tag{1}\label{1} \end{align} It is straightforward to prove \eqref{1}
by expanding $\cos(\beta-\gamma)$ and expressing $\cos\beta,\sin\beta,\cos\gamma,\sin\gamma$ in terms of $a,b,c$ using the cosine rule.
Both sides of equation \eqref{1} are equal to \eqref{2}: \begin{align}
\frac{a^2(b^2+c^2)-(b^2-c^2)^2}{2abc}
\tag{2}\label{2}
.
\end{align} The question is: is there any geometrical proof for \eqref{1}? One possible geometric construction is shown below.","['trigonometry', 'circles', 'triangles']"
4154221,$f'(x)=-\frac{f(x)}{\sqrt{(f(x))^2+(g(x))^2}}$ and $g'(x)=1-\frac{g(x)}{\sqrt{(f(x))^2+(g(x))^2}}$ then evaluate $\lim_{x\to\infty}g(x)=$,"Let $$f'(x)=-\frac{f(x)}{\sqrt{(f(x))^2+(g(x))^2}}\forall x\in R$$ and $$g'(x)=1-\frac{g(x)}{\sqrt{(f(x))^2+(g(x))^2}}\forall x\in R$$ Also, $g(0)=0,f(0)=10$ and $\lim_{x\to\infty}f(x)=0$ , then find value of $\lim_{x\to\infty}g(x)$ My Attempt: I could do till here only $(f'(x))^2+(g'(x)-1)^2=1$","['limits', 'calculus', 'derivatives']"
4154236,Hyperbolic triangles getting thinner,"Let $\Delta$ be a right triangle in $\mathbb{H}^2$ , and let us write $l$ and $d$ for the length of the two legs $L$ and $D$ . Let $p$ be the point on the hypotenuse that projects to the middle point $q$ of $L$ , and let us write $l'$ for the distance $d(p,q)$ . I can see intuitively that if we keep $D$ fixed and let $L$ go to infinity, then $l'$ tends to zero (contrary to the euclidean case where it stays constant), but I don't manage to prove it with some trigonometric argument. Is there any easy way to prove it , without passing through the coordinates?","['trigonometry', 'hyperbolic-geometry', 'geometry']"
4154256,Sphere is a symplectic submanifold of Lie algebra $S0(3)^*$,"I'm working through this example in Peter Olver's textbook Application of Lie Groups to Differential equations and I am having some trouble and was wondering if somebody could point out where I went wrong. So in the first highlighted section, I am trying to understand why this coincides with the tangent space of the sphere. I know that the Hamiltonian vectors are not linearly independent but atleast of them are so they span a $2D$ subspace in the tangent space, however, when we look at the tangent space to the sphere using spherical coordinates I get that it is spanned by $$\frac{d}{d\theta} = -p\sin\theta\sin\phi \frac{d}{du^1} + p \cos\theta\sin\phi \frac{d}{du^2} = -u^2 \frac{d}{du^1} + u^1 \frac{d}{du^2}$$ $$\frac{d}{d\phi} = p\cos\theta\cos\phi \frac{d}{du^1} + p \sin\theta\cos\phi \frac{d}{du^2} - p\sin\phi \frac{d}{du^3}$$ Now the first one coincides with one of the Hamiltonian vectors however the second one I cannot seem to simplify or rewrite so that it looks like any of the other hamiltonian vectors or a linear combination of them. As for the second highlited piece I wrote $ \phi = \arccos(\frac{u^3}{p})$ and $ \theta = \arctan(\frac{u^2}{u^1})$ by solving the spherical coordinates formulas and got the following by computing the gradient with respect to the coordinates $u = (u^1,u^2,u^3)$ $$\nabla_u(\theta) = (\frac{-u^2}{(u^1)^2 +(u^2)^2}, \frac{u^1}{(u^1)^2+(u^2)^2}, 0)$$ $$ \nabla_u(\phi) = (0,0,\frac{-1}{\sqrt{(u^1)^2+ (u^2)^2}}) $$ $$\nabla_u(\theta) \times \nabla_u(\phi) = (\frac{-u^1}{((u^1)^2 + (u^2)^2)^{3/2}}, \frac{-u^2}{((u^1)^2 + (u^2)^2)^{3/2}}, 0)$$ $$ u \cdot (\nabla_u(\theta) \times \nabla_u(\phi)) = (u^1,u^2,u^3) \cdot (\frac{-u^1}{((u^1)^2 + (u^2)^2)^{3/2}}, \frac{-u^2}{((u^1)^2 + (u^2)^2)^{3/2}}, 0) = \frac{-1}{\sqrt{(u^1)^2+(u^2)^2}}$$ Now changing that back into spherical coordinates I get $$   u \cdot (\nabla_u(\theta) \times \nabla_u(\phi)) = \frac{-1}{p\sin\phi}$$ $$ \implies - u \cdot (\nabla_u(\theta) \times \nabla_u(\phi)) = \frac{1}{p\sin\phi} $$ So I am off by a negative sign, I've done the calculations over and I still get the same thing I am not sure where I am going wrong unless this whole process of doing the gradient was wrong.","['symplectic-geometry', 'hamilton-equations', 'symplectic-linear-algebra', 'poisson-geometry', 'differential-geometry']"
4154292,"Find $\int_{0}^{\infty} \frac{\sin x}{x+x\cos^2 x }\,\mathrm{d}x$","I have just done these, but I don't know what to do next...... \begin{align}
\int \frac{\sin x}{x+x\cos^2 x}\,\mathrm{d}x & = \int \frac{1}{x}\cdot\frac{\sin  x}{1+\cos^2 x}\,\mathrm{d}x\\
&=\int \frac{1}{x}\,\mathrm{d}(\arctan(-\cos x))\\\
&=\frac{1}{x} \arctan(-\cos x)-\int -\frac{1}{x^2} \arctan(-\cos x)\,\mathrm{d}x \\
&=\frac{1}{x} \arctan(-\cos x)-\int \frac{\arctan(\cos x)}{x^2}\,\mathrm{d}x
\end{align}","['integration', 'calculus', 'definite-integrals']"
4154363,Positive semi-definite conditional covariance matrix,"Let $X$ be an $n\times m$ random matrix, where each entry is a real square integrable random variable on the probability space $(\Omega,\mathcal A,P)$ . Consider the following matrix: $$E[XX'\mid\mathcal F],$$ where $\mathcal F$ is a sub- $\sigma$ algebra of $\mathcal A$ . If $a\in\mathbb{R}^{n}$ , then from the linearity of conditional expectations we have $$a'E[XX'\mid\mathcal F]a=E[(a'X)^2\mid\mathcal F]\geq 0 \quad P\text{-almost surely,}$$ but the null set might depend on $a$ . Can I find a version of $E[XX'\mid\mathcal F]$ which is positive semi-definite almost surely?","['matrices', 'measure-theory', 'probability-theory', 'conditional-expectation']"
4154397,How do you express that a set has more elements than another,"I wanted to know if you can express that a set has more elements than another set, because I don't know if A > B would be correct Also, is this a correct way to express myself mathematically? R ∈ [0,1] > N ∈ [0,∞] I am really bad with math notations since I mostly just like to understand the concepts, but expressing ideas with symbols seems more efficient, so if anyone would care to help that would be appreciated!","['elementary-set-theory', 'notation']"
4154419,"$ f(x)=\begin{cases} 0 & \text{if $x$ is irrat. } \\ \sin |x| &\text{if $x$ is rat. }\end{cases} $, Show $ \lim_{x \to x_0 }f(x) =0 $ by Heine def.","Problem : $ f(x)=\begin{cases} 0  & \text{if $x$ is irrational}
\\ \sin |x|  &\text{if $x$ is rational}\end{cases} $ Let $ x_0 \in \{ \pi n : n \in \Bbb Z \} $ . Show that $ \lim_{x \to x_0 }f(x) $ exists or not, if it exists, find it. Attempt: Let $ \epsilon > 0$ be arbitrary.  Since we know that $ \sin |x|  $ is continuous then there exists $ \delta >0 $ . Let $ x \in \Bbb R $ be arbitrary. Suppose $ | x- x_0 | < \delta $ . Then we know $ | \sin|x| | <\epsilon $ ( since $ \lim_{x \to 0 } \sin |x| = 0 $ ). Now, If x is irrational, $ f(x) = 0  $ and so $ |f(x)| = 0 < \epsilon $ . If x is rational, $f(x) = \sin|x| $ and so $ |f(x)| = |\sin |x| | < \epsilon $ . Since $ \epsilon>0, x \in \Bbb R  $ were arbitrary, we showed that $ \lim_{x \to x_0 }f(x) = 0 $ . $ \square $ Question: Initially I tried to solve the problem using Heine's definition of limit but I found it to be very difficult. How would one prove the limit above using Heine's definition?","['limits', 'real-analysis']"
