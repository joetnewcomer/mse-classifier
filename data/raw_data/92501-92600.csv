question_id,title,body,tags
1258034,A trick to find a solution for : $ydx + (x+x^2y^4)dy = 0$,"In the first part of my question I already proved that if $P(x,y)dx + Q(x,y)dy = 0$ is an exact equation and its solution is $F(x,y)=c$, then for each differentiable function $\mu(t)$ we get that the following equation is also exact: $$\mu(F(x,y))P(x,y)dx+\mu(F(x,y))Q(x,y)dy = 0 $$ Then, second part says: Let $P(x,y)dx+Q(x,y)dy=0$ be an exact equation, and let $F(x,y)=c$ be its solution, which applies $\nabla F = (P,Q)$. now lets take a  look at: 
$$Pdx + (Q+Q_1)dy=0$$ which is not exact, and we suggest to find an integration factor of the form $\mu = \mu(F(x,y))$. Use this way to find a solution for:
$$ydx + (x+x^2y^4)dy = 0$$ so, I thought that what we really need to deal with is: 
$$\mu(F(x,y))P(x,y)dx+\mu(F(x,y))(Q_1 + Q)dy = 0 $$ And I marked $\widetilde{Q}(x,y)= \mu(F(x,y))(Q_1 + Q) $ And also marked: $\widetilde{P}(x,y)= \mu(F(x,y))P(x,y) $ and then I wanted to prove that the follow partial derivatives are equal: $$\widetilde{P_y}^\prime(x,y)= \widetilde{Q_x}^\prime(x,y)$$ then, here comes the uncertain part of my solution, I derived the upper equation in this way: $\frac{du}{dt}\frac{dF}{dy}P(x,y) +  P_y^\prime$$\mu(F(x,y))$ = $\frac{dy}{dt}\frac{dF}{dx}(Q_1+Q) + (Q_1+Q)_x ^\prime\mu(F(x,y)) $ and I all could get from here by some calculations is: $P_y^\prime$ = $ (Q_1)_{x} ^\prime + Q_x^\prime$ $\widetilde{F}_y ^\prime$$P(x,y)$= $\widetilde{F}_x ^\prime$$(Q_1 + Q)$ And what I mean by $\widetilde{F}_y ^\prime$ and $\widetilde{F}_x ^\prime$ is the solution of the $new$ equation. for somehow, I got stuck here, how do I continue ? and if there are mistakes what are they? Any kind of help would be appreciated.","['partial-derivative', 'ordinary-differential-equations']"
1258079,Proof of equilateral triangle given angles,"Let's say we start with a scalene triangle ABC, with no given angle measures or side lengths: Then, we add 3 Isosceles triangles adjacent to this one, given that they have angle measures 30*30*120, but no side lengths: The question is, how would one go about proving that triangle Alpha-Beta-Gamma is equilateral?: I've been working on this for quite some time now, but I can't seem to figure it out.","['triangles', 'trigonometry']"
1258081,What is the relationship between dimension of eigen space and multiplity of eigen value?,Is there a relationship between dimension of eigen space with respect to an eigen value $\lambda_i$ and multiplicity of eigen value $\lambda_i$ ( by multiplicity I mean if $(\lambda-2 )^3(\lambda -1)^2 = 0$ is the eigen value equation then multiplicity for eigen value 2 is 3 and for 1 is 2 )  for a general square matrix ?,"['linear-algebra', 'matrices']"
1258086,Find solution of problem - Method of characteristics,"I want to find the solution of the problem: 
$$(t+u(x,t))u_x(x,t)+tu_t(x,t)=x-t, x \in \mathbb{R}, t>1 \\ u(x,1)=1+x, x\in \mathbb{R}$$ I have tried the following: $$(x(0), t(0))=(x_0, 1)$$ We will find a curve $(x(s), t(s)), s \in \mathbb{R}$ such that $\sigma (s)=u(x(s),t(s))$ $\sigma'(s)=\frac{d}{ds}(u(x(s), t(s)))=u_x(x(s), t(s))x'(s)+u_t(x(s), t(s))t'(s)$ We choose $x'(s)=t(s)+u(x(s), t(s))=t(s)+\sigma (s), s \in \mathbb{R}, x(0)=x_0 \\ t'(s)=t(s), s \in \mathbb{R}, t(0)=1$ Then $\sigma'(s)=(t(s)+u(x(s), t(s)))u_x(x(s), t(s))+t(s)u_t(x(s), t(s))=x(s)-t(s), s \in \mathbb{R} \\ \sigma (0)=u(x(0), t(0))=u(x_0, 1)=1+x_0$ $$t'(s)=t(s) \Rightarrow t=ce^s \\ t(0)=1 \Rightarrow c=1 \\ \text{ So, } t(s)=e^s$$ So we get $$x'(s)=t(s)+\sigma(s)=e^s +\sigma (s) \\ \sigma'(s)=x(s)-t(s) \Rightarrow \sigma'(s)=x(s)-e^s$$ $$\sigma'(s)=x(s)-e^s \Rightarrow \sigma''(s)=x'(s)-e^s=e^s+\sigma (s)-e^s \Rightarrow \sigma''(s)=\sigma (s) \Rightarrow \sigma (s)=c_1e^s+c_2 e^{-s} \\ \sigma (0)=1+x_0 \Rightarrow c_1+c_2=1+x_0 \Rightarrow c_1=1+x_0-c_2 \\ \Rightarrow \sigma (s)=(1+x_0-c_2)e^s+c_2e^{-s}$$ $$\Rightarrow x'(s)=e^s+(1+x_0-c_2)e^s+c_2e^{-s}=(2+x_0-c_2)e^s+c_2e^{-s} \\ \Rightarrow x(s)=(2+x_0-c_2)e^s-c_2e^{-s}+c_3 \\ \Rightarrow x(0)=x_0 \Rightarrow 2+x_0-c_2-c_2+c_3=x_0 \Rightarrow -2c_2+c_3=-2 \Rightarrow c_3=-2+2c_2 \\ \Rightarrow x(s)=(2+x_0-c_2)e^s-c_2e^{-s}-2+2c_2$$ If $\overline{s}$ is the value of $s$, such that $(x(\overline{s}), t(\overline{s}))=(x_1, t_1)$, then we have $$\left.\begin{matrix}
(2+x_0-c_2)e^{\overline{s}}-c_2e^{-\overline{s}}-2+2c_2=x_1  \\ 
e^{\overline{s}}=t_1
\end{matrix}\right\} \Rightarrow \left.\begin{matrix}
(2+x_0-c_2)t_1-c_2\frac{1}{t_1}-2+2c_2=x_1  \\ 
e^{\overline{s}}=t_1
\end{matrix}\right\} \Rightarrow \left.\begin{matrix}
t_1+(1+x_0-c_2)t_1-c_2\frac{1}{t_1}-2+2c_2=x_1  \\ 
e^{\overline{s}}=t_1
\end{matrix}\right\} \Rightarrow \left.\begin{matrix}
(1+x_0-c_2)t_1=x_1-t_1+\frac{c_2}{t_1}+2-2c_2  \\ 
e^{\overline{s}}=t_1
\end{matrix}\right\}$$ So for $s=\overline{s}$ we have $$\sigma (\overline{s})=u(x(\overline{s}),t(\overline{s}))=u(x_1, t_1)=(1+x_0-c_2)e^{\overline{s}}+c_2e^{-\overline{s}}=(1+x_0-c_2)t_1+\frac{c_2}{t_1}=x_1-t_1+\frac{c_2}{t_1}+2-2c_2+\frac{c_2}{t_1}=x_1-t_1+2\frac{c_2}{t_1}+2-2c_2$$ So the solution is $$u(x,t)=x-t+2\frac{c_2}{t}+2-2c_2$$ Is it right? Can the solution have the constant $c_2$ ?","['ordinary-differential-equations', 'partial-differential-equations']"
1258101,How could a group be a manifold?,"For example a Lie group is defined as a certain differentiable manifold, but what does this mean geometrically, and what is gained by viewing something abstract and algebraic as a manifold? First, I know there are severel quite abstract definitions of a manifold, but what I know from my analysis courses, a manifold is something that could be defined by equations (i.e. something like $f^{-1}(0)$ for a regular function) or for example as defined in Munkres: Analysis on Manifolds , p. 109: A subseteq M of $\mathbb R^n$ is called a $k$-dimensional manifold (in $\mathbb R^n)$ if for every point $x \in M$ the following condition is satisfied: (M) There is an open set $U$ containing $x$, an open set $V \subseteq \mathbb R^n$, and a diffeomorphism $h : U \to V$ such that
  $$
 h(U\cap M) = V \cap (\mathbb R^k \times \{0\}) 
  = \{ y \in V : y^{k+1} = \ldots = y^n = 0 \}.
$$ So a manifold is something concrete, something that I can think of sitting in $\mathbb R^n$ (I know vaguely there are some intrinsic definitions saying something like a set $X$ is manifold if it has a topology and to each point there exists a diffeomorphism on $\mathbb R^n$). So I am used to think of a manifold as a geomtric object, and in some sense this are the explanations I find everywhere, but in what sense could a group be something ""geometrically concrete"", for example $SL(n, K)$ is also a manifold (this could be seen by noting that it is the inverse image $\det^{-1}(1)$), but again what does this mean geometrically and what is gained by seeing for example $SL(n,K)$ as a manifold?","['lie-groups', 'group-theory', 'manifolds', 'analysis', 'differential-geometry']"
1258178,Recursive and Primitive recursive functions,"According to the book that I'm reading, we can define the $\mu-$recursive functions inductively, as follows: The constant, projection, and successor functions are all $\mu-$recursive. If $g_1, \dots , g_m$ are $n-$variable $\mu-$recursive functions and $h$ is an $m-$variable $\mu-$recursive function, then the composite function $f=h \circ (g_1, \dots , g_m)$ is also $\mu-$recursive. If $g$ and $h$ are $n-$ and $(n+2)-$variable $\mu-$recursive functions, then the function $f$ defined from $g$ and $h$ by primitive recursion is also $\mu-$recursive. If $g$ is a total $(n+1)-$variable $\mu-$recursive function, then the function $f$ defined from $g$ by unbounded minimalization is also $\mu-$recursive. The definition of a primitive recursive function is the following: The constant, projection, and successor functions are all primitive recursive functions. If $g_1, \dots , g_m$ are $n-$variable primitive recursive functions, and if $h$ is an $m-$variable primitve recursive function, then the composite function $h \circ (g_1, \dots , g_m)$ is also a primitive recursive function. If $g$ and $h$ are $n-$ and $(n+1)-$variable primitive recursive functions, then $(n+1)-$variable function $f$ defined from $g$ and $h$ by primitive recursion is also a primitive recursive function. The difference between these two definitions is the minimalization, or not?? The minimalzation is the following: $$f(\overline{x})= \left\{\begin{matrix} 
\mu z(g(z,\overline{x})=0)=\min \{z \in \mathbb{N}_0 \mid g(z, \overline{x})=0\} & \text{ if } ( \exists z) (g(z,\overline{x})=0)\\ 
\text{ undefiniert } & \text{ otherwise } 
\end{matrix}\right.$$ All primitve recursive functions are $\mu-$recursive, but the inverse doesn't hold, right?? Does this statement stand because of the minimalization??","['computability', 'discrete-mathematics', 'recursion']"
1258182,Number of pairs of females and males of different groups,"There are n females, which can have m different types. There are n males, which can have m different types. Knowing how many females are of which type and how many males are of which type, how many different pairings are there? As an urn-model: In the first urn are n balls with m different colours. In the second urn are n rectangles with m colours. In each round one ball from the left urn is drawn and one rectangle from the right urn. The colour combination is written down. e.g (red, blue). Afterwards the rectangle and the ball are removed from the urns. How many different outcomes are possible after all rectangles and balls are removed from the urns. The order of the pairs doesnt matter: e.g. (red, red), (blue, blue) is the same as (blue, blue) (red, red)","['discrete-mathematics', 'combinatorics']"
1258219,formula for the $n$th derivative of $e^{-1/x^2}$,"$f(x) = \begin{cases} e^{-1/x^2} & \text{ if } x \ne 0 \\ 0 & \text{ if } x = 0 \end{cases}$ so $\displaystyle f'(0) = \lim_{x \to 0} \frac{f(x) - f(0)}{x - 0} = \lim_{x \to 0} \frac {e^{-1/x^2}}x = \lim_{x \to 0} \frac {1/x}{e^{1/x^2}} = \lim_{x \to 0} \frac x {2e^{1/x^2}} = 0$ (using l'Hospital's Rule and simplifying in the penultimate step). Similarly, we can use the definition of the derivative and l'Hospital's Rule to show that $f''(0) = 0, f^{(3)}(0) = 0, \ldots, f^{(n)}(0) = 0$ , so that the Maclaurin series for $f$ consists entirely of zero terms. But since $f(x) \ne 0$ except for $x = 0$ , we can see that $f$ cannot equal its Maclaurin series except at $x = 0$ . This is part of a proof question. I don't think the answer sufficiently proves that any $n$ th derivative of $f(x)$ is $0$ . Would anyone please expand on the answer? ps: I promise this is not my homework :)",['calculus']
1258260,Degree of minimum polynomial at most $n$ without Cayley-Hamilton?,"Let $T$ be a linear transformation of an $n$-dimensional vector space $V$ over a field $k$. It's pretty easy to define the minimum polynomial of $T$ and make sure its degree is between $1$ and $n^2$, inclusive. Observe $ I = \{ p(x) \in k[x] : p(T) =0\}$ is an ideal in $k[x]$. Indeed, $I$ is the kernel of the evaluation homomorphism $\mathrm{eval}_T: k[x] \to \mathrm{End}(V)$.  Notice also that: $\mathrm{eval}_T$ is unital homomorphism, so $I$ is a proper ideal. The $n^2 + 1$ transformations $I, T,T^2,T^3,\ldots, T^{n^2}$ must be linearly dependent, since $\mathrm{dim}(\mathrm{End}(V)) = n^2$, so there exist scalars $a_0,\ldots,a_{n^2}$, not all zero, such that $a_0I + a_1 T + \ldots + a_{n^2}T^{n^2} = 0$, whence the nonzero polynomial $p(x) = a_0 + a_1 x + \ldots + a_{n^2}x^{n^2}$ belongs to $I$. Since $k[x]$ is a p.i.d., we may define the minimum polynomial $m(x)$ of $T$ to be the monic generator of the ideal $I$. By the preceding two observations, we have $1 \leq \mathrm{deg}(m(x)) \leq n^2$. Now, of course, we know that the degree of $m(x)$ actually satisfies $1 \leq \mathrm{deg}(m(x)) \leq n$. One way to see this is to use the Cayley-Hamilton theorem which shows that the characteristic polynomial $c(x)=\det(xI - T)$, whose degree is $n$, annihilates $T$, whence $m(x)$ divides $c(x)$. Question: Is there another way to see that $T$ is annihilated by a polynomial of degree $\leq n$ which does not require use of the characteristic polynomial?","['matrices', 'determinant', 'abstract-algebra', 'polynomials', 'linear-algebra']"
1258289,Zeroes of sin(x),"Consider the function f = $\sin(x)$ defined as $$ \sin(x) = \frac{e^{ix}- e^{-ix}}{2i} $$ How to prove that the only zeroes of this function lie on the line $i = 0$ in the complex plane and furthermore that those zeroes are of the form $\pi k$ for $k \in \Bbb{Z}$. To begin the first step I note that we really just interested in the zeroes of $$  e^{ix}- e^{-ix} $$ Furthemore consider a zero x such that $i \ne 0 $ we can declare $x = a + bi$ and therefore $$ e^{ai - b} - e^{b - ai} = 0 $$ From here it follows that 
$$ ai - b = b - ai$$ We note that that the only circumstance this can happen is if $b = ai$ and since b,a are real number it then MUST be the case that $b = 0$ resulting in a contradiction. For the second phase I note that $$ e^{ix}  = e^{-ix} $$ Obviously we desire $e^{ix} = \pm 1 $. Given by definition that $e^{i\pi} = -1$ and that integer powers of $-1$ are also in the set $1,-1$ it then follows that each integer times pi is a zero. But how do I know that integer multiples of $pi$ are the ONLY values for which $e^{ix} = \pm 1$ ?","['roots', 'complex-numbers', 'exponential-function', 'trigonometry', 'complex-analysis']"
1258305,Characterization of normal distribution,"I am sorry if this question is vague since I am completely unfamiliar with probability theory. Suppose that we have a family of real-valued random variables $X_n$ (say, all of them have mean 0) on some probability space and we would like to show that $X_n$ converges to Gaussian weakly. What are the standard/general techniques of showing such convergence to Gaussian
  distribution? I am aware of the following two: Moments. Check that $E[X_n^k]$ converges to the $k$-th moment of Gaussian for all $k \in \mathbb{N}$. Work with characteristic functions instead. This seems to be the method to prove for example, the classical central limit theorem. Stein's method also seems to be common in practice these days, although I am not sure if this is a real workhorse in probability theory. and I just want to know if this is how people usually approach this in probability theory, or there are some other general approaches as well. Along the same lines, I also want to know about the techniques of showing convergence of complex random variables to the complex Gaussian. This can probably be reduced to the real case by separating the real/complex part and checking their covariance, but I would be curious to know if there is some uniform way of viewing this as well. Thank you!","['probability-theory', 'normal-distribution']"
1258335,”lesser known” rules to calculate the derivative,"I was reading through the online help of WolframAlpha ( link ) and found this statement: Wolfram|Alpha calls Mathematica's $D$ function, which uses a table of
  identities much larger than one would find in a standard calculus
  textbook. It uses ”well known” rules such as the linearity of the
  derivative, product rule, power rule, chain rule, so on. Additionally, $D$
  uses ”lesser known” rules to calculate the derivative of a wide array
  of special functions. What could these ""lesser known"" rules be?","['computer-algebra-systems', 'derivatives']"
1258364,What kind of vector spaces have exactly one basis?,"Here is the question as an exercise in the book Linear Algebra Done Right, Chapter 2 Find all vector spaces that have exactly one basis.",['linear-algebra']
1258398,Extending regular function on normal variety from a subvariety of codimension 2,"In his book ""Commutative Algebra with a View Toward Algebraic Geometry"" Eisenbud proves the Corollary 11.4 which states the following If $R$ is a normal Noetherian domain, then $R$ is the intersection of its localizations at codimension-1 primes. Then he writes that the geometric version of this fact is the following: If $X$ is a normal variety and $Y\subset X$ is a subvariety of codimension at least 2, then any rational rational function on $X$ regular on $X\setminus Y$ extends to a regular function everywhere on $X$. My question is, why is it true?","['algebraic-geometry', 'commutative-algebra']"
1258414,Proving that $c$ is a Banach space.,"I want to prove that $$c = \{  (x_n)_{n\geq 0} \mid x_n \in \Bbb C \text{ and the sequence converges} \}$$ with the norm $$ \left\|(x_n)_{n\geq 0}\right\|_{\infty} =\sup_{n\geq 0} |x_n|$$ is a Banach space. I have done some work, but I am having trouble concluding. So far: let $(\xi_n)_{n\geq 0} = \left( (x_k^{(n)})_{k\geq 0} \right)_{n\geq 0}$ be a $\|\cdot\|_{\infty}$-Cauchy sequence. Let $\epsilon > 0$, there exists $n_0 \in \Bbb N$ such that: $$\begin{align}  \| \xi_n - \xi_m \|_{\infty} &< \epsilon, \quad \forall\, n,m > n_0  \\ \sup_{k \geq 0} |x_k^{(n)}-x_k^{(m)}| &< \epsilon, \quad \forall\,n,m > n_0 \\  |x_k^{(n)} - x_k^{(m)}| &< \epsilon, \quad \forall\, k\geq 0, \quad \forall\,n,m > n_0 \end{align}$$ So fixed $k$, $(x_k^{(n)})_{n \geq 0}$ is a $|\cdot |$-Cauchy sequence, and since $\Bbb C$ is Banach, there exists a limit $\lim_{n\to \infty} x_k^{(n)} =: x_k$. Then define $\xi = (x_k)_{k \geq 0}$. Now I understand I have two things to do: prove that $\xi_n \stackrel{\|\cdot \|_{\infty}}{\longrightarrow} \xi $: we proceed as before. Let $\epsilon > 0$. Now, there is $n_0 \in \Bbb N$ such that: $$\begin{align}  \| \xi_n - \xi_m \|_{\infty} &< \epsilon, \quad \forall\, n,m > n_0  \\ \sup_{k \geq 0} |x_k^{(n)}-x_k^{(m)}| &< \epsilon, \quad \forall\,n,m > n_0 \\  |x_k^{(n)} - x_k^{(m)}| &< \epsilon, \quad \forall\, k\geq 0, \quad \forall\,n,m > n_0  \\   \lim_{m \to \infty} |x_k^{(n)} - x_k^{(m)}| &\leq \epsilon, \quad \forall\,k\geq 0, \quad \forall\,n >n_0 \\  |x_k^{(n)} - x_k| &\leq \epsilon,  \quad \forall\,k\geq 0 \quad \forall\, n>n_0 \\ \sup_{k\geq 0} |x_k^{(n)} - x_k| &\leq \epsilon,\quad \forall\, n > n_0 \\ \| \xi_n - \xi\|_{\infty} &\leq\epsilon, \quad \forall\,n>n_0, \end{align}$$ so ok. prove that $\xi \in c$. I'm not sure of how to do this. I think I must use that every $(x_k^{(n)})_{k \geq 0}$ converge, because I don't seem to have used this yet. How can I prove that $\xi \in c $? Thanks.","['banach-spaces', 'real-analysis', 'functional-analysis']"
1258447,Doob-style second moment martingale inequality,"Let $\{X_k\}_{k=0}^{\infty}$ be a martingale, supposing
  $X_0 = 0$ and $E[{X_n}^2] <\infty$. Prove that $$P\left(\max_{1\le k
 \le n} X_k \ge r \right) \le \frac{E[{X_n}^2]}{E[{X_n}^2] + r^2}$$ for
  all $r>0$. I have tried to emulate the proof of the Doob maximal inequality as follows. Let $T = \min\{k\, : \, X_k \ge r\}$. Then by Jensen's inequality, $\{{X_k}^2\}$ is a submartingale, so that $E[{X_{T\wedge n}}^2] \le E[{X_n}^2]$ via optional stopping theorem. Since $${X_{T\wedge n}}^2 \ge \mathbb{1}_{T\le n} r^2 + \mathbb{1}_{T> n} {X_n}^2$$ it follows $$E[{X_{T\wedge n}}^2] \ge r^2 P(T\le n) + E[{X_n}^2 \mathbb{1}_{T>n}] $$ If this is the right approach, then we next will have to prove $E[{X_n}^2 \mathbb{1}_{T>n}] \ge P(T\le n) E[{X_n}^2]$ but I am not able to show this. I have tried Cauchy-Schwarz and Jensen's, both to no avail. So I expect this approach is too weak. Another observation I made is the following: By Cauchy-Schwarz, $$E[X_{T\wedge n} \mathbb{1}_{T>n} ]^2 \le E[{X_{T\wedge n}}^2 \mathbb{1}_{T>n}] P(T>n) \le r^2 P(T>n)^2$$ since $X_{T\wedge n} \le r$ when $T>n$. But also $$E[X_{T\wedge n} \mathbb{1}_{T> n} ]^2 = E[X_{T\wedge n} \mathbb{1}_{T\le n} ]^2\ge E[r\cdot \mathbb{1}_{T\le n}]^2 = r^2 P(T\le n)^2$$ by using optional stopping theorem to see $E[X_{T\wedge n}] = E[X_0] = 0$. Thus $P(T\le n) \le P(T>n)$. But I don't know how to proceed from here. Any help/hints are much appreciated!","['probability-theory', 'martingales']"
1258458,A question about the automorphisms of the alternating group $A_n$.,"For $n\ge 3$. Let $A_n$ be the alternating group of degree $n$ acting on $\{1,2,\ldots,n\}$, and let $H_i$ be the isotropy subgroup of $i\in J_n$.  If $\phi:A_n\to A_n$ is automorphism, show that $\phi$ is induced by an inner automorphism of $S_n$ if and only if $\phi(H_1)=H_i$ for some $1\le i\le n$. Showing that $\phi$ induced by an inner automorphism of $S_n$ implies $\phi(H_1)=H_i$ is easy, but I'm stuck on the other direction. I've thought about looking at how the automorphisms act on $3$-cycles, since they generate $A_n$, but this seems messy and inefficient.","['abstract-algebra', 'group-theory']"
1258463,"What is the volume inside $S$, which is the surface given by the level set $\{ (x,y,z): x^2 + xy + y^2 + z^2 =1 \}$?","The solution given uses a linear algebraic argument that doesn't seem very instructive -- and may not even be correct, I think. We notice from the equation, that the surface is a quadratic form, level set = 1. $\bullet$ The solution rewrites it as $x^TAx$, $\bullet$ Finds a symmetric matrix $A$ that gives the resulting quadratic form, $\bullet$ Computes the eigenvalues of $A$, And then (here's where the explanation doesn't really follow, I think) $\bullet$ A change of variables is made so that the surface becomes an ellipsoid, $\bullet$ Finally using the volume of an ellipsoid formula, the final answer is given. Is there another / better way of finding the volume of $S$? Thanks,","['volume', 'linear-algebra', 'multivariable-calculus']"
1258485,Expectation of quotient of random variables,"Let $X_1,...X_n$ be independent, identically distributed and nonnegative random variables, and let $k\le n$. Compute: $$E\left[{\sum_{i=1}^k X_i\over \sum_{i=1}^n X_i}\right].$$ This question has already been asked: Expectation of random variables ratio . The thing is that my teacher told me that the solution in the link wasn't really a ""solution"" the correct thing to do is to compute the conditional expectation of $${\sum_{i=1}^k X_i\over \sum_{i=1}^n X_i}$$ given that $\sum_{i=1}^n X_i=m$ where $m$ is a positive integer, but I have no idea how to do this, I would really appreciate if you can help me with this problem.","['conditional-expectation', 'probability', 'random-variables', 'expectation']"
1258499,Rotate an area around a diagonal line.,"I know how to find the volume of the figure formed when you rotate a $2$-dimensional area around a horizontal or vertical line, but what if it were a diagonal line instead? For example: Rotate the area between the curve $y=x^2$ and $y=x$ around the line $y=x$. That should create a diagonal ""football"" shape, starting at the origin. Anyways, the way I thought to go about the problem was to rotate the equation $45°$ to the right, and then solve the problem as if it were rotated around the x-axis. So I decided to convert $y=x^2$ to a polar equation, add $\frac{pi}{4}$ to $\theta$, and then turn it back into a rectangular equation. That worked, and it gave me this equation, which gives a diagonal parabola opening toward the first quadrant: $\frac{\sqrt{2}}{2}(y+x) = \frac{1}{2}xy(x^2+y^2)$ Only problem is that I need to isolate $y$, such that I can solve this problem like an ordinary rotation problem, and that seems impossible. What should I do to solve the problem? I don't know much linear algebra, but could that be used to solve this instead?","['rotations', 'calculus', 'linear-algebra']"
1258515,Mordell-Weil rank in elliptic surfaces,"Suppose that an elliptic smooth K3 surface $X$ defined over a number field $k$ has arithmetic Picard rank $r$ and assume that it is equipped with a $k$ fibration over $\mathbb{P}^1$ that has a section over $k$, i.e. $f:X\to \mathbb{P}^1$ is a dominant map with the generic fiber $f^{-1}(\eta)$ being an elliptic curve. Is there a relation between the Mordell-Weil rank of the generic fiber and $r$ ?","['elliptic-curves', 'algebraic-geometry', 'number-theory']"
1258530,Algebraic independence via the Jacobian,I have seen being mentioned that algebraic independence of polynomials can be tested by the so called Jacobian Criterion (Apparently one takes the Jacobian matrix of these polynomials and inspects the rank of the matrix (or the rank of its minors)). Where can i find the precise statement and its proof?,"['abstract-algebra', 'polynomials', 'reference-request', 'commutative-algebra']"
1258531,Is $3$ a prime element of $\mathbb{Z[\eta]}$?,"How to check whether $3$ is a prime element or not in $\mathbb{Z[\eta]}$, where $\eta$ is a $17$th primitive root of unity. Also in general how can we check an element is prime or not in $\mathbb{Z[\eta]}$, where $\eta$ is a primitive n-th root of unity","['abstract-algebra', 'algebraic-number-theory', 'ring-theory']"
1258555,Establishing the Identity Confusion,"So I have an establishing the identity problem that I'm trying to figure out. $\frac{\sin\theta + \cos\theta}  {\sec\theta + \csc\theta} = \cos(\theta) \sin(\theta) $ I'm being told that the first step is to multiply the right side of the identity by $\frac{\sec\theta + \csc\theta}  {\sec\theta + csc\theta} $ because ""since this fraction is equivalent to 1, the product is unchanged."" Why are we doing this?",['trigonometry']
1258582,What are all the intermediate fields of $\mathbb{Q}\big(\sqrt{3+\sqrt{5}}\big)$ containing $\mathbb{Q}$?,"I've come to a fork in the road, and it is sending me on wild goose chases.  This question comes from a final exam for an Intermediate Abstract Algebra course I just took this past Spring. I'm re-working the questions to which there were two problems that require using the Galois correspondence.  The second I have yet to start, because I'm trying to finish the final aspect of the first, and I'm almost finished with this except I'm stuck between one of two ways to proceed (if, indeed, I'm correct regarding my work below). Details & Preliminary Work (Somewhat Rough) :  We are working in $\mathbb{Q}\subset\mathbb{Q}\big(\sqrt{3+\sqrt{5}}\big)\subset\mathbb{R}$. Let $\alpha=\sqrt{3+\sqrt{5}}\in\mathbb{R}$, then the minimal/irreducible polynomial over $\mathbb{Q}$ is $f(x)=x^{4}-6x^{2}+4=\text{irr}(\alpha,\mathbb{Q})\in\mathbb{Q}[x]$. So, $[\mathbb{Q}(\alpha):\mathbb{Q}]=\text{deg}(\alpha,\mathbb{Q})=\text{deg}\big(f(x)\big)=4$. I've previously proved that $\sqrt{3-\sqrt{5}}\in\mathbb{Q}(\alpha)$, so $\mathbb{Q}(\alpha)$ is a splitting field for the separable polynomial $f(x)\in\mathbb{Q}[x]$ (as the roots of $f(x)\in\mathbb{Q}[x]$, mentioned below, are all simple [i.e. the roots are distinct]). Hence, the extension $\mathbb{Q}\subset\mathbb{Q}(\alpha)$ must be a Galois extension. Furthermore, $\sigma\in\text{Gal}\big(\mathbb{Q}(\alpha)/\mathbb{Q}\big)$ is uniquely determined by its values of $\sigma\big(\sqrt{3+\sqrt{5}}\big)\in\big\{\sqrt{3+\sqrt{5}},-\sqrt{3+\sqrt{5}},\sqrt{3-\sqrt{5}},-\sqrt{3-\sqrt{5}}\big\}$, so the $\big|\text{Gal}\big(\mathbb{Q}(\alpha)/\mathbb{Q}\big)\big|=4$ since all four possibilities must occur. Label the roots as such: $\alpha=\alpha_{1}=\sqrt{3+\sqrt{5}}=-\alpha_{2}$, and $\alpha_{3}=\sqrt{3-\sqrt{5}}=-\alpha_{4}$. We now define $\sigma,\tau\in\text{Gal}\big(\mathbb{Q}(\alpha)/\mathbb{Q}\big)$ as followed: $\sigma(\alpha_{1})=\alpha_{2}$ $\sigma(\alpha_{2})=\sigma(-\alpha_{1})=-\alpha_{2}=\alpha_{1}$ $\sigma(\alpha_{3})=\sigma\bigg(\dfrac{\pm 2}{\alpha_{1}}\bigg)=\dfrac{\pm 2}{\alpha_{2}}=\alpha_{4}$; where $\alpha_{1}\alpha_{3}=\pm 2=\sqrt{4}=\alpha_{2}\alpha_{4}$ $\sigma(\alpha_{4})=\alpha_{3}$. And: $\tau(\alpha_{1})=\alpha_{3}$ $\tau(\alpha_{2})=\tau(-\alpha_{1})=-\alpha_{3}=\alpha_{4}$ $\tau(\alpha_{3})=\tau\bigg(\dfrac{\pm 2}{\alpha_{1}}\bigg)=\dfrac{\pm 2}{\alpha_{3}}=\alpha_{1}$ $\tau(\alpha_{4})=\alpha_{2}$. Finally, above gives that: $\sigma\tau(\alpha_{1})=\alpha_{4}$ $\sigma\tau(\alpha_{2})=\alpha_{3}$ $\sigma\tau(\alpha_{3})=\alpha_{2}$ $\sigma\tau(\alpha_{4})=\alpha_{1}$; whereas the $\text{Gal}\big(\mathbb{Q}(\alpha)/\mathbb{Q}\big)=\big\{e,\sigma,\tau,\sigma\tau\big\}$. My issue resides with the mappings for $\sigma$ and $\tau$ above. The Galois group is isomorphic to a subgroup of $S_{4}$. Also, the Galois group is of order $4$, so it is isomorphic to either the Klein $4$-group, $V_{4}$, or the cyclic group $\mathbb{Z}/4\mathbb{Z}$; however we have the maps above give that the Galois group is isomorphic to $V_{4}$, and $\sigma\mapsto(1,2)(3,4)$, $\tau\mapsto(1,3)(2,4)$, $\sigma\tau\mapsto(1,4)(2,3)$ which are all (even) permutations in $S_{4}$. As $f(x)\in\mathbb{Q}[x]$ is irreducible, the action by the Galois group on the set $\big\{\alpha_{1},\alpha_{2}, \alpha_{3}, \alpha_{4}\big\}$ is a transitive action, and the maps above justify this transitive action. The Klein $4$-group has three, proper subgroups, so the Galois group must also have three, proper subgroups being $\langle\sigma\rangle$, $\langle\tau\rangle$, $\langle\sigma\tau\rangle\subset\text{Gal}\big(\mathbb{Q}(\alpha)/\mathbb{Q}\big)$. If I am correct above, then the Galois correspondence gives that $\langle\sigma\rangle$ will map to $\mathbb{Q}(\sqrt{5})$, but here is my MAIN problem (again if everything above is correct - especially the automorphisms in the Galois group defined above) - what are the fixed fields that correspond to $\langle\tau\rangle$ and $\langle\sigma\tau\rangle$!? I keep re-working things, and I still can't arrive to even an educated guess, requiring verification, in regards to the fixed fields that $\langle\tau\rangle$ and $\langle\sigma\tau\rangle$ map to by the Galois correspondence. Any help, suggestions, recommendations, hints, tips, etc. will be greatly appreciated. In one attempt, I tried using a basis for $\mathbb{Q}(\alpha)=\mathbb{Q}\big(\sqrt{3+\sqrt{5}}\big)$ as a $\mathbb{Q}$-vector space, and I still can't seem to see what remains fixed - take any $\beta\in\mathbb{Q}(\alpha)$, for $\tau(\beta)$ to remain fixed we must have that $\tau(\beta)=\beta$ whereas $\beta=a_{0}+a_{1}\alpha_{1}+a_{2}\alpha_{1}^{2}+a_{3}\alpha_{1}^{3}$ for some $a_{i}\in\mathbb{Q}$, $i=0,1,2,3$, for example. Moreover, as the $\big|\text{Gal}\big(\mathbb{Q}(\alpha)/\mathbb{Q}\big)\big|=4$, the only other possibility is that $\text{Gal}\big(\mathbb{Q}(\alpha)/\mathbb{Q}\big)\cong\mathbb{Z}/4\mathbb{Z}$, and we are done in this case (?). Lastly, I'm very happy to provide additional details, side-work, explanations, etc., on anything that is above if need be.  The test gives this question with several sub-questions, and I'm stumped on the very last part. That being said, I tried to provide somewhat of a sketch on how I arrived to the last part of the question (which is exactly my Question stated in the title). Overall, thank you for your time, and anything provided to set me straight on this is GREATLY APPRECIATED!","['abstract-algebra', 'galois-theory']"
1258614,Does the inverse of a polynomial matrix have polynomial growth?,"Let $M : \mathbb{R}^n \to \mathbb{R}^{n \times n}$ be a matrix-valued function whose entries $m_{ij}(x_1, \dots, x_n)$ are all multivariate polynomials with real coefficients.  Suppose that $M(\mathbf{x})$ is invertible for every $\mathbf{x} \in \mathbb{R}^n$.  I would like to show that $M^{-1}$ has at most polynomial growth. That is, let $\|\cdot\|$ be your favorite matrix norm on $\mathbb{R}^{n \times n}$.  I want to show there are constants $C,N$ such that $\|M^{-1}(\mathbf{x})\| \le C (1 + |\mathbf{x}|)^N$. One possible approach starts as follows.  Let $p(\lambda, \mathbf{x}) = \det(M(\mathbf{x}) - I \lambda)$ be the characteristic polynomial of $M$.  Then, if we consider $M$ as a matrix over the commutative ring $\mathbb{R}[\mathbf{x}] = \mathbb{R}[x_1, \dots, x_n]$, we can apply the Cayley-Hamilton theorem to conclude that $p(M(\mathbf{x}), \mathbf{x}) = 0$.  Since $M(\mathbf{x})$ is invertible for every $\mathbf{x}$, we have that $p(0, \mathbf{x}) = \det(M(\mathbf{x}))$ is a polynomial with no zeros; let us suppose $p(0, \mathbf{x}) > 0$.  Then by writing $p(\lambda, \mathbf{x}) = \lambda r(\lambda, \mathbf{x}) + p(0, \mathbf{x})$ for some other polynomial $r$, we get that $$M(\mathbf{x})^{-1} = -\frac{r(M(\mathbf{x}), \mathbf{x})}{p(0,\mathbf{x})}.$$  The numerator certainly has polynomial growth, so it would remain to estimate $1/p(0,\mathbf{x}) = 1/\det(M(\mathbf{x}))$.  I am not sure of an easy way to do that; I considered applying Hilbert's 17th problem (as solved by Artin) to write $p(0,\mathbf{x})$ as a sum of squares of rational functions, but that seems much too complicated. I feel like there must be something much simpler that I am missing.","['polynomials', 'linear-algebra', 'matrices']"
1258623,On $a^4 + b^4 = c^4 + d^4 = e^5$.,"Let $a, b, c, d, e$ be distinct positive integers such that $a^4 + b^4 = c^4 + d^4 = e^5$. Show that $ac + bd$ is a composite number.","['contest-math', 'number-theory', 'elementary-number-theory']"
1258647,Steinhaus theorem for topological groups,"$G$ is a locally compact Hausdorff topological group, $m$ is a (left) Haar measure on $X$, $A$ and $B$ are two finite positive measure in $G$, that is $m(A)>0$, $m(B)>0$. My question is: Can we conclude that $AB= \{ab, a\in A, b\in B\}$ contains some non-empty open set of G? Is this question right?
Or is this right just for $G=R^n$, $R^n$ is the Euclid space, and $m$ is the Lebesgue measure on $R^n$. If so, how to prove it? Thanks a lot.","['locally-compact-groups', 'topological-groups', 'measure-theory']"
1258655,How to determine if these graphs are isomorphic?,"I had this question on my last Discrete exam: (the missing vertex on graph G is vertex d) I did prove that the graphs were isomorphic, but my teacher said that I matched up my vertices wrong. However, I have tripled checked it and the all the edges match up with the way I matched the vertices. Can anyone explain to me where I went wrong? Thanks!","['graph-theory', 'graph-isomorphism', 'discrete-mathematics']"
1258661,"Why sum of two little ""o"" notation is equal little ""o"" notation from sum?","Why sum of two little ""o"" notation is equal little ""o"" notation from sum? $o( f(n) ) + o( g(n) ) = o( f(n) + g(n) ) ?$ For example: $f(n) = n^3$ $g(n) = 1/n$ so $o(f(n)) = n^2$ $o(g(n)) = 1/n^2$ and $o( f(n) ) + o( g(n) ) = n^2 + 1/n^2$ $o( f(n) + g(n) ) = n^2$ Of course, I could write it like $o( f(n) ) + o( g(n) ) = n^2 + o( g(n) )$ $o( f(n) + g(n) ) = n^2 + o( g(n) )$ My question is why?
I don't understand it, because in first we always get two parameters.","['analysis', 'notation']"
1258733,"Distribution of occurrences of ""pairs of heads"" in $N$ coin tosses","Let's say we toss a weighted coin $N$ times, each with probability $p$ of landing heads up. What's the distribution of the number of times we'll see $k$ pairs of heads ? For example, HTHHHTHH would count as three pairs, as would HHHH, while HHTHH would count as two. I hope that makes sense. More formally, let $X_i$ be an indicator random variable which is 1 if there is a pair of heads starting at position $i$ and 0 otherwise. What I'm looking for is the distribution of the sum $X=\sum_{i=1}^{N-1}X_i$. At first I thought it would be a simple binomial, with the number of trials being $N-1$, each with success probability $p^2$. But I've since come to the conclusion that it's more complicated than that (from some simple simulations in Matlab). This isn't for homework, just a problem I came across that has been bugging me a bit. Thanks! Edit: I'm thinking perhaps the approach used on this website that I came across could be adapted for this, if you fix $h$ and sum over the possibilities for $t$. Thoughts?","['probability', 'probability-distributions']"
1258739,Graph Theory - How many regions of an n-sided polygon with all chords added?,"The question: Consider an $m-sided$ polygon with all of its chords added, and
assume that no more than two of these chords cross at any one intersection
point.  Make the figure into a planar graph by regarding each junction as a
vertex.  Then use Euler's Theorem to find the number of regions into which the
polygon is divided.  (Note: the answers for $m=3,4,5,6$ are $1,4,11,25$,
respectively.) I've been thinking about this question for about an hour... I know that the number of regions in the polygon is just $f-1$. I don't know if I'm allowed to use recurrence relations to solve this problem, so for now I'm assuming I shouldn't. I know that the number of vertices is $m +$ the number of intersections of chords... I also think that each original vertex (i.e. on the polygon) has degree $m-1$ and each additional vertex has degree $4$..however I don't know how to count exactly how many edges there are since some vertices are incident to the same edge and so I'd overcount if I just summed up all the degrees. I also think there are $m\frac{m-3}{2}$ chords, because each original vertex has degree $m-1$ and 2 of those are already taken up to make up the actual polygon, and you divide by 2 to not overcount since there are 2 vertices to every chord. For example, for $m=5$, each original vertex has degree 4, 2 of those already account in the polygon, so each original vertex is incident to 2 chords, and we divide by 2 because 2 vertices are incident to the same chord. Can anyone point me in the right direction? It'd be much appreciated.","['graph-theory', 'discrete-mathematics']"
1258754,What can be said about the integral part of $(a+\sqrt{b})^n $,"If an irrational number of the form $a+\sqrt{b}$ is raised to the power $n$, with $a, b \in \mathbb{Z}$, and $n \in \mathbb{N}$, what can be said about the integral part of the resulting number? For instance, is it an even number? If yes what are the conditions on $n$? If odd, what are the conditions? I think I should start from considering $(a+\sqrt{b})^n= p+q$ where p is an integer and q is a proper fraction. I don't know how to proceed further. Please help. And also suggest a website that can give more of such properties.","['elementary-number-theory', 'algebra-precalculus']"
1258761,Naive approach to Pythagoras,"The following has occupied me while learning about $a^2+b^2=c^2$, I then forgot about all that and recently (40yrs after) came across that again - and am still unable to understand. But today my next thought was to discuss the matter here and I'm sure we can quickly work it out! :-) Unfortunately my graphic skills are worse than my math, so I'll have try to describe my idea textwise here, I hope my english skills will be good enough. Let's assume we have a triangle with the angles $A$,$B$,$C$ ($A$ on top, $C$ being the $90°$ and $B$ being right) and the connecting lines being named with the lowercase name of the opposing angle. Sorry for not using proper terminology, I've been out of geometry for too long :(( The idea is that $c$ equals the distance of $A$ and $B$, and to determine that distance, we can (instead of the using the direct way $c$) also travel along $b$, then $a$ and get to $B$. So the distance is $a+b$. Obviously we're going too far this way, so let's try to improve the route by ""creating stairs"". We go down b, but after half the way we turn right and walk hald the distance before turning again. So, this way the distance is ${a\over2}+{b\over2}+{a\over2}+{b\over2}$ or $a+b$. And here's the point I do not get: following that idea, we can create an infinite number of steps, down to the width of an atom, which would approach the line of minimal length ($=c$) until they become one. But still: computing the length according to that approach, we'd end up with $a+b$ again! So, where is my fault???","['geometry', 'triangles']"
1258798,"$2^x+7^y=19^z$ has no solution in positive integers $x$, $y$, $z$","How do I show that the diophantine equation $2^x+7^y=19^z$ has no solution in positive integers $x$, $y$, $z$","['number-theory', 'diophantine-equations']"
1258863,What's the best way to think about the covariance matrix?,"Let $X$ be a random vector with covariance matrix $\Sigma$. People often describe $\Sigma$ in terms of its components:
$\Sigma_{ij}$ is the covariance of the $i$th and $j$th components of $X$. But in linear algebra, thinking about a matrix in terms of its components is often discouraged.  It is often more enlightening to avoid thinking in terms of components. So what is the ""best"" way to think about $\Sigma$, particularly for someone who likes linear algebra? I know that $\Sigma = \mathbb E((X - \mu)(X - \mu)^T)$, where $\mu = \mathbb E(X)$.  But I think I am still missing something, because I'm not sure what to make of that formula.  Does this formula shed light on what $\Sigma$ really is and why we care about it?","['covariance', 'linear-algebra']"
1258866,Continuity of the integral as a function of the domain,"Let $f: \mathbb{R}^n \to \mathbb{R}$ be integrable. Let $C \subset \mathbb{R}^n$ be measurable. Is $$
r \mapsto \int_{rC} f \, \mathrm{d} \mu,
$$
where $rC = \left\{ rc \: \middle| \: c \in C \right\}$ continuous? If not, is it under some more restricted contiditons on $f$ or $C$ i.e. compactness?","['calculus', 'multivariable-calculus']"
1258875,Properties of the category of smooth vector bundles over a smooth manifold,"I am wondering if there are any sources that discuss the properties of the category of vector bundles over a smooth manifold. It seems that most differential geometry texts I've looked at avoid explicitly discussing the properties of this category in a systematic fashion. I know that the category is additive, but are there are any other useful things to know about this category? For instance: Is it a closed monoidal category with respect to the tensor product? Is there a sharp result that states the conditions under which kernels and quotients exist? Under what conditions do functorial constructions on vector spaces induce corresponding constructions on smooth vector bundles? How much of homological algebra applies to this category?","['differential-geometry', 'manifolds', 'vector-bundles', 'category-theory']"
1258923,What did Alan Turing mean when he said he didn't fully understand dy/dx?,"Alan Turing's notebook has recently been sold at an auction house in London. In it he says this: Written out: The Leibniz notation $\frac{\mathrm{d}y}{\mathrm{d}x}$ I find
  extremely difficult to understand in spite of it having been the one I
  understood best once! It certainly implies that some relation between
  $x$ and $y$ has been laid down e.g.  \begin{equation} y = x^2 + 3x \end{equation} I am trying to get an idea of what he meant by this. I imagine he was a dab hand at differentiation from first principles so he is obviously hinting at something more subtle but I can't access it. What is the depth of understanding he was trying to acquire about        this mathematical operation? What does intuitive differentiation notation require? Does this notation bring out the full subtlety of differentiation? What did he mean? See here for more pages of the notebook.","['notation', 'calculus', 'math-history', 'intuition', 'derivatives']"
1258952,snugly fitted spheres in a cube,"A larger sphere A, having a radius $R$ is snugly fitted in a cube (i.e. sphere A touches all six faces of the cube). Further, a small sphere B is snugly fitted in the corner of cube (i.e. sphere B touches sphere A & three orthogonal faces meeting at the same vertex). Further, a smaller sphere C, having a radius $r$, is snugly fitted in the same corner of the cube (i.e. sphere C touches sphere B & three orthogonal faces meeting at the same vertex). How to find out the radius $r$ (of smaller sphere C) in terms of the radius $R$ (of larger sphere A)? I have tried this using vector analysis but I did not find the correct answer. Any help or pointing in the right direction is appreciated. Note: None of the spheres touches any of 12 edges of the cube. Unit vector equally inclined with three orthogonal axes X, Y & Z is given as 
$$ \hat{r}=\frac{1}{\sqrt{3}}(\hat{i}+\hat{j}+\hat{k})$$ angle of inclination of the vector with three axes is
$$=\cos^{-1}\left(\frac{1}{\sqrt{3}}\right)\approx 54.73^{o}$$ 
but further if angle of inclination of this vector with each of three orthogonal faces is $\alpha \space$ then I have
$$\sin\alpha =\frac{R_{B}}{R\sqrt{3}-R-R_{B}}$$
Similarly, for radius $r$ 
$$\sin\alpha =\frac{r}{R_{B}\sqrt{3}-R_{B}-r}$$ Further, how I can determine angle of inclination $\alpha$ with the orthogonal planes?","['geometry', 'spheres', 'analytic-geometry']"
1258955,Convergence of the integral $\int_0^{\pi/2}\ln(\cos(x))dx$,"I want to Show that whether the integral
$$\int\limits_0^{\pi/2}\ln(\cos(x))dx$$
is convergent ot not. My Approach: Let $y=cos(x)$, then the above integral reduces to $$\int\limits_0^{1}\frac{\ln(y)}{\sqrt{1-y^2}}dy.$$ At this step since $\ln(y)<<y^p$, $p=1,2,...$, I compare above integral from above with the integral$$\int\limits_0^{1}\frac{y}{\sqrt{1-y^2}}dy,$$ which is convergent. Hence by comprasion I obtain that the integral $\int\limits_0^{\pi/2}\ln(\cos(x))dx$ is convergent. My Question: (1) Is my  approach true? (2) Can you suggest any different aproach? Thanks in advance...","['analysis', 'calculus', 'real-analysis']"
1259009,Why did mathematicians introduce the concept of uniform continuity?,"I have solved many problems regarding uniform continuity, but still I can't understand the following: Is there any practical application of this concept, or it is just a theoretical concept? Is there any wide application of this concept in any theorem or problem? In short, how did the concept of uniform continuity arise?","['continuity', 'uniform-continuity', 'real-analysis', 'general-topology']"
1259019,Find the value of $\lim_{n \rightarrow \infty} \Big( 1-\frac{1}{\sqrt 2} \Big) \cdots \Big(1-\frac{1}{\sqrt {n+1}} \Big)$ [duplicate],This question already has answers here : How can I find $\lim_{n\to \infty} a_n$ [duplicate] (2 answers) Closed 5 years ago . $$\lim_{n \rightarrow \infty} \Big( 1-\dfrac{1}{\sqrt 2} \Big) \cdots  \Big(1-\dfrac{1}{\sqrt {n+1}}  \Big)$$ Attempt: Let $y = \lim_{n \rightarrow \infty} \Big( 1-\dfrac{1}{\sqrt 2} \Big) \cdots  \Big(1-\dfrac{1}{\sqrt {n+1}}  \Big)$ $\ln y = \lim_{n \rightarrow \infty} \ln \Big( 1-\dfrac{1}{\sqrt 2} \Big)+ \cdots  + \ln \Big(1-\dfrac{1}{\sqrt {n+1}}  \Big)$ I am not able to move ahead really from here. Could someone give me an hint on how to move forward with this problem. Thank you very much for your help in this regard.,"['calculus', 'limits']"
1259020,Can a local ring have more than one prime ideal?,"A local ring is defined as a ring which has a unique maximal ideal. This unique maximal ideal consists of only non-units and contains all the non-units of the ring $R$. So examples of local rings include any field, or rings localised at prime ideals and so on. My question is: can we have a local ring which has more than one prime ideal? I can't seem to think of any examples where this is the case. (In response  to the comments below, I have realised that when $R$ is a domain, then $(0)$ is also a prime ideal, but are there any examples where $R$ isn't a domain?)","['abstract-algebra', 'ideals', 'localization']"
1259048,Does $P(X>0)\geq\mathbb{E}[X]^2/\mathbb{E}[X^2]$ hold for a non-negative random variable $X$?,Is there a way to see if the inequality $$P(X>0)\geq\mathbb{E}[X]^2/\mathbb{E}[X^2]$$ is generally true for a random variable $X\geq0$ in $\mathbb{R}$?,"['probability-theory', 'probability', 'expectation']"
1259061,Number of paths of a certain type in a triangular array,"Consider the $X_n$ set of finite integer sequences $(x_1, \ldots, x_{n})$ of length $n$ for which $x_1 = 0$ and $|x_k - x_{k+1}| = 1$ for each $k \in \{1, \cdots, n-1 \}$. Obviously $|X_n| = 2^{n-1}$. As an example $(0,1,2,1,2)$ and $(0,-1,0,1,2)$ are both in $X_5$, but $(0,1,3,1,2)$ is not. Given an element in some $X_n$, sometimes a permutation of the order of the terms will also be in the set. For example, (0,1,0,-1,0) and (0,-1,0,1,0) are both in $X_5$, but (0,1,2,3,4) has no valid permutations except itself. How can I count the number valid permutations for a given element of $X_n$? The question might be viewed geometrically by considering the triangular array of depth $n$ which starts at $-k$ on row $k$ and increases by $2$ on each step from left to right. Here it is for $n=6$: Then the question is, given a descending path in such a triangular array, how many other such paths pass through each integer the same number of times?",['combinatorics']
1259069,"PMF for K, the number of trails up to, but not including, the second success","I'm taking an MIT OCW course on Probability. Question: Al performs an experiment comprising a series of independent trials. On each trial, he simultaneously flips a set of three fair coins. Whenever all three coins land on the same side in any given trial, Al calls the trial a success. Find the PMF for K, the number of trials up to, but not including, the second success. My solution: Success occurs only when we get 3 heads or 3 tails $P(success) = 1/4$ In $k$ trails, we will 1 success, so the PMF is - $PMF = {{k}\choose{1}} * 1/4 * (3/4)^{k-1}$ Solution Given: $PMF = {{k}\choose{1}} * (1/4)^2 * (3/4)^{k-1}$ Can anyone explain what is wrong in my answer? Link to the solution: 2 (b)","['probability-theory', 'probability', 'geometric-probability']"
1259081,"If there are injective homomorphisms between two groups in both directions, are they isomorphic?","If I have two groups, $G$ and $H$ and two injective homomorphisms $\phi:G \to H$ and $\psi: H \to G$, then by the first isomorphism theorem applied to $\phi$, we have that $G \cong \mathrm{Im} (\phi)$, a subgroup of $H$. Similarly, $H$ is isomorphic to a subgroup of $G$. For finite groups, this guarantees that $G \cong H$ but does this hold for infinite groups? Weird things can happen for infinite groups, e.g $n\mathbb Z \subsetneq \mathbb Z$ but $n \mathbb Z \cong \mathbb Z$. I'm wondering if this kind of thing stops an isomorphism from occurring. I think this question generalises to rings, modules, fields and so on. Can this be answered for all of these structures?","['abstract-algebra', 'group-theory']"
1259082,Image of point of codimension one has codimension one?,"I'm working on the following exercise (7.2.3) from Liu's Algebraic Geometry and Arithmetic Curves : Let $f: X \rightarrow Y$ be a morphism of Noetherian schemes. We suppose that either $f$ is flat or $X,Y$ are integral and $f$ is finite surjective.
  Let $x \in X$ be a point of codimension $1$, and $y = f(x)$. Show that $\dim \mathcal{O}_{Y,y} = 1$ if $f$ is finite surjective... Is this statement true? The going-up theorem only shows that $\dim V(y) = \dim V(x)$, which is not the same thing, since it is not always true that $\dim V(y) + \dim \mathcal{O_{Y,y}} = \dim Y$. It seems to me like we need the going-down theorem, which requires an additional normality hypothesis on $Y$. I think the problem is that $\mathcal{O}_{Y,y} \rightarrow \mathcal{O}_{X,x}$ is not necessarily finite.","['algebraic-geometry', 'commutative-algebra']"
1259085,What is the maximum volume of $N$-D slice of an $M$-D hypercube?,"Consider a unit hypercube of M dimensions. We wish to make a cut of dimension N through it. What is the largest N-D size (length, area, volume, ...) we can achieve, $S(M, N)$ , and what cut gives it? Examples: $S(M=2, N=1) = \sqrt2$ Linear cut through a square Largest slice is the diagonal Length $\sqrt{1^2 + 1^2}$ $S(M=3, N=1) = \sqrt3$ Linear cut through a cube Largest slice is the space diagonal Length $\sqrt{1^2 + 1^2 + 1^2}$ $S(M=3, N=2) = \sqrt 2$ Planar cut through a cube Largest slice is the plane through the diagonal Area $\sqrt 2 \cdot 1$ Bits of this function are easy to generalize: $$
S(M, N) = \left\{
\begin{array}{rl}
   \sqrt M & : N = 1\\
   ? & : 1 < N < M\\
   1 & : N = M\\
   0 & : N > M\\
 \end{array}
\right.
$$ Can we generalize for $N = 2, 3, ...$ ?","['volume', 'geometry', 'cross-sections']"
1259131,Is a convex salient cone necessarily contained in an open half-space?,"A cone $C$ in $\Bbb R^n$ is said to be salient if it does not contain any pair of opposite nonzero vectors; that is, if and only if $C \cap (-C) \subset \{0\}$. Obviously, a cone $C$ such that that $C\setminus\{0\}$ is contained in an open half-plane is salient. I suspect the converse might be true for convex cones but I have found no reference to this result. On the other hand, this sentence from Wikipedia seems to make a difference between the two notions, depending on how you interpret the or . The term proper cone is variously defined, depending on the context. It often means a salient and convex cone, or a cone that is contained in an open halfspace of V.","['geometry', 'convex-analysis']"
1259141,Set Theory Proof: Valid or not?,"I'm trying to gain understanding of set proofs and I came across this one. I can't help but think the proof is too simple and that there is more to it. Problem: Prove or disprove for arbitrary sets $A$, $B$, $C$ $A\subset B\Rightarrow (A\cap C\subset B\cap C)$ My proof is: Let $x$ be an arbitrary element 1) $\{x\mid (x\in A \rightarrow x\in B)\rightarrow [(x\in A \wedge x\in C)\rightarrow (x\in B\wedge x\in C)]\}$
by definition of subset and intersection 2) $\{x \mid (x\in A\rightarrow x\in B) \rightarrow (x\in A\rightarrow x\in B)\}$ by Simplification 3) $\{x \mid A\subset B \rightarrow A\subset B\}$ by definition of subset QED Would this be a valid proof for this particular problem? If not, how come?","['elementary-set-theory', 'proof-verification']"
1259142,Compare $\sum_{k=1}^n \left\lfloor \frac{k}{\varphi}\right\rfloor$ ...,"Given two integer sequences \begin{equation*}
\displaystyle A_n=\sum_{k=1}^n \left\lfloor \frac{k}{\varphi}\right\rfloor ,
\end{equation*} \begin{equation*}
B_n=\left\lfloor\dfrac{n^2}{2\varphi}\right\rfloor-\left\lfloor \dfrac{n}{2\varphi^2}\right\rfloor
\end{equation*} here: $\quad\varphi=\dfrac{1+\sqrt{5}}{2}\quad$ (golden ratio) Prove that: $|A_n-B_n|\leq 1.$ I realized that the difference between $A_n$ and $B_n$ is very small but the failure in finding an exact formula for $A_n$
Could you help me?","['summation', 'number-theory', 'ceiling-and-floor-functions', 'golden-ratio']"
1259144,Exercise about continuous functions,"Consider a continuous function $f \, : \, [0,1] \, \longrightarrow \, [0,+\infty)$ such that $f(0)=f(1)=0$ and : $\forall x \in (0,1), \; f(x) > 0$. I would like to prove that there exist $x_{1},x_{2} \in [0,1]$ with $x_{1} < x_{2}$ and such that $f(x_{1}) = f(x_{2}) = x_{2}-x_{1}$. If we assume that $x_{1}$ and $x_{2}$ exist, then let $a = x_{2}-x_{1} > 0$ and we see that the equation $f(x_{1})=f(x_{2})=a$ becomes : $f(x_{1}) = f(x_{1}+a) = a$. Therefore, $f(x_{1}) = f\big( x_{1}+f(x_{1}) \big)=a$. But I do not see how this will help me to find $x_{1}$.","['continuity', 'calculus', 'real-analysis', 'functions']"
1259153,Is there a problem in assuming that a point is the same thing of a vector?,"I've read Apostol's Calculus , in the section on analytic geometry. He says that he's going to use 'vector' and 'point' interchangeably. But in Beardon's Algebra and Geometry , he argues that there is not agreement on what vectors are, some say that they are points on $\mathbb{R}^n$, some say that they are directed line segments and for others they are classes of line segments in which they are the same if they represent the same displacement. I'm curious if problems can appear by assuming that each of the definitions given by Beardon is equivalent.","['linear-algebra', 'analytic-geometry']"
1259163,(Un)distorted subgroups in $\mathbb{F}_2 \times \mathbb{F}_2$,"We consider the product of free groups $$\mathbb{F}_2 \times \mathbb{F}_2 = \langle a,b,c,d \mid [a,b]=[b,c]=[c,d]=[d,a]=1 \rangle.$$ Given some elements $g_1,\ldots,g_n \in \mathbb{F}_2 \times \mathbb{F}_2$, viewed as words over $\{a,b,c,d \}^{\pm 1}$, is there an algorithm to determine if the subgroup $\langle g_1, \ldots,g_n \rangle$ is undistorted (ie. the inclusion induces a quasi-isometric embedding)? For instance, for an explicit example, is the subgroup $\langle a,b,cd \rangle$ undistorted?","['group-theory', 'computational-algebra', 'geometric-group-theory']"
1259166,"$f(x-y)$ considered as a function of $(x,y)\in \mathbb{R^{2n}}$ is measurable if $f$ is measurable","I know there are similar questions up proving this, but I had a question specific to the following proof (specifically in bold): Let $f$ be a Lebesgue measurable function on $\mathbb{R^n}$. Then the function $f(x)$ considered as a function of $(x,x)\in \mathbb{R^n}$ is Lebesgue measurable. The linear transformation given by $T:(x,y)\rightarrow (x−y, y)$ is invertible, and so $f(x−y)$ is a Lebesgue measurable function of $(x,y)\in \mathbb{R^n}$. Thus, we see that $f(y)g(x − y)$ is measurable on $\mathbb{R^2n}$. I know it has something to do with the fact that $T$ maps measurable sets to measurable sets, meaning that $F\circ T = f(x-y)$ is measurable if $F(x,y) = f(x)$, and I'm guessing it has something to do with proving that $T^{-1}$ is Lipschitz, but I just can't make the connection.","['lebesgue-measure', 'lebesgue-integral', 'measure-theory']"
1259180,"$4$-element subsets of the set $\{1,2,3,\ldots,10\}$ that do not contain any pair of consecutive numbers","Find the number of subsets of the set $\{1,2,3,\ldots,10\}$ that contain $4$ elements and do not include any pair of consecutive numbers. For example $\{1,2,5,7\}$ is not an example of such a subset since $1,2$ are consecutive. I think that we can set the $4$ cases to solve it, case one exactly $2$ numbers of consecutive elements, case two exactly $3$ numbers of consecutive elements, case three exactly $3$ numbers of consecutive elements and case four exactly $4$ numbers of consecutive elements, and we sum $4$ cases and minus them from all choices.","['combinations', 'elementary-set-theory', 'combinatorics']"
1259229,How many necklaces are there so that between any two red beads there are at least two blue ones?,There are 8 red beads and 32 blue beads. How many necklaces (which can be rotated) are there such that between any two  red beads there are at least 2 blue beads. I reduced the problem to finding the set of solutions of $$x_1+ x_2 +\cdots +x_{8} = 16$$ Where no 8-tuple is a cyclic permutation of the other. But I really can't see how to make any further progress.,['combinatorics']
1259237,"Let $(X ,\mathfrak T)$ be a topological space and suppose that $A$ is a subset of $X$. Then $Cl(A) = A\cup Bd(A)$. False!","Let $(X ,\mathfrak T)$ be a topological space and suppose that $A$ is a subset of $X$. Then $Cl(A) = A\cup Bd(A)$. I think this statement is false because the definition of closure does have the union of A and the limit points of A. However, I know that it is possible to have boundary points that are not limit points. If I am correct what is a basic example that models this?","['elementary-set-theory', 'general-topology']"
1259295,Probability of Intersecting Two Random Segments in a Circle,"I designed this problem and tried to solve it but didn't solve. Choose four points $A$, $B$, $C$ and $D$ from inside of a circle uniformly and independent. What is the probability that $AC$ intersects $BD$?","['probability', 'geometric-probability']"
1259299,Spectrum of an operator is approximate point spectrum plus spectrum of dual operator,I'm trying to show that given an operator $T \in B(X)$ with $X$ Banach we have $$\sigma(T) = \sigma_{ap}(T) \cup \sigma_p(T') $$ Where $T' \in B(X')$ is the dual operator. I know that $\sigma(T) = \sigma(T')$ and certainly $\sigma_{ap}(T) \subset \sigma(T)$ so we have $\sigma_{ap}(T) \cup \sigma_p(T') \subset \sigma(T)$ and I'm struggling with the other inclusion. If we write $R_{\lambda} = {\lambda}I - T$ and now we want to show if $\lambda$ is outside of $ \sigma_{ap}(T) \cup \sigma_p(T')$ then $R_{\lambda}$ is invertible. Here is what I have so far: $R_{\lambda}$ has closed image. ( $\lambda$ is not in the approx.  point spectrum of $T$ ) $R_{\lambda}$ is injective - the point spectrum of $T$ is contained in the approximate point spectrum of $T$ . I'd like to show that $R_{\lambda}$ has dense image. We know that $\ker(R_{\lambda}')$ is empty - so if $g \in Y'$ is such that $g(R_{\lambda})(x) = 0$ then $g$ must be zero. I'd like to maybe use the Hahn-Banach theorem on an element in $X$ not in the closure of $\operatorname{im} R_{\lambda}$ or perhaps by extending a functional on $\operatorname{im}R_{\lambda}$ but unfortunately I can't seem to get anywhere with this! Thanks for any help!,"['dual-maps', 'spectral-theory', 'functional-analysis']"
1259324,Solving differential equation,"I want to solve the following differential equation with initial conditions: $$\frac{\mathrm{d}^2 y}{\mathrm{d} x^2}=\frac{x \, y(x)}{\sqrt{1-x}}$$
  But do not know how to actually solve it. Any suggestion?",['ordinary-differential-equations']
1259332,Prove the Apollonius' theorem.,"Let in a $\Delta ABC$, D is the midpoint of $BC$.Prove that: $AB^2+AC^2=2CD^2+2AD^2$ MY ATTEMPT : Given that $BD=DC$ and we construct $E \ such \ that\ AE=EC\implies AC=\frac{EC}{2} \ and \ DE||AB \implies DE=\frac{AB}{2}$ For the $\Delta DEC$ we have $DC^2=DE^2+EC^2 \implies 4DC^2=AC^2+AB^2 $ we have $AB^2+AC^2=2CD^2+2CD^2 \tag{1}$ In $\Delta ADE$ we have, $AD^2=AE^2+DE^2 \implies AE^2+DC^2-EC^2 \implies 
AD^2=DC^2$ Hence $2CD^2=2AD^2$ Thus, we have $AB^2+AC^2=2CD^2+2AD^2$ I am not sure of this proof. Though this proof is well explained in wikipedia .I tried to check if this can be solved using elementary geometry.",['geometry']
1259341,Random variables and the topology of weak convergence,"To see what's going on, I am trying to translate the idea of topology of weak convergence on a random variable setting, just to get some concrete intuition. This is what I have got so far (where the topological setting is the standard one from where to start, and the other one is my translation in concrete terms). Topological setting: $X$ metrizable topological space, $\mathcal{B} (X)$ Borel $\sigma$-algebra, $\Delta (X)$ set of all probability measures over $\mathcal{B} (X)$, $C_b (X)$ set of all continuous bounded functionals on $X$. $w^*$ topology on $\Delta (X)$ s.t. for all $f \in C_b (X)$, $\mu \mapsto \int f d \mu$ is $w^*$-continuous. Example of a Probabilistic setting: $X$ represents the faces of two rolled dice that can show up, $\mathcal{B} (X)$ is the set of possible events associated with $X$, $\Delta (X)$ set of all probability measures over $\mathcal{B} (X)$, $C_b (X)$ is the set of all random variables on $X$, such as the random variable $f =$ sum of the two numbers that appear. $w^*$ topology on $\Delta (X)$ s.t. for all $f \in C_b (X)$, $\mu \mapsto \int f d \mu$ is $w^*$-continuous, which means that – by fixing a random variable – there is a functional that maps the probability measure $\mu \in \Delta (X)$ to the expected value of the random variable $f$ according to $\mu$. Questions Apparently, as I found out in this pdf , the standard way of looking at those things is slightly different. Indeed, random variable should enter in the picture as something that is just used in order to talk about the convergence of $\mu_n$, without directly using probability measures. But still my question remains the following: Can we use my alternative way of looking at those things as a sound one, even if not completely standard? If the intuition is correct, in this case – for a fair dice – we have only one possible probability measure $\mu$, right? Any feedback is most welcome. Thank you for your time.","['probability-theory', 'probability-distributions', 'measure-theory']"
1259364,Show that $l^2$ is a Hilbert space,"Let $l^2$ be the space of square summable sequences with the inner product $\langle x,y\rangle=\sum_\limits{i=1}^\infty x_iy_i$. (a) show that $l^2$ is H Hilbert space. To show that it's a Hilbert space I need to show that the space is complete. For that I need to construct a Cauchy sequence and show it converges with respect to the norm. However, I find it confusing to construct a Cauchy sequence of sequences?","['functional-analysis', 'lp-spaces', 'complete-spaces', 'hilbert-spaces', 'banach-spaces']"
1259383,Calculating uncertainty in standard deviation,"I have a distribution with literally an infinite number of potential data points.  I need the standard deviation.  I generate about a hundred points and take the standard deviation of the points.  This gives a hopefully good approximation of the true standard deviation, but it won't, of course, be exact.  How do I estimate the uncertainty in the standard deviation?  This seems like a very basic question, but web searching hasn't provided any solution.  If I missed it somehow, my apologies.","['statistics', 'standard-deviation']"
1259391,When does a function have an inverse?,"I have been told that a function has an inverse if it is one-to-one or injective, but how can we rigorously prove this?  I have been struggling to find a proof for days.","['inverse', 'functions']"
1259401,How to compare the Hardy-Littlewood maximal function for balls and cubes?,"I am currently working through a set of notes I found on the internet at: http://math.msu.edu/~charlesb/Notes/DuoChapter2.pdf I am up to page 8, and the Hardy-Littlewood maximal function for balls has just been introduced. Then it says that we can also define maximal functions over cubes centred at $x$. Then there is the phrase:
""Furthermore, since the $n$-dimensional volumes of the unit cube and unit ball are equal up to a multiplicative
constant depending only on $n$, it is immediate that $Mf$ and $M'f$ are comparable in the sense that $c_nM'f(x)\leq Mf(x)\leq C_nM'f(x)$
for constants $c_n$ and $C_N$ only depending on $n$."" It may be immediate to the author but it is not at all to me! I cannot understand why this is true. Does anyone have a proof? And is there a formula for $c_n$ and $C_n$? All I can think of is maybe it's possible to come up with some sort of comparison between the size of a ball and the size of a cube both using the same $r$, but then the integrals may not be equal in order to compare the entire maximal funnction...","['lebesgue-measure', 'harmonic-analysis', 'functional-analysis', 'measure-theory']"
1259412,Please help collecting examples of finite/infinite rings satisfying different conditions about units/zero divisors (Added question 4),"0) Every nonzero element of a finite ring is either a zero divisor or a unit. This is proved in Every nonzero element in a finite ring is either a unit or a zero divisor 1) If a ring R satisfies the condition that ""every nonzero element is either a zero divisor or a unit"", must R be finite? If not, can you please give at least two non-isomorphic counterexamples? 2) If a ring R satisfies the condition that ""every nonzero element is a unit"", will R be finite or infinite? If both cases are possible, can you please give at least two non-isomorphic examples in both cases? 3) Does there exist finite/infinite rings such that ""every nonzero nonidentity element is a zero divisor""? If both answers are yes, can you please give at least two non-isomorphic examples in both cases? Edit: 4) If a ring R has an element that is neither a unit nor a zero divisor, then R must be infinite. Now will R be countable or uncountable? Can you please give examples (especially if both cases are possible)?------(Ok I know $\mathbb{Z}$ is a countable example. Does there exist uncountable examples?) Any related links are welcome. Thank you first for your help! P.S. I am self-learning undergraduate level mathematics. Sorry if the question is trivial or stupid. Edit: After reading the answers, I find that doing textbook excises is still not enough to be proficient in this subject. I need to learn to think in more various ways.","['abstract-algebra', 'self-learning', 'ring-theory']"
1259427,High Dimensional Rotation Matrices As Product of In-Plane Rotations,"Lately I've been thinking a lot about how to find high-dimensional rotation matrices. In particular, can any rotation in $n$-dimensional space be represented as the product of $2$D plane rotations? I'm having a tough time finding this online. For example, in $4$D, could we just take rotations in the planes $XY$, $XZ$, $YZ$, $XW$, $YW$, $ZW$ and multiply them together? Obviously rotation matrix multiplication is noncommutative, so will any ordering of the multiplication result in an expression that could be used to represent any high-dimentional rotation? Thanks!","['lie-groups', 'rotations', 'machine-learning', 'analysis', 'lie-algebras']"
1259432,Concrete category of topological spaces over preordered sets: what are the initial morphisms?,"I am reading The Joy of Cats to become more familiar with category theory and I came upon the following question on concrete categories. Let Top be the category of topological spaces and let Prost be the category of preordered sets, with monotone maps as morphisms. Since every topological space is equipped with the specialization preorder and since a continuous function is monotone with respect to this preorder, there is a natural forgetful functor from Top to Prost . I now consider Top as a concrete category over Prost .
According to The Joy of Cats (p. 134), a morphism $f: X \to Y$ in Top is called initial if for each topological space $Z$ and for each monotone map $g: Z \to X$, the condition $f \circ g$ continuous implies $g$ continuous. It is not difficult to see that any continuous map $f: X \to Y$, 
where $X$ is equipped with the initial topology defined by $f$, is an initial morphism. Question . Is the converse true? If not, what are the initial morphisms in this case? N.B. The converse is true if Top is considered as a concrete category over Set , but this case is different.","['general-topology', 'category-theory']"
1259470,Why are these matrix row operations even allowed simultanously on more than one matrix?,"My Differential Equations book is going over finding the inverse of Matrices, and clearly I've forgotten my college algebra.  I have no idea why this works. The first example gives this: $\begin{bmatrix}1 & -1 & -1\\3 & -1 & 2\\2 & 2 & 3\end{bmatrix}$ to give the identity matrix, which is of course 
$\begin{bmatrix}1 & 0 & 0\\0 & 1 & 0\\0 & 0 & 1\end{bmatrix}$ The book says: Obtain zeros in the off diagonal position in the first (-3) times the first row to the second row and adding -2 times the first row to the third row. This generates the Matrices: $\begin{bmatrix}1 & -1 & -1\\0 & 2 & 5\\0 & 4 & 5\end{bmatrix} \begin{bmatrix}1 & 0 & 0\\-3 & 1 & 0\\-2 & 0 & 1\end{bmatrix}$ It Continues: Obtain a one in the diagonal position of the second column by multiplying the second row by $\frac{1}{2}$ . $\begin{bmatrix}1 & -1 & -1\\0 & 1 & \frac{5}{2}\\0 & 4 & 5\end{bmatrix} \begin{bmatrix}1 & 0 & 0\\-\frac{3}{2} & \frac{1}{2} & 0\\-2 & 0 & 1\end{bmatrix}$ And it proceeds: Obtain zeros in the off-diagonal positions inthe second column by adding the second row to the first row and adding (-4) times the second row to the third row. $\begin{bmatrix}1 & 0 & \frac{3}{2}\\0 & 1 & \frac{5}{2}\\0 & 0 & -5\end{bmatrix} \begin{bmatrix}-\frac{1}{2} & \frac{1}{2} & 0\\-\frac{3}{2} & \frac{1}{2} & 0\\4 & -2 & 1\end{bmatrix}$. This is where the set of rules I was constructing to repeat this breaks down completely.  However, in the first step, why are we allowed to add a number to the first column of a row being operated on in the left matrix to the second matrix, then multiply it by an arbitrary row? Then How is simply multiplying across rows simultaneously by a constant multiplicatively consistent? and worst of all in the final operation shown, why is the 4 not negative?  Why is anything other than the first column of the row being operated on being modified, unlike the other two additive operations that used a coefficient? I tried to show this for myself by constructing a matrix to represent $\bf{AB}$.  One matrix was constructed of elements $a_1,b_1...$ proceeding first down rows, not columns, and the second of such elements $a_2,b_2...$ It looked like thus: \begin{bmatrix}a_1a_2+b_2d_2+c_2g_2 & a_1b_2+b_1e_2 + c_1h_1 & a_1c_2+b_1f_2+c_1i_2\\... & ... & ...\\... & ... & ...\end{bmatrix} I would like to learn how to do this in row operations, because, well, it's faster, and several of these may be on the test.  However, in essence, all of the operations in the procedure seem arbitrary and I can't construct rules for repeating it.  Thank you for any help. By the way, the book is: Elementary Differential Equations , Ninth Edition, By Boyce and DiPrima.  The book is currently in it's tenth edition.",['matrices']
1259471,"Proof verification for proving $\forall n \ge 2, 1 + \frac1{2^2} + \frac1{3^2} + \cdots + \frac1{n^2} < 2 − \frac1n$ by induction [duplicate]","This question already has answers here : Proving $ 1+\frac{1}{4}+\frac{1}{9}+\cdots+\frac{1}{n^2}\leq 2-\frac{1}{n}$ for all $n\geq 2$ by induction (5 answers) Proof of $\sum_{k = 1}^{n} \frac{1}{k^{2}} < 2 - \frac{1}{n}$ [duplicate] (3 answers) Closed 8 years ago . Prove by mathematical induction: $\forall n \ge 2, 1 + \frac1{2^2} + \frac1{3^2} + \cdots + \frac1{n^2} < 2 − \frac1n$ Basis Step: (We want to show, $P(2)$, which is 1 + $\frac1{2^2}<2-\frac12$). $\frac1{2^2}=\frac14$, and $2-\frac12=\frac32$ So, $\frac1{2^2}<2-\frac12$. Inductive step: (We want to show, $\forall n\ge2,\mathrm P(n)\to\mathrm P(n+1)$. Let $k\ge2$ be an integer, arbitrary & fixed. (We want to show, $\mathrm P(k)\to\mathrm P(k+1)$). (I.H.) Assume $1+\frac1{2^2}+\frac1{3^2}+\cdots+\frac1{k^2}<2-\frac1k$ (""$\mathrm P(k)$"") (We want to show $\mathrm P(k+1)$, which is $1+\frac1{2^2}+\frac1{3^2}+\cdots+\frac1{(k+1)^2}<2−\frac1{k+1}$) $$\begin{align}
1+\frac1{2^2}+\frac1{3^2}+\cdots+\frac1{(k+1)^2}&<2−\frac1{k+1}+\frac1{(k+1)^2}\tag{by I.H.}\\
&=2-\frac1k+\frac1{k^2+2k+1}
\end{align}$$ (Thus, $\mathrm P(k)\to\mathrm P(k+1)$ for all $\mathrm k\ge2$ Also, $\mathrm P(2)$, so by mathematical induction, $\forall k\ge2,\mathrm P(k)$) Can someone look at my solution and see if it is correct? I'm not 100% confident with it.","['induction', 'discrete-mathematics', 'summation', 'solution-verification', 'proof-verification']"
1259483,Find an Angle of a Right Triangle Without Trigonometric Functions,"I have a right triangle triangle. I know the length of the hypotenuse (H) and one adjacent side (A). I would like to find the angle between the A and the H without using $\arccos(A/H)$. I would like to avoid all trigonometric equations. Is there a theorem or method to find this angle without using trig? Here is the general problem I am trying to solve: I am unable to factor the acos() function to isolate H. Perhaps this is due to my limited experience with math. If so, is there a resource I should look into or another way to solve the problem","['geometry', 'triangles', 'trigonometry']"
1259484,Sequence of independent events in a discrete probability space,"Let $(\Omega, \mathcal{A}, \Bbb{P})$ be a discrete probability space. Let $A_1, A_2,...\in \mathcal{A}$ be a sequence of independent events with $p_n = \Bbb{P}(A_n)$. Then $$\sum_{n\in \Bbb{N}} \min(p_n, 1-p_n)< \infty$$ My attempt: Suppose toward contradiction that the sum diverges. Then in particular $\sum_n p_n = \infty$. Applying Borel-Cantelli gives us $\Bbb{P}\left( \bigcap_{n\in \Bbb{N}} \bigcup_{k\geq n} A_n \right) =1$. Discreteness then implies that $\Omega = \bigcap_{n\in \Bbb{N}} \bigcup_{k\geq n} A_n$, so each $\omega \in \Omega$ is contained in infinitely many $A_n$'s. On the other hand, since the sum diverges, we have that $0=\prod_{n\in \Bbb{N}} 1 - \min(p_n, 1-p_n) = \prod_{n\in \Bbb{N}} \max(1-p_n, p_n) \geq \prod_n p_n = \Bbb{P}(\bigcap_n A_n) \geq 0$, and thus $\emptyset = \bigcap_n A_n$. That's not really a contradiction yet, but I feel like I'm almost there. What can I do?","['probability-theory', 'probability', 'statistics']"
1259488,Why is the field of fractions of a domain $R$ a flat $R$ module?,"I want to show  this lemma: Let $R$ be a domain. If $A$ is a torsion $R$-module, then $\operatorname{Tor}_1^R (K,A)\cong A$ where $\operatorname{Frac}(R)=Q$ and $K=Q/R$. When I was reading this proof need to show $Q$ is flat and it back to this problem if $R$ is a domain with $Q=\operatorname{Frac}(R)$, then $Q$ is flat $R$-module? I don't know why $Q$ is flat. Can you help please? thank you","['homological-algebra', 'abstract-algebra', 'modules', 'ring-theory']"
1259531,"If $\lim\limits_{z \to \infty} p(z) = \infty$, then $p(z)$ is a constant","Claim: If $p$ is an entire function and $\lim\limits_{z \to \infty} p(z) = \infty$ and $p(z) \neq 0$ $\forall z \in \Bbb C$, then $p(z)$ is a constant. Proof: Define $f(z) = \frac{1}{p(z)}$ so $\lim\limits_{z \to \infty} f(z) = 0$. Then there exists an $N > 0$ such that $\lvert f(z) \rvert \leq 1$ for all $\lvert z \rvert > N$. Take the closed disk $\lvert z \rvert \leq N$. Since this disk is compact and $f$ is continuous, there exists an $M > 0$ such that $\lvert f(z) \rvert \leq M$ for all $\lvert z \rvert \leq N$. Since $f(x)$ is bounded and entire, by Liouville $f(x)$ is a constant so $p(z)$ is a constant. Since it is obvious that a constant function cannot go to $\infty$, where is the fallacy in this proof since the statement seems not to be true?","['proof-verification', 'complex-analysis']"
1259534,On a connection between Newton's binomial theorem and general Leibniz rule using a new method.,"In calculus the general Leibniz rule asserts that Let $n$ be a natural numbers, if $f$ and $g$ are $n$-times differentiable functions at a point $x$, then the function $fg$ is also $n$-times differentiable and it's $n$-th derivative at this point is given by
  $$
(fg)^{(n)}(x)=\sum_{k=1}^n \binom nkf^{(k)}(x)g^{(n-k)}(x)
$$ Now a similar theorem in algebra named after Newton asserts that Let $n$ be a natural number, if $a$ and $b$ are two real numbers, then we have
  $$
(a+b)^n=\sum_{k=1}^n \binom nka^kb^{n-k}
$$ I'm going to show that the Newton's binomial theorem can be deduced from Leibniz general rule. Let $n$ be a natural number and $a,b$ are two real numbers. And let $f(t)=e^{at}$ and $g(t)=e^{bt}$ and use general Leibniz rule to find $n$-th derivative of the function $f(t)g(t)=e^{(a+b)t}$ at the point $x=0$ to find
$$
\begin{align}
(fg)^{(n)}(0)&=(e^{(a+b)t})^{(n)}|_{t=0}\\
&=\sum_{k=1}^n \binom nkf^{(k)}(0)g^{(n-k)}(0)\\
&=\sum_{k=1}^n \binom nk(e^{at})^{(k)}|_{t=0}(e^{bt})^{(n-k)}|_{t=0}
\end{align}
\tag{I}\label{I}
$$
On the other hand for every real number $c$ and every natural number $k$ we have
$$
(e^{ct})^{(k)}|_{t=0}=c^ke^{ct}|_{t=0}=c^k\tag{II}\label{II}
$$
Now apply $\eqref{II}$ in $\eqref{I}$ to find $$
(a+b)^n=\sum_{k=1}^n \binom nka^kb^{n-k}
$$ $\square$ The question is to find a proof of Leibniz general rule directly from Newton's binomial theorem. Thanks in advance... Note . You can use this method with other functions to find other interesting formulas.","['alternative-proof', 'calculus', 'algebra-precalculus', 'binomial-theorem', 'derivatives']"
1259536,Solving all possible values for a functional composition,"$f(x)$ and $g(x)$ are defined over the real number set $\Bbb R$ as follows:
$$
g(x)=1 - x + x^2 \\
f(x) = ax + b
$$
If $g ◦ f(x) = 16x^2 - 12x + 3$,
determine all the possible values of $a$ and $b$. I have no idea where to even start with this, I've attempted using inverse functions, plotting and expanding out the composition hoping for an equation to be solved, but no luck, just more arbitrary values.","['algebra-precalculus', 'functions']"
1259540,"Why are the Cosine and Sine of obtuse angles defined differently? If by convention, please explain the logic behind.","(I already know the unit circle) Why is it that the sine of an obtuse angle is the sine of its supplementary angle but the cosine of an obtuse angle is the negative of the cosine of its supplementary angle? I can see of course on the unit circle that it is this way, but my question is: Is it purely convention? 
And if it is, what was the logic behind coming up with that specific convention as opposed to another one? Was it just practical to define the unit circle this way? Why not have the cosine of obtuse angles be defined the same as the sine of obtuse angles? Why would that not work?",['trigonometry']
1259551,How to show fraction field is flat (without localization),"Here I asked that if one can prove the field of fraction of a domain is flat. The answers used localization, which I am not familiar with. Can anyone prove it without using localization ?","['homological-algebra', 'abstract-algebra', 'modules', 'ring-theory']"
1259572,Surface integral over parabolic cylinder that lies inside another cylinder,"To be precise, I'm given the following: Find $\iint_K {xdS}$ over the part of parabolic cylinder $z = \frac{{{x^2}}}{2}$  that lies inside the first octant part of the cylinder $x^2+y^2=1$. In my attempt, I tried to parametrize as following: $x=r\cos(\theta),y=r\sin(\theta),z=r^2\cos^2(\theta)/2$. The surface element turned out to be $dS=r\sqrt{r^2\cos^2(\theta)+1}drd\theta$, which means that considering the substitution, the integral (now, a double integral) is: $$\iint\limits_D {{r^2}\cos (\theta )\sqrt {{r^2}{{\cos }^2}(\theta ) + 1} drd\theta },\qquad D = \{ 0 \leqslant r \leqslant 1,0 \leqslant \theta  \leqslant \pi /2\} $$ Numerically, this turns out to be $\pi/8$, but I don't think it is clear how this can be solved analytically. Is there any way to solve these kinds of double integrals where the variables are bounded in such a way, or should a different parametrization be used instead?","['surface-integrals', 'calculus', 'multivariable-calculus']"
1259587,What are the chances of winning Higher/Lower?,"Given the game 'higher/lower': A deck of cards of size N is uniquely ranked. The deck is shuffled, lain in a row, and the first card is turned over. You have to guess whether the next card to be turned over is higher than this one or lower. The next card is flipped. If you're right, you continue with that card for the remaining deck. (After this, the next card flipped is compared to this one.) If you make it to the end, you win. Otherwise, you lose. It is obvious that if someone keeps track of all cards in the deck, they can make the best call between the two choices of ""higher"" and ""lower"" (with the highest chance of winning). Given that the deck is shuffled completely random, what is the chance that someone will win this game (through N cards) if this person tracks all cards in the deck and plays optimally? I thought of this question myself and was intrigued by it, but couldn't find any clues to the answer.","['probability', 'card-games']"
1259603,Show that $\sum_{n \le x} \phi (n)=\frac{x^2}{2\zeta(2)}+ O(x \log x)$,"How do I show that  $\sum_{n \le x} \phi (n)=\frac{x^2}{2\zeta(2)}+ O(x \log x)$, 
where $O$ denotes the big-$O$ notation. And we already know that $\phi (n) =  \sum_{d|n} \mu (d) \frac{n}{d}$.
I believe that I could approach something also from the previous question I posted but I couldn't make the relation.","['number-theory', 'totient-function', 'riemann-zeta']"
1259640,"If $H$ is a cyclic group of even order, $H$ has exactly two elements which square to $1.$","If $H$ is a cyclic group of even order, then $H$ has exactly two elements which square to $1.$ This was used in a answer (Pete Clark's answer) here: Prove that $x^{2} \equiv 1 \pmod{2^k}$ has exactly four incongruent solutions but I am not sure why this is true. Could someone please provide a proof to fill in some extra details?",['abstract-algebra']
1259656,Proving that $SL_2(\mathbb{Z}_5) / \{\pm I\}\simeq A_5$,"I see here that one can prove that
$$
SL_2(\mathbb{Z}_5) / \{\pm I\} \simeq A_5
$$
using the First Isomorphism Theorem. My question is how one would do that. I know that I need a surjective homomorphism 
$$
T: SL_2(\mathbb{Z}_5) \to A_5
$$
with kernel $\{\pm I\}$. The only homomorphism I have come across with matrix groups is the determinant map, so my question is what homomorphism would work here.","['abstract-algebra', 'finite-fields', 'group-isomorphism', 'matrices']"
1259663,Euler's theorem: [3]^2014^2014 mod 98,"Calculate without a calculator: $$\left [ 3 \right ]^{2014^{2014}}\mod 98$$ I know I have to use Euler's Theorem. As a hint it says I might need to use the Chinese Remainder theorem too. I know how both of these work theoretically. For example I can calculate $5^{256} \mod 13$ with Euler's Theorem.
But these large numbers throw me off, especially the double power of 2014. I started of like this for the Chinese Remainder:
$$98 = 2*7^{2}\\
\left [3  \right ]^{2014^{2014}} \mod 2 \\
\left [3  \right ]^{2014^{2014}} \mod 49 \\$$ Now calculate both:
$$\left [3  \right ]^{2014^{2014}\mod \phi_{(2)} = 2} \equiv  \left [3  \right ]^{0} \equiv 1  \mod 2$$
That was lucky.
I fail at mod 49. Applying Euler's Theorem on 2014^2014 (since ([3]^2014)^2014 seems even more impossible) gives the following: $$\phi _{(49)} = 42\\
2014 = 47*42+40\\
2014^{2014} = 2014^{42^{47}}*2014^{40}\equiv 2014^{40} \mod 49$$ (That doesn't really work without a calculator either.)
And what now, applying Euler's Theorem again doesn't work, and I can't do it in my head either. Is my approach right? Did I do something wrong? Please point me in the right direction.
I'm also open to entirely different solutions.","['chinese-remainder-theorem', 'discrete-mathematics', 'modules']"
1259681,How can I simplify $\sqrt{3^2 + 3^2\tan^2\theta}$?,$$\sqrt{3^2 + 3^2\tan^2\theta}$$ $$ = (3)(3\tan\theta) = 9\tan\theta $$ I've simplified it like this but I'm not sure if that's correct.,"['algebra-precalculus', 'trigonometry']"
1259694,How to Separate Charpit Equations,"I've been attempting to solve this non-linear PDE $$4\Omega x^2 y^2 \frac{\partial z}{\partial y} -x^2 y (\frac{\partial z}{\partial x})^2 + 2x^2 y^2 E-N^2=0$$ using Charpit's method.  The variables $\Omega$, $E$, and $N$ are constants.  I've derived the relation between Charpit's auxillary equations, $$\frac{x}{N^2}dp=-\frac{y}{N^2}dq=-\frac{1}{x^2 y p}dx=\frac{1}{2\Omega x^2 y^2} dy$$ but I have been unable to separate these to obtain the second function relating $p$ and $q$.  Most of the examples I have seen are separated very easily with terms such as $$\frac{dp}{p}=\frac{dq}{q}$$ or something similar.  Could anyone please help? EDIT:  A little background on Charpit's method and the Method of Characteristics... We begin by defining the primary non-linear PDE $$F(x,y,z,p,q)=4\Omega x^2 y^2 \frac{\partial z}{\partial y} -x^2 y (\frac{\partial z}{\partial x})^2 + 2x^2 y^2 E-N^2=0$$ Then, by expanding the total derivatives $$\frac{dF}{dx}=0$$ and $$\frac{dF}{dy}=0$$ in terms of partial derivatives, along with the definitions $$\frac{dx}{dt}\equiv\frac{\partial F}{\partial p}$$ and $$\frac{dy}{dt}\equiv\frac{\partial F}{\partial q}$$ we get the five Charpit Equations: $$\frac{dx}{dt}=\frac{\partial F}{\partial p}\\
\frac{dy}{dt}=\frac{\partial F}{\partial q}\\
\frac{dp}{dt}=-\frac{\partial F}{\partial x}-p\frac{\partial F}{\partial z}\\
\frac{dq}{dt}=-\frac{\partial F}{\partial y}-q\frac{\partial F}{\partial z}\\
\frac{dz}{dt}=p\frac{dx}{dt}+q\frac{dy}{dt}$$ By eliminating $dt$ they can all be set equal to each other (see Charpit's auxillary equations mentioned above), and any two (or more) can be used to integrate a total derivative relating $p$ and $q$.  This new relation is then substituted into the original differential equation, which can then be written as $$dz=p(x,y)dx+q(x,y)dy$$ which is a total differential which can be directly integrated to find the solution z=z(x,y).","['ordinary-differential-equations', 'partial-differential-equations']"
1259718,"If $p$ is a positive multivariate polynomial, does $1/p$ have polynomial growth?","I wanted to ask a separate question to focus on an elementary issue from my question Does the inverse of a polynomial matrix have polynomial growth? . Let $p : \mathbb{R}^n \to \mathbb{R}$ be a polynomial in $n$ real variables, with real coefficients, and suppose that $p > 0$ on all of $\mathbb{R}^n$. Does $1/p$ have (at most) polynomial growth?  That is, are there constants $C,N$ such that $1/p(\mathbf{x}) \le C (1+|\mathbf{x}|)^N$ for all $\mathbf{x} \in \mathbb{R}^n$? Of course this is true when $n=1$ because in that case we have $\lim_{x \to \pm \infty} p(x) = +\infty$, and so $1/p$ is actually bounded.  In higher dimensions this is more subtle: consider for example $p(x,y) = x^2 + (1-xy)^2$ which is strictly positive for all $x,y$, yet $1/p$ is unbounded: consider $p(1/y,y)$ as $y \to \infty$.  For this example, I can use some calculus to show that $1/p(\mathbf{x}) \le 1+\|\mathbf{x}\|^2 \le (1+\|\mathbf{x}\|)^2$, so $1/p$ does have polynomial growth.  But I don't know what to do in general.","['asymptotics', 'algebraic-geometry', 'polynomials', 'inequality']"
1259749,40th derivative of a function,"I would like to have some verification to see if my answer is correct. The given function is $f(x)=ln(1+x^2)$ and I need the 40th derivative at $x=0$. Here is my work: Using series one can manipulate $\frac{1}{1+x}=1-x+x^2-x^3+x^4...=SUMx^n(-1)^n$ into $\frac{1}{1+x^2}=1-x^2+x^4-x^6+x^8...=SUMx^{2n}(-1)^n$. Then $\frac{2x}{1+x}=2x-2x^3+2x^5-2x^7+2x^9...=2SUMx^{2n+1}(-1)^n$. Integrating gives $ln(1+x^2)=...=2SUM\frac{x^{2n+2}(-1)^n}{2n+2}$ where a $2$ cancels to arrive at a $n+1$ in that denominator. Now for the 40th derivative, $2n+2=40$ gives $n=19$ and thus I believe the answer is $\frac{-40!}{20}$ Do you concur? If not could you correct me? Thanks.","['power-series', 'solution-verification', 'derivatives']"
1259796,What's wrong with my permutation logic?,"The given question: In how many ways the letters of the word RAINBOW be arranged, such that A is always before I and I is always before O. I gave it a try and thought below: Letters A, I and O should appear in that order. Then there are four places in which all the remaining four letters can appear. It means there are a total of 16 places: $$*_1*_2*_3*_4\quad A\quad*_5*_6*_7*_8\quad I\quad*_9*_{10}*_{11}*_{12}\quad O \quad *_{13}*_{14}*_{15}*_{16}$$ 
  Out of these 16 places remaining, the letter can appear in any of the 4 places, giving total $^{16}P_4 = 43680$ possible arrangements. But the given answer is just 840 with following explanation: All the 7 letters of the word RAINBOW can be arranged in 7! ways and 3 particular letters (A,I,O) can be arranged in 3! ways. But the given condition is satisfied by only 1 out of 6 ways. Hence required number of arrangements 
  $$=\frac{7!}{3!} = 840$$ So what's wrong with my logic? I must be counting certain arrangements multiple times or my logic must be at fault in large. But what's that exactly?","['combinatorics', 'permutations']"
1259819,Expected value of the minimum of a non-negative random variable and a constant,"X is a non-negative random variable. Define Y = MIN(X, c) where c is a constant. What is E[Y]?
I am modeling the constant as another random variable whose pdf is Dirac Delta function: $f_{c}(x) := \delta(x-c)$ . The mean and variance of this ""constant random variable""(!) comes out as $c$ and $0$ , but does this approach have enough mathematical rigor?","['probability', 'random-variables']"
1259833,"Preserving compactness and connectedness implies continuity for functions between locally connected, locally compact spaces?","In this question: Connected and Compact preserving function is not continuous example? It is mentioned that ""a function between locally-compact, locally-connected topological spaces which preserves connected and compact subsets is in fact continuous"". I've come close to coming up with a proof for functions between $ \mathbb{R} $ , but the more general statement above seems much more interesting, however I haven't been able to find a proof anywhere or to think of a way to prove it myself. Could someone provide a reference, where I could find the proof of that? Or a counterexample if the statement is actually false. EDIT: As was pointed out in an answer below, the statement turns out false with a very simple counterexample, in which the spaces are not Hausdorff. Perhaps the additional condition of both spaces being Hausdorff will be sufficient to make it true? EDIT2: Since some people asked, here's my proof for $ f: \mathbb{R} \rightarrow \mathbb{R} $: Assume f is not continuous at a point $ x$. We can find a sequence $ x_n \rightarrow x $ monotonically with no subsequence of $ f(x_n) $ converging to $ f(x) $. If $ f(x_n) $ isn't constant for infinitely many $ n $, we can find a subsequence  $ x_{n_k} $ such that $ f(x_{n_k}) $ is strictly increasing or decreasing. Hence, $ f(x_{n_k}) $ cannot converge to $ f(x) $ nor to $ f(x_{n_k}) $ for any $ k $. $ \{ x_{n_k} \} \cup \{ x \} $ is compact, but $ \{ f(x_{n_k}) \} \cup \{ f(x) \} $ is not, which gives a contradiction, since $ f $ preserves compactness. If $ f(x_n) = c $ for infinitely many $ n $, we can assume it's constant for all $ n $, going to a subsequence if necessary. $ f $ preserves connectedness, so for arguments between $ x $ and $ x_n $ it takes all values between $ f(x) $ and  $ c $. For each $ n $ take  $ y_n $ between $ x $ and $ x_n $ so that $ 0 < |f(y_n) - c| < \frac{1}{n} $. $ \{y_n\} \cup \{x\} $  is compact, but the image is not, since  $ f(y_n) \rightarrow c \ne f(x)$. Contradiction.","['continuity', 'connectedness', 'general-topology', 'compactness']"
1259843,Confidence Interval of Information Entropy?,"Information entropy, $IE$, is defined as: $$IE = \sum_{i} p_i log\frac{1}{p_i}$$ Where $p_i$ is the probability of event $i$ (and we are summing over all possible events). Let's say I have data only, and estimate $p_i$, with $\hat{p_i}$, where: $$\hat{p_i} = \frac{n_i}{N}$$ Where $n_i$ is the number of occurances of event $i$ in the data, and $N$ is the total number of observations. Then I can estimate $IE$ with $\hat{IE}$ as: $$ \hat{IE} = \sum_{i} \hat{p_i} log\frac{1}{\hat{p_i}}$$ Is there a way to analytically find, say, a 95% confidence interval on $\hat{IE}$?","['probability-theory', 'probability', 'statistics', 'information-theory']"
1259845,"Probability distribution of $\int_0^t \frac{W_s}{s} \,ds$","I am currently working on an exercise that requires the knowledge of the distribution of $\int_0^t \frac{W_s}{s} \,ds$, where $W$ is a Brownian motion. I can compute the distribution of $\int_{0}^T W_t \,dt$ easily by using the stochastic Fubini's theorem: \begin{eqnarray}
\int_{0}^T W_t \,dt & = & \int_0^T \int_0^T \mathbf{1}_{[0,t]} (s) \,dW_s \,dt \\
& = & \int_0^T \int_0^T \mathbf{1}_{[0,t]} (s) \,dt  \,dW_s\\
& = &  \int_0^T T-s \,dW_s\\
& \sim & N \bigg( 0, \int_{0}^T (T-s)^2 \,ds \bigg).
\end{eqnarray} However, such method seems to not work in this case, as I do not know how to express $\frac{W_t}{t}$ as a stochastic integral. Any suggestions?","['probability-theory', 'stochastic-calculus', 'stochastic-integrals', 'stochastic-processes', 'brownian-motion']"
1259853,Why the derivative of $n^{1/n}$ is $n^{1/n} \left( \frac{1}{n^2} - \frac{\log(n)}{n^2}\right)$,"Why the derivative of $n^{1/n} = \sqrt[n]{n}$ is $n^{1/n} \left( \frac{1}{n^2} - \frac{\log(n)}{n^2}\right)$ (according to Maxima and other tools online)? I have tried to applied the chain rule, but it comes something completely different: $$\frac{1}{n} n^{\frac{1}{n} - 1} \cdot 1 = \frac{1}{n} n^\frac{1}{n}n^{-1} = \frac{1}{n^2} n^\frac{1}{n} = \frac{\sqrt[n]{n}}{n^2}$$ Sincerely, I am not seeing where that $\log$ and the rest of the stuff comes from. I have a more difficult problem that is similar and whose solution contains a $\log$ somewhere, but I am not seeing where it comes from.","['calculus', 'derivatives']"
1259858,Why is the collection of all algebraic extensions of F not a set?,"When proving that every field has an algebraic closure, you have to be careful. In this article https://proofwiki.org/wiki/Field_has_Algebraic_Closure , and as I have been told on this site, if we have a field F. The "" collection of all algebraic extensions of F"" is not a set. Is there a simple way to explain why this is not a set, and we can not apply zorns lemma on it? Or do you need a lot of reading in deep set-theory and logic to understand this? I have seen the russel paradox, but that is basically how much I know about this. What also is very confusing is that in real analysis we have that ""the space of continuous functions on [0,1] is a vector space"". So there is a set of continuous functions? This doesn't sound any more mysterious than ""all algebraic extensions of a given field F"", however one of them gives rise to a set, and one doesn't?","['elementary-set-theory', 'field-theory']"
1259876,Extending a morphism of schemes,"This question is an exercise 2.4 p.96 from Qing Liu's book ""Algebraic Geometry and Arithmetic Curves"". Let $X$, $Y$ be schemes over a locally Noetherian scheme $S$, with $Y$ of finite type over $S$. Let $x\in X$. Show that for any morphism of $S$-schemes $f_x:\text{Spec}(\mathcal{O}_{X,x})\to Y$, there exist an open subset $U\ni x$ of $X$ and a morphism of $S$-schemes $f:U\to Y$ such that $f_x=f\cdot i_x$, where $i_x:\text{Spec}(\mathcal{O}_{X,x})\to U$
is the canonical morphism (in other words, the morphism $f_x$ extends to
an open neighborhood of $x$). What is the idea of the proof?","['algebraic-geometry', 'schemes']"
