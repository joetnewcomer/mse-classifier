question_id,title,body,tags
750224,Is this double integral always positive on nonzero continuous functions?,"Is this double integral
$$(f,g)=\int_{x=0}^1\int_{y=0}^1\frac{f(x)g(y)}{|y-x|^{\frac14}}dydx$$
an inner product on continuous functions on $[0,1]$? Namely, is $(f,f)$ always positive for all nonzero continuous functions $f$? I don't know if this is true, but I conjecture it being correct.",['analysis']
750243,Formalism in integration,"Let's say we have some $y(t)$. The derivative of $y$ along time axis will be $y'(t)=\frac{dy(t)}{dt}=\frac{dy}{dt}$. So I will integrate like this over time: $\require{cancel}$
$\int_{t=0}^{+\infty}\frac{dy}{\cancel{d\tau}}\cancel{d\tau} = \int_{t=0}^{+\infty}dy=y|_{t=0}^{+\infty}=y(\infty)-y(0)$ , (since $y = y(t)$) Even though the result being correct, is this procedure correct according to formalism?
In other words: can I cancel the differentials? Thinking of interval that makes sense to me. can the integral variable be different from the differential (second step)? I've almost never see the integration variable explicitly shown like I put there. I solved the integral in the third step as my variable was $y$ even though it was actually $\tau$. Can that be done?",['integration']
750249,Analysis on manifolds after course on Lebesgue integration,"I am an junior currently taking a course on measure theory and Lebesgue integration using Royden's text. Before this, I took a standard intro to analysis course covering the first seven chapters of baby Rudin. My institution doesn't offer a multivariable analysis course. I have room in my schedule next semester for two independent studies on topics of my choice. I want to go to graduate school to pursue a PhD in math and do not want to have any holes in my undergraduate preparation. I'm not yet sure what I direction I want to go in during grad school. Would it be worthwhile for someone in my situation to do an independent study using a book such as Munkres' Analysis on Manifolds or similar such as Spivak's?","['advice', 'soft-question', 'analysis']"
750252,Are there surfaces with more than two sides?,"I'm watching a naive introduction to the Möbius band, the lecturer asks if it's possible to construct a one sided surface and then she says that there is one of these surfaces, namely the Möbius band. Then she mentions that some surfaces have two sides. I've also had this doubt when reading Flegg's From Geometry to Topology . So, is it possible to have a surface with more than two sides? My intuition says no, but perhaps someone made some magic trick and made it somehow. I've looked at some wikipedia articles and I've seen no mention to something with more than two surfaces (I've used my browser search tool), unless such surfaces have other names. I guess that the tags should be geometry and topology , if you think there is something more, please edit.","['general-topology', 'geometry']"
750264,Is an onto homomorphism from G to itself an automorphism,"A homomorphism from $G$ to itself is an automorphism if it is bijective. I am trying to make the condition of bijectiveness weaker. 1-1 is not enough because there is a 1-1 homomorphism from $\mathbb{Z}$ to $\mathbb{2Z}$. What about onto? If a homomorphism from $G$ to itself is onto, then is it an automorphism? Or, similarly, if $H$ is a nontrivial normal subgroup of $G$, can $G$ and $G/H$ be isomorphic?",['group-theory']
750271,Unsure with second order complex differential equations,"Solve $$y'' - 4y' + 5y = 0 $$ Where $y(0) = 0 \ , \ y'(0) = 2$. So I solve this as a second degree polynomial (no idea why) $$\frac{4 \pm \sqrt{16-20}}{2} = 2 \pm 2i$$ So the CASE III solution as my book calls it is: $$Ae^{kt} \cos(wt) + B e^{kt} \sin(wt)$$ Where $k = Re$ and $w = Im$. So anyhow, $$y(0) = A \cos(0) + B \sin (0) \Rightarrow A = 0$$ $$y'(0) = -A \sin(0) + B \cos (0) = 2 \Rightarrow B = 2$$ So the solution is thus $$y(t) = 2 \cos(t)$$ Am I even doing this right? I have no idea what I am doing and it seems that differential equations are just taught this way. Plug this and that into these magical formulas.","['complex-numbers', 'ordinary-differential-equations', 'calculus']"
750280,integration in five dimensions space,"I am doing this problem: Consider the differential form $$a=p_1 \, dq_1+p_2 \, dq_2-(p_1^2+p_2^2+q_1^2+q_2^2) \, dt\text{ in }\mathbb R^5=(p_1,p_2,q_1,q_2,t).$$ (a) Compute the differential $da$ and the form $a\wedge da$. (b) Evaluate the integral $\int_S da\wedge da$ where $S$ is the 4-dim surface in $R^5$ defined by the equation $p_1^2+p_2^2+q_1^2+q_2^2=t$ and the inequality $1\le t\le 2$. Here is part of my solution: (a) $da=dp_1\wedge dq_1+dp_2\wedge dq_2-2p_1dp_1\wedge dt-2p_2dp_2\wedge dt-2q_1dq_1\wedge dt-2q_2dq_2\wedge dt$ $a\wedge da=p_1dq_1\wedge(dp_2\wedge dq_2-2p_1dp_1\wedge dt-2p_2dp_2\wedge dt-2q_2dq_2\wedge dt)$
${}+p_2dp_2\wedge(dp_1\wedge dq_1-2p_1dp_1\wedge dt-2p_2dp_2\wedge dt-2q_1dq_1\wedge dt)$ 
${}-(p_1^2+p_2^2+q_1^2+q_2^2)dt\wedge(dp_1\wedge dq_1+dp_2\wedge dq_2)$ I stopped here, since I think this expression is too long. I cannot imagine this problem is so complicated. So my first question is that is there any quick way to calculate $a\wedge da$? Or the only way is to continue the above calculation, and leave so many terms there(I guess only two pairs can be collected)? (b) I realized that the integrand $$da\wedge da=d(a\wedge da)$$, so I think this is a good indicator for us to use the Stokes's theorem $$\int_Sda\wedge da=\int_{\partial S}a\wedge da$$. And $\partial S$ is just $p_1^2+p_2^2+q_1^2+q_2^2=1$ and $p_1^2+p_2^2+q_1^2+q_2^2=2$, which I guess is what the people who made this problem want us to do. So my second question is how do we calculate $\int_{\partial S}a\wedge da$. By this I mean: Do I really need to calculate the integral one by one(I guess there are at least eight terms in the expression of $a\wedge da$ after collecting the terms)? Any orientation issues I need to be careful about the two ""balls"" $p_1^2+p_2^2+q_1^2+q_2^2=1$ and $p_1^2+p_2^2+q_1^2+q_2^2=2$? I think I need to use the parametric form to calculate the integral, right? By the way, do I really need to use the polar coordinate, which is four layers of $sin, cos$? or there is some other way to calculate the final integral? Thank you very much!","['differential-geometry', 'analysis']"
750281,Predictor-Corrector for Adams-Moulton,"What is the order of the corrector of Adams-Moulton type required in order to apply Milne's method for estimating the error in PECE mode? Find the coefficient of the leading term in the truncation error for the third order implicit Adams-Moulton linear multistep scheme
\begin{equation}
y_{n+3}=y_{n+2}+ \frac{h}{12}(5 f_{n+3}+8f_{n+2}-f_{n+1})  \end{equation}
and deduce from the notes the value of Milne's error estimate of the error in this case. =>
first part of this question really confuse me. I know for the order of corrector of PECE is 3 for Adams-Moulton but I really don't know how to apply Milne's method to estimate the error in PECE mode.
now for the second part of question Third order implicit Adams-Moulton linear multistep scheme
\begin{equation}
y_{n+3}=y_{n+2}+ \frac{h}{12}(5 f_{n+3}+8f_{n+2}-f_{n+1})  \end{equation}
The LTE is given by $T_n$ After calculating in paper with massive cancellation I have got LTE as
\begin{equation} hT_n= \left(\frac{27}{8} -\frac{2}{3}-\frac{11}{4}\right) h^4 y_{iv}+O(h^4)+O(h^5)
\end{equation}
\begin{equation} hT_n= -\frac{1}{24} h^4 y_{iv}+O(h^4)+O(h^5)\end{equation}
\begin{equation} T_n= -\frac{1}{24} h^3 y_{iv}+O(h^3)+O(h^4)
\end{equation}
Therefore the coefficient of leading term is $C_{p}=-\frac{1}{24}$
For the last part of the question Milne's error estimate is given by
\begin{equation} e_{n+1}= C_{p} h^{p+1}y^{p+1}+O(h^{p+2}) \end{equation}
\begin{equation} e_{n+1}= -\frac{1}{24} h^3 y^{4}+O(h^5)\end{equation} Someone please kindly  check my solution and reply me if I got wrong.","['ordinary-differential-equations', 'calculus', 'algebra-precalculus', 'analysis', 'mathematical-modeling']"
750286,The Limit: $\lim_{x \to \infty}\frac{e^{f(x+a)}}{e^{f(x)}}$,"I'm doing some challenge review problems and I was wondering whether this proof looked correct: Suppose that $f: \mathbb{R} \rightarrow \mathbb{R}$ be a differentiable function with $\lim_{x \to \infty}f'(x)=1$ and $a \in \mathbb{R}$. Prove the limit exists and find it.
$$
\lim_{x \to \infty} \frac{e^{f(x+a)}}{e^{f(x)}}
$$
Here is what I did,
$$
\begin{align}
\lim_{x \to \infty} \frac{e^{f(x+a)}}{e^{f(x)}}&=\lim_{n \to \infty}e^{f(x+a)}e^{-f(x)} \\
&=\lim_{x \to \infty}e^{f(x+a)-f(x)} \\
&=\lim_{x \to \infty}\left(e^{f(x+a)-f(x)}\right)^\frac{a}{a} \\
&=\lim_{x \to \infty}\left(e^\frac{f(x+a)-f(x)}{a}\right)^a \\
&=\left(e^{\lim_{x \to \infty}\frac{f(x+a)-f(x)}{a}}\right)^a \\
&=\left(e^{\lim_{x \to \infty}f'(x)}\right)^a \\
&=(e^1)^a\\
&=e^a
\end{align}
$$
Though missing verbal explanation of the individual steps to be a 'good' proof, does this look like the right idea?","['proof-verification', 'real-analysis', 'limits']"
750301,One to one mapping from $A(S_1)$ into $A(S_2)$,"Let $S_1$ and $S_2$ be two sets. Suppose there exists a one to one mapping $\phi$ of $S_1$ into $S_2$. Show that there exists an one to one mapping from $A(S_1)$ into $A(S_2)$, where $A(S)$ means the set of all one to one mappings of $S$ onto itself. Suppose $\{\phi_1,\phi_2,....\}$ are the elements of $A(S_1)$ and $\{\phi'_1,\phi'_2,....\}$
are the elements of $A(S_2)$. I thought the way to connect them lies through $'\phi'$. Let $F: A(S_1) \to A(S_2)$ defined by $F(\phi_i)=\phi\circ \phi_i$. Not sure whether $ \phi\circ \phi_i=\phi'_i$??","['functions', 'abstract-algebra']"
750307,Systems of Linear Differential Equations - population models,"I have to solve the following first-order linear system, $x(t)$ represents one population and the $y(t)$ represents another population that lives in the same ecosystem:
(Note: $'$ denotes prime) \begin{align}
x' = -5x - 20y &  \text{(Equation 1)} \\
y' = 5x + 7y  &   \text{(Equation 2)}
\end{align} I start off with finding the derivative of (Equation 1) which gives me: $$x'' = -5x' - 20 y'$$ and then substitute $y'$ using the equations above (Equation 2): $$x'' = -5x' - 20 (5x + 7y)$$ foil expansion: $$x'' = -5x' - 100x - 140y$$ so I insert values for $y$ through reordering an existing equation: $$y = -\frac{1}{20}(x'+5x)$$ thus 
\begin{align}
x'' = -5x'-100x-140\bigg(-\frac{1}{20}x'-\frac{1}{4}x\bigg) \\
x'' = -5x'-100x+7x'+35x \\
x''+ 5x'+100x-7x'-35x=0 \\
x''-2x'+65x=0
\end{align} Then put into auxillary form: $r^2-2r+65=0$
thus $r = 1 \pm 8i$
..etc. Eventually I get $x(t)$ and $y(t)$. I have to predict what will happen to the population densities over a long time. How can I do this? Note: The book describes using equilibrium points and determining their stability to do this. Upvotes to whoever shows this specific method. I appreciate any additional explanations however. Edit: Here is my answer for $x(t)$ and $y(t)$ for clarification purposes.
($C_1$ and $C_2$ are constants) $x(t) = C_1e^{t} \sin 8t + C_2e^{t} \cos 8t$ $y(t) = \frac{1}{10}e^{t} (4C_2 \sin 8t-4C_1 \cos 8t-3C_1 \sin 8t-3C_2 \cos 8t)$","['dynamical-systems', 'linear-algebra', 'ordinary-differential-equations']"
750312,How to find $\lim_{x\to\ln 2} \frac{2e^{3x}-16}{3e^{2x}-12}$?,"Need to find without using L'Hopital's rule or derivatives. I know the answer is 2, but how can I find this analytically, without using limit tables? Thanks!","['calculus', 'limits']"
750324,Two variable function with four different stationary points,"Let $f(x,y)$ has continuous second partial derivative.
Define $$D(x,y)=f_{xx}(x,y)f_{yy}(x,y)-f_{xy}(x,y)^2.$$ If $(x_0,y_0)$ is a stationary point of a function $f(x,y$, then the  second partial derivative test asserts the following: (1) If $D(x_0,y_0)>0$ and $f_{xx}(x_0,y_0)>0$, then $(x_0,y_0)$ is a minimum point. (2) If $D(x_0,y_0)>0$ and $f_{xx}(x_0,y_0)<0$, then $(x_0,y_0)$ is a maximum point. (3) If $D(x_0,y_0)<0$, then $(x_0,y_0)$ is a saddle point. (4) If $D(x_0,y_0)=0$, then this test is inconclusive, and $(x_0,y_0)$ could be any of a minimum, maximum or saddle point. My question is:
Could one give a function $f(x,y)$ with exactly four different stationary points that satisfy $(1),(2),(3)$, and, $(4)$? I try some function but I haven't found such function. For example, $$f(x,y)=x^3-\frac{1}{2}x^2+y^3-\frac{1}{2}y^2$$ has four different stationary points but it doesn't satisfy $(4)$. Meanwhile, $$g(x,y)=x^3-x^2+\frac{1}{4}y^4-\frac{1}{3}y^3$$ has four different stationary points but it doesn't satisfy $(3)$.",['multivariable-calculus']
750328,"When are $\Delta x$, $\delta x$, $dx$, and $\text{đ}x$ exactly the same? When are they approximately the same?","As a follow-up to this related question, I'd like to know under what circumstances, if any, $\Delta x$, $\delta x$ and $dx$ all mean the same thing, and under what circumstances they can all be said to be approximately equivalent in a reasonably valid way. For bonus points, it would also be nice to know where the inexact differential, $\text{đ}x$, can be used in place of one of the other symbols. The reason that I ask this is that all of these symbols are commonly used as notation in thermodynamics and stat mech textbooks, and are often interchanged in ways that can be confusing to follow. In the math used in physics it's common to elide exactly correct statements with approximately correct ones, and it would be nice to have guidance when it comes to picking this sort of thing apart.","['calculus', 'physics', 'derivatives', 'differential', 'statistical-mechanics']"
750330,Determine the number of zeros in the first quadrant,This is a homework question: $$f(z) = z^2 - z + 1$$ sorry for the poor code!,"['functions', 'roots', 'complex-analysis']"
750336,"How prove this can choose two postive integer numbers $a_{m},a_{k},$such $\frac{a_{m}+a_{k}}{3a_{p}}\notin N^{+},$","If  $a_{1},a_{2},\cdots,a_{n}(a_{i}\neq a_{j}),n\ge 3$ are positive integers,show that: we can always choose two positive integers among them, $a_{m},a_{k},m,k\in\{1,2,\cdots,n\}$.such that
  $$\dfrac{a_{m}+a_{k}}{3a_{p}}\notin N^{+},\forall p\in\{1,2,3,\cdots,n\}$$ This problem is from the Jiangxi province Mathematical Contest,2014 .(I failed This year's exam was very difficult) my idea: if $n=3$,Assume that $a_{1}=1,a_{2}=2,a_{3}=3$,then we have $a_{p}=1$,then we can choose $a_{m}=2,a_{k}=3$,then it is 
clearly
$$\dfrac{a_{m}+a_{k}}{3a_{p}}=\dfrac{5}{3}\notin N^{+}$$
if $a_{p}=2$,then we can choose $a_{m}=1,a_{k}=3$ if $a_{p}=3$  then we can choose $a_{m}=1,a_{k}=2$, But for in general $n$,I can't prove it. Thank you very much",['combinatorics']
750338,Finding a partial derivative of a double summation. [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 10 years ago . Improve this question How to find $$\frac{\partial T }{ \partial {\dot q_i}},$$ given that $$T= \sum_i\sum_j{\alpha_{ij}\dot q_i\dot q_j}?$$",['multivariable-calculus']
750339,Solving the functional equation $(x+1)f\left(\frac{y}{f(x)}\right)=f(x+y)$,"Solve functional equation: Find all strictly monotone functions $f:(0,+\infty)\to(0,+\infty)$ such that $$(x+1)f\left(\frac{y}{f(x)}\right)=f(x+y),\forall x,y>0\text.$$","['functions', 'functional-equations']"
750367,Proving that $\mathrm{card}(2^{\mathbb{N}})=\mathrm{card}(\mathbb{N}^\mathbb{N})$,"I'd like to prove that $\mathrm{card}(2^{\mathbb{N}})=\mathrm{card}(\mathbb{N}^\mathbb{N})$, I have the following 'sketch' but I'm not sure if this works. $|2^{\mathbb{N}}|\leq|\mathbb{N}^{\mathbb{N}}|\leq|2^\mathbb{N^{\mathbb{N}}}|=|2^{\mathbb{N}\times\mathbb{N}}|=|2^\mathbb{N}|$, then $|2^{\mathbb{N}}|=|\mathbb{N}^\mathbb{N}|$ I'm taking for granted the first inequality, (i.e: $|2^{\mathbb N}|\leq|\mathbb{N}^{\mathbb{N}}|$), could be done a further proof about this. Would it be enough to point out that the functions in $2^{\mathbb{N}}$ are in $\mathbb{N}^{\mathbb{N}}$ but there are functions in the last one that are not in the first one? Should I try to give a more formal proof?","['cardinals', 'elementary-set-theory']"
750369,Number of subsets of a nonempty finite set with a given property.,"Let $S$ be a set with $|S|=n$, where $n$ is a positive integer. How many subsets $B$ of $S\times S$ are there with the property that $(a,a) \in B$ for all $a \in S$ and $(a,b) \in B \implies (b,a) \in B$ for all $a,b \in S$. So, this is the question I am having a tough time finding a solution for. I was able to do the first part but I couldn't do the second part. Can anyone help me with this?",['elementary-set-theory']
750376,Why do we define addition of matrices only when they have the same size,"What happens if we define
$$
\begin{pmatrix}
        1 & 2 \\
	1 & 2 \\
        1 & 2 
        \end{pmatrix} +
\begin{pmatrix}
        1 & 2 & 3 \\
	1 & 2 & 3 \\
        1 & 2 & 3
        \end{pmatrix} =
\begin{pmatrix}
        2 & 4 & 3 \\
	2 & 4 & 3 \\
        2 & 4 & 3
        \end{pmatrix}
$$
I think computers do this to refresh a part of the screen. But why can't we do it by matrices by filling the remaining rows/columns with zero? Is it related to the definition of vector spaces?","['matrices', 'linear-algebra']"
750380,"Find two $2 \times 2$ matrices $A$ and $B$ with the same rank, determinant,...but they are not similar.","Find two $2 \times 2$ matrices $A$ and $B$ with the same rank, determinant, trace and characteristic polynomial, but that are not similar to each other. I come up with two matrices: $A=\begin{pmatrix}
0 &1 \\ 
0 &0 
\end{pmatrix}$ and $B=\begin{pmatrix}
0 &0 \\ 
1 &0 
\end{pmatrix}.$ It is easy to check that they have same rank, determinant, trace and characteristic polynomial. However, my question is I do not know how to prove two matrices are similar or not. I have learnt the converse in my textbook, i.e. If two matrices are similar, they have the same determinant, characteristic polynomial,etc. I have also known that (but I do now know the proof), we can check by using Jordan form of two matrices. I do not know if this claim is correct: ""If two matrices have the same Jordan form, they are similar to each other."" Yet, go back to the question, is it a quick way to prove? Thank you in advance.","['matrices', 'linear-algebra']"
750391,The product of two rational Dedekind cuts,"If $a,b\in \mathbb{Q}$ and $C_a$ and $C_b$ are both positive rational Dedekind cuts then $C_a\cdot
C_b=C_{a\cdot b}$. First of all this is my definition of product: Let $r,s$ Dedekind cuts such that $r,s\geq 0$. Then $r\cdot s:=C_0\cup\{pq\mid (p,q\geq 0)\wedge (p\in r, q\in s)\} $. I have no problem proving that $ C_a\cdot C_b\subseteq C_{a\cdot b}$, but the other way around. So, let $\alpha \in C_{a\cdot b}$. If $\alpha<0$ or $\alpha=0$ then according to the definition $\alpha\in C_a\cdot C_b$.  If $\alpha>0$ my idea is that since $\alpha <ab$ then $\frac {\alpha}{ab}<1$ and because we need to find $a'<a$ and $b'<b$ such that $a'b'<ab$ we could put $\frac {\alpha}{ab}=mn$ such that $m,n<1$. If this is possible then we can have $\alpha=(a\cdot m)(b\cdot n)=ab(mn)=ab(\frac {\alpha}{ab})$ where  $a'=(a\cdot m)<a$ and also $b'=(b\cdot n)<b$. Therefore $\alpha \in C_a\cdot C_b$.","['elementary-set-theory', 'real-analysis']"
750406,Volume vs. Surface Area Integrals,"In order to find the volume of a sphere radiud $R$, one way is to slice it up into a stack of thin, concentric disks, perpendicular to the $z$-axis. a disk at any point $z$ will have radius $r=\sqrt{R^2-z^2}$ and infinitesimal thickness $dz$, so the volume integral is: $$ V = \int dV = \int_{-R}^{R} \pi r^2 dz = \int_{-R}^{R} \pi (R^2-z^2) \,dz $$ The above integral of course evaluates to $\frac{4}{3}\pi r^3$, the desired answer. Now, when I tried to do the same thing for the surface area, I treated it as a thin, hollow shell, sliced it up into a stack of concentric rings (or simply the outer surface area of the disks). The area integral in that case is: $$ A = \int dA = \int_{-R}^{R} 2\pi r \,dz = \int_{-R}^{R} 2\pi \sqrt{R^2-z^2} dz $$ Unfortunately, this integral does not evaluate to $4\pi r^2$. I discovered that instead of taking the thickness of the thin rings as $dz$, I have to take the infinitesimal arclength $dL$ at point $z$ on the cross-section $x = \sqrt{R^2-z^2}$ $$ dL = \sqrt{\left(\frac{z}{\sqrt{R^2-z^2}} \right)^2 +1} \,dz = \frac{R}{\sqrt{R^2-z^2}} dz $$ Putting that in place of $dz$, the integral evaluates perfectly. Conversely, if the thickness in the volume integral was $dL$, in won't work out. This baffles me. Why do the disks have to have different thickness when you're taking the volume, compared to when you're taking the surface area? Why does it matter? When you take the limit as the number of disk slices goes to infinity and the thickness goes to zero, the sum will approach the sphere, won't it? Is this a case where the sum of those volumes will approach the sphere, but the sum of their surface areas approaches some different shape, or doesn't approach anything if the thickness is $dz$ and reverse if the thickness is $dL$? If so, why? The same thing applies for any arbitrary surface/solid of evolution. The surface area integral always involves $dL$, but the volume only has $dx$","['definite-integrals', 'applications', 'calculus', 'integration']"
750410,"""World's Hardest Easy Geometry Problem""","This question is a ""corollary"" (if you will) to the World's Hardest Easy Geometry Problem (external website). Formally, this is called Langley's Problem . The objective of that problem was to solve for angle $x^{\circ}$, with the given angles of $10^{\circ}, 70^{\circ}, 60^{\circ}, 20^{\circ}$. Someone presented a solution to that problem. Here's also a rather colorful and interactive solution to a problem like this, but with different angles. Now, I wanted to generalize this problem, replacing the angles of $10^{\circ}, 70^{\circ}, 60^{\circ}, 20^{\circ}$ with angles of $W^{\circ}, X^{\circ}, Y^{\circ}, Z^{\circ}$, respectively (see below picture). How can we derive an analytical expression of angle $x^{\circ}$, in terms of $W^{\circ}, X^{\circ}, Y^{\circ}, Z^{\circ}$?","['geometry', 'angle', 'triangles']"
750411,Do normal group endomorphisms form a normal submonoid?,"What it says on the tin. A group endomorphism $v\colon G\to G$ is called normal if $v(aba^{-1})=av(b)a^{-1}$ for all $a,b\in G$.  Equivalently, the map $g\mapsto v(g^{-1})g$ is a group homomorphism. Equivalently, the image of this map commutes with the image of $v$. $\operatorname{End}(G)$, all endomorphisms of G, is a monoid under composition. Let $M$ be the set of all normal endomorphisms of $G$. This forms a submonoid. It is normal if $vM=Mv$ for all $v\in\operatorname{End}(G)$. So is it normal? If we restrict to automorphisms, the normal automorphisms form what are also known as the central automorphism group. This is known to be a normal subgroup, and to my understanding the proof relies on the center being characteristic. This doesn't generalize to endomorphisms, though.","['finite-groups', 'group-theory', 'abstract-algebra', 'monoid']"
750467,differentials on formal schemes,"Let $A$ be a topological ring, we say that it is pseudo-compact if : there is family of ideals $\Lambda_A$ which gives a basis of neighborhoods of $0$ and such that $A/\mathfrak{a}$ is artinian for every $\mathfrak{a} \in \Lambda_A$ $A$ is complete. We have $A = \varprojlim\limits_{\mathfrak{a} \in \Lambda_A} A/\mathfrak{a}$ A topological module $M$ over a pseudo-compact ring $A$ is said to be profinite if : There is a familly of submodules $\Lambda_M$ which forms a basis of neighborhoods of $0$ and such that $M/N$ is finite over $A$ for every $N \in \Lambda_M$ $M$ is complete We have $M = \varprojlim\limits_{N \in \Lambda_M} M/N$ A profinite $A$ algebra is just an $A$-algebra which is profinite as an $A$ module, it is easy to see that it is in fact a pseudocompact ring. Now let $A$ be a pseudo-compact ring and $B$ be a profinite $A$ given by a map $f : A \to B$. (We can consider this a map of formal schemes $ :Spf(B) \to Spf(A)$ if we want to be more geometric) It is very natural to define the differentials of $B$ over $A$ to be
$$\widehat{\Omega}_{B/A} := \varprojlim\limits_{\mathfrak{b},\mathfrak{a}} \Omega_{(B/\mathfrak{b})/(A/\mathfrak{a})}$$
where the limits runs over pairs of ideal over $B$ and $A$ respectively whith $f^{-1}(\mathfrak{b}) \subset \mathfrak{a}$. Indeed we are just saying that $Spf(A)$ is the glueing of real schemes $Spec(A/\mathfrak{a})$ and $Spf(B)$ is the glueing of real schemes $Spec(B/\mathfrak{b})$ over each $Spec(A/\mathfrak{a})$ and we glue our original construction of the Kähler differentials. you can look at http://www.math.jussieu.fr/~polo/SGA3/Exp7B-23mai11.pdf for much more on the subject of pseudo-compact rings My question : I've read that $\widehat{\Omega}_{B/A}$ represent the functor taking a profinite $B$-module to the module of continous derivations from $B$ to $M$. Although it seems intuitive and I'd be glad to accept it as a fact I haven't been able to prove it myself so I'm wondering if someone could give a proof of this (with all the gory details if possible)","['algebraic-geometry', 'abstract-algebra']"
750469,Measure Theory Conjecture,"While I was doing some math here, I made this conjecture. Let $f_n:X\rightarrow \mathbb{R}$ be a sequence of measurable functions from the measure space $(X,\mathcal{A},\mu)$ to the measurable space $(\mathbb{R},\mathcal{B}(\mathbb{R}))$, and let $\mu:\mathcal{A}\rightarrow[0,\infty]$ be a positive measure. Consider that for all $\varepsilon > 0$, the sets $A_n = \{x\in X: \ |f_n(x)| > \varepsilon\}$ are such that $$\limsup_{n\to\infty}\mu(A_n) = 0.$$
Then its true that $f_n\to0$, $\mu$-almost everywhere. The thing is, I think it is true but I don't know how to prove it. You can consider $\mu$ as a probability measure if it makes more easier to prove. Thank you.","['conjectures', 'measure-theory']"
750502,moving part and fixed part of a linear system,"What is the definition of the ""moving part"" and ""fixed part"" of a linear system$|L|$? I think the fixed part should be defined to be the greatest effective divisor $F$ such that $D-F\geq 0$ for every $D$ in the system, and the moving part is the linear system $|M|=|L|-F$. Thus the fixed part is the codimension 1 part in the base locus. $|M|$ may not be point free?(but I think for the curves, it is basepoint free) If $|M|$ defines a rational map (morphism on some open subset) to $P^k$, what is its relation to the rational map defined by $|L|$? When people say moving a divisor in the moving part, does it always mean using implicitly the Bertini theorem? Is there any reference on the moving and fixed part of linear system?",['algebraic-geometry']
750535,"Show that if $f \circ g$ is surjective, then $f$ is surjective, and $g$, the function applied first, needs not to be.","Show that if $f \circ g$ is surjective, then $f$ is surjective, and $g$, the function applied first, needs not to be. (Note:$f \circ g=f(g(s))$, $f$ and $g$ are well defined) This statement originates from http://en.wikipedia.org/wiki/Surjective_function",['functions']
750551,Probability that random byte array is a valid UTF-8 string?,What is the probability that $n$-byte random byte array is a valid UTF-8 string? It doesn't care if it's NFC or NFD.,['probability']
750614,Prove $\frac{x^2}{y}+\frac{y^2}{z}+\frac{z^2}{x}\ge 4+(x-y)^2$ for positives $4 \le x + y + z \le 5$,"Let $x,y,z>0$ , and such $$4\le x+y+z\le 5.$$ Show that $$\dfrac{x^2}{y}+\dfrac{y^2}{z}+\dfrac{z^2}{x}\ge 4+(x-y)^2.$$ It seems that the condition $\dfrac{x^2}{y}+\dfrac{y^2}{z}+\dfrac{z^2}{x}\ge 4+(x-y)^2$ is maybe old, and this condition is strange? http://www.artofproblemsolving.com/Forum/viewtopic.php?f=52&t=586357 .
I want to use $$\dfrac{x^2}{y}=2x-y+\dfrac{(x-y)^2}{y}.$$ If we let $$x\to x'r,y\to y'r,z\to z'r,$$ then $$x'+y'+z'=4, r\in [1,\dfrac{5}{4}]$$ But I can't prove that either, because I felt I can't use the condition. Thank you.","['inequality', 'algebra-precalculus', 'real-analysis']"
750630,Enumeration of rational numbers,"If $\Bbb Q=\{q_n:n\in \Bbb N\}$ be an enumeration of $\Bbb Q$, is it true that $|q_n|<1/n$ for infinitely many $n$? I just come up with this question, it seemed simple but I can't solve it. Is there any idea?","['elementary-set-theory', 'analysis']"
750654,"$X$ complete normed space $\implies\mathrm B(X,Y)$ complete normed space?","$\newcommand{\N}{\mathbf N}\renewcommand{\leq}{\leqslant}\renewcommand{\geq}{\geqslant} 
\newcommand{\eps}{\varepsilon}$I was looking through the functional analysis notes of TWK (on his webpage https://www.dpmms.cam.ac.uk/~twk/ ) and was trying to answer the following question: If $X$ is a complete normed space and $Y$ is a normed space, is $\mathrm B(X,Y)$ complete? ($\mathrm B(X,Y)$ is the normed space of bounded linear operators from $X$ to $Y$, with as norm the operator norm).
It seems to me that this is not the case, and I have thought of (what I think is) a counterexample. I'm not completely sure about the last step I made and therefore would like to ask you to verify that it is correct, show me how to correct it, or tell me that my counterexample is wrong and why. Claim If $X$ is a complete normed space and $Y$ a normed space, $\mathrm B(X,Y)$ is not necessarily complete: Proof Consider $X=\ell^\infty(\N),Y=c_{00}(\N)$ ($c_{00}(\N)$ is the set of sequences of compact support), with the norm on $Y$ the $\ell^1$ norm. Let, for $n\in\N_{\geq 1}$, $T_n:X\to Y$ be given by 
$$(x_1,x_2,x_3,\ldots,x_n,x_{n+1},\ldots)\mapsto\left(\frac{x_1}{1^2},\frac{x_2}{2^2},\frac{x_3}{3^2},\ldots,\frac{x_n}{n^2},0,0,\ldots\right)$$
For every $n\in \N_{\geq 1}$, $T_n$ is linear. Furthermore each $T_n$ is bounded: Let $x=(x_n)\in X$. Then $\|x\|=\sup|x_n|<\infty$. So 
$$\|T_nx\|=\sum\limits_{k=1}^n\frac{|x_k|}{k^2}\leq \sum\limits_{k=1}^n\frac{\sup|x_n|}{k^2}=\sum\limits_{k=1}^n\frac{1}{k^2}\|x\|\leq\frac{\pi^2}{6}\|x\|$$
We conclude that for all $n\in\N_{\geq 1}, T_n\in \mathrm B(X,Y)$, because
$$\|T_n\|\leq \frac{\pi^2}{6}$$
Furthermore $(T_n)$ is a Cauchy sequence in $\mathrm B(X,Y)$: Let $\eps>0$. $\sum_{k=1}^\infty\frac{1}{k^2}$ converges, so there is an $N\in\N$ such that $\forall m,n\geq N$ we have $\sum_{k=m}^n \frac{1}{k^2}=\left|\sum_{k=m}^n \frac{1}{k^2}\right|<\frac{\eps}{2}$. Now let $x=(x_n)\in X$. Then $\|x\|=\sup |x_n|<\infty$ so for $m,n\geq N$ (wlog we assume $m<n$), we have
$$\|(T_n-T_m)x\|=\sum\limits_{k=m+1}^n\frac{|x_k|}{k^2}\leq\sum\limits_{k=m+1}^n \frac{\sup|x_n|}{k^2}<\frac{\eps}{2}\|x\|$$
And from this it follows that for $m,n\geq N$ 
$$\|T_n-T_m\|\leq \frac{\eps}{2}<\eps$$
Finally, $(T_n)$ does not converge: This is the step that I'm not completely sure about. My idea is as follows We have $\mathrm B(X,\ell^1(\N))\supset \mathrm B(X,Y)$. $\mathrm B(X,\ell^1(\N))$ is complete (because $\ell^1(\N)$ is complete) and $(T_n)$ is also a Cauchy sequence in $\mathrm B(X,\ell^1(\N))$ and so it converges to a $T\in \mathrm B(X,\ell^1(\N))$. In fact we can give $T$ explicitly, for $x=(x_n)\in\ell^\infty(\N)$ we have $(Tx)_n=\frac{x_n}{n^2}$ for each $n\in\N_{\geq 1}$. Now it is clear that $T$ does not map $\ell^\infty(\N)$ into $c_{00}(\N)$ (consider the sequence with all entries equal to $1$ and what it gets mapped to) and so $T\not\in \mathrm B(X,Y)$. If $(T_n)$ would converge in $\mathrm B(X,Y)$ then it would converge to the same limit in $\mathrm B(X,\ell^1(\N))$ (because we consider $\mathrm B(X,Y)$ as a metric subspace of $\mathrm B(X,\ell^1(\N))$) so by uniqueness of limits in $\mathrm B(X,\ell^1(\N))$ we conclude that $(T_n)$ does not converge in $\mathrm B(X,Y)$. This shows that $X$ being a complete normed space does not necessarily imply that $\mathrm B(X,Y)$ is complete.","['normed-spaces', 'functional-analysis']"
750674,Surjective Homomorphism,"For $2$ i get that $C_2 \times C_2$ is not cyclic and I understand  that if the homomorphism is surjective it must cover the entirety of $C_2 \times C_2$, but i don't follow why the image must be cyclic. $8. )$ Does there exist a surjective homomorphism from $C_{12}$ onto $C_{4}$ ? from $C_{12}$ onto $C_{2} \times C_{2}$ ? from $D_{8}$ onto $C_{4}$ ? from $D_{8}$ onto $C_{2} \times C_{2}$ ? Give reasons for your answers. (2) No: the image of any homomorphism $C_{12} \rightarrow C_{2} \times C_{2}$ must be cyclic ( as it will be generated by the image of a generator of $C_{12}$ ) So it can't be surjective .",['abstract-algebra']
750699,How to prove that $\zeta*\zeta=\zeta$?,"Let $F$ be a non-archimedean local field and $\mathcal{O}_F$ the ring of integers in $F$. Let $G_F=GL_2(F)$. Let $\pi_i$, $i=1,\ldots,n$,be non-equivalent finite dimensional irreducible representations of $GL_2(\mathcal{O}_F)$ and $$
\zeta(g):=\sum_i \dim(\pi_i) tr(\pi_i(g^{-1})).
$$ I am trying to prove that $\zeta*\zeta=\zeta$. Here $*$ is the multiplication in the Hecke algebra associated to $G_F=GL_2(F)$. We have
$$
(\zeta*\zeta)(h) \\
= \int_{G_F} \zeta(g) \zeta(g^{-1} h ) dg \\
= \int_{G_F} (\sum_i \dim(\pi_i) tr(\pi_i(g^{-1})) ) (\sum_i \dim(\pi_i) tr(\pi_i(h^{-1} g)) )  dg \\ 
= \sum_{i,j} \int_{G_F} \dim(\pi_i) \dim(\pi_j) tr(\pi_i(g^{-1}))  tr(\pi_j(h^{-1} g))  dg.  \quad (1)
$$
But how could we show that $(1) = \zeta(h)$? Thank you very much. Edit: this is an excise on the top of page 5 of the notes .","['automorphic-forms', 'algebraic-groups', 'representation-theory', 'group-theory']"
750703,"Let $Y_1, Y_2,\ldots,Y_n$ denote a random sample from the uniform distrib... Help find finding $ \text{Var}\left[\hat{\theta}_{2}\right]$","Let $Y_1, Y_2,\ldots,Y_n$ denote a random sample from the uniform distribution on the interval $(θ, θ + 1)$. Let $$ \hat{\theta}_2 = Y_{(n)} - \frac{n}{n+1}$$ Find the efficiency of $θ^1$ relative to $θ^2$ We have $Y_i\sim\mathcal{U}(\theta,\theta+1)$ and CDF of $Y_i$ based on Wikipedia $$
G_{Y_i}(y)=\Pr[Y_i\le y]=\frac{y-\theta}{\theta+1-\theta}=y-\theta.
$$
Here, $Y_{(n)}$ is $n$-th order statistics. Therefore, $Y_{(n)}=\max[Y_1,\ldots, Y_n]$. Note that $Y_{(n)}\le y$ equivalence to $Y_i\le y$ for $i=1,2,\ldots,n$. Hence, for $\theta< y<\theta+1$, the fact that $Y_1,Y_2,\ldots, Y_n$ are i.i.d. implies
$$
G_{Y_{(n)}}(y)=\Pr[Y_{(n)}\le y]=\Pr[Y_1\le y,Y_2\le y,\ldots, Y_n\le y]=(\Pr[Y_i\le y])^n=\left(y-\theta\right)^{n}.
$$
The PDF of $Y_{(n)}$ is
$$
g_{Y_{(n)}}(y)=\frac{d}{dy}G_{Y_{(n)}}(y)=\frac{d}{dy}(y-\theta)^n=n(y-\theta)^{n-1}.
$$
The expected value of $Y_{(n)}$ is
$$
\begin{align}
\text{E}\left[Y_{(n)}\right]&=\int_{y=\theta}^{\theta+1}yg_{Y_{(n)}}(y)\ dy\\
&=\int_{y=\theta}^{\theta+1}yn(y-\theta)^{n-1}\ dy\\
&=n\int_{y=\theta}^{\theta+1}y(y-\theta)^{n-1}\ dy.
\end{align}
$$ I have trouble finding  $$
\text{Var}\left[\hat{\theta}_{2}\right]=\text{Var}\left[Y_{(n)}-\frac{n}{n+1}\right]=\text{Var}\left[Y_{(n)}\right]=\text{E}\left[Y_{(n)}^2\right]-\left(\text{E}\left[Y_{(n)}\right]\right)^2.
$$ I found that
$$
\begin{align}
\text{E}\left[Y_{(n)}\right]&=n\left[\frac{y(y-\theta)^n}{n+1}-\frac{\theta(y-\theta)^n}{n(n+1)}\right]_{y=\theta}^{\theta+1}\\
&=\frac{n(\theta+1)}{n+1}+\frac{\theta}{n+1}\\
&=\theta+\frac{n}{n+1}.
\end{align}
$$ The way I calculate $E(Y_{(n)}^2)$ is the following: $$E(Y_{(n)}^2) = ny^2(y-\theta)^{n-1} = n\left[\left.y^2\frac{(y-\theta)^n}{n} \right|_\theta^{\theta+1} - \frac{2}{n} \int_\theta^{\theta+1} y(y-\theta)^n  \,dy\right]$$ $$= \left.(\theta+1)^2 - 2\left(y\frac{(y-\theta)^{n+1}}{n+1} \right|_\theta^{\theta+1} - \int_\theta^{\theta+1} \frac{(y-\theta)^{n+1}}{n+1} dy\right) = (\theta+1)^2 -2 \left(\frac{\theta+1}{n+1} - \left.\frac{(y-\theta)^{n+2}}{(n+1)(n+2)}\right|_\theta^{\theta+1}\right)$$ $$= (\theta+1)^2- 2\frac{\theta +1}{n+1} - \frac{1}{(n+1)(n+2)}$$ Then I use $E(Y_{(n)}^2) - E(Y_{(n)})^2$, based on wolframalpha which gives me the following result ...please go here ...which is different from the correct answer $\text {Var} [\hat{\theta}_2]= V(Y(n))=\frac{n}{(n+2)(n+1)^2}$....Could anyone please check why?","['statistics', 'probability-distributions', 'probability', 'probability-theory']"
750705,Is $i \in \mathbb{Q}[\sqrt[4]{-2}]$?,"I have a homework question from Artin's Algebra that asks Is $i \in \mathbb{Q}[\sqrt[4]{-2}]$? I suspect that this is not true because $i \sqrt{2} \in \mathbb{Q}[\sqrt[4]{-2}]$ and $\sqrt{2}$ is of course not rational, but I am having a hard time proving it.  Perhaps I could consider $\mathbb{Q}[\sqrt[4]{-2}] = \mathbb{Q}[x] / (x^4 + 2)$.  Now if $i \in \mathbb{Q}[\sqrt[4]{-2}]$, then $\mathbb{Q}[i] = \mathbb{Q}[x] / (x^2 + 1) \leq \mathbb{Q}[x] / (x^4 + 2)$ and $x^2 + 1 \mid x^4 + 2$, a contradiction?  I have a feeling this is not right, but I'm stuck.  Also, we haven't covered any Galois theory.  Any thoughts would be appreciated!",['abstract-algebra']
750706,Prove $f $ is identically zero if $f(0)=0$ and $|f'(x)|\le|f(x)|$,"$f:\Bbb R \to\Bbb R $ is differentiable, $f(0)=0$ and $|f'(x)|\le|f(x)|$ for all $x$ then prove $f$ is identically zero. I tried to use mean value theorem and end up in $|f(x)|\le |x||f(c)|$ for some constant $c$ which depends on $x$ . Can you help me little bit...","['derivatives', 'real-analysis']"
750806,Reduction formula for $\int \frac{dx}{x^n \sqrt{ax+b}}$,I want a reduction formula for $$I_n=\int\frac{dx}{x^n \sqrt{ax+b}}$$ in terms of $I_{n-1}$. I have tried various substitutions but I just can't seem to find the right one. Any help or hints will be appreciated,"['calculus', 'integration', 'indefinite-integrals']"
750817,Matrix graph and irreducibility,"How do I prove that if $A\in\mathbb C^{n\times n}$ is a matrix then it is irreducible if and only if its associated graph (defined as at Graph of a matrix ) is strongly connected? Update : Seeing as no-one answered for over a week, I tried to do it by myself. The first thing I did was try to show column or row permutation didn't change the strong connectedness of the graph. I didn't manage, and actually proved the opposite. On the way, though, I managed to show transposition doesn't. The argument is that transposition affects the graph in that it inverts all the arrows, but if there is a loop through all nodes then inverting the arrows means you go through it the other way round, so it's still there, and the graph stays strongly connectedness, and if there isn't, well, transposing can't make one appear, as otherwise transposing back would make it disappear, which we have proved impossible. This result may not be useful, but since I've done it I thought I might well write it down. Then I tried to think of what permuting both rows and columns does to the graph. Why that? Let's recall the notion of irreducible matrix: A matrix $A\in\mathbb{C}^{n\times n}$ is said to be reducible if
  there exists a permutation matrix $\Pi$ for which $\Pi\cdot A\cdot
> \Pi^T$ is a block upper triangular matrix, i.e. has a block of zeroes
  in the bottom-left corner. So if this operation does not alter the graph's strong connectedness, then I can work on the reduced matrix to show its graph is not strongly connected and prove one implication. Now such multiplications as in the definition of a reducible matrix, with $\Pi$ a matrix that swaps line $i$ with line $j$ - what do they do to the graph? Swapping the lines makes all arrows that go out of $i$ go out of $j$ and viceversa; swapping the columns does the same for arrows leaving $i$ (or $j$). So imagine we have a loop. Say it starts from a node other than $i$ and $j$. At a certain point it reaches, say, $i$. Before that, everything is unchanged. When the original loop reaches $i$, the new loop will reach $j$ and go out of it to the same node as it went out from $i$ to before the permutation, if that node wasn't $j$, in which case it will go to $i$. When the original loop enters $j$, the new loop enters $i$, and same as before. So basically the result is just that $i$ and $j$ swap names, and the loop is the same as before taking the name swap into account. So this kind of operations do not alter the strong connectedness of the graph. Suppose $A$ is as follows: $$A=\left(\begin{array}{c|c}\Huge{A_{11}} & \Huge{A_{12}} \\\hline
\Huge{0} & \Huge{A_{22}}\end{array}\right).$$
Suppose the $\Huge{0}$ is $m\times m$ with $m\geq\frac{n}{2}$. Then we have $m$ nodes that are unconnected to other $m$ nodes, going out. But we don't have $2m$ nodes, or have exactly that many, so those $m$ nodes are cut off from all the other $n-m$, going out. So suppose there is a loop. If it starts at one of the $m$ nodes, it can never reach the other $n-m$, and if it starts at one of those, it can reach the $m$ nodes but never get back, so maybe we have a path through all the nodes, but it can't be a loop, i.e. a closed path. So the graph is not strongly connected. Now the definition doesn't say anything about the size of those blocks, so the problem I still have is that if $m<\frac{n}{2}$, the argument above fails because we have at least one node besides the $m$ nodes and the other $m$ nodes that can't be reached from the first $m$, and that node could be the missing link. Of course, when I said ""can't be reached"" up till now, I meant ""be reached directly "", i.e. not passing through other nodes. Of course, if the above is concluded, I have proved that reducibility implies non-strong-connectedness of the graph, so that a strongly connected graph implies irreducibility. But the converse I haven't even tried. So the questions are: how do I finish the above at points 3-4 and how do I prove the converse? Or maybe I'm missing something in the definition, in which case what is it? Update 2: I think I am missing something, as a $3\times3$ matrix with a 0 in the bottom-left corner and no other zeroes does have a strongly connected graph, since the only missing arrow is $3\to1$, but we have the loop $1\to3\to2\to1$. So when is a matrix reducible? Update 3: Browsing the web, I have found some things. I bumped first into this link . Now if that is the definition of reducible matrix, then either I misunderstand the definition of the block triangular form, or the if and only if there doesn't hold, since a matrix with a square block of zeroes bottom-left definitely doesn't satisfy the disjoint set condition but definitely does satisfy the permutation condition, with no permutation at all. Maybe the first condition is equivalent to the non-strong-connection of the graph. Yes, because in that case there are $\mu$ nodes from which you can't reach the remaining $\nu$, so the graph is not strongly connected. So at least that condition implies the non-strong-connectedness of the graph. The converse seems a bit trickier. Looking for that definition, I bumped into this link . Note that no matrix there has a lone corner zero (which would be top-right as the link deals with lower triangular matrixes), and all of them satisfy the disjoint set condition in the link above. So what is the definition of block triangular matrix? If it is that there must be square blocks whose diagonal coincides with part of the original matrix's diagonal and below them there must only be zeroes, then I have finished, since the if and only if in the link above is valid, so reducibility implies non-strong-connectedness of the graph, and whoops, I'm not done yet, I still need the converse, so can someone finally come help me on that ? And if it isn't, then what the bleep is it and how do I make this damn proof?","['matrices', 'linear-algebra', 'graph-theory']"
750842,Does a proportion have to be a rational number?,"Does a proportion have to be a rational number? For example, Assume we have a square with side $2$ units. We are throwing a circle of radius $1$ unit over the square. Let $X$ be the area of the square covered by the circle relative to the area of the whole square. Hence, $X$ takes value in the interval $[0,{\pi  \over 4}]$. Is $X$ considered a proportion? Thanks in advance","['statistics', 'statistical-inference', 'random-variables']"
750858,Combinatorial packages of N items [[How many distinct Boolean Expressions can be made using N variables]],"Using just the AND operator, the number of distinct packages that can be built from a set of size $n$, is simply $2^n$. If one adds the operators OR and XOR, how many packages can be built? That is, with a set of cardinality 1, we just have 2 (the empty set and the whole set). With a set of size two, we have the empty set, the two singletons, and the three paired--one for each operator. As we go to a set of size three, I'm concerned about how associativity might affect things (A OR (B XOR C)) is different from ((A OR B) XOR C) for example. Is there a way in general to count how many distinct packages there are? For background, I'm concerned about combinatorial auctions using a very expressive language which permits building packages using these three boolean operators. Obviously the growth is exponential, but I'm hoping for a more precise figure or hints on how this could be calculated. If parentheses weren't a problem, it seems like it would be $(n,0)+(n,1)+(n,2)*3+(n,3)*3^2+\ldots (n,n)*3^{(n-1)}$ where $(n,k)$ is the binomial coefficient. This is a lower bound because a) it doesn't include parentheses and b) it doesn't consider how the items are ordered and c) I think once you go to packages of size three, items may need to appear in the expression multiple times. So yes, I'm quite lost on how to incorporate all this stuff. Many thanks! EDIT:
Perhaps this could be rephrased as ""How many distinct Boolean expressions can be made using N variables and the operators AND OR and XOR?"" I am not 100% those two questions are the same. Edit 2:
I think the answer is bound above by $2^{2^N}$ since that is the number of boolean functions on $N$ boolean variables.","['logic', 'combinations', 'combinatorics']"
750891,How to closed the sum $\displaystyle \sum_{k=0}^n \dfrac{(-1)^k(2k+1)!!}{(n-k)!k!(k+1)!}$,"How to closed the sum $\displaystyle S=\sum_{k=0}^n \dfrac{(-1)^k(2k+1)!!}{(n-k)!k!(k+1)!}$ I'm trying divide two cases $n$ odd and $n$ even. I predict that $S=\begin{cases}\dfrac{1}{2^n\left[\left(\frac{n}{2}\right)!\right]^2}, & \quad \text{if $n$ even} \\ \\ \dfrac{-1}{2^n\left(\frac{n-1}{2}\right)!\left(\frac{n+1}{2}\right)!}, & \quad \text{if $n$ odd}\end{cases}$ or I can write $\displaystyle S=\sum_{k=0}^n \dfrac{(-1)^k(2k+1)!!}{(n-k)!k!(k+1)!}= \dfrac{(-1)^n\left(\frac{2n-1-(-1)^n}{2}\right)!!}{n!\left(\frac{2n+1-(-1)^n}{2}\right)!!}$ Now I want you to help me prove it.","['summation', 'binomial-coefficients', 'combinatorics']"
750932,"Is (the proof of) Fermat's last theorem completely, utterly, totally accepted like $3+4=7$?","If a mathematician would/does make use of Fermat's last theorem in a proof in a publication, would s/he still make use of some kind of caveat, like: ""assuming Fermat's last theorem is true"" or ""assuming the proof is correct"", or would it be considered totally unnecessary to even explicitly mention that the theorem is used? I assume that the theorem is beyond (reasonable?) doubt, but is it so much beyond doubt that it can be used for anything else as well? (E.g., has the proof been checked by a computer? If such a thing is possible.) (Is there even such a thing as a degree of belief in a proof? Perhaps based on length and complexity? Or is it really a binary thing?)","['proof-writing', 'number-theory']"
750948,$\textbf{Q}$ fails to prove some correct $\forall$-rudimentary sentence,"Show that the existence of a semirecursive set that is not recursive
  implies that any consistent, axiomatizable extension of Q fails to
  prove some correct $\forall$-rudimentary sentence. I have the following facts: Every recursive function is representable in Q (and by an $\exists$-rudimentary formula). Every recursive relation is definable in Q (and by an $\exists$-rudimentary formula). If $T$ is a consistent, axiomatizable theory containing Q : Every set semi-definable in $T$ is semirecursive. Every set definable in $T$ is recursive. Every total function representable in $T$ is recursive. Are there any examples of semirecursive sets where this structure holds? Or must their existence be proven mathematically?","['logic', 'computability', 'functions']"
750978,Using the Jordan form Complex,Let $C$ be a complex $n \times n$ matrix with $\det C \neq 0$. Prove that there is a complex matrix $B$ such that $C = e^B$ Hint: use the Jordan form matrices for comlexas,"['linear-algebra', 'ordinary-differential-equations']"
751002,How do I simplify this difference of angles expression using conjugates?,"I'm trying to fill in the gaps in my knowledge of simplifying rational expressions using conjugates, but this one stumps me. Given $\tan(\frac{\pi}{4}-\frac{\pi}{6})$, I can work the formula down to: $$\frac{9-3\sqrt{3}}{9+3\sqrt{3}}$$ Then I attempt to multiply top and bottom by the denominator's conjugate $9-3\sqrt{3}$, only to get something like: $$\frac{-27\sqrt{3}-27\sqrt{3}+108}{54}$$ which of course is wrong. The correct answer should be $2-\sqrt{3}$. Thanks in advance for any help. I simply can't figure out where my distribution is going wrong...","['trigonometry', 'algebra-precalculus']"
751029,Taylor series of an integral function,"Problem $$I(x) = \int_{1}^x \frac{e^t - 1}{t}$$ Find $I'( \sqrt{x} )$. Solution We know that $F'(x) = f(x)$ by the fundamental theorem of calculus so $$I'(x) = \frac{e^t -1}{t}$$ And so $$I'( \sqrt{x}) = \frac{ e^{\sqrt{x}} -1 }{ \sqrt{x}}$$ Problem Find the fourth taylor polynomial of $I(x)$ around $0$ (Guess this makes it a Maclaurin series?) Taylor series: $$ \sum_{n=o}^\infty \frac{ f^{(n)} (a) }{n!} (x-a)^n$$ So as we know the first derivative, we can compute the first Taylor expansion: $$\frac{e^t-1}{t} (x)^n$$ Do I just continue like this? Am I doing this whole problem right? I'm just not feeling sure at all!","['calculus', 'integration', 'taylor-expansion']"
751042,Recurrence relation practice problem that I can't figure out,"Thanks for taking the time to look at this problem. I'm trying to prepare for a test on Monday by doing some extra odd numbered problems from my textbook. I'm having a lot of trouble trying to solve one of these problems involving recurrence relations: s(n) = s(n/2) + 3, where s(1) = 3 and n=2^m.
* n=2^m implies that n must be even * This isn't a homogeneous recurrence relation, so I'll have to find the associated homogeneous relation and then solve for the remaining term using the 'best guess technique'. Thanks a lot!","['recurrence-relations', 'discrete-mathematics', 'recursion']"
751047,"The phrases ""has ... in "" vs. ""contains ... of"" in Baby Rudin",Consider the following two statements. (Assume $E \subseteq K$.) $E$ has a limit point in $K$. vs. $E$ contains a limit point of $K$. What do they each mean and how are they different?,"['general-topology', 'definition', 'analysis']"
751064,Use Green's function to find solutions for the boundary value problem,"Find a solution using Green's functions
$$y''+y=t;  y(0)=0, y(1)=1$$
So far I have
$$x(t)=c_1 \cos(t)+c_2 \sin(t)$$
so
$$y_1=\cos(t), y_2=\sin(t)$$
and $$W(y_1,y_2)=-1$$
When I put that in the integral for Green's function I get $$(x)t=\int^t_0 (s\cos(t) \sin(s))\:\mathrm{d}s - \sin(t) \int^1_t (s\cos(s))\:\mathrm{d}s$$ 
so I end up getting
$$-\cos(t)\sin(t)+t\cos^2 (t)-\sin(t)\cos(1)-\sin(t)\sin(1)+\sin(t)\cos(t)+t\sin^2 (t)$$
I think I did something wrong at the beginning, but I am not sure what. I do not think I should be getting $\sin(1)$ or $\cos(1)$ in my answer.","['boundary-value-problem', 'ordinary-differential-equations']"
751066,What is the use of iterating over a function?,"If we have a function, say: $$ f(x) = 3x $$ We can get output values based on linearly increasing input: $$ f(1) = 3(1) = 3 $$
$$ f(2) = 3(2) = 6 $$
$$ f(3) = 3(3) = 9 $$
$$ ... $$ Or, we can ""iteratate"" over the function, by taking the last output as input: $$ f(1) = 3(1) = 3 $$
$$ f(3) = 3(3) = 9 $$
$$ f(9) = 3(9) = 27 $$
$$ ... $$ But this is essentially equivalent to: $$ f(x) = 3^x $$ So why iterate over a function when we can just define it in another way? Is defining a function to be iterated over easier than defining for linear input? Are there functions that cannot be defined a different way? What is the practical purpose of iteration (eg. is there a branch of mathematics in which this is useful)?",['functions']
751076,First eigenvalue of Laplacian and Poincaré inequality,"Any idea on how to solve: $\int_{\Omega} |\nabla u|^2 d^n x=\lambda_1\int_{\Omega}u^2 d^n x$, with $u\in H^1_0(\Omega)$ and $\Omega\subset\mathbb{R}^n$, and $\lambda_1$ the first eigenvalue of the negative Laplacian with Dirichlet boundary conditions on $\Omega$ - I am suspecting $u$ is the eigenfunction corresponding to $\lambda_1$, but I am looking for a rigorous proof.","['sobolev-spaces', 'ordinary-differential-equations', 'functional-analysis']"
751080,Prove that $\sqrt{5}$ exists,"Prove that $\sqrt{5}$ exists; in other words prove that there exists a positive number $x\in \mathbb R$ satisfying $x^2=5$ Here's what I've done: I let $A= \{x>0:x^2\leq 5\}$ We know that $A$ is not empty because clearly $2$ is in it: $2^2<5$ and we also know that $A$ is bounded by $3$ because $$x>3\implies x^2>9$$ so by completeness, $\sup A$ exists. I let $α = \sup A$ So now I'm trying to prove that $α^2=5$ (that there exists a positive number $α\in \mathbb R$ satisfying $α^2=5$) and to do this I'm going to try to show that $α^2<5$ and $α^2>5$ are impossible. Case 1: $α^2<5$ to do this I'm assuming a proof by contradiction will work. So, suppose $α^2<5$ then ... Here's where I'm a bit lost, I don't know how to proceed with a proof by contradiction here. If anyone can give me hints or explain what I should do next that would be really appreciated.",['calculus']
751089,What is Cramer's rule used for?,"Cramer's rule appears in introductory linear algebra courses without comments on its utility.  It is a flaw in our system of pedagogy that one learns answers to questions of this kind in courses only if one takes a course on something in which the topic is used. On the discussion page to Wikipedia's article on Cramer's rule, we find this detailed indictment on charges of uselessness , posted in December 2009. But in the present day, we find in the article itself the assertion that it is useful for solving problems in differential geometry; proving a theorem in integer programming; deriving the general solution to an inhomogeneous linear differential equation by the method of variation of parameters; (a surprise) solving small systems of linear equations.  This one is what it superficially purports to be in linear algebra texts, but then elementary row operations turn out to be what is actually used. At some point in its history, the Wikipedia article asserted that it's used in proving the Cayley–Hamilton theorem , but that's not there now.  To me the Cayley–Hamilton theorem has always been a very memorable statement, but at this moment I can't recall anything about the proof. What enlightening expansions on these partial answers to this question can the present company offer?","['applications', 'linear-algebra', 'determinant']"
751106,Complex number with 3 dimensions [duplicate],"This question already has answers here : Why are the only associative division algebras over the real numbers the real numbers, the complex numbers, and the quaternions? (4 answers) Closed 10 years ago . I was looking back on complex analysis and asked myself: ''Why is there no complex number in 3 dimensions ?''. To place this question let me define with what I mean with 3 dimensions in the following. When we first explore the complex numbers we start by looking at roots of negative numbers where we define (in order to solve equations with negative roots):$$i=\sqrt{-1}.$$
Using this imaginary unit we can define a complex number as $$z=a+ib,\text{ with } z_1z_2=(a_1a_2-b_1b_2)+i(a_1b_2+a_2b_1),$$ or written as a 2-tupel:$$z=(a,b),\text{ with } z_1z_2=(a_1a_2-b_1b_2,a_1b_2+a_2b_1).$$The complex number has one real variable and one imaginary variable, so it can be represented as in a 2-dimensional plane. If we make it more complex (no pun intended), we arrive at the quaternions , which are an extention of the complex numbers, here we have 3 imaginary units $i$, $j$ and $k$ with the property:
$$i^2=j^2=k^2=ijk=-1.$$This number can be represented in a 4-dimensional space (so I say it has 4 dimensions). Now I was wondering if there might exist some kind of complex number with only 2 imaginary units $i$ and $j$ which might be represented as a number in the 3 dimensional plane ? 
If i look at the way the quaternions are constructed I think the answer is maybe, the reasoning I have for this is that given 2 imaginary numbers $i$ and $j$ we could construct it analogous and state that $$i^2=j^2=-1,$$ but what to do with the product $ij$? The product should square to one so it should be minus one or one:$$ij=-1\quad\text{ or }\quad ij=1.$$If it squared to minus 1 it would be a new complex number, and then we would get that one of the numbers $i$ or $j$ is redundant (unless we itroduce a third one of course), so we continue with the demand that the second relation holds. Doesn't this give a valid way of constructing 3D-complex numbers ? Now for the quaternions itself you could argue the same and you could also choose that $ijk=+1$, is there a reason that the $-1$ is chosen (maybe this coincides with rotations in 3D ?)?","['complex-numbers', 'quaternions', 'complex-analysis']"
751108,How to show $a+b+ad\geq c+d+bc$ given $a\geq c$ and $a+b\geq c+d$?,"Let $0\leq a,b,c,d\leq 1$ and $a\geq c$ and $a+b\geq c+d$. Show that $a+b+ad\geq c+d+bc.$ Of course we have $a+b\geq c+d$, but how to relate $ad$ and $bc$?","['inequality', 'algebra-precalculus']"
751119,Using the topology of uniform convergence for functions over non-compact spaces,"Let $(X, d)$ be a (complete) metric space, and $C(X)$ be the space of continuous maps over $X$. If $X$ is compact, one often uses the topology of uniform convergence when analyzing $C(X)$. If $X$ is non-compact, there are a bunch of other topologies that one can attach to $C(X)$: the compact-open topology, the strong topology, ... My question is the following: Why don't we just use the topology of uniform convergence on $C(X)$ regardless of the compactness of $X$? Generally, the first answer I get to this question is that the metric $d(f, g) = \sup_{x \in X} d(f(x), g(x))$ becomes unbounded when $X$ is not compact. Why is that a problem? We can always consider it an extended metric, and all the theory carries over without any problems . Even if infinite-valued metrics turn out to break the theory somehow, we can always use $d'(f, g) \equiv d(f, g)(1 + d(f, g))^{-1}$. Therefore, I am suspecting that the topology of uniform convergence has some other disadvantage(s) when treating functions over non-compact spaces. What are these disadvantages? What concern(s) made mathematicians invent the other topologies? Why is this topology not used commonly?","['general-topology', 'metric-spaces', 'functional-analysis']"
751143,Tensor Laplacian,"For a general tensor $T_{\mu_1 \dots \mu_n}$ on a (pseudo-)Riemannian manifold, is it true that $$\Delta (T_{\mu_1 \dots \mu_n})= (\Delta T)_{\mu_1 \dots \mu_n}?$$ In general, it is not true that $(\nabla_{\nu}T)_{\mu}$ versus $\nabla_{\nu}(T_{\mu}),$ where $\nabla$ is the induced covariant derivative. However, the coordinate formula given in this article seems to imply that equality does hold for the Laplacian. Attempt at a solution: If I consider the simplest case where $T$ is a $(1,0)$ vector field, then $$\Delta (T_{\mu}) = \Delta (T(dx^{\mu})) = \Delta T (dx^{\mu})+ 2 \nabla^{\lambda} T \nabla_{\lambda} (dx^{\mu}) + T(\Delta (dx^{\mu})).$$ So it would suffice to show that the second and third term on the right hand side vanish. However, I don't see why they should...","['tensors', 'riemannian-geometry', 'differential-geometry', 'general-relativity']"
751146,Show that $A + A^{-1} \geq 2I$ for $A > 0$.,"For a positive matrix $A$.  Here, we assume that all positive matrices are self-adjoint.  Show that 
\begin{align}
A + A^{-1} \geq 2I.
\end{align} Here, $A≥0$ means that A is self-adjoint and for all $x∈ℂn,⟨Ax,x⟩≥0.$",['linear-algebra']
751156,Help with epsilon-delta proof that 1/(x^2) is continuous at a point.,"I'm trying to prove that $\lim_{x \to x_0} \frac{1}{ x^2 } = \frac{1}{ {x_0}^2 }$. I know this means that for all $\epsilon > 0$, I must show that there exists a $\delta > 0$ such that $\left | x - x_0 \right | < \delta \Rightarrow  \left | \frac{1}{ x^2 } - \frac{1}{ {x_0}^2 } \right | < \epsilon$. Since $f(x) = \frac{1}{x^2}$ is an even function, I'll restrict the domain to $x>0$. After some manipulation of $\left | \frac{1}{ x^2 } - \frac{1}{ {x_0}^2 } \right | < \epsilon$ I have $\left | x - x_0 \right | < \epsilon\frac{x^2{x_0}^2}{x+{x_0}}$. I know that I can't just let $\delta = \epsilon\frac{x^2{x_0}^2}{x+{x_0}}$ since $\delta$ should not depend on $x$, so I need something else. This is where I'm stuck. I think I need to choose an upper bound for $\left|x-x_0\right|$ which depends on $x_0$ and find a sufficient $\delta$ from this restriction, but I'm not sure how to go about this.","['epsilon-delta', 'real-analysis', 'limits']"
751159,Hessian matrix of $g\circ f$,"Say, $f:\mathbb R^n\to\mathbb R^k$ and $g:\mathbb R^k\to\mathbb R$ are both $C^2$. I'd like to express the Hessian matrix of $g\circ f$ $$\left( \frac{\partial^2(g\circ f)}{\partial x_i \partial x_j} (x) \right)_{i,j\in\{1,\ldots, n\}}$$ in terms of partial derivatives of $g$ and $f$. I know that $$(g\circ f)''(x)[v,w]=f''(g(x))\Big[g'(x)[v], g'(x)[w]\Big]+f'(g(x))\circ g''(x)[v,w]$$ yet I have problems writing it out in terms of partial derivatives.","['multivariable-calculus', 'tensors', 'derivatives']"
751167,(Counting problem) very interesting Modular N algebraic eqs - for combinatorics-permutation experts,"Experts in algebra please help: we would like to know the number of solutions for this set of six of modular N algebraic equations: $$
(1) \quad x_1 y_2 \equiv x_2 y_1 \pmod{N}\\
(2) \quad x_1 y_3 \equiv x_3 y_1 \pmod{N}\\
(3) \quad x_4 y_1 \equiv x_1 y_4 \pmod{N}\\
(4) \quad x_2 y_3 \equiv x_3 y_2 \pmod{N}\\
(5) \quad x_2 y_4 \equiv x_4 y_2 \pmod{N}\\
(6) \quad x_3 y_4 \equiv x_4 y_3 \pmod{N}
$$
Suppose that all variables are modulo $N$. Here there are eight variables in total, $x_1,x_2,x_3,x_4, y_1,y_2,y_3,y_4$. Questions: (a) For $N=2$, how many independent solutions there are? (b) For $N=3$, how many independent solutions there are? (c) For a generic prime number $N$, how many independent solutions there are? I emphasize these questions about counting number(how many) of independent solutions , so explicit solutions for these equations are NOT required. Hint (what I have known): the 6 constraint equations are not independent, actually the first equations (1),(2),(3) can guarantee the last three (4)(5)(6)equations. So counting the number of degrees of freedom, roughly the number of independent solutions are about $$
\text{ the number of independent solutions} \geq N^{8-3}=N^5
$$ On the other hand, if we set all $x_1,x_2,x_3,x_4=0$, then there are free choices for $y_1,y_2,y_3,y_4$ (which provides $N^4$ solutions). Similarly, if we set all $y_1,y_2,y_3,y_4=0$ , then there are free choices for $x_1,x_2,x_3,x_4$ (which provides $N^4$ solutions, but overlap one solution such that all $x=y=0$ counted earlier once.), so roughly the number of independent solutions are about $$
\text{ the number of independent solutions} \geq 2\;N^{4}-1
$$ So what is the exact number expression of this number of independent solutions ?","['discrete-mathematics', 'abstract-algebra', 'linear-algebra', 'combinations', 'combinatorics']"
751174,A number related to the roots of a quartic polynomial is a root of a cubic polynomial,"So here is the problem, $a$ and $b$ are two distinct real roots of $f(x)=0$ where $f(x)=x^4-6x+3$, show that $(a+b)^2$ is a root of $g(x)=x^3-12x-36$. I have tried many methods, such as substitution, expanding the polynomial, changing it to different form, and reduce the power of $x$, but still could not make any process. Can anyone help me for some suggestion? Thank you very much.","['algebra-precalculus', 'roots', 'polynomials']"
751182,A limits question,"The following equals?
$$
\lim_{x \to 1}\frac{\displaystyle\int_1^x \sin(t) \, dt}{x^2-1}
$$
I think this can be converted to
$$
\lim_{x \to 1}\frac{\sin(x)}{2x} = \frac{\sin(1)}{2}
$$
by using the fundamental theorem of calculus. But the correct answer is $1/2$. So where I made a mistake?",['limits']
751185,Subadditivity for Analytic Capacity Disjoint Compacts separated by a Line,"The following problem is asked in Greene and Krantz, Problem 9, page 382: Suppose that $C_1$ and $C_2$ are disjoint compact sets in $\mathbb{C}$ that can be separated by a line $l$ with $C_1 \cap l = C_2 \cap l = \emptyset$. Show that
  $$\gamma(C_1 \cup C_2) \leq \gamma(C_1) + \gamma(C_2).$$ Here, $\gamma(C)$ is the analytic capacity of the compact set $C\subset \mathbb{C}$. All my ideas to solve this problem reach a dead end pretty quickly. Two ideas initially felt right. One was Schwarz reflection. The other was finding an open disk containing $C_1$ but that is disjoint with $C_2$, then use the Cauchy integral formula on this disk to help define a function that is holomorphic on $\mathbb{C} \backslash C_2$ that has norm $\leq$ 1. I don't think either of these ideas are useful.",['complex-analysis']
751187,What property of a matrix causes $\|e^{tA}\|_2$ to oscillate as $t\rightarrow\infty$?,"What property of a matrix causes $\|e^{tA}\|_2$ to oscillate as $t\rightarrow\infty$? The best I can come up with is that $A=bi\cdot M$ for $b$ a non-zero real number and $M$ a non-zero idempotent matrix, since in that case we have:
$$\|e^{tA}\|_2 =  \left\|\sum_{i=0}^{\infty}\frac{(tbi)^k\cdot M^k}{k!}\right\|_2= \left\|I+M\sum_{i=1}^{\infty}\frac{(tbi)^k}{k!}\right\|_2= \|I+Me^{tbi}\|_2,$$ which quite clearly oscillates.  However it's certainly not clear to me that this is the only way to obtain oscillation, and if it is, I don't see how to go about proving it.","['matrices', 'linear-algebra', 'limits']"
751195,(Counting problem) more challenging Modular N algebraic eqs - for combinatorics-permutation experts,"Experts in algebra please help - Part II after Part I : we would like to know the number of solutions for this set of six of modular N algebraic equations: $$
x_1 y_2 = x_2 y_1 \pmod N \qquad (1) \\
x_1 y_3=x_3 y_1 \pmod N \qquad (2) \\
x_4 y_1 \neq x_1 y_4 \pmod N \qquad (3) \\
x_2 y_3=x_3 y_2 \pmod N \qquad (4) \\
x_2 y_4 \neq x_4 y_2 \pmod N \qquad (5) \\
x_3 y_4 \neq x_4 y_3 \pmod N \qquad (6)
$$ suppose that all variables are module $N$, i.e. $x_j=x_j \pmod N$ and $y_j=y_j \pmod N$. Here there are $x_1,x_2,x_3,x_4, y_1,y_2,y_3,y_4$ eight variables. NOTE: Here Eq(3),(5),(6) are required to be inequality. Questions: (a) For $N=2$, how many independent solutions there are? (b) For $N=3$, how many independent solutions there are? (c) For a generic prime number $N$, how many independent solutions there are? I emphasize these questions about counting number(how many) of independent solutions , so explicit solutions for these equations are NOT required. Hint (what I have known): is discussed in Part I , an easier question. Here this one is less clear to me. I only know this number $\leq N^8$. So what is the exact number expression of this number of independent solutions ? Thank you. :o)","['permutations', 'discrete-mathematics', 'abstract-algebra', 'linear-algebra', 'combinatorics']"
751205,Generalized Pythagorean triples construction.,"All primitive Pythagorean triples $(a, b, c) : \{ a^2 + b^2 = c^2 \} \wedge \{ a \equiv 0 \pmod{2} \}$ can be expressed in the form:$$\{ a = 2pq, b = p^2 - q^2, c = p^2 + q^2 \}$$ for positive integers $p, q : \{ \gcd(p,q) = 1 \} \wedge \{ p \not\equiv q \pmod{2} \}$. I conjectured that this also holds for imprimitive Pythagorean triples (in this case $p,q$ are not necessarily relatively prime and of opposite parity). However, I could not find any counterexamples and currently I am stuck in the developing of a proof. That is why I am appealing to you. I would really appreciate any counterexamples, proofs, ideas, etc. Thank you.",['number-theory']
751208,Density of a set of functions in Schwartz space,"I have a difficulty doing the following problem: Let $S(\mathbb{R}^n)$ be the Schwartz space. I need to determine whether the following set of functions $A$: $$A= \{f\in S(\mathbb{R}^n): \text{supp}(\hat{f}) \text{ is compact and } 0\not\in \text{supp}(\hat{f})\}$$ is dense in $L^p(\mathbb{R}^n)$, where $\hat{f}$ is the Fourier transform of $f$. I think that the above space is dense in $L^p$ when $p\in (1,\infty)$, but not when $p=1$ or $\infty$, but I don't know how to prove it. Any help is appreciated.","['harmonic-analysis', 'fourier-analysis', 'real-analysis', 'analysis']"
751214,infinitely descending natural numbers,"Show that there is no infinitely descending sequence of natural numbers. I was thinking that there exists no infinite descending chain on the natural numbers, since every chain of natural numbers has a minimal element. And so it reaches a finite minimum. P.s I am just looking for a well built solution, since i cannot express the proof very clearly","['sequences-and-series', 'analysis']"
751217,Evaluating $\lim\limits_{x\mathop\to\infty}\frac{\tan x}{x}$,"I need to find $$\lim\limits_{x\mathop\to\infty}\frac{\tan x}{x}$$
For some reason mathematica just returns my input without evaluating it. For what it's worth, $\dfrac{\tan(10^{100})}{10^{100}}\approx -4\times10^{-101}$, so the limit is probably $0$. (...) I'm guessing this has been asked before but I can't find it.",['limits']
751227,LogSine Integral $I=-\int_0^{\pi/3} \ln^2\big(2\cos \frac{\theta}{2}\big) d\theta$,"These are known as LogSine integrals at $2\pi/3$, so I will call the integral Ls as this is common in the literature.  I am trying to prove
$$
Ls=-\int_0^{\pi/3} \ln^2\big(2\cos \frac{\theta}{2}\big) d\theta=-\frac{13\pi^3}{162}-2Gl_{2,1}\big(\frac{2\pi}{3}\big)
$$
where $Gl_{2,1}$ can be reduced to one-dimensional polylogarithmic constants.  I know we can write
$$
\ln^2\big(2\cos \frac{\theta}{2}\big) =\big(\ln 2+\ln \cos\frac{\theta}{2}\big)^2=\ln^2 2+\ln^2 \cos \frac{\theta}{2} +2\ln 2 \ln \cos \frac{\theta}{2},
$$
but am totally stuck at this point.  Thanks","['calculus', 'integration', 'definite-integrals', 'real-analysis', 'contour-integration']"
751229,Order statistics finding the expectation and variance of the maximum,"Let $X_1,X_2,\ldots,X_n$ be a collection of independent uniformly distributed random variables on the interval from $0$ to $\theta$.
The question has three parts. Find the CDF of $F_{x_n}$(x) of $X_n = \max{\{X_1,\ldots,X_n}\}$
I know that distribution for the uniform distribution is $\frac{1}{b-a}$.
In this case $b=\theta$ and $a=0$. So the pdf is $\frac{1}{\theta}$.
Then the pdf for $X_n$ should be $\left(\frac 1 \theta \right)^n$. The PDF is then $n\left(\frac 1 \theta \right)^{n-1}$ . Part c asks to calculate the mean and variance for $X_n$.
I'm confused on how to do this. Since it is a uniform distribution should I just use the uniform distribution pdf to calculate the expectation and variance?","['statistics', 'probability']"
751234,A differential equation question,"I totally have no idea about this... If $\frac{dy}{dx} = \sqrt{y^2+1}$, then $\frac{d^2y}{dx^2}$=? The correct answer is $y$.",['ordinary-differential-equations']
751254,How to show no periodic orbits exist,"I am trying to show that no periodic orbits exist for the system: $$ x_1'=y+x^2+xy^3$$
$$y'=-2x-y^3$$ I have tried using Dulac's criterion to find a function $g(x,y)$ such that $\Phi(x,y)$ given by : $$\Phi(x,y)=\frac{\partial(gx')}{\partial x}+\frac{\partial(gy')}{\partial y}$$ Is always $>0$ or $<0$ . But I have had no luck guessing such $g(x,y)$ and leaving $g(x,y)$ general results in a complicated first order PDE. Plotting the system on the phase space seems to imply there is no periodic orbits and that the origin is a focus (although a very slowly converging one) . There is one other critical point at $(x,y)=(-2^{1/5} , 2^{2/5})$ . Any ideas how I can show that no periodic orbits exist?","['ordinary-differential-equations', 'chaos-theory']"
751275,Simple series convergence/divergence: $\sum_{k=1}^{\infty}\frac{2^{k}k!}{k^{k}}$,"I have the following problem:
$$\sum_{k=1}^{\infty}\frac{2^{k}k!}{k^{k}}$$ I only need to find whether the series converges or diverges. My initial thinking was to use the ratio test. I hit a stump after simplifying my limit, though: $$\lim_{k}\frac{2k^{k}(k+1)}{(k+1)^{k+1}}$$ Normally I'd just derive by L'Hopital's Rule, but the derivative is pretty long and complicated. I doubt that's the correct route.","['convergence-divergence', 'sequences-and-series']"
751289,Systems of Linear Differential Equations - Is this Correct?,"I have to solve the following first-order linear system, $x(t)$ represents one population and the $y(t)$ represents another population that lives in the same ecosystem: (Note: $'$ denotes prime) Note: $x(t)$ and $y(t)$ must be found using the Reduction method. $x'=−5x−20y$ (Equation 1) $y'=5x+7y$ (Equation 2) I start off with finding the derivative of (Equation 1) which gives me: $x''=−5x'−20y'$ and then substitute $y'$ using the equations above (Equation 2): $x''=−5x'−20(5x+7y)$ foil expansion: $x''=−5x'−100x−140y$ (Equation 3) so I find by by using (Equation 1): $$
\begin{align}
x' &=-5x-20y\\
x'+5x &= -20y\\
\frac{x'+5x}{-20} &= y\\
y&=- \frac 1 {20}(x'+5x)
\end{align}$$ After finding this y, I insert it into (Equation 3) $x''=-5x'-100x-140(-\frac 1 {20}(x'+5x))$ And expand thus getting: $$x''=-5x'-100x+7x'+35x$$ Collect 'like' terms and put on LHS. $$x''-2x'+65x = 0$$ Then put into auxillary form: $$r^(2)−2r+65=0$$ Find roots: $$r=1±8i$$ I can now say that $x(t)= C_1 e^t \sin8t + C_2 e^t \cos8t$ (Note: $\frac {C_1} {C_2}$ are constants) Which I simplified to: 
$$x(t) = e^t(C_1 \sin8t + C_2 \cos8t)$$ Now I need $y(t)$. I will begin with finding the derivative of $x(t)$.
$$
x'(t) = e^t((C_1-8C_2)\sin8t + (8C_1+C_2)\cos8t)$$ $$x'(t) = e^t (C_1 \sin8t - 8C_2 \sin8t + 8C_1 \cos8t + C_2 \cos8t)$$ I input this into the original equation (Equation 1): $x'=3x-5y$ now becomes $$e^t C_1 \sin8t - e^t 8C_2\sin8t + e^t8C_1 \cos8t + e^t C_2 \cos8t = -5 (e^t C_1 \sin8t + e^t C_2 \cos8t - 20(b(t))$$ I expand, collect 'like' terms, find LCD, and determine that: $$b(t) = \dfrac{4C_2 e^t \sin8t - 4 C_1 e^t \cos8t - 3 C_1 e^t \sin8t - 3 C_2 e^t \cos8t}{10} $$ My question: Are my $x(t)$ and $y(t)$ correct using the methods shown above (which is represented using an example in my book).","['linear-algebra', 'ordinary-differential-equations']"
751305,"Spelling has deteriorated by the year of 2075, how many spellings are possible?","By the year 2075, spelling has deteriorated such that the dictionary now defines the spelling of the word “RELIEF” to be any combination (with repetition allowed) of the letters F, L, R, I and E subject to these constraints: The number of letters must not exceed 6; The word must contain at least one L; The word must begin with a string of at least one R’s and end with a string of at least one F’s, and there are no other R’s and F’s; Questions: How many spellings are possible?
(Hint: You can consider the pattern of R/F’s and the letters in between separately, because the choices for them are independent.) What is the fifth spelling before RELIEF, in dictionary order?",['discrete-mathematics']
751331,"functoriality of $K(G,1)$ spaces in a particular situation involving complex elliptic curves","I apologize if the subject doesn't accurately describe my question. Let $F_2$ denote the free group on two generators. Suppose you have some group homomorphism $A : \mathbb{Z}^2\rightarrow\mathbb{Z}^2$. There are many ways to lift $A$ to a map $\alpha : F_2\rightarrow F_2$ (ie, the square diagram with vertical maps being abelianizations, commutes). Firstly, am I right in saying that any two lifts of $A$ to an endomorphism of $F_2$ differ by some inner automorphism of $F_2$? Now let $\alpha : F_2\rightarrow F_2$ denote a particular lift of the map on $\mathbb{Z}^2$ sending $(x,y)\mapsto(mx,my)$ for some positive integer $m$. Let $E$ be a complex elliptic curve (ie, a complex torus). Let $[m] : E\rightarrow E$ denote the multiplication by $m$ map. Let $E^*$ denote the same curve, with the identity point removed. We identify $F_2$ with $\pi_1(E^*)$, and $\mathbb{Z}^2$ with $\pi_1(E)$, with suitable base points. The inclusion maps $E^*\hookrightarrow E$ on the level of fundamental groups become exactly abelianization maps $F_2\rightarrow \mathbb{Z}^2$. The map $[m]$ gives some map $\mathbb{Z}^2\rightarrow\mathbb{Z}^2$, so we're in a similar situation to what I described earlier. Since the universal cover of $E,E^*$ are $\mathbb{C},\mathcal{H}$(the upper half plane) respectively, both are Eilenberg Maclane spaces. By a theorem described in Hatcher, if $Y$ is an Eilenberg-Maclane space, and $X$ a CW complex, then for homomorphism $\pi_1(X,x_0)\rightarrow\pi_1(Y,y_0)$, there is a map $X\rightarrow Y$ sending $x_0$ to $y_0$ which induces the given map on fundamental groups. Furthermore, this map is unique up to homotopy fixing $x_0$. My questions are as follows: Is the theorem true for $X = E^*$? (ie, is it true that $E^*$ can be realized as a CW-complex or perhaps the theorem can be generalized to include the case $X = E^*$?) Assuming the answer to (1) is yes, there should be a map $E^*\rightarrow E^*$ inducing $\alpha$ on fundamental groups. How can we best describe this map? This map certainly cannot be a restriction/lift of $[m]$, since every $m$-torsion point would want to map to the identity of $E$, which is not in $E^*$. On the other hand, this map is ""unique"", which generally says it should somehow be close to a restriction/lift of $[m]$ to $E^*$... Any help would be appreciated. Thanks, Will","['general-topology', 'algebraic-geometry', 'algebraic-topology', 'elliptic-curves']"
751349,"Where did this ""+1"" term come from for this inductive proof?","Where did this ""+1"" term come from for this inductive proof? It is in boxed in black. For context, We are trying to prove this sequence: has the following solution: $$x_{ n }=\frac { 3^{ n+1 }-3-2n }{ 4 }$$","['induction', 'discrete-mathematics', 'proof-verification']"
751353,Applying the Implicit Function Theorem - how to evaluate the partial derivatives that arise?,"The problem: We have a function $f: \mathbb{R}^3 \rightarrow \mathbb{R}^2$ such that $f(x,y,z) = (x-xy, x+2y+z^2)$. For a point $(a,b,c)$ such that $f(a,b,c) = 0$, find a condition on $(a,b,c)$ such that there exists an open interval V containing $a$ and an open set W containing $(b,c)$ such that $g: V \rightarrow W$ exists and is differentiable, and $f(x,g(x)) = 0$ for all $x \in V$. My work: We need to use the implicit function theorem, so create the matrix of partial derivatives. The required condition is given when the determinant is non-zero. Work out the entries individually: For notational convenience, let $f^1 = x-xy, f^2 = x+2y+z^2$ $$a_{11} = \frac {\partial f^1}{\partial y}(a) = -x|_a = -a\\
a_{12} = \frac {\partial f^1}{\partial z}(a) = 0\\
a_{21} = \frac {\partial f^2}{\partial y}(a) = 2\\
a_{11} = \frac {\partial f^2}{\partial z}(a) = 2z|_a = ?$$ My question is how to evaluate this last equation. Do I just substitute $z=a$? It seems like I shouldn't. I did that for the first equation since $a$ is the first entry of the ordered triple $(a,b,c)$ in $\mathbb{R}^3$, but the value corresponding to $z$ would be $c$, so do I sub that in instead?","['multivariable-calculus', 'implicit-function-theorem']"
751375,LogSine Integral $\int_0^{\pi/3}\ln^n\big(2\sin\frac{\theta}{2}\big)\mathrm d\theta$,"I am trying to integrate a special case of the log sine integral $\rm{Ls}_n(\sigma)$ at $\sigma=\pi/3$ : $$
\rm{Ls}_{n}\big(\tfrac{\pi}{3}\big)=-\int_0^{\pi/3}\bigg[\ln\big(2\sin\tfrac{\theta}{2}\big)\bigg]^{n-1}\mathrm d\theta
$$ where $n$ is a non-negative integer.
This problem is strongly related to the hypergeometric form of the Log Sine integral. The closed form is rather simple, although I am having trouble computing it. We can use standard log rules on the inside expression, although I am not sure how this will help us...Thanks","['improper-integrals', 'calculus', 'integration', 'definite-integrals', 'contour-integration']"
751403,How do I prove that $f'(z)=0$ implies $f$ is constant?,"Let $V$ be an open connected subset of $\mathbb{C}$. Let $f:V\rightarrow\mathbb{C}$ be a function whose derivative is $0$ on $V$. How do I prove that $f$ is a constant on $V$? I know that $V$ is path-connected, but I don't know whether this helps.",['complex-analysis']
751410,Prove $f_\infty: A_\infty \rightarrow B_\infty$ is a bijection,"Update: I was given some hints at how to approach this problem $A_\infty $ and $B_\infty$ are sets, not maps. (which is strange because there are function definitions coming into play here) The definition of $f_\infty$ is the restriction of $f$ with the domain and codomain restricted. 1.) Show that $f_\infty $is defined, i.e., if $a \in A_\infty$, then $f(a) \in B_\infty$. 2.) Show that $f_\infty$ is injective, clear because $f_\infty $ is a restriction of the
     injective function f. 3.) Show that $f_\infty$ is onto, i.e., if $ b \in B_\infty$, then there is an $a \in A_\infty$
     and $f(a) = b$. So, show that there is an appropriate element in $A$ that maps to
     $b$, and then show that it even belong to $X_\infty.$ Original Attempt: I am using the Cantor-Schroder-Beenstein Theorem to prove $f_\infty: A_\infty \rightarrow B_\infty$ is a bijection. The cases of $f_+: A_+ \rightarrow B_+$ and $f_-: A_- \rightarrow B_-$  being bijections are already done. The theorem states that for any sets $A$ and $B$, if there exist injections from $A$ into $B$ and from $B$ to $A$, then $\mid A \mid = \mid B \mid$ From definition 5.4.5 A function $f: x \rightarrow y$ with the property $(\forall x_1, x_2 \in X)( x_1 \neq x_2 \rightarrow f(x_1) \neq f(x_2)$ is an injection of $X$ into $Y$. So, from $A $ into $B$ I have, $f: A \rightarrow B$ with the property $(\forall a_1, a_2 \in A)( a_1 \neq a_2 \rightarrow f(a_1) \neq f(a_2)$ and from $B$ into $A$, I have $f: B \rightarrow A$ with the property $(\forall b_1, b_2 \in B)( b_1 \neq b_2 \rightarrow f(b_1) \neq f(b_2)$ Anyway, For $A \in A_+$  the predecessor sequence terminates in A For $B \in B_+$  the predecessor sequence terminates in A The $f_+ $ is the restriction of $f$ to $A_+$, and $f_+$ is a function $A_+ \rightarrow B_+$
so the sequence is $(a \in A_+)$ is $(a,...x)$ Suppose $f_+$ is onto (surjection). The we pick $b \in B_+$ and our sequence is $(f(a),a,...x)$ Since  $b=f(a) $ and $a \in A_+$, $f_+ : A_+ \rightarrow B_+$ is a bijection. For $A \in A_-$  the predecessor sequence terminates in B For $B \in B_-$  the predecessor sequence terminates in B Suppose $g_: B_- \rightarrow A_-$ is well defined and a bijection. Then for $b \in B$, the sequence for $b$ is $(b,a,b_1,a_1...b_m)$. Now set $y_(b)=g(b)$. The sequence for $g(b)$ is $(g(b),b,...b_m)$, so $g(b) \in A$. Now suppose $g_-$ is is an injection. There is a restriction of an injective map $g$. For. $a \in A$ the sequence of $a$ will be $(a,b,...b_m)$ and $g(b_1) = a$ and $a \in A$. Therefore $g_: B_- \rightarrow A_-$ is a bijection For $A \in A_\infty$  the predecessor sequence doesn't terminate. For $B \in B_\infty$  the predecessor sequence doesn't terminate. So now I got to prove that  $f_\infty: A_\infty \rightarrow B_\infty$ is a bijection. So I already know that $A_\infty$ and $B_\infty$ are both going to be a surjection and an injection. But, since the predecessor sequence doesn't terminate, that means it could go on forever. I could have something like $(a,b,a_1,b_a,a_2,b_2,a_3,b_3,...) $ Maybe I think for $a \in A$ the sequence of $a$ is $(a,b,......b_m)$ so $g(b_1) = a $ and $a \in A_\infty$. and for $b \in B$, the sequence of $b$ will be $(b,a,b_1,a_1,...a_m$. Since $g(a_1) = b$ and $b \in B_\infty$, then $g: A_\infty \rightarrow B_\infty$ This is where I am kind of lost since I already mentioned that the $A_\infty$ and $B_\infty$ aren't going to terminate at all, so their sequence will continue, but how do I show that? Do I list a bunch of terms for $A$ and $B$ and then create a surjection and injection map? With the hints, would that mean that $A_\infty$ and $B_\infty$ are restricted in the domain and codomain as well, and the sequence or element belongs inside of $A_\infty$ and $B_\infty$ ?","['proof-writing', 'elementary-set-theory']"
751433,Checking a solution of a P.D.E.,"I have the following P.D.E.: $$-yu_x + xu_y = 0 \quad\text{where } u(0, y) = f(y).$$ I derived a solution as follows: \begin{align}
  -yu_x + xu_y ={}& 0 \\
  \iff{}& \nabla u(x,y) \cdot \langle -y, x\rangle = 0 \\
  \implies{}& \frac{\Bbb dy}{\Bbb dx} = \frac{-x}{y} \\
  \iff{}& y\,\Bbb dy = -x\,\Bbb dx \\
  \iff{}& \int y\,\Bbb dy = \int -x\,\Bbb dx \\
  \iff{}& y^2 = -x^2 + c_2 \\
  \iff{}& y^2 + x^2 = c_2.
\end{align} Since $u(x,y)$ is constant along the O.D.E. ${\Bbb dy}/{\Bbb dx}$ , we have: \begin{align}
u(x,y) &= c_1 \\
       &= f(c_2) \quad\text{for some function $f$} \\
       &= f(y^2+x^2).
\end{align} I want to check that this satisfies the P.D.E. Specifically, any function $f$ should satisfy the P.D.E. My calculus is a bit rusty, and I am not exactly sure how to do this. Here is my reasoning: Since $u(x,y) = -yu_x + xu_y = 0$ we have to substitute in for $f$ which yields $$u(x, y) = -yf_x2x + xf_y2y = 2xyf_y - 2xyf_x.$$ We need the above to equal $0$ . The $2xy$ and $-2xy$ give me evidence that it should cancel, and that my calculus is off $\ldots$ What is not clear to me is that, we don’t know what $f$ is, hence I can’t find out what $f_x$ or $f_y$ are. Though, the problem does look symmetrical, and I could see a potential solution involving polar coordinates, but I’m not quite sure how to solve it in this way either. How can I verify that the above is indeed equal to $0$ and hence satisfies the P.D.E.? Thanks for all the help!","['multivariable-calculus', 'parametric', 'partial-differential-equations']"
751446,What is the inverse image of a sheaf,"Let $f : X \rightarrow Y$ be a continuous map of topological spaces and $\mathcal{G}$ a sheaf on $Y$. What exactly is $f^{-1}\mathcal{G}$? It seems like we should be able to describe the sections $f^{-1}\mathcal{G}(U)$ over some open subset $U$ as equivalence classes of germs, but I get confused when I try and think what exactly the elements of $f^{-1}\mathcal{G}(U)$ are.","['sheaf-theory', 'algebraic-geometry']"
751465,~ The important use of Frobenius–Schur indicators and Frobenius-Schur exponents ~,"I had asked a question on the uses of conjugacy class and centralizer . I had receive various helpful feedback. I appreciate them. Here I like to get some feedback on the Frobenius–Schur indicator. $\bullet$ What is the important use of Frobenius–Schur indicators ? Why such a notion is important? What are examples that using Frobenius–Schur indicators can help us classifying some vector spaces or (discrete) groups? $\bullet$ How about Frobenius-Schur exponents (e.g. here and here )? It seems to me that if I understand that the concept of discrete Fourier transformation for the finite group, the Frobenius–Schur indicators to be some-sort of invariance or measure. $\bullet$ It is said that Frobenius-Schur indicators have values of 0, +1, -1; but I find this paper define the higher Frobenius-Schur indicators $\nu^{(n)}$, which has values larger than $1$, such as $\nu^{(n)}=2$ in the page 22's Appendix. Can someone address why this is the case that the higher Frobenius-Schur indicators $\nu^{(n)}$ may not be 0, +1, -1? - p.s. I had learned that Frobenius-Schur indicators/exponents are useful to classify certain Spherical Categories . Maybe someone can also say a few words about this direction?","['representation-theory', 'abstract-algebra', 'category-theory', 'group-theory', 'quantum-groups']"
751500,Chain rule quesition: proving that the Weingarten map is self-adjoint,"I'm reading through the proof in this paper ( http://www.math.leidenuniv.nl/scripties/JaibiBach.pdf ) but I'm stuck at the line: ""Using the chain rule we get: $L_p(\phi_v) = -Dn(\phi_v) = - \frac {\partial}{\partial v} (n \circ \phi)$"" How does the second equality follow by the chain rule? My working is:
$-Dn(\phi_v) = - \frac{d}{dt} (n \circ \frac {\partial \phi}{\partial v}) = \frac {dn}{dt} ((\frac {\partial \phi}{\partial v})(p)) \frac {d} {dt}( \frac {\partial \phi}{\partial v}) (p) $ but I don't know how to continue this.","['multivariable-calculus', 'riemannian-geometry', 'differential-geometry']"
751507,The group of rigid motions of the cube is isomorphic to $S_4$.,"I want to solve the following exercise from Dummit & Foote. My attempt is down below. Is it correct? Thanks! Show that the group of rigid motions of a cube is isomorphic to $S_4$. My attempt: Let us denote the vertices of the cube so that $1,2,3,4,1$ trace a square and $5,6,7,8$ are the vertices opposite to $1,2,3,4$. Let us also denote the pairs of opposite vertices $d_1,d_2,d_3,d_4$, where vertex $i$ is in $d_i$. To each rigid motion of the cube we associate a permutation of the set $A=\{ d_i \}_{i=1}^4$. Denote this association by $\varphi:G \to S_4$, where $G$ is the group of those rigid motions, and we identified $S_A$ with $S_4$. By definition of function composition we can tell that $\varphi$ is a group homomorphism. We prove that $\varphi$ is injective, using the trivial kernel characterisation: Suppose $\varphi(g)=1$ fixes all of the the pairs of opposite edges (that is we have $g(i) \in \{i,i+4 \}$ for all $i$, where the numbers are reduced mod 8). Suppose $g$ sends vertex $1$ to its opposite $5$. Then the vertices $2,4,7$ adjacent to $1$ must be mapped to their opposite vertices as well. This is because out of the two seemingly possible options for their images, only one (the opposite vertex) is adjacent to $g(1)=5$. This completely determines $g$ to be the negation map which is not included in our group. The contradiction shows that we must have $g(1)=1$, and from that we can find similarly that $g$ is the identity mapping. Since $\ker \varphi$ is trivial $\varphi$ is injective. In order to show that it is surjective, observe that $S_4$ is generated by $\{(1 \; 2),(1 \; 2 \; 3 \; 4) \}$ (this is true because products of these two elements allow us to sort the numbers $1,2,3,4$ in any way we like). We now find elements in $G$ with images under $\varphi$ being those generators. Observe that if $s$ is a $90^\circ$ rotation around the axis through the centres of the squares $1,2,3,4$ and $5,6,7,8$, such that $1$ is mapped to $2$, followed by a rotation by $120^\circ$ around the line through $2,6$ (so that $1$ is mapped to $3$), we have $\varphi(s)=(1 \; 2)$ .Observe also that if $t$ is $90^\circ$ rotation around the axis through the centres of the squares $1,2,3,4$ and $5,6,7,8$, such that $1$ is mapped to $2$, we have $\varphi(t)=(1 \; 2 \; 3 \; 4)$. Now if $\sigma \in S_4$ is any permutation, we express in as a product involving $(1 \; 2),(1 \; 2 \; 3 \;4)$, and the corresponding product involving $s,t$ is mapped to $\sigma$ by $\varphi$. This proves $\varphi$ is surjective. We conclude that $\varphi$ is an isomorphism, so $G \cong S_4$.","['geometry', 'abstract-algebra', 'solution-verification']"
751515,Why is an equation necessarily dimensionally correct?,"I have just read a fascinating proof of the value of the integral $$ \int_{-\infty}^\infty e^{-ax^2} dx, $$ which proceeds by dimensional analysis , as follows: we know that we can write $$ \int_{-\infty}^\infty e^{-ax^2} dx = f(a) $$ for some $f$. Suppose $x$ represents some length, so that $x$ has dimension $[L]$. The argument of the exponential function must be dimensionless, so $a$ must have dimension $[L]^{-2}$. On the LHS, $e^{-ax^2}$ has dimension $[1]$, and $dx$ has dimension $[L]$, so $f(a)$ must also have dimension $[L]$. Hence, we can write $$ f(a) \sim \frac{1}{\sqrt a} $$ where $\sim$ represents proportionality with respect to a dimensionless constant. Now, we need only invoke the well-known result $$ \int_{-\infty}^\infty e^{-x^2} dx = \sqrt{\pi}, $$ which shows that $f(1) = \sqrt{\pi}$. Thus, we have $f(a) = \frac{\sqrt{\pi}}{\sqrt a}$, and $$ \int_{-\infty}^\infty e^{-ax^2} dx = \frac{\sqrt{\pi}}{\sqrt a}. $$ This approach of evaluating an integral by dimensional analysis is one that I have never seen before, and it is not obvious to me that I should accept its validity. Why should I expect an equation to remain dimensionally correct when I introduce an arbitrary dimensional constraint (in this case, $x$ having dimension $[L]$)? Under what conditions is such a step valid?","['improper-integrals', 'calculus', 'integration', 'dimensional-analysis']"
751528,What is the difference between parametrization and change of variables?,"I am embarrassed to ask this, but really need to, in order to clarify my confusion. I am taking multi-variable calculus and I am confused as to the difference between when I should be parametrizing and when I am making a change of variables. My question is really motivated by a question such as this: Compute $	\int \int_{E} e^{-4x^2-9y^2}\,dxdy $ where $E$ is the Ellipse
  $4x^2+9y^2 \le 25$ When I see a question like this, the first thing I think is, ok parametrize the curve, so I simply did: $x = \frac{5}{2}cos\theta \space$and$ \space y = \frac{5}{3}sin\theta$ and then proceeded to to substitute and do: $	\int \int_{E} e^{-25}\,rdrd\theta $ getting the wrong result. Actually as I type this, I think I may have answered my own question. Have I got the wrong idea because, I have essentially turned a double integral with two variables into a 1 variable thing? And use an incorrect  $r$? What is a good rule of thumb to keep in mind the difference of when I am making a change of variables and when I am just parametrizing?",['multivariable-calculus']
751580,Some questions about proof of Theorem 2.43 in Baby Rudin,"I will include the proof here and highlight the parts that are giving me trouble. Theorem $\hspace{5 pt}$ Let $P$ be a nonempty perfect set in $\mathbb{R}^k$. Then $P$ is uncountable. Proof $\hspace{5 pt}$ Since $P$ has limit points, $P$ must be infinite. Suppose $P$ is countable, and denote the points of $P$ by $\mathbf{x_1}, \mathbf{x_2}, \mathbf{x_3}, \ldots$. We shall construct a sequence $\{V_n\}$ of neighborhoods as follows. Let $V_1$ be any neighborhood of $x_1$. 1) ^ Are we using the Axiom of Choice here? How can we have an arbitrary set in a construction? If $V_1$ consists of all $\mathbf{y} \in \mathbb{R}^k$ such that $|\mathbf{y} - \mathbf{x_1}| < r$, the closure $\overline{V_1}$ of $V_1$ is the set of all $\mathbf{y} \in \mathbb{R}^k$ such that $|\mathbf{y} - \mathbf{x_1}| \leq r$. 2) ^ It makes sense intuitively, but how do we prove this last statement? Suppose $V_n$ has been constructed, so that $V_n \cap P$ is not empty. Since every point of $P$ is a limit point of $P$, there is a neighborhood $V_{n+1}$ such that (i) $\overline{V_{n+1}} \subset V_n$, (ii) $x_n \notin \overline{V_{n+1}}$, (iii) $V_{n+1} \cap P$ is not empty. By (iii), $V_{n+1}$ satisfies our induction hypothesis, and the construction can proceed. 3) ^ I really don't get this whole paragraph much at all. Could someone explain it in a more step-by-step way? Put $K_n = \overline{V_n} \cap P$. Since $\overline{V_n}$ is closed and bounded, $\overline{V_n}$ is compact. 4) ^ ""closed"" comes from it being a closure and ""bounded"" comes from the definition of neighborhood, correct? Since $x_n \notin K_{n+1}$, no point of $P$ lies in $\bigcap_1^\infty K_n$. Since $K_n \subset P$, this implies that $\bigcap_1^\infty K_n$ is empty. But each $K_n$ is nonempty, by (iii), and $K_n \supset K_{n+1}$, by (i); this contradicts the Corollary to Theorem 2.36.","['proof-verification', 'analysis']"
751621,Mean & SD of Sampling Distribution,"A population consists of $4$ numbers $\{0, 2, 4, 6\}$ . Consider drawing a random sample of size $n = 2$ with replacement. (a) What is the sampling distribution of $\bar x$ ? Is this a normal distribution ? Since $\bar x $ ~ $N\left(\mu, \dfrac{\sigma^2}{n}\right)$ ? (b) Calculate the mean & standard deviation of the sampling distribution of $\bar x$ . I got the answer of mean $\mu$ by $\frac{0+2+4+6}{4} = 3$ Thereafter, I proceed to calculate $\sigma$ $$\sigma = \frac{(0 - 3)^2 + (2 - 3)^2 + (4 - 3)^2 + (6 - 3)^2}{4} = 5$$ Substituting it back into the sample distribution gives: $$\bar x \sim N\left(3, \dfrac{5^2}{4}\right)$$ Thus, I derive the standard deviation to be: $$\sqrt{\frac{\sigma^2}{n}} = \dfrac{5}{2}.$$ However, the answer given was $\dfrac{\sqrt{5}}{\sqrt{2}}$ . Can someone explain why is this so? I'm really quite confused with the whole concept of sampling distribution.. Thanks a lot!","['statistics', 'normal-distribution', 'sampling']"
751647,Convergence in probability implies convergence in mean under one additional condition [duplicate],"This question already has an answer here : A basic question on convergence in prob. and a.s. convergence (1 answer) Closed 8 years ago . Prove that if random variables $X_n$ are dominated by an integrable random variable then $E[X_n] \to E[X]$ follows if $X_n$ converges to $X$ in probability. Hint: Use the following theorem : A necessary and sufficient condition for $X_n \to _p X$ is that each subsequence $\{X_{n_k}\}$ contain a further subsequence $\{X_{n_{k_j}}\}$ such that $\{X_{n_{k_j}}\}\to X$ with probability 1 as $j, n \to \infty$ The problem is that I have a.s. convergence for a subsequence one level lower for which I have to prove convergence in mean. So, stuck.","['probability-theory', 'convergence-divergence']"
751656,Calculating Angles at Vertices,This is quite a tricky question and I can't answer it.,['geometry']
751663,Why aren't these partial derivatives interchangeable?,"I've ran across something that confuses me regarding multivariable functions and partial derivatives. I'll use an example to illustrate: We let
$$x = f(y,t) = yt^2,$$
and define the operators
$$\partial_{t|x}(\cdot) := \left.\frac{\partial (\cdot)}{\partial t}\right|_{x},$$
($x$ held constant)
$$\partial_{t|y}(\cdot) := \left.\frac{\partial (\cdot)}{\partial t}\right|_{y}.$$ Now we investigate whether these operators are interchangeable. Firstly,
$$\partial_{t|x}x = 0$$
$$\Rightarrow \partial_{t|y}(\partial_{t|x}x) = 0.$$ Secondly,
$$\partial_{t|y}x = \partial_{t|y}f(y,t) = 2yt = 2x/t$$
$$\Rightarrow \partial_{t|x}(\partial_{t|y}x) = -2x/t^2 \neq 0.$$ At this stage I'm confused. Of course, I felt uncomfortable writing out the above, like something was going horribly wrong somewhere, but I can't put my finger on where. So where is the error? Somewhere around the substitution $2yt = 2x/t$? Maybe I'm confusing free/bound variables, or keeping things constant where I'm not allowed to? Perhaps I can't expect the derivatives to be interchangeable at all? In that case, why not? EDIT: Some people are asking me about my notation. The notation for the operators above (i.e. $\partial_{t|x}$) are just something I invented on the spot for this post, and as mentioned above, the vertical line notation indicates which variable should be kept constant during the differentiation (i.e. the variable right of the line, at the bottom). The latter shouldn't be controversial, should it? Let me illustrate the idea. Imagine some nested functions:
$$f(g(x,t), t) = g(x,t)^2 + t^2 = \{g(x,t) = x + t^2\} = x^2 + 2xt^2 + t^4 + t^2$$
Now, if I'm not mistaken, $\frac{\partial f}{\partial t}$ is ambiguous: we can imagine keeping either $g(x,t)$ or $x$ constant when performing this differentiation. I.e.
$$\left.\frac{\partial f(g(x,t),t)}{\partial t}\right|_g = 2t$$
and
$$\left.\frac{\partial f(g(x,t),t)}{t}\right|_x = 4xt + 4t^3 + 2t.$$
Now, we can use the chain rule to obtain
$$\left.\frac{\partial f(g(x,t),t)}{t}\right|_x = \left.\frac{\partial f(g(x,t),t)}{g}\right|_t\left.\frac{\partial g(x,t)}{t}\right|_x + \left.\frac{\partial f(g(x,t),t)}{t}\right|_g = (2g(x,t))(2t) + 2t$$
$$ = 4xt + 4t^3 + 2t$$","['multivariable-calculus', 'calculus', 'partial-derivative']"
751698,Is $I+AA^T$ positive definite matrix?,"If $A$ is real matrix, how can i show that $I+AA^T$ is positive definite matrix? $I$ is the identity matrix and $A^T$ is a transpose of $A$.","['matrices', 'linear-algebra']"
751703,Intuition on second order partial derivatives,"Inspired by smooth submanifolds of $\mathbb{R}^n$, I am looking for a good geometric way to think of second order partial derivatives of a locally smooth function $f:\mathbb{R}^n \rightarrow \mathbb{R}$. To clarify, I do not want to think of the first order partial derivatives as stand-alone functions $\mathbb{R}^n \rightarrow \mathbb{R}$, of which we then again take first-order derivatives, but I would like to see the connection to the original function. Thanks.","['smooth-manifolds', 'derivatives']"
