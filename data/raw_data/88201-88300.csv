question_id,title,body,tags
1183559,Showing that this particular subspace of $\mathbb{R}^2$ is non-triangulable.,"Problem: Let $S=\{(x,y):0\le y\le 1, x=0\;\, \text{or}\;\, x=1/n \quad\text{for}\; n=1,2,\cdots\}\cup ([0,1]\times {0}$), Show that $S$ is not triangulable. Note: my definition for triangulation is that a set $X$ is triangulabe if there exists simplicial complex $K$ such that $|K|$ is homeomorphic to $X$ My attempt: So first thing I spotted was that $S$ is clearly closed, and hence compact in $\mathbb{R}^2$. Therefore, if a simplicial complex $|K|$ was homeomorphic to it, then it must also be compact, meaning that $|K|$ is finite. But here is one point of my concern: I'm not supposed to know that all compact simplicial complexes are finite , simply because I haven't yet covered it. Is there other way more elementary arguement to show that $|K|$ is finite, other than referring to the theorem? Now there is explicit hint given in the problem: show that for any finite simplicial complex for any $x\in |K|$ and open set $U$ containing $x$ there is a connected open set $V$ such that $x\in V\subseteq U$ Apart from the fact that I cannot see how this might help me solving problem, I also am a bit puzzled that it seems too much obvious for me to prove it: since $|K|$ can be embedded into finite dimensional $\mathbb{R}^n$ space, with standard metric on it we can always take $V$ to be a open ball around $x$, and we are done. Is that right? I also see that $S$ is clearly path-connected, so my $|K|$ must be edge-connected and connected. But I seem to be stuck at this stage. It would be really great if someone could explain how one would do this with given hint , if possible. Thanks in advance,","['general-topology', 'simplicial-complex']"
1183566,change order of integration,"Often when I do integrations I need to change the order of integration and this is time consuming. Consider for example 
\begin{align}
\int_0^1\int_0^x f(x,r) \mbox{d}r\mbox{d}x=\int_0^1\int_r^1 f(x,r) \mbox{d}x\mbox{d}r
\end{align}
and to come up with that I draw a little graph and change the order the usual way. But if I have a quadrupole then things get messy and time consuming. I am hence after some technique to cut down on number of graphs I have to draw. In particular, how would you approach the following change of variable problem in a fast way? $$\int_0^1\int_0^1\int_0^r\int_0^xf(x,r,\alpha,\beta)\mbox{d}\beta \mbox{d}\alpha \mbox{d}r\mbox{d}x=\int_0^1\int_?^?\int_?^?\int_?^?\mbox{d}r\mbox{d}x \mbox{d}\beta \mbox{d}\alpha$$ Many thanks in advance.","['definite-integrals', 'geometry', 'integration']"
1183576,Analogue of Moser's circle problem to higher dimensions?,"Does the analogue of Moser's circle problem have a known solution in higher dimensions? Briefly, the original question asks, ""Given $k$ points on a circle, if each point is connected to every other by a chord, what is the maximum number of regions $R$ these points divide the circle into?"" The answer has a neat little formula and a straightforward combinatorial interpretation: $$R= {k\choose 0} + {k \choose 2} + {k \choose 4}$$
where ${k\choose 0}=1$ is the number of original regions, ${k\choose 2}$ is the number of lines, and ${k\choose 4}$ is the number of intersections of lines (this assumes the arrangement of points is not degenerate, so to speak). The natural generalization is: Given $k$ points on an $n$-sphere, each subset of cardinality $n+1$ determines a hyperplane in $(n+1)$-space. What is the maximum number of regions these hyperplanes divide the sphere into? In my estimation this is a much harder problem.  One complication has to do with the special nature of a maximal solution.  In the 1-sphere case, the arrangements of points which produce non-maximal regions have measure zero in the space of all arrangements (this can be made precise).  In higher dimensions the situation is more or less reversed. Maximal arrangements, while probably not constituting a null set, are very unlikely.  An arbitrary hyperplane arrangement will divide up the space into a predictable number of regions, but many of these regions don't intersect with the sphere at all, and the question of which do and which don't is sensitive to the specific constellation of points. Here's hoping someone might have some insight.","['geometry', 'combinatorics']"
1183586,Formal argument on independence of consecutive hitting times of a Markov chain.,"I'm refering to the question: Differences of consecutive hitting times . I'm interested in the independence of consequtive hitting times of certain values of a Markov chain. And I do ""understand"" the argument in the answer, as it seems totally reasonable that this sequence is id. Nevertheless I cannot figure out a proper formal proof.... Main problem: Wy does it suffice to show that two such differences $\tau_k-\tau_{k-1}$ and $\tau_{l}-\tau_{l-1}$ for $k\neq l$ are independent? I.e. why is pairwise independence in this special case equivalent to mutual independence? I think I can figure out a formal proof for two increments. But I'm not sure (outline: using the strong Markov property and assuming that wlog $l>k$, we know that $\tau_l-\tau_{l-1}$ is independent of $\mathcal{F}_{\tau_{l-1}}$, while $\tau_{k}-\tau_{k-1}$ is $\mathcal{F}_{\tau_{l-1}}\subset\mathcal{F}_{\tau_{k}}$-mb. Hence they are independent.) A few nice inputs/ideas would totally suffice, I don't need a rigorous proof! Thank's in advice!","['probability-theory', 'stochastic-processes', 'markov-chains', 'stopping-times']"
1183587,Finding P value,"I have these observations $(2,3.2,3.8,2.5,3.3,2.8,3.0,3.4)$ from $X \sim N(\mu,\sigma^2)$ and i want to calculate the $P$-value testing $H_0: \mu =3.2$ against $H_1 \neq 3.2$ with $\sigma = 0.6$ should i calculate $P r(X > 3.3) $ and $Pr( X < 3)$ and add these togheter ? I tried it using that  $X = Y\sigma + \mu  \sim N(0,1)$ but the answer don't seem to be right according to the solution which is $P = 0.347$ Anyone can tell me what Iam doing wrong?","['statistics', 'probability', 'statistical-inference']"
1183592,Find the matrix given the determinant,"Is there a general method to find a 3x3, or 2x2 matrices, given the determinant? I want to do a project with my students when we start to study Systems of Equations. It would be interesting if the determinant is a prime number and then work backwards. For example, I got this by playing around: if I let $$A = \begin{bmatrix}
       10 & -17\\
       31 & 2
     \end{bmatrix}$$ then the $\det(A)=547$, a prime number. Not sure if this would be a good mini project or not. Thanks for any help.","['matrices', 'determinant']"
1183615,Deriving Ricci identity for co-vector fields,"Let $\nabla$ be the covariant derivative associated with a torsionless connection.  Prove the Ricci identity for covectors: $$\nabla_a \nabla_b \lambda_c - \nabla_b \nabla_a \lambda_c = -R^d_{\,\,cab}\lambda_d$$ Attempt: Consider the object $\nabla_a \nabla_b (X^c \lambda_c)$. Then by distributing using the Leibniz rule, we obtain $$\nabla_a (\nabla_b (X^c) \lambda_c + X^c \nabla_b (\lambda_c))$$ and again, this time operating with $\nabla_a$ gives $$(\nabla_a \nabla_b X^c)\lambda_c + \nabla_b(X^c) \nabla_a (\lambda_a) + \nabla_a(X^c)\nabla_b (\lambda_c) + X^c \nabla_a \nabla_b \lambda_c$$ I can then use the Ricci identity for vector fields on the first term and simplify the middle terms to give $$(R^c_{\,\,dab}X^d + \nabla_b \nabla_a X^c)\lambda_c + e_b(X^c) e_a(\lambda_c) + e_a(X^c) e_b(\lambda_c) + X^c\nabla_a \nabla_b \lambda_c$$ using the fact that for a function, $\nabla_X f = X(f)$ where $X$ is a vector field. $\lambda_c$ and $X^c$ are the components of the covector and vector respectively. I am just not quite sure how to progress. I think I am going to need to symmetrise over the $a$ and $b$ indices to extract another term so I can obtain the required terms in the identity but at the moment I am unsure.  Thanks for any help!","['tensors', 'riemannian-geometry', 'connections', 'differential-geometry']"
1183628,"10 Balls in a row (4B 4W 2R), with not all balls of any color consecutive","Let N be the number of ways to arrange four black balls, four white balls, and two red balls in a row, so that for each color, not all the balls of that color are consecutive. (For example, if B, W, and R represent a black, white, and red ball, respectively, then the arrangement WRWWRBBBBW is not allowed, because all the black balls are consecutive.) Find the remainder when N is divided by 1000. Many cases blew my head off.(first all black, then white, then red, then two together-3 cases and then all three together...)",['combinatorics']
1183641,Solution of differential lyapunov equation,"How would I solve for following, else any implemented algorithms or solvers in matlab (even ways to solve it) for Lyapunov differential equation of form: $$P'(t) + A(t)^TP(t) + P(t)A(t) + Q(t) = 0,$$ where $Q(t)> 0$, with $P(t)$ being symmetric periodic positive definite, $A(t)$ is linearized dynamics of the system.","['dynamical-systems', 'ordinary-differential-equations', 'matrices', 'matrix-equations', 'periodic-functions']"
1183643,What is the number of full binary trees of height less than $h$,"Given a integer $h$ What is $N(h)$ the number of  full binary trees of height less than $h$? For example $N(0)=1,N(1)=2,N(2)=5, N(3)=21$(As pointed by TravisJ in his partial answer) I can't find any expression of $N(h)$ neither a reasonable upper bound. Edit In a full binary tree (sometimes called proper binary tree) every node other than the leaves has two children.","['trees', 'computational-complexity', 'algorithms', 'combinatorics']"
1183652,Is every composite number the average of two primes?,"I'm interested in this question because it relates to a bad joke about people in their prime. It seems to work for the first 20 numbers: 4 is the average of 5 and 3. 6 is the average of 5 and 7. 8 is the average of 5 and 11. $\vdots$ 16 is the average of 13 and 19. $\vdots$ It would not be hard to write a program that checks more cases, but I suspect the hypothesis is true. If that is the case, then a more mathematical approach is needed.",['number-theory']
1183681,Functions - Simple Surjection,$s : \mathbb{R} \to \mathbb{R}$ given by $s(x)$ = $x^2$. In this instance why is the function $s$ not onto?,['functions']
1183686,Find the dimensions of rectangle with given area and ratio of sides,I have memorized this question and I couldn't really get how I could figure it out. The question asks: Its length is 5 times twice its width The area is 7 square meters Find the dimensions of this rectangle I know its $l \times w \times h$... I don't know how I could use factoring to solve this. Should I use gfc? Which factoring should I use? Please give me hints so I can figure out how to do this,"['geometry', 'algebra-precalculus']"
1183696,Proving inequality,"Let $f$ be a twice differentiable function and  let M, N, and P be the least upper bounds of |$f$(x)| |$f'$(x)| and |$f''$(x)| respectively prove that the square of N can never exceed 4 MP. I thought about using Taylor's theorem, but I do not know how to manage it.","['inequality', 'derivatives', 'real-analysis']"
1183705,Find the Pascal's Limit [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Let $P_{n}$ be the product of the numbers in row  of Pascal's Triangle. Then evaluate $$ \lim_{n\rightarrow \infty} \dfrac{P_{n-1}\cdot P_{n+1}}{P_{n}^{2}}$$","['calculus', 'binomial-coefficients', 'limits']"
1183734,Haar's theorem for the rotation-invariant distribution on the sphere,"Let $\mu$ be a Borel probability measure on $\mathcal S^{n-1}$. Suppose $\mu$ is rotation-invariant, that is, $\mu(UA) = \mu(A)$ for any Borel set $A\subset\mathcal S^{n-1}$ and orthogonal matrix $U\in\mathbb R^{n\times n}$. Then $\mu$ is the
  uniform measure. Could you recommend a reference (preferably a book) for this simple version of Haar's theorem?","['probability-theory', 'measure-theory', 'reference-request']"
1183793,$A$ subset $B$ implies $f(A)$ subset $f(B)$,"prove: let $f:X\rightarrow Y$. Then for any subset $A$ and $B$ of $X$, a) $$f(A\cap B) \subset f(A)\cap f(B)$$
b) $$A\subset B \Rightarrow f(A)\subset f(B)$$ proof of a): Let $y\in f(A\cap B)$, then there is an $x\in A\cap B$ so that $$f(x) = y$$ But $x\in A$ so $f(x)\in f(A)$ and $x\in B$ so $f(x)\in f(B)$. Therefore, $$f(x)\in f(A)\cap f(B) \ \   i.e. \  y\in f(A)\cap f(B) $$ proof of b): Take an arbitrary element $x\in f(A)$, then there exists a $y\in A$ so that $$f(y) = x$$ Now, take an arbitrary element $x\in f(B)$, then there exists a $y\in B$ so that $$f(y)= x$$ So, for every $y\in A$ and $y\in B$, $f(y)\in f(A)$ and $f(y)\in f(B)$ also for any $x\in f(A)$, $x\in f(B)$. Therefore, $f(A)\subset f(B)$ Are my proofs correct? If not, can you provide reasoning and if the proof is not complete please show me what I need to further add","['proof-verification', 'elementary-set-theory', 'functions']"
1183825,How to get coordinates of a point after an image is rotated? (with images),"I have a problem that involves a rotating image and finding a previously known point. Firstly, there is a sequence with the rotation. We start with an empty image. A line is drawn vertically, from (0, 0) to a point in the y-axis (assume 50, possible values range from 0 to 100 which is the max). We will call this point 'a'. The image is rotated by 'x' degrees (known value). Another line is drawn vertically, from (0, 0) to a point in the y-axis (assume 60). We will call this point 'b'. My question is, how do I get the coordinates of point 'a' relative to point (0, 0)? Thank you all so much and I really appreciate your replies, good or bad. Please do tell me if you need more info on this.","['analytic-geometry', 'geometry', 'coordinate-systems', 'rotations']"
1183869,weak* continuous linear functional is in the predual,"Let $X$ be a Banach space and $X^*$ its dual. We know that the weak* topology is the least topology that makes every $x \in X$ continuous as an evaluation functional. However, this does not imply that every weak* continuous linear functional is something in $X$ , even though this happens to be true. The question is: how can we prove this? What have I though is: It is enough to show that $\cap_{i=1}^{k} \ker{x_i} \subset \ker{\phi}$ for some $x_i \in X, i=1,2,...,k$ I have shown this for infinitely many $x_i$ s (easy, using the weak* continuity and that 0 is always in the ker) and in order to pass to finitely many I would need some kind of compactness result (probably by using Banach-Alaoglu somehow), but I do not know how to do this. Can anyone help?",['functional-analysis']
1183905,If h is a holomorphic non-vanishing function in the complex plane then h is the exponential of another function.,Show if $h(z)$ is a holomorphic function such  $\forall$$z\in$$\mathbb{C}$  $h(z)\neq0$ then $\exists$ $H(z)$ such $h(z)=e^{iH(z)}$. I think I should define the Taylor series of $h(z)$ and then rearrange it so it looks like the Taylor series of an exponential function but I'm not sure if that will work.,['complex-analysis']
1183954,Intuition behind variation of parameters method for solving differential equations,"I have used the variation of parameters method (and have been taught it, although not hugely in depth) and I was wondering if I've understood the intuition behind it. In particular I've been thinking about the method for second order ODEs: $$a_{2}(x)y''(x)+a_{1}(x)y'(x)+a_{0}(x)y(x)=f(x)$$ Does the motivation for considering a particular solution of the form $$y_{p}(x)=u_{1}(x)y_{1}(x)+u_{2}(x)y_{2}(x)$$ (where $y_{1},\;y_{2}$ are solutions to the corresponding homogeneous equation) because $(y_{p}/y_{1})\neq\text{ constant}$ and likewise $(y_{p}/y_{2})\neq\text{ constant}$, as otherwise we would just obtain the complementary solution again. This suggests that both are instead functions of $x$, i.e. $(y_{p}/y_{1})=u_{1}(x)$ and $(y_{p}/y_{2})=u_{2}(x)$, leading to the form of the ansatz I gave above? Secondly, is the reason why we place a further constraint on the form of $y_{p}$ (other that it be a solution to the original ODE) because, in principal, there will be an infinite number of particular solutions of the form $$y_{p}(x)=u_{1}(x)y_{1}(x)+u_{2}(x)y_{2}(x)$$ but we only require one particular solution, and so by imposing an additional constraint we have two equations for the two unknowns $u_{1}$ and $u_{2}$, thus enabling us to uniquely determine a solution for each of them and subsequently enabling us to find a general solution to the original ODE?!","['ordinary-differential-equations', 'calculus', 'intuition']"
1183994,"""Symmetric"" numerical computation of second derivative","When numerically computing a first derivative, it is better to use $$f'(x) \approx \frac{f(x + \Delta x / 2) - f(x - \Delta x / 2)}{\Delta x}$$ than to use $$f'(x) \approx \frac{f(x + \Delta x) - f(x)}{\Delta x}$$ since it's more symmetric, and hence typically more accurate. However, what is the equivalent of this phenomenon in the second derivative case? In other words, the obvious candidate formula is $$f''(x) \approx \frac{f'(x + \Delta x / 2) - f'(x - \Delta x / 2)}{\Delta x}$$ but is there a more accurate (""symmetric""?) approximation for the second derivative as is the case for the first derivative?","['derivatives', 'numerical-methods']"
1184000,Does this set $\{x + 1 : x \in \mathbb{R}\}$ have an upper and lower bound?,"I understood the concepts of maximum, minimum, lower and upper bound, supremum and infimum, but I am still not so confident when it comes to say if a set has those or not. In one of the exercises I have to do, it's asking us to say if the sets are bounded from below or above, and to give 2 upper and 2 lower bounds if they exits. I will proceed through all the sets, and I will try to give my answer, so that you can tell me where I am wrong. a) $\{ 1, 4, 9, 16, 25\}$ This set is bounded from below and from above. 
Two examples of lower bound are: $1$ and $0$, since for all $x$ in the set, $1\leq x$ and $0\leq x$. Two examples of upper bound are: $25$ and $26$, since for all $x$ in the set, $25 \geq x$ and $26\geq x$. b) $\{ x \in \mathbb{R} : x < 0\}$ This set is bounded from above, but not from below, since it goes to $-\infty$. Two examples of upper bounds are $0$ and $1$, since for any number $x$ in the set, $x \leq 0$ and $x \leq 1$. The range of this set can be represented by: $$\left(-\infty, 0\right)$$ c) $\{1 + \frac{1}{2^n} : n \in \mathbb{N}\}$ This set is bounded from below and above. Two examples of upper bound are $\frac{3}{2}$ or any other number greater than $\frac{3}{2}$, since for any number $x$ in the set $x \leq \frac{3}{2}$. Two examples of lower bound are $1$ or any other number less than $1$, since for any number $x$ in the set, $1 \leq x$. Basically, the set can be represented with the following range: $$\left(1, \frac{3}{2}\right]$$ d) $\{x + 1 : x \in \mathbb{R}\}$ This set has no upper bound or lower bound, if we do not consider $-\infty$ or $\infty$ as respectively the lower and upper bound. This set basically represents the real numbers, right? e) $[0, 2] \cup (3, 4)$ In this case, I have a doubt. Are we talking about ranges of real numbers or natural numbers? Because if we are talking about natural numbers, then the upper bounds could be $3$ and $4$, since they $\not\in [0, 2] \cup (3, 4)$. Otherwise the 2 upper bounds could be $4$ or $5$. Two lower bounds are for example $0$ or $-1$.",['elementary-set-theory']
1184041,"Isomorphism of Proj schemes of graded rings, Hartshorne 2.14","This question is based on exercise $2.14$ of chapter $2$ of Hartshorne. Suppose $\varphi:S\rightarrow T$ is a graded homomorphism of graded (commutative, unital) rings such that $\varphi_d := \varphi|_d$ is an isomorphism for all $d$ sufficiently large. Then I want to show that the natural morphism $ f: $Proj $T \rightarrow $Proj $S$  is an isomorphism by explicitly constructing an inverse. The morphism is given on spaces by $P \mapsto \varphi^{-1}(P)$ and on sections by composing pointwise with $\varphi_P : S_{\varphi^{-1}(P)}\rightarrow T_P$. I would like to show that this is an isomorphism by exhibiting an inverse homeomorphism to $f$ and showing the stalk maps are isomorphic. I appreciate that we can cover Proj $S$ with affine pieces and then show that the corresponding maps from the pullbacks to these pieces are isomorphisms, but I would like to know what $f^{-1}$ looks like explicitly. If there is a good way to see what $f^{-1}$ is by chasing through the local method of showing that $f$ is an isomorphism then I would also appreciate an explanation of that. My candidate for $f^{-1}$ was $P \mapsto \sqrt{(\varphi(P))}=I$, the radical of the ideal generated by $\varphi(P)$. I think that I have shown that $I$ is homogeneous, doesn't contain $T_+$, $\varphi^{-1}(I) = P$ and is almost prime in the sense that if $a,b \in T$ are homogeneous and have degree at least $1$ then $ab \in I \implies a \in I$ or $b \in I$. But I think that in fact $I$ is not in general prime, since the degree $0$ component of $I$ is exactly $\sqrt{(\varphi(P_0))}$ in the ring $T_0$, where $P_0$ is the ideal of $S_0$ given by $S_0 \cap P$ and that for general rings $A$ and $B$, with $\rho:A\rightarrow B$, $P$ prime in $A$ doesn't imply $\sqrt{(\rho(P))}$ prime in $B$.","['algebraic-geometry', 'schemes', 'projective-schemes']"
1184053,Alexandrov Spaces or Topology on a finite point set - is there a distinction between open and closed sets?,"A topology can be defined as a family of sets that is closed under finite intersections and unions. We then call these open sets. But, if the point-set is finite, why should we call these open sets, and not closed sets? Consider: The union of any number of open sets is open, and the intersection of a finite number of open sets is open. The union of a finite number of closed sets is closed, and the intersection of any number of closed sets is closed. But if our family is finite, there is no distinction because we don't have infinite sets to union or intersect. If it is an Alexandrov space, then infinite unions of open sets are always open, so this also removes that distinction. So, given just a family of sets which is finite, closed under unions and intersections, is there a reason to declare these as ""open"" rather than ""closed"" other than convention (and then declaring complements of members of the family the ""closed"" and ""open"" respectively)? I cannot talk about limit points, or any such things, so it seems like in these cases I am just calling them open, and their complements closed, as a matter of convention. Without extra information about the topology in this case, can open sets and closed sets be viewed as ""opposites, but qualitatively equivalent""?",['general-topology']
1184071,Prove the identity $\sin^4α-\cos^4α=2\sin^2α-1$,"Prove the identity $\sin^4α-\cos^4α=2\sin^2α-1$ Well, I thought to start it this way:
$$(\sin^2α-\cos^2α)(\sin^2α+\cos^2α)=2\sin^2α-1=>\\(\sin α-\cos α)(\sin α+\cos α)(\sin^2α+\cos^2α)=2\sin^2α-1$$ I don't know how to continue...",['trigonometry']
1184124,Proving infinity limit of a multivariable function,"I have a function $f(x,y,z): \mathbb{R}^3\rightarrow \mathbb{R}$ which is $f(x,y,z)=3x^2+z^2+y^2-2xy+14$. I'm trying to show that $f(x,y,z)\rightarrow \infty$ when $||(x,y,z)|| \rightarrow \infty$. (Or formally: $\forall_{M>0}\exists_{R>0}\forall_{(x,y,z) \in \mathbb{R}^3}:||(x,y,z)||>R \rightarrow f(x,y,z)>M $) So in the process of building the proof I'm trying to find a suitable $R$, and for that I'm trying to express $f(x,y,z)$ in terms of $||(x,y,z)||$ or $||(x,y,z)||^2$. Try 1 : Let $(x,y,z)$ be such that $||(x,y,z)||>R$, then $f(x,y,z) = 3x^2+z^2+y^2-2xy+14$ $= x^2+z^2+y^2 +2x^2-2xy+14 \geq R^2+2(x^2-xy)+14 $ $\geq R^2-2(\sqrt{2x^2} \sqrt{x^2+y^2})+14 $ $\geq R^2-2(\sqrt{2x^2} \sqrt{x^2+y^2+z^2})+14 $ $\geq R^2-2(\sqrt{2x^2} R)+14 = R^2-2\sqrt{2}|x|R+14$. From which I can't continue since if I try to bound $x$ by $R$ I get a negative expression. Try 2 : Using $xy \leq \frac{x^2+y^2}{2}$, we get that $f(x,y,z) = 3x^2+z^2+y^2-2xy+14 \geq 2x^2+z^2+14$, from which I can't proceed since I can't express $R$ without $y$. Try 3 : Let $R=\sqrt{3}M$, and since $||(x,y,z)||>R$ then $x^2+y^2+x^2>R^2=3M^2$. Meaning that at least one of $x^2$, $y^2$, $z^2$ has to be greater or equal $M^2$. $f(x,y,z) = 3x^2+z^2+y^2-2xy+14 = 2x^2+z^2+(y-x)^2+14$. If $x^2>M^2$ or $z^2>M^2$ then obviously $f(x,y,z)>M$ and we're done. But I don't know how to handle the case of $y^2>M^2$. Any help would be appreciated!","['multivariable-calculus', 'functions', 'limits']"
1184143,Show that $A^TA$ has at least one positive eigenvalue if $A$ is not all-zero,"I need some help on showing that $A^TA$ has at least one positive eigenvalue if $A$ is not all-zero. $A$ is rectangular and can have dependent columns in general. I can show that it cannot have negative eigenvalues. Here is what I have now. $$
A^TA\vec x=\lambda \vec x\\
\vec x^TA^TA\vec x=\vec x^T\lambda \vec x\\
||A\vec x||^2=\lambda||\vec x||^2
$$
Because the two norms are strictly non-negative, $\lambda$ has to be non-negative. However, I cannot guarantee that $||A\vec x||\neq 0$ for at least one $\vec x$. If all eigenvectors of $A^TA$ lies in the null-space of $A$ (which I feel should not be the case), then $\lambda$ is indeed zero for all eigenvectors. So is my feeling wrong or how do I prove it?","['matrices', 'symmetry', 'eigenvalues-eigenvectors']"
1184152,An example of a Ring with many zero divisors,Is there an example of a commutative ring $R$ with identity such that all its elements distinct from $1$ are zero-divisors? I know that in a finite ring all the elements are units or zero-divisors. Is there a finite ring with the property I've required? Obviosuly I'm requiring that $|R|\geq 3$.,"['ring-theory', 'abstract-algebra']"
1184190,Can an inner product on a vector space be negative?,This may be a noob question but I recently read a definition that an inner product on a complex vector space is said to be a positive-definite sesquilinear map. Doesn't positive definite mean that the inner product will only return positive values? (Just started studying Functional Analysis specifically Hilbert Spaces),"['hilbert-spaces', 'functional-analysis']"
1184199,"Prove that there exist linear functionals $L_1, L_2$ on $X$","Let $X$ be a linear space, $p, q$ sublinear functionals on $X$, and $L$ a linear functional on $X$ such that $|L(x)| ≤ p(x) + q(x),$ for all $x ∈ X$. Prove that there exist linear functionals $L_1, L_2$ on $X$ such that $L(x) = L_1(x) + L_2(x),$ and $|L_1(x)| ≤ p(x), |L_2(x)| ≤ q(x),$ for all $x ∈ X.$ My Work: First I thought to use Hahn Banach Theorem. But since there is no known subspace it was useless. Then I tried to make $L(x)$ as $L(x)=\frac{L(x+\lambda)+L(x-\lambda)}{2}$ for some scalar $\lambda$ but failed to find suitable $L_1$ and $L_2$. I think this problem is little bit tricky. I want to try it myself and I only need a hint to start. Can somebody please give me a hint?","['functional-analysis', 'real-analysis']"
1184205,Prove that the limit doesn't exist,"I have to prove that $$\lim_{(x,y)->(0,0)} \frac{xy}{2x-y}$$ doesn't exist. I have tried to use these restrictions: $x=0;  y=0; y=x;  y=mx; y=mx+q; y=ax^2;y=ax^2+bx+c; y=1/x, y=1/x^2,..$ and for each of them, I have done the related limit. But I have always obtained 0. 
I know that I if want to prove that the limit doesn't exist, I have to find two restrictions that have different values for their limits. Any suggestions? Many thanks","['multivariable-calculus', 'limits']"
1184247,Solve $ab = 2(a+b)$ using modular arithmetic,"I have the following equation which I want to solve where $a,b \in \mathbb{N}$ : \begin{align*}ab &= 2(a+b) \\ \iff ab &= 2a + 2b \\ \iff ab - 2a &= 2b \\ \iff a(b-2) &= 2b \\ \iff a &= \frac{2b}{b-2}\end{align*} Therefore I must solve for $a,b$ such that all pairs $\displaystyle(a,b) = \bigg(\frac{2b}{b-2},b\bigg)$ where $a,b\in \mathbb{N}$ I must now find, using modulo properties, the values of $a,b \in \mathbb{N}$ which satisfies the equation. Clearly $(0,0)$ is a solution, but $0 \not\in \mathbb{N}$ . We cannot have $b=1$ since then $a<1$ and $b =2$ is a horizontal asymptote. Notice, however that $ \displaystyle a = \frac{2b}{b-2} = 2 + \frac{4}{b-2}$ . Thus for $a$ to be an integer, we require $(b-2)|4$ . Hence we must have that $4 \equiv 0 \mod{(b-2)}$ . Thus we have the following equations: $b-2 = 1 \implies b = 3 \implies a = 3$ $b-2 = 2 \implies b = 4 \implies a = 4$ $b-2 = 4  \implies b = 6 \implies a = 3$ Thus our solutions are given by $$\{ (6,3), (4,4), (3,6) \} $$ Is the steps and reasoning used correct?","['elementary-number-theory', 'algebra-precalculus']"
1184261,"""Area"" of the topologist's sine curve","Consider the topologist's sine curve : $$
f(x) = \sin\left(1 \over x \right), x \neq 0
$$ The graph of this function resembles a space-filling curve near $x=0$.  It is not a space filling curve, though, because no matter how close we get to the y-axis, there are always points not included in the curve (i.e. $\forall x \neq 0 \exists y \neq \sin(1/x)$).  Indeed, ""most"" points in any given interval are not part of the curve, I think.  Is there a meaningful way to express the ""area"" which this curve fills or appears to fill? More informally, how much of this image is blue?",['general-topology']
1184288,Why the whole exterior algebra?,"So, I've been reading up on multilinear algebra a bit. In particular, I've been reading up on the construction of of the exterior algebra of a finite dimensional vector space $X$, say over $\mathbb{R}$. 
$$ \Lambda(X) = \bigoplus_{n \geq 0} \Lambda^k(X) $$ I'm still at that frustrating early stage where the definitions seem very unmotivated. I'm hoping for some suggestions for improving my grip on them. Let me describe one particular thing which is bothering me in hopes that my concerns are easily dispelled.  I don't understand the point of having a product $\Lambda(X) \times \Lambda(X) \to \Lambda(X)$ instead of just paying attention to the products $\Lambda^k \times \Lambda^\ell(X) \to \Lambda^{k+\ell}(X)$ which seem to be all that is important. My problem may be that the only case I have any experience with is the case $X=X^*$ (a dual space) in which case the elements of $\Lambda^k(X^*)$ can be identified with alternating $k$-linear functionals $X^k \to \mathbb{R}$. It seems strange to me to want to put the forms of different ranks together in the same algebra. What is the use of an expression of mixed rank like 
$$\omega = dx + dy \wedge dz$$ 
which is, I suppose, an element of $\Lambda((\mathbb{R}^3)^*)$? I think the thing which irritates me the most is that these mixed expressions do not even necessarily alternate! I mean, for Pete's sake, look!
\begin{align*}
\omega \wedge \omega & = (dx + dy \wedge dz) \wedge (dx + dy \wedge dz) \\
&= dx \wedge dy \wedge dz + dy \wedge dz \wedge dx \\
&= dx \wedge dy \wedge dz + (-1)^2 dx\wedge dy \wedge dz \\
&= 2 dx \wedge dy \wedge dz \\
&\neq 0
\end{align*}
What's the point of considering all these extraneous elements whose wedge square isn't even zero? Now one answer to my question might be ""well, isn't it useful to consider polynomials which aren't of homogenuous degree?"". I don't think this is good enough for me though. Until I see why it is really useful to put the ""$\Lambda^k(X)$""s together into an algebra, I'm going to be wary of the object $\Lambda(X)$. Added: I noticed there is some relevant information at this thread .","['vector-spaces', 'abstract-algebra', 'linear-algebra', 'category-theory', 'multilinear-algebra']"
1184297,Selecting the Real Analysis Textbooks,"I am a sophomore in the US with double majors in mathematics and microbiology. I am interested in self-studying real analysis since it will help me with my current research in computational microbiology, prepare for upcoming math research (starting this Fall) on analytic number theory, and prepare for the real analysis course I will take this Fall and Putnam competition. I just finished Calculus with Analytic Geometry by G. Simmons, How to Prove It by Daniel Velleman, and How to Solve It by G. Polya. I also read some portions of Apostol's Calculus Vol. I to get a deeper view on calculus theories. (I was originally planning to read Apostol's Calculus Vol. I and Spivak's Calculus first, but I think it would be a better idea to start with real analysis since it covers all the ideas in those ""advanced calculus"" textbooks and much more.) My current plan is to start with one ""dumbed-down"" real analysis textbook and one ""comprehensive, detailed, and intermediate"" textbook, and advance into Rudin's Principles of Mathematical Analysis (required textbook for my real analysis course) starting this Summer, and use it in accordance with other real analysis textbooks. Could you help me on selecting one book from each category? Elementary Real Analysis textbooks: Elementary Analysis: The Theory of Calculus (Kenneth Ross) Understanding Analysis (Steven Abbott) The Way of Analysis (Robert Strichartz) Real Mathematical Analysis (Charles Pugh) Intermediate, detailed Real Analysis textbooks: Mathematical Analysis (Tom Apostol) Undergraduate Analysis (Serge Lang) Introduction to Real Analysis (Bartle, Sherbert) Elements of Real Analysis (Bartle, Sherbert) Mathematical Analysis I (Vladimir Zorich)","['reference-request', 'real-analysis', 'analysis']"
1184304,Inverse Limits in Galois Theory,"I am currently taking a first course in Galois Theory and we are studying finite fields at the moment. In the lectures we have defined the inverse limit of an inverse system of finite groups and had the example of the p-adic integers. However, I am struggling to actually see what an inverse limit actually looks like. Also, it seems implicit from my notes that it is itself a group although I don't know what the operation is (pointwise maybe?). Altogether, this topic seems to have come out of the blue and I don't really understand the basic points of inverse limits. I would be very grateful if someone could give me some basic examples/intuitions. Many thanks!","['galois-theory', 'group-theory', 'abstract-algebra']"
1184309,Show that $\lim_{r\to 0} \frac{1}{r^2}\int_{C_{r}}f(z)dz = 2 \pi i\frac{\partial f}{\partial \bar{z}}(z_0)$,"Well, after spending hours on this problem, I'm still stuck, so I thought I'd turn to you guys. The problem statement is as follows. Let $f$ be a complex-valued function that is $C^1$ in the disk $|z - z_0| < R$ for some $R>0$, and let $C_r$ be the circle $|z - z_0| = r$. Show that $$\lim_{r\to 0} \frac{1}{r^2}\int_{C_{r}}f(z)dz = 2 \pi i\frac{\partial f}{\partial \bar{z}}(z_0).$$ I tried parametrizing the circle in the usual way and differentiating to see if I get something useful, but no luck. I just can't get the LHS to give me something to do with $\frac{\partial f}{\partial \bar{z}}$, so that I could then put the RHS under the integral and show that the difference between the two can be less than arbitrary $\epsilon > 0$ by the continuity of partial derivatives. Thanks in advance for your help!","['complex-analysis', 'contour-integration']"
1184342,"""Redundant"" finite subcovering of a compact space.",Let $M$ be compact and $\mathcal{U}$ an open covering of M such that each $p \in M$ is contained in at least two members of $\mathcal{U}$. Show that $\mathcal{U}$ reduces to a finite subcovering with the same property. I've been struggling with the question for quite a while but did not get anywhere..,"['general-topology', 'compactness', 'real-analysis']"
1184362,A conceptual link between trees and Polish spaces,"Could somebody explain to me why trees are so relevant for the study of Polish spaces and descriptive set theory? I still do not get the proper connection (...and when I think I got it – see the Lusin scheme – I find out my intuition failed). Any feedback or answer is most welcome. 
Thank you for your time.","['general-topology', 'set-theory', 'descriptive-set-theory']"
1184373,"About the integral $\int_{0}^{+\infty}\sin(x\,\log x)\,dx$","It is an interesting exercise to show that the function $f(x)=\sin(x\log x)$ is Riemann-integrable over $\mathbb{R}^+$ (as shown by robjohn in this related question , for instance). Even more interesting is to notice that: $$\int_{0}^{1}f(x)\,dx = \sum_{n\geq 0}\int_{0}^{1}\frac{(-1)^n x^{2n+1}\left(\log x\right)^{2n+1}}{(2n+1)!}\,dx=\sum_{n\geq 1}\frac{(-1)^n}{(2n)^{2n}},\tag{1}$$
with a series related with the one appearing in many sophomore's dreams . Moreover, by using Lambert's W function it is not difficult to check that:
$$ I = \int_{1}^{+\infty}\sin(x\log x)\,dx = \int_{0}^{+\infty}\frac{\sin u}{1+W(u)}\,du. \tag{2} $$ Now my question: is it possible to give a nice closed form to the RHS of $(2)$ through countour integration, the residue theorem or other techniques? For instance, is there a closed form for the almost-digamma-sum:
$$ g(x)=\sum_{n\geq 0}\left(\frac{1}{1+W(x+2n\pi)}-\frac{1}{1+W(2n\pi)}\right) \tag{3}$$
? If so, we have just to compute $\int_{0}^{2\pi}g(x)\sin x\,dx$.","['sequences-and-series', 'calculus', 'integration', 'complex-analysis', 'contour-integration']"
1184416,"Prove if $a_n$ converges to $0$ and $b_n$ is bounded, then $a_n b_n$ converges to $0$","We have these two hypothesis: $$\forall\epsilon_1>0, \exists n_0 | n>n_o \implies |a_n|<\epsilon_1$$
$$|b_n|<M$$
where $M$ is the sequence bound. Therefore, I've used hypothesis 2 to multiply both sides in the hypothesis 1 so we have: $$\forall\epsilon_1>0, \exists n_0 | n>n_o \implies |a_n b_n|<\epsilon_1M$$ then if we choose $\epsilon = \epsilon_1 M$ we have: $$\forall\epsilon>0, \exists n_0 | n>n_o \implies |a_n b_n|<\epsilon$$ Am I right?","['sequences-and-series', 'real-analysis', 'limits']"
1184436,Regarding proof of definite integral of odd functions,"I know that $\int_{-a}^a f(x) \, dx = 0$ if $f$ is an odd function, but I'm having difficulty understanding the proof behind it. Every website I find is telling me basically the same proof in some form or another: here's one of them (scroll down to p. 7) As you can see in the link, they have this: $$\int_{-a}^a f(x)\,dx=\int_{-a}^0f(x)\,dx+\int_0^af(x)\,dx$$ $$\int_{-a}^a f(x)\,dx=-\int_0^{-a}f(x)\,dx+\int_0^af(x)\,dx$$ From which they do a $u$-substitution to get: $$\int_{-a}^a f(x)\,dx=\int_0^{a}f(-u)\,du+\int_0^af(x)\,dx$$ I understand up to this point. What I don't understand is that in this proof and in every proof I've found , they somehow pull this off in the next step: $$\int_{-a}^a f(x)\,dx=\int_0^{a}f(-x)\,dx+\int_0^af(x)\,dx$$ How is this possible? Didn't we already define $u$ to be $-x$? How can we just turn around and claim $x=u$ again?","['definite-integrals', 'integration', 'functions']"
1184445,Torus diffeomorphic to $S^1\times S^1$. [duplicate],"This question already has an answer here : Prove that a standard torus is diffeomorphic to $ \mathbb S^1\times \mathbb S^1$ (1 answer) Closed 7 years ago . This is an exercise from Guillemin/Pollack's Differential Topology . In a previous exercise, I'm asked to give a complete set of parametrizations of $S^1\times S^1$, which I've succeeded in (I think) by the likes of $$\begin{align}\varphi_1(u,v) &= (u,\sqrt{1-u^2},v,\sqrt{1-v^2}) \\ \varphi_2(u,v) &= (u,-\sqrt{1-u^2},v,\sqrt{1-v^2}) \end{align} \\ \vdots\\ \text{etc.}$$ (defined over $[0,1]^2).$ Then, the torus $\Bbb T_{a,b}$ (my notation) is defined as the set of points in $\Bbb R^3$ that are at distance $b < a$ from the circumference in the plane $\{z = 0\}$ of radius $a$ centered at the origin. The exercise is to prove that $\Bbb T_{a,b}$ is diffeomorphic to $S^1\times S^1$. I'm very stuck because I can't seem to characterize neighborhoods of points in $\Bbb T_{a,b}$, can someone help? Answers or just hints are welcome (but I might ask follow up questions in the latter case :) ).","['differential-topology', 'smooth-manifolds', 'differential-geometry']"
1184446,Is E(X|Y) $\sigma(X)$-measurable?,"From the definition of conditional expectation, we have that E(X|Y) is $\sigma(Y)$-measurable. I wonder if E(X|Y) is also $\sigma(X)$ measurable? It seems to be true since E(X|Y) is a ""coarser"" version of X. Thanks!","['measure-theory', 'probability', 'random-variables']"
1184452,Is the Russell paradox the only possible contradiction to the axiom schema of comprehension due to Frege (1893)? $\{x:P(x)\}$ [duplicate],"This question already has answers here : Paradox of General Comprehension in Set Theory, other than Russell's Paradox (2 answers) Closed 9 years ago . Is the Russell paradox the only possible contradiction to the axiom schema of comprehension due to Frege (1893)? The axiom that says that
if $\varphi$ is a property, then there exists a set 
$Y = \{X: \varphi(X)\}$ of all
elements having property $\varphi$. If not then what are other paradoxes that result from that axiom?",['elementary-set-theory']
1184490,Is the identity matrix the only matrix which is its own inverse?,"I just gave a proof for this question . Here's my follow up question: Let $A \in \ \mathbb{M}_n(\mathbb{F})$ where F is a field and there exists $n\in N$  where $A^n$= I. In the case where n=1,2, $A^1$=I and $A^2$=I. Here's my question: In general, if A is it's own inverse, then does it necessarily follow A=I? In other words, is I the only matrix which is it's own inverse? My gut reaction is to say no, but it would probably be fairly tedious to construct a matrix multiplication formula which produces the subset $S\subset \mathbb{M}_n(\mathbb{R})$ where S = {A | AA =I }. Is there such a subset in general? We know the set's nonempty since $I\in S$. Are there any others?","['matrices', 'linear-algebra', 'abstract-algebra']"
1184499,How to solve a hard integral?,"How prove $ \displaystyle \int _{ 0 }^{ \infty }{ (1+x)\arctan { (x) }  } \log^4 { (x) }{\frac{1}{\sqrt{x}(1+x^2)}}   dx=\frac{57\pi^6\sqrt{2}}{64}  $ I found this integral using numerical values.I think the result is correct!
How to prove this result?","['definite-integrals', 'improper-integrals', 'calculus', 'integration']"
1184515,Finding power series solution to differential equation $(1-x)y'=y$ centered at $ x=0$,"I'm trying to teach myself how to solve differential equations with power series. I am stuck on working through $$(1-x)y'=y \text{ centered at } x= 0.$$ I've gotten to the point where I must figure out what $a_{k+1}$ is and I'm completely lost on what to do next... can someone please check if I'm on the right track here, and if so, where to go from here? My work: http://i.imgur.com/KodxeCY.jpg (another photo: i.imgur.com/GaSTqag.jpg) I know from a non-power series method that answer will be of the form $y = \frac{c_1}{1-x}$. FWIW, I've been following this video: https://www.youtube.com/watch?v=RJJKq7Uc-9I with a different equation. If you know of a better method for this, please let me know!","['sequences-and-series', 'power-series', 'ordinary-differential-equations', 'calculus']"
1184525,"Prove that if $A \subseteq B$ and $B \subset C$, then $A \subset C$.","Prove that if $A \subseteq B$ and $B \subset C$, then $A \subset C$. Proof: $A \subseteq B \Longrightarrow \forall x\in A, x \in B.$ Since $B \subset C$, it follows that $x \in B \Longrightarrow x \in C$ but $\exists c \in C \ni c \notin B.$
Since $A \subseteq B$, it follows that $c \notin A$, thus $A \subset C$. Is this good enough?","['elementary-set-theory', 'proof-verification']"
1184532,Inequality involving taking expectations,"There are three convexly decreasing functions $f, g, h:\mathbb{R^+}\rightarrow \mathbb{R^+}$, and $f(x)h(x)<1$ for $\forall x$.
I need to prove that $E[f(x)^2]E[g(x)h(x)]<E[f(x)g(x)]\left(1+E\left[f(x)h(x)\right]\right)$ for an arbitrary probability distribution.","['calculus', 'probability']"
1184541,What exactly is the difference between weak and strong induction?,"I am having trouble seeing the difference between weak and strong induction. There are a few examples in which we can see the difference, such as reaching the $k^{th}$ rung of a ladder and proving every integer $>1$ can be written as a product of primes: To show every $n\ge2$ can be written as a product of primes, first we note that $2$ is prime. Now we assume true for all integers $2 \le m<n$ . If $n$ is prime, we're done. If $n$ is not prime, then it is composite and so $n=ab$ , where $a$ and $b$ are less than $n$ . Since $a$ and $b$ are less than $n$ , $ab$ can be written as a product of primes and hence $n$ can be written as a product of primes. QED However, it seems sort of like weak induction, only a bit dubious. In weak induction, we show a base case is true, then we assume true for all integers $k-1$ , (or $k$ ), then we attempt to show it is true for $k$ , (or $k+1$ ), which implies true $\forall n \in \mathbb N$ . When we assume true for all integers $k$ , isn't that the same as a strong induction hypothesis? That is, we're assuming true for all integers up to some specific one. As a simple demonstrative example, how would we show $1+2+\cdots+n= {n(n+1) \over 2}$ using strong induction? (Learned from Discrete Mathematics by Kenneth Rosen)","['discrete-mathematics', 'definition', 'logic', 'induction', 'proof-writing']"
1184572,Where has this unusually imaginary number been sighted?,"POSTSCRIPT: Haste makes waste.  I wrote ""no member of $\mathbb C\cup\{\infty\}$ behaves like this"".  In fact $\infty$ does the job.  Accordingly, I have revised the question at the bottom of this post. END OF POSTSCRIPT $\newcommand{\d}{\diamond}$In this previous question I asked something about the binary operation
$$
a\d b = \frac{a+b}{1+ab},
$$
which is conjugate via the involution $a\mapsto\dfrac{1-a}{1+a}$ to multiplication.  I think of the domain as being the set $D=(\mathbb C\cup\{\infty\})^2\setminus\{(\pm1,\mp1)\}$. I have noticed that
$$
\frac 1 a \d b = a\d \frac 1 b = \frac 1 {a\d b}. \tag 1
$$
Since this operation is associative, this identity seems to invite us to imagine a number $R$ (for ""reciprocal"") such that for all $a$ we have
$$
R\d a = \frac 1 a
$$
so that $(1)$ would say
$$
(R\d a)\d b = a\d (R\d b) = R\d (a\d b).
$$ However, no member of $\mathbb C\cup\{\infty\}$ behaves like this. This reminds me of the identity $f'\ast g = f\ast g'=(f\ast g)'$ where ""$\ast$"" is convolution.  There is no function whose convolution with $f$ is $f'$, but there is the ""generalized function"" $\delta'$, the derivative of Dirac's delta function. So MY QUESTION IS whether this creature that I have called $R$ has been sighted in literature that can be cited?  (Construe ""literature"" broadly to include all the crap on the internet, etc.) Later revision of the question: Are the identities $(1)$ and $\infty\d a=\dfrac 1 a$ ""out there"" somewhere? (My interest in all this is really an interest in applying this to some geometry problems, concerning which I haven't sorted out all of my thoughts yet.) (I disagree with the deprecation of the ""algebra"" tag or I'd have used it here.)","['reference-request', 'abstract-algebra']"
1184608,"In $Z_{24}$, list all generators for the subgroup of order 8","Q. In $\Bbb Z_{24}$, list all generators for the subgroup of order $8$. So, I know that the divisors of $24$ which are $1,2,3,4,6,8,12
$ and $24$ are the orders of the sets in the subgroup. I'm not sure if this is a trick question but I was only able to find one generator which is $\langle 3\rangle$, so was the plural in generators unnecessary? Or am I missing a generator.",['abstract-algebra']
1184611,"In terms of matrices: $\forall v\in V,\phi(\phi(v))=0$","$\phi: V\to V$( a linear operator here) How to interpret $\forall v\in V,\phi(\phi(v))=0$ in terms of matrices? Can I have some hint? I suppose $\phi(V)= \begin{bmatrix} \phi(v_1)\\\phi(v_2)\\\dots\\\phi(v_n)\end{bmatrix}$ for $V=\dim n$","['linear-transformations', 'matrices', 'abstract-algebra']"
1184632,Show a multivariable function is differentiable at a point,"Define $F(x,y)=(x^2, xy + y^2)$.  How do I show that $F(x,y)$ is differentiable at the point $A=(a,b)$ without using any known theorems? Am I correct by showing that the limit exists, i.e. $lim_{h \to 0} (FA+h)-F(h)/h$? I broke it up into two parts, first, holding 'b' constant and then having $a + h \to h$ as $h \to 0$.  Then I held 'a' constant and did $b + h \to b$ as $h \to 0$.  This got me all 4 partial derivatives, which I know are correct. Is this the correct approach, or am I missing any other important steps?","['multivariable-calculus', 'real-analysis']"
1184657,How to show that all solutions of $ay''+by'+cy=0$ approach $0$ as $t \rightarrow \infty$,"Given $ay''+by'+cy=0$ and assuming that $a, b, c > 0$ show that all solutions approach $0$ as $t\rightarrow\infty$ I was able to begin by seperating the problem into three cases: Case 1: Repeated roots of the characteristic equation I was able to find the limit as $t\rightarrow\infty$ for the general solution to Case 1 using l'Hospital's Rule: Fairly straightforward. Case 2: Imaginary roots of the characteristic equation I was able to find the limit as $t\rightarrow\infty$ for the general solution to Case 2 using the squeeze theorem: Also fairly straightforward. Case 3: Two distinct real roots of the characteristic equation In this case the two roots are:
$$r_1=\frac{-b+\sqrt{b^2-4ac}}{2a}$$
$$r_2=\frac{-b-\sqrt{b^2-4ac}}{2a}$$ and the general solution is:
$$y=c_1e^{r_1t}+c_2e^{r_2t}$$ The only way I see that $\lim_{t\to\infty} y=0$ is if $r_1,r_2<0$.  However if the only information I am given is that $a,b,c>0$ how can I prove that $r_1,r_2<0$?","['ordinary-differential-equations', 'limits']"
1184658,"If a group $G$ contains an element a having exactly two conjugates, then $G$ has a proper normal subgroup $N \ne \{e\}$","If a group $G$ contains an element a having exactly two conjugates, then $G$ has a proper normal subgroup $N \ne \{e\}$ So my take on this is as follows: If we take $C_G(S)$ of S. This is a subgroup of G. If $C_G(S)=G$ , then S has no conjugate but itself, so therefore $C_G(S)$ is a proper subgroup. If we suppose $C_G(S)=\{e\}$ ,then in order for there to be exactly two conjugates of S, then For every $a \ne b \in G \ \{e\}, bxb^{-1} =axa^{-1}$ but $bxb^{-1}=axa^{-1} \to (a^{-1}b)xb^{-1}=xa^{-1} \to (a^{-1}b)x(a^{-1}b)^{-1}=x \to a^{-1}b \in C_G(S)$ Which means that $C_G(S)$ is actually nontrivial or that $a^{-1}b=e$ if and only if $a=b$ , which would be a contradiction. Thus $C_G(S)$ is a nontrivial proper subgroup. Since there are exactly 2 conjugacy classes of S and they are in one to one correspondence with cosets of S, its centralizers' index $[G:C_G(S)]=s$ . Subgroups of index $2$ are normal, so $C_G(S)$ is a proper nontrivial normal subgroup. This approach seemed very different from other examples I have seen so I guess I am wondering if this approach makes sense.","['group-theory', 'abstract-algebra']"
1184671,Is There a de Rham Homology,"For differential manifold category, we can introduce the differential form to make up a cochain, and then get the de Rham cohomology group. My question is that if we use $\text{Hom}$ functor to get the dual chain of the cochain, then does the new homology satisfies the Eilenberg-Steenrod axioms? If so, is there any reference to study it? Any advice is helpful. Thank you.","['homology-cohomology', 'reference-request', 'differential-geometry']"
1184685,Prove by Induction Summation,"Prove by induction: Given that $f(x) = x^{-1}$, then the $k$-th derivative of $f$ is given by $f^{\langle k\rangle}(x) = (−1)^k \cdot k!\;x^{−(k+1)}$ for all $k ≥ 1$. How do I go about proving this? I am very confused as to what the problem is asking and how I can even use induction to solve it. Thank you!","['proof-writing', 'discrete-mathematics', 'proof-verification', 'induction']"
1184718,Covariance and Independence problem,"The Random Variables $X$ and $Y$ can each take on only two values. Show that if $Cov(X,Y)=0$, then $X$ and $Y$ are independent. One can see that the distributions take the form: $P(X=x_1)=p$ and $P(X=x_2)=1-p$ $P(Y=y_1)=q$ and $P(Y=y_1)=1-q$ To show independence one must show that, $P(X=x_i,Y=y_j)=P(X=x_i)P(Y=y_j)$ $\forall i,j$ in {$1,2$}. As the $Cov(X,Y)=0$, then $E[XY]=E[X]E[Y]$, $(1)$ so obviously this can be used to demonstrate the independence. Do I just need to slot in for $(1)$ and investigate? And the above distributions are clearly binomial in nature where $n=1$. Any ideas on how I would continue?","['statistics', 'descriptive-statistics', 'covariance', 'probability-theory']"
1184742,What is the gradient with respect to a vector $\mathbf x$?,"What is the meaning of ""gradient with respect to $\mathbf x$""? http://en.wikipedia.org/wiki/Gradient I am talking about the symbol $$\nabla_\mathbf x$$ Does that simply mean derivative with respect to $\mathbf x$?","['multivariable-calculus', 'calculus']"
1184749,Extending a Chebyshev-polynomial determinant identity,"The following $n\times n$ determinant identity appears as eq. 19 on Mathworld's entry for the Chebyshev polynomials of the second kind : $$U_n(x)=\det{A_n(x)}\equiv \begin{vmatrix}2 x& 1 &  0 &\cdots &0\\ 1 & 2x &1  &\cdots &0 \\
0 & 1 & 2x &\cdots &0\\0 & 0 & 1 & \ddots & \vdots \\
\vdots & \ddots & \ddots &\ddots & 1\\ 0 & 0 & \cdots & 1 & 2x\end{vmatrix}$$ as can be proven (for example) by expanding by minors to get the recurrence relation for $U_n(x)$. While working on a spectral problem for my research, I noticed that this result can be extended. Suppose we consider the determinant of $A_n(x)+t\,\mathbf{e}_k \mathbf{e}^T_k$ where $t$ is some free parameter and $k$ is some index. Then the $k$-th column vector may be expressed as $$\mathbf{e}_{k-1}+(2x+t)\mathbf{e}_k+\mathbf{e}_{k+1}=
(\mathbf{e}_{k-1}+2x\,\mathbf{e}_k+\mathbf{e}_{k+1})+t\,\mathbf{e}_k.$$ Since the determinant is an linear function of its $k$-th column vector, we can expand in two terms: The first is just $\det{A_n(x)}=U_n(x)$, and for the second we can expand by minors to get a block diagonal matrix $\text{diag}(A_{k-1}(x),A_{n-k}(x))$ with determinant simplying to $U_{k-1}(x)U_{n-k}$. Putting these together gives the result $$\det{A_n(x)+t\,\mathbf{e}_k\mathbf{e}^T_k}=U_n(x)+t \, U_{k-1}(x)U_{n-k}(x).$$ One can similarly introduce a second parameter at a different row vector and compute the resulting determinant. Hence there should be a well-defined answer to the following question: Given a set of $n$ parameters $\{t_k\}$, express $\det{(A_n(x)+\text{diag}(\{t_k\}))}$ in terms of $\{U_n(x)\}.$","['matrices', 'linear-algebra', 'chebyshev-polynomials', 'determinant']"
1184767,"Evaluating $\int_0^{2 \pi} e^{\cos x} \cos (nx - \sin x) \,dx$ using complex analysis","I'm taking a complex analysis course and doing some practice computing residues & evaluating integrals. I pulled out an old book called ""The Cauchy Method of Residues: Theory and Applications, Volume I"" On page 196-197, there are some interesting integrals to evaluate. I'm at 5.4.3.10.: I was able to do question 1, but was stumped at how to even begin with question 2: Evaluate the integral
  $$\int_0^{2 \pi} e^{\cos x} \cos (nx - \sin x) \,dx ,$$
  where $n$ is an natural number. The answer is simply $\frac{2 \pi}{n}$. Any hints?","['complex-analysis', 'contour-integration']"
1184803,Proof of $\mathbb{Q}$ is not cyclic,"So i'm trying to understand the proof of: $\mathbb{Q}$ is not cyclic. So this is the proof: We proceed by contradiction. Suppose $\mathbb Q$ is cyclic then it would be generated by a rational number in the form $\frac{a}{b}$ where $a,b \in \mathbb{Z} $ and $a$, $b$ have no common factors. Also, $a,b \neq 0$. The set $\langle\frac{a}{b}\rangle$ consists of all integer multiples of $\frac{a}{b}$. Therefore, if $\mathbb{Q}=\langle\frac{a}{b}\rangle$, then $\frac{a}{2b}$ is an integer multiple of $\frac {a}{b}$ PROBLEM : Why is $\frac{a}{2b}$ an integer multiple? or how is it an integer multiple. I'm not seeing it because isn't $\frac{a}{b} \times \frac{a}{b}=\left(\frac{a}{b}\right)^2$ Anyways, here is the rest of the proof: but if $c \times \frac{a}{b}=\frac{a}{2b}$
then $c=\frac{1}{2}$ is not an integer. Thus, $\mathbb{Q}$ cannot be generated by a single rational number and is not cyclic. If anyone can clarify that would be great. Also, another problem I have is doesn't this show that $\mathbb{Q}-\{0\}$ is not cyclic because I thought $\mathbb{Q}$ under the operation multplication is not a group unless zero is removed.",['abstract-algebra']
1184825,Upper Triangular Block Matrix Determinant by induction,"We want to prove that: 
$$\det\begin{pmatrix}A & C \\ 0 & B\\ \end{pmatrix}= \det(A)\operatorname{det}(B),$$
where $A \in M_{m\times m}(R)$, $C \in M_{m\times n}(R)$,$B \in M_{n\times n}(R)$ and $R$ is a commutative ring with $1$. $\textbf{SOLUTION}:$ Let $L = \begin{pmatrix}A & C \\ 0 & B\\ \end{pmatrix},$ then its clear that $L \in M_{(m+n)\times(m+n)}(R)$ and the matrix $L$ can be represented as follows:
$$L = \begin{pmatrix}\begin{bmatrix} 
a_{11} &a_{12}  &\ldots   & a_{1m}  \\ 
a_{21} &a_{22}  &\ldots   & a_{2m}  \\ 
\vdots  & \ddots  & \ddots  &\vdots  \\ 
 a_{11} &a_{12}  &\dots   & a_{mm}  \\
\end{bmatrix}  & \begin{bmatrix}
c_{11} &c_{12}  &\ldots   & c_{1n}  \\ 
c_{21} &c_{22}  &\ldots   & a_{2n}  \\ 
\vdots  & \ddots  & \ddots  &\vdots  \\ 
 c_{m1} &c_{m2}  &\dots   & c_{mn} \\
\end{bmatrix} \\ \\ \begin{bmatrix}
0 &0  &\ldots   & 0  \\ 
0 &0  &\ldots   & 0  \\ 
\vdots  & \ddots  & \ddots  &\vdots  \\ 
 0 &0  &\dots   & 0 \\
\end{bmatrix}& \begin{bmatrix}
b_{11} &b_{12}  &\ldots   & b_{1n}  \\ 
b_{21} &b_{22}  &\ldots   & b_{2n}  \\ 
\vdots  & \ddots  & \ddots  &\vdots  \\ 
 b_{n1} &b_{n2}  &\dots   & b_{nn} \\
\end{bmatrix}\\ \end{pmatrix}$$ Now, for each fixed $i \in \{1,2,\ldots,m,m+1,\ldots,m+n\}$ ($i$ is the row), We have: $$\det(L) = \sum\limits_{j=1}^m (-1)^{i+j}\alpha_{ij}\det(L_{ij}) + \sum\limits_{j=m+1}^{m+n} (-1)^{i+j}\alpha_{ij}\det(L_{ij})$$
where: $$
\ \alpha_{i,j} = \begin{cases} 
      a_{i,j} & i,j = 1,\ldots , m \\
      c_{i,j-m} & i = 1,\ldots , m & j = m+1, \ldots ,m+n \\
      0 & i= m+1, \ldots ,m+n & j = 1,\ldots , m \\
      b_{i-m,j-m} & i = m+1,\ldots ,m+n & j = m+1, \ldots, m+n
   \end{cases}
\\
$$
and
$$
\\ L_{i,j} = \begin{cases} 
      A_{i,j} & i,j = 1,\ldots , m \\
      C_{i,j-m} & i = 1,\ldots , m & j = m+1, \ldots,m+n \\
      0 & i= m+1, \ldots ,m+n & j = 1,\ldots , m \\
      B_{i-m,j-m} & i = m+1,\ldots ,m+n & j = m+1, \ldots ,m+n
   \end{cases}
\\
$$ $\textbf{FIRST CASE}$ Choose $i > m$, then $i \in \{m+1,\ldots, m+n \}$. Now if $n = 0$, the problem is trivial, since $\det(B) = 1$ and therefore:$$\det(L) = \det(A)\det(B) = \det(A)1 = \det(A).$$
Then, lets assume that 
$$\det(L) = \det(A)\det(B),$$
holds for $n = k$.Then choose $i \in \{m+1,\ldots, m+k \}$, then $\forall j < m+1$, I get: $$(-1)^{i+j}\alpha_{ij}\det(L_{ij}) = 0,$$
Hence: 
$$\det(L) = \sum\limits_{j=m+1}^{m+n} (-1)^{i+j}\alpha_{ij}\det(L_{ij})$$
but $i \in \{m+1,m+2, \ldots, m+k\}$ and $j \in \{m+1,m+2, \ldots, m+k\}$ therefore:
$$\det(L) = \sum\limits_{j=m+1}^{m+k} (-1)^{i+j}b_{ij}\det(B_{ij})$$
Notice that the sub-matrix $B_{ij}$ has the form: $\begin{pmatrix}A & C' \\ 0 & B'\\ \end{pmatrix}$. On the other hand, $B'$ have size smaller than $B$, therefore by our induction hypothesis, we have:
$$\det\begin{pmatrix}A & C' \\ 0 & B'\\ \end{pmatrix} = \det(A)\det(B')$$ $
\textbf{SECOND CASE}
$ On the other hand, Choose $i \leq m$, if $m = 0$, then the problem is trivial, since $\det(A) = 1$ and therefore:
$$\det(L) = \det(A)\det(B) = \det(B) = \det(B).$$ Then, lets assume that  $$\det(L) = \det(A)\det(B)$$ holds for $m = k$. Then choose $i \in \{1,\ldots, k \}$, then for all $j>k$, we have: $$(-1)^{i+j}\alpha_{ij}\det(L_{ij}) = 0,$$ therefore: $$\det(L) = \sum\limits_{j=1}^{k} (-1)^{i+j}\alpha_{ij}\det(L_{ij})$$
but $i \in \{1,2, \ldots, k\}$ and $j \in \{1,2, \ldots, k\}$, therefore:
$$\det(L) = \sum\limits_{j=1}^k (-1)^{i+j}a_{ij}\det(A_{ij})$$ Now, the sub-matrix $A_{ij}$ has the form: $\begin{pmatrix}A' & C' \\ 0 & B\\ \end{pmatrix}$. On the other hand, $A'$ has size smaller than $A$, then by the induction hypothesis, we have:
$$\det\begin{pmatrix}A' & C' \\ 0 & B\\ \end{pmatrix} = \det(A')\det(B) $$ I want to know if this is the right proof.","['ring-theory', 'abstract-algebra', 'matrices', 'linear-algebra', 'determinant']"
1184836,"For a $k$-morphism $X \to Y$ to be determined by $X(\overline{k}) \to Y(\overline{k})$, does $X$ really have to be _geometrically_ reduced?","Question: Does the following hold? Let $k$ be a field, $X$ a reduced scheme locally of finite type over $k$, $Y$ any $k$-scheme and $f,g\colon X \to Y$ two $k$-morphisms. Then $f=g$ if and only if there exists an algebraically closed field extension $\Omega$ of $k$ such that $f(\Omega)=g(\Omega)\colon X_k(\Omega) \to Y_k(\Omega)$ (i.e., the induced maps on $\Omega$-valued points agree). I know that the statement is true if, in addition, $X$ is required to be geometrically reduced and $Y$ is required to be locally of finite type over $k$ (see ""Background""). I am especially interested in the question whether one can weaken the assumption on $X$ as indicated above. Proof (?): (See ""Background"" for why I suspect a mistake here. My suspecting a mistake is also the reason why I am going to be deliberately verbose.) As the ""only if"" statement is clear, we only have to prove the ""if"" statement. Step 1 (Reduction to the affine case): Let $(V_i)_{i \in I}$ be a cover of $Y$ s.t. all $V_i$ are affine, open subschemes. As $X$ is locally of finite type over $k$ so are all $f^{-1}(V_i)$; we can thus write $f^{-1}(V_i) = \bigcup_{j \in J_i} U_{ij}$, $i \in I$, where the $U_{ij}$ are spectra of finitely generated $k$-algebras. Now for all $i \in I, j \in I_j$, the restriction $g|_{U_{ij}}\colon U_{ij} \to Y$ factors scheme-theoretically through the open subscheme $V_i \subset Y$. Here is why: Let $x \in U_{ij}$ be a closed point; then its residue field $\kappa(x)$ is an algebraic extension of $k$, because $U_{ij}$ is locally of finite type over $k$. By choosing a $k$-embedding $\kappa(x) \hookrightarrow \Omega$ we get an $\Omega$-valued point $x \in (U_{ij})_k(\Omega) \subset X_k(\Omega)$ (abuse of notation) with image $x \in U_{ij}$. Thus, $g \circ x\colon \operatorname{Spec} \Omega \to Y$ is an $\Omega$-valued point of $Y$ with image $g(x)$. But by assumption
$$
g \circ x = g(\Omega)(x) = f(\Omega)(x) = f \circ x
$$
and the image of $f \circ x$ is $f(x)$ by construction. We have thus seen $g(x) = f(x) \in V_i$ (remember $x \in U_{ij} \subset f^{-1}(V_i)$). From this it follows that $g(U_{ij}) \subset V_i$ (set-theoretically). Indeed, if $x' \in U_{ij}$ is any point, then $x'$ has a specialization $x \in U_{ij}$ that is closed in $U_{ij}$ (e.g. because $U_{ij}$ is affine). Since continuous maps preserve specialization of points, $g(x')$ is a generization of $g(x)$. Since $V_i \subset Y$ is open, $g(x') \in V_i$ now follows from $g(x) \in V_i$; this last assertion was proved above. Finally, from $g(U_{ij}) \subset V_i$ (set-theoretically) it follows that $g|_{U_{ij}}\colon U_{ij} \to Y$ factors scheme-theoretically through the open subscheme $V_i \subset Y$ because $V_i$ is an open subscheme. To conclude step 1: It clearly suffices to show that (abuse of notation) $f|_{U_{ij}} = g|_{U_{ij}}\colon U_{ij} \to V_i$ for all $i, j$. Moreover, all $U_{ij}$ are reduced (as $X$ is) and from $f(\Omega) = g(\Omega)$ it clearly follows that $f|_{U_{ij}}(\Omega) = g|_{U_{ij}}(\Omega): (U_{ij})_k(\Omega) \to (V_i)_k(\Omega)$ for all $i,j$. Step 2 (proof of the affine case): By step 1, it suffices to show the following: Let $B$ be any $k$-algebra. Let $A$ be a finitely generated, reduced $k$-algebra and $\phi, \psi\colon B \to A$ two morphisms of $k$-algebras. Then $\phi = \psi$ if there exists an algebraically closed extension $\Omega$ of $k$ such that $\phi^* = \psi^*\colon \hom_k(A,\Omega) \to \hom_k(B,\Omega)$ (i.e. for all $\sigma \in \hom_k(A,\Omega)$ we have $\sigma \circ \phi = \sigma \circ \psi$). To this end, let $\mathfrak{m}$ be a maximal ideal of $A$. Then $A/\mathfrak{m}$ is an algebraic field extension of $k$, because $A$ is a finitely generated $k$-algebra. Composing the canonical projection $\pi\colon A \to A/\mathfrak{m}$ with a $k$-embedding $\iota\colon A/\mathfrak{m} \hookrightarrow \Omega$ yields a morphism of $k$-algebras $\iota \circ \pi\colon A \to \Omega$. By assumption we have $\iota \circ \pi \circ \phi = \iota \circ \pi \circ \psi$; since $\iota$ is injective, we get $\pi \circ \phi = \pi \circ \psi$. We have just seen: $\forall b \in B : \phi(b) - \psi(b) \in \bigcap_{\mathfrak{m} \subset A} \mathfrak{m}$, where the intersection is taken over all maximal ideals $\mathfrak{m}$ of $A$. But since a finitely generated $k$-algebra is Jacobson, this intersection $\bigcap_{\mathfrak{m} \subset A} \mathfrak{m}$ is just the nilradical of $A$. Since $A$ was assumed to be reduced, $\phi = \psi$ follows. Background: I have frequently come across variants of the statement in question requiring stronger assumptions on $X$ or $Y$. For example, exercise 5.16 in the book Algebraic Geometry I by Görtz and Wedhorn asks you to prove the following: Let $k$ be a field, let $X$ and $Y$ be $k$-schemes locally of finite type, and let $f,g\colon X \to Y$ be two $k$-morphisms. Assume that $X$ is geometrically reduced over $k$. Then $f=g$ if and only if there exists an algebraically closed extension $\Omega$ of $k$ such that $f$ and $g$ induce the same map $X_k(\Omega) \to Y_k(\Omega)$ on $\Omega$-valued points. See also here , here and here on Math.SE. In particular, I am aware that this answer gives 'an example showing why ""geometrically reduced"" is necessary for the result to hold' - but as far as I can see, in the example given there, $X$ is not even reduced. While I can see why one would not care whether $Y$ is assumed to be locally of finite type or just any $k$-scheme, I am puzzled by $X$ constantly being required to be geometrically reduced. On the other hand, I have the admittedly naive intuition that being geometrically reduced is the ""correct"" requirement if you want to capitalize on another assumption involving an algebraic closure. This is why I am not fully convinced by the above prove (which is the first one I came up with in approaching the stated exericse in Görtz-Wedhorn - only later I realized that it seems to require $X$ to be only reduced). (I should maybe also add that scheme theory is quite new, and the concept of geometrical reducedness very new to me.)",['algebraic-geometry']
1184838,Monotonicity of $\alpha$ in Riemann Stieltjes integral $\int fd\alpha$,"I am recently reading rudin's Principles of Mathematical Analysis, and I am wondering why the monotonicity of $\alpha$ in Riemann Stieltjes integral  $\int fd\alpha$ is always emphasized. For example, if I want to integrate $\int^{a}_{b}{dx^2}$ (here $f=1$ and $\alpha = x^2$), it equals to $\int^{a}_{b}2xdx$ where $\alpha =x^2$ is not monotone. However, in theorem 6.17, it assumes that $\alpha$ is nondecreasing. In most theorem and examples I have encountered, the monotonacity of $\alpha$ is unnecessary. So is there any example to demonstrate the importance of that? Here are some theorem stated in Rudin's book: Theorem 6.17 Assume $\alpha(x)$ increases monotonically and $\alpha'$ is integrable (with respect to $x$) on $[a,b]$. Let $f$ be a bounded real function on $[a,b]$. Then $f$ is $\alpha$-integrable (that is, $\int fd\alpha$ exists) if and only if $f\alpha$ is integrable with respect to $x$. In this case: $$\int^b_afd\alpha=\int^b_af\alpha'dx$$ Theorem 6.18 Suppose $\psi$ is a strictly increasing continuous function that maps an interval $[A,B]$ onto $[a,b]$. Suppose $\alpha$ is monotonically increasing on $[a,b]$ and $f$ is $\alpha$-integrable on $[a,b]$. Define $\beta$ and $g$ on $[A,B]$ by
$$\beta(y)=\alpha(\psi(y))$$$$g(y)=f(\psi(y))$$ Then $g$ is $\beta$-integrable and $$\int^B_Agd\beta=\int^b_afd\alpha$$","['integration', 'analysis']"
1184846,Expectation of product of steps in a branching process.,"I just found an interesting problem from 2013 on Duke's website:
Show that for a branching process ($Z_n$) with expected offspring μ, one has: $$E[Z_nZ_m]=\mu^{n-m}E[Z_m^2] $$ for $0 \leq m \leq n$.
Any ideas?  I can't tell if calculating the covariance first would be useful.  Perhaps, we use conditional probabilities.  I'm not sure how this would play out. Found at: http://sites.duke.edu/probabilityworkbook/covariance-of-a-branching-process/","['probability-theory', 'probability']"
1184866,Hartshorne's Algebraic geometry Chapter III ex. 9.10,"I'm struggling with the exercise said in the title (Hartshorne, III, ex. 9.10). No problems in showing that $\mathbb{P}^1$ is rigid. In the second part, we want to show that $X_0$ being rigid does not imply that $X_0$ does not admit global deformations. The problem asks us to do so building a flat, proper morphism $f:X\to \mathbb{A}^2$ over an algebriacally closed field $k$, with $\mathbb{P}^1$ in the central fiber, but such that for no neighbourhood $U$ of the origin one has $f^{-1}(U)\simeq U\times \mathbb{P}^1$. I'd like to get hints or an example for this part. I know that some properties of the fibres have to be preserved, in particular dimension, degree and arithmetic genus (Hart III, Cor 9.10). I guess this also implies that we want some of the fibres to be singular (otherwise the geometric genus being equal to the arithmetic would force an isomorphism with $\mathbb{P}^1$), but I don't know how to obtain these singularities, or if the problem can be avoided considering a field different from the complex numbers. I haven't tried to think about the third part of the problem yet, but any hints about that one would be well accepted anyway.","['flatness', 'algebraic-geometry', 'deformation-theory']"
1184895,Using jordan form to find nilpotent $4\times 4$ matrices,"Related to this question. I am trying to understand how to show all nilpotent matrices of size $4\times 4$ using the Jordan normal form. From what I understand, we string together Jordan blocks of different sizes. I am trying to find how to find all forms of some matrix $A$ where $A$ is a $4\times 4$ matrix, for $A^2=0$ using these blocks. Since all the eigenvalues of a nilpotent matrix are $0$, the Jordan form should look like this I thought: $$\def\b{\begin{bmatrix}}\def\e{\end{bmatrix}} \b \lambda&1&0&0\\0&\lambda&1&0\\0&0&\lambda&1\\0&0&0&\lambda\e=\b 0&1&0&0\\0&0&1&0\\0&0&0&1\\0&0&0&0\e$$ As a single Jordan block(which seems right, since I have one eigenvalue $0$) This isn't nilpotent of degree $2$ Also Will Jagy suggested in my previous question that I use $2$ $2\times 2$ matrices, or a $2\times 2$ and two $1\times 1$ blocks. I can see that two $2\times 2$ blocks do give me $A^2=0$, but I can't get this to work with one $2\times 2$ block and two $1\times 1$ blocks.","['matrices', 'nilpotence']"
1184899,Does there exist a function such that $f(a)f(b)=f(a^2b^2)?$,"Given $S=\{2,3,4,5,6,7,\cdots,n,\cdots,\} = \Bbb N_{>1}$, prove whether there exists a function $f:S\to S$, such that for any positive $a,b$:
  $$f(a)f(b)=f(a^2b^2),a\neq b?$$ This is 2015 APMO problem 2 (this event ended yesterday: see APMO ), maybe I think didn't exist such a function, but I can't prove it. My attempt: Consider $p_{i}$ is $i^{th}$ prime,such $f(2)=2^2,f(3)=5^2,f(5)=11^2,\cdots?$","['functions', 'functional-equations']"
1184923,Associated bundles: isomorphism between spaces of differential forms.,"I think this will be an easy question for numerous people. Let $\pi:P\rightarrow M$ be a principal bundle and $\rho:G\rightarrow GL(V)$ a representation. The space of $k$ forms on $M$ with values in $P\times_G V$ (denote as $\Omega^k(M;P\times_G V)$  can be identified with with the space of horizontal, right invariant $k$-forms on $M$ (denote as $\Omega^k_G(P;V))$. Ie, there is an isomorphism: $\Omega^k_G(P;V)\cong \Omega^k(M;P\times_G V)$. I am reading through some lecture notes which say Let $\overline{\zeta}\in \Omega^k_G(P;V)$. Define $\zeta_{\alpha}=s_{\alpha}^*\overline{\zeta}\in \Omega^k(U_{\alpha};V)$. ($s_{\alpha}$ is the local section $s_{\alpha}:U_{\alpha}\rightarrow P$). It then asks to show that $\{\zeta_{\alpha}\}$ define a form in $\Omega^k(M;P\times_G V)$ by showing that the 'gluing' equation is satisfied $\zeta_{\alpha}=\rho(g_{\alpha\beta})\circ \zeta_{\beta}$. Here $g_{\alpha\beta}$ is the transition functions related to the local trivialisations which satisfy $s_{\beta}(m)=s_{\alpha}(m)g_{\alpha\beta}(m)$. I have managed to show that the required equation holds. My question is - why do the constructed $\zeta_{\alpha}\in \Omega^k(U_{\alpha};V)$ define forms in $\Omega^k(M;P\times_G V)$? I understand that $P\times_G V$ has the structure of a fibre bundle with typical fibre $V$, but I am not sure why the gluing equation is important. I am guessing it has something to do with the fact that because the equation holds, one is able to extend the local definition go a global one. I'm not sure. If someone can help that would be great.","['principal-bundles', 'differential-geometry', 'differential-forms', 'manifolds', 'lie-groups']"
1184926,Does Markov Chain converge in Variance Norm?,"Assume the chain $\{X_n\}_{n\in\mathbb{N}}$ on the statespace $(S,\mathcal{F})$ is aperiodic, irreducible and additionally positive recurrent. We denote with $\pi$ its (unique) stationary distribution. Is it true that for any $x\in S$ $$\underset{n\rightarrow\infty}{\text{lim}}~\underset{\text{A}\in\mathcal{F}^{\mathbb{Z}_+}}{\text{sup}}~|P_x((X_{n},X_{n+1},\dots)\in\text{A})-P_\pi(X\in A)| =0?$$ I would assume this is true since as a positive recurrent chain it keeps hitting again and again also in its asymptotics. But I can't figure out how to prove it. I tried to come up with any clever coupling which seems to be the way to deal with those Variance norms (see Durrett Chapter 6, Convergence Theorem).","['probability-theory', 'stochastic-processes', 'markov-chains']"
1184937,calculation of the determinant of a block matrix little help,"I need to prove 
  $$\operatorname{det}\begin{pmatrix}A & B \\ C & D\\ \end{pmatrix}= \operatorname{det}(DA-CB),$$
  where $A,B,C,D \in M_{n\times n}(R)$ with the property that $A$ and $B$ commute and moreover $\operatorname{det}(B) \neq 0,$ using the following hint: $$\begin{pmatrix}A & B \\ C & D\\ \end{pmatrix}\begin{pmatrix}B & 0 \\ -A & I_n\\ \end{pmatrix}.$$ Using the hint, I have:
$$\begin{pmatrix}A & B \\ C & D\\ \end{pmatrix}\begin{pmatrix}B & 0 \\ -A & I_n\\ \end{pmatrix} = \begin{pmatrix}0 & B \\ CB-DA & D\\ \end{pmatrix}.$$Then: $$\operatorname{det}\begin{pmatrix}A & B \\ C & D\\ \end{pmatrix}\operatorname{det}\begin{pmatrix}B & 0 \\ -A & I_n\\ \end{pmatrix} = \operatorname{det}\begin{pmatrix}0 & B \\ CB-DA & D\\ \end{pmatrix},$$ but 
$$\operatorname{det}\begin{pmatrix}B & 0 \\ -A & I_n\\ \end{pmatrix} = \operatorname{det}(B)$$
$$\operatorname{det}\begin{pmatrix}0 & B \\ CB-DA & D\\ \end{pmatrix} = (-1)^n\operatorname{det}(CB-DA)\operatorname{det}(B).$$ Then:
$$\operatorname{det}\begin{pmatrix}A & B \\ C & D\\ \end{pmatrix}\operatorname{det}(B) = (-1)^n\operatorname{det}(CB-DA)\operatorname{det}(B)$$ which can be rewritten as follows: $$[\operatorname{det}\begin{pmatrix}A & B \\ C & D\\ \end{pmatrix}+(-1)^{n+1}\operatorname{det}(CB-DA)]\operatorname{det}(B) = 0$$ But $\operatorname{det}(B) \neq 0$, hence:$$\operatorname{det}\begin{pmatrix}A & B \\ C & D\\ \end{pmatrix}+(-1)^{n+1}\operatorname{det}(CB-DA) = 0,$$ So $$\operatorname{det}\begin{pmatrix}A & B \\ C & D\\ \end{pmatrix} = (-1)^n\operatorname{det}(CB-DA).$$ Im stuck on this part, can anyone help me on this step","['matrices', 'linear-algebra', 'abstract-algebra', 'determinant']"
1184947,"If $f:D\to D-\lbrace 0 \rbrace$ is Holomorphic with $f(0)=1/2,$ then $\vert f(1/2)\vert\ge 1/8$","Let $D$ denote the open unit disc centered at $0\in \mathbb{C}$ and suppose $f:D\to D-\lbrace 0 \rbrace$ is analytic and $f(0)=1/2$. Show $\vert f(1/2)\vert \ge 1/8$. The only techniques that come to mind when dealing with inequalities of $\vert f \vert$, when  is defined on $D$, are Schwarz's Lemma and Schwarz-Pick Lemma. By the Schwarz-­Pick Lemma $$\Bigg \vert \frac{f(1/2)-f(0)}{1-\overline{f(1/2)}\cdot f(0)}\Bigg\vert\le 1/2$$
so $\vert f(1/2)-1/2\vert \le \frac{\vert 1-f(1/2)/2\vert}{2}=1/2-f(1/2)/4$. We don't know whether the leftmost side is (a) $\vert f(1/2)\vert-1/2$ or (b) $1/2-\vert f(1/2)\vert.$ In case (a) we have $-1/4\le 5\vert f(1/2)\vert/4$, which doesn't necessarily imply the desired inequality. In case (b) we have $0\le 3\vert f(1/2)\vert/4$ which also doesn't imply the desired inequality. So neither case is of any use. Another idea is to consider the function $\frac{1}{f(z)}-2$ since it maps $0$ to $0$ (in an attempt to apply Schwarz) but I can't show that this function maps into $D$. How can I proceed?",['complex-analysis']
1184961,Proof using strong induction [duplicate],"This question already has answers here : Prove by mathematical induction that: $\forall n \in \mathbb{N}: 3^{n} > n^{3}$ (4 answers) Closed 9 years ago . I need to prove/show that $n^3 \leq 3^n$ for all natural numbers $n$ by strong induction. I have no clue where to begin!!!! :( I know how to do the beginning steps of showing that it's true for $k = 0$ and $k = 1$, etc but get suck on how to start the strong inductive step.","['induction', 'discrete-mathematics', 'proof-writing']"
1184963,Independence intuition,"Toss two fair dice. There are $36$ outcomes in the sample space $\Omega$, each with probability $\frac{1}{36}$. Let: $A$ be the event '$4$ on first die'. $B$ be the event 'sum of numbers is $7$'. $C$ be the event 'sum of numbers is $8$'. It says here $A$ and $B$ are independent. I don't understand why this is the case. What is the intuition behind this? Can someone offer an explanation to me that doesn't involve using the definition of $\mathbb{P}(A \cap B) = \mathbb{P}(A)\mathbb{P}(B)$? My understanding is informally, an event is independent if the occurrence of one does not affect the probability of the other and vise versa. So if $A$ occurs, wouldn't that affect the probability of $B$? Since if I were to roll a $4$ on the first die, the sample space will be reduced and hence the probability of 'sum of numbers is $7$' will be affected? It also says $A$ and $C$ are not independent and $B$ and $C$ are not independent. Why? I think this is because I'm confusing independence with conditional probability?","['independence', 'dice', 'probability']"
1185010,Statistics Binomial Formula,"Jack tosses a fair coin 6 times. Then Jill tosses the same coin 9 times.
Write out an expression for the chance that ""Jack gets 2 heads and Jill gets 4 heads."" I know that you use the binomial formula to get the chances for each but do you then multiply them together since it's asking for the chance of both or do you just add them?",['statistics']
1185014,"Is there a way to prove that the order of an element in a group divides the order of the Group, WITHOUT USING LAGRANGE'S","This is a very easy fact we use in Group Theory, But somehow, I wondered that whether there may be another way (other than Lagrange's Theorem) to prove that the order of an element divides the order of Group. I attempted to go on the term ""exponent"" of the Group (just assume that G is a Finite Group), which we may define the least common multiple of orders of elements in G. But this exponent divides $|G|$, since the order of each element divides $|G|$. So it became a little paradox. Shall I think that; before teaching this fact in a Course, one should teach about Lagrange's first?","['education', 'group-theory']"
1185032,Evaluating $\frac{1}{2\pi j}\int_{c-j\infty}^{c+j\infty}x^{-s}\sigma ^{ms-m} [ \frac{\Gamma ( s )}{\Gamma ( s+2)}]^{m}ds$,"I have been trying to solve the problem for $m=3$:
$$f(x)=\frac{1}{2\pi j}\int_{c-j\infty}^{c+j\infty}x^{-s}\sigma ^{ms-m}\left [ \frac{\Gamma \left ( s \right )}{\Gamma \left ( s+2 \right )} \right ]^{m}ds$$ Starting off, I have then simplified the problem to:
$$f(x)=\frac{1}{2\sigma ^{m}\pi j}\int_{c-j\infty}^{c+j\infty}\left ( \frac{\sigma ^{m}}{x} \right )^{s}\frac{1}{s^{m}\left ( s+1 \right )^{m}}ds$$ For $m=3$ and finding the residues:
$$\text{Res}\left ( \Gamma(s),0\right )=\frac{1}{2!}\lim_{s\rightarrow0}\frac{d^{2}}{ds^{2}}\left ( \frac{\sigma ^{3}}{x} \right )^{s}\frac{1}{\left (s+1  \right )^{3}}$$ $$\text{Res}\left ( \Gamma(s),-1\right )=\frac{1}{2!}\lim_{s\rightarrow-1}\frac{d^{2}}{ds^{2}}\left ( \frac{\sigma ^{3}}{x} \right )^{s}\frac{1}{s^{3}}$$ Using Cauchy's residue theorem, the simplified answer I obtain is:
$$f(x)=\frac{1}{2\sigma ^{3}}\left [ 12\left \{ 1-\frac{x}{\sigma^{3}} \right \}-6log\left ( \frac{\sigma^{3}}{x} \right )\left \{ 1+\frac{x}{\sigma^{3}} \right \}+log\left ( \frac{\sigma^{3}}{x} \right )^{2}\left \{ 1-\frac{x}{\sigma^{3}} \right \} \right ]$$ To check the answer I obtained, I plot it out in MATLAB against the emperical solution and this is what I get (negative side of $x$ is mirror of positive side of $x$): Where am I doing it wrongly? Is my final answer correct? Thanks Answer: There was nothing wrong with the analytical equation. And the problem is for $c>0$ and $x<\sigma$. Otherwise, $f(x)=0$. With that specified, the analytical expression obtained is correct! *Thanks to @RonGordon","['residue-calculus', 'algebra-precalculus', 'integration', 'cauchy-principal-value', 'gamma-function']"
1185053,Radius of convergence $\sum\limits_{n\ge0}q^{n^2}z^n$,"Radius of convergence of the power series $\sum\limits_{n\ge0}q^{n^2}z^n$ By a theorem the radius is $\limsup \lvert q^{n^2}\rvert^{1/n}=\limsup\lvert q^n\rvert=\begin{cases}0,&\text{if}\ |q|<1\\1,&\text{if}\ |q|=1\\ \infty,&\text{if}\ |q|>1\end{cases}$ So either the series converges everywhere, for $|z|<1$, or nowhere, but for the middle case, is $|z|=1$ included or excluded ?","['power-series', 'complex-analysis']"
1185057,coordinate ring of quasi projective varieties are regular?,"Let $X$ be a smooth quasi-projective variety over a field $k$ (you may assume $k$ is alg. closed if necessary). Then, is it true that the coordinate ring of $X$, namely $H^0(X,O_X)$ (the global sections of structure sheaf of $X$), is always a regular ring? Here by a regular ring R I mean that the the local rings of R at prime ideals are all regular local rings. Of course, if X is smooth affine variety then it is true, and if X is projective variety then $H^0(X,O_X)=k$, so the above holds obviously. My question is whether the same thing holds for open subvarieties of smooth projective varieties.",['algebraic-geometry']
1185094,An idenity related to Gamma function,"I know that for gamma function we have $$\int_0^{\infty}v^{k}e^{-av}dv=a^{-k-1}\Gamma(1+k),$$ given that $\Re(k)>-1$ and $\Re(a)>0$. Question : Now considering $$\frac{\int_{x_1}^{x_2}v^{k}e^{-iav}dv}{y}=a^{-k-1}$$ can we choose $x_1,x_2$ and $y$ such that this identity holds, while $x_1,x_2$ and $y$ are free of $a$? I appreciate your help and hints.","['integration', 'complex-analysis']"
1185108,Is the empty set a member of any collection of sets?,"empty set is an subset of any sets maybe any collection of sets. I wonder what about the case of the empty set being a member,not subset, of any collection (family) of sets.",['elementary-set-theory']
1185117,"Phase curve of $\ddot{x}=-x,\ddot{y}=-y$","Rewriting the ODE in the title, we get
$$\dot{x_1}=x_2,\dot{x_2}=-x_1, \dot{x_3}=x_4, \dot{x_4}=-x_3.$$
It is easy to show that
$$x_1=A\cos(t)+B\sin(t),x_2=B\cos(t)-A\sin(t),\\x_3=C\cos(t)+D\sin(t),x_4=D\cos(t)-C\sin(t).$$
In Ordinary Differential Equations by V. I. Arnold , there are several problems on this equation. For example, it can be shown that each phase curve is on a 3-sphere, and is a great circle of that. The next problem is more difficult: show that the phase curves on a given 3-sphere form a 2-sphere . I attempt to find out which 3-dimensional subspace the 2-sphere lies in, but in vain. And the last question is about the linking number . Since 3-sphere can be regarded as $\mathbb R^3\cup \{\infty\}$, a partition of 3-sphere into circles determines a partition of $\mathbb R^3$ into circles and nonclosed circles. Show that any two of the circles of this partition are linked with linking number 1 . I am not sure how to visualize this partition, and what the circles stand for. Can anyone give some advice? Thanks in advance.",['ordinary-differential-equations']
1185123,How to solve this differential equation please?,I'm trying to solve: $$\frac{dz}{dx}+2xz=2x$$ I have got the integrating factor as $$e^{\int 2x dx}=e^{x^2}$$ and so $$ze^{x^2}=\int {2xe^{x^2}} dx+ C$$ But I don't know how to proceed it's mainly an issue with calculating $$\int {2xe^{x^2}} dx$$ Any help?,"['ordinary-differential-equations', 'integration', 'derivatives']"
1185124,"Evaluating $\int_0^{\pi/2} \frac{a}{a^2+\cos^2 \theta} \, d\theta$","I want to evaluate $$
\int_0^{\pi/2} \frac{a}{a^2+\cos^2 \theta} \, d\theta
$$ and here is what Wolfram alpha gave me: $$
\int_0^{\pi/2} \frac{a}{a^2+\cos^2 \theta} \, d\theta=\frac{\tan^{-1} \left(\frac{a \tan x}{\sqrt{a^2+1}}\right)}{\sqrt{1+a^2}}
$$ Seeing the answer, I substituted $\cos \theta = \tan x$ hoping something good would happen, but in the end it didn't lead anywhere, assuming I didn't make any mistake. But after that I'm knid of hopelessly lost. What's the magic trick here?",['integration']
1185169,Tricky 3d geometry problem,"We have a cube with edge length $L$, now rotate it around its major diagonal (a complete turn, that is to say, the angle is 360 degrees), which object are we gonna get? Astoundingly the answer is D. And here is a demonstration: Well now I'm required to calculate the volume of this monster. It's far beyond my capability. I don't know how to analytically describe the curve between the two cones (although I believe it is a hyperbola).  And I'm still not sure why it should be a curve rather than a segment or something.  Could you help me? Thanks in advance.","['analytic-geometry', 'geometry', '3d', 'recreational-mathematics']"
1185178,How to formulate that an equation be shown to have no solutions?,Is there any general way to formulate the statement that an equation has no solution? For example: Prove that this equation has no solution: $$x^{1/\log x}=5$$ N.B. Do not answer with a proof of the example.,"['algebra-precalculus', 'terminology']"
1185191,Sum of series with binary parity in the numerator,"I'm now stuck with this question, and I don't even know where to start:
Find sum of series$$\sum_1^\infty \frac{f(n)}{n(n+1)}$$, where f(n) - number of ones in binary representation of n. I wish I could post some moves, that I've tried but I don't know what to do. Thanks!","['parity', 'fractions', 'sequences-and-series', 'binary']"
1185213,Can we treat finite and infinite primes in scheme theory?,"In Number Theory, I know we have analogy to geometry by considering infinite primes not only Spec $\mathcal{O}_K$ (and I heard the point of view bring the Riemann-Roch theory). My question is can we treat this point of view in the scheme theory? That is, can we define a proper scheme structure on Spec $\mathcal{O}_K \cup \{$infinite primes$\}$?",['number-theory']
1185222,Limit of a function: $\lim_{x\to \infty} \frac x{\sqrt{1+x^2}}$,I'm having trouble finding $\lim_{x\to \infty} {x\over \sqrt{1+x^2}}$ by using the formal analysis proof i.e. $\forall \epsilon>0$ $\exists N: \left|f(x)-L\right|<\epsilon$ $\forall x>N$. I know the answer is 1 but I can't manage to prove it sufficiently. The main problem is rearranging $\left|f(x)-1\right|$ to get a nice equation of $x$ in terms of $\epsilon$. Some help would be greatly appreciated. So far I have got to $\left|\frac{x}{\sqrt{1+x^2}}-1\right|<\epsilon$ and now I need $x$ to be in terms of $\epsilon$ in order to find a suitable $N$ for $x$ to be greater than.,"['functions', 'epsilon-delta', 'real-analysis', 'limits']"
1185250,"Find $E\subseteq\mathbb{R}$ such that $\liminf_{\delta\to 0}\frac{m(E\cap(-\delta,\delta))}{2\delta}=\alpha$","Problem: Let $\alpha$ and $\beta$ be such that $0\leq\alpha\leq\beta\leq 1$. Find a measurable set $E\subseteq\mathbb{R}$ such that
  $$\liminf_{\delta\to 0}\frac{m(E\cap(-\delta,\delta))}{2\delta}=\alpha\quad\text{and}\quad\limsup_{\delta\to 0}\frac{m(E\cap(-\delta,\delta))}{2\delta}=\beta,$$
  where $m$ is the Lebesgue measure. (Taken from Rudin's Real and Complex Analysis , Chapter 7, Exercise 2.) I tried many things, but every attempt fails. All I can get is a set $E$ corresponding to $\alpha=0$ and $\beta=1$. But when $0<\alpha\leq\beta<1$, I don't know how to get $E$.","['measure-theory', 'lebesgue-measure', 'real-analysis']"
1185259,How to explain the formula for the sum of a geometric series without calculus?,"How to explain to a middle-school student  the notion of a geometric series without any calculus (i.e. limits)? For example I want to convince my student that
$$1 + \frac{1}{4} + \frac{1}{4^2} + \ldots + \frac{1}{4^n} = \frac{1 - (\frac{1}{4})^{n+1} }{ 1 - \frac{1}{4}}$$
at $n \to \infty$ gives 4/3?","['summation', 'sequences-and-series', 'education', 'learning']"
1185295,"Determining existence of limit with multiple variables: $\lim_{(x,y)\to (0,0)} \frac{xy^2}{x^3+y^3}$","Given the following limit: $$ \lim_{(x,y)\to (0,0)} \frac{xy^2}{x^3+y^3} $$ And the instrucion to ""Determine whether the limit exists, give a complete argument"", would the following be a ""complete argument""? Approaching the limit from the line y=0, gives $ \lim_{(x,y)\to (0,0)} \frac{xy^2}{x^3+y^3} = \lim_{(x,y)\to (0,0)} \frac{0}{x^3+y^3} = 0 $ Approaching the limit from the line y=x, gives $ \lim_{(x,y)\to (0,0)} \frac{xy^2}{x^3+y^3} = \lim_{(x,y)\to (0,0)} \frac{x^3}{2x^3} = \frac{1}{2} $ These limits do not agree, thus the original limit $$ \lim_{(x,y)\to (0,0)} \frac{xy^2}{x^3+y^3} $$ does not exist. Or should another method besides approaching from different lines be used to give a ""complete argument"" whether this limit exists be given?","['multivariable-calculus', 'limits']"
1185344,Show the Euclidean metric and maximum metric are strongly equivalent.,"I need to show that the Euclidean metric and maximum metric (or square metric??) are strongly equivalent. I have no idea how to start this proof. Any help? $d_1, d_2$ are called strongly equivalent if there exist positive constants $K, M$ such that for all $x, y\in X$:
$Md_1(x,y)\leq d_2(x,y)\leq Kd_1(x,y)$","['general-topology', 'metric-spaces', 'real-analysis']"
1185348,"Prove that for every $A \in P (U)$ there is a unique $B \in P(U)$ such that for every $C \in P (U)$, $C \setminus A = C \cap B$.","I know that there exists such a set $(U\setminus A)$ for which $C \setminus A = C \cap B$ . However I have trouble proving that it is unique. What I am trying to do is prove that $\forall D\in P(U)(C\setminus A = C\cap D \Rightarrow D=U\setminus A)$ . I first assume $C\setminus A = C\cap D$ and try to prove $D=U\setminus A$ , but this leads to nowhere. Any suggestions on how to approach this problem ?","['logic', 'elementary-set-theory']"
1185388,Prove the operator is positive,"I'm searching for an alternative proof of the following: Let $U$ be a self-adjoint operator on a Hilbert space $H$, define $m=\inf_{\|x\|=1}\langle Ux,x\rangle$ and $M=\sup_{\|x\|=1}\langle Ux,x\rangle$. If $P$ is a
  polynomial such that $P(x)\geq 0$ for $x\in[m,M]$, prove that $P(U)$
  is a positive operator. The usual proof is by using that $\sigma(P(U))=P(\sigma(U))$ ( Spectral mapping theorem for polynomials ). The intention is to prove this result without this theorem. Edit: We say that $U$ is a positive operator if $\langle Ux, x\rangle\geq 0$ for every $x\in H$.","['operator-theory', 'alternative-proof', 'functional-analysis']"
1185389,Real Applications of Markov's Inequality,"When is Markov's Inequality useful?  It seems to me that it's a very rough upper bound.  For example, if we roll a die and want to know the probability of the result being a 5 or greater we have that there is at most a 3.5/5 chance.  That's huge relative to the actual chance.  Am I misunderstanding?","['inequality', 'probability']"
