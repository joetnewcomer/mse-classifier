question_id,title,body,tags
2342910,proving existence of a series $(x_n)$,"I'm having trouble with this one, I don't know what is the right path to solve it. $f$ is a differentiable function in $(a,b)$, and let $x_0 \in (a,b)$.
prove that there exists a sequence $(x_n)$ so that: $x_n \neq x_0$ for every $n$, $\lim_{n->\infty} x_n=x_0$ and $\lim_{n->\infty} f'(x_n) = f'(x_0)$ A hint I have: for every natural $n$, regard $[x_0,x_0+1/n]$ and use Lagrange's mean value theorem. I tried to use the property and definition of a derivative to prove that $\lim_{n->\infty} f'(x_n) = f'(x_0)$, but couldn't find a way to conclude that $\lim_{n->\infty} x_n=x_0$. I could assume that, but I don't know how to conclude that or how to prove it. Hoping you could help me. Thank you very much!","['derivatives', 'calculus', 'limits-without-lhopital']"
2342935,Find all primes for which $p^p - 2$ is a perfect square,"Find all primes for which $p^p - 2$ is a perfect square.
Let $p^p-2=a^2.$ We know $a^2 \equiv 0,1\pmod{3}$ If $a^2 \equiv 1\pmod{3}$, then $p^p \equiv 0\pmod{3}\implies p=3$ which is a solution. Now, if $a^2 \equiv 0 \pmod{3},$ then $p^p \equiv 2\pmod{3}$. By Fermat's theorem, $p^p \equiv p \pmod{3}$ as $p$ is coprime to $3$. So, we have $p$ is of the form $3k+2.$ But what to do next?",['number-theory']
2342949,Sum of exponent of jump times,"I have the following question. I have a random variabele that is the sum of the exponent of the jumping times $T_i$ of a Poisson process $N(\cdot)$ with parameter $\lambda$. Say: $$B(t) = \sum_{i=1}^{N(t)} e^{-r (t - \ T_i)} $$ I want to know $\mathbb{E}[B(t)]$. I can condition on $N(t)$ to see that $\mathbb{E}[B(t)] = \sum_{i=1}^\infty\mathbb{E}[B(t)|N(t)=k]\mathbb{P}[N(t)=k]$. Where $\mathbb{P}[N(t)=k] = e^{-\lambda t} \frac{(\lambda t)^k}{k!}$ is a Poisson distribution of parameter $\lambda t$. To understand the problem I first calculated $\mathbb{E}[B(t)|N(t)=1]$, where since we condition on having one jump the jump is uniformly distributed: \begin{align}
\mathbb{E}[B(t)|N(t)=1]&= \int_0^t \mathbb{P}(T_1=s)e^{-r(t-s)}ds \\
&= \int_0^t \frac{1}{t} e^{-rt}e^{rs}ds \\
&= \frac{e^{-rt}}{t} \int_0^t e^{rs} ds \\
&= \frac{1}{e^{rt}} \left[ \frac{e^{rs}}{r} \right]_0^t \\
&= \frac{1}{te^{rt}} \left( \frac{e^{rt}}{r} - 1 \right) = \left( \frac{1}{tr} - \frac{e^{-rt}}{t} \right)
\end{align} Which looks like it makes some sense. I know that the joint distribution of the jump times. Have a joint density distribution of $f(t_1,...,t_n)= \frac{n!}{t^n} 1_{(0 \leq t_1 \leq ... \leq t_n \leq t)}$ so the expected value generalised to some n would look like: \begin{align}
\mathbb{E}[B(t)|N(t)=n] &= \int_0^t ... \int_0^t f(t_1,...,t_n) \sum_{i=1}^n e^{-r(t-t_i)} dt_1 ... dt_n \\
&= \int_0^{t_2} ... \int_{t_{n-2}}^t \frac{n!}{t^n} \sum_{i=1}^n e^{-r(t-t_i)} dt_1 ... dt_n
\end{align} This I am not so sure how to evaluate. Anyone has an idea?","['stochastic-processes', 'markov-chains', 'markov-process', 'poisson-process', 'probability']"
2343012,All terms of this sequence are equal to 1: $x_{n+1}= \frac{nx_{n}^2+1}{n+1}$,"Does anybody know if the following result is true? Let $x_{n}$ be a sequence, such that  $x_{n+1}= \dfrac{nx_{n}^2+1}{n+1}$ and $x_n>0$ for all $n$. There is a positive integer $N$ such that $x_n$ is integer for all $n>N$. Does it follow that $x_n=1$ for all positive integers $n$? I tried to prove that $x_1 \equiv 1 \text{(mod p)}$ for all prime numbers $p$ but  I couldn't make any further progress. I'm looking for a proof or any reference of this result. Any help would be appreciated.","['reference-request', 'recurrence-relations', 'sequences-and-series', 'elementary-number-theory']"
2343016,"Working with normal distributions, how large can noise be before data becomes inaccurate?","I'm measuring a characteristic of a device that has a normal distribution ($0$ mean and std dev of $\sigma_M$). There is, however, noise in the measurement process, which also has a normal distribution ($0$ mean and std dev of $\sigma_N$). I can measure this noise independently. I can estimate the device's true characteristic (without noise) as $\sigma_D = \sqrt{\sigma_M^2 - \sigma_N^2}$. To be compliant with a specific spec, $\sigma_D$ must be less than $L$. If the noise is small compared to the measured value, I have high confidence in my data. But my confidence drops as the noise approaches the measured value. In the extreme case, if $\sigma_M = \sigma_N$, my estimation returns $\sigma_D = 0$, indicating that I've reached the noise floor of my equipment (I think that's the correct interpretation, but let me know if not). My question is, how close can $\sigma_N$ be to $\sigma_M$ such that $\sigma_D$ is still ""accurate""? I don't want to report a value of $\sigma_D$ that contains too much error. Rather, I'd like to report some lower bound for $\sigma_D$ once $\sigma_N$ becomes too close to $\sigma_M$. Any light you can shed to help me define that lower bound would be much appreciated. UPDATE 1 To clarify, I measure $(1)$ the device with noise and, separately at a later time, $(2)$ the noise (without the device). The distribution of noise measured directly (without the device) can be assumed to also exist when the device is measured with noise, and that it is the ONLY noise present when the device is measured with noise. UPDATE 2 Is the following statistically meaningful as a condition where $\sigma_D$ is inaccurate:
$$
\sigma_M - \sigma_N < \dfrac{1.96}{\sqrt{n}}(\sigma_M + \sigma_N) \;,
$$
where $n$ is the number of samples used to compute the sigmas?","['probability-theory', 'probability', 'statistics', 'normal-distribution']"
2343031,Domain of $\frac {\cos x}{\sin x}$ vs $\frac {1}{\tan x}$?,"I was helping a precalculus student  graph a function involving $\cot x$, and I realized that these two different definitions (which I thought equivalent) are slightly different. When finding the domain of  $\dfrac {\cos x}{\sin x}$, we only exclude the zeroes of $\sin x$. When finding the domain of $\dfrac {1}{\tan x} = \dfrac {1}{\frac {\sin x}{\cos x}}$, we must concern ourselves with the zeroes of both $\sin x$ and $\cos x$. My friend said the definitions are equivalent, because $\dfrac {\cos x}{\sin x} = \dfrac {1}{\frac {\sin x}{\cos x}}$ even when $\cos x = 0$, but I don't believe that's true. Which one of us is right?","['algebra-precalculus', 'trigonometry', 'functions']"
2343032,Is it possible to not know if a random variable is continuous or not?,"Suppose I have a sample of a random variable, $X$ but I do not know if the distribution is continuous or discrete. First of all is this possible? Second if it is possible how could I determine if $X$ is continuous from a set of iid random variables $X_1, ... X_n$?","['statistics', 'random-variables']"
2343034,Solve using complex numbers,"$\cos(A-B) + \cos(B-C) + \cos(C-A) = \frac{-3}{2}$ 
We need to prove that $\cos A + \cos B + \cos C = \sin A + \sin B + \sin C = 0$ I was wondering if it's possible to prove this result by showing that the real and imaginary parts of $z = \cos A + \cos B + \cos C$ are equal to zero, somehow invoking Vieta's or De Moivre's theorem if required. I tried, starting with $\cos(B-C)$ and other cyclic terms but couldn't really get anywhere. Any other method is also appreciated. Thanks a lot!","['algebra-precalculus', 'trigonometry']"
2343042,We know the number of digits of a number. Which is it?,"Assume we know the number of digits of a number $n \in \mathbb N$ has in some different bases: $$b_1,b_2,\cdots,b_n$$ What is the best approximation or the best ""narrowing down"" we can make of which number $n$ is?","['integers', 'discrete-mathematics']"
2343063,Show (via Complex Numbers): $\frac{\cos\alpha\cos\beta}{\cos^2\theta}+\frac{\sin\alpha\sin\beta}{\sin^2\theta}+1=0$ under given conditions,"$\alpha$ and $\beta$ do not differ by an even multiple of $\pi$. If $\theta$ satisfies $$\frac{\cos\alpha}{\cos\theta}+ \frac{\sin\alpha}{\sin\theta}=\frac{\cos\beta}{\cos\theta}+\frac{\sin\beta}{\sin\theta}=1$$ then show that $$\frac{\cos\alpha\cos\beta}{\cos^2\theta}+\frac{\sin\alpha\sin\beta}{\sin^2\theta}+1=0$$
  I wish to solve this problem using some elegant method, preferably complex numbers. I've tried using the fact that $\alpha$ and $\beta$ satisfy an equation of the form $\cos x/\cos\theta + \sin x/\sin\theta = 1$, and got the required result. See my solution here: https://www.pdf-archive.com/2017/07/01/solution I'm guessing there's an easier way to go about it. Thanks in advance!","['algebra-precalculus', 'trigonometry', 'complex-numbers']"
2343065,"Value at $0$ of solution to ODE, given asymptotic behaviour","Consider the differential equation
$$\forall x \in \mathbb R\qquad f'(x) =  f(x)^2 -x^2. $$ Every solution that crosses the line $y=x$ where $x>0$ is asymptotic to $y=-x$ as $x$ approaches $+\infty$. The other solutions with $f(0)>0$ are above the former and their limit at $+\infty$ is $+\infty$. I then expect there to be a solution $g$ which is asymptotic to $y=x$ as $x$ approaches $+\infty$. What I'd like to know is $g(0)$ and how to get the result. Well, an explicit expression for $g$ would be better, but I doubt there is one. The only thing I was I able to do so far was making GeoGebra plot some (approximated) solutions, as you can see from the picture.","['real-analysis', 'ordinary-differential-equations']"
2343119,Equation in a fraction - have I solved it right?,I'm solving entrance maths example problems from a university. I solved this one: $\frac {2x-3}5 - \frac {4x+5}3=8$ My solution was -104/14 but in the answer sheet it's -11. Am I wrong? If so why? Edit: I did the following steps: $3(2x-3)-5(4x+5)=8$ $6x-9-20x+25=120$ $-14x=104$ $x= -104/14$,"['algebra-precalculus', 'fractions']"
2343146,"Calculate the directional derivative of $f(x,y)= \frac{x^2-y^2}{x^2+y^2} $ in the direction $\vec{v}=(\cos\phi,\sin\phi)$","$f(x,y)= \frac{x^2-y^2}{x^2+y^2} $ when $(x,y)\neq0$  and $f(x,y)=0$  when $(x,y)=0$ The question:
Calculate the directional derivative of $f$ in the direction $\vec{v}=(\cos\phi,\sin\phi)$ at point $(x,y)$ . What I did: $$\lim_{h\to0}\frac{f((x,y)+h(\cos\phi,\sin\phi))-f(x,y)}{h}$$
$$= \lim_{h\to0}\frac{f(x+h\cos\phi,y+\sin\phi)-f(x,y)}{h}$$
$$=\lim_{h\to0}\frac{1}{h}\left({\frac{(x+h\cos\phi)^2-(y+h\sin\phi)^2}{(x+h\cos\phi)^2+(y+h\sin\phi)^2}-\frac{x^2-y^2}{x^2+y^2}}\right)$$ This seems right, but it seems that if I start multiplying everything, it becomes hell, and I can't get it right when there are three lines of equations. Am I missing something? Is there any better way to do this?","['multivariable-calculus', 'limits']"
2343155,Characteristic polynomial of a $7 \times 7$ matrix whose entries are $5$ [duplicate],"This question already has answers here : Determinant of a rank $1$ update of a scalar matrix, or characteristic polynomial of a rank $1$ matrix (2 answers) Closed 6 years ago . Avoiding too many steps, what is the characteristic polynomial of the following $7 \times 7$ matrix? And why? \begin{pmatrix}5&5&5&5&5&5&5\\5&5&5&5&5&5&5\\5&5&5&5&5&5&5\\5&5&5&5&5&5&5\\5&5&5&5&5&5&5\\5&5&5&5&5&5&5\\5&5&5&5&5&5&5\end{pmatrix}","['matrices', 'characteristic-polynomial', 'linear-algebra']"
2343236,Solving $y''-\frac{1}{x \ln x}y'=12x^2\ln x$,"Could someone help me solve this differential equation? $$y''-\frac{1}{x\ln x}y'=12x^2\ln x$$
I tried doing $y'=z$ and that leads me to $$z'-\frac{1}{x\ln x}z=12x^2\ln x$$ and then I do $z=t\cdot x\:\:and\:z'=t'x+t$ which will lead me to $$t'x+t=12x^2\ln x+\frac{1}{x\ln x}\cdot t\cdot x$$ but trying to solve this(by parts) yields me $$t=x^{6x^2}-e^{3x^2}+\ln x+\frac{1}{x}$$ and to find $y$ I will have to integrate a pretty nasty equation, which leads me to believe I'm doing something wrong. I'm pretty sure I need to use substitution to get a first order linear equation but perhaps I am doing something wrong? Could someone take a look at this and point me in the right direction?",['ordinary-differential-equations']
2343259,Proving this trigonometric result [duplicate],"This question already has answers here : If $\frac{\sin^4 x}{a}+\frac{\cos^4 x}{b}=\frac{1}{a+b}$, then show that $\frac{\sin^6 x}{a^2}+\frac{\cos^6 x}{b^2}=\frac{1}{(a+b)^2}$ (5 answers) Closed 6 years ago . Problem : If $\frac{\sin^4\theta}{x}+\frac{\cos^4\theta}{y}=\frac{1}{x+y}$, show that, $\frac{\sin^{2m+2}\theta}{x^m}+\frac{\cos^{2m+2}\theta}{y^m}=\frac{1}{(x+y)^m}$. My attempt : I observed that if both the sides of the given equation are raised to the power of m, the R.H.S of the resulting equation matches with the R.H.S of the result that has to be proved. After that we need to simplify the L.H.S of the resulting expression to match it with the L.H.S of the result that has to be proved. $\frac{\sin^4\theta}{x}+\frac{\cos^4\theta}{y}=\frac{1}{x+y}$ $\implies(\frac{\sin^4\theta}{x}+\frac{\cos^4\theta}{y})^m=(\frac{1}{x+y})^m$ $\implies(\frac{y\sin^4\theta+x\cos^4\theta}{xy})^m=\frac{1}{(x+y)^m}$ $\implies\frac{(y\sin^4\theta+x\cos^4\theta)^m}{x^my^m}=\frac{1}{(x+y)^m}$ My problem : I am stuck at this step and cannot understand how to proceed, that is, how to simplify $(y\sin^4\theta+x\cos^4\theta)^m$. Please help. A continuation of my method as well as alternate methods (except mathematical induction) are welcome.","['algebra-precalculus', 'proof-writing', 'trigonometry', 'fractions']"
2343261,Sequentially continuous implies continuous,"In topological spaces, which condition is necessary for a sequentially continous function $f: (X,\tau_x) \rightarrow (Y,\tau_y)$ to be continous? I have tried to prove this making the space X be $T_1$ and then making it Hausdorff but I don't get the answer. For example making $\tau_x$ the topology of the complements of countable sets is $T_1$ and $f(x) = x$ is sequentially continuous but not continuous taking $X=Y= \mathbb{R}$ and $\tau_y$ being the usual topology in $\mathbb{R}.$",['general-topology']
2343277,Calculating $ \int_{\partial B_2(0)} \frac{\mathrm{d}z}{(z-3)(z^{13}-1)} $,"$\newcommand{\ind}{\mathrm{ind}}
\newcommand{\out}{\mathrm{out}}
\newcommand{\ins}{\mathrm{ins}}
\newcommand{\res}{\mathrm{res}}
\newcommand{\ord}{\mathrm{ord}}$ I want to calculate $\int_{\partial B_2(0)} \frac{\mathrm{d}z}{(z-3)(z^{13}-1)}  $ using residue theorem. I want to keep it very simple.  The exercise is taken from Freitag, Complex Analysis. The solution is given by $-2\pi i (3^{13}-1)$. He solved it with a theorem we never had in lecture. My Idea: $I:=\int_{\partial B_2(0)} \frac{\mathrm{d}z}{(z-3)(z^{13}-1)}$
\begin{align*}
 I&= 2 \pi i \sum_{j=1}^{k} \res(f(z),z_j) \underbrace{\ind(2 \mathrm{e}^{2\pi i t}, z_j )}_{=1} \\
 &= 2 \pi i \bigg[ \res( f,3) + \res(f,1) + \res(f,-(-1)^{1/13}) + \res(f, (-1)^{2/13}) \\
 &+ \dots + \res(f, (-1)^{12/13})  \bigg]  \\
 &= 2 \pi i \bigg[ \frac{1}{3^{13} -1} - \frac{1}{26} +  \res(f,-(-1)^{1/13}) + \res(f, (-1)^{2/13}) + \dots + \res(f, (-1)^{12/13}) \bigg]
\end{align*}
I used $\res(f/g,a) = f(a)/g'(a)$ with $f(z)=1$, $g(z)=(z-3)(z^{13}-1)$ and  $g'(z) = -3(z^{13}-1) + (z-3)13z^{2}$. 
Now I have to calculate all these residues, but is there some faster way ?","['complex-analysis', 'residue-calculus']"
2343287,Determine whether the series converges or diverges.,"Determine whether the series converges or diverges. $$
 \sum _{n=1}^{\infty }\:\left(\frac{19}{n!}\right)
$$ I know that this question a lot easier if I use ratio test but I  have not learned ratio test yet. The only option I have is divergence, comparison, limit comparison, and integral test. How can I prove that this series converges by using the limited tests. Thanks in advance.","['fractions', 'sequences-and-series', 'calculus', 'factorial', 'convergence-divergence']"
2343323,Is there a way to prove that absolute geometry must take place on a Riemannian manifold?,"Absolute geometry (as I know the term) is just (Euclidean geometry) $-$ (parallel postulate). (It is sometimes also called neutral geometry , because it is ""neutral"" w.r.t. the parallel postulate.) The only spaces I know of which satisfy its axioms are Euclidean spaces and hyperbolic spaces, both of which are obviously Riemannian manifolds. Question: Are these the only possible metric spaces which can satisfy the axioms of absolute geometry? If so, is there a way to prove this? Attempt: By Postulate 4 here , it follows that absolute geometry is a subset of metric geometry. In addition to the existence of a metric, it also requires the existence of lines and angles, in order for the remaining postulates to be defined. Both lines and angles appear to be a concept of ordered geometry , thus we have to restrict to metric spaces with some notion of intermediacy, i.e. ""betweenness"". One way to establish such a notion is by restricting to geodesic spaces, and then to say that the point $P$ lies between the points $A$ and $B$ if and only if it is on a geodesic connecting $A$ and $B$. (The uniqueness of geodesics seems unnecessary, for example elliptic geometry should also be an ordered geometry, I think, but elliptic geometry does not have unique geodesics.) But is it really necessary, rather than just sufficient, to restrict to geodesic spaces, in order to have ordered geometry? Ordered geometry doesn't even seem like it requires the existence of a metric, so why would it require the existence of geodesics? Moreover, I see no reason how or why to restrict further from geodesic spaces to Riemannian manifolds in order to satisfy the axioms of absolute geometry.","['metric-geometry', 'riemannian-geometry', 'geodesic', 'geometry']"
2343354,Prove that there exists a prime $p$ such that $p \mid a^n-b^n$ and $p > n$,"Let $1 < b < a$ be relatively prime positive integers and $n > 2$ a positive integer. Prove that there exists a prime $p$ such that $p \mid a^n-b^n$ and $p > n$. I thought about using Zsigmondy's Theorem. Then we have $$p_1 \mid a-b,p_2 \mid a^2-b^2,a^3-b^3,p_3 \mid a^4-b^4,\ldots$$ where $p_i \nmid a^k-b^k$ for $k < i$. I didn't see how to use this to find a prime such that $p > n$.",['number-theory']
2343365,Why does Green's Theorem require partial derivatives to be continuous,"My book (Stewart's Essential Calculus) states Green's Theorem as follows: Let $C$ be a positively oriented, piecewise-smooth, simple closed curve in the plane and let $D$ be the region bounded by $C$. If $P$ and $Q$ have continous partial derivatives on an open region that contains $D$, then $$\int_C  P \,dx+Q\,dy =\iint_D \left(\frac{\partial Q}{\partial x}-\frac{\partial P}{\partial y} \right)dA $$ My question is why are the partial derivatives required to be continous? I have seen a few examples of Greens Theorem not working when the partial derivatives do not exist but cannot seem to find one when the partial derivatives have a discontinuity. My first idea is that the requirement of continuity insures that no ""kinks"" in the vector field occur but then I found Darboux's theorem which seems to insure that discontinous derivatives will never have this issue. I expect a technical understanding of the details of Green's Theorem to be out of my reach currently (my book doesn't even include a full proof) but is there any intuitive justification of the continuity requirement on the derivatives?","['multivariable-calculus', 'integration', 'vector-analysis']"
2343401,"Find the number of permutations of $1,2,3...,n$ such that $a_{i+1}\neq a_i+1$ for $ i<n $ if $a_i$ is the i-th element in the permutation?","Find the number of permutations of $1,2,3...,n$  such that $a_{i+1}$ does not equal $a_i+1$ if $a_i$ is the $i$-th element in the permutation? I think a good strategy would be to approach this using PIE but I am still not sure about how I could solve it with that method.","['recurrence-relations', 'recursion', 'inclusion-exclusion', 'permutations', 'combinatorics']"
2343406,Is the extension of a locally compact group by a compact group a locally compact group?,"Let $1 \to N \overset{\iota}{\to} G \overset{\pi}{\to} G/N \to 1$ be a short exact sequence of topological groups. Here, $\iota$ is the inclusion of a closed, normal subgroup and $G/N$ has the quotient topology. Question 1: If $N$ and $G/N$ are locally compact (Hausdorff) groups, is $G$ also a locally compact group? In fact, I would be perfectly satisfied with an answer to: Question 2: If $N$ is a compact group and $G/N$ is  a locally compact group, is $G$ a locally compact group? If the quotient map admits a continuous local section $s :W \to G$ defined on a neighbourhood $W$  of $1 \in G/N$, then one easily checks that $G$ is locally homeomorphic to $N \times (G/N)$, so the answer to both questions is ""yes"".  However, as noted here , the existence of such a section is not guaranteed, even if all of $G,N,G/N$ are compact. One may take $G = \mathbb{T}^\mathbb{N}$, $N = \{\pm 1 \}^\mathbb{N}$. It's at least easy to check that $G$ is Hausdorff. Since $\{1\}$ is closed in $N$, and $N$ is closed in $G$, we know that $\{1\}$ is closed in $G$. Thus, the diagonal $\Delta = \{ (x,x) : x \in G\}$ is closed in $G$, because it is the primage of $\{1\} \subseteq G$ under the continuous map $(x,y) \mapsto xy^{-1} G \times G \to G$.","['locally-compact-groups', 'general-topology', 'topological-groups', 'compactness']"
2343408,Divergence of series sin(1/n),Since $-1 \le \sin(1/n)\le 1$ and $\lim_{n\to \infty} -1$ $\neq$ $\lim_{n\to \infty} 1$ can I use the nth-term test to prove that the series will diverge? I've only seen the problem done using the limit comparison test and am not sure if I can use the nth-term test.,"['divergent-series', 'sequences-and-series']"
2343469,"Sum of all of the numbers in row n of Pascal’s triangle? Explain why this happens, in terms of the way the triangle is formed...","Just to clarify there are two questions that need to be answered: 1)Explain why this happens, in terms of the way the triangle is formed. 2) Explain why this happens,in terms of the fact that the combination numbers count subsets of a set. I know the sum of the rows is equal to $2^{n}$. However I am stuck on the other questions. I also have to assume I don't know the binomial theorem just yet. I thought my explanation would be just add up the numbers in the rows, but I'm not sure if that is even what I need to do. Please help. Thank you!","['combinations', 'combinatorics', 'discrete-mathematics']"
2343475,What is the Newton polyhedron of a toric variety?,"First of all I want to say that my knowledge of toric geometry is minimal. A paper I'm reading considers a toric variety $X$ and then claims that in its Newton polyhedron, $\Delta(X)$, the fixed points of the torus action on $X$ correspond to the vertices of $\Delta(X)$ and the torus invariant lines (*) to the edges of $\Delta(X)$. Somehow I've come to think of $\Delta(X)$ as the fan of $X$, but this is a recent paper and I doubt they'd use some nonstandard (?) name for the fan of $X$. Hence the question. Needless to say my google searches have been fruitless. Regarding (*): I don't think that a priori $X$ has to contain any ""lines"" (either affine or projective), so I'm taking the phrase ""torus invariant line"" to mean the torus invariant curves joining to vertices and the term ""line"" coming from the fact that it corresponds to a line in $\Delta(X)$. Is this correct?","['reference-request', 'algebraic-geometry', 'toric-geometry']"
2343478,General solution for an improper node (ODE).,"This might be a really simple question. Define $A=\begin{bmatrix}a&1\\0&a\end{bmatrix}$, $a\neq 0$; the eigenvalue $a$ has only one real associated eigenvector $v$ and then $\phi_1(t)=e^{at}v$ is a solution of the ODE $$\frac{dx}{dt}=Ax,$$ but in order to find the general solution, I need one more linearly independent solution. Is there a ""satisfactory"" way to obtain it? To explain the word ""satisfactory"", I've read somewhere that if $w\in \mathbb{R^2}$ is linearly independent of $v$, then $\phi_2(t):=e^{at}(w+tv)$ is the solution I seek, but the author didn't give further explanation (probably because anyone should be able to figure out where this came from) and I got confused. Writing $Aw=v+\beta w$, the matrix of $x\rightarrow Ax$ in the base $\{v,w\}$ is $\begin{bmatrix}a&1\\0&\beta\end{bmatrix}$, but since the eigenvalues are invariant on representation, $\beta=a$ and $Aw=v+aw$, then $$\frac{d}{dt} \phi_2=ae^ {at}(w+tv)+e^{at}v=e^{at}(atv+aw+v)=A(w+tv)=A\phi_2$$ that is, $\phi_2(t)$ is indeed a solution and there's something very clever about it that makes me think it makes sense, but it is still not a solution I could get on my own. Can anyone explain the meaning of this choice? Thanks in advance!","['ordinary-differential-equations', 'proof-verification']"
2343495,"Rates, Distance and Time Problem 45","2 vehicles left the town $A$ and $C$ at the same time to ride to the other town, crossing each other at town $B$ and both traveling at different steady speeds. The vehicle from town $A$ finished the trip from town $B$ to town $C$ in 45 minutes at a constant speed of 64 kilometers per hour. The vehicle from town $C$ finished the trip from town $B$ to town $A$ in 20 minutes. I know from this info that the 2 vehicles meet at town $B$, so the time it takes to go from town $B$ to town $A$ for the vehicle from town $A$ is equal to the time it takes to go from town $C$ to town $B$ for the vehicle from town $C$. But I do not know how to use this information to solve the problem. Please help me with figuring out what the speed of the vehicle from town $C$ would be and how you would calculate this. Thanks :)",['discrete-mathematics']
2343526,Is it true that the marginal expectation of a probablity density function is still a probablity density?,"Let $p(x,y),x,y\in \Bbb R$ be a probability density function. Let $q(x),x \in \Bbb R$ be another density. Is $f(y)=\Bbb E_q[p(x,y)] = \int q(x)p(x,y)dx$ a density function? Is $f(y)=\exp\{\Bbb E_q [\log p(x,y)]\}$ a density function? I am not sure, but my hunch is that it is not necessarily true. Take 1.  for example, if it is probability density, then $\int f(y)dy=\int{\int q(x)p(x,y)dxdy}=\int q(x) (\int p(x,y)dy) dx=\int q(x) p_X(x) dx=1$ where $p_X$ is the marginal density of $p$. Since $p,q$ has no relation, I think the product of two arbitrary density functions might not be a density. I would appreciate it if someone can help with concrete counterexamples.","['statistics', 'probability']"
2343565,"If $f(x)=\frac{1}{x^2+x+1}$, how to find $f^{(36)} (0)$?","If $f(x)=\frac{1}{x^2+x+1}$, find $f^{(36)} (0)$. So far I have tried letting $a=x^2+x+1$ and then finding the first several derivatives to see if some terms would disappear because the third derivative of $a$ is $0$, but the derivatives keep getting longer and longer. Am I on the right track? Thanks!","['derivatives', 'real-analysis', 'complex-numbers', 'calculus', 'summation']"
2343592,Find the largest constant $k$ such that $\frac{kabc}{a+b+c}\leq(a+b)^2+(a+b+4c)^2$,"Find the largest constant $k$ such that $$\frac{kabc}{a+b+c}\leq(a+b)^2+(a+b+4c)^2$$ My attempt, By A.M-G.M, $$(a+b)^2+(a+b+4c)^2=(a+b)^2+(a+2c+b+2c)^2$$ $$\geq (2\sqrt{ab})^2+(2\sqrt{2ac}+2\sqrt{2bc})^2$$ $$=4ab+8ac+8bc+16c\sqrt{ab}$$ $$\frac{(a+b)^2+(a+b+4c)^2}{abc}\cdot (a+b+c)\geq\frac{4ab+8ac+8bc+16c\sqrt{ab}}{abc}\cdot (a+b+c)$$ $$=(\frac{4}{c}+\frac{8}{b}+\frac{8}{a}+\frac{16}{\sqrt{ab}})(a+b+c)$$ $$=8(\frac{1}{2c}+\frac{1}{b}+\frac{1}{a}+\frac{1}{\sqrt{ab}}+\frac{1}{\sqrt{ab}})(\frac{a}{2}+\frac{a}{2}+\frac{b}{2}+\frac{b}{2}+c)$$ $$\geq 8(5\sqrt[5]{\frac{1}{2a^2b^2c}})(5\sqrt[5]{\frac{a^2b^2c}{2^4}})=100$$ Hence the largest constant $k$ is $100$ Is my answer correct? Is there another way to solve it? Thanks in advance.","['inequality', 'a.m.-g.m.-inequality', 'optimization', 'substitution', 'algebra-precalculus']"
2343654,Error range for the Taylor polynomial Lorentz factor $γ$,"Consider the Lorentz factor (in special theory of relativity) as the function $$γ(x)=\frac{c}{\sqrt{c^2-x^2}},\;x\in[ 0 , c \rangle$$ Where $x=$ is the velocity of an object moving relative to another at rest. $C=$ speed of light in the vacuum. Wikipedia says that if γ (x) approaches with the Taylor polynomial centered on $ x = 0 $ of second degree $ P_2 (x) $, then the approximation error is as follows: The approximation $γ ≈ 1 + (1/2)β^2$ may be used to calculate relativistic effects at low speeds. It holds to within 1% error for $x < 0.4 c$, donde $β=x/c$. I suppose that refers to the error that is obtained with the rest of Lagrange. According to my own calculations this is: $$R_3(α,x)=\frac{{{\gamma ^{(3)}}(\alpha )}}{{3!}} \cdot {x^3}=\frac{{c\!\cdot\!\alpha\!\cdot\!\left( {{\alpha ^2} + \frac{3}{2}{c^2}} \right)}}{{{{\left( {{c^2} - {\alpha ^2}} \right)}^{7/2}}}} \cdot {x^3},\quad 0<α<x$$ Then the question is: How can I prove that $$\color{blue}{R(α,x)<1\%, \;\,\textrm{if}\; x < 0.4 c\,?}$$","['derivatives', 'taylor-expansion', 'special-relativity']"
2343655,Why do we use eigenvalues and eigenvectors to build Solutions of ODE?,"I was wondering about why we use eigenvalues and eigenvectors to build solutions of systems of ODE... What is the reasoning behind this procedure? When building approximated solutions near to fixed points, why do we use eigenvalues and eigenvectors to do this? Hugs!","['ordinary-differential-equations', 'linear-algebra', 'calculus']"
2343724,Help in Apostol Vol 1 integration review exercise,"5.11.9 Show that $$\displaystyle \int_0^x{\frac{\sin(t)}{t+1}\ dt} \geq 0$$
$\forall x\geq 0 $ My attempt - By 2nd mean value theorem, $\displaystyle \int_0^x{\frac{\sin(t)}{t+1}\ dt} = \frac{1}{0+1} (1 - \cos(c)) + \frac{1}{x+1}(\cos(c) - \cos(x)) = 1 - \frac{x}{x+1}\cos(c) - \frac{\cos(x)}{x+1} $ But now how to show the above.","['integration', 'calculus']"
2343803,How do I solve the Riccati Differential Equation with fsolve in MATLAB / Octave?,"Let's say that I have this equation. This is the Riccati Differential Equation: $$P A + A^T P - P B R^{-1} B^T P + Q  = 0$$ I know $ A, B, Q, R$. My goal is to find $P$. How do I use the MATLAB / Octave command fsolve to find $P$ ? EDIT: $$ $$ Here is the answer - An example to get LQR matrix. >> fun = @(P, A, B, Q, R) P*A+A'*P-P*B*inv(R)*B'*P + Q
fun =

@(P, A, B, Q, R) P * A + A' * P - P * B * inv (R) * B' * P + Q
>> p0 = 0.1*ones(2)
p0 =

   0.10000   0.10000
   0.10000   0.10000

>> P = fsolve(@(P) fun(P, Amat, Bmat, Q, R), p0)
P =

   55.67637    0.57241
    0.57241    2.29935

>>

>> inv(R)*Bmat'*P
ans =

   2.2269e-02   2.2895e-04
  -1.4350e-03  -5.7645e-03

>> lqr(Amat, Bmat, Q, R)
ans =

   2.2269e-02   2.2899e-04
  -1.4353e-03  -5.7644e-03

>>","['optimal-control', 'matrices', 'matlab', 'nonlinear-system', 'ordinary-differential-equations']"
2343821,Geometry problem - perpendicular line to angle bisector,"Here is my question: Let $\triangle ABC$ and let $K,L$ be midpoints of $AC$ and $AB$ respectively.
  Let $D$ be some point on $AC$ (between $K$ and $C$) such that $KD=AL$. Show that the perpendicular line from $D$ to the angle bisector of $A$ halves $BC$. First of all, I've drawn the following drawing: I tried to connect $KE$ and $KL$ and to prove that $KE$ is parallel to $AB$, but to no avail. Please give a hint, I find this question very hard.",['geometry']
2343910,Weird differential equation,"How to solve the given differential equation(given $ f(1)=e$ and  $f(0)=0$, $f'(0)=1$,$f''(0)=0.$)
$$f''(x)=2xf'(x)+4f(x).$$ Where $f'(x)$ is first derivative and $f''(x)$ is second derivative.
I was trying to guess that the function is exponential(of form of $e^{x^2}$) but couldn't get the final function.",['ordinary-differential-equations']
2343941,A pattern for BBP-type pi formulas? One for order 28?,"The original BBP pi formula is $$\pi = \sum_{n=0}^\infty \frac1{2^{4n}}\left(\frac{4}{8n+1}-\frac{2}{8n+4}-\frac{1}{8n+5}-\frac{1}{8n+6}\right)$$ Others are: Order 4. Let $P_k = 4n+k$ $$\pi = \sum_{n=0}^\infty \frac{(-1)^n}{\color{blue}{2^{2n}}}\left(\frac{2}{P_1}+\frac{2}{P_2}+\frac{1}{P_3}\right)$$ Order 8. Let $Q_k = 8n+k$ $$\pi = \frac{1}{2}\sum_{n=0}^\infty \frac1{\color{blue}{2^{4n}}}\left(\frac{2^3}{Q_2}+\frac{4}{Q_3}+\frac{4}{Q_4}-\frac{1}{Q_7}\right)$$ Order 12. Let $R_k = 12n+k$ $$\pi = \frac{1}{2^3}\sum_{n=0}^\infty \frac{(-1)^n}{\color{blue}{2^{6n}}}\left(\frac{2^5}{R_2}+\frac{24}{R_3}+\frac{4}{R_6}+\frac{3}{R_9}+\frac{2}{R_{10}}\right)$$ Order 16. Let $S_k = 16n+k$ $$\pi = \frac{1}{2^5}\sum_{n=0}^\infty \frac{1}{\color{blue}{2^{8n}}}\left(\frac{2^7}{S_2}+\frac{2^6}{S_3}+\frac{2^6}{S_4}-\frac{2^4}{S_7}+\frac{2^3}{S_{10}}+\frac{4}{S_{11}}+\frac{4}{S_{12}}-\frac{1}{S_{15}}\right)$$ Order 20. Let $T_k = 20n+k$ $$\pi = \frac{1}{2^6}\sum_{n=0}^\infty \frac{(-1)^n}{\color{blue}{2^{10n}}}\left(\frac{2^9}{T_2}-\frac{160}{T_5}-\frac{2^7}{T_6}-\frac{2^3}{T_{10}}-\frac{2^3}{T_{14}}-\frac{5}{T_{15}}+\frac{2}{T_{18}}\right)$$ Order 24. ( known ) $$\pi = \frac{1}{2^5}\sum_{n=0}^\infty \frac{1}{\color{blue}{2^{12n}}}\left(\sum_{k=1}^{23} \frac{a_k}{24n+k}\right)$$ Order 28. ($\color{red}{unknown}$) $$\pi = \frac{1}{2^{(?)}}\sum_{n=0}^\infty \frac{(-1)^n}{\color{blue}{2^{14n}}}\left(\sum_{k=1}^{27} \frac{b_k}{28n+k}\right)$$ Order 32. ( known ) $$\pi = \frac{1}{2^{12}}\sum_{n=0}^\infty \frac{1}{\color{blue}{2^{16n}}}\left(\sum_{k=1}^{31} \frac{c_k}{32n+k}\right)$$ where $a_k,c_k$ are integers and can be found here . R : However, what (if any) is the sequence of integers $b_k$ such that order 28 is true? P.S. I tried Mathematica's Integer relations feature but couldn't find any, though I may have used too little decimal precision.","['number-theory', 'computational-mathematics', 'pi']"
2343958,Book for quantum information theory,I am interested in a mathematical approach to quantum information theory. I have observed that several probabilists have been working in this area. What can be a suitable background and good book for this subject?,"['reference-request', 'probability-theory', 'book-recommendation', 'mathematical-physics']"
2343963,Small group characterizing identity matrix,"I am looking for a small (say, finite and of small cardinality) subgroup of the general linear group whose centralizer consists only of scalar matrices. I work over complex numbers. A more precise statement of the problem is the following: let $GL_n$ be the general linear group of invertible $n\times n$ matrices. I am looking for $G \subseteq GL_n$ such that $C_{GL_n}(G) = Z(GL_n)$ (where $C_{GL_n}$ is the centralizer and $Z$ is the center), and I would like to know if the smallest such $G$ is known. This question can be rephrased, in terms of representation theory: if $GL_n = GL(V)$ for a vector space of dimension $n$, I am looking for the smallest $G \subseteq GL(V)$ such that the only $G$-invariant in $V^* \otimes V$ is the identity (up to scale). I know it can be done with $G = \mathbb{Z}_2 \wr \mathbb{Z}_n$, where $\mathbb{Z_n}$ acts on $n$ copies of $\mathbb{Z_2}$ by cyclically permuting the factors. $\mathbb{Z}_2^n$ embeds in $GL_n$ as diagonal matrices with entries $\pm 1$ and $\mathbb{Z}_n$ embeds as the group generated by the permutation matrix corresponding to the long $n$-cycle ($1$ in the $n-1$ entries immediately above the diagonal and in the bottom left entry and $0$ elsewhere). This gives a copy of $G$ in $GL_n$ that is centralized only by scalar matrices and its order is $\vert G \vert = n2^n$. I would bet that it is possible to do better (namely a smaller $G$), maybe even a lot better; but I am not sure what a systematic approach to find such small $G$ could be.","['finite-groups', 'invariant-theory', 'group-theory']"
2343964,The Hessian matrix A may be Indefinite or what is known Positive Semidefinite or Negative Semidefinite,"We are about to look at an important type of matrix in multivariable calculus known as Hessian Matrices. We will then formulate a generalized second derivatives test for a real-valued function $z=f(x_1,x_2,...,x_n)$ of n variables with continuous partial derivatives at a critical point $a=(a1,a2,...,an)∈D(f)$ to determine whether $f(a)$ is a local maximum value, local minimum value, or saddle point of $f$ Definition: Let $x=(x1,x2,...,xn)$ and let $z=f(x1,x2,...,xn)=f(x)$ be an n variable real-valued function whose second partial derivatives exist. Then the Hessian Matrix of f is the n×n matrix of second partial derivatives of f denoted $$\mathcal H (\mathbf{x}) = \begin{bmatrix} f_{11} (\mathbf{x}) & f_{12} (\mathbf{x}) & \cdots & f_{1n} (\mathbf{x})\\ f_{21} (\mathbf{x}) & f_{22} (\mathbf{x}) & \cdots & f_{2n} (\mathbf{x})\\ \vdots & \vdots & \ddots & \vdots \\ f_{n1} (\mathbf{x}) & f_{n2} (\mathbf{x}) & \cdots & f_{nn} (\mathbf{x}) \end{bmatrix}$$. a)$\mathcal H (\mathbf{x})$ is said to be $\textbf{Positive Definite}$ if $D_i>0$ for i=1,2,...,n. b) $\mathcal H (\mathbf{x})$ is said to be $\textbf{Negative Definite}$ if $D_i<0$ for odd i∈{1,2,...,n} and Di>0 for even i∈{1,2,...,n}. c) $\mathcal H (\mathbf{x})$ is said to be $\textbf{Indefinite}$ if $det(\mathcal H (\mathbf{x}))=D_n≠0$ and neither a) nor b) hold. d) If $det(\mathcal H (\mathbf{x}))=D_n=0$, then $\mathcal H (\mathbf{x})$ may be Indefinite or what is known Positive Semidefinite or Negative Semidefinite. I'm studying a function that has a $det(\mathcal H (\mathbf{x}))=D_n=0$ What is the appropriate way to classify critical points if $det(\mathcal H (\mathbf{x}))=D_n=0$ For example 
Assume we have this function
$$f(x_1,x_2,x_3)=x_1 x_2+x_1 x_3-x_1x_2 x_3 $$
therefore, we found the critical point by solve this system 
$$\begin{equation}
\begin{cases}
\frac{df}{dx_1}=x_2+x_3-x_3 x_2=0\\
\frac{df}{dx_2}=x_1-x_1 x_3 =0\\
\frac{df}{dx_3}=x_1-x_1 x_2 =0 
\end{cases}
\end{equation}$$
we get $c=(0,0,0)$
,use the Hessian matrix of f
$$\mathcal H (\mathbf{f(x_i)})=\left(
\begin{array}{ccc}
 0 & 1-x_3 & 1-x_2 \\
 1-x_3 & 0 & -x_1 \\
 1-x_2 & -x_1 & 0 \\
\end{array}
\right)$$
$det(\mathcal H (\mathbf{f(x_i)}))=D_n=0$ This test fails to determine the type of critical points Is there another way?
Or modification of the solution and correction.
Thanks for the help.","['multivariable-calculus', 'hessian-matrix', 'positive-semidefinite', 'multivalued-functions']"
2343978,Taking element that is not in given set,"I am reading a solution to a problem, where set $J$ is given and it is said that Let us extend $J$ by one element $x∉J$ , $J′=J∪\{x\}$. I can't understand where did we get this $x$ from? After thinking a bit I think there is Russel's Paradox (or perhaps its non-existence) is involved, I mean since $J$ is (perhaps) not set of all the sets, we can take an element from some set of all sets$-J$ , but this argument seems silly. I vaguely remember I read somewhere that we can go with assuming $x=J$ in this situation and $J'=J\ \cup\{J\}$ will do. Is this approach true?",['elementary-set-theory']
2343986,"Show that $(X,|||\cdot|||)$ is a Banach space.","In the book ' Classical Banach Spaces I and II ' by Lindenstrauss, page $1,$ Chapter $1$, he stated the following: Let $(X,\|\cdot\|)$ be a Banach space with a (Schauder) basis $(x_n)_{n=1}^{\infty}.$ For every $x = \sum_{n=1}^{\infty}a_nx_n$ in $X$ the expression $$|||x||| = \sup_{n}\left\Vert \sum_{i=1}^n a_ix_I \right\Vert$$
   is finite. Evidently, $|||\cdot|||$ is a norm on $X$ and $\|x\| \leq |||x|||$ for every $x \in X.$ A simple argument shows that $X$ is complete also with respect to $|||\cdot|||.$ Question: How to use a simple argument to show that $(X, |||\cdot|||)$ is complete? I manage to prove all statements before it. Below is my partial attempt (actually is not even partial). Suppose that $(x_n)_{n=1}^{\infty}$ is a Cauchy sequence in $(X, |||\cdot|||).$ Since $\|\cdot\| \leq |||\cdot|||$ and $(X,\|\cdot\|)$ is a Banach space, there exists an $x \in X$ such that $\lim_{n}\|x_n - x\| = 0.$ I think I should I show that $x_n$ converges to $x$ in the norm $|||\cdot|||.$ But I have no idea how to show at all. Any hint would be appreciated.","['real-analysis', 'banach-spaces', 'complete-spaces', 'functional-analysis', 'schauder-basis']"
2344033,Why is $\deg_s(\hat\phi_r)=\deg_s[p^r]$?,"I'm having trouble with a part of the proof of V $.3.1$ in Silverman's Arithmetic of Elliptic Curves. In this part of the proof he's trying to prove the following equivalence: Let $K$ be a perfect field of characteristic $p$ , and $E/K$ an elliptic curve. For $r\ge1$ , let $\phi_r:E\to E^{(p^r)}$ be the $p^r$ -power Frobenius map and $\hat\phi_r$ its dual. The following are equivalent: (i) $E[p^r]=0$ (ii) $\hat\phi_r$ is purely inseparable Here is how the proof goes: Since the Frobenius map is purely inseparable, we have $$\deg_s(\hat\phi_r)=\deg_s[p^r]=(\deg_s[p])^r=\deg_s(\hat\phi_1)^r.$$ From this it follows that $$\# E[p^r]=\deg_s(\hat\phi_r)=\deg_s(\hat\phi)^r,$$ from which the equivalence follows immediately. Here, $\deg_s(\psi)$ denotes the separable degree of $\psi$ , $E[m]$ denotes the $m$ -torsion subgroup of $E$ and $[m]$ denotes the $m$ -times multplication map, $P\mapsto m P$ . The author seems to be implicitly using the fact that $\deg_s(\hat\phi_r)=\deg_s[p^r]$ for all $r$ , which is where my confusion lies. Why is this the case? Does this follow from a more general fact? In particular, the author says, ""Since the Frobenius map is purely inseparable, we have..."", but where is the inseparability of $\phi_r$ being used?","['elliptic-curves', 'algebraic-geometry']"
2344069,Can the closure of a countable set be characterized sequentially?,"Suppose that I have a countable subset $S \subset X$, where $(X, \tau)$ is a topological space that is NOT first countable (so that convergence is characterized by nets and not sequences). I'm interested in the closure $cl(S)$, which is defined as the collection of all limit points of $S$, where limit points of $S$ are defined as all elements $x \in X$ such that for every open set of $X$ containing $x$, there exists some element of $A$ within that open set (but does not equal $x$ itself). My question is then the following: Because the set $S$ is itself countable, can we say that $cl(S)$ is equal to the set of limit points of all sequences of elements in $S$ ? I understand when considering a potential limit point of $A$, there could be an uncountable many open sets that elements in $A$ must fall within, but on the other hand, there are only a countable many elements of $A$ in the first place. On which side of things does this fall? Thanks!","['general-topology', 'real-analysis', 'elementary-set-theory']"
2344144,On series $\sum\limits_{n\in\mathbb{Z}}\frac{(-1)^n}{(n+x)^{\alpha}}$,"In the course of some physics related work I met the following series,
\begin{align}
S_{\alpha}(x)=\sum\limits_{n\in\mathbb{Z}}\frac{(-1)^n}{(n+x)^{\alpha}}, && x\in[0,1], ~k\in\mathbb{Z}_{\geq0}
\end{align}
Which can be written in a compact form using a combination of Hurwitz zeta functions. However, with some help from Wolfram Alpha, I realised that computing by hand some particular values of the case I am interested, which are positive odd integers, it seems we can simply write the above as a combination of powers of $\sin^{-1}{\pi x}$. For example, \begin{align}
S_1(x) &= \frac{\pi}{\sin{\pi x}}\\
S_3(x) &= \frac{\pi^3}{4}\frac{3+\cos{2\pi x}}{\sin^3{\pi x}} = \frac{\pi^2}{2}\left[\frac{2}{\sin^3{\pi x}}-\frac{1}{\sin{\pi x}}\right]\\
S_5(x)&=\pi^5\left[\frac{1}{\sin^5{\pi x}}-\frac{5}{6}\frac{1}{\sin^3{\pi x}}+\frac{1}{24}\frac{1}{\sin{\pi x}}\right]
\end{align}
$\textbf{Question}$: Can we write
\begin{align}
S_{2k+1}(x)=\sum\limits_{0<n\leq 2k+1}\frac{a_n}{\sin^n{\pi x}} 
\end{align}
for some explicit coefficients $a_n$? I did some literature research related to the Hurwitz Zeta but did not find much. Suggestions in this direction are also be appreciated.","['trigonometry', 'sequences-and-series']"
2344175,Proof verification - A polynomial $P(x)$ has only real roots $\implies$ $P'(x)$ also has only real roots,"Here's a problem that I've solved but I'm not very confident on my solution. Please check it there's any gap in my arguments. Also, is there a way to come up with a shorter proof ? Thank you. The Problem : If the polynomial $P(x)=a_nx^n+a_{n-1}x^{n-1}+\ldots+a_0,~a_n \neq 0,$ $(a_j \in \mathbb{R} ~\text{ for }~0 \leq j \leq n)$ has only real roots, then it's derivative $P'(x)$ has only real roots. My Solution : We know that, a polynomial of degree $n$ has exactly $n$ roots. Let the roots be denoted by $\alpha_1, \alpha_2,\ldots,\alpha_n$. Case-I : Let all the roots be distinct . WLOG let $\alpha_1<\alpha_2<\ldots<\alpha_n$. Then, by Rolle's theorem $\forall~ 0 \leq j \leq n, \exists~ c_j \in (\alpha_j,\alpha_{j+1})$ such that $P'(c_j)=0$. Note that we're free to use Rolles's theorem as $P$ is differentiable $($and hence also continuous $)$ throughout $\mathbb{R}$. Hence we have $n-1$ real roots of $P'(x)$ namely $c_1,c_2,\ldots,c_{n-1}$. Since $P'(x)$ is a polynomial of degree $n-1,$ and hence has exactly $($though at most suffices, in this case$) ~n-1$ roots. Hence all roots of $P'(x)$ are real. So this case was quite trivial (in a relative sense). Case-II : All the roots are NOT distinct . Suppose we have $m~(<n)$ distinct roots $\beta_1, \beta_2,\ldots,\beta_m$ with respective multiplicities $k_1,k_2,\ldots,k_m$. Clearly, $\sum_{j=1}^m k_j=n$. Claim : If $\beta$ is a root of $P(x)$ with multiplicity $k~(>1),$ then $\beta$ is a root of $P'(x)$ with multiplicity $k-1$. Proof of the claim : $\beta$ is a root of $P(x)$ with multiplicity $k~(>1)$ $\implies$ $P(x)=(x-\beta)^k Q(x)$ where $Q(x),$ is a polynomial of degree $n-k$ and $Q(\beta) \neq 0$. Then $P'(x)=k(x-\beta)^{k-1}Q(x)+(x-\beta)^kQ'(x)=(x-\beta)^{k-1}\{kQ(x)+(x-\beta)Q'(x)\}$ Hence $\beta$ is a root of $P'(x)$ with $k-1$ multiplicity $($Since $kQ(\beta)+(\beta-\beta)Q'(\beta)=kQ(\beta) \neq 0)$. End of Proof of the claim. Using the claim, for all $0 \leq j \leq m, \beta_j$ is a root of $P'(x)$ with multiplicity $k_j-1$. Like in Case-I , we use Rolle's theorem again to come up with $c_j \in (\beta_j,\beta_{j+1})$ such that $P(c_j)=0, ~1 \leq j \leq m-1$. So in total, as roots of $P'(x),$ we have $c_1,c_2,\ldots,c_{m-1}$ each with multiplicity $1,$ $($hence in total $m-1$ roots$),$ and $\beta_1,\beta_2,\ldots,\beta_m$ with respective multiplicities $k_1-1,k_2-1,\ldots,k_m-1$ $($which adds up to $\sum_{j=1}^m(k_j-1)=n-m$ roots$).$ Hence we have $m-1+n-m=n-1$ real roots of $P'(x)$. Since $P'(x)$ is a polynomial of degree $n-1,$ it has exactly $n-1$ roots. Hence all the roots of $P'(x)$ is real. $\blacksquare$","['derivatives', 'real-analysis', 'polynomials', 'roots', 'proof-verification']"
2344185,Showing $\# E(\Bbb F_p)$ is divisible by $4$ for $p\ge3$.,"I'm trying to solve the following exercise from Silverman: Let $E:y^2=x^3+x$. Show that $\# E(\Bbb F_p)$ is divisible by $4$ for prime $p\ge 3$. My approach is as follows: if $\phi$ denotes the $p$-power Frobenius morphism, then $P\in E(\Bbb F_p)$ if and only if $\phi(P)=P$. Therefore $\# E(\Bbb F_p)=\#\ker(1-\phi)$, which because $1-\phi$ is separable is equal to $\deg(1-\phi)$. I have therefore reduced my problem to showing $\deg(1-\phi)$ is divisible by $4$. To do this, I would like to use the more general fact that if $\psi$ is an endomorphism of $E$, and we let $\psi_{\ell}$ denote the induced endomorphism on the Tate module $T_{\ell}(E)$ for any prime $\ell$ not equal to the characteristic, then we have $$\deg(\psi)=\det(\psi_{\ell}).$$ I would like to apply this to our case, when $\psi=1-\phi$ and $\ell=2$. So then I just need to calculate $\det((1-\phi)_2)$. I don't know a good way to do this though, as I'm not sure how to get an explicit $\Bbb Z_{\ell}$-basis of $T_{\ell}(E)\cong\Bbb Z_{\ell}\times\Bbb Z_{\ell}$ (the proof of the latter isomorphism was very non-constructive). I don't even know how to give any explicit points of $E(\Bbb F_p)$ besides $O$, as it relies on $x^3+x$ being a quadratic residue modulo $p$. Does anybody have any suggestions on how to proceed? Does this approach even seem viable, or is there an easier way to do this?","['elliptic-curves', 'algebraic-geometry']"
2344220,How do you get the multiplicative inverse by reading a multiplication table of modulo $n$?,"Here is the multiplication table for$\mod 7$: https://gyazo.com/82fb45bd89f61df3b44f00f67efc63c1 How do I read this to get the multiplicative inverse of something, for example like: $5\mod 7$ I know the answer is $3$ by using a different method, but how do I get it using this table?","['multiplicative-function', 'modular-arithmetic', 'discrete-mathematics']"
2344254,Can we find a well ordering on an infinite set with no largest element?,"According to the well ordering theorem "" Any set can be well ordered "". Whenever we have a well ordering on a set, it is not difficult to construct a new well ordering with a largest element. My question is: For a given infinite set, can we find a well ordering on it such that there is no largest element?","['well-orders', 'elementary-set-theory']"
2344261,Uniqueness for antipodal points of maximum distance on closed convex surface,"Let $S\subset \mathbb{R}^3$ be a closed convex surface and let $p,q\in S$ be points such that $d(p,q)=\operatorname{diam}(S)$ where $\operatorname{diam}(S)$ is the diameter of $S$ with respect to the intrinsic distance $d$ of $S$. Does $d(p,q')=\operatorname{diam}(S)$ imply $q=q'$?","['riemannian-geometry', 'differential-geometry', 'surfaces']"
2344271,Coefficient of $x^2$ in a polynomial,"Let $p(x)$ be an polynomial given by
  $$p(x)=(1+x)(1+3x)(1+5x)\cdots(1+19x)$$ Find the coefficient of $x^2$. I see that this coefficient is the sum of products of roots of $p(x)$ get eight to eight. But, there is ${10 \choose 8}=45$ terms in this sum. Someone know other way more fast?","['algebra-precalculus', 'summation']"
2344276,Best strategy to solve absolut value inequality,"Is there any best strategy to go with when solving inequalities involving absolute values? Up until now I found three different methods, which work more or less for every example I have tried so far. I'm wondering though if these methods have any limitations to when they can be used and which I should generally go for. Let's demonstrate those methods with the following example: $$ |x-1| \le |x+3| $$ 1) Cases We basically use the definition of the absolute value here. $$
|x+3|=\left\{ \begin{align}
x+3 & \text{   , if }x\geq -3 \\
-(x+3) & \text{   , if }x <-3 
\end{align}
\right\}
$$ $$
|x-1|=\left\{ \begin{align}
x-1 & \text{ , if }x\geq 1 \\
-(x-1) & \text{   , if }x < 1 
\end{align}
\right\}
$$ Now we must distinguish the following cases: For $x \lt -3$: $$-(x-1) \le -(x+3) $$
$$ \iff x+3 \le x-1 \iff 1 \le -3 \implies \text{ no solutions for } x \lt -3 $$ For $-3 \le x \lt 1$: $$-(x-1) \le (x+3) $$
$$ \iff -2 \le 2x \iff x \ge -1 $$ For $x \ge 1$: $$x-1 \le x+3 $$
$$ \iff -1 \le 3 \implies \text{ all } x \ge 1 \text { are solutions }$$ So putting all cases together yields to the result: $$x \ge -1 $$ $$\\$$ 2) Squaring We square each sides which doesn't change the inequality. $$ |x-1| \le |x+3| \iff (x-1)^2 \le (x+3)^2 $$
$$ \iff x^2 - 2x + 1 \le x^2 +6x +9 \iff -8 \le 8x \iff x \ge -1 $$
$$\\$$ 3) ""Less / Greater Than rules"" The rules I used here are: (i): $|a| \le b \Leftrightarrow -b \le a \le b$ (which can be written also as $-b \le a$ AND $a \le b $) (ii): $ b \le |a| \Leftrightarrow b \le a$ OR $a \le -b $ Threat the right absolut value of the original inequality like b and proceed with (i): $$ x-1 \le |x+3| \text{ AND } -|x+3| \le x-1 \iff -(x-1) \le |x+3|$$ For the first inequality we can use (ii) to get the following results: $$ x-1 \le x+3 \iff -1 \le 3 \text{ (true statement) }$$
$$ x+3 \le -(x-1) \iff 2x \le -2 \iff x \le -1 $$ Now for the second inequality we use (ii) too: $$ -(x-1) \le x+3 \iff -2 \le 2x \iff x \ge -1 $$
$$ x+3 \le x-1 \iff 3 \le -1 \text{ (false statement) } $$ As we see from the rules, we now got a (a OR b) AND (c OR d) conjunction, so our result is $$ \text{ ( } true \text{ OR } x \le -1 \text{ ) AND ( } x \ge -1 \text{ OR } false \text{ ) } $$ Which reduces to: $$  true \text{ AND } x \ge -1 \text{ and gives again $x \ge -1$ } $$
$$\\$$ While the method with separating the cases should work every time (I guess?) it's sometimes confusing and not the fastest. Squaring can be quick (like here) but only works sometimes, it can often give you a difficult polynomial for which you need a calculator to plot it / get its roots. And for the ""Less / Greater Than Rules"", I'm still wondering when they can be applied and when not.","['algebra-precalculus', 'inequality', 'absolute-value']"
2344299,Solving recurrence relations with a squared term?,"$F_{n+1} = F_n^2+2F_n$.
Is there a way to solve this equation with the standard technique of solving the associated quadratic? In general, when can I and when can I not use the ""polynomial solution"" to a recurrence equation. For anyone wondering where this recurrence comes from see 1985 Putnam A3.","['recurrence-relations', 'recursion', 'contest-math', 'sequences-and-series', 'discrete-mathematics']"
2344314,I need to solve a problem. What branches of math should I study?,"The answer seems to be 'statistics' but I assume that's about as useful as saying a person interested in gravity should study 'physics'. I anticipate a large number of data points to be delivered to me from a sensor. For purposes of discussion, say it's the temperature at the top of a tower, every second.  That is, I'll have one sensor returning values every second. I know the possible range of the values, and I'll know when they were taken. I want to know if a particular value is unusual. I want to know with some certainty that since time X, the values are trending up or down. I want to know if the sensor is misbehaving. For example, if our temperature sensor developed a bad ground and Murphy paid a visit, it could be the temperature would seem very stable. The average could be about right, and nope, not trending at all. Different analysis could point out that the nature of the data has changed, or that it is indeed just noise. So of course this is statistics but is there a specific type I should care about? EDIT - another reasonable use case is to notice cyclic behavior. It would probably be pretty normal for network traffic or memory usage to increase starting at about 8am 5 days a week. We would not be concerned by this. But what if one Monday, it didn't?  What if another cycle was noticed, say, one that peaked on the 3rd Saturday of each month? Analysis of cyclic behavior says FFT to me, but after that I am lost. (This may sound like a climate change question. It isn't. The fact is I am looking for a general solution. I am not even sure what the data will be. I'll ultimately have hundreds of sensors reporting to me... everything from memory usage on computers to the timestamps in which a security door is opened...)",['statistics']
2344320,Minimal Polynomial of $AB$ and $BA$,"Let $A,B$ be linear operators on a complex vector space $V$ . Prove the following: (a) For any polynomial $p$ such that $p(AB)=0$ , if $q(x)=x\cdot p(x)$ , then $q(BA)=0$ (b) Use part (a) to show that if $m_{AB}$ is the minimal polynomial of $AB$ , then $m_{AB}=m_{BA}$ or $m_{AB}=x\cdot m_{BA}$ . I've proven part (a). For part (b), I think there should be a third possibility, $m_{BA}=x\cdot m_{AB}$ . Is this correct? Here's what I have: $m_{AB}(AB)=0$ , so by part (a), if $q(x)=x\cdot m_{AB}$ , then $q(BA)=0$ . So $m_{BA}|q\implies\exists \ g\in\mathbb{C}[x]$ s.t. $m_{BA}\cdot g=q=x\cdot m_{AB}$ . Reversing the roles of $AB$ and $BA$ $\exists\tilde{g}\in\mathbb{C}[x]$ s.t. $m_{AB}\cdot\tilde{g}=x\cdot m_{BA}$ . Hence, \begin{cases}m_{BA}\cdot g=x\cdot m_{AB}\\ m_{AB}\cdot\tilde{g}=x\cdot m_{BA}\end{cases} . This implies $m_{AB}\cdot\tilde{g}g=x\cdot m_{BA}\cdot g=x^2\cdot m_{AB}$ . So $\tilde{g}g=x^2$ . Which means we have three cases: (1) $g=\tilde{g}=x$ , (2) $g=x^2,\tilde{g}=1$ , or (3) $g=1,\tilde{g}=x^2$ . For case (1), $m_{BA}\cdot x=x\cdot m_{AB}\implies m_{AB}=m_{BA}$ . For case (2) $m_{AB}=x\cdot m_{BA}$ . For case (3), $m_{BA}=x\cdot m_{AB}$ .","['minimal-polynomials', 'linear-algebra', 'proof-verification']"
2344341,Expand $f(z)=\cos z$ about $z=\pi$,"I have a question regarding a complex Taylor expansion problem. Expand $f(z)=\cos z$ about $z=\pi$ Using the formula for the Taylor expansion, where $R_0$ is the radius $$ f(z)=\sum_{n=0}^\infty a_n (z-z_0)^n \ (|z-z_0|<R_0) \ \ \text{for} \ \ a_n=\frac{f^{(n)}(z_0)}{n!} \ n=0,1,2,3,...$$ I get following answer: $$ \cos z = \sum_{n=0}^\infty \frac{(-1)^{n+1}}{(2n)!}(z-\pi)^{2n} $$ But my professor gives the solution $$ \cos z = \sum_{n=0}^\infty \frac{(-1)^{n+1}}{(2n)!}z^{2n} \ \ \text{for} \ \ (|z-\pi|<\infty) $$ Can someone please explain the logic in my professor's solution? Thank you! ========= In response to Lord Shark the Unknown's comment, here is my work. $$ f(z)=\cos z, \ f'(z)=-\sin z, \ f''(z)=-\cos z, \ f'''(z)= \sin z, \ f^{(4)}(z) = \cos z \\ \therefore f(\pi) = -1, \ f'(\pi) = 0, \ f''(\pi)=1, \ f'''(\pi)=0, \ f^{(4)}(\pi)=-1$$ So, let $z_0=\pi$. Then $$ a_0 = -1, \ a_1=0, \ a_2=\frac{1}{2!}, \ a_3=0, \ a_4=\frac{-1}{4!} \Rightarrow a_{2n}=\frac{(-1)^{n+1}}{(2n)!} \\ \therefore \cos z = \sum_{n=0}^\infty \frac{(-1)^{n+1}}{(2n)!} (z-\pi)^{2\pi} $$","['complex-analysis', 'taylor-expansion', 'sequences-and-series']"
2344344,Extremepoints of function with two variables,"$f(x,y)=x\cdot y(ax+by+c)\quad x,y>0\quad abc\neq 0\\ $ Partial Derivatives: ${ f }_{ x }=y(ax+by+c)+axy\\ { f }_{ y }=x(ax+by+c)+byx$ How do I find the critical Points out of The system of equations beneath, aren't they depended on the behaviour of a,b,c $y(ax+by+c)+axy=0$ $x(ax+by+c)+byx=0$","['multivariable-calculus', 'real-analysis']"
2344348,Runge-Kutta 4th order,"I am not sure if it's a mathematics question or coding part although I am pretty sure that the code I used is right,so do excuse me if I a wrong. I have a set of three first order ode and I am trying to numerically integrate them in python using RK4 method. The problem arises with the exponential term where python rounds it to $0$ and the values thereafter are returned ""NAN"". I tried various packages to deal with large values still no use. I am thinking if I modify the equations or scale them so that the value is smaller might help, but I am not sure how to do that. Any help or the reference to that would be really helpful as I am kind of in a really stuck up situation.
Here's the code I have used for this purpose: #simple exponential potential
# u = K*phi'/H0; v = omega_matter**(1/3)*(1+z); w = l*K*phi' - ln((K**2)*V0/H0**2),z from 1100(initial) to z=0(final)
# f,g,h are functions of derivation of u,v,w respectively derived w.r.t t*H0 = T

import matplotlib.pyplot as plt
import numpy as np
import math

def f(u,v,w):
    return -3*u*((v**3 + (u**2)/6 + np.exp(-w)/3)**0.5) + l*np.exp(-w)

def g(u,v,w):
    return -v*(v**3 + (u**2)/6 + np.exp(-w)/3)**0.5

def h(u):
    return l*u

z = np.linspace(start=0.0,stop=1.0,num = 10001)
print (z)    
p = 0.1
q = 1.0
n = 10000.0
dh = (q-p)/n
u = [0.0]
v = [1101]
w = [1.8]
l = 1.0


for i in range(0,int(n)):
k1 = f(u[i],v[i],w[i])
r1 = g(u[i],v[i],w[i])
s1 = h(u[i])
k2 = f(u[i] + k1*dh/2,v[i] + r1*dh/2,w[i] + s1*dh/2)
r2 = g(u[i] + k1*dh/2,v[i] + r1*dh/2,w[i] + s1*dh/2)
s2 = h(u[i] + k1*dh/2)
k3 = f(u[i] + k2*dh/2,v[i] + r2*dh/2,w[i] + s2*dh/2)
r3 = g(u[i] + k2*dh/2,v[i] + r2*dh/2,w[i] + s2*dh/2)
s3 = h(u[i] + k2*dh/2)
k4 = f(u[i] + dh*k3,v[i] + dh*r3,w[i] + s3*dh)
r4 = g(u[i] + k3*dh,v[i] + dh*r3,w[i] + s3*dh)
s4 = h(u[i] + dh*k3)
u == u.append(u[i] + (dh/6)*(k1 + 2.0*k2 + 2.0*k3 + k4))
v == v.append(v[i] + (dh/6)*(r1 + 2.0*r2 + 2.0*r3 + r4))
w == w.append(w[i] + (dh/6)*(s1 + 2.0*s2 + 2.0*s3 + s4))

plt.plot(z,u, '-b')
plt.plot(z,v, '-r')
plt.plot(z,w, '-g') 
#plt.plot(u,v)
#plt.plot(u,w)  
plt.title('quintessence cosmological model')
plt.show()","['exponential-function', 'numerical-methods', 'ordinary-differential-equations', 'runge-kutta-methods']"
2344356,How modular are Tarski's axioms of Euclidean geometry?,"Tarski's axioms for Euclidean geometry can also be used to axiomatize absolute geometry (by leaving out his version of the Axiom of Euclid) and hyperbolic/Lobachevskian geometry (by negating that same axiom) (see the last paragraph of ""Discussion"" here ). Question: Can similar subsets of Tarski's axioms be used to define axiomatizations for both: ordered geometry ? affine geometry ? Notes: (The tag first-order-logic is because Tarski's axioms are all written in terms of first-order logic, and to understand them well, e.g. unlike me, one would have to understand first-order logic at least somewhat well. Plus as a result the resulting axiom systems for ordered and affine geometry I am requesting would also be in terms of first-order logic. If this is an inappropriate reason for  the tag, please feel free to remove the tag - I don't use it frequently so don't know its correct usage.) The notion of betweenness which underlies ordered geometry also seems essential to Tarski's axiomatization approach. So to me it would be really surprising to me if Tarski's axioms could not be used to state an axiom system of ordered geometry. The answer for affine geometry may be negative according to this stray quote I found: Szmielew wished to keep the simplicity (in terms of obtaining metamathematical results) of Tarski’s system, but to gain the flexibility to be able to consider different kinds of geometry, such as affine geometry, which was not possible in Tarski’s formalization ... However, a reference for the claim nor details of a proof were forthcoming. I imagine the details might be found in the reviewed book , but I don't have access to said book and have very little knowledge of formal logic, so a dumbed-down summary of any argument why or why not it's possible would be more useful to me right now than the high-level arguments likely found therein. Also the claim seems to possibly contradict other claims made in a paper by Tarski and Givant cited and quoted in the Wikipedia article about Tarski's axioms: The first was the selection of the betweenness and equidistance relations as the only two primitive notions. Both notions have a clear and simple geometrical meaning; the former represents the affine , the latter the metric, aspect of geometry... The axiom... is formulated entirely in terms of betweenness, and hence is useful in the construction of an axiom set for affine geometry . However, the quotes might essentially just be observing that ordered geometry generalizes affine geometry (hence why betweenness as a notion can be essential for both types of geometry) and therefore not actually contradict the claim made in the review of Szmielew's book. Still, the claim being true would be somewhat surprising to me, since Euclidean geometry is a special case of affine geometry, so why wouldn't it be possible to just drop some of Tarski's axioms (assuming that they are all independent of one another) and get affine geometry as the result? A related question if you found this one interesting.","['first-order-logic', 'axioms', 'euclidean-geometry', 'foundations', 'geometry']"
2344374,Edge Coloring - subset with every color exactly once,"I have a question in combinatorics/graph theory about edge coloring that I ran into in some test in combinatorics and that I couldn't solve. The question goes like this: Let $G$ be an undirected graph with $n$ vertices and let $d\ge2$. We give each edge in this graph some color from a set with $k$ colors (where $k\ge n$). Every color is in at most $d$ edges and every vertex touches at least $2d$ colors (that also means $\deg(v)\ge2d$). Prove that there is some subgraph $G'$ (with the same vertices as $G$), so that every edge has a unique color (no color appears twice) and for every $v\in V(G)$ the degree is $\deg_{G'}(v)\ge1$ (so every vertex is connected to at least one edge). I tried to think of some way to use the pigeonhole principle but I do not know what may be the ""pigeons"". I would be glad for some direction for the proof.","['combinatorics', 'graph-theory', 'coloring']"
2344379,Rain droplets falling on a line,"Suppose there is a line of length $L$ cm. And it begins to rain at a constant rate of one droplet per second. Once a drop strike the line and it wets 1 cm of the line. What is the expected number of droplets it takes to wet the whole line? The following condition was suggested by Henry : To avoid the boundary problem, each point on the line within a distance of $\frac{1}{2}$ the point struck is wetted. The centre of the drop can be anywhere on the line (so if near the end would cover less than 1 but at least $\frac{1}{2}$ cm of the line. The discrete version of this problem is a classical coupon collector problem ，I am curious what would happen in the continuous case. The related post of this problem is rain droplets falling in a table . Any help will be appreciated. Thanks in advance.","['stochastic-processes', 'geometric-probability', 'computer-science', 'probability-theory', 'probability']"
2344389,Is this an automorphism of a commutative ring of characteristic $p$?,Let $R$ be a commutative ring of characteristic $p$ (prime). It is evident that $a\mapsto a^p$ is an endomorphism of $R$. But is it an automorphism? This is the same as asking whether the equation $x^p = 0$ has only $x =0$ as a solution. It doesn't look true to me. If $R$ is a domain then clearly this is true. What about non-domains? Maybe this is very easy but I'm a beginner and I don't have much examples of rings. All the rings I know either satisfy this or do not have characteristic $p$.,"['abstract-algebra', 'ring-theory']"
2344391,Automorphism of a reduced scheme determined by its restriction to a dense open?,"Let $X$ be a reduced scheme, and let $U\subset X$ be a dense open, and let $Aut_U(X)$ be the group of automorphisms of $X$ leaving $U$ invariant (ie, which restricts to an action on $U$). Is the natural map $Aut_U(X)\rightarrow Aut(U)$ injective?",['algebraic-geometry']
2344412,Continuity of gaussian stochastic process,"Let $\mathcal{X}\triangleq (\Omega,\mathfrak{F}_t,\mathbb{P})$ be a filtered probability space. Is it possible for a stochastic process $X_t$ defined on $\mathcal{X}$ with increments satisfying $X_t-X_s$ are normal $
Var(X_t - X_s) \underset{t\to s}{\to} 0 ,
$ for every $t\geq s$ to have sample paths to not have $\mathbb{P}$-a.s. continuous sample paths?","['stochastic-processes', 'probability-theory', 'variance', 'stochastic-analysis', 'stochastic-calculus']"
2344414,What are the chances that nobody speaks English in a group of 50 people in a country where 25% of the population does?,"Can anyone tell me how I should calculate this problem? You are on vacation in a foreign country, and you need help. There are 50 people nearby, and you know 25% of the population of this country speaks English . If you were to call for help: (a) What is the probability any of them would understand ? (b) The probability that they will all understand? (c) What's the probability that none will understand what you're saying ? I've got a feeling this falls under the realm of combinatorics but I don't know how to calculate it. A hand anyone? EDIT: This problem wasn't given by a professor, it's just a question that came up while chatting with friends and is being asked out of simple curiosity (actually to settle an argument, but...) Also I'm not a native English speaker, so that's perhaps why the question wasn't clear enough. For the sake of simplicity, asume the sample group is made of random people, all equally likely to speak the language or not. My friend Jerome insists that despite what size the groups is, the chances are still 3 to 1 that no one speaks English, but I differ and gave him the example of 100 coin tosses always resulting in tails. I understand the basic of combinatorics but am in no way a maths expert. So put simply is best. You know, you don't understand it if you can't explain it simply! Thank you all for taking the time to answer!","['combinatorics', 'statistics', 'probability']"
2344430,Baby Rudin: Theorem 3.20 (c) and (d),"In theorem 3.20, Rudin offers a proof to the limits of the following sequences: $$u_n = n^{\frac{1}{n}}$$ $$v_n = \frac{n^{\alpha }}{(1+p)^n}$$ with $\alpha$ a real number and $p > 0$ . In both the proofs, Rudin uses inequalities which I am not familiar with. For $u_n$ , the following inequality is used: $$(1+x)^n \ge \frac{n(n-1)}{2}x^2$$ For $v_n$ : $${n \choose k}p^k > \frac{n^kp^k}{2^kk!}\quad \quad(n>2k) $$ I have painstakingly come up with proofs for both of those inequalities, however, and this is especially true for the second inequality, my proofs are not very elegant nor do they give me any idea of the inequality came to be. Surely Rudin doesn't pull these inequalities out of thin air. Where can I find a more comprehensive derivation of these inequalities? Where do they come from exactly (which field)? Update : Here are my proofs. They both rely on induction.
For the first one: Suppose $x\ge0$ . Then, for $n = 2$ : $$ (1+x)^2 = 1+2x+x^2 $$ $$\frac{2(2-1)}{2} x^2 = x^2$$ Since $x\ge0$ , we have $(1+x)^n \ge \frac{n(n-1)}{2}x^2 $ for n = 2. Suppose our inequality is true for a certain n. Then: $$  (1+x)^n + nx^2 \ge \frac{n(n+1)}{2}x^2$$ Let $D = (1+x)^{n+1} - (1+x)^n - nx^2 $ . Since $\forall n \in \mathbb{N}, \forall m = 0,1, ... n, {n+1 \choose m} \ge {n \choose m} $ , and ${n+1 \choose 2} - {n \choose 2} - n = 0$ , we have: $$D \ge 0 \Rightarrow (1+x)^{n+1} \ge (1+x)^n + nx^2$$ So: $$(1+x)^{n+1} \ge \frac{n(n+1)}{2}x^2$$ The theorem follows by induction. For the second inequality:
We want to prove that $n(n-1)...(n-k+1) > \frac{n^k}{2^k}$ , with $n > 2k$ Let $n\in\mathbb{N}, k$ and integer greater than 0 such that $2n > k$ . Then for $k=1$ , we have: $$n > \frac{n}{2} $$ Now suppose our inequality holds for a certain $k$ . Then: $$n(n-1)...(n-k+1)(n-k) > \frac{n^k}{2^k}(n-k)$$ Since $n > 2k$ , $n-k > \frac{n}{2} > 0$ such that: $$n(n-1)...(n-k+1)(n-k) > \frac{n^k}{2^k}(n-k) > \frac{n^{k+1}}{2^{k+1}} $$ By induction, our inequality holds for all $k$ such that $n > 2k$","['inequality', 'binomial-theorem', 'binomial-coefficients', 'calculus', 'sequences-and-series']"
2344446,"How many miles will Jane be from her starting point if she walks north for $3$ miles, turns $45^\circ$ to the right, then walks another $4$ miles?","If Jane walks north for $3$ miles, turns $45^\circ$ to the right, and then walks another $4$ miles, how many miles will Jane be from her starting point? Give your answer as a decimal rounded to the nearest hundredth. (You may use a calculator to compute the approximation.) So I used the law of cosines to get $2.83$ as my final answer. But, it said I was wrong so please explain if my math is wrong. $$n^2 = 3^2+4^2 - 2 \cdot 3 \cdot 4 \cdot \cos(45^\circ)$$ Thanks for any help!!","['algebra-precalculus', 'trigonometry']"
2344450,Understanding the mathematics behind backpropagation,"I have just finished watching a lecture from Patrick Winston's AI course . In an attempt to understand the mathematics behind back-propagation, I have formulated a simplistic neural network as follows: Task Given an input $x \in \{0, 1\}$, train a simple neural network to mimic the identity function, that is $f(x) = x$. Definitions $x$ = the input $T$ = the target function; in this case $T(x) = x$ $y$ = the desired output; $y = T(x)$ $A$ = the activation function; I'll use $A(x) = \frac{1}{1 + e^{-x}}$ $\hat{T}$ = the feed-forward function $\hat{y}$ = the output of the network; $\hat{y} = \hat{T}(x)$ $E$ = the error function; I'll use $E(\hat{y}, y) = \frac{1}{2}(\hat{y} - y)^2$ $w$ = a weight $\in [0, 1]$ $\alpha$ = the learning rate Note that $\hat{T}(x) = A(wx)$ in this very simplistic example. Formulating back-propagation via gradient descent In a particular iteration $i$ of this network, let $x_i \in \{0, 1\}$ and $\hat{y}_i = \hat{T}(x_i)$. Now, the weight $w_i$ of this network needs to be adjusted such that $E(\hat{y}_i, y_i) > E(\hat{y}_{i+1}, y_{i+1})$. So, $\frac{\partial{E}}{\partial{w_i}}$ will give the rate of change function of $E$ with respect to $w_i$. Computing this partial:
\begin{align}
\tag{1}\frac{\partial{E}}{\partial{w_i}} &= \frac{\partial{E}}{\partial{\hat{y}_i}} \frac{\partial{\hat{y}_i}}{\partial{w}_i} + \frac{\partial{E}}{\partial{y_i}} \frac{\partial{y_i}}{\partial{w}_i}\\
\tag{2}&= (\hat{y}_i - y_i) \cdot \frac{\partial{\hat{y}_i}}{\partial{w}_i} - (\hat{y}_i - y_i) \cdot 0\\
\tag{3}&= (\hat{y}_i - y_i) \cdot \frac{\partial{\hat{y}_i}}{\partial{w}_i}\\
\tag{4}&= (\hat{y}_i - y_i) \cdot \frac{\partial{\hat{T}}}{\partial{w}_i}\\
\tag{5}&= (\hat{y}_i - y_i) \cdot \frac{\partial{A(w_ix_i)}}{\partial{w}_i}\\
\tag{6}&= (\hat{y}_i - y_i) \cdot x_i \cdot (1 - A(w_ix_i)) \cdot A(w_ix_i)
\end{align} The confusion How does finding $\frac{\partial{E}}{\partial{w_i}}$ help in the gradient descent algorithm in this case? I only understand that the gradient of a function at a point returns a vector pointing in the direction of the greatest incline. How can the weight $w_i$ be updated in such a manner?","['multivariable-calculus', 'neural-networks', 'gradient-descent']"
2344457,Notation for a sub-sub-sequence,"In Real Mathematical Analysis by Charles Pugh, sequences are denoted using the standard notation $(a_n)$, which represents $(a_1, a_2, \dots)$. Furthermore, sub-sequences are denoted by $(a_{n_k})$ which expands to $(a_{n_1}, a_{n_2},\dots)$ i.e. $n_k \in \mathbb{N}$ indicates the index (from the mother sequence) of the $k$-th term of the sub-sequence. These two notations are thus far standard. Finally, the confusion arises with the notation of sub-sub-sequences, which are denoted by $(a_{n_{k(l)}})$. The expansion seems to be on the $l$ variable, so that $(a_{n_{k(l)}}) = (a_{n_{k(1)}}, a_{n_{k(2)}}, \dots)$. If this expansion is correct, what is $n_{k(1)}$ supposed to indicate? Is $k$ now a function, where $n_{k(i)} \in \mathbb{N}$ denotes the index of the $i$-th term in the sub-sub-sequence (from the mother sequence, or grandmother sequence)? Is this notation standard?","['real-analysis', 'sequences-and-series', 'notation']"
2344529,Separating convex sets in Vector spaces,"This question just popped on my mind. Let $A, B$ two disjoint, nonempty convex sets in the vector space $X$ , can they be separated via a nonzero linear function in $X' = \{ f : X \to R  ~ | \quad \text{f is linear}  \} ?$ i.e., does there exist $f \in X' \setminus \{ 0\}$ such that $$          f(a) \leq   f(b)   \quad \forall a\in A, ~ \forall b \in B            $$ If not under what minimal condition one can separate them. My Thought  :  Since $A \cap  B = \emptyset  $ using Zorn Lemma we can find two disjoint maximal convex sets, say $U, ~ V$ such that $  A \subseteq U, ~ B \subseteq V $ and through maximality of $U, V$ we can deduce that $U \cup V = X$ in other words $U,~ V$ make a convex partition of the space. Now from this,  can we say that $U, ~V$ are two sides of a hyperplane ?  i.e., $$ U \subseteq \{ x \in  X ~ | \quad  f(x) \leq \alpha  \} , ~ V \subseteq \{ x \in  X ~ | \quad  f(x) \geq \alpha \} $$ for some $f \in X'$ and $\alpha \in \Bbb R$ Question #2: What if we assume $A, B$ are pointed cones with $A \cap B = \{0\}$ EDIT: I realized the answer of question # 1 is No generally see below link Can any two disjoint nonempty convex sets in a vector space be separated by a hyperplane? But Still any answer regarding minimal conditions that guarantees separation is my main interest, and an answer for question #2. Thank for your help.","['functional-analysis', 'real-analysis', 'convex-analysis']"
2344579,Measurability of limit function into a Polish space,"The exercise is to show that if $(X,\mathscr A)$ is a measurable space, and $f_n:X\to Y$ where $Y$ is Polish, $n\in \mathbb N,$ and $f_n$ are measurable, then $f=\lim f_n$ is measurable (presumably with respect to $\mathscr A$ and $\mathscr B(Y)$; this is not specified). The hint is to prove that $L=\left \{ x:\lim f(x)\ \text {exists} \right \}$ is contained in $\mathscr A$. Here is my attempt. I am pretty sure of the first part, but there is a problem with the second. Define for $i,j\in \mathbb N,\ F_{ij}(x)=(f_i(x),f_j(x)).$ Since $\mathscr B(Y\times Y)=\mathscr B(Y)\times \mathscr B(Y)$ because $Y$ is Polish, the $F_{ij}$ are measurable. It follows then that $d\circ F_{ij}$ are measurable, where $d$ is the complete metric on $Y.$ It now follows that for fixed $i,j,n$, the sets $A^{n}_{ij}:=\left \{ x:d(f_i(x),f_j(x))<1/n \right \}$ are measurable, so that $L=\cap_{n}\cup_k\cap_{i\ge k}\cap_{j\ge k}A^{n}_{ij}\in \mathscr A.$ To show that $f$ is measurable, first note that the trace $\mathscr S=\left \{ A\cap L:A\in \mathscr A \right \}$ is a sigma-algebra so now if take an arbitrary continuous function $g:Y\to \mathbb R$ and observe that for $x\in L$ we have  $g\circ f=\lim g\circ f_n=\limsup g\circ f_n,\ $ from which it follows that $f$ is measurable with respect to $\mathscr S$ and $\mathscr B(Y)$, but this is not exactly what we want.","['real-analysis', 'measure-theory']"
2344589,Show that $\int _0^1\frac{\ln \left(1+\left(\frac{1-t}{1+t}\right)^2\right)}{t\left(\ln t\right)^2}dt=\ln 2$,"According to WolframAlpha the integral 
$$\int _0^1\frac{\ln \left(1+\left(\frac{1-t}{1+t}\right)^2\right)}{t\left(\ln t\right)^2}dt$$
is shown to have a decimal expansion exactly identical to $\ln 2$. How can we prove they are equal? This integral came about as a result of integrating by parts $$\int_0^x\frac{\arctan t}{\ln \left(t\right)\left(1+t\right)^2}dt$$ with $u=\frac{1}{\ln t}$ $dv=\frac{\arctan t}{\left(1+t\right)^2}dt$ $du=-\frac{1}{t\ln^2 t}dt$ $v=\frac{1}{4}\ln \left(2\right)-\frac{1}{4}\ln \left(1+\left(\frac{1-t}{1+t}\right)^2\right)-\frac{1}{2}\left(\frac{1-t}{1+t}\right)\arctan \left(t\right)$",['integration']
2344699,"Compute $S_n=\sum\limits_{a_1,a_2,\cdots,a_n=1}^\infty \frac{a_1a_2\cdots a_n}{(a_1+a_2+\cdots+a_n)!}$","It is tagged as an open problem in the book Fractional parts,series and integrals. If this proof is valid , I don't have any idea how to get it published so I posted it here . $\displaystyle \sum_{a_1,a_2,\cdots,a_n=1}^\infty \frac{a_1a_2\cdots a_n}{(a_1+a_2+\cdots+a_n)!} = \; ?$ I am posting a proof for the closed form of the above series, please let me know if there are flaws,I came by some special cases of the above sum, that is for the case of $2$ & $3$ variables. They are . $$ \displaystyle \sum_{a=1}^\infty \sum_{b=1}^\infty \frac{ab}{(a+b)!} \;=\;\frac{2}{3}e $$ $$ \displaystyle \sum_{a=1}^\infty \sum_{b=1}^\infty\sum_{c=1}^\infty \frac{abc}{(a+b+c)!} \;=\;\frac{31}{120}e $$ This led me to solve the general version of the sum for any number of variables,So if $S$ is our sum then,
$$\displaystyle \begin{align} S &= \sum_{k=n}^\infty\frac{1}{k!}\;\left( \sum_{a_1+a_2+\cdots+a_n=k}a_1 a_2\cdots a_n\right) \end{align}$$ This was achieved by setting $\sum_{i=1}^n a_i =k$, and what remains to calculate is the inner sum enclosed by brackets. We start by investigating the lower cases , suppose we have only two variables $a_1,a_2$ with $a_1+a_2=k$ then $$\displaystyle \sum_{a_1+a_2=k}(a_1 a_2) =\sum_{N=1}^{k-1} N(k-N)=\frac{k(k-1)(k+1)}{3!}=\binom{k+1}{3}$$ Now if we take the case of $3$ variables where $a_1+a_2+a_3=k$ , we can achieve the sum as : $$ \displaystyle \sum_{a_1+a_2+a_3=k} a_1 a_2 a_3 = \sum_{N=1}^{k-2} N\binom{k+1-N}{3}= \frac{k(k-1)(k+1)(k-2)(k+2)}{5!}$$ Similarly for $4$ variables it turns out to be , $$ \displaystyle \sum_{a_1+a_2+a_3+a_4=k}a_1 a_2 a_3 a_4 = \frac{k(k-1)(k+1)(k-2)(k+2)(k-3)(k+3)}{7!} $$ I believe for the same reason that that , $$ \displaystyle \sum_{a_1+a_2+\cdots+a_n=k}a_1 a_2\cdots a_n = \frac{k}{(2n-1)!}\prod_{m=1}^{n-1}(k-m)(k+m)$$ This is indeed tough to prove by induction , but I guess it can be proved due to the great symmetry and pattern this sequence follows. I haven't tried but will try to update a proof on this asap, but till then it's reasonable to conjecture this. Lastly we have that , $$\displaystyle \begin{align} S &= \sum_{k=n}^\infty \frac{1}{k!} \left(\frac{k}{(2n-1)!}\prod_{m=1}^{n-1}(k-m)(k+m)\right) \\ &= \frac{1}{(2n-1)!}\sum_{k=n}^\infty \frac{1}{k.k!} (k)_n (k)^n \\ &= \frac{1}{(2n-1)!}\sum_{k=n}^\infty \frac{1}{k.k!} \left(\sum_{r=1}^{n}s(n,r)k^r\right) \left(\sum_{t=1}^n {n\brack t}k^t\right) \\ &= \frac{1}{(2n-1)!}\sum_{r,t=1}^n (-1)^{n+r} {n\brack r}{n\brack t}\left(\sum_{k=n}^\infty \frac{k^{r+t-1}}{k!}\right) \end{align}$$ Now using Dobinski's Formula we have finally, $$\displaystyle \sum_{a_1,a_2,\cdots,a_n=1}^\infty \frac{a_1a_2\cdots a_n}{(a_1+a_2+\cdots+a_n)!}\\ = \frac{1}{(2n-1)!}\sum_{r=1}^n\sum_{t=1}^n (-1)^{n+r} {n\brack r}{n\brack t} \left[eB_{r+t-1}-\sum_{m=1}^{n-1}\frac{m^{r+t-1}}{m!}\right] $$ where $B_n$ is the n-th Bell Number. Edit: After some investigation it was clear that the constant term in the final closed form which always disappeared whenever you calculate the sum for a specific $n$ and you are left with a rational multiple of $e$ was no magic but some logic. I proved it by induction. Firstly, if we separate the answer into two parts and take the constant term which doesn't have $e$ , we get $$\displaystyle \sum_{r=1}^n \sum_{t=1}^n \sum_{m=1}^{n-1} (-1)^{n-r}{n\brack r}{n\brack t}\frac{m^{r+t-1}}{m!} $$ A little modification and interchange of sums will give the result in terms of the Pochammer symbol. $$ \displaystyle \sum_{m=1}^{n-1} \frac{(m)_n m^{t-1}}{m!} =0$$ This sum is eventually equal to zero and is easy to prove by induction. Thus the answer is  : $$\displaystyle \sum_{a_1,a_2,\cdots,a_n=1}^\infty \frac{a_1a_2\cdots a_n}{(a_1+a_2+\cdots+a_n)!}\\ = e\left[\frac{1}{(2n-1)!}\sum_{r=1}^n\sum_{t=1}^n (-1)^{n+r} {n\brack r}{n\brack t} B_{r+t-1}\right]$$","['stirling-numbers', 'bell-numbers', 'combinatorics', 'summation', 'open-problem']"
2344715,Induced Riemannian metric,"I'm new to differential geometry and have only begun reading about Riemannian metrics. I have the following problem from Jost. We equip $\mathbb{R}^{n+1}$ with the inner product $$\langle x,y \rangle = -x^0 y^0 + x^1y^1 + \cdots + x^ny^n$$ for $x = (x^0, x^1, ..., x^n)$ and $y= (y^0, y^1, ..., y^n)$. Set $$H^n = \{ x \in \mathbb{R}^{n+1} \ : \ \langle x,x \rangle = -1, \ x^0 > 0 \}.$$ I want to show that $\langle \cdot, \cdot \rangle$ induces a Riemannian metric on the tangent spaces $T_p H^n \subset T_p \mathbb{R}^{n+1}$ for $p \in H^n$. If there are any surrounding comments that you would like to add for pedagogical purposes that would be greatly appreciated. I have a fair background in manifolds but mainly from the perspective of complex analysis.","['manifolds', 'riemannian-geometry', 'differential-geometry']"
2344720,"Proof verification - $f'(x)$ is increasing on $(0,1)$ $\implies$ $\frac{f(x)}{x}$ is increasing on $(0,1)$ under specified conditions","I'm stating the problem statement and my solution to it. I'd appreciate if someone checks if there's any gap in my arguments and whether the proof could be made shorter. Any other suggestions (regarding proof-writing style or other details) are also welcome. Thank you. The Problem : Let $f$ be continuous on $[0,1],$ differentiable on $(0,1)$ with $f(0)=0$. To show that if $f'$ is increasing on $(0,1),$ then $\frac{f(x)}{x}$ is increasing in $(0,1)$. My Solution : Let $x,y$ be arbitrary such that $0<x<y<1$. Since $f$ is continuous in $[0,1]$ and is differentiable in $(0,1),$ it is continuous in $[a,b]$ and differentiable in $(a,b)$ for any $a,b$ such that $0<a<b<1,$ enabling us to apply MVT on $f|_{(a,b)}$ Applying MVT on $f|_{[0,x]}$ we see that $\exists c \in (0,x)$ such that $f(x)-f(0)=f'(c)(x-0)$. Since $f(0)=0,$ we have $\frac{f(x)}{x}=f'(c)$ Applying MVT on $f|_{[x,y]}$ we see that $\exists d \in (x,y)$ such that $f(y)-f(x)=f'(d)(y-x)$ Note that $0<c<x<d<y<1$. Now, $\frac{f(y)}{y}-\frac{f(x)}{x}=\frac{f(x)+f'(d)(y-x)}{y}-\frac{f(x)}{x}=f(x)\Big(\frac{1}{y}-\frac{1}{x}\Big)+f'(d)\Big(\frac{y-x}{y}\Big)$ $=f(x)\Big(\frac{x-y}{xy}\Big)+f'(d)\Big(\frac{y-x}{y}\Big)=\Big(\frac{y-x}{y}\Big)\Big(f'(d)-\frac{f(x)}{x}\Big)=\Big(\frac{y-x}{y}\Big)\Big(f'(d)-f'(c)\Big)$ Since $f'$ is increasing in $(0,1),$ and $0<c<d<1,$ we have $f'(c)<f'(d)$ Thus $\frac{f(y)}{y}>\frac{f(x)}{x}$. Since $x,y$ are arbitrarily chosen, we have $\frac{f(x)}{x}<\frac{f(y)}{y},$ for all $x,y$ such that $0<x<y<1$. Hence $\frac{f(x)}{x}$ is increasing in $(0,1)$. $\blacksquare$","['derivatives', 'real-analysis', 'proof-verification']"
2344723,Derivation of $\sigma^2 = \frac1N\sum x^2 - \frac1{N^2}\sum\bar{x}^2$,"I saw the above equation in an introductory statistics textbook, as a shortcut for evaluating the variance of a population. I tried to prove it myself: $$\sigma^2 = \frac{\sum (x - \bar{x})^2}{N} \tag{1}$$
$$\sigma^2 = \frac{\sum x^2 - \frac{(\sum x)^2}{N}}{N} \tag{2}$$
We are given that $(1) = (2)$:
$$\frac{\sum (x - \bar{x})^2}{N} = \frac{\sum x^2 - \frac{(\sum x)^2}{N}}{N} \tag{3}$$
Multiply $(3)$ through by $N$:
$$\sum(x - \bar{x})^2 = \sum x^2 - \frac{(\sum x)^2}{N} \tag{4}$$
Expand the LHS in $(4)$:
$$\sum\left(x^2 - 2x\bar{x} + \bar{x}^2\right) = {\sum x^2 - \frac{(\sum x)^2}{N}} \tag{5}$$
Expanding both sides in $(5)$:
$$\sum x^2 - 2x\sum\bar{x} + \sum\bar{x}^2 = \sum x^2 - \frac{\sum x\sum x}{N} \tag{6}$$
From $(6)$:
$$\sum\bar{x}^2 - 2\bar{x}\sum{x} = -\bar{x}\sum{x} \tag{7}$$
From $(7)$:
$$\sum\bar{x}^2 = \bar{x}\sum{x} \tag{8}$$ I don't know how to make the LHS equal RHS in $(8)$.","['statistics', 'standard-deviation']"
2344771,Probability of a random selection of $2$ out of $5$ shoes,"From a set of five pairs of shoes, two of the shoes are selected at random. Find the probability of each of the following: a) Both are from the same pair. My answer: $\dfrac{\binom{10}{1}\binom{1}{1}}{\binom{10}{2}}$ b) One left shoe and one right shoe are selected. My answer: $\dfrac{\binom{10}{1}\binom{5}{1}}{\binom{10}{2}}$ Note: Solve this problem by counting combinations (i.e $\binom{n}{k}$ ). Can someone check if this is correct? If not, can you explain why it is incorrect?","['combinations', 'combinatorics', 'probability', 'discrete-mathematics']"
2344819,"Seven coins are flipped. Find the probability that three land on one side, four on the other.","I'm a little confused as to which answer is correct or if either are correct. Answer #1: $\frac{\binom{7}{3}}{2^{7}}$ 7 choose 3= number of ways three land on one side, $2^{7}$= outcomes of coin toss Answer #2: $\frac{\binom{7}{3}+\binom{4}{4}}{2^{7}}$ 7 choose 3= number of ways three coins land on one side,4 choose 4= number of ways four coins land on the other, and  $2^{7}$= outcomes of coin toss Can someone please double check my results? Thank you!","['combinations', 'combinatorics', 'probability', 'discrete-mathematics']"
2344840,Convergence of a series in a Hilbert space,"Let $\{y_k:k=1,2,\dots\}$ be an orthonormal set in a Hilbert space $\mathcal{H}$. Give and prove the necessary and sufficient conditions for the scalar sequence $\{\lambda_k\}$ such that  the series $$\sum_{k=1}^\infty\lambda_k y_k$$
converges. What is the norm of the sum of the series then? This is my preparatory task before an exam, but I'm struggling with the first step, could you explain in detail what I'm supposed to do and what theorems to make use of?","['functional-analysis', 'normed-spaces', 'orthonormal', 'hilbert-spaces']"
2344862,Random elements and random processes,"Suppose that $X$ is a random element with values in $L^2([0,1],\mathbb C)$ such that $\operatorname E\|X\|<\infty$. The expected value of $X$ is equal to $\mu(t)=\operatorname EX(t)$ for almost all $t\in[0,1]$. So we have a random element, but we actually want to have a random process, i.e. a collection of random variables, to evaluate the mean.  How can we define the random process $\{X(t):t\in[0,1]\}$? Strictly speaking, the space $L^2([0,1],\mathbb C)$ consists of equivalence classes of almost everywhere equal square integrable complex functions. So it does not make sense to consider the value of $f\in L^2([0,1],\mathbb C)$ at some point $t\in[0,1]$ since this value is not defined. However, if we want to define the random variable $X(t)$ for some $t\in[0,1]$ and evaluate its expected value $\operatorname EX(t)$, we need to assign a complex number $X(t,\omega)$ for each $\omega\in\Omega$ (and make sure that this map is measurable). So it seems that it is not even possible to define the random process $\{X(t):t\in[0,1]\}$ because we do not have information about the values of this function at point $t\in[0,1]$. However, I have seen a few times when the expected value of a random element with values in $L^2([0,1],\mathbb C)$ is evaluated pointwise, i.e. $\operatorname EX(t)$ for $t\in[0,1]$, so maybe it is possible to define the random process $\{X(t):t\in[0,1]\}$ in a sensible way. Any help is much appreciated!","['expectation', 'hilbert-spaces', 'probability-theory', 'functional-analysis', 'measure-theory']"
2344871,Best algebraic approximations,"Continued fractions are a well-known method for producing rational approximations to given real numbers, which can be shown to be optimal with an appropriate optimality measure. Does there exist a similar method for producing a best algebraic approximation to a given real (or complex) number? As with rational numbers, the algebraic numbers are dense in the complexes, so it is important to bound the ""complexity"" of the approximation if one hopes to get a definite answer, but since the approximation method will dictate this choice to some degree, I will be vague about the actual complexity measure.","['number-theory', 'transcendental-numbers', 'algebraic-number-theory', 'approximation']"
2344894,Continued Fraction and Random Variable,"Let $X_1,X_2,\dots$ are independent r.v. such that $P(X_i=1)=p=1-P(X_i=\epsilon_i)$ , $0<\epsilon_i<1$ $$Y=X_1+\frac{X_1}{X_2+\frac{X_2}{X_3+\frac{X_3}{X_4+\dots}}}$$ 1.What is the distribution of $Y$ ? 2.What is the characteristic  function of $Y$ ? [If you have read up to this line you must go to ""Added"" part in the end of this post. Starting with $X_i$ 's are i.i.d. would be less complicated] I do not know how to find these type of problem, so I had to post it. Any idea or website link would be helpful. When I thought about it it was very easy for $X_i$ 's are same, i.e. $$Y^*=X_1+\frac{X_1}{X_1+\frac{X_1}{X_1+\frac{X_1}{X_1+\dots}}}$$ In this case $Y^*=\frac{X_1+\sqrt{X^2_1+4X_1}}{2}$ , which is not very interesting. In the end I think it will need condition on $\epsilon_1,\epsilon_2,\dots$ to $Y$ be a random variable. But these are very trivial observation. What can we say about 1 and 2 and what can we say more than 1 and 2 about $Y$ ? Added: As @Henry said we can think of less complicated version: Let $X_1,X_2,\dots$ are i.i.d. r.v. such that $P(X_1=1)=p=1-P(X_1=\epsilon)$ , $0<\epsilon<1$ $$Y=X_1+\frac{X_1}{X_2+\frac{X_2}{X_3+\frac{X_3}{X_4+\dots}}}$$ Here we can write as follows: $Y=Y_1$ . $Y_1=X_1(1+\frac{1}{Y_2})$ , $Y_2=X_2(1+\frac{1}{Y_3}),\dots$ It can be easily seen that $Y_i$ 's are identical but not independent. So first we should find question 1, 2 for this case. Added: Some reference I found here in Mathoverflow . This may help.","['probability-theory', 'continued-fractions', 'random-variables']"
2344900,Green's Theorem in polar coordinates - what am I doing wrong?,"Let $$\vec F = M\hat i + N\hat j = (3x^2y^2)\hat i + (2x^2 + 2x^3y)\hat j$$
be a vector field, and C be the counterclockwise circle centered at $(a,0)$, with a radius of $a$. Find $\oint_C M\,dx + N\,dy$. Applying Green's Theorem: 
$$\oint_C M\,dx + N\,dy = \iint_R N_x-M_y\,dA = 4\iint_R x\,dx$$ Using center of mass with uniform density in place of direct computation:
$$4\iint_R x\,dx=4\frac{\iint_R x\,dx}{\iint_R\,dx}\iint_R\,dx = 4 \bar x *Area(R) = 4\pi a^3$$
The problem I'm having is when I try to do the double integral by direct computation in polar coordinates. I've checked the intermediate steps with an integral calculator to make sure there wasn't a simple calculation error, which there wasn't, so I'm guessing something is wrong with the setup. The first thing I did was get an expression for $x$ and $y$ in terms of $\theta$. I did this by noting that, for any ray coming from the origin, it will intersect the circle and form an isosceles triangle, where the first leg of length $a$ is coincidental with the $x$ axis, and the second leg, call it $l$, is the line segment of length $a$ going through the circle's origin and the point of intersection. From this, the angle between the two legs should be $\pi-2\theta$, since there are two equivalent $\theta$s, and the sum of the angles in a triangle is $\pi$ radians. The angle $\alpha$ between $l$ and the $x$ axis is $\pi - (\pi - 2\theta)$, since the angle between the two legs of the isosceles triangle plus the angle between $l$ and the $x$ axis should be $\pi$. This makes it easy to get an expression for $x$ and $y$ using vectors. If the circle were at the origin, then we would have $\vec r = (a\,cos(\alpha),a\,sin(\alpha)=(a\,cos(2\theta), a\,sin(2\theta)$, to which we add the offset vector $(a,0)$ to obtain $\vec r = (a(1+cos(2\theta), a\,sin(2\theta)$. I did this another way to confirm, by using the law of cosines, and it seems correct from that standpoint as well. If the angle between the two sides of the isosceles triangle is $\pi - 2\theta$, then $r$ should be $\sqrt{2a^2(1-cos(\pi - 2\theta)}$, or equivalently, $\sqrt{2a^2(1+cos(2\theta))}$, since $cos(\pi-x)=-cos(x)$. Then, the unit vector extending in the direction of the ray from the origin is $\hat u = (cos(\theta), sin(\theta))$, and the ray should be $r \hat u$. I graphed both of these representations for the $x$ coordinate, namely, $x=a(1+cos(2\theta))$ and $x=cos(\theta) \sqrt{2a^2(1+cos(2\theta))}$, and they are equivalent on $-\frac{\pi}{2} \le x \le \frac{\pi}{2}$. Finally, I set up the integral:
$$\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}} \int_0^{\sqrt{2a^2(1+cos(2\theta))}}r(4a(1+cos(2\theta)))\,dr\,d\theta$$
Evaluating the inner integral leaves:
$$\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}} 4a^3(1+cos(2\theta))^2\,d\theta = 6 \pi a^3$$
Interestingly, if I evaluate this integral without the square, i.e.,
$$\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}} 4a^3(1+cos(2\theta))\,d\theta$$
then I get the correct answer, although I have no clue at this point how I would get such an expression. Any insight would be much appreciated.","['multivariable-calculus', 'greens-theorem', 'integration', 'polar-coordinates']"
2344916,"Geometric series problem, can not get the correct result?","The number of citizens is rising in the town by the same / equal percentage each year in contrast to the previous year.
In 6 years time, the number of citizens rose from 1 635000 to 2 010000. 
What is the percentage of growth each year equal to? My solution: $$a_6=a_1*q^{n-1}; n=6$$
$$2 010000=1635000*q^5 => q=\sqrt[\leftroot{-2}\uproot{5}5]{\frac{2010000}{1635000}}$$
and then I get: $$q=1.042$$ Can anyone help on this one.. The result should be 3.5%","['algebra-precalculus', 'power-series', 'sequences-and-series', 'geometric-series']"
2344931,"If $a,b$ are roots of $3x^2+2x+1$ then find the value of an expression","It is given that $a,b$ are roots of $3x^2+2x+1$ then find the value of:
$$\left(\dfrac{1-a}{1+a}\right)^3+\left(\dfrac{1-b}{1+b}\right)^3$$ I thought to proceed in this manner: We know $a+b=\frac{-2}{3}$ and $ab=\frac{1}{3}$. Using this I tried to convert everything to sum and product of roots form, but this way is too complicated! Please suggest a simpler process.","['algebra-precalculus', 'roots', 'quadratics']"
2344952,Representation of arbitrary probability distributions by latent variable models with gaussian posteriors and priors,"A probability distribution of a random variable $\mathbf x$ may be expressed as $ P(x)=\int P(x|z) P(z) dz$ where $\mathbf z$ is a latent random variable. In models such as Variational Autoencoders (VAEs) it is common to let $\mathbf z \sim \mathcal{N}(0,I)$ and $P(x|z) = \mathcal{N}\left(x;\mu(z),\Sigma(z)\right)$. However, it is not obvious to me whether those kind of models are able to represent any arbitrary probability distribution $P(x)$ by varying $\mu(z)$ and $\Sigma(z)$, and if that is the case, why it is possible. Do you have any intuition or references where this is proven? Thank you very much","['machine-learning', 'statistics', 'statistical-inference', 'probability-distributions']"
2344971,A question on graphs with all vertices are either central or peripheral,"If there are only two central vertices and rest are peripheral. Can we deduce that central vertices are always
adjacent? I got following examples (eccentricity mentioned) from where I proposed this. Is there any counter example for this?
Any hint or suggestion is hearty welcome. Thanks for the help. One more example in support :","['combinatorics', 'graph-theory', 'discrete-mathematics']"
2344977,"Zeta function identity, formal justification","We have the following identity: (where $\mu$ is Mobius function)
$$\frac{\zeta(s)}{\zeta(2s)} = \sum_{n=1}^{\infty} \frac{|\mu(n)|}{n^s} \quad (\sigma > 1, s = \sigma+it )$$ The proof I read (Titchmarsh p5) is given by the following line: $$\frac{\zeta (s) }{ \zeta(2s) } \stackrel{*}{=} \prod_p \frac{1-p^{-2s}}{1-p^{-s}} = \prod_p \big( 1 + \frac{1}{p^s} \big) \stackrel{**}{=} \sum_{n=1}^\infty \frac{|\mu(n)|}{n^s}$$ My question is how do we justify $*$ and $**$ rigorously? Below is what I think: $(*):$ We note that the partial products, $\hat{f_n} = f_1 \cdots f_n$ where $f_i = \big( 1 - \frac{1}{p_i^s} \big)^{-1}$ converges compactly in region $\sigma >1$. $\zeta(s) = \lim_{n \rightarrow \infty} \hat{f}_n(s)$ (by Euler Product formula). Similarly, $\zeta(2s) = \lim_{n \rightarrow \infty} \hat{g}_n(s)$ where $\hat{g}_n = \hat{f}_n (2s)$. We note that $|\zeta(s)| > 0$ for all $\sigma >1 $. $\frac{1}{\zeta(2s)} = \lim_{n \rightarrow \infty} \frac{1}{\hat{g}_n(s)}$ is well-defined, and so by basic algebra of limits, 
$$\frac{\zeta(s) }{ \zeta(2s)} = \lim_{n \rightarrow \infty} \frac{\hat{f}_n(s)} { \hat{g}_n(s) } $$ $(**):$ Here we argue by taking a prime $P$, and we have, $$ \Big| \sum \frac{|\mu(n)| }{n^s} - \prod_{p \le P} (1+ \frac{1}{p^s} ) \Big| \le \sum \frac{1}{(P+n)^s} \rightarrow 0 $$
as $P \rightarrow \infty$. In general, when do we know we can just expand infinite products like this? For instance, we also have the identity: $$ \frac{\zeta^2(s)}{\zeta(2s)} = \sum \frac{2^{v(n)}}{n^s} $$ 
the proof then utilizes infinite product of infinite sums: $$ \frac{\zeta^2(s)}{\zeta(2s)}  = \prod_p (1 + 2p^{-s} + 2p^{-2s} + \cdots ) = \sum \frac{2^{v(s)}}{n} $$ How do we know the last two equalities coincide?","['complex-analysis', 'infinite-product', 'riemann-zeta', 'zeta-functions']"
2345045,Flux through sphere,"I wish to find the flux of $\mathbf{F}=(x^2,y^2,z^2)$ through $S: (x-1)^2+(y-3)^2+(z+1)^2$ Here is what I tried: I ""moved"" the sphere to $(0,0)$ by changing the variables to: $u=x-1$ , $v=y-3$ , $w=z+1$ so now we have $F=((u+1)^2,(v+3)^2,(w-1)^2)$ and $S$ is the unit sphere. So my calculation is (after switching to polar): $$\int_0^{2\pi}\int_0^1\int_{-\sqrt{1-r^2}}^{\sqrt{1-r^2}}{\rm div}\,\mathbf{F}\,r\,{\rm d}z\,{\rm d}r\,{\rm d}\theta=\int_0^{2\pi}\int_0^1\int_{-\sqrt{1-r^2}}^{\sqrt{1-r^2}}(2r\cos\theta +2r\sin\theta +2z+6)r\,{\rm d}z\,{\rm d}r\,{\rm d}\theta$$ but I got $0$ instead $8\pi$ What did I do wrong?","['multivariable-calculus', 'integration', 'divergence-operator']"
2345086,"How to solve in integers the equation, $(x^2-y^2)^2=1+16y$?","How to solve in integers the equation,
$$(x^2-y^2)^2=1+16y$$ By observation we can take $y=3,5$ Is there another method to solve the equation?","['algebra-precalculus', 'diophantine-equations']"
2345094,How many non-isomorphic groups of infinite order $|X|$ are there?,"Let $X$ be an infinite set. What I'm looking for is a lower bound (or even better, precise cardinal number) of possible non-isomorphic group structures on $X$. Since every group is built on some function $X\times X\to X$ then there is at most $|X|^{|X\times X|}$ group structures on $X$. If I remember correctly this number is equal to $2^{|X|}$ for infinite $X$. What about a lower bound? What I have so far is: if $X$ is infinite then the free group $\mathbb{F}(X)$ generated on $X$ is of order $|X|$. Furthermore $G\times\mathbb{F}(X)$ is still of order $|X|$ if $|G|\leq |X|$. And if $G, H$ are finite then $G\times\mathbb{F}(X)\simeq H\times\mathbb{F}(X)$ if and only if $G\simeq H$. In particular there is at least countably many group structures on $X$. But this lower bound is quite bad. I suspect that there's at least $|X|$ group structures on $X$. And that would be a good enough lower bound for me. If the general case is too difficult then putting $X:=\mathbb{R}$ will be good as well. Can someone help me with the proof? Or give ma a hint at least?","['cardinals', 'group-theory']"
2345163,Sum-to-product formulas for the Weierstrass elliptic functions ( $\wp$ and $\wp^\prime$),"In the theory of trigonometric functions, the following identity is known
$$ \sin(u) + \sin(v)=2\sin \left( \frac{u+v}{2} \right) \cos \left( \frac{u-v}{2} \right) $$ There are other, similar-looking identities , known as the sum-to-product identities. Are there similar identities involving the Weierstrass elliptic functions , $\wp$ and $\wp^\prime$? More specifically, are there identities simplifying the expressions below?
$$\wp(u;g_2,g_3) \pm \wp(v;g_2,g_3) \qquad\qquad\wp'(u;g_2,g_3) \pm \wp'(v;g_2,g_3)$$ Thank you!","['trigonometry', 'elliptic-functions']"
2345170,What Is the Hardest Shape To Roll?,"I was having a discussion with a friend about rolling various shapes, in particular what shape is the worst at rolling.  I thought that a triangular wheel might be particularly bad at rolling while he suggested a thin rectangular wheel, approaching a plate, might be even worse as a wheel. We discussed this rather informally for a while, but we wanted to back it up with some maths, however we couldn't really think of a measure that captures the idea of ""How easily an wheel roll"".  So I thought I would ask some people that are better at maths.  What measure might be used to express the idea of being easy or hard to roll?  Is there any literature on rolling/wheel construction? To get it started a bit I'll cover all the ideas discussed between my friend and myself.  If these are unhelpful or off topic tell me and I'd be happy to edit them out. The only way we could think to measure how easy it is too roll was to put a shaped wheel on a flat surface with a very high coefficient of friction and tilt it until the wheel begins to roll. We would then take the angle as the measurement of roll-ability.  This seems pretty nice because it can be verified to some degree of accuracy empirically (however it seems quite difficult to get the conditions pristine enough that the shapes will roll rather than slide).  In addition circular wheels will begin to roll pretty much at angle 0, and everything will roll by the time the surface is vertical, this is pretty nice because it gives a finite range allowed.  The problem we saw with this is that it only measures the difficulty required to get the shape to begin to roll. For example in the plate idea suggested by my colleague takes quite a deal of effort to start up but once it begins to roll it will roll a half rotation quite easily because the thin edge is very unstable,  at the same time the triangular shape will probably take significantly less effort to start rolling, but once it moves it will only roll a 3rd of a rotation before returning to the start. An additional discussion we had was on what should be kept constant when comparing two shapes.  Our initial thought was to keep the cross sectional are of the wheels constant when comparing them, however as we thought about it more we thought it might be a good idea to compare shapes with equal perimeter , because shapes with larger perimeter tend to roll farther.  My friend pointed out however that concave shapes have perimeter that does not contribute to how far it rolls, we perhaps distance rolled per rotation would be a better measure.","['physics', 'geometry']"
2345175,"What is wrong with my proof: if $c^4$ is not divisible by 16, then c is odd?","Prove that if $c^4$ is not divisible by $16$, then $c$ is odd. Proof By contraposition, let $c$ be an even integer, such that $c=2k$ for some integer $k$. Then, by substitution, $(2k)^4 = 16j$, for some integer j, by the definition of divisibility. Then, $ (2k)^4 = 16j$ $ = 16k^4 = 16j$ $ = 16b = 16j$, where $b$ is an integer and $b=k^4$ Hence, by divisibility, $b=j$, which are both integers. Therefore, by contrapositivity, if $c^4$ is not divisible by $16$, then c is odd. What is wrong with this proof? This was the answer I wrote down on an exam and only got partial credit. Where am I going wrong? Did I assign too many variables? Thanks!","['proof-writing', 'discrete-mathematics']"
2345201,Calculating the price of product before increase in price by 30%?,"Problem to solve: To buy 2 products the seller gives 30% discount for the less expensive product. The customer/buyer payed for both products 300$. What is the biggest/max price of the less expensive product before the 30% discount? What I did: let x be the first product let y be the second product which is lowered by 30%, the less expensive 
product First equation: $x+y-(y*0.3)=300{$}$ Second equation ? I tried but just did a modification of the previous one which is incorrect. Also would derivatives apply here?
$$$$","['algebra-precalculus', 'percentages', 'elementary-number-theory']"
2345202,Computing Jacobian of matrix-valued map $X \mapsto X^TAX$,"Let $A = (a_{i,j})$ be a constant $n \times n$ matrix. Identify points $ (\vec{x}_1 , \dots, \vec{x}_n) \in R^{n^2}$, $\vec{x}_j =(x_{1,j}, \dots, x_{n,j}) \in \mathbb{R}^n$, $1 \le j \le n$,  with $n \times n$ matrices by arranging the vectors $\vec{x}_j$ into columns:
$$\mathbb{R}^{n^2} \ni (\vec{x}_1 , \dots, \vec{x}_n) = \begin{bmatrix} x_{1,1} & \cdots & x_{1,n} \\ \vdots & \cdots & \vdots \\ x_{n,1} & \cdots & x_{n,n} \end{bmatrix} \equiv X. $$ Define the smooth map $F: \mathbb{R}^{n^2} \to \mathbb{R}^{n^2}$ by 
$$F(\vec{x}_1 , \dots, \vec{x}_n) = X^T A X,$$
where $X^T$ denotes matrix transpose. I would like to find the $n^2 \times n^2$ Jacobian matrix $JacF(X)$ of the map $F$ at the point $X = I$, where $I$ is the $n \times n$ identity matrix. In particular, I would like to know if $\det(A) \neq 0$ implies that the Jacobian matrix at $I$ is also nonsingular. If we label the component functions of $F$ as $F = (F_{1,1} \dots F_{n,1}, \dots , F_{1,n}, \dots, F_{n,n})$, I have determined that 
$$F_{i,j} = \sum_{k,l =1}^n a_{k,l}x_{k,i}x_{l,j}. $$ But the calculation appears to become quite tedious from here, since for each $F_{i,j}$ we need to calculated $n^2$ total partial derivatives. Is there a more efficient way to determine whether $\det(JacF(I)) \neq 0$, perhaps by using total derivatives or the chain rule in some fashion? Hints or answers are greatly appreciated!","['matrices', 'multivariable-calculus', 'jacobian']"
2345213,Why aren't the lines in taxicab geometry the geodesics of the $L^1$ metric?,"This page claims that the proper lines for taxicab geometry should be the same lines as for Euclidean geometry (with the $L^2$ norm on $\mathbb{R}^2$ instead of the $L^1$ norm for taxicab geometry). Lines in the Taxicab Plane look like lines in the Euclidean plane... The same claim also appears to be implicit in the Wikipedia page for taxicab geometry , on this webpage , on this one , and also in the book by Millman and Parker, Geometry: A Metric Approach with Models . All of the sources claim as a result that taxicab satisfies all of the same axioms as Euclidean geometry except for the SAS postulate. Question: Isn't this a very bad, artificial, and unnatural definition of lines for taxicab geometry? $\mathbb{R}^2$ with the metric induced by the $L^1$ norm is a length space , according to Exercise 11 here , so the lengths of its geodesics do correspond to the distance between points -- so why shouldn't the ""lines"" in taxicab geometry be the geodesics with respect to the metric induced by the $L^1$ norm? If we want to study taxicab geometry, and not Euclidean geometry, shouldn't we only study structures which arise naturally from the geometry's definition, rather than definitions of angle, line, and triangle which are unnaturally ""imported"" from Euclidean geometry unchanged? Motivation: The answers to this question will allow me to answer my previous question . In particular, because of the choice of lines for taxicab geometry, even though it is a length space , this is not a case where $d(A,B)+d(B,C) = d(A,C)$ implies that $A,B,C$ are collinear (although it would be if we chose the lines to be the geodesics of the length space ). Thus, in order to define a notion of betweenness, we need to say that $B$ is between $A$ and $C$ if and only if (1) $A,B,C$ are collinear, and (2) $d(A,B) + d(B,C) = d(A,C)$. With a more natural definition of line, like that in Euclidean space, the first condition would be redundant, and the notion of betweenness more intimately and naturally related with our choice of metric.","['metric-geometry', 'soft-question', 'geometry']"
2345233,A proof for a Gidas-Ni-Nirenberg's Theorem in their 1981's paper,"I want to ask whether my proof works for a theorem proved in Gidas-Ni-Nirenberg's second paper in 1981 ( MR0634248 ), which is much shorter than the authors' proof. Many thanks for any discussion. Theorem 1' (Page 378) Let $u > 0$ be a $C^2$ solution of $-\Delta u = g(u)$ in $\mathbb{R}^n, n \geq 3$ with $u(x) = O(|x|^{-m})$ at infinity, $m>0$ Assume (i) $g(s) \geq 0$ on $ 0\leq s \leq \|u\|_\infty$ and $g = g_1 + g_2$ with $g_1$ is Lipschitz and $g_2$ is continuous and non-decreasing, (ii) $g(s) = O(s^\alpha)$ near $s=0$ for some $\alpha > \frac{n+1}{m}$ . Then $u$ is radially symmetric about some point in $\mathbb{R}^n$ and $u_r < 0$ where $r$ is the radial coorinate about that point. They shows the symmetry in $x_1$ -axis by the well-known moving plane method. The first step is to show the set $\Lambda := \{ \lambda \in \mathbb{R}: u(x) > u(x^\lambda),  \forall x \in \Sigma_\lambda\}$ contains $[R,\infty)$ for some large $R>0$ . Notations: $x^\lambda$ is the reflection of $x$ with respect to the plane $\{ x_1 = \lambda \}$ . $\Sigma_\lambda = \{ x: x_1 < \lambda\}$ . To obtain such result, they need to show $\int_{\mathbb{R}^n} g(u(y)) dy > 0$ . The authors used 2 pages to prove such thing, which is much longer than mine. (That's why I confused.) My shorter proof: Assume not, then $g(u(y)) = 0$ in $\mathbb{R}^n$ by $g \geq 0$ . So $u$ is harmonic and bounded. The Liouville's theorem and decay condition implies $u \equiv 0$ , which contradicts to the positivity of $u$ .","['elliptic-equations', 'maximum-principle', 'analysis', 'partial-differential-equations']"
