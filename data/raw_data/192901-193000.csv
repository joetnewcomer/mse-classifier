question_id,title,body,tags
3684583,kernel of Haagerup tensor product of maps,"Haagerup tensor product $\otimes_{\rm h}$ is both injective and projective. Pisier, Gilles , Introduction to operator space theory, London Mathematical Society Lecture Note Series 294. Cambridge: Cambridge University Press (ISBN 0-521-81165-1/pbk). vii, 478 p. (2003). ZBL1093.46001 . Can the following be true? Let $q_i : E_i \rightarrow F_i$ be complete quotient maps of operator spaces. Then \begin{equation}
{\rm Ker} \, q_1 \otimes_{\rm h} q_2 = {\rm Ker} \, q_1 \otimes_{\rm h} E_2 + E_1 \otimes_{\rm h} {\rm Ker} \, q_2.
\end{equation} I have some hints which tilt me towards believing it is true. Denote by U the operator space on the right. Then $U \subset {\rm Ker} \, q_1 \otimes_{\rm h} q_2$ . Then the product map drops to a map on $(E_1 \otimes_{\rm h} E_2)/U$ and we need to prove that this map is injective. As for the algebraic tensor product, one finds an inverse map from $F_1 \otimes_{\rm h} F_2 \simeq E_1/{\rm Ker}\, q_1 \otimes_{\rm h} E_2/{\rm Ker} \, q_2$ to $(E_1 \otimes_{\rm h} E_2)/U$ . For this one starts from the bilinear map: $(\hat e_1,\hat e_2) \mapsto \widehat{e_1 \otimes e_2}$ , where the hats are the obvious classes. It is well defined and completely bounded. Hence it defines a linear map on the Haagerup product. Then one checks that it is the inverse map we looked for. I am hesitant because this implies $(E \otimes_{\rm h} F)/(G \otimes_{\rm h} F) \simeq (E/G) \otimes_{\rm h} F$ for any sub-space $G$ of $E$ .","['tensor-products', 'operator-spaces', 'functional-analysis', 'operator-algebras']"
3684606,Prove that a stochastic process cannot have continuous paths,"Consider the following problem. Suppose that a stochastic process $V$ satisfies the conditions below $t_1 \neq t_2$ implies that $V(t_1)$ and $V(t_2)$ are independent. $\{V(t)\}$ is stationary, i.e. the (joint) distribution of $\{V(t_1+t),\dots,V(t_k+t)\}$ does not depend on $t$ . $\mathbb{E}[V(t)] = 0$ for all $t$ . Prove that $V(t)$ cannot have continuous paths. The hint says to consider $E[|V(t,N)-V(s,N)|^2]$ where $$
V(t,N) = \max\{-N,\min\{N,V(t)\}\}, \quad N=1,2,3,\dots
$$ I don't see how this hint helps to prove that $V$ cannot have continuous paths. The only connection I found is the Kolmogorov's continuity Theorem. Any help is appreciated!!","['stochastic-analysis', 'stochastic-processes', 'probability-theory']"
3684636,derivative of hypergeometric function,"I am doing an integral in Mathematica and I find the solution contains derivatives of hypergeometric functions. I would like (ideally) a simple analytic form for these. I have tried HypExp mathematica package but there is no derivative capability. I would like to know how to simplify the following functions: $\text{Hypergeometric2F1}^{(0,0,1,0)}\left(1,1,2,-\frac{2}{a-2}\right)$ $\text{Hypergeometric2F1}^{(0,1,0,0)}\left(1,1,2,-\frac{2}{a-2}\right)$ Thanks","['analytic-functions', 'derivatives', 'hypergeometric-function']"
3684648,"Recursive sequence, $x_{1} \geq 0, x_{n+1}=\sqrt{x_{n}+2}$","Recursive sequence, $x_{1} \geq 0, x_{n+1}=\sqrt{x_{n}+2}$ and it is requested to prove that $\lim_{n \to \infty} x_n=2$ .
This is a common problem, but I found it quite more difficult when the value of $x_1$ is not established, as we cannot say it is a monotonous sequence, and as a part of the exercise we are given a step and a suggestion, which I am pretty sure they can be done by induction. Nevertheless, I am still unable to see the path to prove it. $|x_{n+1}-2|\leq\frac12|x_n-2|, \forall n \in \mathbb{N}$ , and as a suggestion for the problem, note that $|x_n-2|\leq \frac1{2^{n-1}}|x_1-2|, \forall n \in \mathbb{N}.$","['real-numbers', 'induction', 'real-analysis', 'sequences-and-series', 'convergence-divergence']"
3684703,Who first invented/introduced the concept of the trace of a Matrix and Why?,"Could anyone give any information  about the invention of the concept of the trace of a Matrix, as this concept  is so important and useful in linear algebra. I searched on the internet, but found nothing on its origin.","['matrices', 'trace', 'math-history']"
3684708,"Multivariable chain rule for $f(x,y,g(x,y))$ when finding $\tfrac{\partial f}{\partial x}$.","Consider the function $f(x,y,z)$ where $z=g(x,y)$ . If I took the partial derivative wrt $x$ what I'd end up getting would be: $$
\dfrac{\partial f}{\partial x} = \dfrac{\partial f}{\partial x} \dfrac{\partial x}{\partial x}+\dfrac{\partial f}{\partial y} \dfrac{\partial y}{\partial x}+\dfrac{\partial f}{\partial g} \dfrac{\partial g}{\partial x}
$$ $$
\dfrac{\partial f}{\partial x} = \dfrac{\partial f}{\partial x} +\dfrac{\partial f}{\partial y} \dfrac{\partial y}{\partial x}+\dfrac{\partial f}{\partial g} \dfrac{\partial g}{\partial x}
$$ Well, this is quite obviously wrong, since that would imply $\dfrac{\partial f}{\partial g} \dfrac{\partial g}{\partial x} = 0$ . How would I correctly express the partial derivative with respect to x in this case?","['multivariable-calculus', 'calculus', 'analysis', 'chain-rule']"
3684763,"What is the significance of the term ""separable"" in the context of countability properties?","In the context of topological spaces, I see the following major countability properties: A space is: ""separable"" iff it has a countable dense subset ""second countable"" iff if has a countable basis ""first countable"" iff the neighbourhood system of every point has a countable local basis. (Definitions taken from Counterexamples in Topology by Steen and Seebach, 2nd ed. 1978 -- there may be differences in wording from other sources.) The question I have is: ""separable"" into what, exactly? By which I mean to say: what is the thinking behind calling such a condition ""separable""? It appears not to be related to the concept of ""separation axioms"", which do immediately and obviously invoke an intuitive notation of separation, neither does it seem to have anything to do with ""separated sets"". (Anyone using the spelling ""seperable"" or ""seperated"" will be immediately downvoted. :-) )",['general-topology']
3685110,"Given $f$ holomorphic, which are the necessary conditions on $\phi$ in order to make $\phi \circ f \circ \phi^{-1}$ holomorphic?","It is well known that $\bar f(\bar z)$ is holomorphic whenever f is. I was wondering how to generalize this fact... Let $f: \Omega \longrightarrow \mathbb{C}$ be holomorphic and $\phi: \mathbb{C} \longrightarrow \mathbb{C}$ be an homeomorphism where $\Omega \subseteq \mathbb{C}$ is open. We need the exsistence of the limit $\lim_{h \rightarrow 0} \frac{\phi \circ f \circ \phi^{-1}(z_0 + h) - \phi \circ f \circ \phi^{-1}(z_0)}{h}$ , if $\phi$ is Frechet differentiable this is equivalent to asking for the existence of $\lim_{h \rightarrow 0} \frac{D\phi(f \circ \phi^{-1}(z_0))[f'(\phi^{-1}(z_0))\cdot D\phi^{-1}(z_0)[h]]}{h}$ . I've then found the following sufficient conditions: i) $\phi(z + w) = \phi(z) + \eta(w)$ ii) $\eta(z \cdot w) = \psi(z) \cdot \eta(w)$ Where $\eta,\psi: \mathbb{C} \longrightarrow \mathbb{C}$ and $\eta$ is an homeomorphism. Then $\forall z_0 \in \mathbb{C}.$ $D\phi(z_0)$ exists and $D\phi(z_0) = \eta$ thus $\forall y_0 \in \mathbb{C}$ . $D\phi^{-1}(y_0) = \eta^{-1}$ . Moreover we have $D\phi(f \circ \phi^{-1}(z_0))[f'(\phi^{-1}(z_0))\cdot D\phi^{-1}(z_0)[h]] = \eta(f'(\phi^{-1}(z_0)) \cdot \eta^{-1}(h)) = \psi(f'(\phi^{-1}(z_0))) \cdot h$ , thus the limit exists and has value $\psi(f'(\phi^{-1}(z_0)))$ As an example we can take $\phi(z) = \alpha z + \beta$ with $\alpha, \beta \in \mathbb(C)$ , then $\eta(z) = \alpha z$ and $\psi(z) = z$ thus $(\phi \circ f \circ \phi^{-1})'(z_0) = f'(\frac{z_0}{\alpha} - \frac{\beta}{\alpha})$ and sure enough if we use the standard method to evaluate this derivative we get the same resut. Conditions i) and ii) above are then sufficient, are they necessary too? If not does there exist a complete characterization of such $\phi$ 's?","['complex-analysis', 'frechet-derivative', 'functional-analysis']"
3685111,Probability on Topological Space,"Suppose you have a topological space X, assuming it is Hausdorff, compact, connected space.  Is it possible to equip it with probability measure?  I am curious if one could create probabilistic topological space whose points and open sets encode some information about probability distribution and measure.","['statistics', 'geometric-topology', 'general-topology', 'probability-theory', 'algebraic-topology']"
3685145,Construct a circle tangent to sides $BC$ and $CD$ and s.t. its meetings with the diagonal $BD$ are tangent points from tangents draw from point $A$,"Given square $ABCD$ I want to construct (with ruler and compass) the circle in the interior of the square such that it is tangent to sides $BC$ and $CD$ and such that it's meetings with the diagonal $BD$ are tangent points from tangents draw from point $A$ : It is clear that the center of the circle must lie in $AC$ . I tried finding some cyclic quad somewhere and I failed miserably. I then thought about puting $K$ in hyperbola with focii $A$ and the center $O$ of the square. Then again $K$ lies outside the segment $A O .$ This problem is hard because we would think of looking at the locus of the centers of circles such that the meetings of the circle with line $BD$ are the tangents from $A$ . But that this locus is exactly the same as the locus of the centers of the circles tangent to $CD$ and $BC$ : line $AC$ . The proof is simple: as the tangents from $A$ must have the same lenght the meetings $M$ and $N$ of $BD$ with the circles must be reflections of each other with respect to the center $O$ of the square $ABCD$ thus the center of the circle must lie in line $AO$ which is line $AC$ . The real geometric constrain is between the distance of the centers (all of which lie on line $AC$ ) to point $A$ and the radius of the circles. Let $P$ be in line segment $OC$ . $PA = x$ $r$ the radius of the circle centered at $P$ . $a=AB$ we have that $r^2 = x^2 - x \frac{a\sqrt2}2$ and $x = a\frac{\sqrt2}4 + \sqrt{r^2+\frac{a^2}8}$ and these weird relations are the ""locus"" that I desire to work with.","['euclidean-geometry', 'geometry', 'geometric-construction']"
3685200,Polynomial outputs containing a particular Integer sequence,"Does there exist a polynomial $P$ of degree greater than $1$ and with integer coefficients such that for every natural number $m$ there exists a natural number $n$ such that $P(n)=2^m$ ? This question seems very tricky and interesting, I guess some kind of interpolation might be of help but I could not figure out a proper solution. Comparing growth rate in this problem seems to be of no use as its an existence problem and polynomial does tend to infinity. Help. What about proving there does not exist such a monic polynomial?","['number-theory', 'polynomials', 'real-analysis']"
3685262,Is the given ODE system a gradient system?,"Consider the planar system $$\dot{x} = 2x-y-\frac{2x^3+2xy^2-xy}{\sqrt{x^2+y^2}} \\
 \dot{y} = 2y+x-\frac{2x^2y+2y^3+x^2}{\sqrt{x^2+y^2}}$$ which in polar coordinates is given by $$\ \dot{r}=2r(1-r) \\ \dot{\theta}=2sin^2(\theta/2)$$ Is the above system a gradient system? Can you explain? Also,
1. Find all equilibrium points of this system and determine their stability properties. Are equilibria asymptotically stable? 2. What are stable and unstable manifolds of the equilibria. From the polar coordinates, r=0 could be equilibrium but then x=0, y=0 will cause the system to blow up.",['ordinary-differential-equations']
3685292,$\int x^{dx}-1$,"If you go to Flammable Maths 's YouTube channel and scroll through some of his videos you see him solving the following integral: $$\int x^{dx}-1$$ he explains that this is a Product integral. My questions are the following: 1 - What is the geometric meaning of a product integral? 2 - does it make sense to have: $$\int f(x,dx)$$ and if $f(x,dx) = g(x)dx$ then it's just a regular integrals and if $f(x,dx) = g(x)^{dx}$ it's just a product integral? I'll leave the link to the video here .","['integration', 'calculus', 'soft-question']"
3685367,How to prove that if $f_n \rightarrow f$ in measure then $\dfrac{1}{f_n} \rightarrow \dfrac{1}{f}$?,"Let $f_n \rightarrow f$ in measure $\mu$ on A and $\mu(A) < +\infty$ . If $f_n(x)$ and $f(x)$ are both not equal to 0 for all $x \in A$ , then $\dfrac{1}{f_n} \rightarrow \dfrac{1}{f}$ in measure. I don't know how to approach this question, as what I only know is the definition of function converging in measure. Plus, don't you think it's werid when $f_n(x)$ and $f(x)$ can also be $\infty$ ? I'm stuck at how to find my way around. Any hint for this would help a lot.","['measure-theory', 'real-analysis']"
3685456,Area between $x=y^2-7$ and $x=e^y$ for $-1\leq y\leq 1$,"I need to find the area of the shaded region here I thought the area would be $\int_{-1}^{1}{y^2-7-e^y}dy$ ,which got me $-\frac{40}{3}-3+\frac1e$ , but I was marked wrong. By inspection it looks like the area under the blue curve has a symmetry that makes its total area $0$ from $x=-7$ to $x=\frac12$ . So I figured the area I'm looking for is the area under the red curve from $x=1/2$ to $x=3$ . I tried this and got an answer of $\frac13 - 2$ which was also marked wrong. What do I do?","['integration', 'multivariable-calculus', 'multiple-integral', 'area']"
3685463,Is it possible to write a discretized differentiation operator as a Kronecker sum?,"The following is a question that was posted and then deleted . Since I found the question interesting, I am posting it anew. I have this differential operator $$L=\begin{bmatrix}
0 & -\partial_x  \\
-\partial_x & 0 
\end{bmatrix}$$ and I have to discretize (say with 2nd order finite differences). Let's call $A$ the matrix that discretize $\partial_x$ . Then, the discretization of $L$ will result in a block diagonal  matrix like $$\begin{bmatrix}
\mathcal{O} & -A \\
-A & \mathcal{O} 
\end{bmatrix}$$ Of course, I could diagonalize, but my question is: ""is it possible to write this as a Kronecker sum ?""  Something like $I \otimes A + A \otimes I$ ? The zeros along the diagonal in some sense don't look so promising, but I really don't know how to even disprove this.","['ordinary-differential-equations', 'real-analysis', 'matrices', 'linear-algebra', 'partial-differential-equations']"
3685507,Ideals prime to the conductor in the ideal class group of an order,"Let $K$ be a number field, $\mathcal{O}_K$ its ring of integers and $\mathcal{O}$ an order in $K$ .
I'm going through the proof for the formula linking the cardinality of the class group $Cl(\mathcal{O})$ to that of the class group $Cl(\mathcal{O}_K)$ . I'm following the proof given here by Keith Conrad. At the end of page 11 he makes the following statement: From the proof that $Cl(\mathcal{O})\longrightarrow Cl(\mathcal{O}_K)$ is surjective, we can represent each ideal class in $Cl(\mathcal{O})$ by an ideal $\mathfrak{b}$ of $\mathcal{O}$ which is relatively prime to the conductor $\mathfrak{c}$ and the fact that $Cl(\mathcal{O})\longrightarrow Cl(\mathcal{O}_K)$ is surjective is Theorem 5.1, which uses the fact that the same statement is true in the maximal order $\mathcal{O}_K$ . However, although I can easily prove the statement for $\mathcal{O}_K$ using unique factorization and the Chinese Remainder Theorem, I can't really see how this implies that the same statement is true for $\mathcal{O}$ . From the proof of Theorem 5.1 I can see that every coset of the kernel of the map $\psi:Cl(\mathcal{O})\longrightarrow Cl(\mathcal{O}_K)$ contains a class verifying the statement (just take $\mathfrak{a}$ in any class of $Cl(\mathcal{O}_K)$ such that $\mathfrak{a}$ is prime to the conductor and consider $\mathfrak{a}\cap\mathcal{O}$ ), and if I could use the explicit description of $\ker(\psi)$ which Conrad gives soon after that statement, then the statement would follow easily, as is done here on MathOverflow. However this description depends on the statement itself, so we can't use it.
So I guess I'm probably missing some trivial step, but I can't really see that, hence even hints are really appreciated. Edit: The statement in the question is also mentioned by Pete L. Clark here , in the last 3 lines. The reference is Neukirch, but I can't find this claim in his book. Moreover I'm wondering wheter this question is more appropriate for MathOverflow, since I've only seen this claim in MO posts so far. Edit 2: I've just found another claim which implies the aforementioned statement. It is Proposition 20.13 from these notes by Pete L. Clark: Let $R$ be a Dedekind domain, $I$ a fractional ideal of $R$ and $J$ a nonzero integral ideal of $R$ . Then there exists $a\in I$ such that $aI^{-1}+J=R$ . This is stated for Dedekind domains, but it seems to me that the proof given in the notes should still work even if $R$ is a non-maximal order, provided that we assume $I$ to be invertible. Indeed, using the same notations as in the original proof, there should be finitely many distinct prime ideals $\mathfrak{p}_1,...,\mathfrak{p}_r$ containing $J$ , because if $\mathfrak{p}\supseteq J$ , then $\mathfrak{p}$ is minimal on $J$ , since $\dim R=1$ , and in a noetherian ring there are finitely many minimal prime ideals on each ideal. Moreover $I\mathfrak{p}_1\cdots\mathfrak{p}_n\subsetneq I\mathfrak{p}_1\cdots\mathfrak{p}_{i-1}\mathfrak{p}_{i+1}\cdots\mathfrak{p}_n$ for all $1\leq i\leq r$ , for otherwise, using the invertibility of $I$ , we would find $\mathfrak{p}_1\cdots\mathfrak{p}_n=\mathfrak{p}_1\cdots\mathfrak{p}_{i-1}\mathfrak{p}_{i+1}\cdots\mathfrak{p}_n$ , and considering that these are comaximal ideals, this implies $\mathfrak{p}_i\supseteq\mathfrak{p}_j$ for some $j\neq i$ ,  which is clearly impossible. Then the rest of the proof should work without any modification. This is quite different from the proof I had found for the maximal order, since that proof relied too much on unique factorization to be extended to the non-maximal case. Although this should solve my question, I'm not closing it since I'm still interested in knowing if there is a way to prove the statement in non-maximal orders as a consequence of the fact that the same property holds true in the maximal order, which is what K. Conrad seemed to suggest in his notes. Moreover it would be nice if someone could confirm that what I wrote above is correct.","['number-theory', 'algebraic-number-theory']"
3685510,Prime decomposition of pR in $\mathbb{A}\cap \mathbb{Q}[\alpha]$ for $\alpha={^3\sqrt{hk^2}}$ if p is a prime such that $p^2|m$,"I'm going through Marcus number Field chapter 3 an I'm finding very hard to understand the part about the decomposition of pR (theorem 27) that tells us that if $p\not||R/Z[\alpha ]|$ then we can decompose $pR$ by looking at a factorization of it's minimal polynomial (Kummer's theorem?) In partcular I'm stuck on exercise 26 Let $\alpha={ ^{3}\sqrt{m}}$ where m is a cubefree integer, $K = \mathbb{Q}[\alpha]$ , $R = \mathbb{A} \cap \mathbb{Q}[\alpha]$ Show that if p is a prime $\neq 3$ and $p^2 \not|m$ , then the prime decomposition of pR
  can be determined by factoring $x^3 − m\; mod\; p.$ (See Theorem 27 and exercise
  41, chapter 2 (this tells us the discriminat and the integral bases I write below).) Suppose $p^2 | m$ . Writing $m = hk^2$ as in exercise 41, chapter 2, set $ \gamma= \frac{\alpha^2}{k}.$ Show that p does not divide $|R/Z[\gamma ]|$ ; use this to determine the prime decomposition
  of pR. Determine the prime decomposition of 3R when $m\not\equiv \pm 1$ (mod 9). Determine the prime decomposition of 3R when m = 10. (Hint: Set $\beta = (\alpha −
1)^2/3$ and use exercise 18 to show that disc(β) = 4 disc(R). Also note exercise
  41(d), chapter 2 (this tells us that $\beta^3-\beta^2+\left(\frac{
1+2m}{3}\right)\beta-\frac{(m-1)^2}{27}=0))$ Show that this always works for $m\equiv \pm 1\; (mod\; 9)$ except
  possibly when $m\equiv \pm 8\; (mod\; 27)$ . Show that 9 $\not|$ disc(R) when $m\equiv \pm 1\; (mod\; 9)$ ; use this to show that 3R is not
  the cube of a prime ideal. Assuming the converse of Theorem
  24, show that 3R = $P^2Q$ where P and Q are distinct primes of R. I think I've done point 1) using the fact that $p^2\not| disc(\alpha)$ implies we can use theorem 27 that tells us exactly that we can decompose pR simply by factorizing the minimal polynomial of $\alpha$ , but the problem is now point 2) (and the ones after since they rely on 2). I was able to prove that $\gamma=\sqrt{h^2k}$ and that $p^2\not| h^2k$ so either we can use the fact above or $p^2|disc(\alpha)=-27^2*k^2h\Rightarrow p^2|27^2$ so p=3, but now I don't know how to prove that 3 doesn't divide $|R/\mathbb{Z}[\alpha]|$ since for me the latter is always divisible by 3. An integral base of the above is either $$\left(1,\alpha,\frac{\alpha^2+k^2\alpha+k^2}{3k}\right),\quad \left(1,\alpha,\frac{\alpha^2-k^2\alpha+k^2}{3k}\right),\quad \left(1,\alpha,\frac{\alpha^2}{k}\right) $$ if respectivly $m\equiv  1\; (mod\; 9)$ , $m\equiv  -1\; (mod\; 9)$ , $m\not\equiv  \pm1\; (mod\; 9)$ Any help would be welcomed, even more if quite specific on the calculations since I think there is something I miss on a theoretical level. Exercise 18 Let K be a number field of degree n over $\mathbb{Q}$ , and let $\alpha_1, \dots , \alpha_n \in K.$ Show that $disc(r\alpha_1, \alpha_2, \dots , \alpha_n) = r^2 disc(\alpha_1, \dots , \alpha_n)$ for all r $\in \mathbb{Q}$ . Let $\beta$ be a linear combination of $\alpha_2, \dots , \alpha_n$ with coefficients in $\mathbb{Q}$ . Show that $disc(\alpha_1 + \beta, \alpha_2, \dots , \alpha_n) = disc(\alpha_1, \dots , \alpha_n).$ Theorem 24 Let p be a prime in $\mathbb{Z}$ , and suppose p is ramified in a number ring R.
  Then p | disc(R). UPDATE: The question is still without answer so for now I'll post my solution to the first two points, then if a better one comes I'll be happy to set it as solving the question.","['prime-factorization', 'kummer-theory', 'number-theory', 'dedekind-domain', 'extension-field']"
3685529,Prove $\lim_{x \to a} \cos{x} = \cos{a}$ using a $\varepsilon$-$\delta$ argument,Prove $\lim\limits_{x \to a} \cos{x} = \cos{a}$ with $\varepsilon$ - $\delta$ . Is proving $|\cos{x}-\cos{a}| \leq |x-a|$ with MVT the only way possible?,"['limits', 'epsilon-delta', 'real-analysis']"
3685553,Expected value of squared uniform distribution,"Let $X \sim \text{Unif}(\sqrt n S^{n-1})$ where $S^{n-1}$ is the unit Sphere in $\mathbb{R}^n$ . I'm trying to prove that $\mathbb{E}(\left \| X \right \|^2)=n$ .
An interesting fact here is the rotational invariance of the distribution of $X$ . I have used it to prove that $\mathbb{E}( X_i^2)$ is a constant $\alpha$ but I didn't find an argument that helps me determine the constant factor $\alpha$ . Does anyone have a hint or an idea?","['probability-distributions', 'probability-theory', 'probability']"
3685592,The Number of Hyperplanes Intersecting a Unit Hypercube,"Prove that the number of hyperplanes such that $$c_1x_1 + c_2 x_2 + ... + c_n x_n = 0, \pm 1, \pm 2, \pm 3, ...$$ which intersect the unit $n$ -cube, $0< x_i < 1,$ is at most $$|c_1| + |c_2| + ... + |c_n|.$$ I started out by plotting small values of $c_i$ in the 3D Geogebra grapher and this is true. But I don't know how to prove this. Perhaps we could use induction with decomposing to lower-dimensional hypercubes?","['analytic-geometry', 'number-theory', 'calculus', 'linear-algebra']"
3685601,Can a bimodal distribution have a gap?,"The question asks to describe the distribution of aspen tree diameters from the sample. I said that the distribution was bimodal with one peak around 5.2 and the other peak around 9.2. However the correct answer is that the distribution is skewed to the right and has a gap between 7 and 8 inches. I tried looking online for some answers but I can't find any. Why is this a skewed unimodal distribution instead of a bimodal distribution? Is it because in a bimodal distribution there are two peaks but there is no gap, instead it is just a very low frequency between the two peaks?",['statistics']
3685674,Find the parametric equation of a line of intersection,"Find the curve of intersection for the following surfaces: $z= x + \frac{y}{2} + \frac{1}{2}$ and $z^2= -x^2 + y$ I keep trying and trying to set them equal to each other and just end up with a mess. I wonder if it's possible to set them up with sin and cos, but I honestly have no idea. This is the first step to a problem I have, and then I have to calculate the length, which I can do, but I just can't find the parametric equations.",['multivariable-calculus']
3685740,"Counter Example to ""naive"" Dominated Convergence Theorem for Outer Integral","To provide some context, given a probability space $(\Omega,\mathcal{F},P)$ , and for an arbitrary map $T: \Omega \to \bar{\mathbb{R}}$ , the outer expectation is defined as $$
E^*T = \inf\{ EU \: : U \mbox{ is measurable,} \; U \ge T\}.  
$$ Let $\Omega= [0,1]$ , $\mathcal{F}$ the standard Borel $\sigma$ -algebra, and $P$ uniform measure on $[0,1]$ . It is stated in Van Der Vaart and Wellner, Empirical Processes with Applications to Statistics, that there exists a sequence of maps $T_n: [0,1] \to [0,1]$ so that $T_n \downarrow 0$ everywhere, but $E^*T_n =1$ for all $n$ , providing a counter example to ""Naive Outer Dominated Convergence"". This is stated on page 13 without proof. Could someone think up what counter example they had in mind here?","['probability-theory', 'real-analysis']"
3685749,Prove that f is not differentiable at x=0,"Question: For $x$ $\in \mathbb{R}$ , prove that $f(x) = \lim_{n\to \infty}(x+\sqrt{1/n})$ is not differntiable for $x=0$ . My attempt (which my results contradicts the question): $f(x) = \lim_{n\to \infty}(x+\sqrt{1/n})=2x$ which is differentiable $\forall x\in\mathbb{R}$ . And so $f$ is infact differentiable at $x=0$ . I am convinced there must be a mistake in computing $f$ (line 2)?","['limits', 'derivatives']"
3685753,A nice combinatorial identity: $\sum_{k=1}^{n-1}\frac{\binom{k-1}{n-k-1}+\binom{k}{n-k-1}}{\binom nk}=1$,"While answering a recent question I came across the following nice-looking identity: $$
\sum_{k=1}^{n-1}\frac{\binom{k-1}{n-k-1}+\binom{k}{n-k-1}}{\binom nk}=1
$$ valid for all integer $n\ge2$ . Is there a simple algebraic proof of this identity?","['summation', 'binomial-coefficients', 'combinatorics']"
3685792,How can I solve $\displaystyle{\lim_{x \to \infty} \frac{1}{(1+\frac{k}{x})^x}}$?,"How can I solve $\displaystyle{\lim_{x \to \infty} \frac{1}{(1+\frac{k}{x})^x}}$ using $\displaystyle{\lim_{x \to +\infty} {(1+\frac{1}{x})^x = e}}$ ? What should I do with the constant "" $k$ ?","['limits', 'calculus']"
3685839,Is the Katětov extension of $\Bbb N$ zero-dimensional?,"I take $X=\kappa\Bbb N$ to be $\Bbb N\cup\{p:p \text{ is a free ultrafilter on }\Bbb N\}$ .  Each singleton in $\Bbb N$ is open and a local base at any free ultrafilter $p$ is given by $\{\{p\}\cup A:A\in p\}$ . Is X zero-dimensional? In other words, is it true that each point of $X$ has a neighborhood basis of clopen sets? It is claimed for example here that each basic open set $\{p\}\cup A$ is clopen. I am not sure about this claim, as it seems that from basic properties of ultrafilters one can prove it is not the case. Fact: Given an infinite subset $A$ of $\Bbb N$ there is a free ultrafilter containing $A$ . (Take the set of all cofinite subsets of $A$ .  That forms a filter basis that generates a free filter on $\Bbb N$ .  Any ultrafilter that extends that filter will be a free ultrafilter containing $A$ .) Now take a free ultrafilter $p\in X$ and an (open) neighborhood $U=\{p\}\cup A$ with $A\in p$ .  I claim that $U$ is never closed in $X$ . $A$ must be infinite because $p$ is free.  Partition $A$ into two infinite sets: $A=B\cup C$ . By standard ultrafilter properties exactly one of the two subsets, say $B$ , must be in $p$ .  By the Fact above, there is an ultrafilter $q$ containing $C$ , and $q$ is necessarily distinct from $p$ .  This $q$ is in the closure of $U$ .  Indeed, take any neighborhood $\{q\}\cup D$ with $D\in q$ . $D\cap C\in q$ so $D\cap C$ is not empty and $D$ meets $A$ , so any neighborhood of $q$ meets $U$ .  This shows that $U$ is not clopen. Can you see anything wrong with this argument? Added: My original question was the one above about $\kappa\Bbb N$ , but I also mistakenly thought $\kappa\Bbb N$ was the same as the Cech-Stone compactification $\beta\Bbb N$ .  Thanks @EricWofsey for setting me straight.",['general-topology']
3685956,"Is ""{x | (x, y) ∈ A}"" the same thing of ""dom A""?","I am learning set theory using the book ""Axioms and Set Theory"" from Robert André and it defines dom R as dom R = {x : x∈C ∧ (x, y) ∈ R for some y∈C} , R is a relation on C. But English ain't my first language and I always find confusing the sentence ""for some"", but I got what dom R means by the examples. Because of this, I wanted to try using an easier solution (easier for me, of course), and I think I can use the expression dom A = {x : (x, y) ∈ A} instead; I think both expressions have different contexts, but A = {(cake, delicious), (sugar, mehh), (cookie, yummy), (cheese, life)} , would both expression give the same result? Or need I to urgently revise my set theory's knowledge? P.S.: I am not in/on college, so I ain't a mathematician or studying it formally. I used bold on mathematical expressions because with the white background, it hurts my eyes. Thanks for reading my question!",['elementary-set-theory']
3685968,$f(x)^3+g(x)^3+h(x)^3=1$ relation with elliptic integrals,"$x\in\mathbb{C}$ and $f,g,h:\mathbb{C}\to\mathbb{C}$ $f,g,h$ are Meromorphic functions. $$f(x)^3+g(x)^3+h(x)^3=1  \tag{1}$$ $$f'(x)=g(x)^2-h(x)^2  \tag{2}$$ $$g'(x)=h(x)^2-f(x)^2 \tag{3}$$ Initial conditions: $f(0)=m$ , $g(0)=n$ We have 3 unknown functions, and we also have 3 different equations .
I would like to find out $f(x),g(x),h(x),$ If we derivative Equation $(1)$ ,we can get $$3f(x)^2f'(x)+3g(x)^2g'(x)+3h(x)^2h'(x)=0  $$ $$h'(x)=f(x)^2-g(x)^2 \tag{4}$$ It can be shown easily from sum of Equation (2), (3) , (4) that $$f'(x)+g'(x)+h'(x)=0 $$ If we integrate it, $$f(x)+g(x)+h(x)=k$$ and put $x=0$ $$f(0)+g(0)+h(0)=k$$ $k=m+n+(1-(m^3+n^3))^\frac{1}{3}$ I would like to find differential equation for $f(x),g(x),h(x) $ Using Equation $(2)$ and apply derivative both sides $$f''(x)=2g'(x)g(x)-2h'(x)h(x)$$ $$f''(x)=2g(x)(h(x)^2-f(x)^2)-2h(x)(f(x)^2-g(x)^2)$$ $$f''(x)=2(g(x)h(x)-f(x)^2)(g(x)+h(x))$$ $$f''(x)=2(k-f(x))(g(x)h(x)-f(x)^2) \tag {5}$$ If we use Equation (2) $$f'(x)=(g(x)-h(x))(g(x)+h(x))   $$ $$f'(x)=(g(x)-h(x))(k-f(x))   $$ $$g(x)-h(x)=\frac{f'(x)}{k-f(x)}   $$ $$g(x)+h(x)=k-f(x)$$ $$2g(x)=\frac{f'(x)}{k-f(x)} +k-f(x)$$ $$g(x)=\frac{f'(x)+(k-f(x))^2}{2(k-f(x))} $$ $$h(x)=\frac{(k-f(x))^2-f'(x)}{2(k-f(x))} $$ If we put $g(x),h(x)$ in Equation (5) $$f''(x)=2(k-f(x))(\frac{(k-f(x))^4-f'(x)^2}{4(k-f(x))^2}-f(x)^2) \tag {6}$$ $$2f''(x) (k-f(x)) +f'(x)^2 =(k-f(x))^4-4f(x)^2(k-f(x))^2 \tag {7}$$ Let's assume that the solution of the differential equation is : $$f'(x)^2=(f(x)^2-k^2)^2+c(f(x)-k)  \tag{8}$$ Where c is another constant The Proof of my assumption above: $$2 f'(x)f''(x)=4(f(x)^2-k^2)f(x)f'(x)+cf'(x)$$ $$2 f''(x)=4(f(x)^2-k^2)f(x)+c$$ Multiply both side by $ (k-f(x))$ $$2 f''(x)(k-f(x))=4(f(x)^2-k^2)(k-f(x))f(x)+c(k-f(x))  \tag{9}$$ If we add Equation $(8)$ and $(9)$ , $$2 f''(x)(k-f(x))+f'(x)^2=4(f(x)^2-k^2)(k-f(x))f(x)+(f(x)^2-k^2)^2$$ $$2 f''(x)(k-f(x))+f'(x)^2=(f(x)^2-k^2)[4(k-f(x))f(x)+(f(x)^2-k^2)]$$ $$2 f''(x)(k-f(x))+f'(x)^2=(f(x)^2-k^2)(k-f(x))[4f(x)-f(x)-k)]$$ $$2 f''(x)(k-f(x))+f'(x)^2=(f(x)^2-k^2)(k-f(x))(3f(x)-k)$$ $$2 f''(x)(k-f(x))+f'(x)^2=(k-f(x))^4-4f(x)^2(k-f(x))^2$$ It proves that my assumption is true in Equation $(8)$ Solution for $f(x)$ can be written as $$f'(x)^2=(f(x)^2-k^2)^2+c_1(f(x)-k)  $$ $$ x=\int_{f(0)}^{f(x)} \frac{du}{\sqrt{(u^2-k^2)^2+c_1(u-k)}}$$ It is an elliptic function and we apply the same procedure for $g(x)$ and $h(x)$ same result we will get $$ x=\int_{g(0)}^{g(x)} \frac{du}{\sqrt{(u^2-k^2)^2+c_2(u-k)}}$$ $$ x=\int_{h(0)}^{h(x)} \frac{du}{\sqrt{(u^2-k^2)^2+c_3(u-k)}}$$ If we derivative all $f(x)$ $g(x)$ and $h(x)$ $$\frac{f'(x)}{(\sqrt{f(x)^2-k^2)^2+c_1(f(x)-k)}}=\frac{g'(x)}{\sqrt{(g(x)^2-k^2)^2+c_2(g(x)-k)}}=\frac{h'(x)}{\sqrt{(h(x)^2-k^2)^2+c_3(h(x)-k)}}=1$$ Where $c_1,c_2,c_3$ are constants My Question: Are $f(x)$ $g(x)$ and $h(x)$ doubly periodic functions? What are their period formulas if they are doubly periodic functions ? Can We express the $f(x)$ $g(x)$ and $h(x)$ as the Weierstrass Elliptic function? It is very nice result but I have not expected to have such result .Please write comments about your analyzes why the result is elliptic integral  for cubic sum equation? I believe my method can be extended for $$f(x)^n+g(x)^n+h(x)^n=1 $$ $$f'(x)=g(x)^{n-1}-h(x)^{n-1}$$ $$g'(x)=h(x)^{n-1}-f(x)^{n-1}$$ where n is positive integer $n>3$ . Please share if you find some elliptic integrals for $n>3$ Thanks a lot for answers and comments EDIT: I have proved that $c_1=c_2=c_3$ Proof: $f(0)=m$ , $g(0)=n$ , $$f(0)+g(0)+h(0)=k$$ $$k=m+n+(1-(m^3+n^3))^\frac{1}{3}$$ $$h(0)=k-(m+n)$$ Let's define $$h(0)=k-(m+n)=r$$ $$\frac{f'(x)}{(\sqrt{f(x)^2-k^2)^2+c_1(f(x)-k)}}=1$$ $$f'(x)^2=(f(x)^2-k^2)^2+c_1(f(x)-k)  $$ $$f'(x)^2-(f(x)^2-k^2)^2=c_1(f(x)-k)  $$ $x=0$ $$f'(0)^2-(f(0)^2-k^2)^2=c_1(f(0)-k)  $$ $$[f'(0)+(f(0)^2-k^2)][f'(0)-(f(0)^2-k^2)]=c_1(f(0)-k)  $$ $$[\frac{f'(0)}{(f(0)-k)}+f(0)+k][f'(0)-f(0)^2+k^2]=c_1  $$ $$[\frac{n^2-r^2}{(-(n+r)}+m+m+n+r][n^2-r^2-m^2+(m+n+r)^2]=c_1$$ $$[-n+r+m+m+n+r][n^2-r^2-m^2+m^2+n^2+r^2+2mn+2mr+2rn]=c_1$$ $$2[m+r][2n^2+2mn+2mr+2rn]=c_1$$ $$c_1=4(m+r)(n+m)(n+r)$$ $c_1$ is symmetric , It means if we do the same actions for $g(x),h(x)$ , we will find same $c_1=c_2=c_3=4(m+r)(n+m)(n+r)$ If we write as k value, $$c_1=c_2=c_3=4(m+r)(n+m)(n+r)=c$$ $$c=4(m+k-m-n)(n+m)(n+k-n-m)$$ $$c=4(k-n)(n+m)(k-m)$$ $$\frac{f'(x)}{(\sqrt{f(x)^2-k^2)^2+c(f(x)-k)}}=\frac{g'(x)}{\sqrt{(g(x)^2-k^2)^2+c(g(x)-k)}}=\frac{h'(x)}{\sqrt{(h(x)^2-k^2)^2+c(h(x)-k)}}=1$$ $$ x=\int_{f(0)}^{f(x)} \frac{du}{\sqrt{(u^2-k^2)^2+c(u-k)}}$$ $$ x=\int_{g(0)}^{g(x)} \frac{du}{\sqrt{(u^2-k^2)^2+c(u-k)}}$$ $$ x=\int_{h(0)}^{h(x)} \frac{du}{\sqrt{(u^2-k^2)^2+c(u-k)}}$$ $$ x=\int_{m}^{f(x)} \frac{du}{\sqrt{(u^2-(m+n+r)^2)^2+4(m+n)(m+r)(r+n)(u-(m+n+r))}}=\int_{n}^{g(x)} \frac{du}{\sqrt{(u^2-(m+n+r)^2)^2+4(m+n)(m+r)(r+n)(u-(m+n+r))}}=\int_{r}^{h(x)} \frac{du}{\sqrt{(u^2-(m+n+r)^2)^2+4(m+n)(m+r)(r+n)(u-(m+n+r))}}$$ EDIT: I have found an easy way how to produce differential equations $$f(x)^p+g(x)^p+h(x)^p=1 $$ $$f'(x)=g(x)^{p-1}-h(x)^{p-1}$$ $$g'(x)=h(x)^{p-1}-f(x)^{p-1}$$ and we can find other relations as I showed above  that $$h'(x)=f(x)^{p-1}-g(x)^{p-1}$$ $$f'(x)+g'(x)+h'(x)=0 $$ $$f(x)+g(x)+h(x)=k $$ where p is positive integer . $k$ is a constant. $p=2$ $$f(x)^2+g(x)^2+h(x)^2=1 $$ $$f(x)^2+(g(x)+h(x))^2-2g(x)h(x)=1 $$ $$f(x)^2+(k-f(x))^2-2g(x)h(x)=1 $$ $$2g(x)h(x)=1-f(x)^2-k^2+2kf(x)-f(x)^2=1-k^2+2kf(x)-2f(x)^2 $$ $$g(x)h(x)=\frac{1-k^2}{2}+kf(x)-f(x)^2$$ $$f'(x)^2=g(x)-h(x)$$ $$f'(x)^2=(g(x)+h(x))^2-4g(x)h(x)$$ $$f'(x)^2=(k-f(x))^2-2(1-k^2)+4kf(x)-4f(x)^2$$ $$f'(x)^2=k^2-2kf(x)+f(x)^2+2(1-k^2)+4kf(x)-4f(x)^2$$ $$f'(x)^2=-3f(x)^2+2kf(x)+2-k^2$$ If both side derivate, $$f''(x)+3f(x)=k$$ General solution for $p=2$ $f(0)=m$ , $g(0)=n$ , $h(0)=r$ $$m^2+n^2+r^2=1$$ $$m+n+r=k$$ $$f(x)=\frac{m+n+r}{3}+\frac{2m-(n+r)}{3}\cos( \sqrt{3}x)+\frac{(n-r)}{\sqrt{3}}\sin( \sqrt{3}x)$$ $$g(x)=\frac{m+n+r}{3}+\frac{2n-(m+r)}{3}\cos( \sqrt{3}x)+\frac{(r-m)}{\sqrt{3}}\sin( \sqrt{3}x)$$ $$h(x)=\frac{m+n+r}{3}+\frac{2r-(m+n)}{3}\cos( \sqrt{3}x)+\frac{(m-n)}{\sqrt{3}}\sin( \sqrt{3}x)$$ $p=3$ $$f(x)^3+g(x)^3+h(x)^3=1 $$ $$f(x)^3+(g(x)+h(x))^3-3g(x)h(x)(g(x)+h(x))=1 $$ $$f(x)^3+(k-f(x))^3-3g(x)h(x)(k-f(x))=1 $$ $$k^3-3k^2f(x)+3kf(x)^2-3g(x)h(x)(k-f(x))=1$$ $$-3kf(x)(k-f(x))+k^3-1=3g(x)h(x)(k-f(x))$$ $$g(x)h(x)=-kf(x)+\frac{k^3-1}{3(k-f(x))}$$ $$f''(x)=2g'(x)g(x)-2h'(x)h(x)$$ $$f''(x)=2g(x)(h(x)^2-f(x)^2)-2h(x)(f(x)^2-g(x)^2)$$ $$f''(x)=2(g(x)h(x)-f(x)^2)(g(x)+h(x))$$ $$f''(x)=2(-kf(x)+\frac{k^3-1}{3(k-f(x))}-f(x)^2)(k-f(x))$$ $$f''(x)=2(-kf(x)+\frac{k^3-1}{3(k-f(x))}-f(x)^2)(k-f(x))$$ $$f''(x)=2(-kf(x)(k-f(x))+\frac{k^3-1}{3}-f(x)^2(k-f(x)))$$ $$f''(x)=2f(x)^3-2k^2f(x)+\frac{2(k^3-1)}{3}$$ if we put $k=m+n+r$ as shown above we can get the same  differential Equation that I found above Let's check out $p=4$ $$f(x)^4+g(x)^4+h(x)^4=1 $$ $$f(x)^4+(g(x)+h(x))^4-4g(x)h(x)(g(x)^2+h(x)^2)-6g(x)^2h(x)^2=1 $$ $$f(x)^4+(k-f(x))^4-4g(x)h(x)((k-f(x))^2-2g(x)h(x))-6g(x)^2h(x)^2=1 $$ $$f(x)^4+(k-f(x))^4-4g(x)h(x)(k-f(x))^2+2g(x)^2h(x)^2=1  \tag{12} $$ $$f'(x)=g(x)^3-h(x)^3$$ $$f''(x)=3g'(x)g(x)^2-3h'(x)h(x)^2$$ $$f''(x)=3g(x)^2(h(x)^3-f(x)^3)-3h(x)^2(f(x)^3-g(x)^3)$$ $$f''(x)=3g(x)^2h(x)^2(h(x)+g(x))-3f(x)^3(h(x)^2+g(x)^2)$$ $$f''(x)=3g(x)^2h(x)^2(k-f(x))-3f(x)^3((h(x)+g(x))^2-2g(x)h(x))$$ $$f''(x)=3g(x)^2h(x)^2(k-f(x))-3f(x)^3((k-f(x))^2-2g(x)h(x)) $$ $$f''(x)= -3f(x)^3((k-f(x))^2+3g(x)^2h(x)^2(k-f(x))+6g(x)h(x)f(x)^3 \tag{13} $$ I just need to combine Equation (12)  and (13) and cancel $g(x)h(x)$ but I noticed that the result looks ugly . It is not nice like $p=2$ and $p=3$ . Maybe I need to check $f'''(x)=a_0+a_1f(x)+....a_nf(x)^n$ for polynomial result for differential equation. Is there any idea to organize better way? I have not found general formula yet for when $p$ is any integer. Maybe someone can see a way for general case too. Thanks a lot for helps and advice.","['elliptic-functions', 'functions', 'elliptic-integrals']"
3686057,Does a solution to $ax + by \equiv 1$ imply the existence of a relatively prime solution?,"Context: In a paper I'm reading the author indirectly asserts that if $ax + by \equiv 1 \pmod n$ has infinitely many solutions $(x,y)$ with $x$ and $y$ relatively prime (we are given that the congruence has at least one solution). I can see why if we had a relatively prime solution $(p,q)$ , we would have infinitely many of them (take $(p, q + knp)$ for $k \in \mathbb Z$ ; the gcd of this pair does not depend on $k$ ). So the claim would be proven if I could also show that the existence of any solution $(p,q)$ implies the existence of a relatively prime one. In the spirit of the relatively prime case above, I tried considering the set $(p - kb, q + ka)$ with $k \in \mathbb Z$ , but I'm not sure how the gcd of the pair varies with $k$ . The ""obvious"" try of dividing out the gcd of $(p,q)$ doesn't work either, because the congruence is not always satisfied.","['number-theory', 'elementary-number-theory']"
3686151,"Jordan Curve Theorem, Professor Tao's proof","Here is Professor Terry Tao's proof of the Jordan curve theorem using complex analysis, I more or less followed the proof until the following paragraph (see section 4). (Actually there is no need to read everything before the following paragraph to answer the question.) https://terrytao.wordpress.com/2016/10/02/math-246a-notes-3-cauchys-theorem-and-its-consequences/ There are two things which I do not understand: firstly, how do we know the boundary of $\Omega_\delta$ consists of one or more simple closed curves? How to rigorously demonstrate that its boundary cannot be some random collection of segments? Also, why does the fact that the union of squares is connected implies the boundary of $\Omega_\delta$ consists of exactly one simple closed curve. Also the sentence on why the boundary of $\Omega_\delta$ is simple got me confused.(last sentence of the paragraph above the picture) secondly, what's the point of covering $N_{\epsilon/10}(\gamma([a,b]))$ ? Why not just cover $\gamma([a,b])$ ? Any explanation would be immensely appreciated!! Here $W$ denotes the winding number. $N_{\epsilon/10}(\gamma([a,b]))$ is the set of points $z\in\mathbb{C}$ such that $\text{distance}(z,\gamma)<\epsilon/10.$","['complex-analysis', 'algebraic-topology']"
3686160,Minimal number of balls in a cover of a compact set,"Let $K \subseteq \mathbb R^n$ be compact. Let $r>0$ . Can we cover $K$ by $N(r)$ balls of radius $r$ , centered around points that belong to $K$ , with $N(r) \le c \frac{1}{\text{Vol}(B(r))}$ ? Here $\text{Vol}(B(r))$ is the volume of an Eucldean ball of radius $r$ in $\mathbb R^n$ ; I want $c$ to be a constant which may depend on $K$ and on $n$ , but not on $r$ . This upper bound cannot be lowered-since if $K=\cup_{i=1}^N B_i$ , where all the $B_i$ are of radius $r$ , then $$
\text{Vol}(K)\le \sum_{i=1}^N {\text{Vol}(B_i)}=N\text{Vol}(B(r)).
$$ Here $\text{Vol}(K)$ refers to the Lebesgue measure of $K$ . I think that we can always cover $K$ by $\sim c \frac{1}{\text{Vol}(B(r))}$ balls if we don't care whether their centers lie in $K$ : Just take a cube which contains $K$ -and divide it into identical subcubes by putting a grid-now I guess we can replace the cubes with suitable balls and everything will be fine. What if we insist to use only balls centered at points that belong to $K$ ? Since $K$ can be arbitrarily complicated, I am not sure how to adapt this scheme. I tried googling various terms related to ""bounds on the covering number"", but failed to find an answer.","['measure-theory', 'geometry', 'real-analysis', 'combinatorics', 'compactness']"
3686182,How to Motivate Open-Cover Formulation of Compactness in a Metric Space?,"The open cover formulation of compactness always seemed to come out of nowhere for me. I've consulted many Analysis textbooks, but all of them have been like - 'Here's the open cover formulation, now we prove this and the sequential formulation are equivalent.' None of them actually go on to explain where this open cover formulation comes from. So, my question is this - suppose I was a researcher trying to come up with an open set formulation of compactness for the first time. All I know is Real Analysis, and I have defined a compact set as one in which a sequence has a convergent subsequence. How would I go about doing so?","['compactness', 'metric-spaces', 'analysis', 'real-analysis']"
3686199,"What is the reason for an ""irreducible"" (topological) space to be so called?","A topological space $T$ is  ""irreducible"" if and only if no two non-empty open sets of $T$ are disjoint. Such a space is also called ""hyperconnected"". This is the definition given in 1978: Lynn Arthur Steen and J. Arthur Seebach, Jr.: Counterexamples in Topology (2nd ed.) I personally prefer ""hyperconnected"" but have got shouted down on this subject by people who claim superior knowledge and authority. Hence the antithesis between this and an ""ultraconnected"" space, which is when no two non-empty closed sets are disjoint. So having been told that ""irreducible"" is better than ""hyperconnected"", what I need to understand is: what is the reason for calling it ""irreducible""? It predicates the notion that somehow sets which do have disjoint non-empty open sets are thereby ""reducible"". But reducible to (and from) what? In what way can an ""irreducible"" space not be ""reduced""? What is ""reduction"" in this context anyway? Wikipedia merely expands on this concept but does not explain the thinking.","['general-topology', 'terminology']"
3686238,Difference between derivative of a function at a point and limit of the differentiated function at that point,"$f'(x_0)\;=\lim\limits_{x \to x_0}\frac{f(x)-f(x_0)}{x-x_0}$ While solving a question: Prove that the function { $f(x)=\frac{|x|}{x}\; \; \forall x\neq0$ and $f(x) =0 \;\;\forall x=0$ } is not differentiable  at 0. I have done it by evaluating derivative from the left and right individually which gives $\infty$ . So, I have concluded that $f$ is not differentiable at 0. But I  noticed that $f'(x)=0\;\;\;\forall x\neq0$ . So, shouldn't the limit of $f'(x)$ be $0$ ? I know that I have made a false argument. Please tell me where is contradiction in my argument. Edit:  In short, what is the difference between $f'(0)$ and $\lim\limits_{x \to 0}f'(x)$ ?","['functions', 'derivatives', 'real-analysis']"
3686262,Ramanujan's infinite series for $\frac{x^3(3x-2)}{(2x-1)^3}$ for all positive integers $x$,"Here we go, yet another wild infinite series by Ramanujan. If $x$ is a positive integer, then $$1+3\Bigg(\cfrac{x-1}{x+1}\Bigg)^3\cfrac{3x-1}{3x-3}+5\Bigg\{\cfrac{(x-1)(x-2)}{(x+1)(x+2)}\Bigg\}^3\cfrac{(3x-1)(3x)}{(3x-3)(3x-4)}\,$$ $$+\,7\Bigg\{\cfrac{(x-1)(x-2)(x-3)}{(x+1)(x+2)(x+3)}\Bigg\}^3\cfrac{(3x-1)(3x)(3x+1)}{(3x-3)(3x-4)(3x-5)}+\cdots$$ $$=\cfrac{x^3(3x-2)}{(2x-1)^3}.$$ It appears that this result is almost entirely elementary. Perhaps from this infinite series' very peculiar form, some kind of hypergeometric series could be involved (and Ramanujan was very highly fond of that area of study). But, unless disguised, there does not seem to be any ""advanced"" functions (e.g. gamma functions $\Gamma$ ) involved, unlike his other similar infinite series. Is there an elementary, or strictly algebraic, proof (or close/similar derivation) of this theorem? Of course, Ramanujan does not share his methods.","['hypergeometric-function', 'sequences-and-series']"
3686268,"For any sequence of real numbers, one can always find a subsequence that is monotone","Homework Exercise: Let $(x_n)$ be ${\bf any}$ sequence of real
numbers. ${\bf carefully}$ , that is, from first principles, prove that
there exists a subsequence that is monotone. My sol: Let $x \in \mathbb{R}$ . Then, $(x_n)$ either converges to $x$ or not. So, we can do cases. ${\bf Case 1.}$ If $x_n \to x$ , then for any $\epsilon > 0$ one can take $N$ so that for all $n > N$ (in particular, for $n=n_1$ ) we have $|x_{n_1} - x|  < \epsilon $ Applying the definition again with $\epsilon = |x_{n_1} - x| > 0$ and taking $n = n_2 > n_1 > N$ we observe that $|x_{n_2} - x| < |x_{n_1} - x| $ Now, choose $\epsilon = |x_{n_2} - x| > 0$ and take $N > 0$ so that for all $n_3 > n_2 > n_1 > N$ one has $|x_{n_3} - x | < |x_{n_2} - x | $ If we continue in this fashion, we observe that for $n_k > n_{k-1} > ... > n_1$ we have $x_{n_1} < x_{n_2} < .... < x_{n_k} $ . In particular $(x_{n_k})$ is a monotone subsequence of $(x_n)$ ${\bf Case2.}$ Suppose $x_n$ not converges to $x$ . We know $\exists $ some $\epsilon > 0$ and some subsequence $(x_{n_k})$ so that $|x_{n_k}-x| \geq \epsilon$ $\forall k \in \mathbb{N}$ So, notice that $x_{n_k} - x \geq \epsilon \implies x_{n_k} \geq x + \epsilon $ . Also, $x_{n_{k+1} } - x < - \epsilon \implies -x_{n_{k+1}} >-x+\epsilon $ So that $x_{n_k} - x_{n_{k-1}} \geq 2 \epsilon > 0 $ so that $x_{n_k} > x_{n_{k+1}} $ and thus the subsequence is monotone. QED Is this a correct and 'careful' proof?","['calculus', 'solution-verification', 'sequences-and-series', 'real-analysis']"
3686293,Change along some direction is positive,"Let: $$f(x_1,\cdots,x_n) = \prod_{i}x_i(1-x_i) \prod_{i<j}|x_i-x_j|$$ Suppose all $x_i \in (0,1)$ are fixed and $\sum_{i}x_i < \frac{n}{2}$ .
Show that there is some $i$ and a sufficiently small $\epsilon$ so that $x_i \mapsto x_i +\epsilon$ doesn't decrease the value of $f$ . That is to say, at least one of the partial derivatives of $f$ is non-negative. After taking the derivative in each $x_i$ one gets a system of inequalities. I was able to prove the statement for $n=2,3$ this way through basically brute force. This doesn't generalize well though.","['multivariable-calculus', 'inequality']"
3686334,"Interpretation of the notation $x = (x_1,x_2)\in \{0,1\}^2$?","I have a few questions regarding the following notation: $$
x = (x_1,x_2)\in \{0,1\}^2
$$ Question 1: Is the following correct? $\{0,1\}^2$ is the Cartesian product of the 2 sets $\{0,1\}$ and $\{0,1\}$ , i.e. \begin{align}
\{0,1\}^2 &= \{0,1\} \times \{0,1\} \\
&= \{(0,0),(0,1),(1,0),(1,1)\}
\end{align} Question 2: With the notation we mean $``$$(x_1,x_2)$ is an element of the set $\{0,1\}^2$$``$ , so we can write: $$
(x_1,x_2)\in \{(0,0),(0,1),(1,0),(1,1)\}
$$ So $(x_1,x_2)$ can take the values \begin{align}
(x_1,x_2) &= (0,0)\\
(x_1,x_2) &= (0,1)\\
(x_1,x_2) &= (1,0)\\
(x_1,x_2) &= (1,1)
\end{align} ? Question 3: Does the notation mean that $(x_1,x_2)$ only can assign ONE value of $\{0,1\} \times \{0,1\}$ ? I.e. for $(x_1,x_2)$ we have 4 explicit cases: \begin{align}
(x_1,x_2) &= (0,0) \\
\text{or} \quad
(x_1,x_2) &= (0,1)\\
\text{or} \quad 
(x_1,x_2) &= (1,0)\\
\text{or} \quad 
(x_1,x_2) &= (1,1)
\end{align}","['elementary-set-theory', 'calculus', 'notation', 'real-analysis']"
3686380,Are there infinite prime divisors of sequence $a_{n}=2^{F_{0}}+2^{F_{1}}+\dots 2^{F_{n}}$,"We define the sequence of Fibonacci numbers $(F_n)_{n \geqslant 0}$ as follows: $$F_n=
\begin{cases}
0 & \text{if $n=0$} \\
1 & \text{if $n=1$} \\
F_{n-1}+F_{n-2}& \text{if $n>1$}
\end{cases}$$ The sequence goes as follows : $0,1,1,2,3,5,8,13,\ldots$ Furthermore, we define $(a_n)_{n \geqslant 0}$ as follows: $$a_n=\sum_{i=0}^n 2^{F_i} = 2^{F_0}+2^{F_1}+\ldots+2^{F_n}$$ The sequence goes as follows : $1,3,5,9,17,49,305,8497,\ldots$ Question : Are there infinitely many primes dividing some element of $(a_n)$ ? Haran's edit: I believe that this might be related to Kobayashi's Theorem : Let $M$ be an infinite set of positive integers such that the set of prime divisors of the numbers in $M$ is finite. Then, the set of primes dividing the numbers in the set: $$M+a=\{m+a \mid m \in M \}$$ is infinite, where $a$ is a non-zero fixed integer.","['number-theory', 'fibonacci-numbers']"
3686389,Computing $\int_Q \frac{xy}{x^2+y^2}dxdy$,"I'm asked to compute the following integral: $$\int_Q \frac{xy}{x^2+y^2}dxdy \qquad Q=[0,1]^2$$ Solution: First I'm going to study if the integral is convergent. To do this, we notice that $$\int_Q \frac{xy}{x^2+y^2}dxdy < \infty \iff \int_S \frac{xy}{x^2+y^2}dxdy<\infty$$ where $S=\{(x,y) \in \mathbb{R}^2 | x\geq0, y\geq 0, x^2+y^2\leq1\}$ and this is clear because the difference between $\int_Q f(x,y)dxdy$ and $\int_S f(x,y)dxdy$ is a proper integral. Computing we have: $$\int_S \frac{xy}{x^2+y^2}dxdy=\lim_{\epsilon \to 0}\int_{\epsilon}^1\int_0^{\frac{\pi}{2}}\rho^3\sin\theta\cos\theta d\theta d\rho=\frac{1}{8}$$ and so I'm granted the convergence. Now I have to compute the real integral, as we have assured that it's convergent. $$\int_Q \frac{xy}{x^2+y^2}dxdy=\int_0^1 \int_0^1 \frac{xy}{x^2+y^2}dxdy= \int_0^1 \frac{y}{2} \int_0^1 \frac{2x}{x^2+y^2}dxdy = \frac{1}{2} \int_0^1 y(log(1+y^2)-log(y^2))dy =$$ $$ = \frac{\log2}{2}-\frac{1}{2}\lim_{\epsilon \to 0}\int_{\epsilon}^1y\log(y^2)dy= \frac{\log2}{2}$$ I checked the result and it's correct, but I'm asking for a review of the process: did I do anything wrong?","['integration', 'improper-integrals', 'multivariable-calculus', 'calculus', 'solution-verification']"
3686425,"Regarding equivalent definitions of Euclidean Submanifolds in Gallot, Hulin and Lafontaine's book Riemannian Geometry","In the book Riemannian Geometry by Gallot, Hulin and Lafontaine, a proposition which characterises equivalent definitions of submanifolds is given as follows: 1.3 Proposition The following are equivalent: i) $M$ is a $C^p$ submanifold of dimension $n$ of $\mathbb{R}^{n+k}$ . ii) For any $x$ in $M$ , there exist open neighbourhoods $U$ and $V$ of $x$ and $0$ in $\mathbb{R}^{n+k}$ respectively, and a $C^p$ diffeomorphism $f : U\to V$ such that $f(U \cap M)=V \cap (R^n \times \{0\})$ . iii) For any $x$ in $M$ , there exist a neighbourhood $U$ of $x$ in $\mathbb{R}^{n+k}$ , a neighbourhood $\Omega$ of $0$ in $\mathbb{R}^n$ , and a $C^p$ map $g: \Omega \to \mathbb{R}^{n+k}$ such that $( \Omega, g)$ is a local parametrization of $M \cap U$ around $x$ (that is $g$ is an homeomorphism from $\Omega$ onto $M \cap U$ and $g'(0)$ is injective). I am trying to show that iii) implies ii) and I think that I am nearly there except I am having trouble with the following detail. I will first summarise my problem and then fill in the details. In brief, my main problem in going from iii) to ii) is that to make my proof work I seemed to also require that $g(0)=x$ . (Whereas in contrast the authors have just required that $g$ is an homeomorphism from $\Omega$ onto $M \cap U$ and $g'(0)$ is injective). The details of what I attempted are as follows: (Attempted) Proof that iii) $\implies$ ii). Fix $x \in M$ and let $M \subseteq \mathbb{R}^{n+k}$ satisfy the conditions of iii). Therefore we have a neighbourhood $W$ of $x$ in $\mathbb{R}^{n+k}$ , a neighbourhood $\Omega$ of $0$ in $\mathbb{R}^n$ , and a $C^p$ map $g: \Omega \to \mathbb{R}^{n+k}$ such that $g$ is an homeomorphism from $\Omega$ onto $M \cap W$ and $g'(0)$ is injective. Since the Jacobian matrix $g'(0)$ or $Dg$ (at $0$ ), which has $n+k$ rows and $n$ columns, is injective 
  (in this case rank $n$ ), that means that all of its $n$ columns are linearly independent. Therefore we could 'complete' or 'fill out' this matrix up to a full square matrix which has rank $n+k$ (i.e. nonsingular). The filled out matrix is: \begin{bmatrix}
    \frac{\partial g_1}{\partial x_1} & \frac{\partial g_1}{\partial x_2} & \dots \frac{\partial g_1}{\partial x_n} & a_{11} & \dots  & a_{1k} \\
\frac{\partial g_2}{\partial x_1} & \frac{\partial g_2}{\partial x_2} & \dots \frac{\partial g_2}{\partial x_n} & a_{21} & \dots  & a_{2k} \\
\frac{\partial g_3}{\partial x_1} & \frac{\partial g_3}{\partial x_2} & \dots \frac{\partial g_3}{\partial x_n} & a_{31} & \dots  & a_{3k} \\
   \vdots & \vdots & \vdots &\vdots & \dots & \vdots \\
\frac{\partial g_n}{\partial x_1} & \frac{\partial g_n}{\partial x_2} & \dots \frac{\partial g_n}{\partial x_n} & a_{n1} & \dots  & a_{nk} \\
    \vdots & \vdots & \vdots &\vdots & \dots & \vdots \\
\frac{\partial g_{n+k}}{\partial x_1} & \frac{\partial g_{n+k}}{\partial x_2} & \dots \frac{\partial g_{n+k}}{\partial x_n} & a_{(n+k)1} & \dots  & a_{(n+k)k}
\end{bmatrix} With this in mind we can define a new function $h : \Omega \times \mathbb{R}^k \to \mathbb{R}^{n+k}$ by: $h_1=g_1(x_1,x_2,...,x_n) + a_{11} x_{n+1} + a_{12} x_{n+2} + ... + a_{1k} x_{n+k}$ etc. Then $Dh$ at $0$ is just the matrix above. Therefore, by the inverse function theorem, there is a neighbourhood $V$ of $0$ such that $h$ carries $V$ in a one to one fashion onto an open set $U$ of $\mathbb{R}^{n+k}$ and we can guarantee that $h(0)$ (which is equal to $g(0)$ ) is in $U$ . This is where I think my problem arises. Although the original $W$ contained $x$ , I can't seem to guarantee that $U$ contains $x$ because the whole process to show that $U$ exists has relied upon the inverse function theorem, which could return a smaller open set than $W$ . Therefore I am not certain I am on the correct track, but apart from this little detail, if I could assume that $g(0)=x$ , I think this would all work. Help would be much appreciated as I am attempting to self-learn with no mathematical contacts at the moment.","['differential', 'manifolds', 'riemannian-geometry', 'differential-geometry']"
3686458,"Can we justify the behavior ""taking derivative w.r.t. a variable""?","I have come across many times about this situation, which indicates that derivative of a function can be dependent on the ""variable"" that you are concerned with, e.g. one of the expression of the Chain Rule:(assume that all following functions are differentiable) $$
\frac{dy}{dx}= \frac{dy}{du} \cdot \frac{du}{dx}
$$ What does $\displaystyle \frac{dy}{dx}$ and $\displaystyle \frac{dy}{du}$ mean? But with the other expression, if we can write $y=f \circ u$ , then $y'=(f' \circ u)\cdot u'$ . And by contrasting them, we know that $\displaystyle y'= \frac{dy}{dx}$ and $\displaystyle \frac{dy}{du} = f' \circ u, \frac{du}{dx} = u'$ . In my opinion, $\displaystyle \frac{dy}{du}$ tends to misleading people, since we are using functions $f$ and $u$ rather than function $y$ , and I  think the derivative of a function is uniquely determined, regardless of its ""variable"". So, are there insights with which one can justify this behabior?","['derivatives', 'real-analysis']"
3686474,Investment problem with strange answer.,I tried solve that problem and for me the maximum income is $3000$ because $1000\times 3$ . But the answer is $3600$ ! Can anybody explain me how it works? Thanks James has $\$1000$ and wants to invest it in a project. He knows that each dollar brings $\$2$ income per month. He may rent a number of billboards for $\$100$ each. Each billboard increases one dollar’s income by $\$1$ per month. Find the James’ maximal total month income. Write your answer in dollars.,"['optimization', 'algebra-precalculus']"
3686562,"There are no semisimple Lie algebras of dimension $4$, $5$, or $7$","I came across the claim here which states that there are no complex semisimple Lie algebras of dimension $4$ , $5$ , or $7$ . As the problem suggests, we can take a Cartan subalgebra $H$ and root system $\Phi$ so that $L=H\oplus(\oplus_{\alpha\in\Phi}L_\alpha)$ where $$L_\alpha=\{x\in L:(\forall h\in H)\ [h,x]=\alpha(h)x\}$$ As shown in Humphreys' Introduction to Lie Algebras, every $L_\alpha$ has dimension $1$ , so $\dim(L)=\dim(H)+\vert\Phi\vert$ . I know $\Phi$ spans $H^*$ , so $\dim(H)\leq\vert\Phi\vert$ . Furthermore, I know that for any $\alpha\in\Phi$ , the only multiples of $\alpha$ in $\Phi$ are $\alpha$ and $-\alpha$ . Since $\mathbb{C}$ has characteristic $0$ , this implies $\vert\Phi\vert$ is even. Finally, another fact which may be useful is that given nonzero vectors $x_\alpha\in L_\alpha$ and $y_\alpha\in L_{-\alpha}$ , if we take $h_\alpha=[x_\alpha,y_\alpha]\in H$ , then $x_\alpha$ , $y_\alpha$ , and $h_\alpha$ span a three-dimensional subspace of $L$ isomorphic to $\mathfrak{sl}(2,\mathbb{C})$ . However, I am not sure how to connect these pieces to give the desired conclusion. As a follow-up question, would the claim still be true if I replaced the field with an arbitrary field of characteristic $0$ , or is it also necessary for the field to be algebraically closed?","['semisimple-lie-algebras', 'abstract-algebra', 'lie-algebras', 'root-systems']"
3686625,Finding the expected value of coin flip experiment (Dark Souls problem),"I'm trying to find the formula for an expected value, but testing has shown that my formula is incorrect. Can you find either a closed form or recursive formula for the expected values in this experiment, and explain how you found that formula? The Game You have $n$ coins labelled $1$ through $n$ . We will label the set of coins $[n]$ . We have a function $f$ that maps coin $i$ to the probability coin $i$ lands heads. A coin landing heads is a success. A coin landing tails is a failure. We start with flipping coin $1$ . If coin $1$ lands heads, we move on to coin $2$ . If coin $1$ lands tails, we start over with coin 1. On coin $i$ , if coin $i$ lands heads, we move on to coin $i+1$ . If coin $i$ lands tails, we return to coin $1$ . If we land heads on coin $n$ , we win and the game ends. The Variables and Counters We will be keeping track of $n$ variables $\{ X_1, X_2, \ldots , X_n \}$ . Each of these will count some, but not all, of the times we fail coin $n$ . To make the explanation easier, we will treat these as counters in an algorithm, and their final values will be the random variables. If we flip heads on coin $i$ , we do nothing. If we flip tails on coin $i$ , one of two things happens. If the last coin we failed, $j$ , appeared later than $i$ (that is, $j > i$ ), then we reset all counters $\{ X_1, X_2, \ldots , X_n \}$ to $0$ and then set $X_i=1$ . If the last coin we failed, $j$ , appeared earlier than or is equal to $i$ (that is, $j \leq i$ ), then we increment $X_i$ by $1$ and move back to coin $1$ as described in the rules. Edit: If we have not failed a coin previously, then we do $(2)$ . That is, we increment $X_i$ by $1$ and move back to coin $1$ without changing any other counters. The Problem We want to find the expected value of each $X_i$ after successfully flipping heads on coin $n$ . My Solution It's hard to put my solution in math terms, because my solution is wrong. I write the expansion $$E(X_i) = 0*P(X_i = 0) + 1*P(X_i = 1) +  2*P(X_i = 2) + 3*P(X_i = 3) + \ldots \text{.}\tag{1}$$ My intuition then leads me to say $$P(X_i=k+1) = P(X_i=k)*(1-f(i))*\prod_{j = 1}^{j=i-1}f(j)\text{.}\tag{2}$$ The above, $(2)$ , is probably wrong, but here is the intuition. $(1-f(i))$ represents the probability of an extra failed flip of coin $i$ at a particular point in the game followed by successful coin-flips on coins $1$ through $i-1$ represented by the term $\prod_{j = 1}^{j=i-1}f(j)$ . For notational convenience, we set $x=(1-f(i))*\prod_{j = 1}^{j=i-1}f(j)$ . We can then use $P(X_i =0) + P(X_i=1) + P(X_i=2) +\ldots = 1$ to find that $$P(X_i=k)= (1-x)*x^{k} \tag{3}$$ and then $$\mathbb{E}(X_i) = \frac{x}{1-x}\text{.}\tag{4}$$ Displays $(3)$ and $(4)$ are just algebra, and even if there is a typo, I'm fairly certain that is not where the problem lies. Data If $n=5$ and $f(i)=.5$ for all $i$ , then some approximations of the expected values are $\mathbb{E}(X_1) = 1.56902$ , $\mathbb{E}(X_2)= .4036$ , $\mathbb{E}(X_3)= .15541$ , $\mathbb{E}(X_4)= .06952$ , and $\mathbb{E}(X_5)= .03185$ . These were found by running experiments in C++. I provide these so you can test your formulas against the actual values. My formula in display $(4)$ will give you values $\mathbb{E}(X_2)= \frac{1}{3}$ , $\mathbb{E}(X_3)= \frac{1}{7}$ , $\mathbb{E}(X_4)= \frac{1}{15}$ , and $\mathbb{E}(X_5)= \frac{1}{31}$ . Note I really don't care about $\mathbb{E}(X_1)$ , so my rules, formula, and data may not all match up for that case.","['expected-value', 'probability']"
3686666,Representation of a bounded linear operator $T: c \to c$.,"Let $T: c\to c$ be a bounded linear operator, where $c$ is the vector space of convergent real sequences. How can we prove that there exists an infinite matrix $A=(a_{n,k}: n,k\ge 1)$ such that $T(x)=Ax$ for all $x \in c$ ? The proof is very easy replacing $c$ in both places with $c_0=\{x \in c: \lim_n x_n=0\}$ (indeed, in such case, $\{e^i: i\ge 1\}$ is a Schauder basis for $c_0$ , where $e^i \in c$ is the sequence which is constantly equal to $0$ , except $e_i^i=1$ ).","['summability-theory', 'functional-analysis', 'riesz-representation-theorem']"
3686751,Logarithm of a complex function on a non-simply connected space,"It can be proven that on a simply connected set $U$ in $\mathbb{C}$ where a function $f$ has no zeroes there is a function $g$ such that $e^g = f$ on $U$ . This is done by observing that $\frac{f'}{f}$ is holomorphic and therefore has a ""primitive"". I suspect that the requirement that $U$ be simply connected is sufficient, but not required. This comes from an exercise I've been trying to solve: Consider $U=\mathbb{C} \setminus \{ -1,1 \}$ . This set is obviously not
  simply connected, and the function $f(z) = z^2 -1$ has no zeroes on
  it. Does there exist an holomorphic function $h$ on $U$ such that $e^{h(z)}=f(z)$ on $U$ ? Is there really such a function or is simple-connectedness necessary?","['complex-analysis', 'connectedness', 'logarithms']"
3686921,"Prove that for all $\alpha + \beta + \gamma = \pi$, $\sum_{cyc}\frac{\sin\beta}{\cos\beta + 1} = \frac{\sum_{cyc}\cos\beta + 3}{\sum_{cyc}\sin\beta}$.","Prove that for all triangles with angles $\alpha, \beta, \gamma$ , $$\frac{\sin\alpha}{\cos\alpha + 1} + \frac{\sin\beta}{\cos\beta + 1} + \frac{\sin\gamma}{\cos\gamma + 1} = \frac{\cos\alpha + \cos\beta + \cos\gamma + 3}{\sin\alpha + \sin\beta + \sin\gamma}$$ Let $\tan\dfrac{\alpha}{2} = a, \tan\dfrac{\beta}{2} = b, \tan\dfrac{\gamma}{2} = c$ , we have that $$\dfrac{\sin\beta}{\cos\beta + 1} = \dfrac{1}{b}, \cos\beta = \dfrac{1 - b^2}{1 + b^2}, \sin\beta = \dfrac{2b}{1 + b^2}$$ and $bc + ca + ab = 1$ . It needs to be proven that $$\frac{1}{a} + \frac{1}{b} + \frac{1}{c} = \frac{\dfrac{1 - a^2}{1 + a^2} + \dfrac{1 - b^2}{1 + b^2} + \dfrac{1 - c^2}{1 + c^2} + 3}{\dfrac{2a}{1 + a^2} + \dfrac{2b}{1 + b^2} + \dfrac{2c}{1 + c^2}}$$ $$\impliedby \frac{1}{a} + \frac{1}{b} + \frac{1}{c} = \frac{\dfrac{1}{1 + a^2} + \dfrac{1}{1 + b^2} + \dfrac{1}{1 + c^2}}{\dfrac{a}{1 + a^2} + \dfrac{b}{1 + b^2} + \dfrac{c}{1 + c^2}}$$ $$\impliedby \left(\frac{1}{a} + \frac{1}{b} + \frac{1}{c}\right)\left(\frac{a}{1 + a^2} + \frac{b}{1 + b^2} + \frac{c}{1 + c^2}\right) = \frac{1}{1 + a^2} + \frac{1}{1 + b^2} + \frac{1}{1 + c^2}$$ $$\impliedby \left(\frac{a}{b} + \frac{a}{c}\right)\frac{1}{1 + a^2} + \left(\frac{b}{c} + \frac{b}{a}\right)\frac{1}{1 + b^2} + \left(\frac{c}{a} + \frac{c}{b}\right)\frac{1}{1 + c^2} = 0$$ $$\impliedby \frac{a(b + c)}{bc(c + a)(a + b)} + \frac{b(c + a)}{ca(a + b)(b + c)} + \frac{c(a + b)}{ab( b + c)(c + a)} = 0$$ $$\impliedby \frac{1 - bc}{(1 - ca)(1 - ab)} + \frac{1 - ca}{(1 - ab)(1 - bc)} + \frac{1 - ab}{(1 - bc)(1 - ca)} = 0$$ $$\impliedby (1 - bc)^2 + (1 - ca)^2 + (1 - ab)^2 = 0$$ $$\impliedby bc = ca = ab = 1 \impliedby bc + ca + ab = 3,$$ which is definitely incorrect. I've surmised that the correct equality is $$\frac{\sin\alpha}{\cos\alpha + 1} + \frac{\sin\beta}{\cos\beta + 1} + \frac{\sin\gamma}{\cos\gamma + 1} = \frac{\cos\alpha + \cos\beta + \cos\gamma + 1}{\sin\alpha + \sin\beta + \sin\gamma},$$ but then I wouldn't know what to do first.","['algebra-precalculus', 'trigonometry']"
3686935,"Simplify $\frac{\prod\limits_{n=1}^{45} \cos(2n-1)}{\prod\limits_{n=1}^{45} \sin(4n-2)}$, measured in degrees [duplicate]","This question already has answers here : Continued product in $\sin$ series (2 answers) Closed 4 years ago . Absolutely clueless, any help would be appreciated, especially if it can be understood by a grade $12$ student (me). The answer is in the form $2^x$ , and we are supposed to find value of $x$ .","['trigonometry', 'sequences-and-series']"
3687008,Elementary proof that $\pi$ is transcendental,"A popular (and maybe the only) approach to showing that $\pi$ is transcendental is to first prove that for every non-zero algebraic number $a$ , the number $e^a$ is transcendental. That requires tools from complex analysis. But is there a known elementary proof that $\pi$ is transcendental? By an elementary proof I mean proof that does not use complex analysis. For example, there are known proofs that $e$ is transcendental which do not use complex analysis. Also, can it be proved that complex analysis must be used to prove some given theorem?","['complex-analysis', 'pi', 'transcendental-numbers']"
3687043,Counting the roots of $f(z)=z^4+z^3-2z^2+2z+4$,"I have a polynomial $f(z)=z^4+z^3-2z^2+2z+4$ , and I want to find the number of roots in the first quadrant. I'm trying to use the argument principle (or Rouche), and I could try to make my contour the quarter circle, but I've having trouble because I can't justify that there are no roots on the real axis. Please give me some recommendations! So now I understand why there are no roots on the contour; I have also justified that the integral on the arc goes to $2\pi i$ by normal limit considerations. However, I still am unsure how to figure out the arguments.","['complex-analysis', 'roots', 'polynomials']"
3687044,Reading the notation $f(x)=y$ in the context of dependent and independent variables.,"With having a deeper understanding of functions, I am now revisiting using functions in topics such as physics and have a question regarding the notation “ $f(x)=y$ “ for a function $f: X \to Y$ (Where $x \in X$ and $y \in Y$ ). When we say “ $f$ depends on $x$ ” are we really saying “the value of $f$ depends on $x$ “? With that said, since the value of $f$ is understood to be $y$ , doesn’t that mean $y$ depends on $x$ ?","['elementary-set-theory', 'notation']"
3687081,1D Random walk and Dyck paths,"I am trying to answer this question : How can we determine the proportions of a box, only by listening to the ""noise"" it emits, when a particle evolving randomly inside this box bounces on its walls. English isn't my first langage, I hope this is well formulated. For now, let's play with only 1-dimension. My particle starts at $x=0$ and evolve with descrete steps $x+1$ or $x-1$ (the first step must be $x+1$ ).
When the particle hits a ""wall"", it emits a sound $i$ when $x=0$ and a sound $a$ when $x=X$ , with $X$ the lenght of the 1D box, it emits the sound $0$ otherwise. For example, we could ""hear"" $(i, 0, 0, 0, i, 0, 0, 0,0, a, 0, 0, 0, 0, i)$ . By hypothesis, the particle must start and finish at $x=0$ . I know that these kind of trajectories can be well described with Dyck paths (here, all Dyck paths have the same probability to happen). For now, I'm trying to figure out how to calculate the averages $I_n(X)$ and $A_n(X)$ of $i$ 's and $a$ 's as a function of the lenght X. If X=1, the only path we can have is $(i, a, i, a, ..., a, i)$ , so if $n$ is big enough, we have $I_n(1)$ = $A_n(1)$ = $\frac{1}{2}$ If X=2, for $n$ big enough, I find by drawing that $I_n(2)$ = $A_n(2)$ = $\frac{1}{4}$ , and 50% of silence How can I proove this and more important, how can I figure out for all $X$ ?
The only pattern I can see here is $I_n(X)$ = $A_n(X)$ = $\frac{1}{2Cat(X)}$ but I'm pretty sure this is stupid. Here Cat(X) is the $X$ th Catalan number. Any help about how to structurate my ideas would be awesome. Have a good day!","['random-walk', 'combinatorics', 'discrete-mathematics']"
3687082,Proof of the identity $\prod_{j=1}^n (1-q^{3j-2})(1-q^{3j-1})=\sum_{j=-n}^n (-1)^jq^{j(3j+1)/2}\binom{2n}{n+j}_{q^3}$,"I came across the following identity in when I was reading my class lecture notes on Combinatorics.
. It says using the $q$ -binomial theorem, we can deduce that $$\prod_{j=1}^n (1-q^{3j-2})(1-q^{3j-1})=\sum_{j=-n}^n (-1)^jq^{j(3j+1)/2}\binom{2n}{n+j}_{q^3}.\tag{1}$$ Here $\binom{n}{k}_q$ denotes the $q$ -binomial coefficient, which is defined by $$
\binom{n}{k}_q:=\frac{(1-q^n)(1-q^{n-1})\cdots (1-q^{n-k+1})}{(1-q^k)(1-q^{k-1})\cdots (1-q)}.$$ The $q$ -binomial theorem is the following identity $$
(1+x)(1+xq)\cdots (1+xq^{n-1})=\sum_{j=0}^n \binom{n}{j}_q q^{j(j-1)/2}x^j.\tag{2}$$ I tried to use (2) to deduce (1) but cannot complete it. The steps are described as follows.
By the $q$ -binomial theorem, we have $$\prod_{j=1}^n(1-q^{3j-2})=\sum_{j=0}^n (-1)^j q^j q^{3j(j-1)/2}\binom{n}{j}_{q^3}$$ and $$
\prod_{j=1}^n(1-q^{3j-1})=\sum_{j=0}^n (-1)^j q^{2j} q^{3j(j-1)/2}\binom{n}{j}_{q^3}$$ Therefore, $$
\prod_{j=1}^n (1-q^{3j-2})(1-q^{3j-1})=\sum_{j=0}^{2n} (-1)^j\sum_{j_1+j_2=j} q^{j_1(3j_1-1)/2}\binom{n}{j_1}_{q^3}q^{j_2(3j_2+1)/2}\binom{n}{j_2}_{q^3}.$$ I am stuck here. I do not know how to transform the above expression into (1).","['combinatorics', 'polynomials', 'generating-functions']"
3687083,"$ \int\frac{g(x)}{f(x)} \, dx $ where $f(x) = \frac{1}{2}(e^x+e^{-x})$ and $g(x) = \frac{1}{2}(e^x - e^{-x})$?","Given two functions $f(x) = \frac{1}{2}(e^x+e^{-x})$ and $g(x) = \frac{1}{2}(e^x - e^{-x})$ , calculate $$ \int\frac{g(x)}{f(x)} \, dx. $$ Observing that $f'(x) = g(x)$ , this is very easy to do. Just take $u = f(x)$ and therefore $du = f'(x)\,dx$ . Now we have $$
\int \frac{f'(x)}{f(x)} \, dx = \int \frac{du}{u} = \ln |u| + C = \ln\left|\frac{1}{2}(e^x+e^{-x})\right|+C.
$$ However, this solution is wrong. The correct one is $\ln|e^x+e^{-x}| + C$ . I see how we could get that, just plug in $g(x)$ and $f(x)$ directly, and we get $$
\int \frac{\frac{1}{2}(e^x - e^{-x})}{\frac{1}{2}(e^x+e^{-x})} \, dx.
$$ Cancelling the $\frac{1}{2}$ 's and taking $u = e^x+e^{-x}$ and $du = (e^x-e^{-x}) dx$ gives us $$
\int \frac{du}{u} = \ln|u| + C = \ln|e^x+e^{-x}| + C.
$$ I can't see where I made a mistake in my approach, and I'd be really grateful if you pointed it out.","['integration', 'indefinite-integrals']"
3687106,Visualizing derivative of a matrix-valued function of a matrix variable,"Apologies if this is not at an appropriate level for this site or if it's too broad/scrambled of a question, but I was wondering how best to visualize a matrix-valued function of a matrix variable? For some context: in my single variable calculus class, we were taught that the derivative of $f(x)$ at a point $x = a$ is $f'(a)\in\mathbb{R}$ . This represents the slope of the tangent line to the graph of $f(x)\vert_{x = a}$ . The equation of this tangent line is $y = f'(a)(x-a) + f(a)$ , and this is the best linear approximation of $f(x)$ near $x = a$ . In my analysis class, we were taught that the derivative of a map $f:U\to F$ , where $U\subseteq E$ is open, $x\in U$ , and $E,F$ are complete normed linear spaces, is a continuous linear map $\lambda=f'(x):E\to F$ satisfying $$f(x+h)-f(x)=f'(x)(h)+|h|\psi(h) $$ with $\lim_{h\to 0}\psi(h)=0$ and $\psi(0)=0.$ There is no confusion here. This definition is in accordance with our definition from single variable calculus, it's a special case as expected, where the standard matrix of this linear transformation (in the one dimensional case) is the $1\times 1$ matrix $[f'(a)]$ . If we consider the special case of our scenario where $E$ and $F$ are Euclidean spaces rather than arbitrary complete normed linear spaces, i.e. for $f : \mathbb{R}^n \to \mathbb{R}^m$ at a point $a \in \mathbb{R}^n$ then the derivative $Df(a)$ is an $m\times n$ matrix $Df(a) = \left[\frac{\partial f_i}{\partial x_j}( a)\right]$ , or the Jacobian of $f$ at $a$ . On our exam, we were asked to compute the derivative of $f:M_{n\times n}(\mathbb{R})\to M_{n\times n}(\mathbb{R}) $ given by $f(x)=x^3$ . This was a fairly straightforward computational task, however I have been stuck about how to visualize/understand my answer. I know you could just say that $M_{n\times n}(\mathbb{R})$ and $\mathbb{R}^{n^2}$ are isomorphic as complete normed linear spaces, so think of it in terms of the framework that has already been established. My question is: I know how to visualize/conceptualize this exam question when $f$ is a real-valued function of a real variable, but in the case from the exam is there a way to understand it in terms of what a linear map actually does (some combination of rotation and scaling) or where the vectors that comprise the matrix live with respect to the function (in some tangent subspace would be my intuition, but I'm not sure since I don't have that level of background in such a subject)? Is there a meaningful way to answer/subject to address my question or is the whole matter moot since even for a $3\times 3$ matrix, that would correspond to a function of $9$ variables, which we cannot visualize?","['visualization', 'real-analysis', 'matrices', 'linear-algebra', 'derivatives']"
3687196,Question on quotient groups,"I know that $\mathbb{Z}/n\mathbb{Z}$ stands for the quotient group of integers mod $n$ . To be a little more specific, we define the relation: $$a \equiv b \hspace{0.1cm} (\mbox{mod $n$}) \iff n\mid (a-b)$$ and this is an equivalence relation. Thus, $\mathbb{Z}/n\mathbb{Z}$ is defined to be the set of all equivalence classes of this relation. We can prove that: $$\mathbb{Z}/n\mathbb{Z} = \{[0],...,[n-1]\}$$ This being said, I would like to understand the meaning of $\mathbb{Z}^{d}/n\mathbb{Z}^{d}$ and $\mathbb{R}^{d}/n\mathbb{Z}^{d}$ . I'm having trouble understanding these objects because I don't know how to define 'divisibility' in $\mathbb{Z}^{d}$ and $\mathbb{R}^{d}$ . Do we need to demand componentwise divisibility? How to define these groups?","['quotient-group', 'group-theory', 'abstract-algebra']"
3687217,Is The Composition of Two Linear Transformations Invertible,"Assume we have two linear compositions $T: \mathbb{R}^n \rightarrow \mathbb{R}^n$ and $S: \mathbb{R}^n \rightarrow \mathbb{R}^n$ . How would I go about proving that $S \circ T$ ( Composition of $S$ and $T$ ) is an invertible linear transformation T(x1,x2)=(x1-x2,3x1-2x2)
S(x1,x2)=(2x1+3x2,-x1+x2)","['matrices', 'linear-algebra', 'linear-transformations']"
3687230,Need help defining an arc,"I'm trying to form similar arcs (like the blue one) to join points X-Z, and A-B at both ends of the line BX in the following picture. XY and YZ are unknown but equal. How do I find the radius of the arc and the exact position of BX relative to the rest of the shape?",['geometry']
3687237,First sheaf cohomology of closed 1-forms on a homogeneous space,"Given a smooth quasi-projective variety $X$ over $\mathbb C$ , is there a good way to compute the first sheaf cohomologiy $H^1(X,\Omega_{X,cl}^1)$ of closed 1-forms $\Omega_{X,cl}^1$ ? What I'm mostly interested in is the case where $X = G/P$ is a homogeneous space of an algebraic group $G$ , where $P$ is a closed subgroup. 
The case where $G$ is reductive and $P$ is a parabolic subgroup (so $X$ is projective) is my primary concern, but it'll be nice if a description for all algebraic groups can be found. Motivation: $H^1(X,\Omega_{X,cl}^1)$ parametrizes all twisted sheaves of differential operators on $X$ . Some thoughts in the case where $G$ is reductive and $P = B$ is a Borel subgroup that may or may not help: We know $H^i(X,\Omega_X^1) = 0$ if $i \neq 1$ and is $\mathbb{C}^{\mathrm{rank} G}$ if $i =1$ . So the long exact sequence induced by $0 \to \Omega_{X,cl}^1 \to \Omega_X^1 \to \Omega_X^1/\Omega_{X,cl}^1 \to 0$ reduces to $$0 \to H^0(X,\Omega_X^1/\Omega_{X,cl}^1) \to H^1(X,\Omega_{X,cl}^1) \to H^1(X,\Omega_X^1) \to H^1(X,\Omega_X^1/\Omega_{X,cl}^1) \to H^2(X,\Omega_{X,cl}^1) \to 0,$$ but we need more knowledge of cohomologies of $\Omega_X^1/\Omega_{X,cl}^1$ to proceed. Homogeneous twisted sheaves of differential operators (htdo) are parametrized by $\mathfrak{b}/[\mathfrak{b},\mathfrak{b}]$ which has dimension $\mathrm{rank}G$ . Therefore $H^1(X,\Omega_{X,cl}^1) \to H^1(X,\Omega_X^1)$ cannot be a proper injection. If we view $X$ as the variety of all Borel subalgebras of $\mathfrak g$ , and let $\mathcal{N} \subset \mathcal{B} \subset X \times \mathfrak g$ to be subbundles of $X \times \mathfrak g$ whose fibers at a point $x$ (which corresponds to a Borel subalgebra $\mathfrak b_x$ ) are $[\mathfrak{b}_x,\mathfrak{b}_x]$ and $\mathfrak b_x$ , respectively, then we have an exact sequence of vector bundles $0 \to \mathcal B \to X \times \mathfrak g \to TX \to 0$ and the corresponding sequence $0 \to \mathfrak b^\circ \to \mathfrak g^\circ \to \mathcal T_X \to 0$ of sheaves; similarly if $\mathfrak n^\circ$ is the sheaf of $\mathcal N$ then $$\Omega_X^1 \cong \mathfrak n^\circ = \big\{\sum_i h_i \otimes \xi_i \in \mathcal O_X \otimes_{\mathbb C} \mathfrak  g = \mathfrak g^\circ \mid  \sum_i h_i(x) \otimes \xi_i \in [\mathfrak{b}_x,\mathfrak{b}_x] \big\}.$$ Translating the pairing $\mathcal T_X \otimes_{\mathcal O_X} \Omega_X^1 \to \mathcal O_X$ and the definition of exterior derivative on $\Omega_X^1$ , one can write down a condition for a section $\sum_i h_i \otimes \xi_i \in \mathfrak n^\circ$ to be closed, namely that for any $\eta_1,\eta_2 \in \mathfrak g$ (viewed also as global vector fields on $X$ ), the function $$\sum_i \Big(  B(\xi_i,\eta_2) \eta_1(h_i) -  B(\xi_i,\eta_1) \eta_2(h_i) - B(\xi_i,[\eta_1,\eta_2]) h_i \Big) \in \mathcal O_X$$ vanish, where $B(-,-)$ is the Killing form. But I don't know how to proceed. Thanks in advance for any help.","['sheaf-cohomology', 'de-rham-cohomology', 'algebraic-geometry', 'lie-groups', 'differential-forms']"
3687248,Why does the set of countable sets of $\mathbb R$ does not satisfy the conditions of Zorn's lemma?,"In the subject of set theory a student says the following claim : '' Exists at least one maximal countable subset of $\mathbb{R}$ .That it's true because the set of countable subsets of $\mathbb{R}$ is partially ordered set with order, the relation of 'containing'.Every chain of these sets has supremum (the union of chain).So from the Zorn's Lemma the set has maximal element.'' That proof seems right and i can't find false.Can you find any false on this claim?",['elementary-set-theory']
3687271,triangle problems,"So, I was sent a problem by someone. I solved it intuitively but couldn't come up with a logical answer as I got stuck at 1 step. This is how I approached the problem - I drew a side equal in length to create an equilateral triangle coincidentally making an isosceles triangle with the adjoining line segments $MB$ and $AC$ . Now I extended the line going from $C$ till it touched the side $AB$ . The problem is that when I assume that line to be parallel to the segment $MB$ everything fits in perfectly but I've spent hours trying to prove lines $OP$ and $MB$ to be parallel. Can someone please tell me how I can approach at it to prove that $MB$ and $OP$ parallel.","['triangles', 'puzzle', 'geometry']"
3687297,Is $z=0$ a pole of $1/\sqrt{z}$?,"Consider the function $f:z\mapsto 1/\sqrt{z}$ , defined, say on the right half-plane $Re(z)>0$ .  (We can resolve ambiguity by  taking the branch that is positive for real $z$ ). Let $U$ be the right half-plane, and $a=0$ . Then the following conditions, lifted from Wikipedia's page on essential singularities , appear to hold: 1) $f(z)$ is not defined at $a$ but is analytic in the region $U$ .  Moreover, every open neighborhood of $a$ has non-empty intersection with $U$ . 2) $\lim_{z\rightarrow a}f(z)$ does not exist. 3) $\lim_{z\rightarrow a}{1\over f(z)}$ exists (and is equal to zero). According to that Wikipedia page, it follows that $a=0$ is a pole of $f$ . But it seems to me that $f$ has no Laurent series at $a=0$ , which makes me skeptical that this really is a pole. This seems to leave three possibilities:  Either Wikipedia is wrong, or I am wrong, or I have misunderstood Wikipedia.  Which of these is correct?","['complex-analysis', 'singularity']"
3687299,"Why does $f(x,y)= \frac{xy^2}{x^2+y^4}$ have different limits when approaching $(0,0)$ along straight lines vs. along the curve $(1/t^2,1/t)$?","I have a stupid question about continuity in higher dimensions. There are maps, for example, $f(x,y)=\frac{xy^2}{x^2+y^4}$ , when $(x,y)\neq (0,0)$ and $f(x,y)=(0,0)$ when $(x,y)=(0,0)$ , when we approach $(0,0)$ along every straight line, the limit of the function is $0$ , but when along a curve, for example $(\frac{1}{t^2},\frac{1}{t})$ , the limit of $f$ is not $0$ . But, it feels like all the straight lines can cover a neighbourhood of $(0,0)$ , so every point on a curve is also on a different straight line. Why is it that when the same points are arranged in a different way the limit changes?","['multivariable-calculus', 'limits', 'calculus', 'continuity']"
3687316,How were complex geometric shapes drawn without computers?,"How did mathematicians create drawings of complex geometric shapes in the past, without 3d graphics in computers? Here is one example of what I’m talking about, drawn in the 16th century:","['art', 'math-history', 'geometry']"
3687456,"Given a deck of cards, what are the odds of consecutively drawing higher cards for a given number of draws?","Lets say you're trying to draw n cards, each higher than the last. How likely are you to succeed in consecutively drawing higher for a given starting number & number of iterations? Somebody said to add my thoughts to the question! We've had a lot (my brother and i are trying to solve it!) so wait just a few minutes while I write it all out! So first we tried by thinking about some easy cases. Lets say you only wanted 3 consecutive higher draws. If you drew a king or a queen you're done right off the bat, the odds of that are 8/52. If you drew a Jack, you have to get the sequence of JQK to succeed. That's $4/52* 4/51*4/50$ . From this we can't yet establish too much, but we found a general way to answer this problem for three draws. If you start with a Jack, there is one way, a 10 in three ways, a 9 in six ways, an 8 in 10 ways... each drop in card number adds one more positive outcome than the last drop did (K-Q added zero, Q-J added one, J-10 added two, 10-9 added three etc).  So the equation is $1(4/52 * 4/51 * 4/50) + 3(4/52 * 4/51 * 4/50) + 6(4/52 * 4/51 * 4/50)... + 66(4/52 * 4/51 * 4/50)$ . (<- if we aren't mistaken) Around here is where we have gotten stuck as brute force gets more difficult and we don't yet have an overarching formula to apply. Any ideas of where to go from here to apply this to higher numbers of draws? BIG EDIT (unless we're super wrong... not 100% sure of proper math notation)
Okay... so our thought process from where we left off is as follows: If we're drawing four cards instead of three, the sum is the same except the 66 term is lobbed off (because the 1,3,6,10... series starts a number lower [in the case of 4 cards, you can't draw a jack first] and ends at the same place, thereby eliminating one term) and the inside term gains 4/49 (signifying the probability of another drawn card). The equation would be: $1(4/52 * 4/51 * 4/50 * 4/49) + 3(4/52 * 4/51 * 4/50 * 4/49) + 6(4/52 * 4/51 * 4/50 * 4/49)... + 55(4/52 * 4/51 * 4/50 * 4/49)$ . Notice the similarities between the equation for drawing three cards and drawing four. From this, we derived the (hopefully correct) formula for any number of draws.
let x = number of cards you want to draw. 
let y = the value of the first card drawn.
Note: in the following, $n_{-1} = 0$ $$\sum_{n=0}^{14-x}((n + n_{n-1} +1)*(\prod_{n=0}^{x}(4/(52-n))))$$ Posting for now, any questions will be addressed after we eat dinner.",['probability']
3687460,How to define the trace and deteraminant of a linear transformation in a basis-free manner?,Trace and determinant of a matrix can be shown to be invariant under basis transformation. This should suggest that they can be defined for a linear transformation in a basis-free manner. How does one do that? P.S. I'm really looking for a definition which can illustrate why the trace and determinant of matrices are invariant under basis transformations. Thanks very much!,"['matrices', 'linear-algebra']"
3687484,Must continuous $H^1(\mathbb{R}^2)$ function tend to zero at infinity?,"Here, $H^1(\mathbb{R}^2)$ is the standard Sobolev spaces for $L^2(\mathbb{R}^2)$ functions whose weak derivative belongs to $L^2(\mathbb{R}^2).$ My question in the title comes from calculus of variations. It is usually the case that a minimizer of some given energy functional defined on $H^1(\mathbb{R}^2)$ is known to be continuous (or even $C^2(\mathbb{R}^2))$ . I want to know the behavior of this minimizer at infinity. If $u \in L^2(\mathbb{R}^2),$ then is known $\liminf_{|x| \to \infty} u(x) = 0.$ But it cannot say $\limsup_{|x| \to \infty} u(x) = 0$ since counterexamples exist. If we assume $u \in H^{1+\epsilon}(\mathbb{R}^2)$ for some $\epsilon > 0,$ then the classical Morrey's inequality can imply uniform H\""older continuity of $u.$ So we can conclude $\limsup_{|x| \to \infty} u(x) = 0$ via proof by contradiction. So my problem is about the case $\epsilon = 0.$ That is, when $$u \in H^1(\mathbb{R}^2) \cap C(\mathbb{R}^2),$$ is it true that $$\limsup_{|x| \to \infty} u(x) = 0?$$ Using proof by contradiction, I think this should be true. Here is my non-rigorous argument. Assume not, then there are $\epsilon > 0$ and $x_n \in \mathbb{R}^2$ such that $|x_n| \to \infty$ and $|u(x_n)| \geq 2\epsilon.$ By the continuity, there is $r_n > 0$ such that $|u(x)| \geq \epsilon$ for all $x \in B(x_n, r_n).$ Since $u \in L^2, r_n \to 0$ as $n \to \infty.$ I think non-rigorously that $$ \int_{B(x_n, r_n)} |\nabla u|^2 \gtrsim \int_{B(x_n, r_n)} (\frac{\epsilon}{r_n})^2 = \epsilon^2$$ for large $n$ and $$
\int_{\mathbb{R}^2} |\nabla u|^2 \geq \sum_{n\,\text{is large}} \int_{B(x_n, r_n)} |\nabla u|^2.
$$ So they imply a contradiction $\int_{\mathbb{R}^2} |\nabla u|^2 = \infty$ I appreciate any discussion. Edit : How about $u$ is additionally assumed to be $C^1(\mathbb{R}^2)$ or even $C^2(\mathbb{R}^2)?$ Is there any proof or counterexample?","['calculus-of-variations', 'examples-counterexamples', 'real-analysis', 'sobolev-spaces', 'weak-derivatives']"
3687608,Indecomposable elements in a lattice,"Let $L$ be an discrete lattice in $\mathbb R^n$ . We say that a nonzero $a\in L$ is indecomposable if and only if $a$ cannot be written as $a=b+c$ with $b,c$ nonzero and $b^T c>0$ . I was initially trying to prove that the indecomposable elements generate the Voronoi cell (also called Dirichlet domain) $V=\{x\in\mathbb R^n:|x|<|x-v| \mbox{ for all } 0\ne v\in L\}$ , in the sense that if we define $H_v=\{x\in\mathbb R^n:|x|<|x-v|\}$ then $V=\cap H_a$ where the intersection runs over the indecomposable elements. Now, I have managed to show the above by establishing that $u^Tv\ge 0$ implies $H_u\cap H_v\subset H_{u+v}$ . Further I wish to show that this intersection is minimal in the sense that we cannot remove any indecomposable element and still get $V(0)$ . Also it is the unique minimal such set. How is that possible? What I am possibly thinking of is to prove that if $a,b$ are both indecomposable and distinct then we cannot have $H_a\subset H_b$ . But how to prove that? I am not getting an intuition of what is an indecomposable vector. Update: I think the last two paragraphs on Pg 57 of these notes contain the answer. But I am unable to understand them almost entirely. Can someone explain?","['integer-lattices', 'linear-algebra', 'lattices-in-lie-groups']"
3687638,Proving $F(z)=\prod _{k=0}^{\infty}\text{sinc} \left(\frac{\pi z}{2 k+1}\right) $ belongs to Schwartz space,Denote $$F(z)=\prod _{k=0}^{\infty}\text{sinc} \left(\frac{\pi z}{2 k+1}\right)=\prod _{n=1}^{\infty } \cos \left(\frac{\pi z}{2 n}\right)$$ How can we prove $F\in S(\mathbb{R})$ (Schwartz space) ? I've already shown that $F(z)$ is entire and rapidly decreasing in strip $|\Im(z)|≤r$ for $r>0$ . Background: This arises from solving Borwein integrals via Fourier transform.,"['complex-analysis', 'fourier-analysis', 'fourier-transform']"
3687676,What does it mean by saying 'a random variable $\mathit X$ is $\mathcal G$-measurable'?,"This is the definition of ""measurability"": Let $\mathit X$ be a random variable defined on $(\Omega,\mathcal F,\mathbb P)$ . Let $\mathcal G$ be a $\sigma$ -algebra of subsets of $\Omega$ . If $\sigma(\mathit X) \subseteq \mathcal G$ , we say that $\mathit X$ is $\mathcal G$ -measurable. I can understand this definition, but what does it mean that "" a random variable $\mathit X$ is $\mathcal G$ -measurable if and only if the information in $\mathcal G$ is sufficient to determine the value of $\mathit X$ "" ? My questions are: What is the ""information"" in $\mathcal G$ ? As far as I know, $\mathcal G$ is just a set of subsets of $\Omega$ . Thus $\mathcal G$ contains lots of ""events"". Then, when we say ""information"", what exactly do we mean？ I can't imagine other information except for the elements of $\mathcal G$ . How can the ""information"" in $\mathcal G$ determine the value of $\mathit X$ ? Given $\mathcal G$ , what we know is just ""the elements of $\mathcal G$ "". How could this ""infromation"" help us determine the value of $\mathit X$ ? Example: This is an example of rolling a die with $\Omega = \{1, 2, 3, 4, 5, 6\}$ : $$\mathit X_1(\omega)=\omega$$ $$\mathit X_2(\omega)=\begin{cases} 1, & \omega\in \{1,3,5\} \\-1, & \omega\in \{2,4,6\}\end{cases}$$ $\mathit X_1$ and $\mathit X_2$ are both random variables. $\mathit X_1$ gives the exact outcome of the roll, and $\mathit X_2$ is a binary variable whose value depends on whether the roll is odd or even. Let $\mathcal G = \{\emptyset, \Omega, \{1,3,5\}, \{2,4,6\}\}$ , then $\mathit X_2$ is measurable w.r.t $\mathcal G$ but $\mathit X_1$ is not measurable w.r.t $\mathcal G$ . I know this because I can check that $\sigma(X_2)=\mathcal G$ according to the defination of $\sigma(X_2)$ . Here are the questions: What's the information in $\{\emptyset, \Omega, \{1,3,5\}, \{2,4,6\}\}$ ? What else can we know except for those four elements in $\{\emptyset, \Omega, \{1,3,5\}, \{2,4,6\}\}$ ? If this is the only 'information' that we can obtain from $\mathcal G$ (i.e., there are four elements in $\mathcal G$ , and those elements are $\emptyset$ , $\Omega$ , $\{1,3,5\}$ and $\{2,4,6\}$ ), how can we determine what the value of $\mathit X_2$ will be?","['conditional-expectation', 'measure-theory', 'probability-theory', 'filtrations']"
3687682,Confidence Interval. Bernoulli Distribution,"I am reviewing the construction of confidence intervals for a random sample with Bernoulli distribution. The book uses the statistics of the central limit theorem that distributes $N(0,1)$ to estimate the interval : $$Z_n = \frac{X_1 + X_2 + \cdots + X_n -n\mu}{\sigma \sqrt{n}}$$ Why are the intervals constructed from these statistics symmetrical around the origin? The book says: ""Since it is desirable that the length of the interval be as small as possible and since the standard normal distribution is symmetrical around the origin, it turns out that the minimum length interval must also be symmetric around the origin"", but I don't understand this.","['statistics', 'confidence-interval', 'probability']"
3687703,"What is the reason for the term ""zero dimensional"" in the context of topology?","A topological space is zero dimensional if, and only if, it has a basis consisting of sets which are both open and closed (that is, ""clopen""). This definition is according to ""Counterexamples in Topology"" by Steen and Seebach, 2nd ed. 1978. I see that ""zero dimensional"" is tied in with various levels of being ""disconnected"", e.g. extremally disconnected, totally separated, totally, disconnected, scattered, etc. (although the property of being ""zero dimensional"" is itself strictly independent of all disconnectivity properties). I understand (vaguely) the concept of a ""dimension"" in the context of manifolds: a space of $n$ dimensions has a boundary of $n - 1$ dimensions. I also note that a set is clopen if, and only if, it has no boundary. So would that be the basis of this terminology? Is there a source which states this definitively?",['general-topology']
3687722,"Suppose $f(x) \rightarrow M$ as $x \rightarrow a$. Prove that if $f(x) \leq L$ for all $x$ near $a$, then $M \leq L$.","I'm am a student taking a real analysis paper at university. I'm going through some problems on my problem sheet and I've been asked the question above. I'm still getting a hang on what it means for $x$ to be ""near"" $a$ but gather, that if $f(a) \leq L$ and $f(a) = M$ then $M \leq L$ is implied. If anyone can help me understand the mathematical definition of something being ""near"" something else and some tips on how to carve a rigorous proof of the above. It would be much appreciated! Thank you for your time!","['limits', 'real-analysis']"
3687733,Evaluate $\lim_{n \to \infty}\left(\frac{1^{1/x}+2^{1/x}+\ldots+n^{1/x}}{n}\right)^{nx}$,"$$\lim_{n \to \infty}\left(\frac{1^{1/x}+2^{1/x}+\ldots+n^{1/x}}{n}\right)^{nx}$$ I don't know any format or can't think of anything to solve this limit. It looks like it is Riemann's Sum Form but there is an x, so I am confused. Please help out. Thank You!","['integration', 'limits', 'calculus']"
3687823,Generalization of Kolmogorov precompactness criterion,"In the book Elements in functional analysis from Hirsch and Lacombe, the Kolmogorov precompactness criterion for families of $L^p$ functions is stated as follows: Theorem : Let $H \subseteq L^p(\mathbb{R}^d)$ , with $p \in [1,\infty)$ . Then $H$ is precompact if and only if the following conditions hold: $H$ is bounded w.r.t. the norm $||\cdot||_{L^p(\mathbb{R}^d)}$ ; $\lim_{R \to +\infty} \sup_{f \in H} \int_{B_R^c} |f(x)|^p dx = 0$ , where $B_R^c$ denotes the complement in $\mathbb{R}^d$ of the ball of radius $R$ ; $\lim_{|a| \to 0} \sup_{f \in H} ||\tau_af-f||_{L^p(\mathbb{R}^d)}=0$ , where $\tau_a$ denotes the translation operator . Now, I think that one can modify a little bit this statement in order to make it valid also when we consider families of functions in $L^p(\Omega)$ , where $\Omega$ is an open set in $\mathbb{R}^d$ : Claim : Let $\Omega$ be an open set in $\mathbb{R}^d$ , let $p \in [1,\infty)$ , and let $H \subseteq L^p(\Omega)$ . For all $R > 0$ , define $$\Omega_R = \{x \in \Omega: \text{dist}(x, {\Omega}^c) > \frac{1}{R}\} \cap B(0,R)$$ Then $H$ is precompact if and only id the following holds $H$ is bounded w.r.t. the norm $||\cdot||_{L^p(\Omega)}$ ; $\lim_{R \to +\infty} \sup_{f \in H} \int_{\Omega \setminus \Omega_R} |f(x)|^p dx = 0$ ; $\lim_{|a| \to 0} \sup_{f \in H} ||\tau_af-f||_{L^p(\Omega_R)}=0$ for all $R >0$ . I'm trying to prove this last equivalence using the theorem I've written above. So far this is my strategy: First, notice that we can look at $L^p(\Omega)$ as a subset of $L^p(\mathbb{R}^d)$ extending every function $f$ in $L^p(\Omega)$ to a function $\tilde{f}$ in $L^p(\mathbb{R}^d)$ in this way: $$\tilde{f}(x) =\begin{cases} 
      f(x) & x \in \Omega \\
      0 & \text{otherwise} 
   \end{cases}$$ Then $H$ is precompact in $L^p(\Omega)$ if and only if it's precompact in $L^p(\mathbb{R}^d)$ . If I prove that conditions 1,2,3 of the claim are equivalent to conditions 1,2,3 of the theorem, I'm done. Condition 1 is easy, but I can't really see how conditions 2 and 3 of the theorem should be equivalent to conditions 2 and 3 of the claim. Any help, remark or suggestion is appreciated, thank you.","['lp-spaces', 'functional-analysis', 'compactness', 'real-analysis']"
3687852,Riemannian Exponential Map is a Homeomorphism outside the Cut Locus,"Let $M$ be a connected and complete Riemannian manifold.  The Hopf-Rinow Theorem guarantees that $Exp_p$ , for any $p \in M$ , is defined on all of $T_p(M)$ .  Further, this map is a diffeomorphism on a neighbourhood of the origin. However, under the assumption of completeness and connectedness of $M$ is it a homeomorphism, is it a homeomorphism from $M-C_p$ to $T_p(M)$ ; where $C_p$ is the cut-locus of $p$ ?","['manifolds', 'riemannian-geometry', 'differential-geometry']"
3687896,Let $G$ be a p-group. Let $H$ be a proper subgroup of $G$. Show that there exists $g$ $\in$ $G \setminus H$ such that $gHg^{-1}=H$.,"Let $G$ be a p-group. Let $H$ be a proper subgroup of $G$ . Show that there exists $g$ $\in$ $G \setminus H$ such that $gHg^{-1}=H$ . I tried to use a counting argument. Let's assume by contradiction that it isn't true. So, for every $g$ $\in$ $G \setminus H$ , $gHg^{-1}$ is not $H$ . We know that conjugation is also a subgroup of $G$ , so $gHg^{-1}$ is a different subgroup of order $p^k$ where $|G|=p^n$ and $n>k$ . Now, we can see that each conjugation as above is different if $g_1 \neq g_2h$ , since if $g_1Hg_1^{-1} = g_2Hg_2^{-1}$ then we can get $g_2^{-1}g_1Hg_1^{-1}g_2=H$ and clearly, from what's given, $g_2^{-1}g_1 \in H$ . Consequently, we will have $p^{n-k}*(p^k-1)+p^k+1$ elements in $G$ which clearly is not a contradiction, and I don't know how to continue from here.
Any hint will be helpful.","['group-theory', 'p-groups']"
3688042,Kernel of $\mathbb{Z}_p \to \mathbb{Z}/p^{n}\mathbb{Z}$ equals to $p^n \mathbb{Z}_p$.,"Let $\mathbb{Z}_p$ denotes the ring of $p$ -adic integers, i.e., $\mathbb{Z}_p:= \varprojlim \mathbb{Z}/p^{n}\mathbb{Z}$ . Then consider the projection map $\pi_{n}: \mathbb{Z}_p \to \mathbb{Z}/p^{n}\mathbb{Z}$ . Then we know that $\pi_{n}$ is a ring map. I want to show that $\operatorname{ker} \pi_{n}=p^{n}\mathbb{Z}_p$ . It is clear that $p^{n}\mathbb{Z}_p \subset \operatorname{ker}\pi_{n}$ . I am stuck at the converse. For $(x_r) \in \operatorname{ker}{\pi_n}$ we've $p^n \mid x_n$ , and since $p^n \mid x_{n+1}-x_n$ , $p^n \mid x_{n+1}$ . Thus for all $i \in \mathbb{N}$ and $i\geq n$ there exists integers $y_{i}$ such that $x_i=p^{n}y_i$ , so that we obtain $(x_r)=p^{n}(y_r)$ But I'm unable to show that $(y_r) \in \mathbb{Z}_p$ , i.e., $(y_r)$ is coherent. I need some help to complete it. Thanks.","['algebraic-number-theory', 'formal-completions', 'number-theory', 'p-adic-number-theory', 'commutative-algebra']"
3688047,Regular conditional probability on Polish space and absolute continuity,"Let $(\Omega,\mathcal F,\mathbb P)$ is a standard Borel space (i.e. $\Omega$ is Polish and $\mathcal F = \mathcal B(\Omega)$ ). Then $\mathcal F$ is separable and for every sub-sigma-algebra $\mathcal G \subset \mathcal F$ , then there exists a regular conditional probability $\mathbb P^{\mathcal G} : \Omega \times \mathcal F \to [0,1]$ (i.e. satisfying a. for all $A \in \mathcal F$ , $\mathbb P^{\mathcal G}(\cdot,A) : \Omega \to [0,1]$ is $\mathcal G$ -measurable, b. for a.e. $\omega \in \Omega$ , the map $\mathbb P^{\mathcal G}(\omega,\cdot) : \mathcal F \to [0,1]$ is a probability on $(\Omega,\mathcal F)$ and c. for all $A \in \mathcal F$ and $B \in \mathcal G$ , we have $\mathbb P(A \cap B) = \int_\Omega 1_B(\omega) \mathbb P^{\mathcal G}(\omega, A) \mathbb P(d \omega)$ ). I read here and there ( Is the regular conditional probability of $\mathbb P$ dominated by $\mathbb P$ almost surely? ; https://mathoverflow.net/questions/354098/necessary-and-sufficient-conditions-for-almost-sure-absolute-continuity-of-regul ) that under this setting, for every $\mathcal G$ , we have $\mathbb P^{\mathcal G}(\omega, \cdot) \ll \mathbb P$ almost surely (i.e. $\mathbb P({\omega : \mathbb P^{\mathcal G}(\omega, \cdot) \ll \mathbb P}) = 1$ ). I must have a misunderstanding about r.c.p's because of the following: take $(\Omega=[0,1],\mathcal F = \mathcal B([0,1]),\mathbb P = \mathrm{Leb})$ and $\mathcal G = \mathcal F = \mathcal B([0,1])$ . Then for $A \in \mathcal F$ , we have $$\int_\Omega 1_A(\omega) P(d\omega) = \mathbb P(A) = \mathbb P(A \cap A) = \int_\Omega 1_A(\omega) \mathbb P^{\mathcal G}(\omega, A) \mathbb P(d\omega) \,,$$ which gives $1_A = 1_A \mathbb P^{\mathcal G}(\cdot, A)$ , $\mathbb P$ -almost surely. In addition, $$0 = \mathbb P(A \cap A^c) = \int_\Omega 1_{A^c}(\omega) \mathbb P^{\mathcal G}(\omega,A) \mathbb P(d\omega)\,,$$ thus $1_{A^c} \mathbb P^{\mathcal G}(\cdot,A) = 0$ , $\mathbb P$ -almost surely. This implies that $\mathbb P^{\mathcal G}(\cdot,A) = 1_A$ , $\mathbb P$ -almost surely. (I can also directly check that $(\omega,A) \mapsto 1_A(\omega)$ satisfies a.,b.,c. from the definition of r.c.p above). Hence $\mathbb P^{\mathcal G}(\omega, \cdot) = \delta_\omega$ . Is there any reference for the fact that $\mathbb P(\omega : \mathbb P^{\mathcal G}(\omega,\cdot) \ll \mathbb P) = 1$ ? What is wrong with my above ""counterexample"" ?","['measure-theory', 'conditional-expectation', 'fake-proofs', 'absolute-continuity', 'probability-theory']"
3688101,On a question of contractible graphs,"A family $\mathcal{F}$ of graphs $G_1, G_2, \cdots, G_n ,\cdots$ is called contractible if (1) The trivial graph, $\ast \in \mathcal{F}.$ (2) Any graph of $\mathcal{F}$ can be obtained from the trivial graph by finite series of contractible transformations $\{T_1, T_2, T_3, T_4\}$ where $T_1$ : deleting of vertex $v$ . A vertex $v$ of a graph $G$ can be deleted, if $N_G(v):= \{ u \in V(G): \text{ the edge }[uv] \in E(G)\} \in \mathcal{F}.$ $T_2:$ Gluing of a vertex $v$ . If a subgraph $G_1$ of a graph $G$ is contractible, $G_1 \in \mathcal{F}$ the the vertex $v$ can be glued to the graph $G$ in such a manner that $N_G(v) =G_1,$ $T_3:$ deleting of an edge $[v_1v_2]$ . The edge $[v_1v_2]$ of $G$ can be deleted if $N_G(v_1)\cap N_G(v_2)\in \mathcal{F}.$ $T_4:$ Gluing of an edge $[v_1v_2]$ .  Let two vertices $v_1$ and $v_2$ of a graph $G$ be non-adjacent. The edge $[v_1v_2]$ of $G$ can be glued if $N_G(v_1)\cap N_G(v_2)\in \mathcal{F}.$ Any graph $G \in \mathcal{F}$ is called a contractible graph. Question: Is it true that any contractible graph with more than two vertices has at least two contractible vertices ( a vertex is called contractible if $N_G(v) \in \mathcal{F}$ )? Thank you in advance. Any help will be appreciated.","['graph-theory', 'discrete-geometry', 'discrete-mathematics']"
3688148,Evaluate $\lim_{n\to\infty} \prod_{k=1}^n \frac{2k}{2k-1}\int_{-1}^{\infty} \frac{{\left(\cos{x}\right)}^{2n}}{2^x} \; dx$,"Problem 9 in the JHMT 2013 Calculus Test asks to evaluate $$\lim_{n\to\infty} \prod_{k=1}^n \frac{2k}{2k-1}\int_{-1}^{\infty} \frac{{\left(\cos{x}\right)}^{2n}}{2^x} \; dx$$ The answer is $\pi\cdot 2^\pi /(2^{\pi}-1)$ . How can I show this?  I know that the infinite product diverges and the limit cannot be moved into the integral, but I don't know what to do.  Maybe I can represent the integral as a summation?","['integration', 'real-analysis', 'calculus', 'sequences-and-series', 'infinite-product']"
3688382,A simple group with $|\operatorname{Syl}_p⁡ G| \le 6$ is cyclic,"Let $G$ be a simple, finite group, s.t. for every prime $p$ , it satisfies $k_p=\left|\operatorname{Syl}_p⁡ G\right| \le 6$ . Show that $G$ is cyclic. My attempt: Let $n=p_1^{e_1}p_2^{e_2}\ldots p_r^{e_r}$ be the (distinct) prime factorization of $n = \left|G\right|$ . If $n$ is prime, $G$ is cyclic. So we assume $e_1\ge 1$ and $e_2\ge1$ . From Sylow's theorems, we have $k_{p_1}\mid\prod_{i\ne1} p_i^{e_i}$ and $k_{p_1}\equiv1\ (\operatorname{mod} p_1)$ , and the same applies for $p_2$ . Sicne $G$ is simple, $k_{p_{1,2}}\ne1$ , and it is given that $k_{p_{1,2}}\le6$ . The options for $p_1$ are No prime $p_1$ satisfies $k_{p_1}=2$ . The other options are that $p_1=2,3,5$ and $k_{p_1}=3,4,6$ respectively. How do I continue from here?","['finite-groups', 'abstract-algebra', 'sylow-theory', 'p-groups', 'group-theory']"
3688419,Injective morphism and dimension of varieties,Inspired by MSE/95760 I'm wondering whether the following is true: Let $\varphi : V \to W$ be an injective morphism between (affine) varieties. Does it follow that $\dim{V} \leq \dim{W}$ ? I am not able to construct any counterexamples and I cannot see a way to translate chains of irreducible varieties in $V$ to $W$ .,"['krull-dimension', 'algebraic-geometry']"
3688426,Examples of non-positively Curvature Riemannian Manifolds,"When I read about complete, simply connected, and connected Riemannian manifolds of non-positive curvature I only find explicit examples of hyperbolic $n$ -space and Euclidean space.  What are other commonly used spaces meeting these criteria? I'm interested in two types of ""applications"" Used within differential geometry. In this case, I'm most interested in examples of symmetric spaces of non-compact type besides $H^n$ and symmetric positive-definite matrices . Some concrete examples of non-constant and non-positive curvature would be nice. Examples of geometries used in statistics, and applied sciences Gaussian densities I already know about  ( which really follows from the former + some information geometric considerations). I leave this up to interpretation, besides saying I'd like to gain some concrete use with my audience.  So the hyperboloid example, from the comments, is nice but the examples of symmetric PSD matrices above is more reflective of what I am aiming for. Edit: I added some comments following Moishe Kohan's remarks and certain other comments. Thought: I guess we can always generate more ""explicit"" examples as follows: given any $\phi \in Diff(M,N)$ , where $(M,g_M)$ is either of $\mathbb{R}^n,\mathbb{H}^n$ , space of symmetric positive definite matrices, and $N$ is some smooth manifold diffeomorphic to $M$ , then $g_N:=\phi_{\star}(g_M)$ will give us a non-positively curved Riemannian structure on $N$ , since $g_N$ is conformal to $g_M$ ...  Though a direct construction of an example like this is a bit...underwhelming.*","['riemannian-geometry', 'information-geometry', 'reference-request', 'hyperbolic-geometry', 'differential-geometry']"
3688432,Is $\frac{\cos(\frac{1}{z})}{z^2}$ meromorphic or Not?,"my professor used the Cauchy Residue Theorem to evaluate the path integral (along the positively-oriented unit circle about the origin with winding number 3) $$\int_{\gamma}\frac{\cos(\frac{1}{z})}{z^2}$$ His reasoning is that, when expanded into series, $$\operatorname{res}(f,0) = 0.$$ However, I don't see how the integrand is meromorphic since I think it has an essential singularity at the origin. When I did the integral, I just used the fact that the integrand has a primitive on an annulus about the origin and thus the integral must be zero. Is my professor wrong to use the Cauchy Residue Theorem on the justification that it only works for meromorphic functions? Thanks in advance for any help!","['integration', 'singularity', 'complex-analysis', 'meromorphic-functions', 'residue-calculus']"
3688439,how would you prove that polynomial functions are not exponential?,"here is one proof that I know but I am not totally sure if it is acceptable- exponential functions are exponential: no matter how many times you differentiate them 
e.g- f(x)=e^x first derivative f`(x)= e^x 2nd derivative f``(x)= e^x 3rd derivative f```(x)=e^x and so on. now if you differentiate a polynomial function-
let's say, f(x)= x^5 1st derivative f`(x)= 5x^4 2nd de3rivative f``(x)= 20x^3 3rd derivatives f```(x)=60x^2 4th derivative f````(x)=120x 5th derivative f`````(x)= 0 like this every polynomial finally gets differentiated to zero or a constant . this proves that the polynomials are not exponential. **is my proof ok** I want more alternate proofs and a brief explanation about this one.","['calculus', 'solution-verification', 'polynomials', 'algebra-precalculus', 'exponential-function']"
3688480,"$K=\{f \in C^1([0,1]): f(0)=0, |f'(x)|\leq 1 \; \forall x\}$ can be covered with $4^n$ balls of radius $1/n$","I am trying to solve the following problem: Let $ K=\{f \in C^1([0,1], \mathbb{R}): f(0)=0, |f'(x)|\leq 1 \; \forall x\} \subset C([0,1],\mathbb{R})$ Prove that $K$ is precompact in $C([0,1])$ Prove that, for every $n \in \mathbb{N}$ , $K$ can be covered with $4^n$ closed balls in $C([0,1])$ of radius $1/n$ (Hint: Consider balls centered at suitable piecewise affine functions with slope $\pm1$ ). I was able to prove point (1) by applying Arzelà–Ascoli theorem. I am struggling to prove point (2). I tried to construct the balls explicitly but with no success, even in the case $n=1$ . 
I started by observing that for every $f \in K$ we have $$ 
-x \leq f(x) \leq x \quad \forall x \in [0,1]
$$ and I thought that this could be useful. I then considered the balls of radius $1$ centered at $f_1(x) = x$ and $f_2(x) =-x$ but these do not seem enough. I thought that maybe I should consider vertical translations of $f_1$ and $f_2$ or some zig-zag looking functions, but I am unable to conclude. Also, maybe I do not have to construct the balls explicitly, but I do not have in mind other ways to prove their existence (the only thing that comes to my mind is to use the fact that compact metric spaces are totally bounded, but I do not know how). Could you please give me some help? Thank you. P.s. This problem is taken from a past entrance exam to a PhD in Mathematical Analysis. If you recognize that this is from some book, or if you have a source of similar problems, please tell me.","['metric-spaces', 'functional-analysis', 'compactness', 'real-analysis']"
3688492,Question Regarding Proof of Hodge Index Theorem,"I am reading Voisin's proof of the Hodge Index Theorem on pp. 153-154 of her Hodge Theory and Complex Algebraic Geometry I .  The proof is mostly clear except for one technical point. Let $n$ denote an even number, let $X$ denote a compact Kähler manifold, and let $h^{a,b}=h^{a,b}(X)=\dim H^{a,b}(X).$ Voisin claims that using Poincaré duality, we can get $$ 2\sum_{a+b=n-2r,r>0}(-1)^a h^{a,b}=\sum_{a+b\equiv n(2), a+b\ne n} (-1)^ah^{a,b}.$$ It's likely that I'm simply misunderstanding what is meant here, but I understand the left hand side as saying $$ 2\sum_{r=1}^{n/2}\sum_{a+b=n-2r}(-1)^ah^{a,b}=\sum_{a+b\equiv n(2), a+b\ne n} (-1)^ah^{a,b}.$$ However, given my interpretation, choosing $n=2$ , the left hand side is $$ 2\sum_{a+b=0}(-1)^ah^{a,b}=2h^{0,0}$$ because $a+b\ge 0$ implicitly. The right hand side becomes a sum over $a,b$ so that $a+b\equiv 0 \pmod{2}$ , and $a+b\ne 0$ subject to the same constraint of $a+b\ge 0$ . So, it looks like the right hand side reads $h^{0,0}$ . Then it looks like this implies that $$ 2h^{0,0}=h^{0,0}$$ so that $h^{0,0}=0$ . But that implies $h^{0,0}(X)=h^0(X,\mathbb{C})=0$ , which is false for instance in the case of $X=\mathbb{P}^2$ .","['hodge-theory', 'complex-geometry', 'algebraic-geometry', 'homology-cohomology']"
3688508,Distribution of $\{n^p\alpha\}$ for irrational $\alpha$,"Let $\alpha$ be an irational number. Consider sequence $x_n=\{n^p\alpha\}$ , $n\in\mathbb{N}$ (it's the fractional part of $n^p\alpha$ ), where $p$ is a nonzero real number. Question. For which values of $p$ the sequence $\{x_n\}_{n\in\mathbb{N}}$ is equidistributed on $[0,1)$ ? The other qusetion is when $\{x_n\}_{n\in\mathbb{N}}$ is dense on $[0,1)$ . It's known that if $p\in\mathbb{N}$ , then it's true (it's a consequence of Weyl's equdsitribution criterion and van der Corput's difference theorem). However, it's not clear how to apply Weyl's criterion in case when $p\notin\mathbb{N}$ . I encountered similar problem when I was working
on this question Convergence of the product $\prod_{n=1}^{\infty}\left(1+\frac{x^n}{n^p}\right)\cos\frac{x^n}{n^q}$ (in order to study the behaviour of $\cos\frac{1}{n^q}$ , so $\alpha=1/\pi$ in this case). Update. It would be also intersting to investigate the distbution of $\left\{\frac{x^n}{n^p}\right\}$ . If the result is known, please give a link or reference. Any help would be appreciated.","['irrational-numbers', 'equidistribution', 'real-analysis']"
3688518,Converse to a proposition on homogeneous polynomials,"I know that for a homogeneous polynomial $P$ , if $P(x_1, ... , x_n) = 0$ , then $P(ax_1, ..., ax_n) = 0$ for every $a$ in the field of $P$ . Is the converse of this proposition true? That is, if $P(x_1, ... , x_n) =0$ implies $P(ax_1, ..., ax_n) = 0$ for every $a$ in the field of $P$ , is $P$ homogeneous?","['abstract-algebra', 'polynomials']"
3688538,Geometry problem involving a cyclic quadrilateral and power of a point theorem?,"Convex cyclic quadrilateral $ABCD$ are inscribed in circle $O$ . $AB,CD$ intersect at $E$ , $AD,BC$ intersect at $F$ . Diagonals $AC, BD$ intersect at $X$ . $M$ is midpoint of $EF$ . $Y$ is midpoint of $XM$ . circle $Y$ with diameter $XM$ intersects circle $O$ at $P,Q$ . Prove that $PY$ , $QY$ are tangent to circle $O$ It looks like a pretty interesting problem that could be solved by power of a point theorem , because they are a lot of line segments we can use to compute. But I didn't go very far.","['euclidean-geometry', 'geometry', 'plane-geometry']"
3688569,Canonical lift of Elliptic curve in Smart attack,"Smart attack details Given some curve $E/\mathbb F_p$ with order $p$ , lift it to $E_0=E/\mathbb Q_p$ and define the subgroups of $E/\mathbb Q_p$ : $E_r=\{(x,y)\in E/\mathbb Q_p|v_p(x)\leq-2r,v_p(y)\leq-3r\}\cup\{\infty\}$ . We have the following isomorphism of groups: $\frac{E_0}{E_1}\cong E/\mathbb F_p$ and $\frac{E_1}{E_2}\cong\mathbb F^+_p$ , which the first isomorphism given by reduction mod $p$ last isomorphism given by $\psi:(x,y)\to -\frac x{py}\pmod p$ Assume that $kP=Q$ in $E/\mathbb F_p$ , typically if we are only given $P,Q$ , findiing $k$ is computationally hard. However since $E/\mathbb F_p$ is anomalous, Smart attack gives us a simple solution: After lifting the curve to $E/\mathbb Q_p$ , let $\tilde P,\tilde Q$ be the lifted points of $P,Q$ respectively, then we have $p\tilde P,p\tilde Q,k\tilde P-\tilde Q\in E_1$ since $E/\mathbb F_p$ is of order $p$ . Furthermore, since $k\psi(p\tilde P)-\psi(p\tilde Q)=p\psi(k\tilde P-\tilde Q)=0$ , we get that $k=\frac{\psi(p\tilde Q)}{\psi(p\tilde P)}\pmod p$ , which is pretty simple to compute. When the attack fails The proof fails if $pP\in E_2$ , since we get $k=\frac00$ . This is noted in the Smart attack 's paper, where he notes that for 'canonical lifts', such points exist, and 'canonical lifts' occur at a frequency of $\frac1p$ . An example of such a lift can be found at the crypto SE question why the Smart attack fails for a particular lift . My question is why does hitting the canonical lift cause the attack to fail(which was answered in the comments), and why does it happen with a $\frac1p$ probability? Some examples of 'canonical lifts': The curve, $y^2=x^3+3x+5$ over $\mathbb F_7$ , has the following 'canonical lifts' when lifted to $\mathbb Q_7$ , for $m,n>0,m,n\in\mathbb Z$ and $p=7$ : $$y^2=x^3+\left(3+0p+mp^2\right)x+\left(5+3p+np^2\right)$$ $$y^2=x^3+\left(3+1p+mp^2\right)x+\left(5+2p+np^2\right)$$ $$y^2=x^3+\left(3+2p+mp^2\right)x+\left(5+1p+np^2\right)$$ $$y^2=x^3+\left(3+3p+mp^2\right)x+\left(5+0p+np^2\right)$$ $$y^2=x^3+\left(3+4p+mp^2\right)x+\left(5+6p+np^2\right)$$ $$y^2=x^3+\left(3+5p+mp^2\right)x+\left(5+5p+np^2\right)$$ $$y^2=x^3+\left(3+6p+mp^2\right)x+\left(5+4p+np^2\right)$$ In general(as far as I can tell by direct computation), one finds $p$ 'canonical lifts' mod $p^2$ for any anomalous curves, which satisfies the $\frac1p$ probability that Smart gave. About the 'canonical lift'(from comments) If the Frobenius endomorphism $f\in\text{End}\left(E/\overline{\mathbb F_p}\right)$ lifts to a endomorphism $\tilde f\in\text{End}\left(E/\overline{\mathbb Q_p}\right)$ , then this is the 'canonical lift' that Smart is referring to, which is less strict than requiring $\text{End}\left(E/\overline{\mathbb F_p}\right)\cong\text{End}\left(E/\overline{\mathbb Q_p}\right)$ . Since $\left|E/\mathbb F_p\right|=p$ , the dual of $f$ , $f^*$ has kernel $E/\mathbb F_p$ and is separable, which implies the dual of $\tilde f$ , $\tilde f^*$ , has a kernel of order $p$ , which is precisely the $p$ -torsion elements of $E/\mathbb Q_p$ , and is the torsion group $\left(E/\mathbb Q_p\right)_\text{tors}$ of this curve, hence $E_0\cong E_1\times\left(E/\mathbb Q_p\right)_\text{tors}$ , hence $pE_0=pE_1$ and $p\tilde P\in E_2$ , causing the attack to fail.","['cryptography', 'p-adic-number-theory', 'algebraic-geometry', 'elliptic-curves']"
3688572,Every not empty finite subset of a totally ordered set has a maximum and minimum,"Theorem Let be $(X,\le)$ a totally ordered set: then for any not empty finite subset $A$ of $X$ there exist the maximum  element and minimum element. proof . Let be $(X,\le)$ a totally ordered set and we prove by induction that any not empty finite subset $A$ of $X$ has a minimum element. Since $X$ is a totally ordered set, previously we observe that any its subset $Y$ (finite or infinite) is a chain. Obviously any subset $A$ of one element $a$ has trivially a minimum.
So we suppose that any subset of $n$ elements has a minimum element and then  we consider a subset $A$ of $n+1$ elements: since $A$ is finite there exists a bijection $\phi$ from $A$ onto some natural number $m$ , that is the successor of $n$ , and so we can organize the elements of $A$ in a finite succession, that is $A=\{a_1,...,a_{n+1}\}$ . Now we consider the subset $B=\{a_h\in A:h\le n\}$ : obviously $X$ is a subset of $A$ that has $n$ element and so it has a minimum element $b$ ; so since $A=B\cup\{a_{n+1}\}$ and since $A$ is a chain (remember what before we observed), it must be or $a_{n+1}\le b$ or $b<a_{n+1}$ and so for the transitivity property of the order relation $\le$ in any case $A$ has a minimum element. So now we only have to prove that any not empty finite subset $A$ of $X$ has a maximum element. So we consider the inverse relation $\preccurlyeq$ defined as $x\preccurlyeq y\iff y\le x$ for any $x,y\in X$ : clearly $\preccurlyeq$ is a total order, since indeed $\le$ is a total order, and any minimum in $\preccurlyeq$ is a maximum in $\le$ and so since any not empty finite subset $A$ has a minimum in $\preccurlyeq$ it follows that any not empty finite subset in $\le$ has a maximum element. So we concluded the proof. Is my proof correct? If not how prove the theorem? Could someone help me, please?","['elementary-set-theory', 'order-theory', 'induction', 'solution-verification']"
3688580,Show that S is an embedded $k$-submanifold of $M$,"Assume $M$ is a smooth manifold and $S$ is a subset of $M$ such that each point $p\in S$ has a neighborhood $U \subseteq M$ such that $S\cap U$ is an embedded $k$ -submanifold of $U$ . I want to show that from this it follows that $S$ is also an embedded $k$ -submanifold of $M$ . Until now I tried to exploit the $k$ -slice condition for embedded submanifolds $S\cap U$ of $U$ in order to construct a smooth chart $(V,\phi)$ of $M$ such that $V\cap S$ is a $k$ -slice of $V$ , i.e. to show that also $S$ satisfies the $k$ -slice condition and is therefore an embedded submanifold. But haven't succeeded yet. Can someone help and give me a hint how to approach this problem? Definition of a $k$ -submanifold:
A subset $S$ of $M$ is called embedded submanifold of dimension $k$ if for each $p\in S$ there a smooth chart $(U,\phi)$ of $M$ such that $p\in U$ and $U\cap S$ is a $k$ -slice of $U$ . Edit: Here are some details on my approach so far.
We know that $$\forall p \in S ~\exists U \subseteq \mathcal{U}_M(p):~ U\cap S \text{ embedded }~k\text{-submanifold of } U $$ $$\Rightarrow \forall p \in S ~\exists U \subseteq \mathcal{U}_M(p)~\forall q\in U\cap S~\exists (V,\phi) \text{ smooth chart of } U \text{ with } q\in V:$$ $$ U\cap S\cap V ~k\text{-slice of } V $$ But since $V$ is a coordinate neighborhood of $U$ , i.e. $V\subseteq U$ , and as noted in the comments $V$ is also a coordinate neighborhood of $M$ , we have $$\forall p \in S ~\exists U \subseteq \mathcal{U}_M(p)~\forall q\in U\cap S~\exists (V,\phi) \text{ smooth chart of } M\text{ with } q\in V:$$ $$ S\cap V ~k\text{-slice of } V $$ But since $p\in U\cap S$ we also find a smooth chart $(V,\phi)$ of $M$ with $p\in V$ , such that $S\cap V$ is a $k$ -slice of $V$ . Which means that by the definition above we know that $S$ is an embedded $k$ -submanifold of $M$ .","['smooth-manifolds', 'solution-verification', 'manifolds', 'general-topology', 'differential-geometry']"
3688584,How many ways can we choose a team of $16$ people with $1$ leader and $4$ deputies out of $75$ people?,How many ways can we choose a team of 16 people with 1 leader and 4 deputies out of 75 people? I figured that we can select the 16 people out of the 75 simply with $75\choose 16$ but then should i continue multiplying with $16\choose 4$ and $12\choose 1$ Final : $75\choose 16$ * $16\choose 4$ * $12\choose 1$,"['permutations', 'combinatorics', 'discrete-mathematics']"
3688599,Can we find $n$ such that $3061\cdot2^n +1$ is prime?,"Let $p=3061$ . Can we find an integer $n$ such that $2^n p+1$ is prime? If there is no such $n$ , how can we prove it? Or does such $n$ always exist for prime $p$ ? (More generally, instead of $p=3061$ , you can try e.g. $p=5297,5897,7013,8423,\ldots$ -- there are quite a few primes $p$ for which brute force does not seem to work.) Motivation: questions like these arise naturally while reading the paper On the density of odd integers of the form $(p − 1)2^{-n}$ and related questions by Paul Erdös and Andrew Odlyzko.","['number-theory', 'elementary-number-theory']"
3688632,Proving $R_{ij;m}=g^{kl} R_{ikjl;m}$.,"In the coordinate $\{x^i\}$ , the Riemann curvature tensor can be written as $$
R=R_{ijkl}\,dx^i\otimes dx^j\otimes dx^k \otimes dx^l
$$ and the Ricci curvature can be written as $$\text{Ric}=R_{ij}\,dx^i\otimes dx^j$$ where $R_{ij}=g^{kl} R_{kilj}\,$ .
  Prove that $$R_{ij;m}=g^{kl} R_{ikjl;m}$$ where $$\theta_{i_1i_2\cdots i_s;m}=\left(D_{\partial_m}\theta\right)(\partial_{i_1},\cdots,\partial_{i_s})\,.$$ My attempt I got a hint to prove $$R_{jk;m} = (g^{il}R_{ijkl})_{;m} =  g^{il}{}_{;m}R_{ijkl} + g^{il}R_{ijkl;m} = g^{il}R_{ijkl;m}\tag{1}$$ and $$g^{il}{}_{;m}=0 \,.\tag{2}$$ As for $(2)$ , I am not sure what the meaning of $g^{il}{}_{;m}$ is.
I thought that $$
g^{il}{}_{;m}=\left(D_{\partial_m}\,\check g\right)(dx^i,dx^l)
$$ where $\check g(\alpha,\beta)=g(\alpha^\sharp, \beta^\sharp)$ is the dual metric in which $\sharp$ denotes the musical isomorphism .
Am I right? I can prove that $Dg=0$ and hence $g_{ij;k}=0$ . I hope to use this to prove $(2)$ , and I tried to take advantage of $g^{il}g_{lj}=\delta^i_j$ . And then I guess that $$(g^{il}g_{lj})_{;m}=g^{il}{}_{;m}g_{lj}+g^{il}g_{lj;m}$$ but I don't know whether this is right, and I don't know how to get $(1)$ . They are similar and seem like leibniz rule. Could we conclude that? Any help would be highly appreciated!","['curvature', 'manifolds', 'riemannian-geometry', 'differential-geometry']"
