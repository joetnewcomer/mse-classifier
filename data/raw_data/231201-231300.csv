question_id,title,body,tags
4804734,Compactness of a set of functions on an infinite-dimensional function space,"Let X be the space of functions $f:\mathbb{N}\to\mathbb{Z}$ , endowed with the metric $d(f,g)=\sum_{i=0}^{\infty}{\frac{1}{2^{i}}\mathbb{I}(f(i)\neq g(i))}$ , where $\mathbb{I}$ is the indicatorfunction. Now fix $L\geq 0$ and let $K=\{f\in X:f(0)=0, \lvert f(i+1)-f(i)\rvert\leq L\}$ . Does anyone have any ideas how to show that $K$ is compact in $X$ ? Probably the sequential definition of compactness is the way to go but how?","['functional-analysis', 'metric-spaces']"
4804738,"Necessary and sufficient conditions for $\sum_{n = 1}^\infty \mathbb{E}\vert X_n \vert < \infty$ with $X_n \sim \mathcal{N}(\mu_n, \sigma_n^2)$","As the title says, I want to know if there is a simple necessity and sufficient condition for $$
    \sum_{n = 1}^\infty \mathbb{E}\vert X_n \vert < \infty
$$ where $X_n \sim \mathcal{N}(\mu_n, \sigma_n^2)$ .[ It comes from the fact that if this is true then: $$
    \mathbb{E}\left(\sum_{n = 1}^\infty X_n\right) = \sum_{n = 1}^\infty \mathbb{E}(X_n)
$$ ]. I am able to show that if $\sum_{n = 1}^\infty \sqrt{\mu_n^2  + \sigma_n^2} < \infty$ , then $\sum_{n = 1}^\infty \mathbb{E}\vert X_n \vert < \infty$ . Indeed,
since $$
    \mathbb{E}\vert X_n \vert \le \sqrt{\mathbb{E}(X_n^2)} = \sqrt{\mu_n^2 + \sigma_n^2}
$$ Another approach is I try to estimate $\mathbb{E}\vert X_n \vert$ by the identity: $$
     \mathbb{E}\vert X_n \vert = \int_0^\infty \mathbb{P}(\vert X_n \vert > t)dt
$$ But I couldn't come up with a good idea. Any references or hints are appreciated. Thank you","['expected-value', 'normal-distribution', 'probability-theory', 'reference-request']"
4804784,Finding the roots of $f(x) = \left(\pi^x-\frac{1}{\pi}\right)\cdot\left(x^\pi\right)\cdot\left(\frac{1}{\pi^2}-\pi^x\right)$,"One question from the IYMC 2023 was Find the roots of the function $$f(x) = \left(\pi^x-\frac{1}{\pi}\right)\cdot\left(x^\pi\right)\cdot\left(\frac{1}{\pi^2}-\pi^x\right)$$ Looking at it, I set each factor to $0$ and found $-1, -2$ and $0$ as roots. However, when I plot the graph in Desmos, the only root is $0$ , and the domain of $x$ gets limited to the reals greater than or equal to $0$ . When I try $f(-2)$ , Desmos returns ""undefined"", but when I try plug $-2$ into the formula in my calculator, I get $0$ . Is this just an error on Desmos?",['algebra-precalculus']
4804823,How to tell when a degree sequence has a unique unlabelled graph,"While solving the problem ""Determine all graphs with exactly one pair of vertices equal degree (all other degrees are distinct)"" for my intro to combinatorics class I was wondering if there is a way to tell when a degree sequence only has one unlabelled graph. I couldn't find any other threads related to this question about general (simple & undirected) graphs. For the specific problem I was working on, I have the graphs on n vertices with degree sequences: $(1,2,\dots,a,a,\dots,n-2,n-1)\text{ and }(0,1,2,\dots,b,b,\dots n-3,n-2)$ and this is how I determined the graphs that follow the degree sequence is unique. Based on how you can construct a graph with degree sequence $$(1,2,\dots,a,a,\dots,n-2,n-1)$$ I feel like it should be obvious that it has a unique unlabelled graph as: 1.) Create edges from one vertex to all others (so you have all points of degree one and one of degree n-1) 2.) Then continue by choosing any other point of degree one and create edges to all other vertex of degree one until your chosen vertex has degree n-2 (this should leave one vertex of degree one) 3.) Then continue by choosing any other point of degree two and create edges to all other vertex of degree two until your chosen vertex has degree n-3 (this should leave one vertex of degree two) Continue this until you have exactly two vertices of the same degree A similar algorithm works for creating a graph of degree sequence $(0,1,2,\dots,b,b,\dots n-3,n-2)$ and from there I found that $b=\lfloor\frac{n-1}{2}\rfloor$ and that the graphs from these two degree sequences are complements of each other so $a=(n-1)-\lfloor\frac{n-1}{2}\rfloor$ . Assuming this is correct, I was wondering when in general does a degree sequence have a unique unlabelled graph associated with it? I know typically this is not true. Is the case where this is only one pair of of vertices of equal degree the only time this is true (other than trivial sequences such as $(0,0,\dots,0)$ and $(1,1\dots,1)$ )?","['graph-theory', 'combinatorics']"
4804825,Set system containing no chain of length $3$,"Let $n$ be even and let $\mathcal{A}\subset\mathcal{P}(n)$ be a set system that contains no chain of length three. Prove that \begin{equation}|\mathcal{A}|\le{n\choose{n/2}}+{n\choose{n/2-1}}.\end{equation} My thoughts are that this reminds me of Sperner's Lemma, but this is a larger number than Sperner would give. I suppose we could try and use induction. For $n=2$ this is immediate. But actually applying an induction seems tricky. I don't know how to apply the condition that there is no chain of length three.","['solution-verification', 'combinatorics', 'extremal-combinatorics', 'analysis']"
4804863,Has this pattern in Pascal's triangle been already found?,"I found this pattern in Pascal's triangle and I was wondering if it has already been found before : 1                                +(1*1⁰) =  1  = 0!
          1   1                           -(1*1¹)+(1*2¹) =  1  = 1!
        1   2   1                      +(1*1²)-(2*2²)+(1*3²) =  2  = 2!
      1   3   3   1                 -(1*1³)+(3*2³)-(3*3³)+(1*4³) =  6  = 3!
    1   4   6   4   1            +(1*1⁴)-(4*2⁴)+(6*3⁴)-(4*4⁴)+(1*5⁴) =  24  = 4!
  1   5   10  10  5   1       -(1*1⁵)+(5*2⁵)-(10*3⁵)+(10*4⁵)-(5*5⁵)+(1*6⁵) =  120 = 5!
1   6   15  20  15  6   1  +(1*1⁶)-(6*2⁶)+(15*3⁶)-(20*4⁶)+(15*5⁶)-(6*6⁶)+(1*7⁶) =  720 = 6!","['factorial', 'combinatorial-proofs', 'reference-request', 'binomial-coefficients', 'discrete-mathematics']"
4804877,Is there a sequence of unique terms $\{a_k\}$ such that $\sum\limits_{k=1}^\infty a_k=\prod\limits_{k=1}^\infty (1+a_k)\ne0$?,"Is there a sequence of unique terms $\{a_k\}$ such that $\sum\limits_{k=1}^\infty a_k=\prod\limits_{k=1}^\infty (1+a_k)\ne0$ ? I specify ""unique terms"" to rule out trivial cases like $a_1=2$ , $a_2=-0.5$ and the other terms all $0$ . I specify "" $\ne0$ "" to rule out trivial cases like $a_1=-1$ , $a_2=1$ and the other terms sum to $0$ . Certainly some of the terms must be negative, because the expansion of the product contains the series in addition to other terms. Context: From another question , we see that $\int_0^\infty\left(\frac{\sin x}{x}\right)^2\mathrm dx=\dfrac{\pi}{2}$ , and it is conjectured that $\prod\limits_{k=1}^\infty\left(1+\int_{k}^{k+1}\left(\frac{\sin (\pi x)}{x}\right)^2\mathrm dx\right)=\dfrac{\pi}{2}$ . The functions inside each integral are not exactly the same. But anyway, that question made me wonder if there can be a sequence of unique terms $\{a_k\}$ such that $\sum\limits_{k=1}^\infty a_k=\prod\limits_{k=1}^\infty (1+a_k)\ne0$ .","['infinite-product', 'sequences-and-series']"
4804981,Why does this trick make the oscillating exponential integral converge?,"I have the following integral: $$\int_0^{+\infty} e^{iEt} dt,$$ where $E$ is a real constant.
I know this integral does not converge. However, I have seen the following trick which makes it converge: \begin{align}
\int_0^{+\infty} e^{iEt} dt &= \lim_{\epsilon \rightarrow 0} \int_0^{+\infty} e^{i(E+i\epsilon) t} dt\\
&= \lim_{\epsilon \rightarrow 0}\left[ \frac{1}{i(E+i\epsilon)} e^{i(E+i\epsilon) t} \right] _0^{+\infty}\\
&=\frac{1}{iE}
\end{align} If it was just for this, I would have thought that this is completely nonsense, and that it is like thinking that the limit of a discontinuous function at a point actually equals the function at that point. However, if one uses this trick when calculating the solution of the non-homogeneous Klein-Gordon equation $$(\square-m^2)\phi(\vec{x})=g\delta^3(\vec{x})$$ where $g$ is a real constant, using the Green function $G(x)$ , where $x$ is a 4d vector, such that $$-(\square-m^2)G(x)=\delta^4(x)$$ the result is actually the Yukawa potential, which can obviously be calculated using other methods.
I suspect that the delta is playing some role here, but I do not know distribution theory so I am asking here. What is really going on mathematically?","['complex-analysis', 'improper-integrals', 'mathematical-physics', 'distribution-theory']"
4805043,How to find the point of intersection of two lines when we have the equations without plotting?,"Equation of line one is $y=a(\alpha-\beta)(x-\alpha)$ . Equation of line two is $y=a(\beta-\alpha)(x-\beta)$ . We have to find the point of intersection of these lines
which is $\alpha+\beta$ by 2 as given in the question. Let me know if any information is missing! What I tried to do : Equated both equations since they are equal to y but I did not get the answer, I maybe made a mistake while distributing.",['geometry']
4805072,Turning a Sum Into an Integral,"I came up with a measure of dispersion for a sequence of $n$ numbers $(x_i)$ , namely $\sqrt{\sum^n_{i=1}x^2_i - \sum^n_{j=2}\sum^{j-1}_{i=1}\frac{2x_i x_j}{n-1}}$ .  Is there a way to generalize this sum by turning it into an integral for probability distribution functions?","['statistics', 'probability-limit-theorems', 'probability-distributions', 'real-analysis', 'random-variables']"
4805097,Are all Henstock-Kurzweil integrable functions expressible as the sum of a Lebesgue and an improper Riemann integrable function?,"This question is based on this post , where in the comments, Toby Bartels conjectures that every Henstock-Kurzweil (gauge) integrable function $f\in\mathcal{HK}$ can be expressed as $f= g + h$ for a Lebesgue integrable function $g\in\mathcal{L}$ and improper Riemann integrable function $h\in\mathcal{R}^*$ Is this true? Intuitively to me this seems obviously false, since $\mathcal{HK}$ is strictly larger than $\mathcal{L}$ and $\mathcal{R}^*$ , but I can’t think of any counterexamples or a way to prove a counterexample exists. If a counterexample exists, I know from properties of the HK integral that it must not have compact support, $|f|$ must not be HK integrable, and $f$ is not non-negative, as any of these conditions would imply that it would be Lebesgue integrable. If this isn’t true, as I suspect, can we prove a more general statement, let’s say for $f=g\circ h$ . How would one go about proving this true or false?","['measure-theory', 'lebesgue-integral', 'gauge-integral', 'real-analysis']"
4805107,General formula for the definite integral of form $\int \frac{dx}{\tan(ax)^n}$,"I am not entirely sure where I went wrong with my approach so I wanted to share it with you guys, thanks for any insights or for spotting some silly mistake. $$\int \frac{dx}{\tan(ax)^n}$$ First, I made a substitution as such: $u = \tan(ax)$ thus obtaining $$a\sec(ax)^2dx = du$$ rewriting it gives us $$a(\tan(ax)^2+1)dx = du$$ Then I rewrote the integral with the substitution as follows: $$\frac{1}{a}\int \frac{du}{u^n(u^2 + 1)}$$ Then I integrate as follows  (following step is wrong, this should be done with partial decomposition but is it possible?): $$\frac{1}{a}\int \frac{du}{u^n(u^2 + 1)} = \frac{1}{a}\ln \left| u^{n+2} + u^n \right| \cdot \frac{1}{(n+1)u^{n+1} + (n-1)u^{n-1}} + C$$ Then substituting back results in: $$\frac{1}{a}\ln \left| u^{n+2} + u^n \right| \cdot \frac{1}{(n+1)u^{n+1}} + (n-1)u^{n-1} + C = \frac{1}{a}\ln \left| \tan(ax)^{n+2} + \tan(ax)^n \right| \cdot \frac{1}{(n+1)\tan(ax)^{n+1} + (n-1)\tan(ax)^{n-1}} + C$$ Testing my answer with $\cot(3x)^4$ I get a very similar graph to the answer in the book but it is slightly off I can't seem to find the mistake. Thanks for any insights. It seems that my mistake is the integration step as I do not get a log integral, it is wrong. I should consider partial decomposition, but then is it possible to generalise it? $\textbf{Update:}$ so far I have only realised that if we play around with different powers $n$ we notice that if there is a general expression, it is different for odd and even powers; I conjecture that we indeed can find it, but two separate expressions.","['integration', 'analysis', 'real-analysis', 'calculus', 'indefinite-integrals']"
4805117,"Find all possible values of $H(x,y,z,t) = \frac{x}{t+x+y}+\frac{y}{x+y+z}+\frac{z}{y+z+t}+\frac{t}{z+t+x}$ if $x, y, z, t > 0$.","If $x, y, z, t > 0$ , find all possible values of $H(x,y,z,t) = \frac{x}{t+x+y}+\frac{y}{x+y+z}+\frac{z}{y+z+t}+\frac{t}{z+t+x}$ . How I think this can be solved: First off, note that $H$ is an homogeneous function of grade $0$ , which implies that for any point $x_1\in\mathbb{R^+}^4$ , there exists another point $x_2$ in the hypersphere $x^2+y^2+z^2+t^2=1$ which has the same image (you just have to normalize $x_1$ to find $x_2$ ). So by adding the condition $x^2+y^2+z^2+t^2=1$ you are not excluding any value from the image of the function. This may be useful to apply some kind of inequality. I think this problem can be solved by finding a maximum and a minimum value for this expression and then using the Intermediate Value Theorem to justify all values in between the maximum and minimum are in the image of the function, but I haven't succeeded finding that maximum or minimum. This exercise is right next to another one that is solved using Cauchy-Schwarz inequality, so I suspect it might have something to do with a famous inequality.","['maxima-minima', 'multivariable-calculus', 'functional-inequalities', 'inequality']"
4805158,Yoneda lemma for affine Hom functors,"I am trying to prove Exercise 1.5.4 from Brian Conrad's course notes on abelian varieties. Let $S = \mathrm{Spec} R$ , let $X, Y$ be $S$ -schemes, and let $h_{\mathrm{aff}, X}(-)$ denote the Hom functor $\mathrm{Hom}_{S}(-, X)$ on the category of affine $S$ -schemes (hence not representable if $X$ is not affine). The goal of the exercise is to establish a bijection $\mathrm{Hom}_{S}(X, Y) \cong \mathrm{Nat}(h_{\mathrm{aff}, X},h_{\mathrm{aff}, Y})$ , strengthening the usual Yoneda lemma when the base scheme is affine. This is useful because it is often easier to consider functors of points from affine schemes rather than arbitrary schemes. The usual proof of Yoneda does not seem to work since the functor $h_{\mathrm{aff}, Y}(-)$ does not explicitly include the data of morphisms $\mathrm{Hom}_{S}(X, Y)$ if $X$ is not affine. I think I have succeeded in proving the exercise in the case where $X$ is separated over $S$ . If $\eta$ is a natural transformation, we may cover $X$ by open affines $U_i$ and consider the induced maps $\eta_{U_i}: \mathrm{Hom}_{S}(U_i, X) \to \mathrm{Hom}_{S}(U_i, Y)$ . The open embeddings $U_i, U_j \hookrightarrow X$ map to the same element of $\mathrm{Hom}_{S}(U_i \cap U_j, X)$ , hence by naturality of $\eta$ the induced morphisms $U_i \to Y, U_j \to Y$ agree on $U_i \cap U_j$ , hence glue to a unique morphism $X \to Y$ . However, we can only make this argument if $U_i \cap U_j$ is known to be affine; one way to guarantee this if $X$ is separated over $S$ or some other affine scheme. Is the separatedness hypothesis necessary, or is there a way to get around it?","['algebraic-geometry', 'category-theory']"
4805168,Concrete Mathematics: Sum of on or above main diagonal of an array (deriving equation 2.33),"I would like to check my understanding of an Iversonian equation presented shortly before 2.33 (page 37 in paper book) of Concrete Mathematics (Graham, Knuth, Patashnik).  They discuss deriving a simple formula for finding the sum of all elements on or above the main diagonal of an array. The book says: Our goal is to find a simple formula for $$
S_◹ = \sum_{1 \le j \le k \le n} a_ja_k
$$ I get that part however I am unsure of the Iversonian equation: $$
[1 \le j \le k \le n] + [1 \le k \le j \le n] = [1 \le j,k \le n] + [1 \le j=k \le n]
$$ Why is it not simply: $$
[1 \le j \le k \le n] + [1 \le k \le j \le n] = [1 \le j,k \le n]
$$ I think this is because, in the process of doing the LHS $[1 \le j \le k \le n] + [1 \le k \le j \le n]$ we count the diagonal twice. Hence adding it on again on the RHS of that Iversonian equation.","['summation', 'discrete-mathematics']"
4805170,Comprehension problem regarding exercise in chapter on Fundamental Theorem of Calculus,"I am retired and working my way through a Stewart Calculus book for personal interest.  I am having problems understanding part of an exercise at the end of the chapter on the Fundamental Theorem of Calculus.  This is basic, introductory material so please bear with me. The last exercise in this chapter has 4 parts (a, b, c and d).  I am having problems with part b.  I am hoping a reader here will help me understand what exactly is being requested as the solution.  The exercise states the following: A high-tech company purchases a new computing system whose initial value is $V$ .  The system will depreciate at the rate $f=f(t)$ and will accumulate maintenance costs at the rate of $g=g(t)$ , where $t$ is the time measured in months.  The company wants to determine the optimal time to replace the system. Part b) Suppose that $$
f(t)= 
\begin{cases}
\frac{V}{15}-\frac{V}{450}t, & if \ 0 \lt t \le30 \\[2ex]
0, & if \ t>30
\end{cases}
$$ and $$
g(t)=\frac{Vt^2}{12,900},\ t>0
$$ Determine the length of time $T$ for the total depreciation $D(t)= \int_{0}^{t}f(s)ds$ to equal the initial value $V$ . Here are my questions: This part asks us to determine the total depreciation time $D(t)$ .  I understand this is exclusive of the maintenance rate.  In other words, the depreciation rate $f(t)$ is factored into the solution BUT the maintenance rate $g(t)$ is not (i.e. I can ignore the maintenance rate).  Is this correct?  If this is not correct then can you please explain why? Again, we are asked to determine the total depreciation time.  The formula for $f(t)$ shows the time interval of depreciation is $(0,30]$ .  I understand this to mean the computer system has fully depreciated when $t=30$ . So the total depreciation $D(t)=V$ is when $t=30$ . Is this correct? If so then this seems to me to the answer to part $b$ .  This however seems too simplistic an answer and therefore believe I have failed to comprehend exactly what is being asked, and what is required as a solution to part b.  Can someone help me understand where/what the mistake is that I am making? Thank you, Ian","['integration', 'calculus']"
4805179,"Are all complete lattices a pointed complete partial order, and vice versa?","A friend of mine asked for my help in drawing a venn diagram that includes the notions of partial orders (PO) in general, complete partial orders (CPO), pointed complete partial orders (CPPO), total orders (TO), lattices and complete lattices. Here are the relevant definitions: PO: A pair (W, R) where the relation R is reflexive, antisymmetric and transitive TO: A total (or linear) order is a PO (W, R) where all elements are comparable, i.e. for all x,y in W, either R(x,y) or R(y,x) Chain: A subset C of (W, R) is a chain if all elements in C are totally ordered CPO: A partial order (W, R) where every non-empty chain in W has a least upper bound (supremum) in W CPPO: A complete partial order with a least element, i.e. an element '0' such that for all x in W, we have R(0,x) Lattice: A PO where any two elements have a supremum and an infimum (greatest lower bound) Complete Lattice: A lattice where every subset of W has a sup and an inf. This Venn Diagram needs to show which of these categories are included in which. Additionally, he needs to give an example for each intersecting or non-intersecting part of every set of relational structures. Now, so far he knows that all categories fall within the notion of 'partial order', so everything else is included in that, and of course all complete lattices are lattices and all CPPOs are CPOs. Additionally, I helped him by writing a proof that every complete lattice is a cppo (see below). However, I am not 100% sure of this proof, and whether a CPPO that is a lattice is autimatically also a complete lattice. Neither of us can think of an example for a CPPO that is not a complete lattice, but still a non-complete lattice.","['lattice-orders', 'order-theory', 'discrete-mathematics']"
4805193,If $\int_a^b f(x) \ dx > 0$ then $f(x) > 0$ a.e?,"Let $f$ be a measurable nonnegative function with domain $\mathbb{R}$ , such that \begin{equation}
\int_a^b f(x) \ dx > 0
\end{equation} when $a < b$ . Can we say that $f>0$ almost everywhere? (or $f\neq 0$ almost everywhere, because is a nonnegative function) We work with Lebesgue measure and the integral above is a Lebesgue integral. I'm thinking that the answer is yes, but I can't prove it. I tried to get a contradiction if I suppose that $m(\{ f=0 \}) > 0$ , or maybe a subset of $\{ f=0 \}$ with measure greater than $0$ , but I got stuck. I already showed that for any open set $G$ we have \begin{equation}
\int_G f(x) \ dx > 0
\end{equation} So maybe I could find an open set $G$ such that $m(G) = m(\{ f=0 \})$ , but I can't. And that's it, I have no more ideas :( Thanks","['measure-theory', 'lebesgue-measure', 'lebesgue-integral', 'real-analysis']"
4805233,Is it possible for a normal subgroup of a finite group have greater number of elements in the minimal generating set?,"Let $G$ be a finite group, and $1 \lhd N \lhd G$ . With $G = \langle A \rangle$ and $N = \langle B \rangle$ be minimal. Is it possible for $|B|>|A| $ ? Main motivation behind this question was comparison between $A_n$ and $S_n$ . Both groups can be generated via 2 elements, but in some sense, it is much harder to come up with 2 element generator for $A_n$ than $S_n$ , and I am wondering if the inequality is actually possible to attain? And of course we need the assumption that $G$ is finite because all sorts of funny business happens with infinite groups. Edit:
I have discovered that $D_8 \times C_2 < S_6$ , if i throw normality condition away, it is possible","['finite-groups', 'finitely-generated', 'normal-subgroups', 'abstract-algebra', 'group-theory']"
4805238,Symmetry in Probability (AMC 12A 2023),"Flora the frog starts at $0$ on the number line and makes a sequence of
jumps to the right. In any one jump, independent of previous jumps,
Flora leaps a positive integer distance $m$ with probability $\frac{1}{2^m}$ . What
is the probability that Flora will eventually land at $10$ ? (AMC 12A
2023/17) Solution 1 says: At any point, the probabilities of landing at $10$ and landing past $10$ are exactly the same. Therefore, the probability must be $\frac{1}{2}$ . If you apply any of solutions 2(recursion), 3 (combinations), or 7(induction), then solution 1 follows. But is there some elaboration of solution 1 that does not include 2, 3, or 7. Or some really elegant way to solve the question?","['contest-math', 'recursion', 'combinatorics', 'induction', 'probability']"
4805254,An inequality of a complex polynomial,"Let $$P(z)=c_nz^n+...+c_1z+c_0$$ be a polynomial with complex
coefficients, where $c_n\neq 0$ . For each $r>0$ , define $$M(r):=\sup_{|z|=r}|P(z)|.$$ Prove that for any $K>1$ , the following
inequlity holds $$M(Kr)\le K^nM(r).$$ I believe a proper use of the Hadamard’s three-circles theorem will solve it, but I couldn't find the right annulus(the right choice of $r_1$ , $r_2$ and $t$ in the theorem). Also I tried to use induction on $n$ but again didn't get nowhere. I also tried to calculate it by using triangle inequality: \begin{align*}
        M(Kr)&=\sup_{|z|=Kr}|c_nz^n+...+c_1z+c_0|\\
        &\le |c_n||K|^n|r|^n+...+|c_1||K||r|+|c_0|\\
        &< K^n(|c_n||r|^n+...+|c_1||r|+|c_0|)\\
\end{align*} But then I don't know where to go from there. So I'm curious about the solution to this problem. Any comment or hint are appreciated.","['complex-analysis', 'inequality', 'polynomials']"
4805268,Solving $\lim_{n\to\infty}\sqrt n\int_0^{\frac12}(1-3x^2+x^4)^n\mathrm{d}x$,"$$\lim_{n\to\infty}\sqrt n\int_0^{\frac12}(1-3x^2+x^4)^n\:\mathrm{d}x=0.5116...$$ Attempting the limit above. I'm only average in calculus, and none of the usual methods seem to work. I did find these equivalent forms of $1-3x^2+x^4$ that are useless to me but may give you some ideas: $(x+\varphi)(x-\varphi)(x+\varphi-1)(x-\varphi+1)$ , where $\varphi=\frac{1+\sqrt5}2$ $(x^2-\varphi^2)(x^2-(\varphi-1)^2)$ $(x^2+(2\varphi-1)x+1)(x^2-(2\varphi-1)x+1)$ $(x^2-x-1)(x^2+x-1)$ $(x^2-1)^2-x^2$ $\left(x^2-\frac32\right)^2-\frac54$ If the polynomial's domain is restricted to $[0,\varphi-1]$ , its inverse is $\sqrt{\frac32-\sqrt{x+\frac54}}.$ I've found these two infinite series that should be equivalent to the limit by ""solving"" the integral. The first series is derived from expanding $\left(\left(x^2-\frac32\right)^2-\frac54\right)^n$ with the binomial theorem: $$\lim_{n\to\infty}\sqrt n\left(-\frac54\right)^n\sum_{m=0}^n\binom nm\left(-\frac95\right)^m\sum_{k=0}^{2m}\frac1{2k+1}\binom{2m}k\left(-\frac16\right)^k$$ Wolfram Alpha evaluated the indefinite integral as an infinite series , so calculating the integral itself is likely a bad idea. Attached to its answer was a unit complex correction term of some sort that should be $1$ when $0\le x\le\frac12$ . The second series simplifies the output from Wolfram Alpha, where $x^{\bar n}=x(x+1)\cdots(x+n-1)$ is the rising factorial: $$\lim_{n\to\infty}\sqrt{n}\sum_{i=0}^{\infty}\sum_{j=0}^{\infty}\frac{\varphi^{2\left(i-j\right)}}{\left(2i+2j+1\right)4^{i+j}}\frac{(-n)^{\bar i}}{i!}\frac{(-n)^{\bar{j}}}{j!}$$ The only context I have for this problem was that I received it from someone else who sent it after getting stuck, who had received it from someone else who sent it after getting stuck. How should this integral be solved?","['integration', 'limits', 'calculus']"
4805279,When do we need to account for permutations?,"I was solving the following 2 problems: Let A be a 2n-element set where n≥1. Find the number of different
pairings of A. and If there must be at least one person in each table, in how many ways can 6 people be seated in 3 tables? Assuming that the tables are indistinguishable. I know that for the first, the number of pairings is $\frac{1}{n!}\binom{2n}{2}\binom{2n - 2}{2}\binom{2n - 4}{2} \cdots \binom{6}{2}\binom{4}{2}\binom{2}{2}\\$ . If my understanding is correct, what we are doing is picking pairs, and the picking pairs from the remaining number. The reason we divide by n! is because we we don't care about the order of the pairs. The formula above without the n! gives us the permutations of pairs. What I don't understand is why we don't apply that same logic to the second problem. We can split the problem up into these distinct groups: (1/1/4,1/2/3,2/2/2). For the second group (1/2/3), the correct answer is: $\hspace{.2 in}\dbinom{6}{3}\cdot2\cdot\dbinom{3}{2}=20(2)(3)=120$ I don't understand why we don't need to divide this by something. I understand that the groups aren't identical in the same way the pairs in problem 1 were identical, but I don't see why that matters.",['combinatorics']
4805306,A connected set which is not path-wise connected,"Let $S$ be the union of $A$ and $B$ which are defined as $$A=\left\{(x,y): 0<x\leqslant1,y=\sin\left(\frac 1x \right)\right\}\,$$ and $$B=\left\{(x,y): -1\le x \le 0, y=0 \right\}\,$$ Then I am using this definition of connected set which say that $S$ is connected if it cannot be expressed as the union of two sets such that $\,\overline A\bigcap B= \varnothing\,$ or $\,A\bigcap\overline B=\varnothing\,$ . My Attempt: Here clearly $\overline B=B$ which doesn't intersect with $A$ . So $S$ has been expressed as a union of two sets such that $A\bigcap\overline B=\varnothing$ . So it is disconnected.
But the complex analysis text (by Kasana) where I found this set says it is connected but not pathwise connected. I can see why it is not path-connected because I found from wikipedia that the set $A$ is actually a part of topological sine curve and there is no path which connect the set $B$ to set $A$ lying entirely in S, but I am unable to understand it's connectedness. My background: I haven't study any topology book yet, also I am learning mostly by myself so an answer with less advanced terms used in topology is much appreciated however I know basic set topology such as open set, closed set etc. I also tried searching here and found many questions on topological sine curve but they were using different definitions of connected, and I don't find them duplicate so apologies if I overlook something and it's a duplicate.","['general-topology', 'connectedness']"
4805336,Series with only positive integer coefficients,"I want to show that the coefficients of the series $$ 
f(x) = \sqrt[4]{\frac{1+4x}{1-4x}} = 1 + 2x + 2x^2 + 12x^3 + \cdots
$$ are only positive integers. Since differentiating $f(x)$ shows $\left(1 - 16x^2\right)f'(x) = 2f(x)$ , we can compare the coefficients of $x^{n+1}$ on both sides to get the following recurrence relation for the coefficient $a_n$ of $x^n$ in $f(x)$ : $$
(n+2)a_{n+2} = 2a_{n+1} + 16na_n
$$ Now it is easy to see by induction that $a_n$ is positive. However, I'm not sure how to approach showing that it is an integer. Is there a way to inductively show that the right-hand side of the recurrence divided by $\left( n+2 \right)$ , or can we show that it's an integer in a different way?
Any comment or idea would be greatly appreciated.","['power-series', 'integers', 'recurrence-relations', 'discrete-mathematics']"
4805351,The number of real solutions of the equation $e^x=3x$,"The number of real solutions of the equation $e^x=3x$ is ______. I have checked the solution in desmos.com the answer is ""2"". But I am not able to solve it I tried using $x=\ln(3x)$ but could not proceed further. As there any general method I could find the number of intersection of the equation $e^x=ax$ where $a>0$",['functions']
4805352,Finding a suitable substitution for integral,"I'm struggling to find a suitable substitution for this integral: $$
\int{\frac{\sqrt{x^2+4}}{x}}
$$ I've tried $u=x^2+4$ , $u^2=x^2+4$ and some trigonemetric identities, but not much progress. Can anybody help me figure out how to get to the answer: $$
\sqrt{x^2+4}+\ln\left\vert\frac{\sqrt{x^2+4}-2}{\sqrt{x^2+4}+2}\right\vert + c
$$",['integration']
4805417,How to integrate $\int \frac{(x^{4}+x^{7})^{\frac{1}{4}}}{x^{2}}dx$?,"How to evaluate $$\int \frac{(x^{4}+x^{7})^{\frac{1}{4}}}{x^{2}}dx$$ ? I have been trying this question for nearly one month but I can't solve it. I am trying this question by taking $x^{4}$ common. Now the integral will become $$\int \frac{x(1+x^{3})^{\frac{1}{4}}}{x^{2}}dx$$ . Now I multiplied the numerator and denominator by $x$ . (This was my trick). So, the integral will become $$\int \frac{x^{2}(1+x^{3})^{\frac{1}{4}}}{x^{3}}dx$$ . Now, I substituted $x^{3}=u$ . Then $x^{2}dx=\frac{1}{3}du$ . So, the above integral will become $$\frac{1}{3}\int \frac{(1+u)^{\frac{1}{4}}}{u}du$$ . Now I thought of substituting $(1+u)=t$ . So, $du=dt$ . Now, the above integral will become $$\frac{1}{3}\int \frac{t^{\frac{1}{4}}}{t-1}dt$$ . Now I am thinking of substituting $t=\sec^{2}\theta$ . Now $dt=2\sec^{2}\theta×\tan\theta×d\theta$ .So, the above integral will become $$\frac{2}{3}\int \frac{(\sec\theta)^{\frac{5}{2}}}{\tan\theta}d\theta$$ . But after this step, I can't approach further. Please help me out.","['integration', 'indefinite-integrals', 'calculus']"
4805463,Intuitions for $\mathbb{E}|\sum_{i=1}^n\mathbf{X}_i|= n{n-1\choose (n-1)/2}2^{1-n}$,"Suppose $\{\mathbf{X}_1,\mathbf{X}_2,\cdots,\mathbf{X}_n\}$ are $n$ iid random variables uniformly drawn from $\{-1,1\}$ and $n$ is odd. Then \begin{align}
 \mathbb{E}|\sum_{i=1}^n\mathbf{X}_i|= 2\sum_{k=0}^{(n-1)/2} {n\choose k}(2n-k)2^{-n}=n{n-1\choose (n-1)/2}2^{1-n}
\end{align} The above can be verified by some calculations. Why does it hold that every $\mathbf{X}_i$ 's contribution to $\mathbb{E}|\sum_{i=1}^n\mathbf{X}_i|$ is the probability that exactly half of the other random variables are taken $1$ and half are taken $-1$ ? This can be seen from simply do some calculations but are there any intuitive explanations?","['expected-value', 'intuition', 'combinatorics', 'probability']"
4805473,Find supremum of an integral,"Let $F$ be the set of all continuous functions $f : [1, 3] \to [-1, 1]$ such that $\int_{1}^{3} f(x) = 0.$ Then, find $$ \sup \int_{1}^{3} \frac{f(x)}{x} dx,$$ where $f \in F$ . Now, my first thought was to try Cauchy - Schwarz inequality, but it can only be applied when all terms are positive, but $f(x)$ is not always positive. Then, I tried to bound $f(x)$ using range of $f$ . So, $ -1 \le f(x) \le 1, \forall x \in [1,3] \implies \frac{-1}{x} \le \frac{f(x)}{x} \le \frac{1}{x}, \forall x \in [1,3]$ . We can multiply with $\frac{1}{x}$ since it is defined in the domain $[1,3]$ . Then, we integrate, which we can, since all three of them are integrable on $[1,3]$ . Since we need to find an upper bound we can ignore the left inequality and proceed as follows: $$\int_{1}^{3} \frac{f(x)}{x} dx \le \int_{1}^{3} \frac{1}{x} dx = \ln3.$$ Now, I have no idea how to show it is the least upper bound (I tried using the definition of LUB, assuming another arbitrary upper bound and showing that $\ln3$ is smaller, but it didn't work). Also, I suspect it may not be the LUB at all. Where am I going wrong?","['definite-integrals', 'real-analysis', 'upper-lower-bounds', 'inequality', 'supremum-and-infimum']"
4805486,Question about objects in set theory. Terence Tao Analysis 1,"I am reading the chapter on set theory in Terence Tao's Analysis I.He gives an axiom on singleton sets which goes
""If a is an object, then there
exists a set {a} whose only element is a, i.e., for every object y, we have
y ∈ {a} if and only if y = a;""
My question is, how do we know that equality is defined for the objects in discussion.Is there an axiom ? The author gives a definition on when two sets are considered to be equal but he isn't clear about whether all objects are sets or not.",['elementary-set-theory']
4805504,Behaviour and limits of $f(n+1) = \frac{f^5(n)}{2} - f(n-1)$,"Let $f(0) = 0,f(1) = \frac{1}{2}$ and $$f(n+1) = \frac{f^5(n)}{2} - f(n-1)$$ where $*^5$ is a power. Then it seems $$ \sup f(n) = \lim \sup f(n) = \frac{1}{2}$$ and $$ \inf f(n) = \lim \inf f(n) = \frac{-1}{2}$$ Is this true ? How to prove it ? background Why I wonder about this ? Well for most values $0<q<1$ I found that $f(0) = 0,f(1) = q$ and $$f(n+1) = \frac{f^5(n)}{2} - f(n-1)$$ Then $$ \sup f(n) = \lim \sup f(n) \neq q$$ and $$ \inf f(n) = \lim \inf f(n) \neq -q$$ So it seems remarkable that $f(1) = 1/2$ does satisfy it.","['limsup-and-liminf', 'fractions', 'polynomials', 'limits', 'dynamical-systems']"
4805570,Prove that $2 \int_{x}^{x+1} \log(t) dt \geq \log(x (x+1))$.,"Prove that $2 \int_{x}^{x+1} \log(t) \,dt \geq \log(x (x+1))$ . I want proof of this without using geometry. I need to know the techniques to solve this other than geometry. One can easily see this inequality is true for $x>0$ just by drawing $\log(t)$ (real) graph and observing the area of the trapezium which joins $x, x+1, \log(x+1)$ and $\log(x)$ is less than area under the curve between $x$ and $x+1$ .","['calculus', 'logarithms', 'geometry', 'inequality']"
4805596,"is $\min\{\|f\|_\infty , \|f\|_1\}$ a norm on $C[0,1]$ ? proof [duplicate]","This question already has an answer here : If $a>1$ then $\|f||=\min \left\{\max\{|f(t)| : t \in [0,1]\}, a \int_{0}^{1} |f(t)|dt \right\}$ is not a norm in $C[0,1]$ (1 answer) Closed 8 months ago . I have the following question: For $a>0$ and $\|f\|_\infty=\sup_{t \in [0,1]} |f(t)|$ $\|f\|_1 =a\cdot \int_0^1 |f(t)|dt$ Show that $\|f\|:=\min\{\|f\|_1,\|f\|_\infty\}$ is a norm on $C[0,1]$ only iff $a\le1$ I tried to go through the all $3$ characteristics of a norm but i never used the fact that a hast to lower $1$ ... i supsect that for $a>1$ we may get a problem with the triangle-equation... but i dont't see where this problem might come from??","['normed-spaces', 'functional-analysis']"
4805672,Grade 12 Functions,"I'm struggling with the following: Describe a function that has a. a domain of $\{x \in \mathbb{R} \mid 2 \leq x \leq 10\}$ , and b. a range of $\{y \in \mathbb{R} \mid 5 \leq y \leq 10\}$ . Your function does not need to be a polynomial or rational function, and it can be described algebraically, as a graph, or in another form. It's a question from a self-directed online grade 12 functions course I'm doing. I understand that the domain and range are restricted by the given parameters, but how do I CREATE the function to fit within them? I don't feel the unit focused on it from this perspective. Any feedback would be greatly appreciated.","['algebra-precalculus', 'functions']"
4805691,"Properties of $\phi = \hat f \hat g$ for $f,g \in L^1(\Bbb R) \cap L^2(\Bbb R)$","Let $f,g \in L^1 (\Bbb R) \cap L^2(\Bbb R)$ and define $\phi(x) = \widehat f(x) \widehat g(x)$ for all $x\in \Bbb R$ . First of all, I'd like to show that $\phi \in L^1 (\Bbb R) \cap L^2(\Bbb R)$ . As $f,g \in L^2(\Bbb R)$ , we have $\hat f,\hat g \in L^2(\Bbb R)$ . That $\phi \in L^1(\Bbb R)$ follows from $\|\phi\|_1 \le \|\hat f\|_2\|\hat g\|_2 < \infty$ . Next, $|\phi| \le |\hat f \hat g| \le \|f\|_1 |\hat g|$ gives $\|\phi\|_2 \le \|f\|_1 \|\hat g\|_2 < \infty$ . So, $\phi \in L^1 (\Bbb R) \cap L^2(\Bbb R)$ . Next, suppose $f$ is even and $g$ is odd. I want to show $\lim_{\xi \to 0} \widehat \phi(\xi) = 0$ . As $f$ is even and $g$ is odd, we have $\widehat f(\xi) = \widehat f(-\xi)$ and $\widehat g(\xi) + \widehat g(-\xi) = 0$ for all $\xi \in \Bbb R$ . By continuity of $\widehat g$ , this gives $\widehat g(0) = \int_{\Bbb R} g(t)\, dt =  0$ . As $\phi = \hat f \hat g = \widehat{f\ast g}$ , we have $\widehat\phi(\xi) = \widehat{\widehat{f\ast g}}(\xi) = f\ast g(-\xi)$ . That is, $$\widehat\phi(\xi) =f\ast g(-\xi) = \int_{\Bbb R} f(t) g(-\xi - t)\, dt = -\int_{\Bbb R} f(t) g(\xi + t)\, dt$$ The substitution $u = -t$ gives $$-\int_{\Bbb R} f(t) g(\xi + t)\, dt = -\int_{\Bbb R} f(u)g(\xi - u)\, du = - f\ast g(\xi)$$ and so $$\widehat \phi(\xi) = \frac{f\ast g(-\xi) - f\ast g(\xi) }{2}$$ giving $$\lim_{\xi \to 0} \widehat \phi(\xi) = \lim_{\xi \to 0}\frac{f\ast g(-\xi) - f\ast g(\xi) }{2} = 0$$ Lastly, if $\operatorname{supp} f, \operatorname{supp} g \subset [0,1]$ , I want to show $$\int_{-\infty}^\infty \phi(x)\, dx = 0$$ Indeed, $$\int_{-\infty}^\infty \hat f(x) \hat g(x)\, dx  = \int_{-\infty}^\infty f(-x) g(x)\, dx = 0$$ as $\widehat{\widehat{f(x)}} = f(-x)$ and so $\operatorname{supp} \widehat{\widehat{f}} \subset [-1,0]$ . I'd like to know if my work is correct. Thank you!","['solution-verification', 'fourier-analysis', 'fourier-transform', 'analysis']"
4805698,Finding all set partitions where the elements in each part are restricted to specific subsets of the set.,"I want to find the number of ways x people can pick different amount of items from a set, where we can somewhat pinpoint which particular items each person can pick. My case has 3 people picking up to 7 things from up to 21 items. A specific case would be 3 people picking 3 things each from 9 items. In this case, we have a set S we want to partition into 3 parts each with a size 3. I found a formula that could help me find the number of ways to partition a set of size n, into k parts with different sizes r each. P(n,k,r)= n!/(r!)^k⋅k! This assumes all partitions are valid though. In this case, I want to restrict the first part of the partition to only select from a subset of parts. Similarly, I want to restrict the elements in the 2nd and 3rd part of the partition to different subsets of S . These subsets cover S but they're not disjoint. So let's say S = {1,2,3,4,5,6,7,8,9}. I want to know if there's a formula that can give me the number of ways I can make k partitions of size r where: k_1 picks from {1,2,3,4,5}
k_2 picks from {3,4,5,6,7,8}
k_3 picks from {1,5,7,8,9}. The closest I got is the generating function I got after trying this. I made a program that creates a Venn diagram from the sets. Instead of trying to find each partition, it finds all partition shapes. Part 1 of the partition can pick from what's strictly in it's set and not the others. It can pick from what's in the intersection of all of them, and what's in the intersection between it and the other two sets individually. I can find all ""shapes"" the partition can make by treating it like a multi-set and using a generating function to find the number of ways the size of the partition can be made from the items. A: {2}
A and B not C: {3,4}
A and C not B: {1}
A and B and C: {5} So the number of shapes the partitions can make is: (1+x)*(1+x)*(1+x)*(1+x+x^2) which gives us the coefficient 4 for x^3. Similarly I can do that for part 2 and part 3. I'm stuck in putting this all together. In how I can subtract the times where there would be overlap between the shapes P1, P2, and P3 make.","['set-partition', 'combinatorics', 'discrete-mathematics', 'generating-functions']"
4805769,Why is estimating the proportion of Democrats the same as estimating the bias of a coin?,"My textbook is talking about how estimating the proportion of Democrats in the population reduces to estimating the bias of a coin, which I wasn't seeing. Here is the paragraph I was reading: Consider the problem of estimating the proportion $p$ of Democrats in the US population, by taking a small random sample. We can model this as the problem of estimating the bias of a coin
above, where each coin toss corresponds to a person that we select randomly from the entire population.
And the coin tosses are independent: we are assuming here that the sampling is done “with replacement”; i.e., we select each person in the sample from the entire
population, including those we have already picked. So there is a small chance that we will pick the same person twice. I am not seeing how the problem of estimating the proportion of Democrats is equivalent to estimating the bias of a coin. There are 2 things I am confused with here: When we have a bunch of coin tosses from the same coin, each toss has an equal propensity to be heads; but people in a population don't have an equal propensity to be Democrat. And modelling this example as estimating the bias of a coin  assumes that the probability of every person being a Democrat is the same, it seems- are we treating the proportion of people which are Democrat as an equivalent notion to the propensity of a particular person to be Democrat? How is flipping a coin the same thing as sampling a person from the population? I see that while we could classify things into 2 possibilities in both cases, with Democrat and non-Democrat corresponding to heads and tails, there are an amount of possible outcomes equal to the amount of people in the population for sampling a person, making me feel that this is somehow different from when there are only 2 outcomes from flipping a coin. I would be very grateful if anyone could explain how these situations are similar in a way which resolves my confusions.","['statistical-inference', 'statistics', 'probability']"
4805777,Inequality $\frac{a}{\sqrt{b}} + \frac{b}{\sqrt{a}} \ge \sqrt[4]{8}$ on the unit circle,"While playing with a few two variable inequality and AM-GM inequality, I have ran into the following puzzle: Question: Show that if $a, b \in (0,1)$ and $a^2+b^2 = 1$ , then: $\dfrac{a}{\sqrt{b}} + \dfrac{b}{\sqrt{a}} \ge \sqrt[4]{8}$ with equality when $a = b = \dfrac{1}{\sqrt{2}}$ without using calculus. I played with it for hours and didn't get to the break point when I can ""see"" the answer. I used the substitution, AM-GM inequality, and even wolfram alpha to find the min value and I got $\sqrt[4]{8}$ . Originally, I planned to only find the min value and after I found it, I set out to prove it without using calculus . So no derivative is allowed. Any cool idea from experts here?","['substitution', 'algebra-precalculus', 'a.m.-g.m.-inequality', 'inequality']"
4805784,Is it possible to take S.S.S. Congruence criterion as a postulate and prove S.A.S. and A.S.A. through it?,"In all of the treatments of elementary Euclidean geometry which I've seen so far, the section about triangle congruences introduces S.A.S. criterion as the basic postulate from which A.S.A. and S.S.S. criteria are deduced. I remember reading somewhere that one could choose any one of these three as ""the congruence postulate"" and deduce others from it. I am able to produce proofs for S.A.S. by taking A.S.A. as an axiom and vice versa, but S.S.S. seems to be the ""odd"" one since I cannot reach either S.A.S. or A.S.A. by taking it as the axiom. I was unable to find anything online that shows such a proof so my question is whether the premise that any one of these three criteria can be picked as the axiom is true or not. If it is, how can we prove, for example, S.A.S. through S.S.S.?","['axiomatic-geometry', 'geometry']"
4805809,Prove that Brownian Motions have normal distribution using central limit theorem,"In the book Brownian Motion, 3rd edition by Rene Schilling, he defines a $d$ -dimensional Brownian motion $B = (B_t)_{t\geq0}$ indexed by $[0,\infty)$ taking values in $\mathbb R^d$ as a process that satisfies $$ \text{(B0)} \quad   B_0 = 0, \quad a.s \\
 \text{(B1)} \quad      B_{t_n} - B_{t_{n-1}}, B_{t_{n-1}} - B_{t_{n-2}},..., B_{t_1}- B_{t_0}, \quad \text{are independent for all} \quad 0 =t_0 \leq t_1 \leq...\leq t_n \\
    \text{(B2)} \quad   B_t - B_s \sim B_{t+h} - B_{s+h} \quad \text{for all} \quad 0 \leq s <t, h \geq -s \\
 \text{(B3)} \quad  B_t -B_s \sim N(0,t-s)^{\otimes d}, \quad N(0,t)(dx) = \frac{1}{\sqrt{2\pi t}} \exp \left( -\frac{x^2}{2t} \right)dx \\
 \text{(B4)} \quad  t \mapsto B_t(\omega) \quad \text{ is continuous for all $\omega$}
$$ The author claims that if we have B0, B1, B2, and B4, we will automatically have B3 as a consequence of central limit theorem but I don't see how it's done. Clearly, my first intuition is to divide $[0,t]$ into many sub-intervals, but no matter how we divide, $B_t$ can not be expressed as a limit ( $B_t = \lim_{n} \frac{1}{\sqrt n}(X_1+...+X_n)$ ) for some iid sequence $(X_n)$ to have central limit theorem in use.","['central-limit-theorem', 'normal-distribution', 'stochastic-processes', 'brownian-motion', 'probability-theory']"
4805864,"what positive numbers $a,b,c,d $ makes $\frac{x^a y^b}{x^c + y^d}$ bounded in every neighbourhood of $(0,0)$","I was playing with $\lim\limits_{(x,y) \to (0,0)} \frac{x^a y^b}{x^c + y^d}$ for $a,b,c,d ,x,y >0 $ and realised that $\frac{x^a y^b}{x^c + y^d}$ is bounded in every neighbourhood of $(0,0)$ if $c=2a , d=2b$ because the $AM-GM$ inequality so is it the only case where $z:=\frac{x^a y^b}{x^c + y^d}$ bounded ?
it is easy to see that eventually $x,y <1$ so if $z$ is bounded for some $a,b,c,d$ then any $a_1>a$ and $b_1>b$ and $c_1<c$ and $d_1<d$ would also work if $c=2a$ and $d>2b$ then the denominator will be $\frac{x^{a}}{y^b} +\frac{y^{b + \epsilon}}{x^a}$ choose some $0<k<\frac{a}{b+\epsilon }$ $y^{b } = x ^{a-k\epsilon}$ it follows that as $x \to 0$ $ \ z \to \infty $ which means if I replaced one power in the demonstrator with higher value than the $AM-GM$ form then $z$ is unbounded","['multivariable-calculus', 'calculus', 'upper-lower-bounds', 'limits', 'inequality']"
4805871,find the maximum value of $A$ satisfying the following inequality $3x^2+y^2+1\ge A(x^2+xy+x)$,"I would appreciate if somebody could help me with the following problem. For any two natural numbers $x$ , $y$ , find the maximum value of $A$ satisfying the following inequality $$3x^2+y^2+1\ge A(x^2+xy+x)$$ The above issue is the second round of the Korean secondary competition My work : I tried to transform it into a quadratic equation for $x$ and use the discriminant to find it, but it is too difficult to interpret the discriminant under the condition that it is a natural number.","['number-theory', 'inequality', 'quadratics', 'discriminant']"
4805896,"Show that if $B \subseteq Y$, then $f(f^{-1}(B))=B$","Let $X,Y$ be two nonempty sets and consider a function $f : X \rightarrow Y$ . Show that if $B \subseteq Y$ , then $f(f^{-1}(B))=B$ . My attempt: Let $y \in f(f^{-1}(B))$ . Then there is a $x \in f^{-1}(B)$ , and then a $y' \in B$ . But $y'=y$ , since $f(f^{-1}(y')) \neq y$ is impossible (but why...?). Hence $f(f^{-1}(B)) \subset B$ Next, let $y \in B$ . Then there is a $x \in f^{-1}(B)$ , and then a $y' \in f(x)$ . But $y'=y$ , since $f(f^{-1}(y')) \neq y$ is impossible (but why is this...?). Hence $B \subset f(f^{-1}(B))$ . Together, $f(f^{-1}(B))=B$ I regard this proof as flawed, at least incomplete. Why must $f(f^{-1}(y'))=y$ ? If $f$ would be a bijective, the inverse function $f^{-1}$ would be defined and the composition would be the ""identity function""... but the function is not necessarily injective and surjective. I also realise that making the assumption that $f(f^{-1}(y))=y$ actually is what we wanted to show in the first place... :) So, this is not a proof, I realise. Perhaps I could make an assumption that $f(f^{-1}(y))\neq y$ and see that this would lead to a contradiction, in some way...","['elementary-set-theory', 'functions']"
4805898,Understanding the difference between union and symmetric difference of set theory,"In Set Theory, Let $A$ and $B$ be two sets. Now, we denote an element which is either in $A$ or in $B$ as $A\cup B$ . And an element which is in both $A$ and $B$ as $A\cap B$ . But I can't understand why we denote an element which is either in $A$ or in $B$ as $A\cup B$ ? I think it should be denoted by $A\Delta B$ because $A\Delta B=(A\setminus B)\cup (B\setminus A)$ . Because $(A\setminus B)$ includes all the elements of $A$ which are not in $B$ and $(B\setminus A)$ includes all the elements of B which are not in $A$ . Please help me to clear my doubt.",['elementary-set-theory']
4805899,What is the difference between combinatorics and discrete mathematics?,"I wonder about the difference between the two areas combinatorics and discrete mathematics . On closer inspection, I was unable to pinpoint any concrete difference so far. Here are a few thoughts: It might be about the ""advancedness"". My impression is that dicrete math is mostly used in connection with freshman courses. ( Discrete mathematics for computer scientists , etc.), with the notable exception of the journal Discrete Mathematics . While I know about quite a few combinatorialists, I do not remember any mathematician describing himself as a discrete mathematician . So maybe the situation is a bit like calculus vs analysis: Discrete mathematics is basic, combinatorics is advanced. It might be connected to the historical development. My impression is that combinatorics was considered for quite some time as being restricted to basic counting operations based on permutations, combinations, binomial coefficients etc., resulting in combinatorics being a subfield of discrete mathematics. But if you look at modern definitions of combinatorics , they all agree that it is hard to give a precise definition, but also they agree that besides enumerative combinatorics (which is all the counting techniques lik generating functions, Pólya theory, Möbius inversion etc.), combinatorics includes the study of various combinatorial structures, like posets, graphs, matroids, finite geometries, combinatorial designs, error correcting codes etc. So if ""combinatorics is a subset of discrete mathematics"" should indeed be true: I would like to see a concrete example of a subject being discrete math, but not combinatorics. I was a bit surprised to find that the (oldschool?) viewpoint ""combinatorics = counting"" is also suggested by our MSE tag descriptions. From the tag description ""discrete mathematics : For questions about the study of finite or countable discrete structures, especially how to count or enumerate elements in a set (perhaps of all possibilities) or any subset. It includes questions on permutations, combinations, bijective proofs, and generating functions. From the tag description discrete mathematics Consider using a more specific tag instead, such as: (combinatorics), [...] Also note that on MSE, synonyms of combinatorics are enumerative-combinatorics (I don't agree, the existence of this refined notion clearly indicates that it is a subfield) and counting (I don't agree for the same reason, plus having the ""elementary"" feeling attached). Addition: I've looked at two reputable books on combinatorics to see what they include. Here is a selection from their table of contents: J. H. van Lint, R.M. Wilson. ""A Course in Combinatorics"": Graphs, Colorings of graphs and Ramsey's theorem, Flows in networks, Latin squares, Hadamard matrices, Codes and designs, Strongly regular graphs, Projective and combinatorial geometries, Association schemes. Peter J. Cameron. ""Combinatorics"": Latin squares, Finite geometry, Ramsey's Theorem, Graphs, Designs, Error-correcting codes, Graph colorings.","['definition', 'soft-question', 'combinatorics', 'discrete-mathematics']"
4805909,Some problems related to Achim Klenke: Probability theory 3.1.2,"The originally problem is as follows: Give an example for two different probability generating functions that coincide at countably many points $x_i ∈ (0, 1)$ , $i ∈ \mathbb N$ . In other words, find $\{a_n\}_{n \in \mathbb N},\{b_n\}_{n \in \mathbb N}$ , two different sequences of non negative real numbers which sum to $1$ , but such that: $$\sum_{n \in \mathbb N} a_nx^n = \sum_{n \in \mathbb N} b_nx^n $$ For countably infinite many points $x\in (0,1)$ . My thoughts are as follows, if we let the LHS be $f(x)$ and the RHS be $g(x)$ then $f,g$ are holomorphic with radius of convergence $1$ , and hence so is $f-g$ . If the zero set of $f-g$ (i.e. where $f=g$ ) has a limit point in the interior of the unit disc, then $f=g$ identically, so we must have that the zero set of $f-g$ (in the unit interval) is a sequence approaching $1$ . Furthermore, let $t_n$ be the sequence of zeros of $f-g$ , we may cite the Blaschke condition to rule out sequences such as the harmonic series (i.e. $t_n = 1-\frac 1n$ since we need the sum of $1-t_n$ to be finite). Also, suppose $h(x): = \sum_{n \in \mathbb N} c_nx^n$ is analytic such that the sum of its coefficients is absolutely convergent, and such that $h(x_i) = 0$ for some sequence $x_i$ tending to 1, then we can normalize $c_n$ such that $\sum_{n \in \mathbb N}c_n^+ = \sum_{n \in \mathbb N}c_n^-=1$ (the superscript $+$ denotes $\min(\cdot,0)$ , similarly for $-$ ), then letting $f=\sum_{n \in \mathbb N}c_n^+x^n, g=\sum_{n \in \mathbb N}c_n^-x^n$ gives us the desired coefficients. To this end, I think $h(x)=\sin\left(\frac \pi {\ln 2}\ln(1-x)\right)$ works but I have no proof of this (the coefficients are hard to compute explicitly). My question is as follows: I know that the product of Blaschke factors (which satisfy the Blaschke condition) defines a bounded holomorphic function on the unit disc, is it true that such a function also has absolutely convergent coefficients? can the coefficients of $h(x)=\sin\left(\frac \pi {\ln 2}\ln(1-x)\right)$ be explicitly found/proven to be absolutely convergent in sum or is it just too much effort? If so is there a example with simpler example that's easier to work with? If such an example cannot be written down exactly. Is it possible to at least prove the existence of it? Or is there no such example possible?","['complex-analysis', 'probability-theory', 'analytic-functions']"
4805911,Discriminant divisible by prime iff minimal polynomials reduction mod p has multiple roots,"I am attending a first course in algebraic number theory. We have learned the basics of field extensions, integral closures, norm and trace. I am trying to solve the following problem: Let $K=\mathbb{Q}(a)$ be a primitive field extension of $\mathbb{Q}$ of degree $n$ with minimal polynomial $f \in \mathbb{Z}[x]$ of $a \in \mathcal{O}_\mathcal{K}$ . Show that a prime number $p$ divides $d(1, a, a^2, \dots , a^{n-1})$ iff $\overline{f} \in \mathbb{Z}_p [x]$ has a zero of multiplicity 2. What I have tried: I have shown that in this case $d(1, a, a^2, \dots , a^{n-1}) = +- N(f^\prime (a))$ . Furthermore I know that because $a$ is a root of $f$ , $f^\prime(a)=\prod_{\beta \neq a} (a-\beta)$ . Where the $\beta$ are the roots of $f$ in an algebraic closure. Knowing that $N_{K/\mathbb{Q}}(\gamma) = \prod_{i=1}^n \sigma^i(\gamma)$ yields a product for the discriminant. However I do not see how this can help because the factors are in $\mathcal{O}_\mathcal{K}$ and not $\mathbb{Z}$ I have already seen that there is a similar result about ramified primes. However we just introduced Dedekind domains. Therefore I assume there must be a more elementary approach.","['algebraic-number-theory', 'discriminant', 'number-theory', 'minimal-polynomials', 'extension-field']"
4805914,Is Hessian with zero directions indefinite?,"I have a Hessian matrix $\mathbf{H}$ of a function $f(\mathbf{x})$ , evaluated at an extreme point $\mathbf{x}_0$ . Lets assume $\mathbf{H}$ is non-singular. I can show that there exists a direction $\mathbf{z} \neq \mathbf{0}$ for which $\mathbf{z}^T \mathbf{H} \mathbf{z}$ is zero. Does this imply that the Hessian is indefinite and $f$ has a saddle point at $\mathbf{x}_0$ ? My geometric intuition is that a saddle has cross-over lines between positive and negative regions. A minimum or maximum would not have such cross-over lines. Moreover, I would argue that since the Hessian is assumed to be non-singular, $\mathbf{z}$ is not in its kernel, so $\mathbf{H} \mathbf{z} \neq \mathbf{0}$ , so the expression $\mathbf{z}^T \mathbf{H} \mathbf{z}$ is not generally zero, but only in some directions.","['multivariable-calculus', 'linear-algebra', 'hessian-matrix']"
4805919,How to find the inverse of a series?,"Imagine a series like $f(x) = \sum_{n=0}^{\infty } f_n(x)$ . Is there any formula to find the $f^{-1}(x)$ as series expansion like $f^{-1}(x) = \sum_{n=0}^{\infty } g_n(x)$ ? Actually the the question is ""How to find all of $g_n$ functions only using $f_n$ functions?"" For instance; $\sin(x) = x - \frac{x^3}{3!} + \frac{x^5}{5!} - \frac{x^7}{7!} + ...$ ,  and $\arcsin(x) = x + \frac{x^3}{6} + \frac{3x^5}{40} + \frac{5x^7}{112} + \frac{35x^9}{1152} ... $ Of course we have a lot of information about $\sin(x)$ . But what if the given function is a very custom function? For example how to find the inverse of this kind of function: $$f(x) = \sum_{n=0}^{\infty } \frac{x^{11n+5}}{(3n+2)!}$$ In general, if $f(x) = \sum_{n=0}^{\infty } f_n(x)$ , I am searching for an operator, $\text{Inv}[f_0, f_1, f_2, ... ] = [g_0, g_1, g_2, ...]$ such that $f^{-1}(x) = \sum_{n=0}^{\infty } g_n(x)$ .","['analysis', 'real-analysis', 'inverse', 'taylor-expansion', 'sequences-and-series']"
4805922,Why does $(1 + x)(1 + x^2)(1 + x^4)(1+x^8) \cdots = 1 + x + x^2 + x^3 + \cdots$?,"Is there an intuitive explanation as to why $$(1 + x)(1 + x^2)(1 + x^4)(1+x^8) \cdots = 1 + x + x^2 + x^3 + \cdots$$ for $
|{x}| < 1?$ Of course, we can show that each side of the equation is equal to $\frac{1}{1-x}$ fairly easily (RHS is the basic geometric series with common ratio of $x$ and LHS multiplied by $(1-x)$ will collapse the product to $1$ ) ^ This proof is perfect for a quick grasp on the equality, although it still remains unclear as to why an infinite product with factors of the form $1+x^{(2^n)}$ should expand into a simple G.S. I would love to see a more direct proof that would at least make it more obvious as to why we can equate the product into the sum or vice-versa. Any help would be greatly appreciated.","['infinite-product', 'generating-functions', 'binary', 'sequences-and-series']"
4805935,Is the sum of minimum distances of a bounded sequence in $\mathbb{R}^d$ convergent?,"Let $d\in\mathbb{N};\ $ let $ x_n \in \mathbb{R^d}\ \forall\
 n\in\mathbb{N},\ $ with $(x_n)_{n=1}^{\infty}$ bounded. Let $(t_n)_{n=1}^{\infty}$ be the sequence obtained from $\left(\displaystyle\min_{1\leq i<j \leq k} \vert x_i - x_j \vert \right)_{k=2}^{\infty} $ by deleting repetitions (and so $(t_n)_{n=1}^{\infty}$ is strictly decreasing). Does $\displaystyle\sum_{n=1}^{\infty} t_n$ converge? I think it's true for $d=1,$ but even in this case, I'm having a hard time formulating a proof. Any ideas?","['pigeonhole-principle', 'examples-counterexamples', 'real-analysis', 'sequences-and-series', 'convergence-divergence']"
4805939,$\mathbb{E}\left[\sum_{i=1}^{T_{A}}f(X_{i})\mid X_{1}=l\right]\overset{?}{=}\mathbb{E}\left[\sum_{i=0}^{T_{A}}f(X_{i})\mid X_{0}=l\right]$,"Let us consider a Markov chain $(X_{n})_{n\in\mathbb{N}}$ (Note that $\mathbb{N}=\{0,1,2,...\}$ )  with state space $\mathcal{S},$ and let $A\subset\mathcal{S}$ denote a subset of $\mathcal{S}.$ The first time $T_{A}$ the chain hits the subset $A$ , with $$T_{A}=\inf\{n\ge 0: X_{n}\in A\},$$ with $T_{A}=0$ if $X_{0}\in A$ and $$T_{A}=\infty\text{ if }\{n\le0:X_{n}\in A\}=\emptyset.$$ i.e. if $X_{n}\in A$ for all $n\in \mathbb{N}.$ To obtain a recurrence relation of an expectation as the form $$g_{A}(k):=\mathbb{E}\left[\sum_{i=0}^{T_{A}}f(X_{i})\mid X_{0}=k\right],$$ where $k\in\mathcal{S}\setminus A$ and $f(\cdot)$ is a bounded Borel function. $$\begin{align}
    g_{A}(k)&=\mathbb{E}\left[\sum_{i=0}^{T_{A}}f(X_{i})\mid X_{0}=k\right]\tag{1}\\
           &=\sum_{l\in \mathcal{S}}\frac{1}{\mathbf{P}( X_{0}=k)}\cdot\mathbb{E}\left[\sum_{i=0}^{T_{A}}f(X_{i})\cdot\mathbb{I}_{\{X_1=l\}}\cdot\mathbb{I}_{\{X_0=k\}}\right]\tag{2}\\
           &=\sum_{l\in \mathcal{S}}\frac{\mathbf{P}(X_0=k,X_1=l)}{\mathbf{P}(X_0=k)}\cdot\mathbb{E}\left[f(k)+\sum_{i=1}^{T_{A}}f(X_{i})\mid X_{1}=l,X_{0}=k\right]\tag{3}\\
           &=\sum_{l\in \mathcal{S}}p_{kl}\cdot f(k)+\sum_{l\in \mathcal{S}}p_{kl}\cdot\mathbb{E}\left[\sum_{i=1}^{T_{A}}f(X_{i})\mid X_{1}=l,X_{0}=k\right]\tag{4}\\
           &=f(k)\cdot \sum_{l\in \mathcal{S}}p_{kl}+\sum_{l\in \mathcal{S}}p_{kl}\cdot{\color{Red} {\mathbb{E}\left[\sum_{i=1}^{T_{A}}f(X_{i})\mid X_{1}=l\right]}}\tag{5}\\
           &=f(k)+\sum_{l\in \mathcal{S}}p_{kl}\cdot {\color{Red} {\mathbb{E}\left[\sum_{i=0}^{T_{A}}f(X_{i})\mid X_{0}=l\right]}}\tag{6}\\
           &=f(k)+\sum_{l\in \mathcal{S}}p_{kl}\cdot g_{A}(l)\tag{7}
\end{align}$$ I don't know why $$\mathbb{E}\left[\sum_{i=1}^{T_{A}}f(X_{i})\mid X_{1}=l\right]=\mathbb{E}\left[\sum_{i=0}^{T_{A}}f(X_{i})\mid X_{0}=l\right].$$","['stochastic-processes', 'probability-theory', 'markov-chains']"
4805946,"If $\lim_{n\to\infty}A_n=A\neq\varnothing$ as set-theoretic limit, is it true that $\lim_{n\to\infty}\mathrm{diam}(A_n)=\mathrm{diam}(A)$?","Let $(X, d)$ be a metric space with topology induced by the metric $d$ , and $(A_n)_{n=1}^\infty$ be a sequence of sets in $X$ such that $\lim_{n\to\infty}A_n = A \neq \varnothing$ as a set-theoretic limit . Is it then true that $\lim_{n\to\infty}\mathrm{diam}(A_n)=\mathrm{diam}(A)$ ? Knowing the definition of point-wise continuity in the standard topology of metric spaces, I am wondering whether this is a question about the ""set-wise"" continuity of certain functions in metric spaces. So, is there some known result/counterexample that allows me to conclude that $\lim_{n\to\infty}\mathrm{diam}(A_n) =\mathrm{diam}(A)$ in general, or is this something you have to (painfully) check everytime in practise?","['metric-spaces', 'real-analysis', 'continuity', 'limits', 'general-topology']"
4805947,Representations of a Function of a random variable,"Assume we have a real random variable $Y\in \mathbb R^p$ , such that $Y= g(X)$ where $X \in\mathbb  R^n $ , $n< p$ , is some random variable with continuous distribution $P$ and $g:\mathbb R^n \rightarrow\mathbb  R^p$ is some smooth function (with continuous derivatives and second derivatives). Obviously, the representation of $Y$ is not unique, we can find $\tilde g$ and $\tilde X$ satisfying $Y =\tilde g(\tilde X)$ just by defining an invertible map $H$ with $\tilde g = g\circ H$ and $\tilde X = H^{-1}(X)$ . I would like to know if there are other smooth $\tilde g$ and $\tilde X$ satisfying $Y =\tilde g(\tilde X)$ ?","['diffeomorphism', 'functions', 'group-theory', 'transformation', 'probability']"
4805955,Slope of a hyperplane,"I was reading a book (Nonlinear Elliptic Equations of the Second Order by Qin Han), and there was something which I didn't really understand. I will try to simply the setting for my question. Let $a\in \mathbb R^n \setminus \{0\}.$ We consider the function $\ell(x)=M - a\cdot x$ , where $M$ is a positive constant and $x\in \mathbb R^n$ . Then the graph of $\ell$ shall be a hyperplane (in $\mathbb R^{n+1}$ ) passing through the point $(0,M)$ , where $0\in \mathbb R^n$ . The author wrote that, the graph of $\ell$ has the slope $a$ , which I didn't understand. I have two questions. Q1: I looked up on MSE and someone said that the gradient is just the slope (of a plane), cf. https://math.stackexchange.com/a/712763/507382. Is this the definition of the slope of a plane? Why does it make sense geometrically? Q2: Intuitively, $\nabla \ell= -a$ , and the graph of $\ell$ shall have the slope $-a$ , which is not $a$ as he said? Thanks for help.","['multivariable-calculus', 'plane-geometry']"
4805963,"How to write $\min\{a,k\}+\min\{b,k\}$ as one min?","I want to simplify a solution to a problem I have where I used to write $$\min\{a,k\} + \min\{b,k\} \leq \min\{c,k\} + \min\{d,k\}.$$ I am wondering: is there a more simplified way of writing this line since $k$ is a common argument in all minimums?","['real-analysis', 'notation', 'maxima-minima', 'calculus', 'combinatorics']"
4806037,If $f$ is compactly supported and belongs to $L^{p}$ does it also belong to $L^{q}$?,"I got to this result myself, just working with my understanding of $L^{p}$ spaces. I wanted to check with you if this is in fact true, or if I am making some mistake. Suppose that, for some $p \in [1,\infty]$ , $f \in L^{p}(\mathbb{R}^{d})$ is compactly supported (let us assume that its support lies within a compact set $K \subset \mathbb{R}^{d})$ . Is it true that $f \in L^{q}(\mathbb{R}^{d})$ for every $q< p$ ? It seems true by a simple Hölder's inequality argument: $$\|f\|_{q} = \bigg{(}\int dx |f(x)|^{q}\bigg{)}^{\frac{1}{q}} = \bigg{(}\int_{K}dx |f(x)|^{q}\bigg{)}^{\frac{1}{q}} = \bigg{(}\int_{K} dx 1^{\frac{p}{p-q}}\bigg{)}^{\frac{p}{p-q}}\bigg{(}\int_{K}dx|f(x)|^{q\frac{p}{q}}\bigg{)}^{\frac{q}{p}}$$ and the latter is finite because: $$\int_{K}dx 1^{\frac{p}{p-q}} = m(K) < \infty$$ where $m$ is the Lebesgue measure on $\mathbb{R}^{d}$ , which is finite because $K$ is compact and the other integral is finite because $f \in L^{p}(\mathbb{R}^{d})$ . Is this correct?","['lp-spaces', 'functional-analysis', 'analysis']"
4806059,Express $(989)\cdot(1001)\cdot(1007) + 320$ as product of primes.,"We can't use computers for factorising the number, as it's from a math contest where no computers are allowed. So I need to find a way with some manipulations. I can't find a way to approach it.","['elementary-number-theory', 'algebra-precalculus']"
4806064,When is the square root differentiable?,"Let $f$ be a non negative differentiable function (defined on $\mathbb R$ ) and $g(x)=\sqrt{f(x)}$ . Can you characterise the points $x_0$ where $g$ is differentiable at $x_0$ ? It is clear that when $f(x_0)\neq0$ then $g$ is differentiable at $x_0$ (by chain rule). So the real question is, what happens when $f(x_0)=0$ ? For such a point it is easy to see that if $g$ is differentiable at $x_0$ then necessarily $f'(x_0)=0$ . But the converse is not true as the example $f(x)=x^2$ shows (at $x_0=0$ ). So for the points $x_0$ such that $f(x_0)=0$ under what conditions on $f$ is $g$ differentiable at $x_0$ ? I suspect that the answer is: $g$ is differentiable at $x_0$ iff $f''(x_0)=0$ , but I cannot give a proof. Do you have any ideas?","['roots', 'derivatives', 'analysis', 'real-analysis']"
4806084,Is there any twin prime representing function?,"There are many prime representing functions . For example, $\lfloor A^{3^n} \rfloor$ is prime representing function because for all positive integers n ,it generates a different prime number. Here $A$ is approximately $1.306377...$ and $\lfloor . \rfloor$ denotes the floor function. Likewise, is there any function that generates twin primes for all positive integer n?","['number-theory', 'twin-primes', 'prime-numbers']"
4806108,Formulation of total variation measure of Banach space-valued measures,"Disclaimer : the question is rather imprecise, so suggestions to improve it/correct it are warmly welcome. Given a measure $\mu$ valued in a normed Banach space $(B,|\cdot|_B)$ , say $\mu\in\mathcal{M}(\Omega;B)$ (where for the scope of this question we can assume $\Omega\subseteq\mathbb{R}^n$ compact), we can define its total variation measure as \begin{align}\label{eq} |\mu|(E) := {\sup \left\{ ~ \sum_{i\in\mathbb{N}}|\mu(E_i)|_B ~~ | ~~ \{E_i\}_{i\in\mathbb{N}} \text{ partition of } E ~ \right\}}
\end{align} Now, if $B = (\mathbb{R}^n, ||\cdot||_p)$ , with $p\in[1,+\infty]$ , then we can exploit the duality between the $p-$ and the $q-$ norm (with the usual relation $\tfrac{1}{p}+\tfrac{1}{q}=1$ ), i.e. $$ ||x||_p = {\sup_{||y||_q \le 1} y \cdot x} $$ and find that $$ |\mu|_p(E) = {\sup \left\{ \int_\Omega \varphi(x)\cdot d\mu(x) ~~ | ~~ \varphi\in C^\infty_c(E;\mathbb{R}^n), \ ||\varphi(x)||_q\le1 \right\}} $$ (I actually don't know if this is precisely reported, especially the space of test functions, and I am not sure how to make sense of this ""integral of a scalar product between a function and a measure"", but I had seen it done in a similar fashion; references to this topic are again warmly welcome). My question is : how do we generalize this last fact to the case of $\mu\in\mathcal{M}(\Omega,B)$ ? In particular, given a norm, do I choose the dual or the predual norm to test it? Clearly, in the finite-dimensional case, it makes no difference, as for each $p\in[0,+\infty]$ , $(\mathbb{R}^n,||\cdot||_p)$ is reflexive, but what about the case of the codomain a general Banach space? My attempts : I would have two natural generalizations (one of which only works if $B$ admits a predual, which I'll denote as $((^*B), |||\cdot|||)$ , i.e. $(^*B)^* = B$ ; moreover I am denoting the dual of $B$ with $B^* = (B^*, ||\cdot||_*)$ ): $$ |\mu|(E) := { \sup\left\{ \int_\Omega < d\mu(x), \varphi(x) > ~~ | ~~ \varphi:E\to(^*B), ~ |||\varphi(x)||| \le 1 \right\} }$$ $$ |\mu|(E) := { \sup\left\{ \int_\Omega < \psi(x), d\mu(x) > ~~ | ~~ \psi:E\to B^*, ~ ||\psi(x)||_* \le 1 \right\} }$$ however, I don't know again how to make sense of them, and which one (if any) to choose in order to generalize the finite-dimensional case. Any hint would be very appreciated!","['measure-theory', 'functional-analysis']"
4806178,Sanov's Theorem for Empirical Measure,"I am working through the proof of Sanov's Theorem for the Empirical Measure (as found in Hollander's Large Deviations text), and am hoping someone could provide a bit of clarity on some of the steps taken... The theorem is as follows: From what I gather, it applies a technique similar to when Hollander proved the rate function for the Bernoulli case, where we find a suitable bound on the probability of the large deviation, and then applying some asymptotic (e.g. Stirling's Approximation) we manage to obtain the result we need. But I do have a couple of questions regarding the start of this proof (pictured below): I understand we define this set of $r$ -tuplets which are normalised to sum to $n$ . What exactly is meant by the line $\frac{1}{n}K_n \subset \mathscr{M}_1(\Gamma)$ ? I don't quite understand how we can say this set of $r$ -tuplets can be a probability measure on the finite set? How do we deduce that the Empirical Measure itself has a multinomial distribution, $L_n \sim \text{Multi}(n, r, \rho)$ ? It looks to me like the set $K_n$ would be of the right form to be a support of the multinomial distribution, but I feel like I am missing something obvious to make the conclusion. Any clarity that can be provided would be greatly appreciated. I understand the steps taken to obtain the result after this step, but I don't want to move on to further results (i.e. pair empirical measures) without properly understanding how this proof works...","['measure-theory', 'large-deviation-theory', 'probability-theory', 'probability']"
4806232,Books on mathematical frameworks?,"I am a first year student of applied mathematics. I currently find myself always trying to explain any phenomenon with mathematical models- the few ones (models) that I encountered till now. I always try to fit phenomenas to models like a data scientist does when he fits data to models.
Recently I found this book: Robert B. Banks
Growth and Diffusion Phenomena: Mathematical Frameworks and Applications This book is different from other applied mathematics books in the sense that it focuses on a particular model unlike others that try to model the whole world in a single book.
It particularly focuses on growth and diffusion phenomena. Which is , to me , great because this book studies these two models in great detail rather than just stating many models on the surface level.
I also found some book that is specific to some single phenomenon like this book on Hysteresis Mathematical Models of Hysteresis and their Applications Can someone please recommend to me some books like these that describes a certain phenomena or discusses a certain mathematical framework to study in detail and apply some phenomena?
Any book that discusses and models in detail some phenomena: could be growth,could be decline, could be (what happens in the limiting case)- type of thing,  could be something else, could be anything...","['mathematical-modeling', 'ordinary-differential-equations', 'stochastic-processes', 'calculus', 'partial-differential-equations']"
4806263,"Consider $f(n) = |\{ k \ |k \in \mathbb{N}, \ \frac{n^k}{k!} \in \mathbb{N} \}|$. Is this function related to the primorial?","I was playing around with the term $\frac{n^k}{k!}$ . Specifically, given a fixed n, I was wondering for which values of $k$ the term is an integer. Formally Let $f: \mathbb{N} \rightarrow \mathbb{N}, f(n) = |\{ k  \ |k \in \mathbb{N}, \  \frac{n^k}{k!} \in \mathbb{N} \}|$ I wrote a quick python script to compute this: def k_factorial_divisors(n):
    k, k_fac, n_k = 1, 1, n
    divisors = set()
    while k_fac <= n_k:
        if n_k % k_fac == 0:
            divisors.add(k)
        k += 1
        k_fac *= k
        n_k *= n

    return divisors Running this for the first 500 integers yields: print([len(k_factorial_divisors(i+1)) for i in range(500)])

# [1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 6, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 6, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 6, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 6, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 6, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 6, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 10, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 6, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 6, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 6, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 6, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 6, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 6, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 10, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 6, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 6, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1] I wondered for which n we have $f(n) \gt f(m)$ for all $m \lt n$ . Ie: when this function ""jumps"". A quick search yields: $$f(1) = 1\\
f(2) = 2\\
f(6) = 4\\
f(30) = 6\\
f(210) = 10\\
f(2310) = 12$$ Similarly, there is a repeating pattern up to this new maximum, as can be seen by the first 210 digits 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 6
1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 6
1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 6
1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 6, 
1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 6
1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 6, 
1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 4, 1, 2, 1, 2, 1, 10 These maximums seem to be exactly the primorials. Are they? Why? The numbers seem to cycle, until the new maximum value is reached. Why?","['elementary-number-theory', 'discrete-mathematics', 'prime-numbers']"
4806320,When can closedness of a constructible set be checked using DVRs?,"Let $S$ a Noetherian scheme and $X \subseteq |S|$ be a constructible subset of its topological space defined by some point-wise property $P$ . To check whether $X$ is closed, it suffices to show that the property $P$ is stable under specialization, which reduces to checking the same condition on specializations of height 1. That is, we may reduce to the case that $S = \operatorname{Spec} A$ is a 1-dimensional, local Noetherian ring. Often, in the literature, I see arguments now assume that $S$ is a DVR. This reduction corresponds to assuming that $S$ is regular. Why can one make this reduction? There are many examples inside of affine space of height one specializations that do not give rise to DVRs, e.g. the stalk at a singularity on a curve. An example can be found in Conrad's reductive group schemes , the proof of Proposition 3.1.12. Brian is generally a careful guy, so I assume the reduction is valid, but I haven't found a justification for this in e.g. EGA IV3.9.","['algebraic-geometry', 'schemes', 'commutative-algebra']"
4806334,Gradients for partially symmetric CP decomposition of 3rd order tensor,"I am interested in computing a rank- $R$ CP decomposition of a 3rd order tensor that is partially symmetric about the first 2 modes. The factorization of a vanilla CP decomposition is given below as a least squares minimization problem: $$ \min_{A,B,C} || \mathcal{X} - [[A,B,C]]||^{2}
$$ where $\mathcal{X} \in \mathbb{R}^{L \times M \times N} $ is the tensor to approximate, $A \in \mathbb{R}^{L \times R}$ , $B \in \mathbb{R}^{M \times R}$ , and $C \in \mathbb{R}^{N \times R}$ are factor matrices to estimate. [[]] denotes the sum of outer products of each of the corresponding factor matrix columns: $$ [[A,B,C]] = \sum_{r=1}^{R} a_{r} \otimes b_{r} \otimes c_{r}
$$ Where $a_r$ is the $r$ -th column of $A$ , etc. This objective can be rewritten as sub-problems corresponding to each mode: $$ \min_{A} ||\mathcal{X}_{(1)} - A(C \odot B)^{T}||^{2} \\
\min_{B} ||\mathcal{X}_{(2)} - B(C \odot A)^{T}||^{2} \\
\min_{C} ||\mathcal{X}_{(3)} - C(B \odot A)^{T}||^{2} 
$$ where the subscript $\mathcal{X}_{(i)}$ indicates the $i$ -th mode unfolding of a tensor (matricization where the mode- $i$ fibers become the columns of the resulting matrix) and $\odot$ is the Khatri-Rao product. The partial derivatives, with respect to each factor matrix, are as follows: $$ \frac{\partial f}{\partial A} = (\mathcal{T}_{(1)} - \mathcal{X}_{(1)})(C \odot B) \\
\frac{\partial f}{\partial B} = (\mathcal{T}_{(2)} - \mathcal{X}_{(2)})(C \odot A) \\
\frac{\partial f}{\partial C} = (\mathcal{T}_{(3)} - \mathcal{X}_{(3)})(B \odot A)
$$ where $\mathcal{T}=[[A,B,C]]$ . See here and here for details regarding all of the above derivation/implementation. In the case of partial symmetry about the first two modes, my problem becomes: $$ \min_{A,A,C} || \mathcal{X} - [[A,A,C]]||^{2}
$$ where $\mathcal{X} \in \mathbb{R}^{M \times M \times N} $ is a the tensor to approximate, $A \in \mathbb{R}^{M \times R}$ , and $C \in \mathbb{R}^{N \times R}$ . So, my question is, what would the partial derivatives be for the above case with partial symmetry with respect to $A$ and $C$ ? My actual optimization problem is more complex than this, so I want the gradients for non-linear optimization and this is the particular aspect of the objective that I have been unable to compute them. Any advise is greatly appreciated, I've been banging my head trying to figure this out - thank you!","['matrix-calculus', 'linear-algebra', 'partial-derivative', 'optimization', 'tensor-decomposition']"
4806359,How to find $X$ with these given values?,"Question Image : The given values are: $\angle ABD = \angle CAD = 30^{\circ}$ $BD = DC$ And we need to find $\angle ACB = X$ , how to solve this problem? I tried to draw a parallel like that but couldn't go further with it:","['contest-math', 'euclidean-geometry', 'angle', 'geometry', 'triangles']"
4806373,Taylor series of $\displaystyle x!_{(\infty)}=\prod_{j=1}^{\infty} j^{\text{sinc}(x-j)}$ - generalization of $\psi(z)$,"1. Premise I would like to find the closed form of this function: I have the infinite-multifactorial function $f(x)=x!_{(\infty)}$ and I want to calculate its Taylor series. $$f(x)=\prod_{j=1}^{\infty} j^{\text{sinc}(x-j)}\qquad\text{ where}\quad\text{sinc}(z)=\begin{cases}\frac{\sin(\pi z)}{\pi z}&z\neq 0\\
1&z=0\end{cases}$$ After various calculations that I am not here to write (they are very long and repetitive), I have arrived at this formulation: $$f'(x)=f(x)\cdot\underbrace{\sum_{j=1}^{\infty}\ln(j)\text{sinc}(x-j)}_{=: g(x)}$$ and applying the Leibnitz rule $$f^{(n)}(x)=\frac{\mathrm{d}^{n-1}}{\mathrm{d}x^{n-1}}f(x)g(x)=\sum_{k=0}^{n-1}\binom{n-1}{k}f^{(k)}(x)g^{(n-k-1)}(x)$$ I get that: $$f^{(n)}(0)=\sum_{k=0}^{n-1}\binom{n-1}{k}f^{(k)}(0)g^{(n-k-1)}(0)$$ $f^{(n)}$ is obtained iteratively, while $$g^{(n)}(0)=\sum_{j=1}^{\infty}\ln(j)\text{sinc}^{(n)}(-j)=\sum_{k=0}^{\left\lfloor\frac{n-1}{2}\right\rfloor}\frac{(-1)^k\pi^{2k}}{2k+1}\binom{n}{2k} b_{n-2k}$$ $$g^{(n)}(0)=b_n+\sum_{k=1}^{\left\lfloor\frac{n-1}{2}\right\rfloor}\frac{(-1)^k\pi^{2k}}{2k+1}\binom{n}{2k} b_{n-2k}$$ Where: $$b_n=-n!\cdot \eta'(n)=\begin{cases}
\dfrac{\ln\left(2\right)^{2}}{2}-\ln\left(2\right)\gamma&n=1\\
-n!\cdot \dfrac{(2^{n-1}-1)\zeta'\left(n\right)+\zeta\left(n\right)\ln\left(2\right)}{2^{n-1}}&n\geq 2
\end{cases}$$ Where $\eta(z)$ is the Dirichlet Eta function By writing the series up to grade 5 I obtained this sum: $$\begin{align}
f(x)\approx& 1+\frac{b_{1}}{1!}x+\frac{b_{1}^{2}+b_{2}}{2!}x^{2}+\frac{b_{1}^{3}+3b_{1}b_{2}+b_{3}-\pi^{2}b_{1}}{3!}x^{3}\\ +&\frac{\left(b_{1}^{4}+6b_{1}^{2}b_{2}+4b_{1}b_{3}+3b_{2}^{2}+b_{4}\right)-\pi^{2}\left(4b_{1}^{2}+2b_{2}\right)}{4!}x^{4}\\ +&\frac{{\left(b_{1}^{5}+10b_{1}^{3}b_{2}+15b_{1}b_{2}^{2}+10b_{1}^{2}b_{3}+5b_{1}b_{4}+10b_{2}b_{3}+b_{5}\right)+\left(\pi^{4}b_{1}-20b_{1}b_{2}\pi^{2}-10b_{1}^{3}\pi^{2}-20\pi^{2}\frac{b_{3}}{6}\right)}}{5!}x^{5}
\end{align}$$ It may seem complicated but the part of the coefficients not containing powers of $\pi$ can be written as complete Bell polynomials : $$\begin{align}
f(x)\approx&B_0(\{b_i\})+\frac{B_1(\{b_i\})}{1!}x+\frac{B_2(\{b_i\})}{2!}x^{2}+\frac{B_3(\{b_i\})-\pi^{2}b_{1}}{3!}x^{3}\\ +&\frac{B_4(\{b_i\})-\pi^{2}\left(4b_{1}^{2}+2b_{2}\right)}{4!}x^{4}\\ +&\frac{B_5(\{b_i\})+\left(\pi^{4}b_{1}-20b_{1}b_{2}\pi^{2}-10b_{1}^{3}\pi^{2}-20\pi^{2}\frac{b_{3}}{6}\right)}{5!}x^{5}
\end{align}$$ Where: $B_n(\{b_i\})=B_n(b_1,...,b_n)$ is the $n$ -th complete Bell Polynomial So I separated the first part from the rest: $$f(x)=\sum_{k=0}^{\infty}\frac{B_k(\{b_i\})}{k!}x^k+\frac{-\pi^{2}b_{1}}{3!}x^{3}+\frac{-\pi^{2}\left(4b_{1}^{2}+2b_{2}\right)}{4!}x^{4}+\frac{\pi^{4}b_{1}-20b_{1}b_{2}\pi^{2}-10b_{1}^{3}\pi^{2}-20\pi^{2}\frac{b_{3}}{6}}{5!}x^{5}+...$$ My problem now is that I can't understand what the closed formulation of the remaining piece of coefficients could be, can anyone help me? 2. Motivation of interest Using the formula $$\sum _{n=0}^{\infty }{B_{n}(b_{1},\dots ,b_{n}) \over n!}x^{n}=\exp \left(\sum _{i=1}^{\infty }{b_{i} \over i!}x^{i}\right)$$ We have that the part of $f$ in which the first series appears can be written as: $$\sum _{n=0}^{\infty }{B_{n}(\{b_i\}) \over n!}x^{n}=\exp\left[\left(\dfrac{\ln\left(2\right)^{2}}{2}-\ln\left(2\right)\gamma\right)x-\sum_{n=2}^{\infty} \dfrac{(2^{n-1}-1)\zeta'\left(n\right)+\zeta\left(n\right)\ln\left(2\right)}{2^{n-1}}x^n\right]$$ $$=\exp\left[\left(\frac{\ln\left(2\right)^{2}}{2}-\ln\left(2\right)\gamma\right)x+\sum_{n=2}^{\infty}\frac{\zeta'\left(n\right)}{2^{n-1}}x^{n}-\sum_{n=2}^{\infty}\zeta'\left(n\right)x^{n}-\ln\left(2\right)\sum_{n=2}^{\infty}\frac{\zeta\left(n\right)}{2^{n-1}}x^{n}\right]$$ $$=\exp\left[\frac{\ln\left(2\right)^{2}}{2}x+2\sum_{n=2}^{\infty}\zeta'\left(n\right)\left(\frac{x}{2}\right)^{n}-\sum_{n=2}^{\infty}\zeta'\left(n\right)x^{n}+\ln\left(2\right)x\psi\left(1-\frac{x}{2}\right)\right]$$ Here a series involving the derivative of the zeta function appears twice. I would like to find some way to express the series in this way $$\sum_{n=2}^{\infty}\zeta'(n)x^{n-1}=K-\ln(2)f(1-x)$$ so the sum becomes $$\sum _{n=0}^{\infty }{B_{n}(\{b_i\}) \over n!}x^{n}=\exp\left[\ln\left(2\right)x\left(\frac{\ln\left(2\right)}{2}+f\left(1-\frac{x}{2}\right)-f\left(1-x\right)+\psi\left(1-\frac{x}{2}\right)\right)\right]$$ I think there may be connections with the gamma function being $$\sum_{n=2}^{\infty}\zeta(n)x^{n-1}=-\gamma-\psi(1-x)$$ I think it could be interesting that the infinte-multifactorial could be connected with some function deriving from the gamma function. 3. Update I realized I can write the infinite-multifactor function as $$x!_{(\infty)}=\exp\left(\sum_{n=1}^{\infty}\left(-1\right)^{n}\frac{\ln\left(n\right)x}{x-n}\right)^{\text{sinc}(x)}$$ In this way I only study the function inside the exponential so as not to have that uncomfortable presence of $\pi$ $$\begin{align}\sum_{n=1}^{\infty}\left(-1\right)^{n}\frac{\ln\left(n\right)x}{x-n}=&\sum_{s=1}^{\infty}x^{s}\left(-\sum_{n=1}^{\infty}\left(-1\right)^{n}\frac{\ln\left(n\right)}{n^{s}}\right)\\
=&\sum_{s=1}^{\infty}x^{s}\left(\frac{\mathrm{d}}{\mathrm{d}s}\sum_{n=1}^{\infty}\frac{\left(-1\right)^{n}}{n^{s}}\right)\\
=&-\sum_{s=1}^{\infty}\eta'(s)x^s\\
=&-\sum_{s=1}^{\infty}\frac{(2^{s-1}-1)\zeta'(s)+\ln(2)\zeta(s)}{2^{s-1}}x^s
\end{align}$$ So there is a strong correlation between the infinite-multifactorial and the series $$\begin{align}x!_{(\infty)}=&\exp\left(-\sum_{s=1}^{\infty}\eta'(s)x^s\right)^{\text{sinc}(x)}\\
=&\exp\left(-\sum_{s=1}^{\infty}\dfrac{\ln(2)\zeta(s)+(2^{s-1}-1)\zeta'(s)}{2^{s-1}}x^s\right)^{\text{sinc}(x)}\\
=&\exp\left(\ln(2)\psi\left(1-\frac{x}{2}\right)x+\sum_{s=1}^{\infty}(2^{1-s}-1)\zeta'(s)x^{s}\right)^{\text{sinc}(x)}\end{align}$$ We considered the regularized zeta where $\zeta(1)=\gamma$ and $\eta'(1)=\gamma\ln(2)-\dfrac{\ln(2)^2}{2}$","['recurrence-relations', 'calculus', 'taylor-expansion', 'polynomials', 'sequences-and-series']"
4806402,"Explanation for standard deviation rule-of-thumb, $s\approx \text{range}/4$","In several introductory statistics books, for a list of data $X = \{x_1,\ldots,x_n\}$ , I have frequently seen the following rule-of-thumb: $$
s \approx \frac{\text{range}(X)}{4} = \frac{\max(X)-\min(X)}{4}
$$ I have thought about this a little and can offer my heuristics, but I am curious if this (obviously very crass) rule-of-thumb can be put on slightly more stable mathematical footing. Suppose the data are normally distributed. Then approximately $95\%$ of the data should be within two standard deviations of the mean, in which case $4s \approx \text{range}(X)$ is plausible. For a continuous uniform distribution on $(a,b)$ , the exact value is $s = (b-a)/\sqrt{12}$ , compared to the approximation $(b-a)/4$ . Since $1/\sqrt{12}\approx 0.2887$ , the relative error is about $15.47\%$ ; not great, not terrible. This value is in the same ballpark for the discrete normal distribution. Looking at another distribution, the Poisson distribution with mean $\mu>0$ and standard deviation $\sqrt{\mu}$ , this amounts to summing over values $k$ with $|\mu-k|\le 2 \sqrt{\mu}$ ; this becomes weird because of rounding since $k$ can only take integer values, but a plot produced about $0.92$ as a minimum value for this sum. The book did use this rule-of-thumb as a way to approximate the range of a population given a sample (of which the range and standard deviation can be calculated), but I think the reason for the heuristic in the first place is to provide an alternative to students who do not have the tech or computational skills to find the exact value of $s$ in the first place. Any further justification of this heuristic would be appreciated.","['statistics', 'approximation', 'standard-deviation']"
4806405,Series involving derivative of Riemann Zeta function: $\displaystyle \sum_{k=2}^{\infty}\zeta'(k)x^{k-1}$,"1. Question Could anyone recommend a useful method for approaching the following series? $$\sum_{k=2}^{\infty}\zeta'(k)x^{k-1}$$ Where $\zeta(z)$ is the Riemann Zeta function. I've seen that there are many series involving this function, but I haven't found any that involve its derivative, the analogue to this series without the derivative is $$\sum_{k=2}^{\infty}\zeta(k)x^{k-1}=-\gamma-\psi(1-x)$$ which has a closed form solution, I was curious to know if there was a closed form for the version with the derivative too Any suggestions are welcome Update I arrived at this formulation: $$\sum_{n=2}^{\infty}\zeta'\left(n\right)x^{n-1}=-\sum_{n=2}^{\infty}\sum_{k=1}^{\infty}\frac{\ln\left(n\right)}{k^{n}}x^{n-1}=-\sum_{k=1}^{\infty}\sum_{n=2}^{\infty}\frac{\ln\left(n\right)}{k^{n}}x^{n-1}=\frac{1}{x}\sum_{k=1}^{\infty}\text{Li}_{0}^{\left(1,0\right)}\left(\frac{x}{k}\right)$$ Where: $$\text{Li}_{0}^{\left(1,0\right)}\left(z\right):=\left.\frac{\partial}{\partial \nu}\text{Li}_{\nu}\left(z\right)\right|_{\nu=0}=-\sum_{k=1}^{\infty}\ln\left(k\right)z^{k}$$ So $$\sum_{n=2}^{\infty}\zeta'\left(n\right)x^{n-1}=\frac{1}{x}\frac{\partial}{\partial \nu}\left.\left[\sum_{k=1}^{\infty}\text{Li}_{\nu}\left(\frac{x}{k}\right)\right]\right|_{\nu=0}$$","['polygamma', 'calculus', 'sequences-and-series', 'riemann-zeta', 'derivatives']"
4806410,Find $P(B|L)$ and $P(L|B)$,"A deck of cards has $3$ orange cards and $7$ blue cards. Two cards are selected a random without replacement. $B=$ both cards are blue, $L=$ at least one card is blue. $n(S)= C(10,2)$ a) find $P(B|L)$ b) find $P(L|B)$ Attempt: $P(B∣L)=\frac{P(L)}
{P(B∩L)}$ $P(B∩L)=P(B)$ ​
Not sure how to proceed.","['discrete-mathematics', 'probability']"
4806421,Finding the Volume of Region in Three-Dimensional Space.,"I am currently working on calculating the volume of the region $W$ in $\mathbb{R}^3$ , defined by the constraints: $$W=\{ (x,y,z)\in\mathbb{R}^3 ∣ z≥2,  x^2+y^2≤6−z,  y≥x \} .$$ My approach involves using triple integration, and the volume integral I have formulated is as follows: $$V = \int_{2}^{4} \int_{x}^{\sqrt{6-x^2}} \int_{2}^{6-x^2-y^2} \, dz \, dy \, dx .$$ However, I find myself at a stage where I need assistance confirming if my approach is correct. Could you provide guidance on how to tackle this analytical development? Are there any suggestions for simplifying the integration or considerations I should be aware of? I appreciate any help or insights you can offer. Thank you!","['integration', 'volume', 'real-analysis', 'multivariable-calculus', 'calculus']"
4806434,Soft Question - Generalizations of the Derivative,"This is a soft question. I'm asking for any interesting and rather unknown generlizations of the derivative. I know it is generalized through derivations which are functions $\delta$ satisfying $$\delta(uv) = v\delta(u) + u\delta(v)$$ Some of them include for example, q-derivatives used in quantum calculus $$\left(\frac{d}{dx}\right)_qf(x) = \frac{f(qx)- f(x)}{qx-x}$$ The arithmetic derivative used in number theory $$D(n) = n\sum_{p\mid n}\frac{\nu_p(n)}{p}$$ The Hasse derivative $$D^{(r)}X^n = {n\choose r}X^{n-r}$$ and many more...
I'm asking if anyone knows any interesting and perhaps relatively unknown generalizations of the derivative/examples of interesting derivations. Feel free to share anything you feel is interesting","['functions', 'derivatives', 'derivations', 'differential-algebra']"
4806439,Law of total expectation and conditioning to find expectation.,"I have a lottery in which a random number $𝑁$ tickets are sold, where $𝑁$ has PMF given by $$p_N(n) = \frac{\lambda^{n-1}e^{-\lambda}}{(n-1)!}, \quad n = 1, 2, \ldots$$ The tickets are numbered $1, 2, … , 𝑁$ , and a single winning ticket is drawn uniformly at
random. Let $𝑋$ denote the number on the winning ticket. I want to find the Expectation and Variance of X. My thinking so far for the expectation is to use the law of total expectation, i.e. say $E[X] = E[E[X \mid N]]$ , and then use the fact $E[X] = \sum_{n=1}^{\infty} E[X \mid N=n]P(N=n)$ . So surely $E[X \mid N=n] = \frac{n+1}{2}$ as it is uniform, then I should be able to evaluate the sum? Feels like I am missing something obvious, like something within the sum. Any pointers appreciated, thanks in advance.","['probability-distributions', 'conditional-expectation', 'probability']"
4806443,Show continuity of the conditional expectation of a curved exponential family,"I'm working with the paper of Wu - On the Convergence Properties of the EM Algorithm (1983). In a theorem he says that the EM-algorithm converges to a stationary point if the conditional expectation $Q(\phi^{'} | \phi)$ is both continuous in $\phi^{'}$ and $\phi$ . Furthermore he writes that this condition is always fulfilled for curved exponential families.
So, given is the density of a curved exponential family $f(x| \phi) = b(x) \exp(\phi^{T} t(x))/a(\phi)$ , where the parameters $\phi$ lie in a compact submanifold $\Omega_0$ of the r-dimensional convex region $\Omega = \{\phi |\int b(x) \exp(\phi^{T} t(x)) \, dx < \infty \}.$ \ Then $Q(\phi^{'} | \phi) := E[\log(f(x|\phi^{'})) | y, \phi] = - \log(a(\phi^{'})) + E[\log(b(x)) | y, \phi] + \phi'^{\,T} E[t(x) | y, \phi].$ Wu claims that the continuity follows from the compactness of $\Omega_0$ and properties of the exponential family.
I think I can say that $- \log(a(\phi^{'}))$ is continuous as $- \log$ is continuous and $\Omega_0$ is compact, so that $- \log(a(\phi^{'}))$ takes its maximum on this compact set which means that this part must be continuous. For the two other terms I should need the exponential family properties. But I don't know any properties, which give me useful information about $b(x)$ and $t(x)$ . Can I somehow show that they have to be continuous? The definition of exponential families say that they have to be only measureable... Thanks! \ (explanation of the EM-algorithm: E-Step: Determine $Q(\phi | \phi_p)$ , M-Step: Choose $\phi_{p+1}$ to be any value of the parameter space which maximizes $Q(\phi | \phi_p)$ . (Repeat these steps until $Q(\phi_{p+1} | \phi_p)$ convergence against a $Q(\phi^* | \phi^*)$ ))","['conditional-expectation', 'statistics', 'probability-theory', 'algorithms']"
4806446,"Asymptotic density of certain class of finite groups (Solvable, Nilpotent, $p$-Group, etc).","I read that there is a conjecture that most groups are $2$ -groups. This conjecture comes from the fact that by Higman-Sims asymptotic formula, $\#$ of $p$ -group of order $p^k= p^{\frac{2}{27}k^3 + O(\text{lower order terms})}$ with the fact that 2 is the smallest prime, so power of 2 just appears a lot more frequently. ( $2^{100}\approx 3^{63} \approx 5^{43}$ ). But this conjecture seems to be far from being proven. First, we have to formally define what ""most"" means. $\dfrac{\#_{\text{2-Group}}(\leq n)}{\#_{\text{Group}}(\leq n)}\to 1$ as $n\to \infty$ would be the formal statement of what it means for most groups to be $2$ -groups. $\\ $ So I was wondering if there are weaker results? Most $p$ -Groups are $2$ -Groups. ie $\dfrac{\#_{\text{2-Group}}(\leq n)}{\#_{p\text{-Group}}(\leq n)}\to 1$ (It feels like this one is the easiest question, since it could be answered using just Higman-Sims formula along with some knowledge of the density of prime powers) $\\$ Most Groups are $p$ -Groups Most Groups are Nilpotent Most Groups are Solvable Are any of these results proven? I believe all of these questions are true heuristically.","['groups-enumeration', 'computational-algebra', 'asymptotics', 'group-theory', 'solvable-groups']"
4806457,Relate two averages to find a probability?,"I would like to ""relate"" two arithmetic averages and then use them in Poisson. I currently already use a method, but i'm sure we can do better and that there is a better way. I currently sum the two averages, then divide by two, then use the result in Poisson. In the championship matches played so far, Team_A and Team_B have never clashed against each other, but so far the average number of goals scored by Team_A is 1.40 ; Instead the average number of goals conceded by Team_B is 1.80 . I would like to calculate how many probabilità there are that Team_A scores 2 goals against Team_B in the next match when the two teams will clash against each other. The two averages are independent, because there is no correlation between the two datasets. An important observation is that: the goals scored by Team_A against Team_B will be the same as those that Team_B will concede from Team_A. Is there any better way that can replace addition and division by 2? (my Union) and find a value to use in Poisson? TEAM_A__List_Goals = 2, 1, 3, 0, 1
TEAM_A__Total_Goals_Scored = 7
TEAM_A__Number_Match_Played = 5
TEAM_A__Average_Scored = 1.40

TEAM_B__List_Goals = 1, 2, 2, 3, 1
TEAM_B__Total_Goals_Conceded = 9
TEAM_B__Number_Match_Played = 5
TEAM_B__Average_Conceded = 1.8

Union = (TEAM_A__Goals_Scored + TEAM_B__Goals_Conceded) / 2

#POISSON PROBABILITY CALCULATION
two_goals = ((Union ** 2) * 2.7182818284 ** (-Union)) / 2 * 100 Thank you all!","['probability-distributions', 'discrete-mathematics', 'probability-theory', 'probability']"
4806469,Solving $\operatorname{argmin}_\alpha \|I-\alpha \operatorname{diag} h + 2 \alpha^2 \operatorname{diag} h^2 +\alpha^2 h\otimes h\|$,"Suppose $h^*=\left(1,\frac{1}{2},\frac{1}{3},\ldots,\frac{1}{d}\right)$ and $h=h^*/\|h^*\|_1$ . Can someone see a way to approximate the following quantity for $d\approx 10^6$ ? The problem is to minimize the norm of a symmetric diagonal+rank1 matrix: $$\operatorname{argmin}_\alpha \|I-\alpha \operatorname{diag} h + 2 \alpha^2 (\operatorname{diag} h)^2 +\alpha^2 (h\otimes h)\|$$ For $d=2000$ I can compute it using brute-force to be $\approx 1.48$ and the hypothesis is that it goes to $2$ as $d\to\infty$ Motivation: this gives optimal step size for SGD used to solve $0=wx_i$ where $x_i$ are drawn from Gaussian with eigenvalues $h$ .","['semidefinite-programming', 'linear-algebra', 'probability']"
4806477,Indeterminate forms other than the 7 common ones,"The following 7 indeterminate forms are all I can find in any calculus books: $$\frac{0}{0}, \frac{\infty}{\infty}, 0 \cdot \infty, \infty - \infty, 0^0, \infty^0, 1^\infty.$$ For example, by $\frac{0}{0}$ , I am referring to the limit $\lim\limits_{x\to a} \frac{f(x)}{g(x)}$ with $\lim\limits_{x\to a} f(x) = 0$ and $\lim\limits_{x\to a} g(x) = 0$ . Also, $1^\infty$ is understood to include the case $1^{-\infty}$ , just as $\frac{\infty}{\infty}$ is understood to include $-\infty$ in the numerator/denominator. Finally, the function in the base of $0^0$ is understood to be approaching $0$ from the right. In contrast, here are some forms which are not indeterminate forms: $$0^{\pm\infty}, \infty^{\pm\infty}, \frac{0}{\pm\infty}, \frac{\pm\infty}{0^+}, \frac{\pm\infty}{0^-}.$$ Then I came across this post: Are $\log_1 1$ and $\log_0 0$ indeterminate forms? I think the forms $\log_1 1$ and $\log_{0^+} {0^+}$ are also indeterminate forms, and they can respectively be transformed into the types $\frac{0}{0}$ and $\frac{\infty}{\infty}$ using $\log_{f(x)} [g(x)] = \frac{\ln g(x)}{\ln f(x)}$ . Can you list some other indeterminate forms?","['indeterminate-forms', 'limits']"
4806478,"Using the Principle of Well Order, show that for all $n \in N$ it holds that $4^n -1$ is divisible by 3.","Using the Principle of Well Order, show that for all $n \in N$ it holds that $4^n -1$ is divisible by 3.
I have already defined the set of counterexamples $C$ , then I proved that for $n=1$ the proposition holds.
Then I established that $c$ (the minimum element of $C$ ) must be the successor of some natural number $k$ such that $c=k+1$ and therefore $k=c-1$ , so that: $4^k -1$ is divisible by 3, apply the definition of divisibility to this last statement: $4^k -1=3m$ , therefore, $4^{c-1} -1$ is divisible by 3. Then, using the Division Algorithm, I can represent $4^c -1=3m+r$ , from here I don't know how to continue.","['well-orders', 'theorem-provers', 'discrete-mathematics']"
4806505,"Prove $\color{black}{\sqrt{a+1}+\sqrt{b+1}+\sqrt{c+1}\le 1+2\cdot\sqrt{\frac{a+b+c+5}{3}}, }$ when $ab+bc+ca+abc=4.$","If $a,b,c\ge 0: ab+bc+ca+abc=4.$ Prove that $$\color{black}{\sqrt{a+1}+\sqrt{b+1}+\sqrt{c+1}\le 1+2\cdot\sqrt{\frac{a+b+c+5}{3}}.  }$$ I've tried to square both side but it leads to complicated one. Now, if we use the substitution $a=\dfrac{2x}{y+z};b=\dfrac{2y}{x+z};c=\dfrac{2z}{y+x}.$ The problem turns out $$\sum_{cyc}\sqrt{\frac{2x+y+z}{y+z}}\le 1+\frac{2}{\sqrt{3}}\cdot\sqrt{5+2\sum_{cyc}\frac{x}{y+z}}.$$ From here, I don't know how to continue to full proof. Hope you can share some thoughts to help me out. Also, all idea and comment are welcome. Thanks for interest.","['multivariable-calculus', 'algebra-precalculus', 'lagrange-multiplier', 'inequality']"
4806585,Nonstandard infinite / hyperfinite sum in IST,"TLDR:
If anyone could provide a detailed proof that a sum indexed by an unlimited hypernatural number is well-defined using the axioms of IST , I would greatly appreciate it. I am studying Nelson's ""Radically Elementary Probability Theory"" and I am having some trouble. Namely, after introducing the axioms of IST, Nelson proves Theorem 5.3 where the expression $$
\sum_{i=1}^n x_i 
$$ arises (in the statement of the theorem), and he remarks afterwards that ""there is no requirement that $n$ be limited"". However, this is quite confusing to me, as so far he hasn't defined what an unlimited sum would mean. I have seen posts like this one: How are infinite sums in nonstandard analysis defined? that talk about this same issue, but these posts do not specify which framework of NSA they are working in, and, from the references to Keisler's book in the comments, I think they are working in a different framework than Nelson's. With that said, the above post does mention the transfer principle, which I know is also an axiom of IST. What troubles me, however, is that the specifics of the justification are glossed over, and all of the interpretations I can come up with are disturbing. These are some interpretations I came up with (I will use $\mathbb{N}$ to denote the standard naturals and $\mathbb{N}^*$ to denote the hyperrnaturals; though $\mathbb{N}$ is not a set, this is ok because in the nonstandard setting we will only be using the notation $n \in \mathbb{N}$ as a shorthand for "" $n$ is standard""): We define a function $S: \mathbb{N} \to \mathbb{R}^*$ that is the partial sum function with respect to a priorly fixed hyperreal sequence $(a_n)_{n \in \mathbb{N}^*}$ . Then $S$ is defined for every $n \in \mathbb{N}$ , and (I'm not sure if this is correct but I'm guessing so) extends to some function defined on $\mathbb{N}^*$ . You begin in the classical universe and fix a sequence $(a_n)_{n \in \mathbb{N}}$ . Define its corresponding partial sum function $S$ . Then $(a_n)_{n \in \mathbb{N}}$ admits a (unique?) extension to the sequence $(a_n)_{n \in \mathbb{N}^*}$ after applying transfer. Then $S$ also admits a corresponding extension $S^*$ . You begin in the classical universe and define a function $S: \mathbb{N} \times \{\mathrm{all} \ \mathrm{real} \ \mathrm{sequences} \} \to \mathbb{R}$ and then take its transfer. The reasons I find these disturbing/problematic are as follows: You need to fix a sequence indexed by the hyper naturals in order to define a function from the classical naturals to the hyperreals. There is a mixing of universes all over the place here and that seems very wrong. One sequence indexed by the naturals (e.g., $(0)_{n \in \mathbb{N}}$ ) can easily be extended to different sequences indexed by the hypernaturals (e.g., $(0)_{n \in \mathbb{N}^*}$ and $(\delta_{n, \nu})_{n \in \mathbb{N}^*}$ where $\delta_{n, \nu}$ is the Kronecker $\delta$ with $\nu$ an unlimited hypernatural). This would mean the extension of $S$ cannot possibly be unique, and we have to choose an extension that fits the sequence whose partial sums we are trying to determine exist. You are defining a function with a set of sets as a domain, and as far as I vaguely understand, this is not allowed in ""first-order logic"" (I don't know what this means besides that it is some restriction on what quantifiers you can use), and the transfer principle is (or should?) only work for first-order statements. P.S.
I am studying this book because I am interested in studying Herzberg's book on Stochastic calculus (which also glosses over what this sum would mean). I am not so much interested in logic/model theory, but the existence of this sum is crucial to the ability to define probability measures point wise on infinitesimal meshes, and so I would really like to know why we can do this.","['first-order-logic', 'logic', 'nonstandard-analysis', 'real-analysis', 'sequences-and-series']"
4806626,Is $e^{\ln(-7) }= -7$?,"I was going over some homework, and stumbled across a true or false question that presented as the following: $e^{\ln(-7)}$ = -7, True or False Seeing as the definition of a logarithm presents the following: $\log_{b}a$ = x is equal to $b^x$ = a I presumed that the initial statement was true based on the fact that by taking the natural log of both sides you arrive at: $\log_{e}$ ( $e^{\log_{e}(-7)}$ ) = $\log_{e}(-7)$ which simplifies to $\log_{e}(-7)$$\log_{e}(e)$ = $\log_{e}(-7)$ $\log_{e}(-7)$ = $\log_{e}(-7)$ yet the answer was marked incorrect. Logically the equation makes sense to me both forward and backward in the simplification. Am I missing something? I understand that the logarithm of a negative number is undefined, however, can't you use the logarithmic rule as a way to supersede the impossibility of solving this by calculating the values by hand?","['algebra-precalculus', 'logarithms']"
4806627,Relationship between two types of partition functions,"After downvoting my previous thread, here is a more detailed explanation of my question. For $s\in \mathbb{C},\Re(s)>1 $ , consider: $$\prod_{k=1}^{\infty}\prod_{n=2}^{\infty}\frac{1}{1-n^{-ks}}= \prod_{k=1}^{\infty} \sum_{m=1}^{\infty}\frac{\rho(m)}{m^{ks}}=\prod_{n=2}^{\infty}\sum_{j=0}^{\infty}\frac{p(j)}{n^{js}} $$ Where $\rho(m)$ is the multiplicative partition function of the integer $m$ . And $p(j)$ is the additive partition function of the integer $j$ . Based on the equation : $$\prod_{k=1}^{\infty} \sum_{m=1}^{\infty}\frac{\rho(m)}{m^{ks}}=\prod_{n=2}^{\infty}\sum_{j=0}^{\infty}\frac{p(j)}{n^{js}} $$ What arithmetical relationship(s) can be drawn between $\rho(m)$ and $p(j)$ ? For instance, how can we write $\rho(m)$ in terms of $p(j)$ ?","['analytic-number-theory', 'number-theory', 'arithmetic-functions']"
4806644,Description of Riemann surface of polynomial inverse,"My question is about page 4 of the pdf of the following paper , one does $\textbf{not}$ need to read pages 1-3 of the paper to understand my question ( the $\textbf{only}$ part that needs to be read is provided in the image below). The precise part I struggle to understand  is highlighted in the below image: $\textbf{Question}$ : Accepting that $P^{\ast}$ maps the sector $G$ univalently onto the described domain, I fail to see how to apply the Schwarz reflection principle (called the ""Riemann-Schwarz Symmetry principle"" in the paper) to find that the Riemann surface of $(P^{\ast})^{-1}$ has the structure described. $\textbf{Observations}:$ It seems that the description of the Riemann surface of $(P^{\ast})^{-1}$ suggests the following: There must be $n$ possible branches of $(P^{\ast})^{-1}$ definable on $\mathbb{C} \setminus (\bigcup_{k=2}^{n} T_k^{\ast}) = \mathbb{C} \setminus \{ |w| \geq |\alpha_{n}| : \arg(w) = \pi + \frac{2 \pi (k-2)
}{n-1} , 2 \leq k \leq n \}$ , because this domain is simply connected and omits all the critical values of $P^{\ast}$ , here $T_k^{\ast} = \{ |w| \geq |\alpha_{n}| : \arg(w) = \pi + \frac{2 \pi (k-2)
}{n-1} \}$ . This holds independently of the proposed description of the Riemann surface. Now using the description of the Riemann surface, we should have the following: $(\textbf{1})$ : $n-1$ of these $n$ branches, which we may index as $P_{k},k=2,...,n$ are such that each $P_{k}$ is holomorphic on $\mathbb{C} \setminus T_{k}^{\ast}$ and is discontinuous on the corresponding cut $T_k^{\ast}$ ,
and another branch $P_1$ is defined on $\mathbb{C} \setminus (\bigcup_{k=2}^{n} T_k^{\ast})$ and is discontinuous on every cut $T_{k}^{\ast}, k=2,...,n$ $(\textbf{2})$ : The branching behaviour is as follows: Crossing the cut $T_{k}^{\ast}$ starting with the branch $P_1$ moves us to the branch $P_k$ , and vice versa (i.e. starting from $P_k$ and crossing the cut $T_k^{\ast}$ moves us back to $P_1$ ), this comes directly from the description of the surface. In other words, proving (1) and (2) using the method proposed by the author (applying the Riemann-Schwarz symmetry principle) or otherwise, one will also be able to obtain the precise description of the Riemann surface as given in the paper. $\textbf{Edit}$ : I will award the bounty to any answer that explains (in detail) why the Riemann surface has the description proposed by the author regardless of whether the explanation involves the Schwarz reflection principle, unless there is also an answer that explains the description of the Riemann surface using the method proposed by the author (using the Schwarz reflection principle) before the bounty closing time.","['complex-analysis', 'riemann-surfaces', 'polynomials', 'analytic-continuation']"
4806690,Variation of parameters method is giving me the wrong answer.,"I had a Differential Equations midterm yesterday with the problem: $$x^3y''+x^2y'-xy=\frac{x}{x+1}.$$ We were told beforehand that the homogeneous equation had a solution set $y_1(x)=x, y_2(x)=x^{-1}$ , so we were only supposed to find the particular solution using the Variation of parameters method. I started by solving for the Wronskian which should be $$W=\begin{vmatrix}x&\frac{1}{x}\\1&-\frac1{x^2}\end{vmatrix}=-\frac2x.$$ Now, as far as I know, we need to suppose that the particular solution is of the form $y=u_1(x)y_1(x)+u_2(x)y_2(x)$ , and we can calculate $u_1$ and $u_2$ in the following way: $$u_1=\int\frac{-y_2(x)g(x)}{W}dx\text{ and }u_2=\int\frac{y_1(x)g(x)}{W}dx,$$ giving me in this case that $$\begin{align}u_1&=\int\frac{-\frac{1}{x}\frac{x}{1+x}}{\frac{-2}{x}}dx\\&=\frac{1}{2}\int\frac{x}{1+x}dx=\frac{1}{2}(x+1-\ln|x+1|)\\u_2&=\int\frac{x\frac{x}{1+x}}{\frac{-2}{x}}dx\\&=-\frac12\int\frac{x^3}{1+x}dx\\&=-\frac12\left(\frac{(x+1)^3}{3}-\frac{3(x+1)^2}{2}+3(x+1)-\ln|x+1|\right).\end{align}$$ Thus, the particular solution should be $$y=\frac{x}{2}(x+1-\ln|x+1|)-\frac{1}{2x}\left(\frac{(x+1)^3}{3}-\frac{3(x+1)^2}{2}+3(x+1)\ln|x+1|\right).$$ I tried to check this solution using Mathematica, but when I plug it in the ODE, the expression simplifies to $\frac{x^4}{x+1}$ , not $\frac{x}{x+1}$ as the problem initially wanted. Also, in online calculators, the solution sometimes has a $\ln(x)$ that does not appear anywhere on my solution. I'm honestly really confused, I don't see at all where the mistake could be. Any help will be really appreciated.",['ordinary-differential-equations']
4806740,Confusion between blow-up and relative projective space,"Let $k$ be a field, $R = k[x,y]$ and $I = (x,y)$ , so that $\operatorname{Spec}(R) = \mathbb{A}^2_k$ and $V(I) = \{0\} \subset \mathbb{A}^2_k$ . By definition, the blow-up of the plane at the origin is the projective scheme associated to the Rees algebra $S_\bullet = \bigoplus_{n \geq 0} I^n$ . Therefore in our case $S_\bullet = R[X,Y]$ where $X,Y$ are the elements $x,y \in I$ lying in degree 1. On the other hand, $\operatorname{Proj}(S_\bullet) = \mathbb{P}^1_R$ so we seem to have an isomorphism $\mathrm{Bl}_0 \mathbb{A}^2_k \cong \mathbb{P}^1_{\mathbb{A}^2_k}$ . Unless I am missing something about $S_\bullet$ the difference between the two must be in their structure morphism to $\mathbb{A}^2_k$ , since $\mathrm{Bl}_0 \mathbb{A}^2_k$ only has a $\mathbb{P}^1_k$ above the origin and a point $\operatorname{Spec}(k)$ elsewhere, whereas $\mathbb{P}^1_{\mathbb{A}^2_k} \cong \mathbb{P}^1_\mathbb{Z} \times \mathbb{A}^2_k$ has a copy of $\mathbb{P}^1_\mathbb{Z}$ over each point of the plane. If so, how can I describe the difference between the two structure morphisms?",['algebraic-geometry']
4806749,"Closed form: $\displaystyle\int_0^\infty \! \prod_{i=1}^n\frac{1}{a_i^2 + x^2} \, dx$","Can you find the closed form for the following integral $$ \int_0^\infty \prod_{i=1}^n \frac{1}{a_i^2 + x^2} \, dx = \int_0^\infty \! \frac{dx}{(a_1^2 + x^2) (a_2^2 + x^2) \cdots (a_n^2 + x^2)} $$ where $a_i \in \mathbb{R}$ for any $i\in\mathbb{Z}_{\gt0}$ ? For example, $$ \begin{align}
  &\int_0^\infty \! \frac{dx}{a^2 + x^2} = \frac\pi2 \frac1a \\[3pt]
  &\int_0^\infty \! \frac{dx}{(a^2 + x^2)(b^2 + x^2)} = \frac\pi2 \frac1{ab \, (a+b)} \\[3pt]
  &\int_0^\infty \! \frac{dx}{(a^2 + x^2)(b^2 + x^2)(c^2 + x^2)} = \frac\pi2 \frac{a+b+c}{abc \, (a+b)(b+c)(c+a)} \\[3pt]
\end{align} $$","['integration', 'calculus', 'definite-integrals', 'closed-form']"
4806751,$H$ acts on $G/H$. Show that $\mathrm{Fix}(H) = G/H$,"So $H$ acts on $G/H$ . Where $H$ is a subgroup of $G$ . We are also given that $|G| = p^2$ and $|H| = p$ , where p is a prime. $$
H \times G/H \rightarrow G/H, \quad (h, gH) \rightarrow h * (gH) = (hg)H
$$ The question is to show that $\mathrm{Fix}(H) = G/H$ . My attempts so far keep getting to the point where I need to show that H is normal. However, the next part of the question is to show that H is normal so I'm obviously going wrong somewhere. In the previous parts of the question I have shown that $\mathrm{Stab}(gH) = H \cap gHg^{-1}$ and $H \in \mathrm{Fix}(H)$ . Any help would be much appreciated.","['quotient-group', 'group-theory', 'group-actions']"
4806752,Courant & Robbins Intuitive Demonstration of the Non Denumerability of the Real Number Line,"In Courant & Robbins What is Mathematics , on pages 82 & 83 a measure based intuitive proof that the real number line is non denumerable is made : Assume the real numbers in the interval $[0,1]$ can be listed as $a_1,a_2,a_3,...,a_n,...$ Associate with each n a line interval of length $1 / 10^n$ Add up the line lengths around each real number $a_n$ : $$ 1/10^1 + 1/10^2 + 1/10^3 + ... = 1/9$$ Since the length of the interval $[0,1]$ is $1$ we have a contradiction as $1/9\ne 1$ . Hence the real numbers can't be mapped to the natural numbers. A note is added to say that if $\epsilon \cdot 1 / 10^n$ was used instead then that shows the measure (length) of a denumerable set of points would have to be zero (and not the $1/9$ that would apply to a countable set of intervals). The book indicates that this provides a intuitive proof that the real number line can't be mapped to the natural numbers, but needs a ""fuller analysis"" to make a watertight proof. So the question is :  What is the precise mathematical reason why it isn't a full mathematical demonstration that the real numbers are uncountable - i.e. where are the 'gaps' that the ""fuller analysis"" would need to address?",['elementary-set-theory']
4806773,Lower bound for chromatic number of a regular graph,"In a certain exercise I am asked to show that if $G$ is a $k$ -regular graph, then $\chi(G) \geq \frac{n}{n-k}$ , where $\chi(G)$ is the chromatic number of $G$ . As far as I have reached, I know that in a coloring of $G$ using the least colors possible, we can classify the vertices on sets $C_i = \{ v \in V(G): color(v) = i \}$ . It is clear that, if we use a number of colors equal to the chromatic number, this classification of vertices in sets form a partition, and there cannot be edges between vertices of the same set, and there must be at least an edge from $C_i$ to $C_j$ if $i \not = j$ . I also noticed that $\frac{n}{n-k} = 1 + \frac{k}{n-k}$ . Could somebody give a hint to show the inequality?","['graph-theory', 'coloring', 'combinatorics', 'discrete-mathematics']"
4806796,Is There Any Meaning To This Operator? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 months ago . Improve this question A while ago I was thinking about how in physics there is velocity $\frac{d}{dt}x(t)$ being the gradient of a displacement-time function $x(t)$ , and  angular velocity $\frac{d\theta}{dt}$ being the gradient of an angular displacement-time function $\theta(t)$ , and how in mathematics no one seems to define the gradient of the angle of a tangent line of some function at some point $x$ . Perhaps this is for a reason as it might just not be that interesting, but I went ahead and came up with the following: $$\frac{dy}{dx}\mid_{x=x} = \tan\theta(x)$$ We define the operator $\hat{\theta}$ as $$\hat{\theta}[f] = \frac{d}{dx}\arctan\left(\frac{df}{dx}\mid_{x=x}\right)$$ An interesting result is for example, $$\hat{\theta}[\ln x] = \frac{d}{dx}\text{arccot}\,x$$ Which can be thought meaningfully as the rate of change with respect to $x$ of the angle of the tangent at a point of $\ln x$ is the gradient of $\text{arccot}\,x$ . This is interesting but quite obvious if one were to calculate it. Another one is that
if $f'' = 1$ then $$\text{arc length} = \int_a^b\frac{dx}{\sqrt{\hat{\theta}[f]}}$$ There are also interesting differential equations one could explore with $$\hat{\theta}[y] = \frac{y''}{(y')^2+1}$$ Looking at invariants like $$\hat{\theta}[y] = y$$ or perhaps $$\hat{\theta}[y] = y'$$ These are for the most part the only interesting things I've been able to find regarding $\hat{\theta}$ , and so I'm wondering if anyone can come up with anything interesting , or provide insights, or perhaps even reference a paper or something where this is used. Or perhaps if it is completely useless then one could say that too if they wanted. Edit: To clarify, I know that there is a difference between a derivative and a gradient. The first section was just to explain where the idea came from","['calculus', 'functions', 'derivatives', 'ordinary-differential-equations']"
4806815,Power series question.,"How would I go about solving $$\sum_{n=1}^{\infty} \frac{(n-1)x^{n-1}}{(n-1)!}$$ So far I have tried to of course consider the exponential power series, but I seem to get negative factorials.","['summation', 'sequences-and-series', 'real-analysis']"
4806825,Are these two definitions of the coefficient of determination $R^2$ equal?,"I want to do multiple linear regression as explained on this Wikipedia site : I am given data $$
yx=(~(y_1,x_{11},\ldots,x_{1p}),\ldots, (y_n,x_{n1},\ldots,x_{np})~)
$$ of $n$ -many samples where for each sample $(y_i,x_{i1},\ldots,x_{ip})$ the variable $y_i\in\mathbb{R}$ is ''dependent'' (the ''dependent variable'') from $x_{i1},\ldots,x_{ip} \in\mathbb{R}$ . We want to find a hyperplane in $\mathbb{R}^{n+1}$ through these points as one does in (multiple/multivariate) linear regression and as it is explained e.g. here on Wikipedia . How good or bad such a hyperplane describes the points of the sample is measured e.g. by the $R^2$ -factor or ''coefficient of determination''. This is, as explained here on Wikipedia , defined as $$
R^2 = 1-\frac{SS_{res}}{SS_{tot}}
$$ where the values $SS_{res}$ and $SS_{tot}$ depend on the data as explained on the linked Wikipedia-page. (As the notation with the square does not suggest, this may take values in $(-\infty,1]$ . If $\bar y=\frac{1}{n}\sum y_i=0$ , then this may be equally defined as $R^2=\frac{SS_{reg}}{SS_{tot}}\in[0,1]$ .) My problem is as follows: There is another definition of $R^2$ - call it temporarily $R_2^2$ - which is given by $$
R^2_2 = (r_{1y},\ldots, r_{py}) \begin{pmatrix} r_{11}=1 & \cdots & r_{1p}\\
                                                \vdots & \ddots & \vdots\\
                                               r_p1 & \cdots & r_{pp}=1
\end{pmatrix}^{-1}\begin{pmatrix}r_{1y}\\\vdots\\r_{py}\end{pmatrix}
$$ where the $(p\times p)$ -matrix in the middle is called the correlation matrix with $$
r_{ij} = \frac{s_{ij}}{s_{i}s_{j}}\quad\quad\text{and}\quad\quad r_{iy} = \frac{s_{iy}}{s_{i}s_{y}}
$$ the respective correlation coefficients with respective sample variances $$
s_{ij} = \frac{1}{n-1}\sum_{k=1}^n (x_{ki}-\bar{x_{\bullet i}})(x_{kj}-\bar{x_{\bullet j}})\quad\text{and}\quad s_{i} = \sqrt{s_{ii}} \quad\text{and}\quad s_{iy}  = \frac{1}{n-1}\sum_{k=1}^n (x_{ki}-\bar{x_{\bullet i}})(y_{k}-\bar{y}) \quad\text{etc.}
$$ Are these numbers $R^2$ and $R^2_2$ the same and, if yes, why? Thank you! (I would prefer a linear-algebra-argument without involving probability theory and distributions, etc. - everything is empirical and given here.)","['regression', 'statistics', 'linear-regression']"
4806860,"$\int_0^1 x^n f(x)\,\mathrm{d}x = 0$ for all $n$ implies $f=0$ almost everywhere","It has been shown that given $f \in \mathcal C[0,1]$ , we have that if $\int_0^1 x^nf(x)\,dx = 0$ for all $n \in \mathbb N$ , then $f = 0$ . I was thinking of generalizing this statement to $L^p$ spaces. For instance, if we have that $f \in L^\infty[0,1]$ instead, and $\int_0^1 x^nf(x)\,dx = 0$ for all $n \in \mathbb N$ . Can we say that $f = 0$ a.e on $[0,1]$ ? That is, if $f$ is measurable on $[0,1]$ , with $\|f\|_\infty = \inf\{a \geq 0 \mid \lambda(|f|^{-1}(a,\infty]) = 0\} < \infty$ given $|f|^{-1}(a,\infty] = \{x \in [0,1] \mid |f(x)| > a\}$ , can we show that $f = 0$ for all points in $[0,1]$ except some set $B$ where $\lambda(B) = 0$ ? In this case, we use $\lambda$ to denote the Lebesgue measure.","['measure-theory', 'lebesgue-measure', 'measurable-functions']"
4806891,Does there exist a midsquare with natural sides and diagonal?,"A midsquare is an orthodiagonal and equidiagonal quadrilateral. Are there naturals $a,b,c,d,e$ such that the quadrilateral with sides $a,b,c,d$ and diagonal $e$ is a midsquare ? $e^2$ is a solution for $$2e^4-2(a^2+c^2)e^2+(a^2-b^2)^2+(b^2-c^2)^2=0$$ Let $D=4b^2d^2-(a^2-c^2)^2$ Then $e^2=\dfrac{a^2+c^2+\sqrt{D}}2$ p=1 
q=5000
import math
for a in range (p,q): # a = AB 
    for b in range(a,q): # b = BC
        for c in range (p,q): # c = CD
            if a*a+c*c > b*b:
               d = (a*a+c*c-b*b)**.5 # d = DA
               if int(d)==d:
                  d=int(d)
                  D = 4*b*b*d*d-(a*a-c*c)*(a*a-c*c) 
                  if D>=0:
                     if int(D**.5)==D**.5:
                        e = (.5*(a*a+c*c+D**.5))**.5
                        if int(e)==e: 
                           print(a+b+c+d+e+e,a,b,c,d,e,e) Some results with $\sqrt{D}\in\mathbb{N}$ $$\begin{array}{|c|c|c|c|}\hline
\text{perimeter-plus } a+b+c+d+2e & \sqrt{D} & e^2&e\\
\hline
658.488\approx35+101+149+115+2\times 129.244& 9982 & 16704&24\sqrt{29}\\
\hline
1183.63\approx41+181+289+229+2\times 221.815& 13202 &  49202& \sqrt{2\times73\times337}\\
\hline
3738.168\approx221+481+899+791+2\times673.084& 49042 &  453042& 3\sqrt{2\times25169}\\
\hline
6105.496\approx299+925+1405+1099+2\times1188.748&762818 & 1413122& \sqrt{2\times706561}\\
\hline
7611.367\approx85+1339+1939+1405+2\times1421.684&275422& 2021184& 264\sqrt{29}\\
\hline
11665.227\approx845+1513+2525+2191+2\times2295.614& 3450034 & 5269842& 3\sqrt{2\times137\times2137}\\
\hline
11992.528\approx1015+1241+2759+2665+2\times2156.264& 656642 & 4649474& \sqrt{2\times661\times3517}\\
\hline
17069.827\approx709+2759+3995+2975+2\times3315.913 &5527858& 10995282 & 3\sqrt{2\times610849}\\
\hline
21602.357\approx1681+2381+4889+4589+2\times4031.178&5772718 &16250400& 60\sqrt{2\times37\times61}\\
\hline
37947.648\approx1321+6329+9049+6601+2\times 7323.824 &23647358 & 53638400&1360\sqrt{29}\\
\hline
44392.996\approx2509+7081+9901+7361+2\times8770.498&49518382&76921632&12\sqrt{2\times89\times3001}\\
\hline
\end{array}$$ Two special cases The isosceles trapezoid midsquare $a,b,c,b,e,e$ . It's easy to show that with $a,b,c,e$ integers, it doesn't exist: a nice geometric expression for $e$ as a function of $a$ and $c$ . The kite midsquare $a,a,c,c,e,e,~a<c$ . I don't know whether it exists or not. For it, I can't even find an integer $\sqrt{D}=\sqrt{4a^2c^2-(a^2-c^2)^2}$ . 20/11/23 Let $g$ be the distance between the midpoints of the diagonals (see Euler's quadrilateral theorem), we can show that $$a^2+2eg=c^2$$ So $g$ is a rational number, we can look for $e$ and $g$ naturals such that $$(e-g)^2+g^2=2a^2$$ $$(e+g)^2+g^2=2c^2$$ Example of an ""almost solution"" $$(1192-1043)^2+1043^2=1~110~050=2\times745^2~~~~~~~~~~$$ $$(1192+1043)^2+1043^2=6~083~074=2\times1744^2~\boxed{+2}$$ I still don't find any solution.","['modular-arithmetic', 'number-theory', 'elementary-number-theory', 'geometry', 'arithmetic-geometry']"
4806902,"If $\,\lim_{n\to\infty}f(nx)\,$ exists, for all $x\in\mathbb R$, then so does $\,\lim_{x\to\infty}f(x)\,$","Let $\,f:\mathbb{R}\to\mathbb{R}$. The following limit exists for all $x \in \mathbb{R}$: 
$$\lim_{n\to ∞} f(nx) ,$$ where $n \in \mathbb N$. Is it correct that:
$$\lim_{x\to ∞} f(x) ,$$ 
exists if: a) $f$ any function, b) $f$ continuous on $\mathbb{R}$.","['limits', 'calculus', 'continuity', 'real-analysis']"
4806914,"I know that $ \int_0^\infty \frac{|f(x)|^2}{x} < \infty $, which condition should I have to prove $ \int_0^\infty \frac{|f(x)|^2}{x^2} < \infty $?","I'm working with the wavelet transform, and I'm facing a problem proving that $$ \int_0^\infty \frac{|f(x)|^2}{x^2}\mathrm{d}x $$ converges. The only thing I know is that $f(x)$ is the Fourier transform of some analytical mother wavelet which indicates that $$ \int_0^\infty \frac{|f(x)|^2}{x}\mathrm{d}x  < \infty $$ . I have no idea about what conditions should be added to prove that and the ideas to prove that. Thanks for helping.","['integration', 'wavelets']"
4806918,"Expected gain for the coin toss game where you gain \$1 with heads, lose \$2 with tails, and the game ends with 3 heads in a row.","Below is a detailed description of the game: The player starts with an initial balance of zero. On each turn, the player fips a fair coin - if it landson heads (H). the pot balance increases by 1 dollar, if it lands on tails (T), the potbalance decreases by 2 dollars. The game ends when the player flips three heads in arow (HHH), at which point an ertra 8 dollars is added to the balance. At the end of the game, the player gets to take their pot home (if the balance is positive) or is obligated to make up for the shortfall (if the balance is negative). The game cannot be terminated early by the player. On purely mathematical grounds, what is the maximum amount you would pay to enter this game? What do you think is the solution?
In theory we need $2^{3+1}-2=14$ flips to get HHH, which terminates the game. With an expected flip of 14, 7 of them is expected to be H, the rest 7  expected to be T. Is the payoff equal to $7*1+7*(-2)+8=1$ ?","['expected-value', 'probability-theory', 'probability']"
