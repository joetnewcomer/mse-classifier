question_id,title,body,tags
4105199,"$L^p$ convergence of certain ""average"" function","Given a function $f \in L^2([0,1])$ . Let's fix some irrational number $\omega$ . For any $N \in \mathbb{Z}^{+}$ , let's define a function $g_{N}$ as follows: $$g_{N}(x) := \frac{1}{N}\sum_{j=1}^{N}f(x+j\omega)-\int_{0}^{1}f(y)dy$$ Prove that $\lim_{N \rightarrow \infty}||g_{N}||_{L^2(\mathbb{T})} = 0$ . Moreover, if we further assume $f$ is periodic with period $\omega$ , then $f$ must be constant. I'm interested in how the first claim can be proved. How does the ""irrational number"" assumption come into play here? Any hint/discussion would be appreciated!","['fourier-analysis', 'harmonic-analysis', 'ergodic-theory', 'analysis', 'real-analysis']"
4105217,Calculating a limit. Is WolframAlpha wrong or am I wrong?,"What I'm trying to solve: $$\lim _{x\to -\infty \:}\frac{\left(\sqrt{\left(x^2+14\right)}+x\right)}{\left(\sqrt{\left(x^2-2\right)}+x\right)}$$ What I put into WolframAlpha: (sqrt(x^2+14)+x)/(sqrt(x^2-2)+x) My result: $1$ , which I get by simply dividing bot the numerator and the denominator by $x$ , then letting it go towards $-\infty$ $$\frac{\frac{\left(\sqrt{x^2+14}+x\right)}{x}}{\frac{\left(\sqrt{x^2-2}+x\right)}{x}}=\frac{\left(\sqrt{\frac{x^2}{x^2}+\frac{14}{x^2}}+\frac{x}{x}\right)}{\left(\sqrt{\frac{x^2}{x^2}-\frac{2}{x^2}}+\frac{x}{x}\right)}=\frac{\left(\sqrt{1+\frac{14}{x^2}}+1\right)}{\left(\sqrt{1-\frac{2}{x^2}}+1\right)}\:=\:\frac{\left(\sqrt{1}+1\right)}{\left(\sqrt{1}+1\right)}\:=\:\frac22 \ = \ 1$$ WolframAlpha's result: $-7$ . It has a long, complicated 25 step solution. Is the solution $1$ , $-7$ or neither? Edit: of course, I set it $x$ to go towards $-\infty$ in WolframAlpha too","['limits', 'calculus', 'wolfram-alpha', 'real-analysis']"
4105255,What is the reasoning used here to determine the UMVUE?,"Let $X_1, \dots, X_n$ denote a random sample from the PDF $$f_{\varphi}(x)=
\begin{cases}
 \varphi x^{\varphi - 1} &\text{if}\, 0 < x < 1, \varphi > 0\\
      0 &\text{otherwise}
\end{cases}$$ This density function is a member of the one-parameter exponential family. Let $A_i = - \log(X_i)$ , where $A_i$ has an exponential distribution. This means that $2\varphi \sum_{i = 1}^n A_i$ has a $\chi^2$ distribution with $2n$ degrees of freedom. Furthermore, I have that $$ E\left[ \left( 2\varphi \sum_{i = 1}^n A_i \right)^{-1} \right] = \dfrac{1}{2n - 2} $$ and $$\text{Var} \left( \dfrac{1}{2 \varphi \sum_{i = 1}^n A_i} \right) = \dfrac{1}{4(n - 1)^2(n - 2)}$$ Apparently, using this information, we can conclude that $\dfrac{n - 1}{\sum_{i = 1}^n A_i} = \dfrac{n - 1}{- \sum_{i = 1}^n \log(X_i)}$ is a UMVUE for $\varphi$ . However, I am not able to follow this reasoning for how they calculated the UMVUE (nor for how they concluded that this is a UMVUE). If I had to guess, it seems to me that they might have done some kind of bias correction at $\dfrac{n - 1}{\sum_{i = 1}^n A_i} = \dfrac{n - 1}{- \sum_{i = 1}^n \log(X_i)}$ , but I'm honestly not sure. What is the reasoning used here for why $\dfrac{n - 1}{\sum_{i = 1}^n A_i} = \dfrac{n - 1}{- \sum_{i = 1}^n \log(X_i)}$ is a UMVUE for $\varphi$ ? I'd greatly appreciate it if someone would please take the time to fill in the gaps and explain this reasoning so that I may understand it.","['gamma-distribution', 'statistics', 'exponential-distribution', 'chi-squared']"
4105284,What are examples of Antiset?,"A set which transforms via converse functions is called antiset . Antisets usually arise in the context of Chu spaces .
I couldn't understand the notion of antiset and its examples.","['elementary-set-theory', 'functions', 'set-theory']"
4105372,Clean/simple way of computing $\nabla f(U U^\mathsf{T})$ with respect to $U$,"Let $f(X)$ be a matrix function which takes in an $n \times n$ matrix $X$ and spits out something in $\mathbb{R}$ . Let $U \in \mathbb{R}^{n \times r}$ for some $r << n$ . I was reading a paper which stated without explanation that the gradient of $f$ with respect to $U$ is $$
\left(\nabla f (U U^\mathsf{T}) + \nabla f(U U^\mathsf{T})^\mathsf{T}\right) \cdot U.
$$ The gradient of $f$ with respect to $X$ is naturally defined as the matrix $\nabla f(X) \in \mathbb{R}^{n \times n}$ such that $$
[\nabla f(X)]_{ij} = \frac{\partial f(X)}{\partial x_{ij}}.
$$ I spent a significant amount of time trying to derive this using the chain rule for multivariate functions. I.e., I thought of $f : \mathbb{R^{n^2}} \to \mathbb{R}$ as being precomposed by $g : \mathbb{R}^{n \times r} \to \mathbb{R}^{n^2}$ defined by $g(U) = U U^\mathsf{T}$ . But I don't see how the fact given follows easily from this. It could be that I'm just far too used to thinking about gradients as vectors, and I can't translate to these matrix functions. Ultimately, I gave up on that approach and verified the identity by working with individual partial derivatives. But my question is, is there a neat way to derive this fact without resorting to working with partial derivatives? If there is, I really want to know it, because I find that I struggle to compute derivatives of matrix functions. How would someone experienced in this area approach this problem? Thank you! EDIT: Thanks to @greg's answer, I have decided to study (matrix) differentials. Here are some resources (in order from favorite to least favorite; but all three are useful) I found which might help others who stumble upon this question: https://tminka.github.io/papers/matrix/ http://artem.sobolev.name/posts/2017-01-29-matrix-and-vector-calculus-via-differentials.html http://thousandfold.net/cz/2013/11/12/a-useful-trick-for-computing-gradients-w-r-t-matrix-arguments-with-some-examples/","['multivariable-calculus', 'matrix-calculus']"
4105379,Quick Verification for Sufficient Statistic in Exponential Families!,"Problem: Prove that if the distribution of $X=(X_1,...,X_n)$ is in an exponential family, then $T(X)=(T_1(X),...,T_J(X))$ is sufficient for $\theta$ . Progress: Knowing that an exponential family can be written as $f(x;\theta)=1(x\in A)\exp\{\sum_{j=1}^J\gamma_j(\theta)T_j(x)+d(\theta)+S(x)\}$ , is this just the result of using the Neyman-Fisher Factorization Theorem? This seems too trivial so I must be missing something. Any help or verification would be appreciated!","['statistical-inference', 'statistics', 'probability-distributions', 'normal-distribution', 'exponential-function']"
4105419,Reference request for an identity involving Catalan numbers,"I am wondering if a bijective proof of the following identity involving Catalan generating functions has appeared anywhere in the literature. (It's not difficult to simply verify it for the functions involved, so a bijective proof is the interesting part here.) Let $C_n=\displaystyle\frac{1}{n+1}\binom{2n}{n}$ , $C(z)=\displaystyle\sum_{n=0}^{\infty}{C_nz^n}=\dfrac{1-\sqrt{1-4z}}{2z}$ , and $C_{\text{odd}}(z)=\displaystyle\sum_{n=0}^{\infty}{C_{2n+1}z^n}=\dfrac{C(\sqrt{z})-C(-\sqrt{z})}{2\sqrt{z}}$ . Then $$
(zC(z))\circ(zC(4z))=zC_{\text{odd}}(z),
$$ where $\circ$ denotes composition of functions. Expressed in terms of the coefficients of these functions, this yields, after a few cancellations, $$
\sum_{k=0}^{n}{\binom{2n-2k}{n-k}\binom{n+k}{k}4^k}=\binom{4n+1}{2n},
$$ or, equivalently, $$
\sum_{k=0}^{n}{\binom{2k}{k}\binom{2n-k}{n-k}4^{n-k}}=\binom{4n+1}{2n}.
$$ Update: The identity $(zC(z))\circ(zC(4z))=zC_{\text{odd}}(z)$ can be thought of as a companion to Shapiro's Catalan identity for $C_{\text{even}}(z)=\displaystyle\sum_{n=0}^{\infty}{C_{2n}z^n}=\dfrac{C(\sqrt{z})+C(-\sqrt{z})}{2}$ , namely, that $$
C(4z)=C_{\text{even}}(z)^2.
$$","['catalan-numbers', 'combinatorial-proofs', 'binomial-coefficients', 'combinatorics', 'generating-functions']"
4105466,Projection of an non-increasing sequence of closed convex subsets of a Hilbert space [Haim Brezis Exercise 5.5],"This question comes from the Exercise 5.5 of Haim Brezis' Functional analysis, and a related but unsolved post is here: Prove that for every $f$ in $H$, the sequence $u_n$ which is the projection of $f$ on $K_n$ converges to a limit . The question can be described as follows: Let $(K_{n})_{n=1}^{\infty}$ be a family of non-increasing sequence of closed convex set in a Hilbert space $H$ with $K:=\bigcap_{n}K_{n}\neq\varnothing$ . The book has ensured that $K$ is also closed and convex. Let $P_{K_{n}}$ be the projection onto $K_{n}$ , i.e. the map that to $x\in H$ associate the unique point $y=P_{K_{n}}x\in K_{n}$ such that $$\|x-y\|=dist(x,K_{n})=\inf_{z\in K_{n}}\|x-z\|.$$ This property has been ensured by the Hilbert projection theorem. Then, show that for all $x\in H$ , $$\lim_{n\rightarrow\infty}\|P_{K_{n}}x-P_{k}x\|=0.$$ I have deleted my attempt and some edit because they contain several mistakes. I just wrote a proof, so I will directly post it below.","['analysis', 'real-analysis', 'hilbert-spaces', 'functional-analysis', 'convex-analysis']"
4105649,Show that $\lim \sqrt{1-\frac{1}{n}} = 1$ by definition,"Is my attempt correct? Proof . Let $\varepsilon > 0$ . Take $N > \frac{1}{\varepsilon}$ and let $n \geq N$ . Then $\begin{align}\displaystyle\left\lvert \sqrt{1- \frac{1}{n}} -1 \right\rvert 
&= \left\lvert \frac{\sqrt{n-1}}{\sqrt{n}} -1 \right\rvert\\\\
&= \left\lvert \frac{\sqrt{n-1} - \sqrt{n}}{\sqrt{n}} \right\rvert\\\\
&= \left\lvert \frac{(\sqrt{n-1} - \sqrt{n})(\sqrt{n-1} + \sqrt{n})}{\sqrt{n}(\sqrt{n-1} + \sqrt{n})} \right\rvert\\\\
&= \left\lvert \frac{(n-1)-n}{\sqrt{n}(\sqrt{n-1} + \sqrt{n})} \right\rvert\\\\
&= \left\lvert \frac{-1}{\sqrt{n}(\sqrt{n-1} + \sqrt{n})} \right\rvert\\\\
&= \frac{1}{\sqrt{n}(\sqrt{n-1} + \sqrt{n})}\\\\
& < \frac{1}{\sqrt{n}(\sqrt{n})}= \frac{1}{n} \leq \frac{1}{N} < \varepsilon\end{align}$","['limits', 'sequences-and-series', 'real-analysis']"
4105650,Gibbs measure is concentrated in the set of global minima,"So I was reading Chii-Ruey Hwang's paper called ""Laplace's Method Revisited: Weak Convergence of Probability Measures"" I will sort of give the basic premise of the paper: Let $Q$ be a fixed probability measure on $\mathbf{R}^n$ and $H$ be a continuous function on $\mathbf{R}^n$ s.t. $$a \gt \inf_x H(x) \implies Q\{H(x) < a\} \gt 0$$ Let $N$ be the set of minimal states in $H$ , i.e. $$N = \{x | H(x) = \inf_y H(y) \} $$ He goes on to describe a probability measure on the set $N$ . In order to do this he defines the measures $P_\theta$ as $$\frac{dP_\theta}{dQ}(x)  = \frac{\exp{(-H(x)/\theta)}}{\int \exp{(-H(x)/\theta)}dQ(x)}$$ He goes on to claim that if $P_\theta \rightarrow P$ weakly as $\theta \rightarrow 0$ then $P$ is the required measure on the set $N$ . He presents this as a corollary. The theorem he proves is the following: Theorem:
If $H$ does not have a minimum then $P_\theta$ is not tight. He then makes an assumption that the minimum of H(x) exists and is 0. Under this assumption he gives the following corollary If $P_\theta \rightarrow P$ weakly then $P$ concentrates on $N$ . The corollary is presented without proof. Any idea on the motivation of the construction of such measures and why the corollary holds true from the theorem will be helpful. Thanks EDIT If you think the proof of the theorem might help let me know I will post it.
Apparently this has deep ties with statistical mechanics and in a lot of places people have used it directly. I dont see how this is so obvious EDIT So I guess as equivalent question would be to sure why the Gibbs distribution concentrates about the set of minimas. As I found out these are basis of simulated annealing techniques.","['measure-theory', 'laplace-method', 'statistical-mechanics', 'probability-theory', 'stochastic-calculus']"
4105667,Solve differential equation $3x^3 (y')^2 + 3x^2yy' + 5 = 0$,"$3x^3 (y')^2 + 3x^2yy' + 5 = 0$ To me it looks like quadratic equation with respect to $y'$ , so I came up to that $$y' = \dfrac{-3x^2y \pm \sqrt{9x^4y^2 - 12x^3}}{6x^3}$$ And from there I got completely stuck. Also I tried to divide everything with $3x^2y'$ $$xy' + y + \dfrac{5}{3x^2y'} = 0$$ I don't know what to do next because of that $3x^2y'$ in the denominator, but $xy' + y$ part looks like Euler equation",['ordinary-differential-equations']
4105681,Show that the Levy distance is a metric,"The Levy metric is defined on the space of cumulative distribution functions as $$\rho(F,G):=\inf\{\varepsilon>0\mid F(x-\varepsilon)-\varepsilon\leq G(x)\leq F(x+\varepsilon)+\varepsilon,\quad \forall x\in \mathbb R\},$$ where $F$ and $G$ are cumulative distribution functions. I would like to show that it is a metric space. I think I have shown that $\rho(F,G)=\rho(G,F)$ but I also need to show $\rho(F,G)=0\iff F=G$ $\rho(F,H)\leq \rho(F,G)+\rho (G,H)$ For the first one I first managed to show it when I supposed that $F$ and $G$ are continuous but this hypothesis is obviously not required. For the second one I don't know... Thank you.","['probability-distributions', 'probability-theory', 'probability', 'real-analysis']"
4105711,MLE for mixed distribution,"Let $X\sim f(x)=\begin{cases}\lambda e^{-\lambda x},&x\in [0,5)\\e^{-5\lambda},&x=5\end{cases}$ How can I derive the MLE for this distribution?
I know how to derive the MLE for $\lambda$ if $Y\sim\exp(\lambda)$ but I don't know how to work with this mixed distribution","['statistical-inference', 'statistics', 'probability-distributions', 'maximum-likelihood']"
4105727,Covariant derivative of Killing vector field is antisymmetric,"Let $M$ be a Riemannian manifold with Levi-Civita connection $\nabla$ . A vector field $X\in \mathfrak{X}(M)$ is said to be a Killing vector field if, for every $t$ , its flow $X_{t}\colon M_{t}\to M_{t}$ is an isometry. I'm trying to prove that the covariant derivative $(\nabla X)_{p}\colon T_{p}(M)\to T_{p}(M)$ is an antisymmetric linear map with respect to the metric. As far as I know, this statement is easy to prove by using Lie derivatives, but I'd like to see a proof where we do not make explicit use of that. So far, I've attempted the following: Let $v,w\in T_{p}(M)$ . I need to see that $\langle\nabla_{v}X,w\rangle+\langle v,\nabla_{w}X\rangle=0$ . It seems natural to me to consider the function $g(t)=\langle (X_{t})_{*p}(v),(X_{t})_{*p}(w) \rangle=\langle v,w\rangle$ and to try to convert the equation $g'(0)=0$ into either the equation above or something similar. Am I on the right track or should I try a different approach? Thank you in advance! EDIT I've tried to give a proof of the result by using coordinates (I'm not too fond of it, but at least I think it gets the job done). Suppose $X(p)\neq 0$ . Then, by the Tubular Neighborhood Lemma, we can find a coordinate chart $(U,(x^{1},...,x^{n}))$ where $U$ corresponds to the cube $(-\varepsilon,\varepsilon)^{n}$ , and $X=\partial_{1}$ . To prove the result, it suffices to see that $\langle \nabla_{\partial_{i}}\partial_{1},\partial_{j} \rangle=-\langle \partial_{i},\nabla_{\partial_{j}}\partial_{1}\rangle$ . On one hand: $$
\langle \nabla_{\partial_{i}}\partial_{1},\partial_{j} \rangle_{p}=\langle \Gamma_{i1}^{k}\partial_{k},\partial_{j}\rangle_{p}=\Gamma_{i1}^{k}(p)g_{kj}(p).
$$ On the other hand, since $X_{t}$ is now given in coordinates by $(u^{1},...,u^{n})\mapsto(u^{1}+t,...,u^{n})$ and is an isometry, it follows that the functions $g_{ij}$ are independent of $x^{1}$ . That is: $\partial_{1}g_{ij}=0$ . Now, by using the Koszul formula $$
\Gamma_{i 1}^{k} g_{k j}=\frac{1}{2}\left(\partial_{i} g_{1 j}+\partial_{1} g_{i j}-\partial_{j} g_{i 1}\right)=\frac{1}{2}\left(\partial_{i} g_{1 j}-\partial_{j} g_{1 i}\right)=-\Gamma_{j 1}^{k} g_{k i}
$$ so we get the desired result. Now, by continuity, we deduce that $\nabla X$ is antisymmetric at every $p\in \operatorname{Supp}(X)$ . Finally, if $p\notin \operatorname{Supp}(X)$ , then $X=0$ in a neighborhood of $p$ , which implies $(\nabla X)_{p}=0$ (which is clearly antisymmetric). Is this proof correct? Also, is there a way of seeing this without appealing to coordinates?","['vector-fields', 'riemannian-geometry', 'differential-geometry']"
4105749,Definition of double covers,"It's unusual that I have to ask for a definition on this site, but it in the context of spin structures, it seems common to talk about double covers without giving the definition. I understand the definition of a covering space , but the definition of double covers (more generally, $n$ -fold covers) on Wikipedia is based on an alternative definiton of covering spaces I don't understand: Equivalently, a covering space of $X$ may be defined as a fiber bundle $p\colon C\to X$ with discrete fibers. And further below: A covering is a double cover if every fiber has two elements. I don't know what they mean by ""discrete fibers"". The article links to the discrete topology of a set, but I don't see the correlation. In the case of a double cover, does it simply mean that each fiber $p^{-1}(\{x\})$ consists of two elements for all $x\in X$ ?","['general-topology', 'fiber-bundles', 'covering-spaces']"
4105796,Reference for a theorem about necessary and sufficient conditions for configuration of n-tuples of points.,"I'm writing my thesis and at a certain point I claim, and indeed it turns out to be true from discussions with my supervisors, that certain configurations for n-tuples of points, in a curve of degree k, have the following property: If there is an n-tuple of points of $\mathbb{P}^2$ such that the configuration C is possible than the configuration C is possible for any n-tuple For example it's possible that given 8 points on an irreducible quartic one of them is a triple point and the other 7 are non-singular points, but then I claim that given any 8-uple in $\mathbb{P}^2$ there is a quartic with a triple point in one of them and passing through the other 7. I claim that these configurations are all possible configuration on curves of degree bigger than 3 if we consider an 8-uple. What I need is a reference for such theorem that I'm quite sure exists","['algebraic-curves', 'projective-geometry', 'irreducible-polynomials', 'plane-curves', 'algebraic-geometry']"
4105808,Solve first order non-linear ODE,I've got such equation: $$5x^3(y')^2+5x^2yy'-3=0$$ I've tried to solve the quadratic eqaution w.r.t $y'$ but it lead me to non-linear equation which is not usefull as I could see.,['ordinary-differential-equations']
4105821,Function such that $f(x)+f(1-x)=1$,"Given that the function $f$ is continuous and has the property $f(f(x))=1-x$ for all $x\in[0,1]$ . Find $J=\int_{0}^{1}f(x)dx$ . My try: I did this problem without finding the function $f(x)$ , but I am interested in finding the function $f(x)$ . So what I did is I replaced $x$ with $f^{-1}(x)$ in the given equation (I assumed $f^{-1}$ exists.): $$\tag{1} f(x)=1-f^{-1}(x).$$ Also, by applying $f^{-1}$ on both sides of given equation $$f(x)=f^{-1}(1-x).$$ Replacing $x$ with $1-x$ , $$f(1-x)=f^{-1}(x).$$ Substituting in $(1)$ I got $$f(x)+f(1-x)=1.$$ This actually seemed to be a easier equation but I couldn't solve it. Particularly I think that it might be a piecewise function.","['integration', 'calculus', 'definite-integrals', 'functional-analysis']"
4105825,How to find functions in order to apply Squeeze Rule for continuous functions,"In our course we were introduced to the the Squeeze Rule for continuous functions. An example was given where the Squeeze Rule was used to prove that the following function is continuous at point $0$ . $$
f(x) = \begin{cases} 
      x^2 \sin(1/x) & x \not = 0 \\
      0             & x = 0 
   \end{cases}
$$ Looking at a graph of $f(x)$ , $g(x) = x^2$ and $h(x) = -x^2$ makes it pretty obvious that we could use $g, h$ together with the Squeeze Rule. But in a test they won't be so nice as to provide us with visual aids, also the use of a graphing calculator is forbidden. Are there any common techniques/clues we can apply if we are only given $f$ and it's graph (this they will provide in an examination) to find $g, h$ to apply the Squeeze Rule, proving continuity of $f$ ? Based on the definition of $f$ above alone I doubt I would have thought of $g(x) = x^2$ and $h(x) = -x^2$ straight away.","['graphing-functions', 'real-analysis', 'calculus', 'limits', 'inequality']"
4105846,Who first explicitly wrote the determinant identity $\det(1+AB) = \det(1+BA)$?,"Though this identity can be easily proved, I am wondering who first explicitly write it in such a simple and elegant form? I check several textbooks on linear algebra but find no evidence (see below the list of books I have checked).
The entry on wikipedia calls it the Weinstein–Aronszajn identity now but previously attributed it to J. J. Sylvester .
In the blog of Terence Tao , he calls it the Weinstein–Aronszajn identity with a link to the Wikipedia page, but tags and comments for that article imply that he used to call it the 'Sylvester determinant identity'.
The wiki page does not give direct references to the explicit form.
The book referred to in the proof calls this identity 'Sylvester's determinant theorem'.
The old version of the wiki page refers to an 1851 paper by Sylvester .
I have checked it and did not see the explicit form. In conclusion, no direct reference is given for the explicit form.
So I am very confused.
For the identity in the title, who should be attributed to? In case that asking about attribution may offend some mathematicians, I would like to focus on who first use the finite/infinite version of this identity to solve problems, especially in the explicit form. A direct reference would be a good answer. By 'direct reference' I mean a paper — a paper which (1) contains the explicit form of the identity, (2) applies it to solve problems and (3) is authored by whoever presumably be attributed to. Textbooks I have checked: C.D. Meyer, Matrix Analysis and Applied Linear Algebra (2000) R.A. Horn & C.R. Johnson, Matrix Analysis, 2nd ed. (2012)","['random-matrices', 'math-history', 'linear-algebra']"
4105919,Complicated comparing sets,"Consider the open interval $(0, 1)$ and the closed interval $[0, 1]$ . We claim that $[0, 1] ≈ (0, 1)$ .
We deduced the equipotence $[0, 1] ≈ (0, 1)$ by invoking the Schr¨oder–Bernstein theorem, but we didn’t actually construct a bijection $[0, 1] → (0, 1).$ (a) Verify that the function $h$ given by the following scheme is such a bijection: $$0\overset{h}{\longmapsto}\frac12\qquad\qquad\qquad\qquad\qquad\qquad\quad\,{}\\\frac1n\longmapsto\frac1{n+2}\qquad\text{for each integer }n\ge 1\\x\longmapsto x\qquad\qquad\qquad\qquad\quad\;\text{otherwise}$$ (b) The function $h$ in part (a) acts on $[0, 1]$ in a way that resembles the strategy of the hotel manager in the text below. How? Now imagine that there is a hotel with infinitely many rooms along an unending corridor: a first room, a second room, and so on. Imagine further that there is a person occupying each room; so we can reasonably say that there are exactly as many guests as there are rooms. You arrive at the hotel seeking a room, and the manager tells you, “At the moment the rooms are all occupied, but we always have room for one more.” Whereupon she speaks into an intercom connected to all the rooms, saying, “At the sound of the bell, each guest should step out into the corridor and move into the next room in line.” This clears out the first room, and you have a place to stay. Notice that although you thought you had enlarged the collection of guests by your presence, there are still as many hotel rooms as there are guests. My work is below: We are given function $h$ $$h(x)=\begin{cases}\cfrac1{n+2} & \text{for }x=\cfrac1n,n\ge 1\\\cfrac12 & \text{for }x=0\\x & \text{otherwise}\end{cases}$$ We know that a function from $A$ (the domain) to $B$ (the range) is both one-to-one and onto when no element of $B$ is the image of more than one element of $A,$ and all elements in $B$ are used. Functions that are both one-to-one and onto are referred to as bijective. Here we can say that $h(x)$ is an identity function as $x,$ in the form of $\frac1n$ and $x\ne0.$ We know that an identity function is bijective. For $x\ne\frac1n$ and $x\ne 0,$ $h(x)$ is bijective. Let us consider $x_1=\frac{1}{n_1},x_2=\frac{1}{n_2}$ and assume that $h(x_1)=h(x_2),$ so $h\left(\frac{1}{n_1}\right)=h\left(\frac{1}{n_2}\right).$ Then, $$\frac{1}{n_1+2}=\frac{1}{n_2+2}\iff n_1+2=n_2+2$$ Therefore, $n_1=n_2,$ thus $x_1=x_2$ which proves that $h(x)$ is one-to-one. For $n\ge 1,$ $\frac{1}{n+2}\ne\frac{1}{2}$ Thus, only $x=0$ maps to $\frac12.$ Let us check identity function for the numbers $y=\frac1n,n>2.$ If we have $y=\frac1n,$ then there exists an element $x$ such that $\frac1n=h\left(\frac{1}{n-2}\right),$ $x=\frac1{n-2}$ Therefore, $h(x)$ is onto, so we conclude that $h(x)$ is bijective. This question is driving me nuts. Please give me some advice on how to answer it concisely. Thank you so much!","['elementary-set-theory', 'solution-verification']"
4105920,"Prove $P(G\mid E_1) > P(G)$ and $P(G\mid E_2) > P(G)$, but $P(G\mid E_1, E_2) < P(G)$","I have a really simple question. The problem is as below. Let $G$ be the event that a certain individual is guilty of a certain robbery. In gathering evidence, it is learned that an event $E_1$ occurred, and a little later it is also learned that another event $E_2$ also occurred. (a) Is it possible that individually, these pieces of evidence increase the chance of guilt (so $P (G\mid E_1 ) > P (G)$ and $P (G\mid E_2 ) > P (G))$ , but together they decrease the chance of guilt (so $P(G\mid E_1,E_2) < P(G)$ )? The solution is: Yes, this is possible. In fact, it is possible to have two events which separately provide evidence in favor of $G$ , yet which together preclude $G$ ! For example, suppose that the crime was committed between $1$ pm and $3$ pm on a certain day. Let $E_1$ be the event that the suspect was at a nearby co↵eeshop from $1$ pm to $2$ pm that day, and let $E_2$ be the event that the suspect was at the nearby coffeeshop from $2$ pm to $3$ pm that day. Then $P(G\mid E_1) > P(G), P(G\mid E_2) > P(G)$ (assuming that being in the vicinity helps show that the suspect had the opportunity to commit the crime), yet $P(G\mid E_1,E_2) < P(G)$ (as being in the coffeehouse from $1$ pm to $3$ pm gives the suspect an alibi for the full time). I can't understand why the intersection of $E_1$ and $E_2$ is being from $1$ pm to $3$ pm. Isn't it an empty one since there is no intersection between the event from $1$ pm to $2$ pm and the event from $2$ pm to $3$ pm?","['conditional-probability', 'statistics', 'probability']"
4105937,"Understanding the proof of how ""only vector fields that are independent of path are conservative""","In James Stewart's Multivariable Calculus: Concepts and Contexts, this theorem is stated: ""Suppose $\mathbf{\vec F}$ is a vector field that is continuous on an open connected region $\mathit{D}$ . If $\int_C \mathbf{\vec F} \cdot d\mathbf{\vec r}$ is independent of path in $\mathit{D}$ , then $\mathbf{\vec F}$ is a conservative vector field on $\mathit{D}$ ; that is, there exists a function $\mathit{f}$ such that $\nabla \mathit{f} = \mathbf{\vec F}$ ."" The proof is as follows: ""Let $\mathit{A(a,b)}$ be a fixed point in $\mathit{D}$ . We construct the desired potential function $\mathit{f}$ by defining $$ \mathit{f\,(x,y)} = \int_\mathit{(a,b)}^\mathit{(x,y)} \mathbf{\vec F} \cdot d\mathbf{\vec r}$$ for any point in $\mathit{(x,y)}$ in $\mathit{D}$ . Since $\int_C \mathbf{\vec F}\,d\mathbf{\vec r}$ is independent of path, it does not matter which path $\mathit{C}$ from $\mathit{(a,b)}$ to $\mathit{(x,y)}$ is used to evaluate $\mathit{f\,(x,y)}$ . Since $\mathit{D}$ is open, there exists a disk contained in $\mathit{D}$ with center $\mathit{(x,y)}$ . Choose any point $\mathit{(x_1,y)}$ in the disk with $\mathit{x_1 \lt x}$ and let $\mathit{C}$ consist of any path $\mathit{C_1}$ from $\mathit{(a,b)}$ to $\mathit{(x_1,y)}$ followed by the horizontal line segment $\mathit{C_2}$ from $\mathit{(x_1,y)}$ to $\mathit{(x,y)}$ . Then $$ \mathit{f\,(x,y)} = \int_\mathit{C_1} \mathbf{\vec F} \cdot d\mathbf{\vec r}\, + \int_\mathit{C_2} \mathbf{\vec F} \cdot d\mathbf{\vec r} = \int_\mathit{(a,b)}^\mathit{(x_1,y)} \mathbf{\vec F} \cdot d\mathbf{\vec r}\, + \int_\mathit{C_2} \mathbf{\vec F} \cdot d\mathbf{\vec r}$$ Notice that the first of these integrals does not depend on $\mathit{x}$ , so $$ \frac{\partial}{\partial x}\;\mathit{f\,(x,y)} = 0 + \frac{\partial}{\partial x}\int_\mathit{C_2} \mathbf{\vec F} \cdot d\mathbf{\vec r}$$ If we write $\mathbf{\vec F} = \mathit{P}\,\mathbf{\vec i} + \mathit{Q}\,\mathbf{\vec j}$ , then $$ \int_\mathit{C_2} \mathbf{\vec F} \cdot d\mathbf{\vec r} = \int_\mathit{C_2} P\,dx + Q\,dy$$ On $\mathit{C_2}$ , $y$ is constant, so $dy = 0$ . Using $t$ as the parameter, where $x_1 \leqslant t \leqslant x$ , we have $$ \frac{\partial}{\partial x}\;\mathit{f\,(x,y)} = \frac{\partial}{\partial x} \int_\mathit{C_2} P\,dx + Q\,dy = \frac{\partial}{\partial x} \int_\mathit{x_1}^x P(t,y)\,dt = P(x,y)$$ by Part 1 of the Fundamental Theorem of Calculus."" A similar process of deriving $\frac{\partial}{\partial y} \;\mathit{f\,(x,y)}$ is used to show that $\mathbf{\vec F} = \mathit{P}\,\mathbf{\vec i} + \mathit{Q}\,\mathbf{\vec j} = \frac{\partial f}{\partial x}\;\mathbf{\vec i} + \frac{\partial f}{\partial y}\;\mathbf{\vec j} = \nabla f$ . I have two questions: How is $\int_\mathit{(a,b)}^\mathit{(x_1,y)} \mathbf{\vec F} \cdot d\mathbf{\vec r}$ not dependent on $x$ ? How does writing $\mathbf{\vec F} = \mathit{P}\,\mathbf{\vec i} + \mathit{Q}\,\mathbf{\vec j}$ show $ \int_\mathit{C_2} \mathbf{\vec F} \cdot d\mathbf{\vec r} = \int_\mathit{C_2} P\,dx + Q\,dy$ ? Thank you so much for reading through the long post, and apologies in advance for poor formatting. Any help is greatly appreciated.","['vector-fields', 'multivariable-calculus', 'line-integrals']"
4105940,"If $f(x)$ is a twice differentiable function such that $f(x)+f''(x)>2f'(x)$ and $f(0)=0$ , $f(1)=9$, is $f(x) \geq $ or $\leq 9x^{x-1}$?","Let $f(x)$ be a twice differentiable function such that $f(0)=0$ and $f(1)=9$ and $f(x)+f''(x)>2f'(x)$ $ ~\forall x ~\in [0,1]$ . Which of the following statements is true about $f(x)$ ? A) $f(x) \geq 9xe^{x-1}$ B) $f(x) \leq 9xe^{x-1}$ C) $f(x) \geq 27xe^{x-1}$ D) None of the above Attempt: I assumed $g(x)=f(x)-9xe^{x-1}$ so that $g(0)=g(1)=0$ . Then I found $g'(x)=f'(x)-9e^{x-1}-9xe^{x-1}$ and $g''(x)=f''(x)-18e^{x-1}-9xe^{x-1}$ . Now using $f(x)+f''(x)>2f'(x)$ , I obtained a similar relation for $g(x)$ as well , that is, $g(x)+g''(x)>2g'(x)$ . But this got me nowhere. I also assumed $h(x)=f(x)-f'(x)$ so that we have $h'(x)>h''(x)$ , but from here  too I am unable to comment on anything that would lead me to prove either $g(x)\geq $ or $\leq 0$ . Any help would be appreciated. A related question , but it has an equality sign instead of inequality in the differential inequation.","['calculus', 'functions', 'derivatives']"
4106109,Examples/classification of algebraic symplectomorphisms,"I'm curious about examples of algebraic automorphisms of complex varieties which are symplectomorphism. For instance, can we classify the algebraic symplectomorphisms of $\mathbb{P}_{\mathbb{C}}^n$ with regard to the Fubini-Study form? Or can we give large classes thereof for $\mathbb{P}_{\mathbb{C}}^n$ or other complex algebraic varieties? If understand a comment on this question correctly, there ought to be an ample supply....","['complex-geometry', 'algebraic-geometry', 'symplectic-geometry']"
4106115,"What algorithms exist, if any, for pseudo-random dice rolling with non-binominal aggregate properties?","I wasn't sure whether to address this to CS SE, StackOverflow or Math SE, but here goes... What algorithms exist, if any, for (pseudo-) random dice rolls such that some aggregate properties across many rolls are obeyed, for example, I want to get to roll a 3-sided die (faces 'A', 'B', 'C') such that, if rolled 100 times: the expected numbers of 'A', 'B', 'C' are 90, 8, 2 respectively; the number of 'A' rolls will be between 89 and 91 with probability 67% or some other similar such specification. I am still looking for algorithms that can provide a random(-looking) single roll; but over many rolls I do not want the cumulative results to follow a binomial distribution but rather one like I've specified. What should I look into for such pseudo-random rolling algorithms? (Pseudo-code, or actual code in say R or Python or Mathematica, would also be very appreciated) EDIT: I know how to satisfy property (1), it's (2) I'm interested in","['random', 'probability', 'algorithms']"
4106130,Why can $\lim_{x \to c} f(x)$ be found by simplifying $f(x)$?,"More formally, for $f(x)$ undefined at $x = c$ , why does $\lim_{x \to c} f(x) = g(c)$ , where $g(x)$ is an algebraic simplification of $f(x)$ ? To use an example of this simplification from the Wikepedia page about limits , $$
  \lim_{x \to 1} \frac{x^2 - 1}{x - 1}
= \lim_{x \to 1} \frac{(x+1)(x-1)}{x - 1}
= \lim_{x \to 1} (x + 1)
= 1 + 1 = 2.
$$ Here, the function $f(x) = (x^2 - 1)/(x - 1)$ is simplified to $g(x) = x + 1$ , which is continuous at $x = 1$ . $g(1)$ is then evaluated to find the limit of $f(x)$ . How do we know that this results in the correct limit for every case where $f(x)$ can be simplified to a function defined at $x = c$ ?","['limits', 'calculus']"
4106148,Solving this double integral,"I have the following double integral, and I am struggling to solve it. $$\int_{0}^{1} y dy \int_{y}^{1}\frac{dx}{x}\left[f(x)+g(x)\right]h \left(\frac{y}{x}\right)$$ Both $f(x)$ and $g(x)$ are known in their full analytic form. However, we do not have any information about the analytic form of the function $h\left(\frac{y}{x}\right)$ . Could someone give me a hint as to how I can go about solving this? Also, let me know if my question is missing any information.","['multivariable-calculus', 'multiple-integral', 'definite-integrals']"
4106151,How can i solve $\lim _{x\to +\infty } \left(\frac{3+x^{\alpha}}{2+x}\right)^{x}$ without de L'Hopital?,"I'm having some trouble with this limit: $$\lim _{x\to +\infty } \left(\frac{3+x^{\alpha}}{2+x}\right)^{x}$$ The parameter $\alpha \in \mathbb{R}$ . I've tried dividing the problem in three different cases: $\alpha = 1, \alpha < 1$ and $\alpha > 1$ . For the first two the calculation looks pretty easy, as I, first, rewrite both numerator and denominator as the well known limit $(\theta/x + 1)^x \to e^\theta$ : $$\frac{\left( 3 + x \right)^x}{\left( 2 + x \right)^x} = \frac{\left( 3/x + 1 \right)^x}{\left( 2/x + 1 \right)^x}\to \frac{e^3}{e^2} = e .$$ Then I see that for $\alpha > 1$ : $$\left(\frac{3}{x^\alpha}+1\right)^x = \left(\frac{3}{x^\alpha} + 1 \right)^{x^\alpha x^{1-\alpha}}\to e^{3x^{1-\alpha}}\to1$$ and $$\left(\frac{2}{x^\alpha}+x^{1-\alpha}\right)^x \to 0.$$ The $\alpha < 1$ case, instead, gives me the indeterminate form $\frac\infty\infty$ , which I'm unable to solve. This exercise is given in the ""known limits"" sections so I guess I can solve it without using de L'Hopital.","['limits', 'calculus', 'limits-without-lhopital']"
4106171,Finding $f$ s.t. the sequence of functions $f_n(x)=f (x − a_n )$ is not a.e. convergent to $f$,"The following is an exercise from Bruckner's Real Analysis : Let $a_n$ be any sequence of positive numbers converging to zero. If $f$ is continuous, then certainly $f (x − a_n)$ converges to $f (x)$ . Find a bounded measurable function on $[0, 1]$ such that the sequence of functions $f_n(x)=f (x − a_n )$ is not a.e. convergent to f [Hint: Take the characteristic function of a Cantor set of positive measure.] I don't understand how "" $f_n(x)=f (x − a_n )$ is not a.e. convergent to f"" can happen at all : $f (x − a_n )$ are 'moving' to become $f(x)$ as $n \to \infty$ and we only have a solution for not a.e. convergent to f when we consider any different $f$ that is not a.e. limit of $f_n$ ? So what is the use of fat Cantor sets here? Added - There should be an easy solution within the scope of Bruckner's book because when an exercise is hard and even still is at the book's level, it is always mentioned in a parenthesis ""This is hard"", but no notice for this exercise is.","['pointwise-convergence', 'measure-theory', 'measurable-functions']"
4106226,Divergence of the series $\dfrac{x^n}{ \sin ( \pi \alpha n) }$ for $0<x<1$,"$0<x<1$ $u_0 \in \mathbb{N}$ and $u_0 \geq 2$ $u_{n+1}= u_n ^{ u_n}$ $\alpha= \sum_{n=0}^{+ \infty} \dfrac{1}{u_n}$ We want to prove that the series $\sum \dfrac{x^n}{ \sin ( \pi \alpha n) }$ diverges. My attempt :
The sequence $(u_n)_n$ has been studied here $
\begin{align*}
\forall j, u_{j+1}&=u_j^{u_j} \in \mathbb{N} \\
\forall j, \forall k<j,  u_k &| u_j \\
\forall k, k <n, \dfrac{u_n}{u_k} &\in  \mathbb{N} \\
\forall n, u_n \sum_{k=0}^{n} \dfrac{1}{u_k} &\in \mathbb{N}
\end{align*}
$ $
\begin{align*}
\sin( u_n \pi \alpha) &= \sin( u_n \pi \sum_{k=0}^{n} \dfrac{1}{u_n} + u_n \pi \sum_{k=n+1}^{\infty} \dfrac{1}{u_n} ) \\
\sin( u_n \pi \alpha)&= \sin( u_n \pi \sum_{k=n+1}^{\infty} \dfrac{1}{u_n}     ) \\
 |\sin( u_n \pi \alpha)|  & \leq  | u_n \pi \sum_{k=n+1}^{\infty} \dfrac{1}{u_n}  |    \\
 |\sin( u_n \pi \alpha)|  & \leq   u_n \pi \sum_{k=n+1}^{\infty} \dfrac{1}{u_n}    \\
& \leq\dfrac{ \pi C }{  u_n^{u_n -1}  } \\
\end{align*}
$ $C$ is an intercept, that does not depend on $n$ . $$\dfrac{x^{u_n} }{ |\sin( u_n \pi \alpha)|    } \geq \dfrac{1}{ \pi C } x^{u_n} u_n^{u_n -1} (\star) $$ We expect to get the divergence from this last inequality called ( $\star$ ). $u_n \to \infty$ We can prove that $\alpha \notin \mathbb{Q}$ Assume that $\alpha= \dfrac{p}{q}$ where $p,q \in \mathbb{Q}$ then $
\begin{align*}
u_n \sum_{k=0}^{n} \dfrac{1}{u_k} &\in \mathbb{N} \\
\alpha=\sum_{k=0}^{\infty} \dfrac{1}{u_k} &\in \mathbb{N}\\
q u_n \sum_{k=n1}^{\infty} \dfrac{1}{u_k} &\in \mathbb{N} \\
q u_n \sum_{k=n1}^{\infty} \dfrac{1}{u_k} &\to 0
\end{align*}
$ It is a contradiction so $\alpha \not \in \mathbb{Q}$ We get the divergence from $\star$ because $u_n > n+1$","['trigonometry', 'sequences-and-series', 'real-analysis']"
4106248,What's the derivative with respect to a constant?,"Suppose I have $f(x) = 5x$ . I know that $\frac{d\ f(x)}{dx} = 5$ . But what is $\frac{d f(x)}{d 5}$ , the derivative of the function $f$ with respect to the constant 5? The reason I ask is that I'm implementing software that computes auto-differentiation (a la TensorFlow). I want to know if I can treat a constant like a variable (as above) or if I have to do something else. This Stanford deep learning class webpage is what I'm referring to: $$
f(x) = c+x \\
f_a(x) = ax
$$ Where the functions $f_c$ , $f_a$ translate the input by a constant of $c$ and scale the input by a constant of $a$ , respectively. These are technically special cases of addition and multiplication, but we introduce them as (new) unary gates here since we do not need the gradients for the constants $c$ , $a$ . That above statement implies that you could compute the derivative w.r.t. a constant, but they chose not to. This post did not answer my question: derivative with respect to constant. Thanks.","['calculus', 'derivatives']"
4106278,"Double Integral $\iint e^\frac{x+y}{x-y} \,dx \,dy$ solution","In the Multivariable Calculus Class I'm taking we were tasked to solve $$\iint_{R} e^\frac{x+y}{x-y} \,dx \,dy\,,$$ where $R$ is the trapezoidal region with vertices $(1,0)$ , $(2,0)$ , $(0,-1)$ and $(0,-2)$ as part of our optional Problem Set.
While I intuitively knew how to solve the other problems of the Set, I didn't with this one. Since integrating $e^\frac{x+y}{x-y}$ isn't that easy, I tried finding another way of solving this and found the method of Jacobians online (we haven't learned that yet).
I figured that we could maybe set $u=x+y$ and $v=x-y$ based on the exemplary problem I saw online.
Then I would arrive at $$x = \frac12(u+v)$$ $$y=\frac12(u-v)$$ This gives the Jacobian $$J(u,v)=\begin{vmatrix}
\frac12 & \frac12 \\
\frac12 & -\frac12
\end{vmatrix}=-\frac12.$$ Where do I go from here to find the region $D$ in the plane of $(u,v)$ which corresponds to $R$ . My question therefore: How do I find $D$ ? and Is there another way to solve this without knowledge of Jacobians? I would assume that the chance of us being given this problem as a brutal introduction to Jacobian, kind of like ""You all didn't know how to solve this and are very confused now? Let's show you a method how to solve it!"" is rather low.","['jacobian', 'multivariable-calculus', 'change-of-variable', 'multiple-integral']"
4106300,"Expected value, first-order dominance and super modularity","Suppose a function $f:X\times Y\to\mathbb{R}$ - with $X\subseteq\mathbb{R}$ and $Y\subseteq \mathbb{R}$ - satisfies increasing differences and is increasing in $X$ . Consider another function $g:\Delta(X) \times Y\to\mathbb{R}$ given by $g(\mu,y)=\mathbb{E}_{\mu}[f(x,y)]$ . That is, $g(\mu,y)$ is the expected value of $f(x,y)$ with respect to a measure $\mu$ , for a fixed $y$ . Endow $\Delta(X)$ with the first order stochastic dominance relation, so that $(\Delta(X), \succsim_{FSD})$ is a lattice. Question: can we say that $g(\mu,y)$ is super modular in $(\Delta(X), \succsim_{FSD})$ for every fixed y? Relevant definitions : $f$ satisfies increasing differences if $x'\ge x$ and $y'\ge y$ implies $f(x',y')-f(x',y)\ge f(x,y')-f(x,y)$ ; $\mu'\succsim_{FSD}\mu$ if $\mathbb{E}_{\mu'}[h]\ge\mathbb{E}_{\mu}[h]$ for every increasing, bounded function $h$ ; g is supermodular if $g(\mu\vee \mu',y)+g(\mu\wedge\mu',y)\ge g(\mu,y)+g(\mu',y)$ for any $\mu, \mu'$ and any $y$ .","['lattice-orders', 'stochastic-processes', 'monotone-functions', 'discrete-mathematics']"
4106364,Name of an inference rule that I can't find anywhere,"As a homework question, I must prove, using inference rules, that: p ∨ q (¬p ∧ q) → r ∴ ¬r → p I have done the following progress: p ∨ q (¬p ∧ q) → r ¬r → ¬(¬p ∧ q) ¬r → (¬(¬p) ∨ ¬q) ¬r → (p ∨ ¬q) 1 premisse 2 premisse 3 contrapositive from 2 4 morgan law from 3 5 double negation from 4 And I can't figure out how to finish it. I can easily prove the whole expression using a truth-table, but I simply can't find a formal way of merging 1 and 5. Any help would be hugely appreciated.","['logic', 'discrete-mathematics']"
4106384,Discontinuous function $f:\mathbb{R}\to \mathbb{R}$ that takes each of its values twice?,"I know it is impossible to create a continuous $f:\mathbb{R}\to \mathbb{R}$ that takes all of its values twice. I also know it is possible to create a continuous $f:\mathbb{R}\to \mathbb{R}$ that takes all of its values three times, but can someone give me a discontinuous function $f:\mathbb{R}\to \mathbb{R}$ that takes all of its values twice? Thank you.",['functions']
4106385,$k$-rational points of the automorphism functor of a scheme,"Let $X$ be a scheme and let $\operatorname{Aut}_X$ denote the functor sending a scheme $T$ to the set of $T$ -automorphisms of $X \times T$ .
Assume that $\operatorname{Aut}_X$ is representable by a group scheme. I will continue to denote this group scheme by $\operatorname{Aut}_X$ . I am reading ""Notes on automorphism groups of projective varieties"" by Michel Brion. In it he defines $\operatorname{Aut}(X):=\operatorname{Aut}_X(k)$ and then writes that $\operatorname{Aut}_X$ is algebraic if and only if $\operatorname{Aut}(X)$ is. The property of being algebraic is something assigned to a group scheme, but, as far as I understand, $\operatorname{Aut}_X(k)$ is just a group. Does $\operatorname{Aut}_X(k)$ also have a group scheme structure?","['group-schemes', 'algebraic-groups', 'algebraic-geometry', 'schemes']"
4106388,"When considering a finite-type scheme as a ringed space, is it enough to look at its $k$-points?","I am reading a set of notes by Michel Brion about automorphism groups of projective varieties. The following claim appears in the proof of a theorem stating that if G is a connected group scheme, $X$ a $G$ -scheme, and $\pi:X \to Y$ a proper morphism such that $\pi_*(\mathcal{O}_X)=\mathcal{O}_Y$ , then there is a unique $G$ -action on $Y$ such that $\pi$ is $G$ -equivariant. We will consider a finite-type scheme $Z$ , as a ringed space $(Z(k), \mathcal{O}_Z)$ , where the set $Z(k)$ is equipped with the Zariski topology, this makes sense as $Z$ is of finite type. What about considering $Z$ as a ringed space makes it ok to consider only its $k$ -points, and why is it necessary that $Z$ be of finite-type?","['ringed-spaces', 'algebraic-geometry', 'schemes']"
4106402,Did I just discover a new way to calculate the signature of a matrix?,"Due to the complains for more clarity down below I've cut my post into segments. Feel free to skip right to Definitions, Algorithm & Conjecture. If this is not clear enough, then I'm afraid I can't help it. Story I'm taking a course on linear algebra and recently we were covering congruence of matrices.
By Sylvester's law of inertia any two real symmetric matrices are congruent if they have the same signatures.
We were advised to calculate signatures by considering matrices as bilinear forms and finding their orthogonal bases, which from my point of view is extremely tedious and requires painstaking work, both with regard to memorization and computation.
So I was looking out for a better way to do this and by Googling I discovered that simultaneous row and column transformations preserve the signature, which turns out to be quite simple to understand once you consider elementary operations as matrices: $$ \boldsymbol{A'} = \boldsymbol{EAE}^T $$ That is way easier! However, as an extremely lazy person I still wasn't satisfied and here's where the fun part begins: I began looking at the elementary operations and how they affect the outcome. Multiplying a row by a negative constant might trivially change the signature (just consider identity matrix). After a while I found an example of how interchanging rows might also affect it. And adding a row multiplied by $-2$ to itself is equivalent to multiplying it by a negative scalar.
Thus I was left with adding to the row a different row multiplied by a constant and I couldn't find a counterexample for this one. More than that!
Using only this operation I got through my previous assignment and by turning a matrix into a row echelon form I was able to get a correct signature in every exercise. It also helped me spot a mistake in my simultaneous row and column operations on the recent test. By this method I calculated 11 correct signatures - it would be very odd if this was just an accident! I know that chances of me discovering something new in math are infinitesimal but I couldn't resist the clickbaity title. I hope you'll forgive me. But I'm genuinely curious about this one. I tried talking with my professor about it but he seemed uninterested, or maybe I did a poor job explaining it. He just dismissed the entire problem by saying that reduction to a row echelon form does not preserve the signature. Did I stumble upon some already known algorithm? Why then would no one talk about it at uni? I tried thinking about how to prove this but nothing comes to mind. Perhaps I miss some obvious counterexamples? If so, why did it work in all of the previous exercises? Definitions We use this definition of congruence and this definition of signature. Congruence :We say that two squrare matrices A and B over some field are congruent if there exists an invertible matrix P such that: $$\boldsymbol{A} = \boldsymbol{P}^T \boldsymbol{B P}$$ Signature : A real, nondegenerate $n\times n$ symmetric matrix A , and its corresponding symmetric bilinear form $\boldsymbol{G}(v,u) = v^T \boldsymbol{A} u$ , has signature $\boldsymbol{(p,q)}$ (or $\boldsymbol{p-q}$ in a different notation) if there is a nondegenerate matrix C such that $ \boldsymbol{CAC}^T $ is a diagonal matrix with p 1s and q (-1)s. Algorithm Using only this operation - adding a row multiplied by a constant to another row - get a matrix to its upper-triangular form. Let $p$ be a number of positive entries on the diagonal and $q$ be a number of negative ones. Signature of a matrix is equal to $(p,q)$ . Conjecture The aforementioned Algorithm provides a correct signature for all nondegenerate symmetric square matrices. Further questions Why would that be?
How to prove it? Any ideas for counterexamples?
Does it hold for $3 \times 3$ matrices and below but fails for bigger matrices, as suggested by Ben Grossmann in the comments? Any counterexamples of this sort?
In this case - why would it work for n = 3? Examples $$ \boldsymbol{A}=
\begin{bmatrix}
8 & 8 & 5\\
8 & 0 & 4\\
5 & 4 & 3
\end{bmatrix} 
\overset{r_2 \to r_2-r_1}{\longrightarrow}
\begin{bmatrix}
8 & 8 & 5\\
0 & -8 & -1\\
5 & 4 & 3
\end{bmatrix}
\overset{r_3 \to r_3-\frac{5}{8}r_1}{\longrightarrow}
\begin{bmatrix}
8 & 8 & 5\\
0 & -8 & -1\\
0 & -1 & -\frac{1}{8}
\end{bmatrix} 
\overset{r_3 \to r_3-\frac{1}{8}r_2}{\longrightarrow}
\begin{bmatrix}
8 & 8 & 5\\
0 & -8 & -1\\
0 & 0 & 0
\end{bmatrix} $$ And we already see that the signature is (1,1) . Let $x \in \mathbb{R}$ For which values of $x$ the signature of B equals $2$ ? $$ \boldsymbol{B} = \begin{bmatrix}
1 & 0 & 1\\
0 & 2 & 3\\
1 & 3 & x
\end{bmatrix} 
\overset{r_3 \to r_3-r_1}{\longrightarrow}
\begin{bmatrix}
1 & 0 & 1\\
0 & 2 & 3\\
0 & 3 & x-1
\end{bmatrix} 
\overset{r_3 \to r_3-\frac{3}{2}r_2}{\longrightarrow}
\begin{bmatrix}
1 & 0 & 1\\
0 & 2 & 3\\
0 & 0 & x-\frac{11}{2}
\end{bmatrix} $$ And the answer is for $x = \frac{11}{2}$ : $(2,0) = 2$ . Let $t,s \in \mathbb{R} $ $$ \boldsymbol{C}=
\begin{bmatrix}
0 & 0 & 0 & 0 & t^2\\
0 & -1 & 0 & 1 & 0\\
0 & 0 & 1 & s & 0\\
0 & 1 & s & s^2-1 & 0\\
t^2 & 0 & 0 & 0 & 0\\
\end{bmatrix}
\underset{r_5 \to r_5-r_1}{\overset{r_1 \to r_1+r_5}{\longrightarrow}}
\begin{bmatrix}
t^2 & 0 & 0 & 0 & t^2\\
0 & -1 & 0 & 1 & 0\\
0 & 0 & 1 & s & 0\\
0 & 1 & s & s^2-1 & 0\\
0 & 0 & 0 & 0 & -t^2\\
\end{bmatrix}
\underset{r_4 \to r_4-sr_3}{\overset{r_4 \to r_4+r_2}{\longrightarrow}}
\begin{bmatrix}
t^2 & 0 & 0 & 0 & t^2\\
0 & -1 & 0 & 1 & 0\\
0 & 0 & 1 & s & 0\\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & -t^2\\
\end{bmatrix} $$ Singature is (2,2) for $t\not= 0$ and (1,1) for $t = 0$ .
I achieved the same result with simultaneous row and column operations, it took twice as long.","['matrix-congruences', 'matrices', 'linear-algebra', 'symmetric-matrices', 'algebra-precalculus']"
4106466,Approximating Lipschitz Functions by Sigmoidal Functions,"We define a sigmoidal function $\sigma: \mathbb{R} \to [0,1]$ as a non-decreasing, continuous function with $\lim_{x\to\infty}\sigma(x) = 1$ and $\lim_{x\to-\infty}\sigma(x) = 0$ . I want to show that: If $g:\mathbb{R}\to \mathbb{R}$ is a Lipschitz on the interval [a,b],
and $\epsilon>0$ , then I can find $f\in \left\{\sum_{i=1}^{n}a_i
 \sigma(u_ix + b_i): a_i,u_i,b_i\in\mathbb{R}, n\geq 0,\right\}$ such
that $$\sup_{x\in[a,b]} \left\lvert f(x) - g(x) \right\rvert <
 \epsilon$$ In other words I want to show I can approximate $g$ with linear combinations of affinely shifted and scaled versions of $\sigma$ . I know for large enough $M>0$ , $\sigma(Mx)$ acts like the indicator function (i proved that it can approximate lipschitz functions), but I don't know how to go any further.","['approximation', 'functional-analysis', 'approximation-theory', 'real-analysis']"
4106467,"What kind of ""direct product"" is the $p$-adic solenoid?","I'm confused about this paragraph in https://en.wikipedia.org/wiki/P-adic_number : $p$ -adic integers can be extended to $p$ -adic solenoids $\mathbb{T}_{p}$ in the same way that integers can be extended to the real numbers, as the direct product of the circle ring $\mathbb{T}$ and the $p$ -adic integers $\mathbb{Z}_{p}$ . I don't think the real numbers are the kind of topological product (or the kind of group product) that I'm used to of the integers and the circle, and neither is the $p$ -adic solenoid the usual kind of product of the $p$ -adic integers and the circle. Is there a more specific name for this kind of product, and what is it? Only looking at the topological structure, the only relationship I can see is this one: There's a map $\pi_{\mathbb R} : \mathbb R \to \mathbb T$ such that for each point $x \in \mathbb T$ , $\pi_{\mathbb R}^{-1}(x)$ is homeomorphic to $\mathbb Z$ and furthermore there is some open neighborhood $O$ of $x$ such that $\pi_{\mathbb R}^{-1}(O)$ is homeomorphic to $O \times \mathbb Z$ . There's a map $\pi_{\mathbb T_p} : \mathbb T_p \to \mathbb T$ such that for each point $x \in \mathbb T$ , $\pi_{\mathbb T_p}^{-1}(x)$ is homeomorphic to $\mathbb Z_p$ and furthermore there is some open neighborhood $O$ of $x$ such that $\pi_{\mathbb T_p}^{-1}(O)$ is homeomorphic to $O \times \mathbb Z_p$ . But I don't know anything about what this kind of topological product is called, nor how to define this kind of product for topological groups or rings.","['general-topology', 'p-adic-number-theory', 'topological-groups']"
4106497,If two varieties are isomorphic so is their blow-up.,"I'm following Gathmann notes on Algebraic Geometry. In Problem 10.17, we are to prove the affine curves $X_k = V(x_2^2 - x_1^{2k+1})\subset \mathbb{A}^2_{\mathbb{C}}$ are not isomorphic for different $k$ . The last step in the proof I'm struggling with is showing that if $X_k$ is isomorphic to $X_l$ , then $\widetilde{X_k}$ is isomorphic to $\widetilde{X_l}$ . These are their strict transforms in the blow-up of $\mathbb{A}^2$ at the origin (this part is necessary to find a contradiction, after blowing these up $l$ times). I use the affine Jacobi criterion to find that the only singular point in both is the origin. This shows a supposed isomorphism $f: X_k\to X_l$ maps origin to origin. This already shows that there's a natural mapping $\widetilde{X_k}\setminus \pi^{-1}(0)\to \widetilde{X_l}\setminus \pi^{-1}(0)$ where $\pi:\widetilde{\mathbb{A}^2}\to \mathbb{A}^2$ is the blow-up, since $\widetilde{X}\setminus\pi^{-1}(0)\cong X\setminus \{0\}$ . I'm just not sure how we can finish this isomorphism on the exceptional set part.","['algebraic-curves', 'algebraic-geometry', 'blowup']"
4106562,Induced volume form to any regular submanifold of $\mathbb{R}^n$,"I have a doubt that I have not seen answered/reflected anywhere, and is that: suppose that $M$ is a $k$ -dimensional oriented regular submanifold of $\mathbb{R}^n$ . If $k=n-1$ then there is a induced volume form (induced from the canonical Euclidean metric) what is given by $i_N\omega$ , where $i_N$ is interior multiplication of the volume form $\omega$ in $\mathbb{R}^n$ by the unit normal outward-pointing vector field to $M$ . However if $k<n-1$ then $\dim T^\bot M>1$ , so if I apply the interior multiplication various times using a basis of $T^\bot M$ I get a volume form to $M$ . So this would be a way to give a canonically induced volume form to any regular and oriented submanifold of $\mathbb{R}^n$ . Is this approach correct, or is something else needed to make sense? If its correct, it is unique? I mean, as the unique way to define a volume form induced from the Euclidean metric?","['volume', 'differential-geometry']"
4106602,Arranging numbers 1 to 1000 such that the difference of two adjacent numbers is not a square nor a prime number,"I've been working on the following problem for a while:
Prove that it's possible to arrange numbers 1 to 1000 an order such that each number appears once and | $x_j - x_{j+1}$ | is not a perfect square nor a prime number. The idea is just to prove that such an ordering exists, not to explicitly construct it (thankfully). My first thought was to try to construct an explicit ordering of 1 to 10 that satisfies the given constraints and then see if I could extrapolate a pattern. Unfortunately, I wasn't able to do this (5 minus any other number in that sequence gives either a prime or a perfect square, I believe...) I found  online that there are 168 primes and 31 perfect squares between 1 and 1000, and this seems like potentially useful information. However, I'm still not able to  connect the dots and figure out how to think about this problem ... Any help would be much appreciated.","['graph-theory', 'combinatorics']"
4106730,Does $\lim_n v_n(f) = v(f)$ for $v\in T_pM$,"Let $v \in T_pM$ for some smooth manifold,assume $v_n \in T_pM$ also. Assume further that $\lim_n v_n = v$ . Do we have the identity that $$v_n(f) \to v(f)$$ for $f\in C^\infty(M)$ (I was tring to provd the relation  which comes from Lee's smooth manifold book page 231: $$\left.\frac{d}{d s}\right|_{s=0} d\left(\theta_{-t_{0}}\right) \circ d\left(\theta_{-s}\right)\left(W_{\theta_{s}}\left(\theta_{t_{0}}(p)\right)\right.
=d\left(\theta_{-t_{0}}\right)\left(\left.\frac{d}{d s}\right|_{s=0} d\left(\theta_{-s}\right)\left(W_{\theta_{s}}\left(\theta_{t_{0}}(p)\right)\right)\right)$$ Where $W$ is a vector field over M. that is same to prove(notation may differ but the idea is the same): $$\frac{d}{ds} d\theta_{t_0} (v(s)) = d\theta_{t_0}(\frac{d}{ds} v(s)) \tag{*}$$ I try to do it as follows (using definiton of differential) denote $\Delta v(s) = \frac{v(s) - v(0)}{s}$ write everything in coordinate: $$\lim_s d\theta_{t_0} (\Delta v(s)) = \lim_{s\to 0}\sum d\theta_{t_0} (\Delta v(s))(x^i) \partial_i = \sum \lim_{s\to 0}d\theta_{t_0} (\Delta v(s))(x^i) \partial_i $$ Then $$\sum \lim_{s\to 0}d\theta_{t_0} (\Delta v(s))(x^i) \partial_i = \sum \lim_{s\to 0}[(\Delta v(s))(x^i\circ\theta_{t_0})] \partial_i$$ If we can show $$\lim [\Delta v(s) (f)] = (\lim \Delta v(s)) f$$ then we are done since $$ \sum \lim_{s\to 0}[(\Delta v(s))(x^i\circ\theta_{t_0})] \partial_i =  \sum d\theta_{t_0}[(\lim_{s\to 0})(\Delta v(s))](x^i) \partial_i =\sum d\theta_{t_0}(\frac{d}{ds} v(s))(x^i) \partial_i = RHS$$","['lie-derivative', 'smooth-manifolds', 'differential-geometry']"
4106841,Problem on Graph Coloring,"A planar map is called non-degenerate if all vertices have degree $3$ that is borders of only $3$ countries meet at a point. Suppose that in a non-degenerate planar map, all faces have an even number of edges (or all countries have an even number of neighbours). Show that such a map can
be coloured in 3 colours.","['graph-theory', 'proof-writing', 'coloring', 'discrete-mathematics']"
4106877,Find a closed-form solution to the following summation,"I am solving a summation that appears in a paper, it claims that $$\sum_{n=1}^{\infty} \binom{2n}{n+k}z^n=\bigg(\frac{4z}{(1+\sqrt{1-4z})^2}\bigg)^k$$ I found this identity here in equation (66) $$\sum_{n=0}^{\infty} \binom{2n+k}{n}z^n=1+\sum_{n=1}^{\infty} \binom{2n+k}{n}z^n=1+\sum_{n=1}^{\infty} \binom{2n+k}{n+k}z^n =\bigg(\frac{2}{1+\sqrt{1-4z}}\bigg)^k\frac{1}{\sqrt{1-4z}}$$ I didn't found the paper that has the proof of the identity above, but I notice that $$\binom{2n}{n+k}+\sum_{j=0}^{k-1} \binom{2n+j}{n+k-1}=\binom{2n+k}{n+k}$$ Therefore we obtain $$\sum_{n=1}^{\infty} \binom{2n}{n+k}z^n=\bigg(\frac{2}{1+\sqrt{1-4z}}\bigg)^k\frac{1}{\sqrt{1-4z}}-1-\sum_{n=1}^{\infty} \sum_{j=0}^{k-1} \binom{2n+j}{n+k-1}z^n$$ Yet I have no idea for the last summation above, any help would be appreciated.","['summation', 'calculus', 'binomial-coefficients', 'sequences-and-series', 'hypergeometric-function']"
4106933,Prove $\frac{\sin x}{x^2}$ is uniformly continuous at $N(0;r)^c$ for any $r >0$,"I tried $\left \vert \frac{\sin x}{x^2} -  \frac{\sin c}{c^2}\right \vert \leq \frac{1}{x^2} + \frac{1}{c^2} < \epsilon$ , but it doesn't help me much with $\vert x - c \vert < \delta$ . How can I prove this?","['uniform-continuity', 'analysis']"
4106935,Prove that $\arctan x$ cannot be expressed as a rational function,"In my Calculus class the teacher proposed as an exercise to prove that $\arctan(x)$ cannot be expressed as a rational function (fraction of polynomials) in any closed interval $[a,b]$ . I've been thinking on the problem and I haven't been able to prove it by ""analysis"" arguments but using some concepts of an abstract algebra course I took last semester. I've done it this way: Suppose there exist $p(x),q(x)$ such that $\arctan(x) = \frac{p(x)}{q(x)}$ in the interval $[a,b]$ , where $\gcd(p,q)=1$ . Thus, $\left(\frac{p(x)}{q(x)}\right)' = \frac{1}{1+x^2}$ . If we expand the expression of the derivative of a quotient and manipulate the equality, we end up with: $$(1+x^2)(p'(x)q(x)-p(x)q'(x))=q^2(x)$$ Thus, $1+x^2$ divides $q^2(x)$ and as $1+x^2$ is irreducible over $\mathbb R[x]$ , $1+x^2$ divides $q(x)$ . Let $q(x)$ have the prime factor $1+x^2$ ""n"" times. From the equation above, we know that $q^2(x)$ has the factor $(1+x^2)$ ""2n"" times; hence $(p'(x)q(x)-p(x)q'(x))$ has the factor $1+x^2$ exactly ""2n-1"" times. Note that for $n\geq 1$ , $2n-1 \geq n$ . Hence, the factor $(1+x^2)^n$ must divide $(p'(x)q(x)-p(x)q'(x))$ and it also divides $q(x)$ ; thus $(1+x^2)^n$ must divide $p(x)q'(x)$ . Suppose that $1+x^2$ is not a factor of $p(x)$ . Thus, $(1+x^2)^n$ must divide $q'(x)$ . However, note that: $$ q'(x) = ((x^2+1)^n \cdot r(x))' = 2nx(x^2 + 1)^{n-1} r(x) + (x^2+1)^n r'(x),\  \gcd(r,1+x^2)=1 $$ Therefore, $(x^2+1)^n$ divides $q'(x)$ if and only if $(x^2+1)^n$ divides $n(x^2+1)^{n-1} r(x)$ . Then, $x^2 + 1$ must divide $r(x)$ which is a contradiction. Hence, $x^2 +1$ must also be a factor of $p(x)$ . However, by hypothesis $\gcd(p,q)=1$ while $x^2+1$ divides both $p,q$ . This is a contradiction, hence there do not exist such polynomials. First of all, I would appreciate if anyone could tell me if this proof has any error. Moreover, I would appreciate any other approach which uses theorems of elementary one-variable calculus instead of divisibility. NOTE: In the case of $\mathbb R$ , this problem is (almost) trivial. As the arctangent has a horizontal asymptote in both $\pm \infty$ , it follows that if $\arctan(x) = \frac{p(x)}{q(x)}$ for certain $p,q$ , they must verify that $deg(p) = deg (q)$ . Moreover, $x=0$ is a unique real root of $p(x)$ with multiplicity 1 as $\arctan(0) = p(0)/q(0) = 0$ and $\arctan(0)' = 1/(1+0^2) = 1 \not = 0$ . Thus, $deg(p)$ must be odd. However, all the roots of $q(x)$ are complex as $\arctan(x)$ must be continuous on $\mathbb R$ . As complex roots on a real polynomial come in conjugate pairs, it follows that $deg(q)$ must be even. Then $deg(p) \not = deg(q)$ which is a contradiction. Conclusions: I have already proposed a solution to my Calculus professor based on some arguments given in this page and adding some details: Suppose there exist $p,q$ such that $\arctan(x) = p(x)/q(x)$ and $gcd(p,q)=1$ in the interval $[a,b]$ . Therefore, they must have the same derivative, hence: $$ (1+x^2)(p'q-pq') = q^2, \forall x \in [a,b]$$ As an equality of polynomials in an interval $[a,b], a<b$ must hold also in $\mathbb R$ , then if $q(x) /not = 0$ , the equality above holds. We are going to prove that q(x) has no real roots hence $p(x)/q(x)$ is continuous in $\mathbb R$ . Suppose $\alpha \in \mathbb R$ is a root of $q$ . As there are finitely many roots of $q(x)$ as is a non-zero polynomial, there exists $\delta > 0$ such that for any point in $[\alpha - \delta,\alpha + \delta]$ except for $x= \alpha$ , the equality $(\arctan x)' - (\frac{p}{q})' = 0$ holds. Thus, there exists a constant $c \in \mathbb R$ such that $\frac{p}{q} = \arctan + c$ . Thus, $$ \lim_{x \to \alpha} \frac{p}{q} = \lim_{x \to \alpha} (\arctan (x) + c) = arctan(\alpha) + c $$ As arctan is a continuous function. Thus, $\alpha$ must be a root also of $p(x)$ (if not, $\lim_{x \to \alpha} \frac{p}{q} = \infty$ ). But as we suppose that $p,q$ are prime polynomials, this is a contradiction. Hence, there cannot exists any root of q(x). Hence, $p(x)/q(x)$ is continuous in $\mathbb R$ and the equality $(\arctan x)' = (\frac{p}{q})'$ holds in $\mathbb R$ . Therefore, in $\mathbb R$ , $\frac{p}{q} = \arctan x + c$ and in $[a,b]$ , $\arctan = \frac{p}{q}$ . Thus $c = 0$ , and we get that $\forall x \in \mathbb, \arctan = \frac{p}{q}$ . But we have already proved that this is a contradiction, as desired. Further Questions: The same procedure can be used to prove that $log x$ cannot be expressed as a rational function. Moreover, I have come up with two questions I would try to solve: Can the argument applied here be generalised to an arbitrary function under certain hypothesis ( $f$ continuous and $f'$ a rational function, for example)? Can we find a similar argument to prove that $\sin x$ and $\cos x$ cannot be expressed in any interval as a rational function? I would try to publish as soon as possible the conclusions of these questions.","['indefinite-integrals', 'calculus']"
4107032,Combinatorial proof of $\sum_{j=0}^{k} \binom{n}{j} = \sum_{j=0}^k \binom{n-1-j}{k-j}2^j$,"Give a combinatorial proof for this identity for non-negative integer $k$ and $n$ such that $0 \leq k < n$ $$\sum_{j=0}^{k} \binom{n}{j} = \sum_{j=0}^k \binom{n-1-j}{k-j}2^j$$ My attempt: I tried to reduce this identity.
Since $k,n$ are nonnegative integer and $k<n$ then I can change the upper limit into $$\sum_{j=0}^{n-1} \binom{n}{j} =2^n-1 \quad \text{and} \quad \sum_{j=0}^{n-1} \binom{n-1-j}{n-1-j}2^j = \sum_{j=0}^{n-1} 2^j ~.$$ So, I just need to show $$\sum_{j=0}^{n-1} 2^j =2^n-1 $$ using combinatorial proof.
Am I right? I really need helps","['combinatorics', 'combinatorial-proofs']"
4107046,"Ways of choosing $16$ integers from first $150$ integers such that there is no $(a,b,c,d)$ for which $a+b=c+d$","Here is a problem I found out recently: In how many ways one can choose $16$ distinct positive integers from first $150$ positive integers such that there are no $4$ distinct ones $(a,b,c,d)$ for which $a+b=c+d$ ? My approach: If $a+b=c+d$ , then $a-c=d-b$ . There are $149$ differences from the first $150$ positive integers. We have to choose $\binom{16}{2}=120$ differences. So, the answer is $\binom{149}{120}$ . I'm confused with my approach. Edit: My solution is not correct . So, what is the correct solution to the problem? And I noticed that it is hard to find out such $16$ integers. So, if choosing $16$ such numbers is not possible , how to prove that? Any helpful approach is welcome. Source : The problem is self-made and inspired from a problem from the book 102 Combinatorial Problems: From The Training of The USA IMO Team by Titu Andreescu and Zuming Feng .","['contest-math', 'recreational-mathematics', 'combinatorics', 'sequences-and-series']"
4107064,Necessary and sufficient conditions for a function of a specific type on the complement of the Cantor set,"The following is an exercise from Bruckner's Real Analysis: For (a) right limit and left limit must be equal and equals $f(c_n)$ I think this is necessary and also sufficient. For (b) because each $f_n(x)$ must be continuous be definition so the limit is of type mentioned in (a). For (c) $\sum_1^∞ |f(c_n)| < ∞ $ is sufficient and necessary as for all $a_n$ , $b_n$ we have $f(a_n)=f(b_n)=0$ . Is my explanation rigorous enough? A detailed explanation for all parts of the question would be much appreciated.","['measure-theory', 'cantor-set', 'continuity', 'functions', 'solution-verification']"
4107118,A tensor product vs the tensor product.,"I have been reading about tensor products in different books and struggle with the definition of tensor products in the following sense. There is a common construction of a tensor product as a quotient group. One often uses this construction to prove the existence of a tensor product an then denotes the quotient group with $M \otimes_R N$ (assuming the construction was done for $R$ -modules $M,N$ ) and denotes its elements with $m \otimes n$ . The question arises whether this tensor product is unique or not and one usually proves that this is unique up to unique isomorphism. This then leads to several authors saying that it is reasonable to speak of the tensor product of $M$ and $N$ . My questions are the following. $(1)$ Why is this a reason to speak of the tensor product? Can't this cause any harm in whatever sense possible (for example lead to contradictions)? $(2)$ Let $M \otimes_R N$ denote another tensor product of $M$ and $N$ , then what would the elements $m \otimes n$ be? Would they be the image of $m \otimes n$ (in the quotient group construction) under the unique isomorphism? $(3)$ Would it be a possible approach to call the construction of the quotient group the tensor product of $M$ and $N$ and prove all properties with this definition? One could then call any other object satisfying the universal property a tensor product of $M$ and $N$ and that each of those objects is isomorphic (uniquely) to $M \otimes_R N$ (the quotient group construction). It should be the case that the proven properties then also hold for the other tensor products since there exists the unique isomorphism. If this would not cause any ""harm"" this would for now be the more elegant way to work with the definition for me, since this works around the non uniqueness of the tensor product. Edit for clarification: What I mean with ""possible"" in $(3)$ isn't the technical or proof level but rather if proceeding this way would have any downsides or ""harm"" in any way. Edit 2: I read lecture notes, Dummit & Foote and Atiyah Macdonald (perhaps some more, but I can't really remember those). As far as I can tell now, Atiyah actually defines the tensor product as precisely the quotient group, if I am not mistaken. This should then answer my 3rd question. It would be nice, however, to get confirmation by someone that I am not misunderstanding this.","['abstract-algebra', 'tensor-products', 'modules']"
4107129,What is the interpretation of dy/dx in parametric equations and why is it different from the velocity?,"So I know normally that dy/dx is equal to the velocity of a particle at a specific point if the original equation indicates the position of that particle. When dealing with parametric equations, I know velocity is equal to <dx/dt, dy/dt>. But it made sense to me that dividing dy/dt over dx/dt, giving dy/dx, would mean the same thing. Of course, it isn't, but I don't understand why it doesn't work and what the actual interpretation of dy/dx is for parametric equations.","['calculus', 'derivatives', 'parametric']"
4107232,"Integers less than $7000$ achievable by starting with $x=0$ and applying $x\to\lceil x^2/2\rceil$, $x\to\lfloor x/3\rfloor$, $x\to9x+2$","Problem Robert is playing a game with numbers. If he has the number $x$ , then in the next move, he can do one of the following: Replace $x$ by $\lceil{\frac{x^2}{2}}\rceil$ Replace $x$ by $\lfloor{\frac{x}{3}}\rfloor$ Replace $x$ by $9x+2$ He starts with the number $0$ . How many integers less than or equal to $7000$ can he achieve using the above functions? [It is permitted to use a number greater than $7000$ in the way of achieving the desired numbers.] My Approach Call the functions $f_1,f_2,f_3$ respectively. $2$ is easily achievable from $0$ (using $f_3$ ). I've found that all the integers from $0$ to $10$ are achievable (Though we achieve them in a long way). The numbers get messy when we get ahead further. I can't prove that any number is unachievable. I've noticed that base- $3$ numbers can help for $f_2$ and $f_3$ . How to get ahead further? Update: Mr. Mike showed that all integers are achievable by this process through codes. Mr. Calvin also gave a partial proof for that. So, a complete proof is needed currently.","['contest-math', 'number-theory', 'functions', 'combinatorics', 'recreational-mathematics']"
4107243,"$L^p$-convergence of $E[X|\mathcal{F}_k],p \in ]0,1[ \cup \{\infty\}$","Let $p \in ]0,\infty],X \in L^p, (\mathcal{F}_k)_k$ a filtration, define $\mathcal{F}_{\infty}=\sigma(\bigcup_{k \in \mathbb{N}}\mathcal{F}_k).$ If $p \in [1,\infty[,$ this implies that $X \in L^1,$ so $E[X|\mathcal{F}_k]$ converges a.s and in $L^1,$ and since $|E[X|\mathcal{F}_k]-E[X|\mathcal{F}_{\infty}]|^p \leq 2^p (E[|X|^p|\mathcal{F}_k]+E[|X|^p| \mathcal{F}_{\infty}]).$ $E[|X|^p|\mathcal{F}_k]+E[|X|^p| \mathcal{F}_{\infty}]$ converges a.s to $2 E[|X|^p|\mathcal{F}_{\infty}],$ and $E[E[|X|^p|\mathcal{F}_k]+E[|X|^p| \mathcal{F}_{\infty}]]=2E[E[|X|^p|\mathcal{F}_{\infty}]],$ which means that $E[X|\mathcal{F}_k]           $ converges to $E[X|\mathcal{F}_{\infty}]$ in $L^p$ (a version of the dominated convergence theorem, another way is to use the uniform integrability). It seems we also have convergence in $L^{\infty}$ (mentioned in a book on stochastic processes), Why is this true ? Of course it's easy to see that $||E[X|\mathcal{F}_k]-E[X|\mathcal{F}_{\infty}]||_{\infty} \leq ||E[X|\mathcal{F}_k]-X||_{\infty},$ do not know if it's helpful. The $L^{\infty}$ -convergence question is taken from probability and stochastics: Does a.s convergence, $L^p$ convergence of $E[X|\mathcal{F}_k]$ hold for $p<1$ ?","['measure-theory', 'stochastic-analysis', 'stochastic-processes', 'martingales', 'probability-theory']"
4107269,Paths transversal to countable collection of submanifolds,"I am interested in the following claim, which is a bit counter-intuitive to me. I am wondering whether it is correct. Claim. Let $Y$ be an open (path-)connected subset of $\mathbb{R}^d$ , and let $\{Z_i\}_{i\in I}$ be a countable family of closed submanifolds of $Y$ . Then, for any path $\gamma:[0,1]\to Y$ there exists a smooth path $\gamma':[0,1]\to Y$ such that (1) $\gamma$ and $\gamma'$ are path-homotopic and (2) $\gamma'$ is transversal to $Z_i$ for every $i\in I$ . For the case of a single submanifold $Z_i$ , the question has an answer here: Tranversal paths between two points There, a user refers to the following result in Guillemin and Pollack's Differential Topology (p.73), which immediately implies the claim for a singleton $Z_i$ . Corollary. If, for $f:X\to Y$ , the boundary map $\partial f:\partial X\to Y$ is transversal to $Z$ , then there exists a map $g:X\to Y$ homotopic to $f$ such that $\partial g=\partial f$ and $g \pitchfork Z$ . My (very rough) understanding is as follows: The corollary follows from the Transversality Homotopy Extension Theorem (p.72), which is proven from the Transversality Theorem. The constructed homotopy in the Extension Theorem is independent of the particular submanifold $Z$ , thus all proofs should still go through with a countable family because a countable intersection of full measure subsets (of the unit ball ' $S$ ') has full measure. Is this reasoning correct?","['differential-topology', 'transversality', 'differential-geometry']"
4107294,Function with many values in a sequence,"Cool problem I was reading but couldn't solve! Problem: Let $f(n)$ be a function satisfying the following conditions: a) $f(1) = 1$ . b) $f(a) \leq f(b)$ where $a$ and $b$ are positive integers with $a \leq b$ . c) $f(2a) = f(a) + 1$ for all positive integers a. Let $M$ denote the number of possible values of the $2020$ -tuple $(f(1), f(2), f(3), ..., f(2020))$ can take. Find $M ($ mod $1000)$ . $\\$ My solution (incomplete): I started listing a few values of $f(n)$ , such as $f(1) = 1, f(2) = 2, f(3) = 2, 3, f(4) = 3, $ etc. When $n$ is not a power of $2$ , $f(n)$ has many values. For example, $f(3) = 2, 3$ and $f(5), f(6), f(7) = 3, 4.$ Using condition b, there are $2$ ways to select a value for $f(3); 2, 3$ . $f(5), f(6), f(7)$ has $4$ ways to select values for those $3$ ; $(3, 3, 3), (3, 3, 4), (3, 4, 4), (4, 4, 4)$ respectively for $f(5), f(6), f(7).$ For the next ""group""; $f(9), f(10), ..., f(15)$ , there are $8$ ways. This process keeps happening for when a group starts with $f(n)$ such that $n = 2^x + 1$ for some positive integer $x$ and the group ends with $f(n) = 2^{x+1} - 1$ , there are $2^x$ ways to assign values to that group. However, from here, I don't know how to calculate how many total possible values the $2020$ -tuple can take. Do I add or multiply the values in these groups because they are independent events? Please help. Thanks in advance to those who help. By the way, the correct answer is $\boxed{502}$ but I don't know how to get this.",['functions']
4107304,What does $[n=1]$ mean?,"Studying recurrence relations I stumbled upon this expression in a solution to the problem of finding a closed formula to this: $a_n = 5a_{n-1} - 6a_{n-2}; a_0 = 0, a_1 = 1$ To start the solution, the person made the relation valid to all $n$ , introducing the strange term: $a_n = 5a_{n-1} - 6a_{n-2} + [n = 1]$ What does $[n=1]$ mean? Does it add one in the case that $n = 1$ ?","['recurrence-relations', 'discrete-mathematics']"
4107323,Absolute continuity of $g(x) = \int_{-\infty}^{\infty} \frac{\cos(xy^3)}{1+y^2}dy$.,"I'm trying to determine whether the function $$g(x) = \int_{-\infty}^{\infty} \frac{\cos(xy^3)}{1+y^2} dy$$ (a) Is continuous, (b) Is uniformly continuous, (c) Is absolutely continuous, in $(-\infty, \infty)$ . Would $h(x) = \cos(xy^3)$ be absolutely continuous? If so, would it not be enough to write $\cos(xy^3)$ as an indefinite integral since we know that a function is absolutely continuous if and only if it can be written as an indefinite integral? This approach seems to be too easy, which makes me think that there is something that I'm not quite understanding. Any hints would be appreciated.","['absolute-continuity', 'measure-theory', 'lebesgue-integral']"
4107340,Ask linking the Poisson and gamma families,"Let $X_1,..,X_n$ be a random sample from a Poisson population with parameter $\lambda$ and define $Y=\Sigma X_i$ . Y is sufficient for $\lambda$ and $Y \sim Poisson(n\lambda)$ . Now $Y=y_0$ is observed, one equation is $$\Sigma_{k=0}^{y_0} e^{-n\lambda} \frac{(n\lambda)^k}{k!}=\frac{\alpha}{2}$$ Recall the identity linking the Poisson and gamma families: If X is a gamma( $\alpha, \beta$ ) random variable, where $\alpha$ is an integer, then for any x, P(X $\le$ x)=P(Y $\ge \alpha$ ), where $Y \sim Poisson(x/\beta)$ . We can write (remembering that $y_0$ is the observed value of Y): $$\frac{\alpha}{2}=\Sigma_{k=0}^{y_0} e^{-n\lambda} \frac{(n\lambda)^k}{k!}=
P(Y \le y_0|\lambda)=
P(\chi^2_{2(y_0+1)}>2n\lambda)$$ . I didn't get the above equality. My attempt is: $$\frac{\alpha}{2}=\Sigma_{k=0}^{y_0} e^{-n\lambda} \frac{(n\lambda)^k}{k!}=
P(Y \le y_0|\lambda)=1-P(Y \ge y_0+1|\lambda)=
1-P(X \le x)
=P(X>x)$$ , where x is gamma( $y_0+1,\frac{x}{n\lambda}$ ). Because I think $x/\beta = n\lambda$ . I don't know how to do next. And I don't know my attempt is correct or not. I also know the identity that $\chi^2_v=gamma(v/2,2)$ Background of this question:","['statistics', 'probability-distributions', 'probability']"
4107488,"If $N$ is nilpotent, then $N \sim N^2 \Longleftrightarrow N=0$","Given that $N$ is nilpotent, then $N$ is similar to $N^2$ if and only if $N=0$ ( $N$ is the zero matrix).  The "" $\Longleftarrow$ "" direction is easy enough, because $N = 0 = N^2$ so they're equal and trivially similar. But showing $N \text{ nilpotent and } N \sim N^2\Longrightarrow N=0$ is turning out to be more difficult.  I'm comfortable using well-known facts about $n \times n$ nilpotent matrices such as: \begin{align}
N \text{ nilpotent } &\Longleftrightarrow \text {all eigenvalues }=0\\
&\Longleftrightarrow p_N = \lambda^n\\
&\Longleftrightarrow m_N =\lambda^k\,,\, \text{ for some k, }1\leq k \leq n\\
&\Longleftrightarrow {\rm tr}N^q = 0\,,\, \text{ where }q\in \mathbb{N}\text{, minimal, s.t. } N^q=0
\end{align} Note that $p_N$ above is the characteristic and $m_N$ is the minimal polynomial.  My hunch is to use that trace equals 0 for some power $q$ and the fact that each entry $c_{ij}$ of $N^2$ is \begin{align}
c_{ij} = \sum_{k=1}^n n_{ik} n_{kj}
\end{align} to get some cancellations but I can't quite see it yet.  Any thoughts?","['matrices', 'nilpotence', 'linear-algebra']"
4107501,Finding the Radon-Nikodym Derivative for Given Measures $\mu$ and $\nu$,"I am currently reading about the Radon-Nikodym derivative and came across a problem in my textbook the author attempts to work through. The problem is as follows: Given $(\Omega, \mathcal{F})$ , Let $\Omega = [0,1]$ with Lebesgue measure $m$ and consider measures $\mu, \nu$ given by densites $\chi_{A}$ , $\chi_{B}$ respectively. Find a condition on the sets $A,B$ so that $\mu$ dominates $\nu$ and find the Radon-Nikodym derivative $\frac{d\nu}{d\mu}.$ While this question is pretty straightforward, I have a few questions about his work: First assume that $m(A) \neq 0$ . Then $B \subset A$ clearly implies that $\mu$ dominates $\nu$ . Now consider the partition $\mathcal{P} := \{B, A \setminus B, \Omega \setminus A\}$ of $\Omega$ . Therefore for a set $F \in \mathcal{F}$ , $$\nu(F)=m(F \cap B) =\int_{F \cap B}\chi_{B}dm=\int_{F \cap B}\chi_{B}d\mu.$$ My Questions: In the first line: I don't think I understand why we are employing the Lebesgue measure to show that $\mu$ dominates $\nu$ , Could someone explain this in more detail since it is not as clear to me as the author makes it out to be? My definition of a measure dominating another is the following: $\mu$ dominates $\nu$ $\iff$ $0\leq \nu(F) \leq \mu(F)$ $\forall F \in \mathcal{F}$ . I understand why we chose the partition $\mathcal{P} := \{B, A \setminus B, \Omega \setminus A\}$ , however, i'm not following why $\nu(F) = m(F \cap B)$ and how we conclude this equals $\int_{F \cap B}\chi_{B}d\mu$ ? Does this maybe have something to do with the Lebesgue decomposition? I am pretty new to this information, so i'm sure I am just overlooking something.","['integration', 'measure-theory', 'lebesgue-measure', 'real-analysis', 'radon-nikodym']"
4107507,"If $w$, $x$, $y$, and $z$ are real numbers with $w < x$ and $y < z$, is the cardinality of the closed interval $[w,x]$ the same as that of $[y,z]$? [duplicate]","This question already has answers here : bijective function from [a,b] to [c,d] (2 answers) Closed 3 years ago . My reasoning is yes. I tried to draw a few example functions and based on my workings, think that the answer should be yes but I couldn't figure out how exactly I should mathematically prove the fact. Any hints?","['elementary-set-theory', 'cardinals', 'discrete-mathematics']"
4107552,"Showing that the moment map $\langle \mathbf{J}(z),\xi\rangle=\mathbf{i}_{\xi_P}(\Theta)(z)$ is equivariant","I am studying through Introduction to mechanics and symmetry by Marsden and Ratiu, specifically the chapter on Momentum maps, and wanted some confirmation as to whether my argument for the following problem is correct. I have added quite a bit of context to the original problem to make it more readily understandable. Let $G$ be a finite dimensional Lie Group. We take $P=T^*G$ a Poisson manifold with bracket induced by the canonical symplectic form, that is, $\{ F,G\}=\omega(X_F,X_G)=X_F[G]$ , where $\omega=-d\Theta$ , for $\Theta$ the canonical one form on $P$ . We take the left action on $T^*G$ to be the cotangent lift of conjugation on $G$ , namely, $\Phi_g: G\to G$ , by $\Phi_g h:=ghg^{-1}$ and $\Psi_g: T^*G\to T^*G$ , $\Psi_g(\alpha_h)=T^*\Phi_g(\alpha_h)$ . This means that for $v\in T_{g^{-1}hg}G$ , $$\Psi_g(\alpha_{h})(v)=T^*\Phi_g(\alpha_{h})(v)=\langle \alpha_{h}, T\Phi_g\cdot v\rangle $$ We then Let $\mathbf{J}: T^*G\to \mathfrak{g}^* $ be the moment map: $\langle \mathbf{J}(z),\xi\rangle = \Theta(\xi_P)$ , where $\xi_P(h):= \frac{d}{dt}\vert_{t=0}\Psi_{\exp(t\xi)}(\alpha_h)$ . I would like to show that $\mathbf{J}$ is equivariant under our group action, i.e. $\langle \mathbf{J}(\alpha), \mathrm{Ad}_{g^{-1}}\xi\rangle=\langle \mathbf{J}(\Psi_g\alpha), \xi\rangle $ . I am a bit shaky on some of my details, so I wanted to make sure that my proof is correct. We have: $$\langle \mathbf{J}(\Psi_g\alpha_h), \xi\rangle=\mathbf{i}_{\xi_P(\Psi_g\alpha_h)}\Theta=\mathbf{i}_{\Psi^*_{g}(\xi_P(\alpha_h))}\Theta=\mathbf{i}_{(\mathrm{Ad}_{g^{-1}}\xi)_P(\alpha_h)}\Theta=\langle \mathbf{J}(\alpha_h), \mathrm{Ad}_{g^{-1}}\xi\rangle$$ Does this seem correct? It seems as though I did not use the fact that $\Theta$ is the canonical one form on $T^*G$ , though I may have implicitly done this when applying the pushforward. Any comments would be greatly appreciated. EDIT: I've been looking into this some more, particularly in the expression I have that seems to imply $\xi_p(\Psi_g\alpha_h)=\Psi^*_g(\xi_P(\alpha_h))$ . By definition: $\xi_P(\Psi_g(\alpha_h))=\frac{d}{dt}\vert_{t=0}\Psi_{\exp(t\xi)}\Psi_{g}(\alpha_h)$ . I'll investigate this by considering a smooth local vector field $X$ defined on a neighborhood $U$ of $ghg^{-1}\in G$ for for which $\exp(t\xi)ghg^{-1}\exp(-t\xi)\in U$ for $t$ small. We have \begin{align}\langle \Psi_{\exp(t\xi)}\Psi_{g}(\alpha_h),X(t)\rangle&=\langle T^*\Phi_{\exp(t\xi)} T^*\Phi_g(\alpha_h),X(t)\rangle\\
&=\langle T^*(\Phi_{g}\circ \Phi_{\exp(t\xi)})(\alpha_h), X(t)\rangle\\
&=\langle \alpha_h, T\Phi_{g}\cdot T\Phi_{\exp(t\xi)}(X(t))\rangle
\end{align} This seems to imply that my statement was correct, i.e. $\xi_P(\Psi_g(\alpha_h))=\Psi^*_g(\xi_P(\alpha_h))$ but I am unsure how to come to this conclusion formally.","['moment-map', 'poisson-geometry', 'lie-groups', 'differential-geometry']"
4107658,Is $(\sqrt{-7})^2$ defined in the real domain?,"Obviously $\sqrt{-7}$ is undefined in the real domain (since $-7 \lt 0$ ), however I'm wondering if $(\sqrt{-7})^2$ is as well. Per my understanding, using the following rule (listed in my textbook exactly as written): $$
\sqrt{a^n} = (\sqrt{a})^n, a \in \mathbb{R},\space n \in \mathbb{N}
$$ we could say (also knowing that $\sqrt{a^2} = \left|a\right|$ , where $a \in \mathbb{R}$ ): $$
(\sqrt{-7})^2 = \sqrt{(-7)^2} = \left| -7\right| = 7
$$ effectively giving $$
\sqrt{x^2} = (\sqrt{x})^2 = \left|x\right|, x \in \mathbb{R}
$$ Is this approach correct? (the final answer is clearly wrong - but the approach per my initial understanding follows all aforementioned rules).","['algebra-precalculus', 'solution-verification']"
4107660,$\lim \frac{1}{n} \sum_{i=0}^{n-1} X \circ T^{i}$ T-invariant?,"Birkhoff's ergodic theorem goes as follows: Let $T$ be a measure-preserving transformation on $(\Omega,\mathcal{F}, \mathbb{P})$ and $X \in \mathcal{L}^1$ . Then $S_n := \frac{1}{n} \sum_{i=0}^{n-1} X \circ T^{i}$ converges almost surely towards some $\mathcal{J}_T$ -measurable random variable $Y$ satisfying $\mathbb{E}[X] = \mathbb{E}[Y]$ . One way to prove this is by showing: $$\int \bar{X} \mathbb{P} \le \int X d\mathbb{P} \le \int \underline{X} d\mathbb{P}$$ where $\bar{X} := \lim \sup S_n$ and $\underline{X} := \lim \inf S_n$ . You can show that $\bar{X} \in \mathcal{J}_T$ and $\underline{X} \in \mathcal{J}_T$ , where $\mathcal{J}_T$ is the set of $T$ -invariant measurable sets in $\mathcal{F}$ . This follows because if we set $\frac{n+1}{n}S_{n+1}(\omega) = S_n(T\omega) + \frac{1}{n}S_n(\omega)$ we can take the $\lim \sup$ and $\lim \inf$ respectively and derive the desired property. My question is why can't I do this just with $\lim$ as well? Is it because we don't know a priori if this limit exists or have I overlooked something else?","['ergodic-theory', 'probability-theory', 'invariant-theory']"
4107681,What is the determinant of the Pascal matrix $M_{ij} = \binom{2j+1}{i}$?,"For $0\le i,j\le N-1$ , define $M_{ij} = \binom{2j+1}{i}$ . For example, with $N=6$ , we have $$M_6=\left(\begin{array}{cccccc}1&1&1&1&1&1\\1&3&5&7&9&11\\0&3&10&21&36&55\\0&1&10&35&84&165\\0&0&5&35&126&330\\0&0&1&21&126&462\\\end{array}\right)$$ For the first few $N$ s, I have $\det(M_2)=2$ , $\det(M_3)=8$ , $\det(M_4)=64$ , $\det(M_5)=1024$ , $\det(M_6)=32768$ , so it is easy to conjecture $$\det(M_N) = 2^{\binom{n}{2}}$$ This would suggest an induction approach, perhaps similar to the one proposed by Marc in this question . If the same approach worked in this case, we could apply some elimination matrices to reduce $M_N$ to something of the form $$M_N=\left(\begin{array}{cc} 1 & 0 \\ 0 & 2M_{N-1}\end{array}\right)$$ at which point induction would finish the proof. However, the elimination matrices required to obtain this reduction are not as easy to find given that the elements of the matrix satisfy the recursion $$M_{ij}=M_{i,j-1}+2M_{i-1,j-1}+M_{i-2,j-1}$$ where the last term is the one which causes most of the problems. Another idea would be to try to row reduce $M_N$ to a lower triangular matrix. For instance, with $N=6$ , we would have: $$M_6 \to \left(\begin{array}{cccccc}1&1&1&1&1&1\\0&2&4&6&8&10\\0&0&4&12&24&40\\0&0&0&8&32&80\\0&0&0&0&16&80\\0&0&0&0&0&32\\\end{array}\right)$$ from which the result is readily apparent. Dividing the $n$ th row by $2^n$ reveals a Pascal matrix, so I was hoping one could transform the Pascal matrix to only include its odd-numbered columns. However, I have not been able to come up with the required manipulations. Is there something trivial I am missing?","['matrices', 'binomial-coefficients', 'determinant']"
4107690,Is $f: \mathbb{Z} \to \mathbb{Z}$ onto when $f(m)=2m−3$?,"I’ve gotten a different answer than the one provided by the professor, and based on how the question is worded and the definition of onto , the answer ought to be no . Textbook definition of onto : “A function f from A to B is called onto, or a surjection, if and only if for every element b ∈ B there is an element a ∈ A with f(a) = b. A function f is called surjective if it is onto.” This is ℤ: {…, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, …} Both 𝑚 and 𝑓(𝑚) are in ℤ. If 𝑓 is onto, then for every element in ℤ, there should be a corresponding 𝑚 in ℤ that produces it when fed to the function 𝑓. However, the output of 𝑓 is only even integers—excludes odd integers. For that reason, 𝑓 cannot be onto because there are some elements in ℤ that are not outputs of 𝑓. 𝑓 𝑓(𝑚) Comment -1 -5 ❌ -4 no corresponding input in ℤ 0 -3 ❌ -2 no corresponding input in ℤ 1 -1",['functions']
4107707,"$	\int \frac{x^3}{\sqrt{x^2+x}}\, dx$","I'm trying to solve this irrational integral $$	\int \frac{x^3}{\sqrt{x^2+x}}\, dx$$ doing the substitution $$ x= \frac{t^2}{1-2 t}$$ according to the rule. So the integral becomes: $$  \int \frac{-2t^6}{(1-2t)^4}\, dt= \int (-\frac{1}{8}t^2-\frac{1}{4}t-\frac{5}{16}+\frac{1}{16}\frac{-80t^3+90t^2-36t+5}{(1-2t)^4})\, dt=\int (-\frac{1}{8}t^2-\frac{1}{4}t-\frac{5}{16}+\frac{1}{16}(\frac{10}{1-2t}-\frac{15}{2} \frac{1}{(1-2t)^2}+\frac{3}{(1-2t)^3}-\frac{1}{2} \frac{1}{(1-2t)^4}))\, dt=-\frac{1}{24}t^3-\frac{1}{8}t^2-\frac{5}{16}t-\frac{5}{16}\cdot \ln|1-2t|
-\frac{15}{64}\frac{1}{1-2t}+\frac{3}{64} \frac{1}{(1-2t)^2}-\frac{1}{16 \cdot 12} \frac{1}{(1-2t)^3}+cost $$ with $t=-x+ \sqrt{x^2+x}$ . The final result according to my book is instead $(\frac{1}{3}x^2-\frac{5}{12}x+\frac{15}{24})\sqrt{x^2+x}-\frac{5}{16}\ln( x+\frac{1}{2}+ \sqrt{x^2+x})$ And trying to obtain the same solution putting t in the formulas I'm definitely lost in the calculation...
I don't understant why this difference in the complexity of the solution...
Can someone show me where I'm making mistakes?",['integration']
4107725,Why vector field commute but the flow does not commute in this example,"I was doing Lee's smooth manifold Problem 9-19. Which is stated as follows: 9-19. Let $M$ be $\mathbb{R}^{3}$ with the $z$ -axis removed. Define $V, W \in \mathfrak{X}(M)$ by $$
V=\frac{\partial}{\partial x}-\frac{y}{x^{2}+y^{2}} \frac{\partial}{\partial z}, \quad W=\frac{\partial}{\partial y}+\frac{x}{x^{2}+y^{2}} \frac{\partial}{\partial z}
$$ and let $\theta$ and $\psi$ be the flows of $V$ and $W$ , respectively. Prove that $V$ and $W$ commute, but there exist $p \in M$ and $s, t \in \mathbb{R}$ such that $\theta_{t} \circ \psi_{s}(p)$ and $\psi_{s} \circ \theta_{t}(p)$ are both defined but are not equal. I solve the ODE and gets the solution: $$\theta_t\circ \psi_s = (p_1+t,p_2+s,\arctan(\frac{s+p_2}{p_1})+p_3-\arctan(\frac{p_2}{p_1}))\\\psi_s\circ\theta_t = (p_1+t,p_2+s,-\arctan(\frac{t+p_1}{p_2}) +p_3 + \arctan(\frac{p_1}{p_2}))$$ Which is obvious not equal,they both defined for all $(\Bbb{R}^3\setminus \{z\} )\times \Bbb{R}$ ,but it contradict to the theorem 9.44 that vector field commute if and only if flow commute?If I haven't made mistake in the computation.Is my computation correct,it's so hard to compute.Why does it not consistent with the theorem?","['vector-fields', 'smooth-manifolds', 'differential-geometry']"
4107775,What can we say about the number of unique roots of an infinite family of polynomials?,"Context: I saw the following problem on a discord server I'm in Now, this is an obvious meme, but it's a really interesting question so I started working on it with some other people in a math discord. A fairly elementary simplification of the problem is to define the family of polynomials $p_1(c) = c$ , $p_2(c) = c^2 + c$ , $p_3(c) = \left(c^2 + c\right)^2 + c$ , where, in general $p_n = p_{n - 1}^2 + c$ . If $c_0 \in \mathbb{C}$ is a root of any $p_n$ , then $c_0$ only kills finitely many people, because iteratively applying $f$ with that choice of $c$ eventually returns you to $0$ . However, this only gets you a subset of all the values of $c$ that work; in particular, this only gives you the values of $c$ which result in a loop that passes through $0$ , which not all loops will. For example, choosing $c = -2$ gives the loop $0, -2, 2, 2, 2, 2, ...$ , so $-2$ solves the original problem but won't be the root of any $p_i$ . We can make sure we have all the solutions by considering the roots to a difference of two of the $p_i$ , since if $c$ is a root to $p_n(x) - p_m(x)$ where $n \neq m$ , then clearly there is a cycle because two distinct numbers of iterations of $f$ return the same output. At this point, we got stuck, and there are two natural questions (1) What can we say about the roots of these polynomials? Are there infinitely many distinct roots? Are these uniformly bounded, and if not, are they nicely bounded by some function of $n$ ? Are the roots all isolated, or are there accumulation points? And of course, the original problem: (2) Is there a nice way to describe the set of all $c$ which are a root of some $p_i$ or which are the root of some $p_i - p_j$ for $i \neq j$ ? I've tagged the question abstract-algebra and dynamical-systems because the questions about the roots seems algebraic, and the original problem seems like a dynamical systems problem. Please let me know if there are other tags which fit better.","['complex-analysis', 'fractals', 'polynomials', 'dynamical-systems']"
4107777,"If $x^{x^{x+1}}=\sqrt{2}$, then evaluate $x^{x^{p}}$, where $p = 2x^{x+1}+x+1$",I can't figure out how to give a proper form to this expression to use the root of two. If $$x^{x^{x+1}}=\sqrt{2}$$ find the value of $W$ if $$W=x^{x^{p}} \quad\text{where}\; p = 2x^{x+1}+x+1$$ EDIT: This is an algebraic manipulation problem with the exponents. There was an error in the previous version (see the Edit History) that I have corrected.,"['exponentiation', 'algebra-precalculus']"
4107839,Size of a union of sets when only their sizes and those of pairwise intersections are given,"Consider a finite set $X$ and a set of subsets $X_i \subset X$ with $\bigcup_i X_i = X$ . Assume that only the sizes $|X_i|$ and $|X_i \cap X_j|$ are given. Obviously $\sum_i |X_i| - \sum_{ij}|X_i \cap X_j| \leq |X| \leq \sum_i |X_i|$ . Are there tighter lower and upper bounds for $|X|$ ? Edit : To be more specific, assume that a matrix $A = \{\alpha_{ij}\}$ is given, with $\alpha_{ij} = |X_i \cap X_j|$ , esp. $\alpha_{ii} = |X_i|$ . Is there a formulaic matrix invariant $F(A)$ with $F(A) = |\bigcup_i X_i|$ ? Here are some examples, describing families of four 4-element sets: $F\left(\begin{array}{cccc}4&0&0&0\\0&4&0&0\\0&0&4&0\\0&0&0&4\end{array}\right) = 16\ \ \ \ \ \ \ \ \ \ 
F\left(\begin{array}{cccc}4&1&0&0\\1&4&2&0\\0&2&4&1\\0&0&1&4\end{array}\right) = 12\ \ \ \ \ \ \ \ \ \ 
F\left(\begin{array}{cccc}4&2&1&2\\2&4&2&1\\1&2&4&2\\2&1&2&4\end{array}\right) = 9$ $F\left(\begin{array}{cccc}4&1&1&1\\1&4&1&1\\1&1&4&1\\1&1&1&4\end{array}\right) = 13\ \ \ \ \ \ \ \ \ \ 
F\left(\begin{array}{cccc}4&2&2&2\\2&4&2&2\\2&2&4&2\\2&2&2&4\end{array}\right) = 10\ \ \ \ \ \ \ \ \ \ 
F\left(\begin{array}{cccc}4&1&0&1\\1&4&1&0\\0&1&4&1\\1&0&1&4\end{array}\right) = 12$","['inclusion-exclusion', 'combinatorics', 'upper-lower-bounds', 'algorithms']"
4107902,"Limit of the integral of a function in $[0,x]$ with de l'Hopital","I want to compute the following limit: $$\lim_{x\to 0}\frac{\int_0^x {e^{t^5+1}-5} dt }{x}$$ I have thought to use the de l'Hopital rule, but in order to be able to apply this result I have to verify: if the limit gives us an indeterminate form of kind $\frac{0}{0}$ or $\frac{\infty}{\infty}$ if the ratio of the derivative of the functions at the numerator and denominator exists. I want to prove $\textbf{1.}$ Surely the limit of the denominator gives me $0$ , so necessarily I want that the numerator goes to $0$ . $\textbf{First way:}$ I have thought that trivially, since when $x\to 0$ I would have $\lim_{x\to 0}\int_0^x {e^{t^5+1}-5} dt \to \int_0^0 {e^{t^5+1}-5} dt $ and for the fact that in this last integral the upper and lower limits of the integral coincide and they are equal to 0, I have that the limit is $0$ (I am using the fact that in general $\int_a^a f(t)dt=0$ ). $\color{red}{\text{First doubt:}}$ $\textbf{I am not conviced of this fact...if my idea is right this
 would imply that}$ $\textbf{ in general whatever function of kind
 $\int_0^x f(t) dt$ goes to $0$ when $x\to 0$}$ . This could be a
conseguence of the fact that whatever function $G(x)=\int_0^x f(t) dt$ is continous and so $\lim_{x\to 0}G(x)=G(0)=0$ ...am I right or I am failing somewhere? Alternatively I have also thought two different ways to prove that the limit of the numerator is $0$ . $\textbf{First alternative way:}$ The function $f(t)=e^{t^5+1}-5$ is continous and differentiable $\forall t\in \mathbb{R}$ so I can apply the mean value integral theorem in whatever $[0,x]$ with $x>0$ and so: $$\exists \xi\in[0,x]: \int_{0}^x e^{t^5+1}-5=f(\xi)\cdot (x-0)=(e^{\xi^5+1}-5)\cdot x$$ So $\lim_{x\to 0}\int_{0}^x (e^{t^5+1}-5)=\lim_{x\to 0}(e^{\xi^5+1}\-5)cdot x=0$ (remembering that when $x\to 0$ also $\xi\to 0$ ) $\textbf{Second alternative way:}$ The function $f(t)=e^{t^5+1}-5$ is continous $\forall t$ and in particular for $t=0$ so $\forall a>0$ I can say: $$\text{in }[0,a] \exists m,M\in\mathbb{R}: m\leq f(t)\leq M$$ This holds from Weierstrass because in $[0,a]$ I am in a neighbourhood of $0$ , and in $0$ the function is continous, and since I want to understand what happens near $0$ (because I consider $x\to 0$ ) I can say: $$0=\lim_{x\to 0}mx=\lim_{x\to 0}\int_0^xmdt\leq\lim_{x\to 0}\int_0^x {e^{t^5+1}-5} dt \leq \lim_{x\to 0}\int_0^xMdt=\lim_{x\to 0}Mx=0$$ $\color{red}{\text{Second doubt:}}$ I am not so convinced of these tw different ways of proving the limit. Do you thinks they are correct? If yes what is in your opinion the best way to use (the first or one of the last alternative)?","['integration', 'real-analysis', 'continuity', 'solution-verification', 'limits']"
4107961,Finding a local normal form regarding distribution rank properties,"I am working in geometry control field, fall last week on this exercice and I can't figure it out. I have a distribution $\mathscr{D}$ with $rank(\mathscr{D})=m+1$ in $\mathbb{R}^n$ with $n\leq 2m+1$ . I know that there exists an involutive sub-distribution $\mathscr{L}\subset\mathscr{D}$ with rank $m$ . I also know that the growth vector is $(m+1,n)$ . (So I guess $\mathscr{D}+[\mathscr{D},\mathscr{D}]=T\mathbb{R}^n$ , am I right?). What I need is to find a local normal form $\varphi$ of $\mathscr{D}$ so that $\varphi_*\mathscr{D}=span\{f'_1,\dots,f'_n\}$ assuming $\mathscr{D}=span\{f_1,\dots,f_n\}$ . So far, I guess I can use the fact that $\mathscr{F}$ is involutive to write it $\mathscr{F}=span\{\frac{\partial}{\partial x^1},\dots,\frac{\partial}{\partial x^m}\}$ and then add a vector to build $\mathscr{D}$ but I don't know how. I guess Fröbenius can provide some help but can't find how. Someone has an idea ? Thanks in advance. Edit: Reference: Article from Williams Pasillas and Witold Respondek ( https://arxiv.org/abs/math/0004124 )","['vector-fields', 'distribution-theory', 'differential-geometry']"
4107973,A polygon with constant angular momentum bounds a circle,"Let $\alpha:[0,L] \to \mathbb{R}^2$ be a piecewise affine map satisfying $\alpha(0)= \alpha(L)$ and $|\dot \alpha|=1$ . Supopse that $\alpha(t) \times \dot \alpha(t)$ is constant. How to prove that $\operatorname{Image}(\alpha)$ is a tangential polygon, i.e. a polygon whose edges are all tangent to a fixed circle, centered at the origin? It suffices to prove that for each subinterval $[a,b] \subseteq [0,L]$ where $\alpha|_{[a,b]}$ is affine, there exists a $t_0 \in (a,b)$ such that $\dot \alpha(t_0) \perp \alpha(t_0)$ . Indeed, if this is the case, then $|\alpha(t_0)|=|\alpha(t) \times \dot \alpha(t)|=C$ is independent of the segment $[a,b]$ chosen. Thus, every ""edge"" $\alpha([a,b])$ , contains a point $P_{a,b}=\alpha(t_0)$ on the circle with radius $C$ , and the edge is perpendicular to radius at $P_{a,b}$ , i.e. it is tangent to the circle at $P_{a,b}$ . I am not sure how to prove the bold statement. I think we need to use somehow the fact that the polygon ""closes"". The converse implication is easy: If there exists such a circle with radius $R$ , then $|\alpha(t) \times \dot \alpha(t)|=R$ is constant: Indeed, suppose that $\alpha(t_0)$ lies on the circle -- so it is a tangency point. Then $\dot \alpha(t_0) \perp \alpha(t_0)$ , and $|\alpha(t_0)|=R$ . Let $t$ satisfies $\dot \alpha(t)=\dot \alpha(t_0)$ , i.e. $\alpha(t)$ belongs to the same edge as $\alpha(t_0)$ . Then $\alpha(t)=\alpha(t_0)+\beta(t)$ , where $\beta(t) || \dot \alpha(t_0)$ , so $$
\alpha(t) \times \dot \alpha(t)=\big( \alpha(t_0)+\beta(t) \big) \times \dot \alpha(t_0)=\alpha(t_0) \times \dot \alpha(t_0),
$$ which implies $|\alpha(t) \times \dot \alpha(t)|=R$ .","['euclidean-geometry', 'circles', 'polygons', 'symmetry', 'differential-geometry']"
4108111,Continuity of finite Borel measures on Euclidean subspaces,"Let $\mu$ be an absolutely continuous (w.r.t. the Lebeasgue measure $\lambda$ ) Borel measure on $(X, \mathcal{X})$ , where $X \subset \mathbb{R}^p$ , for some $p \geq 1$ . Then we know that $\mu$ is continuous from above/below: i.e. if $(A_n)_{n\geq 1} $ is a sequence of measurable sets then if $A_n \subset A_{n+1}$ (or $A_{n+1} \subset A_n$ ) we have $
\lim_{n \to \infty}\mu_n (A_n)=\mu(\cup_{n\geq 1} A_n)
$ (or $\lim_{n \to \infty}\mu_n (A_n)=\mu(\cap_{n\geq 1} A_n)$ ). However, it is legitimate to wonder whether other forms of continuity hold true. In particular, for a sequence of vectors $x_n=(x_{n,1}, \ldots, x_{n,d})$ with positive components, one may denote $$
x_n \cdot B= \{x \cdot x_n: \, \in B\}, \quad B \in \mathcal{X},
$$ where $x_n \cdot x=(x_{n,1}x_1, \ldots, x_{n,p}x_p)$ denotes the componentwise product, and wonder whether $$
\lim_{n\to \infty}{x_{n,j}}=1, \forall j \in \{1, \ldots,p\} \implies \lim_{n \to \infty}\mu(x_n \cdot B)= \mu (B).
$$ CASE 1: For bounded sets $B$ , it should be true that the condition on the left-hand side implies $\lambda(x_n \cdot B \, \triangle \, B)\to 0$ and hence $\mu(x_n \cdot B \, \triangle \, B)\to 0$ , since $\mu$ is assumed absolutely continuous (see Continuity for sets of asymptotically null Lebesgue measure ); herein $A \, \triangle \, B$ denotes the symmetric set difference. Is this correct? CASE 2: If $B$ is unbounded, we still have that $\mu$ is assumed finite, how can we proceed in this case? EDIT 1 In both cases, if we have that, denoting $B_n:=x_n \cdot B$ , $$
B=\limsup_{n\to \infty}B_n=\liminf_{n\to \infty}B_n=\lim_{n\to \infty} B_n,
$$ then we can apply property (M12) on page 48 of Vestrup's book - The Theory of Measures and Integration , 2003, Wiley - and conclude that $\lim_{n\to \infty}\mu(B_n)=\mu(B)$ . EDIT 2 Resorting to the existence of $\lim_{n \to \infty}B_n$ is not necessary if $B$ is a closed set. Notice that $\mu(B_n \, \triangle \, B)\to 0 \implies \mu(B_n)\to \mu(B)$ . Also, notice that we can always construct a monotone nonincreasing positive sequence $r_n$ such that $1>r_n \downarrow 0$ and, ultimately, $$
\max_{1\leq j \leq d}|x_{n,j}-1|<r_n,
$$ see e.g. Limits and existence of decay rates . Consequently, defining $$
\bar{B}_n^{(1)}=\{x \in X: \, \exists x' \in B\, \text{ s.t. } x'_j(1-r_n)\leq x_j \leq x_j'(1+r_n)\},
$$ $$
\bar{B}_n^{(2)}=\{x \in X: \, \exists x' \in B^c\, \text{ s.t. } x'_j(1-r_n)< x_j < x_j'(1+r_n)\},
$$ we have $\bar{B}_{n+1}^{(j)} \subset \bar{B}_n^{(j)}$ with $j=1,2$ . Ultimately $B_n \subset \bar{B}_n^{(1)}$ and $B_n^c= x_n \cdot B^c \subset \bar{B}_n^{(2)}$ ,
since we can define $T_n(x)=(x_{n,1}^{-1}, \ldots, x_{n,d}^{-1}) \cdot x$ , $\forall x \in X$ , and deduce $$
B_n^c=(T_n^{-1}(B))^c=T_n^{-1}(B^c)=x_n \cdot B^c,
$$ see also Preimage of the complement . Moreover, $\lim_{n\to \infty}\bar{B}_n^{(1)}=B$ and $\lim_{n\to \infty}\bar{B}_n^{(2)}=B^c$ , thus $$
\lim_{n\to \infty}\bar{B}_n^{(1)}\cap B^c=\emptyset, \quad \lim_{n\to \infty}\bar{B}_n^{(2)}\cap B=\emptyset.
$$ Ultimately, $$
\mu(B_n \cap B^c)\leq \mu(\bar{B}_n^{(1)}\cap B^c) \to 0, \quad  \mu(B_n^c \cap B)\leq \mu(\bar{B}_n^{(2)}\cap B) \to 0.
$$ It now follows that $\mu(B_n \, \triangle \, B)\to 0$ . Is this reasoning correct?","['measure-theory', 'convergence-divergence', 'borel-measures']"
4108116,help replicating fuzzy equations from a paper,"I'm trying to replicate Zhou 's Paper on quantifying UX using Fuzzy Math. In their model, there is a weight vector $A$ for a set of characteristics. in the paper's test case the characteristics were Effectiveness, Efficiency , and Satisfaction with weights [0.4434, 0.1692, 0.3874] , respectively. The method of obtaining these weights was something i was able to replicate. along with this, a rating matrix $R$ which in the each category has 5 bins of discrete ratings: [Excellent, Good, Medium, Poor, Very Poor] . So for the category effectiveness , 68.75% of the respondents had an Excellent rating while the remainder had a rating of Good . Giving us [0.6875, 0.3125, 0, 0, 0] the scores for Efficiency and Satisfaction were similarly obtained which gives us: Excellent Good Medium Poor Very Poor Effectiveness 0.6875 0.3125 0 0 0 Efficiency 0.25 0.3125 0.0625 0.25 0.125 Satisfaction 0.125 0.625 0.25 0 0 The next step describes getting the appraisal vector $B$ where $B = A ∘ R$ where they describe the $∘$ as the composition operator $M(⋅,⊕)$ . I'll directly take a chunk of the paper to show how they described the operator: Zhou (2015), Eq(2) This is where I fail to replicate the math. It seems to equate $B$ onto two different values. it says $B$ is equal to $A ⋅ R$ but the the same time is also equal to $min(1, A ⋅ R)$ . Specifically, $b_j = \sum_{i=1}^n a_i r_{ij} = min \{1,\sum_{i=1}^n a_i r_{ij}\}$ where the weight vector is $A = (a_1, a_2, ...)$ and the rating vector is $R = 
$$
    \begin{bmatrix}
    r_{11} & r_{12} & \cdots & r_{1m}\\
    r_{21} & r_{22} & \cdots & r_{2m}\\
    r_{31} & r_{32} & \cdots & r_{3m}\\
    \vdots & \vdots &  & \vdots \\
    r_{n1} & r_{n2} & \cdots & r_{nm} \\
    \end{bmatrix}
$$ 
$ In the example, $B = A ∘ R = [0.3956, 0.3711, 0.1758, 0.0156, 0.0313]$ I should note that $b_1$ or the first element of $B$ which has a value of 0.3956 has the same value when i do a simple dot product, but the rest of the values ( $b_2$ to $b_5$ ) have glaringly different values. i also can't seem to place where I'd actually do the $\alpha ⊕ \beta =  min(1, \alpha + \beta)$ operation as described in the paper. can anybody point me in the right direction?","['elementary-set-theory', 'fuzzy-logic', 'fuzzy-set']"
4108156,What is the double integral of an exponential difference of sines?,"I am keen to know if there's a way of analytically evaluating the following integral: $$
\int_{0}^{2\pi}\int_{0}^{2\pi}e^{i\alpha|\sin(\theta)-\sin(\varphi)|}d\theta d\varphi
$$ where $\alpha$ is a real constant. It arises when considering how arrays of charged wires interact with each other. There are Bessel function representations for similar integrals when one does not have to take the modulus. Here, I simply don't know how to deal with the modulus. I've tried rewriting it using $$
\sin(\theta)-\sin(\varphi)=2\sin\left(\frac{\theta-\varphi}{2}\right)\cos\left(\frac{\theta+\varphi}{2}\right)
$$ and then transforming the coordinates to this rotated version. I think it's then fine to rotate the region of integration accordingly, owing to the double periodicity of the integrand, but then I get stuck. Thanks in advance for any help. Update: I've used Teresa Lisbon's suggestion of expanding out the exponential to find that $$
\int_{0}^{2\pi}\int_{0}^{2\pi}e^{i\alpha|\sin(\theta)-\sin(\varphi)|}d\theta d\varphi=4\pi^{2}J_{0}(\alpha)^{2}+32i\alpha_{p}F_{q}(1,1;\frac{3}{2},\frac{3}{2},\frac{3}{2};-\alpha^{2})
$$ Does anyone know if the hypergeometric function part can be simplified (ideally in terms of Bessel functions)? Thanks!","['integration', 'trigonometric-integrals', 'multiple-integral', 'definite-integrals']"
4108188,A hat allocation problem,"This is an abstraction of a problem that has come up in my research. Imagine we have $N$ wizards and $N$ hats. The hats have $C$ different colours. There are $n_1>0$ hats with the first colour, $n_2>0$ with the second and so on such that $\sum_{i=1}^C n_i=N$ . Hats of the same colour are identical in all respects. Wizards are powerless without their hat and have magical power $0$ . Each hat provides a power up to a wizard. How each coloured hat affects a wizard varies from wizard to wizard. We can think of the wizards as the integers $1,\dots,N$ and each coloured hat as a function $f_i:[N]\to\mathbb N$ where $f_i(m)$ is the magical power of wizard $m$ when wearing any hat with colour $i$ . Question: For which $C$ can we find an efficient algorithm that assigns an allocation of hats to wizards such that each wizard wears one hat and their total power is maximised? In other words, can we find a colouring $i:[N]\to[C]$ , with $n_i$ instances of colour $i$ , such that $\sum_{m=1}^N f_{i(m)}(m)$ is maximised? For $C=1$ this is trivial, while $C=2$ is very similar to the Neyman-Pearson lemma with $f_1-f_2$ the likelihood function ratio. The optimal solution for $C=2$ is given by $\{$ wizards wearing hat 1 $\}=\{m\mid f_1(m)-f_2(m)>k\}$ with $k$ chosen such that this set is of size $n_1$ (there's a subtlety when there is no $k$ such that this is true due to collisions of $f_1-f_2$ , but this is simple to solve). However, for $C=3$ I have no idea how to proceed and am not even sure if the problem is in P.","['matching-theory', 'combinatorics', 'algorithms', 'discrete-optimization', 'computational-complexity']"
4108258,How does one find the moment of inertia about a line passing through the centroid of a cone?,"Image 1 detailing the question Image 2 showing the part of the working I do not understand In image 2 part d, I'm unsure why the moment of inertia with respect to the y axis, Iy, is equal to Ic plus the expression shown. Could someone explain to me the intuition behind this equation? Can this be extended to lines other than that passing through the centroid?","['multivariable-calculus', 'calculus']"
4108260,Evaluating $\frac{13}{1.2 .3 .2}+\frac{26}{2.3 .4 .4}+\frac{43}{3.4 .5 .8}+\frac{64}{4.5 .6 .16}+\cdots$,"$$\frac{13}{1.2 .3 .2}+\frac{26}{2.3 .4 .4}+\frac{43}{3.4 .5 .8}+\frac{64}{4.5 .6 .16}+\cdots$$ I can reduce it to the general term, $$\sum_{r=1}^\infty \frac{2r^2 + 7r +4}{r(r+1)(r+2)2^r}$$ I don't know how to go about this any further though. I also ran this in python and the sum is exceeding $1.5$ for $10,000$ terms, which is weird since it should converge to $1.5$ , so it makes me doubt if the general term I've written is correct.",['sequences-and-series']
4108295,Does this property hold for a Geometric Brownian Motion?,"Assume that $B_t$ is a Brownian motion. Let $\mu, \sigma$ be real numbers greater than zero. Do we then have $$P\left(\liminf\limits_{t\rightarrow \infty}e^{\mu t+\sigma B_t}>1\right)=1?$$ I am not sure how to start to figure this out.","['stochastic-processes', 'brownian-motion', 'probability-theory', 'limsup-and-liminf']"
4108342,Does a continuous distance satisfy the triangle inequality?,"I am not sure what tags are most appropriate here, so any help is appreciated (I found no tags for premetrics, quasimetrics, pseudometric, etc.). My level is undergrad-master . (TL;DR: One can probably start with the questions below, and read the definitions later.) Let a distance function on $X$ be a function $d:X\times X\to[0,\infty]$ that satisfies: reflexive : $d(x,x)=0\quad\forall x\in X$ symmetric : $d(x,y)=d(y,x)\quad\forall x,y\in X$ A distance $d$ could then further satisfy the triangle inequality , i.e. $d(x,y)\leq d(x,z)+d(z,y)\quad\forall x,y,z\in X$ . We can also define convergence of sequences for a distance, saying that $\{x_n\}_n\to x$ if $\lim_{n\to\infty}d(x_n,x)=0$ . And thus we could define a distance $d$ to be continuous if for any sequences $x_n,y_n$ s.t. $x_n\to x, y_n\to y$ then $d(x_n,y_n)\to d(x,y)$ . I am then interested to see what the triangle inequality (3) gives us more precisely. We can prove that a distance $d$ satisfying (3) is continuous: \begin{equation}
|d(x_n,y_n)-d(x,y)|=|d(x_n,y_n)-d(x_n,y)+d(x_n,y)-d(x,y)|\leq \\
|d(x_n,y_n)-d(x_n,y)|+|d(x_n,y)-d(x,y)|\leq d(y_n,y)+d(x_n,x)\to 0, \text{ as }n\to\infty.
\end{equation} Question(s): However, can we prove that a continuous distance satisfies (3)? If so (not): any hints (counterexamples)? Also, could we then somewhat informally say that what (3) gives us is precisely what is needed to talk about continuity of a distance, i.e. that (3) characterizes the notion of continuity of a distance? Am I missing something obvious here or does this reasoning make sense?","['general-topology', 'triangle-inequality', 'analysis']"
4108393,$u^{2}-1$ is a unit in a ring $A$,"Hello I have the next question that I want to prove (or one counterexample if there is one) Let $A$ be a ring with maximal ideal $M$ and quotient field $k=A/M$ of size at least $4$ such that the natural induced map $p:A^{\ast}\rightarrow k^{\ast}$ is surjective, then there exist $u\in A^{\ast}$ such that $u^{2}-1\in A^{\ast}$ I tried this Since $|k|\geq 4$ then there exist $x\in k^{\ast}$ with $x^{2}\not=1$ . Now since $p$ is surjective, we get that $x=p(u)=\overline{u}$ with $u\in A^{\ast}$ , then $\overline{u}^{2}\not=1$ in $k$ . Thus $\overline{u}^{2}-1\in k^{\ast}$ . From this I have that $p(a)=\overline{u}^{2}-1$ with $a\in  A^{\ast}$ .
It follows that $u^{2}-1=a+m$ with $a\in  A^{\ast}$ , $m\in  M$ . From this I am not sure how to continue in order to prove that $u^{2}-1\in A^{\ast}$ . I was thinking that since $a$ is unit then $(u^{2}-1)a^{-1}=1+m'$ with $m'=ma^{-1}\in M$ . Thus $1+m'$ is a unit if and only if $u^{2}-1$ is a unit. So I need to prove that $1+m'$ is unit (That is true if $A$ is a local ring) That's my idea if there is a suggestion or hint  I would appreciate. Thanks!","['ring-theory', 'abstract-algebra', 'commutative-algebra']"
4108441,Does existence of free submonoid implies free subgroup,"Suppose that we have group $G$ that, regarded as a monoid, has a submonoid $M$ , which is free with $r$ generators. Does it imply that is has a subgroup $H$ , free and with $r$ generators? For $r=1$ it is clear, and the case $r=2$ will be, I think, the only interesting one. The first approach might be that $H=\langle M \rangle$ works, but that's not true: for $$ A=\begin{pmatrix}1&0\\1&1 \end{pmatrix}, \textrm{ and }B=\begin{pmatrix}1&1\\0&1 \end{pmatrix} $$ it's not hard to check that they generate a free submonoid, but we have $(A B^{-1} A)^4 = I$ . However, $\langle A^2,B^2 \rangle$ is free, so it isn't a counterexample.","['group-theory', 'abstract-algebra']"
4108572,Holomorphic function not conformal?,"Is it true that holomorphic functions are not conformal?
By my understanding a holomorphic function is angle-preserving. But if I plot the lines $t+i$ and $1+it$ under the map $x \mapsto x^2$ then the images meet at $90^o$ , but if I map the lines under $x \mapsto x^3$ then they do not. What am I missing?","['complex-analysis', 'conformal-geometry']"
4108601,Change of discrete random variables,"I just read this: Let X be a discrete random variable whose probability function is $f(x)$ . Suppose that a discrete random variable U is defined in terms of X by $U = \phi(X)$ , where to each value of X there corresponds one and only one value of U and conversely, so that $X = \psi (U)$ . Then the probability function for U is given by $g(u) = f [\psi(u)]$ . Is this correct? This is from Schaum's  Probability and Statistics 4th edition pg42. Can any of you please help me understand this? I am sorry I am a bit weak in Probability. Thanks!","['statistics', 'probability-theory', 'probability']"
4108647,About condition number,"I have the following exercise: Relate the 2-norm condition of $X\in \Bbb R^{m\times n}\ (m\geq n)$ to the 2-norm condition of the matrices: $$B=\begin{equation}
\begin{bmatrix}
I_m & X\\
0 & I_n
\end{bmatrix}
\end{equation}$$ and $$C=\begin{equation}
\begin{bmatrix}
X\\
I_n
\end{bmatrix}
\end{equation} $$ My attemp: We know that the condition number of a matrix for the 2-norm is given by the following expression: $$\kappa_2(A)=||A||_2 ||A^{-1}||_2$$ Also I have a property that says if $B$ is a submatrix of $A$ then $$||B||_2\leq ||A||_2$$ So using this I have: $$\kappa_2(B)=||B||_2 ||B^{-1}||_2\geq ||X||_2||X^{-1}||_2=\kappa_2(X)$$ But I am not sure that it is necessarily true that if $X$ is a submatrix of $B$ then necessarily $ X^{-1} $ is a submatrix of $B^{-1}$ If this works then for the $ C $ it would be worth the same, but doing numerical experiments this fails, numerically I get: $$\kappa_2(C)\leq \kappa_2(X) \leq \kappa_2(B)$$ I appreciate any help you can give me, regards Edit: I have seen that $X$ is not square so the inverse does not exist and therefore kappa will not exist either. For this, in Golub's book it is said that it can also be calculated as the maximum singular value between the minimum singular value, that is: $$\kappa_2(A)=\dfrac{\sigma_{max}}{\sigma_{min}}$$ And in this case, as the matrix $X$ is given, for any matrix the 2-norm condition can be obtained, applying the singular value decomposition theorem","['condition-number', 'svd', 'spectral-norm', 'matrices', 'matrix-norms']"
4108688,Need to calculate the probability,"In an office, after having a very busy day, the secretary is just leaving the seat when the boss calls her and hands over the drafts for four letters with addresses. The secretary types the letters and addresses on envelopes. But when she is going to put letters in envelopes, she puts letters in envelopes randomly without seeing the corresponding addresses. What is the probability that exactly three letters will be dispatched to correct addresses? I'm new to this so I'm a bit confused. As far as I know P(event) = outcomes that meet our criteria / all possible outcomes So, what I did is: P(3) = 3 / 4 but I'm not fully convinced if this is correct. I'm just not sure.","['discrete-mathematics', 'probability']"
4108730,Finding the derivative of the given piecewise function,"I'm given the following function $$f(x)= \begin{cases} \left(x-a\right)^2\left(x-b\right)^2\;,\quad x \in[a;b]\\ 0\;,\qquad\qquad\qquad\;\; x \notin[a;b] \end{cases}$$ I tried finding the derivative by using the definition, but I couldn't. I am not very familiar with the idea of taking the derivative of a piecewise function, that's why I'm stuck and don't know what to do. I would be great help for me if you could show how to take the derivatives of this kinds of functions or give some hints. Thank you very much.","['analysis', 'real-analysis', 'calculus', 'functions', 'derivatives']"
4108770,Calculate integral $\int\limits_{0}^{2\pi}\frac{dx}{\left ( 1+n^2\sin^2 x \right )^2}$,"I recently saw the integral problem $$\int\limits_{0}^{2\pi}\frac{dx}{\left ( 1+n^2\sin^2 x \right )^2}$$ and tried to solve it. Below is what I did.
Interesting to look at other easier solutions. $$\int\limits_{0}^{2\pi}\frac{dx}{\left ( 1+n^2\sin^2 x \right )^2}=4\int_{0}^{\pi /2}\frac{dx}{\left ( 1+n^2\sin^2 x \right )^2}\\\overset{t=\operatorname{tg} x}{=}\int\limits_{0}^{\infty }\frac{1+t^2}{\left ( 1+\left ( 1+n^2 \right )t^2 \right )^2}dt\\ 
\overset{t=\frac{y}{\sqrt{1+n^2}}}{=}\frac{4}{\left ( 1+n^2 \right )\sqrt{1+n^2}}\int\limits_{0}^{\infty }\frac{1+n^2+y^2}{\left ( 1+y^2 \right )^2}dy\\ \overset{y=\operatorname{tg} \theta }{=}\frac{4}{\left ( 1+n^2 \right )\sqrt{1+n^2}}\int_{0}^{\pi /2}\left ( 1+n^2\cos^2 \theta  \right )d\theta \\
=\frac{\pi \left ( 2+n^2 \right )}{\left ( 1+n^2 \right )\sqrt{1+n^2}}$$","['integration', 'calculus', 'real-analysis']"
4108795,Proof of $\mathsf{SU}(3)/T^2$ is not a symmetric space,How to see that $\mathsf{SU}(3)/T^2$ is not a symmetric space? This is not obvious to me. How to see that it admits a metric of positive curvature? The only clue that I know is the submersion $\mathsf{SU}(3)\to \mathsf{SU}(3)/T^2$ if I am not mistaken. Then O'Neill's formula guarantees its curvature is positive? No matter which definition of symmetric spaces is assumed. e.g. $\nabla R=0$ .,"['riemannian-geometry', 'reference-request', 'symmetric-spaces', 'lie-groups', 'differential-geometry']"
4108798,How do we solve Burgers' equation using the method of characteristics?,"I want to solve \begin{align}&\forall(t,x)\in(0,\infty)\times\mathbb R:\left(\frac{\partial u}{\partial t}+u\frac{\partial u}{\partial x}\right)(t,x)=0;\tag1\\&\forall x\in\mathbb R:u(0,x)=u_0(x)\tag2,\end{align} where $\Omega:=(0,\infty)\times\mathbb R$ and $u_0\in C^1(\mathbb R)$ , using the method of characteristics . Assume that $u_0'\le0$ and $$u_0(x)=\left.\begin{cases}1&\text{, if }x\le0\\0&\text{, if }x\ge1\end{cases}\right\}\;\;\;\text{for all }x\in\mathbb R\tag3.$$ As usual, we consider a path $\gamma$ with $\gamma'=(1,\underbrace{u\circ\gamma}_{=:\:z})$ , since then $(1)$ implies $z'=0$ and hence $z=c_1$ for some $c_1\in\mathbb R$ , which in turn impleis $\gamma=(c_2+t,c_3+c_1t)$ for some $c_2,c_3\in\mathbb R$ . Fix $(t_0,x_0)\in(0,\infty)\times\mathbb R$ . Since we are interested in the value $u(t_0,x_0)$ , we choose $c_2:=0$ and $c_3:=x_0-c_1t_0$ to obtain $$u(t_0,x_0)=z(t_0)=z(0)=u_0(x_0-c_1t_0)\tag4.$$ Question 1 : So, is this the whole story? Do we obtain different values for the solution of $(1)$ by varying $c_1$ ? And does this that $(1)$ and $(2)$ together do not admit a unique solution? (And do they admit a solution at all?) Question 2 : By $(3)$ , we see that if $c_1\ge\frac{x_0}{t_0}$ , then $u(t_0,x_0)=1$ ad if $c_1\le\frac{x_0-1}{t_0}$ , then $u(t_0,x_0)=0$ , but what can we infer from that? Does this somehow yield a contradiction for general $t_0$ ? Or, phrased differently, does this imply that we won't be able to obtain a ""global"" solution, but only a solution up to some finite time $T>0$ ?","['fluid-dynamics', 'characteristics', 'ordinary-differential-equations', 'partial-differential-equations']"
4108966,Proving if $a_{k}\ge a_{k-1}+1$ then $1+\frac{1}{a_{0}}(1+\frac{1}{a_{1}-a_{0}})...(1+\frac{1}{a_{n}-a_{0}})\le \prod_{k=0}^{n}(1+\frac{1}{a_{k}})$,"I've worked on this problem with the sequence $a_{k}$ being the natural numbers, that is $a_{k}=a_{k-1}+1$ and $a_{0}=1$ . Over the naturals, $\prod_{k=0}^{n}(1+\frac{1}{a_{k}})$ can be proven to be $n+1$ . I've been able to recognize the obvious pattern in $\prod_{k=2}^{n}(1+\frac{1}{k-1})$ to be equal to $n$ , but I do not know how to prove this rigorously. That said, over the naturals, the inequality reduces to $n+1 \le n+1$ . That got me thinking if the sequence in question could be expanded with the condition $a_{k}\ge a_{k-1}+1$ and specifically if this would not lead to the situation corresponding with RHS being greater than LHS. Thank you for your help.","['products', 'inequality', 'sequences-and-series']"
4108993,The $221$ groups of order $|G| = 400$,"A book of John Conway suggests there are 221 groups of order $|G| = 400$ .  How do I go about finding these.  Commutative groups with $ab = ba$ can be listed very easily: $\mathbb{Z}/400\mathbb{Z}$ $\mathbb{Z}/200\mathbb{Z} \oplus \mathbb{Z}/2\mathbb{Z}$ $\mathbb{Z}/25\mathbb{Z} \oplus \mathbb{Z}/16\mathbb{Z}$ ... There are 207 super-solvable groups of order $|G| = 400$ How do we list some of them? $$ 1 \leq H_0 \leq H_1 \leq H_2 \leq \dots \leq H_n = G $$ here $H_i \vartriangleleft G$ and $H_{i+1}/H_i$ is cyclic .   This could be a great way to explain the difference between nilpotent and solvable groups.  Some discussion here however I will put in the tag representation-theory which includes (for example) matrix representation or permutation representations. There are 28 nilpotent groups of order $|G| = 400$ . I haven't used GAP the question would be how does the computer program find such objects?  Here's some of what it found: $G = (C_5 \ltimes Q_8 ) \times D_{10}$ $G = (C_5 \ltimes C_5) \ltimes (C_4 \times C_4) $ $G = C_2 \times ((C_5 \times C_5) \ltimes C_8)$ These names or descriptions leave it upon us to say what these symmetries actually look like. Example, $D_{10}$ is the dihedral group or the symmetry group of a 10-gon Also $D_{10} = C_{10} \ltimes C_2 $ , see also [ 1 ] . $C_n \simeq \mathbb{Z}/n\mathbb{Z}$ is the cyclic group .","['representation-theory', 'group-theory', 'finite-groups', 'groups-enumeration']"
4109104,Classification of strange critical point in the plane,"What type is the critical point $(0, 0)$ for the following system? \begin{align}
&x' = y\, ,\\
&y' = -x^2 - x^3\, .
\end{align} The critical points for this system are: $(-1, 0)$ , a center; and $(0, 0)$ . The Jacobian for this problem is $$\pmatrix{0&1\cr -3\,x^2-2\,x&0\cr }\, ,$$ and in $(0,0)$ is $$\pmatrix{0&1\cr 0&0\cr }\, .$$ That  is not-normal and has only one eigenvector. The following is the phase portrait for the system, with a contour plot for the conserved quantity, $$V(x, y) = \frac{(3x + 4)x^3}{12} + \frac{y^2}{2}\, ,$$ and a trajectory for the value of $0$ , the one that cross the $(0, 0)$ critical point. Perturbed critical point If we perturb the system as \begin{align}
&x' = y\, ,\\
&y' = -(x^2 - \epsilon^2) (1 + x)\, ,
\end{align} We have the following critical points: $(-1, 0)$ , a center; $(-\epsilon, 0)$ , a saddle point; and $(\epsilon, 0)$ , a center. So, the original point seems to be a merging of these last two points.","['ordinary-differential-equations', 'dynamical-systems']"
4109156,"Showing $\lim_{p\to\infty}\left(\int_I|f|^p\right)^{1/p} = \max|f|$, where $I$ is a generalized rectangle",Let $I$ be a generalized rectangle and let $f: I \to \mathbb{R}$ be continuous. Show that $$\lim_{p\to\infty}\left(\int_I|f|^p\right)^{1/p} = \max|f|$$ I found it straightforward to show that these integrals in the limit sequence are properly defined and I showed the LHS $\leq$ RHS by using the definition of the integral using partitions. I am confused about the approach to showing that RHS $\leq$ LHS to give equality. Thanks for any hints and suggestions.,"['integration', 'analysis', 'multivariable-calculus', 'sequences-and-series', 'limits']"
4109199,Ideal sheaf is quasi-coherent if and only if its generated by local sections.,"My confusion is lies in Schemes Lemma 10.1 of the Stacks project. First, Modules Definition 8.1 states that a sheaf $\mathcal{F}$ of $\mathcal{O}_X$ -modules is locally generated by sections if for all $x\in X$ , there is a neighborhood $U$ of $x$ and a surjection $\mathcal{O}_U^{(I)}\to\mathcal{F}|_U$ where $\mathcal{O}_U^{(I)} = \bigoplus_{i\in I}\mathcal{O}_U$ . Now Schemes Lemma 10.1 states ""Let $(X,\mathcal{O}_X)$ be a scheme, $i:Z\to X$ be a closed immersion of locally ringed spaces: (1) The locally ringed space $Z$ is a scheme. (2) The kernel $\mathcal{I}$ of the map $\mathcal{O}_X\to i_*\mathcal{O}_Z$ is a quasi-coherent sheaf of ideals. (3) for every affine open $U = \operatorname{Spec}(R)$ of $X$ , the morphism $i^{-1}(U)\to U$ can be identified with $\operatorname{Spec}(R/I)\to\operatorname{Spec}(R)$ for some ideal $I$ of $R$ , and (4) we have $\mathcal{I}|_U = \widetilde{I}$ . In particular, any sheaf of ideals locally generated by sections is a quasi-coherent sheaf of ideals (and vice versa), and any closed subspace of $X$ is a scheme."" Question: It is this remark ""In particular..."" at the end which I do not understand at all. I don't even see where a module that is locally generated by sections appears in the statement, how am I supposed to conclude that a module locally generated by sections is quasi-coherent? I am quite new to scheme theory, so I would very much appreciate a reasonably detailed response.","['ringed-spaces', 'algebraic-geometry', 'schemes', 'quasicoherent-sheaves']"
