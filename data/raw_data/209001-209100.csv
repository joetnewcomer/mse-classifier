question_id,title,body,tags
4193283,Show that $X_n - Y_n\to 0$ in probability.,"Let $X_n$ converges to $X$ in distribution and $Y_n$ converges to $X$ in distribution. If $P(X_n - Y_n < -\epsilon) \rightarrow 0$ then show that $X_n-Y_n \rightarrow 0$ in probability. We know that $P[|X_n-Y_n|>\epsilon] = P[X_n -Y_n > \epsilon]+P[X_n-Y_n < -\epsilon]$ .
So, if it can be shown that $P[X_n-Y_n > \epsilon] \rightarrow 0$ then we are done. But how to show this?","['measure-theory', 'probability-limit-theorems', 'weak-convergence', 'probability-theory', 'probability']"
4193339,Constructing Diffeomorphism with prescribed derivative,"I am in the following situation: I am working in $S^1\times\mathbb{R}^2$ and have two
different frames for $T(S^1\times\mathbb{R}^2)$ , given by $X_1,X_2, X_3$ and $Y_1, Y_2=\partial_x, Y_3=\partial_y$ , both of which are defined only on $S^1$ ( $\partial_\theta$ is a non-zero vector field which is a positive basis for $TS^1$ ). Furthermore all the vector fields are nowhere tangent to $S^1\times\{0\}$ . I now want to find
a diffeomorphism $f$ from an open neighbourhood of $S^1\times\{0\}$ to an open
neighbourhood of $S^1\times\{0\}$ such that $f$ is given by the Identity on $S^1\times\{0\}$ for any $p\in S^1\times\{0\}$ we have $T_p f(X_i) = Y_i$ . Does anyone know of a theorem which would guarantee the existence of $f$ , so far I have only found Whiteney's Extension Theorem and the extension lemma (Lemma 2.26) in Lee's Introduction to Smooth Manifolds , both of which do not cover my case since there are restrictions on either the domain or range of $f$ .","['differential-topology', 'differential-geometry']"
4193340,Sum of three weighted logarithms with three different bases is equal to an integer,"Let $a, b$ and $c$ be real numbers, each greater than $1$ , such that $$\frac2
3
\log_b (a) +
\frac3
5
\log_c
(b) +
\frac5
2
\log_a
(c) = 3$$ If the value of $b$ is $9$ , then the value of $a$ must be Source: ISI BMath UGA 2017 I'm not very familiar on how to solve questions relating to logarithms of the above sort, but I started with the base change rule: $ \log_c (b) = \frac{1}{\log_b(c)}$ , $ \log_a (c) = \frac{\log_b (c)}{\log_b(a)}$ , $b=9$ , plugging the information in: $$ \frac23 \log_9 (a) + \frac35 \frac{1}{\log_9(c)} + \frac52 \frac{\log_9(c)}{\log_9(a)}=3 \tag{1}$$ Making the denominator as one: $$ \frac23 \log_9(a)^2 \log_9(c) + \frac35 \log_9 (a) + \frac52 \log_9 (a) \log_9(c)^2 = \log_9(c) \log_9(a)$$ This doesn't seem the way to go. Observation: The product of coefficient in the lhs of equation-(1) is just one. I thought maybe if I cube both sides then there will be a term of one on the LHS .. but the rest of the expressions is quite ugly, so it doesn't seem to be of much help.","['algebra-precalculus', 'logarithms']"
4193422,Diagonalization argument for convergence in distribution,"Let $Z_{n, N}$ be a sequence of random variable such that, for any fixed $N$ , this sequence of variables converges in distribution to a variable $Z$ as $n \rightarrow \infty$ . There exists a sequence $N(n)$ that goes to infinity ( $N(n) \rightarrow \infty$ as $n \rightarrow \infty$ ) such that $Z_{n, N(n)}$ converges to $Z$ in distribution as $n \rightarrow \infty$ ? How to construct the sequence $N(n)$ and prove the convergence in distribution of the resulting random variables $Z_{n, N(n)}$ ? A little bit of context: An argument similar to the one above is used in Terence Tao, ""Topics in Random Matrix Theory"" book under the name of ""diagonalization argument"" . In Section 2.2.1, the argument is used to show the possibility of considering bounded random variables to prove the central limit theorem without loss of generality. There $N$ is the upper bound for the variables and it is assumed that since central limit theorem yield $Z_{n, N} \rightarrow \mathcal{N}(0, 1)$ , for any fixed $N$ , it hold that $Z_{n, N(n)} \rightarrow \mathcal{N}(0, 1)$ for some sequence of bounds $N(n)$ dependent on $n$ .
.","['analysis', 'probability', 'real-analysis']"
4193425,Number Theory Proof for $\binom{pA}{pB} \equiv \binom{A}{B} \pmod{p^2}$,"I was doing some reading on Wolstenholme's theorem, which states that for prime $p$ , $\binom{pA}{pB} \equiv \binom{A}{B} \pmod{p^3}$ for $p\geq 5$ . The proof of the statement is outside my scope, but I was considering why it is still true for $\pmod p$ and $\pmod {p^2}$ . In Given integers $a \ge b > 0$ and a prime number $p$, prove that ${pa \choose pb} \equiv {a \choose b} \mod p$. , a fairly straightforward argument is given for the case of $\pmod p$ , where they find 2 equivalent expressions and equate them. I was wondering how I could prove this in similar spirit for the case of $\pmod {p^2}$ , without bringing in many additional lemmas, etc.","['binomial-coefficients', 'combinatorics', 'modular-arithmetic']"
4193627,Calculating $f(x)$,"Let $f(x)$ be a non-constant polynomial satisfying the relation $$f(x)f(y)=f(x)+f(y)+f(xy)-2; \forall x,y \in \Bbb R;\\ f(0)\ne1, f(4)=65,$$ By plugging $x=y=1$ , we get two values of $f(1)=1,2$ . But now which is the correct value? Similar when we do it for $x=0$ , $f(0)=1,2$ and following the question $f(0)=2$ . But as it is a non-constant polynomial $f(1)=1$ , is this argument correct? We can easily calculate all the values of $f(x)$ as we did for $x=0,1$ . We can also plug $y=\frac{1}{x}$ but now how should we calculate $f(x)$ . Because we have two values each for $f(1),f(2),f(3)....$ so on. But which is the correct one and what's the reason?","['functional-equations', 'derivatives', 'polynomials']"
4193668,"Simplify the product $\prod_{r=1}^n\left(\cos{\frac{2 \pi}{n}}+\sin{\frac{2\pi}{n}}\cdot \cot{\frac{(2r-1)\pi}{n}}\right)$, where $n$ is even","Question: Simplify the product as far as possible: $$\prod_{r=1}^n\left(\cos{\frac{2 \pi}{n}} + \sin{\frac{2\pi}{n}}\cdot \cot{\frac{(2r-1)\pi}{n}}\right),$$ where $n$ is even. The question is taken from a University entrance exam from the UK called the STEP, for which I am unable to find the solution to online. When attempting similar questions my approach was to substitute values in order to spot a pattern however this isn't really feasible in this question.","['number-theory', 'trigonometry', 'arithmetic', 'products']"
4193690,"$ABCD$ is a convex quadrilateral. If $\angle BAC=10°$, $\angle CAD=40°$, $\angle ADB=50°$, $\angle BDC=20°$, then find $\angle CBD$.","$ABCD$ is a convex quadrilateral. If $\angle BAC=10°$ , $\angle CAD=40°$ , $\angle ADB=50°$ , $\angle BDC=20°$ , then find $\angle CBD$ . The problem appeared in a local math contest last month. I know these kinds of angle chasing problems are called adventitious angles which is discussed in this wikipedia article . But as this is not a $80$ - $80$ - $20$ triangle, I think this is a different problem and I couldn't solve the problem completely. To understand this kind of angle chasing problems, I tried to solve the problem both in trigonometric and geometric way. Here are my workings to do that: Trigonometric solution: Let $P$ be the intersection of $AC$ and $BD$ . Then, applying sine rule in $\triangle PAB$ , $\triangle PBC$ , $\triangle PCD$ and $\triangle PAD$ and multiplying them we have $$\sin(x)\sin(70°)\sin(50°)\sin(10°)=\sin(90-x)\sin(20°)\sin(40°)\sin(80°)$$ $$\implies \tan(x)\tan(60°+10°)\tan(60°-10°)\tan(10°)=1.$$ Using calculator, I found $x=60°$ . But as the contest doesn't allow using calculators, I think the equation can't be solved without knowing the value of $\tan(10°)$ . So, I need a trigonometric solution that doesn't use calculators and doesn't use the value of $\tan(10°)$ . Geometric solution: To solve the problem geometrically, I noticed that $\triangle ABD$ and $\triangle ACD$ are both isosceles with $AB=BD$ and $AC=AD$ respectively. I tried to make an equilateral triangle then. But I don't know how to do that. I also tried drawing circles with center $B$ and arc $AD$ . But $C$ doesn't lie on the circle as $\angle ACD=70°$ . So, I couldn't proceed further. And same for circle with center $A$ and arc $CD$ . So, I need to complete my solutions. Any helpful idea is welcome.","['contest-math', 'euclidean-geometry', 'trigonometry', 'geometry']"
4193691,Prove that this is a measure,"I have problems with this exercise Let $X$ be an uncountable set, and let $\Sigma \subseteq{}\mathcal{P}
 (X) $ the $\sigma$ -algebra. $    $$\boldsymbol{\beta} = \{A \subseteq{X} : A$ it is countable or $A^c$ it is countable $\}$ Define $\mu: \boldsymbol{\beta} \to
 \overline{\mathbb{R}_{+}} $ ( $\mathbb{R}_{+} = [0, + \infty]$ ) as $\mu (A) = \left \{ \begin{matrix} 0 & \mbox{if }A\mbox{ is countable}
 \\ 1 & \mbox{if }A^c\mbox{ is countable}\end{matrix}\right. $ and prove that $\mu$ is a measure. My attempt: $\mu : C \rightarrow{\overline{\mathbb{R}}}$ is a measure if: i) $\mu( \emptyset ) =0$ ii) If ${A_n}_{n \in \mathbb{N}} \subseteq{C}$ such that $ \displaystyle\bigcup_{n \in \mathbb{N}}^{}{A_n } \in C$ and the $A_n$ 's are disjoint $$\mu(\displaystyle\bigcup_{n \in \mathbb{N}}^{}{A_n }) = \displaystyle\sum_{n \in \mathbb{N}}^{}\mu(A_n)$$ i) $\mu( \emptyset ) =0$ because the cardinality of $\emptyset = 0$ ii) Let $\{A_n\mid n \in \Bbb N\}$ a collection of sets of $\boldsymbol{\beta}$ disjoint two by two. Note that, since two by two are disjoint, there can be at most one with a countable complement. Then there are two possibilities: either they are all countable (and therefore their union is countable) or there is exactly one with a countable complement and the others are countable (and therefore the union has countable complement). Thanks","['measure-theory', 'analysis']"
4193696,Is there an upper bound on the trace of positive definite matrix as a function of the trace of the inverse of the matrix?,"Let $A$ be an $n \times n$ positive definite matrix. In this answer (based on a question I previously asked), a lower bound is given on $\text{tr}[A]$ as a function of $\text{tr}[A^{-1}]$ : $$\text{tr}[A] \geq \dfrac{n^2}{\text{tr}[A^{-1}]}$$ Is there also an upper bound? I can think of an upper bound on the trace as a function of the maximum eigenvalue of $A$ . For example, $$ \text{tr}[A] \leq n \lambda_{\max}[A] $$ However, I can't think of an upper bound as a function of the trace of the inverse of $A$ .","['inequality', 'trace', 'linear-algebra', 'upper-lower-bounds']"
4193707,"Limit of a sequence in $\mathbb{R}^3$ given by $x_{n+1}=\sqrt{x_n(y_n+z_n-x_n)}$, etc.","Someone else asked me this, so unfortunately I do not know its context. Put $(x_0,y_0,z_0)=(1,2,\sqrt{7})$ . Recursively define \begin{align}
x_{n+1}&=\sqrt{x_n(y_n+z_n-x_n)}\,, \\
y_{n+1}&=\sqrt{y_n(z_n+x_n-y_n)}\,, \\
z_{n+1}&=\sqrt{z_n(x_n+y_n-z_n)}\,.
\end{align} Prove that the sequences $(x_n)_{n\in\mathbb{N}}$ , $(y_n)_{n\in\mathbb{N}}$ and $(z_n)_{n\in\mathbb{N}}$ converge, and find their limits. My attempt: Note that \begin{align}
y_{n+1}z_{n+1}=\sqrt{y_nz_n(z_n+x_n-y_n)(x_n+y_n-z_n)}=\sqrt{y_nz_n[x_n^2-(y_n-z_n)^2]}\,.
\end{align} Hence $(y_n-z_n)^2<x_n^2<(y_n+z_n)^2$ implies $(y_{n+1}-z_{n+1})^2<x_{n+1}^2<(y_{n+1}+z_{n+1})^2$ . The same is true if $x,y,z$ are permuted. By induction, all $x_n,y_n,z_n$ are well-defined. Put $u_n=(x_n,y_n,z_n)$ for each $n$ . Then \begin{align}
\|u_n\|^2-\|u_{n+1}\|^2=2[(x_n-y_n)^2+(y_n-z_n)^2+(z_n-x_n)^2]\geq 0\,,
\end{align} as a decreasing sequence, $\|u_n\|\to\sqrt{3}\cdot\alpha$ as $n\to\infty$ for some $\alpha\geq 0$ . The distance from $u_n$ to the line $\ell$ given by $x=y=z$ is \begin{align}
d(u_n,\ell)=\sqrt{\frac{2}{3}[(x_n-y_n)^2+(y_n-z_n)^2+(z_n-x_n)^2]}\,,
\end{align} which converges to $0$ as $n\to\infty$ . Hence $u_n\to(\alpha,\alpha,\alpha)$ as $n\to\infty$ . What I cannot find analytically is the limit of $(u_n)_{n\in\mathbb{N}}$ . Numerical computation shows that $\alpha$ should be $\sqrt{2}$ . However, $\alpha$ is dependent on the choice of $u_0$ , whereas the method above does not.",['sequences-and-series']
4193774,Is there a common shorthand for the $n^{\text{th}}$ antiderivative of a function?,The antiderivative of a function $f(x)$ is commonly written as $F(x)$ . $n^{\text{th}}$ derivative is commonly written as $f^{(n)}(x)$ . Is there something for the $n^{\text{th}}$ antiderivative?,"['integration', 'notation', 'soft-question', 'derivatives']"
4193802,Is this integral zero: $\int_{-\infty}^{\infty} e^{-ipr}p\sqrt{p^2 + m^2} dp$?,"I am calculating a propagator in Quantum Field Theory and I get the following integral: $$\int_{-\infty}^{\infty} e^{-ipr}p\sqrt{p^2 + m^2} dp$$ where $r,m$ are constants. Now we see that the exponential can be split up into cosine and sine. Cosine is even and the remaining function is odd and so the $\cos(pr)p\sqrt{p^2 + m^2}$ is odd and so the integral with $\cos(pr)p\sqrt{p^2 + m^2}$ as the integrand is $0$ . Now, sine is odd and the remaining function is odd and so the $\sin(pr)p\sqrt{p^2 + m^2}$ is even. Thus the integral is $-2i\int_{0}^{\infty} \sin(pr)p\sqrt{p^2 + m^2} dp$ . Now I plotted the integrand $\sin(pr)p\sqrt{p^2 + m^2}$ and I noticed rapid oscillations. Does this mean the integral is $0$ ? If not, how do I evaluate this integral?","['integration', 'definite-integrals', 'fourier-transform']"
4193812,"I'm trying to find the point where a line intersects the YZ plane, but having trouble getting my solution to work.","I made a visual of what I'm trying to do here . I have a plane on the XY axis, and I'm trying to trace a ray and find where it lands on the imaginary wall on the other side. We know: Point where the ray intersects the XY plane (P) Normalized direction vector of the ray (V) The imaginary wall is perfectly aligned with the YZ axis, and goes on infinitely in the positive YZ directions. As you can tell from the link above, my idea was to approach it from the POV of a simple middle school algebra problem, where I break it down into two simple Y-intercept problems, one looking from the top and one looking from the side, to figure out the Y & Z coordinates of the intersection point. The point slope equations of the problem would look like: y - P_y = V_y(0 - P_x)

z - 0 = V_z(0 - P_x) and after simplifying them into slope-intercept form to get the coordinates of the intersection I get: x =  0

y = V_y(-P_x) + P_y

z = V_z(-P_x) It's one of those things that seems stupidly simple, but for some reason it's not working. I'm using it in a shader program and it's just giving me a bunch of random gradients like I dropped acid in the middle of trig class. Does anything stand out as something I'm doing wrong? Also, if anyone has a better solution than this, I'm all ears.","['algebra-precalculus', 'trigonometry']"
4193834,How many paths spell MICROWAVE,"M
   I I
  C C C
 R R R R
O X O O O
 W W W W 
  A A A
   V V V
    E E Starting with the M on top and only moving one letter at a time down to the left or right, how many different paths from top to bottom spell MICROWAVE? However you cannot move to the ""X"". I understand this uses Pascal's triangle but I'm not sure where to go once I reach the ""X"" since, it needs to be skipped. I would really appreciate it if someone could help me with this problem. 1
   1 1
  1 2 1 
 1 3 3 1
1 X 6 4 1
 W W 10 5
  A A A
   V V V
    E E The work above is when I tried to solve this using Pascal's triangle, but as you can see above, I'm not sure which number to put after for ""W"".",['combinatorics']
4193891,Evaluate $\int_{0}^{\frac{\pi}{2}} \ln(1+\sin^3 x)\text{d}x$,"Here's the integral that I would like to solve. Purely for recreational purposes: $$I=\int_{0}^{\frac{\pi}{2}} \ln(1+\sin^3x)\text{d}x$$ Here's my shot at it. I would like to stick to this method if possible. Let $I(\alpha)$ be defined as follows: $$I(\alpha) = \int_{0}^{\frac{\pi}{2}} \ln(1+\alpha\sin^3 x)\text{d}x$$ $$\implies I'(\alpha) = \int_{0}^{\frac{\pi}{2}} \frac{\sin^3x}{1+\alpha \sin^3x}\text{d}x$$ Let $t = \tan(\frac{x}{2})$ , and we have $$I'(\alpha) = \int_{0}^{1}\frac{\frac{8t^3}{(1+t^2)^3}}{1 + \alpha\frac{8t^3}{(1+t^2)^3}}\cdot\frac{2}{1+t^2}\text{d}t$$ I will now get rid of the fraction $$I'(\alpha) = \int_{0}^{1}\frac{\frac{8t^3}{(1+t^2)^3}}{1 + \alpha\frac{8t^3}{(1+t^2)^3}}\cdot\frac{2}{1+t^2}\cdot\frac{(1+t^2)^3}{(1+t^2)^3}\text{d}t = \int_{0}^{1} \frac{16t^3}{(1+t^2)(t^6 + 3t^4 + 8\alpha t^3 + 3t^2 + 1)}\text{d}t$$ Perform partial fractions (which was so time consuming): $$I'(\alpha) = \frac{-2}{\alpha}\int_{0}^{1} \left(\frac{(t^2+1)^2}{t^6+3t^4+8\alpha t^3+3t^2+1}-\frac{1}{1+t^2}\right)\text{d}t$$ The second integral is inverse tangent. How would I go about doing the first integral? It's a $4^{th}$ -degree polynomial over a $6^{th}$ degree polynomial, but parts of the polynomials look kind of simple.",['integration']
4193908,kolmogorov's extension theorem and consistent probability measure,"In Durrett's probability theory, Kolmogorov extension theorem is given as below: Theorem 2.1.21 (Kolmogorov's extension theorem) Suppose we are given probability measure $\mu_n$ on $(\mathbf{R}^n, \mathcal{R}^n)$ that are consistent, that is, $$\mu_{n+1}((a_1, b_1] \times \cdots \times (a_n, b_n] \times \mathbf{R}) = \mu_n((a_1, b_1] \times \cdots \times (a_n, b_n])$$ then there is a unique probability measure $P$ on $(\mathbf{R}^\mathbf{N}, \mathcal{R}^\mathbf{N})$ with $$P(\omega: \omega_i \in (a_i, b_i], 1 \le i \le n) = \mu_n((a_1, b_1] \times \cdots \times (a_n, b_n])$$ where $\mathbf{N}$ stands for natural numbers. My questions are: (1) are all probability measures consistent (by the above definition)? I'm assuming they're not but in the above condition, we can write $\mu_{n+1}$ as $\mu_n((a_1, b_1] \times \cdots \times (a_n, b_n]) \mu_1(\mathbf{R}) = \mu_n((a_1, b_1] \times \cdots \times (a_n, b_n])$ . This is always going to hold for any probability measure since $\mu_1(\mathbf{R}) = 1$ for any probability measure on a real line. (2) What are some examples of non-consistent probability measure? (3) the consistency condition is slightly different from the ones shown in Wikipedia where there are two consistency conditions, especially the permutation part: For all $t_1, \cdots , t_k \in T$ , $k \in \mathbb{N}$ let $\nu_{t_1,\cdots,t_k}$ be probability measures on $\mathbb{R}^{nk}$ s.t. $$\nu_{t_{\pi(1)},\cdots,t_{\pi(k)}} (F_1 \times \cdots \times F_k) = \nu_{t_1,\cdots,t_k} (F_{\pi(1)} \times \cdots \times F_{\pi(k)})  $$ for all permutations $\pi$ on $\{1,2, \cdots,k\}$ and $$\nu_{t_1,\cdots,t_k}(F_1\times \cdots \times F_k) = \nu_{t_1,\cdots,t_k, t_{k+1}, \cdots, t_{k+m}}(F_1\times \cdots \times F_k\times \mathbb{R}^n \times \cdots \times\mathbb{R}^n )$$ for all $m \in \mathbb{N}$ Is Durrett's condition of consistency weaker? How are they different?",['probability-theory']
4193950,Finding all polynomials $P(x)$ satisfying $(P(x)+P(\frac{1}{x}))^2 =P(x^2)P(\frac{1}{x^2})$,"Find all polynomials $P(x)$ satisfying $$\left(P(x)+P\left(\frac{1}{x}\right)\right)^2 =P(x^2)P\left(\frac{1}{x^2}\right)$$ If $P'(x)\neq 0$ , then $P(x) = a_nx^n+a_{n-1}x^{n-1}+...+a_1x+a_0$ $\Rightarrow$ ( $a_nx^n+a_{n-1}x^{n-1}+...+a_1x+a_0+a_n\frac{1}{x^n}+a_{n-1}\frac{1}{x^{n-1}}+...+a_1\frac{1}{x}+a_0)^2 = (a_nx^{2n}+a_{n-1}x^{2n-2}+...+a_1x^2+a_0)( a_n\frac{1}{x^{2n}}+a_{n-1}\frac{1}{x^{2(n-1)}}+...+a_1\frac{1}{x^2}+a_0)$ Considering the coefficient of $x^{2n-1}$ we see  : $2a_na_{n-1} = 0$ $\Rightarrow  a_{n-1}= 0$ Considering the coefficient of $x^{2n-3}$ we see  : $2a_na_{n-3}+2a_{n-1}a_{n-2}= 0$ $\Rightarrow  a_{n-3}= 0$ Similarly, we can prove $a_{n-5}=0;a_{n-7}=0;a_{n-9}=0;...$ But at this point I have no further ideas, it is certain that all the coefficients of $P(x)$ cannot simultaneously be zero except $a_n$ (because it is easy to see that $P(x) = Cx^n$ does not satisfy the requirement) Looking forward to getting help from everyone. Thank you very much! I sincerely apologize for my mistake. I have corrected my post, please forgive my ignorance.","['algebra-precalculus', 'polynomials']"
4193974,Is the function $f(x)=\sqrt[x+1]{\Gamma(x+2)}-\sqrt[x]{\Gamma(x+1)}$ decreasing?,"We know that the limit of $a_n=\sqrt[n+1]{(n+1)!}-\sqrt[n]{n!}$ is $$1/e=0.3678794411714423215955237701614608674458111310317678345078368016$$ , see What is $\lim_{n\to \infty }\left(\sqrt[\leftroot{-2}\uproot{2}n+1]{(n+1)!}-\sqrt[\leftroot{-2}\uproot{2}n]{n!}\right)$? Use Wolfram we have that $$a_1=\sqrt[2]{2!}-\sqrt[1]{1!}=0.4142135623730950488016887242096980785696718753769480731766797379$$ $$a_2=\sqrt[3]{3!}-\sqrt[2]{2!}=0.4029070304590446100895230321175624238585385877642715983046545599$$ $$a_3=\sqrt[4]{4!}-\sqrt[3]{3!}=0.3962432465685035259263687905534027946796259019185992795523061114$$ $$a_4=\sqrt[5]{5!}-\sqrt[4]{4!}=0.3918072452967087075081863770479530310194707973034612596596597742$$ ,... an so on.
From this data we can guess that $a_n$ is decreasing. More generally, let $$f(x)=\sqrt[x+1]{\Gamma(x+2)}-\sqrt[x]{\Gamma(x+1)},$$ where $\Gamma(x)$ is the Gamma function $$\Gamma(x)=\int_0^\infty e^{-t}t^x\dfrac{dt}{t}.$$ Plot the graph of $f(x)$ , we can see that $f$ decreases.  Can anyone prove this conclusion？","['gamma-function', 'inequality', 'analysis', 'real-analysis']"
4193981,Help in evaluating $\int\limits_{0}^\infty \frac{dx}{x}\frac{ e^{-x \alpha^2}}{\cos(x)}$,"Specifically, I'm interested in the real part of this integral: $$\int\limits_{\Lambda}^\infty \frac{dx}{x}\frac{ e^{-x \alpha^2}}{\cos(x)} $$ where $\Lambda$ is a real, positive constant that serves as a cutoff to regulate the pole at $x=0$ . Note that $\alpha$ is also real. Of course, there are infinitely many poles on the real axis at $x = n \pi /2$ for odd $n$ . These can be regulated by an analytic continuation in which we get a sum in $n$ of imaginary contributions (one for each pole), so I'm able to solve the imaginary part. The real part is then the Cauchy principal value of the integral: $$\mathcal{P}\int\limits_{\Lambda}^\infty \frac{dx}{x}\frac{e^{-x \alpha^2}}{\cos(x)} $$ though I'm stuck when I try to evaluate this by the definition of Cauchy principal value. To simplify, I've tried expanding the $\sec(x)$ but run into problems with the expansion's radius of convergence (I have made progress with an asymptotic expansion in the limit of large $\alpha$ , but this limit is not ideal in my case). I've also tried a contour integral with the residue theorem in which I take a half semi-circle contour in the first quadrant of the complex plane. This leaves me to solve the real part of: $$\int\limits_{\Lambda}^\infty \frac{dx}{x}\frac{ e^{-i x\alpha^2}}{\cosh(x)} $$ This appears simplified as we're no longer dealing with infinitely many poles, but I'm having difficulty solving it as well.
(Note that there will be an $\Lambda$ -dependent quarter-arc integral that does not vanish when doing the contour integration, but I'm specifically able to ignore this in my use-case.) Any ideas appreciated. Thanks!","['integration', 'improper-integrals', 'complex-integration', 'definite-integrals']"
4194006,find $(p+q)^3$ when $\sec x - \tan x = 2$,"I have a question. Let $x$ be a real number such that $\sec x - \tan x = 2$ If the value of $\sec x + \tan x=\frac{p}{q}$ where $p,q$ are integers, find $(p+q)^3$ What I have tried so far: Taking the equation: $\sec x - \tan x = 2$ $\tan x+2=\sec x$ Now substituting that into $\sec x + \tan x=\frac{p}{q}$ $\tan x+2 + \tan x=\frac{p}{q}$ $= 2\tan x + 2=\frac{p}{q}$ So $2\tan x=\frac{p}{q}-\frac{2q}{q}$ $= 2\tan x=\frac{p-2q}{q}$ I am not sure about this. How can I simplify it further to find the values of $p$ and $q$",['trigonometry']
4194145,Prove that the equation $Q(P(x)) = 1$ has no integer solution,"Let the polynomials $P(x) , Q(x)\in \mathbb Z[x]  $ and integer $ a$ satisfy: $P(a)= P(a+2015) = 0 $ $Q(2014)=2016$ Prove that the equation $Q(P(x)) = 1$ has no integer solution . Assume that equation $Q(P(x)) = 1$ has an integer  solution $\Rightarrow $ Equation $Q(x) = 1$ has  at least one integer solution. $\Rightarrow $ $Q(x)-1 = (x-m)R(x)$ $\Rightarrow $ $Q(2014)-1 = (2014-m)R(x)$ $\Rightarrow $ $2015 = (2014-m)R(x)$ $\Rightarrow $ $(2014-m)$ $\in$ { ${\pm 1 ; \pm 2015 ; \pm 5 ; \pm 403 }$ } $\Rightarrow $ $m$ $\in$ { ${ 2013;2015;-1;4029;2009;2019;1611;2417  }$ } $P(a)= P(a+2015) = 0 $ $\Rightarrow $ $a$ and $(a+2015)$ are solutions of the polynomial $P(x)$ $\Rightarrow $ $P(x) = (x-a)(x-a-2015)g(x) $ But at this point, I have no more ideas. I'm so sorry everyone for my stupidity. I hope to get help from everyone. Thank you very much !","['algebra-precalculus', 'polynomials']"
4194150,Quadrilateral in which diagonal is partially outside?,"I just want an example of a quadrilateral in which a diagonal lies ""partially outside"" the quadrilateral. By that, I mean that some part of a diagonal must be outside and some part inside the quadrilateral. Obviously the quadrilateral will be concave iff one diagonal lies completely outside the figure, but now, I saw a problem related to concave quadrilateral, so to be sure, I looked it up and in the definition, it said Concave quadrilaterals are four sided polygons that have one interior angle greater than $180^{\circ}$ . We can identify concave quadrilaterals by using the fact that one of its diagonals lie partially or completely outside the quadrilateral. So, I tried to come up with a concave quad with diagonal partially outside, like equilateral triangle with centroid and so forth, but none worked, So,Tldr; I just need one example a (concave) quadrilateral in which diagonal lies partially outside.","['quadrilateral', 'trigonometry', 'geometry']"
4194168,How is calculus useful in economics; a subject where the quantities studied are often discrete?,"Problem statement: It costs: $$c(x)=x^{3}-6x^{2}+15x$$ dollars to produce x toys when 8 to 30 toys are produced and that $$r(x)=x^{3}-3x^{2}+12x$$ gives the dollar revenue from selling $x$ toys. Your toy shop currently produces $10$ toys a day. About how much extra will it cost to produce one more toy a day, and what is your estimated increase in revenue for selling 11 toys? Solution: The cost of producing one more toy a day when 10 are produced is about $c'(x)=3x^{2}-12x+15$ and $c'(10)=195$ . The additional cost will be about $195$ dollars. The marginal revenue is: $r'(x)=3x^{2}-6x+12$ . The marginal revenue function estimates the increase in revenue that will result from selling one additional unit. If you currently sell $10$ toys a day, you can expect your revenue to increase by about $r'(10)=252$ dollars if you increase sales to 11 toys a day. Why do we use calculus here? I can already calculate the cost and the revenue of selling $11$ toys by calculating $c(11)$ and $r(11)$ , and then I could calculate the increase in the cost and revenue by simply calculating $c(11)-c(10)$ and $r(11)-r(10)$ . However, if I do that, the result is not the same. I'm confused here. I'd be glad if you could help me with this one. Thank you so much in advance!!!
Cheers",['calculus']
4194173,Prove that any smooth submanifold $H\le G$ of a Lie group is closed,"Let $G\subset{\rm GL}(n,\mathbb R)$ be a matrix Lie group, with Lie algebra $\mathfrak g\equiv T_{I}G$ . Let $H\le G$ be a subgroup of $G$ , and let $\mathfrak h$ be its Lie algebra. Denote with $\mathfrak h^\perp$ the orthogonal complement of $\mathfrak h$ with respect to some inner product. I want to prove that, if $H$ is a smooth submanifold of $G$ , then $H$ must be closed. A way to prove this is given in these pdf notes (page 62, theorem 2.5.26). The idea of the proof, as far as I understand it, is as follows: Take a converging sequence $(h_i)_i\subset H$ with $h_i\to g\in G$ . We want to prove that $g\in H$ . Observe that $h_i^{-1}g\to I$ , where $I\in G$ is the identity in $G$ . Observe that there is a neighbourhood of $I\in G$ all the elements of which can be uniquely represented as $h e^\xi$ for some $h\in H$ and $\xi\in\mathfrak h^\perp$ . This implies that for all $i$ large enough, there are unique $h_i'\in H$ and $\xi_i\in\mathfrak h^\perp$ (taken in suitable neighbourhoods of $I\in H$ and $0\in\mathfrak h^\perp$ , respectively) such that $g=h_i h_i' e^{\xi_i}$ . Observe that any $\xi,\xi'\in\mathfrak h^\perp$ with small enough norm and such that $e^\xi e^{-\xi'}\in H$ must be equal: $\xi=\xi'$ . Because $\xi_i\to 0$ , conclude that for large enough $i$ we must have $\xi_i=0$ , and thus $g=h_i h_i'\in H$ . What I'm trying to understand better is the role of $\mathfrak h^\perp$ in this reasoning. Does it have to do with $\xi\in\mathfrak h^\perp$ representing a ""direction on $G$ orthogonal to those spanning $H$ ""? What is an intuitive argument that might lead one to guess that this way of decomposing elements close to the identity $I\in G$ is suitable to prove this result?","['lie-algebras', 'proof-explanation', 'smooth-manifolds', 'lie-groups', 'differential-geometry']"
4194227,Proving $\frac{2(1+y)\sqrt{1+x}+y\sqrt{1+y}}{2(1+x)\sqrt{1+y}+x\sqrt{1+x}} = \frac{1}{(1+x)^2}$ with $x\sqrt{1+y}+y\sqrt{1+x}=0$,"I want to find $\frac{dy}{dx}$ in $x\sqrt{1+y}+y\sqrt{1+x}=0$ and I proceed through 2 different ways expecting the same answer. Method:-1 $x\sqrt{1+y}=-y\sqrt{1+x}$ $\implies x^2(1+y)=y^2(1+x)$ $\vdots$ $\implies y=-1+\frac{1}{1+x}$ $$\frac{dy}{dx}=\frac{-1}{(1+x)^2}$$ Method:-2 $\phi(x,y)=x\sqrt{1+y}+y\sqrt{1+x}=0$ $$\frac{dy}{dx}=-\frac{\frac{\partial {\phi}}{\partial x}}{ \frac{\partial {\phi}}{\partial y}  }$$ $$\frac{dy}{dx}=-\frac{2(1+y)\sqrt{1+x}+y\sqrt{1+y}}{2(1+x)\sqrt{1+y}+x\sqrt{1+x}}$$ But I could not find any way to convert $\frac{2(1+y)\sqrt{1+x}+y\sqrt{1+y}}{2(1+x)\sqrt{1+y}+x\sqrt{1+x}}$ to $\frac{1}{(1+x)^2}$ . Thank you for your help!","['calculus', 'derivatives', 'arithmetic']"
4194285,Set difference of an open and a bounded set is open?,"An exercise in a course material i'm following doesn't seem right, it asks to show the following: Let $X$ be a metric space and $G\subset X$ open. If $F\subset G$ is bounded, then $G\setminus F$ is open. If we place no extra restrictions on the set $F$ , then we can for example choose $X=\mathbb{R}^2$ with the Euclidean metric, $G=B(0,1)$ and $F=B(0,1)\setminus \{0\}$ . Then $G\setminus F=\{0\}$ , which certainly is not open and thus presents a counterexample? Am i very confused or is the exercise flawed?","['elementary-set-theory', 'general-topology', 'real-analysis']"
4194377,"If $n$ can be expressed as $2x^2+3y^2$ with integers $x$ and $y$, then so can $7n$.","The problem is: Let us say that a positive integer $n$ is obtainable if there exist integers $x$ and $y$ such that $n=2x^2+3y^2$ . If $n$ is obtainable, prove that $7n$ is also obtainable. I tried doing that $$2(x+a)^2+3(y+b)^2=14x^2+21y^2$$ which simplifies to $$4ax+2a^2+6yb+3b^2=12x^2+18y^2$$ but then I seem to be caught in a loop. Am I on the right direction and can someone give me a hint? This is a homework question btw.","['contest-math', 'number-theory', 'algebra-precalculus']"
4194380,Closed form of $\sum_{n=1}^{\infty} \frac{1}{n(e^{2\pi n}+1)}$,"In this question, there's a quite simple closed form mentioned for the series $$\sum_{n=1}^{\infty} \frac{1}{n(e^{2\pi n}-1)}$$ However I'm wondering if there exists any simple closed form known for the series of type $$\sum_{n=1}^{\infty} \frac{1}{n(e^{\pi n}+1)}, \quad \sum_{n=1}^{\infty} \frac{1}{n(e^{2\pi n}+1)},\quad \sum_{n=1}^{\infty} \frac{1}{n(e^{(2k+1)n\pi }+1)},\quad \sum_{n=1}^{\infty} \frac{1}{n(e^{(2k)n\pi}+1)}$$ I've been thinking about thee kinds of series from quite some time and so far
I have tried contour integration, cotangent partial fraction, and applying Poisson summation or converting to an integral but so far no success. Any help would be highly appreciated!","['analytic-number-theory', 'number-theory', 'closed-form', 'sequences-and-series']"
4194396,How to use calculus to solve the equation: $(x^3-4)^3=\left(\sqrt[3]{(x^2+4)^2}+4\right)^2$.,"I've recently started learning calculus. Someone told me that some equations can be solved using derivatives. I was  interested about that and I wanted to learn how to do that. As an example problem, I tried to solve an equation using derivatives. Here is the equation: Problem: Solve for $x \in \mathbb R$ : $$(x^3-4)^3=\left(\sqrt[3]{(x^2+4)^2}+4\right)^2.$$ (The problem is taken from this question , which was posted a few days ago. I answered the question with an algebraic solution and now I want to know about the solution which uses calculus.) Here is my solution to the problem. I want to know whether my solution is correct or not and I also have some questions regarding the solution. My solution: It is easy to see that $x\geq \sqrt[3]4$ for equality. And we notice that $x=2$ is a solution. Let $f(x)=(x^3-4)^3$ and $g(x)=\left(\sqrt[3]{(x^2+4)^2}+4\right)^2$ defined on $[\sqrt[3]4,+\infty)$ . Then, $f'(x)=9x^2(x^3-4)^2$ and $$g'(x)=\frac{8x}{3}(\sqrt[3]{x^2+4})+\frac{8x}{3\sqrt[3]{x^2+4}}.$$ Clearly, $f'(x)>g'(x)$ in the interval. The graph of $f$ is more increasing than $g$ after the equality. So, $f$ and $g$ is can't hold equality for $x>2$ , which implies $x=2$ is the only solution to the equation. The solution says there is no equality for $x>2$ . But how to know there is no equality points in $[\sqrt[3]4,2)$ ? I need an explanation for that. (It would be easy for me if the answer is in terms of graphs of the functions as I don't know much calculus.) And is this kind of solution to an equation suitable for contests i.e. olympiads (I want to know this because I participate in various contests.)? Note: The above solution has some errors. I need to prove that $f(x)$ and $g(x)$ can't be equal except $x=2$ . My workings are not correct as figured out in comments.","['contest-math', 'calculus', 'algebra-precalculus']"
4194424,Subspace of 3D Rotation Group that Provides Unique Cube Orientations,"Let me begin by saying that I am a mechanical engineering student without much of a math background. I am attempting to write a program which requires a selection of unique cube orientations sampled from all orientations as uniformly as possible... no repeated orientations are allowed and the selected orientations should not be too similar (perhaps something like a minimum $5^\circ$ separation about any axis). The cube is completely uniform, so I define uniqueness between two given orientations by whether or not a symmetry transform relates them. If they are related by a symmetry transform - they are not unique.  I am aware that the cube symmetry group is quite large with 24 rotational symmetries existing for any given orientation. In my mind, I picture the 3D rotation group as a vector space with elements $(\theta_x,\theta_y,\theta_z)$ . In trying to tackle this problem, I am attempting to imagine what that vector space would look like with all the rotations that would lead to duplicate cube orientations deleted. If an asymmetric object's unique orientations completely fill the 3D rotation group space, what portions can be deleted in the case of a cubical object? I am also aware that the 3D rotation group can be represented by the space of $3\times 3$ orthogonal matrices. Perhaps there are some ideas from linear algebra that may be directly applicable to this problem. More Concisely (Thank you user7530) For a uniform cube, define sufficiently unique cube orientations as those that have significant difference in orientation under all symmetry transformations. Does $SO(3)/O$ , where $O$ is the octahedral group, have a known structure that may facilitate the selection of sufficiently unique cube orientations? Any advice would be greatly appreciated!","['representation-theory', 'linear-algebra', 'symmetric-groups', 'group-theory', 'symmetry']"
4194440,Where's my mistake evaluating the mass of $V$?,"Evaluate the mass of $V=\{ (x,y,z)|x^2+y^2+z^2\le 2, z \ge 0, x^2+y^2 \ge 1$ } While the density of mass is $\phi(x,y,z)=z$ What I did: How I visualize $V$ : The first part is a ball with a radius $2$ (it's volume), $z\ge 0$ makes me take the upper half of the ball, and for $x^2+y^2 \ge 1$ , I drew the cylinder $x^2+y^2=1$ inside the half ball, and $V$ is the volume between the ball and the cylinder. So I decided to use ball coordinates to solve: $x=r\cos\theta \sin\phi$ . $y=r\sin\theta \sin\phi.$ $z=r\cos\phi$ $|J|=r^2\sin\phi$ . And from how I visualized $V$ , I set the bounds as this: $1 \le r \le 2$ . (from the cylinder to the ball) $0\le \phi \le \frac{\pi}{2}$ . (from the positive $z$ axis to xy plane - half ball). $0 \le \theta \le 2\pi$ . (must go around all $V$ ). And so, $$\int_0^{\pi/2}d\phi \int_0^{2\pi}d\theta\int_1^2r^3\sin\phi\cos\phi=2\pi[4-\frac{1}{4}]\frac{1}{2}\int_0^{\pi/2} \sin(2\phi)d\phi = \pi[4-\frac{1}{4}]$$ But the final answer is : $\frac{\pi}{4}$ . Would love to know which mistakes I made, thanks in advance.","['integration', 'multivariable-calculus']"
4194450,Different answers of same differentiation Question with two different methods,"Question:- Find $\frac{dy}{dx}$ if $$\arccos\bigg({\frac{x^2-y^2}{x^2+y^2}}\bigg)=\arctan( a)$$ $$\arccos\bigg({\frac{x^2-y^2}{x^2+y^2}}\bigg)=\arctan( a)$$ $$\implies  \frac{x^2-y^2}{x^2+y^2}=\cos(\arctan(a))$$ Taking derivative on both sides w.r.t $\space x$ , we get $\frac{dy}{dx}=\frac{y}{x}$ Out of fun, I tried the substitution $y^2=x^2\cos(\theta)$ and got $$\frac{1-\cos(\theta)}{1+\cos(\theta)}=\cos(\arctan(a))$$ for $x\ne 0$ $$\implies \tan^2{\frac{\theta}{2}}=\cos(\arctan(a))$$ Differentiate both sides w.r.t $\space \theta$ , we get $$\tan{\frac{\theta}{2}}\bigg(1+\tan^2{\frac{\theta}{2}}\bigg)=0$$ $$\implies \tan{\frac{\theta}{2}}=0$$ $$\cos(\theta)=\frac{1-\tan^2{\frac{\theta}{2}}}{1+\tan^2{\frac{\theta}{2}}}=1$$ As $y^2=x^2\cos(\theta)$ ,this gives $y^2=x^2$ and $\frac{dy}{dx}=\pm 1$ Which contradicts the other method, So What's wrong with the $2^{nd}$ method?","['calculus', 'derivatives']"
4194452,Does weak convergence of measures preserve independence of marginals?,"Let $X^n = (X^n_1, \dots, X^n_d) ~ q^n$ be a $d$ -dimensional random variable, where all the components are independent. That is, $X_i \perp X_j$ for $i\neq j$ , and $$q^n(X) = \prod_{i=1}^d q^n_i(X^n_i).$$ If the sequence of measures $q^n$ converges weakly to some $q^*$ , then are the resulting marginals also independent? I.e., is $$q^*(X)= \prod_{i=1}^d q^*_i?$$ Edit: convergence of marginals/joints Let the state space be $\Omega^d$ , and $\Omega$ be the state space of each component (i.e. $X_i \in \Omega$ for all $i$ ). If $q^n \xrightarrow{w} q$ , then by definition of weak convergence, $$\int f(X) q^n(dX) \rightarrow \int f(X) q(dX) \;\; \text{ for all } f \in C_b(\Omega^d) \;\;\; \text{ as }n\rightarrow \infty.$$ Since we can take $f$ to have any support in $\Omega^d$ , the above convergence of integrals holds for any combination of components; for example if $f$ has support on the first component only, then $$\int_\Omega f(X_1) q^n_1(dX_1) \rightarrow \int_\Omega f(X_1) q(dX_1).$$ I.e. the marginals converge.  This is also true for any joint, e.g. $q^n(X_1,X_2) \xrightarrow{w} q(X_1,X_2)$ . Since $q^n(X_i|X_j)q^n(X_j) = q^n(X_i,X_j)$ , and we know that $q^n(X_i,X_j) \xrightarrow{w} q(X_i,X_j)$ and $q^n(X_i) \xrightarrow{w} q(X_i)$ , then it seems like $q^n(X_1|X_2) \xrightarrow{w} q(X_1|X_2)$ would be true. Here's an attempt: for all $X_2 \in \Omega$ , \begin{align}
\int q^n(X_1|X_2)f(X_1) dX_1 &= \int \frac{q^n(X_1,X_2)}{q^n(X_2)}f(X_1)dX_1 
\\
&= \frac{1}{q^n(X_2)} \int q^n(X_1,X_2)f(X_1)dX_1 
\\
&\rightarrow  \frac{1}{q^n(X_2)} \int q(X_1,X_2)f(X_1)dX_1
\\
&\stackrel{?}{\rightarrow} \frac{1}{q(X_2)} \int q(X_1,X_2)f(X_1)dX_1 
\\
&= \int q(X_1|X_2)f(X_1)dX_1 
\end{align} I don't think "" $\stackrel{?}{\rightarrow}$ "" is valid under weak convergence since this would require pointwise convergence. Is this where the proof breaks?","['measure-theory', 'probability-distributions', 'marginal-distribution', 'weak-convergence']"
4194509,Is it true that $M\boxtimes N = p_1^* M\otimes_{\mathcal{O}_{X\times X}}p_2^* N$ for D-modules?,"Let $X$ be a smooth algebraic variety over $\mathbb{C}$ and consider the projections $p_1,p_2:X\times X\to X$ . If $M$ and $N$ are left $\mathcal{D}_X$ -modules, their exterior tensor product is defined as $$M\boxtimes N := \mathcal{D}_{X\times X}\otimes_{p_1^{-1}\mathcal{D}_X\otimes_{\mathbb{C}}p_2^{-1}\mathcal{D}_X}(p_1^{-1}M\otimes_{\mathbb{C}}p_2^{-1}N).$$ Question: why isn't this defined simply as $p_1^* M\otimes_{\mathcal{O}_{X\times X}}p_2^* N$ , where a differential operator acts via the Leibniz rule? I imagine both definitions coincide, but I can't see how. (Obs: there was a second question here before which, as I observed in the comments, follows rather directly from the main question. So I decided to focus the post in the hopes of getting an answer.)","['algebraic-geometry', 'd-modules', 'sheaf-theory']"
4194511,solve: $ \frac {d^2y}{dx^2} + 5\frac{dy}{dx} = 15x^2$,"$$ \frac {d^2y}{dx^2} + 5\frac{dy}{dx} = 15x^2 $$ It's solution is $$y = y_{h} + y_{p}$$ where $y_{h} $ is the solution for homogenous equation and $y_{p} $ is the particular solution For homogenous solution:
It's auxiliary equation is $$m^2 + 5m = 0$$ on solving for m, $$m = 0, -5$$ so, $$y_{h} = C_{1} + C_{2} e^{-5x}$$ here $$R = 15x^2$$ so it's particular solution should be : $$y_{p} = K_{2}x^2 + k_{1}x + k_{0}$$ on differentiating with respect to x $$y\prime _{p}= 2xk_{2} + k_{1}$$ again $$y\prime\prime_{p} = 2k_{2}$$ using all above values in the initial equation gives $$2k_{2} + 10xk_{2} + 10k_{1} = 15x^2$$ But using this method I am unable to proceed for the value of all unknown especially $k_{0}$ The given answer is: $$y = C_{1} + C_{2}e^{-5x} + x^3 + \frac{3x^2}{5} - \frac{6}{25}x$$",['ordinary-differential-equations']
4194622,Are Sperner's Lemma and Brouwer's Fixed Point Theorem equivalent?,"On this Wikipedia link I found the following statement: In mathematics, Sperner's lemma is a combinatorial analog of the Brouwer fixed point theorem, which is equivalent to it. At first, I found this statement very strange. How can two theorems be equivalent? Of course you could say that $$``Sperner's~lemma~is~true""~\iff~``Brouwer~fixed~point~theorem~is~true""$$ since both statements are, in fact, true, but you could say the same of any two theorems proved true, and I think I would be considered a fool to say that Fermat's little theorem is equivalent to Pythagoras' theorem. The thing is, it makes sense to talk about property equivalence , and I found that Brouwer's fixed point theorem is, in fact, related to a property of topological spaces. A topological space $(X, \tau)$ is called a fixed-point domain if any continuous function $f:X\rightarrow X$ has a fixed point. Later I realized that ""equivalent"" probably just referred to having a demonstration of one passing through the other and vice versa, but my confusion motivated a strong curiosity: Is there any property about topological spaces that is equivalent to ""being a fixed-point domain"" and somehow generalizes Sperner's lemma? I know that "" triangulation "" is a well-defined concept in topology, which leads me to believe that the answer to my question might be yes. An answer to the case where $X$ is a subspace of $\Bbb{R}^n$ would already satisfy me a lot.","['graph-theory', 'general-topology', 'coloring', 'fixed-point-theorems']"
4194686,"Does there exist a subgroup of $GL(n,\mathbb F_2)$ that requires $n$ generators, when $n>2?$","In the answer to this previous question , we showed that, for prime powers $q>2,$ $GL(n,q)=GL(n,\mathbb F_q)$ has a subgroup $H$ that requires $n$ elements to generate it. But over $\mathbb F_2$ there isn’t always such a subgroup. $n=3$ is known to have no subgroup requiring more than $2$ generators. For each $n,$ the subgroup $U$ of upper-triangular matrices requires $n-1$ generators, at least. That’s because there is an onto homomorphism: $U\mapsto \mathbb F_2^{n-1}$ with $(a_{ij})\mapsto (a_{k(k+1)})_{k=1}^{n-1}.$ Any generators for $U$ must, in the image, generate $\mathbb F_2^{n-1},$ and this requires $n-1$ generators. But I believe the matrices with one off-diagonal, $A_k, k=1,\dots,n-1$ with: $$(A_k)_{ij}=\begin{cases}1&i=j\\1&i=k,j=k+1\\0&\text{otherwise}\end{cases}$$ generate $U.$ I found a paper behind a paywall whose title implies that the whole group $GL(n,2)$ is generated by $2$ elements for any $n.$ If true, that means we’d want proper subgroups. Other than subgroups mapping onto abelian groups, I don’t have a lot of tools for solving this problem. I know all finite simple groups require only $2$ generators (or $1$ if abelian.) (Apparently, this is a deep result, currently requiring the classification of finite simple groups.) Aside: When $q=p^k$ the upper triangular matrices with $1$ along the diagonal in $GL(n,q)$ require at least $k(n-1)$ generators, which can be significantly more than $n$ when $k$ is big. For example, $q=8,n=3$ these upper-triangular matrices requires at least $6$ generators.","['general-linear-group', 'group-theory', 'finite-groups']"
4194696,Integration by rational substitution,"I was taking a look at a proof of the Riemann-Liouville Integral of consecutive integrations, and at some point I reached a step where it shows the following substitution: $$_{a}I_{x}^\alpha(_{a}I_{x}^\beta f(x))=\frac{1}{\Gamma(\alpha)\Gamma(\beta)}\int_{a}^x \int_\zeta^x(x-t)^{\alpha-1}(t-\zeta)^{\beta-1}f(\zeta)\mathop{dt}\mathop{d\zeta}$$ using the substitution $u=\frac{t-\zeta}{x-\zeta}$ we finally obtain $$_{a}I_{x}^\alpha(_{a}I_{x}^\beta f(x))=\frac{B(\beta,\alpha)}{\Gamma(\alpha)\Gamma(\beta)}\int_{a}^x (x-\zeta)^{\alpha+\beta-1}f(\zeta)\mathop{d\zeta}=\,_{a}I_{x}^{\alpha+\beta} f(x)$$ My question is, is there any motivation to make such a substitution as $u=\frac{t-\zeta}{x-\zeta}$ ? I mean, is some kind of special rational substitution? it reminds me to Möbius transformation somewhow but i can't connect the ideas. Edit: After reading Markus Scheuer reply, I've found the next document: Aygören, Aysel - Fractional Derivative and Integral [2014], pp 27-29","['integration', 'substitution', 'fractional-calculus']"
4194722,Exercise I.6.5 of Hartshorne,"Let X be a nonsingular projective curve. Suppose that X is a (locally closed) subvariety of a variety Y (Ex. 3.10). Show that X is in fact a closed subset of Y. See (II, Ex. 4.4) for generalization. If we use the fact that regular morphism defined in Harris (or proper morphism) is closed, then this is just direct consequence of the fact. However, it seems that Hartshorne did not introduce any of such notions of proper map in Chapter I. Is there a way to prove this using materials of Chapter I? What I also tried to do is to assume $\overline{X}$ in $Y$ and apply Theorem 6.8; however, since we may not assure that whether $\overline{X}$ is nonsingular or not, this doesn't work.","['algebraic-geometry', 'commutative-algebra']"
4194726,Is a differential equation involving a multivariate function and exactly one of its partial derivatives an ODE?,"I haven't taken any class in differential equations, so most of what I know about them is from small lectures in other classes. Please forgive my naivete. According to someone in this this post , the answer is yes: a differential equation involving a multivariate function and exactly one of its partial derivatives is indeed an ODE. However, I can't understand why. If we have $f(x,y) = y * \frac{df(x,y)}{dx}$ , don't the various values we can plug into independent variable $y$ affect the output of both $f(x,y)$ and $\frac{df(x,y)}{dx}$ ? Therefore, since we have multiple independent variables to worry about, we should treat this equation more as a PDE than ODE. Please feel free to correct my way of thinking as I am a greenhorn.","['ordinary-differential-equations', 'partial-differential-equations']"
4194787,How to show implication for equivalent equations?,"What does it mean for one event 𝐶 to cause another event 𝐸 — for example, smoking (𝐶) to cause cancer (𝐸)? There is a long history in philosophy, statistics, and the sciences of trying to clearly analyze the concept of a cause. One tradition says that causes raise the probability of their effects; we may write this symbolically is
𝑃(𝐸|𝐶)>𝑃(𝐸).(1) Another way to formulate a probabilistic theory of causation is to say that
𝑃(𝐸|𝐶)>𝑃(𝐸|𝐶^𝐶).(2) I know that equation (1) implies that 𝑃(𝐶|𝐸)>𝑃(𝐶). Show that equation (1) implies equation (2)","['conditional-probability', 'statistics', 'probability']"
4194824,Showing there is exactly one limit cycle for system in unit cirlce,"Consider the system defined by below equations: $$\dot{x}=x+y-a^2x(x^2+y^2)$$ $$\dot{y}=-x+y-a^2y(x^2+y^2), \quad a>1$$ I'm asked to show there is only $1$ limit cycle in unit circle. I think using Poincare-Bendixon implies there is a limit cycle but I have difficulty in showing that there is no equilibrium point in unit circle. I have no idea for showing there is only $1$ such a limit cycle. I think using polar coordinates may help but I don't know how to use it. Any help is appreciated.","['ordinary-differential-equations', 'dynamical-systems']"
4194864,Shape of very long wire between two very tall posts (many km tall) which are attached to the earth,"Consider for a moment a length of uniform wire or chain which goes between two posts of equal height. If we assume the earth to be flat then we can predict the shape of the curve using $$y = a \cosh \left(\frac{x}{a}\right)$$ But if we had a very long wire between two very tall posts which were angle theta apart on a planet which is a perfect sphere then what would the equation be which gives the height of the wire as a function of the angle between the two posts. Note that in the ""flat earth"" wire example the gravitational field strength which the wire is in will be constant, while in the case of a wire which goes between two very tall towers the gravitational field strength is not constant. This is likely to make the problem more complex.","['physics', 'geometry']"
4194868,Definition of sine and cosine [duplicate],"This question already has answers here : Are $\sin$ and $\cos$ the only functions whose derivatives are equal to each other up to a sign? [closed] (5 answers) Closed 2 years ago . I've seen Sine and Cosine defined as the unique solution to: $$\begin{align}
\frac d{dx} \sin(x) &= \cos(x)\\  
\frac d{dx} \cos(x) &= -\sin(x)
\end{align}$$ with $\sin(0) = 0$ and $\cos(0) = 1$ . Is there really only one solution to these functions? How can these functions be defined more formally?","['calculus', 'trigonometry']"
4194890,We have $(x^2+y^2)^2-3(x^2+y^2)+1=0$. What is the value of $\frac{d^2y}{dx^2}$?,"We have $(x^2+y^2)^2-3(x^2+y^2)+1=0$ . What is the value of $\frac{d^2y}{dx^2}$ ? $1)-\frac{x^2+y^2}{y^2}\qquad\qquad2)-\frac{x^2+y^2}{y^3}\qquad\qquad3)\frac{x+y}{x^2+y^2}\qquad\qquad4)\frac{xy}{x^2+y^2}$ Here is my approach: We have a quadratic equation in $x^2+y^2$ . So $x^2+y^2=\frac{3\pm\sqrt5}{2}$ (RHS is a constant). Taking differentiate with respect to $x$ , $$2x+2yy'=0\Rightarrow\quad y'=\frac{-x}{y}$$ $$2+2y'^2+2yy''=0\Rightarrow \quad y''=\frac{-y'^2}{y}\Rightarrow y''=\frac{-x^2}{y^3}$$ Edit: The second option can be written as $-(\frac{x^2}{y^3}+\frac{1}y)$ . It seems it is the correct answer. But why I missed $-\frac1y$ in this approach?","['calculus', 'derivatives']"
4194946,Is $\frac{1-\alpha}{1+\alpha}=y \Rightarrow \alpha=\frac{1-y}{y+1}$ correct even if $y=-1$?,"I was trying to solving this question: If roots of the equation $a x^{2}+b x+c=0$ are $\alpha$ and $\beta$ , find the equation whose roots are $\frac{1-\alpha}{1+\alpha}, \frac{1-\beta}{1+\beta}$ I was not able to solve it so I looked to the solution given in the book, it was as follows: Let $\frac{1-\alpha}{1+\alpha}=y \Rightarrow \alpha=\frac{1-y}{1+y}$ . Now $\alpha$ is root of the equation $a x^{2}+b x+c=0$ $\Rightarrow a \alpha^{2}+b \alpha+c=0$ $\Rightarrow \quad a\left(\frac{1-y}{1+y}\right)^{2}+b\left(\frac{1-y}{1+y}\right)+c=0$ . Hence required equation is $a(1-x)^{2}+b\left(1-x^{2}\right)+c(1+x)^{2}=0$ In the first step the auther used componendo dividend rule as follows: $\begin{aligned} & \frac{1-\alpha}{1+\alpha}=y \\ \Rightarrow & \frac{1-\alpha+1+\alpha}{1-\alpha-(1+\alpha)}=\frac{y+1}{y-1} \\ \Rightarrow & \frac{2}{-2 \alpha}=\frac{y+1}{y-1} \\ \Rightarrow & \frac{-1}{\alpha}=\frac{y+1}{y-1} \Rightarrow \alpha=\frac{1-y}{y+1} \end{aligned}$ But I don't understand how he could use it, as we are not sure whether $y$ could be - 1 and hence $1 +y$ can be zero. So is this an error? If so then how can we solve this question and if not then why not?",['algebra-precalculus']
4194970,Proving surjectivity of $z\exp(z)$ on the complex plane.,"I am trying to prove the surjectivity of $f(z)= z \exp(z)$ on the complex plane. I know this question already has answers here, but they always use Little Picard's theorem which I am not allowed to use. So far I have thought of the following: Certainly, $0 \in f(\mathbb{C})$ . Let $0\neq w \in \mathbb{C}$ . As $\exp$ is surjective onto $\mathbb{C} \backslash \{0\} $ we may write $w=\exp(a+ib)$ with real $a,b$ . Now I am trying to prove that we can write $a+ib= u+\exp(u)$ for $u \in \mathbb{C}$ , because then $\exp(a+ib)=\exp(u)\exp (\exp(u))=v\exp(v) $ for $v= \exp(u)$ , again using the surjectivity of $\exp$ . So basically, I have shifted the problem to showing that $z+ \exp(z)$ is surjective onto $\mathbb{C}$ . Here is where I got stuck. Any hint on how to proceed would be very much appreciated.","['complex-analysis', 'exponential-function']"
4194997,A question involving the use of complex numbers.,"Evaluate $2^{n-1}\left(\cos \theta -\cos(\frac{\pi}{n})\right)\left(\cos \theta -\cos(\frac{2\pi}{n})\right)...\left(\cos \theta -\cos(\frac{(n-1)\pi}{n})\right)$ In the above question the terms $\cos(\frac{\pi}{n}),\cos(\frac{2\pi}{n}),...$ are the real part of a complex number given by $$z=e^{i\frac{k\pi}{n}}$$ where $k = 1,2,...,n-1 $ . If we expand the above problem then we would have to compute $\Sigma_0^{n-1}\cos(\frac{k\pi}{n}),...$ and also the summation of other terms.It will obvious not work. The author has given  a hint to solve the above question. Hint: Use the expansion for $\frac{x^{2n}-1}{x^2-1}$ to solve the problem. I literally have no idea how to proceed further and solve it. I don't know in what way we can use the hint to solve the question as I cannot see or establish any clear connection between them. Any help will be appreciated.","['trigonometry', 'complex-numbers']"
4195031,Integral of $(1+t^2)^{\frac{1}{2}} \cdot (1+t^4)^\frac{1}{4}$,"I was solving the following problem of finding the General Solution of a given ODE given below: $$ \frac{\mathrm dy}{\mathrm dt} + \frac{ty}{1+t^2} = 1 - \frac{t^3y}{1+t^4}$$ I ended up with $$y\cdot(1+t^2)^{1/2}\cdot(1+t^4)^{(1/4)} = \int (1+t^2)^{1/2}\cdot (1+t^4)^{1/4}\,\mathrm dt$$ How do I Integrate the RHS. Even online integral calculators are showing that they are unable to do it. Can someone tell me if it is even integrable or not. Or is there any other idea of solving the ODE. Anyways I wish to learn why the integration on the RHS is not happenning. Is it non integrable? I looked at the parabola formed by $$(1+t^2)^{\frac{1}{2}} \cdot (1+t^4)^\frac{1}{4}$$ on an online graph plotter and it looked like the area under was infinitely much as it was upward opening and a bit narrow in shape(not very).","['integration', 'calculus', 'ordinary-differential-equations']"
4195059,Approximating $L^1(\mu)$ functions by smooth functions,"Let $\mu$ be a Borel measure on $\mathbb R^{d+1}$ having continuous positive density with respect to the Lebesgue measure. Is $C^\infty(\mathbb R^{d+1})$ dense in $L^1(\mu)$ ? What is an approximating sequence? Let $\psi\in L^1(\mu)\,$ . I would like to find a sequence $\psi_n\in C^\infty(\mathbb R^{d+1})$ such that $$\int_{\mathbb R^{d+1}}|\psi_n-\psi|\,d\mu\to0 \;.$$ Moreover I am interested in preserving the support in the following sense: if $\psi$ is continuous and $$\text{supp}\psi\subseteq K\times\mathbb R$$ for some $K$ compact subset of $\mathbb R^d$ , then I would like $\psi_n$ to have the same property (possibly for a larger compact set $K_n$ ). Can I choose $$ \psi_n(x) \,=\, (\psi*f_n)(x) \,=\, \int_{\mathbb R^{d+1}}\psi(x-y)\,f_n(y)\,d y$$ where $f_n$ is a sequence of mollifiers?","['convolution', 'measure-theory', 'functional-analysis', 'approximation-theory']"
4195074,Argument Principle - from J.Bak and D.Newman textbook,"The next example is proposed in the mentioned book, in order to light up the origin for the name of the theorem: $$
\frac{1}{2\pi i} \int_{\gamma} \frac{f'(z)}{f(z)} \mathrm{d}z= \frac{\log f(z(1))-\log(f(z(0)))}{2\pi i}
$$ where the parametrisation of $\gamma$ is given by $z(t)$ . Here is what confuses me: since the curve is closed, $z(1)=z(0)$ . Doesn't this imply that the numerator is $0$ ?","['complex-analysis', 'complex-integration']"
4195105,How can I solve the problem of convex hull,"Let $\DeclareMathOperator{\Conv}{\mathrm{Conv}} C=\Conv(v_1,v_2,...,v_m) $ , where $ v_1,v_2,...,v_m $ are $ m $ points in $ \mathbb{R}^n $ and 'Conv' means the convex hull. Please prove $$ 
\partial C=\cup_{D\in S}\Conv(v_i:i\in D)
$$ where $\partial C $ denotes the boundary of $ C $ and $S$ is defined as the set $$ 
S=\left\{D\subset\{1,2,\dots,m\} : \exists d\in\mathbb{R}^n \text{ such that } 
\begin{split}
\langle v_i,d\rangle=1 &\;\forall i\in D \\
\langle v_i,d\rangle<1 &\:\forall i \notin D
\end{split}\right\} 
$$ I have tried to prove it by using the fact that the boundary of $ C $ is the union of all edges. However I cannot give an explicit proof. Can you give me some concrete hints and references?","['elementary-set-theory', 'metric-geometry', 'convex-hulls']"
4195180,"Solve $\sum_{n=1}^\inftyΓ(-n,-n)\mathop= \pi\left(\frac1e-1\right)i+ \sum_{n=1}^\infty \frac{(-1)^n \text{Ei}(n)}{n!}+\sum_{n=1}^\infty a_n･(-e)^n $","Motivation: It may be possible to find an integral representation using: Integral form(s) of a general tetration/power tower integral solution: $$\sum\limits_{n=0}^\infty \frac{(pn+q)^{rn+s}Γ(An+B,Cn+D)}{Γ(an+b)}$$ After seeing this related question with the use of gamma functions , I found that $$\mathrm{\int_0^\frac1e x^{-x} dx=\mathop \sum_{n≥1}\frac{Q(n,n)}{n^n}=\mathop \sum_{n≥0}\frac{E_{−n}(n+1)}{n!}=.48689…}$$ source 1 source 2 Here is a graph of the real side of the second to last series. This result made me wonder  how to evaluate the following. The definition of the exponential integral function still works as Re(n)<0: $$\mathrm {S\mathop=^{def}\sum_{n\in \Bbb Z^-} Γ(n,n)= \sum_{n=1}^\infty Γ(-n,-n) = \sum_{n=1}^\infty \frac{E_{n+1}(-n)}{(-n)^{n}}=\sum_{n=1}^\infty\int_{-n}^\infty \frac{dt}{t^{n+1}e^t}= \int_1^\infty\sum_{n=1}^\infty\frac{e^{nt}}{(-n)^nt^{n+1}}dt=\sum_{n=1}^\infty Q(n,n)Γ(n)=-0.5948551252563932027611440348… + 1.98586530379887152055255019996… i}$$ Here is an attempt at an integral representation. If only I could find a summation that I could evaluate because this “ $n^n$ ” in the denominator makes it hard to find a closed form for the sum: $$\mathrm{S=\sum_{n=1}^\infty \int \frac d{dx} Γ(-n,-n\,x)\big|_0^1dx=\int_0^1\sum_{n=1}^\infty \frac{e^{nx}}{n^n x^{n+1}}dx\implies S=\sum_{n=1}^\infty \int_a^b \frac d{dx} Γ(-n,-n\,f(x)) dx =\int_a^b \sum_{n=1}^\infty -\frac{e^{n\,f(x)}f’(x)}{(-n\,f(x))^nf(x)}dx}$$ S converges because of this graph and is almost an alternating series with the absolute value of the summand decreasing to 0. Here are the partial sums for the negative bounds of summation: Finally, here is the graph of the summand Γ(n,n) plotted at the index bounds: A possible hint is understanding how to derive the $\int_0^\frac1e x^{-x}dx $ integral sum. You can “reverse” substitute and “reverse” integrate as I showed above. The following uses an exponential integral function . Here is a result showing that you can easily find an alternate expression for “S” using @Steven Clark’s great answer in: Nice result $$\mathrm{\int_1^\infty \frac{dx}{xe^x-1}=\sum_{n\ge1} E_{n}(n)=.269292…}$$ Here is a third example to support that this “S” constant should have another expression. This example uses the lower regularized incomplete gamma function : Related problem $$\mathrm{\int_{-1}^0 e^{te^t}dt=1+\sum_{n=1}^\infty \frac{(-1)^nP(n+1,n)}{n^{n+1}}=.772158…}$$ Here is a possible form using the Abel-Plana formula . It may theoretically converge, but Wolfram Alpha has trouble finding it numerically. There are many possibilities for substitution, so I will leave it here: $$\mathrm{\sum_{n=0}^\infty Γ(-n-1,-n-1)=\frac{Ei(1)-e+i\pi}{2}+\int_0^\infty Γ(-x-1,-x-1)+i\frac{Γ(-ix-1,-ix-1)-Γ(ix-1,ix-1)}{e^{2\pi x}-1}dx}$$ Here is theoretically working integral representation . We can integrate by each term: $$\sum_1^\infty Γ(-n,-n)=\sum_{n=1}^\infty \frac{1}{2\pi i}\int_C\frac{Γ(x-n)Γ(x)}{Γ(x+1)(-n)^x}dx=\frac{1}{2\pi i}\int_{k-i\infty}^{k+i\infty}\frac{1}{ (-1)^{x}x}\sum_{n=1}^\infty \frac{Γ(x-n)}{n^x}dx,\text{max}(\text{Re}(n),0)<k$$ How can I find an alternate representation for “S”? You can find a better integral expression of “S”. If not, then please find a closed form, optional,  or a series representation which converges faster. Please correct me and give me feedback!","['gamma-function', 'sequences-and-series', 'constants', 'tetration', 'complex-numbers']"
4195183,Lower bound on the number of walks of length $l$ in a graph,"Let $G$ be any connected simple graph, fix some positive integer $l$ and denote by $w_l(v,w)$ the number of walks of length $l$ between vertices $v$ and $w$ . I am looking for a lower bound on $\min_{v,w} w_l(v,w)$ . In this publication I found that there is a pair $v,w$ such that $w_l(v,w)\geq \dfrac{\bar{d}^l}{n}$ where $\bar{d}$ is the average degree in $G$ and $n$ the number of vertices in $G$ . I would like to extend this to a uniform lower bound (which will be worse of course). I am particularly interested in the form of the lower bound $\dfrac{x^l}{n}$ for some $x$ to be determined. A first guess was to replace $\bar{d}$ by the minimal degree $\delta$ but I found counter examples where this was not correct. Do you have any suggestions or literature references? EDIT: The existence of such a $x$ is evident, since $x^l\to 0$ continuously as $x\to 0$ . And we know that there is a pair $v,w$ such that $w_l(v,w)\geq \dfrac{\bar{d}^l}{n}$ but (by the averaging argument in the paper) we have $\dfrac{\bar{d}^l}{n}> \min_{v,w} w_l(v,w)>0$ as soon as there is more than one degree in $G$ . Consequently, the function $f(x) = \dfrac{\bar{x}^l}{n}- \min_{v,w} w_l(v,w)$ satisfies $f(\bar{d})>0$ and $f(0) \leq 0$ and $f$ is continuous. Therefore, we have a $x_0$ such that $f(x_0)=0$ . Evidently, deriving $x_0$ from this equation is like a dog chaising its own tail. So we need a $x_0'$ which is a bit smaller than $x_0$ by monotony of $f$ as $x\to 0$ . Again, here I am stuck. EDIT 3: Based on the reponse and comments by Brandon du Preez I decided to put loops on every vertex. Unfortunately, this way, we lose the result from the paper I cited because it uses spectral theoretical tools and especially the fact that the adjacency matrix has trace 0. But, let $\phi'$ be a walk of length $l$ in this new graph $G'$ (with loops) then there is a $k\in\{0,...,l-1\}$ and a walk $\phi$ in $G$ of length $l-k$ such that $\phi$ consists of all ""real"" steps without taking loops. I, then, only have to arrange the loops between the $l-k$ visited vertices (with multiplicities of course). Therefore, I obtain the formula for the number of walks between $v,w$ in $G'$ as $$w_l^{G'}(v,w)=\sum_{k=0}^{l-1}\sum_{h_1+...+h_{l-k}=k}\binom{k}{h_1,...,h_{l-k}}w_{l-k}(v,w)$$ Now, I am stuck again. This is obviously greater zero for any connected graph for sufficiently large $l$ but can I get the desired estimate from it? Looks a bit like a multinomial theorem if you replace $w_l(v,w)$ by something of the form $x^l$ but not quite.
Do you have any more ideas?","['graph-theory', 'combinatorics']"
4195204,"What is the advantage of defining an ordered pair $(a, b)$ as $\{\{a\}, \{a, b\}\}$?","I've been studying set theory and have come across ordered pairs being described as a sets. Why is it beneficial to define the ordered pair $(a, b)$ as the set $\{\{a\}, \{a, b\}\}$ ? It seems to me that there are much simpler ways to define an ordered pair. So we can tell the left element from the right, we could define $(a, b)$ as $\{a, a, b\}$ . We can say that if an element is repeated twice it is the first element of the pair and otherwise it is second. With this definition we are able to decode the set into an ordered pair just like the first, so why choose the more complicated $\{\{a\}, \{a, b\}\}$ ?","['elementary-set-theory', 'definition']"
4195207,"Prove or Disprove that $f(x,y)$ is differentiable in $(0,0)$? (Limit).","Let $f(x,y)=\frac{x^4y}{x^4+y^2}$ whenever $(x,y)\ne(0,0)$ . and $f(x,y)=(0,0)$ otherwise. Prove or disprove: $f$ is differentiable in $(0,0)$ . My attempt: I am trying to prove it by showing that $\epsilon \to 0$ when $\Delta x,\Delta y \to 0$ in this definition: $f(x+\Delta x, y+\Delta y)-f(x,y)=f_x(x,y)\Delta x+f_y \Delta y + \epsilon\sqrt{(\Delta x)^2+(\Delta y)^2}$ . I have checked by definition that $f_x(0,0)=f_y(0,0)=0$ . And so $f(\Delta x, \Delta y)=\epsilon\sqrt{\Delta x^2+ \Delta y^2}$ Which gives me: $$\frac{(\Delta x)^4(\Delta y)}{[(\Delta x)^4 + (\Delta y)^2]\sqrt{(\Delta x)^2+ (\Delta y)^2}}=\epsilon$$ But now  I got stuck trying to show that $\epsilon \to 0$ , I am not sure how to deal with this limit and would appreciate any help and explanations on your thoughts (What did you think when you saw this limit, like did you know it converges to $0$ just by looking at it? What I'm trying to say is I would appreciate explanations on your thoughts ""behind the scenes"" that led you to use your method). Thanks in advance!","['limits', 'multivariable-calculus', 'derivatives']"
4195218,Maximum number of squares a line of length n will intersect,"Based on this CGCC challenge My problem is: Imagine a cartesian plane wih squares each having a side length of 1 unit, And given the euclidian distance of two points on the plane which is $L$ units ( $L ∈\mathbb{R^+}$ ), now calculate maximum number of squares a line segment with length of $L$ will touch (I mean intersect into the interior of a square). Example for $L$ of $3$ and $5$ : The blue lines denote the line for maximum number of squares, which for 3 is 7 and 5 is 9 (Look closely) Now this is the only way for me to solve the problem. Is there any nice formula that will take L and return the number of intersecting squares?","['geometry', 'plane-geometry']"
4195224,Prove $\sum_{n=2}^{\infty}\frac{1}{n^2+e}<\frac{1}{2}$,"Of course, you can use the following formula $$\sum_{n = 1}^\infty \frac{1}{n^2 + a^2} = \frac{\pi\coth(\pi a)}{2a} - \frac{1}{2a^2},$$ but which is too ""advanced"". We want to find a solution only depending on inequality estimation only. Maybe, we can obtain \begin{align*}
\sum_{n=2}^{\infty} \frac{1}{n^2+e}&\le \sum_{n=2}^{100}\frac{1}{n^2+e}+\sum_{101}^{\infty}\frac{1}{n^2}=\sum_{n=2}^{100}\left(\frac{1}{n^2+e}-\frac{1}{n^2}\right)+\sum_{n=2}^{\infty}\frac{1}{n^2}\\
&=\frac{\pi^2}{6}-1+\sum_{n=2}^{100}\left(\frac{1}{n^2+e}-\frac{1}{n^2}\right)<\frac{1}{2},
\end{align*} which is true by checking on machine, but too hard to compute by hand.","['calculus', 'inequality', 'sequences-and-series']"
4195240,Do the Frechet inequalies give us tight bounds?,"Assuming a suitable probability space $(\Omega, \mathcal{F}, P)$ , the Frechet inequalities are given by: $$\max \left( 0, \sum_{k=1}^n P(A_k) - (n-1) \right) \leq P\left( \bigcap_{k=1}^{n} A_k \right) \leq \min_k \{ P(A_k) \}$$ $$\max_k \{P(A_k)\} \leq P\left( \bigcup_{k=1}^{n} A_k \right) \leq \min \left(1, \sum_{k=1}^n P(A_k) \right)$$ Wikipedia defines ""tight bounds"" : An upper bound is said to be a tight upper bound, a least upper bound, or a supremum, if no smaller value is an upper bound. Similarly, a lower bound is said to be a tight lower bound, a greatest lower bound, or an infimum, if no greater value is a lower bound. Are these bounds tight?","['inequality', 'probability-theory', 'upper-lower-bounds']"
4195254,How many paths exist on an $n\times m$ grid?,How many paths exist on an $n\times m$ grid? The Path requirements are You start at the bottom left and end at the top right The Path can not intersect it's self Each Line has to start and end on a point on the grid Reflections and flips of a Path are not necessary an equivalent Path. I found $26$ Paths for a $2$ by $3$ grid I believe that I found all paths for a $2$ by $3$ grid. What can I use to try to make a general formula for an $n\times m$ grid?,"['graph-theory', 'recreational-mathematics', 'combinatorics']"
4195294,The crown-based power $L_k$,"Let $L$ be a non-cyclic finite group which has only one minimal normal subgroup, $M$ , and $M$ is either non-abelian or complemented in $L$ . Let $k$ be a positive integer and consider the subgroup $$L_k=\{(l_1,...,l_k) \in L^k\mid l_1 \equiv \cdots \equiv l_k \bmod M \}$$ of the direct product $L^k$ . Show that the quocient group of $L_{k+1}$ over any minimal normal subgroup is isomorphic to $L_k$ . $\textbf{My attempt}$ : I have shown that the direct factors of $M^{k+1}$ are minimal normal subgroups of $L_{k+1}$ . In the case where $M$ is non-abelian, these are the only ones, so it is easy to show the isomorphism. Now, when $M$ is abelian, I couldn't do it.","['group-theory', 'normal-subgroups', 'finite-groups']"
4195346,open and closed set in non-metric topology,"I have just started on learning topology. And I saw this question in Amann and Escher's Analysis I (Exercise 10, page 247): Let X:={1,2,3,4,5} and $\mathcal{T}:=\{\emptyset, X, \{1\}, \{3, 4\}, \{1, 3, 4\}, \{2,3,4,5\}\}$ Determine the closure of {2, 4, 5} Since my understanding on the basic concepts are still quite weak, I'd like to ask whether my following reasoning steps are correct or not. Thank you! My Reasoning: All the sets in T are open sets. Also, since $\{2, 3, 4, 5\}$ and $\{1\}$ are complement to each other and they are both elements in the topology. They are clopen sets. So the closed sets with regard to this topology are $\emptyset, X, \{2,3,4,5\}, \{1\}, \{1, 2, 5\}, \{2, 5\}$ Thus the closure of $\{2, 4, 5\}$ is the smallest closed set and at the same time its superset,
that is $\{2, 3, 4, 5\}$",['general-topology']
4195352,"Proof of Smale Theorem (Audin-Damian), Lemma 2.2.8","In Audin and Damian, p.43, there is a proof of the following lemma, relating pseudo-gradient vector fields adapted to $f$ on $V$ , namely $X$ and approximation $X'$ . Here, $\alpha_j$ a critical value of the Morse function $f$ : and in the course of the proof there is the claim of an embedding $\Psi$ , in the bottom: I believe there is a mistake in the image of this embedding, namely it should be from $D^k\times Q\times [0,m]$ to $f^{-1}([\alpha_j+\epsilon, \alpha_j+2\epsilon])$ . If it's as written, it doesn't make sense to claim that $\{0\}\times Q\times\{0\}$ is the embedding of $Q$ on $f^{-1}(\alpha_j+\epsilon)$ (same for $\{0\}\times Q\times\{m\}$ ). But having fixed that, how do we know that $$\Psi_{\star}\left( -\dfrac{\partial}{\partial z} \right) = X\quad ?$$ In particular, I feel like to be able to get this pushforward to be exactly $X$ , we would need to calibrate the $\epsilon$ accordingly, is that what is happening? We choose $\epsilon$ carefully and $\Psi$ carefully so that $\Psi$ stretches the tubular neighborhood onto $f^{-1}([\alpha_j+\epsilon, \alpha_j+2\epsilon])$ so that the unit speed vector field $-\dfrac{\partial}{\partial z}$ becomes $X$ ?","['vector-fields', 'differential-topology', 'morse-theory', 'differential-geometry']"
4195411,Find $f(x)$ such that it maximizes $\int_0^1 \left(f(f(x))-f(x)\right) dx$,"Inspired by this post Find the maximun of the sum $\sum_{k=1}^{n}(f(f(k))-f(k))$ , I came to the below question. Find the $f(x)$ that satisfies: $1.$ $0\le f(x)\le 1$ $2.$ Increasing for $\left(0,1\right)$ $3.$ Maximizes $\quad\int_0^1 \left(f(f(x))-f(x)\right) dx$ After I tried a few elementary functions, now I am feeling that there might not be an analytic solution for the $f(x)$ (but I'm not sure). If that's the case, can we at least know the maximum of the integral? I posted this question because the original discrete version of the question didn't seem to have a clear solution I was wondering if this continuous version of the question has any analytic solution. The accepted answer suggests that the solution could be the piecewise linear function. However, there's no clear proof or reasoning why that should be the optimal case. What I've tried was: $1$ . I started from $f(x)=\frac{x+1}2$ as it was assumed in the original question I likned above. $2$ . I tried a few elementary functions such as $f(x)=x^p$ , and then $f(x)=m\ln\left(\frac{x+a}{1+a}\right)+1$ , and two terms, etc. (assumed $f(1)$ should be $1$ to maximize the integral) and could see that the integral kept increasing for more variables, so I thought there must be an optimum (but differentiable) $f(x)$ and that it might not be an analytically obtained due to $f(f(x))$ . At that time I didn't know that I was only approaching to the step function. Thanks for all the comments and the answers. Now I can see that the solution is probably the piecewise step function in the answer, so this question may not be as interesting as I thought. However, I think that we are still missing the proof that it is the optimal $f(x)$ .","['optimization', 'definite-integrals', 'real-analysis']"
4195413,Detect if a sequence is exponential,"We can detect that $x[n] = A^n$ is exponential since the second finite difference of its log is zero everywhere. E.g. x = 2**np.arange(10)
assert np.allclose(np.diff(np.log(x), 2), 0) Log makes it linear, diff makes it a constant, second diff makes it zero. Is there such a definitive approach for the case of $x[n] = A^n + B$ , where $n_0\leq n < n_1$ , and we don't know $n_0$ or $n_1$ ? i.e. we don't actually know which $x_i$ maps to which $n$ (but we do know the length, $n_1 - n_0$ ) An attempt If $x = A^t + B$ , then $d = \log(x)' = \log(A) A^t / x$ , and $d\cdot x$ is exponential. But it's only approximate if the differentiator is a finite difference: x = 2**np.arange(10) + 100
d = np.diff(np.log(x))
out = np.diff(np.log(d * x[1:]), 2)
print(np.abs(out).max()) >>> 0.0143 There are recursive / optimization methods not just to detect exponential but find $A$ and $B$ exactly, but I seek a closed form solution as with the case of $x[n]=A^n$ .","['exponential-function', 'discrete-mathematics', 'logarithms']"
4195441,"Prove that the Jacobian matrix is the matrix representation of the derivative. Non-ambiguous statement for ""is the matrix representation of""?","I'm trying to prove that the matrix representation of the derivative is the jacobian matrix, but I can't find a non-ambiguous statement to complete the proof. I've tried the following proof technique: I'm looking to establish the identity between the $j$ th partial derivative of $f(\vec a)$ designated by $f'(\vec a)(\vec e_j)$ and the $j$ th column of the Jacobian matrix, but I still can't complete the proof. I don't know if I'm making a mistake or if I'm lacking linear algebra here somewhere. Here's the argument: Let $\{ e_1, ..., e_n\}$ be the standard basis of $\mathbb{R}^n$ . Suppose $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ , and suppose $f$ is differentiable at $\vec a$ . Therefore, we have $$\lim \limits_{\vec h \to \vec 0} \frac{f(\vec a + \vec h) - f(\vec a) - f'(\vec a)(\vec h)}{||\vec h||} =0$$ If we let $\vec h = t\vec e_j$ for any $j = 1, ..., n$ and do some manipulations, we can arrive at $$f'(\vec a)(\vec e_j) = \lim \limits_{t \to 0} \frac{f(\vec a + t\vec e_j) - f(\vec a)}{t}$$ Which is the definition of the $j$ th partial derivative of $f(\vec a)$ , from which we can conclude that $f'(\vec a)(\vec e_j)$ is the $j$ th partial derivative of $f(\vec a)$ . This where I'm stuck . What would be a meaningful statement establishing ""the matrix representation of the derivative is the jacobian matrix""? I'm thinking that if I can say: the $j$ th column of the jacobian is identical to $f'(\vec a)(\vec e_j)$ , this could be acceptable, but what exactly is the definition of identity that I'm looking for here? It seems to me that the statement "" $f'(\vec a)(\vec e_j)$ is identical to the $j$ th column of the Jacobian matrix"" is an ambiguous statement. To give an example the element $a_{ij}$ of the Jacobian is not identical to the ith component of the vector $f'(\vec a)(\vec e_j)$ , the former being a real number and the latter being a vector. Any help is appreciated!","['multivariable-calculus', 'proof-writing', 'linear-algebra', 'real-analysis']"
4195449,Is the space of complex structures a loop space?,"Define the space of (normalized) complex structures $\mathcal{J}_{2k}$ on $\mathbb{R}^{2k}$ as the orthogonal transformations in $SO(2k)$ that square to minus the identity. My question is if there exists a space $X$ such that $\Omega (X) \simeq \mathcal{J}_{2k}$ ? I suspect this is not the case - if so then how would one prove this? As far as I am aware, a space can be de-looped if and only if it can be given an $A_\infty$ -structure. Since $\mathcal{J}_{2k}$ can be identified with the quotient $SO(2k)/U(k)$ (and $U(k)$ is not a normal subgroup of $SO(2k)$ for all $k\neq 1$ *) then there is no obvious well-defined product to put on the quotient, and hence $\mathcal{J}_{2k}$ . Another note is that the space of complex structures can be identified with the space of minimal geodesic paths between two fixed points in $SO(2k)$ . So to de-loop $\mathcal{J}_{2k}$ is to de-loop $\Omega^m_{p,q}(SO(2k))$ . A final point of possible interest is that stably (in the colimit) $\mathcal{J}:= SO/U \simeq \Omega SO$ ** can be de-looped. *Note: I am identifying $U(k)$ with the subgroup of $SO(2k)$ of those transformations that commute with a fixed complex structure $J \in \mathcal{J}_{2k}$ **Note: This is a part of the story of Bott periodicity - see eg. Milnor's Morse theory.","['loop-spaces', 'complex-geometry', 'geometric-topology', 'homotopy-theory', 'general-topology']"
4195470,Does the sum of the series $\sum_{n=1}^{\infty}\frac{\sqrt{n+1}-\sqrt n}{n}$ have an analytic expression?,"Just out of curiosity, I'd like to know whether or not the sum of the series $$\sum_{n=1}^\infty \frac{\sqrt{n+1}-\sqrt n}{n}$$ has a known analytic expression. I stumbled across this series while trying to evaluate $$\int_1^\infty\frac{1}{\lfloor x^2\rfloor}dx$$ The convergence of this integral can be seen by making use of the inequality $\lfloor x\rfloor > x-1$ and the fact that $\coth^{-1}(t)\to 0$ as $t\to\infty$ : \begin{align*}
\int_1^t\frac{1}{\lfloor x^2\rfloor}dx &= \int_\sqrt{1}^\sqrt{2}\frac{1}{\lfloor x^2\rfloor}dx+\int_\sqrt{2}^t\frac{1}{\lfloor x^2\rfloor}dx\\
&< \int_\sqrt{1}^\sqrt{2}\frac{1}{\lfloor x^2\rfloor}dx+\int_\sqrt{2}^t\frac{1}{x^2-1}dx\\
&= \int_\sqrt{1}^\sqrt{2}\frac{1}{1}dx-\int_\sqrt{2}^t\frac{1}{1-x^2}dx\\
&= \sqrt{2}-1-\left[\coth^{-1}(t)-\coth^{-1}\left(\sqrt 2\right)\right]\\
&= \sqrt{2}-1-\coth^{-1}(t)+\coth^{-1}\left(\sqrt 2\right)\\
&\to \sqrt{2}-1+\coth^{-1}\left(\sqrt 2\right)\text{ as }t\to\infty\\
\end{align*} Since this implies that $\int_1^t 1/\lfloor x^2\rfloor dx$ is strictly increasing ( $1/\lfloor x^2\rfloor >0$ for every $x\geq 1$ ) and bounded above, the integral necessarily converges. By breaking up the integral $$\int_1^\sqrt{k+1}\frac{1}{\lfloor x^2\rfloor}dx$$ into integrals indexed by the intervals $\left[\sqrt{i},\sqrt{i+1}\right]$ for $i=1,2,3,...,k$ and simplifying the resulting sum, I was able to show that $$\int_1^{\sqrt{k+1}}\frac{1}{\lfloor x^2\rfloor}dx=\sum_{n=1}^{k} \frac{\sqrt{n+1}-\sqrt n}{n}$$ is true for every $k\geq 0$ , which yields $$\int_1^\infty \frac{1}{\lfloor x^2\rfloor}dx=\sum_{n=1}^\infty\frac{\sqrt{n+1}-\sqrt n}{n}$$ after letting $k\to\infty$ . This equality is the main reason why I'm interested in the sum of the aforementioned series. After some (unsurprisingly) futile attempts to evaluate the integral, I expect there to be no closed-form expression for the sum, which is why I'm open to an analytic expression (gamma function, Bessel functions, Riemann zeta function, etc.). Any help is appreciated. Edit : after seeing the bounds provided by Markus Scheuer and Jorge, I thought I'd share some of my own. From the fact that $x-1<\lfloor x\rfloor<x$ is true for every non-integer $x\geq 1$ , we can infer that for every integer $k\geq 1$ , $$\int_\sqrt{k+1}^\infty \frac{1}{x^2}dx<\int_\sqrt{k+1}^\infty \frac{1}{\lfloor x^2\rfloor}dx<\int_\sqrt{k+1}^\infty \frac{1}{x^2-1}dx$$ Using $$\int_{x}^{\infty}\frac{1}{t^2-1}dt=\coth^{-1}(x)$$ and $$\int_\sqrt{k+1}^\infty\frac{1}{\lfloor x^2\rfloor}dx=\int_1^\infty\frac{1}{\lfloor x^2\rfloor}dx-\int_1^\sqrt{k+1}\frac{1}{\lfloor x^2\rfloor}dx=\sum_{n=1}^\infty \frac{\sqrt{n+1}-\sqrt n}{n}-\sum_{n=1}^{k}\frac{\sqrt{n+1}-\sqrt n}{n}$$ we deduce that $$\frac{1}{\sqrt{k+1}}+\sum_{n=1}^{k}\frac{\sqrt{n+1}-\sqrt n}{n}<\sum_{n=1}^\infty\frac{\sqrt{n+1}-\sqrt n}{n}<\coth^{-1}\left(\sqrt{k+1}\right)+\sum_{n=1}^{k}\frac{\sqrt{n+1}-\sqrt n}{n}$$","['improper-integrals', 'closed-form', 'sequences-and-series']"
4195474,Proof of Theorem 13.3 in Convergence of Probability Measures by Billingsley,"I have some trouble understanding the ""symmetric argument"" part in the proof of Theorem 13.3. (how to get the 3rd condition in Equation 13.8 from Equation 13.9). According to the book, some symbles are defined as: $D$ : the space of cadlag functions: real valued functions on $[0,1]$ which are right continuous and have left limit at $t\in(0,1]$ . $P_n,P$ : probability measures on the space $D$ . $T_P$ : The set of $t\in [0,1]$ for which the projection $\pi_t$ is continuous with respect to the Skorohod topology except at points forming a set of P-measure 0 ( $T_P$ is dense in [0,1] and contains 0 and 1). $J_1$ : the set of $x \in D$ where $x(1-)\neq x(1)$ . $\omega_x^{''}(\delta)=\sup_{^{t_1\leq t \leq t_2}_{t_2-t_1\leq\delta}}\,\,\,\{|x(t_1)-x(t)|\wedge|x(t)-x(t_2)|\}$ Some equations and the proof are in the following pictures. The main part of the proof is to show that 2nd and 3rd condition in (13.8) holds for large n. I don't know if it is true to say that by proving $P(J_1)=0$ , the author is saying that somehow $1-$ can be viewed as a point in $T_P$ and $P_n\pi_{1-}^{-1} \Rightarrow P\pi_{1-}^{-1}$ ? In the discussion following Theorem 13.3, the author says 13.9 can be replaced by $P_n\pi_{1-}^{-1} \Rightarrow P\pi_{1-}^{-1}$ . ////////////////////////////////////////////////////////////////////////// //////////////////////////////////////////////////////////////////////////","['weak-convergence', 'statistics', 'probability-theory', 'probability']"
4195481,Is $\mathbb{F}_{2011^2}[x]/(x^4-6x-12)$ a field?,"I'm studying for my qualifying exam and this was one of the questions in the question bank under Field and Galois theory section. I'm currently stuck on this question. Is $\mathbb{F}_{2011^2}[x]/(x^4-6x-12)$ a field? I'm guessing the answer is no but I don't know how do I prove that $x^4-6x-12$ is an irreducible polynomial over $\mathbb{F}_{2011^2}$ . I'm trying to use the following theorem: ""let $p$ be a prime. Over $\mathbb{F}_p, x^{p^n}−x$ factors as the product of all monic irreducible polynomials of degrees $d\mid n$ .""","['irreducible-polynomials', 'finite-fields', 'abstract-algebra', 'polynomials']"
4195493,"Proof of Fundamental Theorem of Elimination Theory, from Vakil's FOAG","I've been working the last section of Chapter 7 of Vakil's FOAG, namely the one on the Fundamental Theorem of Elimination Theory. I'm not entirely sure if I've understood the proof correctly, especially the following paragraph: Here's what I make of this. First of all, since $Z$ is closed, it's something of the form $V(f_1,\ldots)$ . Now we want to see if a particular $p\in \text{Spec} A$ is also in $\pi(Z)$ . Now $p$ is in this image iff there's some $p'$ in each affine open $U_j = \text {Spec }A[x_0/x_j,\ldots,x_n/x_j]$ , which as a prime ideal contracts to $p$ under the usual map of rings produced in the opposite direction. Since we're interested in only primes that contract to $p$ , we may ""throw out"" everything else by working only with $k(p)[x_0/x_j,\ldots,x_n/x_j]$ in each of these cases, and as a result, we may just work with $Proj_{\bullet} k(p)[x_0,\ldots,x_n]$ . If there is a point $q$ at which the images of the $f_i$ vanish then there's a corresponding prime ideal which contracts to $p$ via the corresponding map of rings, and thus $p\in \pi(Z)$ . Next, there is a proof of the fact that if we have homogeneous elements $g_i$ cutting out a closed subset of $\mathbb{P}^n_k$ , for some field $k$ , then the coefficients of these $g_i$ satisfy a Zariski closed condition. I think I understand this bit of the proof and I don't include it here. Finally, there is the following exercise: To this I would say that from the work done in the first paragraph, we work with $k = k(p)$ in the last paragraph above. This would give us that the images of the coefficients of $f_i$ in $k(p)$ satisfy a Zariski closed condition (namely a number of determinants being $0$ , in some $k(p)$ 's). This means that we're looking for precisely those prime ideals where the determinant is $0$ , which is of course closed and we are done. Is my understanding of the situation correct?","['algebraic-geometry', 'solution-verification', 'projective-schemes', 'commutative-algebra']"
4195513,A non-cyclic finite abelian group contains a subgroup isomorphic to the direct product of a group of order $p$ with itself,"In Serge Lang’s Algebra he leaves a direct proof of the following statement to the reader: Let $G$ be a finite abelian group. If $G$ is not cyclic, then there exists a prime $p$ and a subgroup of $G$ isomorphic to $C$$\times$$C$ , where $C$ is cyclic of order $p$ . I have come up with this so far: Consider such a group $G$ and let $r$ , $s$ be non-identity elements such that neither can be expressed as a power of the other (such elements exist because $G$ is not cyclic.) We then have two cases: either the periods of $r$ and $s$ are coprime or they aren't (and thus both are divisible by some prime $p$ .) Suppose the periods are $m$ and $k$ respectively. If their periods are coprime, then let $p$ divide $mk$ . The group $\langle r\rangle\times\langle s\rangle$ is then cyclic(since $m$ and $k$ are coprime) of order $mk$ . Given a generator $x$ , the cyclic group $\langle x^{mk/p}\rangle$ is of order $p$ and we have our group $C$ . Now we must prove that $s^i$ does not equal $r^k$ for any nontrivial $i,k$ . If this were true, then $r^k$ would generate a subgroup of $\langle s\rangle$ . The order of this subgroup then divides both $\langle s\rangle$ and $\langle r\rangle$ contradicting $m$ and $k$ being coprime. Hence we have an isomorphism between the subgroup $\langle s,r\rangle$ and $\langle x^{mk/p}\rangle$ . I can not seem to find a similar result for the case where $m$ and $k$ are not coprime (that is, I cannot figure out why $(r^{m/p})^i=(s^{k/p})^j$ cannot happen for nontrivial $i,j$ .) So, overall, is what I have in the first case correct and how do I prove the second?","['cyclic-groups', 'finite-groups', 'group-theory', 'abstract-algebra', 'abelian-groups']"
4195547,Derivation of Stirling approximation from CLT,"In Casella Berger exercise 5.35, we have a sequence of sample means from the exponential $exp(1)$ . Then, we can derive: $$P\Bigg(\frac{\overline{X}_n - 1}{1/\sqrt{n}} \leq x\Bigg) \rightarrow P(Z \leq x)$$ where $Z \sim n(0, 1)$ . Now, the author suggests to show the Stirling approximation by taking the derivative on both sides. In short, he suggests: $$\frac{d}{dx}P\Bigg(\frac{\overline{X}_n - 1}{1/\sqrt{n}} \leq x\Bigg) \approx \frac{d}{dx}P(Z \leq x)$$ from where the Stirling approximation follows. Why are we justified doing this? Wiki article on weak convergence says: "" In general, convergence in distribution does not imply that the sequence of corresponding probability density functions will also converge. "" Why is taking the derivative justified according to the author? If it is allowed, can be it shown in some simple way without measure theory? Or link to the relevant theorems.","['probability-limit-theorems', 'central-limit-theorem', 'probability-theory', 'probability']"
4195556,Preimage of disjoint sets is disjoint,"I'm reading through James Munkres' book, Topology, and the theorem about the image of a connected space under mapping being connected. To paraphrase the proof: Let $f:X\to f(X)$ be a continuous map; let $X$ be connected. Suppose that $f(X)=A\bigcup B$ is a separation of $f(X)$ into two disjoint nonempty sets open in $f(X)$ . Then $f^{-1}(A)$ and $f^{-1}(B)$ are disjoint sets whose union is $X$ . (The proof continues but I'll stop here.) My question is why are $f^{-1}(A)$ and $f^{-1}(B)$ disjoint in $X$ ? I've seen that $f^{-1}(A\cap B)\subset f^{-1}(A)\cap f^{-1}(B)$ , but I don't think this helps as I want to know $f^{-1}(A)\cap f^{-1}(B)=\emptyset$ from the fact that $A\cap B=\emptyset$ .",['elementary-set-theory']
4195597,Is the product of embedded submanifolds still an embedded submanifold?,"My terminology complies with Lee's ISM. Let $G$ be a Lie group and suppose $H$ is a subgroup of $G$ that is also an embedded submanifold. I would like to show that $H$ is a Lie subgroup. To begin with, I wonder if $H\times H$ is an embedded submanifold of $G\times G$ . In order to answer this question, one needs to check if the inclusion map $\iota_2:H\times H\hookrightarrow G\times G$ is a smooth embedding. And that's where I got stuck. The inclusion map $\iota_1:H\hookrightarrow G$ is a smooth embedding by hypothesis, but I have no idea how to relate this fact to $\iota_2$ . Thank you.","['smooth-manifolds', 'differential-geometry']"
4195614,Inverse of left-invariant metric is left-invariant,"Given a Lie group $G$ of dimension $n$ with a left-invariant metric $g$ .
Is it true that $g^{-1}$ , that is, the metric in the cotangent bundle (whose matrix form is the inverse of that of $g$ ) is also left-invariant? The reason that I think this is not true is that for a particular example, I have $n$ left-invariant 1-forms, and if my computations are correct, some of the inner products (via $g^{-1}$ ) of these forms are not constants.","['riemannian-geometry', 'lie-groups', 'differential-geometry']"
4195640,What is the precise definition about $k$ times continously differentiable at a point $\mathbf{c}\in\mathbb {R}^{m}?$,"$k$ times continously differentiable A funcation $f$ ,with domain $S$ in $\mathbb {R}^{m}$ is $k$ times continously differentiable at an interior point $\mathbf {c}$ of $S$ if it and all of its first-through $(k-1)$ th-order partial derivatives are continuously differentiable at $\mathbf {c}$ or, ${\color{red}{\text{equivalently}}}$ ,if all of the first-through $k$ th-order partial derivatives of $f$ exist and are continuous at every point in some
neighborhood of $\mathbf{c}$ --a vector or matrix of functions is $k$ times continuously differentiable at $\mathbf{c}$ if all of its elements are $k$ times continuously differentiable at $\mathbf{c}.$ I'm really puzzled about the above definition.Conventionally,we say a funcation $f:S\rightarrow \mathbb {R}$ ,with domain $S$ in $\mathbb {R}^{m}$ is continuously differentiable at the point $\mathbf {c}\in \text {int} S$ if $f$ is differentiable in a neighborhood of $\mathbf {c}$ ,and the partial derivatives of the $f$ are continuous at $\mathbf {c}$ . From this statement and the above definition "" if it and all of its first-through $(k-1)$ th-order partial derivatives are continuously differentiable at $\mathbf {c}$ "", I only conclude that all of the first-through $(k-1)$ th-order partial derivatives of $f$ are continuous on a neighborhood of $\mathbf{c}$ , all $k$ th-order partial derivatives of $f$ exist on this neighborhood and are continuous at $\mathbf{c}$ .Why the above defintion said ""equivalently,if all of the first-through $k$ th-order partial derivatives of $f$ exist and are continuous at every point in some neighborhood of $\mathbf{c}$ ""? Why they are equivalent ?  Further ,what is the precise definition about $k$ times continously differentiable at a point $\mathbf{c}\in\mathbb {R}^{m}?$","['multivariable-calculus', 'definition', 'real-analysis']"
4195647,"$f \in L^1$, but $f \not\in L^p$ for all $p > 1$","""Find an $f \in [0,1]$ such that $f \in L^1$ but $f \not\in L^p$ for any $p > 1$."" I've thought about doing something like $$f(x) = \frac{1}{x}$$ where $|f|^p = \frac{1}{x^p}$ doesn't converge when $p > 1$.  But this function isn't itself in $L^1$.  Could someone please give me a hint for how to solve this problem?  I wish there were a situation where you had convergence on the closed half disc $[0,1]$ and divergence on $(1, \infty)$, rather than my current predicament where I have convergence on the open half-disc $[0, 1)$ and divergence on $[1, \infty)$.","['lp-spaces', 'lebesgue-integral', 'real-analysis']"
4195662,On the necessary and sufficient conditions for primitive group action!,"Claim : The transitive group $G$ is primitive on $A$ if and only if $G_a$ is a maximal group of $G$ for all $a \in A$ , where $G_a$ is the stabilizer of $a$ . I've been trying to prove a stronger claim. Concretely, let $P$ be the set of all subgroups of $G$ containing $G_a$ , and $Q$ be the set of all blocks of $A$ containing $a$ . My claim is that there exists a bijective map $f: P \to Q$ . Proof : Given subgroup $G'$ containing $G_a$ , let $f(G')=A = \{ \sigma(a) | \sigma \in G' \}$ . $A$ is indeed a block, for if $\sigma \in G'$ then $\sigma(A) = A$ since $G'$ is subgroup. If $\sigma \notin G'$ , then $\sigma(A) \cap A = \emptyset$ because otherwise we have $\sigma \sigma_1(a)=\sigma_2(a)$ for some $\sigma_1, \sigma_2 \in G'$ , but that means $\sigma_2^{-1}\sigma\sigma_1 \in G_a$ which implies $\sigma \in G'$ , a contradiction. Conversely, for each block $B$ containing $a$ , let the set $G_B = \{\sigma \in G | \sigma(B) = B\}$ . It's clear that $G_B$ is a subgroup containing $G_a$ , this shows that $f$ is surjective. Suppose the map $f$ is not injective, say $G_1$ and $G_2$ are distinct subgroups of $G$ containing $G_a$ , let $\sigma \in G_1$ such that $\sigma \notin G_2 $ , then since $f(G_1)=f(G_2)$ we have $\sigma(a) = \sigma_1(a)$ for some $\sigma_1 \in G_2$ . But this would imply that $\sigma \sigma_1^{-1} \in G_a$ and therefore $\sigma \in G_2$ , we reach a contradiction. I think my proof is right, but I did not use the fact that $G$ acts transitively on $A$ . Moreover, in the proof of the original's claim, the authors did use this condition, but the proof is totally different from mine. So I want to ask if the claim still remains true if we remove the condition that $G$ acts transitively on $A$ ? In other words, is my proof right? Any help would be appreciated! Edit : As @Derek Holt pointed out in the comment below, my stronger claim is wrong since the map $f$ is not well-defined yet, in particular, $f^{-1}(\{a,b,c\}) = f^{-1}(\{a,b\})=\{ (a\text{ } b), e\}$ shows that $f$ is not actually well-defined. Given a block $B$ containing $a$ , the problem came from my proof is that $G_B = \{\sigma \in G | \sigma(B) = B\}$ is still a subgroup containing $G_a$ , but there is no guarantee that $f(G_B) = B$ . In fact, it's only know that $f(G_B) \subset B$ (why?). However, by adding the condition that $G$ acts transitively on $A$ , my stronger claim will become true since in that case, we must have $B \subset f(G_B)$ and therefore $f(G_B) = B$ as desired. Here is the approach: Proof : Since $\sigma (B)=B$ for all $\sigma \in G_B$ , then by the fact that $a\in B$ , we must have $\sigma(a) \in B$ for all $\sigma \in G_B$ . This shows that $f(G_B) \subset B$ . Let $b \in B$ , since $G$ acts transitively on $A$ (here is where we need transitivity), then there is a $\sigma \in G$ sending $a$ to $b$ (i.e $\sigma(a)=b$ ). But then we have $\sigma(B)$ intersects $B$ and by definition of block, we must have $\sigma(B)=B$ which shows that $\sigma \in G_B$ , then $f(G_B)$ contains $b$ . Hence we have $f(G_B) = B$ , completing the proof.","['group-actions', 'group-theory', 'solution-verification']"
4195787,How to evaluate $\lim\limits_{x\to 0}\frac{1-\cos 7x}{3x^2}$?,Evaluate $$\lim\limits_{x\to 0}\frac{1-\cos 7x}{3x^2}.$$ I solved the problem with the Taylor series expansion of $\cos x$ . Here is my solution: $\lim\limits_{x\to 0}\frac{1-\cos 7x}{3x^2}\\ =\lim\limits_{x\to 0}\frac{1-\{1-\frac{(7x)^2}{2!}+\frac{(7x)^4}{4!}-\frac{(7x)^6}{6!}+\dots\}}{3x^2}\\ =\lim\limits_{x\to 0}\frac{x^2(\frac{7^2}{2!}-\frac{7^4x^2}{4!}+\frac{7^6x^4}{6!}-\dots)}{3x^2}\\ =\lim\limits_{x\to 0}\frac{1}{3}(\frac{7^2}{2!}-\frac{7^4x^2}{4!}+\frac{7^6x^4}{6!}-\dots)\\ =\frac {49}{6}$ Can this be solved without using the Taylor series?,"['limits-without-lhopital', 'real-analysis', 'calculus', 'taylor-expansion', 'limits']"
4195805,Sections of a polar action are totally geodesic,"Suppose $G\curvearrowright M$ is an isometric action of a Lie group on a complete Riemannian manifold $M$ , and assume it is polar. This means that the action is proper and there exists a closed (hence complete) embedded submanifold $\Sigma\subseteq M$ (called a section) which meets all orbits orthogonally. It is well known that $\Sigma$ is totally geodesic, but I have not found a convincing proof of that fact. There is a special case where the last statement is easy to prove: the second fundamental form vanishes at regular (and exceptional) points. Indeed, given any $p\in \Sigma$ such that $G\cdot p$ has maximal dimension, $v\in T_{p}(\Sigma)=T_{p}(G\cdot p)$ and $\xi\in T_{p}(G\cdot p)$ , we can find an element $X$ in the Lie algebra $\mathfrak{g}$ of $G$ such that $$\xi=X^{*}(p), \quad X^{*}(q)=\dfrac{d}{dt}\bigg|_{t=0}\operatorname{Exp}(tX)\cdot q.$$ The vector field $X^{*}$ is Killing, so that its covariant derivative is antisymmetric. Let $\mathbb{II}$ be the second fundamental form of $\Sigma$ . Then $\mathbb{II}(v,v)$ is tangent to $G\cdot p$ and $\langle \mathbb{II}(v,v),\xi \rangle=-\langle v,\nabla_{v}X^{*} \rangle=0$ , so $\mathbb{II}(v,v)=0$ . Polarizing, we get $\mathbb{II}=0$ . The usual argument for proving that sections are totally geodesic at all points revolves around the fact that regular points are dense in $\Sigma$ . My problem is that all proofs that I found seem to lack crucial details for it. Here are some examples: In ""Lie Groups and Geometric Aspects of Isometric Actions"", by Alexandrino and Bettiol, it is stated in Exercise 4.9 that the density follows from Kleiner's Lemma (cf Lemma 3.70), but I can't get the connection between the lemma and this fact. In ""Critical Point Theory and Submanifold Geometry"", by Palais and Terng, the authors state that density follows from the theory of Riemannian submersions, without giving further details. In ""Polar Manifolds and Actions"", by Grove and Ziller, the authors state that singular points are isolated along any geodesic, because of the Slice Theorem. This is because if $\gamma$ is any geodesic of $\Sigma$ , and $t_{0}$ lies in the closure of $\{ t\in \mathbb{R}\colon \gamma(t)\in M_{R} \}$ , then $\gamma(t_{0}-\varepsilon)$ and $\gamma(t_{0}+\varepsilon)$ have the same isotropy for sufficiently small $\varepsilon>0$ (again, because of the Slice Theorem), but I don't understand why this is the case. Could somebody elaborate on any of the methods of proof proposed above (preferably the first or the last), or give a reference to a detailed proof of the fact that regular points are dense? Thank you in advance! EDIT I forgot to mention another method of proof, proposed in the book ""Submanifolds and Holonomy"", by Berndt, Console and Olmos. I have an (almost full) solution, which needs to prove the following crucial fact: if $p\in \Sigma$ and there is an open subset $\Omega\subseteq \Sigma$ such that all orbits of the points in $\Omega$ have the same (nonprincipal) type, then $T_{p}(\Sigma)$ is pointwise fixed by the slice representation. If somebody could give a proof of this last fact, I would also accept it as an answer. EDIT 2 I've decided to repost this question on MathOverflow. If anyone is interested in seeing it, you can find it on https://mathoverflow.net/questions/398008/sections-of-a-polar-action-are-totally-geodesic","['submanifold', 'group-actions', 'differential-geometry']"
4195809,"$\left( \frac{r}{\sqrt{r^2-1}}, \frac{r}{\sqrt{r^2+1}} \right)$ are roots of the equation $x^2 - bx + 3 = 0$. What is the value of $b$?","If roots of the equation $x^2 - bx + 3 = 0$ are $\left( \frac{r}{\sqrt{r^2-1}}, \frac{r}{\sqrt{r^2+1}} \right)$ , then what is the value of $b$ ? $1)\pm2\sqrt6\qquad\qquad2)\pm2\sqrt3\qquad\qquad3)2\sqrt6\qquad\qquad4)2\sqrt3$ Here is my approach: We have $\dfrac{r^2}{\sqrt{r^4-1}}=3$ . Hence $\dfrac{r^4}{r^4-1}=9$ and $r^4=\dfrac98\Rightarrow r^2=\dfrac{3}{2\sqrt2}$ . And $b$ is equal to sum of the roots: $$b=\frac{r}{\sqrt{r^2-1}}+\frac{r}{\sqrt{r^2+1}}=\frac{r(\sqrt{r^2+1}+\sqrt{r^2-1})}{\sqrt{r^4-1}}=\frac{\sqrt{r^4+r^2}+\sqrt{r^4-r^2}}{\sqrt{r^4-1}}$$ $$=2\sqrt2\times(\sqrt{\frac98+\frac{3\sqrt8}{8}}+\sqrt{\frac98-\frac{3\sqrt8}{8}})=\sqrt{9+3\sqrt8}+\sqrt{9-3\sqrt8}$$ We have $b^2=24$ . So $b=\pm2\sqrt6$ . My question is, can we solve this problem with other approaches?","['algebra-precalculus', 'quadratics']"
4195816,Is $\mathrm{Var}(Y)\leq \max \mathrm{Var}(Y|X)$?,"Let $E_1,E_2,\cdots, E_n$ be mutually exclusive and disjoint events on a probability space. Let $Y$ be a random variable on the same space. I am trying to check if $\mathrm{Var}(Y)\leq \max_i \mathrm{Var}(Y|E_i)\tag{1}$ By Law of total variance $\mathrm{Var}(Y)=\mathbb{E}[\mathrm{Var}(Y|X)]+\mathrm{Var}(\mathbb{E}[Y|X])$ . So the first term in the total variance formula is less than or equal to $\max_i \mathrm{Var}(Y|E_i)$ but may I know how much can the second term contribute? Is there an example where $(1)$ is false?","['statistics', 'variance', 'probability']"
4195872,Finding minimum value of $1/(p_1n_1) + 1/(p_2n_2) + 1/(p_3n_3) + \dots + 1/(p_kn_k)$,"While solving a statistic problem, I come across an equation: The value of $p_1$ is given, and $p_{i+1} = p_i / (1-(1-p_i)^{n_i})$ . $$E = p_1  (\frac{1}{p_1  n_1} + \frac{1}{p_2 n_2} + \dots + \frac{1}{p_{k-1}  n_{k-1}} + \frac{1}{p_k  n_k})
= p_1  \sum_i\frac{1}{p_i n_i},$$ where $i = 1, \dots, k$ , $0<p_i<1$ , $n_i \in N$ , $n_k = 1$ . The task is to (1) find the number $k$ , and (2) find each $n_i$ , such that $E$ has minimum value. After some brute-force searches with a computer program I wrote, it seems that the $n_i$ and $p_i$ that give the minimum value to $E$ have the property: $n_i/n_{i+1}≈2$ and $p_{i+1}/p_i≈2$ , except the last term. This is all I've got so far. Any thought on solving this problem other than brute-force searches?
Thank you. -- Below are some results of the brute-force searches. For $p_1$ >0.01, this program works well to find the best combination of $n_i$ and $k$ . However, for $p_1$ <0.01, it will be too much work to exhaust the search.","['expected-value', 'statistics']"
4195986,Determine if $\prod\limits_{i=0}^\infty A_i$ countable or uncountable,"Let $A_i$ be a family of non-empty sets for every $i \in N$ Determine if $\prod\limits_{i=0}^\infty A_i$ is always countable, some times countable, or never countable. a) $|A_i| \ge 2$ for each $i \in \mathbb{N}$ $A$ is the set of all infinite sequences whereby $A=\{(a_0,a_1,a_2,\ldots) \mid a_k \in A_k \forall k \in \mathbb {N} \}$ . To show that $A$ is uncountable, it suffices to show that $\not \exists$ bijection $f: \mathbb{N} \to A$ . Assume for contradiction that $\exists$ bijection $f: \mathbb{N} \to A$ . Note that $f(0)=(a_0^{(0)},a_1^{(0)},a_2^{(0)},\ldots)$ and that $f(1)=(a_0^{(1)},a_1^{(1)},a_2^{(1)},\ldots)$ This means that $f(K)=(a_0^{(k)},a_1^{(k)},a_3^{(k)},\ldots)$ . So each natural number is mapped to some infinite sequence. To show that $f$ is not surjective, let $b \in A \ni b=(b_0, b_1, b_2, b_3, \ldots)$ where $b_k \in A_k \forall k \in \mathbb{N}$ . Since $|A_0| \ge 2, |A_1| \ge 2, |A_2| \ge 2,\ldots,|A_i| \ge i$ , let $b_i \in A_i \not \{a_i^{(i )}\}$ for every $i \in \mathbb{N}$ . So the i-th term of $b$ will be different from the i-th term of $f$ , and hence $b \notin f(\mathbb{N})$ . Thus we have $b \in A \ni f(i) \ne b \forall i \in \mathbb{N}$ . However, this contradicts the premise that $f$ is surjective (i.e. $f(\mathbb{N})=A)$ . Therefore, $A$ is uncountable. b) $\exists N \in \mathbb{N} \ni \forall i > N$ , $|A_i| \ge 2$ . I'm having trouble figuring out what this means. Does this mean $|A_1| \ge 2, |A_2| \ge 2, |A_3| \ge 2, \ldots,|A_n| \ge 2$ for every $n \in \mathbb{N}$ except $n=0$ ? I'm thinking that in this case, the zeroth term of $b$ will be the zeroth term of $f$ , so $f(0)=b$ . But $f$ may not be injective since $f(1)=b$ , but $0 \ne 1$ . So it will essentially be some times countable. c) $\exists N \in \mathbb{N} \ni \forall i > N$ , $|A_i| = 1$ . As $f(K)=(a_0^{(k)},a_1^{(k)},a_3^{(k)},\ldots)$ , where each $a_i^{(k)} \in A_i$ for every $k \in \mathbb{N}$ and $|A_n|=1$ . This means that $f$ cannot be surjective since every natural number maps to $b$ . Thus $\prod\limits_{i=0}^n A_n$ is uncountable. However, $\prod\limits_{i=1}^{n}A_n$ is countable since it is finite; that is, $\left|\prod\limits_{i=1}^n A_n\right|=|A_1| \times |A_2| \times |A_3| \times\cdots\times |A_n|=1$ ? d) $\exists N \in \mathbb{N} \ni \forall n > N$ , $|A_n| = 1$ , and $\exists$ injection $f:\mathbb{N} \to A$ . I'm not sure about this. My understanding is that $A$ should infinite since by definition, a set $A$ is infinite if there exists an injection $f: \mathbb{N} \to B$ . Any insight would be appreciated. Thanks!","['elementary-set-theory', 'infinity']"
4195994,Translating Between Fourier Sine Series and Fourier Cosine Series,"A mathematician can choose to represent a target function defined from $0$ to $L$ using a Fourier Sine Series or a Fourier Cosine Series at her discretion, by temporarily introducing either an odd extension or an even extension, respectively, regardless of whether the target function is actually even, odd, or neither.  I want to know if there is a way to translate one type of series into the other, once the choice has been made. I am working with the sinesum function in Pure Data as a means of synthesizing waveshapes, which seems to eat numeric Fourier Sine Series coefficients as parameters.  However, I need to shift some waves vertically, which is easier to do with a Fourier Cosine Series.  There is a cosinesum function which eats Fourier Cosine Series coefficients, but it is not as well documented as sinesum. Given the numeric Fourier Sine Series coefficients of a target wave, can I convert them into numeric Fourier Cosine Series coefficients mathematically? If I can reverse engineer the pattern of the numeric coefficients as a mathematical expression, can I then convert that expression into an expression for the Fourier Cosine Series coefficients? Note that I am not seeking a solution like this , where the answer contains an explicitly shifted series of the other type, but the unshifted series the mathematician would have arrived at if she'd used a different extension of the target function.","['fourier-analysis', 'music-theory', 'functions', 'fourier-series', 'trigonometry']"
4196019,Help with Polar Coordinates proof in Calculus,"so I have this Polar Coordinates Homework Problem, and I don't know really know how to start/prove it. The problem goes as follows: Let $f$ be a differentiable function, and let $P$ be the point $(\theta,f(\theta))$ on the polar graph of $r = f(\theta).$ Let $\alpha$ be the angle between the tangent line to the graph at $P,$ and the line $OP,$ where $O$ is the origin. Assuming that $f'(\theta) \neq 0,$ show that $\tan\alpha=\frac{f(\theta)}{f'(\theta)}$ From what I've understood from this problem, if I extended a tangent line to P all the way to the x-axis, and called that point Q, I make $\angle PQO=\pi-\theta-\alpha,$ beyond that, I am lost.","['calculus', 'derivatives', 'polar-coordinates']"
4196028,Do harmonic functions span the space of functions on manifolds?,"If one considers the Laplace (or Helmholtz) equation in two dimensions, then through separation of variables in plane polar coordinates, the azimuthal dependence is seen to be of the form of a harmonic $e^{in\phi}$ , where $n$ is an integer. The set of harmonics form a basis for any (reasonable) function on a circle i.e. any function with $2\pi$ periodicity (the Fourier series). Similarly, if one considers the Laplace (or Helmholtz) equation in three dimensions, then through separation of variables in spherical polar coordinates, the angular dependence is seen to be of the form of a spherical harmonic $Y_{l}^{m}(\theta,\phi)$ , where $l$ and $m$ are integers with $|m|<l$ . The set of spherical harmonics form a basis for any (reasonable) function on a sphere. Is there a reason why solutions to Laplace's equation form a basis for functions on manifolds of the space? I suspect there is a theorem for this but I've either never come across it or it has completely slipped my mind. Thanks in advance for any help.","['harmonic-functions', 'harmonic-analysis', 'laplacian', 'linear-algebra', 'partial-differential-equations']"
4196035,The existence of a closeness matrix,"A matrix $U$ is a closeness matrix to $A$ if for every $\epsilon\ne0$ , $A+\epsilon U$ is invertible. For example, for $0$ , $I$ is a closeness matrix as $\epsilon I$ is invertible, and for $I$ a matrix with a single non-diagnal 1 is  a closeness matrix. The general question is does a closeness matrix alway exist? If $A$ is diagnalizable, $A=SDS^{-1}$ and $P$ is a matrix with a single non-diagonal 1, then $SPS^{-1}$ is a closeness matrix. A specific example I've not managed to show has a closeness matrix is $$\begin{pmatrix}1&1\\0&1\end{pmatrix}.$$","['matrices', 'linear-algebra']"
4196053,Counting words of length $n$ from $k$-sized alphabet with no substring of $k$ consecutive distinct letters,"How many words of length $n$ are there, if we have an alphabet of $k$ distinct letters, but the words cannot contain any substring that is made of $k$ consecutive distinct letters, i.e, no $k$ -length substring that consists of the entire alphabet? Other than that, no restrictions apply: any amount of distinct letters may be used throughout the entire word, and any letter can be used as many times as we like, as long as every substring inside the length $n$ word complies with the rules above. There is the obvious case of $k=2$ which results in $2$ words, for every $n$ , because you can only start with either letter, and they alternate. For the larger case, I have come up with a recursive formula: $$C(n,k)=f(0,0,k)$$ $$f(i,d,k)=\begin{cases}\displaystyle(k-d) \cdot f(i+1,d+1,k) + \sum_{c=1}^df(i+1,c,k) & i<n,d<k\\ 1 &i=n,d<k\\ 0 &i>n\\ 0 &d\geq k
\end{cases}$$ With $d$ being the current length of consecutive distinct letters, and $i$ the current word length. At every step, we can either use a letter other than the previous $d$ letters, in which case the length is increased by one, and the chain-length $d$ of distinct consecutive letters is also increased by one. In this case, there are $(k-d)$ such letters we can use at the stage, each subsequently resulting in the same contribution. Or, we can use a letter already in the last $d$ letters. In this case, the position of the letter matters. If we use the last of the $d$ letters, a whole new chain begins. If instead we use the second last $d$ letter, then a chain of length $2$ of consecutive distinct letters begins. The $3$ rd last would result in a chain of length $3$ and so on. I was wondering if there is a another way to count the amount of such words, perhaps using matrices or combinatorics?","['combinatorics-on-words', 'recurrence-relations', 'matrices', 'combinatorics', 'discrete-mathematics']"
4196055,Reference request: Pedagogical/tutorial articles on the historical development of modern real analysis.,"Are there any pedagogical/tutorial articles presenting the historical development of modern real analysis? Context and some information on my background. I am currently self-studying Understanding Analysis by Stephen Abbott (2015), with a view to moving to Principles of Mathematical Analysis by Walter Rudin (1976) at some point. This is my first exposure to a more rigorous, abstract, and at times beautiful, style of mathematics; most of my previous exposure to mathematics has been skewed towards applied/computational areas. Hence  the type of article I am looking for is at the 1st year undergraduate level. Article style. Primarily, I am soliciting recommendation on tutorial articles which contextualise mathematical developments of standard topics in analysis within a historical perspective, as a supplement to some of the epilogues in Abbott. I have previously read articles aimed at mathematically literate, but not necessarily deeply technically proficient audiences whereby the author introduces the topic in layman's terms, then some mathematical formalism, then summarises some crowning achievements of the field. All of this is interwoven with a historical narrative of how these tools developed through attempts to resolve seminal problems/paradoxes, together with broader debates in mathematics at the time. As an example, in probability and statistics, I really enjoyed Part II: A tutorial on probability, measure and the laws of large numbers (page 56 onwards) of the following article by Richard Mauldin in a special edition of Los Alamos Science dedicated to the mathematician Stanislaw Ulam: Mauldin, R. D. (1987). Probability and nonlinear systems. Los Alamos Science 15, Special Issue 1987 15. 52-90. Rationale. I've had experience with both formal education and self-study in economics, machine learning and statistics. Generally I've found that in the case of formal education, time constraints generally mean that the courses are taught with a view to getting students up to speed quickly on a battery of techniques/proofs/computational skills. In the case of mathematical monographs, I've often found that historical developments are often treated as ""literature reviews"" at the end of a chapter. This is not a deficiency as the constraint of fluent mathematical flow often does not allow for interwoven comments on historical developments. What can be lost however, is valuable historical context. As a final example, when I studied time-series techniques as part of formal economics training using Time-Series Analysis by Hamilton, and during self-study using Introductory Time Series Analysis by Brockwell and Davis, ergodicity and ensembles of time series are presented without much discussion. It was only when I skimmed some of the communications engineering roots of time series e.g. the work of Norbert Wiener, did it become apparent that modern persentations have decontextualised these methods from their original statistical mechanics overtones (it seems that Norbert Wiener discussed time series in relation to Birkhoff's ergodic theorem). Any suggestions from the community here would be greatly appreciated.","['reference-request', 'real-analysis']"
4196102,Computation of $\int_0^1 \frac{\arctan^2 x\ln x}{1+x}dx$,"I'm searching for a ""simple"" proof of: \begin{align}\int_0^1 \frac{\arctan^2 x\ln x}{1+x}dx=-\frac{233}{5760}\pi^4-\frac{5}{48}\pi^2\ln ^2 2+\text{Li}_4\left(\frac{1}{2}\right)+\frac{7}{16}\zeta(3)\ln 2+\frac{1}{24}\ln^4 2+\pi \Im\left(\text{Li}_3\left(1+i\right)\right)-\frac{1}{4}\text{G}\pi\ln 2\end{align} The context:
I have written a script for Pari GP to search heuristically for certain close-forms.
Testing my script with the above integral i was lucky enough to find something.
The script searchs for integer linear relation between the integral and some constants: \begin{align}\pi^4,\pi\ln^3 2,\pi^2\ln^2 2,\pi^3\ln 2,\text{Li}_4\left(\frac{1}{2}\right),\zeta(3)\ln 2,\zeta(3)\pi,\ln^4 2,\pi \Im\left(\text{Li}_3\left(1+i\right)\right), \Im\left(\text{Li}_3\left(1+i\right)\right)\ln 2,\text{G}^2,\text{G}\ln^2 2,\text{G}\pi^2,\text{G}\pi\ln 2\end{align} NB:
Using the algorithm with $\displaystyle\int_0^1 \dfrac{\ln^2(1+x^2)\ln x}{1+x}dx$ gives someting too. Addendum: PARI GP script: beta(n)={intnum(x=0,1,(-log(x))^(n-1)/(1+x^2))};
lindep4(x)={
NAME=[""x"",""Pi^4"",""Pi log(2)^3"",""Pi^2 log(2)^2"",""Pi^3 log(2)"",""polylog(4,1/2)"",""zeta(3) log(2)"",""zeta(3) Pi"",""log(2)^4"",""Pi imag(polylog(3,1+I))"",""log(2) imag(polylog(3,1+I))"",""Catalan^2"",""Catalan log(2)^2"",""Catalan Pi^2"",""Catalan log(2) Pi"",""beta(4)"",""imag(polylog(4,1+I))""];
VAL=[x,Pi^4,Pi log(2)^3,Pi^2 log(2)^2,Pi^3 log(2),polylog(4,1/2),zeta(3) log(2),zeta(3) Pi,log(2)^4,Pi imag(polylog(3,1+I)),log(2) imag(polylog(3,1+I)),Catalan^2,Catalan log(2)^2,Catalan Pi^2,Catalan*log(2)*Pi,beta(4),imag(polylog(4,1+I))];
L=lindep(VAL);
for(i=2,length(L),if(-L[i]/L[1]>0,print1(""+"",-L[i]/L[1],NAME[i]));if(-L[i]/L[1]<0,print1(-L[i]/L[1],NAME[i])));
} for example: \p 100
lindep4(intnum(x=0,1,atan(x)^2*log(x)/(1+x))) NB: To improve the script add $\beta(4)$ value and some polygamma values. PS: the PARI GP script has been updated to take into account more integrals. Try this one: $\displaystyle \int_0^1\frac{\ln^2(1+x^2)\ln x}{1+x^2}dx$","['integration', 'catalans-constant', 'definite-integrals']"
4196133,showing a path cannot exist,"Let $R = S\cup T$ where $S = \{(x,y) \in \mathbb{R}^2 : x=0, y \in [0,1]\cap \mathbb{Q}\}$ and $T = \{(x,y) \in \mathbb{R}^2 : x > 0, y \in [0,1]\backslash \mathbb{Q}$ . Suppose there was a path from a point $h\in T$ to a point $g\in S$ , say $h=(z,p)$ and $g=(0,q)$ , where $z>0$ , $p$ is irrational, $q$ is rational, and both $p,q\in[0,1]$ . We may assume without loss of generality that $\psi:[0,1]\to R$ is a path with $\psi(0)=h$ and $\psi(1)=g$ . Prove that $\psi$ cannot exist. Let $L_p:=\{(x,p):x>0\}$ and define $K=\{t\ge0:\psi([0,t])\subseteq L_p\}$ and $s=\sup K.$ I think that $K$ is both open and closed, but I'm not sure how to prove this. If I can prove this though, the contradiction is easy as then $\psi(1) \in L_p$ but $\psi(1)=g\not\in L_p.$ I tried proving that any sequence $(a_n)$ in $K$ that converges to $b$ in $[0,1]$ converges in $K$ , but I was unsuccessful. I need to use the continuity of $\psi$ and the fact that $a_n\to b.$ I tried to do the following. Suppose for a contradiction that $b\not\in K.$ Then we may choose $a=(a_1,a_2)\in \alpha([0,b])\backslash C_r.$ Since $a_n\to b,\forall \epsilon > 0,\exists N\in\mathbb{N}$ so that $n\geq N$ implies $|a_n-b| < \epsilon.$ Since $(a_1,a_2)\not\in C_r,$ either $a_1 = 0$ or $a_2\neq r.$ Since $\psi$ is continuous, for all $\epsilon > 0,\exists \delta > 0$ so that for all $x\in [0,1], |x-b|<\delta \Rightarrow |\alpha(x)-\alpha(b)| <\epsilon.$ But here I'm stuck. Also, I'm not sure how to show it's open; I tried proving the complement was closed, but was still unsuccessful.","['connectedness', 'real-analysis', 'continuity', 'elementary-set-theory', 'general-topology']"
4196147,"Show $|f(1)|<\frac{\sqrt{5}}{2}\|f\|_{L^2([0,1])} $","Let $f(x)$ be a twice continuously differentiable real-valued function on the interval [0,1]. If $$f''(x) +xf(x) = 0 \text{, } f'(0)=0 \text{, and } \int_0^1 f(x)dx = 0$$ show that $$ |f(1)|\leq \frac{\sqrt{5}}{2}\|f\|_{L^2([0,1])}$$ So far I have the following: Since $f''(x) = -xf(x) $ we have that $$f'(x) = \int_0^x f''(t)dt +f'(0) = \int_0^x -tf(t)dt$$ I see that we will need to use Cauchy-Schwarz at sometime, but I haven't gotten the $\sqrt{5}/2$ out of it yet. Any hints would be appreciated!","['cauchy-schwarz-inequality', 'lebesgue-integral', 'real-analysis']"
4196163,Integrality of a modified Catalan recurrence relation,"How can it be proved that the sequence $(a_{n})_{n\geq0}$ satisfying the recurrence $$(n+1)a_{n} - r(r-1)(r(n-1)+1)a_{n-1}=0\quad a_{0}=1 \quad r \geq 2 \in \mathbb{N}$$ is always a sequence of integers for any $r$ ? In this case, the recurrence for the Catalan numbers (which are integers) is given by $r=2$ : $(n+1)a_{n} - 2(2(n-1)+1)a_{n-1}=(n+1)a_{n} - (4n-2)a_{n-1}=0\quad a_{0}=1$ As it was pointed out in the commentary $$a_{n}=\dfrac{((r - 1) r^2)^n Γ\left(n + \dfrac{1}{r}\right)}{Γ(n + 2) Γ\left(\dfrac{1}{r}\right)}$$ Using the formula found in https://en.wikipedia.org/wiki/Particular_values_of_the_gamma_function $$Γ\left(n + \dfrac{1}{r}\right)=Γ\left(\dfrac{1}{r}\right)\dfrac{(rn-r+1)!^{(r)}}{r^n}$$ where $(n)!^{(r)}$ is the $r^{th}$ multifactorial of $n$ . Therefore, for $n\in \mathbb{N}_{0}$ and $r\geq 2 \in \mathbb{N}$ $$a_{n}=\dfrac{(r-1)^{n}r^{n}(r(n-1)+1)!^{(r)}}{(n+1)!}$$ Thus, the problem reduces to show that $(n+1)!$ divides $(r-1)^{n}r^{n}(r(n-1)+1)!^{(r)}$ The motivation behind the question arises from a computational experiment to find rational approximations $\dfrac{p_{n}}{q_{n}}$ to $Γ\left(\dfrac{1}{r}\right)$ , looking for a proof of the irrationality of this number. Update: The following formula can be generalized to higher $r$ , but we will deal with $r=3$ for simplicity: $$a_{n}=\dfrac{2^{n}3^{n}(3n-2)!^{(3)}}{(n+1)!}$$ By the definition of the multifactorial: $$(3n-2)!^{(3)}(3n-1)!^{(3)}(3n)!^{(3)}=(3n)!$$ and $$(3n)!^{(3)}=\prod_{i=1}^{n} 3i=3^{n}n!$$ So, $$3^n(3n-2)!^{(3)}=\dfrac{1}{(3n-1)!^{(3)}}\dfrac{3n!}{n!}$$ Plugging this into the first expression $$a_{n}=\dfrac{2^{n}}{(3n-1)!^{(3)}}\dfrac{3n!}{n!}\dfrac{1}{(n+1)!}$$ and multiplying many times by 1 ( $2n!/2n!$ , $n!/n!$ ), we get $$a_{n}=\dfrac{2^{n}n!}{(3n-1)!^{(3)}}\dfrac{3n!}{n!2n!}\dfrac{2n!}{(n+1)!n!}=\dfrac{2^{n}n!}{(3n-1)!^{(3)}}\binom{3n}{n}\dfrac{1}{n+1}\binom{2n}{n}=\dfrac{2^{n}n!}{(3n-1)!^{(3)}}\binom{3n}{n}C_{n}$$ where $C_{n}$ are the Catalan numbers So now the main question is: what is the $p$ -adic valuation (exponent of the prime $p$ in the factorization of the number) of the expression on the left? $$\nu_{p}\left(\dfrac{2^{n}n!}{(3n-1)!^{(3)}}\right)$$ The primes whose $p$ -adic valuation is negative will be in the denominator, and these should divide $\binom{3n}{n}C_{n}$ . If we know this, in theory, we should be able to use Kummer's theorem to settle the divisibility question, but I wasn't able to tackle it. Interestingly, while trying some values of $n$ , not all of these prime factors that are in the denominator divide $\binom{3n}{n}$ : some of them divides $\binom{3n}{n}$ and some of them divides $C_{n}$ . Example with $n=13$ : $$\dfrac{2^{13}13!}{(3*13-1)!^{(3)}}\binom{3*13}{13}=\dfrac{15412156416}{115}$$ but $115=5*23|C_{13}$","['catalan-numbers', 'integers', 'recurrence-relations', 'combinatorics', 'sequences-and-series']"
4196182,Partial derivative of a diagonal matrix w.r.t a vector,"I am trying to find the second partial derivative of the function $Y=diag\boldsymbol(S)\mathbb P diag \boldsymbol(\beta)diag^{-1}(\mathbb P^{T}\boldsymbol S +\mathbb P^{T}\boldsymbol E + \mathbb P^{T}\boldsymbol I +\mathbb P^{T}\boldsymbol R )\mathbb P^{T}\boldsymbol I$ where $\mathbb P=[(p_{ij})]_{n\times n},\boldsymbol S, \boldsymbol E, \boldsymbol I$ and $\boldsymbol R $ are $n\times 1$ vectors with respect to $\boldsymbol S$ . I am not sure of how to approach this. I was thinking of using the product rule whereby I will take $$ X=\underbrace{diag\boldsymbol(S)\mathbb P diag \boldsymbol(\beta)}_{Y}\underbrace{diag^{-1}(\mathbb P^{T}\boldsymbol S +\mathbb P^{T}\boldsymbol E + \mathbb P^{T}\boldsymbol I +\mathbb P^{T}\boldsymbol R )\mathbb P^{T}\boldsymbol I}_{Z}$$ so that $$ \frac{\partial X}{\partial \boldsymbol S}=\frac{\partial Y}{\partial \boldsymbol S}Z+Y\frac{\partial Z}{\partial\boldsymbol S}$$ This is proving difficult to achieve as I am not sure if what I am thinking of doing is correct. For example, I was thinking of taking $D=diag(\mathbb P^{T}\boldsymbol S +\mathbb P^{T}\boldsymbol E + \mathbb P^{T}\boldsymbol I +\mathbb P^{T}\boldsymbol R )$ so that $$\frac{\partial Z}{\partial S}=-D\frac{\partial D}{\partial S}D^{-1}\mathbb P^{T}\boldsymbol I$$ where $$\frac{\partial D}{\partial S}=\mathbb P^{T}$$ Is this process correct? Because I am imagigning that this method (for the derivative of the inverse of a matrix) can only be applied when one is finding the derivative w.r.t a matrix and not w.r.t a vector. Could somebody help me obtain $\frac{\partial X}{\partial\boldsymbol S}$ and subsequently $\frac{\partial^{2} X}{\partial\boldsymbol S^{2}}$ , $\frac{\partial^{2} X}{\partial\boldsymbol I\partial \boldsymbol E}$ . I will appreciate it very much.","['matrices', 'matrix-calculus', 'partial-derivative', 'tridiagonal-matrices']"
4196218,Solve in prime numbers the equation $p^q+q^r=r^p$,"Find all triples of prime numbers $(p,q,r)$ such that $$p^q+q^r=r^p.$$ I proved that when $r=2$ , the equation becomes $$p^q+q^2=2^p.$$ Then I tried to use reciprocity laws and Fermat's little theorem. I could prove that $p\equiv 7\pmod 8$ and that $p>q$ . The equation appeared in some olympiad . They asked to prove that $r=2$ . So I am trying to find at least one triple.","['contest-math', 'number-theory', 'elementary-number-theory', 'diophantine-equations', 'prime-numbers']"
4196220,Is the series $\sum_{i=k}^\infty\frac{\sin\left(\frac{x}{i}\right)}{i}$ bounded?,"Comparison with $\dfrac{1}{i^2}$ shows that the series $$\sum_{i=k}^\infty\frac{\sin\left(x/i\right)}{i}$$ is convergent. However, the function of $x$ thus obtained appears to be bounded, with the bound approaching zero in $k$ . I have no idea how to prove this, every method I know gives me no better a bound than the obvious $\dfrac{\pi^2x}{6}$ . My suspicion is that prior to reaching $\left|\dfrac{x}{i}\right|<1$ , the values of $\dfrac{x}{i}$ modulo $2\pi$ have an asymptotic distribution which helps similar to other identities with alternating signs bounded by $\dfrac{1}{i}$ .",['sequences-and-series']
4196224,Why is the ratio $\overline{HO}/\overline{HM}$ constant for all $\angle IHG$?,"In the first attached diagram, the smaller circle is centered on point $H$ , and the larger circle is centered on point $G$ . The long line connecting points $L$ and $K$ is the perpendicular bisector of segment $\overline{IH}$ . Since $H$ is the center of the smaller circle, $M$ is the midpoint between $L$ and $K$ . Finally, the point $O$ is the circumcircle center of triangle $LKG$ . I notice in Geogebra that the ratio $\overline{HO}/\overline{HM}$ is constant for any angle $\angle IHG$ . Why? I cannot come up with a proof. It's starting to drive me crazy. To be clear, the focal distance $\overline{HG}$ and circle radii are held constant as angle $\angle IHG$ varies. It might also be useful to consider (second attached diagram) that the intersection of the radius $\overline{IG}$ with segment $\overline{LK}$ is a (pink) point $P$ on the blue ellipse with focii $H$ and $G$ .","['euclidean-geometry', 'conic-sections', 'geometry', 'geometric-construction']"
