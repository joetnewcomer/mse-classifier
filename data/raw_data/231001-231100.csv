question_id,title,body,tags
4799373,"Why is this the ""most general"" first order ODE that can be solved?","Braun's book (p. 58) says that the ODE $$\frac{d}{dt}[\Phi(t,y)]=0\tag{1}$$ is clearly the most general first-order differential equation
that we can solve . Of course, all equations having that form can be  (implicitly) solved: $$\Phi(t,y)=c.$$ However, the converse (stated by Braun) is not ""clear"" for me. So, how could we explain it? For example, the most general first-order ODE is $$F(t,y,y')=0.\tag{2}$$ Suppose that this ODE have a solution $$G(t,y)=0.\tag{3}$$ Doesn't Braun's statement mean that (2) and (3) imply that $(2)$ can be written in the form (1)? If so, how to prove it? If not, what does Braun mean?",['ordinary-differential-equations']
4799449,Limit of a Sequence of Spiraling Points,"The Problem Starting with the vertices $P_1(0,1), P_2(1,1), P_3(1,0), P_4(0,0)$ of a square, we construct further points as shown in the figure: $P_5$ is the midpoint of $P_1P_2$ , $P_6$ is the midpoint of $P_2P_3$ , $P_7$ is the midpoint of $P_3P_4$ , and so on. The polygonal spiral path $P_1P_2P_3P_4P_5P_6P_7\cdots$ approaches a point $P$ inside the square. Find the coordinates of $P$ . [ Calculus by James Stewart, $7^\text{th}$ edition, chapter $11$ , problem plus, problem $18$ ] What I Have Tried First of all, let $P_n=(x_n,y_n)$ . We will first consider only $x$ -coordinates first because the $y$ -coordinate of $P$ can be found similarly. I have proven using induction that $\tfrac12x_n+x_{n+1}+x_{n+2}+x_{n+3}=2$ for all $n\geq1$ as follows. Base Case ( $n=1$ ) Since $x_1=0, x_2=1, x_3=1, x_4=0$ , we have $\tfrac12x_1+x_2+x_3+x_4=\tfrac12\cdot0+1+1+0=2.$ Induction ( $n>1$ ) Assume that $\tfrac12x_n+x_{n+1}+x_{n+2}+x_{n+3}=2$ is true for $n=k$ , then \begin{equation}
x_{k+2}+x_{k+3}=2-\tfrac12x_k-x_{k+1}.
\end{equation} Also observe that $x_{n+4}=\tfrac12(x_n+x_{n+1})$ . Therefore \begin{equation}
\begin{split}
\tfrac12x_{k+1}+x_{k+2}+x_{k+3}+x_{k+4}&=\tfrac12x_{k+1}+(2-\tfrac12x_k-x_{k+1})+x_{k+4}\\\\
&=\tfrac12x_{k+1}+2-\tfrac12x_k-x_{k+1}+\tfrac12(x_k+x_{k+1})\\\\ 
&=2
\end{split}
\end{equation} and so it is true for $n=k+1$ . So if $\lim_{n\to\infty}x_n$ exists, we can apply the limit as $n$ approaches $\infty$ to both sides of the equation to get (letting $\lim_{n\to\infty}x_n=L_x$ ) $$\tfrac12L_x+3L_x=2$$ or $$L_x=\tfrac47.$$ My Question The problem arises: how do we prove that $\lim_{n\to\infty}x_n$ does exist? All I've been able to do is prove that $0\leq x_n\leq1$ for all $n\geq1$ . I couldn't find anything on the web that could help. Thanks in advance.","['limits', 'calculus', 'polygons', 'sequences-and-series']"
4799452,Reference request for the existence of Sobolev traces on Lipschitz domains,"Let $\Omega \subseteq \mathbb R^d$ be a bounded Lipschitz domain. Then it is well known that the trace operator $$ \operatorname{tr} \colon W^{1, 1}(\Omega) \to L^1(\partial \Omega)$$ is well-defined and bounded. However, I was unable to find a citable reference for this fact. The references I found either focus on the $L^2$ -setting, assume more regularity of the boundary of $\Omega$ (mostly $C^1$ -boundary) or are kind of obscure. Another reference, is the seminal paper of Gagliardo for that fact, the drawback of this reference being that it is written in Italian. Nonetheless, I cannot imagine that there is not a well-cited book serving as a standard reference for PDEs or Sobolev spaces that contains the result in the above form. So it would be very nice if someone would know a reference including the result. Thanks in advance!","['harmonic-analysis', 'reference-request', 'sobolev-spaces', 'functional-analysis', 'partial-differential-equations']"
4799464,Calculating the intersection point of a hyperbola and a line,"I have a fully automated display program in which a hyperbola (being defined as $\frac{(y-k)^2}{a^2}-\frac{x^2}{b^2} = 1$ or $y=\sqrt{a^2(\frac{x^2}{b^2}+1)}+k$ because I only care about the upper curve is intersecting with a straight line $y-y_1=m(x-x_1)$ and I want to find the $x$ coordinate where they intersect. The issue is that pretty much everything is automated, so all the constants $a$ , $b$ , $k$ , $y_1$ , $m$ , and $x_1$ all need to remain as variables. My algebra skills are not nearly refined enough to solve for $x$ in the equation $\sqrt{a^2(\frac{x^2}{b^2}+1)}+k = m(x-x_1)+y_1$ , so if anybody can help out, or find an easier way to calculate the intersection, I'd appreciate it tons.","['algebra-precalculus', 'geometry']"
4799506,Can You Prove that a Function Is Holomorphic Without Appealing to Cauchy-Integral Formula,"Suppose that the function $f: U \rightarrow \mathbb{C}$ is a holomorphic function on some domain $U$ . Assume that $U$ is simply connected connected and that the point $z_{0} \in U$ . Define a function $g(z)$ as follows: \
\ $$g(z) = \frac{f(z) - f(z_{0})}{z-z_{0}} \ if \ z \neq z_{0}$$ $$g(z) = fâ€™(z_{0})$$ Is it possible to show, WITHOUT assuming the Cauchy Integral Formula, that $g(z)$ is complex differentiable (holomorphic) at $z=z_{0}$ . More generally, can it be shown that if a function $F: \mathbb{R}^{2} \rightarrow \mathbb{R}^{2}$ is real differentiable, satisfies Cauchy-Riemann-Equations on $U-\{ z_{0} \}$ , and if $\lim_{z \rightarrow z_{0}} F(z)$ is finite, then $F$ is real differentiable at $z = z_{0}$ and $F$ satisfies the Cauchy-Riemann Equations at $z = z_{0}$ . My initial thoughts were that we could use the continuity of the partial derivatives to show that the Cauchy-Riemann Equations must be satisfied and $z = z_{0}$ . We could also use the fact that $F$ is real differentiable on $U - \{ z_{0} \}$ along with continuity to show that $F$ is also real-differentiable at $z = z_{0}$ .","['complex-analysis', 'derivatives']"
4799517,In a right triangle $ABC$ calculate the length of $BC$ if $AP=PC=MC=1$,"In a right triangle $ABC$ where the angle $ABC = 90^{\circ}$ , on the side $BC$ is placed a point $P$ and on $AC$ a point $M$ so that $BM$ and $AC$ are perpendicular and $AP=PC=MC=1$ . What would be the length of $BC$ ? No plot is given so I plotted it in GeoGebra. I drew the segment $MP$ to create the isosceles triangle $MPC$ but I did not get any result. I also tried to create the isosceles triangles $MPC$ , $APC$ , and the right triangles $BMC$ , $BMA$ and $ABP$ . I also placed a point at the intersection of $BM$ and $AP$ called $D$ , with this point I created another right triangle $ADM$ . I really appreciate any help you can provide.","['triangles', 'trigonometry', 'angle', 'geometry']"
4799556,Linear ODEs on random orthogonal matrices,"I am working on a music synthesizer project. We are just kind of playing around trying to get an interesting way to vary N parameters with a nice balance of predictability and surprise.  I had an idea to choose a random orthogonal (complex) NxN matrix $\mathbf{A}$ , and then solve the linear ODE $y' = \mathbf{A}y$ (using Euler's method).  But something strange happens -- there are a few seconds of interesting semi-chaos, but then things quickly seem to start to settle into a very simple periodic pattern, with all parameters getting stuck in lock-step with each other, all with the same period and even most with the same phase. My question is, is this behavior some interesting property of ODEs on random matrices, or should this not be happening and my solver has some problem?  What is the typical behavior of $e^{\mathbf{A}t}$ for $\mathbf{A}$ random and orthogonal? Possibly important detail(?):  after each step of Euler's method, I normalize the vector (otherwise I start getting exponential blowup from rounding errors). $\mathbf{A}$ is orthogonal so in a perfect world this wouldn't do anything. But I'm intuitively worried this normalization step is causing some damping of the system that is suppressing its interesting dynamics.","['random-matrices', 'ordinary-differential-equations']"
4799566,"Why is there Symmetry (Equality) between ""the first marble being blue"" and ""the second marble being blue"" here?","This problem is of my own construction, which I have constructed to understand Symmetry better. Say that we have three bags, each having blue and red marbles. In the bag 1, $1 \over 2$ of the marbles are blue; in bag 2, $1 \over 3$ of the marbles are blue; in bag 3, $1 \over 4$ of the marbles are blue. Say our experiment consists of first randomly choosing a bag, grabbing a marble from it, and then choosing another bag, and grabbing a marble from this new bag. When considering the probability of the first marble we pick being blue, I am trying to understand why this is the same as the probability of the second marble we pick being blue, by a symmetrical argument (not just a brute force calculation of the probabilities to show they are equal). So far, I have tried to show that every sample point in the event of the first marble being blue corresponds to a sample point of equal probability in the event of the second marble being blue - and I show such correspondence by switching the first marble we have grabbed with the second. And then the bijection is straightforward to prove. However, I am struggling with arguing that when we swap which marble we have grabbed in the first and second spots, that this necessarily has the same probability of us sampling it - although I certainly feel intuitively that this is so, I can't explain why it is so at all. So why is there Symmetry between ""the first marble being blue"" and ""the second marble being blue"" here? EDIT: Is it valid to justify why the experiments of picking from the first bag and picking from the second bag after it are the same by saying that because we know nothing of the first bag and marble drawn before we drew the second, we act as if it never happened when drawing from the second bag? As if we say that the second experiment was different than the first, then given this we could possibly say what more likely happened on the first picking of the bag, in such a way that would contradict the probabilities we have for the first picking of the bag and marble?","['symmetry', 'discrete-mathematics', 'probability']"
4799607,Is the distribution of the Pearson correlation symmetric around zero?,"Let $(X_1, Y_1), \ldots, (X_n, Y_n)$ be a sample from a bivariate distribution $\operatorname{Law}(X_1, Y_1)$ . It is known that if $\operatorname{Law}(X_1, Y_1)$ is bivariate normal and $\operatorname{Corr}(X_1, Y_1) = 0$ then the density function of the sample Pearson correlation coefficient $\hat{\rho}(X,Y)$ is $$
f(r) = \frac{(1-r^2)^{\frac{n-4}{2}}}{B(0.5, 0.5(n-2))}, \quad r \in [-1,1]
$$ where $$
\hat{\rho}(\mathbf{X},\mathbf{Y}) = \frac{\sum_{i=1}^n (X_i - \overline{X})(Y_i - \overline{Y})}{\sqrt{\sum_{i=1}^n (X_i - \overline{X})^2}\sqrt{\sum_{i=1}^n (Y_i - \overline{Y})^2}}.
$$ So in this case the distribution of the sample correlation is symmetric around zero. Will the symmetric property hold for an arbitrary bivariate distribution with square integrable marginals and zero correlation between components? I know how to prove that even in this arbitrary setting $$
\hat{\rho} \xrightarrow[n \to \infty]{d, \mathbb{P}} 0,
$$ so the asymptotic distribution of $\hat{\rho}$ is symmetric around zero. What I can't handle is how to prove the property for a fixed $n \in \mathbb{N}$ . Thank you in advance. UPD. More general question: consider the general correlation coefficient $$
\hat{r}_f(\mathbf{X},\mathbf{Y}) := \frac{\sum_{i,j}c_{i,j}(\mathbf{X})c_{i,j}(\mathbf{Y})}{\sqrt{\sum_{i,j}c^2_{i,j}(\mathbf{X})}\sqrt{\sum_{i,j}c^2_{i,j}(\mathbf{Y})}},
$$ where $c_{i,j}(\mathbf{X}) = f(X_i, X_j)$ for some antisymmetric function $f: \mathbb{R}^2 \to \mathbb{R}$ . It is easy to derive that for the Pearson correlation coefficient $f(t,s) = t-s$ . Is $\hat{r}_f$ symmetric around zero assuming $X_1, Y_1$ is independent?","['statistics', 'probability-distributions', 'probability']"
4799629,Exchangeable strictly vs. almost surely,"Let $X$ be a standard Borel measurable space (for example, $\Bbb{R}$ ).
Let $p$ be an exchangeable probability measure on the countably infinite product $X^\Bbb{N}$ .
Call a measurable set $A\subseteq X^\Bbb{N}$ Strictly exchangeable if for any finite permutation $\sigma:X^\Bbb{N}\to X^\Bbb{N}$ , $\sigma^{-1}(A)=A$ ; Almost surely exchangeable if for any finite permutation $\sigma:X^\Bbb{N}\to X^\Bbb{N}$ , $p(\sigma^{-1}(A)\,\Delta\,A)=0$ , where $\Delta$ denotes the symmetric difference. Is it true that if $A$ is almost surely exchangeable, then there exists a strictly exchangeable $B$ such that $p(A\,\Delta\,B)=0$ ? (I think I've found a proof, but it seems very specific to permutations, and I would like to see other proofs that might generalize to arbitrary actions.) A reference would also be welcome.","['permutations', 'measure-theory', 'reference-request', 'group-actions', 'probability-theory']"
4799653,Use of elemental length in volume of a truncated cone,"The typical way to compute the volume of a truncated cone is to slice into discs and calculate the volume of a differential cylinder. While doing that we first take the area of the disc $\pi f(x)^2$ and then multiply with thickness which we assume to be $dx$ and then get the final area is $\pi \int f(x)^2dx$ . But why should $dx$ be the thickness. As I have indicated in the picture,we are basically taking a tiny step along the surface which is $dl$ which again can be written as $\sqrt{(dy)^2+(dx)^2}$ . That changes the entire integration with a factor of $1+\sqrt{(\frac{dy}{dx})^2}$ and hence changes the volume? So which integral should be correct and why should $dx$ be the thickness and not $dL$ ? This issue is arising because if we were to calculate the surface area of this solid,we would have actually used $dL$ instead of $dx$ . So why are we taking two different thickness while accounting for the same differential cylinder? An image or animation will be greatly helpful in pointing out the difference,","['definite-integrals', 'derivatives', 'volume', 'infinitesimals']"
4799681,Proving $\sum_{n=0}^{\infty}\frac{1}{2^{n}}\sum_{r=0}^{n}\frac{1}{2^{4r}}\binom{2r}{r}^2=\frac{1}{\pi\sqrt{\pi}}\Gamma\left(\frac{1}{4}\right)^2$,"Infinite Sum : $$S=\sum_{n=0}^{\infty}\frac{1}{2^{n}}\sum_{r=0}^{n}\frac{1}{2^{4r}}\binom{2r}{r}^2$$ This Sum could be said to be related to Moments of Elliptic Integrals as we have : $$\int_{0}^{1}k^{2r}K'(k)dk=\frac{\pi^{2}}{4}\binom{2r}{r}^2\frac{1}{2^{4r}}$$ Where, $K(k)$ is the Complete Elliptical Integral of the First Kind and $K'(k)=K(k')$ with $k^2+k'^2=1$ . Using this one can arrive at the following Integral : $$S=\frac{16}{\pi^{2}}\int_{0}^{1}\frac{K'(k)}{2-k^{2}}dk$$ or the Equivalent using Quadratic Transformation: $$S=\frac{32}{\pi^{2}}\int_{0}^{1}K(k)\left(\frac{1+k}{k^{2}+6k+1}\right)dk$$ But I have been unable to evaluate any of these Integrals. I suspect it might have a closed form as does its similar cousin : $$\sum_{n=0}^{\infty}\frac{1}{\left(2n+1\right)^{2}}\sum_{r=0}^{n}\frac{1}{2^{4r}}\binom{2r}{r}^{2}=\frac{7}{2}\frac{\zeta(3)}{\pi}$$ EDIT:
After Inputting the Value into Wolfram we get this : $$S=\frac{1}{\pi\sqrt{\pi}}\Gamma\left(\frac{1}{4}\right)^2$$","['integration', 'summation', 'elliptic-integrals', 'definite-integrals']"
4799686,"Let $U, W$ be subspaces of finite dimensional Vector Space $V$ over $K$ with $\dim U + \dim W > \dim V$. Show that $U \cap W \neq$ {$0$}.","Let $U, W$ be subspaces of  finite dimensional Vector Space $V$ over $K$ with $\dim U + \dim W > \dim V$ . Show that $U \cap W \neq$ { $0$ }. The first thing I thought about is this formula $$\dim(U+W) = \dim U + \dim W - \dim (U \cap W)$$ We know $\dim(U+W) \leq \dim V$ because $U,W$ are subspaces of $V$ , and the dimension of a subspace is always less than or equal to the dimension of the space they belong to. Using those two formulas, we have $$\dim V \geq \dim(U+W) = \dim U + \dim W - \dim (U \cap W)$$ If we rearrange, we have $$\dim (U \cap W) \geq \dim U + \dim W - \dim (V)$$ We are given that $$\dim U + \dim W > \dim V$$ so we know $$\dim U + \dim W - \dim (V) > 0$$ so we have $$\dim (U \cap W) \geq \dim U + \dim W - \dim (V) > 0$$ which implies $U \cap W \neq$ { $0$ }. Is this correct or did I do something wrong or forgot something ?","['vectors', 'vector-fields', 'vector-spaces', 'matrices', 'linear-algebra']"
4799689,A limit in two variables with absolute value,"I have to calculate (if it exists) the following limit: $$\lim_{(x,y)\rightarrow(0,0)}\frac{\ln(1+|xy|)}{x^{2}+y^{2}}.$$ What I did is simply to consider that $\ln(1+\theta)\sim\theta$ as $\theta\rightarrow 0$ . So I have the limit: $$\lim_{(x,y)\rightarrow(0,0)}\frac{|xy|}{x^{2}+y^{2}}.$$ I know the inequality: $$|x|+|y|\ge \sqrt{x^{2}+y^{2}}$$ which I used in the previous exercises, but here there is a product of absolute values and not a sum, so I think it is not useful. How should I proceed?",['multivariable-calculus']
4799724,Does Strong Stochastic ordering exist?,"For two probability measure $\mu$ and $\nu$ on $\mathbb{R}$ , we call $\mu$ is stochastically smaller than $\nu$ (i.e., $\mu\leq\nu$ ) , if $\int f d\mu\leq\int f d\nu$ for any nonnegative bounded increasing measurable function $f:\mathbb{R}\to\mathbb{R}$ . There is a fact that $\mu\leq\nu$ is equivalent to $F_{\mu}(x)\geq F_{\nu}(x)$ for all $x\in\mathbb{R}$ , where $F_{\mu}(x)$ and $F_{\nu}(x)$ are the cumulative distribution functions of $\mu$ and $\nu$ respectively. Another fact is that, $\mu\leq\nu$ is equivalent to $\int f d\mu\leq\int f d\nu$ for any nonnegative bounded increasing measurable function $f:\mathbb{R}\to\mathbb{R}$ with Lipschitz constant less or equal to 1. I'm curious about whether there is a strong order in this ordered space. That is to say, can we find some topology in probability space such that, there exists two probability measure $\mu$ and $\nu$ , and their open neighborhood $\mu\in U$ , $\nu\in V$ in this topology such that, $\tilde{\mu}\leq \tilde{\nu}$ , for any $\tilde{\mu}\in U$ and $\tilde{\nu}\in V$ . I guess we cannot find such open sets in weak convergence topology, or wasserstein metric $W_p$ (I cannot prove it strictly now). And I cannot imagine such topology and open sets exist, since we need to gurantee the distribution sufficiently close to the inital probability measure in its neighborhood. The same question also exists for other partial order in the probability space. Any advices will be greatly appreciated.","['probability-distributions', 'probability-theory']"
4799757,"Intuition is silent: Find the probability that the smallest circle enclosing $n$ random points on a disk lies completely on the disk, as $n\to\infty$.","On a disk, choose $n$ uniformly random points. Then draw the smallest circle enclosing those points. ( Here are some algorithms for doing so.) The circle may or may not lie completely on the disk. For example, with $n=7$ , here are examples of both cases. What is $\lim\limits_{n\to\infty}\{\text{Probability that the circle lies completely on the disk}\}$ ? Is the limiting probability $0$ ? Or $1$ ? Or something in between? My geometrical intuition fails to tell me anything. The case $n=2$ I have only been able to find that, when $n=2$ , the probability that the smallest enclosing circle lies completely on the disk, is $2/3$ . Without loss of generality, assume that the perimeter of the disk is $x^2+y^2=1$ , and the two points are $(x,y)$ and $(0,\sqrt t)$ where $t$ is uniformly distributed in $[0,1]$ . The smallest enclosing circle has centre $C\left(\frac{x}{2}, \frac{y+\sqrt t}{2}\right)$ and radius $r=\frac12\sqrt{x^2+(y-\sqrt t)^2}$ . If the smallest enclosing circle lies completely on the disk, then $C$ lies within $1-r$ of the origin. That is, $$\sqrt{\left(\frac{x}{2}\right)^2+\left(\frac{y+\sqrt t}{2}\right)^2}\le 1-\frac12\sqrt{x^2+(y-\sqrt t)^2}$$ which is equivalent to $$\frac{x^2}{1-t}+y^2\le1$$ The area of this region is $\pi\sqrt{1-t}$ , and the area of the disk is $\pi$ , so the probability that the smallest enclosing circle lies completely on the disk is $\sqrt{1-t}$ . Integrating from $t=0$ to $t=1$ , the probability is $\int_0^1 \sqrt{1-t}dt=2/3$ . Edit From the comments, @Varun Vejalla has run trials that suggest that, for small values of $n$ , the probability (that the enclosing circle lies completely on the disk) is $\frac{n}{2n-1}$ , and that the limiting probability is $\frac12$ . There should be a way to prove these results. Edit2 I seek to generalize this question here .","['integration', 'circles', 'geometry', 'limits', 'probability']"
4799787,"Minimize $P=\dfrac{\sqrt{a+1}+\sqrt{b+1}+\sqrt{c+1}}{\sqrt{ab+bc+ca+6}},$ if $a+b+c+abc=4.$","Problem. Let $a,b,c\ge 0: a+b+c+abc=4.$ Find minimal value of $P$ $$P=\frac{\sqrt{a+1}+\sqrt{b+1}+\sqrt{c+1}}{\sqrt{ab+bc+ca+6}}.$$ Source: Vo Quoc Ba Can. My attempt: Set $$P(a,b,c)=\frac{\sqrt{a+1}+\sqrt{b+1}+\sqrt{c+1}}{\sqrt{ab+bc+ca+6}}.$$ After some calculating works, I've tried prove $P(1,1,1)$ is the minimal value. It means that we need to prove $$\sqrt{a+1}+\sqrt{b+1}+\sqrt{c+1}\ge\sqrt{2}\cdot\sqrt{ab+bc+ca+6}.$$ By squaring both side, it remains to prove $$a+b+c+3+2\sum_{cyc}\sqrt{(a+1)(b+1)}\ge 2(ab+bc+ca+6),$$ or $$2\sum_{cyc}\sqrt{(a+1)(b+1)}\ge 5+abc+2(ab+bc+ca).$$ I was stucked here. Hope to see some ideas to continue the idea. Also, all answer and comment are welcome. Thank for your attention. Updated edit: Thank you @138 Aspen. I made a mistake in calculating. Minimum should be $$P(0,2,2)=\frac{2\sqrt{30}+\sqrt{10}}{10}.$$ It means that we need to prove $$\sqrt{a+1}+\sqrt{b+1}+\sqrt{c+1}\ge\frac{2\sqrt{30}+\sqrt{10}}{10}\cdot\sqrt{ab+bc+ca+6},$$ which seems ugly. Hope MV (mixing variables) technique works.","['maxima-minima', 'multivariable-calculus', 'inequality']"
4799806,Pdf of Y knowing pdf of X,"Let $Y=1-X^2, f_X(x)=\frac{3}{8} \cdot(x+1)^2 \cdot \mathbf 1_{(-1,1)}(x).$ I think the pdf of $Y$ should be the derivative of $F_Y(y)= F_X(-\sqrt{1-y} )-F_X(\sqrt{1-y})$ , but it can't be negative. Where is my mistake?","['statistics', 'probability']"
4799811,"How do I calculate the intersection points between inverse shared tangent lines, a circle, and a bounding box?","Context I am attempting to draw a polygon with 4 or 5 corners using a software library that draws shapes by taking in an array of corners in the form of $(X,Y)$ . I am somewhat woefully under skilled in my understanding of geometry here, and looking for either the formulas to solve my problem or some good directions to tutorials, videos, and other learning tools, or both. Givens I have two circles defined by their centers and their radius $A_c, A_r, B_c, B_r$ and the bounding box extends from $(0,0)$ to some corner, $(BB_X,BB_Y)$ . This produces either eight or ten values I want to calculate. The shape I want is represented by the pink shaded area, and the points I need to produce it by the green stars. The Question What formulas do I use to calculate these 4 or 5 points? or What are some concepts/tools/videos I can use to learn how to construct the formulas to calculate these 4 or 5 points?","['tangent-line', 'circles', 'geometry']"
4799817,"The number of sequences of length $n$ with digits in $\{0, 1, 2, 3\}$ without consecutive repeated nonzero digits","Let $a_n$ be the number of sequences of length $n$ with digits in $\{0, 1, 2, 3\}$ that do not contain consecutive repeated nonzero digits. The last digit of all such sequences can be 0, 1, 2, or 3. I want o find a Recurrence relation describing $a_n$ . My attempt: If the last digit is $0$ , then the remaining $n-1$ digits can be any sequence satisfying the original condition and therefore can be chosen in $a_{n-1}$ ways. However, if the last digit is $1$ , $2$ , or $3$ , then the $n-1$ remaining digits must have the additional condition of not ending with the last digit. Let $b^a_n$ denote the number of strings that, in addition to the original condition, end with an $a$ with $a \in {1, 2, 3}$ . It's clear that $b^1_n = b^2_n = b^3_n$ , so we might just show it as $b_n$ . In total, we have $a_n = a_{n-1} + 3b_{n-1}$ .
Then we have to express $b_n$ in terms of $a_n$ , but I don't know how to proceed.","['combinatorics', 'recurrence-relations', 'discrete-mathematics']"
4799858,Solution-Verificationï¼šEvaluate $\lim\limits_{n \to \infty}\left(\sum_{k=1}^n\frac{1}{\sqrt{n^2+n+k}}\right)^n$.,"Here is a proof posted in a Chinese website. Since $f_n(x):=\dfrac{1}{\sqrt{n^2+n+x}}$ is convex on $x\in(0,+\infty),$ by discrete Hadamard inequality , it holds that $$f_n\left(\frac{n+1}{2}\right)\le\frac{1}{n}\sum_{k=1}^nf_n(k)\le \frac{f_n(n)+f_n(1)}{2}.\\$$ Therefore $$\frac{n}{\sqrt{n^2+n+\frac{n+1}{2}}}\le\sum_{k=1}^nf_n(k)\le \frac{n}{\sqrt{n^2+n+\frac{n}{2}}},$$ which implies $$\left(\frac{n}{\sqrt{n^2+n+\frac{n+1}{2}}}\right)^n\le\left(\sum_{k=1}^nf_n(k)\right)^n\le \left(\frac{n}{\sqrt{n^2+n+\frac{n}{2}}}\right)^n.$$ The limits of both sides are $e^{-3/4}$ as $n \to \infty$ . By the squeezing theorem, $$\lim_{n \to \infty}\left(\sum_{k=1}^nf_n(k)\right)^n=e^{-3/4}.$$ This solution is likely to be elegant, but I wonder whether it is correct or not. What is discrete Hadamard inequality?","['limits', 'calculus', 'solution-verification']"
4799872,"Limit existing: $\lim_{(x,y)\to(0,0)} \frac{xy^2}{(x^2+y^2)^{3/2}}$","Does this limit exist? $$\lim_{(x,y)\to(0,0)} \frac{xy^2}{(x^2+y^2)^{3/2}}$$ I tried some sequnces $(\frac{1}{n},\frac{1}{n})$ , and similar, and limit is $0$ . But I can't prove for all. Also, can't prove that there is no limit.","['multivariable-calculus', 'limits', 'calculus']"
4799873,Why are the $n$th-order differences of the sequences $a^n$ always equal to $n!$?,"Take a look at this example with $a^2$ (where $a \in \mathbb{N}$ ). 1   4   9  16  25  36  49  64  81 100
  \/  \/  \/  \/  \/  \/  \/  \/  \/
   3   5   7   9  11  13  15  17  19
    \/  \/  \/  \/  \/  \/  \/  \/
     2   2   2   2   2   2   2   2      --- 2nd Order Difference becomes constant at 2! ...and this example with $a^3$ . 1     8     27    64    125   216   343   512   729   1000
  \/    \/    \/    \/    \/    \/    \/    \/    \/
   7    19    37    61    91    127   169   217   271
    \/    \/    \/    \/    \/    \/    \/    \/
    12    18    24    30    36    42    48    54
      \/    \/    \/    \/    \/    \/    \/
       6     6     6     6     6     6     6      --- 3rd Order Difference becomes constant at 3! If this pattern is continued, we find that that the $n$ th-order differences of the sequences $a^n$ become constant and are equal to $n!$ . Why is this? How does it make intuitive sense, and how to prove it? Note: To see similar patterns for higher indices, you can use the following python code:- power = 4  # Change this value to get results for different powers
seq_len = 10  # Change this value to modify the length of the first row

numbers = [
    i ** power
    for i in range(0, seq_len + 1)
]

for i in range(power):
    numbers = [numbers[i] - numbers[i - 1] for i in range(1, len(numbers))]
    print(""\t"" * i + ""\t\t"".join([str(num) for num in numbers]))","['arithmetic-progressions', 'discrete-mathematics', 'sequences-and-series']"
4799887,Differentiation. Why does differentiating a function allows us to find a tangent of a point of that specific function?,I know that the purpose of differentiation is to find the rate of change of a function. But i don't really understand how this is concept can be used to find the tangent of a specific point. It would be greatly appreciated if you can explain this to me.,"['calculus', 'derivatives']"
4799922,How to find the width and height of a constrained rectangle? (Precalculus),"Of all rectangles with base on the $x$ -axis and upper corners on the parabola $y=5âˆ’x^2$ , what are the dimensions (width and height) of the rectangle that has the greatest possible area? I have no idea how to solve this. I tried graphing $y=0$ and the parabola, but then I don't know where to put the vertical lines. How do I approach this question?","['algebra-precalculus', 'quadratics']"
4799950,"Counting (0,1)-matrices with a given characteristic polynomial","Let $f_n\colon \{0,1\}^{n-1} \to \mathbb{N}$ be a function where $f_n(c_{n-1}, c_{n-2}, \dots, c_1)$ is the number of $n\times n$ matrices with coefficients in $\{0,1\}$ whose characteristic polynomial is $$x^n - c_{n-1} x^{n-1} - c_{n-2} x^{n-2} - \dots - c_2 x^2 - c_1 x - 1.$$ For all of the values of that I have checked ( $n \leq 4$ ), it appears that $f_n(\vec{c}) = 2^a3^b$ . Does it hold in general that $$f_n(c_{n-1}, c_{n-2},\dots,c_2,c_1) = 2^{k_1} 3^{k_2} 5^{k_3} \cdots p_n^{k_n},$$ $k_j$ are integers and where $p_n$ is the greatest prime less than $n$ ? Is there a way of computing $f_n$ efficiently? Supporting Data Case $n = 1$ : $$
f_1() = 2^0 \cdot 3^0 = 1.
$$ Case $n = 2$ : $$
f_2(0) = 2^0 \cdot 3^0 = 1.\\
f_2(1) = 2^1 \cdot 3^0 = 2.
$$ Case $n = 3$ : $$
f_3(0,0) = 2^1 \cdot 3^0 = 2\\
f_3(0,1) = 2^1 \cdot 3^1 = 6\\
f_3(1,0) = 2^1 \cdot 3^1 = 6\\
f_3(1,1) = 2^2 \cdot 3^1 = 12.
$$ Case $n = 4$ : $$
f_4(0,0,0) = 2^1 \cdot 3^1 = 6\\
f_4(0,0,1) = 2^3 \cdot 3^1 = 24\\
f_4(0,1,0) = 2^3 \cdot 3^1 = 24\\
f_4(0,1,1) = 2^5 \cdot 3^1 = 96\\
f_4(1,0,0) = 2^3 \cdot 3^1 = 24\\
f_4(1,0,1) = 2^3 \cdot 3^2 = 72\\
f_4(1,1,0) = 2^6 \cdot 3^1 = 192\\
f_4(1,1,1) = 2^6 \cdot 3^1 = 192.
$$ Note that this does not seem to hold when our characteristic polynomial does not end with $-1$ . For example, there are $2004 = 2^2\cdot 3 \cdot 167$ matrices of size $4 \times 4$ with coefficients in $\{0,1\}$ and characteristic polynomial $x^4-x^3-x^2$ .","['linear-algebra', 'combinatorics', 'characteristic-polynomial']"
4799963,"Confusion about ""$:=$"" notation in pointwise convergence definition","My instructor's lecture notes state: Let $\{f_n:E\rightarrow\mathbb{R}\}_{n\in\mathbb{N}}$ be a sequence of functions defined on nonempty $E\subset\mathbb{R}$ . We say that the sequence converges pointwisely to $f$ on $E$ if $\forall x\in E,\forall \epsilon>0,\exists N\in \mathbb{N}:\forall n\geq N, |f_n(x)-f(x)|\leq \epsilon $ . In this case, $f(x):=\lim_{n\rightarrow \infty}f_n(x)$ . Why was $f$ defined in the last sentence? Doesn't the definition of pointwise convergence in the previous sentences imply that $f(x)=\lim_{n\rightarrow \infty}f_n(x)$ for all $x\in E$ when pointwise convergence holds? I assumed that the notation "" $:=$ "" is used to define $f$ .","['real-analysis', 'definition', 'limits', 'soft-question', 'pointwise-convergence']"
4799983,Drinfeld's Invariant Locus of Formal Scheme,"I am reading Drinfeld's Elliptic Modules paper. In the proof of Proposition 5.4, which is on the deformation of Drinfeld modules, he defines an action of a certain group $A_x^{\ast}$ on the functors of deformations of formal modules and Drinfeld modules. In Drinfeld's notations, we let $F_0$ denote the ring representing the deformation functor of a Drinfeld $A$ -module $X$ over $\overline{\mathbf{F}}_p$ , and let $F_0^{'}$ be the ring representing the deformation functor of the same object $X$ but viewed as a Drinfeld $\mathbf{F}_p[x]$ -module, where $A=\Gamma(X-\{\infty\},\mathcal{O}_X)$ is the ring of integers in a global function field. Then with an appropriate action of $A_x^{\ast}$ defined, Drinfeld says $$(\operatorname{Spf}F_0^{'})^{A_x^{\ast}}=\operatorname{Spf}F_0$$ But what is exactly this $(\operatorname{Spf}F_0^{'})^{A_x^{\ast}}$ ? My understanding is that it should be the ""invariant locus"" of the formal scheme $\operatorname{Spf}F_0^{'}$ . Is this correct? Also, on the level of rings, how does one get $F_0$ from $F_0^{'}$ and the action of $A_x^{\ast}$ ?","['number-theory', 'algebraic-geometry']"
4799990,Show that $y=\frac{1}{x}$ is a hyperbola,"I am trying to write $y=\frac{1}{x}$ in the standard form for a hyperbola, $\frac{x^2}{a^2}-\frac{y^2}{b^2}=1$ ( $a>0$ , $b>0$ ) but I can't figure out the last step. My book says to First, rotate the coordinate system through an angle $\theta$ . This
means that the coordinates $x$ and $y$ should be replaced by,
respectively, $x\cos\theta+y\sin\theta$ and $-x\sin\theta+y\cos\theta$ . Choosing an appropriate $\theta$ , we can
make the coefficient of $xy$ equal to zero. With this modification was able to rewrite the equation as $$1=xy=(x\cos\theta+y\sin\theta)(-x\sin\theta+y\cos\theta)=-x^2(\sin\theta\cos\theta)+xy(\cos^2\theta-\sin^2\theta)+y^2(\sin\theta\cos\theta).$$ To get rid of the $xy$ term I set $\theta=\frac{\pi}{4}$ which simplified the equation to $$1=-\frac{x^2}{4}+\frac{y^2}{4}.$$ But the signs on my $x^2$ and $y^2$ terms are not what they should be with real $a,b$ . I assume this can be remedied in the next step, but I'm not sure how. Next we move the origin to $(x_0,y_0)$ , i.e., we replace $x$ by $x+x_0$ and $y$ by $y+y_0$ . By choosing an appropriate pair $(x_0,y_0)$ we can transform the equation into the canonical form.","['polar-coordinates', 'conic-sections', 'geometry']"
4800042,Maximal Inequality for Expectation of Random Variables,"Let $Z_1, Z_2, ..., Z_n$ be non-negative random variables, not necessarily independent. Then, show the expectation of the product of the random variables is less than or equal to the integral of the product of the quantile functions. In other words, show that $$\mathbb{E}(Z_1Z_2...Z_n) \leq \int_{0}^{1} Q_{Z_1}(u)Q_{Z_2}(u)...Q_{Z_n}(u)~du,$$ where $Q_{Z_{i}}$ is the quantile function of $Z_i$ . The only way I currently see to establish the inequality for the expectation is by using Holding's inequality. But I'm not sure how to proceed with using the quantile functions. I'm guessing I need to use Fubini-Tonelli at some point.","['fubini-tonelli-theorems', 'probability-theory', 'probability']"
4800064,"For triangle ABC, DC = DE = DH, Find the value of angle X.","In the triangle ABC, DC = DE = DH, Find the value of angle X. I found the right triangles AHC and BHC, I also found the angle ACH = 40Â°. I found the isosceles triangles HDE, DEC, and DHC, which means the angle DHE = HED, the angle DEC = DCE, and the angle DHC = DCH. Thanks in advance.","['triangles', 'trigonometry', 'angle', 'geometry']"
4800104,"A variant of Slutsky theorem for matrices: $Z_n\to^d N(0,I_d)$ and $T_nT_n^\top\to^P C$. When is it true that $T_nZ_n\to^d N(0,C)$?","Consider a sequence $Z_n$ of random vectors in $R^d$ such that $Z_n\to N(0_d,I_d)$ in distribution. Consider a sequence of random matrices $T_n\in R^{d\times d}$ , not independent of $Z_n$ , such that $T_nT_n^\top\to C$ for a deterministic positive semi-definite $C\in R^{d\times d}$ . I am interested in whether $T_n Z_n$ converges in distribution to $N(0_d, C)$ . Remark 1: This is Slutsky's theorem if $d=1$ . Remark 2 : If $T_n\to^d A$ and $C=AA^T$ , then we may apply the continuous mapping theorem to the function $f(T,z)=Tz$ . But here we only assume convergence of $T_nT_n^\top$ to $C$ . $T_n Z_n$ converges in distribution to $N(0_d, C)$ not true. A counterexample is as follows: $T_n$ is a rotation in $O(d)$ for all $n$ such that the first row is $Z_n/\|Z_n\|$ , so that $T_nT_n^\top = I_d$ but $T_n Z_n$ has zero in coordinates $2,3,...,d$ . So let us add another assumption to avoid this rotation situation: Question : Is it always true that $T_n Z_n$ converges in distribution to $N(0_d, C)$ provided that $T_n$ is lower triangular with $1$ on the diagonal?","['probability-limit-theorems', 'probability-distributions', 'probability-theory', 'probability']"
4800107,How should I evaluate this indefinite integral involving natural logarithm?,"Find $\int \ln(5x^3)dx$ . Based on my Ti-89 graphing calculator, the answer I found is $x\cdot \ln(x^3)+(\ln(5)-3)\cdot x$ . But I want to know how to get to this answer. I know that by the general formula/rule, $\int \ln(x)dx=\frac{1}{x}+c$ but how to apply that to this problem?","['integration', 'indefinite-integrals', 'calculus']"
4800135,Showing that every plane curve with constant curvature is a circle by solving the Frenet differential equation,"I recently saw the following example that shows that plane curves with constant curvature are circles: Let $\kappa \neq 0$ be constant and $s \in \mathbb{R}$ arbitrary, then by the Frenet Matrix-equation we get $E(s)=exp \biggl(s \begin{pmatrix}
0 & -\kappa \\ 
-\kappa & 0 
\end{pmatrix}\biggr)=\begin{pmatrix}
cos(\kappa s) & sin(\kappa s) \\ 
-sin(\kappa s) & cos(\kappa s) 
\end{pmatrix}
$ Now my question is regarding how to get to that result. I know that the Frenet Matrix Equation (for plane curves) is $\begin{pmatrix}
e_{11}'(s) & e_{12}'(s) \\ 
e_{21}'(s) & e_{22}'(s) 
\end{pmatrix}=\begin{pmatrix}
0 & -\kappa \\ 
-\kappa & 0 
\end{pmatrix} \cdot \begin{pmatrix}
e_{11}(s) & e_{12}(s) \\ 
e_{21}(s) & e_{22}(s)
\end{pmatrix}$ Also by googling I now know that the matrix exponential for some $n \times n$ matrix $A$ as $e^{A}=\sum_{k=0}^{\infty}\frac{A^k}{k!}$ . I do know that for some function $f$ , the equation $f'(x)=c f(x)$ has the solution, $C_1 e^{cx}$ , but I don't understand the ""matrix version"". I am not very familiar with differential equations.","['ordinary-differential-equations', 'differential-geometry']"
4800319,Conditional expectation relation,"I am playing a bit with properties of conditional expectation and I am wondering about the following problem. Let $(\Omega,\Sigma, \mathbb P)$ be a probability space, let $X$ be a real random variable taking support in $[0, 1]$ . Let also $\mathcal F$ and $\mathcal G$ be two sub $\sigma$ -algebras of $\Sigma$ . Let also $\mathcal H$ be the smallest sub $\sigma$ -algebra of $\Sigma$ that contains both $\mathcal F$ and $\mathcal G$ , I want to know if the following is true. $\mathbb E[\mathbb E[X|\mathcal F]|\mathcal G]=\mathbb E[X|\mathcal G]$ a.s. $\Leftrightarrow$ $\mathbb E[X|\mathcal H]=\mathbb E[X|\mathcal F]$ a.s. Now it is easy to prove $\Leftarrow$ , since $\mathbb E[X|\mathcal G]=\mathbb E[\mathbb E[X|H]|G]=\mathbb E[\mathbb E[X|\mathcal F]|\mathcal G]$ . I cannot prove the other direction, maybe someone has an idea.","['conditional-probability', 'measure-theory', 'probability-theory', 'conditional-expectation']"
4800332,"Solve, $(\cos x-\sin x)(2\tan x+\sec x)+2=0$","I solved this problem by converting $\sin x,\cos x,\sec x$ all in terms of $\tan(x/2)$ which gives a biquadratic. I found $2$ roots of the biquadratic by hit and trial i.e $\tan^2(x/2) = 1/3$ but the calculations were really lengthy. I had also tried opening all brackets but I was still unsuccessful in finding some nice way to solve the above equation. It would be really appreciated if someone gives a nice solution to the above problem. Thank you.",['trigonometry']
4800378,Prove that $f(\mathbf{x}) = \mathbf{c}^T\mathbf{x} - \sum_{i=1}^{m} \log(b_i - \mathbf{a}_i^T\mathbf{x})$ is convex,"Let's consider $f \colon \mathbb{R}^n \to \mathbb{R}$ , $$f(\mathbf{x}) = \mathbf{c}^T\mathbf{x} - \sum_{i=1}^{m} \log(b_i - \mathbf{a}_i^T\mathbf{x}), $$ with $\mathbf{x} \in \mathbb{R}^n$ , $\mathbf{c} \in \mathbb{R}^n$ , $\mathbf{b} \in \mathbb{R}^m$ , $\mathbf{a}_i \in \mathbb{R}^n$ , for $i = 1,\ldots,m$ , so $\mathbf{A} \in \mathbb{R}^{n\times m}$ . $\mathbf{dom}f$ contains the $\mathbf{x}$ for which the arguments of the logarithms are positive. (Note: I have also seen $f$ written as, $f(\mathbf{x}) = \mathbf{c}^T\mathbf{x} - \operatorname{sum}(\log(\mathbf{b} - \mathbf{A}\mathbf{x}) )$ .) I have to prove that $f$ is convex (most likely using the second derivative theorem). I am not sure how to proceed. I have read that: $$ \nabla \biggl( \sum_{i=1}^{m} \log(b_i - \mathbf{a}_i^T\mathbf{x}) \biggr) = \sum_{i=1}^{m} \frac{1}{b_i - \mathbf{a}_i^T\mathbf{x}}\mathbf{a}_i $$ But I don't fully understand it.","['multivariable-calculus', 'calculus', 'convex-analysis', 'derivatives']"
4800400,Are sets of incommensurable real numbers all denumerable?,"Definition of incommensurability : Supposing $a,b \in \mathbb{R}^*$ , $a,b$ are called incommensurable if and only if $\frac{a}{b}\notin\mathbb{Q}$ . From the lectures of KAM theory concerning the small divisor problem, I wonder: Is it possible to construct a non denumerable subset $S$ of $\mathbb{R}^\ast$ such that $\forall \left(a,b\right) \in S^2, a,b $ incommensurable? Same question without admitting the Axiom of Choice . The problem is inspired by the fact that every integrable Hamiltonian system of $N$ degrees of freedom in classical mechanics, shall have a $N$ -periodic motion. In the case which two fundamental frequencies of such system are commensurable (called the degeneracy ), there will exist additional physical conserved quantities. So it would be interesting to see if one's able or not to construct a non denumerable set of incommensurable real frequencies, for which is the necessary condition of the existence of a classical integrable system with non denumerable degrees of freedom. (Normally the classical smooth fields in finite dimensions are systems with only denumerable degrees of freedom)","['elementary-set-theory', 'discrete-mathematics']"
4800407,Is there a clever way to show that $\sin (\phi+1)<\frac12$?,"I noticed that $\phi+1\approx 1.000015\left(\frac{5\pi}{6}\right)$ , where $\phi=\frac{1+\sqrt5}{2}$ , the golden ratio . So I wonder, is there a clever way to show that $\sin (\phi+1)<\frac12$ , without a calculator? Using Maclaurin series, we have $\sin (\phi+1)<\sum\limits_{k=1}^7 (-1)^{k-1}\frac{(\phi+1)^{2k-1}}{(2k-1)!}$ , which, in principle, could be shown to be less than $\frac12$ without a calculator. But is there a better way?","['golden-ratio', 'trigonometry', 'geometry']"
4800412,Please help me with this calculus question [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 8 months ago . Improve this question The coefficients of the quadratic function $f(x)$ including the constant term, are all
rational and $f(x)$ has a local maximum at $x = 0$ . Let $g(x) =|f'(x)|e^{f(x)}$ have a maximum value of $4\sqrt{e}$ . If $g(x) = 4\sqrt{e}$ has rational solutions then: a) $\int_{-1}^0g(x)dx=e-\frac{1}{e^{7}}$ b) The value of the sgn $(f(0))=-1$ (where sgn is the sign function) c) $g(x)$ is not differentiable at one value of $x$ d) The value of $g\left(\tan{\frac{\pi}{4}}\right)=\frac{2}{e^7}$ (Since, I don't yet know how to write integrals on this website) The entire question is as follows","['integration', 'calculus', 'derivatives']"
4800433,Group laws of the flow of time-dependent vector field,"I am reading the fundamental theorem on time-dependent flows from the book Introduction to Smooth Manifolds, written by John Lee ( see page 237 ). The last line of the theorem (i.e., equation $9.18$ ) says that if $p\in M_{t_1,t_0}$ and $\psi_{t_1,t_0}(p)\in M_{t_2,t_1}$ , then $p\in M_{t_2,t_0}$ and $$\psi_{t_2,t_1}\circ \psi_{t_1,t_0}(p)=\psi_{t_2,t_0}(p).$$ Notice that the left-hand side of this equation depends on $t_1$ , but the right-hand side doesn't. Should it not be $$\psi_{t_2,t_1}\circ \psi_{t_0,t_1}(p)=\psi_{t_2+t_0,t_1}(p)?$$ The question arises while checking the group laws (i.e., equation $9.3$ on page 209 ) of the flow of each vector field $V_t\colon M\to TM$ defined by $V_t(p)=V(t,p)$ .","['ordinary-differential-equations', 'vector-fields', 'smooth-manifolds', 'group-actions', 'dynamical-systems']"
4800434,How do I show that a function of an intersection of sets is not always equal to the intersection of a function of sets?,"I'm currently taking this college calculus course, and this exercise has stumped me. It is in German, but hopefully what it's asking is fairly clear. To summarize, the problem first asks me to prove that the preimage of an intersection of a family of sets is equal to the intersection of a preimage of a family of sets, where f: A -> B and Ui is a family of subsets of B. To do this, I just gave the general example of x being an arbitrary element of A, which means that x is an element of the preimage of the intersection, which means for all values of i f(x) is an element of Ui, which means x is an element of the intersection of the preimage of Ui. Image of my answer cause I can't figure out MathJax: Then the problem asks to show that for a family Vi, a subset of A, an image of the intersection of a family of sets is not in general equal to the intersection of an image of a family of sets. Yet, for every example formula and sets I can conjure up, I always seem to prove that the two sides are in fact equal. No amount of coaxing and complaining on Bing or ChatGPT causes them to bring an example up either - they just keep giving examples where the two sides are equal. So, is this a typo from my professor and this expression actually is generally correct, or am I missing something? What family of sets and functions would make these not equal? Thank you!","['elementary-set-theory', 'functions']"
4800466,An Upper Bound for Entropy $H(X)$ in Terms of $\mathbb{E}(\log X)$,"Suppose that $X:\Omega\to\mathbb{N}$ is a discrete random variable with PMF $p:\mathbb{R}\to\mathbb{R}$ , where $\mathrm{supp} p = \mathrm{range} X = \mathbb{N}$ . I want to prove that If $\mathbb{E}(\log_2 X) < \infty$ then $H(X) < \infty$ . The first thing that comes to mind is to find an upper bound for $H(X)$ in terms of $\mathbb{E}(\log_2 X)$ . Expanding the expression for each of these we have $$
H(X) = \sum_{i=1}^{\infty}p(i)\log\frac{1}{p(i)}, \qquad
\mathbb{E}(\log_2 X) = \sum_{i=1}^{\infty}p(i)\log_2 i.
$$ I have no idea for going on.","['analysis', 'entropy', 'probability', 'upper-lower-bounds']"
4800538,Separating an integral,"Letâ€™s suppose that $x, y$ are independent variables. If we have an integral such as $$\int^{a}_{b} f(x-y) + g(x+y)dx$$ Can we split the integral into something of this form? $$I(a,b) = \int_{a+y}^{b+y} f(t_{-}) \,\mathrm{d}t_{-} + \int_{a-y}^{b-y} g(t_{+}) \,\mathrm{d}t_{+}$$ Where $t_{\pm} = x\pm y$ .","['integration', 'measure-theory']"
4800545,UMP Test for exponential random variable,"Let $X_i\sim \text{Exp}(\theta)$ for $i=1,\dots,n$ i.i.d with density \begin{equation*}
    f(x,\theta) =
    \begin{cases}
        \theta \exp(-x\theta ),& x\geq 0\\
        0,& x< 0.
    \end{cases}
\end{equation*} I want to test the hypothesis \begin{equation*}
    H_0: \theta = \theta_0 \quad \text{vs} \quad H_1: \theta = \theta_1 \quad \text{with}\quad \theta_1 > \theta_0
\end{equation*} The Neyman-Pearson lemma provides a UMP test at level $\alpha$ of the form \begin{equation}
      \varphi^*(X) = 
        \begin{cases}
          1, & \hspace{0.25cm} f_1(X) > k f_0(X),\\
          0, & \hspace{0.25cm} f_1(X) < k f_0(X).
        \end{cases}
    \end{equation} For some critical value $k$ . The densities are absolutely continious, therefore we can look at the likelihood ratio. Since they are i.i.d this is just the product of the respective densities and we get \begin{equation*}
\frac{f(X,\theta_1)}{f(X,\theta_0)}=\frac{\theta_1^{n}\exp(-\theta_1\sum_{i=1}^n x_i )}{\theta_0\exp(-\theta_0\sum_{i=1}^n x_i )} = \frac{\theta_1^n}{\theta_0^n}\exp\left(-(\theta_1-\theta_0)\sum_{i=1}^n x_i\right)>k.
\end{equation*} My problem arises, when I rearrange this term to get an equivalent condition. \begin{align*}
\frac{\theta_1^n}{\theta_0^n}\exp\left(-(\theta_1-\theta_0)\sum_{i=1}^n x_i\right)&>k \\
n\log(\theta_1/\theta_0)-(\theta_1-\theta_0)\sum_{i=1}^n x_i&>k'\\
-(\theta_1-\theta_0)\sum_{i=1}^n x_i&>k'-n\log(\theta_1/\theta_0)\\
-\frac{(\theta_1-\theta_0)}{n}\sum_{i=1}^n x_i&>k'/n-\log(\theta_1/\theta_0)\\
\frac{1}{n}\sum_{i=1}^n x_i&<\frac{k'/n-\log(\theta_1/\theta_0)}{(\theta_1-\theta_0)}=:k''.\\
\end{align*} The inequality switches in the last step since we assume $\theta_1>\theta_0$ . However this seems odd. Using some probability theory we calculate $k''$ for $\alpha=0.05$ . Since the sum of i.i.d exponential r.v. is $Gamma(n,\theta)$ and $Gamma(n,1/2) = \chi^2(2n)$ this results in the condition \begin{equation*}\alpha  = \mathcal{P}_{\theta_0}(\overline{X}_n <k'') = \mathcal{P}_{\theta_0}(2\theta_0n\overline{X}_n <2\theta_0nk'').
\end{equation*} Thus we get $2\theta_0nk''= \chi^2_{1-\alpha}(2n)$ , where the subscript denotes the $1-\alpha$ quantile. Therefore, $$ k''=\frac{\chi^2_{1-\alpha}(2n)}{2\theta_0n}.$$ The reason why the condition $\overline{X}_n <k''$ seems odd is the following simulation for a sample of 15 exponential r.v. with parameter $\theta = 1$ , where i test $\theta_0 = 1$ vs $\theta_1 = 2$ . The likehood ratio process goes very nicely to $0$ , as one would expect with how i chose the values. However the condition for the sample mean would reject the null in favor of the paramter $\theta_1$ , since the sample mean is lower than the critical value $k''$ (in the image discription it says k'). This cannot be right, therefore I assume an error when calculating $k''.$ and","['statistics', 'probability-theory', 'hypothesis-testing']"
4800549,Possible number of open sets in a topology,Question: Is there a topological space with exactly 100 distinct open sets? My attempt : I took topological space which is countable. Maximum number of open sets in a topological space is $2^n$ which is the set of all possible subsets. Number of elements in this topological space can't be lesser than 7 since for 6 elements the maximum possible subsets is $2^6< 100$ . First I took 5 elements as singletons and a single  2 element set Y. ( $\phi$ is already included in thees 32 subset)These 5 elements form 32 subsets.Hence along with Y and whole set  we have 34 sets. On taking union of all possible sets with Y I got 65 sets. I tried different possibilities like this. But this method leads to nowhere. I couldn't find a clear cut logic though I put efforts on this problem for many days. I searched many websites as well but I didn't get any idea. Seems to be like combinatorics problem but I don't know this subject in detail. Is there some key concept behin this problem that I am missing. Please help me in proving or disproving this problem.,['general-topology']
4800567,"Continuous $f$, $\sup_{x,y\in R^m}\|f(x+y)-f(x)-f(y)\|<\infty$, then exist only one real matrix $A$, such that $f(x)-Ax$ is bounded.","Problem: f is a continuous function, $f: \mathbb{R}^m\to \mathbb{R}^n$ , $\sup_{x,y\in R^m}\|f(x+y)-f(x)-f(y)\|<\infty$ , there exist only one real matrix $A$ , such that $f(x)-Ax$ is bounded. My thoughts 1: $\sup_{x,y\in R^m}\|f(x+y)-f(x)-f(y)\|:=M<\infty$ , then $\|f(2x)-2f(x)\|<M$ , $\|f(3x)-f(2x)-f(x)\|<M$ , so $\|f(3x)-3f(x)\|\leq\|f(3x)-f(2x)-f(x)+f(2x)-2f(x)\|\leq2M$ , so by reduction, we have $\|f(nx)-nf(x)\|<(n-1)M$ , then $\|\frac{f(nx)}{n}-f(x)\|<(1-\frac{1}{n})M$ I want to fix $x$ and let $n \to \infty$ , but is hard to show $\lim_{n \to \infty}\frac{f(nx)}{n}$ exists. My thoughts 2: If we assert $\lim_{n \to \infty}\frac{f(nx)}{n}$ exists, define $G(x)=\lim_{n \to \infty}\frac{f(nx)}{n}$ , I want to show $G(x)$ is linear. If so, $G(x)=Ax$ , then the $A$ is what we want. My thoughts:
The limit may not exist. Three steps:
1. $R^1$ case 2.each element in $R^n$ is actually the $R^1$ case 3.different elements are ""independent""",['analysis']
4800575,"Yet another variant of Slutsky's theorem for matrices: If $Z_n\to N(0_d, S)$ and $T_nST_n^\top \to^P C$, is it true that $T_nZ_n\to^d N(0_d,C)$?","Consider a sequence of random vectors $Z_n$ in $R^d$ converging
in distribution to $N(0_d, S)$ for some psd matrix $S$ . Consider a sequence of square matrices $T_n$ lower triangular with diagonal elements all equal to 1 such that $T_nS T_n^\top \to^P C$ (convergence in probability to some psd matrix $C$ in $R^{d\times d}$ ). The sequence $T_n$ is further assumed bounded, $P(\|T_n\|_{op}\le b)=1$ for some non-random $b>0$ . A previous answer to a related question, https://math.stackexchange.com/a/4800263/484640 , shows that $T_nZ_n\to^d N(0,C)$ in the special case $S=I_d$ , thanks to the continuity of the Choesky factor. Questions : is it always true that $T_n Z_n \to^d N(0, C)$ , when $S$ is not invertible? Edit: The case if $S$ invertible (positive definite) can be treated as follows: let $LL^T=S$ be the unique Cholesky decomposition of $S$ .
Then $\tilde T_n=T_nL$ and $\tilde Z_n=L^{-1} Z_n$ satisfy $\tilde Z_n\to^d N(0,I_d)$ and $\tilde T_n\tilde T_n^\top \to^P C$ . Then https://math.stackexchange.com/a/4800263/484640 applied to $\tilde Z_n,\tilde T_n$ completes the proof that $T_n Z_n = \tilde T_n\tilde Z_n\to^d N(0, C)$ .","['probability-limit-theorems', 'probability-theory', 'probability']"
4800586,Why is subset notation often used inconsistently with inequality of numbers notation?,"$A$ , $B$ are sets, $x, y$ are real numbers. (Often) in mathematical texts, the symbols $\subset$ , $\subseteq$ , $\subsetneq$ , $<$ , $\leq$ have the following meanings: $A \subset B$ and $A \subseteq B$ mean the same thing - that $A$ is a subset of $B$ , possibly with $A=B$ . For $A$ a proper subset of $B$ , we often write $A \subsetneq B$ . $x<y$ is a strict inequality, $x \leq y$ means that $x$ is less than or equal to $y$ . The line underneath is required to say there is equality in the numerical case, whereas in the case of sets that is not the case. Indeed, often in elementary/primary school, children would be taught this way of remembering the symbols. When reaching set theory, this is thrown out. Is there any reason for this inconsistency? Is it simply because of tradition and changing it to be more consistent would therefore be confusing? Or is there a justification for this difference?","['elementary-set-theory', 'algebra-precalculus', 'notation']"
4800621,Questions about the explanation of $\displaystyle{\lim_{\theta \to 0}} \frac{\sin \theta}{\theta} = 1$,"This explanation comes from my textbook: In the computation of $\displaystyle{\lim_{\theta \to 0}} \frac{\sin \theta}{\theta}$ , we first use the unit circle to compare the following areas as follows $\frac{\sin\theta}{2} < \frac{\theta}{2} < \frac{\tan \theta}{2}$ or more simply, $\sin \theta < \theta < \tan \theta$ . Taking reciprocals (which we can do since we are only consiering small $\theta > 0$ ) reverses the direction of the inequalities, so we have $\frac{1}{\tan \theta} < \frac{1}{\theta} < \frac{1}{\sin \theta}$ . Then, multiplying by $\sin \theta$ (which is positive), and noting that $\frac{\sin \theta}{\tan \theta} = \cos \theta$ , gives us $\cos \theta < \frac{\sin \theta}{\theta} < 1$ . Our calculation only establishes this inequality chain for $0 < \theta < \frac{\pi}{2}$ , but since all the terms are the same when we replace $\theta$ with $-\theta$ , it is true for all nonzero $\theta \in \big(-\frac{\pi}{2}, \frac{\pi}{2}\big)$ . Furthermore, our above argument shows that $-\theta < \sin \theta < \theta$ for values of $\theta$ close to $0$ , and hence $\cos \theta = \sqrt{1 - \sin^{2} \theta} > \sqrt{1 - \theta^{2}} > 1 - \theta^{2}$ , so we have $1 - \theta^{2} < \cos \theta < \frac{\sin \theta}{\theta} < 1$ for values of $\theta$ sufficiently close to $0$ . Thus, by the Squeeze Theorem, since $\displaystyle{\lim_{\theta \to 0}} (1-\theta^{2}) = \displaystyle{\lim_{\theta \to 0}} 1 =1$ , we conclude that $\displaystyle{\lim_{\theta \to 0}}\frac{\sin \theta}{\theta} = 1$ . Basically, I would like to know how we get $\sqrt{1 - \theta^{2}} > 1 - \theta^{2}$ ? Also, why can't we just use the Squeeze Theorem with $\cos \theta < \frac{\sin \theta}{\theta} < 1$ , where $\theta \to 0$ to show that $\displaystyle{\lim_{\theta \to 0}} \frac{\sin \theta}{\theta}=1$ ? Lastly, I'm not completely sure why $-\theta < \sin \theta$ .","['algebra-precalculus', 'trigonometry']"
4800632,Asymptotic rate of decay of the integrals,"For each $n$ define the sequence $y_n= \int_{0}^\infty (1-e^{-\frac{x^2}{4}})^n e^{-x}dx$ . By dominated convergence theorem, it is clear that $y_n$ converges to $0$ . I am interested in finding the rate at which $y_n$ decays. Mainly I am interested to know if the decay is exponential in $n$ or not i.e $y_n$ is roughly $c^n$ for some n ? I have encountered this in a physics problem. I am unsure how to study the asymptotic behaviour. Any help is appreciated.","['measure-theory', 'definite-integrals', 'asymptotics', 'real-analysis', 'mathematical-physics']"
4800651,"When $|A|<|B|$ and the function $f: A \to B$ is injective, how do we define the inverse function $g$, with or without the extra elements in $B$?","I need help to understand this elementary part in the set theory. We all have seen the diagram of an injective mapping where few elements of the range set has no pre-image in the domain. My reasoning gets stuck when I think about how can I define the inverse function $g : B \to A$ , where $A, B$ are sets with $|A|=n, |B|=n+2$ and there exists an injective mapping $f: A \to B$ , such that each elements of $A$ gets mapped to exactly one element of the set $B$ . Clearly set $B$ has extra two elements. Since $f$ is injective, then $g$ has to be injective. But my doubts are:- $a$ â€¢ when we construct the inverse function $g: B \to A$ , then do we include those two un-mapped elements for mapping? $b$ â€¢ Is there any difference between these terms $\textrm{not mapping two elements of the domain to the range set}$ and $\textrm{two elements in the domain do not have images in the range set}$ ? $c$ â€¢ From the image it is clear that in case of injective mapping there can be elements in the range set who have no pre-image. But can we say that in case of inverse injective mapping some elements in domain (here set $B$ ) may have no image in the range set ( here set $A$ )? If yes, doesn't it contradict the definition of a function? Also my reasoning in case of constructing the inversion function $g$ in such cases is that the function should look like $g: B \setminus 2 \to A$ , to keep the one-one format, but that is not same as $g : B \to A$ . Therefore I request to help me to clear my doubts here. Any response, answer, hints and suggestions are valuable and appreciated. Thanks to everyone for suggesting links, notes and the answer.",['elementary-set-theory']
4800654,"Multiply an integer polynomial with another integer polynomial to get a ""big"" coefficient","I am new to number theory, I was wondering if the following questions have been studied before. Given $f(x) =  a_0 + a_1 x + a_2 x^2 \cdots + a_n x^n \in \mathbb Z[x]$ , we say that $f(x)$ has a big coefficient $a_i$ if $|a_i| >  \frac{1}{2} \sum_{k=0}^n\left|a_k\right|$ (so that $|a_i|$ is larger than the sum of the remaining coefficients' absolute values). It's easy to see that if a polynomial $P(x)$ has a (real or complex) root with modulus $1$ , then for any $Q(x) \in \mathbb Z[x]$ , the product $P(x)Q(x)$ does not have a big coefficient. I was wondering if the converse of this is true: Question: Suppose $P(x)$ does not have a root with modulus $1$ . Does this implies that there exists $Q(x) \in \mathbb Z[x]$ such that $P(x)Q(x)$ has a big coefficient? Thank you for reading. Any relevant idea/reference would be really appreciated.","['number-theory', 'elementary-number-theory', 'abstract-algebra', 'combinatorics', 'polynomials']"
4800662,A clean proof of Chain rule for BV functions,"Let $u\in BV(\mathbb{R})$ then its distributional derivative $u' \in \mathcal{M}(\mathbb{R})$ (the space of bounded radon measures).
I am trying to prove that $f\in C^1(\mathbb{R})$ , the function $f\circ u \in BV(\mathbb{R})$ satisfies the chain rule $(f\circ u)'=(f'\circ u)u'$ in the sense of measures by a smoothening argument. For $u\in BV(\mathbb{R})$ , let $u^{\epsilon}$ denote its mollification, so that $\left|u^{\epsilon}\right|_{BV(\mathbb{R})}=\left|u\right|_{BV(\mathbb{R})}$ thus upto a subsequence $u^{\epsilon} \rightarrow u$ in $L^1_{loc}(\mathbb{R})$ and ${u^{\epsilon}}' \rightarrow  u'$ in weak* $\mathcal{M}(\Omega)$ . Now, consider \begin{align}
\int_{\mathbb{R}} \phi (f \circ u)'= \lim_{\epsilon \rightarrow 0} 
\int_{\mathbb{R}} \phi (f \circ u^{\epsilon})'= \lim_{\epsilon \rightarrow 0} \int_{\mathbb{R}} \phi (f' \circ u^{\epsilon}) {u^{\epsilon}}' = \lim_{\epsilon \rightarrow 0} \int_{\mathbb{R}} \phi (f' \circ u) {u}' \label{1}\tag{1}
\end{align} which implies that the chain rule holds.
I have the follwoing doubts Is the above argument correct?  especially the last equality in \eqref{1} which uses product of a weakly and strongly convergent sequence is weakly convergent Can we justify the same if $f$ is only Lipschitz continuous in particular $f(x)=|x|$ and $f'(x)=\mathrm{sgn}(x)$ .","['functional-analysis', 'analysis', 'weak-derivatives']"
4800666,Lie bracket as a directional derivative,"I'm trying to understand the Lie bracket operation $[X, Y]$ as the rate of change of $Y$ as seen by an observer moving along the flow of $X$ . Example 1 Suppose $X=\{1, x\}^T$ and $Y=\{1, 0\}^T$ , then $[X, Y]=\{0, -1\}$ and the flow of $X$ is $\{t+x_0,\frac{1}{2}t^2+ t\ x_0+ y_0\}$ . But I'm failing to see how along any of the flows below, the rate of change of vector $\{1, 0\}$ is $\{0,-1\}$ . Example 2 Suppose $X=\{x, x\}^T$ and $Y=\{1, 0\}^T$ , then $[X, Y]=\{-1, -1\}$ and the flow of $X$ is $\{e^t\ x_0,e^t\ x_0-x_0+y_0\}$ . Again from plot below I'm not able to see how the directional derivative makes sense. Edit Adding except from the source.","['lie-derivative', 'differential-geometry']"
4800747,Conditions for the derivative of a function to be a proper map,"Let $f:U \to \mathbb{R}^{m}$ be a continuously differentiable function, where $U \subset \mathbb{R}^{n}$ . In this case, the function \begin{equation*}
Df:U \to L(\mathbb{R}^{n},\mathbb{R}^{m})
\end{equation*} is continuous. Could one formulate conditions on $f$ , which would suffice to assure that $Df$ is a proper map? Here by proper I mean the preimage of every compact set is compact.","['general-topology', 'differential-topology', 'analysis', 'real-analysis']"
4800805,Finding the slant asymptote of a radical function,"I have the following function $f(x)=(x-2)^{1/3}(x+4)^{2/3}$ . I'm asked to find all asymptotes of this function. Clearly, there are no vertical asymptotes since there are no points of discontinuity. There are also no horizontal asymptotes since $\displaystyle{\lim_{x\to \infty}f(x)=\infty}$ and $\displaystyle{\lim_{x\to -\infty}f(x)=-\infty}$ So that means I just need to check for the slant asymptotes. I know that $\displaystyle m=\lim_{x \to \infty}\frac{f(x)}{x}$ and $\displaystyle b=\lim_{x \to \infty}{(f(x)-mx)}$ $\displaystyle m=\lim_{x \to \infty}\frac{f(x)}{x}$ $\displaystyle m=\lim_{x \to \infty}\frac{(x-2)^{1/3}(x+4)^{2/3}}{x}$ $\displaystyle m=\lim_{x \to \infty}\frac{x(1-\frac{2}{x})^{1/3}(1+\frac{4}{x})^{2/3}}{x}$ $\displaystyle m=\lim_{x \to \infty}\left(1-\frac{2}{x}\right)^{1/3}\left(1+\frac{4}{x}\right)^{2/3}=1$ The problem is $b$ . $\displaystyle b=\lim_{x \to \infty}{(f(x)-mx)}$ $\displaystyle b=\lim_{x \to \infty}{((x-2)^{1/3}(x+4)^{2/3}-x)}$ The answer SHOULD be $2$ have no idea how to proceed however. I tried to manipulate this into L'hopital's rule but I don't think that's possible here... Can someone help out? Thanks!","['limits', 'calculus']"
4800850,Why does $\mathbb{P}[X_i=j]= (\frac 1 n)^{j-1} \cdot (\frac {n-i +1} 1)$ here?,"I am asking this to understand the coupon collector's problem better. Say that we are trying to collect $n$ coupons, and every time we obtain a box of cereal, we get a coupon, which has an equal probability of giving us any of the $n$ coupons. I am trying to understand why if we let $X_i$ be a random variable which returns the amount of boxes we bought until we got the $i$ th new card, measured from the boxes we bought right after getting the $(i-1)$ st new card, that $\mathbb{P}[X_i=j]= (\frac 1 n)^{j-1} \cdot (\frac {n-i +1} 1)$ . My chief difficulty in understanding this is that the sample points we have vary in terms of which coupons their $i$ th first element and $(i-1)$ st first element are, as well as which boxes it were on which we have found the $i$ th first element and $(i-1)$ st first element. If the question simply was, what is the probability of finding coupon $a$ on the $b$ th box we have opened, and to open $j-1$ boxes after this and still get coupon $a$ , and then to on the $j$ th box after this to get coupon $b$ , I could see that $(\frac 1 n)^{j-1} \cdot (\frac {n-i +1} 1)$ is the probability; but because these things vary on each sample point I am not seeing this. So far, I have tried thinking that the question is essentially that given we have grabbed $i-1$ different things out of $n$ available choices, whatâ€™s the probability that for the next $j-1$ grabbings, we grab the same thing, and for the jth grabbing, grab something different? However, the matter is still rather unclear for me- even when I phrase the question like this, I am not seeing how to relate this to the sample points which vary in terms of how many boxes it took to obtain the $i$ th new coupon and which coupons is the $i$ th, etc. So why does $\mathbb{P}[X_i=j]= (\frac 1 n)^{j-1} \cdot (\frac {n-i +1} 1)$ here?","['geometric-distribution', 'discrete-mathematics', 'probability']"
4800855,How to use the minimal polynomial theorem for this block matrix,"Let $M$ be a matrix made up of two diagonal blocks: $M = \begin{pmatrix} A & 0 \\ 0 & D\\ \end{pmatrix}$ Prove that $M$ is diagonalizable if and only if A and D are diagonalizable. I know I'd have to use the minimal polynomial theorem, I just learnt it with one diagonal matrix in the diagonal, so for some reason I can't think of a way to implement it here Is the minimal polynomial equation $M_{min} = A_{min} * D_{min}$ ?
And after that can I say $A_{min} * D_{min} = 0$ ?","['matrices', 'minimal-polynomials', 'linear-algebra', 'diagonalization', 'block-matrices']"
4800861,Apply Power Series Solution Method on Solving ODE with non-integer x order term $ \frac{d^2y}{dx^2}+x^{1/2}y=0 $,"Problem: Use series solution expansion to solve ODE in region x>0 $$ \frac{d^2y}{dx^2}+x^{1/2}y=0 $$ Attempts: Assume $y=\sum^\infty_{n=0} a_nx^n$ , then $\frac{d^2y}{dx^2}=\sum^\infty_{n=2} n(n-1)a_nx^{n-2}$ . The ODE can then be expressed in series expansion, followed by: $\sum^{\infty}_{n=2}n(n-1)a_nx^{n-2}+\sum^{\infty}_{n=2}a_nx^{n+1/2} =0$ by switching indic, we get $\sum^{\infty}_{n=0}(n+2)(n+1)a_{n+2}x^{n}+a_nx^{n+1/2}=0$ Here is where I get confused: for the equation to hold, every n substitution in the expression will result in 0 and the recurrence relationship can be obtained. But here, say if I let $n=0$ , the recurrence relationship will become involved with x, as shown, $2\cdot 1\cdot a_2=a_n\cdot x^{1/2}$ But I doubt it is correct because then the constants $a_n$ will become x-dependent. I also attempted to solve it on Mathematica, it seemed like somehow the solution can be found to be Bessel functions, $y(x)=C_1\sqrt{x}\;\Gamma (\frac{3}{5}) J_{-\frac{2}{5}}(\frac{4x^{5/4}}{5})+C_2\sqrt{x}\;\Gamma (\frac{7}{5}) J_{\frac{2}{5}}(\frac{4x^{5/4}}{5})$ but I really am not sure whether that is correct or how to get there","['power-series', 'calculus', 'ordinary-differential-equations']"
4800871,How do you solve the ODE $x'+2tx=2t^3$,"I do not get their explanation here at all. In class, we derived the following formula for the method of integrating  factors for solving first order inhomogeneous linear ODEs of the form $$y'(x)+p(x)y(x)=q(x).$$ $$y(x)=\dfrac{1}{u(x)}\left[\int u(x)q(x)dx+C\right],$$ where the integrating factor is given as $$u(x)=e^{\int p(x)dx}.$$ Following their method, I indeed got the same integrating factor $u(t)=e^{t^2}$ , but with the formula above, how then am I supposed to arrive at their conclusion? Integration by parts on the $u(t)q(t)$ part seems to get me nowhere as (without using the error function), $e^{t^2}$ cannot be differentated away or integrated. Could someone perhaps help fill in the missing lines of working or explain the intermediary steps?",['ordinary-differential-equations']
4800894,How do I implicitly differentiate $\frac{d}{dx}(2x(y')^2y'')$,"I know that I will probably need to use the 3-way chain rule: $$(fgh)'=f'gh+fg'h+fgh',$$ but since I'm differentiating in terms of $x$ , I'm very confused as to how to to ""append"" $y'$ terms to the end of each term in the product rule expansion. For instance, I believe $\frac{d}{dx}(xy)=y+xy'(y')$ , where I have ""appended"" a $y'$ to the latter term. This makes sense from the (informal) ""fraction analogy"" in Leibniz notation: $$\frac{d}{dx}=\frac{d}{dy}\cdot\frac{dy}{dx},$$ but I have trouble generalising to more complicated examples like the one mentioned above. It would be much appreciated if someone can write out the full solution or explain how they got it. Note: $y=y(x)$ (is this noteworthy?)","['multivariable-calculus', 'calculus', 'implicit-differentiation', 'derivatives']"
4800951,Does the metric derivative have a weak formulation?,"Given a metric space $(X, d)$ and a function $f:I:=(a,b)\subseteq \mathbb{R} \rightarrow X$ , we can define the metric derivative of $f$ at $t\in I$ as $$ f'(t) := |\frac{d}{dt}|f(t) := \lim_{\delta\rightarrow 0} \frac{d(f(t+\delta),f(t))}{|\delta|} \in \mathbb{R} $$ Of course, the limit in $t$ exists only for those $f$ such that the function $g_t(\delta) := d(f(t+\delta),f(t)) $ is differentiable at $\delta=0$ (since $f'(t) = g_t'(0)$ ). My question is : if the functions $g_t$ are not differentiable, but just weakly differentiable, can we provide a notion of weak metric derivative ? I tried to have a look around and I didn't find this. Is it sufficient to require that ""the $g_t$ are weakly differentiable at $0$ ""? Could someone hint whether there are some subtleties one should pay attention to?","['metric-spaces', 'functional-analysis', 'differential-geometry']"
4800987,Find the eingenvalues for a specific integral operator,"So i've been reading a book on func analysis and in the examples section there is a problem in which it is stated
Find the eigenvalues for $y = Kx$ , where $$y(t)=\int_{-1}^{1}(1-t\tau)x(\tau)d\tau.$$ In my undergrad linear algebra we did find eigenvalues, but only for matrix equations of the form $Tx=\alpha x$ where one looks for the eingenvalue of $det(T-I\lambda)=0$ , where $T$ is some linear mapping.
Then one obtains a characteristic polynomial from which one obtains eigenvalue and then eigenvectors.
I don't know how to begin here.","['linear-algebra', 'functional-analysis', 'analysis']"
4800993,Problems regarding properties of the Floyd's Triangle,"Setup Have a look at the Floyd's Triangle :- 1 
2 3 
4 5 6 
7 8 9 10 
11 12 13 14 15 
16 17 18 19 20 21 
22 23 24 25 26 27 28 
29 30 31 32 33 34 35 36 
37 38 39 40 41 42 43 44 45 
46 47 48 49 50 51 52 53 54 55 
56 57 58 59 60 61 62 63 64 65 66 
67 68 69 70 71 72 73 74 75 76 77 78 
79 80 81 82 83 84 85 86 87 88 89 90 91 
92 93 94 95 96 97 98 99 100 101 102 103 104 105 
106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 
121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 
... Here, all the Natural Numbers $(\mathbb{N})$ are written consecutively, row-wise. Each row has a length of $n$ , where $n$ increments by $1$ in each succession. I noticed several properties of this triangle, related to a few of which I have questions. So, I have broken this post down into $3$ parts for related questions that I found the most perplexing. Part 1 Let's take the sum of each row. Their sequence would be:- 1, 5, 15, 34, 65, 111, 175, 260, 369, 505, 671, 870, 1105, 1379, 1695, 2056, ... which is given by the formula $\frac{n(n^2 + 1)}{2}$ . If we take the averages of all the $n$ th and $(n + 2)$ th terms (where $n$ is an odd number), we get the sequence :- (1 + 15) / 2, (15 + 65) / 2, (65 + 175) / 2, ...

or,

8, 40, 120, 272, 520, 888, ...       --- (i) Then, taking the sequence of the middle or the even $n$ th terms [( $n + 1$ )th terms, where $n$ is odd], we get this :- 5, 34, 111, 260, 505, 870, ...       --- (ii) Subtracting the sequence (ii) of middle terms from the sequence (i) of the averages of the neighbouring extremes, we get:- 8 - 5, 40 - 34, 120 - 111, 272 - 12, 520 - 505, 888 - 870, ...

or,

3, 6, 9, 12, 15, 18, ... Why are multiples of 3 showing up here? Part 2 (Here's a smaller version of the same triangle, for your ease of viewing):- 1 -
2 3 *
4 5 6 *
7 8 9 10 -
11 12 13 14 15 *
16 17 18 19 20 21 *
22 23 24 25 26 27 28 -
29 30 31 32 33 34 35 36 *
37 38 39 40 41 42 43 44 45 *
... Have a look at the ""hypotenuse"" or the right-most numbers of the rows of the triangle. Notice that they are triangular numbers , given by the formula $\frac{n(n + 1)}{2}$ . Take a look at the pairs of $2$ nd and $3$ rd, $5$ th and $6$ th, $8$ th and $9$ th, ... $(3n - 1)$ th and $(3n)$ th of these triangular numbers (shown above by * s). That is, (3, 6), (15, 21), (36, 45), (66, 78), ... Observe that, yet again, all of these numbers are divisible by 3. Written as multiples of 3, they are:- (3 * 1, 3 * 2), (3 * 5, 3 * 7), (3 * 12, 3 * 15), (3 * 22, 3 * 26), ... Here, the differences between the multiplicands in individual pairs are:- 2 - 1, 7 - 5, 13 - 12, 26 - 22, ...

or,

1, 2, 3, 4, 5, ... And their sums are:- 2 + 1, 7 + 5, 13 + 12, 26 + 22, ...

or,

3, 12, 27, 48, ...

or,

3 * 1^2, 3 * 2^2, 3 * 3^2, 3 * 4^2,... Also, observe the difference between the first term of the $(n + 1)$ th pair, and the last term of the $n$ th pair (where $n \in \mathbb{N}$ ):- 15 - 6, 36 - 21, 66 - 45, ...

or,

9, 15, 21, ...

or,

3 * 3, 3 * 5, 3 * 7, ... The multiplicands of the differences are consecutive odd numbers. Moreover, the terms that we left out (denoted by - s):- 1, 10, 28, 55, 91, 136, ... also follow a pattern. The differences between this sequence's consecutive terms are:- 10 - 1, 28 - 10, 55 - 28, 91 - 55, 136 - 91, ...

or,

9, 18, 27, 36, 45, ... What is the reason behind these specific numbers (and mainly $3$ ) popping up? What is the intuitive reason and / or proof for why things are the way they are? Part 3 (The same triangle reproduced again):- @ * - 1
  *   2 3
    - 4 5 6
! *   7 8 9 10
! *   11 12 13 14 15
    - 16 17 18 19 20 21
      22 23 24 25 26 27 28
@ *   29 30 31 32 33 34 35 36
@ *   37 38 39 40 41 42 43 44 45
      46 47 48 49 50 51 52 53 54 55
      56 57 58 59 60 61 62 63 64 65 66
! *   67 68 69 70 71 72 73 74 75 76 77 78
! *   79 80 81 82 83 84 85 86 87 88 89 90 91
      92 93 94 95 96 97 98 99 100 101 102 103 104 105
      106 107 108 109 110 111 112 113 114 115 116 117 118 119 120
    - 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136
      ... This time, observe the ""perpendicular"" or the left-most numbers. The following are the perfect squares that show up (shown by - s):- 1, 4, 16, 121, 529, 4096, 17956, 139129, 609961, 4726276, 20720704, ... Their respective square roots are:- 1, 2, 4, 11, 23, 64, 134, 373, 781, 2174, 4552, ... The differences between consecutive terms of this sequence are:- 1, 2, 7, 12, 41, 70, 239, 408, 1393, 2378, ...      ---- (iii) And the differences between this sequence's (iii) terms are:- 1, 5, 5, 29, 29, 169, 169, 985, 985, ... Why do only these numbers pop up, that too in pairs of two ? Let's look at the vertical gap between two consecutive numbers:- 0, 1, 2, 9, 16, 57, 98, 337, 576, 1969, 3362, ... For example, the first perfect square -- $1$ , shows up in the first row. So, there is $0$ gap between the row containing the previous perfect square and that of $1$ . Similarly, $4$ comes $1$ row after $1$ , so it has a gap of $1$ , $16$ comes $2$ rows after $4$ , so it has a gap of $2$ , $121$ comes $9$ rows after $16$ , so it has a gap of $9$ , etc. This sequence's differences are:- 1, 1, 7, 7, 41, 41, 239, 239, 1393, 1393, ... Notice that this looks like the sequence (iii) [i.e., of the differences between consecutive terms of the square roots of the perfect squares, found in the left-most perpendicular column of the triangle.] 1, 2, 7, 12, 41, 70, 239, 408, 1393, 2378, ...
1, 1, 7, 7, 41, 41, 239, 239, 1393, 1393, ... The only variation being that all of the even $n$ th terms are replaced by the term that precedes them. Why is this? Let's observe the terms that got ""ignored"" [even $n$ th terms]:- 2, 12, 70, 408, 2378, ... Dividing this by 2, 1, 6, 35, 204, 1189, ... Here, after a bit of scrutiny, we observe that the formula for the $n$ th term of this sequence would be:- \begin{align*}
T_n = 6 \cdot T_{n-1} - T_{n-2}
\end{align*} where $T_0 = 0$ and $T_1 = 1$ . For example, $T_2$ will be 6 * 1 - 0 , $T_3$ will be 6 * 6 - 1 , $T_4$ will be 6 * 35 - 6 , etc. What is the reason behind this exact formula here? Searching the sequence up in OEIS, we get this , having the exact formula we just noticed. But it is also given there that $T_n ^ 2$ is a triangular number. Why is this true? Now let's look at the ones that did not get ""ignored"" [odd $n$ th terms]:- 1, 7, 41, 239, 1393, ... We observe that the formula for the $n$ th term of this sequence would be the same as the previous one:- \begin{align*}
T_n = 6 \cdot T_{n-1} - T_{n-2}
\end{align*} where $T_0 = -1$ and $T_1 = 1$ . Note that this time, $T_0 = -1$ , and not $0$ . The same questions emerge again, why is this true? What are the relationships among these numbers and formulae, and what more is special about them? Let's now look for primes in this perpendicular (shown by * s):- 1, 2, 7, 11, 29, 37, 67, 79, 137, 191, 211, 277, 379, 631, 821, 947, 991, 1129, 1327, 1597, 1831, 2017, 2081, 2347, 2557, 2851, 2927, 3571, 3917, 4561, 4657, 4951, 5051, 5779, 6217, 6329, ... The first few of them come clumped in pairs, but then the pattern changes and becomes confusing at best. I tried seeing if there is a pattern in the fact that some are equal to $3$ (shown by ! s), while some are equal to $1$ (shown by @ s) (mod $4$ ), but no luck again. There were many such false alarms, where the patterns ultimately broke apart. Could you try and explain why these primes show up, and what are some interesting relations / properties that you can come up with? I know this is a really long question, so it would be really helpful if you could answer or extend any aspect of any problem. Moreover, more properties and relationships that you can find related to this triangle are really appreciated. Thanks a lot for giving your time.","['elementary-number-theory', 'prime-numbers', 'discrete-mathematics', 'sequences-and-series']"
4801012,Has the diophantine equation $ \sum_{n=1}^{N} \frac{1}{x_{n}} = \prod_{k=1}^{N} \left(1-\frac{1}{x_{k}} \right) $ been studied?,"Background I wonder if there are any rational numbers such that their Egyptian fraction (sum) representations are equal to their Egyptian product analogue. In other words, I am curious about solutions to the diophantine equation $$ \sum_{n=1}^{N} \frac{1}{x_{n}}  = \prod_{k=1}^{N} \left(1-\frac{1}{x_{k}} \right) \tag{1}\label{1} $$ such that $x_{1} < x_{2} < \dots < x_{N-1} < x_{N} \in \mathbb{N}^{+}$ . Solutions When $N=2$ , the only solution appears to be $(x_{1},x_{2}) = (3,5) $ , which yields the rational $\ \ $ number $\frac{8}{15}$ . For $N=3$ , we have the solution $(x_1, x_2, x_3) = (3,6,28)$ , yielding the rational number $\frac{15}{28}$ . For $N=4$ , I've found eight solutions so far. Let $x_{a}^{4}$ be the $a$ 'th solution of the equation in four variables. We then have $x_{1}^{4} = (3, 7, 24, 52)$ , $x_{2}^{4} = (3, 9, 15, 37) $ , $x_{3}^{4} = (3, 9, 16, 32)$ , $x_{4}^{4} = (3, 10, 15, 26)$ , $x_{5}^{4} = (4, 5, 15, 36)$ , $x_{6}^{4} = (4,5,17,28)$ , $x_{7}^{4} = (4,7,8,35)$ , and $x_{8}^{4} = (5,6,10,12)$ . These solutions correspond to the rational numbers $\frac{391}{728}$ , $\frac{899}{1665}$ , $\frac{155}{288}$ , $\frac{7}{13}$ , $\frac{49}{90}$ , $\frac{324}{595}$ , $\frac{153}{280}$ , and $\frac{11}{20} $ , respectively. For $N \geq 5$ , I haven't found any solutions yet. I suspect there are no solutions for the equation with this many variables -- though I'd gladly be proven wrong. Related Equation I know that in ZnÃ¡m's problem , solutions to the equation $$ \sum \frac{1}{x_{i}} + \prod \frac{1}{x_{i}} = y \tag{2}\label{2}  $$ are studied, where $y$ and each $x_{i}$ must be integer. However, this equation is different from the one described above. Question : Has equation \eqref{1} been described and studied in the mathematical literature before?","['number-theory', 'egyptian-fractions', 'diophantine-equations', 'reference-request']"
4801034,Proving the Dimension of the Commutant Space of a $n\times n$ Matrix is Greater Than or Equal to $n$,"$M_n(F)$ represents the collection of all square matrices of size $n \times n$ with elements from the field $F$ . Let's take a specific matrix $A$ from $M_n(\mathbb{R})$ , where $\mathbb{R}$ is the set of real numbers. We'll define a set $C(A)$ as the set of matrices $B$ in $M_n(\mathbb{R})$ such that the product $AB$ is equal to $BA$ : $$C(A)=\{B\in M_n(\mathbb{R})\mid AB=BA\}$$ It can be shown relatively easily that the set $C(A)$ forms a vector space over the field of real numbers, $\mathbb{R}$ . We are tasked with demonstrating that the dimension of this vector space is greater than or equal to $n$ : $$\text{dim }C(A)\geq n$$ I've extensively researched various approaches to this problem and came across several methods that involve concepts like the Jordan canonical form, the minimal polynomial, and the characteristic polynomial of matrix A, ideas related to linear transformations. However, I aim to tackle this problem using the knowledge I've acquired in my course. Some of them are: Matrix elementary operations and elementary matrices. Row-reduced echelon form of a matrix. Vector space, including its definition, properties, basis, dimension, kernel, and nullity. Subspaces and their properties, as well as their relationship to the vector spaces they are part of. Understanding linear independence and dependence of vectors. The concept of a coordinate vector relative to a given basis. Recognition of row and column spaces of a matrix, although I haven't yet encountered the rank of a matrix. $\dots$ Because the methods we have for solving the problem are quite basic, I believe it's enough to demonstrate that we have a minimum of $n$ elements in the basis. I suspect that techniques involving diagonal matrices, nullity, and elementary matrices might be useful for this purpose. I would greatly appreciate any assistance or hints to help me solve this problem using the knowledge and concepts I've already been taught in my course. If you'd like to utilize advanced ideas, try to demonstrate them using fundamental concepts or establish their characteristics using basic tools. For instance, if you plan to utilize the Jordan canonical form, begin by explaining what it is and then highlight the specific characteristics of this concept that you will use to address the main problem. However, in this context, simply mentioning the names of the relevant concepts along with their properties or referencing a source that introduces and proves these properties is sufficient and greatly appreciated. I have carefully read all the comments, and I am extremely grateful to those who took the time to comment. However, their responses were beyond the scope of what we can incorporate or apply. EDIT Please check this solution .","['matrices', 'linear-algebra']"
4801042,Question about only centering a matrix once in the covariance matrix,"From probability, we have $$\Sigma_x = \frac{\left(HX\right)^T\left(HX\right)}{N}$$ Where $\Sigma_X$ is the covariance matrix for a data matrix $X$ . $X$ is constructed by putting all the datapoints in rows, so the first row is the first datapoint, the second row is the second collected data point and so forth. $H$ is the centering matrix $I - \frac{11^T}{N}$ Simplifying $\Sigma_X$ , $$\frac{\left(HX\right)^T\left(HX\right)}{N} = \frac{X^TH^T H X }{N} = \frac{X^T H ^2 X}{N} = \frac{X^THX}{N}$$ This both makes sense and doesn't make sense. At first glance, centering a matrix once and then doing so again does nothing the second time around. Matrix $H$ is idempotent. As a result when we lose the squared above the H, I'm okay with that. However digging a little deeper, it doesn't make sense that only one of my data matrices is centered when I should have two centered data matrices to properly compute covariance. Going to the 1D case, $$E[(X - \mu_{x})^2]  \neq E[X(X - \mu_{x})]$$ Here it doesn't make sense for only one of my $X$ random variables to be centered while the other one isn't.","['statistics', 'covariance', 'variance', 'expected-value', 'linear-algebra']"
4801058,What's the topological par for each positive integer?,"In Possible number of open sets in a topology Shreya Jaganathan asked if there exists a finite topological space with exactly 100 open sets. It was quickly pointed out that for each positive integer $N$ , $\{0,1,\dots,N-2\}$ is a set of size $N-1$ with the topology $\{\{0,1,\dots,n-1\}:0\leq n\leq N-1\}$ of size $N$ . It was then pointed out by bof that for $N=100$ , there exists a topology with $N$ open sets on a set of size $8$ , but M W showed there does not exist such a topology on a set of size $7$ . Let $\operatorname{Par}(N)$ be the topological par of each positive integer $N$ , defined as the smallest cardinality of a topological space that has exactly $N$ open sets. Then $\operatorname{Par}(100)=8$ by bof and M W's results. We can see that $\operatorname{Par}(2^N)=N$ : every topology is a subset of the power set of size $2^N$ , and the power set is itself the discrete topology. This also shows that $\operatorname{Par}(N)\geq \lceil \log_2(N)\rceil$ . Note that $\operatorname{Par}(100)=8>7=\lceil\log_2(100)\rceil$ . How might we compute the topological par for positive integers in general?","['general-topology', 'combinatorics']"
4801065,"Prove that in a topological space, empty intersection of a set and its boundary implies that the set is in the topology","As the title suggests, I would like to prove that in a topological space $(X,\tau)$ $$A\cap\partial A = \varnothing\implies A\in\tau$$ is true for $A\subset X$ . My attempt: For every $x\in A,\,\exists T_x\in\tau$ with $x\in T_x$ and $T_x\subseteq A$ .
This follows from $$\partial A := \{x \in X;\, \mathrm{for\ every\ } T\in\tau\mathrm{\ with\ } x\in T\mathrm{,\ } A\cap T \neq \varnothing \;\mathrm{and}\;(X\setminus A)\cap T \neq\varnothing\}$$ since if no such $T_x$ existed, then either $$T_x\cap A\neq\varnothing\ \wedge\ T_x\cap\partial A\neq\varnothing\ \wedge\ T_x\cap(X\setminus \overline{A})=\varnothing$$ which is impossible by the definition of $\partial A$ , or $$T_x\cap A\neq\varnothing\ \wedge\ T_x\cap\partial A\neq\varnothing\ \wedge\ T_x\cap(X\setminus\overline{A} )\neq\varnothing$$ meaning $x\in\partial A$ which violates the assumption. Since $T_x\subseteq A$ we have $\cup T_x\subseteq A$ . Furthermore, for every $x\in A$ we have $x\in T_x$ so that $A\subseteq\cup T_x$ . Thus $A = \cup T_x$ , giving us $A\in\tau$ by definition. Is this proof correct? and if so, is there a simpler way to prove the same thing? Thanks in advance!","['elementary-set-theory', 'general-topology']"
4801071,index of $Z\mathrm{SL}_2(\mathbb{Q}_p)$ in $\mathrm{GL}_2(\mathbb{Q}_p)$,"Let $p>2$ , consider the subgroup $Z\mathrm{SL}_2(\mathbb{Q}_p)$ of $\mathrm{GL}_2(\mathbb{Q}_p)$ where $Z\cong\mathbb{Q}_p^\times$ is the centre of $\mathrm{GL}_2(\mathbb{Q}_p)$ . Then I'd like to know how to prove that the index of $Z\mathrm{SL}_2(\mathbb{Q}_p)$ in $\mathrm{GL}_2(\mathbb{Q}_p)$ is $4$ . By the determinant map, I know that there is an isomorphism of groups $\mathrm{GL}_2(\mathbb{Q}_p)/\mathrm{SL}_2(\mathbb{Q}_p)\cong\mathbb{Q}_p^\times$ . But I do know how to continue. The motivation is that I want to know the dimension of the induced representation $\mathrm{Ind}_{Z\mathrm{SL}_2(\mathbb{Q}_p)}^{\mathrm{GL}_2(\mathbb{Q}_p)}$ of a character.","['matrices', 'number-theory', 'group-theory', 'p-adic-number-theory']"
4801132,"Show that the equation $(1-x)\cos{x} = \sin{x}$ in $(0,1)$ has at least one solution.","Show that the equation $(1-x)\cos{x} = \sin{x}$ in $(0,1)$ has at least one solution. We have $f(x) = (1-x)\cos{x} - \sin{x} = 0$ We have $$f(0) = (1-0)\cos{0} - \sin{0} = 1 - 0 = 1 > 0$$ $$f(1) = (1-1)\cos{1} - \sin{1} = 0 - \sin{1} = -\sin{1} < 0$$ So there must be a number $c \in (0,1): f(c) = 0$ , so there is at least one solution. Is this correct ? Did I forget or miss something ?","['analysis', 'real-analysis', 'continuity', 'functions', 'trigonometry']"
4801150,Roots of $ 16 x^5 - 20 x^3 + 5x + 1 = 0 $,"The following is from Edexcel further mathematics Core Pure Book 2 A Level Mixed Exercise 1 Question 9 part b: 9 a Use De Moivre's Theorem to show that $$ \cos 5\theta \equiv 16 \cos^5 \theta - 20\cos^3 \theta + 5\cos
 \theta. $$ b Hence find all solutions to the equation $$ 16 x^5 - 20 x^3 + 5x + 1 = 0 $$ giving your answers to $3$ decimal places where necessary. Part $(a)$ was fine. Part $(b)$ I also got the correct answer. However, I find the logic I'm using to be dubious (although I suspect it is the same logic that Edexcel considers correct). My answer to b : When making trigonometric substitutions, I have been told that you should use a bijective function as the substitution: this helps determine the maximum domain on which you can choose the substitution. So let $x=\cos \theta,\quad 0\leq \theta \leq \pi.$ (Note that this is that largest domain on which the substitution is bijective). Using the identity from part $(a),$ the equation $ 16 x^5 - 20 x^3 + 5x + 1 = 0\ $ is equivalent to $\ \cos 5\theta = -1.\ $ Since we defined $0\leq \theta \leq \pi,\ $ it follows that $ 0\leq 5\theta \leq 5\pi.\ $ Thus we have to solve $\ \cos 5\theta = -1\ $ in the interval $ 0\leq 5\theta \leq 5\pi.\ $ This has solutions: $$\ 5\theta = \pi, 3\pi, 5\pi,\ \implies \theta = \frac{\pi}{5}, \frac{3\pi}{5}, \frac{5\pi}{5},$$ $$\implies x=\cos\theta = \cos\frac{\pi}{5},\ \cos\frac{3\pi}{5},\ \cos\frac{5\pi}{5} = 0.809, -0.309, -1. $$ $$$$ The part I find dubious is that we have only looked for roots of the equation $ 16 x^5 - 20 x^3 + 5x + 1 = 0 $ that are between $-1$ and $1$ . We haven't determined if there are roots of this equation $<-1$ or $>1.$ So we don't know yet if we have found all roots of the equation. Am I right about this?","['trigonometry', 'systems-of-equations', 'polynomials', 'substitution']"
4801219,Skewing ellipse axis using Excel,"Somewhat mathematically challenged person here. I use Excel to generate points to feed a CNC Router to cut ellipses. (Strictly for personal use - nothing for sale - retirement learning process.) I calculate the Radian (Rad) by degree (Rad = Degrees * PI() / 180), stepping all the way around the ellipse, one Degree at a time. I use the following two equations for calculate the X and Y points for each Degree step around. X = (Width * COS( Rad )) Y = (Height * SIN( Rad )) (Width and Height are half of the diameter of their respective axis.) I usually use 0,0 as my center point. This works well for dishes and bowls where the major axis is perfectly horizontal or vertical. Now, I'm trying to branch out, and I'm hitting a snag. Would there be a reasonable way to modify these equations to be able to skew the major/minor axis set, using, as an example, a 15 Degree skew? (If I can see how to do it with a specific skew angle, I think I can figure out how to adapt it to what I'm doing. That's why I suggested a 15 Degree skew angle.) Any and all help (and constructive criticism for not learning enough to resolve this by myself) will be greatly appreciated! David C PS: For reference, I want to cut a ""Star Burst"" set if ellipses with constantly gently changing lights in each segment for a night light for each of my twin granddaughters. The electronics for the color changing lights are no problem for me. The equations for the ellipses, however.....",['geometry']
4801232,Complex vector bundle valued forms,"Let $X$ be a smooth manifold and $E \to X$ a complex vector bundle. A connection on $E$ is a $\mathbb{C}$ -linear map $\nabla \colon \Gamma(X,E) \to \Omega^1(X,E)$ where $\Gamma(X,E)$ denotes the space of sections of $E$ and $\Omega^1(X,E)$ is the space of  the space of sections of $T^{\vee} X \otimes_{\mathbb{R}} E$ , where $T^{\vee} X = \mathrm{Hom}_\mathbb{R}(TX,\mathbb{R})$ . Now if $X$ comes with an almost complex structure $J$ , then one usually considers the complexified spaces. In particular $T^{\vee}_{\mathbb{C}} X = T^{\vee} X \otimes_{\mathbb{R}} \mathbb{C} = \mathrm{Hom}_\mathbb{R}(TX,\mathbb{C})$ . This space comes with a splitting induced by the $\pm \mathrm{i}$ -subspaces of $J$ , i.e. $T^{\vee}_{\mathbb{C}} X = T^{1,0}X^{\vee} \oplus T^{1,0}X^{\vee}$ . This induces a splitting $\Omega_{\mathbb{C}}^1(X,E) = \Omega^{1,0}(X,E)\oplus \Omega^{0,1}(X,E)$ . In many textbooks it is then written that $\nabla = \nabla^{1,0} +Â \nabla^{0,1}$ . But $\nabla$ was defined as a map into $\Omega^1(X,E)$ . Is this the same space as $\Omega^1_{\mathbb{C}}(X,E)$ ? Because I believe that this is the space of sections of $T^{\vee}_{\mathbb{C}}X \otimes_{\mathbb{C}} E$ , where the tensor product over $\mathbb{C}$ is used. So $$ T^{\vee}_{\mathbb{C}}X \otimes_{\mathbb{C}} E = T^{\vee}X \otimes_{\mathbb{R}} \mathbb{C} \otimes_{\mathbb{C}} E = T^{\vee}X \otimes_{\mathbb{R}} E $$ and hence, $\Omega^1_{\mathbb{C}}(X,E) =\Omega^1(X,E)$ . Or am I missing something? In many textbooks this construction is unfortunately not written clearly.","['complex-geometry', 'vector-bundles', 'differential-forms', 'differential-geometry']"
4801278,Proof Verification: Prove X and Y are open sets,"The exercise was to show that the following two sets are open given $G$ is a region and $f$ is holomorphic in $G$ . $ X = \{a \in G \text{: there exists } r>0 \text{ such that }f(z)= 0 \text{ for all } z\in D[a,r]\}$ $ Y = \{a \in G \text{: there exists } r>0 \text{ such that }f(z) \ne 0 \text{ for all } z\in D[a,r]\backslash \{a\}\}$ My proof did not use the fact that $f$ is holomorphic, so I fear there is a flaw somewhere. Here is my attempt at proving this. Let $x \in X\cap \partial X$ . Since $x \in X$ , we know there is $r>0$ such that for all $z \in D[x,r], f(z)=0$ . Similarly, since $x\in\partial X$ , for all $R>0$ there is some $z \in D[x,R]$ such that $z\notin X$ . In particular, there is $z^*$ in $D[x,r]$ such that $z^* \notin X$ . Choosing $\delta = \text{min}\{|x-z^*|, r-|x-z^*|\}$ , we know $f(z) \ne 0$ for some $z \in D[z^*, \delta]$ and $D[z^*,\delta] \subset D[x,r]$ . But this contradicts $f(z)=0$ for all $z\in D[x,r]$ , showing $X \cap \partial X = \emptyset$ , meaning $X$ is open. Let $y \in Y$ . Then there is $r>0$ such that $f(z) \ne 0$ for all $z \in D[y,r] \backslash \{y\}$ . Pick $a \in D[y,r] \backslash \{y\}$ . Let $\epsilon = \text{min}\{|y-a|, r-|y-a|\}$ . Then $f(z) \ne 0$ for all $z \in D[a, \epsilon]$ because $D[a,\epsilon] \subset D[y,r] \backslash \{y\}$ . So $a \in Y$ , meaning $D[y,r] \subset Y$ , so $Y$ is open.","['complex-analysis', 'analysis']"
4801347,The number of the union-intersection calculation results equals to the number of paths,"Problem Statement: Let $A_1,A_2,\dots,A_{2k+1}$ be $2k+1$ generic sets. We insert $\cap$ and $\cup$ alternately between them, as $A_1\cap A_2\cup A_3\dots\cup A_{2k+1}$ . The calculation results depend on how I add the parentheses, as $A_1\cap (A_2\cup A_3)$ generally does not equal to $(A_1\cap A_2)\cup A_3$ . But sometimes different parenthesis may have same results, as $((A_1\cap A_2)\cup (A_3\cap A_4))\cup A_5$ equals to $(A_1\cap A_2)\cup ((A_3\cap A_4))\cup A_5)$ . Among all possible ways to add the parentheses, how many distinct results (for general sets) are there? What about $2k$ sets? I have written some code and looked for OEIS, and it turns out the $2k$ -set case matches A032349 and $2k+1$ -set case matches A027307 . The description for the sequence is partially correct. Both of them are considering paths taking steps $(2,1),(1,2),(1,-1)$ and does not fall below $x$ axis, but A032349 has wrong starting point and destination: it should be from $(0,1)$ to $(3k+1,0)$ . I have tried several ways to deal with this, such as deriving a recurrent formula for the number of results (as we deal with Catalan) or doing bijection (I have make the number of sets to some kind of trees). However, I failed. One of my friends hinted this literature as ""schroder path"" or ""full ternary tree"", but I cannot understand it... Can anyone craft an easy-to-understand bijection or proof? Thank you all! P.S. As @Henry pointed out there is another combinatorial interpretation of the sequence, which is A084078 . It is the length of generating sequences, starting from $[0]$ (see it as a python list), and every time we substitute each $k$ the values $-|k+1|,-|k+1|+2,\dots,|k-1|$ . For example, initial is $[0]$ , and the range for $0$ is $[-1,1]$ , so $[0]$ becomes $[-1,1]$ . Now for $-1$ the range is $[0,2]$ and for $1$ it is $[-2,0]$ , so the sequence becomes $[0,2,-2,0]$ . Then the sequence becomes $[-1,1,-3,-1,1,-1,1,3,-1,1]$ , and so on. Now the question becomes more complecated... is there a (bijective, hopefully) proof for proving the number of these three items to be equal?",['combinatorics']
4801389,Does the law of truly large numbers/infinite monkey theorem hold with an ever decreasing probability?,"Recently I've been considering a game (I'm unsure if it has a proper name and is an already studied problem), and it proceeds as follows. Flip a coin, selecting a value of either 0 or 1. Append the result to the sequence of past flips. If the last flip has marked the repetition of the past history of the sequence (i.e. the first half of the sequence now equals the second half), the game terminates. Otherwise, keep playing the game. Some example runs where the game terminates might be 11, or 00, or 101101, or 1001110011, etc. But the longer sequences get exponentially less likely to be repeated, for instance had the last example ended instead with a 0, we would then need to just by chance get the precise sequence of subsequent flips 1001110010 to form 1001110010[|]1001110010. Thus after a certain point, it seems as though we are bound to keep failing forever, eternally making the required sequence we must repeat to finish the game longer and longer. And running some basic simulations of 1000 different random seeds for this game reveals a curious distribution of game lengths: Game Length Occurrences (/1000) 2 521 4 127 6 59 8 15 10 13 12 3 14 1 16 2 18 1 22 1 > 1,000,000 257 Here the final category the game was manually terminated after iterating beyond 1 million in length (as these sequences seem to run forever or for a very long time). The rarest game lengths are actually the intermediate length games, with a decent portion falling into this exceedingly long case instead. This pattern is interesting to me, and my question ultimately is, do these sequences truly run forever ? Past a certain point does this probabilistic game become truly hopeless, even though at each step as the sequence gets longer there is a non-zero probability of the past history of the sequence happening to exactly reoccur? I liken this game by analogy to moving twice as far away from a quantum tunnelling particle every time it fails to tunnel to your location, which also seems to imply it would never catch you even given infinite time. Or perhaps if the works of Shakespeare expanded by an additional character with each wrong keypress by the monkey at the typewriter, multiplying the difficulty. But at the same time this logic contrasts with my intuition about the law of truly large numbers, which says given infinite time any events with non-zero probability will eventually happen. So how is the law affected in the case where the probability in question is not constant, but eternally decreasing (halving with each failure in this case)? Edit: As an interesting aside, if we instead roll a 10 sided die, the result from the long runs becomes a sequence of random digits, e.g. 943669542398...; and I notice by definition this sequence avoids repetitions, and so if we put a decimal point before it, 0.943669542398... for instance, we are essentially constructing an irrational number. The distribution also shifts such that the vast majority of runs end up in the long seemingly endless case.","['binary', 'probability', 'sequences-and-series']"
4801391,"When does a modified distributive law, like $a\cdot(b+c)=(a\cdot a)+(b\cdot c)$, allow polynomials to be written in the standard form?","Suppose we have two unary operations $f,g$ , related by the law $$\forall a,\quad g(f(a))=f(f(g(g(a)))),$$ that is, $gf=ffgg$ . We might then wonder whether any expression involving these operations can be written with all $f$ 's on the left and all $g$ 's on the right. Let's try an example: $$gff$$ $$=(ffgg)f$$ $$=ffg(gf)$$ $$=ffg(ffgg)$$ $$=ff(gff)gg$$ $$=ff\big(ff(gff)gg\big)gg$$ $$=ff\Big(ff\big(ff(gff)gg\big)gg\Big)gg$$ Evidently, applying that law can never get rid of the $gff$ in the middle. On the other hand, if the law is $gf=f^mg^n$ where either $m\leq1$ or $n\leq1$ , then applying it to any expression does eventually end with all $f$ 's on the left and all $g$ 's on the right. Suppose we have two (commutative, associative) binary operations $\oplus,\odot$ , related by the law $$a\odot(b\oplus c)=(a\odot a)\oplus(b\odot c).$$ As before, and in analogy with the ordinary distributive law, we might wonder whether any expression involving these operations can be written in the standard form (i.e. with multiplication inside and addition outside of parentheses). As before, the answer is negative; there's another self-replicating expression: $$(a\oplus b)\odot(a\oplus b)$$ $$=\big((a\oplus b)\odot(a\oplus b)\big)\oplus(a\odot b)$$ So what about other modifications of the distributive law, $$a\odot(b\oplus c)=P(a,b,c)$$ where $P$ is a polynomial in the standard form? What conditions on $P$ ensure that any expression can also be written in the standard form?","['universal-algebra', 'binary-operations', 'abstract-algebra', 'polynomials']"
4801422,What projections of convex bodies can tell us about their volumes,"Let $K,L$ be two origin-symmetric, convex bodies in $\mathbb{R}^3$ . Let $\xi\in\mathbb{S}^2$ be a vector on the unit sphere in $\mathbb{R}^3$ .
Denote $\xi^{\perp}$ the subspace in $\mathbb{R}^3$ orthogonal to $\xi$ . Denote $K\mid_{\xi^{\perp}}$ as the projection of the body $K$ onto the subspace $\xi^{\perp}$ . Same for $L\mid_{\xi^{\perp}}$ . Suppose that for any vector $\xi$ , there exists a rotation $\phi_{\xi}$ of $K\mid_{\xi^{\perp}}$ such that $\phi_{\xi}(K\mid_{\xi^{\perp}})\subset L\mid_{\xi^{\perp}}$ . This rotation is taken in the $2$ -dimensional sense on the plane $\xi^{\perp}$ . Does it follow that the volume of $K$ is less than or equal to the volume of $L$ ?","['convex-analysis', 'geometry', 'real-analysis']"
4801426,$\lim_{n\to+\infty}\int_0^1\cdots\int_0^1\frac{x_1^q+x_2^q+\cdots+x_n^q}{x_1^p+x_2^p+\cdots+x_n^p}dx_1dx_2\cdots dx_n=\frac{p+1}{q+1}$,"I found this on this website: Convergence in probability: $\lim_{n\to\infty}\int_0^1\cdots\int_0^1\frac{x_1^2+x_2^2+\cdots+x_n^2}{x_1+x_2+\cdots +x_n}dx_1\cdots dx_n=\frac23$ $\lim_{n\to\infty} \underbrace{\int_{0}^{1}\cdots \int_{0}^{1}}_{n}\frac{x_1^{505}+\cdots +x_n^{505}}{x_1^{2020}+\cdots +x_n^{2020}}dx_1\cdots dx_n$ I try to generalize its conclusions: I believe: with $q>p>0$ , $$
\lim_{n\to+\infty}\int_0^1\cdots\int_0^1\frac{x_1^q+x_2^q+\cdots+x_n^q}{x_1^p+x_2^p+\cdots+x_n^p}dx_1dx_2\cdots dx_n=\frac{p+1}{q+1}
$$ The answers below this page give perfect proof. We can continue to promote it. With $f\in[0,1]$ , $g\in[0,1]$ , $\exists C>0 ,\forall x\in[0,1]$ , $0<f(x)<Cg(x)<\infty$ and $\int_0^1g(x)dx<\infty$ $$
\lim_{n\to+\infty}\int_0^1\cdots\int_0^1\frac{\sum_{i=1}^{n}f(x_i)}{\sum_{i=1}^{n}g(x_i)}dx_1dx_2\cdots dx_n=\frac{\int_0^1f(x)dx}{\int_0^1g(x)dx}
$$ Proof of this conclusion I find it here today. The proof about $\lim_{n\rightarrow\infty}\int _0^1\cdots\int _0^1 \frac{\sum_{i=1}^n f(x_i)}{\sum_{i=1}^n g(x_i)}dx_1\cdots dx_n$ Well, this is over, and you can use this as a navigation transit page.","['integration', 'limits', 'calculus']"
4801427,Is there really no family of humongous sets?,"I've previously tried to define a meaningful notion of size for sets with this question . It turns out that the notion I tried to define was more then related to the one of measurable cardinals , which can not be proven to exist in ZFC. I'm now here to ask for what I hope to be an weaker version of this notion which was motivated by the previous one. Let $R$ be a set of size $\mathfrak c=2^{\aleph_0}$ and say $\mathcal F\subset \mathcal P(R)$ is a family of humongous sets if it has the two following properties: If $P = \{P_n: n\in\mathbb N\}$ is a countable chain ( $P_0\subset P_1\subset P_2\subset\dots$ ) such that $\displaystyle \bigcup_{n\in\mathbb N} P_n = R$ , then $P\cap\mathcal F\ne\varnothing$ , i.e., there is an humongous set in $P$ . If $\{H_n: n\in\mathbb N\}\subset\mathcal F$ is a countable family of humongous sets, then $\left|\displaystyle\bigcap_{n\in\mathbb N} H_n\right| = \mathfrak c$ . I wonder if one can find a family of humongous sets. The difference from my previous question is the change of $P$ from a partition to a chain. With a partition, by taking $R:=\mathbb R$ , one could force intervals of arbitrarily small length to be humongous, which would contradict the intersection property. With the change for a chain I hope to make this no longer possible.","['measure-theory', 'cardinals', 'set-theory']"
4801473,$\lim_{n\to\infty}\sqrt[n]{\int_0^1x^{\frac{n(n+1)}{2}}(1-x)(1-x^2)\cdots(1-x^n)dx}=\frac{1}{4}$,"$$
\lim_{n\to\infty}\sqrt[n]{\int_0^1x^{\frac{n(n+1)}{2}}(1-x)(1-x^2)\cdots(1-x^n)dx}
$$ $$
=\lim_{n\to\infty}\sum_{i=1}^n\frac{1}{4i(i+1)}
$$ $$
=\lim_{n\to\infty}\sum_{i=1}^n\frac{1}{4}(\frac{1}{i}-\frac{1}{i+1})
$$ $$
=\lim_{n\to\infty}\frac{1}{4}(1-\frac{1}{n+1})
$$ $$
=\lim_{n\to\infty}\frac{1}{4}(\frac{n}{n+1})
$$ $$
=\frac{1}{4}
$$ I found an answer, but I never understood where this first step came from.","['integration', 'limits', 'calculus']"
4801529,What will be the domain and range of $f(x)=\sqrt{\sin^{-1}({\frac{1}{1+3x+2x^{2}}})}$?,"If $f(x)=\sqrt{\sin^{-1}({\frac{1}{1+3x+2x^{2}}})}$ , then what will be the domain and range of $f(x)$ ? I am trying this question by finding the domain of $f(x)$ first. So, $2x^{2}+3x+1\neq0$ . Now $2x^{2}+3x+1=(2x+1)(x+1)$ . Therefore $x\neq-1/2,x\neq-1$ . Now lastly what I can guess is that $-1\leq\frac{1}{2x^{2}+3x+1}\leq1$ . But I can't proceed further. Please help me out with this question.","['calculus', 'functions', 'relations']"
4801570,"Exercise 9, Section 2.5 of Hungerfordâ€™s Abstract Algebra","If $|G| = p^nq$ , with $p\gt q$ primes, then $G$ contains a unique normal subgroup of index $q$ . Why does author mention both unique and normal? Subgroup of index $q$ is Sylow $p$ -subgroup of $G$ . So uniqueness $\iff$ normality. Showing uniqueness is relatively easy compared to normality. Here is proof of above exercise showing uniqueness.","['group-theory', 'normal-subgroups', 'finite-groups', 'sylow-theory']"
4801574,"Prokhorov's Theorem, tightness implies convergence of a subsequence","I'm trying to apply Prokhorov's Theorem on the measure space $M$ , defined as the space of non-negative, finite measures on $[0,T] \times \mathbb R^d$ with the weak topology in the following sense: Given a tight sequence of probability measures $P_n$ on $M$ , not elements of $M$ but rather probability measures over $M$ , can I conclude that there is a weakly convergent subsequence? Do I need $M$ to be complete and separable?","['general-topology', 'probability-theory', 'functional-analysis', 'measure-theory']"
4801584,Generalization of Euler $R\ge2r$,"Let $r$ be the inradius of $\triangle ABC$ , prove that for any point $P$ , $$(PA+PB+PC)(PA\cdot PB+PB\cdot PC+PC\cdot PA)\ge72r^3.$$ Notice that when $PA=PB=PC$ we get Euler inequality $R\ge2r$ . I thought of using the coordinate system. Let $I~(0,1)$ be the incenter, $B~(b,0)$ and $C~(c,0)$ . So we can get $A~\left(\frac{b+c}{bc+1},\frac{2bc}{bc+1}\right)$ and $r=1$ . Let $P~(s,t)$ . We can express the inequality algebraically but there are too many square roots. ""Source"" of this problem: firstly to generalize Euler inequality, I conjectured that $\sum PA\ge6r$ , which is true since we only need to consider the Fermat point. Then I considered other symmetric polynomials about $PA$ , $PB$ , $PC$ . After a few attempts with geogebra, I made the inequality.","['inequality', 'geometry']"
4801588,imaginary part of the multiplication of complex numbers,"I read a formula in a book about complex analysis: $$\text{Im}(\prod(1+ia_k))=0$$ is equivalent to $$\sum \arctan a_k=0 \pmod  \pi$$ I calculated cases $n=2,3$ , but I don't think direct calculation is a right way. Is there any quick idea to prove it? Thank you",['complex-analysis']
4801656,"Show that $\bigcup_{n \in \mathbb{N}}[\frac{1}{n},1]=(0,1]$","Show that $\bigcup_{n \in \mathbb{N}}[\frac{1}{n},1]=(0,1]$ . I struggle a bit with ""mathematical reasoning"" it would seem, but since everything in life is a matter of experience, I seek advice too check if this is OK! This tasks are trivial, but proof writing is perhaps not so easy... My attempt: Since $\frac{1}{N}>0$ for all $N \in \mathbb{N}$ , $\bigcup_{n \in \mathbb{N}}[\frac{1}{n},1] \subset (0,1]$ . Since $\frac{1}{n}\rightarrow 0$ as $n\rightarrow \infty$ but $\frac{1}{N}\neq 0$ for all $N \in \mathbb{N}$ , $(0,1] \subset \bigcup_{n \in \mathbb{N}}[\frac{1}{n},1]$ Thus $\bigcup_{n \in \mathbb{N}}[\frac{1}{n},1]=(0,1]$ .","['elementary-set-theory', 'solution-verification']"
4801670,Inverse morphism of group object is iso,"Let $C$ be a category with finite products (and a terminal object). We can then define group objects as tuples $(G, m, e, inv)$ by requiring that the usual diagrams commute. Is it true that the morphism $inv : G \to G$ is always an isomorphism? It is certainly true for e.g. $C = \mathrm{Set}$ , $C=\mathrm{Grp}$ , $C=\mathrm{Top}$ . I would, in fact like to prove that $inv \circ inv = 1_G$ .
If it is true, the proof should be simple diagram chasing - but I have not been able to deduce the statement. Could you please provide a proof, hint, or reference?","['morphism', 'group-theory', 'category-theory']"
4801689,"Consider the limit $\lim_{(x,y)\to(0,0)}\frac{y^4}{x^2+y^4}.$ Where did I mistake?","Determine whether the limit $$\lim_{(x,y)\to(0,0)}\quad \frac{y^4}{x^2+y^4}.$$ If $(x,y)$ approaches $(0,0)$ satisfying $x=y^2$ , then $\frac{y^4}{x^2+y^4}=\frac{1}{2}\to \frac{1}{2}$ , whilst if $(x,y)$ approaches $(0,0)$ satisfying $x=0$ , then $\frac{y^4}{x^2+y^4}=1\to 1$ . Thus the limit doesn't exist. However, if I consider the polar coodinates, then $$\frac{y^4}{x^2+y^4}=\frac{r^4\sin^4 t}{r^2\cos^2 t+r^4\sin^4 t}
=\frac{r^2\sin^4 t}{\cos^2 t+r^2\sin^4 t}
\to 0$$ as $r\to 0.$ Thus the limit is $0$ ? There should be a mistake somewhere but I cannot find. Where did I mistake ?","['multivariable-calculus', 'limits', 'calculus']"
4801783,What is the fundamental property of the Fourier operator?,"It is well-known that the Fourier transform satisfies the following translation property: $$[\mathcal{F}(\mathcal{T}_hf)](\xi) = \text{e}^{2 \pi \text{i} \xi h} \cdot [\mathcal{F}(f)](\xi),$$ where $\mathcal{T}_h$ is the translation operator defined by $f(x) \mapsto f(x + h)$ . By further defining the derivative operator as $$\mathcal{D} = \lim_{h \to 0} \frac{\mathcal{T}_h - \mathcal{I}}{h},$$ one can show that $$
[\mathcal{F}(\mathcal{Df})](\xi) = 2 \pi \text{i} \xi \cdot [\mathcal{F}(f)](\xi)
$$ by linearity and continuity arguments. I'm wondering if it's also possible to prove the convolution theorem, just using the translation property of the Fourier transform? I know that in the finite dimensional case, a (circluar) convolution can be represented by a circulant matrix, which can in turn be written as a polynomial in simple translation matrices. More generally, my question is: Is the fundamental property of the Fourier operator that it diagonalizes translations? That is, if we define $(\mathcal M_h \varphi)(\xi) = \text{e}^{2 \pi \text{i} \xi h} \cdot \varphi(\xi)$ , is the Fourier operator $\mathcal{F}$ fundamentally characterized by $$
\mathcal{F} \circ \mathcal{T}_h = \mathcal M_h \circ \mathcal{F} \ ?
$$","['fourier-analysis', 'operator-theory', 'linear-algebra', 'functional-analysis', 'diagonalization']"
4801801,Supremum of a bounded set of real-valued functions in $L^\infty(X)$,"Let $(X,\Sigma,\mu)$ be a measure space and consider the algebra $L^\infty(X)$ . The real-valued functions carry a canonical order, where $g\leq f$ if $g(x)\leq f(x)$ a.e. The following result holds: Let $B\subset L^\infty(X)$ be a  set of real valued functions which is bounded (that is, there exists $c>0$ with $\|f\|_\infty\leq c$ for all $f\in B$ ). Then $B$ admits a least upper bound $g$ in $L^\infty(X)$ . As far as I can tell, the straightforward argument that one first thinks about does not work; because one would attempt to take the pointwise supremum, which of course will be bounded but there is no obvious reason why it has to be measurable. Moreover, taking the pointwise supremum raises the issue of dealing with possibly uncountably many nullsets, which kind of puts the argument away from usual measure theory. The reason I know the result is true is the following. For any $F\subset B$ finite, we can define $g_F=\max\{f:\ f\in F\}$ . Because we are dealing with finitely many functions, $g_F\in L^\infty(X)$ and $f\leq g_F$ a.e. for all $f\in F$ . Now the net $\{g_F\}_{F\in B}$ is a monotone increasing bounded net of real-valued functions. Here is where I need to appeal to higher level stuff: since $L^\infty(X)$ is a von Neumann algebra, any bounded monotone increasing net of selfadjoint elements admits a least upper bound.  So there exists $g\in L^\infty(X)$ that is the least upper bound of $\{g_F\}_{F\in B}$ . The reason $g$ is a least upper bound for $B$ is that if $h$ is an upper bound for $B$ , then $g_F\leq h$ for all $F$ , and then $g\leq h$ . And now we get to the question: is it possible to prove the existence of the least upper bound for $B$ , with measure theory arguments? The proof using von Neumann algebra ideas is not crazy hard, but it uses non-trivial results. Basically one represents $L^\infty(X)$ faithfully on $B(\ell^2(X))$ acting by left multiplication, that is $\pi(f)\eta=f\,\eta$ , and considers the numerical nets $\{\langle \pi(g_F)\xi,\xi\rangle\}$ for each $\xi\in \ell^2(X)$ . As these are bounded monotone nets in $\mathbb R$ , they converge. This way one gets numbers $\{\langle T\xi,\xi\rangle\}$ and it is standard Hilbert space theory using polarization that this defines an operator $T\in B(\ell^2(X))$ . The operator $T$ is a weak-operator limit of elements of $\pi(L^\infty(X))$ , which is a von Neumann algebra, and so $T\in \pi(L^\infty(X))$ , which means that there exists $g\in L^\infty(X)$ with $\pi(g)=T$ . It would be nice to have instead a proof intrinsic to $L^\infty(X)$ .","['von-neumann-algebras', 'measure-theory', 'operator-algebras', 'operator-theory', 'measurable-functions']"
4801814,Understanding Vladimir Maz'ya's Problem 72 what is $|dy|$?,"In the article "" Seventy Five (Thousand) Unsolved Problems in Analysis and Partial Differential Equations "" by Vladimir Mazâ€™ya Problem 72 on page 36 is given as follows: Let $C$ be the unit circle let $u(x)$ be a function on the unit circle. Define the operator $A$ as follows $$ A(u)(x) = \int_{C} \frac{u(x)-u(y)}{|x-y|} |dy| $$ The spectrum of A is described in [79, Section 12.2.2].
Then the principal cauchy problem: $$  \frac{d}{dt} + A, t> 0 $$ Which is reminiscent of the modified zeta function $$ f(z) = \sum_{k=1}^{\infty} e^{-z \sum_{n=1}^{k} \frac{1}{n}} $$ Admits a meromorphic extension to the entire complex plane. Study the properties of this extension. So I have a couple questions here. What is $|dy|$ ? The only way I can try to make sense of this is that the sign of the integral should cancel out the sign of $dy$ for every point on the unit circle. I.E. this is the same as: $$ A(u)(x) = \int_{y \in C} \frac{u(x)-u(y)}{y|x-y|}  $$ But if that was true the author would've just gone ahead and said that. So I think I don't understand what $|dy|$ is supposed to be. The author makes a mention that this is ""reminiscent of the modified zeta function"" but not equal to. I assume this means the modified zeta function's meromorphic continuation is known? Is there a reference for this and or an explicit construction? The Reference [79] from the article is: Mazâ€™ya, V., Nazarov, S., Plamenevskij, B.: Asymptotic Theory of Elliptic Baundary Value Problems in Singularly Perturbed Domains, vol. I.
BirkhÂ¨auser, Basel (2000) And I cannot get a hold of this article at this time. Some More Notes: Using approach0 I found this question where again a line integral on the unit circle is occuring and the symbol $|dz|$ indicates the line element here. So perhaps we need not know what $|dy|$ means at all to make sense of this (just consider it as a line integral over $C$ ). To be honest I'm still dissatisfied since usually with line elements $dl$ it is usually possible to expand $dl = a(x)dy + b(y)dy$ . In this particular case I'm still not sure how to crack open $|dz| = ?dx + ?dy$","['complex-analysis', 'analytic-continuation', 'partial-differential-equations']"
4801829,Properties of $\Gamma$ regarding $\operatorname{Hom}$ and $\otimes$.,"In Tu's book on differential geometry he defines a connection on a smooth vector bundle $E \to M$ as $\nabla : \mathfrak{X}(M) \times \Gamma(E) \to \Gamma(E)$ so that $\nabla$ is $C^\infty$ -linear on $X \in \mathfrak{X}(M)$ and $\Bbb R$ -linear on $s \in \Gamma(E)$ . On the other hand wikipedia defines a connection on a vector bundle as $\nabla : \Gamma(E) \to \Gamma(T^*M \otimes E)$ such that it satisfies the Leibniz rule. This question is more about the properties of $\Gamma$ and $\otimes$ than it is about connections, but I would be interested in understanding how I can go from $\nabla : \Gamma(E) \to \Gamma(T^*M \otimes E)$ to the one given by Tu? Do I need that $T^*M\otimes E \cong \mathrm{Hom}(TM,E)$ and is there some property that says something about the Hom functor or the tensor product under $\Gamma$ ?",['differential-geometry']
4801894,Sum of Rows of Vandermonde Matrix,"Question Consider some Vandermonde matrix defined by $$
V = \begin{bmatrix}
1 & x_1 &x_1^2& ... & x_1^{M-1}\\
1 & x_2 &x_2^2& ... & x_2^{M-1}\\
\vdots & \vdots & \vdots && \vdots\\
1 & x_M &x_M^2& ... & x_M^{M-1}\\
\end{bmatrix}
$$ for some $\{x_i :i = 1, 2, ..., M \land x_i \in \mathbb{R}\}$ . Numerically, I have observed that the sum of the rows of the inverse may have an interesting property: $$
\sum_k (V^{-1})_{i,k} = \delta_{i,0}.
$$ That is, this sum is equal to one for the first row of $V^{-1}$ and zero for all others. Is there any way for me to demonstrate this property? Attempts I began with elementwise analytical expressions for the inverse of the Vandermonde matrix but was unable to simplify my expressions. Inspired by this question , I used expressions for the sums of a truncated geometric series and obtained the expression: $$
\sum_k V^{-1}_{i,k}\left(\frac{1 - x_k^M}{1-x_k} \right) = 1
$$ But am unable to see a way forward. Finally, I have found one proof which is necessary for $\sum_k (V^{-1})_{i,k} = \delta_{i,0}$ but certainly not sufficient.","['matrices', 'summation', 'proof-writing', 'linear-algebra']"
4801926,Continuous vs holomorphic functional calculus,"Let $A$ be a bounded self-adjoint operator on a Hilbert space. Then, it has a continuous functional calculus, which I can think of as arising from the fact that polynomials are dense in $\mathcal{C}(\sigma(A))$ . It also has a holomorphic functional calculus. Are these the same? On the one hand, any holomorphic function on a neighborhood of $\sigma(A)$ is certainly continuous on $\sigma(A)$ and the two calculi agree in this case, thanks to Runge's theorem, as discussed at Why does the image of continuous functional calculus coincide with the holomorphic functional calculus when considering holomorphic functions? . On the other hand, any uniform limit of polynomials (or any holomorphic functions) is holomorphic. This seems to suggest that the holomorphic and continuous functional calculi are valid for the exact same classes of functions. Is this correct? Well, clearly it isn't, since there are plenty of continuous functions of one variable which aren't even differentiable, so they certainly don't admit holomorphic extensions on any neighborhood. I suspect that the issue in the last paragraph is that one needs to be careful with domains. Namely, maybe there is a sequence of polynomials which is uniformly convergent on $\sigma(A)$ but not on an open neighborhood thereof, and so the continuous functional calculus could apply while the holomorphic one does not? Finally, if this is the correct reason why the continuous functional calculus is more general, then does this give an easy way to generalize the holomorphic calculus for any bounded operator? Namely, we get an isometry from the closure of $\mathcal{R}_A$ in $\mathcal{C}(\sigma(A))$ to $\mathcal{B}(V)$ , where $\mathcal{R}_A$ is the space of rational functions whose poles are in $\rho(A)$ , which by Runge's theorem accounts for the holomorphic functional calculus but again probably gets a bit more? Thanks in advance!","['complex-analysis', 'spectral-theory', 'functional-analysis']"
4801935,Converting $68^8$ microseconds to years without calculator,"The following problem is easy to solve with the help of a calculator. However, when doing it manually, we are faced with a very large exponentiation ( $68^8$ ) and which, at first glance, does not seem possible to be manipulated together with the remaining data of the problem to find a simplified final result. The problem follows: Suppose a password for a computer system must be exactly $8$ characters long, where each character in the password is either a lowercase letter, an uppercase letter, a decimal digit, or one of six special characters ( *><!+= ). If it takes a microsecond ( $10^{-6}$ seconds) for a hacker to check if a password is correct, how many years would it take for the hacker to try every possible password? What I've tried:
Using a calculator, I first calculate the number of possible passwords, which is $68^8$ . Then I multiply it by the time it takes for the hacker to test each one, so it becomes $68^8\times10^{-6}$ . To convert the result into years, I first convert it into minutes by dividing by $60$ , then into hours by dividing by $60$ again, then I convert it into days by dividing by $24$ , and finally into years by dividing by $365$ . It gives a little more than $14$ years. I have tried to simplify the expression $$\frac{68^8}{10^6\cdot 60^2\cdot 24\cdot 365}$$ that gives the answer of the question, but even finding some common factors between numerator and denominator, this doesn't help too much, because $17$ is a prime factor of $68$ , and I don't think it appears as a factor of anything in the denominator. Also, I would like to do that in a efficient way, in a short time, without spending too much paper. Edit.: We have noticed that a aproximated answer is the better option here. I think it is suficient to get the integer part of the aumont of years.","['combinatorics', 'estimation']"
4801989,How to show this perturbed matrix also has negative real parts for $\delta$ small enough,"Based on this question: Show that the $x_\delta$ is asymptotically stable of the following system ODE . Consider the following nonlinear system of ODE $$
x'(t)=f(x(t))+\delta g(x(t)), x\in R^N
$$ where $f$ and $g$ are two smooth vector fields and $\delta$ is a parameter. Suppose that as $\delta=0$ , the system has a hyperbolic and asymptotically stable equilibrium $x_0$ . That means eigenvalues of its linearization have negative real parts. By implicit function theorem with $\phi(\delta,x) = f(x)+ \delta g(x)$ shows that for small $\delta$ the perturbed system has an equilibrium point $x_\delta$ , and $\lim_{\delta \to 0} x_\delta = x_0$ . Show that for small(er) $\delta$ , the eigenvalues of $A_\delta:={\partial \phi(\delta, x_\delta) \over \partial x}$ also have negative real parts. I am confused about how to show this perturbed matrix also has negative real parts for $\delta$ small enough. We know that as $\delta=0$ , the matrix $$A=\lim_{\delta\to 0}A_\delta=\frac{\partial f(x_0)}{\partial x}$$ has negative real parts. It seems one problem from Linear algebra...","['linear-algebra', 'ordinary-differential-equations']"
4801998,"What is the minimum convergence value, for $x>1$, of $\sum_{n=1}^\infty \frac{n^x}{x^n}$","I am a Calculus 2 student in college and we are learning about the tests for series convergence. One specific question caught my attention. We were to find if the following series is convergent or divergent: $$\sum_{n=1}^\infty \frac{n^9}{9^n}.$$ I found it to be convergent, but I wondered what it converged to (this was outside of the scope of my course) and by putting it into a calculator found it to be about 138.3667. I was curious about what if instead of 9, it were other numbers and made a table similar to this: $$f(x)=\sum_{n=1}^\infty \frac{n^x}{x^n}.$$ x   f(x)      
 -2  -0.44841  
 -1  -0.69265  
  0  Divergent 
  1  Divergent 
  2   6.00000   
  3   4.12500   
  4   4.69136   
  5   6.90430 I was curious about how the function has a minimum between 2 and 4 and used a graphing calculator, Desmos, to see what that minimum is when $x$ is about 3.12 with is converging to about 4.113 I wanted to know if there was a closed form equation to the series for which I could take the derivative and set it to zero. I put the series was put into WolframAlpha; it outputted $\sum_{n=1}^\infty \frac{n^x}{x^n} = \operatorname{Li}_{-x}(\frac{1}{x})$ when $|x|>1$ and $\operatorname{Li}_n(x)$ is the polylogarithm function. I realized I was getting over my head when I had no clue what a polylogarithm was and asked my Calc 2 instructor for help. Unfortunately, he said how he didnâ€™t specialize in this kind of math called Analytic Number Theory. He asked some of his colleagues and none that heâ€™d asked knew much about how to handle this. The only way I thing that was figured out was the derivative of the polylogarithm but for orders of numbers with absolute value less than 1; explicitly where the function diverged. He got back to me a few days later and recommend to ask the question on Stack Exchange (wow, thatâ€™s here!). We both donâ€™t even know if taking the derivative of the polylogarithm is the way to go. Nor do we know of any other closed form equations for the series. I know I can just simply get arbitrarily close enough by computer algorithms to find when the derivative of the original function, $$\sum_{n=1}^\infty \frac{n^x}{x^n},$$ is 0, but I would really like to have an exact solution.","['number-theory', 'sequences-and-series', 'real-analysis']"
