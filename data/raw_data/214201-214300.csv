question_id,title,body,tags
4340166,Prove Implicit Function Theorem directly from Constant Rank Theorem,"For reference: ( $\textbf{Constant Rank Theorem}$ ) Suppose $U_0\subset\mathbb{R}^m$ is open and $F:U_0\rightarrow \mathbb{R}^n$ is a $C^r$ map with constant rank $k$ (that is, its Jacobian matrix has constant rank $k$ on $U_0$ ). Then for any $p\in U_0$ there exist $C^r$ charts $(U,\phi)$ for $\mathbb{R}^m$ and $(V,\psi)$ for $\mathbb{R}^n$ , with $p\in U$ and $F(U)\subset V$ , such that $$\psi\circ F\circ \phi^{-1}(u_1,\ldots,u_k,u_{k+1},\ldots,u_m)=(u_1,\ldots,u_k,0,\ldots,0)$$ ( $\textbf{Implicit Function Theorem}$ ) Suppose $m>n$ , $U_0\subset\mathbb{R}^m$ is open and $F:U_0\rightarrow \mathbb{R}^n$ is a $C^r$ map. Let $p=(a,b)\in U_0$ , $a\in\mathbb{R}^n$ , $b\in\mathbb{R}^{m-n}$ , with $F(p)=q$ , and let the Jacobian matrix of $F$ with respect to the first $n$ coordinates of $\mathbb{R}^m$ be invertible at $p$ . Then there exist neighborhoods $N_a\subset\mathbb{R}^n$ of $a$ and $N_b\subset\mathbb{R}^{m-n}$ of $b$ and a $C^r$ function $h:N_b\rightarrow N_a$ that assigns to each $y\in N_b$ the unique $x\in N_a$ satisfying $F(x,y)=q$ . ( $\textbf{Inverse Function Theorem}$ ) Suppose $U_0\subset\mathbb{R}^n$ is open and $F:U_0\rightarrow \mathbb{R}^n$ is a $C^r$ map. Let $p\in U_0$ and let the Jacobian matrix of $F$ be invertible at $p$ . Then there exist neighborhoods $U\subset\mathbb{R}^n$ of $p$ and $V\subset\mathbb{R}^{n}$ of $F(p)$ such that $F|_U:U\rightarrow V$ is a $C^r$ diffeomorphism. I understand how to derive the Inverse Function Theorem from the Constant Rank Theorem: the rank of $F$ stays maximal on some neighborhood of $p$ . Then, applying the Constant Rank Theorem, $\psi\circ F\circ \phi^{-1}$ becomes the identity on $\phi(U)$ and we get $F|_U:U\rightarrow F(U)$ is a $C^r$ diffeomorphism, with $F(U)\subset\mathbb{R}^n$ open. My question is: how do we derive the Implicit Function Theorem directly from the Constant Rank Theorem? I do not want to go Constant Rank => Inverse => Implicit Function, since that is well-known. I know that the Jacobian matrix of $F$ with respect to the first $n$ coordinates of $\mathbb{R}^m$ stays invertible around $p$ (constant rank $k=n$ ), and applying the Constant Rank Thm. gives $$\psi\circ F\circ \phi^{-1}(u_1,\ldots,u_n,u_{n+1},\ldots,u_m)=(u_1,\ldots,u_n),$$ a projection map onto the first $n$ coordinates. But I don't see how to go from there to arrive at the implicit function $h$ . Thanks a lot!","['manifolds', 'inverse-function-theorem', 'implicit-function-theorem', 'real-analysis']"
4340179,$x^2y''+(3x^2+4x)y'+2(x^2+3x+1)y=0$,$x^2y''+(3x^2+4x)y'+2(x^2+3x+1)y=0$ I try to solve this equation by finding $\mu$ such that the equation become exact.( I know there are other ways for solving this equation). $(\mu y')'=\mu'y'+\mu y''=\mu y''+\mu\frac{3x^{2}+4x}{x^{2}}y'+\mu\frac{2(x^{2}+3x+1)}{x^{2}}y \implies$ $\mu'y'=\mu\frac{3x^{2}+4x}{x^{2}}y'+\mu\frac{2(x^{2}+3x+1)}{x^{2}}y \implies \frac{\mu'}{\mu}=\frac{3x^{2}+4x}{x^{2}}+\frac{2(x^{2}+3x+1)}{x^{2}}\frac{y}{y'}$ How am I supposed to solve it? Thanks !,['ordinary-differential-equations']
4340206,"Ring homomorphisms between $C([0,1], \mathbb{R})$ and $C(\text{Cantor set},\mathbb{R})$","Let $K \subseteq [0,1]$ be the Cantor set. $C([0,1], \mathbb{R})$ be the ring of continuous functions from $[0,1]$ to $\mathbb{R}$ . $C(K,\mathbb{R})$ be the ring of continuous functions from $K$ to $\mathbb{R}$ . Then, what can we say about the homomorphisms from $C([0,1], \mathbb{R})$ to $C(K,\mathbb{R})$ ? How many of them are injective ? How many of them are surjective ?","['ring-theory', 'abstract-algebra', 'continuity', 'cantor-set']"
4340213,"""Statistical Inference"" telephone question, follow-up","I ran across the following problem in Casella and Berger's Statistical Inference (Q1.20, 2nd ed): My telephone rings 12 times each week, the calls being randomly distributed among the 7 days. What is the probability that I get at least one call each day? (Answer: .2285) This seems to be equivalent to putting 12 balls into 7 boxes so that there is at least 1 ball in each box. In that case, this should be a fairly straightforward selection-with-repetition problem: since we have at least 1 ball in each box, that means we must actually count the number of ways to put 5 balls into 7 boxes, and divide by the number of ways to put 12 balls into 7 boxes, which would give [(11 choose 6)/(18 choose 6)] or about 0.0249. I see an answer given here: Statistical Inference Question which approaches it from the bottom up rather than the top down. This method seems reasonable, and I've verified that it gives the authors' desired answer, but what's different about my approach? In both cases, the calls are unordered and identical, the days are ordered and distinct, and repetition is allowed.","['statistics', 'probability']"
4340237,$\sigma$-algebra generated by a subcollection,"Let $\mathcal{A}\subset \wp(X)$ and let $\mathfrak{S}(\mathcal{A})$ be the collection of all $\sigma$ -algebras over $X$ including $\mathcal{A}$ , therefore any $\sigma$ -algebra in $\Sigma \in \mathfrak{S}(\mathcal{A})$ is such that $ \mathcal{A} \subset \Sigma$ . Let $\sigma(\mathcal{A}) =\bigcap \mathfrak{S}(\mathcal{A})$ , that is the smallest $\sigma$ -algebra including $\mathcal{A}$ . This, of course, assumes that we have proved that the intersection of $\sigma$ -algebras is still a $\sigma$ -algebra. If $\mathcal{B}\subset \wp(X)$ is a superset of $\mathcal{A}$ , the inclusion holds for $\sigma()$ too, that is: $$ \mathcal{A} \subset \mathcal{B} \rightarrow  
\sigma(\mathcal{A}) \subset \sigma(\mathcal{B}) $$ To most books, this is trivial and not dealt with.
I understand that most of the sets in $\sigma(\mathcal{A})$ are also in $ \sigma(\mathcal{B}) $ , for example: Any element of $\mathcal{A}$ is  both in $\sigma(\mathcal{A})$ and $ \sigma(\mathcal{B})$ , because $ \mathcal{A} \subset \mathcal{B} \subset \sigma(\mathcal{B})$ . Countable sequences $A_1, \ldots, A_k \in \mathcal{A}$ have their intersections, unions, and complements, both in $\sigma(\mathcal{A})$ and $ \sigma(\mathcal{B})$ , because $A_1, \ldots, A_k \in \mathcal{B}$ too. However, (1) and (2) do not exhaust all the possible sets in $\sigma(\mathcal{A})$ , in fact, I can still reason about   intersections, unions and complements of the sets built in (2). I can analyse specific cases. For example, $(A_1 \cap A_2) \cap (A_3 \cap A_4) \in \sigma(\mathcal{A})$ , and also $$A_1, \ldots, A_4 \in \mathcal{B} 
\rightarrow 
   (A_1 \cap A_2), (A_3 \cap A_4) \in \sigma(\mathcal{B})
\rightarrow 
   (A_1 \cap A_2) \cap (A_3 \cap A_4) \in \sigma(\mathcal{B})
$$ All elements of $\sigma(\mathcal{A})$ I can think of  are  in $ \sigma(\mathcal{B})$ too, but the possibilities are endless, and  I cannot obtain a  proof. Note . This question is a generalisation of other questions, such as math.stackexchange.com/q/1667546/75616 , which refer to specific collections $\mathcal{A},\mathcal{B}$ and is about the  formulation of a formally correct proof.","['elementary-set-theory', 'measure-theory', 'proof-writing']"
4340244,Fundamental Group of the Long Line,"Let $\omega_1$ be the first uncountable ordinal. The long
line $L$ is defined as the cartesian product of the first
uncountable ordinal $ \omega _{1}$ with the half-open interval $ [0,1)$ equipped with the order topology that arises from
the lexicographical order on $\omega _{1} \times [0,1)$ . That means that of we
set by $""<_L""$ the lexicographic order on $\omega_1 \times [0,1)$ then the topology on $\omega_1 \times [0,1)$ is generated by ""interval"" base $$ ((k_1,a), (k_2,b))=\{(k,x) \in \omega_1 \times [0,1) \vert \ \vert  
(k_1,a) <_L (k,x) <_L (k_2,b)$$ My question is if it's true that the fundamental group $\pi(L,l_0)$ of the long
line is trivial (let $l_0$ is any base point) or what's the reason? Although it seems to be a rather
natural question I nowhere found a serious discussion about this problem. My
intuition says that it should be trivial, but I'm not not sure. My idea: I have to show that any loop $f: S^1 \to L$ is contractible.
My idea is to observe that one can find a finite number of ""neighboured"" $a_1, a_2,..., a_n \in \omega_1$ and show that the image
of $f$ is contained in $\bigcup_{i=1}^n {a_i} \times [0,1) \subset L$ and
then because of finiteness of $a_1, a_2,..., a_n \in \omega_1$ one sees that $\bigcup_{i=1}^n \{a_i\} \times [0,1) \cong [0,1)$ and this space is contractible. Is the argument ok? My worries are based on my doubts if that's make
sense to say that we can fetch some ""neighboured"" $a_1, a_2,..., a_n \in \omega_1$ .
What should ""neighboured"" here mean? If we believe in continuum hypothesis
then $\omega_1= \mathbb{R}$ and it is strage to say what does it mean that
two elements $a, b \in \mathbb{R}$ a ""neighboured"". If $a \neq b$ there is always
some other element between them, therefore I doubt that my argument above should
work, or not? note that we were done if we could show that every loop $f: S^1 \to L$ must be contained in a subset of the form $\{r\} \times [0,1) \subset L$ . This one is clearly contractible, so the loop would be also contractible. Is it possible to do it that way around? Does somebody see how can I argue to conclude that the fundamental
group of $L$ is trivial (if that's really the case)?","['general-topology', 'ordinals', 'fundamental-groups']"
4340246,Show that $g$ is $T$-measurable iff $g\phi=g \circ \phi$ is $\Sigma$-measurable?,"I suspect that proving what I wish to prove is not so very tough, but I haven't succeeded yet. I am pushing around sets between three spaces using two functions and their composition. I did some poking around old questions, feeling that this must have been answered already, but I didn't find much besides this (which I do use) and this other one (which I do not think will help). Proposition . Let $\phi:X \to Y$ , and let $(X,\Sigma)$ be a measurable space. We can obtain a related measurable space $(Y,T)$ by defining $T=\{F\subset Y:\phi^{-1}F\in \Sigma\}.$ Also fix a function $g:\text{dom}(g)\to\mathbb{R}$ with $\text{dom}(g)\subset Y.$ Show that $g$ is $T$ -measurable iff $g\phi=g \circ \phi$ is $\Sigma$ -measurable. I have already showed the ""only if"" part, which is straight-forward: Proof. Fix any half-line $H=H_a=\{t:t<a\}$ for $a$ real. We need to show $(g\phi)^{-1}H$ is an element of the subspace sigma-algebra: $$\Sigma_{\phi^{-1}\text{dom}(g)} = \Sigma_{\text{dom}(g\phi)}.$$ We note that $(g\phi)^{-1}H={\phi}^{-1}(g^{-1}H)$ , and that $g^{-1}H$ belongs (by hypothesis) to $$T_{\text{dom}(g)},$$ and hence $g^{-1}H=\text{dom}(g) \cap A$ , for some $A$ in $T$ . Finally, $$(g\phi)^{-1}H={\phi}^{-1}(\text{dom}(g) \cap A)={\phi}^{-1}\text{dom}(g) \;\cap\; {\phi}^{-1}A=\text{dom}(g\phi) \;\cap\; {\phi}^{-1}A,$$ with ${\phi}^{-1}A\in\Sigma.$ This means the composition is a measurable function. We needed to swap the operations $\phi^{-1}$ and $\cap$ to make it work, but apart from that the proof is mainly applying the definition. But for the converse statement, I stalled with trying this approach. You can aim to prove $g^{-1}H \in T_{\text{dom}(g)}$ , knowing that $A$ belongs to $\Sigma$ and $$\phi^{-1}(g^{-1}H)=(\phi^{-1}\text{dom}(g))\cap A,$$ but the forward mapping $\phi$ on sets only cooperates fully with union. Some of the ideas I have tried: We can also pull back a sigma-algebra under a mapping. For example, if $\mathcal{B}$ is the Borel algebra of real sets, we can pull it back to a sigma-algebra of subsets of the domain of $g$ ; equally well we can pull it back to a sigma-algebra of subsets of the domain of $g\phi$ . We aim for $g^{-1}H \in T_{\text{dom}(g)}$ for any half-line. We can note the set of all real sets satisfying this is a sigma-algebra: $\{E\subset\mathbb{R}: g^{-1}E \in T_{\text{dom}(g)}\}$ It might simplify matters to consider the restriction of $\phi$ to the domain of $g\phi$ , say $\phi_0: X_0 \to Y_0.$ Appendix: My attempts to find an existing answer: https://math.stackexchange.com/search?page=1&tab=Relevance&q=%5bmeasure-theory%5d%20%20%20function%20is%20measurable%20iff%20 https://approach0.xyz/search/?q=OR%20content%3Ameasure%2C%20OR%20content%3A%24f%5Ccirc%20g%24%2C%20OR%20content%3Ameasurable%2C%20OR%20content%3Aiff&p=1 https://math.stackexchange.com/search?page=1&tab=Relevance&q=%5bmeasure-theory%5d%20%20%20composition%20of%20functions%20","['measure-theory', 'measurable-functions', 'real-analysis']"
4340255,Find Spherical Integral: $\int_{\| {\bf x}\|=1} x_1 e^{ {\bf x}^T{\bf y}} {\rm d} {\bf x}$,"I am interested in the following integral: given a vector ${\bf y}  \in \mathbb{R}^n$ find \begin{align}
\int_{\| {\bf x}\|=1} x_1 e^{ {\bf x}^T{\bf y}} {\rm d} {\bf x}
\end{align} where $x_1$ denotes the first coordinate of ${\bf x}$ .  In other words, we are integrating over a sphere of unit sphere. After searching around I found that the following integral: \begin{align}
\int_{\| {\bf x}\|=1} e^{ {\bf x}^T{\bf y}} {\rm d} {\bf x}= \left( \frac{\|{\bf y}\|}{2} \right)^{1-\frac{n}{2}} S_{n-1} \Gamma(n/2) I_{n/2-1}(\| {\bf y} \|)
\end{align} where $S_{n-1}$ is the volume of $n-1$ -sphere and $I_v$ is modified Bessel function of the first kind. I think this result can be of use for us. In particular, I was thinking to integrate for $n-2$ sphere fist and then over $x_1$ coordinate","['integration', 'multivariable-calculus', 'bessel-functions']"
4340259,Are there a compact $K$ and an open $O$ such that $K\subseteq B\subseteq O$ and $\mu (O\setminus K)<\varepsilon $?,"I want to know if the following theorem is correct. Let $\mathfrak{B}$ be a Borel $\sigma$ -algebra of a Hausdorff
topological space $X$ . Suppose that $\mu :\mathfrak{B}\to\overline{\mathbb{R}}$ is a locally finite measure. If
there's a collection $\{K_n\}_{n\in\mathbb{N}}$ of compacts sets such
that $X=\cup_{n\in\mathbb{N}}{K}_n^{\color{red}\circ}$ , then for all $B\in\mathfrak{B}$ with $\mu (B)<\infty$ and $\varepsilon\in (0,\infty)$ there're a
compact $K$ and an open $O$ such that $K\subseteq B\subseteq U$ and $\mu (O\setminus K)<\varepsilon $ . I will write my proof since it may contain mistakes. We can use the Corollary 3.4 (which is in the page 74 of the "" Measure and Integration "" written by John L. Menaldi) to conclude that there're a closed $C$ and an open $O$ such that $C\subseteq B\subseteq O$ and $\mu (O\setminus C)<\varepsilon $ . Define $\tilde K_n:=C\cap (\cup_{i=1}^nK_i)$ for all $n\in\mathbb{N}$ . Then $\{\tilde K_n\}_{n\in\mathbb{N}}$ is a collection of compact sets such that $\tilde K_n\nearrow C$ with implies that $O\setminus \tilde K_n\searrow O\setminus C\,\,\color{red}{(1) }$ . Since $C\subseteq B\subseteq O$ , then $O=(O\setminus B)\sqcup B\subseteq (O\setminus C)\cup B$ which implies that $\mu (O)\leq \mu (O\setminus C)+\mu (B)<\infty$ since $\mu (O\setminus C)<\varepsilon $ and, by hypothesis, $\mu (B)<\infty$ . Therefore $\mu (O\setminus \tilde K_1)<\infty$ . With this inequality and $(1)$ we can conclude that $\lim_{n\to\infty}\mu _X(O\setminus \tilde K_n)=\mu _X(O\setminus C)<\varepsilon $ and, consequently, that there's $N\in\mathbb{N}$ such that $\mu _X(O\setminus \tilde K_N)<\varepsilon $ . We conclude the proof since $\tilde K_N$ is a compact satisfying $\tilde K_N\subseteq C\subseteq B\subseteq A$ . I think that the proof above contains an error since with the above theorem we can conclude that $\mu$ is inner regular. Thank you for your attention!","['measure-theory', 'solution-verification', 'borel-measures']"
4340272,What does $F:2^\nu \rightarrow \mathbb{R}$ mean?,"While I was watching https://www.youtube.com/watch?v=Y3u_hvxayDY , which is about submodular optimization, I found $F:2^\nu \rightarrow \mathbb{R}$ in about 1:00 in the video. Could anyone please clarify what it means? I guess that means a function $F$ gets any subset of $\nu$ as an input and it outputs a real number. Am I correct?",['elementary-set-theory']
4340278,How would you encourage an orthogonal matrix to be a permutation matrix?,"We want to solve an optimization problem over the space of permutation matrices (square binary matrices with exactly one entry of $1$ in each row and each column and $0$ s elsewhere) with size $n \times n$ , and we relax the constraint so that we operate optimization for orthogonal matrices ( $P^TP=PP^T=I$ ), how would you encourage the matrices to be permutation matrices? I have tried to add regularization on $\sum_{i,j} P_{ij}^4$ . Would you recommend something else?","['permutations', 'permutation-matrices', 'matrices', 'orthogonal-matrices', 'optimization']"
4340282,How to see Ky Fan metric satisfies the triangle inequality?,"Let $X$ and $Y$ be random variables. The Ky Fan metric is defined as: $$d(X,Y):= \min \{\epsilon>0 :P(|X-Y|>\epsilon)\le\epsilon\} $$ I want to show it is indeed a metric, for which I need to show that it satisfies triangle inequality. Set $d(X,Y)=\epsilon_1, d(Y,Z)=\epsilon_2$ ,and $d(Z,X)=\epsilon_3$ . I approached this by trying to show that $ (\epsilon_1+\epsilon_2)  \in \{\epsilon>0 :P(|X-Z|>\epsilon)\le\epsilon\} $ . But to show that, I had to show $P(|X-Y|>\epsilon_1)+P(|Y-Z|>\epsilon_2) \ge P(|X-Z|>\epsilon_1+\epsilon_2)$ , which I could not.
Can you help in showing this?","['measure-theory', 'metric-spaces', 'real-analysis']"
4340285,Solve $ x^{3}{y}'''+x{y}'-y = x\ln(x) \\ $,"Solve $$ x^{3}{y}'''+x{y}'-y = x\ln(x) \\    $$ using shift $x=e^{z}$ and differential operator $Dz=\frac{d}{dz}$ What does $Dz = d / dz$ mean? I did this but I don't know how to continue. Please help. $$ (e^{z})^{3}{y}'''+e^{z}{y}'-y = e^{z}\ln(e^{z}) \\\\(e^{3z}){y}'''+e^{z}{y}'-y = e^{z}{z}$$ And I tried $\,\,y=z^r$ $$e^{3}r(r^{2}-r-2)z^{r-3}+e^{z}rz^{r-1}-z^{r}=0$$",['ordinary-differential-equations']
4340292,Show that $\frac{1}{n}\sum_{j=1}^\infty \left( 1 - (1-p_j)^n\right) \to 0$ as $n \to \infty$,"Given $p_j \geq 0$ for all $j \geq 1$ and $\sum_{j=1}^\infty p_j = 1$ , I am asked to show that $$\frac{1}{n}\sum_{j=1}^\infty \left( 1 - (1-p_j)^n\right) \to 0 ~~ \textrm{as} ~~ n \to \infty.$$ Unfortunately, using only the fact that $(1-p_j)^n \geq 1 - np_j$ will not be enough, as this inequality only gives us $$\frac{1}{n}\sum_{j=1}^\infty \left( 1 - (1-p_j)^n\right) \leq \frac{1}{n}\sum_{j=1}^\infty np_j = 1.$$ Can anyone provide a hint towards the proof? Remark: This problem is related to exercise 3.8 in this monograph .","['asymptotics', 'analysis']"
4340335,Royden's proof that $L^{p_2}\subseteq L^{p_1}$ if $p_1<p_2$,"I'm reading the proof of the following theorem: Let $E$ be a measurable set of finite measure and $1\le p_1<p_2\le \infty$ .  Then $L^{p_2}(E)\subseteq L^{p_1}(E)$ .  Furthermore $$\|f\|_{p_1}\le c \| f\|_{p_2}$$ for all $f\in L^{p_2}(E)$ , where $c=[m(E)]^{\frac{p_2-p_1}{p_1p_2}}$ if $p_2<\infty$ and $c=[m(E)]^{1/p_{1}}$ if $p_2=\infty$ . Proof: [...] Assume $p_2<\infty$ .  Define $p=p_2/p_1>1$ and let $q$ be the conjugate of $p$ .  Let $f$ belong to $L^{p_2}(E)$ .  Observe that $|f|^{p_1}$ belongs to $L^p(E)$ and $g=\chi_E$ belongs to $L^q(E)$ since $m(E)<\infty$ .  [Note: Royden did not originally have $|f|^{p_1}$ but rather $f^{p_1}$ .  However, this was corrected in the errata, so I included the correction here.]  Apply Holder's Inequality.  Then $$ \int_E|f|^{p_1} = \int_E|f|^{p_1}\cdot g \le \|f\|_{p_2}^{p_1}\cdot \left[ \int_E|g|^q \right]^{1/q} = \|f\|_{p_2}^{p_1}[m(E)]^{1/q}.$$ Now my question is at this first inequality where Holder's is applied.  I certainly agree that we could obtain at this stage $$ \int_E|f|^{p_1}\cdot g \le \||f|^{p_1}\|_{p_2} \cdot \left[ \int_E|g|^q \right]^{1/q}$$ But the move made here seems to assume $\||f|^{p_1}\|_{p_2}=\|f\|^{p_1}_{p_2}$ .  But this doesn't seem obvious or established earlier in the text.  Am I misunderstanding what's going on here?","['normed-spaces', 'functional-analysis', 'real-analysis']"
4340354,Proofs in Real Analysis are too 'convenient',"I'm doing a first course in real analysis and I have studied nearly 10-15 theorems and proofs by now. One thing I've noticed in all of them is that they all seem too 'convenient' and full of assumptions. This, I find very peculiar to real analysis. To understand my point, consider this one for instance: Theorem : Let $\{x_n\}$ be a sequence of $\mathbb R^+$ such that $\lim_{n\to\infty} |\frac{x_{n+1}}{x_n}| = l$ . If $0\le l \lt 1$ , $\lim_{n\to\infty } x_n = 0$ . Proof : Consider $\epsilon \gt 0$ such that $l+\epsilon \lt 1$ . There exists $N \in \mathbb N$ such that $||\frac{x_{n+1}}{x_n}| - l |\lt \epsilon$ for all $n \ge N$ . $l-\epsilon \lt |\frac{x_{n+1}}{x_n}| \lt l+\epsilon$ (for all $n \ge N$ .) Let $m = l + \epsilon$ . Given that $0 \le l \lt 1$ , we could say that $0 \lt m \lt 1$ . This gives $|\frac{x_{n+1}}{x_n}| \lt m$ for all $n \ge N$ $x_{N+1} \lt mx_N$ $x_{N+2} \lt mx_{N+1}\lt m^2x_N$ So for all $n \ge N+1$ , $x_n \lt mx_{n-1} \lt m^{n-N}x_N$ . We are therefore left with $0 \lt x_n \lt Am^n$ where $A = \frac{x_N}{m^n}$ . As $\lim_{n\to \infty} Am^n = 0$ as $m\lt 1$ ,using the Squeeze Theorem, we are able to prove the theorem. You see, the whole thing is dependent on one assumption that $l+\epsilon \lt 1$ . But this should ideally hold true for any $\epsilon$ . I wouldn't call this proof 'complete'! Here's another such proof of the quotient law for limits: Let $\epsilon, k \gt 0.$ Then $\frac{\epsilon}{k}$ is also an arbitrary positive number. If $\{x_n\}$ and $\{y_n\}$ are two sequences, we need to prove that the limit of the quotient of the terms equals the quotient of the limits of the terms( say $l$ and $m$ ). For a certain $N$ , $|\frac{x_n}{y_n} - \frac{l}{m}| = |\frac{m(x_n-l) + l(m-y_n)}{my_n}| \le |\frac{|m||x_n-l| + |l||m-y_n|}{|m||y_n|}| \lt \frac{\epsilon}{ky_n} + \frac{\epsilon}{ky_n}\frac{|l|}{|m|} = \frac{\epsilon}{k}\frac{|m|+|l|}{|m||y_n|} $ $
lim_{n \to \infty} y_n = m$ so $lim_{n \to \infty} |y_n| = |m| $ . Let $ 0 <H<|m|$ . Then $ |y_n| > H $ for all $n \ge N_0, N_0 \in \mathbb N$ Choose $N' = max\{N_0, N\}$ so that for all $n \ge N',|\frac{x_n}{y_n} - \frac{l}{m}| < \frac{\epsilon}{k}\frac{|l|+|m|}{|m|H}$ Now choose k such that $ \frac{|l|+|m|}{|m|H} < 1$ so that $|\frac{x_n}{y_n} - \frac{l}{m}| < \epsilon$ . Q.E.D. The last part again contains too convenient choices of constants. I think this might mean that unless you are choosing them in such a manner, the theorem won't hold. It's as though we are creating the proof such that the theorem comes true, which I find strange. Hopefully I've made myself clear. I wonder if there exist 'more convincing' and more elegant proofs which do not take into account so many arbitrary constants. Thank you! Edit As suggested in one of the comments, I am inserting a theorem whose proof seems elegant to me-the Squeeze Theorem. Theorem :Given that $\{x_n\}$ , $\{y_n\}$ and $\{z_n\}$ are three sequences where $x_n \le y_n \le z_n $ for all $n \ge N,$ where $N \in \mathbb N$ , and $\lim_{n\to\infty } x_n = \lim_{n\to\infty } z_n = l,$ then $lim_{n\to\infty } y_n = l$ Proof : For a given $\epsilon \gt 0$ , we have natural numbers $N_1$ and $N_2$ such that $|x_n-l| < \epsilon$ for all $n \ge N_1$ and $|z_n-l| < \epsilon$ for all $n \ge N_2$ . Let $N_3 = max\{N_1, N_2\}$ , then for all $n \ge N_3$ , $|x_n-l| < \epsilon$ and $|z_n-l| < \epsilon$ . This means $l-\epsilon < x_n<l+\epsilon$ and $l-\epsilon < z_n<l+\epsilon$ for all $n \ge N_3$ . Let $N_4 = max\{N, N_3\}$ . Then it holds that $l-\epsilon < x_n < y_n < z_n <l+\epsilon$ and therefore $l-\epsilon < y_n<l+\epsilon$ or $|y_n-l| < \epsilon$ . Q.E.D We certainly have considered multiple constants here, but we are not arbitrarily assigning them values/choosing them to satisfy certain equations, like so: ' $l+\epsilon<1$ ' or 'choose k such that $ \frac{|l|+|m|}{|m|H} < 1$ '.","['proof-writing', 'real-analysis']"
4340411,does uncorrelatedness of even powers of multivariate Gaussian imply independence?,"Let $X=(X_1,X_2, \dots X_n)$ be a centered multivariate Gaussian vector. If $$
\operatorname{E} \left( X_1^{2\alpha} X_2^{2\alpha} \dots X_n^{2\alpha} \right)
=
\operatorname{E} \left( X_1^{2\alpha} \right) \operatorname{E} \left( X_2^{2\alpha} \right) \dots \operatorname{E} \left( X_n^{2\alpha} \right)
$$ for some positive integer $\alpha$ , does it necessarily follow that all components of $X$ are independent, i.e. that the covariance matrix is diagonal? It is conjectured that the left hand side is always larger than or equal to the right hand side and that equality holds precisely in the case of independence. I'm trying to understand whether this second assertion about equality is nontrivial as well or just some corollary which can also be shown directly. The conjecture is known to be true if $\alpha=1$ or if $n=2$ (with $\alpha$ an arbitrary positive integer), so in these cases my question is answered affirmatively. I'm interested in the general case. Thanks in advance!","['probability-theory', 'real-analysis']"
4340436,Solving a nonlinear autonomous system using polar coordinates,"I have the nonlinear system: \begin{equation}
\begin{array}
fx'=-y-x\sqrt{x^2+y^2}\\
y'=x-y\sqrt{x^2+y^2}
\end{array}
\end{equation} which I solve by polar coordinates transformation. $x=r\cos\phi \ y=r\sin\phi$ and $x'=dr\cos\phi-r\sin\phi$ and $y'=dr\sin\phi+r\cos\phi$ . \begin{equation}
\begin{array}
fdr\cos\phi-r\sin\phi=-r\sin\phi-r\cos\phi\sqrt{r^2\cos^2\phi+r^2\sin^2\phi}\\
dr\sin\phi+r\cos\phi=r\cos\phi-r\sin\phi\sqrt{r^2\cos^2\phi+r^2\sin^2\phi}\
\end{array}
\end{equation} Clearly, many terms cancel out and we obtain: \begin{equation}
\begin{array}
fdr=-r^2\\
dr=-r^2\
\end{array}
\end{equation} Then, integrating both sides with respect to $r$ we obtain: \begin{equation}
\begin{array}
fr=-\frac{1}{3}r^3+c\\
r=-\frac{1}{3}r^3+c
\end{array}
\end{equation} Since $r=\frac{x}{\cos\phi} and r=\frac{y}{\sin\phi}$ we get \begin{equation}
\begin{array}
fx=-\frac{x^3}{\cos^2\phi}+c\cos\phi\\
y=-\frac{y^3}{\sin^3\phi}+c\sin\phi
\end{array}
\end{equation} The summing up the two equations, and isolating for y, I get a very scary result: y≈0.26457 sin^4(ϕ) cos(ϕ) (sqrt((27 c csc^9(ϕ) sec^2(ϕ) + 27 c csc^8(ϕ) sec^3(ϕ) - 27 x^3 csc^9(ϕ) sec^5(ϕ) - 27 x csc^9(ϕ) sec^3(ϕ))^2 + 108 csc^15(ϕ) sec^6(ϕ)) + 27 c csc^9(ϕ) sec^2(ϕ) + 27 c csc^8(ϕ) sec^3(ϕ) - 27 x^3 csc^9(ϕ) sec^5(ϕ) - 27 x csc^9(ϕ) sec^3(ϕ))^(1/3) - (1.2599 csc(ϕ) sec(ϕ))/(sqrt((27 c csc^9(ϕ) sec^2(ϕ) + 27 c csc^8(ϕ) sec^3(ϕ) - 27 x^3 csc^9(ϕ) sec^5(ϕ) - 27 x csc^9(ϕ) sec^3(ϕ))^2 + 108 csc^15(ϕ) sec^6(ϕ)) + 27 c csc^9(ϕ) sec^2(ϕ) + 27 c csc^8(ϕ) sec^3(ϕ) - 27 x^3 csc^9(ϕ) sec^5(ϕ) - 27 x csc^9(ϕ) sec^3(ϕ))^(1/3) and csc(ϕ) sec(ϕ)!=0 What went wrong here? Thanks UPDATE , with Jean Maries correction we get: \begin{equation}
\begin{array}
fx=\frac{\cos\phi}{\phi+c}\\
y=\frac{\sin\phi}{\phi+c}
\end{array}
\end{equation} Summing up and solving for y: \begin{equation}
y=\frac{\cos\phi}{\phi+c}+\frac{\sin\phi}{\phi+c}-x
\end{equation} which plotted , with an arbitrary value of $c$ is:","['nonlinear-system', 'polar-coordinates', 'ordinary-differential-equations']"
4340459,What is the geometric significance of the definition of the derivative for complex-valued functions?,"Roughly speaking, a function $f:\mathbf R^2\to\mathbf R^2$ is differentiable at $\mathbf v$ if there is a linear transformation $Df(\mathbf v)$ such that $f(\mathbf v+\mathbf h)\approx f(\mathbf v)+Df(\mathbf v)\mathbf h$ . If, in addition, $f$ is differentiable as a map from $\mathbf C$ to $\mathbf C$ , then we require that this linear transformation is complex multiplication, which rotates and scales the vector $\mathbf h$ . This means that if you consider a tiny disk $D$ centred at $\mathbf v$ , then (in the limit) the image of $D$ under $f$ will also be a disk. My question is: from a geometric perspective, why does the fact that holomorphic functions map disks to disks give them such nice properties—e.g. a function which is differentiable once is infinitely differentiable.","['complex-analysis', 'calculus', 'derivatives', 'analytic-functions']"
4340556,An Application of Lebesgue Dominated Convergence Theorem,"As an application of the Lebesgue Dominated Convergence Theorem, we would like to evaluate the following limit $$
\lim_{n\to\infty}\int\limits_1^{n}\frac{1}{\displaystyle\left(1+\frac{x}{n}\right)^nx^{1/n}}\ \mathrm dx.
$$ We realized that to put the limit the inside of the integral we need to find an integrable function $g(x)$ such that $$
\left|\frac{1}{\displaystyle\left(1+\frac{x}{n}\right)^nx^{1/n}}\right| \leq g(x).
$$ We did this. But the boundary of the integral also depends on $n$ . At this point, we do not know what we need to do. Thanks in advance for any help.","['measure-theory', 'lebesgue-integral', 'real-analysis']"
4340576,"The smallest $\sigma$-algebra in $X=\{1,2,3,4\}$ that contains a collection of subsets of $X$","I would like to find an efficient way to determine the smallest $\sigma$ -algebra $\sigma(F)$ in $X=\{1,2,3,4\}$ that contains one of the following $F$ 's: (a) $F=\{\{1\},\{2,3\}\}$ (b) $F=\{\{1,3\},\{2,3\}\}$ (c) $F=\{\{1\},\{1,3\}\}$ Before we begin our discussion, let us review some definitions related to $\sigma$ -algebra. We define a $\sigma$ -algebra in $X$ to be a collection of subsets of $X$ that is closed under complements, countable unions, and countable intersections. As for what is meant by $\sigma(F)$ , we define $$\sigma(F)=\bigcap\{\Lambda:\text{$\Lambda$ is a $\sigma$-algebra in $X$ containing $F$}\}.\tag{$*$}$$ For (a), I simply considered what it would be like to have $\Lambda_a$ as a $\sigma$ -algebra in $X$ containing $\{\{1\},\{2,3\}\}$ . For example, because $\{1\}\in\Lambda_a$ , we must have $\{1\}^c=\{2,3,4\}\in\Lambda_a$ . Likewise, we must have $\{2,3\}^c=\{1,4\}\in\Lambda_a$ , thereby leading to $\{2,3,4\}\cap\{1,4\}=\{4\}\in\Lambda_a$ . Continuing in this manner, I got $$\Lambda_a=\{\emptyset,\{1\},\{4\},\{2,3\},\{1,4\},\{1,2,3\},\{2,3,4\},X\}.$$ I claim that $\Lambda_a=\sigma(F)$ because if any set in $\Lambda_a$ is removed, the resulting product $\Lambda_a'$ will no longer be a $\sigma$ -algebra in $X$ containing $F$ . Now let me ask two questions : Is $\Lambda_a$ really equal to $\sigma(F)$ ? I did NOT even make use of ( $*$ ) to get $\sigma(F)$ ! Even if my approach is correct, it is kind of time-consuming. Is there any other approach that is both effective and efficient ? Thank you for your patience.","['measure-theory', 'real-analysis']"
4340587,Convergence of $\sum \frac{a_n }{\sqrt{n}}$ given that $\sum a_n^2$ converges,"Hypothesis : If $\sum_{n=1}^{\infty} a_n^2$ converges then $\sum_{n=1}^{\infty} \frac{a_n}{\sqrt{n}}$ also converges. I'm looking for a counterexample (or proof) to this claim.
The only assumption is $a_n \in \mathbb{R}$ . Here it was proved that convergence of $\sum a_n^2$ implies convergence of $\sum \frac{a_n}{n^p}$ for $p=1$ . I noticed it can be easily generalised for $p>\frac{1}{2}$ since: $$ 0 \leq (a_n - \frac{1}{n^p})^2 = a_n^2 - 2\frac{a_n}{n^p} + \frac{1}{n^{2p}} \implies \frac{a_n}{n^p} \leq \frac{1}{2}(a_n^2 + \frac{1}{n^{2p}}) $$ On the other hand, for $p<\frac{1}{2}$ there is a counterexample: $a_n = \frac{1}{n^{1-p}}$ . But it doesn't apply to $p= \frac{1}{2}$ , since then $\sum a_n^2 = \sum \frac{1}{n}$ , which diverges.
So no p-series can be a counter and also Cauchy-Schwarz doesn't help.","['convergence-divergence', 'sequences-and-series', 'analysis', 'real-analysis']"
4340614,"Evaluate: $\lim_{n\to\infty} \int_{[0,\infty)} (1+x/n)^{-n}\sin(x/n)\,dx.$","Evaluate: $$\lim_{n\to\infty} \int_0^{\infty} \left(1+\frac xn\right)^{-n}\sin \frac xn\,dx.$$ I have tried to use the (Lebesgue) dominated convergence theorem to evaluate the same. At first I noticed that: $\left|\left(1+\frac xn\right)^{-n}\sin \frac xn\right|\le\left|\left(1+\frac xn\right)^{-n}\right|\le1\text{ for all positive }x \text{ and for all natural }n\tag*{}.$ Since $g(x)=1$ is Lebesgue integrable for each $x\in[0,\infty).$ So by DCT, the given limit equals: $$\int_0^{\infty} \lim_{n\to\infty}  \left(1+\frac xn\right)^{-n}\sin \frac xn\,dx=\int_0^{\infty}e^{-x}\cdot0\,dx=0.$$ This question was asked in our end term exam this semester and so I don't know the correct answer to this question. Therefore, just to check whether I have evaluated the limit correctly I am posting the same here on MSE. If I have gone wrong somewhere, please point out and give some insights. Thanks in advance.","['limits', 'measure-theory']"
4340627,If $\lambda \in \partial \sigma (T)$ then $T - \lambda$ cannot be surjective.,Let $\mathcal H$ be a Hilbert space and $T \in \mathcal L(\mathcal H).$ If $\lambda \in \partial \sigma (T)$ then $T - \lambda$ is not surjective. This question appeared in an entrance examination in India for admission into PhD programme which I am unable to solve. Any hint would be a boon for me at this stage. Thanks for your time.,"['hilbert-spaces', 'spectral-theory', 'functional-analysis']"
4340640,Does $\ \sum_{n=1}^{\infty} \frac{\sin(2^n x)}{n}\ $ converge for all $x\ ?$,"Does $\ \displaystyle\sum_{n=1}^{\infty} \frac{\sin(2^n x)}{n}\ $ converge for all $x\ ?$ It obviously converges for any $x\ $ of the form $\ 2^mk \pi\ $ where $\ m,k\in\mathbb{Z},\ $ but for any other values of $\ x\ $ the question is interesting because I don't see how to answer it. I tried using Cauchy's condensation test with $\ 2^nf(2^n) = \frac{\sin(2^n x)}{n},\ $ which implies that $\ f(n) = \frac{\sin(nx)}{n\log_2(n)},\ $ and so $\ \displaystyle\sum_{n=1}^{\infty} \frac{\sin(2^n x)}{n}\ $ converges if and only if $\ \displaystyle\sum_{n=1}^{\infty} \frac{\sin(nx)}{n\log_2(n)}\ $ converges. I then found out from this video that we can prove using some basic complex analysis that $\ \displaystyle\sum_{k=1}^{\infty} \frac{\sin(n x)}{n}\ $ converges for all $\ x,\ $ and then I thought we were done by the limit comparison test . However, this attempt is wrong because $$\ \frac{\frac{\sin(nx)}{n}}{\frac{\sin(nx)}{n\log_2(n)}}\ \not\to c>0. $$ We also cannot use the integral test for convergence because $\ \displaystyle\sum_{n=1}^{\infty} \frac{\sin(2^n x)}{n}\ $ is not monotone. Finally, I do not think we can use Abel's test or Dirichlet's test because $\ \displaystyle\sum_{n=1}^{\infty} \sin(2^n x)\ $ probably diverges. Edit : Maybe we can use Dirichlet's test here. I just realised  we only need to show the series $\ \displaystyle\sum_{n=1}^{\infty} \sin(2^n x)\ $ is bounded, not that it converges! But I don't know how to do this... Apparently $\ \displaystyle\sum_{n=1}^{\infty} \sin(kx)\ $ is bounded but $\ \displaystyle\sum_{n=1}^{\infty} \sin(k^2)\ $ isn't , although proving this is difficult! So whether or not $\ \displaystyle\sum_{n=1}^{\infty} \sin(2^n x)\ $ is bounded is probably difficult too. So we should look to use a different test... Maybe some version of the Alternating Series test or Absolute convergence test ? Although I think that determining convergence of $\ \displaystyle\sum_{n=1}^{\infty} \frac{\lvert \sin(2^n x) \rvert }{n}\ $ is more difficult than the original question here. But Alternating series test might be promising, not sure... Or maybe there is some other test from complex analysis that is applicable here?","['convergence-divergence', 'absolute-convergence', 'sequences-and-series']"
4340651,Is the state space of a transient irreductible Markov chain infinite?,We know that if we have a Markov chain $(X_n)$ on an finite state space $S$ and with at least one recurrent state then the chain is irreducible and hence recurrent. So my question is : If we have an irreducible Markov chain $(X_n)$ with one transient state then does that mean that the state space $S$ is infinite?,"['markov-chains', 'stochastic-processes', 'markov-process', 'probability-theory', 'probability']"
4340671,Reflection of a Light Ray in a Square and the Distribution of $\{\langle n\gamma\rangle\}_{n=1}^\infty$ for $\gamma\in\mathbb R$,"This post concerns the geometric interpretation of the distribution properties of $\{\langle n\gamma\rangle\}_{n=1}^\infty$ for $\gamma\in\mathbb R$ , as discussed in Chapter $4$ , Applications of Fourier Series in Stein & Shakarchi's Fourier Analysis . Suppose that the sides of a square are reflecting mirrors and that a ray of light leaves a point inside the square. What kind of path will the light trace out? To solve this problem, the main idea is to consider the grid of the plane formed by successively reflecting the initial square across its sides. With an appropriate choice of axis, the path traced by the light in the square corresponds to the straight line $P + (t, \gamma t)$ in the plane. As a result, the reader may observe that the path will be either closed and
periodic, or it will be dense in the square. The first of these situations will happen if and only if the slope $\gamma$ of the initial direction of the light (determined with respect to one of the sides of the square) is rational. In the second situation, when $\gamma$ is irrational, the density follows from Kronecker's theorem. What stronger conclusion does one get from the
equidistribution theorem? How does the path traced by the light ray correspond to the straight line $P + (t, \gamma t)$ in the plane $\mathbb R^2$ ? Once I understand this, I should be able to approach further questions, namely: When is the path closed and periodic? When is it dense in the square? What stronger conclusion follows from the equidistribution theorem? Equidistribution theorem : If $\gamma\notin \mathbb Q$ , then the sequence of fractional parts $\{\langle n\gamma\rangle\}_{n=1}^\infty$ is equidistributed in $[0,1)$ . Notation: $\langle x\rangle$ denotes the fractional part of $x\in \mathbb R$ , i.e. $x = \lfloor x\rfloor + \langle x\rangle$ . Let me know if any other details are needed. Thank you!","['number-theory', 'geometry', 'analysis', 'dynamical-systems']"
4340675,Tripos Integral,"Show that $$\int_0^{\frac{\pi}{4}} \frac{\sin^2 x}{e^{2mx}(\cos x-m\sin x)^2}dx=\frac{1}{2m(1+m^2)}\left[\frac{1+m}{1-m}e^{-m\frac{\pi}{2}}-1\right]$$ This problem is from Edwards, Treatise on Integral Calculus II, pg.187. It is a definite integral and is not likely to find the indefinite integral, at this point in the book Edwards has introduces the tricks of substituting $\frac{\pi}{4}-x$ for $x$ , and expansion in series (and of course parts). Unfortunately, I haven't been able to make these tricks work out. My method has been to expand $\frac{\tan^2 x}{(1-m\tan x)^2}$ as a series in $m\tan x$ , and integrate termwise but so far without success. Does anybody have any good ideas?","['integration', 'calculus', 'definite-integrals']"
4340686,"Suppose $\forall x \in X,\sum_{n=1}^\infty|f_n(x)|<\infty$, to prove $\forall F\in X'', \sum_{n=1}^\infty |F(f_n)|\leq C\|F\|$.","Let $X$ be a Banach space, consider $\{ f_n \}_{n \ge 1} \in X'$ s.t. $$\sum_{n = 1}^\infty | f_n(x) | < \infty, x \in X. \tag{1}\label{cond}$$ Please prove that there exists $C \ge 0$ s.t. for each $F \in X''$ , $$\sum_{n = 1}^\infty | F(f_n) | \le C \left\Vert F \right\Vert. \tag{2}\label{goal}$$ I have tried to use the uniform boundedness principle to solve this exercise. From the condition i.e. equation \eqref{cond}, I can prove that $$\frac{ \sum_{n = 1}^\infty | f_n(x) | }{\Vert x \Vert} < \infty. \tag{3}\label{subcond}$$ And I found that if we could prove $$\sum_{n = 1}^\infty | F(f_n) | < \infty, \tag{4}\label{subgoal}$$ we could prove the required conclusion i.e. equation \eqref{goal}. But I don't know how to deduce equation \eqref{subgoal} from equation \eqref{subcond}.","['functional-analysis', 'dual-spaces']"
4340689,Is this an alternative definition of manifolds?,"Suppose $X\subseteq\mathbb{R}^m$ s.t. for any $x\in X$ and any open $U\subseteq\mathbb{R}^m$ that contains $x$ , there exists a smaller open set $V\subseteq U$ also containing $x$ , so that $V\cap X$ is the image of some injective continuous map $f:\mathbb{R}^n\rightarrow\mathbb{R}^m$ . Any $n$ -manifold in $\mathbb{R}^m$ clearly satisfies this property; conversely must such an $X$ be an $n$ -manifold? At least I cannot come up with a $1$ -dimensional counterexample; the usual examples such as the topologist's sine curve are ruled out by this definition, since it implies locally connectedness. It also rules out the figure 8 since the X shape cannot be injective image of $\mathbb{R}$ . I am asking in the continuous category, but feel free to assume smoothness for simplification. Clarification: at first I just let $X$ be any topological space, so people thought I was looking for examples of non-standard manifolds; also someone pointed out that $\mathbb{R}^n$ with discrete topology is a counterexample. Therefore I decided to require $X$ to be a subset of some Euclidean space, but this is really the case which I think is more interesting. Update: I feel like at least this is true for $n=1$ . Here is an outline, although I'm not confident about any of these steps. (i) Show that if $Y\subseteq X$ is the injective image of $\mathbb{R}$ , then it cannot be the injective image of disjoint copies of $\mathbb{R}$ . (ii) If $f,g$ are both injective map from $\mathbb{R}$ to $X$ and $f(\mathbb{R})\cap g(\mathbb{R})\neq\emptyset$ , then $f(\mathbb{R})\cup g(\mathbb{R})$ is either also such an image or a circle. (iii) Each point of $X$ is contained in either a circle or the image of some injective $f:\mathbb{R}\rightarrow X$ that is maximal (no larger set is such an image). (iv) Conclude that $X$ is a $1$ -manifold.","['manifolds', 'general-topology', 'geometric-topology']"
4340778,Why is the standard integral the Riemann Integral?,"In introductory calculus courses, you are introduced to integration via the Riemann Integral . Often, you are shown a diagram such as the following which defines the Riemann Integral as the sum of the areas of these rectangles. My question is, why are these rectangles vertical (domain-defined) instead of horizontal (range-defined) like below? From my understanding, the horizontal orientation is known as the Lebesgue Integral . So then my question becomes: when is it better to use either one over the other? And also, why is that, we are introduced to the Riemann Integral over the Lebesgue Integral in introductory calculus?","['calculus', 'measure-theory', 'real-analysis']"
4340783,Minimum amount of material to make a cuboid with fixed volume,"I'm trying to assist a student with the following optimization problem. The instructions say to use the partial derivative test which I'm having trouble with. I can solve the problem with Lagrange multipliers just fine, but the student doesn't know that method yet. A rectangular metal tank with an open top is to hold $256$ cubic feet of liquid. What are the dimensions of the tank that require the least material to build? Let $A(x,y,z)=xy+2xz+2yz$ . Then we want to find $\min \left\{A(x,y,z) \mid xyz=256\right\}$ . First I find the critical points, $$\begin{cases}\frac{\partial A}{\partial x} = y + 2z = 0 \\ \frac{\partial A}{\partial y} = x + 2z = 0 \\ \frac{\partial A}{\partial z} = 2x + 2y = 0\end{cases} \implies x=y=-2z \implies 4z^3 = 256$$ so that $z=4$ , but $x$ and $y$ both turn out to be negative. Is there something I am missing, or is this problem flawed in some fundamental way?","['partial-derivative', 'optimization', 'multivariable-calculus']"
4340899,"A function that is in the $L^p$ space for some $p \ge 1$ and uniformly continuous, has a limit of $0$ as $||x|| \rightarrow \infty.$","Let, $p \in [1,\infty)$ . Suppose, $g \in L^p(\mathbb{R}^{n},m)$ where $m$ is the n'th dimensional Leb. measure, and $g$ is uniformly cont. Then, $$\lim_{||x|| \rightarrow \infty}g(x) = 0.$$ Suppose, $\lim_{||x|| \rightarrow \infty}g(x) \ne 0$ . Then, $\exists \epsilon >0:$ $$\forall M \in \mathbb{R}, \exists \alpha, ~ ||\alpha|| > M ~ \land  ~  |g(\alpha)| > \epsilon.$$ Then we can construct an increasing sequence (w.r.t. the norm) $\{\alpha_n\}_{n \ge 1}$ such that, $|g(\alpha_n)| > \epsilon$ and $||x_n|| < ||x_{n+1}|| + 1$ for all $n \in \mathbb{N}$ . By uniform continuity we have that, $\exists \delta,\forall x,y \in \mathbb{R}^{n}$ : $$||x-y|| < \delta \implies |f(x) - f(y)| <  \frac{\epsilon}{2}.$$ Let, $S = \bigcup_{n \ge 1} B(\alpha_n,\min\{\delta,1\}).$ The ball of radius $\min\{\delta,1\}$ centered at $\alpha_n$ .
We have that, $$\int_{S}(\frac{\epsilon}{2})^{p} ~ dm \le \int_{\mathbb{R}^n}|g|^p ~ dm.$$ Observe that, $\mu(S) = \sum^{\infty}_{n=1} \min\{1,\alpha\} = \infty.$ Thus, $$\infty \le \int_{\mathbb{R}^n}|g|^p ~ dm. $$ Which is a contradiction to $g \in L^{p}(\mathbb{R}^{n},m)$ . EDIT:
Thank you, @MathematicsStudent1122 for the tip.","['measure-theory', 'lp-spaces']"
4340903,Group theory — Lagrange's theorem does not seem to hold,"As per Lagrange's theorem, the order of a subgroup must perfectly divide the order of the group. Let us take the group $G = (S, \sharp)$ , where $S = \{1,3,5,7,9\}$ and $\sharp$ suggests multiplication mod 10. $G$ is a group, with being $O(G)=5$ Let us take a subset $H = \{1,3,7,9\}$ , then $(H, \sharp)$ is also a group as it is closed, associative, satisfies identity and inverse laws. Thus $H$ is a subgroup of $S$ . $O(H) = 4$ .
But I don't see Lagrange's theorem holding, as 4 does not divide 5. Not sure if I'm making any silly mistake somewhere, would appreciate some help on this.","['group-theory', 'discrete-mathematics']"
4340910,Divide a ball of volume $\frac{e^2}{6}n$ into $n$ slices of equal height. What is the product of the volumes of the slices as $n\rightarrow\infty$?,"Divide a ball of volume $\frac{e^2}{6}n$ into $n$ slices of equal height, as shown below with example $n=8$ . What is the limit of the product of the volumes of the slices as $n\rightarrow\infty$ ? (If the image doesn't load for you, just imagine $n+1$ equally-spaced horizontal planes, and a ball that is tangent to the top and bottom planes. The planes, between the top and bottom planes, are where you cut the ball.) I used volume of revolution, and after simplifying I got: $$\lim_{n\to\infty}\exp{\left(2n-2n\ln{n+\sum_{k=1}^{n}}\ln{\left(k(n+1-k)-\frac{n}{2}-\frac{1}{3}\right)}\right)}$$ Wolfram does not evaluate this limit, but desmos tells me that when $n=10, 100, 1000, 10^6$ , the product is approximately $1.847, 1.977, 1.997, 1.99999513$ , respectively. So apparently the limit converges and equals $2$ , but I do not know how to prove this. (In case you're wondering how I got the number $\frac{e^2}{6}$ : I used trial and error on desmos to hunt for the number that makes the limit converge, assuming such a number exists. I obtained a number like 1.231509. I entered this number into Wolfram and it suggested $\frac{e^2}{6}$ .)","['infinite-product', 'limits', 'volume']"
4340921,Proving a two variable limit using $\epsilon- \delta$ approach.,"I came across a question in which we had to prove the following limit by $\epsilon-\delta$ approach: $\lim_{(x,y)\to(-1,-1)}xy-2x^2=-1$ I had to prove that: For every $\epsilon>0$ in $\sqrt{(x+1)^2+(y+1)^2}<\epsilon$ , there exists a corresponding $\delta$ such that $|xy-2x^2+1|< \delta$ . So, I proceeded by putting $x+1=x_1$ and $y+1=y_1$ so that it would simplify the former expression inside the square root and would also help me to convert it to polar. However, neither of the methods worked and I ran out of ideas. I am new to multivariable calculus and am using $\epsilon-\delta$ approach in two variables for the first time. Would someone please help me with this?","['limits', 'multivariable-calculus', 'epsilon-delta']"
4340924,"How to describe the ring $\mathbb{C}[\cos{x},\sin{x}]$ as a quotient ring of $\mathbb{C}[X,Y]$? [duplicate]","This question already has answers here : Ring of trigonometric functions with real coefficients (3 answers) Closed 2 years ago . Here, $\sin{x}$ and $\cos{x}$ are real functions $\mathbb{R} \to \mathbb{R}$ . First, let me explain what I mean by ""as a quotient ring"". It can be easily shown that $$
\mathbb{R}[\cos{x},\sin{x}] \cong \mathbb{R}[X,Y]/(X^2+Y^2-1).
$$ Here $\mathbb{R}[\cos{x},\sin{x}]$ is a quotient ring of $\mathbb{R}[X,Y]$ . So when the field is changed to $\mathbb{C}$ , I immediately think about the ring $$
\mathbb{C}[X,Y]/(X^2+Y^2-1).
$$ As people have pointed out, my argument quoted below is questionable, so I redo it in update 2. This paragraph is left here only for the record. I elaborated it in update 3. However, $\mathbb{C}[\cos{x},\sin{x}]$ is not a UFD ( $\sin^2{x}=(1+\cos{x})(1-\cos{x})$ ), but $\mathbb{C}[X,Y]/(X^2+Y^2-1) \cong \mathbb{C}[T,T^{-1}]$ is a UFD. Therefore $\mathbb{C}[\cos{x},\sin{x}]$ can only be a proper subring quotient ring of $\mathbb{C}[X,Y]/(X^2+Y^2-1)$ . An extra restriction is missing here. Is there anyway to find it? I think there is something derived from the fact that $\mathbb{C}$ is algebraically closed. Update 1: It is not very obvious that $\mathbb{C}[X,Y]/(X^2+Y^2-1)$ is a PID and therefore UFD, but one can find proofs in this question post: Ring of trigonometric functions with real coefficients Update 2: It seems my argument on being UFD is a little messed up, so I will redo it. First of all, $R=\mathbb{R}[X,Y]/(X^2+Y^2-1)$ is not a UFD (one can show that its ideal class group is $\mathbb{Z}/2\mathbb{Z}$ ) and $S=\mathbb{C}[X,Y]/(X^2+Y^2-1) \cong \mathbb{C}[T,T^{-1}]$ is a UFD (see the link in update 1). Extending the scalar here is not a trivial matter, so the relation between $\mathbb{C}[\cos{x},\sin{x}]$ and $\mathbb{C}[X,Y]/(X^2+Y^2-1)$ is not likely to be as immediate as the case of real scalar. Update 3: In the ring $\mathbb{R}[\cos{x},\sin{x}]$ , irreducible elements are of the form $a\sin{x}+b\cos{x}+c$ where $a^2+b^2 \ne 0$ ; meanwhile, in the ring $\mathbb{C}[\cos{x},\sin{x}]$ , irreducible elements are of the form $\cos{x}+i\sin{x}+a$ where $a \in \mathbb{C}^\ast$ . I found them on this book (section: The Trigonometric Polynomial Rings). Hence $\sin{x}$ , $1-\cos{x}$ and $1+\cos{x}$ are irreducible in $\mathbb{R}[\cos{x},\sin{x}]$ but not irreducible in $\mathbb{C}[\cos{x},\sin{x}]$ . This is the source of my mistake. According to the comments this question have received, this change is not easily spotted. So I think I shall leave it here because it may serve as a counter-example on irreducibility.","['ring-theory', 'trigonometry', 'abstract-algebra', 'commutative-algebra']"
4340946,Minimizing $ |t|^2 + a(|t + u| - |u|) $ over $ t \in \mathbb{C} $,"I'm not so experienced with Complex Analysis and I somewhere stumbled upon the following function $ f: \mathbb{C}^2 \rightarrow \mathbb{R} $ that I need to minimize with respect to the Complex variable $ t $ ( $ t \neq 0 $ ): $$ f(t, u) = |t|^2 + a(|t + u| - |u|), $$ where $ u $ is also Complex and $ a $ is a known Real constant (I can also assume $ a \geq 0 $ ). I wonder if I can find an explicit solution (even considering cases) for $ \hat{t} $ the minimizer, and if so I would appreciate if one can help me how to proceed?","['absolute-value', 'complex-analysis', 'calculus', 'optimization', 'complex-numbers']"
4341025,Why does $\det A$ change sign when any $2$ columns of $A$ are interchanged?,"I have tried to reason, using multilinear forms, the well-known fact that the determinant of a matrix $[A]$ changes its sign if any two columns of $[A]$ are interchanged. I am not confident if my reason is correct however, and would appreciate a nod or a refute. Thanks. (Disclaimer: I am confident that if my reasoning is correct, then it is unlikely that I am presenting anything novel here. I haven't however found the below-described argument -- using multilinear forms -- over math.stackexchange.com in response to similar queries, and therefore thought of seeking a review.) Here goes the reasoning: The definition of the determinant ( $det$ ) of a linear operator on a finite-dimensional vector space ( $\S 53$ of Finite-Dimensional Vector Spaces , $2^{\text{nd}}$ Ed., by Paul R. Halmos) implies the following. If $A$ is any linear operator on any $n$ -dimensional vector space $\mathcal V$ , if $\{x_1, \cdots, x_n\}$ is any basis in $\mathcal V$ , and if $w$ is any alternating $n$ -linear form on $\mathcal V$ , then $$\tag{1}\det \ A = \frac{w(Ax_1, \cdots, Ax_n)}{w(x_1, \cdots, x_n)}.$$ Under the hypothesis of (1), suppose some matrix $[A]$ together with $\{x_1, \cdots, x_n\}$ defines $A$ . Suppose $[A_1]$ is the matrix obtained by interchanging two columns, of $[A]$ , having any indices $h, k \ (h \neq k, 1 \leq h \leq n, 1 \leq k \leq n)$ . Let $A_1$ be the operator defined by $[A_1]$ together with $\{x_1, \cdots, x_n\}$ . It follows from the definition of a matrix ( $\S$ 37 of Finite-Dimensional Vector Spaces , $2^{\text{nd}}$ Ed., by Paul R. Halmos) that $Ax_h = A_1 x_k$ , $Ax_k = A_1 x_h$ , and that $Ax_i = A_1 x_i$ for $i = 1, \cdots, n$ except when $i$ equals $h$ or $k$ . This finding together with the skew-symmetric nature of $w$ ( $\S$ 30 of Finite-Dimensional Vector Spaces , $2^{\text{nd}}$ Ed., by Paul R. Halmos) implies that $$w(A_1 x_1, \cdots, A_1 x_n)= -w(A x_1, \cdots, A x_n).$$ Accordingly, we have $$ \det \ A_1  = \frac{w(A_1x_1, \cdots, A_1x_n)}{w(x_1, \cdots, x_n)} = \frac{-w(Ax_1, \cdots, Ax_n)}{w(x_1, \cdots, x_n)} = -\det \ A.$$ It is clear therefore that $\det\ [A_1] = -\det\ [A]$ .","['determinant', 'matrices', 'multilinear-algebra', 'linear-algebra', 'linear-transformations']"
4341079,The range of a non-computable function that grows faster than computable functions is undecidable,"Let $f$ be a non-computable function that grows faster than every computable function. I have to prove that $R=\text{Range}(f)$ is non-decidable. I'm trying to prove the statement by contradiction. Then the indicator function $1_R$ of $R$ is computable. Now, to get a contradiction,  I want to find a computable function $g$ using $1_R$ , s.t. $f$ doesn't grow faster than $g$ . Can you please give me some tips on the construction of $g$ ? P.S. A precise definition of $f$ is the following: $f$ is a non-computable totally defined $\mathbb{N}\rightarrow\mathbb{N}$ function Given a computable function $h$ , there is $n_h\in\mathbb{N}$ , s.t. for all $m\geq n_h$ we have $f(m)>h(m)$ (if $h(m)$ is defined)","['logic', 'decidability', 'discrete-mathematics', 'computability']"
4341101,Isolating frequencies and phases of superposed cosine functions,"This problem is mathematical, but it is needed in physics. I have points on the graph of the function $y = f(x)$ . These points were obtained experimentally. I have a lot of such graphs. The pictures below show only two examples. It is known that $f(x) = A_{1}\cos(w_{1}x + \phi_{1}) + A_{2}\cos(w_{2}x + \phi_{2})$ , where $A_{1}, w_{1}, A_{2}, w_{2}, \phi_{1}, \phi_{2}$ are some constants (real parameters) and $w_{1} > 0, w_{2} > 0$ . I need to determine the approximate values of numbers $A_{1}, w_{1}, A_{2}, w_{2}$ knowing what the graph of a function looks like. I want to know the values of numbers $A_{1}, w_{1}, A_{2}, w_{2}$ even with a large error, for example, with an error of 50%. I need to know this so that I can use a computer program OriginLab to analyze this graph. My question. The graph of the function clearly shows where the extremum points are. You can see the values of the function at the extremum points. The points of intersection of the graph of the function with the abscissa axis are also clearly visible. Is it possible on the basis of this to find approximate (not exact) values of numbers $A_{1}, w_{1}, A_{2}, w_{2}$ ? I would like to know not only the method of how to do this, but I also want to know the mathematical proof of this method. It is about a method how to calculate these parameters on paper. This is not about numerical methods. My work. At the moment, I do not know this method. On the Internet, you can find special cases of how this function looks, but I'm interested in the general case for arbitrary values of $A_{1}, w_{1}, A_{2}, w_{2}, \phi_{1}, \phi_{2}$ . I tried to investigate the function myself using differentiation, but very complex transcendental equations are obtained.","['trigonometry', 'real-analysis']"
4341130,Is every transitive action conformal?,Suppose that a Lie group $ G $ acts transitively on a manifold $ M $ . If $ G $ is compact then we can equip $ M $ with a Riemannian metric with respect to which $ G $ acts by isometries. What if $ G $ is noncompact? Can we equip $ M $ with a Riemannian metric with respect to which $ G $ acts by conformal transformations? The first counter example that comes to mind is take $ G=E_2 $ the isometry group of the plane. Then take $ M $ to be the Moebius strip or the Klein bottle. $ G $ acts transitively on both these spaces. But I have no idea if the action is conformal (say with respect to the flat metric).,"['riemannian-geometry', 'smooth-manifolds', 'conformal-geometry', 'lie-groups', 'differential-geometry']"
4341144,Invariant subspace problem for $\ell^2(\mathbb{N})$,"Is the invariant subspace problem known for $\ell^2(\mathbb{N})$ or for more general $L^2$ spaces, i.e. does every bounded linear operator $T \colon \ell^2(\mathbb{N}) \to \ell^2(\mathbb{N})$ have a non-trivial (closed) T-invariant subspace?","['invariant-subspace', 'functional-analysis', 'analysis', 'real-analysis']"
4341146,Is there a pathological continuous function $\ f:\mathbb{R}\to\mathbb{R}\ $ that is nowhere increasing or decreasing and has no local extrema?,"Is there a pathological yet continuous function $\
f:\mathbb{R}\to\mathbb{R}\ $ such that: For every $\ x\in\mathbb{R}\ $ and $\ \delta>0,\ \exists\ a,b,\ $ both
in $\ (x,x+\delta),\ $ such that $\ f(a)<f(x)<f(b)\qquad (*)\quad ? $ This makes me think of Weierstrass function because it is nowhere increasing and nowhere decreasing. However, at some points of the Weierstrass function a local maximum is attained. Let $\ x\ $ be a local maximum. Then by definition of local maximum, $\ \exists\ \delta>0\ $ such that $\ a\in(x,x+\delta)\implies f(a)\leq f(x).\ $ Therefore the Weierstrass function does not satisfy $\ (*).$ Maybe such a function is well known? Or if not, I was wondering if we can construct such a function using transfinite induction/recursion, although it might be difficult to prove that the function remains continuous in the limit case? Or maybe there is some clever argument that any real function with the property $\ (*)\ $ cannot be continuous everywhere (in particular, the function cannot be continuous at $ x)$ ?","['real-analysis', 'continuity', 'general-topology', 'transfinite-induction', 'problem-solving']"
4341163,Large sample properties of classical estimator for scale parameter,"I've also post this question on Stats Stackexchange as advised in the comment. Suppose $X=(X_1,X_2,\ldots,X_n)$ are non-negative and have a joint probability density $$\frac{1}{\sigma^n}f\bigl(\frac{x}{\sigma}\bigr)=\frac{1}{\sigma^n}f\bigl(\frac{x_1}{\sigma}, \frac{x_2}{\sigma},\ldots, \frac{x_n}{\sigma}\bigr),$$ where $f$ is known, and $\sigma>0$ is the only unknown parameter. In classical decision theory, we may choose an estimator $d$ to minimize the risk $$\mathbb{E}[L(\sigma,d)|\sigma],$$ here $L(\sigma,d)$ is the loss function. For example, Stein’s loss $L(\sigma,d)=d/\sigma-\log(d/\sigma)-1$ . Then the minimum risk estimator can be written as $$d=X_{1}/\mathbb{E}[X_{1}|\sigma=1,Z],$$ where $Z=(Z_1,\ldots,Z_n)$ with $Z_i=X_i/X_n$ is the maximal invariant. With the different choice of loss function, we may get a different minimum risk estimator. So far they are just some well-established results in the theory of point estimation. Usually, in large sample, we prefer MLE since $f$ is known and MLE is the most efficient $\sqrt{n}$ -consistent estimator in this case. I’m just curious about the limiting behavior of above kind of estimators. Intuitively, it is consistent. But what is the convergence rate and limiting distribution? I’ve searched for literature but got no general results about this question. I know the limiting behavior depends on $f$ , in a special case that $f$ is normal, then above estimator coincides with MLE. Will above estimator sometimes have a faster/slower convergence rate than MLE if $f$ satisfies certain conditions? Any references or ideas are welcome!","['statistical-inference', 'statistics', 'parameter-estimation']"
4341191,if oscillation of a function is zero then the function has a right limit.,"I was given this question: $$\omega_f(I)=sup_{x,y\in I}{|f(x)-f(y)|}$$ True/False: $$\lim_{\epsilon\rightarrow0}{\omega_f((x_0,x_0+\epsilon))}=0 \iff \lim_{x\rightarrow x_0^+}{f(x)} \in \mathbb{R}$$ I think I proved the direction from right to left: if $\lim_{x\rightarrow x_0^+}{f(x)} =L$ then $$\forall \epsilon_0 \exists \delta_0 : x_0<x<x_0+\delta_0 \rightarrow |f(x)-L|<\epsilon_0$$ Therefore, choose $\epsilon_0/2, \forall x,y\in(x_0,x_0+\delta_0) \rightarrow |f(x)-f(y)|<\epsilon_0 \rightarrow \omega_f(I)\le\epsilon_0$ (triangle inequality) therefore when $\epsilon\le\delta_0$ $\rightarrow \lim_{\epsilon\rightarrow0}{\omega_f((x_0,x_0+\epsilon))}=0$ I tried doing the second direction but I can't determine what limit the function would even have, I also couldn't think of any counterexamples, any help would be greatly appreciated!","['limits', 'functions', 'real-analysis']"
4341192,asymptotic order in probability,"Consider the definition of small o in probability . Let $X_n$ be a sequence of random variables and let $X_n=1+o_p(1)$ . That is, for any $\delta,\epsilon>0: P(\lvert X_n-1\rvert\geq \delta)\leq \epsilon$ for sufficiently large $n$ . The sequence converges in probability to one. This led me to conjecture that there should be a constant $1>M>0$ so that $P(\lvert X_n\rvert\geq M)\to1$ as $n\to+\infty$ . Or equivalently, $P(\lvert X_n\rvert < M)\to 0$ . Question . If it is true, then how to prove it?","['statistics', 'probability-theory', 'asymptotics']"
4341199,Does the function $f(x)=\frac{1}{x}$ have an inverse function?,"From what I understand, a function has an inverse if and only if such function is a bijection. So if we consider $f(x)=\frac{1}{x}$ , it is clear that $f$ is not a bijection since $f$ is not surjective (because its codomain is $\mathbb{R}$ , which is not equal to its range $\mathbb{R}$ \{ $0$ } ). However, looking at this website here ( https://www.mechamath.com/algebra/how-to-know-if-a-function-has-an-inverse/ ) it mentions that you can do a ""horizontal line test"" to determine if a function has an inverse - i.e. if you can draw a horizontal line on a sketch of $y=f(x)$ that passes through more than one point, then the function doesn't have an inverse. Using this horizontal line test on $f(x)=\frac{1}{x}$ , no horizontal line can be drawn which crosses through two points on a sketch of $y=\frac{1}{x}$ and so this would suggest that an inverse does exist for $f(x)=\frac{1}{x}$ and that this would be $f^{-1}(x)=\frac{1}{x}$ . So my question is simply whether $f(x)=\frac{1}{x}$ does, or doesn't, have an inverse?","['functions', 'singularity', 'analysis', 'inverse-function']"
4341249,Why is this (image) function transformation not logical?,"I am taking MIT 6.003 course from OCW and came upon the following problem from lecture 1: For the above image $f(x, y)$ , does the below image represent the transformation $f(-x-250, y)$ ? The correct answer is no but my answer was yes and the reasoning was as follows: First, we apply the image transformation using $f(-x, y)$ which will result in the original image being flipped about the vertical axis passing through x=0. Next, we apply the image transformation using $f(x-250, y)$ which will result in the flipped image to translate to the right. In total, intuitively speaking, applying $f(-x, y)$ followed by $f(x-250, y)$ should result in the 2nd image shown. Can anyone tell me what is the flaw in my logic? I can't figure out why this reasoning shouldn't work.","['coordinate-systems', 'functions', 'transformation']"
4341265,Show that $M_{n}=\left(\cfrac{N}{N-1}\right)^n X_n(N-X_n)$ is a martingale,"Suppose we have a random variable $(X_n)$ such that : $X_0=k\in\{1,..,N\}$ The conditional distribution of $X_{n+1}$ conditioned to $\mathcal F_n=\sigma(X_1,..,X_n)$ follows the binomial distribution with parameters $(N,X_n/N)$ Let $M_{n}=\left(\cfrac{N}{N-1}\right)^n X_n(N-X_n)$ .  Prove that $M_n$ is a martingale with respect to $\mathcal F_n$ . Attempt : I already proved that $X_n$ is a martingale. $E(M_{n+1} \, | \, F_n)=E\left( \left(\cfrac{N}{N-1}\right)^{n+1}X_{n+1}(N-X_{n+1}) \, | \, F_n \right)$ I noticed that $X_{n}(N-X_{n})=N*Var(X_{n+1})$ And we know that $E[X_{n+1}\, | \, F_n]=X_n$ But I got stuck there, did I start this wrong? Am I missing a property that I can use?","['expected-value', 'stochastic-processes', 'martingales', 'probability-theory', 'probability']"
4341271,Purely categorical definition of Hausdorff space,"I would like to be able to look at the category Top and, not knowing anything about the ""internals"" of the objects (i.e. topological spaces), be able to identify which ones are Hausdorff and which aren't. You can get most of the way there with the following definition of the Hausdorff axiom: The diagonal $\Delta = \{ (x,x) | x \in X \}$ is closed as a subset of the product space $X \times X$ The ""diagonal"" can be defined categorically (i.e. https://en.wikipedia.org/wiki/Diagonal_morphism ), but it's the ""closed"" that's giving me trouble. Given the diagonal morphism in the product setup, how can we tell via arrows/some universal property that this arrow (or the embedded subobject?) is ""closed""? My end goal here is to apply this notion to the category of pre-varieties to identify varieties in this category. References: https://ncatlab.org/nlab/show/Hausdorff+space#BeyondTopologicalSpaces https://en.wikipedia.org/wiki/Category_of_topological_spaces","['general-topology', 'category-theory']"
4341284,$f(x)f(\frac{1}{x})=f(x)+f(\frac{1}{x})$,"Find a function $f(x)$ such that: $$f(x)f(\frac{1}{x})=f(x)+f(\frac{1}{x})$$ with $f(4)=65$ . I have tried to let $f(x)$ be a general polynomial: $$a_0+a_1x+a_2x^2+\ldots a_nx^n$$ which leaves $f(\frac{1}{x})$ as: $$a_0+a_1{1\over x}+a_2{1\over x^2}+\ldots + a_n{1\over x^n}$$ On comparing the coefficients of both sides, we see that: $$2a_0=(a_0)^2+(a_1)^2+(a_2)^2+\ldots+(a_n)^2$$ And $$a_1=(a_0a_1)+(a_1a_2)+ \ldots +(a_{n-1}a_n)$$ I don't know how to proceed further. I know I need to compare coefficients and come to a conclusion based on their values, but I don't see what to do next.","['functional-equations', 'functions', 'polynomials', 'analysis']"
4341295,Can we think of the Fourier series as a Bochner integral?,"$\newcommand{\norm}[1]{\|#1\|}$ $\newcommand{\vp}{\varphi}$ $\newcommand{\Z}{\mathbb Z}$ $\newcommand{\T}{\mathbf T}$ $\newcommand{\C}{\mathbf C}$ Let $\T$ denote the unit circle and $\lambda$ denote the normalized Haar measure on $\T$ . For each $n\in \Z$ let $\chi_n:\T\to \C$ be the function which takes $z$ to $z^n$ . For any $f\in L^2(\T, \lambda)$ we know that $$
f = \sum_{n\in \Z} \hat f(n) \chi_n
$$ in $L^2(\T, \lambda)$ .
This means that $$
\norm{f - \sum_{n=-N}^N \hat f(n)\chi_n}_2 \to 0
$$ as $N\to \infty$ . Question. My question is can we think of the sum $\sum_{n\in \Z} \hat f(n) \chi_n$ as a Bochner integral ? Here is the natural thing to go for. Let $c$ be the counting measure on $\Z$ and let $\vp:\Z\to L^2(\T, \lambda)$ be the function defined by $$
\vp(n) = \hat f(n) \chi_n
$$ If $\vp$ were Bochner integrable then the integral $\int_\Z \vp\ dc$ exists and is nothing but $\sum_{n\in \Z} \hat f(n) \chi_n$ and things are fine.
However, the Bochner integrability criterion asks for the finiteness of $\int_\Z \norm{\vp}_2\ dc$ , which is same as asking for the finiteness of $$
\sum_{n\in \Z} |\hat f(n)|
$$ But since $\hat f$ is not necessarily in $\ell^1(\Z)$ , we cannot say that $\vp$ is Bochner integrable. In general, I would like to think of any infinite sum as an integral under the counting measure of a suitable function.
The fact that I am unable to do this for the Fourier series is a bit disconcerting.
Perhaps I am making a mistake somewhere?","['fourier-series', 'measure-theory']"
4341367,Lebesgue-counting product measure of the diagonal,"This is an exercise in Folland's real analysis text. Consider $X = (I,\mathcal{B}_I, \mathcal{L})$ and $Y = (I,\mathcal{B}_I,\mathcal{C})$ , where $\mathcal{B}_I$ is the Borel $\sigma$ -algebra on $I$ , and $\mathcal{L}$ and $ \mathcal{C}$ are the Lebesgue and counting measures, respectively. Last, let $D = \{(x,x): x \in I\}$ . I wish to compute: $$\iint \chi_D d\mathcal{L}d\mathcal{C}$$ $$\iint \chi_D d\mathcal{C}d\mathcal{L}$$ $$\iint\chi_D d(\mathcal{L}\times\mathcal{C})$$ To show that $\sigma$ -finiteness is necessary for the Fubini-Tonelli Theorem to hold. I computed the first two, but I am having trouble computing $(\mathcal{L} \times \mathcal{C})(D)$ . I can't approximate $D$ from below as a limit of ""rectangles"" (i.e., points), since it's uncountable. When I try to approximate it from above, I can't apply continuity of measure, since for any set $R$ of rectangles containing $D$ , $(\mathcal{L} \times \mathcal{C})(R) = \infty$ , so the conditions for $$(\mathcal{L} \times \mathcal{C})\left(\bigcap E_i\right) = \lim(\mathcal{L} \times \mathcal{C})(E_i)$$ do not hold. Indeed, it holds that $$\mathcal{C} \left(\bigcap_{i \in \mathbb{N}}(-1+1/i,1-1/i)\right) = 1 \neq \infty = \lim_{i \to \infty}\mathcal{C}((-1+1/i,1-1/i)), $$ so I'm suspicious of this kind of argument in the two-dimensional case. I could see it being both measure $0$ and measure $\infty$ . I see there are other versions of this post, but none of them answer this question in a satisfactory manner. In particular, the two answers I have seen involve outer measures, and hand-waving about cardinality. The first does not answer the question, and the second is too vague.","['measure-theory', 'lebesgue-integral', 'real-analysis']"
4341386,"In GAP, how can I create a group isomorphic but not equal to a given group?","Sorry if the question title is confusing. Let me try to explain my question over an example. Please consider gap> G := SmallGroup(24, 12);; Without knowing that $G$ is $S_4$ , can we define a new group which is $S_4$ but not equal to $G$ . For example, gap> H := SmallGroup(24, 12);;
gap> G = H;
false
gap> K := ShallowCopy(Group(GeneratorsOfGroup(G)));;
gap> K = G;
true So I am trying to find a way to create a new group which has the same structure with $G$ without using SmallGroup . I used ShallowCopy to do that but I failed. I want to do this, because if $S\leq G$ , I cannot use the function IsConjugatorIsomorphism to see that whether a map is given by an element in S yet not in G. Since Parent(S) is $G$ , it does not work. We can also use ConjugatorOfConjugatorIsomorphism to check that, but I would like to avoid doing this as I suspect there might be a simpler solution. Edit.
Another attempt that not helps me is below: gap> S4 := SmallGroup(24, 12);;
gap> D8 := AllSubgroups(S4)[26];;
gap> StructureDescription(D8);
""D8""
gap> D8PermRep := Image(SmallerDegreePermutationRepresentation(
>                         Image(IsomorphismPermGroup(D8))));;
gap> center := Center(D8PermRep);
Group([ (1,3)(2,4) ])
gap> ConjugacyClassesSubgroups(D8PermRep);
[ Group( () )^G, Group( [ (2,4) ] )^G, Group( [ (1,3)(2,4) ] )^G,
  Group( [ (1,2)(3,4) ] )^G, Group( [ (1,3), (2,4) ] )^G, 
  Group( [ (1,2)(3,4), (1,3)(2,4) ] )^G, 
  Group( [ (1,4,3,2), (1,3)(2,4) ] )^G, 
  Group( [ (1,3), (2,4), (1,2)(3,4) ] )^G ]
gap> C2 := ConjugacyClassesSubgroups(D8PermRep)[4][1];
Group([ (1,2)(3,4) ])
gap> IsConjugatorIsomorphism(AllIsomorphisms(center, C2)[1]);
true
gap> Parent(C2) = S4;
false","['gap', 'group-theory', 'math-software']"
4341422,How many group actions are there of $Q_8$ on a 4 element set.,"The question is to determine how many group actions $\alpha:Q_8\times X→ X$ exist, up to isomorphism, where $|X|=4$ . Here is my solution so far: Call a pair $(X,\alpha)$ a $Q_8$ -set if there exists a group action $\alpha:Q_8\times X→ X$ . We know of $4$ actions immediately: The action of $Q_8$ on $\{1,-1,i,-i\},\{1,-1,j,-j\},\{1,-1,k,-k\}$ given by conjugation, and the action of $Q_8$ on $Q_8/\{1,-1\}$ given by left or right translation. The $Q_8$ -sets $(Q_8/\{1,-1\},\text{left})$ and $(Q_8/\{1,-1\},\text{right})$ are isomorphic as $Q_8$ -sets. The three conjugation actions are distinct (right?). I was trying to show that these are the only four actions. An action of $Q_8$ on a $4$ element set is equivalent to a homomorphism $Q_8\to S_4$ , so it comes down to showing there are $4$ such homomorphisms. No homomorphism can have kernel $\{1\}$ , because otherwise $S_4$ has a subgroup isomorphic to $Q_8$ , which is not possible. I was tempted to say that,  since there are only $4$ other normal subgroups, there are only $4$ homomorphisms, hence these are all the group actions. But I know that two distinct homomorphisms can have the same kernel — is this still true in this case, or can I argue that (same kernel) implies (same homomorphism) in this specific case? Are there more than $4$ actions? I wanted to do it avoiding the usual way of tracking element orders/etc.. Is this unavoidable? EDIT : I missed the trivial action — So I'm trying to show that there are five actions, corresponding to all subgroups of $Q_8$ except $\{1\}$ .","['group-theory', 'group-actions', 'quaternions']"
4341440,"Evaluating $\int_{0}^{\infty} \frac{4\pi}{16\pi^2 + x^2} \left(\frac{1}{x}+\frac{1}{e^{-x}-1}\right) \, dx$","I’m trying to evaluate the following integral: $$\int_{0}^{\infty} \frac{4\pi}{16\pi^2 + x^2} \left(\frac{1}{x}+\frac{1}{e^{-x}-1}\right) \, dx$$ I’ve tried using contour integration by using a quarter-circle contour and going around the pole at $z=4\pi i$ with a semi-circular arc, however, I wasn’t able to evaluate the integrals along the imaginary axis. I wasn’t able to come up with a real or complex method for evaluating this integral, so any help would be appreciated. I’m not sure if a closed-form exists.","['integration', 'calculus', 'analysis', 'complex-integration']"
4341445,Convergence in measure iff convergence in distance metric,"I am trying to solve the following question: Let $(X, F, \mu)$ be a measure space. Let $H>0$ be integrable. Let $(E,d)$ be a separable metric space and $f,g: X \rightarrow E$ measurable functions. Let $d_H(f,g):= \int \min\{d(f(x),g(x)),1\}H(x)d\mu(x)$ . Prove $f_n$ converges in measure to $f$ $\iff$ $\lim d_H(f_n, f) = 0$ . I think I have the "" $\implies$ "" part (the sentence in bold later on is the part I am stuck on): Let $1>\epsilon>0$ and $\delta>0$ and let $c:= \int H(x) < \infty$ . $d_H(f_n, f) =  \int_{\{x|d(f_n(x),f(x)>\epsilon\}} \min\{d(f_n(x),f(x)),1\}H(x)d\mu(x) + \int_{\{x|d(f_n(x),f(x)\leq\epsilon\}} \min\{d(f_n(x),f(x)),1\}H(x)d\mu(x)$ From monotonicity and from our assumption that $f_n$ converges in measure to $f$ , for a large enough $n$ we can bound the first term by: $$\leq \int_{\{x|d(f_n(x),f(x))>\epsilon\}} H(x)d\mu(x) \leq c\mu(\{x|d(f_n(x),f(x))>\epsilon\} \leq c\delta$$ For the second term, since $\epsilon<1$ , we obtain: $$ \leq \int_{\{x|d(f_n(x),f(x)\leq \epsilon\}} \epsilon H(x)d\mu(x) \leq \epsilon c$$ Since these two hold for all $1>\epsilon>0$ and $\delta>0$ for a sufficiently large $n$ , we obtain the conclusion. For the "" $\Longleftarrow$ "" part I am stuck with $H(x)$ : Let $1>\epsilon>0$ and $\delta>0$ . $$ \int_{\{x|d(f_n(x),f(x))> \epsilon \}} \epsilon H(x) \leq \int_{\{x|d(f_n(x),f(x))> \epsilon\}} \min\{1,d(f_n(x),f(x))\}H(x) \leq \int \min\{1,d(f_n(x),f(x))\}H(x)$$ Without the $H(x)$ I would be able to write: $$\epsilon \mu(\{x|d(f_n(x),f(x))> \epsilon\}) \leq \int_{\{x|d(f_n(x),f(x))> \epsilon \}} \epsilon \leq \int_{\{x|d(f_n(x),f(x))> \epsilon\}} \min\{1,d(f_n(x),f(x))\} \leq \int \min\{1,d(f_n(x),f(x))\}$$ and then by taking the limit, the right-hand-side would tend to 0 and so for a large enough $n$ the left-hand-side is less than $\epsilon\delta$ and the conclusion follows. How can I do this with $H(x)$ ? Also - if there is feedback for improvement of the first part - would be great to know.","['measure-theory', 'convergence-divergence', 'real-analysis']"
4341448,Exponentiating expression containing ln(abs(x)),"I am trying to figure out when we write +/- after exponentiating expressions containing natural log. So, say that we have integrated (1/x) with respect to x.
Then we have ln(abs(x)) + C. That is, ln(x) with absolute value signs around the x. Now, if this is the left-hand side of the equation and we want to exponentiate both sides in order to solve, do we need to have +/- exp expression due to the absolute value inside the natural log? I am confused about when you need the +/- because in a WolframAlpha solution to a Diff EQs problem that I’m working on, they just took the positive expression instead of using +/-. But some problems in the book have answers with +/- for a similar process, so I am confused. Thanks very much.","['exponentiation', 'absolute-value', 'ordinary-differential-equations', 'logarithms']"
4341449,$\int_0^{\frac{\pi}{2}}\ln(\sin^2 x+k^2\cos^2 x)dx$ not by differentiation under the integral?,"Show that $$\int_0^{\frac{\pi}{2}}\ln(\sin^2 x+k^2\cos^2 x)dx=\pi\ln \frac{1+k}{2}$$ This is an exercise from Edwards Treatise on Integral Calculus II pg.188. What solution to this problem can be given ? In the chapter in Edwards he introduces the technique of substitution $x\mapsto \frac{\pi}{2}-x$ . And proves Euler's formulas $$\int_0^{\frac{\pi}{2}}\ln \sin x dx =\int_0^{\frac{\pi}{2}}\ln \cos x dx=-\frac{\pi}{2}\ln 2$$ The problem can be written as $$\int_0^{\frac{\pi}{2}}\ln(\tan^2 x+k^2 )dx+2\int_0^{\frac{\pi}{2}}\ln(\cos x)dx=\pi\ln(1+k)-\pi\ln 2$$ Which reduces the question to $$\int_0^{\frac{\pi}{2}}\ln(\tan^2 x+k^2 )dx=\pi\ln(1+k)$$ What proof can people give of this equation ? The only success I have had is to differentiate with respect to $k$ , and the result is easy to evaluate.
However, it seems that Edwards has at this stage not discussed that method, (although he lists it as a technique, and there is a chapter on it in part I) so is there another solution ? Of course, these old books may be playing by different rules. And the Tripos type questions dont follow any pedagogical pattern. But I guess I really want to know if someone has another solution.","['integration', 'calculus', 'definite-integrals']"
4341479,Can you uniquely define a smooth manifold M by taking the set of functions from M to R you consider smooth?,Let M be a 2nd countable Hausdorff space. Let F be a set of functions from open sets of M to R. Does there exist a smooth structure on M that makes these functions smooth? Is this an equivalent way to define a manifold? Similar to how you can uniquely define a topological space by having a set of functions to be consider continuous?,"['smooth-functions', 'smooth-manifolds', 'differential-geometry']"
4341482,Integral of $L^2(\mathbb R^+)$ function is $o(\sqrt x)$,"I stumbled upon an exercise which goes as follows : Let $f \in L^2(\mathbb R^+)$ , show that $\int_0^x f(t)\text{d}t = o\left(\sqrt x\right)$ . Cauchy-Schwarz inequality gives the fact that $\int_0^x f = O\left(\sqrt x\right)$ For decreasing functions, the results holds since $xf(x)^2 \leq \int_x^{2x} f^2 = o(1)$ . Simple limit examples such as $f(t) = \dfrac 1 {\sqrt t\ln(t)}$ or $f(t) = \displaystyle\sum_{k} \dfrac{1_{[k,k+1]}}{\sqrt k}$ advocate for the result. However I cannot produce a proof of that result nor find any counterexample, I don't even know what to believe. Given the source of the problem, a proof, if there is, should be elementary. Happy Hollidays",['integration']
4341497,"Why wrong or unacceptable to write Pipes on their own, without any probability?","I see mixed messages. heropup commented $(C \mid B) \mid (A \mid B)$ , but Michael Hardy chided that "" $\color{Red}{\text{There's no such thing as A∣B.}}$ When one writes Pr(A∣B), one is NOT writing about the probability of something that's called A∣B "". Tom Loredo answered Now suppose our information about the problem tells us that $A$ and $B$ are conditionally independent given $C$ .  A conventional notation for this is: $$
A \perp\!\!\!\perp B \,|\, C,
$$ which means (among other implications), $
P(A|B,C) = P(A|C).$ But why is there $\color{Red}{\text{""no such thing as A∣B""}}$ ? A|B makes perfect sense to me. $A \mid B$ would simply mean the event A, given that and after the event B happened and anteceded A. A|B would meaningfully and gainfully differ from $A \cap B$ , because $A \cap B$ reveals nothing about the echelon/order/tier  of A and B! But A|B and B|A do! I retort Bey's rebuttal . $(A|B)|(C|D)$ can be construed  meaningfully! $(A|B)|(C|D)$ would simply mean A, given B, given C, given D. So D happened first. Then C happened after D. Then B happened after C, D. Finally A happened after B, C, D.",['probability']
4341537,Is my demonstration correct?,"Is my demonstration correct? Let $f, g$ infinitely differentiable increasing functions. Suposse that: $$\star \ \ \ \ \  f(0) = g(0) = 0 $$ $$\star \ \ \ \ \  f'(0) = g'(0) = 1 $$ $$\star \ \ \ \ \ \lim\limits_{x \rightarrow 0} \dfrac{f(x)}{x} = 1 $$ $$\star \ \ \ \ \ f(-x) = -f(x), g(-x) = -g(x) $$ $$\star \ \ \ \ \ f(x) \neq g(x) \ \ \ \forall x \neq 0 $$ So $$\lim\limits_{x \rightarrow 0}\dfrac{f(x) - g(x)}{g^{-1}(x) - f^{-1}(x)} = 1 $$ My possible demonstration: We can approximate f(x) next to $x_0$ by $$(1) \ \ \ f(x) \approx f'(x_0)x + (f(x_0) - x_0f'(x_0)) = f'(x_0)x + b(x_0) $$ where $b(x_0) = (f(x_0) - x_0f'(x))$ . Replacing x by $f^{-1}(x)$ in (1) we got: $$(2) \ \ \ f^{-1}(x) = \dfrac{x - b(x_0)}{f'(x_0)} $$ (same about $g$ , $g(x) = g'(x_0)x + (g(x_0) - x_0g'(x_0)) = g'(x_0)x + c(x_0))$ , so $$\dfrac{f(x) - g(x)}{g^{-1}(x) - f^{-1}(x)} \approx \dfrac{f'(x_0)x + b(x_0) - (g'(x_0)x + c(x_0))}{\dfrac{x - c(x_0)}{g'(x_0)}-\dfrac{x - b(x_0)}{f'(x_0)}} $$ But $$\lim\limits_{x_0 \rightarrow 0} f'(x_0) = \lim\limits_{x_0 \rightarrow 0} g'(x_0) =1$$ So we got $$\dfrac{f'(x_0)x + b(x_0) - (g'(x_0)x + c(x_0))}{\dfrac{x - c(x_0)}{g'(x_0)}-\dfrac{x - b(x_0)}{f'(x_0)}} \approx \dfrac{b(x_0) - c(x_0)}{-c(x_0) + b(x_0)} = 1$$ It's wrong? How can I be more accurate? I want to prove for $f(x) = \sin(\tan(x)), g(x) = \tan(\sin(x))$ , however, I believe it is possible to generalize as above","['limits', 'calculus', 'analysis']"
4341551,A sum similar to Harmonic numbers.,"Consider the sum $$S_n=\sum_{r=2}^n \frac{(-1)^r\binom{n}{r}}{r-1}          $$ My question is: What is the asymptotic for $S_n$ as $n\to \infty$ or find $R_n$ such that $S_n\sim R_n$ as $n\to \infty$ . My try:
Define the $n$ th Harmonic number as $$H_n=\sum_{r=1}^n \frac{1}{r}$$ Then we also have (see https://en.m.wikipedia.org/wiki/Harmonic_number ) $$H_n=\sum_{r=1}^{n} \frac{(-1)^{r-1} \binom{n}{r}   }{r} $$ Then we have the asymptotic equivalence of $H_n$ as $$H_n\sim \log n+\gamma +\mathcal{O}\left(\frac{1}{n}\right)$$ How to find the asymptotic equivalence of $S_n$ ?","['asymptotics', 'harmonic-numbers', 'taylor-expansion', 'sequences-and-series', 'limits']"
4341556,Does Frobenius norm (not operator 2 norm) preserve the positive semidefinite order of matrices?,"Let $A, B$ be two real symmetric positive semidefinite $n\times n$ matrices (these conditions might be unnecessary). We say $A\le B$ if $B-A$ is positive semidefinite. If $A\le B$ , do we necessarily have $\|A\|_F \le \|B\|_F$ , where the norm is Frobenius norm? Note that for a general matrix $A$ , $\|A\|_F^2=\text{Trace} (AA^T)$ . So we are really asking whether $\text{Trace} (AA^T) \le \text{Trace} (BB^T)$ . I have tried a couple of examples and believe this is true but can't find a proof. the proof posted below is about the operator 2 norm I believe Symmetric positive semi-definite matrices and norm inequalities","['matrices', 'linear-algebra']"
4341563,Is the area of a triangle a function of its perimeter?,"I've already knew how to express the area of an equilateral triangle as a function of its perimeter, but I don't know how to do it for any triangle. But this is a problem in George Simmons's book ""Calculus with Analytic Geometry"".  Does someone know how to solve it? Thanks in advance.",['algebra-precalculus']
4341574,$\lim_{n \to \infty}\frac{1}{n}\sum_{k = 1}^nf(k)$ where $f(n)$ is the largest prime factor exponent?,"Let $f(n)$ the be largest exponent among exponents of the prime factor of $n$ . E.g. $f(80) = 4$ since $80 = 2^4.5$ and the prime factor of $80$ which has the largest exponent is $2$ which occurs with the exponent $4$ . Trivially for all square-free numbers $n, f(n) = 1$ . Experimental data for $n \le 3.5 \times 10^9$ shows that. $$
\mu = \lim_{n \to \infty}\frac{1}{n}\sum_{k = 1}^nf(n) \approx 1.784744
$$ Q 1 : Does this limit exist and does it have a closed form? If all positive integers were square-free, the above mean would have been exactly $1$ . But this is not the case because there are non-squarefree numbers. Thus, we can say that the square-free numbers contribute exactly $1$ to the above mean value while all numbers which have prime factors with an exponent $> 1$ contribute the remaining $0.784744$ . Using the natural density of $k$ -th power free numbers, I can show that $$
\mu \ge \sum_{k = 1}^{\infty}k \Big(\frac{1}{\zeta(k+1)} - \frac{1}{\zeta(k)}\Big) \approx 1.70521
$$ Q 2 : Is it possible to show a weaker result like $\mu < 2$ ?","['prime-factorization', 'elementary-number-theory', 'number-theory', 'analytic-number-theory', 'limits']"
4341579,Square in a quadrant of a circle (Doubt in a solution given),"From the MindYourDecisions video ""Challenging problem given to 12 year olds - square in a quadrant"" on YouTube: Original question:- I have a doubt in the solution given:- Make 3 additional copies and complete the circle.  There will be 5 squares in the shape of a +. One of the diameters of the circle will intersect the outer corners of 2 outer squares. I have made a very rough diagram following what has being said, I get a shape of +.  After that I am confused.  {The purple, yellow, and orange squares are the ones I got by symmetry} Let $s = \text{side of square}$ .  That diameter will be the hypotenuse of a right triangle ( $2$ ).  The legs are $s$ and $3s$ .  So, $$10s^2 = 4\to s^2 = 2/5$$","['circles', 'geometry']"
4341641,Write the event of drawn cards,"From a deck of $52$ cards, take out $26$ cards. The event $A_{ij}$ means that there are at least $i$ clubs and at least $j$ spades. I need to write the event using set-theoretic operations and $A_{ij}$ that all of these $26$ cards are red. My answer is: $$A_{00} \setminus (A_{01} \cup A_{10})$$ Am I right?","['elementary-set-theory', 'probability-theory', 'probability']"
4341648,Solve the ODE $y'=1-\frac{y}{x}$,"I substituted $u=\frac{y}{x}$ then tried to solve the ODE $$\frac{u'}{2u-1}= -\frac{1}{x}$$ and I came this far $$\frac{1}{2}\ln |{2u-1}|=- \ln |{x}| + c_1$$ but then in the solution there was the step $$c_1=\ln c_2 \in \mathbb{R}, c_2 > 0$$ to get $$\ln |2u-1|=\ln{(\frac{c_2}{x})^2}$$ but why do we have this additional step? Couldn't we just calculate the solution without this step?","['substitution', 'ordinary-differential-equations']"
4341769,Solve $\int _{x=0}^{\infty }\int _{t=-\infty }^{\infty }\exp \left(\frac{-a t^2+i b t}{3 t^2+1}+i t x\right)\frac{x}{3 t^2+1}\mathrm{d}t\mathrm{d}x$,"How to solve this double integral? $$f(a,b)=\int _{x=0}^{\infty }\int _{t=-\infty }^{\infty }\exp \left(\frac{-a t^2+i b t}{3 t^2+1}+i t x\right)\frac{x}{3 t^2+1}\mathrm{d}t\mathrm{d}x$$ $$\text{with }a>0,b\in \mathbb{R},i^2=-1$$ Known special solution for ${\bf b=0}$ $$f(a,0)=\frac{\pi}{\sqrt{3}\, {\rm exp}\left(\frac{a}{6}\right)}\left[(a+3) I_0\left(\frac{a}{6}\right)+a I_1\left(\frac{a}{6}\right)\right]$$ where $I_0,I_1$ are Bessel functions of order $0$ and $1$ ( proof ). The difference of the special solution to the general solution $f(1,0)-f(1,b)$ is calculated numerically for $a=1$ and $-50<b<50$ . What I tried I followed the first steps given here . Substitution of $t\rightarrow t \sqrt{3},x\rightarrow x/\sqrt{3}$ removes the factors in the denominator $$f(a,b)=\sqrt{3}\int _{x=0}^{\infty }\int _{t=-\infty }^{\infty } \exp \left(\frac{1}{t^2+1}\left(\frac{i b t}{\sqrt{3}}-\frac{a t^2}{3}\right)+i t x\right)\frac{x}{t^2+1}\mathrm{d}t\mathrm{d}x$$ $$=\sqrt{3}\int _{x=0}^{\infty }{\rm d}x \frac{x}{\text{exp}(x)}\int _{t=-\infty }^{\infty} \exp \left(\frac{1}{t^2+1}\left(\frac{i b t}{\sqrt{3}}-\frac{a t^2}{3}\right)+i x(t-i) \right)\frac{1}{t^2+1}\mathrm{d}t$$ $$=\frac{\sqrt{3}}{{\rm exp}(a/3)}\int _{x=0}^{\infty }{\rm d}x \frac{x}{\text{exp}(x)}\underbrace{\int _{t=-\infty }^{\infty} \exp \left(\frac{1}{t^2+1}\left(\frac{i b t}{\sqrt{3}}+\frac{a}{3}\right)+i x(t-i) \right)\frac{1}{t^2+1}\mathrm{d}t}_{I(x)}.$$ Now $I(x)$ can be closed in the upper half plane since the contribution along the arc vanishes. Then this $t$ -integral encloses the single essential singularity in the upper half plane at $t=i$ . Hence we have $$I(x)=2\pi i \, {\rm Res} \left(\exp \left(\frac{1}{t^2+1}\left(\frac{i b t}{\sqrt{3}}+\frac{a}{3}\right)+i x(t-i) \right)\frac{1}{t^2+1}\right)\Bigg|_{t=i} \, $$ where $$\exp \left(\frac{1}{t^2+1}\left(\frac{i b t}{\sqrt{3}}+\frac{a}{3}\right)+i x(t-i) \right)\frac{1}{t^2+1}$$ can be written as the series $$\sum_{n,m=0}^{\infty}\frac{(ix)^n}{n!}(t-i)^n\frac{1}{m!} \left(\frac{i b t}{\sqrt{3}}+\frac{a}{3}\right)^m\frac{1}{[(t+i)(t-i)]^{m+1}}$$","['integration', 'complex-integration', 'residue-calculus']"
4341806,How to plot the graph of this given set in $\mathbb{R}^2$,"Can anyone help me plot the graph of the set $$B=\{(a_1+a_2, a_1a_2):a_1,a_2\in\mathbb{R},|a_1|<1,|a_2|<1\}.$$ Ok so I can shade the region $|x|<2$ and $|y|<1$ . But do not know how to proceed after that. And above sets is real roots of the equation $x^2-\alpha x+\beta$ whose modulus is less than 1.
Will it be the intersection of the shaded area above and the parabola $x^2-4y>0$ ?","['multivariable-calculus', 'graphing-functions', 'real-analysis']"
4341813,Dilemma in a proof of the Implicit Function Theorem,"I was studying the Implicit Function Theorem (source Diferencijalni račun funkcija više varijabli by I. Gogić, P. Pandžić and J. Tambača, pages 91-92) and I stumbled across something. First, I'm going to write the statement and proof as given in the script verbatim from Croatian: Let $A\subseteq\Bbb R^n\times\Bbb R^m$ be an open set, $F:A\to\Bbb R^m$ of the class $C^p,p\ge 1.$ Suppose $(x^0,y^0)\in A$ satisfies $F(x^0,y^0)=0_{\Bbb R^m}$ and let $\frac{\partial F}{\partial y}(x^0,y^0)\in M_m(\Bbb R)$ be a regular matrix. Then, there is an open neighbourhood $U\subseteq\Bbb R^n$ of $x^0$ and an open neighbourhood $V\subseteq\Bbb R^m$ of $y^0$ and a unique function $f:U\to V$ of the class $C^p$ s. t. $F(x,f(x))=0_{\Bbb R^m},x\in U.$ Note $$\frac{\partial F}{\partial y}(x,y)=\begin{bmatrix}\frac{\partial F_1}{\partial y_1}(x,y)&\ldots&\frac{\partial F_1}{\partial y_m}(x,y)\\\vdots&\ddots&\vdots\\\frac{\partial F_m}{\partial y_1}(x,y)&\ldots&\frac{\partial F_m}{\partial y_m}(x,y)\end{bmatrix}$$ proof : The idea of the proof is to apply the Inverse Function Theorem so, let's define a function $G:A\to\Bbb R^n\times\Bbb R^m$ by formula $G(x,y)=(x,F(x,y)).$ Since $F$ is of the class $C^p,G$ is also of the class $C^p$ . The Jacobian matrix of the function $G$ is given by $\nabla G(x,y)=\begin{bmatrix}1&\ldots&0&0&\ldots&0\\\vdots&\ddots&\vdots&\vdots&\ddots&\vdots\\0&\ldots&1&0&\ldots&0\\\frac{\partial F_1}{\partial x_1}(x,y)&\ldots&\frac{\partial F_1}{\partial x_n}(x,y)&\frac{\partial F_1}{\partial y_1}&\ldots&\frac{\partial F_1}{\partial y_m}(x,y)\\\vdots&\ddots&\vdots&\vdots&\ddots&\vdots\\\frac{\partial F_m}{\partial x_1}(x,y)&\ldots&\frac{\partial F_m}{\partial x_n}(x,y)&\frac{\partial F_m}{\partial y_1}(x,y)&\ldots&\frac{\partial F_m}{\partial y_m}(x,y)\end{bmatrix}$ Due to the structure of this matrix, $J_G(x,y)=\det\frac{\partial F}{\partial y}(x,y),$ hence $J_G(x^0,y^0)\ne 0$ so we can apply the Inverse Function Theorem: there is an open neighbourhood $S$ of the point $(x^0,y^0),$ and open neighbourhood $W$ of $G(x^0,y^0)=(x^0,0)$ and the inverse function $G^{-1}:W\to S$ of the class $C^p$ . Since $S$ is open, there is an open neighbourhood $U$ of $x^0$ and an open neighbourhood $V$ of $y^0$ s. t. $U\times V\subset S$ . Since $G^{-1}$ is continuous, the set $Y=G(U\times V)\subseteq W$ is open. Therefore, $G:U\times V\to Y$ is a $C^p$ -diffeomorphism. It holds: $(x,y)=G^{-1}(G(x,y))=G^{-1}(x,F(x,y)), (x,y)\in U\times V.$ For $(x,y)\in U\times V,$ which satisfy $F(x,y)=0\tag 1,$ it follows they are dependent through $(x,y)=G^{-1}(x,0).$ Therefore, we define a function $f:U\to V$ by $$f(x)=\pi_2\circ G^{-1}(x,0),$$ where $\pi_1(x,y)=x,\pi_2(x,y)=y.$ As $\pi_2$ and $G^{-1}$ are of the class $C^p,$ the function $f$ is, too, of the class $C^p$ . We now use the other composition $\begin{aligned}(x,0)&=G(G^{-1}(x,0))\\&=G(\pi_1\circ G^{-1}(x,0),\pi_2\circ G^{-1}(x,0))\\&=(\pi_1\circ G^{-1}(x,0), F(\pi_1\circ G^{-1}(x,0),\pi_2\circ G^{-1}(x,0)))\end{aligned}$ so that we conclude $x=\pi_1\circ G^{-1}(x,0),0=F(\pi_1\circ G^{-1}(x,0),\pi_2\circ G^{-1}(x,0)).$ From the definition of the function $f,$ it follows that $$0=F(x,f(x)),x\in U.\boxed{}$$ I have a question regarding $(1)$ : It says : For $(x,y)\in U\times V$ which satisfy $F(x,y)=0,$ ... we define $f:U\to V$ by $f(x)=\pi_2\circ G^{-1}(x,0).$ I know that, since $Y=G(U\times V)$ is open and $G(x^0,y^0)=(x^0,F(x^0,y^0))=(x^0,0)\in Y,$ there certainly must be some other points of the form $(x,0)\in Y,$ where $x\in U,$ but how do we know $G^{-1}(x,0)$ isn't empty for an arbitrary $x\in U,$ i. e., that $f$ is well-defined $\forall x\in U$ ?","['proof-explanation', 'multivariable-calculus', 'implicit-function-theorem', 'real-analysis']"
4341824,"Profinite completion of integers, $\hat{\mathbb{Z}}$, is isomorphic to the product over primes of the $p$-adic integers $\mathbb{Z}_p$","I've been trying to show that the profinite completion of the integers, $\hat{\mathbb{Z}}$ is isomorphic to the product over $p$ of the $p$ -adic integers. But I'm kind of stuck. Here is what I got so far. By the Chinese Remainder Theorem we have $$\mathbb{Z}/n\mathbb{Z}\simeq \underset{p}\prod \mathbb{Z}/p^{v_p(n)}\mathbb{Z}$$ hence $$\underset{n}\varprojlim\ \mathbb{Z}/n\mathbb{Z}\simeq \underset{n}\varprojlim\underset{p}\prod\mathbb{Z}/p^{v_p(n)}\mathbb{Z}$$ where the inverse limit on the right is taken with respect to the projections, for $m|n$ : $$\underset{p}\prod\mathbb{Z}/p^{v_p(n)}\mathbb{Z}\overset{\sim}\longrightarrow\mathbb{Z}/n\mathbb{Z}\longrightarrow\mathbb{Z}/m\mathbb{Z}\overset{\sim}\longrightarrow\underset{p}\prod\mathbb{Z}/p^{v_p(m)}\mathbb{Z}$$ $$(x_p+p^{v_p(n)}\mathbb{Z})_p\mapsto x+n\mathbb{Z}\mapsto x+m\mathbb{Z}\mapsto (x+p^{v_p(m)}\mathbb{Z})_p$$ where $x+n\mathbb{Z}$ is the unique element in $\mathbb{Z}/n\mathbb{Z}$ such that $x+p^{v_p(n)}\mathbb{Z}=x_p+p^{v_p(n)}\mathbb{Z}$ . We now define the map $$\underset{n}\varprojlim\underset{p}\prod\mathbb{Z}/p^{v_p(n)}\mathbb{Z}\longrightarrow \underset{p}\prod\underset{m}\varprojlim\ \mathbb{Z}/p^{v_p(m)}\mathbb{Z}\ ,\ \{(x_{n,p}+p^{v_p(n)}\mathbb{Z})_p\}_n\mapsto(\{x_{m,p}+p^{v_p(m)}\mathbb{Z}\}_m)_p$$ Where inverse limit on the right is taken with repsect to projections, for $k|m$ , $\mathbb{Z}/p^{v_p(m)}\mathbb{Z}\longrightarrow\mathbb{Z}/p^{v_p(k)}\mathbb{Z}$ . The map clearly does not depend on coset representatives and after messing around for a bit we can show that indeed for fixed $p$ , $\{x_{m,p}+p^{v_p(m)}\mathbb{Z}\}_m$ is an element of $\underset{m}\varprojlim\ \mathbb{Z}/p^{v_p(m)}\mathbb{Z}$ . Hence the map is well defined and it's clearly a bijective ring homomorphism hence an isomorphism. Now it remains to show that for fixed $p$ we have $$\underset{m}\varprojlim\ \mathbb{Z}/p^{v_p(m)}\mathbb{Z}\simeq\underset{m}\varprojlim\ \mathbb{Z}/p^m\mathbb{Z}$$ This is where I am having problems. The orderings on $m$ are not compatible and I haven't been able to construct a well defined map so far. I also thought of constructing a chain of isomorphisms $$\underset{m}\varprojlim\ \mathbb{Z}/p^{v_p(m)}\mathbb{Z}\simeq\underset{m}\varprojlim\ \mathbb{Z}/p^{v_p(m!)}\mathbb{Z}\simeq\underset{m}\varprojlim\ \mathbb{Z}/p^m\mathbb{Z}$$ to try and make the orderings on $m$ align a bit better but haven't had any luck in defining suitable maps. Note that I am not looking for categorical proofs regarding the isomorphism in question. I've already been through some of those. I am looking for hints on construcing explicit maps Edit after Eric Wofsey's response : We define the ring homomorphism $$\theta:\underset{m}\varprojlim\ \mathbb{Z}/p^{v_p(m)}\mathbb{Z}\longrightarrow\underset{r}\varprojlim\ \mathbb{Z}/p^r\mathbb{Z}\ ,\ (x_m+p^{v_p(m)}\mathbb{Z})_m\mapsto (x_{p^r}+p^r\mathbb{Z})_r$$ This is well defined by construction since $p^r|p^{r+1}$ hence by the inverse system of $\underset{m}\varprojlim\ \mathbb{Z}/p^{v_p(m)}\mathbb{Z}$ we have that $x_{p^{r+1}}+(p^{v_p(p^r)}=p^r)\mathbb{Z}=x_{p^r}+(p^{v_p(p^r)}=p^r)\mathbb{Z}$ which is exactly what is required for $(x_{p^r}+p^r\mathbb{Z})_r$ to be an element of $\underset{r}\varprojlim\ \mathbb{Z}/p^r\mathbb{Z}$ . Now let $x:=(x_m+p^{v_p(m)}\mathbb{Z})_m\in ker(\theta)$ then $x_{p^r}\in p^r\mathbb{Z}, \forall r\geq 1$ . Now if $p\nmid m$ , $v_p(m)=0$ hence $x_m\in p^{v_p(m)}\mathbb{Z}=\mathbb{Z}$ . If $p|m$ then $v_p(m)=r$ for some $r\geq1$ . Then since $p^r|m$ we have by the inverse system of $\underset{m}\varprojlim\ \mathbb{Z}/p^{v_p(m)}\mathbb{Z}$ that $x_m+p^{v_p(p^r)}\mathbb{Z}=x_{p^r}+p^{v_p(p^r)}\mathbb{Z}=0+p^r\mathbb{Z}$ Hence $x_m\in p^r\mathbb{Z}=p^{v_p(m)}\mathbb{Z}$ . Hence $x=0$ in $\underset{m}\varprojlim\ \mathbb{Z}/p^{v_p(m)}\mathbb{Z}$ , hence $\theta$ injects. Now given an element $y:=(y_r+p^r\mathbb{Z})_r\in\underset{r}\varprojlim\ \mathbb{Z}/p^r\mathbb{Z}$ , we consider the element $x:=(x_m+p^{v_p(m)}\mathbb{Z})_m\in\underset{m}\prod\mathbb{Z}/p^{v_p(m)}\mathbb{Z}$ where $$x_m:=y_{v_p(m)},\ \text{if}\ v_p(m)\geq 1$$ $$x_m:=0,\ \text{if}\ v_p(m)=0$$ Now take $n|m$ then if $p\nmid n$ we have that $x_m+p^{v_p(n)}\mathbb{Z}=x_m+\mathbb{Z}=0+\mathbb{Z}=x_n+p^{v_p(n)}\mathbb{Z}$ . So assume $p|n$ hence $p|m$ which implies that $x_m=y_{v_p(m)}\ ,\ x_n=y_{v_p(n)}$ . Then by the inverse system of $\underset{r}\varprojlim\ \mathbb{Z}/p^r\mathbb{Z}$ we have that $x_m+p^{v_p(n)}\mathbb{Z}=y_{v_p(m)}+p^{v_p(n)}\mathbb{Z}=y_{v_p(n)}+p^{v_p(n)}\mathbb{Z}=x_n+p^{v_p(n)}\mathbb{Z}$ Hence indeed we have that the element $x$ belongs in $\underset{m}\varprojlim\ \mathbb{Z}/p^{v_p(m)}\mathbb{Z}$ and furthermore, it maps to $y$ under $\theta$ . Hence $\theta$ also surjects and we have an isomorphism. So overall we have shown that $$\hat{\mathbb{Z}}=\underset{n}\varprojlim\ \mathbb{Z}/n\mathbb{Z}\overset{\sim}\longrightarrow\underset{n}\varprojlim\ \underset{p}\prod\mathbb{Z}/p^{v_p(n)}\mathbb{Z}\overset{\sim}\longrightarrow\underset{p}\prod\underset{m}\varprojlim\ \mathbb{Z}/p^{v_p(m)}\mathbb{Z}\overset{\sim}\longrightarrow\underset{p}\prod\underset{r}\varprojlim\ \mathbb{Z}/p^r\mathbb{Z}=\underset{p}\prod\mathbb{Z}_p$$ $$\{x_n+n\mathbb{Z}\}_n\mapsto \{(x_n+p^{v_p(n)}\mathbb{Z})_p\}_n\mapsto (\{x_m+p^{v_p(m)}\mathbb{Z}\}_m)_p\mapsto (\{x_{p^r}+p^r\mathbb{Z}\}_r)_p$$ Hence we end up with the isomorphism $$\hat{\mathbb{Z}}\overset{\sim}\longrightarrow\underset{p}\prod\mathbb{Z}_p$$ $$\{x_n+n\mathbb{Z}\}_n\mapsto (\{x_{p^r}+p^r\mathbb{Z}\}_r)_p$$","['number-theory', 'abstract-algebra', 'algebraic-number-theory', 'profinite-groups']"
4341872,Conditional probability exercise is my reasoning here correct?,"Suppose we toss a fair coin infinite number of times so we get the independent and identically distributed r.v sequence $(X_n)_{n \ge 1}$ such that : $P(X_n=0 (tails))=P(X_n=1(head))=0.5$ Let $Y_n=X_n+X_{n-1}$ for $n \ge 2$ Calculate the probabilities $P(Y_4=0 \;|\; Y_2=0,Y_3=1)$ and $P(Y_4=0 \;|\;Y_3=1)$ My attempt: For $n \ge 2$ , $Y_n=0$ iff $X_n=X_{n-1}=0$ and $Y_n = 1$ iff $X_n=1$ or $X_{n-1}=1$ Based on that: $P(Y_4=0 \;|\; Y_2=0;Y_3=1)=0$ because  if $Y_4=Y_2=0$ then $Y_3$ has to be $0$ . $P(Y_4=0 \;|\; Y_3=1)=\cfrac{P(X_2=1)}{P(X_2=1)+P(X_3=1)}=0.5$ Is my reasoning correct? If not, what's my mistake?","['conditional-probability', 'probability-distributions', 'probability-theory', 'probability', 'random-variables']"
4341882,In what scenarios does one have a closed form equation for the Pearson Correlation - especially s.t. changing parameters changes correlation?,"I want to understand when I can create distributions and random variables (e.g. functions) such that when I change the parameters (or distributions) that specify the distribution (or the parameters of the functions that output random variables) I have changes in the correlation. A closed form equation for the correlation would be really nice for example, so that if I change a parameter of the distribution it's clear the correlation would change. So we have: $$ corr(X,Y) = \mathbb E_{X, Y \sim p(x, y \mid \theta_X, \theta_Y)}[ Z_X Z_Y ]$$ where $Z_X, Z_Y$ are the standardized r.v.s (e.g. $(x - \mu_x) / \sigma_x)$ . My hope is to get conditions for where the above varies smoothly or I can control how it varies based on the functions for the random variables I choose and the distributions I choose. I know that the above integral can be complicated - especially for closed form things based on a question I previously asked that required Lebgegue integration - so this is why I am asking here. It would also be nice to be able to somehow separate the distributions $p(x \mid \theta_X), p(y \mid \theta_Y)$ but have them be correlated e.g. they are both normal with similar means or their support sets are similar...or something. The reason for this is that $X, Y$ are in fact two ""tasks"" that are different but correlated, e.g. classifying handwritten digits vs classifying language characters. The tasks can be sampled separately, but in fact are highly correlated. Thus, in a way you can think of $Z_X, Z_Y$ as a standardized (regression) prediction of a model on two related tasks I suppose. How does one define (adjustable) random variables and distributions such one can vary smoothly the correlation? Current attempt ( inspired from MIT's lecture L12.10 RES.6-012 ): Define $X = Z + X'$ and $Y = Z + Y'$ where $Z, X', Y'$ are independent and centered. The R.Vs are correlated by $Z$ . Thus the correlation (closed form) equation is: $$  corr(X,Y) = \frac{Cov(X,Y)}{\sigma_X \sigma_Y} = \frac{\mathbb E[X Y]}{\sigma_X \sigma_Y} = \frac{\sigma^2_Z}{\sigma_X \sigma_Y} = \frac{\sigma^2_Z}{(\sigma_{X'} + \sigma_Z )(\sigma_{Y'} + \sigma_Z)}$$ which makes we wonder if this is really what I am looking for...which feels weird because I am not even ""sampling"" two tasks as I described above. Also, this made me think that whatever distribution and rvs/functions I choose, the values will be wrt their standard deviation. So the closed form equation depends on the standard deviation value that comes form the closed form of the distribution I choose. Similarly, it made me realize that the curcial thing for getting closed form equation is computing the covariance, really in closed form. Or the computing $\mathbb E[Z_X Z_Y]$ (or $\mathbb E[X Y]$ ) - a closed form for an integral/expecation of product of values.","['statistics', 'covariance', 'probability-distributions', 'correlation', 'probability']"
4341893,Why is this stochastic process a Markov chain?,"Suppose we have an independent sequence $(X_n)$ such that $P(X_n=1)=P(X_n=0)=\cfrac{1}{2}$ Let $M_n=(X_{n-1},X_n)$ . The exercise I'm working on supposes that $(M_n)_{n \ge 2}$ is a markov chain but it didn't make sense to me because if we have for example $M_4=(X_4,X_3)$ , $M_3=(X_3,X_2)$ , $M_2=(X_2,X_1)$ then $(M_n)$ is a markov chain iff $P(M_4=m_4 | M_3=m_3,M_2=(1,1))=P(M_4=m_4 | M_3=m_3)$ but that didn't make sense to me since the value of $M_3$ also depends on $M_2$ since they share $X_2$ . In other words, if we were to change $X_2$ in $M_2$ to $(0,1)$ then wouldn't $M_3$ change? What am I getting wrong exactly? Edit : Other than that problem, I was asked to give and graph the transition matrix $P$ . How am I going to do that given that the state space is $E=\{(0,0),(0,1),(1,0),(1,1)\}$","['conditional-probability', 'probability-distributions', 'markov-chains', 'stochastic-processes', 'probability']"
4341917,Poincare inequality for Poisson random variables,"My question comes from exercise 3.21 in this monograph . Specifically, let $f$ be a real-valued function defined on the set of non-negative integers and denote its ""discrete derivative"" by $Df(x) = f(x+1)-f(x)$ . Let $X$ be a Poisson random variable with parameter $\mathbb E[X] = \mu$ . Prove that $$\mathrm{Var}(f(X)) \leq \mu\,\mathbb E\left[(Df(X))^2\right].$$ The hint is to use the Efron-Stein inequality and the infinite divisibility of the Poisson distribution. My attempt: the infinite divisibility of the Poisson distribution means that we can write $X := \sum_{i=1}^n X_i$ , where $X_i$ 's are i.i.d. with Poisson distribution $\mathrm{Poisson}(\mu/n)$ . However, I do not see how the Efron-Stein inequality (Theorem 3.1) $$\mathrm{Var}(f(X)) \leq \sum_{i=1}^n \mathbb{E}\left[\left(f(X) - \mathbb{E}^{(i)}f(X)\right)^2\right] $$ will lead us to the advertised inequality. Here the conditional expectation operator $\mathbb{E}^{(i)}$ means that we are taking the expectation of $f(X)$ with respect to $X_i$ (while keeping $X_1,\ldots,X_{i-1},X_{i+1},\ldots,X_n$ fixed). Any help will be greatly appreciated!","['poisson-distribution', 'inequality', 'probability']"
4341919,The differential of inclusion from S^{2} to R^{3},"I've found this question on Loring Tu's ""Introduction to Manifolds"", numbered 11.4 and titled ""Differential of the inclusion map"" On the upper hemisphere of the unit sphere $S^{2}$ we have the coordinate map $\phi = (u,v)$ , where $u(a,b,c) = a, \ v(a,b,c) = b$ Let $i: S^{2} \to \mathbb{R^3}$ be the inclusion, and $i_*$ it's differential. Calculate: $$
i_*(\frac{\partial}{\partial u}), \ \ 
i_*(\frac{\partial}{\partial v}) 
$$ in terms of $ \frac{\partial}{\partial x}, \frac{\partial}{\partial y}, \frac{\partial}{\partial z}$ , where x,y,z are the standard coordinates I've fiddled with it a bit, and the closest I've came is something like: $$i_*(\frac{\partial}{\partial u})(x) = (\frac{\partial}{\partial u})(x \circ i)  = (\frac{\partial}{\partial u}) (u) = 1\\
i_*(\frac{\partial}{\partial u})(y) = (\frac{\partial}{\partial u})(y \circ i) = 0 \\
i_*(\frac{\partial}{\partial u})(z) = (\frac{\partial}{\partial u}) (z \circ i)
$$ I can't really wrap my head around
calculating $(\frac{\partial}{\partial u}) (z \circ i)$ . Some guidance would be very much appreciated.","['differential', 'smooth-manifolds', 'differential-geometry']"
4341941,"About $\rho(x, y)= \begin{cases}\mathrm{d}(x, y) & \mathrm{d}(x, y)<1 \\ 1 & \mathrm{~d}(x, y) \geq 1\end{cases}$ in a metric space","let $(X,d)$ be a metric space now define $\rho(x, y)= \begin{cases}\mathrm{d}(x, y) & \mathrm{d}(x, y)<1 \\ 1 & \mathrm{~d}(x, y) \geq 1\end{cases}$ now which of following options is false ? $(X,d)$ and $(X,\rho)$ have same open sets . $(X,d)$ and $(X,\rho)$ have same compact  sets . $(X,d)$ and $(X,\rho)$ have same connected sets . $(X,d)$ and $(X,\rho)$ have same bounded sets . Two metrics $d,d'$ are called equivalent if there exist constants $c,C>0$ such that for all $x,y \in X$ : $$ c d(x,y) \le d'(x,y) \le C d(x,y)$$ first of all $X$ has infinite element because for finite case every metric on $X$ is equal to discrete metric .so let $X= \mathbb{R}$ and $ d(x,y)=|x-y|$ so $\mathbb Q$ is not bounded in $(X,d)$ but in $(X,\rho)$ is bounded so ""4"" is false . how we can show another options are true ?","['metric-spaces', 'examples-counterexamples', 'real-analysis', 'general-topology', 'equivalent-metrics']"
4341956,Closed-form expression of the $n$-th derivative of $f_k(x)=e^{-cx}(c(x_u-x))^k$,"We have a function $$f_k(x)=e^{-cx}(c(x_u-x))^k,$$ that is defined for all $x\in[0,x_u]$ . We know that $k\in\mathbb{N}^+$ and $c>0$ is a given constant. We are interested in having a closed-form expression (if possible!) of the $n$ -th derivative of $f_k(x)$ . So far I started from $n=1$ to see if there's any pattern and I was thinking maybe we can formulate it as a series but I was not successful. I also tried using Mathematica but again, I'm not sure how to use the output there. Can anyone please give me a hint?","['calculus', 'derivatives']"
4341962,What are the values of $a$ and $b$ so that $\Bbb Z_2\times\Bbb Z_3\times\Bbb Z_4\times\Bbb Z_9$ is isomorphic with $\Bbb Z_a\times\Bbb Z_b$?,"I'm studying group theory, (at basic level), and i got this problem: Find all pairs $(a,b)$ of positive integers so that: $$\Bbb Z_2\times\Bbb Z_3\times\Bbb Z_4\times\Bbb Z_9$$ is isomorphic with $$\Bbb Z_a\times\Bbb Z_b.$$ I know that $\Bbb Z_2\times\Bbb Z_3$ is cyclic, so is isomorphic with $\Bbb Z_6$ , and so $\Bbb Z_9\times\Bbb Z_4$ with $\Bbb Z_{36}$ , but in this way I just found two isomorphisms, $\Bbb Z_6\times\Bbb Z_{36}$ and $\Bbb Z_{18}\times\Bbb Z_{12}$ . Is it right? Thank you so much.","['group-isomorphism', 'cyclic-groups', 'finite-groups', 'group-theory', 'abelian-groups']"
4341964,Where am I wrong that I can prove every subgroup is normal?,"I appreciate helps for figuring out the problem of the following argument. I know it's certainly not the case that every subgroup of a group is normal. But I cannot find out where is this wrong. Let $G$ be a group and let $H$ be a subgroup of it. Then $G$ acts on left cosets of $H$ by left multiplication. Let $ \Pi : G \to \mathrm{Sym}(G/H)$ be the homomorphism that denotes the action. I think $ H = \ker(\Pi) $ , because for any left coset $aH$ and any $ h \in H$ , we have $ h \cdot aH = aH $ since $ h a a^{-1} \in H$ , and therefore $ H \subset \ker(\Pi) $ . Also if $ g \in G \setminus H$ , then $g \cdot 1H = gH \neq H$ , so $ H = \ker(\Pi)$ . And so $H$ is the kernel of a homomorphism so it's normal. Thanks in advance.","['fake-proofs', 'solution-verification', 'normal-subgroups', 'group-theory', 'group-actions']"
4341981,Proof of weak Mordell-Weil Theorem,"I am studying Rational Points on Elliptic Curves by Silverman and Tate. They only prove the weak Mordell-Weil Theorem in the case when the curve $y^2=f(x)$ has a point of order $2$ , i.e., $f(x)$ has a rational root. This means $f(x)$ is reducible, so I suppose it's a quite special case. The authors say the general case needs some knowledge about ideal class group and units. However all the proofs I can find (for example the proof in the other book by Silverman) seem to also use group cohomology or something similar. Is there a proof that resembles the proof in Silverman and Tate? Namely, if $\alpha$ is a root of $f(x)$ and we view $E:y^2=f(x)$ as a curve over $K=\mathbb{Q}(\alpha)$ (so that we have the $2$ -isogeny), can we define a suitable map from $E(K)$ to $K^*/(K^*)^2$ and show that $E(K)/E(K)^2$ is finite? Maybe instead of $K^*$ we should use the group of fractional ideals? It seems like the same proof goes through in case $K$ has class number $1$ , although that may be rare. The authors motivate the $2$ -isogeny from $E:y^2=x^3+ax^2+bx$ to $\overline{E}:y^2=x^3-2ax^2+(a^2-4b)x$ using uniformization by Weierstrass $\wp$ function; it ultimately comes from cutting the fundamental region of the lattice in half. But since lattices correspond bijectively with elliptic curves in Weierstrass normal form, shouldn't this work for all elliptic curves, not just those with rational roots?","['number-theory', 'algebraic-geometry', 'elliptic-curves']"
4342000,Equivalent characterization of Poisson processes,"Let $\alpha>0$ . We usually say that an $\mathbb N_0$ -valued process $(N_t)_{t\ge0}$ on a filtered probability space $(\Omega,\mathcal A,(\mathcal F_t)_{t\ge0},\operatorname P)$ is Poisson with intensity $\alpha$ if $N_t-N_s$ is independent of $\mathcal F_s$ for all $t\ge s\ge0$ ; $N_t\sim\operatorname{Poi}(t\alpha)$ for all $t\ge0$ . Now I've encountered a different definition replacing (2.) by $f(t):=\operatorname E[N_t]<\infty$ for all $t\ge0$ ; and $f$ is continuous. It's clear to me that (2.) implies (3.) and (4.). But how can we show the converse? I'm aware of the following fact, which might be helpful to show the desired implication: If $g:[0,\infty)\to\mathbb C$ is right-continuous and $g(s+t)=g(s)g(t)$ , then $g(t)=g(1)^t$ for all $t\ge0$ .","['stochastic-analysis', 'measure-theory', 'probability-theory', 'poisson-process']"
4342018,Why is the Hodge Decomposition profound?,"The Hodge Decomposition is the following result: Let $X$ be a smooth projective variety over $\mathbf{C}$ . For every integer $k \geq 0$ , we have a direct sum decomposition $$H^k(X, \mathbf{C}) \cong \bigoplus_{p+q=k} H^{p,q}(X).$$ My question is : why is this result profound? What ""deep underlying truth"" about the topology of complex varieties does this theorem reveal? My guess is that the Hodge decomposition relates the topological structure of $X$ with the complex structure of $X$ . The left side of the above direct sum is defined purely topologically (it is singular cohomology). Whereas the right side is defined in terms of the complex structure (it is Dobeault cohomology). So the theorem says that the ""complex structure of $X$ sees the entire topological structure of $X$ ."" Is this the right way to view this theorem, or is there another way to see its importance?","['hodge-theory', 'manifolds', 'differential-geometry']"
4342047,Is true that $\bigcup\Big(\bigcup( A\times B)\Big)=A\cup B$ for any pair of sets $A$ and $B$?,"So I'd like to know if the identity $$
\bigcup\Big(\bigcup (A\times B)\Big)=A\cup B
$$ is true. So I tried to prove it by the following arguments. First of all I observed that any element of $A\times B$ is a set such as $$
\big\{\{a\},\{a,b\}\big\}
$$ for any $a\in A$ and for any $b\in B$ . Now if $x$ is an element of $\bigcup(A\times B)$ then there exists $X\in A\times B$ such that $x\in X$ so that $$
\Big(x=\{a\}\,\,\,\text{for any}\,a\in A\Big)\vee\Big(x=\{a,b\}\,\,\,\text{for any}\,a\in A\,\,\,\text{and for any}\,b\in B\Big)
$$ Finally if $x$ is an element of $\bigcup\Big(\bigcup(A\times B)\Big)$ then there exists $X\in\bigcup A\times B$ such that $x\in X$ so that $$
\Big(x\in\{a\}\,\,\,\text{for any}\,a\in A\Rightarrow x=a\in A\Big)\vee\Big(x\in\{a,b\}\,\,\,\text{for any}\,a\in A\,\text{and for any}\,b\in B\Rightarrow x=a\in A\vee x=b\in B\Big)
$$ which means that $$
\bigcup\Big(\bigcup(A\times B)\Big)\subseteq(A\cup B)
$$ So unfortunately I am not able to prove the conversely: it seems to me that the preceding arguments can be applied conversely but I am hesitant about now so that I deliberated to post this question. Anyway it is well know that $$
A\times B\subseteq\mathcal P\Big(\mathcal P(A\cup B)\Big)
$$ and so I think that the above argumentation can be avoided but I am yet hesitant about. So could anyone help me, please?","['elementary-set-theory', 'solution-verification', 'examples-counterexamples']"
4342057,"A more succinct group object diagram (all axioms in one connected diagram), questions about its properties...","Here is the definition of group object from nLab.  They give 3 associated maps $* \xrightarrow{1} G$ , $m: G^2 \to G$ , and $-^{-1}: G \to G$ and require 3 commutative diagrams to complete the axioms for a ""group"". Here, however, I've managed to condense these down into one (connected) diagram: We say that $G \in \text{Ob}(C)$ (a category with binary products and a terminal object $*$ ) forms a group object precisely when the above diagram commutes. Note that $1\times \text{id}$ is notation implying that we use the map $1 =$ the composition of the two maps $(G \xrightarrow{!} * \xrightarrow{1} G)$ , and the appropriate $\Delta$ is used wherever it is needed.  I like the simpler notation that I've used. Anyway, this diagram is more economical than the combined other 3 since we re-use arrows such as $m\times \text{id}$ at least twice (two commutative shapes described - triangles or squares). So, that's background and what I've done on the problem so far.  My questions are: Can we define a group object to be an object $G$ such that the above diagram commutes when we replace each $m \times \text{id}, 1\times \text{id}, \text{id}\times -^{-1} \times \text{id}$ with  general morphisms $f, g, h$ in the category $C$ ( $G \in \text{Ob}(C)$ )?  Or can you only recover a group object if you define these edges using products, the diagonal morphism, etc, as shown? In either case, is $G$ the limit of a commutative square (the outer one), since a cone seems to be formed.","['limits-colimits', 'category-theory', 'morphism', 'abstract-algebra', 'group-theory']"
4342074,$f\in \mathcal{C}^{\infty}_{c}(\mathbb{R})$ such that $f(0)=0$ there exists $g\in\mathcal{C}^{\infty}_{c}(\mathbb{R})$ such that $f(x)=xg(x)$,Let $f\in \mathcal{C}^{\infty}_{c}(\mathbb{R})$ such that $f(0)=0$ I want to show that there exists $g\in\mathcal{C}^{\infty}_{c}(\mathbb{R})$ such that $f(x)=xg(x)$ . For this I tried to consider the following function $$ g(x)=\frac{f(x)}{x} \qquad g(0)=f'(0)$$ But I cannot show that $g\in \mathcal{C}^{\infty}_{c}(\mathbb{R})$ . Does this function check the hypotheses?,"['general-topology', 'distribution-theory']"
4342099,"Is $\forall x,\exists y (x \ge y^2 \Rightarrow x>0)$ same as $\forall x (\exists y: x\ge y^2\Rightarrow x > 0)$","The first statement: $\forall x \exists y (x\geq y^2  \Rightarrow x > 0)$ The Second statement: $\forall x (\exists y: x\geq y^2 \Rightarrow x > 0)$ Actually, my question is this: In the first statement, there exists $y$ for every $x$ such that what's in the parenthesis holds but in the second statement, "" $\exists y$ "" is placed inside parenthesis. Does it make any difference to the first statement?","['quantifiers', 'logic', 'discrete-mathematics', 'computer-science']"
4342117,Contour integral $\oint_{|z|=1}\frac{z^2\sin(1/z)}{z-2}dz$,"How do I compute this integral? $$\oint_{|z|=1}\frac{z^2\sin(1/z)}{z-2} \, dz$$ I tried substituting $1/z$ with $z$ and ended up with $$\oint_{|z|=1}\frac{\sin(z)}{z^3(1-2z)} \, dz$$ At this point I thought of using the residue theorem and got $2i\pi(2-4\sin(\frac{1}{2}))$ but the correct answer should be $\frac{i\pi}{6}$ . Can someone help me?","['complex-analysis', 'contour-integration', 'complex-integration']"
4342127,Limit Laws for Composite Functions,"There are two rules I've seen for composite Limits. The first one is: If $f(x)$ is continuous at $x=b$ and $\newcommand{\limto}[2]{\lim\limits_{{#1}\to{#2}}}\limto xa g(x)=b$ then, $$\limto xa f(g(x))= f \left( \limto xa g(x)\right) = f(b).$$ The second one is: Assume that $g(x)$ and $f(x)$ are two functions. Assume that the domain of $g(x)$ contains an open interval containing $a$ , with exception of possibly $a$ , and that the domain of $f(x)$ contains an interval containing $b$ , with exception of possibly $b$ . Furthermore assume that for some number $L$ $$\limto xa g(x)=b \qquad\text{and}\qquad \limto yb f(y)=L.$$ Then $$\limto xa f\circ g(x) = \limto yb f(y) = L.$$ I was wondering if these are saying the same thing, with different variables, or if the 1st case is just a special case of the 2nd.  It seems to me like they have different conditions (continuity vs a limit existing). Is there a way to remember both of these in one rule?","['limits', 'calculus', 'functions', 'function-and-relation-composition']"
4342129,"$K(x,t)=2\pi\sum_{n=1}^\infty n\sin(n\pi x)e^{-n^2\pi^2t}$ satisfies some ""pseudo-good-kernel"" properties","Let $f\in C([0,\infty))$ . Define $$K(x,t)=2\pi\sum_{n=1}^\infty n\sin(n\pi x)e^{-n^2\pi^2t},\qquad x\in[0,1],\  t>0.$$ Show that $$\lim_{x\to0^+}\int_0^tK(x,t-\tau)f(\tau)\,d\tau=f(t), \qquad t>0.$$ I can show the result by assuming that $$\lim_{x\to0^+}\int_0^tK(x,t-\tau)\,d\tau=1,\tag{1}$$ and $$\int_0^1 |K(x,t)|\,dt<\infty \text{ for all } x \text{ close to }0.\tag{2}$$ For any $\varepsilon>0$ , we can find $\delta>0$ so that $|f(\tau)-f(t)|<\varepsilon$ for all $\tau\in(t-\delta,\delta)$ . In $[\delta,t)$ , the series defining $K$ is convergent absolutely and we can change the order of limitation and integration to get $$\lim_{x\to0^+}\int_0^{t-\delta}K(x,t-\tau)f(\tau)\,d\tau=0.$$ As for the integration in $(t-\delta,t)$ , we have $$\int_{t-\delta}^t |K(x,t-\tau)||f(\tau)-f(t)|\,d\tau\leq \varepsilon\int_{t-\delta}^t |K(x,t-\tau)|\,d\tau\lesssim \varepsilon,$$ by $(2)$ . And now the result follows from $(1)$ . I have no idea how to show $(1)$ and $(2)$ . It seems that we need to carefully use the definition of $K$ . For example, if we simply apply Fubini to $(2)$ , we will get a divergent integral. Any help would be appreciated!","['harmonic-analysis', 'fourier-analysis', 'convolution', 'real-analysis']"
4342138,Connection between the Gamma function and gamma distribution,"The Gamma function is a generalization of the factorial ( $\Gamma(n)=(n-1)!$ ). It is also the normalizing constant for the Gamma distribution and this happens to be (when its rate parameter is $1$ ) the time of the $n$ -th event in a standard Poisson process (with rate $1$ ). Is there any connection between these two facts? Perhaps relating to the fact that if we switched the order of the events in the Poisson process, the time until the $n$ -th event wouldn't change?","['gamma-function', 'gamma-distribution', 'probability']"
4342195,Asymptotic equivalence and kth derivative,"Let $f(x)=\log h(x)$ be a real analytic function on the open set $(0,\infty)$ . My question: If $f(x)\sim g(x)$ as $x\to 1$ where $g(x)$ is real analytic on $(0,\infty)$ . Prove that there is a constant $c$ such that $f^{(k)}(x)\sim c g^{(k)}(x)$ as $x\to 1$ . Next how do we find the value of this constant $c$ ? My try: Since $f(x)\sim g(x)$ as $x\to 1$ , so we have $$\lim_{x\to 1}\frac{f(x)}{g(x)}=1$$ Since $f$ is analytic at $x=1$ so it can be written as a power series $$f(x)=f_0+f_1(x-1)+f_2(x-1)^2+... $$ in an appropriate neighbourhood of $1$ .
Also since $g$ is analytic at $x=1$ so it can be written as a power series $$g(x)=g_0+g_1(x-1)+g_2(x-1)^2+... $$ in an appropriate neighbourhood of $1$ .
Since $f(x)\sim g(x)$ as $x\to 1$ , so $g_0=f_0$ . Since $f$ and $g$ are analytic, their derivatives are analytic as well and near $x=1$ they are given by the term-wise differentiated power series. Thus we have $$\lim_{x\to 1}\frac{f'(x)}{g'(x)}=\lim_{x\to 1} \frac{f_1+2f_2(x-1)+...}{g_1+2g_2(x-1)+...} $$ So we have $$\lim_{x\to 1}\frac{f'(x)}{g'(x)}=\frac{f_1}{g_1}$$ Similarly we get $$\lim_{x\to 1} \frac{(f(x))^{(k)}}{(g(x))^{(k)}}=\lim_{x\to 1}\frac{\sum_{n=k}^{\infty}\frac{f^{(n)}(1)}{(n-k)!}(x-1)^{n-k}}{ \sum_{n=k}^{\infty}\frac{g^{(n)}(1)}{(n-k)!}(x-1)^{n-k}   } $$ So we have $$\lim_{x\to 1} \frac{(f(x))^{(k)}}{(g(x))^{(k)}}=\frac{f^{(k)}(1)}{g^{(k)}(1)}$$ So taking $c=\frac{f^{(k)}(1)}{g^{(k)}(1)}$ we have $f^{(k)}(x)\sim c g^{(k)}(x)$ as $x\to 1$ .
How do I find $f^{(k)}(1)$ given that $f(x)=\log h(x)$ ?","['asymptotics', 'calculus', 'functions', 'solution-verification', 'taylor-expansion']"
4342224,How can I get distribution function from characteristic function?,"Suppose $F=F(x)$ is distribution function of r.v. $X$ and its characteristic function is $\varphi_X(t)=\int_{-\infty}^\infty e^{itx}dF(x)$ . Then for any $a<b$ where $F$ is continuous at $a,b$ we have $$F(b)-F(a)=\lim_{c\to\infty}\frac1{2\pi}\int_{-c}^c\frac{e^{-ita}-e^{-itb}}{it}\varphi_X(t)\;{\rm d}t$$ I wonder if we can get the distribution function $F(x)=\mathbb P(X\leqslant x)$ directly by taking $a\to-\infty$ of the equation above, say $$F(x)=\lim_{a\to-\infty}\lim_{c\to\infty}\frac1{2\pi}\int_{-c}^c\frac{e^{-ita}-e^{-itx}}{it}\varphi_X(t)\;{\rm d}t$$ If not, how can I calculate $F(x)$ from its characteristic function $\varphi$ ? Well, actually I read a textbook which says $$F(x)=\frac1{2\pi}\lim_{\sigma\to0^+}\int_{(-\infty,x]}{\rm d}y\int_{\mathbb R} {\rm e}^{-ity-\sigma^2t^2/2}\varphi_X(t)\;{\rm d}t$$ but I have no clue why there is a term $e^{-\sigma^2t^/2}$ .","['characteristic-functions', 'measure-theory', 'probability-distributions', 'probability-theory']"
4342246,Is an NVS complete iff it is non meagre?,"Let $V$ be an NVS. We know by Baires theorem that complete $\implies$ nonmeagre. What about the converse? Can we have a non complete space that is nonmeagre or not? For metric spaces the answer is yes we can, just disjoint union a complete space and a meagre space e.g. $\mathbb{Q}\underline{\cup} \mathbb{R}$ , then the $\mathbb{Q}$ part makes it not complete and the $\mathbb{R}$ part makes it nonmeagre. But no similar tactic works for an NVS so I think the answer will be no.","['normed-spaces', 'linear-algebra', 'functional-analysis', 'general-topology', 'baire-category']"
4342256,"Solving the system $x^2+y^2+x+y=12$, $xy+x+y=-7$","I've been trying to solve this system for well over the past hour. $$x^2+y^2+x+y=12$$ $$xy+x+y=-7$$ I've tried declaring $x$ using $y$ ( $x=\frac{-7-y}{y+1}$ ) and solving from there, but I've gotten to $$y^4+3y^3-9y^2-45y+30=0$$ and I don't see how we can get $y$ from here. If anyone sees it, I'd appreciate the help. But if you see a simpler solution please do not hesitate to leave a comment and notify me of such.","['algebra-precalculus', 'systems-of-equations']"
4342305,Application of Slutsky theroem,"If $X_1,...,X_n$ are independent (no need to be  identically distributed), $\sigma_n=Var(\sum_jX_j)$ $$
\sum_{j=1}^n(X_j-EX_j)/\sigma_n \overset{d}{\rightarrow} N(0,1)
$$ then prove: $$
n^{-1}\sum_{j=1}^n(X_j-EX_j)\overset{P}{\rightarrow}0 \Leftrightarrow \lim_{n\rightarrow\infty}\frac{\sigma_n}{n}=0
$$ “First Edit” For the "" $\Rightarrow$ "" part, It seems if "" $\color{red}{X_n\overset{P}{\rightarrow}0,Y_n\overset{d}{\rightarrow}N(0,1) \Rightarrow X_n/Y_n\overset{d}{\rightarrow}0}$ "" is succeed, then I can naturally deduce that conclusion that $\lim_{n\rightarrow\infty}\frac{\sigma_n}{n}=0$ , but I'm not sure if the red part is right. And for the "" $\Leftarrow$ "" part, $\lim_{n\rightarrow\infty}\frac{\sigma_n}{n}=0$ can deduce that $\frac{\sigma_n}{n}\overset{P}{\rightarrow}0$ , so using the slutsky theorem, I have: $$
n^{-1}\sum_{j=1}^n(X_j-EX_j)=\sum_{j=1}^n(X_j-EX_j)/\sigma_n  \cdot \frac{\sigma_n}{n}\overset{P}{\rightarrow}0
$$ Am I right about the above contents? Please help me, thanks a lot! “Second Edit” With exploring deeper in this question, I find a theroem goes like: $$
\text{If g is a continuous mapping, and if } X_n\overset{d}{\rightarrow}X, \text{then } g(X_n)\overset{d}{\rightarrow} g(X)
$$ I'm not sure if I can use this theorem in this problem as follow. Let $g(x)=1/x$ , obviously $g$ is a continuous function. Let $Z_n=\sum_{j=1}^n(X_j-EX_j)/\sigma_n$ and $Z\sim N(0,1)$ , then according to
to the condition, I have $Z_n\overset{d}{\rightarrow} Z$ , so applying the theorem, I get: $$
g(Z_n)=\frac{\sigma_n}{\sum_{j=1}^n(X_j-EX_j)}\overset{d}{\rightarrow}\frac{1}{Z}=g(Z)
$$ and then using the Slutsky theorem, $$
\frac{\sigma_n}{n}=g(Z_n)\cdot n^{-1}\sum_{j=1}^n(X_j-EX_j)\overset{d}{\rightarrow} g(Z)\cdot 0 = 0
$$ so $\frac{\sigma_n}{n}\rightarrow 0$ . Can this work? I'm looking forward to your reply, Thanks!","['self-learning', 'statistics', 'statistical-inference']"
