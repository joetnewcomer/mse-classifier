question_id,title,body,tags
2581129,"$\max a^3+b^3+c^3+4abc$ sub $0\leq a,b,c \le 3/2$ and $a+b+c=3$","Let $S$ be the set of $(a,b,c) \in \mathbb{R}^3$ such that $0\leq a,b,c \leq \frac{3}{2}$ and $a+b+c=3$. Find 
$$
\max_{(a,b,c) \in S} a^3+b^3+c^3+4abc.
$$",['algebra-precalculus']
2581135,Find: $\lim_{x\to\infty} \frac{\sqrt{x}}{\sqrt{x+\sqrt{x+\sqrt{x}}}}.$,"Find: $\displaystyle\lim_{x\to\infty} \dfrac{\sqrt{x}}{\sqrt{x+\sqrt{x+\sqrt{x}}}}.$ Question from a book on preparation for math contests. All the tricks I know to solve this limit are not working. Wolfram Alpha struggled to find $1$ as the solution, but the solution process presented is not understandable. The answer is $1$. Hints and solutions are appreciated. Sorry if this is a duplicate.","['radicals', 'contest-math', 'nested-radicals', 'limits']"
2581143,Gaussian Hilbert space from white noise,"There are two common ways of obtaining Gaussian processes on Hilbert space: either from a random element of a larger space, or directly as a collection of random variables indexed by the Hilbert space. My question involves the passage from the former to the latter. Let $\Phi\to H$ be a rigged Hilbert space (a nuclear space densely embedded in Hilbert space) and let $\Phi'$ denote the continuous dual of $\Phi$. Embed $H$ in $\Phi'$ such that the duality pairing on $\Phi'\times \Phi$ and the inner product of $H$ are consistent. By the Minlos theorem there is a unique Borel measure $\mu$ (which we call the white noise measure ) on $\Phi'$ satisfying
$$
\mu(\exp i\langle \cdot,f\rangle)=\exp \Bigl(-\tfrac12\langle f,f\rangle\Bigr),\qquad f\in \Phi.
$$
A standard Gaussian process on $H$ is a collection of (centered) Gaussian random variables $\{X_f\colon f\in H\}$ defined on a common probability space $(\Omega,\nu)$ satisfying $\text{Cov}(X_f,X_g)=\langle f,g\rangle$. At the formal level, one obtains a standard Gaussian process by setting $(\Omega,\nu)=(\Phi',\mu)$ and $X_f=\langle \cdot,f\rangle$. However, it is not clear that this is well-defined since $\langle \cdot,\cdot\rangle$ was defined on $\Phi'\times \Phi$ and $H\times H$, but not on $\Phi'\times H$. Question. What is an explicit choice of $\{X_f\colon f\in H\}$ and $(\Omega,\nu)$ given $(\Phi',\mu)$ that gives a standard Gaussian process on $H$?","['functional-analysis', 'probability-theory', 'measure-theory', 'stochastic-processes']"
2581157,Polygon circumscribed about circle has higher perimeter than circle?,"It is quite clear that any (not necessarily regular) convex polygon inscribed in a circle has a perimeter which is lower than that of the circle: between any two adjacent vertices of the polygon the length of the circle segment between these two points exceeds that of the line segment, hence the same inequality holds for the entire perimeter. But what about circumscribed polygons? I know that for regular polygons the circle always has a lower perimeter (although I can't think of an elementary argument for that either), but what about general ones?","['circles', 'euclidean-geometry', 'polygons', 'geometry']"
2581178,Permute any two entries in a $n\times n$ matrix.,"Let $A$ be an $8 \times 8$ matrix with integer coefficients. I want to permute two entries in $A$, any two entries as needed: In general, for any two entries $a_j,b_k$ in the matrix is it possible to do this with some matrix $B$ dependent on $a_j,b_k$? I know about permutation matrices, but they only permute entire rows and columns not individual entries. Edit 1: For example: Say I want to permute $x_{12}$ with $x_{33}$, I want a matrix $B$ suh that : $$
\begin{bmatrix}
    x_{11} & x_{12} & x_{13} & \dots  & x_{1n} \\
    x_{21} & x_{22} & x_{23} & \dots  & x_{2n} \\
    x_{31} & x_{32} & x_{33} & \dots  & x_{3n} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    x_{n1} & x_{n2} & x_{n3} & \dots  & x_{nn}
\end{bmatrix}B=
\begin{bmatrix}
    x_{11} & x_{33} & x_{13} & \dots  & x_{1n} \\
    x_{21} & x_{22} & x_{23} & \dots  & x_{2n} \\
    x_{31} & x_{32} & x_{12} & \dots  & x_{3n} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    x_{n1} & x_{n2} & x_{n3} & \dots  & x_{nn}
\end{bmatrix}$$ Thanks. P.s[Moderators]: Edit tags as appropriate, I added as many as made sense to me.","['matrices', 'permutations', 'linear-algebra', 'linear-transformations']"
2581208,Variation of a differential form,"Physicists sometimes get the lagrangian $$\mathcal{L}=-\frac{1}{2}\mbox dA \wedge \star \mbox dA - A \wedge \star J$$ define a functional given by 
$$S(A)=\int_{N_4} \mathcal{L}= \int_{N_4}-\frac{1}{2}\mbox dA \wedge \star \mbox dA - A \wedge \star J$$ and calculate variation of such functional 
$$\delta S(A)(\delta A)=\delta \int_{N_4} \mathcal L = \int_{N_4} \delta \mathcal{L}=\int_{N_4}-\frac{1}{2}\delta(\mbox dA\wedge \star \mbox dA)-\delta A\wedge \star J$$ etc. and they derive Maxwell's equation $d\star F=-\star J$. The functional is well defined - it's simply an integral of a 4-differential form over an oriented manifold (submanifold). But how is the variation defined? If $M$ is Riemannian manifold we've got Riemannian metric and induced inner product $(\cdot, \cdot)$ given by $$(\omega, \eta):=\int_M \left<\omega, \eta \right>\mathrm{dvol}$$ Hence we've got induced norm of a differential form and can define variation of a functional and of a differential form as its Frechet derivative. Alas, in physics, manifold is psudoriemannian manifold hence  $$(\omega, \eta)=\int_M \left<\omega, \eta \right>\mathrm{dvol}$$ is no longer an inner product. We have no norm of a differential form, and without norm we cannot define variations as Frechet derivatives. Moreover! We cannot even define a local extremum of a functional. Can you tell me how is defined variation of such functional, and a differential form? Does it even make sence what physicists are doing?","['semi-riemannian-geometry', 'riemannian-geometry', 'manifolds', 'calculus-of-variations', 'differential-geometry']"
2581210,Prove that $\det(AB-BA)=0$,"Let $A,B$ be two $3 \times 3$ matrices with complex entries such that $$(A-B)^2=O_3$$ Prove that $$\det(AB-BA)=0$$ I tried to prove this with ranks. I denoted $X=A-B$ and thus $X^2=O_3$ which means that $\det X=0$ and $\operatorname{rank}X \leq 2$ . Then, I wrote $AB-BA=(X+B)B-B(X+B)=XB-BX$ and finally I used $\operatorname{rank}(M \pm N) \leq \operatorname{rank}M+\operatorname{rank}N$ and Frobenius's inequality in order to get $$\operatorname{rank}(XB-BX) \leq \operatorname{rank}BX+\operatorname{rank}XB \leq \operatorname{rank}X+\operatorname{rank}BXB$$ and if we knew that $\operatorname{rank}BXB=0$ , the problem would be solved. However, I don't quite know if the latter is true.","['matrices', 'matrix-rank', 'linear-algebra', 'determinant']"
2581229,Is a group which is equal to its derived subgroup necessarily semisimple?,"let $G$ be either a (connected) Lie group or an (connected) algebraic group over a field (which is algebraically closed of characteristic zero). It is well known that if $G$ is semisimple then $G=G'$ when $G'=[G,G]$. Is the converse true?","['algebraic-groups', 'group-theory', 'lie-groups']"
2581231,A differentiable approximation of modulus?,"I'm trying to find a differentiable approximation of the ""fract"" function, which returns the fractional portion of a real number. $y = x-\lfloor x\rfloor$ I have something that works ""ok"", that I got by adapting a bandlimited saw wave. $y=0.5-\frac{sin(2\pi x)+sin(4\pi x)/2+sin(6\pi x)/3+sin(8\pi x)/4+sin(10\pi x)/5}{\pi}$ I can add more harmonics to make the band limited saw wave closer to the actual ""fract"" function, but for my usage case, all these trig function calls are getting pretty expensive. I was curious, are there other (better quality / lower computational complexity) ways to differentiably approximate this function?","['derivatives', 'approximation']"
2581260,Probability of getting heads given that first flip was a head?,"What's the probability of getting heads on the second toss given that the first toss was a head.  (Trying to refresh my probability a bit).  I've seen this analyzed like this: HH 1/4 HT 1/4 TH 1/4 TT 1/4 So since we are given information (Head on first flip), then TT goes away and were are left with: HH 1/3 HT 1/3 TH 1/3 So we could say that HH now has a 1/3 probability.  Should we not also get rid of TH, since we know that the first flip is a head?  So now we have: HH 1/2 HT 1/2","['bayesian', 'probability-theory', 'bayes-theorem', 'probability']"
2581280,The Determinant of a Special Vandermonde Matrix,"Consider the following  Vandermonde matrix 
$$
V_n = \begin{pmatrix}
  1 & x_1 & x_1^2 & \cdots & x_1^{n-2} & x_1^{n-1} \\
  1 & x_2 & x_2^2 & \cdots & x_2^{n-2} & x_2^{n-1} \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
  1 & x_n & x_n^2 & \cdots & x_n^{n-2} & x_n^{n-1}
\end{pmatrix}.
$$ It is well-known [ 1 ] that the determinant of $V_n$ is defined by 
$$
\displaystyle V_n = \prod_{1 \mathop \le i \mathop < j \mathop \le n} \left({x_j - x_i}\right)\tag{1}
$$ Let $V_{n-1}^{(i,j)}$ be a square matrix such that it is obtained by the removing  $i$th row and $j$th column of $V_n$. My question: Is it possible to get a closed-form for the determinant of $V_{n-1}^{(i,j)}$ similar to $(1)$. My try: If $j=n$ then $V_{n-1}^{(i,n)}$ is a Vandermonde matrix and there is a closed-form for its  determinant as $(1)$. Thanks for any suggestions. Edit: I think the general case of the proposed question is as follows; what is the closed-form of the determinant of the next matrix $$
w_n = \begin{pmatrix}
  1 & x_1^{i_1} & x_1^{i_2} & \cdots & x_1^{i_{n-2}} & x_1^{i_{n-1}} \\
  1 & x_2^{i_1} & x_2^{i_2} & \cdots & x_2^{i_{n-2}} & x_2^{i_{n-1}} \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
  1 & x_n^{i_1} & x_n^{i_2} & \cdots & x_n^{i_{n-2}} & x_n^{i_{n-1}}
\end{pmatrix}.
$$
where $i_t$, for $1\leq t \leq n-1$, are positive integer numbers such that 
$i_1<i_2<\cdots<i_{n-2}<i_{n-1}$.","['matrices', 'linear-algebra', 'determinant']"
2581289,"Borel sigma algebra $\mathcal{B} (\mathbb{\bar{R}}) $ generated by $ (a, \infty], a\in \mathbb{R}.$","So I know that the Borel sigma-algebra of  $\mathbb{\bar{R}} $ is generated by open sets in $\mathbb{\bar{R}}$. I will write:
 $\mathcal{B} (\mathbb{\bar{R}}) $ = $\sigma$ ( { open sets in $\mathbb{\bar{R}} $}). Now I have to show that:
 $\mathcal{B} (\mathbb{\bar{R}}) $ = $\sigma$ ( { $(a, \infty]$ |  $ a\in \mathbb{R}$}) . ''$\supset$'' is okay. ''$\subset$'' I would start with the fact: $(a,\infty]^c$ = $[-\infty,a] \in$ $\sigma$ ( { $(a, \infty]$ |  $ a\in \mathbb{R}$}) . Then {$a$} $\in$ $\sigma$ ( { $(a, \infty]$ |  $ a\in \mathbb{R}$}). ( because {$a$} = ( $\bigcap_{j=1} ( a -\frac{1}{j},\infty]) \cap [-\infty,a]) $ Then $[-\infty,a)$ $\in$ $\sigma$ ( { $(a, \infty]$ |  $ a\in \mathbb{R}$}) (because $[-\infty,a) = [-\infty,a]$ \ {$a$}) Then $(a,b)$ $\in$ $\sigma$ ( { $(a, \infty]$ |  $ a\in \mathbb{R}$}) (because $[-\infty,b) \cap (a,\infty] = (a,b) )$ This means $\sigma$ ( { $(a, \infty]$ |  $ a\in \mathbb{R}$}) $\supset$ {open sets in $\mathbb{\bar{R}}$} Now we conclude: $\sigma$ ( { $(a, \infty]$ |  $ a\in \mathbb{R}$}) $\supset$ $\sigma$({open sets in $\mathbb{\bar{R}}$}) = $\mathcal{B} (\mathbb{\bar{R}}) $. Is this okay? Please look very closely.","['borel-sets', 'real-analysis', 'measure-theory']"
2581290,Is there a connection between the euclidean topology and the Zariski Topology on algebraic varieties over $\mathbb{C}$?,"Recently I was thinking about the following: When talking about the sphere $\mathbb{S}^1$ we can topologize it in two different ways. Either as a subspace of the euclidean space $\mathbb{R}^2$ or as a closed (real) algebraic subvariety of $\mathbb{A}^2_\mathbb{R}$. In either case we think about it as the same ""thing"", namely the points $(x,y)\in\mathbb{R}^2$ such that $x^2+y^2=1$. As algebraic geometry loves algebraically closed sets we can also consider the setup in $\mathbb{C}^2$ and note, that $\lbrace(x,y)\in\mathbb{C}^2\vert x^2+y^2=1\rbrace$ can topologized either with the euclidean topology or the Zariski topology, and although these two versions are different as topological spaces, they are the same ""thing"". Of course we don't need to restrict ourselves to this example, but can consider any affine algebraic variety over $\mathbb{C}$ (or even any subset of $\mathbb{C}^n$) with either the Zariski topology or the euclidean topology and feel that they are the same ""thing"". Is there any way to make this idea formal? Or is it maybe even possible to reconstruct the euclidean topology directly from the Zariski topology?","['general-topology', 'soft-question', 'algebraic-geometry']"
2581294,The Hahn-Banach theorem and the axiom of choice,"Let us recall the Hahn-Banach theorem about extensions of linear functionals: Theorem: Let $E$ be a real vector space and $F$ a subspace. If $p:E\to \mathbb{R}$ is a sublinear function, and $g:F\to \mathbb{R}$ is a linear functional on $F$ which is dominated by $p$ on $F$ i.e. $$g(x)\leq p(x)\qquad \forall x\in F$$ then there exists a linear extension $f:E\to\mathbb{R}$ of $g$ to the whole space $E$ , i.e., there exists a linear functional $f$ such that \begin{gather*}
f(x)=g(x)\qquad \forall x\in F,\\
f(x)\leq p(x)\qquad \forall x\in E.
\end{gather*} During the proof of this theorem, we use Zorn's lemma. It has been shown that Hahn-Banach's theorem is not equivalent to the axiom of choice. There is also some work which has been done showing that for a separable Banach space, a more direct proof can be made. An article by Douglas K. Brown and Stephen G. Simpson studies these kinds of questions of equivalence of axioms in a logic oriented framework. For my part, I am looking for a more practical example. Since the proof uses Zorn's lemma, we could (in theory) give a well enough built example of a linear functional which extension is ""not so obvious"". I will elaborate on what I mean by this. As of most of functional analysis is, extending a linear functional is pretty straightforward in the finite dimensional case. For this reason, students do not have much trouble accepting a proof requiring this kind of machinery. Although, seeing how non-trivial linear functionals behave in the infinite dimensional case gave me the idea to look around for a ""simple, yet troubling"" example. Edit: I think I should clarify what I want. The axiom of choice does not help us with the construction of an extension. I want an example where the extended linear functional is (almost) impossible to figure out/write down on paper. This would show students a problematic with non-constructive mathematics, i.e. they are unpractical in a computational framework. I've been doing my part of work, and I've looked up a bit. Here's what I came up with: Let $g:C_c^{\infty}(\mathbb{R})\to \mathbb{R}$ be defined by $$g(\phi)\mapsto \phi(0)$$ where $C_c^\infty(\mathbb{R})$ is the set of compactly supported smooth functions from $\mathbb{R}$ to $\mathbb{R}$ . We may see this real vector space as a subspace of $L^\infty(\mathbb{R})$ . We know that $L^\infty$ is not separable and that it's dual is the space of Radon measures. We could extended $g$ to all of $L^\infty(\mathbb{R})$ using Hahn-Banach. I do not feel completely satisfied by this example because it may be just me who didn't think far enough to see an adequate extension since I did not spend enough time studying Radon measures. So, as a conclusion of this somewhat long post, I am asking for the following: A well built example which clearly shows the problems we may encounter when using the axiom of choice A computation of the extension of the $g$ I've given or a proof that the extension isn't constructible if the computation isn't possible. Any other ideas which could be added as comments on the axiom of choice and Hahn-Banach's theorem, whatever viewpoint it may come from.","['real-analysis', 'hahn-banach-theorem', 'functional-analysis', 'logic', 'axiom-of-choice']"
2581296,Factoring 2 in $\mathbb{Q}(\sqrt{-7})$,"$7$ is a Heegner number, so all numbers in $\mathbb{Q}(\sqrt{-7})$ have a unique factorization. I'm told that: $2$ is not prime in $\mathbb{Q}(\sqrt{-7})$, $3$ is not prime in $\mathbb{Q}(\sqrt{-11})$, $5$ is not prime in $\mathbb{Q}(\sqrt{-19})$, $11$ is not prime in $\mathbb{Q}(\sqrt{-43})$, $17$ is not prime in $\mathbb{Q}(\sqrt{-67})$, $41$ is not prime in $\mathbb{Q}(\sqrt{-163})$. For the first of these, I've tried finding prime numbers $(a+b (1+\sqrt{-7})/2)$ and $(c+d (1+\sqrt{-7})/2)$, with product $2$, but I haven't had luck so far.","['abstract-algebra', 'field-theory', 'complex-numbers']"
2581304,"For which values of $m$ and $n$ is the probability $P_{(A,A)} = 0.5$?","Given are the classes $A$ and $B$. Suppose there are $n$ instances from class $A$ and $m$ instances from class $B$. Let $X$ be all instances. Thus, $X=\{~\{a\}_n, \{b\}_m~\}$. I denote a pair $p$ as a combination of two instances $(i, j$) from $X$, for which $i \neq j$. Thus, pair $(i, j)$ is the same pair as $(j, i)$. Let $P$ be all possible pairs $p$. Thus $|P| = \frac{|X|(|X|-1)}{2}$. Let $P_{(A,B)}$ be the probability of a pair $p = (i, j)$ with $i$ of class $A$ and $j$ of class $B$ in $P$. (In other words, the number of pairs from class $A$ and $B$ over $|P|$.) Question 1: For which values of $m$ and $n$ is the probability of $P_{(A,A)} =0.5$? A solution is $n=3, m=1$, but what is the next possible solution? Question 2: What is a formula to compute all possible combinations? Example 1: Suppose you have three pingpong-balls. Two are orange (class $A$), one is white (class $B$). Then $X = \{o_1, o_2, w_1\}$. And $P = \{(o_1, o_2), (o_1, w_1), (o_2, w_1)\}$. $P_{(A, A)} = \frac{|\{(o_1, o_2)\}|}{|P|} = \frac{1}{3}$. Example 2: Suppose you have four pingpong-balls. Three are orange (class $A$), one is white (class $B$). Then $X = \{o_1, o_2, o_3, w_1\}$. And $P = \{(o_1, o_2), (o_1, o_3), (o_1, w_1), (o_2, o_3), (o_2, w_1), (o_3, w_1)\}$. $P_{(A, A)} = \frac{|\{(o_1, o_2), (o_1, o_3), (o_2, o_3)\}|}{|P|} = \frac{3}{6} = 0.5$.","['probability-theory', 'probability']"
2581308,Multi-Variate Statistics verification,"Let $X_i$ for $i=1,2,3$ be independent identically distributed random variables with probability density function $f(x)=e^{-x}$ for $x\in(0,\infty)$ (and $0$ otherwise). Find $$P(X_1<X_2<X_3|X_3<1).$$ By the definition of conditional probability I figured 
\begin{align}
P(X_1<X_2<X_3|X_3<1)&=\frac{P(\left[X_1<X_2<X_3\right]\cap [X_3<1])}{P(X_3<1)}\\
&=\frac{P(X_1<X_2<X_3<1)}{P(X_3<1)}
\end{align}
I had to figure out a triple integral to represent the numerator and I believe it comes out to 
$$P(X_1<X_2<X_3<1)=\int_{0}^{1}\int_{0}^{x_3}\int_{0}^{x_3-x_2}e^{-(x_1+x_2+x_3)}\mathrm{d}x_1\mathrm{d}x_2\mathrm{d}x_3$$ and $P(X_3<1)=\int_{0}^{1}e^{-x_3}\mathrm{d}x_3$, is this correct? I am unsure about the triple integral bounds and if they satisfy the inequality given.","['statistics', 'probability']"
2581310,How can I find the general term of this recursive sequence?,"I want to find the general term of the sequence defined by $x_0=1$, $x_1=2$, and $x_n=\binom{2n}{n}-\sum\limits_{i=0}^{n-2}\left(x_i\binom{2n-2-2i}{n-2-i}\right)$. Since I didn't know how to approach this problem I tried by making guesses, and the best one I could find is given by $x_n=\binom{2n}{n}-\frac{2^{n-1}(2^{n-1}-1)}{2}$ which gives the exact same results for $n=1,2,3,4,5$. Although this seems an abstract non-sense, it actually comes from a concrete question on directed graphs. I will add the details of this concrete point of view as soon as I have some time.","['combinatorics', 'graph-theory', 'binomial-coefficients', 'sequences-and-series']"
2581330,Constant of integration change,"So, sometimes the constant of integration changes, and it confuses me a bit when and why it does. So for example, we have a simple antiderivative such as $$\int \frac{1}{x} dx $$ and we know that the result is $$\log|x| + C$$ and the domain is $$x\in\mathbb R \backslash \{0\} $$ If we want to show all the solutions, we need to do something like $$\begin{cases}
\log x+C_1 & x>0\\
\log(-x)+C_2 & x<0
\end{cases}$$ Do we need to do change the constant every time there is a gap in the domain or is it just when the expression changes? For example, $$ \int \frac {x^5} {x^2-1} dx$$ which is $$ \frac {1} {2} \log |x^2-1| + \frac {x^4} {4}  + \frac {x^2}{2} + C$$ and the domain is $$x \in \mathbb R \backslash \{-1,1\}$$ When we want to write all the solutions, is it something like $$ \begin{cases} \ \frac {1} {2} \log (x^2-1) + \frac {x^4} {4}  + \frac {x^2}{2} + C_1 & x>1\\ \frac {1} {2}\log (-x^2+1) + \frac {x^4} {4}  + \frac {x^2}{2} + C_2 & -1<x<1\\ \frac {1} {2} \log (x^2-1) + \frac {x^4} {4}  + \frac {x^2}{2} + C_3 & x<-1  \end{cases}$$ or is it that since the the first and last expression are the same they only have one constant associated? Meaning, the solutions are actually $$ \begin{cases} \ \frac {1} {2} \log (x^2-1) + \frac {x^4} {4}  + \frac {x^2}{2} + C_1 & x \in \mathbb R \backslash [-1, 1]\\ \frac {1} {2}\log (-x^2+1) + \frac {x^4} {4}  + \frac {x^2}{2} + C_2 & -1<x<1\\ \end{cases}$$",['integration']
2581335,Is epsilon-delta continuity at a point equivalent to topological continuity at a point [duplicate],"This question already has an answer here : Topological definition of continuity and its application to epsilon-delta definition? [duplicate] (1 answer) Closed 2 years ago . I was trying to prove the equivalence of the $\epsilon$-$\delta$ and topological notions of continuity at a point. (Given the standard topology on a metric space) I could get one direction, but the $\epsilon$-$\delta$ notion of continuity at a point doesn't seem to imply the topological notion of continuity at a point. (I think I might have messed up my definition) The definition of topological continuity at a point I was using was that a function $f$ is continuous as point $a$ if every open set in the image of $f$ which contain $f(a)$ has an open pre-image. Basically, the $\epsilon$-$\delta$ notion of continuity at point $a$ only says things about neighborhoods of $a$. But I can always union a neighborhood of $a$ with an open set in some other part of the image to get a new open set. And the $\epsilon$-$\delta$ definition gives me no information about this potentially distant set or its pre-image. In other words, take $f$ to map some open set $A$ to some open neighborhood $f(A)$ and some closed set $B$ to some open neighborhood $f(B)$ such that $f(A) \cap f(B)=\emptyset$. Also, let it be that f is $\epsilon$-$\delta$ continuous over all of $A$. So now, let's take some $a \in A$. $f$ is epsilon-delta continuous at $a$. But is it topologically-continuous at $a$? No. Because any open neighborhood of $f(a)$, I can union with $f(B)$ to get an open set whose pre-image is not an open set. I think my problem is that I got my topological definition of continuity at a point wrong. But I can't figure out how to fix it without invoking concepts from metric spaces.","['continuity', 'general-topology', 'analysis']"
2581353,Neyman-Pearson $\alpha$-level test always exists for continuous distributions,"The Neyman-Pearson lemma as in the classical book by Casella and Berger, gives to conditions for the existence of $\alpha$-level tests: The critical region must be of the form: $\{x:f(x|\theta_1) > k f(x|\theta_0)\} \subseteq R \subseteq \{x:f(x|\theta_1) \ge k f(x|\theta_0)\}$ $\alpha = P_{\theta_0}(X \in R)$ One can see that not every $\alpha$ has a suitable critical region such that 2. holds. The problem comes with discrete distributions. Example Let $X \sim Binom(2,\theta)$ and the test $H_0: \theta = 1/2$, $H_1:\theta = 3/4$. In this situation I cannot find a critical region of the form $\{x:f(x|\theta_1)/f(x|\theta_0) \ge k\}$ such that the size of the test is $\alpha = 1/\pi$ the reason is that all computations are done with rational values. I had an argument that tried to proved that for continuous distributions the test of level $\alpha$ always exists but thanks to @Nch I found that it was wrong. How can I prove that for continuous distributions there is always an $\alpha$-level test?","['statistics', 'probability', 'hypothesis-testing', 'statistical-inference']"
2581366,One-point compactification problem,"I am a little bit confused about that question. $A=[0,1)\cup [2,3).$ Prove that the one-point compactification $A^{+}$ of $A$ is homeomorphic to a closed interval. I think it is possible to choose $[0,1]$ and show that $f:[0,1]\rightarrow A^{+}$ is homeomorphic, but I am not sure. Is it a right way to do? And if it is then I have shown that the map is bijective and don't really know how to show that the map $f$ is continuous and that the inverse is continous too. Maybe you have another way to solve the problem. Thank You",['general-topology']
2581394,Looking for another way to calculate this integral.,"Calculate: $$\mathcal{J}=\int_0^{\frac{\sqrt{3}}{2}}{\ln ^2x\frac{\arccos ^3\!\sqrt{1-x^2}}{\sqrt{1-x^2}}}\text{d}x$$ My attempt: $$\int_0^{\frac{\sqrt{3}}{2}}{\ln ^2x\frac{\arccos ^3\!\sqrt{1-x^2}}{\sqrt{1-x^2}}}\text{d}x=\int_0^{\frac{\pi}{3}}{t^3\ln ^2\sin t\text{d}t=\frac{1}{16}}\int_0^{\frac{2\pi}{3}}{t^3\ln^2 \left( \sin \frac{t}{2} \right)}\text{d}t=\frac{1}{16}\mathcal{I}
\\
\begin{align*}
\mathcal{I}&=\int_0^{\frac{2\pi}{3}}{t^3\ln ^2\left( 2\sin \frac{t}{2} \right)}\text{d}t-2\ln 2\int_0^{\frac{2\pi}{3}}{t^3\ln \left( 2\sin \frac{t}{2} \right)}\text{d}t-\ln ^22\int_0^{\frac{2\pi}{3}}{t^3}\text{d}t
\\
&=-\text{Ls}_{6}^{\left( 3 \right)}\left( \frac{2\pi}{3} \right) +2\ln\text{2Ls}_{5}^{\left( 3 \right)}\left( \frac{2\pi}{3} \right) +\frac{4\pi ^4\ln ^22}{81}
\end{align*}
$$
Hence according to the formula
\begin{align*}
\zeta \left( n-k,\left\{ 1 \right\} \!^k \right) &-\sum_{j=0}^k{\frac{\left( -i\tau \right) \!^j}{j!}}\text{Li}_{2+k-j,\left\{ 1 \right\} ^{n-k-2}}\left( \text{e}^{i\tau} \right) \\
&=\frac{i^{k+1}\left( -1 \right) \!^{n-1}}{\left( n-1 \right) !}\sum_{r=0}^{n-k-1}{\sum_{m=0}^r{\left( \begin{array}{c}
	n-1\\
	k,m,r-m\\
\end{array} \right)}}\times \left( \frac{i}{2} \right) \!^r\left( -\pi \right) \!^{r-m}\text{Ls}_{n-\left( r-m \right)}^{\left( k+m \right)}\left( \tau \right)
\end{align*}
we have
$$\begin{align*}
\text{Ls}_{6}^{\left( 3 \right)}\left( \frac{2\pi}{3} \right) =&-\frac{946\pi ^6}{76545}-\frac{16\pi ^3}{27}\text{Gl}_{2,1}\left( \frac{2\pi}{3} \right) -\frac{8\pi ^2}{3}\text{Gl}_{3,1}\left( \frac{2\pi}{3} \right) +8\pi \text{Gl}_{4,1}\left( \frac{2\pi}{3} \right) \\
&+12\text{Gl}_{5,1}\left( \frac{2\pi}{3} \right) +6\zeta ^2\left( 3 \right)
\end{align*} 
\\
\text{Ls}_{5}^{\left( 3 \right)}\left( \frac{2\pi}{3} \right) =\frac{8\pi ^3}{27}\text{Cl}_2\left( \frac{2\pi}{3} \right) -4\pi \text{Cl}_4\left( \frac{2\pi}{3} \right) -\frac{16\pi ^2}{27}\zeta \left( 3 \right) +\frac{242}{27}\zeta \left( 5 \right) $$
and we get
\begin{align*}
\mathcal{I}=&\frac{946\pi ^6}{76545}+\frac{4\pi ^2\ln ^22}{81}+\frac{16\pi ^3}{27}\text{Gl}_{2,1}\left( \frac{2\pi}{3} \right) +\frac{8\pi ^2}{3}\text{Gl}_{3,1}\left( \frac{2\pi}{3} \right) -8\pi \text{Gl}_{4,1}\left( \frac{2\pi}{3} \right) \\
&-12\text{Gl}_{5,1}\left( \frac{2\pi}{3} \right) +\frac{16\ln 2\pi ^3}{27}\text{Cl}_2\left( \frac{2\pi}{3} \right) 
-8\ln 2\pi \text{Cl}_4\left( \frac{2\pi}{3} \right) -6\zeta ^2\left( 3 \right) \\
&-\frac{32\ln 2\pi ^2}{27}\zeta \left( 3 \right) +\frac{848\ln 2}{27}\zeta \left( 5 \right) 
\end{align*}
So
\begin{align*}
\mathcal{J}=\frac{1}{16}\mathcal{I}=&\frac{473\pi ^6}{612360}+\frac{\pi ^3}{27}\text{Gl}_{2,1}\left( \frac{2\pi}{3} \right) +\frac{\pi ^2}{6}\text{Gl}_{3,1}\left( \frac{2\pi}{3} \right) -\frac{\pi}{2}\text{Gl}_{4,1}\left( \frac{2\pi}{3} \right) 
\\
&-\frac{3}{4}\text{Gl}_{5,1}\left( \frac{2\pi}{3} \right) +\frac{\pi ^3\ln 2}{27}\text{Cl}_2\left( \frac{2\pi}{3} \right) -\frac{\pi \ln 2}{2}\text{Cl}_4\left( \frac{2\pi}{3} \right) +\frac{\pi ^4\ln ^22}{324}
\\
&-\frac{2\pi ^2\ln 2}{27}\zeta \left( 3 \right) -\frac{3}{8}\zeta ^2\left( 3 \right) +\frac{121\ln 2}{108}\zeta \left( 5 \right) 
\end{align*} Notations: \begin{align*}
&\text{Cl}_{a_1,...,a_k}\left( \theta \right) =\begin{cases}
	\Im \text{Li}_{a_1,...,a_k}\left( \text{e}^{i\theta} \right) \,\,\text{if}\,\,a_1+\cdots +a_k\,\,\text{even}\\
	\Re \text{Li}_{a_1,...,a_k}\left( \text{e}^{i\theta} \right) \,\,\text{if}\,\,a_1+\cdots +a_k\,\,\text{odd}\\
\end{cases}
\\
&\text{Gl}_{a_1,...,a_k}\left( \theta \right) =\begin{cases}
	\Re \text{Li}_{a_1,...,a_k}\left( \text{e}^{i\theta} \right) \,\,\text{if}\,\,a_1+\cdots +a_k\,\,\text{even}\\
	\Im \text{Li}_{a_1,...,a_k}\left( \text{e}^{i\theta} \right) \,\,\text{if}\,\,a_1+\cdots +a_k\,\,\text{odd}\\
\end{cases}
\\
&\text{Li}_{a_1,.\!\:.\!\:.\!\:,a_k}\left( z \right) =\sum_{n_1>\cdots >n_k>0}{\frac{z^{n_1}}{n_{1}^{a_1}\cdots n_{k}^{a_k}}}
\\
&\zeta \left( a_1,.\!\:.\!\:.\!\:a_k \right) =\text{Li}_{a_1,.\!\:.\!\:.\!\:,a_k}\left( 1 \right) 
\\
&\text{Ls}_{n}^{\left( k \right)}\left( \sigma \right) =-\int_0^{\sigma}{\theta ^k\ln ^{n-1-k}\left| 2\sin \frac{\theta}{2} \right|}\,\text{d}\theta
\end{align*} So I wonder is there another way to calculate the integral,complex or real method. Thanks!","['integration', 'definite-integrals', 'calculus']"
2581396,How can I solve $\frac{wy'(x)}{y'^2(x)+1}=kx$?,"I stumbled upon this differential equation while thinking of a physics problem. $$\frac{wy'(x)}{y'^2(x)+1}=kx$$ I found two solutions using wolframalpha. And when I plot it, the result is magically accurate (according to my intuition). So, I know it's right. My question is, how can I learn to solve this particular differential equation? Is there a procedural approach to arrive at this solution or does solving it mean plugging in the guess and fiddling with the free parameters to arrive at one particular solution?","['nonlinear-analysis', 'ordinary-differential-equations', 'calculus']"
2581397,Picking the correct method for solving this first degree equation,"I have trouble solving this first degree equation: $ 2(x-1) = \sqrt{2} (x + 1) - 1$ I can't see a way to factorize this, so using a distributive aproach I get this: $ 2x  = x \sqrt{2} + \sqrt{2} + 1$ Now my issue is that, how do I get rid of $ \sqrt{2} $ terms ?
I tried to squared the whole equation this way:
$ 2x² = (x \sqrt{2})² + 3$ But now I'm stuck, the expected result is (a non-detailed results is given, hence I'm looking for how to continue solving): $ x = \frac{4 + 3 \sqrt{2}}{2} $ Am I not using the correct way to solve this, or I'm mistaking in the evaluation of:
$(x \sqrt{2})²$ ? Or should I even start by squaring both sides to this  way: $ 2(x-1)² = (x + 1) - 1$ Thanks.",['algebra-precalculus']
2581422,Using Jensen's inequality to prove another inequality?,"Suppose $u(\cdot)$ and $v(\cdot)$ are two differentiable, strictly increasing, and strictly concave real functions. Specifically, $v(\cdot)$ is ""more concave"" than $u(\cdot)$ in the sense that there exists an increasing and strictly concave function $\phi(\cdot)$ such that $v(x)=\phi(u(x))$ at all $x$. It is also equivalent to
\begin{equation}
\frac{v''(x)}{v'(x)}<\frac{u''(x)}{u'(x)} \textrm{ for any }x\,.
\end{equation} Let $p_i\in(0,1), \sum_{i\in I}p_i=1$ be probabilities and $|I|>2$. Let $x_i$ and $y_i$ be strictly positive for all $i\in I$. Assume
\begin{equation}
\sum_{i\in I}p_ix_i<\sum_{i\in I}p_iy_i,
\end{equation}
and
\begin{equation}
\sum_{i\in I}p_iu(x_i)=\sum_{i\in I}p_iu(y_i).
\end{equation} Conjecture:
\begin{equation}
\sum_{i\in I}p_iv(x_i)>\sum_{i\in I}p_iv(y_i).
\end{equation}
I believe this is right (after trying many numerical examples) and I think a clever use of Jensen's inequality (or its variants) will do this. But I'm stuck on doing it formally. Any hints/thoughts on providing a formal proof? Remark: this is related to my other post: Proving an inequality of the expectation of concave functions? Update: after some more attempts, I believe some techniques in convex analysis would be helpful. Geometrically, the middle equation represents a hyperplane in the $R^{|I|}$ space, and the desired result (very roughly) says that a concave transformation of that hyperplane should be separated from a convex transformation of it. To be clear, I wasn't saying the conjecture should be generally true. Any thoughts on finding any sufficient conditions to make it work would be very helpful.","['expectation', 'real-analysis', 'inequality', 'convex-analysis', 'probability']"
2581461,How many fair dice exist?,"We know a coin is a fair die with a 50-50 probability for two alternatives. Similarly, all five Platonic solids are fair dice. That makes six solids that can be fair dice, but can there be more? One example could be a two tetrahedra pasted together along one face. The resulting solid is not platonic since two vertices have three faces meeting at them while three of them have four faces meeting at them. However, this too can be a regular die as far as I can tell since all faces are identical. The question is, how many solids can exist that can be used as fair dice?","['polyhedra', 'platonic-solids', 'probability', 'geometry']"
2581492,How many times are the hands of a clock at 33 degrees?,"How many times are the hands of a clock positioned such that there is an angle of $33°$ between them in a day ? My trying: The minute hand moves 360 degrees in 60 minutes. This means that the angle of the minute hand is given by 6t, where t is number of minutes past midnight. The hour hand moves 30 degrees in 60 minutes. This means that the angle of the hours hand is given by 0.5t. The hands start together at midnight. The first time they make a 33 degree angle is when the minute hand has moved 33 degrees further than the hour hand, so this is given by the equation: $ 6t = 0.5t + 33 \Rightarrow 5.5t = 33 \Rightarrow t = 6 minutes$ . In other words about 6 minutes past midnight. I have found that 22 times the hour and minutes hand are in 33 degree angle . How can I find this number ? How can I find the 22 times ?","['puzzle', 'recreational-mathematics', 'geometry']"
2581498,Extremal problem with positive integer numbers,"Let $a,b$ be two positive integer numbers such that $a\sqrt{3}-b\sqrt{7}>0$. Find the minimum value of
$$
S=(a\sqrt{3}-b\sqrt{7})(a+b).
$$ Attempt I have tried and guess that the minimum value of $S$ is $(55+36)(55\sqrt{3}-36\sqrt{7})$, where $(55,36)$ is the integer solution of the Pell equation $3a^2-7b^2=3$.","['number-theory', 'inequality']"
2581602,$\lim_{n\to \infty}[\frac{1}{a_{1}a_{2}}+\frac{1}{a_{2}a_{3}}\cdot\cdot\cdot\frac{1}{a_{n-1}a_{n}}].$,Let $a_{1}=1$ and $a_{n}=a_{n-1}+4$ for $n\geq 2.$ I have to find $\lim_{n\to \infty}[\frac{1}{a_{1}a_{2}}+\frac{1}{a_{2}a_{3}}\cdot\cdot\cdot\frac{1}{a_{n}a_{n-1}}].$ I tried it as $\lim_{n\to \infty}[\frac{1}{a_{1}a_{2}}+\frac{1}{a_{2}a_{3}}\cdot\cdot\cdot\frac{1}{a_{n}a_{n}}]=\lim_{n\to \infty}[\frac{1}{a_{1}[a_{1}+4]}+\frac{1}{a_{2}[a_{2}+4]}\cdot\cdot\cdot\frac{1}{a_{n-1}[a_{n-1}+4]}]=\frac{1}{4}\lim_{n\to\infty}[\sum \frac{1}{a_{n}}-\sum \frac{1}{a_{n}+4}].$ Now please help me to solve the problem. Thanks .,"['real-analysis', 'arithmetic-progressions', 'sequences-and-series', 'limits']"
2581622,Solve $y'(t)=\operatorname{sin}(t)+\int_0^t y(x)\operatorname{cos}(t-x)dx$ by Laplace transform,"Question: Solve $y'(t)=\operatorname{sin}(t)+\int_0^t y(x)\operatorname{cos}(t-x)dx$ such that $y(0)=0$ My try: I applied Laplace transform on both sides of the equation. $
sL\{y(t)\} = \frac{1}{s^2+1}+L\{cos(t)*y(t)\} \implies sL\{y(t)\}=\frac{1}{s^2+1}+L\{cos(t)\}\times L\{y(t)\} $ $\implies L\{y(t)\} = \frac{s^2-1}{(s^3-s-1)(s^2+1)} $ (*) Now, I'm stuck on applying the inverse Laplace transform on (*) to find $y(t)$. Any idea?","['ordinary-differential-equations', 'laplace-transform']"
2581644,Characterization of a sphere: sub-spheres have two (antipodal) centers?,"Suppose we are given a Riemannian manifold $M$ (it has dimension $n$, is metrically complete and connected) with the following property: for every point $p\in M$ and for every radius $0\leq R$ there is another point $q(p,R)\neq p$ and radius $R'(p,R)\in \mathbb{R}$ such that
$$S(p,R) = S(q(p,R),R'(p,R))$$
where, provided $Q\geq 0$, $S(a,Q)\subset M$ is the set of points which are a distance $Q$ from $a\in M$ (If $Q<0$, $S(a,Q)$ is agreed to be empty). Q : Is $M$ isometric to an $n$-dimensional sphere? (To see why a sphere has the mentioned property, define $q(p,R)$ simply to be the point antipodal to $p$ and define $R'(p,R)=\pi R^*-R$ where $R^*$ is the radius of the sphere) Be sure to have a look at a, virtually identical, question I asked on mathoverflow","['riemannian-geometry', 'differential-geometry']"
2581714,"$A, B$ are $3 \times 3$ matrices such that $(A - B)^2 = 0$. Prove that $\operatorname{Tr}(AB - BA)^3 = 0$.","I have been trying to solve this recent linear algebra problem : Let $A, B$ be $3 \times 3$ matrices such that $(A-B)^2 = 0$. Prove that $\det (AB - BA) = 0$. This was my approach:$\DeclareMathOperator{\Tr}{Tr}$ The following equality holds for any $3\times 3$ matrices $A, B$: $$\det (AB - BA) = \frac13 \Tr(AB - BA)^3$$ It follows from the Hamilton-Cayley theorem applied on $AB - BA$. Therefore, it suffices to prove that $\Tr(AB - BA)^3 = 0$. Expanding gives that $\Tr(AB - BA)^3$ is equal to: \begin{align}
\Tr\left(\color{magenta}{ABABAB} - \color{blue}{ABABBA} - \color{purple}{ABBAAB} + \color{purple}{ABBABA} - \color{blue}{BAABAB} + \color{olive}{BAABBA} + \color{olive}{BABAAB} - \color{magenta}{BABABA}\right)\\
\end{align} where the same-colored terms are cyclic permutations of each other so have the same trace. So, $$\Tr(AB - BA)^3 = 2\Tr BAABBA - 2\Tr BAABAB = 2 \Tr BAAB(BA - AB)$$ I figured this was a good place to try to use the assumption $(A - B)^2 = 0$: $$0 = (A - B)^2 = A^2 + B^2 - AB - BA \implies A^2 = AB + BA - B^2$$ So we have: $$BAAB(BA - AB) = B(AB + BA - B^2)B(AB - BA) = $$
$$\color{OrangeRed}{BABBBA} + \color{green}{BBABBA} - BBBBBA - \color{green}{BABBAB} - \color{OrangeRed}{BBABAB} + BBBBAB$$ Again, the same-colored terms cancel out when taking the trace so: $$\Tr BAAB(BA - AB) = \Tr BBBB(AB - BA)$$ A possible development is: \begin{align}2 \Tr BAAB(BA - AB) &=  \Tr (AB-BA)(BBBB - BAAB)\\
&= \Tr (AB-BA)(BBBB - BAAB)\\
&= \Tr (AB-BA)B(B^2 - A^2)B
\end{align} But using $A^2 + B^2 = AB + BA$ here again gives $\Tr (BA - AB)BAAB$, so nothing new. Is there a way to finish the proof?","['matrices', 'trace', 'linear-algebra']"
2581769,Transformation of random variable and distribution function,"I have task
""Let X by random varaible with exponential distribution and parameter $\lambda$""
We have to find distribution function of random variable $Y=e^x$ My solution is quite easy $1) F_{y}(Y) = P( y <= Y ) = P(y <= e^x ) = P( ln(y) <= x )$ as the last step i used inversion function on both side ( ln ) 2) $P(ln(y) <= x ) = 1 - P( x <= ln(y) = 1 - F_{x}(ln(y))$
And we know that distribution function of x is exponential distribution function so it should be $F_{y} = 1 - 1-e^{-\lambda x} = 1 - 1 - e^{-\lambda ln(y)}$ But i am not sure if i can assume, that when i have x in function its automaticly distribution function of X. Did i make mistake or is this way correct? Thank you very much for help.","['statistics', 'probability']"
2581770,direct definition of the Radon-Nikodym derivative?,"The Radon-Nikodym derivative $f$ for a measurable space $(X,F)$ and measures $\mu,\nu$ where $\nu$'s support contains $\mu$'s support, is defined as follows: $$\frac {d\mu}{d\nu}(x)=f(x) \text { s.t. }\quad \forall A\in F: \quad \mu(A)=\int_A fd\nu$$ My question is: Can we also define the Radon-Nikodym derivative ""directly""? i.e. as follows: $$f(x):=...$$ I am looking for something analogous to the classical derivative in real analysis which is defined as $$\frac {dF(x)}{dx}=f(x)=\lim_{d\to 0}\frac {F(x+d)-F(x)}{d}$$ Rather than:
$$\frac {dF(x)}{dx}=f(x) \text { s.t. }\quad \forall a,b\in \mathbb R: \quad F(b)-F(a)=\int_a^b f(x)dx$$ EDIT: I came up with the following idea, but it only works for some measurable spaces:
Take a sequence of sets $A_n$ that are both $\mu$ and $\nu$ measure positve for all $n$, such that $\lim_{n\to\infty}A_n=\{x \}$. Then
$$f(x)=\lim_{n\to \infty}\frac {\mu(A_n)}{\nu(A_n)}$$ However, out of the examples I've checked, this only works for discrete measurable spaces (in which case the definition is useless anyway), and one-dimensional continuous measure spaces (not sure if it works for all of them, only the examples I picked). It already fails in the measure space $(\mathbb R^2, B)$ because the value you get depends on how you approach the limit.","['derivatives', 'lebesgue-integral', 'measure-theory']"
2581796,A question about the Peano form of the remainder in a function's Maclaurin formula?,"Let the function $f:\mathbb{R} \rightarrow \mathbb{R}$ be $n$ times differentiable at $x=0$, and we have $$f(x)=P_n(x)+o(x^n)$$ where $P_n(x)$ is the $n$-th order Taylor polynomial and $o(x^n)$ is the Peano form of the remainder. Then we surely have $$f(x^2)=P_n(x^2)+o(x^{2n})$$. But in the textbook I'm using, it goes like$$f(x^2)=P_n(x^2)+o(x^{2n+1})$$ How to prove it? Here's my effort: Let $g(x)=o(x^n)$, then we have $\displaystyle \lim_{x\rightarrow0}\frac{g(x)}{x^n} =0$, but it doesn't imply that  $\displaystyle \lim_{x\rightarrow0}\frac{g(x^2)}{x^{2n+1}} =0$. I guess the Peano form of the remainder may not be an ordinary ""Little-O of $x$"", but I can't find out the extra information about it. Could anyone help me out here?","['taylor-expansion', 'analysis']"
2581877,Why these two definitions of induced representations are equivalent?,"Let $F$, $G$ and $H$ be respectively a field, a finite group and a subgroup of $G$. Problem. For a representation $(\sigma, W)$ of $H$, define an $F$-vector space $$\widetilde W=\{f\colon G\to W\mid f(hg)=\sigma(h)f(g),h\in H,g\in G\},$$ and the action of $G$ on $\widetilde W$ is given by $(g'f)(g)=f(gg')$. Then $\widetilde W$ is isomorphic to $F[G]\otimes_{F[H]}W$ (as $G$-representations, or in other words, as $F[G]$-modules). In my algebra lecture, the induced representation of $W$ from$H$ to $G$ is defined as the $F[G]$-module $\mathrm{Ind}_H^GW:=F[G]\otimes_{F[H]}W$, and this problem says that these two definitions are equivalent. However, although tried, I have no idea how to prove it. What I think may be helpful is a proposition which is stated as follows (Here $\mathrm{Res}_H^GV$ is the restriction of $V$ to $H$) Let $V$ be a representation of $G$ and $W$ be a subrepresentation of $\mathrm{Res}_H^GV$. Then $V\cong\mathrm{Ind}_H^GW$ if and only if $V$ is generated by $W$ as an $F[G]$-module and $\dim_FV=[G:H]\dim_FW$. Yet I do not know how to identify $W$ with an $F[H]$-submodule of $\mathrm{Res}_H^G\widetilde W$, let alone how to verify other conditions in this proposition. So I would like to ask if it is correct to prove it in this way, or there is any easier or more straightforward proof? Thanks in advance...","['representation-theory', 'abstract-algebra', 'induction']"
2581902,Circles intersecting at two points orthogonally.,"I am finding the following much harder than it probably is! If a circle $A$ intersects the circle $B$ at two points orthogonally, then why can't $A$ pass through the centre of B?","['analytic-geometry', 'circles', 'orthogonality', 'geometry']"
2581924,How to evaluate $\lim_{x\to 0} \frac{x^2\sin {\frac{1}{x}}}{\sin x}$,"Find the value of $\lim_{x\to 0} \dfrac{x^2\sin {\dfrac{1}{x}}}{\sin x}$. Below I am showing my attempt at the question: $x^2\sin {\dfrac{1}{x}}\to 0$ as $x\to 0$ since $\sin {\dfrac{1}{x}}$ is bounded in a neighbourhood of $0$ and $x^2\to 0$ as $x\to 0$. Hence , we have a $\dfrac{0}{0}$ form. By L'Hospital's Rule ,$\lim_{x\to 0} \dfrac{x^2\sin {\dfrac{1}{x}}}{\sin x}$ reduces to  $\dfrac{2x\sin {\dfrac{1}{x}}-\cos{\dfrac{1}{x}}}{\cos x}$ whose limit can't be computed at $x=0$ since $\lim _{x\to 0} \cos{\dfrac{1}{x}}$ does not exist. How can I evaluate this correctly? Will the answer be limit does not exist ? Do excuse me if I am unable to post a good question as this is my first question on MSE","['real-analysis', 'trigonometry', 'limits']"
2581938,Two definitions of p-value,"According to Casella-Berger (2002) the definition of p-value (def. 8.3.26, §8.3.4, p. 397) is: A p-value $p(X)$ is a test statistic satisfying $0 \le p(x) \le 1$ for
every sample point $x$ . Small values of $p(X)$ give evidence that $H_1$ is true. A p-value is valid if, for every $\theta \in \Theta_0$ and every $0 \le \alpha \le 1$ , $P_{\theta}(p(X) \le \alpha) \le
 \alpha$ . However, other books such as Rohatgi (2001) define it as: The probability of observing under $H_0$ a sample outcome at least as
extreme as the one observed is called the P-value. The smaller the
P-value, the more extreme the outcome and the stronger the evidence
against $H_0$ . I feel this definition is similar in spirit to the one by Schervish (2012): p-value. In general, the p-value is the smallest level $\alpha_0$ such
that we would reject the null-hypothesis at level $\alpha_0$ with the
observed data. How are these definitions equivalent?","['statistical-inference', 'hypothesis-testing', 'statistics', 'probability', 'p-value']"
2581944,Proof of measurability of the sum of extended-real measurable functions,"I was reading Rudin's Real and Complex Analysis (3rd ed.), and on page 19 he says: (The key point here is about arithmetic in $[0, \infty]$ .) Observe that the following useful proposition holds: (with arithmetic in $[0, \infty]$ ) If $0\le a_1 \le a_2 \le \cdots$ , $0\le b_1 \le b_2 \le \cdots$ , $a_n \to a$ and $b_n \to b$ , then $a_nb_n\to ab$ . If we combine this with Theorems $1.17$ and $1.14$ , we see that sums and products of measurable functions into $[0, \infty]$ are measurable. Theorem 1.14 If $f_n:X\to [-\infty, \infty]$ is measurable, and $g=\sup_n f_n$ , then g is measurable. Theorem 1.17 Let $f:X\to [0, \infty]$ be measurable.  There exist simple measurable funcctions $s_n$ on $X$ s.t. $0\le s_1\le s_2\le \cdots\le f$ , and $\forall x\in X: s_n(x)\to f(x)$ as $n\to \infty.$ I am not very sure what he meant.  I list my attempted proof below, and would appreciate it if someone can confirm if it's valid, or point out if I miss his point. Suppose $f, g:X\to[0, \infty]$ are two measurable functions.  I want to show that $f+g$ is also measurable.  I'd proceed as follows: First, I state an easily proved proposition similar to the one given above: If $0\le a_1 \le a_2 \le \cdots$ , $0\le b_1 \le b_2 \le \cdots$ , $a_n \to a$ and $b_n \to b$ , then $a_n+b_n\to a+b$ .  (Even if $a$ or $b$ takes value of $\infty$ .) Since $f, g$ are measurable, by Theorem 1.17, there must exist simple measurable functions $f_n, g_n$ on $X$ s.t. $0\le f_1 \le f_2 \le \cdots\le f$ , and $\forall x\in X:f_n(x)\to f(x).$ Similarly, $0\le g_1 \le g_2 \le \cdots\le g$ , and $\forall x\in X: g_n(x)\to g(x).$ (In Rudin's book, simple functions are real functions, not extended-real.) Hence $\forall x\in X:f_n(x)+g_n(x)\to f(x)+g(x)$ by the proposition above.  (Note that $f_n+g_n$ is real, while $f+g$ is extended-real valued.) But $f_n, g_n$ are simple measurable functions, so their sum $f_n+g_n$ must be simple and measurable.  Moreover, $\{f_n+g_n\}$ is a sequence of increasing functions, so $$\lim_{n\to \infty} (f_n+g_n)=\sup_n(f_n+g_n).$$ Since the supremum of a sequence of real measurable functions must be measurable (Theorem 1.14), we conclude that $f+g$ is measurable (even though it's extended-real).","['real-analysis', 'measure-theory']"
2581966,Integral of rational function - which contour to use?,"Evaluate : $$\int_{-\infty}^{+\infty} \frac {x}{(x^2+2x+2)(x^2+4)}$$ I found that the integrand can be extended to a function on a complex plane has simple poles at $\pm 2i$ and  $-1\pm i$. Now I want to compute the integral by contour integration but I am unable to assume any contour here. Do excuse me , if my approach is wrong.","['complex-analysis', 'improper-integrals', 'complex-integration']"
2581970,Reconstructing vectors from inner products,"Suppose that I have two unknown vectors $v_1,v_2 \in \mathbb{R}^3$, and a known vector $v_{3} \in \mathbb{R}^3$. I know all inner products $\langle v_i, v_j \rangle$, $1 \leq i,j \leq 3$. Is there a closed-form (or least painful iterative) way to reconstruct $v_1$ and $v_2$? intuitively it looks like there might be two solutions since any $v_1$ and $v_2$ can be reflected through the known $v_3$ to get the same inner products. Nevertheless getting them both would be helpful. Note that the problem is not generally rotation invariant as $v_3$ is known.","['linear-algebra', 'inner-products']"
2581995,Abelian extensions of algebraic extensions of $\Bbb Q$,"Let $F/\Bbb Q$ be an algebraic extesnion (may not be finite), do we have class field theory for abelian extensions of $F$ ? In other words, can we describe $G^{ab}_F$? If $F$ contains all roots of unity, then by Kummer's theory we know the pro-p quotient $G_F^{ab}(p) \cong \Bbb Z_p^{dim_{\Bbb F_p}F^{\times}/(F^{\times})^p}$, see remarks on maximal abelian extensions 2.9. in this article . But this isomorphism is not useful in practice and does not have many good properties compared to the Artin homomorphism.","['number-theory', 'abstract-algebra', 'galois-theory', 'class-field-theory']"
2582016,Which is the set for transfinite ordinals $\omega$?,"Let me write down my train of thoughts to make my question clear. If the collection of elements $\{0,1,2,3,....\}$ is defined as the set $\mathbb{N}$. Then if $\omega$ is the first transfinite ordinal. Is there a corresponding definition (set) for the following collection of elements: $\{\omega, \omega +1, \omega +2, ...\}$ And/or $\{0,1,2,...,\omega, \omega +1, \omega +2, ...\}$ And/or $\{\omega, \omega +1, \omega +2, ..., \omega^2, \omega^2 +1, \omega^2 +2, ..., \omega^3, \omega^3 +1, \omega^3 +2, ..., \omega^{\omega}\}$ And/or $\{0,1,2,..., \omega, \omega +1, \omega +2, ..., \omega^2, \omega^2 +1, \omega^2 +2, ..., \omega^3, \omega^3 +1, \omega^3 +2, ..., \omega^{\omega}\}$ ? P.S. If you can write down your sources I would deeply appreciate it","['elementary-set-theory', 'ordinals']"
2582033,"If a function $d(x,y)$ fulfills triangle inequality, is the same true for $|d(x,y)|$?","Suppose we have a function $d\colon X\times X\to\mathbb R$ such that
$$d(x,z)\le d(x,y)+d(y,z).$$
But we do not require other properties of metric (non-negativity, symmetry). If it helps, in the situation which motivated this question we have $d(x,x)=0$. Do we get that then also
$$|d(x,z)|\le |d(x,y)|+|d(y,z)|,$$
(triangle inequality for the absolute value of $d$) holds? Of course, we immediately have 
$$d(x,z)\le d(x,y)+d(y,z) \le |d(x,y)|+|d(y,z)|,$$
so it would suffice to show the same upper bound for $-d(x,z)$. If we had symmetry we could say that
$$-d(x,z) \le d(y,x) - d(y,z) = d(x,y) - d(y,z) \le |d(x,y)|+|d(y,z)|,$$
but it is not clear whether we can do something similar without symmetry, too. This was motivated by my previous question: Does a metric-like space generate a topology if open balls are defined as $B_\sigma(X,\varepsilon)=\{y\in X; |\sigma(x,y)-\sigma(x,x)|<\varepsilon\}$? (You can find there further links including a paper from which the definition mentioned in the question was taken.) The question dealt with function similar to metric but with the condition $p(x,x)=0$ omitted. $p(x,y)\ge0$ $p(x,y)=0$ $\Rightarrow$ $x=y$ $p(x,y)=p(y,x)$ $p(x,z) \le p(x,y) + p(y,z)$ It was pointed out that the suggested way of generating topology, by taking the sets of the form $B(x,r)=\{y\in X; |p(x,y)-p(x,x)|<r\}$, does not give a base of topology. An answer suggested to use this version of triangle inequality:
$$p(x,z) \le p(x,y)+p(y,z)-p(y,y).$$
This is the version of the triangle inequality which is used in the definition of partial metric space. Still some conditions from the definition of partial metric space are missing (for example, we do not have $p(x,x)\le p(x,y)$ or that ""indistancy implies equality"".) 1 If we want to define topology using the base consisting of sets $B(x,r)$ defined above, it is natural to look at the function $d(x,y)=p(x,y)-p(x,x)$ and its absolute value.
We have
\begin{align*}
p(x,z) &\le p(x,y)+p(y,z)-p(y,y)\\
p(x,z)-p(x,x) &\le (p(x,y)-p(x,x))+(p(y,z)-p(y,y))\\
d(x,z)&\le d(x,y)+d(y,z)
\end{align*}
So, the function $d$ fulfills triangle inequality. If the same is true for $|d|$, we would at least have some information about the system $B(x,r)$. Notice that if $p(x,x)\ne p(y,y)$ for some $x$, $y$ than we do not have symmetry. 1 You can find more information on partial metric spaces on website created by Steve Matthews or in the papper
Michael Bukatin, Ralph Kopperman, Steve Matthews and Homeira Pajoohesh: Partial Metric spaces , The American Mathematical Monthly, Vol. 116, No. 8 (Oct., 2009), pp. 708-718 ( author's website , jstor ).","['general-topology', 'metric-spaces', 'triangle-inequality']"
2582037,Folland Exercise 3.8,"Let $\nu$ is a signed measure and $\mu$ is a positive measure then $\nu \ll \mu$ iff ${\nu}^{+} \ll \mu$ and ${\nu}^{-} \ll \mu$. My try: Converse part is easy. For forward implication, let $\nu \ll \mu$ and $E \in \mathcal{M}$ such that $\mu(E)=0$ $\Rightarrow \nu(E)=0$ $\Rightarrow {\nu}^{+}(E)={\nu}^{-}(E)$ Since ${\nu}^{+}\perp {\nu}^{-}$ $\exists P,N \in \mathcal{M}$ such that P is ${\nu}^{-}$ null and N is $\nu^{+}$ null. $\Rightarrow$ $\nu^+(E)=\nu(E \cap P)$ But I am not able to proceed and show ${\nu}^{+}(E)=0$ Thanks for help!","['real-analysis', 'measure-theory']"
2582069,Prove the geodesic on 2-sphere is the great circle,"I want to use the Killing vector fields to prove the geodesic on the sphere is the great circle. First of all, the given metric is $$ds^2=d\theta^{2}+\sin^2\theta d\phi^{2},$$ where I set the radius to be $1$. By Killing equation, $$\nabla_\mu K_\nu+\nabla_\nu K_\mu =0,$$ and some computation, I have the following three Killing vectors:
\begin{align*}
K_1 &= \partial_{\phi} \\
K_2 &= \cos\phi \, \partial_{\theta} - \cot\theta \sin\phi \partial_{\phi} \\
K_3 &= -\sin\phi \partial_{\theta} - \cot\theta \cos\phi \partial_{\phi}
\end{align*}
I want to use the fact that
$$\frac{d}{d\lambda}\left\{K_\mu \frac{dx^{\mu}}{d\lambda}\right\}=0,$$
where $x$ is the curve and $\frac{dx^{\mu}}{d\lambda}$ is the tangent vector. However, as everyone knows, the geodesic is the great circle. Therefore, I thought the final equation would be of the form,
$$aX+bY=0,$$
where $a$ and $b$ are some constant, while $X$ and $Y$ are positions on the sphere. The equation passes the origin. But I can't see how to get the desired results, please give me some help.","['index-notation', 'riemannian-geometry', 'differential-geometry', 'general-relativity']"
2582075,Elementary proof that $\mathbb R\mathbb P^2$ is not homeomorphic to a sphere,"Ok, if one looks at invariants like the fundamental group, the Euler characteristic or orientability, then it is immediate to see that $\mathbb R\mathbb P^2$ is not homeomorphic to $S^2$. Is there any simple (or maybe not simple but still intersting) proof of this fact that makse no use of sophisticated invariants? (like homology, homotopy etc...) The purpose is to teach this fact to a class without any of such tools.","['algebraic-topology', 'general-topology', 'invariant-theory', 'differential-topology']"
2582112,Minimal eigenvalue inequality,"My problem is to show that $$\lambda_{\min}(PA) \leq  \lambda_{\min}((D-M)A) ,$$ where $A$ is an arbitrary $n\times n$ symmetric positive definite matrix and $P$ is a diagonal matrix with $\frac{1}{A_{ii}}$ as the $i$-th diagonal element and $D$ is a diagonal matrix with the $i$-th diagonal element equal to $\sum_{j = 1}^n\frac{p_{ij}A_{jj}}{\det_{ij}}$ and $M$ is a symmetric matrix with $M_{ij} = \frac{p_{ij}A_{ij}}{\det_{ij}}$, where $p_{ij}$ are probabilities with $p_{ii} = 0,\, \forall i$,  $p_{ij} = \frac{1}{n-1}$ for $i\neq j$, and $\det_{ij} = A_{ii}A_{jj} - A_{ij}^2$ for $i \neq j$, and $\det_{ii} = 1$ for all $i$. $A_{ij}$ is the element on the $i$-th row and $j$-th column. I have tried many simulations and this statement always holds, even with some special cases $2$ can be substituted by arbitrary large constant, but I am not able to contstruct a proof except when $A$ is diagonal.","['eigenvalues-eigenvectors', 'positive-definite', 'probability', 'linear-algebra']"
2582113,What is the conditional expected value?,"Consider two random variables, X and Y, with joint density function,
\begin{align} f_{X,Y}(x,y)=8xy, \qquad(0<x<1,0<y<x) \end{align}
Calculate $\mathbb{E}(X\mid Y=y)$. I've tried this,
$$f_{X\mid Y=y}=\frac{f_{X,Y}(x,y)}{f_Y(y)}=\frac{8xy}{\int_0^18xy\;dx}=\frac{8xy}{8y\left[\frac{x^2}{2}\right]_0^1}=\frac{8xy}{4y}=2x$$
$$\mathbb{E}(X\mid Y=y) = \int_0^1 xf_{X\mid Y=y} \; dx = \int_0^1 2x^2 \; dx = 2\left[\frac{x^3}{3}\right]_0^1 =2\cdot\frac{1}{3}=\frac{2}{3}$$
but my book says it is:
$$\frac{2(1-y^3)}{3(1-y^2)},\;(0<y<1)$$
What I'm doing wrong? Help please!","['multivariable-calculus', 'probability', 'random-variables']"
2582149,Plotting $f(x) = x\lfloor 1/x \rfloor$,I want to plot $f(x) = x\lfloor 1/x \rfloor$ near the point zero for finding its limit but I can't choose proper intervals and plot it .,"['graphing-functions', 'ceiling-and-floor-functions', 'limits']"
2582208,What does $\mathrm{Gal}(f) = S_n$ imply about $f$?,"Let $f$ be an irreducible polynomial with Galois group $\mathrm{Gal}(f) = S_n$. What can we deduce about the polynomial $f$? I have seen many things before that address the converse question: When is $\mathrm{Gal}(f) = S_n$? The answer to this usually talks about some specific cases that imply the Galois group is $S_n$ such as if we use mod $p$ reduction and show there is a transposition and an $(n-1)$-cycle then we have $S_n$, look at discriminant, etc. but I am interested in necessary conditions (atleast over certain fields) for when this is true. Intuitively I think if the groups is $S_n$, the roots of $f$ must be 'unrelated' and algebraically independent: If we start with base field $K$ and adjoin the roots $\alpha_{i}$ $$K \leq K( \alpha_{1}) \leq ... \leq K( \alpha_{1}, ... ,\alpha_{n})$$ each of the extensions is non trivial. So OK, we know that our roots must be 'unrelated' enough s.t. $[K( \alpha_{1}, ... ,\alpha_{n}):K] = n!$ But can we say something more? How about over base field $K = \mathbb{Q}$ for instance? My motivation for this question comes from looking at the polynomials $f_n(t) = t^n + t + 3$ over $\mathbb{Q}$. (It is simple to show this is irreducible by considering moduli of roots for all $n >1$). On calculating the Galois groups for first few $n$ (using mostly brute force mod $p$ reduction for $n>3$), I have $\mathrm{Gal}(f_{n}) = S_n$ which makes me wonder if this is true for all $n$. Any ideas would be appreciated.","['galois-theory', 'polynomials', 'group-theory']"
2582274,(Another) Alternative Proof to Baby Rudin 2.8,"Theorem: Every infinite subset of a countable subset A is countable. (Note: below A is the subset of B) I try to do the proofs on my own before reading Rudin's. Sometimes I fail heroically, sometimes comically. I can't see why I failed here, if I did. Consider any $A\subset B$ where $B$ is countable. Assume for contradiction that $A$ is uncountable. Then, by definition, $\exists \alpha\in A : \nexists f(\alpha)\mapsto j\in J= \mathbb{Z}$. Because $B$ is countable, for $\forall\beta\in B, \exists j\in J$ and $\exists g:g(\beta)\mapsto j$ but this is a contradction because $\alpha\in A\subset B$. edit: After a bunch of really unnecessary insults and vagueness, I see that the error is in my taking A from uncountable to there being no map onto Z. Thank you for everyone who (finally) helped me see that. It would have been very simple to point that out and explain it without all of the extracurriculars. Also, I guess the re-tagging on this is OK, but it is literally in a book on analysis in a chapter called basic topology, so I'm not sure why my initial tags were wrong.",['elementary-set-theory']
2582286,Who established the tradition of using $X^{\prime}$ instead of $X^{T}$ to denote the matrix transpose?,"From being away from mathematicians for a while and spending most of my time with econometricians and statisticians, one thing I've noticed is that econometricians and statisticians like to use $\prime$ to denote the matrix transpose, e.g., $X^{\prime}$. However, when you show this notation to a mathematician, they'd think that you mean a matrix distinct from $X$; i.e., $X$ and $X^{\prime}$ are just two distinct matrices with no explicit relation. Hence when I'm on MSE, I always try to use $X^{T}$ instead. My guess is that the $X^{T}$ notation has been around longer than $X^{\prime}$. Who was the first to use $X^{\prime}$ to denote the matrix transpose?","['matrices', 'transpose', 'math-history', 'notation']"
2582304,If $\vert{z_{1}+ \cdots + z_{n}}\vert$ = $\vert{z_{1}}\vert + \cdots + \vert{z_{n}}\vert$ then $z_{j} = c_{j}z_{1}$,"Could someone give me just one suggestion to solve this problem? Let $z_{1}, \ldots z_{n} \in \mathbb{C}$, with $z_{1}\neq 0$. Prove that if 
  $\vert{z_{1}+ \cdots + z_{n}}\vert$ = 
  $\vert{z_{1}}\vert + \cdots  + \vert{z_{n}}\vert$ then $z_{j} = c_{j}z_{1}$, where 
  $c_{j}\geqslant 0 $, $j=1,\ldots n $","['complex-analysis', 'complex-numbers']"
2582317,"Non linear first order ODE, not exact and not separable","I am trying to solve the following differential equation
$$tx'(t)=x(t)\left(\ln(x(t))-\ln(t)\right)$$ 
and I have hit a brick wall.
I tried separating but no luck. The equation is not exact
$$\int t dx=tx+C$$ and $$\int -(\ln x-\ln t) dt = -t\ln x-t+t\ln t+C$$
The only way I know to solve non linear first order ODEs is Bernoulli but this clearly isn't. What other methods are there for solving non linear first order Equations?
P.S. this is not homework, I am self learning for exams.
Second p.s. the solution is
$$x(t)=te^{ct+1}$$",['ordinary-differential-equations']
2582419,A conjecture about primes (1),"Choose some prime $p \neq 2,3$. Now concatenate to the right side of it some prime $q<p$ so to arrive at some other prime. Repeat until you cannot produce any more primes. For example, if we take $p=5$ then we can proceed further to the prime $53$. Then we can proceed further to $5347$. Then to $5347103$. And so on and so on... Intuition suggests that if we start with ""big enough"" primes that this cannot come to an end, that is, that always primes can be produced in this way. I did not check can we always proceed further if we start with all ""small enough"" primes. Can we build in this way larger primes from smaller ones if we start with any prime $p \neq 2,3$ (I conjecture that the answer is yes) ? Intuitively the answer is: ""of course we can"", but how to prove this? What would be the key ingredients in the proof?","['number-theory', 'recreational-mathematics', 'prime-numbers']"
2582472,Prove $\lim_{z \rightarrow \infty}\left(1+\frac{c_{d-1}}{z}+\frac{c_{d-2}}{z^2}+ \cdots + \frac{c_0}{z^d}\right)$=1,"Fix $c_0, c_1, \cdots, c_{d-1} \in \mathbb C$. Prove $$\lim_{z \rightarrow \infty}\left(1+\frac{c_{d-1}}{z}+\frac{c_{d-2}}{z^2}+ \cdots + \frac{c_0}{z^d}\right)=1$$
It seems stupid since I think we can just write it as
$$\begin{aligned}
\lim_{z \rightarrow \infty}\left(1+\frac{c_{d-1}}{z}+\frac{c_{d-2}}{z^2}+ \cdots + \frac{c_0}{z^d}\right) & =\lim_{z \rightarrow \infty}1+\lim_{z \rightarrow \infty}\frac{c_{d-1}}{z}+\lim_{z \rightarrow \infty}\frac{c_{d-2}}{z^2}+ \cdots + \lim_{z \rightarrow \infty}\frac{c_0}{z^d}\\ &=1+0+\cdots +0\\
& =1
\end{aligned}$$
But I am not sure whether I missed something here since it is a problem in the textbook...Thanks for any advise.","['complex-analysis', 'complex-numbers']"
2582477,How to find the left null space from rref(A),"I was working through a problem and was wondering if there was an easier way of finding the basis of the left null space of a given matrix. (For a simple example) Suppose we have a matrix $A = \begin{bmatrix} 1 & 2 & 4 \\ 2 & 4 & 8  \end{bmatrix}$ when reduced we can write it as $\text{rref}(A) = \begin{bmatrix} 1 & 2 & 4 \\ 0 & 0 & 0 \end{bmatrix} $ from rref(A) it is clear that: Basis for $C(A) = \left\{ \begin{pmatrix} 1 \\ 2\end{pmatrix} \right\}$ Basis for $C(A^T) = \left\{ \begin{pmatrix} 1,&2, & 4 \end{pmatrix} \right\}$ Basis for $N(A) = \left\{ \begin{pmatrix} -2 \\ 1 \\ 0\end{pmatrix} , \begin{pmatrix} -4 \\ 0 \\ 1 \end{pmatrix}\right\}$ Now my question is am I able to deduce the left null space just from rref(A)? Else, I would take the transpose of A, row reduce it and then find the left null space that way but I was wondering if there is an easier way?",['linear-algebra']
2582575,Intuition behind logarithm change of base,"I try to understand the actual intuition behind the logarithm properties and came across a post on this site that explains the multiplication and thereby also the division properties very nicely: Suppose you have a table of powers of 2, which looks like this: (after revision) $$\begin{array}{rrrrrrrrrr}
0&1&2&3&4&5&6&7&8&9&10\\
1&2&4&8&16&32&64&128&256&512&1024
\end{array}$$ Each column says how many twos you have to multiply to get the number in that column.  For example, if you multiply 5 twos, you get $2\cdot2\cdot2\cdot2\cdot2=32$, which is the number in column 5. Now suppose you want to multiply two numbers from the bottom row, say $16\cdot 64$.  Well, the $16$ is the product of 4 twos, and the $64$ is the product of 6 twos, so when you multiply them together you get a product of 10 twos, which is $1024$. I found that very helpful to understand the actual proofs for this property. I still struggle to get the idea behind the change of base rule. I'm familiar with the proof that goes like: $$\log_a x = y \implies a^y = x$$
$$\log_b a^y = \log_b x$$
$$y \cdot \log_b a = \log_b x$$
$$y = \frac{\log_b x}{\log_b a}$$ But can somehow provide a explanation in the style of the quoted answer why this actually works?","['algebra-precalculus', 'intuition', 'logarithms']"
2582594,Prove that '$\int_E f_n\leq M$ implies $\int_E f\leq M$ ' is equivalent to Fatou's Lemma,"The following question is taken from Royden Real Analysis $4$th edition, Chapter $4,$ question $20.$ Let $\{f_n\}$ be a sequence of nonnegative measurable functions that converges to $f$ pointwise on $E.$
      Let $M\geq 0$ be such that $\int_E f_n\leq M$ for all $n.$
      Show that $\int_E f\leq M.$
      Verify that this property is equivalent to the statement of Fatou's Lemma. I have proven that $\int_E f\leq M.$ But I have no idea on how to tackle second part. After some googling, I found a solution in MSE , which goes as follows: Let $(f_n,n\in\Bbb N)$ be a sequence of measurable integrable functions and $a_N:=\inf_{k\geqslant N}\int f_kd\mu$. Working with the sequence $(f_n,n\geqslant N)$ (for which the sequence of integrals has the same $\liminf$ as those of the whole sequence), one can see that that $\int fd\mu\leqslant a_N$ for each $N$. Now take the limit $\lim_{N\to +\infty}$. Questions: $(1)$ What is a motivation of considering $a_N:=\inf_{k\geq N}\int f_k d\mu?$ $(2)$ Why does $\int f d\mu\leq a_N$ for each $N?$","['real-analysis', 'lebesgue-integral', 'measure-theory']"
2582606,"Computing $\int\limits_0^\infty x \left \lfloor{\frac1x}\right \rfloor \, dx$","This is an integral I computed but can't find the result online or on wolfram. So here's a proof sketch, please indulge this sanity check: $$\int_0^\infty x \left \lfloor{\frac1x}\right \rfloor \ dx = \int_0^1 x \left \lfloor{\frac1x}\right \rfloor \ dx$$
$$= \sum_{n=1}^\infty \int_{1/(n+1)}^{1/n} nx \ dx =\sum_{n=1}^\infty\frac n2 \left(\frac{1}{n^2} - \frac{1}{(n+1)^2}\right) $$
$$=
\sum_{n=1}^\infty\frac n2 \left(\frac{2n+1}{n^2(n+1)^2}\right)$$
$$= \sum_{n=1}^\infty\frac{1}{(n+1)^2} + \frac12 \sum_{n=1}^\infty \frac{1}{n(n+1)^2}$$
$$= \frac{\pi^2}{6} -1 + \frac12\left(\sum_{n=1}^\infty \frac1n - \frac{1}{n+1} - \frac{1}{(n+1)^2}\right)$$
$$=\frac{\pi^2}{6} -1 + \frac12\left(\sum_{n=1}^\infty \frac1n - \frac{1}{n+1}\right) -\frac12\left(\sum_{n=1}^\infty\frac{1}{(n+1)^2}\right)$$
$$= \left(\frac{\pi^2}{6} -1\right) + \left(\frac12\cdot 1\right) - \frac12\left(\frac{\pi^2}{6} -1\right)$$
$$= \frac{\pi^2}{12}.$$ Basically, I used the Basel sum several times, and the fifth line follows from a partial sum decomposition. The seventh follows from the known result for the Basel sum, as well as the fact that the first series in the 6th line telescopes. I hope this is all correct.","['real-analysis', 'proof-verification', 'calculus', 'integration', 'analysis']"
2582616,Why multiplication of matrix is not done in the same way as matrix addition (i.e. adding corresponding entries)? [duplicate],"This question already has answers here : Fast(est) and intuitive ways to look at matrix multiplication? (7 answers) Intuition behind Matrix Multiplication (14 answers) Closed 6 years ago . Why multiplication of matrix is not done in the same way as matrix addition (i.e. adding corresponding entries)?
I know it is related to linear transformation, but by reading book I'm unable to visualize it.","['matrices', 'linear-algebra']"
2582693,Any Examples Of Unbounded Linear Maps Between Normed Spaces Apart From The Differentiation Operator?,"Let $X$ and $Y$ be normed spaces, either both real or both complex; let $T \colon X \to Y$ be a linear mapping. If there is a real number $r > 0$ such that 
$$ \lVert T(x) \rVert_Y \leq r \lVert x \rVert_X \qquad \mbox{ for all } x \in X, $$
then $T$ is said to be a bounded linear operator . One example of this operator is the following: Let $X$ denote the normed space of all the real polynomials defined everywhere on the closed interval $[0, 1]$, with the maximum norm, and let $T \colon X \to X$ be the map $x \mapsto x^\prime$. Then this linear operator is unbounded. Can we find any other examples of unbounded linear operators? I know that every linear operator whose domain is a finite-dimensional normed space is bounded.","['real-analysis', 'unbounded-operators', 'functional-analysis', 'linear-algebra', 'analysis']"
2582702,Why is the fractional part of the X and Y intercepts of a line equal when the line intersects the sqrt() of a semiprime AND its prime factors?,"I found an interesting property when trying to use some simple geometry to solve a math problem. If anyone can explain why this happens I'd greatly appreciate it! Given $c$ is a semiprime number (made from the product of primes $p_1$ and $p_2$) and $s=\sqrt{c}$, if I have a line that intersects the point ($s$,$s$) and also the point ($p_1$,$p_2$) the fractional parts of the intercepts of the $Y$ and $X$ axes are equal (and also equal to the fractional part of $s$). Why does this happen? Note also that if $c$ is not a semiprime but any old composite number, the fractional parts of the intercepts are equal every time the line intersects factors of $c$ when pivoting about ($s$,$s$). See the image below. It shows an example of the above when $c=77$ ($p_1=11$ and $p_2=7$).","['number-theory', 'prime-factorization', 'prime-numbers', 'geometry']"
2582746,$P(E_{1}\cap E_{2}\cap ... \cap E_{n})\leq \Pi_{i=1}^{n}P(E_{i})$?,"I am working on a probability problem in which there are $n$ events $E_{1},E_{2},...,E_{n}$. I'm interested to know the value or an upper bound of $P(E_{1}\cap E_{2}\cap ... \cap E_{n})$. I've shown that, for these events $E_i$, it holds that $$P(E_{i}\cap E_{j})\leq P(E_{i})P(E_{j}), \quad 1\leq i<j\leq n \tag{$1$}$$ By using $(1)$, can we conclude $P(E_{1}\cap E_{2}\cap ... \cap E_{n})\leq \Pi_{i=1}^{n}P(E_{i})$? For example, let
$\Omega = \{1,2,3,4,5,6\}, E_{1} = \{1,2,3\}$ and $E_{2}=\{1,4,5,6\}$. Then $E_{1}\cap E_{2} = \{1\}$, and $$P(E_{1}\cap E_{2})=\frac{1}{6}\leq P(E_{1})P(E_{2})=\frac{1}{2}\cdot\frac{2}{3}=\frac{1}{3}$$","['independence', 'probability-theory', 'probability']"
2582787,Is $\sqrt{I}/I$ finite dimensional?,"Is $\sqrt{I}/I$ finite dimensional over $\mathbb{C}$, where $I$ is a (non-zero) ideal in $R=\mathbb{C}[x_1,\dots,x_n]$? I know the ring is Noetherian, so they are finitely generated. The question is from an attempt to prove $R/I$ is finite dimensional if and only if $I$ is contained in finitely many maximal ideals.","['ideals', 'algebraic-geometry', 'commutative-algebra']"
2582806,Non-Uniform Continuity Criteria,"Let $A\subseteq\mathbb{R}$, $f:A\rightarrow \mathbb{R}$. Then, TFAE: $f$ is not uniformly continuous on $A$ There exists an $\varepsilon _0>0$ such that for every $\delta >0$ there are points $x_\delta, u_\delta\in A$ such that $\left| x_{s}-u_{s}\right| <\delta$ and $\left| f\left( x_\delta\right) -f\left( u_\delta\right) \right| \geq\varepsilon _0$ There are $\varepsilon >0$ and two sequence $x_n,y_n\in A$ such that $lim (x_n-y_n)=0$ and $\left| f\left( x_{n}\right) -f\left( y_n\right) \right| \geq \varepsilon _{0}$ for all $n\in\mathbb{N}$ Firstly, I want to show that $1)$ $\implies$ $2)$ but I couldn't do anything because I think by the definition it should be clear. Can you give a hint/help?","['continuity', 'analysis']"
2582822,"A simple proof of $\{D \in M_n(\mathbb C),\ D $ diagonalizable with distinct eigenvalues$\}$ $\subset$ Int$\{ D $ diagonalizable$\}$","What is the interior of $\{D \in M_n(\mathbb C),\ D $  diagonalizable$\}$ ? Actually, it is $I = \{D \in M_n(\mathbb C),\ D $  diagonalizable with distinct eigenvalues$\}$. Int$(D) \subset I $ is quite natural but I know a proof of the reciprocal which is astute : $M \in I \iff \gcd (P_M, P'_M) = 1 \iff \exists \ A,B \neq 0$ s.t. $A P_M = BP_M'\ $ with conditions on degrees and then create a continuous function using $\det$ characterizing this. Would you have a more natural way to proof $I \subset$ Int$(D)$ ?","['matrices', 'eigenvalues-eigenvectors', 'general-topology', 'linear-algebra']"
2582826,Invariant differential operators on equivariant vector bundles over Lie groups,"This is a quick and dirty formulation of my question, with the hope that experts will quickly figure out what I am looking for and provide a reference. Let $G$ be a Lie group, and let $\pi:E\to G$ be a finite dimensional real or complex $G$-equivariant vector bundle, i.e., $\pi(gs)=g\pi(s)$ for all $s\in E$. Then $E$ is trivial since the simply transitive action of $G$ defines $G$-invariant sections $e_j\in C^\infty(E)$ that comprise a global frame. The same happens with the tangent bundle $TG$ and the endomorphism bundle $\mathrm{Hom}(E)$. The vector space of (left) $G$-inavriant vector fields in $C^\infty(TG)$ is a Lie algebra isomorphic to the Lie algebra $\mathrm{Lie}(G)$, whereas the vector space of $G$-invariant endomorphism fields in $C^\infty(\mathrm{Hom})$ is an associative algebra isomorphic to $\mathrm{gl}(\dim E/G)$. Let $\nabla$ be the flat connection on $E$ corresponding to $G$-translation. Question: Let $\operatorname{D}:C^\infty(E)\to C^\infty(E)$ be a $G$-invariant partial differential operator. I believe that it can be written as
$$
\operatorname{D}=\sum_{k=0}^m D_k(\nabla^k),
$$ 
where $D_k\in\mathrm{Lie}(G)^{\otimes k}\otimes\mathrm{gl}(\dim E/G)$. I think I know how to prove this, but does anyone know of a reference where this is stated? Thank you.","['reference-request', 'vector-bundles', 'differential-geometry', 'lie-groups']"
2582844,Solution for $\displaystyle f'\left(x\right)=\cos\left(f\left(x\right)\right)$,"Let $f$ be defined for $x \in \mathbb{R}$ by
$$
f\left(x\right)=2\text{arctan}\left(e^x\right)-\frac{\pi}{2}
$$
I've shown that $f$ is odd and satisfies for $x \in \mathbb{R}$
$$f'\left(x\right)=\cos\left(f\left(x\right)\right)$$
To prove it, i've used that
$$
\cos\left(f\left(x\right)\right)=2\sin\left(\text{arctan}\left(e^x\right)\right)\cos\left(\text{arctan}\left(e^x\right)\right)
$$
And then use that
$$
\cos\left(\text{arctan}\left(e^x\right)\right)=\frac{1}{\sqrt{1+e^{2x}}}
$$
I've two questions : $\bullet$ Is that the unique solution for $f(0)=0$ ?
I've tried to prove it by supposing to different solutions and trying to prove there are infact equals with trigonometric formula but it does not seem to work. $\bullet$ Is there another way ( even wiser or faster ) to prove it ? Thanks for those who take time to answer.","['trigonometry', 'ordinary-differential-equations', 'functions']"
2582857,Solve the Recurrence Relation: $(b_{n+2})^2 - 7(b_{n+1})^2 + 12(b_n)^2 = (5n^2 + 3)4^n$,"Given the following recurrence equation: $$
(b_{n+2})^2 - 7(b_{n+1})^2 + 12(b_n)^2 = (5n^2 + 3)4^n
$$ Which after expanding the equation is equal to: $$
(b_{n+2})^2 - 7(b_{n+1})^2 + 12(b_n)^2 = 5n^2(4^n) + 3(4^n)
$$ Now since this is definitely not lineal and I have never worked with such types of recurrence equations before, I am not where where to begin in solving it. Where do I start and how do I proceed? UPDATE: Following from the answer provided by @h-h-rugh: $$
b_n^2 = 4^na_n \\
4^{n+2}a_{n+2} - 7(4^{n+1}a_{n+1}) + 12(4^na_n) = (5n^2+3)4^n \\
(4^2a_{n+2}-7(4)(a_{n+1}) + 12a_n)4^n = (5n^2+3)4^n
$$ Dividing by $4^n$: $$
16a_{n+2} - 28a_{n+1} + 12a_n = 5n^2+3 \rightarrow \text{ eq. 1}
$$ I then separate the equation: $$
a_n^{(h)} = 16a_{n+2} - 28a_{n+1} + 12a_n = 0 \\
a_n^{(p)} = an^2 + bn + c
$$ Substituting a_n^{(p)} en eq. 1: $$
16[a(n+2)^2 + b(n+2) + c] - 28[a(n+1)^2 + b(n+1) + c] + 12[an^2 + bn + c] = 5n^2 + 3 \\
- \\
16[a(n^2 + 4n + 4) + bn +2b + c] - 28[a(n^2 + 2n + 1) + bn + b + c] + 12[an^2 + bn + c] = 5n^2 + 3 \\
- \\
16[an^2 + 4an + 4a + bn + 2b + c] - 28[an^2 + 2an + a + bn + b + c] + 12[an^2 + bn + c] = 5n^2 + 3 \\
- \\
16an^2 + 64an + 64a + 16bn + 32b + 16c - 28an^2 - 56an - 28a - 28bn - 28b - 28c + 12an^2 + 12bn + 12c = 5n^2 + 3 \\
- \\
16an^2 - 28an^2 + 12an^2 + 64an -56an + 64a -28a + 16bn - 28bn + 12bn + 32b - 28b + 16c - 28c + 12c = 5n^2 + 3 \\
- \\
8an + 36a + 4b = 5n^2 + 3
$$ Would that be correct?","['recurrence-relations', 'sequences-and-series', 'discrete-mathematics']"
2582863,Behavior of a function defined by two infinite integrals around the singularity point,"Bounty ending tomorrow: Consider the following function defined by two infinite integrals:
$$
F(\epsilon) = \int_0^\infty 
	\frac{q^3 (1+q)e^{-2q}}{q^4+\epsilon^4} \, \mathrm{d} q
	+  \int_0^\infty \frac{\, G_q \, e^{-q}}{q^2(q^4+\epsilon^4)} \, \mathrm{d} q 
$$
where
$$
G_q := G \left( \left[ \left[\frac{1}{2} \right] , [\,]\right] , 
		\left[ \left[ \frac{9}{2},\frac{5}{2} \right], \left[\frac{3}{2}\right] \right] ,\frac{q^2}{4}  \right) \, , 
$$
with $G$ being the Meijer G-function.
The goal is to study the behavior of $F$ around the singularity point, i.e. as $\epsilon\to 0$.
Numerically, it can clearly be observed that $F$ scales logarithmically with $\epsilon$. I am wondering whether this behavior can be shown analytically via a rigorous analysis, e.g. using perturbation techniques. Your help or hints are highly appreciated and are most welcome. Thanks
H","['real-analysis', 'complex-integration', 'indefinite-integrals', 'integration', 'contour-integration']"
2582895,Finding the limit of a recursive sequence.,"Let $a_n \in \Bbb{R}$ such that: $$ a_{n+1}=1+\frac{2}{a_n}  \text{  and   } a_1=1$$ Prove that $\lim_{n \to +\infty}a_n=2$ We have that $a_n \geq 1,\forall n \in \Bbb{N}$ thus $$\limsup_na_n \geq \liminf_na_n \geq 1$$ Let $\liminf_na_n=l$ Then we have that $l^2-l-2=0$ thus $l=2,-1$ But $a_n \geq 1$ thus $l=2$ . Applying the same argument we prove that $\limsup_na_n=2$ Also we can also derive a contradiction if we assume that the limit superior and inferior are infinite. Is my attempt an efficient way to prove this statement? Also can someone provide additional solutions? Thank you in advance.","['limsup-and-liminf', 'sequences-and-series', 'calculus', 'limits']"
2582911,On necessary conditions for constrained optimization in $\mathbb{R}^{n}$,"I am confused by a what Liberzon writes in his book on optimal control on page 12 and beginning of 13 regarding necessary conditions for optimality. He starts of by showing the first order condition for constrained optimality of $f$ w.r.t $h$ i.e If $x$ is constrained minima of $f$ then $\nabla f(x)+ \lambda\nabla h(x)=0$ for some $\lambda$. To make this more explicit he defines the augmented cost functional, $\ell(x,\lambda)=\nabla f(x)+ \lambda\nabla h(x)$. He then shows that if $x$ is constrained minima and $\lambda$ is its Lagrange multiplier then $\nabla \ell=0$ at this point. All good and well so far. Next he argues, If $(x,\lambda)$ is a minima for $\ell$ then $h(x)=0$ and subject to these constraints $x$ also minimize $f$ i.e the gradient  of $\ell$ is zero. But then he stresses that this is not a necessary condition for optimality if we only assume that $x$ is a constrained minima of $f$. I don't understand what he means here. What situation is he referring to? And what does he wanna say? He just showed that there always are multipliers for any constrained minima and hence we should be able to repeat one of his above arguments to get that the gradient is zero of $\ell$. But this can't be it. Is he referring to a situation where he don't wanna assume anything about $\lambda$?. Does anyone understand this? His book is free online http://liberzon.csl.illinois.edu/teaching/cvoc.pdf Also here is the text OBSERVE THAT THE BOUNTY IS NOT AWARDED TO THE RIGHT ANSWER! Read comments if you wanna understand why he got it.","['multivariable-calculus', 'optimization', 'lagrange-multiplier']"
2582923,Iterations of $x^2 + y^2$,"We construct a sequence $S$ of distinct positive integers as follows 1) the sequence $S$ starts as $1,2,3$ 2) If $x,y$ are in the sequence , then $x^2 + y^2 $ is also in the sequence. 3) the sequence is strictly increasing. 4) the sequence is completely determined by the above. The questions are unsurprisingly the following : How dense are the integers in this set that belong in the interval $[a,b]$ ; A) compared to the Sum of 2 squares in that interval. B) compared to the integers in that interval. C) such that They are prime compared to the primes of the form $v^2 + u^2$. ?? Edit An example of a Number that is not in the sequence $S$ is $4^2 + 1^2 = 17$ because $4$ is neccessary to Sum to 17 ( 17 is a prime hence a Sum of 2 squares in only one way ) and $4$ is not in the set $S$. $4$ is not even a Sum of two (nonzero) squares ! I considered estimating the density as follows : $$z = z_1^2 + z_2^2$$ Where $z_1,z_2$ have the probability of Being a sum of 2 squares equal to ( about ) $ ln(z_1)^{-1/2} * ln(z_2)^{-1/2}$. See the Landau-Ramanujan results : https://en.m.wikipedia.org/wiki/Landau –Ramanujan_constant With Some "" handwaving "" we simplify ( informal )
to probability $ln(z)^{-1}$. However $z_1 = z_3^2 + z_4^2, z_2 = z_5^2 + z_6^2$ When both $z_1,z_2$ are large. And those new variables Also have to be the Sum of 2 squares !
Thus we now estimate the probabilty as $ln(z)^{-1} * ln(\sqrt z)^{-4} = 1/16 * ln(z)^{-5}$
These iterations and estimates go on until $z_k$ becomes small. Kinda. Finally I think the counting function approximation $$ \frac{b-a}{ b^{ln(2)}} $$ ( counting the numbers in $S$ that are in the interval $[a,b]$ approximately , inspired by the informal idea above ) Works pretty well ? Just my guess","['number-theory', 'dynamical-systems', 'sequences-and-series', 'sums-of-squares']"
2582943,Proving that the function $f(x) = \int_0^\infty \cos (w^3/3 - x w ) d w$ satisfies the equation $f'' + x f = 0$,"It is assumed that $x$ is real. Formally, we have $$ f'' = \int_0^\infty  -\cos (w^3/3 - x w ) w^2  d w , $$ and hence $$f'' + x f  = \int_0^\infty  \cos (w^3/3 - x w ) (-w^2 + x )  d w \\ = -\int_0^\infty  \cos (w^3/3 - x w ) d(w^3/3 - x w ) \\ = - \sin(w^3/3- x w ) |_0^\infty .  $$ The problem is that $\sin(w^3/3- x w ) $ does not converges as $w\rightarrow \infty$. Apparently, the problem is rooted in the fact that the expression for $f''$ is not well defined---it does not converge. So, could anyone give a simple, elementary proof?","['special-functions', 'ordinary-differential-equations', 'calculus']"
2582946,Parabola: Another Interesting Property,"Let ABCD be a quadrilateral. Suppose there exists a parabola W A with focus A, tangent to lines BC, CD, and DB, and a parabola W C with focus C tangent to lines AB, BD, and AD. Suppose that W A and W C are tangent to line BD at X and Y respectively. Prove that BX = DY. I wish to solve the above problem using coordinate geometry For the sake of simplicity we can assume either one parabola to be y² = 4ax, right? 
Could someone tell me how to proceed, and possibly post a detailed solution for the same? I think proving BX = DY can possibly be done using pure geometry/congruence, but, as a student of analytic geometry I wish to proceed using the cartesian coordinate system. Thanks a lot!","['analytic-geometry', 'conic-sections', 'algebraic-geometry', 'geometry']"
2582970,The Differential of the Gauss map is the Negative of the Shape Operator?,"$\newcommand{\sff}{II}$ $\newcommand{\mf}{\mathfrak}$ $\newcommand{\R}{\mathbf R}$ $\newcommand{\ab}[1]{\langle #1\rangle}$ Background Let $M$ be a Riemannian submanifold of a Riemannian manifold $\tilde M$ .
Let $\tilde \nabla$ and $\nabla$ denote the Levi-Civita connections on $\tilde M$ and $M$ respectively.
The $\tilde \nabla$ and $\nabla$ are related as follows: Let $X$ and $Y$ be smooth vector fields on $M$ .
Then $\nabla_XY$ at a point $p\in M$ is the orthogonal projection of $\tilde \nabla_XY$ on $T_pM$ . The second fundamental form of $M$ is defined as the map $\sff:\mf X(M)\times \mf X(M)\to \mf X(M)$ which takes a pair of vector fields $(X, Y)$ to be the orthogonal projection of $\tilde \nabla_XY$ onto $T_pM^\perp$ .
Thus $\tilde \nabla_X Y=\nabla_XY + \sff(X, Y)$ . Now suppose $M$ be a hypersurface in $\R^{n+1}$ , and $N$ be a smooth unit normal vector field along $M$ .
Thus for any two smooth vector fields $X$ and $Y$ on $M$ , we have $\sff(X, Y)=h(X, Y)N$ , for some smooth map $h:\mf X(M)\times \mf X(M)\to \R$ .
Define a map $G: M\to S^n$ as $G(q)=N_q$ for all $q\in M$ .
The map $G$ is called the Gauss Map , and the Gaussian curvature of $M$ at $p$ is defined as $\det(dG_p)$ . Question In the theorem below, the minus sign attached to $h$ is bugging me. For this means that the shape operator is the negative of the differential of the Gauss map. While the shape operator is supposed to be the same as the differential of the Gauss map.
I am unable to find a mistake in my proof. ""Theorem."" Let $M$ be a hypersurface in $\mathbf R^{n+1}$ , and let $G:M\to S^n$ be the Gauss map. Let $N$ be a unit normal vector field along $M$ .
Then $\ab{dG_p u, v}= -h(u, v)$ for all $p\in M$ and $u, v\in T_pM$ . Proof. Let $\gamma:(-\varepsilon, \varepsilon)\to M$ be a smooth curve with $\gamma(0)=p$ and $\dot \gamma(0)=u$ .
Let $V:I\to TM$ be the parallel vector field along $\gamma$ with $V(0)=v$ .
Now we have $\ab{G\circ \gamma(t),\ V(t)}=0$ for all $t$ .
Taking the derivative at $t=0$ , we have $$\ab{dG_p(\dot \gamma(0)),\ v} + \ab{G(\gamma(0)),\ \dot V(0)} =  0$$ Note that $\dot V$ is the covariant derivative of $V$ with respect to the Levi-Civita connection on $\R^{n+1}$ .
Thus $\dot V(0) = D_{t}V(0) + h(\dot \gamma(0), V(0)) N_{\gamma(0)}$ .
Substituting this in the equation above, we get $$\ab{dG_p(\dot \gamma(0)),\ v} + \ab{G(\gamma(0)),\ h(\dot \gamma(0), V(0))N_{\gamma(0)}} =  0$$ Putting $\dot \gamma(0)=u$ , $V(0)=v$ , and $G(\gamma(0))=N_p$ , we get $$\ab{dG_pu, v} + \ab{N_p,\ h(u, v) N_p} = \ab{dG_pu, v} + h(u, v) =  0$$ which proves the undesired result.","['riemannian-geometry', 'differential-geometry', 'curvature']"
2583003,How to attack the integral $\int_0^{\pi/4}\frac{\left(e^x-1\right)^3}{1-\cos(x)\cos(2x)}dx$?,"I've considered some integrals, some of the form $$\int_0^{\pi/4}\frac{\left(e^x-1\right)^3}{1-\cos(ax)\cos(bx)}dx.\tag{1}$$ I would like to know how to attack this kind of integrals. Can you help me about next integral? Is not required all details or a closed-form, only how start to work with the purpose to get the indefinite integral if you think that it is feasible, or well the definite integral, or justify a good approximation of it. Question. How to attack the integral $$\int_0^{\pi/4}\frac{\left(e^x-1\right)^3}{1-\cos(x)\cos(2x)}dx\,?$$ Many thanks.","['complex-analysis', 'real-analysis', 'integration', 'definite-integrals']"
2583024,Derive the posterior distribution and compute the posterior mean.,"Exercise: Suppose $X_1,\ldots,X_n|\Theta = \theta \stackrel{iid}{\sim} \operatorname{Pois}(\theta)$ and $\Omega\sim\operatorname{Ga}(\alpha, \beta)$. Derive the posterior distribution and compute the posterior mean. What I've tried: I know that the posterior distribution is equal to $$f_{\Theta|X}(\theta|x) = \dfrac{f_{X|\Theta}(x|\theta)\,f_\Theta(\theta)}{\int f_{X|\Theta}(x|\theta)\,f_\Theta(\theta)d\theta}$$ so with $f_{X|\Theta}(x|\theta)\,f_{\Theta}(\theta) = \dfrac{\theta^{\sum x_i}e^{-n\theta}}{\prod x_i !}\dfrac{\beta^\alpha}{\Gamma(\alpha)}\theta^{\alpha-1}e^{-\beta\theta}$ this means that $$f_{\Theta|X}(\theta|x)=\dfrac{\dfrac{\theta^{\sum x_i}e^{-n\theta}}{\prod x_i !}\dfrac{\beta^\alpha}{\Gamma(\alpha)}\theta^{\alpha-1}e^{-\beta\theta}}{\displaystyle\int\dfrac{\theta^{\sum x_i}e^{-n\theta}}{\prod x_i !}\dfrac{\beta^\alpha}{\Gamma(\alpha)}\theta^{\alpha-1}e^{-\beta\theta}d\theta}.$$ I think this is some sort of Beta distribution. However, I'm not sure which one and how to get there. Question: How do I solve this exercise? Thanks in advance!","['statistical-inference', 'probability-theory', 'probability-distributions', 'statistics', 'probability']"
2583037,"Having event with probability $p$, how can I get an event with probability $\sqrt{p}$?","I'd like a design a circuit that, given a random bit with probability $p$ to be zero, it outputs a random bit with probability $\sqrt{p}$ to be zero. Actually, I am rather looking for $\sqrt{p(1-p)}$, in case it makes things easier. Let's assume that we have an unlimited but finite stream of independent random bits and all the common logic gates (AND, OR, NOT, XOR...). If an exact solution wouldn't be possible in a finite setting. I would also be happy with an approximation, preferably one that can be arbitrarily expanded to reduce the error. I have already looked into a Taylor expansion around $p=1/2$, and it is a good option. The second degree expansion of $\sqrt{p(1-p)}$ is $\frac{1}{2} - (p - \frac{1}{2})^2 + O((p - \frac{1}{2}))^4$ which goes to $\frac{3}{4} + p(1-p)$ which is nice and easy to implement. I am here to see if there is a better solution. This question is highly related to the following question, but the answer is not general enough to satisfy my needs. Understanding what $\sqrt{p}$ means for an event of probability $p$",['probability']
2583053,Show that $f(x) = x^7 + x^5 + x^3 + x$ is bijective,"I want to show that the real polynomial function $f: \mathbb R \to \mathbb R, f(x) = x^7 + x^5 + x^3 + x$ is bijective. I want to show this without using the inverse or the derivative. I'm struggling to prove injectivity, because I see no easy way to arrive at $x = y$. What I have so far: Surjective: Because the degree is odd, we have $\lim_{x \to +\infty}(f(x)) = +\infty$ and $\lim_{x \to -\infty}(f(x)) = -\infty$. Because a polynom is continuous, we can apply the IVT to the interval $I = (-\infty,+\infty) = \mathbb R$ so that for every $y \in I$ there is a $x$ such that $f(x) = y$. Injective: Let $f(x) = f(y)$. Then $x^7 + x^5 + x^3 + x = y^7 + y^5 + y^3 + y$. Then ??? , so $x = y$.","['real-analysis', 'polynomials', 'functions']"
2583082,Solution for $x^9 + x^6 + x^4 - x^2 + 1=0$,"I learned that $5$ and higher order equations can not all be solved. I read the Abel-Ruffini theorem. My question: Let $$x^9+x^6+x^4-x^2+1=0$$ This equation can be expressed in the following way: $$x^9 + x^6 + x^4 - x^2 + 1=(x^4 + x + 1) (x^5 - x + 1)=0$$ Yes, I know there are only non-radical solution for $x^5-x+1=0$. But,  $x^4+x+1=0$ can be solve. Finally, I think we can say that, There are $5$ non-radical solution and there are $4$ radical solution for this equation.
Is it correct? My problem is this: For the same equation, I can not understand, Why some of the roots can be expressed in radicals, but the other part can not be expressed? But, the equation is the one/same equation and this polynominal has only one graph..? Is this equation a ""semi-solvable"" equation? Note: I do not have math education. I apologize for the flawed question.","['algebra-precalculus', 'radicals', 'polynomials', 'calculus']"
2583083,Elegant but pugnacious inequality,"I have a solid and difficult problem that I can't solve, this is the following : Let $p,q,r,s,t,u$ be real positive numbers then we have :
  $$\frac{1}{s}+\frac{1}{t}+\frac{1}{u}+\frac{-3}{\frac{p+r+q}{2}-s-t-u}\geq \frac{1}{p}+\frac{1}{q}+\frac{1}{r} $$ With the condition :
  $$\frac{s}{p}+\frac{t}{q}+\frac{u}{r}=1 $$ My geometric try : We know this : Let ABC be a triangle, and let P, Q, R be any points in the
plane distinct from A; B; C; respectively and suppose the cevians AP; BQ; CR meet at T then we have : $$\frac{TQ}{AQ}+\frac{TP}{BP}+\frac{TR}{CR}=1 $$ So it's a geometric interpretation of our condition . Now put the following substitution :
$\frac{1}{s}=a$$\quad$$\frac{1}{t}=b$ $\frac{1}{u}=c$$\quad$$\frac{1}{p}=x$ $\frac{1}{q}=y$$\quad$$\frac{1}{r}=z$ We get : $$a+b+c+\frac{-3}{\frac{\frac{1}{a}+\frac{1}{b}+\frac{1}{c}}{2}-\frac{1}{x}-\frac{1}{y}-\frac{1}{z}}\geq x+y+z $$ Furthermore we have for an interior point (in $ABC$) $P$, the Barrow's inequality .Finally we remark that the inequality above seems to have the behavior of the Barrow's inequality . Edit : As point out in the comment of Doyun Nam I add the implicit condition $p>q+r$$\quad$$q>p+r$$\quad$$r>p+r$.Thanks to him. After that I have no more idea...Thanks a lot.","['real-analysis', 'inequality', 'euclidean-geometry']"
2583146,Vector bundle isomorphism from a collection of vector space isomorphisms,"I've recently started studying vector bundles, and I cannot seem to understand this. I feel as though I am missing some simple, crucial fact. Suppose we have two vector bundles $E$ and $F$ over the same manifold $M$. My question is this: If I have a collection of functions $\{ f_x \}_{x\in M}$, where each $f_x : E_x \rightarrow F_x$ is an isomorphism of vector spaces, under what conditions can I define a function $f: E \rightarrow F$ that is a vector-bundle isomorphism? Clearly I can just define $f$ fibrewise based on the family of $f_x$'s, but how can I ensure that it is a smooth map between $E$ and $F$?","['vector-bundles', 'differential-geometry']"
2583211,Wilk's proof of convergence of LRT,"In his book, Mathematical statistics, Wilks uses several times an argument that is a bit obscure to me (I'm referring to page 411 of the book). Basically, we have a sequence of roots of the maximum likelihood equation such that $\hat \theta_n \to \theta_0$ almost surely and then he does a Taylor expansion of the form: $$\sum log(f(x_i|\theta)) = \sum log f(x_i|\hat \theta_n)+ (\theta_0 - \hat \theta_n)^2/2 \sum \frac{d^2}{d\theta^2} log f(x_i| \theta^*)$$ with $|\theta_0 - \theta^*| < |\theta_0 - \hat \theta_n|$ where the first term disappears since $\hat \theta_n$ is the mle. Then he states: $\forall \epsilon > 0. \exists n_{\epsilon}.\forall n \ge n_{\epsilon}. P[\text{
 above equality holds}] > 1 - \epsilon$. I think that this should follow from the almost surely convergence but how can I prove it?","['probability-theory', 'hypothesis-testing', 'statistical-inference']"
2583216,Chasing down this lemma on infinite abelian locally finite groups,"I'm translating a paper from the 70's where the author cites László Fuchs' Abelian groups (the entire book, no page number). As you can imagine, given the size and differences in editions of Fuchs' book, I am having trouble connecting the dots. Here is the situation: $G$ is an infinite, abelian, and locally finite group. $H$ is an infinite subgroup of $G$ which is of finite index in $G$. From this, the step in a proof I am reading continues this way ... we deduce the socle of $G$ is of finite length and that $G$ is an Artinian group. The justification of the block above is what I am asking about. In the newest edition (2015) of Fuchs' Abelian groups , I feel think the relevant theorem is this: Theorem 5.3 (Prufer, Kurosh, Yahya) T.F.A.E: (i) $A$ is finitely cogenerated (ii) $A$ is an essential extension of a finite group (iii) $A$ is torsion of finite rank (iv) $A$ is a direct sum of a finite number of cocyclic groups (v) the subgroups of $A$ satisfy the minimum condition [later] Obeserve that (ii) is equivalent to the finiteness of the socle in a torsion group. This is the very first theorem in the section, preceded only by the definition of finite cogeneration. The local finiteness obviously makes $G$ torsion, so I can see why both conclusions are linked... but how do you use the fact that $|G:H|$ is finite to prove one of these conditions? I feel like I'm overlooking some connection between $G/H$ and $G$, possibly about the socles. In general module theory, there usually isn't a connection between the two, but perhaps since $G$ is locally finite there is a connection in my blind spot. I've decided the original source is in order: $A$ is a right self-injective ring, and $G$ is, as proven in an earlier step, at least a locally finite group. The $G_1$ and $H_1$ in this snippet are the $G$ and $H$ I was referring to in my original description. The citation (7) refers to Fuchs' Abelian groups . The theorem of Faith has to do with the injectivity of a free $A[H_1]$ module. Basically it allows you to conclude that $A[G_1]$ is a direct sum of finitely many copies of $A[H_1]$, and this means $|G_1:H_1|$ is finite. Perhaps the theorem that's needed (which did not come out in my description above) is that if $G$ is a locally finite, infinite abelian group whose infinite subgroups are all of finite index, $G$ is Artinian? If this is the case, then a reference to that result would be an acceptable solution to this problem. (Hopefully in Fuchs, but elsewhere would be fine.)","['abelian-groups', 'abstract-algebra']"
2583221,What is the coequalizer of two $m \times n$ matrices in $\text{Matr$_K$}$?,"What is the coequalizer of two $m \times n$ matrices in $\text{Matr$_K$}$?$(K$ is a commutative ring, objects are positive integers, and arrows are $m \times n$ matricies$)$ If $A_{m \times n},B_{m \times n}$ are matrices (arrows $A,B: n \rightarrow m)$, then I would have to find a matrix $E_{e \times m}$ with $EA=EB$ such that for every $H_{d \times m}$ there's a unique $H'_{d \times e}$ such that $H' \circ E = H$. I first tried to work with $E$ being the zero matrix but it doesn't satisfy the above composition. Anyone have any ideas?","['category-theory', 'linear-algebra']"
2583277,Definition of limit in general metric spaces,"Let $(Z,d), (Y,d')$ be metric spaces, $a$ a limit point of $X$ in $Z$.
  Let $X \subset Z, b \in Y$ and $f: X \to Y$ be a function. We then say
  that: $$\lim_{x \to a} f(x) = b \iff  \forall \epsilon > 0:\exists\delta >
 0: \forall x \in B_Z(a, \delta)\cap(X \setminus\{a\}): f(x) \in B_Y(b,
 \epsilon)$$ As far as I know, it is possible that $a \notin X$. For example, consider the set $\{n^{-1}\mid n \in \mathbb{N}_0\}$ which has the limit point $0$ in the real numbers. Then, what does the notation $X \setminus\{a\}$ mean? Isn't set complement only defined for subsets?","['metric-spaces', 'elementary-set-theory', 'limits']"
2583320,Difference between sufficient and non sufficient statistics,"I have a few questions here. Could someone please give me (in plain English, avoiding as much statistical jargon as possible and resorting to examples one might understand with a only a basic knowledge of statistics and/or mathematics whenever possible) an example of a non sufficient statistic, and go on to explain why it isn't sufficient? When we say ""sufficient,"" what does that mean? Sufficient for... what exactly? Why is it so important? What makes a sufficient statistic sufficient?","['statistics', 'statistical-inference']"
2583351,Basu's theorem for normal sample mean and variance,"I'm working on the following problem: Suppose that $X \sim N(\mu,\sigma^2)$. Let the sample mean and variance, $\overline{X}$ and $S^2$ be defined as usual so that $\mathbb{E} S^2 = \sigma^2$. Prove that the sample mean is independent of the sample variance. Given that both $\mu$ and $\sigma^2$ are unknown, find the MVUE for $\mu \sigma^2$. The second part is simple. Given that $\overline{X}$ and $S^2$ are independent, and that $(\overline{X}, S^2)$ is a jointly complete and sufficient statistic for $(\mu, \sigma^2)$, we have that $\phi ( \overline{X}, S^2) = \overline{X} S^2$ is unbiased for $\mu \sigma^2$. By Lehmann-Schefee, $\phi$ is the MVUE for $\mu \sigma^2$. To prove independence, I would like to implore Basu's Theorem. This says, briefly, that any boundedly (which I will ignore) complete sufficient statistic is independent of any ancillary statistic. It is not hard to show that the normal distribution is exponential class. Moreover, given known variance, we get that $\sum X_i$ is a complete and sufficient statistic for $\mu$. It is clear that $S^2$ is ancillary, since (accept my abuse of notation) for any $a \in \mathbb{R}$,
$$
S^2+a = \frac{1}{n-1} \sum_{i=1}^n \Big( X_i + a - \frac{1}{n}\sum_{i=1}^n \big( X_i + a  \big) \Big)^2 = \frac{1}{n-1} \sum_{i=1}^n \Big( X_i - \overline{X}\Big)^2  
$$
so that $S^2$ is location invariant, and hence ancillary. So, if we were only interested in estimating $\mu$, we would be done. My question is, how do I deal with applying Basu's theorem when I have a jointly complete and sufficient statistic. Does it suffice to only show that $\sum X$ is complete and sufficient statistic for $\mu$ even though I'm estimating a function of $\mu$ and $\sigma^2$? Note: there is a similar (mostly unanswered), but not identical question here: UMVUE using complete and sufficient statistic","['independence', 'statistics', 'probability']"
2583409,Measure of the image of a function,"Suppose $f:\mathbb{R}\rightarrow\mathbb{R}$ is a non-decreasing continuous function (probably continuous isn't needed). Let $g(x)=f(x)+x$. Does it follow that for any measurable set $S$, $m(g(S))=m(f(S))+m(S)$? I'm pretty sure it does, but can't figure out how to prove it. This came up while trying to prove Folland 2.1.9.c (where $f$ is the Cantor function and $S$ is the Cantor set).","['real-analysis', 'lebesgue-measure', 'measure-theory']"
2583417,Spectral radius is not matrix norm.,"I have seen an example of matrix $$A = \begin{bmatrix}
			0 & 1  \\
			0 & 0
		\end{bmatrix}$$ whose spectral radius is zero therefore the spectral radius is not matrix norm. Why the spectral radius is not matrix norm in this case Is it possible that $\|A\|=\epsilon$?","['matrices', 'normed-spaces', 'spectral-radius', 'linear-algebra']"
2583429,what is the integration of $\int\cot(e^x)\cdot e^x dx$?,"what is $$
\int\cot(e^x)\cdot e^x dx
$$ This is my answer is it right $$
u=e^x\\
du=e^x dx\\
dx=\frac{1}{e^x}du
$$ Then $$\int\cot(u)\cdot u \frac{1}{u}du=\int \cot(u)\ du=\csc^2(u)$$ Is this answer right","['real-analysis', 'exponential-function', 'trigonometry', 'calculus', 'integration']"
2583447,Median of discrete and continuous random variables.,"If case of continuous random variables, we define median 'm' as a value such that the $P(X\ge m)=0.5$ and $P(X\le m)=0.5$ .
This link also mentions the same https://www.google.co.in/url?sa=t&source=web&rct=j&url=https://www.wiley.com/legacy/Australia/Landing_Pages/c12ContinuousProbabilityDistributions_web.pdf&ved=0ahUKEwiCvLm0va3YAhUBro8KHUaYA-sQFgg6MAI&usg=AOvVaw1qTWw5kE9ZsgWJ6RtMjngD For discrete random variables, $P(X\ge m)\ge0.5$ and $P(X\le m)\ge0.5$ Why is the cumulative probability required to be greater than or equal to $0.5$ in case of discrete random variables and only equals to $0.5$ in case of continuous random variable?","['median', 'probability-distributions', 'statistics', 'probability', 'random-variables']"
2583454,Why does multiplying by $\textbf{A}^T$ make a previously unsolvable linear system solvable,"Consider for instance the linear system: $$\left(
\begin{array}{cc}
 1 & 2 \\
 3 & 4 \\
 5 & 6 \\
\end{array}
\right).\left(
\begin{array}{c}
 x \\
 y \\
\end{array}
\right)=\left(
\begin{array}{c}
 1 \\
 2 \\
 4 \\
\end{array}
\right)$$ This is over determined and thus has no solution. Yet, by simply multiplying both sides by $\textbf{A}^T$: $$\left(
\begin{array}{ccc}
 1 & 3 & 5 \\
 2 & 4 & 6 \\
\end{array}
\right).\left(
\begin{array}{cc}
 1 & 2 \\
 3 & 4 \\
 5 & 6 \\
\end{array}
\right).\left(
\begin{array}{c}
 x \\
 y \\
\end{array}
\right)=\left(
\begin{array}{ccc}
 1 & 3 & 5 \\
 2 & 4 & 6 \\
\end{array}
\right).\left(
\begin{array}{c}
 1 \\
 2 \\
 4 \\
\end{array}
\right)$$ We find that the system now has a unique solution, which is the (x,y) that minimizes the squared error. Now I understand the derivation of why multiplying by the transpose helps to find the pseudoinverse which then helps to perform OLS regression, but my question is perhaps a bit more fundamental. How can multiplying both sides of an equation by a matrix change a system which previously had no solutions into one that has a unique solution? This seems to against what I assumed that the solutions to $\textbf{A}x = \textbf{B}$ were the same as the solutions to $\textbf{P}\textbf{A}x = \textbf{P}\textbf{B}$.",['linear-algebra']
2583484,Is there a name for this type of topology?,"Let $\tau$ be a topology on $X$ such that for every $a, b$ in $X$ there exists a bijective function $f$ that is continuous and has a continuous inverse such that $f(a) = b$. Examples of such a topology: the topology induced by the euclidean metric on $\mathbb R^n$ the discrete topology, the trivial topology non examples:
$\tau = \{\emptyset,a,\{b,c\},X\}$ where $X = \{a,b,c\}$ Is there a name for these type of topologies?",['general-topology']
2583489,Find the minimum value of $f(x)= N-x^{\lfloor log_x(N)\rfloor}$ for $1 <x < 1000$,"Is there any efficient way to find the $x$ value for which $f(x)$ is the minimum in the range of $1 < x < 1000$ so that $$f(x)= N-x^{\large{\lfloor \log_x(N)\rfloor}}\;?$$ Note: $N,x$ are positive integers and $N\gt x$.","['number-theory', 'functions', 'elementary-number-theory']"
