question_id,title,body,tags
4335493,how to solve $(y')^4 x -2 y (y')^3 + 12 x^3 = 0$,"Is there a smart way to solve this first order ode $$
    (y')^4 x -2 y (y')^3 + 12 x^3 = 0
$$ This is problem from Ordinary differential equations and their solutions. By George Moseley Murphy. 1960. I solved it myself, but my method is a brute force. First solved for $y'$ , which gave 4 solutions. This generated 4 ode's to solve. Each one of these ode's turned out to be isobaric. After applying the isobaric transformation, the ode becomes separable. However, the integrals are so complicated and could not solve them even on the computer.  So the solutions are left with unevaluated integrals. But they were verified correct by Maple. But will not post them here, as the integrals are too large. Maple solves this and gives these simple 5(!) solutions The Maple trace says it used trying 1st order ODE linearizable_by_differentiation Which I do not know how. Here is the full trace ode:=x*diff(y(x),x)^4-2*y(x)*diff(y(x),x)^3+12*x^3 = 0; infolevel[dsolve]:=5; 
dsolve(ode)

 trying 1st order ODE linearizable_by_differentiation
 -> Solving 1st order ODE of high degree, Lie methods, 1st trial
 -> Computing symmetries using: way = 2
 -> Solving 1st order ODE of high degree, 2nd attempt. Trying parametric methods
  *** Sublevel 3 ***
  Methods for first order ODEs:
  --- Trying classification methods ---
  trying homogeneous types:
  trying exact
  Looking for potential symmetries
  trying an equivalence to an Abel ODE
  trying 1st order ODE linearizable_by_differentiation
  -> Calling odsolve with the ODE diff(y(x) x) = (3*(x^4+12*y(x)^2)*y(x)/x- 4*y(x)*x^3)/(-x^4+36*y(x)^2) y(x)
  *** Sublevel 3 ***
  Methods for first order ODEs:
  --- Trying classification methods ---
  trying a quadrature
  trying 1st order linear
  <- 1st order linear successful
  <- 1st order, parametric methods successful My question is, how did Maple obtain these simple solutions? From trace it says it used Lie symmetries which I am still learning. Does anyone see a ""simple"" method to solve this ode using some smart transformation and be able to obtain the solutions found by Maple?",['ordinary-differential-equations']
4335514,Real and complex line integrals:,"From complex analysis I know that we define the complex line integral as follows: $$\int_\gamma f(z)dz = \int_a^bf(\gamma(t))\cdot\gamma'(t)dt \tag 1$$ Assuming that $\gamma$ is continuously differentiable with endpoints $a$ and $b$ . Back then I did not give this too much thought, as it seems like we are simply substituting $\gamma(t)$ for $z$ . Now we are defining the line integral over a real valued scalar field $\Omega$ as follows: $$\int_C \Omega(r) = \int_a^b\Omega(r(s))ds$$ Assuming that $r(s) = (x(s), y(s), z(s))$ is a curve parametrized by arc length. To generalise this definition for curves not parametrized by arc length we use the formula for the arc length $s$ : $$s = \int \lvert r'(t)\rvert dt \implies \frac{ds}{dt}=\lvert r'(t)\rvert$$ $$\therefore \quad \int_C \Omega(r) = \int_a^b\Omega(r(s))ds \stackrel{?}{=} \int_a^b\Omega(r(t))\frac{ds}{dt}dt = \int_a^b\Omega(r(t))\lvert r'(t)\rvert dt \tag 2$$ My two questions are: Apart from the modulus, $(1)$ and $(2)$ look very similar. Is there any intuitive connection between the the complex and real line integral? Or is it just a coincidence. After all, the multiplication $\cdot$ in $(1)$ is the complex multiplication, so these definitions may be not as similar as I think. What exactly is happening at the penultimate step in $(2)$ ? My chain of equal signs seems very handwavy, so I am assuming there is some rigour missing in my explanation.","['integration', 'contour-integration', 'vector-analysis']"
4335524,"Finding the Picard Group of $\mathbb{C}[x,y,z]/(x^2+y-z)$. [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I'm having problem comprehending the Picard group of a ring, and figured that perhaps it would be easier if I had a few examples of actual computations of Picard groups to look at. Thus, my question to you good people is, given the ring $\mathbb{C}[x,y,z]/(x^2+y-z)$ , how does one go about finding the Picard group? EDIT 1: Seeing this question has already gotten 3 votes for closing for not following proper community guidelines, let me say I hear your concerns. Just give me another two hours or so to get home and elaborate a little, and I hope I shouls be able to rectify that! EDIT 2: Okay, so, the complaint raised against this question has been that it does not follow certain standards for questions on this wonderful forum, specifically the following: Please provide additional context, which ideally explains why the question is relevant to you and our community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. And, I say, fair enough. It was rather out asked without context, I shall now try to remedy this. The closest analogy that I can think of to finding the Picard group of a ring is finding the representation ring of a finite group, something that is a standard homework question in undergraduate courses in group and representation theory. Usually, the groups being the object of interest in these homeworks are, well, not too sophisticated, but neither are they too trivial. Stuff like, say, the tetrahedral symmetry group or the octahedral symmetry group, etc. Nevertheless, when it comes to finding these representation rings (the question is often phrased rather as find the reduced character table , which is of course equivalent), the student has many tools available to them which are explained in intricate detail during their lectures. They are given character theory with its class functions, its inner products, and the Grand Orthogonality Theorem. They are given Schur's lemma and Maschke's theorem, from which they are given the result that they can decompose the regular representation into the irreducible representations (up to isomorphism) in a rather neat form.  And they are given the result that there exists a bijection between the conjugacy classes of a group and the irreducible representations of the same. As a consequence, though it might prove tedious, they have all the ingredients for actually computing the reduced character table of the group. And so, I suppose that my real question here is, not specifically, what is the Picard group of $\mathbb{C}[x,y,z]/(x^2+y-z)$ , but rather, how does one in general go about finding the Picard group of a ring, in the same manner as one would go about finding the representation ring of a finite group? What is the ""analogy"" for Schur's lemma, Maschke's theorem, the inner product of characters, etc. What is the process? I hope that should satisfy you and provide you with the context you requested.","['k-theory', 'algebraic-geometry', 'ring-theory', 'abstract-algebra']"
4335563,Find the minimum posssible integer value of the summation,"Let $f(x)$ is a continuous, increasing and positive value function in the interval $[0,a]$ such that $$\int_0^af(x)dx=20$$ Then find the minimum posssible integer value of the following summation $$a\left[f\left(\frac a{20}\right)+f\left(\frac {2a}{20}\right)+\cdots+f\left(\frac {20a}{20}\right)\right]$$ My attepmts Let $f(x)=x$ then $\frac {a^2}{2}=20\implies a^2=40$ $$a\left[f\left(\frac a{20}\right)+f\left(\frac {2a}{20}\right)+\cdots+f\left(\frac {20a}{20}\right)\right]=a\left(\frac{21a}{2}\right)=\frac{21}{2}a^2=420$$ I know that my method is completely nonsense , but I dont know anything about the problem...","['integration', 'calculus', 'summation']"
4335639,Does $\sum_{n=2}^{\infty} \frac{(-1)^n}{\ln(n)}$ have a closed form?,"I was considering the sum $ \sum_{n=2}^{\infty} \frac{(-1)^n}{\ln(n)} $ which I realizes converges extremely slowly (due to the slow growing nature of the logarithm). I was curious if someone knew a closed form for this in terms of literally ANY OTHER functions? Plugging this into Wolfram alpha didn't reveal anything particularly interesting: https://www.wolframalpha.com/input/?i=sum+%28-1%29%5En%2F%28ln%28n%29%29+n+%3D2+to+infinity I suppose that a good place to start would be try to understand the function $$ \sum_{n=2}^{\infty} \frac{x^n}{\ln(n)} $$ And then evaluate it at $x=-1$ . This function also was unfamiliar to me, so I tried to look at the even simpler object: $$ \sum_{n=1}^{\infty} \ln(n)x^n $$ Which does happen to have a restatement as a lambert series: See here: https://en.wikipedia.org/wiki/Lambert_series#Examples (particularly the section called Von Mangoldt Function). But even that series seems to be dependent on properties of prime numbers (perhaps there might be a way to get the riemann zeta function involved). Related: Generally speaking $\sum_{n=1}^{\infty} \frac{(-1)^n}{n^q} $ for small rational q (ex: $q = \frac{1}{2}$ or $q = \frac{1}{3}$ ) could be resolved using the riemann zeta function. The next slowest growing function that I could think of beyond polynomial roots was a logarithm which motivates this question.","['number-theory', 'sequences-and-series']"
4335712,How to calculate $ \mathbb{E}\left[X|W=0\right] $,"Let $ X\sim\mathcal{N}\left(0,1\right) $ and define $$ W=\begin{cases}
0 & X<0\\
X & X\geq0
\end{cases} =  X\boldsymbol{1}_{X\geq0} $$ How can I calculate $ \mathbb{E}\left[X|W\right]$ ? Here's what I have tried so far: I want to identify this random variable for any value $ w $ such that $W=w$ . So I first noticed that And now all I have to do is to calculate $ \mathbb{E}\left[X|W=0\right] $ .
Since $W$ is neither a continuous or a discrete time variable, Im not sure how to express the conditioned density. Any help would be appreciated. Thanks in advance.","['conditional-expectation', 'measure-theory', 'probability-theory']"
4335724,Suppose that $X$ is Hausdorff. Show that $X$ is locally path connected.,"Let $f:[0,1]\rightarrow X$ be a continuous surjective function to a Hausdorff space $X$ . Prove that $X$ has the following property: For every $x\in X$ and every neighborhood $U$ of $x$ , there exists a neighborhood $V$ of $x$ such that for all $a,b\in V$ there is a path from $a$ to $b$ contained in $U$ . ######## I have been going through this thinking of things that could potentially help. I realize that what we are trying to show is equivalent to showing that $X$ is locally path connected. I was thinking about using that a space $X$ is locally path connected if and only if for every open set $U$ of $X$ , each path component of $U$ is open in $X$ . I was also thinking of using that since $X$ is Hausdorff, we know that any closed subset is going to be compact, and then somehow intersecting the finite subcover with an open neighborhood of $x\in X$ to get a smaller neighborhood. (Edit: the comments have pointed out that this is false. I was thinking compact in Hausdorff is closed. That being said it made me think of how the image of a compact set under a continuous map is compact)
Edit: using that the image of [0,1] will be compact, can we still use the idea of a finite subcover to find a smaller neighborhood of $x$ ? I was also thinking of how the image of a connected space under a continuous map is connected. With all of these thoughts together, I was unsure of how to proceed. Any suggestions for how to proceed would be appreciated.","['connectedness', 'general-topology', 'path-connected', 'compactness']"
4335730,Prove that $\frac{e^x-1}{x}$ is bijective,"I am trying to prove that this function is bijective, but I don't know how to do it. $$f:x \mapsto \frac{e^x-1}{x}$$ For that, I try to use the fact that this function has necessarily a reciprocal if it is bijective. So, I think I need to prove that : $$f(x)=y \Leftrightarrow x = f^{-1}(y)$$ If I understand correctly, this means that : $$\frac{e^x-1}{x} = y \Leftrightarrow x = \frac{y}{e^y-1}$$ But my problem is that I don't know how to do this, and I don't know if there is a more efficient way to solve the problem. Can anyone help me solve this problem?","['functions', 'exponential-function', 'real-analysis']"
4335733,conditional expectation with random sum,"If $N$ is a random natural number, and $X_1,\ldots,$ is a sequence of not necessarily identically distributed random variables, the generalized Wald identity states that $$E\left(\sum_{i=1}^NX_i\right)=E\left(\sum_{i=1}^NE(X_i)\right).$$ Is there an analogous identity for $$E\left(\frac{1}{N}\sum_{i=1}^NX_i\right)?$$","['probability-theory', 'probability']"
4335745,What is the conormal sheaf?,"I'm having some silly confusion about the definition so I would really appreciate it if someone can help me out with this definition. Let $i: X \hookrightarrow Y$ be a closed immersion cut out by $\mathscr{I} \subseteq \mathcal{O}_Y$ . Then, Vakil defines the conormal sheaf of this closed immersion to be $\mathscr{I}/\mathscr{I}^2$ as viewed as a quasi-coherent sheaf on X. The italicized part is essentially my confusion. How are we viewing this as a quasi-coherent sheaf on $X$ ? The most obvious way to do this is to take $i^*(\mathscr{I}/\mathscr{I}^2)$ but I don't think this is right. In particular, this question seems to take it to be $i^* \mathscr{I} = i^{-1}(\mathscr{I}/\mathscr{I}^2)$ . I find this unsettling though. By definition, $\Delta: X \to X\times_YX$ is a locally closed immersion and we define $\Omega_{X/Y}$ to be the conormal sheaf of this embedding. This would be fine, although Hartshorne defines $\Omega_{X/Y} = \Delta^*(\mathscr{I}/\mathscr{I}^2)$ instead of what I would have expected from above, $\Delta^*(\mathscr{I})$ . What is going on here? To summarize, is the conormal sheaf of $i: X \hookrightarrow Y$ defined to be $i^*(\mathscr{I})$ , $i^*(\mathscr{I}/\mathscr{I}^2)$ , or both? Thank you very much for any help.","['algebraic-geometry', 'commutative-algebra', 'sheaf-theory']"
4335758,"Is $\frac{\partial}{\partial x} \cos{\sqrt{x^2+y^2}}$ indeterminate or zero at (0,0)?","I'd like to know what's the derivative of $f(x,y)=\cos{\sqrt{x^2+y^2}}$ at $(0, 0)$ . WolframAlpha says it is indeterminate . However, if we apply the definition, we can actually evaluate it to zero: $$
f_x(0,0)
  = \lim_{h\to 0} \dfrac{f(h,0) - f(0,0)}{h}
  = \lim_{h\to0} \dfrac{\cos{|h|}-1}{h}
  = \lim_{h\to0} \dfrac{\cos{h}-1}{h}
  = 0
\\
f_y(0,0)
  = \lim_{k\to 0} \dfrac{f(0,k) - f(0,0)}{k}
  = \lim_{k\to0} \dfrac{\cos{|k|}-1}{k}
  = \lim_{k\to0} \dfrac{\cos{k}-1}{k}
  = 0
$$ Aside question: if Wolfram is wrong, how often it happens in your experience? Does it ever happen?","['multivariable-calculus', 'calculus', 'wolfram-alpha']"
4335759,On p-group of maximal class,"Let $G$ be a $p$ -group of order $p^{4}$ and exponent $p$ . I want to prove that if $G$ has no nontrivial direct factors then it is of maximal class. My try:
Let $H$ be a proper nonabelian subgroup of $G$ , then $|H|=p^{3}$ . Now, let $x\in C_G(H)$ . If $x\notin H$ then $G=\langle x\rangle\cdot H$ , a contradiction. So, $C_{G}(H)<H$ . Did I miss something? Thank you in advance.","['group-theory', 'finite-groups', 'p-groups']"
4335764,Fubini and induction for a sum over a set $Q$,"How to calculate $$
\int_{Q}\left(x_{1}+x_{2}+\ldots+x_{n}\right)^{2} d \lambda_{n}
$$ whereas $n \geq 2$ and $$
Q=\left\{\left(x_{1}, \ldots, x_{n}\right) \in \mathbb{R}^{n}: 0 \leq x_{i} \leq 1, i=1, \ldots, n\right\}$$ I know that I have to use induction and Fubini to calculate $$
I(n) = \int\limits_0^1\int\limits_0^1\cdots\int\limits_0^1 \left(x_{1}+x_{2}+\ldots+x_{n}\right)^{2}dx_n\cdots\,dx_2\,dx_1
$$ And I already started to calculate the integrals for $n=2,3,4$ but the numbers I got did not enlighten me and I have no idea for an induction hypothesis. I also exchanged $x$ with $1$ but I still have no clue. Maybe I am doing something wrong. Thanks for help in advance.","['integration', 'measure-theory', 'fubini-tonelli-theorems', 'induction']"
4335814,Solvable number fields,"I think that unsolvability of the general quintic and higher degree polynomial equations by radicals is really cute, but also a little bit of a shame. I mean wouldn't it be nice if they were all solvable. Though formulas would be horrible looking like for the quartic, so may be not a bad thing after all. Anyway I am wondering if there are concrete examples of number fields in which every polynomial equation has a solution by radicals. Am I right in thinking that the algebraic closure of this kind of a field will have a solvable group as its Galois group over the field? In that case it'd need to be a solvable subgroup of the Galois group of $\mathbb{Q}$ whose quotient is a finite group? Are there any known examples of these, or do they not exist, or it's something else?","['galois-theory', 'number-theory']"
4335822,Showing a function is unbounded,"Let $f(x)=x\cos x+\sin x$ . Show that for every $M>0$ and every $k\geq 1$ , there is $x_0>M$ such that $f(x)\geq k$ , $\forall x\in [x_0,x_0+\frac{1}{k}]$ . If we replace $x$ by $2\pi n$ then $|f(2\pi n)=|2\pi n|$ . Can this help? Can I get a help to start it?",['real-analysis']
4335823,"Let $f:\mathbb{C}\to \mathbb{C}$, $f(z)=z^2$ and $B = \{z \in \mathbb{C},Re(z)\leq0\}$. Show that $f^{-1}(B)$ its a closed set","Let $f:\mathbb{C}\to \mathbb{C}$ , $f(z)=z^2$ and $B = \{z \in \mathbb{C},Re(z)\leq0\}$ . Show that $f^{-1}(B)$ its a closed set. This is my attempt: Let $w \in B, w =a+bi, a\leq 0$ Putting in the polar form: $|w|\cdot \cos(\theta) = a \implies \cos(\theta)=\frac{a}{\sqrt{a^2+b^2}}\leq 0$ $\cos(\theta)\leq 0 \implies \frac{\pi}{2}\leq\theta\leq\frac{3\pi}{2}$ I think that the interval of values to the argument of w its relevant to the question.
But I dont know how to proceed from this point. I tried to part of the definition of a inverse function: $$ f: A \to B $$ $$ f^{-1}(X) = \{x\in A: f(x) \in X\} $$ But i didnt get anywhere as well","['complex-analysis', 'complex-numbers']"
4335826,Different solutions to equation with trigonometric functions,"The equation is $\left(\frac{1}{cos\,x}+\frac{1}{sin\,x}\right)(sin\,x+cos\,x)=cot\,x+1$ where $x\in(-\frac{\pi}{2},\frac{\pi}{2})$ . Let's simplify this in two different ways. First Method Second Method $\left(\frac{1}{cos\,x}+\frac{1}{sin\,x}\right)(sin\,x+cos\,x)=cot\,x+1$ $\left(\frac{1}{cos\,x}+\frac{1}{sin\,x}\right)(sin\,x+cos\,x)=cot\,x+1$ $\left(\frac{sin\,x+cos\,x}{cos\,x\,sin\,x}\right)(sin\,x+cos\,x)=cot\,x+1$ $\left(\frac{1}{cos\,x}\right)(sin\,x+cos\,x)+\left(\frac{1}{sin\,x}\right)(sin\,x+cos\,x)=cot\,x+1$ $\frac{(sin\,x+cos\,x)^2}{cos\,x\,sin\,x}=cot\,x+1$ $\frac{sin\,x}{cos\,x}+1+1+\frac{cos\,x}{sin\,x}=cot\,x+1$ $\frac{(sin\,x+cos\,x)^2}{cos\,x\,sin\,x}=\frac{cos\,x}{sin\,x}+1$ $\frac{sin\,x}{cos\,x}+2+\frac{cos\,x}{sin\,x}=\frac{cos\,x}{sin\,x}+1$ $\frac{(sin\,x+cos\,x)^2}{cos\,x\,sin\,x}=\frac{(cos\,x+sin\,x)}{sin\,x}$ $\frac{sin\,x}{cos\,x}=-1$ $\frac{(sin\,x+cos\,x)}{cos\,x\,sin\,x}=\frac{1}{sin\,x}$ $x=-\frac{\pi}{4}$ $\frac{sin\,x+cos\,x}{cos\,x}=1$ $sin\,x+cos\,x=cos\,x$ $sin\,x=0$ $x=0$ ; invalid solution This first method seems an invalid solution because $sin\,x=0$ is not in the domain of the given original equation. Can someone please help me understand the difference ? Thank you.","['trigonometry', 'geometry']"
4335831,$f ≠ 0$ everywhere if $f : \mathbb{R}\to\mathbb{R}$ and $f = f'$ and $f(0) = 1$?,"Let $f:\mathbb R\rightarrow \mathbb R$ be a differentiable function, and suppose $f=f'$ and $f(0)=1$ . Then prove $f(x)\neq 0$ for all $x\in \mathbb R$ The way I solve this is kind of strange. I first suppose there is a closed interval $[0, a]$ on the real line. Since $f$ is differentiable, there exists a $x_0\in[0, a] $ such that $\frac{f(a)-f(0)}{a}=f'(x_0)=f(x_0)$ . Thus when $f(a)=1$ , $f(x_0)=0$ . Then I just let $f(a)=1$ and try to find some contradictions. Since $f(a)=f(0)=1$ , by Rolle thorem, there should exist a $x_1\in[0, a]$ such that $f'(x_1)=f(x_1)=0$ and this $f(x_1)=0$ is supposed to be the maximum or minimum on the interval $[0,a]$ . Then apply MVT again on the interval $[0, x_0]\implies\frac{f(x_0)-f(0)}{x_0}=\frac{-1}{x_0}=f'(x_2)=f'(x_2)\implies-1=x_0f(x_2)\implies f(x_2)<0$ for a point $x_2\in[0, x_0]$ . The existence of $x_2$ make sure that $f(x_1)=0$ is not the minimum on $[0, a]$ and since $0<f(0)=f(a)=1\implies f(x_1)$ is not the maximum on the $[0, a]$ . Also if there exists other points $\beta$ , for example, and $f(\beta)$ is the maximum of $[0,a]$ this implies that $f'(\beta)=0=f(\beta)$ the contradiction remains. Thus $f(a)\neq 1\implies f(x_0)\neq 0$ . Since $a$ is an arbitrary real number, this means $f$ has no zero point on $[0,\infty]$ . By the similar idea $f$ doesn't have zero point on $[-\infty, 0]. Is this a correct idea? And any shorter version of proof？ Thanks in advance!","['solution-verification', 'derivatives', 'ordinary-differential-equations', 'real-analysis']"
4335846,Example of Hausdorff Space Whose Quotient Space is Not Hausdorff,"I'm self-studying Croom's Principles of Topology .  Exercise 7.4.13 says: Give an example of a Hausdorff space which has a quotient space that is not Hausdorff. I came up with the example below, and I'd like your feedback on whether it is correct.  Thank you in advance. Let $A = \mathbb{R} \backslash \{0\}$ .  Consider $\mathbb{R}$ in the standard topology, which is of course Hausdorff, and $\mathbb{R}/A$ , where we identify the members of $A$ to a point.  We see $\mathbb{R}/A = \{ \{0\}, A \}$ .  The only subsets of $\mathbb{R}/A$ containing $\{0\}$ are $\mathbb{R}/A$ and $\{\{ 0 \}\}$ .  Since $\bigcup \{\{0\}\} = \{0\}$ is not open in $\mathbb{R}$ , the set $\{\{0\}\}$ is not open in $\mathbb{R}/A$ .  As a result, the only open subset of $\mathbb{R}/A$ containing $\{0\}$ is $\mathbb{R}/A$ .  Consequently, any open set in $\mathbb{R}/A$ containing $A$ belongs necessarily intersects every open set in $\mathbb{R}/A$ containing $\{0\}$ .  It follows that $\mathbb{R}/A$ is not Hausdorff.","['general-topology', 'solution-verification', 'quotient-spaces']"
4335883,Epsilon Delta definition of a Derivative,"The derivative at a specific point $c$ is represented as a limit by: $$ f'(c) = \lim_{x\to c} \frac{f(x) - f(c)}{x - c} $$ It's clear to me that the epsilon delta definition of a derivative at a point $c$ would be: $$
\forall \epsilon > 0 ~\exists \delta > 0 \forall x: \\
0 < |x-c| < \delta \rightarrow |\frac{f(x)-f(c)}{x-c} - L| < \epsilon
$$ What's unclear to me is how to formally represent the derivative as a function of $x$ , rather than only at point $c$ .  Basically, how would we represent this limit formally (the $\Delta x$ is the part tripping me up): $$ f'(x) = \lim_{\Delta x\to0} \frac{f(x + \Delta x) - f(x)}{\Delta x} $$","['epsilon-delta', 'real-analysis', 'calculus', 'limits', 'derivatives']"
4335896,Ways to find $\frac{1}{2\cdot4}+\frac{1\cdot3}{2\cdot4\cdot6}+\frac{1\cdot3\cdot5}{2\cdot4\cdot6\cdot8}+\cdots$,"$$\frac{1}{2\cdot4}+\frac{1\cdot3}{2\cdot4\cdot6}+\frac{1\cdot3\cdot5}{2\cdot4\cdot6\cdot8}+\frac{1\cdot3\cdot5\cdot7}{2\cdot4\cdot6\cdot8\cdot10}+\cdots$$ is equal to? My approach: We can see that the $n^{th}$ term is \begin{align}a_n&=\frac{1\cdot3\cdot5\cdot\space\dots\space\cdot(2n-3)\cdot(2n-1)}{2\cdot4\cdot6\cdot\space\dots\space\cdot(2n)\cdot(2n+2)}\\&=\frac{1\cdot3\cdot5\cdot\space\dots\space\cdot(2n-3)\cdot(2n-1)}{2\cdot4\cdot6\cdot\space\dots\space\cdot(2n)\cdot(2n+2)}\color{red}{[(2n+2)-(2n+1)}]\\&=\frac{1\cdot3\cdot5\cdot\space\dots\space\cdot(2n-3)\cdot(2n-1)}{2\cdot4\cdot6\cdot\space\dots\space\cdot(2n)}-\frac{1\cdot3\cdot5\cdot\space\dots\space\cdot(2n-3)\cdot(2n-1)\cdot(2n+1)}{2\cdot4\cdot6\cdot\space\dots\space\cdot(2n)\cdot(2n+2)}\\
\end{align} From here I just have a telescopic series to solve, which gave me $$\sum_{n=1}^{\infty}a_n=0.5$$ Another approach : note : $$\frac{(2n)!}{2^nn!}=(2n-1)!!$$ Which gives $$a_n=\frac{1}{2}\left(\frac{(2n)!\left(\frac{1}{4}\right)^n}{n!(n+1)!}\right)$$ So basically I need to compute $$\frac{1}{2}\sum_{n=1}^{\infty}\left(\frac{(2n)!\left(\frac{1}{4}\right)^n}{n!(n+1)!}\right) \tag{*}$$ I'm not able to determine the binomial expression of $(*)$ (if it exists) or else you can just provide me the value of the sum Any hints will be appreciated, and you can provide different approaches to the problem too","['binomial-theorem', 'factorial', 'sequences-and-series']"
4335921,Eliminate $\theta$ from $\sin3\theta=a\cos\theta$ and $\cos3\theta=b\sin\theta$,"Eliminate $\theta$ from $$\sin3\theta=a\cos\theta$$ $$\cos3\theta=b\sin\theta$$ I came to this point while trying to solve this problem: Eliminating $\theta$ from trigonometric system (Remark: Symbols differ from the original question) We can find $$b+a=\frac{2\cos2\theta}{\sin2\theta}$$ $$b-a=\frac{2\cos4\theta}{\sin2\theta}$$ Though at the moment I can't imagine how to arrange these to get a proper relation between $a$ and $b$ . We can replace $\sin$ and $\cos$ with $\tan$ and then proceed, but it is cumbersome. I suspect that there should be a clever way to end this, giving a beautiful answer. For reference, like in this question, a specific method gives the desired answer. $\leftarrow\small \text{(not strictly relevant)}$ So, what is the happy ending of this problem?","['algebra-precalculus', 'trigonometry']"
4335931,"Showing $\lim\limits_{n\to\infty}\int_{[-M,M]}f(x)\sin(nx)\,\mathrm dx=0$ for any mixed $M$","The problem statement is as follows: Given $f\in L^{1}(\mathbb{R})$ , show that $$\lim_{n\to\infty}\int_{[-M,M]}f(x)\sin(nx)\,\mathrm dx=0$$ for any fixed $M$ . My approach is as follows: We have $\lim\limits_{n\to\infty}\int_{[-M,M]}\sin(nx)\,\mathrm dx=0$ . If $f$ is a simple function taking values $a_{1},\cdots,a_{n}$ , $$\lim_{n\to\infty}\int_{[-M,M]}f(x)\sin(nx)\,\mathrm dx = \lim_{n\to\infty}\sum_{i=1}^{n}a_{i}\int_{[-M,M]}\sin(nx)\,\mathrm dx\\
= \sum_{i=1}^{n}a_{i}\lim_{n\to\infty}\int_{[-M,M]}\sin(nx)\,\mathrm dx = 0.$$ My problem is when $f$ is a non-negative function. If $f\geq 0$ , then there exists a sequence of simple functions $f_{k}\nearrow f$ , then $f_{k}\sin(nx)\nearrow f\sin(nx)$ [n is fixed], then by Monotone convergence theorem, we have $$\lim_{n\to\infty}\int_{[-M,M]}f(x)\sin(nx)\,\mathrm dx = \lim_{n\to\infty}\int_{[-M,M]}\lim_{k\to\infty}f_{k}(x)\sin(nx)\,\mathrm dx\\
= \lim_{n\to\infty}\lim_{k\to\infty}\int_{[-M,M]}f_{k}(x)\sin(nx)\,\mathrm dx.$$ My question is: Can swap the two limits $\lim\limits_{n\to\infty}\lim\limits_{k\to\infty}\int_{[-M,M]}f_{k}(x)\sin(nx)\,\mathrm dx$ ? If I can't, then it seems like my approach doesn't work. What is wrong with my approach or are there other approaches ? I've seen proofs u\sing Fourier Transforms, but I've not progressed that far yet, so I'm seeking proofs u\sing only basic integral theorems. Any help is appreciated.","['integration', 'limits', 'real-analysis']"
4335944,$1^2 - 2^2 + 3^2 - 4^2 + \dots + 1999^2$ using closed form formula for sum of squares of first $n$ natural numbers,"The question is simple, the solution, not so much Q.Find the sum of the given expression $1^2- 2^2 + 3^2 - 4^2 + \dots + 1999^2$ My idea is we know $1^2 + 2^2 + 3^2 + 4^2 + \dots + n^2 = \frac{n(n + 1)(2n +  1)}{6}$ So for $n=1999$ I get the sum as $2,66,46,67,000$ From this I need to subtract the squares of the even terms twice because subtracting once leaves with only the sum of the squares of the odd nos. I observed something : $2^2 + 4^2 + 6^2 + \dots + 1998^2 = (2 \cdot 1)^2 + (2 \cdot 2)^2 + \dots + (2 \cdot 999)^2$ Therefore to obtain the sum of the square of the even terms, I can take $4$ as common and use the aforementioned formula for $n=999$ and multiply it by $4$ . therefore sum of square of even terms = $1,33,13,34,000$ I need to subtract this sum twice to get the answer, because subtracting once simply leaves me with the sum of the squares of the odd numbers. The answer is now $1999000$ , which still doesn't match the answer key. Can someone explain where I am going wrong ?","['factoring', 'sequences-and-series']"
4335978,Calculate the following limit using the Central Limit Theorem,"A question about the CLT Using Central Limit Theorem to show that $$\lim_{n \to \infty} \frac{8^n}{27^n} \sum_{k=0}^n \binom{3n}{k}\frac{1}{2^k}=0$$ I have tried to define a sequence ${X_n}$ with $X_n$ ~ $Bi(3,p)$ ,and then put $S_n=X_1+ \ldots + X_n$ then clearly $S_n$ ~ $Bi(3n,p)$ but I have not been able to find the right $p$ , and I am not sure how to use CLT, any advice would be much appreciated.","['binomial-distribution', 'probability-distributions', 'probability-theory', 'central-limit-theorem']"
4336034,Why am I getting a different value for $\sin\left(2\tan^{-1}\frac{4}{3}\right)$ than my calculator?,"The expression: $$\sin\left(2\tan^{-1}\left(\frac{4}{3}\right)\right)$$ Way 1: If I punch the above expression in my calculator, I get $\frac{24}{25}$ . Way 2: $$\sin\left(2\tan^{-1}\left(\frac{4}{3}\right)\right)$$ $$\sin\left(\tan^{-1}\left(\frac{2\times\frac{4}{3}}{1-(\frac{4}{3})^{2}}\right)\right)$$ $$[\text{Using the formula $2\arctan(x)=\arctan\left(\frac{2x}{1-x^2}\right)$}]$$ $$\sin\left(\tan^{-1}\left(\frac{-24}{7}\right)\right)$$ $$-\sin\left(\tan^{-1}\left(\frac{24}{7}\right)\right)$$ $$-\sin\left(\sin^{-1}\left(\frac{24}{25}\right)\right)$$ $$-\frac{24}{25}$$ Why am I getting a different answer than that of my calculator?",['trigonometry']
4336085,Why am I getting the correct value for $\sin\left(2\tan^{-1}\frac{4}{3}\right)$ even though the usage of the formula is incorrect?,"The expression: $$\sin\left(2\tan^{-1}\left(\frac{4}{3}\right)\right)$$ Way 1: If I punch the above expression in my calculator, I get $\frac{24}{25}$ . Way 2: $$\sin\left(2\tan^{-1}\left(\frac{4}{3}\right)\right)$$ $$\sin\left(\sin^{-1}\left(\frac{2\times\frac{4}{3}}{1+(\frac{4}{3})^{2}}\right)\right)$$ $$[\text{Using $2\tan^{-1}x=\sin^{-1}\frac{2x}{1+x^2}$}]$$ $$\frac{2\times\frac{4}{3}}{1+(\frac{4}{3})^{2}}$$ $$\frac{24}{25}$$ My comments: One of the conditions of $2\tan^{-1}x=\sin^{-1}\frac{2x}{1+x^2}$ is that $|x|\leq1$ . In this case $|\frac{4}{3}|\nleq1$ , but still I'm getting the correct answer using the formula. Why is that? Related: Why am I getting a different value for $\sin\left(2\tan^{-1}\frac{4}{3}\right)$ than my calculator? Why does the equation with $2 \arctan(x)$ and other Inverse Trigonometric functions have weird conditions?",['trigonometry']
4336127,How to check if any number of points are coplanar?,"How to check if any number of tridimensional points are coplanar? I have found just how to check if 4 points are coplanar, but I need to know if a few tenths of points are coplanar.","['geometry', '3d']"
4336144,"$\sin^n (x)$ is it same as $(sin \: x)^n$ , n is a Natural number [closed]","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 2 years ago . Improve this question Hi one and all I am confusing that, $\sin^n(x)$ is it will works as $(\sin\:x )^n $ , could please clarify me in detail. And also, is it applicable to remaining all trigonometric functions.",['trigonometry']
4336156,Cauchy's mean value theorem conditions,"Cauchy's mean value theorem conditions
I have seen some statements of the Cauchy's mean value theorem that requires g′ is never 0 on (a,b), and some others that require f′ and g′ to never be simultaneously zero. Lemma: If functions f and g are both continuous on the closed interval [a,b], and differentiable on the open interval (a,b) and g′ is never zero on (a,b), then there exists some c∈(a,b), s.t
f′(c)/g′(c)=f(b)−f(a)/(g(b)−g(a)). Why is it needed for g′ to never be 0 on all (a,b) , don't we just need g′ not to be zero on that value c ?","['calculus', 'derivatives']"
4336171,Obtaining the integral kernel of an operator,"Let's say I have a (bounded for now) linear operator $A$ on $L^2(\mathbb R^n)$ say, and I would like to find its kernel (which I'll suppose exists), i.e. a function $K(x, y)$ such that $Af(x) = \int K(x, y)f(y)dy$ . Is it possible to find an expression for $K(x, y)$ if I know how $A$ acts on every function? For instance, I know $\langle Af, g \rangle$ and some of my initial thoughts is that if I can let $f = \delta_y$ and $g = \delta_x$ then $A\delta_y(z) = \int K(z, u)\delta_y(u)du = K(z, y)$ and therefore $$\langle Af, g \rangle = \int A\delta_y(z)\overline{\delta_x(z)}dz = \int K(z, y) \delta_x(z)dz = K(x, y).$$ But I'm not sure if this method is permitted. I can't think of any counterexamples but I know the delta does not belong to $L^2$ and can't be approximated by $L^2$ functions. What's the usual technique to get the kernel of an integral operator?","['functional-analysis', 'real-analysis']"
4336204,Folland: Why is the product measure well-defined?,"Consider the following fragment from Folland's ""Real analysis"" on p64: I don't understand why $\pi(E)$ is well-defined, i.e. assume that $$E = \bigcup_i A_i \times B_i= \bigcup_j C_j \times D_j$$ where the unions are disjoint. Then why is it true that $$\sum_i \mu(A_i)\nu(B_i) = \sum_j \mu(C_j)\nu(D_j)?$$ Folland talks about a common refinement, but I don't see how that works.","['integration', 'measure-theory', 'lebesgue-integral', 'real-analysis', 'product-measure']"
4336208,For this limit $\lim_{x \to 5}{\sqrt{x-1}}=2$ find $ δ$,"Question on delta epsilon definition of a limit For the limit $\lim_{x \to 5}{\sqrt{x-1}}=2$ , find a $δ > 0$ that works for $ε = 1$ . I'm getting $δ=1$ , But I'm not 100% confirm that my ans is correct. Please help in this question.","['limits', 'calculus', 'epsilon-delta']"
4336214,Steady state distribution of a generalised gauss map,"Question Consider the map $g:[0,1] \to [0,1]$ given by $$
g(x) = \begin{cases}
1 - \Big\lceil \frac{1}{x} \Big\rceil + \frac{1}{x} & x \leq 1/2
\\
\Big\lceil \frac{1}{1-x} \Big\rceil - \frac{1}{1-x} & x > 1/2
\end{cases}
$$ This function is plotted below I would like to calculate the steady state distribution (aka invariant measure, aka leading eigenfunction) $m(x)$ obtained for generic initial values $x_0$ by repeated application of the map $$
\frac{1}{n}\sum_{k=1}^n f \circ g^k(x_0) \to \int f(x) m(x) dx.
$$ The key properties of interest to me are The density at the extremal points of the interval $\lim_{x \to 0} m(x) = \lim_{x \to 1} m(x)$ The average $\int_0^1 \log ( \min(x,1-x) ) m(x) dx = 2 \int_0^{1/2} \log (x) m(x) dx$ Context This is related to the well known Gauss map $h:[0,1] \to [0,1]$ $$
h(x) = \frac{1}{x} - \Big\lfloor \frac{1}{x} \Big\rfloor
$$ in the following way $$
g(x) = \begin{cases}
1 - h(x) & x \leq 1/2
\\
h(1-x) & x > 1/2
\end{cases}
$$ (expect at the measure zero set of points $1/\min(x,1-x) \in \mathbb{N}$ ). For $h(x)$ the steady state distribution is known, and given by $$
m_h(x) = \frac{1}{\log 2} \frac{1}{1+x}
$$ In this case the equivalent properties of interest are $\lim_{x\to 0}m_h(x) = 1 / \log 2$ $\int_0^1 dx \log (x) m_h(x) dx = \pi^2/(12 \log 2)$ (Levy's constant) Operationally we can understand $g(x)$ as being a generalisation of the Gauss map which treat cases of $x$ very close to $0$ , and $x$ very close to $1$ symmetrically. I would like to find the equivalent properties for the map $g(x)$ . My progress I detail my attempt at generalising the calculation of $m_h(x)$ to the case of interest. This yields an implicit equation ( $**$ ) for the cumulative distribution function, however I have not been able to solve this thus far. Moreover, I do not know if this approach is the correct one, so any pointers will be appreciated. Explicitly we are interested in solutions to the equation $$
m(x) = \int_0^1 d y \delta(x - g(y))m(y)
\tag{$*$}
$$ we write $g(x)$ in the following form $$
g(x) = \begin{cases}
1 - n + \frac{1}{x} & \qquad \text{for} \qquad \frac{1}{n} \leq x < \frac{1}{n-1}, \quad n = 3 , 4 ,5 , 6 \ldots
\\
n - \frac{1}{1-x}  & \qquad \text{for} \qquad \frac{n-1}{n} \leq x < \frac{n-2}{n-1}, \quad n = 3 , 4 ,5 , 6 \ldots
\end{cases}
$$ Substituting this in to ( $*$ ) we obtain $$
m(x) = \sum_{n=3}^\infty \left[ \frac{m\left(\frac{1}{n-1+x}\right)}{(n-1+x)^2} + \frac{m\left(1-\frac{1}{n-x}\right)}{(n-x)^2} \right]
$$ We integrate this to obtain $$
c(x) = \sum_{n=3}^\infty \left[ c\left( \frac{1}{n-1} \right) - c\left( \frac{1}{n-1+x} \right) + c\left( 1- \frac{1}{n} \right) - c\left( 1- \frac{1}{n-x} \right) \right]
$$ where $c(x) = \int_0^x m(y) dy$ is the cumulative distribution function. The expected symmetry of the distribution $m(x) = m(1-x)$ allows for some simplification, specifically we use $c(x) = 1 - c(1-x)$ to obtain $$
c(x) = \sum_{n=3}^\infty \left[ c\left( \frac{1}{n-1} \right) - c\left( \frac{1}{n-1+x} \right) - c\left( \frac{1}{n} \right) + c\left( \frac{1}{n-x} \right) \right]
$$ The constant terms telescope and we obtain $$
\begin{equation}
c(x) = 1 - c(1-x) = \frac{1}{2} + \sum_{n=3}^\infty \left[ c\left( \frac{1}{n-x} \right) - c\left( \frac{1}{n-1+x} \right) \right]
\tag{$**$}
\end{equation}
$$ where we have used that $c(1/2)=1/2$ . Unfortunately I am unable to make progress from here. Some futzing around and re-writing yields a different form , but no progress.","['measure-theory', 'probability-distributions', 'ergodic-theory', 'dynamical-systems']"
4336237,Dualizing object in the duality between commutative rings and affine schemes,"Many dualities between geometry and algebra arise via a dualizing object . Roughly, if $\mathcal C$ is a category of spaces and $\mathcal D$ a category of ""algebras"", one often finds a dualizing object $R$ which lives in both categories $\mathcal C$ and $\mathcal D$ such that the constructions $C\mapsto \hom_\mathcal C(C,R)$ and $D\mapsto \hom_\mathcal D(D,R)$ constitute an equivalence or at least adjunction between $\mathcal C$ and $\mathcal D$ . For instance, Pontrjagin duality ( $R=\mathbb R/\mathbb Z$ ), Stone duality ( $R=\mathbb Z/(2)$ ), Gelfand duality ( $R = \mathbb C$ ), and the fundamental theorem of Galois theory ( $R=\bar k$ , for $k$ a field) arise in this way. Question: What is the dualizing object in the duality between affine schemes and commutative rings? (Second question: What is the dualizing object in the fundamental theorem of covering spaces , which roughly states that the category of covering spaces over a space $X$ is equivalent to the category of all sets equipped with an action of the fundamental groupoid on that set? On the one hand, it is not a duality, so maybe it doesn't have a dualizing object in the strict sense, but since this statement is very similar to the fundamental theorem of Galois theory, maybe there's something similar.)","['category-theory', 'ring-theory', 'algebraic-geometry', 'duality-theorems', 'schemes']"
4336243,Proving an interesting identity,"Let $Q(z)=(z-\alpha_1)\cdots(z-\alpha_n)$ be a polynomial of degree $>1$ with distinct roots outside the real line. We have $$\sum_{j=1}^n \frac{1}{Q'(\alpha_j)}=0.$$ I know an interesting but indirect proof using the continuity of the Fourier transform, but I want to know whether there is a proof relying on more rudimentary techniques.",['complex-numbers']
4336270,Inequality for a sorted set,"Given $X = \{x_1, x_2, \cdots, x_n\}, n \geq2$ such that $x_1 \lt x_2 \lt \cdots \lt x_n$ , could you prove: $
\frac{1}{n^2}\sum_{1 \leq i \lt j \leq n}(x_i - x_j)^2 \leq \frac{(x_1 - x_n)^2}{4}
$ I can easily prove for $n=2, 3, 4, 5$ , but I'm failed to do it for an arbitrary $n$ .","['elementary-set-theory', 'inequality', 'variance']"
4336274,Why doesn't the definition of derivative generalize smoothly from single variable to multivariable calculus like the definition of continuity?,"As from my previous questions, one may follow that I am trying to understand the differences between real and complex derivative. My question is: For a function $f:\mathbb{R}\to\mathbb{R}$ , the derivative at a point $c\in\mathbb{R}$ is defined as $$f'(c)=\lim_{h\rightarrow 0}\frac{f(c+h)-f(c)}{h}$$ For a complex function $f:\mathbb{C}\longrightarrow\mathbb{C}$ the complex derivative at a point $z$ is defined as $$f'(z)=\lim_{h\rightarrow 0}\frac{f(z+h)-f(z)}{h}.$$ But for a function $f:\mathbb{R}^2\longrightarrow \mathbb{R}^2$ , why isn't the derivative at a point $(x,y)$ defined as $$f'(x,y)=\lim_{(h,k)\rightarrow (0,0)}\frac{f((x,y)+(h,k))-f(x,y)}{(h,k)}?$$ Also, in this question, they say, Differences between the complex derivative and the multivariable derivative. , that it is because, it is not possible to define division in $\mathbb{R}^2$ . But can we not define the inverse of $(h,k)$ as $(\frac{1}{h}, \frac{1}{k})$ , for $h\neq 0, k\neq 0$ ? And define $(a,b){(c,d)}^{-1}$ this way?","['multivariable-calculus', 'calculus', 'derivatives', 'analysis']"
4336279,Understanding Quantum Measurement in infinite dimensional systems,"I have been relearning quantum mechanics recently since I realized I got stuck in some misunderstanding of fundamentals when trying to solve certain problems. My question is actually simply the following: What is the relationship between the measurement postulate, stated in terms of projection valued measures, and the eigenvectors of observables in the case of an infinite dimensional Hilbert space? Now here is some context to maybe see where my confusion comes from: Most QM textbooks advertise that observables are postulated to be self-adjoint operators because the spectral theorem assures you that their eigenvectors form a basis, which is connected to the measurement postulate via the idea that when you expand the state of the system in the eigenvectors of an observable, the coefficients of this expansion are the probability amplitudes for the measurement of said observable. I was stuck with the idea that somehow you can always use the eigenvectors of some observable to decompose any state. However, now I know that this is only true for finite dimensional Hilbert spaces. On infinite dimensional Hilbert spaces, the eigenvectors of self-adjoint operators need not even be in the Hilbert space (for example, the momentum operator $\hat{p}:D(\hat{p})\rightarrow L^2(\mathbb{R})$ , $\hat{p}=-i\partial_x$ , which has a self-adjoint extention, has eigenvectors $\psi_p(x) = e^{ipx}$ , which aren't square integrable functions. Even so, while they do not really form a basis in the Hilbert space, they still play an important role (related to Fourier transforms). Now, measurement, formulated in a more rigorous manner, has to do with the actual spectral theorem, namely that for any self-adjoint operator there exists a projection-valued measure $P_A$ such that $A$ can be represented in terms of the Lebesgue-Stieltjes integral as $$A = \int_\mathbb{R} \lambda dP_A(\lambda)$$ Now from my understanding so far, PVMs on finite dimensional spaces and the projectors described in QM texbooks as say $P_\psi = |\psi><\psi|$ in Dirac notation (which again is a notation enabled by working with bases given by the eigenvectors of observables) are actually two sides of the same coin so to say. But I cannot seem to wrap my head around (or find any resource on the internet) about the infinite dimensional case. Here would be some of the questions I'm pondering regarding this: What is the connection between PVM and eigenvectors in the case of the momentum operator for example? Are eigenvectors of operators actually relevant in the infinite dimensional case in general? (or the momentum/position are some very special cases?) If the answer to 2 is affirmative, what happens to operators whose eigenvectors have discontinuities with infinite jumps? (one such an example I can think of is: $\hat{T}:D(T)\rightarrow \{f \in L^2(\left[0,2\pi\right])|f(0)=f(2\pi)\}$ , $\hat{T} = i(cos^2 (\theta) - 1/2)\frac{d}{d \theta} + i\, sin(\theta) cos(\theta)$ , for which there should be a self-adjoint extension, or rather some $D(T)$ for which it is self-adjoint - if I'm wrong scream at me, please. The primitive $\int \frac{sin(\theta)cos(\theta)}{cos^2(\theta)-1/2} d\theta$ diverges to $+\infty$ at the points $\theta_0\in\{\pi/4,\, 3\pi/4,\, 5\pi/4,\, 7\pi/4\}$ ). How would one treat the measurement of the operator described in 3.?","['quantum-mechanics', 'spectral-theory', 'functional-analysis', 'mathematical-physics']"
4336290,Graph of a density over one Riemannian manifold itself a Riemannian manifold? And follow up questions,"Intuitively it seemed natural to me that the graph of a density $p(x)$ on $\mathbb{R}^n$ is a manifold. After a little bit of searching I came across this , which I believe(?) confirms my intuition. Call this manifold $\mathcal{M}$ . The follow up questions are: Since $p(x)$ is measurable, does it naturally induce a measure on $\mathcal{M}$ through push-forward measure? If so, is this measure uniform on $\mathcal{M}$ ? Conversely, for the uniform/standard measure on a given Riemannian manifold $\mathcal{M}$ (with necessary assumptions on $\mathcal{M}$ such as smoothness, compactness, etc.), is there any mapping to $\mathbb{R}^n$ where this uniform density also is transformed to a density on $\mathbb{R}^n$ ? Can we replace $\mathbb{R}^n$ with an arbitrary manifold, and then would the results still follow? I apologize if these questions are obvious, and they're answered somewhere that I am not aware of. I would appreciate any pointers if that's the case. Also sorry if my use of terms are sloppy. I am doing my best, mathematics not being my main area of research these days.","['measure-theory', 'smooth-manifolds', 'real-analysis', 'manifolds', 'differential-geometry']"
4336337,Can I use the distributive law to write $\sum_{i=0}^{\infty}2^i$ as $1+ 2\sum_{i=0}^{\infty}2^i$?,"I've come across this problem: $$
S=\sum_{i=0}^{\infty}2^i \\
S= 1+ 2\sum_{i=0}^{\infty}2^i \\
S=1+2S \\
S=-1
$$ I know that you cannot rearrange this series because it is not absolutely convergent , but that seems to only happen from step 3 to 4 and maybe from step 1 to 2, but i am not sure about this. What might be happening from step 1 to 2 is that the distributive law doesn't hold for infinite series like this, but I'm not sure. I found this about it, that leads me to believe otherwise though. So the stuff that happens from step 3 to 4 is definitely wrong, but i want to know, whether the steps before it are allowed or not.",['sequences-and-series']
4336387,"Differential equation $y'(t) = e^{-t \, y(t)}$ [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I was confronted to this equation $y^{'}(t) = e^{-t \, y(t)}$ ; I don't feel it's possible to explicitly solve it, but I don't see how to explain that it's not possible...
Thanks for any suggestion.",['ordinary-differential-equations']
4336404,Sections of sheaves of modules: reference,"I try to understand Section 17.4 in the Stack Project talking about sections of sheaves of modules and their stalks. Especially, I would like to study Lemma 17.4.2 and 17.4.3 there, but the proofs are omitted there. Does anyone know a good reference like Hartshorne, Shafaverich, etc?","['algebraic-geometry', 'schemes', 'reference-request']"
4336420,Dodecahedron and golden ratio algebra,"We can see that the volume of a dodecahedron of size $2\varphi$ , where $\varphi=\frac{\sqrt{5}-1}{2}$ , can be found in two ways. The first one uses the pentagonal pyramids with the faces as basis and height the distance of the face from the center of the dodecahedron and gives the result ( here $\Phi=\frac{1}{\varphi}=\varphi+1$ ): $$
V=4\sqrt{5(2-\varphi)}\sqrt{3-\frac{4\varphi^2}{4-\Phi^2}}
$$ The other way start from a cube of side $a=2$ and add to every face a "" roof"" with penatagonal flaps, as in the figure, and gives the result $$V=8+4\Phi$$ Now I want to prove that $$
4\sqrt{5(2-\varphi)}\sqrt{3-\frac{4\varphi^2}{4-\Phi^2}}=8+4\Phi
$$ only with algebra, using the properties of Golden Ratio: $\varphi=1-\Phi=\frac{1}{\Phi}$","['golden-ratio', 'algebra-precalculus', 'polyhedra']"
4336447,Computing Wronskian when functions are not given [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question If $y_1,y_2$ are solutions of $y''+\sin(t)y'+e^ty=0$ , and $y_1(0)=2,y_1'(0)=-3, y_2(0)=3, y_2'(0)=4$ , compute $W[y_1,y_2](-5\pi)$ . I don't know how to calculate this wronskian without finding $y_1,y_2$ explicitly. Any idea?","['wronskian', 'ordinary-differential-equations']"
4336456,"Question about a proof: If $AB=BA$ then $A$ and $B$ share a common eigenvector, from Strang's Linear Algebra","In Strang's Linear algebra, he proves the following (which has been asked and answered on SE, but my question is about a particular part of his proof): Let $A$ and $B$ be complex $n\times n$ matricies.  Prove that if $AB=BA$ , then $A$ and $B$ share a common eigenvector. His proof is as follows: Let $\lambda$ be a eigenvalue of $A$ .  Starting from $Ax=\lambda x$ , we have $ABx=BAx=B\lambda x=\lambda Bx$ .  So, $x$ and $Bx$ are both eigenvectors of $A$ sharing the same $\lambda$ (or else $Bx=0$ ).  If we assume the eigenvalues of $A$ are distinct, so the eigenspaces are one dimensional, then $Bx$ must be a multiple of $x$ .  In other words, $x$ is an eigenvector of $B$ as well as $A$ . My question is, and I am probably overthinking something quite elementary, but when he says ""if we assume the eigenvalues of $A$ are distinct..."", I agree then with the rest of the argument.... but why can he do that?","['linear-algebra', 'eigenvalues-eigenvectors']"
4336488,Is the expectation of the supremum of a random walk with negative drift finite?,"Let $(\xi_n)_{n\in\mathbb N}$ be iid random variables with negative mean, let $S_n=\sum_{k=1}^n\xi_k$ , and let $M=\sup_{n\ge 0}S_n$ . Is it true that if the $\xi_n$ are nice (say, finite variance), then $M$ has finite expectation? This question implies the result in the case where $\xi_n$ are $\{\pm1\}$ -valued random variables (apply the Monotone convergence theorem to $M_n:=\sup_{0\le k\le n}S_k$ ), but I'm interested in the general case. I wasn't able to find anything that looked relevant (other than the question I linked above) when Googling.","['probability-theory', 'random-walk']"
4336517,integration by substitution from two variables to one,In lecture notes I have seen the substitution $t = x\cdot y$ applied for the following integral: $$\int_{0}^{\infty}e^{-\frac{y^2}{2}}\int_{0}^{\infty}e^{-\frac{(xy)^2}{2}}\cdot y \: dxdy =  \left( \int_{0}^{\infty}e^{-\frac{t^2}{2}}dt \right) ^2$$ Why can two variables be substituted by only one and what are the intermediate steps?,"['integration', 'multivariable-calculus', 'calculus']"
4336560,"Proving Schur-Zassenhaus Theorem, with added assumption that $G/H$ is cyclic","Schur-Zassenhaus Theorem: If there exists normal Hall-subgroup $H$ of finite group $G$ , then there exists complement $K$ of $H$ in $G$ . So if $\exists$ H $\unlhd$ G s.t. |H| is coprime to [G:H] then $\exists K \le G$ s.t. $ G = HK \cong H \rtimes K $ All proofs of this theorem use group cohomology or Burnside's arguments in conjunction with commutator subgroups of the Sylow $p$ -subgroups. My question is, if we assume $G/H$ is cyclic can we somehow take out some of the heavier machinery in the proof? Like somehow showing that $H$ is abelian would be real nice. Then the induction is pretty ok without the use of cohomology. I'm not sure what knowing $K$ is abelian implies.","['group-theory', 'abstract-algebra', 'group-cohomology', 'cyclic-groups']"
4336581,Dimension of image of varieties,"Set $k$ be an algebraically closed field. Let $X$ and $Y$ be two $k$ -varieties.(one can assume they are projective) A question I have met many times is that: if $f$ is a morphism from $X$ to $Y$ , then can we know $\operatorname{dim} f(X)\leq \operatorname{dim} X$ ? I know the conclusion is right if $f$ is open. But most of the times we don't have this property. If we know $X, Y$ are projective, then $f$ is proper, hence closed, but I can't see the dimension of $f(X)$ . Could you give some helpful properties?(references are also welcome) Thanks!",['algebraic-geometry']
4336591,To find the minimum number of $3$-toppings pizza so that it meets the demand of my friend!,In a pizza shop they are offering $3$ -toppings pizza with $10$ choices of toppings. A friend has decided that two of the three toppings on the pizza must be what they want but I don't know which two he has fixed. Let $k$ be the minimum number of $3$ -toppings pizza we have to order so that we can guarantee the friend gets what he wants. We have to prove that $k=17$ . I tried doing it with pigeonhole principle. Suppose $2$ of the toppings of the pizza is fixed then we can choose the $3rd$ one from remaining $8$ in $8$ ways. We can make those $8$ pizzas as one pigeonhole. I am not getting how to partition the remaining into pigeonholes.,"['combinatorial-designs', 'pigeonhole-principle', 'combinatorics', 'discrete-mathematics', 'recreational-mathematics']"
4336595,Least prime divisor of $n!-1$ forms divergent series.,"If we have a sequence $\left\{\alpha_{n}\right\}_{n=3}^{\infty}$ such that $\alpha_{n}$ is the least prime divisors of $n !-1$ To Show: $$\sum_{n=3}^{\infty} \frac{1}{\alpha_{n}} \rightarrow \infty$$ I need help in completing my proof : $\Rightarrow$ Claim : The least prime divisor of $n!-1$ is greater than $n$ . If on the contrary we have prime $p$ s.t. $p \leq n$ then clearly $p \mid n !$ and if we assume $p$ also divides $n !-1$ then $p$ divides $n !-(n !-1)=1$ Hence contradiction. So the least prime divisor of $n !-1$ is greater than $n$ . Now how to prove that $\sum_{n=3}^{\infty} \frac{1}{\alpha_{n}} \rightarrow \infty$ once I have shown $\alpha_n \in \{n+1, \ldots, n !-1 \}$ .","['number-theory', 'elementary-number-theory', 'sequences-and-series']"
4336613,There exists n such that number of primes between $(n+1)^{2021}$ and $n^{2021}$ is greater than 1122021140.,"To show there exists a natural number $n$ such that there are at least 1122021140 primes between $(n+1)^{2021}$ and $n^{2021}$ . Here is my attempt : $\Rightarrow$ We define $\pi(x)=\#\{p \leq x \mid p$ is prime $\}$ . Then by prime number theoremn $\pi(x) \sim \frac{x}{\log x}$ . So I need to show $\pi(y)-\pi(x) > 1122021140$ for $y=(n+1)^{2021}$ and $x=n^{2021}$ For large $x$ we have (saw it here ) $$
\pi(y)-\pi(x) \sim \frac{y-x}{\log x} \tag1
$$ $$
\begin{aligned}
&=\frac{(n+1)^{2021}-n^{2021}}{\log (n)^{2021}} \\
&=\frac{(n+1)^{2021}-n^{2021}}{2021 \log n}
\end{aligned}
$$ I have two questions : How to prove equation $(1)$ i.e., $\pi(y)-\pi(x) \sim \frac{y-x}{\log x}$ for large $x$ . How to show there exists a $n$ such that $\frac{(n+1)^{2021}-n^{2021}}{2021 \log n} > 1122021140$ .","['number-theory', 'elementary-number-theory', 'prime-numbers']"
4336622,Does the finite sequence belong to $\ell^p$?,"For $1\leqq p<\infty,$ $\ell^p$ is defined by $\ell^p=\{ \{x_n\}_{n=1}^\infty \subset K \mid \sum_{n=1}^\infty |x_n|^p<\infty\}.$ Does the arbitrary sequence $\{a_n\}_{n=1}^N \subset K$ belong to $\ell^p$ ? Of course, $\sum_{n=1}^N |a_n|^p<\infty$ but I wonder whether $\{a_n \}_{n=1}^N$ belongs to $\ell^p$ because this is not the form $\{ \cdot \}_{n=1}^\infty$ but the form $\{ \cdot \}_{n=1}^N.$ I think that for given $\{a_n \}_{n=1}^N$ , if I define $a_n=0$ for $n\geqq N+1$ , I can check $\{a_n \}_{n=1}^\infty \in \ell^p$ , but can I say $\{a_n \}_{n=1}^N$ itself is in $\ell^p$ ? Or, in the first place, isn't $\{a_n\}_{n=1}^N$ called ""sequence"" ?","['lp-spaces', 'functional-analysis', 'sequences-and-series']"
4336641,Is this trigonometric inequality widely known?,"The conjunction of this answer and this one would seem to imply that for $\theta_1,\theta_2,\theta_3 \in(-\pi/2,+\pi/2),$ $$
|\theta_1+\theta_2+\theta_3| < \frac \pi 2 \text{ if and only if } \tan\theta_1\tan\theta_2 + \tan\theta_1\tan\theta_3 + \tan\theta_2\tan\theta_3<1. \tag 1
$$ And a bit of number crunching and graphing via software seems to suggest that maybe $$
\sup\big\{ \arctan( \tan\theta_1\tan\theta_2 + \tan\theta_1\tan\theta_3 + \tan\theta_2\tan\theta_3 ) : \theta_1+\theta_2+\theta_3=s \big\} \\[8pt] = \frac\pi4(1-\cos s) \text{ if $|s|<\pi/2$ ??} \tag 2
$$ or some function similar to that. And if $|s|>\pi/2,$ put $\inf$ there instead of $\sup.$ The equality in $(2)$ seems far-fetched, and concerning $(1)$ I wonder if there's an efficient way of showing it, as opposed to juxtaposing the two answers I linked. If the hour were not late, I would figure this all out myself, except for the answer to this question: Is this stuff known?","['algebra-precalculus', 'trigonometry', 'inequality']"
4336647,I'm trying to understand the following derivation,"This is taken from Hastie, Tibshirani and Friedman's Elements of Statistical Learning. They describe the squared loss error and to minimize the expectation of this error, which they call Expected (squared) prediction error, $EPE$ . They first define the squared error loss as $L(Y, f(X))=(Y-f(X))^{2}$ . The criterion for choosing $f$ becomes, $$EPE(f)=E(Y-f(X))^{2}$$ $$EPE(f)=\int(y-f(x))^{2}P(dx, dy)$$ The authors then write the following: the condition on $X$ to get, $$EPE(f)=E_{X}E_{Y|X}([Y-f(X)]^{2}|X)$$ I think I might be missing something basic here, but how did the authors arrive at this by conditioning on X? Then they go on to write: it suffices to minimize $EPE$ pointwise as, $$f(x) = argmin_{c}E_{Y|X}([Y-c]^{2}|X=x)$$ How do the authors arrive at this conclusion i.e. that it suffices to minimize EPE pointwise? This is not very intuitive to me. In the end, the authors say: the solution for the equation above is, $$f(x)=E(Y|X=x)$$ How do they arrive at this solution? Could someone describe the intermediate math to achieve this as a solution?",['statistics']
4336651,Dense rational points of an elliptic curve,$\newcommand\Q{\mathbb Q}$ Could anyone please provide me the following two examples of elliptic curve defined over $\Q$ (if they exist) It has infinite $\Q$ -rational points and these points are (in the Euclidean metric of the affine plane) dense on the curve It has infinite $\Q$ -rational points and these points are (in the Euclidean metric of the affine plane) not dense on the curve I guess proving that the points are infinite is just a proof of having a $>0$ rank (which may or may not be obvious). I would also be interested in an argument (or a reference) why one concludes the points are/aren't dense.,"['arithmetic-geometry', 'algebraic-geometry', 'elliptic-curves']"
4336667,Poincaré’s inequality for a bounded open set in Brezis' book,"I am trying to understand the Corollary 9.19 (Poincaré’s inequality) in Functional Analysis, Sobolev Spaces and Partial Differential Equations , by Haim Brezis. Suppose that $1 \le p < \infty$ and $\Omega$ is a bounded open set. Then there exists a constant $C$ (depending on $\Omega$ and $p$ ) such that $\left\Vert u \right\Vert_{L^p(\Omega)} \le C \left\Vert \nabla u \right\Vert_{L^p(\Omega)}$ for all $u \in W^{1,p}_0(\Omega)$ . So far, I know that the zero extension $\bar u$ of $u\in W^{1,p}_0(\Omega)$ is an element in $W^{1,p}(\mathbb{R}^N)$ (which is a consequence of the preceding Proposition 9.18). Case 1: $1\le p<N$ With the help of Theorem 9.9 (Sobolev, Gagliardo, Nirenberg), $\left\Vert \bar u \right\Vert_{L^{p^*}(\mathbb{R}^N)} \le C \left\Vert \nabla \bar u \right\Vert_{L^p(\mathbb{R}^N)}$ , where $\frac{1}{p*} = \frac{1}{p} - \frac{1}{N}$ , I can show the Poincaré’s inequality due to compact support and $p<p^*$ . Case 2: $p>N$ Theorem 9.12 (Morrey) asserts $|\bar u(x) - \bar u(y)|\le C |x-y|^\alpha \left\Vert \nabla \bar u \right\Vert_{L^p(\mathbb{R}^N)} $ a.e. Take the continuous representative and still denote $\bar u$ . By translation, I may take $y = 0$ and $\bar u(y) = 0$ . Then taking p-norm (or $\infty$ -norm) yields the result. My question is how to conclude the Poincaré’s inequality when $p=N$ ?","['sobolev-spaces', 'functional-analysis']"
4336702,If $f$ is an infinitely differentiable function then does this hold?,"If $f$ is differentiable infinite number of times, then there exist $x\in (0,1)$ such that $$\frac{f(1)-f(x)}{x}=f'(x)$$ . I have tried to use Lagranges Mean Value Theorem . It lands me with $f(1)-f'(x)=(1-x)f'(x+\theta(1-x))$ where $\theta\in(0,1)$ . Then I thought that I could differentiate both sides again and again to get some form of $\theta^{n}$ but to no avail. Even the differential equation $xf'(x)+f(x)=f(1)$ admits a solution but I don't think that it will help me. Can anyone tell me how to proceed with this?","['derivatives', 'real-analysis']"
4336714,Analysis of a calculation of expected number of collisions in hashing,"For a formal problem statement, I quote from the text Introduction to Algorithms by Cormen et. al Suppose we use a hash function $h$ to hash $n$ distinct keys into an array $T$ of length $m$ . Assuming simple uniform hashing, what is the expected number of collisions? More precisely, what is the expected cardinality of $\{\{k, l\} : k\neq l \text{ and } h(k)= h(l)\}$ ? Quite intuitively I proceeded as follows: Let $X$ be the random variable indicating the number of collisions. Let us define an indicator random variable $X_{ij}$ which indicates whether the $i$ th element ( $e_i$ ) already in the table collides with the $j$ th element ( $e_j$ ) when the $j$ th element is to be inserted. $Pr\{h(e_i)=h(e_j)\}=\frac{1}{m}$ . So $E[X_{ij}]=\frac{1}{m}$ . So based on that we have: $$X= \sum_{i=1}^n \sum_{j=i+1}^n X_{ij}$$ Taking expectation on both sides we have: $$E[X]= E\left[\sum_{i=1}^n \sum_{j=i+1}^n X_{ij}\right]$$ Using linearity of expectation: $$E[X]= \sum_{i=1}^n \sum_{j=i+1}^n E[X_{ij}]$$ But, $E[X_{ij}]=\frac{1}{m}$ , which is already found above. So, $$E[X]= \sum_{i=1}^n \sum_{j=i+1}^n \frac{1}{m}$$ $$=\frac{n(n-1)}{2m}$$ Below is how my peer approached: Let $X$ be the random variable indicating the number of collisions. Let $X_i$ be the indicator random variable indicating collision during the $i$ th insertion of the element, $i=1,2,3,..,n$ So, $$X=\sum_{i=1}^n X_i$$ taking expectation on both sides, we have: $$E[X]=E\left[\sum_{i=1}^n X_i\right]$$ Using linearity of expectation: $$E[X]=\sum_{i=1}^n E[X_i]$$ Now let us find $Pr\{X_i=1\}$ then by the property of indicator random variable we shall have $E[X_i]=Pr\{X_i=1\}$ . Probability that there is collision during the first insertion = $0$ [First element is inserted without any collision.] Probability that there is collision during the second insertion= $\frac{1}{m}$ [Assuming open addressing, $1$ slot is already occupied.] Probability that there is collision during the third insertion= $\frac{2}{m}$ [Assuming open addressing, $2$ slots are already occupied.] . . . Probability that there is collision during the $i$ th insertion= $\frac{i-1}{m}$ [Assuming open addressing, $i-1$ slots are already occupied.] So, $$E[X]=\sum_{i=1}^n \frac{i-1}{m}=\frac{n(n-1)}{2m}$$ Though the final answer obtained by my peer is same as that of mine, but I guess there are quite a lot of issues with the approach. First it assumes open addressing, while my approach is a generalized one. Secondly, as per the formal problem statement given in the text, in the worst case if all elements hash to the same slot then we shall have $X=\binom{n}{2}$ . So the spectrum of $X$ is , $X=0,1,2,..,\binom{n}{2}$ But the spectrum of $X$ as per my peer's method is $X=0,1,2..,n$ . Is my peer's method actually correct?","['hash-function', 'probability-theory', 'probability']"
4336741,Section of a line bundle after tensoring with ideal sheaf,"Let $ L$ be a line bundle on a smooth complex projective curve $X$ with genus $ g \geq 2$ such that $  L$ is not base point free. Let $I_Z$ be the ideal sheaf of its base locus. Questions : In this situation can we say that $(I)$ $ L \otimes I_Z$ is a line bundle? And $(II)$ $ h^0( L)= h^0( L \otimes I_Z)$ ? What I tried so far: If we rewrite $L(-Z)=L \otimes \mathcal O(-Z)= L \otimes I_Z$ , then infact one has $h^0(L) \leq h^0(L(-Z))$ (why?). Now if we denote the canonical map induced by the line bundle by: $\phi : X \to \mathbb P((H^0(X,L))^* =\mathbb P^r$ . Then one has the following isomorphisms : $(i)$ $\phi^{*}(\mathcal O_{\mathbb P^r}(1)) \cong L(-Z)$ and $(ii)$$H^0(\mathbb P^r, \mathcal O_{\mathbb P^r}(1)) \cong H^0(X,L)$ From $(i)$ it follows that $L(-Z)$ is a line bundle( Perhaps this also because it's the tensor product of two line bundles given by $L$ and $\mathcal O(-Z)$ ) One can use $(i)$ , $(ii)$ and projection formula to show $ h^0( L)= h^0( L \otimes I_Z)$ . Please correct me if this is wrong.","['algebraic-geometry', 'solution-verification']"
4336749,Does the Zygmund class on a closed interval include all everywhere differentiable functions?,"Let $\mathcal{C}$ be the class of continuous functions in the Zygmund class on the closed interval $[0, 1]$ . In this question, a continuous function $f$ is in the Zygmund class if there is a constant $c>0$ such that— $$|f(x) + f(y) - 2f((x+y)/2)| \le c\epsilon, $$ for every $\epsilon>0$ , whenever $x$ and $y$ are points in $f$ 's domain such that $|x-y|\le\epsilon$ . Then, does $\mathcal{C}$ include every function that is differentiable everywhere on $[0, 1]$ ? (I know the answer is yes for functions that are continuously differentiable everywhere on $[0, 1]$ .) If not, what is an example of an everywhere differentiable function on a closed interval that is not in the Zygmund class? To be clear, this question is not homework or a self-study assignment.","['derivatives', 'real-analysis']"
4336757,Find the area of ​region ABC.,"For reference: In figure $T$ and $K$ are points of tangency, $MT = a$ and $KN = b$ ; calculate area of ​​region $ABC$ . (Answer: $2\sqrt{ab}(\sqrt a+\sqrt b)^2$ ) My progress: $$S_{ABC} = p \cdot r = \frac{r \cdot (AB+BC+AC)}{2}\\
AC +2R = AB+BC\\
S_{ABC} = AG \cdot GC \qquad \text{(property)} \\
S_{ABC} = (AC+R)R \qquad \text{(property)} \\
OTBQ:~\text{square} \implies TK = R\sqrt2 \\
\ldots ?$$ I'm not able to use segments a and b in the resolution","['euclidean-geometry', 'geometry', 'plane-geometry']"
4336759,Is this a good argument on what makes the Lebesgue integral more general than the Riemann integral?,"Ever since I learned about the Lebesgue integral I have tried to understand what exactly makes it more general than the Riemann integral. By this I mean what part of its definition makes it more general. Below I will only talk about positive functions defined on some interval $[a, b]\subset \mathbb{R}$ . Here is what I've come up with: when defining the Riemann integral, we use step functions, which are linear combinations of characteristic functions of intervals ; when defining the Lebesgue integral, we use measurable simple functions, which are linear combinations of characteristic functions of measurable sets. I think that this is the point of the theory that makes the Lebesgue integral more general. In particular, I think that this is what makes the function $$f(x)=\begin{cases} 1 & x\in [0, 1]\cap \mathbb{Q} \\ 0 & x\in [0, 1]\cap (\mathbb{R}\setminus \mathbb{Q}) \end{cases}$$ Lebesgue integrable and not Riemann integrable. Furthermore, I think that the reason why for the Riemann integral we use characteristic functions of intervals is because these are the only Jordan measurable subsets of $\mathbb{R}$ that are ""interesting"" (I can't really think of a Jordan measurable subset of $\mathbb{R}$ that is not an interval except some weird looking examples that end up having Jordan measure zero) and for the Riemann integral we wish to have a good connection with the Jordan measure. So, I think that we may also say that the reason why the Lebesgue integral is more general than the Riemann integrable is that the Lebesgue measure is more general than the Jordan measure. Are the above arguments correct? I have really thought about this a lot and I really want to make sure that I know what part of the definition of the Lebesgue integral grants it its generality.","['measure-theory', 'real-analysis']"
4336780,Showing that the Fibonacci's $\binom{n}{k}_F$ is an integer by following the Benjamin-Plot proof,"In this paper of A. T. Benjamin and S. S. Plot https://www.fq.math.ca/Papers1/46_47-1/Benjamin_11-08.pdf there's the proof that the following coefficient is an integer: $$\binom{n}{k}_F = \frac{F_nF_{n-1}\dots F_{n-k+1}}{F_kF_{k-1}\dots F_1}$$ where $F_n = F_{n-1} + F_{n-2}$ , for $n \ge 2$ , and $F_0=0$ , $F_1=1$ . It is clear that if $f_n$ is the $n$ -th Fibonacci's number, then $f_n = F_{n+1}$ . A well known combinatorial interpretation of these numbers is that $F_n = f_{n-1}$ counts the number of tilings of a $(n-1) \times 1$ board, with squares $1 \times 1$ and dominoes $2 \times 1$ . Hence $F_nF_{n-1}\dots F_{n-k+1}$ enumerates the number of simultaneous tilings, in squares and dominoes, of $(n-1) \times 1$ , $(n-2)\times 1$ , $\dots$ , $(n-k)\times 1$ boards. To show that $\binom{n}{k}_F$ is an integer, they ""cut away"" a disjointed covering in squares and dominoes of $(k-1) \times 1$ , $(k-2)\times 1$ , $\dots$ , $1 \times 1$ boards (counted by $F_kF_{k-1}\dots F_1$ ) from those described above representing the numerator. The procedure is not that hard : first of all we are looking for a $(k-1)\times 1$ covering, starting by the $(n-1)\times 1$ board. If a domino doesn't cover the cells $k-1$ and $k$ , we can break the board at $k-1$ finding what we wanted. It can happen in $f_{k-1}f_{n-k} = F_kF_{n-k+1}$ ways, by considering the number of coverings of the two independent boards we got breaking the $(n-1)\times 1$ at $k-1$ . Naming $T_i$ a tailing of $(n-i)\times 1$ . If a domino is laid on the $k-1$ and $k$ cells, that can happen in $f_{k-2}f_{n-k-1} = F_{k-1}F_{n-k}$ ways, by throwing away that domino and taking into account the new two boards $(k-2)\times 1$ and $(n-k-1)\times 1$ disjointed, then we eliminate the first $k$ cells obtaining a $(n-k-1) \times 1$ tiling, that is $T_{k+1}$ . We follow this procedure on the next $(n-2)\times 1$ , $(n-3)\times 1$ , $\dots$ , since we find a tiling $T_{x_1}$ that is breakable at $k-1$ , which exists as $T_{n-k+1}$ has length $k-1$ . So $1 \le x_1 < n$ . Iterating this method (looking for $(k-2)\times 1$ by starting from $T_{x_1+1}$ , etc.) we find a disjointed covering $T_1, \dots, T_k$ of $(k-1)\times 1, \dots, 1\times1$ . Then they write: ""Following this procedure, we have, for $1 \le x_1 < x_2 < ··· < x_{k-1} \le n$ , the number of tilings $T_1, T_2, \dots , T_k$ that lead to finding a tiling of length $k − i$ at the beginning of tiling $T_{x_i}$ is $f_{k-2}^{x_1-1}f_{k-1}f_{n-x_1 - (k-1)}f_{k-3}^{x_2-x_1-1}f_{k-2}f_{n-x_2 - (k-2)}\dots f_{0}^{x_{k-1}-x_{k-2}-1}f_{1}f_{n-x_{k-1} - 1}$ "" I haven't fully understood this result yet. The process to obtain $(k-1)\times1$ sees the contribute of $f_{k-2}$ any time we can't break the board at $k-1$ , which happens for $x_1-1$ times, so we have $f_{k-2}^{x_1-1}$ . I assume that $f_{k-1}$ derives from the case in which we can break it and stop the iteration for $(k-1)\times 1$ (?), then $f_{n-x_1 - (k-1)}$ how does it come up? Any explanation would be appreciated, thank you. EDIT Maybe I figured it out and it was easier that I thought: when the process stops at $x_1$ we are considering a $n-x_1$ row, so $f_{k-1}f_{n-x_1 - (k-1)}$ is the number of independent tilings of the that board broken at $k-1$ !","['fibonacci-numbers', 'combinatorics', 'combinatorial-proofs', 'discrete-mathematics']"
4336787,Volume of a body bounded by a surface.,"I want to find a volume of a body, bounded by: $$(x^2+y^2+z^2)^2 = az(x^2+y^2)$$ for some $a > 0$ . As I understood I am supposed to say that for аor a fixed value of z, it looks like a circle, which means that we just need to take the integral of the resulting function. But I'm having a little trouble getting the radius $(x ^ 2 + y ^ 2)$ of a circle at fixed z and reducing that to an integral. Maybe there are simpler ways to get the volume of a given shape?","['integration', 'volume', 'real-analysis', 'multivariable-calculus', 'multiple-integral']"
4336807,Injective map of schemes that is not a monomorphism,"I know that it is not true that for a map $f \colon X \rightarrow Y$ of schemes,
injectivity (on underlying sets) of $f$ gives a monomorphism in the category
of schemes. Stronger assumptions are required, see e.g. Tag 01L6 in the Stacks project. But I have not come up with a counterxample yet.
So can someone provide me with some concrete counterexample,
preferrably some well-known class of such morphisms?","['general-topology', 'algebraic-geometry', 'monomorphisms', 'category-theory']"
4336820,Geometry Question: proving that NM and MC are perpendicular,"I have been stuck on this problem for quite a while: Let ABCD be a rectangle and BD its diagonal. Take CE $\perp$ BD and M the midpoint of DE. Then, if N is the midpoint of AB, prove that $\angle{NMC}=90^{\circ}$ I have tried pretty much everything, creating other rectangles inside the rectangle but I can't get how to use the fact that M is the midpoint of DE. I also tried going about this by naming every single angle and taking the relationships between them, but no luck so far. Here is the shape I made in Geogebra. I can solve it using analytic geometry, but I am looking for a purely (euclidean) geometrical solution. Thanks in advance!","['euclidean-geometry', 'geometry']"
4336821,"Is $f(x,y)$ differentiable on some neighborhood of $(x_0,y_0) ?$","A function $f:O\to \mathbb{R}$ , $O$ is an open subset in $\mathbb{R}^2$ ,
all of its first partial derivatives $f_{x}$ , $f_{y}$ are defined on $O$ . If
we assume both $f_{x}$ and $f_{y}$ are differentiable at $(x_0,y_0)\in O$ , is $f(x,y)$ differentiable on some neighborhood of $(x_0,y_0) ?$ From above conditions,it’s apparent that $f(x,y)$ is continuous on a neighborhood of $(x_0,y_0)$ and differentiable at $(x_0,y_0)$ . I don’t think  there exists a neighborhood of $(x_0,y_0)$ such that $f(x,y)$ is differentiable on whole of it. My question is how to find the example which fails to differentiate on any neighborhood of $(x_0,y_0)$ but satisfies above conditions?","['multivariable-calculus', 'examples-counterexamples', 'real-analysis']"
4336833,Are adjoint representation matrices the generators of the Adjoint representation?,"My question is, are the (Lie Algebra) adjoint representation matrices in general the generators of the (Lie Group) Adjoint representation (i.e. treating the Adjoint representation as a matrix Lie Group)? My justification is that $ad(\xi) = T_e (Ad(\xi))$ . Therefore we treat the Adjoint representation as its own matrix Lie Group (living in the Lie Algebra vector space, i.e. it acts on Lie Algebra elements like $\xi$ in the above statement). Therefore, since it is a matrix Lie Group, we can in general define a Lie Algebra representation on it by its generating matrices (with the Lie Bracket represented by the usual matrix commutator) The reason I ask this is because I was confused why the adjoint representation of su(2) (with the basis given below) could seemingly represent the Lie Bracket either by matrix-vector multiplication (with a vector in the Lie Algebra), or by their usual matrix commutator. (e.g. $[T_1,T_2]=T_3$ could be represented by $T_1*(0 1 0)= (0 0 1)$ or represented by $T_1*T_2-T_2*T_1 = T_3$ ) My solution is therefore that adjoint is defined so that the matrix-vector multiplication represents the Lie Bracket but, since (by different reasoning) the basis of the adjoint are also 'raw' elements of the Lie Algebra of the Adjoint matrix Lie Group, they can also be represented by generators of the Adjoint matrix Lie Group (and therefore represent the Lie Bracket by the usual commutator). $$ T_1 = \begin{bmatrix} 0&0&0\\0&0&-1\\0&1&0\end{bmatrix}, T_2 = \begin{bmatrix} 0&0&1\\0&0&0\\-1&0&0\end{bmatrix}, T_3 = \begin{bmatrix} 0&-1&0\\1&0&0\\0&0&0\end{bmatrix}$$","['lie-algebras', 'smooth-manifolds', 'matrices', 'manifolds', 'lie-groups']"
4336900,Reference Request on a Necessary and Sufficient Condition for Isothermality,"I am getting acquainted with the fundamentals of differential geometry for the sake of a problem
I have been thinking about. In Green, G. M. ""Some Geometric Characterization of Isothermal Nets on a Curved Surface."" Transactions of the American Mathematical Society, Vol. 18, No. 4, (1917) pp. 480-488, the author makes the following preamble: 'If a surface $S$ , whose equations are $$
\begin{equation}
x=x(u,v), \hspace{2mm} y=y(u,v), \hspace{2mm}z=z(u,v),
\end{equation}
$$ be referred to an orthogonal net of parameter curves, its first fundamental form is $$
\begin{equation}
ds^2=Edu^2+Gdv^2,
\end{equation}
$$ where $E$ and $G$ are functions of $u$ and $v$ .
This orthogonal net is called isothermal , if by a proper transformation $\bar{u}=\phi(u),\bar{v}=\psi(v)$ of the parameters the first fundamental form may be reduced to $$\begin{equation}
ds^2=\bar{\lambda}(\bar{u},\bar{v})(d\bar{u}^2+d\bar{v}^2).
\end{equation}$$ A necessary and sufficient condition that such a reduction may be effected upon the form as first written is, that the equation $$
\begin{equation}
\frac{\partial^2}{\partial u \partial v} log \Big(\frac{E}{G} \Big)=0
\end{equation}
$$ be satisfied identically.' I would like to know the name of this result (if any); where it originated; whether it has variants pertaining to the classification of surfaces where the middle term
of the first fundamental form does not vanish. Textbooks that would answer my questions are welcome. Thanks for your attention!","['surfaces', 'reference-request', 'differential-geometry']"
4336990,Values of the Liouville function,"Let $\Omega: \mathbb{N} \to \mathbb{N}\cup\{0\}$ be the function which counts how many prime factors a number has, with multiplicity. For example, $\Omega(380) = 4$ , $\Omega(108)= 5$ . More generally, for $p$ prime, $\Omega(p) = 1, \Omega(p^k) = k$ and it is clear that $\Omega(mn) = \Omega(m) + \Omega(n)$ . Furthermore, $\Omega(n) = 0 \iff n =1$ . The Liouville lambda function is defined to be $\lambda:\mathbb{N} \to \{\pm1\}$ , $$\lambda(n) = (-1)^{\Omega(n)}$$ It is apparent that $\lambda(mn) = \lambda(m) \lambda(n)$ . Consider the sequence given by $\lambda(n)$ . Do there exist arbitrarily long stretches in this sequence  of either $+1$ or $-1$ ? That is to say, $\forall N \in \mathbb{N}, \exists n: \lambda(n)=\lambda(n+1)=\cdots=\lambda(n+N)$ ? This seems like quite a natural question to ask about this function, but I was unable to find an answer after searching. I verified this for $N\le4$ by hand, and I would presume that this is true. But this seems like a difficult conjecture to prove, and I know very little number theory in the first place.","['number-theory', 'liouville-function', 'prime-factorization', 'prime-numbers']"
4336993,Proving $\frac{1}{n} \sum_{k=1}^{n}X_{k}\to 0$ a.s.,"$\{X_{n}\}$ is a sequence of independent random variables, $EX_n=0$ , and $\sum_{n=1}^{\infty}n^{-(r+1)}E(|X_n|^{2r})<\infty$ . Proving $\frac{1}{n} \sum_{k=1}^{n}X_{k}\to 0$ a.s. and $r>1$ I think Borel-Cantelli lemma should be a very useful way to prove this kind of problem, but I don’t know how it should be applied to this one. Some approaches are welcome!","['borel-cantelli-lemmas', 'probability-theory', 'almost-everywhere']"
4337007,"If $A$ is an infinite set, prove that $A$ has a proper infinite subset","I considered taking this contradictory approach: Let $A_1\in A$ . Assume that the proper subset $A\smallsetminus\{A_1\}$ is finite such that there exists a bijection $f:A\smallsetminus\{A_1\} \to C_k$ with $C_k=\{1,2,3,\ldots,k\}$ with $k \in N$ . What I wanted to do next is somehow prove that since $A\smallsetminus\{A_1\}$ is finite, then $A$ is finite, contradicting the given and proving my assumption false. However I have no idea how to proceed from here, or even if what I've done so far is plausible. Any help please?",['elementary-set-theory']
4337015,Reference: Generalization of indicator functions (and POVMs) to free probability?,"Question: Is the following a correct notion/generalization of ""indicator function"" and ""positive operator valued measure"" (POVM) to free probability ? Either way, do you know of any references discussing the connection? Let $\mathscr{A}$ be a ( unital ) *-algebra (with unit $1_{\mathscr{A}}$ ) (see Wikipedia link and/or definition below) and let $(X, \Sigma_X)$ be a measurable space. Then a function $\mathbb{I}: \Sigma_X \to \mathscr{A}$ is a "" generalized indicator function "" (or ""generalized observable"") if it satisfies the following axioms: $\mathbb{I}$ is finitely additive (and/or countably additive if/when $\mathscr{A}$ is also a Banach algebra i.e. so that infinite summation can be defined, or assume for simplicity that $X$ is finite). $\mathbb{I}(X) = 1_{\mathscr{A}}$ (""probabilities sum to $1$ ""). for all ""events"" $E \in \Sigma_X$ , $\mathbb{I}(E) = \alpha \alpha^*$ for some $\alpha \in \mathscr{A}$ (""non-negativity"" and self-adjointness). Presumably one could also add the following restrictions to make these more closely resemble the ""classical indicator function"" $\Sigma_X \to \mathbb{R}^{X}$ : Recall that an ""atomic event"" $E \in \Sigma_X$ is such that $E \not= \emptyset$ and $E' \subseteq E$ implies that $E' = E$ or $E' = \emptyset$ . Let $\mathcal{E}_X \subset \Sigma_X$ denote the set of all atomic events. The generalized indicator function $\mathbb{I}$ will be called "" faithful "" if the set $\{ \mathbb{I}(E) : E \in \mathcal{E}_X \}$ is linearly independent in $\mathscr{A}$ . (In particular $\mathbb{I}$ will be injective, but this condition is strictly stronger.) ( Should ""linearly independent"" be replaced with ""orthogonal""? ) The generalized indicator function $\mathbb{I}$ will be called "" full "" if the span of the set $\{ \mathbb{I}(E) : E \in \mathcal{E}_X \}$ equals all of $\mathscr{A}$ . For example, my understanding is that projection-valued measures always have to be ""faithful"", but positive operator valued measures (POVMs) do not. Intuitively, if the dimension of $\mathscr{A}$ is ""too small"" and the cardinality of $X$ is ""too large"", then it should be impossible for any ""generalized indicator function"" $\mathbb{I}: \Sigma_X \to \mathscr{A}$ to be faithful. Similarly, if the dimension of $\mathscr{A}$ is ""too large"" and the cardinality of $X$ is ""too small"", then it should be impossible for any ""generalized indicator function"" $\mathbb{I}: \Sigma_X \to \mathscr{A}$ to be full. Motivation: To better understand free probability theory , and in particular quantum information theory, I want to double-check whether I correctly understand the connection between the Kolmogorov/measure-theoretic formulation of ""classical probability theory"" and the free probability /""commutative algebra of RVs"" formulation. E.g. I'm pretty sure I understand the connection between the ""Kolmogorovian""/measure-theoretic formulation of ""classical probability theory"" and the formulation in terms of indicator functions and expectation axioms/linear functionals, as described e.g. in Peter Whittle's book . I'm not entirely sure, but NCatLab implies this is on the right track. Extra Definitions: Modified from Wikipedia : A *-ring $A$ is a unital ring (with unit $1_A$ ) with a map $*: A \to A$ such that: $(a_1 + a_2)^* = a_1^* + a_2^*$ , $(a_1 a_2)^* = a_2^* a_1^*$ , $1_A^* = 1_A$ , $(a^*)^* = a$ for all $a, a_1, a_2 \in A$ . A $*$ -algebra $\mathscr{A}$ is a (unital) $*$ -ring with involution $*$ that is an associative algebra over a commutative $*$ -ring $R$ with involution $^\dagger$ , such that $(r \alpha)^* = r^\dagger \alpha^*$ for all $r \in R, \alpha \in \mathscr{A}$ . For simplicity we can assume that $R$ is the real numbers with $\dagger$ given by the identity function, or that $R$ is the complex numbers with $\dagger$ given by complex conjugation. But technically the definition is more general. To go from ""generalized indicator functions"" or ""generalized observables"" to generating actual probability distributions, we of course need to consider linear functionals, i.e. ""generalized expectations"" or ""generalized traces"". What follows is basically copy-pasted-modified from Terry Tao's blog . A ""generalized expectation"" or ""generalized trace"" is a linear functional $\tau: \mathscr{A} \to \mathbb{C}$ satisfying the following properties: for all $\alpha \in \mathscr{A}$ , $\tau(\alpha^*) = \overline{\tau(\alpha)}$ , i.e. the complex conjugate of $\tau(\alpha$ ). $\tau(1_{\mathscr{A}}) = 1$ . For all $\alpha \in \mathscr{A}$ , $\tau(\alpha \alpha^*) \ge 0$ . Note that the first property implies that for all self-adjoint elements of $\mathscr{A}$ , i.e. $\beta^* = \beta$ , that $\tau(\beta) \in \mathbb{R}$ . ( Terry Tao calls this property "" $*$ -linear"" , although that risks confusion with conjugate-linear/""antilinear"" . ) Basically one important way this perspective differs from ""Kolmogorovian probability theory"" is that, instead of considering/looking at individual probability distributions one at a time, if we are given a single ""generalized indicator function/POVM"" we will usually then consider the family of probability distributions generated by applying all possible ""generalized expectations/traces"" to that ""generalized indicator function/POVM"". I think that we need the ""generalized indicator function/POVM"" to be ""full"" (in the sense defined above) in order for this family of probability distributions to be ""complete"" or ""as large as possible"". For example, consider $(X, \Sigma_X) = (\{1,2\}, \mathcal{P}(\{1,2\}))$ and $\mathscr{A} = \mathbb{R}^2$ with entrywise multiplication (and unit the all-ones vector). Then if we define $\mathbb{I}(\{1\}) = \mathbb{I}(\{2\}) = (\frac{1}{2}, \frac{1}{2})$ , then this appears to satisfy all of the axioms above, but the only probability distribution we can generate (regardless of what $\tau$ is) is the ""fair coin"". Similarly we can only generate one (biased) ""coin"" distribution by setting, for any given $p \in (0,1)$ , $\mathbb{I}(\{1\}) = (p,p)$ and $\mathbb{I}(\{2\}) = (1-p, 1-p)$ , because by linearity $\mathbb{P}(\{2\}) = \tau(1-p, 1-p) = (1-p)/p \cdot \tau(p,p) = (1-p)/p \cdot \mathbb{P}(\{1\})$ , which forces us to the same values of $\mathbb{P}(\{1\})$ and $\mathbb{P}(\{2\})$ for any $\tau$ because of the constraint $\mathbb{P}(\{1\}) + \mathbb{P}(\{2\}) = 1$ . On the other hand, for the ""standard"" indicator function $\mathcal{P}(\{1,2\}) \to \mathbb{R}^2$ with $\mathbb{I}(\{1\}) = (1,0)$ and $\mathbb{I}(\{2\}) = (0,1)$ the ""full""-ness (and ""faithful""-ness) condition is satisfied, and of course applying all possible $\tau$ then leads to all possible probability distributions on $\{1, 2\}$ . So having this extra flexibility in choosing the ""generalized indicator function"" seems kind of weird when applied to ""classical probability theory"", but it seems to be a standard for POVMs, which is a source of confusion for me.","['c-star-algebras', 'quantum-information', 'probability-theory', 'reference-request']"
4337041,Why can't $\int_1^3 \frac{4}{(2x-3)^4} dx $ be evaluated by calculators or WolframAlpha?,"This is the integral $$\int_1^3 \frac{4}{(2x-3)^4} dx $$ Solving by u-substitution, it works fine. $$ \text{let}~~ u = 2x-3 $$ $$\int_1^34 \cdot  ( 2x-3 )^{-4}dx $$ $$\ x = \frac{u+3}{2} $$ $$\ \frac{ dx }{ du }=\frac{1 }{2} $$ $$\ dx = \frac{ 1 }{ 2 }du $$ EDIT: $$\ u(3) = 2(3)- 3$$ $$\ u(3) = 3 $$ $$\ u(1) = 2(1)- 3$$ $$\ u(1) = -1 $$ $$\int_{-1}^3 4 \cdot  u^-4 \cdot \frac{ 1 }{ 2 }du $$ $$\int_{-1}^3 2\cdot u^{-4}du $$ $$\frac{ 2u^{-3}}{-3 } \Bigg \vert_{-1}^{3} \ $$ $$ = \frac{ 2}{-3(2x-3)^3 } \Bigg \vert_{-1}^{3} \ $$ $$ = \frac{  2}{ -3(2(3)-3)^3 } -  \frac{  2}{ -3(2(-1)-3)^3} $$ $$ = \frac{-2}{81} - \frac{2}{375} $$ $$ = \frac{-84}{125} $$ But plugging this integral into a TI-84 CE Plus throws an error ""Cannot divide by 0"" Also trying online calculators, Wolframalpha and Freemathhelp , the problem is unable to be solved. Why can't this problem be solved on these calculators?","['integration', 'calculus', 'definite-integrals']"
4337092,Solve the heat equation using a transform method,"I need to solve $k\frac{\partial^2U}{\partial x^2}=\frac{\partial U}{\partial t}$ subject to \begin{equation}
U(0,t)=1, t>0 \\ U(x,0)=e^{-x},x>0
\end{equation} I tried using the Laplace transform with respect to $t$ since the function is defined for $t>0$ and we have the initial condition $U(x,0)=e^{-x}, x>0$ . My problem is that I'm left with the complicated function: \begin{equation}
c_{1}\cos\left(\sqrt{\frac{s}{k}}x\right) +c_{2}\sin\left(\sqrt{\frac{s}{k}}x\right)+ \frac{k}{s-k}e^{-x}
\end{equation} which I don't know how to invert. I was wondering if there's any other way to approach this problem (can you apply some other transform? Is there anything I'm not seeing?). I need some guidance urgently since I'm preparing for a final exam. Thank you so much for your help!","['fourier-analysis', 'ordinary-differential-equations', 'partial-differential-equations']"
4337099,"Find Definite Integral $\int_{3}^{4} e^{2x} \,dx $","I am having problems with this question and I do not know if I am doing it correctly. Can someone assist me? $$\int_{3}^{4} e^{2x} \,dx $$ $$t = 2x $$ $$dt = 2dx $$ $$\frac {dt}{2} = dx $$ $$\int \frac{1}{2} \cdot e^t + C$$ $$F(b)-F(a) =$$ $$\frac{1}{2}\cdot e^8 - \frac{1}{2}\cdot e^6$$","['integration', 'derivatives']"
4337114,"function which is not Lipschitz but in $W^{1,\infty}$ on a domain with a slit","Let $\Omega =\left(-1,1 \right)^2\setminus\left( \left[0,1 \right)\times \{ 0\}\right)$ and $u(x_1,x_2)=\hat u(x_1)$ for $x_1,x_2 >0$ and $u(x_1,x_2)=0$ else. I am looking for a function $\hat u$ such that $u \in W^{1,\infty}(\Omega)$ and $u$ is not Lipschitz continous on $\Omega$ . I have failed to come up with a suitable function $\hat u$ . Any hints what I may try ? Would appreciate any help.","['sobolev-spaces', 'functional-analysis', 'real-analysis']"
4337129,Proving single solution of initial value problem is increasing,"Given the initial value problem $$
y'(x)=y(x)-\sin{y(x)}, y(0)=1
$$ I need to prove that there is a solution defined on $\mathbb{R}$ and that the solution, $u(x)$ , is an increasing function where $\lim_{x\to -\infty}{u(x)=0}$ The first part is quite easy, let $f(x, y) = y - \sin{y}$ $$f_y(x, y)=1-\cos{y}\Longrightarrow \left|f_y(x,y)\right| \le 2$$ hence $f(x, y)$ is Lipschitz continuous on $y$ for every $(x, y)\in (-\infty, \infty)\times (-\infty, \infty)$ therefore the initial value problem has a single solution on $\mathbb{R}$ I know that the solution, $u(x)$ has the form $$u(x)=y_0+\int_{x_0}^x{f(t, u(t))dt}=1+\int_0^x{(u(t)-\sin{u(t))}dt}$$ and also $$u(x)=\lim_{n\to\infty}u_n(x)$$ where $$u_n(x)=y_0+\int_{x_0}^x{f(t, u_{n-1}(t))}dt=1+\int_0^x{(u_{n-1}-\sin{(u_{n-1})})}dt,\space\space\space u_0(x)=y_0=1$$ But I'm wasn't able to find a way to prove the solution is increasing and has the desired limit. EDIT:
I got a hint to look at $y\equiv 0$ EDIT2:
Let assume that the solution, $u(x)$ is decreasing at $(x_1, u(x_1))$ because $u(x)$ is continuous, there must be $(x_2, u(x_2)$ where $u'(x_2)=0=u(x_2)-\sin{u(x_2)}\Longrightarrow u(x_2)=0$ now if I look at the initial value problem $$y'(x)=y(x)-\sin{y(x)}, y(x_2)=0$$ I can prove that it has a single solution in $\mathbb{R}$ like I already did, and $u(x)$ is my solution, but $u_1(x)\equiv 0$ is also a solution to this problem, contradiction, hence $u'(x)>0$ for $x\in\mathbb{R}$ , i.e $u(x)$ is increasing. But I still don't know how I can show the limit at $-\infty$","['initial-value-problems', 'ordinary-differential-equations']"
4337198,Prove that the set of skew-symmetric matrices is closed under addition,"I am trying to prove that W is a subspace of V with: $V = M_{n\times n}$ , $W = \{A \in M_{n\times n} : A = -A^T\}$ I am fairly sure $W$ is closed under addition, but am not sure how to prove it for all $M_{n\times n}$ I can prove it for $M_{2\times 2}$ : Let $A = \begin{bmatrix}0 & a\\-a & 0\end{bmatrix} \in W$ Let $B = \begin{bmatrix}0 & b\\-b & 0\end{bmatrix} \in W$ $A + B = \begin{bmatrix}0 & a\\-a & 0\end{bmatrix} + \begin{bmatrix}0 & b\\-b & 0\end{bmatrix}$ $ = \begin{bmatrix}0 & a + b\\-a-b & 0\end{bmatrix} = \begin{bmatrix}0 & a + b\\-(a+b) & 0\end{bmatrix} \in W$ How could I generalise this to all $A, B \in M_{n\times n}$","['matrices', 'skew-symmetric-matrices', 'vector-spaces']"
4337264,Question about gauge transformation,"I'm studying something fundamental about gauge theory and I find that many materials state(without proof) that: for a principal bundle $P$ with correspondent connection $\omega$ and correspondent Lie group $G$ . Take $\phi \in Aut(P)$ , we can view $\phi$ a map $P \to G$ and write the pullback connection along $\phi$ by $\phi^*\omega=h^{-1}\omega h+h^{-1}dh$ .(1) But I'm quite confused about this equation, if I take a vector $v \in T_p P$ , what's the $\phi^*\omega(v)$ exactly means? Is it $h^{-1}\omega|_p(v) h+h^{-1}dh(v)$ or $h^{-1}\omega|_{hp}h_*(v)+h^{-1}dh(v)$ ? Also, as far as I know, we always define pullback as $\phi^*\omega(v)=\omega(\phi_*v)$ , but it seems not coincide to the equation (1) above. Meanwhile in the page 153 of taubes book differential geometry he say that
for a trivial principal bundle $M\times G$ with a connection $A$ on it and a map $h:M \to G$ , we can make an automorphism of $M\times G$ as $\phi:(x,g)\to (x,h(x)g)$ then we will obtains a pullback $\phi^*A=g^{-1}dg+h^{-1}dh$ , also not coincide with the (1). And I see from a material that view $\mathbb{R}^4$ as $\mathbb{H}$ and view $SU(2)$ as $Im(\mathbb{H})$ we can pullback the connection $Im(\frac{xd \bar{x}}{1+|x|^2})$ by scaling $\lambda$ and we can get the result as $Im(\frac{xd\bar{x}}{\lambda^2+|x|^2})$ ,
but I think scaling is not a map from $P$ to $G$ right? How can I apply the pullback operation? Could anyone help
me to clarify the meaning of these notation?Thanks!","['principal-bundles', 'gauge-theory', 'differential-geometry']"
4337266,Geolocation from multiple timestamps,"I'm trying to geolocate where in the world a particular piece of data was sent from (via the Internet). I have multiple computers located in various cities around the world, and they all receive the data at different timestamps. I can potentially take thousands of measurements throughout the day. If I make the huge assumption that latency is proportional to the great circle distance between two points, is there a way to combined my timestamp measurements to come up with a statistical best guess for where a particular piece of data is sent from? For what it's worth, I'm trying to figure out location at the granularity of what country the data is coming from, not down to a residential area or something like that.","['statistics', 'geometry']"
4337313,Tower of Hanoi sequence via eigendecomposition,"The following sequence comes from the Tower of Hanoi . $$ 1, 3, 7, 15, \dots $$ Find the $64$ -th term using eigendecomposition. By general pattern, I know that the answer is obviously $2^{64} -1$ . However, I am asked to solve using eigendecomposition. $$ T(n) = 2^n - 1 $$ Let $F_k$ be the terms of the sequence. $$F_{k+1} = 2 F_k+1$$ In matrix form.. $$\begin{bmatrix} F_{k+1} \\F_k\end{bmatrix} = \begin{bmatrix} 2&1 \\1&0\end{bmatrix}\begin{bmatrix} F_{k} \\1\end{bmatrix}$$ Please help me continue.","['eigenvalues-eigenvectors', 'recurrence-relations', 'matrices', 'linear-algebra', 'sequences-and-series']"
4337320,Is there a name for this property of functions on groups?,"Let $G$ be a group and $F:G^n \to G$ with the following property: If $x_1,…,x_n,h \in G$ , then $F(hx_1,…,hx_n)=hF(x_1,…,x_n)$ . Is there a name for this type of function property? It is something I’ve been investigating lately. For instance, if $G$ is a vector space and $F$ outputs the average vector, then $F$ has this property.","['invariance', 'group-theory', 'definition', 'average', 'terminology']"
4337321,Universal properties of mapping spaces in functional analysis,"Recently I've been trying to learn more about functional analysis, and have been wondering about what universal characterisations are there of mapping spaces for topological vector spaces, (semi)normed spaces, and Banach spaces. In particular, I've seen the following characterisation invoked a few times: The operator norm $||{-}||$ on $\mathcal{Ban}(X,Y)$ with $X$ normed and $Y$ Banach is the unique one such that $\mathcal{Ban}(X,Y)$ is also a Banach space. For any sequence $\{T_{n}\}_{n\in\mathbf{N}}$ of operators, if $\displaystyle\lim_{n\to\infty}(||T_{n}||)=0$ , then $\displaystyle\lim_{n\to\infty}(T_{n}(x))=0$ for all $x\in X$ . So if $||{-}||'$ is another norm on $\mathcal{Ban}(X,Y)$ satisfying those conditions, then it must be the case that $||{-}||$ and $||{-}||'$ are equivalent. What would be a reference (or proof) for the above? Are there other similarly nice such ""universal characterisations"" of $\mathcal{Ban}(X,Y)$ ? Lastly, are there analogous results as the above one for semi/normed spaces and topological vector spaces?","['banach-spaces', 'normed-spaces', 'topological-vector-spaces', 'functional-analysis']"
4337385,Solve a first-order nonlinear ordinary differential equation (boundary value problem),"I have been trying to solve the following boundary value problem: $$(x^2-36)(y')^2-2xyy'+y^2-36=0$$ Given the conditions $y(0)=6$ and $y(10)=0$ , where $y(x)$ is continuously twice differentiable on $[0,10]$ .
The requirement is not really to solve the problem itself, but to find $$\int_{0}^{10}y(x)dx$$ I have almost found the answer, but got stuck. My idea was to solve for $y$ and not for $y'$ : $$y=xy'\pm6\sqrt{(y')^2+1}$$ And then to directly integrate this expression. Finally, I ended up with: $$\int_{0}^{10}y(x)dx=\pm3\int_{0}^{10}\left(\sqrt{(y')^2+1}\right)dx$$ But I do not understand how to solve the integral on the right, I have tried many substitutions, but they led nowhere. This leads me to believe that my overall idea to solve for $y$ was a mistake, but I am not sure. I have tried solving for $y'$ and for $x$ , but this did not help either. I would appreciate any suggestions on how to solve the problem.","['boundary-value-problem', 'definite-integrals', 'ordinary-differential-equations']"
4337399,Example of a non-Lipschitz $f \in \mathrm{C}^1(U)$ where $U \subseteq \mathbb{R}^n$ is a non-convex compact connected set,"Consider a smooth function of several variables $f: U \to \mathbb{R} \in \mathrm{C}^1(U)$ where $U \subseteq \mathbb{R}^n$ is a connected set. It can be proven using the mean value theorem that if $U$ is compact and convex (any two points of $U$ can be connected by a segment in $U$ ), then $f$ is Lipschitz, i.e. $$
\exists L \in \mathbb{R} \quad \forall x, y \in U \quad |f(x) - f(y)| \le L \, ||x - y|| 
.$$ Sketch of a proof goes like this: consider a segment connecting $x$ and $y$ , parametrize it, and consider $f$ along the segment as a function of one variable. Apply mean value theorem to it. Since the differential $\mathrm{d}f$ of $f$ is a continuous mapping on a compact $U$ , it is bounded by Weierstrass theorem, and the inequality follows. See this stackexchange question for a full proof. My textbook (on ODEs) says that if $U$ is compact, but not convex, then $f$ can be non-Lipschitz. It also provides an example. In polar coordinates on a plane, consider $$
(r, \phi) \mapsto (r-1)\phi \quad\text{ with } U = \{(r,\phi) \mid 1 \le r \le 2 \;\land\; \phi \in [(r-1)^2, 2\pi - (r-1)^2] \}
$$ Question: Please help me see that this function is indeed non-Lipschitz. What is the motivation for such a strange set of values of $\phi$ ?
If you know a simpler example, please share. UPD : My confusion came from the definition of ""f in polar coordinates"". It must be understood that $f$ is still a function of usual cartesian coordinates $x,y$ , but is expressed in terms of the transition-to-polar-coordinates map $(x, y) \mapsto (r(x,y), \phi(x,y))$ as $f(x,y) = (r(x,y)-1) \phi(x,y)$ .
The definition of $U$ must be changed accordingly to $$
U = \{(x,y) \mid 1
\le r(x,y) \le 2 \;\land\; \phi(x,y) \in [(r(x,y)-1)^2, 2\pi - (r(x,y)-1)^2]\}.$$","['multivariable-calculus', 'lipschitz-functions', 'examples-counterexamples']"
4337432,Entropy of bivariate negative binomial distribution,"The probability mass function (PMF) of a bivariate negative binomial distribution [1] is given by: $$P(X=x, Y=y) = \frac{(a + x + y - 1)!}{(a-1)! x! y!} p_0^a p_1^x p_2^y $$ where $a, p_0, p_1, p_2 > 0$ and $p_0 + p_1 + p_2 = 1$ . I would like to calculate the entropy $$H(x, y) = - \sum_{x=0}^{\infty}\sum_{y=0}^{\infty} P(x, y) \log P(x, y)$$ of this distribution. After some arithmetic manipulation, I arrived at the following expression: \begin{multline}
H(x, y) = - a \log p_0 - \mathbb{E}[x] \log(p_1) - \mathbb{E}[y] \log(p_2) + \log\big((a-1)!\big) \\
- \mathbb{E}\big[\log\big((a + x + y - 1)!\big)\big] + \mathbb{E}[\log(x!)] + \mathbb{E}[\log(y!)]\end{multline} The terms $\mathbb{E}[\log(x!)]$ and $\mathbb{E}[\log(y!)]$ can be calculated by solving a definite integral in the interval $[0, 1]$ as shown in [2] (equation 23). However, it is not clear to me how the same approach can be extended for the computation of expectation of the log factorial $\mathbb{E}\big[\log\big((a + x + y - 1)!\big)\big]$ over the joint. Can anyone shed some light on this? References: [1] Dunn 1967, Characterization of the Bivariate Negative Binomial Distribution ( pdf ) [2] Cheraghchi 2018, Expressions for the Entropy of Binomial-Type Distributions ( pdf )","['statistics', 'probability-distributions', 'entropy', 'probability']"
4337438,How do we use the dominated convergence theorem here? [duplicate],"This question already has answers here : Royden's Lebesgue Integration 4.4 #34 (3 answers) Closed 2 years ago . Let $f$ be a nonegative measurable function on $\mathbb{R}$ . Show that \begin{equation}
\boxed{\lim_{n \to \infty} \int_{-n}^{n} f=\int_{\mathbb{R}} f}
\end{equation} This is from Royden 4th, chapter 4 #34. I know we can easily do this with the monotone convergence theorem, but what about the Dominated convergence theorem? The statement of the dominated convergence theorem states that the dominating function is integrable, and the obvious candidate for that function would just be $f$ since it clearly dominates $f\chi_{[-n,n]}$ . But we do not know that $\int_{\mathbb{R}}f$ has finite integral, so how can we use the theorem?",['measure-theory']
4337451,Product of areas in a disk,"A disk of area $2n$ is divided into $n$ regions by drawing $n$ evenly spaced points on the perimeter and then drawing line segments joining one fixed point with all the other points. An example is shown with $n=8$ . What is the limit of the product of the areas of the regions, as $n$ approaches $\infty$ ? Using basic trigonometry, I've got: $$\lim_{n\to\infty}\prod_{k=1}^{n}\left(2-\frac{n}{\pi}\sin\left(\frac{2k\pi}{n}\right)+\frac{n}{\pi}\sin\left(\frac{2(k-1)\pi}{n}\right)\right)$$ I do not know how to evaluate this limit. Wolfram does not evaluate the limit, but tells me that when $n=10000$ the product is approximately $8.3$ . UPDATE1: I am fairly confident that the limit is $4\cosh^2\left({\frac{\pi}{2\sqrt{3}}}\right)=8.29674...$ Here's why. I was trying to answer a similar question: A ball is divided into $n$ concentric shells of equal thickness. Can the average volume of the shells be fixed so that the product of the volumes converges to a positive number as $n\to\infty$ ? The answer turns out to be yes. If we fix the average volume of the shells to be $\frac{e^2}{3}$ then the product of the volumes converges to $2\cosh\left({\frac{\pi}{2\sqrt{3}}}\right)=2.8804...$ I noticed that this number seemed to be (to many decimal places) the square root of the answer to the question here. I think there must be a connection. UPDATE2: Another similar question is this. A disk is divided into $n$ regions by equally spaced parallel lines, with the perimeter of the disk being tangent to two of the lines. Can the average area of the regions be fixed so that the product of the areas converges to a positive number as $n\to\infty$ ? The answer seems to be yes. If we fix the average area of the regions to be $\frac{{\pi}e}{8}$ then the product of the areas seems to converge to $2\cos\left(\frac{\pi}{2\sqrt{3}}\right)$ (notice this is cos, not cosh). Again, I think there must be a connection. (As for the source of the original question, I thought of the question by myself. It was inspired by an IB (high school) May 2021 exam question, which goes like this. A unit circle has $n$ evenly distributed points. Line segments are drawn joining one point with all the other points (like a seashell). The exam question led students to find the product of the lengths of the line segments, using complex numbers. The answer turns out to be $n$ . Then I wondered, what is the product of the areas of the enclosed regions? Obviously the product approaches $0$ as $n\to\infty$ (since all the areas approach $0$ ). But could the average area be fixed so that the product of the areas converges to a positive number as $n\to\infty$ ? Definitely beyond the course syllabus. After experimenting on desmos, I found that the answer seems to be yes: if we fix the average area to be $2$ , then the product of the areas seems to converge to a positive number. The question in this post is, what does it converge to?)","['infinite-product', 'limits', 'geometry']"
4337466,"If Haar measure is $\sigma$-finite, is the underlying topological space $\sigma$-compact?","Let $X$ be a locally compact Hausdorff group and $\lambda$ a left Haar measure on $X$ . Assume that $\lambda$ is $\sigma$ -finite. Is it true that $X$ is $\sigma$ -compact? Attempt: Write $X = \bigcup_n X_n$ here $X_n$ is a Borel set of $X$ for all $n$ with $\lambda(X_n) < \infty$ . My idea was to approximate each $X_n$ by a compact subset $K_n$ , and then $X=\bigcup_n X_n$ would be well approximated by $\bigcup_n K_n$ , but we still would have to make up for the complement and I don't see how to do that. I will most likely need to use properties of the Haar measure. Also, note that the converse is true.  Any help is welcome!","['measure-theory', 'haar-measure', 'topological-groups']"
4337485,Minimize the expected value of the product of 2 normally distributed variables,"So, there are 2 variables, $X$ and $Y$ , both are normally distributed. We are given that $E(X)=E(Y)=0$ and $Var(X)=2$ , while $Var(Y)=8$ . Additionally, $Corr(X,Y)=-\frac{1}{2}$ . The question is to find the smallest value of $E(X^5Y^3)$ .
My first instinct was to somehow use the definition of covariance: $$Cov(X^5,Y^3)=E(X^5Y^3)-E(X^5)E(Y^3)$$ $$E(X^5Y^3)=Cov(X^5,Y^3)+E(X^5)E(Y^3)$$ I knew that I could find the $E(Y^3)$ using the moment generating function. Since we are given that $Y\sim N(0,8)$ , the moment generating function is $$M_Y(t)=e^{4t^2}$$ So, $$E(Y^3)=\frac{d^3}{dt^3}M_Y(0)=0\implies E(X^5Y^3)=Cov(X^5,Y^3)$$ The same goes for $E(X^5)=\frac{d^5}{dt^5}M_X(0)=0$ . Another idea is to now play with the definition: $$Cov(X,X^4Y^3)=E(X^5Y^3)-E(X)E(X^4Y^3)=E(X^5Y^3)$$ So that $$Cov(X,X^4Y^3)=Cov(X^5,Y^3)$$ Because we are given the correlation for a reason, I got $Corr(X,Y)=\frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}=-\frac{1}{2}=\frac{Cov(X,Y)}{4} \implies Cov(X,Y)=-2$ .
However, I do not see the connection with my previous result. Perhaps I need to use some different method and not covariance. Can anyone point me in the right direction? $\pmb{Edit:}$ I have used a suspicious formula from the paper by Kan, URL: https://www-2.rotman.utoronto.ca/~kan/papers/moment.pdf on page 5 and got the following result for my particular case: $$E(X^5Y^3)=\frac{45}{128}\sum_{j=0}^{1}{\frac{-1}{(2-j)!(1-j)!(2j+1)!}}=\frac{45}{128}\left(-\frac{1}{2}-\frac{1}{6}\right)=-\frac{15}{64}=-0.234375$$ But this result requires verification. Can anyone tell me if the formula is legit and if yes, then did I use it correctly? $\pmb{Edit\space №2:}$ The question about the formula is resolved. I have also found that $E(X^5Y^3)=5760\sqrt{7}Corr(X^5,Y^3)$ from the definition of correlation coefficient, so the question is: is it possible to minimize the correlation coefficient now?","['normal-distribution', 'correlation', 'expected-value', 'optimization', 'probability']"
4337521,Unitary representation of $G$ induces representation of $L^1(G)$,"I am reading Davidson's ' $C^*$ algebras by example'. In chapter VII regarding group $C^*$ algebras, he makes the following claim which I do not understand: When $\pi$ is a unitary representation of a Hausdorff, locally compact group $G$ , it induces a representation of $L^1(G)$ by integration: $$\tilde \pi(f)=\int f(t)\pi(t)dt$$ Here, a unitary representation is a representation of $G$ on a subgroup of unitaries in $B(H)$ , where $H$ is some Hilbert space. I do not understand how exactly this gives a representation (and in fact how it is even defined). First of all, if $\pi(t)$ is an operator, how do we integrate over it with respect to the function $f$ ? Doesn't this only makes sense if $\pi(t)\in \mathbb C$ ? And why is the LHS even an operator? What Hilbert space does it act on and what does it do? I'd appreciate any clarification. Thanks in advance!","['c-star-algebras', 'representation-of-algebras', 'operator-algebras', 'representation-theory', 'functional-analysis']"
4337562,Expected value for a combination of density functions,"Let $f_1,...,f_k$ density functions with continuous random variables $X_1,...,X_k$ and define $$g(x):=\dfrac{f_1(x)+...+f_k(x)}{k}.\forall x\in\mathbb{R}$$ If $E[X_j]=j$ , for $j=1,...,k$ and Y is a random variable with density $g$ . Find $E[Y]$ Is it ok to do $$E[Y]=\int_{-\infty}^{\infty}yg(y)dy=\dfrac{1}{k}
(\int_{-\infty}^{\infty}x_1f_1(x_1)dx_1+...+\int_{-\infty}^{\infty}x_kf_k(x_k)dx_k)$$ $$=\dfrac{1}{k}(E[X_1]+...+E[X_k])=\dfrac{1}{k}(1+...+k)=\dfrac{k(k+1)}{2k}$$ ?? Thank you.","['expected-value', 'statistics', 'density-function', 'random-variables']"
4337580,Prove/disprove: $f^2+f+1$ is not continuous at $x_0$,"Prove or Disprove: Let $x_0\in\mathbb R$ and let $f$ be a function that is defined on a neighborhood of $x_0$ . If $f$ is not continuous at $x_0$ and $f^3$ is continuous at $x_0$ , then $f^2+f+1$ is not continuous at $x_0$ . I am struggling with this proof (or disproof), because in my mind if we have that $f^3$ is continuous at $x_0$ , then $lim_{x\to x_0} f^3(x)=f^3(x_0)\implies lim_{x\to x_0}f(x)=\sqrt[\leftroot{-2}\uproot{2}3]{lim_{x\to x_0} f^3(x)}=\sqrt[\leftroot{-2}\uproot{2}3]{f^3(x_0)}=f(x_0)$ which just implies that $f$ is continuous at $x_0$ and then this is vacuously a proof. Am I missing something?","['limits', 'calculus', 'continuity', 'real-analysis']"
4337671,Can a function be considered odd or even if has a discontinuity?,"First of all we know, $f(x)=sin(x)$ is an odd function because $f(x)=-f(-x)$ The question is if there is a discontinuity (single , interval ,..etc). I will give you some examples to get what I mean: ex1: $f(x)=sin(x)\times\frac{x^2-4}{x^2-4} :x\neq\pm2$ ex2: $f(x)=sin(x)\times\frac{x-1}{x-1} :x\neq1$ ex3: $f(x)=sin(x)\times\frac{(x-1)(x+2)}{(x-1)(x+2)} :x\neq1 ,x\neq-2$ ex4: $f(x)=sin(x)\times\frac{\sqrt{x^2-1}}{\sqrt{x^2-1}} :x\gt1$ I feel abit confused because I cant say $f(x)=-f(-x) $ for all $x$ , but intuitively it feels ok. is that definition not accurate? is there is something I am missing up?","['continuity', 'functions', 'axioms']"
4337700,Show that the sequence $x_N$ converges weakly and compute the weak limit,"Let $H$ be a Hilbert space and let $\{ e_k \}_{k = 1}^\infty $ be an orthonormal basis for $H$ . I am trying to prove that the sequence $$x_N = \frac{1}{\sqrt N}\sum_{k = 1}^N e_k $$ converges weakly and to find its weak limit. What I know so far: Proving weak convergence of $u_N$ to $u$ means
proving that $\ell (u_N) \to \ell(u)$ for all functionals $\ell$ defined as $\ell :H \to \mathbb R$ . In a Hilbert space, however, linear functionals take the form $\ell(u_N) = \langle u,u_N \rangle $ for some unique $u$ that changes only when $\ell$ changes. Could someone give me any clues in proving the above, i.e., that the weak limit exist and how to find it.","['hilbert-spaces', 'functional-analysis', 'weak-convergence']"
4337750,Question about lengths in graded rings,"Let $A$ be a graded, noetherian ring and $\mathfrak p$ a minimal (minimal in the set of all prime ideals) homogeneous ideal. Is it true that the rings $A_{\mathfrak p}$ and $A_{(\mathfrak p)}$ have the same length? (Here, $A_{(\mathfrak p)}$ is the set of elements of degree $0$ of $T^{-1} A$ , where $T$ is the set of homogeneous elements of $A$ not contained in $\mathfrak p$ ). Also, is it true for finitely generated graded modules over $A$ ? To give some context, I am doing exercises in algebraic geometry and if this was true then my life would be much easier :)","['graded-rings', 'artinian', 'ring-theory', 'abstract-algebra', 'commutative-algebra']"
4337816,sum $\sum_{k=1}^{n-1}(1-\frac{k}{n})^{-a}\left(\frac{\log\left(k\right)}{k}-\frac{\log\left(k+1\right)}{k+1}\right)$,"Let for $a>0$ , $\displaystyle S_n(a)=\sum_{k=1}^{n-1}(1-\frac{k}{n})^{-a}\left(\frac{\log\left(k\right)}{k}-\frac{\log\left(k+1\right)}{k+1}\right)$ . I conjecture that there exist a number $c_a\in\mathbb R$ , such that : $$ \displaystyle S_n(a)\sim c_a  \big(\sum_{k=1}^{n-1}{1\over k^a}\big)n^{a-2} \ln n\quad (n\to +\infty).$$ I found by elementary calculus that When $a=1$ , $ \displaystyle S_n(1)\sim \frac 32   \frac{\ln^2 n}n$ , so $c_1=\frac 32$ and when $a=2$ , $ \displaystyle S_n(2)\sim   \frac{\pi^2}6\ln n$ , so $c_2=1$ I need some help for général case I give this non-rigorous proof $\displaystyle S_n(a) = \sum_{k=1}^{n-1} \Big(1-{k \over n}\Big)^{-a} \Big({\ln k \over k} - {\ln (k+1) \over k+1} \Big). $ with change of index : $\displaystyle S_n (a)= \sum_{k=1}^{n-1} \Big({n \over k}\Big)^a \Big({\ln (n-k) \over n-k} - {\ln (n-k+1) \over n-k+1} \Big).$ I believe we have this asymptotic development : $\displaystyle {\ln (n-k) \over n-k} - {\ln (n-k+1) \over n-k+1} = {\ln n -1\over n^2}\Big(1+ {(2k-1)\over n } {2 \ln n -3 \over 2 \ln n-2} + \big({1\over n^3}\big)\epsilon_{k,n}\Big)$ , Thus ? $\displaystyle S_n (a) \sim c_a \Big(\sum_{k=1}^{n-1}\Big({n \over k}\Big)^a \Big) {\ln n \over n^{2}}$ with a constant $c_a$ to be determined Explanations at the request of Dr. Wolfgang Hintze A- Let us establish that Thank's for my friend Lou $\displaystyle \lim_{n\to + \infty}S_n(a)=\left\{\begin{array}{cl} 0 & \text { si } \: 0\leqslant a <2\\+\infty &\:\text {si } \: a\geqslant 2. \end{array}\right.$ Let $f: x\mapsto \dfrac{\log x}x - \dfrac {\log(x+1)}{x+1}.\qquad f(x)\underset{x\to + \infty}\sim \dfrac {\log x}{x^2}.$ $1)\quad\forall a\geqslant 2, \quad S_n(a)\geqslant \dfrac {n^{a}}{(n-1)^{a}}f(1)+\dfrac {n^{a}}{(n-2)^{a}}f(2) +0+0+\dots +0+\dfrac {n^{a}}{(1)^{a}}f(n-1)\underset{n\to + \infty}\sim n^{a-2}\log n.$ $2)\quad $ It's easy to prove that : $\:\:\forall x\in [3;n], \:\: 0<f(x)<\dfrac{ \log n}{x^2} \quad(1).$ If $a\in [0;2[.\quad S_n(a) =-\dfrac{\log n}n+ \displaystyle \sum _{k=1}^{n-1}\left(\dfrac {n^{a}}{(n-k)^{a}} -1\right) f(k).\quad$ write : $\:T_n(a):=\log n\displaystyle \sum _{k=3}^{n-1}\left(\dfrac {n^{a}}{(n-k)^{a}} -1\right) \dfrac 1{k^2}.$ From $ (1) $ , it therefore suffices to prove that : $ \displaystyle  \lim_{n\to + \infty} T_n(a) =0.$ $\displaystyle T_n(a)=\log n  \sum_{k=1}^{n-3}\dfrac{n^{a} - k^{a}}{k^{a}(n-k)^2} =\dfrac{\log n}{n^2}  \displaystyle \sum_{k=1}^{n-3}(n^{a}- k^{a})\left( k^{-a}+ \dfrac {2 k^{1-a}}{n-k} + \dfrac {k^{2-a}}{(n-k)^2}\right).\quad $ We also have inequalities: $\:\:\forall a \in \mathbb R^+,\: \forall k\in [1;n]: $ $0 \leqslant a<2 \implies 0\leqslant n^{a}- k^{a}\leqslant an^{a-1}(n-k).\quad (2), \quad 0\leqslant a \leqslant1 \implies 0\leqslant n^{a}- k^{a}\leqslant ak^{a-1}(n-k).\quad (3).$ They lead to: $\bullet \:\text{Si }1\leqslant a<2, \:\text {alors}\:\:0<T_n(a) < \log n \left(n^{a-2}\displaystyle \sum_{k=1}^{n-3}k^{-a} +2a n^{a-3}\sum_{k=1}^{n-3}k^{1-a}+an^{-1}\sum_{k=1}^{n-3} (n-k)^{-1}\right). \qquad (4)$ $\bullet \:\text{Si }0\leqslant a \leqslant1, \:\text {alors}\:\: 0<T_n(a) < \log n \left(n^{a-2}\displaystyle \sum_{k=1}^{n-3}k^{-a} +2a n^{-2}\sum_{k=1}^{n-3}1+an^{-1}\sum_{k=1}^{n-3} (n-k)^{-1}\right).\qquad (5)$ We check that the right-hand side of the inequalities (4) and (5) have a zero limit when $ n \to + \infty.$ B-  Show that $\:\:\boxed{S_n(1) \underset{n\to + \infty}\sim \dfrac {3(\log n)^2}{2n}.}\qquad$ Let $g: x\mapsto x^2f(x) -\log x.\quad $ Then : $\:\:\displaystyle \lim_{+\infty} g=-1, \quad g \text { is bounded on  } [1;+\infty[\:\: (1).\qquad$ Let $\:\:T_n:= \displaystyle \sum_{k=1}^{n-1} \dfrac k{n-k} f(k). $ Then: $ \:\: S_n(1) = T_n- \dfrac {\log n}n\:\:(2).\:\:\quad T_n=\dfrac 1n \displaystyle \sum_{k=1}^{n-1}\left( \dfrac 1k + \dfrac 1{n-k}\right) k^2f(k) =U_n +V_n +W_n\:\:$ with $U_n,V_n,W_n $ are defined by  : $\displaystyle U_n:=\dfrac 1n\sum_{k=1}^{n-1} \dfrac {\log k}k, \quad V_n:= \dfrac 1n\sum_{k=1}^{n-1} \dfrac {\log k}{n-k}, \quad W_n:= \dfrac 1n\sum_{k=1}^{n-1}\left( \dfrac 1k + \dfrac 1{n-k}\right) g(k).\:\:\:$ According to $(1): \quad W_n=\mathcal O\left (\dfrac{\log n}n \right)\:\:(3).\quad \:\:U_n \underset{n\to + \infty}\sim \dfrac{(\log n)^2}{2n}\:\: (4). \qquad V_n =\displaystyle \dfrac {\log n}n\sum_{k=1}^{n-1}\dfrac 1{n-k} +\dfrac 1{n^2}\displaystyle\sum_{k=1}^{n-1}\dfrac {\log(k/n)}{1-(k/n)}. $ The Function $h:x\mapsto \dfrac{\ln x}{1-x}\text{ is continuous, monotonic, integrable on  }\:]0;1[,\quad $ so $\:\:\displaystyle \lim_{n\to +\infty}\dfrac 1n\sum_{k=1}^{n-1} \dfrac {\log(k/n)}{1-(k/n)} =\int _0 ^1 h .$ This information, combined with the fact that: $\: \displaystyle \dfrac {\log n}n\sum_{k=1}^{n-1}\dfrac 1{n-k} \underset{n\to + \infty}\sim \dfrac {(\log n)^2}n,\:$ leads to: $\:\:V_n\underset{n\to + \infty}\sim  \dfrac {(\log n)^2}n.\quad (5)$ $(2), (3), (4) $ et $(5)$ provide the equivalent of $ S_n (1) $ announced. I can explain the case a = 2 if needed","['sequences-and-series', 'asymptotics', 'real-analysis']"
