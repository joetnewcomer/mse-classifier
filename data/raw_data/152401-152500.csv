question_id,title,body,tags
2554567,Show that this sequence contains infinitely many composite numbers.,"Let $a_1$ be any number.  Create a sequence $a_1, a_2, a_3$, where $a_n$ is formed by appending a digit from 0 to 8 to the end of $a_{n-1}$.  Show that this sequence contains infinitely many composite numbers.  What happens if you allow for 9?  Can you still guarantee that infinitely many composite numbers must occur?","['prime-numbers', 'sequences-and-series']"
2554612,Change of variables formula for random variable,"On Durrett, it has a theorem saying:
Let $X$ is a random variable, and $f$ is a measurable function on $\mathbb{R}$. Assume $f\geq 0$ or $E|f(X)| < \infty$, then we have $Ef(X) = \int_{\mathbb{R}} f(y) d \mu (y)$, where $\mu $ is the probablity measure induced by the random variable $X$. My question is:
when we compute $Ef(X)$, why do we implicitly ignore the condition $f\geq 0$ or $E|f(X)| < \infty$ ? 
Actually, $E|f(X)| < \infty$ is not easy to be verified. For example,
let $X$ has an exponential distribution with rate 1, then 
$$
EX^k = \int_0 ^{\infty} x^ke^{(-k)} dx = k!
$$
we apply the change of variable without checking that $E|X^3| < \infty$. We do not even know what the random variable is. A more general question is that when we talk about a random variable, why we usually ignore the probability space and the random variable which is a measurable function on such space? I know that for each distribution function, there is a probability space with a random variable whose has the distribution function as its distribution. Namely, $\Omega = (0,1), p$ = lebesgue measure... Thanks for your help.","['probability-theory', 'probability']"
2554662,How to model the probability of this gift exchange?,"There is a gift exchange with $N$ people using the following rules: Everyone starts with a randomly selected wrapped gift. The people take turns rolling a 6-sided die to determine what action to take. Rolling a 1 or a 2 means that person gets to open the gift in front of them - and that person is taken out of the rotation. Rolling a 3, 4, 5, or 6 causes other events that shift around the wrapped gifts (i.e. that person cannot open their gift yet). The game ends once every gift has been opened. I wanted to know how many turns this gift exchange should go on until completion but I couldn't figure out how to model that. I try restructuring it as ""What is the probability it will end in $X$ turns?"" but still didn't get far. Any ideas? I know the probability of opening a gift on any given turn is $\frac{2}{6} = \frac{1}{3}$. So I could calculate the probability of the gift exchange ending as fast as possible ($N$ turns for $N$ people), which would be $\left(\frac{1}{3}\right)^N$. How can I expand on this to compute the probability of the exchange ending in $X$ turns, where each turn has a $\frac{2}{3}$ chance of ""no gifts opened""?","['combinatorics', 'probability']"
2554698,Example of a vector space that is not a topological space?,"I saw this on Wikipedia about the hierarchy of spaces : Later in the article, it states that the fundamental building blocks of mathematical spaces are vector spaces and topological spaces. However, it's not clear to me how topological spaces and vector spaces are related, if at all. Can someone provide an example of a vector space that is not a topological space and vice versa?","['functional-analysis', 'general-topology', 'topological-vector-spaces', 'vector-spaces']"
2554750,Functional Derivative on Manifold?,"In M-theories, there are often Action functionals (in the physics sense), defined on manifold involving p-forms and such. Letting $\mathcal{M}$ denote the manifold with dimension $d$, one might encounter something of the form:
$$S = a \int_\mathcal{M} A \wedge \mathrm{d}A + b\int_\mathcal{M}  A \wedge A \wedge A $$
Where these are some sort of non-abelian differential forms. In the usual case, one defines the integrand as the Lagrangian $\mathcal{L}$ and can determine the equations obeyed by the dynamical variables by solving the Euler-Lagrange equation, for example if $\mathcal{L}= \mathcal{L}(t, x, dx/dt)$, then we solve:
$$ \frac{\partial \mathcal{L}}{\partial x } - \frac{\mathrm{d}}{\mathrm{d} t }\left(\frac{\partial \mathcal{L}}{\partial (\frac{\mathrm{d}x}{\mathrm{d}t})}  \right) = 0 $$ 
If there a mathematically rigorous way to generalize this to the case of differential forms, for example, in the action above, if I abuse notation assume that:
$$ \frac{\partial \mathcal{L}}{\partial A}+ \mathrm{d}\left(\frac{\partial \mathcal{L}}{\partial (\mathrm{d} A)} \right)= 0 $$
Then I get something of the form:
$$ a\mathrm{d}A + 3 b A\wedge A + a \mathrm{d}A = 2a\mathrm{d}A + 3 b A\wedge A = 0    $$
(Where $\mathrm{d}^2 =0$). Where this is in fact the correct equation of motion describing the system and is what one would compute by considering the variation $A \to A+ \delta A$. Is this a fluke? or can this method be trusted in general?","['differential-topology', 'differential-forms', 'calculus-of-variations', 'ordinary-differential-equations', 'differential-geometry']"
2554829,"Let p be an odd prime. Then $\sum a^{-1} ≡ 0\mod p$ where $a$ from $1$ to $p-1$, and $a^{-1}$ is the multiplicative inverse of $a$ modulo p.",Prove or disprove: Let $p$ be an odd prime. Then $\sum_{a=1}^{p-1} a^{-1} ≡ 0\pmod p$ where $a^{-1}$ is the multiplicative inverse of $a$ modulo $p.$ I think it is a true statement. $\sum a^{-1} =(1^{-1} +2^{-1} +....+(p-1)^{-1} =[(p-1)(p)]/2 ≡ 0 \mod p$ so $[(p-1)(p)]/2 ≡ 0 \mod p$ $[(p-1)(p)]/2=np$ so $n=(p-1)/2$ I am not sure am I in right way ? any help with that thank you,"['number-theory', 'elementary-number-theory']"
2554846,Sums of distinct powers of 3 that are also sums of distinct powers of 4. Is there a finite amount?,"Is the set of all numbers that can be represented as a sum of distinct powers of $3$ (e.g. $256=3^{0}+3^{1}+3^{2}+3^{5}$ and as a sum of distinct powers of $4$ (e.g. $256=4^{4}$) finite? Equivalently, are there an infinite number of integers that can be represented in base $3$ and $4$ with only $0$s and $1$s for digits? Relevant OEIS entry: A258981",['number-theory']
2554861,Outward-pointing vector field on Projective space,"Lee Smooth Manifolds problem 8-4 says that for every manifold with boundary there exists a smooth vector field that is outward-pointing when restricted to the boundary. Now if our manifold is $M=\mathbb{R}P^2\times [0,1)$, there should be some smooth outward pointing vector field on $\partial M=\mathbb{R}P^2$. My questions are, why does this not determine an orientation for $\mathbb{R}P^2$, and what does this vector field ""look"" like (pictures welcome) when restricted to $\mathbb{R}P^2$? If you can provide an explicit example of such a vector field that would be much appreciated. Thanks for your help!","['manifolds', 'vector-fields', 'orientation', 'differential-geometry']"
2554972,"How many ""constants of integration"" will a PDE have?","For smooth cases, we expect the solution of an $n$th-order ODE to have $n$ constants of integration. Is there a name to the theorem which more precisely makes this claim? Is there an analogous theorem for PDEs?  That is, do we expect $n$ ""integration functions"" for an $n$th-order PDE?","['reference-request', 'ordinary-differential-equations', 'partial-differential-equations']"
2554996,Show that $T : \mathbb{R}^3 → \mathbb{R}^3$ is a linear transformation.,"Consider the function $T : \mathbb{R}^3 \to \mathbb{R}^3$ given by $$T(a, b, c) = (\operatorname{Curl }\vec{F}_{(a,b,c)} )(1, 1, 1),$$ where $\vec{F}_{(a,b,c)}$ is the vector field given by $$\vec{F}_{(a,b,c)}(x, y, z) = (axy + bxz, bxy + cyz, cxz + ayz).$$ Is $T$ a linear transformation? If so, calculate its matrix. I know that I need to take the curl of $\vec{F}_{(a,b,c)}$, and then plug in the point $(1, 1, 1)$ for $(x, y, z)$. But how would I determine if $T$ is a linear transformation? Thank you for your help!","['multivariable-calculus', 'linear-algebra', 'linear-transformations', 'vector-analysis']"
2555009,Proving that a polynomial over $K[t]$ has a multiple zero if and only if $f$ shares a factor with $f'$.,"The author is trying to prove that a polynomial $f$ has a repeated zero in it's splitting field over $K$ if and only if $f$ shares a common factor with $f'$ of degree $≥1$ . He first proves that if $f$ has a repeated zero, then $f$ and $f'$ have a common factor of degree $≥1$ in $K[t]$ , in the following way: $$f=(t-a)^2g$$ $$f'=(t-a)^2g'+2(t-a)g$$ At the end of the proof he states "" $f$ and $f'$ share a common factor in $S[t]$ , so they share a common factor in $K[t]$ "". Where $S$ is the splitting field of $f$ over $K$ . I understand how these polynomials share a common factor in $S[t]$ , but how is it that the author infeere from this that they share a common factor in $K[t]$ ? He then continues to prove that if $f$ has no repeated roots, then it doesn't share a common factor with $f'$ . This part of the proof is understandable since it doesn't depend on $K[t]$ . I would really appreciate any help/thoughts.","['derivatives', 'abstract-algebra', 'polynomials', 'field-theory']"
2555052,Integral That Requires Differentiating Under the Integral Sign Twice?,"Is it ""legal"" to differentiate under the integral twice? Are there problems where there this is actually useful? If I were to differentiate under the integral sign twice, I would need to integrate the function with respect to the inserted variable twice too to get the original function, but in the process, I would need to find two different constants of integrations too. Does this make sense? Are there problems that actually simplify better by differentiating under the integral twice? Or is this completely not allowed by some definition or theorem that I'm unaware of? Hopefully, this was not too incoherent to understand, 
thanks in advance.","['multivariable-calculus', 'integration', 'calculus']"
2555054,Prove this Kenneth S. Williams inequality,"If $0<a_1\le a_2\le \cdots\le a_n$ , then the following inequality holds: $$\frac{1}{2n^2a_n}{\sum_{1\le i < j\le n}^{} {(a_i-a_j)^2}}\le \frac{a_1+a_2+\cdots + a_n}{n}-\sqrt [n]{a_1 a_2 \cdots a_n }{\le \frac{1}{2n^2a_1}\sum_{1\le i < j\le n}^{} {(a_i-a_j)^2}}.$$ This problem was proposed by Kenneth S. Williams, Carleton University, Ottawa in CRUX 247[1977;131] and in CRUX[1978;23,37] it is said that there is a nice simple proof but I can't find this G. Szekeres (October 1977 was published by Rennie in JCMN, NO.12) shorter proof. Can help me? Thanks. Maybe now this inequality have some methods to solve it, such as AM-GM inequality?","['multivariable-calculus', 'real-analysis', 'inequality', 'a.m.-g.m.-inequality']"
2555058,Number of prime divisors of a proper closed subset of a scheme,"Let $X$ be a noetherian integral separated scheme which is regular in codimension one. Let $Z$ be a proper closed subset of $X$. There is a claim I could not figure out: $Z$ can contain at most finitely many prime divisors of $X$ (as usual, a prime divisor of $X$ is a closed integral subscheme of $X$ of codimension one). Could anyone explain why this is true?",['algebraic-geometry']
2555092,"Solving a particular integro-differential equation: $g(x) = \int_1^x \frac{F(u)}{\sqrt{x^2-u^2}} \, du$","As the title says: I'm interested in the following integro-differential equation. Let $g:(1,\infty) \to [0,1]$ be given, and assume $g$ is smooth. I want to find functions $F:[1,\infty) \to [0,1]$ that satisfy: i) For every $x \in (1, \infty)$, $\displaystyle\int_1^x \frac{F(u)}{\sqrt{x^2-u^2}} \, du = g(x)$ ii) $F$ is non-decreasing, right continuous, $F(1) = 0$, and $\lim_{y \to \infty} F(y) = 1$, i.e. $F$ is a CDF supported on $(1,\infty)$. The first thing to try is differentiating, but this doesn't seem to go anywhere: since that the integrand $\frac{F(u)}{\sqrt{x^2-u^2}}$ blows up at $u = x$, we can't (immediately) pass the derivative through the integral. The next thing is integrating by parts, which yields $\displaystyle g(x) = \frac{\pi}{2} F(x) - \int_1^x F'(u) \arctan\Big(\frac{u}{\sqrt{x^2-u^2}}\Big) \,du.$ (Some additional assumptions are needed here: for example, that $F$ is differentiable.) Now using the Leibniz integral rule , $\displaystyle g'(x) = \int_1^x \frac{F'(u)}{x\sqrt{x^2-u^2}} \, du.$ But this doesn't seem to help much. Are there standard methods for this kind of thing? Is there some way to make sense of passing the derivative inside the integral? I would be happy to see an example of some $g$ and $F$ that satisfy this: I have no such examples right now.","['calculus', 'integro-differential-equations', 'probability', 'ordinary-differential-equations', 'analysis']"
2555101,Equivalence of Path-Connectedness and Arc-Connectedness for Hausdorff Spaces,"I have a classical sort of question.  If we define a path in a space $X$ from points $a$ to $b$ to be a continuous image $f: I \rightarrow X$ from the unit interval to $X$ such that $f(0) = a$ and $f(1) = b$, then we can define an arc from $a$ to $b$ to be the same except we require that $f$ is a topological embedding, i.e. $f$ is a homeomorphism between $I$ and its image in $X$. We can then define path-connectedness as per usual, and define arc-connectedness to mean that for every $a, b \in X$ there is an arc from $a$ to $b$ in $X$.  It's a well-known theorem that for Hausdorff spaces these two properties are equivalent.  However, the only proof I've seen uses the Hahn-Mazurkiewicz Theorem and the Moore/'cut point' characterization of the arc, neither of which are very fun to prove and which require an exposition of the order topology. One of the main applications of these theorems is that every compact, connected, locally connected metric space (the definition of a Peano Continuum ) is locally arc-connected.  Then the equivalence of path-connectedness and arc-connectedness is established as a corollary of this. What I want to do is prove the theorems in the following order: (1a) Path-connectedness and arc-connectedness are equivalent for Hausdorff spaces. (1b) If $X$ is a Peano continuum and $x \in X$ then $x$ has a local basis of Peano (sub)continua. (2) Every Peano continuum is path-connected (more precisely, there is a surjection of $I$ onto $X$, which is the Hahn-Mazurkiewicz Theorem). (3) Immediate corollary: Every Peano continuum is locally arc-connected. The proof of (1b) is standard and involves a concept called Property $S$.  I have developed a short, self-contained proof of (2) using an esoteric theorem called the Order Arc Theorem (which I need for other reasons, anyway) and (1b) .  Then (3) follows immediately. So the only task is to prove (1a) ,  the equivalence of arc-connectedness and path-connectedness, in a self-contained manner.  Does anyone know of a proof of this fact that doesn't involve the Hahn-Mazurkiewicz Theorem/Moore classification/order topology?  These theorems are very fundamental and attractive (particularly (1a) ), so it would be nice to have a shorter proof of them so that they could be included in topology courses.  I have sketched a proof; could someone look it over and let me know if there is an error?  If there is, it's probably of the glaring sort haha I assume familiarity with the devil's staircase function $d$ from the standard Cantor ternary set onto $I$.  It can be shown that this function is continuous, and is 1-1 everywhere except the end points of the deleted middle-third intervals, where it is 2-1.  Using the Moore classification of the Cantor set (or by explicit construction), one can show that the set created by recursively removing intervals of sizes other than $\frac{1}{3^n}$ and/or moving their centers also results in a Cantor set that admits an order-preserving homeomorphism $\phi$ with the standard Cantor set.  For a Cantor set $D$ constructed as such, we call the map $d \circ \phi: D \rightarrow I$ a pinching map for $D$. So suppose $X$ is a path-connected Hausdorff space and let $a, b \in X$.  Let $f$ be a path from $a$ to $b$; we need to show there is a bona-fide arc from $a$ to $b$.  After discarding and reparametrizing if need be, since $X$ is Hausdorff we can assume that $f^{-1}(a)$ and $f^{-1}(b)$ are $\lbrace 0 \rbrace$ and $\lbrace 1 \rbrace$ respectively. Let $E_1$ be a (not necessarily unique) largest non-trivial closed interval in $I$ such that its end points have the same image under $f$, i.e. $f(E_1)$ has a 'loop' (existence uses the Hausdorff property again).  If none exists then $f$ is a bijection so we're done.  Assuming otherwise, consider $J_1 = I \setminus E_1^\circ$, where $E_1^\circ$ is the interior of $E_1$.  This chops $I$ into two closed intervals, and we can then repeat inductively, cutting away successive intervals from the interiors (and possibly leaving some intervals intact if $f$ is bijective on them) of remaining intervals.  We end up with a closed set $Z = \cap J_n$ consisting of closed intervals and Cantor sets. Then form the 'pinching map' $g$ of $Z$ by just having the map be linear on the non-trivial intervals in $Z$.  This will be a continuous map - in fact a quotient map - from $Z$ onto $I$ that is 1-1 everywhere except the end-points of the removed intervals $E_k$.  However, $f$ restricted to $Z$ agrees precisely with the identifications of $g$ so by a standard quotient map argument will lift to a continuous map from $I$ to $f(Z)$. But this will be a bijection on $I$ if we can show it's 1-1 on $Z$ everywhere except the end points of the sets $E_n$, and thus will be our desired arc from $a$ to $b$ since continuous bijections on a compact $I$ into a Hausdorff $X$ are homeomorphisms.  Suppose not, i.e. $f(x) = f(y)$ with $x \neq y$.  Then $|x - y| > 0$ but length$(E_n) \rightarrow 0$ since, for example, it's an infinite collection of disjoint intervals in $[0,1]$, impossible. Does anyone see an error here and/or want to help me out with the fine details?","['connectedness', 'continuum-theory', 'general-topology', 'metric-spaces', 'cantor-set']"
2555105,Am I way off on this proof of continuity of $f(x)=\frac{x+|x|}{2}$?,"We're asked to determine where $f(x)=\frac{x+|x|}{2}$ is continuous and prove. Clearly, it is continuous on $\mathbb{R}$. The answer that I was able to find involved splitting it into three separate intervals $(x<0, x = 0, x>0)$. Intuitively, that is how I decided it was continuous on $\mathbb{R}$, but it seems to me a simpler $\epsilon-\delta$ proof is as follows: A function $f$ is continuous if $|x-a| < \delta$ then $|f(x)-f(a)| < \epsilon$. Alternatively express the above as $|f(x)-f(a)| < \epsilon$ if $|x-a| < \delta$ Suppose $|x-a| < \delta$. Then: 
$$|f(x)- f(a)| = \left|\frac{x+|x|}{2} - \frac{a+|a|}{2}\right| < \epsilon$$
rearrange, and use triangle inequality:
$$|x+|x|-(a-|a|)| = \left|x-a-(|x|-|a|)\right|\leqslant |x-a| + ||x|-|a|| <2\epsilon$$ Using the above definition of $\delta$ and the second triangle inequality
$$|x-a| + ||x|-|a|| < |\delta| + \left||x|-|a|\right| \leqslant |\delta| + |x-a| \leqslant |\delta| +|\delta| =2\delta <2\epsilon$$ Therefore, if $|x-a|<\delta, |f(x)-f(a)| < \epsilon$, and $\epsilon>\delta$, so we can pick $x$ ""close enough"" (i.e., within $\delta$ of the a) so the function is continuous I want to know if that is just totally wrong, so that I don't go about trying to prove things in a similar manner to find out I'm missing the point.","['real-analysis', 'calculus', 'functions', 'continuity', 'absolute-value']"
2555122,"If $f$ is a concave function and $f(0) \geq 0$ then $f(a+b) \leq f(a) + f(b)$ for all $a,b >0$","Well If I've got a nice understanding, this proof involves the use of Rolle's and/or Mean Value Theorems . But I don't even an idea of how to start. We already know that 
$$f\text{ concave}\Rightarrow f''<0$$
but, then...","['derivatives', 'real-analysis', 'calculus']"
2555175,How to prove that $\sum_{n=1}^{\infty}\frac{\left ( -1 \right )^{n+1}n}{n^{2}+x^{2}}$ is always positive for all real $x$?,"How to prove that $$\sum_{n=1}^{\infty}\frac{\left ( -1 \right )^{n+1}n}{n^{2}+x^{2}}>0$$ for all real x? It may looks like a very easy problem, but as it is an alternating series and does NOT converge ABSOLUTELY, it seems very hard -- or even impossible -- to use any inequalities to estimate. Moreover, the limit $$\lim_{x\rightarrow\infty}\sum_{n=1}^{\infty}\frac{\left ( -1 \right )^{n+1}nx^{2}}{n^{2}+x^{2}}=\frac{1}{4}$$. If we can prove this, it will be easier to prove that the series is positive. I used Abel transformation formula (summation by parts) and got that it is the same value as $$\sum_{n=1}^{\infty}\sum_{k=n}^{\infty}\frac{\left(-1\right)^{k+1}}{k^{2}+x^{2}}$$, and then I don't know if the problem get easier. Now we have to prove that $\displaystyle\left|\sum_{k=n}^{\infty}\frac{\left(-1\right)^{k+1}}{k^{2}+x^{2}}\right|$ is strictly decreasing for $n\in\mathbb{N}$, that is equivalent to prove that $$\frac{1}{n^2+x^2}\gt2\left|\sum_{k=n+1}^{\infty}\frac{\left(-1\right)^k}{k^2+x^2}\right|\quad \forall n\in\mathbb{N},x\in\mathbb{R}$$ Anyone has some ideas to contnue my proof?","['sequences-and-series', 'analysis']"
2555218,Expected Value of Normal Random Variable times its CDF,"As usual, let $\Phi$ and $\varphi$ denote the cumulative density function and the density function of a standard normal random variable. On the wiki page "" List of integrals of Gaussian functions "", I have found an expected value integral involving  a standard normal r.v. and its cdf, $$I=\int_{-\infty}^{\infty}x\varphi(x)\Phi(a+bx)dx=\frac{b}{\sqrt{1+b^2}}\varphi\left(\frac{a}{\sqrt{1+b^2}}\right),$$ for which I do not know how to do the last step in my solution: My ansatz is to introduce a parameter integral, $I:=I(a)$, and finding its derivative: $$
\begin{align*}
\frac{\partial I}{\partial a}&=\int_{-\infty}^{\infty}x\varphi(x)\varphi(a+bx)dx\\
&=\int_{-\infty}^{\infty}x\frac{e^{-\frac{1}{2}\left(x^2(1+b^2)+2abx+a^2\right)}}{2\pi}dx\\
&=a\frac{e^{-\frac{1}{2}\frac{a^2}{1+b^2}}}{\sqrt{1+b^2}\sqrt{2\pi}}\\
&=a\frac{\varphi\left(\frac{a}{\sqrt{1+b^2}}\right)}{\sqrt{1+b^2}}
\end{align*}
$$ Integrating the derivative, we obtain: $$
\begin{align}
I&=\int \frac{\partial I}{\partial a}da + C\\
&=\frac{b}{\sqrt{1+b^2}}\varphi\left(\frac{a}{\sqrt{1+b^2}}\right)+C,
\end{align}
$$ which equals the solution on the wiki page plus a constant term $C$. From here on, I do not know how to get rid of the integration constant, i.e. how to show that $C=0$. I do know that for $b=0$ it holds that $I=0$. Is this be sufficient to pin down $C$ to zero? Or do I miss something completely?","['expectation', 'integration', 'probability', 'probability-distributions']"
2555240,Weierstrass theorem to prove Slutsky's lemma,"Let $(Y_n)_{n\ge 1}$ and $(Z_n)_{n\ge 1}$ be two sequences of random variables defined on the same space $(\Omega,\mathcal{F},\Bbb{P}).$ There is Slutsky's lemma which states: $$Y_n\overset{D}\to Y\;\mbox{and}\; Z_n\overset{P}\to c\; \mbox{then}\; (Y_n,Z_n)\overset{D}\to( Y,c).$$ which means that foor all $h\in C_b(\Bbb{R}^2),\quad\Bbb{E}h(Y_n,Z_n)\overset{n\to\infty}\to \Bbb{E} h(Y,c).$ The proof starts by saying it is sufficient to consider $h$ as $h(y,z)=f(y)g(z).$ It doesn't seem trivial at all? I know that for the convergence in distribution it's enough to prove the convergence in a space $\mathcal{H}$ that is total in $C_0(\Bbb{R}).$ Perhaps we can use this idea with Weierstrass theorem but we need compact space, perhaps using Alexandroff one-point compactification for $\Bbb{R}^2.$","['functional-analysis', 'general-topology', 'probability-theory']"
2555273,Normal random variables: expectation of squared empirical mean divided by empirical second moment,"Assume $X_1,\dotsc, X_n$ are iid $\mathcal{N}(\mu, \sigma^2)$-distributed. Define $m=\frac{1}{n} \sum_{i=1}^n X_i$ the empirical mean and $z^2 = \frac{1}{n} \sum_{i=1}^n X_i^2$ the empirical second moment. I am looking for the expectation $$\mathbf{E}\left[\frac{m^2}{z^2}\right].$$ The ultimate goal is to estimate the quantity $$\frac{\mu^2}{\mu^2 + \sigma^2}.$$ What I know so far: Since $z^2\geq m^2$ (Jensen's inequality), the fraction is bounded and the expectation exists. $m\sim \mathcal{N}(\mu, \frac{\sigma^2}{n})$ and, thus, $\mathbf{E}[m^2] = \mu^2 + \frac{\sigma^2}{n}$ $\mathbf{E}[z^2] = \mu^2 + \sigma^2$ $n\frac{z^2}{\sigma^2} = \sum_{i=1}^n \left( \frac{X_i}{\sigma} \right)^2$ follows a non-central chi-squared distribution with $n$ degrees of freedom and non-centrality parameter $\lambda = n\frac{\mu^2}{\sigma^2}$ $m^2$ and $z^2$ are obviously dependent, but $m$ is independent of the empirical variance $s^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - m)^2 = \frac{n}{n-1} (z^2 - m^2)$. It is straight-forward to derive an expression for $\mathbf{cov}(m^2, z^2)$ Does anybody have an idea how to compute $\mathbf{E}[\frac{m^2}{z^2}]$ and/or how to estimate $\frac{\mu^2}{\mu^2+\sigma^2}$?","['statistics', 'probability', 'probability-distributions']"
2555298,A strengthening of a Sylow theorem?,"Let $G$ be a finite group, let $p$ be a prime number. It is a well known theorem of Sylow that the number of Sylow $p$-subgroups of $G$ divides $\vert G \vert$. I wonder if the following strenghtening is true : Statement 1. Let $G$ be a finite group, let $p$ be a prime number, let $G_{0}$ be a $p$-subgroup of $G$. Then the number of Sylow $p$-subgroups of $G$ containing $G_{0}$ divides $\vert G \vert$. If I'm not wrong, the following special case is true : Theorem. Let $G$ be a finite group, let $p$ be a prime number, let $G_{0}$ be a $p$-subgroup of $G$. Assume that $G_{0}$ is normal in every Sylow $p$-subgroup of $G$ containing $G_{0}$. Then the number of Sylow $p$-subgroups of $G$ containing $G_{0}$ divides $\vert G \vert$. Proof. Let $P_{1}, \ldots , P_{r}$ be the distinct Sylow $p$-subgroups of $G$ containing $G_{0}$. We have to prove that $r$ divides $\vert G \vert$. By hypothesis, $P_{1}, \ldots , P_{r}$ normalize $G_{0}$, thus $G_{0}$ is normal in the subgroup $P = <P_{1}, \ldots , P_{r}>$ of $G$ generated by $P_{1}, \ldots , P_{r}$, thus $G_{0}$ is contained in every Sylow $p$-subgroup of $P$. Since the Sylow $p$-subgroups of $P$ are clearly Sylow $p$-subgroups of $G$, we have proved that every Sylow $p$-subgroup of $P$ is a Sylow $p$-subgroup of $G$ containing $G_{0}$. In other words, every Sylow $p$-subgroup of $P$ is a $P_{i}$. Reciprocally, every $P_{i}$ is a Sylow $p$-subgroup of $P$, thus $r$ is the number of Sylow $p$-subgroups of $P$, thus (Sylow theorem) $r$ divides $\vert P \vert $ and thus divides $\vert G \vert $. Do you know if the normality hypothesis can be cancelled in the above theorem ? And perhaps, do you know a reference to the literature concerning this matter ? Thanks in advance. (By the way, I think that the following is true : Let $G$ be a finite group, let $p$ be a prime number, let $G_{0}$ be a $p$-subgroup of $G$; then the number of Sylow $p$-subgroups of $G$ containing $G_{0}$ is $\equiv 1 \pmod{p}$. Thus Statement 1, if true, can be formulated in the more precise manner :  Let $G$ be a finite group, let $p$ be a prime number, let $G_{0}$ be a $p$-subgroup of $G$. Then the number of Sylow $p$-subgroups of $G$ containing $G_{0}$ divides $\vert G \vert / p^{n}$, where $p^{n}$ denotes the greatest power of $p$ dividing $\vert G \vert$.)","['finite-groups', 'group-theory']"
2555333,How to solve Cauchy problem for $y' + y\cos x = e^{-\sin x} \quad y(0) = 1$?,"I have: $$y' + y\cos x = e^{-\sin x} \quad y(0) = 1$$ Applying Lagrange's method I got: $$y' + y\cos x = 0 \\ \frac{dy}{dx} = -y\cos x \\ \int\frac{dy}{y} = -\int \cos x \, dx \\ \ln |y| = -\sin x +C \\ y = C\cdot e^{-\sin x}$$ $$y(x) = C(x)\cdot e^{-\sin x} \\ y'(x) = C'(x)e^{-\sin x} - C(x) e^{-\sin x} \cos x$$ substituting back into the original equation and simplifying I got: $$C'(x)e^{-\sin x} = e^{-\sin x}$$ and therefore $$C(x) = x + \alpha$$ and the final answer is: $$y = (x + \alpha)e^{-\sin x}$$ But how to solve Cauchy problem for the final answer?","['cauchy-problem', 'ordinary-differential-equations']"
2555356,Miller-Rabin: Showing non trivial divisors of $n$,"Let's assume $n$ doesn't pass the Miller-Rabin test and $b$ is a witness. Meaning, $b^{\frac{n-1}{2^r}} \equiv 1 \pmod{n}$ where $\frac{n-1}{2^r}$ is even, but $c = b^{\frac{n-1}{2^{r+1}}} \not\equiv \pm1 \pmod{n}$. Show that $\gcd(c+1,n),\gcd(c-1, n)$ are non-trivial divisors of $n$. So first of all, for my convenience: Denote $n-1 = 2^ls$, $s$ is odd. Denote $k=l-r$. $c = b^{2^{k-1}s}$ $c^2 = b^{2^{k}s}$ $2^ks$ is even It is given that $c^2 \equiv 1 \pmod{n} \implies (c-1)(c+1) \equiv 0 \pmod {n} \implies n\mid (c-1)(c+1)$ Now, I think we can assume that $n$ passed Fermat's theorem test (that's part of the initial tests of the Miller-Rabin algorithm). Hence, $$c^{r+1} = b^{2^l s} = b^{n-1} \equiv 1 \pmod {n}$$ but that isn't revealing anything new other than $r$ is odd  (since $2$ must divide $r+1$). What am I missing?","['number-theory', 'primality-test', 'prime-numbers', 'elementary-number-theory']"
2555359,"What is the possibility that at least one digit will not show up in a 20-digit ""code""?","A ""code"" is composed of 20 digits (numbers from 0 to 9), and we want to choose a number randomly. What is the possibility that at least one digit will not show up in the code? What I did: We have $10^{20}$ possibilities. 
Now, I want to choose 9 numbers out of the ten, and choose them randomly, so the possibility is: $\frac{10\cdot9^{20}}{10^{20}} = \frac{9^{20}}{10^{19}}$ But when I put this in the calculator, I get $1.25\dots$ I thought maybe I need a more precise calculator, but even calculators I found in google returns the same answer. The possibility isn't supposed to be above 1. What is the problem here?","['inclusion-exclusion', 'combinatorics', 'probability']"
2555422,"If $x,y\in E$ then $\frac{x+y}{2}\in E$. Prove that $E$ has an interior point","Let $E\subset \mathbb{R}$ be a set of positive Lebesgue measure. Assume that if $x,y\in E$ then $\frac{x+y}{2}\in E$. Prove that $E$ has at least one interior point. Here is what I have done: (1). By regularity, for any $\epsilon>0$ we can find an open set $O_\epsilon$ such that $E\subseteq O_\epsilon$ and $m(O_\epsilon)-m(E)<\epsilon.$ Write $O_\epsilon$ as a disjoint union of open intervals $\{I_j\}$
$$O_\epsilon=\bigsqcup_{j=1}^\infty I_j$$ (2). WLOG we can do the indexing in such a way that $I_{j+1}$ is the next interval to $I_j$ (in the sense that $I_{j+1}$ is on the right of $I_j$ and there is no $I_k$ which is in between $I_j$ and $I_{j+1}$.) (3). If at least one  $I_j\subseteq E$ then we are done. So assume that $I_j\subsetneq E$ for all $j$. Chose an $I_j$ and pick a point $x\in I_j\cap E$. Chose $y\in I_{j+1}\cap E$. Now $z=\frac{x+y}{2}\in E$ and thanks to the indexing, $z\in I_j$ or $z\in I_{j+1}.$  WLOG we can assume that $z\in I_j$. (4) Now we have two point $x,z\in I_j$. We can recursively pick the midpoints on the line joining $x$ and $z$ and all these points will be in $E$. (First pick $\frac{x+z}{2}$, then pick $\frac{x+\frac{x+z}{2}}{2}$ and $\frac{z+\frac{x+z}{2}}{2}$ and so on) (5). My guess is that one of the midpoints (constructed in the previous step) on the line joining $x$ and $z$ will be an interior point. But I don't know if my guess is correct. Am I moving in the right direction? Is there a different way to solve this problem?","['real-analysis', 'measure-theory']"
2555449,Algebraic tori over function fields,"Let $k$ be any field of characteristic zero, and $K$ be a finitely generated field extension of $k$ with transcendence degree 1 (i.e. a function field of one variable). Is it true that every 1-dimensional algebraic torus over $K$ is either $K^*$, or $SO(2,K)$? And what is the reference?","['algebraic-geometry', 'algebraic-number-theory', 'algebraic-curves', 'algebraic-groups', 'field-theory']"
2555453,Squares which are not 1 + a square in finite fields of odd characteristic?,"Given a finite field $F$ of odd characteristic, does there always exist an element $x$ of $F$ such that $1 + x^2$ is not a square in $F$? If so, can one even find a natural description of such an element, e.g. as a function of $\lvert F \rvert$ and a generator of $F^\times$?","['finite-fields', 'abstract-algebra', 'field-theory']"
2555463,Find the point equidistant from two points and a line,"Given a line $l$ and two points $p_1$ and $p_2$, identify the point $v$ which is equidistant from $l$, $p_1$, and $p_2$, assuming it exists. My idea is to: (1) identify the parabolas containing all points equidistant from each point and the line, then (2) intersect these parabolas. As $v$ is equidistant from all three and each parabola contains all points equidistant from $l$ and each point, the intersection of these parabolas must be $v$. However, I have had no luck in finding a way to compute, much less represent, these parabolas.","['analytic-geometry', 'geometry']"
2555488,Taking the limit of an implicitly defined function,"The original excercise was to solve the
$$\frac{y'(x)}{x}+2y(x)y'(x) \log(x)+\frac{y^2(x)}{x}-\frac{y(x)}{x^2}+1=0$$
Differential equation with $y(0)=0$.
The general solution is
$$\frac{y(x)}{x}+y^2(x) \log(x) + x =C$$
This is not defined in the $x=0$ point, but $C$ might be defined to met the $\lim \limits_{x \to 0} y(x)=0$ condition. So i took the limit of both sides as $(x,y) \to (0,0)$. $x \to 0$, $y^2 \log(x) \to 0$, but the limit of $\frac{y}{x}$ as $(x,y) \to (0,0)$ is path-dependent, so there is not any $C$ that could satisfy it. But I realized that $y$ is a function of $x$, so $y$ ""can't go in every way"" to the $0$; so the $C$ might exist. Is there a way to prove if there is a $C$ which can satisfy the limit, without transforming the equation into an explicit form?","['real-analysis', 'ordinary-differential-equations', 'limits']"
2555490,Convergence of square sum of normal random variables,"Given a sequence of i.i.d. random variables $(X_n)_{n \in \mathbb{N}}$ with distribution $N(0,1)$, for what does the sequence $${Y_n = \frac{X_1^2+ \ldots + X_n^2}{(X_1 - 1)^2 + \ldots + (X_n - 1)^2} \ , n \in \mathbb{N} \ ,
}$$
converges to? I saw a similar argument with the LLN being used for the geometric mean but can't see how to apply it in this case...","['probability-theory', 'random-variables']"
2555576,Does changing the order of double integration (both integral limits are constants) alter the final answer?,"I have studied that changing the order of double integration will not change the answer if both the limits of integration are constants . But this function is not agreeing with what I have studied:
$$1)\int_0^1\left(\int_0^1{\frac{x-y}{(x+y)^3}dy}\right)dx$$
$$2)\int_0^1\left(\int_0^1{\frac{x-y}{(x+y)^3}dx}\right)dy$$
The answer to the first integral is 0.5 and that of the second integral is -0.5 respectively. Can anyone please explain why is this so?","['integration', 'constants', 'calculus', 'limits']"
2555645,Is this an incorrect proof of $\cot (x)+\tan(x)=\csc(x)\sec(x)$?,"If you input the trig identity:
$$\cot (x)+\tan(x)=\csc(x)\sec(x)$$
Into WolframAlpha, it gives the following proof: Expand into basic trigonometric parts:
$$\frac{\cos(x)}{\sin(x)} + \frac{\sin(x)}{\cos(x)} \stackrel{?}{=} \frac{1}{\sin(x)\cos(x)}$$ 
Put over a common denominator: $$\frac{\cos^2(x)+\sin^2(x)}{\cos(x)\sin(x)} \stackrel{?}{=} \frac{1}{\sin(x)\cos(x)}$$ Use the Pythagorean identity $\cos^2(x)+\sin^2(x)=1$: $$\frac{1}{\sin(x)\cos(x)}  \stackrel{?}{=} \frac{1}{\sin(x)\cos(x)}$$ And finally simplify into $$1\stackrel{?}{=} 1$$ The left and right side are identical, so the identity has been verified. However, I take some issue with this. All this is doing is manipulating a statement that we don't know the veracity of into a true statement. And I've learned that any false statement can prove any true statement, so if this identity was wrong you could also reduce it to a true statement. Obviously, this proof can be easily adapted into a proof by simply manipulating one side into the other, but: Is this proof correct on its own? And can the steps WolframAlpha takes be justified, or is it completely wrong?","['algebra-precalculus', 'trigonometry', 'logic', 'wolfram-alpha']"
2555658,References for this theorem,"Here is the statement (it is a sort of a division theorem) : Let $f\in \mathcal{C}^{\infty}(\mathbb{R})$ such that $f(0)=0$  and
  $g:\mathbb{R^*}\to \mathbb{R},\ x\mapsto \frac{f(x)}{x}$ and
  $g(0)=f'(0)$. Then $g\in\mathcal{C}^{\infty}(\mathbb{R})$ and $\forall
 n \in \mathbb{N}$ : $g^{(n)}(0)=\frac{f^{(n+1)}(0)}{n+1}$. I was wondering if there are references for this statement ? I just found the statement as an exercise on the internet. Thanks in advance !","['derivatives', 'reference-request', 'taylor-expansion']"
2555669,Estimate from below of the sine (and from above of cosine),"I'm trying to do the following exercise with no success. I'm asked to prove that $$\sin(x) \ge x-\frac{x^3}{2}\,, \qquad \forall x\in [0,1]$$ By using Taylor's expansion, it's basically immediate that one has the better estimate $$\sin(x) \ge x-\frac{x^3}{6}\,, \qquad \forall x\in [0,1]$$ as the tail converges absolutely, and one can check that the difference of consecutive terms is positive. I suppose then, there is a more elementary way to get the first one. Question is: how? Relatedly, the same exercise asks me to prove that $$\cos(x) \le \frac{1}{\sqrt{1+x^2}}\,,\qquad \forall x\in [0,1]$$ which again I can prove by using differentiation techniques. But these haven't been explained at that point of the text, so I wonder how to do it ""elementarly"".","['real-analysis', 'inequality', 'trigonometry', 'calculus', 'analysis']"
2555678,"How to determine $n$, such that $x\uparrow \uparrow n>10^{100}$?","If $x$ is a real number greater than $e^{e^{-1}}$ , then $x\uparrow \uparrow n$ (A power tower of $n$ $x's$) tends to $\infty$, if $n$ tends to $\infty$. Therefore, there must be a number $n$, such that $x\uparrow\uparrow n>10^{100}$ Can I determine the smallest number $n$ satisfying this inequality without applying the iteration $x_1=x$ , $x_{n+1}=x^{x_n}$ ? For example, for $\color\red {x=e^{e^{-1}}+10^{-10}}$, we have $\color\green {n=323\ 892}$","['big-numbers', 'tetration', 'calculus']"
2555716,deducing Bott vanishing,"In the book of Okonek et al. on vector bundles it is suggested as an exercise to derive the dimensions of cohomology $H^q(\mathbb{P}^n, \Omega^p)$, using Euler sequence and Serre duality, from the vanishing of
$H^q(\mathbb{P^n}, \mathcal{O}_{\mathbb{P}^n})$ when $q > 0$. The latter is claimed to hold, with a reference to the book of Banica and Stanasila, through ""a clever Laurent separation"". Now, while this cohomology vanishing can be deduced over the complex numbers via Serre duality and computation of Hodge numbers for $\mathbb{P}^n$, I am curios as to what could have been meant by this ""Laurent separation"" trick, and if the argument implied is independent of Hodge theory. As there is no precise reference in the Banica and Stanasila's book, I am at a loss. Any guesses at what this remark could mean?","['sheaf-cohomology', 'projective-space', 'algebraic-geometry']"
2555749,Why does $\bigoplus_{i\in\kappa}\Bbb Z$ have cardinality $\kappa$ for an infinite $\kappa$?,"I was reading this post Does every set have a group structure? and I'd like some clarifications if possible. If $X$ is uncountable, by the Axiom of Choice we'll have $|X| = \kappa$. I'm not sure why the direct sum $\bigoplus_{i \in \kappa} \mathbb{Z}$ has cardinality $\kappa$ and why that would make $X$ into a group. Any set you put in bijection with a group can be made into a group, but I'm not seeing with the direct sum.","['cardinals', 'group-theory', 'elementary-set-theory']"
2555784,Is the probability $\frac12$?,"I have $n$ players in total. Note that $n$ is even. We want to pick $\frac n2$ players uniformly at random. We have access to only one unbiased coin. We want to make this bisection in minimum expected number of coin tosses. What should I do? My approach: I will keep tossing-if it is H I will add the player to team A else to team B. If there are already $\frac n2$ players in any team, I stop tossing and put the rest in other team. Leaving aside the problem of proving why this will give minimum number of tosses, I am not sure why the players have equal probability of getting to team A or B. It is clear for the first $\frac n2$ players but after that it gets a little messy. Please don't say that it is symmetric so probability is half trivially.","['combinatorics', 'probability']"
2555807,Limit of: $\lim\limits_{x\to \infty}\left(\frac{1}{n}\sum_{k=1}^{n} k^{1/x}\right)^{nx}$ [duplicate],"This question already has answers here : whats the proof for $\lim_{x → 0} [(a_1^x + a_2^x + .....+ a_n^x)/n]^{1/x} = (a_1.a_2....a_n)^{1/n}$ (4 answers) Closed 6 years ago . Compute the following limit ($n\in \mathbb{N}$)
$$\lim_{x\to \infty}\left(\frac{1}{n}\sum_{k=1}^{n} k^{1/x}\right)^{nx}$$ My idea was to use the inequality: $$\left(\frac{1}{n}\sum_{k=1}^{n} 1^{1/x}\right)^{nx}<\left(\frac{1}{n}\sum_{k=1}^{n} k^{1/x}\right)^{nx}<\left(\frac{1}{n}\sum_{k=1}^{n} n^{1/x}\right)^{nx} \\
\implies1<L<n^n$$ This gives that the required limit $L$ lies between $1$ and $n^n$. But how can we find its value?","['real-analysis', 'sequences-and-series', 'limits']"
2555845,A light can only be turned on if there is a light next to it turned on. In how many ways can you turn on all the lights?,"There are $n$ lights(numbered from $1$ to $n$), some of them are turned on. A turned off light can only be turned on if there is a light next to it turned on.
In how many ways can you turn on all the lights? For example:
$$n=5$$
and the light number $3$ is on.
$$0 0 1 0 0$$ answer is 6. 6 ways 43*12 32*14 42*13 41*23 31*24 21*34 The * means the initial turned on light. The numbers indicate the order in which the  lights were turned on I have tried recursively building the solution. But it is not time feasible. A turned on light never goes off.","['permutations', 'combinatorics', 'binomial-coefficients', 'combinations']"
2555874,Proof of expected value to reach $0$ or $n$ in gambler's ruin,"Consider a gambler betting on the outcome of a sequence of independent fair
coin tosses. If the coin comes up heads, she adds one dollar to her purse; if the coin lands tails up, she loses one dollar. If she ever reaches a fortune of $n$ dollars, she will stop playing. If her purse is ever empty, then she must stop betting. Now the question is to find the expected time when the gambler will stop betting. i.e. Either she reaches $0$ dollars or $n$ dollars. We will assume that initially she has $k$ ($0<k<n$) dollars. Let $f_k$ be the expected time to reach $n$ or $0$ dollars starting from $k$ dollars. Therefore $f_k = \frac{1}{2}\cdot(1 +  f_{k+1}) + \frac{1}{2}\cdot(1 +  f_{k-1})$   . Intuitively, this equation is clear to me. Because with probability $\frac{1}{2}$, the fortune either increases or decreases and we add $1$ because it takes time equal to $1$ unit for fortune to increase or decrease by $1$. Can someone help to derive this equation mathematically without any intuition. I want a rigorous proof.","['random-walk', 'probability-theory', 'stopping-times']"
2555914,The dual $H'$ of a Hilbert Space $H$ is a Hilbert Space,"Let H be a Hilbert Space. Show that the dual space $H'$ of $H$ is a Hilbert Space with inner product $\langle \cdot, \cdot \rangle_1$ defined by $$ \langle f_z , f_v \rangle_1 = \overline{ \langle z,v\rangle}=\langle v,z\rangle,$$ where $f_z(x)=\langle x,z\rangle$ , with $\langle \cdot, \cdot\rangle$ is the inner product in $H$ . I have already shown that $\langle\cdot , \cdot\rangle_1$ is inner product. Now I need to prove that $H'$ is complete, so I started this way: we have that $H'$ is an inner product space and the metric $d:H'\times H'\rightarrow \mathbb{R}$ defined by $$d(f_z, f_v)= \sqrt{\langle f_z -f_v,f_z -f_v\rangle_1}.$$ Let $({f_z}_n)_{n\in\mathbb{N}}$ be a arbitrary Cauchy sequence in $H'$ , that is $$\forall \epsilon >0, \: \exists \: n_0 \in \mathbb{N} \:;\: m,n>n_0 \: \Rightarrow \: d({f_z}_n,{f_z}_m)<\epsilon.$$ I can write that ${f_z}_n=\langle x,z_n\rangle$ ? Where $(z_n)_{n\in\mathbb{N}}$ is a sequence in $H$ . How to continue to prove that such a sequence converges?","['functional-analysis', 'hilbert-spaces']"
2555921,"The positive-definiteness of the special matrix created by shifting the vector $[1\, \cdots \, n\, \cdots\,1]$","I am wondering if there is a good way to prove if matrices with the following structure are positive definite:
\begin{equation}
\left[
\begin{array}{ccccccc}
10 &9 &8 &\cdots &3 &2 &1\\
9  &10 &9 &\cdots &4 &3 &2\\
8 &9 &10 &\cdots &5 &4 &3\\
\vdots &\vdots &\vdots &\ddots &\vdots &\vdots &\vdots \\
3 &4 &5 &\cdots &10 &9 &8\\
2 &3 &4 &\cdots &9 &10 &9\\
1 &2 &3 &\cdots &8 &9 &10
\end{array}
\right]
\end{equation}
Basically, each row is created by shifting the vector $[1\, \cdots \, n\, \cdots\,1]$. Any help is greatly appreciated, thanks so much!","['matrices', 'positive-definite', 'linear-algebra']"
2555945,"Given a local field complete with respect to a discrete valuation, does that imply it is non-Archimedean?",At the beginning of the section of a book I am reading it says $K$ is a local field complete with respect to a discrete valuation. Local fields can be archimedean or non-archimedean. Does the condition that it has a discrete valuation imply we are only dealing with non-archimedean local fields? Thank you very much.,"['valuation-theory', 'abstract-algebra', 'local-rings', 'local-field']"
2555948,Is there a standard way to find the subfields of $\Bbb{Q}(\zeta_n)$ when $n$ is not prime?,"For $n$ prime, there seem to be standard ways of determining the subfields. This question gives such a method. However, this method does not necessarily work for $n$ not prime, and the method that I have employed seems very ad hoc. Essentially, my method has just been to determine the Galois group and find its subgroups, look to see if any roots are fixed or cycle through each other, and then trying adding or multiplying those roots and checking if the minimal polynomials are the same as the degree of the extensions. An Example For example, consider $\zeta = \zeta_{18}$ . I know that $\Phi_{18} = x^6-x^3+1$ , so $\Bbb{Q}(\zeta)$ is of degree $6$ over $\Bbb{Q}$ . Further, if we let $\sigma(\zeta) = \zeta^5$ , we can completely classify the Galois group of $\Bbb{Q}(\zeta)$ : $$\operatorname{Gal}(\Bbb{Q}(\zeta)/\Bbb{Q})= \{e,\sigma, \sigma^2,\sigma^3,\sigma^4,\sigma^5\} \simeq \Bbb{Z}_6.$$ Then I know that by the fundamental theorem, there should be subfields corresponding to each of the following subgroups: \begin{align*}
H_1 &= \{e\}, \\
H_2 &= \langle \sigma_3 \rangle = \{e,\sigma^3\}, \\
H_3 &= \langle \sigma^2\rangle = \{e,\sigma^2, \sigma^4\},\\
H_4 &= \operatorname{Gal}(\Bbb{Q}(\zeta)/\Bbb{Q}).
\end{align*} So obviously the fixed field of $H_1$ is $\Bbb{Q}(\zeta)$ and the fixed field of $H_4$ is $\Bbb{Q}$ , so I just need to figure out the fixed fields of $H_2$ and $H_3$ . So I noticed that $\sigma^3$ takes $\zeta \mapsto \zeta^{-1} \mapsto \zeta$ , $\zeta^5\mapsto \zeta^{-5} \mapsto \zeta^5$ , and $\zeta^7\mapsto \zeta^{-7} \mapsto \zeta^7$ , so clearly $H_2$ fixes $\zeta+\zeta^{-1}$ , $\zeta^5+\zeta^{-5}$ , and $\zeta^7+\zeta^{-7}$ . Here's where I encounter my first question. By looking on Wolfram I know that each of these values have the same minimal polynomial, namely $x^3-3x-1$ , and this is the correct degree for the extension we are looking for. Does this then mean that the fixed field MUST be $\Bbb{Q}(\zeta + \zeta^{-1})$ ? (Side question: to figure out the above, I relied on Wolfram to tell me the minimal polynomial. Is there a way of figuring out the minimal polynomial of such ugly numbers by hand?) If we now consider $H_3$ , I noticed that both $\sigma^2$ and $\sigma^4$ cycle $\zeta, \zeta^7,\zeta^{-5}$ to each other, and they cycle $\zeta^{-1},\zeta^5,\zeta^{-7}$ to each other. So my first guess was that the fixed field must be $\Bbb{Q}(\zeta+\zeta^7+\zeta^{-5})$ , but checking Wolfram tells me that $\zeta+\zeta^7+\zeta^{-5} = 0,$ so clearly this doesn't work. (Side question: is there a way I would have been able to tell this was equal to $0$ without using Wolfram?) So I then guessed that maybe it was $\zeta\zeta^7\zeta^{-5} = e^{\frac{\pi i}{3}}$ . This has a minimal polynomial of degree 2, which matches the degree we want for the extension over $\Bbb{Q}$ . So again, does this mean that this is the extension we want? Summary Clearly the method I have employed to solve this type of problem is quite ad hoc. So is there a better way of determining the subfields of such an extension other than using the correspondence and just kind of guessing and checking?","['abstract-algebra', 'galois-theory', 'roots-of-unity', 'cyclotomic-fields']"
2555964,Cardinality of a supersingular hyperelliptic curve of genus $2$ over $\mathbb{F}_q$ (q square),"I am trying to calculate the cardinality of a genus $g=2$ curve under certain hypotheses, but I do not know if the information I have is enough to infer this number or a bound for it (without using the Hasse inequality or other fancy bounds). Let $H/\mathbb{F}_q$ be a hyperelliptic curve of genus $2$ and $J$ its Jacobian. Suppose that there is an $n\in\mathbb{Z}$ such that for $\phi,[n]\in\text{End}(J)$ we have that $\phi+[n]=0$ where $\phi$ is the $q$-Frobenius Endomorphism. What I want is to calculate the number of points of $\#H(\mathbb{F}_q)=?$ directly without using a fancy inequality (like Hasse-Weil). I have tried the following: Since $\phi=-[n]\in\text{End}(J)$ we have that $\deg\phi=\deg [n]$, therefore $q^g = n^{2g}$ which in our case implies that $q^2 = n^4$. Therefore $q=n^2=(p^k)^2$ is a square. Also we have that $\phi$ satisfies its characteristic polynomial $\chi_\phi(X)$ on the Tate module and since $\phi=[-n]=[p^k]$ we have that $\chi_\phi(X)=(X+p^k)^4$. This $\textit{smells}$ to a supersingular curve $H$. I think that using the previous, I can infer that $J[q]=\{0\}$ (please correct me). And if this is true then $q|\#H(\mathbb{F}_q)-1$. Also an easy counting argument tells me that $\#J(\mathbb{F}_q)=\frac{\#H(\mathbb{F}_q)^2+\#H(\mathbb{F}_{q^2})}{2}-q$ which can be found in Cassels and Flynn page 80, therefore: $\#J(\mathbb{F}_q)=\chi_\phi(1)=(1+p^k)^4=\frac{(\#H(\mathbb{F}_q)^2+\#H(\mathbb{F}_{q^2})}{2}-q$. I do not know where to go next, Is it possible to get $\#H(\mathbb{F}_q)$ with this information? or at least a bound ?","['finite-fields', 'algebraic-geometry', 'number-theory', 'elliptic-curves', 'algebraic-curves']"
2555991,Invariant: Differential Equations.,"I came across a question. Show: $A = x^2 + y^2 $ and $B=x^2+z^2$ . Are invariants of the nonlinear system: $x'(t)=y(t)z(t) .    
y'(t) = -x(t)z(t) .   
z'(t) = -x(t)y(t) . 
$ Now I know the solution is: $A′ = 2xx′ + 2yy′ = 2xyz − 2xyz = 0$ . and $B′ = 2xx′ + 2zz′ = 0.$ My question is: a) What is the significance of invariant solutions? b) Is there anywhere in ordinary differential equations that invariant solutions become useful? c) Does invariance have any influence on existence theory?",['ordinary-differential-equations']
2555997,Event-based rather than flow-based systems?,"This is a very vague question, so I don’t know if people will understand it (I’m not even sure I understand the question myself), but here it goes: Dynamical systems are formulated (in continuous time) as
$$\begin {align}
&\dot x_1=f_1(x_1,...,x_n)\\
&...\\
&\dot x_n=f_n(x_1,...,x_n)
\end {align}$$ These are flows over time . In other words, we have a stock for each variable, and a specified flow at each point in time, from which we will deduce the stock over time. Situations in applied science that are captured well by this system are for example: There is an amount of water $x$ in a cup, and we know that every second, an amount $a$ drips through a hole, so that $\dot x=a$. An object is moving with $x$ meters/s towards the earth, and we know that every second, $x$ increases by $9.8\,\mathrm{m}$. We can of course do this in discrete time as well, which doesn’t fundamentally change the idea:
$$\begin {align}
&\Delta x_1=f_1(x_1,...,x_n)\\
&...\\
&\Delta x_n=f_n(x_1,...,x_n)
\end {align}$$ However, sometimes our logical analysis in an applied setting doesn’t start with knowledge of a flow per unit time, but with knowledge of events . For example, in biology, a birth can occur, which would cause various things to happen (among other things, an increase of the population by $1$). As we do in most of science, we also generally implement births in population models as a flow. Instead of starting with a formal definition of a birth as an event, we simply state that in a given time interval, the population increases. In other words, the analysis starts with the variable population, and with a formula denoting its change over time. In fact the whole concept of a birth, being an event, never enters the mathematical model, and is only a kind of informal interpretation that we stick to the model. I am wondering whether this informal leap from the event of a birth to a flow can somehow be formalized. So my questions are: Is there a (sub)branch of mathematics that starts its analysis with a formalization of an event, rather than of a flow? Is there any mathematical analysis of events at all?
I tend to think of events as function calls in the programming sense, i.e., when a birth occurs, a “birth” function is called and this causes various variables to change, and possibly other events.","['ordinary-differential-equations', 'dynamical-systems', 'analysis', 'discrete-mathematics']"
2555998,Closed geodesic on space diffeomorphic to $S^{2n}$ with positive sectional curvature always contains a conjugate point.,"For reference; a closed geodesic is a geodesic that is a closed loop that is smooth at the origin. I have been stuck on this for a few days; the only theorem I have relating to even dimension has to do with orientability, so it doesn't seem like there's anything I will be able to do with that. The fact that it is an even dimensional sphere makes it seem like I may have to use the fact that there is no non-vanishing vector field, but I am not sure how to make this work. Is it possible I can have some hints?","['riemannian-geometry', 'differential-geometry', 'geodesic']"
2556026,Key Results of Combinatorial Group Theory.,"What are the key theorems of combinatorial group theory ? By ""key theorems"", I mean those most commonly used in the literature. For added context, I have copies of ""Presentation of Groups,"" by D. L. Johnson and ""Combinatorial Group Theory: Presentations of Groups in Terms of Generators and Relations, "" by Magnus et al. , and I've just started a Ph.D. in the area. I suppose a good place to start would be Theorem (Nielson-Shreier Theorem): Every subgroup of a free group is itself free. Update: I've got a copy of Lyndon & Schupp's ""Combinatorial Group Theory"" .","['combinatorial-group-theory', 'abstract-algebra', 'big-list', 'group-theory']"
2556046,"If $z=\tan(\frac{x}{2})$, show that $\sin(x)=\frac{2z}{1+z^2}$","So as the title states I've got the following problem: If $z=\tan(\frac{x}{2})$, show that $\sin(x)=\frac{2z}{1+z^2}$ So I'd guess that this probably involves the formula for half-angles, but that is is a dead-end. Any suggestions? Thank you in advance!","['trigonometry', 'implicit-differentiation']"
2556120,Pointwise Limit of a Sequence of Measurable Functions,"Let $X$ be a measurable space and $Y$ a topological space. I am trying to show that if $f_n : X \to Y$ is measurable for each $n$, and the pointwise limit of $\{f_n\}$ exists, then $f(x) = \lim_{n \to \infty} f_n(x)$ is a measurable function. Let $V$ be some open set in $Y$. I was able to show that $\bigcup_{N=1}^\infty \bigcap_{n \ge N} f^{-1}_n(V)$ is contained in $f^{-1}(V)$ but not the other set inclusion. I could use some help.","['complex-analysis', 'real-analysis', 'measure-theory', 'pointwise-convergence']"
2556166,"What characteristics do functions have, where $f(x,y) = -f(y,x)$?",Edited the question. One commenter said these functions are antisymmetric. Does that mean they're not symmetric? Symmetric to what exactly? What are some general characteristics.,"['algebra-precalculus', 'multivariable-calculus', 'calculus']"
2556169,"Need a *trivial* proof of an ""obvious"" combinatorial result","I've just presented Enflo's theorem on the existence of a Banach space without an approximation property in my Functional Analysis class. The argument is not trivial by itself, but in order to emphasize the really interesting steps, I'd like to dismiss all the ""obvious"" lemmata in the most efficient way. I have cleaned up the end quite a bit (if somebody is interested, the right choice of $k_m$ and $t_m$ is $t_{m+1}\in [t_1t_2\dots t_m,4t_1t_2\dots t_m]$ and $k_m=t_1t_2\dots t_{m-1}t_{m+1}$, which ensures that all interesting ratios are integers, so we can do a perfect fit without any dirty tail bounds and the corresponding estimates; also it is easier to use just random horizontal partitions instead of smart number-theoretic definitions). However some unpleasant pieces remain. The one that irritates me most is the following: Consider all $n-1$-element subsets $I$ of $\{1,2,\dots,2n\}$ and $m\in[1,2n-1]$. Let $N_o$ and $N_e$ be the numbers of $I$'s having odd and even number of elements in $\{1,\dots,m\}$ respectively. Then 
$$
|N_o-N_e|\le c_n(N_o+N_e)=c_n{2n\choose n-1}
$$
with some $c_n$ depending on $n$ only (so it should serve all $m$ simultaneously) such that $\sum_n \frac{c_n}n<+\infty$. The sharp bound is, of course, $c_n=\frac 1n$ and Enflo gets it considering $m=1,2$ separately and using a trigonometric integral representation and Holder inequality for other $m$, on which I wasted about 35 minutes of the class time, but this should be just one-liner (OK, perhaps, three) presentable in under 10 minutes (preferably in under 5). Note that I don't care about sharp $c_n$ as long as the series above converges and will gladly trade its size for any simplification in the proof. So can we make this ""obvious"" fact formally obvious? I'm asking here rather than on MO because it is an education question rather than a research one :-)",['combinatorics']
2556222,Multinomial Coefficient example,"A teacher will divide her class of 17 students into four groups to work on projects. One group will have 5 students and the other three groups will have 4 students. How many ways can students be assigned to group if: All group work on the same project? since there's no order i think the right answer would be : $$ \frac{\binom{17}{5 ,4,4,4}}{4!}$$ But in my notes the professor said $$ \frac{\binom{17}{5 ,4,4,4}}{3!}$$ I don't see any particular reason. Maybe it is because we're trying to make the 3 groups of four look the same ? Is a multinomial coefficients used to distribute those 17 people with taking in account that the 4 groups are different, and dividing that by 3! means we can arrange those groups in 3! ways ?","['permutations', 'combinatorics', 'multinomial-coefficients']"
2556233,Differential equation in optics,"While playing with the eikonal equation (classical optics) I stumbled this differential equation :
$$
y y'' = k\left((y')^2 + 1\right)
$$
where $y=y(x)$ should be defined for $x>0$, $y>0$ and where $k\in \mathbb Z$. I don't know if there is a general solution $y(x)$ to that problem for a given $k$. What I know is that solving this for : $k=0$ is easy $k=1$ gives something like $y(x)=\cosh(x)$ Question : Is there a known solution for $k\in \mathbb Z \backslash \{0,1\}$ ? Remark 1 : The general solution for $k=1$ is, as asked, $y(x)=y_0 \cosh((x-x_0)/y_0)$ for some constants $x_0,y_0$. Remark 2 : The case $k=-1$ is solved below.","['real-analysis', 'ordinary-differential-equations']"
2556250,"Distinct solutions of $f(z) - w_0 = 0$ for $f$ holomorphic, $w_0$ in a punctured neighborhood of $f(z_0) = 0$","Problem: Let $f:\Omega\to\Bbb C$ be a holomorphic function ( $\Omega$ is open), and $f$ has a zero of order $k$ at $z_0\in\Omega.$ Show that there is a neighborhood $U$ of $z_0$ and a neighborhood $V$ of $f(z_0)$ such that if $w_0 \in V-\{f(z_0)\},$ then $f(z)-w_0=0$ has $k$ distinct roots in $U - \{z_0 \}.$ Attempt at solution: I was thinking about showing that there are $z_1,\ldots,z_k$ such that $f(z_i)=w_0$ and $f'(z_i) = (f(z_i) - w_0)'\ne 0,$ thus, showing that there are $k$ distinct solutions to the equation. Maybe I can make use of the Cauchy integral formula and $f(z_0) = 0$ and $w_0$ being near $f(z_0)$ to show that the derivatives are indeed nonzero, however, I am having trouble making this intuition precise and actually finding the $k$ solutions. Any help is appreciated!","['complex-analysis', 'roots']"
2556339,Calculate the derivative using limit definition.,"This is the function $f(x)$$=\frac{1}{\sqrt{3x-2}}$ .
I wrote that $$\lim_{h\to 0}\frac{\frac{\sqrt{3x+3h-2}}{3x+3h-2}-\frac{\sqrt{3x-2}}{3x-2}}{h}.$$
I am not able to continue further.","['derivatives', 'calculus']"
2556388,"Can L, the square lattice on the plane, be partitioned into finitely many subsets that (up to translation) are contained in a rotated version of L?","I ran into this interesting problem while thinking about some stats models the other day. The context was efficient estimation of anisotropic covariance structures in geostatistical models, but it appears to be more of a geometry or number theory problem. This is really not my area, so please forgive any abuse of notation: Let $L_{(0,1)}$ be a square lattice in the Euclidean plane with unit spacing and let $L_{(\theta,d)}$ be the lattice obtained by rotating $L_{(0,1)}$ by angle $\theta$ and scaling (increasing the spacing) by factor $d\geq1$. Now suppose we partition $L_{(0,1)}$ into subsets that, up to translation, are perfectly aligned with $L_{(\theta,d)}$. To make this precise, let $S(\theta,d)$ be the set of partitions $P = \left\lbrace L_1, L_2, ... \right\rbrace$ of $L_{(0,1)}$ that satisfy:
$$
\forall L_i \in P, \quad \exists \quad (x_i, y_i) \in R^2 \quad \text{such that} \quad L_i + (x_i, y_i) \subseteq L_{(\theta,d)}
$$ The set $S(\theta,d)$ is nonempty, since it always contains the partition of singletons (which can be shuffled around however we like). Sometimes it also contains partitions of finite cardinality. Symmetry in the square lattice provides some easy examples: with a right angle and no scaling we can just use the trivial partition,
$$
\left\lbrace  L_{(0,1)} \right\rbrace \in S \left( \frac{\pi}{2},1 \right),
$$ and when $\theta$ is a multiple of $\pi/4$ we can scale by $\sqrt{2}$ and get a partition of cardinality 2,
$$
\left\lbrace \left\lbrace (i,j) \in L_{(0,1)} \mid |i-j| \text{ odd} \right\rbrace,   \left\lbrace (i,j) \in L_{(0,1)} \mid |i-j| \text{ even} \right\rbrace \right\rbrace \in S \left( \frac{\pi}{4},\sqrt{2} \right).
$$ I would like to know in general when $S(\theta,d)$ will contain a partition of finite cardinality (and for my application, I'm really interested in special angles for which we get partitions of low cardinality, say less than 10). Any insights or references to help me better understand the problem are appreciated, but I do have these specific questions: Is there any theory that concisely characterizes the relationship between $(\theta$, $d)$ and $S(\theta,d)$? When does a partition of finite cardinality exist? When does a partition of minimal cardinality exist? Is it unique? Is it easy to find? My intuition is that if $\theta = \arctan(p/q)$ for integers $p$ and $q$ then the set
$$
S \left( \theta,\sqrt{p^2 + q^2} \right) 
$$
should contain a partition of cardinality $p^2 + q^2$, and this would be minimal. But I'm really not sure if this is true, nor how to approach the problem rigorously. Thanks! Here is an example diagram for the case $p=2$, $q=1$. The dotted blue lines are $L_{(0,1)}$ and the dashed green is $L_{(\theta,\sqrt{5})}$. The green box is the rotated and scaled version of the blue box. I count $5 = 1^2 + 2^2$ points (one blue and four green) representing subsets of $L_{(0,1)}$ (together, a partition) that can be translated to lie on the lattice $L_{(\theta,\sqrt{5})}$.","['number-theory', 'multigrid', 'statistics', 'geometry']"
2556420,The order of a modulo p.,"If $p$is prime and $\operatorname{ord}_p(a)=4$, then $1+a+a^2+a^3≡ 0\bmod p$, where $\operatorname{ord}_p(a)$ is the order of $a$ modulo $p$. I think it is true statement $\operatorname{ord}_p(a)=4$, then $a^4≡1\bmod p$ so $a^4-1≡0\bmod p$ since $a^4-1=1+a+a^2+a^3$, then $a^4-1=1+a+a^2+a^3\equiv0\bmod p$ is that correct please?","['number-theory', 'elementary-number-theory']"
2556441,"""Chains"" of Dual Spaces","I have seen examples of the following: A sequence of duals extending forever$$S \subset S^*  \subset S^{**}  \subset S^{***} \subset ... \\ S = L^0 $$ A sequence of duals flipping between 2 spaces
$$S \subset S^*  \subset S  \subset S^{*} \subset ... \\ S = L^p $$ A sequence of duals which is isometric to itself
$$S = S^* \\ S = H $$ Are there sequences of duals that ""return"" every third space or something? Is there some ""deep"" algebraic fact that these are the only 3 possibilities?","['functional-analysis', 'soft-question']"
2556445,Find the mapping such that the given region is mapped onto a rectangle,"I want to find a continuously differentiable and one-to-one mapping from the first quadrant of $\Bbb{R}^2$ to itself such that the region bounded by $x^2\le y\le 2x^2$ and $1\le xy \le 3$ is mapped to a rectangle. Is there some systematic way to find such mappings? Right now I am just guessing and it is not very effective. In other words what change of variables $(x,y)\to(u,v)$ will change the curve of $y=ax^2$ to $y= C_1$ and $y=b/x$ to $x=C_2$ for some constants $C_1,C_2$. Is the best way just by inspection?",['multivariable-calculus']
2556449,Clamped Cubic Spline : interpolates f?,"Given the partition $x_0=0,x_1=.05,x_2=.1 \text{ of } [0,0.1] \text{ and } f(x) = e^{2x}: \\ \text{Find the cubic spline s with clamped boundary condition that interpolates f.} $ The first thing I did, to solve this question is create a table with $x,f(x),f'(x)$ and try to find x. $$
\begin{array}{c|lcr}
x & \text{f(x)} & \text{f'(x)} & \\
\hline
0 & 1& 2  \\
.05 & 1.1052 & 2.2103\\
.1 & 1.2214 & 2.4428
\end{array}
$$ Then I tried to find matrix A and matrix B then solve $AX=B$ but this is where I ran into some problems. $h_0= .05-0=.05 ;h_1= .1-.05=.05$ $$A= 
    \begin{bmatrix}
    2(.05) & .05 & 0 \\
    .05 & 2(.05+.05) & .05 \\
    0 & .05 & 2(.05+.05) \\
    \end{bmatrix} =  \begin{bmatrix}
    .1 & .05 & 0 \\
    .05 & .2 & .05 \\
    0 & .05 & .2 \\
    \end{bmatrix}
$$ $$b=  \begin{bmatrix}
    \frac{3}{.05}(1.105-1)-3(2)=.312 &  \\
    \frac{3}{.05}(1.12214-1)-\frac{3}{.05}(1.1052-1)=1.0164 &  \\
    3(2.4428)-\frac{3}{.05}(1.2214-1.1052=-.35651 &  \\
    \end{bmatrix} $$ The answer is supposed to be $$x=  \begin{bmatrix}
   1.998302 &  \\
    2.208498&  \\
    2.4406449  \\
    \end{bmatrix} $$ Yet when I do $Ax=b$ I get $$x=  \begin{bmatrix}
   .748&  \\
    4.7&  \\
    .59\\
    \end{bmatrix} $$ Does anyone know how to solve this?","['numerical-methods', 'algorithms', 'linear-algebra', 'analysis']"
2556516,Limit of Square Root of a Tridiagonal Matrix,"Consider a tridiagonal $n\times n$ matrix with the following format $$ \mathbf A = \begin{bmatrix}
a & b &        & \phantom{\ddots} & \phantom{\ddots}    \\
b & a & b      & \phantom{\ddots} &     \\
  & b & a      & \ddots           &     \\
  &   & \ddots & \ddots           & b   \\
\phantom{\ddots}  & \phantom{\ddots}  &  \phantom{\ddots}      & b                & a   \\
\end{bmatrix}.$$ This matrix has eigenvalues equal to $$\lambda_k = a-2b\cos\left(\frac{k\pi}{1+n}\right),\;\text{ for }\;k\in\{1,\dots,n\}.$$  It follows that $$\lim_{n\to\infty}\lambda_k=a-2b.$$ It is also symmetric which mean it can be diagonalized into $$\mathbf A= \mathbf Q \mathbf \Lambda \mathbf Q^T,\;\text{ so that  }\;\mathbf A^{1/2}= \mathbf Q \mathbf \Lambda^{1/2} \mathbf Q^T.$$ Is it possible to obtain an analitical formula for the elements of $\lim_{n\to\infty}\mathbf A^{1/2}$? Edit: This paper on tridiagonal Toeplitz matrices appears to be helpful .","['matrices', 'linear-algebra']"
2556531,Negative Length: is there a specific reason that a length cannot be negative?,"As I was studying trigonometric functions, I had to use the Pythagorean Theorem to find the length of the hypotenuse in order to solve one of the functions. This got me thinking though: normally when you take a square root of the number, you have to take the positive and negative square root; but lengths of objects, such as the side of a triangle, you only take the positive square root, because a length cannot be negative. But then again, can it? I learned this theorem back in middle school, and I just took that rule at face value all these years because it made perfect sense; an object can't have a negative length. Now it seems almost counter-intuitive to think that negative length is a possibility, but then again, so was the notion that you could have the square root of a negative number or even the square root of two. So my question is this: is there a specific reason that a length cannot be negative, besides it just making sense? Is there a proof?","['terminology', 'metric-spaces', 'geometry']"
2556537,Definition Unitary Group,"In understanding unitary group, i get confused because there are several definition of unitary group, first, in here: Sven Grützmacher Let A matrix and define $A^{*}=\bar{A}^{T}$, Then we can define the unitary group, $U(n)=\{M \in M_n \mathbb{(C)} | M^*M=I\}$ Then, $U(p,q)=\{M \in M_n \mathbb{(C)} | M^*I_{p,q}M=I_{p,q} \}$ is the indefinite unitary group of signature $(p,q)$, where $p+q=n$. But, in here: Order of finite unitary group $U_n(q)= \{ M \in GL_n(F_{q^{2}}) | MM^*=I_n \}$ Also, from the above link and the book ""The Subgroup Structure of The Finite Classical Groups"", known the order of finite unitary group to be: $q^{(n^2-n)/2} \prod _{k=1} ^{n} (q^k - (-1)^k)$. So, if we take example of unitary group $U(3,5)$, according to the first link, the member of the group is $8\times8$ matrices? and according to the second link, the member of the group is $3\times3$ matrices over Finite Field in a of size 5^2? and when i used GAP, the order of U(3,5) is 126000, but with sage, is 228000. And, by manual counting, with $n=3$ and $q=5$, got 228000. Would you give me clear definition of unitary group, definite and indefinite unitary group? And, in paper ""Unitals, projective planes and other combinatorial structures constructed from the unitary groups $U(3,q), q=3,4,5,7$"", for q=5, which one is used? Because, in the paper, $U(3,5)$ have order of 126000, how they get it? only using GAP? how about the manual? note that: i have not studied lie group yet.","['finite-groups', 'group-theory']"
2556615,How to find $\int \frac{e^{-x^2}}{x^2 + 1} dx$?,"I have a question about improper integrals: How can we find $\lim_{n \rightarrow +\infty}\int_{-n}^{n} \frac{1 - e^{-nx^2}}{x^2(1+nx^2)}dx$? $\textbf{Some effort:}$ $\lim_{n \rightarrow +\infty}\int_{-n}^{n} \frac{1}{n} \frac{1 - e^{-nx^2}}{x^2(1+nx^2)}dx = \lim_{n \rightarrow +\infty} \frac{2}{n}\int_{0}^{n}  \frac{1 - e^{-nx^2}}{x^2(1+nx^2)}dx $ $~~~~~~~~~\textbf{(1)}$ By setting $nx^2 = u$, we have $dx = \frac{1}{2\sqrt{n}} \times \frac{1}{\sqrt{u}}$ and $x = \frac{\sqrt{u}}{\sqrt{n}}$. So by substituting these in $\textbf{(1)}$ we have (I will not put bounds and at the end will come back to the initial bounds and also I will drop the constant in integrals) $\textbf{(1)} = \lim_{n \rightarrow +\infty} \frac{2}{n} \int   \frac{1 - e^{-u}}{\frac{u}{n}(1+u)} \times \frac{1}{2 \sqrt{n}}\times \frac{1}{ \sqrt{u}} du = \lim_{n \rightarrow +\infty}  \frac{1}{\sqrt{n}} \int  \frac{1 - e^{-u}}{ u \sqrt{u}(1+u)}   du $ $= \lim_{n \rightarrow +\infty}  \frac{1}{\sqrt{n}} \int  ( \frac{1 }{ u \sqrt{u}(1+u)}  - \frac{e^{-u} }{ u \sqrt{u}(1+u)}) du$ $~~~~~~~~~\textbf{(2)}$ By setting $\sqrt{u} = v$, we have $\frac{1}{2\sqrt{u}}du = dv$. So by substituting these in $\textbf{(2)}$ we have $\textbf{(2)} = \lim_{n \rightarrow +\infty}  \frac{2}{\sqrt{n}} \int (\frac{1}{v^2(1+v^2)} - \frac{e^{-v^2}}{v^2(1+v^2)} dv) $ $~~~~~~~~~\textbf{(3)}$ $\textbf{(3)}= \lim_{n \rightarrow +\infty}  \frac{2}{\sqrt{n}} \int ( \frac{1}{v^2} - \frac{1}{1+v^2}  - \frac{e^{-v^2}}{v^2} +  \frac{e^{-v^2}}{v^2 + 1})  dv$ Now we will calculate each term separately. First part: For to find $\int \frac{1}{v^2} dv $, by setting $k_1=-\frac{1}{v}$, we have $dk_1 =\frac{1}{v^2} dv$ and so we have $\int \frac{1}{v^2} dv = \int dk_1= k_1= -\frac{1}{v}= -\frac{1}{\sqrt{u}}= -\frac{1}{\sqrt{nx^2}} = -\frac{1}{\sqrt{n}|x|} $ Second part: For to find $-\int \frac{1}{1+v^2} dv$, by setting  $k_2 = \arctan(v)$, we have $dk_2 = \frac{1}{1 + v^2}dv$ and so we have $-\int \frac{1}{1+v^2} dv = -int dk_2= -k_2= -\arctan(v) = -\arctan(\sqrt{u}) = -\arctan(\sqrt{nx^2}) = -\arctan(\sqrt{n}|x|) $ Third part: For to find $-\int \frac{e^{-v^2}}{v^2} dv $, by setting $\begin{cases}
               k_3=e^{-v^2}\\
               -\frac{1}{v^2}=dk_4
            \end{cases}$ we will have $\begin{cases}
               dk_3=-2ve^{-v^2}\\
               k_4= \frac{1}{v}
            \end{cases}$ and our integral will transform to $-\int \frac{e^{-v^2}}{v^2} dv = \frac{e^{-v^2}}{v} - 2 \int e^{-v^2} dv =  \frac{e^{-v^2}}{v} - 2(\frac{\sqrt{\pi}}{2}) = \frac{e^{-v^2}}{v} - \sqrt{\pi} =  \frac{e^{-(\sqrt{u})^2}}{\sqrt{u}} - \sqrt{\pi} = \frac{e^{-u}}{\sqrt{u}} - \sqrt{\pi} =\frac{e^{-nx^2}}{\sqrt{nx^2}} - \sqrt{\pi}=\frac{e^{-nx^2}}{\sqrt{n}|x|} - \sqrt{\pi}$ Forth part: For to find $\int \frac{e^{-v^2}}{v^2 + 1} dv$, I cannot find it! Can someone please help me to find $\int \frac{e^{-v^2}}{v^2 + 1} dv$? Thanks!","['improper-integrals', 'real-analysis', 'integration']"
2556616,There exists a unique abelian extension such that $[K:\mathbb{Q}]=p$ and $disc(K)=p^n$,"I am reading this PDF: http://www.math.uchicago.edu/~may/VIGRE/VIGRE2007/REUPapers/FINALFULL/Culler.pdf On page $7$ it states (and proves) the following assertion: If $p$ is an odd prime, then there is a unique abelian extension $K/\mathbb{Q}$ of degree $p$ with discriminant a power of $p$; in particular, it is the unique subﬁeld of $\mathbb{Q}\left (\zeta\right )$ of degree $p$ over $\mathbb{Q}$, where $\zeta$ is a $p^2$th root of unity. I am trying to read the proof but I really find it incomprehensible. The proof is the following, I will be stopping in order to explain which things I do not understand. Proof. Let $K$ be the unique subﬁeld of the $p^2$th cyclotomic ﬁeld of order $p$. Then $K$ is ramiﬁed only at the prime $p$, which shows existence of an extension with the desired properties. Now suppose that $K'$ is another such extension. We want to show that $K = K'$. To do this, ﬁrst take the composite $K'L$ with the $p$th cyclotomic ﬁeld $L = \mathbb{Q}(\zeta)$. Since $L$ contains the $p$th roots of unity, the standard results of Kummer theory apply, so $K'L = L( \sqrt[p]{\alpha})$ for some $\alpha \in L$. For example, if $K = K'$, then $\alpha$ could be $\zeta$, or a number of the form $\zeta^k\beta^p$ for some $k\in \mathbb{Z}$ not divisibleby $p$ and some $\beta \in L$. Well, here I think I understand. I really do not know too much about Kummer Theory, but I know that $K'L = L( \sqrt[p]{\alpha})$ because of Theorem 6.2 on Lang's Algebra, ""Cyclic extensions"" section. Also, because of what is going next, we need $\alpha$ integral, and we clearly can achieve this: there exists $m\in \mathbb{N}$ such that $m\alpha$ is integral, and $L( \sqrt[p]{\alpha})=L( m^p\sqrt[p]{\alpha})=L( \sqrt[p]{m\alpha})$. Let $\lambda=1-\zeta$. Then $N(\lambda)=p$, so $\lambda$ generates the unique prime ideal of $L$ lying over $p$. I did not understand this, but I know that if $\omega$ is a $p^n$-rooth of unity then $p\mathbb{Z}[\omega]=(1-\omega)^{\varphi\left (p^n\right )}$ so I am OK with that. We will show that $\alpha$ can be chosen to be an algebraic integer satisfying $\alpha\equiv 1\pmod {\lambda^p}$. First we show can choose $\alpha$ to be prime to $p$. To see this, we use the fact that $K'L$ is abelian. Consider a generator $\tau$ for $\text{Gal}(L/\mathbb{Q})$ and extend it to an automorphism $\tau\in \text{Gal}(K'L/L)$. Since $\sigma$ and $\tau$ commute, we have: $\sigma \tau(\sqrt[p]{\alpha})=\tau\sigma(\sqrt[p]{\alpha})=\tau(\zeta \sqrt[p]{\alpha})=\zeta^l\tau(\sqrt[p]{\alpha})$ for some primitive root modulo $p$. This shows that $\sqrt[p]{\alpha}$ is an eigenvector of $\sigma$ with eigenvalue $\zeta^l$. I think this last sentence is wrong. What is true is that $\sqrt[p]{\alpha}$ is an eigenvector of $\sigma$ with eigenvalue $\zeta$ and that $\tau (\sqrt[p]{\alpha})$ is an eigenvector of $\sigma$ with eigenvalue $\zeta^l$. So let's assume that and continue: Hence $\tau(\alpha)=\tau(\sqrt[p]{\alpha})^p=\left (c\sqrt[p]{\alpha^l}\right )^p=c^p\alpha^l$. Who is $c$? This is my attempt to explain what he tried to do: since the $L$-linear transformation $\sigma :K'L\to K'L$ between $p$-dimensional vector spaces admits $\sqrt[p]{\alpha}$ as an eigenvector with eigenvalue $\zeta$, then we easily obtain that $\sqrt[p]{\alpha^i}$ is an eigenvector with eigenvalue $\zeta^i$ for $i=0,1,\cdots ,p-1$. Since the $\zeta^i$ are all pairwise distinct because $\zeta$ is a primitive $p$th root of unity, we obtained $p$ different eigenvectors, and therefore each space of eigenvectors has dimension $1$. Since the eigenvectors with eigenvalue $\zeta^l$ are generated by $\sqrt[p]{\alpha^l}$ and $\tau (\sqrt[p]{\alpha})$ is an eigenvector of $\sigma$ with eigenvalue $\zeta^l$, then there exists $c\in L$ such that $\tau \left (\sqrt[p]{\alpha}\right )=c\sqrt[p]{\alpha^l}$. But this argument has a little problem: we need (because of what is going next) $c$ integral, and it is not clear to me that $c$ is an algebraic integer. Anyone? Now it is clear that $\alpha$ can be chosen to be prime to $p$. Simply replace $\alpha$ by $\frac{\tau(\alpha)}{\alpha}$. Since the ideal generated by $\lambda$ is invariant under $\tau$, any factor of $\lambda$ dividing $\alpha$ cancels out, leaving something prime to $p$. Note that once $\alpha$ is prime to $p$, we can also force $\alpha$ to be congruent to $1$ mod $\lambda$ by raising $\alpha$ to a suitable power, since the multiplicative group of a ﬁnite ﬁeld is cyclic. Why did he say that the multiplicative group of a ﬁnite ﬁeld is cyclic? I mean... Yes, it is true, but didn't it suffice to say just that it is a finite group and just use that $g^{|G|}=1$ for every $g$ in a finite group $G$? Also, using the fact that $\zeta^a\equiv 1-a\lambda\pmod{\lambda^2}$ we can force $\alpha$ to be congruent to $1$ mod $\lambda^2$ by multiplying by a suitable power of $\zeta$. Why? I mean, we know that $\alpha = 1+\lambda s$ with $s\in \mathbb{Z}[\zeta]$ and $\zeta^a\equiv 1-a\lambda\pmod{\lambda^2}$, therefore $\zeta^a\alpha \equiv 1+\lambda (s-a)\pmod {\lambda^2}$, hence we are saying that for every $s\in \mathbb{Z}[\zeta]$ there exists $a\in \mathbb{N}_0$ such that $\lambda \mid s-a$. Why it is true? Finally,  we use induction to obtain the desired congruence. Say we have already shown that $\alpha \equiv 1+ a\lambda^e\pmod{\lambda^{e+1}}$. Now we use again the fact that $K'L$ is abelian. We have the congruence $\sigma(\alpha)\equiv c^p\alpha^l\pmod{\lambda^{e+1}}$ which, given our assumption, gives that $c\equiv c^p\equiv 1\pmod{\lambda}$. And therefore $c^p\equiv 1\pmod p$. As a consequence we have $1+a(l\lambda)^e\equiv \sigma(\alpha)\equiv \alpha^l\equiv 1+al(\lambda^e)\pmod{\lambda^e}$ and hence $l^e\equiv l\pmod{\lambda}$. But $l$ was supposed to be a primitive root modulo $\lambda$, and $e$ was greater than $1$. The inductive step works as long as $e$ is less than $p$, so we have shown $\alpha\equiv 1+a\lambda^p\pmod{\lambda^{p+1}}$ or in other words, $\alpha \equiv 1\pmod {\lambda^p}$, as desired. I could not understand even a simple word. Why do we have the congruence $\sigma(\alpha)\equiv c^p\alpha^l\pmod{\lambda^{e+1}}$? Why does it imply that $c\equiv c^p\equiv 1\pmod{\lambda}$? And what is the meaning of $c^p\equiv 1\pmod p$? Wasn't $c$ an element in $L$? Who is $a$? As you may suspect, I have several questions about the rest of the argument. I really do not understand a simple step, so what I really need here is a detailed explanation. That $K=K'$ follows immediately from this. To see why, consider the number $\xi=\frac{1-\sqrt[p]{\alpha}}{\lambda}$. It is a root of the polynomial $f(x)=\left (x-\frac{1}{\lambda}\right )^p-\frac{\alpha}{\lambda^p}$. Well, I would take $f(x)=\left (x-\frac{1}{\lambda}\right )^p+\frac{\alpha}{\lambda^p}$ instead. It is clear that this polynomial is monic, and that all but the constant term are algebraic integers. But by the preceding argument, $1−\alpha$ is divisible by $\lambda^p$. Hence the constant term is also an algebraic integer. Hence $\xi$ is an algebraic integer. Since $\xi\in \mathcal{O}_{K'KL}$, the discriminant of $KK'L$ over $KL$ must contain the ideal generated by $\pm N(f'(\xi))=\pm N\left (p\left (\xi-\frac{1}{\lambda}\right )^{p-1}\right )=\epsilon \alpha^{p-1}$ for some unit $\epsilon$. I completely understood the first paragraph, but what about the second? It is clear that if $f(x)\in KL[x]$ then the discriminant of $KK'L$ over $KL$ must contain the ideal generated by $\pm N(f'(\xi))$, but why is it true that $f(x)\in KL[x]$? Moreover, why $\pm N\left (p\left (\xi-\frac{1}{\lambda}\right )^{p-1}\right )=\epsilon \alpha^{p-1}$ for some unit $\epsilon$? In particular, this discriminant is prime to $p$, so $p$ is unramiﬁed in the extension $KK'L/KL$. Hence $p$ is unramiﬁed in the inertial ﬁeld $T/\mathbb{Q}$, and this extension is nontrivial. But $p$ was the only ramiﬁed prime in $K$, $K'$, and $L$, and therefore no prime other than $p$ can be ramiﬁed in $T$. Hence $T/\mathbb{Q}$ is unramiﬁed. But this is a contradiction, since there are no nontrivial unramiﬁed extensions of $\mathbb{Q}$. I understood the argument, but we obtained a contradiction from what? What were we supposing that we obtained a contradiction? Since my questions are just too much (I really cannot understand anything of the proof, as you may have seen) I will summarize and enumerate them: 1) Why is $c$ an algebraic integer? 2) Why did he say that the multiplicative group of a ﬁnite ﬁeld is cyclic? Wasn't it enough to say that it was a finite group? 3) Why using the fact that $\zeta^a\equiv 1-a\lambda\pmod{\lambda^2}$ we can force $\alpha$ to be congruent to $1$ mod $\lambda^2$ by multiplying by a suitable power of $\zeta$? If for every $s\in \mathbb{Z}[\zeta]$ there exist $b\in \mathbb{N}_0$ such that $s\equiv b\pmod{\lambda}$ then we are done, but why it is true? 4) How did the author prove that $\alpha \equiv 1\pmod {\lambda^p}$? 5) Why $f(x)\in KL[x]$? 6) Why $\pm N\left (p\left (\xi-\frac{1}{\lambda}\right )^{p-1}\right )=\epsilon \alpha^{p-1}$ for some unit $\epsilon$? 7) The author says at the end that we obtained a contradiction. But I do not know which contradiction we achieved because I do not know which assumption we made in order to reach that contradiction. I suspect it must be that $K\neq K'$, but the author neither says something like ""let's assume that $K=K'$"" (or some other assumption) nor explicits where he is making use of that assumption. So... What assumption are we making and where are we making use of it?","['abstract-algebra', 'algebraic-number-theory', 'dedekind-domain', 'field-theory']"
2556630,Subadditive Sequence Convergence,"Given:  A sequence ($a_n$) is called subadditive if $a_{m+n}$ ≤ $a_m$ + $a_n$ for all m, n ∈ N.
Prove that if ($a_n$) is a subadditive sequence of positive real numbers, then $(\frac{a_n}{n})$ converges. I am unaware of how to start this proof, but I know that it I need to show that $\frac{a_n}{n}$ --> inf{$\frac{a_k}{k}$: k $\geq$ 1}. Any help would be appreciated.","['real-analysis', 'sequences-and-series', 'analysis']"
2556707,A function is uniformly differentiable if its derivative is uniformly continuous?,"Suppose $I$ is an open interval and $f:I\rightarrow\mathbb{R}$ is a differential function. We can say $f$ is uniformly diferentiable if for every $\epsilon> 0$ there exists $\delta> 0$ such that $x,y\in I$ and $0\lt|x-y|<\delta  \Rightarrow  \Big| \frac{f(x)-f(y)}{x-y}-f'(x) \Big|\lt\epsilon$ I would like to prove that, if and only if $f'$ is uniformly continuous, then $f$ is uniformly differentiable.","['derivatives', 'uniform-continuity', 'real-analysis', 'functions']"
2556717,"Challenging problem about $f : \mathbb{R}^2 \rightarrow \mathbb{R}$ such that $f(x,y)+f(y,z)+f(z,x) = 0$ for all real $x, y, z$","Let $f : \mathbb{R}^2 \rightarrow \mathbb{R}$ be a function such that $f(x,y)+f(y,z)+f(z,x) = 0$ for all real numbers $x, y$, and $z$. Prove that there exists a function $g : \mathbb{R} \rightarrow \mathbb{R}$ such that $f(x,y) = g(x)−g(y)$ for all real numbers $x$ and $y$. Attempt: Can we just set $z = 0$ and solve for $f(x,y)$?","['algebra-precalculus', 'functional-equations']"
2556764,Viscosity solution of Hamilton Jacobi equation,"In the book Partial Differential Equations by L.C. Evans there are two solution concepts for Hamilton Jacobi equations A. VISCOSITY SOLUTION (which is defined in chapter 10) B. WEAK SOLUTION (which is defined in chapter 3) I have the following doubts.... 1)Is the visocotiy solution lipschitz continuous? if so does it satatisfy the equation pointwise a.e. 2)Are these two solutions same? if so what is the justification? Definition of Viscosity solution(L.C.Evans, PDE, Chapter 10) : Assume that $u$ is bounded and uniformly continuous on $R^n \times [0,T]$,for each $T\geq 0$. We say that u is viscosity solution of the initial value problem $u_t+H(Du,x)=0$ in $R^n \times (0,\infty)$, u=g on $R^n \times \{t=0\}$  provided A)$u=g$ on $R^n \times (0,\infty)$, and B)for each v $\in C^ \infty (R^n \times (0,\infty))$, if $u-v$ has a local maximum at a point $(x_0,t_0) \in R^n \times (0,\infty)$ then $v_t(x_0,t_0)+H(Dv(x_0,t_0),x_0)\leq 0$ and if if $u-v$ has a local minimum at a point $(x_0,t_0) \in R^n \times (0,\infty)$ then $v_t(x_0,t_0)+H(Dv(x_0,t_0),x_0)\geq 0$","['ordinary-differential-equations', 'hamilton-jacobi-equation', 'partial-differential-equations']"
2556766,Limit as $x\to 0$ of $\frac{(1+x)^{1/x}-e}{x}$,"Find the following limit:
$$\lim_{x\to 0}\frac{(1+x)^{1/x}-e}{x} \tag{1}$$ The following is my approach, although is full of incorrect assumptions (statements etc). 
$$f(x)= \lim_{h\to 0}(1+x+h)^{1/(x+h)}\\$$ From here we can say $f(0) = e$. $$\ln(f(x)) = \lim_{h\to 0}\frac{1}{x+h}\ln(1+x+h)$$ Near $x=0$, we can use the series of logarithm:
$$\ln(f(x)) = \lim_{h\to 0}\frac{1}{x+h}\left(x+h-\frac{(x+h)^2}{2}...\right)$$ Differentiating we get: $$\frac{f'(x)}{f(x)} = \lim_{h\to 0}-\frac{1}{2} +\frac{x+h}{3}  ... \tag{2}$$ Now we note that $(1)$ is actually $f'(0)$ (?) and so from $(2)$ we get: $$f'(0) =\frac{-1}{2} f(0) = \frac{-e}{2}$$ While the answer is seemingly correct, the method is absolutely not. It looks like this is fluke than anything else. I also tried computing taylor series of $(1+x)^{1/x}$ near $x=0$ but couldn't do it.","['exponential-function', 'calculus', 'limits']"
2556875,Contest Math Geometry problem related to Tangents of a circle,"$2009$ concentric circles are drawn with radii from $1$ unit to $2009$ units. From a point on the outermost circle, tangents are drawn to the inner circles. Discover the number of tangents which will have integer measure in the problem. My approach to the problem is that since the tangents are perpendicular to the radius at the point of contact. This means that I could find all the Pythagorean triplets with $2009$ as one measure. However, I wasn't getting anywhere with that approach.","['circles', 'contest-math', 'tangent-line', 'geometry']"
2556960,"If $x,y,z\in {\mathbb R}$, Solve this system equation:","If $x,y,z\in {\mathbb R}$, Solve this system equation: $$
\left\lbrace\begin{array}{ccccccl}
    x^4 & + & y^2 & + & 4         & = & 5yz
\\[1mm]
y^{4} & + & z^{2} & + & 4 & = &5zx
\\[1mm]
z^{4} & + & x^{2} & + & 4 & = & 5xy
\end{array}\right.
$$ This is an olympiad question in Turkey (not international), which that, I could not solve it. My idea: $xy=a \\ yz=b \\ xz=c$ $$
\left\lbrace\begin{array}{ccccccl}
    a^2c^3 & + & b^3a & + & 4b^2c        & = & 5b^3c
\\[1mm]
a^{3}b^2 & + & bc^3 & + & 4c^2a & = &5c^3a
\\[1mm]
b^{3}c^2 & + & a^3c & + & 4a^2b & = & 5a^3b
\end{array}\right.
$$ Yes, I know, this is a stupid idea, because it did not work at all ( last system equation is more difficult).","['polynomials', 'systems-of-equations', 'real-numbers', 'algebra-precalculus', 'contest-math']"
2556965,"A problem of central simple algebras: why $(E,s,\gamma)\cong M_n(F)$ only if $\gamma$ is the norm of an element of $E$?","I am stuck in the following problem, which is the exercise 6 of section 4.6 of N. Jacobson's Basic Algebra II : Problem. Prove that $(E,s,\gamma)\cong M_n(F)$ if and only if $\gamma$ is the norm of an element of $E$. Here $F$ is a field, and $E/F$ is a cyclic extension of degree $n$ with $\mathrm{Gal}(E/F)=\langle s\rangle$. $(E,s,\gamma)$ is an $n^2$-dimensional central simple algebra over $F$ generated by all matrices $\overline b=\mathrm{diag}\{b, s(b),\cdots,s^{n-1}(b)\}$ ( $\forall b\in E$ ) and 
$$
z=\left(\begin{matrix}
0&1& & &\\
0&0&1& &\\
\vdots& &\ddots&\ddots &\\
0& \cdots&\cdots& 0& 1\\
\gamma&\cdots&\cdots & 0& 0
\end{matrix}\right)
$$
for a fixed $\gamma\in F^\times$. Actually the ""if"" part is quite obvious as a concrete isomorphism can be formulated (Indeed, the correspondence $\lambda_b\mapsto\overline b$, $\lambda_as\mapsto z$ gives an isomorphism from $\mathrm{End}_FE$ to $(E,s,\gamma)$, where $\gamma=N_{E/F}(a)$ and $\lambda_b$ is the $\require{enclose}\enclose{horizontalstrike}F$-isomorphism  $\enclose{horizontalstrike}{x=\sum_{i=0}^{n-1}c_i\alpha^{i}\mapsto \sum_{i=0}^{n-1}c_is^{i}(b)\alpha^i}$, in which $\enclose{horizontalstrike}\alpha$ is a primitive element over $\enclose{horizontalstrike}F$). is the $F$-linear homomorphism $x\mapsto bx$. Where I am stuck is the ""only if"" part of this problem. Although the book gives a hint to use Skolem-Noether theorem, I can hardly find where to apply it. So I would like to ask how to obtain the implication $(E,s,\gamma)\cong M_n(F)$ $\Longrightarrow$ $\gamma=N_{E/F}(a)$ (i.e., $\gamma=as(a)\cdots s^{n-1}(a)$) for some $a\in E$. Any help would be greatly appreciated. Here are some attempts (I will continue to update until a solution is come up): I guess this is one probable way. If we denote the isomorphism by $f\colon (E,s,\gamma)\to M_n(F)$ , and let $\enclose{horizontalstrike}g$ be the monomorphism from the algebra generated by all $\enclose{horizontalstrike}{\overline b}$ to $\enclose{horizontalstrike}{M_n(F)}$ that sends $\enclose{horizontalstrike}{\overline b}$ to the matrix of $\enclose{horizontalstrike}{\lambda_b}$ under a basis of $\enclose{horizontalstrike}{E/F}$, then the Skolem-Noether theorem tells us that there is a matrix $\enclose{horizontalstrike}{M\in \mathrm{GL}_n(E)}$ such that $\enclose{horizontalstrike}{f(\cdot)=M^{-1}g(\cdot)M}$. Then  we can define $\enclose{horizontalstrike}{g(z)}$ and it remains to show that $\enclose{horizontalstrike}{g(z)}$ is the matrix of $\enclose{horizontalstrike}{as}$ for some $\enclose{horizontalstrike}{a\in E}$. However, this way seems not working and the use of inner automorphism is still unclear... A previous exercise of this section asserts that for each $x\in(E,s,\gamma)$, it can be written in a unique way as
$$
x=\overline b_0+\overline b_1 z+\cdots+\overline b_{n-1}z^{n-1}.
$$ If we can prove that $\enclose{horizontalstrike}{M\in \mathrm{GL}_n(F)}$, then $\enclose{horizontalstrike}{g(z)}$ corresponds a homomorphism $\enclose{horizontalstrike}{\zeta\in\mathrm{End}_FE}$. Moreover, since if so, $\enclose{horizontalstrike}{g\colon(E,s,\gamma)\to M_n(F)}$ is an $\enclose{horizontalstrike}F$-linear isomorphism, and thus $\enclose{horizontalstrike}{s\in\mathrm{End}_FE}$ can be written in a unique manner as follows
$$
\enclose{horizontalstrike}{s=\lambda_{b_0}+\lambda_{b_1}\zeta+\cdots+\lambda_{b_{n-1}}\zeta^{n-1}}
$$
where $\enclose{horizontalstrike}{b_0,\cdots,b_{n-1}\in E}$. Hopefully by $\enclose{horizontalstrike}{s^n=1}$ we perhaps can get that all $\enclose{horizontalstrike}{b_i=0}$ but $\enclose{horizontalstrike}{b_1}$ (at least this holds for $\enclose{horizontalstrike}{n=2}$), and then $\enclose{horizontalstrike}{\zeta=\lambda_{b_1^{-1}}s}$, which indicates $\enclose{horizontalstrike}{\lambda_\gamma=\zeta^n=(\lambda_{b_1^{-1}}s)^n}$, namely $\enclose{horizontalstrike}{\gamma=N_{E/F}(b_1^{-1})}$. Let us denote by $\iota\colon(E,s,\gamma)\hookrightarrow M_n(E)$ the natural embedding. Then by Skolem-Noether theorem, $\iota(\cdot)=N^{-1}f(\cdot)N$ for some $N\in\mathrm{GL}_n(E)$. Since $f$ is isomorphic, each $x\in M_n(F)$ can be written as
$$
x=f(\overline b_0)+f(\overline b_1)f(z)+\cdots+f(\overline b_{n-1})f(z)^{n-1}
$$
in a unique manner with $b_i\in E$. Note that $s\in \mathrm{End}_FE$ corresponds to a matrix 
$$x_s=f(\overline b_0)+\cdots+f(\overline b_{n-1})f(z)^{n-1}\in M_n(F)$$ 
and $x_s^n=I\in M_n(F)$ (the identity matrix). If we can deduce that $f(\overline b_i)=0$ but $f(\overline b_1)$ (again, this holds at least for $n=2$), then it follows that $\enclose{horizontalstrike}{\det(f(\overline b_1z))^n=1}$, and thus $\enclose{horizontalstrike}{\det(\overline b_1z)^n=1\Longrightarrow N_{E/F}(b_1)^n\gamma^n=1}$. $x_s=f(b_1)f(z)$ and hence $\gamma I=N_{E/F}(b_1^{-1})I$. If it can be shown that $x_sf(\overline b)=f(s(\overline b))x_s$, then that $f(\overline b_i)=0$ but $f(\overline b_1)$ can be deduced, and the arguments before all make sense. A failed attempt: I tried to prove that the $m_b$ defined as follows corresponds to the matrix $f(\overline b)$ under some $F$-basis of $E$ but at last found that I made several fatal mistakes, having no idea how to fix them for the moment. I think this is very likely to be a working way. Assume that $f\colon(E,s,\gamma)\to M_n(F)\subset M_n(E)$. For the inclusion mapping $\iota\colon(E,s,\gamma)\hookrightarrow M_n(E)$, the Skolem-Noether theorem implies that 
$$
f(\cdot)=N^{-1}\iota(\cdot)N
$$
for some $N\in\mathrm{GL}_n(E)$. For a primitive element $\alpha$ of $E/F$, $B:=\{1,\alpha,\cdots,\alpha^{n-1}\}$ is an $F$-basis of $E$. Define for each $b\in E$,
\begin{align}
m_b\colon E&\to E\\
x=\sum_{i=0}^{n-1}c_i\alpha^i&\mapsto\sum_{i=0}^{n-1}c_is^i(b)\alpha^i
\end{align}
and it follows that $m_b$ is $F$-linear, i.e., $m_b\in\mathrm{End}_FE$. We assert that there is an $F$-basis $B'$ of $E$, under which the matrix of $m_b\ (\forall b\in E)$ is $f(\overline b)$. Indeed, for each $b\in E$,
\begin{align}
m_b(1,\alpha,\cdots,\alpha^{n-1})N=&(b,s(b)\alpha,\cdots,s^{n-1}(b)\alpha^{n-1})N\\
=&(1,\alpha,\cdots,\alpha^{n-1})\overline bN\\
=&(1,\alpha,\cdots,\alpha^{n-1})NN^{-1}\iota(\overline b)N\\
=&(1,\alpha,\cdots,\alpha^{n-1})Nf(\overline b).
\end{align}
Thus it remains to show that $B':=(1,\alpha,\cdots,\alpha^{n-1})N$ is an $F$-basis of $E$. Let us consider the matrix
$$
V=\left(
\begin{matrix}
1&\alpha&\cdots&\alpha^{n-1}\\
1&s(\alpha)&\cdots&s(\alpha^{n-1})\\
\vdots&\vdots&&\vdots\\
1&s^{n-1}(\alpha)&\cdots&s^{n-1}(\alpha^{n-1})
\end{matrix}\right)\in M_n(E).
$$
Since $\mathrm{Gal}(E/F)=\langle s\rangle$ and $E=F(\alpha)$, $s^i(\alpha)\neq s^j(\alpha)$ for $i\neq j$ ($i, j=1,\cdots,n$). Therefore the Vandermonde matrix $V\in\mathrm{GL}_n(E)$. If $B'=(1,\alpha,\cdots,\alpha^{n-1})N$ is $F$-linear dependent, then the columns of $\enclose{horizontalstrike}{VN}$ are $\enclose{horizontalstrike}{F}$-(and of course $\enclose{horizontalstrike}{E}$-)linear dependent (since $\enclose{horizontalstrike}{s^i}$ fixes elements of $\enclose{horizontalstrike}{F}$). As $\enclose{horizontalstrike}{N}$ is invertible, it leads to a contradiction that $\enclose{horizontalstrike}{V\notin\mathrm{GL}_n(E)}$. Therefore $\enclose{horizontalstrike}{B'}$ is an $\enclose{horizontalstrike}{F}$-basis of $\enclose{horizontalstrike}{E}$. Since $M_n(F)\cong \mathrm{End}_FE$, under the basis $B'$, $s$ corresponds to a matrix $x_s\in M_n(F)$. Then
$$
x_s=f(\overline b_0)+f(\overline b_1)f(z)+\cdots+f(\overline b_{n-1})f(z^{n-1})
$$
for some $b_0,\cdots,b_{n-1}\in E$. Note that $\enclose{horizontalstrike}{sm_b=m_{s(b)}s}$ and we have for each $\enclose{horizontalstrike}{b\in E}$, $\enclose{horizontalstrike}{x_sf(\overline b)=f(s(\overline b))x_s}$, while
\begin{align}
x_sf(\overline b)=&f(\overline b_0)f(\overline b)+f(\overline b_1)f(z)f(\overline b)+\cdots+f(\overline b_{n-1})f(z)^{n-1}f(\overline b),\\
f(s(\overline b))x_s=&f(s(\overline b))f(\overline b_0)+f(s(\overline b))f(\overline b_1)f(z)+\cdots+f(s(\overline b))f(\overline b_{n-1})f(z)^{n-1}.
\end{align} The exercise 4 of section 4.6 of N. Jacobson's Basic Algebra II asserts that Each $x\in (E,s,\gamma)$ can be written in a unique way as $$x=\overline b_0+\overline b_1z+\cdots+\overline b_{n-1}z^{n-1},$$ where $b_i\in E$. Thus it follows that \begin{align}
&f(\overline b_0)f(\overline b)=f(s(\overline b))f(\overline b_0),\\ &f(\overline b_1)f(s(\overline b))=f(s(\overline b))f(\overline b_1),\\
&\quad\vdots\\
&f(b_{n-1})f(s^{n-1}(\overline b))=f(s(\overline b))f(\overline b_{n-1}).
\end{align} The arbitrariness of $b\in E$ forces that $b_0=b_2=\cdots=b_{n-1}=0$ (precisely speaking,  for each $i=0,2,\cdots,n-1$, pick a $b\in E\setminus F$ which is not fixed by $s^{i-1}$, and then $f(\overline b_i)f(s^i(\overline b))=f(s(\overline b))f(\overline b_i)=f(\overline b_i)f(s(\overline b))$; if $b_i\neq 0$, $f$ being isomorphic entails that $f(s^i(\overline b))=f(s(\overline b))$ and thence, $b=s^{i-1}(b)$, which is a contradiction). Hence $x_s=f(\overline b_1)f(z)$, and the invertibility of $x_s$ implies that $b_1\neq 0$. We thus have \begin{align}
\gamma I=&f(z)^n=(f(\overline b_1)^{-1}x_s)^n=(f(\overline{b_1^{-1}})x_s)^n\\
=&f(\overline{b_1^{-1}})f(s(\overline{b_1^{-1}}))\cdots f(s^{n-1}(\overline{b_1^{-1}}))x_s^n\\
=&N_{E/F}(b_1^{-1})I,
\end{align} i.e., $\gamma=N_{E/F}(a)$, where $a=b_1^{-1}\in E^\times$.","['abstract-algebra', 'noncommutative-algebra', 'ring-theory', 'representation-theory', 'semi-simple-rings']"
2556993,Limits that take on a range rather than a unique value,I've come across two limits which are reported to take on a range of values rather than a unique one. They are: $$\lim \limits_{x \to \infty} \space\frac{1+\cos x}{1-\sin x} = 0 \space to\space \infty$$ $$\lim \limits_{x \to \infty} \space \frac{2+2x+\sin(2x)}{(2x+\sin(2x))e^{\sin x}} =  \frac{1}{e} \space to \space e$$ These answers seem to contradict what I know and understand as the definition of a limit. I'm self-studying maths so am clearly missing something here. My questions: How are these limits solved? Why do they exist? Why is a range of values allowed here? What kind of books should I read to learn more about this? If you could provide references that would be perfect. ETA results from Wolfram: The First The Second,"['real-analysis', 'limits-without-lhopital', 'limits']"
2557025,Determine if $x=0$ is a point of relative extremum for $f(x)= \sin(x) + \frac{x^3}{6}$,"Determine if $x=0$ is a point of relative extremum for $f(x)= \sin(x)
+ \frac{x^3}{6}$ I am trying to use this test Here, $f(x)= \sin(x)
+ \frac{x^3}{6}$ $f'(x)=\cos(x) + \frac{x^2}{2} \Rightarrow f'(0)=1 \neq 0$ So I am unable to proceed further.","['derivatives', 'real-analysis']"
2557027,Orthogonality of major and minor axes of an ellipse,"How would one go about proving that the major and minor axes of an ellipse are perpendicular bisectors of each other? I've read several proofs of expressions for the ellipse in cartesian and polar coordinates given the definition of the ellipse as the collection of points where the sum of the distance from two points is constant, but they always just assume that it is true. Visually it looks true, but is there an argument that follows simply from the definition?","['conic-sections', 'geometry']"
2557049,Nature of $\sum 1/f(n)$ with $f(n) := n\ln(n)\ln(\ln(n))...*\ln^{(k_n)}(n)$ ; $k_n$ being the largest natural integer $k$ such that $\ln^{(k)}(n)≥1$ [duplicate],"This question already has an answer here : Does the series converge (1 answer) Closed last year . For all $n$ in $\mathbb N^*$, let $f(n) :=
n*\ln(n)*\ln(\ln(n))*...*\ln^{(k_n)}(n)$, with $\ln^{(k)}$ being the
  logarithm iterated $k$ times, and $k_n$ being the largest natural
  integer $k$ such that $\ln^{(k)}(n)≥1$. Study the nature of the series $\sum 1/f(n)$. One can show that when $k_n$ is a constant, the series diverges (by comparison with integral) but here  it is not the case. I think the series also diverges. How to prove it ?","['real-analysis', 'limits', 'logarithms', 'calculus', 'sequences-and-series']"
2557050,"Rotman's Advanced Modern Algebra, Third Edition","It has been not a long time since the monolithic Rotman's Advanced Modern Algebra has been re-published, in a reorganized, two volumes new edition. Do you know if and how the two volumes of this third edition differ in quality and quantity from the previous one? I read the respective tables of contents and I have only a vague idea about this. I think it would be useful if someone has a more concrete, fact-based opinion about possible differences between them, with also an (always personal, I know) judgement about which text would be preferable in which context.","['abstract-algebra', 'book-recommendation']"
2557098,"$\ 3\,\tan(x)-\sqrt3 $","You have to find alternate form, that has only multiplication/division signs between trigonometric functions The form is: $\ 3\,\tan(x)-\sqrt3 $ The solution form is: $\ \left(\frac{2*\sqrt 3\sin(x-π/6)}{\cos(x)}\right) $ My try: $\ 3\,\tan(x)-\sqrt3 = 3*\left(\frac{\sin(x)}{\cos(x)}\right)-\left(\frac{\sin(π/3)}{\cos(π/3)}\right)= \left(\frac{\sin(x)\cos(π/3)-\sin(π/3)\cos(x)}{\cos(x)\cos(π/3)}\right)=\left(\frac{\sin(x)\cos(π/3)+\sin(x)\cos(π/3)-\sin(π/(3)\cos(x)}{\cos(x)\cos(π/3)}\right)$ I am certain that I have done something wrong in one of the steps, as this leads to only more complicated form, that isn't even near the solution","['algebra-precalculus', 'trigonometry']"
2557140,"Preparation needed for a book like Munkres' ""Analysis on Manifolds"" or Spivaks' ""Calculus on Manifolds""","At the present moment I find reading the chapters in Munkres' ""Analysis on Manifolds"" and Spivaks' ""Calculus on Manifolds"" to be a very tough read. By that I mean I often don't understand what the theorems and lemmas are truly trying to convey, and that then causes me problems when trying to solve the exercises. My plan is to build my mathematical maturity/strength by reading a good linear algebra book that focuses on proofs, then reading a multivariable calculus text(or notes) to get a basic understanding of the concepts like the ""Inverse function theorem"", and doing computations with them. I did a quick review of the linear algebra concepts used in Munkres and Spivak and took a bit of time to get familiar with mulitvariable calculus topics--such as how the single variable definition of the derivative is modified to better suit the multivariable case-- and that hasn't made the process of engaging with munkres and spivak any easier.  So i've decided to go through an entire linear algebra text making sure I understand every theorem and corresponding proof, and force myself to prove each excercise, then doing the same thing for a multivariable calculus text. Before I begin this process, I want to know if doing this will make reading and solving problems in munkres/spivak a breeze.  In other words, is my inability to properly engage with munkres/spivak due to my lack of proper engagement with a good linear algebra text, working out proofs in a single variable calculus text, and reviewing the multivariable calc text first?  Or must I persist with studying munkres/spivak directly until it becomes clear?","['soft-question', 'real-analysis', 'linear-algebra', 'analysis']"
2557143,Proving a tangent value equality,"Show that $\tan50 \tan60 \tan70 = \tan80$. I have used compound angle formula for tan to bring $\tan 10$ into it as $\frac{1}{\tan 10}=\tan 80$, but I can't seem to get it to come out.",['trigonometry']
2557151,Calculate the limit using de L'Hopital's rule,Calculate the following limit: $\lim_{x \to +\infty}(\sqrt{x}-\log x)$ I started like this: $\lim_{x \to +\infty}(\sqrt{x}-\log x)=[\infty-\infty]=\lim_{x \to +\infty}\frac{(x-(\log x)^2)}{(\sqrt{x}+\log x)}=$ but that's not a good way... I would be gratefull for any tips.,['limits']
2557157,Any divisible subgroup of $G$ splits with internal direct product.,"The question is as follows: Let $G$ be an Abelian group and suppose that $A$ is a divisible
subgroup of finite index. Show that $G = A \dot{\times} B$ for some $B \leq G$. $\textbf{Some definitions and notations:}$ An Abelian group $A$ is said to be divisible if, for every $a \in A$ and positive integer $n$, there exists $b \in A$ such that $b^n = a$. If $G$ is any group having two normal subgroups $M$ and $N$ with  $MN = G$ and $M \cap N = 1$, we say that $G$ is the internal direct product of its 
subgroups $M$ and $N$. And we write $G = M \dot{\times} N$. $\textbf{An idea for to prove it}:$ It is possible to prove that a group $D$ is divisible if and only if it satisfies the following “injectivity” condition:  Any homomorphism $f : A \to D$ from any group $A$ into $D$ extends to any group $B$ which contains $A$, i.e., there exists a homomorphism $\bar{f} : B → D$ so that $f\bar{f}|_A = f$. And then if we can prove this, then for $D \leq G$ divisible, then $D$ will be split, i.e $D$ will have a complement $H$ so that $G = D \dot{\times} H$. Because since $D$ is divisible, it is injective. Thus the identity mapping $D \to D$ extends to a homomorphism $r : G \to  D$. Then $H = \ker r$ is a complement for $D$ in $G$. Can you please let me know if I am wrong? And if it is nice approach, can you please help me to complete it? (Please feel free to post a proof.) Thanks! $\textbf{Proposed proof:}$ Here I will post my proof for this question. Please let me know if I am wrong and please post the edited version as an answer or please post your own proof. I appreciate your help a lot! Thanks! $ \rule{14.8cm}{0.6pt}$ $\textbf{Proof:}$ First we start by claiming that a group $A$ is divisible if and only if it satisfies the following “injectivity” condition:  Any homomorphism $f : L \to A$ from any group $L$ into $A$ extends to any group $H$ which contains $L$, i.e., there exists a homomorphism $\bar{f} : H → A$ so that $f\bar{f}|_L = f$. For to prove this claim, We use Zorn’s Lemma. Consider the partially ordered set $P$ of all pairs $(C, g)$ where $C$ is a subgroup of $H$ containing $L$ and $g : C \to G$ is an
extension of $f$. Let $(C, g) \leq (A, h)$ if $C \leq A$ and $g = h\mid_C$. The set $P$ is nonempty since it contains $(L, f)$. Also any tower $(C_{\alpha}, g_{\alpha})$ in $P$ has an upper bound  $(\cup C_\alpha,\cup g_{\alpha}) \in P$. Consequently, by Zorn’s Lemma, $P$ has a maximal element, say $(C, g)$. We claim that $C = H$ and $g$ is the desired extension of $f$ to $H$. To see this suppose $C < H$. Then there exists
an $x \in H$ so that $x \notin C$. There are two cases Either $x + C$ has finite order in $H/C$ or it has infinite order. In the second case, $<C,x> = C \dot{\times} <x>$ so $g : C \to G$ can be extended to $g \dot{\times} 0 : C \dot{\times} <x> \to G$ contradicting the maximality of $(C, g)$. In the first case, let $n$ be the order of $x+C$ in $H/C$, i.e., $n > 0$ is smallest positive integer so that $nx \in C$. Since $G$ is divisible there is a $z \in G$ so that $nz = g(nx)$. By the Pasting lemma, $g : C \to G$ can be extended to the homomorphism $g \dot{\times} h : C \dot{\times} <x> \to G$ where $h : <x> \to G$ is given by $h(x) = z$. This contradicts the maximality of $(C, g)$. Therefore, divisible groups are
injective. Conversely, if $G$ is injective then it is obviously divisible: given any $x \in G$ and $n > 0$ let $f : n\mathbb{Z} \to G$ be given by $f(n) = x$ .  If $G$ is injective this extends to a homomorphism $\bar{f} : \mathbb{Z} \to G$. But then $n\bar{f}(1) = f(n) = x$. Now we claim that any divisible subgroup of $G$ splits, i.e., if $A \leq G$ is divisible then $A$ has a complement $B$ so that $G = A \dot{\times} B$. For to prove this we see that since $A$ is divisible, it is injective. Thus the identity mapping $A \to A$ extends to a homomorphism $r : G \to  D$. Then $B = \ker r$ is a complement for $A$ in $G$. $ \rule{14.8cm}{0.6pt}$ Please let me know if my proof is not correct? And please post what you think which is correct. Thanks!","['finite-groups', 'infinite-groups', 'abelian-groups', 'group-theory']"
2557170,If $(ab)^n=a^nb^n$ then $(aba^{-1}b^{-1})^{n^2-n}=e$,"I was trying to prove that statement from Resident Dementor's answer, namely: We can prove that if for an integer $n$ and every $a,b\in G$, $(ab)^n=a^nb^n$, then
  $$(aba^{-1}b^{-1})^{n(n-1)}=e$$
  The proof is easy. In fact,
  $$(aba^{-1}b^{-1})^{n^2}=[(aba^{-1}b^{-1})^n]^n=[a^n(ba^{-1}b^{-1})^n]^n=...=a^nb^na^{-n}b^{-n}\\\
 (aba^{-1}b^{-1})^{n}=(ab)^n(a^{-1}b^{-1})^n=a^nb^na^{-n}b^{-n}$$ I was able to understand only the first two equalities and i was trying to do different methods but no results. Can anyone demonstrate detailed proof, please?","['abstract-algebra', 'group-theory', 'proof-verification']"
2557204,What is the intuition behind saying that the nilradical is the intersection of prime ideals?,"Let $R$ be a commutative ring with unity. Let $N$ be it's nilradical, that is the ideal consisting of all the nil potent elements of $R$. We know that $N$ is the intersection of all prime ideals of $R$. The proof , although understandable, to me lacks intuition. I was wondering if anyone could tell me why this is true in a more intuitive way? Thank you.","['intuition', 'abstract-algebra', 'commutative-algebra']"
2557206,Divergence Theorem with a vector field.,I have been stuck on the following question for quite a while and my professor have not been helpful at all. I'm not sure how to make delta(f) a scalar so I can apply the theorem. Any useful hints are appreciated!,['multivariable-calculus']
2557217,What is the name of this 3D shape with 12 outer vertices?,Faces: 48 Outside vertices: 12 Other vertices: 14 (I believe),"['terminology', '3d', 'solid-geometry', 'geometry']"
2557277,Regression Analysis (Line of Best Fit) for Categorical Variables,"Brief Background/Motivation: I am looking at an Income vs. Education table that is adapted from a dissertation and was used in developing a curriculum in a social justice mathematics program. In the dissertation, the author discusses using these data (income vs. education level, broken down by gender) to have students create a line of best fit , but does not explain how the categorical variable is treated or transformed into an ordinal variable. The issue, as I see it, is that education level is categorical and not continuous; I have been unable to find a ""standard"" or even suggested approach regarding how to treat the categorical variable. I see two different ways: Starting at 1, label each category 1-7. This assumes a uniform/linear step size (i.e., the difference between some high school and completing high school is the same as the difference between a master's degree and a doctorate) which is clearly problematic, but one possibility. Approximate the number of years of schooling for each category. For example, ""high school completion"" would be 13, bachelor's degree would be 17, masters would be anything from 18 to 19, etc. Then you have to make some decisions about categories such as ""some high school"": is this a 10, 11 or 12? Also, how should you count the category of ""no high school""?  Is this a 7 or 8 or 9?  This is also clearly subjective and has its own problematics, but is actually roughly the same as (1). Question: Do either of the two approaches suggested above work? Or is there another, better way to treat these data? Pointers to relevant papers or resources would be welcome, too.","['regression', 'reference-request', 'statistics', 'linear-regression']"
2557333,Correct notation for the set of composite numbers,"The set of all prime numbers is usually denoted by $\mathbb{P}$. The set of all composite numbers, however is not denoted by $\mathbb{C}$, given the ambiguity with the set of complex numbers. What is the correct (usual) way of denoting the set of composite numbers (with a single symbol)? EDIT - an example : Given a function $f$ that has a ""prime version"" and a ""composite version"", one may denote the ""prime $f$"" function by $f_{\mathbb{P}}$, but the ""composite $f$"" function cannot be denoted by $f_{\mathbb{C}}$ since it creates ambiguity with the set of complex numbers. What symbol would one use in this case to denote the ""composite $f$"" function?","['notation', 'elementary-set-theory']"
2557339,Extension of Bounded Linear Operator,"Let $X$ and $Y$ be normed spaces and let $W \subset X$ be a linear subspace. Suppose that $T_W \in B(W,Y)$ and that the range of $T_W$ is finite dimensional. I know that $T_W$ can be extended to $T \in B(X,Y)$ with $\|T\|=\|T_W\|$. How about the range?","['functional-analysis', 'analysis']"
2557479,"Motivation for the ring product rule $(a_1, a_2, a_3) \cdot (b_1, b_2, b_3) = (a_1 \cdot b_1, a_2 \cdot b_2, a_1 \cdot b_3 + a_3 \cdot b_2)$","In a lecture, our professor gave an example for a ring. He took it out of another source and mentioned that he does not know the motivation for the chosen operation. Of course, it's likely that somebody just invented an arbitrary operation satisfying ring axioms. I'd still like to try my luck whether anyone here can decipher the operation and give any kind of motivation for that example. On $\mathbb{R}^3$ define the operations $+$ and $\cdot$ by
$$ \begin{aligned} (a_1, a_2, a_3) + (b_1,b_2,b_3) &= (a_1+b_1,a_2+b_2,a_3+b_3)
\\ (a_1, a_2, a_3) \cdot (b_1, b_2, b_3) &= (a_1 \cdot b_1, a_2 \cdot b_2, a_1 \cdot b_3 + a_3 \cdot b_2).
\end{aligned} $$
(The $+$ and $\cdot$ operations on the right side are the usual addition and multiplication from $\mathbb{R}$.)
With those operations, one can confirm that $\left(\mathbb{R}^3, +, \cdot \right)$ is a ring.","['abstract-algebra', 'ring-theory']"
2557485,Cardinality of general linear group of finite rings,"Is there a formula for computing the cardinality of the general linear group of $(\mathbb{Z}/n\mathbb{Z})^d$ as a $\mathbb{Z}/n\mathbb{Z}$-module, where $n$ can be any natural number greater than or equal to $2$, not only a prime ? Even better, is there also a way to decompose it into products/semiproducts of smaller groups. I have tried using ideas of the proof for prime numbers, (counting the number of basis), but it seems much less practical, because of consideration on elements that are part of ideals ($(2,2)$ will not be part of any base in $\mathbb{Z}/10\mathbb{Z}$). Maybe you can do something by decomposing $n$ into primes factors ? Thanks in advance","['finite-groups', 'modular-arithmetic', 'group-theory']"
2557500,$U$ is timelike if its orthogonal complement $U^\perp$ is spacelike,"Consider the bilinear form $\left<x,y\right>_{n,1} = \sum_{j=1}^n x_j y_j - x_{n+1} y_{n+1}$ on $\mathbb{R}^{n+1}$. A vector $x \in \mathbb{R}^{n+1}$ is said to be timelike if $\left<x,x\right>_{n,1} > 0$ while $x$ is called spacelike if $\left<x,x\right>_{n,1} < 0$. Furthermore, a subspace $U \subset \mathbb{R}^{n+1}$ is timelike if there exists a timelike vector $u \in U$ and $U$ is called spacelike if every non-zero vector in $U$ is spacelike.
The orthogonal complement is defined as $U^\perp = \{ v \in \mathbb{R}^{n+1} | \left<v,u\right>_{n,1} = 0 \: \forall u \in U \}$ Now I would like to show that if $U^\perp$ is spacelike then $U$ is timelike. I already know that the converse holds and one has to use the fact that $(U^\perp)^\perp=U$. But I seem to not be able to prove the statement above, even with this hint. Could someone help me out? Thanks!","['mathematical-physics', 'differential-geometry', 'semi-riemannian-geometry', 'linear-algebra']"
2557522,Probability of sum of product of uniform R.V.,"Suppose uniform[0,1] random variables $X_i$, $i=1,...,\infty$ are also independent. Whats the following probability? \begin{split}
P\Big(\sum_{n=1}^\infty\prod_{i=1}^nX_i<\infty\Big)
\end{split}","['uniform-distribution', 'statistics', 'probability']"
2557545,Show homeomorphism from upper (and lower) hemisphere to n-sphere.,"Hello , I am stuck on an exercise from my Algebraic Topology book and I am, unfortunately, unable to solve it (at least the (b) part, (a) were quite straight forward for me). Here is the problem: Consider the map $f:\mathbb{R}^{n+1}\to\mathbb{R}^{n+1}$ defined by
$$f(x_0,..,x_n)=(2x_0x_n,2x_1x_n,..,2x_{n-1}x_n,2x_n^2-1).$$ (a) Verify that $f$ restricts to a map from $S^n$ to $S^n$ (b) Show that the restrictions of $f$ to the upper and lower hemispheres, $$D_+^n=\{x\in S^n|x_n\geq 0\},\quad D_-^n=\{x\in S^n|x_n\leq 0\},$$ give rise to homeomorphisms $$D_+^n/\partial D_+^n\to S^n,\quad D_-^n/\partial D_-^n\to S^n$$ Solution: (a) Let $(x_0,..,x_n)\in\mathbb{R}^{n+1}$ such that $\sum_{i=0}^n x_i^2=1$. then $(2x_0x_n)^2+\cdots + (2x_{n-1}x_n)^2 + (2x_n^2-1)^2=4x_0^2x_n^2+\cdots + 4x_{n-1}^2x_n^2+4x_n^4-4x_n^2+1=4x_n^2\sum_{i=0}^n x_i^2-4x_n^2+1=4x_n^2-4x_n^2+1=1$. $\text{}$ This shows that $f$ restricts to a map from $S^n$ to $S^n$. (b) I think this should be easy too. But I think my biggest issue is that I don't really know what an element of $D_+^n/\partial D_+^n$ and $D_-^n/\partial D_-^n$ looks like. If someone could help me out with one of the cases perhaps I could work out the second one by myself. Also, I don't really understand why (a) was a part of the same problem. Should I use it to solve (b) ? Any help would be really appreciated. :)","['algebraic-topology', 'general-topology', 'homotopy-theory']"
