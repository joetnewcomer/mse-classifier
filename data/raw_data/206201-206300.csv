question_id,title,body,tags
4112390,Uses of Vieta Jumping in research mathematics?,"Vieta jumping has been a prominent method for solving Diophantine equations since 1988. It was popularized when it was used to solve an IMO problem, but has it been applied to research mathematics, and applied in solving previously unsolved questions? Context: Vieta Jumping is a method of showing that for quadratics $f$ and $g$ , if two positive integers $A, B$ satisfy $f(A,B)|g(A,B)$ , then there is a method to generate $A'$ such that $f(A',B)|g(A',B)$ where $A'$ is always larger/smaller than $A$ . We can then continue applying this method to generate infinitely many unique pairs of numbers of the form $(x,B)$ such that $f(x,B)|g(x,B)$ . The method is as follows: Suppose $f(A',B)|g(A',B)$ and $$\frac{g(A,B)}{f(A,B)}=k$$ for some positive integer $k$ . Then, this implies $$g(A,B)-kf(A,B)=0$$ We can then rewrite this as a quadratic in terms of $A$ . As this is a quadratic, there exists some $A'$ which is also a root of $g(x,B)-kf(x,B)=0$ . We can then use Vieta's formulae to show that $A'$ is a positive integer, and also that $A'$ is larger/smaller than $A$ . Hence, there are infinitely many solutions to the diophantine equation $\frac{f(x,y)}{g(x,y)}=k$ . If $A'\leq A$ , then Vieta Jumping can be used as a form of infinite descent. If $A'\geq A$ , then Vieta Jumping tells you that there are infinitely many solutions. Though Vieta Jumping was used in the solution for Problem 6 of the 1988 International Mathematics Olympiad, it has existed before that in various other names.","['number-theory', 'math-history', 'vieta-jumping', 'diophantine-equations']"
4112408,"$C[0,1]$ is separable: Theorem $11.2$ Carothers' Real Analysis","I have some questions about the proof of Theorem 11.2 of Carothers' Real Analysis. The theorem states that $C[0,1]$ is separable. Why is $\|f-g\|_\infty \le\epsilon$ ? Why is $h$ selected the way it is, i.e. $h(k/n) = f(k/n)$ for $k = 0,1,2...,n$ but with $h(k/n)$ rational and satisfying $|h(k/n) - g(k/n)| < \epsilon$ ? I think the reason we can do this is because $\mathbb Q$ is dense in $\mathbb R$ , but I don't understand the motivation. How do we get $\|g-h\|_\infty < \epsilon$ ? Is the proof similar to (1)? The proof is concluded with: ""The set of all polygonal functions taking only rational values at nodes $(k/n)_{k=0}^n$ for some $n$ is countable."" Well, why is that - and how does this show that $C[0,1]$ is separable? To show that $C[0,1]$ is separable, I would expect a countable dense subset of $C[0,1]$ . Where is this set? In case someone is curious as to what Exercise $1$ is, I'm typing it below: For each $n$ , let $Q_n$ be the set of all polygonal functions that have nodes at $x = k/n$ , $k = 0,1,...,n$ and that take on only rational values at these points. Check that $Q_n$ is a countable set and hence that the union of the $Q_n$ 's is a countable dense set in $C[0,1]$ .","['proof-explanation', 'separable-spaces', 'analysis', 'real-analysis']"
4112416,"Finding $\lim_{n\to\infty}\int_0^{\frac{1}{2^n}}(1+\cos(x))^n\,dx$","During our studies on Riemann Integrals we were given the following question in my Calculus II course: $$ \forall n\in\mathbb{N}: \text{let } f_n(x)=(1+\cos(x))^n, f_n:[0,1]\to \mathbb{R}$$ $$\text{ Find}: \lim_{n\to\infty}\int_0^{\frac{1}{2^n}}f_n(x)\, dx $$ I have tried finding the limit using the squeeze theorem. I noticed that the function is defined on $[0,1]$ , where $\cos(x)$ is positive and decreasing. I used the following boundaries: $$ 1\leq f_n\leq 2^n \implies \frac{1}{2^n} = \int_0^{\frac{1}{2^n}}1 \leq \int_0^{\frac{1}{2^n}}f_n(x) \leq \int_0^{\frac{1}{2^n}}2^n = 1  $$ but $ \displaystyle{\lim_{n\to\infty}}\frac{1}{2^n} = 0 $ which means I'm unable to apply the squeeze theroem. I've tried using tighter lower boundaries such as $1+\cos(1)$ But it didn't seem to work. I did some trial and error by calculaing the integrals manually for different values of $n$ and it does seem like $\displaystyle{\lim_{n\to\infty}}\int_0^{\frac{1}{2^n}}f_n(x) dx = 1 $ This means that somehow I should find a lower bundary which tends to 1 when $n\to\infty$ .
Any ideas on how to go forward from here are welcome.","['integration', 'limits', 'riemann-integration', 'sequences-and-series']"
4112460,"Existence of a subsequence in a set of positive measure for any sequence in [0,1]","The following is an exercise from Bruckner's Real Analysis: 5:12.2 Let $E$ be a Lebesgue measurable set of positive measure, and let ${\{x_n}\}$ be any sequence of points in the interval $[0, 1]$ . Show that there must exist a point $y$ and a subsequence ${\{x_{n_k}}\}$ so that $y + x_{n_k} \in  E$ for all $k$ . [Hint: Consider the functions $f_n(t)=χ_E(t − x_n)$ and their integrals.] I tried the hint of the book but it seems to be not useful: $\int_{[0, 1]} χ_E(t − x_n) d \mu = \mu (x_n+E) = \mu(E)$ . By LDCT, $\lim_n \int_{[0, 1]} χ_E(t − x_n) d \mu = \int_{[0, 1]} \lim_n χ_E(t − x_n) d \mu = \int_{[0, 1]} χ_E(t − x) = \mu (x+E) = \mu(E)$ ; as expected. But how does it guide to the existence of $y$ and a sub-sequence ${\{x_{n_k}}\}$ such that $y + x_{n_k} \in  E$ for all $k$ ? Also how there can be a $y \in [0, 1]$ if the set $E$ 'is spread enough' and includes both points ${\{0,1}\}$ ?","['measurable-sets', 'measure-theory', 'real-analysis']"
4112473,Is $TM$ a manifold with or without boundary when $M$ is a manifold with boundary?,"I am working on manifolds with boundary . Assume $(E, \pi)$ is a vector bundle over a manifold with boundary $M$ , for example $TM$ or $T^*M$ . If $\phi : \pi^{-1}(\mathcal{U}) \to \mathcal{U} \times \mathbb{R}^m$ is a vector bundle chart of $E$ and $\varphi : \mathcal{U} \to \mathbb{R}^n$ is a chart (with boundary) of the manifold $M$ , then we can (can we?) induce a manifold chart on $E$ : $$
\xi : \pi^{-1}(\mathcal{U}) \to \varphi(\mathcal{U}) \times \mathbb{R}^m : x \mapsto (\varphi(\pi(x)), t_{\pi(x)}^{-1}(x))
$$ where $t_p : \mathbb{R}^m \to \pi^{-1}(p)$ is the isomorphism between the vector space $\mathbb{R}^m$ and the fiber $\pi^{-1}(p)$ ( $t_p(v) := \varphi^{-1}(p, v)$ ). When $M$ is a manifold without boundary, we can then collect all the charts built in this way into an atlas and induce a manifold structure on $E$ . When $M$ is a manifold without boundary, is the construction above valid? Is $E$ equipped with this manifold structure a manifold with or without boundary? In other words, if $M$ is a manifold with boundary, is $TM$ a manifold with or without boundary?","['vector-bundles', 'tangent-bundle', 'differential-geometry']"
4112474,What is the direct sum of linear space and its dual space?,"For finite-dimensional linear space $L$ and its dual space $L^*$ , take as basis of $L$ the set of vectors $\{e\}$ , and as the basis of the dual space the dual basis $\{e^*\}$ . Given $y\in L^*,x\in L$ , denote $y(x)$ by the bracket $[x,y]$ . I wish to show that $[x,y]$ is a bilinear form in $K=L \oplus L^* $ , the direct sum of the linear space and its dual space. I thought this was an interesting path to take, because the theory of bilinear forms is nicely developed, but I haven't found any information about it. I'm also not sure that it's 'true' (i.e. that it generalizes in a useful way that maintains the bilinear property), because I'm not sure what the direct sum of these spaces is, and how it would play into the bilinear form described. It's clear that the linear space and its dual are isomorphic, so you could represent them by $n$ -tuples, $x=(\xi_1,\xi_2,...,\xi_n)$ , $y=(\eta_1,\eta_2,...,\eta_n)$ , where $[x,y]$ is defined by $$
[x,y] = \eta_1\xi_1+\eta_2\xi_2+...+\eta_n\xi_n
$$ But then we almost fall into the trap of thinking both $x$ and $y$ are in the same basis, which they are not. In the space $K$ , we have to take $$x=\xi_1e_1+\xi_2e_2+...+\xi_ne_n+0e^*_1+0e^*_2+...+0e^*_n$$ $$y=0e_1+0e_2+...+0e_n+\eta_1e^*_1+\eta_2e^*_2+...+\eta_ne^*_n$$ I'm just not sure how to interpret general vectors $z$ , and how to generalize the 'bilinear form' $[x,y]$ (quotes because it may not be a bilinear form for general vectors). Then there's the question of what $[x,x]$ or $[y,y]$ should mean. The whole of it comes down to the models underlying the vector spaces, so in order for this concept to be useful at all, the models need to generalize appropriately. I don't see any issue with the linear spaces themselves, apart from a partially defined mapping $[x,y]$ causing issues. I'm worried I'm going in the wrong direction though. Help! --Edit-- By a bilinear form in a linear space $K$ over a field $\mathcal{F}$ is meant a mapping $A:K\times K\rightarrow\mathcal{F}$ such that $A(x,y)$ for $x,y\in K$ is linear in either argument when the other is fixed.","['bilinear-form', 'linear-algebra', 'dual-spaces']"
4112490,"Evaluate the surface integral $\iint_\Sigma (y^2-z^2)e^{yz}\,ds$ using Stokes' Theorem","I found the following problem in a textbook: Evaluate $$\iint_\Sigma (y^2-z^2)e^{yz}\,ds\,,$$ where $\;\Sigma\;$ is given by $x^2+y^2+z^2=1$ , $\;z\geq0$ , by evaluating the rotation of the field $\;\mathbf{F}=(e^{yz}, 0, 0)$ . I evaluate the rotation to be $(0, ye^{yz}, -ze^{yz})$ , and I also see that the integrand can be written $(0,y,z)\cdot \operatorname{rot}(\mathbf{F})$ . From here I am not sure what to do next. Help appreciated!","['surface-integrals', 'multivariable-calculus', 'multiple-integral', 'stokes-theorem', 'vector-analysis']"
4112571,Calculate arc length of $\sqrt[3]{x^2} + \sqrt[3]{y^2} = \sqrt[3]{9}$,"It is necessary to calculate the length of the arc of the  curve below. There  have been attempts to raise to the 3rd power, a complex derivative and integral are obtained $$\sqrt[3]{x^2} + \sqrt[3]{y^2} = \sqrt[3]{9}$$","['integration', 'arc-length', 'multivariable-calculus', 'calculus', 'line-integrals']"
4112591,"Geodesic completeness of manifolds with ""warped"" metrics","Let $(M,g)$ be a geodesically complete Riemannian manifold, and let $f: M\to \mathbb{R}^+$ be a smooth, bounded strictly positive function on $M$ ; i.e., there exist $L,A\in \mathbb{R}^+$ such that $0<L<f(m)<A<\infty$ for all $m\in M$ . Since $f$ is positive, $f^2g$ defines a metric on $M$ . Since $(M,g)$ is geodesically complete, is $(M, f^2g)$ also geodesically complete? (Note: This question was inspired as a generalization of another question posed recently about the geodesic completeness of $\mathbb{R}\times \mathbb{S}^{n-1}$ , given a warped metric $ds^2=dr^2+\psi^2d\theta^2$ where $dr$ and $d\theta$ are the metrics from $\mathbb{R}$ and $\mathbb{S}^{n-1}$ respectively and $\psi$ is a positive function on $\mathbb{R}\times \mathbb{S}^{n-1}$ .)","['geodesic', 'metric-spaces', 'riemannian-geometry', 'differential-geometry']"
4112695,How do you find the time of the collision of circles moving under uniform circular motion?,"I am working on a simulation involving circular pucks moving in circular orbits at constant angular speed. I found the equation determining when the pucks collide to be intractable so have been calculating the time of collisions in a given time delta via a binary search which has worked adequately; however, I am by no means an expert in numerical techniques. I do believe that a closed form symbolic solution to this problem either does not exist or is so large and unwieldy it is worthless, but I wonder if there is a technique for finding a numerical solution that is more direct than the binary search approach I am using. If I am wrong about a symbolic solution being intractable I'd also be interested in that, obviously. The problem can be formalized as below. One puck is revolving around the origin with angular speed $\alpha_1$ , initial angular position $\theta_1$ , and radius of revolution $r_1$ . The other puck is revolving around $(c_x,c_y)$ with analogous properties. $d$ is the sum of the of the pucks' radii; $t$ is time; we are solving for time: $$ [r_1 \cos(\theta_1 + t \alpha_1) - (c_x + r_2 \cos(\theta_2 + t \alpha_2))]^2 + [r_1 \sin(\theta_1 + t \alpha_1) - (c_y + r_2 \sin(\theta_2 + t \alpha_2))]^2 = d^2$$","['trigonometry', 'systems-of-equations', 'numerical-methods']"
4112713,Supremum of the integral over the compact and convex set belongs the extreme set,"Let $T:X \to X$ be a continuous function on a compact metric space. We say that $\mu$ is $T-$ invariant if $\mu(T^{-1}(A))=\mu(A)$ for all $A \in \mathbb{B}_{X}$ . We denote by $M(X,T)$ the space of all $T-$ invariant measure which is a nonempty, compact and convex in weak* topology. Let $f:X \to \mathbb{R}$ be a continuous function. Denote $G(f)=\sup_{\mu \in M(X,T)} \int f d\mu$ . $\textbf{Question}:$ Why the supremum is always attained by a measure that belongs to the extreme set $M(X,T)$ ? My attempt: I think that is related to the fact $M(X,T)$ is compact and convex in weak* topology, but I don't know why the measure must belong the extreme set $M(X,T)$ ?","['measure-theory', 'ergodic-theory', 'functional-analysis', 'probability', 'dynamical-systems']"
4112764,Does measurability on product space implies strong measurability?,"In Giovanni's book, A First Course in Sobolev Spaces, he says Theorem 8.28. Let $I \subset \mathbb{R}$ be an open interval, let $E \subset \mathbb{R}^n$ be a Lebegue measurable set, and let $1 \le p < \infty$ . Then $L^p(I; L^p(E))$ can be identificed with $L^p(E \times I)$ . I understand most of the proof, except that given $u \in L^p(E \times I)$ , $v(t) := u(\cdot\,; t)$ is a strongly measurable in $L^p(E \times I)$ . Since he defines $T: L^p(E \times I) \to L^p(I; L^p(E))$ with $T(u) = v$ , it would not make sense if $v$ is not strongly measurable. I tried to define a sequence of simple functions in $L^p(I; L^p(E))$ to approximate $v$ , but no luck. I'd appreciate any help! Thank you.","['integration', 'measure-theory', 'real-analysis', 'functional-analysis', 'bochner-spaces']"
4112798,Limits of negative-power tower,"Consider the following: $$n \uparrow -\Bigl((n+1) \uparrow -\bigl((n+2) \uparrow \cdots \uparrow -m \bigr)\Bigr)= n^{{{-(n+1)}^{-(n+2)}}^{\cdots^{-m}}}$$ It doesn't converge for $m \to \infty$ , but eventually alternates between two values where the larger one occurs at even numbers of exponents because for $x \to \infty : n^{-x^{-x}} \approx n^0 > n^{-(n+1)^{-x^{-x}}} \approx n^{-1}$ . $\qquad$ For $2^{-3^{-4^{-5^\cdots}}}$ the limits are for example about $0.6903471$ and $0.6583656$ . Is there a closed form for the two limits of the power tower? Are the limits always irrational? Thanks in advance.","['elementary-number-theory', 'real-analysis', 'power-towers', 'limits', 'algebra-precalculus']"
4112801,Intuition behind surface of cone being the sector of a circle?,"While learning how to calculate the surface area of a cylinder, I saw that one of the methods relies on knowing that if we cut a cone, it is the sector of a circle. As shown in the image below: $\hskip2in$ I wasn't able to intuitively see this without getting scissors and paper and doing it myself. I'm wondering if there's any trick or mathematical method that can easily prove this fact without resorting to a ""crafting"" approach.","['circles', 'geometry']"
4112869,Oriented Angles in Euclidean Geometry,"I was recently introduced to the idea of  oriented angles in a proof of the inscribed angle theorem. While it makes sense to use the oriented angles  in that way (as you can look at them through modulo 180), it wasn‘t entirely obvious to me what the general advantages and disadvantages of oriented angles are. Are there things (lemmas, properties, etc.) in Euclidean Geometry which can only be used when working with unoriented angles? How about oriented angles? Can they just be used interchangeably? If the two cannot be used interchangeably, how does one notice which ones to use? What is the general advice for that? What are the general advantages/disadvantages of oriented angles? The proof also mentioned that oriented angles are more useful than unorientef angles in most cases. If that is the case, why are school students introduced to unoriented angles. Something looks off to me...","['euclidean-geometry', 'orientation', 'linear-algebra', 'geometry']"
4112877,"$\mathcal{S}$, smallest $\sigma$-algebra on $\mathbb{R}$ containing $\{(r,s]:r,s\in\mathbb{Q}\}$, is the collection of Borel subsets of $\mathbb{R}$","I have proved the following statement and I would like to know if my proof is correct, thank you: "" $\mathcal{S}$ , smallest $\sigma$ -algebra on $\mathbb{R}$ containing $\{(r,s]:r,s\in\mathbb{Q}\}$ , is the collection of Borel subsets of $\mathbb{R}$ "" DEF. (Borel set): The  smallest $\sigma$ -algebra on $\mathbb{R}$ containing  all  open  subsets  of $\mathbb{R}$ , $\mathcal{B}$ , is called the collection of Borel subsets of $\mathbb{R}$ . An element of this $\sigma$ -algebra is called a Borel set. LEMMA (1): Let $x\in\mathbb{R}$ : then there exists both a decreasing sequence and an increasing sequence of rational numbers converging to $x$ . LEMMA (2): Every open subset of $\mathbb{R}$ can be written as a countable union of disjoint open intervals. My proof: Let $(a,b), a<b$ be an open interval in $\mathbb{R}$ and take $c\in\mathbb{R}, a<c<b$ , arbitrary: then by Lemma (1) there exist a decreasing sequence of rational numbers $(a_i)$ such that $a_i\overset{i\to +\infty}{\to} a$ and an increasing sequence of rational numbers $(c_i)$ such that $c_i\overset{i\to +\infty}{\to} c$ so $(a_i, c_i]\in \{(r,s]:r,s\in\mathbb{Q}\}\subset\mathcal{S}$ and $(a,c]=\bigcup_{i=1}^{\infty} (a_i, c_i]\in S$ since $\mathcal{S}$ , being a $\sigma$ -algebra, is closed under countable unions. We thus have that every half-open interval $(a,c]\in\mathbb{R}$ belongs to $\mathcal{S}$ so it must also be that $(a,b)=\bigcup_{n=1}^{\infty}(a,b-\frac{1}{n}]\in S$ (i.e. every open interval in $\mathbb{R}$ also belongs to $\mathcal{S}$ ) hence, by LEMMA (2), every open set in $\mathbb{R}$ . So, since every open set belongs to $\mathcal{S}$ , a $\sigma$ -algebra, and $\mathcal{B}$ is the smallest $\sigma$ -algebra containing them by definition, we have that $\mathcal{B}\subset\mathcal{S}$ . Since each half-open interval with rational endpoints $(r,s]$ belongs to $\mathcal{B}$ ( $(r,s)\in\mathcal{B}$ because it is open, $\{s\}\in\mathcal{B}$ because closed sets are Borel sets so their union also belongs to $\mathcal{B}$ ) and $\mathcal{S}$ is the smallest $\sigma$ -algebra containing them by hypothesis we also have that $\mathcal{S}\subset\mathcal{B}$ thus $\mathcal{S}=\mathcal{B}$ , as desired.","['borel-sets', 'measure-theory', 'solution-verification']"
4113029,Find all subgroups of $\mathbb{Z}_3\oplus \mathbb{Z}_3\oplus \mathbb{Z}_3\oplus \mathbb{Z}_3$,"Let $G$ denote the group $\mathbb{Z}_3\oplus\mathbb{Z}_3\oplus \mathbb{Z}_3\oplus\mathbb{Z}_3$ , try to find all subgroups of it I think it may be quite similar to the extension of finite fields since $\mathbb{Z}_3$ is also a finite field, so we can regard $G$ as a four dimensional vector space over $\mathbb{Z}_3$ , so all subgroups are exactly subspaces? I wonder whether it’s correct or false, and how to use it to solve this problem.","['group-theory', 'abstract-algebra']"
4113127,Convergence of $\sum_{n=1}^{\infty} x\frac{\sin(n^2x)}{n^2}$. My try.,"Examine the convergence of: $$\sum_{n=1}^{\infty} x\frac{\sin(n^2x)}{n^2}$$ Pointwise convergence (for every $ x \in \mathbb{R}$ ): $\displaystyle \lim_{n \to \infty} x\frac{\sin(n^2x)}{n^2} = 0$ , because $\sin(n^2x) \in [-1, 1]$ Uniform convergence (for every $ x \in \mathbb{R}$ and $n \in \mathbb{N}$ ): form dirichlet criterion: $$\displaystyle \sum_{n=1}^{\infty} x\frac{\sin(n^2x)}{n^2} = \displaystyle \sum_{n=1}^{\infty} f_n(x) \cdot g_n(x) $$ where: $f_n(x) = \frac{\sin(n^2x)}{n} \leq 2$ which works for: $ \forall x \in \mathbb{R}, n \in \mathbb{N}$ $g_n(x) = \frac{x}{n} \implies \displaystyle \lim_{n \to \infty} g_n(x) = 0$ (monotonously) Therefore the series is uniformly convergent for $x \in \mathbb{R}$ . Is that correct?","['analysis', 'real-analysis', 'uniform-convergence', 'sequences-and-series', 'convergence-divergence']"
4113279,Is this computation of $\lim\limits_{x\to 0^+} x^x $ rigorous enough?,"$\lim\limits_{x\to 0^+} x^x $ For every $x\in (0,1)$ there exists $n_x\in \mathbb N$ such that $\dfrac 1{n_x+1}\lt x\lt \dfrac 1{n_x}$ It follows that $n_x\to \infty$ as $x\to 0^+$ (Refer Note ) $\displaystyle x^{\frac 1{n_x}}\lt x^x\lt x^{\frac 1{n_x+1}}\implies \left({\frac 1{n_x+1}}\right)^{\frac 1{n_x}}\lt x^x\lt \left(\frac{1}{n_x}\right)^{\frac 1{n_x+1}}$ and using $\lim\limits_{n\to \infty}n^{\frac 1n}\to 1$ it follows by squeeze theorem that $x^x\to 1$ as $x\to 0^+$ . Note: $(x\to 0^+)\implies \forall\delta\gt 0\, \exists x \gt 0\, x\lt \delta$ and by Archemedean property it follows that $\exists (n_x+1)\in \mathbb N\,\left(\dfrac 1{n_x+1}\lt \delta\right)\implies n_x+1\gt \dfrac 1\delta\\\implies n_x+1\to \infty\;\text{ as }\;\delta\gt 0\;\text{ is arbitrary.}$ Is the above enough if I don't want to use L'Hospital's rule or logarithm? Thanks.","['limits', 'solution-verification', 'real-analysis']"
4113297,The day I finally get the second sunny day I leave. What is the probability that I stayed exactly one week?,"Each day in Iceland, it rains with a probability p=0.8. Denote X the number of days I stay until I've had 2 sunny days in my holidays. What is the p.m.f. of X? The day I finally get the second sunny day I leave. What is the probability that I stayed exactly one week? My Attempt I know I can model this if it was only one sunny day, but I'm not sure how to do it if it were two. For one sunny day, I can model this as a geometric distribution, which is: $X\sim Geom(p)$ where $p=0.8$ . So I need to find the probability of staying for $7$ days: $$\mathbb{P}(X=k)=(1-p)^{k-1}p=(1-0.2)^{7-1}\cdot 0.2$$ But how would I compute it if I wanted to see two sunny days?",['probability']
4113352,Evaluate $\int _0^{\frac{\pi }{2}}x\cot x\ln ^2(\cos x)\:dx$,"I want to evaluate $$\int _0^{\frac{\pi }{2}}x\cot (x)\ln ^2(\cos x)\:dx $$ but it's quite difficult.
I have tried to rewrite the integral as $$\int _0^{\frac{\pi }{2}}x\cot \left(x\right)\ln ^2\left(\cos \left(x\right)\right)\:dx=\frac{\pi }{2}\int _0^{\frac{\pi }{2}}\tan \left(x\right)\ln ^2\left(\sin \left(x\right)\right)\:dx-\int _0^{\frac{\pi }{2}}x\tan \left(x\right)\ln ^2\left(\sin \left(x\right)\right)\:dx$$ I've also tried to integrate by parts in multiple ways yet I cant go forth with this integral, I also tried using the substitution $t=\tan{\frac{x}{2}}$ but cant get anything to work, I'll appreciate any sort of help. I also tried using the classical expansion $$\ln \left(\cos \left(x\right)\right)=-\ln \left(2\right)-\sum _{n=1}^{\infty }\frac{\left(-1\right)^n\cos \left(2nx\right)}{n},\:-\frac{\pi }{2}<x<\frac{\pi }{2}$$ But it only gets worse.","['integration', 'calculus', 'definite-integrals', 'trigonometric-integrals']"
4113478,"Integration is a compact operator on $L^p([0, 1])$","Let $p \in [1, \infty]$ . I want to prove that this integral operator is compact: $$
T_p: L^p([0, 1]) \to L^p([0, 1]), \quad T(f(x)) := \int_0^x f(t)dt
$$ I can prove it for $L_1$ case and I can prove the following facts: $L_p([0, 1]) \subset L_1([0, 1])$ for any p (including $\infty$ ) Jensen's inequality (for convex $\phi(t) = t^p$ ) gives me $\lVert x \rVert_1^p \leq \lVert x \rVert_p^p$ for all $x \in B_{1, L_p([0, 1])}$ hence $B_{1, L_p} \subset B_{1, L_1}$ $\forall x \in L_p([0, 1])$ , $T_p(x) = T_1(x)$ (in sence of inclusion in (1)), hence we can say that $T_p(B_{1, L_p}) \subset T_1(B_{1, L_1})$ What I want to say: Compact operator send unit ball to totally bounded set. $\forall p \in [1, \infty)$ , $T_p(B_{1, L_p}) \subset T_1(B_{1, L_1})$ and last one - totally bounded. Hence - the first one set is also totally bounded Is it a correct way to proof compactness of integral operator $T_p$ ? Here are 3 open questions I can't fix right now: $T_1(B_{1, L_1})$ totally bounded, but for fix $\epsilon > 0$ we can't take epsilon-net from $T_1(B_{1, L_1})$ and say it is also epsilon-net for $T_p(B_{1, L_p})$ , since first epsilon-net consists from element from $L_1$ and last one should consists from element from $L_p$ . Do we have some density conditions on $L_p$ inside $L_1$ ? I saw only this Why is $L^{1} \cap L^{\infty}$ dense is in $L^{p}$? First epsilon-net also use norm from $L_1$ , so even if $L_p$ dense in $L_1$ (all spaces take place on interval $[0, 1]$ ), we need equivalence of norms in both spaces (I know, that norm in $L_p$ and $L_q$ doesn't equivalent untill $p \neq q$ : Are all the norms of $L^p$ space equivalent? but thats true for functions on real line, in my example functions defined on interval $[0, 1]$ , so maybe in that smaller space they equivalent?) I didn't fully ""proof"" case when $p = \infty$ , I can't give an easy proof of balls inclusion from (2) Can you give any hint how I can fix my proof of compactness? Or maybe some countreexamples/hints how I can proof main statement about compactness of $T_p$ ?","['integral-operators', 'lp-spaces', 'compact-operators', 'functional-analysis']"
4113483,$yy''-(y')^2=-y^2$,"We want to solve the differential equation : $yy''-(y')^2=-y^2$ $y(0)=1, y'(0)=0$ $ \left(\frac{y'}{y}\right)'=\frac{yy''-y'^2}{y^2}=-1$ therefore $y(x) =e^{ \frac{-x^2}2}$ How would you solve it, without trick, with a general method ?","['ordinary-differential-equations', 'real-analysis']"
4113545,"Proving $\int_{1}^{\infty}\int_{0}^{1}\int_{0}^{1}\frac{dz\,dy\,dx}{x(x+y)(x+y+z)} = \frac{5\,\zeta(3)}{24}$","I'm studying advanced analytic number theory and the following identity surprised me the most: $$\int_{1}^{\infty}\int_{0}^{1}\int_{0}^{1}\dfrac{dz\,dy\,dx}{x(x+y)(x+y+z)} = \dfrac{5\,\zeta(3)}{24}$$ My idea is to use triple integral representation of $\zeta(3)$ , that is $$\zeta(3) = \int_{0}^{1}\int_{x}^{1}\int_{y}^{1}\dfrac{dx\,dy\,dz}{(1-x)\,yz}$$ however it appears to be a rather tricky problem to relate the triple integral of $\zeta(3)$ . Integrating with respect to $x$ we get $$\int_{1}^{\infty}\int_{0}^{1}\int_{0}^{1}\dfrac{dz\,dy\,dx}{x(x+y)(x+y+z)} =\int_{0}^{1}\int_{0}^{1}\left(\dfrac{\log(y+1)}{yz} - \dfrac{\log(y+z + 1)}{z(y+z)}\right)dy\,dz$$ I'm unable to make any further progress and I think integrating this way doesn't get us anywhere so I'd not recommend anyone to work on this problem as I did (by integrating as above). Out of curiosity: Can we prove the required using contour integration or residue theorem? I'm in search of a neat detailed answer with proper references for tools used directly. Thanks in advance.","['integration', 'riemann-zeta']"
4113562,"Prove ABC,A'B'C' are congruent:D is on BC,D' is on B'C', $\angle BAD \angle CAD= \angle B'A'D' \angle C'A'D', AB=A'B', AC=A'C', AD=A'D'$","In $\triangle ABC$ and $\triangle A'B'C'$ , $D$ is a point on line segment $BC$ and $D'$ is a point on line segment $B'C'$ . $\frac{\angle BAD}{\angle CAD}=\frac{\angle B'A'D'}{\angle C'A'D'}$ , $AB=A'B'$ , $AC=A'C'$ and $AD=A'D'$ . How to prove that $\triangle ABC \cong \triangle A'B'C'$ ? If $AD$ and $A'D'$ are angle bisectors, the question is much easier: $BD:CD=B'D':C'D'$ (angle bisector theorem), then $\triangle ABC \cong \triangle A'B'C'$ is proved by constructing a pair of similar triangles. But I'm stuck on the general question for days.","['congruences-geometry', 'euclidean-geometry', 'analytic-geometry', 'geometry', 'triangles']"
4113586,realization of an operator,"In [s. Cerrai, Normal deviations from the averaged motion for some reaction–diffusion equations with fast oscillating perturbation] it is given a reaction diffusion PDE with operator $\mathcal{A}$ and some Dirichlet Boundary conditions. Then  it's written as a differential equation in abstract form on some Hilbert space $H$ with operator $A$ which is the realization of $\mathcal{A}$ in $H$ endowed with Dirichlet boundary conditions. What do we mean with the realization of $\mathcal{A}$ in $H$ ?","['functional-analysis', 'partial-differential-equations']"
4113597,Prove $|\int_{a}^{b}f(x)dx| \le \int_{a}^{b}|f(x)|dx $,"I want to prove that $|\int_{a}^{b}f(x)dx| \le \int_{a}^{b}|f(x)|dx $ We know that $-|f(x)| \le f(x) \le |f(x)|$ , so by linearity, we get: $$\int_{a}^{b}-|f(x)|dx \le \int_{a}^{b}f(x)dx \le \int_{a}^{b}|f(x)|dx$$ And: $$-\int_{a}^{b}|f(x)|dx \le \int_{a}^{b}f(x)dx \le \int_{a}^{b}|f(x)|dx$$ But how can I conclude that the statement is correct from here? Thanks!","['integration', 'calculus', 'functions', 'solution-verification', 'inequality']"
4113617,Understanding bijection between $2^X$ and $P(X)$.,"I'm trying to understand this bijection between 2^X and P(X) : Let us define $f: P(X) \rightarrow \{0,1\}^X$ by $f(\alpha) = g_{\alpha}$ where $\alpha \in P(X)$ and $g_{\alpha} : X \rightarrow \{0,1\}$ is defined by $$g_{\alpha}(x) =\begin{cases} 
      1 & x \in \alpha \\
      0 & x \in X-\alpha 
   \end{cases}$$ In that question, everyone believes that $g_{\alpha}$ is a bijection... I tried to show that this function is not injective with an example : Lets denote: $$ X = \{a,b,c\}$$ So: $$P(X)=\{\{a\},\{b\},\{c\},\{a,b\},\{a,c\},\{b,c\},\{a,b,c\},∅\}$$ We can conclude if ${\alpha}$ = $\{a,c\}$ : $$f(a)=1,f(b)=1$$ But $a≠b$ Am I wrong? If yes , could someone please explain it to me?","['elementary-set-theory', 'functions', 'solution-verification']"
4113651,Intuition for formula of variance of Brownian Motion conditioned on two endpoints,"I am familiar with the following interpolation property of Brownian Motion (this is essentially a theorem about a Brownian Bridge): Theorem. Let $W$ be a standard Brownian Motion, and $0<t_1 < t < t_2$ . Then the conditional distribution of $W_t$ given $W_{t_1}=x_1$ and $W_{t_2}=x_2$ is $N(\mu, \sigma^2)$ with $\mu = x_1 + \frac{t-t_1}{t_2-t_1}(x_2-x_1)$ and $\sigma^2 = \frac{(t_2-t)(t-t_1)}{t_2-t_1}$ . I have a good intuition for the mean: $\mu$ is the linear interpolation of the values $(t_1, x_1), (t_2, x_2)$ at time $t$ . However, I have no intuition for where this $\sigma^2$ comes from, other than it just pops out of the proof of the theorem. I've noticed $\sigma^2$ can be written as $(t_2-t_1)\alpha(1-\alpha)$ if we write $t$ as a convex combination $t = \alpha t_1+(1-\alpha)t_2$ if that helps. My question: Is there an intuitive reason for this formula beyond ""it just falls out in the proof""?","['stochastic-analysis', 'probability-distributions', 'brownian-bridge', 'brownian-motion', 'probability-theory']"
4113686,Name for Set Of Functions Smoothly Interpolating Between Points,"Suppose I have a continuous differentiable surface such as the following: $$f(x,z)=\sin(e^{2\pi+x^2+z^2}), \space\space\space x,z\in\mathbb{R}$$ As $x$ and $z$ become large, the function oscillates faster and faster. You can see that in the corners of this image: Now suppose I only evaluate $f$ at some grid of points instead of the whole plane. In other words, at: $$x,z\in{\bigcup_{k=0,\space 2n}\{r(k-n)\}}\space\space n\in\mathbb{N}\space\space r\in(0,\infty)$$ for some given $n$ and $r$ . These points are evenly spaced apart on the $xz$ plane, but as $f$ oscillates more and more, the height of the points becomes less predictable (visually). Now suppose I fit a smooth function $g$ to those points. Here's an animation showing how as I increase $n$ , $g$ changes. You can see how sensitive it is near the edges: https://youtu.be/HtH3YCqR-Pw Is there a name for the set of functions that $g$ belongs to (a set of functions that smoothly interpolate through a set of points)? I'm not really sure what I would call it. Would $g$ be ""a smooth continuous interpolation of a uniform discrete sampling of $f$ ?"" I don't think that sounds right since it implies the points $f$ was evaluated at are random. But I'm not really sure what else to call it.","['interpolation', 'surfaces', 'smooth-functions', 'multivariable-calculus', 'derivatives']"
4113701,What am I writing when I write $\mathbf X \mid \mathbf Y$?,"Suppose $\mathbf X$ is a random variable and $A$ is an event in the same probability space $(\Omega, \mathcal F, \Pr)$ . (Formally, $\mathbf X$ is a function on $\Omega$ , say $\Omega \to \mathbb R$ ; $A$ is a subset of $\Omega$ .) I am comfortable writing $\mathbf X \mid A$ to condition $\mathbf X$ on $A$ . This can be defined as another random variable on a different probability space: replace $\Omega$ by $A$ , $\mathcal F$ by $\{S \cap A : S \in \mathcal F\}$ , and measure $\Pr[\,{\bullet} \mid A]$ . Then, just let $\mathbf X \mid A$ have the same value as $\mathbf X$ on every $\omega \in A$ . With this definition, the conditional expectation $\mathbb E[\mathbf X \mid A]$ is just the ordinary expectation of this new random variable $\mathbf X \mid A$ . I am less comfortable with a different notation, which is what this question is about: Suppose $\mathbf X, \mathbf Y$ are two random variables in the same probability space. It might be convenient to describe their joint distribution as ""choose $\mathbf Y$ , then choose $\mathbf X$ in a way that depends on $\mathbf Y$ "". For example, we flip $10$ coins and let $\mathbf Y$ be the number of heads; we flip those $\mathbf Y$ coins again and let $\mathbf X$ be the number of heads. We can write this distribution as $$
    \mathbf Y \sim \textit{Binomial}(10, \tfrac12) \qquad \mathbf X \mid \mathbf Y \sim \textit{Binomial}(\mathbf Y, \tfrac12).
$$ This notation has some nice features. If $\mathbf Z \sim \textit{Binomial}(n,\frac12)$ for a constant $n$ , then $\mathbb E[\mathbf Z] =  \frac12 n$ . Here, we can pretend that we're in the same boat and write $\mathbb E[\mathbf X \mid \mathbf Y] = \frac12\mathbf Y$ , which is correct as a description of the random variable $\mathbb E[\mathbf X \mid \mathbf Y]$ . But is $\mathbf X \mid \mathbf Y$ really any kind of random variable (or other object) on its own, or is this just abuse of notation? Note: I will deal with $\int$ symbols if I must, but if I get an answer just for discrete random variables where these don't show up, that's fine by me.","['conditional-expectation', 'notation', 'definition', 'probability-theory', 'random-variables']"
4113706,What is crossMatrix and what does it do?,"I read a code about 3x3 matrix related calculation, and there is no comment for all the code. There is a crossMatrix function, which, I don't understand. It is defined as: void Matrix3x3::v31xv31_crossMatrix(Vector inV, double diag, Matrix3x3 *poutM) {
      poutM->m33_setZero(poutM);
      poutM->m[0] = diag;
      poutM->m[4] = diag;
      poutM->m[8] = diag;
      poutM->m[3] = inV.m[2];
      poutM->m[1] = -inV.m[2];
      poutM->m[6] = -inV.m[1];
      poutM->m[2] = inV.m[1];
      poutM->m[7] = inV.m[0];
      poutM->m[5] = -inV.m[0];
  } By writing elements as matrix in math, it looks like this: diag    -m[2]    m[1]
   m[2]     diag   -m[0]
  -m[1]     m[0]    diag I'm not familiar with this form. Any idea what this matrix is called? What does it do?",['matrices']
4113746,Isomorphism of products in a category: Does it involve the axiom of choice?,"Let $(X_i), (Y_i)$ be two families of objects indexed by the same index set $I$ in a category with products. It seems obvious that if $X_i, Y_i$ are isomorphic for all $i$ , then also the products $\prod_{i \in I} X_i$ and $\prod_{i \in I} Y_i$ are isomorphic: We know that for each $i$ there exists an isomorphism $h_i : X_i \to Y_i$ . Then $\prod_{i \in I} h_i : \prod_{i \in I} X_i \to \prod_{i \in I} Y_i$ is an isomorphism. But doesn't this involve the axiom of choice if $I$ is infinite? We have to make infinitely many choices to get a family $(h_i)$ of isomorphisms. Or can AC be avoided?","['elementary-set-theory', 'axiom-of-choice', 'category-theory']"
4113747,Solving integral: $\int_0^1\ln^2{\left(\frac{1+x}{1-x}\right)} dx$,"I want to show that the solution of a BVP: $\ u(x) = \ln{\frac{1+x}{1-x}}$ is in $L^2(0,1)$ , so I need to show that the integral $$\int\limits_0^1\ln^2|\frac{x+1}{x-1}|dx < \infty$$ Just looking at the function, however, it's not even defined in the interval $[0,1]$ , right? So can this not actually be a solution to the BVP? Even more generally about the integral itself, does that make it just $0$ ? And if not, can someone explain how an integral of a function can be defined in a region when the function itself is not? Also, I know that there exist integral calculators online and have looked this up, specifically using: https://www.integral-calculator.com/ If you put in the given integral it says the integral could not be found and then gives a complex number approximation. What should be made of this? I have taken a complex variables course but don't really see how you could solve this using Residue Calculus since we don't have any symmetries. EDIT: Note I had originally forgot the absolute value signs - my mistake.","['integration', 'lp-spaces', 'definite-integrals']"
4113751,Prove that $(n+1)^n < n^{n+1}$ for all $n>3$,"Prove that $(n+1)^n < n^{n+1}$ for all $n>3$ At $n=4$ , $$5^4<4^5$$ which is indeed true. By mathematical induction, we need to prove that $$(n+2)^{n+1} < (n+1)^{n+2}$$ $$\implies (n+2)^n\times (n+2) < (n+1)^n\times(n+1)^2 $$ I am not getting how to proceed further than this. Any hints or help would be highly appreciated! Thanks.","['contest-math', 'algebra-precalculus', 'induction']"
4113781,Why does the lower right derivative of a Brownian motion at a fixed $t \ge 0$ is $ - \infty$?,"I am reading Peter Morters and Yuval Peres's book Brownian Motion. In theorem 1.27, it
is proved that for a fixed $t \ge 0,$ almost surely, the Brownian motion $B(t)$ is not differentiable at $t$ and that $D^*B(t)=+ \infty$ and $D_*B(t)=-\infty.$ First we consider the Brownian motion $X(t) := tB(1/t)$ obtained from the time inversion of $B(t).$ Then $$D^*X(0) = \limsup_{h \downarrow 0} \frac {X(0 + h)-X(0)}{h} \ge \limsup_{n \to \infty} \frac {X(1/n)-X(0)}{1/n} \ge \limsup_{n \to \infty} \sqrt n X(1/n) = \limsup_{n \to \infty} B(n)/\sqrt n=+\infty.$$ (It is already stated before in the book that $\limsup_{n \to \infty} B(n)/ \sqrt n = + \infty$ and $\liminf_{n \to \infty} B(n)/ \sqrt n = - \infty.$ ) If I employ the same strategy for $D_*X(0)$ , then I want the inequalities as obtained above to be reversed for $\liminf$ . But it can be seen that the second inequality doesn't reverse! (it is only based on  the inequality $n \gt \sqrt n$ ). So in this way I can't show that $D_*X(0) = \liminf_{h \downarrow 0} \frac {X(0 + h)-X(0)}{h} \le \liminf_{n \to \infty} B(n)/ \sqrt n.$ So what it is it that I am missing here? Moreover, we want $D_*X(0) = D_*B(t).$ In the book, for a fixed $t \ge 0$ , we define $X(s)=B(t+s)-B(t)$ . But how is that possible? Isn't $X(s) := sB(1/s)?$ (I have this very same doubt for showing $D^*X(0)=D^*B(t).$ )","['proof-explanation', 'limsup-and-liminf', 'derivatives', 'brownian-motion', 'probability-theory']"
4113796,A question related to Frattini subgroup,"I am doing a question which need the following statement as a lemma: Statement: If $G$ is a group with a finitely generated Frattini subgroup $\Phi(G)$ , then the only subgroup $H$ of $G$ such that $H\Phi(G) = G$ is $H = G$ . The proof of this statement on my book is indirect and using the following theorem: Theorem: The Frattini subgroup of a nontrivial group $G$ is the set of all elements $x$ in $G$ with the property that, whenever a set $K\cup \{x\}$ generates $G$ , $K$ generates $G$ . However, my proof of the statement above is rather simply and more directly. So I am afraid there would be some mistake in my proof so I want someone to check it. Thanks! My proof of the statement: Suppose there exist a maximal subgroup $M$ such that $H \subset M \subset G.$ Then since $M$ is maximal, $\Phi(G) \subset G$ , so $G = H\Phi(G) \subseteq M \subset G$ . So $H$ is a maximal subgroup not equal to $G$ or $H = G$ . The former case is impossible since if it does then $G = H\Phi(G) = H$ , a contradiction. So finally we have $H = G.$","['group-theory', 'frattini-subgroup', 'finite-groups']"
4113801,How to show this function is bijective?,"I want to show $$ (0,1)\sim(0,1)∩(R-Q) $$ And after reading and thinking about this answer : (1) Choose an infinite countable set of irrational numbers in $(0,1)$ , call them $(r_n)_{n\geqslant0}$ . (2) Enumerate the rational numbers in $(0,1)$ as $(q_n)_{n\geqslant0}$ . (3) Define $f$ by $f(q_n)=r_{2n+1}$ for every $n\geqslant0$ , $f(r_n)=r_{2n}$ for every $n\geqslant0$ , >and $f(x)=x$ for every irrational number $x$ which does not appear in the sequence  > $(r_n)_{n\geqslant0}$ . Let me suggest you take it from here and show that $f$ is a bijection between $(0,1)$ and $(0,1)\setminus\mathbb Q$ . I wanted to show f is bijection (injective + surjective) ,and first I tried to show thats injective: I must show : $$(r_{2n+1}=r_{2n+1}')~→~(q_n=q_n')$$ And: $$(r_{2n}=r_{2n}')~→~(r_n=r_n')$$ And third one is obvious. But I stucked here because this function is new to me...and I cannot show the relation between $r_{2n+1}$ and $q_n$ in order to conclude injection statements . And moreover , because of this problem(relation understanding) I cannot show thats a surjection too... Could someone help me please?","['elementary-set-theory', 'functions', 'solution-verification']"
4113853,Solution of Differential equation that provides infinitely many solutiions,"$y''+y = 0$ $y(0) = 1, y(k) = 1 $ Find the value of $k$ that the given problem has infinitely many solutions. a) $\pi + 2n \pi$ b) $2n \pi $ c) $\dfrac{\pi}{4} + 2n \pi $ d) $ \dfrac{\pi}{2} + 2n \pi $ My work : The general solution is, $y(t) = C_1 \cos t + C_2 \sin t $ $y(0)  = 1 $ $y = \cos t + C_2 \sin t $ $y(L) = 1 $ $\cos L + C_2 \sin L = 1$ $C_2 = \dfrac{1- \cos L }{\sin L }$ To get $C_1 = 1$ $\cos L = 1 $ $ L = 2 k \pi , k \in N $ Is this approach correct? I have confusion with 4th option as well because, $\dfrac{1- \cos L }{\sin L } = 1$ $\tan (L/2) = 1 $ $ L = \dfrac{\pi}{2} + 2n \pi $ Which option is the perfect match? Any suggestions would be appreciated.",['ordinary-differential-equations']
4113873,"Is there a way to unify curl, gradient, and divergence operators as special cases of a more general operator?","I realize there's probably an answer to this question somewhere on this site, but it would seem I'm having trouble picking the right search terms. In my multivariable calculus class, the professor just spent some time going over the relationships between the integral theorems of calculus by essentially saying that they all are examples of how integrating some function over the boundary of a domain (path, surface, solid) is the same as integrating some form of its ""derivative"" over the interior of that domain. The issue is, while the boundary operator seems to be consistent across all of the integral theorems presented (line integral theorem, Stokes' theorem, and Gauss' theorem), the notion of the derivative is different in each one: for Stokes', it's the curl; for Gauss', it's the divergence; and for the line integral theorem, it's the gradient. I was wondering: is there any natural way of generalizing the curl, divergence, and gradient operators, so that they become special cases of some more broad notion of ""taking the derivative""? Or perhaps to other dimensions? (Or is there something special about $\mathbb R^{3}$ that makes it unique for vector calculus?) (Sorry if this question is too open-ended or ill-defined for this site; regardless, if anybody has any suggestions on topics to study or other resources to consult, I would very much appreciate those as well.)","['multivariable-calculus', 'vector-analysis']"
4113876,Marginal distribution of a normalized vector of Gaussians,"Imagine I have a vector $\mathbf{x} = [x_1, \dots, x_N]^\top$ where each of the $x_n \sim \mathcal{N}(0, 1)$ . Its $\ell_2$ -norm is $||\mathbf{x}||_2 = \sqrt{x_1^2 +\dots + x_N^2}$ . I am interested in the distribution of the marginals of the normalized $\mathbf{y} = \Big[\frac{x_1}{||\mathbf{x}||_2}, \dots, \frac{x_N}{||\mathbf{x}||_2}\Big]^\top$ . Is there a way to specify the distribution of the individual $\frac{x_n}{||\mathbf{x}||_2}, n\in[N]$ at all? By simulation, it seems that at least when $N$ grows, the marginals behave like Gaussian random variables with $\mathcal{N}(0, N^{-1})$ . Or is that effect solely due to an approximate independence between numerator and denominator for large $N$ , which makes the marginals act like a student-t random variable? So far, I have found plenty on the distribution of the entire vector, for example here: On the distribution of a normalized Gaussian vector .","['statistics', 'marginal-distribution', 'normal-distribution', 'probability', 'random-variables']"
4113887,Prove equality of products and intersections of subgroups,"Here is the problem I am working on: Suppose $H$ , $K$ , and $N$ are subgroups of group $G$ with $H\le N$ . Prove that $(HK)\cap N=H(K
\cap N)$ . (The notation $HK$ refers to the subgroup made up of elements of the form hk where $h\in H$ and $k\in K$ .) My work so far: Let $x\in (HK) \cap N$ , which means that $x$ is in both $HK$ and $N$ . In other words, $x=n \in N$ and $x=hk$ for $h \in H$ and $k \in K$ , so $hk=n$ . Now let $x\in H(K\cap N)$ , which means that the products of any $h\in H$ and any $m$ in both $K$ and $N$ is in this subgroup, in other words $m=k\in K$ and $m=n\in N$ . Each $x\in H(K\cap N)$ satisfies $x=hm$ . I don't know how I can get from $(HK)\cap N$ to $H(K\cap N)$ or vice versa; in fact, I am having trouble believing intuitively that the statement I'm supposed to be proving is even true. Am I missing something very obvious?","['group-theory', 'abstract-algebra']"
4113919,Derivative of sandwich product of Geometric Algebra rotor and vector wrt that rotor.,"How do I find the derivative (differential?) of the sandwich product of a rotor, with respect to the rotor? Namely, given a rotor $R$ , and vector $v$ , what is $\frac{\text{d}}{\text{d}R}(Rv\tilde{R})$ ? I've seen how this is done using matrix representations of quaternions, but I'd like to know how to do this staying in Geometric Algebra.  I'd like the answer as a (multi?)vector.","['geometric-algebras', 'exterior-derivative', 'differential-geometry']"
4113937,"If 10 needles are dropped on the floor, how many needles are expected to cross a line?","If 10 needles are dropped on the floor, how many needles are expected to cross over one of the lines? This is a variation of the Buffon Needle problem (short needle). I believe that the probability of a singular needle dropping is $\frac{2L}{\pi}$ , where L is the length of the needle. Would I simply multiply it together 10 times? I'm not quite sure what to do?","['statistics', 'probability']"
4113990,Real roots of $f(x)=8x^3-6x+1$ given in complex form,"The function $f (x) = 8x^3-6x + 1$ has three real roots as it is easy to check (a quick way is to see the graph of f). However, Wolfram gives  the following exact values as roots: There is no Wolfram error here because if asked for approximate root values, it gives the real numbers $-0.93969,0.17365$ and $0.76604$ . In sum, each of the three exact values when simplified must give a real number. I have tried to find at least one of these three values but it was not possible in a first attempt. Can someone describe a method to do it?",['algebra-precalculus']
4114001,What are path and contour integrals?,"I am very confused on what path and contour integrals actually represent. I tried looking for an answer on Google but the only examples were related to Quantum Mechanics. For instance, real Riemann integrals can be thought as ""the area under the curve"". What about path/contour integrals? Also, is there a difference between path and contour integrals, or are they the same thing? I've been thinking of contour integrals as path integrals where the path is a ""closed loop"", but I'm not sure if that's the case. It's all so confusing, and not being able to visualise these concepts makes it feel like I'm just memorising formulas and theorems without any reason. I have been thinking about path integrals as integrating over a curve in three dimensions. For example, in an $xyz$ coordinate system (with $y$ pointing “up”), we have a path on the $x$ -"" $z$ "" plane and integrate over that path. That is, instead of integrating over and along "" $y=0$ "", we integrate over a certain path. But I don't know if this is a correct way to think of path integrals. This issue has been frustrating me a lot because I'm now studying theorems that involve path/contour integrals and I just can't understand what they really do or mean. By path integral I mean $$\int_{\gamma\vert_{[a,b]}}f(z)\hspace{0.2em}dz = \int_{a}^{b}f(\gamma(t))\cdot\gamma'(t)\hspace{0.2em}dt$$ where $f:U\subseteq\mathbb{C}\rightarrow\mathbb{C}$ is a function that satisfies all conditions for complex integration and $\gamma:[a,b]\rightarrow U\subseteq\mathbb{C}$ is at least a piece-wise regular path. Any help would be appreciated.","['integration', 'complex-analysis', 'contour-integration', 'complex-integration']"
4114049,"If $A<B$, does it follow that $A^2\leq B^2$?","Suppose $A$ and $B$ are positive semidefinite matrices satisfying $A\leq B$ (meaning that $x^TAx \leq x^TBx$ for any vector $x$ ).  Does it follow that $A^2\leq B^2$ ?  It certainly follows if $A$ and $B$ commute, but beyond this case I am not so sure -- I suspect there is a counterexample.","['positive-semidefinite', 'matrices', 'linear-algebra', 'symmetric-matrices', 'positive-definite']"
4114072,Using Chernoff's inequality,"Let $X$ denote the number of pairs of $HH$ in a sequence of 1000 coin tosses where we also consider the pair of the last and the first coin toss (cyclic). I'm asked to use Chernoff's inequality (the one which is only applicable for bernoulli variables) so I can't do that directly since $X$ is not distributed binomially. The tip is to split $X$ into pairs of tosses $i$ and $i+1$ where one time $i$ is even and the other time $i$ is odd. Clearly, those two expirements are distributed binomially, but how can I use this fact to use Chernoff's inequality to estimate something of the form $P(X\geq (1 + \delta)E[X])$ whereby I know that $X = X_{i \text{ is even}} + X_{i \text{ is odd}}$ so $E[X] = E[X_{i \text{ is even}}] + E[X_{i \text{ is odd}}]$ .","['expected-value', 'statistics', 'binomial-distribution', 'probability']"
4114105,Hartshorne's exercice V.1.2: $\deg C=C.H$,"Let $H$ be a very ample divisor on a surface $X$ and let $C$ be any curve in $X$ . The divisor $H$ gives an embedding $X\to \mathbb{P}^n$ which gives sense in the degree of the curve $C$ : it is the highest coefficient of the Hilbert polynomial (ie $n\mapsto \chi(\mathcal{O}_C(n))$ ) of $C$ embedded in $\mathbb{P}^n$ . The exercice V.1.2 in R. Hartshorne book ask to prove that $\deg(C)=C.H$ where $C.H$ is the intersection number of $H$ and $C$ in the surface $X$ . I can't see how to prove that. An idea: as the Hilbert polynomial of $C$ in $\mathbb{P}^n$ is in fact linear then $\deg(C)=\chi(\mathcal{O}_C(1))-\chi(\mathcal{O}_C)$ but with $i:C\to X$ we have that $$ \mathcal{O}_C(1)=i^*\mathcal{O}_X(1)=i^*\mathcal{O}_X(H)=\mathcal{L}(H)\otimes_{\mathcal{O}_X}\mathcal{O}_C $$ so that $$ \deg(C)=\chi(\mathcal{L}(H)\otimes\mathcal{O}_C)-\chi(\mathcal{O}_C) $$ and it is proved before that $$ C.H=\deg_C(\mathcal{L}(H)\otimes\mathcal{O}_C) $$ where here $\deg_C$ is the degree of the associated divisor, so that (if what I say is correct) one has to prove that $$ \chi(\mathcal{L}(H)\otimes\mathcal{O}_C)-\chi(\mathcal{O}_C)=\deg_C(\mathcal{L}(H)\otimes\mathcal{O}_C) $$ but how? I'm missing some ingredient.",['algebraic-geometry']
4114112,Schauder Basis for Homeo$^+(\mathbb{R}^2)$,"In the 50's through 70's there was a lot of research into the group of orientation-preserving homeomorphisms of the plane, denoted as Homeo $^+(\mathbb{R}^2)$ (in the compact-open topology, which in this case is just the topology of compact convergence).  Long proofs were used to prove certain local properties like local arc-connectedness and local contractibility. But much more recently it was shown that Homeo $^+(\mathbb{R}^2)$ is actually a Hilbert manifold, i.e. locally homeomorphic to separable Hilbert space.  The proof was rather abstract, but it leaves open the question of whether it can just be displayed directly; it wasn't believed to be so nice back when the foundational work was being done, so I'm not sure anyone ever tried. I guess the best way to get a direct proof is to just find the basis! What is an explicit orthonormal basis for a neighborhood of $id \in$ Homeo $^+(\mathbb{R}^2)$ ? I've been trying to do this myself using very simple functions, but haven't been able to come up with it.  Here I don't want to a priori assume that the group is a Hilbert manifold, I want to prove it directly (or at least the local homeomorphism). It seems like this is more a problem for people in functional analysis, so hopefully somebody can help!","['schauder-basis', 'topological-vector-spaces', 'topological-groups', 'real-analysis', 'functional-analysis']"
4114146,How to solve $\cos x = x \sin x$,"Is it possible to express the solutions to $\cos x = x \sin x$ in closed form? Numerically, the first positive solution seems to be $x = 0.8603335890193...$ , which is is suspiciously close to $\frac{\sqrt{3}}{2} = 0.8660254037844...$ .","['trigonometry', 'closed-form']"
4114162,"Proving that, in an finite set, if intersection is equal to $1$ or $0$, then there is not more than $n$ subsets.","Ok, this seems like a hard question and I never saw anything like this before: Let $N$ be a set with $n$ elements. Let $X_{1}, \dots, X_{m},Y_{1},\dots,Y_{m} \subseteq N$ , where when $1 \leq i,j \leq m $ , if $i\neq j$ then $|X_{i}\cap Y_{j}|=1$ and $|X_{i} \cap Y_{i}|=0$ . Prove that $m\leq n$ . So, I was thinking in use some kind of matrix operations, since the usage of $i,j$ injects in my brain some kind of thinking of matrix theory, but I can't proceed. I've even tried to insert some group theory inside, but I couldn't either. If somebody has some text where this result was proved, I'll be glad to take a look aswell! Any help with this will be very appreciated! Thanks in advance!",['combinatorics']
4114258,On whether $100$ factorials multiplied together can form a factorial,"Problem: Let there be $100$ numbers $x_1, x_2, \cdots, x_{100}$ where all the terms are $\geq 2.$ Is it possible for $x_1!x_2! \cdots x_{100}!=n!$ for some positive integer $n?$ So far, all my progress is that if we choose an arbitrary number as a factorial first, our second factorial must be contained in our first factorial, be equal to our first factorial, or it contains our first factorial. For example, to make things clearer, assume our first factorial is $50!.$ If our second factorial is something like $3!,$ then we have $(3!)^2 \cdot 4\cdot 5 \cdots \cdot 50.$ If our second factorial is $50!$ we have $(50!)^2$ . If our second factorial is something like $150!$ we will have $(50!)^2 \cdot 51 \cdot 51 \cdots 150.$ However, I'm not really sure on how this helps. May I have some assistance? Thanks in advance.","['algebra-precalculus', 'factorial']"
4114274,Area under $ |x| + |y| < k $,"Consider the region $C$ of all points in the $x,y$ plane that satisfy $ |x| + |y| < k $ . Now I wish the find out the area of $C$ using an integral. It's easy all points in the interior of a square of side $k\sqrt{2}$ centered at the origin and tilted by $\pi/4$ satisfy the equation. The image below illustrates the region for $k=6$ So I guess that the area of $C$ is $2k^2$ . I wasn't able to find this by using integral so I'd like an approach by using integrals. Any help would be appreciated. Thanks.","['integration', 'calculus', 'definite-integrals']"
4114309,Solve $AX=B+AXC$ for the missing matrix $X$,"imgur link in case I mess up the formatting Solve the following matrix equation for $X$ $$AX=B+AXC$$ where non-symmetric matrix $A$ is $2 \times 2$ , matrix $B$ is $2 \times 3$ , and non-invertible matrix $C$ is $3 \times 3$ . $$\begin{bmatrix} 2&3\\1&2 \end{bmatrix}X=\begin{bmatrix}1&1&0\\0&1&1\end{bmatrix}+\begin{bmatrix}2&3\\1&2\end{bmatrix}X\begin{bmatrix}0&1&1\\0&0&1\\0&0&0\end{bmatrix}$$ This one has me stumped. The only way I managed to find an answer for $X$ was by doing this: $$\begin{align}X&=A^{-1}(B+AXC)\\\\
X&=A^{-1}B+XC\\\\
X-XC&=A^{-1}B\\\\
X(I-C)&=A^{-1}B\\\\
X&=A^{-1}B(I-C)^{-1}\end{align}$$ obviously, by plugging in this answer for $X$ into the original question, the answer is false. its probably because by doing $X(I-C)$ , you create an identity matrix that doesn't have a clear dimension. Your help would be much appreciated. EDIT: turns out my solution was correct. I must have made a miscalculation somewhere along the way. Thanks everyone!","['matrices', 'matrix-equations', 'linear-algebra']"
4114348,Find the other root of quadratic when $b^2-4ac = 0$?,"According to many sources, the fundamental theorem of algebra states that every polynomial of degree $n$ has exactly n roots. But where's the other root when $b^2-4ac = 0$ ? What's the other root of $4x^2 - 32x + 64$ , for example? (the real root is 4).","['algebra-precalculus', 'roots', 'polynomials']"
4114360,How to split a pot of $100 when a game of flipping coins until first person gets 10 heads is interrupted.(A variation of the points problem),"Question : Andy and Beth are playing a game worth $100. They take turns flipping
a penny. The first person to get 10 heads will win. But they just realized that they have to be in math class right away
and are forced to stop the game. Andy had four heads and Beth had seven heads. How should they divide
the pot? This is a question from the book Probability: With Applications and R by Robert P. Dobrow .Now the given answer is : P(Andy wins) = 0.1445. Andy gets $14.45 \;and\; Beth\; gets\; $ 85.55 But I am not getting this answer . So can someone check my solution and tell me where I am wrong , or else provide a better solution . Thanks in advance. Proposed Solution : Let Andy be A and Beth B:  A needs 6 heads to win, and B needs 3 heads to win. Let $X_a$ be the number of times A needs to flip the coin to get 6 wins , and $X_b$ be the number of flips for B to get 3 wins . Both $X_a$ and $X_b$ are Negative Binomial Variables with the $X_a$ having parameters 6 and $p=\frac{1}{2}$ and $X_b$ having parameters 3 and $p=\frac{1}{2}$ . Now if A needs $k$ flips to win. Then the required probability is : $P(A \;wins) = P(X_a = k)*P(X_b > k)$ (i.e A wins in $k$ moves and B gets 3 heads after $k$ flips) $P(X_a = k)=(^{k-1}C_5)*(\frac{1}{2})^6(\frac{1}{2})^{k-6}=(^{k-1}C_5)*(\frac{1}{2})^k$ (Negative Binomial Distribution formula) $P(X_b > k) =$ The 3rd head should come after $k$ trails , therefore until $k$ trails only $0\;or\;1\;or\;2$ heads can occur. Therefore : $P(X_b>k) = \sum_{i=0}^2{^{k}C_i}*(\frac{1}{2})^i(\frac{1}{2})^{k-i}=\sum_{i=0}^2{^{k}C_i}*(\frac{1}{2})^k$ So $P(A\;wins) = (^{k-1}C_5*(\frac{1}{2})^k) * (\sum_{i=0}^2{^{k}C_i}*(\frac{1}{2})^k) $ . Now minimum 6 tries are required for A to win, so $k:6 \to \infty$ $\begin{equation}
P(A\;wins) = \sum_{k=6}^{\infty}\biggl( (^{k-1}C_5*(\frac{1}{2})^k) * (\sum_{i=0}^2{^{k}C_i}*(\frac{1}{2})^k) \biggr) 
\\
= \sum_{k=6}^{\infty}\biggl((\frac{1}{2})^{2k}*(^{k-1}C_5)*(^kC_0+^kC_1+^kC_2)) \biggr)
\\
=\sum_{k=6}^{\infty}\biggl(\frac{1}{2^{2k+1}}*(^{k-1}C_5)*(k^2+k+2)\biggr)
\end{equation}$ Now I calculated the sum on my calculator for $k : 6 \to 150$ and it is converging to 0.052. Which is very far off from the given answer of $P(Andy\; wins) = 0.1445$ . So is my method wrong ? Or is the sum convergence slow ? Can someone solve this question , employing an entirely different method if necessary.","['probability-distributions', 'negative-binomial', 'probability']"
4114381,Evaluate $\int_{-1}^1 \frac{x^2e^{\arctan x}}{\sqrt{{1+x^2}}}\ \mathrm{d}x$,"$$\int_{-1}^1 \frac{x^2e^{\arctan x}}{\sqrt{{1+x^2}}}\ \mathrm{d}x$$ My try: I tried substituting $x= \tan\theta$ and got, $\displaystyle\int e^\theta \tan^2\theta \sec\theta\ \mathrm{d}\theta$ and cannot move forward. so I thought of finding two integrals( $I$ and $J$ )one of which is required and the other is taken such that $I+J, I-J$ are easy to find.but couldn't find such $J$ $\left(I=\displaystyle\int\frac{x^2e^{\arctan x}}{1+x^2}\ \mathrm{d}x\right)$ Also, I don't think I could make use of definite integral properties. WolframAlpha result for the integral is $\frac{(x-1)\sqrt{1+x^2}e^{arctanx}}{2}+C$ I also want to know how you got the idea for the solution.","['indefinite-integrals', 'calculus', 'definite-integrals']"
4114416,Finite-time criterion for ODE,"In article Finite-Time Stability of Continuous Autonomous Systems i found this [page 4]. That's what I don't understand: Can (2.7) $\dot{y}(t)=-k \cdot {\rm sign}(y(t)) \cdot \lvert y(t) \rvert^{\alpha}$ be reformulated as a condition for convergence in a finite-time, i.e. $\dot{y}(t) + k \cdot {\rm sign}(y(t)) \cdot \lvert y(t) \rvert^{\alpha}=0$ ? What if I have an equation of the form $\dot{y}=\nabla_y f$ and want $\nabla_y f \rightarrow 0$ in finite-time. Does this mean that I must meet the condition $\dot{\nabla_y f} + k \cdot {\rm sign}(\nabla_y f) \cdot \lvert \nabla_y f \rvert^{\alpha}=0$ Please help me figure it out. I would be grateful for help.","['ordinary-differential-equations', 'optimal-control', 'adaptive-control', 'nonlinear-system', 'dynamical-systems']"
4114448,Is $5^2-3^2=16$ the only example for $(P_n)^2 - (P_{n-1})^2 \bmod 6 ≠ 0$,"Is $5^2-3^2=16$ the only example for $(P_n)^2 - (P_{n-1})^2 \bmod 6 ≠ 0$ where $P$ is an element of the consecutive list of prime numbers (without $2$ ): ${3,5,7,11,13,17...P_n}$ As a web developer and a self math learner, out of curiosity I have created a script to find the difference between a prime number its previous prime number on a consecutive list. From the first $100000$ primes, Starting with $5^2-3^2=16$ , only $5^2-3^2=16$ was not divisible by $6$ . Is it proven to be the case (Yes being divisible by $6$ ) for all the remaining differences, and if so what makes  only $5^2-3^2=16$ act different? The only thing that stands out for me is that $5$ is the only prime ending in $5$ , I wish I could post my own ideas for what I think might be  the cause, but I really don't have a clue, so any help is appreciated. Attached you can see a screen shot of the first results:","['proof-explanation', 'algebra-precalculus', 'prime-numbers']"
4114471,Must a $𝐶^1$ curve with constant angular momentum alternate between a straight line or a circular arc?,"Let $\alpha:(0,L) \to \mathbb{R}^2$ be a $C^1$ curve  satisfying $|\dot \alpha|=1$ , and assume that $\alpha(t) \times \dot \alpha(t)$ is constant. Does one of the following hold? $(1)$ $\alpha$ is affine. $(2)$ $\alpha$ is a circular arc. $(3)$ $\alpha$ starts as a circular arc, then at some point becomes affine (i.e. it coincides with the tangent to the circle.) $(4)$ $\alpha$ starts affine, then at some point it swtiches to a circular arc, and after some time, it can also switch one more time to being affine (a tangent). Comments: a. I proved below that the restriction of $\alpha$ to the open set $\{t\, |\,\alpha(t) \not\perp \dot \alpha(t)\}$ is $C^{\infty}$ . Since a $C^2$ curve  having constant momentum must be either affine or a circular arc , and since a circular arc satisfies $\alpha(t)\perp \dot \alpha(t)$ , it follows that $\alpha$ is affine on each connected component of $\{t\, |\,\alpha(t) \not\perp \dot \alpha(t)\}$ . Is there a way to proceed from here? The difficulty seems to be to prove that $\{t\, |\,\alpha(t) \perp \dot \alpha(t)\}$ is a closed interval. Suppose for instance that $\alpha$ starts affine, and that it stays affine up to a certain point $t_0$ satisfying $\alpha(t_0) \perp \dot \alpha(t_0)$ . Suppose that $\alpha$ is not affine after $t_0$ , i.e. there is no $\epsilon>0$ such that $\alpha|_{(t_0-\epsilon,t_0+\epsilon)}$ is affine.  How can we prove that $\alpha(t) \perp \dot \alpha(t)$ still holds for sufficiently small $t>t_0$ ? Assuming this is not the case, there exists a decreasing sequence $t_n \to t_0$ satisfying $\alpha(t_n) \not \perp \dot \alpha(t_n)$ . Thus there exists $\epsilon_n>0$ , such that $\alpha|_{(t_n-\epsilon_n,t_n+\epsilon_n)}$ is affine. However, this fact alone does not imply affinity at $t_0$ . Of course, in our case, all these affine parts around $t_n$ must be tangents to a given circle. Can we use this somehow? b. There is an asymmetry between options $(3)$ and $(4)$ . In option $(4)$ there are two ""switching points"", while in $(3)$ there is only one. Here is  why there cannot be more than one switch in the scenario described in $(3)$ : On a region where $\alpha$ is part of a circle of radius $R$ centered at the origin, it satisfies $|\alpha(t) \times \dot \alpha(t)|=R$ . Switching from a line to a circle is only possible when the line is tangent to that circle. (since we are $C^1$ ). Thus, if we start from a circle, and switch to a tangent, we cannot switch again at some point from the tangent to a tangent circle centered at the origin. (the angle at the contact from the origin is not $\pi/2$ ). Alternatively, after we leave the point of contact of the tangent and circle, the distance from the origin increases, so we cannot switch to a circle centered at the origin, since such a circle would have a different radius, thus violating the constancy of $|\alpha(t) \times \dot \alpha(t)|$ . c. I proved here that if $\alpha$ is assumed to be $C^2$ , then the only options $(1)$ or $(2)$ are possible, i.e. $\alpha$ must be either a circular arc or affine. Clearly, a $C^2$ solution cannot alternate between circular and affine. I don't know how to adapt the proof to this lower regularity setting, since I used there second order derivatives of $\alpha$ . I guess that one possibility would be to use some distributional/weak derivatives instead. I think that a good starting point might be to show $\alpha$ is $C^2$ almost everywhere. Proof that that $\alpha$ is $C^2$ on $\{t\, |\,\alpha(t) \not\perp \dot \alpha(t)\}$ is $C^2$ : Write $\alpha(t)=(X(t),Y(t))$ . The conditions $|\dot \alpha|=1$ and $|\alpha(t) \times \dot \alpha(t)|=c$ , translate into $$
\dot X^2+\dot Y^2=1, \\ X\dot Y-Y\dot X=c.
$$ Thus $$
\dot Y=\frac{c+Y\dot X}{X},
$$ which together with $\dot Y^2=1-\dot X^2$ implies that $$
0=\dot X^2(Y^2+X^2)+(2cY)\dot X+(c^2-X^2).
$$ Since the discriminant $$
(2cY)^2-4(Y^2+X^2)(c^2-X^2)=4X^2(|\alpha|^2-c^2),
$$ we find that whenever $|\alpha| > c$ , $\dot X$ is a smooth function of $X,Y,c$ . Now, we note that $c=|\alpha(t) \times \dot \alpha(t)| \le |\alpha(t)||\dot \alpha(t)|=|\alpha(t)|$ , with equality if and only if $\alpha(t) \perp \dot \alpha(t)$ . Thus $$
\{t\, |\,|\alpha(t)|>c\}=\{t\, |\,\alpha(t) \not\perp \dot \alpha(t)\},
$$ and since $\dot X$ (and similarly $\dot Y$ ) are smooth functions of $X,Y,c$ , we conclude that $\alpha$ is $C^{\infty}$ on this set.","['euclidean-geometry', 'ordinary-differential-equations', 'circles', 'polygons', 'differential-geometry']"
4114484,Maclaurin series for the function $f(x) = \ln\left(\frac{|x^2 - 1|}{x+1}\right)$,"I want to find the Maclaurin series (Taylor at $0$ ) for the following function: $f(x) = \ln\left(\frac{|x^2 - 1|}{x+1}\right)$ So far I have worked on this solution but I am not sure if it's correct: $$
f(x) = \ln\left(\frac{|x^2 - 1|}{x+1}\right) = \ln(|x^2-1|) - \ln|x+1| = \ln|x-1|+\ln|x+1| - \ln|x+1|
= \ln|x-1| = \ln|1-x| 
$$ Since we are around zero I think that we can can drop the absolute value and evaluate $$
\ln|1-x| = \ln(1-x) = -\sum_{i=0}^\infty \frac{x^n}{n}
$$ If the solution is not correct, could you suggest an alternative solution, please?","['sequences-and-series', 'analysis', 'real-analysis']"
4114489,Inverse transform sampling for piecewise function,"https://stats.stackexchange.com/questions/447035/inverse-transform-method-with-piecewise-pdf I have the same question as the one above, which was wasn't answered. I am doing inverse transform sampling with the following pdf: $$f(x) = \begin{cases}x, & 0 \leq x \leq 1 \\ 2 - x, & 1 < x \leq 2 \end{cases}$$ which gives the cdf $$F(x) = \begin{cases}\frac{x^2}{2}, & 0 \leq x \leq 1 \\ 2x - \frac{x^2}{2} - 1, & 1 < x \leq 2\end{cases}$$ I know I take the case of $$\frac{x^2}{2} =u$$ and get $$x = \sqrt{2u}, $$ And then for the other case: $$2x - \frac{x^2}{2} - 1=u $$ $$ - \frac{x^2}{2}+2x  - 1-u =0$$ $$\quad \quad x = \frac{-2 \pm \sqrt{4 - 4(-0.5)(-u-1)}}{-1}.$$ $$\quad \quad x = \frac{-2 \pm \sqrt{4 +2(-u-1)}}{-1}.$$ $$\quad \quad x = \frac{-2 \pm \sqrt{2 -2u}}{-1}.$$ $$\quad \quad x = \frac{2 \mp \sqrt{2 -2u}}{1}.$$ I'm unclear what the next step is.","['monte-carlo', 'probability-distributions', 'probability-theory', 'probability']"
4114501,Product rule of differentiation in the multivariate case from first principles,"$\newcommand{\norm}[1]{\left\lVert#1\right\rVert}$ Hi, I would like to ask if this is a valid and rigorous proof for the product rule of differentiation in the multi-variate case. The book I am using for multivariate calculus doesn't provide a proof. Theorem. Let $f,g:X\subseteq \mathbf{R}^n \to \mathbf{R}$ be two scalar-valued functions that are differentiable at $\mathbf{a}\in X$ . Then, the product function $fg$ is also differentiable at $\mathbf{a}$ , and $$
D(fg)(\mathbf{a})=Df(\mathbf{a})g(\mathbf{a})+f(\mathbf{a})Dg(\mathbf{a})
$$ Sketch. Step 1. Let's first dispose off the fact, that the matrix of partial derivatives of the function $fg$ is indeed given by the expression above. I have: \begin{align*}
D(fg)(\mathbf{x}) &= \left[
\frac{\partial}{\partial x_1}(fg),\frac{\partial}{\partial x_2}(fg),\ldots,\frac{\partial}{\partial x_n}(fg)
\right]\\
&=\left[
\frac{\partial f}{\partial x_1}g + f\frac{\partial g}{\partial x_1},\ldots,\frac{\partial f}{\partial x_n}g + f\frac{\partial g}{\partial x_n} 
\right]  [ \text{Product rule for functions of 1 variable}]\\
&=g\left[\frac{\partial f}{\partial x_1}
, \ldots,\frac{\partial f}{\partial x_n} \right] + f\left[\frac{\partial g}{\partial x_1}
, \ldots,\frac{\partial f}{\partial x_n} \right] \\
&=Df(\mathbf{x})g(\mathbf{x}) + f(\mathbf{x})Dg(\mathbf{x})
\end{align*} Step 2. We are interested to prove that the product function $(fg)(\mathbf{x})=f(\mathbf{x})g(\mathbf{x})$ is indeed differentiable at $\mathbf{x}=\mathbf{a}$ . Let's explore the expression $$
\frac{\norm{f(\mathbf{x})g(\mathbf{x})-[f(\mathbf{a})g(\mathbf{a}) - D(fg)(\mathbf{a})(\mathbf{x}-\mathbf{a})]}}{\norm{\mathbf{x}- \mathbf{a}}}
$$ We can write: \begin{align*}
&\frac{\norm{f(\mathbf{x})g(\mathbf{x})-[f(\mathbf{a})g(\mathbf{a}) + D(fg)(\mathbf{a})(\mathbf{x}-\mathbf{a})]}}{\norm{\mathbf{x}- \mathbf{a}}}\\
=& \frac{\norm{f(\mathbf{x})g(\mathbf{x}) - f(\mathbf{a})g(\mathbf{x}) + f(\mathbf{a})g(\mathbf{x}) - [f(\mathbf{a})g(\mathbf{a}) + D(fg)(\mathbf{a})(\mathbf{x}-\mathbf{a})] }}{\norm{\mathbf{x}- \mathbf{a}}}\\
=& \frac{\norm{f(\mathbf{x})g(\mathbf{x}) - f(\mathbf{a})g(\mathbf{x}) + f(\mathbf{a})g(\mathbf{x}) - [f(\mathbf{a})g(\mathbf{a}) + \{Df(\mathbf{a})g(\mathbf{a}) + f(\mathbf{a})Dg(\mathbf{a})\}(\mathbf{x}-\mathbf{a})] }}{\norm{\mathbf{x}- \mathbf{a}}}\\
=& \small{\frac{\norm{f(\mathbf{x})g(\mathbf{x}) - f(\mathbf{a})g(\mathbf{x}) - Df(\mathbf{a})g(\mathbf{x})(\mathbf{x}-\mathbf{a}) + Df(\mathbf{a})g(\mathbf{x})(\mathbf{x}-\mathbf{a}) + f(\mathbf{a})g(\mathbf{x}) - f(\mathbf{a})g(\mathbf{a}) - f(\mathbf{a})Dg(\mathbf{a})(\mathbf{x}-\mathbf{a}) - Df(\mathbf{a})g(\mathbf{a})(\mathbf{x}-\mathbf{a})}}{\norm{\mathbf{x}- \mathbf{a}}}}\\
\le& \small{ \frac{\norm{g(\mathbf{x})} \norm{f(\mathbf{x}) - f(\mathbf{a}) - Df(\mathbf{a})(\mathbf{x} - \mathbf{a})}}{\norm{\mathbf{x}- \mathbf{a}}} + \frac{\vert f(\mathbf{a})\vert \cdot\norm{g(\mathbf{x}) - g(\mathbf{a}) - Dg(\mathbf{a})(\mathbf{x}-\mathbf{a})} }{\norm{\mathbf{x}- \mathbf{a}}} + \vert Df(\mathbf{a})\vert \norm{g(\mathbf{x}) -g(\mathbf{a}) }}
\end{align*} Since, $f$ is differentiable at $\mathbf{x}=\mathbf{a}$ , there exists a $\delta_1>0$ , such that the first expression can be made smaller than $\epsilon/3$ whenever $0 < \norm{\mathbf{x} - \mathbf{a}} < \delta_1$ . Since, $g$ is differentiable at $\mathbf{x}=\mathbf{a}$ , there exists a $\delta_2>0$ , such that the second term can be made smaller than $\epsilon/(3 \vert f(\mathbf{a})\vert)$ whenever $0 < \norm{\mathbf{x} - \mathbf{a}} < \delta_2$ . Since, $g$ is continuous at $\mathbf{x}=\mathbf{a}$ , there exists a $\delta_3>0$ , such that the third term can be made smaller than $\epsilon/(3 \vert Df(\mathbf{a})\vert)$ whenever $0 < \norm{\mathbf{x} - \mathbf{a}} < \delta_3$ . If I pick $\delta = \min\{\delta_1,\delta_2,\delta_3\}$ , I can argue, that the above sum can be made smaller than an arbitrary $\epsilon$ . I am not sure, how I can argue that the coefficient of the first term $\norm{g(\mathbf{x})}$ is in some way bounded. It is continuous in the neighbourhood $\norm{\mathbf{x} - \mathbf{a}} < \delta_1$ . So, perhaps, it's bounded, but I'm still studying analysis. :)","['epsilon-delta', 'multivariable-calculus', 'solution-verification', 'limits', 'derivatives']"
4114515,Reference request: Elementary geometry done by Geometric Algebra,"I'm impressed by geometric algebra (aka Clifford algebra, done in the style of Hestenes et al), but feel unfamiliar compared to analytical geometry, due to not having done many exercises in it. I ask for recommendations to material where geometric algebra is applied to elementary geometry problems, such as the incircles of triangles, Desargue's Theorem, circle inversions, etc. As examples, New foundations for classical mechanics (Hestenes 2012) has some elementary geometry. Conic Sections and Meet Intersections in Geometric Algebra (Hitzer, 2013) treats conic sections, but it's just a paper rather than a full book.","['book-recommendation', 'geometry', 'reference-request']"
4114592,Example of function that is Gâteaux-differentiable but not Fréchet-differentiable,"I am looking for an example of a function that is Gateaux-differentiable but not Fréchet-differentiable. I know that there is a lot of example of function $f: \mathbb R^2 \to \mathbb R$ that satisfies this property. An example is $$f(x, y) = 
\begin{cases}
\frac{x^3}{x^2 + y^2} & \text{if } (x, y) \neq 0,\\
0 & \text{otherwise.}
\end{cases}$$ But since the Fréchet and Gâteaux derivative are defined for Banach spaces in general, I am looking for a more fancy example such as a function from $L^p$ to $L^q$ for instance. Any idea ?","['frechet-derivative', 'gateaux-derivative', 'banach-spaces', 'derivatives']"
4114594,"The derivative of the cumulative distribution of $\min(X_1, a_1) + \min(X_2, a_2)$","Assume $X_1, X_2$ are continuous random variables in $L^{\infty}$ . Let $a_1, a_2$ be real numbers and $$Y := \min(X_1, a_1)+ \min(X_2, a_2).$$ The dependency structure between $X_1$ and $ X_2$ is not known, but we assume that the joint and marginal distributions are well-defined. Is there a way to calculate \begin{align}
\frac{d}{da_i} F_{Y}(t), 
\end{align} where $F_{Y}(t) := P(\min(X_1, a_1)+ \min(X_2, a_2) \leq t)$ ? I have tried to find an explicit expression for $Y$ , with the idea to express the cumulative distribution function as a double integral, with the hope of appealing to the fundamental theorem of calculus, but I cannot seem to find an expression that makes sense.","['integration', 'derivatives', 'probability']"
4114596,Prove that $f(x)\leq 0$ for all $x\in [0;1]$,Let $f:[0;1]\to \mathbb{R}$ be a twice differentiable function on $(0;1)$ such that $f(0)=f(1)=0$ and $f''(x)+2f'(x)+f(x)\geq 0$ for all $x\in (a;b)$ .  Prove that $f(x)\leq 0$ for all $x\in [0;1]$ I was thinking about considering other function: $g(x)=f(x)e^{x}$ . Then we have $$g(0)=g(1)=0; g'(x)=[f(x)+f'(x)]e^x; g''(x)=[f(x)+2f'(x)+f''(x)]e^x$$ It seems relate to the problem's condition. But I don't know how to continue then. I need some help to clarify this problem. Thanks so much!,"['continuity', 'calculus', 'derivatives']"
4114678,Globally hyperbolic Lorentzian manifold,"I am currently trying to work through properties of globally hyperbolic Lorentzian manifolds and there are some things which aren't clear to me: I have the following definition of globally hyperbolic: Let $(M,g)$ be a connected time-oriented Lorentzian manifold. Then $(M,g)$ is called globally hyperbolic, if one of the following equivalent conditions hold: There exists a Cauchy hypersurface in $M$ . There exists a smooth spacelike Cauchy hypersurface in $M$ . $M$ is isometric to $\mathbb{R} \times S$ with metric \begin{align*}
- Rdt^2 +\sigma_t
\end{align*} where $R$ is a smooth positive function, $(S, \sigma_t)$ is a Riemannian manifold, $\sigma_t$ depending smoothly on $t$ . Moreover, $\{t\} \times S$ is a Cauchy hypersurface in $M$ for each $t \in \mathbb{R}$ . $M$ satisfies the strong causality condition and for all $p,q \in M$ s.t. $p<q$ , it holds that $J^+(p) \cap J^-(q)$ is compact. where a Cauchy hypersurface is defined as: $S$ is called a Cauchy hypersurface, if it is met exactly one by exery inextendible timelike curve $\gamma$ in $M$ . First question : what does ""smooth Cauchy hypersurface"" mean? In the sources I use, this is never defined. Second question : As far as I understand, a Cauchy hypersurface is not necessarily spacelike, right? And if we look at the Cauchy hypersurface from 3. this one is spacelike, because we have a Riemannian metric, right?","['semi-riemannian-geometry', 'riemannian-geometry', 'differential-geometry']"
4114701,Show that every element of $F(\alpha)$ not in $F$ is transcendental over $F$.,"Let $E$ be a field extension of $F$ and $\alpha \in E$ be transcendental over $F$ . Show that every element of $F(\alpha)$ not in $F$ is transcendental over $F$ . Please check my attempt at proving this. Feedback is appreciated. Thanks! Proof. Let $\beta \in F(\alpha) \setminus F$ such that $\beta$ is algebraic over $F$ . Then $\exists p(x) \in F[x], p(x) \neq 0$ such that $p(\beta) = 0$ . We then write $p(x)$ as follows $$p(x) = a_{0} + a_{1}x + \cdot\cdot\cdot + a_{n}x^{n}, a_{i} \in F.$$ Applying the evaluation homomorphism $\phi_{\beta}: F[x] \to E,$ $$p(\beta) = 0 = a_{0} + a_{1}\beta + \cdot\cdot\cdot + a_{n}\beta^{n}.$$ Since $\beta \in F(\alpha) \setminus F$ , $\beta = \frac{q(\alpha)}{r(\alpha)}$ $\exists q(x), r(x) \in F[x]$ (please see question 1). Thus $$a_{0} + a_{1}\beta + \cdot\cdot\cdot + a_{n}\beta^{n} = a_{0} + a_{1}(\frac{q(\alpha)}{r(\alpha)}) + \cdot\cdot\cdot + a_{n}(\frac{q(\alpha)}{r(\alpha)})^{n}.$$ Multiplying both sides with $(r(\alpha))^{n},$ $$(r(\alpha))^{n} (a_{0} + a_{1}(\frac{q(\alpha)}{r(\alpha)}) + \cdot\cdot\cdot + a_{n}(\frac{q(\alpha)}{r(\alpha)})^{n}) = a_{0}(r(\alpha))^{n} + a_{1}q(\alpha)(r(\alpha))^{n-1} + \cdot\cdot\cdot + a_{n}(q(\alpha))^{n} = 0.$$ Thus we found a polynomial $g(x) \in F[x] \setminus \{0\}$ such that $g(\alpha) = 0$ , contrary to the assumption that $\alpha$ is transcendental over $F$ . Therefore $\beta$ is transcendental over $F$ . $\square$ . Some of my questions/clarifications (will probably ask more in the comments/answers): How is $q(\alpha)$ related to $r(\alpha)$ in terms of their form? More importantly, how must they be related so that $\beta$ is guaranteed to be an element of $F(\alpha) \setminus F$ ? Must it be that $r(\alpha) \nmid q(\alpha), gcd(q(\alpha), r(\alpha)) = 1, $ or something else?","['field-theory', 'abstract-algebra', 'polynomials']"
4114728,"With an odd number of terms, the Taylor polynomial of cosine is greater than cosine, but with an even number of terms it's less than cosine","If you use an odd number of terms, $$f_{4n}(x) = 1 - \frac{x^2}{2!} + ... + \frac{x^{4n}}{(4n)!},$$ this Taylor polynomial $f_{4n}(x) \geq \cos(x)$ . With an even number of terms, $$f_{4n-2}(x) = 1 - \frac{x^2}{2!} + ... - \frac{x^{4n-2}}{(4n-2)!}$$ the Taylor polynomial $f_{4n-2}(x) \leq \cos(x).$ How do we prove this? The trouble is that the power series for $\cos$ is infinite, and there are infinitely-many negative terms and infinitely-many positive terms.","['taylor-expansion', 'calculus', 'trigonometry', 'inequality']"
4114767,Failure of L’Hôpital’s Rule: $ \lim\limits_{x\rightarrow 0^+} \dfrac{e^{-1/x}}{x^p} $,"Consider $$
\lim_{x\rightarrow 0^+} \frac{e^{-1/x}}{x^p}
$$ where $p\in \mathbb{R}^+\;(p>0)\;.$ Despite satisfying all of the criteria for L’Hôpital’s Rule, repeated applications continue to yield the same indeterminate limit, whereas it can be shown heuristically, that the limit is indeed zero. I don't see how the Squeeze Theorem would help me here. Is there an alternative? Thanks. Many thanks for all the answers. I wish I'd thought of the clever $u=1/x$ substitution beforehand!","['limits', 'limits-without-lhopital']"
4114853,How is axiom of substitution satisfied for $\in$? (Tao' Analysis I),"Note: This question is not a duplicate of this , due to the corrected version of the book. I'm self-studying Tao's Analysis I . In the corrected third edition, Tao promotes the what was earlier a definition of equality of sets, to the status of an axiom, after someone pointed out an issue with that. Axiom 3.2 (Equality of sets) Two sets $A$ and $B$ are equal, iff every element of $A$ is an element of $B$ and vice versa. To put it another way $A=B$ if and only if every element $x$ of $A$ belongs also to $B$ and every element $y$ of $B$ belongs also to $A$ . Just after this (well, there's Example 3.1.4 in between), he mentions the following, without justification. The ""is an element of"" relation $\in$ obeys the axiom of substitution (see section A.7). I quote the relevant part of section A.7 here: (Substitution axiom). Given any two objects $x$ and $y$ of the same type, if $x=y$ , then $f(x) = f(y)$ for all functions or operations $f$ . Similarly, for any property $P(x)$ depending on $x$ if $x=y$ , then $P(x)$ and $P(y)$ are equivalent statements. Questions: Why does Axiom 3.2 ensure that $\in$ satisfies the axiom of substitution? This is what I have in mind: To prove that $\in$ satisfies axiom of substitution, one needs to first define an equality relation and $\in$ -relation for sets. Then only we can proceed in any way, as far as I can see. But Axiom 3.2 does not define any of these relations. It just gives a condition that equality and $\in$ must satisfy. Don't we need another axiom stating that there is an equality relation on sets, and a relation $\in$ on sets such that $\in$ obeys axiom of substitution with respect to this equality?",['elementary-set-theory']
4114905,Calculate the derivative of $\left[{\sqrt{\left(1+\tan\left(x\right)\right)...\left(1+\tan\left(2^{2015}x\right)\right)}}\right]$ when $x=0$,"$\cssId{diff-var-order-mathjax}{\tfrac{\mathrm{d}}{\mathrm{d}x}}\left[{\sqrt{\left(1+\tan\left(x\right)\right)\left(1+\tan\left(2x\right)\right)\left(1+\tan\left(4x\right)\right)...\left(1+\tan\left(2^{2015}x\right)\right)}}\right]=\:?$ I have tried writing  it with sin and cos, tried some formulas for ${\sin\left(2x\right)}$ , ${\cos\left(2x\right)}$ and ${\tan\left(2x\right)}$ but I have had no meaningful results. Maybe the derivative when $x=0$ can be found without calculating it? I'm guessing it can be calculated pretty easily but I'm missing the answer. Any help would be greatly appreciated.","['derivatives', 'real-analysis']"
4114925,Getting wrong results for multiple power mod 11,"I have $\ 2^{2^{12}} \pmod {11}$ Using Euler's theorem: $\ 2^{2^{12}\pmod {10}} \pmod {11}$ . Now I am computing $\ {2^{12}} \pmod {11} = 4$ . After that, I am putting ${4}$ as power of ${2}$ . --> $\ {2^{4}} \pmod {11} = 5$ . But that's not the correct answer. Actually the result should be: $\ 2^{2^{12}} \pmod {11} = 9$ . I don't what I am doing wrong, just begun with the Euler's theorem, so I would appreciate your help. Regards.","['totient-function', 'discrete-mathematics']"
4114932,Notes on Low-Dimensional Topology,"I am studying algebraic topology at the moment and I'm halfway done with Hatcher's book. I am extremely interested in low-dimensional topology, so I was wondering if anybody knows a good set of notes in knot theory and 4-dimensional manifolds. So any reference would be much appreciated","['4-manifolds', 'reference-request', 'knot-theory', 'general-topology', 'low-dimensional-topology']"
4114953,limit of uncountable strictly increasing sequence of sets,"Let $S_\circ$ be a family of sets in ZFC, indexed by ordinals, with $S_\alpha \subsetneq S_\beta$ for $\alpha < \beta$ . Is it possible to have some uncountable $\gamma$ with $S_\gamma$ countable?",['elementary-set-theory']
4114962,Prove that the product of four consecutive natural numbers are not the square of an integer,"Prove that the product of four consecutive natural numbers are not the square of an integer Would appreciate any thoughts and feedback on my suggested proof, which is as follows: Let $f(n) = n(n+1)(n+2)(n+3) $ .
Multiplying out the expression and refactoring it in a slightly different way gives $$f(n)  = n^4 + 6n^3+11n^2+6n \\= n^4 + 6n^3 + 9n^2 + 2n^2 + 6n = (n^2 + 3n)^2 + 2n(n+3). \tag{1}\label{1} $$ We want to show that the only possible way for $ f(n) $ to be the square of an integer is if $ f(n) = (n^2 + 3n +1 ) ^2. $ We show this by proving that $ (n^2+3n)^2 < f(n) < (n^2+3n+2)^2 $ . The left-hand side follows immediately from $(1)$ , since $ 2n(n+3) > 0 $ for all $ n \geq 1 $ , and the right-hand side can be verified  by multiplying out both sides: $$ 
\begin{align}
(n^2+3n)^2 + 2n(n+3) &< (n^2+3n+2)^2  \\ \iff
n^4 + 6n^3 + 11n^2 + 6n &< n^4 + 9n^2 + 4 + 6n^3+4n^2+12n \\ \iff
0 &< 2n^2 + 6n + 4
\end{align}
$$ which is true for all $n \geq 1 $ . Now we note that $n^2+3n = n(n+3)$ is even since one of the factors $n$ or $n+3$ is even for all $n$ . It follows that $ n^2+3n+1$ must be odd, and so $ (n^2+3n+1)^2 $ must be odd. But $ f(n) $ must be even, since either $n$ and $(n+2)$ is even, or $(n+1)$ and $(n+3)$ is even and an even number multiplied by an odd number is an even number. So $f(n) \neq (n^2 + 3n +1)$ and therefore $f(n)$ cannot be the square of an integer for all $n \geq 1 $ .","['contest-math', 'algebra-precalculus', 'solution-verification', 'recreational-mathematics']"
4115055,"How to solve the following integral inequality $(\alpha+2)\cdot \int_0^1 x^\alpha(f(x)+f^{-1}(x)) \, dx \le 2$","Let $f:[0, 1]\to[0, 1]$ be continuous and bijective with $f(0) = 0$ . Prove that for every $\alpha \ge 0$ : $$(\alpha+2)\cdot \int_0^1 x^\alpha (f(x)+f^{-1}(x)) \, dx \le 2$$ To be fair I don't even know how to begin solving it. I'm pretty sure $f(0)=0$ should help me but I'm not sure how Solution: $f$ continuous and bijective with $f(0) = 0$ and $f(1)=1$$\implies$ $f$ is increasing Using Young: $\int_0^x f(t) \, dt+\int_0^x f^{-1}(t) \, dt \ge x^2, x\in[0, 1]$ if and only if $f(x)=x$ ; so we can rewrite it as $\int_0^x (f(t)+f^{-1}(t)-2t) \, dt\ge 0, x\in[0, 1]$ Let $G:[0, 1]\to \mathbb{R}$ with $G(x)=\int_0^x (f(t)+f^{-1}(t)-2t) \, dt$ . Then $G(1)=G(0)=0, G(x)\ge 0, x\in[0, 1]$ $G$ differentiable and for every $\alpha\ge 0$ : $$\int_0^1 x^{\alpha}(f(x)+f^{-1}(x)) \, dx = 2\int_0^1 x^{\alpha+1} \, dx + \int_0^1 x^\alpha(f(x)+f^{-1}(x)-2x) \, dx = \frac{2}{\alpha+2} + \int_0^1 x^\alpha G'(x)\,dx = \frac{2}{\alpha+2}+\lim_{a \to0} (G(1)-G(a)\cdot a^\alpha -\int_a^1 \alpha\cdot x^{\alpha-1} G(x) \, dx) \le\frac{2}{\alpha+2}$$ therefore $$(\alpha+2)\cdot \int_0^1 x^\alpha (f(x)+f^{-1}(x)) \, dx \le 2$$","['integration', 'inequality', 'analysis']"
4115069,Thinking of the limit as a functional..?,"I understand 'functionals' as functions of functions, for example: $$ S[y(x)]= \int_{t_1}^{t_2} \sqrt{1+(y')^2} dx$$ Which is the famous arc length integral Now, in a similar way, a limit we can write as: $$L(a, [y(x)] ) = \lim_{x \to a} y(x) \tag{1}$$ In this way, we can think of a limit as a function of a 'function' and a 'number'. So, would it be correct to call the above object a functionals? Why/Why not? Examples of (1): $$L(0,\frac{\sin x}{x}) = \lim_{x \to 0} \frac{\sin x}{x} = 1$$ $$L(0,e^x) = 1$$ etc This doubt mainly emerged while I was answering through this post","['functions', 'functional-analysis']"
4115124,Why viruses such as covid-19 are shaped like a sphere from a mathematical point of view [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. This question is not about mathematics, within the scope defined in the help center . Closed 3 years ago . Improve this question I can't find anything online about the relationship between spheres and the envelope of a virus from a purely mathematical point of view, what I need to know is why the outer-layer of viruses is a sphere, asked my professor about it but even he couldn't answer me.","['biology', 'geometry']"
4115134,"If you picked a number between $1$ to $10^n$ with no zero digits,find the probability that the digits' product is less than the number's square root?","What is the probability if you picked a number with no zero digits between $1$ to $10^n$ that the digits' product is less than the number's square root? You can't pick a number like $371790$ because it has a zero. An example for it being bigger than the square root is $\;2\times2\times2\times2\times9\times9>\sqrt{222299}.$ An example for it being smaller than the square root is ; $2\times2\times2\times2\times9\times2<\sqrt{222292}.$ If there is a limit of the probability as n goes to infinity, what is it?","['limits', 'number-theory', 'radicals', 'probability']"
4115161,How to prove the following integral equation? $\int_{0}^{c}x^2f(x)=0$,"Let $f:\mathbb{R}\to \mathbb{R}$ be a continuous function such that $\int_{0}^{1}f(x)(1-x)dx=0$ . Prove that there exists $c\in(0, 1)$ such that $$\int_{0}^{c}x^2f(x)=0$$ Didn't try anything because I don't know how to approach it.","['integration', 'calculus', 'analysis']"
4115173,Integrating in the right hand plane,"I am trying to find the value of: $$\int_\Gamma \frac{1}{z} dz.$$ where $\Gamma$ is the semi circle in the right hand plane, traversed from $-i$ to $i$ my plan was to use the parameterization $\gamma(t)=\exp(it)$ . So $\gamma(t)'=i\exp(it)$ . Now using the formula: $$\int_\Gamma f(z) \, dz= \int_0^1 f(\gamma(t))\gamma(t)' \, dt$$ $$\int_\Gamma \frac{1}{z} dz= \int_0^1 \frac{i\exp(it)}{\exp(it)} \, dt$$ $$\int_\Gamma \frac{1}{z} dz= \int_0^1 i \, dt$$ $$\int_\Gamma \frac{1}{z} \, dz= i.$$ However this doesn't seem right and I am not sure how to verify it! Can anyone confirm? Also does anyone have any tips to check these things like online calculators or intuition because I can't see how complex integration works compared to real integration!","['complex-analysis', 'contour-integration']"
4115181,Matrix roots of the Hamilton–Cayley equation,"The Hamilton–Cayley theorem, or Cayley–Hamilton theorem, says that every $n\times n$ matrix is a zero of its own characteristic polynomial. The ring of $n\times n$ matrices is not a field and in particular, if the polynomial is factored over the field of scalars (assuming the entries in the matrix are in a field) the matrix is in some instances not a zero of any of the factors. Can anything interesting be said about the set of $n\times n$ matrices that are zeros of the characteristic polynomial?","['matrices', 'cayley-hamilton', 'polynomials']"
4115182,"Complex integration using the residue theorem $\int_{-\infty}^\infty \frac{x\sin(x)}{x^4+1}\,dx$","I've beat my head against a wall with this one for the last couple days. Evaluate: $$\int_{-\infty}^\infty \frac{x\sin(x)}{x^4+1}\,dx$$ I factored the denominator using Euler's identity such that the roots are $$x=\frac{1+i}{\sqrt2}, \frac{1-i}{\sqrt2}, \frac{-1-i}{\sqrt2}, \frac{-1+i}{\sqrt2}$$ I know that we can apply the residue theorem and calculate and sum the residues in the top half of $\mathbb{C}$ to get our solution to the integral. I set $$H(z)=\frac{ze^{iz}}{(z-(\frac {-1-i}{\sqrt2}))(z-(\frac{1-i}{\sqrt2}))}$$ and $$f(z)=\frac{H(z)}{(z-(\frac{-1+i}{\sqrt2}))(z-(\frac{1+i}{\sqrt2}))}$$ But when I try to proceed from here to take the residue I am not sure that I have the right setup and quickly get lost in the algebra, could someone please let me know if I am at least on the right path?","['complex-analysis', 'complex-integration', 'residue-calculus']"
4115191,Maximize $x + y$ with constraint $y \cdot x^2=15$.,"As the title says, I am asked to maximize the sum of two numbers $x + y$ when $x^2 \cdot y = 15$ . The thing is... I tried substitution but got nowhere, it seems $h(x) = x + \frac{15}{x^2}$ only has a minimum value, not a maximum. I've been starting to think there is an error in the excercise. I tried Lagrange multiplicators next, having: \begin{align}
f(x,y) &= x+y\\
g(x,y) &= x^2\cdot y - 15 = 0
\end{align} Then: \begin{align}
\nabla f = \lambda \nabla g
\end{align} that gives: \begin{align}
2xy &= 1\\
2x^2 &= 1\\
x^2\cdot y &= 15
\end{align} then we find \begin{align}
2xy &= 2x^2\\
x(2y - x) &= 0\\
x = 0 &\lor x = 2y
\end{align} Now, from $x^2 \cdot y = 15$ we know $x,y>0$ , then we only have one solution $x=2y$ , applying that to $g(x)$ we get $x^2 \cdot \frac{x}{2} = 15 \implies x = \sqrt[3]{30}, y = \frac{\sqrt[3]{30}}{2}$ . Then the only point that I can use from Lagrange is $(\sqrt[3]{30}, \frac{\sqrt[3]{30}}{2})$ . But what guarantee do I have that said point is the maximum of the sum of $x + y$ ?. Doesn't Lagrange only find extremes? How do I check this result? As I said earlier, if I graph $h(x) = x + \frac{15}{x^2}$ in $R^2$ I find that $x=\sqrt[3]{30}$ is the value in the $x$ axis where the minimum value of $h(x)$ is, so this only feeds my suspicions about the problem having an error and the point needed being actually a minimum instead of a maximum. Can anyone shed some light into this?","['calculus', 'derivatives']"
4115222,Is every compact $n$-manifold that immerses in $\mathbb{R}^{n+1}$ smoothable?,"Suppose that $M$ is a compact topological $n$ -manifold, and $f: M \rightarrow \mathbb{R}^{n+1}$ is a topological immersion, i.e. a local topological embedding.  Then is $M$ smoothable?  By that, I just mean 'does $M$ carry a compatible smooth structure as a manifold', the particular immersion doesn't have to be 'smoothable' here. I feel like the answer is yes, but the problem is that if the original immersion doesn't behave well globally then you could get some problems trying to 'smooth its image' using tubular neighborhoods or something.  Maybe a partition of unity argument is enough, though?  I'm not sure how to deal with this problem in higher dimensions, or even in dimension $4$ which is the case I'm the most curious about. This is related to a followup question in this thread: https://mathoverflow.net/questions/390922/is-there-a-4-manifold-which-immerses-in-mathbbr6-but-doesnt-embed-in","['riemannian-geometry', 'geometric-topology', 'manifolds', 'general-topology', 'differential-topology']"
4115260,How to find $\sum_{r\ge 0} \binom{n}{r}\binom{n-r}{r} 2^{n-2r}$?,"Problem was to find $$\sum_{r\ge 0} \binom{n}{r}\binom{n-r}{r} 2^{n-2r}.$$ My partial progress i tried to motivate such that upper term in binomial terms gets constant rather than variable , so i thought of changing it to form $\binom{n}{n-r}\binom{n-r}{n-2r}$ . Now it's of the form $\binom{n}{m}\binom{m}{k}$ . We know it is equivalent to $\binom{n}{k}\binom{n-k}{m-k}$ . Doing this we get $\binom{n}{2r}\binom{2r}{r}2^{n-2r}$ . Now what I need to do as next step as such still nothing the upper or lower terms r any constant values(sum to ) , all are variables still. Now by one answer by Robpratt i got by snake oil , is there any other way to solve this problem ?","['summation', 'combinatorial-proofs', 'binomial-coefficients', 'combinatorics', 'generating-functions']"
4115262,"Is there any English translation of the Gergonne paper ""Variétés. Essai de dialectique rationnelle"" (""Varieties. Essay about rational dialectic"")?","Is there any English translation of this Gergonne paper? ""Variétés. Essai de dialectique rationnelle"", Annales de Mathématiques pures et appliquées, tome 7 (1816-1817), p. 189-228. (""Varieties. Essay about rational dialectic"", By J.D. Gergonne).
Between else, in this paper we find the first use of the symbols ""C"" and ""Ɔ"", which later evolved into the modern symbols "" $\subset$ "" and "" $\supset$ "", used in set theory and, in older texts, for implication. You can find it here (PDF link via numdam.org) , in French. If there isn't any, I believe it deserves one. I would certainly do it, if I could. This question has also been posted on HSM .","['logic', 'reference-works', 'reference-request', 'elementary-set-theory', 'math-history']"
4115431,Prove that the following congruence involving Lucas sequence is true,"I am proving a congruence that appears in a paper, it claims that for $t\in\mathbb{Z}$ and prime $p\geq5$ $$p\sum_{k=1}^{p-1} \frac{t^k}{k^2\binom{2k}{k}}\equiv \frac{2-v_p(2-t)-t^p}{2p}-p^2\sum_{k=1}^{p-1} \frac{v_k(2-t)}{k^3} \pmod{p^3}$$ where $v_n(P)$ is the Lucas sequence when $Q=1$ , i.e. $v_n(P)=a^n+b^n$ where $a$ and $b$ are the roots of $x^2-Px+1=0$ . The paper uses the following identity \begin{align*}\begin{split}p\binom{2p}{p}\sum_{k=1}^{p-1} \frac{t^k}{k^2\binom{2k}{k}} &= \frac{v_p(t-2)-t^p}{p}+p\sum_{k=1}^{p-1} \binom{2p}{p-k}\frac{v_k(t-2)}{k^2}+p\binom{2p}{p}H_{p-1}^{(2)}+\frac{1}{p}\binom{2p}{p}\\ &= \frac{v_p(t-2)-t^p}{p}+p\sum_{k=1}^{p-1} \binom{2p}{k}\frac{v_{p-k}(t-2)}{(p-k)^2}+p\binom{2p}{p}H_{p-1}^{(2)}+\frac{1}{p}\binom{2p}{p}\end{split}\end{align*} Now using the fact that $\binom{2p}{p}\equiv 2\pmod{p^3}$ and $p\binom{2p}{k}\equiv \frac{2p^2}{k}(-1)^{k-1} \pmod{p^3}$ for $k=1,\dots,p-1$ . It is then easy to deduce that $$p\binom{2p}{p}\sum_{k=1}^{p-1} \frac{t^k}{k^2\binom{2k}{k}}\equiv 2p\sum_{k=1}^{p-1} \frac{t^k}{k^2\binom{2k}{k}} \pmod{p^3}$$ and \begin{align*}\begin{split}p\sum_{k=1}^{p-1} \binom{2p}{k}\frac{v_{p-k}(t-2)}{(p-k)^2} &\equiv 2p^2\sum_{k=1}^{p-1} \frac{(-1)^{k-1}v_{p-k}(t-2)}{k(p-k)^2} \pmod{p^3}\\ &\equiv 2p^2\sum_{k=1}^{p-1} \frac{(-1)^{p-k-1}v_{k}(t-2)}{(p-k)k^2} \pmod{p^3}\\ &\equiv 2p^2\sum_{k=1}^{p-1} \frac{v_{k}(2-t)}{(p-k)k^2} \pmod{p^3}\\ &\equiv -2p^2\sum_{k=1}^{p-1} \frac{v_k(2-t)}{k^3} \pmod{p^3}\end{split}\end{align*} since we have $v_k(-x)=(-1)^k v_k(x)$ . Now the problem arise from the last two expression, notice that we have $$p\binom{2p}{p}H_{p-1}^{(2)}=p\Bigg(\binom{2p}{p}-2+2\Bigg)H_{p-1}^{(2)}\equiv 2pH_{p-1}^{(2)} \pmod{p^3}$$ Hence it suffices to prove that $$2pH_{p-1}^{(2)}+\frac{1}{p}\binom{2p}{p}\equiv \frac{2}{p}\pmod{p^3}$$ which is equivalent to prove that $$2p^2H_{p-1}^{(2)}\equiv 2-\binom{2p}{p}\pmod{p^4}$$ and I could not prove this, the paper says using Wolstenholme's theorem $H_{p-1}^{(2)}\equiv 0 \pmod{p}$ , yet I have no idea how could I use this, any helps would be appreciated.","['summation', 'modular-arithmetic', 'number-theory', 'analytic-number-theory', 'combinatorics']"
4115441,Why can we linearize the exponential regression?,"When we are using observed data $(x_1,y_1)\ldots(x_m,y_m)$ for the exponential model $$y(x)=a_1\mathrm{e}^{a_2x}~(a_1>0),$$ it is natural to think about the linearized model $$\ln y=\ln a_1+a_2x.$$ It is not hard to understand this approach, but my question is, how can we prove rigorously that we are obtaining the same results? I.e., the following two optimization problems for $a_1$ and $a_2$ $$\text{minimize}\sum_{k=1}^m(a_1\mathrm{e}^{a_2 x_k}-y_k)^2$$ and $$\text{minimize}\sum_{k=1}^m(\ln a_1+a_2 x_k-\ln y_k)^2$$ yield the same $a_1$ , $a_2$ ? My attempts using multivariate calculus failed, and I can only say that since the problems are ""equivalent"" with unique solution, the answer should be unique. Is that acceptable?","['statistics', 'regression', 'nonlinear-optimization', 'calculus', 'optimization']"
4115464,Prove that $\mathrm{Tr}(B^\mathsf{T}Y^{-1}B)$ is independent of $B$,"Given diagonal $A\in\mathbb{R}^{n\times n}$ with all eigenvalues larger than $1$ , and minimal polynomial $\alpha(\lambda)$ . Matrix is called cyclic if its minimal polynomial is equal to characteristic polynomial. Here $A=\begin{bmatrix}
A_1 & \\
 & A_2
\end{bmatrix}$ , where $A_i$ , for $i=1,2,$ are cyclic with minimal polynomials $\alpha_i(\lambda)$ , such that $\alpha_1(\lambda)=\alpha(\lambda)$ and $\alpha_2(\lambda)$ divides $\alpha_1(\lambda)$ . For example: $A=\mathrm{diag}(5,4,3,2,3,2)$ , here $A_1=\begin{bmatrix}
5 & &&\\
 & 4&&\\
 & &3&\\
 & &&2
\end{bmatrix}$ and $A_2=\begin{bmatrix}
3 & \\
 & 2
\end{bmatrix}$ . Basically, $A_2$ collects all repeating diagonal elements, but $A$ cannot have more that $2$ same diagonal elements. $A=\mathrm{diag}(2,2,2)$ is not possible. And we assume $\mathrm{rank}[\lambda I-A \quad B]=n$ for all $\lambda\in\mathbb{R^+}$ , i.e. $(A,B)$ is stabilizable. If $BB^\mathsf{T}=AYA-Y$ where $B\in\mathbb{R}^{n\times2}$ , prove that $\mathrm{Tr}(B^\mathsf{T}Y^{-1}B)$ is independent of $B$ . My attempt: For special case when all eigenvalues of $A$ are equal to $a$ , we have $\mathrm{vec}(BB^\mathsf{T}) = (A \otimes A - I)\mathrm{vec}(Y)$ , where $\otimes$ denote the Kronecker product. And since $(A \otimes A - I)$ is nonsingular, we have \begin{align}
\mathrm{vec}(Y) &= (A \otimes A - I)^{-1}\mathrm{vec}(BB^\mathsf{T})\\
&=\frac{1}{a^2-1}\mathrm{vec}(BB^\mathsf{T})\\
\end{align} Which means $Y=\frac{1}{a^2-1}BB^\mathsf{T}$ and $$\mathrm{Tr}(B^\mathsf{T}Y^{-1}B)=\mathrm{Tr}(BB^\mathsf{T}Y^{-1})=\mathrm{Tr}(BB^\mathsf{T}(a^2-1)(BB^\mathsf{T})^{-1})=2(a^2-1).$$ I think $BB^\mathsf{T}$ is singular, but maybe we can take pseudo-inverse. Conjecture: For general case \begin{align}
\mathrm{Tr}(B^\mathsf{T}Y^{-1}B)=\mathrm{det}(A_1)+\mathrm{det}(A_2)-2,
\end{align} Matlab simulation agrees with the conjecture. I have edited the question. Since this math problem arises from engineering problem, I think my initial question was not clear for mathematical audience. I hope now it is self-containing.","['optimal-control', 'trace', 'linear-algebra', 'kronecker-product']"
4115477,Evaluate the series $ \sum_{n=1}^{\infty} \frac{1}{n(e^{2\pi n}-1)} $.,"In Ramanujan's Notebooks Volume 2 by B.C. Berndt I came across the formula $$ \sum_{n=1}^{\infty} \frac{1}{n(e^{2\pi n}-1)} = \log 2 + \frac{3}{4} \log \pi - \frac{\pi}{12} -\log \Gamma \left(\frac{1}{4} \right). $$ The proof provided uses some special values from the theory of elliptic functions, but I am unfamiliar with the subject. Does there exist a different evaluation of the series? I tried contour integration, cotangent partial fraction, and applying Poisson summation or converting to an integral but so far no success. Any help is appreciated!","['hyperbolic-functions', 'sequences-and-series']"
4115488,What is $\lim\limits_{x \to \infty}\dfrac{\sqrt{x^2+1}}{x+1}$?,"This question is from Differential and Integral Calculus by Piskunov. I've to evaluate the following limit: $$\lim\limits_{x \to \infty}\dfrac{\sqrt{x^2+1}}{x+1}$$ This is how I tried solving it, Put $t=\frac{1}{x}$ . Then the limit becomes $$\lim\limits_{t \to 0}\dfrac{\sqrt{\dfrac{1}{t^2}+1}}{\dfrac{1}{t}+1}$$ Simplifying a bit gets me, $$\lim\limits_{t \to 0}\dfrac{\sqrt{1+t^2}}{1+t}$$ As $t \to 0$ , I think $\dfrac{\sqrt{1+t^2}}{1+t} \to 1$ . But according to Piskunov, if $x \to +\infty$ then the limit is $+1$ , and if $x \to -\infty$ then the limit is $-1$ . Why is the limit not simply $1$ ?",['limits']
4115588,About a result by Hardy-Littlewood-Polya-Blackwell-Stein-Sherman-Cartier on two order operators.,"I am trying to adapt a result in Phelps : ""Lectures on Choquet's Theorem"" in chapter 15. Let $X$ be a convex subset of a locally convex space $E$ , let $P_1$ denote the set of all regular Borel probability measures on $X$ . We define the following two partial ordering with the hope to show that, under some aditional constraints, they match. Definition : For two non-negative measure $\mu$ and $\nu$ on $X$ , we say that $\mu\preccurlyeq \nu$ if for all bounded convex continuous function $h:X\to\mathbb R$ , $\mu(h)\leq \nu(h)$ . Definition : For two non-negative measure $\mu$ and $\nu$ on $X$ , we say that $\mu\curlyeqprec \nu$ if there is a dilatation $T:x\to T_x$ such that $\nu=\mu T$ (defined on page 93 of the link above). A theorem by Hardy-Littlewood-Polya-Blackwell-Stein-Sherman-Cartier (on page 94) states If $X$ is compact and if $\mu$ and $\nu$ regular Borel probability measures on $X$ then $\mu\preccurlyeq\nu$ if and only if $\mu\curlyeqprec\nu$ . To me it feels like compactness is used only in the second part of the proof when they use a result of Bourbaki on page 96. I am wondering if the proof can be modified to avoid the use of compactness. Here is my proof and saddly I cannot be sure it is correct. The outline of the proof is to show that $A\triangleq\{ (\epsilon_x,\lambda)\in P_1\times P_1 : \epsilon_x\preccurlyeq\lambda \} = \{ (\epsilon_x,\lambda)\in P_1\times P_1 : \epsilon_x\curlyeqprec\lambda \}\triangleq B$ where $\varepsilon_x(A)=\mathbf 1(x\in A)$ for all $x\in X$ and $A\subseteq X$ measurable. Then we show that $C\triangleq\{ (\mu,\nu):\mu\preccurlyeq \nu \}$ and $D\triangleq\{ (\mu,\nu):\mu\curlyeqprec \nu \}$ are respectively the closed convex hulls of $A$ and $B$ which means that $C=D$ . The proof relies on Jensen inequality that gives $\mu\curlyeqprec\nu\Rightarrow\mu\preccurlyeq \nu$ indeed if we have a kernel $T$ such that $\nu=\mu T$ and a convex function $h$ , then for any $x\in X$ , $T_x(h)\geq h(x)$ , averaging over $\mu$ gives that $\nu(h)\geq\mu(h)$ . It also relies on the definition of the upper concave enveloppe of a bounded function $g$ defined for all $x\in X$ as $\bar g(x)=\sup\{ \lambda(g) : \lambda\sim \epsilon_x \}$ (where $\lambda\sim\epsilon_x$ means that $\lambda$ averages to $x$ , i.e. for all affine function $h$ on $X$ , $\lambda(h)=h(x)$ ). This function is concave and such that $g\leq \bar g$ and, as we will show, $\lambda\sim\epsilon_x\Leftrightarrow \epsilon_x\preccurlyeq \lambda\Leftrightarrow\epsilon_x\curlyeqprec \lambda$ which means that $\bar g(x) = \sup\{ \lambda(g) : \epsilon_x\preccurlyeq \lambda \} = \sup\{ \lambda(g) : \epsilon_x\curlyeqprec \lambda \}$ . Jensen inequality gives that $A\supseteq B$ so we prove that $A\subseteq B$ . Suppose that $\epsilon_x\preccurlyeq \lambda$ , then for all $f\in X^*$ , both $f$ and $-f$ are convex hence $f(x)=\varepsilon_x(f)=\lambda(f)$ and so $\lambda$ averages to $x$ therefore $\lambda\sim\epsilon_x$ . The statement $\lambda\sim\epsilon_x\Leftrightarrow \epsilon_x\curlyeqprec \lambda$ is a tautology, hence $A=B$ . The proof of $C$ and $D$ are the closed convex hulls of respectively $A$ and $B$ are very similar and differ only in few steps. in order to show that $C$ is the closed convex hull of $A$ , it is enough to show that for all affine function $L$ on $P_1\times P_1$ such that $L\geq 0$ on $A$ we have $L\geq 0$ on $C$ , this is because the closed onvex hull of a set is the intersection of all half spaces that contains it. without loss of generality we can write for all $(\alpha,\beta)\in P_1\times P_1$ , $L(\alpha,\beta)=\alpha(f)-\beta(g)$ for some function affine functions $f$ and $g$ on $X$ . Assuming that $L\geq 0$ on $A$ means that for all $\epsilon_x\preccurlyeq \lambda$ , we have $f(x)=\epsilon_x(f)\geq \lambda(g)$ . Assume that $\mu\preccurlyeq \nu$ , then $\mu(-\bar g) \leq \nu(-\bar g)$ by concavity of $g$ and so $\mu(\bar g)\geq \nu(\bar g)$ . From $g\leq \bar g$ we get that $\nu(g)\leq \nu(\bar g)$ . Since for any $\epsilon_x\preccurlyeq \lambda$ we have $f(x)\geq \lambda(g)$ , we have $f(x)\geq \sup\{ \lambda(g): \epsilon_x\preccurlyeq \lambda\}=\bar g(x)$ and so we have $\mu(f)\geq\mu(\bar g)\geq \nu(\bar g)\geq \nu(g)$ . This means that $L\geq 0$ on $C$ and shows that the closed convex hull of $A$ is $C$ . To show the closed convex hull of $B$ is $D$ we take a similar approach. Again suppose that $L : (\alpha,\beta)\to \alpha(f)-\beta(g)$ is such that $L\geq 0$ on $B$ , we show $L\geq 0$ on $D$ . For all $\epsilon_x\curlyeqprec\lambda$ , we have $f(x)\geq \lambda(g)$ and so $f(x)\geq \sup\{ \lambda(g) : \epsilon_x\curlyeqprec\lambda \}=\bar g(x)$ which yields $\mu(f)\geq \mu(\bar g)$ . By Jensen inequality, $\mu\curlyeqprec \nu$ implies $\mu\preccurlyeq \nu$ which implies $\mu(\bar g)\geq \nu(\bar g)$ since $\bar g$ is concave. Finally $\nu(g)\leq \nu(\bar g)$ follows again from $g\leq \bar g$ and so $\mu(f)\geq\mu(\bar g)\geq \nu(\bar g)\geq \nu(g)$ . From this we deduce that $D$ is the closed convex hull of $B$ and so $C=D$ . To me this proof feels correct but I can't know that for a fact so any review or comment would be welcome, I can add more detail if needed. It also seems that this proof does not rely on compactness of $X$ and so if this proof is correct the original result can be generalized. I wonder if the compactness we actually need is the weak one, and here we have it, indeed the set of probability measures on $X$ is bounded (with the $\sup$ norm, the norm of the difference between two probability measures is at most $2$ ) and we also have that this set is weakly closed, because it is the intersection of closed half spaces. This means that the set of probability measures on $X$ is weakly compact, this may be true only if $X$ is itself weakly compact, but I don't know where this would make the proof wrong otherwise.","['representation-theory', 'order-theory', 'solution-verification', 'general-topology', 'convex-analysis']"
4115664,"$ (a_n) $ is a sequence s.t. its partial limits are $ 3 , - 1 $. Define $ b_n = | a_n - 1 | $. Show $ b_n \rightarrow 2 $","Problem: Let $ (a_n) $ be a sequence s.t. its partial limits are only from the set $ \{ 3 , - 1 \} $ ( infinite limits are not allowed to be partial limits of $ (a_n) $ ). Define a new sequence $ b_n = | a_n - 1 | $ . Show that $ b_n \rightarrow 2 $ . Attempt: Suppose that $ b_n \nrightarrow 2 $ . Meaning there exists $ \epsilon>0 $ s.t. $ \forall N \in \mathbb{N} . \exists n^*>N.| |a_n - 1 | - 2 | \geq \epsilon  $ .
Choose $ N=1 $ , then there exists $ n^* > N $ s.t. $ | |a_n^* - 1 | - 2 | \geq \epsilon \iff | |a_n^* - 1 | - 2 | \geq \epsilon \iff  |a_n^* - 1 | - 2  \geq \epsilon$ or $ |a_n^* - 1 | - 2  \leq - \epsilon $ . [ I am stuck. I don't see how I can continue if I assume wlog either $ |a_n^* - 1 | - 2  \geq \epsilon $ or $ |a_n^* - 1 | - 2  \leq - \epsilon $ , I think a better option would be If I wouldn't  try to prove by contradiction, but that attempt was failure for me as well ]. Do you have any ideas how I should continue? or if I don't choose to prove by contradiction, how would one proceed after taking arbitrary $ \epsilon > 0 $ and using the givens that $3 ,-1  $ are partial limits? Edit : The teacher-assistant told me that the theorem is true.","['limits', 'real-analysis']"
4115673,Solve Non-Linear First-Order ODE for Parabola Focus point,"I'd like to know how to solve this Non-Linear First-Order ODE , step-by-step: $$
\frac{2y'}{{y'}^2-1} = \frac{x}{y-D}
$$ where $y=f(x)$ and $D$ is a constant. Wolfram Alpha gives solution, but hides step-by-step guide: https://www.wolframalpha.com/input/?i=2*f%27%28x%29%2F%281-%28f%27%28x%29%29%5E2%29%3Dx%2F%28D-f%28x%29%29 It also states that it's a D'Alembert's equation , but when I tried to research how to solve it, Google was overwhelmed with the D'Alembert's wave equation instead. Long story: I was trying to solve, which function reflects incoming parallel rays of light to a single point. I know it is a parabola, but wanted to prove it myself. The train of thoughts I had: Out of geometric reasoning and from the reflection of rays one can deduce the followings (see image ): $\frac{x}{y-D}=\tan(\pi-2\alpha)=-\tan(2\alpha)=-\frac{2\tan(\alpha)}{1-\tan^2(\alpha)}=\frac{2y'}{{y'}^2-1}$ And this is where I'm stuck. If anyone has a better method to prove this property, all suggestion is welcome.","['physics', 'ordinary-differential-equations']"
4115695,Prove that a complete graph with $n$ vertices has $\frac{n(n-1)}{2}$ edges using induction,"Let us assume a complete graph $K_{n}$ Base case : Let $n = 1$ , in such case, we do not have any edges since this is an isolated vertex. By the formula we get $\frac{1(1-1)}{2} = 0$ . For the base case, claim holds. Let us assume that claim holds for $K_{n}$ and that $n \geq 1$ , Prove that claim holds for $n+1$ Let us have $K_{n+1}$ and remove a vertex $v\in V(K_{n+1})$ , therefore we get that $|V(K_{n+1})|-v = n (K_{n})$ , which holds by the $IH$ . Let us add back vertex v, we need to make sure graph still remains as a complete graph, therefore, while adding the $n+1$ 'th vertex (v), we need to have $deg(v) = n$ , so number of edges will be $\frac{n(n-1)}{2}+\frac{2n}{2} = \frac{n(n+1)}{2}$ , which means claim holds for $K_{n+1}$ as well. Therefore, claim must be true Would this proof be accurate ? if not, why ? Thank you","['connectedness', 'graph-theory', 'solution-verification', 'discrete-mathematics', 'induction']"
4115714,The $2N$ balls problem [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Le $N$ be an integer, $N\ge 1$ .
Suppose we have $2N$ balls, numbered by pairs from $1$ to $N$ .
So we have $2$ balls numbered $1$ , $2$ balls numbered $2$ , etc. until $N$ . For which value of $N$ can we arrange these $2N$ balls so that between any $2$ balls numbered $k$ , $1\le k\lt N$ , there is exactly $k$ balls ? For example, $3-1-2-1-3-2$ is a correct arrangement for $N=3$ . Additional question : for such values of $N$ , expose the method to construct such an arrangement","['arithmetic', 'combinatorics']"
