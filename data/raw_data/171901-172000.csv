question_id,title,body,tags
3052027,Why can't I differentiate $ x^{\sin x} $ using the power rule?,"I'm trying to differentiate $ x^{\sin x} $ , with respect to $ x $ and $x > 0 $ . My textbook initiates with $ y = x^{\sin x} $ , takes logarithms on both sides and arrives at the answer $$ x^{\sin x - 1}.\sin x + x^{\sin x}.\cos x \ \log x $$ Why can't I use the power rule to proceed like this: $ y' = (\sin x) \ x^{\sin x-1} \cos x $ . Here, I've first differentiated $ x $ with respect to $ \sin x $ and then I've differentiated $ \sin x $ with respect to $ x $ to get $ \cos x $ .",['derivatives']
3052063,Reference: riemannian metric with operator norm arbitrarily small compact manifolds,"I read this posts: uniform equivalence of norms induced by riemannian-metrics and supremum of operator norm of the differential So, it is natural to ask the following. Let $f: M \to M$ to be a $C^{1}$ local diffeomorphism map in a compact finite dimensional manifold $M$ , fixed $\beta >0$ .   Does  there exist a Riemannian metric on $TM$ such that $\|df_{x}\| \leq \beta$ ? I suspect that the answer is negative, I tried to solve this for only one point of manifold, but I stuck. Does someone know any reference related with this topic? Thanks","['riemannian-geometry', 'differential-geometry']"
3052073,Let $\sum_{k=1}^\infty a_n$ be convergent show that $\sum_{k=1}^\infty n(a_n-a_{n+1})$ converges,"Let $\sum\limits_{k=1}^\infty a_n$ be a convergent series where $a_n\geq0$ and $(a_n)$ is a monotone decreasing sequence prove that the series $\sum\limits_{k=1}^\infty n(a_n-a_{n+1})$ also converges. What I tried : Let $(A_n)$ be the sequence of partial sums of the series $\sum\limits_{k=1}^\infty a_n$ and $(B_n)$ be the sequence of partial sums of the series $\sum\limits_{k=1}^\infty n(a_n-a_{n+1})$ . Since $A_n=\sum\limits_{k=1}^n a_k$ and $B_n=\sum\limits_{k=1}^n k(a_k-a_{k+1})$ we get that: \begin{align}
B_n&=A_n-na_{n+1}\\
&=(a_1-a_{n+1})+(a_2-a_{n+1})+...+(a_n-a_{n+1})\\
&>(a_1-a_n)+(a_2-a_n)+...+(a_n-a_n)\\
&=B_{n-1}
\end{align} we see that $(B_n)$ is a monotone increasing sequence $...(1)$ and $B_n=A_n-na_{n+1}<A_n$ this implies that the sequence $(B_n)$ is bounded above ...(2) Therefore (from (1) and (2)) the sequence $(B_n)$ converges so the series $\sum\limits_{k=1}^\infty n(a_n-a_{n+1})$ also converges Is my proof correct?","['convergence-divergence', 'proof-verification', 'sequences-and-series', 'real-analysis']"
3052110,A question from 1989 leningrad mathematical olympiad,"Prove that we cannot define an binary operation $*$ on the set of integers Z satisfy all of the three properties below simultaneously: For any $A∈Z,B∈Z,C∈Z:$ 1. $A*B=-(B*A)$ 2. $(A*B)*C=A*(B*C)$ (Associative Law) 3.For every $A\in Z$ there exist $B∈Z,C∈Z$ such that $A=B*C$ I have got stucked for three days on this questions.Anyway,I will show some result and idea I had: 1.for any X∈Z,we have $X*X=-(X*X)$ .So we have $X*X=0$ 2.for any X∈Z,we have $X*0=-(0*X)=X*(X*X)=(X*X)*X=0*X$ .So we have $X*0=0*X=0$ 3.Now For any $X∈Z$ ( $X≠0$ ).We define the orbit of X-- $Ox$ to be the set $Ox$ ={S| $∃Y∈Z$ such that $X*Y=S$ },and the stabilizer of X-- $Fx$ to be the set $Fx$ ={T| $T*X=X*T=0$ }. My goal is to prove that actually $Ox=Fx$ .And therefore since $X∈Fx$ ,so $X∈Ox$ ,and we reach a contradiction since $X∉Ox$ (otherwise if $∃Y∈Z$ such that $X*Y=X$ ,then $(X*Y)*Y=X*(Y*Y)=X*0=X*Y=X=0$ ) It is easy to see that $Ox⊆Fx$ ,since for any $S∈Ox$ ,we have $X*S=X*(X*Y)=(X*X)*Y=0*Y=0$ ,so $S∈Fx$ However,for the other direction,I cannot deduce out,which I need help. I think the backgroud of this question is the orbit&stabilizer theorem in the course abstract algebra.So I have a strong intuition that I am on the right track.","['contest-math', 'binary-operations', 'abstract-algebra']"
3052167,Partial derivative 8,"I have the following derivative: $$f'(k)=\frac{a(1+bk)}{k^{1-a}(1+abk)^{^{a}}}$$ and have to compute the partial derivative with respect to b, possibly obtaining the following result: $$\frac{\partial f'(k)}{\partial b}=\frac{ak^{a}(1-a)(1+a+abk)}{(1+abk)^{a+1}}$$ I've tried to do the exercise but I cannot get closer to the desired result than this: $$\frac{\partial f'{(k)}}{\partial b}=ak^{a}(1+abk)^{-a}\left [ 1-\frac{a(a+abk)}{(1+abk)} \right ]$$ Thank you so much and Happy Christmas! :)","['partial-derivative', 'calculus', 'derivatives']"
3052176,"If $ S_t $ follows a log-normal Brownian motion, what SDE does the square of $ S_t $ follow?","If $ S_t $ follows a log-normal Brownian motion, what SDE does the square of $ S_t $ follow? I have found two possibles ways of solving it. But, they diverge with respect to the drift. First solution: 
We recall the stochastic equation: $ dS_t = \mu S_t dt + \sigma S_t \, dW_t $ and its solutions $ S_t = S_0 \exp((\mu-\sigma^2/2)t + \sigma W_t) $ . Applying directly to $ S_t^2 $ , we got: $ S_t^2 = S_0^2 \exp((2\mu-\sigma^2)t + 2\sigma W_t) $ . The drift component is $ (2\mu-\sigma^2) $ . Second solution: 
We say that $ f(S_t)=S_t^2 $ . Applying Itô's formula, we get: $ dS_t^2 = 2S_td \, S_t + \sigma^2 S_t^2 \, dt \rightarrow dS_t^2 = (2\mu + \sigma^2)S_t^2 \, dt + 2\sigma S_t^2 \, dW_t $ . The drift component is $ (2\mu + \sigma^2) $ Which is the correct solution? Where am I making a mistake? Thanks!","['brownian-motion', 'probability-theory', 'stochastic-calculus']"
3052257,Why $L_{p}(\mathbb{R})$ is separable?,"It's easy to prove that $L_{p}(K)$ is separable , using Stone theorem. But how can we show the same result for real line ? I thought about considering : $ \mathbb{R} = \cup (a_i , b_i] $ and $\mathbb{1}((a_i , b_i])$ .
But my teacher said that they are not in $L_{p}$ .","['separable-spaces', 'lp-spaces', 'functional-analysis', 'real-analysis']"
3052273,"What, if anything, is the sum of all complex numbers?","If this question is ill-defined or is otherwise of poor quality, then I'm sorry. What, if anything , is the sum of all complex numbers? If anything at all, it's an uncountable sum, that's for sure. I'm guessing some version of the Riemann series theorem would mean that there is no such thing as the sum of complex numbers, although - and I hesitate to add this - I would imagine that $$\sum_{z\in\Bbb C}z=0\tag{$\Sigma$}$$ is, in some sense, what one might call the ""principal value"" of the sum. For all $w\in\Bbb C$ , we have $-w\in\Bbb C$ with $w+(-w)=0$ , so, if we proceed naïvely, we could say that we are summing $0$ infinitely many times ${}^\dagger$ , hence $(\Sigma)$ . We have to make clear what we mean by ""sum"", though, since, of course, with real numbers, one can define all sorts of different types of infinite sums. I'm at a loss here. Has this sort of thing been studied before? I'd be surprised if not. Please help :) $\dagger$ I'm well aware that this is a bit too naïve. It's not something I take seriously.","['summation', 'analysis', 'definition', 'sequences-and-series', 'complex-numbers']"
3052280,Find a Closed form for the Combinatorial Sum $\sum_{k=0}^m\binom{n-k}{m-k} $ and Provide a Combinatorial Proof of the Result,"Question Find a closed form for the combinatorial sum $\sum_{k=0}^m\binom{n-k}{m-k} $ and provide a combinatorial proof of the resulting identity. My attempt I was able to find a closed form using the method of ""snake-oil"" but unable to provide a combinatorial proof. We claim that $$
\sum_{k=0}^m\binom{n-k}{m-k}=\binom{n+1}{m}\tag{0}.
$$ Indeed note that (formally) $$
\begin{align}
\sum_{m=0}^\infty\left(\sum_{k=0}^m\binom{n-k}{m-k}\right)z^m&=\sum_{k=0}^\infty\left(\sum_{m=k}^\infty\binom{n-k}{m-k}\right)z^m\tag{1}\\
&=\sum_{k=0}^\infty z^k\left(\sum_{u=0}^\infty\binom{n-k}{u}z^u\right)\\
&=\sum_{k=0}^\infty z^k(1+z)^{n-k}\tag{2}\\
&=(1+z)^n\sum_{k=0}^\infty\left(\frac{z}{1+z}\right)^k\\
&=(1+z)^n\frac{1}{1-\frac{z}{1+z}}=(1+z)^{n+1}.\tag{3}
\end{align}
$$ Taking the coefficient of $z^m$ from $(1+z)^{n+1}$ yields the result. In the above computation we interchanged summation in $(1)$ , used the binomial thoerem in $(2)$ and used the formula for a geometric series in $(3)$ . My problem The simplicity of the identity in $(0)$ (supposing I have not made any mistakes) suggests a combinatorial proof. Unfortunately, I have not been able to make much progress here. I don't know how to classify the $m$ element subsets of $[n+1]$ to obtain $(0)$ . Any help is appreciated.","['binomial-coefficients', 'combinatorics', 'discrete-mathematics', 'generating-functions']"
3052307,Chebyshev polynomials and trace of $A \in SL_2(\mathbb{C})$,"Defining $C_n(z) = \frac{z^m + z^{-m}}{2}$ , the Chebyshev polynomials are defined by $$T_n(C_1(z)) = C_n(z)$$ and are given by $T_1(z) = z, T_2(z) = 2z^2-1, T_3(z) = 4z^3-3z$ , etc. Since for $z=e^{i\theta}$ we have $C_1(z) = \cos\theta$ , they also satisfy $$T_n(\cos\theta) = \cos(n\theta)$$ thereby generalizing the double angle trig identity. The notes I'm reading also claim $$T_n(\text{tr } A/2) = \text{tr } (A^n/2)$$ for $A \in SL_2(\mathbb{C})$ . Why does this follow? Attempt: if $A= \begin{pmatrix} a&b \\ c &d \end{pmatrix}$ then choosing $z=\frac12(\sqrt{(a+d)^2-4}- (a+d))$ implies $C_1(z) = \text{tr }A/2$ , but then $T_n(C_1(z)) = \frac{z^n+z^{-n}}{2}$ does not simplify as far as I see to $\text{tr} A^n/2$ .","['complex-analysis', 'trigonometry', 'functions', 'chebyshev-polynomials']"
3052310,Differential of scalar product,"Task from homework: Let $f:\Bbb R^n\to\Bbb R$ , $f(x,y)=\langle x, y\rangle$ , where $\langle\cdot,\cdot\rangle$ means the scalar product in $\Bbb R^n$ .
  Find the differential $Df(x,y)(h,k)$ . First, the domain of $f$ is surely wrong, so with correcting it to $\Bbb R^n\times \Bbb R^n$ , I'm struggling to even start because every theorem we ever mentioned in class was about functions with the domain in $\Bbb R^n$ . How can $f$ be partially differentiated if the components $x$ and $y$ are again vectors?
Partial derivatives were my first idea, but any help would be appreciated.","['multivariable-calculus', 'scalar-fields']"
3052313,What is the most fair way to scale grades?,"In many universities, professors scale or ""curve"" grades at the end to ensure (among other things) that there is no grade inflation. I'm interested in studying ""fair"" ways of doing this from a mathematical standpoint. Let $S = \{X_1, X_2 \cdots X_k\}$ where $X_i \in [0,100]$ be the multiset of grades for a given class. A $\textit{scale}$ $S'$ of $S$ is some other multiset $S'=\{\phi(X_1), \phi(X_2), \cdots \phi(X_k)\}$ where $\phi:[0,100] \to [0,100]$ is some function. We say a scale is fair if $\phi$ is monotone increasing. Given two fair scales $S'$ and $S''$ with respective scale-functions $\phi, \psi$ , we say $S'$ is fairer than $S''$ if $\sum_i |\phi(X_i) - X_i| \leq \sum_i |\psi(X_i) - X_i|$ Let us suppose that the professor wants to scale the grades such that the mean grade is $70 \pm 5 \%$ . Given the above definitions, which scale function $\phi$ should he choose to ensure the scale is as fair as possible?  If there's not a simple function that always works, is there an algorithm or a strategy that might be helpful? This is, of course, but one model. There's also issues of subjectivity associated with the word ""fairness"". Perhaps there's some notion of ""fairness"" that this model doesn't quite capture. If so, please mention it. 
My opinion is that the ""fairest"" way of scaling is ensuring that the scaling preserves the original order, and disturbs the original dataset as little as possible. One other possible notion (which you may consider if you are interested in, but not specifically the one I've chosen to ask about)  is considering the double sum $$\sum_{i,k} \left||\phi(X_i) - X_i| - |\phi(X_k) - X_k|\right|$$ and trying to minimize this among all possible (fair/monotone) scale functions $\phi$ . With my original model above, a scale is ""fair"" if it doesn't disturb the original dataset much. With this above model, a scale may disturb the original dataset a lot, but it still might be quite fair so long as students' grade are all altered a similar amount (for instance, a fixed scale of $20$ %). Feel free to discuss other mathematically rigorous notions of ""fair"" scaling which you believe are pertinent, or possibly cite relevant literature.","['optimization', 'statistics']"
3052328,Asymptotics of the sum $\sum_{n=1}^\infty \frac{x^n}{n^n}$,"How does the sum $$f(x)=\sum_{n=1}^\infty \frac{x^n}{n^n}$$ behave asymptotically as $x\to\infty$ ? It appears that $f(x)$ asymptotically dominates any polynomial and is dominated by any exponential, so we might consider $\log f(x)$ rather than $f(x)$ . I apologize for having no work to show on this problem; I have no idea how to begin tackling a problem regarding the asymptotics of a function given its power series (of which there is no hope of evaluating in closed form). Hopefully an answer will provide me with some tools for doing so.","['summation', 'taylor-expansion', 'asymptotics', 'sequences-and-series']"
3052379,"Degree of $a+b$ over a field $k$, where $a$ and $b$ are distinct roots of the same polynomial","Let $K/k$ be an algebraic extension of fields with $a$ and $b$ distinct roots in $K$ of the same irreducible polynomial $f(x) \in k[x]$ of degree $n$ . Show that the degree of $k(a+b)/k$ is less than or equal to $\frac{n(n-1)}{2}$ . Also, how does one construct fields $k$ and $K$ together with roots $a,b\in K$ so that the preceding inequality is actually an equality? I'm pretty sure I can get that $k(a+b)/k$ has degree less than or equal to $n(n-1)$ since the minimal polynomial of $b$ over $k(a)$ has degree less than or equal to $n-1$ , but I'm not sure how to reduce this by a factor of $1/2$ . I've also seen that there are computational techniques for computing the minimal polynomial of a sum, but a proof that avoids things such as resolvents would be ideal.","['field-theory', 'galois-theory', 'abstract-algebra']"
3052399,extract Catalan numbers from generating function via residues,"Consider the catalan numbers $C_n=\frac{1}{n+1}\binom{2n}{n}$ , which have the following generating function: $$C(x)=\sum_{n=0}^{\infty}C_nx^n=\frac{1-\sqrt{1-4x}}{2x}.$$ I am well aware of the usual proofs of this identity: for example, one can write $C(x)$ as the root of a quadratic equation via combinatorial identities, or one can Taylor-expand $\sqrt{1-4x}$ . I am wondering if there is a way to start from $C(x)=\frac{1-\sqrt{1-4x}}{2x}$ and extract the Taylor coefficients via residue calculus. That is, if there is a direct way to evaluate the integral $$\oint_{C}\frac{1-\sqrt{1-4z}}{2z^{n+1}}dz$$ for some appropriately chosen $C$ .","['integration', 'complex-analysis', 'combinatorics', 'generating-functions']"
3052408,How can I prove that $x_{n+1} = x_n \sin(x_n)$ converges for any $x_0$?,I have already prove that it is true if exist $\sin(x_m) = 1$ or $\sin(x_m) = -1$ . So I need make proof only for absolute-decreasing progression. But I am afraid that $\sin(x_n)$ can has limit $1$ and can go to $1$ very fast. Is it a good way to count limit $\sin(x_n)$ ? And what solution is correct?,"['trigonometry', 'convergence-divergence']"
3052436,Linear relation inside of a triangle,"Let $d(P,l)$ be distance between point $P$ and line $l$ . Inside triangle $ABC$ there are points $D$ and $E$ , for which $$
d(D,AC)+d(D,BC)=d(D,AB),
$$ $$
d(E,AC)+d(E,BC)=d(E,AB).
$$ Prove that $d(X,AC)+d(X,BC)=d(X,AB)$ for any point $X$ on segment $DE$ . Looks like there is a laborious analytical proof relying on the coordinate method. But could there be an easier solution, that is also purely geometric?","['triangles', 'geometry']"
3052456,$\dfrac{\cos \alpha}{\cos (\beta-\gamma)}$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question I have a minimum and maximum problem connected to trigonometry. Please help, it is from an American olympiad from 2013. Suppose that the angles of a triangle are $\alpha, \beta$ and $\gamma$ . Let $H$ be the set $\left\{\dfrac{\cos \alpha}{\cos (\beta-\gamma)}, \dfrac{\cos \beta}{\cos (\gamma-\alpha)}, \dfrac{\cos \gamma}{\cos (\alpha-\beta)}\right\}$ . a) Find the minimal possible value of the largest item of $H$ . b) Find the maximal possible value of the smallest item of $H$ .","['contest-math', 'maxima-minima', 'trigonometry', 'geometry']"
3052473,Is echelon form a requirement to show a matrix has no solutions?,"Do I need to get a matrix into echelon form to prove that it has no solutions, or do I only need a pivot in the last column. For example, would the following row operation show that there are no solutions to the linear system represented that by the augmented matrix below? $\begin{bmatrix}1&-3&0&5\\-1&1&5&2\\-1&1&5&3\end{bmatrix}$ $\Rightarrow$ $\begin{bmatrix}1&-3&0&5\\-1&1&5&2\\0&0&0&1\end{bmatrix}$","['matrices', 'linear-algebra']"
3052491,How to prove $\int_0^1x\ln^2(1+x)\ln(\frac{x^2}{1+x})\frac{dx}{1+x^2}$,"How to prove $$\int_0^1x\ln^2(1+x)\ln\left(\frac{x^2}{1+x}\right)\frac{dx}{1+x^2}=-\frac{7}{32}\cdot\zeta{(3)}\ln2+\frac{3\pi^2}{128}\cdot\ln^22-\frac{1}{64}\cdot\ln^42-\frac{13\pi^4}{46080}$$ The substitution $$x=\frac{1-y}{1+y}$$ leads to calculate the integrals that are unknown: $$\int_0^1y\ln(1-y)\ln^2(1+y)\frac{dy}{1+y^2}, \int_0^1\frac{y\ln^3(1+y)}{1+y^2}dy$$ For the moment,I do not see how to calculate this integral.","['integration', 'polylogarithm', 'definite-integrals']"
3052506,Word Problem Involving Generating Functions,"Question Let $(a_n)_{n\geq 0}$ be an increasing sequence of non-negative integers such  that every non-negative integer can be expressed uniquely in the form $a_i+2a_j+4a_k$ where $i,j,k$ are not necessarily distinct. Compute $a_{1998}$ . My attempt Put $A(x)=\sum_{j\geq 0}x^{a_j}$ and translate the given condition in the language of generating functions to yield that $$
\frac{1}{1-x}=A(x)A(x^2)A(x^4).\tag{0}
$$ Substituting $x^2$ in place of $x$ in $(0)$ yields that $$
\frac{A(x)A(x^2)A(x^4)}{1+x}=\frac{1}{1-x^2}=A(x^2)A(x^4)A(x^8)\tag{1}
$$ So $A(x)=(1+x)A(x^8)$ . Iterating we get that $$
A(x)=\prod_{j\geq0}(1+x^{8^j})=\sum_{j\geq0}x^{a_j}.\tag{2}
$$ My Problem But I am do not know how to deduce $a_{1998}$ from $(2)$ . Any help is appreciated.","['combinatorics', 'discrete-mathematics', 'generating-functions']"
3052512,Conditions for two optimization problems to yield the same solution,"Problem: Consider the optimization problems $$\min_\beta \|y-X\beta\|^2+\alpha\|\beta\|^2 \tag 1$$ and $$\min_\beta \|\beta\|^2 \text{ subject to } \|y-X\beta\|^2 \le c \tag 2$$ where $\|x\|$ is the $2$ -norm. Fix $\alpha$ , and suppose $\beta^*$ is the solution to ( $1$ ), and let $c=\|y-X\beta^*\|^2$ . Is it true that the solution to ( $2$ ) is also $\beta^*$ ? Attempt: I believe this is true. The argument should be very similar to the one in Why are additional constraint and penalty term equivalent in ridge regression? . However, I was running some numerical experiments and it turns out the two problems have different solutions. Hence my question here: are the two problems really yielding the same solutions? Are there exceptions that I should be careful of?","['convex-optimization', 'regression', 'analysis', 'optimization', 'numerical-methods']"
3052530,Can any proof by contrapositive be rephrased into a proof by contradiction?,"From my understanding, Proof by contrapositive: Prove $P \implies Q$ , by proving that $\neg Q \implies \neg P$ since they are logically equivalent. Proof by contradiction: Prove $P \implies Q$ by showing that $P \wedge \neg Q$ yields an absurdity and hence false. So $\neg (P \wedge \neg Q)$ is equivalent to $\neg (\neg (P \implies Q))$ and $P \implies Q$ by double negation so showing that $\neg (P \wedge \neg Q)$ proves $P \implies Q$ . If the absurdity derived during the procedure for a proof by contradiction is $P \wedge \neg Q \implies\neg P$ , we have essentially already proven $P \implies Q$ by contrapositive since $\neg Q \implies \neg P$ is precisely the required condition for proof by contrapositive. But $(P \wedge \neg Q) \implies \neg P$ is also a contradictory statement which means that $P \implies Q$ must be true. Now the question is this. Is this proof by contradiction still a valid form of proof even though its a proof by contrapositive in disguise? To me, this proof by contradiction also seems to be a valid proof as it does seem to satisfy the conditions(if they are correct) for proof by contradiction. Additionally, if you have a contrapositive proof, so you have shown that $\neg Q \implies \neg P$ , is it possible to rephrase this in a proof by contradiction by supposing that $P \wedge \neg Q$ instead of just $\neg Q$ . If this is the case, what is the point in distinguishing proof by contradiction from proof by contrapositive? edit: My thought is that proof by contrapositive is a direct proof while proof by contradiction, in this case, depends on the validity of the double negation law which apparently isn't valid in intuitionistic logic.","['propositional-calculus', 'proof-writing', 'logic', 'discrete-mathematics']"
3052537,Understanding compact extensions and almost-periodic functions,"This question comes from my attempt to understand theorem $7.21$ in E-W . This concerns the dichotomy between relatively weak-mixing extensions and compact extensions. I cannot understand the proof as I do not understand these concepts. I will sketch my understanding of the proof until the part I'm certain I don't understand. All systems are taken to be ergodic, invertible, and Borel. Definition 1: An extension $\psi:(X,\mathscr{B},\mu,T) \to (Y,\mathscr{A},\nu,S)$ is said to be relatively weak-mixing if the system $(X\times X,\mathscr{B}\otimes\mathscr{B},\bar{\mu},T\times T)$ is ergodic. Here $\bar{\mu}$ is the relatively independent join over $Y$ (See E-W definition $6.15$ ). Definition 2: Again, let $\psi:(X,\mathscr{B},\mu,T) \to (Y,\mathscr{A},\nu,S)$ be an extension. Let $\mu^{\psi^{-1}\mathscr{A}}_x$ be the conditional measures on $(X,\mathscr{B})$ . A function $f\in L^2(X,\mathscr{B},\mu)$ is almost-periodic $(AP)$ with respect to $Y$ if given $\varepsilon>0$ , there exist $g_1,\dots,g_r\in L^2(X,\mathscr{B},\mu)$ such that $\forall n \in \mathbb{Z}$ , $\nu$ -a.e $y$ , $$\min_{i}\|U_T^nf-g_i\|_{L^2(\mu_y^{\mathscr{A}})}<\varepsilon.$$ Here $U_Tf:=f\circ T$ and $\mu_y^{\mathscr{A}}$ are the measures on $(X,\mathscr{B})$ obtained by completing the conditional measure diagram: $\require{AMScd}$ \begin{CD}
X @>\mu^{\psi^{-1}\mathscr{A}}_.>> M\left(\bar{X}\right)\\
@V{\psi}VV  @| \\
Y @>\mu^{\mathscr{A}}_.>> M\left(\bar{X}\right)
\end{CD} Definition 3: The extension $\psi:(X,\mathscr{B},\mu,T) \to (Y,\mathscr{A},\nu,S)$ is said to be a compact extension if the set of $AP$ (with respect to $Y$ ) functions in $L^2(X,\mathscr{B},\mu)$ is dense. Theorem 7.21: If the extension $\psi:(X,\mathscr{B},\mu,T) \to (Y,\mathscr{A},\nu,S)$ is not relatively weak-mixing, there exists a non-trivial intermediate factor $X\to X^* \to Y$ with the property that $X^* \to Y$ is a compact extension. Sketch of proof: (For details see E-W page $200$ onwards) By the hypothesis, there exists a non-constant $H \in \mathscr{L}^\infty(X\times X,\bar{\mu})$ invariant under $T\times T$ ( $\mathscr{L}$ denotes the fact that $H$ is bounded and not just essentially-bounded). For $\phi \in L^2(X,\mathscr{B},\mu)$ , define $$H*\phi(x) = \int H(x,x')\phi(x')d\mu^{\psi^{-1}\mathscr{A}}_x(x').$$ One can show this formula defines a bounded operator $L^2(X,\mathscr{B},\mu)\to L^2(X,\mathscr{B},\mu).$ And, a priori, this formula defines a compact operator $L^2(X,\mathscr{B},\mu^{\psi^{-1}\mathscr{A}}_x) \to L^2(X,\mathscr{B},\mu^{\psi^{-1}\mathscr{A}}_x)$ . Using this and the additional fact that $U_T(H*\phi)=H*(U_T\phi)$ , one shows that $\lbrace H*\phi : \phi \in \mathscr{L}^{\infty}(X,\mathscr{B},\mu)\rbrace$ is an $AP$ (wrt. $Y$ ) family in $L^{\infty}(X,\mathscr{B},\mu)$ . Moreover, one can show that this family contains functions which are not $\psi^{-1}\mathscr{A}$ -measureable. Then if we define $\mathscr{F}=\lbrace f\in L^{\infty}(X,\mathscr{B},\mu): f \text{ is } AP \text{ wrt. } Y\rbrace $ and $\mathscr{B}^*$ to be the smallest $\sigma$ -algebra making the functions in $\mathscr{F}$ measurable, one can show that $\mathscr{F} \subset L^2(X,\mathscr{B}^*,\mu)$ is dense. Here's the part I don't understand: It is then claimed that the $T$ -invariant $\sigma$ -algebra $\mathscr{B}^* (\subset \mathscr{B}$ ) gives rise to the intermediate, non-trivial, compact extension. I assume that this is done by completing the following system of extensions from $M\left(\bar{X}\right)$ to $Y$ : \begin{CD}
X @>\psi>> Y\\
@V\mu^{\mathscr{B}^*}_.VV   \\
 M\left(\bar{X}\right) 
\end{CD} The $T$ -invariance of $\mathscr{B}^*$ shows that $\mu^{\mathscr{B}^*}_.$ gives an extension. And I guess the density of $\mathscr{F}$ will be used to show $M\left( \bar{X}\right)$ is a compact extension of $Y$ . Question: To complete the above diagram, don't we need to show that $\psi^{-1}\mathscr{A}\subset \mathscr{B}^*$ ? Is this obvious? It is not explicitly addressed in the text. Thanks for reading, all help is appreciated. Edit: As John Griesmer points out, $\psi^{-1}\mathscr{A}\subset\mathscr{B}^*$ would follow if one could show every characteristic function $1_{\psi^{-1}A}$ on $X$ was AP with respect to Y. I don't quite see why such a statement should be true though. Take the function $1_{A}\circ\psi$ . I want to find functions $g_i\in L^2(X,\mathscr{B},\mu)$ and estimate $$\int|1_{A}(\psi(T^nx))-g_i(x)|^2d\mu_{x'}^{\psi^{-1}\mathscr{A}}(x) = \int|1_{A}(S^n(\psi x))-g_i(x)|^2d\mu_{x'}^{\psi^{-1}\mathscr{A}}(x)$$ as $x'$ varies in $X$ . Now if the $g_i$ 's happened to be functions on $Y$ , I could rewrite the last integral as $$\int|1_{A}(S^ny)-g_i(y)|^2d\nu_{\psi x'}^{\mathscr{A}}(x) = E\left(|U_S^n1_A-g_i|^2:\mathscr{A},\nu\right)(\psi(x')) = |U^n_S1_A - g_i|^2(\psi(x'))$$ by the properties of the conditional expectation operator. So I guess, I'm left trying to play around with the equation $$\min_i|U_s^n1_A(y) - g_i(y)|^2 < \varepsilon, $$ and hope that it holds for almost every $y$ and for all $n$ . Am I on the right track? what would be sensible choices for the $g_i$ ? Apologies if I'm missing something obvious; I only understand these things formally and have no intuition for what's actually going on.","['measure-theory', 'additive-combinatorics', 'ergodic-theory', 'functional-analysis', 'dynamical-systems']"
3052550,The image of the exterior derivative is closed: Hodge theory,If we have an operator $T$ in a Hilbert space $H$ then one can decmpose $H$ as an orthogonal sum $ker(T^*) \oplus \overline{ran(T)}$ . This is purely operator theoretic context. On the other hand there is an important theory which is called Hodge theory : one possible application of this theory is to find a special (unique) representative of cohomology class (as a harmonic differential form). My question is the following: Does the Hodge theory imply that the range of the exterior derivative is closed?,"['hodge-theory', 'operator-theory', 'differential-geometry']"
3052551,Commutative Banach algebras and maximum ideal space,"Let $A, B$ be commutative unital Banach algebras and let $\varphi: A \rightarrow B$ be a continuous unital map such that $$\overline{\varphi(A)} = B$$ Let $$\varphi^{*}: \text{Max}(B) \rightarrow \text{Max}(A)$$ $$\varphi^{*}(m) = m(\varphi)$$ be the map from the space of maximal ideals of $B$ to the space of maximal ideals of $A$ induced by $\varphi$ . How to prove that $\varphi^{*}$ is a topologically injective map? (Recall that an operator $T: X \rightarrow Y$ is called topologically injective if $T: X \rightarrow \text{im}(T)$ is a homeomorphism) My progress on the problem is the following: first of all, the space of maximal ideals of a commutative Banach algebra $A$ can be identified with the space of continuous functionals of the form $m: A \rightarrow \mathbb{C}$ . Clearly the map above is continuous, since the pointwise convergence of a net $(n_{i})$ in $\text{Max}(B)$ implies the convergence of the net $ m_{i} \circ \varphi) $ (the space of continuous linear functional is endowed with the weak* topology) If we assume for the moment that the map is bijective, then the fact that a continuous bijective map between compact Hausdorff spaces is a homeomorphism, yields the result. (here the space of maximal ideals is compact in weak* topology since the algebra is unital) The suggested proposition looks like a relaxation of the aformentioned reasoning above though i cannot figure out an easy way to modify it to make it work. Are there any hints?","['banach-algebras', 'functional-analysis', 'maximal-and-prime-ideals']"
3052604,Existence of $n$ distinct roots of a set of polynomials,"The question is Let $n$ be a positive integer. Show that there are positive real numbers $a_0,a_1,\dots, a_n$ such that for each choice of signs the polynomial $$\pm a_{n} x^{n} \pm a_{n-1} x^{n-1}\pm a_{n-2} x^{n-2 }+\cdots \pm a_{1} x^{1}\pm a_{0} x^{0}$$ has $n$ distinct real roots. I cannot determine a condition to guarantee $n$ distinct real roots for a polynomial. One way for getting multiple roots is to use intermediate value theorem to produce at least one root in $(0,1),(1,2),(2,3), \dots, (n-1,n)$ , but we are changing signs of coefficient for the rest of polynomials so I am not able to proceed from here.","['real-analysis', 'galois-theory', 'abstract-algebra', 'polynomials', 'algebra-precalculus']"
3052625,Find the closed form of $\sum_{k=1}^{n}\cos^{2m+1}\left(\frac{(2k-1)\pi}{2n+1}\right)$,"Add it:  Let $m,n$ be postive integers. Find the closed form of $$f=\sum_{k=1}^{n} \cos^{2m+1}{\left(\dfrac{(2k-1)\pi}{2n+1}\right)}$$ for $m, n \in \mathbb{N}^{+}$ . Maybe use Euler \begin{align}
2\cos{x} &=e^{ix}+e^{-ix} \\
\dfrac{\pi}{2n+1} &=x
\end{align} then \begin{align}
f &= \dfrac{1}{2^{2m+1}}\sum_{k=1}^{n}(w^{(2k-1)}+w^{-(2k-1)})^{2m+1} \\
&=\dfrac{1}{2^{2m+1}}\sum_{k=1}^{n}\sum_{i=0}^{2m+1}\binom{2m+1}{i}w^{-(2k-1)i}w^{((2m+1)-i)(2k-1)}\\
&=\dfrac{1}{(2w)^{2m+1}}\sum_{i=0}^{2m+1}\binom{2m+1}{i}w^{2i}\sum_{k=1}^{n}w^{(4m-4i+2)k}
\end{align} where $w=e^{ix}$ .",['trigonometry']
3052654,A Problem With Coordinate Systems,"Consider a coordinate system $\cal{C}$ such that the concentric half circles around two fixed points $P_1,P_2$ in the plane above line $P_1P_2$ create the grid. So any point in the upper half plane in this coordinate system looks like $(r,s)$ where $r$ and $s$ are the radius of circle around $P_1,P_2$ respectively. Consider an arbitrary point in $\cal{C}$ be $(r_1,s_1)$ . Consider this point as the origin of upper half of a Cartesian coordinate with $x$ -axis parallel to the line $P_1P_2$ . Let $(a,b)$ be any point in this Cartesian coordinate system and $(r_2,s_2)$ be the corresponding point in $\cal{C}$ . Determine $r_2-r_1$ and $s_2-s_1$ in terms of $a$ and $b$ . Can anyone please help me with this? Thanks","['coordinate-systems', 'circles', 'geometry', 'plane-geometry']"
3052692,Finding a vector equation for the tangent line of curve formed by the intersection of two cylinders,"I am currently working through questions in the textbook ""Stewart Calculus: Early Transcendentals, 8th Edition"". Fortunately, the textbook also has a solution manual but I'm having a hard time understanding why they did what they did. The question is to find a vector equation for the tangent line to the curve of intersection of the cylinders $x^2+y^2=25$ and $y^2+z^2=20$ at the point $(3,4,2)$ . This was their solution: I'm confused about the first part where they mention that the projection is contained in the circle $x^2+y^2=25$ . I graphed the two cylinders using GeoGabra and got: From the looks of it, the projection of the intersection doesn't look like a circle. Unless they literally meant contained within that circle which I guess is true but could we have also said that this curve was contained within $x^2+y^2=36$ and working with $x=6$ $\cos{t}$ and $y=6$ $\sin{t}$ for the rest of the problem and have that work as well? I'm having a hard time wrapping my head around using the projection when the entire trace isn't covered in the intersection like in the question above where there is a small portion in red that isn't intersected by the other cylinder. I hope my question makes sense, please let me know if there are parts I should expand on.","['multivariable-calculus', 'calculus', 'vectors']"
3052703,"Prove that there exists $\xi \in (-1,1)$ such that $f'''(\xi)=3.$","Problem Assume that $f(x)$ has the $3$ -order continuous derivative over $[-1,1]$ , and $f(-1)=0,f(1)=1,f'(0)=0$ . Prove that there exists $\xi \in (-1,1)$ such that $f'''(\xi)=3.$ Proof According to Taylor's formula of $f(x)$ expanding at $x=0$ $$f(x)=f(0)+\frac{x^2f''(0)}{2}+\frac{x^3f'''(\xi)}{6},~~~0\lessgtr \xi \lessgtr x,$$ we obtain $$0=f(-1)=f(0)+\frac{f''(0)}{2}-\frac{f'''(\xi_1)}{6},~~~-1<\xi_1<0,$$ $$1=f(1)=f(0)+\frac{f''(0)}{2}+\frac{f'''(\xi_2)}{6},~~~0<\xi_2<1.$$ By substraction, we have $$f'''(\xi_1)+f'''(\xi_2)=6,$$ which implies, $f'''(\xi_1),f'''(\xi_2)$ are both equal to $3$ ，or，one of them is greater than $3$ and the other less than $3$ . For the former case,the conclusion holds trivially；as for the latter one, according to the continuity of $f'''(x)$ , applying the intermediate value theorem, there exists $\xi_3 \in (\xi_1,\xi_2)\subset (-1,1)$ such that $f'''(\xi_3)=3$ ，the conclusion holds as well. Comment But I suggest that, we only need the existence of $f'''(x)$ , and the continuity of it is not necessary. That's because, according to Darboux's theorem, the derivative function itself has the intermediate value property.","['calculus', 'proof-verification']"
3052710,Is there any interesting description of $\Bbb Q_p^\times / \Bbb Q^\times$?,"Trying to reply to a comment to this answer of mine , I realised I know no better description of the quotient of multiplicative groups $\Bbb Q_p^\times / \Bbb Q^\times$ than just that. Of course I am aware of $$ \Bbb Q_p^\times \simeq p^\Bbb Z \times \mu_{p-1} \times (1+p\Bbb Z_p)$$ for odd $p$ , the slight modification for $p=2$ , and throwing the logarithm on the last factor, but that does not help a lot. The basic problem being that we mod out a non-closed subgroup. Well I guess $\Bbb R^\times / \Bbb Q^\times$ or $\Bbb C^\times / \Bbb Q^\times$ are not that easy either, so what should one expect -- but maybe there is something surprising out there? This question and its answer deal with the corresponding question for additive groups (and admit that there's not much nice to see there).","['number-theory', 'group-theory', 'p-adic-number-theory']"
3052740,How can this multiple integral be evaluated?,"I am stuck trying to solve the following integral: $$\int_R (y+2x^2)(y-x^2) dA$$ where $R$ is defined by the following equations: $xy=1$ , $xy=2$ , $y=x^2$ , $y=x^2-1$ with $x$ and $y$ positives. I've tried several changes of variables for example: $u=xy$ , $v=y-x^2$ or $u=y-x^2$ , $v=x^2$ but I get stuck because for the Jacobian ot for the limits of integration I have to solve a third degree equation. I know that I could solve it using Cardano's formula but it has to be an easy way to do it. Thank you very much.
Merry Christmas.","['integration', 'multivariable-calculus', 'change-of-variable']"
3052746,Mistake in solving an equation involving a square root,"I want to solve $2x = \sqrt{x+3}$ , which I have tried as below: $$\begin{equation}
4x^2 - x -3 = 0 \\
x^2 - \frac14 x - \frac34 = 0 \\
x^2 - \frac14x = \frac34 \\
\left(x - \frac12 \right)^2 = 1 \\
x = \frac32 , -\frac12
\end{equation}$$ This, however, is incorrect. What is wrong with my solution?","['algebra-precalculus', 'quadratics']"
3052805,Formal Power Series and Inverse Limit,"Let $G$ be a free group with generators $g_1,g_2,\dots$ . Consider the group ring $RG$ , where $R$ is a commutative ring. Let $\epsilon: RG\to G$ be the augmentation mapping and let $I=\ker\epsilon$ be the augmentation ideal of $RG$ . To be precise, $\epsilon$ is the map $$\epsilon(\sum_{g\in G}a_gg)=\sum_{g\in G}a_g$$ Let $A=\lim_n RG/I^n$ , the inverse limit. How do we describe $A$ as a formal power series? I was told that $A$ is an algebra of formal power series on non-commuting variables ( $g_1+1,g_2+1,\dots$ ?), but I am not sure how to prove it. Thanks. I read the question in Exercise from Rotman: formal power series ring as inverse limit , possibly the first step is to construct an inverse system, which is: $\psi_{n,m}: RG/I^m\to RG/I^n$ $\alpha+I^m\mapsto \alpha+I^n$ for all $m\geq n$ .","['group-theory', 'category-theory']"
3052812,Apply Green's theorem to prove Goursat's theorem,"Suppose $f$ is continuously complex differentiable on $\Omega$ , and $T \subset \Omega$ is a triangle whose interior is also contained in $\Omega$ . Apply Green's theorem to show that $$\int_T f(z) \, dz=0$$ This is an exercise in Stein's complex analysis Page $65$ . My attempt: Let $f(z)=u(x,y)+iv(x,y)$ , $dz=dx+idy$ then apply Green's theorem I can get the desire conclusion . But I can not prove that $dz=dx+idy$ , since the definition of integral along a curve only has one variable . $$\int_T f(z) \, dz=\int_a^b f(g(t)) g'(t) \, dt$$",['complex-analysis']
3052818,Modifying unitary matrix eigenvalues by right multiplication by orthogonal matrix,"I have a matrix $U \in U(n)$ ( $U^* U=Id$ ), with eigenvalues $\lambda_1, \dots \lambda_n \in S^1$ . I would like to know if its always possible to find a matrix $O \in O(n)$ such that the eigenvalues $\lambda^{'}_1, \dots, \lambda^{'}_n$ of $UO \in U(n)$ can be written as: $$\lambda^{'}_j=e^{i \pi \alpha_j}$$ Where $\alpha_j \in [0,1)$ . There is an easy case. If $U$ is a diagonal matrix and $J \subset \{1, \dots , n \}$ is the subset such that for $j \in J$ we have $\lambda_j=x_j + iy_j$ with $x,y \in \mathbb{R}$ and $(y<0 \vee x=-1)$ . Then choosing: $$\{O\}_{mn}=\begin{cases}
0 & m\ne n \\
-1 & m=n\in J \\
1 & else
\end{cases}$$ This will define an orthogonal matrix  and $UO$ will be as desired. 
The problem is that when $U$ is not a diagonal matrix the matrix $S=P^*OP$ for $P \in U(n)$ and $O \in O(n)$ is not generally an orthogonal matrix so there is no simple algorithm as far as I can see to find $O \in O(n)$","['unitary-matrices', 'lie-algebras', 'eigenvalues-eigenvectors', 'matrices', 'lie-groups']"
3052823,Is there a fixed point theorem I could use to solve this problem?,"let $E = C([0,1]),\,\,$ $K : E \to E, \,\,
(Kf)(x) = \int_0^1K(x,y)f(y)dy$ also $\|K\| \leq a < 1$ I want to prove that there for $g \in E$ there exists a unique $f_g \in E$ that satisfies the following equation : $f_g + Kf_g = g$ which is equivalent to showing that $T : E \to E,\,\,T(f) = g-Kf$ has a fixed point. with what I have in hands I feel like there must be some theorem I'm missing. any help will be greatly appreciated !","['functional-analysis', 'fixed-point-theorems']"
3052848,2D Random Walk Hitting Time,"Suppose there is a grid $[1,N]^2$ . A person standing at some initial point $(x_0,y_0)$ walk randomly within the grid. At each location, he/she walks to a neighboring location with equal probability (e.g., for an interior point, the probability is $\frac{1}{4}$ ; for a corner, it's $\frac{1}{2}$ .). Suppose there are $m$ absorbing barriers $B=\{(x_1,y_1),\cdots,(x_m,y_m)\}$ inside the grid. Once the person is on a barrier, the random walk process stops. I'd like to ask how to calculate the hitting probability and the expected number of steps for each barrier. Edit: The problem can be transformed into a Markov chain. But the expected hitting time for each absorbing state is still not easy to calculate.","['random-walk', 'markov-chains', 'probability']"
3052850,"Show that $S_n=1+{x\over1!}+{x^2\over2!}+\cdots+{x^n\over n!}$ converges for $n\in\Bbb N,\ x \in\Bbb R$ without using Taylor series.","Given a sequence $\{S_n\}$ , $n\in\Bbb N$ : $$
S_n=1+{x\over1!}+{x^2\over2!}+\cdots+{x^n\over n!}
$$ Prove that $S_n$ converges for all $x\in\Bbb R$ . Please note that i know $S_n$ is a simple Taylor series for $e^x$ , the case is I'm not supposed to know (and/or use) that fact when solving this problem since derivatives have not been defined yet. I've started with a simpler case assuming $x = 1$ . So the sequence becomes: $$
S_n = 1+{1\over1!}+{1\over2!}+\cdots+{1\over n!}
$$ It seems reasonable to use Cauchy Criterion for the sequence. Namely suppose $m>n$ , then we want to show: $$
|x_n - x_m| < \epsilon \\
|x_m - x_n| = \left|\sum_{k=n+1}^m {1\over k!}\right|
$$ Lets try to synthetically bound the sum by: $$
\begin{align}
{1\over k!} &= {1\over k!}\left(1 - {1\over k} + {1\over k}\right) \\
&\le {1\over k!}\left(1 + {1\over k-1} - {1\over k}\right) \\
&= {1\over k!}\left({k\over k -1} - {1\over k}\right) \\
&= {1\over (k - 1)(k-1)!} - {1\over k\cdot k!}
\end{align}
$$ By telescoping we obtain: $$
\left|\sum_{k=n+1}^m {1\over k!}\right| \le \left|{1\over n\cdot n!} - {1\over m\cdot m!}\right|
$$ Given $m>n$ and $n,m \in \Bbb N$ : $$
\left|{1\over n\cdot n!} - {1\over m\cdot m!}\right| \le \left|1\over n\cdot n!\right| = {1 \over n\cdot n!}
$$ Applying the limit to $|x_m - x_n|$ one may obtain: $$
0 \le \lim_{n\to\infty}|x_{n+p} - x_n| \le \lim_{n\to\infty} {1\over n\cdot n!} = 0
$$ So squeezing $|x_m - x_n|$ gives: $$
\lim_{n\to\infty}|x_{n+p} - x_n| = 0,\ p\in \Bbb N
$$ Which would eventually mean: $$
|x_m - x_n| < \epsilon
$$ However I'm not sure how to find the index $N_\epsilon$ from which the inequality becomes true since the expression for the upper bound involves a factorial. If we now put $x = x_0 \in \Bbb R$ : $$
0 \le \lim_{n\to\infty}|x_{n+p} - x_n| \le \lim_{n\to\infty} {x_0\over n\cdot n!} = 0
$$ Which doesn't influence the value of the limit. There are three questions in my mind: Is the overall reasoning valid? Is it possible to find a closed form of $N(\epsilon)$ , such that $n, m > N_\epsilon \implies |x_n - x_m| < \epsilon$ ? Should I consider two cases for $x\ge 0$ and $x<0$ Thank you!","['cauchy-sequences', 'proof-verification', 'calculus', 'sequences-and-series', 'limits']"
3052863,"$f$ is continuous on $[0,1]$, $f(0)=f(1)$,$n\in \mathbb N$ constant $\Rightarrow$ There exists $x\in[0,1-\frac{1}{n}]$ : $f(x+\frac{1}{n})=f(x).$ [duplicate]","This question already has an answer here : If $f$ is continuous, then there exists $x+\frac{1}{n}\in[0,1]$ and $f\left(x+\frac{1}{n}\right)=f(x)$ (1 answer) Closed 5 years ago . I've been trying to prove this statement: Let $f$ be a continuous function on $[0,1]$ such that $f(0)=f(1):=c$ , and let $n$ be a natural constant. Prove that there exists $x\in[0,1-\frac{1}{n}], x\in \mathbb R$ such that $f(x+\frac{1}{n})=f(x).$ I tried to use the Intermediate Value Theorem by defining $g(x)=f(x+\frac{1}{n})-f(x)$ . I could see that $g(0)=f(\frac{1}{n})-c$ and $g(1-\frac{1}{n})=c-f(1-\frac{1}{n})$ yet I could not figure out how to prove that $\mathrm s\mathrm i\mathrm g\mathrm n (g(0)\cdot g(1-\frac{1}{n}))=-1.$ Edit: Please notice that this statement is not (necessarily) true for every n. The statement is about some constant n. Thank you and have a good day!.","['limits', 'calculus', 'functions', 'continuity']"
3052899,The application $\mu^*$ is an outer measure. A proof without the Fubin's Theorem.,"Theorem. Let $\mathcal{K}\subseteq 2^{X}$ a set family such that $\emptyset \in\mathcal{K}$ and be given an application $\nu\colon\mathcal{K}\to[0,+\infty]$ such that $\nu(\emptyset)=0$ , we define $$\mu^*(E)=\inf\left\{\sum_{n\in\mathbb{N}}\nu(I_n)\;\middle|\;E\subseteq\bigcup_{n\in\mathbb{N}}I_n\;\text{and}\;\{I_n\}_{n\in\mathbb{N}}\subseteq\mathcal{K}\right\},$$ then $\mu^*$ is an outer measure. Proof. (With Fubini's Theorem ) We prove that $\mu^*$ is $\sigma$ -subadditive, the other properties are trivial. Let $\{E_n\}_{n\in\mathbb{N}}\subseteq 2^{X}$ a countable set family. We suppose that $\mu^*(E_n)<+\infty$ for all $n\in\mathbb{N}$ , that is for all $n\in\mathbb{N}$ exists $\{I_{n,k}\}_{k\in\mathbb{N}}\subseteq \mathcal{K}$ such that $E_n\subseteq\bigcup_{k\in\mathbb{N}}I_{n,k}$ . Be fixed $\varepsilon> 0$ . For what has been said above and for the properties of the infimum, for all $n\in\mathbb{N}$ exists $\{I_{n,k}\}\subseteq\mathcal{K}$ such that $$E_n\subseteq\bigcup_{k\in\mathbb{N}}I_{n,k}\quad\text{and}\quad\mu^*(E_n)+\frac{\varepsilon}{2^n}>\sum_{k\in\mathbb{N}}\nu(I_{n,k}).$$ We observe that $$\bigcup_{n\in\mathbb{N}}E_n\subseteq\bigcup_{n\in\mathbb{N}}\bigcup_{k\in\mathbb{N}}I_{n,k}=\bigcup_{(n,k)\in\mathbb{N}\times\mathbb{N}}I_{n,k}\quad\text{and}\quad\{I_{n,k}\}_{n,k\in\mathbb{N}}\subseteq\mathcal{K}.$$ At this point, in order to apply the definition of $\mu^*$ , it is necessary to specify that it is equivalent to $$\mu^*(E)=\inf\left\{\sum_{(n,k)\in\mathbb{N}\times\mathbb{N}}\nu(I_{n,k})\;\middle|\;E\subseteq\bigcup_{(n,k)\in\mathbb{N}\times\mathbb{N}}I_{n,k}\;\text{and}\;\{I_{n,k}\}_{(n,k)\in\mathbb{N}\times\mathbb{N}}\subseteq\mathcal{K}\right\}.$$ In general, what is important is that the set $E$ can be covered by a family $\{I_\gamma\}_{\gamma\in\Gamma}\subseteq\mathcal{K}$ , where $\Gamma$ is a countable set of indices. By definition of $\mu^*$ we have \begin{equation}
\mu^*\bigg(\bigcup_{n\in\mathbb{N}}E_n\bigg)\le\sum_{(n,k)\in\mathbb{N}\times\mathbb{N}}\nu(I_{n,k})\color{BLUE}{=}\sum_{n\in\mathbb{N}}\sum_{k\in\mathbb{N}}\nu(I_{n,k})<\sum_{n\in\mathbb{N}}\bigg[\nu(I_{n,k})+\frac{\varepsilon}{2^n}\bigg]=\sum_{n\in\mathbb{N}}\mu^*(E_n)+\varepsilon,
\end{equation} where the blue equal is the Fubini's Theorem. $\hspace{9cm}\square$ Proof. (Without Fubini's Theorem ) Let $\{E_n\}_{n\in\mathbb{N}}\subseteq 2^{X}$ a countable set family. We suppose that $\mu^*(E_n)<+\infty$ for all $n\in\mathbb{N}$ , that is for all $n\in\mathbb{N}$ exists $\{I_{nk}\}_{k\in\mathbb{N}}\subseteq \mathcal{K}$ such that $E_n\subseteq\bigcup_{k\in\mathbb{N}}I_{nk}$ . Be fixed $\varepsilon> 0$ . For what has been said above and for the properties of the infimum, for all $n\in\mathbb{N}$ exists $\{I_{nk}\}\subseteq\mathcal{K}$ such that $$E_n\subseteq\bigcup_{k\in\mathbb{N}}I_{nk}\quad\text{and}\quad\mu^*(E_n)+\frac{\varepsilon}{2^n}>\sum_{k\in\mathbb{N}}\nu(I_{nk}).$$ Let $f\colon\mathbb{N}\to\mathbb{N}\times\mathbb{N}$ be a bijection and we place $I_{n,k}:=I_{nk}$ for all $(n,k)\in\mathbb{N}\times\mathbb{N}$ . We consider the sequence $\{I_{f(r)}\}_{r\in\mathbb{N}}$ . We prove that $\{I_{f(r)}\}_{r\in\mathbb{N}}\subseteq\mathcal{K}$ and that is a covering of $\bigcup_{n\in\mathbb{N}} E_n$ .We observe that $$\bigcup_{r\in\mathbb{N}}I_{f(r)}= \bigcup_{(n,k)\in\mathbb{N}\times\mathbb{N}}I_{nk}=\bigcup_{n\in\mathbb{N}}\bigg[\bigcup_{k\in\mathbb{N}}I_{nk}\bigg]\supseteq\bigcup_{n\in\mathbb{N}}E_n.$$ By definition of $\mu^*$ we have $$\mu^*\bigg(\bigcup_{n\in\mathbb{N}}E_n\bigg)\le\sum_{r\in\mathbb{N}}\nu(I_{f(r)}).$$ Now we place $$f(r):=(n_r,k_r)\quad\text{and}\quad K_r=\max\{k_1,\dots, k_r\},$$ and we consider the partial sum s-th. Therefore \begin{equation}
\begin{split}
\sum_{r=1}^s \nu(I_{f(r)})=&\nu(I_{f(1)})+\cdots+\nu(I_{f(s)})\\
=&\nu(I_{n_1k_1})+\cdots+\nu(I_{n_sk_s})\quad\text{We remember that}\quad I_{nk}:=I_{n,k}\\
\color{RED}{\le}& \sum_{n=1}^{K_r}\color{BLUE}{\sum_{k\in\mathbb{N}}\nu(I_{nk})}\\
<&\sum_{n=1}^{K_r}\bigg[\mu^*(E_n)+\frac{\varepsilon}{2^n}\bigg]\\
<&\sum_{n\in\mathbb{N}}\bigg[\mu^*(E_n)+\frac{\varepsilon}{2^n}\bigg]\\
=&\sum_{n\in\mathbb{N}}\mu^*(E_n)+\varepsilon.
\end{split}
\end{equation} For $\varepsilon\to 0$ we have $$\sum_{r=1}^s \nu(I_{f(r)})\le\sum_{n\in\mathbb{N}}\mu^*(E_n).$$ Question 1. Why is the inequality in red true? $$$$ Question 2. Could it be that some blue series is divergent? $$$$ Therefore $$\sum_{r\in\mathbb{N}}\nu(I_{f(r)}):=\lim_{s\to+\infty}\sum_{r=1}^s \nu(I_{f(r)})\color{BLUE}{\le}\sum_{n\in\mathbb{N}}\mu^*(E_n)$$ Question 3. Why is the inequality in blue true? In general if $\{a_n\}$ and $\{b_n\}$ are two real number sequence such that $\lim a_n=a\in\mathbb{R}$ and $\lim b_n=b\in\mathbb{R}$ , then if $a_n\le b_n$ for all $n\in\mathbb{N}$ , then $a<b$ . Is this the case? That is, the series $\sum_{n\in\mathbb{N}}\mu^*(E_n)$ is convergent? Thanks for your patience!","['proof-explanation', 'measure-theory', 'proof-verification', 'proof-writing']"
3052912,"In algebraic geometry, what kind of theory can only be described by topos but not a site?","A (Grothendieck) topos is defined to be a category equivalent to the category of sheaves on some site. The main difference for a topos and a site is about their morphisms. I noticed that some books about Etale cohomology don't use the notion of topos. It seems study of sites is enough for Etale cohomology. But topos was invented in algebraic geometry. I wonder in what area of algebraic geometry, study of topoi can not be replaced by study of sites?","['topos-theory', 'algebraic-geometry', 'big-picture']"
3052915,Find $x$ such that $(x^2 + 4x + 3)^x + (2x + 4)^x = (x^2 + 4x + 5)^x$,"Find all $x \in (-1, +\infty)$ such that $(x^2 + 4x + 3)^x + (2x + 4)^x = (x^2 + 4x + 5)^x$ . What I have done so far was a substitution $y = x + 2$ which results in a nicer form of the equation: $(y - 1)^{y - 2}(y + 1)^{y - 2} + 2^{y - 2}y^{y - 2} = (y^2 + 1)^{y - 2}$ and $y > 1$ , which can then be rewritten as: $$ \left( \frac{y^2 + 1}{2y} \right)^{y - 2} - \left( \frac{y^2 - 1}{2y} \right)^{y - 2} = 1 $$ Also, my intuition is that $y = 4$ is the unique solution and initially I wanted to prove that this function of $y$ is injective, but it didn't work either. Do you have any suggestion on how I can continue?","['algebra-precalculus', 'exponential-function']"
3052935,Octahedra with four equilateral faces,"Let $A_1, A_2, A_3, A'_1, A'_2, A'_3$ be the vertices of a (not-necessarily convex) octahedron; here $X'$ is the vertex not on an edge with $X$ . Suppose that the four non-adjacent triangular faces $A_1 A_2 A_3$ , $A_1 A'_2 A'_3$ , $A'_1 A_2 A'_3$ , $A'_1 A'_2 A_3$ are equilateral with sides of lengths $x_0, x_1, x_2, x_3$ . I am interested in knowing the conditions on $x_0,x_1,x_2,x_3$ that must be satisfied for such a configuration to exist; and how unique the configuration is.  This is a preliminary step to understanding this Question on Octahedra and the answer by @Blue. Clearly it is necessary that $x_i>0$ for each $i$ ; and by using the triangle inequality on the other four faces, that $x_i+x_j>x_k$ for every 3-set $\{i,j,k\}\subset \{0,1,2,3\}$ . Question 1: are these conditions sufficient for existence? I think so, but have only a hand-waving argument and would like something better. It seems to me that if such a configuration exists for some $x_0,x_1,x_2,x_3$ then in general if we fix $A_2, A_3, A'_1, A'_2, A'_3$ there are two points $A_1$ which satisfy the right conditions, giving a convex octahedron and a non-convex one. Again I can only wave my hands, and my intuition may be wrong. So, Question 2: what can  be proved about uniqueness? Lastly, to get some feeling for which $(x_0,x_1, x_2,x_3)$ yield an octahedron, I would like to understand their geometry. Question 3: Describe geometrically  the (subset of) the solid bounded by \begin{align*}
x_0+x_1+x_2+x_3&=1;&\\ 
x_i&=0, &i=0,\dots,3; \\
x_i+x_j&=x_k &\textrm{  for every 3-set  }  \{i,j,k\}\subset \{0,1,2,3\}
\end{align*} which yield solutions.","['polyhedra', 'geometry']"
3052970,"Meaning of ""Derive cosine of $\theta$ from $a·b$"" ? (Not an native english speaker)","I'm currently trying to implement some vehicle physics in a game, and this obviously requires a lot of maths. However, I'm not an english native speaker, so I have trouble understanding some terms and instructions that are given to me. In my case, this is what they asked me to do : Derive cosine of the rotation angle from the dot product of $a$ and $b$ . I assume that $\theta$ here is the ""rotation angle"" they are talking about, and $a$ and $b$ are both vectors.
But what do they mean by ""Derive... from..."" ? Surely they are not asking me to calculate the derivative of $\cos \theta$ , because they would instruct me to calculate "" $-\sin \theta$ "" instead. Can anyone please enlighten me ?","['trigonometry', 'terminology']"
3052997,Eigenvalues of a rank-one update of a matrix,"Let $\lambda_1,\dots,\lambda_n$ be the eigenvalues of the symmetric matrix $A \in {\Bbb R}^{n \times n}$ . Consider a rank-one perturbation $$B=A+\rho\,uu^T$$ where $u \in {\Bbb R}^n$ . Is there an analytical expression for the eigenvalues of $B$ , exploiting the known eigenvalues of $A$ ? I'm thinking something similar to the Sherman-Woodbury formula for the update of the inverse could work, but I have not succeeded. I found some related results: Bunch–Nielsen–Sorensen formula A stable and efficient algorithm for the rank-one modification of the symmetric eigenproblem (1994) But both seem to require that $u$ is an eigenvector of $A$ . Here, I am considering a more general statement, where $u$ need not be an eigenvector of $A$ .","['matrices', 'spectral-theory', 'linear-algebra', 'eigenvalues-eigenvectors']"
3053011,About weak convergence in $L^{\infty}$,"doing my homework I'm dealing with this: Let, for all $n\in \mathbb{N} \quad f_n(t) := e^{-nt^2}, \quad t \in [-1,1]$ Show that 1) $f_n \overset{\ast}{\rightharpoonup} 0$ in $L^\infty(-1,1)$ 2) $f_n$ does not converge weakly to $0$ in $L^\infty (-1,1)$ So I did 1) simply considering \begin{equation}
|f_n(x)g(x)| \le|g(x)| \quad \forall g \in L^1(-1,1)
\end{equation} So the result follows easily from the dominated convergence theorem. But for 2) I know that given $f_n, f \in X^*$ \begin{equation}
f_n \rightharpoonup f \quad \Leftrightarrow \quad \phi(f_n)\rightarrow \phi(f) \quad \forall \phi \in X^{**}
\end{equation} But how can I identify the dual of $L^\infty$ to solve this?","['functional-analysis', 'weak-convergence', 'dual-spaces']"
3053045,Are line integral function of a function differentiable?,"I was reading Tom Apostol calculus volume 2 and came across theorem 10.4 FIRST FUNDAMENTAL THEOREM FOR LINE INTEGRALS (page 338) The theorem roughly stated: $\vec f$ be a vector field that is continuous on an open connected set S in $R^n$ and assume that the line integral of $\vec f$ is independent of path in S . $\vec a$ be  a fixed point of S and define a scalar field $\phi$ on S by the equation $$\phi(\vec x)=\int_{\vec a}^\vec x\vec f.d\vec \alpha$$ Where $\vec\alpha$ is any piecewise smooth path in S joining $\vec a $ and $\vec x $ .Then the gradient of $\phi$ exists and is equal to $\vec f $ . My question is in the proof it was proved that the $D_t \phi(\vec x)=f_t$ for all k in {1,2,3,4,5,....,n}, Whereas the theorem states gradient should exist that is the function $\phi(\vec x)$ should be differentiable, if so how to PROVE IT ?","['real-analysis', 'multivariable-calculus', 'calculus', 'functional-analysis', 'vector-analysis']"
3053088,Are there infinitely many sets of relatively prime numbers with equal number and sum of divisors?,"Consider the prime factorization of the numbers $14$ and $15$ : $$14 = 2 \cdot 7 \implies \tau(14) = 2 \cdot 2 = 4 \space ;\space \sigma(14) = 3 \cdot 8 = 24$$ $$15=3 \cdot 5 \implies \tau(15) = 2 \cdot 2 = 4 \space ;\space \sigma(15) = 4 \cdot 6 = 24$$ Thus, we can note that $\tau(14) = \tau(15)$ and $\sigma(14) = \sigma(15)$ . Here, $\tau(x)$ is the number of positive divisors of $x$ and $\sigma(x)$ is the sum of positive divisors of $x$ . There are many examples such as $\{14,15\}$ that show the above properties. Another example is $\{33,35\}$ and $\{46,51,55\}$ . More than just pairs or triples, we can have even larger sets. The smallest sextuplet (set of six positive integers) is $\{282,310,322,345,357,385\}$ . We can see that as $\{14,15\}$ have their union of prime factors as $\{2,3,5,7\}$ and since the $\tau$ and $\sigma$ functions are multiplicative functions, we can take any positive integer $k$ such that $\gcd(k,210) = 1$ and then the family of solutions $\{14k,15k\}$ would satisfy the properties. Thus, there are infinitely many pairs of distinct positive integers with equal number of divisors and equal sum of divisors. However, the question gets more interesting if we ask whether there are infinitely many such pairs with relatively prime positive integers, i.e. whether there are infinitely many such families of solutions. The 'relatively prime' part is removed to untrivialize the problem. We could however look at it at a different angle. The problem is only trivial if we could multiply equal powers of primes $p$ . Thus, we could also ask if there are infinitely many pairs of positive integers $\{a,b\}$ such that $\tau(a) = \tau(b)$ and $\sigma(a) = \sigma(b)$ , with the condition that for no prime $p$ , $\nu_p(a) = \nu_p(b) \neq 0$ . This problem is deeply related with conjectures such as Dickson's conjecture and Schinzel's Hypothesis. However, it is much weaker than such problems. Yet, when we concentrate on certain cases, for example, $\tau(a)=\tau(b)=4$ where $a,b$ are both products of two distinct primes, we get for primes $p,q,r,s$ : $$(p+1)(q+1) = (r+1)(s+1)$$ By fixing $s=7$ and $q=3$ for example , we get $p+1 = 2r+2$ which gives that for primes $p,r$ ; we have $p=2r+1$ . Thus, we are taken to the Sophie-Germain problem. Our problem is solved directly by solving any such single cases of Dickson's conjecture. Primes approximately have $0.5$ probability to be $1 \pmod{4}$ and $0.5$ probability to be $3 \pmod{4}$ . Thus, it is expected that $8 \mid (p+1)(q+1)$ . Similarly, primes approximately have $0.5$ probability to be $1 \pmod{3}$ and $0.5$ probability to be $2 \pmod{3}$ . Thus, it is expected that $3 \mid (p+1)(q+1)$ . Hence, it is better to search for $(p+1)(q+1) = (r+1)(s+1) = 24k$ . It can be seen quite clearly that even such small cases of the problem are likely to generate infinitely many solutions, like the above. Thus, it seems most likely that the answer to this question is affirmative. We can extend our problem to not only pairs, but to sets of distinct positive integers of size $n$ . We can also extend our problem by asking whether there are arbitrarily large sets of distinct positive integers with equal number of divisors and equal sum of divisors. This seems likely from the example of the sextuplet. This also directly follows from the full-fledged version of Dickson's conjecture. Finally, this is a stronger version of our previous question as we cannot have infinitely many finite positive integers in a set as they would have finite value for $\sigma(x) \geqslant x$ , which would bound the size of the set. Any suggestions and progress would be accepted. Please share any ideas/remarks in the comments. Ideas can be shared on any of the problems above.","['number-theory', 'divisor-sum']"
3053097,Find all integers $a$ that satisfy $c \equiv a \pmod{ab+1}$,"Let $a,b,c$ positive integers with $b|c$ I am looking for triplets that satisfy $c \equiv a \pmod{ab+1}$ . What I found by now is that given a solution, we can write $c=q(ab+1)+a=a(qb+1)+q$ So we get $c \equiv q \pmod{qb+1}$ where q is the whole part of $\frac{c}{ab+1}$ . Another thing I observed is that both $a$ and $c$ are coprime to $ab+1$ . This means they are both elements of the multiplicative group modulo $ab+1$ . 
It seems like something can be done to get more information, but I am stuck. Thanks","['number-theory', 'modular-arithmetic']"
3053109,How to demonstrate that a differentiable function $f: \mathbb Q\rightarrow \mathbb Q$ is not uniformly continuous,"Following the scheme given by mercio in the answer to this question , one can construct a function $f: \mathbb Q \rightarrow \mathbb Q$ , differentiable everywhere, with derivative function $$
f'(x) = \begin{cases}
0, & (x \neq 0),\\
1, & (x = 0).
\end{cases}\tag{1}\label{one}
$$ Such function cannot be uniformly continuous, because otherwise, in any closed interval containing $0$ , $f$ could be extended to a continuous function on $\mathbb R$ . Such extended function would give \begin{equation}
\lim_{h \rightarrow 0} \frac{f(h)-f(0)}{h} = 1
\end{equation} and, at the same time, \begin{equation}
\lim_{x\rightarrow 0}f'(x) = 0,
\end{equation} which would contradict de l'Hôpital's rule. My question is whether it is possible or not to demonstrate that $f$ is not uniformly continuous by using only condition \eqref{one} and the definition of uniform continuity, i.e. (for the sake of completeness) \begin{equation}
\forall \epsilon,\ \exists \delta:\ \forall x,\ y \in \mathbb Q,\ |x-y|<\delta \Rightarrow |f(x)-f(y)|<\epsilon .
\end{equation} EDIT : Wojowu pointed out to me that the function may not be differentiable once extended on $\mathbb R$ (thus not contradicting de l'Hôpital's rule) . So either $f$ is not uniformly continuous or its extension to the reals fails to be differentiable. So the question now is whether this can be estabished only by means of \eqref{one} or not, and how.","['analysis', 'real-analysis']"
3053124,prove that this function has Lebesgue measurable image,"Denote by $\lambda$ the standard Lebesgue measure. Let $E$ be a Lebesgue-measurable subset of $\mathbb{R}$ with $\lambda(E)<\infty$ . By an initial segment of $E$ we mean a set $E'\subseteq E$ satisfying $E'<E\setminus E'$ (in the sense that $x<y$ for all $x\in E'$ and $y\in E\setminus E'$ ).  Note that initial segments of $E$ must always have the form $E\cap(-\infty,y)$ or $E\cap(-\infty,y]$ for some $y\in[-\infty,\infty]$ . It can be shown that for each $t\in[0,\lambda(E)]$ , there exists an initial segment $E_t$ of $E$ with $\lambda(E_t)=t$ . Let us define the function $m:E\to[0,\lambda(E)]$ by the rule $$m(x)=\inf\{t\in[0,\lambda(E)]:x\in E_t\}.$$ Conjecture 1. The image $m(E)$ is a Lebesgue-measurable set. Discussion. (i) Clearly, $m$ is order-preserving (i.e., nondecreasing) in the sense that $x\leq y$ if and only if $m(x)\leq m(y)$ . (ii) It can be shown that $m$ is measure-preserving in the following sense:  If $A\subseteq[0,\lambda(E)]$ is Lebesgue-measurable then $m^{-1}(A)$ is also Lebesgue-measurable with $\lambda(A)=\lambda[m^{-1}(A)]$ . (iii) If $F\subseteq E$ and $m(F)$ is measurable then $\lambda[m(F)]=\lambda(F)$ . (iv) There are definite counter-examples showing that $m$ need not be surjective. In fact, $[0,\lambda(E)]\setminus m(E)$ may even be uncountable. Sorry to keep asking so many similar questions.  I keep running into these technical, seemingly obvious facts which are resistant to a simple proof (that I can find, anyway).","['measure-theory', 'lebesgue-measure', 'real-analysis']"
3053128,Expectation value of the average displacement squared in a random walk,"Consider a simple 1D random walk, with equal probability of going to the right (toward positive x) by one unit of distance and to the left (toward negative x) with one unit of distance. Let x=0 be the initial position of the particle and  D be the position of the particle at the end of the walk. If the random walk consist of N steps, then $<D^2>=N$ . So my question is why exactly is $<D^2>$ equal to N. I have seen mathematical proofs as to why this is the case  and so I am not really looking for one. I am more looking for an intuitive answer, as my intuition tells me that $<D^2>$ should be 0, since the particle shouldn't really be moving away from the origin as the probability of going to the right or left is the same, so it should really oscillate near the origin. Does anyone have an intuitive reason as to why this is not the case? Thank you.","['brownian-motion', 'probability']"
3053131,Why are the local extrema of a log-transformed function equal to local extrema of the original function?,"I am studying maximum likelihood and to simplify taking the derivative of the likelihood function, it is often transformed by the natural log before taking the derivative. I have read in other posts that this is because the logarithm is a monotonic function, so its extrema will be the same as the original function. However, I do not understand why this is the case. Can someone explain intuitively why the transformation does not affect the local extrema?","['monotone-functions', 'derivatives', 'maximum-likelihood', 'logarithms']"
3053133,Is every infinite compact space with no isolated points uncountable?,"I know that every nonempty Hausdorff compact space with no isolated points is uncountable, so I was wondering if we could substitute the nonempty Hausdorff part with it being infinite.",['general-topology']
3053156,How do I solve this inequality? $3\sin(2x)> \sin(x)+\cos(x)+1$,How do I solve this inequality? $3\sin(2x)> \sin(x)+\cos(x)+1$ I am unable to think of a method to solve this. Writing $\sin(2x)=2\sin(x)\cos(x)$ don't seem to help,"['trigonometry', 'inequality']"
3053165,A vector field with an integrating factor has a first integral,"I have a problem that I don't know how to solve it, It says: Let $X=(f,g)\in \mathcal C^1(\mathbb{R})^2$ be a vector field and consider the system $\dot x = f(x,y), \; \dot y = g(x,y) $ . If the system has an integrating factor $\mu(x,y) $ , prove that $X$ has a first integral $H(x,y)$ . The hints and definitions that I have are: $\mu(x,y) $ is a integrating factor iff $\mu(x,y)\neq 0, \; \forall
   (x,y)$ and $div (\mu X)=0 $ If $H$ is a first integral of $X$ , then $DH(x,y)X(x,y)=0$ If $divX=0$ , then $X$ has a first integral (with some conditions of the domain I think, but it works in this problem) With all of this, we know that $\mu  X$ has a first integral $\tilde H$ , and I think I should prove that $\frac 1\mu \tilde H$ is the first integral of $X$ that I'm looking for. Hence, I'm trying to prove that $D\left(\frac 1\mu \tilde H \right)(x,y)X(x,y)=0$ , using that $D\tilde H(x,y) (\mu X)(x,y) =0$ and $div(\mu X)=0 $ On the one hand, $$ div (\mu X)=\mu \;div X + \frac{\partial \mu}{\partial x} f+\frac{\partial \mu}{\partial y} g=0 $$ On the other hand, $$D\left(\frac 1\mu \tilde H \right)(x,y)X(x,y)=-\frac1{\mu^2}\tilde H \left(\frac{\partial \mu}{\partial x} f+\frac{\partial \mu}{\partial y} g\right)+D\tilde H\cdot X = \frac 1\mu div X \cdot \tilde H$$ amd I'm not sure how I can continue, or if I'm in the correct way. Any help is welcome!
Thanks","['integrating-factor', 'vector-fields', 'ordinary-differential-equations']"
3053174,Notation for Higher Antiderivatives?,"Higher derivative are blessed with many notations. For example $$ f',f'',...$$ or $$ \frac {dy}{dx}, \frac {d^2 y}{dx^2},...$$ I have not seen any notations for higher anti-derivatives. For example, the higher anti-derivatives of $$f(x)=2x+5$$ are $$x^2+5x+c_1, \frac {x^3}{3} +\frac {5}{2}  x^2 + c_1x +c_2,.....$$ Are there notations for higher anti-derivatives?",['integration']
3053269,Continuous image of a Polish space to another has the Baire property,"Here's a theorem in the course notes of a course on Polish groups: Let $X$ and $Y$ be Polish spaces and $f:X\to Y$ continuous. $f(X)$ has the Baire property. In the course note, it's written that this follows from the existence of $U(A)$ (which I define bellow). The proof should be easy otherwise I would have written it. Unfortunately, while reviewing the notes, I forgot the proof and I'm unable to prove the theorem above. Here's the definition of $U(A)$ : Let $X$ be a Polish space and $A\subseteq X$ . $$U(A):=\bigcup\{O\mid A\text{ is comeager in $O$, i.e, $O\backslash A$ is meager}\}.$$ We have the following facts: $U(A)$ is open. $A$ is comeager in $U(A)$ . $A$ has the Baire property $\iff$ $A\backslash U(A)$ is meager. Attempt So I tried to show that $f(X)\backslash U(f(X))$ is meager. If $f(X)$ is meager, we're done. Otherwise $U(f(X))\neq\emptyset$ and $U(f(X))\backslash f(X)$ is meager. I coudln't find a way to use the continuity of $f$ to conclude. I also tried to prove the theorem by contradiction using $$f(X)\backslash U(f(X))=\bigcap\{f(X)\backslash O\mid f(X)\text{ is comeager in $O$}\}$$ but I couldn't get to the result. Could you please help me?","['general-topology', 'descriptive-set-theory', 'polish-spaces']"
3053296,"ODE of second order, proving that polynomials at $t_0$ are zero","The following ODE is given: $$y''(t) + p(t)y'(t) + q(t)y(t)=0$$ When $p(t), q(t)$ are continuous functions.
We are given two linear independent solutions $y_1(t), y_2(t)$ and also $y_1''(t_0) = y_2''(t_0) = 0$ . I need to prove that $p(t_0) = q(t_0) = 0$ . What I've tried is just placing zero in the second derivative for each function in the ODE, and working with the Wronskian. However I end up with $$p(t)(y_1'(t_0) - y_2'(t_0)) + q(t)(y_1(t_0) - y_2(t_0))$$ which is not the Wronskian. Any help?",['ordinary-differential-equations']
3053298,"Loomis and Sternberg: Tangent Space to a manifold, using equivalence classes; help justifying one step of an argument","I am currently reading through the section in Loomis and Sternberg's Advanced Calculus (Revised Edition) on tangent spaces, but I'm having trouble justifying one step of the argument (on pp. 373-374, shown below). Here are the definitions and notations used by them. Let $M$ be a differentiable manifold (here they model their manifolds on a Banach space $V$ rather than some $\mathbb{R}^n$ ). Let $x \in M$ , and let $\varphi : I \to M$ be a differentiable map where $I$ is an interval in $\mathbb{R}$ containing $0$ , and $\varphi(0) = x.$ Then, they define an operator $D_{\varphi}: C^{\infty}(M) \to \mathbb{R}$ by $D_{\varphi}(f) = (f \circ \varphi)'(0)$ . Next, they define an equivalence relation on all the curves passing through $x$ by $\varphi \sim \psi$ if and only if $D_{\varphi} = D_{\psi}$ , and call an equivalence class of curves, $\xi$ to be a tangent vector at $x$ . So far so good. Next, they go to a chart $(W, \alpha)$ , and the underlined section below is what I don't fully understand. I get that $\varphi \sim \psi$ if and only if for every $f \in C^{\infty}(M)$ , $d(f \circ \alpha^{-1})_{\alpha(x)}((\alpha \circ \varphi)'(0))$ $= d(f \circ \alpha^{-1})_{\alpha(x)}((\alpha \circ \psi)'(0))$ . But I don't see how to conclude from here that the two vectors $(\alpha \circ \varphi)'(0)$ and $(\alpha \circ \psi)'(0)$ are equal. I'm guessing it has something to do with the fact that the two derivatives are equal for every $f$ ; if we somehow choose an $f$ such that the differential $d(f \circ \alpha^{-1})_{\alpha(x)} : V \to \mathbb{R}$ is injective, then its kernel is $\{0\}$ , and thus equality follows. However I doubt this is always possible, since in general $\dim(V) > 1$ , so a linear map from $V$ to $\mathbb{R}$ cannot be injective. Any help justifying this step is much appreciated.","['differential', 'tangent-spaces', 'smooth-manifolds', 'differential-topology', 'differential-geometry']"
3053318,What conditions set an upper bound to the number of roots of a real function?,"Let $f$ be a real function. Which conditions on $f$ let me give an upper bound to the number of roots of $f$ ?
I'm intentionally vague on the conditions of $f$ , everything it's ok.
I would like to have some non-trivial general result, I think this statement should be true: If $f \in C^k(I)$ and $f^{(k)}$ has exactly $m$ roots then $f$ has at most $m+k$ roots. Anything better?","['calculus', 'functions', 'roots']"
3053327,All norms defined on a finite dimensional normed linear space are equivalent,"Given that $E$ is a finite dimensional normed linear space. Let $\dim E=n\geq 1$ and $\{e_i\}^{n}_{i=1}$ be a basis for $E.$ Then, there exists unique scalars $\{\alpha_i\}^{n}_{i=1}$ such that \begin{align}x=\sum_{i=1}^{n}\alpha_i e_i.\end{align} PROOF I proved here Prove that $\| \cdot \|_0$ defined by $\| x \|_0=\max\limits_{1\leq i\leq n}|\alpha_i|$ is a norm on $E$ . that $\| \cdot \|_0$ defined by $\| x \|_0=\max\limits_{1\leq i\leq n}|\alpha_i|$ is a norm on $E$ . So, the next thing to do, is to prove that any norm $\| \cdot \|$ defined on $E,$ is equivalent to $\| \cdot \|_0$ . So, for any $x\in E,$ \begin{align}\|x\|=\|\sum_{i=1}^{n}\alpha_i e_i\|\leq \max\limits_{1\leq i\leq n}|\alpha_i|\big\|\sum_{i=1}^{n}e_i\big\| \leq \max\limits_{1\leq i\leq n}|\alpha_i|\sum_{i=1}^{n}\big\|e_i\big\|=\beta\| x \|_0,\end{align} where $\beta:=\sum_{i=1}^{n}\big\|e_i\big\|.$ Now, define $S=\{x\in E: \| x \|_0=1\}.$ Clearly, $S$ is compact. Let \begin{align}\psi:(E&,\| \cdot \|_0)\longrightarrow \Bbb{R},\\& x\mapsto \psi(x)=\|x\|\end{align} Let $\epsilon>0$ and $ x,y\in E$ be arbitrary such that $\Big\Vert   x-y\Big\Vert_0<\delta,$ then \begin{align}\left|\psi\left(x \right)-\psi\left(y \right)\right|&=\left|\Vert x\Vert-\Big\Vert y \Big\Vert\right| \\&\leq \Big\Vert   x-y\Big\Vert \\&\leq \beta\,\Big\Vert   x-y\Big\Vert_0 \\&<\beta \delta. \end{align} So, given any $\epsilon>0$ , choose $\delta=\dfrac{\epsilon}{\beta+1}>0,$ then \begin{align}\left|\psi\left(x \right)-\psi\left(y \right)\right|&<\beta \delta=\beta\left(\frac{\epsilon}{\beta+1}\right)<\epsilon. \end{align} Thus, $\psi$ is uniformly continuous on $E$ and is automatically continuous on $E$ . Since $S\subseteq E$ , then $\psi$ is continuous on $S$ , and the minimum is attained in the set, i.e. there exists $t_0\in S$ such that $\psi(t_0)=\min\limits_{t\in S} \psi(t)$ and \begin{align}0<\psi(t_0)\leq \psi(t)=\|t\|,\;\;t\in S.\end{align} Let $u=\frac{x}{\| x\|_0}$ , then $u\in S$ and \begin{align}\gamma\leq \psi(u)=\Big\|\frac{x}{\| x\|_0}\Big\|\implies \gamma \| x\|_0\leq \| x\|,\;\;\gamma:=\psi(t_0).\end{align} Finally, we have \begin{align}\gamma\leq \psi(t)=\Big\|\frac{x}{\| x\|_0}\Big\|\implies \gamma \| x\|_0\leq \| x\|\leq \beta\| x \|_0, \;\;\text{for some} \;\;\gamma,\beta>0.\end{align} Therefore, any norm $\| \cdot \|$ defined on $E,$ is equivalent to $\| \cdot \|_0$ and we are done! Kindly help check if the proof is correct. QUESTION: What gives the assurance that $\psi(t_0)>0?$","['normed-spaces', 'proof-verification', 'linear-algebra', 'functional-analysis']"
3053339,How to find interval where function $f(x)=x+\frac{1}{x^{3}}$ is one to one\injective?,"How to find interval where function is one to one $f(x)=x+\frac{1}{x^{3}}$ ?(graphically or algebraically analytically)
 Let $f(x) =f(y)$ this gives $ (x-y)(\frac{(xy)^{3}-y^{2}-x^{2}+xy}{(xy)^{3}})=0 $ I don't know how to proceed further.","['calculus', 'algebra-precalculus', 'real-analysis']"
3053349,What is a simple definition of the pullback of a section?,"I am simply asking for a definition for something everyone uses but nobody defines. Really, this is used in class and in Hartshorne, and I have tried to look for a definition in Hartshorne, Qing Liu, Wikipedia, nothing comes up, so I am wondering whether somebody on this planet knows a definition of this. Let $X,Y$ be topological spaces and $F, G$ be sheaves of modules over $X,Y$ respectively. The pull-back of a sheaf is very well-documented and defined everywhere with high precision: If $f:X\rightarrow Y$ is a continuous map, then $f^*G=f^{-1}G\otimes_{f^{-1}O_Y} O_X$ So I know what $f^*G$ and what $(f^*G)(U)$ are (with $U \subset X$ ). But what is $f^*s$ if $s\in G(Y)$ , or more generally $s \in G(U)$ where $U \subset Y$ is some open subset of $Y$ ? I know there is already a discussion in this thread and apparently the definition is given in a comment for affine schemes (it is just the image by the induced ring map), but I don't find it particularly enlightening. Could somebody please provide a straightforward definition for the pull-back of a section of a sheaf of modules on a general scheme? Can it be defined in a simple way (with e.g. a formula) without using high-powered, unintelligible stuff? In particular I don't know what adjunction correspondance is...","['pullback', 'definition', 'algebraic-geometry', 'sheaf-theory']"
3053366,Evaluating $\lim_{n\rightarrow\infty} n[(1+\frac{1}{n})^{n+1} - e]$,"How can I show, preferably with elementary methods, that $$\lim_{n\rightarrow\infty} n\left[\left(1+\frac{1}{n}\right)^{n+1} - e\right] = \frac{e}{2}$$ ? All that would suffice for me is to show that the limit exists and is not negative. I've tried toying with binomial expansion but it didn't amount to anything unfortunately.","['limits', 'calculus']"
3053395,How to solve the ODE: $(x-1)^2y'' -2y = (x-1)^2-\frac{1}{x-1}$?,"How to solve the ODE: $(x-1)^2y'' -2y = (x-1)^2-\frac{1}{x-1}$ for $x \ne 1$ I can multiply the equation by $(x-1)$ but I still don't see how that helps. This is clearly not an Euler equation and otherwise I only know to solve such equation with constant coefficients. What I've tried is guessing that there is a solution of the form of a polynomial, which I can later use to reduce the order. However I am struggling to find such a polynom. Help will be appreciated.",['ordinary-differential-equations']
3053412,Fake proof of differentiability,"It's a theorem that if $f\colon U\subset\Bbb R^n\to \Bbb R^m$ has the property that each of the partial derivatives $\partial_if_j$ exist and are continuous $p\in U$ , then $f$ is differentiable at $p$ . When I was trying to prove this, I came up with the following ""proof"" which doesn't use the  continuity hypothesis. Can someone tell me what's wrong with this proof? Since $f_j$ is differentiable at $p$ , we can write $$
f_j(p+v) = f_j(p) + \sum_i \partial_if_j(p)v_i + R_j(v),
$$ where $|R_j(v)|/|v| \to 0$ as $v\to 0$ . Hence, we can write \begin{align*}
f(p+v) &= f(p) + \big(\sum_i \partial_if_1(p)v_i + R_1(v),\dots,\sum_i \partial_if_m(p)v_i + R_m(v)\big) \\
&= f(p) + \sum_j\big(\sum_i\partial_if_j(p)v_i\big)e_j + R_j(v)e_j \\
&= f(p) + [Df_p][v] + (R_1,\dots,R_m)(v),
\end{align*} where $[Df_p] = [\partial_if_j(p)]$ is the usual Jacobian  matrix, and $[v]$ is the column vector $[v_1\ \dotsb\ v_n]^T$ . Now, $$
\frac{|(R_1,\dots,R_m)(v)|^2}{|v|^2} = \frac{R_1(v)^2 + \dots + R_m(v)^2}{|v|^2} \to 0,
$$ where the last expression goes to $0$ as $v\to 0$ since it is a sum of finitely many terms, each of which goes to $0$ . Hence we have written $f(p+v)$ as a sum of a constant term, a linear part, and a sublinear piece, so $f$ is differentiable at $p$ . At no point did I explicitly use the continuity hypothesis, so what exactly is wrong with this proof? Best.","['multivariable-calculus', 'proof-verification', 'derivatives', 'real-analysis']"
3053426,Intuition for why $f_{xy} = f_{yx}$,"If we have a function $f(x,y)$ , why is it that $f_{xy} = f_{yx}$ ? I'm looking for an intuitive, qualitative reason rather than a rigorous proof. $f_{yx}$ represents the rate of change of the gradient parallel to the $x$ axis, as you move along the $y$ axis. Similarly, $f_{xy}$ represents the rate of change of the gradient parallel to the $y$ axis, as you move along the $x$ axis. At least, this is how I understand it. However, I can't see any reason why the two should be the same.","['multivariable-calculus', 'intuition']"
3053435,Automorphisms of generic hyperplane sections,"Let $X\subset \mathbb {P}^n=\mathbb {CP}^n$ be a smooth hypersurface of degree $d$ , $\{H_\lambda\}_{\lambda\in {\mathbb P^n}}$ be the set of hyperplane sections of $X$ . We exclude the case $(d,n-2)=(4,2)$ or $(3,1)$ to ensure the automorphism of $H$ preserve polarizations, and also exclude the trivial case $d\leq 2$ . I want to know if the following is true: For a general $\lambda\in \mathbb P^n$ , ${\rm Aut}(H_\lambda)=id$ . Easy to see it is enough to show there exist one $\lambda$ with $Aut(H_\lambda)=id$ , but it is still unknown to me. Is this some known fact? I am aware of the fact that generic hypersurfaces (or more general, generic complete intersections) have trivial automorphism group. But I didn't see how to relate it with this.","['complex-geometry', 'algebraic-geometry', 'intersection-theory']"
3053446,Curvature of curve: equivalence between tangent vector and angle definitions,"I know that curvature for some curve $C$ defined parametrically is: $$\kappa=\left\|{d\vec{T}\over ds}\right\|$$ Which basically is the rate at which the tangent vector to the curve changes, as the arclength of the curve changes. In another source, I saw the definition of curvature as the following: If $P_1$ and $P_2$ are two points on the curve, $|P_1P_2|$ is the arclength between those two points, and $\Phi$ is the limit of the angle between tangent vectors at the points $P_1$ and $P_2$ (as it goes to zero I assume), then the curvature is defined as: $$\kappa=\lim_{|P_1P_2|\to 0}{\Phi\over |P_1P_2|}$$ Which basically means, the rate at which the angle of tangent vectors in global frame of reference changes, as the arclength of the curve changes. I assume that this second definition can be rewritten using the notation from the first example as: $$\kappa={d\phi\over ds}$$ Where $\phi$ is the angle between the vector tangent to the curve, and some constant global axis of reference (which could be the x axis, but realy it could be any line or vector on the same plane). Given the second (weird in my opinion) definition of curvature, I can't see how those two definitions can be equivalent. Maybe they are not, I don't know. May be they are; if yes, how? Also, here's a picture of the section from the book where the second definition appears in (it's not in English): Note that the text agrees with yet another definition of curvature, which I am aware of: $\kappa=\frac1r$ , where $r$ is radius of curvature.","['vectors', 'curves', 'curvature', 'calculus', 'differential-geometry']"
3053448,"Puzzle group of $4\times 4$ ""flip"" game","I have been goofing around with the game ""flip,"" which can be played at the following link . The puzzle consists of an $n\times n$ grid of squares that are either black or white, and when one clicks on one of the squares, its color and the colors of each of its four neighbors are toggled. It is easy to show that any board configuration is solvable for the $2\times 2$ and $3\times 3$ versions of the puzzle. Thus, since game moves commute, the groups corresponding to these puzzles are $\mathbb Z_2^4$ and $\mathbb Z_2^9$ respectively. However, I have discovered that the $4\times 4$ game is not so simple; there exist some board configurations that are not solvable. This can be demonstrated as follows. Suppose we color the board like this: One may easily verify that any move toggles an even number of the red-colored squares; thus, in any solvable puzzles, the number of black squares in the red region is even. This narrows down the group of solvable puzzles to $\mathbb Z_2^{15}$ ; however, since reflections/rotations of this coloring produce further restrictions, the group must be even smaller than this. In fact, using this strategy, I have narrowed it down to a subgroup of $\mathbb Z_2^{14}$ , and I suspect it can be narrowed down even further to $\mathbb Z_2^{13}$ . My question is: what is the puzzle group of the $4\times 4$ puzzle? I know this problem can be reduced to finding the rank of a $16\times 16$ matrix, but... surely there's a better way.","['puzzle', 'logic', 'group-theory', 'linear-algebra', 'abelian-groups']"
3053456,"If $f(4xy)=2y[f(x+y)+f(x-y)]$ and $f(5)=3$, find $f(2015)$","Suppose the function $f:\Bbb R\to\Bbb R$ satisfies the following conditions: $$\begin{align}
f(4xy)&=2y[f(x+y)+f(x-y)] \\[4pt]
f(5)&=3
\end{align}$$ Find the value of $f(2015)$ . I have tried to find some other hiding condition, like $f(0)=0,$ but which is useless.",['functions']
3053473,Does first countable imply equivalence of sequential and limit point compactness?,"Steen and Seebach say that: If a topological space $X$ is first countable, sequentially compactness is equivalent to limit point compactness in $X$ . Take $X=\mathbb{N}\times\{0,1\}$ , where $\mathbb{N}$ has the discrete topology and $\{0,1\}$ has the indiscrete topology. This space is first countable since the topologies on $\mathbb{N}$ and $\{0,1\}$ are first countable. Thus it should satisfy the statement in the previous paragraph. However, if one takes a sequence like $\{(n,0)\}$ in $X$ , there are no convergent subsequences for limit points like $(1,1)$ . In other words, one can't find a subsequence of $\{(n,0)\}$ that converges to $(1,1)$ , even though $(1,1)$ is a limit point of the sequence. This seems to violate the statement in the first paragraph. I don't see where I'm making the error with this example. Any help is appreciated.","['general-topology', 'first-countable', 'compactness']"
3053477,"Geometric significance of the differential of the Gauss map, $dN_p: T_p(S) \rightarrow T_p(S)$, being a self adjoint linear map.","i'm studying differential geometry for the first time and I just started reading about the gauss map and I've gotta say this stuff is pretty cool. The claim made in the title of this post is from page 142 of Do Carmo's Differential geometry book, where it is stated as ""proposition 1"". I have never taken a proper geometry class before and the text is starting to get to the point where the density of the calculus/linear algebra is making the geometric significance of the statements rather opaque. That being said, I feel that there is an 'aha' moment just around the corner with this one, and people here are usually pretty good at breaking things down for me so I thought i'd ask y'll this rather soft question. Anyway, just to refresh your memory, I'm looking to understand the geometric significance of the differential of the Gauss map, $dN_p: T_p(S) \rightarrow T_p(S)$ , being a self adjoint linear map. (in particular, what is the geometric significance of this linear map being 'self adjoint'.) Where $N: S \rightarrow S^2$ is a map from the set of normal unit vectors of a regualer surface $S$ to the unit sphere $S^2$ and $T_p(S)$ is that tangent plane of an arbitrary $p \in S$ . Also, note that $T_p(S)$ and $T_{N(p)}(S^2)$ are the same as vector spaces, which is why we can view $dN_p$ as a linear map from $T_p(S) \rightarrow T_p(S)$",['differential-geometry']
3053481,Existence of abelian group which has no “square-root” but whose “cube” has a “square-root”,"Does there exist an abelian group $G$ such that $G \ncong H \times H$ for every abelian group $H$ but $G \times G \times G \cong K \times K$ for some abelian group $K$ ? Also see Existence of topological space which has no ""square-root"" but whose ""cube"" has a ""square-root""","['group-homomorphism', 'group-isomorphism', 'category-theory', 'group-theory', 'abelian-groups']"
3053486,Sum of squared inner product of vector with spokes around unit circle is constant,"Let $v$ be any vector in the plane, and $\{w_i\}$ be $n>2$ vectors evenly spaced around the unit circle. Then it seems true that $$\sum_{i=1}^n (v\cdot w_i)^2 = k \|v\|^2$$ where $k$ is a constant independent of $v$ . My intuition is that the left-hand side, as a function of the argument $\theta$ of $v$ , has ""too much symmetry"" for its low frequency. Is there some slick proof of this identity, using e.g. the algebraic structure of the $n$ roots of unity?","['linear-algebra', 'plane-geometry']"
3053490,Galois Action on Scheme,"Let $X$ be a $K$ -scheme and $L \vert K$ be a Galois extension with Galois group $G= Gal(L,K)$ . Let consider the base change $X_L := X \otimes_K L:= X \times_{Spec(K)} Spec(L)$ . Since $X_L$ is a $L$ -scheme $G$ can act on it. My problems are following: I see some ways $G$ acting on $X_L$ and on it's structure sheaf but I'm not sure if they all coinside/corelate to each other and why? Let $g \in G$ then it induce an automorphism $g: Spec(L) \to Spec(L)$ and one can define the action of $g$ on $X_L$ via commutative diagram $$
\require{AMScd}
\begin{CD}
X_L @>{\bar{g}}  >> X_L \\
@VVprV  @VVprV  \\
Spec(L) @>{g}>> Spec(L);
\end{CD}
$$ or sugestively $\bar{g}: id_X \times g$ Let $\mathcal{F}$ be a $\mathcal{O}_{X}$ -module.
I heard that $G$ can induce canonically an "" $\mathcal{O}_{X_L}$ -linear-action"" on tnduced sheaf $\mathscr{F}\otimes \mathcal{O}_L$ acting on local sections $\mathcal{F}(U)$ for open $U$ . How concretely this action is described? Comes it from the same action as in case 1.? In the sense of local action $(\mathcal{F}(U) \otimes_K L) ^g =  \mathcal{F}(U) \otimes_K L^g$ ? So only on second summand? Or are these two actions different? Espesially I don't see how could $G$ act on local sections of an arbitrary $\mathcal{O}_{X}$ -module $\mathcal{F}$ . Futhermore this concept allows to define the so called sub- $\mathcal{O}_{X_L}$ -module $\mathcal{F}^G \subset \mathcal{F}$ of invariants. But with respect to which action of $G$ ? So the main point of my question is if 1. and 2. ""generate"" the same action and how this action extends to $\mathcal{O}_{X_L}$ -modules. Is this exactly  THE canonical Galois action on a scheme which in the literature often mentioned but nowhere explicitely described?","['group-actions', 'algebraic-geometry', 'schemes']"
3053506,Concerning the subalgebra generated by two elements,"Let $f=f(t),g=g(t) \in \mathbb{C}[t]$ be two separable polynomials of degrees $\deg(f)=n \geq 2$ and $\deg(g)=m \geq 2$ , namely, $f$ has $n$ distinct roots and $g$ has $m$ distinct roots. Denote $d=d(t)=\gcd(f(t),g(t))$ , and assume that $\deg(d)=l \geq 1$ . Assume that $d(t)=(t-\gamma_1)\cdots(t-\gamma_l)$ , 
so we can write $f(t)=(t-\gamma_1)\cdots(t-\gamma_l)(t-\alpha_1)\cdots(t-\alpha_{n-l})$ and $g(t)=(t-\gamma_1)\cdots(t-\gamma_l)(t-\beta_1)\cdots(t-\beta_{m-l})$ ,
where $\{\alpha_1,\ldots,\alpha_{n-l},\beta_1,\ldots,\beta_{m-l},\gamma_1,\cdots,\gamma_l\}$ are distinct. Further assume that $\mathbb{C}(f,g)=\mathbb{C}(t)$ . Are there nice (necessary and sufficient) conditions for $\mathbb{C}[f,g]=\mathbb[t]$ ? Examples: (1) $f=t(t-1)(t+1)=t(t^2-1)=t^3-t, g=t(t-i)(t+i)=t(t^2+1)=t^3+t$ We have $f-g=t^3-t-(t^3+t)=-2t$ , so $\mathbb{C}[f,g]=\mathbb{C}[t]$ . (2) $f=(t+1)t(t-1), g=(t+1)(t+2)(t-2)$ . By this we get that $\mathbb{C}(f,g)=\mathbb{C}(t)$ (since $\deg(d)=1$ ).
Here $\mathbb{C}[f,g]\neq \mathbb{C}[t]$ , since the D-resultant of $f$ and $g$ is $s(s+1)$ , and a necessary and sufficient condition for $\mathbb{C}[f,g]=\mathbb{C}[t]$ is that the D-resultant of $f$ and $g$ is a non-zero scalar (namely, $\in \mathbb{C}^{\times}$ ), see Theorem 2.1 . Several ideas: One plausible answer is to adjust that Theorem 2.1 to our special case; however, computing the D-resultant is very complicated for higher degrees of $f$ and $g$ , or maybe I am missing something and the D-resultant is nicer that I thought? What about sub-resultants ? (I think I can obtain a nice condition involving sub-resultants) but again in higher degrees the computations are complicated. What about SAGBI bases; it seems that $\{f,g\}$ is not a SAGBI basis, but does this tell something intersting? By Abhyankar-Moh-Suzuki theroem, $m$ must divide $n$ or vice versa. A related question is this question, which asks: For which $f,g \in k[t]$ , $k[f,g]$ is integrally closed in its field of fractions $k(f,g)=k(t)$ ?
Notice that, in our case, if $\mathbb{C}[f,g]$ is integrally closed in $\mathbb{C}(f,g)$ , then, since here $\mathbb{C}(f,g)=\mathbb{C}(t)$ , we obtain that $t$ , which is obviously integral over $\mathbb{C}[f,g]$ , already belongs to $\mathbb{C}[f,g]$ , hence $\mathbb{C}[f,g]=\mathbb{C}[t]$ . See also this question. Special cases are also wellcome. Plausible special cases: (1) $\deg(d)=1$ : Notice that in each of the two examples $\deg(d)=1$ , but this does not help in determinig if $\mathbb{C}[f,g]=\mathbb{C}[t]$ or not, because in the first example we have $\mathbb{C}[f,g]=\mathbb{C}[t]$ , while in the second example we have $\mathbb{C}[f,g]\neq\mathbb{C}[t]$ .
Perhaps I am missing a nice criterion that distinguishes between those two examples? (2) Special forms of $f$ and/or $g$ , for example $f(t)=t^n+at^{\tilde{n}}+b$ and $g(t)=t^m+ct^{\tilde{m}}+d$ . Any hints and comments are welcome! As promised in the comments, I now prove the following claim: Claim: Let $k$ be an algebraically closed field of characteristic zero and let $f=f(t),g=g(t) \in k[t]$ be such that $f'$ and $g'$ are not simultaneously zero (this, in some cases, implies that $k(f,g)=k(t)$ ).
Then the following two conditions are equivalent: (i) $k[f,g]=k[t]$ . (ii) For all $\lambda,\mu \in k$ , we have $\deg(\gcd(f-\lambda,g-\mu)) \leq 1$ . Proof: Lemma (it is necessary to assume that $k$ is algebraically closed): $k[f,g]=k[t]$ if and only if $f'$ and $g'$ are not simultaneously zero and $\psi: t \mapsto (f(t),g(t))$ is injective. (i) implies (ii): Denote $F_{\lambda}=F_{\lambda}(t):=f(t)-\lambda$ and $G_{\mu}=G_{\mu}(t):=g(t)-\mu$ . From $k[f,g]=k[t]$ follows that, for all $\lambda,\mu \in k$ , $k[F_\lambda,G_\mu]=k[f-\lambda,g-\mu]=k[t]$ By the lemma we get that, for all $\lambda,\mu \in k$ , $\psi_{\lambda,\mu}: t \mapsto (F_\lambda(t),G_\mu(t))$ is injective. Assume that there exist $\lambda_0,\mu_0 \in k$ such that $\deg(\gcd(F_{\lambda_0},G_{\mu_0}))=\deg(\gcd(f-\lambda_0,g-\mu_0)) \geq 2$ . Then there exist $a,b \in k$ , with $a \neq b$ (otherwise, $a$ is a common root of $f'$ and $g'$ contrary to our assumption that they are not simultaneously zero) , such that $(t-a)(t-b)$ divides both $F_{\lambda_0}$ and $G_{\mu_0}$ . Then $F_{\lambda_0}(a)=F_{\lambda_0}(b)=G_{\mu_0}(a)=G_{\mu_0}(b)=0$ ,
so $\psi_{\lambda_0,\mu_0}(a)=(F_{\lambda_0}(a),G_{\mu_0}(a))=(F_{\lambda_0}(b),G_{\mu_0}(b))=\psi_{\lambda_0,\mu_0}(b)$ , which contradicts the injectivity of $\psi_{\lambda_0,\mu_0}: t \mapsto (F_{\lambda_0}(t),G_{\mu_0}(t))$ . Therefore, there exist no such $\lambda_0,\mu_0 \in k$ . (ii) implies (i): If $k[f,g] \neq k[t]$ , then by the lemma we get that $\psi: t \mapsto (f(t),g(t))$ is not injective, namely, there exist $a,b$ , $a \neq b$ , such that $\psi(a)=\psi(b)$ , so $f(a)=f(b):=\lambda_0$ and $g(a)=g(b):=\mu_0$ Take $F_{\lambda_0}:=f-\lambda_0$ and $G_{\mu_0}:=g-\mu_0$ . Then, $F_{\lambda_0}(a)=f(a)-\lambda_0=0$ and $F_{\lambda_0}(b)=f(b)-\lambda_0=0$ , so $(t-a)(t-b)$ divides $F_{\lambda_0}$ ,
and $G_{\mu_0}(a)=g(a)-\mu_0=0$ and $G_{\mu_0}(b)=g(b)-\mu_0=0$ , so $(t-a)(t-b)$ divides $G_{\mu_0}$ . We obtained that $(t-a)(t-b)$ divides $\gcd(F_{\lambda_0},G_{\mu_0})=\gcd(f-\lambda_0,g-\mu_0)$ , so $\deg(\gcd(f-\lambda_0,g-\lambda_0)) \geq 2$ , 
a contradiction.","['algebraic-geometry', 'resultant', 'polynomials', 'commutative-algebra']"
3053514,Evaluate:$S_{n}=\binom{n}{0}-\binom{n-1}{1}+\binom{n-2}{n-3}-\binom{n-3}{n-6}+.......$,If $$S_{n}=\binom{n}{0}-\binom{n-1}{1}+\binom{n-2}{n-3}-\binom{n-3}{n-6}+.......$$ Does $S_{n}$ have a closed form. My Attempt $$S_{n}=\binom{n}{0}-\binom{n-1}{n-2}+\binom{n-2}{2}-\binom{n-3}{3}+.......$$ $$S_{n}=\binom{n}{n}-\binom{n-1}{1}+\binom{n-2}{2}-\binom{n-3}{3}+.......$$ $=$ coefficient of $x^n$ in $\left\{(1+x)^n-x^2(1+x)^{n-1}+x^4(1+x)^{n-2}-x^6(1+x)^{n-3}+....\right\}$ After this not able to proceed,"['binomial-theorem', 'combinatorics']"
3053528,Normal Curves of Ellipses,"Consider a graph of a lot of different ellipses with foci $(-1,0)$ and $(1,0)$ These ellipses take on the form $$\sqrt{(x+1)^2 + y^2} + \sqrt{(x-1)^2+y^2}= K $$ where $2 < K < \infty$ (the case of $K = 2$ is a degenerate case). You can graph these for several K such as $K=2.4, 2.3, 2.2, 2.1, 2.05, ...$ on a graph calculator of your choice and basically the ellipses nest nicely into each other (see here ). Now I want to generalize the following image . Which is basically depicts how lines from the origin form ""normal"" curves w.r.t circles (i.e. they always intersect at a right angle from the tangent of the circle at the point of intersect around the origin) What are the ""normal curves"" of the collection of ellipses i've listed? Work: One route is to try to describe the ellipses in standard form (which looks somewhat algebraically messy) and then try to characterize their derivatives and proceed but this feels like it would quickly get out of hand in terms of sheer amount of work so i'm hoping someone here with better intuition can highlight what the general form of the curves are.","['conic-sections', 'geometry']"
3053568,Trying to prove Sard's lemma,"This is an exercise of Analysis III of Amann and Escher: Note: here $X\subset\Bbb R^n$ is open. I was trying to follow the hint, however I cant finish a proof. My work so far: Let $x_0\in C$ . Now let $J_0$ the cube centered at $x_0$ with side length $r>0$ such that $J_0\subset X$ . Then $\Phi(J_0)$ is compact and convex, and from the mean value theorem we find that $$
\Phi(x)-\Phi(x_0)=\int_0^1\partial\Phi(x_0+t(x-x_0))(x-x_0)\, dt\tag1
$$ Hence $$
|\Phi(x)-\Phi(x_0)|\le |x-x_0|\int_0^1\|\partial\Phi(x_0+t(x-x_0))\|\, dt\tag2
$$ Then setting $\rho(r):=\max_{x\in J_0}\int_0^1\|\partial\Phi(x_0+t(x-x_0))\|\, dt$ we find that $$
\lambda_n(\Phi(J_0))\le \rho(r)^n\lambda_n(J_0)= (r\rho(r))^n\tag3
$$ Now note that $\|\partial \Phi(x_0)\|\le\max_{1\le k\le n}|\lambda_k|$ , where $\lambda_1,\ldots,\lambda_n$ are the eigenvalues of $\partial\Phi(x)_{\Bbb C}$ (where $\partial\Phi(x)_{\Bbb C}$ is the complexification of $\partial\Phi(x)$ ). However Im stuck here. I cant see that $\lim_{r\to 0^+}\rho(r)=0$ , what I see is that $\lim_{r\to 0^+}\rho(r)=\|\partial\Phi(x_0)\|$ , what is not necessarily zero. Some help will be appreciated, thank you.","['measure-theory', 'lebesgue-measure', 'real-analysis']"
3053569,When does the tensor product distribute over an infinite direct product?,"It is well known that the tensor product of $R$ -modules over some ring $R$ does not, in general, distribute over infinite direct products, an obvious example being $\mathbb Z_p \otimes_\mathbb Z \mathbb Q \neq 0$ . I also know that a sufficient condition for the tensor product to distribute is this . What other sufficient conditions are there? For instance, it would seem intuitive that $\mathbb Q \otimes_\mathbb Z \prod_\mathbb N \mathbb Z \cong \prod_\mathbb N \mathbb Q$ . But is it?","['infinite-product', 'abstract-algebra', 'tensor-products', 'modules']"
3053596,Compute $\sum\limits_{j = 0}^{m - 1} \left(c_j + 1\right)\ln\left(c_j + 1\right)$ where $c_j = \cos\left(\frac{\pi}{2m}\left(1 + 2j\right) \right)$,"As part of solving: \begin{equation}
 I_m = \int_0^1 \ln\left(1 + x^{2m}\right)\:dx.
\end{equation} where $m \in \mathbb{N}$ . I found an unresolved component that I'm unsure how to start: \begin{equation} 
 G_m = \sum_{j = 0}^{m - 1} \left(c_j + 1\right)\ln\left(c_j + 1\right),
\end{equation} where $c_j = \cos\left(\frac{\pi}{2m}\left(1 + 2j\right) \right)$ I'm just looking for a starting point. Any tips would be greatly appreciated. By the way, I was able to show (and this was part of the solution too) : \begin{equation}
 \sum_{j = 0}^{m - 1} c_j = 0
\end{equation} Edit: For those that may be interested. In collaboration with clathratus , we found that for $n > 1$ \begin{equation}
    \int_{0}^{1} \frac{1}{t^n + 1}\:dt = \frac{1}{n}\left[\frac{\pi}{\sin\left(\frac{\pi}{n} \right)}- B\left(1 - \frac{1}{n}, \frac{1}{n},  \frac{1}{2}\right)\right]
\end{equation} Or for any positive upper bound $x$ : \begin{align}
 I_n(x) &= \int_{0}^{x} \frac{1}{t^n + 1}\:dt = \frac{1}{n}\left[\Gamma\left(1 - \frac{1}{n} \right)\Gamma\left(\frac{1}{n} \right)- B\left(1 - \frac{1}{n}, \frac{1}{n},  \frac{1}{x^n + 1}\right)\right]
\end{align} Here though, I was curious to investigate when $n$ was an even integer. This is my work: Here we will consider $r = 2m$ where $m \in \mathbb{N}$ . In doing so, we observe that the roots of the function are $m$ pairs of complex roots $(z, c(z))$ where $c(z)$ is the conjugate of $z$ . To verify this: \begin{align}
 x^{2m} + 1 = 0 \rightarrow x^{2m} = e^{\pi i}
\end{align} By De Moivre's formula , we observe that: \begin{align}
 x = \exp\left({\frac{\pi + 2\pi j}{2m} i} \right) \mbox{ for } j = 0\dots 2m - 1,
\end{align} which we can express as the set \begin{align}
 S &= \Bigg\{  \exp\left({\frac{\pi + 2\pi \cdot 0}{2m} i} \right) , \:\exp\left({\frac{\pi + 2\pi \cdot 1}{2m} i} \right),\dots,\:\exp\left({\frac{\pi + 2\pi \cdot (2m - 2)}{2m} i} \right)\\
&\qquad\:\exp\left({\frac{\pi + 2\pi \cdot (2m - 1)}{2m} i} \right)\Bigg\},
\end{align} which can be expressed as the set of $2$ -tuples \begin{align}
 S &= \left\{ \left( \exp\left({\frac{\pi + 2\pi j}{2m} i} \right) , \:\exp\left({\frac{\pi + 2\pi(2m - 1 - j )}{2m} i} \right)\right)\: \bigg|\: j = 0 \dots m - 1\right\}\\
& = \left\{ (z_j, c\left(z_j\right)\:|\: j = 0 \dots m - 1 \right\}
\end{align} From here, we can factor $x^{2m} + 1$ into the form \begin{align}
x^{2m} + 1 &= \prod_{r \in S} \left(x + r_j\right)\left(x + c(r_j)\right) \\
 &= \prod_{i = 0}^{m - 1} \left(x^2 + \left(r_j + c(r_j)\right)x + r_j c(r_j)\right) \\
 &= \prod_{i = 0}^{m - 1}  \left(x^2 + 2\Re\left(r_j\right)x + \left|r_j \right|^2\right)
\end{align} For our case here $\left|r_j \right|^2 = 1$ and $\Re\left(r_j\right) = \cos\left(\frac{\pi + 2\pi j}{2m} \right)= \cos\left(\frac{\pi}{2m}\left(1 + 2j\right)\right) = c_j$ \begin{align}
    \int_0^1 \log\left( x^{2m} + 1\right)\:dx &=  \int_0^1 \log\left(\prod_{r \in S} \left(x^2 + 2c_jx+ \left|r_j \right|^2\right)\right)\\
    &= \sum_{j = 0}^{m - 1} \int_0^1 \log\left(x^2 + 2c_jx + 1 \right)\\
    &= \sum_{j = 0}^{m - 1} \left[2\sqrt{1 - c_j^2}\arctan\left(\frac{x + c_j}{\sqrt{1 - c_j^2}}\right) + \left(x + c_j\right)\log\left(x^2 + 2c_jx + 1\right) - 2x \right]_0^1 \\
    &= \sum_{j = 0}^{m - 1} \left[ 2\sqrt{1 - c_j^2}\arctan\left(\sqrt{\frac{1 - c_j}{1 + c_j}} \right) + \log(2)c_j + \left(\log(2) - 2\right) + \left(c_j + 1\right)\log\left(c_j + 1\right) \right] \\
    &= 2\sum_{j = 0}^{m - 1}\sqrt{1 - c_j^2}\arctan\left(\sqrt{\frac{1 - c_j}{1 + c_j}} \right) + \log(2)\sum_{j = 0}^{m - 1} c_j + m\left(\log(2) - 2\right)\\
    &\qquad+ \sum_{j = 0}^{m - 1}\left(c_j + 1\right)\log\left(c_j + 1\right)
\end{align} Thus, \begin{align}
\int_0^1 \log\left( x^{2m} + 1\right)\:dx &=\sum_{j = 0}^{m - 1}c_j\sin\left(\frac{\pi}{2m}\left(1 + 2j\right)\right)  + \log(2)\sum_{j = 0}^{m - 1} c_j + m\left(\log(2) - 2\right)\\
&\qquad+ \sum_{j = 0}^{m - 1}\left(c_j + 1\right)\log\left(c_j + 1\right)
\end{align}","['integration', 'definite-integrals', 'logarithms', 'trigonometric-series', 'sequences-and-series']"
3053609,Chern class of a principal $G$ bundle for a compact Lie group $G$,"This question is related to this question . The user who asked this question is not active since September. So, asking a separate question here. Let $G$ be a compact Lie group and $P\rightarrow M$ be a principal $G$ bundle. We want to associate Chern classes for this bundle. This article says in its second page that ""A compact Lie group $G$ may be shown (with some work, using the theory of compact operators) to be none other than a closed subgroup of the Unitary group $U(n)$ of matrices for some $n$ "". Thus, for structure group $G$ of $P(M,G)$ there exists an embedding $G\hookrightarrow U(n)$ . See the embedding $G\hookrightarrow U(n)$ as an action of $G$ on $U(n)$ . Given a manifold $P'$ and an action of $G$ on $P'$ there is a notion of what is called an associated bundle whose fibres are $P'$ . In case $P'$ is a Lie group $H$ , we get a principal $H$ bundle. This $G$ action on $U(n)$ gives a principal $U(n)$ bundle $Q\rightarrow M$ . For this principal $U(n)$ bundle $Q\rightarrow M$ , we fix a connection $\Gamma$ with curvature $\Omega$ . We then get what is called  Weil homomorphism $W:I(U(n))\rightarrow H^*(M,\mathbb{R})$ . We choose elements $f_k:\mathfrak{u}(n,\mathbb{C})\rightarrow \mathbb{R}$ from $I(U(n))$ such that $$\text{det}\left(\lambda I-\frac{1}{2\pi\sqrt{-1}}X\right)=\sum_{k=0}^n f_k(X)\lambda^{n-k}$$ The images $W(f_k)\in H^{2k}(M,\mathbb{R})$ are called the $k$ -th Chern classes of $Q\rightarrow M$ . How do we define Chern classes of the principal $G$ bundle $P\rightarrow M$ that we have started with? Is the $k$ -th Chern class of $G$ bundle $P(M,G)$ the $k$ -th Chern class of the $U(n)$ bundle $Q(M,U(n))$ ? Does this depend on the embedding $G\hookrightarrow U(n)$ we have chosen? In case it is dependent on the choice of $G\hookrightarrow U(n)$ , what invariants of $P(M,G)$ does these give?","['de-rham-cohomology', 'characteristic-classes', 'differential-geometry']"
3053614,How to solve $\int_0^\pi(\sin x +2\sin^2 x+3\sin^3 x+\dots +100\sin^{100} x)^2 dx$,How to solve this integration? $\int_0^\pi (\sin x +2\sin^2 x+3\sin^3 x+\dots +100\sin^{100} x)^2 dx$ I tried to solve by using geometric sequence  but couldn't solve it. Does anyone have an idea?,"['integration', 'analysis', 'calculus', 'sequences-and-series', 'algebra-precalculus']"
3053616,Find $\sum_{n=1}^{\infty}\tan^{-1}\frac{2}{n^2}$,"Find $$M:=\sum_{n=1}^{\infty}\tan^{-1}\frac{2}{n^2}$$ There's a solution here that uses complex numbers which I didn't understand and I was wondering if the following is also a correct method. My proposed solution $$\begin{align}
&\sum_{n=1}^{\infty}\tan^{-1}\frac{2}{n^2}\\
=&\sum_{n=1}^{\infty}\tan^{-1}\frac{(1+n)+(1-n)}{1-(1+n)(1-n)}\\
=&\sum_{n=1}^{\infty}(\tan^{-1}(1+n)+\tan^{-1}(1-n))\\
=&\sum_{n=1}^{\infty}(\tan^{-1}(n+1)-\tan^{-1}(n-1))
\end{align}
$$ And this implies $$M=\lim_{m\to\infty}(\tan^{-1}(m+1)+\tan^{-1}m-\tan^{-1}1-\tan^{-1}0)=\frac{3\pi}{4}$$","['summation', 'proof-verification', 'real-analysis', 'sequences-and-series', 'trigonometry']"
3053618,question about martingale about maximal inequality for submartigales,"Suppose ${X_n}$ is a martingale satisfying, for some $\alpha > 1$ , $E\left[|X_n|^\alpha\right]<\infty$ , for all n.
Show $$E\left[\max_{0\leq k \leq n}|X_k| \right]\leq \frac{\alpha}{\alpha-1} E[|X_n|^{\alpha}]^{\frac{1}{\alpha}}$$ Hint: $$E\left[\max_{0\leq k \leq n}|X_k| \right]=\int _{0}^{\infty}\{\max_{0\leq k \leq n}|X_k|>t\}dt.$$ Now use the maximal inequality on the submartingale $|X_n|^{\alpha}$ Note the maximal inequality on the submartingale ${X_n}$ .
Let ${X_n}$ be a submartingale for which $X_n\geq 0$ for all n. Then for any positive $\lambda \Pr\{\max_{0\leq k\leq n} X_k>\lambda\}\leq E[X_n]$ .","['martingales', 'inequality', 'probability-theory']"
3053643,"Prove if $ABCD$ is a parallelogram, $\angle OBA = \angle ADO \iff \angle BOC = \angle AOD$","The following is a lemma someone has presented to me: Lemma: Let $ABCD$ be a parallelogram in the plane, and let $O$ be a point outside of the parallelogram.
  Prove $\angle OBA = \angle ADO$ if and only $\angle BOC = \angle AOD$ , where all angles are directed. For example, here is a diagram: However, my attempts to find the proof and name, if one exists, of this lemma have not been successful. I have tried my hand at proving this myself, but I’m not all the best at geometry, so I didn’t make much progress. Here’s a sketch of what I did: I considered the isogonal conjugation of the points $A$ and $C$ with respect to triangle $OBD$ . If we assume $\angle OBA = \angle ADO$ , then both conjugates lie on the perpendicular bisector of $BC$ , and if $\angle BOC = \angle AOD$ , then the isogonal conjugate of $A$ , $A^*$ , lies on $OC$ , and vice versa. However this method falls apart as it doesn’t encode the fact that $AB \parallel CD$ in a neat way. Any hints or input will be appreciated.","['euclidean-geometry', 'geometric-transformation', 'geometry']"
3053677,Probability of a single trial within binomial experiment vs. stand-alone bernoulli experiment,"When a flip a coin several times, each throw is independent from another. In other words, my coin does not know what came out previous time. So, each next flip the result is unpredictable and random. Now, suppose I flipped a fair coin three times and got each time a head (head-head-head). Intuitively, the head cannot come out “head” all the time, so I can expect the on fourth throw to have higher chances to get finally a tail. But each flip is an independent event - the coin does not know what came out last time. How this paradox is resolved? Many thanks!","['paradoxes', 'binomial-distribution', 'probability']"
3053683,Prove that $\sqrt[8]5 > \sqrt[9]6 > \sqrt[10]7 > \cdots$,"Prove that $\sqrt[8]5 > \sqrt[9]6 > \sqrt[10]7 > \cdots$ My friend came up with this and gave this to me as a challenge and I'm totally stuck. I have tried proving this by induction $\root{n+3}\of{n} > \root{n+4} \of {n+1} $ for all integers $n \geq 5$ with no luck. I don't even know how to prove the base case without a calculator. Also, it turns out that this is not true for $n \leq 4$ . Why would this inequality only true from $5$ onwards?","['roots', 'radicals', 'analysis', 'sequences-and-series', 'inequality']"
3053715,Relationship between Catalan's constant and $\pi$,"How related are $G$ (Catalan's constant) and $\pi$ ? I seem to encounter $G$ a lot when computing definite integrals involving logarithms and trig functions. Example: It is well known that $$G=\int_0^{\pi/4}\log\cot x\,\mathrm{d}x$$ So we see that $$G=\int_0^{\pi/4}\log\sin(x+\pi/2)\,\mathrm{d}x-\int_0^{\pi/4}\log\sin x\,\mathrm{d}x$$ So we set out on the evaluation of $$L(\phi)=\int_0^\phi\log\sin x\,\mathrm{d}x,\qquad \phi\in(0,\pi)$$ we recall that $$\sin x=x\prod_{n\geq1}\frac{\pi^2n^2-x^2}{\pi^2n^2}$$ Applying $\log$ on both sides, $$\log\sin x=\log x+\sum_{n\geq1}\log\frac{\pi^2n^2-x^2}{\pi^2n^2}$$ integrating both sides from $0$ to $\phi$ , $$L(\phi)=\phi(\log\phi-3)+\sum_{n\geq1}\phi\log\frac{\pi^2n^2-\phi^2}{\pi^2n^2}+\pi n\log\frac{\pi n+\phi}{\pi n-\phi}$$ With the substitution $u=x+\pi/2$ , $$
\begin{align}
\int_0^\phi \log\cos x\,\mathrm{d}x=&\int_0^{\phi}\log\sin(x+\pi/2)\,\mathrm{d}x\\
=&\int_{\pi/2}^{\phi+\pi/2}\log\sin x\,\mathrm{d}x\\
=&\int_{0}^{\phi+\pi/2}\log\sin x\,\mathrm{d}x-\int_{0}^{\pi/2}\log\sin x\,\mathrm{d}x\\
=&L(\phi+\pi/2)+\frac\pi2\log2
\end{align}
$$ So $$G=L\bigg(\frac{3\pi}4\bigg)-L\bigg(\frac\pi4\bigg)+\frac\pi2\log2$$ And after a lot of algebra, $$G=\frac\pi4\bigg(\log\frac{27\pi^2}{16}+2\log2-6\bigg)+\pi\sum_{n\geq1}\bigg[\frac14\log\frac{(16n^2-9)^3}{256n^4(16n^2-1)}+n\log\frac{(4n+3)(4n-1)}{(4n-3)(4n+1)}\bigg]$$ So yeah I guess I found a series for $G$ in terms of $\pi$ , but are there any other sort of these representations of $G$ in terms of $\pi$ ? really important edit As it turns out, the series $$\frac\pi4\bigg(\log\frac{27\pi^2}{16}+2\log2-6\bigg)+\pi\sum_{n\geq1}\bigg[\frac14\log\frac{(16n^2-9)^3}{256n^4(16n^2-1)}+n\log\frac{(4n+3)(4n-1)}{(4n-3)(4n+1)}\bigg]$$ does not converge, however it is a simple fix, and the series $$G=\frac\pi4\bigg(\log\frac{3\pi\sqrt{3}}2-1\bigg)+\pi\sum_{n\geq1}\bigg[\frac14\log\frac{(16n^2-9)^3}{256n^4(16n^2-1)}+n\log\frac{(4n+3)(4n-1)}{(4n-3)(4n+1)}-1\bigg]$$ does converge to $G$ . Quite amazingly, we can use this to find a really neat infinite product identity. Here's how. Using the rules of exponents and logarithms, we may see that $$\frac{G}\pi+\frac12-\log\bigg(3^{3/4}\sqrt{\frac\pi2}\bigg)=\sum_{n\geq1}\log\bigg[\frac1{4en}\bigg(\frac{(16n^2-9)^3}{16n^2-1}\bigg)^{1/4}\bigg(\frac{(4n+3)(4n-1)}{(4n-3)(4n+1)}\bigg)^n\bigg]$$ Then using the fact that $$\log\prod_{i}a_i=\sum_{i}\log a_i$$ We have $$\frac{G}\pi+\frac12-\log\bigg(3^{3/4}\sqrt{\frac\pi2}\bigg)=\log\bigg[\prod_{n\geq1}\frac1{4en}\bigg(\frac{(16n^2-9)^3}{16n^2-1}\bigg)^{1/4}\bigg(\frac{(4n+3)(4n-1)}{(4n-3)(4n+1)}\bigg)^n\bigg]$$ Then taking $\exp$ on both sides, $$\prod_{n\geq1}\frac1{4en}\bigg(\frac{(16n^2-9)^3}{16n^2-1}\bigg)^{1/4}\bigg(\frac{(4n+3)(4n-1)}{(4n-3)(4n+1)}\bigg)^n=\sqrt{\frac{2e}{3\pi\sqrt{3}}}e^{G/\pi}$$ Or perhaps more aesthetically, $$\prod_{n\geq1}\frac1{4en}\bigg(\frac{(16n^2-9)^3}{16n^2-1}\bigg)^{1/4}\bigg(\frac{(4n+3)(4n-1)}{(4n-3)(4n+1)}\bigg)^n=\sqrt{\frac{2}{3\pi\sqrt{3}}}\exp\bigg(\frac{G}{\pi}+\frac12\bigg)$$","['integration', 'pi', 'sequences-and-series', 'constants', 'catalans-constant']"
3053717,How to deal with the absolute value sign in the process of solving differential equation?,"Imagine we are given a differential equation as follows $$y'\sin x-y\cos x=0,$$ which seems to be very simple. We can solve it like this: \begin{align*}
&y'\sin x-y\cos x=0\\
\implies &\frac{{\rm d}y}{y}=\cot x{\rm d}x\\
\implies &\int \frac{{\rm d}y}{y}=\int \cot x{\rm d}x\\
\implies &\ln y=\ln\sin x+C'\\
\implies &y=e^{\ln\sin x+C'}\\
\implies &y=C\sin x.
\end{align*} Right? May be. But wait! Notice the transformation or operation from the the 3rd line to the 4th line. In general, we ought to write as these: $$\int \frac{{\rm d}y}{y}=\ln |y|+C_1,~~~\int \cot x{\rm d}x=\ln|\sin x|+C_2,$$ when we are finding the indefinite integral, namely, we should add an absolute value sign in the result. Of course, sometimes, you can cancel the sign in the final result again, indeed. But some books boldly omit the sign in the process. This is irresponsible negligence, or a valid trick?",['ordinary-differential-equations']
3053746,Is Venn diagram sufficient to prove statements for two or three sets?,"We know the general figure of Venn diagrams for two or three distinct sets. There are many formulas related to two or three sets. For example, one of Distributive Law is $$A \cup (B \cap C) = (A \cup B) \cap (A \cup C).$$ We can visualize it by using Venn Diagram, and guess that it is true. And also, we can prove it to show that each side is contained in the other side. It is my question. For rigorous proof, I know we should only use mathematical logic and theorem. Nonetheless, I want to check that Venn Diagram proof is also available for some easier cases. Can Venn Diagram be one method of proof? Can we prove that all proof by using Venn Diagram method for two or three sets is true? If we prove that, then all statements for two or three sets can be strictly proved by using Venn Diagram.",['elementary-set-theory']
3053809,"Why is the operation of differentiation called ""instantaneous change""?",It is said that Differentiation is an operation which finds the instantaneous change of the direction of a curve. Why is the word change emphasized by the word instantaneous ?,"['calculus', 'derivatives']"
3053838,Evaluate a binomial-like sum,Let $x \in \mathbb{R}$ and let $n \in \mathbb{N}$ then evaluate: $$\sum_{k=0}^n{n \choose k}\sin\left(x+\frac{k \pi }{2}\right)$$ I could only go up to breaking this sum in two parts of $\sin x$ and $\cos x$ in the following way: $$\sum_{k=0}^n{n \choose k}\sin\left(x+\frac{k \pi }{2}\right)=\sum_{k=0}^{\lfloor{n/2}\rfloor}(-1)^k{n \choose 2k}\sin x+\sum_{k=0}^{\lfloor{n/2}\rfloor}(-1)^k{n \choose 2k+1}\cos x$$ I have the intuition that $\sin(x+y)=\sin x\cos y+\sin y\cos x$ should be used at some point. Can someone please help me out?,"['trigonometry', 'binomial-theorem']"
3053882,Is strong topology on a W*-algebra a dual topology?,"I've come up with this question when reading Shoichiro Sakai's book $C^{*}$ -Algebras and $W^{*}$ -algebras Chapter 1.8. Let $A$ be a $W^{*}$ -algebra (a $C^{*}$ -algebra with a Banach space predual) and $V$ a norm-dense subspace of the predual $A_{*}\subseteq A^{*}$ of $A$ . $V$ is assumed to be invariant in the sense that for each $a\in A$ and $\varphi\in V$ , all of $(x\mapsto\varphi(ax))$ , $(x\mapsto\varphi(xa))$ , and $(x\mapsto\overline{\varphi(x^{*})})$ are still in $V$ . For example, if $A=\mathcal{B}(H)$ for some Hilbert space $H$ , then we might choose $V$ to be the set of finite-rank operators on $H$ , which is a dense subspace of $\mathcal{B}(H)_{*}$ , the set of trace-class operators on $H$ .
Or, $A_{*}$ itself is an example of invariant subspace. We can define the $V$ -strong topology on $A$ as the locally convex topology generated by the set of seminorms given by $$x\mapsto\varphi(x^{*}x)^{1/2}$$ for each positive linear functional $\varphi\in V$ . Let us denote this topology as $s(A,V)$ . Question : Is $s(A,V)$ a dual topology for the pairing $(A,V)$ ? That is, is the following true? $$\sigma(A,V)\subseteq s(A,V)\subseteq\tau(A,V)$$ I believe I could show the followings: For $A=\mathcal{B}(H)$ and $V=H^{*}\otimes H$ the set of finite rank operators, then $s(A,V)$ is the usual strong operator topology. In this case, the above claim is true. $\sigma(A,V)\subseteq s(A,V)\subseteq\tau(A,A_{*})$ . Hence, the dual of $(A,s(A,V))$ is a subspace of $A_{*}$ containing $V$ . If $B$ is the unit ball in $A$ , then $\mathbf{1}_{B}:(B,\tau(A,V))\rightarrow(B,s(A,V))$ is continuous. Edit Here is the precise definition of $\sigma(A,V)$ and $\tau(A,V)$ . $\sigma(A,V)$ is the coarsest topology making each element in $V$ a continuous linear functional on $A$ . More concretely, a neighborhood base at $0$ is given as $$\left\{a\in A:|\varphi(a)|<\epsilon\right\}$$ for $\varphi\in V$ and $\epsilon>0$ . $\tau(A,V)$ is the Mackey topology of the pairing $(A,V)$ . More concretely, a neighborhood base at $0$ is given as $$\left\{a\in A:\sup_{\varphi\in K}|\varphi(a)|<\epsilon\right\}$$ for $\sigma(V,A)$ -compact absolutely convex subset $K\subseteq V$ and $\epsilon>0$ . In other words, $K$ is a weakly compact absolutely convex subset of the Banach space $A_{*}$ , and at the same time $K$ is contained in $V$ .","['von-neumann-algebras', 'operator-theory', 'functional-analysis']"
3053909,Find an example which shows that the following inequality is sharp,"Let $E$ be a complex Hilbert space. In ( arXiv ) it was shown that for $A=(A_1,...,A_n) \in \mathcal{B}(E)^n$ we have, $$\displaystyle\frac{1}{2\sqrt{n}}\|A\|\leq \omega(A) \leq \|A\|,$$ where $$
\omega(A) = \sup_{\|x\|=1} \left(\sum_{k=1}^n |\langle A_kx,x\rangle|^2\right)^{1/2},
$$ and $$\|A\|= \left\|\displaystyle\sum_{k=1}^nA_k^*A_k \right\|^{1/2}.$$ How can we prove that $\frac{1}{2\sqrt{n}}$ is optimal?","['linear-algebra', 'functional-analysis']"
3053935,Determinant of an n x n matrix,"I do not know what this kind of matrix is called, it does not really look Circulant, but I tried to do many row and columns operation in order to make it into an upper triangular matrix so the determinant would be the product of the diagonal elements but I couldn't find a way. Any thoughts? This is the matrix : $$\begin{bmatrix}n&n-1&n-2&\cdots&2&1\\1&n&n-1&\cdots&3&2\\1&1&n&\cdots&4&3\\\vdots&\vdots&\vdots&\ddots&\vdots&\vdots\\1&1&1&\cdots&n&n-1\\1&1&1&\cdots&1&\lambda\end{bmatrix}$$","['matrices', 'determinant', 'linear-algebra']"
