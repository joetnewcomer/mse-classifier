question_id,title,body,tags
1359636,"Inverse image of $[-2,2]$ under cosine.","I solved the following problem: Let $g(z) = \cos z$.  Find $g^{-1}[-2,2]$. but my solution was kind of long.  I was wondering if there was a faster way to do this problem. Here's my solution: Write $\cos z = \frac{e^{iz} + e^{-iz}}{2}$, and $z = a+bi$, so $$\cos z = \frac{e^{-b + ia} + e^{b - ia}}{2} = \frac{e^{-b}}{2}[\cos a + i \sin a] + \frac{e^b}{2}[\cos a - i \sin a]$$ $$ = \frac{e^{-b} + e^b}{2} \cos a + i \frac{e^{-b} - e^b}{2} \sin a$$ We first of all want $\cos z$ to be real, so either (i) $e^{-b} - e^b = 0$ or (ii) $\sin a = 0$.  The first case is not interesting, this just says that $b = 0$, and we already know in this case that $\cos a \in [-1,1]$.  For the second case, we will have $a = k \pi$ for $k \in \mathbb{Z}$.  This implies that $$\cos z = \pm \frac{e^{-b} + e^{b}}{2}$$ so to make $\cos z \in [-2,2]$ we need $h(b) := e^{-b} + e^b$ to be in $[-4,4]$.  Clearly $h$ is an even function, increasing in either direction, so we need to solve $e^{-x} + e^x = 4$ for $x$.  Let $y = e^x$, then $\frac{1}{y} + y =4$, so $y^2 - 4y + 1 =0$.  Then $$y= \frac{4 \pm \sqrt{16 - 4}}{2} = 2 \pm \sqrt{3}$$ so $x = \log (2 \pm \sqrt{3})$.  Looking at the equation we were supposed to solve, we know without calculating anything that $- \log(2 + \sqrt{3}) = \log(2 - \sqrt{3})$.  Letting $\alpha = \log(2 + \sqrt{3})$, it holds that $e^b + e^{-b} \leq 4$ for $- \alpha \leq b \leq \alpha$. Thus $g^{-1}[-2,2]$ consists of the real line unioned together with $$\bigcup\limits_{k \in \mathbb{Z}} \{ k \pi + bi : -\alpha \leq b \leq \alpha\}$$","['complex-analysis', 'complex-numbers']"
1359640,Is There A Way To Calculate Expected value From Variance And Vice Versa?,"If I am given the value of one (just the value), can I calculate the value of the other?",['probability']
1359692,Uniform continuous distribution for cycles.,Let there be $n$ people standing in a circle and holding hands with probability $p$. What is the expectation value $E(X)$ for the number of 'chains' when $p=.5$? For what $p$ is $E(X)$ largest? Edit: a chain is defined by two or more people next to one another holding hands.,"['probability-theory', 'conditional-expectation', 'probability', 'expectation']"
1359698,dimension formula for fiber product of affine varieties,"Let $X \subset \mathbb{A}^n, \,  Y \subset \mathbb{A}^m, \,  Z \subset \mathbb{A}^{\ell}$ be irreducible affine varieties and let $f: X \rightarrow Z, \, g: Y \rightarrow Z$ be surjective morphisms. Problem: Show that $\dim X \times_Z Y = \dim X + \dim Y - \dim Z$, where 
  $X \times_Z Y$ is the fiber product $\left\{(a,b)\in X\times Y: \, f(a)=g(b)\right\}$. My proof when $Z=\mathbb{A}^d$: The map $f$ is given by a $d$-tuple $(f_1,\dots,f_d)$ of elements of $A(X)$ and similarly $g=(g_1,\dots,g_d)$, where $g_i \in A(Y)$. 
Since $f$ is surjective, we have an injective ring homomorphism $A(Z)=k[z_1,\dots,z_d] \hookrightarrow A(X)$, given by $z_i \mapsto f_i$. Thus we can write $A(X)=k[f_1,\dots,f_d,x_1,\dots,x_s]$, where $f_1,\dots,f_d$ are algebraically independent over $k$. Similarly, we can write $A(Y)=k[g_1,\dots,g_d,y_1,\dots,y_t]$, where $g_1,\dots,g_d$ are algebraically independent over $k$. Now $A(X) \otimes_k A(Y) = k[f_1,\dots,f_d,g_1,\dots,g_d,x_1,\dots,x_s,y_1,\dots,y_t]$, with $f_1,\dots,f_d,g_1,\dots,g_d$ algebraically independent over $k$. Then 
\begin{align}
A(X \times_Z Y) &= k[f_1,\dots,f_d,g_1,\dots,g_d,x_1,\dots,x_s,y_1,\dots,y_t]/(f_1-g_1,\dots,f_d-g_d)\\&=k[f_1,\dots,f_d,x_1,\dots,x_s,y_1,\dots,y_t] = A(X) \otimes_{k} \left(A(Y)/(g_1,\dots,g_d)\right)
\end{align} and the dimension of this last ring is $\dim X+ (\dim Y - \dim Z)$. Question 1: Any comments on the proof above? Issue with the general case: When $Z \subset \mathbb{A}^\ell$ is any $d$-dimensional affine variety, we can write $f=(f_1,\dots,f_d,\dots,f_\ell)$, where $f_i \in A(X)$, $f_1,\dots,f_d$ are algebraically independent over $k$ and $f_{d+1},\dots,f_\ell$ are integral over $k[f_1,\dots,f_d]$. Similarly for $g=(g_1,\dots,g_d,\dots,g_\ell)$.
Now, to obtain $A(X \times_Z Y)$ we have to take the quotient of $A(X \times Y)$ by all $f_i-g_i$. This gives 
\begin{align}
A(X \times_Z Y) = k[f_1,\dots,f_d,x_1,\dots,x_s,y_1,\dots,y_t] / (f_{d+1}-g_{d+1},\dots,f_\ell - g_\ell).
\end{align} The challenge now is to show that 
\begin{align}
\dim k[f_1,\dots,f_d,x_1,\dots,x_s,y_1,\dots,y_t] = \dim \frac{k[f_1,\dots,f_d,x_1,\dots,x_s,y_1,\dots,y_t]} {(f_{d+1}-g_{d+1},\dots,f_\ell - g_\ell)}\end{align} All i know about the
$f_{d+1}-g_{d+1},\dots,f_\ell - g_\ell$ is that they are integral over 
$k[f_1,\dots,f_d,g_1,\dots,g_d]$ but this seems insufficient to show that the dimension remains unchanged. Question 2: Can the algebraic argument above be adjusted to yield a proof? Question 3: Is there any other, possibly more geometric, proof of the statement?","['algebraic-geometry', 'commutative-algebra']"
1359701,Conditional probability as a primitive concept,"Most of the popular axiomatizations/theories of probability define conditional probability as a ratio involving unconditional probability.
Therefore, conditional probability is second class to unconditional one in such axiomatizations, and I suspect may suffer from some irregularities because of that. I am looking for alternative axiomatizations that have conditional probability as a primitive concept. I am aware of a few of them at this moment: J. M. Keynes. A Treatise on Probability, London: Macmillan, 1921 K. Popper. The Propensity Interpretation of Probability, British Journal of the
Philosophy of Science 10, 25â€“42, 1959 More references from http://philosophy.anu.edu.au/sites/default/files/Hajek%20-%20Conditional%20Probability.pdf What I am looking for is some kind of overview: was this approach fruitful or not, are there any recent developments, what are the major differences among attempted axiomatizations.",['probability-theory']
1359720,The length of an quarter outer circle given an inner quarter circle of known length unknown radius.,"This question was asked by someone on reddit.  He wants to know the length, $K$, that a beam must be to surround a quarter circle segment of length $L$ at a distance $d$. This is his drawing. https://i.sstatic.net/9dLlq.jpg In the drawing the length of the inner quarter circle is $L$ and $d$ is the distance between the inner and outer quarter circle. The formula I derived is $$K= \frac{\pi d}{2} + L$$ But I'm not sure. I have a tendency to make trivial mistakes. I also think there might be an easier or more elegant way to derive this. I calculated the radius of the inner circle using $C/2r = \pi$ with $C=4L$ and  $L=\frac{(r\pi)}{2}$ so $r=\frac{2L}{\pi}$. Then I added $d$ to that to get the radius of the outer circle $R=r+d$ and used $\frac{C}{2R}=\pi$ with $C=4K$ and $K=\frac{R\pi}{2}$ to calculate $K$. This is my calculation: Assuming that the curved part is a quarter circle then from $\frac{C}{D} = \frac{C}{2r}= \pi$ and dividing by $4$ because you are dealing with a quarter circle and taking the length of the curved part equal to $L (C=4L)$ Then $$\frac{L}{r} = \frac{\pi}{2}$$ and solve for $r$ to get the radius of the inner circle. $$\frac{1}{r} = \frac{\pi}{2L}$$ $r=\frac{2L}{\pi}$ Then $r+d=R$ where $R$ is the radius of the outer circle Then reapply the formula $\frac{L}{r} = \frac{\pi}{2}$ for the outer curved part with unknown length $K$. (So replace $L$ with $K$ and $r$ with $R=r+d= \frac{2L}{\pi} + d$) $$\frac{K}{R}= \frac{\pi}{2}$$ where $R= d+r = d+\frac{2L}{\pi}$ Solve for $K$. $$K=\frac{R\pi}{2} =\left(d+\frac{2L}{\pi}\right)\left(\frac{\pi}{2}\right)=\left(\frac{d\pi}{2}\right) +L$$ $$=\frac{d\pi}{2} + L$$","['geometry', 'algebra-precalculus']"
1359730,When are we permitted to multiply or divide both sides of an equation by a variable?,"As it is said in the mathematics books (at least the one I have), we are not permitted to divide or multiply both sides of an equation by a variable, because it is possible to lose some answers. For example, in the following equation $$x^2=x$$ if we divide both sides by $x$, we would have $x = 1$, but the original equation has two answers, $0$ and $1$, and we've lost $x = 0$ by dividing it by a variable. But in the same book, in order to solve a rational equation, the author multiplies both sides of the equation by $x(x-2)$, solving the equation: $$\begin{align*}
\frac {x+2}{x-2} - \frac1x &= \frac{2}{x(x-2)}\\\\
x(x-2)\frac {x+2}{x-2} - x(x-2)\frac 1x &= x(x-2)\frac{2}{x(x-2)}\\\\
x(x+2)-(x-2)&=2\\\\
x^2+x&=0\\\\
x(x+1)&=0\\\\
x&=0\\
x&=-1
\end{align*}$$ Can someone please explain when we are allowed to do this and when we are not? I got a bit confused!","['polynomials', 'algebra-precalculus']"
1359743,"If $\tan \theta = 3\frac{15}{16}$, then find $\sin \theta$","If $\tan \theta = 3\cfrac{15}{16}$, then find $\sin \theta$.",['trigonometry']
1359759,Construction of Projective Varieties Question,"I am struggling in understanding Mumford's construction of Projective Varieties. In the image I uploaded here, Are we to understand each $R_n$ as $M_n/P_n$, where $M_n:=${homogeneous polynomials in $k[x_1,...,x_n]$ of degree n} and $P_n:=${homogeneous polynomials in $P$ of degree n}? In which case, how does one consider $K(X)$, as Mumford defines it, as a ring?",['algebraic-geometry']
1359761,"Proving the integral series $\int _0^1\left(1-x^2\right)^n\,dx=\frac{2}{3}\cdot \frac{4}{5}\cdot\ldots\cdot \frac{2n}{2n+1}$","We have the series $\left(I_n\right)_{n\ge 1\:}$ where $$I_n=\int _0^1\left(1-x^2\right)^n\,dx.$$ Prove that $$I_n=\frac{2}{3}\cdot \frac{4}{5}\cdot\ldots\cdot \frac{2n}{2n+1}.$$ I tried to integrate that function for $n=1,2,3$ to see whether there's a pattern (recurrence relation), but I just can't figure out how to write the pattern and how to prove that statement. Must I use induction?","['sequences-and-series', 'integration']"
1359762,Is it possible to put an equilateral triangle onto a square grid so that all the vertices are in corners?,"In the following collection of problems - arXiv:1110.1556v2 [math.HO] - the following question is posed: Is it possible to put an equilateral triangle onto a square grid so that all the vertices are in corners? The first approach that springs to mind is to use Pick's Theorem (e.g. http://www.geometer.org/mathcircles/pick.pdf ) assuming that all vertices are on lattice points. It turns out that it is not possible (by Pythagoras, the area of an equilateral triangle with two vertices on lattice points is a rational multiple of $\sqrt3$). My question then is - how can one establish the impossibility of such a placement without resorting to Pick's Theorem?","['euclidean-geometry', 'geometry', 'analytic-geometry']"
1359781,Cycles in the Fibonacci Sequence mod n with matrices,"I was just looking at this question about Fibonacci sequence cycles modulo 5, and I happened to see a very nice solution that involved using matrices.  Using the matrix representation of the Fibonacci sequence, one can reduce the problem of finding cycles modulo $n$ to a certain problem concerning matrices.  Thus, my question is such: Given an integer $n$, does there exist an integer $x$ such that \begin{align}
\begin{bmatrix}
1 & 1 \\
1 & 0 
\end{bmatrix}^x \equiv
\begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix} \pmod n
\end{align} Surely there are some values of $n$ that have no associated $x$ satisfying the above equation.  In general, what values of $n$ have an associated $x$ that satisfies the above equation.  If you haven't looked at the linked answer, my question relates to the Fibonacci sequence in the following way:  If there exists an integer $n$ and an integer $x$ that satisfy the equation above, then the fibonacci sequence $ \pmod n$ repeats every $x$ terms.  That is,  $F_{i}\equiv F_{i+x}\pmod n$ To reiterate, can we characterize those integers $n$ for which there exists an associated integer $x$ that satisfies the matrix equation above? For $n=5$ we have the $x$ value of 20. For $n=6$ we have the $x$ value of 24. I am very interested to know if there are infinitely many solutions like this, or perhaps there is some largest $n$?  (I doubt the latter)  I am also interested to know what sorts of techniques can be used to solve the above matrix equation.  As always, please leave a comment to let me know if I messed something up or was unclear anywhere.","['fibonacci-numbers', 'modular-arithmetic', 'matrices']"
1359801,order of infinite countable ordinal numbers,"I'm trying to understand ordinal arithmetic.  If one had an ordered list of the some subset of countable ordinal numbers, what order would the following 6 countably infinite ordinals be in?  If the following order is not correct, what is correct order and why is that the correct order? $$\omega\;<\; \omega^2 \;<\; 2^\omega \;<\; \omega^\omega \;<\; {^\omega}2 \;<\;{^\omega} \omega$$ I know $\epsilon_0 = {^\omega} \omega$ is the largest, but is still countable, but I'm not sure where the powers of $2$ fit in versus the powers of $\omega$. I understand why $\omega^2$ or $\omega^n$ for any finite value of $n$ needs to be countable.  But, why does $\omega^\omega$ need to be countable?  For cardinal numbers, $2^{\aleph_0}$ is uncountably infinite.  Presumably, there would be some contradiction in mathematics if any finite ordinal arithmetic equation involving $\omega$ generated an uncountable infinity.","['ordinals', 'elementary-set-theory']"
1359802,"""At least"" type probability question.","Recently, I asked a question: Team A has more Points than team B Though I ultimately got the right answer, it took extreme casework, and long computations. My question is: suppose the question was restated: Team A plays $5$ matches with Team $B, C, D, E, F$. Every team has probability $\frac{1}{2}$ of any match it plays. What is the probability that Team A wins at least two of the matches it plays? It would take casework like: $$\frac{\binom{5}{2} + \binom{5}{3} + \binom{5}{4} + \binom{5}{5}}{1024}$$ Is there another, easier/efficient way to do this?","['contest-math', 'algebra-precalculus', 'statistics', 'combinatorics', 'probability']"
1359813,Solving for $a$ in this equation $\sin^a(a) = b$,"$$\sin^{a}a=b$$
Do you know how to solve for $a$ here algebraicaly? This is a little bit too abstract for my understanding. Thank you very much.","['calculus', 'real-analysis', 'algebra-precalculus', 'trigonometry']"
1359815,Why is it true that $|AB:A|=|B:A\cap B|$ even if $A$ is not normal in $AB$? (Second Isomorphism Theorem),"I just read about the First and Second Isomorphism Theorems in the book Abstract Algebra by Dummit and Foote. After proving the Second Isomorphism Theorem, they said: Proposition 13 isn't really important for my question (I guess) but anyway it is the one that states that if $H$ and $K$ are two finite subgroups of $G$ then $|HK|=\dfrac{|H||K|}{|H\cap K|}$. I didn't understand why $|AB:A|=|B:A\cap B|$. It is obvious if $AB$ is finite because using the Second Isomorphism theorem we have $|AB:B|=|A:A\cap B|$ because $AB/ B \cong A/ A\cap B$. Then
$$
|AB:B|=\dfrac{|AB|}{|B|}\quad\textsf{and}\quad|A:A\cap B|=\dfrac{|A|}{|A\cap B|}\\[0.3in]
\implies\dfrac{|AB|}{|B|}=\dfrac{|A|}{|A\cap B|}\\[0.3in]
\implies\dfrac{|AB|}{|A|}=\dfrac{|B|}{|A\cap B|}
$$
which proves that $|AB:A|=|B:A\cap B|$, but this doesn't prove it if $A$ or $B$ is infinite and I believe there's a simple general proof that under the asssumptions in Theorem 18 we get $|AB:A|=|B:A\cap B|$. Could anyone please help me? If the proof is easy (and I guess so because they stated the relation in the book like if it was obvious) then could you give me hints? Thank you in advance!","['abstract-algebra', 'group-theory', 'group-isomorphism']"
1359876,Does a one-to-one linear transformation from $\mathbb R^4$ to $\mathbb R^3$ exist?,"Is it possible to have a one-to-one (injective) linear transformation: $$f: \mathbb R^4 \to \mathbb R^3$$ If so, is it possible to prove that using dimension theorem ?","['linear-algebra', 'linear-transformations']"
1359880,Prove that $\lim\limits_{n \to \infty } \int_0^\infty (1 + x/n)^{-n}x^{-1/n}dx= 1$ using DCT,"Prove that $\mathop {\lim }\limits_{n \to \infty }  \int_0^\infty  {\frac{{dx}}{{{{(1 + \frac{x}{n})}^n}{x^{\frac{1}{n}}}}}}  = 1$ using dominated convergence theorem (DCT). By DCT we need to show $\left| {\frac{1}{{{{(1 + \frac{x}{n})}^n}{x^{\frac{1}{n}}}}}} \right| \le \phi (x)$ for some Lebesgue integral function $\phi(x)$ on $(0,\infty)$. My attempt solved part of the problem on $[1,\infty)$. When $x\geq1$, we have $\frac{1}{{{{(1 + \frac{x}{n})}^n}{x^{\frac{1}{n}}}}} \le \frac{1}{{{{(1 + \frac{x}{n})}^n}}}$.
Then ${(1 + \frac{x}{n})^n} = 1 + C_n^1\frac{x}{n} + C_n^2\frac{{{x^2}}}{{{n^2}}} + ... \le 1 + C_n^1\frac{x}{n} + C_n^2\frac{{{x^2}}}{{{n^2}}} = 1 + x + \frac{{n(n - 1)}}{{2{n^2}}}{x^2}$. Since $\frac{{n(n - 1)}}{{2{n^2}}} \ge \frac{1}{4}$ when $n\ge2$, we get ${(1 + \frac{x}{n})^n} \ge 1 + x + \frac{{{x^2}}}{4} \Rightarrow \frac{1}{{{{(1 + \frac{x}{n})}^n}}} \le \frac{1}{{1 + x + \frac{1}{4}{x^2}}}$ when $n\ge2$. $\frac{1}{{1 + x + \frac{1}{4}{x^2}}}$ is Lebesgue integrable on $[1,\infty)$, then by dominated convergence theorem $\mathop {\lim }\limits_{n \to \infty } \int_1^\infty  {\frac{{dx}}{{{{(1 + \frac{x}{n})}^n}{x^{\frac{1}{n}}}}}}  = \int_1^\infty  {\mathop {\lim }\limits_{n \to \infty } \frac{{dx}}{{{{(1 + \frac{x}{n})}^n}{x^{\frac{1}{n}}}}}}  = \int_1^\infty  { - {e^{ - x}}dx} $ But I cannot prove the limit and integration are exchangeable on $[0,1]$. Hope someone can help. Thank you.","['lebesgue-measure', 'real-analysis', 'measure-theory']"
1359887,Geodesic vector field is well-defined,"Let $(M,g)$ be a Riemannian manifold. I just learnt that for a curve $x:I\to M$ to be a geodesic, the geodesic equation 
$$\ddot{x}^k+\dot{x}^i\dot{x}^j\Gamma^k_{ij}=0$$
is equivalent to the condition that the curve $(x,\dot{x})$ in $TM$ is an integral curve of the vector field
$$G=v^k\frac{\partial}{\partial x^k}-v^iv^j\Gamma^k_{ij}\frac{\partial}{\partial v^k}.$$
This is straightforward to verify, but I am unable to show that $G$ is a well-defined global vector field. Question: How do you show that if $(\tilde{x},\tilde{v})$ are other coordinates for the tangent bundle $TM$, then
  $$\tilde{v}^k\frac{\partial}{\partial \tilde{x}^k}-\tilde{v}^i\tilde{v}^j\Gamma^k_{ij}\frac{\partial}{\partial \tilde{v}^k}=v^k\frac{\partial}{\partial x^k}-v^iv^j\Gamma^k_{ij}\frac{\partial}{\partial v^k}?\tag{1}$$
  i.e., in the language of physicists, I want to show that ""$G$ transforms like a vector"". (I want to prove this without assuming existence and uniqueness of geodesics.) Attempt: Denote by $\tilde{G}$ and $G$ the left and right hand-side of $(1)$, respectively. Using the transformation
$$\tilde{\Gamma}^k_{ij}=\frac{\partial x^p}{\partial\tilde{x}^i}\frac{\partial x^q}{\partial\tilde{x}^j}\Gamma^m_{pq}\frac{\partial\tilde{x}^k}{\partial x^m}+\frac{\partial\tilde{x}^k}{\partial x^m}\frac{\partial^2x^m}{\partial\tilde{x}^i\partial\tilde{x}^j}$$
I get
$$\tilde{G}=G-v^rv^s\frac{\partial\tilde x^i}{\partial x^r}\frac{\partial\tilde x^j}{\partial x^s}\frac{\partial^2x^m}{\partial\tilde{x}^i\partial\tilde{x}^j}\frac{\partial}{\partial v^m}$$ But then, why does
  $$v^rv^s\frac{\partial\tilde x^i}{\partial x^r}\frac{\partial\tilde x^j}{\partial x^s}\frac{\partial^2x^m}{\partial\tilde{x}^i\partial\tilde{x}^j}=0?$$","['differential-geometry', 'smooth-manifolds', 'riemannian-geometry', 'geodesic']"
1359905,Does same cardinality imply a bijection?,"This came up today when people showed that there is no linear transformation $\mathbb{R}^4\to \mathbb{R}^3$ . However, we know that these sets have the same cardinality. I was under the impression that if two sets have the same cardinality then there exists a bijection between them. Is this true? Or is it just that any two sets which have a bijection between them have the same cardinality. Edit: the question I linked to is asking specifically about a linear transformation. My question still holds for arbitrary maps.","['real-numbers', 'elementary-set-theory']"
1359923,"Numerical methods (for ODE/PDE) that could take approximate solutions/good initial guesses, and further refine it to an certain accuracy","I am currently playing with an old analog computer, which could solve time-dependent ODE/PDEs pretty fast, without time-stepping; thus there is no convergence issues caused by time-stepping because of its computing nature. But the problem with analog computer's solutions is that they are not accurate due to physical limitations. I am very curious that: is there any numerical methods/solvers which can take analog computer's approximate solution (over the time domain) to further process it, and generate a more accurate solution?? Let me give an example of solving second order ODE describing the motion a mass-spring damper. The equation is the following:
$$
x'' = -0.2\cdot x' - 0.4\cdot x;\quad x(0)=1, x'(0) =0;\quad t_{stop} = 60s.
$$
To solve the above equation on an analog computer, we need to map the above equation to an electrical system. Usually an analog computer could perform several basic arithmetic operation in the continuous-time domain, e.g. addition, subtraction, multiplication, integration etc. The output of an integrator represent an state-variable of the ODE; the input of that integrator represent the corresponding first-order time derivative. By configuring the basic computing blocks in feedback loops, we could map the equation as the following: (I use Simulink) After you load the initial conditions onto the integrators, you can let the analog computer run and solve. If you measure the electrical signal at the output of integrator1, you will get the solution of $x(t)$ over the time domain: But, due to the physical limitations (e.g. electrical noise, offsets), the solution of $x(t)$ is not accurate. What I am looking for is a numerical method that can take the above solution of $x(t)$ by analog computer, e.g. the solutions $x(t=1s), x(t=2s), x(t=3s), x(t=4s)... x(t=60s)$, start from these approximate solution points and further refine these solution $x(t=1s), ... x(t=60s)$ to a much higher accuracy. (This second order ODE is just a simple case for illustration purpose; it happens to have analytic expression of solutions. The more general case would be nonlinear ODEs with no analytic solution.) Thanks in advance!! Any thoughts and suggestions are greatly welcome and appreciated!!","['approximation', 'convergence-divergence', 'numerical-methods', 'ordinary-differential-equations', 'partial-differential-equations']"
1359969,"If $A_1 \subset A_2 \subset \mathbb R$ and $m^*(A_1) = m^*(A_2)$, will $m^*(A_1 \cap T) = m^*(A_2 \cap T), \forall T \subset \mathbb R$?","Definition of Lebesgue Outer Measure : Given a set $E$ of $\mathbb R$ , we define the Lebesgue Outer Measure of $E$ by, $$m^*(E) = \inf \left\{\sum_{n=1}^{+\infty} l(I_n): E \subset \bigcup_{n=1}^{+\infty}I_n \right\}$$ where $l(I_n)$ denotes the length of interval (bounded and nonempty interval). Definition of measurable set : A set $E$ measurable if $$m^*(T) = m^*(T \cap E) + m^*(T \cap E^c)$$ for every subset of $T$ of $\mathbb R$ . If $A_1 \subset A_2 \subset \mathbb R$ and $m^*(A_1) = m^*(A_2)$ , will $$m^*(A_1 \cap T) = m^*(A_2 \cap T)$$ for all $T \subset \mathbb R$ ? Why? How about the result if $A_1$ being a measurable set added? Update: Sincerely, I appreciate user140776's answer and he gives the counterexample that equality doesn't hold for $m^*(A_1) = m^*(A_2) = +\infty$ . I still have a question: if $m^*(A_1) = m^*(A_2) < +\infty$ , does the equality hold? Or it doesn't hold until adding the condition that $A_1$ is measurable? Besides, if I remove the restriction of $A_1$ being a subset of $A_2$ that is $A_1, A_2 \subset \mathbb R$ and $m^*(A_1) = m^*(A_2) < +\infty$ , will that equality $$m^*(A_1 \cap T) = m^*(A_2 \cap T)$$ for all $T \subset \mathbb R$ still hold?","['lebesgue-measure', 'real-analysis', 'measure-theory']"
1359977,"Extension of group with Ext$^{1} (A, B) = 0.$","Are there any infinite torsion free abelian groups $A$ and $B,$ with $A$ is not projective and $B$ is not divisible but  $$\text{Ext} ^{1}(A, B) = 0.$$ Thanks","['group-theory', 'homological-algebra']"
1359983,Every group of order $150$ has a normal subgroup of order $25$,"Let $G$ be a group of order $150$.  I must show that it has a normal subgroup of order $25$.  The hint says to show that is has a normal subgroup of order $5$ or $25$. Now from Sylow, I know that the number $n_5$ of Sylow-$5$ subgroups (which each have $25$ elements) must be either $1$ or $6$, since $n_5$ must also divide $6$.  Now clearly if $n_5=1$ we are done, so I can assume that $n_5=6$.  My problem is I haven't figured out how to use this information (I am going for a contradiction, just based on what the problem asks me to prove).  I know by Cauchy's theorem I can get a subgroup of order $5$, but I don't see immediately if it must be normal. Any direction I should try to be moving?","['abstract-algebra', 'sylow-theory', 'group-theory', 'finite-groups']"
1359993,Example of a bilinear map whose image is not a subspace,"I am looking for an example of a bilinear map $\tau:V \times V \to W$ whose image $im(\tau)=(\tau(u,v):u,v \in V)$ is not a subspace of $W$. I considered the tensor map $\tau:U \times V \to U \otimes V$, since its images consists of all decomposable tensors. I have the idea that if $u \otimes v, u'\otimes v'$ are decomposable tensors, then $u \otimes v + u'\otimes v'$ is not necessarily a decomposable vector, but I am not sure why.","['tensor-products', 'linear-algebra']"
1359997,Conditional expectation involving some complications around exponential random variables,"Here is my problem. Consider four independent exponential distributions $X^A_1$, $X^B_1$, $X^A_2$, $X^B_2$ where $X^A_1$ and $X^B_1$ are $\exp(\lambda_1)$ and $X^A_2$ and $X^B_2$ are $\exp(\lambda_2)$. There is another random variable $\mu$ where $\mu=\mu^G$ when $X_1=X^A_1+X^B_1 < X_2=X^A_2+X^B_2$ and $\mu=\mu^B$ otherwise. In this setup, I'd like to calculate $E[e^{-rX_1}\mu]$. The approach I've taken is use $E[e^{-rX}\mu]=E[e^{-rX}\mu^G|X_1<X_2]P(X_1<X_2)+E[e^{-rX}\mu^B|X_1>X_2]P(X_1>X_2)$, and since $X_1$ and $X_2$ follow gamma distribution with (2,$\lambda_1$) and (2,$\lambda_2$), respectively, I calculated the density function $f_Y(y)$ where $Y=X_1-X_2$. And it is easy to show $$f_{{X_1},Y}(x_1,y)=f_{X_1}(x_1)f_{X_2}(x_1-y),$$ and from this point, I obtained the conditional density $f_{X_1}(x_1|Y=y)$ and tried to calculated the conditional expectation. But, I ended up with having a complicated form in the integrand when calculating the conditional expectation $E[e^{-rX}\mu^G|X_1<X_2]$, and probably I could proceed further, but I'd like to ask you if there is an easier way to get $E[e^{-rX}\mu]$ without calculating all the density functions. Thank you very much!","['conditional-expectation', 'probability-distributions', 'exponential-distribution', 'independence', 'probability']"
1360019,"Prob. 4, Sec. 28, in Munkres' TOPOLOGY, 2nd ed: For $T_1$-spaces countable compactness is equivalent to limit-point-compactness.","Definition (Countable Compactness): A topological space $X$ is said to be countably compact if every countable open covering of $X$ has a finite subcollection that also covers $X$ . Definition (Limit-Point Compactness): A topological space $X$ is said to be limit-point-compact if every infinite subset of $X$ has a limit point in $X$ . Then how to prove the following result? Let $X$ be a $T_1$ -space. Then $X$ is countably compact if and only if $X$ is limit point compact. My effort: Suppose $X$ is countably compact. If $X$ is not limit point compact, then let $A$ be an infinite subset of $X$ such that $A$ has no limit point in $X$ . Let $$ B \colon= \left\{ b_1, b_2, b_3, \ldots \right\} $$ be a countably infinite subset of $A$ . Since we have assumed that $A$ has no limit point in $X$ and since $B \subset A$ , therefore $B$ has no limit point in $X$ either. So, the set $B^\prime$ of all the limit points of $B$ in $X$ is empty and thus contained in $B$ ; hence $B$ is closed in $X$ . Since $B$ has no limit points in $X$ and since $B \subset X$ , none of the elements of the set $B$ itself is a limit point of $B$ ; so for each element $b_n \in B$ , there is an open set $U_n$ in $X$ such that $$ U_n \cap B = \left\{b_n \right\} \tag{1} .$$ Now the collection $$ \left\{ \ U_n \ \colon \ n \in \mathbb{N} \ \right\} \bigcup \{ X-B \} $$ forms a countable open covering of the countably compact space $X$ , so some finite subcollection of this covering also covers $X$ and hence that finite subcollection also covers $B$ . But the set $X-B$ contains no point of $B$ . So $B$ is covered by finitely many of the sets $$ \left\{ \ U_n \ \colon \ n \in \mathbb{N} \ \right\}, $$ each of which contains exactly one point of set $B$ [Refer to (1) above.]. This implies that set $B$ is a finite set, contrary to our choice of $B$ . Hence $X$ is limit point compact. Is this proof correct? How to prove the converse? PS: From the above proof (where we haven't required $X$ to be a T $_1$ -space), we can even state the following: Every (countably) compact topological space $X$ is also limit point compact. Am I right? P.S.: Based on the answers below, I state and prove the following result: Let $X$ be a $T_1$ topological space. If $X$ is limit point compact, then $X$ is also countably compact. Proof: Suppose that $X$ is a limit point compact, $T_1$ topological space that is not countably compact. Then there is a countable open covering $\left\{ \ U_n \ \colon \ n \in \mathbb{N} \ \right\}$ of $X$ that has no finite subcollection that also covers $X$ . Let us define the collection $\left\{ \ V_n \ \colon \ n \in \mathbb{N} \ \right\}$ of sets as follows: $$ V_n \colon= \begin{cases} U_1 \ & \mbox{ if } \ n = 1, \\ V_{n-1} \cup U_n \ & \mbox{ if } \ n = 2, 3, 4, \ldots. \end{cases} $$ That is, $$
\begin{align}
 V_1 & \colon= U_1, \\
 V_2 & \colon= U_1 \cup U_2, \\
 V_3 & \colon= U_1 \cup U_2 \cup U_3, \\
 V_4 & \colon= U_1 \cup U_2 \cup U_3 \cup U_4, \\
     & \cdots \\
\end{align}
$$ We note that $$ V_1 \subset V_2 \subset V_3 \subset \cdots. \tag{1} $$ For each $n \in \mathbb{N}$ , as $U_n \subset V_n \subset X $ and as $$ \bigcup_{n=1}^\infty U_n = X, \tag{2} $$ so we must also have $$ \bigcup_{n = 1}^\infty V_n = X. \tag{2*} $$ Thus $\left\{ \ V_n \ \colon \ n \in \mathbb{N} \ \right\}$ is also a countable open covering of $X$ . Now as $V_1 = U_1$ is a proper subset of $X$ [Refer to the first paragraph of this proof.], so there exists a point $x_1 \in X \setminus V_1$ . In fact, the set $X \setminus V_1$ is an infinite set, becuase if this set were finite, then  the open covering $\left\{ \ U_n \ \colon \ n \in \mathbb{N} \ \right\}$ of $X$ would have a finite subcollection also covering $X$ . Suppose that $n \in \mathbb{N}$ and $n > 1$ , and suppose that the $n-1$ distinct points $x_1, \ldots, x_{n-1} \in X$ have been chosen. Using the same reasoning as in the paragraph preceding the last one, the set $V_n = U_1 \cup \cdots \cup U_n$ is a proper subset of $X$ [Refer to the first paragraph of this proof again.] and as  the set $X \setminus V_n$ is an infinite set; so there exists a point $x_n \in X \setminus V_n$ such that $x_n \neq x_j$ for any $j = 1, \ldots, n-1$ . In this way we obtain the infinite subset $S$ of $X$ given by $$ S \colon= \left\{ \ x_1, x_2, x_3, \ldots \ \right\}. \tag{3} $$ Furthermore we note that, for any $n \in \mathbb{N}$ , as $x_n \not\in V_n$ , and as $V_n \supset V_j$ for each $j = 1, \ldots, n$ , by virtue of (1) above, so we can also conclude, for each $n \in \mathbb{N}$ , the following: $$ x_n \not\in V_j  \mbox{ for any } j = 1 \ldots n. \tag{4} $$ Since $X$ is limit point compact, the infinite set $S$ of $X$ as defined in (3) above in the paragraph prior to the preceding paragraph has a limit point $p$ in $X$ . Now using (2*) above, as $$ p \in X = \bigcup_{n = 1}^ \infty V_n, $$ so there exists a natural number $r$ such that $p \in V_r$ ; let $r$ be the smallest such natural number. Now as $p$ is a limit point in $X$ of set $S$ and as $V_r$ is an open set of $X$ containing $p$ , so $$ V_r \cap \big( S \setminus \{ p \} \big) \neq \emptyset, $$ that is, there exists an element $x_k$ of $S \setminus \{ p \}$ such that $x_k \in V_r$ also. Thus we have $x_k \in V_r$ . But by (4) above as $x_k \not\in V_k$ , so $$ V_r \not\subset V_k, $$ and hence in view of (1) above we can conclude that $$ r \not\leq k, $$ which implies that $$ r > k , $$ that is [Note that $x_k \in S$ . Refer to (3) above.], $$ k \in \{ 1, \ldots, r-1 \}. $$ Therefore the open set $V_r$ containing the limit point $p$ of set $S$ can intersect $S$ in only finitely nany points. But since $X$ is a $T_1$ space, $V_r$ must intersect $S$ in infinitely many points, by Theorem 17.9 in Munkres. Thus we have a contradiction. Thus if a $T_1$ topological space $X$ is limit point compact, then $X$ is also countably compact. Is this proof correct and clear enough in its presentation? Or, are there any problems in it of accuracy, clarity, or detail?","['general-topology', 'compactness']"
1360044,Looking for a direct proof of the following exercise,"A friend of mine told me about the following problem: Let $\{r_n\}$ be a sequence of rational numbers such that $\lim_{n\to\infty}r_n=x\in\Bbb R,$ $r_n\neq x,$ for every $n\in\Bbb N$ and $r_n=\dfrac{a_n}{b_n},$ for each $n\in\Bbb N,$ where $\{a_n\}$ is a sequence of integers and $\{b_n\}$ is a sequence of positive integers. Prove that $\lim_{n\to\infty}b_n=+\infty.$ I proved such result by contradiction: If $\lim_{n\to\infty}b_n\neq+\infty,$ then $\exists M\in\Bbb R$ such that $\forall N\in\Bbb N,$ there exists some $n\geq N$ such that $b_n<M.$ Therefore, we can construct a subsequence $\{b_{m_k}\}$ of $\{b_n\}$ as follows: First $\exists m_0\geq0$ such that $b_{m_0}<M.$ Having chosen $m_1,\ldots,m_p,$ let $m_{p+1}$ be such that $m_{p+1}>m_p$ and $b_{m_{p+1}}<M.$ Since $1\leq b_{m_k}<M,$ for every $k\in\Bbb N$ and $\{b_n\}$ is a sequence of positive integers, there must exist some constant subsequence $\{b_{n_k}\}$ of $\{b_n\}.$ Let $b$ be the positive integer such that $b_{n_k}=b$ for every $k\in\Bbb N$ and let $\delta>0$ be arbitrary. Since $\{r_n\}$ converges to $x,$ then $\{r_{n_k}\}$ converges to $x$ and hence, there is some natural $N_0$ such that $$0<\left|\dfrac{a_{n_k}}{b}-x\right|<\dfrac{\delta}{b},$$ for each $k\geq N_0.$ Then $$0<\left|a_{n_k}-bx\right|<\delta,$$ for every $k\geq N_0.$ Since $\delta$ is arbitrary, this means that $\lim_{k\to\infty}a_{n_k}=bx.$ Therefore $\exists N_1\in\Bbb N$ and $\exists a\in\Bbb Z$ such that $a_{n_k}=a,$ for every $k\geq N_1.$ Therefore, $r_{n_k}=\dfrac{a}{b},$ for sufficiently large $k,$ which contradicts the fact that $r_n\neq x,$ for every $n\in\Bbb N.$ My questions are , is there a direct proof of the exercise? or is there an elegant solution? and, is the idea of my proof correct?","['sequences-and-series', 'alternative-proof', 'proof-verification', 'real-analysis']"
1360060,Finding Critical Points and Local Maxima/Minima or Saddle Point,"I need help to find critical points of the function:
$$f(x,y)=\frac{-x^3}{3}+x-y^2$$
Then I have to classify these critical points as local maxima/minima or saddle points. I thought that to find the critical points, I have to find the 1st derivative and to find local max/min or saddle, I have to use the second derivative test. I am having a little trouble both in finding first and second derivatives and how to use it to find the given above. Can someone help me? Edit: I found the critical points to be $(1,0)$ and $(-1,0)$. Can someone verify this as well?","['calculus', 'multivariable-calculus']"
1360070,Maximum Modulus path,"Consider any entire, non constant function $f:\Bbb C\to \Bbb C$. Choose any $z\in\Bbb C$ and define $m(r)\in\overline D(z,r)$, for any $r\ge 0$, with this property:
$$|f(m(r))|\ge|f(w)|\;\forall w\in \overline D(z,r)$$ I'm aware that this definition may be ambiguous, since the maximum modulus needn't be met in a single point. I'm also aware that $|m(r)-z|=r$, by the maximum modulus principle. Questions : Is it always possible to choose $m(r)$ in such a way that $m$ is continuous, as a function from $[0,\infty)$ to $\Bbb C$? If/when it is the case, has this $m$ any known properties? Is there some theory about this? EDIT: I suspect that the answer to the first question is yes, since the modulus of an entire function can't have any local maxima. But I haven't anything rigorous.",['complex-analysis']
1360080,Log-Likelihood Ratio of signals with multivariate normal distribution.,"I am on a project and I had to study about signal detection, Bayes decision theory etc.
The basic paper I am reading, gives an equation for LLR, referring to these equations: $H_0 : Y\sim N(\mu_0,\Sigma_0)$ $H_1 : Y\sim N(\mu_1,\Sigma_1)$ The log-likelihood ratio is given then by: $$\log(L(y))=\frac{1}{2}\left[\log\frac{| \Sigma_0 |}{| \Sigma_1 |} + y^T(\Sigma_0^{-1} - \Sigma_1^{-1})y+2 (\mu_1^T\Sigma_1^{-1} -\mu_0^T \Sigma_0^{-1} )y+\mu_0^T \Sigma_0^{-1}\mu_0-\mu_1^T\Sigma_1^{-1}\mu_1\right]$$ But as I wanted to reproduce the final equation, I ended in this:
$$\log(L(y))=\frac{1}{2}\left[\log\frac{| \Sigma_0 |}{| \Sigma_1 |}+y^{T}(\Sigma _0^{-1}-\Sigma_1^{-1})y+(\mu_1^T\Sigma_1^{-1}-\mu_0^T \Sigma_0^{-1}) y + \mu_0^T \Sigma_0^{-1} \mu_0-\mu_1^T\Sigma_1^{-1}\mu_1+y^T(\Sigma_1^{-1}\mu_1-\Sigma_0^{-1} \mu_0)\right]$$ The above are equal if only: $$y^T(\Sigma_1^{-1}\mu_1-\Sigma_0^{-1}\mu_0) = (\mu_1^T\Sigma_1^{-1}-\mu_0^T\Sigma_0^{-1})y$$ which I find obviously wrong. Can someone help me with this?","['statistics', 'linear-algebra']"
1360087,limits of integration and derivative,"I have an integral that gives
$$\left[\frac{d}{dx}[f(x)]\right]_a^b$$ is it possible in general to claim that this is equal to 
$$\frac{d}{dx}\left(\left[f(x)\right]_a^b\right)\text{ ?}$$ If not in general, are there any sufficient conditions to claim this? Thanks!!","['derivatives', 'definite-integrals', 'integration']"
1360122,Minimize Total Cost of Box,"So there is a rectangular box that has a volume of $8 m^3$. The top and bottom of the box is made with some material that has a cost of $8$ dollars per square meter. The sides are made with another material that costs $1$ dollar per square meter. How can I find the dimensions of the box that would minimize the total cost? My thoughts: I first started off with the equation, $V=lwh$. I think that we need to find an equation for cost in which we substitute $h$ into and then solve for two variables, then use the first derivative. I can get the concept but can't get how to work on the mechanics.",['multivariable-calculus']
1360125,equivalent form of almost sure convergence,"Consider random variables $X_1, X_2, \dots$ and $X$ on $(\Omega, \mathcal F, \mathbb P)$. We say that $X_n$ converges to $X$ almost surely if $$\mathbb P\left(\lim_{n \to \infty} X_n =X\right)=1.$$ It is claimed that an equivalent condition is: for any $\epsilon >0$, $$\lim_{n \to\infty} \mathbb P \left( |X_m -X| < \epsilon, \mbox{for all } m \geq n \right)=1. (*)$$ I have never seen this condition before, is it generally true and widely used, please? Any reference to this formula? What does this formula mean exactly, please? In particular, the part ""for all $m \geq n$""? We already have $\lim_{n\to\infty}$, does this mean $m$ goes to $\infty$ faster than $n$, please? Is there another way to write this which is understandable? The proof of this equivalence is as follows, which I also have questions about. It says that
$$
\left\{ \omega: \lim_{n \to\infty} X_n(\omega) = X(\omega) \right\} = \cap_{\epsilon >0} \cup_{n=1}^\infty \left\{ \omega: |X_m(\omega)-X(\omega)| < \epsilon, \mbox{for all } m \geq n \right\}. (**)
$$ The $\cap_{\epsilon>0}$ part does not seem right since it produces an UN-countable intersection of measurable set, which might not be measurable any more. I suppose it should be something like $\cap_{\epsilon \in \{1/2, 1/3, \dots\}}$. Right? From $(**)$ it claims that we have
$$
\left\{ \omega: \lim_{n \to\infty} X_n(\omega) = X(\omega) \right\} = \lim_{\epsilon \to 0} \lim_{n \to\infty} \left\{ \omega: |X_m(\omega)-X(\omega)| < \epsilon, \mbox{for all } m \geq n \right\}. (***)
$$ I am not sure whether we can rewrite set operation in $(**)$ as limit operation in $(***).$ Is it correct? Then by continuity of probability measure, $(***)$ implies that
$$
\mathbb P(X_n \to X) =\lim_{\epsilon \to 0} \lim_{n \to\infty} \mathbb P\left\{ |X_m-X| < \epsilon, \mbox{for all } m \geq n \right\},
$$
which yields one direction of the equivalence. Likewide, $(**)$ implies for any $\epsilon >0$,
$$
\mathbb P(X_n \to X) \leq \lim_{n \to\infty} \mathbb P\left\{ |X_m-X| < \epsilon, \mbox{for all } m \geq n \right\},
$$
which yields the other direction.","['probability-theory', 'self-learning', 'measure-theory']"
1360146,Curve of genus $g$ with a point removed,"Let $C$ be a smooth projective curve of genus $g$. If we pick two distinct points $p,q\in C$, when are $C\setminus\{p\}$ and $C\setminus\{q\}$ isomorphic or not isomorphic? When $g = 0$, they are always isomorphic. But for $g\ge 1$, I wasn't sure how to approach this problem. Since curves of genus $g\ge 2$ have finite automorphism groups, I would expect $C\setminus\{p\}$ and $C\setminus\{q\}$ to be not isomorphic to each other. But I wasn't sure if this is a correct approach to the problem.","['abstract-algebra', 'algebraic-geometry']"
1360247,"""If A then B"" in Venn (or Euler) Diagrams","How can I represent ""If A then B"" in a diagram?
I thought it would be a simple subset like $A âŠ‚ B$. However this material says that If $A$ then $B$ $=$ $A^c âˆª B$. Now I am confused.",['discrete-mathematics']
1360281,Huisken's distance comparison principle and type II singularities.,"I've been reading Huisken's paper on his distance comparison principle and he remarked that in particular his theorem rules out the formation of type II singularities. These are singularities where in particular cusps form. Why is this true? For reference I will include his principle below Huisken's Distance Comparison Principle Let $F:\Gamma\times[0,T]\rightarrow\mathbb{R}^2$ be a smooth embedded solution of the curve shortening flow (1.1). Let $\Gamma\neq S^1$, such that $l$ is smoothly defined on $\Gamma\times \Gamma$. Suppose $d/l$ attains a local minimum at (p,q) in the interior of $\gamma\times\gamma$ at time $t_0\in[0,T]$. Then$$\frac{d}{dt}(d/l)(p,q,t_0)\geq0$$
with equality if and only if $\Gamma$ is a straight line. Note: $$d(p,q,t)=|F(p,t)-F(q,t)|$$
and $$l(p,q,t)=|\int_p^qds_t|$$ Reference: A Distance Comparison Principle for Evolving Curves by Huisken","['differential-topology', 'differential-geometry']"
1360293,Fourier transforms of $f(t)=\frac{\sin{at}}{t}$,"I want to derive the following pair of Fourier transforms: First: $$f(t)=\dfrac{\sin{at}}{t}$$ $$F(\lambda)=
\begin{cases}
    \sqrt{\dfrac{\pi}{2}}, & \text{if } |\lambda|<a \\
    0, & \text{if } |\lambda|>a
\end{cases}$$ Second: $$f(t)=(a^2+t^2)^{-1}$$ $$F(\lambda)=\sqrt{\dfrac{\pi}{2}}\cdot\dfrac{e^{-a|\lambda|}}{a}$$ What I have done: I have started using the definition of $F(\lambda)$, the fourier transform of $f(t)$, as:
$$F(\lambda)=\dfrac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}{e^{i\lambda \tau}f(\tau)d\tau}$$
So, for the first function:
$f(t)=\dfrac{\sin{at}}{t}$
$$\implies F(\lambda)=\dfrac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}{e^{i\lambda \tau}\dfrac{\sin{a\tau}}{\tau}d\tau}$$ But I don't know how to integrate this.","['fourier-analysis', 'complex-analysis']"
1360298,Does this proposition from complex analysis depend on AC?,"I was reading III vol. of Princeton lectures on analysis. Proposition 1.4:
""If $\Omega_{1}\supset\Omega_{2}\supset\ldots\supset\Omega_{n}\supset\ldots $ is a sequence of non-empty
compact sets in $\Bbb C$ with the property that: $$\operatorname{diam}(\Omega_{n})\to 0\text{ as } n\to\infty,$$ then there exists a unique point $w\in\Bbb C$ such that $w \in \Omega_{n}$ for all $n$."" And in the proof:""Choose point $z_{n}$ in each $\Omega_{n}$""
Apparently this proof relies on Axiom of Choice. But I'm interested if it can be proved without reference to AC?","['complex-analysis', 'axiom-of-choice']"
1360311,Why is the average of a sum equal the sum of the averages?,"I came across this website showing the proof of the above question in the Expected Value section. However, I do not quite understand why the probability of $xy$ becomes the probability of $x$ (or $y$) and why then the two summations reduce to only one in each term? Is it possible that $P_{xy}(x,y) = P_x(x) = P_y(y)$?","['summation', 'statistics', 'average']"
1360330,"Zeta function, $\mathbb{F}_5[T, \sqrt{T(T-1)(T+1)}]$","Let $A = \mathbb{F}_5[T, \sqrt{T(T-1)(T+1)}]$. My question is, what is the easiest way to see that$$\zeta_A(s) = {{1 + 2 \cdot 5^{-s} + 5^{1 - 2s}}\over{1 - 5^{1 - s}}}?$$Much thanks in advance. Perhaps Gauss and Jacobi sums would be helpful? Elliptic curves over finite fields? EDIT: We are defining $\zeta_A(s)$ as$$\zeta_A(s) = \prod_{m \in \text{max}(A)} {1\over{1 - \#(A/m)^{-s}}},$$for a finitely generated commutative ring $A$ over $\mathbb{Z}$. See here and here .","['algebraic-geometry', 'number-theory', 'algebraic-number-theory']"
1360332,Computing the shape operator,"I am trying to compute the shape operator and Gaussian curvature for some smooth zero sets of polynomials $f$ in $\mathbb{R}^n$, oriented by $N = \nabla f / || \nabla f||$ The approach I thinking about is this (which I didn't learn from a text, so maybe there is a problem with it): Compute the normal at a point $p$, and pick some vectors $v, w \ldots $ that are a local frame for the tangent space. Compute $\langle \nabla_{v} N, w \rangle = \frac{1}{||\nabla f||}\Sigma_{i,j=1}^{n+1} \frac{\partial^2 f}{\partial x_i \partial x_j} v_i w_j$. (The justification for this formula: $\nabla_v \frac{ \nabla f}{|| \nabla f ||} = (\nabla_v (\nabla f)) (1/ ||\nabla f||) + NormalComponent$) Deduce from this the matrix for $L_p(v) = - \nabla_v N$. However, something seems to be wrong with this approach. For example, in my computation below for the sphere, I get a Gaussian curvature that is not constant. We pick some point where $y \not = 0$ and $x \not = 0$, then $((x,y,z),-y,x,0)$ and $((x,y,z),0,-z,y)$ describes a local frame. (The first triple is the point, the next three coordinates describe the vector in the tangent space of $\mathbb{R}^3$). Computing the matrix $L_p$ in this basis gives $1/r^2 \begin{pmatrix} x^2 + y^2 &  -xz \\ -xz & z^2 + y^2 \end{pmatrix}$, which has determinant $-y^2 / r^2$... I am really confused. I would appreciate someone pointing out my mistake.","['differential-geometry', 'multivariable-calculus']"
1360345,prior probability vs a priori probability,"What is the difference between ""Prior probability"" and ""a priori probability"" Wikipedia have two distinct pages for them. As of my inference i thought ""Prior"" and ""a priori"" are same, i.e., P(y) in Bayes' theorem P(y,x) = P(x,y)P(y) / P(x)","['probability-theory', 'probability', 'terminology']"
1360353,Lebesgue's monotone convergence theorem for upper integrals,"Let $(X, \mathcal A, \mu)$ be a measure space.
Let $f:X \rightarrow [0, \infty]$ be a non-negative extended real-valued function.
It is sometimes useful to consider the so-called upper integral $\int^* f d\mu$(for example we would like to define the outer measure of a subset $A$ of $X$ which should be the upper integral of the characteristic function of $A$).
We define the upper integral $\int^* f d\mu$ as follows. $\int^* f d\mu = \text{inf }\{\int g d\mu: f \le g, g:\text{ measurable}\}$. It seems to me that the following proposition is correct. Let $f_1\le f_2 \le \cdots$ be a non-decreasing sequence of functions $X\rightarrow [0, \infty]$.
  Let $f(x) = \text{lim}_{n\rightarrow\infty} f_n(x)$ for all $x\in X$.
  Then $\int^* f d\mu = \text{lim}_{n\rightarrow \infty} \int^* f_n d\mu$. How do you prove this if it is correct? Actually I think I have a proof of it, but I am not 100% sure that it is correct. I think it would be nice that someone confirms that I am on the right track.",['measure-theory']
1360381,Radius of convergence in power series $\sum_{n=0}^{\infty}(-1)^nx^{2^n}$,"Given the series $$\sum_{n=0}^{\infty}(-1)^nx^{2^n}$$
  determine the radius of convergence, and what can we say when $x=R$ and $-R$? Is it a power series? Power series should have the form of $$\sum_{n=0}^{\infty}a_nx^n$$
but the given series does not match this form.  If not a power series, why can we say about its radius of convergence? By the ratio test, I get that this series converges when $|x|<1$, diverges when $|x|>1$, so $R=1$, is that right? When $x=1$ or $-1$, series both becomes $$\sum_{n=0}^{\infty}(-1)^n,$$
then obviously, series diverges. Right?","['sequences-and-series', 'convergence-divergence', 'real-analysis']"
1360411,How to prove $A=(A\setminus B)\cup (A\cap B)$ [duplicate],"This question already has answers here : Prove $A = (A \setminus B) \cup (A \cap B)$ (3 answers) Closed 8 years ago . How to prove $A=(A\setminus B)\cup (A\cap B)$. I have seen this problem and the solution is clear to me. Initially I was satisfied by my prove but now I think it is wrong. How I have proved $$(A\cup B)=(A\setminus B)\cup(A\cap B)\cup(B\setminus A)\\
  (A\cup B)\cap A=[(A\setminus B)\cup(A\cap B)\cup(B\setminus A)]\cap A=[(A\setminus B)\cap A]\cup[(A\cap B)\cap A]\cup[(B\setminus A)\cap A]\\
  A=(A\setminus B)\cup(A\cap B)\cup\phi=(A\setminus B)\cup(A\cap B)$$ Have I proved it correctly. If yes then how? I mean that I have used $A=(A\setminus B)\cup (A\cap B)$ and $B=(B\setminus A)\cup (A\cap B)$ to get the $(A\cup B)=(A\setminus B)\cup(A\cap B)\cup(B\setminus A)$ and then I'm using it to prove $A=(A\setminus B)\cup (A\cap B)$. This means that I'm using a statement to prove itself! Kindly help me.","['elementary-set-theory', 'proof-writing']"
1360428,$|\mathbb N^{\mathbb N}| = |2^\mathbb N|$ by finding a bijection,"I was trying to find a direct proof that $|\mathbb N^{\mathbb N}| = |2^\mathbb N|$, by finding a bijection between the two sets. The idea that came to mind was to start with the sequence of natural numbers. Each natural number would be 'translated' in a sequence of zeroes of the length given by the natural number. A  1 would be added to the sequence obtained, and all the 01 sequences thus obtained would be concatenated in the order dictated by the original sequence of naturals. However, this would run into problems, since all complete 01 sequences thus obtained would end on 1, so that the function from $\mathbb N^{\mathbb N}$ to $2^\mathbb N$ would not be surjective. I turned to Stack Exchange for advice and found out that the post Cardinality of the set of all natural sequences is $2^{\aleph_0}$ contained the same solution, except that ones were switched for zeroes and vice versa. Of course, this could not work either: all 01 sequences obtained end on 0. As a workaround to the problem, I would propose the function as described above, except that the last natural number in the original series would now be translated in its binary form, and no 0 or 1 added. I think that would result in a bijection. Could that be right? Would there be other possibilities? Edit: I am now convinced that the approach I proposed cannot work either. The function is not injective. At first I thought that the last part of the 01 sequence corresponding to the last natural number of the original sequence would be identifiable by its starting with 11 (one 1 stemming from the previous number, the other from the binary form of the last natural in the sequence - except when that would be 0). I now realize that 11 may occur within the binary form itself.
Therefore, still at a loss...",['elementary-set-theory']
1360446,"Are the unit partial quotients of $\pi, \log(2), \zeta(3) $ and other constants $all$ governed by $H=0.415\dots$?","Khinchin showed that given the simple continued fraction of a real number, $$r = a_0+\cfrac{1}{a_1+\cfrac{1}{a_2+\cfrac{1} {\ddots}}}$$ then it is almost always true that the partial quotients $a_i$ satisfy, $$K = \lim_{n \rightarrow \infty } \left( a_1 a_2 ... a_n \right) ^{1/n} =2.685452\dots$$ where $K$ is Khinchin's constant . (Some exceptional $r$ are the rationals, roots of quadratic equations, and rational powers of $e$.) Q1: Given $n$, let $T_n$ be the total number of partial quotients $a_i = 1$. Is it almost always true that,
  $$H=\lim_{n \rightarrow \infty } \frac{T_n}{n} = 0.415\dots$$
  exists and converges? If it does, can $H$ be expressed in terms of $K$ and other constants? Numerical evidence for various transcendental and algebraic constants are given below with $n=10^k$ and entries as $T_n$: $$\begin{array}{|c|c|c|c|c|}
\hline
\text{constant}&10^3&10^4&10^5&10^6&10^7\\
\hline
\pi&412& 4206& 41494& 414526& 4148280\\
\Gamma\big(\tfrac{1}{2}\big)&417& 4178& 41620& 415352& 4151849\\
\log(2)&433& 4148& 41430& 415443&-\\
\log(3)&429& 4170& 41458& 414919&-\\
T&396& 4084& 41172& 414458&-\\
P&410& 4087& 41364& 415180&-\\
K&418& 4111& 41379&-&-\\
C&412& 4147& 41543&-&-\\
\zeta(3)&418& 4223&-&-&-\\
\hline
\end{array}$$ where $T, P, K, C$ are the tribonacci , plastic , Khinchin , and Catalan constants. Q2 : Anybody able to fill in the blanks? Or extend it to $n>10^{7}$ so we can have more decimal digits of $H$? (I know the continued fraction for $\pi$ has been computed to more than $n>10^{10}$ terms.)","['computational-mathematics', 'real-analysis', 'analytic-number-theory', 'constants', 'continued-fractions']"
1360475,Bivariate infinite series: explicit sum?,"Let $S_n(x,y)=\sum_{k=0}^n\binom{n}{k}\frac{x^k}{k!}y^{n-k}$, and consider the series $S(x,y)=\sum_{n=0}^\infty S_n(x,y)$, where $x,y\in \mathbb{R}$.  My question is: does this series have an explicit sum (i.e. closed-form expression)? The main reason why such a sum may exist is that if the binomial coefficient $\binom{n}{k}$ were set equal to unity in $S_n$, then the series becomes $S(x,y)=\frac{\exp x}{1-y}$ by the Cauchy product.  It is possible that we have a ""simple"" modification of this function that produces the binomial coefficients in $S_n$.  This seems plausible, since it can be shown that $S$ is absolutely convergent for all $x$ and for $y\in(-1,1)$, which mimics the properties of $\frac{\exp x}{1-y}$.","['sequences-and-series', 'multivariable-calculus', 'real-analysis']"
1360476,A polynomial sequence,"I have a sequence of polynomials $Q_k(x, y)$, $k\geq 1$ defined recursively as follows: $Q_1=x$. There is a sequence of polynomials $p_j(y)$ of degree $j$ such that $Q_{2m}$ is of the form
\begin{eqnarray}\frac{p_{0}(y)}{(2m)!}x^{2m}+\frac{p_{1}(y)}{(2m-2)!}x^{2m-2}+\cdots+\frac{p_{m-1}(y)}{2!}x^2+p_{m}(y)\end{eqnarray}
and $Q_{2m+1}$
\begin{eqnarray}
\frac{p_{0}(y)}{(2m+1)!}x^{2m+1}+\frac{p_{1}(y)}{(2m-1)!}x^{2m-1}+\cdots+\frac{p_{m-1}(y)}{6}x^3+p_{m}(y)x
\end{eqnarray} $Q_k(k+1-2i, k+1)$ as a polynomial in $i$ has roots $1, 2, \cdots, k$. Find a general formula of $Q_k(x, y)$. The following are the first 4 polynomials in the sequence:
\begin{align*}
Q_1&=x\\
Q_2&=\frac{x^2}{2}-\frac{y}{6}\\
Q_3&=\frac{x^3-xy}{6}\\
Q_4&=\frac{x^4-2x^2y}{24}+\frac{y(5y+2)}{360}
\end{align*} In fact, it suffices to find the sequence of polynomials $p_j(y)$. Here's what I've got so far. By condition (2) and (3), we have
\begin{eqnarray}
\sum_{j=0}^m \frac{p_j(2m+1)}{(2m-2j)!}(2i-2m-1)^{2m-2j}=\frac{2^{2m}}{(2m)!}(i-1)(i-2)\cdots(i-2m)
\end{eqnarray}
If we let $m=j+k$, differentiate both sides with respect to $i$ $2k$ times and put $\displaystyle i=\frac{2j+2k+1}{2}$, we have
\begin{eqnarray}
p_j(2j+2k+1)=\left.\frac{d^{2k}}{di^{2k}}\right|_{i=\frac{2j+2k+1}{2}}\frac{2^{2j}}{(2(j+k))!}(i-1)(i-2)\cdots(i-2(j+k))
\end{eqnarray}
Letting $k=0, 1, \cdots, j$, we get the $j+1$ values taken by $p_j$ at $2j+1, 2j+3, \cdots, 4j+1$. $p_j$ then can be computed using Lagrangian interpolation. It seems to me that, though the above algorithm can be implemented on a computer to get a few polynomials in the sequence, it does not yield directly a general formula I want. Is there another better way to go about getting a general formula? Edit: The first 6 members in the polynomial sequence $p_j(y)$ are the following:
\begin{align*}
p_0(y)&=1\\
p_1(y)&=-\frac{y}{6}\\
p_2(y)&=\frac{y(5y+2)}{360}\\
p_3(y)&=-\frac{y(35y^2+42y+16)}{45360}\\
p_4(y)&=\frac{y(5y+4)(35y^2+56y+36)}{5443200}\\
p_5(y)&=-\frac{y(385y^4+1540y^3+2684y^2+2288y+768)}{359251200}
\end{align*} It is noteworthy that the denominators happen to be the first 6 numbers in this sequence , as pointed out by Solomonoff's Secret.","['recurrence-relations', 'polynomials', 'combinatorics']"
1360494,Locally constant property,"Suppose f is positive and Schwartz function. Fix $N>0$ and $A>0$. Suppose that for any $x \in [-N,N]$,
  $$A \leq \int_{-N}^{N}f(x-z)dz$$
  Then do the inequality
  $$A \leq C_{r} \frac{1}{N^{1/r}}\int_{-N}^{N}(\int_{-N}^{N}|f(x-z)|^rdx)^{1/r}dz$$
  hold for some $C_{r}$ and for any $r>0$? I solved the case $r \geq 1$. By integration on $[-N,N]$,
$$2NA \leq \int_{-N}^{N}\int_{-N}^{N}f(x-z)dxdz$$
Now, just apply Holder inequality. Then we get the desired result. If $f$ is a monotone function, then the case $0<r<1$ also true but I don't know how to prove general case. Thanks in advance. EDIT : (Scaling argument) Put $g(z)=\frac{1}{N}f(\frac{z}{N})$. Then we may assume that $N=1$.","['analysis', 'harmonic-analysis', 'real-analysis', 'measure-theory']"
1360509,What exacty is the role played by Jacobian or Wronskian?,"In many of our derivations or in differential equations we come across the terms Jacobian or Wronskian. For example, to check the linear independence of solutions of differential equations, we ensure that the Wronskian is non zero. What role do these play? What do they actually account for?","['determinant', 'ordinary-differential-equations', 'matrices']"
1360512,"If $a,b \in$ group $G$ such that $a^2=e, a*b^4*a=b^7$, prove that $b^{33}=e$","$e$ is the identity of the group. My understanding: To prove that $b^{33}=e$ is the same as proving $b^{34}=b$ Now, $a * b^4 * a=b^7$ $\Rightarrow b^4= a*b^7*a=(a*b*a)^7$ This is how far I went. I'm stuck here. Please help.",['group-theory']
1360586,Show that $V(y^5-x^2)\subset \mathbb{R}^2$ is not isomorphic to $\mathbb{R}$ as a variety.,"This is an exercise from Ideals, Varieties and Algorithms by Cox et al. Show that $V(y^5-x^2)\subset \mathbb{R}^2$ is not isomorphic to $\mathbb{R}$ as a variety by showing that there is no ring isomorphism from $\mathbb{R}[V]$ to $\mathbb{R}[t]$. There is another proof in the textbook using pullback. This problem asks to show it without using that. Hint : Every element of $\mathbb{R}[V]$ can be written as $a(y)+b(y)x$. Suppose there were some ring isomorphism $\alpha: \mathbb{R}[t]\rightarrow \mathbb{R}[V]$, such that $\alpha(f(t))=x, \alpha(g(t))=y$. Using the unique factorization of $f,g$, deduce a contradiction. My attempt : I didn't do anything very helpful. Suppose $f,g$ has the following unique factorization in $\mathbb{R}[t]$: $$f=f_1^{a_1}\cdots f_r^{a_r}\\
g=g_1^{b_1}\cdots g_s^{b_s}\\
x=\alpha(f)=\alpha(f_1)^{a_1}\cdots \alpha(f_r)^{a_r}\\
y=\alpha(g)=\alpha(g_1)^{b_1}\cdots \alpha(g_s)^{b_s}$$ I cannot see how to connect this with the representation in $\mathbb{R}[V]$. Any help will be appreciated!","['ring-theory', 'algebraic-geometry']"
1360595,How many four digit numbers are perfect square whose first and last two digits are same?,I tried it by assuming the number as $\sqrt{1100a+11b}$ and than tried to find figure out perfect square but I am unable to approach further.,"['number-theory', 'elementary-number-theory']"
1360604,Under what conditions is the homology of a dg coalgebra a graded coalgebra?,"I'm trying to get a feel for some differential graded (dg) structures. Suppose $C$ is a differential graded coalgebra over a commutative ring $k$ , i.e. a graded $k$ -module equipped with a coproduct $\Delta : C \to C \otimes C$ and a counit $\varepsilon : C \to k$ satisfying the usual axioms. I'm interested in some sufficient conditions for the coalgebra structure on $C$ to induce coalgebra structure on the homology $H(C)$ (which is a graded $k$ -module). I guess if $k$ is a field (or a ring for which the relevant $\operatorname{Tor}$ 's in the KÃ¼nneth sequence vanish) then the map $H(C)\otimes H(C) \to H(C \otimes C)$ is an isomorphism, so the inverse can be used to define a coalgebra structure on $H(C)$ . What if $k$ is a more complicated ring? What about conditions ""about $C$ "" instead of conditions ""about $k$ ""? Is it correct that when dealing with a product-type structure (e.g. a dg algebra or a dg Lie algebra) then no use of the KÃ¼nneth formula is needed to induce the product-type structure on homology? Many thanks!","['abstract-algebra', 'algebraic-topology', 'coalgebras', 'homological-algebra']"
1360605,"solve $\dfrac{x^2-|x|-12}{x-3}\geq 2x,\ \ x\in\mathbb{R}$.","solve $\dfrac{x^2-|x|-12}{x-3}\geq 2x,\ \ x\in\mathbb{R}$. options $a.)\ -101<x<25\\
b.)\ [-\infty,3]\\
c.)\ x\leq 3\\
\color{green}{d.)\ x<3}\\
$ I tried , Case $1$ ,for $ \boxed{x\geq 0}\\
\dfrac{x^2-x-12}{x-3}\geq 2x\\
\implies \dfrac{x^2-5x+12}{x-3}\leq 0 \\
\implies x<3\\
x\in \emptyset $ Case $2$ ,for $\boxed{x< 0}\\
\dfrac{x^2+x-12}{x-3}\geq 2x\\
\implies \dfrac{(x-4)(x-3)}{x-3}\leq 0 \\
\implies x\leq 4\\
\implies x< 0\\
  $ But the answer given is option $d.)$ I look for a short and simple way. I have studied maths up to $12$th grade.","['quadratics', 'absolute-value', 'algebra-precalculus', 'inequality']"
1360609,Expected number of return for a random walk on a graph,"Let $G$ be a simple, connected undirected graph of order $n$ and vertex set $\{v_1,\ldots,v_n\}$ and let $P = (p_{i,j})$ be a $n \times n$ matrix where $$p_{i,j} = \left\{
	\begin{array}{ll}
		\frac{1}{d(v_i)}  & \mbox{if } v_i \sim v_j \\
		0 & \mbox{otherwise } 
	\end{array}
\right.$$ If we let $\pi = \frac{1}{2|E(G)|}(d(v_i), \ldots, d(v_n) )$ then it is not hard to see that $$\pi P = \pi.$$ Let $H_v$ be the expected number of steps for a random walk to reach back $v$ given that it started the random walk in $v.$ Reading through some textbooks it seems like at this point it should easy to infer that $H_v = \frac{2|E(G)}{d(v)}$ yet I do not see why. As the comments suggest one needs to apply the law of large numbers yet I do not see how to set up the framework for applying the statement of LLN. Hence I am asking How do we apply the law of large numbers in this context?","['probability-theory', 'graph-theory', 'probability', 'random-walk']"
1360610,Confused (disoriented?) by questions about orientation,"I believe I have a reasonable basic understanding of orientation. Yet, i'm finding myself utterly confused when facing a specific question. Here are several examples: Exhibit an ordered basis of positive orientation for $S^2$ (as a boundary of $B^3$) at an arbitrary point point $p=(a,b,c)$. Let $f: S^2 \to (-1,1)$, be gicen by $f:(x,y,z) \mapsto z$. Exhibit an ordered basis of positive orientation for a typical point
  on $f^{-1}(t)$. Show that the boundary orientation of $S^k = \partial B^{k+1}$ is the same as its preimage orientation given by: $$g:\mathbb{R}^{k+1} \to \mathbb{R}, g(x)= |x|^2$$ (The questions are taken from the book ""differential topology"" by Guillemin and Pollack). My issues: I don't quite understand how an answer is supposed to look like. Couldn't i just say $\{v_p,w_p\}$ is a basis for $T_p S^2$ so (if $n_p$ is the outward pointing normal) according to whether $sign(n_p,v_p,w_p)$ is positive or negative i can switch $v_p$ and $w_p$ and obtain a positive orientation? Same problem with (1). Only now there's another issue. I'm not sure what's the correct formal way of computing an orientation of a preimage like $f^{-1}(t)$. Would it be right to look at $(T_p f o \pi_p)^{-1}(1)$ where $\pi_p: T_p S^2 \to T_p (f^{-1}(t))$ is the projection? Same as (2), plus some general unfocused non-specific confusion. I must say that I didn't have any problems with the chapter itself, everything was done in a coordinate-free way that didn't raise these problems. Thanks for the help. EDIT: In the book an orinetation is defined as a continuous choice of equivalence class on the tangent space at each point where the relation is: $$v \sim w \iff Av=w \text{ for some $A$ with } |A| > 0$$ I do feel comfortable with the definition of an atlas with positive determinant transition functions as well.","['differential-topology', 'differential-geometry', 'manifolds', 'orientation']"
1360627,Compute the derivatives of $\frac{d^{2\ell}}{dx^{2\ell}}\tanh(x)^{2k}$ in $x=0$,"I would like to compute the derivatives $\frac{d^{2\ell}}{dx^{2\ell}}_{\vert x=0}\tanh(x)^{2k}$ at $x=0$ where $k,\ell\in \mathbb{N}$ positive integers with $\ell\geq k$. I am not sure how to attack this problem seriously. Can you give me an instruction what to do in this situation? Best wishes Edit:
In case of $k>\ell$ we have $\frac{d^{2\ell}}{dx^{2\ell}}_{\vert x=0}\tanh(x)^{2k}=0$. (see the commentary and answer below).","['analysis', 'real-analysis', 'derivatives']"
1360630,A positive integer is equal to the sum of digits of a multiple of itself.,"Let $n$ be a positive integer, prove there is a positive integer $k$ so that $n$ is equal to the sum of digits of $nk$. I'm not really sure how I should approach this problem, I tried to do a constructive approach but I got lost. I tried just proving existence but that didn't work either. I'm sorry if this doesn't look like a put work into it but I feel like nothing I have done is going to yield any results. So I hope you guys can solve this problem. Thank you very much in advance, regards.","['contest-math', 'number-theory', 'decimal-expansion']"
1360660,Every quasi-compact scheme has a closed point,"I know this question has been asked here before, but I have trouble understanding the following proof, taken from a Schwede's write-up . I have underlined the bit I don't understand. In particular, I have no idea how we can conclude that $P_3$ is not in $U_2$. Didn't we pick $P_2$ to be any point in the closure of $P_1$? So, can someone explain rigorously why the point $P_3$, which is any point in the closure of $P_2$ (other than $P_2$, right?) ought not to be in $U_2$? I know I am missing something obvious here, because this seems to be a common proof. Also, its probably topology that's giving me trouble here, so I will also tag it as such.","['algebraic-geometry', 'general-topology']"
1360680,Kovacic's algorithm,"Is there any reference with some example, about how to solve a ""riccati"" equation in this (below) form :$$y'(x)+a(x)y^2(x)+b(x)y(x)+c(x)=0$$ by Kovacic's algorithm? Or can anybody help me to understand how Kovacic's algorithm works? (In Kovacic's 1985 article , the algorithm is described only for the case of $y''=ry$ with some complicated examples. I am searching for some easier examples. I am new to this study.) Thanks in advance.","['closed-form', 'differential-geometry', 'ordinary-differential-equations']"
1360685,Specific resources to self-learn algebra,I'm attempting to self-learn algebra/trig from the ground up and looking for good resources that will help. Currently my goal is to learn enough to be able to take: MIT OCW Sing Var Calc MIT OCW Mathematics for CS Linear Algebra I'm currently going through some of the material on Khan Academy but not sure if their content is sufficient for the above goal/an overkill Essentially I am looking for a good route/content with links to resources that I can follow. Any help would be appreciated.,"['calculus', 'algebra-precalculus', 'trigonometry']"
1360715,"Let $a,b \in$ group $G$ such that $ab=ba, \gcd(O(a),O(b))=1$. Prove that $O(ab)=O(a)O(b)$.","My attempt: Let $O(a)=m, O(b)=n$, then $mx+ny=1$ Let $O(ab)=p$, then using commutative property, $(ab)^p=a^pb^p=e$, which is the identity. Then $a^p=e, b^p=e$, hence, $m | p$ and $n | p$. So, $p=mk_1=nk_2$. I'm stuck here. I understand I need to prove $p=mn$, please help.","['abstract-algebra', 'group-theory']"
1360747,How to show $ \Big\vert \frac{\sin(x)}{x} \Big\vert $ is bounded by $1$?,"This may be a silly question, but I cannot figure it out. I want to prove that $ \Big\vert \frac{\sin(x)}{x} \Big\vert \leq 1 $ for $x\in[-1,0)\cup(0,1]$, but I don't even know where to start.","['inequality', 'functional-inequalities', 'trigonometry']"
1360776,Show that no $T\in M_{5\times 5}(\mathbb{Q})$ has order $8$.,"Before I get too far, I'll say that I think the above statement is incorrect. Assume that $T\in M_{5\times 5}(\mathbb{Q})$, with $|T|=8$, and let $f(x)=x^8-1$. Since $f(T) = 0$, it follows that, if $m(x)$ is the minimal polynomial of $T$, then we must have $m(x)$ dividing $f(x)=x^8-1=(x-1)(x+1)(x^2+1)(x^4+1)$. Since $T\in M_{5\times 5}(\mathbb{Q})$, we must also have $\deg(m)\leq 5$, and so $m(x)=(x-1)^{e_1}(x+1)^{e_2}(x^2+1)^{e_3}(x^4+1)^{e_4}$, with each $e_i \in \{0,1\}$. If $e_4 = 0$, then $|T|\leq 4$; so $x^4 + 1$ must divide $m(x)$. As the largest of $T$'s invariant factors, $m(x)$ must be divisible by any smaller invariant factors; however, $x^4+1$ is irreducible in $\mathbb{Q}[x]$. And so, recalling our constraint on the degree of $m$, we must have $m(x) = (x-1)^{e_1}(x+1)^{e_2}(x^4+1)$, where $e_1+e_2 =1$. (By the way, $m(x)=p(x)$, the characteristic polynomial of $T$.) At this point, my first time through the problem, I still thought I was proving the statement and wanted to show a contradiction. I wasn't sure what to do, so, using each of the possible minimal polynomials, I put $T$ in rational canonical form and plugged it into my TI-82, raising it to the eighth power, and sure enough got the identity matrix. Moreover, $T^4 \neq I$. I've spent the summer feeling like a big stupid in algebra, so although my reasoning seems pretty tight to me, I can't help but think I'm missing something. Am I? (The question comes from a qual-prep seminar I'm sitting in on.) If the statement is false, does my logic check out? And what conditions would make it a true statement? It doesn't look like it works if $T$ must have order $6$; it seems as though it does hold for $T\in M_{3\times 3}(\mathbb{Q})$ (again, with $|T|=8$).","['solution-verification', 'linear-algebra']"
1360793,Continuous marginal distributions do not imply continuous joint distribution,"I already proved the other implication. I need to find an explicit example that shows that if there is some random vector $(X,Y)$ and $X$ and $Y$ have both continuous marginal distributions, then distribution of ($X,Y$) is not necessarily continuous type. Any help please?","['probability-theory', 'examples-counterexamples']"
1360799,Effect of adding a constant to the torsion of a 3D curve,"Let $\gamma$ be an arc-length parametrized curve in $\mathbb{R}^3$. Let say I add a constant to the torsion of $\gamma$ and let $\widetilde{\gamma}$ be the curve associated to the curvature of $\gamma$ and this new torsion. Is there an easy way to characterize $\widetilde{\gamma}$ in terms of $\gamma$? In other words, is there a map that links $\gamma$ to $\widetilde{\gamma}$?","['curves', 'differential-geometry', 'calculus', 'ordinary-differential-equations']"
1360805,Does there exist such an invertible matrix?,"Let $n \geq 1$ and $A = \mathbb{k}[x]$, where $\mathbb{k}$ is a field. Let $a_1, \dots, a_n \in A$ be such that $$Aa_1 + \dots + Aa_n = A.$$ Does there exist an invertible matrix $\|r_{ij}\| \in M_n\left(A\right)$ such that $r_{1j} = a_j$ for all $j = 1, \dots, n$?","['abstract-algebra', 'principal-ideal-domains', 'linear-algebra', 'commutative-algebra']"
1360868,Example of a surjective local homeomorphism that is not a covering? [duplicate],"This question already has answers here : Local homeomorphisms which are not covering map? (4 answers) Closed 8 years ago . Let $X$ and $Y$ be connected, locally path connected, and Hausdorff topological spaces. Can someone give me an example of a surjective local homeomorphism that is not a covering? I don't think this is the same question as here ? There's different conditions?","['covering-spaces', 'algebraic-topology', 'general-topology', 'examples-counterexamples']"
1360891,Find Quadratic Bezier curve equation based on its control points,"If the 3 control points of the quadratic BÃ©zier curve are known, how do you calculate algebraically the equation of that curve (which is an y=f(x) function)?
Let's say I have.. P0 (x,y) - startPoint P1 (x,y) - controlPoint P2 (x,y) - endPoint and I want to get implicit equation for that, something like that: f(x) = âˆ’0.5x^2â€‹â€‹ + 3.5x âˆ’ 1 (for example). I've found the fang's solution here , but he says Although all quadric Bezier curve is part of a certain parabola, not all parabola can be represented as $f(x)=ax^2+bx+c$ . So, the first thing you need to do is check if $x_2=\frac{x_1+x_3}{2}$ . If this check fails, then your quadratic Bezier curve is not a segment of $f(x)=ax^2+bx+c$ . Ok, but if check fails how can I find its equation? Another article about cubic Bezier is http://www.moshplant.com/direct-or/bezier/math.html Is there any approach for quadratic?","['algebraic-geometry', 'bezier-curve', 'quadratics']"
1360900,Existence of exhaustion by compact sets,"When it is known that a set $A$ in topological space $X$ can be exhausted by compact sets, that is there exists increasing sequence of compact sets covering $A$ ? Comments: I guess this should involve both conditions on topology of $X$ and the set $A$ itself. I remember my lecturer in analysis using such exhaustion for open set in Euclidean space but I don't even know proof for that case. I will appreciate both answers and references to sources where I can find it.","['reference-request', 'general-topology', 'compactness']"
1360901,Dirichlet energy and Fourier transform,"Is there a direct relationship between the Dirichlet energy of a function: $$E(f)=\int_{\Omega}\lvert\nabla f(\mathbf{x})\rvert^2\mathrm{d}V$$ and its Fourier transform $$\hat{f}(\mathbf{k})=\int_{\Omega}f(\mathbf{x})e^{-2\pi i\mathbf{k}\cdot\mathbf{x}}\mathrm{d}V$$ Since the Dirichlet energy measures the variability of a function in some region, and the Fourier transform measures the amplitude of its frequencies, I think some expression involving the Fourier transform at high frequencies should yield the Dirichlet energy. Is it possible to connect these two expressions? If so, how? I think the functional relationships of the Fourier transform listed here might be relevant.","['integral-transforms', 'fourier-analysis', 'functional-analysis', 'functional-calculus', 'calculus-of-variations']"
1360913,"Show that the tangent surface of the twisted cubic curve $V(y-x^2,z-x^3)$ is not isomorphic to $\mathbb{R}^2$.","This is an exercise in Ideals, Varieties and Algorithms by Cox et al. Show that the tangent surface V of the twisted cubic curve $V(y-x^2,z-x^3)$, given by $$x=t+u\\y=t^2+2tu\\z=t^3+3t^2u$$ is not isomorphic to $\mathbb{R}^2$. Following their hints, I found that $V$ is singular at all points on the twisted cubic curve. The singular points are where $f(x,y,z)=0$ and $\nabla f(x,y,z)=0$. From here I can see $V$ is not isomorphic to $\mathbb{R}^2$ since $V$ is not smooth whereas $\mathbb{R}^2$ is. To prove that, the hint says that consider a polynomial map $\alpha: \mathbb{R}^2\rightarrow V$ such that $\alpha(a,b)$ is on the twisted cubic curve. Then the derivative matrix of $\alpha$ must have rank strictly less than $2$ at $(a,b)$. My question : With this mapping, we should have the surface defined by 
$$f(\alpha(u,v))=0$$ So $\nabla f \cdot \alpha'$ is the gradient of the surface on $u,v$. Shouldn't this be zero? And $\nabla f$ is zero at $(a,b)$ since it is singular on the twisted cubic. How does that tell the rank of the derivative matrix? Thanks for any help!","['algebraic-geometry', 'differential-geometry']"
1360927,How to prove $\cos{\frac{\pi}{11}}$ is a root of,"I want to show that $x=\cos{\frac{\pi}{11}}$ is a solution of equation :
$$8x^2-4x+\frac{1}{x}-4=4\sqrt{\frac{1-x}{2}}$$ Thanks in advance.","['algebra-precalculus', 'trigonometry']"
1360936,Regarding a complex analysis problem.,"I'm trying to do this problem from Gamelin's book: Let $f_n(z)$ be a sequence of analytic functions on a domain (= open connected set) $D$ such that $f_n(D) \subset D,$ and suppose that $f_n$ converges to $f$ uniformly on each compact subset of $D$. Show that either $f(D)\subset D$, or $f(D)$ consists of a single point in $\partial D$. Now, I can see that $f(D) \subset D\cup \partial D$. If $f$ is not constant, then by open mapping theorem, $f(D)$ is open, so no point in the boundary can be in the image of $f$, hence in this case $f(D) \subset D$.
But if $f$ is constant, I can't figure out why the mage of $f$ must be a point in the boundary. Why can't the image be in $D$? Thanks in advance.",['complex-analysis']
1360945,Correct order of taking dot product and derivatives in spherical coordinates,"I tried to derive definition of divergence in spherical coordinates from gradient and got:
$${\vec \nabla \cdot \vec A=\bigg (\frac{\partial}{\partial r}\hat r+\frac{1}{r}\frac{\partial}{\partial \theta}\hat \theta+\frac{1}{r\sin\theta}\frac{\partial}{\partial\phi}\hat\phi\bigg)\cdot\bigg(A_{r}\hat r+A_{\theta}\hat \theta+A_{\phi}\hat\phi\bigg)}$$
$${=\frac{\partial A_{r}}{\partial r}+\frac{1}{r}\frac{\partial A_{\theta}}{\partial\theta}+\frac{1}{r\sin\theta}\frac{\partial A_{\phi}}{\partial\phi}}$$
But according to Wikipedia and other sources this be:
$${\vec \nabla \cdot \vec A =\frac{1}{r^2} \frac{\partial (r^2 A_{r})}{\partial r}+\frac{1}{r\sin\theta}\frac{\partial (A_{\theta}\sin\theta)}{\partial\theta}+\frac{1}{r\sin\theta}\frac{\partial A_{\phi}}{\partial\phi}}$$
Why should we first take the derivative and then the dot product?
Why do we take first dot product when we have cartesian coordinates?
I think I read that it is because vectors themselves are functions of the coordinates but I do not understand what are consequences of this.","['spherical-coordinates', 'calculus', 'multivariable-calculus']"
1360959,Prove or disprove the inequality $\sqrt{1+\sin b}-\sqrt{1+\sin a}\leq\frac{b-a}{2}$,"I'm asked to prove or to disprove the inequality $$\sqrt{1+\sin b}-\sqrt{1+\sin a}\leq\frac{b-a}{2}$$ for all $0<a<b<\pi/2$. I believe that it is true. Proof : Let $f(x):=\sqrt{1+\sin x}$. The function $f(x)$ is continuous on the interval $[a,b]\subseteq[0,\pi/2]$ and is differentiable on $(a,b)$. Thus, by the mean value theorem there exists a point $c\in (a,b)$ such that $$f'(c)=\frac{\cos c}{2\sqrt{1+\sin c}}=\frac{f(b)-f(a)}{b-a}$$ and because $0<c<\pi/2$, $$\frac{\cos c}{2\sqrt{1+\sin c}}<\frac{\cos 0}{2\sqrt{1+\sin 0}}=\frac{1}{2}$$ And thus: $$f'(c)=\frac{f(b)-f(a)}{b-a}<\frac{1}{2} \Rightarrow \sqrt{1+\sin b}-\sqrt{1+\sin a}\leq\frac{b-a}{2}$$ Is it correct?","['calculus', 'proof-verification']"
1360985,Find a function $f(x)$ in an integral,"(Related question here ). Is there a way to calculate the function $f(x)$ in this integral in terms of $x$ without using $a,b,c$: $$\int_{a}^{b} f(x)dx=c$$ Two examples $\rightarrow$ how do find these functions $f(x)$: 1) How do find that $f(x)$ can be $x^2$?:$$\int_{0}^{1}f(x)dx=\frac{1}{3}\Longleftrightarrow f(x)=x^2$$ 2) How do find that $f(x)$ can be $\frac{x^4(1-x)^4}{1+x^2}$?:$$\int_{0}^{1}f(x)dx=\frac{22}{7}-\pi\Longleftrightarrow f(x)=\frac{x^4(1-x)^4}{1+x^2}$$","['calculus', 'pi', 'real-analysis', 'integration', 'derivatives']"
1360986,"Let $a,b \in$ group $G$ such that $ab^3a^{-1}=b^2, b^{-1}a^2b=a^3$ Prove that $a=b=e$ (identity) [duplicate]","This question already has answers here : A Particular Two-Variable System in a Group (4 answers) Closed 8 years ago . I got $ab^3=b^2a$ and $a^2b=ba^3$ by getting rid of the inverses via composing on both sides. I tried writing $a=aaa^{-1}$, which didn't help. Any suggestions please?",['group-theory']
1360997,"Brownian Motion is almost surely unbounded, and a proof for the discrete Random Walk","If $B_t$ is a Brownian Motion, why we have 
  $$
 P(\liminf_{t\to\infty} B_t = -\infty) = 1
$$
  and $P(\limsup_{t\to\infty} B_t = +\infty) = 1$? I guess for the following ""discrete version"" of the ""Brownian motion"" (which isn't called Brownian Motion but Random Walk I guess) I have an argument. Let $B_n$ with $B_n \sim \mathcal N(0, n)$. Set
$$
 A_k := \{ B_n > k \mbox{ for infinitely many }n \}
            = \limsup_n ~ \{ B_n > k \}
$$
and $A := \bigcap_k A_k$, then $\omega \in A$ iff the path of $\omega$ is unbounded, i.e. $A = \{\limsup_n B_n = +\infty\}$. Each set $A_k$ is in the tail-$\sigma$-algebra as the limsup of sets, and so $A$ as a countable intersection is a tail-event, therefore by Kolmogoroff's 0-1-law we have $P(A) \in \{0,1\}$. As for each $n$ we have
$$ 
 P(B_n > \sqrt n) = 1 - \Phi\left(\frac{\sqrt n}{\sqrt n}\right) > 0.1
$$
where $\Phi$ denotes the cumulative distribution function of the normal distribution, and as
$$ 
 \bigcup_n \{ B_n > \sqrt n \} \subseteq A
$$
and by the above $0.1 < P( \bigcup_n \{ B_n > \sqrt n \} )$
we could not have $P(A) = 0$, therefore $P(A) = 1$. Similarly the case $P(\liminf_n B_n = -\infty) = 1$ might be establised. In my approach I used that the process is discrete, but I do not know how to show it in the general case for continuous time. So any help would be greatly appreciated! Also if my solution for the discrete case is way to complicated I would also be thankful for feedback and alternative approaches.","['probability-theory', 'probability', 'stochastic-processes']"
1361025,"Conformal map from $\{z \in \mathbb{C}: |z|>1\}\setminus (-\infty,-1)$ onto $\mathbb{C}\setminus(-\infty,0]$","Find a conformal map from the set $\{z \in \mathbb{C}: |z|>1\}\setminus (-\infty,-1)$ onto the set $\mathbb{C}\setminus(-\infty,0]$ . Here is my thought, but I'm not sure if it is correct, can anyone help me to verify my answer? 1: $\frac{1}{z}$ maps the set to $\mathbb{D}\setminus(-1,0]$ . 2: $z^{1/2}$ maps the disk with slit to the right half disk. 3: rotate the right half disk to lower half disk. 4: the map $z+1/z$ maps the lower half disk to upper half plane. 5: rotate the upper half plane to right half plane. 6: $z^2$ maps the right half plane to $\mathbb{C}\setminus(-\infty,0]$ . Also, if anyone can think of an alternate route, it would be nice to see! Thanks!","['solution-verification', 'alternative-proof', 'complex-analysis', 'conformal-geometry']"
1361028,Limit of a function with 2 variables,"I am given this function: $$f(x,y)=\begin{cases}\frac{xy^3}{x^2+y^4} & \text{ for } (x,y)\not=(0,0)\\ 0 & \text{ for } (x,y)=(0,0)\end{cases}$$ and I have to check if it is continuous in $(0,0)$. Therefore I want to calculate $\lim \limits_{(x,y) \rightarrow 0}{\frac{xy^3}{x^2+y^4}}$. I already tried substituting and polar coordinates but did not come to a solution yet. Can someone give a few possibilities to calculate a limit of a function with multiple variables. Which is the best one to try first? I found that polarcoordinates often works very well. There has already been a question concerning this function and it's partial derivates and if it's differentiable. You may find that one here: determine whether $f(x, y) = \frac{xy^3}{x^2 + y^4}$ is differentiable at $(0, 0)$. Thanks!","['continuity', 'multivariable-calculus', 'limits']"
1361057,Is there a general rule for how to pick the base case value for proofs by mathematical induction?,I was looking at how to do mathematical induction. One source said to use $n = 1$ for the basis step. But I have seen other sources choose the value $n = 0$. So the question is as follows: Question: How do I know when it is right to use $n = 0$ versus $n = 1$? I don't see a pattern. Is there a general rule about how to pick the value? This source uses $n = 1$: http://www.mathsisfun.com/algebra/mathematical-induction.html This source uses $n = 1$: http://cims.nyu.edu/~kiryl/teaching/aa/review1.pdf But this source uses $n = 0$: http://www.cs.odu.edu/~toida/nerzic/level-a/induction/example1/example1.html But how can't $n = 1$ be used for the problem in the last source? I'm confused.,"['induction', 'discrete-mathematics']"
1361079,$\sin x + x\cos x=0$ solution?,Any idea of solving this equation? $$\sin x + x\cos x=0$$ I have also tried by setting a function $g(x)=\sin x+x\cos x$ and searching for solutions using the derivative but my atempts w,['functions']
1361091,Quantile Regression - Linear Loss Minimization,"I'm currently reading Quantile Regression by Roger Koenker, and for some reason, I'm having a lot of trouble deriving one of his equations (sect. 1.3, p. 5-6). He goes on to demonstrate that $\hat{x}$ minimizing a linear loss corresponds to the $Ï„$ th quartile of the distribution, when the loss is defined by $$
\rho_\tau(u) = (\tau - 1)\min(0,u) + \tau \max(0,u).
$$ To do so, he first writes down the expected loss using the CDF $F$ of the distribution: $$
E[\rho_\tau(X-\hat x)] = (\tau -1)\int_{-\infty}^{\hat x}(x-\hat x)\,dF(x) + \tau \int_{\hat x}^\infty (x-\hat x)\,dF(x), \tag 1
$$ where $X$ is a random variable, which he then differentiates by $\hat x$ to obtain $$
(1-\tau)\int_{-\infty}^{\hat x} dF(x) - Ï„\int_{\hat x}^\infty dF(x) \tag 2
$$ Now I'm a bit rusty with calculus, and I couldn't find how to go from (1) to (2). I tried to integrate by parts and to apply the fundamental theorem, but I couldn't obtained the result (2).","['statistics', 'calculus', 'regression']"
1361093,Showing a set is a compact subset of $\mathbb{R}$,Question: Let $$A=\{ x \in \mathbb{R}: x(x^{3}-3x-1)\leq15 \}.$$ Show that A is a compact subset of $\mathbb{R}$. I am just wondering how to approach this problem. Should I try showing it directly? Or is it enough to show that the set is closed and bounded in $\mathbb{R}$? Thanks,"['real-analysis', 'general-topology', 'compactness']"
1361137,Finding all groups of order $7$ up to isomorphism?,"I'm learning group theory but I didn't learn any concepts of building groups. I know that there exists the identity group $\{e\}$, and the group with 2 elements: $\{e,a\}$. If I try to create a group with 3 elements, let's say: $\{e,a,b\}$ then we would have: $ea = a, eb = b, aa = ?$, and what about $ab$? Am I supposed to try this for the $7$ elements? What does the statement ``up to isomorphism"" mean? In particular, how many groups of order $7$ are there up to isomorphism? I'm really confused, and my book says nothing about it.","['abstract-algebra', 'group-theory']"
1361160,Urn with increasing number of distinct balls,"Suppose we have an urn that initially has only one labelled ball inside. At each time step, we flip a biased coin with probability $p$ $(\in(0,1))$ of landing on heads and probability $1-p$ of landing on tails. If we get heads, we add a new ball into the urn that has a distinct label from all the other balls. If we get tails, we draw a random ball from the urn, note its label, then put it back. What I would like to know is whether or not, w.p. 1, there will be a ball that will be drawn infinitely often from the urn as we indefinitely continue this experiment. My hunch tells me that such a ball does exist a.s., but I can't seem to prove it. I tried using Borel-Cantelli on the sequence of events in which the initial ball is drawn at time $n$. However, I found the sum of the probabilities of those events to be infinite, so Borel-Cantelli can't help us (the events are neither independent nor monotone increasing). If you have any help to offer me for this problem, I'll very much appreciate it.","['probability-theory', 'probability']"
1361193,Regularity of solutions to a transport equation,"Currently I am working on a transport equation and have been able to prove the existence and uniqueness of a weak measurable solution to said equation. I am now working in trying to jot down (with proof) some regularity results. Other papers have stated (without proof) that given a weak solution to the equation $$\partial_t f(t,\mathbf{x}) + div_{\mathbf{x}}(a(t,\mathbf{x})f(t,\mathbf{x})) = 0$$
$$f(0,\mathbf{x}) = f_0(\mathbf{x})$$
$$t \geq 0, \mathbf{x} \in \mathbb{R}^d$$ with initial datum $f_0$ Lipschitz and $a(t,\mathbf{x})$ (vector valued function) bounded and Lipschitz in $\mathbf{x}$ then the solution $f(t,\mathbf{x})$ is Lipschitz in $\mathbf{x}$. How is this proven? Moreover, if we impose stronger conditions on $a$ and $f_0$ will that also comply with a stronger regularity of the solution $f$?","['regularity-theory-of-pdes', 'optimal-transport', 'ordinary-differential-equations', 'partial-differential-equations']"
1361218,Is $f$ bijective on$\mathbb{R}^2$?,"Let $f(x,y)=(x^2-y^2,2xy)$ a function from $\mathbb{R}^2\to\mathbb{R}^2$. Study if $f$ does have an inverse in whole $\mathbb{R}^2$? My approach: Since $\det(Df(x,y))=(2x)(2x)-(-2y)(2y)=4x^2+4y^2\neq 0$ for $x,y\neq 0$ then $f$ is locally invertible, for any $(x,y)\neq (0,0)$. But how can I know if this inverse function is the same for all the points in $\mathbb{R}^2$? Thanks!",['multivariable-calculus']
1361288,"If $g(x,y)=(x+f(y),y+f(x))$ Why this function $g$ is onto?","Let $f:\mathbb{R}\to\mathbb{R}$ of class $C^1$ such that $|f'(x)|\leq b<1$ for all $x\in\mathbb{R}$. If we define $g:\mathbb{R}^2\to\mathbb{R}^2$ by $g(x,y)=(x+f(y),y+f(x))$ then why is $g$ an onto function? I been trying the following: since $f$ is of class $C^1$ then $g$ is also of class $C^1$, computing the Jacobian yields $\det(Dg(x,y))=1\cdot 1-f'(y)\cdot f'(x)=1-f'(y)f'(x)\neq 0$ (because $f'(y)f'(x)<1$). Then by the Inverse Theorem $g$ is locally invertible in whole $\mathbb{R}^2$. 
Is that sufficient to conclude $g$ is an onto function? Also from the hypothesis I know that $f$ satisfies a Lipschitz condition with $b$ constant of Lipschitz. Thanks in advance!!","['multivariable-calculus', 'real-analysis']"
1361315,Euler Lagrange differential equation.,"The Lagrange equation is a second order differential equation. However is it an ordinary or partial differential equation? Looking at wikipedia it says it is both, here it is a PDE and here it is an ODE. So which is it? 
\begin{equation}
\frac{d}{dt}\bigg(\frac{\partial \mathscr L}{\partial \dot q}\bigg)-\frac{\partial \mathscr L}{\partial q}=0
\end{equation}
In classical mechanics?","['euler-lagrange-equation', 'ordinary-differential-equations']"
1361376,"$f$ is positive continuous function on $[0,1]$.","$f$ is positive continuous function on $[0,1]$. Define 
$$\int_{0}^{a_n} f(x) dx =  \frac{1}{n} \int_{0}^1 f(x) dx$$
where $a_n>0$.
Find $ \lim_{n\to \infty} n a_n$. It is clear that $lim_{n\to \infty} a_n =0$ because $f(x)$ is positive.I tried to use Weierstrass approximation of continuous function by polynomials but could no quite get the right way.I do not see a  way to bring down this equation $\int_{0}^{a_n} f(x) dx =  \frac{1}{n} \int_{0}^1 f(x) dx$ to $n a_n$. This is a qualifying problem of real analysis.
Small hint works for me. Thanks.",['real-analysis']
1361377,"$\lim\limits_{|z| \to \infty} f(z) = \infty$, show that $f$ is a polynomial. [duplicate]","This question already has answers here : Show that this entire function is polynomial. (2 answers) Closed 2 years ago . I had an approach to the following problem which now I'm not sure will work: If $f$ is entire, and $\lim\limits_{z \to \infty} f(z) = \infty$, show that $f$ is a polynomial. Case 1: there exist $C > 0, N \in \mathbb{N}$ such that $|f(z)| \leq C \cdot |z|^N$ for all $z$.  A standard argument shows that $f$ has to be a polynomial. Case 2 is where Case 1 does not hold.  Since $f(z)$ goes to infinity, $f$ can have only finitely many zeroes $c_1, ... , c_r$ with multiplicities $m_1, ... , m_r$.  Then I'd like to say that $f$ grows so fast that $$g(z) := \frac{f(z)}{(z-c_1)^{m_1} \cdots (z - c_r)^{m_r}}$$ also goes to infinity as $z \to \infty$.  But it's not clear that this will actually be the case.  I might have to weaken the hypothesis of Case 1 to make this work, but I'm not sure how weak I can make it. Assuming I can have $g$ go to infinity, then $g$ will be an entire function with no zeroes, so $\frac{1}{g}$ will be bounded, hence constant.  Thus $f(z) = k(z-c_1)^{m_1} \cdots (z - c_r)^{m_r}$ for some $k \in \mathbb{C}$, as required.",['complex-analysis']
1361401,Why is $z^2$ a conformal mapping?,"It's not a one-to-one mapping, by the Fundamental Theorem of Algebra. $e^z$ is one-to-one, when restricted to a horizontal strip of width = $2\pi i$. Is it a similar argument for $z^2$? Thanks, Edit: $z^2$ doubles angles (and squares magnitudes of numbers in complex exponential form.), so it seems one-to-one - I guess I am possibly confusing myself with the conformal mapping $z^2$ with the polynomial equation, $z^2$ + az = 0.","['complex-analysis', 'conformal-geometry']"
1361410,Why am I getting two different answers for this diff. equation? Completing the square vs quad form.,"With this differential equation, after seperating and integrating, and using the initial condition to solve for C, and then substituting that value of C into the general solution, I must solve for Y to get the particular solution.  Why am I getting two different solutions, where in method 1 I complete the square, and method 2, use the quadratic formula? The ODE is: $\frac{dy}{dx} = \frac{2x}{1+2y}$ The initial condition is: $y(2) = 0$ Separating and integrating, I get the general soln.: $y + y^2 = x^2 + c$ Plugging in the initial condition: $0 + 0^2 = (2)^2 + c$ $c = -4$ Now, plugging in (-4) into the general solution: $y + y^2 = x^2 - 4$ Now, method 1, completing the square: $y^2 + y = x^2 - 4$ $(y + \frac{1}{2})^2 = x^2 - 4 + \frac{1}{4}$ $y = -\frac{1}{2} ^+_- (4x^2 - 15)^{\frac{1}{2}}$ Now, method 2, quad. formula: $y = \frac{-1 ^+_- (1^2 - 4(1)(-x^2+4))^{\frac{1}{2}}}{2}$ $y = -\frac{1}{2} ^+_- \frac{1}{2}(4x^2-15)^{\frac{1}{2}}$ So, what did I do wrong with completing the square?  
The quad. formula is the same answer from the back of the
book.  There is a 1/2 in front of the square root, whereas with 
completing the square, the 1/2 is not there.   What did I do wrong?","['ordinary-differential-equations', 'algebra-precalculus']"
1361414,"Critical points of $f(x,y) = \sin x \sin y, -\pi<x<\pi, -\pi<y<\pi$","$\frac{df}{dx} = \cos x\sin y = 0$ $\frac{df}{dy} = \cos y\sin x = 0$ $\cos x\sin y = \cos y\sin x$ $\frac{\cos x}{\sin x} = \frac{\cos y}{\sin y}$ $\cot x = \cot y$ $P_1 = (\pi/2,\pi/2)$ $P_2 = (-\pi/2,\pi/2)$ $P_3 = (\pi/2,-\pi/2)$ $P_4 = (-\pi/2,-\pi/2)$ $P5 = (0,0)$ This gives me that $P_1$ and $P_4$ are maximum, $P_2$ and $P_3$ are minimum and $P_5$ is saddle. But when testing with Wolfram Alpha I get something different: http://www.wolframalpha.com/input/?i=local+maxima+senxseny+%2C+-pi%3Cx%3Cpi+%2C+-pi%3Cy%3Cpi http://www.wolframalpha.com/input/?i=local+minima+senxseny+%2C+-pi%3Cx%3Cpi+%2C+-pi%3Cy%3Cpi Did I do anything wrong? EDIT: Can people please stop removing the second wolfram link? They aren't the same, one is for maximum point and the other is for minimum.",['multivariable-calculus']
1361450,Linear independence of primitive Dirichlet characters and convolution,"This is not an exercise but merely a question I have. Fix $N \in \mathbb{N}$ and suppose there exist some values $a_k \in \mathbb{C}$, for $k \in \mathbb{Z}_N$, such that 
$$
\sum_{k \in \mathbb{Z}_N} a_k \chi(m - k) = 0
$$
for every $m \in \mathbb{Z}_N$ and every primitive Dirichlet character $\chi$ modulo $N$. Prove or disprove that each $a_k = 0$ for every $k \in (\mathbb{Z}/N\mathbb{Z})^\times$. Thanks!","['fourier-analysis', 'number-theory', 'analytic-number-theory', 'characters', 'convolution']"
