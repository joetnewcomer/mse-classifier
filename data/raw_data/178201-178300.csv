question_id,title,body,tags
3222355,How can the derivative of $f:M\to \mathbb R$ where $M$ is a differentiable manifold can be well defined since it's not unique?,"I have a doubt about the fact that a derivative of $f:M\to \mathbb R$ of a $\mathcal C^1$ manifold is well defined... Indeed, let $a\in M$ and $(U,\varphi )$ a chart from a $\mathbb C^1$ atlas s.t. $a\in A$ . Then, $$f'(a)=\left.\frac{d}{dx}\right|_{x=\varphi (a)}f(\varphi ^{-1}(x)).$$ But if $(\psi,V)$ is an other chart s.t. $a\in V$ , then, $$f'(a)=\left.\frac{d}{dx}\right|_{x=\psi(a)}f(\psi^{-1}(x)).$$ The thing is that I don't see any reason to have $$\left.\frac{d}{dx}\right|_{x=\varphi (a)}f(\varphi ^{-1}(x))=\left.\frac{d}{dx}\right|_{x=\psi(a)}f(\psi^{-1}(x)).$$","['manifolds', 'derivatives']"
3222356,"Minimal Number of Monochromatic Edges in a ""under-colored"" Kneser Graph","Define the $n,k$ Kneser Graph $KN(n,k)$ as follows: $V=\binom{[n]}{k}$ and $ E=\{ (a,b) |a,b\in V, a\cap b = \emptyset\} $ .
In other words, the vertices are the $k$ -sets of an $n$ -element ground set and we connect them if they are disjoint. The Kneser‚ÄìLov√°sz Theorem showed that $\chi(KN(n,k))=n-2k+2$ . Now define $\mu(n,k)$ to be the minimum number of monochromatic edges in $KN(n,k)$ colored with $n-2k+1$ colors (so one less color). I have already shown that $\mu(n,k)\leq\binom{2k-1}{k}$ . Essentially, I created $n-2k+2$ disjoint independent sets where the first $n-2k+1$ of them had the same smallest element and the last set just had all the rest. Then, I combined the last two independent sets and showed that the corresponding graph had at most $\binom{2k-1}{k}$ monochromatic edges. $\textbf{QUESTION:}$ I am struggling to show equality holds when (1) $k=2$ and (2) $n=2k+1$ . I fiddled around with it, but I was not able to find any leads on where the proof would be. Any help is greatly appreciated. \textbf{UPDATE:} I have been thinking about (1) for a bit and I speculate that it has to do with the Erdos-Ko-Rado Theorem that states that the largest family of $k$ element subsets of an $n$ -set such that each subset is pairwise intersecting has $\binom{n-1}{k-1}$ sets. This means that the largest independent set in $KN(n,2)$ has size $\binom{n-1}{2-1}=n-1$ . To add a useful fact, each vertex is adjacent to the number of sets it is disjoint from, so $\binom{n-2}{2}$ sets.","['graph-theory', 'combinatorics', 'coloring']"
3222368,Example of function $g(x)$ s.t. $E(g|X_n|) \not\to 0$,"Consider the following result (assuming probability space ( $\Omega, \mathcal F, \mathsf P$ )) If $g : [0, \infty) \to [0,\infty)$ bounded, strictly increasing, $g(x) > 0$ for $x > 0$ and $\lim_{x \to 0^+}g(x) = 0$ , then $$X_n \to 0 \quad \text{in probability} \iff E(g(X_n)) \to 0.$$ I managed to prove this result. I am interested to see what happens if we relax some of the conditions needed for this result. In particular, when we relax boundedness we see that for $X_n(\omega) = n \mathbb {1}_{[0,1/n)}(\omega)$ we have that $X_n \to 0$ in probability but for $g(x) = x$ , we see that $$E(g(X)) = n \mathsf P(\{\omega \in [0,1/n)\}) = n(1/n) = 1 \not\to 0.$$ Here is where my question comes in: Can we find a $g$ and $(X_n)$ from above satisfying all the conditions except for strictly increasing , such that $$X_n \to 0 \quad \text{in probability but} \quad E(g(X_n)) \not\to0?$$","['measure-theory', 'examples-counterexamples', 'expected-value', 'convergence-divergence', 'probability-theory']"
3222371,Derivative of a quadratic form ‚Äî how to derive it? [duplicate],"This question already has answers here : How to take the gradient of the quadratic form? (6 answers) Closed 4 years ago . I want to know how $$\frac{\delta(x^TAx)}{\delta(x)}=2Ax$$ I think here's what happens (please correct me where wrong): (by: the rule for matrix derivative) $$
\frac{\delta(x^TAx)}{\delta(w)}=x^T(A^T+A)
$$ (by: I assume we can transpose matrices whenever we need to?) $$
x^T(A^T+A) = x^T(A^T+A^T) = x^T(2A^T)
$$ (by: transpose property) $$
x^T(2A^T) = (x^T2A^T)^T = 2Ax
$$ Is that how this happens?","['matrices', 'matrix-calculus', 'linear-algebra', 'derivatives', 'quadratic-forms']"
3222405,"Determining eigenvalues of sum of 2 matrices, and then evaluating whether the limit exists","I'm studying for an exam on Tuesday and have been stumped on this question for a little while. Any hints or help at all would be appreciate! I'm given 2 matrices, $A$ and $R$ as shown below. I'm also given that the eigenvalues for matrix $A$ are: 4, 3, -3, & -2. I am then told to determine the eigenvalues of $C(\alpha, \beta) = \alpha A + \beta R$ , and thus determine when the limit $\lim \limits_{n \to \infty} C(\alpha, \beta)^n$ exists, given that $\alpha$ and $\beta$ are both greater than 0. For the second part (determining when the limit exists), I think I have to use the rule that $\lim \limits_{n \to \infty} C(\alpha, \beta)^n$ will only exist if the eigenvalues $\lvert \lambda\rvert < 1$ , so I imagine I'll have to set up some inequalities to do so. Any help is appreciated, since I've been stuck on this for over a day now! I've attempted to use the fact that the rank of matrix $R$ is 1, but I'm unsure how. $A$ : $$
    \begin{pmatrix}
    2 & 0 & 2 & 0 \\
    2 & -1 & 3 & 0 \\
    2 & -1 & 2 & 1 \\
    -16 & 8 & 13 & -1 \\
    \end{pmatrix}
$$ $R$ : $$
    \begin{pmatrix}
    1 & 1 & 1 & 1 \\
    1 & 1 & 1 & 1 \\
    1 & 1 & 1 & 1 \\
    1 & 1 & 1 & 1 \\
    \end{pmatrix}
$$ EDIT: I realise that given the eigenvalues of A, the eigenvalues of $\alpha A$ will simply be the eigenvalues of A multiplied by $\alpha$ . Is this property true of all matrices? Or is it just because they have the same row sums? EDIT #2: I also realise that the eigenvalues of $A + R$ are 8, 3, -3, 2 i.e. the highest eigenvalue was multiplied by 2 while the rest remained the same. Similarly, eigenvalues of $A + 2R$ are 12, 3, -3, 2 and so on. I again fail to see why this is the case - would appreciate any pointers. EDIT #3: Based on the previous 2 edits, the eigenvalues for $C(\alpha, \beta) = \alpha A + \beta R$ will be: $4\alpha + 4\beta,  3\alpha,  -3\alpha,  -2\alpha$ . I figured this out by manually calculating the eigenvalues for the first couple of $\alpha's$ and $\beta's$ , however is there a trick I'm missing to simplifying this problem, since on the exam I don't think I'll be asked to compute the eigenvalues of a 4X4 matrix.","['limits', 'convergence-divergence', 'linear-algebra', 'eigenvalues-eigenvectors']"
3222406,Triangle parameterisation,"I get how to answer the qs below, the problem is actually finding path $2$ $ \left( 2, 0, 0 \right) $ to $ \left( 0, 1, 0 \right) $ I get $(2-t)i+tj$ yet the answer for path 2 is... $$
(2-t)i+(t/2)j
$$ Don't understand why, any help would be appreciated. Some-more context... Let $G$ be the vector field given by $$
G = 2{y i} + x^{2}{j} + z k
$$ Evaluate the line integral $$
I = \oint_{c} G \cdot dr
$$ where $C$ is given by the three sides of the triangle with verticies $ \left( 0, 0, 0 \right) $ , $ \left( 2, 0, 0 \right) $ and $ \left( 0, 0, 0 \right) $ , and the integration is preformed in the following direction: from $ \left( 0, 0, 0 \right) $ to $ \left( 2, 0, 0 \right) $ then to $ \left( 0, 1, 0 \right) $ and finally back to $ \left( 0, 0, 0 \right) $ . You may evaluate the integral $I$ ...","['trigonometry', 'line-integrals', 'parametrization']"
3222482,Alternative proof of Taylor's formula by only using the linear approximation property,"So a function $f: E \to F$ between the normed spaces $E,F$ is called differentiable in $x \in E$ if there exists a bounded linear map $Df(x): E \to F$ such that for every $h \in E$ we have $$f(x+h)=f(x)+Df(x)h + o(||h||). \tag{1}$$ If $f$ is differentiable for every $x \in E$ and $Df: x \mapsto Df(x)$ is differentiable for every $x \in E$ too we get analogously $$Df(x+e)=Df(x)+D^2f(x)e+o(||e||). \tag{2}$$ Then $f$ is called twice differentiable and for every $h\in E$ we have the ""Taylor expansion of second degree"" $$f(x+h)=f(x)+Df(x)h+\frac{1}{2}D^2f(x)[h] + o(||h||^2), \tag{3}$$ where $D^2f(x)[h]:=(D^2f(x)h)h$ for better readability. I have two questions: How can $(3)$ be proven without resorting to the ""standard proof"" of using integrals? I want to show it by only using the linear approximations  given in $(1)$ and $(2)$ . Inserting $(2)$ in $(1)$ doesn't result in something useful though. Can this be done? Can $(3)$ be used as an alternative definition off twice-differentiability? Analogously what about  the general case of $n$ -times differentiability: $$ f(x+h) = f(x) + \sum_{j=1}^{n} \frac{1}{j!} D^jf(x)[h] + o(\|h\|^n) \tag{4}$$","['banach-spaces', 'frechet-derivative', 'alternative-proof', 'taylor-expansion', 'functional-analysis']"
3222499,"""Conditional distribution"" of Brownian sample paths","I would like to consider the ""conditional distribution"" of the Brownian sample paths conditional on certain sample path functionals, in a similar way that one considers the Brownian bridge. For example, consider the functional $\phi: C[0,1] \rightarrow \mathbb{R}$ defined by, say $$
\phi( W(\cdot) ) = \left( \int_0^1 W dW \right)^2,
$$ or $$
\phi( W(\cdot) ) = \int_0^1 W^2_t dt,  
$$ etc., what is the process obtained by conditioning $W$ on $\phi$ ? Are there any results of this type?","['stochastic-integrals', 'reference-request', 'stochastic-processes', 'brownian-motion', 'probability-theory']"
3222501,Understanding integration over Orthogonal Group,"Let $M$ be an $n \times n$ matrix, and $O_n$ be the orthogonal group of $n \times n$ matrix. Calculate $ m_1 = \int_{O_n}tr(M)dV$ and $m_2 = \int_{O_n} tr(M)^2dV$ where tr(M) is defined as the trace of M. I was given the answer key, but I don't quite understand it. The answer key made the following claims: "" Since the volume integral is invariant under translation in the
orthogonal group, it is invariant under permuting the coordinates, and under
multiplying a row or column by ‚àí1. We have $\int_{O_n} tr(M) dV =  \int_{O_n} \sum_i M_{ii}dV = n\int_{O_n}M_{11}dV = 0$ Also $\int_{O_n}tr(M)^2dV = \int_{O_n} (nM^2_{11} + (n^2 -n) M_{11} M_{22}dV = \int_{O_n} n M^2_{11}dV = \int_{O_n} \sum_i M^2_{1i}dV = \int_{O_n} 1dV$ "" I have very little understanding of this, specifically I don't understand the following: what it means by integrating over the orthogonal group Each step of $ \int_{O_n} \sum_i M_{ii}dV = n\int_{O_n}M_{11}dV = 0$ with these two, I think I can figure out the rest. I would really appreciate it if anyone can help me, thanks in advance. edit 1: So from my understanding so far, $O_n$ can be seen  as a set of point where the real_valued function, tr(M) is being integrated over. $M_{11}$ is integrated over $O_n$ as a constant function. I still don't quite understand the part about permuting the $i^{th}$ row and column of M to the first row and column is equal to $\int_{O_n} M_{11}dV$ If M is given by \begin{bmatrix} a & b \\c & d \end{bmatrix} then $\int_{O_2} tr(M)dV = \int_{O_2} (a + d) dV = 2 \int_{O_2} a dV$ ?
If so, then $2 \int_{O_2} a dV = 2 \int_{O_2} d dV = 2d \int_{O_2} dV  = 2a \int_{O_2} dV$ , then $a = d$ or $\int_{O_2} dV = 0$ ?","['integration', 'volume', 'differential-geometry']"
3222541,Frenet- Serret and Darboux frame,"I am aware the two frames are different, aside from sharing the same tangent unit vector in their basis. But I was wondering, why or when would one choose to use/work with one other frame and not the other? What information does one of frame have that the other does not?","['frenet-frame', 'differential-geometry']"
3222556,Proof that the inverse image of a single element is a discrete space,"Let $f: X \rightarrow Y$ be a local homeomorphism. I want to prove that, for each $y \in Y$ , the fiber $f^{-1}(y)$ is a discrete set, or discrete space (Is there any difference between these two last terms?). These are the posts I have read so far: Local homeomorphism and inverse image When is a local homeomorphism a covering map? Showing the fibre over a point in a covering map is a discrete space. How do I show that a topological space is discrete if all its subsets are closed? Show that in a discrete metric space, every subset is both open and closed. However, I have not been able to fully understand the proof. Some of the posts start the proof by mentioning that the fact that $f$ is a local homeomorphism implies that the fiber is finite; but I do not understand where does that come from, even after browsing MathSE and Wikipedia. Other posts try instead to prove that the fiber is finite, and they do so by first stating that the fiber is a discrete space. All of this make it look like circular reasoning, which does not make sense to me. If there is a concept I do not know, I am willing to visit places like Wikipedia or Subwiki.org ; however this time I have not been able to understand the proof even after reading many articles. So, how can I prove this?","['general-topology', 'metric-spaces', 'covering-spaces']"
3222605,$S^4\setminus S^2$ is homeomorphic to $\Bbb{R}^4 \setminus\Bbb{R}^2$,"I need to show that $S^4\setminus S^2$ is homeomorphic to $\Bbb{R}^4 \setminus\Bbb{R}^2$ , with $\Bbb{R}^2 = \{(x,y,0,0):x,y\in\Bbb{R}\}\subseteq\Bbb{R}^4$ and $S^2 = \{(x,y,z,0):x^2+y^2+z^2=1\}$ . Now, the model solution states that the homeomorphism holds by stereographic projection from a point in $S^2$ . To be a model solution, it seems bold to me. I would have not felt confident enough to just state that in an exam. Even though we are talking about 4 dimensions, is there any way to have a clearer intuition about this fact?",['general-topology']
3222619,Why is a differential a dual basis vector (i.e. why $dx^i \frac{\partial}{\partial x^j} =\delta^i_j$)?,"I have been learning about differential forms, but do not understand exactly why a differential $dx$ forms a dual basis to the basis $\frac{\partial}{\partial x}$ . For example, expand a vector $\vec{w}$ as $\vec{w}=w^je_j$ , $i=1,2,\ldots, n$ on an $n$ -dimensional manifold $M$ with coordinates $x^i$ . Why is it that $$dx^i\vec{w}=dx^iw^je_j=w^i$$ In other words, why is is true that $$dx^ie_j=\delta^i_j$$ I am used to thinking of the differential $dx$ as a differential displacement in the $x$ direction, and not as a dual basis. Is it possible to reconcile the two notions?","['differential-forms', 'differential-geometry']"
3222667,Calculating $\lim_{n\rightarrow\infty} e^{-t\sqrt{n}}\left(1-\frac{t}{\sqrt{n}}\right)^{-n}$,"For my probability homework I have to show that a certain limit exists and equals $e^{\frac{1}{2}t^2}$ . The limit in question is $\lim_{n\rightarrow\infty} e^{-t\sqrt{n}}\left(1-\frac{t}{\sqrt{n}}\right)^{-n}$ . I have tried the following simplifications: \begin{align*}
	&\quad\ \text{substitute $m = \sqrt{n}$}\\
	&= \lim_{m\rightarrow\infty} e^{-tm}\left(1+\frac{-t}{m}\right)^{-m^2}\\
	&= \lim_{m\rightarrow\infty} e^{-tm}\left(\left(1+\frac{-t}{m}\right)^m\right)^{-m}\\
	&= \lim_{m\rightarrow\infty} e^{-tm}\left(e^{-t}\right)^{-m}\\
	&= \lim_{m\rightarrow\infty} e^{-tm+tm}\\
	&= 1
\end{align*} But according to wolfram alpha during the third equality the outcome changes. Can anyone help me on how to properly calculate this limit?",['limits']
3222681,"When is $\int_0^1 \int_0^1 \frac{f(x) - f(y)}{x-y} \, \text{d} x \, \text{d} y = 2 \int_0^1 f(t) \log\left(\frac{t}{1-t}\right) \, \mathrm{d} t$?","Double integrals of this type sometimes appear when using differentiation under the integral sign with respect to two variables. Therefore, I am interested in reducing them to (simpler) single integrals. If $f$ is H√∂lder continuous, i.e. $f \in C^{0,\alpha}([0,1])$ for some $\alpha \in (0,1]$ , the integrals in the following calculation exist and all manipulations are easy to justify ( $F$ is an arbitrary antiderivative of $f$ ): \begin{align}
D(f) &\equiv \int \limits_0^1 \int \limits_0^1 \frac{f(x) - f(y)}{x-y} \, \text{d} x \, \text{d} y = 2 \int \limits_0^1 \int \limits_0^y \frac{f(x) - f(y)}{x-y} \, \text{d} x \, \text{d} y \\
&= 2 \int \limits_0^1 \int \limits_0^y \frac{f(y) - f(y-t)}{t} \, \text{d} t \, \text{d} y = 2 \int \limits_0^1 \int \limits_t^1 \frac{f(y) - f(y-t)}{t} \, \text{d} y \, \text{d} t \\
&= 2 \int \limits_0^1 \frac{F(1) - F(1-t) - F(t) + F(0)}{t} \, \mathrm{d} t = 2 \int \limits_0^1 \log(t) [f(t) - f(1-t)] \, \mathrm{d} t \\
&= 2 \int \limits_0^1 \log\left(\frac{t}{1-t}\right) f(t) \, \mathrm{d} t \equiv S(f) \, .
\end{align} However, this identity is also true for many less regular functions. The integral on the right-hand side exists for all $f$ in the weighted $L^1$ space $$A \equiv L^1 \left([0,1], \left\lvert\log \left(\frac{\cdot}{1-\cdot}\right)\right\rvert \lambda^1\right)$$ ( $\lambda^1$ is the one-dimensional Lebesgue measure). It contains the function $g \equiv t \mapsto \left(t-\frac{1}{2}\right)^{-1}$ , for which we have $S(g) = \pi^2$ , while $D(g)$ does not exist. Therefore, I would like to find the set $$B = \{f \in A : D(f) \text{ exists and } D(f) = S(f)\} \subsetneq A$$ on which the above equation holds. In a first attempt, I have tried to work with $$ C \equiv L^1 \left([0,1], - \log \left[\cdot (1-\cdot)\right] \lambda^1\right) = L^1 ([0,1],\lambda^1) \cap A \, .$$ $f \in C$ has an absolutely continuous antiderivative, so we can work backwards to show $$ S(f) = 2 \int \limits_0^1 \int \limits_t^1 \frac{f(y) - f(y-t)}{t} \, \text{d} y \, \text{d} t \stackrel{?}{=} 2 \int \limits_0^1 \int \limits_0^y \frac{f(y) - f(y-t)}{t} \, \text{d} t \, \text{d} y \, ,$$ but I do not know how to justify changing the order of integration here. Tonelli's theorem does the trick for monotone functions, but according to numerical calculations $S(f) = D(f)$ holds for other $f \in C$ as well, which leaves me with the following questions: Can we prove that the order of integration may be changed or is there an alternative way to show that $C \subset B$ holds? (No! See edit.) What is the largest subset of $B$ (maybe even $B$ itself?) we can find? Some examples ( $\mathrm{G}$ is Catalan's constant): \begin{align}
D(\arctan) \stackrel{\text{H√∂lder cont.}}{=} S(\arctan) &= 2 \mathrm{G} - \frac{\pi^2}{16} - \frac{\pi}{4}\log(2) + \frac{1}{4} \log^2(2) \\
D(\log^3) \stackrel{\text{monotone}}{=} S(\log^3) &= 2 \pi^2 + \frac{2\pi^4}{15} + 12 \zeta(3)\\
D\left(t \mapsto \frac{\log(1-t)}{t^{3/2}}\right) \stackrel{?}{=} S\left(t \mapsto \frac{\log(1-t)}{t^{3/2}}\right) &= \frac{2\pi^2}{3} - 16 \log(2) (1 - \log(2))
\end{align} The last function is neither H√∂lder continuous nor monotone, so it is unclear whether the integrals are equal. Mathematica gives approximately $3.1754$ for the double integral as opposed to $3.1766$ for the single integral, so there is numerical evidence, but a proof would be better, obviously. Edit 26 April 2020 The answers to this question show that there are measurable sets $M \subset [0,1]$ whose indicator function $1_M$ lies in $C$ (obvious) but not in $B$ , since $$\int \limits_0^1 \int \limits_0^1 \frac{\lvert 1_M(x) - 1_M(y) \rvert}{\lvert x-y \rvert} \, \mathrm{d} x \, \mathrm{d} y = 2 \int \limits_{M \times M^\text{c}} \frac{\mathrm{d} x \, \mathrm{d} y}{\lvert x-y \rvert} = \infty \, .$$ This confirms fedja's comment and disproves $C \subset B$ , so the first question is obsolete.","['integration', 'multivariable-calculus', 'lebesgue-integral', 'definite-integrals']"
3222694,"Integral $T_n=\int_{0}^{\pi/2}x^{n}\ln(1+\tan x)\,dx$","For $n\in\Bbb N_0$ , evaluate in closed form $$T_n=\int_{0}^{\pi/2}x^{n}\ln(1+\tan x)\,dx$$ After seeing @mrtaurho's answer to this question , I realized that it would be possible to generalize his method and compute many integrals in the form $$\int_0^{\pi/2}P(x)\ln(1+\tan x)\,dx$$ where $P$ is a polynomial in $x$ . This would be possible once one broke down the integral into lots of little pieces, many of which would be in the forms $$\int_{\pi/4}^{3\pi/4}x^n\ln\sin x\,dx$$ or $$\int_0^{\pi/2}x^n\ln\cos x\,dx$$ or some other similar integrals. I figured that such generalizations would be fairly 'easy' once the general pattern was pinned down. My attempts are below. For starters, we see that $$\begin{align}
T_n&=\int_0^{\pi/2}x^n\ln(\sin x+\cos x)\,dx-\int_0^{\pi/2}x^n\ln\cos x\,dx\\
&=\int_0^{\pi/2}x^n\ln\left(\sqrt{2}\sin\left(x+\frac{\pi}4\right)\right)\,dx-\int_0^{\pi/2}x^n\ln\cos x\,dx\\
&=\int_0^{\pi/2}x^n\ln\left(\sqrt{2}\sin\left(x+\frac{\pi}4\right)\right)\,dx-\int_0^{\pi/2}x^n\ln\cos x\,dx\\
&=\frac12\left(\frac\pi2\right)^{n+1}\frac{\ln2}{n+1}+\int_0^{\pi/2}x^n\ln\sin\left(x+\frac{\pi}4\right)\,dx-\int_0^{\pi/2}x^n\ln\cos x\,dx\\
&=\frac12\left(\frac\pi2\right)^{n+1}\frac{\ln2}{n+1}-\int_0^{\pi/2}x^n\ln\cos x\,dx+\sum_{k=0}^{n}(-1)^{n-k}{n\choose k}\left(\frac\pi4\right)^{n-k}\int_{\pi/4}^{3\pi/4}x^k\ln\sin x\,dx\\
&=\frac12\left(\frac\pi2\right)^{n+1}\frac{\ln2}{n+1}-c_n+\sum_{k=0}^{n}(-1)^{n-k}{n\choose k}\left(\frac\pi4\right)^{n-k}s_k
\end{align}$$ From this point on, we will be making heavy use of the Clausen functions $\mathrm{Cl}_s(z)$ . To evaluate $s_n$ , we will need to note that $\int\ln\sin x\,dx=-\frac12\mathrm{Cl}_2(2x)-x\ln2$ . With this in mind, we integrate by parts: $$\begin{align}
s_n&=-x^n\left(\frac12\mathrm{Cl}_2(2x)+x\ln2\right)\bigg|_{\pi/4}^{3\pi/4}+n\int_{\pi/4}^{3\pi/4}x^{n-1}\left(\frac12\mathrm{Cl}_2(2x)+x\ln2\right)dx\\
&=\frac12\left(\frac\pi4\right)^n\left[(3^n+1)\mathrm G+\frac{1-3^n}{2}\pi\ln2\right]+n\int_{\pi/4}^{3\pi/4}x^{n-1}\left(\frac12\mathrm{Cl}_2(2x)+x\ln2\right)dx\\
&=\frac12\left(\frac\pi4\right)^n\left[(3^n+1)\mathrm G+\frac{1+3^n(2n-1)}{n+1}\frac\pi2\ln2\right]+\frac{n}2\int_{\pi/4}^{3\pi/4}x^{n-1}\mathrm{Cl}_2(2x)dx\\
&=\alpha_n+\frac{n}{2^{n+1}}\int_{\pi/2}^{3\pi/2}x^{n-1}\mathrm{Cl}_2(x)dx\tag{1}
\end{align}$$ Where $\mathrm G$ is Catalan's constant. I know that the remaining integral can be tackled through repeated integration by parts: $$\begin{align}
\int_{\pi/2}^{3\pi/2}x^{n-1}\mathrm{Cl}_2(x)dx&=-x^{n-1}\mathrm{Cl}_3(x)\bigg|_{\pi/2}^{3\pi/2}+(n-1)\int_{\pi/2}^{3\pi/2}x^{n-2}\mathrm{Cl}_3(x)dx\\
f_{n-1}&=\frac{3}{32}\left(\frac\pi2\right)^{n-1}(3^{n-1}-1)\zeta(3)+(n-1)f_{n-2}
\end{align}$$ Where $$f_m=\int_{\pi/2}^{3\pi/2}x^{m}\mathrm{Cl}_{n-m+1}(x)dx$$ Anyway, we have from integration by parts that $$f_j=\underbrace{(-1)^{n-j}\left(\frac\pi2\right)^{n-j}\left[3^j\mathrm{Cl}_{n-j+2}\left(\frac{3\pi}{2}\right)-\mathrm{Cl}_{n-j+2}\left(\frac{\pi}{2}\right)\right]}_{u_j}+\underbrace{(-1)^{n-j+1}j}_{v_j}f_{j-1}$$ And from here , we have $$f_j=f_0\prod_{k=1}^{j}v_k+\sum_{k=0}^{j-1}u_{j-k}\prod_{\ell=1}^{k}v_{j-\ell+1}$$ Which is $$f_j=(-1)^{\frac{j}2(2n-j+1)}j!f_0+n!\sum_{k=0}^{j-1}(-1)^{\frac{k(k+1)}2}\frac{u_{j-k}}{(n-k)!}$$ So $$f_{n-1}=(-1)^{\frac{(n-1)(n+2)}2}(n-1)!f_0+n!\sum_{k=0}^{n-2}(-1)^{\frac{k(k+1)}2}\frac{u_{n-k-1}}{(n-k-1)!}\tag{2}$$ Plugging $(2)$ into $(1)$ gives $s_n$ . As for closed forms, we may evaluate the $\mathrm{Cl}$ expressions in $u_j$ by noting that $$\mathrm{Cl}_{2n}\left(\frac{3\pi}{2}\right)=-\mathrm{Cl}_{2n}\left(\frac{\pi}{2}\right)=-\beta(2n)$$ and $$\mathrm{Cl}_{2n+1}\left(\frac{3\pi}{2}\right)=\mathrm{Cl}_{2n+1}\left(\frac{\pi}{2}\right)=\frac{1-2^{2n}}{2^{4n+1}}\zeta(2n+1)$$ Where $$\beta(s)=\sum_{k\geq0}\frac{(-1)^k}{(2k+1)^s}$$ is the Dirichlet Beta function. As for $c_n$ , the process would probably be similar but way more nasty--which begs my question: Is there a more efficient/different way to evaluate $T_n$ ? Answers involving special functions (including hypergeometric functions) are welcome. Edit: Confirming my previous suspicions, we find (from integration by parts) that $$c_n=-\left(\frac\pi2\right)^{n+1}\frac{\ln2}{n+1}+\frac{n}{2^n}\sum_{k=0}^{n-1}(-1)^{n-k-1}{n-1\choose k}\pi^{n-k-1}g_k$$ Where $$g_k=\int_{\pi}^{2\pi}x^{k}\mathrm{Cl}_2(x)dx$$ Then from IBP again, $$g_k=\left(\frac34-2^k\right)\pi^k\zeta(3)+kd_{k-1}$$ where $$d_j=\int_\pi^{2\pi} x^j\mathrm{Cl}_{k-j+2}(x)dx$$ IBP again provides the (solvable) recurrence $$d_j=(-1)^{k-j+1}x^j\mathrm{Cl}_{k-j+3}(x)\bigg|_\pi^{2\pi}+(-1)^{k-j}jd_{j-1}$$ So, in effect, we have found a horrendous finite sum for $T_n$ . As for closed forms, we note that $$\mathrm{Cl}_{2m}(a\pi)=0\qquad a,m\in\Bbb Z, m\geq1$$ And $$\mathrm{Cl}_{2m+1}(2a\pi)=\zeta(2m+1)$$ $$\mathrm{Cl}_{2m+1}((2a+1)\pi)=(1-2^{-2m})\zeta(2m+1)$$ So after all, $$\begin{align}
T_n&=\frac32\left(\frac\pi2\right)^{n+1}\frac{\ln2}{n+1}+\frac{n}{2^n}\sum_{k=0}^{n-1}(-1)^{n-k}{n-1\choose k}\pi^{n-k-1}\left[\left(\frac34-2^k\right)\pi^k\zeta(3)+kd_{k-1}\right]\\
&+\sum_{k=0}^{n}(-1)^{n-k}{n\choose k}\left(\frac\pi4\right)^{n-k}\left[\alpha_k+\frac{k}{2^{k+1}}f_{k-1}\right]
\end{align}$$ Which is the nastiest integral I've ever seen. I will see if this sum confirms the known results.","['integration', 'definite-integrals', 'special-functions', 'calculus', 'sequences-and-series']"
3222702,Quartic as a product of quadratics,"The statement is Every degree $4$ polynomial with real coefficients is expressible as the product of two degree $2$ polynomials with real coefficients. This and much more general versions are of course simple consequences of the Fundamental Theorem of Algebra and there are of course numerous classical ways to even calculate all the roots of the quartic, thereby establishing much stronger statements constructively. However, suppose that we are only interested in proving the existence of such a factorisation and do not care about what the quadratics are. What's more, we want to do this for quartics only and there is no need for the argument to be applicable to polynomials of higher even degree. Do we still need an instance of the FTA and or a classical method that does this by working out all the coefficients? Or is there now a more elementary (perhaps purely existential) argument? Perhaps one justifying the expressibility of the quartic as the difference of two squared polynomials? This question is inspired by a similar question asked by a high school student and I am actually wondering if there is a proper proof of the above statement that is within the grasp of a high school student, i.e. no FTA, in fact no complex numbers at all, and no extensive calculations of roots/coefficients (and setting up four nonlinear equations simply by comparing coefficients falls into this class).","['galois-theory', 'abstract-algebra', 'polynomials', 'alternative-proof']"
3222709,Find $\lambda $ such that P(X=1)=$\frac{1}{2}$ where X is Poisson($\lambda$),"Find $\lambda $ such that P(X=1)= $\frac{1}{2}$ where X is Poisson( $\lambda$ ). 
Using the formula $P\left(X=x\right)=\frac{\lambda^xe^{-\lambda}}{x!}$ and plugging in 1 for x I was able to simplify it down to $\lambda e^{-\lambda}=\frac{1}{2}$ . I was not too sure about how to solve this equation by hand, so I graphed the function of $\lambda e^{-\lambda}$ and y = $\frac{1}{2}$ , but these graphs never intersect which makes it seem that there is never a time that P(X=1)= $\frac{1}{2}$ because the max of the graph of $\lambda e^{-\lambda}$ is at 0.368. Is this a correct assumption that there is no solution to this problem, if not, how would you solve this equation the proper way?",['statistics']
3222737,"Let $A$ be a real $n\times n$ such that the diagonal entries are positive, the off diagonal entries are negative, and the row sums are positive.","Let $A$ be a $n\times n$ matrix over the reals such that the diagonal entries are all positive, the off-diagonal entries are all negative, and the row sums are all positive. Show that $\det A \neq 0$ . To show that $\det A\neq 0$ , it would be sufficient to show that the system $AX = 0$ does not have a nontrivial solution. Then I suppose that to show that, one could assume that it has a nontrivial solution and then obtain a contradiction. But I'm stuck on actually doing that part. Any suggestions? I'm also interested in where I can find questions similar to this one to practice.",['linear-algebra']
3222754,How many are the non-negative integer solutions of $ùë• + ùë¶ + ùë§ + ùëß = 16$ where $x < y$? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question How many are the non-negative integer solutions of $ùë• + ùë¶ + ùë§ + ùëß = 16$ where $x < y$ ? Anyone can explain how to think to approach this type of problem? 
The answer is 444.","['permutations', 'combinatorics', 'discrete-mathematics']"
3222768,"Finding a potential function for $\vec F=\frac{-y}{x^2+y^2}\vec i+\frac{x}{x^2+y^2}\vec j$ on $\Omega=\mathbb{R}^2- \{(x,y)\,|\,y=0,\,x\geq0 \}$","In one of my Calculus III classes the professor presented the following vector field, defined on the set $S$ of all points $(x,y) \neq(0,0)$ : $$
\bbox[6px,border:1px solid black] {
\vec{F}=\frac{-y}{x^2+y^2}\vec{i} + \frac{x}{x^2+y^2}\vec{j}
}
$$ Although $\vec{F}$ is not a gradient on $S$ , it is a gradient on the set $\Omega=\mathbb{R}^2-\left\{(x,y)\,|\,y=0, \, x\geq0\right\}$ , i.e, all points in the xy-plane except those on the positive x-axis. In the class, my professor wrote a potential function for $\vec{F}$ on $\Omega$ using the following functions: $$
\bbox[6px,border:1px solid black] {
\begin{alignat}{0}
\Psi_1(x,y)=-\arctan\left(\frac x y \right), \,y\neq0 &\text{and} &\Psi_2(x,y)=\arctan\left(\frac y x \right), \,x\neq0
\end{alignat}
}
$$ The first is a potential of $\vec F$ on $\Omega^+=\{(x,y)\in\mathbb{R}^2\,|\,y>0\}$ and on $\Omega^-=\{(x,y)\in\mathbb{R}^2\,|\,y<0\}$ . The last one is a potential of $\vec F$ too, but on $\Omega_+=\{(x,y)\in\mathbb{R}^2\,|\,x>0\}$ and on $\Omega_-=\{(x,y)\in\mathbb{R}^2\,|\,x<0\}$ The potential function wrote by my professor: $$
\bbox[8px,border:1px solid black] {
\phi (x,y) =
\begin{cases}
\Psi_1(x,y) & \text{if $(x,y) \in R_1$} \\[2ex]
\Psi_2(x,y)+\frac\pi 2 & \text{if $(x,y) \in R_2$} \\[2ex]
\Psi_1(x,y)+2\pi & \text{if $(x,y) \in R_3$} \\
\end{cases}
}
$$ (Note: I remember that my professor started with the argument that since $\nabla\Psi_1=\vec F = \nabla\Psi_2$ , then $\Psi_1-\Psi_2=k, \,k \in \mathbb{R}$ ) My doubts: I've tried a lot, but still have not figured out how to build $\phi (x,y)$ using $\Psi_1$ and $\Psi_2$ . So, how to find this specific potential function of $\vec F$ ? Why can't I simply write potential functions of $\vec{F}$ in the following ways $$
\phi_A (x,y) =
\begin{alignat}{0}
\begin{cases}
\Psi_1(x,y) & \text{if $(x,y) \in R_1$} \\[2ex]
\Psi_2(x,y) & \text{if $(x,y) \in R_2$} \\[2ex]
\Psi_1(x,y) & \text{if $(x,y) \in R_3$} \\
\end{cases}
&\text{or}
&\phi_B(x,y)=
\begin{cases}
\Psi_1(x,y) & \text{if $y>0$} \\[2ex]
0 & \text{if $y=0$ and $x<0$} \\[2ex]
\Psi_1(x,y) & \text{if $y<0$} \\
\end{cases}
\end{alignat}
$$ since $\nabla\phi_A=\nabla\phi_B=\vec F$ on $\Omega$ ?","['differential-operators', 'vector-fields', 'multivariable-calculus', 'calculus', 'vector-analysis']"
3222804,If $f(b)-f(a)=(b-a)f'(\frac{a+b}{2})$ prove that any such function is a polynomial of degree $2$ [duplicate],"This question already has an answer here : Functions satisfying $(b-a)f'(\tfrac{a+b}{2}) = f(b)- f(a)$ (1 answer) Closed 5 years ago . Consider $f:\Bbb R \to \Bbb R$ to be differentiable function. $f(b)-f(a)=(b-a)f'(\frac{a+b}{2})$ for all $a,b \in \Bbb R$ prove that any such function is a polynomial of degree $2$ . I am trying using Taylor's expansion that any point $a$ we have $$f(x)=f(a)+(x-a)f'(a)+(x-a)^2/2f''(a)+\cdots$$ . then I observed that the function is not infinitely differentiable. So we can't use the Taylor Series. So, I don't think that proving that the higher order differentiation vanishes also won't help. We can use Lagrange mean value theorem, but how to proceed? Any hint!!","['analysis', 'real-analysis', 'calculus', 'functions', 'derivatives']"
3222829,Finding a cubic where tangent line at one point is normal at another intersection,"I have a cubic polynomial in terms of an unknown value $a>0$ that allows the tangent line at $x=1$ to also be the normal to the curve at $x=6$ . The equation is given as $$g(x)=ax^2(x-8)$$ I have found the derivative of the equation to be $g'(x)=3ax^2-16a$ , and the equation of the tangent at $(1,-7a)$ is $y=6a-13ax$ . This tangent line intersects $g(x)$ at $(1,-7a)$ and $(6,-72a)$ . I have tried equating the gradient of the tangent ( $-13a$ ) with the gradient of the normal ( $\frac{-1}{12a}$ ) which only results in $a$ being equal to $\frac{1}{156}$ , which is way too small. I have also tried finding equations for both the tangent and normal using the point-gradient formula: $y+y_1=m(x-x_1)$ . For the tangent at $(1,-7a)$ with gradient $-13a$ , $y=6a-3ax$ ; and the normal at $(6,-72a)$ with gradient $12a$ : $y=\frac{-x+6}{12a}-72a$ . Equating them at point $x=6$ gives $a=0$ . I have been stuck on this problem for a while now. Everything I try seems to yield the wrong answer. Thanks for any help.","['calculus', 'derivatives']"
3222854,Hall subgroups of $\mathrm{PSL}$,"The following is an exercise in Peter Cameron's notes on classical groups. Exercise 2.10 (a) Show that $\mathrm{PSL}(2,5)$ fails to have a Hall subgroup of some admissible order. (b) Show that $\mathrm{PSL}(2,7)$ has non-conjugate Hall subgroups of the same order. (c) Show that $\mathrm{PSL}(2,11)$ has non-isomorphic Hall subgroups of the same order. (d) Show that each of these groups is the smallest with the stated property. I know how to solve these problems. My questions: (1) First consider $\mathrm{PSL}(2,p)$ where $p>3$ is a prime number. If $\mathrm{PSL}(2,p)$ satisfies each of the above conditions, what can I say about $p$ ? (2) What can I say about $p$ if $\mathrm{PSL}(2,p)$ meets ALL the conditions (a)-(c)? Does such $p$ exist? (This is just (1), my bad. But how can I find $p$ such that (b) holds but (c) does not?) (3) For a more general case, what if $p$ my first question is replaced by $q$ , where $q>3$ is a prime power? (4) How can we generalize (d) to $\mathrm{PSL}(n,p)$ or $\mathrm{PSL}(n,q)$ for some fixed $n$ ? (5) The answer shows that (2) without (3) happens infinitely many times if we restrict our attention to a specific set $\pi$ of primes.  Is it still the case for the problem as originally framed (i.e., there are no non-isomorphic $\rho$ -Hall subgroups for any set of primes $\rho$ ). Any idea is a help. Thank you in advance!","['classical-groups', 'finite-groups', 'simple-groups', 'abstract-algebra', 'group-theory']"
3222860,Fractional Sobolev Spaces and Trace Theory,"I've been working with fractional Sobolev Spaces for a while and I still don't get how is it connected to trace theory, is there any literature which goes deeper into such relationship? From the boook Fractional Spaces for the Theory of Elliptic PDE by Fran√ßoise Demengel
  Gilbert Demengel It says that the need of such spaces lies on the existence of the trace for the derivatives , which makes sense since we have things like Neumman conditions. However it doesn't really tell you how a trace is defined for derivatives. The big question is why on such spaces, what is the real advantage on fractional Sobolev spaces and the relation to the distance of traces? And if there is any intuitive idea of such spaces and the need of them? Thanks in advance.","['trace', 'sobolev-spaces', 'functional-analysis', 'fractional-sobolev-spaces']"
3222889,Evaluating $\int_0^\infty \left( \frac{x}{e^x-e^{-x}}-\frac{1}{2} \right) \frac{dx}{x^2}$,"Evaluating $$\int_0^\infty \left( \frac{x}{e^x-e^{-x}}-\frac{1}{2} \right) \frac{dx}{x^2}$$ I tried to calculate it by Mathematica, but it failed to give me an answer. Then I got interested in this problem because it actually can be well evaluated. My attempt Put $$
g(x)=\frac{1}{x}\left(\frac{1}{e^x-1}-\frac{1}{x}+\frac{1}{2}e^{-x}\right)
$$ Considering that $$
\left( \frac{x}{e^x-e^{-x}}-\frac{1}{2} \right) \frac{1}{x^2}
=
-\frac{1}{2x}\left(e^{-x}-e^{-2x}\right)
+ g(x)-2g(2x)
$$ and $$
\int_0^\infty g(x) dx = 2 \int_0^\infty g(2x) dx
$$ thus via Frullani's integral we have $$
\int_0^\infty \left( \frac{x}{e^x-e^{-x}}-\frac{1}{2} \right) \frac{dx}{x^2}
=
-\frac{1}{2} \int_0^\infty \frac{1}{x}\left(e^{-x}-e^{-2x}\right)
=-\frac{1}{2} \log 2
$$ But I am looking forward to other approaches, beacuse this method doesn't seem quite natural. And I would highly appreciate it if you could share any thoughts on how to solve this problem. Thanks in advance!","['complex-analysis', 'calculus', 'real-analysis']"
3222920,What is the difference between cyclic groups and periodic groups?,"I have read that a cyclic group G is one that can be generated by a single element a called the generator , aœµG . 
While looking up Wikipedia for Torsion Groups(periodic groups), I found: ""In group theory, a branch of mathematics, a torsion group or a periodic group is a group in which each element has finite order. All finite groups are periodic. The concept of a periodic group should not be confused with that of a cyclic group ."" I am confused, after this I couldn't find a satisfying difference between the two(periodic and cyclic groups). Thanks","['number-theory', 'group-theory', 'abstract-algebra']"
3222947,Show that $\int_\mathbb{R^n} F(|g(x)|) dx = - \int_0^{\infty} F(\alpha) d \lambda(\alpha) = \int_0^{\infty} F'(\alpha) \lambda(\alpha) d\alpha$,"So, I've been tasked with proving the following: Show that if $F$ is a non-negative differentiable function defined on $[0, \infty)$ with $F(0) = 0$ and $g$ is a measurable function on $\mathbb{R^n}$ with $0 < \int_{\mathbb{R^n}} F(|g(x)|) dx < \infty$ then $\int_\mathbb{R^n} F(|g(x)|) dx = - \int_0^{\infty} F(\alpha) d \lambda(\alpha) = \int_0^{\infty} F'(\alpha) \lambda(\alpha) d\alpha$ Here, $\lambda(\alpha) = \mu\{x : |g(x)| > \alpha\}$ I've been able to prove the left and right-most sides are equal: Observe that $$ \begin{align} \int_{\mathbb{R^n}} F(|g(x)|) dx &= \int_{\mathbb{R^n}} \int_0^{|g(x)|} F'(\alpha) d\alpha dx \\ &= \int_{\mathbb{R^n}} \int_0^{\infty} 1_{(\alpha, \infty)} |g(x)| F'(\alpha) d\alpha dx \\ &= \int_0^{\infty} \int_\mathbb{R^n} 1_{(\alpha, \infty)} |g(x)| F'(\alpha) dx d\alpha \\ &= \int_0^\infty F'(\alpha) \lambda(\alpha) d\alpha \end{align}$$ However, now I'm stuck at showing why the middle equality is true. Any help would be greatly appreciated!","['measure-theory', 'lebesgue-integral', 'functional-analysis', 'real-analysis']"
3222950,Conditional probability - sum of dice is even given that at least one is a five,"Question: Calculate the conditional probability that the sum of two dice tosses is even given that at least one of the tosses gives a five. I'm a bit confused by this. Shouldn't the probability just be 1/2, since we know that at least one of the dice tosses gave us a five, thus the other must give us an odd number?",['probability']
3222958,Connected scheme of dimension $0$ which admits a monomorphism to an affine scheme,"Let $X$ be a connected scheme of dimension $0$ (not necessarily Noetherian). If there exists an affine scheme $Y$ such that there exists a monomorphism (in the category of schemes) $X \to Y$ , then is $X$ affine ? If this is not true in general, what if we also assume $Y$ is Noetherian ?","['affine-schemes', 'algebraic-geometry', 'schemes', 'category-theory']"
3223013,How many 3-word sentences can be created?,"I'm doing this old exam in my course and I stumble upon the question: How many $3$ -word sentences can be created from $8$ letters A, $8$ letters B and one of each of the letters C,D,E,F,G when each of these $21$ letters is used exactly once in each sentence? So after some thinking I reason, if all the letters were unique/distinct the number of 3-word sentences should be the number of bijections. But now I have to somehow deduct that I have 8 A's and 8 B's. Can someone please provide an answer but emphasize on how to account for multiple A's and B's, meaning what would be the difference between all unique letters and with letters repeating. Thank you.","['permutations', 'combinatorics', 'discrete-mathematics']"
3223044,Showing that $C=\{h \in L^1(\Omega) : u_1(z) \leq h(z) \leq u_2(z)\}$ is weakly compact.,"Exercise : Let $\Omega \subseteq \mathbb R^n$ be open and bounded, $u_1, u_2 \in L^1(\Omega)$ with $u_1(z) \leq u_2(z)$ almost everywhere in $\Omega$ . We let $C$ be the set $C=\{h \in L^1(\Omega) : u_1(z) \leq h(z) \leq u_2(z)\}$ . Show that $C \subseteq L^1(\Omega)$ is weakly compact. Attempt : I know that if $C$ is uniformly integrable, then, by the Dunford-Pettis Theorem, it will also be relatively weakly compact. Note that $\Omega$ is bounded. Starting off, if $u_1(z) \leq u_2(z)$ holds for positive values, then $|u_1(z)| \leq |u_2(z)|$ . If it holds for negative values, then $|u_1(z)| \geq |u_2(z)|$ . In both cases, $|h(z)|$ will be bounded, thus the set $C$ is bounded. For  the case of positive values and for $\varepsilon \geq 0 \; \exists \delta > 0 :$ \begin{align*}
|A| < \delta &\Rightarrow \int_A |u_2|\mathrm{d}x < \varepsilon \; \forall u_2 \in \Omega\\
&\Rightarrow\int_A |h|\mathrm{d}x < \varepsilon \; \forall h \in C
\end{align*} For  the case of negative values and for $\varepsilon \geq 0 \; \exists \delta > 0 :$ \begin{align*}
|A| < \delta &\Rightarrow \int_A |u_1|\mathrm{d}x < \varepsilon \; \forall u_2 \in \Omega\\
&\Rightarrow\int_A |h|\mathrm{d}x < \varepsilon \; \forall h \in C
\end{align*} Thus, in both cases we yield the $\varepsilon-\delta$ definition of uniform integrability and by the Dunford-Pettis Theorem we get that $C$ is relatively weakly compact. Question : I have failed to show that $C$ is weakly compact and only proved that it is relatively weakly compact. Any hints or elaborations will be greatly appreciated.","['measure-theory', 'weak-convergence', 'uniform-integrability', 'operator-theory', 'functional-analysis']"
3223070,Showing $\int_{-\infty}^{\infty}\frac{\sin^2(\omega)}{\omega^2}d\omega=\pi$,"Given the function $f$ with $f(t)=1$ for $|t|<1$ and $f(t)=0$ otherwise, I have to calculate its Fourier-transform, the convolution of $f$ with itself
and from that I have to show that $$\int_{-\infty}^{\infty}\frac{\sin^2(\omega)}{\omega^2}d\omega=\pi$$ and $$\int_{-\infty}^{\infty}\frac{\sin^4(\omega)}{\omega^4}d\omega
=\frac{2\pi}{3}$$ ( $\tilde{f}(\omega)=\frac{1}{\sqrt{2\pi}}$$\int_{-t}^{t}1\cdot 
e^{-i\omega t}dt$ be the Fourier-transform of $f$ ) For the first two parts I have: $\tilde{f}(\omega)=\frac{2}{\sqrt{2\pi}}\frac{\sin(\omega t)}{\omega}$ and $(f*f)(\omega)=\frac{2}{\pi}\frac{\sin^2(\omega t)}{\omega^2}$ . But from here I dont know how to compute the integrals.
My idea for the first one was using Fourier-Inversion of $(f*f)$ and then
putting $t=1$ . But that gives me $0$ for the integral. Does someone has another idea?
I would be grateful for any hint or advice! Thank you.","['fourier-analysis', 'fourier-transform', 'real-analysis', 'complex-analysis', 'fourier-series']"
3223080,Why is $T: C^\infty(\mathbb{R}) \ni f \mapsto (a_n) = (f^{(n)}(0)) \in \mathbb{R}^\mathbb{N}$ surjective?,"I am reading ""Linear Algebra"" by Takeshi Saito. Let $f \in C^\infty(\mathbb{R})$ . Let $T$ be a mapping such that $T: C^\infty(\mathbb{R}) \ni f \mapsto (a_n) = (f^{(n)}(0)) \in \mathbb{R}^\mathbb{N}$ . $C^\infty(\mathbb{R})$ and $\mathbb{R}^\mathbb{N}$ are linear spaces. $T$ is a linear mapping. In this book, the author says that $T$ is surjective without a proof. Why is $T$ surjective?","['derivatives', 'calculus', 'linear-algebra']"
3223162,"Different methods give different answers. Let A,B,C be three angles such that $ A=\frac{\pi}{4} $ and $ \tan B \tan C=p $.","Let $A,B,C$ be three angles such that $ A=\frac{\pi}{4} $ and $ \tan B \tan C=p $ . Find all possible values of $p$ such that $A,B$ and $C$ are angles of a triangle. case 1- discriminant We can rewrite the following equation $ f(x) = x^2 - (p-1)x + p $ As we know the sum and product of $ \tan C $ and $ \tan B $ Settings discriminant greater than equal to zero. $ { (p-1)}^2 - 4p \ge 0 $ This gives $ p \le 3 - 2\sqrt2 $ . Or $ p \ge 3 + 2\sqrt2 $ solving both equation $ A + B + C = \pi $ $ C + B + \frac{\pi}{4} = \pi $ $ C + B  = \frac{3\pi}{4} $ Using this to solve both the equation give $ p \in $ real I found this on Quora. https://www.quora.com/Let-A-B-C-be-three-angles-such-that-A-frac-pi-4-and-tan-B-tan-C-p-What-are-all-the-possible-value-of-p-such-that-A-B-C-are-the-angles-of-the-triangle the right method $ 0 \lt B , C \lt \frac{3\pi}{4} $ Converting tan into sin and cos gives $ \dfrac {\sin B \sin C}{\cos B \cos C} = p $ Now using componendo and dividendo $ \frac{\cos (B-C) }{- \cos(B+C) } = \frac{p+1}{p-1} $ We know $ \cos (B+C)  =  1/\sqrt2 $ We know the range of $B$ and $C$ $(0, 3œÄ/4)$ Thus the range of $B - C$ . $(0, 3œÄ/4 )$ Thus range of $\cos(B+C)$ is $ \frac{ -1}{\sqrt2} $ to $1$ Thus using this to find range gives $ P \lt 0 $ or $ p \ge 3+ 2\sqrt2 $","['trigonometry', 'triangles']"
3223187,area of quadrilateral! only areas given!,Find the $$ar(CEF)+ar(FGB) =\;\;?$$ I am really stuck on this ... spend some hours... did not what to solve and how to proceed? Any suggestions ? Hints also work :),"['euclidean-geometry', 'geometry']"
3223223,Modular characters (Y),"I am reading the proof of following proposition from the book ""A course in Arithmetic"" from J-P Serre: Let a be a non-zero square-free integer and let $m = 4 \cdot |a|$ . Then there exists a unique character $\chi_a$ modulo $m$ such that $\chi_a(p) = \left(\frac{a}{p} \right)$ for all prime numbers $p$ not dividing $m$ . One has $\chi_a ^2 = 1$ and $\chi_a \neq 1$ if $a \neq 1$ . The book states for the proof of the uniqueness part:
The uniqueness of $\chi_a$ is clear because all integers prime to $m$ are products of prime numbers not dividing $m$ . Why is this clear? I just don't see it.",['number-theory']
3223293,Finding matrix with lowest possible rank,"Find the values of $x$ for which the matrix \begin{bmatrix}
        x & 0 & 1 \\
        0 & 1 & 0 \\
        1 & 0 & x \\
  \end{bmatrix} has the lowest rank. Since the rank is the dimension of the row space of $A$ , I should find $x$ for which the dimension of the row space is as low as it can possibly be. I performed Gaussian elimination on $A$ via the following steps: \begin{bmatrix}
        x & 0 & 1 \\
        0 & 1 & 0 \\
        1 & 0 & x \\
\end{bmatrix} \begin{bmatrix}
        1 & 0 & \frac{1}{x} \\
        0 & 1 & 0 \\
        1 & 0 & x \\
\end{bmatrix} \begin{bmatrix}
        1 & 0 & \frac{1}{x} \\
        0 & 1 & 0 \\
        0 & 0 & x -\frac{1}{x}  \\
\end{bmatrix} So I should now let $x - \frac{1}{x} = 0 $ , more specifically let $x^2 = 1$ and thus $x = \pm 1$ . Overall resulting in an overall lowest rank of $2$ . Is this the correct answer? And is it the correct way of thinking about questions such as these? Should I have considered the null space instead? Thanks in advance.","['matrices', 'matrix-rank', 'linear-algebra', 'matrix-completion']"
3223302,"If $G$ is a linear functional on $L^p$ given by $G(f)=\int fg \,d\mu$ for all $f \in L^p$, then $g \in L^q$","$\textbf{Problem.}$ Let $\frac{1}{p} +\frac{1}{q}=1 $ ,  and let $G$ be a linear functional on $L^p$ given by $$G(f)=\int fg \,d\mu $$ for all $f\in L^p$ . Show that $g\in L^q$ . I don't know that $g\in L^q$ holds for arbitrary measrue $\mu$ . If $\mu$ is $\sigma$ -finite and $fg\in L^1$ , I can prove that $g\in L^q$ . Also if $G$ is bounded, Riesz representation theorem implies that there exist such $g\in L^q$ . But I'm having a hard time to prove $G$ is bounded linear functional. My professor said that Use Hahn-Banach theorem. How can I use that theorem in this problem? Any help will be appreciated.","['measure-theory', 'real-analysis']"
3223315,Proving $\frac{S_n}{n^{1/p}} \to 0$ almost surely implies $E|X_i|^p < \infty$ [duplicate],"This question already has answers here : Almost Sure Convergence and Bounded Expectation (2 answers) Closed 3 years ago . The following is a execise from Durrett's: Probability: Theory and Examples . Let $p>0$ .  Let $X_i$ be i.i.d random variables such that $EX_i =0$ , and define $S_n = \sum_{i=1}^n X_i$ . Show that if $\dfrac{S_n}{n^{1/p}}\to 0\,$ almost surely, then $E|X_i|^p < \infty$ . He says it's an easy exercise so I imagine there is a simple trick here. To start, I know that $$E|X_i|^p = \int_0^\infty P(|X_i|^p > x)dx = \int_0^\infty P(|X_i| > x^{1/p}) dx \leq 1+ \sum_{n=1}^\infty P(|X_n|>n^{1/p})$$ which I think might be helpful. I also think a contrapositive proof might be best? Suppose $E|X_i|^p = \infty$ . Then this implies $\sum_{n=1}^\infty P(|X_n|>n^{1/p}) = \infty$ . Since the $X_i$ 's are independent, the second Borel-Cantelli lemma then tells us: $$P(|X_n|>n^{1/p} \quad i.o) =  1.$$ And I think that this implies $S_n/n^{1/p}$ cannot converge to $0$ , since for the above implies $S_n/n^{1/p}>1$ infinitely often. Hence, giving the result by contrapositive. I'm really not confident with my probability theory. So I want to check my reasoning is correct here.","['proof-verification', 'probability-theory', 'probability']"
3223318,Introducing a product measure from a sequence of transition kernels,"Given measurable spaces $(\Omega_k,\mathcal {F_k })_{k=1 } ^n$ , $P_1 $ a probability measure on $\Omega_1 $ , $P_2 $ a transition kernel from $\mathcal {F_1}$ to $\mathcal {F_2 }$ , $P_3$ a transition kernel from $\mathcal {F_1}\otimes \mathcal {F_2} $ to $\mathcal {F_3}$ ... $P_n $ a transition kernel from $\mathcal {F_1}\otimes \ldots\otimes \mathcal {F_{n-1 }} $ to $\mathcal {F_n } \ $ . Here $ \ P_m $ is a transition kernel from $\mathcal {F_1}\otimes \ldots \otimes \mathcal {F_{m-1 }} $ to $\mathcal F_m $ means that for every $B \in \mathcal {F_m }  , \ P_m( \ , B) $ is a measurable function from $\Omega_1 \times \ldots \times \Omega_{m-1 } $ to $[0,1]$ and for every $(\omega_1,\ldots,\omega_{m-1 } ) \in \Omega_1 \times \ldots \times \Omega_{m-1 } $ a probability  measure on $\mathcal {F_m}  $ . I would like to show that $$P(B)=\int P_1(d\omega_1) \int P_2(\omega_1,d \omega_2) \ldots \int 1_B(\omega_1,\ldots,\omega_n)P_n(\omega_1,\ldots,\omega_{n-1 } , d \omega_n)$$ defines a measure on $\Omega_1 \times \ldots\times \Omega_n $ This should be done by an argument using induction. That is one shows that $P$ is a countably additive set function on the set of measurable rectangles $B_0\times \ldots\times B_n$ (and then extend this set function by the usual arguments). To show countable additivity of $P $ on measurable rectangles we derive this for $n=1 $ and then extended the result to an arbitrary number $n$ with an argument using induction. And this inductive step is what I need help with. For the base case $n=1 $ one do as follows: First one shows that $\omega_1 \mapsto \int 1_B(\omega_1,\omega_2)P_2(\omega_1,d \omega _2)$ is $\mathcal F_1 $ measurable and this is done exactly as usually done in a first step in Fubini's theorem. Then noting that $$B \mapsto \int P_1(d \omega_1)\int 1_B(\omega_1,\omega_2)P_2(\omega_1,d \omega _2)$$ is nonnegative and monotone all we need to verify is the $\sigma $ -additivity. This follows from the monotone convergence theorem: \begin{align*}\int P_1(d \omega_1)\int 1_{\cup_{k=1}^{\infty }F_k} (\omega_1,\omega_2)P_2(\omega_1,d \omega _2) &=\int P_1(d \omega_1)\sum_{k=1 }^ {\infty } \int 1_{F_k} (\omega_1,\omega_2)P_2(\omega_1,d \omega _2) \\ &=\sum_{k=1 }^ {\infty }\int P_1(d \omega_1) \int 1_{F_k} (\omega_1,\omega_2)P_2(\omega_1,d \omega _2). \end{align*} And the inductive step should be something like : assume that $$Q(B)=\int P_1(d\omega_1) \int P_2(\omega_1,d \omega_2)\ldots\int 1_B(\omega_1,\ldots,\omega_{n-1 } )P_{n-2 } (\omega_1,\ldots,\omega_{n-2 } , d \omega_{n-1 } )$$ defines a measure on $\Omega_1 \times \ldots\times \Omega_{n-1 }$ then \begin{align*} & \int P_1(d\omega_1) \int P_2(\omega_1,d \omega_2)\ldots\int 1_{B_0 \times \ldots \times B_n}(\omega_1,\ldots,\omega_n)P_n(\omega_1,\ldots,\omega_{n-1 } , d \omega_n) \\ &=\int 1_{B_0 \times \ldots \times B_{n-1}}   \left(\int 1_{B_n } P_n(\omega_1,\ldots,\omega_{n-1 } , d \omega_n) \right) Q(d \omega_1,\ldots,d \omega_{n-1 } ) \end{align*} And we would then be in a similar situation as in the base case. My question is: If this is correct, how is the equality motivated? Or if not, how should the inductive step be? Thanks in advance!","['measure-theory', 'probability-theory']"
3223322,Is the ring $R$ a topological ring with respect to the following topology?,"Background This question is motivated by trying to answer this question . But before going into the question straight let me give some background. Definition 1. Let $R$ be a ring and $A$ be an ideal of $R$ . Let $\mathcal{T}_A$ denote the set of all semiprime ideals of $R$ containing $A$ . Then, $$\beta(A):=\bigcap_{Q\in \mathcal{T}_A}A$$ called the prime radical of $A$ . Let us now make the following definition, Definition 2. Let $R$ be a ring and $A\subseteq R$ . Let $\mathcal{T}_A$ denote the set of all semiprime ideals of $R$ containing $A$ . Then, $$\eta(A):=\bigcap_{Q\in \mathcal{T}_A}A$$ we call $\eta(A)$ to be the prime radical of $A$ in $R$ . We now note the following, Theorem 1. Let $R$ be a ring and $A\subseteq R$ . Let $\mathscr{P}(R)$ denote the power set of $R$ . Define, $\Phi:\mathscr{P}(R)\to \mathscr{P}(R)$ by, $$\Phi(A)=\eta(A)$$ Then, $A\subseteq \Phi(A)$ for all $A\in \mathscr{P}(R)$ $A\subseteq B\implies \Phi(A)\subseteq \Phi(B)$ for all $A,B\in \mathscr{P}(R)$ $\Phi(\Phi(A))=\Phi(A)$ for all $A\in \mathscr{P}(R)$ Each of the properties follows immediately from Definition 2 . Consequently we claim that the operator $\Phi$ is a closure operator on $R$ . So, it generates a topology on $R$ . Denote this topological space by $(R,\tau)$ and call this space (for the time being) the prime radical space of $R$ , the topology being prime radical topology on $R$ . Question Is $(R,+,\cdot)$ a topological ring with respect to the prime radical topology? I am interested in this question because if the answer of this question is affirmative then the answer of MO post linked above seems to be affirmative for at least for this topology on $R$ .","['general-topology', 'noncommutative-algebra', 'topological-rings', 'modules']"
3223348,"Counterexample for ""continuous image of closed and bounded is closed and bounded"" (in normed spaces).","It's well known that: If $X$ is a finite-dimensional normed space, $C$ is a closed and bounded subset of $X$ and $f:C\subset X\to X$ is continuous, then $f(C)$ is closed and bounded. If $X$ is any normed space, $C$ is a compact subset of $X$ and $f:C\subset X\to X$ is continuous, then $f(C)$ is compact. In the finite-dimensional case, ""compact"" is the same as ""closed and bounded"". Therefore, Item 1 is a particular case of Item 2. Question: Item 2 does not hold with ""compact"" replaced by ""closed and bounded"", right? What are the standard counterexamples? More precisely: What is an example of a Banach space $X$ , a closed and bounded subset $C$ of $X$ and a continuous function $f:C\subset X\to X$ such that $f(C)$ is not bounded? What is an example of a Banach space $X$ , a closed and bounded subset $C$ of $X$ and a continuous function $f:C\subset X\to X$ such that $f(C)$ is not closed?","['banach-spaces', 'normed-spaces', 'functional-analysis', 'general-topology', 'compactness']"
3223386,From $\sup_{n\in \mathbb{N}} \left|\int_0^\Lambda e^{nx} f(x) dx \right| < \infty$ to $f\equiv 0$,"Given $f\in C[0,\Lambda]$ satisfying $$\sup_{n\in \mathbb{N}} \left|\int_0^\Lambda e^{nx} f(x) dx \right| < \infty$$ Prove that $f\equiv 0$ $\,\forall x\in[0,\Lambda]$ I found a weaker proposition If $f\in C[0,1]$ satisfies $$ \left|\int_0^1 e^{nx} f(x) dx \right| =0\,\,\,\forall n\in \mathbb{N}$$ then $f\equiv 0$ $\,\forall x\in[0,1]$ But the solution of that doesn't seem to work here. My attempt $f(\Lambda)=0$ Suppose for contradiction that $f(\Lambda)\ne 0$ . WLOG, we assume that $f(\Lambda)>0$ . Then there exists $\varepsilon > 0$ such that $f(x)>\frac{f(\Lambda)}{2}\,\,\forall x \in [\Lambda-\varepsilon,\Lambda]$ . Denote $M = \sup_{[0,\Lambda]}f$ and $c=\frac{f(\Lambda)}{2}$ . \begin{align}
\int_0^\Lambda e^{nx} f(x) dx
&= \int_0^{\Lambda-\varepsilon} e^{nx} f(x) dx + \int_{\Lambda-\varepsilon}^\Lambda e^{nx} f(x) dx \\
&\ge c\int_{\Lambda-\varepsilon}^\Lambda e^{nx} dx - M\int_0^{\Lambda-\varepsilon} e^{nx} dx \\
&= c\left( \frac{e^{n\Lambda}}{n} - \frac{e^{n(\Lambda-\varepsilon)}}{n} 
\right) - M \left( \frac{e^{n(\Lambda-\varepsilon)}}{n} - \frac{1}{n} \right)
\end{align} Thus $$
\lim_{n\to \infty} \int_0^\Lambda e^{nx} f(x) dx = +\infty
$$ Contradiction. Put $X= \left\{ m : f \equiv 0 \,\, \forall x \in [m,\Lambda] \right\}$ . I aim to show $\inf X = 0$ . Suppose for contradiction that $\inf X = m > 0$ If there exists $\delta>0$ such that $f(x)>0$ or $f(x)<0$ $\forall x \in ]m-\delta,m[$ , using the method in $1.$ leads to a contradiction. But how to deal with the functions like $$f(x) = (\Lambda -x) \sin \frac{1}{\Lambda - x}$$ of which we can't find such $\delta$ ? I would highly appreciate it if you could share any thoughts on how to solve this problem. Thanks in advance! Added Here is a proof. This solution completely solved the trouble I encountered. But I don't quite understand how we can figure out the lemma.
I would highly appreciate it if you could give me some hints to figure it out, or post a new approach. Proof $\ $ It suffices to show that \begin{gather}
\int_{\Lambda-\lambda}^\Lambda f(x)dx=0 \quad \forall \lambda \in ]0,\Lambda]  \tag{1}
\end{gather} We prove $(1)$ via the following lemma, of which we attach a proof at the end. Lemma \begin{gather}
\lim_{x \uparrow \infty} \sum_{k=1}^\infty \frac{(-1)^{k-1}}{k!} \int_0^\Lambda e^{kx(\lambda-s)} \phi(s) ds = \int_0^\lambda \phi \quad \forall \lambda \in [0,\Lambda[ \nonumber
\end{gather} Choose $\phi(s)=f(\Lambda - s)$ , and then from lemma we have $\forall \lambda \in [0,\Lambda[$ $$
\lim_{x \uparrow \infty} \sum_{k=1}^\infty \frac{(-1)^{k-1}}{k!} \int_0^\Lambda e^{kx(\lambda-s)} f(\Lambda-s) ds = \int_0^\lambda f(\Lambda-s)ds 
$$ $$
\lim_{x \uparrow \infty} \sum_{k=1}^\infty \frac{(-1)^{k-1}}{k!} e^{kx(\lambda-\Lambda)} \int_0^\Lambda e^{kxu} f(u) du = \int_{\Lambda-\lambda}^\Lambda f(s)ds 
$$ Denote $\displaystyle\sup_{n\in \mathbb{N}} \left|\int_0^\Lambda e^{nx} f(x) dx \right| = C$ . Thus \begin{align}
\left|\int_{\Lambda-\lambda}^\Lambda f(s)ds\right| \nonumber
&\le C \lim_{x \uparrow \infty} \left(-1 + \sum_{k=0}^\infty \frac{1}{k!} e^{kx(\lambda-\Lambda)}\right) \nonumber \\
&\le C \lim_{x \uparrow \infty} \left(-1 + \exp{\{e^{x(\lambda-\Lambda)}\}} \right) \nonumber \\
&= 0 \nonumber
\end{align} Done. Now we attach a proof of the lemma. Proof of lemma $\ $ We aim to check \begin{align}
\lim_{x \uparrow \infty} \sum_{k=1}^\infty \frac{(-1)^{k-1}}{k!} \int_0^\Lambda e^{kx(\lambda-s)} \phi(s) ds
&\overset{1}{=}
\lim_{x \uparrow \infty} \int_0^\Lambda \sum_{k=1}^\infty \frac{(-1)^{k-1}}{k!} e^{kx(\lambda-s)} \phi(s) ds \nonumber \\
&=
\lim_{x \uparrow \infty} \int_0^\Lambda \left[ 1-\exp{\{ -e^{x(\lambda-s)} \}} \right] \phi(s) ds \nonumber \\
&\overset{2}{=}
\int_0^\Lambda \lim_{x \uparrow \infty} \left[ 1-\exp{\{ -e^{x(\lambda-s)} \}} \right] \phi(s) ds \nonumber \\
&=
\int_0^\lambda \phi \nonumber
\end{align} Denote $$
I_N = \sum_{k=1}^N \frac{(-1)^{k-1}}{k!} \int_0^\lambda e^{kx(\lambda-s)} \phi(s) ds
$$ $$
J_N = \sum_{k=1}^N \frac{(-1)^{k-1}}{k!} \int_\lambda^\Lambda e^{kx(\lambda-s)} \phi(s) ds
$$ Then we have \begin{align}
I_N 
&=
\int_0^\lambda \left( 1-\sum_{k=0}^\infty \frac{ (-1)^k }{k!}e^{kx(\lambda-s)} + \sum_{k=N+1}^\infty \frac{ (-1)^k }{k!}e^{kx(\lambda-s)} \right) \phi(s) ds \nonumber \\
&=
\int_0^\lambda \left[ 1-\exp{\{ -e^{x(\lambda-s)} \}} \right] \phi(s) ds + \int_0^\lambda \sum_{k=N+1}^\infty \frac{ (-1)^k }{k!} e^{kx(\lambda-s)} \phi(s) ds \nonumber \\
&= :
\int_0^\lambda \left[ 1-\exp{\{ -e^{x(\lambda-s)} \}} \right] \phi(s) ds + G_N \nonumber
\end{align} Note that \begin{align}
|G_N|
&\le ||\phi||_\infty \int_0^\lambda \sum_{k=N+1}^\infty \frac{ e^{kx(\lambda-s)} }{k!} ds \nonumber \\
&= ||\phi||_\infty \sum_{k=N+1}^\infty \int_0^\lambda \frac{ e^{kxu} }{k!} du \nonumber \\
&= ||\phi||_\infty \sum_{k=N+1}^\infty \frac{ e^{kx\lambda}-1 }{xk \cdot k!} \nonumber
\end{align} which implies that $$
\lim_{N \uparrow \infty} |G_N| = 0
$$ i.e. $$
\lim_{N \uparrow \infty} I_N = \int_0^\lambda \left[ 1-\exp{\{ -e^{x(\lambda-s)} \}} \right] \phi(s) ds
$$ And note that \begin{align}
|J_N|
&\le ||\phi||_\infty \int_\lambda^\Lambda \sum_{k=1}^\infty \frac{ e^{kx(\lambda-s)} }{k!} ds \nonumber \\
&\le ||\phi||_\infty \int_0^{\Lambda-\lambda} e^{-xu} du \nonumber \\
&= ||\phi||_\infty \frac{1-e^{x(\lambda-\Lambda)} }{x} \nonumber
\end{align} Thus \begin{align}
\sum_{k=1}^\infty \frac{(-1)^{k-1}}{k!} \int_0^\Lambda e^{kx(\lambda-s)} \phi(s) ds
&=
\lim_{N \uparrow \infty} \left( I_N + J_N \right) \nonumber \\
&=
\int_0^\lambda \left[ 1-\exp{\{ -e^{x(\lambda-s)} \}} \right] \phi(s) ds + O(\frac{1}{x}) \nonumber
\end{align} which implies that $$
\lim_{x \uparrow \infty} \sum_{k=1}^\infty \frac{(-1)^{k-1}}{k!} \int_0^\Lambda e^{kx(\lambda-s)} \phi(s) ds
=
\lim_{x \uparrow \infty} \int_0^\lambda \left[ 1-\exp{\{ -e^{x(\lambda-s)} \}} \right] \phi(s) ds \nonumber \\
$$ It remains to prove that $$
R:=\lim_{x \uparrow \infty} \int_0^\lambda \exp{\{ -e^{x(\lambda-s)} \}} \phi(s) ds = 0
$$ Note that $$
|R|
\le
||\phi||_\infty \lim_{x \uparrow \infty} \int_0^\lambda \exp{\{ -e^{xu} \}} du 
$$ and $$
\int_0^\lambda \exp{\{ -e^{xu} \}} e^{ux} du 
= \frac{ \frac{1}{e}-\exp{ \{ -e^{x\lambda} \} }}{x} \nonumber 
\ge \int_0^\lambda \exp{\{ -e^{xu} \}} du \nonumber
$$ Thus we have $$
|R|
\le
||\phi||_\infty \lim_{x \uparrow \infty} \int_0^\lambda \exp{\{ -e^{xu} \}} du 
\le
||\phi||_\infty \lim_{x \uparrow \infty} \frac{1}{ex} = 0
$$ Done.","['banach-spaces', 'functional-analysis', 'real-analysis']"
3223403,Expectation with order statistics,"Let $X_1,\ldots,X_n$ be non-negative i.i.d. random variables with cdf $F$ and let $X_{(n)}$ denote their maximum. For simplicity suppose that $X_1$ has a density $f$ and that $0\le X_1\le M$ almost surely. Let $h: \mathbb R^n\to \mathbb R_{+}$ be some function that is strictly increasing in all components. I am trying to calculate the conditional expectation $$E(h(X_1,\ldots,X_n) | X_{(n)} = x)\tag{1}$$ but I get stuck on the density. I guess we should have something like $$E(h(X_1,\ldots,X_n) | X_{(n)} = x) = \int_0^M\ldots\int_0^M h(x_1,\ldots,x_n) \psi(x_1,\ldots,x_n, x) / (nF^{n-1}(x)f(x))dx_1\ldots dx_n,$$ where $nF^{n-1}(\cdot)f(\cdot)$ is the density of $X_{(n)}$ and the joint density $\psi$ should be zero outside the rectangle $[0,x]^n.$ I tried to get $\psi$ by differentiating the joint cdf with respect to $x_1,\ldots,x_n,x$ but this does not appear to work: Note that if $x_1,\ldots,x_n\le x$ we get that $$P(X_1\le x_1,\ldots X_n\le x_n, X_{(n)} \le x) = P(X_1\le x_1,\ldots X_n\le x_n) = F(x_1)\ldots F(x_n)$$ and if $x_j > x$ for some $j$ we get $$P(X_1\le x_1,\ldots X_n\le x_n, X_{(n)} \le x) = 0.$$ But then differentiating with respect to $x$ yields zero in any case, and $\psi=0$ everywhere cannot be correct. Note that if $X_1$ were discrete we could simply write $$E(h(X_1,\ldots,X_n) | X_{(n)} = x) \\
= \sum_{x_1,\ldots,x_n:x_{(n)} = x} h(x_1,\ldots,x_n) Pr(X_1=x_1,\ldots,X_n=x_n, X_{(n)}=x)/Pr(X_{(n)}=x)\\
= \sum_{x_1,\ldots,x_n:x_{(n)} = x} h(x_1,\ldots,x_n) Pr(X_1=x_1,\ldots,X_n=x_n)/Pr(X_{(n)}=x)\\
= \sum_{x_1,\ldots,x_n:x_{(n)} = x} h(x_1,\ldots,x_n) Pr(X_1=x_1)\ldots Pr(X_n=x_n)/Pr(X_1=x)^n$$ but I'm not sure how to generalize that intuition to the continuous case because the set $\{x_1,\ldots,x_n:x_{(n)} = x\}$ has probability zero. In addition to the properties mentioned above I know that $h$ is symmetric in all components, that is $h(x_1,\ldots,x_n) = h(x_{\pi(1)},\ldots,x_{\pi(n)})$ for all permutations $\pi$ of $n$ letters. But I'm not sure if this is helpful for this calculation.","['probability-distributions', 'conditional-expectation', 'real-analysis', 'probability-theory', 'probability']"
3223408,"Visual representation of difference between closed, bounded and compact sets","I have trouble grasping the difference between bounded, closed and compact sets. As a picture is worth a thousand words (especially for a person with a light math background), I would like to get a graphical representation of those concepts. Definitions: Bounded set A set having all its points lie within some fixed distance of each other. A set in $\mathbb{R}^n$ is bounded if all of the points are contained within a ball of finite radius Closed set A set containing all its limit points. The closure of the set is equal to the set. Compact set compactness is a property that generalizes the notion of a subset of Euclidean space being closed and bounded Here is a figure that I took from this other question and modified: my question Can we say that the subfigures ( $1$ ) and ( $4$ ) of the figure are compact?","['general-topology', 'compactness']"
3223416,All graphs with maximum degree $\ge\frac{|V(G)|}{2}$ is Class 1,"Are all even order graphs with maximum degree $\ge\frac{|V(G)|}{2}$ is Class 1(edge-colorable(chromatic index) with $\Delta(G)$ colors, where $\Delta(G)$ is maximun degree)? Here, $|V(G)|$ denotes the number of vertices in the graph. I think yes, because, by Erdos-Posa theorem on the number of maximal disjoint circuits in a graph, we have that any graph has a matching of size at least $min(\Delta(G), \frac{|V(G)|}{2})$ . Now, consider a regular graph of degree $\Delta(G)$ . Then, by the Erd√≥s-P√≥sa theorem, it will have a perfect matching(as $\Delta(G)\ge\frac{|V(G)|}{2})$ , in fact, $\Delta$ perfect matchings(I think), and thus, should be 1-factorizable, that is must be Class 1. Thus, any graph with maximum degree at least $\Delta(G)$ should be Class 1. Any hints. Thanks beforehand.","['coloring', 'graph-theory', 'matching-theory', 'combinatorics', 'discrete-mathematics']"
3223451,Why the space have to be complex in the spectral theorem for bounded self-adjoint operators?,"In the spectral theorem for compact self-adjoint linear operators $T:H\to H$ (as stated in Conway's book ), the Hilbert space $H$ can be real or complex. However, in the spectral theorem for bounded self-adjoint linear operators $T:H\to H$ (as stated in Bachman's book ), the Hilbert space $H$ have to be complex. Is this assumption really needed? Is there any version for bounded self-adjoint linear operators defined on real spaces ?","['operator-theory', 'self-adjoint-operators', 'hilbert-spaces', 'functional-analysis', 'spectral-theory']"
3223461,"Find a rank one non negative matrix $C$ such that the Matrix $B+C$ will have eigenvalues $13,2,-1$","I have been given the matrix $$B = \begin{pmatrix} 1 & 3 & 3 \\ 2 & 3 & 2\\ 2 & 1 & 4\end{pmatrix}$$ and I've been given that its eigenvalues are: $7, 2, -1$ Firsly I was asked to find the column and row eigenvectors corresponding to the Perron eigenvalue. I know the Perron eigenvalue is $7$ so was able to find the eigenvectors to be $$ v = \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix}, \ w^{T} =\begin{pmatrix} \frac{5}{9} & \frac{2}{3} & 1 \end{pmatrix}$$ The next part asks: Find a rank $1$ non negative matrix C such that the matrix $B+C$ will have eigenvalues $13,2,-1$ . I know from a theorem I've done in class that since $C$ is a rank $1$ matrix it can be expressed as $vy^{T}$ such that $B+C$ has the same eigenvalues as $B$ except that $\lambda_{1}$ of $B$ is replaced by $\lambda_{1} +y^{T}v$ . In this example then I have let $y = \begin{pmatrix} a & b & c \end{pmatrix}^{T}$ and from this I have $13 = 7 + \begin{pmatrix} a & b & c \end{pmatrix}\begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix}$ , therefore $a + b + c = 6$ . I'm uncertain how to solve to find $C$ from this, do I need to use the row eigenvector?","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
3223465,Is the circle homeomorphic to a $6$ petal rose?,"I am trying to explain that a circle is homeomorphic to a $6$ -petal rose with the standard topology of $\mathbb R^2$ , for that i'll need to explain (just in words, it is not necessary to come up with a formula) the homeomorphic function that maps one into the other. So starting from the center of the rose, a series of continous changes can start to push neighboring points of the rose to the border of the circle, in the end every point should be at an equal $r$ distance from the center of the rose. The same thing goes for the inverse, starting with six equidistance points in the circumference, a series of continous chanages should push the their neighboing points following the trajectory of the petals to the center of the flower. The map and the inverse should be continous because an arbitrary open ball of a certain radius in one figure, should be map to another open ball of a certain radius in the other one. The problem is in the center of the flower where the points converge, there is no simil open set in the circle, would that mean that there is no bijection relationship between their topologies?, even though they both share the same. Is the open ball at the center of the flower different from the other ones?, so despite that the and its inverse are continous, it is not homeomorphic?, or the function is not continous?. I am stuck and can't find an explanaition that makes sense, so any insight would be greatly appreciated. Kind Regards","['continuity', 'general-topology', 'circles']"
3223481,Why is a set not a partition of itself?,"Take the set {1,2,3,4} , why is {1,2,3,4} not a partition of this, which condition does it not meet? By my understanding, a partition of a finite set $S$ is any set $\{ S_{1},...S_{n} \}$ of n subsets of $S$ , which satisfy, $S_{i} \ne \emptyset$ for all $1 \leq i \leq n$ , $S_{i} \cap S_{j} = \emptyset$ for all $1 \leq i,j \leq n$ , $i \neq j$ $S_{1} \cup \cdots \cup S_{n} = S$ Seeing as $S$ is a subset of $S$ , which part of the definition breaks down here? Note I think $\{\{1,2,3,4\}\}$ is a partition of $\{1,2,3,4\}$ ... could this be explained to?","['elementary-set-theory', 'combinatorics']"
3223536,Prove recursive formula for number of spanning trees in complete graph.,When $t(n)$ is number of spanning trees in complete graph $K_n$ prove recursive formula for $t(n)$ : $$t(n) = {1\over (n-1)}\sum_{k=1}^{n-1} k(n-k){n-1 \choose k-1}t(k)t(n-k)$$ Could someone prove it or at least help somehow? I can use any method and Cayley's formula( $t(n)=n^{(n-2)}$ ) which I have already proven.,"['graph-theory', 'alternative-proof', 'combinatorics']"
3223622,Proving determinants is independent of theta,"$$A=\left|\begin{array}{ccc}
\sin\left(\theta+\alpha\right) & \cos\left(\theta+\alpha\right) & 1\\
\sin\left(\theta+\beta\right) & \cos\left(\theta+\beta\right) & 1\\
\sin\left(\theta+\gamma\right) & \cos\left(\theta+\gamma\right) & 1
\end{array}\right|$$ My approach is to subtract row 1 from row 2 and row 3 then open the determinant. The problem with this approach is it involves way too much calculation. I tried to take something common but nothing comes to mind. Can somebody give me a better way to approach this problem?","['trigonometry', 'determinant']"
3223627,Independence of Events and Conditional Probability,"A person tried by a 3-judge panel is declared guilty if at least 2 judges cast votes of guilty. Suppose that when the defendant is in fact guilty, each judge will independently vote guilty with probability 0.7, whereas when the defendant is in fact innocent, this probability drops to 0.2. Assume 70 percent of defendants are guilty. Let Ei, i = 1, 2, 3 denote the event that judge i casts a guilty vote. Are these events independent? Explain Are Ei‚Äôs, i = 1, 2, 3, conditionally independent? Explain. Compute the conditional probability that judge number 3 votes guilty given that judges 1 and 2 vote guilty. I am honestly stuck on where to even start with this problem. I would assume that the events are independent because the problem states it as such.","['conditional-probability', 'independence', 'probability']"
3223633,Area under the curve - Integrals (Antiderivatives),"I have a question regarding antiderivatives and area under the curve. I've learned that first, you must to a graph to see if the area is above or below the curve. If it is above the $x$ -axis the area is ""positive"" and I must use $A=\int f(x) dx $ . If it is below the $x$ -axis the area is ""negative"" and I must use $A=-\int f(x) dx $ . In this last one, I've understood that negative outside the integral is because the integration alone will be negative because is under $x$ -axis, but an area can't be negative so that's why is multiply by that negative. I've also seen this with absolute value $A=|\int f(x) dx| $ that I think have the same purpose. This is an example of an exercise: Determine the area of the region bounded by the curve of the function $f(x)=4x^3-16x$ the $x$ -axis and the lines $x=-2$ y $x=2$ . Ok. I'll show you my work. I first do the graph. I see that between $-2$ and $0$ the region bounded is above the $x$ -axis so is positive, and that between $0$ and $2$ the region bounded is below the $x$ -axis so is negative. So I'll call the first one $A_1$ and the second $A_2$ . $$A_{total}=\int_{-2}^2 (4x^3-16x) dx$$ $$A_{total}=A_1+A_2$$ $$A_{total}=\int_{-2}^0 f(x) dx+(-\int_0^2 f(x) dx)$$ $$A_{total}=\int_{-2}^0 (4x^3-16x) dx+(-\int_0^2 (4x^3-16x) dx)$$ $$A_{total}=[\frac{4x^4}{4}-\frac{16x^2}{2}]|^{0}_{-2} - [\frac{4x^4}{4}-\frac{16x^2}{2}]|^{2}_{0} $$ $$A_{total}=[x^4-8x^2]|^{0}_{-2} - [x^4-8x^2]|^{2}_{0} $$ $$A_{total}=[((0)^4-8(0)^2)-((-2)^4-8(-2)^2)]-[((2)^4-8(2)^2)-((0)^4-8(0)^2)]$$ $$A_{total}=[-(16-32)]-[16-32]$$ $$A_{total}=[-(-16)]-[-16]$$ $$A_{total}=16+16$$ $$A_{total}=32u^2$$ So I got that the total area is 32 square units. But I was wondering why is this different from doing the integration of $\int_{-2}^2 (4x^3-16x) dx$ . This gives $0$ . $$\int_{-2}^2 (4x^3-16x) dx$$ $$=[\frac{4x^4}{4}-\frac{16x^2}{2}]|^{2}_{-2}$$ $$=[x^4-8x^2]|^{2}_{-2}$$ $$=[(2)^4-8(2)^2]-[(-2)^4-8(-2)^2]$$ $$=[16-32]-[16-32]$$ $$=-16-[-16]$$ $$=-16+16$$ $$=0$$ So I'm a bit confused. Which is one correct? Please help.","['integration', 'calculus', 'definite-integrals']"
3223710,Every $(2n)$-gon have a diagonal which isn't parallel to any side of this polygon,"Prove, that every $(2n)$ -gon have a diagonal which isn't parallel to any side of this polygon. I was thinking about sth like: Let's suppose that ( $2n-3$ ) diagonals from one point are parallel with ( $2n-3$ ) sides of the polygonal. Then the task is to find $4$ other diagonals, which are not parallel with them (so we will have ( $2n+1$ ) not parallel diagonals and only $2n$ sides, so one diagonal have to be not parallel to any side). But I don't know which one should I choose and if it`s a good idea at all. If we assume the polygon to be convex, then it will be possible to do this with pigeon-hole, but I am not sure how (in my opinion probably by counting that there are more not parallel diagonals than sides, but there is the same problem - how to count them?). Thanks for any help","['discrete-mathematics', 'polygons']"
3223765,"How $A$ can be divided to $n+1$ sets $a = \cup_{i=0}^n A_i$ such that $diam(A_i) \lt 1$ for $i=0,...,n.$","Given: Let $ A \subset \mathbb R^n$ be a closed convex set with smooth boundary and diameter $diam(A) = 1$ . Question: How $A$ can be divided to $n+1$ sets $a = \cup_{i=0}^n A_i$ such that $diam(A_i) \lt 1$ for $i=0,...,n.$ I'm still not sure how to approach this, any hints/approaches wouch be highly appreciated.","['elementary-set-theory', 'convex-analysis']"
3223815,Showing that the points of the unit circle have the same infinity of elements as $\mathbb{R}$,"I know that using Bernstein's theorem, I simply need to find two one-to-one functions that map from the unit circle to $\mathbb{R}$ and vice versa. I thought something like $f(x, y)=\arctan\big(\frac{x}{y}\big)$ could work as a mapping from the unit circle to $\mathbb{R}$ , but it seems like isn't one-to-one since and there seems to be problems when $y=0$ . I also really can't think of a one-to-one function that maps from the reals to the unit circle. Is there a better way to do this problem?",['elementary-set-theory']
3223835,Why does $G \times G$ have 1 end?,"Let $G$ be a locally finite, connected graph. Then $G \times G$ has 1 end. This is true, if, for example, we consider $G$ to be a Cayley graph of $\mathbb{Z}$ , just by looking at $\mathbb{Z}^2$ , which is ""obviously"" single ended. I cannot think of a rigorous argument for why $G \times G$ is single ended. If you remove a compact set $K$ , you can always go around the points of $K$ in some way to get between points in $G \times G - K$ . But how, exactly?","['graph-theory', 'general-topology']"
3223898,Difference between several books on complex geometry,"I would like to learn some complex geometry, especially the interaction between algebraic geometry and complex geometry. I found that there are several famous books: Huybrechts, Complex Geometry ; Voisin, Hodge theory and complex algebraic geometry ; Griffiths & Harris, Principles of algebraic geometry ; Demailly, Complex analytic and differential geometry ; Carlson, Muller-Stach, Peters, Period Mappings and Period Domains ; Cattani,  El Zein, etc., Hodge Theory . I am not familiar with differential geometry. So I would like to start with the most basic (or self-contained) books, as well as the one can help me understand algebraic geometry. In order to choose the best book for me, I would like to know what these books mainly talk about, difference between them, and the roadmap between these books. The only thing I know is, according to what Huybrechts said in preface, Voisin and GH's books can be regarded as further readings. However, since I am an entire layman to complex geometry, even after going through the tables of contents of both books, I still don't know if these two books are equivalent, or focus on different topics. Any comments, reviews, and instruction are welcome! Thanks a lot!","['complex-geometry', 'reference-request', 'algebraic-geometry', 'soft-question', 'differential-geometry']"
3223907,What is $(N)+ (N-1) + (N -2) + \cdots + 1$ called?,"This is purely for figuring the name of a mathematical concept. For example, $N \times (N-1) \times \cdots \times  1$ is called factorial. Question: What is $N + (N-1) + (N -2) + \cdots + 1$ called?",['sequences-and-series']
3223929,Why do I get two different answers when solving for arclength?,"I am given that $\frac{dx}{dt}=8t\cos(t)$ and $\frac{dy}{dt}=8t\sin(t)$ . I tried solving for the arclength from $t=0$ to $t=1.$ Method 1: $$\text{Arclength} = \int_{0}^{1} \sqrt{\left(\frac{dx}{dt}\right)^2+\left(\frac{dy}{dt}\right)^2} dx = 4.$$ Method 2: $$\text{Arclength} = \int_{0}^{1} \sqrt{1+\left(\frac{dy}{dx}\right)^2} dx.$$ However, when I solve using method 2, I get $1.22619,$ when the answer should be $4.$ What is causing this difference?","['integration', 'arc-length']"
3223950,Discrete math predicate logic - which of my answers are correct?,"Let $F(x)$ be the predicate ‚Äúx is a frog‚Äù, $T(x)$ be ‚Äúx has a long tongue‚Äù, and $J(x)$ be the predicate ‚Äúx likes to jump‚Äù. The universe of discourse is all animals. I'm asked to write: 
Every frog has a long tongue and likes to jump. I came up with two answers and I am struggling to figure out which one of them would be the correct one (assuming that any of them are correct). I was wondering if someone could distinguish the difference between the two. $\forall x(F(x)\wedge T(x)\wedge J(x))$ $\forall x(F(x) \to (T(x)\wedge J(x)))$","['predicate-logic', 'discrete-mathematics']"
3223993,limit $\sqrt{-n^4+4n^2+4}-in^2 = -2i$?,"What is the limit of the following complex sequence? I get a different result than Wolfram Alpha. My approach was: $$\sqrt{-n^4+4n^2+4}-in^2 = \sqrt{i^2n^4+4n^2+4}-in^2 = in^2\sqrt{1-\frac{1}{4n^2}-\frac{4}{n^4}}-in^2 \xrightarrow{n\xrightarrow{}\infty} 0$$ Wolfram Alpha says the limit is $-2i$ . I'm confused. I must've made a stupid mistake, but cannot find it. Thanks in advance!","['complex-analysis', 'limits', 'sequences-and-series']"
3224017,Applications of the $\frac{5}{8}$ Theorem,"The 5/8 theorem for compact groups says the following: Theorem (5/8 Theorem for Compact Groups) Let $G$ be a compact Hausdorff topological group with Haar measure $\mu$ . If $G$ is not abelian then the probability that two elements of $G$ commute is at most $5/8$ . More precisely, if $G$ is not abelian then $$(\mu \times \mu)(\{(g,g') \in G \times G : [g,g'] = e\}) \leq 5/8.$$ If you don't care or already know how this is proved, skip down the page, past the next horizontal rule. Lemma 1. Let $G$ be a compact Hausdorff topological group with a Borel subgroup $H$ . Let $\mu$ be the Haar measure on $G$ . Then $\mu(H) = 1/[G:H]$ (this is $0$ by convention when $[G:H]$ is infinite). Proof of Lemma 1: The cosets of $H$ partition $G$ , and all have the same measure by translation-invariance of $\mu$ . If there are finitely many cosets, the result follows directly from additivity of $\mu$ . If there are infinitely many cosets, suppose for contradiction that $\mu(H) > 0$ , and pick any sequence $(C_n)_{n \geq 0}$ of distinct cosets of $H$ . Then $$1 = \mu(G) \geq \mu\left(\bigcup_{n \geq 0} C_n\right) = \sum_{n = 0}^\infty \mu(C_n) = \sum_{n=0}^\infty \mu(H) = \infty,$$ a contradiction. Lemma 2. Let $G$ be a group such that $G/Z(G)$ is cyclic. Then $G$ is abelian. Proof of Lemma 2: Let $g \in G$ such that $gZ(G)$ generates $G/Z(G)$ . Let $x,y \in G$ be arbitrary. Then $x \in g^nZ(G)$ , $y \in g^mZ(G)$ for some $n,m \in \mathbb{Z}$ . Write $x = g^n z$ , $y = g^m z'$ for some $z, z' \in Z(G)$ . Since $g$ , $z$ , and $z'$ pairwise commute, $x$ and $y$ commute. Proof of Theorem: Let $$X = \{(g,g') \in G \times G : [g,g'] = e\} = \{(g,g') \in G \times G : g' \in Z(g)\},$$ where $Z(g)$ denotes the centralizer of $g$ in $G$ . By Fubini's Theorem, the measure of $X$ (which we aim to show is at most $5/8$ ) equals $\int_G \mu(Z(g)) \; \mathrm{d}\mu(g)$ . The center of $G$ (which we will denote by $Z$ ) is closed, since it can be written the intersection of closed sets $\bigcap_{g \in G} Z(g)$ ( $Z(g)$ is the inverse image of $\{e\}$ under the continuous map $x \mapsto xgx^{-1} : G \to G$ ). Thus, $$\begin{multline*}\mu(X) = \int_G \mu(Z(g)) \;\mathrm{d}\mu(g) = \int_Z \mu(Z(g)) \;\mathrm{d}\mu(g) + \int_{G \setminus Z} \mu(Z(g)) \;\mathrm{d}\mu(g)\\
= \mu(Z) + \int_{G \setminus Z} \mu(Z(g)) \;\mathrm{d}\mu(g).\end{multline*}$$ If $g \in G\setminus Z$ then $Z(g) \neq G$ , so $[G : Z(g)] \geq 2$ , so $\mu(Z(g)) \leq 1/2$ by Lemma 1. This means that $$\mu(X) \leq \mu(Z) + \frac{1}{2}\mu(G \setminus Z) = \mu(Z) + \frac{1}{2}\left(1 - \mu(Z)\right) = \frac{\mu(Z) + 1}{2}.$$ By Lemma 2, we must have $[G : Z] \geq 4$ (or else $G/Z$ would be cyclic), so by Lemma 1 again, we have $\mu(Z) \leq 1/4$ . Therefore, $\mu(X) \leq 5/8$ , as desired. Corollary (5/8 Theorem for Finite Groups) Let $G$ be a finite group. If the probability that two randomly chosen elements of $G$ commute is greater than $5/8$ , then $G$ is abelian. My question is this: Are there any interesting applications of this result? Interesting examples may include: A finite (or compact) group which is not obviously abelian, but for which it is relatively easy to prove that elements commute with probability >5/8. A non-abelian group which has no compact Hausdorff topology making it into a topological group because ""too many pairs of elements commute"" (i.e. a proof by contradiction that no such topology exists, using the result of the 5/8 Theorem). These are the kinds of applications I was able to imagine, but there are probably many others; I'd be interested to hear if anyone has come across any application of the 5/8 Theorem!","['measure-theory', 'big-list', 'abstract-algebra', 'group-theory', 'probability']"
3224040,Dual of an elliptic curve?,"Since an elliptic curve given by the homogenized polynomial $$y^2z=x^3+axz^2+bz^3$$ is a plane projective curve, we can get its dual. From this Wikipedia link , eliminating $p$ , $q$ , $r$ , and $Œª$ from the following equations, $$X-\lambda \frac{\partial f}{\partial x}(p,q,r) = X- \lambda (-3p^2-ar^2) =0\tag{1}$$ $$Y-\lambda \frac{\partial f}{\partial y}(p,q,r) = Y- \lambda (2qr) =0\tag{2}$$ $$Z-\lambda \frac{\partial f}{\partial z}(p,q,r) = Z- \lambda (q^2-2apr-3br^2) =0\tag{3}$$ $$Xp+Yq+Zr=0\tag{4}$$ yields the equation of the dual curve. However, no matter how I try to solve it, I always fail to eliminate all those variables. Any help will be greatly appreciated.","['algebraic-curves', 'projective-geometry', 'elliptic-curves', 'algebraic-geometry', 'duality-theorems']"
3224056,Prove that the function $f(x)=\frac{\sin(x^3)}{x}$ is uniformly continuous.,"Continuous function in the interval $(0,\infty)$ $f(x)=\frac{\sin(x^3)}{x}$ . To prove that the function is uniformly continuous. The function is clearly continuous. Now $|f(x)-f(y)|=|\frac{\sin(x^3)}{x}-\frac{\sin(y^3)}{y}|\leq |\frac{1}{x}|+|\frac{1}{y}|$ . But I don't think whether this will work. I was trying in the other way, using Lagrange Mean Value theorem so that we can apply any Lipschitz condition or not!! but $f'(x)=3x^2\frac{\cos(x^3)}x-\frac{\sin(x^3)}{x^2}$ Any hint...","['uniform-continuity', 'analysis', 'real-analysis', 'continuity', 'calculus']"
3224131,Can the radius of convergence of a sum of two power series be an arbitrary number?,"Let $\sum_{n=0}^\infty a_n z^n,\sum_{n=0}^\infty b_n z^n$ be two series with the same radius convergence $R>0$ . Can the radius of convergence of their sum be any positive real number which is greater than $R$ ?","['complex-analysis', 'convergence-divergence', 'power-series', 'analytic-functions']"
3224185,Example in Measure theory about Borel Measures,"I need to show that there are distinct $\sigma$ - finite measures in $B(\mathbb{R})$ that are the same in the open sets. Im not really seeing a good example cause every open set is a countable union of compact sets, so the sum has to be the same , but they have to interchange themselves. Any help is appreciated.","['measure-theory', 'examples-counterexamples']"
3224187,"Prove transformation $z \mapsto \frac{az+b}{cz+d}, z = x+ iy, ad-bc = 1$ is isometry of Hyperbolic plane.","Prove transformation $$f: z \mapsto \frac{az+b}{cz+d},\ z = x+ iy,\ ad-bc = 1$$ is isometry of Hyperbolic plane $$M=\{(x,y)\in \Bbb R^2:y>0\} \text{ with Riemannian metric } g= \frac{1}{y^2}(dx \otimes dx + dy \otimes dy).$$ To solve this problem, we can write $ds^2=-\frac{dzd \bar z}{(z- \bar z)^2}$ , for $w = f(z) = \frac{az+b}{cz+d}, f^*ds^2=-\frac{dzd \bar z}{(z- \bar z)^2}$ . Thus $f$ is isometry. I'm trying the tensor calculus approach, but I meet some problems... My effort: Suppose $f:(M,\tilde g)\to (M,g)$ , $f$ is isometry means $f^*\tilde g=g.$ Write $z = x +iy,\ \bar z= x -iy$ , then $g|_z=\frac{1}{y^2}(dx \otimes dx + dy \otimes dy)=\frac{1}{2(\Im z)^2}(dz\otimes d \bar z + d\bar z\otimes dz).$ For $f(z)=\frac{az+b}{cz+d},\ \frac{\partial f}{\partial z}=\frac{1}{(cz+d)^2}, \ \frac{\partial f}{\partial \bar z}=0.$ $df=\frac{\partial f}{\partial z}dz+\frac{\partial f}{\partial \bar z}d\bar z=\frac{dz}{(cz+d)^2}.\ \Im f(z)=\frac{1}{2i}\frac{z-\bar z}{(cz+d)(c\bar z+d)}.$ $g|_z(\frac{\partial}{\partial z},\frac{\partial}{\partial z})= g|_z(\frac{\partial}{\partial \bar z},\frac{\partial}{\partial \bar z})=0,\ g|_z(\frac{\partial}{\partial z},\frac{\partial}{\partial \bar z})=g|_z(\frac{\partial}{\partial \bar z},\frac{\partial}{\partial z})=\frac{1}{2(\Im z)^2}.$ $f^*g|_z(\frac{\partial}{\partial z},\frac{\partial}{\partial z})=g|_{f(z)}(df(\frac{\partial}{\partial z}),df(\frac{\partial}{\partial z}))=g|_{f(z)}(f'(z), f'(z))=?$ How to insert a number $f'(z)$ into tensor $g|_{f(z)}(\cdot,\cdot)$ ? And how to proceed? Thank you for your time and effort.","['mobius-transformation', 'hyperbolic-geometry', 'riemannian-geometry', 'differential-geometry']"
3224214,Is the Artinian property dual to the Noetherian property?,"If $R$ is a ring and $M$ is a left $R$ -module, we say that $M$ is Noetherian whenever it satisfies any of the equivalent conditions: 1N. Every ascending (under subspace inclusion) chain of submodules of $M$ stabilises, 2N. Every non-empty collection of submodules of $M$ has a maximal element under subspace inclusion, 3N. Every submodule of $M$ is finitely generated. Whilst we say that $M$ is Artinian whenever it satisfies either of the equivalent conditions: 1A. Every descending (under subspace inclusion) chain of submodules of $M$ stabilises, 2A. Every non-empty collection of submodules of $M$ has a minimal element under subspace inclusion. It seems to be that conditions 1N and 1A are dual, whilst conditions 2N and 2A are also dual. However I can't seem to think of a property of Artinian modules that seems obviously dual to condition 3N. My question is whether or not there does exist such a condition?","['abstract-algebra', 'modules']"
3224288,Formula for Lie derivative along a time-dependent vector field,"I want to prove the following (if it is true) Let be $M$ a manifold, $\Lambda \in \Omega^k(M)$ a $k$ -form on $M$ , $X_t \in \mathfrak{X}(M)$ a time-dependent vector field on $M$ and $\phi_t \in Diff(M)$ a time-dependent diffeomorphism of $M$ such that $\phi_t' = X_t\phi_t$ .  Then for every $t$ : $\mathcal{L}_{X_t}\Lambda = \frac{\partial}{\partial s}(\phi_{t+s}^{\ }\phi_t^{-1})^*\Lambda|_{s=0}$ My attempt was: Consider the projection $\pi: \mathbb{R} \times M \to M$ . Define $\bar{\Lambda} = \pi^*\Lambda \in \Omega^k(\mathbb{R} \times M)$ . Define $\bar{X} \in \mathfrak{X}(\mathbb{R} \times M)$ such that $\bar{X}(t, p) = \partial_t + X_t(p)$ . Define $\bar{\phi_s} \in Diff(\mathbb{R} \times M)$ such that $\bar{\phi_s}(t, p) = (t + s, \phi^{\ }_{t+s}\phi_t^{-1}(p))$ . Then $\bar{\phi_s}$ is a flow for $\bar{X}$ . So $\frac{\partial}{\partial s}(\phi_{t+s}^{\ }\phi_t^{-1})^*\Lambda|_{s=0} = \frac{\partial}{\partial s}(\pi\bar{\phi_s}\pi_t^{-1})^*\Lambda|_{s=0} = {(\pi_t^{-1})}^*(\frac{\partial}{\partial s}\bar{\phi_s}^*\bar\Lambda)|_{s=0} = {(\pi_t^{-1})}^*\mathcal{L}_{\bar{X}}\bar{\Lambda}$ where $\pi_t = \pi|_{\{t\}\times M}$ . What I can't prove (at least formally, because it seems obvious to me) is that ${(\pi_t^{-1})}^*\mathcal{L}_{\bar{X}}\bar{\Lambda} = \mathcal{L}_{X_t}\Lambda$ for every $t \in \mathbb{R}$ . EDIT One idea is to use Cartan's formula and then discard the $dt$ term, but I hope there is a more elegant way.","['lie-derivative', 'vector-fields', 'differential-forms', 'differential-geometry']"
3224308,"Indefinite integral of $\int x\sqrt{x-1} \, \mathrm dx$","How can I evaluate the indefinite integral $$\int x\sqrt{x-1} \, \mathrm dx?$$ I tried to calculate it using integration by parts, 
I get $$\int x\sqrt{x-1} \, \mathrm dx = \frac{2}{3}x(x-1)^{3/2} - \frac{2}{3}\cdot\frac{2}{5}\cdot(x-1)^{5/2}$$ But this is not the correct solution, and I don't understand why. In the integration by parts formula I used $f(x)=x$ and $g'(x)=(x-1)^{1/2}$ so $f'(x)=1$ and $g(x)=(x-1)^{3/2}\cdot\frac{2}{3}$ . What did I do wrong? I know I should use substitutional integral, but why does my solution not work? Thank you",['integration']
3224342,"In a triangle, prove that $ \sin A + \sin B + \sin C \leq 3 \sin \left(\frac{A+B+C}{3}\right) $",Prove that for any $\Delta ABC$ we have the following inequality: $$ \sin A + \sin B + \sin C \le 3 \sin \left(\frac{A+B+C}{3}\right) $$ Could you use AM-GM to prove that?,"['triangles', 'trigonometry', 'geometry']"
3224343,Estimating a nonlinear function,"Let $F$ be a function in $C(\mathbb{C},\mathbb{C})$ with $F(x)=|x|^\mu x$ , where $\mu>0$ . I can show that that there exists a constant $C$ such that $$
|F(x)-F(y)|\le C(|x|^\mu+|y|^\mu)|x-y|
$$ for all $x,y\in\mathbb{C}$ and $\mu>0$ . However, I am wondering if it's also possible to prove something similar for a function $G_j\in C(\mathbb{C}^n, \mathbb{C})$ such that $$
G_j(x_1,\dots,x_n)=\left(\sum_{k=1}^n |x_k|^2\right)^\mu x_j.
$$ I would expect to get a similar result, i.e. $$
|G_j(x_1,\dots,x_n)-G_j(y_1,\ldots,y_n)|\le C\left(\sum_{k=1}^n |x_k|^{2\mu}+\sum_{k=1}^n |y_k|^{2\mu}\right)|x_j-y_j|
$$ for all $\mu>0$ , $x_1,\ldots,x_n, y_1,\ldots,y_n \in \mathbb{C}$ , and for all $j\in\{1,\ldots,n\}$ . Unfortunately, I wasn't able to show this. What can we say about $|G_j(x_1,\dots,x_n)-G_j(y_1,\ldots,y_n)|$ ?","['complex-analysis', 'calculus', 'analysis', 'inequality']"
3224415,A topology over $\Bbb N$ based on convergence of series.,"Define $\tau=\{U\subseteq \Bbb N:U\in\{\Bbb N,\emptyset\}\vee\sum_{n\notin U}n^{-1}<\infty\}$ . In other words, a subset of $\Bbb N$ is closed iff it is $\Bbb N$ or the sum of the inverses of its elements converges. I have proved so far (without much effort): $\tau$ is a topology over $\Bbb N$ . Singletons are closed. A set is compact iff it is finite. A sequence converges iff it is eventually constant (this is the ""hardest"" fact I proved about this topology; not too hard, though). The space is not Hausdorff, but it is connected. I'd like to know if there is more than at first sight in this topology, that is, if it has deeper properties and if it has some use in number theory or something at all.","['elementary-number-theory', 'general-topology', 'sequences-and-series']"
3224419,Sturm-Liouville differential equation eigenvalue problem,"If we have a Sturm-Liouville differential equation of the form $$
\frac{d}{dx}[p(x)\frac{dy}{dx}]+q(x)y=-\lambda w(x)y
$$ and define the linear operator $L$ as $$L(u) = \frac{d}{dx}[p(x)\frac{du}{dx}]+q(x)u
$$ then we get the equation $L(y)=-\lambda w(x)y$ which defines what is called the eigenvalue problem of the Sturm-Liouville differential equation. My question : why is it called that way despite the fact that there is still a function $w(x)$ in the equation? I thought an eigenvalue problem would have the form $L(y)=-\lambda y$ . What's happening here?","['analysis', 'sturm-liouville', 'ordinary-differential-equations', 'eigenvalues-eigenvectors']"
3224446,Inverse of (i*I+A) for self adjoint operator,"If we have that $A: H \rightarrow H$ is a bounded self-adjoint operator on a Hilbert space, then the spectrum of $A$ is entirely real, i.e. $\sigma(A) \subseteq \mathbb{R}$ .  Hence we know that $i$ is in the resolvent set of $A$ , and in particular $(iI+A)^{-1}$ exists where $I$ is the identity.  Is there any way to find $(iI+A)^{-1}$ explicitly?  I tried to write it in terms of a geometric series (something like $-i\sum_{n=0}^\infty (iA)^n$ ) like we did for showing the resolvent set is open, but that obviously won't converge unless $\|A\| < 1$ .  Is there any other way that the inverse can be solved for explicitly?","['self-adjoint-operators', 'spectral-theory', 'functional-analysis']"
3224457,Does every finite partition of a topological space have a finite refinement such that the closure of every block is a union of blocks?,"Let $X$ be a topological space.  Call a partition $\pi$ of $X$ compatible with the topology, or just compatible, if the closure of each block is a union of blocks (in other words, closures of blocks are saturated with respect to $\pi).$ For example, a compatible finite partition $\pi$ of $\mathbb{R}$ is used here to represent the Kuratowski closure-complement problem in $\mathbb{R}$ as a problem in the finite quotient space $\mathbb{R}/\pi.$ Question: Does every finite partition $\pi$ of an arbitrary topological space $X$ have a compatible finite refinement? A while back I conjectured here that every connected finite space $X$ is homeomorphic to $\mathbb{R}/\pi$ for some compatible partition $\pi$ of $\mathbb{R}.$ I verified in an answer that this holds for all connected $X$ such that $|X|\leq5.$",['general-topology']
3224510,Function that maps strings from one formal language into string of another formal language?,"Is there branch of mathematics and mathematical theories, that considers mappings from strings of one language into strings of another formal language? Example. Let's consider two languages that can be used for syntax of two different logics and that can implement the formal semantics of natural language. One language uses the predicate approach to the modalities https://www.springer.com/us/book/9783319225562 and defer the inference about modalities till the reasoning time. The example of legal sentence in this language is: believes(John, in_conservatives) Other language uses the modalities in the form of modal operators as the first class citizens of the logic and now all the non-logical information about the notion ""believe"" is encoded in the axioms for the modal operator Belief_Diamond_(agent)(predicate) and hence, the sentence becomes in this language: Belief_Diamond_(John)(in_conservatives) So - I just had hopes that there is some theory, methods, best practices about translation from the first language into the second language (e.g. in the case when one decides to fix the notion of ""belief"" and one wishes to investigate it using the methods of logic). Of course, as a programmer, I have no problem to make program, that parses the first sentence into the abstract syntax tree, then do some manipulation in this syntax tree (AST) or maybe event do mapping into the syntax tree of another language and then output the other language sentence. So - essentially - this is the question about mapping from the AST of one language to the AST of another language . But if I had intention to make my efforts methodological and scientific, what theories, good practices, approaches should I use? I am afraid to follow the hammering approach of just pure programming in industrial languages. I have found good article about translating and combining logics (and hopefully - it operates at the level of institutions) https://academic.oup.com/logcom/article-abstract/27/6/1753/2687725 - maybe this is one direction.","['model-theory', 'logic', 'category-theory', 'functions', 'formal-languages']"
3224588,Inverse Laplace transform of $\frac{œÄ\cosh(\sqrt s)}{2 s^{3/2} \sinh(\sqrt s)}$ using complex integration,"I want to find the inverse Laplace transform of $$F(s)=\frac{œÄ\cosh(\sqrt s)}{2 s^{3/2} \sinh(\sqrt s)}$$ using the Bromwich integral $$f(t)= \frac{1}{2\pi i} \int_{c-i\infty}^{c+i\infty} \frac{œÄ\cosh(\sqrt s)}{2 s^{3/2} \sinh(\sqrt s)}e^{st}\, ds$$ The residue at the pole $s=-(n\pi)^2$ is $$\frac{-1}{\pi}\sum_{n=1}^{\infty}\frac{1}{n^2}e^{-(n\pi)^2t}$$ My progress so far has been stunted by the fact that we have a branch point at $s=0$ . Any help is appreciated.","['complex-analysis', 'contour-integration', 'laplace-transform']"
3224598,Evaluate $\lim_{x \to 0} \frac{\cos (\sin x) - \cos x}{x^4}$ [duplicate],"This question already has answers here : Calculating limit of function (3 answers) Closed 5 years ago . Evaluate $\displaystyle \lim_{x\to0} \frac{\cos (\sin x) - \cos x}{x^4}$ The answer stated is $\displaystyle {1 \over 6}$ . What I've tried: $$\displaystyle \lim_{x\to0} \frac{\cos (\sin x) - \cos x}{x^4}$$ $$=\displaystyle \lim_{x\to0} \frac{\cos (\sin x) -1+1- \cos x}{x^4}$$ $$=\displaystyle \lim_{x\to0} \frac{1- \cos x}{x^4} - \frac {1-\cos (\sin x)}{x^4}$$ $$=\displaystyle \lim_{x\to0} \frac{2 \sin^2(\frac {x}{2})}{x^4} - \frac {2 \sin^2(\frac {\sin x}{2})}{x^4}$$ $$=\displaystyle \lim_{x\to0} \left(\frac{\sin(\frac {x}{2})}{x} \right)^2. \left( \dfrac{1}{2x^2} \right) - \frac {2 \sin^2(\frac {\sin x}{2})}{x^4}$$ I'm not sure how I can evaluate the limit by proceeding this way. All help will be appreciated. P.S. I'd prefer not using L'H√¥pital's rule, it can get really messy. EDIT : I should have mentioned that I would prefer if the solution does not use taylor series approximations (or any approximations) for that matter.","['limits', 'calculus', 'limits-without-lhopital']"
3224626,Complement a finite dimensional subspace in a Banach space,"Given a Banach space $(X,\|,\|)$ , and a finite dimensional subspace $F \subset X$ , is it always possible to choose a closed linear complement to $F$ . Explicitly, I mean to say, will there always exist a closed linear subspace $K \subset X$ , such that $$
X \simeq F \oplus W? 
$$ If yes, then what is the easiest way to see that this is the case?","['banach-spaces', 'normed-spaces', 'functional-analysis', 'operator-algebras']"
3224628,Floor and ceiling functions measurable,"I want to show, that $$f: (\mathbb{R}, B(\mathbb{R})) \rightarrow   (\mathbb{R}, B(\mathbb{R}))$$ $$ f(x)=\left[\frac{1}{x}\right] \forall x\ne 0 $$ $$ 0: x=0$$ is measurable. I have to consider $$f^{-1}(\mathbb{Z} \setminus \{0\}) $$ Is this the right idea?","['measure-theory', 'functions', 'analysis', 'real-analysis']"
3224632,About matrices whose row and column sums are 0,"I think the following theorem is true but I'm not sure Let $(A_{ij})$ be a nonzero $m\times n$ matrix of real numbers such that the sum of the entries of each row and each column is $0$ . Then there are indices $i_1,j_1,i_2,j_2$ with $i_1\neq i_2$ and $j_1\neq j_2$ such that \begin{equation}
A_{i_1j_1}>0, A_{i_2j_2}>0, A_{i_1j_2}<0 \text{ and } A_{i_2j_1}<0
\end{equation} I'm looking for a proof or counterexample.",['matrices']
3224660,"Is this a group? If so, what group is it?","I have the following group (at least, I think it's a group) generated by $\langle a,b,c \rangle$ where the operation $\cdot$ obeys the following rules: $a^2=b^2=c^2=1$ (where $1$ is the identity). $\cdot$ is associative. $(cb)(bc) = (bc)(cb) = 1$ , $(bc)^3 = (cb)^3 = 1$ , $(bc)^2 = cb$ , $(bcb)^2 = 1$ , $cbc = bcb$ , $\forall x, \space xa = ax$ . Some of these might be redundant, which is fine, but it's obviously an issue if there's a contradiction, but I can't find one. From these rules, I believe this is a group of order 12 with the elements $\{1,a,b,c,ab,ac,bc,cb,abc,acb,bcb,abcb\}$ . However, I've been looking at the groups of order 12, and this one doesn't seem to be isomorphic to any of them. It's not Abelian, which narrows it down to the Alternating Group, the Dihedral Group, and the Dicyclic Group. It's not the Alternating Group, as that only has 3 elements that square to $1$ , whereas my group has 8 that do ( $\{1,a,b,c,ab,ac,bcb,abcb\}$ ). The Dihedral Group has the right amount of elements that square to $1$ , but has an order 6 element, which my group doesn't have. I hadn't heard of the Dicyclic Group until today, and I've been having trouble finding information on it, but it also seems like it has an order 6 element. So what am I doing wrong here? So did I miss something here and it is actually isomorphic to one of these groups? Is my group ill-defined to begin with? Is it well-defined but not a group? (I'm almost sure this isn't it, because I forced the operation to be associative, and I have an identity, and everything seems to have an inverse.) Did I miscalculate the number of elements? Or some other mistake entirely?","['group-presentation', 'finite-groups', 'combinatorial-group-theory', 'abstract-algebra', 'group-theory']"
3224667,Smooth function that vanishes only on unit cube,"I am having hard time defining a smooth function $f:\Bbb R^3 \to \Bbb R$ such that : $f(x,y,z) = 0$ if and only if $(x,y,z)$ belongs to the unit cube $[0,1]^3$ . I tried generalizing the case of $f:\Bbb R\to \Bbb R$ , such that $f$ vanishes only on $[0,1]$ but failed in the process. I would really appreciate any help,
Thanks in advance!","['calculus', 'differential-geometry']"
3224682,"in 2D dimensional plane, is it problematic to have Frenet-Serret frame with zero curvature?","I have a Frenet-Serret frame moving on a 2-D plane. As of now, I do not care about the binormal vector. So my equations are given by, \begin{align}
\dot{T} = v\kappa N \\
\dot{N} = -v\kappa T
\end{align} Here $v$ is the constant speed and $\kappa$ is the curvature. I don't see any problem with these equations if $\kappa = 0$ , but I have read that the frame is not defined if curvature is zero. Can anyone please explain it?","['frenet-frame', 'vector-analysis', 'differential-geometry']"
3224689,Easiest way to solve $2^{\sin^2x}-2^{\cos^2x}=\cos 2x$,"I'm trying to find best shortcut to crack this task $$2^{\sin^2x}-2^{\cos^2x}=\cos 2x$$ I tried first to go through using trigonometric identities $$\cos^2x+\sin^2x=1 \quad\text{and}\quad 2\sin^2x=1-\cos 2x$$ After substitution I reached this equation $$2^{1-\cos x}-2^{(1-\cos 2x)/2}\cos 2x -2=0$$ Later, I supposed to use substitution to obtain, $$u^4-2u^2-2u=0$$ where $u=\sqrt{v}$ , $v=\frac{2}{w}$ , and $w=\cos 2x$ As a result , I obtained two roots in $\mathbb{R}$ one of them is zero and the other is $1.76929$ . This leads me to doubt in my process. So, is there Hint or procedure can be followed to obtain the desire result?","['algebra-precalculus', 'trigonometry', 'discrete-mathematics']"
3224715,Find antiderivative $\int (2x^3+x)(\arctan x)^2dx $,Find antiderivative $$\int (2x^3+x)(\arctan x)^2dx $$ My try: $$\int (2x^3+x)(\arctan x)^2dx =(\arctan x)^2(\frac{1}{2}x^4+\frac{1}{2}x^2)-\int \frac{2\arctan x}{1+x^2}(\frac{1}{2}x^4+\frac{1}{2}x^2)dx=(\arctan x)^2(\frac{1}{2}x^4+\frac{1}{2}x^2)-\int (\arctan x) (x^2)dx= (\arctan x)^2(\frac{1}{2}x^4+\frac{1}{2}x^2)-\arctan x\cdot \frac{1}{3}x^3-\int (\frac{1}{1+x^2}) (\frac{1}{3}x^3)dx$$ And then I don't know how I can find $\int (\frac{1}{1+x^2}) (\frac{1}{3}x^3)dx$ . Can you help me with it?,"['integration', 'indefinite-integrals', 'real-analysis']"
3224743,Explain why $\frac{d}{dx} \frac{x}{x+1}= \frac{d}{dx}\frac{-1}{x+1}$ and what does the Mean Value Theorem have to do with it?,"My Calculus 1 professor gave us this problem to help us prepare for the final exam. I can show $\frac{d}{dx} \frac{x}{x+1}= \frac{d}{dx}\frac{-1}{x+1}$ and explain why $\frac{d}{dx} \frac{x}{x+1}= \frac{d}{dx}\frac{-1}{x+1}$ is the case, but I have no idea what it has to do with the Mean Value Theorem. Could someone please help me understand what the Mean Value Theorem has to do with this? Mean Value Theorem: If $f(x)$ is defined and continuous on the interval $[a, b]$ and differentiable on $(a,b)$ then there is at least one number $c$ such that $$f'(c)=\frac{f(b)-f(a)}{b-a}$$ Problem Text: Show that $\frac{d}{dx} \frac{x}{x+1}= \frac{d}{dx}\frac{-1}{x+1}$ even though $\frac{x}{x+1} \neq \frac{-1}{x+1}$ . Why is this true and what does the Mean Value Theorem have to do with it?","['calculus', 'derivatives']"
3224764,Why is the definition of inductive set well defined?,"I've been studying from Enderton's Mathematical Introduction to Logic in which he defines an inductive set as follows: To simplify our discussion, we will consider an initial set $B \subseteq U$ and a class $F$ of functions containing just two members $f$ and $g$ , where $f:U√óU‚ÜíU$ and $g:U‚ÜíU$ .
  Thus $f$ is a binary operation on $U$ and $g$ is a unary operation. (Actually $F$ need not be finite; it will be seen that our simplified discussion here is, in fact, applicable to a more general situation. $F$ can be any set of relations on $U$ , and in Chapter 2 this greater generality will be utilized. But the case discussed here is easier to visualize and is general enough to illustrate the ideas. For a less restricted version, see Exercise 3.)
  If $B$ contains points $a$ and $b$ , then the set $C$ we wish to construct will contain, for example, $b, f (b, b), g(a), f (g(a), f (b, b)), g( f (g(a), f (b, b)))$ .
  Of course these might not all be distinct. The idea is that we are given certain bricks to work with, and certain types of mortar, and we want $C$ to contain just the things we are able to build. In defining $C$ more formally, we have our choice of two definitions. We can define it ‚Äúfrom the top down‚Äù as follows: Say that a subset $S$ of $U$ is closed under $f$ and $g$ iff whenever elements $x$ and y belong to $S$ , then so also do $f(x,y)$ and $g(x)$ .Say that $S$ is inductive iff $B \subseteq S$ and $S$ is closed under $f$ and $g$ . Let $C^*$ be the intersection of all the inductive subsets of $U$ ; thus $x \in C^*$ iff $x$ belongs to every inductive subset of $U$ . It is not hard to see (and the reader should check) that $C^*$ is itself inductive. Furthermore, $C^*$ is the smallest such set, being included in all the other inductive sets. The second (and equivalent) definition works ‚Äúfrom the bottom up.‚Äù We want $C_*$ to contain the things that can be reached from $B$ by applying $f$ and $g$ a finite number of times. Temporarily define a construction sequence to be a finite sequence $<x_1, . . . , x_n>$ of elements of $U$ such that
  for each $i \leq n$ we have at least one of $x_i \in B$ , $x_i=f(x_j,x_k)$ for some $j<i,k<i$ , $x_i =g(x_j)$ for
  some $j<i$ . In other words, each member of the sequence either is in B or results from earlier members by applying f or g. Then let C be the set of all points x such that some construction sequence ends with x. I am confused as in other texts I've read an inductive set to be defined in a similar fashion to as follows from Enderton's Elements of Set Theory : A set $A$ is said to be inductive iff $\emptyset \in A$ and it is ""closed under successor,"" i.e., $(\forall a \in A) a^+ \in a$ or similarly for For Natural Numbers What confuses me most is that by just defining an inductive set as such "" $S$ is inductive iff $B \subseteq S$ and $S$ is closed under $f$ and $g$ "" doesnt that lead to inductive sets such as $\{1,2,3,4\}$ where $f$ and $g$ can be sent to map all to the same element $1$ . What is the advantage to this type of definition for induction and how is it consistent with other definition. Using this definition of induction how may one say that the natural numbers are an inductive set any more than my finite set above. What is the intuition in this definition? Thank you for help.","['logic', 'natural-numbers', 'intuition', 'elementary-set-theory', 'induction']"
3224786,Looking for a criterion for split exact sequence of sheaves of modules similar to Miyata's theorem,"Miyata's theorem states that: For a commutative Noetherian ring R, and a short exact sequence: $0 \rightarrow M \rightarrow P \rightarrow N \rightarrow 0$ if $P$ is isomorphic to a direct sum of $M$ and $N$ , then the sequence split. Is there a similar criterion for sheaves of $O_X$ -modules on a variety? Thanks.","['algebraic-geometry', 'sheaf-theory']"
3224818,Find $a$ such that $-3\sin^{2}(x)-4\sin(x)+3-a=0$ has solution,"I have the function $f:[0,2\pi]\rightarrow \mathbb{R},f(x)=3\cos^{2}(x)-4\sin(x)$ I need to find the values of $a$ , a real parameter such that $f(x)=a$ has solution. My try:I got $-3\sin^{2}(x)-4\sin(x)+3=a$ so $-3\sin^{2}(x)-4\sin(x)+3-a=0$ I noted $\sin(x)=t$ and I got a quadratic equation and I put the condition that the discriminant to be $\geq0$ and I got $a\leq\frac{13}{3}$ and the right answer is $[-4,\frac{13}{3}]$ Which condition I forgot?",['trigonometry']
3224829,Solutions of $x^2+3y^2=p$ for $p$ prime [duplicate],This question already has answers here : Let $p$ be prime and $\left(\frac{-3}p\right)=1$. Prove that $p$ is of the form $p=a^2+3b^2$ (3 answers) Closed 5 years ago . $x^2+3y^2=p \; \;$ for $p$ prime greater than $3$ has a solution if and only if $p\equiv 1\pmod 3$ I am supposed to use the fact that the class number of $\mathbb Q(\sqrt-3)$ is 1. I already got the first direction. Would appreciate it if anyone could point me to the right answer (hints) for the 2nd direction,"['number-theory', 'algebraic-number-theory']"
3224834,Definition of a locally lifted measure,"I have just begun studying $g$ -measures, and thought it a good idea to study Keane's paper [1], where they were first introduced. However, I almost immediately stumbled upon a concept I am not familiar with, which is crucial to understanding the motivation for the rest of the paper. The setting is as follows: Let $(X,T)$ be a dynamical system, where $X$ is a compact metric space $(X,d)$ , and $T$ is a minimal covering transformation. By this we (read Keane) mean that: $T$ is everywhere $n$ -to-1 ( $n\geq 2$ ). $T$ is a local homeomorphism. There exists a $C>1$ , such that for all $x\in X$ there exists a $\delta_x>0$ , such that $d(x,y)<\delta_x\implies d(Tx,Ty)\geq Cd(x,y)$ . For all $\varepsilon>0$ there exists a $N\in\mathbb N$ , such that $T^{-N}(x)$ is $\varepsilon$ dense in $X$ , for all $x$ . Now comes the concept I am not familiar with: Keane states that, for any measure $\mu$ on $X$ we can denote the measure on $X$ ""obtained by lifting $\mu$ locally by $T^{-1}$ by $Q\mu$ "". What exactly is this $Q\mu$ ? My first thought was that $Q\mu$ was simply the pushforward $T_*\mu$ (defined by $T_\mu(A)=\mu(T^{-1}(A)))$ , which agrees with his next line: ""The total mass of $Q\mu$ is $n$ times that of $\mu$ "". However this is a global construction, and so does not lend itself to the use of the word ""local"". My other thought is that $T$ being a minimal covering transformation allows us to construct a partition $X=\bigsqcup_{i=1}^nX_i$ , where for each $i$ , $T|_{X_i}$ is a homeomorphism. Let us denote each such restricted function by $T_i$ . We can then define $Q\mu$ by $$Q\mu(A)=\sum_{i=1}^n\mu(T_i(A\cap X_i)).$$ This is most probably my best guess as to how $Q\mu$ is supposed to be defined, but I haven't proven rigorously yet that $T$ being a covering transformation allows such a decomposition (but it seems entirely natural that it should). I will be very appreciative if someone can definitively tell me what the correct definition of $Q\mu$ should be. [1] Keane, Michael , Strongly mixing g-measures , Invent. Math. 16, 309-324 (1972). ZBL0241.28014 .","['general-topology', 'definition', 'measure-theory', 'dynamical-systems']"
3224846,"In an irregular grid comprised of squares, how might we find the most efficient way to remove squares such that there is no 2x2 square possible?","Normally in a square grid such as a 5x5 grid the way to accomplish this would to be to remove only 4 squares such that no two touch the wall of the grid, nor do they touch one another This has a very similar arrangement even when you reduce the size of the overall grid by 1 row and/or 1 column What I'm trying to figure out is how few squares I could remove from an irregular shape or one with squares already removed in order to stop any 2x2 square from being possible.","['optimization', 'combinatorics', 'geometry']"
3224861,A proof of Faa di Bruno's Formula,"I am writing a proof of Calculus which uses Faa di Bruno's formula to show that the composition of two analytic functions is analytic. I tried to prove the result with induction. The base case was easy, but I got stuck in the inductive step. This is my work for the inductive step: Define $h(x)=f(g(x))$ . Then $$\begin{align} h^{(n+1)}(x)&=\left(h^{(n)}(x)\right)'\\\\
&=\left(\sum\limits_{1k_1+2k_2+\cdots+nk_n=n}\frac{n!}{k_1!\cdots k_n!}f^{(k_1+\cdots+k_n)}(g(x))\prod\limits_{j=1}^n\left(\frac{g^{(j)}(x)}{j!}\right)^{k_j}\right)'\\\\
&=\sum\limits_{1k_1+2k_2+\cdots+nk_n=n}\frac{n!}{k_1!\cdots k_n!}\left(f^{(k_1+\cdots+k_n)}(g(x))\prod\limits_{j=1}^n\left(\frac{g^{(j)}(x)}{j!}\right)^{k_j}\right)'\\\\
        &=\sum\limits_{1k_1+2k_2+\cdots+nk_n=n}\frac{n!}{k_1!\cdots k_n!}\left[\left(f^{(k_1+\cdots+k_n)}(g(x))\right)'\left(\prod\limits_{j=1}^n\left(\frac{g^{(j)}(x)}{j!}\right)^{k_j}\right)\right.\\
&\left.\qquad\qquad+\left(\prod\limits_{j=1}^n\left(\frac{g^{(j)}(x)}{j!}\right)^{k_j}\right)'\left(f^{(k_1+\cdots+k_n)}(g(x))\right)\right]\\\\
&=\sum\limits_{1k_1+2k_2+\cdots+nk_n=n}\frac{n!}{k_1!\cdots k_n!}\left[\left(f^{(k_1+\cdots+k_n+1)}(g(x))g'(x)\right)\left(\prod\limits_{j=1}^n\left(\frac{g^{(j)}(x)}{j!}\right)^{k_j}\right)\right.\\\\
&\left.\qquad\qquad+\left(\sum\limits_{k=1}^{n} \left(\left(\frac{g^{(k)}(x)}{k!}\right)^{k_k}\right)'\prod\limits_{\substack{j=1\\j\ne k}}^{n} \left(\frac{g^{(j)}(x)}{j!}\right)^{k_j}\right)\left(f^{(k_1+\cdots+k_n)}(g(x))\right)\right]\\\\ 
&=\sum\limits_{1k_1+2k_2+\cdots+nk_n=n}\frac{n!}{k_1!\cdots k_n!}\left[\left(f^{(k_1+\cdots+k_n+1)}(g(x))g'(x)\right)\left(\prod\limits_{j=1}^n\left(\frac{g^{(j)}(x)}{j!}\right)^{k_j}\right)\right.\\\\
&\left.\qquad\qquad+\left(\sum\limits_{k=1}^{n} k_k\left(\left(\frac{g^{(k)}(x)}{k!}\right)^{k_k-1}\left(\frac{g^{(k+1)}(x)}{k!}\right)\right)\prod\limits_{\substack{j=1\\j\ne k}}^{n} \left(\frac{g^{(j)}(x)}{j!}\right)^{k_j}\right)\left(f^{(k_1+\cdots+k_n)}(g(x))\right)\right]\\
&=\sum\limits_{1k_1+2k_2+\cdots+nk_n=n}\frac{n!}{k_1!\cdots k_n!}\left[\left(f^{(k_1+\cdots+k_n+1)}(g(x))g'(x)\right)\left(\prod\limits_{j=1}^n\left(\frac{g^{(j)}(x)}{j!}\right)^{k_j}\right)\right.\\
&\left.\qquad\qquad+\left(\sum\limits_{k=1}^{n} k_k\left(\frac{g^{(k)}(x)}{k!}\right)^{k_k-1}\left(\frac{g^{(k+1)}(x)}{k!}\right)\prod\limits_{\substack{j=1\\j\ne k}}^{n} \left(\frac{g^{(j)}(x)}{j!}\right)^{k_j}\right)\left(f^{(k_1+\cdots+k_n)}(g(x))\right)\right]\\
\end{align}$$ I eventually want to simplify this expression to $$\sum\limits_{1k_1+2k_2+\cdots+nk_n+(n+1)k_{n+1}=n+1}\frac{(n+1)!}{k_1!\cdots k_n!k_{n+1}!}f^{(k_1+\cdots+k_n+k_{n+1})}(g(x))\prod\limits_{j=1}^{n+1}\left(\frac{g^{(j)}(x)}{j!}\right)^{k_j}$$ I noticed that the first term of the right hand side of my last step is equivalent to that simplified sum under the restriction that $k_1\ge 1$ except it is missing a factor of $n+1$ . The $nth$ term of the sum from $j=1$ to $n$ on the right hand side exactly matches the term on the simplified version where $k_{n+1}=1$ and the other $k_j's$ are $0$ . The remaining terms of the sum (i.e. the terms for $j=1$ to $j=n-1$ ) correspond to a relative increase in $k_{j+1}$ by $1$ and relative decrease in $k_j$ by $1$ , with the exception of a missing factor of $\frac{n+1}{(k+1)k_k}$ . I am not sure how to address these missing factors and particuticularly how to rewrite the terms from $j=1$ to $n-1$ of the sum of the right hand side. I tested my work for $n=2$ and the two expressions were equivalent. This makes me think my work is correct so far I just don't know how to proceed. Thanks, Andrew Murdza","['derivatives', 'chain-rule', 'real-analysis']"
3224872,"Show that $\min\{X_{1},X_{2},\ldots,X_{n}\}$ is sufficient for $\mu$ when $\sigma$ is fixed","Let $X_{1},X_{2},\ldots,X_{n}$ be a sample from a population with density $p(x,\theta)$ given by \begin{align*}
p(x,\theta) = \frac{1}{\sigma}\exp\left\{-\left(\frac{x-\mu}{\sigma}\right)\right\}
\end{align*} if $x\geq \mu$ and $0$ otherwise. Here $\theta = (\mu,\sigma)$ with $-\infty < \mu < \infty$ and $\sigma > 0$ . (a) Show that $\min\{X_{1},X_{2},\ldots,X_{n}\}$ is sufficient for $\mu$ when $\sigma$ is fixed. (b) Find a one-dimensional sufficient statistic for $\sigma$ when $\mu$ is fixed. (c) Exhibit a two-dimensional sufficient statistic for $\theta$ . MY ATTEMPT (b) In the first place, let us determine the maximum likelihood function for this sample considering that $\mu$ is fixed: \begin{align*}
L(\textbf{x},\theta) = \prod_{i=1}^{n}\frac{1}{\sigma}\exp\left\{-\left(\frac{x_{i}-\mu}{\sigma}\right)\right\} = \frac{1}{\sigma^{n}}\exp\left\{-\frac{1}{\sigma}\left(\sum_{i=1}^{n}x_{i} - n\mu\right)\right\}
\end{align*} In this case, we can factor $L(\textbf{x},\theta) = h(x)g_{\sigma}(T(\textbf{x}))$ , where \begin{align*}
h(x) = 1\quad\text{and}\quad g_{\sigma}(T(\textbf{x})) = \frac{1}{\sigma^{n}}\exp\left\{-\frac{1}{\sigma}\left(\sum_{i=1}^{n}x_{i} - n\mu\right)\right\}
\end{align*} Therefore the statistic $T(\textbf{x}) = \sum_{i=1}^{n}x_{i}$ is sufficient for $\sigma$ . But I do not know if this is right neither do I know how to approach the other two items. Can somebody help me out? Thanks in advance!","['self-learning', 'statistics', 'statistical-inference', 'maximum-likelihood']"
3224889,Sub-gaussian norm of sample mean?,"Suppose $X$ is a mean-zero random variable with subgaussian norm $$
k = \|X\|_{\Psi_2}:= \inf\{t>0: \mathbb{E} \exp(X^2/t^2)\leq 2\}.
$$ What can we say about the sub-gaussian norm of $m$ iid samples $X_1, \dots, X_m$ from the same distribution as $m$ ? I believe that $$
\bigg\| \frac{1}{m}\sum_{i=1}^{m}X_{i}\bigg\|_{\Psi_2}\leq c \frac{k}{\sqrt{m}}
$$ for some absolute constant $c>0$ .",['probability-theory']
