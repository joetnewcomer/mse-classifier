question_id,title,body,tags
4611075,"Solving $\tan\beta\sin\gamma-\tan\alpha\sec\beta\cos\gamma=b/a$, $\tan\alpha\tan\beta\sin\gamma+\sec\beta\cos\gamma=c/a$ for $\beta$ and $\gamma$","I am trying to solve the following tricky system of two trigonometric equations. $\alpha, a, b, c$ are all given (and $a$ is nonzero) so I am trying to solve for $\beta$ and $\gamma$ . $$\tan{\beta}\sin{\gamma}-\tan{\alpha}\sec{\beta}\cos{\gamma}=\frac{b}{a}$$ $$\tan{\alpha}\tan{\beta}\sin{\gamma}+\sec{\beta}\cos{\gamma}=\frac{c}{a}$$ If this is too difficult to solve by hand, are there any pieces of software/programming libraries that could help me solve this? I plugged it into woflramalpha and got a horrendous ~34 line equation for $\gamma$ . I am hoping that a cleaner solution exists after some simplification.","['trigonometry', 'systems-of-equations']"
4611108,is this property possible for a polynomial with integer coefficients?,"Found this question about polynomials with integer coefficients in a book about problem solving: Let $\ p(x) $ be a polynomial with integer coefficients. Let $\ a, b, c $ be distinct integers. Is it possible that $\ p(a) = b, p(b) = c, p(c) = a $ ?","['polynomials', 'discrete-mathematics']"
4611123,"Sharp bounds for power towers $x^x,x^{x^{x^x}},x^{x^{x^{x^{x^x}}}},\cdots$","I am looking for a reference to results on sharp upper and lower bounds for the $2n$ th power towers $$x^x,x^{x^{x^x}},x^{x^{x^{x^{x^x}}}},x^{x^{x^{x^{x^{x^{x^x}}}}}},\cdots$$ over the intervals $x\in[0,1]$ and/or $[1,\infty)$ for any natural number $n$ , such that the bounds contain exponent terms stacked no higher than the form $a^b$ . An example would be the inequality — valid for $[0,\infty)$ : $$\frac12x^2+\frac12\le x^{x^{x^{x^{x^x}}}}$$ that has been shown on MSE ; however, this is for a specific $n=3$ . I believe the case $x^x$ with $n=1$ is a bit better established, where we can derive lower bounds using Padé approximants, for instance. Are there any papers that investigate this problem for this set of power towers?","['tetration', 'reference-request', 'upper-lower-bounds', 'real-analysis']"
4611124,Smooth partitions of unity of subsets of $\mathbb{R}^n$ with the same index set as the open cover,"This question is strongly related to this one , but the existing answer doesn't cover the point I will ask about below. The context I am concerned about is the following. Let $S \subseteq \mathbb{R}^n$ be some subset, and $\{U_i\}$ some open cover of $S$ . A smooth partition of unity for $S$ subordinate to the open cover $\{U_i\}$ is a collection $\{\varphi_j\}$ of smooth (i.e. $C^\infty$ ) functions $\varphi_j: V \to [0,1]$ defined on some open neighbourhood $V$ of $S$ , such that: For each $j$ , there exists some $i$ such that $\operatorname{supp} \varphi_j \subseteq U_i$ ; Each $x \in S$ has an open neighbourhood where all but finitely many of the $\varphi_j$ vanish; For all $x \in S$ , $\sum_j \varphi_j(x) = 1$ (this is a finite sum by point 2). The existence of such partitions of unity is proved, for example, in Spivak's Calculus on Manifolds (Theorem 3-11). However, sometimes we like that the partition of unity is indexed by the same set as the open cover. Is it true that we can always modify a partition of unity as defined above such that it has the same index set as the open cover? The first approach one would think of trying is something akin to the one in the linked question: choose a map from the $j$ indexes to the $i$ indexes (respecting condition 1 above) and then sum all the $\varphi_j$ corresponding to the same $i$ . The linked question was concerned about the well-definedness of this sum (it is well-defined, since it is actually a finite sum for each $x$ , and with a bit more care we can see that this gives a smooth function, at least if we construct the partition of unity following Spivak's proof). However, we still need that this sum has support contained in $U_i$ . This is clear in the case where we only have finitely many $j$ corresponding to the same $i$ , but in the infinite case it seems to me that the sum could possibly be nonzero over the whole open $U_i$ . I have a feeling that maybe this difficulty could be avoided by some clever multiplication with a cutoff function which makes the $\varphi_j$ vanish close to the boundary in a careful way, but I couldn't actually make it work.","['multivariable-calculus', 'differential-topology', 'differential-geometry', 'real-analysis']"
4611154,How do I know when I'm done factoring?,"I was attempting to factor $$\frac{3x^3 + x^2}{x^3 + x^2 + x}$$ I gave up and looked up a solution which was $$\frac{x(3x + 1)}{x^2 + x + 1}$$ which allowed me to solve for the limit as $x$ approaches $0$ , which is $0$ . However, at one point when factoring myself I ended up with $$\frac{3x^2 + 1}{x^2 + x + 1}$$ If THIS were factored enough, the limit would be $1$ , but it wasn't. So, I'm just curious how I'm supposed to know that I'm done factoring.","['limits', 'calculus', 'factoring']"
4611157,Definition of a Derivative,"So, I recently decided to major in mathematics, and I take calculus next semester, but I bought a calculus textbook and have been learning a little bit of calculus. I was wondering if I understand this correctly, but the derivative is not the instantaneous rate of change but rather the best constant approximation for the rate of change. Instantaneous means ""no lapse in time"". To me that makes the definition of a derivative nonsensical because you can't measure a rate of change when there is no lapse in time.","['calculus', 'definition', 'derivatives']"
4611168,What is really the TRUE definition of an implicit function?,"First of all I would like to say that I have already found similar questions on stack exchange but somehow my confusion regarding the definition of an implicit function still linger. The title says it all, but here's the question: what is really the TRUE definition of an implicit function? I've scoured the internet and books for definitions and what I've found is that the word implicit function is used/defined in two ways: An implicit function is an equation involving two variables (e.g., x and y) that is possible to solve for y
in terms of x but is sometimes hard/messy/impractical. An example of an implicit function using this definition is $x^2+y^2=1$ . (some sources: MITx, statisticshowto ) Given a relation of the form $f(x,y,)=g(x,y)$ where $f$ and $g$ are functions, an implicit function is any function we get by taking the relation f(x,y,)=g(x,y) and solving for y. An example of an implicit function using this definition would be: $y=+\sqrt{(1-x^2)}$ and $y=-\sqrt{(1-x^2)}$ from the relation $x^2+y^2=1$ . The functions $y=+\sqrt{(1-x^2)}$ and $y=-\sqrt{(1-x^2)}$ are also called the implicit function of the relation $x^2+y^2=1$ . Some implicit functions from a relation can also be impractical/hard/messy to find. (Source: dartmouthpdf , definition clipped from dartmouth pdf ). This is my confusion: on one hand, definition 1. says that an implicit function is the equation $x^2+y^2=1$ . On the other hand, definition 2. says that an implicit function is the function that we get by isolating y from a relation of the form $f(x,y)=g(x,y)$ where $f$ and $g$ are functions (e.g., $y=+\sqrt{(1-x^2)}$ and $y=-\sqrt{(1-x^2)}$ from the relation $x^2+y^2=1$ ) On one hand, some resources refer to the $x^2+y^2=1$ as the implicit function . On the other, some refer to $x^2+y^2=1$ as a relation which has two implicit functions : $y=+\sqrt{(1-x^2)}$ and $y=-\sqrt{(1-x^2)}$ . Thank you for reading my question.","['implicit-function', 'calculus', 'implicit-differentiation', 'derivatives']"
4611205,Proving existence of matrix N such that M = MNM,"Doing practice problems for my qualifying exam and am a bit stumped by the following: Let $n \ge 1$ , $F$ be a field, and let $M$ be a matrix in $M_n(F)$ . Show that there exists an $N \in M_n(F)$ such that $M = MNM$ . Here is my attempt at a solution: We first define the vector space $V = F^n$ on which the matrices in $M_n(F)$ act. Now take $M \in M_n(F)$ and by the rank-nullity theorem we have that $$ker(M) \oplus Im(M) = V$$ Now, first note that if $M$ is invertible, then there exists $N = M^{-1}$ such that $NM = I$ and moreover $$M = MNM$$ giving us the result. We now assume that $M$ is not invertible (ker $(M) \neq 0$ ). We have by the First isomorphism theorem that $$V / ker(M) \cong Im(M)$$ I want to use the above fact with the property that ideals of fields are trivial to give us that $Im(M) = 0$ and therefore $M$ is just the zero map or something to this effect?","['matrices', 'vector-fields', 'abstract-algebra', 'direct-sum']"
4611208,"A stronger version of discrete ""Liouville's theorem""","If a function $f : \mathbb Z\times \mathbb Z \rightarrow \mathbb{R}^{+} $ satisfies the following condition $$\forall x, y \in \mathbb{Z}, f(x,y) = \dfrac{f(x + 1, y)+f(x, y + 1) + f(x - 1, y) +f(x, y - 1)}{4}$$ then is $f$ constant function?","['discrete-mathematics', 'recurrence-relations', 'analysis']"
4611239,Probability that one person is selected and other is not selected in a group,"There are $30000$ students, out of which only $1000$ students are selected. There are two students $A$ and $B$ , what is the probability that $A$ is selected and $B$ is not selected? The solution is given as $\frac{1000}{30000}* \frac{29000}{30000}$ , but this does not seems correct to me. The way I look at this problem, the sample space has $_{30000}C_{1000}$ elements. The event, since we need all the sets from $SS$ that has $A$ but not $B$ , we have $_{29998}C_{999}$ Since we fix $A$ in the set and exclude $B$ Both give very different results, request your help in understanding which one is correct and why, Thanks.","['combinatorics', 'probability']"
4611242,Intuition behind showing that $\{x ∈ \mathbb{R} $ $: f(x) > 0\}$ is countable for $\sum f(x)< \infty$,"Problem In a book on Measure Theory, I encountered the following problem: Let $f : \mathbb{R} → [0,+∞]$ and suppose that $\sum _{x \in \mathbb{R}} f(x) < ∞$ . Show that the set $\{x ∈ \mathbb{R} : f(x) > 0\}$ is countable. The solution is provided, although I find the general direction it takes to be quite unintuitive and unclear in parts too. The beginning of the solution is written below alongside my question below it. Comments Let $M := \sum _{x∈R} f(x) < ∞$ . We claim the set { $x ∈ R : f(x) > \frac{M}{k} $ } has at most k elements, hence it is finite. What is the intuition behind this? When attempting this type of problem, is there a way in which I could have been motivated to make the above conjecture? The proof itself makes sense when I look through the rest of it, but I find it unlikely that I would have stumbled upon the answer myself from this. Is this just a trick or there is something to learn from this example that I can take forward when considering similar problems of this nature? Terminology To clarify for those unsure about the definition used for the sum over the real numbers, we use the following definition: $$ \sum _{x \in \mathbb{R}} f(x) = \sup _{\{\text{finite} \space A \subseteq \mathbb{R}\}} \sum_{x \in A}f(x) $$ (as mentioned in the comments by PrincessEev).","['measure-theory', 'summation', 'proof-explanation', 'functions', 'intuition']"
4611254,Set theories combining ZFC and NFU,"I know very little set theory, but I have heard of NFU before and am wondering whether it can be used as a ""platform"" to ""host"" an arbitrary first-order theory in its urelements, in this case ZFC. I sometimes talk about class-sized things (and frequently make mistakes when doing so), so I am in the market for a set theory that supports multiple levels of classes or something similar like the flattened hierarchy of NFU, even if the resulting classes are less flexible than sets. I'm wondering whether this strawman theory is consistent given that ZFC and NFU are both consistent individually. I'm also curious whether there's a real way to combine these theories together so you have a set theory with ZFC at the bottom rung and more rigid classes on the higher rungs. The alternative set theory NFU can be axiomatized with two axioms and an axiom schema. An axiom of infinity or axiom of choice can also be added without breaking consistency as far as I know. $\varnothing$ does not contain any elements. Any entities that contain at least one element are equal if their elements are equal. Let $\varphi$ be a stratified formula with parameters. There exists a set $X$ such that all entities $e$ are in $X$ if and only if they satisfy $\varphi$ . Superficially, sets in NFU seem very classlike to me. You can have a set of all groups and even a universal set as long as you respect the quasi-type-system when invoking comprehension. This made me wonder whether we can combine NFU and ZFC into a Frankenstein theory where ZFC holds for the urelements. To that end, here is the axiomatization for NFU+ZFC. I introduce two binary relation symbols $\in$ and $E$ . ZFC gets $\in$ and NFU gets $E$ . In prose, $xEy$ is written $x$ is an $E$ -element of $y$ or $y$ $E$ -contains $x$ . I'll define a set as an entity that does not $E$ -contain anything. An entity is defined to be a class if and only if it does not $\in$ -contain anything. An entity is a set and a class if and only if it is $\varnothing$ . Every entity is a set or a class. If a set contains a class $k$ , then $k$ is $\varnothing$ . Any entities that $E$ -contain at least one element are equal if their $E$ -elements are equal. Let $\varphi$ be a stratified formula with parameters, unlike $E$ , both arguments to $\in$ are constrained to have the same type. There exists a class consisting of all entities satisfying $\varphi$ . All the axioms of ZFC with quantifiers replaced in such a way to constrain their bound variables to be sets and all parameters constrained to be sets as well. $\varnothing$ is the empty set in ZFC. So, the both-set-and-class $\varnothing$ is a bit strange, and needing to juggle two elementhood relations is inconvenient, but I'm curious whether this approach of stapling two set theories together is valid in general or has any hidden drawbacks.","['elementary-set-theory', 'logic', 'alternative-set-theories']"
4611291,How many random numbers must be drawn on average to make the sequence fall for the first time?,"Consider a game in which random numbers are constantly selected with equal probability in the interval [0, 1] until the first time the number selected is smaller than the previous one.
Then, how many random numbers need to be drawn on average for this to happen? I saw the answer elsewhere, but it was explained in a way that I had trouble understanding, so have to re-ask here, if anyone could explain in a more understandable way. BTW, this is the thought process I had when handling this question, not sure at which step the error was introduced: Probability of needing to draw 2 times: $\frac{1}{2}$ Probability of needing to draw 3 times: On the basis that the first two is increasing (probability is $\frac{1}{2}$ ), the probability that the third time decreasing is again $\frac{1}{2}$ , so the final probability of needing to draw 3 times is $(\frac{1}{2})^2$ ... Probability of needing to draw n+1 times: $(\frac{1}{2})^n$ Therefore the average number of times needed to draw = $\sum_{n=1}^\infty \frac{n+1}{2^n}=3$","['probability-theory', 'probability', 'sequences-and-series']"
4611295,Five circles in a rectangle: can the circles move?,"Five unit circles are in a rectangle. In the beginning, their centres are the vertices of a regular pentagon, and each circle is tangent to two other circles and one edge of the rectangle. Can the circles move without overlapping? I will post my answer below. I hope to get a more intuitive answer.","['classical-mechanics', 'circles', 'geometry', 'packing-problem', 'intuition']"
4611390,Without calculator prove that $9^{\sqrt{2}} < \sqrt{2}^9$,Without calculator prove that $9^{\sqrt{2}} < \sqrt{2}^9$ . My effort: I tried using the fact $9^{\sqrt{2}}<9^{1.5}=27.$ Also We have $512 <729 \Rightarrow 2^9<27^2 \Rightarrow 2^{\frac{9}{2}}<27 \Rightarrow \sqrt{2}^9=2^{4.5}<27$ . But both are below $27$ .,"['calculus', 'number-comparison', 'algebra-precalculus', 'inequality']"
4611407,Determining the critical points of a two-variable function,"Edit So it seems that, if we assume the function is correct, then the suggested solution is most likely wrong. However, I am inclined to think that the question has been tweaked while the answer remained as it is. Thus, as a challenge to myself, I am trying to figure out the original function, assuming the suggested solution is correct. Just putting this out there too since I’ve already made the post; if anyone wants to try (and succeeds), feel free to pen down what the function could be (either in an answer or a comment) :) Question Find all the critical points of $f$ , where $$f(x, y) = \frac 1 2 x^2 - x + ay(x - 1) - \frac 1 3 y^3 - a^2y^2$$ and $a$ is a constant. My working I know that, for a two-variable function, a critical point is one where the partial derivates with respect to both variables is zero. In our case, we have $$\begin{aligned}
\frac {\partial{f}} {\partial{x}} & = x - 1 + ay\\[2 mm]
& = 0
\end{aligned}$$ and $$\begin{aligned}
\frac {\partial{f}} {\partial{y}} & = a(x - 1) - y^2 - 2a^2y\\[2 mm]
& = 0.
\end{aligned}$$ Now, substituting the first equation into the second gives $$a(-ay) - y^2 -2a^2y = 0$$ which implies that $$y = 0$$ or $$y = -3a^2.$$ Finally, we see that the critical points are $(1, 0)$ and $(1 + 3a^2, -3a^2)$ . Answer The answer, however, is $(0, 0)$ and $(1 - a^3, a^2)$ . Is the suggested answer incorrect? If not, then where have I gone wrong? Any intuitive explanations or suggestions will be greatly appreciated!","['partial-derivative', 'multivariable-calculus', 'calculus', 'derivatives']"
4611435,"Prove a simple module must be ""trivial""","Let $P$ be a finite $p$ -group, i.e. $|P|=p^n$ for some prime $p$ . Let $F$ be a finite field of $p$ elements. Show that every simple $FP$ -module trivial , where $FP$ is a group ring (which is actually a division ring because $F$ is a field). Definition: A trivial $FP$ -module is a one dimensional $F$ -vector space where all elements of $G$ acts as identity. My attempt: Let $M$ be a simple $FP$ -module. I know $M$ is simple if and only if it can be generated by every non-zero element, i.e. $\forall 0\neq\alpha\in M$ , $M=(FP)\alpha$ . If $M$ is a one dimensional $F$ -vector space, then we are done. So, we may assume $M$ is not a one dimensional $F$ -vector space, which implies there exists some $e_G\neq g\in G$ and some $m\in M$ such that $g\cdot m\neq m$ . We want to construct a non-zero proper submodule of $M$ , which will contradict with the simplicity of $M$ and then we are done. To this end, I consider the cyclic module $(FP)(g\cdot m)$ as the condition suggests. And I found that $FP=FP(g^{-1})$ , which leads to $(FP)(g\cdot m)=[FP(g^{-1})](g\cdot m)=FP[(g^{-1})(g\cdot m)]=FP(e_G\cdot m)=FP(m)$ . But then I just get stuck here. I didn't (and didn't know how to) use the conditions that $|P|=p^n$ , $|F|=p$ and $g\cdot m\neq m$ . My question is how to use these conditions to show that $FP(m)$ is a non-zero proper submodule of $M$ ? Or alternatively, can we use other ways to find a a non-zero proper submodule of $M$ ? Thanks for help.","['finite-groups', 'modules', 'abstract-algebra', 'p-groups', 'group-theory']"
4611477,How to simplify $\sin(n\frac{\pi}{2})$ as we do with $\cos(n\pi)$,"When calculating Fourier series we almost always get some expression that look like: \begin{equation} ...\left[\cos(nx) \right]_0^\pi.
\end{equation} Then we use $\cos(n\pi) = (-1)^n$ . However, lately I have come across expressions like this: \begin{equation} ...\left[\sin(nx) \right]_0^{\pi/2}.
\end{equation} Which is equal to: \begin{array}{|c|c|c|c|}
\hline
n& 1 & 2 & 3 & 4 \\ \hline
\sin(nx)&\sin(\frac{\pi}{2}) & \sin(\pi) & \sin(\frac{3\pi}{2})& \sin(2\pi)\\ \hline
 val &1 &0 &-1 &0\\ \hline
\end{array} How do we simplify this so that it looks like the $\cos(n\pi) = (-1)^n$ .",['trigonometry']
4611489,Transitive Relations clarification,"Given set $A = \{1,2,3,4,5\}$ . is it okay to say that {(1,2),(2,3),(1,3)} is transitive? Even though not all the elements of $A$ are not present inside the ordered pairs?","['elementary-set-theory', 'relations']"
4611501,Solving triple integral with cylindrical coordinates,"We are told to evaluate the triple integral: $$\iiint_E z dV$$ where $E$ is bounded by $x=4y^2+4z^2$ and $x=4$ . My attempt: First I noticed that this represents a paraboloid on the x axis so I thought to use cylindrical coordinates (however as the paraboloid was centered around x I wasnt sure whether to let $z=r\cos\theta$ , $y=r\sin\theta$ or the other way around?) $$z=r\cos\theta$$ $$y=r\sin\theta$$ $$0<r<\frac{\sqrt x}{2}$$ $$0<\theta<2\pi$$ $$0<x<4$$ and our integral becomes $$\int_0^4\int_0^{2\pi}\int_0^{\frac{\sqrt x}{2}} r\cos\theta r dr d\theta dx=0$$ However my textbook James Stewart Calculus gives me $\frac{16\pi}{3}$ . Where have I gone wrong?","['integration', 'multivariable-calculus', 'cylindrical-coordinates']"
4611530,particular differential equation [duplicate],"This question already has an answer here : What is the general solution of $\frac{df(x)}{dx} = f(x-a)$? [duplicate] (1 answer) Closed last year . Probably this is a stupid question, but I have a doubt How can we solve the differential equation $y’(t)=y(t-1)$ ? I have no assumption on $y$ , only that it is a function from $\mathbb{R}$ in itself.","['analysis', 'ordinary-differential-equations']"
4611553,Verify my proof: $\cup (\mathcal F \cap \mathcal G) \subseteq (\cup \mathcal F) \cap (\cup \mathcal G) $,"I'm self learning from book ""How to Prove it"" by Velleman (3rd edition). I don't have access to a math professor, so I need a little help from the community. Please verify my proof. Problem 18 pag. 140 Suppose $\mathcal F$ and $\mathcal G$ are families of sets. Prove that $\cup (\mathcal F \cap \mathcal G) \subseteq (\cup   \mathcal F) \cap (\cup \mathcal G) $ Proof . Let's introduce the following notations $A = \cup (\mathcal F \cap \mathcal G) $ and $ B = (\cup   \mathcal F) \cap (\cup \mathcal G) $ . We must prove that $A \subseteq B$ , this means that if we take an arbitrary element $x$ from $A$ it must be also an element from $B$ . $x \in A$ means that there is a set $z$ which is a member of both families of sets $\mathcal F$ and $\mathcal G$ , so $z \in \mathcal F$ and $z \in \mathcal G$ and $x \in z$ . Let's mark this sentence as (1). $x \in B$ means that there is a set $v$ which is a member of $\mathcal F$ and a set $w$ which is a member of $\mathcal G$ so $v \in \mathcal F$ and $w \in \mathcal G$ and $x \in v \cap w$ . Let's mark this sentence as (2). Now observe the difference from (1), $z$ is a member of both $\mathcal F$ and $\mathcal G$ , but $v \in \mathcal F$ and $w \in \mathcal G$ , as a particular case $v=w=z$ . But generally speaking elements from $A$ are a subset of elements from $B$ $\blacksquare $","['elementary-set-theory', 'proof-writing', 'solution-verification']"
4611603,A proof of quadratic reciprocity law.,"I am reading Cox's Primes of the Form $x^2+ny^2$ and solving Exercise 1.13, which depends on Lemma 1.14. Lemma 1.14. If $D\equiv 0, 1\pmod{4}$ is a nonzero integer, then there is a unique homomorphism $\chi:(\mathbb{Z}/D\mathbb{Z})^*\to \{\pm 1\}$ such that $\chi([p])=(D/p)$ for odd primes $p$ not dividing $D$ . Furthermore, $$
\chi([-1])=\left\{
\begin{array}{ll}
1 & \text{when }D>0, \\
-1 & \text{when }D>0 \\
\end{array}
\right.
$$ Exercise 1.13. We will assume that Lemma 1.14 holds for all nonzero integers $D\equiv 0,1 \pmod{4}$ , and we will prove quadratic reciprocity and the supplementary laws. (a) Let $p$ and $q$ be distinct odd primes, and let $q^*=(-1)^{(q-1)/2}q$ . By applying the lemma with $D=q^*$ , show that $(q^*/\cdot)$ induces a homomorphism from $(\mathbb{Z}/q\mathbb{Z})^*$ to $\{\pm 1\}$ . Since $(\cdot /q)$ can be regarded as a homomorphism between the same two groups and $(\mathbb{Z}/q\mathbb{Z})^*$ is cyclic, conclude that the two are equal. My goal is proving that $(\cdot /q)$ and $(q^*/\cdot)$ are not trivial. That is, there exists $a, b\in (\mathbb{Z}/q\mathbb{Z})^*$ such that $(a/q)=(q^*/b)=-1$ . I have no problem with $(\cdot /q)$ . Suppose that $(\mathbb{Z}/q\mathbb{Z})^*=\langle g\rangle$ . I am trying to show that $(q^*/g)=-1$ . I have proven the case $q\equiv 1\pmod{4}$ , but my argument uses the Generalized Quadratic Reciprocity Law, which I am not supposed to use it because this exercise is a proof of that law. Here is my argument: since $(q/g)(g/q)=(-1)^{(q-1)/2 \cdot (g-1)/2}=1$ , $(q/g)$ and $(g/q)$ have the same sign. Since $(g/q)=g^{(q-1)/2}=-1 \pmod{q}$ , we have $(g/q)=-1$ .
I am stuck here.","['number-theory', 'algebraic-number-theory', 'elementary-number-theory', 'quadratic-reciprocity']"
4611613,Fubini's Theorem solving an integral,"I've been given the following integral $\displaystyle \underset{0}{\overset{1}{\int}}\underset{0}{\overset{1}{\int}} \frac{y}{\left(1+x^{2}+y^{2}\right)^{\frac{3}{2}}}dxdy$ And the topic of the exercise is using Fubini's theorem to solve it, I was able to reach a strange integral after swapping the integration order but I have a feeling I'm missing something. Does anyone have an approach I should use instead of brute-forcing it? This is the outcome of the swap : $ \displaystyle \underset{0}{\overset{1}{\int}}\frac{1}{\sqrt{1+x^{2}}}-\frac{1}{\sqrt{2+x^{2}}}dx$ EDIT: So after all this help, I was able to finish the integral properly, my main take on this is to remember the quick substitution which really come in handy in cases like this.
Im adding my final answer if someone ever was wondering : $\displaystyle \underset{0}{\overset{1}{\int}}\frac{1}{\sqrt{1+x^{2}}}dx-\underset{0}{\overset{1}{\int}}\frac{1}{\sqrt{2+x^{2}}}dx=\ln\left(\sqrt{2}+1\right)-\ln\left(\frac{\sqrt{3}+1}{\sqrt{2}}\right)=\ln\left(\frac{\sqrt{2}+1}{\frac{\sqrt{3}+1}{\sqrt{2}}}\right)=\ln\left(\frac{\sqrt{2}+2}{\sqrt{3}+1}\right)$","['integration', 'multivariable-calculus', 'definite-integrals', 'fubini-tonelli-theorems']"
4611635,Evaluate the limit $\lim_{n \rightarrow \infty}\sum_{k=1}^n\frac{1}{x_{1}^{2}x_{2}^2...x_{k}^2}$,"We have the sequences $(x_{n})_{n\geq1}$$(y_{n})_{n\geq1}$ with positive real numbers. $x_{1}=\sqrt{2}$ , $y_{1}=1$ and $y_{n}=y_{n-1}\cdot x_{n}^{2}-3$ for every $n\geq2$ . We know the sequence $(y_{n})_{n\geq1}$ is bounded. Find $$\lim_{n \rightarrow \infty}\sum_{k=1}^n\frac{1}{x_{1}^{2}x_{2}^2...x_{k}^2}$$ I tried rewriting $y_{n}$ only with terms from $(x_{n})_{n\geq1}$ but we get an ugly formula. I don't know exactly how we can we use the fact that $(y_{n})_{n\geq1}$ is bounded.","['limits', 'inequality', 'convergence-divergence', 'sequences-and-series']"
4611644,Are all discrete harmonic function $f:\mathbb Z^2\to \mathbb R$ with less than exponential growth polynomials?,"Is the following conjecture true? Let $f:\mathbb Z^2\to \mathbb R$ be a discrete harmonic function. If $f$ has less than exponential growth then it is a polynomial. Definitions: Growth of $f$ is less then exponential if for every $a>0$ it is that $$
  \lim_{\|x\|\to \infty} e^{-a\|x\|} f(x) = 0.
$$ A function $f:\mathbb Z^2\to \mathbb R$ is discrete harmonic iff $$
f(x) = \dfrac{f(x_1 + 1, x_2)+f(x_1, x_2 + 1) + f(x_1 - 1, x_2) +f(x_1, x_2 - 1)}{4}.
$$ Context: Given a discrete harmonic function $f:\mathbb Z^2\to \mathbb R$ , it is known that if $f$ is bounded, then it is constant: A stronger version of discrete ""Liouville's theorem"" if the gradient of $f$ is bounded, then $f$ is linear: Harmonic functions on $\mathbf{Z}^2$ analogously: if all the $k$ -th gradients of $f$ are bounded, then $f$ is a polynomial of at most order $k$ . Does the conjecture follow?","['random-walk', 'exponential-function', 'harmonic-functions', 'discrete-mathematics']"
4611681,Show by contradiction that a martingale converge to zero almost surely.,"Theorem 5.2.9. If $X_n \geq 0$ is a supermartingale then as $n \rightarrow \infty, X_n \rightarrow X$ a.s. and $E X \leq E X_0$ 5.2.9. Let $Y_1, Y_2, \ldots$ be nonnegative i.i.d. random variables with $E Y_m=1$ and $P\left(Y_m=1\right)<1$ . (i) Show that $X_n=\prod_{m \leq n} Y_m$ defines a martingale. (ii) Use Theorem $5.2 .9$ and an argument by contradiction to show $X_n
   \rightarrow 0$ a.s. I am trying to show that $X_n \to 0$ a.s., and I want to try to contradict the fact that $X_n$ is a martingale and must not be a supermartingale by showing the inequality is actually strict. To contradict the statement, my idea is to assume there exists a set $A$ of positive probability on which $\lim X_n $ does not converge to zero, and derive the contradiction from this. However, all I would obtain is $0<E[X]\leq E[X_0]$ however $E[X_0]=E[Y_1]=1$ which I am having trouble realizing as a contradcition. Thank you","['stochastic-processes', 'martingales', 'probability-theory', 'probability']"
4611682,Prove $f(z) = cz$ for all complex numbers and some $c$,"Suppose $f$ is entire and $|f(z)|\geqslant |z|$ $\forall z \in \mathbb{C}$ , prove there exists $c \in \mathbb{C} $ such that $f(z) = cz $ $\forall z \in \mathbb{C}$ . I want to use Liouville’s theorem for $\frac{z}{f(z)}$ but I don’t know what to do when $z = 0$ and $f(z) \neq 0$ .","['complex-analysis', 'constants']"
4611727,"A ""développement limité"" for function of several variables at the order $2$","Starting from the Taylor theorem of order 2 for functions of several variables (a proof is tried here A proof of multivariable Taylor theorem of order 2 ) I would like to find what we call in french ""un developpement limité d'ordre 2"", I state this now : Consider a function $f:A\subset\mathbb{R}^n\to\mathbb{R}$ that is $C^{2}$ on $N(X_0, r)\in A$ . Then we have $f(X_0 + Y) = f(X_0) + Df(X_0)Y + \frac{1}{2}Y^{t}D^{2}f(X_0)Y + \epsilon(X_0, Y)\lVert Y\rVert^{2}$ with $\lim_{Y\to 0_{n}}\epsilon(X_0, Y) = 0$ . Thus we can put rewrite this as $f(X_0 + Y) = f(X_0) + Df(X_0)Y + \frac{1}{2}Y^{t}D^{2}f(X_0)Y + o\left(\lVert Y\rVert^{2}\right)$ for $Y$ in a neighborhood of $0_n$ . The idea of the proof is the following : If $Y = 0_{n}$ we consider $\epsilon_{0}(X_0,Y) = 0$ which proves the result above. Otherwise (i.e. $\lVert Y\rVert\leq r$ ), consider $\epsilon(X_0,Y) = \frac{1}{\lVert Y\rVert^{2}}\left(f(X_0+Y) - f(X_0) - Df(X_0)Y - \frac{1}{2}Y^{t}D^{2}f(X_0)Y\right)$ The proof ends if we show that $\lim_{Y\to 0_{n}}\epsilon(X_0, Y) = 0$ . Since $f$ is $C^{2}$ in the neighborhood of $X_0$ we can use Taylor theorem of order $2$ on $f(X_0 + Y)$ to get $\epsilon(X_0,Y) = \frac{1}{\lVert Y\rVert^{2}}\left(f(X_0) + Df(X_0)Y + \frac{1}{2}Y^{t}D^{2}f(X_0+\theta Y)Y - f(X_0) - Df(X_0)Y - \frac{1}{2}Y^{t}D^{2}f(X_0)Y\right)$ $=\frac{1}{2\lVert Y\rVert^{2}}\left(Y^{t}D^{2}f(X_0+\theta Y)Y - Y^{t}D^{2}f(X_0)Y\right) = \frac{1}{2\lVert Y\rVert^{2}}Y^{t}\left(D^{2}f(X_0+\theta Y) - D^{2}f(X_0)\right)Y $ $ =\sum_{i=1}^{n}\sum_{j=1}^{n}\frac{y_i y_j}{2\lVert Y\rVert^{2}}\left(f_{x_i x_j}^{''}(X_0 + \theta Y) - f_{x_i x_j}^{''}(X_0)\right)$ It follows that (by the triangle inequality) $\lvert\epsilon(X_0, Y)\rvert\leq\sum_{i=1}^{n}\sum_{j=1}^{n}\left\rvert f_{x_i x_j}^{''}(X_0 + \theta Y) - f_{x_i x_j}^{''}(X_0)\right\rvert$ which shows that $\lim_{Y\to 0_{n}}\epsilon(X_0, Y) = 0$ using the continuity of the second order partial derivatives at $X_0$ This seems correct to you ? And do you an elegant way to extend this at the order $p$ ? I thought of an induction, thank you a lot !","['approximation', 'multivariable-calculus', 'taylor-expansion', 'analysis']"
4611744,What is the difference between $y$ and $y(x)$?,"When you have an equation such as $y=2x$ and you want to rewrite it using function notation, it is conventional to define the function using another letter ( $f$ for example). Why is the function not defined using the same letter? Is it because there would be ambiguity about whether you are referring to the coordinate or the function when using that letter?",['functions']
4611815,Geodesic curvature on hyperbolic manifold with boundary,Let $\Sigma$ be a compact oriented surface of genus $1$ having a single boundary component (i.e. $T^2$ minus an open disk) and let $g$ be a Riemannian metric on $\Sigma$ with constant Gaussian curvature $K=-1$ . Is it necessarily the case that $\int_{\partial \Sigma} k_gds \leq 0$ ? The Gauss-Bonnet theorem gives a lower bound $\int_{\partial \Sigma} k_gds  = \text{Vol}(\Sigma)-2\pi \geq -2\pi $ but I am wondering about an upper bound.,"['hyperbolic-geometry', 'riemannian-geometry', 'differential-geometry']"
4611855,Solving $x=\frac{2^{1+y}}{\left(y+1\right)\left(y+2\right)}$ for $y$,"Do you know how to solve the following equation to make $y$ the subject: $$x=\frac{2^{1+y}}{\left(y+1\right)\left(y+2\right)}$$ My attempts:
I multiplied both sides by $\ln(2)(y+1)^2$ to get $$x\ln\left(2\right)\left(y+1\right)^{2}\left(y+2\right)=\ln\left(2\right)\left(y+1\right)e^{\ln\left(2\right)\left(y+1\right)}$$ The reasoning behind this was to get an expression of the form $xe^x$ to use the Lambert W function. However I was unable to make it work. I also tried considering the more general function $$x=\frac{z^{1+y}}{\left(y+1\right)\left(y+2\right)}$$ Differentiating both sides with respect to z cancels the $(y+1)$ term in the denominator which I thought could make it easier to work with. However again I was unable to find a solution. Any help is appreciated. Thanks",['functions']
4611891,Expectation of the maximum of independent poisson random variables,"I am trying to prove the following estimation of the expectation of the maximum of independent poisson random variables. I've become interested in this problem while reading Joel A. Tropp's "" An Introduction to Matrix Concentration Inequalities "" (Section 5.1.2). The following estimation is used as a key step to show the optimality of the matrix Chernoff bound. I also believe this estimation is also useful in explaining sharpness of the matrix Bernstein inequality. Desired estimation: Let $X_i, i = 1, 2, \dots, n$ be independent Possion(1)
random variables. Then, $$
\mathbb{E}\max_{1 \leq i\leq n}X_i \approx C\frac{\log n}{\log \log n}, 
$$ where $C$ is some constant. Tropp used this fact without giving any proof or reference. But, after working on this for a while, it doesn't seem something trivial to me. After searching for a while, I found that the desired result seems to follow from the results by A.C.Kimber and C.W.Anderson . However, I think it would be nicer if we can find more direct or an elementary way to obtain the desired estimation. Any advice, suggestions or references would be very much appreciated!","['order-statistics', 'probability-theory', 'poisson-distribution']"
4611896,Prove a binomial identity: $\sum_{i=1}^n i \binom{2n}{n-i}=\frac12(n+1) \binom{2n}{n-1}$,"I want to prove the product of even/odd power with a combinatorial number: \begin{aligned}
  \sum_{i=1}^n i \binom{2n}{n-i}&=\frac12(n+1) \binom{2n}{n-1},  \\
  \sum_{i=1}^n i^2 \binom{2n}{n-i}&=2^{2n-2} n.
\end{aligned} I am sure these results are correct since WolframAlpha verifies them. I wonder if the first formula can be proved, and also the general version can be proved: Formula related to combinatorial number . Below is proof for the second formula: First, note that $r\binom{n}{r}=n\binom{n-1}{r-1}$ , and $\sum_{i=0}^n\binom{n}{i}=2^n$ . We thus have \begin{aligned}
    \sum_{i=0}^n i\binom{n}{i}&=2^{n-1}n,  \\
    \sum_{i=0}^n i^2\binom{n}{i}
    &= \sum_{i=0}^n i(i-1)\binom{n}{i}+\sum_{i=0}^n i\binom{n}{i} \\
    &= 2^{n-2}(n^2-n) + 2^{n-1} n  \\
    &= n(n+1)2^{n-2}. 
\end{aligned} Therefore, \begin{aligned}
    \sum_{i=0}^{2n}(n-i)^2\binom{2n}{i}
    &= n^2\sum_{i=0}^{2n}\binom{2n}{i}-2n\sum_{i=0}^{2n}i\binom{2n}{i}+\sum_{i=0}^{2n}i^2 \binom{2n}{i}  \\
    &= 2^{2n}n^2-2^{2n+1}n^2+n(2n+1)2^{2n-1}  \\
    &= 2^{2n-1} n,
\end{aligned} and thus \begin{aligned}
    \sum_{i=0}^n i^2 \binom{2n}{n-i} 
    = \sum_{i=0}^n (n-i)^2 \binom{2n}{i} 
    = \frac12 \sum_{i=0}^{2n} (n-i)^2 \binom{2n}{i}
    = 2^{2n-2} n.
\end{aligned}","['summation', 'binomial-coefficients', 'combinatorics', 'binomial-theorem']"
4611907,What's the smallest set that covers $\mathbb{B}^n$ with at most one XOR operation?,"More specifically, what's the smallest set of $n$ -bit binary numbers, $X\subset \mathbb{B}^n$ , such that every $n$ -bit binary number can be obtained from $X$ by at most one XOR operation. Equivalently, the smallest set $X\subset \mathbb{F}_2^n$ such that for any $y\in \mathbb{F}_2^n$ , either $y\in X$ or there exists $x_1, x_2\in X$ such that $y=x_1+x_2$ . For $n=4$ , the set $$X=\{1000, 0100, 0010, 0001, 1111\}$$ is smallest. I'd like to know what the smallest such $X$ is for $n=10$ . For my own attempts, I tried at least to bound the size of $X$ from below. Since if $|X|=k$ , then we can get at most $k+\binom{k}{2}=\frac{k^2+k}{2}$ distinct values from at most one XOR operation (or any binary operation for that matter). And since these values must cover all of $\mathbb{B}^n$ , there must be at least $2^n$ of them. Well, $2^n-1$ actually; this is being used for a data encoding experiment and we don't have to bother about generating $\vec{0}$ from $X$ . Working through the scratch, I found that $$k\ge\frac{-1+\sqrt{2^{n+3}-7}}{2}$$ which tells us that $k\ge 5$ for $n=4$ and thus the set above is smallest. For $n=5$ the corresponding bound is $|X|=k\ge 8$ but with a brute-force search, the only solution I found was with $|X|=9$ . In particular, $$X=\{10000, 01000, 00100, 00010, 00001, 11111, 11000, 10100, 01100\}$$ For $n=10$ , the case I'm interested in, the bound is $|X|\ge 45$ but computer searches only yielded a solution with $|X|=65$ . So I'm wondering, what can I do to find such sets other than brute search? One attempt that stumped me was to start by duplicating some of the binary numbers from the $n=5$ solution as follows: $$1000010000, 0100010000, 0010010000, 0001010000, 0000110000, 1111110000,$$ $$1000001000, 0100001000, 0010001000, 0001001000, 0000101000, 1111101000,$$ $$...$$ $$1000011111, 0100011111, 0010011111, 0001011111, 0000111111, 1111111111.$$ So this gives us a starting point of 36 binary which carry over some of the properties of the $n=5$ solution. But even these have some redundancy that the smaller solution didn't, like $$1000010000+0100001000=1000001000+0100010000=1100011000$$ And obviously, if we're trying to get as many distinct numbers as possible out of $X$ , we want to minimize pairs like this with the same sum. But even then, I wasn't sure how to add more elements to $X$ here. Another thought I had was to start with 11 elements in $X$ : $$1000000000, 0100000000, 0010000000, ..., 0000000010, 0000000001, 1111111111$$ This would take care of all 1-token, 2-token, 9-token, and 10-token binary numbers (as I've been calling them). My hope from here was that 4-token numbers could be added in strategically. They could generate all 8-token codes codes by pairing with each other, all 6 token codes by pairing with $1111111111$ , all 3-token and 5-token codes by pairing with the 1-token codes. But this would still miss 7-token codes entirely. I don't need the optimal solution; even knocking that $|X|=65$ down to a 60 or a 55 would be great.","['binary', 'combinatorics', 'extremal-combinatorics']"
4611994,"Measurable set in a square, and containing a Cartesian square","Let $A\subseteq [0,1]^2$ be a measurable subset with full measure, $\mu(A)=1$ . For $X\subseteq [0,1]$ , $D(X)$ denotes the diagonal of $X$ , i.e. $D(X)=\lbrace (x,x) \ | \ x \in X \rbrace$ . Question. Must there always be an $X$ with positive measure such that $(X \times X) \setminus D(X) \subseteq A$ ? My thoughts : if $A_1=(0,\frac{1}{2}) \times (\frac{1}{2},1)$ , $A_2=(\frac{1}{2},1) \times (0,\frac{1}{2})$ and $A=A_1\cup A_2$ , then $A$ has measure $\frac{1}{2}$ , and there is no $X$ containing more than two elements such that $X\times X \setminus D(X) \subseteq A$ . I tried to find similar examples with $\mu(A)\gt \frac{1}{2}$ but failed. This leads me to ask the following : Second question. Let $A$ be as above ( $A\subseteq [0,1]^2$ with $\mu(A)=1$ ). Must there always be an $X$ containing at least three elements such that $(X \times X) \setminus D(X) \subseteq A$ ? Update According to the answer linked in PhoemueX's comment below, the answer to the first question is no if $\mu(A)=1$ is replaced with $\mu(A)=1-\varepsilon$ for any $\varepsilon \gt 0$ . Indeed, this answer produces an $E\subseteq [0,1]^2$ with $\mu(E)=1-\varepsilon$ , such that $E$ contains no ""measurable rectangles"", i.e. $E$ contains no $Y\times Z$ where $Y,Z$ have positive measure. Then we may take $A=E \cup D([0,1])$ .","['measure-theory', 'lebesgue-measure', 'real-analysis']"
4611997,How to prove $M = N$ is equivalent to $M =_{\beta} N$ in Lambda Calculus,"Lambda calculus is equipped with a primitive $=$ with the following definition: (1) For any variable $x$ and lambda term $M, N$ , $\left(\lambda x. M\right) N = M\left[x := N\right]$ ; (2) For any term $M$ , $M = M$ ; (3) For any terms $M, N$ , $M = N$ implies $N = M$ ; (4) For any terms $M, N, L$ , $M = N$ and $N = L$ implies $M = L$ . On the other hand, we have the following definition for $=_{\beta}$ ( $\beta$ -conversion ): For any terms $M, N$ , $M =_{\beta} N$ if and only if for some $n \geq 0$ , there exists some sequence $s$ of lambda terms of length $n + 1$ such that $s_{0} \equiv M$ and $s_{n} \equiv N$ and for any $0 \leq i < n$ , $s_{i} \rightarrow_{\beta} s_{i + 1}$ or $s_{i + 1} \rightarrow_{\beta} s_{i}$ , where $\rightarrow_{\beta}$ is one-step beta reduction. I would like to prove that $=$ and $=_{\beta}$ are equivalent. To prove that $M =_{\beta} N$ implies $M = N$ , we can first prove the following lemma: for any $n \geq 0$ , if there exists some sequence $s$ of lambda terms of length $n + 1$ such that $s_{0} \equiv M$ and $s_{n} \equiv N$ and for any $0 \leq i < n$ , $s_{i} \rightarrow_{\beta} s_{i + 1}$ or $s_{i + 1} \rightarrow_{\beta} s_{i}$ , then $M = N$ . This is not hard to prove from mathematical induction using properties of $\rightarrow_{\beta}$ . However, I haven't found a good way to prove the inverse direction: $M = N$ implies that for some $n \geq 0$ , there exists some sequence $s$ of lambda terms of length $n + 1$ such that $s_{0} \equiv M$ and $s_{n} \equiv N$ and for any $0 \leq i < n$ , $s_{i} \rightarrow_{\beta} s_{i + 1}$ or $s_{i + 1} \rightarrow_{\beta} s_{i}$ . It seems to me that from $M = N$ , I cannot infer anything that helps me proceed. I tried proving in the other direction: if there is no $n \geq 0$ such that there exists some sequence $s$ of lambda terms of length $n + 1$ such that $s_{0} \equiv M$ and $s_{n} \equiv N$ and for any $0 \leq i < n$ , $s_{i} \rightarrow_{\beta} s_{i + 1}$ or $s_{i + 1} \rightarrow_{\beta} s_{i}$ , then $M \neq N$ . Then the problem is, our definition defines $M = N$ , but does not indicate much about $M \neq N$ . Does anyone have any clue about this proof?","['lambda-calculus', 'induction', 'discrete-mathematics']"
4612015,function with zero derivatives at certain points,"Let $f:\mathbb{R}_{\ge0}\rightarrow \mathbb{R}$ be a $\mathcal{C}^{\infty}$ function such that $\lim_{x\rightarrow \infty}f(x) = 0$ . If $f(0)=0$ , I know how to show that $f'(c)=0$ for some $c$ , and this is intuitively clear. My question is : Is it possible to find a sequence $x_n$ such that $f^{(n)}(x_n)=0$ for every $n$ ? It looks like a very strong statement, but I feel like once it is true for $f'$ it is also true for all the other derivatives of $f$ at some point since we can just ""repeat"" by induction. I am not able to construct a proof though...","['derivatives', 'analysis']"
4612059,Can $\frac{1}{1-x^2}$ be integrated by parts?,"I was watching a video of a professor who said that $\int \frac{1}{1-x^2} dx$ can be done using the method of integration by parts. Though he skipped the solution and simply gave the answer, I tried solving it myself but I'm having difficulty moving forward. What I did was: Let $u = \frac{1}{1-x^2}, du = \frac{2x}{(1-x^2)^2} dx , dv = dx, v=x$ . Then, $$\int \frac{dx}{1-x^2} = \frac{x}{1-x^2} - \int \frac{2x^2}{(1-x^2)^2} dx$$ However, the new integral seems problematic. I tried doing the method again but the equation just ends up to be $0=0$ . Is integration by parts not a viable way of solving this integral? P.S.: I already know the answer using a different method (partial fraction decomposition). I just want to know how to solve the integral in a different way.","['integration', 'indefinite-integrals', 'calculus']"
4612065,Change of basis vector,"I don't have a problem, I just need help with the theory. I feel like I keep misunderstanding things when it comes to solving problems that involves changing from a basis to another.
Lets say we are given 3 linearly independent vectors $v_1, v_2, v_3$ . If we are to build a basis of these vectors, we would put them in a bracket right, but they are said to be a new basis that has it's coordinates with respect to the standard basis? But what if we wanted to write these new basis with its own coordinates? Also how can we write the standard basis with respect to this new basis? I'd really appreciate it if someone can help me out, or give me a YT video recommendation or another website. I would also really appreciate it if someone could give me questions to work with to understand it better. I tried googling myself but I only found what I already know, which is how to change from one base to another etc.","['vector-spaces', 'matrices', 'change-of-basis', 'linear-algebra', 'linear-transformations']"
4612091,Is it possible to find an analytical solution of a system of two 2nd order ODEs?,"I am trying to solve two 2nd order ODEs of the general form: $$\frac{d^2r}{dt^2}=f(r,\theta,\dot{r},\dot{\theta})$$ $$\frac{d^2\theta}{dt^2}=g(r,\theta,\dot{r},\dot{\theta})$$ where $$\dot{r}=\frac{dr}{dt},\quad \dot{\theta}=\frac{d\theta}{dt}$$ Although I can solve the two equations numerically for any given expression of the functions $f(r,\theta,\dot{r},\dot{\theta})$ and $g(r,\theta,\dot{r},\dot{\theta})$ , I am interested to know whether a general analytical solution can be obtained for the problem. I am not aware of any such method that can be used to find analytical solution for systems of ODEs.",['ordinary-differential-equations']
4612147,Three circles in a triangle: can the circles move?,"Three unit circles are in an isosceles right triangle. In the beginning, each circle is tangent to the other circles and one edge of the triangle; there is a vertical line of symmetry. Can the circles move without overlapping? This is a variation of another question of mine. My attempt I superimposed a cartesian coordinate system. I assumed that the red circle moves left a small distance left, and the only possible loss of tangency is between the red and green circles. I let the coordinates of the red circle's centre be $(t,0)$ , then found that the  distance between the red circle's centre and the green circle's centre is $\sqrt{\left(t-a\right)^2+\left(a-\sqrt2-\sqrt6\right)^2}$ where $a=\frac12 \left(\sqrt2+\sqrt6-\sqrt{4-t^2}+\sqrt{2(\sqrt2+\sqrt6)\sqrt{4-t^2}+t^2-4-4\sqrt3}\right)$ . I tried to show algebraically that this distance is greater than $2$ , which would show that the circles can move, but this is rather difficult. This desmos animation suggests that the circles can move. I am hoping for an intuitive answer.","['circles', 'geometry', 'intuition', 'packing-problem']"
4612158,"Why are these two results ""contradictory""?","I was thinking about the effect of coarseness of topology but got two results which seem to be ""contradictory"". On the one hand, in Compactness for finer topology Andres Mejia's answer suggests that If a set is compact under the topology of uniform convergence, then it is compact under the topology of pointwise convergence. (because the topology of uniform convergence is finer than the topology of pointwise convergence) In other words, it is easier for a set to be compact under the topology of pointwise convergence. On the other hand, consider the set of continuous/Rieman integrable functions, it is sequentially closed with the topology of uniform convergence but not with the topology of pointwise convergence. So I guess If a set is sequentially closed under the topology of pointwise convergence, then it is sequentially closed under the topology of uniform convergence. In other words, it is more difficult for a set to be sequentially closed under the topology of pointwise convergence. However, sequential closeness is a necessary condition for compactness. So the above two results look ""contradictory"". Do I make any mistake or this is just reasonable?","['general-topology', 'functional-analysis', 'real-analysis']"
4612187,Determining all endpoints (vertices) on set of linear constraints,"I would like to know how to determine all endpoints or intersections (vertices) of multidimenzional planes. Let's say I have an object (for example multidimensional cube) given by the following sets of inequalities ( $p$ -times leq ; $q$ times geq ): $$
\begin{bmatrix}
    a_{1,1} & \dots & a_{1,i} & \dots  & a_{1,n} \\
    a_{2,1} & \dots & a_{2,i} & \dots  & a_{2,n} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    a_{p,1} & \dots & a_{p,i} & \dots  & a_{p,n}
\end{bmatrix} 
*
\begin{bmatrix}
    x_{1} \\
    \vdots \\
    x_{i} \\
    \vdots \\
    x_{n} 
\end{bmatrix} 
\leq
\begin{bmatrix}
    H_{1} \\
    H_{2} \\
    \vdots \\
    H_{p} 
\end{bmatrix} 
$$ $$
\begin{bmatrix}
    b_{1,1} & \dots & b_{1,i} & \dots  & b_{1,n} \\
    b_{2,1} & \dots & b_{2,i} & \dots  & b_{2,n} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    b_{q,1} & \dots & b_{q,i} & \dots  & b_{q,n}
\end{bmatrix} 
*
\begin{bmatrix}
    x_{1} \\
    \vdots \\
    x_{i} \\
    \vdots \\
    x_{n} 
\end{bmatrix} 
\geq
\begin{bmatrix}
    L_{1} \\
    L_{2} \\
    \vdots \\
    L_{q} 
\end{bmatrix} 
$$ Where $\boldsymbol{x}$ are variables, $\boldsymbol{a} $ , $\boldsymbol{b} $ , $\boldsymbol{L} $ , and $\boldsymbol{H}$ are parameters: $\boldsymbol{a}, \boldsymbol{b}, \boldsymbol{L}, \boldsymbol{H} \in \mathbb{R}$ . In other words I have some angular object, for example as provided in this illustration , and would like to find all the endpoints. I am more interested in those violet ones, because considering additional constraints for nonnegativity, the object would be compact. What I thought of yet is to use some modification of simplex algorithm. However, I am not sure how to modify it such that it identifies all the endpoints. Also, I am inquiring if there is something a little bit better and easier to implement. Thank you very much.","['analytic-geometry', 'functions', 'geometry', 'analysis']"
4612206,Continuous Approximation of Lp function on $\mathbf{R}^n$,"I would like to show the following statement: For each $g\in L_1(\mathbf{R}^n,m)$ and each $\epsilon>0$ , there exists a continuous function $f:\mathbf{R}^n\to\mathbf{R}$ such
that $$
\int |f-g|\,dm <\epsilon.
$$ I have proven the following statement which was told to be useful: For each bounded Lebesgue measurable $A\subset\mathbf{R}^n$ and each $\epsilon>0$ ,
there exists a continuous function $f_A:\mathbf{R}^n \to [0,1]$ with compact support such that $$
\int |f_A-\chi_A|\,dm <\epsilon.
$$ Attempted Proof for the first statement: Let $g \in L_1(\mathbf{R}^n, m)$ , $\epsilon > 0$ be given. In particular, there exists some $k \in \mathbf{R}$ such that $$
g = g|_{[-k, k]^n} + g|_{\mathbf{R}^n - [-k, k]^n},
$$ and that $$
\int_{\mathbf{R}^n - [-k, k]^p} |g| \, dm < \frac{\epsilon}{2}.
$$ Now note that there exists simple functions $$
s(x)= \sum_{i = 1} ^m a_i \chi_{A_i}(x)
$$ such that $\| g|_{[-k, k]^p} - s \|_{L_1} < \epsilon/2$ with $A_i$ bounded. Now note that by the statement proven, for all $i$ , there exists continuous functions $f_{A_i}: \mathbf{R}^n \to [0, 1]$ such that $$
\int |f_{A_i} - \chi_{A_i}| \, dm < \epsilon.
$$ Therefore, we have \begin{align*}
    \int |g - \sum_{i = 1} ^m a_i f_{A_i}| \, dm &= \int | g|_{[-k, k]^n} - \sum_{i = 1} ^m a_i f_{A_i}| \, dm + \int |g|_{\mathbf{R}^n - [-k, k]^n}| \, dm \\
    &\leq | g|_{[-k, k]^n} - \sum_{i = 1} ^m a_i \chi_{A_i}| \, dm + \int |\sum_{i = 1} ^m a_i f_{A_i} - \sum_{i = 1}^m a_i \chi_{A_i}| \, dm + \int |g|_{\mathbf{R}^n - [-k, k]^n}| \\
    &< \frac{\epsilon}{2} + \sum_{i = 1} ^m |a_i| \int |f_{A_i} - \chi_{A_i}| \, dm + \frac{\epsilon}{2} \\
    &< \epsilon + \sum_{i = 1} ^m |a_i| \epsilon \\
    &= \epsilon(1 + \sum_{i = 1} ^m |a_i|).
\end{align*} Since $\epsilon > 0$ is arbitrary, we are done. My Doubts: I think the last sentence where we reached the conclusion is faulty. In particular, I think our $a_i$ in the last part of the inequality actually depends on $\epsilon > 0$ . Therefore, it is not very clear how this gives the desired result. However, I am not sure how to fix this.","['measure-theory', 'approximation-theory', 'real-analysis', 'solution-verification', 'lp-spaces']"
4612214,Find the minimum of $\sqrt{\cos x+3}+\sqrt{2\sin x+7}$ without derivative,"How do we find the minimum of $$f(x)=\sqrt{\cos x+3}+\sqrt{2\sin x+7}$$ without using derivatives? This problem is probably related to circles of Apollonius. I have tried AM-GM and Cauchy-Schwarz inequality but I can't work it out. Anyway, I have solved it in a more geometric way. Here's my answer. Firstly we can do some identical transformation. $$f(x)=\dfrac{\sqrt{2}}{2}(\sqrt{(\cos x+1)^2+(\sin x)^2+4}+\sqrt{(\cos x)^2+(\sin x+2)^2+9})$$ So that it makes sense in geometry. $P(\cos x,\sin x)$ is on the circle $x^2+y^2=1$ , and the value of $f(x)$ equals to sum of the distance from $A(0,-2)$ to $P$ and from $B(-1,0)$ to $P$ . In other words: $$f(x)=\dfrac{\sqrt{2}}{2}(\sqrt{|PB|^2+4}+\sqrt{|PA|^2+9}).$$ And here we can use Minkowski inequality. $$f(x)\geq \dfrac{\sqrt{2}}{2} \sqrt{(|PA|+|PB|)^2+25}$$ When $P$ , $A$ , $B$ is collinear, $RHS$ gets the minimum. Meanwhile, $LHS = RHS$ . Therefore, $f(x)_{min}=\sqrt{15}$ .","['calculus', 'trigonometry', 'analysis', 'inequality']"
4612217,"Let $X, Y$ be Hilbert spaces. Is $\mathcal L(X, Y)$ togerther with the operator norm a Hilbert space?","Let $(X, |\cdot|_1)$ and $(Y, |\cdot|_2)$ be Banach spaces. Let $\mathcal L(X, Y)$ be the space of all continuous linear maps from $X$ to $Y$ . We endow $\mathcal L(X, Y)$ with the operator norm $\|\cdot\|$ . Then $(\mathcal L(X, Y), \|\cdot\|)$ is a Banach space. I would like to ask if the following statement is true, i.e., Statement If $X, Y$ are Hilbert spaces, then so is $\mathcal L(X, Y)$ . If $Y =\mathbb R$ , the statement is true by Riesz representation theorem. Thank you so much for your elaboration!","['hilbert-spaces', 'operator-theory', 'functional-analysis']"
4612227,How to tell if a polynomial has only two distinct roots,"So by only two distinct roots I meant that the polynomial, say $f(x)$ , is of the form $f(x) = (x-a)^{p}(x-b)^{q}$ . Now, I also wish to know this if $a, b \in \mathbb{C}$ . That is it is possible that $a$ has a non-zero imaginary part while $b$ is an integer and vice versa. It would really help if there was some easy method to find it out. Also I would like to know the suitability of the term only two distinct roots. Thank you for the help!","['algebra-precalculus', 'polynomials']"
4612257,Open subgroups of the arithmetic fundamental groups of algebraic curves over finite fields,"Let $q$ be a power of a fixed prime number $p$ , and let $\mathbb{F}_q$ be the finite field of $q$ elements. For a geometrically connected algebraic curve $X$ defined over $\mathbb{F}_q$ , we will denote by $\pi_1(X)$ the arithmetric fundamental group of $X$ . Now suppose that $G$ is a profinite group which is isomorphic to $\pi_1(X)$ for some smooth geometrically connected algebraic curve $X$ defined over $\mathbb{F}_q$ . My question is the following. If $H$ is an open subgroup of $G$ , then is there some smooth geometrically connected algebraic curve $Y$ defined over $\mathbb{F}_q$ such that $\pi_1(Y)\cong H$ ? My motivation for asking this question is that such fact holds for global number fields. That is, if $K$ is a number field, $S$ a finite set of primes of $K$ containing all all archimedean primes of $ K $ and $G_{K,S}$ the Galois group of the maximal extension of $K$ inside a fixed algebraic closure of $K$ which is unramified outside $S$ , then any open subgroup of $G_{K,S}$ also has this form. In this sense, I'm asking if the analogy of the above fact for function fields holds.","['etale-cohomology', 'number-theory', 'algebraic-geometry', 'arithmetic-geometry']"
4612271,$n$th derivative of $\ln(x)e^{2x}$,"I was asked to calculate the $n$ th derivative of $f(x)=\ln(x)e^{2x}$ , and my solution was to use the Leibniz formula , so we have: $f^{(n)}(x)=\sum_{k=0}^{k=n}C^{k}_{n}.\frac{(-1)^{k-1}.(k-1)!}{x^k}.2^{n-k}.e^{2x}$ . But I don't know how I can calculate this sum.","['analysis', 'real-analysis', 'complex-analysis', 'calculus', 'derivatives']"
4612282,"Proving Implication of Feller's Paper ""A Limit Theorem for Random Variables With Infinite Moments""","Let $(X_n)_{n\in\mathbb{N}}$ be a sequence of real i.i.d. random variables with $\mathbb{E}(X_1)=\infty$ and $(a_n)_{n\in\mathbb{N}}$ a positive real sequence such that $\frac{a_n}{n}$ is non-decreasing in $n$ .
I want to prove that the following is equivalent: $$
\sum_{n=1}^{\infty}P(|X_n|>a_n)<\infty \iff P(\limsup\limits_{n\to\infty}|S_n|/a_n<\infty)=1
$$ My attempt: Apparently this is a direct consequence of a paper of a Theorem from Fellers paper ""A Limit Theorem for Random Variables with Infinite Moments"" from 1946. The Theorem in the paper says: Let $(X_n)_{n\in\mathbb{N}}$ and $(a_n)_{n\in\mathbb{N}}$ be given as above, then $$
P(|S_n|>a_n \text{ i.o.})=0 \text{ if }\sum_{n=1}^{\infty}P(|X_n|>a_n)<\infty
$$ and $$
P(|S_n|>a_n \text{ i.o.})=1 \text{ if }\sum_{n=1}^{\infty}P(|X_n|>a_n)=\infty
$$ I can prove that $\sum_{n=1}^{\infty}P(|X_n|>a_n)<\infty \Rightarrow P(\limsup\limits_{n\to\infty}|S_n|/a_n<\infty)=1$ , but I'm lost for the other direction. One of my ideas was the following: Let $c\geq1$ . If $\sum_{n=1}^{\infty}P(|X_n|>a_n c)=\infty$ then according to the Theorem cited above we have $$
1=P(|S_n|>a_nc \text{ i.o.})=P\Bigl(\frac{|S_n|}{a_n}>c \text{ i.o.}\Bigr).
$$ So if we let $c\to \infty$ , we can find a random subsequence $(n_k)_{k\in\mathbb{N}}$ such that $\lim\limits_{k\to\infty}|S_{n_k}|/a_{n_k} = \infty$ a.s. and hence $P(\limsup\limits_{n\to\infty}|S_n|/a_n=\infty)=1$ . Using Kolmogorov's zero-one law we get that $P(\limsup\limits_{n\to\infty}|S_n|/a_n<\infty)<1$ implies $P(\limsup\limits_{n\to\infty}|S_n|/a_n<\infty)=0$ and hence we arrive at a contradiction. However the problem is that I can't say for sure that $\sum_{n=1}^{\infty}P(|X_n|>a_n c)=\infty$ , since for $c\geq 1$ it only holds $\sum_{n=1}^{\infty}P(|X_n|>a_n c)\leq \sum_{n=1}^{\infty}P(|X_n|>a_n)=\infty$ . Any advice would be much appreciated!","['probability-limit-theorems', 'random-walk', 'stochastic-processes', 'probability-theory', 'stochastic-calculus']"
4612292,"Conditional expectation of $X\sim U[0,2]$ given $\min(X,t)$, where $t\in [0,2]$.","I want to determine the conditional expectation of $X\sim U[0,2]$ given $\min(X,t)$ , where $t\in [0,2]$ . Using the definition $$\mathbb E[X|\min(X,t)] = \frac{\mathbb E[X\chi_{\min(X,t)}]}{\mathbb P(\min(X,t))}$$ does not look that inviting to me. I am unsure if I should consider the probability for a specific value $\mathbb P(\min(X,t) = x)$ for $x\in [t,2]$ if the minimum is larger than $t$ or the probability that $\mathbb P(\min(X,t) \ne t)$ Intuitively I would say that since we know $t$ if $\min(X,t)$ is not $t$ , it has to be $X$ , and thus the conditional expectation has to be $\min(X,t) = X$ which seems tautological. If $\min(X,t)$ we know that $X>t$ and because $X\sim U[0,2]$ it follows that $X$ is uniformly distributed between $t$ and $2$ . The conditional expectation, in that case, should be $\frac{2+t}{2}.$ Is this correct? If so, how does this follow from the definition given above?","['conditional-expectation', 'solution-verification', 'probability-theory', 'probability']"
4612387,Continuity of the thin QR factorization.,"In section 5.2 of the book Matrix Computations by Golub & Van Loan we can find the following result: Let $A\in\mathbb R^{m\times k}$ be a matrix with full column rank. Then we have the factorization $A=QR$ for some matrix $Q\in \mathbb R^{m\times k}$ with orthonormal columns and some upper triangular matrix $R \in \mathbb R^{k\times k}$ with positive diagonal entries. Moreover the factorization is unique and is called the thin QR factorization of $A$ . I want to show that the factorization is continuous with respect to $A$ . So let $(A_n)$ be a sequence of matrices in $\mathbb R^{m\times k}$ , each with full column rank, and suppose that $A_n\to A$ . For each $n$ , let $Q_nR_n$ denote the thin factorization of $A_n$ . Suppose $Q_{n'}\to Q_0 \in \mathbb R^{m\times k}$ for some subsequence $n'$ . Then $Q_0^\top Q_0=\lim Q_{n'}^\top Q_{n'} =I_k$ so $Q_0$ has orthonormal columns. Moreover we have $R_{n'}=Q^\top_{n'}Q_{n'}R_{n'}=Q^\top_{n'}A_{n'}\to R_0$ where $R_0=Q^\top_0A$ . Since each $R_{n'}$ is upper triangular with nonnegative diagonal entries, so is $R_0$ . Moreover we have $$QR=\lim Q_{n'}R_{n'}=Q_0R_0$$ and so $\text{rank}(R_0)=\text{rank}(Q_0R_0)=\text{rank}(QR)=\text{rank}(R)=k$ . Therefore the diagonal entries of $R_0$ are in fact positive. From the uniqueness of the thin factorization we conclude that $Q_0=Q$ and $R_0=R$ . We have shown that any convergent subsequence of $(Q_n)$ must converge to $Q$ . We claim that this implies that $Q_n\to Q$ . Otherwise we can construct a subsequence $(Q_{n'})$ which is bounded away from $Q$ by some distance greater than some $\epsilon>0$ . Since $\|Q_n\|=\sqrt{k}$ for all $n$ , the sequence $(Q_{n'})$ is bounded, and so we can extract a further subsequence converging to some matrix other than $Q$ , a contradiction. Hence $Q_n\to Q$ , and repeating the first part of the proof we get $R_n\to R$ as well. Is this proof correct? Thanks a lot for your help.","['matrix-rank', 'matrices', 'continuity', 'linear-algebra', 'matrix-decomposition']"
4612404,Show that $\displaystyle \lim_{k\rightarrow \infty}f(k^\alpha x) \rightarrow 0$,"Let $f$ be an integrable function over the positive real numbers. Prove that $f(k^\alpha x) \rightarrow 0$ whenever $k \rightarrow \infty$ for almost everywhere $x>0$ and any real number $\alpha>1$ . I have two attempts: (1) Use monotone convergence theorem for series. Since \begin{align*}
\int \sum_{k=1}^\infty f(k^\alpha x)=\sum_{k=1}^\infty \int f(k^\alpha x) dx &= \sum_{k=1}^\infty \int \frac{1}{k^\alpha} f(y)dy \\
&=\sum_{k=1}^\infty \frac{1}{k^\alpha}  \int f(y)dy\\
&=\sum_{k=1}^\infty \frac{1}{k^\alpha}||f||_1<\infty 
\end{align*} Then we have the integrand $\sum_{k=1}^\infty f(k^\alpha x)<\infty$ for almost everywhere $x>0$ . Thus, the general term $f(k^\alpha x)$ goes to zero for almost everywhere $x>0$ . I think this attempt is valid. (2) However I am trying a second solution:
Let $E_n=\mathbb{R}_+ \cap (-n, n)$ which has finite measure (so that I can use the continuity of measure from below and thus Borel-Cantelli lemma). Now consider the following set: $$\{x\in E_n\mid \lim_{k\rightarrow \infty} f(k^\alpha x)=0\},$$ I want to show that for each $k\in \mathbb{N}$ and $\alpha>1$ , the set $$A_{k, \alpha}=\{x\in E_n| |f(k^\alpha x)|> M\}$$ has measure zero on each $E_n$ , where $M$ is any positive real number depends on $k, \alpha$ . Choose $M=\frac{1}{k^{\alpha-2}}$ , then I want to use Markov's inequality to show that \begin{align*}
\mu(A_{k, \alpha})=\mu\{x \in E_n| |f(k^\alpha x)|> \frac{1}{k^{\alpha-2}}\}&<k^{\alpha-2}||f||_1 \\
&= k^{\alpha-2} \int \frac{1}{k^{\alpha}} f(y) dy \\
&=\frac{1}{k^2} ||f||_1 
\end{align*} Then by Borel-Cantelli lemma, $$\mu(\limsup A_k)=\mu(\bigcap_{N=1}^\infty \bigcup_{k=N}^\infty A_k)=0$$ Can somebody help me to validate this approach?","['measure-theory', 'solution-verification', 'lebesgue-integral', 'real-analysis']"
4612408,Using the Sum Formula to solve $\cos\theta – \sin3\theta = \cos2\theta$,"This problem is from S.L. Loney's book on Plane Trigonometry. To solve the following equation: $$\cos\theta – \sin3\theta = \cos2\theta$$ Now, my initial approach was to algebraically manipulate the equation to yield a difference of cosines: $$\cos\theta– \cos2\theta = \sin3\theta$$ From this, the sum formula could be applied to the RHS to yield $$\cos\theta – \cos2\theta = \sin2\theta \cos\theta + \cos2\theta \sin\theta$$ Now, this equation is true if $\sin2\theta = 1$ and $\sin\theta = –1$ . Hence $\theta$ must be equal to $1\over2$$(n\pi + (–1)^n$$\pi\over2$$)$ and equal to $n\pi – (–1)^n$$\pi\over2$ For one, I do not know how to combine the two equations into one form, or if it is even needed to. Secondly, I have been informed that there are even more values that satisfy the equation. How am I to find them? I also don't know if the solution I provided is even accurate, so could the reader also perhaps check if the reasoning I provided is correct? I am still relatively new in the field of trigonometry, hence your time would be much appreciated. Thank you in advance.","['algebra-precalculus', 'trigonometry']"
4612419,Are there infinitely many primes less than $q^{1+\epsilon}$ equivalent to $1$ mod $q$?,"Fix $\epsilon>0$ . As $q$ becomes large, is it true that the number of primes less than $q^{1+\epsilon}$ congruent to $1$ modulo $q$ will tend to infinity? A conjecture of Montgomery says that the number of primes congruent to $a$ mod $q$ should tend to $\pi(x)/\varphi(q)$ when $q<x^{1-\epsilon}$ , which gives an extremely strong version of what I want. I don't need the count of primes to be close to $\pi(x)/\varphi(q)$ though, just infinite. Moreover, I know that often these sorts of computations are easier for the residue class $1$ , so perhaps that will help here. I've done some searching online, but I can't find anything. If anybody knows of such a result, that would be fantastic!","['number-theory', 'distribution-of-primes']"
4612425,Stokes' Theorem and vector integrand in curvilinear coordinates,"I want to compute the following vector integral: $$ \iint_\Omega d\Omega \{\nabla_s\times \mathbf{F}\}  \tag{1}\label{eq1}$$ Note that $d\Omega$ is a scalar surface element, the result should therefore be vectorial. Further we can not apply Stoke's Theorem, as it would require an oriented surface element $\mathbf{d\Omega}$ such that the integrand becomes a scalar. In the particular case I'm studying $\mathbf{F}$ is tangential to the surface $\Omega$ , so in curvilinear coordinates, using Dupin's orthogonal system $\hat{\mathbf{v}}_1, \hat{\mathbf{v}}_2, \hat{\mathbf{n}}$ : $$ F_n=0 \implies \mathbf{F} = F_1 \hat{\mathbf{v}}_1+ F_2 \hat{\mathbf{v}}_2, $$ with $\hat{\mathbf{v}}_1, \hat{\mathbf{v}}_2$ being aligned to the lines of curvature. The surface curl $\nabla_s\times$ is defined as $$ \nabla_s \times \mathbf{G} =  (\nabla_s G_n)\times\hat{\mathbf{n}}+\frac{G_2}{R_2}\hat{\mathbf{v}_1}-\frac{G_1}{R_1} \hat{\mathbf{v}_2}  + \hat{\mathbf{n}} \nabla_s\cdot(\mathbf{G}\times \hat{\mathbf{z}}) $$ This definition was found in ""J. van Bladel, Electromagnetic fields, 2nd ed., 2007"" and has the advantage that there is no normal derivative applied to the tangential components, which is important when dealing with surface vectors that are non-zero only on the surface $\Omega$ . The quantities $R_1, R_2$ denote the radii of curvature. Now, considering first a plane in Cartesian coordinates, we have $R_1=R_2=\infty$ and a constant normal which is invariant to the location, e.g., $\hat{\mathbf{n}}=\hat{\mathbf{z}},\hat{\mathbf{v}_1}=\hat{\mathbf{x}},\hat{\mathbf{v}_2}=\hat{\mathbf{y}}$ , and therefore $$ \nabla_s \times \mathbf{F} =  \hat{\mathbf{z}} \nabla_s\cdot(\mathbf{F}\times \hat{\mathbf{z}}) \\
\iint_\Omega d\Omega \{\nabla_s\times \mathbf{F}\} = \hat{\mathbf{z}} \iint_\Omega d\Omega \{ \nabla_s\cdot(\mathbf{F}\times \hat{\mathbf{z}}) \}.
$$ In the last equation it was possible to move $\hat{\mathbf{z}}$ in front of the integral because $\hat{\mathbf{z}}$ does not vary over $(x,y)$ . The integrand is now a scalar, and thus the (Surface) Divergence Theorem can be applied which yields $$
\hat{\mathbf{z}} \iint_\Omega d\Omega \{ \nabla_s\cdot(\mathbf{F}\times \hat{\mathbf{z}}) \} = \hat{\mathbf{z}} \int_{\partial\Omega} dc \{ (\mathbf{F}\times \hat{\mathbf{z}})\cdot \hat{\mathbf{m}} \} = \hat{\mathbf{z}} \int_{\partial\Omega} dc \{ \mathbf{F} \cdot \hat{\mathbf{t}}\},
$$ where $\hat{\mathbf{t}}$ is the unit vector aligned to the boundary $\partial\Omega$ and $\hat{\mathbf{m}}$ is defined such that $\hat{\mathbf{n}}\times\hat{\mathbf{m}}=\hat{\mathbf{t}}$ . In other words, a transformation of $\eqref{eq1}$ into a boundary line integral is possible for a plane in Cartesian coordinates by using Stokes' or the Divergence Theorem, because the normal is invariant. As an application, e.g., take the case that $\mathbf{F}$ vanishes outside a circular region, centered at the origin. If the boundary $\partial\Omega$ encloses this region, we find $$ \iint_\Omega d\Omega \{\nabla_s\times \mathbf{F}\}  =0, $$ because $\mathbf{F} \cdot \hat{\mathbf{t}}=0$ on the entire boundary $\partial\Omega$ . Now I would like to check if the same applies for curved surfaces. More importantly, I'm wondering if the surface $\Omega$ is closed (consider e.g. a sphere), that is it has no Stokes Boundary $\partial\Omega$ , is it true that $$ \iint_\Omega d\Omega \{\nabla_s\times \mathbf{F}\}  = 0 ?$$ I was not able to find any hint in my favorite text books  ... so I'm happy for any advice.
Thank you.","['multivariable-calculus', 'calculus', 'stokes-theorem', 'vector-analysis']"
4612427,Largest ball guaranteed to fit in a bounded polyhedron of volume $V$,"Suppose that I have some polyhedron $F \subset [0,1]^d$ (the figure is convex and lies in a bounded unit cube in $d$ -dimensional space). The figure also has a positive volume $V > 0$ . My question is, what is the largest value of $a \ge 0$ such that a ball (in $\mathbb{R}^d$ ) of radius $a \cdot V$ is guaranteed to fit inside $F$ ? An easy upper bound is $a \le 1/2$ , because for an $V \times 1 \times 1 \times \cdots \times 1$ box, one can fit at most a $V/2$ radius ball inside. But perhaps there are other polyhedra that make this worse. Is there any guarantee that $a > 0$ ?",['geometry']
4612450,Find an $\omega \in \text{Alt}^2(\Bbb R^4)$ such that $\omega \wedge \omega \ne 0$.,"Find an $\omega \in \text{Alt}^2(\Bbb R^4)$ such that $\omega \wedge \omega \ne 0$ . What is the intuition for the problem? Since $\omega \in \text{Alt}^2(\Bbb R^4)$ we know that $\omega : \Bbb R^4 \to \Bbb R$ is an alternating $4$ -linear map. That is $$\omega(x_1,x_2,x_3,x_4) \in \Bbb R$$ and that $$\omega(x_1,x_2,x_3,x_4) = -\omega(x_2,x_1,x_3,x_4)$$ for example. I've also seen that this $\omega \wedge \omega$ can be tought of as the volume of a parallelotope so is the question asking me to find an $\omega \in \text{Alt}^2(\Bbb R^4)$ such that the ""volume"" with itself is nonzero? Was it not true that the wedge product of any $n$ -form with itself was always $0$ ?","['exterior-algebra', 'differential-geometry']"
4612465,"What is a ""Gaussian"" point process?","I try to understand the definition of a ""Gaussian point process"" which is (vaguely) given in this paper in section 2.2.1. Let $(E,\mathcal E)$ be a measurable space. A random measure on $(E,\mathcal E)$ is a transition kernel from a probability space $(\Omega,\mathcal A,\operatorname P)$ to $(E,\mathcal E)$ . A point process $\kappa$ on $(E,\mathcal E)$ is a random measure on $(E,\mathcal E)$ such that $\kappa(\omega,\;\cdot\;)$ is a counting measure for $\operatorname P$ -almost all $\omega\in\Omega$ . Since they don't give such a definition in the paper, they might assume that $$\kappa(\omega,\;\cdot\;)=\sum_{i\in I}\delta_{X_i(\omega)}\;\;\;\text{for all }\omega\in\Omega\tag1$$ for some countable set $I$ and an $(E,\mathcal E)$ -valued process $(X_i)_{i\in I}$ on $(\Omega,\mathcal A,\operatorname P)$ . In that case, we need to assume $\{x\}\in\mathcal E$ for all $x\in E$ . Now, they assume $E=\mathbb R^d$ for some $d\in\mathbb N$ and ""define"" that the point process is Gaussian if for every $k\in\mathbb N$ and $B_1,\ldots,B_k\in\mathcal B(\mathbb R^d)$ , the random variable $(\kappa(\;\cdot\;,B_1),\ldots,\kappa(\;\cdot\;,B_k))$ has a multivariate Gaussian distribution. My problem with this is: Is that possible at all? I mean, each $\kappa(\;\cdot\;,B_i)$ is $\mathbb N_0\cup\{\infty\}$ -valued ... So, can it be normally distributed? I think the answer should be no ... So, what am I missing? EDIT : If this actually does not make sense, can we fix the definition?","['measure-theory', 'probability-theory', 'sampling', 'normal-distribution']"
4612490,Maximum of derivatives of a differentiable function,"I am preparing for the ""concours aux grandes écoles"" and I was looking for some problems on analysis. These problem are hard but I generally am able to solve them. However, I am stuck on a problem since this morning and have absolutely no idea about how to solve it. I am new to this forum so do not blame me if I express myself incorrectly or in the wrong feed. Here is the problem. Let $n\geq 2$ and $f:\mathbb{R}\rightarrow \mathbb{R}$ be a $\mathcal{C}^n$ function. For any $0\leq k\leq n$ define $$M_k=\sup_{t\in \mathbb{R}}\vert f^{(k)}(t)\vert.$$ Suppose that $M_0$ and $M_n$ are finite. Show that $$M_k < +\infty$$ for every $k$ . Show furthermore that $$M_1 \leq \sqrt{2M_0 M_2}.$$ For the second one, Taylor at the left and at the right of some point $x$ , up to $2$ nd derivative. This gives me $$f'(x)\leq \frac{2M_0}{2h}+\frac{h}{4}2M_2$$ Thank to Ted Shifrin, I noticed that I can get the second inequality by just minimizing the bound over $h$ , silly me !
Doing this, I get the desired bound. However, I have still no idea how to do the first question.","['taylor-expansion', 'analysis']"
4612495,Does the abelianisation functor $\mathrm{Grp} → \mathrm{AbGrp}$ preserve composition?,"I am to show there is a functor $F : \mathrm{Grp} → \mathrm{AbGrp}$ . I have already checked that the assignments $F_0(G) := G_{\mathrm{ab}} := G/[G,G]$ and $F_1  (f \colon G → H) := \bar{f}: G_{\mathrm{ab}} → H_{\mathrm{ab}}$ are well-defined. (Using some elementary algebra theorems about factoring through quotients.) In checking that $F$ preserves composition, however, I run into some trouble. I have seen that for $f \colon G → H$ and $g \colon H → K$ group homomorphisms, $F_1(g ∘ f) = \overline{g ∘ f}$ and $F_1(f) ∘ F_1(g) = \bar{f} ∘ \bar{g}$ are both maps $G_{\mathrm{ab}} → K_{\mathrm{ab}}$ . (All this required some verification using factorisation through quotient groups to the abelianised groups.) However, I fail to see that these maps are the same ! I could of course evaluate them on the elements of $G_{\mathrm{ab}}$ , but I’m learning category theory and therefore want to show this not the ‘intrinsic’ way but the ‘extrinsic’ way. Now I drew the following diagrams, to show my thinking: such that both $\bar{f}, \bar{g}$ , and $\overline{g ∘ f}$ appear. Now what I know is that, for instance $g ∘ f = \overline{g ∘ f} ∘ \pi_G$ . What I’d like to do is just chase the diagrams to show $\overline{g ∘ f} = \bar{g} ∘ \bar{f}$ holds. What would help is if $\pi_H$ would be the identity¹, which would be so if $H$ were already abelian. Because then, we would just conclude that $$\overline{g ∘ f} ∘ \pi_G = \bar{g} ∘ \bar{f} ∘ \pi_G \,.
$$ Which would bring us a lot closer. (Still not sure how to finish then, though.) ¹In a sense, this should indeed be true, because $\bar{f}$ , I think, should automatically map to an abelian group (one of the aforementioned elementary algebra theorems). HOWEVER, and this is where I get really confused, at no point was $H$ itself to be required anything but a general group! I feel like I’m really close here, so could someone help me fill in the gaps / patch up the mistakes in my thinking? Best wishes!","['abelian-groups', 'abstract-algebra', 'functors', 'category-theory']"
4612510,confusion about chain rule in linearity proof,"Suppose $v,p\in\mathbb{R}^n$ , then $D_v|_p:C^\infty\to\mathbb{R}$ is defined by $$D_v|_p(f):=\frac{d}{dt}|_{t=0}f(p+tv).$$ Now I'm looking at a proposition that states that the map $v\mapsto D_v|_p$ is a linear isomorphism from $\mathbb{R}^n\to T_p\mathbb{R}^n$ . I understand most of the proof, but I'm confused by the linearity part: $$\frac{d}{dt}|_{t=0}f(p+tv+\lambda tw) = \frac{d}{dt}|_{t=0}f(p+tv) + \lambda\cdot\frac{d}{dt}|_{t=0}f(p+tw)$$ for $v,w,p\in\mathbb{R}^n$ and $\lambda\in\mathbb{R}$ . This equality should immediately follow from the chain rule, but I don't see how. Probably I'm just a bit confused by the notation of (directional) derivatives... but I already searched quite a bit and just can't see how the chain rule is used here. Can someone explain this?","['tangent-spaces', 'analysis', 'smooth-manifolds', 'calculus', 'chain-rule']"
4612536,What pentagonal tiling is this?,"I'm a little baffled by the following pentagonal tiling: It clearly has an 8-tile primitive unit, and thus should be one of types 7-8 (Richard Kershner) or else one of types 9, 11, 12 or 13 (Marjorie Rice) in https://en.wikipedia.org/wiki/Pentagonal_tiling However, the angles (as appearing in the photograph, or as in the diagrams below) do not seem to match any of these types (though perhaps, indeed probably, I'm making a mistake). What gives? What is the type of this tiling? (Also, what are the angles, exactly? And are all five side of exactly the same length?)","['trigonometry', 'tiling', 'geometry', 'polygons']"
4612575,Elementary set theory the difference between the two proofs of the distributive law,"I have come across two different proofs for $(A\cap B)\cup C =(A\cup C) \cap(B \cup C)$ . I do not know much about logic, I am just curious if one proof is better than the other. Do these different types of proof have a name in logic/proof theory? Proof 1: \begin{align}
x \in (A\cap B)\cup C &\Leftrightarrow (x \in A ~~ \text{and}~~ x \in B)~~\text{or}~~ x \in C\tag{1}\\
&\Leftrightarrow  (x \in A ~~ \text{or}~~ x \in C)~~\text{and}~~ (x \in B ~~ \text{or}~~ x \in C)\tag{2}\\
&\Leftrightarrow x \in (A\cup C) \cap(B \cup C)\tag{3}
\end{align} My problem with this proof is that it seems we simply swap $\cup$ to ""or"" and $\cap$ to ""and"" with no formal definition and also from step (1) to (2) we provide no proof. Proof 2: We know: \begin{equation}\tag{1}
 (A\cap B)\cup C \subset (A \cup C)
\end{equation} Also: \begin{equation}\tag{2}
 (A\cap B)\cup C \subset (B \cup C)
\end{equation} Hence, \begin{equation}\tag{3}
 (A\cap B)\cup C \subset (A \cup C) \cap (B \cup C) 
\end{equation} We can also show  ( $A \cup C) \cap (B \cup C) \subset (A\cap B)\cup C$ . Consequently, $(A\cap B)\cup C =(A\cup C) \cap(B \cup C)$ . Although each statement in the second proof also needs to be proven, all the proofs rely entirely on the definitions of set, membership of a set, and subsets (everything is defined based on set and set definitions as opposed to Proof 1 , which I think is using the equivalency between set union and intersections with logical 'and' and 'or'). Is there a formal distinction between these two proofs in logic/proof theory? I am very much interested in these sort of questions, is there any reference I can read so I can educate myself better in this topic?","['proof-writing', 'logic', 'reference-request', 'elementary-set-theory', 'terminology']"
4612608,How to elegantly predict average number of zero crossings for cumulative sum plot for N flips of a coin where heads is +1 and tails is -1,"I believe the same question was asked in question 1338097, but I think the answer might be flawed. However, I'm not familiar with the terms and have only the most basic understanding of probability theory. I know that if you create a sequence of +1s and -1s by flipping a fair coin N times (e.g. heads for +1, tails for -1) the number of possible sequences, ranging from all +1s to all -1s is 2^N. And assuming the sequence is truly random, each of the possible sequences will have the same probability of occurring. Therefore, if you generated each of the 2^N possible sequences, performing the cumulative summation process for each and counting the number of times the sum crosses 0 (i.e. 1 -> 0 -> -1 or -1 -> 0 -> 1), then dividing that count by 2^N gives the answer I'm looking for. And for N = 5 or less, you could even figure it out on a piece of paper in a few minutes if you weren't a computer programmer or didn't have a computer. But I wrote a program to do it and here's my table of answers for N = 2 to 10: N    average 0 crossings
 2    none (takes at least 3 moves to cross 0)
 3    .25 
 4    .25
 5    .4375
 6    .4375
 7    .59375
 8    .59375
 9    .730469
 10   .730469 And since the process to come up with these answers involves nothing more than simple arithmetic, I believe these answers are as exact as single-precision floating point operations can give. Of course, the problem with this method is that the computer is brought to its knees for values of N higher than about 30. But I don't care about being that exact. Approximations to, say, 5 decimal places would be fine. Is there an elegant solution that would have my computer spitting out the answer for values of N greater than 100 or even 1000?","['statistics', 'probability']"
4612634,Solution to $u''(x)-V(x)u(x) = 0$ is identically zero,"I am struggling to prove a property of a solution to the following ODE: Suppose $u\in \mathcal{C}([a,b])$ is twice continuously
differentiable, $V\in\mathcal{C}([a,b])$ , $V(x)\geq 0$ $\forall x \in
> [a,b]$ , and $$u''(x) - V(x)u(x)=0 \quad x\in[a,b],$$ $$u(a)=u(b)=0.$$ Then $u(x)=0$ for all $x$ in $[a,b]$ . I've tried a few different things to prove this. We know that $u$ is continuous on $[a,b]$ and since $u(a)=u(b)=0$ , then by Rolle's theorem $\exists c\in(a,b)$ such that $u'(c)=0.$ Then I would want to somehow prove that $u'(x)=0$ over the whole interval, meaning that $u$ is constant at 0, but I am not sure how to get there. Something else I noticed was that from the equation we have that $u''(a)=u''(b)=0$ , so by the fundamental theorem of calculus, $$0=f''(b)-f''(a)=\int_a^b f'''(x)\mathrm{d}x,$$ where by the product rule, $u'''(x) = V(x)u'(x) + V'(x)u(x)$ . Then I thought to proceed via integration by parts, but I didn't get anywhere that way either. Any help is appreciated.","['ordinary-differential-equations', 'real-analysis']"
4612637,"If the pushforward of $\mu$ under the group action is invariant, then $\mu$ must be the Haar measure.","Let $G=\text{PU}(d)$ the projective unitary group, acting on the complex projective space $X=\mathbb{P}(\mathbb{C}^d)$ , in the usual way. Let $\mu:\mathcal{B}(G)\to[0,\infty]$ be a Radon measure, where $\mathcal{B}(G)$ denotes the Borel sets of $G$ . For every $x\in X$ , denote by $\mu_x:\mathcal{B}(X)\to[0,\infty]$ the pushforward measure of $\mu$ under the group action $g\mapsto gx$ , which is  given by $\mu_x(A)=\mu(\{g\in G\,|\, gx\in A\})$ . Consider the following property: $(\star)$ For all $x\in X$ , $\mu_x$ is $G$ -invariant, meaning that $$\forall x\in X, \forall h\in G, \forall A\in\mathcal{B}(X): \mu_x(hA)=\mu_x(A).$$ Note that if $\mu$ is the Haar measure of $G$ , then $(\star)$ holds. Is the converse true? That is, if $(\star)$ holds, must $\mu$ be the Haar measure? I believe the answer is yes, but can't seem find an argument. Is there a way to write $\mu$ in terms of the measures $\{\mu_x\,|\,x\in X\}$ that would allow to conclude that $\mu$ must be itself invariant using the invariance of the pushforward measures $\mu_x$ ?","['homogeneous-spaces', 'measure-theory', 'haar-measure', 'lie-groups']"
4612702,"Solution $x:\mathbb R_+\to [0,1]$ of the ODE $\dot x = \tfrac 1 2 x^2 - (1-x) (1-e^{-at})$ is a concave function of $g(t)=1-e^{-at}$.","Is any solution $x_t:\mathbb R_+\to [0,1]$ of the ODE $\dot x = \tfrac 1 2 x^2 - (1-x) (1-e^{-at})$ is a concave function of $g_t=1-e^{-at}$ for any $a>0$ ? Note: A solution has to converge to the critical point $x_* = \sqrt{3}-1$ , otherwise it would explode (and leave the interval $[0,1]$ , so it won't be a solution). There is in fact unique such solution. This is a simplification of the question: Convexity / concavity of $(g_t,x_t)$, where $x_t$ solves the ODE $\dot x_t=\tfrac 1 2 x_t^2 - (1-x_t) g_t$. , which is a simplification of my original problem: System of quadratic autonomous ODEs - convexity of the solution curve A slightly more general question would be: Is any solution $x:\mathbb R_+\to [0,1]$ of the ODE $\dot x = \tfrac 1 2 x^2 - b(1-x) (1-e^{-at})$ a concave function of $g_t=1-e^{-at}$ for any $a,b>0$ ? I will appreciate a solution or a reference to any similar problems.","['nonlinear-analysis', 'ordinary-differential-equations', 'examples-counterexamples', 'calculus', 'convex-analysis']"
4612742,"Let $f:\mathbb{R}^n\rightarrow \mathbb{R}$ be continuous such that $\lim\limits_{\|x\| \to\infty} f(x)=\infty$, then there exist a global minimum.","Statement*: Let $f:\mathbb{R}^n\rightarrow \mathbb{R}$ be continuous such that $\lim\limits_{\|x\|
\to\infty} f(x)=\infty$ , then there exist a global minimum. Note: $\|.\|$ is the euclidean norm. Proof: Because $f(x)$ is diverging to infinity if $\|x\|
\to\infty$ there exists $K > 0$ for every $M > 0$ , such that if $x\in \mathbb{R}^n$ with $\|x\|>K$ then $f(x) > M$ and if $x\in \mathbb{R}^n$ with $\|x\|\leq K$ then $f(x) \leq M$ . Define $S^{n-1}=\{x\in \mathbb{R}^n |x_1^2+...+x_n^2\leq K^2\}$ then $S^{n-1}$ is compact because it is bounded and closed. So the restriction of $f$ to the compact set $S^{n-1}$ has a global minimum and maximum because $f|_{S^{n-1}}$ is continuuous.
Let $a\in S^{n-1}$ be the point in which $f|_{S^{n-1}}$ has its global minimum $f(a)$ then $\|a\|\leq K$ and $f(a)\leq M$ . For any $x\in \mathbb{R}^n\setminus S^{n-1}$ , $\|x\|> K$ , and thereby $f(x) > M\geq f(a)$ . So $f(a)$ is a global minimum. q.e.d. *The statement is almost the same as in the following question: Limit goes to infinity, show that the f has a finite minimum. I want to adapt the proof given in the answer for the domain $\mathbb{R}^n$ instead of $\mathbb{R}$ and show that there is a global minimum. Did I made any mistakes?","['multivariable-calculus', 'solution-verification', 'real-analysis']"
4612750,Limits problem (unable to solve further),"Struggling to solve this problem, $\displaystyle \lim\limits_{n \to \infty} \left(\frac{1}{n} + \frac{1}{n+1} + \frac{1}{n+2} +\dots+ \frac{1}{6n}\right)$ My approach: $\displaystyle \lim\limits_{n \to \infty} \left(\frac{1}{n} + \frac{1}{n+1} + \frac{1}{n+2} +...+ \frac{1}{6n}\right)$ = $\displaystyle\lim\limits_{n \to \infty} \int_{0}^{1}(x^{n-1} + x^{n} + x^{n-2}+...+ x^{6n-1}) dx$ = $\displaystyle\lim\limits_{n \to \infty}\int_{0}^{1}\left(x^{n-1} \cdot \frac{x^{5n+1} - 1}{x-1}\right)dx$ got stuck here and don't know how to solve it further (other approaches which are simpler would also help)","['integration', 'limits', 'calculus']"
4612783,"Why the divergence of vector field F(x, y) = (x, y) equal to 2 at every point?","Suppose there's a 2d vector field: $F(x,y) = (x,y)$ . Meaning that at the coordinate $(0,0)$ it's a $0$ -vector, at $(0,1)$ it's a $(0,1)$ vector and so on. Here's a picture of it: If you mathematically calculate the divergence of this vector field, you will get the following scalar field: $F(x,y) = 2$ . So, at every point the divergence is constant and equals 2. Now I'm trying to understand what that does physically mean. According to Wikipedia: In physical terms, the divergence of a vector field is the extent to which the vector field flux behaves like a source at a given point. It is a local measure of its ""outgoingness"" – the extent to which there are more of the field vectors exiting from an infinitesimal region of space than entering it. <..>  The greater the flux of field through a small surface enclosing a given point, the greater the value of divergence at that point. So, for example, we take a point $(0, 0)$ and draw a small circle around it and see how much of the outward going flux there is: According to this picture, all the arrows point outwards from the circle, no matter how small the circle is. So divergence at point $(0,0)$ is positive, which checks out with the mathematically calculated value of 2. So, at point $(0,0)$ the vector field acts like a 'source', which makes sense. Now we take another point $(1,1)$ and also draw a circle around it: On this image there are arrows entering the circle area and the same amount of arrows leaving the circle area, so the net flux at the point $(1,1)$ should be 0, and the divergence at this point also should be 0, but mathematically calculated value is 2. Why? The same applies to any other point which isn't $(0, 0)$ .","['divergence-operator', 'multivariable-calculus', 'terminology']"
4612810,Hadamard product operator norm,"suppose that $A$ and $B$ be $n×n$ nonnegative matrices. Consider the operator (spectral) norm on $A$ $\lVert A \rVert_{op} = sup_{x \ne 0} \dfrac{\lVert Ax \rVert}{\lVert x \rVert}= sup_{\lVert x \rVert=1} \lVert Ax \rVert.$ Then $ \lVert A \circ B \rVert _{op} ≤ \rho (A^{T} B)$ ? where $\rho (A)$ spectral radius and $A \circ B=( a_{ij}  b_{ij})$ is a the Hadamard product.
I know that $\rho (A \circ B ) ≤ \rho (AB),$ and $(A \circ B)(B \circ A) ≤  AB \circ BA$ .","['matrices', 'linear-algebra']"
4612838,Why does taking the derivative of a parabola give the axis of symmetry? [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed last year . Improve this question So, I was doing this exercise and it says that to find the axis of symmetry of a general parabola $$ax^2+bxy+cy^2+dx+ey+f=0$$ I can just take the partial derivative of this conic. As I didn't understand why this works, I tried doing the derivative on a normal parabola (the one that has the axis parallel to the x or y axis $$ ax^2+bx+c=0 $$ ) and it works as well. Can someone explain why this works?","['derivatives', 'linear-algebra', 'geometry']"
4612852,Sequentially closure on Hilbert separable space,"Let $H$ a Hilbert separable space with a Hilbert basis $(e_n)_{n\in\mathbb{N}}$ . Let $$
 F = \{e_m + m e_n \, : \, n,m \geq 1\}
$$ Show that 0 is not in the sequentially weak closure of F. Show that 0 in sequentially weak closure of sequentially weak closure of F Any hint of suggestion? I tried by contradiction but I'm stuck","['hilbert-spaces', 'functional-analysis', 'weak-topology']"
4612868,Why is the area of a square $a^2$ and volume of a cube $a^3$?,"How area and volume emerge from lower dimensions This might sound as a silly question but I think there is some hidden complexity behind it. Esentially, my question is how is it possible that: multiplication of something with essentially no area grants you an object with an area . multiplication of something with essentially no volume grants you an object with a volume . The thing is, even if I acknowledge that line segment could have some nonzero but really small area (for example a wooden toothpick), why its according-times multiplication would give me an area of a square? If I put these toothpicks together, they still won't fill the appropriate square (see illustration): I know, that if I discretize it and assign some fundamental ""area value"" it would make sense, for example having 4 cabbages in a row and 4 rows in a total gives me 16 cabbages... BUT how to make a sense of it for the case with toothpicks etc.? Thank you very much!","['euclidean-geometry', 'recreational-mathematics', 'geometry']"
4612887,"Integrating $\int\ln^e(x)dx$, constant wrong?","I am practicing to integrate with the incomplete gamma function. As such, I want to integrate $$\int\ln^e(x)dx$$ I use the following substitutions $$u=\ln(x)\implies \int u^e e^u du$$ $$v=-u\implies \int(-v)^e e^{-v}\cdot -dv$$ Now, we can convert this to a gamma function $$\int(-v)^e e^{-v}\cdot -dv = - \int(-1)^e v^e e^{-v} dv= -(-1)^e\int v^{(1+e)-1} e^{-v} dv = -(-1)^e\cdot -\Gamma(1+e, -\ln(x)) $$ which simplifies to $$(-1)^e\cdot \Gamma(1+e, -\ln(x)) +C$$ This is wrong though, and wolfram alpha gives me an answer of $${\color{red}{(-1)^{-e}}}\cdot \Gamma(1+e, -\ln(x)) +C$$ Something clearly went wrong here (and here , for that matter since I somehow also have a missing negative from the power?) and I'm not exactly sure what went wrong. I conjecture that perhaps splitting $(-v)^e = (-1)^e v^e$ was incorrect since the interior is negative, but I am not sure what to do otherwise with such a term. Can anyone enlighten me as to what my mistake is and how I would properly integrate this function? Thanks Edit : Okay, differentiating the results seem to provide interesting results. Differentiating my own result on wolfram alpha gives $$(-1)^e (-\ln(x))^e$$ which equals the initial function if i combine the powers, while the negative power does not work. I find this odd. I again conjecture that since the inner parts of the power functions are negative, I violated this rule when integrating and violating  the rule again here reverts it. I'm still not sure what to do with the original integral though.","['integration', 'indefinite-integrals', 'gamma-function', 'exponentiation']"
4612912,How do I integrate this rational function?,"Evaluate this integral $$I=∫\dfrac{1-x^2}{x^4+x^2+1}dx$$ (with $x$ is different from 0).
I tried divided both the numerator and denominator by $x^2$ , and I got $$∫\dfrac{\frac{1}{x^2}-1}{x^2+\frac{1}{x^2}+1}dx.$$ I then put $t=x+1/x$ and got this $$I=∫-\dfrac{dt}{t^2-1}.$$ Finally I got the answer $$\dfrac12{\ln|-x^2-\dfrac{1}{x^2}-1|}+C$$ or the alternative form is $$\dfrac{\ln\left(\left|x^2+x+1\right|\right)+\ln\left(\left|x^2-x+1\right|\right)-2\ln\left(\left|x\right|\right)}{2}+C. $$ Where did I make mistake? Because when I compute the function, the result is different $$\dfrac{\ln\left(x^2+x+1\right)-\ln\left(x^2-x+1\right)}{2}+C.$$","['integration', 'polynomials']"
4612990,How do I find the limit of this function involving complex numbers as $x → 0$?,"I have the function $$
f(x) = \frac{\Im\left(\frac{M^{x + 1} - m^{x + 1}}{M^x - m^x}\right) - \frac{\Im(a)}{2}}{\Re\left(\frac{M^{x + 1} - m^{x + 1}}{M^x - m^x}\right) - \frac{\Re(a)}{2}}
$$ where $a$ and $b$ are complex numbers, $M = \frac{a + \sqrt{a^2 + 4 b}}{2}$ , $m = \frac{a - \sqrt{a^2 + 4 b}}{2}$ , and $\Re(z)$ and $\Im(z)$ are the real and imaginary parts of any complex-valued $z$ , respectively. I want to know how to find the limit of this function as $x → 0$ in terms of $a$ , $b$ , $M$ , and/or $m$ . By plotting the function in graphing software I can see that the limit exists, as $\lim_{x → 0^+} f(x) = \lim_{x → 0^-} f(x)$ . However, the presence of the complex numbers is confusing me as to how to proceed. Edit with additional information on implementing Vishu's answer: The graphing software I'm using, GeoGebra, does not allow specifying which branch of the complex logarithm is used, so for some values of $a$ and $b$ Vishu's answer $$
\lim_{x → 0} f(x) = \frac{
\Im\left(\frac{M - m}{\ln\left(\frac{M}{m}\right)}\right)
}{
\Re\left(\frac{M - m}{ln\left(\frac{M}{m}\right)}\right)
}
$$ appears to not work. This can be fixed by instead using the equation $$
\lim_{x → 0} f(x) = \frac{\Im\left(\frac{M - m}{\ln\left(\left|\frac{M}{m}\right|\right) + i \left(\arg\left(\frac{M}{m}\right) + 2 π k\right)}\right)}
{\Re\left(\frac{M - m}{\ln\left(\left|\frac{M}{m}\right|\right) + i \left(\arg\left(\frac{M}{m}\right) + 2 π k\right)}\right)}
$$ where $k$ is defined as $$
k = \begin{cases}
1 & : 0 < \arg(b) ≤ π ∧ \arg(\sqrt{b}) ≤ \arg(M) ≤ \arg(b) ∨ -π ≤ \arg(b) < 0 ∧ \arg(-\sqrt{b}) ≤ \arg(M) ≤ π \\
−1 & : -π ≤ \arg(b) < 0 ∧ \arg(b) ≤ \arg(M) ≤ \arg(\sqrt{b}) ∨ 0 < \arg(b) ≤ π ∧ -π ≤ \arg(M) ≤ \arg(-\sqrt{b}) \\
0 & : \text{otherwise}
\end{cases}
$$","['limits', 'calculus', 'complex-numbers']"
4612993,"If $O$ is a point in square $ABCD$, then $\angle OAB+\angle OBC+\angle OCD+\angle ODA\ge\frac{3\pi}4$","$O$ is a point in square $ABCD$ . Show that $$\angle OAB+\angle OBC+\angle OCD+\angle ODA\ge\frac{3\pi}4$$ My solution is complex numbers. Let $-C=A=1$ , $-D=B=i$ . So \begin{align*}&\mathrm{Arg}\left(\frac{B-A}{O-A}\cdot\frac{C-B}{O-B}\cdot\frac{D-C}{O-C}\cdot\frac{A-D}{O-D}\right)
\\={}&\mathrm{Arg}\left(\frac4{O^4-1}\right)=-\mathrm{Arg}(O^4-1).
\end{align*} Here, $O$ is taken in the square, but $O^4$ is hard to manage. Firstly, when $|O|\le\dfrac{\sqrt2}2$ , it can be in any direction then $O^4$ ’s locus is circle centered at $0$ with radius $\dfrac14$ . However if $|O|$ is bigger, only some direction is in the square, namely $O$ moves on four arcs. So my solution gets stuck here. You could actually change a method completely, but I’d like to see if my method would work.","['inequality', 'geometry']"
4613071,Domination number is at most $n/2$,"Let $G$ be a graph of $n$ vertices with no isolated vertex. Prove that the domination number of $G$ is at most $\lfloor n/2 \rfloor.$ Turns out this result can easily solve a combinatorics problem I'm working on right now. And since the problem only asks for even $n$ I'm going to ignore the odd numbers for the moment. I tried induction like this: The base case is clear so assume the result hold for some even number $n$ . Now given a graph of $n+2$ vertices I want to remove $2$ vertices so I can use the induction hypothesis. The problem is that for some graphs, removing any two vertices will give a graph with isolated vertices.","['graph-theory', 'combinatorics', 'discrete-mathematics']"
4613149,Algebraic manipulation question coming from Method of Moments application,So today in my statistical inference class the professor wrote on the board: Using the Method of Moments: $$\sigma^2 = E[Y_1^2|\theta] - E[Y_1|\theta]^2$$ $$= \frac{1}{n}\sum_{j = 1}^{n}{Y_j^2} - \left( \frac{1}{n}\sum_{j = 1}^{n}{Y_j}\right)^2$$ $$= \frac{1}{n}\sum_{j = 1}^{n}{\left( Y_j - \bar{Y}\right)^2 }$$ where $\bar{Y} = \frac{1}{n}\sum_{j = 1}^{n}Y_j$ (average values of Y's) I am confused at how he arrived at the last equation from the one above that and I've been trying to figure it out. Here's what I have so far: $$= \frac{1}{n}\sum_{j = 1}^{n}{Y_j^2} - \left( \frac{1}{n}\sum_{j = 1}^{n}{Y_j}\right)^2$$ $$ = \frac{1}{n}\sum_{j = 1}^{n}{Y_j^2} - \frac{1}{n^2}\left(n\bar Y\right)^2$$ $$ = \frac{1}{n}\sum_{j = 1}^{n}{Y_j^2} - \bar Y^2$$ Which is fundamentally different that the equation that my professor had on the board. My equation takes the difference of squares while his takes the square of a difference and then sums those up... Where did I go so wrong? Is there just some statistical insight that I don't know and haven't used?,"['statistical-inference', 'statistics', 'discrete-mathematics']"
4613185,How to solve nonlinear differential equation system?,"I'm trying to solve this problem about nonlinear differential system: Consider the first order nonlinear differential equation system given by $$
\left\{
\begin{array}{}
     x'& = \ 1-x-y
  \\ y'& = \ x(y^2-1)(1-x-y).
\end{array}
\right.     \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \   (*)
$$ a) Calculate the equilibrium points of the system. b) Find the solution that passes through the point $(0,2)$ and determine its orbit. The item a) its clear for me. I find that all points of the line $y=-x+1$ are equilibrium points of $(*)$ . My problem comes with item b). I don't really know how to solve the problem, because since it is a non-linear problem I don't know how to find the solution :( In what way does it affect that the solution must pass through the point $(0,2)$ ? Any help is welcome, thank you!!","['nonlinear-dynamics', 'ordinary-differential-equations', 'dynamical-systems']"
4613194,"Classify, up to isomorphism, all groups of order $5\cdot 7 \cdot 19$.","I know that the general process for these types of problems is to first apply the Fundamental Theorem of Finite Abelian Groups (FTFAG), then find some semi-direct products. First, if we assume $G$ is abelian, then it is isomorphic to $\mathbb{Z}_5 \times \mathbb{Z}_7 \times \mathbb{Z}_{19}$ by FTFAG. There is no other abelian groups of the proper order. I am stuck, now, on finding the appropriate semi-direct products to find the non-abelian groups of the proper order. By the Sylow counting theorems, I know that the Sylow- $5$ , Sylow- $7$ , and Sylow- $19$ subgroups are all normal in $G$ , but I can't seem to come up with a proper homomorphism to construct a semidirect product. In addition to this specific problem, I would appreciate any help when it comes to constructing groups of a certain order as a semidirect product in general.","['finite-groups', 'semidirect-product', 'abstract-algebra', 'sylow-theory', 'group-theory']"
4613282,Prove $\frac{H(x^2)}{H(x)}$ increases.,"For $x\in[0,1]$ , let $f(x):=-x\ln x$ and the two-sample entropy function $H(x)=f(x)+f(1-x)$ . Prove $h(x):=\displaystyle\frac{H(x^2)}{H(x)}$ increases. Here is my proof which is a bit cumbersome. I am seeking a much more elegant approach. The numerator of the derivative of the sought fraction is \begin{align}
&H(x)^2\frac{dh(x)}{dx} \\
=&\frac d{dx}H(x^2)H(x)-H(x^2)\frac d{dx}H(x) \\
=& 2x\ln\frac{x^2}{1-x^2}\,\big(x\ln x+(1-x)\ln(1-x)\big)-\big(x^2\ln x^2+(1-x^2)\ln(1-x^2)\big)\ln\frac x{1-x} \\
=& 2x^2\ln^2 x+2x(2-x)\ln x\ln(1-x)-(x^2+1)\ln x\ln(1-x^2)+(1-x)^2\ln(1-x)\ln(1-x^2). \tag1\label1
\end{align} All four terms above except the third are positive. I combine the second and the third term together and divide it by $-\ln x$ which is positive, and get \begin{align}
g(x):&=-2x(2-x)\ln(1-x)+(x^2+1)\ln(1-x^2) \\
&=(3x-1)(x-1)\ln(1-x)+(x^2+1)\ln(1+x) \tag2\label2 \\
&= \int_0^x \Big(g''(a)-\int_t^a g'''(s)ds\Big)(x-t)dt
\end{align} for some $a\in[0,x]$ . So we only need to show $g(x)>0, \forall x\in\big(0,\frac13\big]$ . $$\frac{d^3g(x)}{dx^3}= \frac{4x(2x^3 +3x^2-2x-7)}{(1-x)^2(1+x)^3}.$$ Let $p(x):=2x^3+3x^2-2x-7$ . $p(x)\le p(1)=-4, \forall x\in[0,1]$ . This is true since $p(x)$ is convex as $p''(x)=12(x+\frac12)>0$ on that interval and $p(0)=-7<-4=p(1)$ . We can take $a=\frac13$ since we can show, with a bit of work, $g''(\frac13)>0$ . (to be continued)","['calculus', 'entropy', 'inequality', 'real-analysis']"
4613291,Evaluating $\int\frac{1}{(x-2)^4 \sqrt{x^2 + 6x + 2}}dx$,"I'm struggling with the integral, $$\int\frac{1}{(x-2)^4 \sqrt{x^2 + 6x + 2}}dx.$$ I tried it as follows: Substituting $x-2 = \frac1t \implies dx = \frac{-dt}{t^2}.$ $$\therefore \int\frac{dx}{(x-2)^4 \sqrt{x^2 + 6x + 2}} = \int \frac{- dt}{\frac{t^2}{t^4} \sqrt{(\frac1t + 2)^2 + 6 (\frac1t + 2) + 2}} = \int \frac{-t^3}{\sqrt{18t^2 + 10t + 1}}\ dt$$ How to continue from here?","['integration', 'indefinite-integrals', 'calculus']"
4613328,"Reference Request from Chapter 12 of ""Modular Forms and Fermat's Last Theorem""","I am looking for a reference that covers the following result from page 360 of the text. For context, we have a Galois representation $\overline{\rho}: G_{\mathbb Q} \to \operatorname{GL}_2(k)$ , where $k$ is a finite field of characteristic $\ell$ . We assume $\ell$ is an odd prime, that this representation is irreducible, that its restriction to a decomposition group at $\ell$ is finite flat or ordinary, that it has cyclotomic determinant, and that it has squarefree conductor. We also assume that $\overline{\rho}$ is equivalent over $\overline{\mathbb F}_\ell$ to $\overline{\rho}_f$ for some newform of weight 2 and level $N_f$ with trivial character. We know that if this holds, then there are infinitely many choices for $f$ . Given a finite set of primes $\Sigma$ , we might ask which of these $f$ give representations of so-called type $\Sigma$ , meaning those $f$ such that $\rho_{f, \lambda}$ (the attached $\ell$ -adic representation to $f$ ) is semistable at $\ell$ (meaning flat or ordinary at $\ell$ ) and such that $\Sigma$ contains the set of primes dividing $N(\rho_{f, \lambda})/N(\overline{\rho})$ (the quotient of the conductors). The authors claim without reference that a sufficient condition is that $N_f \mid N_\Sigma$ , where $$N_\Sigma = N(\overline{\rho})\prod_{p \in \Sigma} p^{m_p}$$ where $m_p = 2$ if $p \nmid \ell N(\overline{\rho})$ , $m_p = 1$ if $p \neq \ell$ and $p \mid N(\overline{\rho})$ , $m_\ell = 1$ if $\overline{\rho}$ is flat and ordinary at $\ell$ , and $m_\ell = 0$ otherwise. Can someone provide either a detailed proof of this fact, or a reference(s) containing such a proof?","['elliptic-curves', 'number-theory', 'representation-theory', 'reference-request', 'modular-forms']"
4613333,Marble drawing without replacement question,There are $7$ marbles in a bag. $4$ white and $3$ black. Marbles are removed from the bag one at a time without replacement. What is the probability that the fifth marble removed is black? My thought is $\frac37$ . The thinking is like you are trying to label $1$ to $7$ on these balls. The chance that a black marble gets the label $5$ is $\frac37$ . Is my thought correct?,"['combinatorics', 'probability']"
4613361,Martingale convergence theorem and martingale property,"Let $X_1,X_2,...$ be independent random variables: $$
\
X_n =
\begin{cases}
       0   \mbox{ with  probability }= 1 - \frac{1}{n} \\
       1   \mbox{ with  probability } = \frac{1}{2n} \\
      -1   \mbox{ with  probability } = \frac{1}{2n}
    \end{cases}\
$$ Let $Y_1 = X_1$ and for n $\geq 2$ $$
\
Y_n =
\begin{cases}
       X_n &\mbox{ if } Y_{n-1} = 0 \\
       nY_{n-1}|X_n| &  \mbox{ if } Y_{n-1} \neq 0 \\
    \end{cases}\
$$ Show that $Y_n$ is a martingale w.r.t. to $\mathcal{F}_n$ and show why the martingale converce theorem is unapplicable. Martingale : $$
E[Y_n|F_{n-1}] = E[X_n * 1_{[Y_{n-1} = 0]} + nY_{n-1}|X_n|* 1_{[Y_{n-1} \neq 0]}|F_{n-1}]
$$ Splitting the two conditional expectation and seeing that $E[X_n]$ = 0 and $E[|X_n|] = \frac{1}{n}$ the martingale property is satisfied. About the martingale convergence theorem I don't know how to prove it. I would think that the sup E $[|X_n|]$ is $\infty$ but Idk how to show it.","['martingales', 'measure-theory', 'convergence-divergence', 'probability-theory']"
4613388,Green's Function and Zero Modes of Differential Operators,"I have this question from reading QFT textbooks. Consider harmonic oscillator as a simpler model for differential operators. The Lagrangian is given by $$L=\frac{1}{2}\dot{x}^{2}-\frac{1}{2}\omega^{2}x^{2}.$$ After integration by parts, one can express the classical action as $$S[x;j]=\int_{-\infty}^{\infty}dt\left[\frac{1}{2}x(t)\left(-\frac{d^{2}}{dt^{2}}-\omega^{2}\right)x(t)+x(t)j(t)\right],$$ where $j(t)$ is an external source. One can define the following second order differential operator $$L(t,s)\equiv\left(\frac{d^{2}}{dt^{2}}+\omega^{2}\right)\delta(t-s),$$ where $\delta(t-s)$ is the Dirac delta function . Using this notation, one can write the classical action in a compact form $$S[x;j]=-\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}dtds\frac{1}{2}x(t)L(t,s)x(s)+\int_{-\infty}^{\infty}dtx(t)j(t)\equiv-\frac{1}{2}\langle x,Lx\rangle+\langle j,x\rangle.$$ In QFT, one defines the Greens's function of the differential operator $L$ as its ""right inverse"", i.e. $$\left(\frac{d^{2}}{dt^{2}}+\omega^{2}\right)G(t-s)=\delta(t-s),$$ where $\delta(t-s)$ is treated as the ""continuous generalization"" of the Kronecker delta symbol . In our compact notation, this is just $$LG=𝟙.$$ Then, it is well-known that the equation of motion $$\ddot{x}(t)+\omega^{2}x(t)=j(t) \tag{1}$$ has a unique solution $$x(t)=x_{0}(t)+\int_{-\infty}^{\infty}dsG(t-s)j(s), \tag{$\star$}$$ where $x_{0}$ is the unique solution of the homogeneous ODE $$\ddot{x}(t)+\omega^{2}x(t)=0. \tag{2}$$ If we use the compact notation introduced above, the solution ( $\star$ ) is very easy to understand: $$Lx=Lx_{0}+LGj=LGj=𝟙j=j,$$ which is precisely equation (1). However, there is a problem. In linear algebra, if an operator has an inverse, then it cannot have zero-eigenmodes. But from equation (2), it is clear that $x_{0}$ is a zero mode of the linear operator $L$ . Then, we cannot define its Green's function $G(t-s)$ , which is absurd. The same problem arises when we are dealing with QFT in higher dimensions, and $L$ becomes a second order partial differential operator. What's going on? Where did I make mistakes?","['quantum-field-theory', 'greens-function', 'ordinary-differential-equations', 'partial-differential-equations']"
4613397,Why do we use dx when X is not a function?,"I know what a differential is, to the extent of Riemann sums. But when differentiating a function, say y=f(x), then oftentimes I see dy = f'(x)dx by the chain rule, which makes sense. But X isn't a function, it's the base variable, so how does it's derivative matter? On any case, if X is understood as the axis, wouldn't it's derivative simply be 1?","['differential', 'calculus', 'functions', 'derivatives', 'chain-rule']"
4613400,Reported breakthrough with union-closed sets,"I saw a brief newspaper report today of an apparent breakthrough  by Gilmer on a conjecture by Frankl regarding union-closed sets. The conjecture is that if a family of sets is ""union-closed"" then it must have at least one element appearing in at least half the sets. When I looked at the paper , which I do not completely understand, the lower bound developed by Gilmer is far short of the ""1/2"" conjectured by Frankl. Is the report correct, in the sense of suggesting that the professional mathematics community consider a breakthrough to have been made, and if so, why is it such a breakthrough?",['combinatorics']
4613441,Stuck (I think quite far) on a hard nonlinear optimization problem,"The problem is this: The standard-form of Pringles chips is achieved by creating the hyperbolic paraboloïd $z=\frac{y^2}{4}-\frac{x^2}{2}$ on a domain given by the elliptic plate $x^2+\frac{y^2}{2}\le q^2$ , $q\in\mathbb R_+$ . For a special edition they change the domain to $x^2+y^4-ay^2=1$ . For which values of $a$ does the maximum of $z$ on this curve lie on the $y$ -axis, but the minimum of $z$ on this curve not on the $x$ -axis. I have tried and tried but I can't seem to get past this: $$f(x,y)=\frac{y^2}{4}-\frac{x^2}{2}=\frac{y^2}{4}-\frac{1}{2}(1-y^4+ay^2)=\frac{y^4}{2}+\frac{y^2}{4}-\frac{a}{2}y^2-\frac{1}{2}$$ Then $$\frac{df}{dy}=2y^3+\frac{y}{2}-ay=y(2y^2+\frac{1}{2}-a)=0$$ if $y=0$ or $2y^2=a-\frac{1}{2} \Leftrightarrow y^2=\frac{2a-1}{4}$ $(a\ne1/2)$ . If $y=0$ then $f(y)=-1/2$ and so $x^2=1$ . (probably minima, so then $a\ne1/2$ ) Is this enough? Do I have to use the second derivative test here to show that this is a minima (but then I wouldn't have showed that there aren't other minima, right?)? If $y^2=\frac{2a-1}{4}$ where $a\ne1/2$ then $f(y)=\ldots=\frac{1}{4}(-a^2+a-\frac{17}{32})=\frac{2a-1}{8}-x^2$ $\Leftrightarrow$ $\frac{a^2}{4}+\frac{1}{128}=x^2$ . Do I have to do anything with this?","['lagrange-multiplier', 'multivariable-calculus', 'nonlinear-optimization']"
4613442,Computing a residue (complex analysis),"Compute the following residue: \begin{equation}
\textrm{res}_{z=0}\frac{(\sin(z))^2}{(\sinh(z))^5}
\end{equation} I thought of using the power series expansions, and trying to find the $\frac{1}{z}$ coefficient as follows: \begin{equation}
\frac{\sin^2(z)}{\sinh^5(z)}=\frac{z^2(1-\frac{z^2}{6}+O(z^4))^2}{z^5(1+\frac{z^2}{6}+O(z^4))^5}
\end{equation} But it seems very hard to find the $\frac{1}{z}$ term from this - any ideas?","['analysis', 'complex-analysis', 'calculus', 'residue-calculus', 'complex-integration']"
4613479,Very difficult recurrence relation for the coefficients of an ODE solution,"Context :
I was working on answering this ODE question with a new method that I have never seen anyone use before (Frobenius method but with a twist) and I reduced the problem to a very difficult recurrence relation question; the terms in this case are the coefficients of a particular solution $y(x)$ with initial conditions $y(0)=0$ and $y'(0)=1.$ The recurrence relation is $c_{n\geq 3}=$ $$\frac1{n}\sum_{k=2}^{n-1}(n+1-k)c_{n+1-k}c_k-\frac{1}{n^2}$$ which I deduced is equivalent to $$\frac{n+1}n\sum_{k=2}^{\lceil (n-1)/2\rceil}c_{n+1-k}c_k\\+\left(n\bmod 2\right)\frac{n+1}{2n} c_{(n+1)/2}^2\\-\frac1{n^2}$$ where $c_0=0, c_1=1, c_2=\frac14.$ The first few terms are $$\left\{0,1,\frac14,-\frac5{72},-\frac{97}{1152},-\frac{10777}{172800},-\frac{243521}{6220800},-\frac{27545531}{1219276800},-\frac{1492739209}{78033715200},-\frac{212648420707}{21069103104000},-\frac{6885617878439}{1053455155200000},\cdots\right\}.$$ I was also able to deduce (though not rigorously so would be nice if someone could prove) that $\lim_{n\to\infty}c_n=0$ and $\sum_{n=0}^\infty c_n=1$ (which means the terms eventually become positive again and most probably alternate between the $2$ signs while becoming absolutely small to converge to $0.$ ) I found (not rigorously) that for large $n\to\infty,$ the recurrence relation reduces to $$c_{2n+1}=\pm c_{n+1}$$ which might be hinting at when the sign change happens. I tried nonlinear substitutions like $c_n=e^n$ to extract the terms out of the sum, but I always get remaining terms in $n.$ I also tried $nc_n=d_n$ and then the same substitution $d_n=e^n$ but I get a constant equal to an $n$ -dependent harmonic number. Other things I tried are expressing the terms in terms of $c_1$ (where $c_2=\frac{c_1}{2(3c_1-1)}$ ) for example to find a polynomial solution. I tried the same stuff for the more general case of the ODE, where we have $c_0=y(0)≠0,$ but still couldn't solve it. This is generally a problem when solving these types of non-linear ODEs, as you can see via my other answers, so it would be very helpful to have a general approach so that we can tackle the case when $c_\infty=\infty$ for example (where accurate calculation is necesssary,) though I think this is too ambitious. I am not an expert in recurrence relations, so I would like the answers, if any, to be a bit comprehensive and beginner-friendly. Any somewhat accurate approximating simpler sequence, insight on the sequence's behaviour; when the terms are positive/negative, are they bounded, (dis)proofs of what I deduced on my own heuristically (which might be very wrong,) how we could have maybe readjusted the original (general) problem to make it easier on ourselves to add more conditions in our toolkit like for example expanding $y(u=\frac1x)$ instead of $y(x)$ for $x\in(-\infty,\infty)\setminus [-1,1]\implies u\in(-1,1),$ which makes it easier to make an approximating series as the later coefficients don't matter as much, would be very helpful. Thank you.","['functional-equations', 'ordinary-differential-equations', 'recurrence-relations', 'sequences-and-series', 'numerical-methods']"
4613523,Evaluate $\int_{0}^{\infty}x\left ( \operatorname{Ci}(x)^2 +\operatorname{si}(x)^2\right )\operatorname{Ci}(x)\text{d}x$,"Evaluate $$
I_1=\int_{0}^{\infty}x\left ( \operatorname{Ci}(x)^2
+\operatorname{si}(x)^2\right )\operatorname{Ci}(x)\text{d}x.
$$ Where $\operatorname{Ci}(x)=-\int_{x}^{\infty}\frac{\cos(t)}{t}\text{d}t,\operatorname{si}(x)=-\int_{x}^{\infty}\frac{\sin(t)}{t}\text{d}t$ are cosine integral function and (modified) sine integral function respectively. By integrating numerically, one gives $I_1=0$ . My evaluation is a bit complex, so I would like to see a canonical one. Thanks for replying. A related version: $$
I_2=\int_{0}^{\infty}x\left ( \operatorname{Ci}(x)^2
+\operatorname{si}(x)^2\right )\operatorname{si}(x)\text{d}x=\frac\pi2\left(1-2\ln(2)\right).
$$ A sophisticated version: $$
I_3=\int_{0}^{\infty}\left ( \operatorname{Ci}(x)^2+\operatorname{si}(x)^2\right )^2\left ( 
 \operatorname{Ci}(x)^2+\operatorname{si}(x)^2+3\pi\operatorname{si}(x)\right ) \text{d}x
=6\pi^3\operatorname{Li}_2\left ( \frac{1}{4}  \right ) +12\pi^3\ln(2)^2.
$$ Where $\operatorname{Li}_2(z)=\sum_{k=1}^{\infty}\frac{z^k}{k^2}$ is the dilogarithm.","['integration', 'definite-integrals', 'special-functions', 'real-analysis', 'calculus']"
4613556,Show that $(S_n)_{n\in\mathbb{N}}$ is tight,"Let $(X_n)_{n\in\mathbb{N}}$ be independent Bernoulli random variables such that $$\mathbf{P}(X_{n}=1)=1-\mathbf{P}(X_{n}=0)=1/n^2.$$ Set $S_{n}:=X_1+\cdots+X_n.$ Show that $(S_n)_{n\in\mathbb{N}}$ is tight. For each $n$ , $X_{k}$ $(1\le k\le n)$ be independent random variables with characteristic function $\phi_{X_k}(t)=(1-\frac{1}{k^2})+\frac{1}{k^2}e^{it} (1\le k\le n).$ Then $$\phi_{S_{n}}(t)=E\exp(itS_{n})=\prod_{k=1}^{n}(1+\frac{1}{k^2}(e^{it}-1))$$ By Lévy’s continuity theorem, If $\phi_{S_{n}}(t)$ converges pointwise to a  limit $\phi_{S_{\infty}}(t)$ that is continuous at $0$ , then the asssociated sequence of distributions $S_{n}$ is tight.
But how can I find the $\phi_{S_{\infty}}(t):=\displaystyle \lim_{ n\to \infty}\phi_{S_{n}}(t), t\in \mathbb{R}$ ,which is continuous at $0$ ?","['complex-analysis', 'characteristic-functions', 'probability-theory']"
4613596,"Can I get from $\frac{d^2 x}{dt^2} = g$ to $\iint d^2 x = \iint g\,dt^2$?","The second law of Newton is $$a = \frac{F}{m}$$ and so in a constant gravitational field with gravity $g$ I have $$a = \frac{d^2x}{dt^2} = g$$ To solve this I multiply with $dt^2$ and get $$d^2x=g\,dt^2$$ and integrate $$\iint d^2x = g\,\iint dt^2$$ and so I get $$\frac{1}{2}x^2=\frac{1}{2}g\,t^2$$ which is wrong! The solution must be $x=\frac{1}{2}g\,t^2$ . What am I missing? And what is the difference between $dx^2$ and $d^2x$ ? I know it is a trivial question but I got a bit rusty with integrations...",['integration']
4613633,Complex representation of orthocenter : a signed area issue.,"I was recently looking up an equation for the orthocenter in the complex plane and found the following in Zwikker , C. (1968), The Advanced Geometry of Plane Curves and Their Applications , Dover Press: $$z_O=\frac{z_1\{z_1(z_2^*-z_3^*)+z_1^*(z_2-z_3)\}+\text{cycl.}}{4iA}$$ where $^*$ denotes the conjugate and $A$ is the area. I've not encountered the term cycl. previously and assumed it meant cycling through the indices, such as $z_2,z_3,z_1$ and $z_3,z_1,z_2$ . But this did not give the correct result. Zwikker similarly give the circumcenter as $$z_C=\frac{z_1z_1^*(z_3-z_2)+\text{cycl.}}{4iA}$$ Does anyone know what this means?","['geometry', 'triangles', 'trigonometry', 'soft-question', 'terminology']"
4613657,Inverse Fibonacci sequence,"I was having fun with Fibonacci numbers, and I had the idea to consider the sequence $
F_n=F_{n-1}^{-1}+F_{n-2}^{-1}
$ instead. I wrote a simple program to compute the first terms and the sequence seemed chaotic, although converging to $\sqrt{2}$ whatever $F_0$ and $F_1$ were. ( $F_0,F_1\neq0$ ). How can I prove that the sequence indeed converges, and it converges to $\sqrt{2}$ ? Is it hopeless to try and find a closed-term formula for $F_n$ ?","['numerical-methods', 'fibonacci-numbers', 'discrete-mathematics', 'sequences-and-series']"
