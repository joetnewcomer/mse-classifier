question_id,title,body,tags
4351728,Solve the equation over the integers,"$2^a b + 2^b a = a^2+b^2$ Clearly all possible solutions are $(0,0), (0,1), (1,0)$ What is a proper method of solving it? Thank you. EDIT:
Following Dietrich Burde advice, I will make an attempt: For every $a, b \geq 0, 2^a>a$ Without loss of generality and since the equation is totally symmetric, we can assume $a \geq b$ . So we have: $2^a>a \geq b \Rightarrow 2^a.b \geq b^2$ . Similarly, $2^b.a \geq b.a$ . but this is as far as I can go.","['calculus', 'algebra-precalculus']"
4351729,Exponential regression GLM,"Consider some positive random variables $X^1, X^2$ and $Y\sim Exp(p)$ where $p=\beta_0+\beta_1X^1 + \beta_2X^2$ . We have a random sample $\{X^1_i, X^2_i, Y_i\}$ . Now, estimate $\beta_1, \beta_2$ is not hard, e.g. using GLM. Now let's say that we do not observe $X^2$ . Is it still possible to consistently estimate $\beta_1$ (i.e. using only a statistic consisting of $\{X^1_i, Y_i\}$ )? If not, is there some assumption (normality etc...) for $X^i$ under which we can consistently estimate $\beta_1$ ? And some confidence intervals for $\beta_1$ ? A nice method is shown by @TomChen here Beta regression , but it doesn't work for a linear $p$ .","['regression', 'statistics', 'exponential-distribution', 'maximum-likelihood']"
4351757,Adjusting bounds of integration after a substitution with mutually dependent variables for a double integral,"I really have trouble with figuring out the correct bounds in such cases. Consider the substitution $$ x = function_{1}(u, v),\\y = function_{2}(u, v) $$ for the integral $$ \int_{p}^{q} \int_{j(y)}^{e(y)}f(x, y)dxdy. $$ How am I supposed to proceed? I understand, having searched the internet, that ""each case is different,"" but is there anything that could be thought of as a general rule? Examples I found that were supposed to explain Jacobians quite often (if not always) skip the adjustment of bounds. Of course, I could just plug in the newly introduced functions right away but then the outer integral's bounds depend on the inner integral's iterating variable, which is not supposed to happen. Since ""each case is different,"" I came up with a substitution the explanation of which I would appreciate. Please, do show me every detail of changing the integrals' intervals, the more detail the better. And any visuals will be much appreciated as well! The example: $$ \int_{a}^{b} \int_{1+b-y}^{1+b+y} \frac{1}{1-x^{2}y}dxdy,\\x = \sqrt2\frac{\tan v}{u},\\y = \frac{u^{2}\cos^{2}v}{2}. $$","['integration', 'jacobian', 'multivariable-calculus', 'change-of-variable', 'substitution']"
4351762,In what sense $\bigwedge(V\oplus W) \simeq \bigwedge V \otimes \bigwedge W$?,"According to the Wikipedia article on the exterior algebra, there is a natural isomorphism $$\bigwedge(V\oplus W) \simeq \bigwedge V \otimes \bigwedge W,$$ where $V,W$ is a finite-dimensional $\Bbbk$ -vector space. My question is, in what sense they are isomorphic? Isomorphic as a vector space? (this is trivial.) Or isomorphic as a $\Bbbk$ -algebra? I hope that they are isomorphic as a $\Bbbk$ -algebra, but I think this is not true due to the anticommutative nature. In particular, for $v_1,v_2\in V$ and $w_1,w_2\in W$ , $$(v_1\otimes w_1)(v_2\otimes w_2) = (v_1\wedge v_2) \otimes (w_1\wedge w_2)$$ in RHS but it is not true in LHS that $$v_1\wedge w_1 \wedge v_2 \wedge w_2 = v_1\wedge v_2\wedge w_1\wedge w_2.$$ (However, for symmetric algebra, analogous relation is true and the isomorphism is in the $\Bbbk$ -algebra sense.)
Is my reasoning correct?","['linear-algebra', 'exterior-algebra']"
4351768,Deterministic Policy Gradient Theorem Proof,"This paper states the deterministic policy gradient theorem. The proof of the theorem is provided in a supplement . This question is about the very first step in the proof. It begins with $$ \nabla_\theta V^{\mu_\theta}(s) = \nabla_\theta Q^{\mu_\theta}(s, \mu_\theta(s))
 = \nabla_\theta \left( r(s, \mu_\theta(s)) + \int_{\mathcal{S}} \gamma p(s' | s, \mu_\theta(s)) V^{\mu_\theta} (s')ds' \right)  $$ and then more steps follow.
The first equality is clear. This follows from the relation between the state value function $V$ and the state-action value function $Q$ . In the more general stochastic case, with a stochastic policy $\pi$ we have this relation: $$ V^\pi (s) = \sum_a \pi(a|s) Q^\pi(s, a). $$ When the policy $\pi$ is changed to $\mu_\theta $ and is deterministic, only 1 term in the sum remains and we get $$ V^{\mu_\theta}(s) = Q^{\mu_\theta}(s, \mu_\theta(s))
. $$ Now we apply $\nabla_\theta$ on each side. This is where the confusion starts. On the right side we can use the chain rule and we would get $$ \nabla_\theta V^{\mu_\theta}(s) = \nabla_\theta Q^{\mu_\theta}(s, \mu_\theta(s))
 = \nabla_\theta \mu_\theta(s) \nabla_a Q^{\mu_\theta}(s, a)|_{a=\mu_\theta(s)}  $$ and be done. Doesn't this directly prove the theorem? Surely I must have missed something. Can you please explain why this simple differentiation rule is not used in the proof and why the long substitution in the next step is needed, and where does that come from? Also, it would be helpful to see what do the Bellman equations look like for a deterministic policy? Many thanks",['multivariable-calculus']
4351841,Vitali set proof: Why is $1 \leq \sum_{k \in \mathbb{N}}m(\mathcal{N}_k) \leq 3$ impossible?,"So I am reading the construction of non-measurable set $\mathcal{N}$ . This is done so by considering an enumeration $\{r_k\}$ of $\mathbb{Q} \cap [-1,1]$ I am stuck on seeing the obviousness in why (by monotonicity) The following is a contradiction. So we claim and show the following Inclusion $$[0,1] \subset \bigcup_{k} \mathcal{N_k} \subset [-1,2]$$ Where $\mathcal{N}_k= \mathcal{N}+r_k$ I get that by monotonicity, we have $$1 \leq \sum_{k \in \mathbb{N}}m(\mathcal{N}_k) \leq 3$$ But I do not see the contradiction as to why since neither $m(\mathcal{N})=0$ nor $m(\mathcal{N})>0$ we are done by contradiction, can't the measure be strictly between 1 and 3? so 2?","['measurable-sets', 'measure-theory', 'lebesgue-measure']"
4351866,Differentiation of inner product with matrices,"Let $n \in \mathbb{N}  (n \neq 0)$ , $A$ a real $\mathbb{nxn}$ square matrix, and $\mathbf{c}$ a vector in $\mathbb{R}^{n}$ . Consider a real function $h: \mathbb{R} \longrightarrow \mathbb{R}, h \in C^{2}(\mathbb{R})$ , and introduce the function $g: \mathbb{R}^{n} \longrightarrow \mathbb{R}$ , defined by $$
g(\mathbf{x})=h\left(\langle A x, A x\rangle\right)- \langle \mathbf{c}, \mathbf{x} \rangle, \quad \forall \mathbf{x} \in \mathbb{R}^{n} .
$$ I want to compute $\nabla g$ and $H(g)$ (using only matrices and vector terms) My partial attempt: $$
g^{\prime}(x)=h^{\prime}(\langle A x, A x\rangle) \cdot \text { term }-c
$$ Now $
\langle A x, A x\rangle
$ is a sum  of terms of the form: $$
\left(\sum_{i=1}^{n} a_{s i} x_{i}\right)\left(\sum_{i=1}^{n} a_{s i} x_{i}\right)=\sum_{k_{1}+k_{2}, \ldots+k_{n}=2}^{n}\left(\begin{array}{c}
2 \\
k_{1}, k_{2}, \ldots, k_{n}
\end{array}\right) x^{k_{1}} \cdot \ldots \cdot x^{k_{n}}
$$ How to continue? is there any option to avoid using so detailed form? Thank you","['inner-products', 'calculus', 'matrix-calculus', 'multinomial-coefficients', 'derivatives']"
4351870,The Golden Ratio shows up in the cosine function. Do other related numbers do so as well?,"I just learned that $\cos\left(\frac{π}{5}\right)$ = $\frac{φ}{2}$ , $\cos\left(\frac{2π}{5}\right)$ = $\frac{1}{2φ}$ = $-\frac{ψ}{2}$ , $\cos\left(\frac{3π}{5}\right)$ = $-\frac{1}{2φ}$ = $\frac{ψ}{2}$ , and $\cos\left(\frac{4π}{5}\right)$ = $-\frac{φ}{2}$ , where $φ = \frac{1 + \sqrt{5}}{2}$ is the Golden Ratio and $ψ = \frac{1 - \sqrt{5}}{2}$ is the Golden Ratio Conjugate. It seems suggestive to me that the Golden Ratio involves the square root of five and also appears in the output of the cosine function for inputs proportional to the inverse of five. So I did some digging. The Golden Ratio is a member of the group of numbers which are solutions to the cubic equation $x^3 - a x - 1 = 0$ , with $a = 1$ giving the Plastic Ratio $ρ = \sqrt[3]{\frac{9 + \sqrt{69}}{18}} + \sqrt[3]{\frac{9 - \sqrt{69}}{18}}$ and two complex roots, and $a = 2$ giving the Golden Ratio $φ$ plus the Golden Ratio Conjugate $ψ$ and $-1$ . The value for $a = 3$ does not seem to have a name that I can find, but it does also show up in the cosine function. If we denote the real-valued roots as $R_n(a)$ , going from most positive at $n = 1$ to most negative at $n = 3$ , then $\cos\left(\frac{π}{9}\right) = \frac{R_1(3)}{2} ≈ \frac{1.879385}{2}$ , $\cos\left(\frac{2π}{9}\right) = -\frac{R_3(3)}{2} ≈ -\frac{-1.532089}{2}$ , $\cos\left(\frac{4π}{9}\right) = -\frac{R_2(3)}{2} ≈ -\frac{-0.347296}{2}$ , $\cos\left(\frac{5π}{9}\right) = \frac{R_2(3)}{2} ≈ \frac{-0.347296}{2}$ , $\cos\left(\frac{7π}{9}\right) = \frac{R_3(3)}{2} ≈ \frac{-1.532089}{2}$ , and $\cos\left(\frac{8π}{9}\right) = \frac{-R_1(3)}{2} ≈ -\frac{1.879385}{2}$ . Given that these two sets of roots for $a = 2$ and $a = 3$ show up, it makes me wonder if more do so as well. However, I haven't been able to find an way to fit the Plastic Ratio into the cosine function for $a = 1$ , or the unnamed roots for $a = 4$ . Do the roots for other values of $a$ appear in the output of the cosine function? Is the appearance of these two sets of roots purely a coincidence, or is there some deeper mathematical connection?","['trigonometry', 'roots', 'polynomials']"
4351955,Is $\sum_{j=1}^\infty\sum_{n=1}^\infty\left(\frac{e^{-j/n}}{n^2}-\frac{e^{-n/j}}{j^2}\right)=\gamma ?$,"A friend proposed the following problem : $$\sum_{j=1}^\infty\sum_{n=1}^\infty\left(\frac{e^{-j/n}}{n^2}-\frac{e^{-n/j}}{j^2}\right)=\gamma,$$ where $\gamma$ is the Euler-Mascheroni constant. The result I got is zero and here is what I did: $$\sum_{j=1}^\infty\sum_{n=1}^\infty\left(\frac{e^{-j/n}}{n^2}-\frac{e^{-n/j}}{j^2}\right)=\sum_{j=1}^\infty\sum_{n=1}^\infty\frac{e^{-j/n}}{n^2}-\sum_{j=1}^\infty\sum_{n=1}^\infty\frac{e^{-n/j}}{j^2}$$ (swap $n$ and $j $ in the first double sum then change the order of summations) $$=\sum_{j=1}^\infty\sum_{n=1}^\infty\frac{e^{-n/j}}{j^2}-\sum_{j=1}^\infty\sum_{n=1}^\infty\frac{e^{-n/j}}{j^2}=0.$$ Even Mathematica gives zero. Is it possible that this double sum equals $\gamma$ or my friend could be wrong? Thanks to @Thomas Andrews for noticing that its not valid to break up the summand as the double sums don't converge and that tells us that we can't always rely on results given by Mathematica or Wolfram.","['double-sequence', 'euler-mascheroni-constant', 'solution-verification', 'sequences-and-series']"
4351958,"Show that $\limsup_{s,\:t\:\to\:\tau}\left\|x(s)-x(t)\right\|_E\ge r$ implies $\left\|\Delta x(\tau)\right\|_E\ge r$","Let $E$ be a normed $\mathbb R$ -vector space, and $x:[0,\infty)\to\mathbb R$ be right-continuous. Assume $x$ $$x(t-):=\lim_{s\to t-}x(s)$$ exists for all $t\in O$ and let $$\Delta x(t):=x(t)-x(t-)\;\;\;\text{for }t\ge0.$$ Let $\tau\ge0$ and $r>0$ with $$\ell:=\limsup_{s,\:t\:\to\:\tau}\left\|x(s)-x(t)\right\|_E\ge r\tag1.$$ How can we conclude that $\left\|x(\tau)\right\|_E\ge r$ ? Intuitively, this should be easy to verify, but I'm unable to prove it rigorously. Since we can show that the assumptions imply that $x$ is bounded on each compact interval, we've clearly got $\ell<\infty$ . From this we can deduce that for all $\varepsilon>0$ , there is a $\delta>0$ with $$\left\|x(s)-x(t)\right\|_E<\ell+\varepsilon\tag2$$ for all $s,t\ge0$ with $\max(|s-\tau|,|t-\tau|)<\delta$ . In particular, $$\left\|x(\tau)-x(t)\right\|_E<\ell+\varepsilon\tag3$$ for all $t\in(\tau-\delta,\tau)$ . On the other hand, we have $$\left\|\Delta x(\tau)\right\|_E=\lim_{t\to\tau-}\left\|x(\tau)-x(t)\right\|_E\tag4$$ and hence there is a $\tilde\delta$ with $$\left|\left\|x(\tau)-x(t)\right\|_E-\left\|\Delta x(\tau)\right\|_E\right|<\varepsilon\tag5$$ for all $t\in(\tau-\delta\tilde,\tau)$ . But how can we conclude?","['stochastic-analysis', 'continuity', 'functional-analysis', 'real-analysis']"
4351968,Compatibility and consistency between two definitions of cohomology in two books (about coboundary operators and 1-cocycles and computing cohomology),"I was reading cohomology from Neukirch's book, and there he referenced to Hall's book. The two approaches are almost the same (are they not?), and they should give us the same results (cohomology groups). But I can not see their compatibility and consistency, and I can not recover them from each other. My problem is that: I can not propose the compatibility between these two books. Hall's book: 15.7. Cochains, Coboundcries, and Cohomology Groups. Given a double $\Omega$ -modulo $A$ we define $C^n = C^n(A, \Omega)$ to be the additive group of all functions $f$ of $n$ variables which range independently over $\Omega$ and taking values in $A$ , subject to the condition $$ (15.7.1) \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
f(\xi_1, \dots, \xi_n) = 0,$$ whenever at least one of the $\xi_i = 1$ . The elements of $C_n$ are called $n$ -dimensional cochains . $C^0 = A$ by definition and a zero dimensional
cochain is simply any element of A.
The coboundary operator $\delta$ maps each $C_n$ into the next, $C_{n+1}$ in
accordance with the rule $$(\delta f)(\xi_0, \xi_1, \dots, \xi_n)\\
=\xi_0f(\xi_1, \dots, \xi_n) \\ 
+\sum_{t=1}^{n} (-1)^t (\xi_0, \xi_1, \dots, \xi_{t-2}, \xi_{t-1}\xi_{t}, \xi_{t+1}, \dots, \xi_{n}) \\ 
+f(\xi_0\xi_1, \dots, \xi_{n-1})\xi_n . {\mbox something \ that \ I \ can \ not \ read}, 
$$ And let $Z^n=Z^n(A, \Omega)= \ker (C^n\longrightarrow C^{n+1})$ , and let $B^n=B^n(A, \Omega)= Im (C^{n-1}\longrightarrow C^{n})$ . The quotient group $Z^n/B^n$ is called the $n$ -dimensional cohomology group of the double $\Omega$ -module $A$ . We write it $$H^n(A, \Omega) = Z^n/B^n.$$ In the part something that I can not read , in the above formula , probably it is written $(-1)^{n+1}$ , but I am not sure. I have some confusions about coboundary operators and 1-cocycles and computing cohomology, and I can not find where the bugs are. In Neukirch's Class field theory, page 13, does the last summand of $d_q(\sigma_1, \dots, \sigma_q)$ written truly? Or should it be equal to $$d_q(\sigma_1, \dots, \sigma_q)= 
\sigma_1(\sigma_2, \dots, \sigma_{q}) 
+ 
\sum_{i=1}^{q-1} (-1)^i (\sigma_1, \dots, \sigma_{i-1}, \sigma_{i}\sigma_{i+1}, \sigma_{i+2}, \dots, \sigma_{q})
+ 
(-1)^q(\sigma_1, \dots, \sigma_{q-1})\sigma_q ?$$ I've seen something closely similar to these relations in the Hall's book (The theory of groups), but I think at least one of them should have typo. I know that they are not exactly the same, but I think they are not compatible unless for instance we replace the definition of Neukirch as above. Also I have ambiguity about cocycles and 1-cocycles. If the things in Neukirch's book are correct, then I expected the cocycle should be $f(\sigma\tau)=\sigma f(\tau)+f(\sigma)$ , and I am confused with $\delta(f(\sigma, \tau))=\sigma f(\tau)-f(\sigma\tau)+f(\sigma)\tau$ , and I can not see the compatibility.","['class-field-theory', 'group-theory', 'group-cohomology', 'homology-cohomology']"
4351990,What are the connections between representation theory and differential equations?,"I have just finished my undergrad and while I haven't studied much in representation theory I find it a very fascinating subject. My current interest is in differential equations, and I am wondering is there any ongoing research that combines these two areas?","['number-theory', 'representation-theory', 'ordinary-differential-equations', 'partial-differential-equations']"
4352052,Equivalence of triangulations and piecewise-linear triangulations in dimension $d\leq 4$,"There are two different notions of triangulations for a manifold $\mathcal{M}$ : A (simplicial) triangulation is an abstract simplicial complex $\Delta$ such that $\vert\Delta\vert\cong \mathcal{M}$ . A combinatorial triangulation is an abstract simplicial complex $\Delta$ such that $\vert\Delta\vert\cong \mathcal{M}$ with the extra property that the link of every vertex is PL-homeomorphic to the standard PL-sphere, i.e. the boundary of a single $d$ -simplex. (or in the case of manifolds with boundary, the link of a vertex is also allowed to be PL-homeomorphic to the standard PL-ball, i.e. a single $d$ -simplex; In this case, the corresponding vertex is on the boundary). Now, in sufficient high dimensions there are well-known examples of triangulations of manifolds, which are not combinatorial, like the famous example of $S^{5}$ constructed using the suspension theorem. However, I have often found the following claim (i.e. on wikipedia ): Every triangulation of a $d$ -dimensional manifold with $d\leq 4$ is
combinatorial. For $d=1$ the claim is of course trivial. Now, I have in essence two (very closely related) questions: Often, it is stated that the case $d=2,3$ is a consequence of the famous triangulation theorems of Rado and Moise-Bing. However, I can't understand why. These theorems ""just"" say that every $2$ (resp. 3)-dimensional manifold has a piecewise-linear structure (equivalently a combinatorial triangulation), which is unique up to PL-homeomorphism. Why does this imply that every triangulation is a combinatorial one? Do I miss something, or are there other, stricter results of these Theorems of which I am not aware of? What about the case $d=4$ ? Does anyone know a reference for this? I have heard somewhere that this case is equivalent to Poincare's conjecture, however, I do no remember where. I know that not every $4$ -manifolds admits a triangulation (like $E_{8}$ ). Furthermore, I know that in $d=4$ , the smooth and PL-category are equivalent, but I do not know if this helps.","['triangulation', 'combinatorics', 'manifolds', 'general-topology', 'low-dimensional-topology']"
4352154,In how many ways can we distribute $2$ types of gifts?,"The problem: In how many ways can we distribute $2$ types of gifts, $m$ of the first kind and $n$ of the second to $k$ kids, if there can be kids with no gifts? From the stars and bars method i know that you can distribute m objects to k boxes in $\binom{m+k-1}{k-1}$ ways. So in my case i can distribute m gifts to k kids in $\binom{m+k-1}{k-1}$ ways, same for n gifts i can distribute them in $\binom{n+k-1}{k-1}$ ways. So now if we have to distribute m and n gifts we can first distribute m gifts in $\binom{m+k-1}{k-1}$ ways, then n gifts in $\binom{n+k-1}{k-1}$ ways, so in total we have: $$\binom{m+k-1}{k-1} \cdot \binom{n+k-1}{k-1} \quad \textrm{ways.}$$ Is my reasoning correct? What about when we have to give at least 1 gift to each kid, can we do that in $$\binom{m-1}{k-1} \cdot \binom{n+k-1}{k-1}+\binom{n-1}{k-1} \cdot \binom{m+k-1}{k-1}  \quad \textrm{ways?}$$ Any help would be appreciated.","['combinations', 'combinatorics', 'discrete-mathematics']"
4352201,Is Zorn's lemma applicable to this integration problem?,"Please consider the following problem and my proposed solution. Suppose $(X,\mathcal{M},μ)$ is a measure space, and $\mathcal{F}$ is a family of non-negative integrable
functions on $X$ with the following properties: (a) If $φ, ψ ∈F$ , then $φ + ψ ∈ \mathcal{F}$ . (b) If $φ, ψ ∈F$ , then $\max\{φ,ψ\}∈\mathcal{F}$ . (c) If $f$ is measurable, $f ≥ 0$ and $∫f dμ > 0$ , then there is $φ ∈ \mathcal{F}$ such that $φ ≤ f$ and $∫φdμ > 0$ . Prove that if $f$ is non-negative and measurable then $$
\int f dμ = \sup\left\{ \int \varphi \ \mathrm{d}\mu : \varphi \in \mathcal{F}, \varphi \le f\right\}$$ My Attempt Assume that $\int f \ \mathrm{d}\mu > 0$ . Then $\int f \ \mathrm{d}\mu$ is clearly an upper bound for the set above. We must prove that it is the least upper bound. Select $\varphi_1 \in \mathcal{F}$ such that $\varphi_1 \le f$ and $∫φ_1dμ > 0$ . If $\varphi_1 = f$ a.e., then we are done, otherwise $\int (f - \varphi_1) \mathrm{d}\mu > 0$ and hence there is $\varphi_2 \in \mathcal{F}$ such that $\varphi_2 \le f - \varphi_1$ , or $\varphi_1 + \varphi_2 \le f$ and $∫φ_2 \ dμ > 0$ . If $\varphi_1 + \varphi_2 \ne f$ a.e., then similarly we can find $\varphi_3$ such that $\varphi_1 + \varphi_2 + \varphi_3\le f$ and $∫φ_3 \ dμ > 0$ . We may continue this process, if it does not end after $n$ steps to get $$
\sum_{k=1}^n \varphi_n \le f.
$$ For each $n$ , define $\Phi_n = \max\{\varphi_1,\dots,\varphi_n\}$ . Then $\Phi_n \in \mathcal{F}$ using induction and property (b), and $\Phi_n \le \Phi_{n+1}$ . Hence $\Phi_n \to \Phi$ for some measurable function $\Phi$ , and by the monotone convergence theorem, $$
\int \Phi_n \mathrm{d} \mu = \int \Phi \mathrm{d}\mu.
$$ If $\int \Phi \mathrm{d}\mu \ne \int f \mathrm{d}\mu$ , then we may repeat this procedure yet again, starting instead with $f- \Phi$ . If $\int f \mathrm{d} \mu = \infty$ , then we can repeat this procdure indefinitely to find $\Phi$ so that $\int \Phi \mathrm{d}\mu$ exceeds any finite bound. It seems to me that since we can always extend this procedure, we must eventually reach a maximal $\Phi$ and this $\Phi$ must be $f$ , otherwise we may still extend. Now I am thinking of Zorn's lemma, but I am not exactly sure how it applies. I would like some clarification in that regard, or a different approach to the problem if possible. Thanks in advance.","['measure-theory', 'lebesgue-integral', 'ordinals']"
4352215,Probability that given weight is on the heavier side of a balance,"Assume we have 6 balls weighting 101, 102, 103, 104, 105 and 106 in kg. If we randomly pick 3 balls and place them on one side of the scale and the other 3 left are placed on the other side of the scale. What is the probability that the ball weighting 106 kg is on the heavier side?","['combinations', 'combinatorics', 'probability-theory', 'probability']"
4352347,What is the minimum of $BP+\frac{1}{2}CP$?,"In $Rt\triangle ABC,\ \angle A=90^{\circ},\ AB=4,\ AC=6.$ The radius of $\odot A$ is $2$ . What is the minimum of $BP+\frac{1}{2}CP$ ? Actually, the question should be ""What is the minimum of $BP+\frac{1}{3}CP$ "", and then it will be simple by using similarity. But I still wonder how to deal with this question. One possible way I've got is to put it in Rectangular Coordinates and assume $P(p\ ,\ \sqrt{4-p^2})$ . By using $d=\sqrt{(x_1-x_2)^2+(y_1+y_2)^2\ }$ I can write $BP+\frac{1}{2}CP$ with $p$ . And then it can be worked out by calculating the minimum of the function $f(p)=BP+\frac{1}{2}CP$ . However , it is clear that it is very hard to work out. I tried to work it out with better and easier methods but failed. Are there other possible approaches? Thank you for your ideas in advance!","['euclidean-geometry', 'circles', 'geometry']"
4352396,Understanding weakly dense.,"Let $c_0 = \{x \in l^\infty \ | \ x_i \ \to 0 \}$ . Show that $c_0$ is not weakly dense in $l^\infty$ . This means that the closure of $c_0$ w.r.t the weak topology is not $l^\infty$ , right? Does that mean that $\exists x\in l^\infty$ such that $\not\exists (x_1, x_2, ....), x_i \in c_0: x_i \to^w x $ ?","['functional-analysis', 'weak-convergence']"
4352400,Find the area $ S_ {OGBH}$ in the figure below,"For reference: In the figure, calculate area $ S_ {OGBH} $ if the triangle area $ABC=9\ \mathrm{m^2}$ and $AB=3\ \mathrm m$ and $AO = 2\ \mathrm m$ . (Answer: $5\ \mathrm{m^2}$ ) My progress: Draw $BO.$ Let $AG = x$ and $HO=GO=R.$ $BG = BH.$ $S_{BGO} = S_{BOH}$ $\displaystyle \frac{9}{S_{CHO}} = \frac{BC\cdot CH}{AC\cdot CO}$ $\displaystyle \frac{S_{BGO}}{S_{CHO}}=\frac{R\cdot BG}{R\cdot CH}=\frac{BG}{CH}$ $\displaystyle \frac{S_{AGO}}{S_{BGO}}=\frac{Rx}{R\cdot BG}=\frac{x}{BG}$ $\displaystyle S_{OAG} = \frac{Rx}{2}$ $\displaystyle \frac{S_{ABC}}{S_{ABO}} = \frac{3AC}{3\cdot2}\implies \frac{9}{S_{ABO}} = \frac{3AC}{6}\implies S_{ABO}=\frac{18}{AC}$ ....???","['euclidean-geometry', 'area', 'geometry', 'plane-geometry']"
4352414,Hölder and Slobodeckij spaces,"For a bounded domain $\Omega\subset\mathbb{R}^d$ , $s\in(0,1)$ , and $p\in(1,\infty)$ , the Sobolev-Slobodeckij seminorm is given by $$
[f]_{W^{s,p}}^p=\int_\Omega\int_\Omega\frac{|f(x)-f(y)|^p}{|x-y|^{d+sp}}dxdy.
$$ A) Is it true and B) if it is, is there a direct proof (without going into fractional Laplacians, interpolation spaces, etc.) that $s$ -Hölder continuous functions have finite $[f]_{W^{s,p}}$ seminorm? Naively bounding $|f(x)-f(y)|$ by the Hölder norm times $|x-y|^s$ just barely fails.","['functional-analysis', 'real-analysis']"
4352436,Putnam 1988 - Exercise A.5 - Generalization and solution verification,"I tried to generalize the question given in 1988 Putnam competition, A.5, as follows: Prove that, for all real positive $\alpha$ , there exists a unique function $f$ from the set $\mathbb{R}^+$ of positive real numbers to $\mathbb{R}^+$ such that $$f(f(x)) = \alpha x-f(x) \tag{1}\label{1}.$$ My proof is as follows. For any positive $\alpha$ we have a linear solution given by $f(x) = kx,$ with $$k=-\frac12 + \frac{\sqrt{1+4\alpha}}2>0.$$ Suppose there is another solution $g(x)$ such that, for some $a_0>0$ , $g(a_0) < ka_0$ , i.e., $g(a_0) = (k-\varepsilon_0) a_0,$ with $0 < \varepsilon_0 < k$ . Two iterations of \eqref{1} (recalling that $\alpha =k^2+k$ ) yield \begin{eqnarray}
g(g(g(a_0))) &=& g((k^2+\varepsilon_0)a_0) = g(a_1) =\\
&=&k(k+1)(k-\varepsilon_0)a_0 -(k^2+\varepsilon_0)a_0 =\\
&=&\left(k-\frac{k^2+2k+1}{k^2+\varepsilon_0}\varepsilon_0\right)a_1=\\
&=&(k-\varepsilon_1) a_1.
\end{eqnarray} Since $\varepsilon_0 < k< 2k$ , we have that $$\varepsilon_1>\left(1+\frac{1}{k^2+k}\right)\varepsilon_0.$$ If $\varepsilon_1 > k$ we have found a contradiction, because $g(a_1) < 0$ . Otherwise we can proceed further with the iterations, and generate $$\varepsilon_n >\left(1+\frac{1}{k^2+k}\right) \varepsilon_{n-1}> \cdots > \left(1+\frac{1}{k^2+k}\right)^n \varepsilon_0.$$ Since $\lim_{n\to \infty}\varepsilon_n = +\infty$ , for $n$ large enough we obtain $g(a_n) < 0$ . If, on the other hand, we find a $g(x)$ such that, for some $a_0$ , we get $g(a_0) > ka_0$ , that is $g(a_0) = (k+\varepsilon_0) a_0$ for some $\varepsilon_0>0$ , then one iteration of \eqref{1} gives \begin{eqnarray}
g(g(a_0)) &=& g((k+\varepsilon_0)a_0)= g(a_1)=\\
&=&k(k+1)a_0 - (k+\varepsilon_0)a_0 =\\
&=&\frac{k^2-\varepsilon_0}{k+\varepsilon_0}a_1=\\
&=&\left(k-\frac{k+1}{k+\varepsilon_0}\varepsilon_0\right)a_1<ka_1.
\end{eqnarray} And therefore we can again proceed as in the first case shown above. $\blacksquare$ Can you check if The problem statement is true, and if my solution is valid?","['contest-math', 'functions', 'solution-verification', 'real-analysis']"
4352454,How to calculate the volume of this set using spherical coordinates?,"Let $$K = \left\{(x, y, z) \in \mathbb R^3 ; \; x^2 + y^2 + z^2 < b^2, \, x^2 + y^2 > a^2 \right\}.$$ How to calculate the volume of $K$ using spherical coordinates? I know that the conditions translate to $r < b$ and $r \sin \theta > a$ but I really struggle to get the right integral. Could someone explain, please?","['integration', 'volume', 'real-analysis', 'multivariable-calculus', 'spherical-coordinates']"
4352480,Why are there two possible values for the degree of the ODE $\frac{\mathrm{d}^3}{\mathrm{dx^3}}(\frac{\mathrm{d}^2y}{\mathrm{dx^2}})^{-3/2}=0$?,"I want to find the order and degree of the ODE $\frac{\mathrm{d}^3}{\mathrm{dx^3}}(\frac{\mathrm{d}^2y}{\mathrm{dx^2}})^{-3/2}=0$ ? I'm getting two different values of the degree of the ODE. Simplifying gives: $-\frac 52 (y'')^{-\frac 72}(y''')^2+y'''' (y'')^{-5/2}=0$ We have two possible continuations from here. Continuation $1$ : Multiplying both sides by $(y'')^{\frac 72}$ yields: $-\frac 52 (y''')^2+\color{blue}{y''''}y''=0$ . From here, the order is $4$ and the degree is $1$ . Continuation $2$ : $-\frac 52 (y'')^{-\frac 72}(y''')^2+y'''' (y'')^{-5/2}=0\implies\frac {25}4(y''')^4(y'')^{-7}=(\color{blue}{y''''})^2 (y'')^{-5}$ and multiplying both sides by $(y'')^{-7}$ suggests that the ode has order 4 and the degree equal to $2$ . So which one of the apparently two possible degrees is correct? Please help. Thanks.","['calculus', 'ordinary-differential-equations', 'real-analysis']"
4352483,Explanation of Durrett example 5.2.13,"I am referring to Durrett's "" Probability Theory and Examples"". I am not including everything that's in the example as given in the book but only the relevant part for which I need explanation Example 5.2.13: Consider a simple symmetric random walk $S_n = S_{n-1}+\zeta_n$ , where $\zeta_n$ takes values $1$ or $-1$ with probability $\frac{1}{2}$ each. Given $S_0 = 1$ Consider $\tau = inf \{n: S_n = 0\}$ . Then, I know that $X_n = S_{min(\tau,n)}$ is a non-negative martingale, (so by martingale convergence theorem) $X_n$ converges to a random variable, call it $X$ . Clearly, X is non-negative almost surely, and $X$ is finite almost surely. The part I don't understand is how the author argues that $X = 0$ almost surely. The author's argument as given in the book: "" $X_n$ converges to a limit $X$ , which is finite almost surely, that must be 0, since convergence to $k > 0 $ is impossible (If $X_n = k$ , then $X_{n+1} = k+1$ or $k-1$ )"" Can anyone clarify how ""(If $X_n = k$ , then $X_{n+1} = k+1$ or $k-1$ )"" this implies $X = 0 $ almost surely?","['martingales', 'measure-theory', 'probability-theory', 'random-walk']"
4352499,Showing that $X(t)=\int_0^tB(s)ds $ is a Gaussian process.,"The time integration of Brownian motion is given by $X(t)=\int_0^tB(s)ds$ . In order to show that $X(t)$ is a gaussian process I work with the definition that  for any $t_0<t_1 < t_2<\ldots <t_{n-1}$ the random variable $$ \langle v,(X_{t_0}, ... , X_{t_{n-1}}) \rangle $$ is again Gaussian. I tried to write out the quantity in terms of Riemann sums i.e. $$ X(t) = \lim_{n\to {\infty}}\sum_{k=0}^{n-1}B(t_k)(t_{k+1}-t_k) $$ But got stuck with all the indices, how to continue? We can assume that limits of gaussian random variables are again Gaussian.","['stochastic-calculus', 'stochastic-processes', 'brownian-motion', 'probability-theory', 'probability']"
4352557,Deducing $\int_0^{\pi}\log \sin x dx =-\pi\log 2$ from $\int_0^{\pi}\log (-2ie^{ix}\sin x) dx = 0$,"On Ahlfors Complex Analysis, chpter 5.3, the author explains how to evaluate $\int_0^{\pi}\log \sin x dx =-\pi\log 2$ . He does so by using Cauchy's formula to deduce that $$\int_0^{\pi}\log (-2ie^{ix}\sin x) dx  = 0.$$ I get why the above is true, thanks to Ahlfors' explanation in the texbook. However, he claims the following: If we choose $\log {e^{ix}}= ix$ the imaginary part lies between $0$ and $\pi $ . Therefore, in order to obtain the principal branch with an imaginary part between $-\pi $ and $\pi $ , we must choose $\log (-i) = -\pi i / 2$ . The equation can now be written in the form $$ \pi \log 2 - (\frac {\pi^2}{2})i + \int_0^\pi\log\sin x dx + (\frac {\pi^2}{2})i = 0.$$ What does the author mean by this? I know that $\log (z_1z_2) = \log z_1 + \log z_2 + 2\pi i n $ for some integer $n$ depending on $z_1$ and $z_2$ , and I imagine that the author is trying to make the $m$ zero where $m $ is the integer satisfying $\log (-2ie^{ix}\sin x) = \log 2 + \log (-i) + \log {e^{ix}} + \log\sin x + 2\pi i m $ but I do not get how exactly he is coming up with a branch of a logarithm.","['logarithms', 'complex-analysis', 'branch-cuts', 'complex-integration', 'complex-numbers']"
4352564,Trouble with extending Doob's Optional Stopping Theorem,"Let $\tau\geq0$ be a stopping time, $\mathbb{E}\tau<\infty$ . Show $\{\tau\geq k\}\in\mathcal{F}_{k-1}$ . Based on the identity $$
|X_{T\land n}|= \bigg|\sum_{k=1}^n(X_k-X_{k-1})\cdot\mathbf{1}\{\tau\geq k\}\bigg|\leq\sum_{k=1}^n|X_k-X_{k-1}|\cdot\mathbf{1}\{\tau\geq k\}
$$ show the following: If $X$ is a supermartingale for which there exists a $C\in\mathbb{R}$ with $$
\mathbb{E}(|X_k-X_{k-1}|\big|\mathcal{F}_{k-1})\leq C\quad\forall k>0,\text{ a.s.},
$$ then $\mathbb{E}X_\tau\leq\mathbb{E}X_0$ . Of course we have equality in case $X$ is a martingale. The first part is easy, but I am struggling with understanding the solution to the second part. The solution it gives for the second part goes as follows: $$
\begin{align*}
\mathbb{E}\sum_{k=1}^\infty\big(|X_k-X_{k-1}|\cdot\mathbf{1}\{\tau\geq k\}\big)&=\sum_{k=1}^\infty\mathbb{E}\big(|X_k-X_{k-1}|\cdot\mathbf{1}\{\tau\geq k\}\big)\\
&=\sum_{k=1}^\infty\mathbb{E}\mathbb{E}\big(|X_k-X_{k-1}|\cdot\mathbf{1}\{\tau\geq k\}\big|\mathcal{F}_{k-1}\big)\\
&=\sum_{k=1}^\infty\mathbb{E}\big[\mathbf{1}\{\tau\geq k\}\cdot\mathbb{E}\big(|X_k-X_{k-1}|\big|\mathcal{F}_{k-1}\big]\\
&\leq C\sum_{k=1}^\infty\mathbb{E}\mathbf{1}\{\tau\geq k\}=C\sum_{k=1}^\infty\mathbb{P}\{\tau\geq k\}=C\mathbb{E}\tau<\infty.
\end{align*}
$$ So far so good. But then the next step in the solution confuses me. It says that we can now apply dominated convergence and a.s. finiteness of $\tau$ to claim $$
0\geq\lim_{n\to\infty}\mathbb{E}(X_{T\land n}-X_0)=\mathbb{E}X_\tau-\mathbb{E}X_0.
$$ However, what confuses me, is that I thought to apply dominated convergence we needed to show that the variable itself could be bounded, not just the expectation of the variable. I.e. the statement of the dominated convergence theorem I have is: Let $Y,X,X_1,X_2,X_3,\dots$ be random varialbes, and assume $|X_n|\leq Y$ for each $n$ , $\mathbb{E}Y<\infty$ , and $X_n\to X$ almost surely. Then $\mathbb{E}X_n\to\mathbb{E}X$ . It seems that this statement does not apply directly to this question. What am I missing here that has allowed the use of dominated convergence in this situation? Any help is sincerely appreciated!","['stochastic-analysis', 'stochastic-processes', 'martingales', 'stopping-times', 'probability-theory']"
4352575,Dimension of the variety of rank 1 decompositions of a matrix,"Let $A\in\mathrm{GL}_n(\mathbb{C})$ , and let's consider its decompositions into a sum of rank 1 matrices $$A=\sum_{i=1}^t A_i,\ \text{rank}(A_i)=1,$$ where $t\geqslant n$ . When $t=n$ , Wikipedia claims that if we write $A_i=u_iv_i^T$ for some vectors $u_i,v_i\in \mathbb{C}^n$ and collect all the vectors into matrices $U=[u_i]_{i=1}^n,\ V=[u_i]_{i=1}^n$ , then for a general matrix $X\in\mathrm{GL}(\mathbb{C})$ defining $c_i,d_i\in \mathbb{C}^n$ by $$UX^{-1}=[c_i]_{i=1}^n,\ VX^{T}=[d_i]_{i=1}^n,$$ yields a new decomposition. It refers to Joe Harris' ""Algebraic Geometry: A First Course"", but I couldn't find this result there. It's also not immediately clear why acting by $X$ like this on a decomposition yields all decompositions. But I am also interested in the more general question when $t>n$ , especially when furthermore $t\geqslant 2n-1$ , which is the dimension of the space of rank 1 matrices. What can be said about such decompositions?","['matrix-rank', 'algebraic-geometry', 'linear-algebra', 'tensor-rank']"
4352591,Construct $f$ satisfying certain given conditions,"Given $\textbf x\in \mathbb R^n$ and $r>0$ , construct a function $f:\mathbb R^n\to [0,1]$ of class $C^\infty$ such that $$f^{-1}(1)=\overline{B(\textbf{x},r/2)}\\f^{-1}(0)=\mathbb R^n\setminus B(\textbf{x},r)$$ Being helpless about the original question (apart from the geometrical realization that it looks like a peninsula), I was trying the $n=1$ variant out. This translates to Given $a<b$ , find a function $g:\mathbb R\to [0,1]$ of class $C^\infty$ such that $$g|_{(-\infty,a]}\equiv1\\g|_{[b,\infty)}\equiv0$$ Even this one seems quite out of my reach. The first idea I had was to use the Sigmoid function. But, that never gives a zero derivative except at $x\to \pm \infty$ . So, my next idea was to use the $C^\infty$ property of the function $y=e^{-\frac 1x}$ and construct $g$ locally at $x=a$ using the function $$\alpha(x)=1-e^{-\frac 1{x-a}}$$ and locally at $x=b$ using the function $$\beta(x)=e^{\frac 1{x-b}}$$ But, I couldn't proceed with this as well.","['analysis', 'multivariable-calculus', 'calculus', 'functions', 'derivatives']"
4352592,Evaluating Integral of $x_1^{b_1-1}\cdots x_k^{b_k-1}$ over $x_1+\dots+x_k\leq 1$,"I am struggling with this Integral: $$\idotsint\limits_{\begin{subarray}{l}x_1\ +\ \dots\ +\ x_k\ \leq\ 1 \\[1mm] x1,\,\dots\,,\,x_k\ \geq\ 0 \end
{subarray}} x_1^{b_1-1}\cdots x_k^{b_k-1} dx_1 \cdots dx_k
$$ for $b_1,\dots,b_k>0$ . My attempt so far is to use Fubini's theorem and get to $\frac{x_1^{b_1}\cdots x_k^{b_k}}{b_1\cdots b_k}$ but I am struggling to evaluate for $\Big|_{\begin{subarray}{l}x_1+\dots+x_k\leq 1 \\ x1,\dots,x_k \geq 0 \end
{subarray}}$ . I have also thought about using induction and while $k=1$ is clear, I'm having a hard time with $k\geq 2$ . Edit: The integral evaluates to $\frac{\Gamma(b_1)\cdots \Gamma(b_k)}{\Gamma(1 + b_1 + \cdots + b_k)}$ . I have proven this in a way similar to @Shannon Starr's answer. I now want to evaluate the integral using a substitution of polar coordinates. I already proved that in polar coordinates: \begin{align}
x_1&=r \cos(\varphi_1)\\
x_2&= r \sin(\varphi_1)\cos(\varphi_2)\\
&\vdots\\
x_{k-1}&=r\sin(\varphi_1)\cdots \sin(\varphi_{k-2})\cos(\varphi_{k-1})\\
x_k&=r\sin(\varphi_1)\cdots \sin(\varphi_{k-2})\sin(\varphi_{k-1})
\end{align} and the calculation for $x_i^{b_k-1}$ is obvious. I was however given the hint of calculating this by substituting the $\textbf{square} $ of the polar coordinates, so I am not sure how to proceed. Edit edit: I would just like to emphasize that I am looking for a solution using the substitution \begin{align}
x_1&=(r \cos(\varphi_1))^2\\
x_2&= (r \sin(\varphi_1)\cos(\varphi_2))^2\\
&\vdots\\
x_{k-1}&=(r\sin(\varphi_1)\cdots \sin(\varphi_{k-2})\cos(\varphi_{k-1}))^2\\
x_k&=(r\sin(\varphi_1)\cdots \sin(\varphi_{k-2})\sin(\varphi_{k-1}))^2
\end{align}","['integration', 'definite-integrals', 'analysis']"
4352599,How to understand what a 'noncommutative space' is,"Whilst acquainting myself with the fundamentals of $C^*$ -algebras and their $K$ -theory, I read about how Gelfand Duality allows various $C^*$ -algebraic concepts to be directly translated into a geometrical/topological context and vice versa. I briefly relay my current understanding of Gelfand duality below for the sake of clarity. Each commutative $C^*$ -algebra $A$ has a spectrum $\Delta$ consisting of its non-zero complex homomorphisms, which is a locally compact Hausdorff space with the Gelfand topology. Conversely, for each locally compact Hausdorff space $X$ , the algebra of continuous complex-valued functions on $X$ that vanish at infinity forms a commutative $C^*$ -algebra $C_0(X)$ w.r.t. pointwise conjugation and the supremum norm $f\mapsto\|f\|_\infty$ . The Abstract Spectral Theorem (Theorem 2.13 here ) states that each commutative $C^*$ -algebra $A$ with (necessarily non-empty) spectrum $\Delta$ is canonically isomorphic to $C_0(\Delta)$ . This isomorphism is induced by the Gelfand transforms. Conversely, mapping each $x\in X$ to its point evaluation $f\mapsto f(x):C_0(X)\to\mathbb{C}$ gives a canonical homeomorphism $X\to \Delta(C_0(X))$ . In a similar manner, we can match up proper continuous maps between LCH spaces with $*$ -homomorphisms, and find that $C_0$ is a two-way contravariant functor from the category of LCH spaces to the category of commutative $C^*$ -algebras. The following table [ Basic Noncommutative Geometry , Khalkhali, pg. 7] describes some correspondences that result from the Gelfand duality. Space Algebra compact unital 1-point compactification unitization Stone–Cech compactification multiplier algebra closed subspace; inclusion closed ideal; quotient algebra surjection injection injection surjection homeomorphism automorphism Borel measure positive functional probability measure state disjoint union direct sum Cartesian product minimal tensor product So commutative $C^*$ -algebras correspond with locally compact Hausdorff spaces. Based on my rather limited understanding of noncommutative geometry, it would appear that Gelfand duality may be extended to arbitrary (not necessarily commutative) $C^*$ -algebras, whose geometrical analogue is a noncommutative space . But what exactly is a noncommutative space? I am aware that it might be difficult or downright impossible to give an explicit definition, so I simply hope to gain a better intuition here. How can I make sense of noncommutative spaces?","['noncommutative-geometry', 'c-star-algebras', 'definition', 'gelfand-duality', 'general-topology']"
4352606,"Let $X_n,Y_n$ be two sequences of random variable, where $0<X_n<Y_n$. Does $Y_n=O_p(1)$ imply $X_n=O_p(1)?$","Let $X_n,Y_n$ be two sequences of random variable, where $0<X_n<Y_n$ . Does $Y_n=O_p(1)$ imply $X_n=O_p(1)?$ My proof Since $Y_n=O_p(1)$ , $$
(\forall \epsilon>0)(\exists k)\Big(P(|Y_n|>k)\leq \epsilon\Big)\ \ 
$$ Next, by law of total probability $$
P(|X_n|>k)=\underbrace{P(|Y_n|>k)}_{\leq \epsilon}\underbrace{P(|X_n|>k|\ |Y_n|>k)}_{<1}+\underbrace{P(|Y_n|\leq k)}_{\geq 1-\epsilon }\underbrace{P(|X_n|>k|\ |Y_n|\leq k)}_{0}\leq \epsilon
$$ So the same $k$ works. Is my proof correct? I will also accept any alternative proof (or counter proof) as answer.","['statistics', 'conditional-probability', 'real-analysis', 'solution-verification', 'probability-theory']"
4352659,Theorem 18.1 of Munkres’ Topology,"Let $X$ and $Y$ be topological spaces; let $f:X\to Y$ . Then the following are equivalent: (1) $f$ is continuous (2) for every subset $A$ of $X$ , one has $f(\overline{A}) \subseteq \overline{f(A)}$ . (3) for every closed set $B$ of $Y$ , the set $f^{-1}(B)$ is closed in $X$ . (4) for each $x\in X$ and every neighbourhood $V$ of $f(x)$ , there is a neighbourhood $U$ of $x$ such that $f(U)\subseteq V$ . Now there are lots of way to prove this theorem, like $(1)\Rightarrow (2)\Rightarrow (3)\Rightarrow (4) \Rightarrow (1)$ , I think this is the least number of implication needed to prove this theorem(Note I’m not saying it is the most easiest way to do it) and $(1)\Leftrightarrow (2)$ , $(1) \Leftrightarrow (3)$ , $(1)\Leftrightarrow (4)$ , this is maybe the most number of implication needed to prove this theorem. You would agree proof of this $(1)\Leftrightarrow (3)$ is trivial/easy and $(1)\Leftrightarrow (4)$ is decent exercise. We have already show $(1)\Rightarrow (2)$ . How do we show $(2)\Rightarrow (1)$ ? Can we use the similar approach as in the proof of $(2)\Rightarrow (3)$ ? i.e. let $V\in \mathcal{T}_{Y}$ . We need to show $X-f^{-1} (V)$ is closed in $X$ . Put $A= X-f^{-1} (V)$ . Let $x\in \overline{A}$ . So $f(x)\in f(\overline{A})\subseteq \overline{f(A)}$ . $f(A) \supseteq f(X) - V= Y- V$ . Thus $Y-V \subseteq \overline{f(A)}
$ . Here I’m stuck.","['alternative-proof', 'continuity', 'general-topology', 'proof-writing']"
4352667,"Is $f(x) = 1/x$ over $[0.1 , 1]$ uniformly continuous but $1/x$ on $(0,1)$ is not?","I know that there is a theorem that says that ""Every continuous function on a bounded closed interval $[a,b]$ is uniformly continuous therein."" But I know that the function $f(x) = 1/x$ defined on (0,1) is not uniformly continuous (I know how to prove this). Does the previous theorem is saying that $f(x) = 1/x$ defined on [0.1,1] is uniformly continuous? why? what should I change in my proof of being not uniform continuous. Here is my proof of not being uniformly continuous: Let $\delta > 0,$ take $\epsilon = 1$ and $x = \min \{1, \delta\}$ and $a = \frac{x}{2}.$ then $|x - a | = |x - x/2| = \frac{x}{2} < \delta$ but $|\frac{1}{a} - \frac{1}{x}| = |\frac{2}{x} - \frac{1}{x}| = \frac{1}{x} > 1.$ And so $f$ is not uniformly continuous. Also, my justification that it is continuous because it is the division of 2 polynomials (and in this justification I do not see any use of open or closed intervals) Could anyone help me in answering my questions, please? Thanks in advance!","['continuity', 'uniform-continuity', 'analysis', 'real-analysis']"
4352690,Finding the error of $f''(x) \approx \frac{f(x+2h) - 2f(x+h) + f(x)}{h^2}~$,"I'm having some trouble with the following exercise: Deduce the following approximation: $$f''(x) \approx \frac{f(x+2h) - 2f(x+h) + f(x)}{h^2}$$ for small values of $h$ , and find an expression for the error commited when using this approximation. I was able to get to the expression but I don't know how to get an expression for the error. In my classnotes my teacher claimed that the error commited in the approximation: $$f''(x) \approx \frac{f(x+h) - 2f(x) + f(x-h)}{h^2}$$ is $$E=-\frac {h^2}{12}f^{(4)}(t)$$ for some $t \in (x-h,x+h)$ , but my teacher didn't show us the proof and thus I have no reference to do this problem. How can I find an expression for the error?","['numerical-calculus', 'numerical-methods', 'derivatives']"
4352728,Why don't we define algebraic variety by an atlas of charts with polynomials transition maps?,"The question is in the title but I will give details. Origin of this question: I ask myself this question because I'm trying to legitimate the definition of algebraic variety (and then I would be able to legitimate schemes: it is just a simple generalization since point of an algebraic variety over a algebraically closed field are in bijection with maximal ideal). I recall you for what follow that category of manifold define by an atlas (say $\mathcal{C}^{k}$ -manifold but it work for analytic manifold and topological to) is equivalent to the category of ringed space that are locally isomorphic to the ring space of the $\mathcal{C}^{k}$ function of an open set of $\mathbb{R}^{n}$ (if we consider for example real manifold). See here for a rigorous statement and proof of this result https://pub.math.leidenuniv.nl/~edixhovensj/teaching/2010-2011/tag/sunil1.pdf . What is my question: For me definition of (topological, $\mathcal{C}^{k}$ ,analytic) variety by an atlas of charts is more intuitive, more natural than the one using sheaves (even if sheaves are better to work with). So I'm asking: Why have I never heard about algebraic variety define by an atlas of charts (where the charts are homeomorphic to classical open subset of $\mathbb{R}^{n}$ , not Zariski) with polynomial transition maps (instead of $\mathcal{C}^{k}$ or analytic transition maps)? If it exists does someone know a reference? Since the ""good definition"" of algebraic variety involve Zariski topology, that means the objects of my question 1 above (i.e. topological spaces endowed with polynomials atlas) are not satisfying.. But in what sense? I think it is because the relation with sheaves is not true in that case but does anyone understand why (if I'm correct)? An example will be be great! In conclusion I'm just trying to understand why the ""naïve definition"" of algebraic variety, similar to the definition of the over kind of variety is not satisfying and why we HAVE TO consider Zariski topology. EDIT: I could (and maybe I should) have asked the same question but with rational transition maps instead of polynomial.","['algebraic-geometry', 'differential-geometry']"
4352737,Is my approach to showing $(x\in A)\Rightarrow(\exists y\in B\cap C)$ correct?,"I wish to prove the converse of the statement in my previous question : Let $X$ and $Y$ be arbitrary sets. Further, let $A$ be a subset of $X,$ and let $B,C$ be subsets of $Y.$ I am currently trying to prove a statement of the form \begin{gather}
(x\in A)\Rightarrow(\exists y\in B\cap C).
\end{gather} My approach is to prove the (equivalent) contrapositive statement \begin{gather}
(\nexists y\in B\cap C)\Rightarrow(x\notin A).\tag1
\end{gather} In order to show this last statement, I show two things: \begin{gather}
\forall y\in Y,((y\notin C)\Rightarrow(x\notin A))\tag2\\
\forall y\in Y,((y\notin B)\Rightarrow(x\notin A))\tag3
\end{gather} Unfortunately, I’m not sure whether my approach to the problem is legit.","['propositional-calculus', 'proof-writing', 'logic', 'solution-verification', 'elementary-set-theory']"
4352768,What are the solutions for $y(t)\cdot\left(y'(t) + a\right)=-b\sin(t)$?,"What are the solutions for $y(t)\cdot\left(y'(t) + a\right)=-b\sin(t)$ ? It could be proben that there exists some solutions? Are these solutions unique? and obviously, which are these solutions? (Closed-form if possible) Actually the question is simple, and no other info is required, but to encourage you to participate I will share a motivation, but this intro is not needed to find the answer (just in case you don´t want to read it - but is quite interesting in my opinion). Motivation I am trying to figure out if the equation of the classic damped non-linear pendulum $$y''(t) + ay'(t)+b\sin\left(y(t)\right)=0$$ admits finite-duration solutions ( $\{a,\,b\}$ are constants): I believe it must to, because is the simplest realistic physical model I know, and experimentally it stops moving after some finite time. But along with being the simplest case of ""realistic"" dynamical system, it is also known to don´t have any known close form solution, and every solution it has through approximations are ""vanishing at infinite"", don´t really being a ""true"" finite-duration functions. Recently I have learned that every non-constant dynamical system, to be of finite-duration, it must be nonlinear , so its solutions won´t be analytical in the full domain... so everything I know as engineer through Taylor Series, Linear ODEs, and Power Series expansions are approximations: quite shocking at first... I know that equations are ""models"" tied to assumptions, but I never think before that every model I know are actually approximations since no finite-duration solution can be supported by them ... neither dynamical systems with stands uniqueness of solutions will be an accurate model for finite-duration phenomena. But also I know now that a finite-duration solution could been represented within its duration using known functions, as bump functions $\in C_c^\infty$ , like $f(t) = e^{t^2/(t^2-1)},\,|t|\leq 1; \,(f(t)\text{ = 0, else})$ where the exponential function is defined to represent the solution only within its compact support. And studying compact-supported function, I found a really interesting condition: if a function is continuous and compact-supported (as a finite-duration position-vs-time model where ""teleportation"" is forbidden), then it is also: (i) a bounded function, and (ii) its Fourier Transform is an analytic function, both actually quite huge restrictions, and I start to wonder if being of finite-duration could make same kind of restrictions to the maximum rate of change that dynamical system could achieve, like restrictions to fulfill causality. So now, I am trying to see if a finite-duration alternative could be found for the solution of the nonlinear damped pendulum without using approximations (here for start from the very beginning of dynamical systems I know). I have found recently two papers from the same author (V. T. Haimo / Vardia Haimo), that analyze finite-duration differential equations [ 1 ] and [ 2 ]. And in [1] on Theorem 2 point (i), it is said that, without losing generality by considering that the finishing time of the finite-duration solution happens at $t_F = 0$ , for a second order dynamical system described by $\ddot{x}(t) = g(x(t),\dot{x}(t))$ such $g(0,0)=0$ (the system dynamics ""die"" at $t_F = 0$ ), with $g \in C^1(\mathbb{R}\setminus \{0\})$ , then for the system to support finite-duration solutions, the following another differential equation must have solutions: $$q(z)\frac{dq(z)}{dz} = g(z,q(z)),\,q(0)=0$$ Honestly the papers are bit advanced to my mathematical skills, but if I didn´t made any mistakes, following the example given on the papers, the corresponding equation for finding if the nonlinear damped pendulum supports finite-duration solutions is: $$q(z)\cdot\left(q'(z) + a\right)=-b\sin(z)$$ where I have used and abuse of notation in the main question: the $y(t)$ of the main equation are not the same function $y(t)$ of the solutions of the nonlinear damped pendulum, neither their variables $t$ are the same (it just look more natural for asking as a differential equation dependent in time instead of an arbitrary variable $z$ ). But I get stack here, since I don´t know how to figure out if the equation of the main question have solutions. Hope you get interested in this as I am, unfortunately, I have found just a few papers on Google about continuous time finite-duration differential equations, and neither of them as a whole studied Theory, neither a Wikipedia page, so or it is a quite unexplored topic, or because of security reasons the publication are not published for general public (the papers where published in a corporation that works for the military, so it could be a feasible reason).","['nonlinear-dynamics', 'ordinary-differential-equations', 'real-analysis', 'physics', 'finite-duration']"
4352812,Using a simulation to show that any obtuse triangle whose largest angle is $\leq100^{\circ}$ has a stable periodic billiard orbit,"In 2009, Richard Schwartz proved that any obtuse triangle whose largest angle is $\leq100^{\circ}$ has a stable periodic billiard orbit. My question then, is: How can I reproduce Schwartz's result using a simulation? More specifically, How can I develop a numerical method converging to a $(P_0,V_0)$ (initial position, initial direction) giving a Schwartz periodic orbit? I've created a functioning simulation which produces paths in a triangular billiard table, given the initial position and direction of the ball: However, I have no idea how to even begin computationally reproducing Schwartz's result. A brute-force approach of supplying the ball with every initial condition (position, direction) for every triangle with angles $100\leq \alpha \leq 180$ is simply infeasible for an infinite search space. Given this, I would very much appreciate any insights from the Math Stack Exchange community for how to go about reproducing this result. And of course, I can make any modifications to my program as necessary.","['computational-mathematics', 'billiards', 'geometry']"
4352841,"If $a_n>0;\ a_n\to 0,$ is there an enumeration $\{ x_n \}_{n\in\mathbb{N}}$ of $\mathbb{Q}\cap (0,1]$ such that $a_1 x_1 > a_2 x_2 > a_3 x_3\ldots\ ?$","Suppose $\ a_n>0\ \forall n\in\mathbb{N}\ $ with $\ a_n\to 0.\ $ Proposition: There exists a bijective enumeration $\ \{ x_n \}_{n\in\mathbb{N}}\ $ of $\ \mathbb{Q}\cap (0,1],\ $ such that $\ a_1 x_1 > a_2 x_2 > a_3 x_3 \ldots\ .$ I have many ideas of how to prove this, although I am finding it tricky to actually write out a proof. I was wondering if there are particularly simple/straightforward proofs of this. I'm also just interested in seeing different ideas and methods for how to prove this... I'll start posting one or more of my proofs once I've formalised them... but they are to do with the peaks and troughs of $\ a_n\ $ like in the Lemma to the proof of B-W. The peaks form a decreasing sequence which gives a good starting point for a proof. The troughs also give a decreasing sequence which gives a good starting point for a proof. By ""trough"" here, I mean, a point $\ a_k\ $ such that $\ i<k\implies a_i>a_k.$","['proof-writing', 'real-analysis', 'alternative-proof', 'general-topology', 'problem-solving']"
4352874,What is the $\textbf{Set}$-theoretical intuition behind understanding how power objects work in Topos Theory?,"Definition. Let $\mathcal{E}$ be an elementary topos, $B \in \text{Ob}(\mathcal{E})$ .  Then a power object for $B$ is an object $PB \in \text{Ob}(\mathcal{E})$ together with a morphism $B \times PB \xrightarrow{\in_B} \Omega$ such that for every $f: B \times A \to \Omega$ , there exists a uniqe arrow $g : A \to PB$ such that $\in_B(1\times g) = f$ , where $1\times g$ is the product map of $\text{id}_B \equiv 1$ and $g$ . In diagramatic form, it's just the following triangle, that uses dashed lines to mean $\exists 1\times g$ .  I forgot to put in uniqueness though, so imagine a $!$ next to the $1\times g$ . So my question is basic. What is the $\textbf{Set}$ -theoretical intuition behind this definition? I understand how $\in$ works in set theory, but I'm having trouble grasping all the machinery needed here to transfer over to a topos. I'm thinking that we want to treat $f$ as a function mapping to either true or false (for some topoi examples) which describes a relation $R \subset B \times A$ , but it doesn't seem to me how any arbitrary relation could make sense or be related to how the $b = a$ or $B \ni a$ works. I'm having major confusion about this and can't find a good explanation on the web.","['relations', 'category-theory', 'diagram-chasing', 'elementary-set-theory', 'topos-theory']"
4352876,Clarification about the triple product identity for partial derivatives,"I'm having a hard time understanding why \begin{equation}\frac{\partial x}{\partial y}\frac{\partial y}{\partial z}\frac{\partial z}{\partial x}=-1 \tag{1}\label{1} \end{equation} Wikipedia provides this derivation. I have two problems with it. The proof starts by stating that there is a function f such that $f(x,y,z)=0$ and that $z$ can be made a function of $x,y$ . Furthermore it states that there can be found a curve, along which $dz=0$ and $y$ is a function of $x$ , such that we can then write the differential of $z$ in terms of the differential of $x$ as $$dz=\frac{\partial z}{\partial x}dx+\frac{\partial z}{\partial y}\frac{\partial y}{\partial x}dx$$ The rest follows naturally from setting $dz=0$ and multiplying some partial derivatives by their inverses. I have two problems with this proof 1.Chain rule The first one is that since \begin{equation} \tag{2} \label{2} \frac{\partial z}{\partial x}=-\frac{\partial z}{\partial y}\frac{\partial y}{\partial x} \end{equation} That would mean that, by chain rule, $$\frac{\partial z}{\partial x}=-\frac{\partial z}{\partial x}$$ which would imply this partial derivative to be zero. However if this is true, \ref{1} yields $0=-1$ . Is the chain rule not valid in this case? If so, why? 2.Inverse of the partials The second is that, while applying the last step, it is implied that we obtain \ref{1} by multiplying by the inverse of the righthand-side in \ref{2}. I thought the relationship $$\frac{\partial y}{\partial x}=\frac{1}{\frac{\partial x}{\partial y}}$$ was in general not true, as pointed out in this post . Is it true in this case? And if so, why is that? Also, if that really is the case, then using the chain rule again yields, from \ref{1}, $$\frac{\partial x}{\partial z}\frac{\partial z}{\partial x}=-1 \iff 1=-1$$ What am I doing wrong?","['partial-derivative', 'multivariable-calculus', 'calculus', 'proof-explanation']"
4352891,Geometric Brownian motion and its inverse,"Consider a geometric Brownian motion described by the SDE: $$dS_t = \mu S_t dt + \sigma S_t dB_t$$ where $B_t$ is a Brownian motion and $\mu$ and $\sigma$ are constants. We have that: $$d(\ln S_t) = \sigma dB_t +\left(\mu - \frac{\sigma^2}{2}\right)dt$$ from which we immediately obtain: $$S_t = S_0 \exp\left[(\sigma B_t)+\left(\mu - \frac{\sigma^2}{2}\right)t\right]$$ Consider now the process $R_t = S_t^{-1}$ ; in this case, since $dR_t = - S_t^{-2} dS_t$ , we obtain the SDE: $$dR_t = -\mu R_t dt - \sigma R_t dB_t$$ and by applying the same procedure, we obtain: $$R_t = R_0 \exp\left[(- \sigma B_t)+\left(- \mu - \frac{\sigma^2}{2}\right)t\right]$$ On the other hand, from inverting immediately the $S_t$ solution I would obtain: $$R_t = R_0 \exp\left[(- \sigma B_t)+\left(- \mu + \frac{\sigma^2}{2}\right)t\right]$$ Obviously there's something wrong here, but which method is correct? And where's the mistake?","['stochastic-processes', 'stochastic-differential-equations', 'brownian-motion', 'probability-theory', 'stochastic-calculus']"
4352950,volume preserving version of Moser's theorem,"There exists an well-known theorem of Moser : Thereom(Moser) Let $M$ be a compact oriented smooth manifold and $\alpha,\beta$ be volume forms whose total volumes are the same. Then, there exists a diffeomorphism $\phi:M\rightarrow M$ such that $\phi^*\beta=\alpha$ . After that, I modify this theorem under the fixed volume form. ??? Let $M$ be a compact oriented smooth manifold and $\mu_0$ be a volume form. For positive $f,g\in C^\infty(M)$ with $\int_M f\mu_0=\int_M g\mu_0$ , there exists a volume-preserving diffeomorphism $\phi\in \text{Diff}(M,\mu_0)$ such that $f\circ \phi = g$ . Is the theorem is true? How could I prove this? I try to prove this by using Moser's decomposition technique but there is some difficulty to verify the existence of solution of some PDE due to the volume preserving condition.","['volume', 'symplectic-geometry', 'differential-forms', 'differential-geometry']"
4352972,Euclidean embeddings of cayley graphs,"For any finite group $G$ , we can define a 'dimension' of the group as the smallest $n$ such that there is some choice of generators $S$ of $G$ where $G$ equipped with word metric with respect to $S$ is isometrically embeddable into euclidean space $\mathbb{R}^n$ . Equivalently, this means the cayley graph on $G$ (equipped with shortest-path-length metric) with edges labelled by elements in $S$ and edge-lengths $1$ is isometrically embeddable into $\mathbb{R}^n$ . This is a finite number, since taking $S = G$ you get the discrete metric, which is embeddable in $\mathbb{R}^{|G|-1}$ as a simplex. Is there a name for this 'dimension'? I'm not having any luck with google searches. I'd be interested to know bounds on this dimension (assuming it is studied as is nontrivial) for the finite simple groups","['euclidean-geometry', 'geometry', 'recreational-mathematics', 'geometric-group-theory', 'group-theory']"
4352997,notation problem about $\left|\nabla f\right|$ and choice of vector norm,"Given $f\in W^{1,p}(\mathbb{R}^N,\mathbb{R})$ , I have seen a lot about this notation $|\nabla f|$ without specific pick of vector norm. Does the choice of a particular vector norm effect the equalities/ inequalities? For example, the Coarea formula: $$\int_{\Omega} |\nabla f| = \int_{-\infty}^{\infty} H_{N-1} (f^{-1}(t))\mathrm{d}t.$$ And the total variation of a differentiable function (See the wiki Total variation ) : for a $C^1(\bar{\Omega})$ function $f$ , the total variation is \begin{align*}
V(f,\Omega):=&\sup \left\{\int_{\Omega} f(x)\mathrm{div}\phi(x)\mathrm{d}x: \phi \in C_{c}^1(\Omega,\mathbb{R}^N), \|\phi\|_{\infty}\leqslant 1\right\}\\
=&\int_{\Omega}|\nabla f|\mathrm{d}x.
\end{align*} The total variation seems to pick $\ell_1$ or $\ell_2$ norm, since \begin{align}
\int f\mathrm{div}\phi \leqslant \left|\int \phi \cdot \nabla f\right| &\leqslant |\phi|_{\infty}\int |\nabla f|_1\leqslant \int |\nabla f|_1\\
&\leqslant \int |\phi|_2\cdot |\nabla f|_2\leqslant \int |\nabla f|_2
\end{align} However, by definition $V(f,\Omega)$ should be unique in its value.","['measure-theory', 'sobolev-spaces', 'matrix-norms', 'vector-analysis', 'total-variation']"
4353012,Clifford algebra and Spin,"I am reading Riemannian Geometry and Geometric Analysis by Jurgen Jost. Let me mention the notations and what I have learned from the book. In the book, the Clifford algebra $Cl(V)$ of a real vector field $V$ is defined to be the quotient of the tensor algebra $\bigoplus_{k\ge0} \bigotimes^k V$ by the two-sided ideal generated by elements of the form $v \otimes v + \langle v,v \rangle$ . The spin group $Spin(V)$ is defined as all elements of the form $$a = a_1...a_{2k} \text{ with } a_i \in V \text{, }\left\Vert a_i \right\Vert = 1 \text{ for } i = 1,...,2k $$ The chirality operator is $\Gamma = i^m e_1...e_n \in Cl^\mathbb{C}(V)$ where the upperscript $\mathbb{C}$ is the complexification, $m = \frac{n}{2} = \frac{dim_\mathbb{R}V}{2}$ and $e_i$ is an positive orthonormal basis (I am only interested in even dimensional case). Since $\Gamma^2 = 1$ , we may use $\Gamma$ to obtain a decomposition $Cl^\mathbb{C}(V)^\pm$ of $Cl^\mathbb{C}(V)$ into eigenspaces with eigenvalues $\pm 1$ under the multiplication of $\Gamma$ . Theorem 2.6.4 tells us that $Cl^\mathbb{C}(V)$ is isomorphic to the algebra of complex linear endomorphisms of the spinor space $S = \bigwedge W$ . Under the isomorphism, $\Gamma$ equals $(-1)^k$ on $\bigwedge^k W$ so that we have the decomposition $$ S^\pm = {\bigwedge}^\pm W $$ where the +(-) sign on the right denotes elements of even (odd) degree. I understand the material so far, but cannot see why this statement is true: Since $Spin(V)\subset Cl^+(V)$ , $Spin(V)$ leaves the space $S^+$ and $S^-$ invariant. I am thinking that for example, $e_1e_2\in Spin(V)$ , but $\Gamma (e_1e_2) = (e_2e_1)\Gamma = i^m e_3...e_n $ ; Then why do we have the containment $Spin(V)\subset Cl^+(V)$ ? Do I misunderstand something? Even if we have the containment, why the spin group leaves $S^\pm \subset S$ invariant? This is my first time seeing the Clifford algebra and representation, so I appreciate any relevant material for self-study.","['clifford-algebras', 'spin-geometry', 'riemannian-geometry', 'differential-geometry']"
4353022,Solution to Linear Time-invariant Matrix ODE $\frac{d\mathbf{P}}{dt} = \mathbf{A}\mathbf{P} + \mathbf{B}$,"Do we have a solution for the following matrix ODE: $$\frac{d\mathbf{P}}{dt} = \mathbf{A}\mathbf{P} + \mathbf{B}$$ where all matrices are square. My guess is: $$
\mathbf{P}(t) = \exp{(\mathbf{A}t)}\mathbf{P}(0) + \mathbf{A}^{-1}[\exp{(\mathbf{A}t)}\mathbf{B} - \mathbf{B}]
$$ when $\mathbf{A}$ is invertible. I am not sure it is correct and not clear when $\mathbf{A}$ is not full rank.","['ordinary-differential-equations', 'analysis', 'matrices', 'linear-algebra', 'matrix-equations']"
4353051,Possible Close-form solution for 2 dimensional $\frac{d\Sigma}{dt} = \mathbf{A}\Sigma + \Sigma \mathbf{A}^T + \mathbf{B}\mathbf{B}^T$,"When $\mathbf{A},\mathbf{B}$ are 2x2 matrix and $\Sigma(t)$ are PSD, can we expect a close form solution for the following ODE, $$
\frac{d\Sigma}{dt} = \mathbf{A}\Sigma + \Sigma \mathbf{A}^T + \mathbf{B}\mathbf{B}^T.
$$ The problem is from solving the covariance of a time-invariant SDE $$
 x = \mathbf{A}x + \mathbf{B}dw.
$$ When $x$ is in two dimensions, I guess there could be a close-form solution for $\Sigma$ . But I am not sure how to solve it.","['ordinary-differential-equations', 'matrix-calculus', 'linear-algebra', 'stochastic-differential-equations', 'matrix-equations']"
4353054,Prove $n + H(1) + H(2) + H(3) + ... + H(n-1) = nH(n)$ by induction,"The question below is an extension question from an exercise booklet and I am unable to complete a small part of this problem at the end. I seem to be stuck on simplifying the left hand side during the induction proof. Question: Let $H(n) = 1 + \frac{1}{2} + \frac{1}{3} + ... + \frac{1}{n}$ . Use induction to prove for all positive integers $n > 0$ , $n + H(1) + H(2) + ... + H(n-1) = nH(n)$ . Attempt: During the induction process: Part A: When $n = 1$ , $RHS = 1×H(1)$ = $1 × 1$ = $1$ = $LHS$ so the statement is true for $n = 1$ Part B: Prove for $k$ (Using the fact that n = k) $$ k + H(1) + H(2) + ... + H(k - 1) = kH(k) $$ Prove for $k + 1$ $$ (k + 1) + H(1) + H(2) + ... + H(k) = (k + 1)H(k + 1) $$ Left hand side: $ (k + 1) + H(1) + H(2) + ... + H(k)$ = $ kH(k) + H(k) + 1$ (** By induction hypothesis) = $ H(k)(k+1) + 1 $ This is as far as I can progress. By substitution, it proves that, $ H(k)(k+1) + 1 $ = $ (k + 1)H(k+1)$ , however, I am not sure how to simplify the left hand side further so it can equal the right hand side. I must be missing some important detail. Can you please help?","['induction', 'functions']"
4353065,Is this sufficient condition for infinite number generation from a finite seed also a necessary one?,"Given an initial finite set of natural numbers (the seed). Use the following rule to add numbers to the set. If $a,b,a+b \in S \rightarrow ab \in S$ . Where $S$ is the is set. For example take the initial seed $\{2,3,5\}$ . $2,3,5 \in S \rightarrow 6 \in S \quad$ So now the new set is $\{2,3,5,6\}$ $3,3,6 \in S \rightarrow 9 \in S \quad$ So now the new set is $\{2,3,5,6,9\}$ $3,6,9 \in S \rightarrow 18 \in S \quad$ So now the new set is $\{2,3,5,6,9,18\}$ $9,9,18 \in S \rightarrow 81 \in S \quad$ So now the new set is $\{2,3,5,6,9,18,81\}$ This seven-element set ends the generation, no new numbers can be added. Now let's look at a second example where the initial seed is $\{2,3,4,5\}$ $2,3,5 \in S \rightarrow 6 \in S \quad$ So now the new set is $\{2,3,4,5,6\}$ $2,4,6 \in S \rightarrow 8 \in S \quad$ So now the new set is $\{2,3,4,5,6,8\}$ There is enough at this point to show that the set will be infinite. Let $x \in \Bbb{N}\space|\space x\gt 1$ . If there is the pattern of elements $x,2x,3x,4x$ in $S$ then the rule will produce infinite numbers. (In this example we have $2,4,6,8$ .) $x,x,2x \in S \rightarrow x^2 \in S$ $x,2x,3x \in S \rightarrow 2x^2 \in S$ $x,3x,4x \in S \rightarrow 3x^2 \in S$ $2x,2x,4x \in S \rightarrow 4x^2 \in S$ This means the new set will now contain $x^2, 2x^2, 3x^2, 4x^2$ . This is inductive. This condition of having $x,2x,3x,4x$ in the set is sufficient to show that there is an infinite number of numbers that will be generated. My question is this a necessary condition to produce an infinite set?
In other words does there exist a finite seed that can generate infinite sets without any subset (from the seed or generated sets) of the form ${x,2x,3x,4x}$ ? (@Ingix showed that the previous version of the question wasn't explicit enough the new formulation was made because of his critique) I want to address potential confusion about sets and cardinality to people in the future who are reading this problem. I had this confusion and so did one other person in the comments. When we are determining the size of a set (some finite size, or infinite) only the number of unique items gets counted. This subtle fact about sets is important to this problem because if we had the seed $\{0\}$ . What we could do is $0,0,0 \in S \rightarrow 0 \in S$ .  Then our new set is $\{0,0\}$ . This new set is just an inefficient way of writing the same set we had previously because both sets have the same number of unique items. Adding another $0$ doesn't change any of the properties of the set including the size. So you cannot create a set with infinite size just from the seed $\{0\}$","['elementary-set-theory', 'number-theory', 'recreational-mathematics']"
4353095,Change of variable formula for Haar measure on product of Lie Groups,"First let me recall the usual change of variable formula: Let $\varphi:\mathbb{R}^n\to\mathbb{R}^n$ be a bijection which is Frechet differentiable, $U\in\mathrm{Open}(\mathbb{R}^n)$ , and $f:\mathbb{R}^n\to\mathbb{R}$ be given. Then the change of variable formula says that $$ \int_U f\circ\varphi = \int_{\varphi(U)}f\,\cdot|\det(D\left(\varphi^{-1}\right))| \tag{CoV}$$ where $D\left(\varphi^{-1}\right)$ is the Frechet derivative of $\varphi^{-1}$ . Now let me present the context I am actually interested in: Let now $G$ be a (matrix?) Lie group. Let us interpret $G^n$ as the Cartesian product (which is itself a Lie group I presume). Let $\varphi:G^n\to G^n$ be a given bijection, $U\in\mathrm{Open}(G^n)$ and $\mu$ be the Haar measure on $G$ . If $f:G^n\to\mathbb{R}$ is given, then My question: What is the analog of (CoV) for my context? I.e., what is $$ \int_{(g_1,\dots,g_n)\in U} (f\circ\varphi)(g_1,\dots,g_n)\,\mathrm{d}\mu(g_1)\dots\mathrm{d}\mu(g_n) = ? $$","['measure-theory', 'geometric-measure-theory', 'haar-measure', 'change-of-variable', 'lie-groups']"
4353111,Evaluating integrals of the form $\int_{-\infty}^{\infty}R(x)\sin(x)dx$,"I was reading a complex analysis textbook written by Joseph Bak and Donald Newman. And I was on the part of evaluating integrals of the form $\int_{-\infty}^{\infty}R(x)\sin(x)dx$ where $R(x)=P(x)/Q(x)$ is an integrable rational function, specifically $degQ>deg P$ . I have a few questions regarding some arguments on it (I will write down some arguments below, and there is a list of questions that I have in the very end of the post). The argument goes as follows: Consider the contour shown below: And let the radius of this semicircle be $M$ , and denote by $\Gamma_M$ the circular boundary of this contour. And let $C_M$ denote the entire contour. Consider the integral $$\int_{C_M}R(z)e^{iz}dz.$$ We want to show that $$
\int_{\Gamma_M}R(z)e^{iz}dz\to 0
$$ as $M\to 0$ . Fix a constant $h$ and let $A=\{z\in\Gamma_M:\operatorname{Im}(z)\geq h\}$ and $B=\{z\in\Gamma_M:\operatorname{Im}(z)< h\}$ . We can do $\int_A$ and show it goes to zero by taking $h=\sqrt{M}$ , which I understand, but when we do $\int_B$ , it says $$
\left|\int_B R(z)e^{iz}dz\right|\leq \frac{K}{M}4h.
$$ Here we assume $|R(z)|\leq K/|z|$ for large enough $z$ . But how did we get the bound $\left|\int_B e^{iz}dz\right|\leq 4h$ ? If we can show this then again by taking $h=\sqrt{M}$ we get $\int_B\to 0$ as $M\to \infty$ . Sorry, this is kind of a mess. A full proof is in Bak and Newman's complex analysis, pg 145-147 in the third edition. So my questions are: How did we get $\left|\int_B R(z)e^{iz}dz\right|\leq \frac{K}{M}4h$ when doing an integral along $B$ ? This is kinda stupid, but how can we show that if $R(z)=P(z)/Q(z)$ where $deg P<deg Q$ , then we have $|R(z)|\leq K/|z|$ for some constant $K$ ? I assume this happens when $z$ is large though. I am new to residue theorem and using it to evaluate integrals. I tried finding similar posts on the forum but I didn't find much. Thank comment will be helpful! Thank you so much!","['complex-analysis', 'contour-integration', 'residue-calculus', 'complex-integration']"
4353119,(Conditional) coin flips mean and variance,"Roll a fair die to obtain a random number $1 ≤ n ≤ 6$ , then flip a fair coin $n$ times. Let $X$ be the random variable that expresses the number of heads in the coin flips.
Find the mean and variance of $X$ . Since the coin flips are Bernoulli trials, the expected number for heads ( $H$ ) and tails ( $T$ ) are $\mathbb{E}[H]=\mathbb{E}[T]=n/2$ . Furthermore, $H$ (and $T$ ) is distributed as per the binomial distribution, which has mean $n/2$ and variance $n/4$ ,  ( $n\in\{1,2,3,4,5,6\}$ ). I don't know how to combine this with the outcome of the die roll.
I have very elementary knowledge with probability and statistics, but I know, at least, that the coin tosses are independent of any other event. (This is a problem I found in a Facebook group - not homework or anything). Thank you!","['statistics', 'probability']"
4353182,Multiplicity of root $1$ of $nX^{n+2}-(n+2)X^{n+1}+(n+2)X-n$?,"I'm solving this homework problem: Show that $1$ is a root of the polynomial $nX^{n+2}-(n+2)X^{n+1}+(n+2)X-n$ . Determine its multiplicity. I believe the answer is $n+2,$ and currently working on proving this. My intuition stems from the fact that when $n=1, P(x)=(x-1)^3, so$ 1 is a polynomial of multiplicity $3.$ I attempted this: calling the polynomial $P(x),$ we see that $P(1)=0,$ hence $1$ is a root for sure. To determine its multiplicity, I'm thinking about: Taking successive derivatives $P'(1), P''(1) \dots P^{(n+3)}(1) $ of the polynomial and showing that all but the last one $P^{(n+3)}(1)$ vanishes. But I'm a bit skeptical because the antiderivative of a polynomial doesn't always have multiplicity one more than the original polynomial, which forms the basis/idea of the method I'm thinking of. For example $Q(x):=2x$ has the root $0$ of multiplicity $1,$ but the antiderivative $x^2+1$ doesn't have any real root. So my question is: in order to answer the question in the image, what is the exact theorem should we use? Is it something like this? Proposed theorem: If $Q(x)$ has the real root $a$ of order $k,$ then its antiderivative with constant of integration $0$ has the real root $a$ of order $k+1.$ The above seems to be true, and if yes, can we use this to show that the successive derivatives $P'(1), P''(1) \dots P^{(n+2)}(1) $ of the polynomial vanish but $P^{(n+2)}(1) $ doesn't vanish, and this'll show that $1$ is a root of multiplicity $n+2.$ Is my idea correct? ADDENDUM/EDIT: Can we arrive at the proof that $1$ is a root of multiplicity $n+2$ by using induction? At $n=1,$ the multiplicity is $1+2=3,$ so the induction can start, and now we just need to show the induction step. Will this work? P.S. As per Dietrich's answer, the multiplicity seems to be $3$ irrespective of $n.$ So can we just prove this using the derivatives above or by induction?","['abstract-algebra', 'derivatives', 'polynomials', 'roots']"
4353238,Is the power of $2$ in the Euclidean norm related to the fact that the algebraic closure of the reals is $2$-dimensional?,"The bounty expires in 7 hours . Answers to this question are eligible for a +500 reputation bounty. Qiaochu Yuan wants to draw more attention to this question: This is a great question and I'd love to see any thoughts on it! Consider any local field $K$ , endowed with its topological field structure. We define the function $| \cdot | : K \to \mathbb{R}_{\ge 0}$ as $$|x| = \frac{\mu(xS)}{\mu(S)},$$ where $\mu$ is any Haar measure (which exists because a local field is additively a locally compact group), and $S$ is any measurable set of nonzero measure (see e.g. Definition 12.28 here ). This definition can be shown to be independent of $\mu$ and $S$ . Now consider a vector space $V \simeq K^n$ over $K$ , and equip it with the $p$ -length function $$||(x_1,x_2,\ldots,x_n)||_p = (|x_1|^p+|x_2|^p+\ldots+|x_n|^p)^{1/p}$$ for some $p \in [1,\infty]$ (where as usual we define $|| \cdot ||_\infty$ as the limit of $|| \cdot ||_p$ as $p\to \infty$ ). There is a unique critical value $p=p^*(K)$ for which this length function becomes especially symmetric, in that its associated group of isometries (i.e., the subgroup of $GL(V)$ that preserves $|| \cdot ||_{p^*(K)}$ ) acts transitively on the corresponding unit sphere. The amazing fact is that this critical value $p^*(K)$ always coincides with the degree $[\bar{K}:K]$ of the algebraic closure of $K$ ! Here is a proof sketch: From the classification of local fields, it is known ${}^{(\dagger)}$ that $K$ is either $\mathbb{R}$ , $\mathbb{C}$ or a non-Archimedean local field (that is, a finite extension of either $\mathbb{Q}_P$ or $\mathbb{F}_P((t))$ for some prime $P$ ). If $K=\mathbb{R}$ , $|\cdot|$ is the usual absolute value, and the critical length function turns out to be $|| \cdot ||_2$ (the usual Euclidean length), invariant under $O(n)$ . On the other hand, $[\bar{\mathbb{R}}:\mathbb{R}]=[\mathbb{C}:\mathbb{R}]=2$ . If $K=\mathbb{C}$ , we have $|x|=\bar{x} x$ , and the corresponding critical length function is $|| \cdot ||_1$ , invariant under $U(n)$ . ${}^{(\ddagger)}$ On the other hand, $[\bar{\mathbb{C}}:\mathbb{C}]=[\mathbb{C}:\mathbb{C}]=1$ . Note that neither $|\cdot|$ nor $||\cdot||$ satisfy the triangle inequality in this particular case (this is why I avoided using the terms absolute value and norm throughout the question). If $K$ is non-Archimedean, $|\cdot|$ is a suitably normalized $\chi$ -adic absolute value for some $\chi$ , and the corresponding critical length function is the supremum norm $|| \cdot ||_\infty$ , which can be shown to be invariant under $GL(\mathcal{O}_K^n)$ , where $\mathcal{O}_K$ is the ring of integers of $K$ . On the other hand, it is known that the algebraic closure of $K$ has infinite degree, i.e. $[\bar{K}:K]=\infty$ . Thus in all cases we have $p^*(K) = [\bar{K}:K]$ . A problem with this proof is that it gives no indication as to why the relationship holds: after all, the values could just happen to be the same by complete coincidence. For that reason, I am wondering if there exists an alternative proof of this fact that is "" $K$ -agnostic"", i.e. a proof that does not use at any point the Artin-Schreier theorem , nor the classification of local fields, nor any properties from a specific such field (such as the existence of a symmetric/Hermitian inner product) other than the degree of an algebraic closure and properties derived from it. Such a proof would allow one to meaningfully speak of a relationship between those two quantities, and to state things like ""in a field with a $3$ -dimensional algebraic closure, the natural analogue of the Pythagorean theorem would be $|a|^3+|b|^3=|c|^3$ "" without appealing to the principle of explosion. I know it is hard in general to formally determine whether a proof does or does not use a fact, hence why I'm labeling this with the soft-question tag, but hopefully the kind of proof I want is clear, at least informally. In summary, my question is: Given a local field $K$ , with $| \cdot |$ defined as above, is there any $K$ -agnostic proof that the group of isometries of $||\cdot||_p$ (defined in terms of $| \cdot |$ as above) acts transitively on unit spheres if and only if $p = [\bar{K}:K]$ ? It seems reasonable to try proving first that $p^*(K) = 1$ iff $K$ is algebraically closed. That is where I am currently stuck. The thing that makes linear algebra over an algebraically closed field ""special"" is that every linear transformation has an eigenvalue, but I am not sure how to apply that fact to show that the critical length function is $||\cdot||_1$ . UPDATE : As suggested by Torsten Schoeneberg in the comments, perhaps it will be easier to prove the relationship $p^*(K) = [L:K]\: p^*(L)$ for a finite field extension $L/K$ . Consider such an extension with $[L:K]=n$ , and suppose we know $L$ has a critical $p$ -length function for some $p$ . Now, $L \simeq K^n$ is additively a vector space over $K$ , which is equipped with the product topology. This means that, if we identify $k\in K $ with its image under the inclusion $K \subseteq L$ , we have $$|k|_L = \frac{\mu_L(k S^n)}{\mu_L(S^n)} = \left(\frac{\mu_K(k S)}{\mu_K(S)}\right)^n = |k|_K^n.$$ On the other hand, note that the subgroup of $GL_1(L) = L^\times$ preserving $|\cdot|_L$ , i.e. the group of invertible elements of norm $1$ , acts transitively on the associated unit sphere (which coincides with the group itself) in a tautological way, since any element $x$ of norm $1$ can be sent to any other element $y$ of norm $1$ through multiplication by $x^{-1}y$ . Since $GL_1(L) \subseteq GL_n(K)$ , this proves that the length function on $K^n$ defined through the identification $L \simeq K^n$ as $||\cdot|| = |\cdot|_L^{1/n}$ is a critical length function, that moreover satisfies the homogeneity property $||kv||=|k|_L ||v||$ (which follows by the above relationship between the Haar measures and multiplicativity of $|\cdot|$ ). In a similar way, from the inclusion $GL_m(L) \subseteq GL_{mn}(K)$ we can prove that any critical length function $||\cdot||$ on $L^m$ induces a corresponding critical length function on $K^{mn}$ . The only ingredient left would be to prove is that there always exists some basis $\{1=a_0, a_1, \ldots, a_{n-1}\}$ of $L$ such that the above defined length functions are of the required form, that is, $|\sum_{i=0}^{n-1} k_i a_i|_L = \sum_{i=0}^{n-1} |k_i|_K^n$ . Once we have that, we can show using isotropy subgroups that the length function induced from $L$ is critical in any dimension, not just multiples of $n$ , so that $p^*(K) = [L:K]\: p^*(L)$ . However, I don't know how to prove the existence of such a basis in a $K$ -agnostic way. EDIT: A possible way to restate the problem is by noticing that for any $(v,w)$ in the direct sum $V \oplus W$ , we have $||(v,w)||^p_p = ||v||^p_p + ||w||^p_p$ , i.e. the $p$ th power of $||\cdot||_p$ acts additively on ""independent"" vectors (for finite $p$ , of course). Given a length function on vector spaces $V$ , we can define the associated Gaussian function as an integrable function $g : V \to \mathbb{R}_{\ge 0}$ depending only on length (i.e. factorizing as the composition $f \circ ||\cdot||_p : V \to \mathbb{R}_{\ge 0} \to \mathbb{R}_{\ge 0}$ for some $f$ ), and satisfying the independence property $$g( (v,w) ) = g(v) g(w).$$ With the above choice of maximally symmetric length functions, using the aforementioned additivity property immediately leads to the usual Gaussian function $\exp(-k ||x||_2^2)$ for $\mathbb{R}$ and to $\exp(-k' ||x||_1)$ for $\mathbb{C}$ (where the constants $k, k' >0$ depend on the normalization convention). For $p=\infty$ , this reasoning can't be used directly, but if we interpret it as a limit like before, we do recover the standard Gaussian function for non-Archimedean fields $$\lim_{p\to \infty} \exp(-k'' ||x||_p^p) = \begin{cases} 1 & ||x||_p \le 1 \\ 0 & \text{otherwise} \end{cases} = \mathbf{1}_\mathcal{O_K^n}(x),$$ which is the indicator function of $\mathcal{O}_K^n$ in $V=K^n$ . Focusing on the $1$ -dimensional case $V=K$ , these Gaussian functions coincide with the standard ones as used e.g. in Tate's thesis . So if I'm not mistaken, the problem can be restated as proving that $[\bar{K} : K] = p$ if and only if the standard Gaussian function associated to $K$ is $\exp(-k ||x||_p^p)$ (with a limit intended if $p=\infty$ ). A possible advantage of this restatement is that standard Gaussian functions have different characterizations that do not previously assume any length function (for example, they are their own Fourier transform, or satisfy analogues of the central limit theorem, see e.g. here ). $(\dagger)$ Note that by the Artin-Schreier theorem, the only degrees $[\bar{K}:K]$ that could possibly occur for any field $K$ are $1, 2, \infty$ (this, in turn, seems to be related to the fact that a nonzero integer can only have multiplicative order $1, 2$ or $\infty$ , but that is a topic for another question). $(\ddagger)$ As a fun fact, this can be shown to imply that the hypervolume of the $n$ -dimensional complex unit ball $|x_1|+|x_2|+\ldots+|x_n| = 1$ , i.e. the ordinary $2n$ -dimensional unit ball, equals $V(B_{2n})=V(B_{2})^n /n!=\pi^n /n!$ ; compare with the hypervolume $V(X_{n})=V(X_{1})^n /n!=2^n /n!$ of the $n$ -dimensional cross-polytope $|x_1|+|x_2|+\ldots+|x_n| = 1$ , where $| \cdot |$ now denotes the real absolute value.","['measure-theory', 'number-theory', 'local-field', 'linear-algebra', 'soft-question']"
4353293,Find the area of ​a regular pentagon as a function of its diagonal,"For reference: Calculate the area of ​​a regular pentagon
as a function of its diagonal of length $a$ . (Answer: $\frac{a^2}{4}\sqrt\frac{25-5\sqrt5}{2}$ ) My progress: $R$ = radius inscribed circle $S = \frac{5R^2}{8}(\sqrt{10+2\sqrt5})$ $L=\frac{R}{2}(\sqrt{10-2\sqrt5})\implies R^2 = \frac{4L^2}{10-2\sqrt5}\tag{I}$ $\cos36^\circ=\frac{a}{2L} \implies a = L(\frac{1+\sqrt5)}{2}\implies L= \frac{2a}{1+\sqrt5}$ $\implies L^2 = \frac{a^2}{4}(6-2\sqrt5)\tag{II}$ $\text{From (I)}: S = \frac{20L^2}{8(10-2\sqrt5)}.(\sqrt{10+2\sqrt5})$ $\implies S =\frac{5L^2}{2(10-2\sqrt5)}.(\sqrt{10+2\sqrt5})$ $\text{From (II)}: S = \frac{5a^2(6-2\sqrt5)}{8.(10-2\sqrt5)}\cdot(\sqrt{10+2\sqrt5})$ $S = \frac{a^2(5-\sqrt5)}{16}(\sqrt{10+2\sqrt5})$ $\boxed{S = \frac{a^2}{4} \sqrt{\frac{25-5\sqrt5}{2} }}$","['euclidean-geometry', 'geometry', 'polygons', 'plane-geometry']"
4353320,Inverse this Stokes Matrix.,"Let $\xi \in \mathbb{R}^d$ . I am looking for a way to compute the inverse of the $d \times d$ matrix by defined $M_\xi:= \xi \otimes \xi - |\xi|^2 I_d,$ where $(\xi \otimes \xi)_{ij}= \xi_i \, \xi_j.$ To give a litle bit of physical background, I believe this matrix should be invertible as it is the pseudo-differential operator of the Stokes operator $- P_\sigma \Delta$ where $P_\sigma$ is the Leray projector. I'm also expecting $M_\xi^{-1}$ to depend morally on $|\xi|^{-2}$ as we should ""integrate twice"" in the Fourier space. Any help is welcomed. Edit 1 Thanks for your help @BenGrossmann and @CalvinKhor. Indeed it seems like my matrix is not invertible since $Ker(M_{\xi})=span(\xi) \neq \{0 \}$ . I was thinking of restricting the beginning space of the linear function $f_\xi$ linked to $M_\xi$ . If I assume that $u \in \{v \in \mathbb{R}^d, \ \text{such that }v \cdot \xi = 0 \} = Span(\xi)^\perp$ , I think I can inverse the equality $$M_{\xi} u = w$$ for any $w \in Span(\xi)^\perp$ . Moreover this restriction to $ Span(\xi)^\perp$ makes sense as it corresponds to the divergence-free condition of the Stokes equation. I need to study the linear function $\tilde{f}_{\xi} : Span(\xi)^\perp \rightarrow Span(\xi)^\perp $ and try to find a good invertible expression of its corresponding matrix ( $d-1 \times d-1$ ). Edit 2 Actually, one can show that for any $u \in Span(\xi)^\perp$ , one has $M_\xi u = -|\xi|^2 u$ , so the matrix of $\tilde{f}_{\xi} $ is the same in every base of $Span(\xi)^\perp$ and is given by $- |\xi|^2 I_{d-1}$ . This matrix is invertible and its inverse is given by $-|\xi|^{-2} I_{d-1}$ .","['matrices', 'linear-algebra', 'fourier-analysis', 'partial-differential-equations']"
4353389,"If $f(0) = 0$, then, is $\mathrm{lim}_{x\to0} xf'(x) = 0$?","Is the above claim true? I think that it is. I am assuming that $f$ is continuous and differentiable at $0$ . My not-so-rigorous attempt: $$ \mathrm{lim}_{x \to 0} x f'(x) = \mathrm{lim}_{x \to 0} \mathrm{lim}_{h\to0} x \frac{f(h)}{h} $$ Now choosing the approach direction $x=h$ , we see that the limit equals $0$ because $f(0)=0$ . Is the idea correct? If not, can this condition be true under some additional constraints?","['limits', 'calculus', 'derivatives', 'real-analysis']"
4353458,Are there four numbers in AP such that their prime factors are also in AP?,"The prime factors of $40807$ are $(13, 43, 73)$ are in arithmetic progression. The prime factors of $55125 $ are $(3, 5, 7)$ are also in AP. The prime factors of $69443$ are $(11, 59, 107)$ are also in AP. Moreover, the three numbers $(40807, 55125, 69443)$ are also in AP. It is the smallest AP of three positive composite integers $(a,b,c)$ such that $\gcd(a,b,c) = 1$ and the prime factors of each of these numbers are also in some arithmetic progression. There are several other such triplets. But when it comes to four such numbers, there seems to be none. Is there any reason why a such a AP of four numbers cannot exist? Question 1 : Is there an AP of four composite coprime positive integers such that the prime factors of each of these four numbers are in some AP? Update : No solutions below $2.5 \times 10^9$ .","['divisibility', 'number-theory', 'elementary-number-theory', 'sequences-and-series', 'recreational-mathematics']"
4353462,Is the category of measurable spaces MONOIDAL closed? (fake-proof),"As discussed on MathOverflow , there is a preprint on ArXiv that claims the category of measurable spaces is monoidal closed (in particular that exponential objects $Y^X$ always exist and have measurable evaluation maps $X \times Y^X \to Y$ ). Also as discussed in the same post , such a claim appears to contradict results by Robert Aumann that exponential objects do not always exist in the category of measurable spaces (i.e. there exists no $\sigma$ -algebra on $Y^X$ for which the evaluation map $X \times Y^X \to Y$ is measurable). So the claim is likely to be false. I have been scrutinizing the preprint and I think I may have identified the error in the argument. Am I correct? Or is the claim actually true? High-level overview of my argument: Because $\sigma$ -algebras are only closed under countable operations, rather than arbitrary ones, it is easier for $\sigma$ -algebras that make all members of a certain class of functions measurable to not exist? (And therefore easier for there to not be any initial or final $\sigma$ -algebras generated by those functions?) Cf. this comment on the MathOverflow post . I.e. in contrast to the situation with topologies, which are closed under arbitrary unions, which makes it easier to guarantee that there always exists some topology for which a collection of functions is measurable? (Thus allowing the existence of initial or final topologies.) Details of attempted disproof: The main issue seems to be that the preprint uses a non-standard definition for the $\sigma$ -algebra on the Cartesian product of two measurable spaces $X \times Y$ when defining the monoidal product $X \otimes Y$ : The coinduced (final) $\sigma$ -algebra [on $X \times Y$ ] such that all graph functions $$\begin{array}{rccl}
 \Gamma_f: & X  &\to & X \times Y \\
& x & \mapsto & (x, f(x))
 \end{array}$$ for $f: X \to Y$ measurable, as well as all the graph functions $$\begin{array}{rccl}
 \Gamma_g: & Y  &\to & X \times Y \\
& y & \mapsto & (g(y), y)
 \end{array}$$ for $g: Y \to X$ measurable, are measurable. Main question : The above $\sigma$ -algebra isn't even guaranteed to exist in general, correct? Asides/extra questions/comments: Normally the product $\sigma$ -algebra would be defined as the induced (initial) $\sigma$ -algebra such that the projections $(x,y) \mapsto x$ and $(x,y) \mapsto y$ are measurable, right? In particular the definition from the preprint, and the standard definition, are not always equivalent, correct? ( As an aside, are they ever equivalent? ) I can go into the details of how the preprint then defines the initial topology on $Y^X$ generated by the pointwise projections/evaluations, and from there the argument for why the evaluation map $X \otimes Y^X = X \times Y^X \to Y$ is measurable, but if the above topology isn't even guaranteed to exist, then the argument definitely seems to fail. (With Robert Aumann providing a counterexample in the case that $X = Y = [0,1]$ .) Terminology: By ""initial $\sigma$ -algebra"" on $X$ generated by functions $f_{\alpha}: X \to Y$ , with $Y$ a measurable space, I mean the ""coarsest"" or ""smallest"" possible $\sigma$ -algebra on $X$ such that all of the $\{f_{\alpha}\}$ are measurable. By ""final $\sigma$ -algebra on $Y$ generated by functions $f_{\alpha}: X \to Y$ , with $X$ a measurable space, I mean the ""finest"" or ""largest"" possible $\sigma$ -algebra on $Y$ such that all of the $\{f_{\alpha}\}$ are measurable. (I guess technically one could also require that $\{f_{\beta}\}$ for $f_{\beta}: X_2 \to Y$ are measurable for $X_2$ a measurable space, which is what the preprint appears to do.)","['measure-theory', 'fake-proofs', 'category-theory', 'solution-verification', 'measurable-functions']"
4353486,Measure that is translation invariant but not invariant to negation,"Does a measure $\mu: \mathscr{B}^1\to[0,\infty]$ (where $\mathscr{B}^1$ is the Borel sigma-algebra) that is translation invariant (i.e. $\mu(x)=\mu(c+x)$ ) but not invariant under $t:\mathbb{R}\to\mathbb{R}, t(x)=-x$ exist? If yes, what value does it assign to the unit interval? Edit: As @GEdgar has pointed out, this measure will not be locally finite, so $\mu([0,1])$ will be $\infty$ . Edit edit: I was given the hint to construct a Borel set $C\subseteq [0,1]$ such that for every sequence of real numbers $(a_n)_n$ it holds that $-C=t(C)\nsubseteq \bigcup_{n\geq0} (a_n + C)$ . Define $\mu: \mathscr{B}^1\to\mathbb{R}$ such that $$\mu(A)=\begin{cases}
0 &\text{$\exists$ real sequence $(a_n)_n$ with $A\subseteq\bigcup_{n\geq 0}(a_n+C)$}\\ \infty &\text{else}\end{cases}$$ Additional hint: For $C$ take the set of $x\in[0,1]$ that can be written as $x=\sum_{n\geq 1} \frac{x_n}{4^n}$ where $x_n\in\{0,1,3\}$ .",['measure-theory']
4353490,"Why did my contour integration for $\int_0^{\pi/2}\frac{1}{1+\sin x}\,\mathrm{d}x$ fail?","$\newcommand{\d}{\,\mathrm{d}}$ Integrals of the titular form can be successfully contour-integrated, e.g.: $$\int_0^{2\pi}\frac{1}{4\cos x-5}\d x=-\frac{2}{3}\pi$$ Follows from a complex substitution very similar to what I've done here. However: $$\int_0^{\pi/2}\frac{1}{1+\sin x}\d x=1$$ Is easily evaluated with Weierstrass's substitution, but out of curiosity I attempted to form a contour integration approach, which failed: First translate by $\pi/4$ : $$\int_{-\pi/4}^{\pi/4}\frac{1}{1+\sin(x+\pi/4)}\d x=\sqrt{2}\int_{-\pi/4}^{\pi/4}\frac{1}{\sqrt{2}+\sin x+\cos x}\d x$$ Let $\Bbb T=\{z\in\Bbb C:|z|=1\}$ . Let $\log$ be the principal branch, and put $x=\frac{1}{4i}\log z$ , $z=e^{4ix}$ and $[-\pi/4,\pi/4)\mapsto\Bbb T$ : $$\begin{align}\frac{\sqrt{2}}{4i}\oint_{\Bbb T}\frac{1}{\sqrt{2}+\sin\left(\frac{1}{4i}\log z\right)+\cos\left(\frac{1}{4i}\log z\right)}\frac{\d z}{z}&=\frac{\sqrt{2}}{4i}\oint_{\Bbb T}\frac{1}{\sqrt{2}+z^{1/4}}\frac{\d z}{z}\\&=\pi\frac{\sqrt{2}}{2}\sum\mathrm{Res}\end{align}$$ We have a pole at $z=0$ which is contained by $\Bbb T$ . The other pole is when $\exp\frac{1}{4}\log z=-2^{1/2}$ : $$\exp\frac{1}{4}\log z=\exp(\pi i+\frac{1}{2}\ln 2),\,\log z=4\pi i+2\ln 2+8\pi i\cdot k$$ But this doesn't really make sense, since the principal $\log z$ should have imaginary part contained in $(-\pi,\pi]$ , but this expression clearly does not satisfy this for any $k$ . I will infer from this that no principal solutions exist, and then that $z=0$ is the only pole, with residue $\frac{1}{\sqrt{2}}$ . We then arrive at the incorrect answer: $$\int_0^{\pi/2}\frac{1}{1+\sin x}\d x=\frac{\pi}{2}$$ What did I do wrong?","['integration', 'complex-analysis', 'contour-integration', 'logarithms']"
4353507,Explicit Local Fundamental Class,"Let $L/K$ be a Galois extension of local fields of degree $n:=[L:K]<\infty$ with Galois group $G:=\operatorname{Gal}(L/K)$ . In short, my question is as follows. Is there an explicit computation of the fundamental class $u_{L/K}\in H^2\left(G,L^\times\right)$ ? Here, the fundamental class is the generator of $H^2\left(G,L^\times\right)$ found by pulling back the generator $\frac1n\in\frac1n\mathbb Z/\mathbb Z$ along the invariant map $\operatorname{inv}_{L/K}:H^2\left(G,L^\times\right)\to\frac1n\mathbb Z/\mathbb Z$ . I record some thoughts below. In the case where $L/K$ is unramified with Galois group $G:=\operatorname{Gal}(L/K)=\langle\operatorname{Frob}_{L/K}\rangle$ , we have an explicit construction of the (local) invariant map $\operatorname{inv}_{L/K}:H^2\left(G,L^\times\right)\to\frac1n\mathbb Z/\mathbb Z$ by $$H^2\left(G,L^\times\right)\stackrel{\operatorname{ord}_L}\to H^2(G,\mathbb Z)\stackrel\delta\leftarrow H^1(G,\mathbb Q/\mathbb Z)\simeq\operatorname{Hom}_\mathbb Z(G,\mathbb Q/\mathbb Z)\stackrel{f\mapsto f(\operatorname{Frob}_{L/K})}\to\frac1n\mathbb Z/\mathbb Z,$$ and then we can pull the (more or less canonical) generator $\frac1n$ back to the (canonical) generator $u_{L/K}\in H^2(G,L^\times),$ which is called the fundamental class. All of this computation can be made explicit (see, for example the discussion preceding Proposition III.1.9 of Milne's notes ); for completeness, we record that the computation gives $$u_{L/K}\left(1,\operatorname{Frob}_{L/K}^k,\operatorname{Frob}_{L/K}^{k+\ell}\right)=\begin{cases}
\varpi & k+\ell\ge n, \\
1 & k+\ell<n,
\end{cases}$$ where $\varpi$ is a uniformizer of $L.$ In the general case (i.e., dropping the assumption that $L/K$ is unramified), the invariant map is not so explicit. Looking the proofs I've found around, the most explicit construction comes from writing the exact sequence $$0\to H^2\left(\operatorname{Gal}(L/K),L^\times\right)\to H^2\left(\operatorname{Gal}(K^{\text{unr}}/K),K^{\text{unr}\times}\right)\to H^2\left(\operatorname{Gal}(L^{\text{unr}}/L),L^{\text{unr}\times}\right)$$ (It takes some amount of work to define the maps and check that this is in fact a short exact sequence; it is essentially Lemma III.2.2 in Milne's notes .) In theory, we could say that $u_{L/K}$ will be the pull-back of $\operatorname{inv}_{K^{\text{unr}}/K}^{-1}\left(1/n\right)$ to $H^2(G,L^\times)$ . However, I have been unable to make this computation explicit.","['algebraic-number-theory', 'number-theory', 'galois-cohomology', 'local-field', 'class-field-theory']"
4353521,Find the area of ​the shaded region PQGF,"For reference: Calculate the area of ​​the shaded region.
AQ = 8m; PC = 9 m (Answer: $15m^2$ ) My progress: FP is angle bissector $\triangle AFG$ GQ 1s angle bissector $\triangle FGC$ $\frac{S_{ABP}}{S_{ABQ}}=\frac{AP}{8}\\
\frac{S_{CBQ}}{S_{BCP}}=\frac{CQ}{9}\\
\frac{S_{ABQ}}{S_{BCP}}=\frac{8}{9}\\
S_{FGPQ}=S_{ABC}-S_{BFG}-S_{CGQ}\\
2\alpha+2\theta = 270 \implies \alpha +\theta = 135^o\\
S_{ABC}=BH^2=AH^2=AC^2$ tried to draw some auxiliary lines like the other question but it didn't solve","['euclidean-geometry', 'geometry', 'plane-geometry']"
4353540,Understanding integration along fibers,"I understand the definition of integration of a differential form along fibers as it is stated in Wikipedia article as follows: Let $\pi :E \rightarrow B$ be a fiber bundle over a manifold with compact oriented fibers. If $\alpha$ is a $k$ -form on $E$ , then for tangent vectors $w_i$ 's at $b$ , let $ (\pi _{*}\alpha )_{b}(w_{1},... ,w_{k-m})=\int _{\pi ^{-1}(b)}\beta  $ where $\beta$ is the induced top-form on the fiber $\pi^{-1}(b)$ ; i.e., an $m$ -form given by: with $\widetilde{w_i}$ lifts of $w_i$ to $E$ , $$
\beta(v_1, \dots, v_m) = \alpha(v_1, \dots, v_m, \widetilde{w_1}, \dots, \widetilde{w_{k-m}}).
$$ However, I don't intuitively understand why it is defined like this and don't know why it is called integration along fibers ?","['differential-forms', 'differential-geometry']"
4353542,Putnam 2006 - Exercise B.6 - Alternative solution verification and questions about generalizations,"CONTEXT Here is the orginal problem statement. Let $k$ be an integer greater than 1. Suppose $a_0 > 0$ , and define $$
a_{n+1} = a_n + \frac{1}{\sqrt[k]{a_n}}
$$ for $n > 0$ . Evaluate $$
\lim_{n \to \infty} \frac{a_n^{k+1}}{n^k}.
$$ ** PROPOSED SOLUTION Note that for $n>0$ $$a_n \geq a_0 + \frac1{\sqrt[k]{a_0}}\sum_{j=1}^n \frac1{\sqrt[k]{j}},$$ and hence $(a_n)$ diverges. EDIT. As pointed out in comment, this step is not correct . I am working on an alternative approach. EDIT 2. A nice approach is proposed in the comment by Thomas Andrews Define $b_n = a_n^{\frac{k+1}k}$ . (Observe that $(b_n)$ , too, is divergent.) Now, we have \begin{eqnarray}\lim_{n\to \infty}(b_{n+1}-b_n)&= &\lim_{n\to \infty}\left(a_n+a_n^{-\frac1{k}}\right)^{\frac{k+1}k}-a_n^{\frac{k+1}k}=\\&=&\lim_{n\to\infty} \frac{\left(1+a_n^{-\frac{k+1}k}\right)^{\frac{k+1}k}-1}{a_n^{-\frac{k+1}k}}=\frac{k+1}{k},\end{eqnarray} where the fundamental limit $$\frac{(1+\alpha)^m-1}{\alpha}\to m, \ \ \ \mbox{for} \ \ \alpha\to 0$$ has been used. By Stolz-Cesàro Theorem, we have $$\lim_{n\to\infty}\frac{b_n}{n} = \frac{k+1}k,$$ and hence $$\lim_{n\to\infty} \frac{a_n^{k+1}}{n^k} = \left(\frac{k+1}k\right)^k.$$ $\blacksquare$ QUESTIONS Is my solution correct? Is it correct to state that the solution is valid also for any real number $k\geq 1$ ? Can we further generalize? For example: what can be stated for $0<k<1$ , maybe with some restriction on $a_0$ ?","['contest-math', 'solution-verification', 'sequences-and-series', 'real-analysis']"
4353546,Is there an angle chase solution for this problem?,"Given two disjointed circles $c_1$ and $c_2$ , external to each other, let $A$ be the meeting of their internal tangents and let $K$ be the orthogonal projection of $A$ in one the common external tangents between $c_1$ and $c_2$ . Prove that $AK$ bissects angle $O_1KO_2$ . I managed to solve it by realizing that $\frac{KT_1}{KT_2} = \frac{r_1}{r_2}$ , which clearly solves the problem when one look at $\triangle O_1T_1K$ and $\triangle O_2KT_2$ . But this problem seems to have a simpler solution. What do you think?","['euclidean-geometry', 'homothety', 'tangent-line', 'circles', 'geometry']"
4353692,Expected number of summations of positive iid discrete random variables,"Let $X_i$ be a sequence of discrete i.i.d. random variables taking values in $\{ 1, \dots, K \}$ , $K \in \mathbb{N}$ with given distribution $p_j = P(X_1 = j)$ . Set $S_n := \sum_{i=1}^n X_i$ and for $M \geq 0$ consider the number of summation steps $N := \inf \{ n \mid S_n \geq M \}$ until the sum reaches $[M, \infty)$ . Is there some closed-form expression for $\mathbb{E}[N]$ in terms of the given data ( $p_j$ , $M$ )? I know that one can formalize this problem as a discrete phase-type distribution comprising a discrete-time Markov chain on the finite state space $\{ 0, 1, \dots, M+K-1 \}$ with initial state $0$ , absorbing states $M, \dots, M+K-1$ and transitions $k \to k+j$ with probability $p_j$ for $k = 0, \dots, M-1$ . Then $N$ is the (discrete) time until absorption and there is an expression for $\mathbb{E}[N]$ in matrix notation as $\delta_0 (I - T)^{-1} \mathbf{1}$ where $T$ is the transition probability matrix for the transient part. My hope is that one can simplify this expression directly in terms of ( $p_j$ , $M$ ) by e.g. some matrix-geometric methods since $T$ has a simple upper triangular form: $$ T = \begin{pmatrix} 0 & p_1 & \dots & p_K & 0 & 0 & \dots & 0 \\ & 0 & p_1 & \dots & p_K & 0 & \dots & 0 \\ & & \ddots & \ddots & & \ddots & \\ 0 & 0 & \dots & & & & \dots & 0 \end{pmatrix} \in \mathbb{R}^{(M-1) \times (M-1)} $$ (On each next row, the row vector $(p_1, \dots, p_K)$ is shifted to the right-hand side until the last row is completely $0$ (the only transitions from state $M-1$ are to the absorbing states $M, \dots, M+K-1$ ).)","['stochastic-processes', 'probability-theory']"
4353757,"Ahlfors page 123: Compute $\int_{|z|=2}z^n(1-z)^mdz$. What happens when $n,m<0$? Residue Theory? Cauchy's Theorem?","This is a question from Ahlfors, page $123$ , number $1b$ :  Compute $\int_{|z|=2}z^n(1-z)^mdz$ .  He doesn't specify anything about $n$ and $m$ on the page, so I am not sure if they are natural numbers or integers.  If they are natural numbers, then the integrand is analytic and so the integral is $0$ , so I assume they aren't natural.  So, I imagine we consider cases depending on which $n$ or $m$ is less than $0$ and which one is greater than or equal to $0$ .  When doing so, we either get a pole of order $n$ at $0$ or a pole of order $m$ at $1$ .  I can write these out in the standard residue form, but is there a way to ""clean them up"", so to speak?  In particular, if you were, say, teaching a class on Complex Analysis, what would you expect from your students?  (note: I am not a current student, just looking back through some old notes and problems from several years ago). EDIT: In particular, I am talking about, for instance, when $n\geq 0$ and $m<0$ , then we have $\int_{|z|=2}\frac{z^n}{(1-z)^m}dz=\frac{1}{(m-1)!}\lim_{z\rightarrow m}\frac{z^{m-1}}{dz^{m-1}}z^n$ .  But, $m<0$ , so what is the negative-th derivative of $z^n$ ?  Similarly, we can consider the case when $n<0$ , $m\geq 0$ and $n,m<0$ .  (I feel like I am missing some cases).  Moreover, this problem comes in Ahlfors book BEFORE Residue theory.  So, maybe there is a better way to tackle it?  I just wasn't seeing a nice way of using Cauchy's theorem. EDIT (number 2): The question was asked here: Computing $\int_{|z|=2} z^n(1 - z)^m\ dz$ for $n,m\in\Bbb Z$ , but I'm finding issues with the answer.  In particular, I am not seeing how they arrived to their answer, and they are only dealing with one case. EDIT (number 3, last update): The question was also asked here: Computing $\int_{|z|=2} z^n(1 - z)^m\ dz$ , handling most of the cases.  However, the case whenever both $n,m$ are strictly negative is still not clear to me.","['complex-analysis', 'complex-integration', 'cauchy-integral-formula', 'residue-calculus']"
4353775,"If $\log(x^2+2ax)=\log(4x-4a-13)$ has only one solution, find the exhaustive set of values of $a$","If $\log(x^2+2ax)=\log(4x-4a-13)$ has only one solution, then what is the exhaustive set of values of $a$ ? This question was asked at Equation $\log(x^2+2ax)=\log(4x-4a-13)$ has only one solution; then exhaustive set of values of $a$ is , but the answers there don't match my book's given answer $$\left(-{13}4,-\frac{13}{12}\right) \cup [-1].$$ My Approach: For $\log(4x-4a-13)$ to be valid, $4x-4a-13>0,$ i.e., $x>\frac{4a+13}{4}.$ For $\log(x^2+2ax)$ to be valid, $x^2+2ax>0,$ i.e., $x(x+2a)>0;$ here we have two cases case 1 $a>0$ $\implies$ $x\in(-\infty,-2a)\cup (0,\infty)$ case 2 $a<0$ $\implies$ $x\in(-\infty,0)\cup (2a,\infty).$ For one solution, $x^2+2ax=4x-4a-13$ and its discriminant equals $0.$ So, I got $a=-1$ and $a=9.$ For $a=9,$ I got $x=-7$ which not a valid solution because $\log(x^2+2ax)$ will be invalid. But how to arrive at the other part of the given solution?","['algebra-precalculus', 'logarithms']"
4353776,"What is the function of a curve like a ""」""?","I'm doing curve fitting and the curve look like this . I've tried polynomial or exponetial functions, but none of them fit the curve very well. Can someone give some advice? Thanks in advance!","['curves', 'functions']"
4353854,"If $\lim_{x\to\infty}\frac{f'(x)}{f(x)}=1$, is $f$ asymptotic to $\exp$?","For a differentiable and nonzero function $f:(a,\infty)\to\mathbb R$ , it seems like the local and end behavior of $f'(x)/f(x)$ gives a measure for how similar $f$ is to an exponential function. This is a reasonable guess because the quantity $f'(x)/f(x)$ gives the derivative of $\ln\circ f$ at $x$ . My suspicion began when I noticed that for any exponential $a^x$ , we have $$\frac{f'(x)}{f(x)}=\frac{a^x\ln(a)}{a^x}=\ln(a)$$ indicating that exponential functions are the only functions for which $f'/f$ is constant. Exploring this further with the hyperbolic cosine $\cosh(x)$ , a function which is basically identical to $\exp(x)$ as $x\to\infty$ , I arrived at $$\frac{f'(x)}{f(x)}=\frac{\sinh(x)}{\cosh(x)}=\tanh(x)\to 1\text{ as }x\to\infty$$ These examples were enough to convince me, perhaps erroneously, that $f'/f$ gives a measure for how similar a function is to an exponential, leading me to the following conjecture: Suppose $f:(a,\infty)\to\mathbb R$ is a differentiable function that is never zero. If $\lim_{x\to\infty}\frac{f'(x)}{f(x)}=L$ , does it follow that for some constant $C$ , $f(x)\sim C\exp(Lx)$ as $x\to\infty$ ? For simplicity, I focused my attention to the case where $f'(x)/f(x)\to 1$ , $f$ is strictly positive, and $f'/f$ is ""eventually integrable"", i.e. there is a constant $c$ such that for every $x$ greater than $c$ , $f'/f$ is integrable over $[c,x]$ . Unraveling $\lim_{x\to\infty}\frac{f'(x)}{f(x)}=1$ with the definition of a limit and leveraging these simplifying assumptions, I was able to deduce the following: For every $\varepsilon>0$ , there is a $\delta\in\mathbb R$ such that for some constants $C_1,C_2>0$ , $$C_1\exp\left((1-\varepsilon)x\right)<f(x)<C_2\exp\left((1+\varepsilon)x\right)\text{ for every }x\in\text{dom}[f'/f]\text{ with }x>\delta$$ As we make $\varepsilon$ smaller and smaller, the functions $\exp\left((1-\varepsilon)x\right)$ and $\exp\left((1+\varepsilon)x\right)$ will look more and more like $\exp(x)$ , suggesting that the ultimate goal of $\lim_{x\to\infty}\frac{f(x)}{C\exp(x)}=1$ might be true. However, it doesn't seem like my result is sufficient to arrive at this conclusion. The constants $C_1,C_2>0$ depend on $\varepsilon$ and $\delta$ , so I'm afraid they may fluctuate enough to ruin any hopes of asymptotic equivalence. Is my simplified conjecture, equipped with the integrability assumption, actually true? If so, how can I reach the goal? If not, what other assumptions on $f$ are needed to ensure asymptotic equivalence?","['asymptotics', 'real-analysis', 'calculus', 'limits', 'exponential-function']"
4353916,Understanding our prof's definition of P vs NP,"(I have read a lot of online articles, including on MO, SO, etc. but my question stays) We have the following definitions: P are all deciding problems that have a polynomial algorithm So as I understand: For $Y \subset X$ and some $x \in X$ we can say in polynomial time if $x \in Y$ or not. Next definition: A deciding problem $A=(X,Y)$ is in $NP$ if there is a polynomial $p$ and a deciding problem $B=(X',Y')\in P$ so that the following holds: $X' = \{x\#c \,\,|\,\ x\in X, \, c\in \{0,1\}^{\lfloor p(\text{size}(x) \rfloor}\}$ and $Y = \{y \in X \,\,|\,\,\exists c\in\{0,1\}^{\lfloor p(\text{size}(x) \rfloor}\,:\,y\#c \in Y'\}$ So I understand this definition the following way: Given some $x\in X$ we want to know if $x \in Y$ . In case we have a certificate $c\in\{0,1\}^{\lfloor p(\text{size}(x) \rfloor}$ appended to $x$ we can see in polynomial time if $x \in Y$ , since $B\in P$ . First problem: I don't see the purpose of such a certificate. Like what would be such a certificate in praxis? Don't we just get $x$ alone instead of $x\#c$ in praxis? Second problem: Since $x$ is non-infinite, so is $\lfloor p(\text{size}(x) \rfloor$ . Therefore, we could just iterate over all $2^{\lfloor p(\text{size}(x) \rfloor}$ certificates, append them each time to $x$ and check in polynomial time if $x\in Y$ . Final $O$ would be still polynomial time. So wouldn't that mean $P = NP$ ? I guess I don't really see the difference between $P$ and $NP$ here.","['computational-complexity', 'np-complete', 'discrete-mathematics', 'algorithms']"
4353944,"Prove that $k_{n+1}\in \left\{{Nk_{n}+1,Nk_{n}-1}\right\}$ is not a recursive prime generator","For positive $N,k_{0}$ prove that the recurrence relation does not always produce prime numbers (regardless of the recurrence choice). I have seen this claimed for $N=2$ and the proof thereof is fairly clear: you separate the recurrence choices according to divisibility of $k_{0}$ by 3 and then proceed to compare the remainders of the numbers in the sequence by the prime $k_{0}$ . It turns out the mapping from the remainder of $k_{n}$ to the remainder of the following is always injective and because the range of values is finite, the remainders have to repeat, specifically the remainder 0. I have been wondering however how to prove this holds for other $N$ (I do not believe any would produce a prime generator). The previous approach does not lend itself well to this. I have seen an old contest question claiming it isnt a generator for $N=1986$ . How to go about proving this, if not in general, at least for a specific number? Thank you!","['recurrence-relations', 'prime-numbers', 'sequences-and-series']"
4353951,"if $P(A \cap B \cap C)=P(A)P(B)P(C)$, does it necessarily means $A,B,C$ are independants?","Lets say we have the events $A,B,C$ . We know that if they are independants, then the following occurs: $$P(A \cap B \cap C)=P(A)P(B)P(C)$$ But does it work the opposite way? If some $A,B,C$ events satisfies the equation above, does it mean they are necessarily independent? Another question: when we say that $A_1,A_2,...,A_n$ are independants, does it mean they are independants in pairs or in any $2 \le k \le n$ groups?","['probability-theory', 'probability']"
4353967,"$\left|\int_0^1\frac d{dt}u(tx)\,dt\right|\le\int_0^1\sum_{i=1}^N|x_i|\left|\frac{\partial u(tx)}{\partial x_i}\right|dt$ for $u\in C^1_c(\Bbb{R}^N)$","It’s my first question so excuse me if it is probably not very well written. Reading the proof of Morrey's inequality (Theorem 9.12 in "" Functional  Analysis,  Sobolev  Spaces  and  Partial  Differential  Equations "" by H. Brezis) I got stuck at the following inequality: $$|u(x)-u(0)|=\left|\int_{0}^{1} \frac{d}{dt}u(tx)\,dt \right|\leq \int_{0}^{1}\sum_{i=1}^{N}|x_{i}|\left| \frac{\partial u (tx)}{\partial x_{i}}\right| dt$$ with $x \in \mathbb{R}^{N} $ and $u \in C^{1}_{c}(\mathbb{R}^{N})$ . I'm new to advanced math, so can someone give me some hints about it? I apologize if it’s something trivial. Thank a lot for your kindness!  :)","['integration', 'real-analysis', 'functional-analysis', 'partial-derivative', 'inequality']"
4354002,Predual of disc algebra and $W(\mathbb{T})\neq A(\mathbb{D})$,"I want prove that the Wiener algebra , $W(\mathbb{T})$ , does not coincide with the disc algebra $A(\mathbb{D})$ . I know that there are some ways to do this. For example, one smart way is use Rudin-Shapiro sequence, see this post . Note that $W(\mathbb{T})$ is an isometry to $\ell^1$ , with pre-dual $c$ (or $c_0$ , even something else ). However, I never hear story about the pre-dual of disc algebra . Question: Does $A(\mathbb{D})$ has pre-dual? What it is?","['banach-spaces', 'functional-analysis', 'banach-algebras', 'dual-spaces']"
4354047,Tangential planes on a surface at the points of intersection with the sphere,"I was looking at a question in an old exam at it happens that someone has already asked it. However, I have one question regarding the formulation of the answer: Prove that tangent planes to the surface $a(xy+yz+xz)=xyz,a>0$ at the points of intersection with the sphere $x^2+y^2+z^2=r^2$ cut off segments on the coordinate axes whose sum is constant. In the accepted answer, it is suggested to rewrite the equation of the surface as $\frac1x+\frac1y+\frac1z=\frac1a$ in order to use $f(x,y,z)=\frac1x+\frac1y+\frac1z-\frac1a$ and get $\nabla f(x,y,z)=-\left(\frac1{x^2},\frac1{y^2},\frac1{z^2}\right)$ and the equation of the tangent plane at a point of intersection $(x_0,y_0,z_0)$ with the sphere is: $$\frac1{x_0^2}(x-x_0)+\frac1{y_0^2}(y-y_0)+\frac1{z_0^2}(z-z_0)=0.$$ It follows that the lengths of the segments are $$x=x_0^2\frac{x_0y_0+y_0z_0+x_0z_0}{x_0y_0z_0}\\y=y_0^2\frac{x_0y_0+y_0z_0+x_0z_0}{x_0y_0z_0}\\z=z_0^2\frac{x_0y_0+y_0z_0+x_0z_0}{x_0y_0z_0}$$ and $$\begin{aligned}x+y+z&=(x_0^2+y_0^2+z_0^2)\frac{x_0y_0+y_0z_0+x_0z_0}{x_0y_0z_0}\\&=\frac{r^2}a\end{aligned}$$ However, there are intersection points on the axes, so, $f(x,y,z)=a(xy+yz+xz)-xyz$ gives $\nabla f(x,y,z)=(ay+az-xz,ax+az-xz,ax+ay-xy).$ If $$(x_0,y_0,z_0)=(\pm r,0,0), \nabla f(x_0,y_0,z_0)=(0,\pm ar,\pm ar),$$ the tangential plane is parallel to the $x$ axis, moreover, the $x-$ axis lies in the plane. How is then the length of a segment on the axis defined? Should we isolate this as a special case?","['multivariable-calculus', 'tangent-spaces']"
4354072,Smoothing in analytic number theory,"Often we are interested in the sum
\[ \sum _{n\leq x}a_n\]
for some number theoretic sequence $a_n$ and often the study of the ""smooth"" sum
\[ \sum _{n=1}^\infty a_n\phi _x(n)\]
if simpler (so here $\phi _x(n)$ is a nice ""smooth"" function). Can anyone provide me with any material that helped them understand ""smoothing"" better?  In particular examples from number theory would be great.  This resource on smoothing sums is already pretty good  but I'd love to have some more material.  I do get the idea, but I'd like to see a few more examples.  I often see it used in papers but with all the details left out, and I'm aware that I can't really fill in the gaps in some papers, see e.g. the proof of Theorem 1 here .  They don't explicitly say anything about their weight function so I don't really know how to calculate with it - in particular I don't understand the bounds on page 277.  (I'm not sure what goes on in the ""iterated integration by parts"" bit.) I think Kowalski's book Un cours de théorie analytique des nombres has some stuff on smoothing done with some details, but only in French... does anyone know if there's a translation?","['analytic-number-theory', 'number-theory', 'smooth-functions', 'elementary-number-theory']"
4354073,No. of arrangements of letters of SUCCESS such that first C precedes first S,"In how many ways can we arrange the letters of the word SUCCESS such that the first C precedes the first S My attempt there are four strings that satisfy the above condition, namely $$CSSSC, CSSCS, CSCSS, CCSSS$$ Now, we need to arrange $U, E$ . In each string, there are six gaps where we can place our $U,E$ . Since, each gap can contain both $U$ and $E$ , either one of them or neither. Thus, number of ways of arranging $U$ and $E$ is 36. Thus, number of arrangements should be $36 \cdot 4=144$ . Answer is given $168$ . Can anyone point out where am I going wrong",['combinatorics']
4354087,Comparing $\frac {9}{\sqrt{11} - \sqrt{2}}$ and $\frac {6}{3 - \sqrt{3}}$ (without calculator),"We want to compare the following two numbers: $$x = \frac {9}{\sqrt{11} - \sqrt{2}} \quad\text{and}\quad y = \frac {6}{3 - \sqrt{3}}$$ My attempts so far: I multiply both numerator and denominator of $x$ by $\sqrt{11} + \sqrt{2}$ so I get: $$x = \frac {9(\sqrt{11} + \sqrt{2})}{(\sqrt{11} - \sqrt{2})\cdot(\sqrt{11} + \sqrt{2})}$$ so $$x = \frac {9(\sqrt{11} + \sqrt{2})}{(11 - 2)} = \sqrt{11} + \sqrt{2}$$ Similarly, $y = 3 + \sqrt{3}$ . But how do I take it from this point forward?
Of course $y>x$ but I must prove it.
I also tried to compare $\sqrt 11$ with $\sqrt 12$ which equals $2 \sqrt3$ but again I am not getting anywhere. Thank you.",['algebra-precalculus']
4354109,Would anyone know of a book with a proof for the HNN extension which is purely algebraic in it?,"Would anyone know of a book with a proof for why you HNN extension which is purely algebraic in it? I understand that proofs for it usually involve topology, and I haven't done any topology. By this, I mean a proof for why you can apply the HNN extension to any group G with isomorphic subgroups A and B to get a group G* such that G is a subgroup of G* and A and B are conjugate in G. Why does this work for all such groups G, A and B?","['group-theory', 'combinatorics', 'reference-request']"
4354133,Nilradical in an algebra over a field,"In general, if $K$ is a field, it could be that exists $f(x)\in K[x]$ such that $f(a)=0$ for all $a\in K$ ; for example, set $K:=\mathbb Z/(2)$ and $f(x):=x^2+x$ . Now, if $f(x)$ vanishes on all $\operatorname{Spec} K[x]$ , it means that $f(x)\in \mathfrak p$ for all $\mathfrak p\subset K[x]$ , so that $f(x)$ is nilpotent. However $1$ is clearly not nilpotent. Is this because the sets $\operatorname{Spec} K[x]\neq K$ in general, and the fact that $f$ vanishes on all $K$ only means that $f$ belongs to the (always maximal) ideals of the form $(x+a)$ for $a\in K$ ? That would make sense to me, in fact in the example above $x^2+x$ belongs to both $(x)$ and $(x+1)$ . So is there any characterization of the fields $K$ for which, if $f\in K[x_1,\dots ,x_n]$ vanishes on all $K^n$ , it vanishes on $\operatorname {Max Spec}K[x_1,\dots ,x_n]$ ? For example if $K$ is algebraically closed and $n=1$ , then as sets $\operatorname{Max Spec}K[x_1]=K$ . However this already isn't true anymore for $n\gt 1$ or if $K$ is not algebraically closed. For example, are there any polynomials in $\mathbb R[x]$ belonging to all the ideals of the form $(x-a)$ but not in $x^2+1$ ? What is the geometric interpretation of this fact, if there is any?","['maximal-and-prime-ideals', 'affine-varieties', 'algebraic-geometry', 'abstract-algebra', 'commutative-algebra']"
4354154,Is this series representation of the hypotenuse symmetric with respect to the sides of a right triangle?,"This expression is more of a curiosity (perhaps even a tautology) than a practical method of finding the hypotenuse, as it requires taking the root of the sum of the squares of the catheti, which itself yields the result, ie, the length of the hypotenuse. Still, it leads to a question that might be of interest. Given a right triangle with smaller cathetus a , larger cathetus b and hypotenuse c : The infinite series is readily derived by constructing the normal line to the hypotenuse through the right angle and observing that this creates a similar triangle with dimensions in a ratio of b/c to the first. Iteration of this process ad infinitum  leads to a subdivision of the hypotenuse into lengths that converge to zero and whose infinite sum is equal to the hypotenuse. My question is, as this series is derived with assumption that a is the smaller cathetus, is it symmetric with respect to both catheti, a and b ? In other words, are both sides interchangeable in the series, and if so, why? $$c=a^2\sum_{n=1}^\infty\frac{b^{2n-2}}{(a^2+b^2)^{\frac{2n-1}2}}$$","['geometry', 'sequences-and-series']"
4354160,Differentiating vector function-matrix-vector function products,"Consider the following scalar which is the result of a vector-matrix-vector product: $$ h( \bf{x} ) = \bf{f}( \bf{x} ) A \bf{g}(\bf{x})^T$$ where $\bf{x} = (x_1, x_2, \ldots, x_k)$ is an input vector $\bf{f}(\bf{x}) = (f_1(\bf{x}), f_2(\bf{x}), \ldots, f_p(\bf{x}))$ is a $p$ vector of simple, known functions $\bf{g}(\bf{x}) = (g_1(\bf{x}), g_2(\bf{x}), \ldots, g_q(\bf{x}))$ is a $q$ vector of simple, known functions $A$ is a $p \times q$ matrix of (known) constants. Denote the $i$ th row of $A$ by $\bf{a}_i$ and the $j$ th column by $\bf{a}_{(j)}$ . What I require is $\frac{\partial h(\bf{x})}{\partial \bf{x}}$ . I am relatively un-familiar with matrix and vector type calculus. I studied some vector-calculus some years ago but the introduction of matrices makes this fiddly for me. but this is what I tried to use the fact that the final result is just a double sum to help me: $$\frac{\partial}{\partial \bf{x} } h(\bf{x}) = \sum_{i=1}^p \sum_{j=1}^q \frac{\partial}{\partial \bf{x}}f_i a_{ij} g_j$$ (dropping dependence on $\bf{x}$ in $f_i$ and $g_j$ ) $$= \sum_{i=1}^p \sum_{j=1}^q \frac{\partial f_i}{\partial \bf{x}} a_{ij} g_j + \sum_{i=1}^p \sum_{j=1}^qf_i a_{ij} \frac{\partial g_i}{\partial \bf{x}}$$ (product rule for partial derivatives + break up the sum)
Now in principle this is enough, however, I'd like to write the final result as a matrix-vector product for (a) consiseness and (b) I'd like to use the matrix-vector product rather than the double sum for computation. Ploughing ahead gives us $$= \sum_{i=1}^p \frac{\partial f_i}{\partial \bf{x}}\sum_{j=1}^q  a_{ij} g_j + \sum_{j=1}^q\frac{\partial g_j}{\partial \bf{x}} \sum_{i=p}^q a_{ij}f_i $$ (re-order the sum) $$= \sum_{i=1}^p \frac{\partial f_i}{\partial \bf{x}} \bf{a}_{i} \bf{g}^T + \sum_{j=1}^q\frac{\partial g_j}{\partial \bf{x}} \bf{a}_{(j)}^T \bf{f}^T $$ (collecting the ""2nd sums"" into vector products). Now here is where I am stumped. The dimension of the result so far is correct (dim = $1 \times k$ ) but I'm struggling to write this as a vector-matrix product. The result looks a bit like $\frac{\partial \bf{f}}{\partial \bf{x}} A \bf{g}^T + \frac{\partial \bf{g}}{\partial \bf{x}} A^T \bf{f}^T$ but clearly the dimensions are incorrect. Dimension of $\frac{\partial{h}}{\partial \bf{x}}$ are $1 \times k$ whereas I think the dimension of my guess is $(p \times k)(p \times q)(q \times 1) + (q \times k)(q \times p)(q \times 1)$ . These dimensions are not compatible in $2$ ways!","['matrices', 'multivariable-calculus', 'derivatives']"
4354181,"Flipping k consecutive binary numbers at a time, how many zeros can we attain?","Given a sequence of $n$ numbers, all numbers are $1.$ At each step , we will choose any $k$ consecutive numbers and change their states ( $1 -> 0 $ and $0->1$ .) Find the maximal number of zeros we can have? I think the answer to this problem is: We have $n\equiv s \mod k$ , $(0\le s < k)$ . If $2s>k$ the answer is: $[\frac{n}{k}]*k+2s-k$ else , the answer is : $[\frac{n}{k}]*k.$ Obviously, the first answer that pops up in our heads is: $[\frac{n}{k}]*k$ . From there I give you an example like this: For example, if $n=18 , k  = 5$ and we currently have $000000000000000111$ then one possible step is: $000000000000000111 → 000000000000011000$ Hope to get help from everyone. I'm a newbie. Thanks very much!","['combinations', 'combinatorics']"
4354195,Hahn-Banach extension of positive functional is positive,"Consider the following lemma from Takesaki's book ""Theory of operator algebra I"": Why is the sentence ""Any Hahn-Banach extension of a positive linear functional on a $C^*$ -subalgebra of $A$ is positive"" true? Attempt: Let $B\subseteq A$ a $C^*$ -algebra and $\omega: B \to \mathbb{C}$ positive. If I can show that $\|\omega\|= \omega(b)$ for some positive $b \in B$ , then a Hahn-Banach extension $\widetilde{\omega}: A \to \mathbb{C}$ satisfies $\|\widetilde{\omega}\| = \widetilde{\omega}(b)$ as well and thus the lemma implies the result. But is it true that the operator norm of a positive functional is attained at a positive element of the unit ball? Any help/comments are highly appreciated!","['von-neumann-algebras', 'c-star-algebras', 'operator-algebras', 'functional-analysis', 'hahn-banach-theorem']"
4354239,Prove that $\cot 99^\circ=3\cot36^\circ-\sec18^\circ-\csc18^\circ$,"Prove that: $$\cot 99^\circ=3\cot36^\circ-\sec18^\circ-\csc18^\circ$$ Actually, I have a proof here (see edit section at the bottom of the text), which I needed to solve a geometry problem fully. It's definitely correct but I don't like it. The proof seems to be simple and short but it's very hard to do it completely by hand without some machine help. So I'm looking for a more ellegant proof and also hoping to learn something new.",['trigonometry']
4354269,every proper compact subgroup of the circle group is finite,"let $\mathbb T$ be the circle group and $H \subsetneq \mathbb T$ be a proper, compact subgroup . i'm trying to show that $H$ must be finite using only topological arguments, but i'm having trouble doing so. so i want to show that $1$ is isolated in $H$ , as then $H$ is discrete and compact, hence finite. but what if $1$ happens to be a limit point of $H$ ? does anyone have an idea? background . i'm trying to give a high level argument for why $\langle x \rangle \subseteq \mathbb T$ must be dense for all infinite order elements $x \in \mathbb T$ . i'm aware of low level analysis arguments for this, but i'm not interested in them.","['general-topology', 'topological-groups']"
4354289,Computing the expected value of the fourth power of Brownian motion,"I am trying to derive the variance of the stochastic process $Y_t=W_t^2-t$ , where $W_t$ is a Brownian motion on $( \Omega , F, P, F_t)$ .
At a certain point it is necessary to compute the following expectation $$\mathbb{E}[W_t^4]= 4\mathbb{E}\left[\int_0^t W_s^3 dW_s\right] +6\mathbb{E}\left[\int_0^t W_s^2 ds \right]$$ I know the solution but I do not understand how I could use the property of the stochastic integral for $W_t^3 \in L^2(\Omega , F, P)$ which takes to compute $$\int_0^t \mathbb{E}\left[(W_s^3)^2\right]ds$$ Similarly, why is it allowed in the second term $$\int_0^t \mathbb{E}[W_s^2]ds$$ to move the expectation inside the integral?","['stochastic-integrals', 'stochastic-processes', 'martingales', 'probability-theory', 'stochastic-calculus']"
4354299,Discrete Dirichlet problem has a unique solution,"I am studying Artin's Algebra . In Chapter 1 Exercise M11, he asks to show that every discrete Dirichlet problem on a finite discrete set in plane has a unique solution. This is essentially the question: Let $R$ be a finite subset of $\mathbb{ Z\times Z}$ . Let $\partial R$ be the set of all the points, not in $R$ , which are unit distance away from some point in $R$ . Let $\beta$ be a function on $\partial R$ . Then we need to show that there exists a unique function $f$ on $R\cup \partial R$ such that $f(u, v) = \beta_{u, v}$ for all $(u, v)\in\partial R$ and that $f$ satisfies the discrete Laplace equation $^1$ for all points in $R$ . The full question can be found here . My attempt at solving this: After arbitrarily ordering $R$ , we get a linear system $LX = B$ to solve for $X$ where $B$ contains combinations of $\beta_{u, v}$ 's. We just need to show that any such $L$ is invertible, or equivalently, has a nonzero determinant. I abstracted out the following properties that any coefficient matrix $L$ must satisfy (possibly after a rearrangement of equations, provided that we write the equations as mentioned in the footnote): The diagonal entries are all $4$ . Nondiagonal entries can be either $0$ or $-1$ . Any row can have at most four $-1$ entries (since any point in $R$ can have at most four points in $\partial R$ that are a unit distance away from it). Any column in $L$ can also have a maximum of four $-1$ entries (since any point in $R$ can be a unit distance away from at most four points in $R$ ). Now I consider an arbitrary matrix $A$ satisfying the above properties. I rearrange the columns and rows (by elementary row and column operations) to get the following matrix: $$
\begin{bmatrix}
B & \ast\\
\ast & B'
\end{bmatrix}.
$$ Here $B$ and $B'$ are matrices satisfying the above properties, with the additional condition on $B'$ : its first column is entirely nonzero, and contains all the $-1$ 's of the first column of the original $A$ . Thus $b'$ has size at most $5\times 5$ and at least $1\times 1$ . By induction, we can row reduce $B'$ to $I$ , and thus we can row reduce $A$ to $$
\begin{bmatrix}
B' & 0\\
\ast & I
\end{bmatrix}.
$$ Now all I need to show is that $B'$ is also invertible. The brute force method contains $2^{20}$ cases with the general matrix to be analyzed for invertibility being $$
\begin{bmatrix}
4 & &\ast\\
&\ddots&\\
\ast&&4
\end{bmatrix}_{5\times 5}
$$ with the nondiagonal entries being $-1$ or $0$ . (This case will also suffice for the cases when $B$ has a smaller size.) But I am unable to prove this base case without brute force. Can you help? $^1$$f$ satisfies the Laplace equation at $(u, v)\in R$ iff $4f(u, v) = f(u+1, v) + f(u-1, v) + f(u, v+1) + f(u, v-1)$ .","['harmonic-functions', 'matrices', 'linear-algebra', 'discrete-mathematics', 'finite-differences']"
4354344,Proving an implication of the form: $\lim_{x \to \infty}$ of derivative/function implies $\lim_{x \to \infty}$ function/derivative?,"(Assume $?$ is some function that is continous and differentiable everywhere) For example, if $\lim_{x \to \infty} ?' = 7$ implies $\lim_{x \to \infty} ? = \infty$ or $\lim_{x \to \infty} ? = 5$ implies $\lim_{x \to \infty} ?' = something$ Basically, how do you (or how would you ) start writing a proof that is something along the lines of: $\lim_{x \to \infty} ?' = something$ implies $\lim_{x \to \infty} ? = something$ $\lim_{x \to \infty} ? = something$ implies $\lim_{x \to \infty} ?' = something$ TL;DR How would you start to think about this type of problem (i.e. limit of derivative/function implies limit function/derivative) and formulate a plan and prove the question? I cannot think of a good starting point to snowball into some kind of solution since the definition of limit doesn't get me anywhere. I have thought about using L'Hopital's rule shown here , but you would have to show $\lim_{x \to \infty} \frac{f(x)}{x} = a$ implies $f(x)→\infty$ for $x→\infty$ to be rigourous, but that seems like an equally difficult problem. If $a = 0$ it does not hold (as it says a, a must be greater than zero), but under what circumstances can you assume $\lim_{x \to \infty} \frac{f(x)}{x} = a$ and that $a$ must be greater than zero? Some other proofs of similar questions use MVT but also Cauchy's subsequence and a lot of different messy variables and inequalities, which is much more complicated then these problems should require (for a 'simple' analysis question) I think (or maybe not, I'm not sure). The last proof here only uses MVT but the conclusion doesn't make much sense to me ( $f(y)>M$ ), and the post is so old that commenting is probably pointless. To me it seems like using the definition of limit is the best place to start, such as trying to show $\forall \varepsilon > 0$ , $\exists M>0$ such that $x>M$ implies $\mid f(x) - L \mid < \varepsilon$ for some limit as $x$ approaches infinity and equals some number $L$ . If $L = \infty$ then perhaps showing $\forall M > 0$ , $\exists N>0$ such that $f(x)>M,  \forall x>N$ would work as well. But that is the hard part that I get stuck on. My problem is that I do not see how MVT or L'Hopital's Theorem (The most useful theorems I could think of to solve the problem) or the given assumption that $\lim_{x \to \infty} ? = something$ or $\lim_{x \to \infty} ?' = something$ (depending on the problem, since usually the if part is used to prove the then part) could be used to show that $f(x)>M$ , $\forall x>N$ or $\mid f(x) - L \mid < \varepsilon$ etc depending on what $something$ is equal to and what $?$ is. To me it seems impossible to prove with the limited amount of information given (which it is not, but it seems like it), which leaves me stuck.","['proof-explanation', 'proof-writing', 'analysis', 'real-analysis']"
4354350,Extracting an asymptotic from a sequence defined by a recurrence relation,"Suppose I have a sequence defined via its first term and a recurrence relation involving summation over all previous values with some coefficients. Here is the sequence I am interested in right now (although I would like to find a general method applicable to other sequences as well, if possible): $$a_0=1,\quad a_n=\sum_{k=0}^{n-1}\frac{a_k}{(n-k+1)!\,(2^n-1)}.\tag1$$ This sequence is positive rational, monotone decreasing, decaying faster than exponentially, with a few initial terms: $$1,\,\frac12,\,\frac5{36},\,\frac1{36},\,\frac{143}{32400},\,\frac{19}{32400},\,\frac{1153}{17146080},\,\frac{583}{857
   30400},\,\dots\tag2$$ It is related to values of the Fabius function . I'm interested in its asymptotic expansion for $n\to\infty$ . Using empirical numeric methods, and with some luck, I came to this conjecture: $$\log_2a_n\stackrel{\color{#aaaaaa}?}=-n \log_2n+\frac{n}{\log2}-\frac{\log _2^2n}{2} +\left(1-\frac{1}{2 \log ^22}-\frac{2}{\log2}\right)+\mathcal O\!\left(\frac{\log^2n}{n}\right)\!.\tag3$$ Could you suggest a way to prove it and find a few next terms of this asymptotic expansion? How should I approach problems like this in general? Update: The sequence in question is directly connected to the values of the Fabius function at negative integer powers of two: $$F\!\left(2^{-n}\right)=2^{-\binom n2}a_n.\tag4$$ There is also a direct non-recursive formula for it (conjectured): $$a_n\stackrel{\color{#aaaaaa}?}=\frac{(-1)^n}{(2;2)_n}\sum_{k=0}^n\frac{\binom n k_2}{2^{(n-1)k}(n+k)!}\sum_{\ell=0}^{2^k-1}(-1)^{\sigma_2(\ell)}\left(\ell-2^k+\tfrac12\right)^{n+k},\tag5$$ where $\left(q;q\right)_n=\prod_{k=1}^n\!\left(1-q^k\right)$ is the q ‑Pochhammer symbol , ${\binom n k}_q=\frac{\left(q;q\right)_n}{\left(q;q\right)_k\,\left(q;q\right)_{n-k}}$ is the q ‑binomial coefficient , and $\sigma_2(\ell)$ is the sum of binary digits of $\ell$ ; note that $(-1)^{\sigma_2\left(\ell\right)}$ is just the signed version of the Thue–Morse sequence .","['summation', 'recurrence-relations', 'asymptotics', 'experimental-mathematics', 'sequences-and-series']"
4354439,"Is (Baby) Rudin being lazy in Theorem 2.38? (I.e., assuming something holds for all sets in an infinite intersection is sufficient)","The theorem in question is: If $\{I_n\}$ is a sequence of intervals on $\mathbb{R}^1$ such that $I_n\supset I_{n+1}$ , then $\cap_{n=1}^\infty I_n\neq\emptyset.$ Rudin says to let $I_n=[a_n,b_n]$ , and since $\{a_n\}$ is bounded above by $b_1$ , it has a sup, which we can call $x$ . $a_m \leq x\leq b_m$ , so $x\in I_m$ for all $m=1,2,3,...$ , so $x\in\cap_{n=1}^\infty I_n\Rightarrow\cap_{n=1}^\infty I_n\neq\emptyset.$ This strikes me as a bit odd: Rudin says something about every set in an infinite intersection and doesn't really worry about the potential qualitative difference between each object in the intersection and the intersection itself--e.g., each interval has a nonzero length, but the intersection may not. I suppose it doesn't matter here because membership doesn't seem to change qualitatively in the same way length does. Nevertheless, it does seem a bit 'sloppy' in a way to make the final deduction Rudin does. Or should it be clear on a case by case basis when this 'jump' is valid to make? It seems to me less 'sloppy' to show that $\cap_{n=1}^\infty I_n \supset [\alpha,\beta],$ where $\alpha=\sup\{a_n\}$ and $\beta=\inf\{b_n\}$ . And then show $\alpha\leq\beta$ . Since $\alpha\leq\beta$ , $[\alpha,\beta]\neq\emptyset$ , $\cap_{n=1}^\infty I_n\neq\emptyset.$ But perhaps the same 'jump' I accuse Rudin of making is still here, just better hidden from me...","['elementary-set-theory', 'general-topology', 'real-analysis']"
4354491,Is $xSx^{-1} \subseteq S$ equivalent to $xSx^{-1} =S$ in a group?,"Let $G$ be a group and $\varnothing ≠S \subseteq G$ and given an element $x \in G$ I want to prove if the following implication holds (or disprove, if it doesn't): $xSx^{-1} \subseteq S \iff xSx^{-1} = S$ My attempt: ( $\Leftarrow$ ) is trivial, so I'll try to prove ( $\Rightarrow$ ). It is sufficient to show that $S \subseteq xSx^{-1}$ , so I'll try prove by contradiction. Assume $S \nsubseteq xSx^{-1} \Rightarrow \exists s \in S: s \notin xSx^{-1}$ and hence $xsx^{-1} \in xSx^{-1} \subseteq S$ , so $xsx^{-1} =s'$ for some $s' ∈ S \Rightarrow s =x^{-1}s'x \Rightarrow s \in x^{-1}Sx$ But now I'm not sure how to produce a contradiction. Any help?","['group-theory', 'abstract-algebra']"
4354494,"A question on shifting and scaling graphs (in calculus, algebra)","I am reviewing high school algebra before studying the university calculus.
I have a question about shifting and scaling graphs. My problem is as follows. $$y=\sqrt{1-{x\over2}}$$ I need to sketch the graph of this function when I already know the graph of $y=\sqrt x$ . First, I rewrote the expression. $$y=\sqrt{{-1\over2}(x-2)}$$ Incorrect Answer I thought I needed to shift the graph by 2 units, and then stretch the graph in the $x$ direction by 2. Then I tried to reflect the graph across the $y$ -axis. Correct Answer Actually, I know the correct answer. I need to reflect the graph across the $y$ -axis first. Then I need to stretch the graph in the $x$ direction by 2. Finally, I need to shift the graph by 2 units to the right. The problem is, I don't understand why I should do it this way. Why is the first approach a wrong answer? Please help me figure out this problem.",['algebra-precalculus']
4354504,"Find all primes $m$ and $l$ such that $2m-1, 2l-1, 2ml-1$ are perfect squares [duplicate]","This question already has answers here : Finding all primes $(p,q)$ for perfect squares. (4 answers) Closed 2 years ago . Find all primes $m$ and $l$ such that the integers $2m-1$ , $2l-1$ and $2ml-1$ are perfect squares. I have been trying to solve this problem from the 2022 Moroccan Maths Olympiad training program, but I wasn’t able to find anything.
By some few calculations, I found that (5,5) is the only couple between 1-100 satisfying the conditions, so I tried proving that it is the only solution using congruence but nothing worked. Help me please.","['contest-math', 'number-theory', 'elementary-number-theory']"
4354548,Expected number of samples to estimate the mode of categorical distribution,"Suppose I have a categorical distribution with pmf $(p_1, \dots, p_n)$ . What is the expected number of iid samples $\mathcal{S} = \{x_1, \dots, x_m\}$ I have to samples for the empirical mode to be equal to the actual mode $\mathrm{mode}(\mathcal{S}) = \arg\max_i p_i$ . That seems like a very standard question but I can't find anything about it online. Clearly, if the mode has probability 1 then the expected number of samples is 1 and when the mode goes to $0$ then the number of samples has to goes $\infty$ . Intuitively the number of samples is related $\frac{1}{\max_i p_i}$ , although the actual number probably depends on the pmf and not only on the probability of the mode. Has this expected number of samples been studied ? Is the intuition of the number of samples being related to $\frac{1}{\max_i p_i}$ true and if so what is the formal relation ?","['expected-value', 'statistics', 'rate-of-convergence', 'probability']"
4354556,Does $E(Y)=0$ hold in this setting?,"Given two continuous random variables $X$ and $Y$ with the same support, we have $E(X)=0$ and $Y\leq|X|$ . Does this setting imply $E(Y)=0$ ? My proof: $Y\leq|X|\Longrightarrow -X\leq Y\leq X \Longrightarrow 0=-E(X)\leq E(Y)\leq E(X) = 0 \Longrightarrow E(Y)=0$ . Does this make sense?","['statistics', 'probability-distributions', 'expected-value', 'probability-theory', 'probability']"
