question_id,title,body,tags
4159844,Generalizing complex derivative as Fréchet/Gateaux derivative,"So it is well-known that complex differentiability of a function $f:\mathbb{C}\rightarrow\mathbb{C}$ is equivalent to the function being Fréchet/Gateaux differentiable and the component functions (obtained by regarding $\mathbb{C}$ as a 2-dimensional vector space over $\mathbb{R}$ ) satisfying the Cauchy-Riemann equations, i.e. the Fréchet/Gateaux derivative at $c\in\mathbb{C}$ should be a linear operator representing multiplication by a complex number and thus be of the form $$f'(c) = \begin{pmatrix}a&-b\\b&a\end{pmatrix}.$$ The fact that the ""directional derivatives"" are required to coincide for all directions in complex analysis leads to a very rigid, yet rich theory. So, related to the above point of view, I wondered if there is a more general theory in which functions on an algebra with Fréchet/Gateaux derivatives that are represented by multiplication operators play an important role? And if so, whether this theory is as rich as complex analysis?","['complex-analysis', 'frechet-derivative', 'gateaux-derivative']"
4159846,"When is an operator $T$ on $L^1(\mu)$ of the form $(Tf)(x)=\int\mu({\rm d}y)p(x,y)f(y)$?","Let $(E,\mathcal E,\mu)$ be a $\sigma$ -finite measure space and $T\in\mathfrak L(\mathcal L^1(\mu))$ . I would like to know which conditions on $T$ would be sufficient to conclude that there is a (possibly signed) transition kernel $\kappa$ on $(E,\mathcal E)$ s.t. $$(Tf)(x)=\int\kappa(x,{\rm d}y)f(y)\;\;\;\text{for all }x\in E\text{ and }f\in\mathcal L^1(\mu).\tag1$$ On page 159 of Analysis of Heat Equations on Domains it is claimed that $$\left\|T\right\|_{\mathfrak L(L^1(\mu),\:L^\infty(\mu))}<\infty\tag2$$ is equivalent to $(1)$ with the additional claim that $\kappa$ has a density with respect to $\mu$ , i.e. $$\kappa(x,B)=\int_B\mu({\rm d}y)p(x,y)\;\;\;\text{for all }(x,B)\in E\times\mathcal E\tag3$$ for some $\mathcal E^{\otimes2}$ -measurable $p:E^2\to\mathbb R$ . How do we see this? And is there a weaker condition which at least implies the existence of a kernel $\kappa$ (which possibly has no density wrt $\mu$ )?","['measure-theory', 'operator-theory', 'lp-spaces', 'functional-analysis', 'semigroup-of-operators']"
4159859,Detail in Matsumura - Commutative Ring Theory - Lemma 1 page 216: structure of complete local ring with coefficient field,"In Mastumura's book Commutative Ring Theory in the proof of lemma 1 page 216 is written that $$ A/\mathfrak{m}^2\simeq K[X_1,\ldots,X_n]/(X_1,\ldots,X_n)^2 $$ and I don't understand why it is true. I explain the context: Lemma 1: Suppose that $(A,\mathfrak{m},K)$ is a Noetherian local rink containing a field $k$ . I $A$ is $\mathfrak{m}$ -smooth over $k$ then $A$ is regular. Recall: $A$ is $\mathfrak{m}$ -smooth over B iff for all $B$ -algebra $C$ with square zero ideal $N$ , all morphism $u:A\to C/N$ with $u(\mathfrak{m}^k)=0$ for some $k$ lift to a morphism $A\to C$ : $\require{AMScd}$ \begin{CD}
A @>>> C/I\\
@AAA @AAA\\
B @>>> C
\end{CD} Proof of the lemma: take a perfect subfield $k_0\subseteq k$ ; then $k$ is 0-smooth over $k_0$ (because of separation of $k/k_0$ : a previous theorem), so that by transitivity, $A$ is also $\mathfrak{m}$ -smooth (a previous theorem), so that we can assume that $k$ is a pefect field. Also replacing $A$ by $\widehat{A}$ , we can assume that $A$ is complete (because $A$ regular iff $\widehat{A}$ is regular). Then A has a coefficient field containing $k$ (previous theorem, recall: a coefficient field is a subfield of $A$ isomorphic to the residue field by quotienting with $\mathfrak{m}$ ). For ease of notation we write $K$ for this. If $\{x_1,\ldots,x_n\}$ is a minimal basis of $\mathfrak{m}$ then as $K$ -algebra we have $$ A/\mathfrak{m}^2\simeq K[X_1,\ldots,X_n]/(X_1,\ldots,X_n)^2 $$ Here is my problem! The solution is not $A\simeq K[X_1,\ldots, X_n]$ . I think the hypothesis $A$ complete should be used but I don't see how. Thanks for help","['algebraic-geometry', 'ring-theory', 'abstract-algebra', 'commutative-algebra']"
4159887,Two-variable limit via change of variables,"It often happens that by considering polar coordinates we can transform a limit of the form $$\lim_{(x,y) \to (0,0)} f(x,y) \quad \quad \quad (1)$$ into one of the form $$\lim_{(r,\theta),\, r \to 0} F(r)G(\theta). \quad \quad (2)$$ I am assuming that $f(x,y)$ is continuous in a punctured ball centered at the origin of $\mathbb{R}^{2}$ . I have two questions regarding this process: a) Given that $x^{2}+y^{2} = r^{2}$ and $(x,y) \to (0,0)$ iff $r \to 0$ , is it OK to omit the $\theta$ in $(2)$ ? b) In case $F(r)\to 0$ as $r \to 0$ and $G$ is a bounded function, it does follow that $\lim_{(x,y) \to (0,0)} f(x,y)=0$ , right? Can you recommend a calculus text wherein an ""observation"" similar to the one in b is stated and proved in detail? Thanks in advance for you replies...","['analysis', 'real-analysis', 'multivariable-calculus', 'calculus', 'limits']"
4159946,Resize and reposition a disk so its bounding box accommodates other objects,"This is a continuation of this post . I have a circle or disk with some rectangles located on its circumference
which have arbitrary widths, heights, and angles (by angle I mean the line connecting center of the circle to center of the rectangle). An example angle is shown for one of the rectangles with pink lines. The center coordinate of the circle is ( centerX , centerY ) and its radius is r . The widths and heights of the rectangles and angles of the lines connecting circle center to each rectangle center are available too. How can I find a new radius r and a new centerX and CenterY for the disk so that it has maximum area and the previous bounding box of the disk (dashed green line) can fully accommodate all the rectangles? A relaxed version of the problem If the above problem is too hard to solve, then, how can I just modify the circle radius r (its center stays the same) so that its previous bounding box can contain all the rectangles?","['coordinate-systems', 'trigonometry', 'circles', 'geometry']"
4159951,Sigma-algebra generated by one to one function,"Nice measurable space is defined as follows in Durrett's Probability: Theory and Examples : $(\Omega,\mathcal{F})$ is said to be nice if there is a 1-1 map $\varphi$ from $\Omega$ into $\mathbb{R}$ such that $\varphi$ and $\varphi^{-1}$ are both measurable. Does this imply the $\sigma$ -algebra generated by $\varphi$ is equal to $\mathcal{F}$ ? I think it is true in an easier case, where $(\Omega,\mathcal{F})$ is the real line equipped with Borel measure, $(\mathbb{R},\mathcal{B})$ , and $\varphi$ is strictly increasing instead of merely one-to-one. A short proof can be: Fix x in $\mathbb{R}$ . If $\omega \leq x$ , then $\varphi(\omega) \leq \varphi(x)$ , so $\omega \in \varphi^{-1}((-\infty,x])$ . If $\omega > x$ , then $\varphi(w) > \varphi(x)$ , so $\omega \notin \varphi^{-1}((-\infty,x])$ . This shows $\{\omega: \omega \leq x\} = \varphi^{-1}((-\infty,\varphi(x)])$ . Since $\{(-\infty,x]: x \in \mathbb{R}\}$ generates $\mathcal{B}$ , $\sigma(\varphi) = \mathcal{B}$ . On a second thought, the above can be generalized. Fix $E$ in $\mathcal{F}$ . By the hypothesis $\varphi^{-1}$ is measurable, I think Durrett assumes $\varphi^{-1}$ exists so $\varphi$ is 1-1 and onto. Then $\varphi(E) = (\varphi^{-1})^{-1}(E)$ . Since $\varphi^{-1}$ is measurable, $\varphi(E)$ is Borel. Since $\varphi$ is 1-1, $E = \varphi^{-1}(\varphi(E))$ . So any set in $\mathcal{F}$ is in $\sigma(\varphi)$ , this shows $\mathcal{F} \subseteq \sigma(\varphi)$ . Also $\mathcal{F} \supseteq \sigma(\varphi)$ . So $\mathcal{F} =\sigma(\varphi)$ .
Is this correct?","['measure-theory', 'probability-theory']"
4159962,"Characteristics of equations of the form $u_{xy}=f(u_x,u_y,u)$","In the usual treatment of hyperbolic differential equations, it is always assumed that there are two families of characteristics. That is, if the equation $L[u]-f(u_x,u_y,u)=au_{xx}+2bu_{xy}+cu_{yy}-f(u_x,u_y,u)=0$ is hyperbolic, by definition the roots $\zeta_{\pm}$ of the polynomial $q(\zeta)=a\zeta^2-2b\zeta+c$ are real, and then one defines the characteristic curves by means of the ODEs $dy/dx=\zeta_{\pm}(x,y)$ . But for example for the equation $u_{xy}=u$ , the only root is $\zeta_+=\zeta_-=0$ , then we would have only one family of characteristics so the equation is not hyperbolic but parabolic, but this contradicts the fact that a equation is parabolic iff $b^2-ac=0$ and hyperbolic iff $b^2-ac>0$ . What's the true nature of the equations of the form $u_{xy}=f(u_x,u_y,u)$ ,  and what are the implications of the fact that only one set of characteristics exists, even though it's supposed to be a hyperbolic equation?","['calculus', 'ordinary-differential-equations', 'partial-differential-equations']"
4160026,Closed form of the sum $s_4 = \sum_{n=1}^{\infty}(-1)^n \frac{H_{n}}{(2n+1)^4}$,"I am interested to know if the following sum has a closed form $$s_4 = \sum_{n=1}^{\infty}(-1)^n \frac{H_{n}}{(2n+1)^4}\tag{1}$$ I stumbled on this question while studying a very useful book about harmonic series and logarithmic integrals whichh has appeared recently [1]. Checking it for possible missing entries I was led to consider this family alternating Euler sums $$s(a) = \sum_{n=1}^{\infty}(-1)^n \frac{H_{n}}{(2n+1)^a}$$ where $H_{n} =\sum_{k=1}^{n}\frac{1}{k}$ is the harmonic number, as well as the corresponding integrals $$i(a) = \int_0^1 \frac{\log ^{a-1}\left(\frac{1}{x}\right) \log \left(x^2+1\right)}{x^2+1} \, dx$$ These are related by $$i(a) = - \Gamma(a) s(a)$$ As listed in detail in the appendix, closed forms exist for all odd $a=2q+1$ . For even $a=2q$ only the case $a=2, q=1$ is known. Hence, the natural question is to aks for a closed form for the smallest case open up to now, $a = 4$ . What did I do so far The integral $i(a)$ can be found by differentiation with respect to a parameter $u$ from the generating integral $$g_i(u) = \int_0^1 \frac{t^u \log \left(t^2+1\right)}{t^2+1} \, dt$$ which is evaluated by Mathematica in terms of hypergeometric functions as follows $$g_i(u) = \frac{1}{4} \left(-\frac{2^{\frac{u+5}{2}} \, _3F_2\left(\frac{1}{2}-\frac{u}{2},\frac{1}{2}-\frac{u}{2},\frac{1}{2}-\frac{u}{2};\frac{3}{2}-\frac{u}{2},\frac{3}{2}-\frac{u}{2};\frac{1}{2}\right)}{(u-1)^2}\\-2 \pi  H_{-\frac{u}{2}-\frac{1}{2}} \sec \left(\frac{\pi  u}{2}\right)-\log (4) B_{\frac{1}{2}}\left(\frac{1-u}{2},\frac{u+1}{2}\right)\right)$$ As the parameter $u$ appears in 7 places each derivative generates a factor 7 in the length of the result. Unless someone comes up with a very clever idea to simplify the hypergeometric expressions this path seems to be hopeless. Appendix: known closed forms For positive integer values of $a$ the following results have been obtained: a) for odd $a=2q+1$ the closed form was calculated in [1], 4.1.15 (4.91) as: $$s(2q+1) = (2q+1)\beta(2q+2) + \frac{\pi}{(2q)! 4^{q+1}}\lim_{m\to \frac12 }\frac{\mathrm{d}^{2q}}{\mathrm{d} m^{2q}} \frac{\psi(1-m) + \gamma}{\sin(m\pi)}$$ Here $$\beta(z)=\sum_{k=1}^{\infty}\frac{(-1)^k}{2k+1)^z}$$ is Dirichelt's beta function. As for a simplification of r.h.s. see https://math.stackexchange.com/a/4139359/198592 b) for even $a=2q$ there is a closed form just for $a=2, i.e. q=1$ , found in  [1] 4.5.5 (4.187) $$s(2) = 2 \;\Im \text{Li}_3(1-i)+ \frac{3\pi^3}{32}+\frac{\pi}{8}\log^2(2) -\log(2) G$$ where $G = \beta(2)$ is Catalan's constant. References [1] Ali Shadhar Olaikhan, ""An introduction to harmonic series and logarithmic integrals"", April 2021, ISBN 978-1-7367360-0-5","['harmonic-numbers', 'polylogarithm', 'logarithms', 'sequences-and-series']"
4160055,Create a unitary matrix out of a column vector,"I need an algorithmic way, to create a unitary matrix out of a first, given, column vector. i.e. if the column vector was $(0 1 1 1)^T$ , I'd need an algorithm to complete it to a matrix like this. $$
\frac{1}{\sqrt{3}}  \quad
\begin{pmatrix}
0 & 1 & 1 & 1 \\
1 & 0 & 1 & -1 \\
1 & -1 & 0 & 1 \\
1 & 1 & -1 & 0
\end{pmatrix}$$ The content of the other column vectors is not relevant, as long as it makes the matrix unitary. The only partial answer I found was in this previous post, but it only covers vectors containing only $1$ . Does anyone know a smart way to accomplish this? EDIT Based on the answer by @ben-grossmann, I've written this Python function to create the matrix, if anyone else needs it. def create_unitary(v):
    dim = v.size
    # Return identity if v is a multiple of e1
    if v[0][0] and not np.any(v[0][1:]):
        return np.identity(dim)
    e1 = np.zeros(dim)
    e1[0] = 1
    w = v/np.linalg.norm(v) - e1
    return np.identity(dim) - 2*((np.dot(w.T, w))/(np.dot(w, w.T)))","['matrices', 'orthogonal-matrices', 'linear-algebra']"
4160066,"If $\nu \ll \mu$ and $\mu$ is finite, does it implies that $\nu$ is also finite?","Let $\mu$ and $\nu$ be two (positive) measures, where $\mu$ is finite. Is it true that
if $\nu \ll \mu$ and $\mu$ is finite, does it imply that $\nu$ is also finite? If so, how does one prove it? If not, what is a counter-example?","['measure-theory', 'absolute-continuity']"
4160096,Prerequisites for rigorous Fourier analysis,"My side project for 2021 is to get a grasp for Fourier analysis. At first I thought that perhaps I should take an application heavy approach and do the heavy math later on. But the more I dwell deeper into the subject I find myself needing proper rigorous proofs and reasoning for the material I am presented. As Fourier analysis seems to rely heavily upon both measure theory and functional analysis, what books/material would you recommend to work on in order to build a solid & rigid foundations in both measure theory and functional analysis? Books and the material should preferably be self-contained. Bonus points if the book/material has been going around long enough, so that I can find discussion regarding the practice problems - mega bonus points if the material happens to have a solution manual. Currently my plan is to first go through both baby Rudin and Ahlfors ( Principles of Mathematical Analysis , Walter Rudin, Comples Analysis , Lars Ahlfors) in order to check that I don't have any serious holes in my understanding of real and complex analysis. From there I was planning to use Functional Analysis by Rudin as my primary source of functional analysis, and a yet undecided book for measure theory. I am also happy to takes notes and comments on this general study plan!","['measure-theory', 'soft-question', 'functional-analysis', 'fourier-analysis']"
4160148,Cauchy problem for an ordinary equation not in normal form,"Let's consider a one-dimensional physical system ( $x$ is the position and $t$ is time) described by a first order differential equation. I'm aware of the fact that if the equation can be put in normal form: \begin{equation}
\dot{x}=f(x,t)
\end{equation} the Cauchy problem, provided some mathematical hypotheses are verified, tells us that the equation admits the existence of an unique solution. Therefore the system is deterministic, i.e., given the initial conditions there is only one admissible motion. I was wondering what happens when the system is described by an equation that cannot be put in normal form: \begin{equation}
g(\dot{x},x,t)=0
\end{equation} Is there anything that proves that the Cauchy problem still provides the existence of an unique solution for an equation not in normal form or are there specific conditions?","['mathematical-physics', 'analysis', 'classical-mechanics', 'ordinary-differential-equations']"
4160228,Minimal number of masks to safely meet all contacts during pandemic,"This is a generalization of a variant of Doctor's Dilemma puzzle . The general problem Given an undirected connected graph $G$ , one meeting is organized for each edge. When two vertices meet each other, one or both can wear any number of available two-sided masks. All masks start out clean, but diseases spread from vertices and masks to vertices and masks, when they are in contact. Any mask can be safely inverted to swap its two sides. We do not want to risk any vertex spreading (WLOG, its own unique) disease to any other vertex. What is the minimal number of masks that we need to order? Given a graph $G$ , is there an efficient way to find minimal number of masks $M(G)$ ? Can we find exact solutions for $M$ for specific families of graphs? Was a similar problem discussed before? I'm looking for references. Lets represent a mask and its (inner, outer) contamination with $(I,O)$ . Short notation $v$ for $\{v\}$ will be used for one element contamination. For example, $(v,\{u,w\})$ is contaminated by $v$ on inner side, by $u$ and $w$ on outer side. Trivially $M(G)\lt |V|$ , where $|V|$ is the number of vertices. If everyone meets with everyone and only $v_n$ doesn't wear a mask, all masks will end up as $(v_i,v_n),i=1,\dots,n-1$ . For example , Given graph $K_{2,2}=C_4$ , it is known that $M=2$ . Let vertices be $\{a_1,b_1,a_2,b_2\}$ and edges be $\{a_1b_1,a_2b_1,a_2b_2,a_1b_2\}$ . First, $a_1b_1$ meet by $a_1$ wearing a clean mask and $b_1$ wearing a clean mask (or equivalently, $a_1$ wears two clean masks stacked on each other while $b_1$ doesn't wear any masks). This results in masks $(a_1,\emptyset),(\emptyset,b_1)$ . Then, $a_1b_2$ meet by $a_1$ wearing the mask that they already contaminated, but $b_2$ does not wear any mask. At the same time, to meet $a_2b_1$ , let $a_2$ wear clean side of mask contaminated by $b_1$ . These two meetings result in masks $(a_1,b_2),(a_2,b_1)$ . Finally, $a_2b_2$ can meet, resulting in masks $(a_2,\{b_1,a_1\}),(\{a_1,b_1\},b_2)$ . We see that at every step, no vertices were contaminated by new diseases, as requested. Notice that $M(G)\ge \left\lceil \frac{|V|}{2} \right\rceil$ where $|V|$ is the number of vertices of graph $G$ , as every vertex will contaminate at least one side of some mask, which confirms that $M=2$ is optimal here. Complete bipartite graphs Can we solve the problem for $K_{n,n}$ graphs? Let $a_i$ and $b_i$ be vertices in first and second set of bipartite graph . When $n=1$ one mask is trivial, and $n=2$ is the previous example. For $n\ge 3$ , It is not hard to see $M(K_{n,n})\le n+\lfloor\frac{n}{2}\rfloor$ , as: $a_i,i=1,\dots,\lfloor \frac{n}{2} \rfloor$ meet with $b_j,j=1,\dots,n$ by contaminating masks to $(a_i,\emptyset)(\emptyset,b_j)$ . Next $a_x$ meets all $b_j$ using all masks of form $(\emptyset,b_j)$ , contaminating them to $(a_x,b_j)$ . Remaining $a_k,k\gt\frac{n}{2}$ meet all $b_j$ using inverted and stacked masks $(\emptyset,a_i),(a_x,b_j)$ . But, this is not optimal. (Also, step 2. is not needed when $n$ is even.) This can be improved to: (see equivalent question on puzzling.SE ) $$M(K_{n,n})\le \left\lceil\frac{5n}{4}\right\rceil,$$ using a strategy that simulates $\frac{m}{2}$ new masks by stacking $m$ masks in pairs $(\emptyset,X),(Y,\emptyset)$ , until about half of $a_i$ met with all $b_j$ . Then, split $b_j$ into two groups and resolve remaining meetings. Can we prove that this improvement is optimal? Other families Can we find both upper and lower bound for some common families? E.g. In comments, someone asked about $K_{1,n}$ . In this case, the lower bound stated at the end of first example can be achieved. Therefore, $M(K_{1,n})=\left\lceil \frac{n+1}{2} \right\rceil$ . E.g. meeting $3$ consecutive edges with $2$ masks in cycle $C_n$ gives $M(C_n)\le \left\lfloor \frac{2n}{3} \right\rfloor$ . E.g. what about complete $K_n$ graphs? General algorithm Can there exist an efficient algorithm for an arbitrary graph? I can recognize three possible ways (moves) to meet some $v_i v_j$ , Contaminate both sides $(v_i, v_j)$ of a new mask $(\emptyset,\emptyset)$ . Contaminate a side $(v_i, v_j)$ of an existing mask $(v_i,\emptyset)$ or $(\emptyset,v_j)$ . Contaminate one side on one or both stacked masks $(v_i, X),(Y, v_j)$ . This will result in masks $(v_i, X\cup Y),(X\cup Y, v_j)$ , unless $(X, Y)$ exists to be placed in between. This question then boils down to, is there a smart way to combine these moves?","['graph-theory', 'extremal-combinatorics', 'reference-request', 'combinatorics', 'recreational-mathematics']"
4160286,Given a real sequence $\{ c_n \}$ does there exist a smooth function g such that $c_n = \int_0^1 t^n g(t) dt$ for all $n \in \mathbb{N}$,"Are either of the following 2 claims correct? Let $\{ c_n \}_{n \in \mathbb{N}}$ be a real sequence. Suppose that there exists an $N \in \mathbb{N}$ such that for all $n > N$ we have $c_n = 0$ . There exists $f \in C^{\infty}(\mathbb{R})$ such that $f(t) = f(2\pi + t)$ for all $t \in \mathbb{R}$ and \begin{align}
c_n = \int_{0}^{2\pi} t^n f(t) \ dt \qquad \text{for all } n \in \mathbb{N}
\end{align} There exists $g \in C^{\infty}(\mathbb{R})$ such that $\text{support}(g) = [0,1]$ and \begin{align}
c_n = \int_{0}^{1} t^n g(t) \ dt \qquad \text{for all } n \in \mathbb{N}
\end{align} Any advice or ideas on how to prove/disprove the claims will be greatly appreciated. If this result exists already, then a reference would be great. EDIT: AN IDEA FOR PART 2 Let \begin{align}
g(t) = \sum_{m=0}^{\infty} a_m t^m
\end{align} on $[0,1]$ and $g(t) = 0$ elsewhere,
then \begin{align}
\int_{0}^1 t^n g(t) dt &= \int_0^1 t^n \sum_{m=0}^{\infty} a_m t^m dt \\
&= \sum_{m=0}^{\infty}a_m\int_0^1 t^{n+m} dt \\
&= \sum_{m=0}^{\infty}\frac{a_m}{n+m+1} \\
&= \sum_{m=0}^{\infty} b_{nm}a_m
\end{align} where $b_{nm} = 1/(n+m+1)$ . Now we define the operator $B : \ell^2 \to \ell^2$ on the Hilbert space of summable sequences $\ell^2$ by \begin{align}
B(a)_n = \sum_{m=0}^{\infty} b_{nm} a_m.
\end{align} If $B$ is surjective, then there exists an $a \in \ell^2$ such that $B(a) = c$ . But is $B$ surjective? I think a similar argument may work for part 1.","['integration', 'functional-analysis', 'real-analysis']"
4160289,Dual solutions of the 1d optimal transport problem,"Let $\mu$ and $\nu$ be two probability measures on the real line with finite $p$ moment for $p\in [1, \infty)$ . I am interested in the functions $f:\mathbb{R}\rightarrow \mathbb{R}$ and $g:\mathbb{R}\rightarrow \mathbb{R}$ that solve the dual of the optimal transport problem $$
OT(\mu, \nu) = \max\left\{ \int f\,d\mu + \int g\,d\nu,\, \Big|\,f\in L^1(\mu), \, g\in L^1(\nu) \, \text{and}\, f(x)+g(y)\leq h(x-y)\,  \right\},
$$ for an arbitrary convex function $h:\mathbb{R}\rightarrow \mathbb{R}$ .
In particular I would be curious to know how $f$ (and $g$ ) look like if $\mu$ and $\nu$ are discrete measures (or in general not absolutely continuous). Thanks in advance for any help :) Update:
If $\mu$ and $\nu$ are discrete then, if we set $f(x_1)=0$ , one of the dual solutions is given by $$
f(x_{i}) = \sum_{t=1}^{i-1} h(G^{-1}(F(x_t)) - x_{t+1}) -  h(G^{-1}(F(x_t)), x_{t}) \quad \text{with} \quad i = 2, \dots, n,
$$ where $F$ is the cdf of $\mu$ and $G^{-1}$ is the quantile function of $\nu$ . This works even if there is no transport map between $\mu$ and $\nu$ . Moreover if $\mu$ and $\nu$ are continuous and have a connected support and $h$ is differentialble, then: $$
f(x) = - \int_{-\infty}^x h'(G^{-1}(F(t)) - t)\, dt.
$$ (More on the motivation of this will come soon in the form of an answer).","['optimization', 'optimal-transport', 'analysis']"
4160305,Derivative Intuition [duplicate],"This question already has answers here : The problem of instant velocity (15 answers) Closed 3 years ago . I'm doing self-study in Calculus, and I'm having trouble with intuitively understanding derivatives. The definition of a derivative is: $\lim _{h\to 0}\left(\frac{f\left(x+h\right)-f\left(x\right)}{h}\right)$ . Also, I will note that H Does not Equal Zero. Here is the issue I'm having: As $\lim _{h\to 0}\:$ does not equal zero, and therefore $h$ does not equal $0$ , there will always be some infinitesimally small distance between $x$ and $x+h$ . Therefore, the derivative is NOT a rate of change at one point, but a rate of change between $x$ and some small infinitesimal distance. I suppose one could say the slope between $x$ and $x+\text{infinitesimal}$ is close to a rate of change at a single point, but it's not the same thing. Maybe an approximation, but an approximation is not the same as the actual. What am I missing here?
Thanks in advance for any assistance!","['calculus', 'derivatives']"
4160312,Nine points in $\Bbb{P}^2$ determine a cubic?,"I'm sure this is a silly question, but for some reason I'm stuck in it. The $k$ -vector space generated by the monomials $x,y,z$ give all possible equations for lines in $\Bbb{P}^2$ and has projective dimension $3-1=2$ . So two points in $\Bbb{P}^2$ determine a line. Analogously, the space of $x^2,xy,xz, y^2,yz,z^2$ gives the equations of conics and has projective dimension $6-1=5$ . So five points determine a conic. For cubics, the space is $x^3,x^2y,x^2z,xy^2,xz^2,xyz,y^3, y^2z, yz^3, z^3$ with projective dimension $10-1=9$ . Then shouldn't nine points determine a cubic? Take for example a cubic $C$ and three concurrent lines $L_1,L_2,L_3$ meeting at $P\notin C$ , each $L_i$ meeting $C$ simply. (for example, take $C:yz^2-(x^2-3z^2)x=0$ , $L_1:y=0$ , $L_2:y-z=0$ and $L_3:y+z=0$ ). The intersection $C\cap L_1L_2L_3$ consists of $9$ distinct points, but $C$ and $L_1L_2L_3$ are distinct cubic through them. What am I missing?","['algebraic-curves', 'algebraic-geometry', 'projective-space']"
4160325,Finding the $p$-value of a test with exponential data,"My problem Assume that $X$ is exponentially distributed such that $X\in\operatorname{Exp}(\lambda)$ . Let $H_0$ be $\lambda=3$ . We want to test $H_0$ against the alternative $H_1$ : $\lambda=1$ and reject $H_0$ if we observe a great value $x$ for $X$ . Assume that we have observed $x = 0.30$ . Determine the p-value of the test. My attempt of solving the solution $X \in\operatorname{Exp}(3)$ p-value $= P(X \geq 0.30) = 0.40657$ My question I think these problems are very difficult, because I am always unsure if I have calculated them correctly. How do I check that my solution is correct?","['statistics', 'p-value', 'exponential-distribution', 'hypothesis-testing', 'probability']"
4160335,Poisson bracket and co-adjoint orbits for $sl(2)$,"So I am trying to do this problem from Peter Olver's book Application of Lie groups to differential equations and I am wondering if somebody could check my work because I am not really sure about it and I am trying to really understand this material by testing myself. So to start off for the Lie-Poisson bracket I am pretty convinced this is not correct because it just doesn't look right. So for $sl(2)$ I used the basis matrices $$ A_1 = \begin{bmatrix} 0 \;\; 1
\\ 0 \;\; 0
\end{bmatrix}
A_2 = \begin{bmatrix} 1  \;\;\;\;\;\; 0
\\ 0 \;\; -1
\end{bmatrix} 
A_3 = \begin{bmatrix} 0 \;\; 0
\\ 1 \;\; 0
\end{bmatrix}$$ Then I used this definition for the Poisson bracket So I calculated all the structure constants with respect to my basis by using the Lie-brack of the matrices and I got $$\{F,H\} = (2x^3,x^2,2x^1) \cdot \nabla F \times \nabla H$$ where $\nabla F = (\frac{dF}{dx^1}, \frac{dF}{dx^2}, \frac{dF}{dx^3})$ now this just feels very wrong... perhaps what is meant by the question is the poisson bracket on $sl(2)^*$ ? which might be what I calculated... I am not sure. Now for the co-adjoint orbits, I first computed the adjoint map and then looked at the dual of that map to find the co-adjoint map and orbits. So what I did was took $X = \begin{bmatrix} a \;\; b
\\ c \;\; d
\end{bmatrix}  \in SL(2) \implies ad-bc = 1$ then $$Ad_X(A_1) = XA_1X^{-1} = a^2 \cdot A_1 -ac \cdot A_2 -c^2\cdot A_3$$ $$ Ad_X(A_2) = XA_2X^{-1} = -2ab \cdot A_1 + (ad+bc) \cdot A_2 + 2dc\cdot A_3 $$ $$Ad_X(A_3) = XA_3X^{-1} = -b^2 \cdot A_1 +bd \cdot A_2 +d^2\cdot A_3$$ So the matrix representation of $Ad_X$ is $$ R= \begin{bmatrix}a^2 \;\;\;\; -2ab \;\;\;\; -b^2 \\
-ac \;\;\;\; ab+bc \;\;\;\; bd \\
-c^2 \;\;\;\; 2dc \;\;\;\; d^2
\end{bmatrix}$$ And so then the matrix representation $Ad_X^*$ is $(R^{-1})^T$ which I got to be $$ (R^{-1})^T= \begin{bmatrix}d^2 \;\;\;\; cd \;\;\;\; -c^2 \\
2bd \;\;\;\; ab+bc \;\;\;\; -2ac \\
-b^2 \;\;\;\; -ab \;\;\;\; a^2
\end{bmatrix}$$ but then I am kind of confused as to how to even think about the orbits here. If I take $A \in sl(2) \implies A = xA_1 + y A_2 + zA_3$ so then $$Ad_X^*(A) = \begin{bmatrix}d^2 \;\;\;\; cd \;\;\;\; -c^2 \\
2bd \;\;\;\; ab+bc \;\;\;\; -2ac \\
-b^2 \;\;\;\; -ab \;\;\;\; a^2
\end{bmatrix} \cdot \begin{bmatrix} x \\ y \\ z\\ 
\end{bmatrix} = \begin{bmatrix} xd^2 +ycd -zc^2 \\ x2bd + yad+ybc -2zac \\ -xb^2 -yab + za^2
\end{bmatrix}$$ this is the basis representation with respect to $A_1,A_2,A_3$ but I am not really sure what this gives me... it doesn't look obvious to me, and I cannot seem to find any solution on the web so I am wondering if I made a mistake or if this is just the final answer and I cannot see what it represents.","['symplectic-geometry', 'lie-algebras', 'adjoint-operators', 'poisson-geometry', 'differential-geometry']"
4160340,On the definition of the stochastic $o$ and $O$ symbols,"Given two sequence $(X_n),(Y_n)$ of random variables with values in $\mathbb{R}^d$ , consider the following definitions: Write $X_n=O_p(Y_n)$ if, for all $\epsilon>0$ , there exists $M>0$ and $N\geq1$ such that $P\Big[\|X_n\|>M\|Y_n\|\Big]<\epsilon$ whenever $n\geq N$ . Write $X_n=o_p(Y_n)$ if, for all $\epsilon>0$ , $P\Big[\|X_n\|>\epsilon\|Y_n\|\Big]\to 0 \text{ as } n\to \infty.$ In the book Asymptotic Statistics , A.W. van der Vaart defines $X_n=O_p(Y_n)$ if $X_n=Z_nY_n$ with $Z_n=O_p(1)$ , and $X_n=o_p(Y_n)$ if $X_n=Z_nY_n$ with $Z_n=o_p(1)$ . But these definitions are not equivalent to the definitions $1,2$ above. For if we take $Y_n=0$ for all $n$ , then $1,2$ are equivalent to $X_n=0$ with probability approaching $1$ , while A.W. van der Vaart's definitions are equivalent to $X_n=0$ for all $n$ . Which of these definitions are preferable? It seems to me that $1,2$ are better since they reduce to the usual meanings of $o,O$ in the nonstochastic case (see here for example). Am I missing something? Thanks a lot for any help. EDIT. Van der Vaart's definition of $X_n=O_p(1)$ is, for all $\epsilon>0$ , there exists $M>0$ such that $\sup_{n\geq 1} P[\|X_n\|>M] <\epsilon$ . Using the fact that any real random variable is tight (see here ), we see that this definition is equivalent to definition $1$ with $Y_n=1$ for all $n$ . Van der Vaart's definition of $X_n=o_p(1)$ is $X_n\overset{p}\to  0$ as $n\to\infty$ .","['statistics', 'asymptotics', 'sequences-and-series', 'probability-theory', 'probability']"
4160363,Prove there exists $y_0 \in \mathbb{R} $ such that $ P(X-Y \in B) \geq P(X-y_0 \in B)$,"I wish to prove that for $ B \in \mathcal{B}(\mathbb{R}) $ and $X$ and $Y$ denote independent random variables there exists $y_0 \in \mathbb{R} $ such that $$
P(X-Y \in B) \geq P(X-y_0 \in B).
$$ My attempt: Consider the measurable function $m:\mathbb{R}^2 \to \mathbb{R}:(x,y) \mapsto x-y$ . Then \begin{align*}
P(X-Y \in B) &= E[1_B(m(X,Y))] \\
&= \int_{\mathbb{R}^2} 1_B(m) d(P_X \otimes P_Y) \\
&= \int_\mathbb{R} \int_\mathbb{R} 1_B(m(x,y)) P_X(dx)P_Y(dy) \\
&\geq \int_\mathbb{R} \inf_{z \in \mathbb{R}} P(X-z \in B) P_Y(dy) \\
&= \inf_{z \in \mathbb{R}} P(X-z \in B).
\end{align*} The infimum exists but how can I pick $y_0$ ? If I pick $y_0$ as an element giving the infimum, I need to know such an element exists. Can I pick it in a different way?","['probability-theory', 'real-analysis']"
4160412,Finding a rare biased coin from an infinite set,"I'm trying to develop an algorithm for finding biased coins. The basic problem formulation is this: There are an infinite number of coins Some proportion $t$ of the coins is biased (this number is known) All biased coins have the same probability $p_b$ of coming up heads (this number is also known) All other coins are fair Biased coins are otherwise indistinguishable from fair ones The task is to find one biased coin, with some confidence, using the fewest number of coin flips. I know the basic solution to a related problem, i.e. determining whether a single coin is biased. Following the formulation in https://en.wikipedia.org/wiki/Checking_whether_a_coin_is_fair , I can set my desired maximum error $E$ to be equal to $|p_b - 0.5|$ and use the equation $n = \frac{Z^2}{4E^2}$ to get the number of coin tosses $n$ required to determine whether the coin is indeed fair using a given $Z$ value. However, I'm curious how the method might change given my formulation, where there are multiple coins and a known proportion are biased. A brute force algorithm, I suppose, would be to select a coin, flip it $n$ times, select another coin, flip it $n$ times, etc. until a biased one is found. But this feels sub-optimal. Is it possible, for instance, to abandon a coin before $n$ flips is reached based on some criteria, i.e. using the evidence collected so far to judge whether it is worthwhile to keep flipping that coin or move on to another? It seems like the value of $t$ , particularly if it is low, should be a useful prior that I can leverage. I'm also concerned that if I test multiple coins, I am at risk of inadvertently finding significance where there is none.",['probability']
4160456,Units of ${\rm Mat}_{n\times n}(\mathbb{Z})$,"I am trying to calculate the units of the ring ${\rm Mat}_{n\times n}(\mathbb{Z})$ , but I am a little bit stuck. Here is my try: Let's take $A\in U({\rm Mat}_{n\times n}(\mathbb{Z}))$ ; then, there exists $B\in {\rm Mat}_{n\times n}(\mathbb{Z})$ such that $AB=BA=Id$ ; taking determinants, we conclude that this implies that $\det(A)\det(B)=\det(Id)=1$ ; so, taking on account that the determinant of a matrix with integer coeficients is also an integer, we have two options from the last conclusion: or $\det(A)=\det(B)=1$ or $\det(A)=\det(B)=-1$ ; so by all of this, I would be able to conclude that $$U({\rm Mat}_{n\times n}(\mathbb{Z}))\subseteq\{A\in{\rm Mat}_{n\times n}(\mathbb{Z}): \det(A)=\pm 1\}$$ How should I proceed for finishing and giving the concrete units of this ring? Thank you in advance for your help!","['matrices', 'group-theory', 'ring-theory']"
4160490,Filtration and convergence,"Considering any $(\mathcal{F}_r)_{r \geq 0}$ -adapted processes $X_r,$ if we want to prove the convergence of $X_r$ and depending on the properties of the processes, we often use filtrations other than $\mathcal{F}_r$ $(\mathcal{F}_{r+},\mathcal{F}_{r-},...),$ in order to facilitate the work or to obtain a desired property. In general, the filtration, does it affect the convergence of $X_r$ ? How?","['stochastic-analysis', 'stochastic-processes', 'martingales', 'probability-theory', 'stochastic-calculus']"
4162543,What am I missing with Cantor's diagonal argument?,"Let's start with Hilbert's Hotel . A hotel exists with infinite rooms. The rooms are all full with infinite guests. A new guest arrives. The manager asks every person to go to their room number plus one and voila - Room 1 is open for the new guest. $\infty + 1 = \infty$ . Now let's look at Cantor's diagonal argument . Are there more natural numbers than numbers between 0 and 1? We have an infinite sheet of paper. On the left we write our naturals, 1, 2, 3, and so on. On the right we write numbers between 0 and 1 in any order. The argument says I can add a new number to the right by adding 1 to the nth decimal place of the nth number on the list. And it must be unique. Therefore the list wasn't complete and it proves that there are more of these than integers. I just don't agree with this logic. I'm wrong but I can't grasp why I am wrong and what I am missing. If $\infty + 1 = \infty$ (Hilbert) then why does $\infty + 1 = uncountable$ $\infty$ (Cantor)? If all we did in Cantor is prove that the list of decimals between 0 and 1 was incomplete, then didn't we prove that the list of integers was incomplete in Hilbert? Seems like the same argument? Can't the last guy that moved have taken a number mapping to Cantor's new found number? I can directly map a 1-1 set of all rational numbers in the set [0, 1) to integers. How? Take any integer and reverse the digits. Now add ""0."" to the start. So the integer 12345 becomes 0.54321 and the integer 100 becomes 0.001. The reason for the reversal is because 10 and 100 would map  to 0.10 and 0.100 which is the same thing if you didn't do this. Any decimal you give me I can remove the ""0."" from it, reverse it, and make it a new and unique integer. Doing this I can find Cantor's new number found by the diagonal modification. If Cantor's argument included irrational numbers from the start then the argument was never needed. The entire natural set of numbers could be represented as $\frac{\sqrt 2}{n}$ (except 1) and fit between [0,1) no problem. And that's only covering irrationals and only a small fraction of those. I understand that there are different levels of infinity, but the count of all natural numbers and the count of rationals between [0,1) seem to both be $\aleph_0$ to me.","['elementary-set-theory', 'infinity']"
4162562,Angle of mass hanging from two points connected at two points,I'm trying to calculate the angle (to the floor) a mass would hang if it were connected from either end to two points above it on the ceiling. Let's call the distance between the points on the ceiling D1 and the distance between the points on the mass D2. The length of the first line L1 and the second L2. For the purpose of this example let's assume the center of gravity of the mass is directly in the middle and the lines are rigid. My initial thought was to find the lowest place the center point of a line of size D2 could exist if its ends were coincident with circles of size L1 and L2 drawn from their respective positions on the ceiling (the ends of line D1). Then you could calculate the angle of that line relative to the ground if the positions of its endpoints were known. Not really sure how to approach this issue. Any advice would be greatly appreciated. Thanks!,"['classical-mechanics', 'angle', 'geometry', 'physics', 'trigonometry']"
4162581,"Two-generator, one-relator presentations of the free group","Suppose I have a finitely presented group $G$ with two generators $x$ and $y$ , with one relator $r$ . And suppose the relator is a cyclically reduced word (in $x$ and $y$ ), i.e., every cyclic permutation of the word is reduced. If $G$ turns out to be a free group, then must the relator have length at most two, in the sense that $r=x^a y^b$ (or $y^b x^a$ ) with $a,b \in \mathbb{Z}$ ?","['combinatorial-group-theory', 'group-presentation', 'group-theory']"
4162612,"Closures, intersections and sums of subspaces in Hilbert space","Let $X$ be a real Hilbert space, and let $U$ and $V$ be two closed linear subspaces of $X$ . Is it true that $$\overline{(U+V) \cap (U^\perp+V^\perp)} = \overline{U+V} \cap \overline{U^\perp + V^\perp}\quad?$$ The left-hand side is clearly a subset of the right-hand side, but the opposite inclusion stumps me. (The result is clearly true if $X$ is finite-dimensional because all subspaces are automatically closed.) I checked my trusted Functional Analysis book, math.stackexchange, as well as Halmos' A Hilbert Space Problem book but couldn't find anything. This should be known! Please provide a reference or thought. Thanks!","['hilbert-spaces', 'functional-analysis', 'reference-request']"
4162667,A question on exchangeable variables in Erdős-Rényi graphs,"Let $H$ be a subgraph of the complete graph on $n$ vertices without isolated vertices, and let $\Gamma$ be the set of isomorphic copies of $H$ . For any $\alpha\in \Gamma$ , let $X_\alpha$ be the indicatrix of the event $\alpha\in G(n,p)$ . I am told that $\{X_\alpha\}$ is a sequence of exchangeable random variables, but I do not see how to prove it (apart from the fact that is suffices to prove exchangeability for permutations of type $(a,b)$ ). Actually, I am not even sure this is true: it could be the case, for example, that $\{X_1=1,\dots, X_k=1\}$ implies $X_{k+1}=1$ , while it does not necessarily imply $X_n=1$ (for example, if $H$ is a triangle, it could be the case that the existence of a certain number of triangles in $G$ implies the existence of another one without implying the existence of all the remaining ones). Am I right, or am I missing something?","['graph-theory', 'random-graphs', 'probability-theory', 'probability']"
4162670,Understanding Rudin Theorem 3.3(c) [duplicate],"This question already has an answer here : Question about writing proofs for limit (1 answer) Closed 3 years ago . Rudin's Theorem 3.3 states: Suppose $\{s_n\}$ , $\{t_n\}$ are complex sequences, and $\lim\limits_{n \to \infty} s_n = s$ , $\lim\limits_{n \to \infty} t_n = t$ . Then (a) $\lim\limits_{n \to \infty} \left(s_n + t_n\right) = s + t$ ; (b) $\lim\limits_{n \to \infty} cs_n = cs$ , $\lim\limits_{n \to \infty} \left(c + s_n\right) = c + s$ , for any number $c$ ; (c) $\lim\limits_{n \to \infty} s_n t_n = st$ ; (d) $\lim\limits_{n \to \infty} \frac{1}{s_n} = \frac{1}{s}$ , provided $s_n \neq 0$ ( $n = 1, 2, 3, \ldots$ ), and $s \neq 0$ . I proved (a) and (b) without trouble. I'm trying to understand his proof of (c). He notes that for any $s$ we have: $$ 
s_n t_n - st = (s_n - s)(t_n - t) + s(t_n - t) + t(s_n - s).
$$ My first question is: where does this come from? By brute-forcing the right-side, I can tell that this amounts to adding and subtracting $st$ , $ts_n$ , and $st_n$ . But is there another reason why this holds? Rudin calls it an ""identity,' but I've never seen it before. Then, given $\epsilon > 0$ , he takes $N_1, N_2$ , so that $n \geq N_1$ implies $|s_n - s| < \sqrt{\epsilon}$ and $n \geq N_2$ implies $|t_n - t| < \sqrt{\epsilon}$ . So for $n \geq \max(N_1, N_2)$ , we have \begin{align*}
|(s_n - s)(t_n - t) - 0| = |s_n - s||t_n - t| < \sqrt{\epsilon} \cdot \sqrt{\epsilon} = \epsilon, 
\end{align*} so $\lim\limits_{n \to \infty} (s_n - s)(t_n - t) = 0$ . Rudin then claims that the remaining terms in the first identity converge to $0$ by using $(a)$ and $b$ . The first term converges, as we just proved, and the second and third are scaled versions of $t_n$ and $s_n$ , so they converge. (I'm not sure how to word this exactly, since I need to have convergence first to use this identity, but I'm using the identity to prove they converge, so this is a very circular argument. I'm trying to figure out how best to word it.) With that aside, we have: \begin{align*}
\lim\limits_{n \to \infty} [(s_n t_n - st) & = \lim\limits_{n \to \infty}  (s_n - s)(t_n - t) + s(t_n - t) + t(s_n - s)] \\
& = \lim\limits_{n \to \infty} (s_n - s)(t_n - t) + \lim\limits_{n \to \infty} s (t_n - t) + \lim\limits_{n \to \infty} t(s_n - s) \\
& = 0 + \lim\limits_{n \to \infty} (st_n - st) + \lim\limits_{n \to \infty} (ts_n - ts) \\
& = s \lim\limits_{n \to \infty}t_n - st + t \lim\limits_{n \to \infty} s_n - ts \\
& = st - st + ts - ts \\
& = 0
\end{align*} So $\lim\limits_{n \to \infty} (s_n t_n - st) = 0$ . I'm trying to figure out exactly how this implies the result, because I can't just invoke part (a)/b to break the limit apart, as that requires convergence of $s_n t_n$ , which is what I need. I think, however, it can be used to prove the fact directly. Given $\epsilon > 0$ , we use this limit to find $N$ so that for $n \geq N$ , we have $|s_n t_n - st| < \epsilon$ for all $n \geq N$ , so $(s_n t_n) \to st$ . As a general principle, am I allowed to just ""break it apart,"" or was that just a coincidence? I would appreciate any help and any feedback on what I've written above.","['sequences-and-series', 'real-analysis']"
4162677,Partitioning a metric space into Cantor sets,"A ""Cantor set"" is a topological space which is homeomorphic to the standard Cantor set $C$ . In my answer to the question Another way for partition of perfect set by user 00GB I pointed out that the assertion ""every (nonempty) perfect subset of $\mathbb R$ can be partitioned into $\mathfrak c$ Cantor sets"" is an easy consequence of Theorem 1.14 of Paul Bankston and Richard J. McGovern, Topological partitions, General Topology and its Applications 10 (1979), 215–229 ( pdf ), which says that $\mathbb R$ itself can be partitioned into Cantor sets. The proof of Theorem 1.14 consists of first partitioning $\mathbb R$ into countably many Cantor sets and one set which is isomorphic to the space $\mathbb P$ of irrational numbers, and then using the fact that $\mathbb P$ can be partitioned into $\mathfrak c$ Cantor sets. This is very nice, but does not seem to generalize in any obvious way to higher-dimensional spaces. Bankston & McGovern prove more general results (Theorem 1.12) about partitionability into Cantor sets, but only under special set-theoretic axioms; namely, they use Martin's axiom to prove that every complete separable metric space with no isolated points can be partitioned into Cantor sets, and they use the continuum hypothesis to prove the same for complete metric spaces of cardinality $\mathfrak c$ with no isolated points. Question. Can those more general results of Bankston & McGovern be proved in ZFC? Can you prove in ZFC that every complete separable metric space with no isolated points, or even every complete metric space of cardinality $\mathfrak c$ with no isolated points, can be partitioned into Cantor sets? By the way, since the Cantor set $C$ is homeomorphic to $C\times C$ , any Cantor set can be partitioned into $\mathfrak c$ Cantor sets. Consequently, if a nonempty separable metric space $X$ can be partitioned into Cantor sets, then $X$ can be partitioned into $\mathfrak c$ Cantor sets.","['general-topology', 'cantor-set', 'metric-spaces', 'set-theory']"
4162723,Prove that this set of points is finite or countable,"Let $X \subset \mathbb{R} $ be such that $|x-y| > 3 \ \  \forall x, y \in X, x \ne y$ . Prove that $X$ is a finite or countable set. Here, normally I would try to find a formula or any other reasonable way to build a bijection between the set and $\mathbb{N}$ . In this case, however, the approach when I tried to fix $y$ and then consider $x > y_0+3$ and $x < y_0 - 3$ and thus show that we can enumerate each set in the union with natural numbers resulted in a counter-intuitive conclusion. How do I build the bijection in this case?","['elementary-set-theory', 'real-numbers']"
4162748,Finding a function that satisfies $f(y)-f(x)=x/y-1$,"I want to find all (differentiable) functions $f:[0,1]\rightarrow[0,1]$ that satisfies $$f(y)-f(x)=\frac{x}{y}-1$$ My approach was taking $y=x+d$ , so that I can have $$f(x+d)-f(x)=\frac{x}{x+d}-1=-\frac{d}{x+d}.$$ If I devide both sides with $d$ and then take $d\rightarrow 0$ , I should have $$f'(x)=-\frac{1}{x}, $$ which implies $f(x)=ln ~x+C$ for some constant $C$ . However, if I plug in $x$ and $y$ back to the derived $f(x)$ , I cannot have $f(y)-f(x)=\frac{x}{y}-1$ .. Can anyone identify the mistake in my derivation or anyone knows how to derive such $f$ ?","['derivatives', 'ordinary-differential-equations']"
4162811,Continuous onto function preserves number of path-connected components,"Let $X = \{(x,y) \in \mathbb{R}^2 : y = 0\} \cup \{(x,y)\in \mathbb{R}^2: -x+y = 1\}$ . We want to prove that there is no continuous onto map $$
A:=X \cup \{(x,y) \in \mathbb{R}^2: x=0\} \overset{f}{\longrightarrow} B:=X \cup \{(x,y) \in \mathbb{R}^2 : y=1\}
$$ considering both $A$ and $B$ equipped with the Euclidean topology. Our approach to the problem has been to remove the point $p = (0,1)$ , which is the intersection of two lines, from the set of departure, and observe that this leaves $A \setminus \{p\}$ with three path-connected components. Then, regardless of where $f(p)$ lands in the codomain of the function, the number of connected components of $B \setminus \{f(p)\}$ is not preserved. Our main doubt is whether we can use this fact to prove that such a function $f$ does not exist. In other words, is the number of path-connected components preserved by a continuous and surjective function between topological spaces? EDIT:
As mathcounterexamples.net has suggested in his answer, we cannot use the reasoning that $f$ preserves the number of path-connected components of $A$ and $B$ . We are now wondering what other method of proof could we use to solve the problem. Could it be, maybe, something related to the fact that $A$ contains a closed path (triangle) and $B$ contains none? Thanks in advance for your help and answers.","['general-topology', 'connectedness']"
4162824,"Show that $W^{s_2, 2}(\mathbb{T}) \hookrightarrow W^{s_1,2} (\mathbb{T})$ is a compact operator","How can one prove that for $0 \le s_1 < s_2$ , the embedding map $W^{s_2, 2}(\mathbb{T}) \hookrightarrow W^{s_1,2} (\mathbb{T})$ is a compact operator? My proof Let $f \in W^{s_2,2}(\mathbb{T})$ , then $f \in L^2(\mathbb{T})$ and $\sum_{n \in \mathbb{Z}} (1+ n^2)^{s_2} |\widehat{f}(n)| < \infty$ .
Since $s_1 < s_2$ , then we have $$ \sum_{n \in \mathbb{Z}} (1+ n^2)^{s_1} |\widehat{f}(n)| \le \sum_{n \in \mathbb{Z}} (1+ n^2)^{s_2} |\widehat{f}(n)| < \infty.$$ Thus, $f \in W^{s_1,2}(\mathbb{T})$ . Consider the orthonormal basis $\{e_n \}_{n \in \mathbb{Z}}$ for $L^2(\mathbb{T})$ . Let $f \in W^{s_1,2}(\mathbb{T})$ . Since $f = \sum_{j \in \mathbb{Z}} \langle f, e_j \rangle e_j$ , $\sum_{n \in \mathbb{Z}} (1+n^2)^{s_1} \left| \widehat{f}(n) \right|^2<\infty$ , and $\widehat{f}(n) = \langle f, e_n \rangle$ . Then we have $\sum_{n \in \mathbb{Z}} (1+n^2)^{s_1} \left| \widehat{f}(n) 
\right|^2  = \sum_{n \in \mathbb{Z}} (1+n^2)^{s_1} \langle f, e_n \rangle^2<\infty.$ Given $\epsilon >0$ , $\exists N >0$ such that $ \left| \sum_{|n| > N+1 } (1 + n^2)^{s_1} \langle f,e_n \rangle \right| < \epsilon$ . Since $(1+n^2)^{s_1}$ is never zero, then $\underset{n \to \infty}{\lim}\langle f,e_n \rangle =0, \forall |n| > N+1$ . Then for every $f \in W^{s_1,2}(\mathbb{T})$ , $f$ can be expressed as a finite dimensional orthonormal bases $\{ e_n\}_{|n| \le N}$ . Namely $f = \sum_{|n| \le \mathbb{Z}} \langle f, e_n \rangle e_n.$ Thus the embedding operator is compact. Added 2- If $m \in \mathbb{N}_0$ and $s  > m + \frac{1}{2}$ , then $W^{s,2}(T) \subset C^n (T) \subset W^{n,2}(T)$ . Where $T=\mathbb{R}/2 \pi \mathbb{Z}$ . Is my proof to (1) is ok?  could help me with the point (2). Edited I am thinking of using this theorem: Let $T \in B(H)$ be a diagonal operator, i.e; $\exists  \{ e_i \}_{i \in I}$ orthonormal basis of $H$ such that $Te_i = \lambda_i e_i$ , $\lambda_i \in \mathbb{C}$ . Then the following conditions are equivalent: 1- $T \in K(H)$ 2- $\lim_{i \to \infty} \lambda_i =0$ , i.e. $\forall \epsilon >0, 
\exists F \subset I$ finite subset such that $\forall i \in I \subset F$ , $|\lambda_i|<\epsilon$","['sobolev-spaces', 'fourier-analysis', 'functional-analysis', 'compact-operators']"
4162829,Changing the order of integration of these two double integrals.,"$\int^2_1dx\int^{\sqrt{2x-x^2}}_{2-x}f(x,y)dy$ $\int^e_1dx\int^{ln(x)}_{0}f(x,y)dy$ My attempts: For the first Integral: I can see that $y=\sqrt{2x-x^2}$ defines half of a circle with radius $1$ . And $2-x$ is basically a line, I drew them, and I can see that if I wanted to change the order of integration, $y$ will be between $0$ and $1$ , and now my problem is $x$ , I need to get what is the upper $x$ from this equation $y=\sqrt{2x-x^2} \Longrightarrow y^2=2x-x^2$ , but how do I keep going? I'm not succeeding to get $x$ from that equation. For the second integral : I can see the area between $y=ln(x)$ , $x=e$ , $x=1$ . In order to change the order of integration, I must see the borders of $y$ , and that's the intersection between $y=ln(x)$ and $x=e$ , which is $y=1$ . So the integral will be: $\int^1_0dy\int^{e}_{e^y}f(x,y)dx$ Any help and feedback is really appreciated, thanks in advance!","['integration', 'multivariable-calculus']"
4162853,Arc length between spaced points on circle when viewed under an inclination,"Short version: Say you have a circle of radius $r$ with a set of $n$ equally spaced points on it, each with an arc length of $\Delta\theta=\frac{2\pi}{n}$ between each other. When that circle is viewed under an inclination $i$ , the phase $\theta$ of any point w.r.t. the origin changes. How does that projected arc length $\Delta\theta$ change when the circle is viewed under an inclination $i$ EDIT2 in terms of the initial phase $theta$ of the unit circle? Long version: I am an astronomer and am making a model of a ring around a star (let's ignore the astrophysics for this question). That ring has a number of $n$ ""parcels of gas"" on it, each with an arc length of $\Delta\theta=\frac{2\pi}{n}$ EDIT2 at a phase $\theta$ . When that ring is viewed under an inclination $i$ ( $0$ deg=face-on, $90$ deg=edge-on), that angle $\theta^{circ}->\theta^{ellip}$ changes of course. How does the observed arc length, call it $\Delta\theta'$ , of the parcels of gas change with that inclination $i$ in terms of the $\textbf{initial phase $\theta$}$ ? Of course this is dependent on the azimuth/phase $\theta$ of each parcel, where any $\theta$ is defined as $0$ at either equinox/along the semi major axis of the ellipse. For the parcels at the peri/apastron, that is for $\theta^{ellip}=\pi\pm\frac{\pi}{2}$ , the change should be negligible, even for a high $i$ . For those at the equinoxes, i.e. for $\theta^{ellip}=\{0,\pi\}$ , the observed arc length should become smaller and smaller for an increasing $i$ . My colleague has derived a rather simple solution which I do not think is totally correct. Hence, I would love to know your take on this matter. EDIT: My colleague's solution was simply: $\Delta\theta' = \rho\cdot\Delta\theta$ , where $\rho$ is the distance from the origin to the position of the gas parcel in the ellipse - however, that is confusing the azimuth on the circle $\theta$ with the eccentric anomaly in the ellipse. Hence, that cannot be the right answer. EDIT2: The problem is that I need the solution in terms of the point's initial phase on the circle.
Hence the big question: I have a point on a circle that is being projected as an ellipse under an inclination. How would I convert the phase of the point on the circle to a phase in the ellipse? That phase is not conserved.","['conic-sections', 'projection', 'trigonometry', 'circles']"
4162869,Subset as arithmetic mean of geometric means. Not really?,"Let $\,M \le N\,$ and $X$ be a finite set of $N$ reals $x_i\,$ : $X = \{x_1,x_2, .. ,x_i, .. ,x_N\}\,$ , $Y$ be a finite set of $M$ reals $y_j\,$ : $Y = \{y_1,y_2, .. ,y_j, .. ,y_M\}\,$ .
In order to determine whether $X$ is a subset of $Y$ , we could compare the
elements: $X$ is a subset of $Y$ , $\,X \subseteq Y\,$ , if and only if for each of
the $\,x_i\,$ one of them is equal to $\,y_j\,$ : $$
\quad ((x_1 = y_1) \vee (x_1 = y_2) \vee \cdots \vee (x_1 = y_j) \vee \cdots \vee (x_1 = y_M)) \\
\wedge ((x_2 = y_1) \vee (x_2 = y_2) \vee \cdots \vee (x_2 = y_j) \vee \cdots \vee (x_2 = y_M)) \\
\wedge \cdots \cdots \wedge \quad \\
\quad ((x_i = y_1) \vee (x_i = y_2) \vee \cdots \vee (x_i = y_j) \vee \cdots \vee (x_i = y_M)) \\
\wedge \cdots \cdots \wedge \quad \\
\quad ((x_N = y_1) \vee (x_N = y_2) \vee \cdots \vee (x_N = y_j) \vee \cdots \vee (x_N = y_M))
$$ $$
(X \subseteq Y) \Longleftrightarrow
\bigwedge_{i=1}^N \left[ \bigvee_{j=1}^M \left(x_i=y_j\right) \right]
$$ Because all $\,x_i\,$ and $\,y_j\,$ are real numbers, we can alsow write this
as follows; a product of terms is zero, if and only if one (or more)
of the factors is zero. $$
(X \subseteq Y) \Longleftrightarrow
\bigwedge_{i=1}^N \left[ \prod_{j=1}^M \left(x_i-y_j\right) = 0\right]
$$ And each of the terms is zero if and only if the sum of the squares of
all of these terms is zero: $$
(X \subseteq Y) \Longleftrightarrow
\sum_{i=1}^N \left[ \prod_{j=1}^M \left(x_i-y_j\right) \right]^2 = 0
$$ One can make this somewhat more computation friendly by implementing
products as geometric means : take the $M$ -th root of each of the terms.
For the same reason, we shall implement the sum as an arithmetic mean .
Furthermore the above can easily be generalized to vectors $(\vec{x}_i,\vec{y}_j)$ .
The we have at last, for the discrete case: $$
(X \subseteq Y) \Longleftrightarrow
\sum_{i=1}^N \frac{1}{N} \left[ \prod_{j=1}^M \left|\vec{x}_i-\vec{y}_j\right|^2 \right]^{1/M} = 0
$$ In Wikipedia a definition is found of the Geometric Mean .
And there is a relationship with the arithmetic mean of logarithms at that page: $$
{\displaystyle \left(\prod _{i=1}^{n}a_{i}\right)^{\frac {1}{n}}=\exp \left[{\frac {1}{n}}\sum _{i=1}^{n}\ln a_{i}\right];} \quad a_i > 0
$$ Together with the above then we have: $$
(X \subseteq Y) \Longleftrightarrow
\sum_{i=1}^N \frac{1}{N} \exp\left[\frac{1}{M}\sum_{j=1}^M \ln(\left|\vec{x}_i-\vec{y}_j\right|^2)\right] = 0
$$ The following essential reading is needed, in order to be able to proceed. Product integral Namely for converting the discrete into the continuous: $$
(X \subseteq Y) \Longleftrightarrow
\int_0^1 \exp\left(\int_0^1 \ln(\left|\vec{x}(t)-\vec{y}(u)\right|^2)\,dt\right)\,du = 0
$$ where a minor detail is restriction to the integration interval $[0,1]$ by parameter transformation. So far so good. Let's try now the simplest example possible, namely $\;\vec{x}(t)=t \; ; \; 0 \le t\le 1\;$ and $\;\vec{y}(u)=u \; ; \; 0 \le u\le 1\;$ . Then it is clear that $(X=Y$ and so $(X \subseteq Y)$ and $(Y \subseteq X)$ .
And our calculation effort is defined by proving that: $$
\int_0^1 \exp\left(\int_0^1 \ln\left(|t-u\right|^2)\,dt\right)\,du 
= \int_0^1 \exp\left(\int_0^1 \ln(\left|t-u\right|^2)\,du\right)\,dt = 0 \\
= \exp(-2)\int_0^1 u^{2u}(1-u)^{2(1-u)}\,du \approx 0.05378539284
$$ This is not by far the accuracy we did expect!
I think it has something to do with evaluating the logarithm for $t=u$ but I'm not sure. What precisely is going on?","['integration', 'elementary-set-theory', 'infinite-product', 'real-analysis']"
4162877,Condition for inverse function.,"I found this line about the condition of inverse function at Inverse function .Here is the line. In the verification step we technically really do need to check that both $\left( {{f} \circ f^{-1}} \right)\left( x \right) = x$ and $\left( {{f^{ - 1}} \circ f} \right)\left( x \right) = x$ are true. For all the functions that we are going to be looking at in this course if one is true then the other will also be true. However, there are functions (they are beyond the scope of this course however) for which it is possible for only one of these to be true. This is brought up because in all the problems here we will be just checking one of them. We just need to always remember that technically we should check both. Can someone give me an example of such a function?","['inverse-function', 'functions', 'discrete-mathematics', 'elementary-set-theory', 'algebra-precalculus']"
4162885,"If $\theta_t=\frac{1}{r}\frac{\partial}{\partial{r}}(r\theta_r)$, show that $\int_0^{\infty} \theta(r,t)rdr=\int_0^{\infty} \theta(r,0)rdr$?","We have $r>0, t>0$ and we're given the conditions: $$r\theta_r \to 0  \text{ as } r\to \infty$$ and $$\theta(r,t)\leq K\in \mathbb{R}\text{ as }r\to 0$$ I tried taking integrals with respect to $r$ over the PDE. Rearranging the PDE and applying the integral: $\int_0^{\infty}\big(r\theta_t-\frac{\partial}{\partial{r}}(r\theta_r)\big)dr=\int_0^{\infty}r\theta_tdr+\int_0^{\infty}\frac{\partial}{\partial{r}}(r\theta_r)dr=\int_0^{\infty}r\theta_tdr+r\theta_r |_{0}^{\infty}=\int_0^{\infty}r\theta_t(r,t)dr=0 $ Is this correct? From here I can't see how to proceed to get the result in the title?","['integration', 'multivariable-calculus', 'calculus', 'partial-differential-equations', 'partial-derivative']"
4162914,Equivalent definitions of Fréchet differentiability,"In their book The Ricci Flow in Riemannian Geometry Andrews and Hopper have an appendix on Gâteaux and Fréchet differentiability. They define Gâteaux differentiability as follows: Let $X,Y$ be Banach spaces. A function $f: X \rightarrow Y$ is said to be Gâteaux differentiable at $x$ if there exists a bounded linear operator $T_x \in L(X,Y)$ such that for all $v \in X$ , $$\lim_{t \to 0} \frac{f(x+tv)-f(x)}{t} = T_x v.$$ The operator $T_x$ is called the Gâteaux derivative . This is the definition I was presented with in class.
Fréchet differentiability, however, is defined differently than in class (In class we used the second definition mentioned in the following passage): If the limit (in the sense of the Gâteaux derivative) exists uniformly in $v$ on the unit sphere of X, we say $f$ is Fréchet differentiable at $x$ and $T_x$ is the Fréchet derivative of $f$ at $x$ . Equivalently, if we set $y=tv$ then $t \rightarrow 0$ if and only if $y \rightarrow 0$ . Thus $f$ is Fréchet differentiable at $x$ if  for all $y$ , $$f(x+y)-f(x)-T_x(y)=o(\|y\|).$$ Questions What does ""the limit exists uniformly in $v$ on the unit sphere of X"" mean exactly? I know what uniform convergence is in regards to a sequence of functions. However, I cannot figure out how this concept relates to the one presented here, i.e. what sequence of functions is considered here? How are the two presented definitions of Fréchet differentiability equivalent?","['frechet-derivative', 'definition', 'derivatives', 'functional-analysis']"
4162919,Limit of Function raised to a power of another function,"I'm trying to evaluate the following limit: $$A =\lim_{x\to\infty}\left(\frac{x^2-3x+1}{x^2+x+2}\right)^{2x-5}$$ So far, I have exponentiated the limit and whatnot and now I am at this stage: $$A =\exp\left(\lim_{x\to\infty}(2x-5)\ln\frac{x^2-3x+1}{x^2+x+2}\right)$$ Now, I don't know what to do here, I know via simple algebra that: $$\lim_{x\to\infty}\ln\frac{x^2-3x+1}{x^2+x+2}=0$$ But $2x-5$ has no limit, and thus I cannot separate $A$ into the product of two limits. Perhaps I am missing something? WolframAlpha says $$\lim_{x\to\infty}(2x-5)\ln\frac{x^2-3x+1}{x^2+x+2}=-8$$ And that therefore $A = e^{-8}$ , but gives no insight as to how this is the case.","['limits', 'calculus']"
4162951,"Evaluation of Integral $\int_0^{+\infty}\frac{\sin^2(\tan\,\!x)}{x^2}\mathrm{d}x$","Evaluation of $$\int_0^{+\infty}\dfrac{\sin^2(\tan\,\!x)}{x^2}\mathrm{d}x$$ Evaluate $\int_0^{\infty} {\sin(\tan(x)) \over x}dx$ I try to give the result through the method in the link, but I don't know what to do next. \begin{align*}
I&\overset{\mathrm{def}}{=} \int_0^{+\infty}\frac{\sin^2(\tan\,\!x)}{x^2}\,\mathrm{d}x\\
&= \frac12 \int_{-\infty}^{+\infty}\frac{\sin^2(\tan\,\!x)}{x^2}\,\mathrm{d}x\\
&=\frac12 \left(\sum_{n=-\infty}^\infty \int_{(n-\frac12)\pi}^{(n+\frac12)\pi}\right)\frac{\sin^2(\tan\,\!x)}{x^2}\,\mathrm{d}x
\end{align*}","['integration', 'improper-integrals', 'calculus', 'definite-integrals']"
4162966,Is square of a VMO function in VMO,"We say a function $f\in L^1_{loc}(\mathbb{R})$ is in $\mathrm{BMO}(\mathbb{R})$ if $$\|f\|_{\mathrm{BMO}}=\sup_{I}\frac{1}{|I|}\int\limits_I |f(y)-f_I|\, dy<\infty$$ for all intervals $I\subset\mathbb{R},$ where $f_I$ is the average value of $f$ : $$f_I=\frac{1}{|I|}\int_I f(y)\, dy.$$ $f\in \mathrm{VMO}(\mathbb{R})$ if $f \in\mathrm{BMO}$ and $$\lim_{|I|\rightarrow 0}\frac{1}{|I|}\int_I|f(y)-f_I|\, dy\rightarrow 0. $$ For more details, see Bounded mean oscillation . Some facts that maybe useful UC $\cap$ BMO $\subset$ VMO, where UC means uniformly continuous functions in $\mathbb{R}$ . VMO is the closure of UC $\cap$ BMO in BMO. My question is 1.If $f\in $ VMO, is $f^2\in$ VMO? (If yes, next question is trivial; if not, please see next question) 2.If $\omega\in A_\infty$ , A_p weight and $\log \omega\in$ VMO, is $(\log \omega)^2 \in$ VMO? My try: Divide $f=f_1+f_2$ , where $f_1\in \mathrm{UC}\cap \mathrm{BMO},\ \|f_2\|_{BMO}<\epsilon$ . But I can't go on next. I am not sure it is correct, if not, could you give me a counterexample? Thanks very much.","['harmonic-analysis', 'analysis', 'real-analysis']"
4163003,"Is $g(x)= \frac{x_1^2}{r_1^2}+\frac{x_2^2}{r_2^2}- c$ a unique solution of $E[g(Z)\mid M=\mu]=0, \forall \mu \in \text{ellipse } C$ for $Z$ Gaussian","Let $Z \in \mathbb{R}^2$ be an i.i.d. Gaussian vector with mean $M$ where $P_{Z\mid M}$ is its distribution. Let $g: \mathbb{R}^2 \to \mathbb{R}$ and consider the following equation: $$
E[g(Z)\mid M=\mu]=0,  \forall \mu \in C,
$$ where $C=\{\mu: \frac{\mu_1^2}{r_1^2}+\frac{\mu_2^2}{r_2^2}=1 \}$ for some given $r_1,r_2 > 0$ . That is, $C$ is an ellipse. It is not difficult to verify (see this question , see also Edit 3) that a solution to this equation is given by $$
g(x)= \frac{x_1^2}{r_1^2}+\frac{x_2^2}{r_2^2}- c,
$$ where $c=\frac{1}{r_1^2}+\frac{1}{r_2^2}+1$ .
In fact any function $g_a(x)= a g(x)$ for any $a \in \mathbb{R}$ is a solution. Question: Is $g$ a unique solution up to a multiplicative constant? Edit: If we need to make an assumption on  the class of allowed functions $g$ . Let us assume that $g$ 's are bounded by a quadratic monomial (i.e., for every $g$ there exists $a$ and $b$ such that $g(x) \le a \|x \|^2 +b$ ). Edit 2: If you want to avoid expectation notation. Everything can be alternatively written as $$
\iint g(z)\frac{1}{2 \pi} e^{-\frac{\|z-m\|^2}{2}} \, {\rm d} z=0, \, m\in C.
$$ From here, one can see that this question is about a convolution. Edit 3: To see that $g(x)$ is a solution we use that the second moment of Gaussian is given by $E[Z_i^2\mid M_i=\mu_i]=1+\mu_i^2$ , which leads to \begin{align}
E\left[\frac{Z_1^2}{r_1^2}+\frac{Z_2^2}{r_2^2}- c\mid M=\mu \right]&=  \frac{E[Z_1^2\mid M_1=\mu_1]}{r_1^2}+\frac{  E[Z_2^2\mid M_2=\mu_2]}{r_2^2}-c\\[6pt]
&=\frac{1+\mu_1^2}{r_1^2}+\frac{  1+\mu_1^2}{r_2^2}-c\\[6pt]
&=\frac{1}{r_1^2}+\frac 1 {r_2^2}+1-c,
\end{align} where in the last step we used that $\mu$ is on the ellipse. Edit 4: The comment below shows that the solution is not unique when an ellipse is a circle.  However, I would still like to know the answer for a general ellipse.","['real-analysis', 'expected-value', 'functional-analysis', 'gaussian-integral', 'probability-theory']"
4163102,Does the derivative of a polynomial over an ordered ring behave like a rate of change?,"Suppose I have an ordered commutative ring $R$ . I can define the collection $P$ of polynomial functions defined on $R$ as functions of the form $f(r) = p_0 + p_1 r + p_2 r^2 + \cdots$ where $p_0, p_1, p_2, \cdots$ are elements of $R$ . The collection $P$ is closed under the formal derivative operation that maps $f(r) = p_0 + p_1 r + p_2 r^2 + \cdots$ to $f'(r) = p_1 + (p_2 + p_2) r + \cdots$ Does this derivative satisfy any ""rate of change"" properties that play nicely with the ordering on $R$ ? For example, consider the elements $r_1 \leq r_2$ in $R$ and suppose that for any $r \in R$ where $r_1 \leq r \leq r_2$ we have that $f'(r) \geq 0$ . Does this imply that $f(r_2) - f(r_1) \geq 0$ ? If not, what additional assumptions do I need? Obviously this works when $R$ is the reals, but I am curious what assumptions I can drop.","['ordered-rings', 'ring-theory', 'derivatives', 'polynomials']"
4163143,Is the integral operator bounded?,"I have the following problem. Suppose $\mu$ - measure on $T$ , $K(s,t) \geq 0$ and measurable function on $T \times T$ with respect to $\mu \times \mu$ . U is the integral operator: $$(Uf)(s) = \int\limits_T K(s,t)f(t) dt.$$ I know that $Uf \in L^2(\mu)$ if $f \in L^\infty(\mu)$ and the following estimate: $$\int\limits_T K(s,t) K(s,x) ds \leq C (K(t,x) + K(x,t)),$$ where $C>0$ . I want to prove that $U$ is bounded operator from $L^2(\mu)$ to itself. I observe that \begin{align}
(Uf, Uf) &= \int\limits_T \left( \int\limits_T K(s, t) f(t) dt \int\limits_T K(s, \tau) f(\tau) d\tau\right) ds\\
& = \int\limits_T \int\limits_T \int\limits_T K(s, t) K(s, \tau)ds f(\tau) f(t) dt d\tau \\
& \leq \int\limits_T \int\limits_T f(\tau) f(t) C(K(t, \tau) + K(\tau, t)) dt d\tau \\
&\leq 2C(Uf, f)
\end{align} for every $f \geq 0$ and $(\cdot,\cdot)$ means inner product in $L^2(\mu)$ .
But I don't is it useful.","['operator-theory', 'functional-analysis']"
4163214,Simple proof of Krein-Smulian,"I have a question concerning a simple and short proof of Krein-Shmulian, the proof can be found here https://people.math.ethz.ch/~jteichma/slides_ftap.pdf on page 35/36. Here the proof: Let $X$ be a Banach space. The Krein-Smulian theorem tells that a convex subset $C\subset X^∗$ is weak-∗-closed if and only if its intersections with balls in $X^∗$ are weak-∗-closed. We can conclude this theorem from a separation theorem (see Conways’ book on functional analysis): assume that for a convex set $C \subset X^*$ all its intersections with balls in $X^*$ are weak-∗-closed, and assume that the intersection of $C$ with the unit ball (centered at $0$ ) is empty, then there is $x \in X$ such that $(x,x∗)\geq 1$ for all $x^* \in C$ . From this we can conclude immediately:  let $x^* \in X^*$ be in the weak-∗=closure of $C$ but not in $C$ , then – due to the fact that $C$ is norm closed (prove it!)  – there is a ball of radius $r$ around $x^∗$ which does not intersect $C$ .  Whence $r^{−1}(C−x^*)$ does not intersect the unit ball centered at $0$ . By the previous separation statement this however means that $x^*$ cannot lie in the weak-∗-closure of $C$ . While the statements on the first page are clear to me, I have some problems with page 36: First, it is a well known fact that if $S \subset X$ and convex ( $X$ being a vector space carrying a norm) is a weak closed convex set if and only if $S$ is also strongly closed. But how one get's that that if $C$ is a convex set where its intersection with balls in $X^*$ are weak*-closed, then $C$ is norm closed ? And further, having this fact, why we can find a ball of radius $r$ around $x^*$ (where $x^*$ is in the weak*-closure but not in $C$ ) which does not intersect $C$ ? And why this finally implies the statement ?",['functional-analysis']
4163215,Proof Verification: $\lim_{p\to 0} \|f\|_p = \exp\left\{\int_X \log |f|\ d\mu \right\}$ (Big Rudin's Ex $3.5$),"This appears as Ex. $3.5(d)$ in Rudin's Real and Complex Analysis . Suppose $f:X\to\mathbb C$ is a measurable function on $X$ , $\mu$ is a positive measure on $X$ , and $\mu(X) = 1$ . Assume that $\|f\|_r < \infty$ for some $r >0$ , and prove that $$\lim_{p\to 0} \|f\|_p = \exp\left\{\int_X \log |f|\ d\mu \right\}$$ if $\exp\{-\infty\}$ is defined to be $0$ . I saw a related proof here , and I write this post to confirm the correctness of my proof which fills in all details and removes apparently unnecessary assumptions from the linked answer. My work: For $0 < p < \infty$ , we have $$\tag{1}\int_X \log|f|\ d\mu \le \log\|f\|_p$$ using Jensen's inequality (and that $\log$ is a concave function). From Q5(a) of Rudin, we have $$\tag{2} 0 < r < s \le \infty \implies \|f\|_r \le \|f\|_s$$ From the graph of $x \mapsto \frac{a^x - 1}{x}$ , $\log a \le \frac{a^x - 1}{x}$ for $x > 0$ . Put $a = \left(\int_X |f|^{1/n} \ d\mu\right)^{n}$ to get $$\tag{3} \log \|f\|_{1/n} \le \int_X \frac{|f|^{1/n} - 1}{1/n}\ d\mu$$ Let $A = \{x: |f(x)| \ge 1\}$ and $B = \{x: |f(x)| < 1\}$ . $$\int_X \frac{|f|^{1/n} - 1}{1/n}\ d\mu = \int_A \frac{|f|^{1/n} - 1}{1/n}\ d\mu + \int_B \frac{|f|^{1/n} - 1}{1/n}\ d\mu$$ Recall that $\|f\|_r < \infty$ for some $r > 0$ . For $a >0$ , $x \mapsto \frac{a^x - 1}{x}$ is an  increasing function in $x$ . $\frac{|f|^{1/n} - 1}{1/n}$ is dominated by $\frac{|f|^{r} - 1}{r}$ for large enough $N$ , where the latter is integrable. On $B$ , $\frac{|f|^{1/n} - 1}{1/n} < 0$ , so $\frac{1 - |f|^{1/n}}{1/n} > 0$ . In fact, on $B$ , as $n\to\infty$ , $\frac{1 - |f|^{1/n}}{1/n}\nearrow -\log|f|$ pointwise. We can apply MCT on $B$ , and the DCT on $A$ , to get $$\lim_{n\to\infty}\int_X \frac{|f|^{1/n} - 1}{1/n}\ d\mu = \lim_{n\to\infty} \int_A \frac{|f|^{1/n} - 1}{1/n}\ d\mu - \lim_{n\to\infty}\int_B \frac{1 - |f|^{1/n}}{1/n}\ d\mu\\
= \int_A \log|f|\ d\mu + \int_B \log|f|\ d\mu= \int_X \log|f|\ d\mu$$ Due to inequality (2), and inequality (1), $\lim_{p\to 0} \log \|f\|_p$ exists. In inequality (3), take limits as $n\to\infty$ to get $$\lim_{n\to\infty}\log\|f\|_{1/n} \le \int_X \log|f|\ d\mu$$ $$\lim_{p\to 0}\log\|f\|_{p} \le \int_X \log|f|\ d\mu \tag{4}$$ Taking $p\to 0$ in inequality (1), we have $$\lim_{p\to 0}\log\|f\|_{p} \ge \int_X \log|f|\ d\mu \tag{5}$$ $$\implies \lim_{p\to 0}\log\|f\|_{p} = \int_X \log|f|\ d\mu \tag{6}$$ Since $\log$ is continuous, $$\lim_{p\to 0} \|f\|_p = \exp\left\{\int_X \log |f|\ d\mu \right\}$$ Thank you! Please help fill in the gaps if any, and in particular, check if the MCT/DCT part is good.","['measure-theory', 'solution-verification', 'real-analysis']"
4163218,What is the probability the sample mean is more than 10 words?,"The distribution of the number of words in text messages between employees at a large company is skewed right with a mean of $8.6$ words and a standard deviation of $4.3$ words. If a random sample of $39$ messages is selected, what is the probability the sample mean is more than $10$ words? Firstly, I used the z-score formula, substituting $8.6$ as the population mean, $10$ as the observed value, and $4.3$ as the standard deviation. The equation is set up as the following: $(10-8.6)/4.3$ . Secondly, the equation calculates to approx. $0.3255$ and its probability, using a z-table, is approx. $0.6276$ . I subtracted this value from $1$ because the prompt asks to find the probability of ""more than 10 words"" not less than. The final value then is approx. $0.3724$ . My answer is $0.3724$ as the probability of the sample mean being more than $10$ words. However, I believe there is an extra step using the sample size. If this is correct, how do I use the sample size to find the correct answer? Do I, instead of substituting $4.3$ as the standard deviation in the z-score formula, substitute $4.3\sqrt39$ , which, after going through the listed steps, have the probability as approx. $0.0210$ ?","['statistics', 'probability-distributions', 'probability']"
4163255,$A$ is compact if and only if $e^{itA}-I$ is compact for all $t$,"Suppose $\mathcal{H}$ is a Hilbert space, $A\in \mathcal{L}(\mathcal{H})$ is self adjoint, and define $U(t)=e^{itA}$ . Show that $U(t)-I$ is compact for all $t\in \mathbb{R}$ if and only if $A$ is compact. One direction is relatively straightforward: if $A$ is compact, write $$
U(t)-I=A\big(itI+\frac{(it)^2}{2!}A+\frac{(it)^3}{3!}A^2+\dots\big),
$$ from which it is apparent that $U(t)-I$ is itself compact. However, I am having trouble with the other direction. One notes that as $A$ is self adjoint, $U(t)$ is unitary, but I am not sure where to head from here. I have also attempted to apply the spectral theorem, but to no avail. A hint would be appreciated!","['compact-operators', 'functional-analysis']"
4163267,Result of replacing $1$ to $n$ in pairs by the sum?,"I have the following problem: Alice writes the numbers $1, 2, 3, 4, 5, 6, \ldots, n$ on a blackboard.  Bob selects two of these numbers,  erases both of them,  and writes down their sum on the blackboard.  For example, if Bob chose the numbers 3 and 4, the blackboard would contain the numbers $1, 2, 5, 6, 7, 7, 8, \ldots, n\quad$ ( $3$ and $4$ are removed from the list and $7$ is appended to the list)
Bob continues until there is only one number left on the board.  What are the possible values of that number in terms of $n$ ? I have tried using brute force on this problem with smaller values of $n$ , and I seem to obtain that the number left on the blackboard is $\dfrac{n(n + 1)}{2}$ . However, I cannot rigorously prove this, and I do not know if there are other possible values for the number. I would appreciate your help with this problem. Thank you!","['contest-math', 'invariance', 'algebra-precalculus', 'arithmetic']"
4163278,Understanding Rudin Theorem 3.3 (d),"For reference, here is the full Theorem 3.3 (d) from Rudin. Suppose $\{s_n\}$ , $\{t_n\}$ are complex sequences, and $\lim\limits_{n \to \infty} s_n = s$ , $\lim\limits_{n \to \infty} t_n = t$ . Then (a) $\lim\limits_{n \to \infty} \left(s_n + t_n\right) = s + t$ ; (b) $\lim\limits_{n \to \infty} cs_n = cs$ , $\lim\limits_{n \to \infty} \left(c + s_n\right) = c + s$ , for any number $c$ ; (c) $\lim\limits_{n \to \infty} s_n t_n = st$ ; (d) $\lim\limits_{n \to \infty} \frac{1}{s_n} = \frac{1}{s}$ , provided $s_n \neq 0$ ( $n = 1, 2, 3, \ldots$ ), and $s \neq 0$ . There's one key step in the proof of (d) that I don't understand. First, Rudin uses convergence of $s_n$ to find an $m \in \mathbb{N}$ so that for all $n \geq m$ , we have $|s_n - s| < \frac{1}{2} |s|$ . He then asserts that for $n \geq m$ , we have $$ 
|s_n| > \frac{1}{2} |s|.
$$ This is a very important step, but I cannot follow it. I've tried contradiction and the triangle inequality, but I can't get the inequality signs to line up. For example, I tried (for $n \geq m$ ), $$ 
|s_n| = |(s_n - s) + s| \leq |s_n - s| + |s| < \frac{1}{2} |s| + |s| = \frac{3}{2} |s|.
$$ It seems as though I've bounded $|s_n|$ ""in the opposite direction."" If I knew $s$ were positive, expanding the absolute values might work, but we only know it's non-zero. The rest of the proof looks pretty straightforward to me, with one slight doubt. He asserts the existence of an $N$ (I don't know why he requires $N > m$ when he could just take the maximum of $N$ and $m$ ; does this make a difference?) so that $n \geq N$ implies $|s_n - s| < \frac{1}{2} |s|^2 \epsilon$ . As $|s_n| > \frac{1}{2} |s|$ , we have $\frac{1}{|s_n|} < \frac{2}{|s|}$ . We then have: \begin{align*}
\left \lvert \frac{1}{s_n} - \frac{1}{s} \right \rvert & = \left \lvert \frac{s - s_n}{s_n \cdot s} \right \rvert \\
& = \frac{|s - s_n|}{|s||s_n|} \\
& < \frac{2|s - s_n|}{|s|^2} \\
& < \frac{2}{|s|^2} \cdot \frac{1}{2} |s|^2 \epsilon \\
& = \epsilon
\end{align*} I would appreciate some feedback on the above and some help with those two questions (how Rudin deduces $|s_n| > \frac{1}{2} |s|$ and why he takes $N > m$ instead of $\max(N,m)$ .)","['limits', 'proof-explanation', 'sequences-and-series', 'real-analysis']"
4163322,"How many integers $q(m,n)=\sum_m^np_k=p_m\cdot p_n$?","Let $p_n$ denote the $n^{th}$ prime number; $p_1=2,p_2=3,p_3=5$ , etc. Question: Are the infinitely many pairs of integers $(m,n)$ for which $m\le n$ and $$
\sum_{k=m}^n p_k=p_m\cdot p_n?
$$ For example, $(1,3)$ , $(2,6)$ , $(3,11)$ and $(4,16)$ are all valid pairs, since $$10=\sum_{k=1}^3p_k=p_1\cdot p_3=2\cdot5$$ $$39=\sum_{k=2}^6p_k=p_2\cdot p_6=3\cdot13$$ $$155=\sum_{k=3}^{11}p_k=p_3\cdot p_{11}=5\cdot31$$ $$371=\sum_{k=4}^{16}p_k=p_4\cdot p_{16}=7\cdot53$$ Up to $m=1000$ no other valid pair exists. Many thanks. Edit: I used the following Python code: from sympy import prime, nextprime
m = int(input('n: '))
for i in range(1, m+1):
    sum = pn = pm = prime(i)
    while sum < pm*pn:
        pn = nextprime(pn)
        sum += pn
        if sum == pm*pn:
            print(pm, pn, sum)","['number-theory', 'summation', 'combinatorics', 'prime-numbers']"
4163423,"Find $\iint (x+y)\,dx\,dy$ in the following domain.","Find $$\iint_D(x+y)\,dx\,dy$$ While $D$ is the domain bounded by the lines: $$x+y=1,\, x+y=3,\, y=5x,\, y=10x$$ My Work: First off, I thought of changing the variables, where $u=x+y$ , and $v=\frac{y}{x}$ ( $x$ is outside my domain so I didn't care about $x=0$ , would love to hear feedback about this step). So now my bounds are $u=1, u=3, v=5, v=10$ , and I get a rectangular domain. To calculate the Jacobian that I need to multiply my integral with, I'll calculate first the inverse, $$J^{-1}
   =
  \left[ {\begin{array}{cc}
   u_x & v_x \\
   u_y & v_y \\
  \end{array} } \right] = \left[ {\begin{array}{cc}
   1 & \frac{-y}{x^2} \\
   1 & \frac{1}{x} \\
  \end{array} } \right] = \frac{1}{x} +\frac{y}{x^2}=\frac{x+y}{x^2} \Longrightarrow J = \frac{x^2}{x+y}=\frac{x^2}{u}$$ So I got: $$\iint_{D^{*}}u\frac{x^2}{u}=\iint_{D^*}(\frac{u}{v+1})^2=\int^3_1du\int^{10}_5(\frac{u}{1+v})^2dv=\int^3_1du\int^{10}_5u^2*(1+v)^{-2}dv = \int^3_1-\frac{u^2}{1+v} |^{10}_5du= \int^3_1\frac{5u^2}{66}du=\frac{5*3^3}{198} - \frac{5}{198}=\frac{65}{99}$$ EDITS: Fixed the integral after the help comment from MathLover that $x=\frac{u}{1+v}$ Updated the solution, had mistake in calculations","['multivariable-calculus', 'solution-verification', 'multiple-integral']"
4163439,Why are Tensors (Vectors of the form a⊗b...⊗z) multilinear maps?,"In our linear algebra course we defined the Tensor Product between two vector spaces as an Operation so that this Diagram commutes: Here $\varphi$ and $\iota$ are multilinear maps and $\psi$ is a linear map, that is uniquely determined by $\varphi$ . $\iota$ is uniquely 1 determined by $U$ and $V$ . We constructed the tensor product between two vector spaces $U$ and $V$ as the quotient of the free space on $U \times W$ and a certain subspace $R$ .
However as far as I know this construction is not really that important, important is that it obeys the above diagram, so an alternative construction would be equally suitable. Now I have seen multiple posts on the math and physics stack-exchange that seem to define Tensors as multilinear maps. Of course I am very aware that multilinear (or in this case bilinear) maps form a vector space and I suppose that this vector space could somehow be isomorphic to $U \otimes V$ .
And I see that the concept of a tensor is very closely linked to multilinear maps (especially the multilinear map $\iota$ ). But I don't see a way to say that tensors are multilinear maps, so I am very confused. Is the statement true, that a tensor is a multilinear map? 1 up to a unique isomorphism","['tensors', 'linear-algebra', 'tensor-products', 'multilinear-algebra']"
4163446,"Number of $n \times n$ $\{0,1\}$-matrices $A$ such that $A^2$ is the transpose of $A$?","While browsing the OEIS, I saw the above nice question.
The few terms provided suggest that they were found by brute force. We conjecture that the number of $\{0, 1\}$ -matrices, which also satisfy the given condition, can be determined from the recurrence $$ 2\, a(n - 1)  +  (n^2 - 3 n + 2) \, a(n - 3)  \quad (n \ge 3), $$ starting $ a(0) = 1,\, a(1) = 2,\, a(2) = 4. $ Or, equivalently,  that $$ a(n) = \sum_{k=0}^{\lfloor n / 3 \rfloor} \frac{ 2^{n - 3 k}  \,  n!  }{3^k \,  (n - 3 k)! \, k!} \quad (n \ge 0) . $$ Can anyone prove this? The sequence is OEIS A336614 .","['linear-algebra', 'combinatorics', 'sequences-and-series']"
4163467,Must a sequence be a function on $\mathbb{N}$?,"I saw many books giving definition of a sequence as: A sequence $\left\{a_n\right\}$ is a function, $a:\mathbb{N}\to \mathbb{R}.$ This means that the domain is the set of natural numbers. But there are some sequences for which the first few natural numbers are not the part of the domain.  An example is $$a_n=\frac{1}{n^2-n}.$$ So will it be more better if we define the sequence as, a function $$a:A \to \mathbb{R},$$ where $A \subseteq \mathbb{N}$ .","['definition', 'functions', 'sequences-and-series', 'real-analysis']"
4163487,Find the best upper bound on the probability that I catch at least 5 salmon given that the total number of fish I catch is 10 on average,"Let X be the number of fishes that I fish every day. Assume that $X \sim Poisson(\lambda)$ . Each fish that I fish is a salmon with probability $0.3$ , a tuna with probability $0.4$ , a swordfish with probability $0.2$ and a carp with probability $0.1$ . Are the numbers of salmons, tunas, swordfishes and carps that I fish every day independent? Using some concentration inequality, give an upper bound on the probability that I fish at least 5 salmons given that the total number of fish that I get is on average 10 (you can admit that the number of salmons is distributed as $Poisson(\lambda p_1)$ ). Make sure that you obtain the best possible bound from the different concentration inequalities that you know. My Attempt For this one, Since you catch on average 10 fish a day, this would mean that if you catch more salmon, then you would catch fewer other types of fish, so intuitively, the number of salmons, tunas, swordfish, and carp are not independent. However, this is not formal enough.
Let $W$ denote the number of salmon per day, $V$ the number of tuna, $Y$ the number of swordfishes and $Z$ the number of carp. Then I to show independence, I need to show $$\mathbb{P}(W=w)\mathbb{P}(V=v)=\mathbb{P}(W=w, V=v)$$ and so on for the other random variables. Would the random variables for each fish also be a Poisson distribution? If so, how would I find $\mathbb{P}(W=w, V=v)$ ? The concentration inequalities I know are Markov's inequality, Chebyshev's inequality, and Chernoff's bound. The formulas are $$\mathbb{P}(X\geq c)\leq\frac{\mathbb{E}[X]}{c}$$ $$\mathbb{P}(|X-\mathbb{E}[X]|\geq c)\leq\frac{Var(X)}{c^2}$$ $$\mathbb{P}(X\geq c)\leq\frac{\mathbb{E}[e^{tX}]}{e^{ct}}$$ My question for this part is, if we are only given the average number of fish caught a day, how would we figure out the average number of salmons caught? Also, is it always true that Chebyshev's inequality is more accurate than Markov's and Chernoff's bound is the best?","['poisson-distribution', 'probability', 'upper-lower-bounds']"
4163494,Continuous maps from $\Bbb R^2 \to \Bbb R$,"Which of the following statements is true ? There are at most countably many continuous maps from $\Bbb R^2 \to \Bbb R$ There are at most finitely many continuous surjective maps from $\Bbb R^2 \to \Bbb R$ There are infinitely many continuous injective maps from $\Bbb R^2 \to \Bbb R$ There are no continuous bijective maps from $\Bbb R^2 \to \Bbb R$ My Attempt: If I take a function from $\Bbb R^2 \to \Bbb R$ that maps Circle to Line such that $f(x,y) = \alpha x$ which is continuous where $\alpha \neq 0$ and $\alpha \in \Bbb R$ but cardinality of $\Bbb R$ is $C$ which is uncountable.  So first two options are discarded. Am I right? If not, correct me with good explanation. Also cardinality of $\Bbb R^2 = C^2$ > $C$ = cardinality of $\Bbb R$ . So there does not exist injective map from $\Bbb R^2$ $\to$ $\Bbb R$ Hence option 3 is not true and 4 is only option that is true. Please help me. Thanks in advance","['continuity', 'functions', 'real-analysis']"
4163497,Question on martingale convergence for a martingale generated from a single $L^1$ function.,"I've been studying Martingale theory from William's Probability With Martingales , and I have a question. Let $X\in L^1(\Omega)$ be a real function, and let $\{\mathcal{F}_{n}\}_{n=1}^{\infty}$ be a filtration on the probability space $\Omega$ . Define $M_{n} := \mathbf{E}[X\mid \mathcal{F}_{N}]$ . Then the set $\{M_{n}\}_{n=1}^{\infty}$ forms a martingale with respect to the filtration. Williams proves that there exists a limit $M_\infty = \lim_{n \to \infty} M_{n}$ in both $L^1$ and almost surely. My question is, are there easy-to-understand, or insightful, necessary and sufficient conditions for when $M_{\infty} = X$ . One answer is to define the limit $\sigma$ -algebra $\mathcal{F}_{\infty}:= \bigcup_{n=1}^{\infty}\mathcal{F}$ . The limit function $M_\infty$ is $\mathcal{F}_{\infty}$ -measurable and $M_\infty = \mathbf{E}[X\mid \mathcal{F}_\infty]$ . Thus, we have $M_\infty = X$ if and only if $\sigma(X)\subseteq \mathcal{F}_\infty$ . Is this the only statement which is true in general? Can we add any additional conditions on $X$ which yield different results? Is there a cleaner characterization of when the martingale converges to the function which creates it?","['stochastic-processes', 'measure-theory', 'probability-theory', 'martingales']"
4163548,Spin derivative on a submanifold,"I am having some trouble proving the following expression. For a 3-manifold $Y$ , spinor bundle $S$ , codimension 1 submanifold $M\subset Y$ , subbundle $S_M$ , and normal vector $N$ , the covariant derivative is $$\nabla_X ^S \phi=\nabla_X^{S_M} \phi|_M -\frac{1}{2}\nabla_X^{M}N\cdot N\cdot \phi,$$ where the $\cdot$ are Clifford products, $X\in TM$ and $\phi \in \Gamma(S)$ . I first saw this here , which references here , which references something in German. So what I've tried so far: That book (Baum et al) gives the following formula for the spin derivative: $$\nabla_X^S \phi=X(\phi)+\frac{1}{2}\sum_{1\leq k < l\leq n} \omega_{kl} s_k\cdot s_l\cdot \phi$$ that spin connection is defined as $\omega_{kl}=g(\nabla ^M s_k,s_l)$ with orthogonal coordinates (""tetrads"", I think) $(s_1,s_2,...)$ I think I understand those like $$\omega_{jk}=<\nabla_X e_j,e_k>=<\omega^i_j e_i,e_k>,$$ where the $\omega^i_j$ are what I am more used to seeing, i.e. from tetrads, and the inner product is over the manifold coordinates. If I use this expression to try and directly derive what I can't get from above, I get $$\nabla_X ^S \phi-\nabla_X^{S_M} \phi|_M =\frac{1}{2}\sum_{1\leq k < l\leq n} \omega_{kl}s_k\cdot s_l\cdot \phi-\frac{1}{2}\sum_{1\leq k < l\leq n-1} \omega_{kl}s_k\cdot s_l\cdot \phi|_M$$ So a bunch of terms on the right cancel, and I think you are left with $$\frac{1}{2}(\omega_{1,3}s_1+\omega_{2,3}s_2)\cdot s_3\cdot \phi$$ just two terms, and the spinor is the one from $S$ . So I want to relate this to the covariant derivative of the normal vector, which I guess I want to express in the tetrad basis (say $N=N_3 s_3$ and $X=X_aS_a$ picking the normal and tangent directions so that a,b,c=1,2), $$ \nabla_X N=X_a\nabla_a (N_3 s_3)=X_a N_3 (\partial_a s_3+\omega_{a3b} s_b)=X_a N_3 \omega_{a3b}\cdot s_b=\omega_{3b} N_3\cdot s_b$$ (repeated indices summed, and that partial vanishes since $a,b\neq 3$ ). So this is kinda close, since the sum indeed gives me two terms, but I don't actually have the right number of spinors - $N_3$ is just the component here. So there are several steps here I am a bit uncomfortable with, but if I had gotten down to something like $\omega_{23}$ I would think I was basically doing it correctly. Can anyone help?","['fiber-bundles', 'spin-geometry', 'differential-geometry']"
4163556,Defining the complex exponential,"I was pondering a bit about how to define the exponential and trigonometric functions. The definitions I find the most appealing seem to be their definitions as the unique solutions to the ODEs : $y'=y,\ y_0=1$ ; $y''=-y,\ y_0=0/1$ and $y'_0=1/0$ .
However, I was wondering if this could be extended as follows: $e^z:\mathbb{C}\to \mathbb{C}$ is the unique solution to the ODE $\frac{\mathbb{d}}{\mathbb{dz}}f = f$ with $f(0)=1$ . Then you define $\cos{x}=\Re(e^{ix})$ and $\sin{x}=\Im(e^{ix})$ . Is there a theorem that allows one to prove the existence of uniqueness in this case?","['complex-analysis', 'ordinary-differential-equations']"
4163564,Rudin Theorem $3.4 (b)$,"Rudin left this proof as an exercise, so I would appreciate if someone could look over my attempt. (b) Suppose $\{x_n\}$ , $\{y_n\}$ are sequences in $\mathbb{R}^k$ , $\{\beta_n\}$ is a sequence of real numbers, and $x_n \to x$ , $y_n \to y$ , $\beta_n \to \beta$ . Then \begin{align*}
\lim\limits_{n \to \infty} (x_n + y_n) & = x + y \\
\lim\limits_{n \to \infty} x_n \cdot y_n & = x \cdot y \\
\lim\limits_{n \to \infty} \beta_n x_n & = \beta x.
\end{align*} Here is my attempt. I'm going to stick to the notation used by Rudin. Let $x_n = (a_{1,n}, \ldots, a_{k,n})$ , $y_n = (b_{1,n} \ldots, b_{k,n})$ , $\beta_n = (\beta_1, \ldots, \beta_k)$ , $x = (a_1, \ldots, a_n)$ , $y = (b_1, \ldots, b_n)$ . Then for any $n$ , we have: $$ 
x_n + y_n = (a_{1,n} + b_{1,n}, \ldots, a_{k,n} + b_{k,n})
$$ Since $x_n \to x$ , by part (a), the sequence converges component-wise, so $a_{j,n} \to a_j$ for each $j$ . Since $y_n \to y$ , we have $b_{j,n} \to b_j$ for each $j$ . So for each $j$ , $a_{j,n} + b_{j,n} \to a_j + b_j$ by Theorem 3.3(a) (sum of convergent sequences). So, by part (a) again, since $x_n + y_n$ converges componentwise, we have $$ 
x_n + y_n \to (a_1 + b_1, \ldots, a_k + b_k) = (a_1, \ldots, a_k) + (b_1, \ldots, b_k) = x + y.
$$ Moving to the second limit. For any $n$ , we have $$ 
x_n \cdot y_n = (a_{1,n}, \ldots, a_{k,n}) \cdot (b_{1,n}, \ldots, a_{k,n}) = \sum\limits_{i=1}^k a_{i,n} b_{i,n}.
$$ Again, as $x_n \to x$ , we have $a_{j,n} \to a_j$ , and as $y_n \to y$ , we have $b_{j,n} \to b_j$ . By Theorem 3.3, products of convergent sequences convergence, so for each $j$ , we have $a_{j,n} b_{j,n}$ converges to $a_j b_j$ . By Theorem 3.3 again and induction on $k$ , we have \begin{align*}
\sum\limits_{i=1}^k a_{i,n} b_{i,n} \to \sum\limits_{i=1}^k a_i b_i = (a_1, \ldots, a_k) \cdot (b_1, \ldots, b_k) = x \cdot y.
\end{align*} Moving now to the third limit. For any $n$ , we have $$ 
\beta_n x_n = (\beta_n a_{1,n}, \ldots, \beta_{n} a_{k,n}).
$$ By Theorem 3.3, for each $j$ , we have $\beta_n a_{j,n} \to \beta a_j$ (product of convergent sequences). So $\beta_n x_n$ converges component-wise, so it converges, and we have $$ 
\beta_n x_n \to (\beta a_1, \ldots, \beta a_k) = \beta (a_1, \ldots, a_k) = \beta x.
$$ How do these look?","['solution-verification', 'sequences-and-series']"
4163593,Is every ordered field a topological field?,"Let $F$ be an ordered field and give $F$ the order-topology. Then is $F$ a topological field (that is, are the operations \begin{equation}
\begin{split}
+&:F\times F\to F\\
-&:F\to F\\
\times&:F\times F\to F\\
^{-1}&:F\setminus\{0\}\to F
\end{split}
\end{equation} continuous)? I checked that $+,-$ and $^{-1}$ are continuous. But I could not prove the continuity of $\times$ . Here is my attempt to prove the continuity of $\times$ . Let $(x_0,y_0)\in F\times F$ and let $\epsilon\in F$ be positive. I want to show that there is a positive $\delta\in F$ such that \begin{equation}
|x-x_0|<\delta, |y-y_0|<\delta\quad \text{implies}\quad |xy-x_0y_0|<\epsilon
\end{equation} where the absolute value is defined in the obvious way. If $\delta\in F$ is any positive element where $|x-x_0|<\delta, |y-y_0|<\delta$ , then \begin{equation}
\begin{split}
|xy-x_0y_0| &= |xy-xy_0+xy_0-x_0y_0|\\
&\leq |x||y-y_0| + |x-x_0||y_0|\\
&<(|x_0|+\delta)\delta + \delta|y_0|\\
&= \delta^2 + \alpha\delta
\end{split}
\end{equation} where $\alpha = |x_0|+|y_0|$ . Thus the only thing I need to prove is that for any positive $\epsilon\in F$ and any nonnegative $\alpha\in F$ , there is a positive $\delta\in F$ such that \begin{equation}
\delta^2+\alpha\delta<\epsilon\text{.}
\end{equation} This is true if $F$ is a subfield of $\mathbb R$ , but I cannot prove it when $F$ is an arbitrary ordered field.","['order-topology', 'continuity', 'abstract-algebra', 'ordered-fields', 'general-topology']"
4163626,Find a counterexample of: If $1_{A}+1_{B}$ is a random variable then $A$ and $B$ are measurable,"I was trying to prove the following proposition, if $1_{A}+1_{B}$ is a random variable then $A$ and $B$ are measurable. Where $1_{A}$ is the indicator, function given by \begin{equation}
1_{A}(x) = 
\begin{cases}
1, \ \   x \in A \\
0, \ \   x \notin A
\end{cases}
\end{equation} I proved that the converse is true. Here's the proof: Let $A$ and $B$ be measurable sets, i.e, $A$ , $B \in \mathbb{F}$ , where $\mathbb{F}$ is a $\sigma$ -algebra.  Let $x \in \mathbb{R}$ , If $x<0$ then $(1_{A}+1_{B}\leq x ) =  \emptyset \in  \mathbb{F} $ if $x \in [0,1)$ then $(1_{A}+1_{B}\leq x ) = (A \cup B)^{\complement} \in \mathbb{F} $ if $x \in [1,2) $ then $ (1_{A}+1_{B} \leq x ) = (A \cap B )^{ \complement } \in \mathbb{F} $ if $x \geq 2  $ then $ (1_{A}+1_{B} \leq x ) = \Omega \in \mathbb{F} $ This implies that $1_{A}+1_{B}$ is random variable. But not able to prove that if $1_{A}+1_{B}$ is a random variable then $A$ and $B$ are measurable. I don't know where to start. I have the following Questions: Do you know a counterexample to this proposition? Do you have an idea that could help me to prove it? Thanks in advance.","['measure-theory', 'examples-counterexamples', 'random-variables']"
4163635,"Density and dimensionality of zeros in inverse square force fields of randomly distributed sources in (at least) 1, 2 and 3 dimensions?","Background: In this answer to Are there places in the Universe without gravity? in Astronomy SE I did a quick finite 2D calculation for 20 random sources to see if there was at least one zero, and without rigor convinced myself that there might always be some finite density of zeros. See image below which shows log 10 normalize force magnitude, the script is in the linked answer. Question: For a random distribution of discrete inverse square force sources (e.g. stars in space) of finite density , what is the density and dimensionality of zeros in the net force field relative to the density of sources? Please address the 1, 2 and 3 dimension cases at least. Results of a very quick exploration, looking for zero-dimensional zeros in 2D. The circa -14 values are simply a result of the cutoff in the minimization. The original script can be found in the linked answer. The labeled minima are the result of minimization from the lowest grid point value. I haven't made an attempt yet to look for all zeros .","['physics', 'linear-algebra', 'mathematical-physics', 'morse-theory']"
4163641,"Solution verification of expressing one variable in terms of other, used in Schweinler-Wigner orthogonalization procedures.","I don't understand how $$\mathbf{w}_{\kappa}=\sum_{k} p_{\kappa}^{-\frac{1}{2}} u_{k \kappa} \mathbf{v}_{k}\implies\mathbf{v}_{k}=\sum_{\kappa} u_{k \kappa}^{*} p_{\kappa}^{\frac{1}{2}} \mathbf{w}_{k}$$ given $\sum_{\kappa=1}^{n} u_{k \kappa}^{*} u_{\ell \kappa}=\delta_{k l}$ . When I tried to solve it, I got an extra summation: We have $\omega_{\kappa}=\sum_{k=1}^{n} p_{\kappa}^{-1 / 2} u_{k \kappa} v_{k}$ so $$u_{k \kappa}^{*} p_{\kappa}^{1 / 2} \omega_{\kappa}=\sum_{k=1}^{n} u_{k \kappa}^{*} p_{\kappa}^{1 / 2} p_{\kappa}^{-1 / 2} u_{k \kappa} v_{k}\implies\sum_{\kappa=1}^{n} u_{k \kappa}^{*} p_{\kappa}^{1 / 2} \omega_{\kappa}=\sum_{k=1}^{n}\left(\sum_{\kappa=1}^{n} u_{k \kappa}^{*} u_{k \kappa}\right) v_{k}.$$ Since $\sum_{\kappa=1}^{n} u_{k \kappa}^{*} u_{\ell \kappa}=\delta_{k l}$ , it follows that $$\sum_{k=1}^{n} u_{k \kappa}^{*} p^{1/2} \omega_{\kappa}=\sum_{k=1}^{n}\left(\delta_{k k}\right) v_{k}=\sum_{k=1}^{n} v_{k}.$$ For more details on this problem see equations $(3')$ , $(6)$ and $(6')$ of Schweinler (1970) .","['matrices', 'solution-verification', 'linear-algebra', 'orthogonality']"
4163682,What is the intuition of this problem about $|f'(x)|$?,"$\newcommand{\d}{\mathrm{d}}$ Recently, I met a problem, which says that: Given that $f(x)$ have continious derivatives on close interval $[0,2]$ , $f(0)=f(2)=0$ , $M=\max_{[0,2]} |f(x)|$ . Prove that: (1) there exists a number $\xi \in (0,2)$ , s.t. $|f'(\xi)| \geq M$ . (2) If for every $x \in (0,2)$ , $|f'(x)|\leq M$ , then $M=0$ . I can prove the first problem. However, the second problem makes me mad. So I look up the solution. The proof is here: We know that there exists a number $x_0$ , s.t. $M=|f(x_0)|=|f(x_0)-f(0)|=|\int_{0}^{x_0}f'(x)dx|\leq\int_{0}^{x_0}|f'(x)|dx\leq\int_{0}^{x_0}Mdx\leq Mx_0$ , $M=|f(x_0)|=|f(x_0)-f(2)|=|\int_{x_0}^{2}f'(x)dx|\leq\int_{x_0}^{2}|f'(x)|dx\leq\int_{x_0}^{2}Mdx\leq M(2-x_0)$ So, $M(1-x_0) \leq 0 and  M(1-x_0)\geq 0$ . If $x_0 \neq 1$ , $M$ is apparently $0$ , if $x_0=1$ , then $M \leq \int_{0}^{1}|f'(x)|dx \leq M$ and $M \leq \int_{1}^{2}|f'(x)|dx \leq M$ , if $M \neq 0$ , then it contradicts. I can understand this solution, but I don't know the intuition or motivation behind this. I don't believe that there exists a solution which has no motivation, but I can't find the motivation behind this problem. In fact, everytime I meet this kind of problems, I often have no idea, I really think that this kind of problems are hard to solve, can anyone give some advice or some resource about this kind of problems?","['calculus', 'derivatives']"
4163719,"How to prove that if $g*g=0$, then $g=0$?","Given a continuous function $g$ , if the convolution $g*g(t)$ (defined as: $\int^t_0(g(r)g(t−r))dr$ equals $0$ , $\forall t\geq 0$ , then $g=0$ . My attempt was to use Laplace transformation, but such transformation for the function could be non-existent. I'm also aware of the Titchmarsh theorem, but all the proofs I've found involve material that I hadn't studied yet. The proof for the claim above should neither be very lengthy, nor include material that is studied in Harmonic Analysis. How can one prove this claim using relatively simple means?","['convolution', 'calculus', 'laplace-transform', 'ordinary-differential-equations']"
4163726,How can I prove that $a_n>0$ infinitely often?,"Let $n\in\mathbb{N}$ and $$a_n:=sin(2\pi^2(2n+1)!)$$ How can I prove that $a_n>0$ infinitely often? Clearly, $a_n>0$ infinitely often is equivalent to { $\pi(2n+1)!$ } $\leq 0.5$ infinitely often where {.} is fractional part function. Since $\pi$ is irrational, { $\pi(2n+1)!$ } will never be zero. How can we show that { $\pi(2n+1)!$ } $\leq 0.5$ infinitely many times? Graph of $y=a_n$ is available here (Desmos). On observing the graph, it seems that $a_n>0$ is true infinitely often. I  tried method of induction etc.  but I am not able to prove this. What are the various ways to prove it and how can it be proved?","['factorial', 'fractional-part', 'functions', 'pi', 'trigonometry']"
4163769,continuous functions preserve convergence in probability (proof from definition),"This is Exercise 2.3.2 from Durrett's Probability: Theory and Examples , Prove from the definition that if $f$ is continuous and $X_n \rightarrow X$ in probability then $f(X_n) \rightarrow f(X)$ in probability. Durrett has provided proof through the fact that $X_n \rightarrow X$ in probability if and only if for every subsequence $X_{n(m)}$ there is a further subsequence $X_{n(m_k)}$ that converges almost surely to X. But I can't figure out a direct proof. I tried the following:
Fix $\epsilon >0$ . $P(|f(X_n)-f(X)|>\epsilon) \leq P(|X_n - X|>b_n) + P(|X_n - X|\leq b_n 
 \text{ and } |f(X_n)-f(X)|>\epsilon$ ). I want to choose $b_n$ decreasing slowly so that the first term will go to zero, and hope that the second term will go to zero since f is continuous. Could you help with any way to continue or any other ways?",['probability-theory']
4163778,Limsup of random variables vs limsup of events,"In 2.7 of this notes, it is shown that, using Borel-Cantelli lemma, if $E_n = \left\{\frac{X_n}{\log n} \geq 1\right\}$ then $\mathbb{P}(\limsup \ E_n) = 1$ but why does author conclude that if $Y = \limsup \frac{X_n}{\log n}$ , then $\mathbb{P}(Y \geq 1) = 1$ ? My question is Borel-cantelli lemma says events $\limsup$ of some events is 0 or 1, but in most books and online notes, an event in terms of random variable $\limsup X_n$ concluded to have probability 0 or 1. Can someone clarify the confusion ?","['measure-theory', 'limsup-and-liminf', 'borel-cantelli-lemmas', 'probability-theory', 'probability']"
4163835,How to complete this proof on the codimension of the intersection of affine varieties,"Edit: I think maybe I'm making this way harder than it needs to be.  If $L\subsetneq R:= k[z_1,...,z_m]$ is an ideal such that $V(L)$ is equidimensional and $f_1,...,f_p\in R$ , then it seems like Krull's PIT gives $$dim(R/\mathfrak{p}) \geq dim(R/L)-p  $$ for any prime $\frak p$ minimal over $(f_1,...,f_p)+L$ . Maybe all the talk about regular embeddings and localizing just confused me. Or maybe I'm still confused. I've been trying to do this Ravi Vakil exercise for a while now: It all makes sense until the end. I need to show that the diagonal $$\Delta = V(x_1-y_1,x_2-y_2,...,x_d-y_d)\subset Spec~k[x_1,...,x_d,y_1,...,y_d]= \mathbb{A}_k^d\times \mathbb{A}_k^d$$ cuts away no more than $d$ dimensions from $X\times Y \subset \mathbb{A}_k^d\times \mathbb{A}_k^d$ . $~~$ (This suffices since then $$dim(X\cap Y) = dim(X\times Y \cap \Delta) \geq dim(X)+dim(Y)-d.)$$ Vakil says to do this ""locally"" and use Krull's PIT.  Ok so it suffices to show that the embedding $$\iota: (X\times Y) \cap \Delta \longrightarrow X\times Y $$ is ""a regular embedding of codimension $d$ "" in the sense that for every point $p\in (X\times Y) \cap \Delta$ , the kernel of the induced map of stalks $$ ker~\big(~\mathcal{O}_{X\times Y ,~ \iota(p)} \longrightarrow
 \mathcal{O}_{(X\times Y) \cap \Delta, ~p}\big) $$ is generated by a ""regular sequence"" of length $d$ , $$\sigma_1,...,\sigma_d.$$ ""Regular sequence"" means that for $i=1,...,d$ , we have that $\sigma_i$ is not a zero-divisor of $$\mathcal{O}_{X\times Y ,~ \iota(p)}\big/(\sigma_1,...,\sigma_{i-1}). $$ Showing this ""regularity"" condition suffices because then it is also true that every $p\in X\times Y$ is contained in some open neighborhood $U\subset X\times Y$ such that $\Delta\cap U$ is cut out from $U$ by such a sequence, so that we can then apply Krull's PIT one element of the sequence at a time to show that each element cuts away exactly $1$ dimension from $U$ (recalling that $X\times Y$ and therefore $U$ is equidimensional since $X$ and $Y$ are equidimensional -- thanks to Alex Youcis for the proof of that.) OK MY QUESTION IS:  How do I find this regular sequence x_1,...,x_d at each point $p$ ? In algebraic terms, given radical ideals $I\subset k[x_1,...,x_n]=k[\overline x]$ , $J\subset k[y_1,...,y_n]=k[\overline y]$ (such that $V(I), V(J)$ are equidimensional), and a prime ideal $$\mathfrak p\subset k[\overline x]\otimes_k k[\overline y]\big/ (I\otimes k[\overline y]+ k[\overline x]\otimes J)$$ I need to somehow find a regular sequence of elements in $$\bigg(k[\overline x]\otimes_k k[\overline y]\big/ (I\otimes k[\overline y]+ k[\overline x]\otimes J)\bigg)_\frak p$$ that generates the ideal $(x_1-y_1,x_2-y_2,...,x_d-y_d)_\frak p$ . And another peripheral question that occurs to me just now: If we can complete the proof in this way, haven't we actually shown that the codimension of every component of $X\cap Y$ is exactly $codim_{\mathbb{A}_k^d}(X)+codim_{\mathbb{A}_k^d}(Y)$ ? After all, we covered $X\cap Y$ with open sets $W$ , each obtained by starting with $U$ , a pure $dim(X)+dim(Y)$ -dimensional variety, and cutting out $W$ with a regular sequence of length $d$ .  So isn't every such $W$ of pure dimension $dim(X)+dim(Y)-d$ ?  Anyway that's probably a stupid question that just occurred to me; my real interest is in the first question of how to complete the proof.",['algebraic-geometry']
4163850,Eigenfunction expansion and boundary conditions - understanding the nuances,"Given an orthonormal basis of eigenfunctions $\{ f_n\}_{n=1}^{\infty}$ for a Sturm-Liouville
(SL) problem in the interval $[0,L]$ , and a function $f(x)$ that
does not satisfy the boundary conditions (BC) of the SL problem, in which
cases does the fact that $f$ does not satisfy the BC of the SL problem
directly imply that the eigenfunction expansion for $f$ does not
uniformly converge in $[0,L ]$ ? As shown in example 5 (page 7) in these lecture notes that confuse me , the eigenfunction expansion of $f(x)=1$ in the orthonormal basis $\left \{ \sqrt{\frac{2}{L}} \sin(n\pi x /L)  \right \}_{n=1}^{\infty}$ does not converge uniformly on $[ 0,L ]$ . In example 6, it is shown that the eigenfunction expansion of $f(x)=x$ in the orthonormal basis $\left \{ \sqrt{\frac{1}{L}} \right \} \cup \left \{ \sqrt{\frac{2}{L}} \cos(n\pi x /L)  \right \}_{n=1}^{\infty}$ does converge uniformly on $[ 0,L ]$ (I completely understand how to prove it using Weierstrass M-test and continuity). In both cases, the function $f$ does not satisfy the corresponding boundary conditions, but in the first case, we deduced that this implies that it is not possible to have uniform convergence in the entire interval, but in the second example we did not. What is the reason for this difference? Is this conclusion only true when the boundary conditions are specified for the function itself but not when they are specified on the derivative of the function? For your convenience, here are screenshots of the relevant example, (Jim Lambers Mat 606, usm.edu)","['sturm-liouville', 'uniform-convergence', 'ordinary-differential-equations', 'eigenfunctions']"
4163868,Number of roots of f(x) = $\Sigma_{r=1}^{2009} \frac{r}{x-r}$,"This question is from a practice workbook for a college entrance exam. State whether true or false -
The number of times the function $f(x) = \Sigma_{r=1}^{2009} \frac{r}{x-r}$ vanishes is 2008. I rewrote $\frac{r}{x-r}$ as $\frac{x}{x-r} - 1$ This brings the equation to $x \Sigma_{r=1}^{2009}\frac{1}{x-r} = 2009.$ Now I notice that this sigma notation can be reached if I took the logarithm of a polynomial with roots 1,2,3,...,2009 and differentiated.
So $$\Sigma_{r=1}^{2009} \frac{1}{x-r} = \frac{2009}{x} = \frac{1}{x^{2009} + ax^{2008} +...+ 2009!}$$ We need to find the number of values of x that can satisfy this.
But I don't know what to do with this.","['summation', 'functions', 'polynomials']"
4163920,Prove that flow map $\theta_t$ is orientation preserving,"I was doing Lee's smooth manifold book exercise in Problem 15.4 needs to show Let $M$ be a oriented smooth manifold ,and $\theta$ the flow generated by some smooth vector field. Prove that $$\theta_t :M_t \to M_{-t}$$ is orientation preserving diffeomorphism. First it's diffeomorphism by fundamental theorem of flow.To prove that is orientation preserving seems rather complicated,the rough idea is simple we need to prove that Jacobian under positive oriented chart has positive determinant.Formally if all of them lies in the single chart for all time $t\in \Bbb{R}$ and all point $p\in M$ then the Jacobian is $$\frac{\partial \theta^j}{\partial x^i}(t,p)$$ is smooth w.r.t to time $t$ which is defined for all $t$ . which connect $0$ and $t$ .hence they have same sign of determinate. The question become rather complicated,the reason is when $t$ varies,the point may varies the chart may vary that Jacobian is not defined in a consistent way.So I have no idea how to deal with this problem?","['orientation', 'smooth-manifolds', 'differential-geometry']"
4163965,Set of points at which the sequence $(f_n(x))_{n=1}^{\infty}$ converges is $\mathcal{S}$-measurable,"I have proved the following statement(s) and I would like to know if my proof is correct and/or if it could be improved somehow. (a) Suppose $f_1,f_2,\dots$ is a sequence of functions from a set $X$ to $\mathbb{R}$ .
Explain why $\{x\in X:\text{ the sequence }f_1(x), f_2(x),\dots\text{ has a limit in }\mathbb{R}\}=\bigcap_{n=1}^{\infty}\bigcup_{j=1}^{\infty}\bigcap_{k=j}^{\infty}(f_j-f_k)^{-1}((-\frac{1}{n},\frac{1}{n}))$ (b) Suppose $(X,\mathcal{S})$ is a measurable space and $f_1,f_2,\dots$ is  sequence of $\mathcal{S}$ -measurable functions from $X$ to $\mathbb{R}$ . Prove that $\{x\in X:\text{ the sequence }f_1(x), f_2(x),\dots\text{ has a limit in }\mathbb{R}\}$ is an $\mathcal{S}$ -measurable subset of $X$ . My proofs: (a) Let $x\in\{x\in X:\text{ the sequence }f_1(x), f_2(x),\dots\text{ has a limit in }\mathbb{R}\}$ : then $(f_n(x))_{n=1}^{\infty}$ is a convergent sequence of real numbers so it is also a Cauchy sequence of real numbers which implies* that for every $n\geq 1$ there exists $N\geq 1$ such that $|f_j(x)-f_k(x)|<\frac{1}{n}$ for all $j,k>N$ so if we set $j_{N}:=N+1$ we have that $|f_{j_N}(x)-f_k(x)|<\frac{1}{n}$ for all $k>j_N$ thus $-\frac{1}{n}<f_{j_N}(x)-f_k(x)<\frac{1}{n}\Leftrightarrow (f_{j_N}-f_k)(x)\in (-\frac{1}{n},\frac{1}{n})\Leftrightarrow x\in (f_{j_N}-f_k)^{-1}((-\frac{1}{n},\frac{1}{n}))$ ; since this is valid for every $n\geq 1$ , for a certain $j\geq 1$ (which we have called $j_N$ ), and for all the $k$ s equal or greater than this $j$ it follows that $x\in\bigcap_{n=1}^{\infty}\bigcup_{j=1}^{\infty}\bigcap_{k=j}^{\infty}(f_j-f_k)^{-1}((-\frac{1}{n},\frac{1}{n}))$ . Now, let $x\in\bigcap_{n=1}^{\infty}\bigcup_{j=1}^{\infty}\bigcap_{k=j}^{\infty}(f_j-f_k)^{-1}((-\frac{1}{n},\frac{1}{n}))$ : then for every $n\geq 1$ there exists some $j\geq 1$ such that for all $k\geq j$ it is $x\in (f_j-f_k)^{-1}((-\frac{1}{n},\frac{1}{n}))$ (which is equivalent to saying that $|f_j(x)-f_k(x)|<\frac{1}{n}$ ) so if we fix $n^*\geq 1$ there is $j^*\geq 1$ such that for all $k\geq j^*,\ |f_j(x)-f_k(x)|<\frac{1}{2n}$ thus $|f_j(x)-f_k(x)|=|f_j(x)-f_{j^*}(x)+f_{j^*}(x)-f_k(x)|\leq |f_j(x)-f_{j^*}(x)|+|f_{j^*}(x)-f_k(x)|<\frac{1}{2n}+\frac{1}{2n}=\frac{1}{n}$ for all $n\geq 1$ and $j,k\geq j^*$ . In summary we have found that for every $n\geq 1$ there exists $N\geq 1$ (the one we called $j^*$ in the previous sentence) such that $|f_j(x)-f_k(x)|<\frac{1}{n}$ for all $j,k\geq N$ so $(f_n(x))_{n=1}^{\infty}$ is a Cauchy sequence* of real numbers and hence convergent thus $x\in\{x\in X:\text{ the sequence }f_1(x), f_2(x),\dots\text{ has a limit in }\mathbb{R}\}$ . (b) By hypothesis all the $f_n$ are $\mathcal{S}$ -measurable functions so their difference is also a $\mathcal{S}$ -measurable function and since $(-\frac{1}{n},\frac{1}{n})$ is a Borel set for every $n\geq 1$ (being open) the preimages $(f_j-f_k)^{-1}((-\frac{1}{n},\frac{1}{n}))\in\mathcal{S}$ and being $\mathcal{S}$ a $\sigma$ -algebra it is closed under intersections and unions so $\bigcap_{n=1}^{\infty}\bigcup_{j=1}^{\infty}\bigcap_{k=j}^{\infty}(f_j-f_k)^{-1}((-\frac{1}{n},\frac{1}{n}))\in\mathcal{S}$ , as desired. Note: in part (a) of the proof I have implicitly used the fact that if $(a_n)_{n=1}^{\infty}$ is a Cauchy sequence of real numbers then for every $n\geq 1$ there exists $j\geq 1$ such that $|a_j-a_k|<\frac{1}{n}$ for all $k\geq j$ . Proof. Let $n\geq 1$ and set $\varepsilon=\frac{1}{n}$ : then being $(a_n)_{n=1}^{\infty}$ Cauchy we have that there exists $N\geq 1$ such that $|a_j-a_k|<\frac{1}{n}$ for all $j,k>N$ so if we set $j:=N+1$ we have $|a_j-a_k|<\frac{1}{n}$ for all $k\geq j$ and so for $n\geq 1$ we have thus found that there exists $j\geq 1$ such that $|a_j-a_k|<\frac{1}{n}$ for all $k\geq j$ , and the claim is proved.","['measure-theory', 'solution-verification', 'real-analysis']"
4164004,Prove convergence of $\int_0^1 \frac{\ln(x)}{1-x^2} dx$,I'm really lost about how to prove the integral converges. I tried using the fraction expansion theorem and got to $$ \frac{\ln(x)}{1-x^2} = \frac{\ln(x)}{2(1+x)} + \frac{\ln(x)}{2(1-x)}$$ But I didn't find a way to use it. I can't use the Dirichlet's test or Comparing tests since the function is not positive. What am I missing? Thank you in advance and have a nice day!,"['integration', 'definite-integrals']"
4164011,Variance of a shifted exponential,"I am having trouble finding the variance of this distribution.
the mean is 2 $\lambda$ , but the variance is $\lambda^2$ ?
In the solution, they say $\operatorname{Var}(\bar Y) =\lambda^2 $ , I'm really puzzled... Shifting a distribution will not really cause its variance to change right? Really appreciate any help! Thanks","['statistics', 'probability-distributions', 'variance']"
4164033,For how many integer values of $m$ parabola $y=(m-2)x^2+12x+m+3$ passes through only three quadrants?,"For how many integer values of $m$ the graph of parabola $y=(m-2)x^2+12x+m+3$ passes through only three quadrants? $1)0\qquad\qquad2)7\qquad\qquad3)12\qquad\qquad4)\text{infinity}$ If the parabola $ax^2+bx+c=0$ passes through three quadrant I think we should have $\frac ca>0$ and $\Delta>0$ : $$\frac{m+3}{m-2}>0\Rightarrow m\in(-\infty,-3] \cup(2,+\infty)$$ $$\Delta'>0\Rightarrow 36-(m-2)(m+3)>0\Rightarrow m^2+m-42<0\Rightarrow m\in [-7,6]$$ Also for $m=2$ we have $y=12x+5$ and it passes through three quadrants. So $m$ can be $-7,-6,\cdots,-3, $ or $2,3,4,5,6$ . so there are $10$ possible values for $m$ but this isn't in the options. What am I missing?","['algebra-precalculus', 'quadratics']"
4164083,Finding expected total number of die rolls,"Question Ann and Bob take turns to roll a fair six-sided die. The game ends after a six or three consecutive fives come up, with the winner being the last person who threw the die. Ann will go first. $(a)\quad$ Find the probability that Ann will win. $(b)\quad$ Find the expected total number of rolls. My working Let $(A, 0)$ denote the state in which it is $A$ 's turn and the prior toss was not a $5$ or a $6$ , $(A, 5)$ the state in which it is $A$ 's turn and the prior toss was a $5$ and $(A, 55)$ the state in which it is $A$ 's turn and the prior two tosses were a $5$ . The states for $B$ are defined similarly and note also that $(A, 0)$ is the starting state. Now, for any state $S$ , let $P(S)$ denote the probability that $A$ will eventually win, given that we are now in state $S$ and we have the following relationships: $$\begin{aligned}
P(A, 0) & = \frac 1 6 + \frac 1 6 P(B, 5) + \frac 4 6 P(B, 0)
\\[5 mm] P(A, 5) & = \frac 1 6 + \frac 1 6 P(B, 55) + \frac 4 6 P(B, 0)
\\[5 mm] P(A, 55) & = \frac 2 6 + \frac 4 6 P(B, 0)
\\[5 mm] P(B, 0) & = \frac 1 6 P(A, 5) + \frac 4 6 P(A, 0)
\\[5 mm] P(B, 5) & = \frac 1 6 P(A, 55) + \frac 4 6 P(A, 0)
\\[5 mm] P(B, 55) & = \frac 4 6 P(A, 0)
\end{aligned}$$ The system of linear equations above can be easily solved to give $P(A, 0) = \frac {93} {170}$ , which is the correct answer for $(a)$ . However, I am not sure how to approach $(b)$ , whose answer is $\frac {129} {22}$ . Any intuitive suggestions will be greatly appreciated :) Edit Following some hints Joe posted in an answer, I managed to solve $(b)$ :) Let $X$ be the number of dice rolls it takes for the game to stop, $A$ be the event that a $5$ is first rolled, $B$ the event that a $6$ is first rolled and $C$ the event that neither a $5$ nor a $6$ is first rolled. By the law of iterated expectation, we have $$\begin{aligned}
\mathbb{E}(X) & = \mathbb{E}(X \mid A) \mathbb{P}(A) + \mathbb{E}(X \mid B) \mathbb{P}(B) + \mathbb{E}(X \mid C) \mathbb{P}(C)
\\[5 mm] & = (1)\left(\frac 1 6\right) + \mathbb{E}(X \mid B) \mathbb{P}(B) + [\mathbb{E}(X) + 1]\left(\frac 4 6\right)
\end{aligned}$$ To find $\mathbb{E}(X \mid B) \mathbb{P}(B)$ , I chose to consider different cases. The game ends with three $5$ s or two $5$ s and a $6$ , each case happening with probability $\left(\frac 1 6\right)^3$ and taking three turns to end. The game ends with one $5$ and one $6$ , which happens with probability $\left(\frac 1 6\right)^2$ and takes two turns to end. We must also not forget the cases where the game does not end (immediately). Two $5$ s are thrown, followed by neither a $5$ nor a $6$ , which happens with probability $\left(\frac 1 6\right)^2\left(\frac 4 6\right)$ and takes $\mathbb{E}(X + 3)$ turns to end. One $5$ is thrown, followed by neither a $5$ nor a $6$ , which happens with probability $\left(\frac 1 6\right)\left(\frac 4 6\right)$ and takes $\mathbb{E}(X + 2)$ turns to end. These four cases, when added up, will give $$\begin{aligned}
\mathbb{E}(X \mid B) \mathbb{P}(B) & = \left(\frac 1 6\right)^3(3)(2) + \left(\frac 1 6\right)^2(2) + \left(\frac 1 6\right)^2\left(\frac 4 6\right)[\mathbb{E}(X + 3)] + \left(\frac 1 6\right)\left(\frac 4 6\right)[\mathbb{E}(X + 2)]
\\[5 mm] & = \frac 1 {12} + \frac 1 {54} \mathbb{E}(X + 3) + \frac 1 9 \mathbb{E}(X + 2)
\end{aligned}$$ Thus, $$\begin{aligned}
\mathbb{E}(X) & = (1)\left(\frac 1 6\right) + \mathbb{E}(X \mid B) \mathbb{P}(B) + [\mathbb{E}(X) + 1]\left(\frac 4 6\right)
\\[5 mm] & = \frac 1 6 + \frac 1 {12} + \frac 1 {54} \mathbb{E}(X + 3) + \frac 1 9 \mathbb{E}(X + 2) + \frac 4 6 \mathbb{E}(X + 1)
\\[5 mm] & = \frac 1 4 + \frac 1 {54} \mathbb{E}(X + 3) + \frac 1 9 \mathbb{E}(X + 2) + \frac 4 6 \mathbb{E}(X + 1)
\\[5 mm] & = \frac {43} {36} + \frac {43} {54} \mathbb{E}(X)
\\[5 mm] \implies \mathbb{E}(X) & = \frac {129} {22}
\end{aligned}$$","['expected-value', 'statistics', 'dice', 'probability']"
4164086,A proof that Gaussian binomial coefficients are integers,"Recall that the Gaussian binomial coefficients are defined as $$\binom{m}{n}_q= \dfrac{(q^m-1)(q^m-q)\cdot\ldots\cdot(q^m-q^{n-1})}{(q^n-1)(q^n-q)\cdot\ldots\cdot(q^n-q^{n-1})}.$$ In this post , is explained how to prove that those coefficients are integers using induction. However, in a question of a French written competitive examination, it is suggested to prove it first when $q=p^i$ is the power of a prime. And then extend the result to any $q$ integer. I'm able to prove that if $q=p^i$ , $$\binom{m}{n}_q$$ is the number of linear subspaces of dimension $n$ in the linear space $\mathbb F_q^m$ where $\mathbb F_{q}$ stands for the finite field of cardinality $q$ . $\binom{m}{n}_q$ is, therefore, an integer in that case. But I'm not able to generalize the result for $q$ integer. The fundamental theorem of arithmetic should be at play as well as a way to factor $(r^m s^m -r^j s^j)$ . Any idea?","['arithmetic', 'combinatorics', 'prime-numbers']"
4164107,Proving that it's impossible to prove irrationality of all real numbers.,"Suppose that we are given a real number $r\in \mathbb R$ , then certainly it is either rational or irrational. Let $S\subset \mathbb R$ be a set of real numbers for which it is possible to prove that $s\in S$ is an irrational number. Now I claim the following: Claim: $S$ is at most countable. Proof: Let $P$ be the set of all proofs in this universe. It's clear that $P$ is at most countable. Let $P'\subset P$ be a set of proofs that are used to prove that $S$ consists of real numbers whose irrationality can be proved. By Cantor's theorem and hypothesis, it follows that card $(P')=$ card $(S)\lt$ card $(\mathbb R\setminus\mathbb Q)$ and this proves the claim. In other words, it's impossible to prove irrationality of all irrational numbers. Is the above idea correct? Please let me know if something is wrong in this. And if my claim is true then it is also possible in similar lines to show that Euler Mascheroni constant may never be proven to be transcendental as there are not enough proofs (at most countable). Thanks. I am sorry if this question sounds very silly or trivial.","['elementary-set-theory', 'solution-verification', 'real-analysis']"
4164169,$\measuredangle C=120^\circ$ and two altitudes,"$AH$ and $BD$ are altitudes of $\triangle ABC$ and $\measuredangle ACB=120^\circ$ . If $S_{\triangle HCD}=\dfrac{15\sqrt3}{4},$ find the area of $\triangle ABC$ . $$S_{\triangle HCD}=\dfrac12\cdot CH\cdot CD\cdot\sin\measuredangle HCD=\dfrac{\sqrt3}{4}CH\cdot CD=\dfrac{15\sqrt{3}}{4}\ \implies CH\cdot CD=15$$ On the other hand $$S_{\triangle ABC}=\dfrac12\cdot AC\cdot BC\cdot\sin\measuredangle ACB=\dfrac{\sqrt3}{4}AC\cdot BC=?$$ I noted that $ABDH$ is inscribed, because $\measuredangle ADB=\measuredangle AHB=90^\circ$ , so $$AC\cdot CD=BC\cdot CH.$$ I am stuck here. Thank you in advance!","['triangles', 'trigonometry', 'area', 'geometry']"
4164240,"If $S$ is symmetric positive definite and $SA$ symmetric, is then $A$ symmetric?","We are given real matrices $S$ and $A$ . We know that $S$ is symmetric positive definite and that $SA$ is symmetric. Is A necessarily symmetric then? I've figured out that if $A$ is symmetric, then $S$ and $A$ must commute. I've tried finding a $2 \times 2$ and a $3 \times 3$ counterexample, as it seemed to me that this is not generally true, but I couldn't find any.","['matrices', 'linear-algebra', 'linear-transformations', 'symmetric-matrices', 'positive-definite']"
4164263,Compute $\iiint x+y+z$ over the region inside $x^2+y^2+z^2 \le 1$ in the fist octant,"I feel this should be an easy question, but I seem to be struggling with it. So, I started by finding my bounds of integration. In this case, I get $0 \le x \le 1$ , $0 \le y \le \sqrt{1-x^2}$ and $0 \le z \le \sqrt{1-x^2-y^2}$ . Then, upon integrating, I get: \begin{align}
= {} & \int_0^1 \int_0^{\sqrt{1-x^2}}\int_0^{\sqrt{1-x^2-y^2}}(x+y+z) \, dz \, dy \, dx \\[8pt]
= {} &  \int_0^1 \int_0^{\sqrt{1-x^2}}(\sqrt{1-x^2-y^2})(x+y) - \frac{1-x^2-y^2}{2} \, dy \, dx
\end{align} From here the integration got pretty hairy, and using an online calculator the next inner integral with respect to $y$ resulted in imaginary numbers, which seems way too complex for a final answer of $\frac{3\pi}{16}$ . My guess is my bounds of integration are wrong, but I'm not sure why or what the right ones should be. Thanks!","['integration', 'multivariable-calculus', 'multiple-integral']"
4164264,Can the $X$ pentomino and the $2\times 2$ square mutually tile any nontrivial cofinite region?,"It is easy to see that there is no nonempty finite region in the square grid that can be tiled by both the $X$ pentomino and the $2\times 2$ square: if we look at any cell of maximal $y$ -coordinate, a $2\times2$ tiling would force one of its left or right neighbors to be in the region, but the $X$ tiling would force neither to be included. On the other hand, there are infinite regions that both can tile. The entire plane is one example, but there are also regions like the following: Note that this region contains infinitely many cells and infinitely many missing cells. I am interested in whether the complement of a nonempty finite region can ever be tiled by both shapes. After trying several initial candidates, I suspect not, but I haven't found a proof yet.","['polyomino', 'geometry', 'tiling']"
4164279,Find the rational parametrization of this curve,"I need to find the rational parametrization of the complex curve with affine equation $$y^2-6x^2y-3x^4+4x^3y+4x^3=0.$$ I did find a solution, I don't know if  there is a more elegant solution, but this is what I did nonetheless. I took the lines passing through the origin $y=kx$ and intersecting these I find $$x(k)=\frac{(4-6k)+(4i(k-1))^2\sqrt {k-1}}{2k^2}.$$ Now this is obviously not a rational function, but composing the function $k(t)=t^2+1$ rationalizes the square root and it should give a solution to the problem.Is there a better way to solve this?",['algebraic-geometry']
4164289,Each $2\times 5$ rectangle contains $1\times 3$ rectangle,"A $60\times 60$ board is partitioned into rectangles of size $2\times 5$ (or $5\times 2$). Is it true that there always exist another partition into rectangles of size $1\times 3$ (or $3\times 1$) such that any $2\times 5$ (or $5\times 2$) rectangle contains a $1\times 3$ (or $3\times 1)$ rectangle? For the ""simple"" partition into $2\times 5$ rectangles, this is certainly true: use the simple partition into $1\times 3$ rectangle. Another way to partition is to first partition the $60\times 60$ board into $12\times 10$ boards, and then for each such board, put two $2\times 5$ at the top and ten $5\times 2$ below. It is not hard to tile each such $12\times 10$ board with $1\times 3$ (or $3\times 1)$ rectangles so that each $2\times 5$ (or $5\times 2$) contains a $1\times 3$ (or $3\times 1)$ rectangle. (I'm assuming that an $m\times n$ rectangle has $m$ rows and $n$ columns.)",['combinatorics']
4164309,"If $x^3-\frac1{x^3}=108+76\sqrt2$, find $x-\frac1x$","If $x^3-\frac1{x^3}=108+76\sqrt2$ , find $x-\frac1x$ LHS = $(x-\frac1x)(x^2+\frac1{x^2}+1)=(x-\frac1x)((x-\frac1x)^2+3)$ Now, maybe RHS needs to be factorized so that some comparisons can be made, but not able to do so. Or maybe LHS can be written as $(x-\frac1x)^3+3(x-\frac1x)$ . Now, RHS can be broken down into two terms. One could be the cube of one third of the other term, but not able to do this either. Any ideas how to approach such questions?",['algebra-precalculus']
4164344,Definitions of f(x) when they are in the Dirac delta function argument ( δ[ f(x) ] ).,"I edited the question to explore definitions other than this question . I`m trying to simplify $$δ((x^2-a^2)^{1/2})$$ using $$\delta\big(f(x)\big) = \sum_{i}\frac{\delta(x-a_{i})}{\left|{\frac{df}{dx}(a_{i})}\right|}$$ but the derivative in the denominator diverges in the points $a$ and $-a$ . From what I've been reading here on the mathstackexchange, the argument f(x) of the Dirac delta must be continuously differentiable.
The derivative of $$(x^2-a^2)^{1/2}$$ is $$\frac{x}{\sqrt{x^2-a^2}}$$ . If the domain and image of f(x) are real, since, as far as I've learned, the Dirac delta argument cannot be complex, this derivative is discontinuous in the closed interval [-a;a]. Is this argument correct? From this, is it possible to say f(x) is not continuously differentiable and that the function $$δ((x^2-a^2)^{1/2})$$ is undefined?","['dirac-delta', 'function-and-relation-composition', 'real-analysis', 'continuity', 'derivatives']"
4164345,What does being smaller that to a join means in a lattice?,"This sounds like a very naïve question, but I couldn't find a correct argument to prove/disprove it rigorously. Suppose we have a a subset $A$ of a lattice (or any join-semilattice), and some $x\le\bigvee A.$ Dose this means that there is $a\in A$ with $x\le a$ ? The answer is clearly ""Yes"" in a totally ordered set. But unfortunately, I am working in an arbitrary lattice. In case you need, the definition of the join $\bigvee A$ says that: $a\le \bigvee A$ for all $a\in A$ ; if there is some $b$ such that $a\le b$ for all $a\in A,$ then $\bigvee A\le b.$","['well-orders', 'order-theory', 'lattice-orders', 'elementary-set-theory', 'supremum-and-infimum']"
4164466,"Equational identities of $(\mathbb{R}, +, \cdot, \sin)$","Consider the structure $(\mathbb{R},+,\cdot,\sin)$ , where $+$ denotes addition, $\cdot$ denotes multiplication, and $\sin$ denotes the sine function. I conjecture that the equational identities of that structure are generated by the associative and commutative laws of addition and multiplication and the distributive law. Is this conjecture true? In other words, what I am really asking is, does the sine function interact non-trivially with addition and multiplication? Edit: Also, is there an explicit finite basis for the identities of this structure?","['universal-algebra', 'trigonometry']"
4164500,"Find a relation on $\{1, 2, 3\}$ [duplicate]","This question already has answers here : Examples and Counterexamples of Relations which Satisfy Certain Properties (2 answers) Closed 3 years ago . For each of the eight subsets of {reflexive, symmetric, transitive}, find a relation
on $\{1, 2, 3\}$ that has the properties in that subset, but not the properties that are
not in the subset. What I have done This is the same question asked here . I have found the same possible subsets. But for each of them instead of establishing the relationship via ordered pairs, I have defined them with an operation. The possible subsets of {reflexive, symmetric, transitive} are $T_0=\{\varnothing \}, T_1=\{\text{Reflexive}\}, T_2=\{\text{Symmetric}\}, T_3=\{\text{Transitive}\}, T_4=\{\text{Reflexive, Symmetric}\},  T_5=\{\text{Reflexive, Transitive}\}, T_6=\{\text{Symmetric, Transitive}\}, T_7=\{\text{Reflexive, Symmetric, Transitive}\}$ . For $T_{0}$ , define the relation $\rho$ on $\{1, 2, 3\}$ via $a\rho b$ iff $a-b=1$ . For $T_{1}$ ? For $T_{2}$ , define the relation $\rho$ on $\{1, 2, 3\}$ via $a\rho b$ iff $ab$ is even. For $T_{3}$ , define the relation $\rho$ on $\{1, 2, 3\}$ via $a\rho b$ iff $a<b$ . For $T_{4}$ , define the relation $\rho$ on $\{1, 2, 3\}$ via $a\rho b$ iff $|a-b|\leq 1$ . For $T_{5}$ , define the relation $\rho$ on $\{1, 2, 3\}$ via $a\rho b$ iff $a\leq b$ . For $T_{6}$ ? For $T_{7}$ , define the relation $\rho$ on $\{1, 2, 3\}$ via $a\rho b$ iff $a= b$ . For $T_1$ and for $T_6$ it is easy to find the relation via ordered pairs, but defining the relation by means of an operation has not been possible for me, could you please help me? If I completed the list using ordered pairs, I think it would be inelegant.","['elementary-set-theory', 'relations', 'examples-counterexamples']"
4164526,Could anyone please help me solve this differential? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question $$b∙v_Y^2/m-dv_Y/dt=g$$ $v_Y$ here simply means vertical velocity, b is coefficient for drag force, g is the gravity constant. For $v_Y$ , you may simply regard this as $x$ or other variables. I don't think it would affect calculations. I don't know the steps but I know the answer to this differential. I have attached the image of the answer below. Please teach me the steps to the answer. To explain a bit about the image, $V_{yi}$ stands for the initial velocity and $V_t'$ stands for the terminal velocity. here $V_t'=(mg/b)^{.5}$ Thank You in advance.",['ordinary-differential-equations']
4164533,Drawing Conclusions From Only Medians,"At a small high school, the 12th grade is only 20 students. They all take the same five classes (English, Math, Science, History, and Latin) together. At the end of the year each student is assigned a numerical grade between 0 and 100 for each class. Obviously, to calculate their gpa for the year, a student adds their five numbers and then divides by five. The median grade for each class is known, but nothing else. We cannot even assume that the grade distributions in each class are normal. Can one draw any conclusions about the median or mean gpa? If concrete numbers helps, pretend that the median grades are:
English: 83,
Math: 84,
Science: 85,
History: 86,
Latin: 87.",['statistics']
4164636,Question about Hecke correspondence (operator) on modular curve,"I'm reading Rohrlich's article about modular curves in the Cornell-Silverman-Stevens' book Modular forms and Fermat's Last Theorem . I read the proof of the following theorem about description of the Hecke correspondence on the modular curve as a moduli space of elliptic curves: The author defines modular curve by defining its function field first (as a subfield of certain extension of $\mathbb{Q}(t)$ ) and use curve-function field correspondence. Along the proof, here are the lines that I can't fully understand. which follows from the compatibility of reduction at a good place with base extension. In the article, Hecke correspondence on $X_{0}(N)$ is defined as a triple $(X_{0}(N, p), \varphi_{p}, \psi_{p})$ where $\varphi_{p}: X_{0}(N, p) \to X_{0}(N)$ simply corresponds to the inclusion of the function fields (the function field for $X_{0}(N, p)$ is a certain extension of it of $X_{0}(N)$ which is fixed by a smaller subgroup).  At a glance, it seems that the reduced curve $E_{z} / \mathbb{C}$ corresponds to $z \in X_{0}(N)$ is a reduction $E_{\varphi_{p}(z)}/\mathbb{C}$ of some base extension of the curve $E/\mathcal{O}_{z}$ , but I'm not sure what the author actually intended. by the compatibility of reduction with isogenies. I actually don't get why the reduction and isogeny are compatible and why it implies the equation (2).","['number-theory', 'modular-forms', 'algebraic-geometry', 'elliptic-curves']"
4164647,Alternating Riemann Zeta Function (Dirichlet eta function) convergence proof,I am reading a proof on proving the convergence of the alternating Riemann Zeta Function but I cant understand how they went from the 2nd line of math to the 3rd. Can someone please explain why the inequality $\left| \frac{1}{(2n-1)^s}-\frac{1}{(2n)^s}\right|\leq\left|\frac{s}{(2n-1)^{s+1}}\right|$ is true? I understand everything else but why this inequality is true.,"['complex-analysis', 'riemann-zeta']"
4164705,"Perron Frobenius theorem for matrices with entries from $\{-1,0,1\}$","The Perron-Frobenius theorem is a well known theorem for positive symmetric matrices and irreducible non-negative matrices (it gives information about the largest eigenvalue and the existence of a positive/non-negative eigenvector corresponding to it). I am looking for some reading material on the Perron-Frobenius theory for matrices with negative entries also. Particularly, is there anything known for symmetric matrices having entries $\{-1,0,1\}$ ? EDIT: specific results on the existence of Perron like eigenvector, or if there is any nice characterization of the eigenvector(s) corresponding to the eigen value of maximum modulus, would be useful.","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors', 'reference-request']"
