question_id,title,body,tags
3984119,Enumerating separating subcollections of a set,"Let $S$ be a (finite) set, and let $C \subseteq 2^S$ be a collection of subsets of $S$ .  We say that a subcollection $C' \subseteq C$ is separating if for any two elements of $S$ there exists a subset in $C'$ that contains exactly one of the two elements. For example, any collection of cardinality less than $\log_2 \left\lvert S \right\rvert$ cannot be separating, while any collection of cardinality greater than $2^{\left\lvert S \right\rvert - 1}$ is separating. Is there an efficient algorithm to construct all separating subcollections given $S$ and $C$ ? I am particularly interested in finding separating subcollections of small cardinality.  Perhaps this question is equivalent to some other more well-known problem?","['combinatorial-designs', 'boolean-algebra', 'combinatorics', 'discrete-mathematics', 'algorithms']"
3984147,Ito's Lemma: from $df$ to $df^2$ to get $\operatorname{Var}[df]$,"Consider the stochastic process \begin{align*}
dX_t&=\mu_t dt+\sqrt{Y_t}X_t dB_t \\
dY_t&=m_t dt+s\sqrt{Y_t} dW_t
\end{align*} where $dB_tdW_t=\rho dt$ , $s$ is a constant and $\mu_t$ and $m_t$ are well-behaved drifts. According to Ito's formula: \begin{align*}
df(X_t,Y_t) &= \left(\frac{1}{2}Y_tX_t^2f_{xx}(X_t,Y_t)+\rho s Y_tX_t f_{xy}(X_t,Y_t)+\frac{1}{2}s^2Y_tf_{yy}(X_t,Y_t)+\mu_t f_x(X_t,Y_t)+m_t f_y(X_t,Y_t)\right)dt \\
&\;\;\;\;\; +\sqrt{Y_t} X_t f_{x}(X_t,Y_t)dB_t+s\sqrt{Y_t} f_{y}(X_t,Y_t)dW_t.
\end{align*} My question: Is the following assertion correct? \begin{align*}
df(X_t,Y_t)^2 &= \bigg( Y_t X_t^2f_{x}(X_t,Y_t)^2+2\rho s X_tY_t f_{x} (X_t,Y_t)f_y(X_t,Y_t)+s^2 Y_t f_y(X_t,Y_t)^2\bigg)\text{d}t
\end{align*} This expression then gives $\operatorname{Var}[df]=E[df^2]-E[df]^2=E[df^2]=df^2$ , because $E[df]$ is of order $dt$ and squaring it makes it negligible whereas $df^2$ is non-random.","['variance', 'stochastic-processes', 'stochastic-differential-equations', 'probability-theory', 'stochastic-calculus']"
3984229,Determinant of associated real matrix from a complex one,"Given $X\in \operatorname{GL}(n, \mathbb{C}) $ , let $X_r\in \operatorname{GL}(2n, \mathbb{R}) $ be the real matrix obtained by substituting to each complex entry $a+ib$ the matrix $\begin{pmatrix} a & & - b \\ b & & a \end{pmatrix}$ .
Does someone can give me the details of the proof that $\det(X_r) =|\det(X)|^2$ ?
The only thing I have tried is to use the fact that the determinant is a wedge product but it seems not work.","['matrices', 'linear-algebra', 'lie-groups', 'block-matrices']"
3984307,Find $\int_0^1\ln\left(\frac{1+x}{1-x}\right)~dx$ without resorting to series,"Find $$\int_0^1\ln\left(\frac{1+x}{1-x}\right)~dx$$ Firstly, note that the upper limit makes this an improper integral. The method I employed against this problem was integration by parts, as shown below: $$\int_0^1\ln\left(\frac{1+x}{1-x}\right)~dx=2\int_0^1\text{artanh} x ~dx=2\left[x~\text{artanh}~x+\frac{1}{2}\ln\lvert 1-x^2\rvert\right]_0^1$$ However, I am unsure how to find the value that this converges to. I then tried to find a series representation of the value of the integral using the Maclaurin expansion of $\text{artanh}~x$ , which was successful; I found that the value of the integral is $$2\sum_{r=0}^\infty\frac{(2r)!}{(2r+2)!}=2\sum_{r=0}^\infty\left(\frac{1}{2r+1}-\frac{1}{2r+2}\right)=2\sum_{r=0}^\infty\frac{(-1)^r}{r+1}$$ which I know is $2\ln2$ , which is the answer.
However, I would like to know how to use a regular method of integration such as my first method and still calculate the limit it converges to. Thank you for your help.","['integration', 'definite-integrals', 'real-analysis', 'calculus', 'limits']"
3984368,Trying to understand the total derivative/differential.,"I'm asking this question because I'm having problems understanting the definition of differential/total derivative in multivariable calculus, and in order to improve my understanding of it, I want to make sure I have the right intuition behind the definition. Let $F:\mathbb{R}^n\to\mathbb{R}^m$ be a function and $a\in\mathbb{R}^n$ . We say that $F$ is differentiable at $a$ iff there exists a linear transformation $L:\mathbb{R}^n\to\mathbb{R}^m$ such that $$\lim_{x\to a}\frac{\|F(x)-(L(x-a)+F(a))\|_m}{\|x-a\|_n}=0.$$ In this case $L$ will be called the differential of $F$ at $a$ (as it is later proven that this linear transformation is unique) and will be denoted $DF_a$ . As my proffesor explained, the differential is the best linear aproximation there is to $F$ close to $a$ . Now, I understand that $G:\mathbb{R}^n\to\mathbb{R}^m$ defined as $G(x)=DF_a(x-a)+F(a)$ is moving the graph of the function $DF_a$ ( $Gr(DF_a):=\{(x_1,...,x_n,DF_a^1(x_1,...,x_n),...,DF_a^m(x_1,...,x_n))|x_i\in\mathbb{R},i=1,...,n\}$ , where each $DF_a^i:\mathbb{R}^n\to\mathbb{R}$ is the $i$ -th component function of $DF_a$ ), to the point $(a_1,...,a_n,F_1(a_1,...,a_n),...,F_m(a_1,...,a_n))$ , and in this sense $G(x)$ approaches $F(x)$ as $x$ goes to $a$ . This is where the confusion starts (please correct me in anything if I'm wrong). How is it justified that $G$ approaches $F$ as $x$ goes to $a$ ? Is it true that $\lim_{x\to a}\|F(x)-G(x)\|_m=0$ ? If so, why is that not the definition? How does dividing over something that goes to zero while preserving the existence and value of the limit guarantee that $G$ is the best linear approximation? As I've heard before, it is to be understood that $\|F(x)-G(x)\|_m$ goes to zero faster than $\|x-a\|_n$ does, as $x$ approaches $a$ , but once again, how is this justified? I'm sorry if this is too much or if it seems as if I have not worked as much in trying to learn this, but I really have been trying a lot lately and I'm having a hard time in doing so. Any help is greatly thanked for. Greetings.","['limits', 'multivariable-calculus', 'derivatives', 'intuition']"
3984378,The course of the function $f(x) = \sin^3 (x) + \cos^3 (x)$. [closed],"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 3 years ago . Improve this question First, sorry for my english, hope you understand. so I'm examining the course of this function $f(x) = \sin^3 (x) + \cos^3 (x)$ where I need to find everything I can about this function (e.g. definition field, range of values, inflection points, asymptots, global and local max/min, etc.). Now I'm struggling with 3 tasks. Prove there are no limits in $+\infty$ by showing there are different limits of $f(\pi/2+2k\pi)$ and $f(2k\pi)$ where $k$ goes to $+\infty$ . Is this right? Are the results of the limits different? How to get inflection points? I know I get them by solving the 2nd derivative of $f(x)$ which I think I have but I don't know how to get the exact values.
graph and inflex points Find intersection of $f(x)$ with axes. I got to the point where I have two equations and I need to prove why is it like that. Thank you in advance edit: Thank you all for helping me with this problem. I've used @Robert Lee solution in the end.","['limits', 'calculus', 'graphing-functions']"
3984396,"Why is $\operatorname{Hom}_\Bbb{Z}(A,A)$ a ring?","I am trying to prove and understand why the homomorphisms from an abelian group A to itself $\operatorname{Hom}_\Bbb{Z}(A,A)$ is a ring. Is my reasoning correct in the following proof? Show the homomorphisms from an abelian group A to itself $\text{Hom}_{\mathbb{Z}}(A,A)$ form a ring. For two homomorphisms $\delta_1\ \colon V\to V$ and $\delta_2\ \colon V\to V$ $\in \text{Hom}_{\mathbb{Z}}(A,A)$ define their additive and multiplicative binary operataions as $[\delta_1+\delta_2](x)=\delta_1(x)+\delta_2(x)$ and $\delta_1 * \delta_2=\delta_1 \circ \delta_2$ respectively. Additive Identity This would be the zero map so fix $\delta_1\in \text{Hom}_\mathbb{Z}(A,A)$ then $[\delta_1 + 0]=\delta_1$ $[0+\delta_1]=\delta_1$ Additive Associativity Addition of functions is associative. Inverse Suppose we have a homomorphism $\delta$ in $\text{Hom}_{\mathbb{Z}}(A,A)$ . Define another homomorphism from A to itself as $\delta^{-1}(v):=(\delta(v))^{-1}$ which exists because the image of A under $\delta$ is a subgroup of A (well known theorem of Homomorphisms). Now we must show that the composition of these functions yields the zero function which is the identity of the Abelian group. Let $v$ be an arbitrary element in $A$ . Then $\delta(v)+\delta^{-1}(v)=\delta(v)+\delta(v^{-1})=\delta(v+v^{-1})=\delta(0)=0$ with the last equlaity following from the fact that $\delta$ is a group homomorphism. Multiplicative Associativity Composition of functions is associative. Multiplicative Identity This would be the identity homomorphism so fix $\delta_1 \in \text{Hom}_{\mathbb{Z}}(A,A)$ then $(\delta_1 + 1)=(\delta_1 \circ 1)=\delta_1$ . Similarly, $(1 + \delta_1)=(1 \circ \delta_1)=\delta_1$ Multiplicative Distributive Laws Fix three homomorphisms $\delta_1, \delta_2, \delta_3$ . $\delta_1 * (\delta_2 + \delta_3) (x)= \delta_1 \circ (\delta_2 + \delta_3) (x) =\delta_1((\delta_2 + \delta_3)(x))=\delta_1(\delta_2(x) + \delta_3(x))=
\delta_1(\delta_2(x)) + \delta_1(\delta_3(x))=(\delta_1*\delta_2 + \delta_1*\delta_3)$ Using the fact that $\delta_1$ is a homomorphism.","['category-theory', 'ring-theory', 'abstract-algebra', 'linear-algebra', 'group-theory']"
3984403,How can we derive the dot product from the covariance of two random vectors?,"I'm trying to understand a geometric interpretation of covariance as explained in this lecture pdf , which states: If X, Y are two random variables of zero mean, then the covariance Cov[XY ] = E[X · Y ] is the dot product of X and Y. The standard deviation of X is the length of X. The correlation is the cosine of the angle between the two vectors. I can understand how, given a zero mean, the standard deviation of X is the length of X. What I don't understand is how the the covariance of X and Y is equal to it's dot product. Specifically, the covariance is defined as: $Cov(X,Y) = E[(X - \mu_X) (Y - \mu_Y)]$ Which with we can write as: $Cov(X,Y) = \sum\limits_{i}\sum\limits_{j} p_{ij} (x- \mu_X) (y-\mu_Y)$ And since for two centered random variables the means are zero, and the probability is uniform, we can rewrite the formula as: $Cov(X,Y) = \dfrac{1}{N} \sum\limits_{i}\sum\limits_{j} x y$ (Eqn 1) The dot product, in contrast consists of a single summation: $dot(X,Y) = \sum\limits_{i} x_i y_i$ (Eqn 2) Note that the $\frac{1}{N}$ factors out of Eqn 1 when I divide by the standard deviations of X and Y. So I almost can derive the dot product from the covariance, but the covariance still contains a double summation and the dot product has a single element-wise summation. What am I missing? UPDATE I don't have the answer, but I realize that this formula: $Cov(X,Y) = \sum\limits_{i}\sum\limits_{j} p_{ij} (x- \mu_X) (y-\mu_Y)$ ... where N is the dimension of the vectors, is not equal to Eqn 1: $Cov(X,Y) = \dfrac{1}{N} \sum\limits_{i}\sum\limits_{j} x y$ (Eqn 1) Since if $p_{ij}$ equals $\dfrac{1}{N}$ , it would sum to be greater then one in the double summation. $p_{ij}$ only equals $\dfrac{1}{N}$ when it's a single summation, which maps easily to the dot product. I've posted a new question here to try and address this confusion in isolation, which I hope in turn will clarify this derivation problem.","['inner-products', 'proof-explanation', 'covariance', 'probability']"
3984408,Find all functions $f(x^2-y^2)=f(xy)(f(x)+f(y))$,"Find all functions $f:\mathbb R\to\mathbb R$ such that, for all $x,y\in\mathbb R$ , we have $$f(x^2-y^2)=f(xy)(f(x)+f(y)).$$ Both $f(x)=0$ and $f(x)=1/2$ are solutions. Swapping $x,y$ in the equation implies that $f$ is even. Plugging in $x=y=0$ implies that $f(0)=0$ or $f(0)=1/2$ . If $f(0)=0$ , then plugging in $y=0$ implies that $f(x^2)=0$ . So $f(t)=0$ for $t\geq0$ , and $f$ even means that $f=0$ is the only solution in this case. I'm struggling though with the case $f(0)=1/2$ .","['functional-equations', 'algebra-precalculus']"
3984418,Find the derivative of the function $f(x)=(P(x))^a$ where $a\in\Bbb{R}$ and $P(x)$ is a real polynomial with no real roots,"I need help to clarify this statement. I found it in a friend's old homework notebook and I think that the answer is $$f'(x)=aP'(x)\cdot(P(x))^{a-1}$$ However, I just don't get why the fact of $P(x)$ having no real roots is relevant. Is there a way to state the answer using only $P(x)$ instead of $P'(x)$ ? Am I missing something? I think his teacher was trying to make this problem look more complex than it is, but I just wanted to check if maybe I am the one who is mistaken.","['algebra-precalculus', 'derivatives', 'polynomials']"
3984436,How to calculate a combinatorial sum,"I have a combinatorial sum in hand which I suspect equals zero. But I do not know how to prove it. Can you guys help me? (I am even not sure if this is a hard question or not) Is $$
\sum_{k=0}^n (-1)^k k^{n-1} \left(\begin{array}{l}
n \\
k
\end{array}\right) = 0
$$ ?","['combinations', 'combinatorics', 'combinatorial-proofs']"
3984452,Interesting Theorems on Finitely Generated Abelian Groups?,"First time teaching algebraic topology, probably gonna be related to most of my questions on here for a while. I was wondering if anyone knows of particularly interesting theorems or examples from the theory of finitely generated abelian groups, beyond the standard auxiliary stuff and the classification/primary decomposition.  By auxiliary I mean stuff like the basic results on subgroups, quotients, torsion/rank uniqueness and change of base.  Doesn't have to be something that can quickly be PROVEN from first principles, but as long as it's something digestible and believable (or even better, unbelievable).  Perhaps certain actions on manifolds or trees or the like?  Something for topology-minded folks rather than algebra-minded would be ideal, but whatever you think is cool I'd love to hear about! It could also be surprising internal/structural features, or results about their automorphism groups. Thanks, hope someone finds this topic interesting!","['geometric-group-theory', 'group-theory', 'group-actions', 'abelian-groups', 'algebraic-topology']"
3984498,"How to solve $\int_{0}^{\frac{\pi}{2}} \frac{2304\cos x}{(\cos 4x-8\cos 2x+15)^2} \,dx$","\begin{equation}
\int_{0}^{\frac{\pi}{2}} \frac{2304\cos x}{(\cos 4x-8\cos 2x+15)^2} \,dx
\end{equation} This is a MCQ question and there are 5 options to choose which are ""A. $2\sqrt{3}\pi+9\ln 3$ , B. $2\sqrt{7}\pi+8\ln 3$ , C. $2\sqrt{3}\pi+8\ln 3$ , D. $2\sqrt{2}\pi+2\ln 3$ , E.Other solution."" How do you solve this integral? This appears in MCQ test, so I think there should be a trick to solve this without using too much force. The test have 30 questions and 2 hours, so each question should be finished under 4mn ( I doubted it though). Here is my solution that I spend around 3h to solve it. ( Sorry for not using latex and sorry for the inconveniences ) Photo I appreciate any solution tricks. Thanks!",['integration']
3984526,How to get the coefficients of Taylor series in several variables when sums are grouped by order of partial derivatives,"Assume $f$ is real analytic in several variables. The Taylor series of $f$ can be represented in two forms (cf. e.g., wikipedia ). The first form is power series form: $$T(x_1,\dots,x_d)=\sum\limits_{(n_1,\dots,n_d)\in\Bbb N^d}\frac{1}{n_1!\cdots n_d!}\frac{\partial^{n_1+\cdots+n_d}f(a_1,\dots,a_d)}{\partial x_1^{n_1}\cdots\partial x_d^{n_d}}(x_1-a_1)^{n_1}\cdots(x_d-a_d)^{n_d}.\tag{1}$$ The second form is grouping summands by the order (number of variables) of partial derivatives: $$T(x_1,\dots,x_d)=f(a_1,\dots,a_d)+\sum\limits_{j=1}^d\frac{\partial f(a_1,\dots,a_d)}{\partial x_j}(x_j-a_j)\\+\frac{1}{2!}\sum\limits_{j=1}^d\sum\limits_{k=1}^d\frac{\partial^2f(a_1,\dots,a_d)}{\partial x_j\partial x_k}(x_j-a_j)(x_k-a_k)+\cdots\\+\frac{1}{n!}\sum\limits_{i_1=1}^d\cdots\sum\limits_{i_n=1}^d\frac{\partial^n f(a_1,\dots,a_d)}{\partial x_{i_1}\cdots\partial x_{i_n}}(x_{i_1}-a_{i_1})\cdots(x_{i_n}-a_{i_n})+\cdots.\tag{2}$$ I can derive the coefficients in power series form $(1)$ by differentiating term-by-term. But I encountered difficulty in deriving coefficients in the second form. Following is my attempt. The general $n$ -th term in the second form is like this (please correct me if I am wrong): $$\sum\limits_{i_1=1}^d\cdots\sum\limits_{i_n=1}^d c_{(i_1,\cdots,i_n)}(x_{i_1}-a_{i_1})\cdots(x_{i_n}-a_{i_n}).\tag{3}$$ $i_1,\dots,i_n$ can take duplicated values in $1..d$ . Assuming in $n$ indices $i_1,\dots,i_n$ , there are $n_1\ 1$ 's, $n_2\ 2$ 's, $\cdots$ , $n_d\ d$ 's, we have actually constructed a histogram mapping $\varphi: (i_1,\cdots,i_n)\to (n_1,\dots,n_d)$ with the restriction that $n_1+\cdots+n_d=n$ . Based on different histogram $d$ -tuple $(n_1,\dots,n_d)$ , we can further subgroup the summands in sum $(3)$ as follows. For a specific histogram distribution $(n_1,\dots,n_d)$ , by merging same factors, the polynomial part of $(3)$ becomes $(x_1-a_1)^{n_1}\cdots(x_d-a_d)^{n_d}$ . So, $(3)$ can be rewritten in accordance with this subgrouping as $$\sum\limits_{\begin{array}{c}(n_1,\dots,n_d)\in\Bbb N^d\\n_1+\cdots+n_d=n\end{array}}c_{(n_1,\dots,n_d)}(x_1-a_1)^{n_1}\cdots(x_d-a_d)^{n_d}.$$ This looks much like the power series form. If we take partial derivative $\frac{\partial^{n}f}{\partial x_1^{n_1}\cdots\partial x_d^{n_d}}$ and plugging in $(a_1,\dots,a_d)$ , all other terms not corresponding to this $(n_1,\dots,n_d)$ tuple will vanish either because the exponent is less or because the factor is evaluated to zero, and not surprisingly we get the coefficient in exactly the same form as ones in the power series form: $c_{(n_1,\dots,n_d)}=\frac{1}{n_1!\cdots n_d!}\frac{\partial^n f(a_1,\dots,a_d)}{\partial x_1^{n_1}\cdots\partial x_d^{n_d}}=\frac{1}{n_1!\cdots n_d!}\frac{\partial^n f(a_1,\dots,a_d)}{\partial x_{i_1}\cdots\partial x_{i_n}}$ since $f$ is $C^\infty$ and therefore changing the order in which variables are taken does not change the result of mixed partial derivative. But the denominator before the mixed partial derivative is supposed to be $n!$ from the ground truth in $(2)$ as opposed to $n_1!\cdots n_d!$ . I know I must have committed some error by messing up something during the derivation, but I cannot spot it. So, could you please help me figure out the error so that I can correct it and eventually derive the coefficients in the second form having expected $n!$ in the denominator. Thank you.","['partial-derivative', 'multivariable-calculus', 'combinatorics', 'taylor-expansion']"
3984589,Combinatorics and pigeon hole principle,"How many subsets of $\{1,2,3,\dots,10\}$ , each containing at least 3 and at most 5 elements, must be selected in order to ensure that the selection contains three subsets $A , B , C\subset \{1,2,\dots,10\}$ such that sums of elements of each of the subsets are equal? 35 sums are possible, from minimum sum 6 (for the subset $\{1,2,3\}$ ) to maximum sum 40 (for the subset $\{6,7,8,9,10\}$ ). So by the Pigeonhole Principle, at least 71 sets must be chosen to ensure there are three with equal sums. But in the book where I found this question, the answer is given as 67. Where am I wrong?","['permutations', 'pigeonhole-principle', 'combinations', 'combinatorics']"
3984600,How to find the limit of $\frac{\frac{1 - 2n}{n}}{5 + 3^{-n}}$,"I have an intuition that the limit is $\frac{-2}{5}$ , by noting that $\frac{1}{n} \to 0$ and $3^{-n} \to 0$ as $n \to \infty$ . But I do not know how to show this formally, i.e. we can always find a starting index $n$ such that $\left| \frac{2}{5} - \frac{\frac{1 - 2n}{n}}{5 + 3^{-n}} \right| < \epsilon$ for a given epsilon. By manipulating the absolute value I managed to get it to form $\left| \frac{-2n3^{-n} - 5}{5n3^{-n} + 25n}\right|$ . I suppose the squeeze theorem is one way to go, but as of now I do not know what my two other sequences should be.","['limits', 'sequences-and-series', 'real-analysis']"
3984603,Step by step solution of an Indefinite Integral,"I had this on my test, I just couldn't get it, after solving it a lot. $$\int\left\{\frac{\log x-1}{1+(\log x)^2}\right\}^2\,dx=\,?$$ I have tried substituting $\log(x)=t$ and then $$
\int \frac{(t-1)^{2} e^{t} d t}{\left(t^{2}+1\right)^{2}}=\int\left\{\frac{\left(t^{2}+1\right)}{\left(t^{2}+1\right)^{2}}-\frac{2 t}{\left(t^{2}+1\right)^{2}}\right\} e^{t} d t
$$ and then, applying the by parts rule, $$
=e^{t}\left(\tan ^{-1} t-\frac{1}{t^{2}+1}\right)-\int e^{t}\left(\tan ^{-1} t-\frac{1}{t^{2}+1}\right) d t
$$ after that I can't get ahead. P.S.- All Curly brackets used here are normal brackets, not the ones used in fractional part. Thanks to @ParclyTaxel for pointing it out.","['integration', 'indefinite-integrals', 'calculus', 'substitution']"
3984613,"What is the probability of spelling out ""miss"" in ""mississippi""?","Suppose that the word ""mississippi"" is written on a piece of paper. If we cut this piece of paper into 11 smaller pieces of paper, each containing exactly one letter, and then place these letters in a bag, what is the probability that if we select four numbers at random with no replacement, the first letter will be ""m"", the second letter ""i"", the third letter ""s"", and the fourth letter is ""s"" (so that we form the word ""miss"")? At this point, I know that computing ${}_{11}P_4$ will give us the total number of ways we can form $4$ -letter words. Would I have to divide ${}_{11}P_4$ by $4!\cdot 4! \cdot 2!$ to get the total number of ways to uniquely spell out ""miss"". Then, I could find the probability by dividing $1$ by the total number of ways found?","['permutations', 'combinatorics', 'probability']"
3984671,Sampling from an arbitrary distribution on Polish spaces,"Let $U\sim \text{Unif}(0,1)$ , and let $\mu \in \mathcal{P}(\mathbb{R})$ be an arbitrary probability measure on $\mathbb{R}$ . Then from $\mu$ , we can derive an associated CDF $F(x) = \mu((-\infty,x])$ . We consider the following inverse of $F$ : $$F^{-1}(u) = \inf\{x \in \mathbb{R}: F(x) \geq u\}.$$ Then it's easy to show that $F^{-1}(U)\sim \mu$ . In other words, $F^{-1}(U)$ is a sample of a random variable distributed like $\mu$ . Is there a similar construction when we now assume that $\mu \in \mathcal{P}(S)$ is a probability distribution on some arbitrary Polish space $S$ ? That is, does there exist a random element $X$ taking values in some other Polish space $T$ and a mapping $\psi: \mathcal{P}(S) \to (T \to S)$ from probability measures on $S$ to measurable functions from $T$ to $S$ such that $\psi(\mu)(X)\sim \mu$ for all $\mu \in \mathcal{P}(S)$ ? I would be interested in proofs that such a mapping exists (or even more interesting, doesn't), but it would be absolutely amazing if the proof was constructive. I'd also be interested in references to the literature where this problem may have already been addressed. Thank you!","['probability-distributions', 'coupling', 'sampling', 'probability-theory', 'probability']"
3984672,Whats the relation between the Veronese embedding and the isomorphism $\operatorname{Proj}A\cong\operatorname{Proj}A^{(d)}$?,"In the book Introduction to Schemes , G. Ellingsrud and J. Ottem show (in section 6.7) that if $A$ is a graded ring and $$A^{(d)}:=\bigoplus_{n\geq 0} A_{nd}$$ then the natural inclusion $A^{(d)}\to A$ induces an isomorphism $\operatorname{Proj}A\cong\operatorname{Proj}A^{(d)}$ . They call it the Veronese embedding . While I understand what they did, I don't understand why they called it this way. Isn't the Veronese embedding the inclusion $\operatorname{Proj} k[x_0,\dotsc,x_n]_d \to \mathbb{P}_k^{N-1}$ , where $N=\binom{n+d}{d}$ ?",['algebraic-geometry']
3984693,Proper definition of a function,"I'm sorry if this is a pedantic question, but I want to be sure I'm using terminology correctly. Without a second thought, I would make statements of the following form: ""Consider the function $f: A \to B$ defined by $f(a) = b$ ."" This isn't fully correct, however, because the full definition of the function $f$ should be the domain, codomain, and the rule. Without one, the definition is ambiguous. Am I incorrect on this, or is this a commonly accepted shorthand? Would it make more sense to say ""consider the function $f: A \to B$ given by"" or ""governed by""?","['definition', 'functions']"
3984716,Combinatorics - prove white can always force a win or draw in double chess,"The game of double chess is played like regular chess, except
each player makes two moves in their turn (white plays twice,
then black plays twice, and so on). Show that white can always
win or draw. Credit: Olympiad Combinatorics - Pranav Sriram EDIT:
I tried a few thing's like finding the specific strategy, but nothing seems to work. I dont know how to approach this question.","['contest-math', 'chessboard', 'combinatorics', 'combinatorial-game-theory']"
3984740,In how many ways can $6$ different candies be divided between three children?,"I am the freshman in Computer Science at university, and currently struggling with some tasks from the Combinatorics part of the Discrete Math class. Any help would be appreciated. In how many ways can $6$ different candies be divided between three children? My vision of solving this problem: For every candy there is $3$ children to give it to $\implies 3^6$ possible ways of candy distribution OR Each child can get from $0$ to $6$ candies: $\implies 7$ possible distributions $\implies 7^3$ possible ways of candy distribution between the $3$ children? But I feel like I am missing something which leads to the duality of my solutions. (Probably that the candies are different?) And I don't understand how to count that in.","['combinatorics', 'discrete-mathematics']"
3984742,"We throw 2 fair dice together 6 times. What is the probability of getting 3 ""doubles"", i.e. 3 times the same number in both dice?","We throw 2 fair dice together 6 times. What is the probability of getting 3 ""doubles"", i.e. 3 times the same number in both dice? My attempt:
For the first draw of 2 dice together, the sample space has 36 possible outcomes and the ones with the same number in both dice are $(1,1), (2,2)...(6,6)$ so a total of 6. The probability for one throw is $P_1 = \frac {6}{36} = \frac {1}{6}$ Therefore for 2 such throws, the probability to get doubles is $(\frac {1}{6})^2$ . The probability to get doubles in 3 out of 6 throws is $(\frac {1}{6})^3*(\frac {5}{6})^3$ ?? Is this correct? or maybe $\frac {6^3}{36^6}$ ? Thank you!","['combinatorics', 'probability']"
3984800,"Let $\exp( i u X_{t}+\frac{1}{2}u^{2}t)$ be a local martingale for any $u$, show that $X_{t}$ is a Brownian motion","Let $X$ be an $\mathcal{F}_{t},\; t\geq 0$ , adapted a.s. continuous process with $X_{0}=0$ and $M^{u,X}_{t}:=\exp( i u X_{t}+\frac{1}{2}u^{2}t)$ be a local martingale for any $u$ , show that $X_{t}$ is a $\mathcal{F}_{t}$ -Brownian motion. My idea: We need to show that $\operatorname{cov}(X_{t},X_{s})=t \land s$ and that $X$ is a Gaussian process. Let $0 \leq t_{0} < ... < t_{n} <\infty$ . I know that for $T > 0, \; u \in \mathbb R$ the process $M_{t\land T}^{u,X}=\exp( i u X_{t\land T}+\frac{1}{2}u^{2}(t\land T))$ is a local martingale too. Furthermore, $\lvert M^{u,X}_{t\land T}\rvert\leq \exp(\frac{1}{2}u^{2}T)< \infty$ . Therefore, $(M_{t\land T}^{u,X})_{t\geq 0}$ is a martingale. By Doob's optional sampling Theorem, we obtain for $t < T$ , $$E[M_{t}^{u,X}]=E[M_{t\land T}^{u,X}]=E[M_{0\land T}^{u,X}]=E[M_{0}^{u,X}]=1$$ This would then imply that $$ E[\exp(iuX_{t})]=\exp(-\frac{u^{2}t}{2})$$ which is the characteristic function of a normal random variable $\mathcal{N}(0,t)$ evaluated at $u$ . But I am struggling to do the same for the random vector $(X_{t_{0}},...,X_{t_{n}})$ and $0 \leq t_{0} < ... < t_{n}< \infty$ . I think I need to use induction and the tower property to reduce it all to the random variable $X_{t_{0}}$ , but that seems less efficient. On the issue of showing $ \operatorname{cov}(X_{t},X_{s})= t\land s$ , I am completely stuck. Maybe I can set $u:=\log(E[X_{t}X_{s}])$ and by assumption: $ M^{u,X}_{r}:=\exp( i \log(E[X_{t}X_{s}]) X_{r}+\frac{1}{2}\log(2E[X_{t}X_{s}])r)$ is a local martingale We then have: $$ E[\exp( i \log(E[X_{t}X_{s}]) X_{r}]=\exp(-\frac{\log(E[X_{t}X_{s}])r}{2})$$ Then $$ E[\exp( i \log(E[X_{t}X_{s}]) X_{r}]=E[X_{t}X_{s}]^{-\frac{r}{2}}$$ I do not see how I get to $t\land s $ from here. Any ideas? Or is something simply missing in the question's assumptions?","['stochastic-analysis', 'functional-analysis', 'brownian-motion', 'probability-theory', 'stochastic-calculus']"
3984824,Calculate $x^3 + \frac{1}{x^3}$,"Question $x^2 + \frac{1}{x^2}=34$ and $x$ is a natural number. Find the value of $x^3 +  \frac{1}{x^3}$ and choose the correct answer from the following options: 198 216 200 186 What I have did yet I tried to find the value of $x + \frac{1}{x}$ . Here are my steps to do so: $$x^2 + \frac{1}{x^2} = 34$$ $$\text{Since}, (x+\frac{1}{x})^2 = x^2 + 2 + \frac{1}{x^2}$$ $$\Rightarrow (x+\frac{1}{x})^2-2=34$$ $$\Rightarrow (x+\frac{1}{x})^2=34+2 = 36$$ $$\Rightarrow x+\frac{1}{x}=\sqrt{36}=6$$ I have calculated the value of $x+\frac{1}{x}$ is $6$ . I do not know what to do next.",['algebra-precalculus']
3984880,Intuitive explanation for orthogonality relations between matrix elements of irreps,"It is well known that the matrix elements of complex irreducible representations of a finite group $G$ are orthogonal, i.e. $$
\langle T_{ij}^\lambda, T_{kl}^{\rho} \rangle = \frac{1}{|G|} \sum_{g\in G} T_{ij}^\lambda(g) T_{kl}^\rho(g)^* = \frac{\delta_{ik} \delta_{jl} \delta_{\lambda \rho}}{d_\lambda},
$$ where $T_{ij}^\lambda$ is the $(i,j)$ matrix element of irreducible representation (irrep) indexed by $\lambda$ , $d_\lambda$ is the dimension of this irrep, and $\delta_{\lambda \rho}$ means that $\lambda$ and $\rho$ correspond to an equivalent irrep. The above can be strengthened, because in fact the matrix elements of irreps form a basis for the space of functions on $G$ . As a consequence, we can expand a function on $G$ as a linear combination of matrix elements, giving us some kind of generalization of classical Fourier theory. I want to explain this idea to an audience that knows classical Fourier theory but hardly anything about representations. My goal is to keep it high level and intuitive. I have looked at some proofs of the statement above, but they all seem fairly long, reliant on other results, and it's hard to distill the ""essence"" of why the above must be true. I would appreciate a short and intuitive explanation of the result above, that conveys the gist to an audience who does not yet know much representation theory.","['representation-theory', 'group-theory', 'fourier-analysis']"
3984896,Intersection of two homogeneous linear second-order recurrence sequences with constant coefficients,"Say we are given two sequences $u_n$ and $v_n$ , defined by the following homogeneous second-order recurrence relations: $$u_n=3u_{n-1}-u_{n-2}$$ $$v_n=3av_{n-1}-v_{n-2}$$ where $a$ is a constant and $u_0=u_1=1$ . The first recurrence relation implies $u_n$ is a subset of the Fibonacci sequence. Is there an algorithm for determining the intersection of these two sequences? I know that if $a=1,v_0=2,v_1=5$ then this would be the intersection of subsequences of the Fibonacci and Pell numbers, and this has been shown to be finite. Is there a more general algorithm?","['fibonacci-numbers', 'number-theory', 'recurrence-relations', 'discrete-mathematics', 'sequences-and-series']"
3984909,Find the value of $k$ in $\frac{b+c}{\sqrt{a}}+\frac{c+a}{\sqrt{b}}+\frac{a+b}{\sqrt{c}} \ge \sqrt{a}+\sqrt{b}+\sqrt{c}+k$,It is also given that $abc = 1$ . I used AM-GM inequality to reach till $\frac{b+c}{\sqrt{a}}+\frac{c+a}{\sqrt{b}}+\frac{a+b}{\sqrt{c}} \ge \sqrt{\frac{bc}{a}} + \sqrt{\frac{ac}{b}} + \sqrt{\frac{ab}{c}} $ How to go further,"['contest-math', 'algebra-precalculus', 'a.m.-g.m.-inequality', 'inequality']"
3984937,Prove difference between a set and the union of other two sets,"This question asked which of the two following formulas was always right whilst the other one was sometimes wrong, and what was the necessary and sufficient condition for the formula which was sometimes incorrect to be always right: ( i ) $\; A - (B - C) = (A - B) \cup C $ ( ii ) $\; A - (B \cup C) = (A - B) - C $ Using Venn diagrams, I guessed ( ii ) was always right, and ( i ) would be wrong if $A$ and $C$ are disjoint. Thus, the necessary and sufficient condition for ( i ) to be correct was $A$ and $C$ contain some elements in common. My questions are: Were my guesses correct? How could I prove the two formulas above using words/expressions rather than using Venn diagrams? Thanks a lot.",['elementary-set-theory']
3984946,"What does ""most"" mean in ""the most general solution of $\tan\theta=-1$ and $\cos\theta=\frac{1}{\sqrt2}$""?","What does ""most"" mean here? The most general solution of $\tan\theta = -1$ and $\cos\theta = \dfrac{1}{\sqrt2}$ is: (A) $n\pi +  \dfrac{7\pi}{4}$ , $n\in I$ (B) $n\pi+{(-1)^n} \cdot \dfrac{7\pi}{4}$ , $n\in I$ (C) $2n\pi+\dfrac{7\pi}{4}$ , $n\in I$ (D) none of these",['trigonometry']
3984958,Prove that the integral of a rapidly oscillating function is $0$,"Claim: If $n$ is a nonzero integer, then $$\int_{-\pi/2}^{\pi/2} \exp[{2in(x+\tan x)}] \ \mathrm{d}x = 0$$ Using Euler's identity $e^{ix}= \cos x + i \sin x$ , the fact that $\sin$ is an odd function so integrated over a symmetric integral gives $0$ , and the parity of $\cos$ , one gets $$\int_{0}^{\pi/2} \! \! \cos(2n(x+\tan x)) \ \mathrm{d}x$$ I tried using substitution, but it didn't get me anywhere. Is there maybe a general way to approach these rapidly oscillating integrals to show that the positive and negative areas cancel out?","['calculus', 'definite-integrals', 'trigonometry', 'oscillatory-integral']"
3985006,Estimate volume of Tubes around a smooth hypersurface,"Suppose $\Sigma \subset \mathbb{R}^n$ is a smooth compact hypersurface and the boundary of some set $\Omega$ .  Now set $\Sigma_r:=\{x \in \Omega^c: \inf_{y\in \Sigma} |x-y|\leq r\}$ to be a tube of radius $r$ around the hypersurface. I was wondering if it is possible to somehow estimate the volume of these tubes in terms of the area of $\Sigma$ , like for example $$
vol(\Sigma_r) \leq C_{\Sigma} \cdot r \cdot area(\Sigma). 
$$ It should be possible for small $r$ due to this formula. There is of course a trivial bound like if I take some ball $B_R(y)$ such that $\Sigma \in B_R(y)$ then $
vol(\Sigma_r) \leq vol(B_{R+r}(y)).
$ Is there something better?","['measure-theory', 'differential-geometry']"
3985022,Can any function of two variables be expressed as a linear combination?,"Can any function of two variables be expressed as a linear combination (may be infinite) of a product of two single-variable functions? $f(x,y) = \sum A_n g_n(x) h_n(y)$",['analysis']
3985025,"The Fourier Transform of a Bessel function: $J_a(be^{-x}), a,b>0$","Currently we require the Fourier transform $g(p)$ of $f(x)=J_a(be^{-x}), a,b>0$ $$g(p)=\frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty}e^{ipx} f(x) dx$$ We could not find it in Tables and Handbooks. Mathematica gives it in terms of Gamma functions and Gauss Hyper geometric function. A closer look suggested that Hypergeometric function was actually a hoax. As we were finally interested in $|g(p)|^2$ , it was a pleasant surprise to get $$|g(p)|^2=\frac{1}{2\pi(p^2+a^2)},~~~~~(1)$$ which is independent of $b$ !, even the verification of (1) by Mathematica in analytic mode  for various sets of $a,b,p$ has been time consuming. The question is how to prove (1) and get $g(p)$ by hand? EDIT: By Integrate of  Mathematica, I finally get $$g(p)=\frac{1}{2\sqrt{2\pi}} \left(\frac{2}{b}\right)^{ip} \frac{\Gamma[(a+ip)/2]}{\Gamma[1+(a-ip)/2]}$$ EDIT: Here is the Mathematica out put, see the pre=factor of hypergeometric function is 0. g(p)=1/2 Gamma[
1/2 (a + I p)] ((2^(I p) (1/b^2)^((I p)/2))/
Gamma[1/2 (2 + a - I p)] +
2^-a (-(1/b^2)^(-a/2) + b^
a) HypergeometricPFQRegularized[{1/2 (a + I p)}, {1 + a,
1/2 (2 + a + I p)}, -(b^2/4)])","['integration', 'definite-integrals', 'fourier-transform', 'bessel-functions']"
3985046,"A doubt on the Sobolev space $W_0^{1,p}(\Omega)$","Let $\Omega \subset \Bbb R^d$ be open and $1\leq p<\infty$ . Recall that $W_0^{1,p}(\Omega)$ is the closure of $C_c^\infty(\Omega)$ (smooth function with compact support in $\Omega$ ) in $W^{1,p}(\Omega)$ where $$W^{1,p}(\Omega)= \{u\in L^p(\Omega): \nabla u \in L^p(\Omega) \}$$ is equiped with the norm $$\|u\|^p_{W^{1,p}(\Omega)}= \|u\|^p_{L^p(\Omega)} + \|\nabla u\|^p_{L^p(\Omega)}$$ Define $$W_\Omega= \{u\in W^{1,p}(\Bbb R^d): u =0 ~~a.e.~~ on~~ \Bbb R^d\setminus \Omega \}$$ Clearly $W_\Omega$ is closed subspace of $W^{1,p}(\Bbb R^d)$ and we have $W_0^{1,p}(\Omega)\subset W_\Omega$ Question: Do we have $W_0^{1,p}(\Omega)= W_\Omega$ ? Or is there  a counter example?","['analysis', 'sobolev-spaces', 'trace-map', 'functional-analysis', 'partial-differential-equations']"
3985052,Is there a way to find the value of the largest entry of a hidden vector of integers?,"Consider the following set-up: A person has a vector of with integer entries, which is not known to
you. The person refuses to reveal the vector to you; however, you may
supply another vector with integer entries, and they will tell you the
result of taking the dot product with the hidden vector. A straightforward way to extract the value at the $i$ -th index is to supply the vector $(0, 0, \ldots, 1, \ldots, 0)$ - where the number $1$ occurs at the $i$ -th position. Doing this for all values of $i$ lets you recover the hidden vector in number of steps equal to the length of the vector. Is there a shorter way to just extract the absolute value of the largest entry (or an upper bound for the same) in this set-up? For example, if it is known that all the entries happen to be positive integers, one can supply the vector $(1, 1, \ldots, 1)$ and obtain an upper bound for the largest entry in a single step. Also, if it happens that it is not possible to do this in fewer than $n$ steps (where $n$ is the length of the vector), could you please refer me to a proof for why this is the case? Another variant of the question - is it possible to determine the positions of the negative numbers in the vector in fewer than $n$ steps? Edit: I came across a variant of this question a while ago. The set-up was almost identical, except the vector had positive integer entries instead of just integer entries. The solution involved finding an upper bound $M$ for the largest term (exactly as I have outlined above, and then taking the dot product with the vector $(1, M, M^2, \ldots, M^{n-1})$ to get a number whose base- $M$ expansion was the entries of the vector. I found this really interesting, and I was wondering if this could be generalized to either the full set of integers or the rationals. Since it seems unlikely that one will be able to determine the full vector in fewer than $n$ steps, I am interested in knowing whether it will be possible to determine a simpler property - such as the positions of the negative entries - in fewer than $n$ steps.","['coding-theory', 'linear-algebra', 'vectors', 'information-theory']"
3985054,"For some sequentially compact space $X$, is $X^{\omega_1}$ not sequentially compact?","When we assume the continuum hypothesis, for $X=\{0, 1\}$ , the $\omega_1$ product $X^{\omega_1} = X^{\mathfrak{c}}$ is not sequentially compact while $X$ is sequentially compact.
So the proposition in the title is relative consistent from ZFC.
(the assumpution CH can be weaken to $\mathfrak{s} = \aleph_1$ , where $\mathfrak{s}$ is the splitting number.) Then, is it proved in ZFC that for some sequentially compact space $X$ , $X^{\omega_1}$ is not sequentially compact? I suspect that an cardinal $\kappa$ that has $\aleph_1$ cofinality and that is greater than $\mathfrak{c}$ suffices (with the order topology).","['infinitary-combinatorics', 'general-topology', 'set-theory', 'compactness']"
3985102,Expected area covered by circles inside a square,"$n$ circles of radius $r$ are to be placed randomly inside a square of side length $l$ . What is the expected area the circles will enclose? Edit: As @Mirko suggested, we define random placement of the circles by picking points from a square of side length $l-2r$ concentric to the initial one from a uniform distribution and using each one as a center for a specific circle. The problem I'm facing is that I do not know how to statistically account for overlaps with the circles. I have tried to divide the square into little square ""cells"" of side $2r$ such that each circle can exist in one of the cells and obtained an expression for the probability of $k$ circles to exist with no overlap. However, this approach drastically understates the actual probability as it doesn't account for circles existing in between cells. Any alternative approach would be greatly appreciated.","['statistics', 'geometry']"
3985119,If $X_n$ converges to X in probability then $X_n$ converges to X a.s.,"We know that if $X_n$ converges to X a.s. then $X_n$ converges to X in probability in the space ( Ω, A, P) . But the converse is not true in general . We can construct  such examples on any probability space (Ω, A, P) which has a “non-atomic part”, meaning that, there is an $Ω_0$ ∈ A with P( $Ω_0$ )>0, such that, for any A ∈ A, A ⊂ $Ω_0$ , with P(A)>0 and any 0< δ <1, one has B ∈A, B ⊂A with P(B)= δP(A).
But when the probability space is “purely atomic”, not only do we have no counter-example as above, but indeed, convergence in probability implies a.s. convergence. My question is how can I show that if (Ω, A, P) be a discrete probability space, that is, Ω is a countable set and A consists of all subsets of Ω. Then for random variables $X_n$ , n ≥ 0 and X on (Ω, A, P), $X_n$ converges to X in probability then $X_n$ converges to X a.s.","['measure-theory', 'probability-theory']"
3985132,Linearization of the Derivative Block (MATLAB),"In MATLABs documentation for the derivative block here , it states the following: The exact linearization of the Derivative block is difficult because the dynamic equation for the block is y= $\dot{u}$ , which you cannot represent as a state-space system. However, you can approximate the linearization by adding a pole to the Derivative block to create a transfer function s/(c∗s+1). Can anyone explain what it means to linearize the Derivative block? I thought differentiation was a linear operator, so I don't see why you'd want to linearize it. Thanks in advance!","['linearization', 'derivatives', 'matlab']"
3985199,How to set boundaries when approximating a discrete RV with a normally distributed RV?,"I am approximating a random variable $S_n \sim \text{Bin}(n,0.02)$ with $Z_n=\frac{S_n-\mu n}{\sqrt{np(1-p)}} \sim \mathcal{N}(0,1)$ for high enough $n$ (by central limit theorem). My problem is how to set interval boundaries when calculating probabilities with that approach. For example when calculating the chance of $2$ or more (edited the value, but not important) successes in $100$ trials, what is the interval I would like $S_n$ to be in? Is it $[2,100]$ (meaning $S_n\geq2$ ) or $(1, 100]$ (meaning $S_n>1$ ), or something inbetween like $[1.5, 100]$ ? Calculation for $S_n \in (1,100]$ : $$P(S_n \in (1,100])=1-P(S_n \notin (1,100])=1-P\Big[\frac{S_{100}-0.02\cdot100}{\sqrt{100\cdot0.02\cdot0.98}}\leq \frac{1-0.02\cdot100}{\sqrt{100\cdot0.02\cdot0.98}}\Big]=1-\Phi(-0.71)=\Phi(0.71)\approx0,76$$ Calculation for $S_{100} \in [2,100]$ : $$P(S_{100} \in [2,100])=1-P(S_{100} \notin [2,100])=1-P\Big[\frac{S_{100}-0.02\cdot100}{\sqrt{100\cdot0.02\cdot0.98}}\leq \frac{2-0.02\cdot100}{\sqrt{100\cdot0.02\cdot0.98}}\Big]=1-\Phi(0)=0.5$$ Both results are not exactly good approximations of the exact result $0.6$ calculated with the binominal distribution. I strongly suspect it is due to bad boundary setting.","['statistics', 'central-limit-theorem', 'normal-distribution']"
3985222,Number of grid points outside a rook circuit,"When I draw a circuit on a 8x8 chess board that passes through all squares, no matter how many turns I draw, I always end up with 18 grid points outside my circuit (and 31 grid point inside). It looks to me that I get also constant values for any $n$ x $m$ board. Has anybody ever seen a formula of how many grid points are inside/outside for an $n$ x $n$ board? It looks obvious that there are (n/2 - 1) * (n - 2) points outside if $n$ is even, but I am searching for a proof. If you could point me to some literature, that would be great!","['complex-analysis', 'graph-theory', 'combinatorics']"
3985228,Continued radical of powers of 4 equals 3 [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Can someone explain to me why $$\sqrt{4 + \sqrt{4^2 + \sqrt{4^3 + \sqrt{4^4 + \dots}}}} = 3???$$ I need an answer","['nested-radicals', 'algebra-precalculus']"
3985298,Is the set that contains the empty set {∅} also a subset of all sets?,"I think it isn't; although $\emptyset$ is a subset of every set, $\{\emptyset\}$ is only a subset of a set $A$ if $A$ contains the element $\emptyset$ . So if $A = \{\emptyset\}$ , then both $\emptyset$ and $\{\emptyset\}$ are subsets of A, but if we have a set $B$ where $B = \{0, 1\}$ then only $\emptyset$ is a subset of it. If my reasoning is true, then if there was a set $C$ where $C = \{\emptyset, 0, 1\}$ , would $\{\emptyset\}$ then be a subset of $C$ ? Moreover, is this a valid set (i.e. can you have the empty set as an element of a set that contains other elements)?",['elementary-set-theory']
3985327,"Is ""lines remain lines"" sufficient to characterise linearity in $\mathbb R^2$?","Note : There are other questions on this site similar to this one, but they are either unanswered or have a different definition of ""lines  remain lines"". 3Blue1Brown uses the following intuitive definition of linearity in his video series on linear algebra: A linear transformation is a transformation which fixes the origin and keeps lines straight. I mainly care about this as an intuition for the plane, so I do not really want to generalise it to $\mathbb R^n$ . So my question is, if $f\colon\mathbb R^2\to\mathbb R^2$ is a map such that $f(\boldsymbol 0) = \boldsymbol 0$ , and for all $\boldsymbol a,\boldsymbol b\in\mathbb R^2$ , there exist $\boldsymbol u, \boldsymbol v\in\mathbb R^2$ such that $f(\boldsymbol a+\mathbb R\boldsymbol b) = \boldsymbol u+\mathbb R\boldsymbol v$ , can I show that $f$ is a linear map on $\mathbb R^2$ ?
(here $\boldsymbol a+\mathbb R\boldsymbol b$ denotes the obvious coset $\{\boldsymbol a+t\boldsymbol b:t\in\mathbb R\}$ ). I've been playing around with these two properties a lot, but 2 doesn't seem to be strong enough to allow me to go from statements about lines to statements about individual vectors in an obvious way.","['linear-algebra', 'vector-spaces', 'linear-transformations']"
3985462,Striking applications of linearity of expectation,"Linearity of expectation is a very simple and ""obvious"" statement, but has many non-trivial applications, e.g., to analyze randomized algorithms (for instance, the coupon collector's problem ), or in some proofs where dealing with non-independent random variables would otherwise make any calculation daunting. What are the cleanest, most elegant, or striking applications of the linearity of expectation you've encountered?","['expected-value', 'big-list', 'probability', 'reference-request']"
3985595,"If $\sum\limits_{n=0}^{\infty} a_n\ $ is a conditionally convergent series, then is $\sum\limits_{n=0}^{\infty} |a_n+a_{n+1}|\ $ convergent?","If $\sum\limits_{n=0}^{\infty} a_n\ $ is a conditionally convergent series, then is the sum of the average of consecutive terms necessarily convergent? In other words, is $\sum\limits_{n=0}^{\infty} |a_n+a_{n+1}|\ $ convergent? For the most standard example, the alternating harmonic series $\sum\limits_{n=0}^\infty {(-1)^n\over n+1},$ we get that $\sum\limits_{n=0}^{\infty} |a_n+a_{n+1}|\ $ is half the sum of the reciprocal triangular numbers, which does converge. But does every conditionally convergent (but not necessarily alternating sign) series have this property, or is there one that diverges? I feel like I'm missing some obvious triangle inequality trick, but I just don't see it.","['sequences-and-series', 'real-analysis']"
3985615,"If $f$ is Schwartz, does $f=\mathcal{O}\left(e^{-a|x|^{\epsilon}}\right)$ for some $a,\epsilon>0$?","If $f$ is Schwartz, i.e. $f\in\mathcal{S}\left(\mathbb{R}\right)$ , is it true that $f=\mathcal{O}\left(e^{-a|x|^{\epsilon}}\right)$ for some $a,\epsilon>0$ as $|x|\to\infty$ ? Functions in the Schwartz class, as well as as their derivatives of all orders, decay faster than any negative power of a monomial. I would expect that the answer to the question is yes, but I am not sure how to start thinking about the positivity or negativity of the statement. The standard examples of Schwartz functions, such as $e^{-x^2}$ , $x^2e^{-x^2}$ , and $e^{-x^4}$ all satisfy this bound. We do know that since $f$ is Schwartz, $f=\mathcal{O}\left(\frac{1}{|x|^m}\right)$ , for all $m\in\mathbb{N}$ . Additionally, $e^{-a|x|^{\epsilon}}=\mathcal{O}\left(\frac{1}{|x|^m}\right)$ for all natural $m$ and $a, \epsilon > 0$ . This means that both $\mathcal{S}$ and $e^{-a|x|^{\epsilon}}$ are contained in $\displaystyle \bigcap_{m=1}^\infty \mathcal{O}\left(\frac{1}{|x|^m}\right)$ (again, for all $a, \epsilon > 0$ ). This inclusion at least lets us understand where these functions live a little better, but does not answer the initial question. What is an approach that could shed light on whether the statement in the question holds true? (As a reminder, we say that $f(x)\in \mathcal{O}\left(g(x)\right)$ , or equivalently $f(x)= \mathcal{O}\left(g(x)\right)$ , if, beyond a sufficiently large value of $x$ , $|f|$ is at most a constant times $g$ .)","['schwartz-space', 'exponential-function', 'functional-analysis']"
3985631,Equivalence class for the following relation,"A relation of $\mathbb{R}$ is defined as $a\sim b : a^4-b^2=b^4-a^2$ Show that $\sim$ is equivalence relation (I have done this part) Determine the equivalence class $[-1]_\sim$ Prove or disprove: Every equivalence class in $\mathbb{R}/\sim$ contains exactly 2 real numbers. I am facing difficulty in second and third part of the question. For the second one I think it is $[-1]_\sim=\{-1,1\}$ as $-1\sim x: (-1)^4-x^2 =x^4-(-1)^2 $ so both $-1$ and $1$ are proving the equality. I think the third statement is true but how can I prove it?
Please help.","['elementary-set-theory', 'equivalence-relations', 'abstract-algebra']"
3985670,Find $E[Y\mid X=x]$,"Let $(X,Y)$ $$f(x,y)=\frac{x^2}{4}\hspace{1cm} 0\leq y\leq x\leq 2$$ Find $E[Y\mid X=x]$ I know $$\dfrac{f(x,y)}{f_X(x)}=\frac{x^2/4}{x^3/4}=\frac{1}{x}$$ My question is about the limits of integration, I don't know if they are $[0,2]$ $$E[Y\mid X=x]=\int_0^2y\cdot\frac{1}{x}\,dy$$ or $[0,x]$ $$E[Y\mid X=x]=\int_0^x y\cdot\frac{1}{x}\,dy$$","['expected-value', 'statistics', 'conditional-expectation', 'probability']"
3985689,Limit of a integral,"For $f:[0,1]\rightarrow \mathbb{R}$ continuous, let $a >0.$ \begin{eqnarray*}
L=\lim_{\varepsilon \rightarrow 0}\int_{\varepsilon a}^{\varepsilon b} \frac{f(x)}{x} dx
\end{eqnarray*} Show that $L=f(0)\ln(\frac{b}{a})$ .The first I thought was in mean value theorem for integrals. but  I don't know how can start, if can give me some hint to start. I will be gratefull.","['limits', 'calculus', 'analysis', 'real-analysis']"
3985736,How can I know if an equilibrium is stable in a difference equation?,"Suppose that I have the following equation: $$
y_{t+1} = \frac{2+y_t-y^3_t}{2y_t}
$$ Then $$
\bar{y} = \frac{2+\bar{y} -\bar{y}^3}{2\bar{y}}
$$ And so I obtain these solutions: $$y_1 = -2, \quad y_2 = -1, \quad y_3 = 1 $$ Now how can I know if these equilibria are stable? I believe that it is useful to compute the derivative with respect to the solutions found. Therefore I obtain the derivative: $$
\frac{d}{dy}
=
-\frac{(y^3+1)}{y^2}
$$ For $y_1 = -2$ the derivative becomes: $\frac{7}{4}$ For $y_2 = -1$ the derivative becomes: $0$ For $y_3 = 1$ the derivative becomes: $-2$ So how can I know if the equilibria found are stable?",['ordinary-differential-equations']
3985758,Prove or Disprove $A \subseteq C \land B \subseteq D $ is equivalent to $A \times B \subseteq C \times D$,"Can this statement be proved? because if i insert values, LHS is not equal to RHS.
For example,
let $A=\{1\}, B=\{1,2\}, C=\{1\}, D=\{1\}$ then $A\times B = \{1,2\}, C\times D=\{1\}$","['elementary-set-theory', 'logic']"
3985773,Connection Between Étale Fundamental Group and Topological Fundamental Group,"I have some problems with the understanding of the theory of fundamental groups. I hope everything I write down here is understandable and correctly written. Let $X$ be a (smooth) complex algebraic variety. Realize the variety as a complex manifold $X(\mathbb{C})$ . This question is concerned about the connection between the étale fundamental group $$\pi_1^{\text{ét}}(X,x)$$ and the topological fundamental group $$\pi_1(X(\mathbb{C}),x).$$ More precisely, I would like to understand how I can describe the étale fundamental group by using the classical fundamental group - which should be, if I'm not mistaken; $$
\pi_1^{\text{ét}}(X,x)=\widehat{\pi_1(X(\mathbb{C}),x)},
$$ where the right hand side denotes the profinite completion of the topological fundamental group. There are a few facts I have proven. Let me begin by mentioning them. Proven Results Statement 1. If we let $X$ denote a topological space with base point $x$ , $\mathbf{Cov}(X)$ denote the category of covering spaces over $X$ , and $\pi_1(X,x)-\mathbf{Sets}$ be the category of sets equipped by a group action by the fundamental group. Then we have the following equivalence of categories $$
\mathbf{Cov}(X)\simeq \pi_1(X,x)-\mathbf{Sets}
$$ Also, I have proven the following two statements: Statement 2. If $\mathbf{G}$ is a group considered as a category and $P_{\mathbf{G}}:\mathbf{G}-\mathbf{Sets}\to\mathbf{Sets}$ is the forgetful functor, then $$
\mathbf{G}\cong \operatorname{Aut}(P_{\mathbf{G}}),
$$ where $\operatorname{Aut}(P_{\mathbf{G}})$ is the automorphism group of $P_{\mathbf{G}}$ . Statement 3. If $\mathbf{\hat{G}}$ denotes the profinite completion of $\mathbf{G}$ and $\hat{P}_{\mathbf{G}}:\mathbf{G}-\mathbf{FinSets}\to\mathbf{FinSets}$ is the forgetful functor from the category of profinite completion actions on finite sets to the category of finite sets, then $$
\mathbf{\hat{G}}\cong\operatorname{Aut}(\hat{P}_{\mathbf{G}}).
$$ Lastly, we also have the following theorem called The Riemann Existence Theorem, which can be found in Lectures on Étale Cohomology by James Milne (see page 28): Riemann Existence Theorem. Let $X$ be a nonsingular variety over $\mathbb{C}$ . The functor sending a finite étale covering $(Y,\pi)$ of $X$ to the finite covering space $(Y(\mathbb{C}),\pi)$ of $X(\mathbb{C})$ is an equivalence of categories. A Quest to Prove the Isomorphism I had a discussion with a professor (not too long ago) over Zoom, he told me that the above is enough to find the connection between the fundamental groups. But I am not exactly sure how. In the notes, this is how professor Milne gave the proof From the Riemann Existence Theorem it follows that the étale universal
covering space $\widetilde{X}=(X_i)_{i\in I}$ of $X$ has the property
that every finite topological covering space of $X(\mathbb{C})$ is a
quotient of some $X_i(\mathbb{C})$ . Let $x$ any element of $X(\mathbb{C})$ . Then $$\pi_1^{\text{ét}}(X,x)=\varprojlim_i\operatorname{Aut}_X(X_i)=\varprojlim_i\operatorname{Aut}_{X(\mathbb{C})}(X_i(\mathbb{C})) =\widehat{\pi_1(X(\mathbb{C}),x)}.$$ I cannot see how anything I have proven comes into the picture in the above quotation. Questions Question 1. To use the results I have proven I guess I would like to put $\mathbf{G}=\pi_1(X(\mathbb{C}),x)$ in Statement 2 and Statement 3? This gives us $$
\pi_1(X(\mathbb{C}),x)\cong\operatorname{Aut}(P_{\mathbf{G}})
\\
\widehat{\pi_1(X(\mathbb{C}),x)}\cong \operatorname{Aut}(\hat{P}_{\mathbf{G}})
$$ where do I go from where? Question 2. To make The Riemann Existence Theorem more compact, let $\mathbf{FinÉtCov}(X)$ denote the category of finite étale coverings of $X$ and $\mathbf{FinCov}(X(\mathbb{C}))$ denote the finite covers of the topological space. Then the theorem says $$
(1)\text{ }\text{ }\text{ }\mathbf{FinÉtCov}(X)\simeq\mathbf{FinCov}(X(\mathbb{C}))
$$ As given in Statement 1, I have the following equivalence $$(2)\text{ }\text{ }\text{ }\mathbf{Cov}(X(\mathbb{C}))\simeq \pi_1(X(\mathbb{C}),x)-\mathbf{Sets}.$$ The two equivalence just given are not exactly the same, but ""almost"". Can I use the above equivalence somehow? I am quite sure I cannot do it like this, but something similar to $$\mathbf{FinCov}(X(\mathbb{C}))\simeq\pi_1(X(\mathbb{C}),x)-\mathbf{Sets}?$$ I don't think I can use the above because $(1)$ is about finite covers, when $(2)$ is not. After having done the above, then I could replace the fundamental group, with the automorphism group of the forgetful functor. But this seems just like it is making things much more complicated for me. I'm just trying to use all of the results I have proved. ´Something which bothers me is that I want to prove that two groups are isomorphic. But right now, everything I am doing is at a category-theoretical level - which makes me confused. I want to do things at a group-theoretic level. Question 3. At the end of the day, I would like someone to help me put everything together. Maybe someone can give me a guiding hand and tell me how this relates to what Milne did? Maybe there are more work to do before I can prove the isomorphism? I would be really happy if someone could help me get a better understanding of this problem. Best wishes, Joel","['fundamental-groups', 'algebraic-geometry', 'category-theory', 'algebraic-topology']"
3985776,Is The Inverse Laplace Transform of $e^{st}\operatorname{Log}\left(\frac{s+1}{s}\right)$ doable using inversion formula?,"I'm trying to solve inverse laplace transform using inversion formula and given by this integral: $$\frac{1}{2\pi i}\int_{\gamma-i\infty}^{\gamma+i \infty} e^{st}\operatorname{Log}\left(\frac{s+1}{s}\right)\,\Bbb ds.$$ Here is my contour, since the branch points of $\operatorname{Log} \left(\frac{s+1}{s}\right)$ are $0$ and $-1$ First, i want to show integral on $L_u\cup L_d$ is $0$ by bounding the integral with ML and then take the limit when $R$ goes to $\infty$ . By letting $L_u,\, L_d: s= \xi\pm iR,0\leq \xi\leq \gamma$ , where $\gamma$ is the real number that the vertical line of the given contour passed by. Since the $L$ is $\lvert e^{t(\xi\pm iR)} \rvert$ , then i have ML inequality as below: $$\lvert F(s)e^{st} \rvert \leq M_R \lvert e^{t(\xi\pm iR)} \rvert = M_R e^{\xi t} \leq M_R e^{at}$$ Next, i need to find $M_R$ and take the limit. $$\begin{align}
\left|F(s)\right| &= \left|\operatorname{Log}\left(\frac{s+1}{s}\right)\right|\\
&= \left|\operatorname{Log}\left(\frac{\xi\pm iR+1}{\xi\pm iR}\right)\right| = M_R
\end{align}$$ And by taking the limit of the last expression when $R$ goes to infinity yields $0$ . Meaning the integrals along those lines are $0$ . So, from here, am i doing this right? I'm not sure my work is correct. Maybe there are some mistakes there. Help me please! Edit:
Working with my $L_u$ with $ML$ inequality, i have $L=\gamma$ . Assuming $-\pi<\operatorname{arg}{s}\leq \pi$ and parametrizing $s=-\xi+iR$ , $\xi\in [-\gamma,0]$ : $\begin{align}
\left|\int_{L_u}\right| 
&\leq \left|e^{st} \log\left(1 + \frac 1s\right)\right|\\
&\leq \left|e^{-\xi t}\right| \left|e^{iRt}\right|\left|\ln\left|1+ \frac{1}{-\xi+iR}\right| + i\pi\right|\\
&\leq 1\cdot 1 \cdot \ln\left(1+\frac 1R\right) + \pi\\
&\approx \frac 1R + \pi
\end{align}$ Combining the $ML$ i have $$\frac{\gamma}{R} +\gamma\pi$$ Which does NOT approach to $0$ . Why? Please spot my mistake. I can't think about how to make it goes to $0$ since yesterday. Hope you kind to help me. New Edit : Now my main question is not about ML. I managed to set the big and small arc goes to $0$ . My main question now is ""is it possible to evaluate this by using inversion formula (without differentiate both sides) and use log form instead?"" What i mean by differentiate both sides is: $$\begin{align}
\mathcal{L}^{-1} &= \operatorname{Log}\left(\frac{s+1}{s}\right)\\
(\mathcal{L}^{-1})' &= \frac{1}{1+s} - \frac{1}{s}
\end{align}$$ I don't want this kind of solution. What i really want is evaluating $\operatorname{Log}\left(\frac{s+1}{s}\right)$ by inversion formula. That's it.
because I keep getting integrals on the contour lines above and below the branch cut cancel each other and make everything 0, which of course I don't want it. Attempt:","['inverse-laplace', 'complex-integration', 'laplace-transform', 'complex-analysis', 'inequality']"
3985814,Given a directional derivative to find a point,"Let $ f(x,y,z)$ be differentiable, and assume that $$ f(x, y, x^2 + y) = 3x - y $$ (for all values of $x,y$ ). Also, given the direct-derivative of the point $A=(0, 12, 12)$ and the direction vector is $(1, 0, 1)$ is equal to $3$ , I need to find the gradient of $f$ at $A$ . I have no idea how to even start, because $f$ is not given explicitly, only the derivative — but again, not explicitly for $f(x,y,z)$ , but for this weird combination $f(x, y, x^2 + y)$ . What should I do?",['multivariable-calculus']
3985860,Singularity at 0 is at most a pole of order 7 if the complex function is 1/4 integrable.,"So the real question is, given f analytic in the punctured disk $(D_1 \backslash \{ 0\})$ and $\int_{D_1} |f(z)|^{1/4} < \infty$ , characterize the smoothness of f at 0. It is easy to show that if $z=0$ is a pole, then it is at most of order 7. However, I cannot show that it cannot be an essential singularity. (Or find a counter example) Remark, if instead we had that $\int_{D_1} |f(z)|^p < \infty$ , for $p >=1$ , it is relatively easy to show that it cannot be an essential singularity, however it heavily depends on Holder inequality.","['complex-analysis', 'lp-spaces']"
3985890,Integral cohomology. follows from GAGA?,"Let $X$ be a projective variety over $\Bbb C$ . One can compute the integral cohomology groups $H^p(X,\Bbb Z)$ by looking at the constant sheaf $\Bbb Z$ in the Zariski topology on $X$ , but one can do the same with respect the Euclidean topology on $X$ as well. Question: what guarantees that the two definitions will give the same cohomology groups? My initial reaction was that this should follow from GAGA, but $\Bbb Z$ is not a coherent $\mathcal O_X$ -module, so not sure what to do. I am sure this is addressed in the literature, so a reference would be fine as well.","['projective-varieties', 'algebraic-geometry', 'reference-request']"
3985907,How is $y'=x^2+1$ a differential equation?,"From James Stewart Essential Calculus Early Transcendentals Textbook, A differential equation is an equation that contains an unknown function and one or more of its derivatives. But the equation $y'=x^2+1$ doesn't contain the unknown function $y$ , it only contains $y'$ and $x$ . And yet my textbook says its a DE of order 1. Even Wikipedia says a DE ""relates one or more functions and their derivatives"", but $y'=x^2+1$ doesn't contain the function $y$ so how is it a DE ?",['ordinary-differential-equations']
3985932,Inequality concerning a function :${(f(x))}^2=1+xf(x+1)$,"If $f:[0,\infty]\rightarrow [0,\infty]$ is such that for all $x\in [0,\infty]$ : $${(f(x))}^2=1+xf(x+1)$$ Prove that for all $x\ge 1$ $$\frac{x+1}{2}\le f(x)\le  2(x+1)$$ First of all I have never had experience proving such inequalities and since nothing is said about the differentiability I don't know whether calculus would be a good approach. We can however draw the following conclusions for all $x\ge 1$ As $f(x)\ge 1$ we have $$f(x)^2=1+xf(x+1)\ge 1+x \implies f(x)\ge \sqrt{x+1}$$ By Bernoulli's inequality $$f(x)=\sqrt{1+xf(x+1)}\le 1+\frac{xf(x+1)}{2}$$ As $f(x+1)\le f^2(x+1)$ and $x\le x^2$ $$f(x)^2=1+xf(x+1)\le 1+x^2f^2(x+1)$$ $$(f(x)-xf(x+1))(f(x)+xf(x+1))\le 1$$ That's it, I couldn't get any more ideas ... P.S. The problem is from here .","['contest-math', 'functional-equations', 'inequality', 'functions']"
3985979,Show that $n! ≥ 2^{n-1}$ is true by induction for n being a positive integer [duplicate],"This question already has answers here : Prove the inequality $n! \geq 2^n$ by induction (3 answers) Closed 3 years ago . I start off by doing the base case: Base Case: Let P(n) be $n! \ge 2^{n-1}$ . Then $P(1)$ will be: $$
1! \ge 2^0 
$$ $$
1 \ge 1
$$ So the base case is true. Induction: I assume $P(k)$ is true for some arbitrary positive integer $k$ . Now I need to show that $P(k+1)$ is true. So assuming $k! \ge 2^{k-1} $ is true, I need to show that $(k+1)! \ge 2^{(k+1)-1} $ is true. $$
2^{(k+1)-1} = 2 \cdot  2^{k-1}
$$ $$\le 2 \cdot k! \tag{By inductive hypothesis}$$ My question So I have solved this to this point but I don't really know where to take it from here. I would appreciate any help on completing this proof.","['induction', 'discrete-mathematics']"
3986046,Proving Theorem 19.2 in <Topology> by Munkres,"Theorem 19.2 in Munkres Suppose the topology on each space $X_{\alpha}$ is given by a basis $\mathcal{B}_{\alpha}$ . The collection of all sets of the form $$\prod_{\alpha \in J} B_{\alpha}$$ where $B_{\alpha} \in \mathcal{B}_{\alpha}$ for each ${\alpha}$ , will serve as a basis for the box topology on $\prod_{\alpha \in J} X_{\alpha}$ I am trying to prove this theorem but I am wondering if my proof is solid. (For reminder, the definition, lemma, or any theorem used in the proof will be the ones mentioned in the Munkres.) The way I tried to prove it is as follows: proof. Use Lemma 13.2 in Munkres. Let $\mathcal{C}$ be the collection of all sets of the form $\prod_{\alpha \in J} B_{\alpha}$ (mentioned in the Theorem 19.2. ). Then $\mathcal{C}$ is collection of open sets of the box topology since each $B_{\alpha}$ is open in $X_{\alpha}$ . ( $\because$ Box topology is the topology having as basis all sets of the form $\prod U_{\alpha}$ , where $U_{\alpha}$ is open in $X_{\alpha}$ .) Let $U=\prod_{\alpha \in J} U_{\alpha}$ be open set in $\prod X_{\alpha}$ and $x\in U$ . For $\alpha \:th$ coordinate of $x$ (denoted as $x_{\alpha}$ ), there is $B_{\alpha}$ s.t. $x_{\alpha} \in B_{\alpha}\subset U_{\alpha}$ . ( $\because \: U_{\alpha}$ is open in $X_{\alpha}$ and $B_{\alpha}$ is a basis element of $X_{\alpha}$ . Thus by property of basis, the inclusion holds.) Since this can be done for all coordinates of $x$ , we can say that $x$ is contained in some set $B$ which is an element of the $\prod_{\alpha \in J} B_{\alpha}$ mentioned in the Theorem 19.2 and $B$ is contained in the open set $U$ ( $x\in B \subset U)$ . Hence by Lemma 13.2 in Munkres, the collection of all sets of the form $\prod_{\alpha \in J} B_{\alpha}$ is basis for the box topology on $\prod_{\alpha \in J} X_{\alpha}$ . $$\tag*{$\blacksquare$}$$ The part that I am especially concerned about is the part where is start with ""Let $U=\prod_{\alpha \in J} U_{\alpha}$ be open set in $\prod X_{\alpha}$ "" Even though I think this seems okay (since box topology is defined as the topology having as basis all sets of the form $\prod U_a$ where $U_a$ is open in $X_a$ for each $a$ . I thought then arbitrary open set in the box topology will be of the form $\prod_{a \in J} U_{a}$ for open set is union of basis of the topology), I am still wondering if it was a hasty statement. Thank you for your time.","['general-topology', 'box-topology', 'product-space']"
3986079,Sufficient Conditions for Borel-Cantelli,"The first Borel-Cantelli Lemma states If $\sum \mathbf{P}(A_n) < \infty$ then $\mathbf{P}(A_n i.o.) = 0$ Question : If $\mathbf{P}(A_n) \rightarrow 0$ and $\sum \mathbf{P}(A_{n+1} / A_{n}) < \infty$ then $\mathbf{P}(A_n i.o.) = 0$ Now of course $\sum \mathbf{P}(A_{n+1} / A_{n}) < \infty$ does not imply $\sum \mathbf{P}(A_n) < \infty$ . For example we can take $\mathbf{P}(A_n) = 1/n$ . So how can we takle such a problem. My approach:
We can find indices $n_k$ s.t. $\sum_{n_k}^{n_{k+1}}\mathbf{P}(A_n) < 1/k^2$ $k = 1,2,3..$ Define an event $B_k = \cup_{n_k}^{n_{k+1}}\mathbf{P}(A_n) $ Then By Borel Cantelli $\mathbf{P}(B_n i.o.) = 0$ Thus as only a finite number of $B_n$ s happen we conclude on a finite number of $A_n$ s happen. Any help is appreciated","['borel-cantelli-lemmas', 'measure-theory', 'probability-limit-theorems', 'convergence-divergence']"
3986081,"Prove: if a $\sigma$-algebra over $X$ contains one unique atom $A$, then $A=X$.","Let $\mathcal{F}$ be a $\sigma$ -algebra over some non-empty $X$ . Given that $A$ is the only atom of $\mathcal{F}$ (recall $A \in \mathcal{F}$ is an atom of $\mathcal{F}$ if it's non-empty and no proper subset of $A$ other than the empty set is in $\mathcal{F}$ ),  show that $A=X$ . My attempt: It's clear $A \subset X$ . To show $X \subset A$ , suppose for contradiction there's some $x \in X$ and $x \in A^c$ . Let $$
B = \bigcap_{S \in \mathcal{F}: x \in S}  S
$$ (which is clearly non-empty as $x \in A^c \in \mathcal{F}$ ). My idea is to show that $B$ is also an atom of $\mathcal{F}$ (basically using this idea ), and $A\cap B = \emptyset \implies A \neq B$ , contradicting that $A$ is the unique atom. However, I'm stuck on showing $B \in \mathcal{F}$ , because the intersection defining $B$ might not be countable. It's intuitively clear that if $\mathcal{F}$ has $n \in \mathbb{N}$ distinct atoms, then $ | \mathcal{F}| = 2^n$ (or at least $\mathcal{F}$ should be finite), but I have yet to prove this (it's actually the last part of this problem, and I'm stuck on part 1)...",['measure-theory']
3986272,Calculate rotation matrix that flips the frame up,"Let's start with the standard basis frame: [1, 0, 0], [0, 1, 0], and [0, 0, 1]. Imagine this frame goes through some arbitrary rotation R, with one constraint, that we know that after the R is applied to the frame, the z-axis becomes [0, 0, -1]. I want to calculate a rotation that ""flips"" the [0, 0, -1] back to the [0, 0, 1] vector with the smallest angle possible. Note: Inverting the R is NOT correct, because we want the smallest angle, and inverting R would always completely erase the rotation to give us the identity matrix. The top answer here: Calculate Rotation Matrix to align Vector A to Vector B in 3d? does not apply, because I am describing the edge case that Jur van den Berg says is not supported. Geometrically, if we start with the orientation corresponding with the z-axis at [0, 0, -1], I think to ""flip"" the frame we need to swap the corresponding x and y axises, and simply multiply [0, 0, -1] by -1 to get [0, 0, 1]. You can understand what I'm saying if you use your hand to form a frame by right hand rule, then 'flip' your thumb to negate the z-axis, and then 'flip' it back. How do I calculate this 'flip' relative rotation for any arbitrary orientation?","['linear-algebra', 'geometry', '3d', 'rotations']"
3986308,Runge-Kutta method for a 2nd order nonlinear ODE/ CMC surface meridian,"I'm trying to solve numerically the following IVP nonlinear second-order differential equation: $$f''(x)=A\left(1+f'(x)^2\right)^{3/2}+\frac{\left(1+f'(x)^2\right)}{f(x)}$$ where $A$ is a constant and the problem has the initial conditions (IVP) $f(0)=R$ and $f'(0)=0$ , where $R$ is also a constant. I'm used to applying the RK4 method for first-order differential equations, but is it possible to solve it by the RK4 method as well? If so could someone provide me a good reference to understand this specific application for method? I also open to any other useful numerical method that can solve this IVP. Thanks in advance.","['integration', 'ordinary-differential-equations', 'runge-kutta-methods', 'numerical-calculus', 'numerical-methods']"
3986329,"Let $x_0$ be a transcendental number, $x_{n+1}=\frac{3-x_n}{x_n^2+3x_n-2}$. What is the limit of $x_n$?","Let $x_0$ be a transcendental number, $$x_{n+1}=\frac{3-x_n}{x_{n}^{2}+3x_{n}-2}$$ What is the limit of $x_{n}$ ? Choose $x_0=\pi$ , and is seems that the limit of $x_n$ is $-1$ . But what is the proof for this $\pi$ and other numbers? Let $$f(x)=\frac{3-x}{x^{2}+3x-2}$$ The following may be helpful. $$f'(x)=\frac{(x-7)(x+1)}{(x^{2}+3x-2)^2}$$ $$f(x)-x=\frac{-(x-1)(x+1)(x+3)}{x^{2}+3x-2}$$ $$f(x)+1=\frac{(x+1)^{2}}{x^{2}+3x-2}$$ .","['limits', 'calculus']"
3986485,Determine whether language L is regular,"Let $L\subseteq \{0, 1\}^*$ be the set of natural numbers in binary notation, which together compose the infinite arithmetic progression with first term 4 and common difference 3. Is it true that $L$ is regular? I tried to tackle this down using $a^{-1}L$ but failed to accomplish much. Any help is very much appreciated.","['formal-languages', 'discrete-mathematics', 'regular-language']"
3986536,Prove that a strictly decreasing function from $f:\Bbb R \to \Bbb R$ is one-to-one,"I would like to prove that a strictly decreasing function from $f:\Bbb R \to \Bbb R$ is one-to-one. We want to show that show that $f(a) = f(b)$ implies $ a = b$ for all $a, b \in \Bbb R$ . One proof I saw online was as follows (although I did the same proof using contrapositive technique), but I just want to get better understanding as to why he did the proof as follows: Proof: Since the function is strictly decreasing, it means that if $ x \lt y \implies f(x) \gt f(y)$ . To proof that it's one-to-one function, we need to prove that if $f(a)=f(b) \implies a=b$ . Let $f(a) = f(b)$ . Case 1 :  Consider when $a \lt b$ , then this implies that $f(a) \gt f(b)$ since $f(x)$ is strictly decreasing. This implies that $f(a) \ne f(b) \therefore a\ge b $ . Case 2 : Consider when $a \gt b$ , then this implies that $f(a) \lt f(b)$ since $f(x)$ is strictly decreasing. This implies that $f(a) \ne f(b) \therefore a = b $ . Questions: It seems the proof that was used in the question is proof by cases, was not it? Why it was assumed, in Case 1, that $f(a) = f(b)$ although what is given in the question is that $f(x)$ is strictly decreasing? Why it was concluded ,in Case 1, that since $f(a) \ne f(b) \therefore a\ge b $ ? I assume that it was finally concluded that $\therefore a = b $ is because no other scenarios left as to why $f(a) = f(b)$ except by equality of $a$ and $b$ .",['discrete-mathematics']
3986582,Cut-off function multiplied by Lipschitz continuous function in the Schwartz space,"In ""An SPDE Model for systemic risk with endogeneous contagion"" they claim Let $\varphi_\lambda\in C_c^\infty(\mathbb{R}, [0,1])$ a standard cut-off function, which is equal to one inside an interval $[-\lambda, \lambda]$ for $\lambda\in\mathbb{R}$ , $\psi\in\text{Lip}(\mathbb{R})$ (i.e. a Lipschitz continuous function on $\mathbb{R}$ ), then $\Gamma_\lambda:=\varphi_\lambda \psi\in\mathscr{S}(\mathbb{R})$ (which is the Schwartz space). How do I show, that this holds, i.e. the function $\Gamma_\lambda$ is smooth, and fullfills the property $\forall \alpha,\beta\in\mathbb{N}_0:\;\sup_{x\in\mathbb{R}}|x^\alpha D^\beta(\Gamma_\lambda(x))|<\infty.$ $\psi$ as a Lipschitz continuous function doesn't have to be smooth.","['functions', 'derivatives', 'functional-analysis']"
3986619,$\lim_{ x \to 0 } \frac{e-(1+\arctan x)^{\frac{1}{x}}}{x}$,"I have to evaluate this limit: $$\lim_{ x \to 0 } 	\frac{e-(1+\arctan x)^{\frac{1}{x}}}{x}$$ I have an indeterminated form $[\frac{0}{0}]$ .
So I applied the Hopital rule $$\frac{e-(1+\arctan x)^{\frac{1}{x}}}{x} \sim $$ $$-e^{\frac{\log(1+\arctan x)}{x}} \cdot \frac{\frac{x}{(1+\arctan x)(1+x^2)}-\log(1+\arctan x)}{x^2} \sim$$ $$ -e \cdot \frac{1}{x^2} \cdot \left(\frac{x}{(1+x)(1+x^2)}-x\right)
\sim +e $$ According to my book and wolfram alpha the final result should be $\frac{e}{2}$ and I don't know where I made mistakes.","['limits', 'real-analysis']"
3986638,How to check that given relation is a function?,"If f is defined from set of integers to set of natural numbers,
f(x)=|x|
Then is f a function?
Obviously it's not one-one but is it even a function?
I got confused because 0 belongs to integers and f(0)=0 but 0 does not belong to natural numbers so 0 has no image under f, hence f is not a function.
Thanks in advance.","['functions', 'absolute-value', 'relations', 'modules']"
3986650,"Does dx in dA=dx*dy represent change, or is it a notation that denotes an infinitesimal arbitrary length? Change vs. an arbitrary physical length","I'm not sure how to explain this, but I have a gap in my understanding of infinitesimals/differentials. I've so far had calc 1 and 2, and have been taught that dy/dx represents a slope, which represents a change. So, my current understanding is if I had a function dy/dx(x) and plugged in x, I could deduce how much of a change in y I could expect, if I moved a unit x, at that particular x of interest. Where my understanding falls apart is, I don't see how dx in a slope, which represents change, would also represent ""change"" in dA=dx*dy. I mean in the classical meaning of the word change. I know we could say ""well in the same way delta X is a change in x and represents length"", but I don't view it as such. Not sure if this is related, but here's another thing bothering me, say we have P=dF/dA. Why dF and not only F? I understand dA would represent this infinitesimal area, but what does dF represent then? It's supposed to be a change in F, right? Why do we need a change? I'd really appreciate someone clearing this up or me, because I'm expected to pass a fluid mechanics test while our uni doesn't even offer a calc 3 course. You can understand my frustration. Thanks","['differential', 'calculus', 'derivatives', 'infinitesimals']"
3986660,Local isometry which is not covering map,"The theorem of Ambrose establishes that given a local isometry between two connected Riemannian manifolds $(M, g)$ and $(N,h)$ if $M$ is complete then $f$ is a covering. I was wondering why completeness is necessary and I was trying to construct a local isometry which is not a covering until I realized that I do not know of many examples of local isometries, particularly on non complete manifolds such as the punctured plane. Does anyone know of a few examples? Any help would be welcome, thanks in advance.","['manifolds', 'isometry', 'differential-geometry']"
3986708,Doubt with improper integral,"I have to study the convergence of the improper integral $$\int_0^1 \frac{\sin(\sqrt{x})}{x(x-1)(\sqrt{x+2})}dx$$ So I split it $$\int_0^a \frac{\sin(\sqrt{x})}{x(x-1)(\sqrt{x+2})}dx + \int_a^1 \frac{\sin(\sqrt{x})}{x(x-1)(\sqrt{x+2})}dx$$ the first is asymptotic to $$\frac{\sqrt{x}}{x}=\frac{1}{\sqrt{x}} $$ that converges.
The second is asymptotic to $$\frac{\sin{1}}{(x-1)(\sqrt{2})} $$ that diverges. So the integral diverges, is that correct?","['improper-integrals', 'analysis']"
3986722,since a symmetric tridiagonal matrix contains only two distinct vectors,"I don't understand meaning of ""since a symmetric tridiagonal matrix contains only two distinct vectors"" I write example for symmetric tridiagonal matrix. The 1st row vector and 1st column vector are the same, the 2nd row vector and 2nd column vector are the same. This way they are all eliminated. What does ""contains only two distinct vectors"" mean?","['matrices', 'linear-algebra', 'numerical-linear-algebra', 'numerical-methods', 'tridiagonal-matrices']"
3986767,Euler method fails,"Consider the differential equation $$y'=y-2e^{-x},$$ with initial value $y(0)=1$ . It's not hard to see that $y=e^{-x}$ is the solution. I am trying to solve this numerically via the forward Euler method. However, no matter how I shrink my step size $h$ , the error of my numerical solution always diverges: in fact, the error at $x$ seems to be growing like $e^x$ . My question is why does the numerical solution error always diverge? The numerical solution from the Euler method can be expressed in the form $$y_n=A(1+h)^n+Be^{-hn},$$ for appropriate constants $A,B$ . I have a feeling that divergence has something to do with the fact that $|1+h|>1$ , but I'm not sure how to formalize this. As a follow-up, which numerical methods will solve this DE without the error blowing up?","['numerical-methods', 'ordinary-differential-equations']"
3986785,"What does ""well defined"" mean?","On page 153 of Linear Algebra Done Right the second edition, it says: Define a linear map $S_1: \text{range}(\sqrt{T^*T} ) \to \text{range}(T)$ by: 7.43: $S_1 (\sqrt{T^* T}v)=Tv$ First we must check that $S_1$ is well defined . To do this, suppose $v_1, v_2 \in V$ are such that $\sqrt {T^*T}v_1 = \sqrt{T^*T}v_2$ . For the definition given by 7.43 to make sense, we must show that $Tv_1=T v_2$ . It is not entirely clear to me what the term 'well-defined' means here. Can someone clarify? Thanks","['definition', 'linear-algebra']"
3986786,Finding a function that satisfies this property.,"Suppose that $f\in L^2([0,1],\mathbb{C})$ , that $0 < \alpha < 1$ and that $$(T_{\alpha}f)(x) = \int_0^{x^{\alpha}}f(y)dy$$ is continuous (which is actually something you can prove)
I need to find an $f\not = 0$ a.e. such that \begin{equation}(1) \mbox{     ....... } \int_0^{x^{\alpha}}f(y)dy = (1-\alpha) f(x) .\end{equation} In tecnical terms I want to show $(1-\alpha)$ is an eigenvalue for the operator $T_{\alpha}$ by showing there is an associated eigenvector. I could only observe that since the left hand side in (1) is continuous, then $f$ (on the right) is too, so that going back on the left we have $f$ is $C^1$ ,... and so on; so I think we can deduce that $f\in C^\infty$ , but also deriving (1) I could not get anywhere. Would you be able to give me some hints or a way to find such a function? Thanks in advance","['integration', 'eigenvalues-eigenvectors', 'real-analysis', 'hilbert-spaces', 'derivatives']"
3986869,What are the steps in breaking down the exponent in this limit analysis?,"I'm trying to understand the reasoning in the following step of a limit analysis: $$\lim_{n \to \infty} n\left(\left[1- \frac{1+c}{\frac{n}{\ln(n)}} \right]^{\frac{n}{\ln (n)}}\right)^{(n-1)\ln n/n} = \lim_{n \to \infty} ne^{-[(1+c)\ln(n)]}$$ I understand the ""inner"" part; $\lim_{n \to \infty} [1-\frac{1+c}{\frac{n}{\ln (n)}}]^{\frac{n}{\ln n}} = e^{-(1+c)}.$ And I sort of see that outer exponent $((n-1)\ln n) /n = (\ln n) - (\ln n / n)$ and the second part goes to 0, but it's not clear to me what rules actually justify ""bringing the limit to the exponent"". What are the actual steps involved in deducing this limit? More generally, these types of asymptotic analysis show up in comp sci all the time and I feel there is a bag of tricks that I am missing.","['limits', 'calculus']"
3987001,Couples are equally likely to have 1 or 2 children. How likely does a randomly chosen person have a sibling?,"Assume that every couple can only have exactly 1 child or two children, with those outcomes being equally likely. Ignore any silly extra factors (e.g. 1 child dying, another being alive). If I choose a human on the face of the earth randomly, what is the probability that they had a sibling? As a follow-up, what would be the general answer if couples had $i$ children with probability $\pi_i$ ? My thought for the simple case is either 1/2 or 2/3, and I can think of compelling reasons for both. Siblings get counted twice, and only-children get counted once, leading to 2/3. A given person's parents were equally likely to give him a sibling or not, leading to 1/2. This kind of problem occurred to me one day. A similar problem occurs to me when I think about divorce rates. (e.g. if the divorce rate is 50%, is the probability a person is divorced 50% or 66.6%?) Any help is appreciated!","['conditional-probability', 'paradoxes', 'probability']"
3987016,"How many solutions for $\,\sum_{k=1}^{n}p_k=p_m\cdot p_{m+1}\,$?","I ask for which pairs $(m,n)$ is satisfied $$\sum_{k=1}^{n}p_k=p_m\cdot p_{m+1}\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;(1)$$ where $p_k$ is the $k$ -th prime. Up to $n=10^7$ I have found only the solution $(4,8)$ that is $$2+3+5+7+11+13+17+19=7\cdot11$$ Could it be the only solution of the equation $(1)$ ? Many thanks.","['number-theory', 'summation', 'elementary-number-theory', 'prime-numbers']"
3987050,"Proving that for any 3 infinite sequences $\{a_n\},\{b_n\},\{c_n\}$ in $\Bbb N,\exists p,q\in\Bbb N$ s. t. $a_p\ge a_q,b_p\ge b_q,c_p\ge c_q.$","Prove that for any three infinite sequences $\{a_n\},\{b_n\},\{c_n\}$ in $\Bbb N$ , there are distinct $p,q\in\Bbb N$ s. t. $$a_p\ge a_q\space\land\space b_p\ge b_q\space\land\space c_p\ge c_q.$$ The task appeared on the $1961$ All Soviet Union Olympiad . My attempt: I tried to use the following lemma: Every sequence in $\{x_n\}$ in $\Bbb R$ has a monotonic subsequence. and its corollary: For finitely many sequences $a^{(1)},a^{(2)},\ldots,a^{(k)}$ there is a strictly increasing subsequence $p:\Bbb N\to\Bbb N$ s. t. all of the subsequences $a^{(1)}\circ p,a^{(2)}\circ p,\ldots,a^{(k)}\circ p$ are monotonic. and the proof of the corollary is: From the lemma, we know there a strictly increasing sequence $q^{(1)}:\Bbb N\to\Bbb N$ s. t. $a^{(1)}\circ q^{(1)}$ is a monotonic subsequence of $a^{(1)}$ . We now look at the subsequences $a^{(1)}\circ q^{(1)},a^{(2)}\circ q^{(1)},\ldots,a^{(k)}\circ q^{(1)}$ where the first one is monotonic, while others don't have to be. Again, from the lemma, there is a strictly increasing sequence $q^{(2)}:\Bbb N\to\Bbb N$ s. t. $a^{(2)}\circ q^{(1)}\circ q^{(2)}$ is a monotonic subsequence of $a^{(2)}\circ q^{(1)}$ . Since a subsequence of a monotonic sequence is also monotonic, among the subsequences $a^{(1)}\circ q^{(1)}\circ q^{(2)},a^{(2)}\circ q^{(1)}\circ q^{(2)},\ldots,a^{(k)}\circ q^{(1)}\circ q^{(2)}$ , the first two are monotonic, while others don't have to be. We repeat the process $(k-2)$ more times and obtain the subsequences $a^{(1)}\circ p,a^{(2)}\circ p,\ldots,a^{(k)}\circ p$ , where $p=q^{(1)}\circ q^{(2)}\circ\cdots\circ q^{(k)}$ . To avoid confusion, let $d:\Bbb N\to\Bbb N$ take the role of the strictly increasing sequence $p:\Bbb N\to\Bbb N$ from the corollary of the lemma. Now, every sequence in $\Bbb N$ has a minimum element, so, if some of the sequences $a\circ d,b\circ d,c\circ d$ happen to be decreasing , at some point they should become constant and hence monotonically increasing so, e.g., if $a'=a\circ d$ is decreasing $$\space m_a=\min\{m\in\Bbb N\mid\space a'_l\ge a'_m,l> m\},$$ and if we similarly define $m_b,m_c$ (if necessary), $p=\max\{m_a,m_b,m_c\}$ , and any $q\ge p$ should work as we end up with three monotonically increasing sequences $a\circ d,b\circ d,c\circ d$ starting from the index $p$ . May I ask if this argument is valid and if there is anything wrong? Thank you in advance!","['contest-math', 'solution-verification', 'sequences-and-series', 'real-analysis']"
3987055,"Let $A$ be a set and $R,S$ are relations on $A$. Then....","Let $A$ be a set and $R,S$ are relations on $A$ . Define the relation $R+S$ on $A$ as $$R+S = (R-S)\cup(S-R)$$ Now show that If $A$ is countable then so does $R+S$ Give and example of a set $A$ that is uncountable but $R+S$ is countable. Prove or disprove: If $R$ and $S$ are partial orders on A then so does $R+S$ For the first part, I am thinking that a relation gives us subset of the set on which it is defined, which is in our case $A$ so if $A$ is a countable set then its subset is countable as well. For the second part, Lets say $A=\mathbb{R}$ relation $S$ gives us rational numbers (or its subset) and $R$ gives us Natural numbers (or its subset) then their symmetric difference which is $R+S$ becomes a countable set however $\mathbb{R}$ is uncountable. For third part, I am thinking of relations $R=S=\{(a,a)|a\in \mathbb{Z} \}$ which holds both reflexive and transitive property but I am not sure about anti symmetry. Also this is just an example. How can we prove (if it is right) or disprove (Give counter example) Please help","['elementary-set-theory', 'abstract-algebra', 'relations']"
3987063,"Solution verification: Showing that $\|A\|_\infty = \max_{1 \le i \le n} \sum_{k=1}^n |a_{i,k}|$ for $A \in \Bbb R^{n\times n}$","$
\newcommand{\norm}[1]{\| #1 \|}
\newcommand{\inorm}[1]{\norm{#1}_\infty}
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\para}[1]{\left( #1 \right)}
\newcommand{\R}{\mathbb{R}}
$ Context: This ultimately ties back to a homework assignment. (Specifically it's from Fundamentals of Matrix Calculations by Watkins, Exercise $2.1.30$ from $\S2.1$ but that's not really a huge deal, seems like a fairly common exercise.) The goal is to show that, knowing matrix $p$ -norms are induced by the corresponding vector norms, $\forall A \in \R^{n\times n}$ , $$\|A\|_\infty = \max \limits_{1 \le i \le n} \sum_{k=1}^n |a_{i,k}|$$ I have an approach and was just curious as to how valid it is; I just don't feel very sure of myself. I feel like I made some sort of small-yet-critical mistake somewhere, but I can't figure out where... My Attempt: Ultimately, we will show that $$\max \limits_{1 \le i \le n} \sum_{k=1}^n |a_{i,k}| \le \|A\|_\infty \le \max \limits_{1 \le i \le n} \sum_{k=1}^n |a_{i,k}|$$ which will let me conclude with the desired equality ( $a \le b \le a \implies a=b$ ). From the definition of the $\infty$ -norm, with $A := (a_{i,j})_{1 \le i,j \le n} \in \R^{n \times n}$ and $x := (x_i)_{1 \le i \le n} \in \R^n$ , $$
\inorm{Ax} = \max_{1 \le i \le n} \abs{ \sum_{k=1}^n a_{i,k} x_k }
$$ Applying the triangle inequality yields $$
\inorm{Ax} \le \max_{1 \le i \le n} \sum_{k=1}^n |a_{i,k} x_k|  = \max_{1 \le i \le n} \sum_{k=1}^n |a_{i,k}| \cdot | x_k|
$$ Note that $$
|x_k| \le \max_{1 \le j \le n} |x_j| =: \norm{x}_\infty
$$ and thus $$
\inorm{Ax} \le  \max_{1 \le i \le n} \sum_{k=1}^n |a_{i,k}| \cdot \norm{x}_\infty 
$$ $\norm{x}_\infty$ is independent of $i$ , so we factor it out and conclude $$
\inorm{Ax} \le \norm{x}_\infty \para{ \max_{1 \le i \le n} \sum_{k=1}^n |a_{i,k}| }
$$ Thus, $$
\inorm{A} = \max_{x \ne \vec 0} \frac{\inorm{Ax}}{\inorm{x}} \le \frac{\displaystyle \norm{x}_\infty \para{ \max_{1 \le i \le n} \sum_{k=1}^n |a_{i,k}| }}{\norm{x}_\infty} = \max_{1 \le i \le n} \sum_{k=1}^n |a_{i,k}|
$$ Thus, with $\inorm{A}$ less than or equal to the desired expression, we need to find an $\hat x$ such that equality is achieved. Suppose in row $r$ the maximum is achieved; for every $k$ , let $$
\hat x_k = \begin{cases}
+1 & \text{if } a_{r,k} \ge 0 \\
-1 & \text{if } a_{r,k} < 0
\end{cases} = \mathrm{sign}(a_{r,k})
$$ Define $\hat x := (\hat x_i)_{1 \le i \le n}$ as defined above; clearly $\inorm{\hat x}=1$ . This ensures $a_{r,k}\hat x_k = |a_{r,k}|$ . Then \begin{align*}
\norm{A}_\infty
&=  \max_{x \ne \vec 0} \frac{\inorm{Ax}}{\inorm{x}}  \tag{def. of matrix norm} \\
&\ge \frac{\inorm{A \hat x}}{\inorm{\hat x}} \tag{def. of maximum}\\
&= \inorm{A \hat x} \tag{$\inorm{\hat x} = 1$} \\
&= \max_{1 \le i \le n} \abs{ \sum_{k=1}^n a_{i,k} \hat x_k } \tag{definition}\\
&=  \abs{ \sum_{k=1}^n a_{r,k} \hat x_k } \tag{definition of $r$}\\
&=  \abs{ \sum_{k=1}^n |a_{r,k}| } \tag{choice of $\hat x_k$, $r$}\\
&=   \sum_{k=1}^n |a_{r,k}|  \tag{$|z| \ge 0 \implies \sum_i |z_i| \ge 0$}\\
&= \max_{1 \le i \le n} \sum_{k=1}^n |a_{i,k}| \tag{def. of $r$}
\end{align*} Thus what we have seen is that $$
\max_{1 \le i \le n} \sum_{k=1}^n |a_{i,k}| \le \norm{A}_\infty \le \max_{1 \le i \le n} \sum_{k=1}^n |a_{i,k}|
$$ thus letting us conclude equality: $$
\norm{A}_\infty = \max_{1 \le i \le n} \sum_{k=1}^n |a_{i,k}|
$$","['matrices', 'solution-verification', 'linear-algebra', 'normed-spaces']"
3987064,"Is it possible/meaningful to have a right triangle with sides $0$, $1$, and $i$?","If $i^2 = -1$ , does that mean you could have a triangle with side lengths $0$ , $1$ , and $i$ ?  But if you draw such as triangle on a complex number grid, the hypotenuse length is $\sqrt 2$ , not $0$ .  Does the pythagorean theorem only apply to real numbers?","['triangles', 'geometry', 'complex-numbers']"
3987127,Counting permutations $\sigma \in S_n$ such that $\sigma$ is a product of $k$ disjoint cycles of length $r$.,"Problem: If $kr \leq n$ , with $1<r\leq n$ , then the number of permutations that are products of $k$ disjoint cycles of length $r$ is $$\frac{1}{k!}\frac{1}{r^k}[ n(n-1)\cdots(n-(kr-1)]$$ I already found that the number of $r$ -cycles in $S_n$ is $$A=\frac{1}{r}[n(n-1)\cdots(n-(r-1)]$$ I can't seem to find a way to get the result. I think I understand what I need to do, but I don't know how to do the counting. Given the set $C_r$ of all $r$ -cycles, $|C_r|=A$ , I choose one of them, say $\sigma_1$ , for which I have $A$ choices. Once I make a choice, I need to remove all the $r$ -cycles in $C_r$ that are not disjoint with $\sigma_1=(i_1\cdots i_r)$ How many of those are there? How do I count them? I need to remove all the cycles where any of the $i_j$ occur, but I don't know how to do that. Once I do that, I can choose one of those, say $\sigma_2$ , and repeat the same thing , until I have chosen $k$ . Any help would be appreciated","['permutations', 'group-theory', 'combinatorics']"
3987133,How can I describe a linear equation that becomes sinusoidal after a certain point?,"For example, take $y = x$ , but at $x \geq 5$ , $y$ now equals $\sin x + 5$ .
So for all $x$ values $\lt 5$ , $y$ has linear relationship, but at $\geq 5$ , $y$ now has sinusoidal relationship.
Is there a concise way to express this, perhaps a single equation?","['trigonometry', 'graphing-functions', 'linear-programming']"
3987145,Is it legitimate to quantify a variable twice?,"I am wondering if it is legitimate in first-order logic to quantify one variable twice in a single formula, for instance, as in the following proposition: \begin{equation}
\left(\forall s\right) \left(\exists t\right) \left[P\left(t\right) \wedge R\left(s\right) \wedge \left(\exists s\right) Q\left(s\right)\right].
\end{equation} It seems to me that there is no problem in $s$ appearing twice as quantifiers. But it looks a little weird. P.S. I am having this question as I read the following statement in set theory about variable substitution: If $\Psi$ is $\left(\forall v_{k}\right)\Theta$ for some formula $\Theta$ , and if $k \neq i$ , then $\Psi^{*}$ , which is achieved from substitution of $v_{j}$ for $v_{i}$ in $\Psi$ , is just $\left(\forall v_{k}\right)\Theta^{*}$ , where $\Theta^{*}$ is achieved from substitution of $v_{j}$ for $v_{i}$ in $\Theta$ . If $k = i$ , then $\Psi^{*}$ is $\left(\forall v_{j}\right)\Gamma$ , where $\Gamma$ is achieved from substitution of $v_{j}$ for $v_{i}$ in $\Theta$ . It seems that in set theory, variables are quantified only once. Otherwise, $\Gamma$ should be achieved from substitution of $v_{j}$ for FREE occurrences of $v_{i}$ in $\Theta$ , which means that the substition should only take place the first time $v_{i}$ is met as a quantifier, and not for others included inside. Is it that in set theory, laws from first-order logic are relaxed so that each variable is assumed to be quantified only once?","['elementary-set-theory', 'first-order-logic']"
3987196,Method of Frobenius for $x^2y^{''}+x(1-x)y^{'}-(1-2x)y=0$ about $x_0=0$,"I'm trying to solve this rather mean looking ODE by Method of Frobenius and could use some help on finding the first linearly independent solution. $x^2y^{''}+x(1+x)y^{'}-(1-2x)y=0$ about $x_0=0$ The point $x_0=0$ is a regular singular point as the following limits exist, $p_0=\lim\limits_{x \to 0}[(x)\frac{x(1+x)}{(x^2)}]=1$ and $q_0=\lim\limits_{x \to 0}[(x^2)\frac{-(1-2x)}{(x^2)}]=-1$ The indicial equation will be of the form $r(r-1)+p_0r+q_0$ . Substituting in the values of $q_o$ and $q_0$ from above we find, $r^2-1=0$ with the roots $r=\pm1$ As $r_1-r_2=(1)-(-1)=2$ is a positive integer, we will have our first and second solutions in the form, $y_1=\sum_{n=0}^\infty a_n(x)^{n+r^1}$ and $y_2=Cy_1(x)
\ln(x-x_0)+\sum_{n=0}^\infty b_n(x)^{n+r^1}$ . As stated before, I'm having trouble finding the series solution for $y_1$ . Here is my work so far, $y=\sum_{n=0}^\infty a_n(x)^{n+r}$ , $y'=\sum_{n=0}^\infty(n+r)a_n(x)^{n+r-1}$ , $y''=\sum_{n=0}^\infty(n+r)(n+r-1)a_n(x)^{n+r-2}$ Substitute the above series into the given ODE, $x^2[\sum_{n=0}^\infty(n+r)(n+r-1)a_n(x)^{n+r-2}]+(x+x^2)[\sum_{n=0}^\infty(n+r)a_n(x)^{n+r-1}]-(1-2x)[\sum_{n=0}^\infty a_n(x)^{n+r}]=0$ $\sum_{n=0}^\infty(n+r)(n+r-1)a_n(x)^{n+r}+\sum_{n=0}^\infty(n+r)a_n(x)^{n+r}+\sum_{n=0}^\infty(n+r)a_n(x)^{n+r+1}-\sum_{n=0}^\infty a_n(x)^{n+r}+2\sum_{n=0}^\infty a_n(x)^{n+r+1}=0$ Now shift the index so all series are of $x^k$ , $\sum_{k=r}^\infty k(k-1) a_{k-r}x^k+\sum_{k=r}^\infty k a_{k-r}x^k+\sum_{k=r+1}^\infty (k+1) a_{k-r-1}x^k-\sum_{k=r}^\infty a_{k-r}x^k+2\sum_{k=r+1}^\infty a_{k-r-1}x^k=0$ $\sum_{k=r}^\infty [k(k-1)+k-1]a_{k-r}+\sum_{k=r+1}^\infty [(k+1)+2]a_{k-r-1}x^k=0$ Expand the first summation for $k=r$ , $r(r-1)+r-1]a_0x^r+\sum_{k=r+1}^\infty [k(k-1)+k-1]a_{k-1}x^k+\sum_{k=r+1}^\infty [(k+1)+2]a_{k-r-1}x^k=0$ Choose the largest root $r_1=1$ and further collect the series under one grouping, $0a_0x^r+\sum_{k=2}^\infty([k(k-1)+k-1]a_{k-1}+[(k+1)+2]a_{k-2})x^r=0$ Now set the coefficients equal to zero to find the recurrence relation, $[k(k-1)+k-1]a_{k-1}+[(k+1)+2]a_{k-2}=0$ , for all $k\geq2$ $a_{k-1}=-\frac {k+3}{k^2-1}a_{k-2}$ , for all $k\geq2$ Now this is where I begin to fail in finding the first solution. As I choose values of k, there is no observable pattern for the coefficients of $a$ . Could I please receive some help on how to finish this question or identifying where I went wrong?",['ordinary-differential-equations']
3987205,Invertibility of $I-A^s$ for each $s \geq 1$ implying nilpotence,"If $R$ is a unital ring and $x \in R$ a nilpotent element, then $1-x$ is a unit, and so is $1-x^s$ for any $s \geq 1 $ since powers of $x$ are again nilpotent. My question concerns the converse of this statement in the case in which $R = M_n(\mathbb{C})$ , that is: Question 1: if $A \in M_n(\mathbb{C})$ is a square matrix with complex entries and $I-A^s$ is invertible for all $s \geq 1$ , what conditions can we impose on $A$ to guarantee that it is nilpotent? Some condition has to be in place, as the general statement is already false in dimension $1$ . In that case, the matrices $1-a^n$ will be always invertible whenever $a$ is not a root of unity, and the only nilpotent element is zero. More generally, any diagonal matrix $\mathrm{diag}(a_1, \ldots, a_n)$ of non-zero elements that are not roots of unity will do the trick. Here's a partial result, If $\{\det(I-A^n)\}_n \subset \mathbb{R} \setminus \{0\}$ converges, then either the spectrum of $A$ is contained in $B_1(0)$ . In particular,
if $A$ has integer coefficients then it is nilpotent. A proof follows: let $c_n := \det(I-A^s)$ and $\lambda_1, \ldots, \lambda_n \in \mathbb{C}$ be the eigenvalues of $A$ , counted with multiplicity. Then $$
(1-\lambda_1^s) \cdots (1-\lambda_n^s) = c_s 
$$ is a convergent subsequence. Noting $d_s = \prod_{i \ : \ |\lambda_i| \neq 1}(1-\lambda_i^s)$ , we then have $$
\prod_{i \ : \ |\lambda_i| = 1}(1-\lambda_i^s) = c_s/d_s \to L.
$$ with either $L = 0$ or $L = \lim_s c_s$ , depending on whether there are eigenvalues with absolute value greater than $1$ . If has some eigenvalue outside the unit disc, the right hand side converges and moreover the left hand side has some non-trival factor. Observe that since $c_n$ is never zero, no $\lambda_i$ can be a root of unity; and so we are left with showing that if $k \geq 1$ and $\alpha_1, \ldots, \alpha_k \in [0,1)$ are irrational then $$
a_s := \prod_{j = 1}^k(1-e^{i2\pi\alpha_j s}) 
$$ does not converge to $0$ nor $\lim_s c_s$ . By Dirichlet's approximation theorem, for each $k \geq 1$ we can find a natural number $s_k$ and an integer $p_k$ for which $|s_k\alpha_j - p_k| < 1/k$ , and thus there is a subsequence $(s_k)_k$ for which $e^{i2\pi\alpha_j s_k} \to 1$ for all $j$ , and $a_{s_k}\to 0$ . But then taking $s'_k = s_k+1$ yields $e^{i2\pi\alpha_j s_k'} \to e^{i2\pi\alpha_j s}$ and $a_{s_k'} \not \to 0$ . Hence $(a_s)$ never converges. Are there results in this direction whose proof is purely algebraic? In particular, can we recover the former partial results without doing analysis ? It seems to me like there is some relation between the spectrum of $A$ and the fact that $I-A^s$ being invertible implies the invertibility of $\sum_{i = 0}^s A^i$ , the limit of the latter - were to exist- being the inverse of $I-A$ . Is there a reference for this statement, at least in the case in which $\det(I-A^s) = \pm 1$ for each $s$ and $A$ has integer coefficients? I suspect this specific case can be proved in a purely algebraic fashion but so far I haven't been able to come up with a proof. Here's what I've done so far: Suppose that $f = \chi_A = X^m g, g = (X-\lambda_1) \cdots (X-\lambda_k)$ with $X \not \mid g$ . Note $p^{(m)}$ the polynomial whose roots are the $m$ -th powers of $p$ 's roots. Then our hypothesis is precisely that $f^{(m)}(1)$ is always a unit in $\mathbb{Z}$ . Thus, the same holds for $g$ , write $g = p_1 \cdots p_r$ where $p_j \in \mathbb{Z}[X]$ are irreducible. Since no eigenvalue $\lambda_i$ is a root of unity, once again by Kronecker's  theorem each $p_j$ has a - possibly complex - root of absolute value greater than $1$ . Moreover the proof notes that in general if $p \in \mathbb{Z}[X]$ then $p^{(m)} \in \mathbb{Z}[X]$ , so in any case, we have $$
\pm 1 = g^{(m)}(1) = p_1^{(m)}(1) \cdots p_r^{(m)}(1)
$$ and so $p_j^{(m)}(1) = \pm 1$ . This allows us to reduce the original problem to: Attempted Subproblem 1. Let $f = \prod_{i=1}^k (X-\alpha_i) \in \mathbb{Z}[X]$ be an irreducible monic polynomial. If $f^{(m)}(1) = \pm 1$ for each $m \geq 1$ , where $f^{(m)} = \prod_{i=1}^k (X-\alpha_i^m)$ , then $f = X$ . Since $\det(I-A^s) = \pm 1$ for all $s$ , dividing by $\det(I-A)$ gives $\det(\sum_{i=0}^s A^i) = \pm  1$ for all $s$ . This yields a pretty large sum of all monomials $x_{i_1}^{j_1} \ldots x_{i_r}^{j_r}$ where $\{i_1, \ldots, i_r\} \subset \{1,\ldots, k\}$ and $j_t \in \{0,\ldots, s\}$ , being equal to $\pm 1$ . Maybe there's some underlying interpretation in terms of symmetric polynomials.","['alternative-proof', 'algebraic-number-theory', 'nilpotence', 'linear-algebra']"
3987242,Inverse Laplace Transform hint,"I am away from Laplace transform for years, and now I have to solve $$\mathcal{L}^{-1} \left\{ s^{-\frac32}\sqrt{\frac{as+b}{cs+b}} \right\}(t),$$ where $a,b,c$ are real positive numbers. I can find inverse Laplace for $s^{-k}$ but really I got stuck on this. Please guide me to solve this form. Thanks in advance. Implicitly it has the condition $\lim\limits_{s\to\infty}F(s)=0$ Honestly I don't know what should I do for $\sqrt{\frac{as+b}{cs+b}}$ .","['integration', 'laplace-transform', 'ordinary-differential-equations']"
3987247,Convergent subsequences of $\{ n \alpha \bmod 1 \}$,"It's well known that if $\alpha \not\in \mathbb{Q}$ , then the sequence $\{n \alpha \bmod 1\}_{n \geq 0}$ is dense in the torus $\mathbb{T} = \mathbb{R}/\mathbb{Z}$ .  Does every convergent subsequence necessarily have zero density?  To be a bit more precise, if the sequence $\{n_j \alpha \bmod 1\}_{j \geq 1}$ converges, does the set $\{n_j : j \geq 1\}$ have density zero in $\mathbb{N}$ ?","['convergence-divergence', 'diophantine-approximation', 'dynamical-systems', 'sequences-and-series']"
3987263,What simple set theory topics do you find interesting? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Questions about choosing a course, academic program, career path, etc. are off-topic. Such questions should be directed to those employed by the institution in question, or other qualified individuals who know your specific circumstances. Closed 3 years ago . Improve this question I'm currently in my final year of a math undergrad degree and need to start writing my senior capstone. I've recently gotten very interested in the concept of set theory and have read through some information online and most recently I finished Halmos's Naive Set Theory. I was speaking to my advisor about the possibility of writing a set theoretic definition of the real numbers, but he seems concerned that giving a thorough explanation will be too long. My capstone prompt is incredibly broad:  write at least 10 pages about something to do with math. I'm overwhelmed by the possibilities and feel like everything I come across would take 20-30 pages minimum to describe. Although it's not a specific math solution I'm looking for, I was wondering if anyone has any relatively simple concepts that they find interesting or even a book/article suggestion so I have somewhere to start? Thank you in advance! I'm excited about the field and doing the research, just overwhelmed with options.","['elementary-set-theory', 'set-theory', 'reference-request']"
3987287,Multiplying a ring(or an ideal) by an element.,"I was looking at a proof of the statement: Given a ring $R$ and an ideal $M$ , $R/M$ is a field $\iff$ M is maximal. The backwards direction (assume $M$ maximal) proof involved a statement where we take an $r\in R$ such that $r\notin M$ and take $M + r*R$ which is an ideal as $M$ and $R$ are ideals, which implies that this construction must be $R$ itself. I had two questions about this statement. First of all, I realized very quickly that for a given ring $r*R\neq R$ . Is this simply because $R$ is not a group under $*$ , i.e., if $R$ were a field, would $r*R=R$ , or is there something else going on? If it's because of the group assumption, what would the reasoning behind a proof be (I assume something to do with existence and uniqueness of inverses but not 100% sure)? The second was assuming that $r*R$ is an ideal. By definition of an ideal I know $r*R\in R$ , but by the above question there's no guarantee that $r*R=R$ . How then do we know that $r*R$ is an ideal?","['ring-theory', 'group-theory', 'abstract-algebra', 'ideals']"
3987302,Prove that if a graph $G$ has an independent vertex subset $X \subseteq V G$ such that $|X| > |N (X)|$ then $G$ is non-Hamiltonian.,"Prove that if a graph $G$ has an independent vertex subset $X \subseteq V G$ such that $|X| > |N (X)|$ then $G$ is non-Hamiltonian. I have tried to delete m vertices in order to produce m component, but it doesn't work. can someone please help!","['graph-theory', 'hamiltonicity', 'discrete-mathematics']"
3987316,Extension of the concept of summation.,"I am not a mathematician and I am just asking this question out of my curiosity. $Warning:$ This question might not have the right tags. In an infinite series: $\sum_{n\in\mathbb{N}}a_n$ , if you create a set $S_0=$ { $a_k|k\in\mathbb{N}$ },then $Cardinality(S_0)=ℵ_0$ We know that we can write integrals( $∫$ ) as riemann sums then it seems like adding infinitely many infinitesimal amounts and getting a finite value. Like $\int_{C}a_n dn$ where $C$ is a continuous curve Now if we let $S_1=$ { $x|x\in C$ }(The set of all point in $C$ ) Then $Cardinality(S_1)=\aleph_1$ So it seems like integral is an extension of regular summation to higher cardinality. Integral is the continuous counterpart of regular sums and regular sum is the discrete counterpart of integral. So finally my question is that: Is it possible to extend the concept further to higher and higher cardinalities or is my question nonsense? (I am not a mathematician that is why I couldn't describe my question in a precise way, but I still hope that my question is understandable and answerable)","['integration', 'elementary-set-theory', 'set-theory', 'sequences-and-series']"
3987320,How to prove this inequality $\left|x\sin{\frac{1}{x}}-y\sin{\frac{1}{y}}\right|<2\sqrt{|x-y|}$?,"For any real numbers $x,y\neq 0$ ,show that $$\left|x\sin{\dfrac{1}{x}}-y\sin{\dfrac{1}{y}}\right|<2\sqrt{|x-y|}$$ I found this problem when I dealt with this problem . But I can't prove it. Maybe the constant $2$ on the right hand side can be replaced by the better constant $\sqrt{2}$ ? Thank you.",['inequality']
3987357,"If $\psi=x\sin(\frac{1}{x})$, and $f$ is integrable, is then $f\circ \psi$ also integrable?","Let $$
\psi(x)=
\left\{
\begin{array}{cll}
x \sin\Big(\dfrac{1}{x}\Big) & \text{if} & x\in (0,1],\, \\
0 & \text{if} & x=0,
\end{array}
\right.
$$ and let $f:[-1,1]\rightarrow \mathbb{R}$ be Riemann integrable. How can I show that $f\circ \psi$ is Riemann integrable? I have several theorems in my book that could work if $\psi$ were a $C^1$ diffeomorphism or a homeomorphism with inverse satisfying Lipschitz condition. But $\psi$ is clearly neither of those. Any help with a zero set argument or reverting to definitions?","['integration', 'calculus', 'riemann-integration', 'real-analysis']"
