question_id,title,body,tags
4500952,Solve the inequality $3^{(x+3)^2}+\frac{1}{9}\leq 3^{x^2-2}+27^{2x+3}$,"I tried to group the summands so that I could decompose them into multipliers, but nothing worked... $$3^{(x+3)^2}+\frac{1}{9}\leq 3^{x^2-2}+27^{2x+3}\Leftrightarrow 3^{x^2+6x+9}+\frac{1}{9}\leq 3^{x^2-2}+3^{6x+9}$$ $$3^{x^2+6x+7}+1\leq 3^{x^2-4}+3^{6x+7}$$ How to solve it further?","['algebra-precalculus', 'inequality']"
4500985,Counting Binary Sequences Arising from a Numerical Semigroup,"This is a sequel to this question , though none of the background there should be necessary. Yly's excellent answer to that question gives a few different ways of stating the below question for $n = 2$ , which may be helpful. Short version: Consider a set of positive integers $A =\{m_i\}_{i=1}^n$ with $m_i < m_{i+1}$ and $\mathrm{gcd}(A) = 1$ . Form an infinite binary sequence $s$ with $x$ $1$ s and arbitrarily many $0$ s subject to the following constraint: If there is a $0$ in position $j$ , i.e., $s_j = 0$ , then $s_{j+m_i} = 0$ for all $i$ . Let the first entry of $s$ be $s_0$ , and set $s_0 = 0$ . How many such sequences are there for fixed $x$ and $\{m_i\}$ ? More details: It quickly follows that the positions of the entries in $s$ which must be $0$ s are the elements of a numerical semigroup, i.e., an infinite set of nonnegative integers containing $0$ that is closed under addition. (Wikipedia link here.) If $S$ is a semigroup, it has a minimal set of generators, i.e., the smallest set of relatively prime non-negative integers $\{m_i\}$ such that every element in $S$ can be written in the form $\sum_i a_i m_i$ for non-negative $a_i$ . By removing superfluous $m_i$ if necessary, we can take $A$ from above to be the minimal set of generators of the semigroup $S$ that gives the positions of the required $0$ s in the sequence. The positive integers not contained in $S$ are called the gaps of $S$ , denoted $G(S)$ . The number of gaps, called the genus of $S$ , is denoted $g(S)$ . For our sequence, the gaps are the possible positions of the $1$ s. A few results quickly follow from this picture. If for fixed $\{m_i\}$ , we let $N_0(x)$ be the number of sequences beginning with a $0$ and with $x$ $1$ s, then $N_0(x = g(S)) = 1$ , when all the gaps are filled, and $N_0(x) = 0$ for $x > g(S)$ . It can also be shown that $N_0(1) = m_1 - 1$ , as only the gaps before the first generator can be filled by a solitary 1, since if $s_i = 0$ for $i = 0, 1, \ldots, m_1$ , then $s_i = 0$ for all $i$ . However, finding $N_0(x)$ for $1 < x < g(S)$ seems much more difficult. For example, Let $A = \{3,7\}$ . We see that $G(S) = \{1,2,4,5,8,11\}$ and $g(S) = 6$ . Let $x = 2$ . Clearly we can take $s_1 = s_2 = 1$ . We see that if we fill the gap at position 1 with a $0$ , i.e. set $s_1 = 0$ , then $s_4 = s_8 = s_{11} = 0$ as well. It follows that if $s_1 = 0$ , we must have $s_2 = s_5 = 1$ . If we instead take $s_1 = 1$ and $s_2 = 0$ , then $s_5 = s_8 = s_{11} = 0$ , so we can either take $s_2 = 1$ or $s_4 = 1$ . Note that setting $s_2 = 0$ in the latter case does not force $s_4 = 0$ . Thus we find $N_0(2) = 3$ , with the gaps filled with $1$ s and the actual sequences being $$(1,2) \text{ and } (1,4) \text{ and } (2,5), \qquad 011 \text{ and } 01001 \text{ and } 001001,$$ with the infinite trailing $0$ s suppressed. We can do similar computations to find $N_0(x) = 2, 2, 1$ for $x = 3, 4,$ and $5$ , respectively. The gaps filled with $1$ s and the sequences are $$\begin{matrix}
x = 3: & (1,2,4) \text{ and } (1,2,5) & 01101 \text{ and } 011001\\
x = 4: & (1,2,4,5) \text{ and } (1,2,5,8) & 011011 \text{ and } 011001001\\
x = 5: & (1,2,4,5,8) & 011011001\\
\end{matrix}.$$ It is unclear how to systematize this method. Clearly some understanding of how the gaps of $S$ ""knock out"" some of the subsequent gaps when the entries of $s$ corresponding to the gaps are set to $0$ is necessary. It seems like thinking about the residues of the gaps modulo the $m_i$ might be useful, but I've been unable to make much progress for general $\{m_i\}$ . There is reason to believe (see Yly's answer to the question linked at the top) that the solution involves counting integer partitions subject to various restrictions, so a closed form for $N_0(x)$ is likely impossible, but a generating function or an expression in terms of objects derived from $S$ would still be useful.","['binary', 'semigroups', 'combinatorics', 'sequences-and-series']"
4501009,Integrating $\int^{\infty}_0\frac{x^n}{e^x-1}\text{ d}x$ using contour integration only,"I have the integral $$I=\int^{\infty}_0\frac{x^n}{e^x-1}\text{ d}x$$ for real $n>0$ that I want to evaluate only with contour integration. (I already know the identity that $\Gamma(s)\zeta(s)=\int^{\infty}_0\frac{x^{s-1}}{\exp(x)-1}\text{ d}x$ ) I let $$f(z)=\frac{z^n}{e^z-1}$$ noting that there is a removable singularity at $0$ , and poles every $z=2\pi im$ for $m\in\mathbb{Z}, m\neq0$ . As a result, I set up the following contour $\mathcal{C}$ shown below, where I would take the limit as $R\rightarrow+\infty$ and $\epsilon\rightarrow+0$ . By the Residue theorem, $$\oint_{\mathcal{C}}f(z)\text{ d}z=\int_{\text{Right}}+\int_{\text{Top}}+\int_{\text{Left}}+\int_{\text{Bottom}}+\int_rf(z)\text{ d}z=0$$ Parameterizing the integrals yields $$\int^{2\pi}_0\frac{(R+iy)^n}{e^R e^{iy}-1}i\text{ d}y + \int^{\epsilon}_{R}\frac{(2\pi i+x)^n}{e^{2\pi i}e^x-1}\text{ d}x + \int^{0}_{2\pi-\epsilon}\frac{(iy)^n}{e^{iy}-1}i\text{ d}y + \int^{R}_0\frac{x^n}{e^x-1}\text{ d}x + \int^{-\frac\pi 2}_{0}\frac{(2\pi i+\epsilon e^{i\theta})^n}{e^{2\pi i}e^{\epsilon e^{i\theta}}-1}i\epsilon e^{i\theta}\text{ d}\theta=0$$ Since $f(z)$ approaches $0$ as $\Re(z)$ becomes large, $\int_{\text{Right}}f(z)\text{ d}z=\int^{2\pi}_0\frac{(R+iy)^n}{e^r e^{iy}-1}i\text{ d}y$ should go to $0$ . More rigorously, I moved the limit as $R\rightarrow +\infty$ inside (the function should uniformly converge I think so this is legal?) and indeed it would go to $0$ . $\int_{\text{Bottom}}f(z)\text{ d}z=I$ so we can ignore that for now. I'm thinking that we possibly could use a Laurent series for $\int_rf(z)\text{ d}z=\int^{-\frac\pi 2}_{0}\frac{(2\pi i+\epsilon e^{i\theta})^n}{e^{2\pi i}e^{\epsilon e^{i\theta}}-1}i\epsilon e^{i\theta}\text{ d}\theta$ since we're taking the limit $\epsilon\rightarrow +0$ but honestly I'm not sure how. If I directly take the limit $\epsilon\rightarrow +0$ of the integrand, Wolfram Alpha tells me it's equal to $(2\pi i)^n$ , which would mean the entire integral is equal to $2^{n-1}(i\pi)^{n+1}$ , but since it's such a complicated function I'm not sure if it uniformly converges on those bounds and thus if it is legal or not... I also tried using the Cauchy-Schwartz Inequality on $\int_{\text{Left}}f(z)\text{ d}z=\int^{0}_{2\pi-\epsilon}\frac{(iy)^n}{e^{iy}-1}i\text{ d}y$ where $$\left|\int^{0}_{2\pi}\frac{(iy)^n}{e^{iy}-1}i\text{ d}y\right|\le \int^{0}_{2\pi-\epsilon}\frac{\left|i^n\right|\left|y^n\right|}{\left|e^{iy}-1\right|}|i|\text{ d}y = \int^{0}_{2\pi-\epsilon}\frac{y^n}{\left|e^{iy}-1\right|}\text{ d}y\le \int^{0}_{2\pi-\epsilon}\frac{y^n}{\left|\left|e^{iy}\right|-\left|1\right|\right|}\text{ d}y$$ where the last part follows from $\frac{1}{|a-b|}\le\frac{1}{||a|-|b||}$ but this makes the denominator $0$ which is illegal so I probably did something wrong here. So in the end, I would have the following equation: $$I-\int^{\infty}_{0}\frac{(2\pi i+x)^n}{e^x-1}\text{ d}x - \int^{2\pi}_{0}\frac{(iy)^n}{e^{iy}-1}i\text{ d}y=2^{n-1} (i\pi)^{n+1}$$ And... I'm stuck. If anyone can help it would be really appreciated! Thanks in advance! :D","['integration', 'improper-integrals', 'complex-analysis', 'contour-integration', 'complex-integration']"
4501034,Is there an efficient way to determine whether a given contiguous sequence can appear within a de Bruijn sequence?,"I am curious whether an efficient algorithm exists to determine whether a sequence $S$ can appear within a de Bruijn sequence of order $n$ . For example, for $n=3$ , I believe the only de Bruijn sequence is 00010111 (and its rotations/reflection). By definition, any sequence of length $3$ or less will appear at some point, assuming you allow wrapping around or the equivalent. However, 0110 does not appear; you can even prove it cannot fairly easily, as it would still require 111 at some point, granting you a total of 5 1 -bits, where the maximum can only be $2^{n-1}=4$ of the 1 -bits. For larger values of $n$ where there are a great many distinct de Bruijn sequences, it seems like deciding whether some sequence may ever appear might turn out to be very difficult to work out. Or perhaps there's some easy trick to it. If anyone can describe how you could do this/point me towards an algorithm for it, or alternatively confirm that it is difficult after all (in the rough sense of being not computationally feasible past more than the first handful of $n$ ), I'll consider this answered.","['computer-science', 'computational-complexity', 'combinatorics', 'sequences-and-series']"
4501050,"If two finite Abelian groups posses equal number of elements of some particular order, then justify they are isomorphic.","Let $G, G'$ be two finite Abelian groups of equal order. Let $n_d(G)$ denote the number of elements of order $d$ where $d$ is a divisor of $|G|$ . Note that, $n_d(G)\geqslant 0$ since for example, $n_4(\mathbb{Z}_2\times \mathbb{Z}_2)=0$ and $n_4(\mathbb{Z}_4\times \mathbb{Z}_2)=4$ . If $G\simeq G'$ then $n_d(G)=n_d(G')$ for every positive divisor $d$ of $|G|$ and vice-versa. I found a proof in this paper: ""Ronald McHaffey (1965). Isomorphism of Finite Abelian Groups. The American Mathematical Monthly, 72(1), 48â€“50. doi:10.2307/2313001 "". However a question raised in my mind. Suppose we do not know if $G\simeq G'$ but for some particular positive divisor $d>1$ of $|G|$ , let $n_d(G)=n_d(G')$ . My question is, does this imply $n_d(G)=n_d(G')$ for all positive divisors $d$ of $|G|$ ? If this is true, then eventually $G$ and $G'$ become isomorphic by the above theorem. I could not find any counter-example, neither I was able to establish this proof. Can anyone help me in this regard ? If any work is done earlier, share the link plz if there be any.","['abelian-groups', 'group-theory', 'group-isomorphism', 'finite-groups']"
4501118,Solve trig equation: $\sin{(70^{\circ}-x)}=\frac{1}2 + \frac{1}{4\cos{(20^{\circ})}}$,"$$\text{Solve} \ \ \sin{(70^{\circ}-x)}=\frac{1}2 + \frac{1}{4\cos{(20^{\circ})}}$$ I came across this equation while solving a problem. The computer provides $x=20^{\circ}$ , and I tried to draw a shape with that in mind: Then I got the equation $\sin{(50^{\circ})}=2\sin{(20^{\circ})}\tan{(30^{\circ})}+\sin{(40^{\circ})}+\tan{(30^{\circ})}$ . Can this be somehow converted to $\frac{1}2 + \frac{1}{4\cos{(20^{\circ})}}$ ? Any help will be appreciated.","['trigonometry', 'euclidean-geometry', 'algebra-precalculus', 'geometry']"
4501166,Finding the greatest sum of 10 integers with a given set of constraints.,"This is a problem that comes up often in my job. Not as transparently as this (it's not a heavily-math focused job, haha), but at a fundamental level it does, and I was wondering if it can be generalised in any way. I'm not a very math-savvy person, so please bear with my phrasing of this like a textbook question: You have a linear sequence of 10 spaces labelled A through J. Into each space you may place 1 and only 1 counter. You have 4 types of counter: four counters marked ""2"", three counters marked ""3"", two counters marked ""4"", and two counters marked ""5"". You may place counters into each space as you see fit, as long as you respect the following rule: A counter marked with $x$ cannot be placed within $x$ spaces of another counter marked with $x$ . For example, if a ""2"" counter is placed in space A, no ""2"" counters can be placed in spaces B or C. If a ""2"" counter is placed in space C, no ""2"" counters can be placed in spaces A, B, D, or E. Finally, you are not required to place a counter in every space. Given the above constraints, what is the greatest possible value for the sum of the numbers marked on the counters placed in spaces A through J? Like I said, the problem in practice is not usually that transparent, so I'm more interested in whether the answer can be generalised in some way (like into a formula of some kind), or if it's too complex for that. Thank you for your time, and apologies in advance if I've mis-tagged this question.","['discrete-optimization', 'combinatorics']"
4501169,The infinite series $\sum_{k=-\infty}^\infty I_k(a)\operatorname{sinc}(k\phi)$,"While trying to solve the differential equation $$
a \frac{\partial^2y}{\partial a^2} + \frac{\partial y}{\partial a} -a y = e^{a\cos\phi}\sin\phi\;\;\;,\;\;\; y(0,\phi) = \phi
$$ during a calculation I was doing, I found an intriguing infinite series form of the solution: $$
y(a,\phi) = \phi\sum_{k=-\infty}^\infty I_k(a)\operatorname{sinc}(k\phi).
$$ This looks like the kind of thing that would have a ""known"" form, but no amount of massaging seemed to get Mathematica to give an answer (the above is in fact the result of said massaging--the initial form was much more complicated). Anyone have any ideas?","['bessel-functions', 'ordinary-differential-equations', 'sequences-and-series']"
4501170,Issues with integrating $\int x^2 \sqrt{x^3 +1}$ via integration by parts,"I want to integrate $$\int x^2 \sqrt{x^3 +1}~dx$$ I tried it with integration by parts (because we have a product here), but an online calculator did it with integration by substition. Would this still be correct? $$\frac{1}{3}x^3  (x^3+1)^\frac{1}{2} - \int \frac{1}{3} x^3 \frac{1}{2} (x^3+1)^{-\frac{1}{2}} \cdot 3~dx \\
= \frac{1}{3}x^3  (x^3+1)^\frac{1}{2} - \int x^3 \frac{1}{2} (x^3+1)^{-\frac{1}{2}} ~dx\\
= \frac{1}{3}x^3  (x^3+1)^\frac{1}{2} - \frac{1}{4} x^4\cdot 2(x^3 +1)^{-\frac{1}{2}} \\
= \frac{1}{3}x^3  \sqrt{x^3+1} - \frac{1}{2} x^4 \frac{1}{\sqrt{x^3+1}}$$ I think this is wrong because when $x=1$ I get a different result than when I insert $x=1$ into Can someone tell me where I went wrong and why we rather use integration by substition instead of integration by parts here?","['integration', 'indefinite-integrals', 'calculus', 'substitution']"
4501184,The pullback of a foliation,"I would like to know how to prove that the pullback of a foliation is actually a well-defined concept. To be precise, if $M$ is an $n$ -dimensional smooth manifold, a $k$ -dimensional foliation $\mathcal F$ of $M$ is a set of nonempty, connected, mutally disjoint, immersed $k$ -dimensional submanifolds of $M$ called the leaves of the foliation, such that their union covers $M$ and for each $p\in M$ there exists a flat chart for $\mathcal F$ , that is, a chart $(U,\varphi)$ of $M$ such that $\varphi(U)$ is a cube in $\mathbb R^n$ and the intersection of $U$ with each element of $\mathcal F$ is either the empty set or a countable union of slices of the form $x^{k+1}=c^{k+1},\dots,x^n=c^n$ . Lets recall that if $\mathcal F$ is a foliation of a smooth manifold $M$ and $f:N\rightarrow M$ is a smooth map transversal to the leaves of the foliation, then there exists a unique foliation on $N$ , the pullback of $\mathcal F$ , denoted by $f^*\mathcal F$ , such that $f$ becomes a foliation-preserving map (it maps leaves of $f^*\mathcal F$ into leaves of $\mathcal F$ ). I have seen how to apply the transversality of $f$ in order to construct a flat chart for $f^*\mathcal F$ , but I don't know how to prove that the connected components of $f^{-1}(L)$ , where $L$ is a leaf of $\mathcal F$ , are actually immersed submanifolds of $N$ . For instance, I don't know hot to prove that such sets are Hausdorff, etc., and I have not seen a proof of this fact anywhere. It may be a technicality, but proving that there are flat charts does not suffice to conclude that the given set is a foliation. Thanks in advance for your answers.","['foliations', 'smooth-manifolds', 'manifolds', 'differential-topology', 'differential-geometry']"
4501242,what is a distribution of $X$ given $X-Y>0$,"given two normally distributed variables $X\sim\mathcal N(\mu_X,\sigma_X)$ and $Y\sim\mathcal N(\mu_Y,\sigma_Y)$ that are independent $\rho_{XY} =0$ , what is a distribution of $X$ given $X>Y$ . I have run several simulations in Matlab and $X|X>Y$ looks suspiciously normally distributed, but what are its mean and standard deviation? Thank you for your help. EDIT: I agree with comments below. Although it looks suspiciously normally distributed, it definitely isn't.","['statistics', 'conditional-probability', 'probability-distributions', 'normal-distribution', 'probability']"
4501281,Closure operators coincide means that the topologies coincide,"Assume there are two topologies on a single space such that  the closure of any given set in either topology is the same. Is it true that the topologies are also the same? I think they are: assume there is an open set in the first topology that is not open in the second topology. Then, its complement in the first topology is closed, so the complement of the set is equal to its own closure in the first topology, which is equal to the complementâ€™s closure in the second topology. Hence, the complement is closed in the second topology, which means the complement of the complement is open in the second topology. But the complement  of the complement of a set is the set itself. Hence the set is open in the second topology and we have a contradiction. If this is false: what if we impose that the topological be sequential spaces?","['metric-spaces', 'analysis', 'solution-verification', 'sequences-and-series', 'general-topology']"
4501289,Find the maximum value of the integral $\int_{-1}^1|x-a|e^xdx$ where $|a|\le1$,"Question: Find the maximum value of the integral $\int_{-1}^1|x-a|e^xdx$ where $|a|\le1$ My Attempt: Let $f(a)=\int_{-1}^a(a-x)e^xdx+\int_{a}^1(x-a)e^xdx$ $f'(a)=\int_{-1}^ae^xdx+\int_{a}^1-e^xdx$ $f'(a)=e^a-e^{-1}-(e-e^a)$ For maximum, $f'(a)=0$ So, $2e^a-e-e^{-1}=0\implies 2e^a=e+\frac1e$ So, $e^a=\frac{e^2+1}{2e}\implies a=\ln\left(\frac{e^2+1}{2e}\right)$ But the answer given is: Max value $e+e^{-1}$ when $a=-1$ What's wrong in my approach? Edit: For derivative, I have used Leibniz Integral Rule","['integration', 'definite-integrals', 'maxima-minima', 'calculus', 'functions']"
4501293,"What is the difference, geometrically, between row vectors and column vectors?","I have been told in the textbook I'm following that ""By convention, always assume that vectors are in column orientation, unless stated otherwise."" (Cohen, 2021: 27). Fair enough. But as I progress into the study of vector spaces,this distinction between row and column vectors seems to take on more significance. In particular,I am introduced to fields and vector spaces and being told that ""The space $\mathbb{R}^n$ consists of all column vectors with $n$ components"". So is this different from having your space made of all row vectors with n components? Generally: what is the geometrical interpretation of the distinction between row vectors and column vectors? (As I understand it, algebraically, vectors are just ordered list of numbers. The different between row vectors and column vectors here only shows up in the outcome of basic vectors operations like the dot product.)","['linear-algebra', 'vectors']"
4501300,Quadratic Approximation for Log-Likelihood Ratio Processes,"I'm trying to understand why the quadratic equation can approximate the log likelihood ratio , and how it is derived: $$\mathrm{Log}(\mathrm{LR})=\frac{1}{2}\left(\frac{\mathrm{MLE}-\theta}{S}\right)^2$$ Is this approximated using Taylor's series or normal distribution equation or anything else? Using Taylor's expansion, I get $ -\frac{1}{2}(MLE-\theta)^2 (\frac{1}{\theta^2}+\frac{1}{MLE^2})$ , instead of $ -\frac{1}{2}(\frac{MLE-\theta}{S})^2$ This was brought up in book 'Essential Medical statistics' Chapter $28$ , the main goal was to derive a supported range (similar to the $95\%$ CI) for the likelihood ratio. It was mentioned in the book that the log of the likelihood ratio (LR) is used instead of the likelihood itself, because the $\log(\mathrm{LR})$ can be approximated by a quadratic equation (the one shown above), for easier calculation. It is also said that this equation is chosen so as to meet the curve of and to have the same curvature as the $\log(\mathrm{LR})$ at the MLE .","['statistics', 'quadratics', 'log-likelihood']"
4501326,"Munkres' ""Topology"" - Exercise 7(d) Section 9","This is the exercise: (c) Construct a sequence $\{A_n\}_{n \in \Bbb Z_+}$ of infinite sets such that $\forall n \in \Bbb Z_+, A_{n+1} \text{ has cardinality greater than } A_{n}$ . This is the definition of ""cardinality greater than"": Let $A.B \neq \emptyset$ . We say that $A$ has cardinality greater than $B$ if and only if there exists an injection mapping $B$ into $A$ and there is no injection mapping $A$ into $B$ . My attempt The natural choice is letting $A_1 = \Bbb Z_+$ and $A_{n+1} = \mathcal{P}(A_n)$ , where $\mathcal{P}$ denotes the power-set.
Now, my problem is that in order to do so we must define a function $h$ , by the Principle of Recursive Definition, such that: $$ \begin{align} h(1) &= A_1 \\ h(n+1) &= \mathcal{P}(A_n) \end{align}$$ however I am missing the codomain of such function (without which the definition is not well-posed) and I do not know how to construct it from the Principles of Naive Set Theory. As always, any hint or comment is welcome and let me know if I can explain myself clearer!","['elementary-set-theory', 'proof-writing', 'set-theory', 'recursion']"
4501344,"Exercise 4, Section 3.4 of Hoffmanâ€™s Linear Algebra","Let $V$ be a two-dimensional vector space over the field $F$ , and let $B$ be an ordered basis for $V$ . If $T$ is a linear operator on $V$ and $[T]_B=
\begin{bmatrix} a & b \\c & d\\ \end{bmatrix}$ . Prove that $T^2-(a+d)T+(ad-bc)I=0$ . My attempt: Itâ€™s easy to check $T^2-(a+d)\cdot T+(ad-bc)\cdot\text{id}_V\in L(V,V)$ . Let $B=\{\alpha_1,\alpha_2\}$ be basis of $V$ . Then $T(\alpha_1)=a\cdot \alpha_1+c\cdot \alpha_2$ and $T(\alpha_2)=b\cdot \alpha_1 +d\cdot \alpha_2$ . So \begin{align}
T^2(\alpha_1) &=T(T(\alpha_1)) \\
&=T(a\cdot \alpha_1 +c\cdot \alpha_2) \\
&=a\cdot T(\alpha_1)+c\cdot T(\alpha_2) \\
&=a\cdot (a\cdot \alpha_1+c\cdot \alpha_2)+c\cdot (b\cdot \alpha_1+d\cdot \alpha_2)\\
&=(a^2+cb)\cdot \alpha_1 +(ac+cd)\alpha_2.
\end{align} Then $(T^2-(a+d)\cdot T+(ad-bc)\cdot\text{id}_V)(\alpha_1)$ $=T^2(\alpha_1)-(a+d)\cdot T(\alpha_1)+(ad-bc)\cdot \text{id}_V(\alpha_1)$ $= [(a^2+cb)\cdot \alpha_1 +(ac+cd)\alpha_2]-(a+d)\cdot [a\cdot \alpha_1+c\cdot \alpha_2] +(ad-bc)\cdot \alpha_1$ $=[(a^2+cb)\cdot \alpha_1 +(ac+cd)\alpha_2]$$-$$[(a^2+ad)\cdot \alpha_1+(ac+cd)\cdot \alpha_2]$$+$$[(ad)\cdot \alpha_1-(bc)\cdot \alpha_1]$ $=0_V$ . Similarly $(T^2-(a+d)\cdot T+(ad-bc)\cdot\text{id}_V)(\alpha_2)=0_V$ . By theorem 1 section 3.1, $T^2-(a+d)\cdot T+(ad-bc)\cdot\text{id}_V=0_{L(V)}$ . Is my proof correct? Once can also show $(T^2-(a+d)\cdot T+(ad-bc)\cdot\text{id}_V)(\alpha)=0_V$ , $\forall \alpha \in V$ . But that is not efficient. Observation: See $\text{tr}([T]_{B})=a+d$ and $\text{det}([T]_B)=ad-bc$ . So $T^2-\text{tr}([T]_{B})\cdot T+ \text{det}([T]_B) \cdot \text{id}_V=0_{L(V)}$ . Can we generalize this problem?","['proof-writing', 'matrices', 'solution-verification', 'linear-algebra', 'linear-transformations']"
4501355,"Showing that $k^*(H_f)\lrcorner k^*\sigma=k^*(H_f\lrcorner\sigma)$for symplectomorphism $k$, contraction $\lrcorner$ and symplectic product $\sigma$","Showing that $k^*(H_f)\lrcorner k^*\sigma=k^*(H_f\lrcorner\sigma)$ for symplectomorphism $k$ , contraction $\lrcorner$ and symplectic product $\sigma$ Definitions: Let $f:\mathbb{R}^n\to \mathbb{R}^n$ be a smooth mapping and $Jf$ be the Jacobian of $f$ . Then if $\nu$ is an $m$ -form on $\mathbb{R}^n$ , the pullback of $\nu$ by $f$ is defined as $(f^*\nu)(w) = \nu(Jf w_1,\dots,Jf w_m)$ for $w \in (\mathbb{R}^n)^m$ . Let $\alpha = (x, \xi), \beta = (y, \nu) \in \mathbb{R}^{2n}$ . Then the symplectic product between $\alpha$ and $\beta$ is defined as $\sigma(\alpha, \beta) = \left<\xi, y\right> - \left<\nu, x\right>$ . Let $k:\mathbb{R}^{2n}\to \mathbb{R}^{2n}$ . Then $k$ is said to be a symplectomorphism , if $k^*\sigma = \sigma$ . Let $\nu$ is a differential $m$ -form and $X$ be a vector field. Then, the contraction of $\nu$ by $X$ is defined as the $(m-1)$ -form $(X\lrcorner \nu)(w) = \nu(X, w), w \in (\mathbb{R}^n)^{m-1}$ . Let $f$ be a smooth function defined on $\mathbb{R}^{2n}$ . Then the Hamiltonian vector field $H_f$ is a vector field such that $\forall w \in \mathbb{R}^{2n}:\sigma(w, H_f) = df(w)$ where $d$ is the differential 1-form. Preamble: I am trying to understand an intermediary step in a proof for Jacobi's theorem, that is $H_f = k_*(H_{k^*}f)$ , where one needs the equality $k^*(H_f)\lrcorner k^*\sigma = k^*(H_f\lrcorner \sigma)$ . If I write out the LHS I get $(k^*(H_f)\lrcorner k^*\sigma)(w) = k^*\sigma(k^*(H_f), w) = \sigma(k^*(H_f), w) = -\sigma(w, H_f \circ Jk)$ ( $\sigma$ is antisymmetric), where I am not sure how to either ""move"" the $Jk$ or argue why $H_f \circ Jk$ is necessarily a Hamiltonian vector field. The RHS evaluates nicely to $(k^*(H_f\lrcorner \sigma))(w) = (H_f\lrcorner \sigma)(Jk w) = \sigma(H_f, Jk w) = -\sigma(Jk w, H_f) = -df(Jk w)$ . Question: Is there a neat way to show the equality $k^*(H_f)\lrcorner k^*\sigma=k^*(H_f\lrcorner\sigma)$ ? If there is, what is it? My current approach radiates guess-and-check ness rather than intelligent observations.","['symplectic-geometry', 'linear-algebra', 'symplectic-linear-algebra', 'differential-forms', 'differential-geometry']"
4501384,"$a_0,a_1,\ldots,a_n$, a sequence of integers. Then $\prod\limits_{0\leq i<j \leq n} (a_j-a_i)$ is divisible by $\prod\limits_{1\leq i\leq n} i!$.","Let $a_0,a_1,\ldots,a_n$ be a sequence of integers.
Then $\prod\limits_{0\leq i<j \leq n} (a_j-a_i)$ is divisible by $\prod\limits_{1\leq i\leq n} i!$ . The same result for only $n!$ is straightforward.
I've tried to do it using induction in $n$ and then on the value of $n$ .",['discrete-mathematics']
4501419,Elementary proof for $\varphi(n)\ge\frac n{2\log(\log(n))}$,"Trying to derive an elementary bound for Euler totient function $\varphi(n)$ to be $\mathcal{O}(\frac n{\log(\log(n))})$ , I thought to prove a weak version of the well-known inequality $${\displaystyle \varphi (n)>{\frac {n}{e^{\gamma }\;\log \log n+{\frac {3}{\log \log n}}}}\quad {\text{for }}n>2}$$ for large integers $n\in\mathbb{N}$ . Simplest modification of the theorem would be considering $$n>e^{e^3} \implies 2\log(\log(n))>e^{\gamma }\;\log \log n+{\frac {3}{\log \log n}}$$ And hence deriving $\varphi(n)>\frac{n}{2\log(\log(n))}$ . I was then stuck trying to prove this. I've reached bounds of other orders in non-analytic methods (such as $\varphi(n)>\frac{6n}{\pi^2(1+\log(n))}$ and $\varphi(n)>\frac n{4\log(n)}$ ) but I've had no progress trying to prove bounds of this order. (Proving the bound with any other constant would also count, so it's basically enough to prove that $\varphi(n)\ge\frac n{c\log(\log(n))}$ for some $c\in\mathbb{R}^+$ for all $n>N$ where $N$ is a fixed positive integer, $e^{e^3}$ in my case) Any hints or ideas would be appreciated!","['number-theory', 'elementary-number-theory', 'euler-mascheroni-constant', 'totient-function', 'upper-lower-bounds']"
4501458,how to prove this ( about surjective functions and inverse functions),I  was making excercises for an upcoming exam and when trying to make this one I really got stuck. The question is: Prove that a function $f$ from $X$ to $Y$ is surjective if and only if there exists a function $g$ from $Y$ to $X$ with $g$ after $f$ is the unit function (the function that maps every $x$ on $x$ ). My thoughts directly went to the inverse function but than I started having doubts because if the domain is bigger than the co-domain the inverse function would have to have more than one value for every input and than g wouldn't be a function. My second thought was the same but with a reduced domain so that f becomes bijective but I do't think thats allowed in this question. Does someone know a way to solve/proof this question?,"['elementary-set-theory', 'functions']"
4501498,Why does calculating probability of 2 pairs seem so different from for a full house?,"For 5 card poker without discards/draws, the probability of being dealt a 2 pair hand is about 0.047539, and for a full house is about 0.001441. 1 I first learned about calculating for a 2 pair hand.  I incorrectly came up with: $${
{{13\choose 1}{4\choose 2}{12\choose 1}{4\choose 2}{44\choose 1}}
\over{52\choose 5}}={{(13)(6)(12)(6)(44)}\over2598960}\approx0.095078$$ Notably, my answer is exactly double the correct answer. I learned that the correct way to calculate this is to choose the 2 ranks together that will be the pairs: $${
{{13\choose 2}{4\choose 2}{4\choose 2}{44\choose 1}}
\over{52\choose 5}}={{(78)(6)(6)(44)}\over2598960}\approx0.047539$$ So, later, I moved on to calculating for a full house.  I proudly remembered to choose the ranks together and incorrectly came up with: $${
{{13\choose 2}{4\choose 3}{4\choose 2}}
\over{52\choose 5}}={{(78)(4)(6)}\over2598960}\approx0.000720$$ I learned that the correct way to calculate this is to choose the 2 ranks separately: $${
{{13\choose 1}{4\choose 3}{12\choose 1}{4\choose 2}}
\over{52\choose 5}}={{(13)(4)(12)(6)}\over2598960}\approx0.001441$$ Q1: For a 2 pair hand, why does choosing the ranks separately double count the possibilities?  Why must we choose the 2 ranks together (or divide by 2?) Q2: When we move onto a full house, why does this change?  Why does choosing the ranks together under count the possibilities?  Why must we choose the 2 ranks separately (or multiply by 2?) Q3: I'm having a surprisingly hard time grasping how to come up with these on my own.  Are there any tips or resources to help polish up these issues?","['discrete-mathematics', 'poker', 'combinatorics', 'probability']"
4501538,"Is it possible to evaluate $\int_{-\infty}^{\infty}e^{-x^2}\operatorname{sech}^2(\frac{x}{2})\,dx$?","I am curious to know if it is possible to evaluate the following integral: $$\int_{-\infty}^{\infty}e^{-x^2}\operatorname{sech}^2\left(\frac{x}{2}\right)\,dx$$ Here is the context: Someone had asked me about the error when using the trapezoidal rule on the integral $$I=\int_{0}^{1} \exp\left(-\ln^2\left(\frac{x}{1-x}\right)\right)\,dx= \int_{0}^{1} \exp\left(-\ln^2\left(\frac{x}{x-2}\right)\right)\,dx$$ I was however interested to potentially find a closed-form for this integral. Here are my attempts: After setting $u=\ln \left(\frac{x}{1-x}\right)$ one can show that $$I=\int_{-\infty}^{\infty} \frac{e^{u-u^2}}{(1+e^u)^2}\,du\stackrel{\text{IBP}}{=}-2\int_{-\infty}^{\infty} \frac{u e^{-u^2}}{1+e^u}\,du$$ Here I tried to use contour integration, however, the residue sum diverges rapidly, so I was stuck with this attempt. Splitting the integral at $0$ , and using $$\frac{1}{1+e^u} =\sum_{n=1}^{\infty} (-1)^{n-1} e^{-n u}$$ one can determine $$I=\sum_{n=1}^{\infty} (-1)^{n-1} n \sqrt{\pi} e^{n^2/4} \operatorname{erfc}\left(\frac{n}{2}\right)$$ but Iâ€™m not sure how to evaluate this series. Performing some substitutions on $I$ one can arrive at $$I=\int_{-\infty}^{\infty} x e^{-x^2} \tanh\left(\frac{x}{2}\right)\,dx\stackrel{\text{IBP}}{=}\frac{1}{4}\int_{-\infty}^{\infty} e^{-x^2} \operatorname{sech}^2\left(\frac{x}{2}\right)\,dx=\int_{-\infty}^{\infty}\frac{e^{-x^2}}{e^x+e^{-x}+2}\,dx$$ which appears like an integral that potentially contour integration can be applied to but I still arrive at the same rapidly-diverging residue series. I also arrived at the radically different form $$I=1-\frac{4}{\pi} \int_{0}^{\infty} \sin (t) \left(\Im \left(\psi \left(\frac{\sqrt{it}}{2\pi}\right)-2\psi\left(\frac{\sqrt{it}}{\pi}\right)\right)+\frac{\pi}{4}\right)\, dt$$ where $\psi$ is the digamma function. This doesnâ€™t appear too hopeful, however, since integrals of the form $\sin (x) \psi (x)$ generally donâ€™t have closed-forms and can only be expressed in terms of a logarithm series of similar form to my question on $\sum_{k=3}^{\infty} \frac{\ln (k)}{k^2-4}$ due to the results known about the Laplace transform of the digamma function, but perhaps the case $\sin (x) \psi (\sqrt{x})$ works out nicer- I will also accept some logarithmic series of similar form for $I$ . I found this question which asked about a generalised form of this integral.","['integration', 'analysis', 'real-analysis', 'contour-integration', 'sequences-and-series']"
4501565,Completeness of the codomain of a surjective continuous map implies completeness of the domain?,"Let $f:(X,d_X)\to (Y,d_Y)$ be continuous and surjective map between metric spaces such that $$d_X(x_1,x_2) \leq d_Y(f(x_1),f(x_2)).$$ Prove or disprove: If $X$ is complete then $Y$ is complete. If $Y$ is complete then $X$ is complete. The first is easy since, using the inequality, one can construct a Cauchy sequence in $X$ as the preimage of a Cauchy sequence in $Y$ and then use completeness of $X$ and continuity of $f$ . The second I think is false, but I am not able to think of a counter example. I know that the image of a Cauchy sequence is not necessarily Cauchy under only continuous maps (need uniform continuity). Any ideas how to solve 2. Thanks in advance!","['general-topology', 'metric-spaces', 'analysis', 'real-analysis']"
4501570,Injective bounded linear operator on Banach space maps non-dense set to dense set.,"Let $A\subseteq X$ be a subspace ( $X$ is a Banach space). Let $Y$ be another Banach space. Consider a continuous injective linear functional $f:X\rightarrow Y$ . Suppose $f(X)\subseteq \overline{f(A)}$ , is it true $X\subseteq \overline{A}$ ? I kind of feel this is obvious but cannot figure out a proof. (I guess this might be related to Baire Category theorem, but it seems open mapping theorem cannot be applied here.) The context of the problem is from the proof of Jacod Martingale representation theorem. $\mathcal{S}(A)\subseteq \mathcal{H}^2$ be the closed linear stable space generated by $A$ . The injection from $\mathcal{H}^2$ to $\mathcal{H}^1$ satisfying $\overline{i(S(A))}=\mathcal{H}^1$ . With the statement above, we can conclude $\mathcal{H}^2=\mathcal{S}(A)$ . Any comments or idea? Thanks in advance!","['functional-analysis', 'real-analysis']"
4501583,Is there a name for $(\det JJ^\top)^{1/2}$ where $J$ is a non-square Jacobian matrix?,"Background: Let $f: \mathbb R^n \to \mathbb R^m$ be a differentiable vector-valued function, with components $f_i: \mathbb R^n \to \mathbb R, \textrm{ for }1\le i\le m$ .  Then the Jacobian matrix is the $m \times n$ matrix $$\textbf{J} = \begin{bmatrix}
\dfrac{\partial f_1}{\partial x_1} & \cdots & \dfrac{\partial f_1}{\partial x_n} \\
\vdots & \ddots & \vdots \\
\dfrac{\partial f_m}{\partial x_1} & \cdots & \dfrac{\partial f_m}{\partial x_n} \end{bmatrix}$$ As is well-known, in the case in which $n = m$ the Jacobian matrix is square, and its determinant, the Jacobian determinant , is the scalar $J = \det \textbf{J}$ . The Jacobian determinant has many uses, including when using a substitution during integration. But what if $m \ne n$ ? In this case, $\textbf{J}$ is not square, so $\det \textbf{J}$ is not defined.  However, in this case, the quantity $\left(\det \textbf{J} \textbf{J}^\top \right)^{1/2}$ is still meaningful, and generalizes the idea (and some of the uses) of the Jacobian determinant: For example, consider a function $\vec r: \mathbb R^2 \to \mathbb R^3$ , which we can regard as a parametrization of a surface in $\mathbb R^3$ . If we write $\vec r(u,v) = \langle x(u,v), y(u,v), z(u,v) \rangle$ , then the Jacobian matrix is $\textbf{J} = \begin{bmatrix} x_u & y_u & z_u \\ x_v & y_v & z_v \end{bmatrix}$ ; in this situation it can be shown that $\left(\det \textbf{J} \textbf{J}^\top \right)^{1/2} = \| \vec r_u \times \vec r_v \|$ .  We recognize the quantity that relates area in the surface to area in the domain: $dS = \| \vec r_u \times \vec r_v \| \, dA$ . On the other hand consider a function $\vec r: \mathbb R^1 \to \mathbb R^n$ , which we can interpret as a parametrization of a path in $\mathbb R^n$ .  If we write $\vec r(t) = \langle x_1(t), x_2(t), \dots, x_n(t) \rangle$ then in this case $\left(\det \textbf{J} \textbf{J}^\top \right)^{1/2} = \left| \frac{d\vec r}{dt} \right|$ . Once again we recognize the quantity that relates arc length in the path to length in the domain: $ds = \left| \frac{d\vec r}{dt} \right| \, dt$ . Finally we note that if $n = m$ then $\left(\det \textbf{J} \textbf{J}^\top \right)^{1/2}$ is identical to the ""usual"" Jacobian determinant $J$ . My question: Does the quantity $\left(\det \textbf{J} \textbf{J}^\top \right)^{1/2}$ have a standard name in the case of a non-square Jacobian matrix $\textbf{J}$ ?  The phrase ""Jacobian determinant"" seems, as far as I can tell, to only be used in the context of square matrices, despite the fact that the quantity generalizes quite naturally to the non-square case, and encompasses several ""special formulas"" in a single neat form.  But I am unaware of any established terminology for this quantity.  Is there one?","['jacobian', 'multivariable-calculus', 'terminology']"
4501588,Evaluate $\lim\limits_{n\to\infty}\big(\frac{n}{2}+\min\limits_{x\in\mathbb{R}}\sum_{k=0}^n{\cos(2^k x)}\big)$,"What is the exact value of the following limit? $$L=\lim_{n\to\infty}\left(\frac{n}{2}+\min_{x\in\mathbb{R}}\sum_{k=0}^n{\cos(2^k x)}\right)$$ Experimenting on desmos suggests the following claims: $\sum_{k=0}^n{\cos{(2^k x)}}$ is minimized when $x\approx \left(2m\pm\dfrac{2}{3}\right)\pi,m\in\mathbb{Z}$ , with the approximation approaching equality as $n\to\infty$ $L\approx -0.704$ I do not know how to prove these claims. (This question was inspired by another question .)","['limits', 'trigonometry', 'maxima-minima', 'real-analysis']"
4501696,"In a group $G$, if every elements are of finite order then $G$ is at most countable.","Let us consider a group $G$ . If $G$ is a finite group then every element of $G$ has finite order. But the converse need not be true, i.e., every elements in a group has finite order doesn't imply the group is finite. Examples: The factor group $\mathbb{Q}/\mathbb{Z}$ , the PrÃ¼fer $p$ -group etc. (all these groups are countable). Conjecture $1$ : In a group $G$ , if every element has finite order
then $G$ is at most countable. Conjecture $2$ : Every uncountable groups contain an infinite cyclic
group. Let $S$ is a countable subset of $G$ (possible because of Axiom of Choice). Since $G$ is not countably generated i.e $G\neq \langle S\rangle$ , $\exists g_1\in G$ such that $g_1\notin \langle S\rangle$ . Again $\langle S, g_1\rangle$ is countable implies $G\neq \langle S, g_1\rangle $ .Hence $\exists g_2\in G$ such that $g_2\notin \langle S, g_1\rangle $ $\vdots$ Hence $\exists (g_n) \in G$ such that $g_{n+1}\notin \langle S,  g_1,\ldots,g_n\rangle $ Let $H=\{g_n\}_{n\in\Bbb{N}}$ Claim: $\langle H\rangle\cong \mathbb{Z}$ I have no way to proceed further. Is it possible to define an isomorphism between $H$ and $\mathbb{Z}$ ? Conjecture $1$ and $2$ are equivalent. I want to prove the second one using the fact that ""an uncountable group can't be countably generated"". One final thought : Does $ G=\Pi_{i\in \Bbb{N}} \mathbb{Z}_2$ provide a counter example?","['elementary-set-theory', 'group-theory', 'abstract-algebra', 'infinite-groups']"
4501711,Proving a field is algebraically closed with algebraic geometry,"There's a fun proof of the fundamental theorem of algebra that goes like this: Let $\mathbb{C}_n[X]$ be the complex vector space of polynomials of degree $\leq n$ in a variable $X$ and consider the holomorphic map $$
f : \prod_{j=1}^n \mathbb{P}(\mathbb{C}_1[X]) \to \mathbb{P}(\mathbb{C}_n[X]),
\quad
([p_1], \ldots, [p_n]) \mapsto [p_1 \cdots p_n]
$$ between two compact complex manifolds. By inspection in well-chosen charts, $df$ is injective on a dense set and thus surjective as well because the tangent bundles have the same dimension. The map $f$ is therefore regular there, and its fiber at such a point has $|S_n| = n!$ points, so $f$ is finite. Finally, a finite holomorphic map between compact complex manifolds of the same dimension is surjective, either by general differential topology facts about finite maps and the orientability of complex manifolds, or by Grauert and Remmert's open mapping theorem and standard topological arguments involving compactness and Hausdorff spaces. Therefore any polynomial of degree $n$ splits into linear factors, so $\mathbb{C}$ is algebraically closed. There are of course algebraicially closed fields $k$ that are not the complex numbers. Question: Suppose we do the same thing but over an arbitrary field $k$ . Then we have a finite morphism between algebraic varieties. Is there a property of $k$ (other than "" $k$ is algebraically closed"") that lets us conclude that the algebraic morphism $f$ is surjective? I don't know enough algebraic geometry (or algebra, honestly) to say much about what happens in the general case. When $k = \mathbb{R}$ it's easy to prove directly that this map isn't surjective for $n = 2$ by considering a kind of projectivized version of the discriminant of a polynomial that takes values in $\{\text{positive}, 0, \text{negative}\}$ and noting that the image of $f$ never takes a negative value. That argument doesn't work for (for example) a finite field, which is also not algebraically closed.","['field-theory', 'algebraic-geometry', 'abstract-algebra']"
4501723,Multiplicity in restriction of representations,"Let $H\subset G$ be groups. Let $\pi$ be an irreducible complex representation of $G$ and let $\sigma$ be an irreducible complex representation of $H$ . The multiplicity of $\sigma$ in the restriction of $\pi$ to $H$ is $$\dim \operatorname{Hom}_H(\sigma,\pi).$$ Sometimes in the literature, I see the notation $$\dim \operatorname{Hom}_{H} (\pi\otimes \sigma,\mathbf{C})$$ where $H$ is diagonally embedded in $G\times H$ . In which cases are those two equal ? I think that $\operatorname{Hom}_H(\pi\otimes \sigma,\mathbf{C})=\operatorname{Hom}_H(\pi,\sigma^\vee)$ , so when $\sigma$ is self-dual, it is the case. This paper says that if $F$ is a non-archimedean local field of residue characteristic $\neq 2$ , then an irreducible (admissible) representation of a (special) orthogonal group $O(V)$ or $SO(V)$ is self-dual. Are there any other cases ? What can be said about this in general ? Any help is appreciated.","['group-theory', 'representation-theory']"
4501736,Is there any example of a real-analytic approach to evaluate a definite integral (with an elementary integrand) whose value involves Lambert W?,"I have never seen a real-analytic approach before to evaluate integrals of the form below $$\int_a^b\text{elementary function}(x)\,dx=\text{constant involving}\,W(\cdot)\,\text{in its simplest form}\tag1.$$ For instance, on MSE, all use the residue theorem: $\int_{-\infty}^{\infty}{e^x+1\over (e^x-x+1)^2+\pi^2}\mathrm dx=\int_{-\infty}^{\infty}{e^x+1\over (e^x+x+1)^2+\pi^2}\mathrm dx=1$ Interesting integral related to the Omega Constant/Lambert W Function Prove that $\int_0^\infty \frac{1+2\cos x+x\sin x}{1+2x\sin x +x^2}dx=\frac{\pi}{1+\Omega}$ where $\Omega e^\Omega=1$ Proof for integral representation of Lambert W function Evaluate $\int_{0}^{\infty} \ln(1+\frac{2\cos x}{x^2} +\frac{1}{x^4}) \, dx$ And the same applies to some of the wider literature I have come across: Stieltjes, Poisson and other integral representations for functions of Lambert W An Integral Representation of the Lambert $W$ Function Note: the proof is real-analytic, but the very first line assumes the validity of an integral identity which was only proven using complex analysis (Hankel contour). So, my question is this: Does anyone know of a proof of an identity of the form in $(1)$ that involves only real analysis (i.e. does not assume the existence of $\sqrt{-1}$ )?","['integration', 'definite-integrals', 'real-analysis', 'lambert-w', 'elementary-functions']"
4501764,Did Newton and Leibniz use limits in their derivations of differential calculus?,"In modern treatments of calculus, limits are used to motivate the derivation of differential calculus. However, when I searched for the history of limits I came across this Wikipedia article , which says: ...the modern idea of the limit of a function goes back to Bolzano who, in 1817... If the modern idea of limits is dated to 1817, what kind of limits did Newton and Leibniz use if any?","['limits', 'calculus', 'math-history', 'derivatives']"
4501815,"How to find this two-variables limit of $f$ as $(x,y)$ approaches to $(0,0)$?","I am trying to evaluate the limit $$\lim_{(x,y) \to (0,0)}\frac{2x^6-y^{10}}{x^2-y^3}$$ I have seen that both the reiterated limits are zero. Moreover, I have evaluated the limit by different curves such as $y=x^2$ and the limit equals zero as well. However, the polar coordinates $$\lim_{r \to 0}\frac{r^4(2\cos^6(\theta)-r^4\sin^{10}(\theta))}{\cos^2(\theta)-r\sin^3(\theta)}$$ make me think that the limit does not exist because it depends on the $\cos^2\theta$ factor, but I don't know how to prove it or which curve to use.","['limits', 'multivariable-calculus', 'continuity']"
4501876,At what point do two lines extending from the end of a third equal each other?,"Depicted are three quarter circles with the same center point and different radii. The smaller circle has a radius of 2 (pink line), the mid of 4, and the largest of 5. From the end of the pink line, 2 perpendicular lines extend out. One (blue line) is bound by the quarter circle with radii 4. The other (red line) is bound by quarter circle of radii 5. As the pink line sweeps from 0 to 90 degrees, the blue line goes from its shortest length (2) to its longest length (3.46410161514) and the red line goes from its longest length (4.58257569496) to its shortest (3). My question is at what point in the pink lines sweep from 0 to 90 degrees does the length of the blue line and the length of the red line equal each other? The question is aimed to answer a math problem from this video: https://youtu.be/iszucjbTh5I The video offers a solution, but I haven't watched it and want to solve it differently. My math knowledge is at a pre-calc level. I have good trig familiarity and I do a lot of recreational math. My current intuition is to ascertain two functions that give a curve that represents the increase and decrease of the red and blue lines based on a degree value between 0 and 90. Then I'd set the two functions equal to each other and solve for sine of X. I could be way off in my approach but that is where I am at. Any help or suggestions would be greatly appreciated.","['trigonometry', 'functions']"
4501895,How to use $\gtrsim$ in integrals?,"Notation: Let $\{x_n\}$ and $\{y_n\}$ be two real non-negative sequences such that $y_n\neq 0$ for sufficiently large $n$ , then $x_n \lesssim y_n$ if $\exists C>0$ such that $x_n\leq C y_n$ for sufficiently large $n$ . In a paper I am reading it states that $$
\int_1^2 \exp\left(-\xi\right) \left(\frac{\xi}{R_n^2+2\xi }\right)^{b-1/2}\left(\frac{1}{R_n^2+2\xi }\right)^{a_n+1/2}d\xi\gtrsim (R_n^2+2)^{-b+\frac{1}{2}}(R_n^2+4)^{-a_n-\frac{1}{2}}
$$ where $a_n\rightarrow 0, b>1$ and $R_n$ is nondecreasing. Why is this true? Do I just substitute $\xi=2$ in the integral?","['integration', 'calculus', 'sequences-and-series']"
4501922,Rietz representation theorem for local statements of theorems,"I've known about Rietz representation theorem for some time, but I've always wondered if I got it right or I am missing something about local statements of theorems which make use of it. The Riesz theorem (let me state it for $\mathbb{R}$ not to complicate things) says that the dual of $C_0 (\mathbb{R})$ is the set $\mathcal{M} (\mathbb{R})$ of finite Radon measures on $\mathbb{R}$ , where $C_0$ denotes the functions vanishing at infinity. So $$(C_0 (\mathbb{R}))^* = \mathcal{M} (\mathbb{R}) $$ The main advantage here is that for a bounded sequence of finite Radon measures (with respect to the total variation if I am not mistaken), we have weak-star compactness by standard functional analysis results. Really nice. Now, very often it's used (implicitly) another version of the theorem, which basically says $$ (C_c (\mathbb{R}))^* = \mathcal{M}_{loc} (\mathbb{R}),$$ where $C_c (\mathbb{R})$ denotes the set of continuous functions supported in a compact set and $\mathcal{M}_{loc}$ . And so the weak-star convergence here is with respect to the functions in $C_c$ . The question is: are these two always interchangeable? I mean, the first veriosn provides a sort of more ""global"" behavior, the second one a completely local one. After all, the restriction of a locally finite Radon measure to a compact set is a finite Radon measure. I am asking this because many times in various courses there is a theorem and in the book it talks about finite Radon measures, while in class it is proved for locally finite Radon measures (one example that comes to my mind now are, Reshetnyak semicontinuity/continuity theorems). I know it's not a big deal but I want to be sure once and for all. Is it really true that one can take a theorem which uses finite Radon measures weak-star converging (with respect to functions in $C_0$ ) to something and simply change the statement by taking locally Radon measures (and weak-star convergence with respect to $C_c$ ) to have a local version of the theorem? It sounds  quite right and it honestly sounds almost trivial, but the devil is in the details and maybe I'm missing something.","['measure-theory', 'functional-analysis', 'analysis', 'real-analysis']"
4501941,Alice and Bob sometimes lie; a die is thrown and they claim different results; what is the probability that Bob is being honest?,"Alice speaks the truth with probability $3/4$ and Bob speaks the truth with probability $2/3$ . A die is thrown and both Alice and Bob observe the number. Afterwards, Alice asserts to Carl (who does not know the number) that the number is $3$ while Bob says (to Carl) the number is $1$ . Find the probability that the number is actually $1$ . UPDATE: To clear ambiguity, note that if person decides to lie, he/she will choose a false answer randomly from all the possible false answer ( $\{1, 2, \cdots, 6\}$ - $\{\text{The number that actually showed up}\}$ ). Also, a die is thrown, and then both Alice and Bob will see the number. Then, they will lie/say truth accordingly. My attempt: Case $1$ : Number $1$ showed up. Chance of all this happening = $\frac{1}{6} \cdot \frac{2}{3} \cdot \big(\frac{1}{4} \cdot \frac{1}{5}\big) = \frac{1}{180}$ Case $2$ : Number $3$ showed up Chance of all this happening = $\frac{1}{6} \cdot \frac{3}{4} \cdot \big(\frac{1}{3} \cdot \frac{1}{5}\big) = \frac{1}{120}$ Case $3$ : Other number showed up Chance of all this happening = $\frac{4}{6} \cdot (\frac{1}{4} \cdot \frac{1}{5}) \cdot (\frac{1}{3} \cdot \frac{1}{5}) = \frac{1}{450}$ So, total = $\boxed{\frac{\frac{1}{180}}{\frac{29}{1800}} = \frac{10}{29}}$ Is my attempt correct? If not, how to do this problem?",['probability']
4501955,Estimating variance of population with variance of sample mean,"Suppose a population has a given random variable $X$ with unknown mean $\mu$ and unknown variance $\sigma^2$ . Sampling theory tells us that for a sample of size $n$ , $E(\bar{X}) = \mu$ and $V(\bar{X})=\frac{\sigma^2}{n}$ . It is well known that $E(\bar{X})$ is an unbiased estimator of $\mu$ . To estimate the parameter $\sigma^2$ , how bad of an estimator is $n V(\bar{X})$ ?","['statistics', 'sampling', 'estimation']"
4501957,Hard integral $\int_{0}^{\frac{\pi}{4}} \ln ^{n}(\tan \theta) d \theta$?,"In my post ,
I had found an elegant integral $$\int_{0}^{\frac{\pi}{4}} \ln (\tan x)~ d x=-G. $$ I then try to generalise the integral $$
I_{n}=\int_{0}^{\frac{\pi}{4}} \ln ^{n}(\tan \theta) d \theta, ~~~~\textrm{ where }n\in N,
$$ by letting $e^{-x}=\tan \theta$ . Consequently $-e^{-x} d x=\left(1+e^{-2 x}\right) d \theta$ converts the integral to $$
\begin{aligned}
I_{n} &=\int_{\infty}^{0} \frac{\ln ^{n}\left(e^{-x}\right)\left(-e^{-x}\right)}{1+e^{-2 x}} d x =\int_{0}^{\infty} \frac{(-x)^{n} e^{-x}}{1+e^{-2 x}} d x
\end{aligned}
$$ Expanding the integrand into a power series yields $$
\begin{aligned}
I_{n} &=(-1)^{n} \int_{0}^{\infty} \sum_{k=0}^{\infty}(-1)^{k} x^{n} e^{-(2 k+1) x} d x\\&=(-1)^{n} \sum_{k=0}^{\infty}(-1)^{k} \int_{0}^{\infty} x^{n} e^{-(2 k+1) x} d x
\end{aligned}
$$ Letting $(2k+1)x\mapsto x$ transforms the integral into a Gamma function. $$
\\ \boxed{\int_{0}^{\frac{\pi}{4}} \ln ^{n}(\tan \theta) d \theta =(-1)^{n} \sum_{k=0}^{\infty} \frac{(-1)^{k}}{\left({2 k+1}\right)^{n+1}} \int_{0}^{\infty}x^ne^{-x} d x=(-1)^{n} \beta(n+1) \Gamma(n+1)}
$$ where $\beta(s)$ is the Dirichlet beta function. For examples, $$
\begin{array}{l}
\displaystyle  I_{1}=\int_{0}^{\frac{\pi}{4}} \ln (\tan \theta)d\theta=-\beta(2) \Gamma(2)=-G \\
\displaystyle  I_{2}=\int_{0}^{\frac{\pi}{4}} \ln ^{2}(\tan \theta) d\theta =\beta(3) \Gamma(3)=\frac{\pi^{3}}{32} \cdot 2=\frac{\pi^{3}}{16} \\
\displaystyle  I_{10}=\int_{0}^{\frac{\pi}{4}} \ln ^{10}(\tan \theta) d\theta =\beta(11) \Gamma(11)=\frac{50521\pi^{11}}{14863564800}\times 10!= \frac{50521 \pi^{11}}{4096} \approx 3.62878 \times 10^{6}
\end{array}
$$ Looking forwards to getting more alternate solutions and opinions from you!","['integration', 'improper-integrals', 'definite-integrals', 'calculus', 'catalans-constant']"
4501975,Finding $a$ and $b$ for the given polynomial,"Question: All the roots of $x^4 â€“ 12x^3 + ax^2 + bx + 81 = 0$ are non-negative. The ordered pair $(a, b)$ can be? Options: A) $(9,36)$ B) $(27,-108)$ C) $(54,-108)$ D) $(36,108)$ Here. I eliminated option (A) and (D) as $b$ must be negative. But I was unable to calculate the value of $a$ as it required solving quite complex equations which were beyond my scope. Following were the equations: $\alpha +\beta +\gamma +\delta = 12$ $\alpha\beta + \alpha\gamma + \alpha\delta + \beta\gamma + \beta\delta + \gamma\delta = a$ $\alpha \beta \gamma +\alpha \beta \delta + \alpha \gamma \delta + \beta \gamma \delta = -b$ $\alpha \beta \gamma \delta = 81$ This must not be the way to go about finding $a$ but I can't think of anything else. Please help. Answer: Option (C)","['contest-math', 'roots', 'calculus', 'polynomials', 'algebra-precalculus']"
4502000,Using gradient descent in probability case,"Suppose we have i.i.d. samples $x_i\sim N(0,\Sigma)$ and $y_i\sim x_i^T\omega^*+\xi_i,\xi_i\sim N(0,1)$ where $\omega^*$ is the fixed point of: $$\omega_{i+1} = \omega_i âˆ’ \eta\nabla_\omega f(\omega_i, x_i
, y_i), \quad \omega_0 = 0$$ where $f(\omega, x_i, y_i) = \frac{1}{2}|y_i âˆ’ \omega^T x_i|^2 $ and $\eta$ is the step size. Formulate an appropriate range
of $\eta$ so that $\omega_i$ will become an ergodic process. Also find mean and covariance under the equilibrium distribution. So, I tried to calculate it and got another form of the expression: $$\omega_{i+1}-\omega^*=(I-\eta x_i x_i^T)(\omega_i-\omega^*)+\eta\xi_ix_i$$ which could help to directly express $\omega_i$ by a successive multiplication of $(I-\eta x_i x_i^T)$ and $\omega^*$ . Then I wanted to calculate $\frac{\sum \omega_i}{n}$ to get the ergodicity, but I didn't make it.","['stochastic-processes', 'gradient-descent', 'ergodic-theory', 'probability-theory']"
4502013,Functionals in Banach-*-algebras,"I would like to know if every bounded linear functional $\varphi :A \to \mathbb{C}$ of a Banach-*-algebra $A$ is a linear combination of positive ones, i.e. of functionals $\varphi:A \to \mathbb{C}$ satisfying $\varphi(a^*a)\ge 0$ .
For $C^*$ -algebras this comes from a theorem of Gelfand-Naimark, the Riesz-Markow representation theorem which identifies these functionals with complex Borel measure, as well as the Hahn-Jordan-decomposition of measures. Otherwise, is there maybe such a result for functionals of the type $\varphi(a^*)=\overline{\varphi(a)}$ ? Best regards,
Dominik","['banach-algebras', 'functional-analysis']"
4502055,Show that these two sum forms are equal,"I am trying to prove the following lemma (related to formal derivatives for reference): Let $R$ be a ring, $k\in \mathbb{Z}^+ (\text{this means }k\geq 1)$ , and $f_0,\dots,f_k,g_0,\dots,g_k\in R$ . Then $$k\sum_{l=0}^{k}f_lg_{k-l} = \sum_{l=0}^{k-1}\Big[(k-l)f_lg_{k-l}+(l+1)f_{l+1}g_{k-l-1}\Big].$$ I am currently trying induction and this result certainly holds for k=1 and k=2. I am currently lost in the inductive step and suspect there may be some substitution trick I am unaware of. ( Current Attempt ) In short, I've tried the following: If we assume the result holds for $k=n\geq 1$ , then $k=n+1\Rightarrow$ $$RHS = \sum_{l=0}^{n}\Big[(n+1-l)f_lg_{n+1-l}+(l+1)f_{l+1}g_{n-l}\Big] \text{ and}$$ $$LHS = (n+1)\sum_{l=0}^{n+1}f_lg_{n+1-l}.$$ If you algebraically manipulate the RHS, you can see that most of the LHS cancels with it. Not sure if this is the best approach if I plan to use the inductive hypothesis. With this route and working backwards, I currently have that $$LHS=RHS \leftrightarrow (n+1)f_{n+1}g_0 = \sum_{l=0}^n\Big[(l+1)f_{l+1}g_{n-l} - lf_lg_{n+1-l}\Big].$$ Currently looking for advice or key insights, potentially a better strategy.","['algebra-precalculus', 'summation']"
4502059,A weakly locally path connected space that is not locally path connected,"I am reading through an article by Shelah where it has a definition for a weakly locally path connected space: Say that $X$ is weakly locally path connected (WLPC) if for every $x\in X$ and every neighborhood $u$ of $x$ , there exists a neighborhood $v$ of $x$ in $u$ such that every point in $v$ can be joind to $x$ by a path through $u$ . Also I know the definition of locally path connectedness as ""for every $x \in X$ and every neighborhood $u$ of $x$ , there is a path connected neighborhood $v$ of $x$ contained in $u$ ."" I wonder what is an example of a space that is not locally path connected but it is weakly locally path connected. I gave it some thought for several hours but I couldn't come up with any examples.","['locally-connected', 'general-topology', 'path-connected', 'connectedness']"
4502080,Continuous functions satisfying $f(x) \left( f(x) - \frac1x \int_0^x f(t) dt \right) \geq (f(x) - 1)^2$,"Find all continuous functions $f: [0, \infty) \to \mathbb{R}$ such that, for all positive $x$ \begin{equation}
f(x) \left( f(x) - \frac1x \int_0^x f(t) dt \right) \geq (f(x) - 1)^2
\end{equation} From the inequality, one can deduce $f(0) = 1$ , but from there it's not clear how to proceed. Some hints would be greatly appreciated!","['integration', 'inequality']"
4502089,Textbook for graduate analysis over a general complete valued field?,"At my university, the standard sequence of first-year graduate analysis courses is taken by students who plan to go into analysis, differential equations, and applied math. These students know why they have to learn analysis. The first-year graduate analysis courses are also taken by students who plan to go into algebraic topology, algebraic number theory, and algebraic geometry. These students sometimes are disinterested in learning classical analysis (over the real or complex numbers), because their perspective is not yet broad enough to see the fundamental importance of analysis and/or how it is related to the subjects they plan to specialize in. My efforts to explain to these students why they ought to care about analysis have generally not been successful. My colleagues and I are considering the possibility of running our first-year graduate analysis courses as courses in analysis over a general complete valued field. Over the archimedean fields, we would recover classical analysis over the real and complex numbers; over the non-archimedean fields, we would recover many cases of interest to the students who plan to study algebra, particularly analysis over p-adic fields. We hope the resulting courses would be of interest and of use to all our students. But it is not yet clear to us that this idea makes sense; perhaps some topics (e.g. integration theory) are simply too different in the non-archimedean case to treat them, at this level, alongside the classical treatment. There are many good references on functional analysis which work over non-archimedean fields, or which allow the base complete valued field to be archimedean or non-archimedean. However, I do not know of a graduate analysis textbook (not just functional analysis, but rather roughly the contents of a standard first year of graduate analysis) which works over a general complete valued field. Do you know of any such textbook? Thanks.","['analysis', 'reference-request']"
4502091,Expected value of the maximum of $3$ normal variables,"I want to prove that for $3$ variables $X_1,X_2,X_3$ from normal distribution with $\mu =0,\sigma =1$ for each, $\mathbb{E}(\max \{X_1,X_2,X_3\})\approx 0.85$ . I found the following formula: $\mu +\frac{3}{2\sqrt{\pi}}\sigma$ , which shows this, but I do not know how to prove it. (It would be sufficient to prove it only for $\mu =0,\sigma =1$ ). In addition I need to prove for a general $k\in \mathbb{N}$ that $\mathbb{E}(\max \{X_1,X_2,\ldots ,X_k\})<\mathbb{E}(\max \{X_1,X_2,\ldots ,X_{k+1}\})$ . How can this be shown? Thank you!","['statistics', 'order-statistics', 'normal-distribution', 'probability']"
4502097,How much truth does Hensel's Analogy really hold?,"F. GouvÃªa's "" $p$ -adic Numbers. An Introduction"" motivates the titular object in mainly two different ways. First, there is the idea of succesively lifting solutions modulo prime powers formalised by Hensel's Lemma and, second, there is something he dubs Hensel's Analogy . Let me expand on the latter (which is more or less the table on page $1$ here ). Recall that any positive integer $n$ can be expanded into powers of any fixed prime number $p$ defining its $p$ -adic expansion. More explicitly, there are non-negative integers $a_k$ for $k=0,\dots,m$ so that $0\le a_i<p$ and $$
n=\sum_{k=0}^m a_kp^k\,.
$$ Put differently, any positive integer is expressible as a polynomial in $p$ with coefficients between $0$ and $p-1$ . It then seems natural to ask whether this can be extended to all (positive) rational numbers by somewhat simplifying the resulting fraction of polynomials. Drawing from complex analysis we know that for polynomials $P(z),Q(z)\in\mathbb C[z]$ we can expand $$
\frac{P(z)}{Q(z)}
=\sum_{k=-m}^\infty a_k(z-\alpha)^k
$$ for, say, $\alpha$ a root of $Q(z)$ . This is the Laurent series of the rational function. Jumping ahead, the $p$ -adic numbers $\mathbb Q_p$ give a way of making sense of such infinite series in powers of the fixed prime $p$ corresponding to the ""local"" study at $p$ . The crucial technical part here is to introduce a sensible notion of convergence to allow infinite power series in place of mere polynomials. We arrive then at $\mathbb Q_p$ having as elements $$
\sum_{k=-m}^\infty a_kp^k
$$ with integers $0\le a_k<p$ . This naturally extends the $p$ -adic expansions we are used to while simultaneously introducing a very useful and powerful number theoretic tool. However, I was wondering: Is there more to Hensel's Analogy then formal similarities? Conceptually, studying the local fields $\mathbb Q_p$ is a way of focusing on the prime $p$ . For example, any number field $K/\mathbb Q$ has a corresponding local extensions $K_{\mathfrak p}/\mathbb Q_p$ with $\mathfrak p\mid p$ in which we can more easily study the, say, ramification behavior of $K/\mathbb Q$ at $p$ . So the $p$ -adic numbers provide a framework for the ""local"" study near $p$ similar to what a Laurent series expansion does for a rational function (which is also an idea close to algebraic geometry). Hence the analogy seems to hold some truth. But how much truth does Hensel's Analogy really hold? That is, what non-trivial techniques or results (if any) can be carried over from complex analysis to number theory? What can be said besides the things which follow from the formal equity to the Laurent series? Thanks in advance!","['complex-analysis', 'algebraic-number-theory', 'p-adic-number-theory']"
4502103,Give combinatorial a proof that $\binom{n}{2}^2=\binom{n}{2}+6\binom{n}{3}+6\binom{n}{4}$,"Give combinatorial a proof that $$\binom{n}{2}^2=\binom{n}{2}+6\binom{n}{3}+6\binom{n}{4}$$ I firstly thought it like classical commission questions consisting of $n$ men and $n$ women etc. However , i did not work. After that   , i thought about manipulating the question such that $$\binom{n}{2}^2=\binom{n}{2}+6\binom{n}{3}+6\binom{n}{4}$$ $$\binom{n}{2}\binom{n}{n-2}=\binom{n}{2}+6\binom{n+1}{4}$$ However , i cannot still find anything. Can you help me ? NOTE: Although @Phicar's answer get credit from you , i could not comprehend it :( So , i am open to other proofs. NOTE 2: Do not write ""HINTS"" , if you want to write somethings , please be clear and elaborately","['binomial-coefficients', 'combinatorics', 'combinatorial-proofs']"
4502120,Actions on Bernoulli space have almost the same orbits,"Let $(\mathbb{B}, \nu)$ be the binary space with probability measure $\nu(0) = \nu(1) = \frac{1}{2}$ . The map $T : \mathbb{B}^{\mathbb{N}} \to \mathbb{B}^{\mathbb{N}}$ is defined as left addition with carry, i.e. if $x = (1, \ldots, 1, 0, x_k, \ldots)$ then $T(x) = (0, \ldots, 0, 1, x_k, \ldots)$ . This map is a measure preserving Borel isomorphism. We can define two actions on the product space $(\mathbb{B}^{\mathbb{N}}, \nu^{\otimes \mathbb{N}})$ as follows: $\mathbb{B}^{\mathbb{N}} \curvearrowright \mathbb{B}^{\mathbb{N}}$ through componentwise addition modulo 2. $\mathbb{Z} \curvearrowright \mathbb{B}^{\mathbb{N}}$ through $k \cdot x := T^k(x)$ . Show that these actions are orbit equivalent, in other words, there exists a Borel isomorphism $\varphi : \mathbb{B}^{\mathbb{N}} \to \mathbb{B}^{\mathbb{N}}$ which is measure preserving and satisfies $\varphi(\mathbb{B}^{\mathbb{N}} \cdot x) = \mathbb{Z} \cdot \varphi(x)$ for almost every $x \in \mathbb{B}^{\mathbb{N}}$ . I'm pretty stumped on this problem. If I'm not mistaken the orbit $\mathbb{B}^{\mathbb{N}} \cdot x$ should be the entire space $\mathbb{B}^{\mathbb{N}}$ itself. Since $\varphi$ is supposed to be an isomorphism I assume this means $\mathbb{Z} \cdot \varphi(x) = \mathbb{B}^{\mathbb{N}}$ as well. However the former seems to be a countable set, while the latter isn't? Is there something obvious I'm seeing wrong here? This problem is taken from the paper A. Ioana's lecture notes ""Orbit Equivalence of Ergodic Group Actions"" , Exercise 1.21. Edit: So I thought of a potential solution, but I'm not sure I can take it all the way to its end, so here goes. For any $x \in \mathbb{B}^\mathbb{N}$ , it seems clear on one hand that $\mathbb{B}^\mathbb{N} \cdot x = \mathbb{B}^\mathbb{N}$ . On the other hand, the set $\mathbb{Z} \cdot x$ contains all sequences that eventually have the same tail end as $x$ , in other words it contains all the following sets: $$
\begin{align}
\{x_0\} \times \{x_1\} \times \{x_2\} &\times \cdots \\
\mathbb{B} \times \{x_1\} \times \{x_2\} &\times \cdots \\
\mathbb{B} \times \mathbb{B} \times \{x_2\} &\times \cdots \\
&\vdots
\end{align}
$$ Hence if we define the Borel sets $Z_k = \mathbb{B}^k \times \{x_k\} \times \{x_{k+1}\} \times \cdots$ for all $k \in \mathbb{N}$ it follows from $Z_k \subseteq \mathbb{Z} \cdot x$ that $\bigcup^\infty_{k=0} Z_k \subseteq \mathbb{Z} \cdot x$ . At this point I'd use the measure of the union as a lower bound for the measure of $\mathbb{Z} \cdot x$ but I now realize that's not as obvious as it first looked.","['topological-dynamics', 'measure-theory', 'ergodic-theory', 'group-actions', 'dynamical-systems']"
4502133,"Evaluating $\oint_c x\,dx-y\,dy$ (and others) over unit circle $c$. I'm not sure I understand how those separate derivatives are supposed to work.","The question says: Solve the following line integrals, where $c$ is the circle with radius $1$ centered at the origin, that can be parametrized by $ c : [0,2\pi]\rightarrow \mathbb{R}^2 : t \mapsto c(t) = (\cos(t),\sin(t))$ . (i) $\;\oint_c x\,dx-y\,dy$ (ii) $\;\oint_c x\,dx+x\,dy$ (iii) $\;\oint_c y\,dx$ (iv) $\;\oint_c dy$ I think I'm supposed to use Green's theorem and solve these integrals as a double integral on the region of a circle, and then use polar coordinate substitution? But I'm not sure I understand how those separate derivatives are supposed to work like in "" $x\,\mathrm{d}x-y\,\mathrm{d}y$ "" for example.","['multivariable-calculus', 'calculus', 'line-integrals']"
4502160,"Closed form solution for $t_1$ and $t_2$ in the system $\sin(t_1 - t_2)=c_1\sin(t_1)$, $\sin(t_2)=c_2\sin(t_1)$","I have the following system of equations $$\sin(t_1 - t_2) = c_1 \sin(t_1)$$ $$\sin(t_2) = c_2 \sin(t_1)$$ where $c_1, c_2$ are known constants. And I would like to find all the solutions in closed form over $(t_1, t_2) \in (0, 2\pi) \times (0, 2\pi) $ Note that it is required that $t_1 \ne 0$ and $t_2 \ne 0 $ and $t_1 \ne t_2 $ . Context : The context of this problem is finding conjugate diameters of an ellipse passing through three given points, and that has a known center. In particular, suppose the center is the vector $C$ and the three points through which the ellipse passes are $P_1, P_2, P_3$ then a parametric equation of this ellipse is $ P(t) = C + F_1 \cos(t) + F_2 \sin(t) $ where $F_1$ and $F_2$ are called the conjugate semi-diameters, and are not unique.  So now define $V_1 = P_1 - C , V_2 = P_2 - C , P_3 - C $ then Then the shifted ellipse has the equation $ V(t) = F_1 \cos(t) + F_2 \sin(t) $ We can choose $V(0)$ to be $V_1$ , then $ V(t) = V_1 \cos(t) + F_2 \sin(t) $ At times $t_1$ and $t_2$ we have $ V(t_1) = V_2 = V_1  \cos(t_1) + F_2 \sin(t_1) $ Now, since we are in $2D$ , then $V_3 = c_1 V_1 + c_2 V_2$ , and $F_2 = a V_1 + b V_2 $ where $c_1$ and $c_2$ are known, while $a$ and $b$ are unknown. With this, and from the linear independence of $V_1$ and $V_2$ , it follows that $ 0 = \cos(t_1) + a \sin(t_1) \hspace{15pt}(1)$ $ 1 = b \sin(t_1) \hspace{15pt}(2)$ And finally, at $t = t_2$ we have $ V(t_2) = V_3 = c_1 V_1 + c_2 V_2 = V_1 \cos(t_2)+ (a V_1 + b V_2) \sin(t_2) $ From which we get another two equations $ c_1 = \cos(t_2) + a \sin(t_2) \hspace{15pt}(3) $ $ c_2 = b \sin(t_2) \hspace{15pt}(4)$ Eliminating $a$ and $b$ from equations $(1)-(4)$ yields the system of two equations above in $t_1 $ and $t_2 $ .","['calculus', 'trigonometry']"
4502163,Can A Dependent Variable be Factored Out of Integral?,"say
I have a the equation $$y = xw + z$$ And I am trying to compute $$\begin{aligned}\int \text{exp}((y - xw)^2 - w^2)dw &= \int\text{exp}(y^2 - 2xwy + x^2w^2 - w^2)dw
\\&= \int\text{exp}(y^2)exp(-2xwy + x^2 -w^2)dw \end{aligned}$$ Am I allowed to factor out the $\text{exp}(y^2)$ term considering $y = xw + z$ to get $$\text{exp}(y^2)\int\text{exp}(-2xwy + x^2 - w^2)dw$$ I appreciate all of the swift replies. I did believe that was not correct, but needed a sanity check. Thank you all. To answer some comments, x and z are constants here.",['multivariable-calculus']
4502183,Proving that simple harmonic motion and pendulum motion are periodic,"Consider the equation of motion for simple harmonic motion, \begin{align}
f'' + \omega^2 f = 0,
\end{align} where $f: \mathbf{R} \rightarrow \mathbf{R}$ and $\omega \in \mathbf{R}$ . Suppose we have some initial conditions $f(0) = x_0$ , $f'(0) = v_0$ . Question : Without explicitly solving this equation , is it possible to prove that any solution is periodic? That is, can we show that if $f$ is a solution, then \begin{align}
f(t + T) = f(t)
\end{align} for some $T$ ? I also have the same question for the equation of motion of a simple pendulum, \begin{align}
f'' + \omega^2 \sin f = 0.
\end{align} Feel free to make any assumptions you want about the niceness of the solutions.","['classical-mechanics', 'periodic-functions', 'ordinary-differential-equations']"
4502186,Prove that $a^{4/a} + b^{4/b} + c^{4/c} \ge 3$,"Let $a, b, c > 0$ with $a + b + c = 3$ . Prove that $$a^{4/a} + b^{4/b} + c^{4/c} \ge 3.$$ This question was posted recently, closed and then deleted, due to missing of contexts etc. By https://approach0.xyz/ , the problem was proposed by Grotex@AoPS. My strategy is to split into many cases. WLOG, assume that $a \ge b \ge c$ . If $a \ge 8/5$ , true. If $a \le 10/7$ , let $f(x) = x^{4/x} - 1  - 4(x - 1)$ .
We have $f(x) \ge 0$ for all $x \in (0, 10/7)$ . If $10/7 < a < 8/5$ and $b \ge 4/5$ , true. (I stopped here since this approach is ugly. Actually, the proof of $x^{4/x} - 1  - 4(x - 1) \ge 0$ for all $x\in (0, 10/7)$ is complicated.) I hope to see nice proofs.","['inequality', 'real-analysis']"
4502188,What is the geometric meaning that the directional derivative is positive?,"We have the function $$f(x,y)=x^2y+xy^2+\frac{1}{3}y^3-4y$$ I have calculated the gradient $$\nabla f=\binom{2xy+y^2}{x^2+2xy+y^2-4}$$ The directional derivative of $f$ at the point $(1,1)$ at the direction $v=\binom{2}{1}$ is equal to $\frac{6}{\sqrt{5}}>0$ . What is the geometric meaning that the directional derivative is positive?","['partial-derivative', 'calculus', 'derivatives', 'vector-analysis']"
4502207,Conjugation of integer matrices by $\mathrm{SL}_n \Bbb Z$ and narrowly equivalent ideal classes,"I am trying to understand K. Conrad's notes on the Latimer-MacDuffee theorem and related results. Let $\alpha$ be an algebraic integer with minimal polynomial $f$ of degree $n$ . Let $M_f$ be the set of matrices in $M_n \Bbb Z$ with characteristic polynomial $f$ . The Latimer-MacDuffee theorem (Theorem 2.1 in the notes) establishes a bijection between the conjugation classes of matrices in $M_f$ and the ideal class monoid $ICM(\Bbb Z[\alpha])$ of $\Bbb Z[\alpha]$ . Namely, we send a fractional ideal $I$ to the matrix (in some $\Bbb Z$ -basis $B$ ) associated to multiplication by $\alpha$ , viewed as a $\Bbb Z$ -linear map $L_\alpha \colon I \to I$ . In Remark 2.7 of the aforementioned notes, it is claimed that if we instead consider $SL_n \Bbb Z$ conjugacy classes of elements of $M_f$ , then we need to replace the notion of equivalent ideals by narrow equivalence in the following sense: two fractional ideals $I$ and $J$ are narrowly equivalent if there exists $x \in \Bbb Q(\alpha)$ such that $N_{\Bbb Q(\alpha)/\Bbb Q}(x) > 0$ and $I = xJ$ . (This is not in general the relation defining the narrow class group.) I think I have found a counterexample to the claim above, either that or I cannot find the error in my argument. I will first write a concrete counterexample, followed by the more general argument which I would like to proof-check. If $\alpha = i$ and $f = X^2+1$ , the norm of elements in $\Bbb Q(i)$ is always non-negative and $\Bbb Z[i]$ is the maximal order of $\Bbb Q(i)$ . Thus, if the claim were true, $SL_2 \Bbb Z$ -conjugacy classes of integer matrices satisfying $A^2 = -I$ should be in correspondence with $Cl(\Bbb Z[i]) = 0$ . In other words, every two such matrices should be $SL_2 \Bbb Z$ -conjugate. However, if we put $A = \begin{pmatrix}0 & 1\\ -1 & 0\end{pmatrix}, B = \begin{pmatrix}0 & -1 \\ 1 & 0\end{pmatrix}$ any matrix $C = \begin{pmatrix}a & b \\ c & d\end{pmatrix}$ such that $CA = BC$ must satisfy $a=-d, b = c$ and so $$\det(C) = -(a^2+b^2) \le 0.$$ Now the more general situation: suppose for simplicity that $O := \Bbb Z[\alpha]$ is the maximal order of $K:= \Bbb Q(\alpha)$ . Put $PN(K)$ for the elements of positive norm of $K$ and $PN(O) = PN(K) \cap O$ . Let $NCl(O)$ be the narrow class group in the aforementioned sense and $Cl(O)$ the usual class group. There is an exact sequence $$0 \to O^\times/PN(O) \hookrightarrow K^\times/PN(K) \to NCl(O) \to Cl(O) \to 0,$$ this follows from a similar argument that yields an exact sequence comparing $Cl(O)$ and the usual narrow class group (e.g. as noted here ). Now, $PN(K) = \ker(K^\times \xrightarrow{N} \Bbb Q^\times \xrightarrow{\mathrm{sgn}} \{-1,1\})$ , hence $K^\times/PN(K)$ is isomorphic to a subgroup of $G_2 = \{-1,1\}$ , it will be non-trivial only if there exists an element of negative norm. Likewise $O^\times/PN(O)$ will be either $G_2$ or $1$ depending on the existence of a norm $-1$ integral unit. In particular, $NCl(O) = Cl(O)$ if and only if one of the following holds: there exists $x \in O^\times$ of norm $-1$ . every element in $K^\times$ has positive norm. If the claim of Remark 2.7 were true, then these conditions should characterize when the $SL_n \Bbb Z$ conjugation classes coincide with the $GL_n \Bbb Z$ ones. However, I think the conjugation classes coincide only when (1) holds (hence the counterexample for $\Bbb Q(i)$ ). The relevant direction (for the counterexample) argument is roughly as follows: If $GL_n \Bbb Z$ and $SL_n \Bbb Z$ conjugacy classes coincide for a given matrix $A$ , then $A$ and $DAD^{-1}$ for some matrix such that $\det D= -1$ are $SL_n \Bbb Z$ conjugate. Thus $A = (UD)A(UD)^{-1}$ for some $U$ such that $\det U = 1$ . Hence $A$ commutes with a determinant $-1$ matrix. Now suppose that there exists $A\in M_f$ such that its $GL_n \Bbb Z$ and $SL_n \Bbb Z$ conjugacy classes coincide. By the Latimer-MacDufee theorem,there exists $I$ a fractional ideal and $B$ a $\Bbb Z$ -basis of $I$ such that $A$ is $GL_n \Bbb Z$ conjugate to $X := [L_\alpha \colon I \to I]_B$ . By hypothesis $X$ and $A$ are $SL_n \Bbb Z$ conjugate. Thus the $SL_n \Bbb Z$ and $GL_n \Bbb Z$ conjugacy clases of $X$ also coincide since $$(SL_n \Bbb Z) \cdot X = (SL_n \Bbb Z) \cdot A = (GL_n \Bbb Z) \cdot A = (GL_n \Bbb Z) \cdot X.$$ Thus it must exist $D$ such that $\det D = -1$ commuting with $X$ . In basis $B$ , this defines an $O$ -linear iso $I \to I$ , which must me multiplication by some $x \in K^\times$ . In particular $-1 = \det D$ is the norm of $x$ . Since $xI = I$ , multiplying by $I^{-1}$ it follows that $x \in O^\times$ . I would very much appreciate a sanity check on whether my reasoning is correct and, if so, any comments on how one could compute the $SL_n  \Bbb Z$ conjugacy classes in terms of ideal classes.","['matrices', 'algebraic-number-theory', 'solution-verification']"
4502271,Is $N \trianglelefteq G$?,"Assumption throughout the post: Suppose that $G = \bigoplus_{i=0}^{\infty} \mathbb{Z}$ and $N = \bigoplus_{i=1}^{\infty} \mathbb{Z}$ . Question #1: Is the following attempt correct? My attempt: Since $\mathbb{Z}$ is an abelian group, then $G$ is an abelian group. Therefore, every subgroup of $G$ is a normal subgroup of $G$ . Hence, $N \le G$ , implies $N \trianglelefteq G$ . Question #2: Is $N = \bigoplus_{i=1}^{\infty} \mathbb{Z} := \{(0, a_1, a_2, \dots , a_k, 0, 0, \dots) \, \mid \, (\forall i \in \mathbb{N})([1 \le k \le i] \Rightarrow [a_k \ne 0])\, \wedge \, (\forall k > i)(a_k = 0)\}$ , where $a_j = f(j)$ , for some $f \in \prod_{i=1}^{\infty} \mathbb{Z}$ ? Here, $\prod_{i=1}^{\infty} \mathbb{Z}$ is the infinite direct product of groups.","['direct-product', 'direct-sum', 'normal-subgroups', 'group-theory', 'abelian-groups']"
4502277,"There exists an open set $A:=(a_1,b_1)\cup (a_2,b_2)\cup\dots$ such that $\operatorname{Bd}A$ does not have measure zero Munkres Analysis on Manifolds","I am reading ""Analysis on Manifolds"" by James R. Munkres. Example 1 on p.113: We construct a bounded open set $A$ in $\mathbb{R}$ such that $\operatorname{Bd}A$ does not have measure zero. The rational numbers in the open interval $(0,1)$ are countable; let us arrange them in a sequence $q_1,q_2,\dots$ . Let $0<a<1$ be fixed. For each $i$ , choose an open interval $(a_i,b_i)$ of length less than $\frac{a}{2^i}$ that contains $q_i$ and is contained in $(0,1)$ . These intervals will overlap, of course, but that doesn't matter. Let $A$ be the following open set of $\mathbb{R}$ : $$A=(a_1,b_1)\cup (a_2,b_2)\cup\dots.$$ The author proved that $\operatorname{Bd}A$ doesn't have measure zero. My question is here: $S:=\{a_1,a_2,\dots\}\cup\{b_1,b_2,\dots\}$ is a countable set. So, $S$ has measure zero. So, there exists $b\in\operatorname{Bd}A$ such that $b\notin S$ . What is $b$ ? Please give me an example of $b$ .",['measure-theory']
4502288,Why is the antiderivative of $\dfrac{1}{x}=\ln(x)$? [duplicate],This question already has an answer here : Justify the fact that antiderivative of $\frac{1}{x}$ is $\ln |x|$ not $\ln x$ (1 answer) Closed 1 year ago . I understand that the derivative of $\ln(x)$ is equal to $\dfrac{1}{x}$ . What I don't get is why the antiderivative of $1/x$ is $\ln(|x|)$ instead of $\ln(x)$ . Why is the absolute value sign necessary? I would like a simple explanation as I am new to calculus. Thanks!,"['calculus', 'derivatives']"
4502291,How to calculate this average? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question I'm not sure if there is a term for what I'm trying to calculate, but imagine that there is a game where a win rewards you x points and a loss rewards you y points.  If you happen to know that your win rate w for this game is 70%, how can you calculate what the average reward will be?  What is that formula?","['average', 'algebra-precalculus']"
4502294,Proof Verification for Almost Uniform Convergence implies Convergence in Measure,"I shall attach my proof for the statement ""Almost Uniform Convergence Imply Convergence in Measure"" here. I know this is a lot longer than what it is necessary to be, but I want everything in the proof to be crystal clear to me. I also bolded the part that I am unsure about: Let $(X, \mathcal{F}, \mu)$ be the measure space. Suppose $f_n \xrightarrow{a.u.} f$ . That is, for all $\epsilon > 0$ , there exists $E_\epsilon \in \mathcal{F}$ such that $\mu(E_\epsilon) < \epsilon$ and $$
\lim_{n \to \infty} \sup_{x \in X - E_\epsilon}|f_n(x) - f(x)| = 0.
$$ We wish to show for all $c > 0$ that $$
\lim_{n \to \infty} \mu(\{ x \in X: |f_n(x) - f(x)| \geq c \}) = 0.
$$ To this end, for all $c > 0$ and $n \in \mathbf{N}$ , we define $A_c^n = \{ x \in X: |f_n(x) - f(x)| \geq c \}$ . Then for all $c > 0$ and $\epsilon > 0$ , we must have $A_c^n \cap (X - E_\epsilon) = \emptyset$ for $n$ large enough.
To see this, let $\epsilon > 0$ . Then we must have some $E_\epsilon \in \mathcal{F}$ such that $\mu(E_\epsilon) < \epsilon$ and $$
\lim_{n \to \infty} \sup_{x \in X - E_\epsilon} |f_n(x) - f(x)| = 0.
$$ Therefore, for all $a > 0$ , there exists $N(a, \epsilon) \in \mathbf{N}$ such that if $n \geq N(a, \epsilon)$ , then $$
\sup_{x \in X - E_\epsilon} |f_n(x) - f(x)| < a.
$$ Thus we must have if $n \geq N(a, \epsilon)$ , then for all $x \in X - E_\epsilon$ that $$
|f_n(x) - f(x)| \leq \sup_{x \in X - E_\epsilon} |f_n(x) - f(x)| < a.
$$ In particular, for all $c > 0$ , there exists $N(c, \epsilon) \in \mathbf{N}$ such that if $n \geq N(c, \epsilon)$ , then for all $x \in X - E_\epsilon$ that $$
|f_n(x) - f(x)| < c.
$$ Thus for all $\epsilon > 0$ and $c > 0$ , there exists $N(c, \epsilon) \in \mathbf{N}$ such that if $n \geq N(c, \epsilon)$ , then $x \not \in A_c^n$ . In particular, this implies for all $\epsilon > 0$ and $c > 0$ , there exists $N(c, \epsilon) \in \mathbf{N}$ such that if $n \geq N(c, \epsilon)$ , then $A_c^n \subseteq E_\epsilon$ . Therefore, we have for all $\epsilon > 0$ and $c > 0$ , there exists $N(\epsilon, c) \in \mathbf{N}$ such that if $n \geq N(\epsilon, c)$ , then $$
\mu(A_c^n) \leq \mu(E_\epsilon) < \epsilon.
$$ Thus, we have for all $c > 0$ and $\epsilon > 0$ that $$
\lim_{n \to \infty} \mu(A_c^n) \leq \epsilon.
$$ Thus we have for all $c > 0$ that $$
0 \leq \lim_{n \to \infty} \mu(A_c^n) = \lim_{n \to \infty} \mu(\{ x \in X: |f_n(x) - f(x)| \geq c \}) \leq 0.
$$ In particular, we must have for all $c > 0$ that $$
\lim_{n \to \infty} \mu(\{ x \in X: |f_n(x) - f(x)| \geq c \}) = 0.
$$ This proves that $f_n \xrightarrow{\mu} f$ by definition. My Question: Is the part that I marked above correct? I am unsure because our $N$ is dependent on $\epsilon > 0$ . However, since $\epsilon$ is fixed and given, I think the inequality should be fine by, for example, this result: Comparison for Sequences . How about other parts of the proof? Is the proof correct?","['measure-theory', 'real-analysis', 'alternative-proof', 'solution-verification', 'convergence-divergence']"
4502318,Evaluating $\lim _{n \rightarrow \infty} \sum_{k=n+1}^{2 n} \sin \frac{n}{n^{2}+k^{2}}$,"Evaluate $$S=\lim _{n \rightarrow \infty} \sum_{k=n+1}^{2 n} \sin \frac{n}{n^{2}+k^{2}}$$ My try:
Since $\sin x$ is monotone increasing in $(0,1)$ , we have: $$n \sin \left (\frac{n}{n^2+(2n)^2}\right)\leq S \leq n\sin \left(\frac{n}{n^2+(n+1)^2}\right)$$ taking $\lim$ throughout we get: $$0.2 \leq S \leq 0.5$$ I am stuck now?","['limits', 'algebra-precalculus', 'definite-integrals', 'sequences-and-series']"
4502329,"Spivak, Ch. 18, Problem 21: $f$ satisfies $f'=cf$ for some number $c$, show that $f(x)=ke^{cx}$ for some $k$.","Suppose that on some interval the function $f$ satisfies $f'=cf$ for some number $c$ . (a) Assuming that $f$ is never $0$ , use Problem 20(b) to prove that $|f(x)|=le^{cx}$ for some number $l>0$ . It follows that $f(x)=ke^{cx}$ for some $k$ . (b) Show that this result holds without the added assumption that $f$ is never $0$ . Hint: show that $f$ can't be $0$ at the endpoint of an
open interval on which it is nowhere $0$ . I am interested specifically in the solution to part $(b)$ . A question about this problem has been asked before . However, I am not fully satisfied with the accepted answer where it pertains to item $(b)$ . Though I understand the reasoning in that answer, I felt that the final part was left a bit vague we can continue the same argument by replacing $a$ with $(a+p)$ (or $(a-p)$ ) and thereby extend the region where $f$ does not vanish.
Using the same argument repeatedly we can show that $f$ does not
vanish at any point in $I$ . How exactly do we use the same argument repeatedly (ie, I would like to see what that looks like formally)? I think I reached a similar point in my own attempt at a proof, and that is why I am asking this question. Here is my attempt Our only assumption is that $$f'=cf\tag{1}$$ on some interval. Let $[a,b]$ be the interval. $(1)$ implies that $f$ is differentiable
and hence continuous on $[a,b]$ . First, there is a trivial case: $f(x)=0=0\cdot e^{cx}$ . Now let's consider the case where $f\neq 0$ at some point $x_1$ .
Suppose $f(x_1)>0$ . Then, by continuity, there is an interval $(m,n)$ around $x_1$ such that $f(x)>0$ for $x\in (m,n)$ . By part $(a)$ , $f(x)=ke^{cx}$ for $k>0$ on $(m,n)$ . Suppose $f(n)\leq0$ . But $\lim\limits_{x\to n^-} ke^{cx}=ke^{cn}>0\neq f(n)$ . Thus $f$ is
discontinuous at $n$ , which contradicts our assumptions. Therefore, $f(n)>0$ . Analogously, we can show that $f(m)>0$ . So, if the above is correct (is it?), I am at a similar point to the linked answer. I need to extend my argument such that $m$ and $n$ move outwards to encompass all of $[a,b]$ . How do I finish this proof? Edit: I had forgotten to check the solution manual, but I just did and it is also kind of terse. It uses the same argument I used above, but leaves the part I am asking about as a hint. It says On an interval where $f$ is non-zero it has the form $f(x)=ke^{cx}$ .
But this can't approach $0$ at the endpoint of the interval; so $f$ couldn't be $0$ at the endpoints. This proves that if $f$ is non-zero
at one point $x_0$ , then it is nowhere $0$ (consider $\sup\{x>x_0:f(x)\neq 0\}$ and $\inf\{x<x_0:f(x)\neq 0\}$","['logarithms', 'calculus', 'solution-verification', 'derivatives', 'exponential-function']"
4502380,Approximating the result of probability,"So, here's a question asked in an interview: 10 dices are rolled simultaneously. What is the probability of getting the sum of numbers appearing on top of all the dice as 35? If we calculate manually, I think it's pretty tedious with no calculators allowed. So, I thought of approximating the answer. We know that rolling $10$ dice, has total number of ways as $6^{10}$ which is ~ $ 1e7$ which makes it ideal for using a normal distribution. So, mean of $10$ throws is $35$ and SD of 10 throws is ~ $ 5.4$ and so $Z = \dfrac{(X-35)}{5.4}$ and so we want $P(\leq35) - P(\leq34)$ as my answer, $P(\leq35) = 1/2$ and the other one can be obtained from the Normal graph and hence, we have the answer. But to my surprise, the interviewer didn't seem convinced! Is there something wrong with my method? Please point out. Thanks!",['probability']
4502460,Showing $\sum_{cyc} \frac{\cos(\frac{\alpha+\beta}{2})}{\cos \frac{\alpha}{2}\cos \frac{\beta}{2}}=2$ when $\alpha+\beta+\gamma=\pi$,"I saw this problem in a math magazine: Let $\alpha,\beta$ and $\gamma$ be the angles of a triangle, so that $$
\alpha+\beta+\gamma=\pi.
$$ Then it holds that $$
\sum_{cyc} \frac{\cos(\frac{\alpha+\beta}{2})}{\cos \frac{\alpha}{2}\cos \frac{\beta}{2}}=\frac{\cos\left(\frac{\alpha+\beta}{2}\right)}{\cos \frac{\alpha}{2}\cos \frac{\beta}{2}}+\frac{\cos\left(\frac{\alpha+\gamma}{2}\right)}{\cos \frac{\alpha}{2}\cos \frac{\gamma}{2}}+
\frac{\cos\left(\frac{\gamma+\beta}{2}\right)}{\cos \frac{\gamma}{2}\cos \frac{\beta}{2}}=2.
$$ I tried to use $\alpha+\beta=\pi-\gamma$ so that $$
\cos\left(\frac{\alpha+\beta}{2}\right)=\cos\left(\frac{\pi}{2}-\frac{\gamma}{2}\right)= \cos \frac{\pi}{2}\cos \frac{\gamma}{2}+ \sin\frac{\pi}{2}\sin \frac{\gamma}{2}= \sin\frac{\gamma}{2}.
$$ Now the above equation equals $$
\frac{\sin\frac{\gamma}{2}}{\cos \frac{\alpha}{2}\cos \frac{\beta}{2}}+\frac{\sin\frac{\beta}{2}}{\cos \frac{\alpha}{2}\cos \frac{\gamma}{2}}+
\frac{\sin\frac{\alpha}{2}}{\cos \frac{\gamma}{2}\cos \frac{\beta}{2}}=2.
$$ But now I don't have ideas for continuation. Any hints to tackle this?",['trigonometry']
4502476,Have I evaluated this question correctly$?$,"If $y=f(e^{-x})$ and $f'(x)=\ln x$ then $\frac{dy}{dx}$ at $x=\ln2$ is My work: I calculated $f(x)=x\ln x-x+C$ Now, $y=f(e^{-x})=e^{-x}\cdot\ln(e^{-x})-e^{-x}+C=-e^{-x}\cdot x-e^{-x}+C$ Then $$\frac{dy}{dx}=e^{-x}\cdot x$$ Now put $x=\ln2$ $$\frac{dy}{dx}|_{x=\ln2}=e^{-\ln2}\cdot\ln2$$ Now let $e^{-\ln2}=a$$\implies$ $$\ln(e^{-\ln2})=\ln a$$ or $$\ln2^{-1}=\ln a$$ or $$a=e^{-\ln2}=\frac12$$ Now doing $e^{-x}\cdot x=\frac{\ln2}{2}$ But the correct answer given is $\ln\sqrt{2}$ Where did I go wrong $?$ Any help is greatly appreciated.","['integration', 'real-analysis', 'calculus', 'solution-verification', 'derivatives']"
4502487,Can the difference of two bounded decreasing functions oscillate?,"Let $f,g$ be two functions defined near $x=0$ , e.g. on $[0,1]$ . Suppose that $f,g$ are continuous (differentiable on the interior), bounded, positive, and decreasing on this interval. Suppose further that $f(0) = g(0)$ . Can $f-g$ have an infinite number of zeroes? In particular, can the zeroes of $f-g$ accumulate at $x=0$ ? I tried to construct an example by requiring $f = g + x\sin(1/x)$ , so $$ f' = g' +  \sin(1/x) - \frac{\cos(1/x)}{x^3} \leq g' + 1 + \frac{1}{x^3},$$ and we would need that $g'(x) \leq -1 -1/x^3$ to make $f$ (and incidentally $g$ ) decreasing. But I am not sure how to construct $g$ to satisfy this while keeping it bounded.","['limits', 'real-analysis']"
4502495,Reduction of elliptic integrals to Legendre's normal form,"Let $P(x)$ be a real polynomial of degree four without multiple roots and suppose that all roots are real. According to my textbook (KÃ¶nigsberger, Analysis 1, first printing, p. 208), there is a rational transformation $x=T(t)=\frac{at+b}{ct+d}$ such that $$
\frac{dx}{\sqrt{P(x)}}=\frac{1}{\sqrt{P(T(t))}}\cdot\frac{ad-bc}{(ct+d)^2}dt=\text{const.}\cdot\frac{dt}{\sqrt{Q(t)}}
$$ where $Q(t)=(1-t^2)(1-k^2t^2)$ and $k=\frac{x_1-x_2}{x_1-x_4}\colon\!\frac{x_3-x_2}{x_3-x_4}$ with $x_1<x_2<x_3<x_4$ - the real roots in order. By making the Ansatz $P(T(t))(ct+d)^4=Q(t)$ I was able to determined $a,b,c,d$ , namely $a=\frac{2x_1}{x_1-x_2}-1$ , $b=1$ , $c=\frac{2}{x_1-x_2}$ , $d=0$ , so that $P(T(t))(ct+d)^4$ becomes $$
(1-t^2)(1+(2(x_1-x_3)/(x_1-x_2)-1)t))(1+(2(x_1-x_4)/(x_1-x_2)-1)t)
$$ but the last two factors in the last displayed equation do not summarize nicely as $(1-k^2t^2)$ and I suspect that I am doing something wrong although I get something close to the desired DoppelverhÃ¤ltnis expression for $k$ . What is the right fractional linear transformation in terms of $a,b,c,d\in\mathbb{R}$ and especially what is the constant $\text{const.}$ in the first displayed equation? Is it $ad-bc$ as I expected? Any help is appreciated! ---Edit--- You can write $P(x)=a_0(x-x_1)(x-x_2)(x-x_3)(x-x_4)$ , then, because $\sqrt{a_0}$ merges with $\text{const.}$ , we get $$
P(T(t))(ct+d)^4=(at+b-(ct+d)x_1)(at+b-(ct+d)x_2)(at+b-(ct+d)x_3)(at+b-(ct+d)x_4)\,.
$$ By comparing coefficients and still changing the constant, we get a system of equations as follows: $$
a-cx_1=-q\,,\,a-cx_2=p\,,\,b-dx_1=q\,,\,b-x_2d=p\,,\,a-cx_3=-kr\,,a-cx_4=ks,b-x_3d=r\,,\,b-x_4d=s
$$ with yet to be determined constants $p,q,r,s$ and where $k$ is the DoppelverhÃ¤ltnis of $x_1,x_2,x_3,x_4$ in a, yet to be determined, order. Solving these euqations should be possible and $k$ should be as desired. I tried but I did not come to an end. The values of $a,b,c,d$ above are obviously wrong and arise from a special choice of constants $p,q,r,s$ which is too restrictive.","['analysis', 'mobius-transformation', 'real-analysis', 'indefinite-integrals', 'elliptic-integrals']"
4502496,"Linear algebra done right, Exercise 16, section 2.A","This question asks to prove that the real vector space of all continuous real-valued functions on the interval $[0,1]$ is infinite-dimensional. In an earlier question it was proved that a vector space $V$ is infinite-dimensional if and only if there exists a sequence of vectors $\{v_j\}$ such that $$v_1, v_2, \ldots, v_m$$ is linearly independent for every positive integer $m$ . I worked with the question and got this proof. Consider a sequence of such functions $\{f_j\}$ where $f_k(x) = x^k$ for all $k \in \mathbb N$ . It's easy to see that $$f_1, f_2, \ldots, f_m$$ is linearly independent for all positive integer $m$ . Thus this vector space is infinite-dimensional. However, when I checked the solution on this website, https://linearalgebras.com/2a.html , they constructed the function as $$f_k(x) = \begin{cases}x-\frac 1 k, & x\geq \frac 1 k; \\ 0, & \text{otherwise}. \end{cases}$$ I want to ask if there are any flaws in my proof.","['solution-verification', 'linear-algebra', 'vector-spaces']"
4502539,"Halmos' ""Naive Set Theory"" - Exercise on Equivalence Classes","In Section 7 of Halmos' ""Naive Set Theory"" the following exercise is proposed: Exercise: show that $X/R$ [that is, the collection of the equivalence classes of elements of $X$ with respect to the equivalence relation $R$ in $X$ ] is indeed a set by exhibiting a condition that specifies exactly the subset $X/R$ of the power-set $\mathcal{P}(X)$ . My attempt Up to this point in the book, a set can be constructed only by applying one of the following Axioms: Specification, Pairing, Union, Powers. Therefore it seems clear to me that given the set $X$ and the equivalence relation $R$ in it, we first construct the power-set $\mathcal{P}(X)$ by applying the Axiom of Powers, then we extract $X/R$ by applying the Axiom of Specification in the following manner: $$ X/R = \{Y\in\mathcal{P}(X)-\{\emptyset\}: [\exists x \in X:(\forall y \in Y, (x, y) \in R)]\}$$ in which we have an embedding set (namely $\mathcal{P}(X)-{\emptyset}$ ) and the specifying condition contains $Y$ as a ""free variable"" (it appears at least once without being preceded by a quantifier). Edit As pointed out in the comments, the above specification is not right. Therefore I changed it into: $$ A = \{Y\in\mathcal{P}(X): [\exists x \in X:(y \in Y \iff (x,y) \in R)]\}$$ Now, we need to show that $A = X/R$ . In order to do so, we shall prove that if $Y \in A$ and $x \in Y$ , then $Y = x/R$ (that is, $Y$ is the equivalence class of $x \in X$ modulo $R$ ). In fact, suppose $y \in Y$ , then $(x, y) \in R$ , hence $y \in x/R$ and, viceversa, suppose $y \in x/R$ , then $(x, y) \in R$ , hence $y \in Y$ . Since from this it is clear that $A \subseteq X/R$ , it suffices to show that if $Y \in X/R$ , then $Y \in A$ . In fact, $Y \in X/R$ implies that $Y$ is the equivalence class of some $x \in X$ modulo $R$ , hence there exists $x \in X$ such that $x \in Y$ (by the Reflexivity of $R$ ) and such that $Y$ contains exactly the elements to which $x$ stands in relation. Therefore, $Y \in A$ . Thus, the proof is concluded.","['elementary-set-theory', 'proof-writing', 'solution-verification', 'set-theory']"
4502540,Evaluate $\lim\limits_{n\to\infty}{\dfrac{1}{n}\sum_{k=1}^n\sin{\left(\dfrac{n}{k}\right)}}$.,"Numerical experimentation suggests that the following limit converges. $$L=\lim_{n\to\infty}{\dfrac{1}{n}\sum_{k=1}^n\sin{\left(\dfrac{n}{k}\right)}}$$ According to both Desmos and Wolfram, for $n=10^5, 10^6, 10^7$ , the values of $L$ are $0.504116, 0.504069, 0.504068$ , respectively. Does this limit converge, and if so, is there a closed form? By looking at the graph of $y=\sin{\left(\frac{n}{x}\right)}$ from $x=1$ to $x=n$ , I can see why the limit should converge, but I don't know how to prove this rigorously. I doubt there is a closed form. (In case you're wondering where this question comes from, I just made it up after thinking about another question .)","['limits', 'trigonometry', 'real-analysis']"
4502543,Question on solving partial derivative in probability theory,"When the diffusion process $X_{t}$ is stationary, $F_{s, x}(s+t, y)$ does not depend on $s$ . Hence we have a well-defined function $$
F_{t}(x, y)=F_{s, x}(s+t, y), \quad t>0 .
$$ Since $F_{s, x}(s+t, y)$ does not depend on $s, \frac{d}{d s} F_{s, x}(s+t, y)=0$ . This implies the second equality below: $$
\frac{\partial}{\partial t} F_{t}\left(x, y_{0}\right)=\frac{\partial}{\partial(s+t)} F_{s, x}\left(s+t, y_{0}\right)=\left.\left(-\frac{\partial}{\partial s} F_{s, x}\left(u, y_{0}\right)\right)\right|_{u=s+t} .
$$ Then use Equation (10.9.8) to get $$
\frac{\partial}{\partial t} F_{t}\left(x, y_{0}\right)=\rho(x) \frac{\partial}{\partial x} F_{t}\left(x, y_{0}\right)+\frac{1}{2} Q(x) \frac{\partial^{2}}{\partial x^{2}} F_{t}\left(x, y_{0}\right)
$$ I am reading a textbook and see this part. I wonder how do I get the second equality as mentioned here.","['partial-derivative', 'probability-theory']"
4502557,Find the value of $l = \lim \limits_{k \to \infty} \int_{0}^{k} \left(1-\frac{x}{k}\right)^k \cdot e^{\frac{x}{3}} dx$ .,"This is a question from a mock exam of national engineering test in my country. Firstly, we can't take limit inside the integral as the limits of integral are not independent. I also tried applying the property: $$\int_{a}^{b}f(x)dx=\int_{a}^{b}f(a+b-x)dx$$ but that also doesn't give anything significant which can help in simplifying the problem further. Leibniz integral rule also doesn't seem to give anything useful. I would be grateful if I could get more ideas from the community on how to approach this.","['integration', 'limits', 'calculus']"
4502597,Mean value theorem under an expectation?,"Let $f:\mathbb{R} \rightarrow \mathbb{R}$ be a continuous function. Consider the expected value of $f$ under the probability density function $\pi(x)$ defined on $(-\infty,\infty)$ . $$
\int_{-\infty}^{\infty} \pi(x) f(x) dx = \mu.
$$ Is there some type of ""mean value theorem under expectations"" that would allow us to write $\mu = Mf(x^*)$ where $M\in\mathbb{R}$ under some restrictions to $\pi$ and $f$ without requiring that the domain be bounded?","['probability-distributions', 'probability-theory', 'probability']"
4502616,Derivative of dot-product involving a matrix function,"I am struggling with the following derivative $$\frac{\partial }{\partial \mathbf{x}}(\mathbf{b}\cdot(\mathbf{A}(\mathbf{x})\,\mathbf{c}))$$ with $\mathbf{b}\in \mathbb{R}^n$ , $\mathbf{c}\in \mathbb{R}^m$ constants, and $\mathbf{A}\in \mathbb{R}^{n\times m}$ function of $\mathbf{x}\in \mathbb{R}^l$ .
I know that $$\frac{\partial \mathbf{A}(\mathbf{x})}{\partial \mathbf{x}}=\mathbf{M}$$ with $\mathbf{M}$ a $3$ D tensor, such that $$M_{n,m,l}=\frac{\partial A_{n,m}}{\partial x_l}$$ but I am not able to find the results of the original problem starting from this result. I am primarly having trouble in retrieving the correct dimensions. Can you give me a hint?
Thanks","['matrices', 'matrix-calculus', 'linear-algebra', 'partial-derivative', 'derivatives']"
4502626,Confusion regarding the proof for De Morgan's law,"From Wikipedia's formal proof of De Morgan's law $ (A \cap B )'= A' \cup B' $ .
Given proof: \begin{align} x  \in (A \cap B )'
\end{align} \begin{align} \implies x  \notin (A \cap B )
\end{align} \begin{align} \implies x  \notin A  \lor x \notin B
\end{align} And proof goes on. But my question is that isn't the following proof also correct? \begin{align} x  \in (A \cap B )'
\end{align} \begin{align} \implies x  \notin (A \cap B )
\end{align} \begin{align} \implies x  \in A  \lor x \in B
\end{align} If it's correct then what's wrong with the following proof? \begin{align} x  \in (A \cap B )'
\end{align} \begin{align} \implies x  \notin (A \cap B )
\end{align} \begin{align} \implies x  \in A  \lor x \in B
\end{align} \begin{align} \implies x  \notin A'  \lor x \notin B'
\end{align} \begin{align} \implies x  \notin (A' \cap B' )
\end{align} \begin{align} \implies x  \in (A' \cap B' )'
\end{align} \begin{align} \therefore    (A \cap B )' \subseteq (A' \cap B' )'
\end{align}","['elementary-set-theory', 'solution-verification']"
4502639,Function arrow notation in multivariable functions,"How can I implement the function arrow notation in a multivariable function? I know that $f:\mathbb{R}\rightarrow\mathbb{Z}$ means that the domain is $\mathbb{R}$ and the range is $\mathbb{Z}$ for example a function $f(x)=\lfloor x \rfloor$ , but what in a multivariable function, for example: $$f(z,x)=w$$ $$z,w\in\mathbb{C},x\in\mathbb{R}$$ How would this be expressed?","['notation', 'functions', 'complex-numbers', 'multivalued-functions']"
4502660,"Prove that $A_5$ does not have a subgroup of order 15, 20, nor 30 (without using simple group and Sylow's Theorem)","Show that $A_5$ does not have a subgroup of order 15, 20, nor 30. I looked up other posts such as the one below, but they used some things we don't cover in the course. $A_5$ has no subgroup of order 15 and 20 Instead, we did a similar example in class: Show that $A_4$ has no subgroups of order 6. Suppose $H \preceq A_4$ with $|H| = 6$ . Then $[A_4: H] = 2$ and so $H \unlhd A_4$ . Therefore, $(aH)^2 = H \forall a \in A_4$ . This means $a^2 \in H \forall a \in
 A_4$ . We can compute to show that $A_4$ has 9 elements of the form $a^2$ , and by above they must all be in $H$ . But this is a
contradiction since $|H| = 6$ . Hence no such $H$ exist. But the problem with trying to apply a similar proof to my question is that, since $[A_5: H]$ = 4, we cannot say that $H \unlhd A_5$ . Hence, we cannot say $(aH)^4 = H$ since we cannot form the factor group $A_5/H$ . If that is the case, how else can I solve the question without using simple group and  Sylow's Theorem.","['quotient-group', 'group-theory', 'normal-subgroups']"
4502663,Exercises from section 14.3 of Katok's Introduction to the Modern Theory of Dynamical Systems,"I am reading the book "" Introduction to the Modern Theory of Dynamical Systems "" by Anatole Katok and Boris Hasselblatt. I find myself in section 14.3 which deals with minimal sets (specifically from Schwartz's theorem). I am trying to do the following exercises: Show that any $C^1$ flow on the orientable surface of genus $g$ has no more than $g$ different minimal sets that are not fixed points or periodic orbits. Given an orientable surface of genus $g$ and $1\leq k\leq g$ show that there exists a $C^1$ flow with exactly $k$ nowhere-dense minimal sets that are not fixed point or circles. I don't know how to attack the problems, for example if $M$ were an orientable manifold of genus $3$ then it can be thought of as a $3-$ torus then using Schwartz's theorem comes to mind: Let $M$ be a two-dimensional differentiable manifold of class $C^2$ compact and connected. Let $\varphi: \mathbb R \times M \to M$ be a flow of class $C^2$ in $M$ . A minimal set $\mu\subset M$ can be either: a singular point, or a periodic orbit, or an entire manifold $M$ which is homeomorphic to the torus $\mathbb T^2$ . but I can't really see how this can help me since the flow is of class $C^1$ . Any ideas please? I found the following hint given by the author, for the first question: Use the fact that any $g+1$ disjoint closed curves divide the surface, and the PoincarÃ©-Bendixson theorem.","['surfaces', 'smooth-manifolds', 'ordinary-differential-equations', 'dynamical-systems']"
4502730,When is multiplication by an element from a transformation group bijective?,"I am reading Allan Clark's Elements of Abstract Algebra and he states the following about transformation groups. We note that each $g \in G$ determines a one-to-one correspondence $g: X \to X$ given by $g(x) = g * x$ whose inverse is $g^{-1}: X \to X$ . Note that $G$ is the transformation group acting on $X$ and that Clark uses ""one-to-one correspondence"" in place of ""bijective mapping"". Next, he defines: A transformation group $G$ acts effectively on $X$ iff for all $g, g * x = x; \forall x \in X \implies g = e$ . I actually have a question about this, too. Can anyone give a single example of a group transformation that's not effective? Please don't make it too complicated; I'm a beginner in group theory. The main confusion is here: Show that the set $A(X)$ of one-to-one correspondences forms the largest effective group on $X$ . Didn't we say earlier that every transformation group is isomorphic to a set of one-to-one correspondences? How come now we're restricting ourselves to effective groups?","['group-theory', 'group-actions']"
4502734,An example which disproves $\|p(T)\|_{op} = \|p\|_\sup$ in arbitrary Banach spaces (Gelfand theory).,"Let me go straight to the question and then write some discussion about it. Question: Does there exist a Banach space $X$ , an operator $T:X\rightarrow X$ with operator norm $\|T\|_{op}\leq 1$ and a sequence of polynomials $p_n:\mathbb{C}\rightarrow \mathbb{C}$ so that $\sup_{x\in D} |p_n(x)|\rightarrow 0$ as $n$ goes to infinity, where $D$ is the unit dist, but $\|p_n(T)\|_{op}=1$ (or any constant greater than zero that is independent on $n$ )? Equivalently, the map $p\mapsto p(T)$ from $(\mathrm{Poly}(D),\|\cdot\|_\infty)$ to $(\mathcal{L}(X),\|\cdot\|_{op})$ is not continuous. Two notes: (1) It is not likely, but it could be that there are no such examples, in that case a proof or reference would be nice :-) (2) It is much appreciated if you can provide an example in the case where $X=L^p(\Omega)$ for every $p>2$ , where $\Omega$ is some measured space (say $\Omega=[0,1]$ ). Discussion:
I do know that there is no counterexample in the case where $X$ is a Hilbert space (edit: and $T$ is normal), but that result requires the use of Gelfand theory, which is not valid for arbitrary Banach spaces (because we do not have natural $C^\star$ algebras). Let $H$ be a Hilbert space and $T:H\rightarrow H$ be a continuous operator. For a polynomial $p(x) = \sum_n a_n x^n$ we can define a new operator $p(T) = \sum_n a_n\cdot T^n$ on $H$ . A well known, seemingly trivial, but extremely deep result in functional analysis is the fact that the operator norm of $p(T)$ is exactly the supremum of $p$ on the spectrum of $T$ . The main key point in the proof is the fact that for an operator $T$ , if $\sigma(T)$ is the spectrum of $T$ then $$\|T\|_{op} = \max \sigma(T).$$ This combined with the fact that $\sigma(p(T))=p(\sigma(T))$ gives the assertion above immediately. (The equality, $\|T\|_{op} = \max \sigma(T)$ , does not seem to be true for arbitrary Banach spaces, at least I fail to incorporate the original proof, because Gelfand theory is not valid.) Now, if we assume that $\|T\|_{op}\leq 1$ , we can already deduce that $\sigma(T)\subseteq D$ . Therefore, if $p_n$ are polynomials so that $\|p_n\|_{\infty,D}\rightarrow 0$ , then $\|p_n(T)\|_{op}\leq \|p_n\|_{\infty,D}\rightarrow 0$ . This proves that if $X$ is an Hilbert space then there is no example of the form I am looking for. I suspect that this is not true for arbitrary Banach spaces. A concrete example will help me greatly.","['operator-algebras', 'operator-theory', 'lp-spaces', 'gelfand-representation', 'functional-analysis']"
4502740,Fundamental group of the boundary of a torus with a point removed,"The question below is from an old topology qualifying exam. I am mostly stuck on parts (c) and (d). Let $X$ be a 2-dimensional torus $T^2$ with the interior of a small disk $D \subset T^2$ removed (this space is also called a handle). (a) Prove that $X$ is homotopically equivalent to a bouquet of two circles $S^1 \vee S^1$ . To do this, itâ€™s convenient to represent $T^2$ as a unit square with the opposite sides
identified. I am fine with part (a) - I understand that if you puncture the torus, it deformation retracts to $S^1 \vee S^1$ . (b) Compute the fundamental group $\pi_1(X)$ of $X$ and present its generators explicitly
as loops in $X$ . And for part (b), I can use Van Kampen on $S^1 \vee S^1$ - however, I'm confused by the wording, and wonder if $\pi_1(X) = \langle a\rangle * \langle b\rangle$ is the correct answer. (c) Explicitly express the class of $\partial X = \partial D$ in $\pi_1(X)$ in terms of the generators from part (b). I tried to find the boundary in $\mathbb{R}^2$ using the definition: $$ \partial A = \overline{A} \cap \overline{(\mathbb{R}^2 - A)}$$ and the identification of $X$ as a square with a disc removed from part (a). I found that $\overline{T^2 - D}$ is $T^2 - D$ plus the boundary of $D$ (the picture on the left), and $\overline{(\mathbb{R}^2 - A)}$ is everything outside the square (including the boundary)[picture on the right]. So if I intersect them, I get that the boundary would be a circle inside a square (where both are not filled in)[the bottom picture]. Am I thinking of that correctly? I have no idea what the fundamental group of this would be (I can only guess it's $S^1 \vee S^1$ plus the circle on its own, which doesn't make sense to me. And by class, does that mean all the elements of the fundamental group, $\pi_1(X)$ , that are along the boundary of $X$ ? (d) Prove that $\partial X$ is not a retract of $X$ . Hint: What is $\pi_1(\partial X)$ ? Visually, it makes sense that the boundary can't retract to $X$ if I'm thinking of $\partial X$ correctly. I think for this part the best thing would be to find $\pi_1(X)$ , but I can't without part (c). I apologize for the atrocious pictures. Feel free to draw bad pictures to help with an answer!","['fundamental-groups', 'retraction', 'knot-theory', 'general-topology', 'algebraic-topology']"
4502746,If property holds for sample in this space does it hold almost surely?,"Suppose we have a sequence of coin tosses $B = B_1, B_2, \dots$ , which are not assumed to be neither fair or independent. Let $\mathcal{B}$ be the set of all infinite binary sequences, and let $T : \mathcal{B} \to \{0, 1\}$ be a discrete random variable. If $T(b) = 1$ for any sequence $b \in \mathcal{B}$ , does it follow that $\mathbb{P}(T(B) = 1) = 1$ ? Attempted argument: Let $(\mathcal{B}, \mathcal{F}, \mathbb{P})$ be the probability space associated with the random coin toss experiment above. Then $$\mathbb{P}(T(B) = 1) = \mathbb{P}(\{b \in \mathcal{B} : T(b) = 1\})$$ and since $\{b \in \mathcal{B} : T(b) = 1\} = \mathcal{B}$ , we have $$\mathbb{P}(T(B) = 1) = \mathbb{P}(b \in \mathcal{B}) = 1.$$ Is there something wrong with this? Does it qualify as a rigorous argument? A part of this that I am a little uneasy about is whether there exists a well-defined probability space $(\mathcal{B}, \mathcal{F}, \mathbb{P})$ for this process.","['measure-theory', 'probability-theory', 'probability']"
4502760,"stronger variant of ""$l^p$ spaces are increasing in $p$""ï¼š $\bigcup_{1\leq p\lneq q}l^p\neq l^q$","Problem Let $\mathbb{K}$ be either $\mathbb{R}$ or $\mathbb{C}$ . Let $x\in\mathbb{K}^\mathbb{N}$ be any sequence in $\mathbb{K}$ . Let the $p$ -norm be defined by $$\lVert x\rVert_p:=\left(\sum_{n\in\mathbb{N}}\left\lvert x_n\right\rvert^p\right)^\frac{1}{p}$$ for $1\leq p\lneq\infty$ and $$\lVert x\rVert_\infty:=\lim_{p\to\infty}\lVert x\rVert_p=\sup_{n\in\mathbb{N}}\left\lvert x_n\right\rvert$$ for $p=\infty$ . The $l^p$ -space is defined by $$l^p:=l^p(\mathbb{K}):=\left\{x\in\mathbb{K}^\mathbb{N}\ \mid\ \lVert x\rVert_p\lneq\infty\right\}$$ Let $1\leq q\leq\infty$ . Prove: $$\bigcup_{1\leq p\lneq q}l^p\subsetneq l^q$$ My Attempt I can prove the following weaker statement: Let $1\leq p\lneq q\leq\infty$ , then $l^p\subsetneq l^q$ . $$\forall x\in\mathbb{K}^\mathbb{N}: \lVert x\rVert_q\leq\lVert x\rVert_p\ \Rightarrow\ \left(\lVert x\rVert_p\lneq\infty \Rightarrow \lVert x\rVert_p\lneq\infty\right)\ \Rightarrow\ l^p\subseteq l^q$$ $1\leq p\lneq q\lneq \infty$ : Consider $x=\left(\frac{1}{n^{\frac{1}{p}}}\right)_{n\in\mathbb{N}}$ , then $\lVert x\rVert_p=\infty$ and $\lVert x\rVert_q\lneq\infty$ , because $\sum_{n\in\infty}\frac{1}{n^r}\lneq\infty \Leftrightarrow r>1$ . Therefore $x\in l_q$ and $x\notin l_p$ . $1\leq p\lneq q=\infty$ : Consider $x=(1)_{n\in\mathbb{N}}$ , then $\lVert x\rVert_\infty=1\lneq\infty$ and $\lVert x\rVert_p=\infty$ . Therefore $x\in l_\infty$ and $x\notin l_p$ . Question How can I prove $$\bigcup_{1\leq p\lneq q}l^p\neq l^q$$ for $1\leq p\lneq q\lneq \infty$ ? Remark A discussion of the weaker statement can be found here .","['lp-spaces', 'functional-analysis', 'sequences-and-series', 'limits', 'supremum-and-infimum']"
4502775,Is a distance function differentiable on $\Bbb R^n$?,"Let $C\subset \Bbb R^n$ be closed and convex. Define $d_X\colon \Bbb R^n\to \Bbb R$ by the formula $d_X(x):=\mathrm{dist}(x,C)$ . Is it differentiable outside $ C$ ? How to prove it? What's the derivative? Moreover, if it's true that it is not differentiable for any closed convex set $C$ and any $x\in\partial C$ ? (this isn't covered by my proof below and I'll be happy to see the answer to this question). Context: In the question the derivative of $d_X$ is considered. In the discussion there was a doubt if this function is always differentiable and the smooth boundary is assumed to force differentiability. While trying to answer the question I realized that the question of differentiability is interesting on its own and putting my proof as the answer to that question may obfuscate both the proof of differentiability of $d_X$ and the answer of that question. Short googling gave the following statement in an article from Transactions of the AMS : 'For convex $C$ , the
differentiability of $d_C$ everywhere outside of $C$ is well known' (page 1 line -4). So it turns out that the condition on $\partial X$ is not needed! Of course if $x\in \mathrm{int}\,C$ then $\nabla d_X(x)=0$ .","['derivatives', 'real-analysis']"
4502799,Expected value of the expression $3 â–¡ 3 â–¡ 3 â–¡ â‹¯ â–¡ 3 â–¡ 3$,"Let $p = 2017$ be a prime number. Let $E$ be the expected value of the expression $3
â–¡
3
â–¡
3
â–¡
â‹¯
â–¡
3
â–¡
3$ where there are $p
+
3$ threes and $p
+
2$ boxes, and one of the four arithmetic operations
{
+
,
âˆ’
,
Ã—
,
Ã·
}
is uniformly chosen at random to replace each of the boxes. If $E
=
\frac{m}{n}$ , where $m$ and $n$ are relatively prime positive integers, find the remainder when $m
+
n$ is divided by $p$ . It's a question from https://gonitzoggo.com/archive/problem/431/english Recently, I've been taking preparation for junior Math Olympiad Contest and found this problem. I've tried many combinations of arithmetic operations to find the value of $E $ but ended up each time getting a new value which doesn't match with the answer. How can I get the exact value of $E$ ?","['contest-math', 'expected-value', 'discrete-mathematics']"
4502845,Matrix problems over a finite field,"Assume $F_p$ is a finite field, $|F_p|=p$ and $M_n(F_p)$ is all the $n\times n$ matrices over $F_p$ . A relation is defined over $M_n(F_p)$ : $$A,B\in M_n(F_p),A\sim B \Leftrightarrow \exists~~u,v\in \mathbb{Z^+}~~A^u=B^v.$$ It is easy to verify that this is an equivalence relation.. So for any $A\in M_n(F_p)$ , we define $$S_A=\left\{B\in M_n(F_p)|B\sim A\right\}.$$ So we have $$S_0=\left\{ B\in M_n(F_p)|B\sim 0 \right\} $$ and $$S_I=\left\{ B\in M_n(F_p)|B\sim I \right\},$$ where $0$ is the $0$ matrix and $I$ is the identity matrix. The question is how to calculate the cardinality of $S_0$ and $S_I$ . If we define $$S=\left\{S_A|A\in M_n(F_p)\right\},$$ the question is how to calculate the cardinality of $S$ . By reviewing the article, I have known that $S_0$ is the $\mathrm{Fine}-\mathrm{Herstein}$ theorem, so $|S_0|=q^{n^2-n}.$ I would like to ask about the remaining two questions. I would like to get some articles or links to articles. Thanks.","['equivalence-relations', 'finite-fields', 'reference-request', 'matrices', 'linear-algebra']"
4502884,Why is the Lebesgue Integral defined through integrals of simple functions?,"I'm reading the Measure Theory and Integration chapter from Terrance Tao's Analysis-2. In the chapter of Lebesgue Integration (11), initially the integral is introduced for positive valued functions and then generalized to negative valued function by first defining a way to split up the negative and positive value of function. What I don't understand is, why can't we directly integrate functions which take any value in the Lebesgue Integration? Could someone give an intuition to me on why we do this? I understand stuff like defining the measure function, why we need measurable sets , measurable functions etc but this point I am confused as heck. Edit : I found this relevant MO post https://mathoverflow.net/q/25161/159957","['measure-theory', 'lebesgue-integral']"
4502893,"Find the relationship among adjusted 3-pointer (not parameter, but formula like Pythagenport, Pythagenpat), goals scored and goals allowed in football","I was seeking a formula to relate goals scored and goals allowed to points in association football . In association football, three points are awarded to the team winning a match, with no points awarded to the losing team. If the game is drawn, each team receives one point. Firstly, I worked with points percentage â€” points divided by available points, which I set to the number of matches multiplied by adjusted 3-pointer. The general form was: $${\rm points}\,{\rm percentage}= \frac{{\rm goals}\,{\rm scored}^{\mathcal{A}}}{{\rm goals}\,{\rm scored}^{\mathcal{A}}+ {\rm goals}\,{\rm allowed}^{\mathcal{B}}}$$ The league tables' total points percentage is almost sure not an integer . This is Pythagorean expectation (only for wins/losses, not for draws) with two forced different exponents! Football presents a Pythagorean challenge because about a quarter of its matches end in draws (*related to trimean ? no .), worth one point each in the league tables. Because wins are worth three points, the most accurate Pythagorean analyses overestimate most teamsâ€™ points totals, as football analyst Howard Hamilton has found . I call adjusted 3-pointer instead of three because how many average points each team receives are not same. I want to achieve 'reasonable' relationship among adjusted 3-pointer (not parameter, but formula like Pythagenport , Pythagenpat ), goals scored and goals allowed for each team: $${\rm points}_{predicted}= {\rm points}\,{\rm percentage}\cdot{\rm adjusted}\,3{\rm -pointer}$$ If you treat adjusted 3-pointer as parameter : Here is the win estimator (Python Code) # 17 Th12 '12
https://docs.google.com/spreadsheets/d/1XwKs4Do3x95hGTnzgwJWJqCXa4Zqns74bXPwXFFv3IE

import scipy.optimize
import numpy as np
x=np.array([33,43,28,26,28,30,26,24,15,21,23,28,19,18,19,22,15,19,18,15])
y=np.array([15,24,17,16,21,25,22,21,13,20,23,29,25,24,26,32,24,31,32,30])
z=np.array([36,42,29,24,27,29,23,27,24,23,22,20,25,16,17,15,18, 9,15,10])
N=np.array([17,17,16,16,17,17,17,17,17,17,17,17,17,17,17,16,17,16,17,17])
pred=lambda w: (x**w[0]/(x**w[0]+y**w[1]))*(2+1/(w[2]**2+1))*N
w,_=scipy.optimize.leastsq(lambda w: (z-pred(w)),(1,1,.0009332150))
w print(w[0])
print(w[1])
print(2+1/(w[2]**2+1)) Hence, we obtain A=1.5981207542056926
B=1.6055482567109685
adjusted 3-pointer=2.742957368624336 As you can see, I choose ${\rm adjusted}\,3{\rm -pointer}= 2+ 1/(w^{2}+ 1)\in\left ( 2, 3 \right )$ because the average total points of football matches is on this interval. And ${\rm adjusted}\,3{\rm -pointer}\lessapprox 2.75$ is scientific* (noted above). On the other side, $w> {\rm Best}\,w\Rightarrow 2\lessapprox{\rm adjusted}\,3{\rm -pointer}$ leads to many under-performing teams, despite of well RMSE, I still want adjusted 3-pointer as a formula ( function ), but parameter. I have tried to turning that into an ODE (In fact, a PDE.) but unsuccessfully. I need your help to find it.","['statistics', 'functional-inequalities', 'regression', 'linear-pde', 'optimization']"
4502957,Is $2p-2$ a period of $\{a_n\}$ modulo a prime $p$?,"Let ${a_0}=1$ , ${a_n}=\sum_{k=0}^{n-1} \binom{2n}{2k}a_ka_{n-k-1}$ for $n>0$ . That is, ${a_n}=$ A002067 (n) in OEIS. Question: for any prime $p$ , is $2p-2$ a period of $\{a_n\}$ modulo $p$ ? And it looks like $p-1$ is a period of $\{a_n\}$ modulo $p$ when $p\equiv3\mod 4$ .","['number-theory', 'modular-arithmetic', 'elementary-number-theory']"
4502962,Computation of adjoint operator,"I am struggeling with the computation of the adjoint operator $T^*$ to the following given operator $T$ : Let $T: \mathbb{R}^n \rightarrow L^2(0,1), u_0 \mapsto u(t)$ , where $u(t)$ is given via \begin{align*}
u'(t) &= Au, t>0\\
u(0) &= u_0
\end{align*} I know that the adjoint operator is defined through the equation \begin{align*}
 \langle T^*v, u_0 \rangle_2 = \langle v, Tu_0\rangle_{L^2(0,1)}
\end{align*} Since $T$ is defined in an implicit way through the given differential equation, I assumed that $T^*$ has to be definite in a similar way through another ODE. I tried the following: \begin{align*}
\langle T^*v, u_0 \rangle_2 = \langle v, Tu_0\rangle_{L^2(0,1)} = \int_0^1 v(t) \cdot Tu_0(t) dt = \int_0^1 v(t) \cdot u(t) dt 
\end{align*} Then I tried to use integration by parts but that didn't seem to help since there are no derivatives in the integral. Can someone give me a hint? Edit: I am looking for a representation of $T^*$ that derives from the solution of the corresponding adjoint problem. So I tried to derive equations that have to be satisfied by $v$ . Unfortunately, I wasn't very successful so far. Thanks in advance:)","['adjoint-operators', 'ordinary-differential-equations']"
4502967,$x^{10}+x^{11}+\dots+x^{20}$ divided by $x^3+x$. Remainder?,"Question: If $x^{10}+x^{11}+\dots+x^{20}$ is divided by $x^3+x$ , then what is the remainder? Options: (A) $x\qquad\quad$ (B) $-x\qquad\quad$ (C) $x^2\qquad\quad$ (D) $-x^2$ In these types of questions generally I follow the following approach: Since divisor is cubic so the remainder must be a constant/linear/quadratic expression. $\Rightarrow F(x)=(x^3+x)Q(x)+ax^2+bx+c$ For $x=0$ , we get $c=0$ But since $x^3+x$ has no other roots so I can't find $a$ and $b$ .
Please help. Answer: Option (B)","['contest-math', 'roots', 'calculus', 'polynomials', 'algebra-precalculus']"
4502987,Tao Analysis. Definition of positive rational numbers.,"Tao Analysis. Definition of positive rational numbers. Definition $4.2.6.$ A rational number $ x$ is said to be positive iff we have $x = a/b$ for some positive integers $a$ and $b$ . It is said to be negative iff
we have $x = âˆ’y$ for some positive rational $y$ ( $i.e., $$x$ = $(âˆ’a)/b$ for some
positive integers $a$ and $b$ ). My question: Does not $x = a/b$ for some negative integers $a$ and $b$ satisfy? as in $x = -2/-3$ . So why does this definition use iff?  Thank you!",['analysis']
4502996,Approximating the roots of $\sum_{k=1}^n{\sin{\left(\frac{x}{k}\right)}}$ for large $n$ and large $x$.,"Here is the graph of $\sum_{k=1}^{n}{\sin{\left(\frac{x}{k}\right)}}$ for $n=1000$ . There appear to be roots, or actually ""clusters"" of roots, at: $x\approx2.17n$ $x\approx5.05n$ $x\approx8.07n$ $x\approx11.2n$ $x\approx14.3n$ $x\approx17.4n$ $x\approx20.5n$ $x\approx23.7n$ $x\approx26.9n$ It seems that, for large $n$ and large $x$ , the gap between clusters is $\pi n$ . For large $n$ and large $x$ , what pattern emerges for the locations of these clusters of roots, and why? My attempt: Expressing the series as a Riemann sum, we are looking for the values of $x$ such that $$n\int_0^1{\sin{\left(\frac{x/n}{t}\right)}}dt=0$$ What values of $x$ satisfy this equation?","['approximation', 'roots', 'real-analysis', 'sequences-and-series', 'trigonometry']"
4503017,What are the possible values of $\lim\limits_{n\to\infty} \prod_{i=1}^n (1+ y_i^{\frac 1n}/n)$?,"For any sequence $(y_i)_{i=1}^\infty$ of positive real numbers, let $(X_n)_{n=1}^\infty$ be another sequence defined by $$X_n = \prod_{i=1}^n \left(1+\frac{y_i^{\frac{1}{n}}}{n}\right).$$ It is clear that $X_n >1$ for all $n$ . Question: For any $c\ge 1$ , can one find a sequence $(y_i)$ of positive real numbers such that $\left(X_n\right)$ converges to $c$ as $n\to \infty$ ? In an answer to a previous question , it is shown that if $y_i, y_i^{-1}$ grows slower than polynomials, then the limit of $(X_n)$ is always $e$ . There are also simple examples $\left(y_i = \left(i^2\right)^i\right)$ which shows that $(X_n)$ might be unbounded. Some remarks: One can find $(y_i)$ such that $(X_n)$ is bounded, but has no limit (this can be done by choosing $y_i$ to be $1$ most of the time, and a huge number once in a while), Changing finitely many members of $(y_i)$ do not alter the limit of $(X_n)$ . Think of $(y_i)$ as a function $y: \mathbb N \to (0,\infty)$ . Then $f_n : \mathbb N \to (0,\infty)$ defined by $f_n(i) = y_i^{1/n}$ is a sequence of functions which converges pointwisely to the constant function $1$ . If the convergence is uniform, then limit of $(X_n)$ exists, but again the limit is $e$ .","['limits', 'sequences-and-series', 'real-analysis']"
4503051,A property of the convergents of the continued fraction expansion of a rational number,"I'm looking for a proof of the following result (theorem 6.14 of the book Cryptography. Theory and practice by Paterson and Stinson): Theorem 6.14 Suppose that $\text{gcd}(a,b) = \text{gcd}(c,d) = 1$ and: $$\Bigl\rvert \frac{a}{b} - \frac{c}{d}\Bigr\rvert < \frac{1}{2d^2}$$ Then $c/d$ is one of the convergents of the continued fraction expansion of $a/b$ . I already know the analogous result for irrational numbers (theorem 7.14 of the book An Introduction to the Theory of Numbers by Niven, Zuckerman and Montgomery with a slight change of notations to avoid ambiguities): Theorem 7.14 Let $\xi$ denote any irrational number. If there is a rational number $c/d$ with $d \geq 1$ such that: $$\Bigl\rvert \xi - \frac{c}{d} \Bigr\rvert < \frac{1}{2d^2}$$ Then $c/d$ equals one of the convergents of the continued fraction expansion of $\xi$ . However, it seems to me that the proof of theorem 7.14 cannot immediately be generalized to the case of rational numbers: one calls $h_j/k_j$ the $j$ -th convergent of the simple continued fraction expansion of $\xi$ and then one uses the fact that $\lim k_j = +\infty$ to pick an integer $n$ such that $k_n \leq b < k_{n+1}$ . This cannot be done in the case of rational numbers (theorem 6.14) simply because the convergents of the continued fraction expansion of $a/b$ are finite: it might be the case that the convergents of $a/b$ are $h_0/k_0,h_1/k_1,\dots,h_m/k_m = a/b$ and that $b \leq d$ . So, how exactly is theorem 6.14 established?","['number-theory', 'cryptography', 'continued-fractions', 'inequality', 'rational-numbers']"
4503061,"Tossing 2 Coins, Fair coin and a two headed coin","There are two coins. One is two headed coin, the other one is a fair coin. A coin is selected randomly and flipped. a) what is the probability of selecting a fair coin? b) what is the probability of head from the flipped coin? c) what is the probability of selecting a fair coin and having head on the top? So A and B are relatively simple, because A is $1/2$ and for B its $3/4$ but I am concerned with C Shouldn't be the chance of having a fair coin $+$ head
is basically $1/4$ ? Correct me if I am wrong but If I want to pick a fair coin, the probability is $0.5$ , lets say we picked it, and now we are going to throw it, isn't also chance between head and tail is $0.5$ ?
Which effectively means that getting a head from a fair coin the intersection between them is $0.5\times0.5$ which equals $0.25$ ?","['probability-theory', 'probability']"
4503067,Computing whether a particular finitely presented group is infinite with GAP,"I am working with GAP 4 and trying to check whether a finitely presented group is infinite.
Since the Size -function dies on me I followed the suggestion in the GAP manual to try working with the low index subgroups.
Based on https://www.gap-system.org/Doc/Examples/cavicchioli.html I came up with the following code: F:=FreeGroup([""f1"", ""f2"", ""t1"", ""t2"", ""t3""]);
AssignGeneratorVariables(F);
T:=F/[f1^2,f2^2,t1^3,t3^3,(t2)^5,(t1*f1)^2,t3^-1*f1*t2,f2*t2^-1*t1^-1];

maxIndex:=30;
u := LowIndexSubgroupsFpGroup(T,TrivialSubgroup(T),maxIndex);
u := Filtered(u,i->Index(T,i)>1);
Collected(List(u,i->IsInfiniteAbelianizationGroup(i))); Which gives me the result: [ [ true, 1 ], [ false, 30 ] ] Can this be counted as a proof that the finitely presented group above is infinite? This question is related to Finding low-index normal subgroups of finitely presented groups in GAP .","['group-presentation', 'group-theory', 'infinite-groups', 'gap']"
4503096,Regular heptagon area formula,"Be a regular heptagon. Can anyone demonstrate this formula? I know:
Let apothem = ap side = l $ S=\frac{p.ap}{2}\implies S = \frac{7l.ap}{2}\\
\triangle OBC: OC^2 = a^2+l^2\\
\triangle PAB: b^2+l^2 = PB^2\\
OC^2-a^2=PB^2-b^2$ but i can't go on","['euclidean-geometry', 'geometry', 'plane-geometry']"
